commit 42612e7763315cf5d43c4422781e75f9ee57597a
Merge: ad57a1022f9e b7b911d59dac
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jun 9 11:28:59 2020 -0700

    Merge tag 'f2fs-for-5.8' of git://git.kernel.org/pub/scm/linux/kernel/git/jaegeuk/f2fs
    
    Pull f2fs updates from Jaegeuk Kim:
     "In this round, we've added some knobs to enhance compression feature
      and harden testing environment. In addition, we've fixed several bugs
      reported from Android devices such as long discarding latency, device
      hanging during quota_sync, etc.
    
      Enhancements:
       - support lzo-rle algorithm
       - add two ioctls to release and reserve blocks for compression
       - support partial truncation/fiemap on compressed file
       - introduce sysfs entries to attach IO flags explicitly
       - add iostat trace point along with read io stat
    
      Bug fixes:
       - fix long discard latency
       - flush quota data by f2fs_quota_sync correctly
       - fix to recover parent inode number for power-cut recovery
       - fix lz4/zstd output buffer budget
       - parse checkpoint mount option correctly
       - avoid inifinite loop to wait for flushing node/meta pages
       - manage discard space correctly
    
      And some refactoring and clean up patches were added"
    
    * tag 'f2fs-for-5.8' of git://git.kernel.org/pub/scm/linux/kernel/git/jaegeuk/f2fs: (51 commits)
      f2fs: attach IO flags to the missing cases
      f2fs: add node_io_flag for bio flags likewise data_io_flag
      f2fs: remove unused parameter of f2fs_put_rpages_mapping()
      f2fs: handle readonly filesystem in f2fs_ioc_shutdown()
      f2fs: avoid utf8_strncasecmp() with unstable name
      f2fs: don't return vmalloc() memory from f2fs_kmalloc()
      f2fs: fix retry logic in f2fs_write_cache_pages()
      f2fs: fix wrong discard space
      f2fs: compress: don't compress any datas after cp stop
      f2fs: remove unneeded return value of __insert_discard_tree()
      f2fs: fix wrong value of tracepoint parameter
      f2fs: protect new segment allocation in expand_inode_data
      f2fs: code cleanup by removing ifdef macro surrounding
      f2fs: avoid inifinite loop to wait for flushing node pages at cp_error
      f2fs: flush dirty meta pages when flushing them
      f2fs: fix checkpoint=disable:%u%%
      f2fs: compress: fix zstd data corruption
      f2fs: add compressed/gc data read IO stat
      f2fs: fix potential use-after-free issue
      f2fs: compress: don't handle non-compressed data in workqueue
      ...

commit b7b911d59dacb47511a1e604bbfa901beb108305
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Jun 4 16:45:30 2020 -0700

    f2fs: attach IO flags to the missing cases
    
    This adds more IOs to attach flags.
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 798c325a820d..267b5e76a02b 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -691,6 +691,7 @@ int f2fs_submit_page_bio(struct f2fs_io_info *fio)
 	if (fio->io_wbc && !is_read_io(fio->op))
 		wbc_account_cgroup_owner(fio->io_wbc, page, PAGE_SIZE);
 
+	__attach_io_flag(fio);
 	bio_set_op_attrs(bio, fio->op, fio->op_flags);
 
 	inc_page_count(fio->sbi, is_read_io(fio->op) ?
@@ -877,6 +878,7 @@ int f2fs_merge_page_bio(struct f2fs_io_info *fio)
 alloc_new:
 	if (!bio) {
 		bio = __bio_alloc(fio, BIO_MAX_PAGES);
+		__attach_io_flag(fio);
 		bio_set_op_attrs(bio, fio->op, fio->op_flags);
 
 		add_bio_entry(fio->sbi, bio, page, fio->temp);

commit 32b6aba85c8dbd1d3a155543986574c6969f0a48
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Jun 4 11:49:43 2020 -0700

    f2fs: add node_io_flag for bio flags likewise data_io_flag
    
    This patch adds another way to attach bio flags to node writes.
    
    Description:   Give a way to attach REQ_META|FUA to node writes
                   given temperature-based bits. Now the bits indicate:
                   *      REQ_META     |      REQ_FUA      |
                   *    5 |    4 |   3 |    2 |    1 |   0 |
                   * Cold | Warm | Hot | Cold | Warm | Hot |
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index a65bfc07ddb9..798c325a820d 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -514,22 +514,28 @@ void f2fs_submit_bio(struct f2fs_sb_info *sbi,
 	__submit_bio(sbi, bio, type);
 }
 
-static void __attach_data_io_flag(struct f2fs_io_info *fio)
+static void __attach_io_flag(struct f2fs_io_info *fio)
 {
 	struct f2fs_sb_info *sbi = fio->sbi;
 	unsigned int temp_mask = (1 << NR_TEMP_TYPE) - 1;
-	unsigned int fua_flag = sbi->data_io_flag & temp_mask;
-	unsigned int meta_flag = (sbi->data_io_flag >> NR_TEMP_TYPE) &
-								temp_mask;
+	unsigned int io_flag, fua_flag, meta_flag;
+
+	if (fio->type == DATA)
+		io_flag = sbi->data_io_flag;
+	else if (fio->type == NODE)
+		io_flag = sbi->node_io_flag;
+	else
+		return;
+
+	fua_flag = io_flag & temp_mask;
+	meta_flag = (io_flag >> NR_TEMP_TYPE) & temp_mask;
+
 	/*
-	 * data io flag bits per temp:
+	 * data/node io flag bits per temp:
 	 *      REQ_META     |      REQ_FUA      |
 	 *    5 |    4 |   3 |    2 |    1 |   0 |
 	 * Cold | Warm | Hot | Cold | Warm | Hot |
 	 */
-	if (fio->type != DATA)
-		return;
-
 	if ((1 << fio->temp) & meta_flag)
 		fio->op_flags |= REQ_META;
 	if ((1 << fio->temp) & fua_flag)
@@ -543,7 +549,7 @@ static void __submit_merged_bio(struct f2fs_bio_info *io)
 	if (!io->bio)
 		return;
 
-	__attach_data_io_flag(fio);
+	__attach_io_flag(fio);
 	bio_set_op_attrs(io->bio, fio->op, fio->op_flags);
 
 	if (is_read_io(fio->op))

commit 0b166a57e6222666292a481b742af92b50c3ba50
Merge: b25c6644bfd3 6b8ed62008a4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jun 5 16:19:28 2020 -0700

    Merge tag 'ext4_for_linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tytso/ext4
    
    Pull ext4 updates from Ted Ts'o:
     "A lot of bug fixes and cleanups for ext4, including:
    
       - Fix performance problems found in dioread_nolock now that it is the
         default, caused by transaction leaks.
    
       - Clean up fiemap handling in ext4
    
       - Clean up and refactor multiple block allocator (mballoc) code
    
       - Fix a problem with mballoc with a smaller file systems running out
         of blocks because they couldn't properly use blocks that had been
         reserved by inode preallocation.
    
       - Fixed a race in ext4_sync_parent() versus rename()
    
       - Simplify the error handling in the extent manipulation code
    
       - Make sure all metadata I/O errors are felected to
         ext4_ext_dirty()'s and ext4_make_inode_dirty()'s callers.
    
       - Avoid passing an error pointer to brelse in ext4_xattr_set()
    
       - Fix race which could result to freeing an inode on the dirty last
         in data=journal mode.
    
       - Fix refcount handling if ext4_iget() fails
    
       - Fix a crash in generic/019 caused by a corrupted extent node"
    
    * tag 'ext4_for_linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tytso/ext4: (58 commits)
      ext4: avoid unnecessary transaction starts during writeback
      ext4: don't block for O_DIRECT if IOCB_NOWAIT is set
      ext4: remove the access_ok() check in ext4_ioctl_get_es_cache
      fs: remove the access_ok() check in ioctl_fiemap
      fs: handle FIEMAP_FLAG_SYNC in fiemap_prep
      fs: move fiemap range validation into the file systems instances
      iomap: fix the iomap_fiemap prototype
      fs: move the fiemap definitions out of fs.h
      fs: mark __generic_block_fiemap static
      ext4: remove the call to fiemap_check_flags in ext4_fiemap
      ext4: split _ext4_fiemap
      ext4: fix fiemap size checks for bitmap files
      ext4: fix EXT4_MAX_LOGICAL_BLOCK macro
      add comment for ext4_dir_entry_2 file_type member
      jbd2: avoid leaking transaction credits when unreserving handle
      ext4: drop ext4_journal_free_reserved()
      ext4: mballoc: use lock for checking free blocks while retrying
      ext4: mballoc: refactor ext4_mb_good_group()
      ext4: mballoc: introduce pcpu seqcnt for freeing PA to improve ENOSPC handling
      ext4: mballoc: refactor ext4_mb_discard_preallocations()
      ...

commit e78790f84a5417287965a06cd4dea85df0743935
Author: Sahitya Tummala <stummala@codeaurora.org>
Date:   Tue Jun 2 18:11:47 2020 +0530

    f2fs: fix retry logic in f2fs_write_cache_pages()
    
    In case a compressed file is getting overwritten, the current retry
    logic doesn't include the current page to be retried now as it sets
    the new start index as 0 and new end index as writeback_index - 1.
    This causes the corresponding cluster to be uncompressed and written
    as normal pages without compression. Fix this by allowing writeback to
    be retried for the current page as well (in case of compressed page
    getting retried due to index mismatch with cluster index). So that
    this cluster can be written compressed in case of overwrite.
    
    Also, align f2fs_write_cache_pages() according to the change -
    <64081362e8ff>("mm/page-writeback.c: fix range_cyclic writeback vs
    writepages deadlock").
    
    Signed-off-by: Sahitya Tummala <stummala@codeaurora.org>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 48a622b95b76..a65bfc07ddb9 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -2861,7 +2861,6 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 	pgoff_t index;
 	pgoff_t end;		/* Inclusive */
 	pgoff_t done_index;
-	int cycled;
 	int range_whole = 0;
 	xa_mark_t tag;
 	int nwritten = 0;
@@ -2879,17 +2878,12 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 	if (wbc->range_cyclic) {
 		writeback_index = mapping->writeback_index; /* prev offset */
 		index = writeback_index;
-		if (index == 0)
-			cycled = 1;
-		else
-			cycled = 0;
 		end = -1;
 	} else {
 		index = wbc->range_start >> PAGE_SHIFT;
 		end = wbc->range_end >> PAGE_SHIFT;
 		if (wbc->range_start == 0 && wbc->range_end == LLONG_MAX)
 			range_whole = 1;
-		cycled = 1; /* ignore range_cyclic tests */
 	}
 	if (wbc->sync_mode == WB_SYNC_ALL || wbc->tagged_writepages)
 		tag = PAGECACHE_TAG_TOWRITE;
@@ -3054,12 +3048,13 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 		}
 	}
 #endif
-	if ((!cycled && !done) || retry) {
-		cycled = 1;
+	if (retry) {
 		index = 0;
-		end = writeback_index - 1;
+		end = -1;
 		goto retry;
 	}
+	if (wbc->range_cyclic && !done)
+		done_index = 0;
 	if (wbc->range_cyclic || (range_whole && wbc->nr_to_write > 0))
 		mapping->writeback_index = done_index;
 

commit 45dd052e67ad17c7a24874a783f41aeab15bc294
Author: Christoph Hellwig <hch@lst.de>
Date:   Sat May 23 09:30:14 2020 +0200

    fs: handle FIEMAP_FLAG_SYNC in fiemap_prep
    
    By moving FIEMAP_FLAG_SYNC handling to fiemap_prep we ensure it is
    handled once instead of duplicated, but can still be done under fs locks,
    like xfs/iomap intended with its duplicate handling.  Also make sure the
    error value of filemap_write_and_wait is propagated to user space.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Amir Goldstein <amir73il@gmail.com>
    Reviewed-by: Darrick J. Wong <darrick.wong@oracle.com>
    Link: https://lore.kernel.org/r/20200523073016.2944131-8-hch@lst.de
    Signed-off-by: Theodore Ts'o <tytso@mit.edu>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 03faafc591b1..9de7dc476ed1 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1825,8 +1825,7 @@ int f2fs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,
 			return ret;
 	}
 
-	ret = fiemap_prep(inode, fieinfo, start, &len,
-			FIEMAP_FLAG_SYNC | FIEMAP_FLAG_XATTR);
+	ret = fiemap_prep(inode, fieinfo, start, &len, FIEMAP_FLAG_XATTR);
 	if (ret)
 		return ret;
 

commit cddf8a2c4a8286ae60fc866eab59c8bc524e93a0
Author: Christoph Hellwig <hch@lst.de>
Date:   Sat May 23 09:30:13 2020 +0200

    fs: move fiemap range validation into the file systems instances
    
    Replace fiemap_check_flags with a fiemap_prep helper that also takes the
    inode and mapped range, and performs the sanity check and truncation
    previously done in fiemap_check_range.  This way the validation is inside
    the file system itself and thus properly works for the stacked overlayfs
    case as well.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Amir Goldstein <amir73il@gmail.com>
    Reviewed-by: Darrick J. Wong <darrick.wong@oracle.com>
    Link: https://lore.kernel.org/r/20200523073016.2944131-7-hch@lst.de
    Signed-off-by: Theodore Ts'o <tytso@mit.edu>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 25abbbb65ba0..03faafc591b1 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1825,7 +1825,8 @@ int f2fs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,
 			return ret;
 	}
 
-	ret = fiemap_check_flags(fieinfo, FIEMAP_FLAG_SYNC | FIEMAP_FLAG_XATTR);
+	ret = fiemap_prep(inode, fieinfo, start, &len,
+			FIEMAP_FLAG_SYNC | FIEMAP_FLAG_XATTR);
 	if (ret)
 		return ret;
 

commit 10c5db286452b8c60e8f58e9a4c1cbc5a91e4e5b
Author: Christoph Hellwig <hch@lst.de>
Date:   Sat May 23 09:30:11 2020 +0200

    fs: move the fiemap definitions out of fs.h
    
    No need to pull the fiemap definitions into almost every file in the
    kernel build.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Ritesh Harjani <riteshh@linux.ibm.com>
    Reviewed-by: Darrick J. Wong <darrick.wong@oracle.com>
    Link: https://lore.kernel.org/r/20200523073016.2944131-5-hch@lst.de
    Signed-off-by: Theodore Ts'o <tytso@mit.edu>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index cdf2f626bea7..25abbbb65ba0 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -19,6 +19,7 @@
 #include <linux/uio.h>
 #include <linux/cleancache.h>
 #include <linux/sched/signal.h>
+#include <linux/fiemap.h>
 
 #include "f2fs.h"
 #include "node.h"

commit e20a7693644ebf6f8005d8cdc8c8ece49bb70253
Author: Matthew Wilcox (Oracle) <willy@infradead.org>
Date:   Mon Jun 1 21:47:27 2020 -0700

    f2fs: pass the inode to f2fs_mpage_readpages
    
    This function now only uses the mapping argument to look up the inode, and
    both callers already have the inode, so just pass the inode instead of the
    mapping.
    
    Signed-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: William Kucharski <william.kucharski@oracle.com>
    Reviewed-by: Eric Biggers <ebiggers@google.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Acked-by: Jaegeuk Kim <jaegeuk@kernel.org>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Cong Wang <xiyou.wangcong@gmail.com>
    Cc: Darrick J. Wong <darrick.wong@oracle.com>
    Cc: Dave Chinner <dchinner@redhat.com>
    Cc: Gao Xiang <gaoxiang25@huawei.com>
    Cc: John Hubbard <jhubbard@nvidia.com>
    Cc: Joseph Qi <joseph.qi@linux.alibaba.com>
    Cc: Junxiao Bi <junxiao.bi@oracle.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Zi Yan <ziy@nvidia.com>
    Cc: Johannes Thumshirn <johannes.thumshirn@wdc.com>
    Cc: Miklos Szeredi <mszeredi@redhat.com>
    Link: http://lkml.kernel.org/r/20200414150233.24495-24-willy@infradead.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 5b72945bf9f1..03ec97f28235 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -2177,12 +2177,11 @@ int f2fs_read_multi_pages(struct compress_ctx *cc, struct bio **bio_ret,
  * use ->readpage() or do the necessary surgery to decouple ->readpages()
  * from read-ahead.
  */
-static int f2fs_mpage_readpages(struct address_space *mapping,
+static int f2fs_mpage_readpages(struct inode *inode,
 		struct readahead_control *rac, struct page *page)
 {
 	struct bio *bio = NULL;
 	sector_t last_block_in_bio = 0;
-	struct inode *inode = mapping->host;
 	struct f2fs_map_blocks map;
 #ifdef CONFIG_F2FS_FS_COMPRESSION
 	struct compress_ctx cc = {
@@ -2294,7 +2293,7 @@ static int f2fs_read_data_page(struct file *file, struct page *page)
 	if (f2fs_has_inline_data(inode))
 		ret = f2fs_read_inline_data(inode, page);
 	if (ret == -EAGAIN)
-		ret = f2fs_mpage_readpages(page_file_mapping(page), NULL, page);
+		ret = f2fs_mpage_readpages(inode, NULL, page);
 	return ret;
 }
 
@@ -2311,7 +2310,7 @@ static void f2fs_readahead(struct readahead_control *rac)
 	if (f2fs_has_inline_data(inode))
 		return;
 
-	f2fs_mpage_readpages(rac->mapping, rac, NULL);
+	f2fs_mpage_readpages(inode, rac, NULL);
 }
 
 int f2fs_encrypt_one_page(struct f2fs_io_info *fio)

commit 2332319625cc5c703f79d185ac9a53db20913748
Author: Matthew Wilcox (Oracle) <willy@infradead.org>
Date:   Mon Jun 1 21:47:23 2020 -0700

    f2fs: convert from readpages to readahead
    
    Use the new readahead operation in f2fs
    
    Signed-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: William Kucharski <william.kucharski@oracle.com>
    Reviewed-by: Eric Biggers <ebiggers@google.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Acked-by: Jaegeuk Kim <jaegeuk@kernel.org>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Cong Wang <xiyou.wangcong@gmail.com>
    Cc: Darrick J. Wong <darrick.wong@oracle.com>
    Cc: Dave Chinner <dchinner@redhat.com>
    Cc: Gao Xiang <gaoxiang25@huawei.com>
    Cc: John Hubbard <jhubbard@nvidia.com>
    Cc: Joseph Qi <joseph.qi@linux.alibaba.com>
    Cc: Junxiao Bi <junxiao.bi@oracle.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Zi Yan <ziy@nvidia.com>
    Cc: Johannes Thumshirn <johannes.thumshirn@wdc.com>
    Cc: Miklos Szeredi <mszeredi@redhat.com>
    Link: http://lkml.kernel.org/r/20200414150233.24495-23-willy@infradead.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index ae14e952df4f..5b72945bf9f1 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -2178,8 +2178,7 @@ int f2fs_read_multi_pages(struct compress_ctx *cc, struct bio **bio_ret,
  * from read-ahead.
  */
 static int f2fs_mpage_readpages(struct address_space *mapping,
-			struct list_head *pages, struct page *page,
-			unsigned nr_pages, bool is_readahead)
+		struct readahead_control *rac, struct page *page)
 {
 	struct bio *bio = NULL;
 	sector_t last_block_in_bio = 0;
@@ -2197,6 +2196,7 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 		.nr_cpages = 0,
 	};
 #endif
+	unsigned nr_pages = rac ? readahead_count(rac) : 1;
 	unsigned max_nr_pages = nr_pages;
 	int ret = 0;
 
@@ -2210,15 +2210,9 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 	map.m_may_create = false;
 
 	for (; nr_pages; nr_pages--) {
-		if (pages) {
-			page = list_last_entry(pages, struct page, lru);
-
+		if (rac) {
+			page = readahead_page(rac);
 			prefetchw(&page->flags);
-			list_del(&page->lru);
-			if (add_to_page_cache_lru(page, mapping,
-						  page_index(page),
-						  readahead_gfp_mask(mapping)))
-				goto next_page;
 		}
 
 #ifdef CONFIG_F2FS_FS_COMPRESSION
@@ -2228,7 +2222,7 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 				ret = f2fs_read_multi_pages(&cc, &bio,
 							max_nr_pages,
 							&last_block_in_bio,
-							is_readahead, false);
+							rac != NULL, false);
 				f2fs_destroy_compress_ctx(&cc);
 				if (ret)
 					goto set_error_page;
@@ -2251,7 +2245,7 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 #endif
 
 		ret = f2fs_read_single_page(inode, page, max_nr_pages, &map,
-					&bio, &last_block_in_bio, is_readahead);
+					&bio, &last_block_in_bio, rac);
 		if (ret) {
 #ifdef CONFIG_F2FS_FS_COMPRESSION
 set_error_page:
@@ -2260,8 +2254,10 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 			zero_user_segment(page, 0, PAGE_SIZE);
 			unlock_page(page);
 		}
+#ifdef CONFIG_F2FS_FS_COMPRESSION
 next_page:
-		if (pages)
+#endif
+		if (rac)
 			put_page(page);
 
 #ifdef CONFIG_F2FS_FS_COMPRESSION
@@ -2271,16 +2267,15 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 				ret = f2fs_read_multi_pages(&cc, &bio,
 							max_nr_pages,
 							&last_block_in_bio,
-							is_readahead, false);
+							rac != NULL, false);
 				f2fs_destroy_compress_ctx(&cc);
 			}
 		}
 #endif
 	}
-	BUG_ON(pages && !list_empty(pages));
 	if (bio)
 		__submit_bio(F2FS_I_SB(inode), bio, DATA);
-	return pages ? 0 : ret;
+	return ret;
 }
 
 static int f2fs_read_data_page(struct file *file, struct page *page)
@@ -2299,28 +2294,24 @@ static int f2fs_read_data_page(struct file *file, struct page *page)
 	if (f2fs_has_inline_data(inode))
 		ret = f2fs_read_inline_data(inode, page);
 	if (ret == -EAGAIN)
-		ret = f2fs_mpage_readpages(page_file_mapping(page),
-						NULL, page, 1, false);
+		ret = f2fs_mpage_readpages(page_file_mapping(page), NULL, page);
 	return ret;
 }
 
-static int f2fs_read_data_pages(struct file *file,
-			struct address_space *mapping,
-			struct list_head *pages, unsigned nr_pages)
+static void f2fs_readahead(struct readahead_control *rac)
 {
-	struct inode *inode = mapping->host;
-	struct page *page = list_last_entry(pages, struct page, lru);
+	struct inode *inode = rac->mapping->host;
 
-	trace_f2fs_readpages(inode, page, nr_pages);
+	trace_f2fs_readpages(inode, readahead_index(rac), readahead_count(rac));
 
 	if (!f2fs_is_compress_backend_ready(inode))
-		return 0;
+		return;
 
 	/* If the file has inline data, skip readpages */
 	if (f2fs_has_inline_data(inode))
-		return 0;
+		return;
 
-	return f2fs_mpage_readpages(mapping, pages, NULL, nr_pages, true);
+	f2fs_mpage_readpages(rac->mapping, rac, NULL);
 }
 
 int f2fs_encrypt_one_page(struct f2fs_io_info *fio)
@@ -3805,7 +3796,7 @@ static void f2fs_swap_deactivate(struct file *file)
 
 const struct address_space_operations f2fs_dblock_aops = {
 	.readpage	= f2fs_read_data_page,
-	.readpages	= f2fs_read_data_pages,
+	.readahead	= f2fs_readahead,
 	.writepage	= f2fs_write_data_page,
 	.writepages	= f2fs_write_data_pages,
 	.write_begin	= f2fs_write_begin,

commit 2c684234d36f7e8c80414e4a772911d407e821fa
Author: Matthew Wilcox (Oracle) <willy@infradead.org>
Date:   Mon Jun 1 21:46:51 2020 -0700

    mm: add page_cache_readahead_unbounded
    
    ext4 and f2fs have duplicated the guts of the readahead code so they can
    read past i_size.  Instead, separate out the guts of the readahead code
    so they can call it directly.
    
    Signed-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Tested-by: Eric Biggers <ebiggers@google.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: William Kucharski <william.kucharski@oracle.com>
    Reviewed-by: Eric Biggers <ebiggers@google.com>
    Cc: Chao Yu <yuchao0@huawei.com>
    Cc: Cong Wang <xiyou.wangcong@gmail.com>
    Cc: Darrick J. Wong <darrick.wong@oracle.com>
    Cc: Dave Chinner <dchinner@redhat.com>
    Cc: Gao Xiang <gaoxiang25@huawei.com>
    Cc: Jaegeuk Kim <jaegeuk@kernel.org>
    Cc: John Hubbard <jhubbard@nvidia.com>
    Cc: Joseph Qi <joseph.qi@linux.alibaba.com>
    Cc: Junxiao Bi <junxiao.bi@oracle.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Zi Yan <ziy@nvidia.com>
    Cc: Johannes Thumshirn <johannes.thumshirn@wdc.com>
    Cc: Miklos Szeredi <mszeredi@redhat.com>
    Link: http://lkml.kernel.org/r/20200414150233.24495-14-willy@infradead.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index cdf2f626bea7..ae14e952df4f 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -2177,7 +2177,7 @@ int f2fs_read_multi_pages(struct compress_ctx *cc, struct bio **bio_ret,
  * use ->readpage() or do the necessary surgery to decouple ->readpages()
  * from read-ahead.
  */
-int f2fs_mpage_readpages(struct address_space *mapping,
+static int f2fs_mpage_readpages(struct address_space *mapping,
 			struct list_head *pages, struct page *page,
 			unsigned nr_pages, bool is_readahead)
 {

commit 9c1223845a37ce09fd498b8c8ed061decff20eda
Author: Chao Yu <yuchao0@huawei.com>
Date:   Thu Apr 23 18:03:06 2020 +0800

    f2fs: add compressed/gc data read IO stat
    
    in order to account data read IOs more accurately.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 4d871d27a85f..48a622b95b76 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -2234,6 +2234,7 @@ int f2fs_read_multi_pages(struct compress_ctx *cc, struct bio **bio_ret,
 
 		inc_page_count(sbi, F2FS_RD_DATA);
 		f2fs_update_iostat(sbi, FS_DATA_READ_IO, F2FS_BLKSIZE);
+		f2fs_update_iostat(sbi, FS_CDATA_READ_IO, F2FS_BLKSIZE);
 		ClearPageError(page);
 		*last_block_in_bio = blkaddr;
 	}

commit f3494345ce9999624b36109252a4bf5f00e51a46
Author: Chao Yu <yuchao0@huawei.com>
Date:   Thu Apr 23 17:57:33 2020 +0800

    f2fs: fix potential use-after-free issue
    
    In error path of f2fs_read_multi_pages(), it should let last referrer
    release decompress io context memory, otherwise, other referrer will
    cause use-after-free issue.
    
    Fixes: 4c8ff7095bef ("f2fs: support data compression")
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 8607c02e8f96..4d871d27a85f 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -2208,16 +2208,16 @@ int f2fs_read_multi_pages(struct compress_ctx *cc, struct bio **bio_ret,
 					page->index, for_write);
 			if (IS_ERR(bio)) {
 				ret = PTR_ERR(bio);
-				bio = NULL;
 				dic->failed = true;
 				if (refcount_sub_and_test(dic->nr_cpages - i,
-							&dic->ref))
+							&dic->ref)) {
 					f2fs_decompress_end_io(dic->rpages,
 							cc->cluster_size, true,
 							false);
-				f2fs_free_dic(dic);
+					f2fs_free_dic(dic);
+				}
 				f2fs_put_dnode(&dn);
-				*bio_ret = bio;
+				*bio_ret = NULL;
 				return ret;
 			}
 		}

commit 03382f1aa99f438f8e9dc7562c7368f55ba8f56c
Author: Chao Yu <yuchao0@huawei.com>
Date:   Tue Apr 21 19:36:21 2020 +0800

    f2fs: compress: don't handle non-compressed data in workqueue
    
    If bio has no compressed data, we don't need to handle end_io work in
    workqueue, instead, it should just let interrupter handle it directly
    to speed up IO response.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index ff8a92df8d99..8607c02e8f96 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -114,7 +114,8 @@ static enum count_type __read_io_type(struct page *page)
 /* postprocessing steps for read bios */
 enum bio_post_read_step {
 	STEP_DECRYPT,
-	STEP_DECOMPRESS,
+	STEP_DECOMPRESS_NOWQ,		/* handle normal cluster data inplace */
+	STEP_DECOMPRESS,		/* handle compressed cluster data in workqueue */
 	STEP_VERITY,
 };
 
@@ -990,7 +991,7 @@ static struct bio *f2fs_grab_read_bio(struct inode *inode, block_t blkaddr,
 	if (f2fs_encrypted_file(inode))
 		post_read_steps |= 1 << STEP_DECRYPT;
 	if (f2fs_compressed_file(inode))
-		post_read_steps |= 1 << STEP_DECOMPRESS;
+		post_read_steps |= 1 << STEP_DECOMPRESS_NOWQ;
 	if (f2fs_need_verity(inode, first_idx))
 		post_read_steps |= 1 << STEP_VERITY;
 
@@ -2189,6 +2190,7 @@ int f2fs_read_multi_pages(struct compress_ctx *cc, struct bio **bio_ret,
 	for (i = 0; i < dic->nr_cpages; i++) {
 		struct page *page = dic->cpages[i];
 		block_t blkaddr;
+		struct bio_post_read_ctx *ctx;
 
 		blkaddr = data_blkaddr(dn.inode, dn.node_page,
 						dn.ofs_in_node + i + 1);
@@ -2225,6 +2227,11 @@ int f2fs_read_multi_pages(struct compress_ctx *cc, struct bio **bio_ret,
 		if (bio_add_page(bio, page, blocksize, 0) < blocksize)
 			goto submit_and_realloc;
 
+		/* tag STEP_DECOMPRESS to handle IO in wq */
+		ctx = bio->bi_private;
+		if (!(ctx->enabled_steps & (1 << STEP_DECOMPRESS)))
+			ctx->enabled_steps |= 1 << STEP_DECOMPRESS;
+
 		inc_page_count(sbi, F2FS_RD_DATA);
 		f2fs_update_iostat(sbi, FS_DATA_READ_IO, F2FS_BLKSIZE);
 		ClearPageError(page);

commit c1c633878662341eb0c502b3cab6e8c5cc83f44c
Author: Chao Yu <yuchao0@huawei.com>
Date:   Mon Mar 30 17:13:29 2020 +0800

    f2fs: introduce f2fs_bmap_compress()
    
    to support bmap() on compressed inode: if queried block locates in
    non-compressed cluster, return its physical block address.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index b79d643470d6..ff8a92df8d99 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -3666,6 +3666,37 @@ static int f2fs_set_data_page_dirty(struct page *page)
 	return 0;
 }
 
+
+static sector_t f2fs_bmap_compress(struct inode *inode, sector_t block)
+{
+#ifdef CONFIG_F2FS_FS_COMPRESSION
+	struct dnode_of_data dn;
+	sector_t start_idx, blknr = 0;
+	int ret;
+
+	start_idx = round_down(block, F2FS_I(inode)->i_cluster_size);
+
+	set_new_dnode(&dn, inode, NULL, NULL, 0);
+	ret = f2fs_get_dnode_of_data(&dn, start_idx, LOOKUP_NODE);
+	if (ret)
+		return 0;
+
+	if (dn.data_blkaddr != COMPRESS_ADDR) {
+		dn.ofs_in_node += block - start_idx;
+		blknr = f2fs_data_blkaddr(&dn);
+		if (!__is_valid_data_blkaddr(blknr))
+			blknr = 0;
+	}
+
+	f2fs_put_dnode(&dn);
+
+	return blknr;
+#else
+	return -EOPNOTSUPP;
+#endif
+}
+
+
 static sector_t f2fs_bmap(struct address_space *mapping, sector_t block)
 {
 	struct inode *inode = mapping->host;
@@ -3677,6 +3708,9 @@ static sector_t f2fs_bmap(struct address_space *mapping, sector_t block)
 	if (mapping_tagged(mapping, PAGECACHE_TAG_DIRTY))
 		filemap_write_and_wait(mapping);
 
+	if (f2fs_compressed_file(inode))
+		return f2fs_bmap_compress(inode, block);
+
 	return generic_block_bmap(mapping, block, get_data_block_bmap);
 }
 

commit bf38fbad12b365223f9c5d4057f741bb03372737
Author: Chao Yu <yuchao0@huawei.com>
Date:   Sat Mar 28 17:40:40 2020 +0800

    f2fs: support fiemap on compressed inode
    
    Map normal/compressed cluster of compressed inode correctly, and give
    the right fiemap flag FIEMAP_EXTENT_ENCODED on mapped compressed extent.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index a5505aba2a1f..b79d643470d6 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1832,6 +1832,25 @@ static int f2fs_xattr_fiemap(struct inode *inode,
 	return (err < 0 ? err : 0);
 }
 
+static loff_t max_inode_blocks(struct inode *inode)
+{
+	loff_t result = ADDRS_PER_INODE(inode);
+	loff_t leaf_count = ADDRS_PER_BLOCK(inode);
+
+	/* two direct node blocks */
+	result += (leaf_count * 2);
+
+	/* two indirect node blocks */
+	leaf_count *= NIDS_PER_BLOCK;
+	result += (leaf_count * 2);
+
+	/* one double indirect node block */
+	leaf_count *= NIDS_PER_BLOCK;
+	result += leaf_count;
+
+	return result;
+}
+
 int f2fs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,
 		u64 start, u64 len)
 {
@@ -1841,6 +1860,8 @@ int f2fs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,
 	u64 logical = 0, phys = 0, size = 0;
 	u32 flags = 0;
 	int ret = 0;
+	bool compr_cluster = false;
+	unsigned int cluster_size = F2FS_I(inode)->i_cluster_size;
 
 	if (fieinfo->fi_flags & FIEMAP_FLAG_CACHE) {
 		ret = f2fs_precache_extents(inode);
@@ -1875,6 +1896,9 @@ int f2fs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,
 	memset(&map_bh, 0, sizeof(struct buffer_head));
 	map_bh.b_size = len;
 
+	if (compr_cluster)
+		map_bh.b_size = blk_to_logical(inode, cluster_size - 1);
+
 	ret = get_data_block(inode, start_blk, &map_bh, 0,
 					F2FS_GET_BLOCK_FIEMAP, &next_pgofs);
 	if (ret)
@@ -1885,7 +1909,7 @@ int f2fs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,
 		start_blk = next_pgofs;
 
 		if (blk_to_logical(inode, start_blk) < blk_to_logical(inode,
-					F2FS_I_SB(inode)->max_file_blocks))
+						max_inode_blocks(inode)))
 			goto prep_next;
 
 		flags |= FIEMAP_EXTENT_LAST;
@@ -1897,11 +1921,38 @@ int f2fs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,
 
 		ret = fiemap_fill_next_extent(fieinfo, logical,
 				phys, size, flags);
+		if (ret)
+			goto out;
+		size = 0;
 	}
 
-	if (start_blk > last_blk || ret)
+	if (start_blk > last_blk)
 		goto out;
 
+	if (compr_cluster) {
+		compr_cluster = false;
+
+
+		logical = blk_to_logical(inode, start_blk - 1);
+		phys = blk_to_logical(inode, map_bh.b_blocknr);
+		size = blk_to_logical(inode, cluster_size);
+
+		flags |= FIEMAP_EXTENT_ENCODED;
+
+		start_blk += cluster_size - 1;
+
+		if (start_blk > last_blk)
+			goto out;
+
+		goto prep_next;
+	}
+
+	if (map_bh.b_blocknr == COMPRESS_ADDR) {
+		compr_cluster = true;
+		start_blk++;
+		goto prep_next;
+	}
+
 	logical = blk_to_logical(inode, start_blk);
 	phys = blk_to_logical(inode, map_bh.b_blocknr);
 	size = map_bh.b_size;

commit 435cbab95e3966cd8310addd9e9b758dce0e8b84
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Apr 9 10:25:21 2020 -0700

    f2fs: fix quota_sync failure due to f2fs_lock_op
    
    f2fs_quota_sync() uses f2fs_lock_op() before flushing dirty pages, but
    f2fs_write_data_page() returns EAGAIN.
    Likewise dentry blocks, we can just bypass getting the lock, since quota
    blocks are also maintained by checkpoint.
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 199877cb57fe..a5505aba2a1f 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -2659,8 +2659,8 @@ int f2fs_write_single_data_page(struct page *page, int *submitted,
 			f2fs_available_free_memory(sbi, BASE_CHECK))))
 		goto redirty_out;
 
-	/* Dentry blocks are controlled by checkpoint */
-	if (S_ISDIR(inode->i_mode)) {
+	/* Dentry/quota blocks are controlled by checkpoint */
+	if (S_ISDIR(inode->i_mode) || IS_NOQUOTA(inode)) {
 		fio.need_lock = LOCK_DONE;
 		err = f2fs_do_write_data_page(&fio);
 		goto done;

commit 8b83ac81f4283ae3bd05c9a7e15dca721014dd03
Author: Chao Yu <yuchao0@huawei.com>
Date:   Thu Apr 16 18:16:56 2020 +0800

    f2fs: support read iostat
    
    Adds to support accounting read IOs from userspace/kernel.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index accd28728642..199877cb57fe 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1033,6 +1033,7 @@ static int f2fs_submit_page_read(struct inode *inode, struct page *page,
 	}
 	ClearPageError(page);
 	inc_page_count(sbi, F2FS_RD_DATA);
+	f2fs_update_iostat(sbi, FS_DATA_READ_IO, F2FS_BLKSIZE);
 	__submit_bio(sbi, bio, DATA);
 	return 0;
 }
@@ -2038,6 +2039,7 @@ static int f2fs_read_single_page(struct inode *inode, struct page *page,
 		goto submit_and_realloc;
 
 	inc_page_count(F2FS_I_SB(inode), F2FS_RD_DATA);
+	f2fs_update_iostat(F2FS_I_SB(inode), FS_DATA_READ_IO, F2FS_BLKSIZE);
 	ClearPageError(page);
 	*last_block_in_bio = block_nr;
 	goto out;
@@ -2173,6 +2175,7 @@ int f2fs_read_multi_pages(struct compress_ctx *cc, struct bio **bio_ret,
 			goto submit_and_realloc;
 
 		inc_page_count(sbi, F2FS_RD_DATA);
+		f2fs_update_iostat(sbi, FS_DATA_READ_IO, F2FS_BLKSIZE);
 		ClearPageError(page);
 		*last_block_in_bio = blkaddr;
 	}
@@ -3526,6 +3529,9 @@ static ssize_t f2fs_direct_IO(struct kiocb *iocb, struct iov_iter *iter)
 		} else if (err < 0) {
 			f2fs_write_failed(mapping, offset + count);
 		}
+	} else {
+		if (err > 0)
+			f2fs_update_iostat(sbi, APP_DIRECT_READ_IO, err);
 	}
 
 out:

commit da9953b729c12ece6d35fd15d236457eee679228
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Apr 2 09:32:35 2020 -0700

    f2fs: introduce sysfs/data_io_flag to attach REQ_META/FUA
    
    This patch introduces a way to attach REQ_META/FUA explicitly
    to all the data writes given temperature.
    
    -> attach REQ_FUA to Hot Data writes
    
    -> attach REQ_FUA to Hot|Warm Data writes
    
    -> attach REQ_FUA to Hot|Warm|Cold Data writes
    
    -> attach REQ_FUA to Hot|Warm|Cold Data writes as well as
              REQ_META to Hot Data writes
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index cdf2f626bea7..accd28728642 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -513,6 +513,28 @@ void f2fs_submit_bio(struct f2fs_sb_info *sbi,
 	__submit_bio(sbi, bio, type);
 }
 
+static void __attach_data_io_flag(struct f2fs_io_info *fio)
+{
+	struct f2fs_sb_info *sbi = fio->sbi;
+	unsigned int temp_mask = (1 << NR_TEMP_TYPE) - 1;
+	unsigned int fua_flag = sbi->data_io_flag & temp_mask;
+	unsigned int meta_flag = (sbi->data_io_flag >> NR_TEMP_TYPE) &
+								temp_mask;
+	/*
+	 * data io flag bits per temp:
+	 *      REQ_META     |      REQ_FUA      |
+	 *    5 |    4 |   3 |    2 |    1 |   0 |
+	 * Cold | Warm | Hot | Cold | Warm | Hot |
+	 */
+	if (fio->type != DATA)
+		return;
+
+	if ((1 << fio->temp) & meta_flag)
+		fio->op_flags |= REQ_META;
+	if ((1 << fio->temp) & fua_flag)
+		fio->op_flags |= REQ_FUA;
+}
+
 static void __submit_merged_bio(struct f2fs_bio_info *io)
 {
 	struct f2fs_io_info *fio = &io->fio;
@@ -520,6 +542,7 @@ static void __submit_merged_bio(struct f2fs_bio_info *io)
 	if (!io->bio)
 		return;
 
+	__attach_data_io_flag(fio);
 	bio_set_op_attrs(io->bio, fio->op, fio->op_flags);
 
 	if (is_read_io(fio->op))

commit 7496affa3258e6d0e5fbf84bd031fd5a23f43289
Author: Chao Yu <yuchao0@huawei.com>
Date:   Mon Mar 30 18:03:15 2020 +0800

    f2fs: fix to use f2fs_readpage_limit() in f2fs_read_multi_pages()
    
    Multipage read flow should consider fsverity, so it needs to use
    f2fs_readpage_limit() instead of i_size_read() to check EOF condition.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 24643680489b..cdf2f626bea7 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -2048,7 +2048,8 @@ int f2fs_read_multi_pages(struct compress_ctx *cc, struct bio **bio_ret,
 
 	f2fs_bug_on(sbi, f2fs_cluster_is_empty(cc));
 
-	last_block_in_file = (i_size_read(inode) + blocksize - 1) >> blkbits;
+	last_block_in_file = (f2fs_readpage_limit(inode) +
+					blocksize - 1) >> blkbits;
 
 	/* get rid of pages beyond EOF */
 	for (i = 0; i < cc->cluster_size; i++) {

commit 74878565fbbf29d9be092504e87ec219e1359a43
Author: Chao Yu <yuchao0@huawei.com>
Date:   Tue Mar 24 14:20:57 2020 +0800

    f2fs: fix to avoid double unlock
    
    On image that has verity and compression feature, if compressed pages
    and non-compressed pages are mixed in one bio, we may double unlock
    non-compressed page in below flow:
    
    - f2fs_post_read_work
     - f2fs_decompress_work
      - f2fs_decompress_bio
       - __read_end_io
        - unlock_page
     - fsverity_enqueue_verify_work
      - f2fs_verity_work
       - f2fs_verify_bio
        - unlock_page
    
    So it should skip handling non-compressed page in f2fs_decompress_work()
    if verity is on.
    
    Besides, add missing dec_page_count() in f2fs_verify_bio().
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 0197b7b80d19..24643680489b 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -139,6 +139,8 @@ static void __read_end_io(struct bio *bio, bool compr, bool verity)
 			f2fs_decompress_pages(bio, page, verity);
 			continue;
 		}
+		if (verity)
+			continue;
 #endif
 
 		/* PG_error was set if any post_read step failed */
@@ -216,6 +218,7 @@ static void f2fs_verify_bio(struct bio *bio)
 		ClearPageUptodate(page);
 		ClearPageError(page);
 unlock:
+		dec_page_count(F2FS_P_SB(page), __read_io_type(page));
 		unlock_page(page);
 	}
 }

commit 79bbefb19f1359fb2cbd144d5a054649e7e583be
Author: Chao Yu <yuchao0@huawei.com>
Date:   Mon Mar 23 17:43:04 2020 +0800

    f2fs: fix NULL pointer dereference in f2fs_verity_work()
    
    If both compression and fsverity feature is on, generic/572 will
    report below NULL pointer dereference bug.
    
     BUG: kernel NULL pointer dereference, address: 0000000000000018
     RIP: 0010:f2fs_verity_work+0x60/0x90 [f2fs]
     #PF: supervisor read access in kernel mode
     Workqueue: fsverity_read_queue f2fs_verity_work [f2fs]
     RIP: 0010:f2fs_verity_work+0x60/0x90 [f2fs]
     Call Trace:
      process_one_work+0x16c/0x3f0
      worker_thread+0x4c/0x440
      ? rescuer_thread+0x350/0x350
      kthread+0xf8/0x130
      ? kthread_unpark+0x70/0x70
      ret_from_fork+0x35/0x40
    
    There are two issue in f2fs_verity_work():
    - it needs to traverse and verify all pages in bio.
    - if pages in bio belong to non-compressed cluster, accessing
    decompress IO context stored in page private will cause NULL
    pointer dereference.
    
    Fix them.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index bb66faf09eea..0197b7b80d19 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -187,12 +187,37 @@ static void f2fs_verify_pages(struct page **rpages, unsigned int cluster_size)
 
 static void f2fs_verify_bio(struct bio *bio)
 {
-	struct page *page = bio_first_page_all(bio);
-	struct decompress_io_ctx *dic =
-			(struct decompress_io_ctx *)page_private(page);
+	struct bio_vec *bv;
+	struct bvec_iter_all iter_all;
+
+	bio_for_each_segment_all(bv, bio, iter_all) {
+		struct page *page = bv->bv_page;
+		struct decompress_io_ctx *dic;
+
+		dic = (struct decompress_io_ctx *)page_private(page);
+
+		if (dic) {
+			if (refcount_dec_not_one(&dic->ref))
+				continue;
+			f2fs_verify_pages(dic->rpages,
+						dic->cluster_size);
+			f2fs_free_dic(dic);
+			continue;
+		}
+
+		if (bio->bi_status || PageError(page))
+			goto clear_uptodate;
 
-	f2fs_verify_pages(dic->rpages, dic->cluster_size);
-	f2fs_free_dic(dic);
+		if (fsverity_verify_page(page)) {
+			SetPageUptodate(page);
+			goto unlock;
+		}
+clear_uptodate:
+		ClearPageUptodate(page);
+		ClearPageError(page);
+unlock:
+		unlock_page(page);
+	}
 }
 #endif
 

commit b13f67ffe347cad69323a17dcc698e83c92ccb3d
Author: Chao Yu <yuchao0@huawei.com>
Date:   Thu Mar 19 19:57:57 2020 +0800

    f2fs: fix to avoid potential deadlock
    
    We should always check F2FS_I(inode)->cp_task condition in prior to other
    conditions in __should_serialize_io() to avoid deadloop described in
    commit 040d2bb318d1 ("f2fs: fix to avoid deadloop if data_flush is on"),
    however we break this rule when we support compression, fix it.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 326e6342c578..bb66faf09eea 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -2962,15 +2962,17 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 static inline bool __should_serialize_io(struct inode *inode,
 					struct writeback_control *wbc)
 {
+	/* to avoid deadlock in path of data flush */
+	if (F2FS_I(inode)->cp_task)
+		return false;
+
 	if (!S_ISREG(inode->i_mode))
 		return false;
-	if (f2fs_compressed_file(inode))
-		return true;
 	if (IS_NOQUOTA(inode))
 		return false;
-	/* to avoid deadlock in path of data flush */
-	if (F2FS_I(inode)->cp_task)
-		return false;
+
+	if (f2fs_compressed_file(inode))
+		return true;
 	if (wbc->sync_mode != WB_SYNC_ALL)
 		return true;
 	if (get_dirty_pages(inode) >= SM_I(F2FS_I_SB(inode))->min_seq_blocks)

commit ad8d6a02d685ecf046c369d72285a5e69adaf66e
Author: DongDongJu <commisori28@gmail.com>
Date:   Fri Mar 20 15:01:32 2020 +0900

    f2fs: delete DIO read lock
    
    This lock can be a contention with multi 4k random read IO with single inode.
    
    example) fio --output=test --name=test --numjobs=60 --filename=/media/samsung960pro/file_test --rw=randread --bs=4k
     --direct=1 --time_based --runtime=7 --ioengine=libaio --iodepth=256 --group_reporting --size=10G
    
    With this commit, it remove that possible lock contention.
    
    Signed-off-by: Dongjoo Seo <commisori28@gmail.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index dbde309349d0..326e6342c578 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -3453,7 +3453,8 @@ static ssize_t f2fs_direct_IO(struct kiocb *iocb, struct iov_iter *iter)
 	err = __blockdev_direct_IO(iocb, inode, inode->i_sb->s_bdev,
 			iter, rw == WRITE ? get_data_block_dio_write :
 			get_data_block_dio, NULL, f2fs_dio_submit_bio,
-			DIO_LOCKING | DIO_SKIP_HOLES);
+			rw == WRITE ? DIO_LOCKING | DIO_SKIP_HOLES :
+			DIO_SKIP_HOLES);
 
 	if (do_opu)
 		up_read(&fi->i_gc_rwsem[READ]);

commit ca9e968a5e637a105aa3b847a9fb489cb53abd8c
Author: Chao Yu <yuchao0@huawei.com>
Date:   Tue Feb 18 18:21:34 2020 +0800

    f2fs: avoid __GFP_NOFAIL in f2fs_bio_alloc
    
    __f2fs_bio_alloc() won't fail due to memory pool backend, remove unneeded
    __GFP_NOFAIL flag in __f2fs_bio_alloc().
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 9a9391a1790e..dbde309349d0 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -54,17 +54,13 @@ static inline struct bio *__f2fs_bio_alloc(gfp_t gfp_mask,
 	return bio_alloc_bioset(gfp_mask, nr_iovecs, &f2fs_bioset);
 }
 
-struct bio *f2fs_bio_alloc(struct f2fs_sb_info *sbi, int npages, bool no_fail)
+struct bio *f2fs_bio_alloc(struct f2fs_sb_info *sbi, int npages, bool noio)
 {
-	struct bio *bio;
-
-	if (no_fail) {
+	if (noio) {
 		/* No failure on bio allocation */
-		bio = __f2fs_bio_alloc(GFP_NOIO, npages);
-		if (!bio)
-			bio = __f2fs_bio_alloc(GFP_NOIO | __GFP_NOFAIL, npages);
-		return bio;
+		return __f2fs_bio_alloc(GFP_NOIO, npages);
 	}
+
 	if (time_to_inject(sbi, FAULT_ALLOC_BIO)) {
 		f2fs_show_injection_info(sbi, FAULT_ALLOC_BIO);
 		return NULL;

commit 0683728adab251d7747ce61279e249944effa78d
Author: Chao Yu <yuchao0@huawei.com>
Date:   Tue Feb 18 18:21:35 2020 +0800

    f2fs: fix to avoid triggering IO in write path
    
    If we are in write IO path, we need to avoid using GFP_KERNEL.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 68da6036cadd..9a9391a1790e 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -925,14 +925,15 @@ static inline bool f2fs_need_verity(const struct inode *inode, pgoff_t idx)
 
 static struct bio *f2fs_grab_read_bio(struct inode *inode, block_t blkaddr,
 				      unsigned nr_pages, unsigned op_flag,
-				      pgoff_t first_idx)
+				      pgoff_t first_idx, bool for_write)
 {
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
 	struct bio *bio;
 	struct bio_post_read_ctx *ctx;
 	unsigned int post_read_steps = 0;
 
-	bio = f2fs_bio_alloc(sbi, min_t(int, nr_pages, BIO_MAX_PAGES), false);
+	bio = f2fs_bio_alloc(sbi, min_t(int, nr_pages, BIO_MAX_PAGES),
+								for_write);
 	if (!bio)
 		return ERR_PTR(-ENOMEM);
 	f2fs_target_device(sbi, blkaddr, bio);
@@ -967,12 +968,12 @@ static void f2fs_release_read_bio(struct bio *bio)
 
 /* This can handle encryption stuffs */
 static int f2fs_submit_page_read(struct inode *inode, struct page *page,
-							block_t blkaddr)
+						block_t blkaddr, bool for_write)
 {
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
 	struct bio *bio;
 
-	bio = f2fs_grab_read_bio(inode, blkaddr, 1, 0, page->index);
+	bio = f2fs_grab_read_bio(inode, blkaddr, 1, 0, page->index, for_write);
 	if (IS_ERR(bio))
 		return PTR_ERR(bio);
 
@@ -1158,7 +1159,7 @@ struct page *f2fs_get_read_data_page(struct inode *inode, pgoff_t index,
 		return page;
 	}
 
-	err = f2fs_submit_page_read(inode, page, dn.data_blkaddr);
+	err = f2fs_submit_page_read(inode, page, dn.data_blkaddr, for_write);
 	if (err)
 		goto put_err;
 	return page;
@@ -1971,7 +1972,8 @@ static int f2fs_read_single_page(struct inode *inode, struct page *page,
 	}
 	if (bio == NULL) {
 		bio = f2fs_grab_read_bio(inode, block_nr, nr_pages,
-				is_readahead ? REQ_RAHEAD : 0, page->index);
+				is_readahead ? REQ_RAHEAD : 0, page->index,
+				false);
 		if (IS_ERR(bio)) {
 			ret = PTR_ERR(bio);
 			bio = NULL;
@@ -2006,7 +2008,7 @@ static int f2fs_read_single_page(struct inode *inode, struct page *page,
 #ifdef CONFIG_F2FS_FS_COMPRESSION
 int f2fs_read_multi_pages(struct compress_ctx *cc, struct bio **bio_ret,
 				unsigned nr_pages, sector_t *last_block_in_bio,
-				bool is_readahead)
+				bool is_readahead, bool for_write)
 {
 	struct dnode_of_data dn;
 	struct inode *inode = cc->inode;
@@ -2100,7 +2102,7 @@ int f2fs_read_multi_pages(struct compress_ctx *cc, struct bio **bio_ret,
 		if (!bio) {
 			bio = f2fs_grab_read_bio(inode, blkaddr, nr_pages,
 					is_readahead ? REQ_RAHEAD : 0,
-					page->index);
+					page->index, for_write);
 			if (IS_ERR(bio)) {
 				ret = PTR_ERR(bio);
 				bio = NULL;
@@ -2201,7 +2203,7 @@ int f2fs_mpage_readpages(struct address_space *mapping,
 				ret = f2fs_read_multi_pages(&cc, &bio,
 							max_nr_pages,
 							&last_block_in_bio,
-							is_readahead);
+							is_readahead, false);
 				f2fs_destroy_compress_ctx(&cc);
 				if (ret)
 					goto set_error_page;
@@ -2244,7 +2246,7 @@ int f2fs_mpage_readpages(struct address_space *mapping,
 				ret = f2fs_read_multi_pages(&cc, &bio,
 							max_nr_pages,
 							&last_block_in_bio,
-							is_readahead);
+							is_readahead, false);
 				f2fs_destroy_compress_ctx(&cc);
 			}
 		}
@@ -3274,7 +3276,7 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 			err = -EFSCORRUPTED;
 			goto fail;
 		}
-		err = f2fs_submit_page_read(inode, page, blkaddr);
+		err = f2fs_submit_page_read(inode, page, blkaddr, true);
 		if (err)
 			goto fail;
 

commit 985100035ef5a78889bd2295d1cd91000b133846
Author: Chao Yu <yuchao0@huawei.com>
Date:   Mon Feb 17 17:46:20 2020 +0800

    f2fs: add prefix for f2fs slab cache name
    
    In order to avoid polluting global slab cache namespace.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 43d705727aa5..68da6036cadd 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -3852,7 +3852,7 @@ void f2fs_destroy_post_read_wq(struct f2fs_sb_info *sbi)
 
 int __init f2fs_init_bio_entry_cache(void)
 {
-	bio_entry_slab = f2fs_kmem_cache_create("bio_entry_slab",
+	bio_entry_slab = f2fs_kmem_cache_create("f2fs_bio_entry_slab",
 			sizeof(struct bio_entry));
 	if (!bio_entry_slab)
 		return -ENOMEM;

commit 5df7731f60c2a933695a68d732f8b39fca788de6
Author: Chao Yu <yuchao0@huawei.com>
Date:   Mon Feb 17 17:45:44 2020 +0800

    f2fs: introduce DEFAULT_IO_TIMEOUT
    
    As Geert Uytterhoeven reported:
    
    for parameter HZ/50 in congestion_wait(BLK_RW_ASYNC, HZ/50);
    
    On some platforms, HZ can be less than 50, then unexpected 0 timeout
    jiffies will be set in congestion_wait().
    
    This patch introduces a macro DEFAULT_IO_TIMEOUT to wrap a determinate
    value with msecs_to_jiffies(20) to instead HZ/50 to avoid such issue.
    
    Quoted from Geert Uytterhoeven:
    
    "A timeout of HZ means 1 second.
    HZ/50 means 20 ms, but has the risk of being zero, if HZ < 50.
    
    If you want to use a timeout of 20 ms, you best use msecs_to_jiffies(20),
    as that takes care of the special cases, and never returns 0."
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 9fba196bd808..43d705727aa5 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -2317,7 +2317,7 @@ int f2fs_encrypt_one_page(struct f2fs_io_info *fio)
 		/* flush pending IOs and wait for a while in the ENOMEM case */
 		if (PTR_ERR(fio->encrypted_page) == -ENOMEM) {
 			f2fs_flush_merged_writes(fio->sbi);
-			congestion_wait(BLK_RW_ASYNC, HZ/50);
+			congestion_wait(BLK_RW_ASYNC, DEFAULT_IO_TIMEOUT);
 			gfp_flags |= __GFP_NOFAIL;
 			goto retry_encrypt;
 		}
@@ -2908,7 +2908,7 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 					if (wbc->sync_mode == WB_SYNC_ALL) {
 						cond_resched();
 						congestion_wait(BLK_RW_ASYNC,
-								HZ/50);
+							DEFAULT_IO_TIMEOUT);
 						goto retry_write;
 					}
 					goto next;

commit b0332a0f957ca818642cfafdb9515d4fd3b24663
Author: Chao Yu <yuchao0@huawei.com>
Date:   Fri Feb 14 17:44:12 2020 +0800

    f2fs: clean up lfs/adaptive mount option
    
    This patch removes F2FS_MOUNT_ADAPTIVE and F2FS_MOUNT_LFS mount options,
    and add F2FS_OPTION.fs_mode with below two status to indicate filesystem
    mode.
    
    enum {
            FS_MODE_ADAPTIVE,       /* use both lfs/ssr allocation */
            FS_MODE_LFS,            /* use lfs allocation only */
    };
    
    It can enhance code readability and fs mode's scalability.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index e589508ee155..9fba196bd808 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -442,7 +442,7 @@ static inline void __submit_bio(struct f2fs_sb_info *sbi,
 		if (type != DATA && type != NODE)
 			goto submit_io;
 
-		if (test_opt(sbi, LFS) && current->plug)
+		if (f2fs_lfs_mode(sbi) && current->plug)
 			blk_finish_plug(current->plug);
 
 		if (F2FS_IO_ALIGNED(sbi))
@@ -1413,7 +1413,7 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 	end = pgofs + maxblocks;
 
 	if (!create && f2fs_lookup_extent_cache(inode, pgofs, &ei)) {
-		if (test_opt(sbi, LFS) && flag == F2FS_GET_BLOCK_DIO &&
+		if (f2fs_lfs_mode(sbi) && flag == F2FS_GET_BLOCK_DIO &&
 							map->m_may_create)
 			goto next_dnode;
 
@@ -1468,7 +1468,7 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 
 	if (__is_valid_data_blkaddr(blkaddr)) {
 		/* use out-place-update for driect IO under LFS mode */
-		if (test_opt(sbi, LFS) && flag == F2FS_GET_BLOCK_DIO &&
+		if (f2fs_lfs_mode(sbi) && flag == F2FS_GET_BLOCK_DIO &&
 							map->m_may_create) {
 			err = __allocate_data_block(&dn, map->m_seg_type);
 			if (err)
@@ -2388,7 +2388,7 @@ bool f2fs_should_update_outplace(struct inode *inode, struct f2fs_io_info *fio)
 {
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
 
-	if (test_opt(sbi, LFS))
+	if (f2fs_lfs_mode(sbi))
 		return true;
 	if (S_ISDIR(inode->i_mode))
 		return true;

commit a2ced1ce1087a19361b7845c85a2d910fc591344
Author: Chao Yu <yuchao0@huawei.com>
Date:   Fri Feb 14 17:44:10 2020 +0800

    f2fs: clean up codes with {f2fs_,}data_blkaddr()
    
    - rename datablock_addr() to data_blkaddr().
    - wrap data_blkaddr() with f2fs_data_blkaddr() to clean up
    parameters.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index cd388e72390b..e589508ee155 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1044,8 +1044,7 @@ int f2fs_reserve_new_blocks(struct dnode_of_data *dn, blkcnt_t count)
 	f2fs_wait_on_page_writeback(dn->node_page, NODE, true, true);
 
 	for (; count > 0; dn->ofs_in_node++) {
-		block_t blkaddr = datablock_addr(dn->inode,
-					dn->node_page, dn->ofs_in_node);
+		block_t blkaddr = f2fs_data_blkaddr(dn);
 		if (blkaddr == NULL_ADDR) {
 			dn->data_blkaddr = NEW_ADDR;
 			__set_data_blkaddr(dn);
@@ -1297,8 +1296,7 @@ static int __allocate_data_block(struct dnode_of_data *dn, int seg_type)
 	if (err)
 		return err;
 
-	dn->data_blkaddr = datablock_addr(dn->inode,
-				dn->node_page, dn->ofs_in_node);
+	dn->data_blkaddr = f2fs_data_blkaddr(dn);
 	if (dn->data_blkaddr != NULL_ADDR)
 		goto alloc;
 
@@ -1460,7 +1458,7 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 	end_offset = ADDRS_PER_PAGE(dn.node_page, inode);
 
 next_block:
-	blkaddr = datablock_addr(dn.inode, dn.node_page, dn.ofs_in_node);
+	blkaddr = f2fs_data_blkaddr(&dn);
 
 	if (__is_valid_data_blkaddr(blkaddr) &&
 		!f2fs_is_valid_blkaddr(sbi, blkaddr, DATA_GENERIC_ENHANCE)) {
@@ -2060,7 +2058,7 @@ int f2fs_read_multi_pages(struct compress_ctx *cc, struct bio **bio_ret,
 	for (i = 1; i < cc->cluster_size; i++) {
 		block_t blkaddr;
 
-		blkaddr = datablock_addr(dn.inode, dn.node_page,
+		blkaddr = data_blkaddr(dn.inode, dn.node_page,
 						dn.ofs_in_node + i);
 
 		if (!__is_valid_data_blkaddr(blkaddr))
@@ -2089,7 +2087,7 @@ int f2fs_read_multi_pages(struct compress_ctx *cc, struct bio **bio_ret,
 		struct page *page = dic->cpages[i];
 		block_t blkaddr;
 
-		blkaddr = datablock_addr(dn.inode, dn.node_page,
+		blkaddr = data_blkaddr(dn.inode, dn.node_page,
 						dn.ofs_in_node + i + 1);
 
 		if (bio && !page_is_mergeable(sbi, bio,

commit 7a88ddb56077d07257a5d0393a4be13e424ca755
Author: Chao Yu <yuchao0@huawei.com>
Date:   Thu Feb 27 19:30:05 2020 +0800

    f2fs: fix inconsistent comments
    
    Lack of maintenance on comments may mislead developers, fix them.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index e5dc1ebb5305..cd388e72390b 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -364,9 +364,6 @@ static void f2fs_write_end_io(struct bio *bio)
 	bio_put(bio);
 }
 
-/*
- * Return true, if pre_bio's bdev is same as its target device.
- */
 struct block_device *f2fs_target_device(struct f2fs_sb_info *sbi,
 				block_t blk_addr, struct bio *bio)
 {
@@ -403,6 +400,9 @@ int f2fs_target_device_index(struct f2fs_sb_info *sbi, block_t blkaddr)
 	return 0;
 }
 
+/*
+ * Return true, if pre_bio's bdev is same as its target device.
+ */
 static bool __same_bdev(struct f2fs_sb_info *sbi,
 				block_t blk_addr, struct bio *bio)
 {
@@ -410,9 +410,6 @@ static bool __same_bdev(struct f2fs_sb_info *sbi,
 	return bio->bi_disk == b->bd_disk && bio->bi_partno == b->bd_partno;
 }
 
-/*
- * Low-level block read/write IO operations.
- */
 static struct bio *__bio_alloc(struct f2fs_io_info *fio, int npages)
 {
 	struct f2fs_sb_info *sbi = fio->sbi;
@@ -1388,13 +1385,9 @@ void __do_map_lock(struct f2fs_sb_info *sbi, int flag, bool lock)
 }
 
 /*
- * f2fs_map_blocks() now supported readahead/bmap/rw direct_IO with
- * f2fs_map_blocks structure.
- * If original data blocks are allocated, then give them to blockdev.
- * Otherwise,
- *     a. preallocate requested block addresses
- *     b. do not use extent cache for better performance
- *     c. give the block addresses to blockdev
+ * f2fs_map_blocks() tries to find or build mapping relationship which
+ * maps continuous logical blocks to physical blocks, and return such
+ * info via f2fs_map_blocks structure.
  */
 int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 						int create, int flag)

commit c10c98203222f4baef78a641bbb525a00eeebd2f
Author: Chao Yu <yuchao0@huawei.com>
Date:   Thu Feb 27 19:30:03 2020 +0800

    f2fs: cover last_disk_size update with spinlock
    
    This change solves below hangtask issue:
    
    INFO: task kworker/u16:1:58 blocked for more than 122 seconds.
          Not tainted 5.6.0-rc2-00590-g9983bdae4974e #11
    "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
    kworker/u16:1   D    0    58      2 0x00000000
    Workqueue: writeback wb_workfn (flush-179:0)
    Backtrace:
     (__schedule) from [<c0913234>] (schedule+0x78/0xf4)
     (schedule) from [<c017ec74>] (rwsem_down_write_slowpath+0x24c/0x4c0)
     (rwsem_down_write_slowpath) from [<c0915f2c>] (down_write+0x6c/0x70)
     (down_write) from [<c0435b80>] (f2fs_write_single_data_page+0x608/0x7ac)
     (f2fs_write_single_data_page) from [<c0435fd8>] (f2fs_write_cache_pages+0x2b4/0x7c4)
     (f2fs_write_cache_pages) from [<c043682c>] (f2fs_write_data_pages+0x344/0x35c)
     (f2fs_write_data_pages) from [<c0267ee8>] (do_writepages+0x3c/0xd4)
     (do_writepages) from [<c0310cbc>] (__writeback_single_inode+0x44/0x454)
     (__writeback_single_inode) from [<c03112d0>] (writeback_sb_inodes+0x204/0x4b0)
     (writeback_sb_inodes) from [<c03115cc>] (__writeback_inodes_wb+0x50/0xe4)
     (__writeback_inodes_wb) from [<c03118f4>] (wb_writeback+0x294/0x338)
     (wb_writeback) from [<c0312dac>] (wb_workfn+0x35c/0x54c)
     (wb_workfn) from [<c014f2b8>] (process_one_work+0x214/0x544)
     (process_one_work) from [<c014f634>] (worker_thread+0x4c/0x574)
     (worker_thread) from [<c01564fc>] (kthread+0x144/0x170)
     (kthread) from [<c01010e8>] (ret_from_fork+0x14/0x2c)
    
    Reported-and-tested-by: Ondřej Jirman <megi@xff.cz>
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index b27b72107911..e5dc1ebb5305 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -2647,10 +2647,10 @@ int f2fs_write_single_data_page(struct page *page, int *submitted,
 	if (err) {
 		file_set_keep_isize(inode);
 	} else {
-		down_write(&F2FS_I(inode)->i_sem);
+		spin_lock(&F2FS_I(inode)->i_size_lock);
 		if (F2FS_I(inode)->last_disk_size < psize)
 			F2FS_I(inode)->last_disk_size = psize;
-		up_write(&F2FS_I(inode)->i_sem);
+		spin_unlock(&F2FS_I(inode)->i_size_lock);
 	}
 
 done:

commit 236f45329460f76d058111de1a1cea12f5a8b734
Merge: 995933305e11 12efec560274
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Feb 8 13:04:49 2020 -0800

    Merge branch 'work.misc' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs
    
    Pull misc vfs updates from Al Viro:
    
     - bmap series from cmaiolino
    
     - getting rid of convolutions in copy_mount_options() (use a couple of
       copy_from_user() instead of the __get_user() crap)
    
    * 'work.misc' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs:
      saner copy_mount_options()
      fibmap: Reject negative block numbers
      fibmap: Use bmap instead of ->bmap method in ioctl_fibmap
      ecryptfs: drop direct calls to ->bmap
      cachefiles: drop direct usage of ->bmap method.
      fs: Enable bmap() function to properly return errors

commit 30460e1ea3e62f8457e087db9a309ed1031630da
Author: Carlos Maiolino <cmaiolino@redhat.com>
Date:   Thu Jan 9 14:30:41 2020 +0100

    fs: Enable bmap() function to properly return errors
    
    By now, bmap() will either return the physical block number related to
    the requested file offset or 0 in case of error or the requested offset
    maps into a hole.
    This patch makes the needed changes to enable bmap() to proper return
    errors, using the return value as an error return, and now, a pointer
    must be passed to bmap() to be filled with the mapped physical block.
    
    It will change the behavior of bmap() on return:
    
    - negative value in case of error
    - zero on success or map fell into a hole
    
    In case of a hole, the *block will be zero too
    
    Since this is a prep patch, by now, the only error return is -EINVAL if
    ->bmap doesn't exist.
    
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Carlos Maiolino <cmaiolino@redhat.com>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index a034cd0ce021..e948902c4ec5 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -3170,12 +3170,16 @@ static int check_swap_activate(struct file *swap_file, unsigned int max)
 	while ((probe_block + blocks_per_page) <= last_block && page_no < max) {
 		unsigned block_in_page;
 		sector_t first_block;
+		sector_t block = 0;
+		int	 err = 0;
 
 		cond_resched();
 
-		first_block = bmap(inode, probe_block);
-		if (first_block == 0)
+		block = probe_block;
+		err = bmap(inode, &block);
+		if (err || !block)
 			goto bad_bmap;
+		first_block = block;
 
 		/*
 		 * It must be PAGE_SIZE aligned on-disk
@@ -3187,11 +3191,13 @@ static int check_swap_activate(struct file *swap_file, unsigned int max)
 
 		for (block_in_page = 1; block_in_page < blocks_per_page;
 					block_in_page++) {
-			sector_t block;
 
-			block = bmap(inode, probe_block + block_in_page);
-			if (block == 0)
+			block = probe_block + block_in_page;
+			err = bmap(inode, &block);
+
+			if (err || !block)
 				goto bad_bmap;
+
 			if (block != first_block + block_in_page) {
 				/* Discontiguity */
 				probe_block++;

commit 644c8c92adb669bdb2d4b2a271dfa9569ae07ee8
Author: Eric Biggers <ebiggers@google.com>
Date:   Tue Dec 31 12:14:16 2019 -0600

    f2fs: fix deadlock allocating bio_post_read_ctx from mempool
    
    Without any form of coordination, any case where multiple allocations
    from the same mempool are needed at a time to make forward progress can
    deadlock under memory pressure.
    
    This is the case for struct bio_post_read_ctx, as one can be allocated
    to decrypt a Merkle tree page during fsverity_verify_bio(), which itself
    is running from a post-read callback for a data bio which has its own
    struct bio_post_read_ctx.
    
    Fix this by freeing first bio_post_read_ctx before calling
    fsverity_verify_bio().  This works because verity (if enabled) is always
    the last post-read step.
    
    This deadlock can be reproduced by trying to read from an encrypted
    verity file after reducing NUM_PREALLOC_POST_READ_CTXS to 1 and patching
    mempool_alloc() to pretend that pool->alloc() always fails.
    
    Note that since NUM_PREALLOC_POST_READ_CTXS is actually 128, to actually
    hit this bug in practice would require reading from lots of encrypted
    verity files at the same time.  But it's theoretically possible, as N
    available objects doesn't guarantee forward progress when > N/2 threads
    each need 2 objects at a time.
    
    Fixes: 95ae251fe828 ("f2fs: add fs-verity support")
    Signed-off-by: Eric Biggers <ebiggers@google.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 63e0b814b567..8bd9afa81c54 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -204,19 +204,32 @@ static void f2fs_verity_work(struct work_struct *work)
 {
 	struct bio_post_read_ctx *ctx =
 		container_of(work, struct bio_post_read_ctx, work);
+	struct bio *bio = ctx->bio;
+#ifdef CONFIG_F2FS_FS_COMPRESSION
+	unsigned int enabled_steps = ctx->enabled_steps;
+#endif
+
+	/*
+	 * fsverity_verify_bio() may call readpages() again, and while verity
+	 * will be disabled for this, decryption may still be needed, resulting
+	 * in another bio_post_read_ctx being allocated.  So to prevent
+	 * deadlocks we need to release the current ctx to the mempool first.
+	 * This assumes that verity is the last post-read step.
+	 */
+	mempool_free(ctx, bio_post_read_ctx_pool);
+	bio->bi_private = NULL;
 
 #ifdef CONFIG_F2FS_FS_COMPRESSION
 	/* previous step is decompression */
-	if (ctx->enabled_steps & (1 << STEP_DECOMPRESS)) {
-
-		f2fs_verify_bio(ctx->bio);
-		f2fs_release_read_bio(ctx->bio);
+	if (enabled_steps & (1 << STEP_DECOMPRESS)) {
+		f2fs_verify_bio(bio);
+		f2fs_release_read_bio(bio);
 		return;
 	}
 #endif
 
-	fsverity_verify_bio(ctx->bio);
-	__f2fs_read_end_io(ctx->bio, false, false);
+	fsverity_verify_bio(bio);
+	__f2fs_read_end_io(bio, false, false);
 }
 
 static void f2fs_post_read_work(struct work_struct *work)

commit e8ce5749d781ec0ccdf03f4f174cc8f709c0057a
Author: Eric Biggers <ebiggers@google.com>
Date:   Tue Dec 31 12:14:56 2019 -0600

    f2fs: remove unneeded check for error allocating bio_post_read_ctx
    
    Since allocating an object from a mempool never fails when
    __GFP_DIRECT_RECLAIM (which is included in GFP_NOFS) is set, the check
    for failure to allocate a bio_post_read_ctx is unnecessary.  Remove it.
    
    Signed-off-by: Eric Biggers <ebiggers@google.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index b56437d4973d..63e0b814b567 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -937,11 +937,8 @@ static struct bio *f2fs_grab_read_bio(struct inode *inode, block_t blkaddr,
 		post_read_steps |= 1 << STEP_VERITY;
 
 	if (post_read_steps) {
+		/* Due to the mempool, this never fails. */
 		ctx = mempool_alloc(bio_post_read_ctx_pool, GFP_NOFS);
-		if (!ctx) {
-			bio_put(bio);
-			return ERR_PTR(-ENOMEM);
-		}
 		ctx->bio = bio;
 		ctx->sbi = sbi;
 		ctx->enabled_steps = post_read_steps;

commit 3e5e479a39ce9ed60cd63f7565cc1d9da77c2a4e
Author: Chao Yu <yuchao0@huawei.com>
Date:   Fri Dec 27 18:44:56 2019 +0800

    f2fs: fix to add swap extent correctly
    
    As Youling reported in mailing list:
    
    https://www.linuxquestions.org/questions/linux-newbie-8/the-file-system-f2fs-is-broken-4175666043/
    
    https://www.linux.org/threads/the-file-system-f2fs-is-broken.26490/
    
    There is a test case can corrupt f2fs image:
    - dd if=/dev/zero of=/swapfile bs=1M count=4096
    - chmod 600 /swapfile
    - mkswap /swapfile
    - swapon --discard /swapfile
    
    The root cause is f2fs_swap_activate() intends to return zero value
    to setup_swap_extents() to enable SWP_FS mode (swap file goes through
    fs), in this flow, setup_swap_extents() setups swap extent with wrong
    block address range, result in discard_swap() erasing incorrect address.
    
    Because f2fs_swap_activate() has pinned swapfile, its data block
    address will not change, it's safe to let swap to handle IO through
    raw device, so we can get rid of SWAP_FS mode and initial swap extents
    inside f2fs_swap_activate(), by this way, later discard_swap() can trim
    in right address range.
    
    Fixes: 4969c06a0d83 ("f2fs: support swap file w/ DIO")
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 98c946dfee13..b56437d4973d 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -3627,7 +3627,8 @@ int f2fs_migrate_page(struct address_space *mapping,
 
 #ifdef CONFIG_SWAP
 /* Copied from generic_swapfile_activate() to check any holes */
-static int check_swap_activate(struct file *swap_file, unsigned int max)
+static int check_swap_activate(struct swap_info_struct *sis,
+				struct file *swap_file, sector_t *span)
 {
 	struct address_space *mapping = swap_file->f_mapping;
 	struct inode *inode = mapping->host;
@@ -3638,6 +3639,8 @@ static int check_swap_activate(struct file *swap_file, unsigned int max)
 	sector_t last_block;
 	sector_t lowest_block = -1;
 	sector_t highest_block = 0;
+	int nr_extents = 0;
+	int ret;
 
 	blkbits = inode->i_blkbits;
 	blocks_per_page = PAGE_SIZE >> blkbits;
@@ -3649,7 +3652,8 @@ static int check_swap_activate(struct file *swap_file, unsigned int max)
 	probe_block = 0;
 	page_no = 0;
 	last_block = i_size_read(inode) >> blkbits;
-	while ((probe_block + blocks_per_page) <= last_block && page_no < max) {
+	while ((probe_block + blocks_per_page) <= last_block &&
+			page_no < sis->max) {
 		unsigned block_in_page;
 		sector_t first_block;
 
@@ -3689,13 +3693,27 @@ static int check_swap_activate(struct file *swap_file, unsigned int max)
 				highest_block = first_block;
 		}
 
+		/*
+		 * We found a PAGE_SIZE-length, PAGE_SIZE-aligned run of blocks
+		 */
+		ret = add_swap_extent(sis, page_no, 1, first_block);
+		if (ret < 0)
+			goto out;
+		nr_extents += ret;
 		page_no++;
 		probe_block += blocks_per_page;
 reprobe:
 		continue;
 	}
-	return 0;
-
+	ret = nr_extents;
+	*span = 1 + highest_block - lowest_block;
+	if (page_no == 0)
+		page_no = 1;	/* force Empty message */
+	sis->max = page_no;
+	sis->pages = page_no - 1;
+	sis->highest_bit = page_no - 1;
+out:
+	return ret;
 bad_bmap:
 	pr_err("swapon: swapfile has holes\n");
 	return -EINVAL;
@@ -3720,14 +3738,14 @@ static int f2fs_swap_activate(struct swap_info_struct *sis, struct file *file,
 	if (f2fs_disable_compressed_file(inode))
 		return -EINVAL;
 
-	ret = check_swap_activate(file, sis->max);
-	if (ret)
+	ret = check_swap_activate(sis, file, span);
+	if (ret < 0)
 		return ret;
 
 	set_inode_flag(inode, FI_PIN_FILE);
 	f2fs_precache_extents(inode);
 	f2fs_update_time(F2FS_I_SB(inode), REQ_TIME);
-	return 0;
+	return ret;
 }
 
 static void f2fs_swap_deactivate(struct file *file)

commit 4c8ff7095bef64fc47e996a938f7d57f9e077da3
Author: Chao Yu <yuchao0@huawei.com>
Date:   Fri Nov 1 18:07:14 2019 +0800

    f2fs: support data compression
    
    This patch tries to support compression in f2fs.
    
    - New term named cluster is defined as basic unit of compression, file can
    be divided into multiple clusters logically. One cluster includes 4 << n
    (n >= 0) logical pages, compression size is also cluster size, each of
    cluster can be compressed or not.
    
    - In cluster metadata layout, one special flag is used to indicate cluster
    is compressed one or normal one, for compressed cluster, following metadata
    maps cluster to [1, 4 << n - 1] physical blocks, in where f2fs stores
    data including compress header and compressed data.
    
    - In order to eliminate write amplification during overwrite, F2FS only
    support compression on write-once file, data can be compressed only when
    all logical blocks in file are valid and cluster compress ratio is lower
    than specified threshold.
    
    - To enable compression on regular inode, there are three ways:
    * chattr +c file
    * chattr +c dir; touch dir/file
    * mount w/ -o compress_extension=ext; touch file.ext
    
    Compress metadata layout:
                                 [Dnode Structure]
                 +-----------------------------------------------+
                 | cluster 1 | cluster 2 | ......... | cluster N |
                 +-----------------------------------------------+
                 .           .                       .           .
           .                       .                .                      .
      .         Compressed Cluster       .        .        Normal Cluster            .
    +----------+---------+---------+---------+  +---------+---------+---------+---------+
    |compr flag| block 1 | block 2 | block 3 |  | block 1 | block 2 | block 3 | block 4 |
    +----------+---------+---------+---------+  +---------+---------+---------+---------+
               .                             .
             .                                           .
           .                                                           .
          +-------------+-------------+----------+----------------------------+
          | data length | data chksum | reserved |      compressed data       |
          +-------------+-------------+----------+----------------------------+
    
    Changelog:
    
    20190326:
    - fix error handling of read_end_io().
    - remove unneeded comments in f2fs_encrypt_one_page().
    
    20190327:
    - fix wrong use of f2fs_cluster_is_full() in f2fs_mpage_readpages().
    - don't jump into loop directly to avoid uninitialized variables.
    - add TODO tag in error path of f2fs_write_cache_pages().
    
    20190328:
    - fix wrong merge condition in f2fs_read_multi_pages().
    - check compressed file in f2fs_post_read_required().
    
    20190401
    - allow overwrite on non-compressed cluster.
    - check cluster meta before writing compressed data.
    
    20190402
    - don't preallocate blocks for compressed file.
    
    - add lz4 compress algorithm
    - process multiple post read works in one workqueue
      Now f2fs supports processing post read work in multiple workqueue,
      it shows low performance due to schedule overhead of multiple
      workqueue executing orderly.
    
    20190921
    - compress: support buffered overwrite
    C: compress cluster flag
    V: valid block address
    N: NEW_ADDR
    
    One cluster contain 4 blocks
    
     before overwrite   after overwrite
    
    - VVVV          ->      CVNN
    - CVNN          ->      VVVV
    
    - CVNN          ->      CVNN
    - CVNN          ->      CVVV
    
    - CVVV          ->      CVNN
    - CVVV          ->      CVVV
    
    20191029
    - add kconfig F2FS_FS_COMPRESSION to isolate compression related
    codes, add kconfig F2FS_FS_{LZO,LZ4} to cover backend algorithm.
    note that: will remove lzo backend if Jaegeuk agreed that too.
    - update codes according to Eric's comments.
    
    20191101
    - apply fixes from Jaegeuk
    
    20191113
    - apply fixes from Jaegeuk
    - split workqueue for fsverity
    
    20191216
    - apply fixes from Jaegeuk
    
    20200117
    - fix to avoid NULL pointer dereference
    
    [Jaegeuk Kim]
    - add tracepoint for f2fs_{,de}compress_pages()
    - fix many bugs and add some compression stats
    - fix overwrite/mmap bugs
    - address 32bit build error, reported by Geert.
    - bug fixes when handling errors and i_compressed_blocks
    
    Reported-by: <noreply@ellerman.id.au>
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index f89aeaaea90e..98c946dfee13 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -82,6 +82,9 @@ static bool __is_cp_guaranteed(struct page *page)
 	if (!mapping)
 		return false;
 
+	if (f2fs_is_compressed_page(page))
+		return false;
+
 	inode = mapping->host;
 	sbi = F2FS_I_SB(inode);
 
@@ -114,19 +117,19 @@ static enum count_type __read_io_type(struct page *page)
 
 /* postprocessing steps for read bios */
 enum bio_post_read_step {
-	STEP_INITIAL = 0,
 	STEP_DECRYPT,
+	STEP_DECOMPRESS,
 	STEP_VERITY,
 };
 
 struct bio_post_read_ctx {
 	struct bio *bio;
+	struct f2fs_sb_info *sbi;
 	struct work_struct work;
-	unsigned int cur_step;
 	unsigned int enabled_steps;
 };
 
-static void __read_end_io(struct bio *bio)
+static void __read_end_io(struct bio *bio, bool compr, bool verity)
 {
 	struct page *page;
 	struct bio_vec *bv;
@@ -135,6 +138,13 @@ static void __read_end_io(struct bio *bio)
 	bio_for_each_segment_all(bv, bio, iter_all) {
 		page = bv->bv_page;
 
+#ifdef CONFIG_F2FS_FS_COMPRESSION
+		if (compr && f2fs_is_compressed_page(page)) {
+			f2fs_decompress_pages(bio, page, verity);
+			continue;
+		}
+#endif
+
 		/* PG_error was set if any post_read step failed */
 		if (bio->bi_status || PageError(page)) {
 			ClearPageUptodate(page);
@@ -146,31 +156,94 @@ static void __read_end_io(struct bio *bio)
 		dec_page_count(F2FS_P_SB(page), __read_io_type(page));
 		unlock_page(page);
 	}
-	if (bio->bi_private)
-		mempool_free(bio->bi_private, bio_post_read_ctx_pool);
-	bio_put(bio);
+}
+
+static void f2fs_release_read_bio(struct bio *bio);
+static void __f2fs_read_end_io(struct bio *bio, bool compr, bool verity)
+{
+	if (!compr)
+		__read_end_io(bio, false, verity);
+	f2fs_release_read_bio(bio);
+}
+
+static void f2fs_decompress_bio(struct bio *bio, bool verity)
+{
+	__read_end_io(bio, true, verity);
 }
 
 static void bio_post_read_processing(struct bio_post_read_ctx *ctx);
 
-static void decrypt_work(struct work_struct *work)
+static void f2fs_decrypt_work(struct bio_post_read_ctx *ctx)
+{
+	fscrypt_decrypt_bio(ctx->bio);
+}
+
+static void f2fs_decompress_work(struct bio_post_read_ctx *ctx)
+{
+	f2fs_decompress_bio(ctx->bio, ctx->enabled_steps & (1 << STEP_VERITY));
+}
+
+#ifdef CONFIG_F2FS_FS_COMPRESSION
+static void f2fs_verify_pages(struct page **rpages, unsigned int cluster_size)
+{
+	f2fs_decompress_end_io(rpages, cluster_size, false, true);
+}
+
+static void f2fs_verify_bio(struct bio *bio)
+{
+	struct page *page = bio_first_page_all(bio);
+	struct decompress_io_ctx *dic =
+			(struct decompress_io_ctx *)page_private(page);
+
+	f2fs_verify_pages(dic->rpages, dic->cluster_size);
+	f2fs_free_dic(dic);
+}
+#endif
+
+static void f2fs_verity_work(struct work_struct *work)
 {
 	struct bio_post_read_ctx *ctx =
 		container_of(work, struct bio_post_read_ctx, work);
 
-	fscrypt_decrypt_bio(ctx->bio);
+#ifdef CONFIG_F2FS_FS_COMPRESSION
+	/* previous step is decompression */
+	if (ctx->enabled_steps & (1 << STEP_DECOMPRESS)) {
+
+		f2fs_verify_bio(ctx->bio);
+		f2fs_release_read_bio(ctx->bio);
+		return;
+	}
+#endif
 
-	bio_post_read_processing(ctx);
+	fsverity_verify_bio(ctx->bio);
+	__f2fs_read_end_io(ctx->bio, false, false);
 }
 
-static void verity_work(struct work_struct *work)
+static void f2fs_post_read_work(struct work_struct *work)
 {
 	struct bio_post_read_ctx *ctx =
 		container_of(work, struct bio_post_read_ctx, work);
 
-	fsverity_verify_bio(ctx->bio);
+	if (ctx->enabled_steps & (1 << STEP_DECRYPT))
+		f2fs_decrypt_work(ctx);
+
+	if (ctx->enabled_steps & (1 << STEP_DECOMPRESS))
+		f2fs_decompress_work(ctx);
+
+	if (ctx->enabled_steps & (1 << STEP_VERITY)) {
+		INIT_WORK(&ctx->work, f2fs_verity_work);
+		fsverity_enqueue_verify_work(&ctx->work);
+		return;
+	}
+
+	__f2fs_read_end_io(ctx->bio,
+		ctx->enabled_steps & (1 << STEP_DECOMPRESS), false);
+}
 
-	bio_post_read_processing(ctx);
+static void f2fs_enqueue_post_read_work(struct f2fs_sb_info *sbi,
+						struct work_struct *work)
+{
+	queue_work(sbi->post_read_wq, work);
 }
 
 static void bio_post_read_processing(struct bio_post_read_ctx *ctx)
@@ -180,31 +253,26 @@ static void bio_post_read_processing(struct bio_post_read_ctx *ctx)
 	 * verity may require reading metadata pages that need decryption, and
 	 * we shouldn't recurse to the same workqueue.
 	 */
-	switch (++ctx->cur_step) {
-	case STEP_DECRYPT:
-		if (ctx->enabled_steps & (1 << STEP_DECRYPT)) {
-			INIT_WORK(&ctx->work, decrypt_work);
-			fscrypt_enqueue_decrypt_work(&ctx->work);
-			return;
-		}
-		ctx->cur_step++;
-		/* fall-through */
-	case STEP_VERITY:
-		if (ctx->enabled_steps & (1 << STEP_VERITY)) {
-			INIT_WORK(&ctx->work, verity_work);
-			fsverity_enqueue_verify_work(&ctx->work);
-			return;
-		}
-		ctx->cur_step++;
-		/* fall-through */
-	default:
-		__read_end_io(ctx->bio);
+
+	if (ctx->enabled_steps & (1 << STEP_DECRYPT) ||
+		ctx->enabled_steps & (1 << STEP_DECOMPRESS)) {
+		INIT_WORK(&ctx->work, f2fs_post_read_work);
+		f2fs_enqueue_post_read_work(ctx->sbi, &ctx->work);
+		return;
+	}
+
+	if (ctx->enabled_steps & (1 << STEP_VERITY)) {
+		INIT_WORK(&ctx->work, f2fs_verity_work);
+		fsverity_enqueue_verify_work(&ctx->work);
+		return;
 	}
+
+	__f2fs_read_end_io(ctx->bio, false, false);
 }
 
 static bool f2fs_bio_post_read_required(struct bio *bio)
 {
-	return bio->bi_private && !bio->bi_status;
+	return bio->bi_private;
 }
 
 static void f2fs_read_end_io(struct bio *bio)
@@ -219,12 +287,11 @@ static void f2fs_read_end_io(struct bio *bio)
 	if (f2fs_bio_post_read_required(bio)) {
 		struct bio_post_read_ctx *ctx = bio->bi_private;
 
-		ctx->cur_step = STEP_INITIAL;
 		bio_post_read_processing(ctx);
 		return;
 	}
 
-	__read_end_io(bio);
+	__f2fs_read_end_io(bio, false, false);
 }
 
 static void f2fs_write_end_io(struct bio *bio)
@@ -255,6 +322,13 @@ static void f2fs_write_end_io(struct bio *bio)
 
 		fscrypt_finalize_bounce_page(&page);
 
+#ifdef CONFIG_F2FS_FS_COMPRESSION
+		if (f2fs_is_compressed_page(page)) {
+			f2fs_compress_write_end_io(bio, page);
+			continue;
+		}
+#endif
+
 		if (unlikely(bio->bi_status)) {
 			mapping_set_error(page->mapping, -EIO);
 			if (type == F2FS_WB_CP_DATA)
@@ -399,6 +473,12 @@ static inline void __submit_bio(struct f2fs_sb_info *sbi,
 	submit_bio(bio);
 }
 
+void f2fs_submit_bio(struct f2fs_sb_info *sbi,
+				struct bio *bio, enum page_type type)
+{
+	__submit_bio(sbi, bio, type);
+}
+
 static void __submit_merged_bio(struct f2fs_bio_info *io)
 {
 	struct f2fs_io_info *fio = &io->fio;
@@ -421,7 +501,6 @@ static bool __has_merged_page(struct bio *bio, struct inode *inode,
 						struct page *page, nid_t ino)
 {
 	struct bio_vec *bvec;
-	struct page *target;
 	struct bvec_iter_all iter_all;
 
 	if (!bio)
@@ -431,10 +510,18 @@ static bool __has_merged_page(struct bio *bio, struct inode *inode,
 		return true;
 
 	bio_for_each_segment_all(bvec, bio, iter_all) {
+		struct page *target = bvec->bv_page;
 
-		target = bvec->bv_page;
-		if (fscrypt_is_bounce_page(target))
+		if (fscrypt_is_bounce_page(target)) {
 			target = fscrypt_pagecache_page(target);
+			if (IS_ERR(target))
+				continue;
+		}
+		if (f2fs_is_compressed_page(target)) {
+			target = f2fs_compress_control_page(target);
+			if (IS_ERR(target))
+				continue;
+		}
 
 		if (inode && inode == target->mapping->host)
 			return true;
@@ -629,7 +716,8 @@ static int add_ipu_page(struct f2fs_sb_info *sbi, struct bio **bio,
 
 			found = true;
 
-			if (bio_add_page(*bio, page, PAGE_SIZE, 0) == PAGE_SIZE) {
+			if (bio_add_page(*bio, page, PAGE_SIZE, 0) ==
+							PAGE_SIZE) {
 				ret = 0;
 				break;
 			}
@@ -769,7 +857,12 @@ void f2fs_submit_page_write(struct f2fs_io_info *fio)
 
 	verify_fio_blkaddr(fio);
 
-	bio_page = fio->encrypted_page ? fio->encrypted_page : fio->page;
+	if (fio->encrypted_page)
+		bio_page = fio->encrypted_page;
+	else if (fio->compressed_page)
+		bio_page = fio->compressed_page;
+	else
+		bio_page = fio->page;
 
 	/* set submitted = true as a return value */
 	fio->submitted = true;
@@ -838,7 +931,8 @@ static struct bio *f2fs_grab_read_bio(struct inode *inode, block_t blkaddr,
 
 	if (f2fs_encrypted_file(inode))
 		post_read_steps |= 1 << STEP_DECRYPT;
-
+	if (f2fs_compressed_file(inode))
+		post_read_steps |= 1 << STEP_DECOMPRESS;
 	if (f2fs_need_verity(inode, first_idx))
 		post_read_steps |= 1 << STEP_VERITY;
 
@@ -849,6 +943,7 @@ static struct bio *f2fs_grab_read_bio(struct inode *inode, block_t blkaddr,
 			return ERR_PTR(-ENOMEM);
 		}
 		ctx->bio = bio;
+		ctx->sbi = sbi;
 		ctx->enabled_steps = post_read_steps;
 		bio->bi_private = ctx;
 	}
@@ -856,6 +951,13 @@ static struct bio *f2fs_grab_read_bio(struct inode *inode, block_t blkaddr,
 	return bio;
 }
 
+static void f2fs_release_read_bio(struct bio *bio)
+{
+	if (bio->bi_private)
+		mempool_free(bio->bi_private, bio_post_read_ctx_pool);
+	bio_put(bio);
+}
+
 /* This can handle encryption stuffs */
 static int f2fs_submit_page_read(struct inode *inode, struct page *page,
 							block_t blkaddr)
@@ -1900,6 +2002,144 @@ static int f2fs_read_single_page(struct inode *inode, struct page *page,
 	return ret;
 }
 
+#ifdef CONFIG_F2FS_FS_COMPRESSION
+int f2fs_read_multi_pages(struct compress_ctx *cc, struct bio **bio_ret,
+				unsigned nr_pages, sector_t *last_block_in_bio,
+				bool is_readahead)
+{
+	struct dnode_of_data dn;
+	struct inode *inode = cc->inode;
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct bio *bio = *bio_ret;
+	unsigned int start_idx = cc->cluster_idx << cc->log_cluster_size;
+	sector_t last_block_in_file;
+	const unsigned blkbits = inode->i_blkbits;
+	const unsigned blocksize = 1 << blkbits;
+	struct decompress_io_ctx *dic = NULL;
+	int i;
+	int ret = 0;
+
+	f2fs_bug_on(sbi, f2fs_cluster_is_empty(cc));
+
+	last_block_in_file = (i_size_read(inode) + blocksize - 1) >> blkbits;
+
+	/* get rid of pages beyond EOF */
+	for (i = 0; i < cc->cluster_size; i++) {
+		struct page *page = cc->rpages[i];
+
+		if (!page)
+			continue;
+		if ((sector_t)page->index >= last_block_in_file) {
+			zero_user_segment(page, 0, PAGE_SIZE);
+			if (!PageUptodate(page))
+				SetPageUptodate(page);
+		} else if (!PageUptodate(page)) {
+			continue;
+		}
+		unlock_page(page);
+		cc->rpages[i] = NULL;
+		cc->nr_rpages--;
+	}
+
+	/* we are done since all pages are beyond EOF */
+	if (f2fs_cluster_is_empty(cc))
+		goto out;
+
+	set_new_dnode(&dn, inode, NULL, NULL, 0);
+	ret = f2fs_get_dnode_of_data(&dn, start_idx, LOOKUP_NODE);
+	if (ret)
+		goto out;
+
+	/* cluster was overwritten as normal cluster */
+	if (dn.data_blkaddr != COMPRESS_ADDR)
+		goto out;
+
+	for (i = 1; i < cc->cluster_size; i++) {
+		block_t blkaddr;
+
+		blkaddr = datablock_addr(dn.inode, dn.node_page,
+						dn.ofs_in_node + i);
+
+		if (!__is_valid_data_blkaddr(blkaddr))
+			break;
+
+		if (!f2fs_is_valid_blkaddr(sbi, blkaddr, DATA_GENERIC)) {
+			ret = -EFAULT;
+			goto out_put_dnode;
+		}
+		cc->nr_cpages++;
+	}
+
+	/* nothing to decompress */
+	if (cc->nr_cpages == 0) {
+		ret = 0;
+		goto out_put_dnode;
+	}
+
+	dic = f2fs_alloc_dic(cc);
+	if (IS_ERR(dic)) {
+		ret = PTR_ERR(dic);
+		goto out_put_dnode;
+	}
+
+	for (i = 0; i < dic->nr_cpages; i++) {
+		struct page *page = dic->cpages[i];
+		block_t blkaddr;
+
+		blkaddr = datablock_addr(dn.inode, dn.node_page,
+						dn.ofs_in_node + i + 1);
+
+		if (bio && !page_is_mergeable(sbi, bio,
+					*last_block_in_bio, blkaddr)) {
+submit_and_realloc:
+			__submit_bio(sbi, bio, DATA);
+			bio = NULL;
+		}
+
+		if (!bio) {
+			bio = f2fs_grab_read_bio(inode, blkaddr, nr_pages,
+					is_readahead ? REQ_RAHEAD : 0,
+					page->index);
+			if (IS_ERR(bio)) {
+				ret = PTR_ERR(bio);
+				bio = NULL;
+				dic->failed = true;
+				if (refcount_sub_and_test(dic->nr_cpages - i,
+							&dic->ref))
+					f2fs_decompress_end_io(dic->rpages,
+							cc->cluster_size, true,
+							false);
+				f2fs_free_dic(dic);
+				f2fs_put_dnode(&dn);
+				*bio_ret = bio;
+				return ret;
+			}
+		}
+
+		f2fs_wait_on_block_writeback(inode, blkaddr);
+
+		if (bio_add_page(bio, page, blocksize, 0) < blocksize)
+			goto submit_and_realloc;
+
+		inc_page_count(sbi, F2FS_RD_DATA);
+		ClearPageError(page);
+		*last_block_in_bio = blkaddr;
+	}
+
+	f2fs_put_dnode(&dn);
+
+	*bio_ret = bio;
+	return 0;
+
+out_put_dnode:
+	f2fs_put_dnode(&dn);
+out:
+	f2fs_decompress_end_io(cc->rpages, cc->cluster_size, true, false);
+	*bio_ret = bio;
+	return ret;
+}
+#endif
+
 /*
  * This function was originally taken from fs/mpage.c, and customized for f2fs.
  * Major change was from block_size == page_size in f2fs by default.
@@ -1909,7 +2149,7 @@ static int f2fs_read_single_page(struct inode *inode, struct page *page,
  * use ->readpage() or do the necessary surgery to decouple ->readpages()
  * from read-ahead.
  */
-static int f2fs_mpage_readpages(struct address_space *mapping,
+int f2fs_mpage_readpages(struct address_space *mapping,
 			struct list_head *pages, struct page *page,
 			unsigned nr_pages, bool is_readahead)
 {
@@ -1917,6 +2157,19 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 	sector_t last_block_in_bio = 0;
 	struct inode *inode = mapping->host;
 	struct f2fs_map_blocks map;
+#ifdef CONFIG_F2FS_FS_COMPRESSION
+	struct compress_ctx cc = {
+		.inode = inode,
+		.log_cluster_size = F2FS_I(inode)->i_log_cluster_size,
+		.cluster_size = F2FS_I(inode)->i_cluster_size,
+		.cluster_idx = NULL_CLUSTER,
+		.rpages = NULL,
+		.cpages = NULL,
+		.nr_rpages = 0,
+		.nr_cpages = 0,
+	};
+#endif
+	unsigned max_nr_pages = nr_pages;
 	int ret = 0;
 
 	map.m_pblk = 0;
@@ -1940,9 +2193,41 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 				goto next_page;
 		}
 
-		ret = f2fs_read_single_page(inode, page, nr_pages, &map, &bio,
-					&last_block_in_bio, is_readahead);
+#ifdef CONFIG_F2FS_FS_COMPRESSION
+		if (f2fs_compressed_file(inode)) {
+			/* there are remained comressed pages, submit them */
+			if (!f2fs_cluster_can_merge_page(&cc, page->index)) {
+				ret = f2fs_read_multi_pages(&cc, &bio,
+							max_nr_pages,
+							&last_block_in_bio,
+							is_readahead);
+				f2fs_destroy_compress_ctx(&cc);
+				if (ret)
+					goto set_error_page;
+			}
+			ret = f2fs_is_compressed_cluster(inode, page->index);
+			if (ret < 0)
+				goto set_error_page;
+			else if (!ret)
+				goto read_single_page;
+
+			ret = f2fs_init_compress_ctx(&cc);
+			if (ret)
+				goto set_error_page;
+
+			f2fs_compress_ctx_add_page(&cc, page);
+
+			goto next_page;
+		}
+read_single_page:
+#endif
+
+		ret = f2fs_read_single_page(inode, page, max_nr_pages, &map,
+					&bio, &last_block_in_bio, is_readahead);
 		if (ret) {
+#ifdef CONFIG_F2FS_FS_COMPRESSION
+set_error_page:
+#endif
 			SetPageError(page);
 			zero_user_segment(page, 0, PAGE_SIZE);
 			unlock_page(page);
@@ -1950,6 +2235,19 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 next_page:
 		if (pages)
 			put_page(page);
+
+#ifdef CONFIG_F2FS_FS_COMPRESSION
+		if (f2fs_compressed_file(inode)) {
+			/* last page */
+			if (nr_pages == 1 && !f2fs_cluster_is_empty(&cc)) {
+				ret = f2fs_read_multi_pages(&cc, &bio,
+							max_nr_pages,
+							&last_block_in_bio,
+							is_readahead);
+				f2fs_destroy_compress_ctx(&cc);
+			}
+		}
+#endif
 	}
 	BUG_ON(pages && !list_empty(pages));
 	if (bio)
@@ -1964,6 +2262,11 @@ static int f2fs_read_data_page(struct file *file, struct page *page)
 
 	trace_f2fs_readpage(page, DATA);
 
+	if (!f2fs_is_compress_backend_ready(inode)) {
+		unlock_page(page);
+		return -EOPNOTSUPP;
+	}
+
 	/* If the file has inline data, try to read it directly */
 	if (f2fs_has_inline_data(inode))
 		ret = f2fs_read_inline_data(inode, page);
@@ -1982,6 +2285,9 @@ static int f2fs_read_data_pages(struct file *file,
 
 	trace_f2fs_readpages(inode, page, nr_pages);
 
+	if (!f2fs_is_compress_backend_ready(inode))
+		return 0;
+
 	/* If the file has inline data, skip readpages */
 	if (f2fs_has_inline_data(inode))
 		return 0;
@@ -1989,22 +2295,23 @@ static int f2fs_read_data_pages(struct file *file,
 	return f2fs_mpage_readpages(mapping, pages, NULL, nr_pages, true);
 }
 
-static int encrypt_one_page(struct f2fs_io_info *fio)
+int f2fs_encrypt_one_page(struct f2fs_io_info *fio)
 {
 	struct inode *inode = fio->page->mapping->host;
-	struct page *mpage;
+	struct page *mpage, *page;
 	gfp_t gfp_flags = GFP_NOFS;
 
 	if (!f2fs_encrypted_file(inode))
 		return 0;
 
+	page = fio->compressed_page ? fio->compressed_page : fio->page;
+
 	/* wait for GCed page writeback via META_MAPPING */
 	f2fs_wait_on_block_writeback(inode, fio->old_blkaddr);
 
 retry_encrypt:
-	fio->encrypted_page = fscrypt_encrypt_pagecache_blocks(fio->page,
-							       PAGE_SIZE, 0,
-							       gfp_flags);
+	fio->encrypted_page = fscrypt_encrypt_pagecache_blocks(page,
+					PAGE_SIZE, 0, gfp_flags);
 	if (IS_ERR(fio->encrypted_page)) {
 		/* flush pending IOs and wait for a while in the ENOMEM case */
 		if (PTR_ERR(fio->encrypted_page) == -ENOMEM) {
@@ -2164,7 +2471,7 @@ int f2fs_do_write_data_page(struct f2fs_io_info *fio)
 	if (ipu_force ||
 		(__is_valid_data_blkaddr(fio->old_blkaddr) &&
 					need_inplace_update(fio))) {
-		err = encrypt_one_page(fio);
+		err = f2fs_encrypt_one_page(fio);
 		if (err)
 			goto out_writepage;
 
@@ -2200,13 +2507,16 @@ int f2fs_do_write_data_page(struct f2fs_io_info *fio)
 
 	fio->version = ni.version;
 
-	err = encrypt_one_page(fio);
+	err = f2fs_encrypt_one_page(fio);
 	if (err)
 		goto out_writepage;
 
 	set_page_writeback(page);
 	ClearPageError(page);
 
+	if (fio->compr_blocks && fio->old_blkaddr == COMPRESS_ADDR)
+		f2fs_i_compr_blocks_update(inode, fio->compr_blocks - 1, false);
+
 	/* LFS mode write path */
 	f2fs_outplace_write_data(&dn, fio);
 	trace_f2fs_do_write_data_page(page, OPU);
@@ -2221,16 +2531,17 @@ int f2fs_do_write_data_page(struct f2fs_io_info *fio)
 	return err;
 }
 
-static int __write_data_page(struct page *page, bool *submitted,
+int f2fs_write_single_data_page(struct page *page, int *submitted,
 				struct bio **bio,
 				sector_t *last_block,
 				struct writeback_control *wbc,
-				enum iostat_type io_type)
+				enum iostat_type io_type,
+				int compr_blocks)
 {
 	struct inode *inode = page->mapping->host;
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
 	loff_t i_size = i_size_read(inode);
-	const pgoff_t end_index = ((unsigned long long) i_size)
+	const pgoff_t end_index = ((unsigned long long)i_size)
 							>> PAGE_SHIFT;
 	loff_t psize = (loff_t)(page->index + 1) << PAGE_SHIFT;
 	unsigned offset = 0;
@@ -2246,6 +2557,7 @@ static int __write_data_page(struct page *page, bool *submitted,
 		.page = page,
 		.encrypted_page = NULL,
 		.submitted = false,
+		.compr_blocks = compr_blocks,
 		.need_lock = LOCK_RETRY,
 		.io_type = io_type,
 		.io_wbc = wbc,
@@ -2270,7 +2582,9 @@ static int __write_data_page(struct page *page, bool *submitted,
 	if (unlikely(is_sbi_flag_set(sbi, SBI_POR_DOING)))
 		goto redirty_out;
 
-	if (page->index < end_index || f2fs_verity_in_progress(inode))
+	if (page->index < end_index ||
+			f2fs_verity_in_progress(inode) ||
+			compr_blocks)
 		goto write;
 
 	/*
@@ -2346,7 +2660,6 @@ static int __write_data_page(struct page *page, bool *submitted,
 		f2fs_remove_dirty_inode(inode);
 		submitted = NULL;
 	}
-
 	unlock_page(page);
 	if (!S_ISDIR(inode->i_mode) && !IS_NOQUOTA(inode) &&
 					!F2FS_I(inode)->cp_task)
@@ -2359,7 +2672,7 @@ static int __write_data_page(struct page *page, bool *submitted,
 	}
 
 	if (submitted)
-		*submitted = fio.submitted;
+		*submitted = fio.submitted ? 1 : 0;
 
 	return 0;
 
@@ -2380,7 +2693,23 @@ static int __write_data_page(struct page *page, bool *submitted,
 static int f2fs_write_data_page(struct page *page,
 					struct writeback_control *wbc)
 {
-	return __write_data_page(page, NULL, NULL, NULL, wbc, FS_DATA_IO);
+#ifdef CONFIG_F2FS_FS_COMPRESSION
+	struct inode *inode = page->mapping->host;
+
+	if (unlikely(f2fs_cp_error(F2FS_I_SB(inode))))
+		goto out;
+
+	if (f2fs_compressed_file(inode)) {
+		if (f2fs_is_compressed_cluster(inode, page->index)) {
+			redirty_page_for_writepage(wbc, page);
+			return AOP_WRITEPAGE_ACTIVATE;
+		}
+	}
+out:
+#endif
+
+	return f2fs_write_single_data_page(page, NULL, NULL, NULL,
+						wbc, FS_DATA_IO, 0);
 }
 
 /*
@@ -2393,11 +2722,27 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 					enum iostat_type io_type)
 {
 	int ret = 0;
-	int done = 0;
+	int done = 0, retry = 0;
 	struct pagevec pvec;
 	struct f2fs_sb_info *sbi = F2FS_M_SB(mapping);
 	struct bio *bio = NULL;
 	sector_t last_block;
+#ifdef CONFIG_F2FS_FS_COMPRESSION
+	struct inode *inode = mapping->host;
+	struct compress_ctx cc = {
+		.inode = inode,
+		.log_cluster_size = F2FS_I(inode)->i_log_cluster_size,
+		.cluster_size = F2FS_I(inode)->i_cluster_size,
+		.cluster_idx = NULL_CLUSTER,
+		.rpages = NULL,
+		.nr_rpages = 0,
+		.cpages = NULL,
+		.rbuf = NULL,
+		.cbuf = NULL,
+		.rlen = PAGE_SIZE * F2FS_I(inode)->i_cluster_size,
+		.private = NULL,
+	};
+#endif
 	int nr_pages;
 	pgoff_t uninitialized_var(writeback_index);
 	pgoff_t index;
@@ -2407,6 +2752,8 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 	int range_whole = 0;
 	xa_mark_t tag;
 	int nwritten = 0;
+	int submitted = 0;
+	int i;
 
 	pagevec_init(&pvec);
 
@@ -2436,12 +2783,11 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 	else
 		tag = PAGECACHE_TAG_DIRTY;
 retry:
+	retry = 0;
 	if (wbc->sync_mode == WB_SYNC_ALL || wbc->tagged_writepages)
 		tag_pages_for_writeback(mapping, index, end);
 	done_index = index;
-	while (!done && (index <= end)) {
-		int i;
-
+	while (!done && !retry && (index <= end)) {
 		nr_pages = pagevec_lookup_range_tag(&pvec, mapping, &index, end,
 				tag);
 		if (nr_pages == 0)
@@ -2449,15 +2795,62 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 
 		for (i = 0; i < nr_pages; i++) {
 			struct page *page = pvec.pages[i];
-			bool submitted = false;
+			bool need_readd;
+readd:
+			need_readd = false;
+#ifdef CONFIG_F2FS_FS_COMPRESSION
+			if (f2fs_compressed_file(inode)) {
+				ret = f2fs_init_compress_ctx(&cc);
+				if (ret) {
+					done = 1;
+					break;
+				}
+
+				if (!f2fs_cluster_can_merge_page(&cc,
+								page->index)) {
+					ret = f2fs_write_multi_pages(&cc,
+						&submitted, wbc, io_type);
+					if (!ret)
+						need_readd = true;
+					goto result;
+				}
 
+				if (unlikely(f2fs_cp_error(sbi)))
+					goto lock_page;
+
+				if (f2fs_cluster_is_empty(&cc)) {
+					void *fsdata = NULL;
+					struct page *pagep;
+					int ret2;
+
+					ret2 = f2fs_prepare_compress_overwrite(
+							inode, &pagep,
+							page->index, &fsdata);
+					if (ret2 < 0) {
+						ret = ret2;
+						done = 1;
+						break;
+					} else if (ret2 &&
+						!f2fs_compress_write_end(inode,
+								fsdata, page->index,
+								1)) {
+						retry = 1;
+						break;
+					}
+				} else {
+					goto lock_page;
+				}
+			}
+#endif
 			/* give a priority to WB_SYNC threads */
 			if (atomic_read(&sbi->wb_sync_req[DATA]) &&
 					wbc->sync_mode == WB_SYNC_NONE) {
 				done = 1;
 				break;
 			}
-
+#ifdef CONFIG_F2FS_FS_COMPRESSION
+lock_page:
+#endif
 			done_index = page->index;
 retry_write:
 			lock_page(page);
@@ -2484,45 +2877,71 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 			if (!clear_page_dirty_for_io(page))
 				goto continue_unlock;
 
-			ret = __write_data_page(page, &submitted, &bio,
-					&last_block, wbc, io_type);
+#ifdef CONFIG_F2FS_FS_COMPRESSION
+			if (f2fs_compressed_file(inode)) {
+				get_page(page);
+				f2fs_compress_ctx_add_page(&cc, page);
+				continue;
+			}
+#endif
+			ret = f2fs_write_single_data_page(page, &submitted,
+					&bio, &last_block, wbc, io_type, 0);
+			if (ret == AOP_WRITEPAGE_ACTIVATE)
+				unlock_page(page);
+#ifdef CONFIG_F2FS_FS_COMPRESSION
+result:
+#endif
+			nwritten += submitted;
+			wbc->nr_to_write -= submitted;
+
 			if (unlikely(ret)) {
 				/*
 				 * keep nr_to_write, since vfs uses this to
 				 * get # of written pages.
 				 */
 				if (ret == AOP_WRITEPAGE_ACTIVATE) {
-					unlock_page(page);
 					ret = 0;
-					continue;
+					goto next;
 				} else if (ret == -EAGAIN) {
 					ret = 0;
 					if (wbc->sync_mode == WB_SYNC_ALL) {
 						cond_resched();
 						congestion_wait(BLK_RW_ASYNC,
-									HZ/50);
+								HZ/50);
 						goto retry_write;
 					}
-					continue;
+					goto next;
 				}
 				done_index = page->index + 1;
 				done = 1;
 				break;
-			} else if (submitted) {
-				nwritten++;
 			}
 
-			if (--wbc->nr_to_write <= 0 &&
+			if (wbc->nr_to_write <= 0 &&
 					wbc->sync_mode == WB_SYNC_NONE) {
 				done = 1;
 				break;
 			}
+next:
+			if (need_readd)
+				goto readd;
 		}
 		pagevec_release(&pvec);
 		cond_resched();
 	}
-
-	if (!cycled && !done) {
+#ifdef CONFIG_F2FS_FS_COMPRESSION
+	/* flush remained pages in compress cluster */
+	if (f2fs_compressed_file(inode) && !f2fs_cluster_is_empty(&cc)) {
+		ret = f2fs_write_multi_pages(&cc, &submitted, wbc, io_type);
+		nwritten += submitted;
+		wbc->nr_to_write -= submitted;
+		if (ret) {
+			done = 1;
+			retry = 0;
+		}
+	}
+#endif
+	if ((!cycled && !done) || retry) {
 		cycled = 1;
 		index = 0;
 		end = writeback_index - 1;
@@ -2546,6 +2965,8 @@ static inline bool __should_serialize_io(struct inode *inode,
 {
 	if (!S_ISREG(inode->i_mode))
 		return false;
+	if (f2fs_compressed_file(inode))
+		return true;
 	if (IS_NOQUOTA(inode))
 		return false;
 	/* to avoid deadlock in path of data flush */
@@ -2690,6 +3111,7 @@ static int prepare_write_begin(struct f2fs_sb_info *sbi,
 		__do_map_lock(sbi, flag, true);
 		locked = true;
 	}
+
 restart:
 	/* check inline_data */
 	ipage = f2fs_get_node_page(sbi, inode->i_ino);
@@ -2780,6 +3202,24 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 		if (err)
 			goto fail;
 	}
+
+#ifdef CONFIG_F2FS_FS_COMPRESSION
+	if (f2fs_compressed_file(inode)) {
+		int ret;
+
+		*fsdata = NULL;
+
+		ret = f2fs_prepare_compress_overwrite(inode, pagep,
+							index, fsdata);
+		if (ret < 0) {
+			err = ret;
+			goto fail;
+		} else if (ret) {
+			return 0;
+		}
+	}
+#endif
+
 repeat:
 	/*
 	 * Do not use grab_cache_page_write_begin() to avoid deadlock due to
@@ -2792,6 +3232,8 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 		goto fail;
 	}
 
+	/* TODO: cluster can be compressed due to race with .writepage */
+
 	*pagep = page;
 
 	err = prepare_write_begin(sbi, page, pos, len,
@@ -2875,6 +3317,16 @@ static int f2fs_write_end(struct file *file,
 		else
 			SetPageUptodate(page);
 	}
+
+#ifdef CONFIG_F2FS_FS_COMPRESSION
+	/* overwrite compressed file */
+	if (f2fs_compressed_file(inode) && fsdata) {
+		f2fs_compress_write_end(inode, fsdata, page->index, copied);
+		f2fs_update_time(F2FS_I_SB(inode), REQ_TIME);
+		return copied;
+	}
+#endif
+
 	if (!copied)
 		goto unlock_out;
 
@@ -3265,6 +3717,9 @@ static int f2fs_swap_activate(struct swap_info_struct *sis, struct file *file,
 	if (ret)
 		return ret;
 
+	if (f2fs_disable_compressed_file(inode))
+		return -EINVAL;
+
 	ret = check_swap_activate(file, sis->max);
 	if (ret)
 		return ret;
@@ -3349,6 +3804,27 @@ void f2fs_destroy_post_read_processing(void)
 	kmem_cache_destroy(bio_post_read_ctx_cache);
 }
 
+int f2fs_init_post_read_wq(struct f2fs_sb_info *sbi)
+{
+	if (!f2fs_sb_has_encrypt(sbi) &&
+		!f2fs_sb_has_verity(sbi) &&
+		!f2fs_sb_has_compression(sbi))
+		return 0;
+
+	sbi->post_read_wq = alloc_workqueue("f2fs_post_read_wq",
+						 WQ_UNBOUND | WQ_HIGHPRI,
+						 num_online_cpus());
+	if (!sbi->post_read_wq)
+		return -ENOMEM;
+	return 0;
+}
+
+void f2fs_destroy_post_read_wq(struct f2fs_sb_info *sbi)
+{
+	if (sbi->post_read_wq)
+		destroy_workqueue(sbi->post_read_wq);
+}
+
 int __init f2fs_init_bio_entry_cache(void)
 {
 	bio_entry_slab = f2fs_kmem_cache_create("bio_entry_slab",

commit f543805fcd60f3f9a491cfa2f2dc9284d2569c28
Author: Chao Yu <yuchao0@huawei.com>
Date:   Wed Dec 4 09:52:58 2019 +0800

    f2fs: introduce private bioset
    
    In low memory scenario, we can allocate multiple bios without
    submitting any of them.
    
    - f2fs_write_checkpoint()
     - block_operations()
      - f2fs_sync_node_pages()
       step 1) flush cold nodes, allocate new bio from mempool
       - bio_alloc()
        - mempool_alloc()
       step 2) flush hot nodes, allocate a bio from mempool
       - bio_alloc()
        - mempool_alloc()
       step 3) flush warm nodes, be stuck in below call path
       - bio_alloc()
        - mempool_alloc()
         - loop to wait mempool element release, as we only
           reserved memory for two bio allocation, however above
           allocated two bios may never be submitted.
    
    So we need avoid using default bioset, in this patch we introduce a
    private bioset, in where we enlarg mempool element count to total
    number of log header, so that we can make sure we have enough
    backuped memory pool in scenario of allocating/holding multiple
    bios.
    
    Signed-off-by: Gao Xiang <gaoxiang25@huawei.com>
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 3b2945121557..f89aeaaea90e 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -31,6 +31,47 @@
 static struct kmem_cache *bio_post_read_ctx_cache;
 static struct kmem_cache *bio_entry_slab;
 static mempool_t *bio_post_read_ctx_pool;
+static struct bio_set f2fs_bioset;
+
+#define	F2FS_BIO_POOL_SIZE	NR_CURSEG_TYPE
+
+int __init f2fs_init_bioset(void)
+{
+	if (bioset_init(&f2fs_bioset, F2FS_BIO_POOL_SIZE,
+					0, BIOSET_NEED_BVECS))
+		return -ENOMEM;
+	return 0;
+}
+
+void f2fs_destroy_bioset(void)
+{
+	bioset_exit(&f2fs_bioset);
+}
+
+static inline struct bio *__f2fs_bio_alloc(gfp_t gfp_mask,
+						unsigned int nr_iovecs)
+{
+	return bio_alloc_bioset(gfp_mask, nr_iovecs, &f2fs_bioset);
+}
+
+struct bio *f2fs_bio_alloc(struct f2fs_sb_info *sbi, int npages, bool no_fail)
+{
+	struct bio *bio;
+
+	if (no_fail) {
+		/* No failure on bio allocation */
+		bio = __f2fs_bio_alloc(GFP_NOIO, npages);
+		if (!bio)
+			bio = __f2fs_bio_alloc(GFP_NOIO | __GFP_NOFAIL, npages);
+		return bio;
+	}
+	if (time_to_inject(sbi, FAULT_ALLOC_BIO)) {
+		f2fs_show_injection_info(sbi, FAULT_ALLOC_BIO);
+		return NULL;
+	}
+
+	return __f2fs_bio_alloc(GFP_KERNEL, npages);
+}
 
 static bool __is_cp_guaranteed(struct page *page)
 {
@@ -3317,7 +3358,7 @@ int __init f2fs_init_bio_entry_cache(void)
 	return 0;
 }
 
-void __exit f2fs_destroy_bio_entry_cache(void)
+void f2fs_destroy_bio_entry_cache(void)
 {
 	kmem_cache_destroy(bio_entry_slab);
 }

commit 3f188c23d774646dcfa35e937f75f66872e62507
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Tue Dec 3 18:54:29 2019 -0800

    f2fs: keep quota data on write_begin failure
    
    This patch avoids some unnecessary locks for quota files when write_begin
    fails.
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index fc40a72f7827..3b2945121557 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -2600,14 +2600,16 @@ static void f2fs_write_failed(struct address_space *mapping, loff_t to)
 	struct inode *inode = mapping->host;
 	loff_t i_size = i_size_read(inode);
 
+	if (IS_NOQUOTA(inode))
+		return;
+
 	/* In the fs-verity case, f2fs_end_enable_verity() does the truncate */
 	if (to > i_size && !f2fs_verity_in_progress(inode)) {
 		down_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
 		down_write(&F2FS_I(inode)->i_mmap_sem);
 
 		truncate_pagecache(inode, i_size);
-		if (!IS_NOQUOTA(inode))
-			f2fs_truncate_blocks(inode, i_size, true);
+		f2fs_truncate_blocks(inode, i_size, true);
 
 		up_write(&F2FS_I(inode)->i_mmap_sem);
 		up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);

commit 47501f87c61ad2aa234add63e1ae231521dbc3f5
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Tue Nov 26 15:01:42 2019 -0800

    f2fs: preallocate DIO blocks when forcing buffered_io
    
    The previous preallocation and DIO decision like below.
    
                             allow_outplace_dio              !allow_outplace_dio
    f2fs_force_buffered_io   (*) No_Prealloc / Buffered_IO   Prealloc / Buffered_IO
    !f2fs_force_buffered_io  No_Prealloc / DIO               Prealloc / DIO
    
    But, Javier reported Case (*) where zoned device bypassed preallocation but
    fell back to buffered writes in f2fs_direct_IO(), resulting in stale data
    being read.
    
    In order to fix the issue, actually we need to preallocate blocks whenever
    we fall back to buffered IO like this. No change is made in the other cases.
    
                             allow_outplace_dio              !allow_outplace_dio
    f2fs_force_buffered_io   (*) Prealloc / Buffered_IO      Prealloc / Buffered_IO
    !f2fs_force_buffered_io  No_Prealloc / DIO               Prealloc / DIO
    
    Reported-and-tested-by: Javier Gonzalez <javier@javigon.com>
    Signed-off-by: Damien Le Moal <damien.lemoal@wdc.com>
    Tested-by: Shin'ichiro Kawasaki <shinichiro.kawasaki@wdc.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Reviewed-by: Javier González <javier@javigon.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index a034cd0ce021..fc40a72f7827 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1180,19 +1180,6 @@ int f2fs_preallocate_blocks(struct kiocb *iocb, struct iov_iter *from)
 	int err = 0;
 	bool direct_io = iocb->ki_flags & IOCB_DIRECT;
 
-	/* convert inline data for Direct I/O*/
-	if (direct_io) {
-		err = f2fs_convert_inline_inode(inode);
-		if (err)
-			return err;
-	}
-
-	if (direct_io && allow_outplace_dio(inode, iocb, from))
-		return 0;
-
-	if (is_inode_flag_set(inode, FI_NO_PREALLOC))
-		return 0;
-
 	map.m_lblk = F2FS_BLK_ALIGN(iocb->ki_pos);
 	map.m_len = F2FS_BYTES_TO_BLK(iocb->ki_pos + iov_iter_count(from));
 	if (map.m_len > map.m_lblk)

commit c45d6002ff7a322022560e9b19ad867b01fec77f
Author: Chao Yu <yuchao0@huawei.com>
Date:   Fri Nov 1 17:53:23 2019 +0800

    f2fs: show f2fs instance in printk_ratelimited
    
    As Eric mentioned, bare printk{,_ratelimited} won't show which
    filesystem instance these message is coming from, this patch tries
    to show fs instance with sb->s_id field in all places we missed
    before.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 8a80fc86a44f..a034cd0ce021 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -168,9 +168,10 @@ static bool f2fs_bio_post_read_required(struct bio *bio)
 
 static void f2fs_read_end_io(struct bio *bio)
 {
-	if (time_to_inject(F2FS_P_SB(bio_first_page_all(bio)),
-						FAULT_READ_IO)) {
-		f2fs_show_injection_info(FAULT_READ_IO);
+	struct f2fs_sb_info *sbi = F2FS_P_SB(bio_first_page_all(bio));
+
+	if (time_to_inject(sbi, FAULT_READ_IO)) {
+		f2fs_show_injection_info(sbi, FAULT_READ_IO);
 		bio->bi_status = BLK_STS_IOERR;
 	}
 
@@ -192,7 +193,7 @@ static void f2fs_write_end_io(struct bio *bio)
 	struct bvec_iter_all iter_all;
 
 	if (time_to_inject(sbi, FAULT_WRITE_IO)) {
-		f2fs_show_injection_info(FAULT_WRITE_IO);
+		f2fs_show_injection_info(sbi, FAULT_WRITE_IO);
 		bio->bi_status = BLK_STS_IOERR;
 	}
 

commit 1f0d5c911b64165c9754139a26c8c2fad352c132
Author: Chao Yu <yuchao0@huawei.com>
Date:   Thu Nov 7 17:29:00 2019 +0800

    f2fs: fix potential overflow
    
    We expect 64-bit calculation result from below statement, however
    in 32-bit machine, looped left shift operation on pgoff_t type
    variable may cause overflow issue, fix it by forcing type cast.
    
    page->index << PAGE_SHIFT;
    
    Fixes: 26de9b117130 ("f2fs: avoid unnecessary updating inode during fsync")
    Fixes: 0a2aa8fbb969 ("f2fs: refactor __exchange_data_block for speed up")
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index ba3bcf4c7889..8a80fc86a44f 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -2203,7 +2203,7 @@ static int __write_data_page(struct page *page, bool *submitted,
 	loff_t i_size = i_size_read(inode);
 	const pgoff_t end_index = ((unsigned long long) i_size)
 							>> PAGE_SHIFT;
-	loff_t psize = (page->index + 1) << PAGE_SHIFT;
+	loff_t psize = (loff_t)(page->index + 1) << PAGE_SHIFT;
 	unsigned offset = 0;
 	bool need_balance_fs = false;
 	int err = 0;

commit 0b20fcec8651569935a10afe03fedc0b812d044e
Author: Chao Yu <yuchao0@huawei.com>
Date:   Mon Sep 30 18:53:25 2019 +0800

    f2fs: cache global IPU bio
    
    In commit 8648de2c581e ("f2fs: add bio cache for IPU"), we added
    f2fs_submit_ipu_bio() in __write_data_page() as below:
    
    __write_data_page()
    
            if (!S_ISDIR(inode->i_mode) && !IS_NOQUOTA(inode)) {
                    f2fs_submit_ipu_bio(sbi, bio, page);
                    ....
            }
    
    in order to avoid below deadlock:
    
    Thread A                                Thread B
    - __write_data_page (inode x, page y)
     - f2fs_do_write_data_page
      - set_page_writeback        ---- set writeback flag in page y
      - f2fs_inplace_write_data
     - f2fs_balance_fs
                                             - lock gc_mutex
     - lock gc_mutex
                                              - f2fs_gc
                                               - do_garbage_collect
                                                - gc_data_segment
                                                 - move_data_page
                                                  - f2fs_wait_on_page_writeback
                                                   - wait_on_page_writeback  --- wait writeback of page y
    
    However, the bio submission breaks the merge of IPU IOs.
    
    So in this patch let's add a global bio cache for merged IPU pages,
    then f2fs_wait_on_page_writeback() is able to submit bio if a
    writebacked page is cached in global bio cache.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 5755e897a5f0..ba3bcf4c7889 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -29,6 +29,7 @@
 #define NUM_PREALLOC_POST_READ_CTXS	128
 
 static struct kmem_cache *bio_post_read_ctx_cache;
+static struct kmem_cache *bio_entry_slab;
 static mempool_t *bio_post_read_ctx_pool;
 
 static bool __is_cp_guaranteed(struct page *page)
@@ -543,6 +544,126 @@ static bool io_is_mergeable(struct f2fs_sb_info *sbi, struct bio *bio,
 	return io_type_is_mergeable(io, fio);
 }
 
+static void add_bio_entry(struct f2fs_sb_info *sbi, struct bio *bio,
+				struct page *page, enum temp_type temp)
+{
+	struct f2fs_bio_info *io = sbi->write_io[DATA] + temp;
+	struct bio_entry *be;
+
+	be = f2fs_kmem_cache_alloc(bio_entry_slab, GFP_NOFS);
+	be->bio = bio;
+	bio_get(bio);
+
+	if (bio_add_page(bio, page, PAGE_SIZE, 0) != PAGE_SIZE)
+		f2fs_bug_on(sbi, 1);
+
+	down_write(&io->bio_list_lock);
+	list_add_tail(&be->list, &io->bio_list);
+	up_write(&io->bio_list_lock);
+}
+
+static void del_bio_entry(struct bio_entry *be)
+{
+	list_del(&be->list);
+	kmem_cache_free(bio_entry_slab, be);
+}
+
+static int add_ipu_page(struct f2fs_sb_info *sbi, struct bio **bio,
+							struct page *page)
+{
+	enum temp_type temp;
+	bool found = false;
+	int ret = -EAGAIN;
+
+	for (temp = HOT; temp < NR_TEMP_TYPE && !found; temp++) {
+		struct f2fs_bio_info *io = sbi->write_io[DATA] + temp;
+		struct list_head *head = &io->bio_list;
+		struct bio_entry *be;
+
+		down_write(&io->bio_list_lock);
+		list_for_each_entry(be, head, list) {
+			if (be->bio != *bio)
+				continue;
+
+			found = true;
+
+			if (bio_add_page(*bio, page, PAGE_SIZE, 0) == PAGE_SIZE) {
+				ret = 0;
+				break;
+			}
+
+			/* bio is full */
+			del_bio_entry(be);
+			__submit_bio(sbi, *bio, DATA);
+			break;
+		}
+		up_write(&io->bio_list_lock);
+	}
+
+	if (ret) {
+		bio_put(*bio);
+		*bio = NULL;
+	}
+
+	return ret;
+}
+
+void f2fs_submit_merged_ipu_write(struct f2fs_sb_info *sbi,
+					struct bio **bio, struct page *page)
+{
+	enum temp_type temp;
+	bool found = false;
+	struct bio *target = bio ? *bio : NULL;
+
+	for (temp = HOT; temp < NR_TEMP_TYPE && !found; temp++) {
+		struct f2fs_bio_info *io = sbi->write_io[DATA] + temp;
+		struct list_head *head = &io->bio_list;
+		struct bio_entry *be;
+
+		if (list_empty(head))
+			continue;
+
+		down_read(&io->bio_list_lock);
+		list_for_each_entry(be, head, list) {
+			if (target)
+				found = (target == be->bio);
+			else
+				found = __has_merged_page(be->bio, NULL,
+								page, 0);
+			if (found)
+				break;
+		}
+		up_read(&io->bio_list_lock);
+
+		if (!found)
+			continue;
+
+		found = false;
+
+		down_write(&io->bio_list_lock);
+		list_for_each_entry(be, head, list) {
+			if (target)
+				found = (target == be->bio);
+			else
+				found = __has_merged_page(be->bio, NULL,
+								page, 0);
+			if (found) {
+				target = be->bio;
+				del_bio_entry(be);
+				break;
+			}
+		}
+		up_write(&io->bio_list_lock);
+	}
+
+	if (found)
+		__submit_bio(sbi, target, DATA);
+	if (bio && *bio) {
+		bio_put(*bio);
+		*bio = NULL;
+	}
+}
+
 int f2fs_merge_page_bio(struct f2fs_io_info *fio)
 {
 	struct bio *bio = *fio->bio;
@@ -557,20 +678,17 @@ int f2fs_merge_page_bio(struct f2fs_io_info *fio)
 	f2fs_trace_ios(fio, 0);
 
 	if (bio && !page_is_mergeable(fio->sbi, bio, *fio->last_block,
-						fio->new_blkaddr)) {
-		__submit_bio(fio->sbi, bio, fio->type);
-		bio = NULL;
-	}
+						fio->new_blkaddr))
+		f2fs_submit_merged_ipu_write(fio->sbi, &bio, NULL);
 alloc_new:
 	if (!bio) {
 		bio = __bio_alloc(fio, BIO_MAX_PAGES);
 		bio_set_op_attrs(bio, fio->op, fio->op_flags);
-	}
 
-	if (bio_add_page(bio, page, PAGE_SIZE, 0) < PAGE_SIZE) {
-		__submit_bio(fio->sbi, bio, fio->type);
-		bio = NULL;
-		goto alloc_new;
+		add_bio_entry(fio->sbi, bio, page, fio->temp);
+	} else {
+		if (add_ipu_page(fio->sbi, &bio, page))
+			goto alloc_new;
 	}
 
 	if (fio->io_wbc)
@@ -584,19 +702,6 @@ int f2fs_merge_page_bio(struct f2fs_io_info *fio)
 	return 0;
 }
 
-static void f2fs_submit_ipu_bio(struct f2fs_sb_info *sbi, struct bio **bio,
-							struct page *page)
-{
-	if (!bio)
-		return;
-
-	if (!__has_merged_page(*bio, NULL, page, 0))
-		return;
-
-	__submit_bio(sbi, *bio, DATA);
-	*bio = NULL;
-}
-
 void f2fs_submit_page_write(struct f2fs_io_info *fio)
 {
 	struct f2fs_sb_info *sbi = fio->sbi;
@@ -2215,14 +2320,12 @@ static int __write_data_page(struct page *page, bool *submitted,
 
 	unlock_page(page);
 	if (!S_ISDIR(inode->i_mode) && !IS_NOQUOTA(inode) &&
-					!F2FS_I(inode)->cp_task) {
-		f2fs_submit_ipu_bio(sbi, bio, page);
+					!F2FS_I(inode)->cp_task)
 		f2fs_balance_fs(sbi, need_balance_fs);
-	}
 
 	if (unlikely(f2fs_cp_error(sbi))) {
-		f2fs_submit_ipu_bio(sbi, bio, page);
 		f2fs_submit_merged_write(sbi, DATA);
+		f2fs_submit_merged_ipu_write(sbi, bio, NULL);
 		submitted = NULL;
 	}
 
@@ -2342,13 +2445,11 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 			}
 
 			if (PageWriteback(page)) {
-				if (wbc->sync_mode != WB_SYNC_NONE) {
+				if (wbc->sync_mode != WB_SYNC_NONE)
 					f2fs_wait_on_page_writeback(page,
 							DATA, true, true);
-					f2fs_submit_ipu_bio(sbi, &bio, page);
-				} else {
+				else
 					goto continue_unlock;
-				}
 			}
 
 			if (!clear_page_dirty_for_io(page))
@@ -2406,7 +2507,7 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 								NULL, 0, DATA);
 	/* submit cached bio of IPU write */
 	if (bio)
-		__submit_bio(sbi, bio, DATA);
+		f2fs_submit_merged_ipu_write(sbi, &bio, NULL);
 
 	return ret;
 }
@@ -3211,8 +3312,22 @@ int __init f2fs_init_post_read_processing(void)
 	return -ENOMEM;
 }
 
-void __exit f2fs_destroy_post_read_processing(void)
+void f2fs_destroy_post_read_processing(void)
 {
 	mempool_destroy(bio_post_read_ctx_pool);
 	kmem_cache_destroy(bio_post_read_ctx_cache);
 }
+
+int __init f2fs_init_bio_entry_cache(void)
+{
+	bio_entry_slab = f2fs_kmem_cache_create("bio_entry_slab",
+			sizeof(struct bio_entry));
+	if (!bio_entry_slab)
+		return -ENOMEM;
+	return 0;
+}
+
+void __exit f2fs_destroy_bio_entry_cache(void)
+{
+	kmem_cache_destroy(bio_entry_slab);
+}

commit fbc246a12aac27f7b25a37f9398bb3bc552cec92
Merge: 7ce1e15d9a85 fbbf779989d2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Sep 21 14:26:33 2019 -0700

    Merge tag 'f2fs-for-5.4' of git://git.kernel.org/pub/scm/linux/kernel/git/jaegeuk/f2fs
    
    Pull f2fs updates from Jaegeuk Kim:
     "In this round, we introduced casefolding support in f2fs, and fixed
      various bugs in individual features such as IO alignment,
      checkpoint=disable, quota, and swapfile.
    
      Enhancement:
       - support casefolding w/ enhancement in ext4
       - support fiemap for directory
       - support FS_IO_GET|SET_FSLABEL
    
      Bug fix:
       - fix IO stuck during checkpoint=disable
       - avoid infinite GC loop
       - fix panic/overflow related to IO alignment feature
       - fix livelock in swap file
       - fix discard command leak
       - disallow dio for atomic_write"
    
    * tag 'f2fs-for-5.4' of git://git.kernel.org/pub/scm/linux/kernel/git/jaegeuk/f2fs: (51 commits)
      f2fs: add a condition to detect overflow in f2fs_ioc_gc_range()
      f2fs: fix to add missing F2FS_IO_ALIGNED() condition
      f2fs: fix to fallback to buffered IO in IO aligned mode
      f2fs: fix to handle error path correctly in f2fs_map_blocks
      f2fs: fix extent corrupotion during directIO in LFS mode
      f2fs: check all the data segments against all node ones
      f2fs: Add a small clarification to CONFIG_FS_F2FS_FS_SECURITY
      f2fs: fix inode rwsem regression
      f2fs: fix to avoid accessing uninitialized field of inode page in is_alive()
      f2fs: avoid infinite GC loop due to stale atomic files
      f2fs: Fix indefinite loop in f2fs_gc()
      f2fs: convert inline_data in prior to i_size_write
      f2fs: fix error path of f2fs_convert_inline_page()
      f2fs: add missing documents of reserve_root/resuid/resgid
      f2fs: fix flushing node pages when checkpoint is disabled
      f2fs: enhance f2fs_is_checkpoint_ready()'s readability
      f2fs: clean up __bio_alloc()'s parameter
      f2fs: fix wrong error injection path in inc_valid_block_count()
      f2fs: fix to writeout dirty inode during node flush
      f2fs: optimize case-insensitive lookups
      ...

commit 8223ecc456d079ef9b7a1fed237134cf62e9e870
Author: Chao Yu <yuchao0@huawei.com>
Date:   Wed Aug 28 17:33:38 2019 +0800

    f2fs: fix to add missing F2FS_IO_ALIGNED() condition
    
    In f2fs_allocate_data_block(), we will reset fio.retry for IO
    alignment feature instead of IO serialization feature.
    
    In addition, spread F2FS_IO_ALIGNED() to check IO alignment
    feature status explicitly.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index a3e2ce5a6b22..adc64d514b79 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -294,6 +294,9 @@ static inline void __submit_bio(struct f2fs_sb_info *sbi,
 		if (test_opt(sbi, LFS) && current->plug)
 			blk_finish_plug(current->plug);
 
+		if (F2FS_IO_ALIGNED(sbi))
+			goto submit_io;
+
 		start = bio->bi_iter.bi_size >> F2FS_BLKSIZE_BITS;
 		start %= F2FS_IO_SIZE(sbi);
 
@@ -607,7 +610,8 @@ void f2fs_submit_page_write(struct f2fs_io_info *fio)
 		__submit_merged_bio(io);
 alloc_new:
 	if (io->bio == NULL) {
-		if ((fio->type == DATA || fio->type == NODE) &&
+		if (F2FS_IO_ALIGNED(sbi) &&
+				(fio->type == DATA || fio->type == NODE) &&
 				fio->new_blkaddr & F2FS_IO_SIZE_MASK(sbi)) {
 			dec_page_count(sbi, WB_DATA_TYPE(bio_page));
 			fio->retry = true;

commit 05e360061cbdcbfa93f8fcace2e7b53b2baed191
Author: Chao Yu <yuchao0@huawei.com>
Date:   Wed Aug 28 17:33:36 2019 +0800

    f2fs: fix to handle error path correctly in f2fs_map_blocks
    
    In f2fs_map_blocks(), we should bail out once __allocate_data_block()
    failed.
    
    Fixes: f847c699cff3 ("f2fs: allow out-place-update for direct IO in LFS mode")
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 7e9fafd44cbc..a3e2ce5a6b22 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1195,10 +1195,10 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 		if (test_opt(sbi, LFS) && flag == F2FS_GET_BLOCK_DIO &&
 							map->m_may_create) {
 			err = __allocate_data_block(&dn, map->m_seg_type);
-			if (!err) {
-				blkaddr = dn.data_blkaddr;
-				set_inode_flag(inode, FI_APPEND_WRITE);
-			}
+			if (err)
+				goto sync_out;
+			blkaddr = dn.data_blkaddr;
+			set_inode_flag(inode, FI_APPEND_WRITE);
 		}
 	} else {
 		if (create) {

commit 86f35dc39ef9cdc5d33548e2d4ddac815a39e542
Author: Chao Yu <yuchao0@huawei.com>
Date:   Wed Aug 28 17:33:35 2019 +0800

    f2fs: fix extent corrupotion during directIO in LFS mode
    
    In LFS mode, por_fsstress testcase reports a bug as below:
    
    [ASSERT] (fsck_chk_inode_blk: 931)  --> ino: 0x12fe has wrong ext: [pgofs:142, blk:215424, len:16]
    
    Since commit f847c699cff3 ("f2fs: allow out-place-update for direct
    IO in LFS mode"), we start to allow OPU mode for direct IO, however,
    we missed to update extent cache in __allocate_data_block(), finally,
    it cause extent field being inconsistent with physical block address,
    fix it.
    
    Fixes: f847c699cff3 ("f2fs: allow out-place-update for direct IO in LFS mode")
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index ab8c8f2fff70..7e9fafd44cbc 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1018,7 +1018,7 @@ static int __allocate_data_block(struct dnode_of_data *dn, int seg_type)
 	if (GET_SEGNO(sbi, old_blkaddr) != NULL_SEGNO)
 		invalidate_mapping_pages(META_MAPPING(sbi),
 					old_blkaddr, old_blkaddr);
-	f2fs_set_data_blkaddr(dn);
+	f2fs_update_data_blkaddr(dn, dn->data_blkaddr);
 
 	/*
 	 * i_size will be updated by direct_IO. Otherwise, we'll get stale

commit 00e09c0bccc71825ca9a659eb145ed7c4dc95588
Author: Chao Yu <yuchao0@huawei.com>
Date:   Fri Aug 23 17:58:36 2019 +0800

    f2fs: enhance f2fs_is_checkpoint_ready()'s readability
    
    This patch changes sematics of f2fs_is_checkpoint_ready()'s return
    value as: return true when checkpoint is ready, other return false,
    it can improve readability of below conditions.
    
    f2fs_submit_page_write()
    ...
            if (is_sbi_flag_set(sbi, SBI_IS_SHUTDOWN) ||
                                    !f2fs_is_checkpoint_ready(sbi))
                    __submit_merged_bio(io);
    
    f2fs_balance_fs()
    ...
            if (!f2fs_is_checkpoint_ready(sbi))
                    return;
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 0b3728f58d17..ab8c8f2fff70 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -634,7 +634,7 @@ void f2fs_submit_page_write(struct f2fs_io_info *fio)
 		goto next;
 out:
 	if (is_sbi_flag_set(sbi, SBI_IS_SHUTDOWN) ||
-				f2fs_is_checkpoint_ready(sbi))
+				!f2fs_is_checkpoint_ready(sbi))
 		__submit_merged_bio(io);
 	up_write(&io->io_rwsem);
 }
@@ -2570,9 +2570,10 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 
 	trace_f2fs_write_begin(inode, pos, len, flags);
 
-	err = f2fs_is_checkpoint_ready(sbi);
-	if (err)
+	if (!f2fs_is_checkpoint_ready(sbi)) {
+		err = -ENOSPC;
 		goto fail;
+	}
 
 	if ((f2fs_is_atomic_file(inode) &&
 			!f2fs_available_free_memory(sbi, INMEM_PAGES)) ||

commit b757f6edbeddd0c43135edfdee18103bd73f0991
Author: Chao Yu <yuchao0@huawei.com>
Date:   Fri Aug 23 17:58:35 2019 +0800

    f2fs: clean up __bio_alloc()'s parameter
    
    Just cleanup, no logic change.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 73ed4ff9d01c..0b3728f58d17 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -259,26 +259,25 @@ static bool __same_bdev(struct f2fs_sb_info *sbi,
 /*
  * Low-level block read/write IO operations.
  */
-static struct bio *__bio_alloc(struct f2fs_sb_info *sbi, block_t blk_addr,
-				struct writeback_control *wbc,
-				int npages, bool is_read,
-				enum page_type type, enum temp_type temp)
+static struct bio *__bio_alloc(struct f2fs_io_info *fio, int npages)
 {
+	struct f2fs_sb_info *sbi = fio->sbi;
 	struct bio *bio;
 
 	bio = f2fs_bio_alloc(sbi, npages, true);
 
-	f2fs_target_device(sbi, blk_addr, bio);
-	if (is_read) {
+	f2fs_target_device(sbi, fio->new_blkaddr, bio);
+	if (is_read_io(fio->op)) {
 		bio->bi_end_io = f2fs_read_end_io;
 		bio->bi_private = NULL;
 	} else {
 		bio->bi_end_io = f2fs_write_end_io;
 		bio->bi_private = sbi;
-		bio->bi_write_hint = f2fs_io_type_to_rw_hint(sbi, type, temp);
+		bio->bi_write_hint = f2fs_io_type_to_rw_hint(sbi,
+						fio->type, fio->temp);
 	}
-	if (wbc)
-		wbc_init_bio(wbc, bio);
+	if (fio->io_wbc)
+		wbc_init_bio(fio->io_wbc, bio);
 
 	return bio;
 }
@@ -461,8 +460,7 @@ int f2fs_submit_page_bio(struct f2fs_io_info *fio)
 	f2fs_trace_ios(fio, 0);
 
 	/* Allocate a new bio */
-	bio = __bio_alloc(fio->sbi, fio->new_blkaddr, fio->io_wbc,
-				1, is_read_io(fio->op), fio->type, fio->temp);
+	bio = __bio_alloc(fio, 1);
 
 	if (bio_add_page(bio, page, PAGE_SIZE, 0) < PAGE_SIZE) {
 		bio_put(bio);
@@ -538,8 +536,7 @@ int f2fs_merge_page_bio(struct f2fs_io_info *fio)
 	}
 alloc_new:
 	if (!bio) {
-		bio = __bio_alloc(fio->sbi, fio->new_blkaddr, fio->io_wbc,
-				BIO_MAX_PAGES, false, fio->type, fio->temp);
+		bio = __bio_alloc(fio, BIO_MAX_PAGES);
 		bio_set_op_attrs(bio, fio->op, fio->op_flags);
 	}
 
@@ -616,9 +613,7 @@ void f2fs_submit_page_write(struct f2fs_io_info *fio)
 			fio->retry = true;
 			goto skip;
 		}
-		io->bio = __bio_alloc(sbi, fio->new_blkaddr, fio->io_wbc,
-						BIO_MAX_PAGES, false,
-						fio->type, fio->temp);
+		io->bio = __bio_alloc(fio, BIO_MAX_PAGES);
 		io->fio = *fio;
 	}
 

commit 7975f3498dc0403d8177c0775b9514158ec66681
Author: Chao Yu <yuchao0@huawei.com>
Date:   Mon Jul 22 18:03:50 2019 +0800

    f2fs: support fiemap() for directory inode
    
    Adjust f2fs_fiemap() to support fiemap() on directory inode.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 5bce20005add..73ed4ff9d01c 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1539,7 +1539,7 @@ int f2fs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,
 		goto out;
 	}
 
-	if (f2fs_has_inline_data(inode)) {
+	if (f2fs_has_inline_data(inode) || f2fs_has_inline_dentry(inode)) {
 		ret = f2fs_inline_data_fiemap(inode, fieinfo, start, len);
 		if (ret != -EAGAIN)
 			goto out;

commit c72db71ed61ff51c2b8189ac9889dd18f22eb612
Author: Chao Yu <yuchao0@huawei.com>
Date:   Fri Jul 12 16:55:42 2019 +0800

    f2fs: fix panic of IO alignment feature
    
    Since 07173c3ec276 ("block: enable multipage bvecs"), one bio vector
    can store multi pages, so that we can not calculate max IO size of
    bio as PAGE_SIZE * bio->bi_max_vecs. However IO alignment feature of
    f2fs always has that assumption, so finally, it may cause panic during
    IO submission as below stack.
    
     kernel BUG at fs/f2fs/data.c:317!
     RIP: 0010:__submit_merged_bio+0x8b0/0x8c0
     Call Trace:
      f2fs_submit_page_write+0x3cd/0xdd0
      do_write_page+0x15d/0x360
      f2fs_outplace_write_data+0xd7/0x210
      f2fs_do_write_data_page+0x43b/0xf30
      __write_data_page+0xcf6/0x1140
      f2fs_write_cache_pages+0x3ba/0xb40
      f2fs_write_data_pages+0x3dd/0x8b0
      do_writepages+0xbb/0x1e0
      __writeback_single_inode+0xb6/0x800
      writeback_sb_inodes+0x441/0x910
      wb_writeback+0x261/0x650
      wb_workfn+0x1f9/0x7a0
      process_one_work+0x503/0x970
      worker_thread+0x7d/0x820
      kthread+0x1ad/0x210
      ret_from_fork+0x35/0x40
    
    This patch adds one extra condition to check left space in bio while
    trying merging page to bio, to avoid panic.
    
    This bug was reported in bugzilla:
    
    https://bugzilla.kernel.org/show_bug.cgi?id=204043
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 0686306ed988..5bce20005add 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -503,6 +503,16 @@ static bool io_is_mergeable(struct f2fs_sb_info *sbi, struct bio *bio,
 					block_t last_blkaddr,
 					block_t cur_blkaddr)
 {
+	if (F2FS_IO_ALIGNED(sbi) && (fio->type == DATA || fio->type == NODE)) {
+		unsigned int filled_blocks =
+				F2FS_BYTES_TO_BLK(bio->bi_iter.bi_size);
+		unsigned int io_size = F2FS_IO_SIZE(sbi);
+		unsigned int left_vecs = bio->bi_max_vecs - bio->bi_vcnt;
+
+		/* IOs in bio is aligned and left space of vectors is not enough */
+		if (!(filled_blocks % io_size) && left_vecs < io_size)
+			return false;
+	}
 	if (!page_is_mergeable(sbi, bio, last_blkaddr, cur_blkaddr))
 		return false;
 	return io_type_is_mergeable(io, fio);

commit 8896cbdfed0ca34452252b72d6ee97bcfca9abd2
Author: Chao Yu <yuchao0@huawei.com>
Date:   Fri Jul 12 16:55:41 2019 +0800

    f2fs: introduce {page,io}_is_mergeable() for readability
    
    Wrap merge condition into function for readability, no logic change.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index f49f243fd54f..0686306ed988 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -481,6 +481,33 @@ int f2fs_submit_page_bio(struct f2fs_io_info *fio)
 	return 0;
 }
 
+static bool page_is_mergeable(struct f2fs_sb_info *sbi, struct bio *bio,
+				block_t last_blkaddr, block_t cur_blkaddr)
+{
+	if (last_blkaddr + 1 != cur_blkaddr)
+		return false;
+	return __same_bdev(sbi, cur_blkaddr, bio);
+}
+
+static bool io_type_is_mergeable(struct f2fs_bio_info *io,
+						struct f2fs_io_info *fio)
+{
+	if (io->fio.op != fio->op)
+		return false;
+	return io->fio.op_flags == fio->op_flags;
+}
+
+static bool io_is_mergeable(struct f2fs_sb_info *sbi, struct bio *bio,
+					struct f2fs_bio_info *io,
+					struct f2fs_io_info *fio,
+					block_t last_blkaddr,
+					block_t cur_blkaddr)
+{
+	if (!page_is_mergeable(sbi, bio, last_blkaddr, cur_blkaddr))
+		return false;
+	return io_type_is_mergeable(io, fio);
+}
+
 int f2fs_merge_page_bio(struct f2fs_io_info *fio)
 {
 	struct bio *bio = *fio->bio;
@@ -494,8 +521,8 @@ int f2fs_merge_page_bio(struct f2fs_io_info *fio)
 	trace_f2fs_submit_page_bio(page, fio);
 	f2fs_trace_ios(fio, 0);
 
-	if (bio && (*fio->last_block + 1 != fio->new_blkaddr ||
-			!__same_bdev(fio->sbi, fio->new_blkaddr, bio))) {
+	if (bio && !page_is_mergeable(fio->sbi, bio, *fio->last_block,
+						fio->new_blkaddr)) {
 		__submit_bio(fio->sbi, bio, fio->type);
 		bio = NULL;
 	}
@@ -568,9 +595,8 @@ void f2fs_submit_page_write(struct f2fs_io_info *fio)
 
 	inc_page_count(sbi, WB_DATA_TYPE(bio_page));
 
-	if (io->bio && (io->last_block_in_bio != fio->new_blkaddr - 1 ||
-	    (io->fio.op != fio->op || io->fio.op_flags != fio->op_flags) ||
-			!__same_bdev(sbi, fio->new_blkaddr, io->bio)))
+	if (io->bio && !io_is_mergeable(sbi, io->bio, io, fio,
+			io->last_block_in_bio, fio->new_blkaddr))
 		__submit_merged_bio(io);
 alloc_new:
 	if (io->bio == NULL) {
@@ -1642,8 +1668,8 @@ static int f2fs_read_single_page(struct inode *inode, struct page *page,
 	 * This page will go to BIO.  Do we need to send this
 	 * BIO off first?
 	 */
-	if (bio && (*last_block_in_bio != block_nr - 1 ||
-		!__same_bdev(F2FS_I_SB(inode), block_nr, bio))) {
+	if (bio && !page_is_mergeable(F2FS_I_SB(inode), bio,
+				*last_block_in_bio, block_nr)) {
 submit_and_realloc:
 		__submit_bio(F2FS_I_SB(inode), bio, DATA);
 		bio = NULL;

commit 75a037f3604ceb781ae23167e0cdfbe5d71533d7
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed Jul 31 13:27:05 2019 -0700

    f2fs: fix livelock in swapfile writes
    
    This patch fixes livelock in the below call path when writing swap pages.
    
    [46374.617256] c2    701  __switch_to+0xe4/0x100
    [46374.617265] c2    701  __schedule+0x80c/0xbc4
    [46374.617273] c2    701  schedule+0x74/0x98
    [46374.617281] c2    701  rwsem_down_read_failed+0x190/0x234
    [46374.617291] c2    701  down_read+0x58/0x5c
    [46374.617300] c2    701  f2fs_map_blocks+0x138/0x9a8
    [46374.617310] c2    701  get_data_block_dio_write+0x74/0x104
    [46374.617320] c2    701  __blockdev_direct_IO+0x1350/0x3930
    [46374.617331] c2    701  f2fs_direct_IO+0x55c/0x8bc
    [46374.617341] c2    701  __swap_writepage+0x1d0/0x3e8
    [46374.617351] c2    701  swap_writepage+0x44/0x54
    [46374.617360] c2    701  shrink_page_list+0x140/0xe80
    [46374.617371] c2    701  shrink_inactive_list+0x510/0x918
    [46374.617381] c2    701  shrink_node_memcg+0x2d4/0x804
    [46374.617391] c2    701  shrink_node+0x10c/0x2f8
    [46374.617400] c2    701  do_try_to_free_pages+0x178/0x38c
    [46374.617410] c2    701  try_to_free_pages+0x348/0x4b8
    [46374.617419] c2    701  __alloc_pages_nodemask+0x7f8/0x1014
    [46374.617429] c2    701  pagecache_get_page+0x184/0x2cc
    [46374.617438] c2    701  f2fs_new_node_page+0x60/0x41c
    [46374.617449] c2    701  f2fs_new_inode_page+0x50/0x7c
    [46374.617460] c2    701  f2fs_init_inode_metadata+0x128/0x530
    [46374.617472] c2    701  f2fs_add_inline_entry+0x138/0xd64
    [46374.617480] c2    701  f2fs_do_add_link+0xf4/0x178
    [46374.617488] c2    701  f2fs_create+0x1e4/0x3ac
    [46374.617497] c2    701  path_openat+0xdc0/0x1308
    [46374.617507] c2    701  do_filp_open+0x78/0x124
    [46374.617516] c2    701  do_sys_open+0x134/0x248
    [46374.617525] c2    701  SyS_openat+0x14/0x20
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index abbf14e9bd72..f49f243fd54f 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1372,7 +1372,7 @@ static int get_data_block_dio_write(struct inode *inode, sector_t iblock,
 	return __get_data_block(inode, iblock, bh_result, create,
 				F2FS_GET_BLOCK_DIO, NULL,
 				f2fs_rw_hint_to_seg_type(inode->i_write_hint),
-				true);
+				IS_SWAPFILE(inode) ? false : true);
 }
 
 static int get_data_block_dio(struct inode *inode, sector_t iblock,

commit 95ae251fe82838b85c6d37e5a1775006e2a42ae0
Author: Eric Biggers <ebiggers@google.com>
Date:   Mon Jul 22 09:26:24 2019 -0700

    f2fs: add fs-verity support
    
    Add fs-verity support to f2fs.  fs-verity is a filesystem feature that
    enables transparent integrity protection and authentication of read-only
    files.  It uses a dm-verity like mechanism at the file level: a Merkle
    tree is used to verify any block in the file in log(filesize) time.  It
    is implemented mainly by helper functions in fs/verity/.  See
    Documentation/filesystems/fsverity.rst for the full documentation.
    
    The f2fs support for fs-verity consists of:
    
    - Adding a filesystem feature flag and an inode flag for fs-verity.
    
    - Implementing the fsverity_operations to support enabling verity on an
      inode and reading/writing the verity metadata.
    
    - Updating ->readpages() to verify data as it's read from verity files
      and to support reading verity metadata pages.
    
    - Updating ->write_begin(), ->write_end(), and ->writepages() to support
      writing verity metadata pages.
    
    - Calling the fs-verity hooks for ->open(), ->setattr(), and ->ioctl().
    
    Like ext4, f2fs stores the verity metadata (Merkle tree and
    fsverity_descriptor) past the end of the file, starting at the first 64K
    boundary beyond i_size.  This approach works because (a) verity files
    are readonly, and (b) pages fully beyond i_size aren't visible to
    userspace but can be read/written internally by f2fs with only some
    relatively small changes to f2fs.  Extended attributes cannot be used
    because (a) f2fs limits the total size of an inode's xattr entries to
    4096 bytes, which wouldn't be enough for even a single Merkle tree
    block, and (b) f2fs encryption doesn't encrypt xattrs, yet the verity
    metadata *must* be encrypted when the file is because it contains hashes
    of the plaintext data.
    
    Acked-by: Jaegeuk Kim <jaegeuk@kernel.org>
    Acked-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Eric Biggers <ebiggers@google.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index abbf14e9bd72..54cad80acb7d 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -74,6 +74,7 @@ static enum count_type __read_io_type(struct page *page)
 enum bio_post_read_step {
 	STEP_INITIAL = 0,
 	STEP_DECRYPT,
+	STEP_VERITY,
 };
 
 struct bio_post_read_ctx {
@@ -120,8 +121,23 @@ static void decrypt_work(struct work_struct *work)
 	bio_post_read_processing(ctx);
 }
 
+static void verity_work(struct work_struct *work)
+{
+	struct bio_post_read_ctx *ctx =
+		container_of(work, struct bio_post_read_ctx, work);
+
+	fsverity_verify_bio(ctx->bio);
+
+	bio_post_read_processing(ctx);
+}
+
 static void bio_post_read_processing(struct bio_post_read_ctx *ctx)
 {
+	/*
+	 * We use different work queues for decryption and for verity because
+	 * verity may require reading metadata pages that need decryption, and
+	 * we shouldn't recurse to the same workqueue.
+	 */
 	switch (++ctx->cur_step) {
 	case STEP_DECRYPT:
 		if (ctx->enabled_steps & (1 << STEP_DECRYPT)) {
@@ -131,6 +147,14 @@ static void bio_post_read_processing(struct bio_post_read_ctx *ctx)
 		}
 		ctx->cur_step++;
 		/* fall-through */
+	case STEP_VERITY:
+		if (ctx->enabled_steps & (1 << STEP_VERITY)) {
+			INIT_WORK(&ctx->work, verity_work);
+			fsverity_enqueue_verify_work(&ctx->work);
+			return;
+		}
+		ctx->cur_step++;
+		/* fall-through */
 	default:
 		__read_end_io(ctx->bio);
 	}
@@ -608,8 +632,15 @@ void f2fs_submit_page_write(struct f2fs_io_info *fio)
 	up_write(&io->io_rwsem);
 }
 
+static inline bool f2fs_need_verity(const struct inode *inode, pgoff_t idx)
+{
+	return fsverity_active(inode) &&
+	       idx < DIV_ROUND_UP(inode->i_size, PAGE_SIZE);
+}
+
 static struct bio *f2fs_grab_read_bio(struct inode *inode, block_t blkaddr,
-					unsigned nr_pages, unsigned op_flag)
+				      unsigned nr_pages, unsigned op_flag,
+				      pgoff_t first_idx)
 {
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
 	struct bio *bio;
@@ -625,6 +656,10 @@ static struct bio *f2fs_grab_read_bio(struct inode *inode, block_t blkaddr,
 
 	if (f2fs_encrypted_file(inode))
 		post_read_steps |= 1 << STEP_DECRYPT;
+
+	if (f2fs_need_verity(inode, first_idx))
+		post_read_steps |= 1 << STEP_VERITY;
+
 	if (post_read_steps) {
 		ctx = mempool_alloc(bio_post_read_ctx_pool, GFP_NOFS);
 		if (!ctx) {
@@ -646,7 +681,7 @@ static int f2fs_submit_page_read(struct inode *inode, struct page *page,
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
 	struct bio *bio;
 
-	bio = f2fs_grab_read_bio(inode, blkaddr, 1, 0);
+	bio = f2fs_grab_read_bio(inode, blkaddr, 1, 0, page->index);
 	if (IS_ERR(bio))
 		return PTR_ERR(bio);
 
@@ -1569,6 +1604,15 @@ int f2fs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,
 	return ret;
 }
 
+static inline loff_t f2fs_readpage_limit(struct inode *inode)
+{
+	if (IS_ENABLED(CONFIG_FS_VERITY) &&
+	    (IS_VERITY(inode) || f2fs_verity_in_progress(inode)))
+		return inode->i_sb->s_maxbytes;
+
+	return i_size_read(inode);
+}
+
 static int f2fs_read_single_page(struct inode *inode, struct page *page,
 					unsigned nr_pages,
 					struct f2fs_map_blocks *map,
@@ -1587,7 +1631,7 @@ static int f2fs_read_single_page(struct inode *inode, struct page *page,
 
 	block_in_file = (sector_t)page_index(page);
 	last_block = block_in_file + nr_pages;
-	last_block_in_file = (i_size_read(inode) + blocksize - 1) >>
+	last_block_in_file = (f2fs_readpage_limit(inode) + blocksize - 1) >>
 							blkbits;
 	if (last_block > last_block_in_file)
 		last_block = last_block_in_file;
@@ -1632,6 +1676,11 @@ static int f2fs_read_single_page(struct inode *inode, struct page *page,
 	} else {
 zero_out:
 		zero_user_segment(page, 0, PAGE_SIZE);
+		if (f2fs_need_verity(inode, page->index) &&
+		    !fsverity_verify_page(page)) {
+			ret = -EIO;
+			goto out;
+		}
 		if (!PageUptodate(page))
 			SetPageUptodate(page);
 		unlock_page(page);
@@ -1650,7 +1699,7 @@ static int f2fs_read_single_page(struct inode *inode, struct page *page,
 	}
 	if (bio == NULL) {
 		bio = f2fs_grab_read_bio(inode, block_nr, nr_pages,
-				is_readahead ? REQ_RAHEAD : 0);
+				is_readahead ? REQ_RAHEAD : 0, page->index);
 		if (IS_ERR(bio)) {
 			ret = PTR_ERR(bio);
 			bio = NULL;
@@ -2052,7 +2101,7 @@ static int __write_data_page(struct page *page, bool *submitted,
 	if (unlikely(is_sbi_flag_set(sbi, SBI_POR_DOING)))
 		goto redirty_out;
 
-	if (page->index < end_index)
+	if (page->index < end_index || f2fs_verity_in_progress(inode))
 		goto write;
 
 	/*
@@ -2427,7 +2476,8 @@ static void f2fs_write_failed(struct address_space *mapping, loff_t to)
 	struct inode *inode = mapping->host;
 	loff_t i_size = i_size_read(inode);
 
-	if (to > i_size) {
+	/* In the fs-verity case, f2fs_end_enable_verity() does the truncate */
+	if (to > i_size && !f2fs_verity_in_progress(inode)) {
 		down_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
 		down_write(&F2FS_I(inode)->i_mmap_sem);
 
@@ -2458,7 +2508,8 @@ static int prepare_write_begin(struct f2fs_sb_info *sbi,
 	 * the block addresses when there is no need to fill the page.
 	 */
 	if (!f2fs_has_inline_data(inode) && len == PAGE_SIZE &&
-			!is_inode_flag_set(inode, FI_NO_PREALLOC))
+	    !is_inode_flag_set(inode, FI_NO_PREALLOC) &&
+	    !f2fs_verity_in_progress(inode))
 		return 0;
 
 	/* f2fs_lock_op avoids race between write CP and convert_inline_page */
@@ -2597,7 +2648,8 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 	if (len == PAGE_SIZE || PageUptodate(page))
 		return 0;
 
-	if (!(pos & (PAGE_SIZE - 1)) && (pos + len) >= i_size_read(inode)) {
+	if (!(pos & (PAGE_SIZE - 1)) && (pos + len) >= i_size_read(inode) &&
+	    !f2fs_verity_in_progress(inode)) {
 		zero_user_segment(page, len, PAGE_SIZE);
 		return 0;
 	}
@@ -2660,7 +2712,8 @@ static int f2fs_write_end(struct file *file,
 
 	set_page_dirty(page);
 
-	if (pos + copied > i_size_read(inode))
+	if (pos + copied > i_size_read(inode) &&
+	    !f2fs_verity_in_progress(inode))
 		f2fs_i_size_write(inode, pos + copied);
 unlock_out:
 	f2fs_put_page(page, 1);
@@ -3104,7 +3157,9 @@ void f2fs_clear_page_cache_dirty_tag(struct page *page)
 
 int __init f2fs_init_post_read_processing(void)
 {
-	bio_post_read_ctx_cache = KMEM_CACHE(bio_post_read_ctx, 0);
+	bio_post_read_ctx_cache =
+		kmem_cache_create("f2fs_bio_post_read_ctx",
+				  sizeof(struct bio_post_read_ctx), 0, 0, NULL);
 	if (!bio_post_read_ctx_cache)
 		goto fail;
 	bio_post_read_ctx_pool =

commit 371096949f0ad3950b06729989bd27de51b8c5f5
Author: Keith Busch <keith.busch@intel.com>
Date:   Thu Jul 18 15:58:46 2019 -0700

    mm: migrate: remove unused mode argument
    
    migrate_page_move_mapping() doesn't use the mode argument.  Remove it
    and update callers accordingly.
    
    Link: http://lkml.kernel.org/r/20190508210301.8472-1-keith.busch@intel.com
    Signed-off-by: Keith Busch <keith.busch@intel.com>
    Reviewed-by: Zi Yan <ziy@nvidia.com>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 4eb2f3920140..abbf14e9bd72 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -2919,7 +2919,7 @@ int f2fs_migrate_page(struct address_space *mapping,
 	/* one extra reference was held for atomic_write page */
 	extra_count = atomic_written ? 1 : 0;
 	rc = migrate_page_move_mapping(mapping, newpage,
-				page, mode, extra_count);
+				page, extra_count);
 	if (rc != MIGRATEPAGE_SUCCESS) {
 		if (atomic_written)
 			mutex_unlock(&fi->inmem_lock);

commit 9637d517347e80ee2fe1c5d8ce45ba1b88d8b5cd
Merge: 273cbf61c3dd 787c79d6393f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 15 21:20:52 2019 -0700

    Merge tag 'for-linus-20190715' of git://git.kernel.dk/linux-block
    
    Pull more block updates from Jens Axboe:
     "A later pull request with some followup items. I had some vacation
      coming up to the merge window, so certain things items were delayed a
      bit. This pull request also contains fixes that came in within the
      last few days of the merge window, which I didn't want to push right
      before sending you a pull request.
    
      This contains:
    
       - NVMe pull request, mostly fixes, but also a few minor items on the
         feature side that were timing constrained (Christoph et al)
    
       - Report zones fixes (Damien)
    
       - Removal of dead code (Damien)
    
       - Turn on cgroup psi memstall (Josef)
    
       - block cgroup MAINTAINERS entry (Konstantin)
    
       - Flush init fix (Josef)
    
       - blk-throttle low iops timing fix (Konstantin)
    
       - nbd resize fixes (Mike)
    
       - nbd 0 blocksize crash fix (Xiubo)
    
       - block integrity error leak fix (Wenwen)
    
       - blk-cgroup writeback and priority inheritance fixes (Tejun)"
    
    * tag 'for-linus-20190715' of git://git.kernel.dk/linux-block: (42 commits)
      MAINTAINERS: add entry for block io cgroup
      null_blk: fixup ->report_zones() for !CONFIG_BLK_DEV_ZONED
      block: Limit zone array allocation size
      sd_zbc: Fix report zones buffer allocation
      block: Kill gfp_t argument of blkdev_report_zones()
      block: Allow mapping of vmalloc-ed buffers
      block/bio-integrity: fix a memory leak bug
      nvme: fix NULL deref for fabrics options
      nbd: add netlink reconfigure resize support
      nbd: fix crash when the blksize is zero
      block: Disable write plugging for zoned block devices
      block: Fix elevator name declaration
      block: Remove unused definitions
      nvme: fix regression upon hot device removal and insertion
      blk-throttle: fix zero wait time for iops throttled group
      block: Fix potential overflow in blk_report_zones()
      blkcg: implement REQ_CGROUP_PUNT
      blkcg, writeback: Implement wbc_blkcg_css()
      blkcg, writeback: Add wbc->no_cgroup_owner
      blkcg, writeback: Rename wbc_account_io() to wbc_account_cgroup_owner()
      ...

commit a641a88e5d6864f20b2608cb01165c756794e645
Merge: 4ce9d181ebe5 2d008835ec2f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jul 12 17:28:24 2019 -0700

    Merge tag 'f2fs-for-5.3' of git://git.kernel.org/pub/scm/linux/kernel/git/jaegeuk/f2fs
    
    Pull f2fs updates from Jaegeuk Kim:
     "In this round, we've introduced native swap file support which can
      exploit DIO, enhanced existing checkpoint=disable feature with
      additional mount option to tune the triggering condition, and allowed
      user to preallocate physical blocks in a pinned file which will be
      useful to avoid f2fs fragmentation in append-only workloads. In
      addition, we've fixed subtle quota corruption issue.
    
      Enhancements:
       - add swap file support which uses DIO
       - allocate blocks for pinned file
       - allow SSR and mount option to enhance checkpoint=disable
       - enhance IPU IOs
       - add more sanity checks such as memory boundary access
    
      Bug fixes:
       - quota corruption in very corner case of error-injected SPO case
       - fix root_reserved on remount and some wrong counts
       - add missing fsck flag
    
      Some patches were also introduced to clean up ambiguous i_flags and
      debugging messages codes"
    
    * tag 'f2fs-for-5.3' of git://git.kernel.org/pub/scm/linux/kernel/git/jaegeuk/f2fs: (33 commits)
      f2fs: improve print log in f2fs_sanity_check_ckpt()
      f2fs: avoid out-of-range memory access
      f2fs: fix to avoid long latency during umount
      f2fs: allow all the users to pin a file
      f2fs: support swap file w/ DIO
      f2fs: allocate blocks for pinned file
      f2fs: fix is_idle() check for discard type
      f2fs: add a rw_sem to cover quota flag changes
      f2fs: set SBI_NEED_FSCK for xattr corruption case
      f2fs: use generic EFSBADCRC/EFSCORRUPTED
      f2fs: Use DIV_ROUND_UP() instead of open-coding
      f2fs: print kernel message if filesystem is inconsistent
      f2fs: introduce f2fs_<level> macros to wrap f2fs_printk()
      f2fs: avoid get_valid_blocks() for cleanup
      f2fs: ioctl for removing a range from F2FS
      f2fs: only set project inherit bit for directory
      f2fs: separate f2fs i_flags from fs_flags and ext4 i_flags
      f2fs: replace ktype default_attrs with default_groups
      f2fs: Add option to limit required GC for checkpoint=disable
      f2fs: Fix accounting for unusable blocks
      ...

commit 34e51a5e1a6e939ed7d99c38173821ab86d577f4
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Jun 27 13:39:49 2019 -0700

    blkcg, writeback: Rename wbc_account_io() to wbc_account_cgroup_owner()
    
    wbc_account_io() does a very specific job - try to see which cgroup is
    actually dirtying an inode and transfer its ownership to the majority
    dirtier if needed.  The name is too generic and confusing.  Let's
    rename it to something more specific.
    
    Reviewed-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index eda4181d2092..e1cab1717ac7 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -470,7 +470,7 @@ int f2fs_submit_page_bio(struct f2fs_io_info *fio)
 	}
 
 	if (fio->io_wbc && !is_read_io(fio->op))
-		wbc_account_io(fio->io_wbc, page, PAGE_SIZE);
+		wbc_account_cgroup_owner(fio->io_wbc, page, PAGE_SIZE);
 
 	bio_set_op_attrs(bio, fio->op, fio->op_flags);
 
@@ -537,7 +537,7 @@ void f2fs_submit_page_write(struct f2fs_io_info *fio)
 	}
 
 	if (fio->io_wbc)
-		wbc_account_io(fio->io_wbc, bio_page, PAGE_SIZE);
+		wbc_account_cgroup_owner(fio->io_wbc, bio_page, PAGE_SIZE);
 
 	io->last_block_in_bio = fio->new_blkaddr;
 	f2fs_trace_ios(fio, 0);

commit 4969c06a0d83c9c3dc50b8efcdc8eeedfce896f6
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Mon Jul 1 19:15:29 2019 -0700

    f2fs: support swap file w/ DIO
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 1e2d924e2ea7..6a8db4abdf5f 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -14,6 +14,7 @@
 #include <linux/pagevec.h>
 #include <linux/blkdev.h>
 #include <linux/bio.h>
+#include <linux/swap.h>
 #include <linux/prefetch.h>
 #include <linux/uio.h>
 #include <linux/cleancache.h>
@@ -54,7 +55,7 @@ static bool __is_cp_guaranteed(struct page *page)
 
 static enum count_type __read_io_type(struct page *page)
 {
-	struct address_space *mapping = page->mapping;
+	struct address_space *mapping = page_file_mapping(page);
 
 	if (mapping) {
 		struct inode *inode = mapping->host;
@@ -1585,7 +1586,7 @@ static int f2fs_read_single_page(struct inode *inode, struct page *page,
 	sector_t block_nr;
 	int ret = 0;
 
-	block_in_file = (sector_t)page->index;
+	block_in_file = (sector_t)page_index(page);
 	last_block = block_in_file + nr_pages;
 	last_block_in_file = (i_size_read(inode) + blocksize - 1) >>
 							blkbits;
@@ -1618,7 +1619,8 @@ static int f2fs_read_single_page(struct inode *inode, struct page *page,
 		block_nr = map->m_pblk + block_in_file - map->m_lblk;
 		SetPageMappedToDisk(page);
 
-		if (!PageUptodate(page) && !cleancache_get_page(page)) {
+		if (!PageUptodate(page) && (!PageSwapCache(page) &&
+					!cleancache_get_page(page))) {
 			SetPageUptodate(page);
 			goto confused;
 		}
@@ -1716,7 +1718,7 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 			prefetchw(&page->flags);
 			list_del(&page->lru);
 			if (add_to_page_cache_lru(page, mapping,
-						  page->index,
+						  page_index(page),
 						  readahead_gfp_mask(mapping)))
 				goto next_page;
 		}
@@ -1740,7 +1742,7 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 
 static int f2fs_read_data_page(struct file *file, struct page *page)
 {
-	struct inode *inode = page->mapping->host;
+	struct inode *inode = page_file_mapping(page)->host;
 	int ret = -EAGAIN;
 
 	trace_f2fs_readpage(page, DATA);
@@ -1749,7 +1751,8 @@ static int f2fs_read_data_page(struct file *file, struct page *page)
 	if (f2fs_has_inline_data(inode))
 		ret = f2fs_read_inline_data(inode, page);
 	if (ret == -EAGAIN)
-		ret = f2fs_mpage_readpages(page->mapping, NULL, page, 1, false);
+		ret = f2fs_mpage_readpages(page_file_mapping(page),
+						NULL, page, 1, false);
 	return ret;
 }
 
@@ -2851,13 +2854,14 @@ int f2fs_release_page(struct page *page, gfp_t wait)
 
 static int f2fs_set_data_page_dirty(struct page *page)
 {
-	struct address_space *mapping = page->mapping;
-	struct inode *inode = mapping->host;
+	struct inode *inode = page_file_mapping(page)->host;
 
 	trace_f2fs_set_page_dirty(page, DATA);
 
 	if (!PageUptodate(page))
 		SetPageUptodate(page);
+	if (PageSwapCache(page))
+		return __set_page_dirty_nobuffers(page);
 
 	if (f2fs_is_atomic_file(inode) && !f2fs_is_commit_atomic_write(inode)) {
 		if (!IS_ATOMIC_WRITTEN_PAGE(page)) {
@@ -2949,6 +2953,126 @@ int f2fs_migrate_page(struct address_space *mapping,
 }
 #endif
 
+#ifdef CONFIG_SWAP
+/* Copied from generic_swapfile_activate() to check any holes */
+static int check_swap_activate(struct file *swap_file, unsigned int max)
+{
+	struct address_space *mapping = swap_file->f_mapping;
+	struct inode *inode = mapping->host;
+	unsigned blocks_per_page;
+	unsigned long page_no;
+	unsigned blkbits;
+	sector_t probe_block;
+	sector_t last_block;
+	sector_t lowest_block = -1;
+	sector_t highest_block = 0;
+
+	blkbits = inode->i_blkbits;
+	blocks_per_page = PAGE_SIZE >> blkbits;
+
+	/*
+	 * Map all the blocks into the extent list.  This code doesn't try
+	 * to be very smart.
+	 */
+	probe_block = 0;
+	page_no = 0;
+	last_block = i_size_read(inode) >> blkbits;
+	while ((probe_block + blocks_per_page) <= last_block && page_no < max) {
+		unsigned block_in_page;
+		sector_t first_block;
+
+		cond_resched();
+
+		first_block = bmap(inode, probe_block);
+		if (first_block == 0)
+			goto bad_bmap;
+
+		/*
+		 * It must be PAGE_SIZE aligned on-disk
+		 */
+		if (first_block & (blocks_per_page - 1)) {
+			probe_block++;
+			goto reprobe;
+		}
+
+		for (block_in_page = 1; block_in_page < blocks_per_page;
+					block_in_page++) {
+			sector_t block;
+
+			block = bmap(inode, probe_block + block_in_page);
+			if (block == 0)
+				goto bad_bmap;
+			if (block != first_block + block_in_page) {
+				/* Discontiguity */
+				probe_block++;
+				goto reprobe;
+			}
+		}
+
+		first_block >>= (PAGE_SHIFT - blkbits);
+		if (page_no) {	/* exclude the header page */
+			if (first_block < lowest_block)
+				lowest_block = first_block;
+			if (first_block > highest_block)
+				highest_block = first_block;
+		}
+
+		page_no++;
+		probe_block += blocks_per_page;
+reprobe:
+		continue;
+	}
+	return 0;
+
+bad_bmap:
+	pr_err("swapon: swapfile has holes\n");
+	return -EINVAL;
+}
+
+static int f2fs_swap_activate(struct swap_info_struct *sis, struct file *file,
+				sector_t *span)
+{
+	struct inode *inode = file_inode(file);
+	int ret;
+
+	if (!S_ISREG(inode->i_mode))
+		return -EINVAL;
+
+	if (f2fs_readonly(F2FS_I_SB(inode)->sb))
+		return -EROFS;
+
+	ret = f2fs_convert_inline_inode(inode);
+	if (ret)
+		return ret;
+
+	ret = check_swap_activate(file, sis->max);
+	if (ret)
+		return ret;
+
+	set_inode_flag(inode, FI_PIN_FILE);
+	f2fs_precache_extents(inode);
+	f2fs_update_time(F2FS_I_SB(inode), REQ_TIME);
+	return 0;
+}
+
+static void f2fs_swap_deactivate(struct file *file)
+{
+	struct inode *inode = file_inode(file);
+
+	clear_inode_flag(inode, FI_PIN_FILE);
+}
+#else
+static int f2fs_swap_activate(struct swap_info_struct *sis, struct file *file,
+				sector_t *span)
+{
+	return -EOPNOTSUPP;
+}
+
+static void f2fs_swap_deactivate(struct file *file)
+{
+}
+#endif
+
 const struct address_space_operations f2fs_dblock_aops = {
 	.readpage	= f2fs_read_data_page,
 	.readpages	= f2fs_read_data_pages,
@@ -2961,6 +3085,8 @@ const struct address_space_operations f2fs_dblock_aops = {
 	.releasepage	= f2fs_release_page,
 	.direct_IO	= f2fs_direct_IO,
 	.bmap		= f2fs_bmap,
+	.swap_activate  = f2fs_swap_activate,
+	.swap_deactivate = f2fs_swap_deactivate,
 #ifdef CONFIG_MIGRATION
 	.migratepage    = f2fs_migrate_page,
 #endif

commit 10f966bbf521bb9b2e497bbca496a5141f4071d0
Author: Chao Yu <yuchao0@huawei.com>
Date:   Thu Jun 20 11:36:14 2019 +0800

    f2fs: use generic EFSBADCRC/EFSCORRUPTED
    
    f2fs uses EFAULT as error number to indicate filesystem is corrupted
    all the time, but generic filesystems use EUCLEAN for such condition,
    we need to change to follow others.
    
    This patch adds two new macros as below to wrap more generic error
    code macros, and spread them in code.
    
    EFSBADCRC       EBADMSG         /* Bad CRC detected */
    EFSCORRUPTED    EUCLEAN         /* Filesystem is corrupted */
    
    Reported-by: Pavel Machek <pavel@ucw.cz>
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Acked-by: Pavel Machek <pavel@ucw.cz>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index f4e1672bd96e..1e2d924e2ea7 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -455,7 +455,7 @@ int f2fs_submit_page_bio(struct f2fs_io_info *fio)
 	if (!f2fs_is_valid_blkaddr(fio->sbi, fio->new_blkaddr,
 			fio->is_por ? META_POR : (__is_meta_io(fio) ?
 			META_GENERIC : DATA_GENERIC_ENHANCE)))
-		return -EFAULT;
+		return -EFSCORRUPTED;
 
 	trace_f2fs_submit_page_bio(page, fio);
 	f2fs_trace_ios(fio, 0);
@@ -489,7 +489,7 @@ int f2fs_merge_page_bio(struct f2fs_io_info *fio)
 
 	if (!f2fs_is_valid_blkaddr(fio->sbi, fio->new_blkaddr,
 			__is_meta_io(fio) ? META_GENERIC : DATA_GENERIC))
-		return -EFAULT;
+		return -EFSCORRUPTED;
 
 	trace_f2fs_submit_page_bio(page, fio);
 	f2fs_trace_ios(fio, 0);
@@ -789,7 +789,7 @@ struct page *f2fs_get_read_data_page(struct inode *inode, pgoff_t index,
 		dn.data_blkaddr = ei.blk + index - ei.fofs;
 		if (!f2fs_is_valid_blkaddr(F2FS_I_SB(inode), dn.data_blkaddr,
 						DATA_GENERIC_ENHANCE_READ)) {
-			err = -EFAULT;
+			err = -EFSCORRUPTED;
 			goto put_err;
 		}
 		goto got_it;
@@ -809,7 +809,7 @@ struct page *f2fs_get_read_data_page(struct inode *inode, pgoff_t index,
 			!f2fs_is_valid_blkaddr(F2FS_I_SB(inode),
 						dn.data_blkaddr,
 						DATA_GENERIC_ENHANCE)) {
-		err = -EFAULT;
+		err = -EFSCORRUPTED;
 		goto put_err;
 	}
 got_it:
@@ -1155,7 +1155,7 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 
 	if (__is_valid_data_blkaddr(blkaddr) &&
 		!f2fs_is_valid_blkaddr(sbi, blkaddr, DATA_GENERIC_ENHANCE)) {
-		err = -EFAULT;
+		err = -EFSCORRUPTED;
 		goto sync_out;
 	}
 
@@ -1625,7 +1625,7 @@ static int f2fs_read_single_page(struct inode *inode, struct page *page,
 
 		if (!f2fs_is_valid_blkaddr(F2FS_I_SB(inode), block_nr,
 						DATA_GENERIC_ENHANCE_READ)) {
-			ret = -EFAULT;
+			ret = -EFSCORRUPTED;
 			goto out;
 		}
 	} else {
@@ -1906,7 +1906,7 @@ int f2fs_do_write_data_page(struct f2fs_io_info *fio)
 
 		if (!f2fs_is_valid_blkaddr(fio->sbi, fio->old_blkaddr,
 						DATA_GENERIC_ENHANCE))
-			return -EFAULT;
+			return -EFSCORRUPTED;
 
 		ipu_force = true;
 		fio->need_lock = LOCK_DONE;
@@ -1933,7 +1933,7 @@ int f2fs_do_write_data_page(struct f2fs_io_info *fio)
 	if (__is_valid_data_blkaddr(fio->old_blkaddr) &&
 		!f2fs_is_valid_blkaddr(fio->sbi, fio->old_blkaddr,
 						DATA_GENERIC_ENHANCE)) {
-		err = -EFAULT;
+		err = -EFSCORRUPTED;
 		goto out_writepage;
 	}
 	/*
@@ -2606,7 +2606,7 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 	} else {
 		if (!f2fs_is_valid_blkaddr(sbi, blkaddr,
 				DATA_GENERIC_ENHANCE_READ)) {
-			err = -EFAULT;
+			err = -EFSCORRUPTED;
 			goto fail;
 		}
 		err = f2fs_submit_page_read(inode, page, blkaddr);

commit 53bc1d854c64c20d967dab15b111baca02a6d99e
Author: Eric Biggers <ebiggers@google.com>
Date:   Mon May 20 09:29:44 2019 -0700

    fscrypt: support encrypting multiple filesystem blocks per page
    
    Rename fscrypt_encrypt_page() to fscrypt_encrypt_pagecache_blocks() and
    redefine its behavior to encrypt all filesystem blocks from the given
    region of the given page, rather than assuming that the region consists
    of just one filesystem block.  Also remove the 'inode' and 'lblk_num'
    parameters, since they can be retrieved from the page as it's already
    assumed to be a pagecache page.
    
    This is in preparation for allowing encryption on ext4 filesystems with
    blocksize != PAGE_SIZE.
    
    This is based on work by Chandan Rajendra.
    
    Reviewed-by: Chandan Rajendra <chandan@linux.ibm.com>
    Signed-off-by: Eric Biggers <ebiggers@google.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 968ebdbcb583..a546ac8685ea 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1726,8 +1726,9 @@ static int encrypt_one_page(struct f2fs_io_info *fio)
 	f2fs_wait_on_block_writeback(inode, fio->old_blkaddr);
 
 retry_encrypt:
-	fio->encrypted_page = fscrypt_encrypt_page(inode, fio->page,
-			PAGE_SIZE, 0, fio->page->index, gfp_flags);
+	fio->encrypted_page = fscrypt_encrypt_pagecache_blocks(fio->page,
+							       PAGE_SIZE, 0,
+							       gfp_flags);
 	if (IS_ERR(fio->encrypted_page)) {
 		/* flush pending IOs and wait for a while in the ENOMEM case */
 		if (PTR_ERR(fio->encrypted_page) == -ENOMEM) {

commit d2d0727b1654e11563f181f4d3d48b9275514480
Author: Eric Biggers <ebiggers@google.com>
Date:   Mon May 20 09:29:39 2019 -0700

    fscrypt: simplify bounce page handling
    
    Currently, bounce page handling for writes to encrypted files is
    unnecessarily complicated.  A fscrypt_ctx is allocated along with each
    bounce page, page_private(bounce_page) points to this fscrypt_ctx, and
    fscrypt_ctx::w::control_page points to the original pagecache page.
    
    However, because writes don't use the fscrypt_ctx for anything else,
    there's no reason why page_private(bounce_page) can't just point to the
    original pagecache page directly.
    
    Therefore, this patch makes this change.  In the process, it also cleans
    up the API exposed to filesystems that allows testing whether a page is
    a bounce page, getting the pagecache page from a bounce page, and
    freeing a bounce page.
    
    Reviewed-by: Chandan Rajendra <chandan@linux.ibm.com>
    Signed-off-by: Eric Biggers <ebiggers@google.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index eda4181d2092..968ebdbcb583 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -185,7 +185,7 @@ static void f2fs_write_end_io(struct bio *bio)
 			continue;
 		}
 
-		fscrypt_pullback_bio_page(&page, true);
+		fscrypt_finalize_bounce_page(&page);
 
 		if (unlikely(bio->bi_status)) {
 			mapping_set_error(page->mapping, -EIO);
@@ -362,10 +362,9 @@ static bool __has_merged_page(struct f2fs_bio_info *io, struct inode *inode,
 
 	bio_for_each_segment_all(bvec, io->bio, iter_all) {
 
-		if (bvec->bv_page->mapping)
-			target = bvec->bv_page;
-		else
-			target = fscrypt_control_page(bvec->bv_page);
+		target = bvec->bv_page;
+		if (fscrypt_is_bounce_page(target))
+			target = fscrypt_pagecache_page(target);
 
 		if (inode && inode == target->mapping->host)
 			return true;
@@ -1900,8 +1899,7 @@ int f2fs_do_write_data_page(struct f2fs_io_info *fio)
 		err = f2fs_inplace_write_data(fio);
 		if (err) {
 			if (f2fs_encrypted_file(inode))
-				fscrypt_pullback_bio_page(&fio->encrypted_page,
-									true);
+				fscrypt_finalize_bounce_page(&fio->encrypted_page);
 			if (PageWriteback(page))
 				end_page_writeback(page);
 		} else {

commit 040d2bb318d1aea4f28cc22504b44e446666c86e
Author: Chao Yu <yuchao0@huawei.com>
Date:   Mon May 20 17:36:59 2019 +0800

    f2fs: fix to avoid deadloop if data_flush is on
    
    As Hagbard Celine reported:
    
    [  615.697824] INFO: task kworker/u16:5:344 blocked for more than 120 seconds.
    [  615.697825]       Not tainted 5.0.15-gentoo-f2fslog #4
    [  615.697826] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs"
    disables this message.
    [  615.697827] kworker/u16:5   D    0   344      2 0x80000000
    [  615.697831] Workqueue: writeback wb_workfn (flush-259:0)
    [  615.697832] Call Trace:
    [  615.697836]  ? __schedule+0x2c5/0x8b0
    [  615.697839]  schedule+0x32/0x80
    [  615.697841]  schedule_preempt_disabled+0x14/0x20
    [  615.697842]  __mutex_lock.isra.8+0x2ba/0x4d0
    [  615.697845]  ? log_store+0xf5/0x260
    [  615.697848]  f2fs_write_data_pages+0x133/0x320
    [  615.697851]  ? trace_hardirqs_on+0x2c/0xe0
    [  615.697854]  do_writepages+0x41/0xd0
    [  615.697857]  __filemap_fdatawrite_range+0x81/0xb0
    [  615.697859]  f2fs_sync_dirty_inodes+0x1dd/0x200
    [  615.697861]  f2fs_balance_fs_bg+0x2a7/0x2c0
    [  615.697863]  ? up_read+0x5/0x20
    [  615.697865]  ? f2fs_do_write_data_page+0x2cb/0x940
    [  615.697867]  f2fs_balance_fs+0xe5/0x2c0
    [  615.697869]  __write_data_page+0x1c8/0x6e0
    [  615.697873]  f2fs_write_cache_pages+0x1e0/0x450
    [  615.697878]  f2fs_write_data_pages+0x14b/0x320
    [  615.697880]  ? trace_hardirqs_on+0x2c/0xe0
    [  615.697883]  do_writepages+0x41/0xd0
    [  615.697885]  __filemap_fdatawrite_range+0x81/0xb0
    [  615.697887]  f2fs_sync_dirty_inodes+0x1dd/0x200
    [  615.697889]  f2fs_balance_fs_bg+0x2a7/0x2c0
    [  615.697891]  f2fs_write_node_pages+0x51/0x220
    [  615.697894]  do_writepages+0x41/0xd0
    [  615.697897]  __writeback_single_inode+0x3d/0x3d0
    [  615.697899]  writeback_sb_inodes+0x1e8/0x410
    [  615.697902]  __writeback_inodes_wb+0x5d/0xb0
    [  615.697904]  wb_writeback+0x28f/0x340
    [  615.697906]  ? cpumask_next+0x16/0x20
    [  615.697908]  wb_workfn+0x33e/0x420
    [  615.697911]  process_one_work+0x1a1/0x3d0
    [  615.697913]  worker_thread+0x30/0x380
    [  615.697915]  ? process_one_work+0x3d0/0x3d0
    [  615.697916]  kthread+0x116/0x130
    [  615.697918]  ? kthread_create_worker_on_cpu+0x70/0x70
    [  615.697921]  ret_from_fork+0x3a/0x50
    
    There is still deadloop in below condition:
    
    d A
    - do_writepages
     - f2fs_write_node_pages
      - f2fs_balance_fs_bg
       - f2fs_sync_dirty_inodes
        - f2fs_write_cache_pages
         - mutex_lock(&sbi->writepages)     -- lock once
         - __write_data_page
          - f2fs_balance_fs_bg
           - f2fs_sync_dirty_inodes
            - f2fs_write_data_pages
             - mutex_lock(&sbi->writepages) -- lock again
    
    Thread A                        Thread B
    - do_writepages
     - f2fs_write_node_pages
      - f2fs_balance_fs_bg
       - f2fs_sync_dirty_inodes
        - .cp_task = current
                                    - f2fs_sync_dirty_inodes
                                     - .cp_task = current
                                     - filemap_fdatawrite
                                     - .cp_task = NULL
        - filemap_fdatawrite
         - f2fs_write_cache_pages
          - enter f2fs_balance_fs_bg since .cp_task is NULL
        - .cp_task = NULL
    
    Change as below to avoid this:
    - add condition to avoid holding .writepages mutex lock in path
    of data flush
    - introduce mutex lock sbi.flush_lock to exclude concurrent data
    flush in background.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 03ea11f8a3c3..f4e1672bd96e 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -2332,6 +2332,9 @@ static inline bool __should_serialize_io(struct inode *inode,
 		return false;
 	if (IS_NOQUOTA(inode))
 		return false;
+	/* to avoid deadlock in path of data flush */
+	if (F2FS_I(inode)->cp_task)
+		return false;
 	if (wbc->sync_mode != WB_SYNC_ALL)
 		return true;
 	if (get_dirty_pages(inode) >= SM_I(F2FS_I_SB(inode))->min_seq_blocks)

commit 8648de2c581eeda7e412d6e38bf19e25bbb795ba
Author: Chao Yu <yuchao0@huawei.com>
Date:   Tue Feb 19 16:15:29 2019 +0800

    f2fs: add bio cache for IPU
    
    SQLite in Wal mode may trigger sequential IPU write in db-wal file, after
    commit d1b3e72d5490 ("f2fs: submit bio of in-place-update pages"), we
    lost the chance of merging page in inner managed bio cache, result in
    submitting more small-sized IO.
    
    So let's add temporary bio in writepages() to cache mergeable write IO as
    much as possible.
    
    Test case:
    1. xfs_io -f /mnt/f2fs/file -c "pwrite 0 65536" -c "fsync"
    2. xfs_io -f /mnt/f2fs/file -c "pwrite 0 65536" -c "fsync"
    
    Before:
    f2fs_submit_write_bio: dev = (251,0)/(251,0), rw = WRITE(S), DATA, sector = 65544, size = 4096
    f2fs_submit_write_bio: dev = (251,0)/(251,0), rw = WRITE(S), DATA, sector = 65552, size = 4096
    f2fs_submit_write_bio: dev = (251,0)/(251,0), rw = WRITE(S), DATA, sector = 65560, size = 4096
    f2fs_submit_write_bio: dev = (251,0)/(251,0), rw = WRITE(S), DATA, sector = 65568, size = 4096
    f2fs_submit_write_bio: dev = (251,0)/(251,0), rw = WRITE(S), DATA, sector = 65576, size = 4096
    f2fs_submit_write_bio: dev = (251,0)/(251,0), rw = WRITE(S), DATA, sector = 65584, size = 4096
    f2fs_submit_write_bio: dev = (251,0)/(251,0), rw = WRITE(S), DATA, sector = 65592, size = 4096
    f2fs_submit_write_bio: dev = (251,0)/(251,0), rw = WRITE(S), DATA, sector = 65600, size = 4096
    f2fs_submit_write_bio: dev = (251,0)/(251,0), rw = WRITE(S), DATA, sector = 65608, size = 4096
    f2fs_submit_write_bio: dev = (251,0)/(251,0), rw = WRITE(S), DATA, sector = 65616, size = 4096
    f2fs_submit_write_bio: dev = (251,0)/(251,0), rw = WRITE(S), DATA, sector = 65624, size = 4096
    f2fs_submit_write_bio: dev = (251,0)/(251,0), rw = WRITE(S), DATA, sector = 65632, size = 4096
    f2fs_submit_write_bio: dev = (251,0)/(251,0), rw = WRITE(S), DATA, sector = 65640, size = 4096
    f2fs_submit_write_bio: dev = (251,0)/(251,0), rw = WRITE(S), DATA, sector = 65648, size = 4096
    f2fs_submit_write_bio: dev = (251,0)/(251,0), rw = WRITE(S), DATA, sector = 65656, size = 4096
    f2fs_submit_write_bio: dev = (251,0)/(251,0), rw = WRITE(S), DATA, sector = 65664, size = 4096
    f2fs_submit_write_bio: dev = (251,0)/(251,0), rw = WRITE(S), NODE, sector = 57352, size = 4096
    
    After:
    f2fs_submit_write_bio: dev = (251,0)/(251,0), rw = WRITE(S), DATA, sector = 65544, size = 65536
    f2fs_submit_write_bio: dev = (251,0)/(251,0), rw = WRITE(S), NODE, sector = 57368, size = 4096
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index eda4181d2092..03ea11f8a3c3 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -347,20 +347,20 @@ static void __submit_merged_bio(struct f2fs_bio_info *io)
 	io->bio = NULL;
 }
 
-static bool __has_merged_page(struct f2fs_bio_info *io, struct inode *inode,
+static bool __has_merged_page(struct bio *bio, struct inode *inode,
 						struct page *page, nid_t ino)
 {
 	struct bio_vec *bvec;
 	struct page *target;
 	struct bvec_iter_all iter_all;
 
-	if (!io->bio)
+	if (!bio)
 		return false;
 
 	if (!inode && !page && !ino)
 		return true;
 
-	bio_for_each_segment_all(bvec, io->bio, iter_all) {
+	bio_for_each_segment_all(bvec, bio, iter_all) {
 
 		if (bvec->bv_page->mapping)
 			target = bvec->bv_page;
@@ -411,7 +411,7 @@ static void __submit_merged_write_cond(struct f2fs_sb_info *sbi,
 			struct f2fs_bio_info *io = sbi->write_io[btype] + temp;
 
 			down_read(&io->io_rwsem);
-			ret = __has_merged_page(io, inode, page, ino);
+			ret = __has_merged_page(io->bio, inode, page, ino);
 			up_read(&io->io_rwsem);
 		}
 		if (ret)
@@ -481,6 +481,61 @@ int f2fs_submit_page_bio(struct f2fs_io_info *fio)
 	return 0;
 }
 
+int f2fs_merge_page_bio(struct f2fs_io_info *fio)
+{
+	struct bio *bio = *fio->bio;
+	struct page *page = fio->encrypted_page ?
+			fio->encrypted_page : fio->page;
+
+	if (!f2fs_is_valid_blkaddr(fio->sbi, fio->new_blkaddr,
+			__is_meta_io(fio) ? META_GENERIC : DATA_GENERIC))
+		return -EFAULT;
+
+	trace_f2fs_submit_page_bio(page, fio);
+	f2fs_trace_ios(fio, 0);
+
+	if (bio && (*fio->last_block + 1 != fio->new_blkaddr ||
+			!__same_bdev(fio->sbi, fio->new_blkaddr, bio))) {
+		__submit_bio(fio->sbi, bio, fio->type);
+		bio = NULL;
+	}
+alloc_new:
+	if (!bio) {
+		bio = __bio_alloc(fio->sbi, fio->new_blkaddr, fio->io_wbc,
+				BIO_MAX_PAGES, false, fio->type, fio->temp);
+		bio_set_op_attrs(bio, fio->op, fio->op_flags);
+	}
+
+	if (bio_add_page(bio, page, PAGE_SIZE, 0) < PAGE_SIZE) {
+		__submit_bio(fio->sbi, bio, fio->type);
+		bio = NULL;
+		goto alloc_new;
+	}
+
+	if (fio->io_wbc)
+		wbc_account_io(fio->io_wbc, page, PAGE_SIZE);
+
+	inc_page_count(fio->sbi, WB_DATA_TYPE(page));
+
+	*fio->last_block = fio->new_blkaddr;
+	*fio->bio = bio;
+
+	return 0;
+}
+
+static void f2fs_submit_ipu_bio(struct f2fs_sb_info *sbi, struct bio **bio,
+							struct page *page)
+{
+	if (!bio)
+		return;
+
+	if (!__has_merged_page(*bio, NULL, page, 0))
+		return;
+
+	__submit_bio(sbi, *bio, DATA);
+	*bio = NULL;
+}
+
 void f2fs_submit_page_write(struct f2fs_io_info *fio)
 {
 	struct f2fs_sb_info *sbi = fio->sbi;
@@ -1947,6 +2002,8 @@ int f2fs_do_write_data_page(struct f2fs_io_info *fio)
 }
 
 static int __write_data_page(struct page *page, bool *submitted,
+				struct bio **bio,
+				sector_t *last_block,
 				struct writeback_control *wbc,
 				enum iostat_type io_type)
 {
@@ -1972,6 +2029,8 @@ static int __write_data_page(struct page *page, bool *submitted,
 		.need_lock = LOCK_RETRY,
 		.io_type = io_type,
 		.io_wbc = wbc,
+		.bio = bio,
+		.last_block = last_block,
 	};
 
 	trace_f2fs_writepage(page, DATA);
@@ -2070,10 +2129,13 @@ static int __write_data_page(struct page *page, bool *submitted,
 
 	unlock_page(page);
 	if (!S_ISDIR(inode->i_mode) && !IS_NOQUOTA(inode) &&
-					!F2FS_I(inode)->cp_task)
+					!F2FS_I(inode)->cp_task) {
+		f2fs_submit_ipu_bio(sbi, bio, page);
 		f2fs_balance_fs(sbi, need_balance_fs);
+	}
 
 	if (unlikely(f2fs_cp_error(sbi))) {
+		f2fs_submit_ipu_bio(sbi, bio, page);
 		f2fs_submit_merged_write(sbi, DATA);
 		submitted = NULL;
 	}
@@ -2100,7 +2162,7 @@ static int __write_data_page(struct page *page, bool *submitted,
 static int f2fs_write_data_page(struct page *page,
 					struct writeback_control *wbc)
 {
-	return __write_data_page(page, NULL, wbc, FS_DATA_IO);
+	return __write_data_page(page, NULL, NULL, NULL, wbc, FS_DATA_IO);
 }
 
 /*
@@ -2116,6 +2178,8 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 	int done = 0;
 	struct pagevec pvec;
 	struct f2fs_sb_info *sbi = F2FS_M_SB(mapping);
+	struct bio *bio = NULL;
+	sector_t last_block;
 	int nr_pages;
 	pgoff_t uninitialized_var(writeback_index);
 	pgoff_t index;
@@ -2192,17 +2256,20 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 			}
 
 			if (PageWriteback(page)) {
-				if (wbc->sync_mode != WB_SYNC_NONE)
+				if (wbc->sync_mode != WB_SYNC_NONE) {
 					f2fs_wait_on_page_writeback(page,
 							DATA, true, true);
-				else
+					f2fs_submit_ipu_bio(sbi, &bio, page);
+				} else {
 					goto continue_unlock;
+				}
 			}
 
 			if (!clear_page_dirty_for_io(page))
 				goto continue_unlock;
 
-			ret = __write_data_page(page, &submitted, wbc, io_type);
+			ret = __write_data_page(page, &submitted, &bio,
+					&last_block, wbc, io_type);
 			if (unlikely(ret)) {
 				/*
 				 * keep nr_to_write, since vfs uses this to
@@ -2251,6 +2318,9 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 	if (nwritten)
 		f2fs_submit_merged_write_cond(F2FS_M_SB(mapping), mapping->host,
 								NULL, 0, DATA);
+	/* submit cached bio of IPU write */
+	if (bio)
+		__submit_bio(sbi, bio, DATA);
 
 	return ret;
 }

commit 0d28544117fa9dcd0d202aeb4459bb15f42bb7de
Merge: fa4bff165070 2777e654371d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 14 08:55:43 2019 -0700

    Merge tag 'f2fs-for-v5.2-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/jaegeuk/f2fs
    
    Pull f2fs updates from Jaegeuk Kim:
     "Another round of various bug fixes came in. Damien improved SMR drive
      support a bit, and Chao replaced BUG_ON() with reporting errors to
      user since we've not hit from users but did hit from crafted images.
      We've found a disk layout bug in large_nat_bits feature which supports
      very large NAT entries enabled at mkfs. If the feature is enabled, it
      will give a notice to run fsck to correct the on-disk layout.
    
      Enhancements:
       - reduce memory consumption for SMR drive
       - better discard handling for multiple partitions
       - tracepoints for f2fs_file_write_iter/f2fs_filemap_fault
       - allow to change CP_CHKSUM_OFFSET
       - detect wrong layout of large_nat_bitmap feature
       - enhance checking valid data indices
    
      Bug fixes:
       - Multiple partition support for SMR drive
       - deadlock problem in f2fs_balance_fs_bg
       - add boundary checks to fix abnormal behaviors on fuzzed images
       - inline_xattr space calculations
       - replace f2fs_bug_on with errors
    
      In addition, this series contains various memory boundary check and
      sanity check of on-disk consistency"
    
    * tag 'f2fs-for-v5.2-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/jaegeuk/f2fs: (40 commits)
      f2fs: fix to avoid accessing xattr across the boundary
      f2fs: fix to avoid potential race on sbi->unusable_block_count access/update
      f2fs: add tracepoint for f2fs_filemap_fault()
      f2fs: introduce DATA_GENERIC_ENHANCE
      f2fs: fix to handle error in f2fs_disable_checkpoint()
      f2fs: remove redundant check in f2fs_file_write_iter()
      f2fs: fix to be aware of readonly device in write_checkpoint()
      f2fs: fix to skip recovery on readonly device
      f2fs: fix to consider multiple device for readonly check
      f2fs: relocate chksum_offset for large_nat_bitmap feature
      f2fs: allow unfixed f2fs_checkpoint.checksum_offset
      f2fs: Replace spaces with tab
      f2fs: insert space before the open parenthesis '('
      f2fs: allow address pointer number of dnode aligning to specified size
      f2fs: introduce f2fs_read_single_page() for cleanup
      f2fs: mark is_extension_exist() inline
      f2fs: fix to set FI_UPDATE_WRITE correctly
      f2fs: fix to avoid panic in f2fs_inplace_write_data()
      f2fs: fix to do sanity check on valid block count of segment
      f2fs: fix to do sanity check on valid node/block count
      ...

commit 93770ab7a6e963147a5dbca25278b69ba6c8f8c5
Author: Chao Yu <yuchao0@huawei.com>
Date:   Mon Apr 15 15:26:32 2019 +0800

    f2fs: introduce DATA_GENERIC_ENHANCE
    
    Previously, f2fs_is_valid_blkaddr(, blkaddr, DATA_GENERIC) will check
    whether @blkaddr locates in main area or not.
    
    That check is weak, since the block address in range of main area can
    point to the address which is not valid in segment info table, and we
    can not detect such condition, we may suffer worse corruption as system
    continues running.
    
    So this patch introduce DATA_GENERIC_ENHANCE to enhance the sanity check
    which trigger SIT bitmap check rather than only range check.
    
    This patch did below changes as wel:
    - set SBI_NEED_FSCK in f2fs_is_valid_blkaddr().
    - get rid of is_valid_data_blkaddr() to avoid panic if blkaddr is invalid.
    - introduce verify_fio_blkaddr() to wrap fio {new,old}_blkaddr validation check.
    - spread blkaddr check in:
     * f2fs_get_node_info()
     * __read_out_blkaddrs()
     * f2fs_submit_page_read()
     * ra_data_block()
     * do_recover_data()
    
    This patch can fix bug reported from bugzilla below:
    
    https://bugzilla.kernel.org/show_bug.cgi?id=203215
    https://bugzilla.kernel.org/show_bug.cgi?id=203223
    https://bugzilla.kernel.org/show_bug.cgi?id=203231
    https://bugzilla.kernel.org/show_bug.cgi?id=203235
    https://bugzilla.kernel.org/show_bug.cgi?id=203241
    
    = Update by Jaegeuk Kim =
    
    DATA_GENERIC_ENHANCE enhanced to validate block addresses on read/write paths.
    But, xfstest/generic/446 compalins some generated kernel messages saying invalid
    bitmap was detected when reading a block. The reaons is, when we get the
    block addresses from extent_cache, there is no lock to synchronize it from
    truncating the blocks in parallel.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index cf89e36577bf..d32a82f25f5a 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -456,8 +456,8 @@ int f2fs_submit_page_bio(struct f2fs_io_info *fio)
 			fio->encrypted_page : fio->page;
 
 	if (!f2fs_is_valid_blkaddr(fio->sbi, fio->new_blkaddr,
-			fio->is_por ? META_POR :
-			(__is_meta_io(fio) ? META_GENERIC : DATA_GENERIC)))
+			fio->is_por ? META_POR : (__is_meta_io(fio) ?
+			META_GENERIC : DATA_GENERIC_ENHANCE)))
 		return -EFAULT;
 
 	trace_f2fs_submit_page_bio(page, fio);
@@ -507,9 +507,7 @@ void f2fs_submit_page_write(struct f2fs_io_info *fio)
 		spin_unlock(&io->io_lock);
 	}
 
-	if (__is_valid_data_blkaddr(fio->old_blkaddr))
-		verify_block_addr(fio, fio->old_blkaddr);
-	verify_block_addr(fio, fio->new_blkaddr);
+	verify_fio_blkaddr(fio);
 
 	bio_page = fio->encrypted_page ? fio->encrypted_page : fio->page;
 
@@ -566,9 +564,6 @@ static struct bio *f2fs_grab_read_bio(struct inode *inode, block_t blkaddr,
 	struct bio_post_read_ctx *ctx;
 	unsigned int post_read_steps = 0;
 
-	if (!f2fs_is_valid_blkaddr(sbi, blkaddr, DATA_GENERIC))
-		return ERR_PTR(-EFAULT);
-
 	bio = f2fs_bio_alloc(sbi, min_t(int, nr_pages, BIO_MAX_PAGES), false);
 	if (!bio)
 		return ERR_PTR(-ENOMEM);
@@ -596,8 +591,10 @@ static struct bio *f2fs_grab_read_bio(struct inode *inode, block_t blkaddr,
 static int f2fs_submit_page_read(struct inode *inode, struct page *page,
 							block_t blkaddr)
 {
-	struct bio *bio = f2fs_grab_read_bio(inode, blkaddr, 1, 0);
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct bio *bio;
 
+	bio = f2fs_grab_read_bio(inode, blkaddr, 1, 0);
 	if (IS_ERR(bio))
 		return PTR_ERR(bio);
 
@@ -609,8 +606,8 @@ static int f2fs_submit_page_read(struct inode *inode, struct page *page,
 		return -EFAULT;
 	}
 	ClearPageError(page);
-	inc_page_count(F2FS_I_SB(inode), F2FS_RD_DATA);
-	__submit_bio(F2FS_I_SB(inode), bio, DATA);
+	inc_page_count(sbi, F2FS_RD_DATA);
+	__submit_bio(sbi, bio, DATA);
 	return 0;
 }
 
@@ -738,6 +735,11 @@ struct page *f2fs_get_read_data_page(struct inode *inode, pgoff_t index,
 
 	if (f2fs_lookup_extent_cache(inode, index, &ei)) {
 		dn.data_blkaddr = ei.blk + index - ei.fofs;
+		if (!f2fs_is_valid_blkaddr(F2FS_I_SB(inode), dn.data_blkaddr,
+						DATA_GENERIC_ENHANCE_READ)) {
+			err = -EFAULT;
+			goto put_err;
+		}
 		goto got_it;
 	}
 
@@ -751,6 +753,13 @@ struct page *f2fs_get_read_data_page(struct inode *inode, pgoff_t index,
 		err = -ENOENT;
 		goto put_err;
 	}
+	if (dn.data_blkaddr != NEW_ADDR &&
+			!f2fs_is_valid_blkaddr(F2FS_I_SB(inode),
+						dn.data_blkaddr,
+						DATA_GENERIC_ENHANCE)) {
+		err = -EFAULT;
+		goto put_err;
+	}
 got_it:
 	if (PageUptodate(page)) {
 		unlock_page(page);
@@ -1093,12 +1102,12 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 	blkaddr = datablock_addr(dn.inode, dn.node_page, dn.ofs_in_node);
 
 	if (__is_valid_data_blkaddr(blkaddr) &&
-		!f2fs_is_valid_blkaddr(sbi, blkaddr, DATA_GENERIC)) {
+		!f2fs_is_valid_blkaddr(sbi, blkaddr, DATA_GENERIC_ENHANCE)) {
 		err = -EFAULT;
 		goto sync_out;
 	}
 
-	if (is_valid_data_blkaddr(sbi, blkaddr)) {
+	if (__is_valid_data_blkaddr(blkaddr)) {
 		/* use out-place-update for driect IO under LFS mode */
 		if (test_opt(sbi, LFS) && flag == F2FS_GET_BLOCK_DIO &&
 							map->m_may_create) {
@@ -1563,7 +1572,7 @@ static int f2fs_read_single_page(struct inode *inode, struct page *page,
 		}
 
 		if (!f2fs_is_valid_blkaddr(F2FS_I_SB(inode), block_nr,
-							DATA_GENERIC)) {
+						DATA_GENERIC_ENHANCE_READ)) {
 			ret = -EFAULT;
 			goto out;
 		}
@@ -1844,7 +1853,7 @@ int f2fs_do_write_data_page(struct f2fs_io_info *fio)
 		fio->old_blkaddr = ei.blk + page->index - ei.fofs;
 
 		if (!f2fs_is_valid_blkaddr(fio->sbi, fio->old_blkaddr,
-							DATA_GENERIC))
+						DATA_GENERIC_ENHANCE))
 			return -EFAULT;
 
 		ipu_force = true;
@@ -1871,7 +1880,7 @@ int f2fs_do_write_data_page(struct f2fs_io_info *fio)
 got_it:
 	if (__is_valid_data_blkaddr(fio->old_blkaddr) &&
 		!f2fs_is_valid_blkaddr(fio->sbi, fio->old_blkaddr,
-							DATA_GENERIC)) {
+						DATA_GENERIC_ENHANCE)) {
 		err = -EFAULT;
 		goto out_writepage;
 	}
@@ -1879,7 +1888,8 @@ int f2fs_do_write_data_page(struct f2fs_io_info *fio)
 	 * If current allocation needs SSR,
 	 * it had better in-place writes for updated data.
 	 */
-	if (ipu_force || (is_valid_data_blkaddr(fio->sbi, fio->old_blkaddr) &&
+	if (ipu_force ||
+		(__is_valid_data_blkaddr(fio->old_blkaddr) &&
 					need_inplace_update(fio))) {
 		err = encrypt_one_page(fio);
 		if (err)
@@ -2524,6 +2534,11 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 		zero_user_segment(page, 0, PAGE_SIZE);
 		SetPageUptodate(page);
 	} else {
+		if (!f2fs_is_valid_blkaddr(sbi, blkaddr,
+				DATA_GENERIC_ENHANCE_READ)) {
+			err = -EFAULT;
+			goto fail;
+		}
 		err = f2fs_submit_page_read(inode, page, blkaddr);
 		if (err)
 			goto fail;

commit 2df0ab045784a1ca904437601a5086f570e8cf16
Author: Chao Yu <yuchao0@huawei.com>
Date:   Mon Mar 25 21:07:30 2019 +0800

    f2fs: introduce f2fs_read_single_page() for cleanup
    
    This patch introduces f2fs_read_single_page() to wrap core operations
    of reading one page in f2fs_mpage_readpages().
    
    In addition, if we failed in f2fs_mpage_readpages(), propagate error
    number to f2fs_read_data_page(), for f2fs_read_data_pages() path,
    always return success.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 7a67d6161b84..cf89e36577bf 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1508,6 +1508,118 @@ int f2fs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,
 	return ret;
 }
 
+static int f2fs_read_single_page(struct inode *inode, struct page *page,
+					unsigned nr_pages,
+					struct f2fs_map_blocks *map,
+					struct bio **bio_ret,
+					sector_t *last_block_in_bio,
+					bool is_readahead)
+{
+	struct bio *bio = *bio_ret;
+	const unsigned blkbits = inode->i_blkbits;
+	const unsigned blocksize = 1 << blkbits;
+	sector_t block_in_file;
+	sector_t last_block;
+	sector_t last_block_in_file;
+	sector_t block_nr;
+	int ret = 0;
+
+	block_in_file = (sector_t)page->index;
+	last_block = block_in_file + nr_pages;
+	last_block_in_file = (i_size_read(inode) + blocksize - 1) >>
+							blkbits;
+	if (last_block > last_block_in_file)
+		last_block = last_block_in_file;
+
+	/* just zeroing out page which is beyond EOF */
+	if (block_in_file >= last_block)
+		goto zero_out;
+	/*
+	 * Map blocks using the previous result first.
+	 */
+	if ((map->m_flags & F2FS_MAP_MAPPED) &&
+			block_in_file > map->m_lblk &&
+			block_in_file < (map->m_lblk + map->m_len))
+		goto got_it;
+
+	/*
+	 * Then do more f2fs_map_blocks() calls until we are
+	 * done with this page.
+	 */
+	map->m_lblk = block_in_file;
+	map->m_len = last_block - block_in_file;
+
+	ret = f2fs_map_blocks(inode, map, 0, F2FS_GET_BLOCK_DEFAULT);
+	if (ret)
+		goto out;
+got_it:
+	if ((map->m_flags & F2FS_MAP_MAPPED)) {
+		block_nr = map->m_pblk + block_in_file - map->m_lblk;
+		SetPageMappedToDisk(page);
+
+		if (!PageUptodate(page) && !cleancache_get_page(page)) {
+			SetPageUptodate(page);
+			goto confused;
+		}
+
+		if (!f2fs_is_valid_blkaddr(F2FS_I_SB(inode), block_nr,
+							DATA_GENERIC)) {
+			ret = -EFAULT;
+			goto out;
+		}
+	} else {
+zero_out:
+		zero_user_segment(page, 0, PAGE_SIZE);
+		if (!PageUptodate(page))
+			SetPageUptodate(page);
+		unlock_page(page);
+		goto out;
+	}
+
+	/*
+	 * This page will go to BIO.  Do we need to send this
+	 * BIO off first?
+	 */
+	if (bio && (*last_block_in_bio != block_nr - 1 ||
+		!__same_bdev(F2FS_I_SB(inode), block_nr, bio))) {
+submit_and_realloc:
+		__submit_bio(F2FS_I_SB(inode), bio, DATA);
+		bio = NULL;
+	}
+	if (bio == NULL) {
+		bio = f2fs_grab_read_bio(inode, block_nr, nr_pages,
+				is_readahead ? REQ_RAHEAD : 0);
+		if (IS_ERR(bio)) {
+			ret = PTR_ERR(bio);
+			bio = NULL;
+			goto out;
+		}
+	}
+
+	/*
+	 * If the page is under writeback, we need to wait for
+	 * its completion to see the correct decrypted data.
+	 */
+	f2fs_wait_on_block_writeback(inode, block_nr);
+
+	if (bio_add_page(bio, page, blocksize, 0) < blocksize)
+		goto submit_and_realloc;
+
+	inc_page_count(F2FS_I_SB(inode), F2FS_RD_DATA);
+	ClearPageError(page);
+	*last_block_in_bio = block_nr;
+	goto out;
+confused:
+	if (bio) {
+		__submit_bio(F2FS_I_SB(inode), bio, DATA);
+		bio = NULL;
+	}
+	unlock_page(page);
+out:
+	*bio_ret = bio;
+	return ret;
+}
+
 /*
  * This function was originally taken from fs/mpage.c, and customized for f2fs.
  * Major change was from block_size == page_size in f2fs by default.
@@ -1524,13 +1636,8 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 	struct bio *bio = NULL;
 	sector_t last_block_in_bio = 0;
 	struct inode *inode = mapping->host;
-	const unsigned blkbits = inode->i_blkbits;
-	const unsigned blocksize = 1 << blkbits;
-	sector_t block_in_file;
-	sector_t last_block;
-	sector_t last_block_in_file;
-	sector_t block_nr;
 	struct f2fs_map_blocks map;
+	int ret = 0;
 
 	map.m_pblk = 0;
 	map.m_lblk = 0;
@@ -1553,98 +1660,13 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 				goto next_page;
 		}
 
-		block_in_file = (sector_t)page->index;
-		last_block = block_in_file + nr_pages;
-		last_block_in_file = (i_size_read(inode) + blocksize - 1) >>
-								blkbits;
-		if (last_block > last_block_in_file)
-			last_block = last_block_in_file;
-
-		/* just zeroing out page which is beyond EOF */
-		if (block_in_file >= last_block)
-			goto zero_out;
-		/*
-		 * Map blocks using the previous result first.
-		 */
-		if ((map.m_flags & F2FS_MAP_MAPPED) &&
-				block_in_file > map.m_lblk &&
-				block_in_file < (map.m_lblk + map.m_len))
-			goto got_it;
-
-		/*
-		 * Then do more f2fs_map_blocks() calls until we are
-		 * done with this page.
-		 */
-		map.m_lblk = block_in_file;
-		map.m_len = last_block - block_in_file;
-
-		if (f2fs_map_blocks(inode, &map, 0, F2FS_GET_BLOCK_DEFAULT))
-			goto set_error_page;
-got_it:
-		if ((map.m_flags & F2FS_MAP_MAPPED)) {
-			block_nr = map.m_pblk + block_in_file - map.m_lblk;
-			SetPageMappedToDisk(page);
-
-			if (!PageUptodate(page) && !cleancache_get_page(page)) {
-				SetPageUptodate(page);
-				goto confused;
-			}
-
-			if (!f2fs_is_valid_blkaddr(F2FS_I_SB(inode), block_nr,
-								DATA_GENERIC))
-				goto set_error_page;
-		} else {
-zero_out:
+		ret = f2fs_read_single_page(inode, page, nr_pages, &map, &bio,
+					&last_block_in_bio, is_readahead);
+		if (ret) {
+			SetPageError(page);
 			zero_user_segment(page, 0, PAGE_SIZE);
-			if (!PageUptodate(page))
-				SetPageUptodate(page);
 			unlock_page(page);
-			goto next_page;
 		}
-
-		/*
-		 * This page will go to BIO.  Do we need to send this
-		 * BIO off first?
-		 */
-		if (bio && (last_block_in_bio != block_nr - 1 ||
-			!__same_bdev(F2FS_I_SB(inode), block_nr, bio))) {
-submit_and_realloc:
-			__submit_bio(F2FS_I_SB(inode), bio, DATA);
-			bio = NULL;
-		}
-		if (bio == NULL) {
-			bio = f2fs_grab_read_bio(inode, block_nr, nr_pages,
-					is_readahead ? REQ_RAHEAD : 0);
-			if (IS_ERR(bio)) {
-				bio = NULL;
-				goto set_error_page;
-			}
-		}
-
-		/*
-		 * If the page is under writeback, we need to wait for
-		 * its completion to see the correct decrypted data.
-		 */
-		f2fs_wait_on_block_writeback(inode, block_nr);
-
-		if (bio_add_page(bio, page, blocksize, 0) < blocksize)
-			goto submit_and_realloc;
-
-		inc_page_count(F2FS_I_SB(inode), F2FS_RD_DATA);
-		ClearPageError(page);
-		last_block_in_bio = block_nr;
-		goto next_page;
-set_error_page:
-		SetPageError(page);
-		zero_user_segment(page, 0, PAGE_SIZE);
-		unlock_page(page);
-		goto next_page;
-confused:
-		if (bio) {
-			__submit_bio(F2FS_I_SB(inode), bio, DATA);
-			bio = NULL;
-		}
-		unlock_page(page);
 next_page:
 		if (pages)
 			put_page(page);
@@ -1652,7 +1674,7 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 	BUG_ON(pages && !list_empty(pages));
 	if (bio)
 		__submit_bio(F2FS_I_SB(inode), bio, DATA);
-	return 0;
+	return pages ? 0 : ret;
 }
 
 static int f2fs_read_data_page(struct file *file, struct page *page)

commit cd23ffa9fcba071c6a6129c46bf41acca77fab4a
Author: Chao Yu <yuchao0@huawei.com>
Date:   Mon Apr 15 15:30:53 2019 +0800

    f2fs: fix to set FI_UPDATE_WRITE correctly
    
    This patch fixes to set FI_UPDATE_WRITE only if in-place IO was issued.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index da932eeecf30..7a67d6161b84 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1875,9 +1875,10 @@ int f2fs_do_write_data_page(struct f2fs_io_info *fio)
 									true);
 			if (PageWriteback(page))
 				end_page_writeback(page);
+		} else {
+			set_inode_flag(inode, FI_UPDATE_WRITE);
 		}
 		trace_f2fs_do_write_data_page(fio->page, IPU);
-		set_inode_flag(inode, FI_UPDATE_WRITE);
 		return err;
 	}
 

commit 6dc3a12663c8a99ef033287f48bbdd61b6b1979b
Author: Chao Yu <yuchao0@huawei.com>
Date:   Mon Apr 15 15:26:31 2019 +0800

    f2fs: fix wrong __is_meta_io() macro
    
    This patch changes codes as below:
    - don't use is_read_io() as a condition to judge the meta IO.
    - use .is_por to replace .is_meta to indicate IO is from recovery explicitly.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 0c582b388742..da932eeecf30 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -456,7 +456,8 @@ int f2fs_submit_page_bio(struct f2fs_io_info *fio)
 			fio->encrypted_page : fio->page;
 
 	if (!f2fs_is_valid_blkaddr(fio->sbi, fio->new_blkaddr,
-			__is_meta_io(fio) ? META_GENERIC : DATA_GENERIC))
+			fio->is_por ? META_POR :
+			(__is_meta_io(fio) ? META_GENERIC : DATA_GENERIC)))
 		return -EFAULT;
 
 	trace_f2fs_submit_page_bio(page, fio);

commit 2b070cfe582b8e99fec6ada57d2e59e194aae202
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Apr 25 09:03:00 2019 +0200

    block: remove the i argument to bio_for_each_segment_all
    
    We only have two callers that need the integer loop iterator, and they
    can easily maintain it themselves.
    
    Suggested-by: Matthew Wilcox <willy@infradead.org>
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Acked-by: David Sterba <dsterba@suse.com>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Acked-by: Coly Li <colyli@suse.de>
    Reviewed-by: Matthew Wilcox <willy@infradead.org>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 9727944139f2..64040e998439 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -86,10 +86,9 @@ static void __read_end_io(struct bio *bio)
 {
 	struct page *page;
 	struct bio_vec *bv;
-	int i;
 	struct bvec_iter_all iter_all;
 
-	bio_for_each_segment_all(bv, bio, i, iter_all) {
+	bio_for_each_segment_all(bv, bio, iter_all) {
 		page = bv->bv_page;
 
 		/* PG_error was set if any post_read step failed */
@@ -164,7 +163,6 @@ static void f2fs_write_end_io(struct bio *bio)
 {
 	struct f2fs_sb_info *sbi = bio->bi_private;
 	struct bio_vec *bvec;
-	int i;
 	struct bvec_iter_all iter_all;
 
 	if (time_to_inject(sbi, FAULT_WRITE_IO)) {
@@ -172,7 +170,7 @@ static void f2fs_write_end_io(struct bio *bio)
 		bio->bi_status = BLK_STS_IOERR;
 	}
 
-	bio_for_each_segment_all(bvec, bio, i, iter_all) {
+	bio_for_each_segment_all(bvec, bio, iter_all) {
 		struct page *page = bvec->bv_page;
 		enum count_type type = WB_DATA_TYPE(page);
 
@@ -349,7 +347,6 @@ static bool __has_merged_page(struct f2fs_bio_info *io, struct inode *inode,
 {
 	struct bio_vec *bvec;
 	struct page *target;
-	int i;
 	struct bvec_iter_all iter_all;
 
 	if (!io->bio)
@@ -358,7 +355,7 @@ static bool __has_merged_page(struct f2fs_bio_info *io, struct inode *inode,
 	if (!inode && !page && !ino)
 		return true;
 
-	bio_for_each_segment_all(bvec, io->bio, i, iter_all) {
+	bio_for_each_segment_all(bvec, io->bio, iter_all) {
 
 		if (bvec->bv_page->mapping)
 			target = bvec->bv_page;

commit adcc00f7dcbf0131070ecc750cf83ee1840f603d
Author: Hariprasad Kelam <hariprasad.kelam@gmail.com>
Date:   Sat Apr 6 16:29:36 2019 +0530

    f2fs: data: fix warning Using plain integer as NULL pointer
    
    changed passing function argument "0 to NULL" to fix below sparse
    warning
    
    fs/f2fs/data.c:426:47: warning: Using plain integer as NULL pointer
    
    Signed-off-by: Hariprasad Kelam <hariprasad.kelam@gmail.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Reviewed-by: Mukesh Ojha <mojha@codeaurora.org>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 9d3c11e09a03..0c582b388742 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -428,7 +428,7 @@ static void __submit_merged_write_cond(struct f2fs_sb_info *sbi,
 
 void f2fs_submit_merged_write(struct f2fs_sb_info *sbi, enum page_type type)
 {
-	__submit_merged_write_cond(sbi, NULL, 0, 0, type, true);
+	__submit_merged_write_cond(sbi, NULL, NULL, 0, type, true);
 }
 
 void f2fs_submit_merged_write_cond(struct f2fs_sb_info *sbi,

commit 186857c5a14aee85cace2ae7a36c6e43b9d3c7a5
Author: Chao Yu <yuchao0@huawei.com>
Date:   Tue Apr 2 18:52:19 2019 +0800

    f2fs: fix potential recursive call when enabling data_flush
    
    As Hagbard Celine reported:
    
    Hi, this is a long standing bug that I've hit before on older kernels,
    but I was not able to get the syslog saved because of the nature of
    the bug. This time I had booted form a pen-drive, and was able to save
    the log to it's efi-partition.
    What i did to trigger it was to create a partition and format it f2fs,
    then mount it with options:
    "rw,relatime,lazytime,background_gc=on,disable_ext_identify,discard,heap,user_xattr,inline_xattr,acl,inline_data,inline_dentry,flush_merge,data_flush,extent_cache,mode=adaptive,active_logs=6,whint_mode=fs-based,alloc_mode=default,fsync_mode=strict".
    Then I unpacked a big .tar.xz to the partition (I used a
    gentoo-stage3-tarball as I was in process of installing Gentoo).
    
    Same options just without data_flush gives no problems.
    
    Mar 20 20:54:01 usbgentoo kernel: FAT-fs (nvme0n1p4): Volume was not
    properly unmounted. Some data may be corrupt. Please run fsck.
    Mar 20 21:05:23 usbgentoo kernel: kworker/dying (1588) used greatest
    stack depth: 12064 bytes left
    Mar 20 21:06:40 usbgentoo kernel: BUG: stack guard page was hit at
    00000000a4b0733c (stack is 0000000056016422..0000000096e7463f)
    Mar 20 21:06:40 usbgentoo kernel: kernel stack overflow
    
    ......
    
    Mar 20 21:06:40 usbgentoo kernel: Call Trace:
    Mar 20 21:06:40 usbgentoo kernel:  read_node_page+0x71/0xf0
    Mar 20 21:06:40 usbgentoo kernel:  ? xas_load+0x8/0x50
    Mar 20 21:06:40 usbgentoo kernel:  __get_node_page+0x73/0x2a0
    Mar 20 21:06:40 usbgentoo kernel:  f2fs_get_dnode_of_data+0x34e/0x580
    Mar 20 21:06:40 usbgentoo kernel:  f2fs_write_inline_data+0x5e/0x2a0
    Mar 20 21:06:40 usbgentoo kernel:  __write_data_page+0x421/0x690
    Mar 20 21:06:40 usbgentoo kernel:  f2fs_write_cache_pages+0x1cf/0x460
    Mar 20 21:06:40 usbgentoo kernel:  f2fs_write_data_pages+0x2b3/0x2e0
    Mar 20 21:06:40 usbgentoo kernel:  ? f2fs_inode_chksum_verify+0x1d/0xc0
    Mar 20 21:06:40 usbgentoo kernel:  ? read_node_page+0x71/0xf0
    Mar 20 21:06:40 usbgentoo kernel:  do_writepages+0x3c/0xd0
    Mar 20 21:06:40 usbgentoo kernel:  __filemap_fdatawrite_range+0x7c/0xb0
    Mar 20 21:06:40 usbgentoo kernel:  f2fs_sync_dirty_inodes+0xf2/0x200
    Mar 20 21:06:40 usbgentoo kernel:  f2fs_balance_fs_bg+0x2a3/0x2c0
    Mar 20 21:06:40 usbgentoo kernel:  ? f2fs_inode_dirtied+0x21/0xc0
    Mar 20 21:06:40 usbgentoo kernel:  f2fs_balance_fs+0xd6/0x2b0
    Mar 20 21:06:40 usbgentoo kernel:  __write_data_page+0x4fb/0x690
    
    ......
    
    Mar 20 21:06:40 usbgentoo kernel:  __writeback_single_inode+0x2a1/0x340
    Mar 20 21:06:40 usbgentoo kernel:  ? soft_cursor+0x1b4/0x220
    Mar 20 21:06:40 usbgentoo kernel:  writeback_sb_inodes+0x1d5/0x3e0
    Mar 20 21:06:40 usbgentoo kernel:  __writeback_inodes_wb+0x58/0xa0
    Mar 20 21:06:40 usbgentoo kernel:  wb_writeback+0x250/0x2e0
    Mar 20 21:06:40 usbgentoo kernel:  ? 0xffffffff8c000000
    Mar 20 21:06:40 usbgentoo kernel:  ? cpumask_next+0x16/0x20
    Mar 20 21:06:40 usbgentoo kernel:  wb_workfn+0x2f6/0x3b0
    Mar 20 21:06:40 usbgentoo kernel:  ? __switch_to_asm+0x40/0x70
    Mar 20 21:06:40 usbgentoo kernel:  process_one_work+0x1f5/0x3f0
    Mar 20 21:06:40 usbgentoo kernel:  worker_thread+0x28/0x3c0
    Mar 20 21:06:40 usbgentoo kernel:  ? rescuer_thread+0x330/0x330
    Mar 20 21:06:40 usbgentoo kernel:  kthread+0x10e/0x130
    Mar 20 21:06:40 usbgentoo kernel:  ? kthread_create_on_node+0x60/0x60
    Mar 20 21:06:40 usbgentoo kernel:  ret_from_fork+0x35/0x40
    
    The root cause is that we run into an infinite recursive calling in
    between f2fs_balance_fs_bg and writepage() as described below:
    
    - f2fs_write_data_pages         --- A
     - __write_data_page
      - f2fs_balance_fs
       - f2fs_balance_fs_bg         --- B
        - f2fs_sync_dirty_inodes
         - filemap_fdatawrite
          - f2fs_write_data_pages   --- A
    ...
              - f2fs_balance_fs_bg  --- B
    ...
    
    In order to fix this issue, let's detect such condition in __write_data_page()
    and just skip calling f2fs_balance_fs() recursively.
    
    Reported-by: Hagbard Celine <hagbardcelin@gmail.com>
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index d87dfa5aa112..9d3c11e09a03 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -2038,7 +2038,8 @@ static int __write_data_page(struct page *page, bool *submitted,
 	}
 
 	unlock_page(page);
-	if (!S_ISDIR(inode->i_mode) && !IS_NOQUOTA(inode))
+	if (!S_ISDIR(inode->i_mode) && !IS_NOQUOTA(inode) &&
+					!F2FS_I(inode)->cp_task)
 		f2fs_balance_fs(sbi, need_balance_fs);
 
 	if (unlikely(f2fs_cp_error(sbi))) {

commit 0916878da355650d7e77104a7ac0fa1784eca852
Author: Damien Le Moal <damien.lemoal@wdc.com>
Date:   Sat Mar 16 09:13:06 2019 +0900

    f2fs: Fix use of number of devices
    
    For a single device mount using a zoned block device, the zone
    information for the device is stored in the sbi->devs single entry
    array and sbi->s_ndevs is set to 1. This differs from a single device
    mount using a regular block device which does not allocate sbi->devs
    and sets sbi->s_ndevs to 0.
    
    However, sbi->s_devs == 0 condition is used throughout the code to
    differentiate a single device mount from a multi-device mount where
    sbi->s_ndevs is always larger than 1. This results in problems with
    single zoned block device volumes as these are treated as multi-device
    mounts but do not have the start_blk and end_blk information set. One
    of the problem observed is skipping of zone discard issuing resulting in
    write commands being issued to full zones or unaligned to a zone write
    pointer.
    
    Fix this problem by simply treating the cases sbi->s_ndevs == 0 (single
    regular block device mount) and sbi->s_ndevs == 1 (single zoned block
    device mount) in the same manner. This is done by introducing the
    helper function f2fs_is_multi_device() and using this helper in place
    of direct tests of sbi->s_ndevs value, improving code readability.
    
    Fixes: 7bb3a371d199 ("f2fs: Fix zoned block device support")
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Damien Le Moal <damien.lemoal@wdc.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 9727944139f2..d87dfa5aa112 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -220,12 +220,14 @@ struct block_device *f2fs_target_device(struct f2fs_sb_info *sbi,
 	struct block_device *bdev = sbi->sb->s_bdev;
 	int i;
 
-	for (i = 0; i < sbi->s_ndevs; i++) {
-		if (FDEV(i).start_blk <= blk_addr &&
-					FDEV(i).end_blk >= blk_addr) {
-			blk_addr -= FDEV(i).start_blk;
-			bdev = FDEV(i).bdev;
-			break;
+	if (f2fs_is_multi_device(sbi)) {
+		for (i = 0; i < sbi->s_ndevs; i++) {
+			if (FDEV(i).start_blk <= blk_addr &&
+			    FDEV(i).end_blk >= blk_addr) {
+				blk_addr -= FDEV(i).start_blk;
+				bdev = FDEV(i).bdev;
+				break;
+			}
 		}
 	}
 	if (bio) {
@@ -239,6 +241,9 @@ int f2fs_target_device_index(struct f2fs_sb_info *sbi, block_t blkaddr)
 {
 	int i;
 
+	if (!f2fs_is_multi_device(sbi))
+		return 0;
+
 	for (i = 0; i < sbi->s_ndevs; i++)
 		if (FDEV(i).start_blk <= blkaddr && FDEV(i).end_blk >= blkaddr)
 			return i;

commit 5160bcce5c3c80de7d8722511c144d3041409657
Merge: f91f2ee54a21 aff7b628ac2d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Mar 15 13:42:53 2019 -0700

    Merge tag 'f2fs-for-5.1' of git://git.kernel.org/pub/scm/linux/kernel/git/jaegeuk/f2fs
    
    Pull f2fs updates from Jaegeuk Kim:
     "We've continued mainly to fix bugs in this round, as f2fs has been
      shipped in more devices. Especially, we've focused on stabilizing
      checkpoint=disable feature, and provided some interfaces for QA.
    
      Enhancements:
       - expose FS_NOCOW_FL for pin_file
       - run discard jobs at unmount time with timeout
       - tune discarding thread to avoid idling which consumes power
       - some checking codes to address vulnerabilities
       - give random value to i_generation
       - shutdown with more flags for QA
    
      Bug fixes:
       - clean up stale objects when mount is failed along with
         checkpoint=disable
       - fix system being stuck due to wrong count by atomic writes
       - handle some corrupted disk cases
       - fix a deadlock in f2fs_read_inline_dir
    
      We've also added some minor build error fixes and clean-up patches"
    
    * tag 'f2fs-for-5.1' of git://git.kernel.org/pub/scm/linux/kernel/git/jaegeuk/f2fs: (53 commits)
      f2fs: set pin_file under CAP_SYS_ADMIN
      f2fs: fix to avoid deadlock in f2fs_read_inline_dir()
      f2fs: fix to adapt small inline xattr space in __find_inline_xattr()
      f2fs: fix to do sanity check with inode.i_inline_xattr_size
      f2fs: give some messages for inline_xattr_size
      f2fs: don't trigger read IO for beyond EOF page
      f2fs: fix to add refcount once page is tagged PG_private
      f2fs: remove wrong comment in f2fs_invalidate_page()
      f2fs: fix to use kvfree instead of kzfree
      f2fs: print more parameters in trace_f2fs_map_blocks
      f2fs: trace f2fs_ioc_shutdown
      f2fs: fix to avoid deadlock of atomic file operations
      f2fs: fix to dirty inode for i_mode recovery
      f2fs: give random value to i_generation
      f2fs: no need to take page lock in readdir
      f2fs: fix to update iostat correctly in IPU path
      f2fs: fix encrypted page memory leak
      f2fs: make fault injection covering __submit_flush_wait()
      f2fs: fix to retry fill_super only if recovery failed
      f2fs: silence VM_WARN_ON_ONCE in mempool_alloc
      ...

commit 86109c9064daf2ac44ef7b4f1eeb039260351e9c
Author: Chao Yu <yuchao0@huawei.com>
Date:   Thu Mar 7 17:31:30 2019 +0800

    f2fs: don't trigger read IO for beyond EOF page
    
    In f2fs_mpage_readpages(), if page is beyond EOF, we should just
    zero out it, but previously, before checking previous mapping
    info, we missed to check filesize boundary, fix it.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index fa6318549af5..912fdcc2dbb8 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1551,6 +1551,9 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 		if (last_block > last_block_in_file)
 			last_block = last_block_in_file;
 
+		/* just zeroing out page which is beyond EOF */
+		if (block_in_file >= last_block)
+			goto zero_out;
 		/*
 		 * Map blocks using the previous result first.
 		 */
@@ -1563,16 +1566,11 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 		 * Then do more f2fs_map_blocks() calls until we are
 		 * done with this page.
 		 */
-		map.m_flags = 0;
+		map.m_lblk = block_in_file;
+		map.m_len = last_block - block_in_file;
 
-		if (block_in_file < last_block) {
-			map.m_lblk = block_in_file;
-			map.m_len = last_block - block_in_file;
-
-			if (f2fs_map_blocks(inode, &map, 0,
-						F2FS_GET_BLOCK_DEFAULT))
-				goto set_error_page;
-		}
+		if (f2fs_map_blocks(inode, &map, 0, F2FS_GET_BLOCK_DEFAULT))
+			goto set_error_page;
 got_it:
 		if ((map.m_flags & F2FS_MAP_MAPPED)) {
 			block_nr = map.m_pblk + block_in_file - map.m_lblk;
@@ -1587,6 +1585,7 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 								DATA_GENERIC))
 				goto set_error_page;
 		} else {
+zero_out:
 			zero_user_segment(page, 0, PAGE_SIZE);
 			if (!PageUptodate(page))
 				SetPageUptodate(page);

commit 240a59156d9bcfabceddb66be449e7b32fb5dc4a
Author: Chao Yu <yuchao0@huawei.com>
Date:   Wed Mar 6 17:30:59 2019 +0800

    f2fs: fix to add refcount once page is tagged PG_private
    
    As Gao Xiang reported in bugzilla:
    
    https://bugzilla.kernel.org/show_bug.cgi?id=202749
    
    f2fs may skip pageout() due to incorrect page reference count.
    
    The problem here is that MM defined the rule [1] very clearly that
    once page was set with PG_private flag, we should increment the
    refcount in that page, also main flows like pageout(), migrate_page()
    will assume there is one additional page reference count if
    page_has_private() returns true.
    
    But currently, f2fs won't add/del refcount when changing PG_private
    flag. Anyway, f2fs should follow MM's rule to make MM's related flows
    running as expected.
    
    [1] https://lore.kernel.org/lkml/2b19b3c4-2bc4-15fa-15cc-27a13e5c7af1@aol.com/
    
    Reported-by: Gao Xiang <gaoxiang25@huawei.com>
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 2510e935301e..fa6318549af5 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -2714,8 +2714,7 @@ void f2fs_invalidate_page(struct page *page, unsigned int offset,
 	if (IS_ATOMIC_WRITTEN_PAGE(page))
 		return f2fs_drop_inmem_page(inode, page);
 
-	set_page_private(page, 0);
-	ClearPagePrivate(page);
+	f2fs_clear_page_private(page);
 }
 
 int f2fs_release_page(struct page *page, gfp_t wait)
@@ -2729,8 +2728,7 @@ int f2fs_release_page(struct page *page, gfp_t wait)
 		return 0;
 
 	clear_cold_data(page);
-	set_page_private(page, 0);
-	ClearPagePrivate(page);
+	f2fs_clear_page_private(page);
 	return 1;
 }
 
@@ -2798,12 +2796,8 @@ int f2fs_migrate_page(struct address_space *mapping,
 			return -EAGAIN;
 	}
 
-	/*
-	 * A reference is expected if PagePrivate set when move mapping,
-	 * however F2FS breaks this for maintaining dirty page counts when
-	 * truncating pages. So here adjusting the 'extra_count' make it work.
-	 */
-	extra_count = (atomic_written ? 1 : 0) - page_has_private(page);
+	/* one extra reference was held for atomic_write page */
+	extra_count = atomic_written ? 1 : 0;
 	rc = migrate_page_move_mapping(mapping, newpage,
 				page, mode, extra_count);
 	if (rc != MIGRATEPAGE_SUCCESS) {
@@ -2824,9 +2818,10 @@ int f2fs_migrate_page(struct address_space *mapping,
 		get_page(newpage);
 	}
 
-	if (PagePrivate(page))
-		SetPagePrivate(newpage);
-	set_page_private(newpage, page_private(page));
+	if (PagePrivate(page)) {
+		f2fs_set_page_private(newpage, page_private(page));
+		f2fs_clear_page_private(page);
+	}
 
 	if (mode != MIGRATE_SYNC_NO_COPY)
 		migrate_page_copy(newpage, page);

commit 25720cc05e492467099a2d4d21a50f6ee8555cfd
Author: Chao Yu <yuchao0@huawei.com>
Date:   Wed Mar 6 16:18:33 2019 +0800

    f2fs: remove wrong comment in f2fs_invalidate_page()
    
    Since 8c242db9b8c0 ("f2fs: fix stale ATOMIC_WRITTEN_PAGE private pointer"),
    we've started to not skip clear private flag for atomic_write page
    truncation, so removing old wrong comment in f2fs_invalidate_page().
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 3f3becd46362..2510e935301e 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -2711,7 +2711,6 @@ void f2fs_invalidate_page(struct page *page, unsigned int offset,
 
 	clear_cold_data(page);
 
-	/* This is atomic written page, keep Private */
 	if (IS_ATOMIC_WRITTEN_PAGE(page))
 		return f2fs_drop_inmem_page(inode, page);
 

commit 6492a335fd8084fa894beb0c4182de439a12e8d5
Author: Chao Yu <yuchao0@huawei.com>
Date:   Thu Feb 21 20:37:14 2019 +0800

    f2fs: fix encrypted page memory leak
    
    For IPU path of f2fs_do_write_data_page(), in its error path, we
    need to release encrypted page and fscrypt context, otherwise it
    will cause memory leak.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 65c9586b2952..3f3becd46362 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1861,8 +1861,13 @@ int f2fs_do_write_data_page(struct f2fs_io_info *fio)
 		if (fio->need_lock == LOCK_REQ)
 			f2fs_unlock_op(fio->sbi);
 		err = f2fs_inplace_write_data(fio);
-		if (err && PageWriteback(page))
-			end_page_writeback(page);
+		if (err) {
+			if (f2fs_encrypted_file(inode))
+				fscrypt_pullback_bio_page(&fio->encrypted_page,
+									true);
+			if (PageWriteback(page))
+				end_page_writeback(page);
+		}
 		trace_f2fs_do_write_data_page(fio->page, IPU);
 		set_inode_flag(inode, FI_UPDATE_WRITE);
 		return err;

commit bc73a4b2414f2914fa895747166312b1527a97bb
Author: Gao Xiang <gaoxiang25@huawei.com>
Date:   Tue Feb 19 10:31:52 2019 +0800

    f2fs: silence VM_WARN_ON_ONCE in mempool_alloc
    
    Note that __GFP_ZERO is not supported for mempool_alloc,
    which also documented in the mempool_alloc comments.
    
    Signed-off-by: Gao Xiang <gaoxiang25@huawei.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index e099babf85bd..65c9586b2952 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -299,9 +299,10 @@ static inline void __submit_bio(struct f2fs_sb_info *sbi,
 		for (; start < F2FS_IO_SIZE(sbi); start++) {
 			struct page *page =
 				mempool_alloc(sbi->write_io_dummy,
-					GFP_NOIO | __GFP_ZERO | __GFP_NOFAIL);
+					      GFP_NOIO | __GFP_NOFAIL);
 			f2fs_bug_on(sbi, !page);
 
+			zero_user_segment(page, 0, PAGE_SIZE);
 			SetPagePrivate(page);
 			set_page_private(page, (unsigned long)DUMMY_WRITTEN_PAGE);
 			lock_page(page);

commit d1cae94871330cb9f5fdcea34529abf7917e682e
Merge: 99b25a7fc615 129ca2d2a83f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Mar 9 10:54:24 2019 -0800

    Merge tag 'fscrypt-for-linus' of git://git.kernel.org/pub/scm/fs/fscrypt/fscrypt
    
    Pull fscrypt updates from Eric Biggers:
     "First: Ted, Jaegeuk, and I have decided to add me as a co-maintainer
      for fscrypt, and we're now using a shared git tree. So we've updated
      MAINTAINERS accordingly, and I'm doing the pull request this time.
    
      The actual changes for v5.1 are:
    
       - Remove the fs-specific kconfig options like CONFIG_EXT4_ENCRYPTION
         and make fscrypt support for all fscrypt-capable filesystems be
         controlled by CONFIG_FS_ENCRYPTION, similar to how CONFIG_QUOTA
         works.
    
       - Improve error code for rename() and link() into encrypted
         directories.
    
       - Various cleanups"
    
    * tag 'fscrypt-for-linus' of git://git.kernel.org/pub/scm/fs/fscrypt/fscrypt:
      MAINTAINERS: add Eric Biggers as an fscrypt maintainer
      fscrypt: return -EXDEV for incompatible rename or link into encrypted dir
      fscrypt: remove filesystem specific build config option
      f2fs: use IS_ENCRYPTED() to check encryption status
      ext4: use IS_ENCRYPTED() to check encryption status
      fscrypt: remove CRYPTO_CTR dependency

commit c42d28ce3e16dbd88e575c0acfda96d221dae2c9
Author: Chao Yu <yuchao0@huawei.com>
Date:   Sat Feb 2 17:33:01 2019 +0800

    f2fs: fix potential data inconsistence of checkpoint
    
    Previously, we changed lock from cp_rwsem to node_change, it solved
    the deadlock issue which was caused by below race condition:
    
    Thread A                        Thread B
    - f2fs_setattr
     - f2fs_lock_op  -- read_lock
     - dquot_transfer
      - __dquot_transfer
       - dquot_acquire
        - commit_dqblk
         - f2fs_quota_write
          - f2fs_write_begin
           - f2fs_write_failed
                                    - write_checkpoint
                                     - block_operations
                                      - f2fs_lock_all  -- write_lock
            - f2fs_truncate_blocks
             - f2fs_lock_op  -- read_lock
    
    But it breaks the sematics of cp_rwsem, in other callers like:
    - f2fs_file_write_iter -> f2fs_write_begin -> f2fs_write_failed
    - f2fs_direct_IO -> f2fs_write_failed
    
    We allow to truncate dnode w/o cp_rwsem held, result in incorrect sit
    bitmap update, which can cause further data corruption.
    
    So this patch reverts previous fix implementation, and try to fix
    deadlock by skipping calling f2fs_truncate_blocks() in f2fs_write_failed()
    only for quota file, and keep the preallocated data/node in the tail of
    quota file, we can expecte that the preallocated space can be used to
    store quota info latter soon.
    
    Fixes: af033b2aa8a8 ("f2fs: guarantee journalled quota data by checkpoint")
    Signed-off-by: Gao Xiang <gaoxiang25@huawei.com>
    Signed-off-by: Sheng Yong <shengyong1@huawei.com>
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 4e402add2e60..e099babf85bd 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -2312,7 +2312,8 @@ static void f2fs_write_failed(struct address_space *mapping, loff_t to)
 		down_write(&F2FS_I(inode)->i_mmap_sem);
 
 		truncate_pagecache(inode, i_size);
-		f2fs_truncate_blocks(inode, i_size, true, true);
+		if (!IS_NOQUOTA(inode))
+			f2fs_truncate_blocks(inode, i_size, true);
 
 		up_write(&F2FS_I(inode)->i_mmap_sem);
 		up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);

commit 6dc4f100c175dd0511ae8674786e7c9006cdfbfa
Author: Ming Lei <ming.lei@redhat.com>
Date:   Fri Feb 15 19:13:19 2019 +0800

    block: allow bio_for_each_segment_all() to iterate over multi-page bvec
    
    This patch introduces one extra iterator variable to bio_for_each_segment_all(),
    then we can allow bio_for_each_segment_all() to iterate over multi-page bvec.
    
    Given it is just one mechannical & simple change on all bio_for_each_segment_all()
    users, this patch does tree-wide change in one single patch, so that we can
    avoid to use a temporary helper for this conversion.
    
    Reviewed-by: Omar Sandoval <osandov@fb.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index f91d8630c9a2..da060b77f64d 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -87,8 +87,9 @@ static void __read_end_io(struct bio *bio)
 	struct page *page;
 	struct bio_vec *bv;
 	int i;
+	struct bvec_iter_all iter_all;
 
-	bio_for_each_segment_all(bv, bio, i) {
+	bio_for_each_segment_all(bv, bio, i, iter_all) {
 		page = bv->bv_page;
 
 		/* PG_error was set if any post_read step failed */
@@ -164,13 +165,14 @@ static void f2fs_write_end_io(struct bio *bio)
 	struct f2fs_sb_info *sbi = bio->bi_private;
 	struct bio_vec *bvec;
 	int i;
+	struct bvec_iter_all iter_all;
 
 	if (time_to_inject(sbi, FAULT_WRITE_IO)) {
 		f2fs_show_injection_info(FAULT_WRITE_IO);
 		bio->bi_status = BLK_STS_IOERR;
 	}
 
-	bio_for_each_segment_all(bvec, bio, i) {
+	bio_for_each_segment_all(bvec, bio, i, iter_all) {
 		struct page *page = bvec->bv_page;
 		enum count_type type = WB_DATA_TYPE(page);
 
@@ -347,6 +349,7 @@ static bool __has_merged_page(struct f2fs_bio_info *io, struct inode *inode,
 	struct bio_vec *bvec;
 	struct page *target;
 	int i;
+	struct bvec_iter_all iter_all;
 
 	if (!io->bio)
 		return false;
@@ -354,7 +357,7 @@ static bool __has_merged_page(struct f2fs_bio_info *io, struct inode *inode,
 	if (!inode && !page && !ino)
 		return true;
 
-	bio_for_each_segment_all(bvec, io->bio, i) {
+	bio_for_each_segment_all(bvec, io->bio, i, iter_all) {
 
 		if (bvec->bv_page->mapping)
 			target = bvec->bv_page;

commit 62230e0d702f613e2f93e9c3ffd2893b36eff2db
Author: Chandan Rajendra <chandan@linux.vnet.ibm.com>
Date:   Wed Dec 12 15:20:11 2018 +0530

    f2fs: use IS_ENCRYPTED() to check encryption status
    
    This commit removes the f2fs specific f2fs_encrypted_inode() and makes
    use of the generic IS_ENCRYPTED() macro to check for the encryption
    status of an inode.
    
    Acked-by: Chao Yu <yuchao0@huawei.com>
    Reviewed-by: Eric Biggers <ebiggers@google.com>
    Signed-off-by: Chandan Rajendra <chandan@linux.vnet.ibm.com>
    Signed-off-by: Eric Biggers <ebiggers@google.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index f91d8630c9a2..a36eb260656e 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1465,7 +1465,7 @@ int f2fs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,
 	}
 
 	if (size) {
-		if (f2fs_encrypted_inode(inode))
+		if (IS_ENCRYPTED(inode))
 			flags |= FIEMAP_EXTENT_DATA_ENCRYPTED;
 
 		ret = fiemap_fill_next_extent(fieinfo, logical,
@@ -1736,7 +1736,7 @@ static inline bool check_inplace_update_policy(struct inode *inode,
 	if (policy & (0x1 << F2FS_IPU_ASYNC) &&
 			fio && fio->op == REQ_OP_WRITE &&
 			!(fio->op_flags & REQ_SYNC) &&
-			!f2fs_encrypted_inode(inode))
+			!IS_ENCRYPTED(inode))
 		return true;
 
 	/* this is only set during fdatasync */

commit 8e11403876b1e29d290ea42bb20bf45ef927ef6e
Author: YueHaibing <yuehaibing@huawei.com>
Date:   Fri Jan 4 01:38:29 2019 +0000

    f2fs: remove set but not used variable 'err'
    
    Fixes gcc '-Wunused-but-set-variable' warning:
    
    fs/f2fs/data.c: In function 'f2fs_dio_submit_bio':
    fs/f2fs/data.c:2585:6: warning:
     variable 'err' set but not used [-Wunused-but-set-variable]
    
    Signed-off-by: YueHaibing <yuehaibing@huawei.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index f91d8630c9a2..4e402add2e60 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -2582,14 +2582,11 @@ static void f2fs_dio_submit_bio(struct bio *bio, struct inode *inode,
 {
 	struct f2fs_private_dio *dio;
 	bool write = (bio_op(bio) == REQ_OP_WRITE);
-	int err;
 
 	dio = f2fs_kzalloc(F2FS_I_SB(inode),
 			sizeof(struct f2fs_private_dio), GFP_NOFS);
-	if (!dio) {
-		err = -ENOMEM;
+	if (!dio)
 		goto out;
-	}
 
 	dio->inode = inode;
 	dio->orig_end_io = bio->bi_end_io;

commit 9ab97aea85cca43a6aedc90e0d1feba91eebe1ad
Merge: 195303136f19 64beba0558fc
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Dec 31 09:41:37 2018 -0800

    Merge tag 'f2fs-for-4.21' of git://git.kernel.org/pub/scm/linux/kernel/git/jaegeuk/f2fs
    
    Pull f2fs updates from Jaegeuk Kim:
     "In this round, we've focused on bug fixes since Pixel devices have
      been shipping with f2fs. Some of them were related to hardware
      encryption support which are actually not an issue in mainline, but
      would be better to merge them in order to avoid potential bugs.
    
      Enhancements:
       - do GC sub-sections when the section is large
       - add a flag in ioctl(SHUTDOWN) to trigger fsck for QA
       - use kvmalloc() in order to give another chance to avoid ENOMEM
    
      Bug fixes:
       - fix accessing memory boundaries in a malformed iamge
       - GC gives stale unencrypted block
       - GC counts in large sections
       - detect idle time more precisely
       - block allocation of DIO writes
       - race conditions between write_begin and write_checkpoint
       - allow GCs for node segments via ioctl()
    
      There are various clean-ups and minor bug fixes as well"
    
    * tag 'f2fs-for-4.21' of git://git.kernel.org/pub/scm/linux/kernel/git/jaegeuk/f2fs: (43 commits)
      f2fs: sanity check of xattr entry size
      f2fs: fix use-after-free issue when accessing sbi->stat_info
      f2fs: check PageWriteback flag for ordered case
      f2fs: fix validation of the block count in sanity_check_raw_super
      f2fs: fix missing unlock(sbi->gc_mutex)
      f2fs: fix to dirty inode synchronously
      f2fs: clean up structure extent_node
      f2fs: fix block address for __check_sit_bitmap
      f2fs: fix sbi->extent_list corruption issue
      f2fs: clean up checkpoint flow
      f2fs: flush stale issued discard candidates
      f2fs: correct wrong spelling, issing_*
      f2fs: use kvmalloc, if kmalloc is failed
      f2fs: remove redundant comment of unused wio_mutex
      f2fs: fix to reorder set_page_dirty and wait_on_page_writeback
      f2fs: clear PG_writeback if IPU failed
      f2fs: add an ioctl() to explicitly trigger fsck later
      f2fs: avoid frequent costly fsck triggers
      f2fs: fix m_may_create to make OPU DIO write correctly
      f2fs: fix to update new block address correctly for OPU
      ...

commit ab41ee6879981b3d3a16a1079a33fa6fd043eb3c
Author: Jan Kara <jack@suse.cz>
Date:   Fri Dec 28 00:39:20 2018 -0800

    mm: migrate: drop unused argument of migrate_page_move_mapping()
    
    All callers of migrate_page_move_mapping() now pass NULL for 'head'
    argument.  Drop it.
    
    Link: http://lkml.kernel.org/r/20181211172143.7358-7-jack@suse.cz
    Signed-off-by: Jan Kara <jack@suse.cz>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Cc: Michal Hocko <mhocko@suse.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index b293cb3e27a2..008b74eff00d 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -2738,7 +2738,7 @@ int f2fs_migrate_page(struct address_space *mapping,
 	 */
 	extra_count = (atomic_written ? 1 : 0) - page_has_private(page);
 	rc = migrate_page_move_mapping(mapping, newpage,
-				page, NULL, mode, extra_count);
+				page, mode, extra_count);
 	if (rc != MIGRATEPAGE_SUCCESS) {
 		if (atomic_written)
 			mutex_unlock(&fi->inmem_lock);

commit bae0ee7a767ceeea6d8e170da3f228fbc7480331
Author: Chao Yu <yuchao0@huawei.com>
Date:   Tue Dec 25 17:43:42 2018 +0800

    f2fs: check PageWriteback flag for ordered case
    
    For all ordered cases in f2fs_wait_on_page_writeback(), we need to
    check PageWriteback status, so let's clean up to relocate the check
    into f2fs_wait_on_page_writeback().
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 59d86f692c84..a0524616f3f1 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -626,7 +626,7 @@ static void __set_data_blkaddr(struct dnode_of_data *dn)
  */
 void f2fs_set_data_blkaddr(struct dnode_of_data *dn)
 {
-	f2fs_wait_on_page_writeback(dn->node_page, NODE, true);
+	f2fs_wait_on_page_writeback(dn->node_page, NODE, true, true);
 	__set_data_blkaddr(dn);
 	if (set_page_dirty(dn->node_page))
 		dn->node_changed = true;
@@ -656,7 +656,7 @@ int f2fs_reserve_new_blocks(struct dnode_of_data *dn, blkcnt_t count)
 	trace_f2fs_reserve_new_blocks(dn->inode, dn->nid,
 						dn->ofs_in_node, count);
 
-	f2fs_wait_on_page_writeback(dn->node_page, NODE, true);
+	f2fs_wait_on_page_writeback(dn->node_page, NODE, true, true);
 
 	for (; count > 0; dn->ofs_in_node++) {
 		block_t blkaddr = datablock_addr(dn->inode,
@@ -2149,12 +2149,11 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 			if (PageWriteback(page)) {
 				if (wbc->sync_mode != WB_SYNC_NONE)
 					f2fs_wait_on_page_writeback(page,
-								DATA, true);
+							DATA, true, true);
 				else
 					goto continue_unlock;
 			}
 
-			BUG_ON(PageWriteback(page));
 			if (!clear_page_dirty_for_io(page))
 				goto continue_unlock;
 
@@ -2472,7 +2471,7 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 		}
 	}
 
-	f2fs_wait_on_page_writeback(page, DATA, false);
+	f2fs_wait_on_page_writeback(page, DATA, false, true);
 
 	if (len == PAGE_SIZE || PageUptodate(page))
 		return 0;

commit 5222595d093ebe80329d38d255d14316257afb3e
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Dec 13 18:38:33 2018 -0800

    f2fs: use kvmalloc, if kmalloc is failed
    
    One report says memalloc failure during mount.
    
     (unwind_backtrace) from [<c010cd4c>] (show_stack+0x10/0x14)
     (show_stack) from [<c049c6b8>] (dump_stack+0x8c/0xa0)
     (dump_stack) from [<c024fcf0>] (warn_alloc+0xc4/0x160)
     (warn_alloc) from [<c0250218>] (__alloc_pages_nodemask+0x3f4/0x10d0)
     (__alloc_pages_nodemask) from [<c0270450>] (kmalloc_order_trace+0x2c/0x120)
     (kmalloc_order_trace) from [<c03fa748>] (build_node_manager+0x35c/0x688)
     (build_node_manager) from [<c03de494>] (f2fs_fill_super+0xf0c/0x16cc)
     (f2fs_fill_super) from [<c02a5864>] (mount_bdev+0x15c/0x188)
     (mount_bdev) from [<c03da624>] (f2fs_mount+0x18/0x20)
     (f2fs_mount) from [<c02a68b8>] (mount_fs+0x158/0x19c)
     (mount_fs) from [<c02c3c9c>] (vfs_kern_mount+0x78/0x134)
     (vfs_kern_mount) from [<c02c76ac>] (do_mount+0x474/0xca4)
     (do_mount) from [<c02c8264>] (SyS_mount+0x94/0xbc)
     (SyS_mount) from [<c0108180>] (ret_fast_syscall+0x0/0x48)
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index fd3a1e5ab6d9..59d86f692c84 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -2573,7 +2573,7 @@ static void f2fs_dio_end_io(struct bio *bio)
 	bio->bi_private = dio->orig_private;
 	bio->bi_end_io = dio->orig_end_io;
 
-	kfree(dio);
+	kvfree(dio);
 
 	bio_endio(bio);
 }

commit 2062e0c3daa0bcfc806f4f7331f8bba16dd42632
Author: Sheng Yong <shengyong1@huawei.com>
Date:   Tue Dec 4 22:59:21 2018 +0800

    f2fs: clear PG_writeback if IPU failed
    
    If IPU failed, nothing is commited, we should end page writeback.
    
    Signed-off-by: Sheng Yong <shengyong1@huawei.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 7ddc0e57468c..fd3a1e5ab6d9 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1860,6 +1860,8 @@ int f2fs_do_write_data_page(struct f2fs_io_info *fio)
 		if (fio->need_lock == LOCK_REQ)
 			f2fs_unlock_op(fio->sbi);
 		err = f2fs_inplace_write_data(fio);
+		if (err && PageWriteback(page))
+			end_page_writeback(page);
 		trace_f2fs_do_write_data_page(fio->page, IPU);
 		set_inode_flag(inode, FI_UPDATE_WRITE);
 		return err;

commit f4f0b6777db4e7a4ba4f713d1d68f8e8f0ef421a
Author: Jia Zhu <zhujia13@huawei.com>
Date:   Tue Nov 20 04:29:35 2018 +0800

    f2fs: fix m_may_create to make OPU DIO write correctly
    
    Previously, we added a parameter @map.m_may_create to trigger OPU
    allocation and call f2fs_balance_fs() correctly.
    
    But in get_more_blocks(), @create has been overwritten by below code.
    So the function f2fs_map_blocks() will not allocate new block address
    but directly go out. Meanwile,there are several functions calling
    f2fs_map_blocks() directly and @map.m_may_create not initialized.
    CODE:
    create = dio->op == REQ_OP_WRITE;
            if (dio->flags & DIO_SKIP_HOLES) {
                    if (fs_startblk <= ((i_size_read(dio->inode) - 1) >>
                                                    i_blkbits))
                            create = 0;
            }
    
    This patch fixes it.
    
    Signed-off-by: Jia Zhu <zhujia13@huawei.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 3b7ccb31ad30..7ddc0e57468c 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1035,6 +1035,10 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 	end = pgofs + maxblocks;
 
 	if (!create && f2fs_lookup_extent_cache(inode, pgofs, &ei)) {
+		if (test_opt(sbi, LFS) && flag == F2FS_GET_BLOCK_DIO &&
+							map->m_may_create)
+			goto next_dnode;
+
 		map->m_pblk = ei.blk + pgofs - ei.fofs;
 		map->m_len = min((pgoff_t)maxblocks, ei.fofs + ei.len - pgofs);
 		map->m_flags = F2FS_MAP_MAPPED;
@@ -1246,6 +1250,7 @@ bool f2fs_overwrite_io(struct inode *inode, loff_t pos, size_t len)
 	map.m_next_pgofs = NULL;
 	map.m_next_extent = NULL;
 	map.m_seg_type = NO_CHECK_TYPE;
+	map.m_may_create = false;
 	last_lblk = F2FS_BLK_ALIGN(pos + len);
 
 	while (map.m_lblk < last_lblk) {

commit 73c0a9272a7d2942bcae29d4829bf63277cc57c8
Author: Jia Zhu <zhujia13@huawei.com>
Date:   Tue Nov 27 02:32:32 2018 +0800

    f2fs: fix to update new block address correctly for OPU
    
    Previously, we allocated a new block address for OPU mode in direct_IO.
    
    But the new address couldn't be assigned to @map->m_pblk correctly.
    
    This patch fix it.
    
    Cc: <stable@vger.kernel.org>
    Fixes: 511f52d02f05 ("f2fs: allow out-place-update for direct IO in LFS mode")
    Signed-off-by: Jia Zhu <zhujia13@huawei.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 8780f3d737c4..3b7ccb31ad30 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1089,8 +1089,10 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 		if (test_opt(sbi, LFS) && flag == F2FS_GET_BLOCK_DIO &&
 							map->m_may_create) {
 			err = __allocate_data_block(&dn, map->m_seg_type);
-			if (!err)
+			if (!err) {
+				blkaddr = dn.data_blkaddr;
 				set_inode_flag(inode, FI_APPEND_WRITE);
+			}
 		}
 	} else {
 		if (create) {

commit 2866fb16d67992195b0526d19e65acb6640fb87f
Author: Sheng Yong <shengyong1@huawei.com>
Date:   Wed Nov 14 19:34:28 2018 +0800

    f2fs: fix race between write_checkpoint and write_begin
    
    The following race could lead to inconsistent SIT bitmap:
    
    Task A                          Task B
    ======                          ======
    f2fs_write_checkpoint
      block_operations
        f2fs_lock_all
          down_write(node_change)
          down_write(node_write)
          ... sync ...
          up_write(node_change)
                                    f2fs_file_write_iter
                                      set_inode_flag(FI_NO_PREALLOC)
                                      ......
                                      f2fs_write_begin(index=0, has inline data)
                                        prepare_write_begin
                                          __do_map_lock(AIO) => down_read(node_change)
                                          f2fs_convert_inline_page => update SIT
                                          __do_map_lock(AIO) => up_read(node_change)
      f2fs_flush_sit_entries <= inconsistent SIT
      finish write checkpoint
      sudden-power-off
    
    If SPO occurs after checkpoint is finished, SIT bitmap will be set
    incorrectly.
    
    Signed-off-by: Sheng Yong <shengyong1@huawei.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 8aef1abc910a..8780f3d737c4 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -2322,6 +2322,7 @@ static int prepare_write_begin(struct f2fs_sb_info *sbi,
 	bool locked = false;
 	struct extent_info ei = {0,0,0};
 	int err = 0;
+	int flag;
 
 	/*
 	 * we already allocated all the blocks, so we don't need to get
@@ -2331,9 +2332,15 @@ static int prepare_write_begin(struct f2fs_sb_info *sbi,
 			!is_inode_flag_set(inode, FI_NO_PREALLOC))
 		return 0;
 
+	/* f2fs_lock_op avoids race between write CP and convert_inline_page */
+	if (f2fs_has_inline_data(inode) && pos + len > MAX_INLINE_DATA(inode))
+		flag = F2FS_GET_BLOCK_DEFAULT;
+	else
+		flag = F2FS_GET_BLOCK_PRE_AIO;
+
 	if (f2fs_has_inline_data(inode) ||
 			(pos & PAGE_MASK) >= i_size_read(inode)) {
-		__do_map_lock(sbi, F2FS_GET_BLOCK_PRE_AIO, true);
+		__do_map_lock(sbi, flag, true);
 		locked = true;
 	}
 restart:
@@ -2371,6 +2378,7 @@ static int prepare_write_begin(struct f2fs_sb_info *sbi,
 				f2fs_put_dnode(&dn);
 				__do_map_lock(sbi, F2FS_GET_BLOCK_PRE_AIO,
 								true);
+				WARN_ON(flag != F2FS_GET_BLOCK_PRE_AIO);
 				locked = true;
 				goto restart;
 			}
@@ -2384,7 +2392,7 @@ static int prepare_write_begin(struct f2fs_sb_info *sbi,
 	f2fs_put_dnode(&dn);
 unlock_out:
 	if (locked)
-		__do_map_lock(sbi, F2FS_GET_BLOCK_PRE_AIO, false);
+		__do_map_lock(sbi, flag, false);
 	return err;
 }
 

commit 1e771e83ce26d0ba2ce6c7df7effb7822f032c4a
Author: Yunlong Song <yunlong.song@huawei.com>
Date:   Tue Nov 13 11:57:32 2018 +0800

    f2fs: only flush the single temp bio cache which owns the target page
    
    Previously, when f2fs finds which temp bio cache owns the target page,
    it will flush all the three temp bio caches, but we only need to flush
    one single bio cache indeed, which can help to keep bio merged.
    
    Signed-off-by: Yunlong Song <yunlong.song@huawei.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index aa8843a3e5bc..8aef1abc910a 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -372,29 +372,6 @@ static bool __has_merged_page(struct f2fs_bio_info *io, struct inode *inode,
 	return false;
 }
 
-static bool has_merged_page(struct f2fs_sb_info *sbi, struct inode *inode,
-						struct page *page, nid_t ino,
-						enum page_type type)
-{
-	enum page_type btype = PAGE_TYPE_OF_BIO(type);
-	enum temp_type temp;
-	struct f2fs_bio_info *io;
-	bool ret = false;
-
-	for (temp = HOT; temp < NR_TEMP_TYPE; temp++) {
-		io = sbi->write_io[btype] + temp;
-
-		down_read(&io->io_rwsem);
-		ret = __has_merged_page(io, inode, page, ino);
-		up_read(&io->io_rwsem);
-
-		/* TODO: use HOT temp only for meta pages now. */
-		if (ret || btype == META)
-			break;
-	}
-	return ret;
-}
-
 static void __f2fs_submit_merged_write(struct f2fs_sb_info *sbi,
 				enum page_type type, enum temp_type temp)
 {
@@ -420,13 +397,19 @@ static void __submit_merged_write_cond(struct f2fs_sb_info *sbi,
 				nid_t ino, enum page_type type, bool force)
 {
 	enum temp_type temp;
-
-	if (!force && !has_merged_page(sbi, inode, page, ino, type))
-		return;
+	bool ret = true;
 
 	for (temp = HOT; temp < NR_TEMP_TYPE; temp++) {
+		if (!force)	{
+			enum page_type btype = PAGE_TYPE_OF_BIO(type);
+			struct f2fs_bio_info *io = sbi->write_io[btype] + temp;
 
-		__f2fs_submit_merged_write(sbi, type, temp);
+			down_read(&io->io_rwsem);
+			ret = __has_merged_page(io, inode, page, ino);
+			up_read(&io->io_rwsem);
+		}
+		if (ret)
+			__f2fs_submit_merged_write(sbi, type, temp);
 
 		/* TODO: use HOT temp only for meta pages now. */
 		if (type >= META)

commit f9d6d0597698c0d27ac54c7dca459a7a1b931315
Author: Chao Yu <yuchao0@huawei.com>
Date:   Tue Nov 13 14:33:45 2018 +0800

    f2fs: fix out-place-update DIO write
    
    In get_more_blocks(), we may override @create as below code:
    
            create = dio->op == REQ_OP_WRITE;
            if (dio->flags & DIO_SKIP_HOLES) {
                    if (fs_startblk <= ((i_size_read(dio->inode) - 1) >>
                                                    i_blkbits))
                            create = 0;
            }
    
    But in f2fs_map_blocks(), we only trigger f2fs_balance_fs() if @create
    is 1, so in LFS mode, dio overwrite under LFS mode can easily run out
    of free segments, result in below panic.
    
     Call Trace:
      allocate_segment_by_default+0xa8/0x270 [f2fs]
      f2fs_allocate_data_block+0x1ea/0x5c0 [f2fs]
      __allocate_data_block+0x306/0x480 [f2fs]
      f2fs_map_blocks+0x6f6/0x920 [f2fs]
      __get_data_block+0x4f/0xb0 [f2fs]
      get_data_block_dio_write+0x50/0x60 [f2fs]
      do_blockdev_direct_IO+0xcd5/0x21e0
      __blockdev_direct_IO+0x3a/0x3c
      f2fs_direct_IO+0x1ff/0x4a0 [f2fs]
      generic_file_direct_write+0xd9/0x160
      __generic_file_write_iter+0xbb/0x1e0
      f2fs_file_write_iter+0xaf/0x220 [f2fs]
      __vfs_write+0xd0/0x130
      vfs_write+0xb2/0x1b0
      SyS_pwrite64+0x69/0xa0
      ? vtime_user_exit+0x29/0x70
      do_syscall_64+0x6e/0x160
      entry_SYSCALL64_slow_path+0x25/0x25
     RIP: new_curseg+0x36f/0x380 [f2fs] RSP: ffffac570393f7a8
    
    So this patch introduces a parameter map.m_may_create to indicate that
    f2fs_map_blocks() is called from write or read path, which can give the
    right hint to let f2fs_map_blocks() trigger OPU allocation and call
    f2fs_balanc_fs() correctly.
    
    BTW, it disables physical address preallocation for direct IO in
    f2fs_preallocate_blocks, which is redundant to OPU allocation of
    f2fs_map_blocks.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 1d15535e3b42..aa8843a3e5bc 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -957,6 +957,9 @@ int f2fs_preallocate_blocks(struct kiocb *iocb, struct iov_iter *from)
 			return err;
 	}
 
+	if (direct_io && allow_outplace_dio(inode, iocb, from))
+		return 0;
+
 	if (is_inode_flag_set(inode, FI_NO_PREALLOC))
 		return 0;
 
@@ -970,6 +973,7 @@ int f2fs_preallocate_blocks(struct kiocb *iocb, struct iov_iter *from)
 	map.m_next_pgofs = NULL;
 	map.m_next_extent = NULL;
 	map.m_seg_type = NO_CHECK_TYPE;
+	map.m_may_create = true;
 
 	if (direct_io) {
 		map.m_seg_type = f2fs_rw_hint_to_seg_type(iocb->ki_hint);
@@ -1028,7 +1032,7 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 	unsigned int maxblocks = map->m_len;
 	struct dnode_of_data dn;
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	int mode = create ? ALLOC_NODE : LOOKUP_NODE;
+	int mode = map->m_may_create ? ALLOC_NODE : LOOKUP_NODE;
 	pgoff_t pgofs, end_offset, end;
 	int err = 0, ofs = 1;
 	unsigned int ofs_in_node, last_ofs_in_node;
@@ -1062,7 +1066,7 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 	}
 
 next_dnode:
-	if (create)
+	if (map->m_may_create)
 		__do_map_lock(sbi, flag, true);
 
 	/* When reading holes, we need its node page */
@@ -1099,8 +1103,8 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 
 	if (is_valid_data_blkaddr(sbi, blkaddr)) {
 		/* use out-place-update for driect IO under LFS mode */
-		if (test_opt(sbi, LFS) && create &&
-				flag == F2FS_GET_BLOCK_DIO) {
+		if (test_opt(sbi, LFS) && flag == F2FS_GET_BLOCK_DIO &&
+							map->m_may_create) {
 			err = __allocate_data_block(&dn, map->m_seg_type);
 			if (!err)
 				set_inode_flag(inode, FI_APPEND_WRITE);
@@ -1209,7 +1213,7 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 
 	f2fs_put_dnode(&dn);
 
-	if (create) {
+	if (map->m_may_create) {
 		__do_map_lock(sbi, flag, false);
 		f2fs_balance_fs(sbi, dn.node_changed);
 	}
@@ -1235,7 +1239,7 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 	}
 	f2fs_put_dnode(&dn);
 unlock_out:
-	if (create) {
+	if (map->m_may_create) {
 		__do_map_lock(sbi, flag, false);
 		f2fs_balance_fs(sbi, dn.node_changed);
 	}
@@ -1271,7 +1275,7 @@ bool f2fs_overwrite_io(struct inode *inode, loff_t pos, size_t len)
 
 static int __get_data_block(struct inode *inode, sector_t iblock,
 			struct buffer_head *bh, int create, int flag,
-			pgoff_t *next_pgofs, int seg_type)
+			pgoff_t *next_pgofs, int seg_type, bool may_write)
 {
 	struct f2fs_map_blocks map;
 	int err;
@@ -1281,6 +1285,7 @@ static int __get_data_block(struct inode *inode, sector_t iblock,
 	map.m_next_pgofs = next_pgofs;
 	map.m_next_extent = NULL;
 	map.m_seg_type = seg_type;
+	map.m_may_create = may_write;
 
 	err = f2fs_map_blocks(inode, &map, create, flag);
 	if (!err) {
@@ -1297,16 +1302,25 @@ static int get_data_block(struct inode *inode, sector_t iblock,
 {
 	return __get_data_block(inode, iblock, bh_result, create,
 							flag, next_pgofs,
-							NO_CHECK_TYPE);
+							NO_CHECK_TYPE, create);
+}
+
+static int get_data_block_dio_write(struct inode *inode, sector_t iblock,
+			struct buffer_head *bh_result, int create)
+{
+	return __get_data_block(inode, iblock, bh_result, create,
+				F2FS_GET_BLOCK_DIO, NULL,
+				f2fs_rw_hint_to_seg_type(inode->i_write_hint),
+				true);
 }
 
 static int get_data_block_dio(struct inode *inode, sector_t iblock,
 			struct buffer_head *bh_result, int create)
 {
 	return __get_data_block(inode, iblock, bh_result, create,
-						F2FS_GET_BLOCK_DIO, NULL,
-						f2fs_rw_hint_to_seg_type(
-							inode->i_write_hint));
+				F2FS_GET_BLOCK_DIO, NULL,
+				f2fs_rw_hint_to_seg_type(inode->i_write_hint),
+				false);
 }
 
 static int get_data_block_bmap(struct inode *inode, sector_t iblock,
@@ -1318,7 +1332,7 @@ static int get_data_block_bmap(struct inode *inode, sector_t iblock,
 
 	return __get_data_block(inode, iblock, bh_result, create,
 						F2FS_GET_BLOCK_BMAP, NULL,
-						NO_CHECK_TYPE);
+						NO_CHECK_TYPE, create);
 }
 
 static inline sector_t logical_to_blk(struct inode *inode, loff_t offset)
@@ -1525,6 +1539,7 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 	map.m_next_pgofs = NULL;
 	map.m_next_extent = NULL;
 	map.m_seg_type = NO_CHECK_TYPE;
+	map.m_may_create = false;
 
 	for (; nr_pages; nr_pages--) {
 		if (pages) {
@@ -2642,7 +2657,8 @@ static ssize_t f2fs_direct_IO(struct kiocb *iocb, struct iov_iter *iter)
 	}
 
 	err = __blockdev_direct_IO(iocb, inode, inode->i_sb->s_bdev,
-			iter, get_data_block_dio, NULL, f2fs_dio_submit_bio,
+			iter, rw == WRITE ? get_data_block_dio_write :
+			get_data_block_dio, NULL, f2fs_dio_submit_bio,
 			DIO_LOCKING | DIO_SKIP_HOLES);
 
 	if (do_opu)

commit 02b16d0a34a188a21d08be52b37505e531aa558a
Author: Chao Yu <yuchao0@huawei.com>
Date:   Mon Nov 12 00:46:46 2018 +0800

    f2fs: add to account direct IO
    
    This patch adds f2fs_dio_submit_bio() to hook submit_io/end_io functions
    in direct IO path, in order to account DIO.
    
    Later, we will add this count into is_idle() to let background GC/Discard
    thread be aware of DIO.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index b293cb3e27a2..1d15535e3b42 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -2548,6 +2548,53 @@ static int check_direct_IO(struct inode *inode, struct iov_iter *iter,
 	return 0;
 }
 
+static void f2fs_dio_end_io(struct bio *bio)
+{
+	struct f2fs_private_dio *dio = bio->bi_private;
+
+	dec_page_count(F2FS_I_SB(dio->inode),
+			dio->write ? F2FS_DIO_WRITE : F2FS_DIO_READ);
+
+	bio->bi_private = dio->orig_private;
+	bio->bi_end_io = dio->orig_end_io;
+
+	kfree(dio);
+
+	bio_endio(bio);
+}
+
+static void f2fs_dio_submit_bio(struct bio *bio, struct inode *inode,
+							loff_t file_offset)
+{
+	struct f2fs_private_dio *dio;
+	bool write = (bio_op(bio) == REQ_OP_WRITE);
+	int err;
+
+	dio = f2fs_kzalloc(F2FS_I_SB(inode),
+			sizeof(struct f2fs_private_dio), GFP_NOFS);
+	if (!dio) {
+		err = -ENOMEM;
+		goto out;
+	}
+
+	dio->inode = inode;
+	dio->orig_end_io = bio->bi_end_io;
+	dio->orig_private = bio->bi_private;
+	dio->write = write;
+
+	bio->bi_end_io = f2fs_dio_end_io;
+	bio->bi_private = dio;
+
+	inc_page_count(F2FS_I_SB(inode),
+			write ? F2FS_DIO_WRITE : F2FS_DIO_READ);
+
+	submit_bio(bio);
+	return;
+out:
+	bio->bi_status = BLK_STS_IOERR;
+	bio_endio(bio);
+}
+
 static ssize_t f2fs_direct_IO(struct kiocb *iocb, struct iov_iter *iter)
 {
 	struct address_space *mapping = iocb->ki_filp->f_mapping;
@@ -2594,7 +2641,9 @@ static ssize_t f2fs_direct_IO(struct kiocb *iocb, struct iov_iter *iter)
 			down_read(&fi->i_gc_rwsem[READ]);
 	}
 
-	err = blockdev_direct_IO(iocb, inode, iter, get_data_block_dio);
+	err = __blockdev_direct_IO(iocb, inode, inode->i_sb->s_bdev,
+			iter, get_data_block_dio, NULL, f2fs_dio_submit_bio,
+			DIO_LOCKING | DIO_SKIP_HOLES);
 
 	if (do_opu)
 		up_read(&fi->i_gc_rwsem[READ]);

commit dad4f140edaa3f6bb452b6913d41af1ffd672e45
Merge: 69d5b97c5973 3a08cd52c37c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Oct 28 11:35:40 2018 -0700

    Merge branch 'xarray' of git://git.infradead.org/users/willy/linux-dax
    
    Pull XArray conversion from Matthew Wilcox:
     "The XArray provides an improved interface to the radix tree data
      structure, providing locking as part of the API, specifying GFP flags
      at allocation time, eliminating preloading, less re-walking the tree,
      more efficient iterations and not exposing RCU-protected pointers to
      its users.
    
      This patch set
    
       1. Introduces the XArray implementation
    
       2. Converts the pagecache to use it
    
       3. Converts memremap to use it
    
      The page cache is the most complex and important user of the radix
      tree, so converting it was most important. Converting the memremap
      code removes the only other user of the multiorder code, which allows
      us to remove the radix tree code that supported it.
    
      I have 40+ followup patches to convert many other users of the radix
      tree over to the XArray, but I'd like to get this part in first. The
      other conversions haven't been in linux-next and aren't suitable for
      applying yet, but you can see them in the xarray-conv branch if you're
      interested"
    
    * 'xarray' of git://git.infradead.org/users/willy/linux-dax: (90 commits)
      radix tree: Remove multiorder support
      radix tree test: Convert multiorder tests to XArray
      radix tree tests: Convert item_delete_rcu to XArray
      radix tree tests: Convert item_kill_tree to XArray
      radix tree tests: Move item_insert_order
      radix tree test suite: Remove multiorder benchmarking
      radix tree test suite: Remove __item_insert
      memremap: Convert to XArray
      xarray: Add range store functionality
      xarray: Move multiorder_check to in-kernel tests
      xarray: Move multiorder_shrink to kernel tests
      xarray: Move multiorder account test in-kernel
      radix tree test suite: Convert iteration test to XArray
      radix tree test suite: Convert tag_tagged_items to XArray
      radix tree: Remove radix_tree_clear_tags
      radix tree: Remove radix_tree_maybe_preload_order
      radix tree: Remove split/join code
      radix tree: Remove radix_tree_update_node_t
      page cache: Finish XArray conversion
      dax: Convert page fault handlers to XArray
      ...

commit af033b2aa8a874fd5737fafe90d159136527b5b4
Author: Chao Yu <yuchao0@huawei.com>
Date:   Thu Sep 20 20:05:00 2018 +0800

    f2fs: guarantee journalled quota data by checkpoint
    
    For journalled quota mode, let checkpoint to flush dquot dirty data
    and quota file data to guarntee persistence of all quota sysfile in
    last checkpoint, by this way, we can avoid corrupting quota sysfile
    when encountering SPO.
    
    The implementation is as below:
    
    1. add a global state SBI_QUOTA_NEED_FLUSH to indicate that there is
    cached dquot metadata changes in quota subsystem, and later checkpoint
    should:
     a) flush dquot metadata into quota file.
     b) flush quota file to storage to keep file usage be consistent.
    
    2. add a global state SBI_QUOTA_NEED_REPAIR to indicate that quota
    operation failed due to -EIO or -ENOSPC, so later,
     a) checkpoint will skip syncing dquot metadata.
     b) CP_QUOTA_NEED_FSCK_FLAG will be set in last cp pack to give a
        hint for fsck repairing.
    
    3. add a global state SBI_QUOTA_SKIP_FLUSH, in checkpoint, if quota
    data updating is very heavy, it may cause hungtask in block_operation().
    To avoid this, if our retry time exceed threshold, let's just skip
    flushing and retry in next checkpoint().
    
    Signed-off-by: Weichao Guo <guoweichao@huawei.com>
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    [Jaegeuk Kim: avoid warnings and set fsck flag]
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 00b37a1bd15c..106f116466bf 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -46,7 +46,7 @@ static bool __is_cp_guaranteed(struct page *page)
 			inode->i_ino ==  F2FS_NODE_INO(sbi) ||
 			S_ISDIR(inode->i_mode) ||
 			(S_ISREG(inode->i_mode) &&
-			is_inode_flag_set(inode, FI_ATOMIC_FILE)) ||
+			(f2fs_is_atomic_file(inode) || IS_NOQUOTA(inode))) ||
 			is_cold_data(page))
 		return true;
 	return false;
@@ -1766,6 +1766,8 @@ bool f2fs_should_update_outplace(struct inode *inode, struct f2fs_io_info *fio)
 		return true;
 	if (S_ISDIR(inode->i_mode))
 		return true;
+	if (IS_NOQUOTA(inode))
+		return true;
 	if (f2fs_is_atomic_file(inode))
 		return true;
 	if (fio) {
@@ -2016,7 +2018,7 @@ static int __write_data_page(struct page *page, bool *submitted,
 	}
 
 	unlock_page(page);
-	if (!S_ISDIR(inode->i_mode))
+	if (!S_ISDIR(inode->i_mode) && !IS_NOQUOTA(inode))
 		f2fs_balance_fs(sbi, need_balance_fs);
 
 	if (unlikely(f2fs_cp_error(sbi))) {
@@ -2207,6 +2209,8 @@ static inline bool __should_serialize_io(struct inode *inode,
 {
 	if (!S_ISREG(inode->i_mode))
 		return false;
+	if (IS_NOQUOTA(inode))
+		return false;
 	if (wbc->sync_mode != WB_SYNC_ALL)
 		return true;
 	if (get_dirty_pages(inode) >= SM_I(F2FS_I_SB(inode))->min_seq_blocks)
@@ -2236,7 +2240,8 @@ static int __f2fs_write_data_pages(struct address_space *mapping,
 	if (unlikely(is_sbi_flag_set(sbi, SBI_POR_DOING)))
 		goto skip_write;
 
-	if (S_ISDIR(inode->i_mode) && wbc->sync_mode == WB_SYNC_NONE &&
+	if ((S_ISDIR(inode->i_mode) || IS_NOQUOTA(inode)) &&
+			wbc->sync_mode == WB_SYNC_NONE &&
 			get_dirty_pages(inode) < nr_pages_to_skip(sbi, DATA) &&
 			f2fs_available_free_memory(sbi, DIRTY_DENTS))
 		goto skip_write;
@@ -2301,7 +2306,7 @@ static void f2fs_write_failed(struct address_space *mapping, loff_t to)
 		down_write(&F2FS_I(inode)->i_mmap_sem);
 
 		truncate_pagecache(inode, i_size);
-		f2fs_truncate_blocks(inode, i_size, true);
+		f2fs_truncate_blocks(inode, i_size, true, true);
 
 		up_write(&F2FS_I(inode)->i_mmap_sem);
 		up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
@@ -2440,7 +2445,8 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 	if (err)
 		goto fail;
 
-	if (need_balance && has_not_enough_free_secs(sbi, 0, 0)) {
+	if (need_balance && !IS_NOQUOTA(inode) &&
+			has_not_enough_free_secs(sbi, 0, 0)) {
 		unlock_page(page);
 		f2fs_balance_fs(sbi, true);
 		lock_page(page);

commit 1e78e8bd9d107c351930cdb1e11202caec01b311
Author: Sahitya Tummala <stummala@codeaurora.org>
Date:   Wed Oct 10 10:56:22 2018 +0530

    f2fs: fix data corruption issue with hardware encryption
    
    Direct IO can be used in case of hardware encryption. The following
    scenario results into data corruption issue in this path -
    
    Thread A -                          Thread B-
    -> write file#1 in direct IO
                                        -> GC gets kicked in
                                        -> GC submitted bio on meta mapping
                                           for file#1, but pending completion
    -> write file#1 again with new data
       in direct IO
                                        -> GC bio gets completed now
                                        -> GC writes old data to the new
                                           location and thus file#1 is
                                           corrupted.
    
    Fix this by submitting and waiting for pending io on meta mapping
    for direct IO case in f2fs_map_blocks().
    
    Signed-off-by: Sahitya Tummala <stummala@codeaurora.org>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 8a831f17503e..00b37a1bd15c 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1053,6 +1053,11 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 		map->m_flags = F2FS_MAP_MAPPED;
 		if (map->m_next_extent)
 			*map->m_next_extent = pgofs + map->m_len;
+
+		/* for hardware encryption, but to avoid potential issue in future */
+		if (flag == F2FS_GET_BLOCK_DIO)
+			f2fs_wait_on_block_writeback_range(inode,
+						map->m_pblk, map->m_len);
 		goto out;
 	}
 
@@ -1211,6 +1216,12 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 	goto next_dnode;
 
 sync_out:
+
+	/* for hardware encryption, but to avoid potential issue in future */
+	if (flag == F2FS_GET_BLOCK_DIO && map->m_flags & F2FS_MAP_MAPPED)
+		f2fs_wait_on_block_writeback_range(inode,
+						map->m_pblk, map->m_len);
+
 	if (flag == F2FS_GET_BLOCK_PRECACHE) {
 		if (map->m_flags & F2FS_MAP_MAPPED) {
 			unsigned int ofs = start_pgofs - map->m_lblk;

commit 2baf07818549c8bb8d7b3437e889b86eab56d38e
Author: Chao Yu <yuchao0@huawei.com>
Date:   Fri Jul 27 18:15:16 2018 +0800

    f2fs: fix to spread clear_cold_data()
    
    We need to drop PG_checked flag on page as well when we clear PG_uptodate
    flag, in order to avoid treating the page as GCing one later.
    
    Signed-off-by: Weichao Guo <guoweichao@huawei.com>
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index bd6434ad3aa0..8a831f17503e 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1816,6 +1816,7 @@ int f2fs_do_write_data_page(struct f2fs_io_info *fio)
 	/* This page is already truncated */
 	if (fio->old_blkaddr == NULL_ADDR) {
 		ClearPageUptodate(page);
+		clear_cold_data(page);
 		goto out_writepage;
 	}
 got_it:
@@ -1991,8 +1992,10 @@ static int __write_data_page(struct page *page, bool *submitted,
 
 out:
 	inode_dec_dirty_pages(inode);
-	if (err)
+	if (err) {
 		ClearPageUptodate(page);
+		clear_cold_data(page);
+	}
 
 	if (wbc->for_reclaim) {
 		f2fs_submit_merged_write_cond(sbi, NULL, page, 0, DATA);
@@ -2621,6 +2624,8 @@ void f2fs_invalidate_page(struct page *page, unsigned int offset,
 		}
 	}
 
+	clear_cold_data(page);
+
 	/* This is atomic written page, keep Private */
 	if (IS_ATOMIC_WRITTEN_PAGE(page))
 		return f2fs_drop_inmem_page(inode, page);
@@ -2639,6 +2644,7 @@ int f2fs_release_page(struct page *page, gfp_t wait)
 	if (IS_ATOMIC_WRITTEN_PAGE(page))
 		return 0;
 
+	clear_cold_data(page);
 	set_page_private(page, 0);
 	ClearPagePrivate(page);
 	return 1;

commit 164a63fa6b384e30ceb96ed80bc7dc3379bc0960
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Tue Oct 16 19:30:13 2018 -0700

    Revert "f2fs: fix to clear PG_checked flag in set_page_dirty()"
    
    This reverts commit 66110abc4c931f879d70e83e1281f891699364bf.
    
    If we clear the cold data flag out of the writeback flow, we can miscount
    -1 by end_io, which incurs a deadlock caused by all I/Os being blocked during
    heavy GC.
    
    Balancing F2FS Async:
     - IO (CP:    1, Data:   -1, Flush: (   0    0    1), Discard: (   ...
    
    GC thread:                              IRQ
    - move_data_page()
     - set_page_dirty()
      - clear_cold_data()
                                            - f2fs_write_end_io()
                                             - type = WB_DATA_TYPE(page);
                                               here, we get wrong type
                                             - dec_page_count(sbi, type);
     - f2fs_wait_on_page_writeback()
    
    Cc: <stable@vger.kernel.org>
    Reported-and-Tested-by: Park Ju Hyung <qkrwngud825@gmail.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index b5de10102775..bd6434ad3aa0 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -2654,10 +2654,6 @@ static int f2fs_set_data_page_dirty(struct page *page)
 	if (!PageUptodate(page))
 		SetPageUptodate(page);
 
-	/* don't remain PG_checked flag which was set during GC */
-	if (is_cold_data(page))
-		clear_cold_data(page);
-
 	if (f2fs_is_atomic_file(inode) && !f2fs_is_commit_atomic_write(inode)) {
 		if (!IS_ATOMIC_WRITTEN_PAGE(page)) {
 			f2fs_register_inmem_page(inode, page);

commit 5f9abab42b60e67846cd13dafc6a61d70d7a2682
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Tue Oct 16 10:20:53 2018 -0700

    f2fs: account read IOs and use IO counts for is_idle
    
    This patch adds issued read IO counts which is under block layer.
    
    Chao modified a bit, since:
    
    Below race can cause reversed reference on F2FS_RD_DATA, there is
    the same issue in f2fs_submit_page_bio(), fix them by relocate
    __submit_bio() and inc_page_count.
    
    Thread A                        Thread B
    - f2fs_write_begin
     - f2fs_submit_page_read
     - __submit_bio
                                    - f2fs_read_end_io
                                     - __read_end_io
                                     - dec_page_count(, F2FS_RD_DATA)
     - inc_page_count(, F2FS_RD_DATA)
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 4b312b73d8a8..b5de10102775 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -52,6 +52,23 @@ static bool __is_cp_guaranteed(struct page *page)
 	return false;
 }
 
+static enum count_type __read_io_type(struct page *page)
+{
+	struct address_space *mapping = page->mapping;
+
+	if (mapping) {
+		struct inode *inode = mapping->host;
+		struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+
+		if (inode->i_ino == F2FS_META_INO(sbi))
+			return F2FS_RD_META;
+
+		if (inode->i_ino == F2FS_NODE_INO(sbi))
+			return F2FS_RD_NODE;
+	}
+	return F2FS_RD_DATA;
+}
+
 /* postprocessing steps for read bios */
 enum bio_post_read_step {
 	STEP_INITIAL = 0,
@@ -82,6 +99,7 @@ static void __read_end_io(struct bio *bio)
 		} else {
 			SetPageUptodate(page);
 		}
+		dec_page_count(F2FS_P_SB(page), __read_io_type(page));
 		unlock_page(page);
 	}
 	if (bio->bi_private)
@@ -466,8 +484,8 @@ int f2fs_submit_page_bio(struct f2fs_io_info *fio)
 
 	bio_set_op_attrs(bio, fio->op, fio->op_flags);
 
-	if (!is_read_io(fio->op))
-		inc_page_count(fio->sbi, WB_DATA_TYPE(fio->page));
+	inc_page_count(fio->sbi, is_read_io(fio->op) ?
+			__read_io_type(page): WB_DATA_TYPE(fio->page));
 
 	__submit_bio(fio->sbi, bio, fio->type);
 	return 0;
@@ -598,6 +616,7 @@ static int f2fs_submit_page_read(struct inode *inode, struct page *page,
 		return -EFAULT;
 	}
 	ClearPageError(page);
+	inc_page_count(F2FS_I_SB(inode), F2FS_RD_DATA);
 	__submit_bio(F2FS_I_SB(inode), bio, DATA);
 	return 0;
 }
@@ -1586,6 +1605,7 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 		if (bio_add_page(bio, page, blocksize, 0) < blocksize)
 			goto submit_and_realloc;
 
+		inc_page_count(F2FS_I_SB(inode), F2FS_RD_DATA);
 		ClearPageError(page);
 		last_block_in_bio = block_nr;
 		goto next_page;

commit 78efac537de33faab9a4302cc05a70bb4a8b3b63
Author: Chao Yu <yuchao0@huawei.com>
Date:   Mon Oct 22 23:24:28 2018 +0800

    f2fs: fix to account IO correctly for cgroup writeback
    
    Now, we have supported cgroup writeback, it depends on correctly IO
    account of specified filesystem.
    
    But in commit d1b3e72d5490 ("f2fs: submit bio of in-place-update pages"),
    we split write paths from f2fs_submit_page_mbio() to two:
    - f2fs_submit_page_bio() for IPU path
    - f2fs_submit_page_bio() for OPU path
    
    But still we account write IO only in f2fs_submit_page_mbio(), result in
    incorrect IO account, fix it by adding missing IO account in IPU path.
    
    Fixes: d1b3e72d5490 ("f2fs: submit bio of in-place-update pages")
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 83b1983094bd..4b312b73d8a8 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -460,6 +460,10 @@ int f2fs_submit_page_bio(struct f2fs_io_info *fio)
 		bio_put(bio);
 		return -EFAULT;
 	}
+
+	if (fio->io_wbc && !is_read_io(fio->op))
+		wbc_account_io(fio->io_wbc, page, PAGE_SIZE);
+
 	bio_set_op_attrs(bio, fio->op, fio->op_flags);
 
 	if (!is_read_io(fio->op))

commit 4c58ed076875f36dae0f240da1e25e99e5d4afb8
Author: Chao Yu <yuchao0@huawei.com>
Date:   Mon Oct 22 09:12:51 2018 +0800

    f2fs: fix to account IO correctly
    
    Below race can cause reversed reference on dirty count, fix it by
    relocating __submit_bio() and inc_page_count().
    
    Thread A                                Thread B
    - f2fs_inplace_write_data
     - f2fs_submit_page_bio
      - __submit_bio
                                            - f2fs_write_end_io
                                             - dec_page_count
      - inc_page_count
    
    Cc: <stable@vger.kernel.org>
    Fixes: d1b3e72d5490 ("f2fs: submit bio of in-place-update pages")
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 9ef6f1f01eda..83b1983094bd 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -462,10 +462,10 @@ int f2fs_submit_page_bio(struct f2fs_io_info *fio)
 	}
 	bio_set_op_attrs(bio, fio->op, fio->op_flags);
 
-	__submit_bio(fio->sbi, bio, fio->type);
-
 	if (!is_read_io(fio->op))
 		inc_page_count(fio->sbi, WB_DATA_TYPE(fio->page));
+
+	__submit_bio(fio->sbi, bio, fio->type);
 	return 0;
 }
 

commit 5ec2d99de7427c84bb7250d23f5acf49a3670a63
Author: Matthew Wilcox <willy@infradead.org>
Date:   Mon Dec 4 20:25:25 2017 -0500

    f2fs: Convert to XArray
    
    This is a straightforward conversion.
    
    Signed-off-by: Matthew Wilcox <willy@infradead.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 5b760809eecc..6962491172a5 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -2697,13 +2697,13 @@ const struct address_space_operations f2fs_dblock_aops = {
 #endif
 };
 
-void f2fs_clear_radix_tree_dirty_tag(struct page *page)
+void f2fs_clear_page_cache_dirty_tag(struct page *page)
 {
 	struct address_space *mapping = page_mapping(page);
 	unsigned long flags;
 
 	xa_lock_irqsave(&mapping->i_pages, flags);
-	radix_tree_tag_clear(&mapping->i_pages, page_index(page),
+	__xa_clear_mark(&mapping->i_pages, page_index(page),
 						PAGECACHE_TAG_DIRTY);
 	xa_unlock_irqrestore(&mapping->i_pages, flags);
 }

commit 10bbd235859bf483f9a8a4ebe95463d700bae394
Author: Matthew Wilcox <willy@infradead.org>
Date:   Tue Dec 5 17:30:38 2017 -0500

    pagevec: Use xa_mark_t
    
    Removes sparse warnings.
    
    Signed-off-by: Matthew Wilcox <willy@infradead.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 382c1ef9a9e4..5b760809eecc 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -2003,7 +2003,7 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 	pgoff_t last_idx = ULONG_MAX;
 	int cycled;
 	int range_whole = 0;
-	int tag;
+	xa_mark_t tag;
 
 	pagevec_init(&pvec);
 

commit 4354994f097d068a894aa1a0860da54571df3582
Author: Daniel Rosenberg <drosen@google.com>
Date:   Mon Aug 20 19:21:43 2018 -0700

    f2fs: checkpoint disabling
    
    Note that, it requires "f2fs: return correct errno in f2fs_gc".
    
    This adds a lightweight non-persistent snapshotting scheme to f2fs.
    
    To use, mount with the option checkpoint=disable, and to return to
    normal operation, remount with checkpoint=enable. If the filesystem
    is shut down before remounting with checkpoint=enable, it will revert
    back to its apparent state when it was first mounted with
    checkpoint=disable. This is useful for situations where you wish to be
    able to roll back the state of the disk in case of some critical
    failure.
    
    Signed-off-by: Daniel Rosenberg <drosen@google.com>
    [Jaegeuk Kim: use SB_RDONLY instead of MS_RDONLY]
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 873f9ea7769f..9ef6f1f01eda 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -537,7 +537,8 @@ void f2fs_submit_page_write(struct f2fs_io_info *fio)
 	if (fio->in_list)
 		goto next;
 out:
-	if (is_sbi_flag_set(sbi, SBI_IS_SHUTDOWN))
+	if (is_sbi_flag_set(sbi, SBI_IS_SHUTDOWN) ||
+				f2fs_is_checkpoint_ready(sbi))
 		__submit_merged_bio(io);
 	up_write(&io->io_rwsem);
 }
@@ -1703,6 +1704,10 @@ static inline bool check_inplace_update_policy(struct inode *inode,
 			is_inode_flag_set(inode, FI_NEED_IPU))
 		return true;
 
+	if (unlikely(fio && is_sbi_flag_set(sbi, SBI_CP_DISABLED) &&
+			!f2fs_is_checkpointed_data(sbi, fio->old_blkaddr)))
+		return true;
+
 	return false;
 }
 
@@ -1733,6 +1738,9 @@ bool f2fs_should_update_outplace(struct inode *inode, struct f2fs_io_info *fio)
 			return true;
 		if (IS_ATOMIC_WRITTEN_PAGE(fio->page))
 			return true;
+		if (unlikely(is_sbi_flag_set(sbi, SBI_CP_DISABLED) &&
+			f2fs_is_checkpointed_data(sbi, fio->old_blkaddr)))
+			return true;
 	}
 	return false;
 }
@@ -2353,6 +2361,10 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 
 	trace_f2fs_write_begin(inode, pos, len, flags);
 
+	err = f2fs_is_checkpoint_ready(sbi);
+	if (err)
+		goto fail;
+
 	if ((f2fs_is_atomic_file(inode) &&
 			!f2fs_available_free_memory(sbi, INMEM_PAGES)) ||
 			is_inode_flag_set(inode, FI_ATOMIC_REVOKE_REQUEST)) {

commit fb7d70db305a1446864227abf711b756568f8242
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Tue Sep 25 13:54:33 2018 -0700

    f2fs: clear PageError on the read path
    
    When running fault injection test, I hit somewhat wrong behavior in f2fs_gc ->
    gc_data_segment():
    
    0. fault injection generated some PageError'ed pages
    
    1. gc_data_segment
     -> f2fs_get_read_data_page(REQ_RAHEAD)
    
    2. move_data_page
     -> f2fs_get_lock_data_page()
      -> f2f_get_read_data_page()
       -> f2fs_submit_page_read()
        -> submit_bio(READ)
      -> return EIO due to PageError
      -> fail to move data
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 02d5ce888a4a..873f9ea7769f 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -77,7 +77,8 @@ static void __read_end_io(struct bio *bio)
 		/* PG_error was set if any post_read step failed */
 		if (bio->bi_status || PageError(page)) {
 			ClearPageUptodate(page);
-			SetPageError(page);
+			/* will re-read again later */
+			ClearPageError(page);
 		} else {
 			SetPageUptodate(page);
 		}
@@ -591,6 +592,7 @@ static int f2fs_submit_page_read(struct inode *inode, struct page *page,
 		bio_put(bio);
 		return -EFAULT;
 	}
+	ClearPageError(page);
 	__submit_bio(F2FS_I_SB(inode), bio, DATA);
 	return 0;
 }
@@ -1579,6 +1581,7 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 		if (bio_add_page(bio, page, blocksize, 0) < blocksize)
 			goto submit_and_realloc;
 
+		ClearPageError(page);
 		last_block_in_bio = block_nr;
 		goto next_page;
 set_error_page:

commit f847c699cff3f050286ee0a08632046468e7a511
Author: Chao Yu <yuchao0@huawei.com>
Date:   Thu Sep 27 18:34:52 2018 +0800

    f2fs: allow out-place-update for direct IO in LFS mode
    
    Normally, DIO uses in-pllace-update, but in LFS mode, f2fs doesn't
    allow triggering any in-place-update writes, so we fallback direct
    write to buffered write, result in bad performance of large size
    write.
    
    This patch adds to support triggering out-place-update for direct IO
    to enhance its performance.
    
    Note that it needs to exclude direct read IO during direct write,
    since new data writing to new block address will no be valid until
    write finished.
    
    storage: zram
    
    time xfs_io -f -d /mnt/f2fs/file -c "pwrite 0 1073741824" -c "fsync"
    
    Before:
    real    0m13.061s
    user    0m0.327s
    sys     0m12.486s
    
    After:
    real    0m6.448s
    user    0m0.228s
    sys     0m6.212s
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 3f01bc2d73eb..02d5ce888a4a 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -893,7 +893,7 @@ static int __allocate_data_block(struct dnode_of_data *dn, int seg_type)
 
 	dn->data_blkaddr = datablock_addr(dn->inode,
 				dn->node_page, dn->ofs_in_node);
-	if (dn->data_blkaddr == NEW_ADDR)
+	if (dn->data_blkaddr != NULL_ADDR)
 		goto alloc;
 
 	if (unlikely((err = inc_valid_block_count(sbi, dn->inode, &count))))
@@ -947,7 +947,7 @@ int f2fs_preallocate_blocks(struct kiocb *iocb, struct iov_iter *from)
 
 	if (direct_io) {
 		map.m_seg_type = f2fs_rw_hint_to_seg_type(iocb->ki_hint);
-		flag = f2fs_force_buffered_io(inode, WRITE) ?
+		flag = f2fs_force_buffered_io(inode, iocb, from) ?
 					F2FS_GET_BLOCK_PRE_AIO :
 					F2FS_GET_BLOCK_PRE_DIO;
 		goto map_blocks;
@@ -1066,7 +1066,15 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 		goto sync_out;
 	}
 
-	if (!is_valid_data_blkaddr(sbi, blkaddr)) {
+	if (is_valid_data_blkaddr(sbi, blkaddr)) {
+		/* use out-place-update for driect IO under LFS mode */
+		if (test_opt(sbi, LFS) && create &&
+				flag == F2FS_GET_BLOCK_DIO) {
+			err = __allocate_data_block(&dn, map->m_seg_type);
+			if (!err)
+				set_inode_flag(inode, FI_APPEND_WRITE);
+		}
+	} else {
 		if (create) {
 			if (unlikely(f2fs_cp_error(sbi))) {
 				err = -EIO;
@@ -2486,36 +2494,53 @@ static ssize_t f2fs_direct_IO(struct kiocb *iocb, struct iov_iter *iter)
 	struct address_space *mapping = iocb->ki_filp->f_mapping;
 	struct inode *inode = mapping->host;
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct f2fs_inode_info *fi = F2FS_I(inode);
 	size_t count = iov_iter_count(iter);
 	loff_t offset = iocb->ki_pos;
 	int rw = iov_iter_rw(iter);
 	int err;
 	enum rw_hint hint = iocb->ki_hint;
 	int whint_mode = F2FS_OPTION(sbi).whint_mode;
+	bool do_opu;
 
 	err = check_direct_IO(inode, iter, offset);
 	if (err)
 		return err < 0 ? err : 0;
 
-	if (f2fs_force_buffered_io(inode, rw))
+	if (f2fs_force_buffered_io(inode, iocb, iter))
 		return 0;
 
+	do_opu = allow_outplace_dio(inode, iocb, iter);
+
 	trace_f2fs_direct_IO_enter(inode, offset, count, rw);
 
 	if (rw == WRITE && whint_mode == WHINT_MODE_OFF)
 		iocb->ki_hint = WRITE_LIFE_NOT_SET;
 
-	if (!down_read_trylock(&F2FS_I(inode)->i_gc_rwsem[rw])) {
-		if (iocb->ki_flags & IOCB_NOWAIT) {
+	if (iocb->ki_flags & IOCB_NOWAIT) {
+		if (!down_read_trylock(&fi->i_gc_rwsem[rw])) {
+			iocb->ki_hint = hint;
+			err = -EAGAIN;
+			goto out;
+		}
+		if (do_opu && !down_read_trylock(&fi->i_gc_rwsem[READ])) {
+			up_read(&fi->i_gc_rwsem[rw]);
 			iocb->ki_hint = hint;
 			err = -EAGAIN;
 			goto out;
 		}
-		down_read(&F2FS_I(inode)->i_gc_rwsem[rw]);
+	} else {
+		down_read(&fi->i_gc_rwsem[rw]);
+		if (do_opu)
+			down_read(&fi->i_gc_rwsem[READ]);
 	}
 
 	err = blockdev_direct_IO(iocb, inode, iter, get_data_block_dio);
-	up_read(&F2FS_I(inode)->i_gc_rwsem[rw]);
+
+	if (do_opu)
+		up_read(&fi->i_gc_rwsem[READ]);
+
+	up_read(&fi->i_gc_rwsem[rw]);
 
 	if (rw == WRITE) {
 		if (whint_mode == WHINT_MODE_OFF)
@@ -2523,7 +2548,8 @@ static ssize_t f2fs_direct_IO(struct kiocb *iocb, struct iov_iter *iter)
 		if (err > 0) {
 			f2fs_update_iostat(F2FS_I_SB(inode), APP_DIRECT_IO,
 									err);
-			set_inode_flag(inode, FI_UPDATE_WRITE);
+			if (!do_opu)
+				set_inode_flag(inode, FI_UPDATE_WRITE);
 		} else if (err < 0) {
 			f2fs_write_failed(mapping, offset + count);
 		}

commit 39a8695824510a951ded696d69b8dea3c720b109
Author: Chao Yu <yuchao0@huawei.com>
Date:   Thu Sep 27 18:33:18 2018 +0800

    f2fs: refactor ->page_mkwrite() flow
    
    Thread A                                Thread B
    - f2fs_vm_page_mkwrite
                                            - f2fs_setattr
                                             - down_write(i_mmap_sem)
                                             - truncate_setsize
                                             - f2fs_truncate
                                             - up_write(i_mmap_sem)
     - f2fs_reserve_block
     reserve NEW_ADDR
     - skip dirty page due to truncation
    
    1. we don't need to rserve new block address for a truncated page.
    2. dn.data_blkaddr is used out of node page lock coverage.
    
    Refactor ->page_mkwrite() flow to fix above issues:
    - use __do_map_lock() to avoid racing checkpoint()
    - lock data page in prior to dnode page
    - cover f2fs_reserve_block with i_mmap_sem lock
    - wait page writeback before zeroing page
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index ea16cadd416e..3f01bc2d73eb 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -972,7 +972,7 @@ int f2fs_preallocate_blocks(struct kiocb *iocb, struct iov_iter *from)
 	return err;
 }
 
-static inline void __do_map_lock(struct f2fs_sb_info *sbi, int flag, bool lock)
+void __do_map_lock(struct f2fs_sb_info *sbi, int flag, bool lock)
 {
 	if (flag == F2FS_GET_BLOCK_PRE_AIO) {
 		if (lock)

commit bab475c5414e8d1fa182fd17ae966864e9c85741
Author: Chao Yu <yuchao0@huawei.com>
Date:   Thu Sep 27 23:41:16 2018 +0800

    Revert: "f2fs: check last page index in cached bio to decide submission"
    
    There is one case that we can leave bio in f2fs, result in hanging
    page writeback waiter.
    
    Thread A                                Thread B
    - f2fs_write_cache_pages
     - f2fs_submit_page_write
     page #0 cached in bio #0 of cold log
     - f2fs_submit_page_write
     page #1 cached in bio #1 of warm log
                                            - f2fs_write_cache_pages
                                             - f2fs_submit_page_write
                                             bio is full, submit bio #1 contain page #1
     - f2fs_submit_merged_write_cond(, page #1)
     fail to submit bio #0 due to page #1 is not in any cached bios.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 41102c227eee..ea16cadd416e 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -322,8 +322,8 @@ static void __submit_merged_bio(struct f2fs_bio_info *io)
 	io->bio = NULL;
 }
 
-static bool __has_merged_page(struct f2fs_bio_info *io,
-				struct inode *inode, nid_t ino, pgoff_t idx)
+static bool __has_merged_page(struct f2fs_bio_info *io, struct inode *inode,
+						struct page *page, nid_t ino)
 {
 	struct bio_vec *bvec;
 	struct page *target;
@@ -332,7 +332,7 @@ static bool __has_merged_page(struct f2fs_bio_info *io,
 	if (!io->bio)
 		return false;
 
-	if (!inode && !ino)
+	if (!inode && !page && !ino)
 		return true;
 
 	bio_for_each_segment_all(bvec, io->bio, i) {
@@ -342,11 +342,10 @@ static bool __has_merged_page(struct f2fs_bio_info *io,
 		else
 			target = fscrypt_control_page(bvec->bv_page);
 
-		if (idx != target->index)
-			continue;
-
 		if (inode && inode == target->mapping->host)
 			return true;
+		if (page && page == target)
+			return true;
 		if (ino && ino == ino_of_node(target))
 			return true;
 	}
@@ -355,7 +354,8 @@ static bool __has_merged_page(struct f2fs_bio_info *io,
 }
 
 static bool has_merged_page(struct f2fs_sb_info *sbi, struct inode *inode,
-				nid_t ino, pgoff_t idx, enum page_type type)
+						struct page *page, nid_t ino,
+						enum page_type type)
 {
 	enum page_type btype = PAGE_TYPE_OF_BIO(type);
 	enum temp_type temp;
@@ -366,7 +366,7 @@ static bool has_merged_page(struct f2fs_sb_info *sbi, struct inode *inode,
 		io = sbi->write_io[btype] + temp;
 
 		down_read(&io->io_rwsem);
-		ret = __has_merged_page(io, inode, ino, idx);
+		ret = __has_merged_page(io, inode, page, ino);
 		up_read(&io->io_rwsem);
 
 		/* TODO: use HOT temp only for meta pages now. */
@@ -397,12 +397,12 @@ static void __f2fs_submit_merged_write(struct f2fs_sb_info *sbi,
 }
 
 static void __submit_merged_write_cond(struct f2fs_sb_info *sbi,
-				struct inode *inode, nid_t ino, pgoff_t idx,
-				enum page_type type, bool force)
+				struct inode *inode, struct page *page,
+				nid_t ino, enum page_type type, bool force)
 {
 	enum temp_type temp;
 
-	if (!force && !has_merged_page(sbi, inode, ino, idx, type))
+	if (!force && !has_merged_page(sbi, inode, page, ino, type))
 		return;
 
 	for (temp = HOT; temp < NR_TEMP_TYPE; temp++) {
@@ -421,10 +421,10 @@ void f2fs_submit_merged_write(struct f2fs_sb_info *sbi, enum page_type type)
 }
 
 void f2fs_submit_merged_write_cond(struct f2fs_sb_info *sbi,
-				struct inode *inode, nid_t ino, pgoff_t idx,
-				enum page_type type)
+				struct inode *inode, struct page *page,
+				nid_t ino, enum page_type type)
 {
-	__submit_merged_write_cond(sbi, inode, ino, idx, type, false);
+	__submit_merged_write_cond(sbi, inode, page, ino, type, false);
 }
 
 void f2fs_flush_merged_writes(struct f2fs_sb_info *sbi)
@@ -1952,7 +1952,7 @@ static int __write_data_page(struct page *page, bool *submitted,
 		ClearPageUptodate(page);
 
 	if (wbc->for_reclaim) {
-		f2fs_submit_merged_write_cond(sbi, inode, 0, page->index, DATA);
+		f2fs_submit_merged_write_cond(sbi, NULL, page, 0, DATA);
 		clear_inode_flag(inode, FI_HOT_DATA);
 		f2fs_remove_dirty_inode(inode);
 		submitted = NULL;
@@ -2010,10 +2010,10 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 	pgoff_t index;
 	pgoff_t end;		/* Inclusive */
 	pgoff_t done_index;
-	pgoff_t last_idx = ULONG_MAX;
 	int cycled;
 	int range_whole = 0;
 	int tag;
+	int nwritten = 0;
 
 	pagevec_init(&pvec);
 
@@ -2116,7 +2116,7 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 				done = 1;
 				break;
 			} else if (submitted) {
-				last_idx = page->index;
+				nwritten++;
 			}
 
 			if (--wbc->nr_to_write <= 0 &&
@@ -2138,9 +2138,9 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 	if (wbc->range_cyclic || (range_whole && wbc->nr_to_write > 0))
 		mapping->writeback_index = done_index;
 
-	if (last_idx != ULONG_MAX)
+	if (nwritten)
 		f2fs_submit_merged_write_cond(F2FS_M_SB(mapping), mapping->host,
-						0, last_idx, DATA);
+								NULL, 0, DATA);
 
 	return ret;
 }

commit 0a4daae5ffea39f5015334e4d18a6a80b447cae4
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed Sep 19 15:28:40 2018 -0700

    f2fs: update i_size after DIO completion
    
    This is related to
    ee70daaba82d ("xfs: update i_size after unwritten conversion in dio completion")
    
    If we update i_size during dio_write, dio_read can read out stale data, which
    breaks xfstests/465.
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 57c0823d22e0..41102c227eee 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -881,7 +881,6 @@ static int __allocate_data_block(struct dnode_of_data *dn, int seg_type)
 	struct f2fs_summary sum;
 	struct node_info ni;
 	block_t old_blkaddr;
-	pgoff_t fofs;
 	blkcnt_t count = 1;
 	int err;
 
@@ -910,12 +909,10 @@ static int __allocate_data_block(struct dnode_of_data *dn, int seg_type)
 					old_blkaddr, old_blkaddr);
 	f2fs_set_data_blkaddr(dn);
 
-	/* update i_size */
-	fofs = f2fs_start_bidx_of_node(ofs_of_node(dn->node_page), dn->inode) +
-							dn->ofs_in_node;
-	if (i_size_read(dn->inode) < ((loff_t)(fofs + 1) << PAGE_SHIFT))
-		f2fs_i_size_write(dn->inode,
-				((loff_t)(fofs + 1) << PAGE_SHIFT));
+	/*
+	 * i_size will be updated by direct_IO. Otherwise, we'll get stale
+	 * data from unwritten block via dio_read.
+	 */
 	return 0;
 }
 
@@ -1081,6 +1078,8 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 					last_ofs_in_node = dn.ofs_in_node;
 				}
 			} else {
+				WARN_ON(flag != F2FS_GET_BLOCK_PRE_DIO &&
+					flag != F2FS_GET_BLOCK_DIO);
 				err = __allocate_data_block(&dn,
 							map->m_seg_type);
 				if (!err)
@@ -1260,7 +1259,7 @@ static int get_data_block_dio(struct inode *inode, sector_t iblock,
 			struct buffer_head *bh_result, int create)
 {
 	return __get_data_block(inode, iblock, bh_result, create,
-						F2FS_GET_BLOCK_DEFAULT, NULL,
+						F2FS_GET_BLOCK_DIO, NULL,
 						f2fs_rw_hint_to_seg_type(
 							inode->i_write_hint));
 }

commit 6f5c2ed0a26fae6904a88622c126dfb9369548a3
Author: Chao Yu <yuchao0@huawei.com>
Date:   Wed Sep 12 09:22:29 2018 +0800

    f2fs: split IO error injection according to RW
    
    This patch adds to support injecting error for write IO, this can simulate
    IO error like fail_make_request or dm_flakey does.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index c8c4b54e2bbf..57c0823d22e0 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -123,8 +123,9 @@ static bool f2fs_bio_post_read_required(struct bio *bio)
 
 static void f2fs_read_end_io(struct bio *bio)
 {
-	if (time_to_inject(F2FS_P_SB(bio_first_page_all(bio)), FAULT_IO)) {
-		f2fs_show_injection_info(FAULT_IO);
+	if (time_to_inject(F2FS_P_SB(bio_first_page_all(bio)),
+						FAULT_READ_IO)) {
+		f2fs_show_injection_info(FAULT_READ_IO);
 		bio->bi_status = BLK_STS_IOERR;
 	}
 
@@ -145,6 +146,11 @@ static void f2fs_write_end_io(struct bio *bio)
 	struct bio_vec *bvec;
 	int i;
 
+	if (time_to_inject(sbi, FAULT_WRITE_IO)) {
+		f2fs_show_injection_info(FAULT_WRITE_IO);
+		bio->bi_status = BLK_STS_IOERR;
+	}
+
 	bio_for_each_segment_all(bvec, bio, i) {
 		struct page *page = bvec->bv_page;
 		enum count_type type = WB_DATA_TYPE(page);

commit 7c1a000d466235c875a989971cfda344e6bb1166
Author: Chao Yu <yuchao0@huawei.com>
Date:   Wed Sep 12 09:16:07 2018 +0800

    f2fs: add SPDX license identifiers
    
    Remove the verbose license text from f2fs files and replace them with
    SPDX tags.  This does not change the license of any of the code.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 26f38b224bb2..c8c4b54e2bbf 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1,12 +1,9 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * fs/f2fs/data.c
  *
  * Copyright (c) 2012 Samsung Electronics Co., Ltd.
  *             http://www.samsung.com/
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
  */
 #include <linux/fs.h>
 #include <linux/f2fs_fs.h>

commit 5ce805869cbed93267ed26552ff76e30f05c91f7
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Sep 6 11:40:12 2018 -0700

    f2fs: submit bio after shutdown
    
    Sometimes, some merged IOs could get a chance to be submitted, resulting in
    system hang in shutdown test. This issues IOs all the time after shutdown.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 8c204f896c22..26f38b224bb2 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -533,6 +533,8 @@ void f2fs_submit_page_write(struct f2fs_io_info *fio)
 	if (fio->in_list)
 		goto next;
 out:
+	if (is_sbi_flag_set(sbi, SBI_IS_SHUTDOWN))
+		__submit_merged_bio(io);
 	up_write(&io->io_rwsem);
 }
 

commit 0ded69f632bb717be9aeea3ae74e29050fcb060c
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed Aug 22 21:18:00 2018 -0700

    f2fs: avoid wrong decrypted data from disk
    
    1. Create a file in an encrypted directory
    2. Do GC & drop caches
    3. Read stale data before its bio for metapage was not issued yet
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 382c1ef9a9e4..8c204f896c22 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -565,9 +565,6 @@ static struct bio *f2fs_grab_read_bio(struct inode *inode, block_t blkaddr,
 		ctx->bio = bio;
 		ctx->enabled_steps = post_read_steps;
 		bio->bi_private = ctx;
-
-		/* wait the page to be moved by cleaning */
-		f2fs_wait_on_block_writeback(sbi, blkaddr);
 	}
 
 	return bio;
@@ -582,6 +579,9 @@ static int f2fs_submit_page_read(struct inode *inode, struct page *page,
 	if (IS_ERR(bio))
 		return PTR_ERR(bio);
 
+	/* wait for GCed page writeback via META_MAPPING */
+	f2fs_wait_on_block_writeback(inode, blkaddr);
+
 	if (bio_add_page(bio, page, PAGE_SIZE, 0) < PAGE_SIZE) {
 		bio_put(bio);
 		return -EFAULT;
@@ -1558,6 +1558,12 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 			}
 		}
 
+		/*
+		 * If the page is under writeback, we need to wait for
+		 * its completion to see the correct decrypted data.
+		 */
+		f2fs_wait_on_block_writeback(inode, block_nr);
+
 		if (bio_add_page(bio, page, blocksize, 0) < blocksize)
 			goto submit_and_realloc;
 
@@ -1625,7 +1631,7 @@ static int encrypt_one_page(struct f2fs_io_info *fio)
 		return 0;
 
 	/* wait for GCed page writeback via META_MAPPING */
-	f2fs_wait_on_block_writeback(fio->sbi, fio->old_blkaddr);
+	f2fs_wait_on_block_writeback(inode, fio->old_blkaddr);
 
 retry_encrypt:
 	fio->encrypted_page = fscrypt_encrypt_page(inode, fio->page,
@@ -2382,10 +2388,6 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 
 	f2fs_wait_on_page_writeback(page, DATA, false);
 
-	/* wait for GCed page writeback via META_MAPPING */
-	if (f2fs_post_read_required(inode))
-		f2fs_wait_on_block_writeback(sbi, blkaddr);
-
 	if (len == PAGE_SIZE || PageUptodate(page))
 		return 0;
 

commit 6aa58d8ad20a3323f42274c25820a6f54192422d
Author: Chao Yu <yuchao0@huawei.com>
Date:   Tue Aug 14 22:37:25 2018 +0800

    f2fs: readahead encrypted block during GC
    
    During GC, for each encrypted block, we will read block synchronously
    into meta page, and then submit it into current cold data log area.
    
    So this block read model with 4k granularity can make poor performance,
    like migrating non-encrypted block, let's readahead encrypted block
    as well to improve migration performance.
    
    To implement this, we choose meta page that its index is old block
    address of the encrypted block, and readahead ciphertext into this
    page, later, if readaheaded page is still updated, we will load its
    data into target meta page, and submit the write IO.
    
    Note that for OPU, truncation, deletion, we need to invalid meta
    page after we invalid old block address, to make sure we won't load
    invalid data from target meta page during encrypted block migration.
    
    for ((i = 0; i < 1000; i++))
    do {
            xfs_io -f /mnt/f2fs/dir/$i -c "pwrite 0 128k" -c "fsync";
    } done
    
    for ((i = 0; i < 1000; i+=2))
    do {
            rm /mnt/f2fs/dir/$i;
    } done
    
    ret = ioctl(fd, F2FS_IOC_GARBAGE_COLLECT, 0);
    
    Before:
                  gc-6549  [001] d..1 214682.212797: block_rq_insert: 8,32 RA 32768 () 786400 + 64 [gc]
                  gc-6549  [001] d..1 214682.212802: block_unplug: [gc] 1
                  gc-6549  [001] .... 214682.213892: block_bio_queue: 8,32 R 67494144 + 8 [gc]
                  gc-6549  [001] .... 214682.213899: block_getrq: 8,32 R 67494144 + 8 [gc]
                  gc-6549  [001] .... 214682.213902: block_plug: [gc]
                  gc-6549  [001] d..1 214682.213905: block_rq_insert: 8,32 R 4096 () 67494144 + 8 [gc]
                  gc-6549  [001] d..1 214682.213908: block_unplug: [gc] 1
                  gc-6549  [001] .... 214682.226405: block_bio_queue: 8,32 R 67494152 + 8 [gc]
                  gc-6549  [001] .... 214682.226412: block_getrq: 8,32 R 67494152 + 8 [gc]
                  gc-6549  [001] .... 214682.226414: block_plug: [gc]
                  gc-6549  [001] d..1 214682.226417: block_rq_insert: 8,32 R 4096 () 67494152 + 8 [gc]
                  gc-6549  [001] d..1 214682.226420: block_unplug: [gc] 1
                  gc-6549  [001] .... 214682.226904: block_bio_queue: 8,32 R 67494160 + 8 [gc]
                  gc-6549  [001] .... 214682.226910: block_getrq: 8,32 R 67494160 + 8 [gc]
                  gc-6549  [001] .... 214682.226911: block_plug: [gc]
                  gc-6549  [001] d..1 214682.226914: block_rq_insert: 8,32 R 4096 () 67494160 + 8 [gc]
                  gc-6549  [001] d..1 214682.226916: block_unplug: [gc] 1
    
    After:
                  gc-5678  [003] .... 214327.025906: block_bio_queue: 8,32 R 67493824 + 8 [gc]
                  gc-5678  [003] .... 214327.025908: block_bio_backmerge: 8,32 R 67493824 + 8 [gc]
                  gc-5678  [003] .... 214327.025915: block_bio_queue: 8,32 R 67493832 + 8 [gc]
                  gc-5678  [003] .... 214327.025917: block_bio_backmerge: 8,32 R 67493832 + 8 [gc]
                  gc-5678  [003] .... 214327.025923: block_bio_queue: 8,32 R 67493840 + 8 [gc]
                  gc-5678  [003] .... 214327.025925: block_bio_backmerge: 8,32 R 67493840 + 8 [gc]
                  gc-5678  [003] .... 214327.025932: block_bio_queue: 8,32 R 67493848 + 8 [gc]
                  gc-5678  [003] .... 214327.025934: block_bio_backmerge: 8,32 R 67493848 + 8 [gc]
                  gc-5678  [003] .... 214327.025941: block_bio_queue: 8,32 R 67493856 + 8 [gc]
                  gc-5678  [003] .... 214327.025943: block_bio_backmerge: 8,32 R 67493856 + 8 [gc]
                  gc-5678  [003] .... 214327.025953: block_bio_queue: 8,32 R 67493864 + 8 [gc]
                  gc-5678  [003] .... 214327.025955: block_bio_backmerge: 8,32 R 67493864 + 8 [gc]
                  gc-5678  [003] .... 214327.025962: block_bio_queue: 8,32 R 67493872 + 8 [gc]
                  gc-5678  [003] .... 214327.025964: block_bio_backmerge: 8,32 R 67493872 + 8 [gc]
                  gc-5678  [003] .... 214327.025970: block_bio_queue: 8,32 R 67493880 + 8 [gc]
                  gc-5678  [003] .... 214327.025972: block_bio_backmerge: 8,32 R 67493880 + 8 [gc]
                  gc-5678  [003] .... 214327.026000: block_bio_queue: 8,32 WS 34123776 + 2048 [gc]
                  gc-5678  [003] .... 214327.026019: block_getrq: 8,32 WS 34123776 + 2048 [gc]
                  gc-5678  [003] d..1 214327.026021: block_rq_insert: 8,32 R 131072 () 67493632 + 256 [gc]
                  gc-5678  [003] d..1 214327.026023: block_unplug: [gc] 1
                  gc-5678  [003] d..1 214327.026026: block_rq_issue: 8,32 R 131072 () 67493632 + 256 [gc]
                  gc-5678  [003] .... 214327.026046: block_plug: [gc]
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index e73ce11de02d..382c1ef9a9e4 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -875,6 +875,7 @@ static int __allocate_data_block(struct dnode_of_data *dn, int seg_type)
 	struct f2fs_sb_info *sbi = F2FS_I_SB(dn->inode);
 	struct f2fs_summary sum;
 	struct node_info ni;
+	block_t old_blkaddr;
 	pgoff_t fofs;
 	blkcnt_t count = 1;
 	int err;
@@ -896,9 +897,12 @@ static int __allocate_data_block(struct dnode_of_data *dn, int seg_type)
 
 alloc:
 	set_summary(&sum, dn->nid, dn->ofs_in_node, ni.version);
-
-	f2fs_allocate_data_block(sbi, NULL, dn->data_blkaddr, &dn->data_blkaddr,
+	old_blkaddr = dn->data_blkaddr;
+	f2fs_allocate_data_block(sbi, NULL, old_blkaddr, &dn->data_blkaddr,
 					&sum, seg_type, NULL, false);
+	if (GET_SEGNO(sbi, old_blkaddr) != NULL_SEGNO)
+		invalidate_mapping_pages(META_MAPPING(sbi),
+					old_blkaddr, old_blkaddr);
 	f2fs_set_data_blkaddr(dn);
 
 	/* update i_size */
@@ -1614,6 +1618,7 @@ static int f2fs_read_data_pages(struct file *file,
 static int encrypt_one_page(struct f2fs_io_info *fio)
 {
 	struct inode *inode = fio->page->mapping->host;
+	struct page *mpage;
 	gfp_t gfp_flags = GFP_NOFS;
 
 	if (!f2fs_encrypted_file(inode))
@@ -1625,17 +1630,25 @@ static int encrypt_one_page(struct f2fs_io_info *fio)
 retry_encrypt:
 	fio->encrypted_page = fscrypt_encrypt_page(inode, fio->page,
 			PAGE_SIZE, 0, fio->page->index, gfp_flags);
-	if (!IS_ERR(fio->encrypted_page))
-		return 0;
+	if (IS_ERR(fio->encrypted_page)) {
+		/* flush pending IOs and wait for a while in the ENOMEM case */
+		if (PTR_ERR(fio->encrypted_page) == -ENOMEM) {
+			f2fs_flush_merged_writes(fio->sbi);
+			congestion_wait(BLK_RW_ASYNC, HZ/50);
+			gfp_flags |= __GFP_NOFAIL;
+			goto retry_encrypt;
+		}
+		return PTR_ERR(fio->encrypted_page);
+	}
 
-	/* flush pending IOs and wait for a while in the ENOMEM case */
-	if (PTR_ERR(fio->encrypted_page) == -ENOMEM) {
-		f2fs_flush_merged_writes(fio->sbi);
-		congestion_wait(BLK_RW_ASYNC, HZ/50);
-		gfp_flags |= __GFP_NOFAIL;
-		goto retry_encrypt;
+	mpage = find_lock_page(META_MAPPING(fio->sbi), fio->old_blkaddr);
+	if (mpage) {
+		if (PageUptodate(mpage))
+			memcpy(page_address(mpage),
+				page_address(fio->encrypted_page), PAGE_SIZE);
+		f2fs_put_page(mpage, 1);
 	}
-	return PTR_ERR(fio->encrypted_page);
+	return 0;
 }
 
 static inline bool check_inplace_update_policy(struct inode *inode,

commit 6f8d4455060dfb0e32dfb8e685b97caf4ed1be41
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed Jul 25 12:11:56 2018 +0900

    f2fs: avoid fi->i_gc_rwsem[WRITE] lock in f2fs_gc
    
    The f2fs_gc() called by f2fs_balance_fs() requires to be called outside of
    fi->i_gc_rwsem[WRITE], since f2fs_gc() can try to grab it in a loop.
    
    If it hits the miximum retrials in GC, let's give a chance to release
    gc_mutex for a short time in order not to go into live lock in the worst
    case.
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index bdcb023506a7..e73ce11de02d 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -2217,14 +2217,14 @@ static void f2fs_write_failed(struct address_space *mapping, loff_t to)
 	loff_t i_size = i_size_read(inode);
 
 	if (to > i_size) {
-		down_write(&F2FS_I(inode)->i_mmap_sem);
 		down_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+		down_write(&F2FS_I(inode)->i_mmap_sem);
 
 		truncate_pagecache(inode, i_size);
 		f2fs_truncate_blocks(inode, i_size, true);
 
-		up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
 		up_write(&F2FS_I(inode)->i_mmap_sem);
+		up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
 	}
 }
 

commit 853137cef46ccc490e6fd4b160a1c252d6459842
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Aug 9 17:53:34 2018 -0700

    f2fs: fix performance issue observed with multi-thread sequential read
    
    This reverts the commit - "b93f771 - f2fs: remove writepages lock"
    to fix the drop in sequential read throughput.
    
    Test: ./tiotest -t 32 -d /data/tio_tmp -f 32 -b 524288 -k 1 -k 3 -L
    device: UFS
    
    Before -
    read throughput: 185 MB/s
    total read requests: 85177 (of these ~80000 are 4KB size requests).
    total write requests: 2546 (of these ~2208 requests are written in 512KB).
    
    After -
    read throughput: 758 MB/s
    total read requests: 2417 (of these ~2042 are 512KB reads).
    total write requests: 2701 (of these ~2034 requests are written in 512KB).
    
    Signed-off-by: Sahitya Tummala <stummala@codeaurora.org>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 43d3723dc886..bdcb023506a7 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -2122,6 +2122,18 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 	return ret;
 }
 
+static inline bool __should_serialize_io(struct inode *inode,
+					struct writeback_control *wbc)
+{
+	if (!S_ISREG(inode->i_mode))
+		return false;
+	if (wbc->sync_mode != WB_SYNC_ALL)
+		return true;
+	if (get_dirty_pages(inode) >= SM_I(F2FS_I_SB(inode))->min_seq_blocks)
+		return true;
+	return false;
+}
+
 static int __f2fs_write_data_pages(struct address_space *mapping,
 						struct writeback_control *wbc,
 						enum iostat_type io_type)
@@ -2130,6 +2142,7 @@ static int __f2fs_write_data_pages(struct address_space *mapping,
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
 	struct blk_plug plug;
 	int ret;
+	bool locked = false;
 
 	/* deal with chardevs and other special file */
 	if (!mapping->a_ops->writepage)
@@ -2160,10 +2173,18 @@ static int __f2fs_write_data_pages(struct address_space *mapping,
 	else if (atomic_read(&sbi->wb_sync_req[DATA]))
 		goto skip_write;
 
+	if (__should_serialize_io(inode, wbc)) {
+		mutex_lock(&sbi->writepages);
+		locked = true;
+	}
+
 	blk_start_plug(&plug);
 	ret = f2fs_write_cache_pages(mapping, wbc, io_type);
 	blk_finish_plug(&plug);
 
+	if (locked)
+		mutex_unlock(&sbi->writepages);
+
 	if (wbc->sync_mode == WB_SYNC_ALL)
 		atomic_dec(&sbi->wb_sync_req[DATA]);
 	/*

commit 7fa750a163089cf96866de402314d853a96cb342
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Mon Aug 13 23:38:06 2018 +0200

    f2fs: rework fault injection handling to avoid a warning
    
    When CONFIG_F2FS_FAULT_INJECTION is disabled, we get a warning about an
    unused label:
    
    fs/f2fs/segment.c: In function '__submit_discard_cmd':
    fs/f2fs/segment.c:1059:1: error: label 'submit' defined but not used [-Werror=unused-label]
    
    This could be fixed by adding another #ifdef around it, but the more
    reliable way of doing this seems to be to remove the other #ifdefs
    where that is easily possible.
    
    By defining time_to_inject() as a trivial stub, most of the checks for
    CONFIG_F2FS_FAULT_INJECTION can go away. This also leads to nicer
    formatting of the code.
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 45f043ee48bd..43d3723dc886 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -126,12 +126,10 @@ static bool f2fs_bio_post_read_required(struct bio *bio)
 
 static void f2fs_read_end_io(struct bio *bio)
 {
-#ifdef CONFIG_F2FS_FAULT_INJECTION
 	if (time_to_inject(F2FS_P_SB(bio_first_page_all(bio)), FAULT_IO)) {
 		f2fs_show_injection_info(FAULT_IO);
 		bio->bi_status = BLK_STS_IOERR;
 	}
-#endif
 
 	if (f2fs_bio_post_read_required(bio)) {
 		struct bio_post_read_ctx *ctx = bio->bi_private;

commit a33c150237a20d97a174243bc658c86502f9d370
Author: Chao Yu <yuchao0@huawei.com>
Date:   Sun Aug 5 23:04:25 2018 +0800

    f2fs: fix avoid race between truncate and background GC
    
    Thread A                                Background GC
    - f2fs_setattr isize to 0
     - truncate_setsize
                                            - gc_data_segment
                                             - f2fs_get_read_data_page page #0
                                              - set_page_dirty
                                              - set_cold_data
     - f2fs_truncate
    
    - f2fs_setattr isize to 4k
    - read 4k <--- hit data in cached page #0
    
    Above race condition can cause read out invalid data in a truncated
    page, fix it by i_gc_rwsem[WRITE] lock.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 1e6cc68fb7c4..45f043ee48bd 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -2199,8 +2199,12 @@ static void f2fs_write_failed(struct address_space *mapping, loff_t to)
 
 	if (to > i_size) {
 		down_write(&F2FS_I(inode)->i_mmap_sem);
+		down_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+
 		truncate_pagecache(inode, i_size);
 		f2fs_truncate_blocks(inode, i_size, true);
+
+		up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
 		up_write(&F2FS_I(inode)->i_mmap_sem);
 	}
 }

commit 91291e9998d208370eb8156c760691b873bd7522
Author: Chao Yu <yuchao0@huawei.com>
Date:   Tue Jul 10 23:01:45 2018 +0800

    f2fs: fix to do sanity check with block address in main area v2
    
    This patch adds f2fs_is_valid_blkaddr() in below functions to do sanity
    check with block address to avoid pentential panic:
    - f2fs_grab_read_bio()
    - __written_first_block()
    
    https://bugzilla.kernel.org/show_bug.cgi?id=200465
    
    - Reproduce
    
    - POC (poc.c)
        #define _GNU_SOURCE
        #include <sys/types.h>
        #include <sys/mount.h>
        #include <sys/mman.h>
        #include <sys/stat.h>
        #include <sys/xattr.h>
    
        #include <dirent.h>
        #include <errno.h>
        #include <error.h>
        #include <fcntl.h>
        #include <stdio.h>
        #include <stdlib.h>
        #include <string.h>
        #include <unistd.h>
    
        #include <linux/falloc.h>
        #include <linux/loop.h>
    
        static void activity(char *mpoint) {
    
          char *xattr;
          int err;
    
          err = asprintf(&xattr, "%s/foo/bar/xattr", mpoint);
    
          char buf2[113];
          memset(buf2, 0, sizeof(buf2));
          listxattr(xattr, buf2, sizeof(buf2));
    
        }
    
        int main(int argc, char *argv[]) {
          activity(argv[1]);
          return 0;
        }
    
    - kernel message
    [  844.718738] F2FS-fs (loop0): Mounted with checkpoint version = 2
    [  846.430929] F2FS-fs (loop0): access invalid blkaddr:1024
    [  846.431058] WARNING: CPU: 1 PID: 1249 at fs/f2fs/checkpoint.c:154 f2fs_is_valid_blkaddr+0x10f/0x160
    [  846.431059] Modules linked in: snd_hda_codec_generic snd_hda_intel snd_hda_codec snd_hda_core snd_hwdep snd_pcm snd_timer snd input_leds joydev soundcore serio_raw i2c_piix4 mac_hid ib_iser rdma_cm iw_cm ib_cm ib_core configfs iscsi_tcp libiscsi_tcp libiscsi scsi_transport_iscsi autofs4 raid10 raid456 libcrc32c async_raid6_recov async_memcpy async_pq async_xor xor async_tx raid6_pq raid1 raid0 multipath linear qxl ttm crct10dif_pclmul crc32_pclmul drm_kms_helper ghash_clmulni_intel syscopyarea sysfillrect sysimgblt fb_sys_fops pcbc drm 8139too aesni_intel 8139cp floppy psmouse mii aes_x86_64 crypto_simd pata_acpi cryptd glue_helper
    [  846.431310] CPU: 1 PID: 1249 Comm: a.out Not tainted 4.18.0-rc3+ #1
    [  846.431312] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS Ubuntu-1.8.2-1ubuntu1 04/01/2014
    [  846.431315] RIP: 0010:f2fs_is_valid_blkaddr+0x10f/0x160
    [  846.431316] Code: 00 eb ed 31 c0 83 fa 05 75 ae 48 83 ec 08 48 8b 3f 89 f1 48 c7 c2 fc 0b 0f 8b 48 c7 c6 8b d7 09 8b 88 44 24 07 e8 61 8b ff ff <0f> 0b 0f b6 44 24 07 48 83 c4 08 eb 81 4c 8b 47 10 8b 8f 38 04 00
    [  846.431347] RSP: 0018:ffff961c414a7bc0 EFLAGS: 00010282
    [  846.431349] RAX: 0000000000000000 RBX: ffffc5f787b8ea80 RCX: 0000000000000000
    [  846.431350] RDX: 0000000000000000 RSI: ffff89dfffd165d8 RDI: ffff89dfffd165d8
    [  846.431351] RBP: ffff961c414a7c20 R08: 0000000000000001 R09: 0000000000000248
    [  846.431353] R10: 0000000000000000 R11: 0000000000000248 R12: 0000000000000007
    [  846.431369] R13: ffff89dff5492800 R14: ffff89dfae3aa000 R15: ffff89dff4ff88d0
    [  846.431372] FS:  00007f882e2fb700(0000) GS:ffff89dfffd00000(0000) knlGS:0000000000000000
    [  846.431373] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [  846.431374] CR2: 0000000001a88008 CR3: 00000001eb572000 CR4: 00000000000006e0
    [  846.431384] Call Trace:
    [  846.431426]  f2fs_iget+0x6f4/0xe70
    [  846.431430]  ? f2fs_find_entry+0x71/0x90
    [  846.431432]  f2fs_lookup+0x1aa/0x390
    [  846.431452]  __lookup_slow+0x97/0x150
    [  846.431459]  lookup_slow+0x35/0x50
    [  846.431462]  walk_component+0x1c6/0x470
    [  846.431479]  ? memcg_kmem_charge_memcg+0x70/0x90
    [  846.431488]  ? page_add_file_rmap+0x13/0x200
    [  846.431491]  path_lookupat+0x76/0x230
    [  846.431501]  ? __alloc_pages_nodemask+0xfc/0x280
    [  846.431504]  filename_lookup+0xb8/0x1a0
    [  846.431534]  ? _cond_resched+0x16/0x40
    [  846.431541]  ? kmem_cache_alloc+0x160/0x1d0
    [  846.431549]  ? path_listxattr+0x41/0xa0
    [  846.431551]  path_listxattr+0x41/0xa0
    [  846.431570]  do_syscall_64+0x55/0x100
    [  846.431583]  entry_SYSCALL_64_after_hwframe+0x44/0xa9
    [  846.431607] RIP: 0033:0x7f882de1c0d7
    [  846.431607] Code: f0 ff ff 73 01 c3 48 8b 0d be dd 2b 00 f7 d8 64 89 01 48 83 c8 ff c3 66 2e 0f 1f 84 00 00 00 00 00 66 90 b8 c2 00 00 00 0f 05 <48> 3d 01 f0 ff ff 73 01 c3 48 8b 0d 91 dd 2b 00 f7 d8 64 89 01 48
    [  846.431639] RSP: 002b:00007ffe8e66c238 EFLAGS: 00000202 ORIG_RAX: 00000000000000c2
    [  846.431641] RAX: ffffffffffffffda RBX: 0000000000000000 RCX: 00007f882de1c0d7
    [  846.431642] RDX: 0000000000000071 RSI: 00007ffe8e66c280 RDI: 0000000001a880c0
    [  846.431643] RBP: 00007ffe8e66c300 R08: 0000000001a88010 R09: 0000000000000000
    [  846.431645] R10: 00000000000001ab R11: 0000000000000202 R12: 0000000000400550
    [  846.431646] R13: 00007ffe8e66c400 R14: 0000000000000000 R15: 0000000000000000
    [  846.431648] ---[ end trace abca54df39d14f5c ]---
    [  846.431651] F2FS-fs (loop0): invalid blkaddr: 1024, type: 5, run fsck to fix.
    [  846.431762] WARNING: CPU: 1 PID: 1249 at fs/f2fs/f2fs.h:2697 f2fs_iget+0xd17/0xe70
    [  846.431763] Modules linked in: snd_hda_codec_generic snd_hda_intel snd_hda_codec snd_hda_core snd_hwdep snd_pcm snd_timer snd input_leds joydev soundcore serio_raw i2c_piix4 mac_hid ib_iser rdma_cm iw_cm ib_cm ib_core configfs iscsi_tcp libiscsi_tcp libiscsi scsi_transport_iscsi autofs4 raid10 raid456 libcrc32c async_raid6_recov async_memcpy async_pq async_xor xor async_tx raid6_pq raid1 raid0 multipath linear qxl ttm crct10dif_pclmul crc32_pclmul drm_kms_helper ghash_clmulni_intel syscopyarea sysfillrect sysimgblt fb_sys_fops pcbc drm 8139too aesni_intel 8139cp floppy psmouse mii aes_x86_64 crypto_simd pata_acpi cryptd glue_helper
    [  846.431797] CPU: 1 PID: 1249 Comm: a.out Tainted: G        W         4.18.0-rc3+ #1
    [  846.431798] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS Ubuntu-1.8.2-1ubuntu1 04/01/2014
    [  846.431800] RIP: 0010:f2fs_iget+0xd17/0xe70
    [  846.431801] Code: ff ff 48 63 d8 e9 e1 f6 ff ff 48 8b 45 c8 41 b8 05 00 00 00 48 c7 c2 d8 e8 0e 8b 48 c7 c6 1d b0 0a 8b 48 8b 38 e8 f9 b4 00 00 <0f> 0b 48 8b 45 c8 f0 80 48 48 04 e9 d8 f9 ff ff 0f 0b 48 8b 43 18
    [  846.431832] RSP: 0018:ffff961c414a7bd0 EFLAGS: 00010282
    [  846.431834] RAX: 0000000000000000 RBX: ffffc5f787b8ea80 RCX: 0000000000000006
    [  846.431835] RDX: 0000000000000000 RSI: 0000000000000096 RDI: ffff89dfffd165d0
    [  846.431836] RBP: ffff961c414a7c20 R08: 0000000000000000 R09: 0000000000000273
    [  846.431837] R10: 0000000000000000 R11: ffff89dfad50ca60 R12: 0000000000000007
    [  846.431838] R13: ffff89dff5492800 R14: ffff89dfae3aa000 R15: ffff89dff4ff88d0
    [  846.431840] FS:  00007f882e2fb700(0000) GS:ffff89dfffd00000(0000) knlGS:0000000000000000
    [  846.431841] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [  846.431842] CR2: 0000000001a88008 CR3: 00000001eb572000 CR4: 00000000000006e0
    [  846.431846] Call Trace:
    [  846.431850]  ? f2fs_find_entry+0x71/0x90
    [  846.431853]  f2fs_lookup+0x1aa/0x390
    [  846.431856]  __lookup_slow+0x97/0x150
    [  846.431858]  lookup_slow+0x35/0x50
    [  846.431874]  walk_component+0x1c6/0x470
    [  846.431878]  ? memcg_kmem_charge_memcg+0x70/0x90
    [  846.431880]  ? page_add_file_rmap+0x13/0x200
    [  846.431882]  path_lookupat+0x76/0x230
    [  846.431884]  ? __alloc_pages_nodemask+0xfc/0x280
    [  846.431886]  filename_lookup+0xb8/0x1a0
    [  846.431890]  ? _cond_resched+0x16/0x40
    [  846.431891]  ? kmem_cache_alloc+0x160/0x1d0
    [  846.431894]  ? path_listxattr+0x41/0xa0
    [  846.431896]  path_listxattr+0x41/0xa0
    [  846.431898]  do_syscall_64+0x55/0x100
    [  846.431901]  entry_SYSCALL_64_after_hwframe+0x44/0xa9
    [  846.431902] RIP: 0033:0x7f882de1c0d7
    [  846.431903] Code: f0 ff ff 73 01 c3 48 8b 0d be dd 2b 00 f7 d8 64 89 01 48 83 c8 ff c3 66 2e 0f 1f 84 00 00 00 00 00 66 90 b8 c2 00 00 00 0f 05 <48> 3d 01 f0 ff ff 73 01 c3 48 8b 0d 91 dd 2b 00 f7 d8 64 89 01 48
    [  846.431934] RSP: 002b:00007ffe8e66c238 EFLAGS: 00000202 ORIG_RAX: 00000000000000c2
    [  846.431936] RAX: ffffffffffffffda RBX: 0000000000000000 RCX: 00007f882de1c0d7
    [  846.431937] RDX: 0000000000000071 RSI: 00007ffe8e66c280 RDI: 0000000001a880c0
    [  846.431939] RBP: 00007ffe8e66c300 R08: 0000000001a88010 R09: 0000000000000000
    [  846.431940] R10: 00000000000001ab R11: 0000000000000202 R12: 0000000000400550
    [  846.431941] R13: 00007ffe8e66c400 R14: 0000000000000000 R15: 0000000000000000
    [  846.431943] ---[ end trace abca54df39d14f5d ]---
    [  846.432033] F2FS-fs (loop0): access invalid blkaddr:1024
    [  846.432051] WARNING: CPU: 1 PID: 1249 at fs/f2fs/checkpoint.c:154 f2fs_is_valid_blkaddr+0x10f/0x160
    [  846.432051] Modules linked in: snd_hda_codec_generic snd_hda_intel snd_hda_codec snd_hda_core snd_hwdep snd_pcm snd_timer snd input_leds joydev soundcore serio_raw i2c_piix4 mac_hid ib_iser rdma_cm iw_cm ib_cm ib_core configfs iscsi_tcp libiscsi_tcp libiscsi scsi_transport_iscsi autofs4 raid10 raid456 libcrc32c async_raid6_recov async_memcpy async_pq async_xor xor async_tx raid6_pq raid1 raid0 multipath linear qxl ttm crct10dif_pclmul crc32_pclmul drm_kms_helper ghash_clmulni_intel syscopyarea sysfillrect sysimgblt fb_sys_fops pcbc drm 8139too aesni_intel 8139cp floppy psmouse mii aes_x86_64 crypto_simd pata_acpi cryptd glue_helper
    [  846.432085] CPU: 1 PID: 1249 Comm: a.out Tainted: G        W         4.18.0-rc3+ #1
    [  846.432086] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS Ubuntu-1.8.2-1ubuntu1 04/01/2014
    [  846.432089] RIP: 0010:f2fs_is_valid_blkaddr+0x10f/0x160
    [  846.432089] Code: 00 eb ed 31 c0 83 fa 05 75 ae 48 83 ec 08 48 8b 3f 89 f1 48 c7 c2 fc 0b 0f 8b 48 c7 c6 8b d7 09 8b 88 44 24 07 e8 61 8b ff ff <0f> 0b 0f b6 44 24 07 48 83 c4 08 eb 81 4c 8b 47 10 8b 8f 38 04 00
    [  846.432120] RSP: 0018:ffff961c414a7900 EFLAGS: 00010286
    [  846.432122] RAX: 0000000000000000 RBX: 0000000000000400 RCX: 0000000000000006
    [  846.432123] RDX: 0000000000000000 RSI: 0000000000000096 RDI: ffff89dfffd165d0
    [  846.432124] RBP: ffff89dff5492800 R08: 0000000000000001 R09: 000000000000029d
    [  846.432125] R10: ffff961c414a7820 R11: 000000000000029d R12: 0000000000000400
    [  846.432126] R13: 0000000000000000 R14: ffff89dff4ff88d0 R15: 0000000000000000
    [  846.432128] FS:  00007f882e2fb700(0000) GS:ffff89dfffd00000(0000) knlGS:0000000000000000
    [  846.432130] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [  846.432131] CR2: 0000000001a88008 CR3: 00000001eb572000 CR4: 00000000000006e0
    [  846.432135] Call Trace:
    [  846.432151]  f2fs_wait_on_block_writeback+0x20/0x110
    [  846.432158]  f2fs_grab_read_bio+0xbc/0xe0
    [  846.432161]  f2fs_submit_page_read+0x21/0x280
    [  846.432163]  f2fs_get_read_data_page+0xb7/0x3c0
    [  846.432165]  f2fs_get_lock_data_page+0x29/0x1e0
    [  846.432167]  f2fs_get_new_data_page+0x148/0x550
    [  846.432170]  f2fs_add_regular_entry+0x1d2/0x550
    [  846.432178]  ? __switch_to+0x12f/0x460
    [  846.432181]  f2fs_add_dentry+0x6a/0xd0
    [  846.432184]  f2fs_do_add_link+0xe9/0x140
    [  846.432186]  __recover_dot_dentries+0x260/0x280
    [  846.432189]  f2fs_lookup+0x343/0x390
    [  846.432193]  __lookup_slow+0x97/0x150
    [  846.432195]  lookup_slow+0x35/0x50
    [  846.432208]  walk_component+0x1c6/0x470
    [  846.432212]  ? memcg_kmem_charge_memcg+0x70/0x90
    [  846.432215]  ? page_add_file_rmap+0x13/0x200
    [  846.432217]  path_lookupat+0x76/0x230
    [  846.432219]  ? __alloc_pages_nodemask+0xfc/0x280
    [  846.432221]  filename_lookup+0xb8/0x1a0
    [  846.432224]  ? _cond_resched+0x16/0x40
    [  846.432226]  ? kmem_cache_alloc+0x160/0x1d0
    [  846.432228]  ? path_listxattr+0x41/0xa0
    [  846.432230]  path_listxattr+0x41/0xa0
    [  846.432233]  do_syscall_64+0x55/0x100
    [  846.432235]  entry_SYSCALL_64_after_hwframe+0x44/0xa9
    [  846.432237] RIP: 0033:0x7f882de1c0d7
    [  846.432237] Code: f0 ff ff 73 01 c3 48 8b 0d be dd 2b 00 f7 d8 64 89 01 48 83 c8 ff c3 66 2e 0f 1f 84 00 00 00 00 00 66 90 b8 c2 00 00 00 0f 05 <48> 3d 01 f0 ff ff 73 01 c3 48 8b 0d 91 dd 2b 00 f7 d8 64 89 01 48
    [  846.432269] RSP: 002b:00007ffe8e66c238 EFLAGS: 00000202 ORIG_RAX: 00000000000000c2
    [  846.432271] RAX: ffffffffffffffda RBX: 0000000000000000 RCX: 00007f882de1c0d7
    [  846.432272] RDX: 0000000000000071 RSI: 00007ffe8e66c280 RDI: 0000000001a880c0
    [  846.432273] RBP: 00007ffe8e66c300 R08: 0000000001a88010 R09: 0000000000000000
    [  846.432274] R10: 00000000000001ab R11: 0000000000000202 R12: 0000000000400550
    [  846.432275] R13: 00007ffe8e66c400 R14: 0000000000000000 R15: 0000000000000000
    [  846.432277] ---[ end trace abca54df39d14f5e ]---
    [  846.432279] F2FS-fs (loop0): invalid blkaddr: 1024, type: 5, run fsck to fix.
    [  846.432376] WARNING: CPU: 1 PID: 1249 at fs/f2fs/f2fs.h:2697 f2fs_wait_on_block_writeback+0xb1/0x110
    [  846.432376] Modules linked in: snd_hda_codec_generic snd_hda_intel snd_hda_codec snd_hda_core snd_hwdep snd_pcm snd_timer snd input_leds joydev soundcore serio_raw i2c_piix4 mac_hid ib_iser rdma_cm iw_cm ib_cm ib_core configfs iscsi_tcp libiscsi_tcp libiscsi scsi_transport_iscsi autofs4 raid10 raid456 libcrc32c async_raid6_recov async_memcpy async_pq async_xor xor async_tx raid6_pq raid1 raid0 multipath linear qxl ttm crct10dif_pclmul crc32_pclmul drm_kms_helper ghash_clmulni_intel syscopyarea sysfillrect sysimgblt fb_sys_fops pcbc drm 8139too aesni_intel 8139cp floppy psmouse mii aes_x86_64 crypto_simd pata_acpi cryptd glue_helper
    [  846.432410] CPU: 1 PID: 1249 Comm: a.out Tainted: G        W         4.18.0-rc3+ #1
    [  846.432411] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS Ubuntu-1.8.2-1ubuntu1 04/01/2014
    [  846.432413] RIP: 0010:f2fs_wait_on_block_writeback+0xb1/0x110
    [  846.432414] Code: 66 90 f0 ff 4b 34 74 59 5b 5d c3 48 8b 7d 00 41 b8 05 00 00 00 89 d9 48 c7 c2 d8 e8 0e 8b 48 c7 c6 1d b0 0a 8b e8 df bc fd ff <0f> 0b f0 80 4d 48 04 e9 67 ff ff ff 48 8b 03 48 c1 e8 37 83 e0 07
    [  846.432445] RSP: 0018:ffff961c414a7910 EFLAGS: 00010286
    [  846.432447] RAX: 0000000000000000 RBX: 0000000000000400 RCX: 0000000000000006
    [  846.432448] RDX: 0000000000000000 RSI: 0000000000000092 RDI: ffff89dfffd165d0
    [  846.432449] RBP: ffff89dff5492800 R08: 0000000000000000 R09: 00000000000002d1
    [  846.432450] R10: ffff961c414a7820 R11: ffff89dfad50cf80 R12: 0000000000000400
    [  846.432451] R13: 0000000000000000 R14: ffff89dff4ff88d0 R15: 0000000000000000
    [  846.432453] FS:  00007f882e2fb700(0000) GS:ffff89dfffd00000(0000) knlGS:0000000000000000
    [  846.432454] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [  846.432455] CR2: 0000000001a88008 CR3: 00000001eb572000 CR4: 00000000000006e0
    [  846.432459] Call Trace:
    [  846.432463]  f2fs_grab_read_bio+0xbc/0xe0
    [  846.432464]  f2fs_submit_page_read+0x21/0x280
    [  846.432466]  f2fs_get_read_data_page+0xb7/0x3c0
    [  846.432468]  f2fs_get_lock_data_page+0x29/0x1e0
    [  846.432470]  f2fs_get_new_data_page+0x148/0x550
    [  846.432473]  f2fs_add_regular_entry+0x1d2/0x550
    [  846.432475]  ? __switch_to+0x12f/0x460
    [  846.432477]  f2fs_add_dentry+0x6a/0xd0
    [  846.432480]  f2fs_do_add_link+0xe9/0x140
    [  846.432483]  __recover_dot_dentries+0x260/0x280
    [  846.432485]  f2fs_lookup+0x343/0x390
    [  846.432488]  __lookup_slow+0x97/0x150
    [  846.432490]  lookup_slow+0x35/0x50
    [  846.432505]  walk_component+0x1c6/0x470
    [  846.432509]  ? memcg_kmem_charge_memcg+0x70/0x90
    [  846.432511]  ? page_add_file_rmap+0x13/0x200
    [  846.432513]  path_lookupat+0x76/0x230
    [  846.432515]  ? __alloc_pages_nodemask+0xfc/0x280
    [  846.432517]  filename_lookup+0xb8/0x1a0
    [  846.432520]  ? _cond_resched+0x16/0x40
    [  846.432522]  ? kmem_cache_alloc+0x160/0x1d0
    [  846.432525]  ? path_listxattr+0x41/0xa0
    [  846.432526]  path_listxattr+0x41/0xa0
    [  846.432529]  do_syscall_64+0x55/0x100
    [  846.432531]  entry_SYSCALL_64_after_hwframe+0x44/0xa9
    [  846.432533] RIP: 0033:0x7f882de1c0d7
    [  846.432533] Code: f0 ff ff 73 01 c3 48 8b 0d be dd 2b 00 f7 d8 64 89 01 48 83 c8 ff c3 66 2e 0f 1f 84 00 00 00 00 00 66 90 b8 c2 00 00 00 0f 05 <48> 3d 01 f0 ff ff 73 01 c3 48 8b 0d 91 dd 2b 00 f7 d8 64 89 01 48
    [  846.432565] RSP: 002b:00007ffe8e66c238 EFLAGS: 00000202 ORIG_RAX: 00000000000000c2
    [  846.432567] RAX: ffffffffffffffda RBX: 0000000000000000 RCX: 00007f882de1c0d7
    [  846.432568] RDX: 0000000000000071 RSI: 00007ffe8e66c280 RDI: 0000000001a880c0
    [  846.432569] RBP: 00007ffe8e66c300 R08: 0000000001a88010 R09: 0000000000000000
    [  846.432570] R10: 00000000000001ab R11: 0000000000000202 R12: 0000000000400550
    [  846.432571] R13: 00007ffe8e66c400 R14: 0000000000000000 R15: 0000000000000000
    [  846.432573] ---[ end trace abca54df39d14f5f ]---
    [  846.434280] BUG: unable to handle kernel NULL pointer dereference at 0000000000000008
    [  846.434424] PGD 80000001ebd3a067 P4D 80000001ebd3a067 PUD 1eb1ae067 PMD 0
    [  846.434551] Oops: 0000 [#1] SMP PTI
    [  846.434697] CPU: 0 PID: 44 Comm: kworker/u5:0 Tainted: G        W         4.18.0-rc3+ #1
    [  846.434805] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS Ubuntu-1.8.2-1ubuntu1 04/01/2014
    [  846.435000] Workqueue: fscrypt_read_queue decrypt_work
    [  846.435174] RIP: 0010:fscrypt_do_page_crypto+0x6e/0x2d0
    [  846.435351] Code: 00 65 48 8b 04 25 28 00 00 00 48 89 84 24 88 00 00 00 31 c0 e8 43 c2 e0 ff 49 8b 86 48 02 00 00 85 ed c7 44 24 70 00 00 00 00 <48> 8b 58 08 0f 84 14 02 00 00 48 8b 78 10 48 8b 0c 24 48 c7 84 24
    [  846.435696] RSP: 0018:ffff961c40f9bd60 EFLAGS: 00010206
    [  846.435870] RAX: 0000000000000000 RBX: ffffc5f787719b80 RCX: ffffc5f787719b80
    [  846.436051] RDX: ffffffff8b9f4b88 RSI: ffffffff8b0ae622 RDI: ffff961c40f9bdb8
    [  846.436261] RBP: 0000000000001000 R08: ffffc5f787719b80 R09: 0000000000001000
    [  846.436433] R10: 0000000000000018 R11: fefefefefefefeff R12: ffffc5f787719b80
    [  846.436562] R13: ffffc5f787719b80 R14: ffff89dff4ff88d0 R15: 0ffff89dfaddee60
    [  846.436658] FS:  0000000000000000(0000) GS:ffff89dfffc00000(0000) knlGS:0000000000000000
    [  846.436758] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [  846.436898] CR2: 0000000000000008 CR3: 00000001eddd0000 CR4: 00000000000006f0
    [  846.437001] Call Trace:
    [  846.437181]  ? check_preempt_wakeup+0xf2/0x230
    [  846.437276]  ? check_preempt_curr+0x7c/0x90
    [  846.437370]  fscrypt_decrypt_page+0x48/0x4d
    [  846.437466]  __fscrypt_decrypt_bio+0x5b/0x90
    [  846.437542]  decrypt_work+0x12/0x20
    [  846.437651]  process_one_work+0x15e/0x3d0
    [  846.437740]  worker_thread+0x4c/0x440
    [  846.437848]  kthread+0xf8/0x130
    [  846.437938]  ? rescuer_thread+0x350/0x350
    [  846.438022]  ? kthread_associate_blkcg+0x90/0x90
    [  846.438117]  ret_from_fork+0x35/0x40
    [  846.438201] Modules linked in: snd_hda_codec_generic snd_hda_intel snd_hda_codec snd_hda_core snd_hwdep snd_pcm snd_timer snd input_leds joydev soundcore serio_raw i2c_piix4 mac_hid ib_iser rdma_cm iw_cm ib_cm ib_core configfs iscsi_tcp libiscsi_tcp libiscsi scsi_transport_iscsi autofs4 raid10 raid456 libcrc32c async_raid6_recov async_memcpy async_pq async_xor xor async_tx raid6_pq raid1 raid0 multipath linear qxl ttm crct10dif_pclmul crc32_pclmul drm_kms_helper ghash_clmulni_intel syscopyarea sysfillrect sysimgblt fb_sys_fops pcbc drm 8139too aesni_intel 8139cp floppy psmouse mii aes_x86_64 crypto_simd pata_acpi cryptd glue_helper
    [  846.438653] CR2: 0000000000000008
    [  846.438713] ---[ end trace abca54df39d14f60 ]---
    [  846.438796] RIP: 0010:fscrypt_do_page_crypto+0x6e/0x2d0
    [  846.438844] Code: 00 65 48 8b 04 25 28 00 00 00 48 89 84 24 88 00 00 00 31 c0 e8 43 c2 e0 ff 49 8b 86 48 02 00 00 85 ed c7 44 24 70 00 00 00 00 <48> 8b 58 08 0f 84 14 02 00 00 48 8b 78 10 48 8b 0c 24 48 c7 84 24
    [  846.439084] RSP: 0018:ffff961c40f9bd60 EFLAGS: 00010206
    [  846.439176] RAX: 0000000000000000 RBX: ffffc5f787719b80 RCX: ffffc5f787719b80
    [  846.440927] RDX: ffffffff8b9f4b88 RSI: ffffffff8b0ae622 RDI: ffff961c40f9bdb8
    [  846.442083] RBP: 0000000000001000 R08: ffffc5f787719b80 R09: 0000000000001000
    [  846.443284] R10: 0000000000000018 R11: fefefefefefefeff R12: ffffc5f787719b80
    [  846.444448] R13: ffffc5f787719b80 R14: ffff89dff4ff88d0 R15: 0ffff89dfaddee60
    [  846.445558] FS:  0000000000000000(0000) GS:ffff89dfffc00000(0000) knlGS:0000000000000000
    [  846.446687] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [  846.447796] CR2: 0000000000000008 CR3: 00000001eddd0000 CR4: 00000000000006f0
    
    - Location
    https://elixir.bootlin.com/linux/v4.18-rc4/source/fs/crypto/crypto.c#L149
            struct crypto_skcipher *tfm = ci->ci_ctfm;
    Here ci can be NULL
    
    Note that this issue maybe require CONFIG_F2FS_FS_ENCRYPTION=y to reproduce.
    
    Reported-by Wen Xu <wen.xu@gatech.edu>
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 363520ee099a..1e6cc68fb7c4 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -546,6 +546,9 @@ static struct bio *f2fs_grab_read_bio(struct inode *inode, block_t blkaddr,
 	struct bio_post_read_ctx *ctx;
 	unsigned int post_read_steps = 0;
 
+	if (!f2fs_is_valid_blkaddr(sbi, blkaddr, DATA_GENERIC))
+		return ERR_PTR(-EFAULT);
+
 	bio = f2fs_bio_alloc(sbi, min_t(int, nr_pages, BIO_MAX_PAGES), false);
 	if (!bio)
 		return ERR_PTR(-ENOMEM);

commit 50fa53eccf9f911a5b435248a2b0bd484fd82e5e
Author: Chao Yu <yuchao0@huawei.com>
Date:   Thu Aug 2 23:03:19 2018 +0800

    f2fs: fix to avoid broken of dnode block list
    
    f2fs recovery flow is relying on dnode block link list, it means fsynced
    file recovery depends on previous dnode's persistence in the list, so
    during fsync() we should wait on all regular inode's dnode writebacked
    before issuing flush.
    
    By this way, we can avoid dnode block list being broken by out-of-order
    IO submission due to IO scheduler or driver.
    
    Sheng Yong helps to do the test with this patch:
    
    Target:/data (f2fs, -)
    64MB / 32768KB / 4KB / 8
    
    1 / PERSIST / Index
    
    Base:
            SEQ-RD(MB/s)    SEQ-WR(MB/s)    RND-RD(IOPS)    RND-WR(IOPS)    Insert(TPS)     Update(TPS)     Delete(TPS)
    1       867.82          204.15          41440.03        41370.54        680.8           1025.94         1031.08
    2       871.87          205.87          41370.3         40275.2         791.14          1065.84         1101.7
    3       866.52          205.69          41795.67        40596.16        694.69          1037.16         1031.48
    Avg     868.7366667     205.2366667     41535.33333     40747.3         722.21          1042.98         1054.753333
    
    After:
            SEQ-RD(MB/s)    SEQ-WR(MB/s)    RND-RD(IOPS)    RND-WR(IOPS)    Insert(TPS)     Update(TPS)     Delete(TPS)
    1       798.81          202.5           41143           40613.87        602.71          838.08          913.83
    2       805.79          206.47          40297.2         41291.46        604.44          840.75          924.27
    3       814.83          206.17          41209.57        40453.62        602.85          834.66          927.91
    Avg     806.4766667     205.0466667     40883.25667     40786.31667     603.3333333     837.83          922.0033333
    
    Patched/Original:
            0.928332713     0.999074239     0.984300676     1.000957528     0.835398753     0.803303994     0.874141189
    
    It looks like atomic write will suffer performance regression.
    
    I suspect that the criminal is that we forcing to wait all dnode being in
    storage cache before we issue PREFLUSH+FUA.
    
    BTW, will commit ("f2fs: don't need to wait for node writes for atomic write")
    cause the problem: we will lose data of last transaction after SPO, even if
    atomic write return no error:
    
    - atomic_open();
    - write() P1, P2, P3;
    - atomic_commit();
     - writeback data: P1, P2, P3;
     - writeback node: N1, N2, N3;  <--- If N1, N2 is not writebacked, N3 with fsync_mark is
    writebacked, In SPOR, we won't find N3 since node chain is broken, turns out that losing
    last transaction.
     - preflush + fua;
    - power-cut
    
    If we don't wait dnode writeback for atomic_write:
    
            SEQ-RD(MB/s)    SEQ-WR(MB/s)    RND-RD(IOPS)    RND-WR(IOPS)    Insert(TPS)     Update(TPS)     Delete(TPS)
    1       779.91          206.03          41621.5         40333.16        716.9           1038.21         1034.85
    2       848.51          204.35          40082.44        39486.17        791.83          1119.96         1083.77
    3       772.12          206.27          41335.25        41599.65        723.29          1055.07         971.92
    Avg     800.18          205.55          41013.06333     40472.99333     744.0066667     1071.08         1030.18
    
    Patched/Original:
            0.92108464      1.001526693     0.987425886     0.993268102     1.030180511     1.026942031     0.976702294
    
    SQLite's performance recovers.
    
    Jaegeuk:
    "Practically, I don't see db corruption becase of this. We can excuse to lose
    the last transaction."
    
    Finally, we decide to keep original implementation of atomic write interface
    sematics that we don't wait all dnode writeback before preflush+fua submission.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index b7986b2e5d1d..363520ee099a 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -177,6 +177,8 @@ static void f2fs_write_end_io(struct bio *bio)
 					page->index != nid_of_node(page));
 
 		dec_page_count(sbi, type);
+		if (f2fs_in_warm_node_list(sbi, page))
+			f2fs_del_fsync_node_entry(sbi, page);
 		clear_cold_data(page);
 		end_page_writeback(page);
 	}

commit 66110abc4c931f879d70e83e1281f891699364bf
Author: Chao Yu <yuchao0@huawei.com>
Date:   Sun Jul 29 12:16:59 2018 +0800

    f2fs: fix to clear PG_checked flag in set_page_dirty()
    
    PG_checked flag will be set on data page during GC, later, we can
    recognize such page by the flag and migrate page to cold segment.
    
    But previously, we don't clear this flag when invalidating data page,
    after page redirtying, we will write it into wrong log.
    
    Let's clear PG_checked flag in set_page_dirty() to avoid this.
    
    Signed-off-by: Weichao Guo <guoweichao@huawei.com>
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 5d152de30449..b7986b2e5d1d 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -2542,6 +2542,10 @@ static int f2fs_set_data_page_dirty(struct page *page)
 	if (!PageUptodate(page))
 		SetPageUptodate(page);
 
+	/* don't remain PG_checked flag which was set during GC */
+	if (is_cold_data(page))
+		clear_cold_data(page);
+
 	if (f2fs_is_atomic_file(inode) && !f2fs_is_commit_atomic_write(inode)) {
 		if (!IS_ATOMIC_WRITTEN_PAGE(page)) {
 			f2fs_register_inmem_page(inode, page);

commit 455e3a5887ee7ebec5c885a8f398c2c3c0a33165
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Jul 27 18:15:11 2018 +0900

    f2fs: don't allow any writes on aborted atomic writes
    
    In order to prevent abusing atomic writes by abnormal users, we've added a
    threshold, 20% over memory footprint, which disallows further atomic writes.
    Previously, however, SQLite doesn't know the files became normal, so that
    it could write stale data and commit on revoked normal database file.
    
    Once f2fs detects such the abnormal behavior, this patch tries to avoid further
    writes in write_begin().
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 7f860405cd6e..5d152de30449 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -2291,8 +2291,9 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 
 	trace_f2fs_write_begin(inode, pos, len, flags);
 
-	if (f2fs_is_atomic_file(inode) &&
-			!f2fs_available_free_memory(sbi, INMEM_PAGES)) {
+	if ((f2fs_is_atomic_file(inode) &&
+			!f2fs_available_free_memory(sbi, INMEM_PAGES)) ||
+			is_inode_flag_set(inode, FI_ATOMIC_REVOKE_REQUEST)) {
 		err = -ENOMEM;
 		drop_atomic = true;
 		goto fail;

commit 7735730d39d75e70476c1b01435b9b1f41637f0e
Author: Chao Yu <yuchao0@huawei.com>
Date:   Tue Jul 17 00:02:17 2018 +0800

    f2fs: fix to propagate error from __get_meta_page()
    
    If caller of __get_meta_page() can handle error, let's propagate error
    from __get_meta_page().
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 2b28f0a6f751..7f860405cd6e 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -879,6 +879,10 @@ static int __allocate_data_block(struct dnode_of_data *dn, int seg_type)
 	if (unlikely(is_inode_flag_set(dn->inode, FI_NO_ALLOC)))
 		return -EPERM;
 
+	err = f2fs_get_node_info(sbi, dn->nid, &ni);
+	if (err)
+		return err;
+
 	dn->data_blkaddr = datablock_addr(dn->inode,
 				dn->node_page, dn->ofs_in_node);
 	if (dn->data_blkaddr == NEW_ADDR)
@@ -888,7 +892,6 @@ static int __allocate_data_block(struct dnode_of_data *dn, int seg_type)
 		return err;
 
 alloc:
-	f2fs_get_node_info(sbi, dn->nid, &ni);
 	set_summary(&sum, dn->nid, dn->ofs_in_node, ni.version);
 
 	f2fs_allocate_data_block(sbi, NULL, dn->data_blkaddr, &dn->data_blkaddr,
@@ -1291,7 +1294,11 @@ static int f2fs_xattr_fiemap(struct inode *inode,
 		if (!page)
 			return -ENOMEM;
 
-		f2fs_get_node_info(sbi, inode->i_ino, &ni);
+		err = f2fs_get_node_info(sbi, inode->i_ino, &ni);
+		if (err) {
+			f2fs_put_page(page, 1);
+			return err;
+		}
 
 		phys = (__u64)blk_to_logical(inode, ni.blk_addr);
 		offset = offsetof(struct f2fs_inode, i_addr) +
@@ -1318,7 +1325,11 @@ static int f2fs_xattr_fiemap(struct inode *inode,
 		if (!page)
 			return -ENOMEM;
 
-		f2fs_get_node_info(sbi, xnid, &ni);
+		err = f2fs_get_node_info(sbi, xnid, &ni);
+		if (err) {
+			f2fs_put_page(page, 1);
+			return err;
+		}
 
 		phys = (__u64)blk_to_logical(inode, ni.blk_addr);
 		len = inode->i_sb->s_blocksize;
@@ -1705,6 +1716,7 @@ int f2fs_do_write_data_page(struct f2fs_io_info *fio)
 	struct inode *inode = page->mapping->host;
 	struct dnode_of_data dn;
 	struct extent_info ei = {0,0,0};
+	struct node_info ni;
 	bool ipu_force = false;
 	int err = 0;
 
@@ -1773,6 +1785,12 @@ int f2fs_do_write_data_page(struct f2fs_io_info *fio)
 		fio->need_lock = LOCK_REQ;
 	}
 
+	err = f2fs_get_node_info(fio->sbi, dn.nid, &ni);
+	if (err)
+		goto out_writepage;
+
+	fio->version = ni.version;
+
 	err = encrypt_one_page(fio);
 	if (err)
 		goto out_writepage;

commit 66415cee3d341b19eb2766c118cb5f6fddda077c
Author: Yunlong Song <yunlong.song@huawei.com>
Date:   Thu Jul 12 23:09:28 2018 +0800

    f2fs: blk_finish_plug of submit_bio in lfs mode
    
    Expand the blk_finish_plug action from blkzoned to normal lfs mode,
    since plug will cause the out-of-order IO submission, which is not
    friendly to flash in lfs mode.
    
    Signed-off-by: Yunlong Song <yunlong.song@huawei.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 4064ce246c13..2b28f0a6f751 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -264,7 +264,7 @@ static inline void __submit_bio(struct f2fs_sb_info *sbi,
 		if (type != DATA && type != NODE)
 			goto submit_io;
 
-		if (f2fs_sb_has_blkzoned(sbi->sb) && current->plug)
+		if (test_opt(sbi, LFS) && current->plug)
 			blk_finish_plug(current->plug);
 
 		start = bio->bi_iter.bi_size >> F2FS_BLKSIZE_BITS;

commit c9b60788fc760d136211853f10ce73dc152d1f4a
Author: Chao Yu <yuchao0@huawei.com>
Date:   Wed Aug 1 19:13:44 2018 +0800

    f2fs: fix to do sanity check with block address in main area
    
    This patch add to do sanity check with below field:
    - cp_pack_total_block_count
    - blkaddr of data/node
    - extent info
    
    - Overview
    BUG() in verify_block_addr() when writing to a corrupted f2fs image
    
    - Reproduce (4.18 upstream kernel)
    
    - POC (poc.c)
    
    static void activity(char *mpoint) {
    
      char *foo_bar_baz;
      int err;
    
      static int buf[8192];
      memset(buf, 0, sizeof(buf));
    
      err = asprintf(&foo_bar_baz, "%s/foo/bar/baz", mpoint);
    
      int fd = open(foo_bar_baz, O_RDWR | O_TRUNC, 0777);
      if (fd >= 0) {
        write(fd, (char *)buf, sizeof(buf));
        fdatasync(fd);
        close(fd);
      }
    }
    
    int main(int argc, char *argv[]) {
      activity(argv[1]);
      return 0;
    }
    
    - Kernel message
    [  689.349473] F2FS-fs (loop0): Mounted with checkpoint version = 3
    [  699.728662] WARNING: CPU: 0 PID: 1309 at fs/f2fs/segment.c:2860 f2fs_inplace_write_data+0x232/0x240
    [  699.728670] Modules linked in: snd_hda_codec_generic snd_hda_intel snd_hda_codec snd_hwdep snd_hda_core snd_pcm snd_timer snd mac_hid i2c_piix4 soundcore ib_iser rdma_cm iw_cm ib_cm ib_core iscsi_tcp libiscsi_tcp libiscsi scsi_transport_iscsi raid10 raid456 async_raid6_recov async_memcpy async_pq async_xor async_tx raid1 raid0 multipath linear 8139too crct10dif_pclmul crc32_pclmul qxl drm_kms_helper syscopyarea aesni_intel sysfillrect sysimgblt fb_sys_fops ttm drm aes_x86_64 crypto_simd cryptd 8139cp glue_helper mii pata_acpi floppy
    [  699.729056] CPU: 0 PID: 1309 Comm: a.out Not tainted 4.18.0-rc1+ #4
    [  699.729064] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS Ubuntu-1.8.2-1ubuntu1 04/01/2014
    [  699.729074] RIP: 0010:f2fs_inplace_write_data+0x232/0x240
    [  699.729076] Code: ff e9 cf fe ff ff 49 8d 7d 10 e8 39 45 ad ff 4d 8b 7d 10 be 04 00 00 00 49 8d 7f 48 e8 07 49 ad ff 45 8b 7f 48 e9 fb fe ff ff <0f> 0b f0 41 80 4d 48 04 e9 65 fe ff ff 90 66 66 66 66 90 55 48 8d
    [  699.729130] RSP: 0018:ffff8801f43af568 EFLAGS: 00010202
    [  699.729139] RAX: 000000000000003f RBX: ffff8801f43af7b8 RCX: ffffffffb88c9113
    [  699.729142] RDX: 0000000000000003 RSI: dffffc0000000000 RDI: ffff8802024e5540
    [  699.729144] RBP: ffff8801f43af590 R08: 0000000000000009 R09: ffffffffffffffe8
    [  699.729147] R10: 0000000000000001 R11: ffffed0039b0596a R12: ffff8802024e5540
    [  699.729149] R13: ffff8801f0335500 R14: ffff8801e3e7a700 R15: ffff8801e1ee4450
    [  699.729154] FS:  00007f9bf97f5700(0000) GS:ffff8801f6e00000(0000) knlGS:0000000000000000
    [  699.729156] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [  699.729159] CR2: 00007f9bf925d170 CR3: 00000001f0c34000 CR4: 00000000000006f0
    [  699.729171] Call Trace:
    [  699.729192]  f2fs_do_write_data_page+0x2e2/0xe00
    [  699.729203]  ? f2fs_should_update_outplace+0xd0/0xd0
    [  699.729238]  ? memcg_drain_all_list_lrus+0x280/0x280
    [  699.729269]  ? __radix_tree_replace+0xa3/0x120
    [  699.729276]  __write_data_page+0x5c7/0xe30
    [  699.729291]  ? kasan_check_read+0x11/0x20
    [  699.729310]  ? page_mapped+0x8a/0x110
    [  699.729321]  ? page_mkclean+0xe9/0x160
    [  699.729327]  ? f2fs_do_write_data_page+0xe00/0xe00
    [  699.729331]  ? invalid_page_referenced_vma+0x130/0x130
    [  699.729345]  ? clear_page_dirty_for_io+0x332/0x450
    [  699.729351]  f2fs_write_cache_pages+0x4ca/0x860
    [  699.729358]  ? __write_data_page+0xe30/0xe30
    [  699.729374]  ? percpu_counter_add_batch+0x22/0xa0
    [  699.729380]  ? kasan_check_write+0x14/0x20
    [  699.729391]  ? _raw_spin_lock+0x17/0x40
    [  699.729403]  ? f2fs_mark_inode_dirty_sync.part.18+0x16/0x30
    [  699.729413]  ? iov_iter_advance+0x113/0x640
    [  699.729418]  ? f2fs_write_end+0x133/0x2e0
    [  699.729423]  ? balance_dirty_pages_ratelimited+0x239/0x640
    [  699.729428]  f2fs_write_data_pages+0x329/0x520
    [  699.729433]  ? generic_perform_write+0x250/0x320
    [  699.729438]  ? f2fs_write_cache_pages+0x860/0x860
    [  699.729454]  ? current_time+0x110/0x110
    [  699.729459]  ? f2fs_preallocate_blocks+0x1ef/0x370
    [  699.729464]  do_writepages+0x37/0xb0
    [  699.729468]  ? f2fs_write_cache_pages+0x860/0x860
    [  699.729472]  ? do_writepages+0x37/0xb0
    [  699.729478]  __filemap_fdatawrite_range+0x19a/0x1f0
    [  699.729483]  ? delete_from_page_cache_batch+0x4e0/0x4e0
    [  699.729496]  ? __vfs_write+0x2b2/0x410
    [  699.729501]  file_write_and_wait_range+0x66/0xb0
    [  699.729506]  f2fs_do_sync_file+0x1f9/0xd90
    [  699.729511]  ? truncate_partial_data_page+0x290/0x290
    [  699.729521]  ? __sb_end_write+0x30/0x50
    [  699.729526]  ? vfs_write+0x20f/0x260
    [  699.729530]  f2fs_sync_file+0x9a/0xb0
    [  699.729534]  ? f2fs_do_sync_file+0xd90/0xd90
    [  699.729548]  vfs_fsync_range+0x68/0x100
    [  699.729554]  ? __fget_light+0xc9/0xe0
    [  699.729558]  do_fsync+0x3d/0x70
    [  699.729562]  __x64_sys_fdatasync+0x24/0x30
    [  699.729585]  do_syscall_64+0x78/0x170
    [  699.729595]  entry_SYSCALL_64_after_hwframe+0x44/0xa9
    [  699.729613] RIP: 0033:0x7f9bf930d800
    [  699.729615] Code: 00 f7 d8 64 89 01 48 83 c8 ff c3 66 2e 0f 1f 84 00 00 00 00 00 0f 1f 44 00 00 83 3d 49 bf 2c 00 00 75 10 b8 4b 00 00 00 0f 05 <48> 3d 01 f0 ff ff 73 31 c3 48 83 ec 08 e8 be 78 01 00 48 89 04 24
    [  699.729668] RSP: 002b:00007ffee3606c68 EFLAGS: 00000246 ORIG_RAX: 000000000000004b
    [  699.729673] RAX: ffffffffffffffda RBX: 0000000000000000 RCX: 00007f9bf930d800
    [  699.729675] RDX: 0000000000008000 RSI: 00000000006010a0 RDI: 0000000000000003
    [  699.729678] RBP: 00007ffee3606ca0 R08: 0000000001503010 R09: 0000000000000000
    [  699.729680] R10: 00000000000002e8 R11: 0000000000000246 R12: 0000000000400610
    [  699.729683] R13: 00007ffee3606da0 R14: 0000000000000000 R15: 0000000000000000
    [  699.729687] ---[ end trace 4ce02f25ff7d3df5 ]---
    [  699.729782] ------------[ cut here ]------------
    [  699.729785] kernel BUG at fs/f2fs/segment.h:654!
    [  699.731055] invalid opcode: 0000 [#1] SMP KASAN PTI
    [  699.732104] CPU: 0 PID: 1309 Comm: a.out Tainted: G        W         4.18.0-rc1+ #4
    [  699.733684] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS Ubuntu-1.8.2-1ubuntu1 04/01/2014
    [  699.735611] RIP: 0010:f2fs_submit_page_bio+0x29b/0x730
    [  699.736649] Code: 54 49 8d bd 18 04 00 00 e8 b2 59 af ff 41 8b 8d 18 04 00 00 8b 45 b8 41 d3 e6 44 01 f0 4c 8d 73 14 41 39 c7 0f 82 37 fe ff ff <0f> 0b 65 8b 05 2c 04 77 47 89 c0 48 0f a3 05 52 c1 d5 01 0f 92 c0
    [  699.740524] RSP: 0018:ffff8801f43af508 EFLAGS: 00010283
    [  699.741573] RAX: 0000000000000000 RBX: ffff8801f43af7b8 RCX: ffffffffb88a7cef
    [  699.743006] RDX: 0000000000000007 RSI: dffffc0000000000 RDI: ffff8801e3e7a64c
    [  699.744426] RBP: ffff8801f43af558 R08: ffffed003e066b55 R09: ffffed003e066b55
    [  699.745833] R10: 0000000000000001 R11: ffffed003e066b54 R12: ffffea0007876940
    [  699.747256] R13: ffff8801f0335500 R14: ffff8801e3e7a600 R15: 0000000000000001
    [  699.748683] FS:  00007f9bf97f5700(0000) GS:ffff8801f6e00000(0000) knlGS:0000000000000000
    [  699.750293] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [  699.751462] CR2: 00007f9bf925d170 CR3: 00000001f0c34000 CR4: 00000000000006f0
    [  699.752874] Call Trace:
    [  699.753386]  ? f2fs_inplace_write_data+0x93/0x240
    [  699.754341]  f2fs_inplace_write_data+0xd2/0x240
    [  699.755271]  f2fs_do_write_data_page+0x2e2/0xe00
    [  699.756214]  ? f2fs_should_update_outplace+0xd0/0xd0
    [  699.757215]  ? memcg_drain_all_list_lrus+0x280/0x280
    [  699.758209]  ? __radix_tree_replace+0xa3/0x120
    [  699.759164]  __write_data_page+0x5c7/0xe30
    [  699.760002]  ? kasan_check_read+0x11/0x20
    [  699.760823]  ? page_mapped+0x8a/0x110
    [  699.761573]  ? page_mkclean+0xe9/0x160
    [  699.762345]  ? f2fs_do_write_data_page+0xe00/0xe00
    [  699.763332]  ? invalid_page_referenced_vma+0x130/0x130
    [  699.764374]  ? clear_page_dirty_for_io+0x332/0x450
    [  699.765347]  f2fs_write_cache_pages+0x4ca/0x860
    [  699.766276]  ? __write_data_page+0xe30/0xe30
    [  699.767161]  ? percpu_counter_add_batch+0x22/0xa0
    [  699.768112]  ? kasan_check_write+0x14/0x20
    [  699.768951]  ? _raw_spin_lock+0x17/0x40
    [  699.769739]  ? f2fs_mark_inode_dirty_sync.part.18+0x16/0x30
    [  699.770885]  ? iov_iter_advance+0x113/0x640
    [  699.771743]  ? f2fs_write_end+0x133/0x2e0
    [  699.772569]  ? balance_dirty_pages_ratelimited+0x239/0x640
    [  699.773680]  f2fs_write_data_pages+0x329/0x520
    [  699.774603]  ? generic_perform_write+0x250/0x320
    [  699.775544]  ? f2fs_write_cache_pages+0x860/0x860
    [  699.776510]  ? current_time+0x110/0x110
    [  699.777299]  ? f2fs_preallocate_blocks+0x1ef/0x370
    [  699.778279]  do_writepages+0x37/0xb0
    [  699.779026]  ? f2fs_write_cache_pages+0x860/0x860
    [  699.779978]  ? do_writepages+0x37/0xb0
    [  699.780755]  __filemap_fdatawrite_range+0x19a/0x1f0
    [  699.781746]  ? delete_from_page_cache_batch+0x4e0/0x4e0
    [  699.782820]  ? __vfs_write+0x2b2/0x410
    [  699.783597]  file_write_and_wait_range+0x66/0xb0
    [  699.784540]  f2fs_do_sync_file+0x1f9/0xd90
    [  699.785381]  ? truncate_partial_data_page+0x290/0x290
    [  699.786415]  ? __sb_end_write+0x30/0x50
    [  699.787204]  ? vfs_write+0x20f/0x260
    [  699.787941]  f2fs_sync_file+0x9a/0xb0
    [  699.788694]  ? f2fs_do_sync_file+0xd90/0xd90
    [  699.789572]  vfs_fsync_range+0x68/0x100
    [  699.790360]  ? __fget_light+0xc9/0xe0
    [  699.791128]  do_fsync+0x3d/0x70
    [  699.791779]  __x64_sys_fdatasync+0x24/0x30
    [  699.792614]  do_syscall_64+0x78/0x170
    [  699.793371]  entry_SYSCALL_64_after_hwframe+0x44/0xa9
    [  699.794406] RIP: 0033:0x7f9bf930d800
    [  699.795134] Code: 00 f7 d8 64 89 01 48 83 c8 ff c3 66 2e 0f 1f 84 00 00 00 00 00 0f 1f 44 00 00 83 3d 49 bf 2c 00 00 75 10 b8 4b 00 00 00 0f 05 <48> 3d 01 f0 ff ff 73 31 c3 48 83 ec 08 e8 be 78 01 00 48 89 04 24
    [  699.798960] RSP: 002b:00007ffee3606c68 EFLAGS: 00000246 ORIG_RAX: 000000000000004b
    [  699.800483] RAX: ffffffffffffffda RBX: 0000000000000000 RCX: 00007f9bf930d800
    [  699.801923] RDX: 0000000000008000 RSI: 00000000006010a0 RDI: 0000000000000003
    [  699.803373] RBP: 00007ffee3606ca0 R08: 0000000001503010 R09: 0000000000000000
    [  699.804798] R10: 00000000000002e8 R11: 0000000000000246 R12: 0000000000400610
    [  699.806233] R13: 00007ffee3606da0 R14: 0000000000000000 R15: 0000000000000000
    [  699.807667] Modules linked in: snd_hda_codec_generic snd_hda_intel snd_hda_codec snd_hwdep snd_hda_core snd_pcm snd_timer snd mac_hid i2c_piix4 soundcore ib_iser rdma_cm iw_cm ib_cm ib_core iscsi_tcp libiscsi_tcp libiscsi scsi_transport_iscsi raid10 raid456 async_raid6_recov async_memcpy async_pq async_xor async_tx raid1 raid0 multipath linear 8139too crct10dif_pclmul crc32_pclmul qxl drm_kms_helper syscopyarea aesni_intel sysfillrect sysimgblt fb_sys_fops ttm drm aes_x86_64 crypto_simd cryptd 8139cp glue_helper mii pata_acpi floppy
    [  699.817079] ---[ end trace 4ce02f25ff7d3df6 ]---
    [  699.818068] RIP: 0010:f2fs_submit_page_bio+0x29b/0x730
    [  699.819114] Code: 54 49 8d bd 18 04 00 00 e8 b2 59 af ff 41 8b 8d 18 04 00 00 8b 45 b8 41 d3 e6 44 01 f0 4c 8d 73 14 41 39 c7 0f 82 37 fe ff ff <0f> 0b 65 8b 05 2c 04 77 47 89 c0 48 0f a3 05 52 c1 d5 01 0f 92 c0
    [  699.822919] RSP: 0018:ffff8801f43af508 EFLAGS: 00010283
    [  699.823977] RAX: 0000000000000000 RBX: ffff8801f43af7b8 RCX: ffffffffb88a7cef
    [  699.825436] RDX: 0000000000000007 RSI: dffffc0000000000 RDI: ffff8801e3e7a64c
    [  699.826881] RBP: ffff8801f43af558 R08: ffffed003e066b55 R09: ffffed003e066b55
    [  699.828292] R10: 0000000000000001 R11: ffffed003e066b54 R12: ffffea0007876940
    [  699.829750] R13: ffff8801f0335500 R14: ffff8801e3e7a600 R15: 0000000000000001
    [  699.831192] FS:  00007f9bf97f5700(0000) GS:ffff8801f6e00000(0000) knlGS:0000000000000000
    [  699.832793] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [  699.833981] CR2: 00007f9bf925d170 CR3: 00000001f0c34000 CR4: 00000000000006f0
    [  699.835556] ==================================================================
    [  699.837029] BUG: KASAN: stack-out-of-bounds in update_stack_state+0x38c/0x3e0
    [  699.838462] Read of size 8 at addr ffff8801f43af970 by task a.out/1309
    
    [  699.840086] CPU: 0 PID: 1309 Comm: a.out Tainted: G      D W         4.18.0-rc1+ #4
    [  699.841603] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS Ubuntu-1.8.2-1ubuntu1 04/01/2014
    [  699.843475] Call Trace:
    [  699.843982]  dump_stack+0x7b/0xb5
    [  699.844661]  print_address_description+0x70/0x290
    [  699.845607]  kasan_report+0x291/0x390
    [  699.846351]  ? update_stack_state+0x38c/0x3e0
    [  699.853831]  __asan_load8+0x54/0x90
    [  699.854569]  update_stack_state+0x38c/0x3e0
    [  699.855428]  ? __read_once_size_nocheck.constprop.7+0x20/0x20
    [  699.856601]  ? __save_stack_trace+0x5e/0x100
    [  699.857476]  unwind_next_frame.part.5+0x18e/0x490
    [  699.858448]  ? unwind_dump+0x290/0x290
    [  699.859217]  ? clear_page_dirty_for_io+0x332/0x450
    [  699.860185]  __unwind_start+0x106/0x190
    [  699.860974]  __save_stack_trace+0x5e/0x100
    [  699.861808]  ? __save_stack_trace+0x5e/0x100
    [  699.862691]  ? unlink_anon_vmas+0xba/0x2c0
    [  699.863525]  save_stack_trace+0x1f/0x30
    [  699.864312]  save_stack+0x46/0xd0
    [  699.864993]  ? __alloc_pages_slowpath+0x1420/0x1420
    [  699.865990]  ? flush_tlb_mm_range+0x15e/0x220
    [  699.866889]  ? kasan_check_write+0x14/0x20
    [  699.867724]  ? __dec_node_state+0x92/0xb0
    [  699.868543]  ? lock_page_memcg+0x85/0xf0
    [  699.869350]  ? unlock_page_memcg+0x16/0x80
    [  699.870185]  ? page_remove_rmap+0x198/0x520
    [  699.871048]  ? mark_page_accessed+0x133/0x200
    [  699.871930]  ? _cond_resched+0x1a/0x50
    [  699.872700]  ? unmap_page_range+0xcd4/0xe50
    [  699.873551]  ? rb_next+0x58/0x80
    [  699.874217]  ? rb_next+0x58/0x80
    [  699.874895]  __kasan_slab_free+0x13c/0x1a0
    [  699.875734]  ? unlink_anon_vmas+0xba/0x2c0
    [  699.876563]  kasan_slab_free+0xe/0x10
    [  699.877315]  kmem_cache_free+0x89/0x1e0
    [  699.878095]  unlink_anon_vmas+0xba/0x2c0
    [  699.878913]  free_pgtables+0x101/0x1b0
    [  699.879677]  exit_mmap+0x146/0x2a0
    [  699.880378]  ? __ia32_sys_munmap+0x50/0x50
    [  699.881214]  ? kasan_check_read+0x11/0x20
    [  699.882052]  ? mm_update_next_owner+0x322/0x380
    [  699.882985]  mmput+0x8b/0x1d0
    [  699.883602]  do_exit+0x43a/0x1390
    [  699.884288]  ? mm_update_next_owner+0x380/0x380
    [  699.885212]  ? f2fs_sync_file+0x9a/0xb0
    [  699.885995]  ? f2fs_do_sync_file+0xd90/0xd90
    [  699.886877]  ? vfs_fsync_range+0x68/0x100
    [  699.887694]  ? __fget_light+0xc9/0xe0
    [  699.888442]  ? do_fsync+0x3d/0x70
    [  699.889118]  ? __x64_sys_fdatasync+0x24/0x30
    [  699.889996]  rewind_stack_do_exit+0x17/0x20
    [  699.890860] RIP: 0033:0x7f9bf930d800
    [  699.891585] Code: Bad RIP value.
    [  699.892268] RSP: 002b:00007ffee3606c68 EFLAGS: 00000246 ORIG_RAX: 000000000000004b
    [  699.893781] RAX: ffffffffffffffda RBX: 0000000000000000 RCX: 00007f9bf930d800
    [  699.895220] RDX: 0000000000008000 RSI: 00000000006010a0 RDI: 0000000000000003
    [  699.896643] RBP: 00007ffee3606ca0 R08: 0000000001503010 R09: 0000000000000000
    [  699.898069] R10: 00000000000002e8 R11: 0000000000000246 R12: 0000000000400610
    [  699.899505] R13: 00007ffee3606da0 R14: 0000000000000000 R15: 0000000000000000
    
    [  699.901241] The buggy address belongs to the page:
    [  699.902215] page:ffffea0007d0ebc0 count:0 mapcount:0 mapping:0000000000000000 index:0x0
    [  699.903811] flags: 0x2ffff0000000000()
    [  699.904585] raw: 02ffff0000000000 0000000000000000 ffffffff07d00101 0000000000000000
    [  699.906125] raw: 0000000000000000 0000000000240000 00000000ffffffff 0000000000000000
    [  699.907673] page dumped because: kasan: bad access detected
    
    [  699.909108] Memory state around the buggy address:
    [  699.910077]  ffff8801f43af800: 00 f1 f1 f1 f1 00 f4 f4 f4 f3 f3 f3 f3 00 00 00
    [  699.911528]  ffff8801f43af880: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
    [  699.912953] >ffff8801f43af900: 00 00 00 00 00 00 00 00 f1 01 f4 f4 f4 f2 f2 f2
    [  699.914392]                                                              ^
    [  699.915758]  ffff8801f43af980: f2 00 f4 f4 00 00 00 00 f2 00 00 00 00 00 00 00
    [  699.917193]  ffff8801f43afa00: 00 00 00 00 00 00 00 00 00 f3 f3 f3 00 00 00 00
    [  699.918634] ==================================================================
    
    - Location
    https://elixir.bootlin.com/linux/v4.18-rc1/source/fs/f2fs/segment.h#L644
    
    Reported-by Wen Xu <wen.xu@gatech.edu>
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 635a98db5d65..4064ce246c13 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -441,7 +441,10 @@ int f2fs_submit_page_bio(struct f2fs_io_info *fio)
 	struct page *page = fio->encrypted_page ?
 			fio->encrypted_page : fio->page;
 
-	verify_block_addr(fio, fio->new_blkaddr);
+	if (!f2fs_is_valid_blkaddr(fio->sbi, fio->new_blkaddr,
+			__is_meta_io(fio) ? META_GENERIC : DATA_GENERIC))
+		return -EFAULT;
+
 	trace_f2fs_submit_page_bio(page, fio);
 	f2fs_trace_ios(fio, 0);
 
@@ -1045,6 +1048,12 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 next_block:
 	blkaddr = datablock_addr(dn.inode, dn.node_page, dn.ofs_in_node);
 
+	if (__is_valid_data_blkaddr(blkaddr) &&
+		!f2fs_is_valid_blkaddr(sbi, blkaddr, DATA_GENERIC)) {
+		err = -EFAULT;
+		goto sync_out;
+	}
+
 	if (!is_valid_data_blkaddr(sbi, blkaddr)) {
 		if (create) {
 			if (unlikely(f2fs_cp_error(sbi))) {
@@ -1500,6 +1509,10 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 				SetPageUptodate(page);
 				goto confused;
 			}
+
+			if (!f2fs_is_valid_blkaddr(F2FS_I_SB(inode), block_nr,
+								DATA_GENERIC))
+				goto set_error_page;
 		} else {
 			zero_user_segment(page, 0, PAGE_SIZE);
 			if (!PageUptodate(page))
@@ -1700,11 +1713,13 @@ int f2fs_do_write_data_page(struct f2fs_io_info *fio)
 			f2fs_lookup_extent_cache(inode, page->index, &ei)) {
 		fio->old_blkaddr = ei.blk + page->index - ei.fofs;
 
-		if (is_valid_data_blkaddr(fio->sbi, fio->old_blkaddr)) {
-			ipu_force = true;
-			fio->need_lock = LOCK_DONE;
-			goto got_it;
-		}
+		if (!f2fs_is_valid_blkaddr(fio->sbi, fio->old_blkaddr,
+							DATA_GENERIC))
+			return -EFAULT;
+
+		ipu_force = true;
+		fio->need_lock = LOCK_DONE;
+		goto got_it;
 	}
 
 	/* Deadlock due to between page->lock and f2fs_lock_op */
@@ -1723,6 +1738,12 @@ int f2fs_do_write_data_page(struct f2fs_io_info *fio)
 		goto out_writepage;
 	}
 got_it:
+	if (__is_valid_data_blkaddr(fio->old_blkaddr) &&
+		!f2fs_is_valid_blkaddr(fio->sbi, fio->old_blkaddr,
+							DATA_GENERIC)) {
+		err = -EFAULT;
+		goto out_writepage;
+	}
 	/*
 	 * If current allocation needs SSR,
 	 * it had better in-place writes for updated data.

commit e1da7872f6eda977bd812346bf588c35e4495a1e
Author: Chao Yu <yuchao0@huawei.com>
Date:   Tue Jun 5 17:44:11 2018 +0800

    f2fs: introduce and spread verify_blkaddr
    
    This patch introduces verify_blkaddr to check meta/data block address
    with valid range to detect bug earlier.
    
    In addition, once we encounter an invalid blkaddr, notice user to run
    fsck to fix, and let the kernel panic.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index b6a90c853221..635a98db5d65 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -485,7 +485,7 @@ void f2fs_submit_page_write(struct f2fs_io_info *fio)
 		spin_unlock(&io->io_lock);
 	}
 
-	if (is_valid_blkaddr(fio->old_blkaddr))
+	if (__is_valid_data_blkaddr(fio->old_blkaddr))
 		verify_block_addr(fio, fio->old_blkaddr);
 	verify_block_addr(fio, fio->new_blkaddr);
 
@@ -1045,7 +1045,7 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 next_block:
 	blkaddr = datablock_addr(dn.inode, dn.node_page, dn.ofs_in_node);
 
-	if (!is_valid_blkaddr(blkaddr)) {
+	if (!is_valid_data_blkaddr(sbi, blkaddr)) {
 		if (create) {
 			if (unlikely(f2fs_cp_error(sbi))) {
 				err = -EIO;
@@ -1700,7 +1700,7 @@ int f2fs_do_write_data_page(struct f2fs_io_info *fio)
 			f2fs_lookup_extent_cache(inode, page->index, &ei)) {
 		fio->old_blkaddr = ei.blk + page->index - ei.fofs;
 
-		if (is_valid_blkaddr(fio->old_blkaddr)) {
+		if (is_valid_data_blkaddr(fio->sbi, fio->old_blkaddr)) {
 			ipu_force = true;
 			fio->need_lock = LOCK_DONE;
 			goto got_it;
@@ -1727,7 +1727,7 @@ int f2fs_do_write_data_page(struct f2fs_io_info *fio)
 	 * If current allocation needs SSR,
 	 * it had better in-place writes for updated data.
 	 */
-	if (ipu_force || (is_valid_blkaddr(fio->old_blkaddr) &&
+	if (ipu_force || (is_valid_data_blkaddr(fio->sbi, fio->old_blkaddr) &&
 					need_inplace_update(fio))) {
 		err = encrypt_one_page(fio);
 		if (err)

commit e2e59414aae2c8036dfaa57cf6a578e1694945e8
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Jun 21 11:29:43 2018 -0700

    f2fs: assign REQ_RAHEAD to bio for ->readpages
    
    As Jens reported, we'd better assign REQ_RAHEAD to bio by the fact that
    ->readpages is called only from read-ahead.
    
    In Documentation/filesystems/vfs.txt,
    
    readpages: called by the VM to read pages associated with the address_space
            object. This is essentially just a vector version of
            readpage.  Instead of just one page, several pages are
            requested.
            readpages is only used for read-ahead, so read errors are
            ignored.  If anything goes wrong, feel free to give up.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 5e53d210e222..b6a90c853221 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -534,7 +534,7 @@ void f2fs_submit_page_write(struct f2fs_io_info *fio)
 }
 
 static struct bio *f2fs_grab_read_bio(struct inode *inode, block_t blkaddr,
-							 unsigned nr_pages)
+					unsigned nr_pages, unsigned op_flag)
 {
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
 	struct bio *bio;
@@ -546,7 +546,7 @@ static struct bio *f2fs_grab_read_bio(struct inode *inode, block_t blkaddr,
 		return ERR_PTR(-ENOMEM);
 	f2fs_target_device(sbi, blkaddr, bio);
 	bio->bi_end_io = f2fs_read_end_io;
-	bio_set_op_attrs(bio, REQ_OP_READ, 0);
+	bio_set_op_attrs(bio, REQ_OP_READ, op_flag);
 
 	if (f2fs_encrypted_file(inode))
 		post_read_steps |= 1 << STEP_DECRYPT;
@@ -571,7 +571,7 @@ static struct bio *f2fs_grab_read_bio(struct inode *inode, block_t blkaddr,
 static int f2fs_submit_page_read(struct inode *inode, struct page *page,
 							block_t blkaddr)
 {
-	struct bio *bio = f2fs_grab_read_bio(inode, blkaddr, 1);
+	struct bio *bio = f2fs_grab_read_bio(inode, blkaddr, 1, 0);
 
 	if (IS_ERR(bio))
 		return PTR_ERR(bio);
@@ -1421,10 +1421,15 @@ int f2fs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,
 /*
  * This function was originally taken from fs/mpage.c, and customized for f2fs.
  * Major change was from block_size == page_size in f2fs by default.
+ *
+ * Note that the aops->readpages() function is ONLY used for read-ahead. If
+ * this function ever deviates from doing just read-ahead, it should either
+ * use ->readpage() or do the necessary surgery to decouple ->readpages()
+ * from read-ahead.
  */
 static int f2fs_mpage_readpages(struct address_space *mapping,
 			struct list_head *pages, struct page *page,
-			unsigned nr_pages)
+			unsigned nr_pages, bool is_readahead)
 {
 	struct bio *bio = NULL;
 	sector_t last_block_in_bio = 0;
@@ -1514,7 +1519,8 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 			bio = NULL;
 		}
 		if (bio == NULL) {
-			bio = f2fs_grab_read_bio(inode, block_nr, nr_pages);
+			bio = f2fs_grab_read_bio(inode, block_nr, nr_pages,
+					is_readahead ? REQ_RAHEAD : 0);
 			if (IS_ERR(bio)) {
 				bio = NULL;
 				goto set_error_page;
@@ -1558,7 +1564,7 @@ static int f2fs_read_data_page(struct file *file, struct page *page)
 	if (f2fs_has_inline_data(inode))
 		ret = f2fs_read_inline_data(inode, page);
 	if (ret == -EAGAIN)
-		ret = f2fs_mpage_readpages(page->mapping, NULL, page, 1);
+		ret = f2fs_mpage_readpages(page->mapping, NULL, page, 1, false);
 	return ret;
 }
 
@@ -1575,7 +1581,7 @@ static int f2fs_read_data_pages(struct file *file,
 	if (f2fs_has_inline_data(inode))
 		return 0;
 
-	return f2fs_mpage_readpages(mapping, pages, NULL, nr_pages);
+	return f2fs_mpage_readpages(mapping, pages, NULL, nr_pages, true);
 }
 
 static int encrypt_one_page(struct f2fs_io_info *fio)

commit 8a56dd9685d6531d09b370ab22a61b9687131875
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Jun 29 18:55:12 2018 -0700

    f2fs: allow wrong configured dio to buffered write
    
    This fixes to support dio having unaligned buffers as buffered writes.
    
    xfs_io -f -d -c "pwrite 0 512" $testfile
     -> okay
    
    xfs_io -f -d -c "pwrite 1 512" $testfile
     -> EINVAL
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 8f931d699287..5e53d210e222 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -2371,14 +2371,20 @@ static int f2fs_write_end(struct file *file,
 static int check_direct_IO(struct inode *inode, struct iov_iter *iter,
 			   loff_t offset)
 {
-	unsigned blocksize_mask = inode->i_sb->s_blocksize - 1;
-
-	if (offset & blocksize_mask)
-		return -EINVAL;
-
-	if (iov_iter_alignment(iter) & blocksize_mask)
-		return -EINVAL;
-
+	unsigned i_blkbits = READ_ONCE(inode->i_blkbits);
+	unsigned blkbits = i_blkbits;
+	unsigned blocksize_mask = (1 << blkbits) - 1;
+	unsigned long align = offset | iov_iter_alignment(iter);
+	struct block_device *bdev = inode->i_sb->s_bdev;
+
+	if (align & blocksize_mask) {
+		if (bdev)
+			blkbits = blksize_bits(bdev_logical_block_size(bdev));
+		blocksize_mask = (1 << blkbits) - 1;
+		if (align & blocksize_mask)
+			return -EINVAL;
+		return 1;
+	}
 	return 0;
 }
 
@@ -2396,7 +2402,7 @@ static ssize_t f2fs_direct_IO(struct kiocb *iocb, struct iov_iter *iter)
 
 	err = check_direct_IO(inode, iter, offset);
 	if (err)
-		return err;
+		return err < 0 ? err : 0;
 
 	if (f2fs_force_buffered_io(inode, rw))
 		return 0;

commit c29fd0c0e26dacb7a33ad166587059818a94b4e0
Author: Chao Yu <yuchao0@huawei.com>
Date:   Mon Jun 4 23:20:36 2018 +0800

    f2fs: let sync node IO interrupt async one
    
    Although mixed sync/async IOs can have continuous LBA, as they have
    different IO priority, block IO scheduler will add them into different
    queues and commit them separately, result in splited IOs which causes
    wrose performance.
    
    This patch gives high priority to synchronous IO of nodes, means that
    once synchronous flow starts, it can interrupt asynchronous writeback
    flow of system flusher, so more big IOs can be expected.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index a2e30c403ef2..8f931d699287 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1933,6 +1933,7 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 	int ret = 0;
 	int done = 0;
 	struct pagevec pvec;
+	struct f2fs_sb_info *sbi = F2FS_M_SB(mapping);
 	int nr_pages;
 	pgoff_t uninitialized_var(writeback_index);
 	pgoff_t index;
@@ -1987,7 +1988,7 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 			bool submitted = false;
 
 			/* give a priority to WB_SYNC threads */
-			if (atomic_read(&F2FS_M_SB(mapping)->wb_sync_req) &&
+			if (atomic_read(&sbi->wb_sync_req[DATA]) &&
 					wbc->sync_mode == WB_SYNC_NONE) {
 				done = 1;
 				break;
@@ -2107,8 +2108,8 @@ static int __f2fs_write_data_pages(struct address_space *mapping,
 
 	/* to avoid spliting IOs due to mixed WB_SYNC_ALL and WB_SYNC_NONE */
 	if (wbc->sync_mode == WB_SYNC_ALL)
-		atomic_inc(&sbi->wb_sync_req);
-	else if (atomic_read(&sbi->wb_sync_req))
+		atomic_inc(&sbi->wb_sync_req[DATA]);
+	else if (atomic_read(&sbi->wb_sync_req[DATA]))
 		goto skip_write;
 
 	blk_start_plug(&plug);
@@ -2116,7 +2117,7 @@ static int __f2fs_write_data_pages(struct address_space *mapping,
 	blk_finish_plug(&plug);
 
 	if (wbc->sync_mode == WB_SYNC_ALL)
-		atomic_dec(&sbi->wb_sync_req);
+		atomic_dec(&sbi->wb_sync_req[DATA]);
 	/*
 	 * if some pages were truncated, we cannot guarantee its mapping->host
 	 * to detect pending bios.

commit 4d57b86dd86404fd8bb4f87d277d5a86a7fe537e
Author: Chao Yu <yuchao0@huawei.com>
Date:   Wed May 30 00:20:41 2018 +0800

    f2fs: clean up symbol namespace
    
    As Ted reported:
    
    "Hi, I was looking at f2fs's sources recently, and I noticed that there
    is a very large number of non-static symbols which don't have a f2fs
    prefix.  There's well over a hundred (see attached below).
    
    As one example, in fs/f2fs/dir.c there is:
    
    unsigned char get_de_type(struct f2fs_dir_entry *de)
    
    This function is clearly only useful for f2fs, but it has a generic
    name.  This means that if any other file system tries to have the same
    symbol name, there will be a symbol conflict and the kernel would not
    successfully build.  It also means that when someone is looking f2fs
    sources, it's not at all obvious whether a function such as
    read_data_page(), invalidate_blocks(), is a generic kernel function
    found in the fs, mm, or block layers, or a f2fs specific function.
    
    You might want to fix this at some point.  Hopefully Kent's bcachefs
    isn't similarly using genericly named functions, since that might
    cause conflicts with f2fs's functions --- but just as this would be a
    problem that we would rightly insist that Kent fix, this is something
    that we should have rightly insisted that f2fs should have fixed
    before it was integrated into the mainline kernel.
    
    acquire_orphan_inode
    add_ino_entry
    add_orphan_inode
    allocate_data_block
    allocate_new_segments
    alloc_nid
    alloc_nid_done
    alloc_nid_failed
    available_free_memory
    ...."
    
    This patch adds "f2fs_" prefix for all non-static symbols in order to:
    a) avoid conflict with other kernel generic symbols;
    b) to indicate the function is f2fs specific one instead of generic
    one;
    
    Reported-by: Theodore Ts'o <tytso@mit.edu>
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 10aa4d9a4b7c..a2e30c403ef2 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -247,7 +247,7 @@ static struct bio *__bio_alloc(struct f2fs_sb_info *sbi, block_t blk_addr,
 	} else {
 		bio->bi_end_io = f2fs_write_end_io;
 		bio->bi_private = sbi;
-		bio->bi_write_hint = io_type_to_rw_hint(sbi, type, temp);
+		bio->bi_write_hint = f2fs_io_type_to_rw_hint(sbi, type, temp);
 	}
 	if (wbc)
 		wbc_init_bio(wbc, bio);
@@ -604,7 +604,7 @@ static void __set_data_blkaddr(struct dnode_of_data *dn)
  *  ->node_page
  *    update block addresses in the node page
  */
-void set_data_blkaddr(struct dnode_of_data *dn)
+void f2fs_set_data_blkaddr(struct dnode_of_data *dn)
 {
 	f2fs_wait_on_page_writeback(dn->node_page, NODE, true);
 	__set_data_blkaddr(dn);
@@ -615,12 +615,12 @@ void set_data_blkaddr(struct dnode_of_data *dn)
 void f2fs_update_data_blkaddr(struct dnode_of_data *dn, block_t blkaddr)
 {
 	dn->data_blkaddr = blkaddr;
-	set_data_blkaddr(dn);
+	f2fs_set_data_blkaddr(dn);
 	f2fs_update_extent_cache(dn);
 }
 
 /* dn->ofs_in_node will be returned with up-to-date last block pointer */
-int reserve_new_blocks(struct dnode_of_data *dn, blkcnt_t count)
+int f2fs_reserve_new_blocks(struct dnode_of_data *dn, blkcnt_t count)
 {
 	struct f2fs_sb_info *sbi = F2FS_I_SB(dn->inode);
 	int err;
@@ -654,12 +654,12 @@ int reserve_new_blocks(struct dnode_of_data *dn, blkcnt_t count)
 }
 
 /* Should keep dn->ofs_in_node unchanged */
-int reserve_new_block(struct dnode_of_data *dn)
+int f2fs_reserve_new_block(struct dnode_of_data *dn)
 {
 	unsigned int ofs_in_node = dn->ofs_in_node;
 	int ret;
 
-	ret = reserve_new_blocks(dn, 1);
+	ret = f2fs_reserve_new_blocks(dn, 1);
 	dn->ofs_in_node = ofs_in_node;
 	return ret;
 }
@@ -669,12 +669,12 @@ int f2fs_reserve_block(struct dnode_of_data *dn, pgoff_t index)
 	bool need_put = dn->inode_page ? false : true;
 	int err;
 
-	err = get_dnode_of_data(dn, index, ALLOC_NODE);
+	err = f2fs_get_dnode_of_data(dn, index, ALLOC_NODE);
 	if (err)
 		return err;
 
 	if (dn->data_blkaddr == NULL_ADDR)
-		err = reserve_new_block(dn);
+		err = f2fs_reserve_new_block(dn);
 	if (err || need_put)
 		f2fs_put_dnode(dn);
 	return err;
@@ -693,7 +693,7 @@ int f2fs_get_block(struct dnode_of_data *dn, pgoff_t index)
 	return f2fs_reserve_block(dn, index);
 }
 
-struct page *get_read_data_page(struct inode *inode, pgoff_t index,
+struct page *f2fs_get_read_data_page(struct inode *inode, pgoff_t index,
 						int op_flags, bool for_write)
 {
 	struct address_space *mapping = inode->i_mapping;
@@ -712,7 +712,7 @@ struct page *get_read_data_page(struct inode *inode, pgoff_t index,
 	}
 
 	set_new_dnode(&dn, inode, NULL, NULL, 0);
-	err = get_dnode_of_data(&dn, index, LOOKUP_NODE);
+	err = f2fs_get_dnode_of_data(&dn, index, LOOKUP_NODE);
 	if (err)
 		goto put_err;
 	f2fs_put_dnode(&dn);
@@ -731,7 +731,8 @@ struct page *get_read_data_page(struct inode *inode, pgoff_t index,
 	 * A new dentry page is allocated but not able to be written, since its
 	 * new inode page couldn't be allocated due to -ENOSPC.
 	 * In such the case, its blkaddr can be remained as NEW_ADDR.
-	 * see, f2fs_add_link -> get_new_data_page -> init_inode_metadata.
+	 * see, f2fs_add_link -> f2fs_get_new_data_page ->
+	 * f2fs_init_inode_metadata.
 	 */
 	if (dn.data_blkaddr == NEW_ADDR) {
 		zero_user_segment(page, 0, PAGE_SIZE);
@@ -751,7 +752,7 @@ struct page *get_read_data_page(struct inode *inode, pgoff_t index,
 	return ERR_PTR(err);
 }
 
-struct page *find_data_page(struct inode *inode, pgoff_t index)
+struct page *f2fs_find_data_page(struct inode *inode, pgoff_t index)
 {
 	struct address_space *mapping = inode->i_mapping;
 	struct page *page;
@@ -761,7 +762,7 @@ struct page *find_data_page(struct inode *inode, pgoff_t index)
 		return page;
 	f2fs_put_page(page, 0);
 
-	page = get_read_data_page(inode, index, 0, false);
+	page = f2fs_get_read_data_page(inode, index, 0, false);
 	if (IS_ERR(page))
 		return page;
 
@@ -781,13 +782,13 @@ struct page *find_data_page(struct inode *inode, pgoff_t index)
  * Because, the callers, functions in dir.c and GC, should be able to know
  * whether this page exists or not.
  */
-struct page *get_lock_data_page(struct inode *inode, pgoff_t index,
+struct page *f2fs_get_lock_data_page(struct inode *inode, pgoff_t index,
 							bool for_write)
 {
 	struct address_space *mapping = inode->i_mapping;
 	struct page *page;
 repeat:
-	page = get_read_data_page(inode, index, 0, for_write);
+	page = f2fs_get_read_data_page(inode, index, 0, for_write);
 	if (IS_ERR(page))
 		return page;
 
@@ -813,7 +814,7 @@ struct page *get_lock_data_page(struct inode *inode, pgoff_t index,
  * Note that, ipage is set only by make_empty_dir, and if any error occur,
  * ipage should be released by this function.
  */
-struct page *get_new_data_page(struct inode *inode,
+struct page *f2fs_get_new_data_page(struct inode *inode,
 		struct page *ipage, pgoff_t index, bool new_i_size)
 {
 	struct address_space *mapping = inode->i_mapping;
@@ -852,7 +853,7 @@ struct page *get_new_data_page(struct inode *inode,
 
 		/* if ipage exists, blkaddr should be NEW_ADDR */
 		f2fs_bug_on(F2FS_I_SB(inode), ipage);
-		page = get_lock_data_page(inode, index, true);
+		page = f2fs_get_lock_data_page(inode, index, true);
 		if (IS_ERR(page))
 			return page;
 	}
@@ -884,15 +885,15 @@ static int __allocate_data_block(struct dnode_of_data *dn, int seg_type)
 		return err;
 
 alloc:
-	get_node_info(sbi, dn->nid, &ni);
+	f2fs_get_node_info(sbi, dn->nid, &ni);
 	set_summary(&sum, dn->nid, dn->ofs_in_node, ni.version);
 
-	allocate_data_block(sbi, NULL, dn->data_blkaddr, &dn->data_blkaddr,
+	f2fs_allocate_data_block(sbi, NULL, dn->data_blkaddr, &dn->data_blkaddr,
 					&sum, seg_type, NULL, false);
-	set_data_blkaddr(dn);
+	f2fs_set_data_blkaddr(dn);
 
 	/* update i_size */
-	fofs = start_bidx_of_node(ofs_of_node(dn->node_page), dn->inode) +
+	fofs = f2fs_start_bidx_of_node(ofs_of_node(dn->node_page), dn->inode) +
 							dn->ofs_in_node;
 	if (i_size_read(dn->inode) < ((loff_t)(fofs + 1) << PAGE_SHIFT))
 		f2fs_i_size_write(dn->inode,
@@ -930,7 +931,7 @@ int f2fs_preallocate_blocks(struct kiocb *iocb, struct iov_iter *from)
 	map.m_seg_type = NO_CHECK_TYPE;
 
 	if (direct_io) {
-		map.m_seg_type = rw_hint_to_seg_type(iocb->ki_hint);
+		map.m_seg_type = f2fs_rw_hint_to_seg_type(iocb->ki_hint);
 		flag = f2fs_force_buffered_io(inode, WRITE) ?
 					F2FS_GET_BLOCK_PRE_AIO :
 					F2FS_GET_BLOCK_PRE_DIO;
@@ -1020,7 +1021,7 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 
 	/* When reading holes, we need its node page */
 	set_new_dnode(&dn, inode, NULL, NULL, 0);
-	err = get_dnode_of_data(&dn, pgofs, mode);
+	err = f2fs_get_dnode_of_data(&dn, pgofs, mode);
 	if (err) {
 		if (flag == F2FS_GET_BLOCK_BMAP)
 			map->m_pblk = 0;
@@ -1028,10 +1029,10 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 			err = 0;
 			if (map->m_next_pgofs)
 				*map->m_next_pgofs =
-					get_next_page_offset(&dn, pgofs);
+					f2fs_get_next_page_offset(&dn, pgofs);
 			if (map->m_next_extent)
 				*map->m_next_extent =
-					get_next_page_offset(&dn, pgofs);
+					f2fs_get_next_page_offset(&dn, pgofs);
 		}
 		goto unlock_out;
 	}
@@ -1117,7 +1118,7 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 			(pgofs == end || dn.ofs_in_node == end_offset)) {
 
 		dn.ofs_in_node = ofs_in_node;
-		err = reserve_new_blocks(&dn, prealloc);
+		err = f2fs_reserve_new_blocks(&dn, prealloc);
 		if (err)
 			goto sync_out;
 
@@ -1236,7 +1237,7 @@ static int get_data_block_dio(struct inode *inode, sector_t iblock,
 {
 	return __get_data_block(inode, iblock, bh_result, create,
 						F2FS_GET_BLOCK_DEFAULT, NULL,
-						rw_hint_to_seg_type(
+						f2fs_rw_hint_to_seg_type(
 							inode->i_write_hint));
 }
 
@@ -1281,7 +1282,7 @@ static int f2fs_xattr_fiemap(struct inode *inode,
 		if (!page)
 			return -ENOMEM;
 
-		get_node_info(sbi, inode->i_ino, &ni);
+		f2fs_get_node_info(sbi, inode->i_ino, &ni);
 
 		phys = (__u64)blk_to_logical(inode, ni.blk_addr);
 		offset = offsetof(struct f2fs_inode, i_addr) +
@@ -1308,7 +1309,7 @@ static int f2fs_xattr_fiemap(struct inode *inode,
 		if (!page)
 			return -ENOMEM;
 
-		get_node_info(sbi, xnid, &ni);
+		f2fs_get_node_info(sbi, xnid, &ni);
 
 		phys = (__u64)blk_to_logical(inode, ni.blk_addr);
 		len = inode->i_sb->s_blocksize;
@@ -1612,12 +1613,12 @@ static inline bool check_inplace_update_policy(struct inode *inode,
 
 	if (policy & (0x1 << F2FS_IPU_FORCE))
 		return true;
-	if (policy & (0x1 << F2FS_IPU_SSR) && need_SSR(sbi))
+	if (policy & (0x1 << F2FS_IPU_SSR) && f2fs_need_SSR(sbi))
 		return true;
 	if (policy & (0x1 << F2FS_IPU_UTIL) &&
 			utilization(sbi) > SM_I(sbi)->min_ipu_util)
 		return true;
-	if (policy & (0x1 << F2FS_IPU_SSR_UTIL) && need_SSR(sbi) &&
+	if (policy & (0x1 << F2FS_IPU_SSR_UTIL) && f2fs_need_SSR(sbi) &&
 			utilization(sbi) > SM_I(sbi)->min_ipu_util)
 		return true;
 
@@ -1638,7 +1639,7 @@ static inline bool check_inplace_update_policy(struct inode *inode,
 	return false;
 }
 
-bool should_update_inplace(struct inode *inode, struct f2fs_io_info *fio)
+bool f2fs_should_update_inplace(struct inode *inode, struct f2fs_io_info *fio)
 {
 	if (f2fs_is_pinned_file(inode))
 		return true;
@@ -1650,7 +1651,7 @@ bool should_update_inplace(struct inode *inode, struct f2fs_io_info *fio)
 	return check_inplace_update_policy(inode, fio);
 }
 
-bool should_update_outplace(struct inode *inode, struct f2fs_io_info *fio)
+bool f2fs_should_update_outplace(struct inode *inode, struct f2fs_io_info *fio)
 {
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
 
@@ -1673,13 +1674,13 @@ static inline bool need_inplace_update(struct f2fs_io_info *fio)
 {
 	struct inode *inode = fio->page->mapping->host;
 
-	if (should_update_outplace(inode, fio))
+	if (f2fs_should_update_outplace(inode, fio))
 		return false;
 
-	return should_update_inplace(inode, fio);
+	return f2fs_should_update_inplace(inode, fio);
 }
 
-int do_write_data_page(struct f2fs_io_info *fio)
+int f2fs_do_write_data_page(struct f2fs_io_info *fio)
 {
 	struct page *page = fio->page;
 	struct inode *inode = page->mapping->host;
@@ -1704,7 +1705,7 @@ int do_write_data_page(struct f2fs_io_info *fio)
 	if (fio->need_lock == LOCK_REQ && !f2fs_trylock_op(fio->sbi))
 		return -EAGAIN;
 
-	err = get_dnode_of_data(&dn, page->index, LOOKUP_NODE);
+	err = f2fs_get_dnode_of_data(&dn, page->index, LOOKUP_NODE);
 	if (err)
 		goto out;
 
@@ -1731,7 +1732,7 @@ int do_write_data_page(struct f2fs_io_info *fio)
 		f2fs_put_dnode(&dn);
 		if (fio->need_lock == LOCK_REQ)
 			f2fs_unlock_op(fio->sbi);
-		err = rewrite_data_page(fio);
+		err = f2fs_inplace_write_data(fio);
 		trace_f2fs_do_write_data_page(fio->page, IPU);
 		set_inode_flag(inode, FI_UPDATE_WRITE);
 		return err;
@@ -1753,7 +1754,7 @@ int do_write_data_page(struct f2fs_io_info *fio)
 	ClearPageError(page);
 
 	/* LFS mode write path */
-	write_data_page(&dn, fio);
+	f2fs_outplace_write_data(&dn, fio);
 	trace_f2fs_do_write_data_page(page, OPU);
 	set_inode_flag(inode, FI_APPEND_WRITE);
 	if (page->index == 0)
@@ -1829,13 +1830,13 @@ static int __write_data_page(struct page *page, bool *submitted,
 	/* we should not write 0'th page having journal header */
 	if (f2fs_is_volatile_file(inode) && (!page->index ||
 			(!wbc->for_reclaim &&
-			available_free_memory(sbi, BASE_CHECK))))
+			f2fs_available_free_memory(sbi, BASE_CHECK))))
 		goto redirty_out;
 
 	/* Dentry blocks are controlled by checkpoint */
 	if (S_ISDIR(inode->i_mode)) {
 		fio.need_lock = LOCK_DONE;
-		err = do_write_data_page(&fio);
+		err = f2fs_do_write_data_page(&fio);
 		goto done;
 	}
 
@@ -1854,10 +1855,10 @@ static int __write_data_page(struct page *page, bool *submitted,
 	}
 
 	if (err == -EAGAIN) {
-		err = do_write_data_page(&fio);
+		err = f2fs_do_write_data_page(&fio);
 		if (err == -EAGAIN) {
 			fio.need_lock = LOCK_REQ;
-			err = do_write_data_page(&fio);
+			err = f2fs_do_write_data_page(&fio);
 		}
 	}
 
@@ -1882,7 +1883,7 @@ static int __write_data_page(struct page *page, bool *submitted,
 	if (wbc->for_reclaim) {
 		f2fs_submit_merged_write_cond(sbi, inode, 0, page->index, DATA);
 		clear_inode_flag(inode, FI_HOT_DATA);
-		remove_dirty_inode(inode);
+		f2fs_remove_dirty_inode(inode);
 		submitted = NULL;
 	}
 
@@ -2095,7 +2096,7 @@ static int __f2fs_write_data_pages(struct address_space *mapping,
 
 	if (S_ISDIR(inode->i_mode) && wbc->sync_mode == WB_SYNC_NONE &&
 			get_dirty_pages(inode) < nr_pages_to_skip(sbi, DATA) &&
-			available_free_memory(sbi, DIRTY_DENTS))
+			f2fs_available_free_memory(sbi, DIRTY_DENTS))
 		goto skip_write;
 
 	/* skip writing during file defragment */
@@ -2121,7 +2122,7 @@ static int __f2fs_write_data_pages(struct address_space *mapping,
 	 * to detect pending bios.
 	 */
 
-	remove_dirty_inode(inode);
+	f2fs_remove_dirty_inode(inode);
 	return ret;
 
 skip_write:
@@ -2148,7 +2149,7 @@ static void f2fs_write_failed(struct address_space *mapping, loff_t to)
 	if (to > i_size) {
 		down_write(&F2FS_I(inode)->i_mmap_sem);
 		truncate_pagecache(inode, i_size);
-		truncate_blocks(inode, i_size, true);
+		f2fs_truncate_blocks(inode, i_size, true);
 		up_write(&F2FS_I(inode)->i_mmap_sem);
 	}
 }
@@ -2180,7 +2181,7 @@ static int prepare_write_begin(struct f2fs_sb_info *sbi,
 	}
 restart:
 	/* check inline_data */
-	ipage = get_node_page(sbi, inode->i_ino);
+	ipage = f2fs_get_node_page(sbi, inode->i_ino);
 	if (IS_ERR(ipage)) {
 		err = PTR_ERR(ipage);
 		goto unlock_out;
@@ -2190,7 +2191,7 @@ static int prepare_write_begin(struct f2fs_sb_info *sbi,
 
 	if (f2fs_has_inline_data(inode)) {
 		if (pos + len <= MAX_INLINE_DATA(inode)) {
-			read_inline_data(page, ipage);
+			f2fs_do_read_inline_data(page, ipage);
 			set_inode_flag(inode, FI_DATA_EXIST);
 			if (inode->i_nlink)
 				set_inline_node(ipage);
@@ -2208,7 +2209,7 @@ static int prepare_write_begin(struct f2fs_sb_info *sbi,
 			dn.data_blkaddr = ei.blk + index - ei.fofs;
 		} else {
 			/* hole case */
-			err = get_dnode_of_data(&dn, index, LOOKUP_NODE);
+			err = f2fs_get_dnode_of_data(&dn, index, LOOKUP_NODE);
 			if (err || dn.data_blkaddr == NULL_ADDR) {
 				f2fs_put_dnode(&dn);
 				__do_map_lock(sbi, F2FS_GET_BLOCK_PRE_AIO,
@@ -2245,7 +2246,7 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 	trace_f2fs_write_begin(inode, pos, len, flags);
 
 	if (f2fs_is_atomic_file(inode) &&
-			!available_free_memory(sbi, INMEM_PAGES)) {
+			!f2fs_available_free_memory(sbi, INMEM_PAGES)) {
 		err = -ENOMEM;
 		drop_atomic = true;
 		goto fail;
@@ -2329,7 +2330,7 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 	f2fs_put_page(page, 1);
 	f2fs_write_failed(mapping, pos + len);
 	if (drop_atomic)
-		drop_inmem_pages_all(sbi, false);
+		f2fs_drop_inmem_pages_all(sbi, false);
 	return err;
 }
 
@@ -2451,13 +2452,13 @@ void f2fs_invalidate_page(struct page *page, unsigned int offset,
 			dec_page_count(sbi, F2FS_DIRTY_NODES);
 		} else {
 			inode_dec_dirty_pages(inode);
-			remove_dirty_inode(inode);
+			f2fs_remove_dirty_inode(inode);
 		}
 	}
 
 	/* This is atomic written page, keep Private */
 	if (IS_ATOMIC_WRITTEN_PAGE(page))
-		return drop_inmem_page(inode, page);
+		return f2fs_drop_inmem_page(inode, page);
 
 	set_page_private(page, 0);
 	ClearPagePrivate(page);
@@ -2490,7 +2491,7 @@ static int f2fs_set_data_page_dirty(struct page *page)
 
 	if (f2fs_is_atomic_file(inode) && !f2fs_is_commit_atomic_write(inode)) {
 		if (!IS_ATOMIC_WRITTEN_PAGE(page)) {
-			register_inmem_page(inode, page);
+			f2fs_register_inmem_page(inode, page);
 			return 1;
 		}
 		/*
@@ -2502,7 +2503,7 @@ static int f2fs_set_data_page_dirty(struct page *page)
 
 	if (!PageDirty(page)) {
 		__set_page_dirty_nobuffers(page);
-		update_dirty_page(inode, page);
+		f2fs_update_dirty_page(inode, page);
 		return 1;
 	}
 	return 0;
@@ -2598,7 +2599,7 @@ const struct address_space_operations f2fs_dblock_aops = {
 #endif
 };
 
-void clear_radix_tree_dirty_tag(struct page *page)
+void f2fs_clear_radix_tree_dirty_tag(struct page *page)
 {
 	struct address_space *mapping = page_mapping(page);
 	unsigned long flags;

commit fc99fe27b9a2cfb4ae5d869a116a73a17b075abf
Author: Chao Yu <yuchao0@huawei.com>
Date:   Wed May 30 00:20:39 2018 +0800

    f2fs: make __f2fs_write_data_pages() static
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 97e6df852f37..10aa4d9a4b7c 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -2072,7 +2072,7 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 	return ret;
 }
 
-int __f2fs_write_data_pages(struct address_space *mapping,
+static int __f2fs_write_data_pages(struct address_space *mapping,
 						struct writeback_control *wbc,
 						enum iostat_type io_type)
 {

commit fe16efe6a7952f59d596a02e4fc57f966fafdafe
Author: Chao Yu <yuchao0@huawei.com>
Date:   Mon May 28 23:47:18 2018 +0800

    f2fs: fix to let caller retry allocating block address
    
    Configure io_bits with 2 and enable LFS mode, generic/013 reports below dmesg:
    
    BUG: unable to handle kernel NULL pointer dereference at 00000104
    *pdpt = 0000000029b7b001 *pde = 0000000000000000
    Oops: 0002 [#1] PREEMPT SMP
    Modules linked in: crc32_generic zram f2fs(O) rfcomm bnep bluetooth ecdh_generic snd_intel8x0 snd_ac97_codec ac97_bus snd_pcm snd_seq_midi snd_seq_midi_event snd_rawmidi snd_seq pcbc joydev snd_seq_device aesni_intel snd_timer aes_i586 snd crypto_simd cryptd soundcore i2c_piix4 serio_raw mac_hid video parport_pc ppdev lp parport hid_generic psmouse usbhid hid e1000
    CPU: 0 PID: 11161 Comm: fsstress Tainted: G           O      4.17.0-rc2 #38
    Hardware name: innotek GmbH VirtualBox/VirtualBox, BIOS VirtualBox 12/01/2006
    EIP: f2fs_submit_page_write+0x28d/0x550 [f2fs]
    EFLAGS: 00010206 CPU: 0
    EAX: e863dcd8 EBX: 00000000 ECX: 00000100 EDX: 00000200
    ESI: e863dcf4 EDI: f6f82768 EBP: e863dbb0 ESP: e863db74
     DS: 007b ES: 007b FS: 00d8 GS: 0033 SS: 0068
    CR0: 80050033 CR2: 00000104 CR3: 29a62020 CR4: 000406f0
    Call Trace:
     do_write_page+0x6f/0xc0 [f2fs]
     write_data_page+0x4a/0xd0 [f2fs]
     do_write_data_page+0x327/0x630 [f2fs]
     __write_data_page+0x34b/0x820 [f2fs]
     __f2fs_write_data_pages+0x42d/0x8c0 [f2fs]
     f2fs_write_data_pages+0x27/0x30 [f2fs]
     do_writepages+0x1a/0x70
     __filemap_fdatawrite_range+0x94/0xd0
     filemap_write_and_wait_range+0x3d/0xa0
     __generic_file_write_iter+0x11a/0x1f0
     f2fs_file_write_iter+0xdd/0x3b0 [f2fs]
     __vfs_write+0xd2/0x150
     vfs_write+0x9b/0x190
     ksys_write+0x45/0x90
     sys_write+0x16/0x20
     do_fast_syscall_32+0xaa/0x22c
     entry_SYSENTER_32+0x4c/0x7b
    EIP: 0xb7fc8c51
    EFLAGS: 00000246 CPU: 0
    EAX: ffffffda EBX: 00000003 ECX: 09cde000 EDX: 00001000
    ESI: 00000003 EDI: 00001000 EBP: 00000000 ESP: bfbded38
     DS: 007b ES: 007b FS: 0000 GS: 0033 SS: 007b
    Code: e8 f9 77 34 c9 8b 45 e0 8b 80 b8 00 00 00 39 45 d8 0f 84 bb 02 00 00 8b 45 e0 8b 80 b8 00 00 00 8d 50 d8 8b 08 89 55 f0 8b 50 04 <89> 51 04 89 0a c7 00 00 01 00 00 c7 40 04 00 02 00 00 8b 45 dc
    EIP: f2fs_submit_page_write+0x28d/0x550 [f2fs] SS:ESP: 0068:e863db74
    CR2: 0000000000000104
    ---[ end trace 4cac79c0d1305ee6 ]---
    
    allocate_data_block will submit all sequential pending IOs sorted by a
    FIFO list, If we failed to submit other user's IO due to unaligned write,
    we will retry to allocate new block address for current IO, then it will
    initialize fio.list again, if fio was in the list before, it can break
    FIFO list, result in above panic.
    
    Thread A                        Thread B
    - do_write_page
     - allocate_data_block
      - list_add_tail
      : fioA cached in FIFO list.
                                    - do_write_page
                                     - allocate_data_block
                                      - list_add_tail
                                      : fioB cached in FIFO list.
                                     - f2fs_submit_page_write
                                     : fail to submit IO
                                     - allocate_data_block
                                      - INIT_LIST_HEAD
     - f2fs_submit_page_write
      - list_del  <-- NULL pointer dereference
    
    This patch adds fio.retry parameter to indicate failure status for each
    IO, and avoid bailing out if there is still pending IO in FIFO list for
    fixing.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 31c2edb217ec..97e6df852f37 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -462,13 +462,12 @@ int f2fs_submit_page_bio(struct f2fs_io_info *fio)
 	return 0;
 }
 
-int f2fs_submit_page_write(struct f2fs_io_info *fio)
+void f2fs_submit_page_write(struct f2fs_io_info *fio)
 {
 	struct f2fs_sb_info *sbi = fio->sbi;
 	enum page_type btype = PAGE_TYPE_OF_BIO(fio->type);
 	struct f2fs_bio_info *io = sbi->write_io[btype] + fio->temp;
 	struct page *bio_page;
-	int err = 0;
 
 	f2fs_bug_on(sbi, is_read_io(fio->op));
 
@@ -478,7 +477,7 @@ int f2fs_submit_page_write(struct f2fs_io_info *fio)
 		spin_lock(&io->io_lock);
 		if (list_empty(&io->io_list)) {
 			spin_unlock(&io->io_lock);
-			goto out_fail;
+			goto out;
 		}
 		fio = list_first_entry(&io->io_list,
 						struct f2fs_io_info, list);
@@ -505,9 +504,9 @@ int f2fs_submit_page_write(struct f2fs_io_info *fio)
 	if (io->bio == NULL) {
 		if ((fio->type == DATA || fio->type == NODE) &&
 				fio->new_blkaddr & F2FS_IO_SIZE_MASK(sbi)) {
-			err = -EAGAIN;
 			dec_page_count(sbi, WB_DATA_TYPE(bio_page));
-			goto out_fail;
+			fio->retry = true;
+			goto skip;
 		}
 		io->bio = __bio_alloc(sbi, fio->new_blkaddr, fio->io_wbc,
 						BIO_MAX_PAGES, false,
@@ -527,12 +526,11 @@ int f2fs_submit_page_write(struct f2fs_io_info *fio)
 	f2fs_trace_ios(fio, 0);
 
 	trace_f2fs_submit_page_write(fio->page, fio);
-
+skip:
 	if (fio->in_list)
 		goto next;
-out_fail:
+out:
 	up_write(&io->io_rwsem);
-	return err;
 }
 
 static struct bio *f2fs_grab_read_bio(struct inode *inode, block_t blkaddr,

commit 1174abfd8309f4c47d454734233aa3b694560e10
Author: Chao Yu <yuchao0@huawei.com>
Date:   Mon May 28 16:59:26 2018 +0800

    f2fs: don't drop dentry pages after fs shutdown
    
    As description in commit "f2fs: don't drop any page on f2fs_cp_error()
    case":
    
    "We still provide readdir() after shtudown, so we should keep pages to
    avoid additional IOs."
    
    In order to provider lastest directory structure, let's keep dentry
    pages in cache after fs shutdown.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 9d3e2e1c1e33..31c2edb217ec 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1801,6 +1801,12 @@ static int __write_data_page(struct page *page, bool *submitted,
 	/* we should bypass data pages to proceed the kworkder jobs */
 	if (unlikely(f2fs_cp_error(sbi))) {
 		mapping_set_error(page->mapping, -EIO);
+		/*
+		 * don't drop any dirty dentry pages for keeping lastest
+		 * directory structure.
+		 */
+		if (S_ISDIR(inode->i_mode))
+			goto redirty_out;
 		goto out;
 	}
 

commit aec2f729fca13661e9bc651839ae23bf8367195a
Author: Chao Yu <yuchao0@huawei.com>
Date:   Sat May 26 18:03:35 2018 +0800

    f2fs: clean up with clear_radix_tree_dirty_tag
    
    Introduce clear_radix_tree_dirty_tag to include common codes for cleanup.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 98a12526e3d0..9d3e2e1c1e33 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -2594,6 +2594,17 @@ const struct address_space_operations f2fs_dblock_aops = {
 #endif
 };
 
+void clear_radix_tree_dirty_tag(struct page *page)
+{
+	struct address_space *mapping = page_mapping(page);
+	unsigned long flags;
+
+	xa_lock_irqsave(&mapping->i_pages, flags);
+	radix_tree_tag_clear(&mapping->i_pages, page_index(page),
+						PAGECACHE_TAG_DIRTY);
+	xa_unlock_irqrestore(&mapping->i_pages, flags);
+}
+
 int __init f2fs_init_post_read_processing(void)
 {
 	bio_post_read_ctx_cache = KMEM_CACHE(bio_post_read_ctx, 0);

commit 2ef79ecb5e906d87475d3e0c49b22425499a89f3
Author: Chao Yu <yuchao0@huawei.com>
Date:   Mon May 7 20:28:54 2018 +0800

    f2fs: avoid stucking GC due to atomic write
    
    f2fs doesn't allow abuse on atomic write class interface, so except
    limiting in-mem pages' total memory usage capacity, we need to limit
    atomic-write usage as well when filesystem is seriously fragmented,
    otherwise we may run into infinite loop during foreground GC because
    target blocks in victim segment are belong to atomic opened file for
    long time.
    
    Now, we will detect failure due to atomic write in foreground GC, if
    the count exceeds threshold, we will drop all atomic written data in
    cache, by this, I expect it can keep our system running safely to
    prevent Dos attack.
    
    In addition, his patch adds to show GC skip information in debugfs,
    now it just shows count of skipped caused by atomic write.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index abb45f5653f9..98a12526e3d0 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -2325,7 +2325,7 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 	f2fs_put_page(page, 1);
 	f2fs_write_failed(mapping, pos + len);
 	if (drop_atomic)
-		drop_inmem_pages_all(sbi);
+		drop_inmem_pages_all(sbi, false);
 	return err;
 }
 

commit f8de4331224513271a99dce967e637cf14f4bc28
Author: Chao Yu <yuchao0@huawei.com>
Date:   Wed May 23 22:25:09 2018 +0800

    f2fs: detect synchronous writeback more earlier
    
    This patch changes to detect synchronous writeback more earlier before,
    in order to avoid unnecessary page writeback before exiting asynchronous
    writeback.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 19c3c2180cca..abb45f5653f9 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1981,6 +1981,13 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 			struct page *page = pvec.pages[i];
 			bool submitted = false;
 
+			/* give a priority to WB_SYNC threads */
+			if (atomic_read(&F2FS_M_SB(mapping)->wb_sync_req) &&
+					wbc->sync_mode == WB_SYNC_NONE) {
+				done = 1;
+				break;
+			}
+
 			done_index = page->index;
 retry_write:
 			lock_page(page);
@@ -2035,9 +2042,7 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 				last_idx = page->index;
 			}
 
-			/* give a priority to WB_SYNC threads */
-			if ((atomic_read(&F2FS_M_SB(mapping)->wb_sync_req) ||
-					--wbc->nr_to_write <= 0) &&
+			if (--wbc->nr_to_write <= 0 &&
 					wbc->sync_mode == WB_SYNC_NONE) {
 				done = 1;
 				break;

commit 7b525dd01365c6764018e374d391c92466be1b7a
Author: Chao Yu <yuchao0@huawei.com>
Date:   Wed May 23 22:25:08 2018 +0800

    f2fs: clean up with is_valid_blkaddr()
    
    - rename is_valid_blkaddr() to is_valid_meta_blkaddr() for readability.
    - introduce is_valid_blkaddr() for cleanup.
    
    No logic change in this patch.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 01135cc3a292..19c3c2180cca 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -486,7 +486,7 @@ int f2fs_submit_page_write(struct f2fs_io_info *fio)
 		spin_unlock(&io->io_lock);
 	}
 
-	if (fio->old_blkaddr != NEW_ADDR)
+	if (is_valid_blkaddr(fio->old_blkaddr))
 		verify_block_addr(fio, fio->old_blkaddr);
 	verify_block_addr(fio, fio->new_blkaddr);
 
@@ -1046,7 +1046,7 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 next_block:
 	blkaddr = datablock_addr(dn.inode, dn.node_page, dn.ofs_in_node);
 
-	if (blkaddr == NEW_ADDR || blkaddr == NULL_ADDR) {
+	if (!is_valid_blkaddr(blkaddr)) {
 		if (create) {
 			if (unlikely(f2fs_cp_error(sbi))) {
 				err = -EIO;
@@ -1681,15 +1681,6 @@ static inline bool need_inplace_update(struct f2fs_io_info *fio)
 	return should_update_inplace(inode, fio);
 }
 
-static inline bool valid_ipu_blkaddr(struct f2fs_io_info *fio)
-{
-	if (fio->old_blkaddr == NEW_ADDR)
-		return false;
-	if (fio->old_blkaddr == NULL_ADDR)
-		return false;
-	return true;
-}
-
 int do_write_data_page(struct f2fs_io_info *fio)
 {
 	struct page *page = fio->page;
@@ -1704,7 +1695,7 @@ int do_write_data_page(struct f2fs_io_info *fio)
 			f2fs_lookup_extent_cache(inode, page->index, &ei)) {
 		fio->old_blkaddr = ei.blk + page->index - ei.fofs;
 
-		if (valid_ipu_blkaddr(fio)) {
+		if (is_valid_blkaddr(fio->old_blkaddr)) {
 			ipu_force = true;
 			fio->need_lock = LOCK_DONE;
 			goto got_it;
@@ -1731,7 +1722,8 @@ int do_write_data_page(struct f2fs_io_info *fio)
 	 * If current allocation needs SSR,
 	 * it had better in-place writes for updated data.
 	 */
-	if (ipu_force || (valid_ipu_blkaddr(fio) && need_inplace_update(fio))) {
+	if (ipu_force || (is_valid_blkaddr(fio->old_blkaddr) &&
+					need_inplace_update(fio))) {
 		err = encrypt_one_page(fio);
 		if (err)
 			goto out_writepage;

commit e7a4feb0ab320eac466e39b283f52c1ec595bc03
Author: Chao Yu <yuchao0@huawei.com>
Date:   Tue May 8 14:06:03 2018 +0800

    f2fs: fix to let checkpoint guarantee atomic page persistence
    
    1. thread A: commit_inmem_pages submit data into block layer, but
    haven't waited it writeback.
    2. thread A: commit_inmem_pages update related node.
    3. thread B: do checkpoint, flush all nodes to disk.
    4. SPOR
    
    Then, atomic file becomes corrupted since nodes is flushed before data.
    
    This patch fixes to treat atomic page as checkpoint guaranteed one,
    then in checkpoint, we can make sure all atomic page can be writebacked
    with metadata of atomic file.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 72287fa2489c..01135cc3a292 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -48,6 +48,8 @@ static bool __is_cp_guaranteed(struct page *page)
 	if (inode->i_ino == F2FS_META_INO(sbi) ||
 			inode->i_ino ==  F2FS_NODE_INO(sbi) ||
 			S_ISDIR(inode->i_mode) ||
+			(S_ISREG(inode->i_mode) &&
+			is_inode_flag_set(inode, FI_ATOMIC_FILE)) ||
 			is_cold_data(page))
 		return true;
 	return false;

commit b2532c694033fb6762478846c457382061f9f630
Author: Chao Yu <yuchao0@huawei.com>
Date:   Tue Apr 24 10:55:28 2018 +0800

    f2fs: rename dio_rwsem to i_gc_rwsem
    
    RW semphore dio_rwsem in struct f2fs_inode_info is introduced to avoid
    race between dio and data gc, but now, it is more wildly used to avoid
    foreground operation vs data gc. So rename it to i_gc_rwsem to improve
    its readability.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index ac8720fa5868..72287fa2489c 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -2401,17 +2401,17 @@ static ssize_t f2fs_direct_IO(struct kiocb *iocb, struct iov_iter *iter)
 	if (rw == WRITE && whint_mode == WHINT_MODE_OFF)
 		iocb->ki_hint = WRITE_LIFE_NOT_SET;
 
-	if (!down_read_trylock(&F2FS_I(inode)->dio_rwsem[rw])) {
+	if (!down_read_trylock(&F2FS_I(inode)->i_gc_rwsem[rw])) {
 		if (iocb->ki_flags & IOCB_NOWAIT) {
 			iocb->ki_hint = hint;
 			err = -EAGAIN;
 			goto out;
 		}
-		down_read(&F2FS_I(inode)->dio_rwsem[rw]);
+		down_read(&F2FS_I(inode)->i_gc_rwsem[rw]);
 	}
 
 	err = blockdev_direct_IO(iocb, inode, iter, get_data_block_dio);
-	up_read(&F2FS_I(inode)->dio_rwsem[rw]);
+	up_read(&F2FS_I(inode)->i_gc_rwsem[rw]);
 
 	if (rw == WRITE) {
 		if (whint_mode == WHINT_MODE_OFF)

commit 5b19d284f5195a925dd015a6397bfce184097378
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu May 3 23:26:02 2018 -0700

    f2fs: avoid fsync() failure caused by EAGAIN in writepage()
    
    pageout() in MM traslates EAGAIN, so calls handle_write_error()
     -> mapping_set_error() -> set_bit(AS_EIO, ...).
     file_write_and_wait_range() will see EIO error, which is critical
     to return value of fsync() followed by atomic_write failure to user.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 5477fc09c3cd..ac8720fa5868 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1904,7 +1904,13 @@ static int __write_data_page(struct page *page, bool *submitted,
 
 redirty_out:
 	redirty_page_for_writepage(wbc, page);
-	if (!err)
+	/*
+	 * pageout() in MM traslates EAGAIN, so calls handle_write_error()
+	 * -> mapping_set_error() -> set_bit(AS_EIO, ...).
+	 * file_write_and_wait_range() will see EIO error, which is critical
+	 * to return value of fsync() followed by atomic_write failure to user.
+	 */
+	if (!err || wbc->for_reclaim)
 		return AOP_WRITEPAGE_ACTIVATE;
 	unlock_page(page);
 	return err;

commit 17c500350b3e1a1430cbcc7efb54eb859446fc8a
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed Apr 11 23:09:04 2018 -0700

    f2fs: clear PageError on writepage
    
    This patch clears PageError in some pages tagged by read path, but when we
    write the pages with valid contents, writepage should clear the bit likewise
    ext4.
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 0f652b05decc..5477fc09c3cd 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1735,6 +1735,7 @@ int do_write_data_page(struct f2fs_io_info *fio)
 			goto out_writepage;
 
 		set_page_writeback(page);
+		ClearPageError(page);
 		f2fs_put_dnode(&dn);
 		if (fio->need_lock == LOCK_REQ)
 			f2fs_unlock_op(fio->sbi);
@@ -1757,6 +1758,7 @@ int do_write_data_page(struct f2fs_io_info *fio)
 		goto out_writepage;
 
 	set_page_writeback(page);
+	ClearPageError(page);
 
 	/* LFS mode write path */
 	write_data_page(&dn, fio);

commit b87078ad3aec0d116e8bdc91ecbe18b5c0e49896
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Apr 20 19:29:52 2018 -0700

    Revert "f2fs: introduce f2fs_set_page_dirty_nobuffer"
    
    This patch reverts copied f2fs_set_page_dirty_nobuffer to use generic function
    for stability.
    
    This reverts commit fe76b796fc5194cc3d57265002e3a748566d073f.
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index ad503603ceec..0f652b05decc 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -19,8 +19,6 @@
 #include <linux/bio.h>
 #include <linux/prefetch.h>
 #include <linux/uio.h>
-#include <linux/mm.h>
-#include <linux/memcontrol.h>
 #include <linux/cleancache.h>
 #include <linux/sched/signal.h>
 
@@ -2469,35 +2467,6 @@ int f2fs_release_page(struct page *page, gfp_t wait)
 	return 1;
 }
 
-/*
- * This was copied from __set_page_dirty_buffers which gives higher performance
- * in very high speed storages. (e.g., pmem)
- */
-void f2fs_set_page_dirty_nobuffers(struct page *page)
-{
-	struct address_space *mapping = page->mapping;
-	unsigned long flags;
-
-	if (unlikely(!mapping))
-		return;
-
-	spin_lock(&mapping->private_lock);
-	lock_page_memcg(page);
-	SetPageDirty(page);
-	spin_unlock(&mapping->private_lock);
-
-	xa_lock_irqsave(&mapping->i_pages, flags);
-	WARN_ON_ONCE(!PageUptodate(page));
-	account_page_dirtied(page, mapping);
-	radix_tree_tag_set(&mapping->i_pages,
-			page_index(page), PAGECACHE_TAG_DIRTY);
-	xa_unlock_irqrestore(&mapping->i_pages, flags);
-	unlock_page_memcg(page);
-
-	__mark_inode_dirty(mapping->host, I_DIRTY_PAGES);
-	return;
-}
-
 static int f2fs_set_data_page_dirty(struct page *page)
 {
 	struct address_space *mapping = page->mapping;
@@ -2521,7 +2490,7 @@ static int f2fs_set_data_page_dirty(struct page *page)
 	}
 
 	if (!PageDirty(page)) {
-		f2fs_set_page_dirty_nobuffers(page);
+		__set_page_dirty_nobuffers(page);
 		update_dirty_page(inode, page);
 		return 1;
 	}

commit 6dbb17961f46b2eafcea2f2627aabb309553e068
Author: Eric Biggers <ebiggers@google.com>
Date:   Wed Apr 18 11:09:48 2018 -0700

    f2fs: refactor read path to allow multiple postprocessing steps
    
    Currently f2fs's ->readpage() and ->readpages() assume that either the
    data undergoes no postprocessing, or decryption only.  But with
    fs-verity, there will be an additional authenticity verification step,
    and it may be needed either by itself, or combined with decryption.
    
    To support this, store a 'struct bio_post_read_ctx' in ->bi_private
    which contains a work struct, a bitmask of postprocessing steps that are
    enabled, and an indicator of the current step.  The bio completion
    routine, if there was no I/O error, enqueues the first postprocessing
    step.  When that completes, it continues to the next step.  Pages that
    fail any postprocessing step have PageError set.  Once all steps have
    completed, pages without PageError set are set Uptodate, and all pages
    are unlocked.
    
    Also replace f2fs_encrypted_file() with a new function
    f2fs_post_read_required() in places like direct I/O and garbage
    collection that really should be testing whether the file needs special
    I/O processing, not whether it is encrypted specifically.
    
    This may also be useful for other future f2fs features such as
    compression.
    
    Signed-off-by: Eric Biggers <ebiggers@google.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 39225519de1c..ad503603ceec 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -30,6 +30,11 @@
 #include "trace.h"
 #include <trace/events/f2fs.h>
 
+#define NUM_PREALLOC_POST_READ_CTXS	128
+
+static struct kmem_cache *bio_post_read_ctx_cache;
+static mempool_t *bio_post_read_ctx_pool;
+
 static bool __is_cp_guaranteed(struct page *page)
 {
 	struct address_space *mapping = page->mapping;
@@ -50,11 +55,77 @@ static bool __is_cp_guaranteed(struct page *page)
 	return false;
 }
 
-static void f2fs_read_end_io(struct bio *bio)
+/* postprocessing steps for read bios */
+enum bio_post_read_step {
+	STEP_INITIAL = 0,
+	STEP_DECRYPT,
+};
+
+struct bio_post_read_ctx {
+	struct bio *bio;
+	struct work_struct work;
+	unsigned int cur_step;
+	unsigned int enabled_steps;
+};
+
+static void __read_end_io(struct bio *bio)
 {
-	struct bio_vec *bvec;
+	struct page *page;
+	struct bio_vec *bv;
 	int i;
 
+	bio_for_each_segment_all(bv, bio, i) {
+		page = bv->bv_page;
+
+		/* PG_error was set if any post_read step failed */
+		if (bio->bi_status || PageError(page)) {
+			ClearPageUptodate(page);
+			SetPageError(page);
+		} else {
+			SetPageUptodate(page);
+		}
+		unlock_page(page);
+	}
+	if (bio->bi_private)
+		mempool_free(bio->bi_private, bio_post_read_ctx_pool);
+	bio_put(bio);
+}
+
+static void bio_post_read_processing(struct bio_post_read_ctx *ctx);
+
+static void decrypt_work(struct work_struct *work)
+{
+	struct bio_post_read_ctx *ctx =
+		container_of(work, struct bio_post_read_ctx, work);
+
+	fscrypt_decrypt_bio(ctx->bio);
+
+	bio_post_read_processing(ctx);
+}
+
+static void bio_post_read_processing(struct bio_post_read_ctx *ctx)
+{
+	switch (++ctx->cur_step) {
+	case STEP_DECRYPT:
+		if (ctx->enabled_steps & (1 << STEP_DECRYPT)) {
+			INIT_WORK(&ctx->work, decrypt_work);
+			fscrypt_enqueue_decrypt_work(&ctx->work);
+			return;
+		}
+		ctx->cur_step++;
+		/* fall-through */
+	default:
+		__read_end_io(ctx->bio);
+	}
+}
+
+static bool f2fs_bio_post_read_required(struct bio *bio)
+{
+	return bio->bi_private && !bio->bi_status;
+}
+
+static void f2fs_read_end_io(struct bio *bio)
+{
 #ifdef CONFIG_F2FS_FAULT_INJECTION
 	if (time_to_inject(F2FS_P_SB(bio_first_page_all(bio)), FAULT_IO)) {
 		f2fs_show_injection_info(FAULT_IO);
@@ -62,28 +133,15 @@ static void f2fs_read_end_io(struct bio *bio)
 	}
 #endif
 
-	if (f2fs_bio_encrypted(bio)) {
-		if (bio->bi_status) {
-			fscrypt_release_ctx(bio->bi_private);
-		} else {
-			fscrypt_enqueue_decrypt_bio(bio->bi_private, bio);
-			return;
-		}
-	}
-
-	bio_for_each_segment_all(bvec, bio, i) {
-		struct page *page = bvec->bv_page;
+	if (f2fs_bio_post_read_required(bio)) {
+		struct bio_post_read_ctx *ctx = bio->bi_private;
 
-		if (!bio->bi_status) {
-			if (!PageUptodate(page))
-				SetPageUptodate(page);
-		} else {
-			ClearPageUptodate(page);
-			SetPageError(page);
-		}
-		unlock_page(page);
+		ctx->cur_step = STEP_INITIAL;
+		bio_post_read_processing(ctx);
+		return;
 	}
-	bio_put(bio);
+
+	__read_end_io(bio);
 }
 
 static void f2fs_write_end_io(struct bio *bio)
@@ -481,29 +539,33 @@ static struct bio *f2fs_grab_read_bio(struct inode *inode, block_t blkaddr,
 							 unsigned nr_pages)
 {
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	struct fscrypt_ctx *ctx = NULL;
 	struct bio *bio;
-
-	if (f2fs_encrypted_file(inode)) {
-		ctx = fscrypt_get_ctx(inode, GFP_NOFS);
-		if (IS_ERR(ctx))
-			return ERR_CAST(ctx);
-
-		/* wait the page to be moved by cleaning */
-		f2fs_wait_on_block_writeback(sbi, blkaddr);
-	}
+	struct bio_post_read_ctx *ctx;
+	unsigned int post_read_steps = 0;
 
 	bio = f2fs_bio_alloc(sbi, min_t(int, nr_pages, BIO_MAX_PAGES), false);
-	if (!bio) {
-		if (ctx)
-			fscrypt_release_ctx(ctx);
+	if (!bio)
 		return ERR_PTR(-ENOMEM);
-	}
 	f2fs_target_device(sbi, blkaddr, bio);
 	bio->bi_end_io = f2fs_read_end_io;
-	bio->bi_private = ctx;
 	bio_set_op_attrs(bio, REQ_OP_READ, 0);
 
+	if (f2fs_encrypted_file(inode))
+		post_read_steps |= 1 << STEP_DECRYPT;
+	if (post_read_steps) {
+		ctx = mempool_alloc(bio_post_read_ctx_pool, GFP_NOFS);
+		if (!ctx) {
+			bio_put(bio);
+			return ERR_PTR(-ENOMEM);
+		}
+		ctx->bio = bio;
+		ctx->enabled_steps = post_read_steps;
+		bio->bi_private = ctx;
+
+		/* wait the page to be moved by cleaning */
+		f2fs_wait_on_block_writeback(sbi, blkaddr);
+	}
+
 	return bio;
 }
 
@@ -1525,7 +1587,7 @@ static int encrypt_one_page(struct f2fs_io_info *fio)
 	if (!f2fs_encrypted_file(inode))
 		return 0;
 
-	/* wait for GCed encrypted page writeback */
+	/* wait for GCed page writeback via META_MAPPING */
 	f2fs_wait_on_block_writeback(fio->sbi, fio->old_blkaddr);
 
 retry_encrypt:
@@ -2222,8 +2284,8 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 
 	f2fs_wait_on_page_writeback(page, DATA, false);
 
-	/* wait for GCed encrypted page writeback */
-	if (f2fs_encrypted_file(inode))
+	/* wait for GCed page writeback via META_MAPPING */
+	if (f2fs_post_read_required(inode))
 		f2fs_wait_on_block_writeback(sbi, blkaddr);
 
 	if (len == PAGE_SIZE || PageUptodate(page))
@@ -2555,3 +2617,27 @@ const struct address_space_operations f2fs_dblock_aops = {
 	.migratepage    = f2fs_migrate_page,
 #endif
 };
+
+int __init f2fs_init_post_read_processing(void)
+{
+	bio_post_read_ctx_cache = KMEM_CACHE(bio_post_read_ctx, 0);
+	if (!bio_post_read_ctx_cache)
+		goto fail;
+	bio_post_read_ctx_pool =
+		mempool_create_slab_pool(NUM_PREALLOC_POST_READ_CTXS,
+					 bio_post_read_ctx_cache);
+	if (!bio_post_read_ctx_pool)
+		goto fail_free_cache;
+	return 0;
+
+fail_free_cache:
+	kmem_cache_destroy(bio_post_read_ctx_cache);
+fail:
+	return -ENOMEM;
+}
+
+void __exit f2fs_destroy_post_read_processing(void)
+{
+	mempool_destroy(bio_post_read_ctx_pool);
+	kmem_cache_destroy(bio_post_read_ctx_cache);
+}

commit 0cb8dae4a0df2a977847c2dc6766a7783ce50f9d
Author: Eric Biggers <ebiggers@google.com>
Date:   Wed Apr 18 11:09:47 2018 -0700

    fscrypt: allow synchronous bio decryption
    
    Currently, fscrypt provides fscrypt_decrypt_bio_pages() which decrypts a
    bio's pages asynchronously, then unlocks them afterwards.  But, this
    assumes that decryption is the last "postprocessing step" for the bio,
    so it's incompatible with additional postprocessing steps such as
    authenticity verification after decryption.
    
    Therefore, rename the existing fscrypt_decrypt_bio_pages() to
    fscrypt_enqueue_decrypt_bio().  Then, add fscrypt_decrypt_bio() which
    decrypts the pages in the bio synchronously without unlocking the pages,
    nor setting them Uptodate; and add fscrypt_enqueue_decrypt_work(), which
    enqueues work on the fscrypt_read_workqueue.  The new functions will be
    used by filesystems that support both fscrypt and fs-verity.
    
    Signed-off-by: Eric Biggers <ebiggers@google.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 02237d4d91f5..39225519de1c 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -66,7 +66,7 @@ static void f2fs_read_end_io(struct bio *bio)
 		if (bio->bi_status) {
 			fscrypt_release_ctx(bio->bi_private);
 		} else {
-			fscrypt_decrypt_bio_pages(bio->bi_private, bio);
+			fscrypt_enqueue_decrypt_bio(bio->bi_private, bio);
 			return;
 		}
 	}

commit b93b016313b3ba8003c3b8bb71f569af91f19fc7
Author: Matthew Wilcox <mawilcox@microsoft.com>
Date:   Tue Apr 10 16:36:56 2018 -0700

    page cache: use xa_lock
    
    Remove the address_space ->tree_lock and use the xa_lock newly added to
    the radix_tree_root.  Rename the address_space ->page_tree to ->i_pages,
    since we don't really care that it's a tree.
    
    [willy@infradead.org: fix nds32, fs/dax.c]
      Link: http://lkml.kernel.org/r/20180406145415.GB20605@bombadil.infradead.orgLink: http://lkml.kernel.org/r/20180313132639.17387-9-willy@infradead.org
    Signed-off-by: Matthew Wilcox <mawilcox@microsoft.com>
    Acked-by: Jeff Layton <jlayton@redhat.com>
    Cc: Darrick J. Wong <darrick.wong@oracle.com>
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: Ryusuke Konishi <konishi.ryusuke@lab.ntt.co.jp>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index db50686f5096..02237d4d91f5 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -2424,12 +2424,12 @@ void f2fs_set_page_dirty_nobuffers(struct page *page)
 	SetPageDirty(page);
 	spin_unlock(&mapping->private_lock);
 
-	spin_lock_irqsave(&mapping->tree_lock, flags);
+	xa_lock_irqsave(&mapping->i_pages, flags);
 	WARN_ON_ONCE(!PageUptodate(page));
 	account_page_dirtied(page, mapping);
-	radix_tree_tag_set(&mapping->page_tree,
+	radix_tree_tag_set(&mapping->i_pages,
 			page_index(page), PAGECACHE_TAG_DIRTY);
-	spin_unlock_irqrestore(&mapping->tree_lock, flags);
+	xa_unlock_irqrestore(&mapping->i_pages, flags);
 	unlock_page_memcg(page);
 
 	__mark_inode_dirty(mapping->host, I_DIRTY_PAGES);

commit 0833721ec3658a4e9d5e58b6fa82cf9edc431e59
Author: Yunlei He <heyunlei@huawei.com>
Date:   Thu Mar 8 16:29:13 2018 +0800

    f2fs: check blkaddr more accuratly before issue a bio
    
    This patch check blkaddr more accuratly before issue a
    write or read bio.
    
    Signed-off-by: Yunlei He <heyunlei@huawei.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index f6cf99a311c3..db50686f5096 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -383,6 +383,7 @@ int f2fs_submit_page_bio(struct f2fs_io_info *fio)
 	struct page *page = fio->encrypted_page ?
 			fio->encrypted_page : fio->page;
 
+	verify_block_addr(fio, fio->new_blkaddr);
 	trace_f2fs_submit_page_bio(page, fio);
 	f2fs_trace_ios(fio, 0);
 
@@ -428,8 +429,8 @@ int f2fs_submit_page_write(struct f2fs_io_info *fio)
 	}
 
 	if (fio->old_blkaddr != NEW_ADDR)
-		verify_block_addr(sbi, fio->old_blkaddr);
-	verify_block_addr(sbi, fio->new_blkaddr);
+		verify_block_addr(fio, fio->old_blkaddr);
+	verify_block_addr(fio, fio->new_blkaddr);
 
 	bio_page = fio->encrypted_page ? fio->encrypted_page : fio->page;
 

commit b91050a80cec3daf5a21f78274330df64a4936a3
Author: Hyunchul Lee <cheol.lee@lge.com>
Date:   Thu Mar 8 19:34:38 2018 +0900

    f2fs: add nowait aio support
    
    This patch adds nowait aio support[1].
    
    Return EAGAIN if any of the following checks fail for direct I/O:
      - i_rwsem is not lockable
      - Blocks are not allocated at the write location
    
    And xfstests generic/471 is passed.
    
     [1]: 6be96d "Introduce RWF_NOWAIT and FMODE_AIO_NOWAIT"
    
    Signed-off-by: Hyunchul Lee <cheol.lee@lge.com>
    Reviewed-by: Goldwyn Rodrigues <rgoldwyn@suse.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 3d6ae3173f98..f6cf99a311c3 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -839,13 +839,6 @@ static int __allocate_data_block(struct dnode_of_data *dn, int seg_type)
 	return 0;
 }
 
-static inline bool __force_buffered_io(struct inode *inode, int rw)
-{
-	return (f2fs_encrypted_file(inode) ||
-			(rw == WRITE && test_opt(F2FS_I_SB(inode), LFS)) ||
-			F2FS_I_SB(inode)->s_ndevs);
-}
-
 int f2fs_preallocate_blocks(struct kiocb *iocb, struct iov_iter *from)
 {
 	struct inode *inode = file_inode(iocb->ki_filp);
@@ -877,7 +870,7 @@ int f2fs_preallocate_blocks(struct kiocb *iocb, struct iov_iter *from)
 
 	if (direct_io) {
 		map.m_seg_type = rw_hint_to_seg_type(iocb->ki_hint);
-		flag = __force_buffered_io(inode, WRITE) ?
+		flag = f2fs_force_buffered_io(inode, WRITE) ?
 					F2FS_GET_BLOCK_PRE_AIO :
 					F2FS_GET_BLOCK_PRE_DIO;
 		goto map_blocks;
@@ -1121,6 +1114,31 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 	return err;
 }
 
+bool f2fs_overwrite_io(struct inode *inode, loff_t pos, size_t len)
+{
+	struct f2fs_map_blocks map;
+	block_t last_lblk;
+	int err;
+
+	if (pos + len > i_size_read(inode))
+		return false;
+
+	map.m_lblk = F2FS_BYTES_TO_BLK(pos);
+	map.m_next_pgofs = NULL;
+	map.m_next_extent = NULL;
+	map.m_seg_type = NO_CHECK_TYPE;
+	last_lblk = F2FS_BLK_ALIGN(pos + len);
+
+	while (map.m_lblk < last_lblk) {
+		map.m_len = last_lblk - map.m_lblk;
+		err = f2fs_map_blocks(inode, &map, 0, F2FS_GET_BLOCK_DEFAULT);
+		if (err || map.m_len == 0)
+			return false;
+		map.m_lblk += map.m_len;
+	}
+	return true;
+}
+
 static int __get_data_block(struct inode *inode, sector_t iblock,
 			struct buffer_head *bh, int create, int flag,
 			pgoff_t *next_pgofs, int seg_type)
@@ -2306,7 +2324,7 @@ static ssize_t f2fs_direct_IO(struct kiocb *iocb, struct iov_iter *iter)
 	if (err)
 		return err;
 
-	if (__force_buffered_io(inode, rw))
+	if (f2fs_force_buffered_io(inode, rw))
 		return 0;
 
 	trace_f2fs_direct_IO_enter(inode, offset, count, rw);
@@ -2314,7 +2332,15 @@ static ssize_t f2fs_direct_IO(struct kiocb *iocb, struct iov_iter *iter)
 	if (rw == WRITE && whint_mode == WHINT_MODE_OFF)
 		iocb->ki_hint = WRITE_LIFE_NOT_SET;
 
-	down_read(&F2FS_I(inode)->dio_rwsem[rw]);
+	if (!down_read_trylock(&F2FS_I(inode)->dio_rwsem[rw])) {
+		if (iocb->ki_flags & IOCB_NOWAIT) {
+			iocb->ki_hint = hint;
+			err = -EAGAIN;
+			goto out;
+		}
+		down_read(&F2FS_I(inode)->dio_rwsem[rw]);
+	}
+
 	err = blockdev_direct_IO(iocb, inode, iter, get_data_block_dio);
 	up_read(&F2FS_I(inode)->dio_rwsem[rw]);
 
@@ -2330,6 +2356,7 @@ static ssize_t f2fs_direct_IO(struct kiocb *iocb, struct iov_iter *iter)
 		}
 	}
 
+out:
 	trace_f2fs_direct_IO_exit(inode, offset, count, rw, err);
 
 	return err;

commit 63189b785960c3346d1af347516b7438f7ada8ec
Author: Chao Yu <yuchao0@huawei.com>
Date:   Thu Mar 8 14:22:56 2018 +0800

    f2fs: wrap all options with f2fs_sb_info.mount_opt
    
    This patch merges miscellaneous mount options into struct f2fs_mount_info,
    After this patch, once we add new mount option, we don't need to worry
    about recovery of it in remount_fs(), since we will recover the
    f2fs_sb_info.mount_opt including all options.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 6c3c9784de06..3d6ae3173f98 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -2300,7 +2300,7 @@ static ssize_t f2fs_direct_IO(struct kiocb *iocb, struct iov_iter *iter)
 	int rw = iov_iter_rw(iter);
 	int err;
 	enum rw_hint hint = iocb->ki_hint;
-	int whint_mode = sbi->whint_mode;
+	int whint_mode = F2FS_OPTION(sbi).whint_mode;
 
 	err = check_direct_IO(inode, iter, offset);
 	if (err)

commit ccd31cb28ff2113a2de96ebf19a02b3c920b9fd1
Author: Sheng Yong <shengyong1@huawei.com>
Date:   Tue Feb 6 12:31:17 2018 +0800

    f2fs: clean up f2fs_sb_has_xxx functions
    
    This patch introduces F2FS_FEATURE_FUNCS to clean up the definitions of
    different f2fs_sb_has_xxx functions.
    
    Signed-off-by: Sheng Yong <shengyong1@huawei.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 1c5b050ef900..6c3c9784de06 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -206,7 +206,7 @@ static inline void __submit_bio(struct f2fs_sb_info *sbi,
 		if (type != DATA && type != NODE)
 			goto submit_io;
 
-		if (f2fs_sb_mounted_blkzoned(sbi->sb) && current->plug)
+		if (f2fs_sb_has_blkzoned(sbi->sb) && current->plug)
 			blk_finish_plug(current->plug);
 
 		start = bio->bi_iter.bi_size >> F2FS_BLKSIZE_BITS;

commit 3bb09a0e7e807e2cb7a97e35d5075d0d82bd4085
Author: Tiezhu Yang <kernelpatch@126.com>
Date:   Tue Feb 6 08:21:45 2018 +0800

    f2fs: remove redundant check of page type when submit bio
    
    This patch removes redundant check of page type when submit bio to
    make the logic more clear.
    
    Signed-off-by: Tiezhu Yang <kernelpatch@126.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 0dae8561addb..1c5b050ef900 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -203,13 +203,12 @@ static inline void __submit_bio(struct f2fs_sb_info *sbi,
 	if (!is_read_io(bio_op(bio))) {
 		unsigned int start;
 
-		if (f2fs_sb_mounted_blkzoned(sbi->sb) &&
-			current->plug && (type == DATA || type == NODE))
-			blk_finish_plug(current->plug);
-
 		if (type != DATA && type != NODE)
 			goto submit_io;
 
+		if (f2fs_sb_mounted_blkzoned(sbi->sb) && current->plug)
+			blk_finish_plug(current->plug);
+
 		start = bio->bi_iter.bi_size >> F2FS_BLKSIZE_BITS;
 		start %= F2FS_IO_SIZE(sbi);
 

commit 0cdd31953967b25c216ddcb630ec9a6bb8a91371
Author: Hyunchul Lee <cheol.lee@lge.com>
Date:   Wed Jan 31 11:36:57 2018 +0900

    f2fs: support passing down write hints given by users to block layer
    
    Add the 'whint_mode' mount option that controls which write
    hints are passed down to block layer. There are "off" and
    "user-based" mode. The default mode is "off".
    
    1) whint_mode=off. F2FS only passes down WRITE_LIFE_NOT_SET.
    
    2) whint_mode=user-based. F2FS tries to pass down hints given
    by users.
    
    User                  F2FS                     Block
    ----                  ----                     -----
                          META                     WRITE_LIFE_NOT_SET
                          HOT_NODE                 "
                          WARM_NODE                "
                          COLD_NODE                "
    ioctl(COLD)           COLD_DATA                WRITE_LIFE_EXTREME
    extension list        "                        "
    
    -- buffered io
    WRITE_LIFE_EXTREME    COLD_DATA                WRITE_LIFE_EXTREME
    WRITE_LIFE_SHORT      HOT_DATA                 WRITE_LIFE_SHORT
    WRITE_LIFE_NOT_SET    WARM_DATA                WRITE_LIFE_NOT_SET
    WRITE_LIFE_NONE       "                        "
    WRITE_LIFE_MEDIUM     "                        "
    WRITE_LIFE_LONG       "                        "
    
    -- direct io
    WRITE_LIFE_EXTREME    COLD_DATA                WRITE_LIFE_EXTREME
    WRITE_LIFE_SHORT      HOT_DATA                 WRITE_LIFE_SHORT
    WRITE_LIFE_NOT_SET    WARM_DATA                WRITE_LIFE_NOT_SET
    WRITE_LIFE_NONE       "                        WRITE_LIFE_NONE
    WRITE_LIFE_MEDIUM     "                        WRITE_LIFE_MEDIUM
    WRITE_LIFE_LONG       "                        WRITE_LIFE_LONG
    
    Many thanks to Chao Yu and Jaegeuk Kim for comments to
    implement this patch.
    
    Signed-off-by: Hyunchul Lee <cheol.lee@lge.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    [Jaegeuk Kim: avoid build warning]
    [Chao Yu: fix to restore whint_mode in ->remount_fs]
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 7578ed1a85e0..0dae8561addb 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -175,15 +175,22 @@ static bool __same_bdev(struct f2fs_sb_info *sbi,
  */
 static struct bio *__bio_alloc(struct f2fs_sb_info *sbi, block_t blk_addr,
 				struct writeback_control *wbc,
-				int npages, bool is_read)
+				int npages, bool is_read,
+				enum page_type type, enum temp_type temp)
 {
 	struct bio *bio;
 
 	bio = f2fs_bio_alloc(sbi, npages, true);
 
 	f2fs_target_device(sbi, blk_addr, bio);
-	bio->bi_end_io = is_read ? f2fs_read_end_io : f2fs_write_end_io;
-	bio->bi_private = is_read ? NULL : sbi;
+	if (is_read) {
+		bio->bi_end_io = f2fs_read_end_io;
+		bio->bi_private = NULL;
+	} else {
+		bio->bi_end_io = f2fs_write_end_io;
+		bio->bi_private = sbi;
+		bio->bi_write_hint = io_type_to_rw_hint(sbi, type, temp);
+	}
 	if (wbc)
 		wbc_init_bio(wbc, bio);
 
@@ -382,7 +389,7 @@ int f2fs_submit_page_bio(struct f2fs_io_info *fio)
 
 	/* Allocate a new bio */
 	bio = __bio_alloc(fio->sbi, fio->new_blkaddr, fio->io_wbc,
-				1, is_read_io(fio->op));
+				1, is_read_io(fio->op), fio->type, fio->temp);
 
 	if (bio_add_page(bio, page, PAGE_SIZE, 0) < PAGE_SIZE) {
 		bio_put(bio);
@@ -445,7 +452,8 @@ int f2fs_submit_page_write(struct f2fs_io_info *fio)
 			goto out_fail;
 		}
 		io->bio = __bio_alloc(sbi, fio->new_blkaddr, fio->io_wbc,
-						BIO_MAX_PAGES, false);
+						BIO_MAX_PAGES, false,
+						fio->type, fio->temp);
 		io->fio = *fio;
 	}
 
@@ -2287,10 +2295,13 @@ static ssize_t f2fs_direct_IO(struct kiocb *iocb, struct iov_iter *iter)
 {
 	struct address_space *mapping = iocb->ki_filp->f_mapping;
 	struct inode *inode = mapping->host;
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
 	size_t count = iov_iter_count(iter);
 	loff_t offset = iocb->ki_pos;
 	int rw = iov_iter_rw(iter);
 	int err;
+	enum rw_hint hint = iocb->ki_hint;
+	int whint_mode = sbi->whint_mode;
 
 	err = check_direct_IO(inode, iter, offset);
 	if (err)
@@ -2301,11 +2312,16 @@ static ssize_t f2fs_direct_IO(struct kiocb *iocb, struct iov_iter *iter)
 
 	trace_f2fs_direct_IO_enter(inode, offset, count, rw);
 
+	if (rw == WRITE && whint_mode == WHINT_MODE_OFF)
+		iocb->ki_hint = WRITE_LIFE_NOT_SET;
+
 	down_read(&F2FS_I(inode)->dio_rwsem[rw]);
 	err = blockdev_direct_IO(iocb, inode, iter, get_data_block_dio);
 	up_read(&F2FS_I(inode)->dio_rwsem[rw]);
 
 	if (rw == WRITE) {
+		if (whint_mode == WHINT_MODE_OFF)
+			iocb->ki_hint = hint;
 		if (err > 0) {
 			f2fs_update_iostat(F2FS_I_SB(inode), APP_DIRECT_IO,
 									err);

commit 3da90b159b146672f830bcd2489dd3a1f4e9e089
Merge: efd52b5d363e 1c1d35df7110
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jan 30 19:07:32 2018 -0800

    Merge tag 'f2fs-for-4.16-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/jaegeuk/f2fs
    
    Pull f2fs updates from Jaegeuk Kim:
     "In this round, we've followed up to support some generic features such
      as cgroup, block reservation, linking fscrypt_ops, delivering
      write_hints, and some ioctls. And, we could fix some corner cases in
      terms of power-cut recovery and subtle deadlocks.
    
      Enhancements:
       - bitmap operations to handle NAT blocks
       - readahead to improve readdir speed
       - switch to use fscrypt_*
       - apply write hints for direct IO
       - add reserve_root=%u,resuid=%u,resgid=%u to reserve blocks for root/uid/gid
       - modify b_avail and b_free to consider root reserved blocks
       - support cgroup writeback
       - support FIEMAP_FLAG_XATTR for fibmap
       - add F2FS_IOC_PRECACHE_EXTENTS to pre-cache extents
       - add F2FS_IOC_{GET/SET}_PIN_FILE to pin LBAs for data blocks
       - support inode creation time
    
      Bug fixs:
       - sysfile-based quota operations
       - memory footprint accounting
       - allow to write data on partial preallocation case
       - fix deadlock case on fallocate
       - fix to handle fill_super errors
       - fix missing inode updates of fsync'ed file
       - recover renamed file which was fsycn'ed before
       - drop inmemory pages in corner error case
       - keep last_disk_size correctly
       - recover missing i_inline flags during roll-forward
    
      Various clean-up patches were added as well"
    
    * tag 'f2fs-for-4.16-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/jaegeuk/f2fs: (72 commits)
      f2fs: support inode creation time
      f2fs: rebuild sit page from sit info in mem
      f2fs: stop issuing discard if fs is readonly
      f2fs: clean up duplicated assignment in init_discard_policy
      f2fs: use GFP_F2FS_ZERO for cleanup
      f2fs: allow to recover node blocks given updated checkpoint
      f2fs: recover some i_inline flags
      f2fs: correct removexattr behavior for null valued extended attribute
      f2fs: drop page cache after fs shutdown
      f2fs: stop gc/discard thread after fs shutdown
      f2fs: hanlde error case in f2fs_ioc_shutdown
      f2fs: split need_inplace_update
      f2fs: fix to update last_disk_size correctly
      f2fs: kill F2FS_INLINE_XATTR_ADDRS for cleanup
      f2fs: clean up error path of fill_super
      f2fs: avoid hungtask when GC encrypted block if io_bits is set
      f2fs: allow quota to use reserved blocks
      f2fs: fix to drop all inmem pages correctly
      f2fs: speed up defragment on sparse file
      f2fs: support F2FS_IOC_PRECACHE_EXTENTS
      ...

commit db198ae0f823e13e3698b24049e741978a0f14e3
Author: Chao Yu <yuchao0@huawei.com>
Date:   Thu Jan 18 17:29:10 2018 +0800

    f2fs: drop page cache after fs shutdown
    
    Don't remain dirtied page cache in f2fs after shutdown, it can mitigate
    memory pressure of whole system, in order to keep other modules working
    properly.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index f428f7b79c64..6cba74eb09a7 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1716,6 +1716,12 @@ static int __write_data_page(struct page *page, bool *submitted,
 
 	trace_f2fs_writepage(page, DATA);
 
+	/* we should bypass data pages to proceed the kworkder jobs */
+	if (unlikely(f2fs_cp_error(sbi))) {
+		mapping_set_error(page->mapping, -EIO);
+		goto out;
+	}
+
 	if (unlikely(is_sbi_flag_set(sbi, SBI_POR_DOING)))
 		goto redirty_out;
 
@@ -1740,12 +1746,6 @@ static int __write_data_page(struct page *page, bool *submitted,
 			available_free_memory(sbi, BASE_CHECK))))
 		goto redirty_out;
 
-	/* we should bypass data pages to proceed the kworkder jobs */
-	if (unlikely(f2fs_cp_error(sbi))) {
-		mapping_set_error(page->mapping, -EIO);
-		goto out;
-	}
-
 	/* Dentry blocks are controlled by checkpoint */
 	if (S_ISDIR(inode->i_mode)) {
 		fio.need_lock = LOCK_DONE;

commit bb9e3bb8dbf5975a493e4e2dfbb900018579890c
Author: Chao Yu <yuchao0@huawei.com>
Date:   Wed Jan 17 16:31:38 2018 +0800

    f2fs: split need_inplace_update
    
    This patch splits need_inplace_update to two functions:
    a. should_update_inplace() includes all conditions that we must use IPU.
    b. should_update_outplace() includes all conditions that we must use OPU.
    
    So that, in f2fs_ioc_set_pin_file() and f2fs_defragment_range(), we can
    use corresponding function to check whether we can trigger OPU/IPU or not.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index fa91bfeb754b..f428f7b79c64 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1518,20 +1518,79 @@ static int encrypt_one_page(struct f2fs_io_info *fio)
 	return PTR_ERR(fio->encrypted_page);
 }
 
-static inline bool need_inplace_update(struct f2fs_io_info *fio)
+static inline bool check_inplace_update_policy(struct inode *inode,
+				struct f2fs_io_info *fio)
 {
-	struct inode *inode = fio->page->mapping->host;
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	unsigned int policy = SM_I(sbi)->ipu_policy;
 
+	if (policy & (0x1 << F2FS_IPU_FORCE))
+		return true;
+	if (policy & (0x1 << F2FS_IPU_SSR) && need_SSR(sbi))
+		return true;
+	if (policy & (0x1 << F2FS_IPU_UTIL) &&
+			utilization(sbi) > SM_I(sbi)->min_ipu_util)
+		return true;
+	if (policy & (0x1 << F2FS_IPU_SSR_UTIL) && need_SSR(sbi) &&
+			utilization(sbi) > SM_I(sbi)->min_ipu_util)
+		return true;
+
+	/*
+	 * IPU for rewrite async pages
+	 */
+	if (policy & (0x1 << F2FS_IPU_ASYNC) &&
+			fio && fio->op == REQ_OP_WRITE &&
+			!(fio->op_flags & REQ_SYNC) &&
+			!f2fs_encrypted_inode(inode))
+		return true;
+
+	/* this is only set during fdatasync */
+	if (policy & (0x1 << F2FS_IPU_FSYNC) &&
+			is_inode_flag_set(inode, FI_NEED_IPU))
+		return true;
+
+	return false;
+}
+
+bool should_update_inplace(struct inode *inode, struct f2fs_io_info *fio)
+{
 	if (f2fs_is_pinned_file(inode))
 		return true;
-	if (S_ISDIR(inode->i_mode) || f2fs_is_atomic_file(inode))
-		return false;
-	if (is_cold_data(fio->page))
-		return false;
-	if (IS_ATOMIC_WRITTEN_PAGE(fio->page))
+
+	/* if this is cold file, we should overwrite to avoid fragmentation */
+	if (file_is_cold(inode))
+		return true;
+
+	return check_inplace_update_policy(inode, fio);
+}
+
+bool should_update_outplace(struct inode *inode, struct f2fs_io_info *fio)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+
+	if (test_opt(sbi, LFS))
+		return true;
+	if (S_ISDIR(inode->i_mode))
+		return true;
+	if (f2fs_is_atomic_file(inode))
+		return true;
+	if (fio) {
+		if (is_cold_data(fio->page))
+			return true;
+		if (IS_ATOMIC_WRITTEN_PAGE(fio->page))
+			return true;
+	}
+	return false;
+}
+
+static inline bool need_inplace_update(struct f2fs_io_info *fio)
+{
+	struct inode *inode = fio->page->mapping->host;
+
+	if (should_update_outplace(inode, fio))
 		return false;
 
-	return need_inplace_update_policy(inode, fio);
+	return should_update_inplace(inode, fio);
 }
 
 static inline bool valid_ipu_blkaddr(struct f2fs_io_info *fio)

commit eb4497975e82448746bb7de3d13b743b7145aed7
Author: Chao Yu <yuchao0@huawei.com>
Date:   Wed Jan 17 16:31:37 2018 +0800

    f2fs: fix to update last_disk_size correctly
    
    This patch fixes to update last_disk_size only when writing out page
    successfully.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 2b892a619d71..fa91bfeb754b 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1716,10 +1716,14 @@ static int __write_data_page(struct page *page, bool *submitted,
 		}
 	}
 
-	down_write(&F2FS_I(inode)->i_sem);
-	if (F2FS_I(inode)->last_disk_size < psize)
-		F2FS_I(inode)->last_disk_size = psize;
-	up_write(&F2FS_I(inode)->i_sem);
+	if (err) {
+		file_set_keep_isize(inode);
+	} else {
+		down_write(&F2FS_I(inode)->i_sem);
+		if (F2FS_I(inode)->last_disk_size < psize)
+			F2FS_I(inode)->last_disk_size = psize;
+		up_write(&F2FS_I(inode)->i_sem);
+	}
 
 done:
 	if (err && err != -ENOENT)

commit b323fd28bbf95c3181e02e7a642b7d24f3e32714
Author: Chao Yu <yuchao0@huawei.com>
Date:   Wed Jan 17 16:31:36 2018 +0800

    f2fs: kill F2FS_INLINE_XATTR_ADDRS for cleanup
    
    Use get_inline_xattr_addrs directly instead of F2FS_INLINE_XATTR_ADDRS.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 2db26388b6bf..2b892a619d71 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1200,7 +1200,7 @@ static int f2fs_xattr_fiemap(struct inode *inode,
 		phys = (__u64)blk_to_logical(inode, ni.blk_addr);
 		offset = offsetof(struct f2fs_inode, i_addr) +
 					sizeof(__le32) * (DEF_ADDRS_PER_INODE -
-					F2FS_INLINE_XATTR_ADDRS(inode));
+					get_inline_xattr_addrs(inode));
 
 		phys += offset;
 		len = inline_xattr_size(inode);

commit a2e2e76b23038b187e5656c467ec76eeb29b8275
Author: Chao Yu <yuchao0@huawei.com>
Date:   Mon Jan 15 17:16:46 2018 +0800

    f2fs: fix to drop all inmem pages correctly
    
    In commit 57864ae5ce3a ("f2fs: limit # of inmemory pages"), we have
    limited memory footprint of all inmem pages with 20% of total memory,
    otherwise, if we exceed the threshold, we will try to drop all inmem
    pages to avoid excessive memory pressure resulting in performance
    regression.
    
    But in some unrelated error paths, we will also drop all inmem pages,
    which should be wrong, fix it in this patch.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 81fd4823a071..2db26388b6bf 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -2078,7 +2078,7 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
 	struct page *page = NULL;
 	pgoff_t index = ((unsigned long long) pos) >> PAGE_SHIFT;
-	bool need_balance = false;
+	bool need_balance = false, drop_atomic = false;
 	block_t blkaddr = NULL_ADDR;
 	int err = 0;
 
@@ -2087,6 +2087,7 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 	if (f2fs_is_atomic_file(inode) &&
 			!available_free_memory(sbi, INMEM_PAGES)) {
 		err = -ENOMEM;
+		drop_atomic = true;
 		goto fail;
 	}
 
@@ -2167,7 +2168,7 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 fail:
 	f2fs_put_page(page, 1);
 	f2fs_write_failed(mapping, pos + len);
-	if (f2fs_is_atomic_file(inode))
+	if (drop_atomic)
 		drop_inmem_pages_all(sbi);
 	return err;
 }

commit f3d98e74fcddb23dba00a88145272fc8223baaee
Author: Chao Yu <yuchao0@huawei.com>
Date:   Wed Jan 10 18:18:52 2018 +0800

    f2fs: speed up defragment on sparse file
    
    We have supported to get next page offset with valid mapping crossing
    hole in f2fs_map_blocks, utilizing it to speed up defragment on sparse
    file.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 28a3328f1617..81fd4823a071 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1017,8 +1017,12 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 					*map->m_next_pgofs = pgofs + 1;
 				goto sync_out;
 			}
-			if (flag != F2FS_GET_BLOCK_FIEMAP)
+			if (flag != F2FS_GET_BLOCK_FIEMAP) {
+				/* for defragment case */
+				if (map->m_next_pgofs)
+					*map->m_next_pgofs = pgofs + 1;
 				goto sync_out;
+			}
 		}
 	}
 

commit c4020b2da4c9e84d63e30ce5a85dc287507f0e60
Author: Chao Yu <yuchao0@huawei.com>
Date:   Thu Jan 11 14:42:30 2018 +0800

    f2fs: support F2FS_IOC_PRECACHE_EXTENTS
    
    This patch introduces a new ioctl F2FS_IOC_PRECACHE_EXTENTS to precache
    extent info like ext4, in order to gain better performance during
    triggering AIO by eliminating synchronous waiting of mapping info.
    
    Referred commit: 7869a4a6c5ca ("ext4: add support for extent pre-caching")
    
    In addition, with newly added extent precache abilitiy, this patch add
    to support FIEMAP_FLAG_CACHE in ->fiemap.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 945f15607484..28a3328f1617 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -865,6 +865,7 @@ int f2fs_preallocate_blocks(struct kiocb *iocb, struct iov_iter *from)
 		map.m_len = 0;
 
 	map.m_next_pgofs = NULL;
+	map.m_next_extent = NULL;
 	map.m_seg_type = NO_CHECK_TYPE;
 
 	if (direct_io) {
@@ -931,6 +932,7 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 	blkcnt_t prealloc;
 	struct extent_info ei = {0,0,0};
 	block_t blkaddr;
+	unsigned int start_pgofs;
 
 	if (!maxblocks)
 		return 0;
@@ -946,6 +948,8 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 		map->m_pblk = ei.blk + pgofs - ei.fofs;
 		map->m_len = min((pgoff_t)maxblocks, ei.fofs + ei.len - pgofs);
 		map->m_flags = F2FS_MAP_MAPPED;
+		if (map->m_next_extent)
+			*map->m_next_extent = pgofs + map->m_len;
 		goto out;
 	}
 
@@ -964,10 +968,14 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 			if (map->m_next_pgofs)
 				*map->m_next_pgofs =
 					get_next_page_offset(&dn, pgofs);
+			if (map->m_next_extent)
+				*map->m_next_extent =
+					get_next_page_offset(&dn, pgofs);
 		}
 		goto unlock_out;
 	}
 
+	start_pgofs = pgofs;
 	prealloc = 0;
 	last_ofs_in_node = ofs_in_node = dn.ofs_in_node;
 	end_offset = ADDRS_PER_PAGE(dn.node_page, inode);
@@ -1001,6 +1009,8 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 				map->m_pblk = 0;
 				goto sync_out;
 			}
+			if (flag == F2FS_GET_BLOCK_PRECACHE)
+				goto sync_out;
 			if (flag == F2FS_GET_BLOCK_FIEMAP &&
 						blkaddr == NULL_ADDR) {
 				if (map->m_next_pgofs)
@@ -1059,6 +1069,16 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 	else if (dn.ofs_in_node < end_offset)
 		goto next_block;
 
+	if (flag == F2FS_GET_BLOCK_PRECACHE) {
+		if (map->m_flags & F2FS_MAP_MAPPED) {
+			unsigned int ofs = start_pgofs - map->m_lblk;
+
+			f2fs_update_extent_cache_range(&dn,
+				start_pgofs, map->m_pblk + ofs,
+				map->m_len - ofs);
+		}
+	}
+
 	f2fs_put_dnode(&dn);
 
 	if (create) {
@@ -1068,6 +1088,17 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 	goto next_dnode;
 
 sync_out:
+	if (flag == F2FS_GET_BLOCK_PRECACHE) {
+		if (map->m_flags & F2FS_MAP_MAPPED) {
+			unsigned int ofs = start_pgofs - map->m_lblk;
+
+			f2fs_update_extent_cache_range(&dn,
+				start_pgofs, map->m_pblk + ofs,
+				map->m_len - ofs);
+		}
+		if (map->m_next_extent)
+			*map->m_next_extent = pgofs + 1;
+	}
 	f2fs_put_dnode(&dn);
 unlock_out:
 	if (create) {
@@ -1089,6 +1120,7 @@ static int __get_data_block(struct inode *inode, sector_t iblock,
 	map.m_lblk = iblock;
 	map.m_len = bh->b_size >> inode->i_blkbits;
 	map.m_next_pgofs = next_pgofs;
+	map.m_next_extent = NULL;
 	map.m_seg_type = seg_type;
 
 	err = f2fs_map_blocks(inode, &map, create, flag);
@@ -1212,6 +1244,12 @@ int f2fs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,
 	u32 flags = 0;
 	int ret = 0;
 
+	if (fieinfo->fi_flags & FIEMAP_FLAG_CACHE) {
+		ret = f2fs_precache_extents(inode);
+		if (ret)
+			return ret;
+	}
+
 	ret = fiemap_check_flags(fieinfo, FIEMAP_FLAG_SYNC | FIEMAP_FLAG_XATTR);
 	if (ret)
 		return ret;
@@ -1313,6 +1351,7 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 	map.m_len = 0;
 	map.m_flags = 0;
 	map.m_next_pgofs = NULL;
+	map.m_next_extent = NULL;
 	map.m_seg_type = NO_CHECK_TYPE;
 
 	for (; nr_pages; nr_pages--) {

commit 1ad71a27124caf0b68ddd3c92be01aa2b2a72b2a
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Dec 7 16:25:39 2017 -0800

    f2fs: add an ioctl to disable GC for specific file
    
    This patch gives a flag to disable GC on given file, which would be useful, when
    user wants to keep its block map. It also conducts in-place-update for dontmove
    file.
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index cde2a3264b4c..945f15607484 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1479,6 +1479,8 @@ static inline bool need_inplace_update(struct f2fs_io_info *fio)
 {
 	struct inode *inode = fio->page->mapping->host;
 
+	if (f2fs_is_pinned_file(inode))
+		return true;
 	if (S_ISDIR(inode->i_mode) || f2fs_is_atomic_file(inode))
 		return false;
 	if (is_cold_data(fio->page))

commit 442a9dbd577e38211d296f35443b5e257bb5a9b3
Author: Chao Yu <yuchao0@huawei.com>
Date:   Thu Jan 11 14:39:57 2018 +0800

    f2fs: support FIEMAP_FLAG_XATTR
    
    This patch enables ->fiemap to handle FIEMAP_FLAG_XATTR flag for xattr
    mapping info lookup purpose.
    
    It makes f2fs passing generic/425 test in fstest.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 664fe0aa18bb..cde2a3264b4c 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1140,6 +1140,68 @@ static inline loff_t blk_to_logical(struct inode *inode, sector_t blk)
 	return (blk << inode->i_blkbits);
 }
 
+static int f2fs_xattr_fiemap(struct inode *inode,
+				struct fiemap_extent_info *fieinfo)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct page *page;
+	struct node_info ni;
+	__u64 phys = 0, len;
+	__u32 flags;
+	nid_t xnid = F2FS_I(inode)->i_xattr_nid;
+	int err = 0;
+
+	if (f2fs_has_inline_xattr(inode)) {
+		int offset;
+
+		page = f2fs_grab_cache_page(NODE_MAPPING(sbi),
+						inode->i_ino, false);
+		if (!page)
+			return -ENOMEM;
+
+		get_node_info(sbi, inode->i_ino, &ni);
+
+		phys = (__u64)blk_to_logical(inode, ni.blk_addr);
+		offset = offsetof(struct f2fs_inode, i_addr) +
+					sizeof(__le32) * (DEF_ADDRS_PER_INODE -
+					F2FS_INLINE_XATTR_ADDRS(inode));
+
+		phys += offset;
+		len = inline_xattr_size(inode);
+
+		f2fs_put_page(page, 1);
+
+		flags = FIEMAP_EXTENT_DATA_INLINE | FIEMAP_EXTENT_NOT_ALIGNED;
+
+		if (!xnid)
+			flags |= FIEMAP_EXTENT_LAST;
+
+		err = fiemap_fill_next_extent(fieinfo, 0, phys, len, flags);
+		if (err || err == 1)
+			return err;
+	}
+
+	if (xnid) {
+		page = f2fs_grab_cache_page(NODE_MAPPING(sbi), xnid, false);
+		if (!page)
+			return -ENOMEM;
+
+		get_node_info(sbi, xnid, &ni);
+
+		phys = (__u64)blk_to_logical(inode, ni.blk_addr);
+		len = inode->i_sb->s_blocksize;
+
+		f2fs_put_page(page, 1);
+
+		flags = FIEMAP_EXTENT_LAST;
+	}
+
+	if (phys)
+		err = fiemap_fill_next_extent(fieinfo, 0, phys, len, flags);
+
+	return (err < 0 ? err : 0);
+}
+
 int f2fs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,
 		u64 start, u64 len)
 {
@@ -1150,12 +1212,17 @@ int f2fs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,
 	u32 flags = 0;
 	int ret = 0;
 
-	ret = fiemap_check_flags(fieinfo, FIEMAP_FLAG_SYNC);
+	ret = fiemap_check_flags(fieinfo, FIEMAP_FLAG_SYNC | FIEMAP_FLAG_XATTR);
 	if (ret)
 		return ret;
 
 	inode_lock(inode);
 
+	if (fieinfo->fi_flags & FIEMAP_FLAG_XATTR) {
+		ret = f2fs_xattr_fiemap(inode, fieinfo);
+		goto out;
+	}
+
 	if (f2fs_has_inline_data(inode)) {
 		ret = f2fs_inline_data_fiemap(inode, fieinfo, start, len);
 		if (ret != -EAGAIN)

commit f1b43d4cd5f2563f0f1bb2c84eff94faa4c2853b
Author: Chao Yu <yuchao0@huawei.com>
Date:   Thu Jan 11 14:37:35 2018 +0800

    f2fs: fix to cover f2fs_inline_data_fiemap with inode_lock
    
    This patch fix to cover f2fs_inline_data_fiemap with inode_lock in order
    to make that interface avoiding race with mapping change.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 31add841ec39..664fe0aa18bb 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1154,14 +1154,14 @@ int f2fs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,
 	if (ret)
 		return ret;
 
+	inode_lock(inode);
+
 	if (f2fs_has_inline_data(inode)) {
 		ret = f2fs_inline_data_fiemap(inode, fieinfo, start, len);
 		if (ret != -EAGAIN)
-			return ret;
+			goto out;
 	}
 
-	inode_lock(inode);
-
 	if (logical_to_blk(inode, len) == 0)
 		len = blk_to_logical(inode, 1);
 

commit 7dff55d27e07c571b6c532b714c22d7ce9fba1e0
Author: Yunlei He <heyunlei@huawei.com>
Date:   Thu Jan 11 14:19:32 2018 +0800

    f2fs: check node page again in write end io
    
    Check node page again in write end io in case of
    data corruption during inflght IO.
    
    Signed-off-by: Yunlei He <heyunlei@huawei.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 304b899a6892..31add841ec39 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -114,6 +114,10 @@ static void f2fs_write_end_io(struct bio *bio)
 			if (type == F2FS_WB_CP_DATA)
 				f2fs_stop_checkpoint(sbi, true);
 		}
+
+		f2fs_bug_on(sbi, page->mapping == NODE_MAPPING(sbi) &&
+					page->index != nid_of_node(page));
+
 		dec_page_count(sbi, type);
 		clear_cold_data(page);
 		end_page_writeback(page);

commit 578c647879f74c333d20762375fd970907f2e97c
Author: Yufen Yu <yuyufen@huawei.com>
Date:   Tue Jan 9 19:33:39 2018 +0800

    f2fs: implement cgroup writeback support
    
    Cgroup writeback requires explicit support from the filesystem.
    f2fs's data and node writeback IOs go through __write_data_page,
    which sets fio for submiting IOs. So, we add io_wbc for fio,
    associate bios with blkcg by invoking wbc_init_bio() and
    account IOs issuing by wbc_account_io().
    In addtion, f2fs_fill_super() is updated to set SB_I_CGROUPWB.
    
    Meta writeback IOs is left alone by this patch and will always be
    attributed to the root cgroup.
    
    The results show that f2fs can throttle writeback nicely for
    data writing and file creating.
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Yufen Yu <yuyufen@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index a023863ef27f..304b899a6892 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -170,6 +170,7 @@ static bool __same_bdev(struct f2fs_sb_info *sbi,
  * Low-level block read/write IO operations.
  */
 static struct bio *__bio_alloc(struct f2fs_sb_info *sbi, block_t blk_addr,
+				struct writeback_control *wbc,
 				int npages, bool is_read)
 {
 	struct bio *bio;
@@ -179,6 +180,8 @@ static struct bio *__bio_alloc(struct f2fs_sb_info *sbi, block_t blk_addr,
 	f2fs_target_device(sbi, blk_addr, bio);
 	bio->bi_end_io = is_read ? f2fs_read_end_io : f2fs_write_end_io;
 	bio->bi_private = is_read ? NULL : sbi;
+	if (wbc)
+		wbc_init_bio(wbc, bio);
 
 	return bio;
 }
@@ -374,7 +377,8 @@ int f2fs_submit_page_bio(struct f2fs_io_info *fio)
 	f2fs_trace_ios(fio, 0);
 
 	/* Allocate a new bio */
-	bio = __bio_alloc(fio->sbi, fio->new_blkaddr, 1, is_read_io(fio->op));
+	bio = __bio_alloc(fio->sbi, fio->new_blkaddr, fio->io_wbc,
+				1, is_read_io(fio->op));
 
 	if (bio_add_page(bio, page, PAGE_SIZE, 0) < PAGE_SIZE) {
 		bio_put(bio);
@@ -436,7 +440,7 @@ int f2fs_submit_page_write(struct f2fs_io_info *fio)
 			dec_page_count(sbi, WB_DATA_TYPE(bio_page));
 			goto out_fail;
 		}
-		io->bio = __bio_alloc(sbi, fio->new_blkaddr,
+		io->bio = __bio_alloc(sbi, fio->new_blkaddr, fio->io_wbc,
 						BIO_MAX_PAGES, false);
 		io->fio = *fio;
 	}
@@ -446,6 +450,9 @@ int f2fs_submit_page_write(struct f2fs_io_info *fio)
 		goto alloc_new;
 	}
 
+	if (fio->io_wbc)
+		wbc_account_io(fio->io_wbc, bio_page, PAGE_SIZE);
+
 	io->last_block_in_bio = fio->new_blkaddr;
 	f2fs_trace_ios(fio, 0);
 
@@ -1529,6 +1536,7 @@ static int __write_data_page(struct page *page, bool *submitted,
 		.submitted = false,
 		.need_lock = LOCK_RETRY,
 		.io_type = io_type,
+		.io_wbc = wbc,
 	};
 
 	trace_f2fs_writepage(page, DATA);

commit 263663cd3c4fbfc40cb7504c4be2dadbc0992cc1
Author: Ming Lei <ming.lei@redhat.com>
Date:   Mon Dec 18 20:22:04 2017 +0800

    block: convert to bio_first_bvec_all & bio_first_page_all
    
    This patch converts to bio_first_bvec_all() & bio_first_page_all() for
    retrieving the 1st bvec/page, and prepares for supporting multipage bvec.
    
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 516fa0d3ff9c..455f086cce3d 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -56,7 +56,7 @@ static void f2fs_read_end_io(struct bio *bio)
 	int i;
 
 #ifdef CONFIG_F2FS_FAULT_INJECTION
-	if (time_to_inject(F2FS_P_SB(bio->bi_io_vec->bv_page), FAULT_IO)) {
+	if (time_to_inject(F2FS_P_SB(bio_first_page_all(bio)), FAULT_IO)) {
 		f2fs_show_injection_info(FAULT_IO);
 		bio->bi_status = BLK_STS_IOERR;
 	}

commit d6d478a14bd3bc77afe813fa9c2ce8ebebf01b2d
Author: Chao Yu <yuchao0@huawei.com>
Date:   Wed Jan 3 17:30:19 2018 +0800

    f2fs: continue to do direct IO if we only preallocate partial blocks
    
    While doing direct IO, if we run out-of-space when we preallocate blocks,
    we should not return ENOSPC error directly, instead, we should continue
    to do following direct IO, which will keep directIO of f2fs acting like
    other filesystems.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 449b0aaa3905..a023863ef27f 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -832,10 +832,12 @@ int f2fs_preallocate_blocks(struct kiocb *iocb, struct iov_iter *from)
 {
 	struct inode *inode = file_inode(iocb->ki_filp);
 	struct f2fs_map_blocks map;
+	int flag;
 	int err = 0;
+	bool direct_io = iocb->ki_flags & IOCB_DIRECT;
 
 	/* convert inline data for Direct I/O*/
-	if (iocb->ki_flags & IOCB_DIRECT) {
+	if (direct_io) {
 		err = f2fs_convert_inline_inode(inode);
 		if (err)
 			return err;
@@ -854,25 +856,29 @@ int f2fs_preallocate_blocks(struct kiocb *iocb, struct iov_iter *from)
 	map.m_next_pgofs = NULL;
 	map.m_seg_type = NO_CHECK_TYPE;
 
-	if (iocb->ki_flags & IOCB_DIRECT) {
+	if (direct_io) {
 		map.m_seg_type = rw_hint_to_seg_type(iocb->ki_hint);
-		return f2fs_map_blocks(inode, &map, 1,
-			__force_buffered_io(inode, WRITE) ?
-				F2FS_GET_BLOCK_PRE_AIO :
-				F2FS_GET_BLOCK_PRE_DIO);
+		flag = __force_buffered_io(inode, WRITE) ?
+					F2FS_GET_BLOCK_PRE_AIO :
+					F2FS_GET_BLOCK_PRE_DIO;
+		goto map_blocks;
 	}
 	if (iocb->ki_pos + iov_iter_count(from) > MAX_INLINE_DATA(inode)) {
 		err = f2fs_convert_inline_inode(inode);
 		if (err)
 			return err;
 	}
-	if (!f2fs_has_inline_data(inode)) {
-		err = f2fs_map_blocks(inode, &map, 1, F2FS_GET_BLOCK_PRE_AIO);
-		if (map.m_len > 0 && err == -ENOSPC) {
-			set_inode_flag(inode, FI_NO_PREALLOC);
-			err = 0;
-		}
+	if (f2fs_has_inline_data(inode))
 		return err;
+
+	flag = F2FS_GET_BLOCK_PRE_AIO;
+
+map_blocks:
+	err = f2fs_map_blocks(inode, &map, 1, flag);
+	if (map.m_len > 0 && err == -ENOSPC) {
+		if (!direct_io)
+			set_inode_flag(inode, FI_NO_PREALLOC);
+		err = 0;
 	}
 	return err;
 }

commit b1ca321d1cd8f09965e7306ccf3998c7eb7e7b19
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Sun Dec 31 16:26:38 2017 -0800

    f2fs: skip stop_checkpoint for user data writes
    
    We can give another chance to write user data, which can resolve
    generic/441.
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 7aca6ccd01f6..449b0aaa3905 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -111,7 +111,8 @@ static void f2fs_write_end_io(struct bio *bio)
 
 		if (unlikely(bio->bi_status)) {
 			mapping_set_error(page->mapping, -EIO);
-			f2fs_stop_checkpoint(sbi, true);
+			if (type == F2FS_WB_CP_DATA)
+				f2fs_stop_checkpoint(sbi, true);
 		}
 		dec_page_count(sbi, type);
 		clear_cold_data(page);

commit 4c2ac6a86073a4cbcea123f8be84bd4417eae001
Author: Chao Yu <yuchao0@huawei.com>
Date:   Thu Nov 30 19:28:22 2017 +0800

    f2fs: clean up f2fs_map_blocks
    
    f2fs_map_blocks():
    
    if (blkaddr == NEW_ADDR || blkaddr == NULL_ADDR) {
            if (create) {
                    ...
            } else {
                    ...
                    if (flag == F2FS_GET_BLOCK_FIEMAP &&
                                            blkaddr == NULL_ADDR) {
                            ...
                    }
                    if (flag != F2FS_GET_BLOCK_FIEMAP ||
                                            blkaddr != NEW_ADDR)
                            goto sync_out;
            }
    
    It means we can break the loop in cases of:
    a) flag != F2FS_GET_BLOCK_FIEMAP or
    b) flag == F2FS_GET_BLOCK_FIEMAP && blkaddr == NULL_ADDR
    
    Condition b) is the same as previous one, so merge operations of them
    for readability.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index ce1a4e94e551..7aca6ccd01f6 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -987,9 +987,9 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 						blkaddr == NULL_ADDR) {
 				if (map->m_next_pgofs)
 					*map->m_next_pgofs = pgofs + 1;
+				goto sync_out;
 			}
-			if (flag != F2FS_GET_BLOCK_FIEMAP ||
-						blkaddr != NEW_ADDR)
+			if (flag != F2FS_GET_BLOCK_FIEMAP)
 				goto sync_out;
 		}
 	}

commit d5097be55c21c103d2227591708425aab2e9682d
Author: Hyunchul Lee <cheol.lee@lge.com>
Date:   Tue Nov 28 09:23:00 2017 +0900

    f2fs: apply write hints to select the type of segment for direct write
    
    When blocks are allocated for direct write, select the type of
    segment using the kiocb hint. But if an inode has FI_NO_ALLOC,
    use the inode hint.
    
    Signed-off-by: Hyunchul Lee <cheol.lee@lge.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 457a60ffd3ee..ce1a4e94e551 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -783,7 +783,7 @@ struct page *get_new_data_page(struct inode *inode,
 	return page;
 }
 
-static int __allocate_data_block(struct dnode_of_data *dn)
+static int __allocate_data_block(struct dnode_of_data *dn, int seg_type)
 {
 	struct f2fs_sb_info *sbi = F2FS_I_SB(dn->inode);
 	struct f2fs_summary sum;
@@ -808,7 +808,7 @@ static int __allocate_data_block(struct dnode_of_data *dn)
 	set_summary(&sum, dn->nid, dn->ofs_in_node, ni.version);
 
 	allocate_data_block(sbi, NULL, dn->data_blkaddr, &dn->data_blkaddr,
-					&sum, CURSEG_WARM_DATA, NULL, false);
+					&sum, seg_type, NULL, false);
 	set_data_blkaddr(dn);
 
 	/* update i_size */
@@ -851,12 +851,15 @@ int f2fs_preallocate_blocks(struct kiocb *iocb, struct iov_iter *from)
 		map.m_len = 0;
 
 	map.m_next_pgofs = NULL;
+	map.m_seg_type = NO_CHECK_TYPE;
 
-	if (iocb->ki_flags & IOCB_DIRECT)
+	if (iocb->ki_flags & IOCB_DIRECT) {
+		map.m_seg_type = rw_hint_to_seg_type(iocb->ki_hint);
 		return f2fs_map_blocks(inode, &map, 1,
 			__force_buffered_io(inode, WRITE) ?
 				F2FS_GET_BLOCK_PRE_AIO :
 				F2FS_GET_BLOCK_PRE_DIO);
+	}
 	if (iocb->ki_pos + iov_iter_count(from) > MAX_INLINE_DATA(inode)) {
 		err = f2fs_convert_inline_inode(inode);
 		if (err)
@@ -966,7 +969,8 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 					last_ofs_in_node = dn.ofs_in_node;
 				}
 			} else {
-				err = __allocate_data_block(&dn);
+				err = __allocate_data_block(&dn,
+							map->m_seg_type);
 				if (!err)
 					set_inode_flag(inode, FI_APPEND_WRITE);
 			}
@@ -1059,7 +1063,7 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 
 static int __get_data_block(struct inode *inode, sector_t iblock,
 			struct buffer_head *bh, int create, int flag,
-			pgoff_t *next_pgofs)
+			pgoff_t *next_pgofs, int seg_type)
 {
 	struct f2fs_map_blocks map;
 	int err;
@@ -1067,6 +1071,7 @@ static int __get_data_block(struct inode *inode, sector_t iblock,
 	map.m_lblk = iblock;
 	map.m_len = bh->b_size >> inode->i_blkbits;
 	map.m_next_pgofs = next_pgofs;
+	map.m_seg_type = seg_type;
 
 	err = f2fs_map_blocks(inode, &map, create, flag);
 	if (!err) {
@@ -1082,14 +1087,17 @@ static int get_data_block(struct inode *inode, sector_t iblock,
 			pgoff_t *next_pgofs)
 {
 	return __get_data_block(inode, iblock, bh_result, create,
-							flag, next_pgofs);
+							flag, next_pgofs,
+							NO_CHECK_TYPE);
 }
 
 static int get_data_block_dio(struct inode *inode, sector_t iblock,
 			struct buffer_head *bh_result, int create)
 {
 	return __get_data_block(inode, iblock, bh_result, create,
-						F2FS_GET_BLOCK_DEFAULT, NULL);
+						F2FS_GET_BLOCK_DEFAULT, NULL,
+						rw_hint_to_seg_type(
+							inode->i_write_hint));
 }
 
 static int get_data_block_bmap(struct inode *inode, sector_t iblock,
@@ -1100,7 +1108,8 @@ static int get_data_block_bmap(struct inode *inode, sector_t iblock,
 		return -EFBIG;
 
 	return __get_data_block(inode, iblock, bh_result, create,
-						F2FS_GET_BLOCK_BMAP, NULL);
+						F2FS_GET_BLOCK_BMAP, NULL,
+						NO_CHECK_TYPE);
 }
 
 static inline sector_t logical_to_blk(struct inode *inode, loff_t offset)
@@ -1219,6 +1228,7 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 	map.m_len = 0;
 	map.m_flags = 0;
 	map.m_next_pgofs = NULL;
+	map.m_seg_type = NO_CHECK_TYPE;
 
 	for (; nr_pages; nr_pages--) {
 		if (pages) {

commit 736c0a74856d762b09a997d28e3ff6d8bdcf942c
Author: LiFan <fanofcode.li@samsung.com>
Date:   Sat Nov 25 11:46:18 2017 +0800

    f2fs: remove an excess variable
    
    Remove the variable page_idx which no one would miss.
    
    Signed-off-by: Fan li <fanofcode.li@samsung.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 3fa20a932dd7..457a60ffd3ee 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1204,7 +1204,6 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 			unsigned nr_pages)
 {
 	struct bio *bio = NULL;
-	unsigned page_idx;
 	sector_t last_block_in_bio = 0;
 	struct inode *inode = mapping->host;
 	const unsigned blkbits = inode->i_blkbits;
@@ -1221,8 +1220,7 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 	map.m_flags = 0;
 	map.m_next_pgofs = NULL;
 
-	for (page_idx = 0; nr_pages; page_idx++, nr_pages--) {
-
+	for (; nr_pages; nr_pages--) {
 		if (pages) {
 			page = list_last_entry(pages, struct page, lru);
 

commit 25006645d2e79422b6b3f26e5a82b6cbb5d49a0e
Author: Sheng Yong <shengyong1@huawei.com>
Date:   Wed Nov 22 18:23:39 2017 +0800

    f2fs: still write data if preallocate only partial blocks
    
    If there is not enough space left, f2fs_preallocate_blocks may only
    preallocte partial blocks. As a result, the write operation fails
    but i_blocks is not 0.  To avoid this, f2fs should write data in
    non-preallocation way and write as many data as the size of i_blocks.
    
    Signed-off-by: Sheng Yong <shengyong1@huawei.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 516fa0d3ff9c..3fa20a932dd7 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -862,8 +862,14 @@ int f2fs_preallocate_blocks(struct kiocb *iocb, struct iov_iter *from)
 		if (err)
 			return err;
 	}
-	if (!f2fs_has_inline_data(inode))
-		return f2fs_map_blocks(inode, &map, 1, F2FS_GET_BLOCK_PRE_AIO);
+	if (!f2fs_has_inline_data(inode)) {
+		err = f2fs_map_blocks(inode, &map, 1, F2FS_GET_BLOCK_PRE_AIO);
+		if (map.m_len > 0 && err == -ENOSPC) {
+			set_inode_flag(inode, FI_NO_PREALLOC);
+			err = 0;
+		}
+		return err;
+	}
 	return err;
 }
 

commit a02cd4229e298aadbe8f5cf286edee8058d87116
Merge: 487e2c9f44c4 ead710b7d82d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Nov 16 12:10:21 2017 -0800

    Merge tag 'f2fs-for-4.15-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/jaegeuk/f2fs
    
    Pull f2fs updates from Jaegeuk Kim:
     "In this round, we introduce sysfile-based quota support which is
      required for Android by default. In addition, we allow that users are
      able to reserve some blocks in runtime to mitigate performance drops
      in low free space.
    
      Enhancements:
       - assign proper data segments according to write_hints given by user
       - issue cache_flush on dirty devices only among multiple devices
       - exploit cp_error flag and add more faults to enhance fault
         injection test
       - conduct more readaheads during f2fs_readdir
       - add a range for discard commands
    
      Bug fixes:
       - fix zero stat->st_blocks when inline_data is set
       - drop crypto key and free stale memory pointer while evict_inode is
         failing
       - fix some corner cases in free space and segment management
       - fix wrong last_disk_size
    
      This series includes lots of clean-ups and code enhancement in terms
      of xattr operations, discard/flush command control. In addition, it
      adds versatile debugfs entries to monitor f2fs status"
    
    * tag 'f2fs-for-4.15-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/jaegeuk/f2fs: (75 commits)
      f2fs: deny accessing encryption policy if encryption is off
      f2fs: inject fault in inc_valid_node_count
      f2fs: fix to clear FI_NO_PREALLOC
      f2fs: expose quota information in debugfs
      f2fs: separate nat entry mem alloc from nat_tree_lock
      f2fs: validate before set/clear free nat bitmap
      f2fs: avoid opened loop codes in __add_ino_entry
      f2fs: apply write hints to select the type of segments for buffered write
      f2fs: introduce scan_curseg_cache for cleanup
      f2fs: optimize the way of traversing free_nid_bitmap
      f2fs: keep scanning until enough free nids are acquired
      f2fs: trace checkpoint reason in fsync()
      f2fs: keep isize once block is reserved cross EOF
      f2fs: avoid race in between GC and block exchange
      f2fs: save a multiplication for last_nid calculation
      f2fs: fix summary info corruption
      f2fs: remove dead code in update_meta_page
      f2fs: remove unneeded semicolon
      f2fs: don't bother with inode->i_version
      f2fs: check curseg space before foreground GC
      ...

commit 8667982014d6048e0b5e286b6247ff24f48d4cc6
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Wed Nov 15 17:37:52 2017 -0800

    mm, pagevec: remove cold parameter for pagevecs
    
    Every pagevec_init user claims the pages being released are hot even in
    cases where it is unlikely the pages are hot.  As no one cares about the
    hotness of pages being released to the allocator, just ditch the
    parameter.
    
    No performance impact is expected as the overhead is marginal.  The
    parameter is removed simply because it is a bit stupid to have a useless
    parameter copied everywhere.
    
    Link: http://lkml.kernel.org/r/20171018075952.10627-6-mgorman@techsingularity.net
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 687703755824..7b3ad5d8e2e9 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1635,7 +1635,7 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 	int range_whole = 0;
 	int tag;
 
-	pagevec_init(&pvec, 0);
+	pagevec_init(&pvec);
 
 	if (get_dirty_pages(mapping->host) <=
 				SM_I(F2FS_M_SB(mapping))->min_hot_blocks)

commit 67fd707f468142d0f689a6240044bb45c1913003
Author: Jan Kara <jack@suse.cz>
Date:   Wed Nov 15 17:35:19 2017 -0800

    mm: remove nr_pages argument from pagevec_lookup_{,range}_tag()
    
    All users of pagevec_lookup() and pagevec_lookup_range() now pass
    PAGEVEC_SIZE as a desired number of pages.  Just drop the argument.
    
    Link: http://lkml.kernel.org/r/20171009151359.31984-15-jack@suse.cz
    Signed-off-by: Jan Kara <jack@suse.cz>
    Reviewed-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 17d2c2997ddd..687703755824 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1670,7 +1670,7 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 		int i;
 
 		nr_pages = pagevec_lookup_range_tag(&pvec, mapping, &index, end,
-				tag, PAGEVEC_SIZE);
+				tag);
 		if (nr_pages == 0)
 			break;
 

commit 69c4f35d25e475c07d1e5c033bfb514db360a079
Author: Jan Kara <jack@suse.cz>
Date:   Wed Nov 15 17:34:48 2017 -0800

    f2fs: use pagevec_lookup_range_tag()
    
    We want only pages from given range in f2fs_write_cache_pages().  Use
    pagevec_lookup_range_tag() instead of pagevec_lookup_tag() and remove
    unnecessary code.
    
    Link: http://lkml.kernel.org/r/20171009151359.31984-6-jack@suse.cz
    Signed-off-by: Jan Kara <jack@suse.cz>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Reviewed-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Cc: Jaegeuk Kim <jaegeuk@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 36b535207c88..17d2c2997ddd 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1669,8 +1669,8 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 	while (!done && (index <= end)) {
 		int i;
 
-		nr_pages = pagevec_lookup_tag(&pvec, mapping, &index, tag,
-			      min(end - index, (pgoff_t)PAGEVEC_SIZE - 1) + 1);
+		nr_pages = pagevec_lookup_range_tag(&pvec, mapping, &index, end,
+				tag, PAGEVEC_SIZE);
 		if (nr_pages == 0)
 			break;
 
@@ -1678,11 +1678,6 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 			struct page *page = pvec.pages[i];
 			bool submitted = false;
 
-			if (page->index > end) {
-				done = 1;
-				break;
-			}
-
 			done_index = page->index;
 retry_write:
 			lock_page(page);

commit d62fe971485f15fa08fec2a75ac4e4ffead40a5a
Author: Chao Yu <yuchao0@huawei.com>
Date:   Sat Oct 28 16:52:31 2017 +0800

    f2fs: support bio allocation error injection
    
    This patch adds to support bio allocation error injection to simulate
    out-of-memory test scenario.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index acb531788c43..b0781edc9ada 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -173,7 +173,7 @@ static struct bio *__bio_alloc(struct f2fs_sb_info *sbi, block_t blk_addr,
 {
 	struct bio *bio;
 
-	bio = f2fs_bio_alloc(npages);
+	bio = f2fs_bio_alloc(sbi, npages, true);
 
 	f2fs_target_device(sbi, blk_addr, bio);
 	bio->bi_end_io = is_read ? f2fs_read_end_io : f2fs_write_end_io;
@@ -473,7 +473,7 @@ static struct bio *f2fs_grab_read_bio(struct inode *inode, block_t blkaddr,
 		f2fs_wait_on_block_writeback(sbi, blkaddr);
 	}
 
-	bio = bio_alloc(GFP_KERNEL, min_t(int, nr_pages, BIO_MAX_PAGES));
+	bio = f2fs_bio_alloc(sbi, min_t(int, nr_pages, BIO_MAX_PAGES), false);
 	if (!bio) {
 		if (ctx)
 			fscrypt_release_ctx(ctx);

commit 01eccef7930f137bed9501bf0923931f45909b94
Author: Chao Yu <yuchao0@huawei.com>
Date:   Sat Oct 28 16:52:30 2017 +0800

    f2fs: support get_page error injection
    
    This patch adds to support get_page error injection to simulate
    out-of-memory test scenario.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 7fd09837820a..acb531788c43 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1965,7 +1965,7 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 	 * Do not use grab_cache_page_write_begin() to avoid deadlock due to
 	 * wait_for_stable_page. Will wait that below with our IO control.
 	 */
-	page = pagecache_get_page(mapping, index,
+	page = f2fs_pagecache_get_page(mapping, index,
 				FGP_LOCK | FGP_WRITE | FGP_CREAT, GFP_NOFS);
 	if (!page) {
 		err = -ENOMEM;

commit 57864ae5ce3ab5c6e3137dd03edefdb2e5531ba1
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed Oct 18 19:05:57 2017 -0700

    f2fs: limit # of inmemory pages
    
    If some abnormal users try lots of atomic write operations, f2fs is able to
    produce pinned pages in the main memory which affects system performance.
    This patch limits that as 20% over total memory size, and if f2fs reaches
    to the limit, it will drop all the inmemory pages.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 95fdbe3e6cca..7fd09837820a 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1944,6 +1944,12 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 
 	trace_f2fs_write_begin(inode, pos, len, flags);
 
+	if (f2fs_is_atomic_file(inode) &&
+			!available_free_memory(sbi, INMEM_PAGES)) {
+		err = -ENOMEM;
+		goto fail;
+	}
+
 	/*
 	 * We should check this at this moment to avoid deadlock on inode page
 	 * and #0 page. The locking rule for inline_data conversion should be:
@@ -2021,6 +2027,8 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 fail:
 	f2fs_put_page(page, 1);
 	f2fs_write_failed(mapping, pos + len);
+	if (f2fs_is_atomic_file(inode))
+		drop_inmem_pages_all(sbi);
 	return err;
 }
 

commit a0d00fad353d4a315f54fb345aa1c65b3771389b
Author: Chao Yu <yuchao0@huawei.com>
Date:   Mon Oct 9 17:55:19 2017 +0800

    f2fs: fix to avoid race when accessing last_disk_size
    
    last_disk_size could be wrong due to concurrently updating, so using
    i_sem semaphore to make last_disk_size updating exclusive to fix this
    issue.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 60a07fb26475..95fdbe3e6cca 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1570,8 +1570,11 @@ static int __write_data_page(struct page *page, bool *submitted,
 			err = do_write_data_page(&fio);
 		}
 	}
+
+	down_write(&F2FS_I(inode)->i_sem);
 	if (F2FS_I(inode)->last_disk_size < psize)
 		F2FS_I(inode)->last_disk_size = psize;
+	up_write(&F2FS_I(inode)->i_sem);
 
 done:
 	if (err && err != -ENOENT)

commit ebf7c522fdc2ec2fa0fc2351ef613841f79586e5
Author: Thomas Meyer <thomas@m3y3r.de>
Date:   Sat Oct 7 16:02:21 2017 +0200

    f2fs: Fix bool initialization/comparison
    
    Bool initializations should use true and false. Bool tests don't need
    comparisons.
    
    Signed-off-by: Thomas Meyer <thomas@m3y3r.de>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 030fb5cf2b51..60a07fb26475 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -418,8 +418,8 @@ int f2fs_submit_page_write(struct f2fs_io_info *fio)
 
 	bio_page = fio->encrypted_page ? fio->encrypted_page : fio->page;
 
-	/* set submitted = 1 as a return value */
-	fio->submitted = 1;
+	/* set submitted = true as a return value */
+	fio->submitted = true;
 
 	inc_page_count(sbi, WB_DATA_TYPE(bio_page));
 

commit 39d787bec4f792e69e24b11aa3d61ae1c0e4830b
Author: Chao Yu <yuchao0@huawei.com>
Date:   Fri Sep 29 13:59:38 2017 +0800

    f2fs: enhance multiple device flush
    
    When multiple device feature is enabled, during ->fsync we will issue
    flush in all devices to make sure node/data of the file being persisted
    into storage. But some flushes of device could be unneeded as file's
    data may be not writebacked into those devices. So this patch adds and
    manage bitmap per inode in global cache to indicate which device is
    dirty and it needs to issue flush during ->fsync, hence, we could improve
    performance of fsync in scenario of multiple device.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index a6288e276ff9..030fb5cf2b51 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1498,6 +1498,7 @@ static int __write_data_page(struct page *page, bool *submitted,
 	int err = 0;
 	struct f2fs_io_info fio = {
 		.sbi = sbi,
+		.ino = inode->i_ino,
 		.type = DATA,
 		.op = REQ_OP_WRITE,
 		.op_flags = wbc_to_write_flags(wbc),

commit 71ad682c1c295f1ba4eec7478384138b6ad4dab2
Author: Weichao Guo <guoweichao@huawei.com>
Date:   Fri Sep 29 22:43:23 2017 +0800

    f2fs: convert inline data for direct I/O & FI_NO_PREALLOC
    
    In FI_NO_PREALLOC cases, direct I/O path may allocate blocks for an
    inode but keep its inline data flag. This inconsistency may trigger
    vfs clear_inode nrpages bug_on when evicting the inode. We should
    convert inline data first in this case.
    
    Signed-off-by: Weichao Guo <guoweichao@huawei.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 9b68fb7eb5f1..a6288e276ff9 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -833,6 +833,13 @@ int f2fs_preallocate_blocks(struct kiocb *iocb, struct iov_iter *from)
 	struct f2fs_map_blocks map;
 	int err = 0;
 
+	/* convert inline data for Direct I/O*/
+	if (iocb->ki_flags & IOCB_DIRECT) {
+		err = f2fs_convert_inline_inode(inode);
+		if (err)
+			return err;
+	}
+
 	if (is_inode_flag_set(inode, FI_NO_PREALLOC))
 		return 0;
 
@@ -845,15 +852,11 @@ int f2fs_preallocate_blocks(struct kiocb *iocb, struct iov_iter *from)
 
 	map.m_next_pgofs = NULL;
 
-	if (iocb->ki_flags & IOCB_DIRECT) {
-		err = f2fs_convert_inline_inode(inode);
-		if (err)
-			return err;
+	if (iocb->ki_flags & IOCB_DIRECT)
 		return f2fs_map_blocks(inode, &map, 1,
 			__force_buffered_io(inode, WRITE) ?
 				F2FS_GET_BLOCK_PRE_AIO :
 				F2FS_GET_BLOCK_PRE_DIO);
-	}
 	if (iocb->ki_pos + iov_iter_count(from) > MAX_INLINE_DATA(inode)) {
 		err = f2fs_convert_inline_inode(inode);
 		if (err)

commit 71cb4afff80cee2b0f0dfda32f55e4e8d4f1153d
Author: Hsiang Kao <hsiangkao@aol.com>
Date:   Sun Sep 24 02:45:42 2017 +0800

    f2fs: allow readpages with NULL file pointer
    
    Keep in line with the other Linux file system implementations
    since page_cache_sync_readahead supports NULL file pointer,
    and thus we can readahead data by f2fs itself without file opening
    (something like the btrfs behavior).
    
    Signed-off-by: Gao Xiang <gaoxiang25@huawei.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 36b535207c88..9b68fb7eb5f1 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1334,7 +1334,7 @@ static int f2fs_read_data_pages(struct file *file,
 			struct address_space *mapping,
 			struct list_head *pages, unsigned nr_pages)
 {
-	struct inode *inode = file->f_mapping->host;
+	struct inode *inode = mapping->host;
 	struct page *page = list_last_entry(pages, struct page, lru);
 
 	trace_f2fs_readpages(inode, page, nr_pages);

commit 6d8ef53e8b2fed8b0f91df0c6da7cc92747d934a
Merge: cdb897e3279a e6c6de18f010
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Sep 12 20:05:58 2017 -0700

    Merge tag 'f2fs-for-4.14' of git://git.kernel.org/pub/scm/linux/kernel/git/jaegeuk/f2fs
    
    Pull f2fs updates from Jaegeuk Kim:
     "In this round, we've mostly tuned f2fs to provide better user
      experience for Android. Especially, we've worked on atomic write
      feature again with SQLite community in order to support it officially.
      And we added or modified several facilities to analyze and enhance IO
      behaviors.
    
      Major changes include:
       - add app/fs io stat
       - add inode checksum feature
       - support project/journalled quota
       - enhance atomic write with new ioctl() which exposes feature set
       - enhance background gc/discard/fstrim flows with new gc_urgent mode
       - add F2FS_IOC_FS{GET,SET}XATTR
       - fix some quota flows"
    
    * tag 'f2fs-for-4.14' of git://git.kernel.org/pub/scm/linux/kernel/git/jaegeuk/f2fs: (63 commits)
      f2fs: hurry up to issue discard after io interruption
      f2fs: fix to show correct discard_granularity in sysfs
      f2fs: detect dirty inode in evict_inode
      f2fs: clear radix tree dirty tag of pages whose dirty flag is cleared
      f2fs: speed up gc_urgent mode with SSR
      f2fs: better to wait for fstrim completion
      f2fs: avoid race in between read xattr & write xattr
      f2fs: make get_lock_data_page to handle encrypted inode
      f2fs: use generic terms used for encrypted block management
      f2fs: introduce f2fs_encrypted_file for clean-up
      Revert "f2fs: add a new function get_ssr_cost"
      f2fs: constify super_operations
      f2fs: fix to wake up all sleeping flusher
      f2fs: avoid race in between atomic_read & atomic_inc
      f2fs: remove unneeded parameter of change_curseg
      f2fs: update i_flags correctly
      f2fs: don't check inode's checksum if it was dirtied or writebacked
      f2fs: don't need to update inode checksum for recovery
      f2fs: trigger fdatasync for non-atomic_write file
      f2fs: fix to avoid race in between aio and gc
      ...

commit 2916ecc0f9d435d849c98f4da50e453124c87531
Author: Jérôme Glisse <jglisse@redhat.com>
Date:   Fri Sep 8 16:12:06 2017 -0700

    mm/migrate: new migrate mode MIGRATE_SYNC_NO_COPY
    
    Introduce a new migration mode that allow to offload the copy to a device
    DMA engine.  This changes the workflow of migration and not all
    address_space migratepage callback can support this.
    
    This is intended to be use by migrate_vma() which itself is use for thing
    like HMM (see include/linux/hmm.h).
    
    No additional per-filesystem migratepage testing is needed.  I disables
    MIGRATE_SYNC_NO_COPY in all problematic migratepage() callback and i
    added comment in those to explain why (part of this patch).  The commit
    message is unclear it should say that any callback that wish to support
    this new mode need to be aware of the difference in the migration flow
    from other mode.
    
    Some of these callbacks do extra locking while copying (aio, zsmalloc,
    balloon, ...) and for DMA to be effective you want to copy multiple
    pages in one DMA operations.  But in the problematic case you can not
    easily hold the extra lock accross multiple call to this callback.
    
    Usual flow is:
    
    For each page {
     1 - lock page
     2 - call migratepage() callback
     3 - (extra locking in some migratepage() callback)
     4 - migrate page state (freeze refcount, update page cache, buffer
         head, ...)
     5 - copy page
     6 - (unlock any extra lock of migratepage() callback)
     7 - return from migratepage() callback
     8 - unlock page
    }
    
    The new mode MIGRATE_SYNC_NO_COPY:
     1 - lock multiple pages
    For each page {
     2 - call migratepage() callback
     3 - abort in all problematic migratepage() callback
     4 - migrate page state (freeze refcount, update page cache, buffer
         head, ...)
    } // finished all calls to migratepage() callback
     5 - DMA copy multiple pages
     6 - unlock all the pages
    
    To support MIGRATE_SYNC_NO_COPY in the problematic case we would need a
    new callback migratepages() (for instance) that deals with multiple
    pages in one transaction.
    
    Because the problematic cases are not important for current usage I did
    not wanted to complexify this patchset even more for no good reason.
    
    Link: http://lkml.kernel.org/r/20170817000548.32038-14-jglisse@redhat.com
    Signed-off-by: Jérôme Glisse <jglisse@redhat.com>
    Cc: Aneesh Kumar <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Balbir Singh <bsingharora@gmail.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: David Nellans <dnellans@nvidia.com>
    Cc: Evgeny Baskakov <ebaskakov@nvidia.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: John Hubbard <jhubbard@nvidia.com>
    Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Mark Hairgrove <mhairgrove@nvidia.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Ross Zwisler <ross.zwisler@linux.intel.com>
    Cc: Sherry Cheung <SCheung@nvidia.com>
    Cc: Subhash Gutti <sgutti@nvidia.com>
    Cc: Vladimir Davydov <vdavydov.dev@gmail.com>
    Cc: Bob Liu <liubo95@huawei.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index a791aac4c5af..fb96bb71da00 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -2253,7 +2253,10 @@ int f2fs_migrate_page(struct address_space *mapping,
 		SetPagePrivate(newpage);
 	set_page_private(newpage, page_private(page));
 
-	migrate_page_copy(newpage, page);
+	if (mode != MIGRATE_SYNC_NO_COPY)
+		migrate_page_copy(newpage, page);
+	else
+		migrate_page_states(newpage, page);
 
 	return MIGRATEPAGE_SUCCESS;
 }

commit 13ba41e346170e594b7ce582561b3efa5b85f18f
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed Sep 6 21:04:44 2017 -0700

    f2fs: make get_lock_data_page to handle encrypted inode
    
    This patch refactors get_lock_data_page() to handle encryption case directly.
    In order to do that, it introduces common f2fs_submit_page_read().
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index ee6801fdbdec..95f30f0000b6 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -456,6 +456,53 @@ int f2fs_submit_page_write(struct f2fs_io_info *fio)
 	return err;
 }
 
+static struct bio *f2fs_grab_read_bio(struct inode *inode, block_t blkaddr,
+							 unsigned nr_pages)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct fscrypt_ctx *ctx = NULL;
+	struct bio *bio;
+
+	if (f2fs_encrypted_file(inode)) {
+		ctx = fscrypt_get_ctx(inode, GFP_NOFS);
+		if (IS_ERR(ctx))
+			return ERR_CAST(ctx);
+
+		/* wait the page to be moved by cleaning */
+		f2fs_wait_on_block_writeback(sbi, blkaddr);
+	}
+
+	bio = bio_alloc(GFP_KERNEL, min_t(int, nr_pages, BIO_MAX_PAGES));
+	if (!bio) {
+		if (ctx)
+			fscrypt_release_ctx(ctx);
+		return ERR_PTR(-ENOMEM);
+	}
+	f2fs_target_device(sbi, blkaddr, bio);
+	bio->bi_end_io = f2fs_read_end_io;
+	bio->bi_private = ctx;
+	bio_set_op_attrs(bio, REQ_OP_READ, 0);
+
+	return bio;
+}
+
+/* This can handle encryption stuffs */
+static int f2fs_submit_page_read(struct inode *inode, struct page *page,
+							block_t blkaddr)
+{
+	struct bio *bio = f2fs_grab_read_bio(inode, blkaddr, 1);
+
+	if (IS_ERR(bio))
+		return PTR_ERR(bio);
+
+	if (bio_add_page(bio, page, PAGE_SIZE, 0) < PAGE_SIZE) {
+		bio_put(bio);
+		return -EFAULT;
+	}
+	__submit_bio(F2FS_I_SB(inode), bio, DATA);
+	return 0;
+}
+
 static void __set_data_blkaddr(struct dnode_of_data *dn)
 {
 	struct f2fs_node *rn = F2FS_NODE(dn->node_page);
@@ -573,16 +620,6 @@ struct page *get_read_data_page(struct inode *inode, pgoff_t index,
 	struct page *page;
 	struct extent_info ei = {0,0,0};
 	int err;
-	struct f2fs_io_info fio = {
-		.sbi = F2FS_I_SB(inode),
-		.type = DATA,
-		.op = REQ_OP_READ,
-		.op_flags = op_flags,
-		.encrypted_page = NULL,
-	};
-
-	if (f2fs_encrypted_file(inode))
-		return read_mapping_page(mapping, index, NULL);
 
 	page = f2fs_grab_cache_page(mapping, index, for_write);
 	if (!page)
@@ -623,9 +660,7 @@ struct page *get_read_data_page(struct inode *inode, pgoff_t index,
 		return page;
 	}
 
-	fio.new_blkaddr = fio.old_blkaddr = dn.data_blkaddr;
-	fio.page = page;
-	err = f2fs_submit_page_bio(&fio);
+	err = f2fs_submit_page_read(inode, page, dn.data_blkaddr);
 	if (err)
 		goto put_err;
 	return page;
@@ -1150,35 +1185,6 @@ int f2fs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,
 	return ret;
 }
 
-static struct bio *f2fs_grab_bio(struct inode *inode, block_t blkaddr,
-				 unsigned nr_pages)
-{
-	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	struct fscrypt_ctx *ctx = NULL;
-	struct bio *bio;
-
-	if (f2fs_encrypted_file(inode)) {
-		ctx = fscrypt_get_ctx(inode, GFP_NOFS);
-		if (IS_ERR(ctx))
-			return ERR_CAST(ctx);
-
-		/* wait the page to be moved by cleaning */
-		f2fs_wait_on_block_writeback(sbi, blkaddr);
-	}
-
-	bio = bio_alloc(GFP_KERNEL, min_t(int, nr_pages, BIO_MAX_PAGES));
-	if (!bio) {
-		if (ctx)
-			fscrypt_release_ctx(ctx);
-		return ERR_PTR(-ENOMEM);
-	}
-	f2fs_target_device(sbi, blkaddr, bio);
-	bio->bi_end_io = f2fs_read_end_io;
-	bio->bi_private = ctx;
-
-	return bio;
-}
-
 /*
  * This function was originally taken from fs/mpage.c, and customized for f2fs.
  * Major change was from block_size == page_size in f2fs by default.
@@ -1275,12 +1281,11 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 			bio = NULL;
 		}
 		if (bio == NULL) {
-			bio = f2fs_grab_bio(inode, block_nr, nr_pages);
+			bio = f2fs_grab_read_bio(inode, block_nr, nr_pages);
 			if (IS_ERR(bio)) {
 				bio = NULL;
 				goto set_error_page;
 			}
-			bio_set_op_attrs(bio, REQ_OP_READ, 0);
 		}
 
 		if (bio_add_page(bio, page, blocksize, 0) < blocksize)
@@ -1989,21 +1994,9 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 		zero_user_segment(page, 0, PAGE_SIZE);
 		SetPageUptodate(page);
 	} else {
-		struct bio *bio;
-
-		bio = f2fs_grab_bio(inode, blkaddr, 1);
-		if (IS_ERR(bio)) {
-			err = PTR_ERR(bio);
-			goto fail;
-		}
-		bio->bi_opf = REQ_OP_READ;
-		if (bio_add_page(bio, page, PAGE_SIZE, 0) < PAGE_SIZE) {
-			bio_put(bio);
-			err = -EFAULT;
+		err = f2fs_submit_page_read(inode, page, blkaddr);
+		if (err)
 			goto fail;
-		}
-
-		__submit_bio(sbi, bio, DATA);
 
 		lock_page(page);
 		if (unlikely(page->mapping != mapping)) {

commit d4c759ee5faa51e0b0ee55d8a229ba5b80c4917e
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Tue Sep 5 17:04:35 2017 -0700

    f2fs: use generic terms used for encrypted block management
    
    This patch renames functions regarding to buffer management via META_MAPPING
    used for encrypted blocks especially. We can actually use them in generic way.
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index e6c683e7a10e..ee6801fdbdec 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1163,7 +1163,7 @@ static struct bio *f2fs_grab_bio(struct inode *inode, block_t blkaddr,
 			return ERR_CAST(ctx);
 
 		/* wait the page to be moved by cleaning */
-		f2fs_wait_on_encrypted_page_writeback(sbi, blkaddr);
+		f2fs_wait_on_block_writeback(sbi, blkaddr);
 	}
 
 	bio = bio_alloc(GFP_KERNEL, min_t(int, nr_pages, BIO_MAX_PAGES));
@@ -1349,7 +1349,7 @@ static int encrypt_one_page(struct f2fs_io_info *fio)
 		return 0;
 
 	/* wait for GCed encrypted page writeback */
-	f2fs_wait_on_encrypted_page_writeback(fio->sbi, fio->old_blkaddr);
+	f2fs_wait_on_block_writeback(fio->sbi, fio->old_blkaddr);
 
 retry_encrypt:
 	fio->encrypted_page = fscrypt_encrypt_page(inode, fio->page,
@@ -1975,7 +1975,7 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 
 	/* wait for GCed encrypted page writeback */
 	if (f2fs_encrypted_file(inode))
-		f2fs_wait_on_encrypted_page_writeback(sbi, blkaddr);
+		f2fs_wait_on_block_writeback(sbi, blkaddr);
 
 	if (len == PAGE_SIZE || PageUptodate(page))
 		return 0;

commit 1958593e4fa9fa9e3e2d03b27e72af314c2891be
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Tue Sep 5 16:54:24 2017 -0700

    f2fs: introduce f2fs_encrypted_file for clean-up
    
    This patch replaces (f2fs_encrypted_inode() && S_ISREG()) with
    f2fs_encrypted_file(), which gives no functional change.
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 67da4f6eeaa0..e6c683e7a10e 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -581,7 +581,7 @@ struct page *get_read_data_page(struct inode *inode, pgoff_t index,
 		.encrypted_page = NULL,
 	};
 
-	if (f2fs_encrypted_inode(inode) && S_ISREG(inode->i_mode))
+	if (f2fs_encrypted_file(inode))
 		return read_mapping_page(mapping, index, NULL);
 
 	page = f2fs_grab_cache_page(mapping, index, for_write);
@@ -786,7 +786,7 @@ static int __allocate_data_block(struct dnode_of_data *dn)
 
 static inline bool __force_buffered_io(struct inode *inode, int rw)
 {
-	return ((f2fs_encrypted_inode(inode) && S_ISREG(inode->i_mode)) ||
+	return (f2fs_encrypted_file(inode) ||
 			(rw == WRITE && test_opt(F2FS_I_SB(inode), LFS)) ||
 			F2FS_I_SB(inode)->s_ndevs);
 }
@@ -1157,7 +1157,7 @@ static struct bio *f2fs_grab_bio(struct inode *inode, block_t blkaddr,
 	struct fscrypt_ctx *ctx = NULL;
 	struct bio *bio;
 
-	if (f2fs_encrypted_inode(inode) && S_ISREG(inode->i_mode)) {
+	if (f2fs_encrypted_file(inode)) {
 		ctx = fscrypt_get_ctx(inode, GFP_NOFS);
 		if (IS_ERR(ctx))
 			return ERR_CAST(ctx);
@@ -1345,7 +1345,7 @@ static int encrypt_one_page(struct f2fs_io_info *fio)
 	struct inode *inode = fio->page->mapping->host;
 	gfp_t gfp_flags = GFP_NOFS;
 
-	if (!f2fs_encrypted_inode(inode) || !S_ISREG(inode->i_mode))
+	if (!f2fs_encrypted_file(inode))
 		return 0;
 
 	/* wait for GCed encrypted page writeback */
@@ -1974,7 +1974,7 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 	f2fs_wait_on_page_writeback(page, DATA, false);
 
 	/* wait for GCed encrypted page writeback */
-	if (f2fs_encrypted_inode(inode) && S_ISREG(inode->i_mode))
+	if (f2fs_encrypted_file(inode))
 		f2fs_wait_on_encrypted_page_writeback(sbi, blkaddr);
 
 	if (len == PAGE_SIZE || PageUptodate(page))

commit 74d46992e0d9dee7f1f376de0d56d31614c8a17a
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Aug 23 19:10:32 2017 +0200

    block: replace bi_bdev with a gendisk pointer and partitions index
    
    This way we don't need a block_device structure to submit I/O.  The
    block_device has different life time rules from the gendisk and
    request_queue and is usually only available when the block device node
    is open.  Other callers need to explicitly create one (e.g. the lightnvm
    passthrough code, or the new nvme multipathing code).
    
    For the actual I/O path all that we need is the gendisk, which exists
    once per block device.  But given that the block layer also does
    partition remapping we additionally need a partition index, which is
    used for said remapping in generic_make_request.
    
    Note that all the block drivers generally want request_queue or
    sometimes the gendisk, so this removes a layer of indirection all
    over the stack.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 87c1f4150c64..a791aac4c5af 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -142,7 +142,7 @@ struct block_device *f2fs_target_device(struct f2fs_sb_info *sbi,
 		}
 	}
 	if (bio) {
-		bio->bi_bdev = bdev;
+		bio_set_dev(bio, bdev);
 		bio->bi_iter.bi_sector = SECTOR_FROM_BLOCK(blk_addr);
 	}
 	return bdev;
@@ -161,7 +161,8 @@ int f2fs_target_device_index(struct f2fs_sb_info *sbi, block_t blkaddr)
 static bool __same_bdev(struct f2fs_sb_info *sbi,
 				block_t blk_addr, struct bio *bio)
 {
-	return f2fs_target_device(sbi, blk_addr, NULL) == bio->bi_bdev;
+	struct block_device *b = f2fs_target_device(sbi, blk_addr, NULL);
+	return bio->bi_disk == b->bd_disk && bio->bi_partno == b->bd_partno;
 }
 
 /*

commit f2220c7f155c99392b307be3012aae976a503afe
Author: Qiuyang Sun <sunqiuyang@huawei.com>
Date:   Wed Aug 9 17:27:30 2017 +0800

    f2fs: merge equivalent flags F2FS_GET_BLOCK_[READ|DIO]
    
    Currently, the two flags F2FS_GET_BLOCK_[READ|DIO] are totally equivalent
    and can be used interchangably in all scenarios they are involved in.
    Neither of the flags is referenced in f2fs_map_blocks(), making them both
    the default case. To remove the ambiguity, this patch merges both flags
    into F2FS_GET_BLOCK_DEFAULT, and introduces an enum for all distinct flags.
    
    Signed-off-by: Qiuyang Sun <sunqiuyang@huawei.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index c43262dc36de..67da4f6eeaa0 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1044,7 +1044,7 @@ static int get_data_block_dio(struct inode *inode, sector_t iblock,
 			struct buffer_head *bh_result, int create)
 {
 	return __get_data_block(inode, iblock, bh_result, create,
-						F2FS_GET_BLOCK_DIO, NULL);
+						F2FS_GET_BLOCK_DEFAULT, NULL);
 }
 
 static int get_data_block_bmap(struct inode *inode, sector_t iblock,
@@ -1244,7 +1244,7 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 			map.m_len = last_block - block_in_file;
 
 			if (f2fs_map_blocks(inode, &map, 0,
-						F2FS_GET_BLOCK_READ))
+						F2FS_GET_BLOCK_DEFAULT))
 				goto set_error_page;
 		}
 got_it:

commit b0af6d491a6b5f5622fa91ac75f34f3640f862c4
Author: Chao Yu <yuchao0@huawei.com>
Date:   Wed Aug 2 23:21:48 2017 +0800

    f2fs: add app/fs io stat
    
    This patch enables inner app/fs io stats and introduces below virtual fs
    nodes for exposing stats info:
    /sys/fs/f2fs/<dev>/iostat_enable
    /proc/fs/f2fs/<dev>/iostat_info
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    [Jaegeuk Kim: fix wrong stat assignment]
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index aefc2a5745d3..c43262dc36de 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1475,7 +1475,8 @@ int do_write_data_page(struct f2fs_io_info *fio)
 }
 
 static int __write_data_page(struct page *page, bool *submitted,
-				struct writeback_control *wbc)
+				struct writeback_control *wbc,
+				enum iostat_type io_type)
 {
 	struct inode *inode = page->mapping->host;
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
@@ -1496,6 +1497,7 @@ static int __write_data_page(struct page *page, bool *submitted,
 		.encrypted_page = NULL,
 		.submitted = false,
 		.need_lock = LOCK_RETRY,
+		.io_type = io_type,
 	};
 
 	trace_f2fs_writepage(page, DATA);
@@ -1602,7 +1604,7 @@ static int __write_data_page(struct page *page, bool *submitted,
 static int f2fs_write_data_page(struct page *page,
 					struct writeback_control *wbc)
 {
-	return __write_data_page(page, NULL, wbc);
+	return __write_data_page(page, NULL, wbc, FS_DATA_IO);
 }
 
 /*
@@ -1611,7 +1613,8 @@ static int f2fs_write_data_page(struct page *page,
  * warm/hot data page.
  */
 static int f2fs_write_cache_pages(struct address_space *mapping,
-					struct writeback_control *wbc)
+					struct writeback_control *wbc,
+					enum iostat_type io_type)
 {
 	int ret = 0;
 	int done = 0;
@@ -1701,7 +1704,7 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 			if (!clear_page_dirty_for_io(page))
 				goto continue_unlock;
 
-			ret = __write_data_page(page, &submitted, wbc);
+			ret = __write_data_page(page, &submitted, wbc, io_type);
 			if (unlikely(ret)) {
 				/*
 				 * keep nr_to_write, since vfs uses this to
@@ -1756,8 +1759,9 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 	return ret;
 }
 
-static int f2fs_write_data_pages(struct address_space *mapping,
-			    struct writeback_control *wbc)
+int __f2fs_write_data_pages(struct address_space *mapping,
+						struct writeback_control *wbc,
+						enum iostat_type io_type)
 {
 	struct inode *inode = mapping->host;
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
@@ -1794,7 +1798,7 @@ static int f2fs_write_data_pages(struct address_space *mapping,
 		goto skip_write;
 
 	blk_start_plug(&plug);
-	ret = f2fs_write_cache_pages(mapping, wbc);
+	ret = f2fs_write_cache_pages(mapping, wbc, io_type);
 	blk_finish_plug(&plug);
 
 	if (wbc->sync_mode == WB_SYNC_ALL)
@@ -1813,6 +1817,16 @@ static int f2fs_write_data_pages(struct address_space *mapping,
 	return 0;
 }
 
+static int f2fs_write_data_pages(struct address_space *mapping,
+			    struct writeback_control *wbc)
+{
+	struct inode *inode = mapping->host;
+
+	return __f2fs_write_data_pages(mapping, wbc,
+			F2FS_I(inode)->cp_task == current ?
+			FS_CP_DATA_IO : FS_DATA_IO);
+}
+
 static void f2fs_write_failed(struct address_space *mapping, loff_t to)
 {
 	struct inode *inode = mapping->host;
@@ -2079,10 +2093,13 @@ static ssize_t f2fs_direct_IO(struct kiocb *iocb, struct iov_iter *iter)
 	up_read(&F2FS_I(inode)->dio_rwsem[rw]);
 
 	if (rw == WRITE) {
-		if (err > 0)
+		if (err > 0) {
+			f2fs_update_iostat(F2FS_I_SB(inode), APP_DIRECT_IO,
+									err);
 			set_inode_flag(inode, FI_UPDATE_WRITE);
-		else if (err < 0)
+		} else if (err < 0) {
 			f2fs_write_failed(mapping, offset + count);
+		}
 	}
 
 	trace_f2fs_direct_IO_exit(inode, offset, count, rw, err);

commit 7a2af766af15887754f7f7a0869b4603b390876a
Author: Chao Yu <yuchao0@huawei.com>
Date:   Wed Jul 19 00:19:06 2017 +0800

    f2fs: enhance on-disk inode structure scalability
    
    This patch add new flag F2FS_EXTRA_ATTR storing in inode.i_inline
    to indicate that on-disk structure of current inode is extended.
    
    In order to extend, we changed the inode structure a bit:
    
    Original one:
    
    struct f2fs_inode {
            ...
            struct f2fs_extent i_ext;
            __le32 i_addr[DEF_ADDRS_PER_INODE];
            __le32 i_nid[DEF_NIDS_PER_INODE];
    }
    
    Extended one:
    
    struct f2fs_inode {
            ...
            struct f2fs_extent i_ext;
            union {
                    struct {
                            __le16 i_extra_isize;
                            __le16 i_padding;
                            __le32 i_extra_end[0];
                    };
                    __le32 i_addr[DEF_ADDRS_PER_INODE];
            };
            __le32 i_nid[DEF_NIDS_PER_INODE];
    }
    
    Once F2FS_EXTRA_ATTR is set, we will steal four bytes in the head of
    i_addr field for storing i_extra_isize and i_padding. with i_extra_isize,
    we can calculate actual size of reserved space in i_addr, available
    attribute fields included in total extra attribute fields for current
    inode can be described as below:
    
      +--------------------+
      | .i_mode            |
      | ...                |
      | .i_ext             |
      +--------------------+
      | .i_extra_isize     |-----+
      | .i_padding         |     |
      | .i_prjid           |     |
      | .i_atime_extra     |     |
      | .i_ctime_extra     |     |
      | .i_mtime_extra     |<----+
      | .i_inode_cs        |<----- store blkaddr/inline from here
      | .i_xattr_cs        |
      | ...                |
      +--------------------+
      |                    |
      |    block address   |
      |                    |
      +--------------------+
      | .i_nid             |
      +--------------------+
      |   node_footer      |
      | (nid, ino, offset) |
      +--------------------+
    
    Hence, with this patch, we would enhance scalability of f2fs inode for
    storing more newly added attribute.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index ca978c32ae00..aefc2a5745d3 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -460,10 +460,14 @@ static void __set_data_blkaddr(struct dnode_of_data *dn)
 {
 	struct f2fs_node *rn = F2FS_NODE(dn->node_page);
 	__le32 *addr_array;
+	int base = 0;
+
+	if (IS_INODE(dn->node_page) && f2fs_has_extra_attr(dn->inode))
+		base = get_extra_isize(dn->inode);
 
 	/* Get physical address of data block */
 	addr_array = blkaddr_in_node(rn);
-	addr_array[dn->ofs_in_node] = cpu_to_le32(dn->data_blkaddr);
+	addr_array[base + dn->ofs_in_node] = cpu_to_le32(dn->data_blkaddr);
 }
 
 /*
@@ -507,8 +511,8 @@ int reserve_new_blocks(struct dnode_of_data *dn, blkcnt_t count)
 	f2fs_wait_on_page_writeback(dn->node_page, NODE, true);
 
 	for (; count > 0; dn->ofs_in_node++) {
-		block_t blkaddr =
-			datablock_addr(dn->node_page, dn->ofs_in_node);
+		block_t blkaddr = datablock_addr(dn->inode,
+					dn->node_page, dn->ofs_in_node);
 		if (blkaddr == NULL_ADDR) {
 			dn->data_blkaddr = NEW_ADDR;
 			__set_data_blkaddr(dn);
@@ -755,7 +759,8 @@ static int __allocate_data_block(struct dnode_of_data *dn)
 	if (unlikely(is_inode_flag_set(dn->inode, FI_NO_ALLOC)))
 		return -EPERM;
 
-	dn->data_blkaddr = datablock_addr(dn->node_page, dn->ofs_in_node);
+	dn->data_blkaddr = datablock_addr(dn->inode,
+				dn->node_page, dn->ofs_in_node);
 	if (dn->data_blkaddr == NEW_ADDR)
 		goto alloc;
 
@@ -902,7 +907,7 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 	end_offset = ADDRS_PER_PAGE(dn.node_page, inode);
 
 next_block:
-	blkaddr = datablock_addr(dn.node_page, dn.ofs_in_node);
+	blkaddr = datablock_addr(dn.inode, dn.node_page, dn.ofs_in_node);
 
 	if (blkaddr == NEW_ADDR || blkaddr == NULL_ADDR) {
 		if (create) {

commit f247037120ecd3dcbbc196b51ded8b57edf4904f
Author: Chao Yu <yuchao0@huawei.com>
Date:   Wed Jul 19 00:19:05 2017 +0800

    f2fs: make max inline size changeable
    
    This patch tries to make below macros calculating max inline size,
    inline dentry field size considerring reserving size-changeable
    space:
    - MAX_INLINE_DATA
    - NR_INLINE_DENTRY
    - INLINE_DENTRY_BITMAP_SIZE
    - INLINE_RESERVED_SIZE
    
    Then, when inline_{data,dentry} options is enabled, it allows us to
    reserve inline space with different size flexibly for adding newly
    introduced inode attribute.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 87c1f4150c64..ca978c32ae00 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -813,7 +813,7 @@ int f2fs_preallocate_blocks(struct kiocb *iocb, struct iov_iter *from)
 				F2FS_GET_BLOCK_PRE_AIO :
 				F2FS_GET_BLOCK_PRE_DIO);
 	}
-	if (iocb->ki_pos + iov_iter_count(from) > MAX_INLINE_DATA) {
+	if (iocb->ki_pos + iov_iter_count(from) > MAX_INLINE_DATA(inode)) {
 		err = f2fs_convert_inline_inode(inode);
 		if (err)
 			return err;
@@ -1857,7 +1857,7 @@ static int prepare_write_begin(struct f2fs_sb_info *sbi,
 	set_new_dnode(&dn, inode, ipage, ipage, 0);
 
 	if (f2fs_has_inline_data(inode)) {
-		if (pos + len <= MAX_INLINE_DATA) {
+		if (pos + len <= MAX_INLINE_DATA(inode)) {
 			read_inline_data(page, ipage);
 			set_inode_flag(inode, FI_DATA_EXIST);
 			if (inode->i_nlink)

commit 5cdd4c046864827e7ac140eed081c6768a4dbb16
Merge: 7cee9384cb3e 0abd675e97e6
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 10 14:29:45 2017 -0700

    Merge tag 'for-f2fs-4.13' of git://git.kernel.org/pub/scm/linux/kernel/git/jaegeuk/f2fs
    
    Pull f2fs updates from Jaegeuk Kim:
     "In this round, we've added new features such as disk quota and statx,
      and modified internal bio management flow to merge more IOs depending
      on block types. We've also made internal threads freezeable for
      Android battery life. In addition to them, there are some patches to
      avoid lock contention as well as a couple of deadlock conditions.
    
      Enhancements:
       - support usrquota, grpquota, and statx
       - manage DATA/NODE typed bios separately to serialize more IOs
       - modify f2fs_lock_op/wio_mutex to avoid lock contention
       - prevent lock contention in migratepage
    
      Bug fixes:
       - fix missing load of written inode flag
       - fix worst case victim selection in GC
       - freezeable GC and discard threads for Android battery life
       - sanitize f2fs metadata to deal with security hole
       - clean up sysfs-related code and docs"
    
    * tag 'for-f2fs-4.13' of git://git.kernel.org/pub/scm/linux/kernel/git/jaegeuk/f2fs: (59 commits)
      f2fs: support plain user/group quota
      f2fs: avoid deadlock caused by lock order of page and lock_op
      f2fs: use spin_{,un}lock_irq{save,restore}
      f2fs: relax migratepage for atomic written page
      f2fs: don't count inode block in in-memory inode.i_blocks
      Revert "f2fs: fix to clean previous mount option when remount_fs"
      f2fs: do not set LOST_PINO for renamed dir
      f2fs: do not set LOST_PINO for newly created dir
      f2fs: skip ->writepages for {mete,node}_inode during recovery
      f2fs: introduce __check_sit_bitmap
      f2fs: stop gc/discard thread in prior during umount
      f2fs: introduce reserved_blocks in sysfs
      f2fs: avoid redundant f2fs_flush after remount
      f2fs: report # of free inodes more precisely
      f2fs: add ioctl to do gc with target block address
      f2fs: don't need to check encrypted inode for partial truncation
      f2fs: measure inode.i_blocks as generic filesystem
      f2fs: set CP_TRIMMED_FLAG correctly
      f2fs: require key for truncate(2) of encrypted file
      f2fs: move sysfs code from super.c to fs/f2fs/sysfs.c
      ...

commit 0abd675e97e60d40e61d59532f8118b0e439034e
Author: Chao Yu <yuchao0@huawei.com>
Date:   Sun Jul 9 00:13:07 2017 +0800

    f2fs: support plain user/group quota
    
    This patch adds to support plain user/group quota.
    
    Change Note by Jaegeuk Kim.
    
    - Use f2fs page cache for quota files in order to consider garbage collection.
      so, quota files are not tolerable for sudden power-cuts, so user needs to do
      quotacheck.
    
    - setattr() calls dquot_transfer which will transfer inode->i_blocks.
      We can't reclaim that during f2fs_evict_inode(). So, we need to count
      node blocks as well in order to match i_blocks with dquot's space.
    
      Note that, Chao wrote a patch to count inode->i_blocks without inode block.
      (f2fs: don't count inode block in in-memory inode.i_blocks)
    
    - in f2fs_remount, we need to make RW in prior to dquot_resume.
    
    - handle fault_injection case during f2fs_quota_off_umount
    
    - TODO: Project quota
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 7dd5fb647d43..251356859476 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -491,14 +491,15 @@ void f2fs_update_data_blkaddr(struct dnode_of_data *dn, block_t blkaddr)
 int reserve_new_blocks(struct dnode_of_data *dn, blkcnt_t count)
 {
 	struct f2fs_sb_info *sbi = F2FS_I_SB(dn->inode);
+	int err;
 
 	if (!count)
 		return 0;
 
 	if (unlikely(is_inode_flag_set(dn->inode, FI_NO_ALLOC)))
 		return -EPERM;
-	if (unlikely(!inc_valid_block_count(sbi, dn->inode, &count)))
-		return -ENOSPC;
+	if (unlikely((err = inc_valid_block_count(sbi, dn->inode, &count))))
+		return err;
 
 	trace_f2fs_reserve_new_blocks(dn->inode, dn->nid,
 						dn->ofs_in_node, count);
@@ -749,6 +750,7 @@ static int __allocate_data_block(struct dnode_of_data *dn)
 	struct node_info ni;
 	pgoff_t fofs;
 	blkcnt_t count = 1;
+	int err;
 
 	if (unlikely(is_inode_flag_set(dn->inode, FI_NO_ALLOC)))
 		return -EPERM;
@@ -757,8 +759,8 @@ static int __allocate_data_block(struct dnode_of_data *dn)
 	if (dn->data_blkaddr == NEW_ADDR)
 		goto alloc;
 
-	if (unlikely(!inc_valid_block_count(sbi, dn->inode, &count)))
-		return -ENOSPC;
+	if (unlikely((err = inc_valid_block_count(sbi, dn->inode, &count))))
+		return err;
 
 alloc:
 	get_node_info(sbi, dn->nid, &ni);

commit d29460e5cfc9bc2241886f9f60d0650ad745cf10
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed Jun 21 17:52:39 2017 -0700

    f2fs: avoid deadlock caused by lock order of page and lock_op
    
    - punch_hole
     - fill_zero
      - f2fs_lock_op
      - get_new_data_page
       - lock_page
    
    - f2fs_write_data_pages
     - lock_page
     - do_write_data_page
      - f2fs_lock_op
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 72fc866cad19..7dd5fb647d43 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1404,8 +1404,9 @@ int do_write_data_page(struct f2fs_io_info *fio)
 		}
 	}
 
-	if (fio->need_lock == LOCK_REQ)
-		f2fs_lock_op(fio->sbi);
+	/* Deadlock due to between page->lock and f2fs_lock_op */
+	if (fio->need_lock == LOCK_REQ && !f2fs_trylock_op(fio->sbi))
+		return -EAGAIN;
 
 	err = get_dnode_of_data(&dn, page->index, LOOKUP_NODE);
 	if (err)
@@ -1667,7 +1668,7 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 			}
 
 			done_index = page->index;
-
+retry_write:
 			lock_page(page);
 
 			if (unlikely(page->mapping != mapping)) {
@@ -1703,6 +1704,15 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 					unlock_page(page);
 					ret = 0;
 					continue;
+				} else if (ret == -EAGAIN) {
+					ret = 0;
+					if (wbc->sync_mode == WB_SYNC_ALL) {
+						cond_resched();
+						congestion_wait(BLK_RW_ASYNC,
+									HZ/50);
+						goto retry_write;
+					}
+					continue;
 				}
 				done_index = page->index + 1;
 				done = 1;

commit ff1048e7dffe0582a50e2eaf90e13fc76ea8493d
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Jul 6 14:46:01 2017 -0700

    f2fs: relax migratepage for atomic written page
    
    In order to avoid lock contention for atomic written pages, we'd better give
    EBUSY in f2fs_migrate_page when mode is asynchronous. We expect it will be
    released soon as transaction commits.
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index cffcfa8d2571..72fc866cad19 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -2203,8 +2203,12 @@ int f2fs_migrate_page(struct address_space *mapping,
 	BUG_ON(PageWriteback(page));
 
 	/* migrating an atomic written page is safe with the inmem_lock hold */
-	if (atomic_written && !mutex_trylock(&fi->inmem_lock))
-		return -EAGAIN;
+	if (atomic_written) {
+		if (mode != MIGRATE_SYNC)
+			return -EBUSY;
+		if (!mutex_trylock(&fi->inmem_lock))
+			return -EAGAIN;
+	}
 
 	/*
 	 * A reference is expected if PagePrivate set when move mapping,

commit 0771fcc71c0c28bf31ac5c2c863b9f0de0fdf00d
Author: Chao Yu <yuchao0@huawei.com>
Date:   Thu Jun 29 23:20:45 2017 +0800

    f2fs: skip ->writepages for {mete,node}_inode during recovery
    
    Skip ->writepages in prior to ->writepage for {meta,node}_inode during
    recovery, hence unneeded loop in ->writepages can be avoided.
    
    Moreover, check SBI_POR_DOING earlier while writebacking pages.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 7d3af48d34a9..cffcfa8d2571 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1492,6 +1492,9 @@ static int __write_data_page(struct page *page, bool *submitted,
 
 	trace_f2fs_writepage(page, DATA);
 
+	if (unlikely(is_sbi_flag_set(sbi, SBI_POR_DOING)))
+		goto redirty_out;
+
 	if (page->index < end_index)
 		goto write;
 
@@ -1505,8 +1508,6 @@ static int __write_data_page(struct page *page, bool *submitted,
 
 	zero_user_segment(page, offset, PAGE_SIZE);
 write:
-	if (unlikely(is_sbi_flag_set(sbi, SBI_POR_DOING)))
-		goto redirty_out;
 	if (f2fs_is_drop_cache(inode))
 		goto out;
 	/* we should not write 0'th page having journal header */
@@ -1754,6 +1755,10 @@ static int f2fs_write_data_pages(struct address_space *mapping,
 	if (!get_dirty_pages(inode) && wbc->sync_mode == WB_SYNC_NONE)
 		return 0;
 
+	/* during POR, we don't need to trigger writepage at all. */
+	if (unlikely(is_sbi_flag_set(sbi, SBI_POR_DOING)))
+		goto skip_write;
+
 	if (S_ISDIR(inode->i_mode) && wbc->sync_mode == WB_SYNC_NONE &&
 			get_dirty_pages(inode) < nr_pages_to_skip(sbi, DATA) &&
 			available_free_memory(sbi, DIRTY_DENTS))
@@ -1763,10 +1768,6 @@ static int f2fs_write_data_pages(struct address_space *mapping,
 	if (is_inode_flag_set(inode, FI_DO_DEFRAG))
 		goto skip_write;
 
-	/* during POR, we don't need to trigger writepage at all. */
-	if (unlikely(is_sbi_flag_set(sbi, SBI_POR_DOING)))
-		goto skip_write;
-
 	trace_f2fs_writepages(mapping->host, wbc, DATA);
 
 	/* to avoid spliting IOs due to mixed WB_SYNC_ALL and WB_SYNC_NONE */

commit 5a3a2d83cda82df7f8c306df85647d2c368e829a
Author: Qiuyang Sun <sunqiuyang@huawei.com>
Date:   Thu May 18 11:06:45 2017 +0800

    f2fs: dax: fix races between page faults and truncating pages
    
    Currently in F2FS, page faults and operations that truncate the pagecahe
    or data blocks, are completely unsynchronized. This can result in page
    fault faulting in a page into a range that we are changing after
    truncating, and thus we can end up with a page mapped to disk blocks that
    will be shortly freed. Filesystem corruption will shortly follow.
    
    This patch fixes the problem by creating new rw semaphore i_mmap_sem in
    f2fs_inode_info and grab it for functions removing blocks from extent tree
    and for read over page faults. The mechanism is similar to that in ext4.
    
    Signed-off-by: Qiuyang Sun <sunqiuyang@huawei.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 2ed90f5db832..7d3af48d34a9 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1801,8 +1801,10 @@ static void f2fs_write_failed(struct address_space *mapping, loff_t to)
 	loff_t i_size = i_size_read(inode);
 
 	if (to > i_size) {
+		down_write(&F2FS_I(inode)->i_mmap_sem);
 		truncate_pagecache(inode, i_size);
 		truncate_blocks(inode, i_size, true);
+		up_write(&F2FS_I(inode)->i_mmap_sem);
 	}
 }
 

commit 4e4cbee93d56137ebff722be022cae5f70ef84fb
Author: Christoph Hellwig <hch@lst.de>
Date:   Sat Jun 3 09:38:06 2017 +0200

    block: switch bios to blk_status_t
    
    Replace bi_error with a new bi_status to allow for a clear conversion.
    Note that device mapper overloaded bi_error with a private value, which
    we'll have to keep arround at least for now and thus propagate to a
    proper blk_status_t value.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 7c0f6bdf817d..36fe82012a33 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -58,12 +58,12 @@ static void f2fs_read_end_io(struct bio *bio)
 #ifdef CONFIG_F2FS_FAULT_INJECTION
 	if (time_to_inject(F2FS_P_SB(bio->bi_io_vec->bv_page), FAULT_IO)) {
 		f2fs_show_injection_info(FAULT_IO);
-		bio->bi_error = -EIO;
+		bio->bi_status = BLK_STS_IOERR;
 	}
 #endif
 
 	if (f2fs_bio_encrypted(bio)) {
-		if (bio->bi_error) {
+		if (bio->bi_status) {
 			fscrypt_release_ctx(bio->bi_private);
 		} else {
 			fscrypt_decrypt_bio_pages(bio->bi_private, bio);
@@ -74,7 +74,7 @@ static void f2fs_read_end_io(struct bio *bio)
 	bio_for_each_segment_all(bvec, bio, i) {
 		struct page *page = bvec->bv_page;
 
-		if (!bio->bi_error) {
+		if (!bio->bi_status) {
 			if (!PageUptodate(page))
 				SetPageUptodate(page);
 		} else {
@@ -102,14 +102,14 @@ static void f2fs_write_end_io(struct bio *bio)
 			unlock_page(page);
 			mempool_free(page, sbi->write_io_dummy);
 
-			if (unlikely(bio->bi_error))
+			if (unlikely(bio->bi_status))
 				f2fs_stop_checkpoint(sbi, true);
 			continue;
 		}
 
 		fscrypt_pullback_bio_page(&page, true);
 
-		if (unlikely(bio->bi_error)) {
+		if (unlikely(bio->bi_status)) {
 			mapping_set_error(page->mapping, -EIO);
 			f2fs_stop_checkpoint(sbi, true);
 		}

commit fb830fc5cfc90ba8236921aacb72c6d70bf78af7
Author: Chao Yu <yuchao0@huawei.com>
Date:   Fri May 19 23:37:01 2017 +0800

    f2fs: introduce io_list for serialize data/node IOs
    
    Serialize data/node IOs by using fifo list instead of mutex lock,
    it will help to enhance concurrency of f2fs, meanwhile keeping LFS
    IO semantics.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 779a306858a2..2ed90f5db832 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -397,6 +397,20 @@ int f2fs_submit_page_write(struct f2fs_io_info *fio)
 
 	f2fs_bug_on(sbi, is_read_io(fio->op));
 
+	down_write(&io->io_rwsem);
+next:
+	if (fio->in_list) {
+		spin_lock(&io->io_lock);
+		if (list_empty(&io->io_list)) {
+			spin_unlock(&io->io_lock);
+			goto out_fail;
+		}
+		fio = list_first_entry(&io->io_list,
+						struct f2fs_io_info, list);
+		list_del(&fio->list);
+		spin_unlock(&io->io_lock);
+	}
+
 	if (fio->old_blkaddr != NEW_ADDR)
 		verify_block_addr(sbi, fio->old_blkaddr);
 	verify_block_addr(sbi, fio->new_blkaddr);
@@ -408,8 +422,6 @@ int f2fs_submit_page_write(struct f2fs_io_info *fio)
 
 	inc_page_count(sbi, WB_DATA_TYPE(bio_page));
 
-	down_write(&io->io_rwsem);
-
 	if (io->bio && (io->last_block_in_bio != fio->new_blkaddr - 1 ||
 	    (io->fio.op != fio->op || io->fio.op_flags != fio->op_flags) ||
 			!__same_bdev(sbi, fio->new_blkaddr, io->bio)))
@@ -434,9 +446,13 @@ int f2fs_submit_page_write(struct f2fs_io_info *fio)
 
 	io->last_block_in_bio = fio->new_blkaddr;
 	f2fs_trace_ios(fio, 0);
+
+	trace_f2fs_submit_page_write(fio->page, fio);
+
+	if (fio->in_list)
+		goto next;
 out_fail:
 	up_write(&io->io_rwsem);
-	trace_f2fs_submit_page_write(fio->page, fio);
 	return err;
 }
 
@@ -749,7 +765,7 @@ static int __allocate_data_block(struct dnode_of_data *dn)
 	set_summary(&sum, dn->nid, dn->ofs_in_node, ni.version);
 
 	allocate_data_block(sbi, NULL, dn->data_blkaddr, &dn->data_blkaddr,
-						&sum, CURSEG_WARM_DATA);
+					&sum, CURSEG_WARM_DATA, NULL, false);
 	set_data_blkaddr(dn);
 
 	/* update i_size */

commit cc15620bc826b14006956fd321e026ae96aff53a
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri May 12 13:51:34 2017 -0700

    f2fs: avoid f2fs_lock_op for IPU writes
    
    Currently, if we do get_node_of_data before f2fs_lock_op, there may be dead lock
    as follows, where process A would be in infinite loop, and B will NOT be awaked.
    
    Process A(cp):            Process B:
    f2fs_lock_all(sbi)
                            get_dnode_of_data <---- lock dn.node_page
    flush_nodes             f2fs_lock_op
    
    So, this patch adds f2fs_trylock_op to avoid f2fs_lock_op done by IPU.
    
    Signed-off-by: Hou Pengyang <houpengyang@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 4ca0899621a0..779a306858a2 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1383,12 +1383,12 @@ int do_write_data_page(struct f2fs_io_info *fio)
 
 		if (valid_ipu_blkaddr(fio)) {
 			ipu_force = true;
-			fio->need_lock = false;
+			fio->need_lock = LOCK_DONE;
 			goto got_it;
 		}
 	}
 
-	if (fio->need_lock)
+	if (fio->need_lock == LOCK_REQ)
 		f2fs_lock_op(fio->sbi);
 
 	err = get_dnode_of_data(&dn, page->index, LOOKUP_NODE);
@@ -1403,19 +1403,18 @@ int do_write_data_page(struct f2fs_io_info *fio)
 		goto out_writepage;
 	}
 got_it:
-	err = encrypt_one_page(fio);
-	if (err)
-		goto out_writepage;
-
-	set_page_writeback(page);
-
 	/*
 	 * If current allocation needs SSR,
 	 * it had better in-place writes for updated data.
 	 */
 	if (ipu_force || (valid_ipu_blkaddr(fio) && need_inplace_update(fio))) {
+		err = encrypt_one_page(fio);
+		if (err)
+			goto out_writepage;
+
+		set_page_writeback(page);
 		f2fs_put_dnode(&dn);
-		if (fio->need_lock)
+		if (fio->need_lock == LOCK_REQ)
 			f2fs_unlock_op(fio->sbi);
 		err = rewrite_data_page(fio);
 		trace_f2fs_do_write_data_page(fio->page, IPU);
@@ -1423,6 +1422,20 @@ int do_write_data_page(struct f2fs_io_info *fio)
 		return err;
 	}
 
+	if (fio->need_lock == LOCK_RETRY) {
+		if (!f2fs_trylock_op(fio->sbi)) {
+			err = -EAGAIN;
+			goto out_writepage;
+		}
+		fio->need_lock = LOCK_REQ;
+	}
+
+	err = encrypt_one_page(fio);
+	if (err)
+		goto out_writepage;
+
+	set_page_writeback(page);
+
 	/* LFS mode write path */
 	write_data_page(&dn, fio);
 	trace_f2fs_do_write_data_page(page, OPU);
@@ -1432,7 +1445,7 @@ int do_write_data_page(struct f2fs_io_info *fio)
 out_writepage:
 	f2fs_put_dnode(&dn);
 out:
-	if (fio->need_lock)
+	if (fio->need_lock == LOCK_REQ)
 		f2fs_unlock_op(fio->sbi);
 	return err;
 }
@@ -1458,7 +1471,7 @@ static int __write_data_page(struct page *page, bool *submitted,
 		.page = page,
 		.encrypted_page = NULL,
 		.submitted = false,
-		.need_lock = true,
+		.need_lock = LOCK_RETRY,
 	};
 
 	trace_f2fs_writepage(page, DATA);
@@ -1494,7 +1507,7 @@ static int __write_data_page(struct page *page, bool *submitted,
 
 	/* Dentry blocks are controlled by checkpoint */
 	if (S_ISDIR(inode->i_mode)) {
-		fio.need_lock = false;
+		fio.need_lock = LOCK_DONE;
 		err = do_write_data_page(&fio);
 		goto done;
 	}
@@ -1513,8 +1526,13 @@ static int __write_data_page(struct page *page, bool *submitted,
 			goto out;
 	}
 
-	if (err == -EAGAIN)
+	if (err == -EAGAIN) {
 		err = do_write_data_page(&fio);
+		if (err == -EAGAIN) {
+			fio.need_lock = LOCK_REQ;
+			err = do_write_data_page(&fio);
+		}
+	}
 	if (F2FS_I(inode)->last_disk_size < psize)
 		F2FS_I(inode)->last_disk_size = psize;
 

commit a912b54d3aaa011266dc266e3694f782f27233cf
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed May 10 11:18:25 2017 -0700

    f2fs: split bio cache
    
    Split DATA/NODE type bio cache according to different temperature,
    so write IOs with the same temperature can be merged in corresponding
    bio cache as much as possible, otherwise, different temperature write
    IOs submitting into one bio cache will always cause split of bio.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 06bb2042385e..4ca0899621a0 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -282,27 +282,32 @@ static bool has_merged_page(struct f2fs_sb_info *sbi, struct inode *inode,
 				nid_t ino, pgoff_t idx, enum page_type type)
 {
 	enum page_type btype = PAGE_TYPE_OF_BIO(type);
-	struct f2fs_bio_info *io = &sbi->write_io[btype];
-	bool ret;
+	enum temp_type temp;
+	struct f2fs_bio_info *io;
+	bool ret = false;
 
-	down_read(&io->io_rwsem);
-	ret = __has_merged_page(io, inode, ino, idx);
-	up_read(&io->io_rwsem);
+	for (temp = HOT; temp < NR_TEMP_TYPE; temp++) {
+		io = sbi->write_io[btype] + temp;
+
+		down_read(&io->io_rwsem);
+		ret = __has_merged_page(io, inode, ino, idx);
+		up_read(&io->io_rwsem);
+
+		/* TODO: use HOT temp only for meta pages now. */
+		if (ret || btype == META)
+			break;
+	}
 	return ret;
 }
 
 static void __f2fs_submit_merged_write(struct f2fs_sb_info *sbi,
-				struct inode *inode, nid_t ino, pgoff_t idx,
-				enum page_type type)
+				enum page_type type, enum temp_type temp)
 {
 	enum page_type btype = PAGE_TYPE_OF_BIO(type);
-	struct f2fs_bio_info *io = &sbi->write_io[btype];
+	struct f2fs_bio_info *io = sbi->write_io[btype] + temp;
 
 	down_write(&io->io_rwsem);
 
-	if (!__has_merged_page(io, inode, ino, idx))
-		goto out;
-
 	/* change META to META_FLUSH in the checkpoint procedure */
 	if (type >= META_FLUSH) {
 		io->fio.type = META_FLUSH;
@@ -312,21 +317,38 @@ static void __f2fs_submit_merged_write(struct f2fs_sb_info *sbi,
 			io->fio.op_flags |= REQ_PREFLUSH | REQ_FUA;
 	}
 	__submit_merged_bio(io);
-out:
 	up_write(&io->io_rwsem);
 }
 
+static void __submit_merged_write_cond(struct f2fs_sb_info *sbi,
+				struct inode *inode, nid_t ino, pgoff_t idx,
+				enum page_type type, bool force)
+{
+	enum temp_type temp;
+
+	if (!force && !has_merged_page(sbi, inode, ino, idx, type))
+		return;
+
+	for (temp = HOT; temp < NR_TEMP_TYPE; temp++) {
+
+		__f2fs_submit_merged_write(sbi, type, temp);
+
+		/* TODO: use HOT temp only for meta pages now. */
+		if (type >= META)
+			break;
+	}
+}
+
 void f2fs_submit_merged_write(struct f2fs_sb_info *sbi, enum page_type type)
 {
-	__f2fs_submit_merged_write(sbi, NULL, 0, 0, type);
+	__submit_merged_write_cond(sbi, NULL, 0, 0, type, true);
 }
 
 void f2fs_submit_merged_write_cond(struct f2fs_sb_info *sbi,
 				struct inode *inode, nid_t ino, pgoff_t idx,
 				enum page_type type)
 {
-	if (has_merged_page(sbi, inode, ino, idx, type))
-		__f2fs_submit_merged_write(sbi, inode, ino, idx, type);
+	__submit_merged_write_cond(sbi, inode, ino, idx, type, false);
 }
 
 void f2fs_flush_merged_writes(struct f2fs_sb_info *sbi)
@@ -369,7 +391,7 @@ int f2fs_submit_page_write(struct f2fs_io_info *fio)
 {
 	struct f2fs_sb_info *sbi = fio->sbi;
 	enum page_type btype = PAGE_TYPE_OF_BIO(fio->type);
-	struct f2fs_bio_info *io = &sbi->write_io[btype];
+	struct f2fs_bio_info *io = sbi->write_io[btype] + fio->temp;
 	struct page *bio_page;
 	int err = 0;
 
@@ -405,8 +427,7 @@ int f2fs_submit_page_write(struct f2fs_io_info *fio)
 		io->fio = *fio;
 	}
 
-	if (bio_add_page(io->bio, bio_page, PAGE_SIZE, 0) <
-							PAGE_SIZE) {
+	if (bio_add_page(io->bio, bio_page, PAGE_SIZE, 0) < PAGE_SIZE) {
 		__submit_merged_bio(io);
 		goto alloc_new;
 	}

commit b9109b0e49b93b0ae663330acb36561b8f4f6905
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed May 10 11:28:38 2017 -0700

    f2fs: remove unnecessary read cases in merged IO flow
    
    Merged IO flow doesn't need to care about read IOs.
    
    f2fs_submit_merged_bio -> f2fs_submit_merged_write
    f2fs_submit_merged_bios -> f2fs_submit_merged_writes
    f2fs_submit_merged_bio_cond -> f2fs_submit_merged_write_cond
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 7c0f6bdf817d..06bb2042385e 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -291,14 +291,12 @@ static bool has_merged_page(struct f2fs_sb_info *sbi, struct inode *inode,
 	return ret;
 }
 
-static void __f2fs_submit_merged_bio(struct f2fs_sb_info *sbi,
+static void __f2fs_submit_merged_write(struct f2fs_sb_info *sbi,
 				struct inode *inode, nid_t ino, pgoff_t idx,
-				enum page_type type, int rw)
+				enum page_type type)
 {
 	enum page_type btype = PAGE_TYPE_OF_BIO(type);
-	struct f2fs_bio_info *io;
-
-	io = is_read_io(rw) ? &sbi->read_io : &sbi->write_io[btype];
+	struct f2fs_bio_info *io = &sbi->write_io[btype];
 
 	down_write(&io->io_rwsem);
 
@@ -318,25 +316,24 @@ static void __f2fs_submit_merged_bio(struct f2fs_sb_info *sbi,
 	up_write(&io->io_rwsem);
 }
 
-void f2fs_submit_merged_bio(struct f2fs_sb_info *sbi, enum page_type type,
-									int rw)
+void f2fs_submit_merged_write(struct f2fs_sb_info *sbi, enum page_type type)
 {
-	__f2fs_submit_merged_bio(sbi, NULL, 0, 0, type, rw);
+	__f2fs_submit_merged_write(sbi, NULL, 0, 0, type);
 }
 
-void f2fs_submit_merged_bio_cond(struct f2fs_sb_info *sbi,
+void f2fs_submit_merged_write_cond(struct f2fs_sb_info *sbi,
 				struct inode *inode, nid_t ino, pgoff_t idx,
-				enum page_type type, int rw)
+				enum page_type type)
 {
 	if (has_merged_page(sbi, inode, ino, idx, type))
-		__f2fs_submit_merged_bio(sbi, inode, ino, idx, type, rw);
+		__f2fs_submit_merged_write(sbi, inode, ino, idx, type);
 }
 
-void f2fs_flush_merged_bios(struct f2fs_sb_info *sbi)
+void f2fs_flush_merged_writes(struct f2fs_sb_info *sbi)
 {
-	f2fs_submit_merged_bio(sbi, DATA, WRITE);
-	f2fs_submit_merged_bio(sbi, NODE, WRITE);
-	f2fs_submit_merged_bio(sbi, META, WRITE);
+	f2fs_submit_merged_write(sbi, DATA);
+	f2fs_submit_merged_write(sbi, NODE);
+	f2fs_submit_merged_write(sbi, META);
 }
 
 /*
@@ -368,16 +365,15 @@ int f2fs_submit_page_bio(struct f2fs_io_info *fio)
 	return 0;
 }
 
-int f2fs_submit_page_mbio(struct f2fs_io_info *fio)
+int f2fs_submit_page_write(struct f2fs_io_info *fio)
 {
 	struct f2fs_sb_info *sbi = fio->sbi;
 	enum page_type btype = PAGE_TYPE_OF_BIO(fio->type);
-	struct f2fs_bio_info *io;
-	bool is_read = is_read_io(fio->op);
+	struct f2fs_bio_info *io = &sbi->write_io[btype];
 	struct page *bio_page;
 	int err = 0;
 
-	io = is_read ? &sbi->read_io : &sbi->write_io[btype];
+	f2fs_bug_on(sbi, is_read_io(fio->op));
 
 	if (fio->old_blkaddr != NEW_ADDR)
 		verify_block_addr(sbi, fio->old_blkaddr);
@@ -388,8 +384,7 @@ int f2fs_submit_page_mbio(struct f2fs_io_info *fio)
 	/* set submitted = 1 as a return value */
 	fio->submitted = 1;
 
-	if (!is_read)
-		inc_page_count(sbi, WB_DATA_TYPE(bio_page));
+	inc_page_count(sbi, WB_DATA_TYPE(bio_page));
 
 	down_write(&io->io_rwsem);
 
@@ -402,12 +397,11 @@ int f2fs_submit_page_mbio(struct f2fs_io_info *fio)
 		if ((fio->type == DATA || fio->type == NODE) &&
 				fio->new_blkaddr & F2FS_IO_SIZE_MASK(sbi)) {
 			err = -EAGAIN;
-			if (!is_read)
-				dec_page_count(sbi, WB_DATA_TYPE(bio_page));
+			dec_page_count(sbi, WB_DATA_TYPE(bio_page));
 			goto out_fail;
 		}
 		io->bio = __bio_alloc(sbi, fio->new_blkaddr,
-						BIO_MAX_PAGES, is_read);
+						BIO_MAX_PAGES, false);
 		io->fio = *fio;
 	}
 
@@ -421,7 +415,7 @@ int f2fs_submit_page_mbio(struct f2fs_io_info *fio)
 	f2fs_trace_ios(fio, 0);
 out_fail:
 	up_write(&io->io_rwsem);
-	trace_f2fs_submit_page_mbio(fio->page, fio);
+	trace_f2fs_submit_page_write(fio->page, fio);
 	return err;
 }
 
@@ -1321,7 +1315,7 @@ static int encrypt_one_page(struct f2fs_io_info *fio)
 
 	/* flush pending IOs and wait for a while in the ENOMEM case */
 	if (PTR_ERR(fio->encrypted_page) == -ENOMEM) {
-		f2fs_flush_merged_bios(fio->sbi);
+		f2fs_flush_merged_writes(fio->sbi);
 		congestion_wait(BLK_RW_ASYNC, HZ/50);
 		gfp_flags |= __GFP_NOFAIL;
 		goto retry_encrypt;
@@ -1513,8 +1507,7 @@ static int __write_data_page(struct page *page, bool *submitted,
 		ClearPageUptodate(page);
 
 	if (wbc->for_reclaim) {
-		f2fs_submit_merged_bio_cond(sbi, inode, 0, page->index,
-						DATA, WRITE);
+		f2fs_submit_merged_write_cond(sbi, inode, 0, page->index, DATA);
 		clear_inode_flag(inode, FI_HOT_DATA);
 		remove_dirty_inode(inode);
 		submitted = NULL;
@@ -1525,7 +1518,7 @@ static int __write_data_page(struct page *page, bool *submitted,
 		f2fs_balance_fs(sbi, need_balance_fs);
 
 	if (unlikely(f2fs_cp_error(sbi))) {
-		f2fs_submit_merged_bio(sbi, DATA, WRITE);
+		f2fs_submit_merged_write(sbi, DATA);
 		submitted = NULL;
 	}
 
@@ -1684,8 +1677,8 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 		mapping->writeback_index = done_index;
 
 	if (last_idx != ULONG_MAX)
-		f2fs_submit_merged_bio_cond(F2FS_M_SB(mapping), mapping->host,
-						0, last_idx, DATA, WRITE);
+		f2fs_submit_merged_write_cond(F2FS_M_SB(mapping), mapping->host,
+						0, last_idx, DATA);
 
 	return ret;
 }

commit 3adc5fcb7edf5f8dfe8d37dcb50ba6b30077c905
Author: Jan Kara <jack@suse.cz>
Date:   Tue May 2 17:03:47 2017 +0200

    f2fs: Make flush bios explicitely sync
    
    Commit b685d3d65ac7 "block: treat REQ_FUA and REQ_PREFLUSH as
    synchronous" removed REQ_SYNC flag from WRITE_{FUA|PREFLUSH|...}
    definitions.  generic_make_request_checks() however strips REQ_FUA and
    REQ_PREFLUSH flags from a bio when the storage doesn't report volatile
    write cache and thus write effectively becomes asynchronous which can
    lead to performance regressions.
    
    Fix the problem by making sure all bios which are synchronous are
    properly marked with REQ_SYNC.
    
    Fixes: b685d3d65ac791406e0dfd8779cc9b3707fea5a3
    Cc: stable@vger.kernel.org # 4.9+
    CC: Jaegeuk Kim <jaegeuk@kernel.org>
    CC: linux-f2fs-devel@lists.sourceforge.net
    Signed-off-by: Jan Kara <jack@suse.cz>
    Acked-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 1254986dd6eb..7c0f6bdf817d 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -309,7 +309,7 @@ static void __f2fs_submit_merged_bio(struct f2fs_sb_info *sbi,
 	if (type >= META_FLUSH) {
 		io->fio.type = META_FLUSH;
 		io->fio.op = REQ_OP_WRITE;
-		io->fio.op_flags = REQ_META | REQ_PRIO;
+		io->fio.op_flags = REQ_META | REQ_PRIO | REQ_SYNC;
 		if (!test_opt(sbi, NOBARRIER))
 			io->fio.op_flags |= REQ_PREFLUSH | REQ_FUA;
 	}

commit 279d6df20c94079d35e012f1602d40c42632e8f3
Author: Hou Pengyang <houpengyang@huawei.com>
Date:   Thu Apr 27 00:17:21 2017 +0800

    f2fs: release cp and dnode lock before IPU
    
    We don't need to rewrite the page under cp_rwsem and dnode locks.
    
    Signed-off-by: Hou Pengyang <houpengyang@huawei.com>
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 2f2de18d24fe..1254986dd6eb 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1368,12 +1368,17 @@ int do_write_data_page(struct f2fs_io_info *fio)
 
 		if (valid_ipu_blkaddr(fio)) {
 			ipu_force = true;
+			fio->need_lock = false;
 			goto got_it;
 		}
 	}
+
+	if (fio->need_lock)
+		f2fs_lock_op(fio->sbi);
+
 	err = get_dnode_of_data(&dn, page->index, LOOKUP_NODE);
 	if (err)
-		return err;
+		goto out;
 
 	fio->old_blkaddr = dn.data_blkaddr;
 
@@ -1394,22 +1399,26 @@ int do_write_data_page(struct f2fs_io_info *fio)
 	 * it had better in-place writes for updated data.
 	 */
 	if (ipu_force || (valid_ipu_blkaddr(fio) && need_inplace_update(fio))) {
-		f2fs_bug_on(fio->sbi, !fio->cp_rwsem_locked);
-		f2fs_unlock_op(fio->sbi);
-		fio->cp_rwsem_locked = false;
-
+		f2fs_put_dnode(&dn);
+		if (fio->need_lock)
+			f2fs_unlock_op(fio->sbi);
 		err = rewrite_data_page(fio);
 		trace_f2fs_do_write_data_page(fio->page, IPU);
 		set_inode_flag(inode, FI_UPDATE_WRITE);
-	} else {
-		write_data_page(&dn, fio);
-		trace_f2fs_do_write_data_page(page, OPU);
-		set_inode_flag(inode, FI_APPEND_WRITE);
-		if (page->index == 0)
-			set_inode_flag(inode, FI_FIRST_BLOCK_WRITTEN);
+		return err;
 	}
+
+	/* LFS mode write path */
+	write_data_page(&dn, fio);
+	trace_f2fs_do_write_data_page(page, OPU);
+	set_inode_flag(inode, FI_APPEND_WRITE);
+	if (page->index == 0)
+		set_inode_flag(inode, FI_FIRST_BLOCK_WRITTEN);
 out_writepage:
 	f2fs_put_dnode(&dn);
+out:
+	if (fio->need_lock)
+		f2fs_unlock_op(fio->sbi);
 	return err;
 }
 
@@ -1434,7 +1443,7 @@ static int __write_data_page(struct page *page, bool *submitted,
 		.page = page,
 		.encrypted_page = NULL,
 		.submitted = false,
-		.cp_rwsem_locked = true,
+		.need_lock = true,
 	};
 
 	trace_f2fs_writepage(page, DATA);
@@ -1470,6 +1479,7 @@ static int __write_data_page(struct page *page, bool *submitted,
 
 	/* Dentry blocks are controlled by checkpoint */
 	if (S_ISDIR(inode->i_mode)) {
+		fio.need_lock = false;
 		err = do_write_data_page(&fio);
 		goto done;
 	}
@@ -1487,13 +1497,12 @@ static int __write_data_page(struct page *page, bool *submitted,
 		if (!err)
 			goto out;
 	}
-	f2fs_lock_op(sbi);
+
 	if (err == -EAGAIN)
 		err = do_write_data_page(&fio);
 	if (F2FS_I(inode)->last_disk_size < psize)
 		F2FS_I(inode)->last_disk_size = psize;
-	if (fio.cp_rwsem_locked)
-		f2fs_unlock_op(sbi);
+
 done:
 	if (err && err != -ENOENT)
 		goto redirty_out;

commit a817737e87d506ea7b3983d287b4578c99922d85
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Mon Apr 24 15:20:16 2017 -0700

    f2fs: introduce valid_ipu_blkaddr to clean up
    
    This patch introduces valid_ipu_blkaddr to clean up checking block address for
    inplace-update.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index ca21ecbd6bbd..2f2de18d24fe 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1333,8 +1333,6 @@ static inline bool need_inplace_update(struct f2fs_io_info *fio)
 {
 	struct inode *inode = fio->page->mapping->host;
 
-	if (fio->old_blkaddr == NEW_ADDR)
-		return false;
 	if (S_ISDIR(inode->i_mode) || f2fs_is_atomic_file(inode))
 		return false;
 	if (is_cold_data(fio->page))
@@ -1345,6 +1343,15 @@ static inline bool need_inplace_update(struct f2fs_io_info *fio)
 	return need_inplace_update_policy(inode, fio);
 }
 
+static inline bool valid_ipu_blkaddr(struct f2fs_io_info *fio)
+{
+	if (fio->old_blkaddr == NEW_ADDR)
+		return false;
+	if (fio->old_blkaddr == NULL_ADDR)
+		return false;
+	return true;
+}
+
 int do_write_data_page(struct f2fs_io_info *fio)
 {
 	struct page *page = fio->page;
@@ -1358,8 +1365,8 @@ int do_write_data_page(struct f2fs_io_info *fio)
 	if (need_inplace_update(fio) &&
 			f2fs_lookup_extent_cache(inode, page->index, &ei)) {
 		fio->old_blkaddr = ei.blk + page->index - ei.fofs;
-		if (fio->old_blkaddr != NULL_ADDR &&
-				fio->old_blkaddr != NEW_ADDR) {
+
+		if (valid_ipu_blkaddr(fio)) {
 			ipu_force = true;
 			goto got_it;
 		}
@@ -1386,7 +1393,7 @@ int do_write_data_page(struct f2fs_io_info *fio)
 	 * If current allocation needs SSR,
 	 * it had better in-place writes for updated data.
 	 */
-	if (ipu_force || need_inplace_update(fio)) {
+	if (ipu_force || (valid_ipu_blkaddr(fio) && need_inplace_update(fio))) {
 		f2fs_bug_on(fio->sbi, !fio->cp_rwsem_locked);
 		f2fs_unlock_op(fio->sbi);
 		fio->cp_rwsem_locked = false;

commit e959c8f543e11dadf7f6923427fb3acb452a0de6
Author: Hou Pengyang <houpengyang@huawei.com>
Date:   Tue Apr 25 12:45:13 2017 +0000

    f2fs: lookup extent cache first under IPU scenario
    
    If a page is cold, NOT atomit written and need_ipu now, there is
    a high probability that IPU should be adapted. For IPU, we try to
    check extent tree to get the block index first, instead of reading
    the dnode page, where may lead to an useless dnode IO, since no need to
    update the dnode index for IPU.
    
    Signed-off-by: Hou Pengyang <houpengyang@huawei.com>
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index dfc974c95dd2..ca21ecbd6bbd 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1350,9 +1350,20 @@ int do_write_data_page(struct f2fs_io_info *fio)
 	struct page *page = fio->page;
 	struct inode *inode = page->mapping->host;
 	struct dnode_of_data dn;
+	struct extent_info ei = {0,0,0};
+	bool ipu_force = false;
 	int err = 0;
 
 	set_new_dnode(&dn, inode, NULL, NULL, 0);
+	if (need_inplace_update(fio) &&
+			f2fs_lookup_extent_cache(inode, page->index, &ei)) {
+		fio->old_blkaddr = ei.blk + page->index - ei.fofs;
+		if (fio->old_blkaddr != NULL_ADDR &&
+				fio->old_blkaddr != NEW_ADDR) {
+			ipu_force = true;
+			goto got_it;
+		}
+	}
 	err = get_dnode_of_data(&dn, page->index, LOOKUP_NODE);
 	if (err)
 		return err;
@@ -1364,7 +1375,7 @@ int do_write_data_page(struct f2fs_io_info *fio)
 		ClearPageUptodate(page);
 		goto out_writepage;
 	}
-
+got_it:
 	err = encrypt_one_page(fio);
 	if (err)
 		goto out_writepage;
@@ -1375,7 +1386,7 @@ int do_write_data_page(struct f2fs_io_info *fio)
 	 * If current allocation needs SSR,
 	 * it had better in-place writes for updated data.
 	 */
-	if (need_inplace_update(fio)) {
+	if (ipu_force || need_inplace_update(fio)) {
 		f2fs_bug_on(fio->sbi, !fio->cp_rwsem_locked);
 		f2fs_unlock_op(fio->sbi);
 		fio->cp_rwsem_locked = false;
@@ -1412,6 +1423,7 @@ static int __write_data_page(struct page *page, bool *submitted,
 		.type = DATA,
 		.op = REQ_OP_WRITE,
 		.op_flags = wbc_to_write_flags(wbc),
+		.old_blkaddr = NULL_ADDR,
 		.page = page,
 		.encrypted_page = NULL,
 		.submitted = false,

commit 7eab0c0df8d1a8c460f7d660d3ffd06fd448e590
Author: Hou Pengyang <houpengyang@huawei.com>
Date:   Tue Apr 25 12:45:12 2017 +0000

    f2fs: reconstruct code to write a data page
    
    This patch introduces encrypt_one_page which encrypts one data page before
    submit_bio, and change the use of need_inplace_update.
    
    Signed-off-by: Hou Pengyang <houpengyang@huawei.com>
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index b8dcd1e224e8..dfc974c95dd2 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1302,6 +1302,49 @@ static int f2fs_read_data_pages(struct file *file,
 	return f2fs_mpage_readpages(mapping, pages, NULL, nr_pages);
 }
 
+static int encrypt_one_page(struct f2fs_io_info *fio)
+{
+	struct inode *inode = fio->page->mapping->host;
+	gfp_t gfp_flags = GFP_NOFS;
+
+	if (!f2fs_encrypted_inode(inode) || !S_ISREG(inode->i_mode))
+		return 0;
+
+	/* wait for GCed encrypted page writeback */
+	f2fs_wait_on_encrypted_page_writeback(fio->sbi, fio->old_blkaddr);
+
+retry_encrypt:
+	fio->encrypted_page = fscrypt_encrypt_page(inode, fio->page,
+			PAGE_SIZE, 0, fio->page->index, gfp_flags);
+	if (!IS_ERR(fio->encrypted_page))
+		return 0;
+
+	/* flush pending IOs and wait for a while in the ENOMEM case */
+	if (PTR_ERR(fio->encrypted_page) == -ENOMEM) {
+		f2fs_flush_merged_bios(fio->sbi);
+		congestion_wait(BLK_RW_ASYNC, HZ/50);
+		gfp_flags |= __GFP_NOFAIL;
+		goto retry_encrypt;
+	}
+	return PTR_ERR(fio->encrypted_page);
+}
+
+static inline bool need_inplace_update(struct f2fs_io_info *fio)
+{
+	struct inode *inode = fio->page->mapping->host;
+
+	if (fio->old_blkaddr == NEW_ADDR)
+		return false;
+	if (S_ISDIR(inode->i_mode) || f2fs_is_atomic_file(inode))
+		return false;
+	if (is_cold_data(fio->page))
+		return false;
+	if (IS_ATOMIC_WRITTEN_PAGE(fio->page))
+		return false;
+
+	return need_inplace_update_policy(inode, fio);
+}
+
 int do_write_data_page(struct f2fs_io_info *fio)
 {
 	struct page *page = fio->page;
@@ -1322,30 +1365,9 @@ int do_write_data_page(struct f2fs_io_info *fio)
 		goto out_writepage;
 	}
 
-	if (f2fs_encrypted_inode(inode) && S_ISREG(inode->i_mode)) {
-		gfp_t gfp_flags = GFP_NOFS;
-
-		/* wait for GCed encrypted page writeback */
-		f2fs_wait_on_encrypted_page_writeback(F2FS_I_SB(inode),
-							fio->old_blkaddr);
-retry_encrypt:
-		fio->encrypted_page = fscrypt_encrypt_page(inode, fio->page,
-							PAGE_SIZE, 0,
-							fio->page->index,
-							gfp_flags);
-		if (IS_ERR(fio->encrypted_page)) {
-			err = PTR_ERR(fio->encrypted_page);
-			if (err == -ENOMEM) {
-				/* flush pending ios and wait for a while */
-				f2fs_flush_merged_bios(F2FS_I_SB(inode));
-				congestion_wait(BLK_RW_ASYNC, HZ/50);
-				gfp_flags |= __GFP_NOFAIL;
-				err = 0;
-				goto retry_encrypt;
-			}
-			goto out_writepage;
-		}
-	}
+	err = encrypt_one_page(fio);
+	if (err)
+		goto out_writepage;
 
 	set_page_writeback(page);
 
@@ -1353,15 +1375,14 @@ int do_write_data_page(struct f2fs_io_info *fio)
 	 * If current allocation needs SSR,
 	 * it had better in-place writes for updated data.
 	 */
-	if (unlikely(fio->old_blkaddr != NEW_ADDR &&
-			!is_cold_data(page) &&
-			!IS_ATOMIC_WRITTEN_PAGE(page) &&
-			need_inplace_update(inode, fio))) {
-		f2fs_unlock_op(F2FS_I_SB(inode));
+	if (need_inplace_update(fio)) {
+		f2fs_bug_on(fio->sbi, !fio->cp_rwsem_locked);
+		f2fs_unlock_op(fio->sbi);
 		fio->cp_rwsem_locked = false;
+
 		err = rewrite_data_page(fio);
+		trace_f2fs_do_write_data_page(fio->page, IPU);
 		set_inode_flag(inode, FI_UPDATE_WRITE);
-		trace_f2fs_do_write_data_page(page, IPU);
 	} else {
 		write_data_page(&dn, fio);
 		trace_f2fs_do_write_data_page(page, OPU);

commit a788189305df9fa617e5e26dc0914d80d981cd57
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Apr 20 13:51:57 2017 -0700

    f2fs: fix out-of free segments
    
    This patch also reverts d0db7703ac1 ("f2fs: do SSR in higher priority").
    
    This patch fixes out of free segments caused by many small file creation by
    1) mkfs -s 1 2G
    2) mount
    3) untar
     - preoduce 60000 small files burstly
    4) sync
     - flush node pages
     - flush imeta
    
    Here, when we do f2fs_balance_fs, we missed # of imeta blocks, resulting in
    skipping to check has_not_enough_free_secs.
    
    Another test is done by
    1) mkfs -s 12 2G
    2) mount
    3) untar
     - preoduce 60000 small files burstly
    4) sync
     - flush node pages
     - flush imeta
    
    In this case, this patch also fixes wrong block allocation under large section
    size.
    
    Reported-by: William Brana <wbrana@gmail.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 7d46a8e6d350..b8dcd1e224e8 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1472,7 +1472,8 @@ static int __write_data_page(struct page *page, bool *submitted,
 	}
 
 	unlock_page(page);
-	f2fs_balance_fs(sbi, need_balance_fs);
+	if (!S_ISDIR(inode->i_mode))
+		f2fs_balance_fs(sbi, need_balance_fs);
 
 	if (unlikely(f2fs_cp_error(sbi))) {
 		f2fs_submit_merged_bio(sbi, DATA, WRITE);

commit 04485987f05388ffec04cdab7808ba26db30c9b8
Author: Hou Pengyang <houpengyang@huawei.com>
Date:   Tue Apr 18 11:57:16 2017 +0000

    f2fs: introduce async IPU policy
    
    This patch introduces an ASYNC IPU policy.
    
    Under senario of large # of async updating(e.g. log writing in Android),
    disk would be seriously fragmented, and higher frequent gc would be triggered.
    
    This patch uses IPU to rewrite the async update writting, since async is
    NOT sensitive to io latency.
    
    Signed-off-by: Hou Pengyang <houpengyang@huawei.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 32d5a3b38a3f..7d46a8e6d350 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1356,7 +1356,7 @@ int do_write_data_page(struct f2fs_io_info *fio)
 	if (unlikely(fio->old_blkaddr != NEW_ADDR &&
 			!is_cold_data(page) &&
 			!IS_ATOMIC_WRITTEN_PAGE(page) &&
-			need_inplace_update(inode))) {
+			need_inplace_update(inode, fio))) {
 		f2fs_unlock_op(F2FS_I_SB(inode));
 		fio->cp_rwsem_locked = false;
 		err = rewrite_data_page(fio);

commit 001c584cca6fce8e91f19eca88781b8c16d1ea42
Author: Chao Yu <yuchao0@huawei.com>
Date:   Tue Apr 18 19:23:39 2017 +0800

    f2fs: unlock cp_rwsem early for IPU writes
    
    For IPU writes, there won't be any udpates in dnode page since we
    will reuse old block address instead of allocating new one, so we
    don't need to lock cp_rwsem during IPU IO submitting.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index e984a42eabf4..32d5a3b38a3f 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1357,6 +1357,8 @@ int do_write_data_page(struct f2fs_io_info *fio)
 			!is_cold_data(page) &&
 			!IS_ATOMIC_WRITTEN_PAGE(page) &&
 			need_inplace_update(inode))) {
+		f2fs_unlock_op(F2FS_I_SB(inode));
+		fio->cp_rwsem_locked = false;
 		err = rewrite_data_page(fio);
 		set_inode_flag(inode, FI_UPDATE_WRITE);
 		trace_f2fs_do_write_data_page(page, IPU);
@@ -1392,6 +1394,7 @@ static int __write_data_page(struct page *page, bool *submitted,
 		.page = page,
 		.encrypted_page = NULL,
 		.submitted = false,
+		.cp_rwsem_locked = true,
 	};
 
 	trace_f2fs_writepage(page, DATA);
@@ -1449,7 +1452,8 @@ static int __write_data_page(struct page *page, bool *submitted,
 		err = do_write_data_page(&fio);
 	if (F2FS_I(inode)->last_disk_size < psize)
 		F2FS_I(inode)->last_disk_size = psize;
-	f2fs_unlock_op(sbi);
+	if (fio.cp_rwsem_locked)
+		f2fs_unlock_op(sbi);
 done:
 	if (err && err != -ENOENT)
 		goto redirty_out;

commit 771a9a71778def4098e8baaa23854d24e33fdb2f
Author: Tomohiro Kusumi <tkusumi@tuxera.com>
Date:   Wed Apr 5 22:49:44 2017 +0300

    f2fs: fix comment on f2fs_flush_merged_bios() after 86531d6b
    
    Callers are to unlock the page on failure after 86531d6b.
    
    Signed-off-by: Tomohiro Kusumi <tkusumi@tuxera.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 3d74c0ffa4c7..e984a42eabf4 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -341,7 +341,7 @@ void f2fs_flush_merged_bios(struct f2fs_sb_info *sbi)
 
 /*
  * Fill the locked page with data located in the block address.
- * Return unlocked page.
+ * A caller needs to unlock the page on failure.
  */
 int f2fs_submit_page_bio(struct f2fs_io_info *fio)
 {

commit d1b3e72d549094317c12c79c7817861a97004a56
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Mar 30 21:02:46 2017 -0700

    f2fs: submit bio of in-place-update pages
    
    This patch tries to split in-place-update bios from sequential bios.
    
    Suggested-by: Yunlei He <heyunlei@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index b1cac6d85bcb..3d74c0ffa4c7 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -362,6 +362,9 @@ int f2fs_submit_page_bio(struct f2fs_io_info *fio)
 	bio_set_op_attrs(bio, fio->op, fio->op_flags);
 
 	__submit_bio(fio->sbi, bio, fio->type);
+
+	if (!is_read_io(fio->op))
+		inc_page_count(fio->sbi, WB_DATA_TYPE(fio->page));
 	return 0;
 }
 
@@ -1354,7 +1357,7 @@ int do_write_data_page(struct f2fs_io_info *fio)
 			!is_cold_data(page) &&
 			!IS_ATOMIC_WRITTEN_PAGE(page) &&
 			need_inplace_update(inode))) {
-		rewrite_data_page(fio);
+		err = rewrite_data_page(fio);
 		set_inode_flag(inode, FI_UPDATE_WRITE);
 		trace_f2fs_do_write_data_page(page, IPU);
 	} else {

commit 687de7f1010cb819d04b768556960d3689abe02b
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Tue Mar 28 18:07:38 2017 -0700

    f2fs: avoid IO split due to mixed WB_SYNC_ALL and WB_SYNC_NONE
    
    If two threads try to flush dirty pages in different inodes respectively,
    f2fs_write_data_pages() will produce WRITE and WRITE_SYNC one at a time,
    resulting in a lot of 4KB seperated IOs.
    
    So, this patch gives higher priority to WB_SYNC_ALL IOs and gathers write
    IOs with a big WRITE_SYNC'ed bio.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 8f36080b47c4..b1cac6d85bcb 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1605,8 +1605,10 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 				last_idx = page->index;
 			}
 
-			if (--wbc->nr_to_write <= 0 &&
-			    wbc->sync_mode == WB_SYNC_NONE) {
+			/* give a priority to WB_SYNC threads */
+			if ((atomic_read(&F2FS_M_SB(mapping)->wb_sync_req) ||
+					--wbc->nr_to_write <= 0) &&
+					wbc->sync_mode == WB_SYNC_NONE) {
 				done = 1;
 				break;
 			}
@@ -1662,9 +1664,18 @@ static int f2fs_write_data_pages(struct address_space *mapping,
 
 	trace_f2fs_writepages(mapping->host, wbc, DATA);
 
+	/* to avoid spliting IOs due to mixed WB_SYNC_ALL and WB_SYNC_NONE */
+	if (wbc->sync_mode == WB_SYNC_ALL)
+		atomic_inc(&sbi->wb_sync_req);
+	else if (atomic_read(&sbi->wb_sync_req))
+		goto skip_write;
+
 	blk_start_plug(&plug);
 	ret = f2fs_write_cache_pages(mapping, wbc);
 	blk_finish_plug(&plug);
+
+	if (wbc->sync_mode == WB_SYNC_ALL)
+		atomic_dec(&sbi->wb_sync_req);
 	/*
 	 * if some pages were truncated, we cannot guarantee its mapping->host
 	 * to detect pending bios.

commit ef095d19e82f25bbdead472b8b71f4ef3b7a636d
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Mar 24 20:05:13 2017 -0400

    f2fs: write small sized IO to hot log
    
    It would better split small and large IOs separately in order to get more
    consecutive big writes.
    
    The default threshold is set to 64KB, but configurable by sysfs/min_hot_blocks.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 090413236b27..8f36080b47c4 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1432,6 +1432,8 @@ static int __write_data_page(struct page *page, bool *submitted,
 		need_balance_fs = true;
 	else if (has_not_enough_free_secs(sbi, 0, 0))
 		goto redirty_out;
+	else
+		set_inode_flag(inode, FI_HOT_DATA);
 
 	err = -EAGAIN;
 	if (f2fs_has_inline_data(inode)) {
@@ -1457,6 +1459,7 @@ static int __write_data_page(struct page *page, bool *submitted,
 	if (wbc->for_reclaim) {
 		f2fs_submit_merged_bio_cond(sbi, inode, 0, page->index,
 						DATA, WRITE);
+		clear_inode_flag(inode, FI_HOT_DATA);
 		remove_dirty_inode(inode);
 		submitted = NULL;
 	}
@@ -1511,6 +1514,12 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 
 	pagevec_init(&pvec, 0);
 
+	if (get_dirty_pages(mapping->host) <=
+				SM_I(F2FS_M_SB(mapping))->min_hot_blocks)
+		set_inode_flag(mapping->host, FI_HOT_DATA);
+	else
+		clear_inode_flag(mapping->host, FI_HOT_DATA);
+
 	if (wbc->range_cyclic) {
 		writeback_index = mapping->writeback_index; /* prev offset */
 		index = writeback_index;

commit 59c9081bc86ef0b273a41abf2c1f413301429a6d
Author: Yunlei He <heyunlei@huawei.com>
Date:   Mon Mar 13 20:22:18 2017 +0800

    f2fs: allow write page cache when writting cp
    
    This patch allow write data to normal file when writting
    new checkpoint.
    
    We relax three limitations for write_begin path:
    1. data allocation
    2. node allocation
    3. variables in checkpoint
    
    Signed-off-by: Yunlei He <heyunlei@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 2c8485bb6eb1..090413236b27 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -787,6 +787,21 @@ int f2fs_preallocate_blocks(struct kiocb *iocb, struct iov_iter *from)
 	return err;
 }
 
+static inline void __do_map_lock(struct f2fs_sb_info *sbi, int flag, bool lock)
+{
+	if (flag == F2FS_GET_BLOCK_PRE_AIO) {
+		if (lock)
+			down_read(&sbi->node_change);
+		else
+			up_read(&sbi->node_change);
+	} else {
+		if (lock)
+			f2fs_lock_op(sbi);
+		else
+			f2fs_unlock_op(sbi);
+	}
+}
+
 /*
  * f2fs_map_blocks() now supported readahead/bmap/rw direct_IO with
  * f2fs_map_blocks structure.
@@ -829,7 +844,7 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 
 next_dnode:
 	if (create)
-		f2fs_lock_op(sbi);
+		__do_map_lock(sbi, flag, true);
 
 	/* When reading holes, we need its node page */
 	set_new_dnode(&dn, inode, NULL, NULL, 0);
@@ -939,7 +954,7 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 	f2fs_put_dnode(&dn);
 
 	if (create) {
-		f2fs_unlock_op(sbi);
+		__do_map_lock(sbi, flag, false);
 		f2fs_balance_fs(sbi, dn.node_changed);
 	}
 	goto next_dnode;
@@ -948,7 +963,7 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 	f2fs_put_dnode(&dn);
 unlock_out:
 	if (create) {
-		f2fs_unlock_op(sbi);
+		__do_map_lock(sbi, flag, false);
 		f2fs_balance_fs(sbi, dn.node_changed);
 	}
 out:
@@ -1688,7 +1703,7 @@ static int prepare_write_begin(struct f2fs_sb_info *sbi,
 
 	if (f2fs_has_inline_data(inode) ||
 			(pos & PAGE_MASK) >= i_size_read(inode)) {
-		f2fs_lock_op(sbi);
+		__do_map_lock(sbi, F2FS_GET_BLOCK_PRE_AIO, true);
 		locked = true;
 	}
 restart:
@@ -1724,7 +1739,8 @@ static int prepare_write_begin(struct f2fs_sb_info *sbi,
 			err = get_dnode_of_data(&dn, index, LOOKUP_NODE);
 			if (err || dn.data_blkaddr == NULL_ADDR) {
 				f2fs_put_dnode(&dn);
-				f2fs_lock_op(sbi);
+				__do_map_lock(sbi, F2FS_GET_BLOCK_PRE_AIO,
+								true);
 				locked = true;
 				goto restart;
 			}
@@ -1738,7 +1754,7 @@ static int prepare_write_begin(struct f2fs_sb_info *sbi,
 	f2fs_put_dnode(&dn);
 unlock_out:
 	if (locked)
-		f2fs_unlock_op(sbi);
+		__do_map_lock(sbi, F2FS_GET_BLOCK_PRE_AIO, false);
 	return err;
 }
 

commit a83d50bc16c4f5d0a359790015b5b32f3f0e52db
Author: Kinglong Mee <kinglongmee@gmail.com>
Date:   Mon Mar 13 16:35:13 2017 +0800

    f2fs: fix bad prefetchw of NULL page
    
    For f2fs_read_data_pages, the f2fs_mpage_readpages gets "page == NULL",
    so that, the prefetchw(&page->flags) is operated on NULL.
    
    Fixes: f1e8866016 ("f2fs: expose f2fs_mpage_readpages")
    Signed-off-by: Kinglong Mee <kinglongmee@gmail.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index e341d446205a..2c8485bb6eb1 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1151,9 +1151,10 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 
 	for (page_idx = 0; nr_pages; page_idx++, nr_pages--) {
 
-		prefetchw(&page->flags);
 		if (pages) {
 			page = list_last_entry(pages, struct page, lru);
+
+			prefetchw(&page->flags);
 			list_del(&page->lru);
 			if (add_to_page_cache_lru(page, mapping,
 						  page->index,

commit 8c242db9b8c01b252290e23827163787f07e01d1
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Mar 17 09:55:52 2017 +0800

    f2fs: fix stale ATOMIC_WRITTEN_PAGE private pointer
    
    When I forced to enable atomic operations intentionally, I could hit the below
    panic, since we didn't clear page->private in f2fs_invalidate_page called by
    file truncation.
    
    The panic occurs due to NULL mapping having page->private.
    
    BUG: unable to handle kernel paging request at ffffffffffffffff
    IP: drop_buffers+0x38/0xe0
    PGD 5d00c067
    PUD 5d00e067
    PMD 0
    CPU: 3 PID: 1648 Comm: fsstress Tainted: G      D    OE   4.10.0+ #5
    Hardware name: innotek GmbH VirtualBox/VirtualBox, BIOS VirtualBox 12/01/2006
    task: ffff9151952863c0 task.stack: ffffaaec40db4000
    RIP: 0010:drop_buffers+0x38/0xe0
    RSP: 0018:ffffaaec40db74c8 EFLAGS: 00010292
    Call Trace:
     ? page_referenced+0x8b/0x170
     try_to_free_buffers+0xc5/0xe0
     try_to_release_page+0x49/0x50
     shrink_page_list+0x8bc/0x9f0
     shrink_inactive_list+0x1dd/0x500
     ? shrink_active_list+0x2c0/0x430
     shrink_node_memcg+0x5eb/0x7c0
     shrink_node+0xe1/0x320
     do_try_to_free_pages+0xef/0x2e0
     try_to_free_pages+0xe9/0x190
     __alloc_pages_slowpath+0x390/0xe70
     __alloc_pages_nodemask+0x291/0x2b0
     alloc_pages_current+0x95/0x140
     __page_cache_alloc+0xc4/0xe0
     pagecache_get_page+0xab/0x2a0
     grab_cache_page_write_begin+0x20/0x40
     get_read_data_page+0x2e6/0x4c0 [f2fs]
     ? f2fs_mark_inode_dirty_sync+0x16/0x30 [f2fs]
     ? truncate_data_blocks_range+0x238/0x2b0 [f2fs]
     get_lock_data_page+0x30/0x190 [f2fs]
     __exchange_data_block+0xaaf/0xf40 [f2fs]
     f2fs_fallocate+0x418/0xd00 [f2fs]
     vfs_fallocate+0x157/0x220
     SyS_fallocate+0x48/0x80
    
    Signed-off-by: Yunlei He <heyunlei@huawei.com>
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    [Chao Yu: use INMEM_INVALIDATE for better tracing]
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 1602b4bccae6..e341d446205a 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1951,7 +1951,7 @@ void f2fs_invalidate_page(struct page *page, unsigned int offset,
 
 	/* This is atomic written page, keep Private */
 	if (IS_ATOMIC_WRITTEN_PAGE(page))
-		return;
+		return drop_inmem_page(inode, page);
 
 	set_page_private(page, 0);
 	ClearPagePrivate(page);

commit 174cd4b1e5fbd0d74c68cf3a74f5bd4923485512
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Feb 2 19:15:33 2017 +0100

    sched/headers: Prepare to move signal wakeup & sigpending methods from <linux/sched.h> into <linux/sched/signal.h>
    
    Fix up affected files that include this signal functionality via sched.h.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 1375fef11146..1602b4bccae6 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -22,6 +22,7 @@
 #include <linux/mm.h>
 #include <linux/memcontrol.h>
 #include <linux/cleancache.h>
+#include <linux/sched/signal.h>
 
 #include "f2fs.h"
 #include "node.h"

commit 540faedb0036aca14ff90a7f679b008442bc9dd7
Author: Chao Yu <yuchao0@huawei.com>
Date:   Mon Feb 27 17:10:45 2017 +0800

    f2fs: fix to update F2FS_{CP_}WB_DATA count correctly
    
    We should only account F2FS_{CP_}WB_DATA IOs for write path, fix it.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index b0a2e3faabb2..1375fef11146 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -398,7 +398,8 @@ int f2fs_submit_page_mbio(struct f2fs_io_info *fio)
 		if ((fio->type == DATA || fio->type == NODE) &&
 				fio->new_blkaddr & F2FS_IO_SIZE_MASK(sbi)) {
 			err = -EAGAIN;
-			dec_page_count(sbi, WB_DATA_TYPE(bio_page));
+			if (!is_read)
+				dec_page_count(sbi, WB_DATA_TYPE(bio_page));
 			goto out_fail;
 		}
 		io->bio = __bio_alloc(sbi, fio->new_blkaddr,

commit 55523519bc7227e651fd4febeb3aafdd22b8af1c
Author: Chao Yu <yuchao0@huawei.com>
Date:   Sat Feb 25 11:08:28 2017 +0800

    f2fs: show simple call stack in fault injection message
    
    Previously kernel message can show that in which function we do the
    injection, but unfortunately, most of the caller are the same, for
    tracking more information of injection path, it needs to show upper
    caller's name. This patch supports that ability.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 9e51c5e40ce1..b0a2e3faabb2 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -55,8 +55,10 @@ static void f2fs_read_end_io(struct bio *bio)
 	int i;
 
 #ifdef CONFIG_F2FS_FAULT_INJECTION
-	if (time_to_inject(F2FS_P_SB(bio->bi_io_vec->bv_page), FAULT_IO))
+	if (time_to_inject(F2FS_P_SB(bio->bi_io_vec->bv_page), FAULT_IO)) {
+		f2fs_show_injection_info(FAULT_IO);
 		bio->bi_error = -EIO;
+	}
 #endif
 
 	if (f2fs_bio_encrypted(bio)) {

commit dd7b2333e6cd31584682382fcf0a1c1e5140b936
Author: Yunlei He <heyunlei@huawei.com>
Date:   Thu Feb 23 20:31:20 2017 +0800

    f2fs: no need lock_op in f2fs_write_inline_data
    
    Similar as f2fs_write_inode, f2fs_write_inline_data just
    mark inode page dirty, so it's no need to write inline data
    under read lock of cp_rwsem.
    
    Signed-off-by: Yunlei He <heyunlei@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 80f9863dc4b0..9e51c5e40ce1 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1414,9 +1414,12 @@ static int __write_data_page(struct page *page, bool *submitted,
 		goto redirty_out;
 
 	err = -EAGAIN;
-	f2fs_lock_op(sbi);
-	if (f2fs_has_inline_data(inode))
+	if (f2fs_has_inline_data(inode)) {
 		err = f2fs_write_inline_data(inode, page);
+		if (!err)
+			goto out;
+	}
+	f2fs_lock_op(sbi);
 	if (err == -EAGAIN)
 		err = do_write_data_page(&fio);
 	if (F2FS_I(inode)->last_disk_size < psize)

commit 3f2be04304cf10f2ef074399f8dd565bd00ddcae
Author: Kinglong Mee <kinglongmee@gmail.com>
Date:   Thu Feb 23 19:55:05 2017 +0800

    f2fs: avoid m_flags overlay when allocating more data blocks
    
    When more than one data blocks are allocated, the F2FS_MAP_UNWRITTEN/MAPPED
    flags will be overlapped by F2FS_MAP_NEW at the later times.
    
    Signed-off-by: Kinglong Mee <kinglongmee@gmail.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index f72493d8c8e4..80f9863dc4b0 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -867,7 +867,7 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 			}
 			if (err)
 				goto sync_out;
-			map->m_flags = F2FS_MAP_NEW;
+			map->m_flags |= F2FS_MAP_NEW;
 			blkaddr = dn.data_blkaddr;
 		} else {
 			if (flag == F2FS_GET_BLOCK_BMAP) {

commit e15882b6c6caff427fe387e878e2f23de58c053b
Author: Hou Pengyang <houpengyang@huawei.com>
Date:   Thu Feb 23 09:18:05 2017 +0000

    f2fs: init local extent_info to avoid stale stack info in tp
    
    To avoid such stale(fops, blk, len) info in f2fs_lookup_extent_tree_end tp
    
    dio-23095 [005] ...1 17878.856859: f2fs_lookup_extent_tree_end:
                            dev = (259,30), ino = 856, pgofs = 0,
                            ext_info(fofs: 3441207040, blk: 4294967232, len: 3481143808)
    
    Signed-off-by: Hou Pengyang <houpengyang@huawei.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 5f3bc98a387d..f72493d8c8e4 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -511,7 +511,7 @@ int f2fs_reserve_block(struct dnode_of_data *dn, pgoff_t index)
 
 int f2fs_get_block(struct dnode_of_data *dn, pgoff_t index)
 {
-	struct extent_info ei;
+	struct extent_info ei  = {0,0,0};
 	struct inode *inode = dn->inode;
 
 	if (f2fs_lookup_extent_cache(inode, index, &ei)) {
@@ -528,7 +528,7 @@ struct page *get_read_data_page(struct inode *inode, pgoff_t index,
 	struct address_space *mapping = inode->i_mapping;
 	struct dnode_of_data dn;
 	struct page *page;
-	struct extent_info ei;
+	struct extent_info ei = {0,0,0};
 	int err;
 	struct f2fs_io_info fio = {
 		.sbi = F2FS_I_SB(inode),
@@ -803,7 +803,7 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 	int err = 0, ofs = 1;
 	unsigned int ofs_in_node, last_ofs_in_node;
 	blkcnt_t prealloc;
-	struct extent_info ei;
+	struct extent_info ei = {0,0,0};
 	block_t blkaddr;
 
 	if (!maxblocks)
@@ -1667,7 +1667,7 @@ static int prepare_write_begin(struct f2fs_sb_info *sbi,
 	struct dnode_of_data dn;
 	struct page *ipage;
 	bool locked = false;
-	struct extent_info ei;
+	struct extent_info ei = {0,0,0};
 	int err = 0;
 
 	/*

commit 86d54795c94532075d862aa0a79f0c981dab4bdd
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Feb 17 09:55:55 2017 -0800

    f2fs: do not wait for writeback in write_begin
    
    Otherwise we can get livelock like below.
    
    [79880.428136] dbench          D    0 18405  18404 0x00000000
    [79880.428139] Call Trace:
    [79880.428142]  __schedule+0x219/0x6b0
    [79880.428144]  schedule+0x36/0x80
    [79880.428147]  schedule_timeout+0x243/0x2e0
    [79880.428152]  ? update_sd_lb_stats+0x16b/0x5f0
    [79880.428155]  ? ktime_get+0x3c/0xb0
    [79880.428157]  io_schedule_timeout+0xa6/0x110
    [79880.428161]  __lock_page+0xf7/0x130
    [79880.428164]  ? unlock_page+0x30/0x30
    [79880.428167]  pagecache_get_page+0x16b/0x250
    [79880.428171]  grab_cache_page_write_begin+0x20/0x40
    [79880.428182]  f2fs_write_begin+0xa2/0xdb0 [f2fs]
    [79880.428192]  ? f2fs_mark_inode_dirty_sync+0x16/0x30 [f2fs]
    [79880.428197]  ? kmem_cache_free+0x79/0x200
    [79880.428203]  ? __mark_inode_dirty+0x17f/0x360
    [79880.428206]  generic_perform_write+0xbb/0x190
    [79880.428213]  ? file_update_time+0xa4/0xf0
    [79880.428217]  __generic_file_write_iter+0x19b/0x1e0
    [79880.428226]  f2fs_file_write_iter+0x9c/0x180 [f2fs]
    [79880.428231]  __vfs_write+0xc5/0x140
    [79880.428235]  vfs_write+0xb2/0x1b0
    [79880.428238]  SyS_write+0x46/0xa0
    [79880.428242]  entry_SYSCALL_64_fastpath+0x1e/0xad
    
    Fixes: cae96a5c8ab6 ("f2fs: check io submission more precisely")
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 8c61fa7fd27d..5f3bc98a387d 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1759,7 +1759,12 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 			goto fail;
 	}
 repeat:
-	page = grab_cache_page_write_begin(mapping, index, flags);
+	/*
+	 * Do not use grab_cache_page_write_begin() to avoid deadlock due to
+	 * wait_for_stable_page. Will wait that below with our IO control.
+	 */
+	page = pagecache_get_page(mapping, index,
+				FGP_LOCK | FGP_WRITE | FGP_CREAT, GFP_NOFS);
 	if (!page) {
 		err = -ENOMEM;
 		goto fail;

commit 7f54f51f46f69f3ccde2b65e682a51b0aa6199c3
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Mon Feb 6 13:57:58 2017 -0800

    f2fs: remove preflush for nobarrier case
    
    This patch removes REQ_PREFLUSH in the nobarrier case.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index d14cc8be14f1..8c61fa7fd27d 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -306,9 +306,9 @@ static void __f2fs_submit_merged_bio(struct f2fs_sb_info *sbi,
 	if (type >= META_FLUSH) {
 		io->fio.type = META_FLUSH;
 		io->fio.op = REQ_OP_WRITE;
-		io->fio.op_flags = REQ_PREFLUSH | REQ_META | REQ_PRIO;
+		io->fio.op_flags = REQ_META | REQ_PRIO;
 		if (!test_opt(sbi, NOBARRIER))
-			io->fio.op_flags |= REQ_FUA;
+			io->fio.op_flags |= REQ_PREFLUSH | REQ_FUA;
 	}
 	__submit_merged_bio(io);
 out:

commit 942fd3192f83cef54bc0d485937fd5382ac5acd0
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed Feb 1 16:51:22 2017 -0800

    f2fs: check last page index in cached bio to decide submission
    
    If the cached bio has the last page's index, then we need to submit it.
    Otherwise, we don't need to submit it and can wait for further IO merges.
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index e0b74b9ea388..d14cc8be14f1 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -243,8 +243,8 @@ static void __submit_merged_bio(struct f2fs_bio_info *io)
 	io->bio = NULL;
 }
 
-static bool __has_merged_page(struct f2fs_bio_info *io, struct inode *inode,
-						struct page *page, nid_t ino)
+static bool __has_merged_page(struct f2fs_bio_info *io,
+				struct inode *inode, nid_t ino, pgoff_t idx)
 {
 	struct bio_vec *bvec;
 	struct page *target;
@@ -253,7 +253,7 @@ static bool __has_merged_page(struct f2fs_bio_info *io, struct inode *inode,
 	if (!io->bio)
 		return false;
 
-	if (!inode && !page && !ino)
+	if (!inode && !ino)
 		return true;
 
 	bio_for_each_segment_all(bvec, io->bio, i) {
@@ -263,10 +263,11 @@ static bool __has_merged_page(struct f2fs_bio_info *io, struct inode *inode,
 		else
 			target = fscrypt_control_page(bvec->bv_page);
 
+		if (idx != target->index)
+			continue;
+
 		if (inode && inode == target->mapping->host)
 			return true;
-		if (page && page == target)
-			return true;
 		if (ino && ino == ino_of_node(target))
 			return true;
 	}
@@ -275,22 +276,21 @@ static bool __has_merged_page(struct f2fs_bio_info *io, struct inode *inode,
 }
 
 static bool has_merged_page(struct f2fs_sb_info *sbi, struct inode *inode,
-						struct page *page, nid_t ino,
-						enum page_type type)
+				nid_t ino, pgoff_t idx, enum page_type type)
 {
 	enum page_type btype = PAGE_TYPE_OF_BIO(type);
 	struct f2fs_bio_info *io = &sbi->write_io[btype];
 	bool ret;
 
 	down_read(&io->io_rwsem);
-	ret = __has_merged_page(io, inode, page, ino);
+	ret = __has_merged_page(io, inode, ino, idx);
 	up_read(&io->io_rwsem);
 	return ret;
 }
 
 static void __f2fs_submit_merged_bio(struct f2fs_sb_info *sbi,
-				struct inode *inode, struct page *page,
-				nid_t ino, enum page_type type, int rw)
+				struct inode *inode, nid_t ino, pgoff_t idx,
+				enum page_type type, int rw)
 {
 	enum page_type btype = PAGE_TYPE_OF_BIO(type);
 	struct f2fs_bio_info *io;
@@ -299,7 +299,7 @@ static void __f2fs_submit_merged_bio(struct f2fs_sb_info *sbi,
 
 	down_write(&io->io_rwsem);
 
-	if (!__has_merged_page(io, inode, page, ino))
+	if (!__has_merged_page(io, inode, ino, idx))
 		goto out;
 
 	/* change META to META_FLUSH in the checkpoint procedure */
@@ -318,15 +318,15 @@ static void __f2fs_submit_merged_bio(struct f2fs_sb_info *sbi,
 void f2fs_submit_merged_bio(struct f2fs_sb_info *sbi, enum page_type type,
 									int rw)
 {
-	__f2fs_submit_merged_bio(sbi, NULL, NULL, 0, type, rw);
+	__f2fs_submit_merged_bio(sbi, NULL, 0, 0, type, rw);
 }
 
 void f2fs_submit_merged_bio_cond(struct f2fs_sb_info *sbi,
-				struct inode *inode, struct page *page,
-				nid_t ino, enum page_type type, int rw)
+				struct inode *inode, nid_t ino, pgoff_t idx,
+				enum page_type type, int rw)
 {
-	if (has_merged_page(sbi, inode, page, ino, type))
-		__f2fs_submit_merged_bio(sbi, inode, page, ino, type, rw);
+	if (has_merged_page(sbi, inode, ino, idx, type))
+		__f2fs_submit_merged_bio(sbi, inode, ino, idx, type, rw);
 }
 
 void f2fs_flush_merged_bios(struct f2fs_sb_info *sbi)
@@ -1432,7 +1432,8 @@ static int __write_data_page(struct page *page, bool *submitted,
 		ClearPageUptodate(page);
 
 	if (wbc->for_reclaim) {
-		f2fs_submit_merged_bio_cond(sbi, NULL, page, 0, DATA, WRITE);
+		f2fs_submit_merged_bio_cond(sbi, inode, 0, page->index,
+						DATA, WRITE);
 		remove_dirty_inode(inode);
 		submitted = NULL;
 	}
@@ -1480,10 +1481,10 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 	pgoff_t index;
 	pgoff_t end;		/* Inclusive */
 	pgoff_t done_index;
+	pgoff_t last_idx = ULONG_MAX;
 	int cycled;
 	int range_whole = 0;
 	int tag;
-	int nwritten = 0;
 
 	pagevec_init(&pvec, 0);
 
@@ -1569,7 +1570,7 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 				done = 1;
 				break;
 			} else if (submitted) {
-				nwritten++;
+				last_idx = page->index;
 			}
 
 			if (--wbc->nr_to_write <= 0 &&
@@ -1591,9 +1592,9 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 	if (wbc->range_cyclic || (range_whole && wbc->nr_to_write > 0))
 		mapping->writeback_index = done_index;
 
-	if (nwritten)
+	if (last_idx != ULONG_MAX)
 		f2fs_submit_merged_bio_cond(F2FS_M_SB(mapping), mapping->host,
-							NULL, 0, DATA, WRITE);
+						0, last_idx, DATA, WRITE);
 
 	return ret;
 }

commit d68f735b3bc934a7523a047aa952a577cf6ca171
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Feb 3 17:44:04 2017 -0800

    f2fs: check io submission more precisely
    
    This patch check IO submission more precisely than previous rough check.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index e78286ee3cc7..e0b74b9ea388 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -379,6 +379,9 @@ int f2fs_submit_page_mbio(struct f2fs_io_info *fio)
 
 	bio_page = fio->encrypted_page ? fio->encrypted_page : fio->page;
 
+	/* set submitted = 1 as a return value */
+	fio->submitted = 1;
+
 	if (!is_read)
 		inc_page_count(sbi, WB_DATA_TYPE(bio_page));
 
@@ -1346,8 +1349,8 @@ int do_write_data_page(struct f2fs_io_info *fio)
 	return err;
 }
 
-static int __write_data_page(struct page *page,
-					struct writeback_control *wbc)
+static int __write_data_page(struct page *page, bool *submitted,
+				struct writeback_control *wbc)
 {
 	struct inode *inode = page->mapping->host;
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
@@ -1365,6 +1368,7 @@ static int __write_data_page(struct page *page,
 		.op_flags = wbc_to_write_flags(wbc),
 		.page = page,
 		.encrypted_page = NULL,
+		.submitted = false,
 	};
 
 	trace_f2fs_writepage(page, DATA);
@@ -1430,13 +1434,19 @@ static int __write_data_page(struct page *page,
 	if (wbc->for_reclaim) {
 		f2fs_submit_merged_bio_cond(sbi, NULL, page, 0, DATA, WRITE);
 		remove_dirty_inode(inode);
+		submitted = NULL;
 	}
 
 	unlock_page(page);
 	f2fs_balance_fs(sbi, need_balance_fs);
 
-	if (unlikely(f2fs_cp_error(sbi)))
+	if (unlikely(f2fs_cp_error(sbi))) {
 		f2fs_submit_merged_bio(sbi, DATA, WRITE);
+		submitted = NULL;
+	}
+
+	if (submitted)
+		*submitted = fio.submitted;
 
 	return 0;
 
@@ -1451,7 +1461,7 @@ static int __write_data_page(struct page *page,
 static int f2fs_write_data_page(struct page *page,
 					struct writeback_control *wbc)
 {
-	return __write_data_page(page, wbc);
+	return __write_data_page(page, NULL, wbc);
 }
 
 /*
@@ -1510,6 +1520,7 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 
 		for (i = 0; i < nr_pages; i++) {
 			struct page *page = pvec.pages[i];
+			bool submitted = false;
 
 			if (page->index > end) {
 				done = 1;
@@ -1543,7 +1554,7 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 			if (!clear_page_dirty_for_io(page))
 				goto continue_unlock;
 
-			ret = __write_data_page(page, wbc);
+			ret = __write_data_page(page, &submitted, wbc);
 			if (unlikely(ret)) {
 				/*
 				 * keep nr_to_write, since vfs uses this to
@@ -1557,7 +1568,7 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 				done_index = page->index + 1;
 				done = 1;
 				break;
-			} else {
+			} else if (submitted) {
 				nwritten++;
 			}
 

commit f566bae8462c4a48b55bbf4e58d5b1e0a8563819
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Feb 3 17:18:00 2017 -0800

    f2fs: call internal __write_data_page directly
    
    This patch introduces __write_data_page to call it by f2fs_write_cache_pages
    directly..
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 3b5f1d14cab3..e78286ee3cc7 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1346,7 +1346,7 @@ int do_write_data_page(struct f2fs_io_info *fio)
 	return err;
 }
 
-static int f2fs_write_data_page(struct page *page,
+static int __write_data_page(struct page *page,
 					struct writeback_control *wbc)
 {
 	struct inode *inode = page->mapping->host;
@@ -1448,6 +1448,12 @@ static int f2fs_write_data_page(struct page *page,
 	return err;
 }
 
+static int f2fs_write_data_page(struct page *page,
+					struct writeback_control *wbc)
+{
+	return __write_data_page(page, wbc);
+}
+
 /*
  * This function was copied from write_cche_pages from mm/page-writeback.c.
  * The major change is making write step of cold data page separately from
@@ -1537,7 +1543,7 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 			if (!clear_page_dirty_for_io(page))
 				goto continue_unlock;
 
-			ret = mapping->a_ops->writepage(page, wbc);
+			ret = __write_data_page(page, wbc);
 			if (unlikely(ret)) {
 				/*
 				 * keep nr_to_write, since vfs uses this to

commit b86e33075ed1909d8002745b56ecf73b833db143
Author: Wei Fang <fangwei1@huawei.com>
Date:   Sun Jan 22 12:21:02 2017 +0800

    f2fs: fix a dead loop in f2fs_fiemap()
    
    A dead loop can be triggered in f2fs_fiemap() using the test case
    as below:
    
            ...
            fd = open();
            fallocate(fd, 0, 0, 4294967296);
            ioctl(fd, FS_IOC_FIEMAP, fiemap_buf);
            ...
    
    It's caused by an overflow in __get_data_block():
            ...
            bh->b_size = map.m_len << inode->i_blkbits;
            ...
    map.m_len is an unsigned int, and bh->b_size is a size_t which is 64 bits
    on 64 bits archtecture, type conversion from an unsigned int to a size_t
    will result in an overflow.
    
    In the above-mentioned case, bh->b_size will be zero, and f2fs_fiemap()
    will call get_data_block() at block 0 again an again.
    
    Fix this by adding a force conversion before left shift.
    
    Signed-off-by: Wei Fang <fangwei1@huawei.com>
    Acked-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 2ea80215f26c..3b5f1d14cab3 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -964,7 +964,7 @@ static int __get_data_block(struct inode *inode, sector_t iblock,
 	if (!err) {
 		map_bh(bh, inode->i_sb, map.m_pblk);
 		bh->b_state = (bh->b_state & ~F2FS_MAP_FLAGS) | map.m_flags;
-		bh->b_size = map.m_len << inode->i_blkbits;
+		bh->b_size = (u64)map.m_len << inode->i_blkbits;
 	}
 	return err;
 }

commit dc91de78e5e1d44238b5dd2b57d2e8e67cbc00a1
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Jan 13 13:12:29 2017 -0800

    f2fs: do not preallocate blocks which has wrong buffer
    
    Sheng Yong reports needless preallocation if write(small_buffer, large_size)
    is called.
    
    In that case, f2fs preallocates large_size, but vfs returns early due to
    small_buffer size. Let's detect it before preallocation phase in f2fs.
    
    Reported-by: Sheng Yong <shengyong1@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 848d110dc1ca..2ea80215f26c 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -749,6 +749,9 @@ int f2fs_preallocate_blocks(struct kiocb *iocb, struct iov_iter *from)
 	struct f2fs_map_blocks map;
 	int err = 0;
 
+	if (is_inode_flag_set(inode, FI_NO_PREALLOC))
+		return 0;
+
 	map.m_lblk = F2FS_BLK_ALIGN(iocb->ki_pos);
 	map.m_len = F2FS_BYTES_TO_BLK(iocb->ki_pos + iov_iter_count(from));
 	if (map.m_len > map.m_lblk)
@@ -1653,7 +1656,8 @@ static int prepare_write_begin(struct f2fs_sb_info *sbi,
 	 * we already allocated all the blocks, so we don't need to get
 	 * the block addresses when there is no need to fill the page.
 	 */
-	if (!f2fs_has_inline_data(inode) && len == PAGE_SIZE)
+	if (!f2fs_has_inline_data(inode) && len == PAGE_SIZE &&
+			!is_inode_flag_set(inode, FI_NO_PREALLOC))
 		return 0;
 
 	if (f2fs_has_inline_data(inode) ||

commit 5fe457430e554a2f5188f13c1a2e36ad845640c5
Author: Chao Yu <yuchao0@huawei.com>
Date:   Sat Jan 7 18:50:26 2017 +0800

    f2fs: introduce FI_ATOMIC_COMMIT
    
    This patch introduces a new flag to indicate inode status of doing atomic
    write committing, so that, we can keep atomic write status for inode
    during atomic committing, then we can skip GCing pages of atomic write inode,
    that avoids random GCed datas being mixed with current transaction, so
    isolation of transaction can be kept.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index ab2008dea921..848d110dc1ca 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1977,7 +1977,7 @@ static int f2fs_set_data_page_dirty(struct page *page)
 	if (!PageUptodate(page))
 		SetPageUptodate(page);
 
-	if (f2fs_is_atomic_file(inode)) {
+	if (f2fs_is_atomic_file(inode) && !f2fs_is_commit_atomic_write(inode)) {
 		if (!IS_ATOMIC_WRITTEN_PAGE(page)) {
 			register_inmem_page(inode, page);
 			return 1;

commit 939afa943c5290a3b92f01612a792af17bc98115
Author: Chao Yu <yuchao0@huawei.com>
Date:   Sat Jan 7 18:49:42 2017 +0800

    f2fs: clean up with list_{first, last}_entry
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 12d235f2c771..ab2008dea921 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1143,7 +1143,7 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 
 		prefetchw(&page->flags);
 		if (pages) {
-			page = list_entry(pages->prev, struct page, lru);
+			page = list_last_entry(pages, struct page, lru);
 			list_del(&page->lru);
 			if (add_to_page_cache_lru(page, mapping,
 						  page->index,
@@ -1262,7 +1262,7 @@ static int f2fs_read_data_pages(struct file *file,
 			struct list_head *pages, unsigned nr_pages)
 {
 	struct inode *inode = file->f_mapping->host;
-	struct page *page = list_entry(pages->prev, struct page, lru);
+	struct page *page = list_last_entry(pages, struct page, lru);
 
 	trace_f2fs_readpages(inode, page, nr_pages);
 

commit 0a595ebaaa6b53a2226d3fee2a2fd616ea5ba378
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed Dec 14 10:12:56 2016 -0800

    f2fs: support IO alignment for DATA and NODE writes
    
    This patch implements IO alignment by filling dummy blocks in DATA and NODE
    write bios. If we can guarantee, for example, 32KB or 64KB for such the IOs,
    we can eliminate underlying dummy page problem which FTL conducts in order to
    close MLC or TLC partial written pages.
    
    Note that,
     - it requires "-o mode=lfs".
     - IO size should be power of 2, not exceed BIO_MAX_PAGES, 256.
     - read IO is still 4KB.
     - do checkpoint at fsync, if dummy NODE page was written.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index a06b2d187aec..12d235f2c771 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -93,6 +93,17 @@ static void f2fs_write_end_io(struct bio *bio)
 		struct page *page = bvec->bv_page;
 		enum count_type type = WB_DATA_TYPE(page);
 
+		if (IS_DUMMY_WRITTEN_PAGE(page)) {
+			set_page_private(page, (unsigned long)NULL);
+			ClearPagePrivate(page);
+			unlock_page(page);
+			mempool_free(page, sbi->write_io_dummy);
+
+			if (unlikely(bio->bi_error))
+				f2fs_stop_checkpoint(sbi, true);
+			continue;
+		}
+
 		fscrypt_pullback_bio_page(&page, true);
 
 		if (unlikely(bio->bi_error)) {
@@ -171,10 +182,42 @@ static inline void __submit_bio(struct f2fs_sb_info *sbi,
 				struct bio *bio, enum page_type type)
 {
 	if (!is_read_io(bio_op(bio))) {
+		unsigned int start;
+
 		if (f2fs_sb_mounted_blkzoned(sbi->sb) &&
 			current->plug && (type == DATA || type == NODE))
 			blk_finish_plug(current->plug);
+
+		if (type != DATA && type != NODE)
+			goto submit_io;
+
+		start = bio->bi_iter.bi_size >> F2FS_BLKSIZE_BITS;
+		start %= F2FS_IO_SIZE(sbi);
+
+		if (start == 0)
+			goto submit_io;
+
+		/* fill dummy pages */
+		for (; start < F2FS_IO_SIZE(sbi); start++) {
+			struct page *page =
+				mempool_alloc(sbi->write_io_dummy,
+					GFP_NOIO | __GFP_ZERO | __GFP_NOFAIL);
+			f2fs_bug_on(sbi, !page);
+
+			SetPagePrivate(page);
+			set_page_private(page, (unsigned long)DUMMY_WRITTEN_PAGE);
+			lock_page(page);
+			if (bio_add_page(bio, page, PAGE_SIZE, 0) < PAGE_SIZE)
+				f2fs_bug_on(sbi, 1);
+		}
+		/*
+		 * In the NODE case, we lose next block address chain. So, we
+		 * need to do checkpoint in f2fs_sync_file.
+		 */
+		if (type == NODE)
+			set_sbi_flag(sbi, SBI_NEED_CP);
 	}
+submit_io:
 	if (is_read_io(bio_op(bio)))
 		trace_f2fs_submit_read_bio(sbi->sb, type, bio);
 	else
@@ -319,13 +362,14 @@ int f2fs_submit_page_bio(struct f2fs_io_info *fio)
 	return 0;
 }
 
-void f2fs_submit_page_mbio(struct f2fs_io_info *fio)
+int f2fs_submit_page_mbio(struct f2fs_io_info *fio)
 {
 	struct f2fs_sb_info *sbi = fio->sbi;
 	enum page_type btype = PAGE_TYPE_OF_BIO(fio->type);
 	struct f2fs_bio_info *io;
 	bool is_read = is_read_io(fio->op);
 	struct page *bio_page;
+	int err = 0;
 
 	io = is_read ? &sbi->read_io : &sbi->write_io[btype];
 
@@ -346,6 +390,12 @@ void f2fs_submit_page_mbio(struct f2fs_io_info *fio)
 		__submit_merged_bio(io);
 alloc_new:
 	if (io->bio == NULL) {
+		if ((fio->type == DATA || fio->type == NODE) &&
+				fio->new_blkaddr & F2FS_IO_SIZE_MASK(sbi)) {
+			err = -EAGAIN;
+			dec_page_count(sbi, WB_DATA_TYPE(bio_page));
+			goto out_fail;
+		}
 		io->bio = __bio_alloc(sbi, fio->new_blkaddr,
 						BIO_MAX_PAGES, is_read);
 		io->fio = *fio;
@@ -359,9 +409,10 @@ void f2fs_submit_page_mbio(struct f2fs_io_info *fio)
 
 	io->last_block_in_bio = fio->new_blkaddr;
 	f2fs_trace_ios(fio, 0);
-
+out_fail:
 	up_write(&io->io_rwsem);
 	trace_f2fs_submit_page_mbio(fio->page, fio);
+	return err;
 }
 
 static void __set_data_blkaddr(struct dnode_of_data *dn)

commit 554b5125f5cfca6653461fd52bad24d4ef35ec29
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed Dec 21 12:13:03 2016 -0800

    f2fs: add submit_bio tracepoint
    
    This patch adds final submit_bio() tracepoint.
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 2c5df1dc1479..a06b2d187aec 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -175,6 +175,10 @@ static inline void __submit_bio(struct f2fs_sb_info *sbi,
 			current->plug && (type == DATA || type == NODE))
 			blk_finish_plug(current->plug);
 	}
+	if (is_read_io(bio_op(bio)))
+		trace_f2fs_submit_read_bio(sbi->sb, type, bio);
+	else
+		trace_f2fs_submit_write_bio(sbi->sb, type, bio);
 	submit_bio(bio);
 }
 
@@ -185,12 +189,12 @@ static void __submit_merged_bio(struct f2fs_bio_info *io)
 	if (!io->bio)
 		return;
 
+	bio_set_op_attrs(io->bio, fio->op, fio->op_flags);
+
 	if (is_read_io(fio->op))
-		trace_f2fs_submit_read_bio(io->sbi->sb, fio, io->bio);
+		trace_f2fs_prepare_read_bio(io->sbi->sb, fio->type, io->bio);
 	else
-		trace_f2fs_submit_write_bio(io->sbi->sb, fio, io->bio);
-
-	bio_set_op_attrs(io->bio, fio->op, fio->op_flags);
+		trace_f2fs_prepare_write_bio(io->sbi->sb, fio->type, io->bio);
 
 	__submit_bio(io->sbi, io->bio, fio->type);
 	io->bio = NULL;

commit 746e2403927efbd7c7f2e796314e3cfb3cfabaa4
Author: Yunlei He <heyunlei@huawei.com>
Date:   Tue Dec 20 11:11:35 2016 +0800

    f2fs: add a case of no need to read a page in write begin
    
    If the range we write cover the whole valid data in the last page,
    we do not need to read it.
    
    Signed-off-by: Yunlei He <heyunlei@huawei.com>
    [Jaegeuk Kim: nullify the remaining area (fix: xfstests/f2fs/001)]
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 9ac262564fa6..2c5df1dc1479 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1715,6 +1715,11 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 	if (len == PAGE_SIZE || PageUptodate(page))
 		return 0;
 
+	if (!(pos & (PAGE_SIZE - 1)) && (pos + len) >= i_size_read(inode)) {
+		zero_user_segment(page, len, PAGE_SIZE);
+		return 0;
+	}
+
 	if (blkaddr == NEW_ADDR) {
 		zero_user_segment(page, 0, PAGE_SIZE);
 		SetPageUptodate(page);
@@ -1768,7 +1773,7 @@ static int f2fs_write_end(struct file *file,
 	 * let generic_perform_write() try to copy data again through copied=0.
 	 */
 	if (!PageUptodate(page)) {
-		if (unlikely(copied != PAGE_SIZE))
+		if (unlikely(copied != len))
 			copied = 0;
 		else
 			SetPageUptodate(page);

commit 5084fdf081739b7455c7aeecda6d7b83ec59c85f
Merge: 09cb6464fe5e a551d7c8deef
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Dec 14 09:17:42 2016 -0800

    Merge tag 'ext4_for_linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tytso/ext4
    
    Pull ext4 updates from Ted Ts'o:
     "This merge request includes the dax-4.0-iomap-pmd branch which is
      needed for both ext4 and xfs dax changes to use iomap for DAX. It also
      includes the fscrypt branch which is needed for ubifs encryption work
      as well as ext4 encryption and fscrypt cleanups.
    
      Lots of cleanups and bug fixes, especially making sure ext4 is robust
      against maliciously corrupted file systems --- especially maliciously
      corrupted xattr blocks and a maliciously corrupted superblock. Also
      fix ext4 support for 64k block sizes so it works well on ppcle. Fixed
      mbcache so we don't miss some common xattr blocks that can be merged"
    
    * tag 'ext4_for_linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tytso/ext4: (86 commits)
      dax: Fix sleep in atomic contex in grab_mapping_entry()
      fscrypt: Rename FS_WRITE_PATH_FL to FS_CTX_HAS_BOUNCE_BUFFER_FL
      fscrypt: Delay bounce page pool allocation until needed
      fscrypt: Cleanup page locking requirements for fscrypt_{decrypt,encrypt}_page()
      fscrypt: Cleanup fscrypt_{decrypt,encrypt}_page()
      fscrypt: Never allocate fscrypt_ctx on in-place encryption
      fscrypt: Use correct index in decrypt path.
      fscrypt: move the policy flags and encryption mode definitions to uapi header
      fscrypt: move non-public structures and constants to fscrypt_private.h
      fscrypt: unexport fscrypt_initialize()
      fscrypt: rename get_crypt_info() to fscrypt_get_crypt_info()
      fscrypto: move ioctl processing more fully into common code
      fscrypto: remove unneeded Kconfig dependencies
      MAINTAINERS: fscrypto: recommend linux-fsdevel for fscrypto patches
      ext4: do not perform data journaling when data is encrypted
      ext4: return -ENOMEM instead of success
      ext4: reject inodes with negative size
      ext4: remove another test in ext4_alloc_file_blocks()
      Documentation: fix description of ext4's block_validity mount option
      ext4: fix checks for data=ordered and journal_async_commit options
      ...

commit 09cb6464fe5e7fcd5177911429badd139c4481b7
Merge: 19d37ce2a715 c0ed4405a99e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Dec 14 09:07:36 2016 -0800

    Merge tag 'for-f2fs-4.10' of git://git.kernel.org/pub/scm/linux/kernel/git/jaegeuk/f2fs
    
    Pull f2fs updates from Jaegeuk Kim:
     "This patch series contains several performance tuning patches
      regarding to the IO submission flow, in addition to supporting new
      features such as a ZBC-base drive and multiple devices.
    
      It also includes some major bug fixes such as:
       - checkpoint version control
       - fdatasync-related roll-forward recovery routine
       - memory boundary or null-pointer access in corner cases
       - missing error cases
    
      It has various minor clean-up patches as well"
    
    * tag 'for-f2fs-4.10' of git://git.kernel.org/pub/scm/linux/kernel/git/jaegeuk/f2fs: (66 commits)
      f2fs: fix a missing size change in f2fs_setattr
      f2fs: fix to access nullified flush_cmd_control pointer
      f2fs: free meta pages if sanity check for ckpt is failed
      f2fs: detect wrong layout
      f2fs: call sync_fs when f2fs is idle
      Revert "f2fs: use percpu_counter for # of dirty pages in inode"
      f2fs: return AOP_WRITEPAGE_ACTIVATE for writepage
      f2fs: do not activate auto_recovery for fallocated i_size
      f2fs: fix to determine start_cp_addr by sbi->cur_cp_pack
      f2fs: fix 32-bit build
      f2fs: set ->owner for debugfs status file's file_operations
      f2fs: fix incorrect free inode count in ->statfs
      f2fs: drop duplicate header timer.h
      f2fs: fix wrong AUTO_RECOVER condition
      f2fs: do not recover i_size if it's valid
      f2fs: fix fdatasync
      f2fs: fix to account total free nid correctly
      f2fs: fix an infinite loop when flush nodes in cp
      f2fs: don't wait writeback for datas during checkpoint
      f2fs: fix wrong written_valid_blocks counting
      ...

commit bd7b8290388dd58a8c0a3710b171e58ef952ca4d
Author: David Gstir <david@sigma-star.at>
Date:   Tue Dec 6 23:53:56 2016 +0100

    fscrypt: Cleanup page locking requirements for fscrypt_{decrypt,encrypt}_page()
    
    Rename the FS_CFLG_INPLACE_ENCRYPTION flag to FS_CFLG_OWN_PAGES which,
    when set, indicates that the fs uses pages under its own control as
    opposed to writeback pages which require locking and a bounce buffer for
    encryption.
    
    Signed-off-by: David Gstir <david@sigma-star.at>
    Signed-off-by: Theodore Ts'o <tytso@mit.edu>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 435590c4b341..9f0ba90b92e4 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1194,7 +1194,6 @@ int do_write_data_page(struct f2fs_io_info *fio)
 		f2fs_wait_on_encrypted_page_writeback(F2FS_I_SB(inode),
 							fio->old_blkaddr);
 retry_encrypt:
-		BUG_ON(!PageLocked(fio->page));
 		fio->encrypted_page = fscrypt_encrypt_page(inode, fio->page,
 							PAGE_SIZE, 0,
 							fio->page->index,

commit 0002b61bdaac732bcff364a18f5bd57c95def0a5
Author: Chao Yu <yuchao0@huawei.com>
Date:   Mon Nov 28 19:13:43 2016 -0800

    f2fs: return AOP_WRITEPAGE_ACTIVATE for writepage
    
    We should use AOP_WRITEPAGE_ACTIVATE when we bypass writing pages.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Miao Xie <miaoxie@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 8c5b63bda68b..b90fb010a991 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1384,6 +1384,8 @@ static int f2fs_write_data_page(struct page *page,
 
 redirty_out:
 	redirty_page_for_writepage(wbc, page);
+	if (!err)
+		return AOP_WRITEPAGE_ACTIVATE;
 	unlock_page(page);
 	return err;
 }
@@ -1479,6 +1481,15 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 
 			ret = mapping->a_ops->writepage(page, wbc);
 			if (unlikely(ret)) {
+				/*
+				 * keep nr_to_write, since vfs uses this to
+				 * get # of written pages.
+				 */
+				if (ret == AOP_WRITEPAGE_ACTIVATE) {
+					unlock_page(page);
+					ret = 0;
+					continue;
+				}
 				done_index = page->index + 1;
 				done = 1;
 				break;

commit 36951b38d13ac7cce9fcf89e0e01c22ed0d05688
Author: Chao Yu <yuchao0@huawei.com>
Date:   Wed Nov 16 10:41:20 2016 +0800

    f2fs: don't wait writeback for datas during checkpoint
    
    Normally, while committing checkpoint, we will wait on all pages to be
    writebacked no matter the page is data or metadata, so in scenario where
    there are lots of data IO being submitted with metadata, we may suffer
    long latency for waiting writeback during checkpoint.
    
    Indeed, we only care about persistence for pages with metadata, but not
    pages with data, as file system consistent are only related to metadate,
    so in order to avoid encountering long latency in above scenario, let's
    recognize and reference metadata in submitted IOs, wait writeback only
    for metadatas.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 59203a3c143e..8c5b63bda68b 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -29,6 +29,26 @@
 #include "trace.h"
 #include <trace/events/f2fs.h>
 
+static bool __is_cp_guaranteed(struct page *page)
+{
+	struct address_space *mapping = page->mapping;
+	struct inode *inode;
+	struct f2fs_sb_info *sbi;
+
+	if (!mapping)
+		return false;
+
+	inode = mapping->host;
+	sbi = F2FS_I_SB(inode);
+
+	if (inode->i_ino == F2FS_META_INO(sbi) ||
+			inode->i_ino ==  F2FS_NODE_INO(sbi) ||
+			S_ISDIR(inode->i_mode) ||
+			is_cold_data(page))
+		return true;
+	return false;
+}
+
 static void f2fs_read_end_io(struct bio *bio)
 {
 	struct bio_vec *bvec;
@@ -71,6 +91,7 @@ static void f2fs_write_end_io(struct bio *bio)
 
 	bio_for_each_segment_all(bvec, bio, i) {
 		struct page *page = bvec->bv_page;
+		enum count_type type = WB_DATA_TYPE(page);
 
 		fscrypt_pullback_bio_page(&page, true);
 
@@ -78,9 +99,11 @@ static void f2fs_write_end_io(struct bio *bio)
 			mapping_set_error(page->mapping, -EIO);
 			f2fs_stop_checkpoint(sbi, true);
 		}
+		dec_page_count(sbi, type);
+		clear_cold_data(page);
 		end_page_writeback(page);
 	}
-	if (atomic_dec_and_test(&sbi->nr_wb_bios) &&
+	if (!get_pages(sbi, F2FS_WB_CP_DATA) &&
 				wq_has_sleeper(&sbi->cp_wait))
 		wake_up(&sbi->cp_wait);
 
@@ -148,7 +171,6 @@ static inline void __submit_bio(struct f2fs_sb_info *sbi,
 				struct bio *bio, enum page_type type)
 {
 	if (!is_read_io(bio_op(bio))) {
-		atomic_inc(&sbi->nr_wb_bios);
 		if (f2fs_sb_mounted_blkzoned(sbi->sb) &&
 			current->plug && (type == DATA || type == NODE))
 			blk_finish_plug(current->plug);
@@ -309,6 +331,11 @@ void f2fs_submit_page_mbio(struct f2fs_io_info *fio)
 		verify_block_addr(sbi, fio->old_blkaddr);
 	verify_block_addr(sbi, fio->new_blkaddr);
 
+	bio_page = fio->encrypted_page ? fio->encrypted_page : fio->page;
+
+	if (!is_read)
+		inc_page_count(sbi, WB_DATA_TYPE(bio_page));
+
 	down_write(&io->io_rwsem);
 
 	if (io->bio && (io->last_block_in_bio != fio->new_blkaddr - 1 ||
@@ -322,8 +349,6 @@ void f2fs_submit_page_mbio(struct f2fs_io_info *fio)
 		io->fio = *fio;
 	}
 
-	bio_page = fio->encrypted_page ? fio->encrypted_page : fio->page;
-
 	if (bio_add_page(io->bio, bio_page, PAGE_SIZE, 0) <
 							PAGE_SIZE) {
 		__submit_merged_bio(io);
@@ -1339,7 +1364,6 @@ static int f2fs_write_data_page(struct page *page,
 	if (err && err != -ENOENT)
 		goto redirty_out;
 
-	clear_cold_data(page);
 out:
 	inode_dec_dirty_pages(inode);
 	if (err)
@@ -1742,7 +1766,6 @@ static int f2fs_write_end(struct file *file,
 		goto unlock_out;
 
 	set_page_dirty(page);
-	clear_cold_data(page);
 
 	if (pos + copied > i_size_read(inode))
 		f2fs_i_size_write(inode, pos + copied);

commit c040ff9d69fd1d782fe577ba9e35c1f5798158ae
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Nov 11 16:46:40 2016 -0800

    f2fs: fix redundant block allocation
    
    In direct_IO path of f2fs_file_write_iter(),
    1. f2fs_preallocate_blocks(F2FS_GET_BLOCK_PRE_DIO)
       -> allocate LBA X
    2. f2fs_direct_IO()
       -> return 0;
    
    Then,
    f2fs_write_data_page() will allocate another LBA X+1.
    
    This makes EIO triggered by HM-SMR.
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 2938311c97bf..59203a3c143e 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -658,6 +658,13 @@ static int __allocate_data_block(struct dnode_of_data *dn)
 	return 0;
 }
 
+static inline bool __force_buffered_io(struct inode *inode, int rw)
+{
+	return ((f2fs_encrypted_inode(inode) && S_ISREG(inode->i_mode)) ||
+			(rw == WRITE && test_opt(F2FS_I_SB(inode), LFS)) ||
+			F2FS_I_SB(inode)->s_ndevs);
+}
+
 int f2fs_preallocate_blocks(struct kiocb *iocb, struct iov_iter *from)
 {
 	struct inode *inode = file_inode(iocb->ki_filp);
@@ -677,7 +684,10 @@ int f2fs_preallocate_blocks(struct kiocb *iocb, struct iov_iter *from)
 		err = f2fs_convert_inline_inode(inode);
 		if (err)
 			return err;
-		return f2fs_map_blocks(inode, &map, 1, F2FS_GET_BLOCK_PRE_DIO);
+		return f2fs_map_blocks(inode, &map, 1,
+			__force_buffered_io(inode, WRITE) ?
+				F2FS_GET_BLOCK_PRE_AIO :
+				F2FS_GET_BLOCK_PRE_DIO);
 	}
 	if (iocb->ki_pos + iov_iter_count(from) > MAX_INLINE_DATA) {
 		err = f2fs_convert_inline_inode(inode);
@@ -1769,11 +1779,7 @@ static ssize_t f2fs_direct_IO(struct kiocb *iocb, struct iov_iter *iter)
 	if (err)
 		return err;
 
-	if (f2fs_encrypted_inode(inode) && S_ISREG(inode->i_mode))
-		return 0;
-	if (rw == WRITE && test_opt(F2FS_I_SB(inode), LFS))
-		return 0;
-	if (F2FS_I_SB(inode)->s_ndevs)
+	if (__force_buffered_io(inode, rw))
 		return 0;
 
 	trace_f2fs_direct_IO_enter(inode, offset, count, rw);

commit a7de608691f766cd148971a71d4f13aa1692d4c8
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Nov 11 16:31:56 2016 -0800

    f2fs: use err for f2fs_preallocate_blocks
    
    This patch has no functional change.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 861173f00459..2938311c97bf 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -658,11 +658,11 @@ static int __allocate_data_block(struct dnode_of_data *dn)
 	return 0;
 }
 
-ssize_t f2fs_preallocate_blocks(struct kiocb *iocb, struct iov_iter *from)
+int f2fs_preallocate_blocks(struct kiocb *iocb, struct iov_iter *from)
 {
 	struct inode *inode = file_inode(iocb->ki_filp);
 	struct f2fs_map_blocks map;
-	ssize_t ret = 0;
+	int err = 0;
 
 	map.m_lblk = F2FS_BLK_ALIGN(iocb->ki_pos);
 	map.m_len = F2FS_BYTES_TO_BLK(iocb->ki_pos + iov_iter_count(from));
@@ -674,19 +674,19 @@ ssize_t f2fs_preallocate_blocks(struct kiocb *iocb, struct iov_iter *from)
 	map.m_next_pgofs = NULL;
 
 	if (iocb->ki_flags & IOCB_DIRECT) {
-		ret = f2fs_convert_inline_inode(inode);
-		if (ret)
-			return ret;
+		err = f2fs_convert_inline_inode(inode);
+		if (err)
+			return err;
 		return f2fs_map_blocks(inode, &map, 1, F2FS_GET_BLOCK_PRE_DIO);
 	}
 	if (iocb->ki_pos + iov_iter_count(from) > MAX_INLINE_DATA) {
-		ret = f2fs_convert_inline_inode(inode);
-		if (ret)
-			return ret;
+		err = f2fs_convert_inline_inode(inode);
+		if (err)
+			return err;
 	}
 	if (!f2fs_has_inline_data(inode))
 		return f2fs_map_blocks(inode, &map, 1, F2FS_GET_BLOCK_PRE_AIO);
-	return ret;
+	return err;
 }
 
 /*
@@ -863,19 +863,19 @@ static int __get_data_block(struct inode *inode, sector_t iblock,
 			pgoff_t *next_pgofs)
 {
 	struct f2fs_map_blocks map;
-	int ret;
+	int err;
 
 	map.m_lblk = iblock;
 	map.m_len = bh->b_size >> inode->i_blkbits;
 	map.m_next_pgofs = next_pgofs;
 
-	ret = f2fs_map_blocks(inode, &map, create, flag);
-	if (!ret) {
+	err = f2fs_map_blocks(inode, &map, create, flag);
+	if (!err) {
 		map_bh(bh, inode->i_sb, map.m_pblk);
 		bh->b_state = (bh->b_state & ~F2FS_MAP_FLAGS) | map.m_flags;
 		bh->b_size = map.m_len << inode->i_blkbits;
 	}
-	return ret;
+	return err;
 }
 
 static int get_data_block(struct inode *inode, sector_t iblock,

commit 3c62be17d4f562f43fe1d03b48194399caa35aa5
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Oct 6 19:02:05 2016 -0700

    f2fs: support multiple devices
    
    This patch implements multiple devices support for f2fs.
    Given multiple devices by mkfs.f2fs, f2fs shows them entirely as one big
    volume under one f2fs instance.
    
    Internal block management is very simple, but we will modify block allocation
    and background GC policy to boost IO speed by exploiting them accoording to
    each device speed.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 996b9aef94ce..861173f00459 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -87,6 +87,46 @@ static void f2fs_write_end_io(struct bio *bio)
 	bio_put(bio);
 }
 
+/*
+ * Return true, if pre_bio's bdev is same as its target device.
+ */
+struct block_device *f2fs_target_device(struct f2fs_sb_info *sbi,
+				block_t blk_addr, struct bio *bio)
+{
+	struct block_device *bdev = sbi->sb->s_bdev;
+	int i;
+
+	for (i = 0; i < sbi->s_ndevs; i++) {
+		if (FDEV(i).start_blk <= blk_addr &&
+					FDEV(i).end_blk >= blk_addr) {
+			blk_addr -= FDEV(i).start_blk;
+			bdev = FDEV(i).bdev;
+			break;
+		}
+	}
+	if (bio) {
+		bio->bi_bdev = bdev;
+		bio->bi_iter.bi_sector = SECTOR_FROM_BLOCK(blk_addr);
+	}
+	return bdev;
+}
+
+int f2fs_target_device_index(struct f2fs_sb_info *sbi, block_t blkaddr)
+{
+	int i;
+
+	for (i = 0; i < sbi->s_ndevs; i++)
+		if (FDEV(i).start_blk <= blkaddr && FDEV(i).end_blk >= blkaddr)
+			return i;
+	return 0;
+}
+
+static bool __same_bdev(struct f2fs_sb_info *sbi,
+				block_t blk_addr, struct bio *bio)
+{
+	return f2fs_target_device(sbi, blk_addr, NULL) == bio->bi_bdev;
+}
+
 /*
  * Low-level block read/write IO operations.
  */
@@ -97,8 +137,7 @@ static struct bio *__bio_alloc(struct f2fs_sb_info *sbi, block_t blk_addr,
 
 	bio = f2fs_bio_alloc(npages);
 
-	bio->bi_bdev = sbi->sb->s_bdev;
-	bio->bi_iter.bi_sector = SECTOR_FROM_BLOCK(blk_addr);
+	f2fs_target_device(sbi, blk_addr, bio);
 	bio->bi_end_io = is_read ? f2fs_read_end_io : f2fs_write_end_io;
 	bio->bi_private = is_read ? NULL : sbi;
 
@@ -273,7 +312,8 @@ void f2fs_submit_page_mbio(struct f2fs_io_info *fio)
 	down_write(&io->io_rwsem);
 
 	if (io->bio && (io->last_block_in_bio != fio->new_blkaddr - 1 ||
-	    (io->fio.op != fio->op || io->fio.op_flags != fio->op_flags)))
+	    (io->fio.op != fio->op || io->fio.op_flags != fio->op_flags) ||
+			!__same_bdev(sbi, fio->new_blkaddr, io->bio)))
 		__submit_merged_bio(io);
 alloc_new:
 	if (io->bio == NULL) {
@@ -961,7 +1001,6 @@ static struct bio *f2fs_grab_bio(struct inode *inode, block_t blkaddr,
 {
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
 	struct fscrypt_ctx *ctx = NULL;
-	struct block_device *bdev = sbi->sb->s_bdev;
 	struct bio *bio;
 
 	if (f2fs_encrypted_inode(inode) && S_ISREG(inode->i_mode)) {
@@ -979,8 +1018,7 @@ static struct bio *f2fs_grab_bio(struct inode *inode, block_t blkaddr,
 			fscrypt_release_ctx(ctx);
 		return ERR_PTR(-ENOMEM);
 	}
-	bio->bi_bdev = bdev;
-	bio->bi_iter.bi_sector = SECTOR_FROM_BLOCK(blkaddr);
+	f2fs_target_device(sbi, blkaddr, bio);
 	bio->bi_end_io = f2fs_read_end_io;
 	bio->bi_private = ctx;
 
@@ -1075,7 +1113,8 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 		 * This page will go to BIO.  Do we need to send this
 		 * BIO off first?
 		 */
-		if (bio && (last_block_in_bio != block_nr - 1)) {
+		if (bio && (last_block_in_bio != block_nr - 1 ||
+			!__same_bdev(F2FS_I_SB(inode), block_nr, bio))) {
 submit_and_realloc:
 			__submit_bio(F2FS_I_SB(inode), bio, DATA);
 			bio = NULL;
@@ -1734,6 +1773,8 @@ static ssize_t f2fs_direct_IO(struct kiocb *iocb, struct iov_iter *iter)
 		return 0;
 	if (rw == WRITE && test_opt(F2FS_I_SB(inode), LFS))
 		return 0;
+	if (F2FS_I_SB(inode)->s_ndevs)
+		return 0;
 
 	trace_f2fs_direct_IO_enter(inode, offset, count, rw);
 

commit e57e9ae5b179a6b243c42bf6d9549d1595c27089
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Nov 11 12:08:22 2016 -0800

    f2fs: allow dio read for LFS mode
    
    We can allow dio reads for LFS mode, while doing buffered writes for dio writes.
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index ca53da568633..996b9aef94ce 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1732,7 +1732,7 @@ static ssize_t f2fs_direct_IO(struct kiocb *iocb, struct iov_iter *iter)
 
 	if (f2fs_encrypted_inode(inode) && S_ISREG(inode->i_mode))
 		return 0;
-	if (test_opt(F2FS_I_SB(inode), LFS))
+	if (rw == WRITE && test_opt(F2FS_I_SB(inode), LFS))
 		return 0;
 
 	trace_f2fs_direct_IO_enter(inode, offset, count, rw);

commit 6ae1be13e85f4c42c8ca371fda50ae39eebbfd96
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Nov 11 12:31:40 2016 -0800

    f2fs: revert segment allocation for direct IO
    
    Now we don't need to be too much careful about storage alignment for dio, since
    its speed becomes quite fast and we'd better avoid any misalignment first.
    
    Revert: 38aa0889b250 (f2fs: align direct_io'ed data to section)
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 47ded0c34df1..ca53da568633 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -588,7 +588,6 @@ static int __allocate_data_block(struct dnode_of_data *dn)
 	struct f2fs_sb_info *sbi = F2FS_I_SB(dn->inode);
 	struct f2fs_summary sum;
 	struct node_info ni;
-	int seg = CURSEG_WARM_DATA;
 	pgoff_t fofs;
 	blkcnt_t count = 1;
 
@@ -606,11 +605,8 @@ static int __allocate_data_block(struct dnode_of_data *dn)
 	get_node_info(sbi, dn->nid, &ni);
 	set_summary(&sum, dn->nid, dn->ofs_in_node, ni.version);
 
-	if (dn->ofs_in_node == 0 && dn->inode_page == dn->node_page)
-		seg = CURSEG_DIRECT_IO;
-
 	allocate_data_block(sbi, NULL, dn->data_blkaddr, &dn->data_blkaddr,
-								&sum, seg);
+						&sum, CURSEG_WARM_DATA);
 	set_data_blkaddr(dn);
 
 	/* update i_size */

commit 0bfd7a091c19132489a0f977b8dbf9f6b5ae0a1c
Author: Damien Le Moal <damien.lemoal@wdc.com>
Date:   Fri Oct 28 17:45:00 2016 +0900

    f2fs: Use generic zoned block device terminology
    
    SMR stands for "Shingled Magnetic Recording" which makes sense
    only for hard disk drives (spinning rust). The ZBC/ZAC standards
    enable management of SMR disks, but solid state drives may also
    support those standards. So rename the HMSMR feature to BLKZONED
    to avoid a HDD centric terminology. For the same reason, rename
    f2fs_sb_mounted_hmsmr to f2fs_sb_mounted_blkzoned.
    
    Signed-off-by: Damien Le Moal <damien.lemoal@wdc.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index cf5ec39f907d..47ded0c34df1 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -110,7 +110,7 @@ static inline void __submit_bio(struct f2fs_sb_info *sbi,
 {
 	if (!is_read_io(bio_op(bio))) {
 		atomic_inc(&sbi->nr_wb_bios);
-		if (f2fs_sb_mounted_hmsmr(sbi->sb) &&
+		if (f2fs_sb_mounted_blkzoned(sbi->sb) &&
 			current->plug && (type == DATA || type == NODE))
 			blk_finish_plug(current->plug);
 	}

commit 230436b3ef3fd7d4a1da19edf5e87bb2d74e0fc2
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Wed Nov 2 14:52:15 2016 +0100

    f2fs: hide a maybe-uninitialized warning
    
    gcc is unsure about the use of last_ofs_in_node, which might happen
    without a prior initialization:
    
    fs/f2fs//git/arm-soc/fs/f2fs/data.c: In function ‘f2fs_map_blocks’:
    fs/f2fs/data.c:799:54: warning: ‘last_ofs_in_node’ may be used uninitialized in this function [-Wmaybe-uninitialized]
       if (prealloc && dn.ofs_in_node != last_ofs_in_node + 1) {
    
    As pointed out by Chao Yu, the code is actually correct as 'prealloc'
    is only set if the last_ofs_in_node has been set, the two always
    get updated together.
    
    This initializes last_ofs_in_node to dn.ofs_in_node for each
    new dnode at the start of the 'next_block' loop, which at that
    point is a correct initialization as well. I assume that compilers
    that correctly track the contents of the variables and do not
    warn about the condition also figure out that they can eliminate
    the extra assignment here.
    
    Fixes: 46008c6d4232 ("f2fs: support in batch multi blocks preallocation")
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 68edb47f5f71..cf5ec39f907d 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -713,7 +713,7 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 	}
 
 	prealloc = 0;
-	ofs_in_node = dn.ofs_in_node;
+	last_ofs_in_node = ofs_in_node = dn.ofs_in_node;
 	end_offset = ADDRS_PER_PAGE(dn.node_page, inode);
 
 next_block:

commit 664ba972df9b96942191db3068274cc1db899774
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Tue Oct 18 11:07:45 2016 -0700

    f2fs: use BIO_MAX_PAGES for bio allocation
    
    We don't need to allocate bio partially in order to maximize sequential writes.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index b544e0b2e7e5..68edb47f5f71 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -277,10 +277,8 @@ void f2fs_submit_page_mbio(struct f2fs_io_info *fio)
 		__submit_merged_bio(io);
 alloc_new:
 	if (io->bio == NULL) {
-		int bio_blocks = MAX_BIO_BLOCKS(sbi);
-
 		io->bio = __bio_alloc(sbi, fio->new_blkaddr,
-						bio_blocks, is_read);
+						BIO_MAX_PAGES, is_read);
 		io->fio = *fio;
 	}
 

commit 58736fa60f6ae659ac72da8b1580c308b47e8edd
Author: Chao Yu <yuchao0@huawei.com>
Date:   Tue Oct 11 22:57:04 2016 +0800

    f2fs: be aware of extent beyond EOF in fiemap
    
    f2fs can support fallocating blocks beyond file size without changing the
    size, but ->fiemap of f2fs was restricted and can't detect these extents
    fallocated past EOF, now relieve the restriction.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 7fb081516f5f..b544e0b2e7e5 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -886,7 +886,6 @@ int f2fs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,
 	struct buffer_head map_bh;
 	sector_t start_blk, last_blk;
 	pgoff_t next_pgofs;
-	loff_t isize;
 	u64 logical = 0, phys = 0, size = 0;
 	u32 flags = 0;
 	int ret = 0;
@@ -903,13 +902,6 @@ int f2fs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,
 
 	inode_lock(inode);
 
-	isize = i_size_read(inode);
-	if (start >= isize)
-		goto out;
-
-	if (start + len > isize)
-		len = isize - start;
-
 	if (logical_to_blk(inode, len) == 0)
 		len = blk_to_logical(inode, 1);
 
@@ -928,13 +920,11 @@ int f2fs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,
 	/* HOLE */
 	if (!buffer_mapped(&map_bh)) {
 		start_blk = next_pgofs;
-		/* Go through holes util pass the EOF */
-		if (blk_to_logical(inode, start_blk) < isize)
+
+		if (blk_to_logical(inode, start_blk) < blk_to_logical(inode,
+					F2FS_I_SB(inode)->max_file_blocks))
 			goto prep_next;
-		/* Found a hole beyond isize means no more extents.
-		 * Note that the premise is that filesystems don't
-		 * punch holes beyond isize and keep size unchanged.
-		 */
+
 		flags |= FIEMAP_EXTENT_LAST;
 	}
 

commit 6f2d8ed654bfa391854df4de854953f772a16a9d
Author: Chao Yu <yuchao0@huawei.com>
Date:   Tue Oct 11 22:57:03 2016 +0800

    f2fs: don't miss any f2fs_balance_fs cases
    
    In f2fs_map_blocks, let f2fs_balance_fs detects node page modification
    with dn.node_changed to avoid miss some corner cases.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index b2289dfab9a2..7fb081516f5f 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -676,7 +676,6 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 	unsigned int ofs_in_node, last_ofs_in_node;
 	blkcnt_t prealloc;
 	struct extent_info ei;
-	bool allocated = false;
 	block_t blkaddr;
 
 	if (!maxblocks)
@@ -735,10 +734,8 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 				}
 			} else {
 				err = __allocate_data_block(&dn);
-				if (!err) {
+				if (!err)
 					set_inode_flag(inode, FI_APPEND_WRITE);
-					allocated = true;
-				}
 			}
 			if (err)
 				goto sync_out;
@@ -793,7 +790,6 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 		err = reserve_new_blocks(&dn, prealloc);
 		if (err)
 			goto sync_out;
-		allocated = dn.node_changed;
 
 		map->m_len += dn.ofs_in_node - ofs_in_node;
 		if (prealloc && dn.ofs_in_node != last_ofs_in_node + 1) {
@@ -812,9 +808,8 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 
 	if (create) {
 		f2fs_unlock_op(sbi);
-		f2fs_balance_fs(sbi, allocated);
+		f2fs_balance_fs(sbi, dn.node_changed);
 	}
-	allocated = false;
 	goto next_dnode;
 
 sync_out:
@@ -822,7 +817,7 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 unlock_out:
 	if (create) {
 		f2fs_unlock_op(sbi);
-		f2fs_balance_fs(sbi, allocated);
+		f2fs_balance_fs(sbi, dn.node_changed);
 	}
 out:
 	trace_f2fs_map_blocks(inode, map, err);

commit 933439c8f3474e329709b715b43b0b8168bbecf8
Author: Chao Yu <yuchao0@huawei.com>
Date:   Tue Oct 11 22:57:01 2016 +0800

    f2fs: give a chance to detach from dirty list
    
    If there is no dirty pages in inode, we should give a chance to detach
    the inode from global dirty list, otherwise it needs to call another
    unnecessary .writepages for detaching.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 9ae194fd2fdb..b2289dfab9a2 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1785,12 +1785,14 @@ void f2fs_invalidate_page(struct page *page, unsigned int offset,
 		return;
 
 	if (PageDirty(page)) {
-		if (inode->i_ino == F2FS_META_INO(sbi))
+		if (inode->i_ino == F2FS_META_INO(sbi)) {
 			dec_page_count(sbi, F2FS_DIRTY_META);
-		else if (inode->i_ino == F2FS_NODE_INO(sbi))
+		} else if (inode->i_ino == F2FS_NODE_INO(sbi)) {
 			dec_page_count(sbi, F2FS_DIRTY_NODES);
-		else
+		} else {
 			inode_dec_dirty_pages(inode);
+			remove_dirty_inode(inode);
+		}
 	}
 
 	/* This is atomic written page, keep Private */

commit 9c4bb8a3a9b4de21753053d667310c2b7cb39916
Author: David Gstir <david@sigma-star.at>
Date:   Sun Nov 13 22:20:48 2016 +0100

    fscrypt: Let fs select encryption index/tweak
    
    Avoid re-use of page index as tweak for AES-XTS when multiple parts of
    same page are encrypted. This will happen on multiple (partial) calls of
    fscrypt_encrypt_page on same page.
    page->index is only valid for writeback pages.
    
    Signed-off-by: David Gstir <david@sigma-star.at>
    Signed-off-by: Richard Weinberger <richard@nod.at>
    Signed-off-by: Theodore Ts'o <tytso@mit.edu>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index fac207254e8d..435590c4b341 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1196,8 +1196,9 @@ int do_write_data_page(struct f2fs_io_info *fio)
 retry_encrypt:
 		BUG_ON(!PageLocked(fio->page));
 		fio->encrypted_page = fscrypt_encrypt_page(inode, fio->page,
-								PAGE_SIZE, 0,
-								gfp_flags);
+							PAGE_SIZE, 0,
+							fio->page->index,
+							gfp_flags);
 		if (IS_ERR(fio->encrypted_page)) {
 			err = PTR_ERR(fio->encrypted_page);
 			if (err == -ENOMEM) {

commit 7821d4dd4589ce5af54f3e46d04a29439ba3c2e5
Author: David Gstir <david@sigma-star.at>
Date:   Sun Nov 13 22:20:46 2016 +0100

    fscrypt: Enable partial page encryption
    
    Not all filesystems work on full pages, thus we should allow them to
    hand partial pages to fscrypt for en/decryption.
    
    Signed-off-by: David Gstir <david@sigma-star.at>
    Signed-off-by: Richard Weinberger <richard@nod.at>
    Signed-off-by: Theodore Ts'o <tytso@mit.edu>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 9ae194fd2fdb..fac207254e8d 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1194,7 +1194,9 @@ int do_write_data_page(struct f2fs_io_info *fio)
 		f2fs_wait_on_encrypted_page_writeback(F2FS_I_SB(inode),
 							fio->old_blkaddr);
 retry_encrypt:
+		BUG_ON(!PageLocked(fio->page));
 		fio->encrypted_page = fscrypt_encrypt_page(inode, fio->page,
+								PAGE_SIZE, 0,
 								gfp_flags);
 		if (IS_ERR(fio->encrypted_page)) {
 			err = PTR_ERR(fio->encrypted_page);

commit 7637241e651ec36e409412869f986dd5f097735f
Author: Jens Axboe <axboe@fb.com>
Date:   Tue Nov 1 10:00:38 2016 -0600

    writeback: add wbc_to_write_flags()
    
    Add wbc_to_write_flags(), which returns the write modifier flags to use,
    based on a struct writeback_control. No functional changes in this
    patch, but it prepares us for factoring other wbc fields for write type.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>
    Reviewed-by: Jan Kara <jack@suse.cz>
    Reviewed-by: Christoph Hellwig <hch@lst.de>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index b80bf10603d7..9e5561fa4cb6 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1249,7 +1249,7 @@ static int f2fs_write_data_page(struct page *page,
 		.sbi = sbi,
 		.type = DATA,
 		.op = REQ_OP_WRITE,
-		.op_flags = (wbc->sync_mode == WB_SYNC_ALL) ? REQ_SYNC : 0,
+		.op_flags = wbc_to_write_flags(wbc),
 		.page = page,
 		.encrypted_page = NULL,
 	};

commit 70fd76140a6cb63262bd47b68d57b42e889c10ee
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Nov 1 07:40:10 2016 -0600

    block,fs: use REQ_* flags directly
    
    Remove the WRITE_* and READ_SYNC wrappers, and just use the flags
    directly.  Where applicable this also drops usage of the
    bio_set_op_attrs wrapper.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 9ae194fd2fdb..b80bf10603d7 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -198,11 +198,9 @@ static void __f2fs_submit_merged_bio(struct f2fs_sb_info *sbi,
 	if (type >= META_FLUSH) {
 		io->fio.type = META_FLUSH;
 		io->fio.op = REQ_OP_WRITE;
-		if (test_opt(sbi, NOBARRIER))
-			io->fio.op_flags = WRITE_FLUSH | REQ_META | REQ_PRIO;
-		else
-			io->fio.op_flags = WRITE_FLUSH_FUA | REQ_META |
-								REQ_PRIO;
+		io->fio.op_flags = REQ_PREFLUSH | REQ_META | REQ_PRIO;
+		if (!test_opt(sbi, NOBARRIER))
+			io->fio.op_flags |= REQ_FUA;
 	}
 	__submit_merged_bio(io);
 out:
@@ -483,7 +481,7 @@ struct page *find_data_page(struct inode *inode, pgoff_t index)
 		return page;
 	f2fs_put_page(page, 0);
 
-	page = get_read_data_page(inode, index, READ_SYNC, false);
+	page = get_read_data_page(inode, index, 0, false);
 	if (IS_ERR(page))
 		return page;
 
@@ -509,7 +507,7 @@ struct page *get_lock_data_page(struct inode *inode, pgoff_t index,
 	struct address_space *mapping = inode->i_mapping;
 	struct page *page;
 repeat:
-	page = get_read_data_page(inode, index, READ_SYNC, for_write);
+	page = get_read_data_page(inode, index, 0, for_write);
 	if (IS_ERR(page))
 		return page;
 
@@ -1251,7 +1249,7 @@ static int f2fs_write_data_page(struct page *page,
 		.sbi = sbi,
 		.type = DATA,
 		.op = REQ_OP_WRITE,
-		.op_flags = (wbc->sync_mode == WB_SYNC_ALL) ? WRITE_SYNC : 0,
+		.op_flags = (wbc->sync_mode == WB_SYNC_ALL) ? REQ_SYNC : 0,
 		.page = page,
 		.encrypted_page = NULL,
 	};
@@ -1663,7 +1661,7 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 			err = PTR_ERR(bio);
 			goto fail;
 		}
-		bio_set_op_attrs(bio, REQ_OP_READ, READ_SYNC);
+		bio->bi_opf = REQ_OP_READ;
 		if (bio_add_page(bio, page, PAGE_SIZE, 0) < PAGE_SIZE) {
 			bio_put(bio);
 			err = -EFAULT;

commit 5114a97a8bce7f4ead29a32b67dee85438699b9e
Author: Michal Hocko <mhocko@suse.com>
Date:   Tue Oct 11 13:56:01 2016 -0700

    fs: use mapping_set_error instead of opencoded set_bit
    
    The mapping_set_error() helper sets the correct AS_ flag for the mapping
    so there is no reason to open code it.  Use the helper directly.
    
    [akpm@linux-foundation.org: be honest about conversion from -ENXIO to -EIO]
    Link: http://lkml.kernel.org/r/20160912111608.2588-2-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 0d0177c9149c..9ae194fd2fdb 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -75,7 +75,7 @@ static void f2fs_write_end_io(struct bio *bio)
 		fscrypt_pullback_bio_page(&page, true);
 
 		if (unlikely(bio->bi_error)) {
-			set_bit(AS_EIO, &page->mapping->flags);
+			mapping_set_error(page->mapping, -EIO);
 			f2fs_stop_checkpoint(sbi, true);
 		}
 		end_page_writeback(page);

commit 6ca56ca429aa94399534ec00598f7f9847c4cae2
Author: Chao Yu <yuchao0@huawei.com>
Date:   Thu Sep 29 18:50:11 2016 +0800

    f2fs: don't submit irrelevant page
    
    While we call ->writepages, there are two cases:
    a. we didn't writeout any dirty pages, since they are writebacked by other
    thread concurrently.
    b. we writeout dirty pages, and have already submitted bio to block layer.
    
    In these cases, we don't need to do additional bio flushing unnecessarily,
    it may split bio in cache into smaller one.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 8b9a1dc151b2..0d0177c9149c 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1355,6 +1355,7 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 	int cycled;
 	int range_whole = 0;
 	int tag;
+	int nwritten = 0;
 
 	pagevec_init(&pvec, 0);
 
@@ -1429,6 +1430,8 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 				done_index = page->index + 1;
 				done = 1;
 				break;
+			} else {
+				nwritten++;
 			}
 
 			if (--wbc->nr_to_write <= 0 &&
@@ -1450,6 +1453,10 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 	if (wbc->range_cyclic || (range_whole && wbc->nr_to_write > 0))
 		mapping->writeback_index = done_index;
 
+	if (nwritten)
+		f2fs_submit_merged_bio_cond(F2FS_M_SB(mapping), mapping->host,
+							NULL, 0, DATA, WRITE);
+
 	return ret;
 }
 
@@ -1491,7 +1498,6 @@ static int f2fs_write_data_pages(struct address_space *mapping,
 	 * if some pages were truncated, we cannot guarantee its mapping->host
 	 * to detect pending bios.
 	 */
-	f2fs_submit_merged_bio(sbi, DATA, WRITE);
 
 	remove_dirty_inode(inode);
 	return ret;

commit 1ecc0c5c50ce8834f7e35b63be7480bf1aaa4155
Author: Chao Yu <yuchao0@huawei.com>
Date:   Fri Sep 23 21:30:09 2016 +0800

    f2fs: support configuring fault injection per superblock
    
    Previously, we only support global fault injection configuration, so that
    when we configure type/rate of fault injection through sysfs, mount
    option, it will influence all f2fs partition which is being used.
    
    It is not make sence, since it will be not convenient if developer want
    to test separated partitions with different fault injection rate/type
    simultaneously, also it's not possible to enable fault injection in one
    partition and disable fault injection in other one.
    
    >From now on, we move global configuration of fault injection in module
    into per-superblock, hence injection testing can be more flexible.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index ed834cd02502..8b9a1dc151b2 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -35,7 +35,7 @@ static void f2fs_read_end_io(struct bio *bio)
 	int i;
 
 #ifdef CONFIG_F2FS_FAULT_INJECTION
-	if (time_to_inject(FAULT_IO))
+	if (time_to_inject(F2FS_P_SB(bio->bi_io_vec->bv_page), FAULT_IO))
 		bio->bi_error = -EIO;
 #endif
 

commit 5b7a487cf32d3a266fea83d590d3226b5ad817a7
Author: Weichao Guo <guoweichao@huawei.com>
Date:   Tue Sep 20 05:03:27 2016 +0800

    f2fs: add customized migrate_page callback
    
    This patch improves the migration of dirty pages and allows migrating atomic
    written pages that F2FS uses in Page Cache. Instead of the fallback releasing
    page path, it provides better performance for memory compaction, CMA and other
    users of memory page migrating. For dirty pages, there is no need to write back
    first when migrating. For an atomic written page before committing, we can
    migrate the page and update the related 'inmem_pages' list at the same time.
    
    Signed-off-by: Weichao Guo <guoweichao@huawei.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    [Jaegeuk Kim: fix some coding style]
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 637b81d0f9f0..ed834cd02502 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1883,6 +1883,58 @@ static sector_t f2fs_bmap(struct address_space *mapping, sector_t block)
 	return generic_block_bmap(mapping, block, get_data_block_bmap);
 }
 
+#ifdef CONFIG_MIGRATION
+#include <linux/migrate.h>
+
+int f2fs_migrate_page(struct address_space *mapping,
+		struct page *newpage, struct page *page, enum migrate_mode mode)
+{
+	int rc, extra_count;
+	struct f2fs_inode_info *fi = F2FS_I(mapping->host);
+	bool atomic_written = IS_ATOMIC_WRITTEN_PAGE(page);
+
+	BUG_ON(PageWriteback(page));
+
+	/* migrating an atomic written page is safe with the inmem_lock hold */
+	if (atomic_written && !mutex_trylock(&fi->inmem_lock))
+		return -EAGAIN;
+
+	/*
+	 * A reference is expected if PagePrivate set when move mapping,
+	 * however F2FS breaks this for maintaining dirty page counts when
+	 * truncating pages. So here adjusting the 'extra_count' make it work.
+	 */
+	extra_count = (atomic_written ? 1 : 0) - page_has_private(page);
+	rc = migrate_page_move_mapping(mapping, newpage,
+				page, NULL, mode, extra_count);
+	if (rc != MIGRATEPAGE_SUCCESS) {
+		if (atomic_written)
+			mutex_unlock(&fi->inmem_lock);
+		return rc;
+	}
+
+	if (atomic_written) {
+		struct inmem_pages *cur;
+		list_for_each_entry(cur, &fi->inmem_pages, list)
+			if (cur->page == page) {
+				cur->page = newpage;
+				break;
+			}
+		mutex_unlock(&fi->inmem_lock);
+		put_page(page);
+		get_page(newpage);
+	}
+
+	if (PagePrivate(page))
+		SetPagePrivate(newpage);
+	set_page_private(newpage, page_private(page));
+
+	migrate_page_copy(newpage, page);
+
+	return MIGRATEPAGE_SUCCESS;
+}
+#endif
+
 const struct address_space_operations f2fs_dblock_aops = {
 	.readpage	= f2fs_read_data_page,
 	.readpages	= f2fs_read_data_pages,
@@ -1895,4 +1947,7 @@ const struct address_space_operations f2fs_dblock_aops = {
 	.releasepage	= f2fs_release_page,
 	.direct_IO	= f2fs_direct_IO,
 	.bmap		= f2fs_bmap,
+#ifdef CONFIG_MIGRATION
+	.migratepage    = f2fs_migrate_page,
+#endif
 };

commit 5d4c0af41fd4cc26cb75af4f3de7fb63c91209c1
Author: Yunlei He <heyunlei@huawei.com>
Date:   Sun Sep 18 08:16:56 2016 +0800

    f2fs: preallocate blocks for encrypted file
    
    This patch allow preallocates data blocks for buffered aio writes
    in encrypted file.
    
    Signed-off-by: Yunlei He <heyunlei@huawei.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    [Jaegeuk Kim: fix to avoid BUG_ON]
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 25e3c302b72f..637b81d0f9f0 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -639,9 +639,6 @@ ssize_t f2fs_preallocate_blocks(struct kiocb *iocb, struct iov_iter *from)
 
 	map.m_next_pgofs = NULL;
 
-	if (f2fs_encrypted_inode(inode))
-		return 0;
-
 	if (iocb->ki_flags & IOCB_DIRECT) {
 		ret = f2fs_convert_inline_inode(inode);
 		if (ret)
@@ -1532,8 +1529,7 @@ static int prepare_write_begin(struct f2fs_sb_info *sbi,
 	 * we already allocated all the blocks, so we don't need to get
 	 * the block addresses when there is no need to fill the page.
 	 */
-	if (!f2fs_has_inline_data(inode) && !f2fs_encrypted_inode(inode) &&
-					len == PAGE_SIZE)
+	if (!f2fs_has_inline_data(inode) && len == PAGE_SIZE)
 		return 0;
 
 	if (f2fs_has_inline_data(inode) ||

commit 8b038c70dfe4fd7b62573917a9e976f826ac6ad3
Author: Chao Yu <yuchao0@huawei.com>
Date:   Sun Sep 18 23:30:07 2016 +0800

    f2fs: support IO error injection
    
    This patch adds to support IO error injection for testing IO error
    tolerance of f2fs.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 528c3c0d55a2..25e3c302b72f 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -34,6 +34,11 @@ static void f2fs_read_end_io(struct bio *bio)
 	struct bio_vec *bvec;
 	int i;
 
+#ifdef CONFIG_F2FS_FAULT_INJECTION
+	if (time_to_inject(FAULT_IO))
+		bio->bi_error = -EIO;
+#endif
+
 	if (f2fs_bio_encrypted(bio)) {
 		if (bio->bi_error) {
 			fscrypt_release_ctx(bio->bi_private);

commit 649d7df29ca83b2c9e81a4a305a8de8ab02b5e9d
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Tue Sep 6 11:02:03 2016 -0700

    f2fs: fix to set PageUptodate in f2fs_write_end correctly
    
    Previously, f2fs_write_begin sets PageUptodate all the time. But, when user
    tries to update the entire page (i.e., len == PAGE_SIZE), we need to consider
    that the page is able to be copied partially afterwards. In such the case,
    we will lose the remaing region in the page.
    
    This patch fixes this by setting PageUptodate in f2fs_write_end as given copied
    result. In the short copy case, it returns zero to let generic_perform_write
    retry copying user data again.
    
    As a result, f2fs_write_end() works:
       PageUptodate      len      copied    return   retry
    1. no                4096     4096      4096     false  -> return 4096
    2. no                4096     1024      0        true   -> goto #1 case
    3. yes               2048     2048      2048     false  -> return 2048
    4. yes               2048     1024      1024     false  -> return 1024
    
    Suggested-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 357a4235dde8..528c3c0d55a2 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1642,13 +1642,12 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 	if (f2fs_encrypted_inode(inode) && S_ISREG(inode->i_mode))
 		f2fs_wait_on_encrypted_page_writeback(sbi, blkaddr);
 
-	if (len == PAGE_SIZE)
-		goto out_update;
-	if (PageUptodate(page))
-		goto out_clear;
+	if (len == PAGE_SIZE || PageUptodate(page))
+		return 0;
 
 	if (blkaddr == NEW_ADDR) {
 		zero_user_segment(page, 0, PAGE_SIZE);
+		SetPageUptodate(page);
 	} else {
 		struct bio *bio;
 
@@ -1676,11 +1675,6 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 			goto fail;
 		}
 	}
-out_update:
-	if (!PageUptodate(page))
-		SetPageUptodate(page);
-out_clear:
-	clear_cold_data(page);
 	return 0;
 
 fail:
@@ -1698,11 +1692,26 @@ static int f2fs_write_end(struct file *file,
 
 	trace_f2fs_write_end(inode, pos, len, copied);
 
+	/*
+	 * This should be come from len == PAGE_SIZE, and we expect copied
+	 * should be PAGE_SIZE. Otherwise, we treat it with zero copied and
+	 * let generic_perform_write() try to copy data again through copied=0.
+	 */
+	if (!PageUptodate(page)) {
+		if (unlikely(copied != PAGE_SIZE))
+			copied = 0;
+		else
+			SetPageUptodate(page);
+	}
+	if (!copied)
+		goto unlock_out;
+
 	set_page_dirty(page);
+	clear_cold_data(page);
 
 	if (pos + copied > i_size_read(inode))
 		f2fs_i_size_write(inode, pos + copied);
-
+unlock_out:
 	f2fs_put_page(page, 1);
 	f2fs_update_time(F2FS_I_SB(inode), REQ_TIME);
 	return copied;

commit 7f3037a5ec0672e03f96d4b0b86169c4c48e479e
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Sep 1 12:02:51 2016 -0700

    f2fs: check free_sections for defragmentation
    
    Fix wrong condition check for defragmentation of a file.
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 8ffb480935b3..357a4235dde8 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1293,7 +1293,7 @@ static int f2fs_write_data_page(struct page *page,
 
 	if (!wbc->for_reclaim)
 		need_balance_fs = true;
-	else if (has_not_enough_free_secs(sbi, 0))
+	else if (has_not_enough_free_secs(sbi, 0, 0))
 		goto redirty_out;
 
 	err = -EAGAIN;
@@ -1625,7 +1625,7 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 	if (err)
 		goto fail;
 
-	if (need_balance && has_not_enough_free_secs(sbi, 0)) {
+	if (need_balance && has_not_enough_free_secs(sbi, 0, 0)) {
 		unlock_page(page);
 		f2fs_balance_fs(sbi, true);
 		lock_page(page);

commit 68f313935fb205822ed1f923f7833639f3c78573
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Tue Sep 6 13:31:56 2016 -0700

    f2fs: no need to make zeros beyond i_size
    
    We don't need to make zeros beyond i_size, since we already wrote that through
    NEW_ADDR case.
    
    Reported-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 7c8e219f93dc..8ffb480935b3 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1647,15 +1647,6 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 	if (PageUptodate(page))
 		goto out_clear;
 
-	if ((pos & PAGE_MASK) >= i_size_read(inode)) {
-		unsigned start = pos & (PAGE_SIZE - 1);
-		unsigned end = start + len;
-
-		/* Reading beyond i_size is simple: memset to zero */
-		zero_user_segments(page, 0, start, end, PAGE_SIZE);
-		goto out_update;
-	}
-
 	if (blkaddr == NEW_ADDR) {
 		zero_user_segment(page, 0, PAGE_SIZE);
 	} else {

commit dfd02e4de1c5f40c268984254045d388ab0c3e74
Author: Chao Yu <yuchao0@huawei.com>
Date:   Sat Aug 20 15:12:01 2016 +0800

    f2fs: fix to preallocate block only aligned to 4K
    
    In write_begin(), we skip checking dnode block for preallocating block
    when whole block needs to be updated since we preallocated its block in
    f2fs_preallocate_blocks, for partial updated block, we will still try
    to lock its node and do preallocation in write_begin(), so in
    f2fs_preallocate_blocks we should not preallocate its block.
    
    But previously, the calculation of preallocating block number is
    incorrect, fix it.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    [Jaegeuk Kim: fix a bug]
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index ced6beb5debc..7c8e219f93dc 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -626,7 +626,12 @@ ssize_t f2fs_preallocate_blocks(struct kiocb *iocb, struct iov_iter *from)
 	ssize_t ret = 0;
 
 	map.m_lblk = F2FS_BLK_ALIGN(iocb->ki_pos);
-	map.m_len = F2FS_BYTES_TO_BLK(iov_iter_count(from));
+	map.m_len = F2FS_BYTES_TO_BLK(iocb->ki_pos + iov_iter_count(from));
+	if (map.m_len > map.m_lblk)
+		map.m_len -= map.m_lblk;
+	else
+		map.m_len = 0;
+
 	map.m_next_pgofs = NULL;
 
 	if (f2fs_encrypted_inode(inode))
@@ -672,6 +677,9 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 	bool allocated = false;
 	block_t blkaddr;
 
+	if (!maxblocks)
+		return 0;
+
 	map->m_len = 0;
 	map->m_flags = 0;
 

commit 6a7a3aedd562838fd402cccb5ad07b8063a3582e
Author: Wei Yongjun <weiyongjun1@huawei.com>
Date:   Tue Aug 23 15:23:59 2016 +0000

    f2fs: fix non static symbol warning
    
    Fixes the following sparse warning:
    
    fs/f2fs/data.c:969:12: warning:
     symbol 'f2fs_grab_bio' was not declared. Should it be static?
    
    Signed-off-by: Wei Yongjun <weiyongjun1@huawei.com>
    Acked-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 7d618a969524..ced6beb5debc 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -967,8 +967,8 @@ int f2fs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,
 	return ret;
 }
 
-struct bio *f2fs_grab_bio(struct inode *inode, block_t blkaddr,
-							unsigned nr_pages)
+static struct bio *f2fs_grab_bio(struct inode *inode, block_t blkaddr,
+				 unsigned nr_pages)
 {
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
 	struct fscrypt_ctx *ctx = NULL;

commit 58383befc3377b4e2305b98f91e445af73ba8d62
Author: Chao Yu <yuchao0@huawei.com>
Date:   Sat Aug 20 15:12:02 2016 +0800

    f2fs: fix to do f2fs_balance_fs in f2fs_map_blocks correctly
    
    If we preallocate blocks with f2fs_reserve_blocks in f2fs_map_blocks, we
    should call f2fs_balance_fs for checking and reclaiming space, fix it.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index ccb401eebc11..7d618a969524 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -783,6 +783,7 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 		err = reserve_new_blocks(&dn, prealloc);
 		if (err)
 			goto sync_out;
+		allocated = dn.node_changed;
 
 		map->m_len += dn.ofs_in_node - ofs_in_node;
 		if (prealloc && dn.ofs_in_node != last_ofs_in_node + 1) {

commit 3024c9a1fefb3ac0d1b0b078a2e3f2f69478daab
Author: Chao Yu <yuchao0@huawei.com>
Date:   Sat Aug 6 21:09:41 2016 +0800

    Revert "f2fs: move i_size_write in f2fs_write_end"
    
    This reverts commit a2ee0a300344a6da76186129b078113354fe13d2.
    
    When testing with generic/032 of xfstest suit, failure message will be
    reported as below:
    
    generic/032 8s ... [failed, exit status 1] - output mismatch (see results/generic/032.out.bad)
        --- tests/generic/032.out   2015-01-11 16:52:27.643681072 +0800
        +++ results/generic/032.out.bad     2016-08-06 13:44:43.861330500 +0800
        @@ -1,5 +1,5 @@
         QA output created by 032
        -100 iterations
        -0000000 cdcd cdcd cdcd cdcd cdcd cdcd cdcd cdcd
        -*
        -0100000
        +1: [768..775]: unwritten
        +Unwritten extents found!
        ...
        (Run 'diff -u tests/generic/032.out results/generic/032.out.bad'  to see the entire diff)
    Ran: generic/032
    Failures: generic/032
    Failed 1 of 1 tests
    
    In write_end(), we should update i_size of inode before unlock page,
    otherwise, we will lose newly updated data in following race condition.
    
    Thread A                        Thread B
    - write_end
     - unlock page
                                    - writepages
                                     - lock_page
                                      - writepage
                                      if page is out-of-range of file size,
                                      we will skip writting the page.
     - update i_size
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index d64d2a515cb2..ccb401eebc11 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1699,11 +1699,11 @@ static int f2fs_write_end(struct file *file,
 	trace_f2fs_write_end(inode, pos, len, copied);
 
 	set_page_dirty(page);
-	f2fs_put_page(page, 1);
 
 	if (pos + copied > i_size_read(inode))
 		f2fs_i_size_write(inode, pos + copied);
 
+	f2fs_put_page(page, 1);
 	f2fs_update_time(F2FS_I_SB(inode), REQ_TIME);
 	return copied;
 }

commit 1aee6b9a7d947d42ed84baa4cf461e9d943b80f0
Author: Jens Axboe <axboe@fb.com>
Date:   Wed Jul 27 12:52:21 2016 -0600

    f2fs: drop bio->bi_rw manual assignment
    
    Merge 4fc29c1aa375 included this extra line, but it's not needed (or
    useful) since we'll bio_set_op_attrs() right after to properly set
    the op and flags for the bio.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index e2624275d828..d64d2a515cb2 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -245,7 +245,6 @@ int f2fs_submit_page_bio(struct f2fs_io_info *fio)
 		bio_put(bio);
 		return -EFAULT;
 	}
-	bio->bi_rw = fio->op_flags;
 	bio_set_op_attrs(bio, fio->op, fio->op_flags);
 
 	__submit_bio(fio->sbi, bio, fio->type);

commit 4fc29c1aa375353ffe7c8fa171bf941b71ce29ef
Merge: 0e6acf0204da 5302fb000def
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jul 27 10:36:31 2016 -0700

    Merge tag 'for-f2fs-4.8' of git://git.kernel.org/pub/scm/linux/kernel/git/jaegeuk/f2fs
    
    Pull f2fs updates from Jaegeuk Kim:
     "The major change in this version is mitigating cpu overheads on write
      paths by replacing redundant inode page updates with mark_inode_dirty
      calls.  And we tried to reduce lock contentions as well to improve
      filesystem scalability.  Other feature is setting F2FS automatically
      when detecting host-managed SMR.
    
      Enhancements:
       - ioctl to move a range of data between files
       - inject orphan inode errors
       - avoid flush commands congestion
       - support lazytime
    
      Bug fixes:
       - return proper results for some dentry operations
       - fix deadlock in add_link failure
       - disable extent_cache for fcollapse/finsert"
    
    * tag 'for-f2fs-4.8' of git://git.kernel.org/pub/scm/linux/kernel/git/jaegeuk/f2fs: (68 commits)
      f2fs: clean up coding style and redundancy
      f2fs: get victim segment again after new cp
      f2fs: handle error case with f2fs_bug_on
      f2fs: avoid data race when deciding checkpoin in f2fs_sync_file
      f2fs: support an ioctl to move a range of data blocks
      f2fs: fix to report error number of f2fs_find_entry
      f2fs: avoid memory allocation failure due to a long length
      f2fs: reset default idle interval value
      f2fs: use blk_plug in all the possible paths
      f2fs: fix to avoid data update racing between GC and DIO
      f2fs: add maximum prefree segments
      f2fs: disable extent_cache for fcollapse/finsert inodes
      f2fs: refactor __exchange_data_block for speed up
      f2fs: fix ERR_PTR returned by bio
      f2fs: avoid mark_inode_dirty
      f2fs: move i_size_write in f2fs_write_end
      f2fs: fix to avoid redundant discard during fstrim
      f2fs: avoid mismatching block range for discard
      f2fs: fix incorrect f_bfree calculation in ->statfs
      f2fs: use percpu_rw_semaphore
      ...

commit 0e06f5c0deeef0332a5da2ecb8f1fcf3e024d958
Merge: f7816ad0f878 8f19b0c058d9
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jul 26 19:55:54 2016 -0700

    Merge branch 'akpm' (patches from Andrew)
    
    Merge updates from Andrew Morton:
    
     - a few misc bits
    
     - ocfs2
    
     - most(?) of MM
    
    * emailed patches from Andrew Morton <akpm@linux-foundation.org>: (125 commits)
      thp: fix comments of __pmd_trans_huge_lock()
      cgroup: remove unnecessary 0 check from css_from_id()
      cgroup: fix idr leak for the first cgroup root
      mm: memcontrol: fix documentation for compound parameter
      mm: memcontrol: remove BUG_ON in uncharge_list
      mm: fix build warnings in <linux/compaction.h>
      mm, thp: convert from optimistic swapin collapsing to conservative
      mm, thp: fix comment inconsistency for swapin readahead functions
      thp: update Documentation/{vm/transhuge,filesystems/proc}.txt
      shmem: split huge pages beyond i_size under memory pressure
      thp: introduce CONFIG_TRANSPARENT_HUGE_PAGECACHE
      khugepaged: add support of collapse for tmpfs/shmem pages
      shmem: make shmem_inode_info::lock irq-safe
      khugepaged: move up_read(mmap_sem) out of khugepaged_alloc_page()
      thp: extract khugepaged from mm/huge_memory.c
      shmem, thp: respect MADV_{NO,}HUGEPAGE for file mappings
      shmem: add huge pages support
      shmem: get_unmapped_area align huge page
      shmem: prepare huge= mount option and sysfs knob
      mm, rmap: account shmem thp pages
      ...

commit 8a5c743e308dd2b90ad10d1faaa7a1b09173a132
Author: Michal Hocko <mhocko@suse.com>
Date:   Tue Jul 26 15:24:53 2016 -0700

    mm, memcg: use consistent gfp flags during readahead
    
    Vladimir has noticed that we might declare memcg oom even during
    readahead because read_pages only uses GFP_KERNEL (with mapping_gfp
    restriction) while __do_page_cache_readahead uses
    page_cache_alloc_readahead which adds __GFP_NORETRY to prevent from
    OOMs.  This gfp mask discrepancy is really unfortunate and easily
    fixable.  Drop page_cache_alloc_readahead() which only has one user and
    outsource the gfp_mask logic into readahead_gfp_mask and propagate this
    mask from __do_page_cache_readahead down to read_pages.
    
    This alone would have only very limited impact as most filesystems are
    implementing ->readpages and the common implementation mpage_readpages
    does GFP_KERNEL (with mapping_gfp restriction) again.  We can tell it to
    use readahead_gfp_mask instead as this function is called only during
    readahead as well.  The same applies to read_cache_pages.
    
    ext4 has its own ext4_mpage_readpages but the path which has pages !=
    NULL can use the same gfp mask.  Btrfs, cifs, f2fs and orangefs are
    doing a very similar pattern to mpage_readpages so the same can be
    applied to them as well.
    
    [akpm@linux-foundation.org: coding-style fixes]
    [mhocko@suse.com: restrict gfp mask in mpage_alloc]
      Link: http://lkml.kernel.org/r/20160610074223.GC32285@dhcp22.suse.cz
    Link: http://lkml.kernel.org/r/1465301556-26431-1-git-send-email-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Cc: Vladimir Davydov <vdavydov@parallels.com>
    Cc: Chris Mason <clm@fb.com>
    Cc: Steve French <sfrench@samba.org>
    Cc: Theodore Ts'o <tytso@mit.edu>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Mike Marshall <hubcap@omnibond.com>
    Cc: Jaegeuk Kim <jaegeuk@kernel.org>
    Cc: Changman Lee <cm224.lee@samsung.com>
    Cc: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 9a8bbc1fb1fa..c80dda4bdff8 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -996,7 +996,8 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 			page = list_entry(pages->prev, struct page, lru);
 			list_del(&page->lru);
 			if (add_to_page_cache_lru(page, mapping,
-						  page->index, GFP_KERNEL))
+						  page->index,
+						  readahead_gfp_mask(mapping)))
 				goto next_page;
 		}
 

commit 5302fb000def84100740a84d7f176c0e167b2141
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Jul 22 15:25:47 2016 -0700

    f2fs: clean up coding style and redundancy
    
    This patch includes minor clean-ups.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 87a458fb8afe..614154f9bb35 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1731,7 +1731,7 @@ static ssize_t f2fs_direct_IO(struct kiocb *iocb, struct iov_iter *iter)
 	if (test_opt(F2FS_I_SB(inode), LFS))
 		return 0;
 
-	trace_f2fs_direct_IO_enter(inode, offset, count, iov_iter_rw(iter));
+	trace_f2fs_direct_IO_enter(inode, offset, count, rw);
 
 	down_read(&F2FS_I(inode)->dio_rwsem[rw]);
 	err = blockdev_direct_IO(iocb, inode, iter, get_data_block_dio);
@@ -1744,7 +1744,7 @@ static ssize_t f2fs_direct_IO(struct kiocb *iocb, struct iov_iter *iter)
 			f2fs_write_failed(mapping, offset + count);
 	}
 
-	trace_f2fs_direct_IO_exit(inode, offset, count, iov_iter_rw(iter), err);
+	trace_f2fs_direct_IO_exit(inode, offset, count, rw, err);
 
 	return err;
 }

commit 9dfa1baff76d08843aaf5e3c78f6da6950957702
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed Jul 13 19:33:19 2016 -0700

    f2fs: use blk_plug in all the possible paths
    
    This patch reverts 19a5f5e2ef37 (f2fs: drop any block plugging),
    and adds blk_plug in write paths additionally.
    
    The main reason is that blk_start_plug can be used to wake up from low-power
    mode before submitting further bios.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index adfe47b21991..87a458fb8afe 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1438,6 +1438,7 @@ static int f2fs_write_data_pages(struct address_space *mapping,
 {
 	struct inode *inode = mapping->host;
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct blk_plug plug;
 	int ret;
 
 	/* deal with chardevs and other special file */
@@ -1463,7 +1464,9 @@ static int f2fs_write_data_pages(struct address_space *mapping,
 
 	trace_f2fs_writepages(mapping->host, wbc, DATA);
 
+	blk_start_plug(&plug);
 	ret = f2fs_write_cache_pages(mapping, wbc);
+	blk_finish_plug(&plug);
 	/*
 	 * if some pages were truncated, we cannot guarantee its mapping->host
 	 * to detect pending bios.

commit 82e0a5aa5ddf794b3e1b21fcd091228736871882
Author: Chao Yu <yuchao0@huawei.com>
Date:   Wed Jul 13 09:18:29 2016 +0800

    f2fs: fix to avoid data update racing between GC and DIO
    
    Datas in file can be operated by GC and DIO simultaneously, so we will
    face race case as below:
    
    For write case:
    Thread A                                Thread B
    - generic_file_direct_write
     - invalidate_inode_pages2_range
     - f2fs_direct_IO
      - do_blockdev_direct_IO
       - do_direct_IO
        - get_more_blocks
                                            - f2fs_gc
                                             - do_garbage_collect
                                              - gc_data_segment
                                               - move_data_page
                                                - do_write_data_page
                                                migrate data block to new block address
       - dio_bio_submit
       update user data to old block address
    
    For read case:
    Thread A                                Thread B
    - generic_file_direct_write
     - invalidate_inode_pages2_range
     - f2fs_direct_IO
      - do_blockdev_direct_IO
       - do_direct_IO
        - get_more_blocks
                                            - f2fs_balance_fs
                                             - f2fs_gc
                                              - do_garbage_collect
                                               - gc_data_segment
                                                - move_data_page
                                                 - do_write_data_page
                                                 migrate data block to new block address
                                              - write_checkpoint
                                               - do_checkpoint
                                                - clear_prefree_segments
                                                 - f2fs_issue_discard
                                                 discard old block adress
       - dio_bio_submit
       update user buffer from obsolete block address
    
    In order to fix this, for one file, we should let DIO and GC getting exclusion
    against with each other.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 650099597dd2..adfe47b21991 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1716,6 +1716,7 @@ static ssize_t f2fs_direct_IO(struct kiocb *iocb, struct iov_iter *iter)
 	struct inode *inode = mapping->host;
 	size_t count = iov_iter_count(iter);
 	loff_t offset = iocb->ki_pos;
+	int rw = iov_iter_rw(iter);
 	int err;
 
 	err = check_direct_IO(inode, iter, offset);
@@ -1729,8 +1730,11 @@ static ssize_t f2fs_direct_IO(struct kiocb *iocb, struct iov_iter *iter)
 
 	trace_f2fs_direct_IO_enter(inode, offset, count, iov_iter_rw(iter));
 
+	down_read(&F2FS_I(inode)->dio_rwsem[rw]);
 	err = blockdev_direct_IO(iocb, inode, iter, get_data_block_dio);
-	if (iov_iter_rw(iter) == WRITE) {
+	up_read(&F2FS_I(inode)->dio_rwsem[rw]);
+
+	if (rw == WRITE) {
 		if (err > 0)
 			set_inode_flag(inode, FI_UPDATE_WRITE);
 		else if (err < 0)

commit 1d353eb7e43853335a3c535c204c4e86f82eaf66
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Tue Jul 12 09:38:48 2016 -0700

    f2fs: fix ERR_PTR returned by bio
    
    This is to fix wrong error pointer handling flow reported by Dan.
    
    Reported-by: Dan Carpenter <dan.carpenter@oracle.com>
    Signed-off-by: Chao Yu <chao@kernel.org>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 20b30162e7b4..650099597dd2 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1085,8 +1085,10 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 		}
 		if (bio == NULL) {
 			bio = f2fs_grab_bio(inode, block_nr, nr_pages);
-			if (IS_ERR(bio))
+			if (IS_ERR(bio)) {
+				bio = NULL;
 				goto set_error_page;
+			}
 		}
 
 		if (bio_add_page(bio, page, blocksize, 0) < blocksize)

commit a2ee0a300344a6da76186129b078113354fe13d2
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Jul 7 10:06:23 2016 -0700

    f2fs: move i_size_write in f2fs_write_end
    
    We don't need to do i_size_write under page lock.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index b6fd5bd05e41..20b30162e7b4 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1685,11 +1685,11 @@ static int f2fs_write_end(struct file *file,
 	trace_f2fs_write_end(inode, pos, len, copied);
 
 	set_page_dirty(page);
+	f2fs_put_page(page, 1);
 
 	if (pos + copied > i_size_read(inode))
 		f2fs_i_size_write(inode, pos + copied);
 
-	f2fs_put_page(page, 1);
 	f2fs_update_time(F2FS_I_SB(inode), REQ_TIME);
 	return copied;
 }

commit 237c0790e54020d522b8fd23097e8dcafb4e331d
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Jun 30 18:49:15 2016 -0700

    f2fs: call SetPageUptodate if needed
    
    SetPageUptodate() issues memory barrier, resulting in performance degrdation.
    Let's avoid that.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 4a2e97dad9e9..b6fd5bd05e41 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -47,7 +47,8 @@ static void f2fs_read_end_io(struct bio *bio)
 		struct page *page = bvec->bv_page;
 
 		if (!bio->bi_error) {
-			SetPageUptodate(page);
+			if (!PageUptodate(page))
+				SetPageUptodate(page);
 		} else {
 			ClearPageUptodate(page);
 			SetPageError(page);
@@ -443,7 +444,8 @@ struct page *get_read_data_page(struct inode *inode, pgoff_t index,
 	 */
 	if (dn.data_blkaddr == NEW_ADDR) {
 		zero_user_segment(page, 0, PAGE_SIZE);
-		SetPageUptodate(page);
+		if (!PageUptodate(page))
+			SetPageUptodate(page);
 		unlock_page(page);
 		return page;
 	}
@@ -554,7 +556,8 @@ struct page *get_new_data_page(struct inode *inode,
 
 	if (dn.data_blkaddr == NEW_ADDR) {
 		zero_user_segment(page, 0, PAGE_SIZE);
-		SetPageUptodate(page);
+		if (!PageUptodate(page))
+			SetPageUptodate(page);
 	} else {
 		f2fs_put_page(page, 1);
 
@@ -1065,7 +1068,8 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 			}
 		} else {
 			zero_user_segment(page, 0, PAGE_SIZE);
-			SetPageUptodate(page);
+			if (!PageUptodate(page))
+				SetPageUptodate(page);
 			unlock_page(page);
 			goto next_page;
 		}
@@ -1659,7 +1663,8 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 		}
 	}
 out_update:
-	SetPageUptodate(page);
+	if (!PageUptodate(page))
+		SetPageUptodate(page);
 out_clear:
 	clear_cold_data(page);
 	return 0;
@@ -1813,7 +1818,8 @@ static int f2fs_set_data_page_dirty(struct page *page)
 
 	trace_f2fs_set_page_dirty(page, DATA);
 
-	SetPageUptodate(page);
+	if (!PageUptodate(page))
+		SetPageUptodate(page);
 
 	if (f2fs_is_atomic_file(inode)) {
 		if (!IS_ATOMIC_WRITTEN_PAGE(page)) {

commit fe76b796fc5194cc3d57265002e3a748566d073f
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Jun 30 18:40:10 2016 -0700

    f2fs: introduce f2fs_set_page_dirty_nobuffer
    
    This patch adds f2fs_set_page_dirty_nobuffer() copied from __set_page_dirty_buffer.
    When appending 4KB blocks in f2fs on pmem with multiple cores, this improves the
    overall performance.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 3d93cf184114..4a2e97dad9e9 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -19,6 +19,8 @@
 #include <linux/bio.h>
 #include <linux/prefetch.h>
 #include <linux/uio.h>
+#include <linux/mm.h>
+#include <linux/memcontrol.h>
 #include <linux/cleancache.h>
 
 #include "f2fs.h"
@@ -1775,6 +1777,35 @@ int f2fs_release_page(struct page *page, gfp_t wait)
 	return 1;
 }
 
+/*
+ * This was copied from __set_page_dirty_buffers which gives higher performance
+ * in very high speed storages. (e.g., pmem)
+ */
+void f2fs_set_page_dirty_nobuffers(struct page *page)
+{
+	struct address_space *mapping = page->mapping;
+	unsigned long flags;
+
+	if (unlikely(!mapping))
+		return;
+
+	spin_lock(&mapping->private_lock);
+	lock_page_memcg(page);
+	SetPageDirty(page);
+	spin_unlock(&mapping->private_lock);
+
+	spin_lock_irqsave(&mapping->tree_lock, flags);
+	WARN_ON_ONCE(!PageUptodate(page));
+	account_page_dirtied(page, mapping);
+	radix_tree_tag_set(&mapping->page_tree,
+			page_index(page), PAGECACHE_TAG_DIRTY);
+	spin_unlock_irqrestore(&mapping->tree_lock, flags);
+	unlock_page_memcg(page);
+
+	__mark_inode_dirty(mapping->host, I_DIRTY_PAGES);
+	return;
+}
+
 static int f2fs_set_data_page_dirty(struct page *page)
 {
 	struct address_space *mapping = page->mapping;
@@ -1797,7 +1828,7 @@ static int f2fs_set_data_page_dirty(struct page *page)
 	}
 
 	if (!PageDirty(page)) {
-		__set_page_dirty_nobuffers(page);
+		f2fs_set_page_dirty_nobuffers(page);
 		update_dirty_page(inode, page);
 		return 1;
 	}

commit 1563ac75e7e45adcdc1271e6bb55fe27a23d4e4e
Author: Chao Yu <yuchao0@huawei.com>
Date:   Sun Jul 3 22:05:12 2016 +0800

    f2fs: fix to detect truncation prior rather than EIO during read
    
    In procedure of synchonized read, after sending out the read request, reader
    will try to lock the page for waiting device to finish the read jobs and
    unlock the page, but meanwhile, truncater will race with reader, so after
    reader get lock of the page, it should check page's mapping to detect
    whether someone has truncated the page in advance, then reader has the
    chance to do the retry if truncation was done, otherwise read can be failed
    due to previous condition check.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 53fae38009d3..3d93cf184114 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -500,14 +500,14 @@ struct page *get_lock_data_page(struct inode *inode, pgoff_t index,
 
 	/* wait for read completion */
 	lock_page(page);
-	if (unlikely(!PageUptodate(page))) {
-		f2fs_put_page(page, 1);
-		return ERR_PTR(-EIO);
-	}
 	if (unlikely(page->mapping != mapping)) {
 		f2fs_put_page(page, 1);
 		goto repeat;
 	}
+	if (unlikely(!PageUptodate(page))) {
+		f2fs_put_page(page, 1);
+		return ERR_PTR(-EIO);
+	}
 	return page;
 }
 
@@ -1647,14 +1647,14 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 		__submit_bio(sbi, READ_SYNC, bio, DATA);
 
 		lock_page(page);
-		if (unlikely(!PageUptodate(page))) {
-			err = -EIO;
-			goto fail;
-		}
 		if (unlikely(page->mapping != mapping)) {
 			f2fs_put_page(page, 1);
 			goto repeat;
 		}
+		if (unlikely(!PageUptodate(page))) {
+			err = -EIO;
+			goto fail;
+		}
 	}
 out_update:
 	SetPageUptodate(page);

commit 78682f79447998369a85f12b6437fa8fdbbdca50
Author: Chao Yu <yuchao0@huawei.com>
Date:   Sun Jul 3 22:05:11 2016 +0800

    f2fs: fix to avoid reading out encrypted data in page cache
    
    For encrypted inode, if user overwrites data of the inode, f2fs will read
    encrypted data into page cache, and then do the decryption.
    
    However reader can race with overwriter, and it will see encrypted data
    which has not been decrypted by overwriter yet. Fix it by moving decrypting
    work to background and keep page non-uptodated until data is decrypted.
    
    Thread A                                Thread B
    - f2fs_file_write_iter
     - __generic_file_write_iter
      - generic_perform_write
       - f2fs_write_begin
        - f2fs_submit_page_bio
                                            - generic_file_read_iter
                                             - do_generic_file_read
                                              - lock_page_killable
                                              - unlock_page
                                              - copy_page_to_iter
                                              hit the encrypted data in updated page
        - lock_page
        - fscrypt_decrypt_page
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index ba4963f51bee..53fae38009d3 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -955,6 +955,37 @@ int f2fs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,
 	return ret;
 }
 
+struct bio *f2fs_grab_bio(struct inode *inode, block_t blkaddr,
+							unsigned nr_pages)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct fscrypt_ctx *ctx = NULL;
+	struct block_device *bdev = sbi->sb->s_bdev;
+	struct bio *bio;
+
+	if (f2fs_encrypted_inode(inode) && S_ISREG(inode->i_mode)) {
+		ctx = fscrypt_get_ctx(inode, GFP_NOFS);
+		if (IS_ERR(ctx))
+			return ERR_CAST(ctx);
+
+		/* wait the page to be moved by cleaning */
+		f2fs_wait_on_encrypted_page_writeback(sbi, blkaddr);
+	}
+
+	bio = bio_alloc(GFP_KERNEL, min_t(int, nr_pages, BIO_MAX_PAGES));
+	if (!bio) {
+		if (ctx)
+			fscrypt_release_ctx(ctx);
+		return ERR_PTR(-ENOMEM);
+	}
+	bio->bi_bdev = bdev;
+	bio->bi_iter.bi_sector = SECTOR_FROM_BLOCK(blkaddr);
+	bio->bi_end_io = f2fs_read_end_io;
+	bio->bi_private = ctx;
+
+	return bio;
+}
+
 /*
  * This function was originally taken from fs/mpage.c, and customized for f2fs.
  * Major change was from block_size == page_size in f2fs by default.
@@ -973,7 +1004,6 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 	sector_t last_block;
 	sector_t last_block_in_file;
 	sector_t block_nr;
-	struct block_device *bdev = inode->i_sb->s_bdev;
 	struct f2fs_map_blocks map;
 
 	map.m_pblk = 0;
@@ -1048,31 +1078,9 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 			bio = NULL;
 		}
 		if (bio == NULL) {
-			struct fscrypt_ctx *ctx = NULL;
-
-			if (f2fs_encrypted_inode(inode) &&
-					S_ISREG(inode->i_mode)) {
-
-				ctx = fscrypt_get_ctx(inode, GFP_NOFS);
-				if (IS_ERR(ctx))
-					goto set_error_page;
-
-				/* wait the page to be moved by cleaning */
-				f2fs_wait_on_encrypted_page_writeback(
-						F2FS_I_SB(inode), block_nr);
-			}
-
-			bio = bio_alloc(GFP_KERNEL,
-				min_t(int, nr_pages, BIO_MAX_PAGES));
-			if (!bio) {
-				if (ctx)
-					fscrypt_release_ctx(ctx);
+			bio = f2fs_grab_bio(inode, block_nr, nr_pages);
+			if (IS_ERR(bio))
 				goto set_error_page;
-			}
-			bio->bi_bdev = bdev;
-			bio->bi_iter.bi_sector = SECTOR_FROM_BLOCK(block_nr);
-			bio->bi_end_io = f2fs_read_end_io;
-			bio->bi_private = ctx;
 		}
 
 		if (bio_add_page(bio, page, blocksize, 0) < blocksize)
@@ -1622,18 +1630,21 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 	if (blkaddr == NEW_ADDR) {
 		zero_user_segment(page, 0, PAGE_SIZE);
 	} else {
-		struct f2fs_io_info fio = {
-			.sbi = sbi,
-			.type = DATA,
-			.rw = READ_SYNC,
-			.old_blkaddr = blkaddr,
-			.new_blkaddr = blkaddr,
-			.page = page,
-			.encrypted_page = NULL,
-		};
-		err = f2fs_submit_page_bio(&fio);
-		if (err)
+		struct bio *bio;
+
+		bio = f2fs_grab_bio(inode, blkaddr, 1);
+		if (IS_ERR(bio)) {
+			err = PTR_ERR(bio);
 			goto fail;
+		}
+
+		if (bio_add_page(bio, page, PAGE_SIZE, 0) < PAGE_SIZE) {
+			bio_put(bio);
+			err = -EFAULT;
+			goto fail;
+		}
+
+		__submit_bio(sbi, READ_SYNC, bio, DATA);
 
 		lock_page(page);
 		if (unlikely(!PageUptodate(page))) {
@@ -1644,13 +1655,6 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 			f2fs_put_page(page, 1);
 			goto repeat;
 		}
-
-		/* avoid symlink page */
-		if (f2fs_encrypted_inode(inode) && S_ISREG(inode->i_mode)) {
-			err = fscrypt_decrypt_page(page);
-			if (err)
-				goto fail;
-		}
 	}
 out_update:
 	SetPageUptodate(page);

commit ac6f199984a667eb017897b8528f7687eac8fa45
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Jun 16 17:03:23 2016 -0700

    f2fs: avoid latency-critical readahead of node pages
    
    The f2fs_map_blocks is very related to the performance, so let's avoid any
    latency to read ahead node pages.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 08325280d03c..ba4963f51bee 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -652,7 +652,7 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 	unsigned int maxblocks = map->m_len;
 	struct dnode_of_data dn;
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	int mode = create ? ALLOC_NODE : LOOKUP_NODE_RA;
+	int mode = create ? ALLOC_NODE : LOOKUP_NODE;
 	pgoff_t pgofs, end_offset, end;
 	int err = 0, ofs = 1;
 	unsigned int ofs_in_node, last_ofs_in_node;

commit 52763a4b7a2112743745c5bbfe43fe6f54d4b39a
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Mon Jun 13 09:47:48 2016 -0700

    f2fs: detect host-managed SMR by feature flag
    
    If mkfs.f2fs gives a feature flag for host-managed SMR, we can set mode=lfs
    by default.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 607ef4397330..08325280d03c 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -102,7 +102,8 @@ static inline void __submit_bio(struct f2fs_sb_info *sbi, int rw,
 {
 	if (!is_read_io(rw)) {
 		atomic_inc(&sbi->nr_wb_bios);
-		if (current->plug && (type == DATA || type == NODE))
+		if (f2fs_sb_mounted_hmsmr(sbi->sb) &&
+			current->plug && (type == DATA || type == NODE))
 			blk_finish_plug(current->plug);
 	}
 	submit_bio(rw, bio);

commit 36abef4e796d382e81a0c2d21ea5327481dd7154
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Jun 3 19:29:38 2016 -0700

    f2fs: introduce mode=lfs mount option
    
    This mount option is to enable original log-structured filesystem forcefully.
    So, there should be no random writes for main area.
    
    Especially, this supports host-managed SMR device.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 5f655d0c5b1f..607ef4397330 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1710,6 +1710,8 @@ static ssize_t f2fs_direct_IO(struct kiocb *iocb, struct iov_iter *iter)
 
 	if (f2fs_encrypted_inode(inode) && S_ISREG(inode->i_mode))
 		return 0;
+	if (test_opt(F2FS_I_SB(inode), LFS))
+		return 0;
 
 	trace_f2fs_direct_IO_enter(inode, offset, count, iov_iter_rw(iter));
 

commit 19a5f5e2ef37f032efd840ada257bce2e91c8066
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Sat Jun 4 14:25:24 2016 -0700

    f2fs: drop any block plugging
    
    In f2fs, we don't need to keep block plugging for NODE and DATA writes, since
    we already merged bios as much as possible.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 30dc448cae60..5f655d0c5b1f 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -98,10 +98,13 @@ static struct bio *__bio_alloc(struct f2fs_sb_info *sbi, block_t blk_addr,
 }
 
 static inline void __submit_bio(struct f2fs_sb_info *sbi, int rw,
-						struct bio *bio)
+			struct bio *bio, enum page_type type)
 {
-	if (!is_read_io(rw))
+	if (!is_read_io(rw)) {
 		atomic_inc(&sbi->nr_wb_bios);
+		if (current->plug && (type == DATA || type == NODE))
+			blk_finish_plug(current->plug);
+	}
 	submit_bio(rw, bio);
 }
 
@@ -117,7 +120,7 @@ static void __submit_merged_bio(struct f2fs_bio_info *io)
 	else
 		trace_f2fs_submit_write_bio(io->sbi->sb, fio, io->bio);
 
-	__submit_bio(io->sbi, fio->rw, io->bio);
+	__submit_bio(io->sbi, fio->rw, io->bio, fio->type);
 	io->bio = NULL;
 }
 
@@ -235,7 +238,7 @@ int f2fs_submit_page_bio(struct f2fs_io_info *fio)
 		return -EFAULT;
 	}
 
-	__submit_bio(fio->sbi, fio->rw, bio);
+	__submit_bio(fio->sbi, fio->rw, bio, fio->type);
 	return 0;
 }
 
@@ -1040,7 +1043,7 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 		 */
 		if (bio && (last_block_in_bio != block_nr - 1)) {
 submit_and_realloc:
-			__submit_bio(F2FS_I_SB(inode), READ, bio);
+			__submit_bio(F2FS_I_SB(inode), READ, bio, DATA);
 			bio = NULL;
 		}
 		if (bio == NULL) {
@@ -1083,7 +1086,7 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 		goto next_page;
 confused:
 		if (bio) {
-			__submit_bio(F2FS_I_SB(inode), READ, bio);
+			__submit_bio(F2FS_I_SB(inode), READ, bio, DATA);
 			bio = NULL;
 		}
 		unlock_page(page);
@@ -1093,7 +1096,7 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 	}
 	BUG_ON(pages && !list_empty(pages));
 	if (bio)
-		__submit_bio(F2FS_I_SB(inode), READ, bio);
+		__submit_bio(F2FS_I_SB(inode), READ, bio, DATA);
 	return 0;
 }
 

commit 7f319975ccea80cf03377ff579b0a9e613308570
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Jun 3 12:28:26 2016 -0700

    f2fs: set mapping error for EIO
    
    If EIO occurred, we need to set all the mapping to avoid any further IOs.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index c9d6fe28ae1e..30dc448cae60 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1243,7 +1243,7 @@ static int f2fs_write_data_page(struct page *page,
 
 	/* we should bypass data pages to proceed the kworkder jobs */
 	if (unlikely(f2fs_cp_error(sbi))) {
-		SetPageError(page);
+		mapping_set_error(page->mapping, -EIO);
 		goto out;
 	}
 

commit 04d328defd06257bf386d58f359013e0ef329226
Author: Mike Christie <mchristi@redhat.com>
Date:   Sun Jun 5 14:31:55 2016 -0500

    f2fs: use bio op accessors
    
    Separate the op from the rq_flag_bits and have f2fs
    set/get the bio using bio_set_op_attrs/bio_op.
    
    Signed-off-by: Mike Christie <mchristi@redhat.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index c595c8f84c48..8769e8349dff 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -97,12 +97,10 @@ static struct bio *__bio_alloc(struct f2fs_sb_info *sbi, block_t blk_addr,
 	return bio;
 }
 
-static inline void __submit_bio(struct f2fs_sb_info *sbi, int rw,
-						struct bio *bio)
+static inline void __submit_bio(struct f2fs_sb_info *sbi, struct bio *bio)
 {
-	if (!is_read_io(rw))
+	if (!is_read_io(bio_op(bio)))
 		atomic_inc(&sbi->nr_wb_bios);
-	bio->bi_rw = rw;
 	submit_bio(bio);
 }
 
@@ -113,12 +111,14 @@ static void __submit_merged_bio(struct f2fs_bio_info *io)
 	if (!io->bio)
 		return;
 
-	if (is_read_io(fio->rw))
+	if (is_read_io(fio->op))
 		trace_f2fs_submit_read_bio(io->sbi->sb, fio, io->bio);
 	else
 		trace_f2fs_submit_write_bio(io->sbi->sb, fio, io->bio);
 
-	__submit_bio(io->sbi, fio->rw, io->bio);
+	bio_set_op_attrs(io->bio, fio->op, fio->op_flags);
+
+	__submit_bio(io->sbi, io->bio);
 	io->bio = NULL;
 }
 
@@ -184,10 +184,12 @@ static void __f2fs_submit_merged_bio(struct f2fs_sb_info *sbi,
 	/* change META to META_FLUSH in the checkpoint procedure */
 	if (type >= META_FLUSH) {
 		io->fio.type = META_FLUSH;
+		io->fio.op = REQ_OP_WRITE;
 		if (test_opt(sbi, NOBARRIER))
-			io->fio.rw = WRITE_FLUSH | REQ_META | REQ_PRIO;
+			io->fio.op_flags = WRITE_FLUSH | REQ_META | REQ_PRIO;
 		else
-			io->fio.rw = WRITE_FLUSH_FUA | REQ_META | REQ_PRIO;
+			io->fio.op_flags = WRITE_FLUSH_FUA | REQ_META |
+								REQ_PRIO;
 	}
 	__submit_merged_bio(io);
 out:
@@ -229,14 +231,16 @@ int f2fs_submit_page_bio(struct f2fs_io_info *fio)
 	f2fs_trace_ios(fio, 0);
 
 	/* Allocate a new bio */
-	bio = __bio_alloc(fio->sbi, fio->new_blkaddr, 1, is_read_io(fio->rw));
+	bio = __bio_alloc(fio->sbi, fio->new_blkaddr, 1, is_read_io(fio->op));
 
 	if (bio_add_page(bio, page, PAGE_SIZE, 0) < PAGE_SIZE) {
 		bio_put(bio);
 		return -EFAULT;
 	}
+	bio->bi_rw = fio->op_flags;
+	bio_set_op_attrs(bio, fio->op, fio->op_flags);
 
-	__submit_bio(fio->sbi, fio->rw, bio);
+	__submit_bio(fio->sbi, bio);
 	return 0;
 }
 
@@ -245,7 +249,7 @@ void f2fs_submit_page_mbio(struct f2fs_io_info *fio)
 	struct f2fs_sb_info *sbi = fio->sbi;
 	enum page_type btype = PAGE_TYPE_OF_BIO(fio->type);
 	struct f2fs_bio_info *io;
-	bool is_read = is_read_io(fio->rw);
+	bool is_read = is_read_io(fio->op);
 	struct page *bio_page;
 
 	io = is_read ? &sbi->read_io : &sbi->write_io[btype];
@@ -257,7 +261,7 @@ void f2fs_submit_page_mbio(struct f2fs_io_info *fio)
 	down_write(&io->io_rwsem);
 
 	if (io->bio && (io->last_block_in_bio != fio->new_blkaddr - 1 ||
-						io->fio.rw != fio->rw))
+	    (io->fio.op != fio->op || io->fio.op_flags != fio->op_flags)))
 		__submit_merged_bio(io);
 alloc_new:
 	if (io->bio == NULL) {
@@ -391,7 +395,7 @@ int f2fs_get_block(struct dnode_of_data *dn, pgoff_t index)
 }
 
 struct page *get_read_data_page(struct inode *inode, pgoff_t index,
-						int rw, bool for_write)
+						int op_flags, bool for_write)
 {
 	struct address_space *mapping = inode->i_mapping;
 	struct dnode_of_data dn;
@@ -401,7 +405,8 @@ struct page *get_read_data_page(struct inode *inode, pgoff_t index,
 	struct f2fs_io_info fio = {
 		.sbi = F2FS_I_SB(inode),
 		.type = DATA,
-		.rw = rw,
+		.op = REQ_OP_READ,
+		.op_flags = op_flags,
 		.encrypted_page = NULL,
 	};
 
@@ -1052,7 +1057,7 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 		 */
 		if (bio && (last_block_in_bio != block_nr - 1)) {
 submit_and_realloc:
-			__submit_bio(F2FS_I_SB(inode), READ, bio);
+			__submit_bio(F2FS_I_SB(inode), bio);
 			bio = NULL;
 		}
 		if (bio == NULL) {
@@ -1081,7 +1086,7 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 			bio->bi_iter.bi_sector = SECTOR_FROM_BLOCK(block_nr);
 			bio->bi_end_io = f2fs_read_end_io;
 			bio->bi_private = ctx;
-			bio->bi_rw = READ;
+			bio_set_op_attrs(bio, REQ_OP_READ, 0);
 		}
 
 		if (bio_add_page(bio, page, blocksize, 0) < blocksize)
@@ -1096,7 +1101,7 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 		goto next_page;
 confused:
 		if (bio) {
-			__submit_bio(F2FS_I_SB(inode), READ, bio);
+			__submit_bio(F2FS_I_SB(inode), bio);
 			bio = NULL;
 		}
 		unlock_page(page);
@@ -1106,7 +1111,7 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 	}
 	BUG_ON(pages && !list_empty(pages));
 	if (bio)
-		__submit_bio(F2FS_I_SB(inode), READ, bio);
+		__submit_bio(F2FS_I_SB(inode), bio);
 	return 0;
 }
 
@@ -1223,7 +1228,8 @@ static int f2fs_write_data_page(struct page *page,
 	struct f2fs_io_info fio = {
 		.sbi = sbi,
 		.type = DATA,
-		.rw = (wbc->sync_mode == WB_SYNC_ALL) ? WRITE_SYNC : WRITE,
+		.op = REQ_OP_WRITE,
+		.op_flags = (wbc->sync_mode == WB_SYNC_ALL) ? WRITE_SYNC : 0,
 		.page = page,
 		.encrypted_page = NULL,
 	};
@@ -1664,7 +1670,8 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 		struct f2fs_io_info fio = {
 			.sbi = sbi,
 			.type = DATA,
-			.rw = READ_SYNC,
+			.op = REQ_OP_READ,
+			.op_flags = READ_SYNC,
 			.old_blkaddr = blkaddr,
 			.new_blkaddr = blkaddr,
 			.page = page,

commit 4e49ea4a3d276365bf7396c9b77b4d1d5923835a
Author: Mike Christie <mchristi@redhat.com>
Date:   Sun Jun 5 14:31:41 2016 -0500

    block/fs/drivers: remove rw argument from submit_bio
    
    This has callers of submit_bio/submit_bio_wait set the bio->bi_rw
    instead of passing it in. This makes that use the same as
    generic_make_request and how we set the other bio fields.
    
    Signed-off-by: Mike Christie <mchristi@redhat.com>
    
    Fixed up fs/ext4/crypto.c
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 9a8bbc1fb1fa..c595c8f84c48 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -102,7 +102,8 @@ static inline void __submit_bio(struct f2fs_sb_info *sbi, int rw,
 {
 	if (!is_read_io(rw))
 		atomic_inc(&sbi->nr_wb_bios);
-	submit_bio(rw, bio);
+	bio->bi_rw = rw;
+	submit_bio(bio);
 }
 
 static void __submit_merged_bio(struct f2fs_bio_info *io)
@@ -1080,6 +1081,7 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 			bio->bi_iter.bi_sector = SECTOR_FROM_BLOCK(block_nr);
 			bio->bi_end_io = f2fs_read_end_io;
 			bio->bi_private = ctx;
+			bio->bi_rw = READ;
 		}
 
 		if (bio_add_page(bio, page, blocksize, 0) < blocksize)

commit b230e6cabf9e77e210fe7990fea12f8894af0fc1
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Sun May 29 21:18:23 2016 -0700

    f2fs: handle writepage correctly
    
    Previously, f2fs_write_data_pages() calls __f2fs_writepage() which calls
    f2fs_write_data_page().
    If f2fs_write_data_page() returns AOP_WRITEPAGE_ACTIVATE, __f2fs_writepage()
    calls mapping_set_error(). But, this should not happen at every time, since
    sometimes f2fs_write_data_page() tries to skip writing pages without error.
    For example, volatile_write() gives EIO all the time, as Shuoran Liu pointed
    out.
    
    Reported-by: Shuoran Liu <liushuoran@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 5dcd8dbe2064..c9d6fe28ae1e 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1241,20 +1241,18 @@ static int f2fs_write_data_page(struct page *page,
 			available_free_memory(sbi, BASE_CHECK))))
 		goto redirty_out;
 
-	/* Dentry blocks are controlled by checkpoint */
-	if (S_ISDIR(inode->i_mode)) {
-		if (unlikely(f2fs_cp_error(sbi)))
-			goto redirty_out;
-		err = do_write_data_page(&fio);
-		goto done;
-	}
-
 	/* we should bypass data pages to proceed the kworkder jobs */
 	if (unlikely(f2fs_cp_error(sbi))) {
 		SetPageError(page);
 		goto out;
 	}
 
+	/* Dentry blocks are controlled by checkpoint */
+	if (S_ISDIR(inode->i_mode)) {
+		err = do_write_data_page(&fio);
+		goto done;
+	}
+
 	if (!wbc->for_reclaim)
 		need_balance_fs = true;
 	else if (has_not_enough_free_secs(sbi, 0))
@@ -1294,16 +1292,8 @@ static int f2fs_write_data_page(struct page *page,
 
 redirty_out:
 	redirty_page_for_writepage(wbc, page);
-	return AOP_WRITEPAGE_ACTIVATE;
-}
-
-static int __f2fs_writepage(struct page *page, struct writeback_control *wbc,
-			void *data)
-{
-	struct address_space *mapping = data;
-	int ret = mapping->a_ops->writepage(page, wbc);
-	mapping_set_error(mapping, ret);
-	return ret;
+	unlock_page(page);
+	return err;
 }
 
 /*
@@ -1312,8 +1302,7 @@ static int __f2fs_writepage(struct page *page, struct writeback_control *wbc,
  * warm/hot data page.
  */
 static int f2fs_write_cache_pages(struct address_space *mapping,
-			struct writeback_control *wbc, writepage_t writepage,
-			void *data)
+					struct writeback_control *wbc)
 {
 	int ret = 0;
 	int done = 0;
@@ -1395,16 +1384,11 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 			if (!clear_page_dirty_for_io(page))
 				goto continue_unlock;
 
-			ret = (*writepage)(page, wbc, data);
+			ret = mapping->a_ops->writepage(page, wbc);
 			if (unlikely(ret)) {
-				if (ret == AOP_WRITEPAGE_ACTIVATE) {
-					unlock_page(page);
-					ret = 0;
-				} else {
-					done_index = page->index + 1;
-					done = 1;
-					break;
-				}
+				done_index = page->index + 1;
+				done = 1;
+				break;
 			}
 
 			if (--wbc->nr_to_write <= 0 &&
@@ -1459,7 +1443,7 @@ static int f2fs_write_data_pages(struct address_space *mapping,
 
 	trace_f2fs_writepages(mapping->host, wbc, DATA);
 
-	ret = f2fs_write_cache_pages(mapping, wbc, __f2fs_writepage, mapping);
+	ret = f2fs_write_cache_pages(mapping, wbc);
 	/*
 	 * if some pages were truncated, we cannot guarantee its mapping->host
 	 * to detect pending bios.

commit 46ae957f9b1611be2935ae626f601cda74f8160e
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed May 25 20:57:16 2016 -0700

    f2fs: remove two steps to flush dirty data pages
    
    If there is no cold page, we don't need to do a loop to flush dirty
    data pages.
    
    On /dev/pmem0,
    
    1. dd if=/dev/zero of=/mnt/test/testfile bs=1M count=2048 conv=fsync
     Before : 1.1 GB/s
     After  : 1.2 GB/s
    
    2. dd if=/dev/zero of=/mnt/test/testfile bs=1M count=2048
     Before : 2.2 GB/s
     After  : 2.3 GB/s
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 85ceb2be4ac9..5dcd8dbe2064 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1326,10 +1326,9 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 	int cycled;
 	int range_whole = 0;
 	int tag;
-	int step = 0;
 
 	pagevec_init(&pvec, 0);
-next:
+
 	if (wbc->range_cyclic) {
 		writeback_index = mapping->writeback_index; /* prev offset */
 		index = writeback_index;
@@ -1384,9 +1383,6 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 				goto continue_unlock;
 			}
 
-			if (step == is_cold_data(page))
-				goto continue_unlock;
-
 			if (PageWriteback(page)) {
 				if (wbc->sync_mode != WB_SYNC_NONE)
 					f2fs_wait_on_page_writeback(page,
@@ -1421,11 +1417,6 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 		cond_resched();
 	}
 
-	if (step < 1) {
-		step++;
-		goto next;
-	}
-
 	if (!cycled && !done) {
 		cycled = 1;
 		index = 0;

commit 28ea6162e29ba0db87a512dda2bb6d6e63a6006f
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed May 25 17:17:56 2016 -0700

    f2fs: do not skip writing data pages
    
    For data pages, let's try to flush as much as possible in background.
    
    On /dev/pmem0,
    
    1. dd if=/dev/zero of=/mnt/test/testfile bs=1M count=2048 conv=fsync
     Before : 800 MB/s
     After  : 1.1 GB/s
    
    2. dd if=/dev/zero of=/mnt/test/testfile bs=1M count=2048
     Before : 1.3 GB/s
     After  : 2.2 GB/s
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 7132b0202860..85ceb2be4ac9 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1444,7 +1444,6 @@ static int f2fs_write_data_pages(struct address_space *mapping,
 	struct inode *inode = mapping->host;
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
 	int ret;
-	long diff;
 
 	/* deal with chardevs and other special file */
 	if (!mapping->a_ops->writepage)
@@ -1469,14 +1468,14 @@ static int f2fs_write_data_pages(struct address_space *mapping,
 
 	trace_f2fs_writepages(mapping->host, wbc, DATA);
 
-	diff = nr_pages_to_write(sbi, DATA, wbc);
-
 	ret = f2fs_write_cache_pages(mapping, wbc, __f2fs_writepage, mapping);
-	f2fs_submit_merged_bio_cond(sbi, inode, NULL, 0, DATA, WRITE);
+	/*
+	 * if some pages were truncated, we cannot guarantee its mapping->host
+	 * to detect pending bios.
+	 */
+	f2fs_submit_merged_bio(sbi, DATA, WRITE);
 
 	remove_dirty_inode(inode);
-
-	wbc->nr_to_write = max((long)0, wbc->nr_to_write - diff);
 	return ret;
 
 skip_write:

commit b93f7712868648c0529eed6b568cea1493d3d9f9
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri May 20 22:50:29 2016 -0700

    f2fs: remove writepages lock
    
    This patch removes writepages lock.
    We can improve multi-threading performance.
    
    tiobench, 32 threads, 4KB write per fsync on SSD
    Before: 25.88 MB/s
    After: 28.03 MB/s
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 287582e12f8f..7132b0202860 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1443,7 +1443,6 @@ static int f2fs_write_data_pages(struct address_space *mapping,
 {
 	struct inode *inode = mapping->host;
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	bool locked = false;
 	int ret;
 	long diff;
 
@@ -1472,14 +1471,8 @@ static int f2fs_write_data_pages(struct address_space *mapping,
 
 	diff = nr_pages_to_write(sbi, DATA, wbc);
 
-	if (!S_ISDIR(inode->i_mode) && wbc->sync_mode == WB_SYNC_ALL) {
-		mutex_lock(&sbi->writepages);
-		locked = true;
-	}
 	ret = f2fs_write_cache_pages(mapping, wbc, __f2fs_writepage, mapping);
 	f2fs_submit_merged_bio_cond(sbi, inode, NULL, 0, DATA, WRITE);
-	if (locked)
-		mutex_unlock(&sbi->writepages);
 
 	remove_dirty_inode(inode);
 

commit 26de9b11713057a16a9220423a2f137774763b0e
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri May 20 20:42:37 2016 -0700

    f2fs: avoid unnecessary updating inode during fsync
    
    If roll-forward recovery can recover i_size, we don't need to update inode's
    metadata during fsync.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index a3dea51f4702..287582e12f8f 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1204,6 +1204,7 @@ static int f2fs_write_data_page(struct page *page,
 	loff_t i_size = i_size_read(inode);
 	const pgoff_t end_index = ((unsigned long long) i_size)
 							>> PAGE_SHIFT;
+	loff_t psize = (page->index + 1) << PAGE_SHIFT;
 	unsigned offset = 0;
 	bool need_balance_fs = false;
 	int err = 0;
@@ -1265,6 +1266,8 @@ static int f2fs_write_data_page(struct page *page,
 		err = f2fs_write_inline_data(inode, page);
 	if (err == -EAGAIN)
 		err = do_write_data_page(&fio);
+	if (F2FS_I(inode)->last_disk_size < psize)
+		F2FS_I(inode)->last_disk_size = psize;
 	f2fs_unlock_op(sbi);
 done:
 	if (err && err != -ENOENT)

commit ee6d182f2a19d5d44607b5ae4bec523726d76a99
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri May 20 16:32:49 2016 -0700

    f2fs: remove syncing inode page in all the cases
    
    This patch reduces to call them across the whole tree.
    - sync_inode_page()
    - update_inode_page()
    - update_inode()
    - f2fs_write_inode()
    
    Instead, checkpoint will flush all the dirty inode metadata before syncing
    node pages.
    Note that, this is doable, since we call mark_inode_dirty_sync() for all
    inode's field change which needs to update on-disk inode as well.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 6a4c60c2fd3a..a3dea51f4702 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -343,8 +343,6 @@ int reserve_new_blocks(struct dnode_of_data *dn, blkcnt_t count)
 
 	if (set_page_dirty(dn->node_page))
 		dn->node_changed = true;
-
-	sync_inode_page(dn);
 	return 0;
 }
 
@@ -562,11 +560,8 @@ struct page *get_new_data_page(struct inode *inode,
 	}
 got_it:
 	if (new_i_size && i_size_read(inode) <
-				((loff_t)(index + 1) << PAGE_SHIFT)) {
+				((loff_t)(index + 1) << PAGE_SHIFT))
 		f2fs_i_size_write(inode, ((loff_t)(index + 1) << PAGE_SHIFT));
-		/* Only the directory inode sets new_i_size */
-		set_inode_flag(inode, FI_UPDATE_DIR);
-	}
 	return page;
 }
 
@@ -787,8 +782,6 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 	else if (dn.ofs_in_node < end_offset)
 		goto next_block;
 
-	if (allocated)
-		sync_inode_page(&dn);
 	f2fs_put_dnode(&dn);
 
 	if (create) {
@@ -799,8 +792,6 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 	goto next_dnode;
 
 sync_out:
-	if (allocated)
-		sync_inode_page(&dn);
 	f2fs_put_dnode(&dn);
 unlock_out:
 	if (create) {

commit 8edd03c870e4eb8d635d507a7d83fe35d76117c2
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri May 20 09:26:06 2016 -0700

    f2fs: introduce f2fs_i_blocks_write with mark_inode_dirty_sync
    
    This patch introduces f2fs_i_blocks_write() to call mark_inode_dirty_sync() when
    changing inode->i_blocks.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 0dd42b6ce6f3..6a4c60c2fd3a 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -344,7 +344,6 @@ int reserve_new_blocks(struct dnode_of_data *dn, blkcnt_t count)
 	if (set_page_dirty(dn->node_page))
 		dn->node_changed = true;
 
-	mark_inode_dirty(dn->inode);
 	sync_inode_page(dn);
 	return 0;
 }

commit fc9581c809722960c46a02445f2434120e5e483b
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri May 20 09:22:03 2016 -0700

    f2fs: introduce f2fs_i_size_write with mark_inode_dirty_sync
    
    This patch introduces f2fs_i_size_write() to call mark_inode_dirty_sync() with
    i_size_write().
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index dd46ca8e0f83..0dd42b6ce6f3 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -564,7 +564,7 @@ struct page *get_new_data_page(struct inode *inode,
 got_it:
 	if (new_i_size && i_size_read(inode) <
 				((loff_t)(index + 1) << PAGE_SHIFT)) {
-		i_size_write(inode, ((loff_t)(index + 1) << PAGE_SHIFT));
+		f2fs_i_size_write(inode, ((loff_t)(index + 1) << PAGE_SHIFT));
 		/* Only the directory inode sets new_i_size */
 		set_inode_flag(inode, FI_UPDATE_DIR);
 	}
@@ -605,7 +605,7 @@ static int __allocate_data_block(struct dnode_of_data *dn)
 	fofs = start_bidx_of_node(ofs_of_node(dn->node_page), dn->inode) +
 							dn->ofs_in_node;
 	if (i_size_read(dn->inode) < ((loff_t)(fofs + 1) << PAGE_SHIFT))
-		i_size_write(dn->inode,
+		f2fs_i_size_write(dn->inode,
 				((loff_t)(fofs + 1) << PAGE_SHIFT));
 	return 0;
 }
@@ -1711,10 +1711,8 @@ static int f2fs_write_end(struct file *file,
 
 	set_page_dirty(page);
 
-	if (pos + copied > i_size_read(inode)) {
-		i_size_write(inode, pos + copied);
-		mark_inode_dirty(inode);
-	}
+	if (pos + copied > i_size_read(inode))
+		f2fs_i_size_write(inode, pos + copied);
 
 	f2fs_put_page(page, 1);
 	f2fs_update_time(F2FS_I_SB(inode), REQ_TIME);

commit 91942321e4c9f8460f260cdfcf0a7a48a73a84a4
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri May 20 10:13:22 2016 -0700

    f2fs: use inode pointer for {set, clear}_inode_flag
    
    This patch refactors to use inode pointer for set_inode_flag and
    clear_inode_flag.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 9a8bbc1fb1fa..dd46ca8e0f83 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -321,7 +321,7 @@ int reserve_new_blocks(struct dnode_of_data *dn, blkcnt_t count)
 	if (!count)
 		return 0;
 
-	if (unlikely(is_inode_flag_set(F2FS_I(dn->inode), FI_NO_ALLOC)))
+	if (unlikely(is_inode_flag_set(dn->inode, FI_NO_ALLOC)))
 		return -EPERM;
 	if (unlikely(!inc_valid_block_count(sbi, dn->inode, &count)))
 		return -ENOSPC;
@@ -566,7 +566,7 @@ struct page *get_new_data_page(struct inode *inode,
 				((loff_t)(index + 1) << PAGE_SHIFT)) {
 		i_size_write(inode, ((loff_t)(index + 1) << PAGE_SHIFT));
 		/* Only the directory inode sets new_i_size */
-		set_inode_flag(F2FS_I(inode), FI_UPDATE_DIR);
+		set_inode_flag(inode, FI_UPDATE_DIR);
 	}
 	return page;
 }
@@ -580,7 +580,7 @@ static int __allocate_data_block(struct dnode_of_data *dn)
 	pgoff_t fofs;
 	blkcnt_t count = 1;
 
-	if (unlikely(is_inode_flag_set(F2FS_I(dn->inode), FI_NO_ALLOC)))
+	if (unlikely(is_inode_flag_set(dn->inode, FI_NO_ALLOC)))
 		return -EPERM;
 
 	dn->data_blkaddr = datablock_addr(dn->node_page, dn->ofs_in_node);
@@ -717,8 +717,7 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 			} else {
 				err = __allocate_data_block(&dn);
 				if (!err) {
-					set_inode_flag(F2FS_I(inode),
-							FI_APPEND_WRITE);
+					set_inode_flag(inode, FI_APPEND_WRITE);
 					allocated = true;
 				}
 			}
@@ -1193,14 +1192,14 @@ int do_write_data_page(struct f2fs_io_info *fio)
 			!IS_ATOMIC_WRITTEN_PAGE(page) &&
 			need_inplace_update(inode))) {
 		rewrite_data_page(fio);
-		set_inode_flag(F2FS_I(inode), FI_UPDATE_WRITE);
+		set_inode_flag(inode, FI_UPDATE_WRITE);
 		trace_f2fs_do_write_data_page(page, IPU);
 	} else {
 		write_data_page(&dn, fio);
 		trace_f2fs_do_write_data_page(page, OPU);
-		set_inode_flag(F2FS_I(inode), FI_APPEND_WRITE);
+		set_inode_flag(inode, FI_APPEND_WRITE);
 		if (page->index == 0)
-			set_inode_flag(F2FS_I(inode), FI_FIRST_BLOCK_WRITTEN);
+			set_inode_flag(inode, FI_FIRST_BLOCK_WRITTEN);
 	}
 out_writepage:
 	f2fs_put_dnode(&dn);
@@ -1469,7 +1468,7 @@ static int f2fs_write_data_pages(struct address_space *mapping,
 		goto skip_write;
 
 	/* skip writing during file defragment */
-	if (is_inode_flag_set(F2FS_I(inode), FI_DO_DEFRAG))
+	if (is_inode_flag_set(inode, FI_DO_DEFRAG))
 		goto skip_write;
 
 	/* during POR, we don't need to trigger writepage at all. */
@@ -1549,7 +1548,7 @@ static int prepare_write_begin(struct f2fs_sb_info *sbi,
 	if (f2fs_has_inline_data(inode)) {
 		if (pos + len <= MAX_INLINE_DATA) {
 			read_inline_data(page, ipage);
-			set_inode_flag(F2FS_I(inode), FI_DATA_EXIST);
+			set_inode_flag(inode, FI_DATA_EXIST);
 			if (inode->i_nlink)
 				set_inline_node(ipage);
 		} else {
@@ -1756,7 +1755,7 @@ static ssize_t f2fs_direct_IO(struct kiocb *iocb, struct iov_iter *iter)
 	err = blockdev_direct_IO(iocb, inode, iter, get_data_block_dio);
 	if (iov_iter_rw(iter) == WRITE) {
 		if (err > 0)
-			set_inode_flag(F2FS_I(inode), FI_UPDATE_WRITE);
+			set_inode_flag(inode, FI_UPDATE_WRITE);
 		else if (err < 0)
 			f2fs_write_failed(mapping, offset + count);
 	}

commit f6c658df63856db3bf8f467024b1dbee37b5399c
Merge: 07be1337b9e8 0f3311a8c266
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat May 21 18:25:28 2016 -0700

    Merge tag 'for-f2fs-4.7' of git://git.kernel.org/pub/scm/linux/kernel/git/jaegeuk/f2fs
    
    Pull f2fs updates from Jaegeuk Kim:
     "In this round, as Ted pointed out, fscrypto allows one more key prefix
      given by filesystem to resolve backward compatibility issues.  Other
      than that, we've fixed several error handling cases by introducing
      a fault injection facility.  We've also achieved performance
      improvement in some workloads as well as a bunch of bug fixes.
    
      Summary:
    
      Enhancements:
       - fs-specific prefix for fscrypto
       - fault injection facility
       - expose validity bitmaps for user to be aware of fragmentation
       - fallocate/rm/preallocation speed up
       - use percpu counters
    
      Bug fixes:
       - some inline_dentry/inline_data bugs
       - error handling for atomic/volatile/orphan inodes
       - recover broken superblock"
    
    * tag 'for-f2fs-4.7' of git://git.kernel.org/pub/scm/linux/kernel/git/jaegeuk/f2fs: (73 commits)
      f2fs: fix to update dirty page count correctly
      f2fs: flush pending bios right away when error occurs
      f2fs: avoid ENOSPC fault in the recovery process
      f2fs: make exit_f2fs_fs more clear
      f2fs: use percpu_counter for total_valid_inode_count
      f2fs: use percpu_counter for alloc_valid_block_count
      f2fs: use percpu_counter for # of dirty pages in inode
      f2fs: use percpu_counter for page counters
      f2fs: use bio count instead of F2FS_WRITEBACK page count
      f2fs: manipulate dirty file inodes when DATA_FLUSH is set
      f2fs: add fault injection to sysfs
      f2fs: no need inc dirty pages under inode lock
      f2fs: fix incorrect error path handling in f2fs_move_rehashed_dirents
      f2fs: fix i_current_depth during inline dentry conversion
      f2fs: correct return value type of f2fs_fill_super
      f2fs: fix deadlock when flush inline data
      f2fs: avoid f2fs_bug_on during recovery
      f2fs: show # of orphan inodes
      f2fs: support in batch fzero in dnode page
      f2fs: support in batch multi blocks preallocation
      ...

commit 38f91ca8c0ea69f707c26f592dcc70f937088bcc
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed May 18 14:07:56 2016 -0700

    f2fs: flush pending bios right away when error occurs
    
    Given errors, this patch flushes pending bios as soon as possible.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index faef6667ba45..105dd3d5cf5e 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -68,7 +68,7 @@ static void f2fs_write_end_io(struct bio *bio)
 
 		if (unlikely(bio->bi_error)) {
 			set_bit(AS_EIO, &page->mapping->flags);
-			f2fs_stop_checkpoint(sbi);
+			f2fs_stop_checkpoint(sbi, true);
 		}
 		end_page_writeback(page);
 	}

commit f573018491fd823e909d587cfe16758f3dd9e6d6
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Tue May 17 16:23:36 2016 -0700

    f2fs: use bio count instead of F2FS_WRITEBACK page count
    
    This can reduce page counting overhead.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 1013836c5b6b..faef6667ba45 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -71,10 +71,9 @@ static void f2fs_write_end_io(struct bio *bio)
 			f2fs_stop_checkpoint(sbi);
 		}
 		end_page_writeback(page);
-		dec_page_count(sbi, F2FS_WRITEBACK);
 	}
-
-	if (!get_pages(sbi, F2FS_WRITEBACK) && wq_has_sleeper(&sbi->cp_wait))
+	if (atomic_dec_and_test(&sbi->nr_wb_bios) &&
+				wq_has_sleeper(&sbi->cp_wait))
 		wake_up(&sbi->cp_wait);
 
 	bio_put(bio);
@@ -98,6 +97,14 @@ static struct bio *__bio_alloc(struct f2fs_sb_info *sbi, block_t blk_addr,
 	return bio;
 }
 
+static inline void __submit_bio(struct f2fs_sb_info *sbi, int rw,
+						struct bio *bio)
+{
+	if (!is_read_io(rw))
+		atomic_inc(&sbi->nr_wb_bios);
+	submit_bio(rw, bio);
+}
+
 static void __submit_merged_bio(struct f2fs_bio_info *io)
 {
 	struct f2fs_io_info *fio = &io->fio;
@@ -110,7 +117,7 @@ static void __submit_merged_bio(struct f2fs_bio_info *io)
 	else
 		trace_f2fs_submit_write_bio(io->sbi->sb, fio, io->bio);
 
-	submit_bio(fio->rw, io->bio);
+	__submit_bio(io->sbi, fio->rw, io->bio);
 	io->bio = NULL;
 }
 
@@ -228,7 +235,7 @@ int f2fs_submit_page_bio(struct f2fs_io_info *fio)
 		return -EFAULT;
 	}
 
-	submit_bio(fio->rw, bio);
+	__submit_bio(fio->sbi, fio->rw, bio);
 	return 0;
 }
 
@@ -248,9 +255,6 @@ void f2fs_submit_page_mbio(struct f2fs_io_info *fio)
 
 	down_write(&io->io_rwsem);
 
-	if (!is_read)
-		inc_page_count(sbi, F2FS_WRITEBACK);
-
 	if (io->bio && (io->last_block_in_bio != fio->new_blkaddr - 1 ||
 						io->fio.rw != fio->rw))
 		__submit_merged_bio(io);
@@ -1047,7 +1051,7 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 		 */
 		if (bio && (last_block_in_bio != block_nr - 1)) {
 submit_and_realloc:
-			submit_bio(READ, bio);
+			__submit_bio(F2FS_I_SB(inode), READ, bio);
 			bio = NULL;
 		}
 		if (bio == NULL) {
@@ -1090,7 +1094,7 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 		goto next_page;
 confused:
 		if (bio) {
-			submit_bio(READ, bio);
+			__submit_bio(F2FS_I_SB(inode), READ, bio);
 			bio = NULL;
 		}
 		unlock_page(page);
@@ -1100,7 +1104,7 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 	}
 	BUG_ON(pages && !list_empty(pages));
 	if (bio)
-		submit_bio(READ, bio);
+		__submit_bio(F2FS_I_SB(inode), READ, bio);
 	return 0;
 }
 

commit c2e7b207058d4ff6a9010430763fb561f307eb67
Merge: c52b76185b7a 24368aad47dc
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 17 15:05:23 2016 -0700

    Merge branch 'work.preadv2' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs
    
    Pull vfs cleanups from Al Viro:
     "More cleanups from Christoph"
    
    * 'work.preadv2' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs:
      nfsd: use RWF_SYNC
      fs: add RWF_DSYNC aand RWF_SYNC
      ceph: use generic_write_sync
      fs: simplify the generic_write_sync prototype
      fs: add IOCB_SYNC and IOCB_DSYNC
      direct-io: remove the offset argument to dio_complete
      direct-io: eliminate the offset argument to ->direct_IO
      xfs: eliminate the pos variable in xfs_file_dio_aio_write
      filemap: remove the pos argument to generic_file_direct_write
      filemap: remove pos variables in generic_file_read_iter

commit ab47036d8f7227361cad7894adee8e66ab6f95b2
Author: Chao Yu <yuchao0@huawei.com>
Date:   Wed May 11 19:48:44 2016 +0800

    f2fs: fix deadlock when flush inline data
    
    Below backtrace info was reported by Yunlei He:
    
    Call Trace:
     [<ffffffff817a9395>] schedule+0x35/0x80
     [<ffffffff817abb7d>] rwsem_down_read_failed+0xed/0x130
     [<ffffffff813c12a8>] call_rwsem_down_read_failed+0x18/0x
     [<ffffffff817ab1d0>] down_read+0x20/0x30
     [<ffffffffa02a1a12>] f2fs_evict_inode+0x242/0x3a0 [f2fs]
     [<ffffffff81217057>] evict+0xc7/0x1a0
     [<ffffffff81217cd6>] iput+0x196/0x200
     [<ffffffff812134f9>] __dentry_kill+0x179/0x1e0
     [<ffffffff812136f9>] dput+0x199/0x1f0
     [<ffffffff811fe77b>] __fput+0x18b/0x220
     [<ffffffff811fe84e>] ____fput+0xe/0x10
     [<ffffffff81097427>] task_work_run+0x77/0x90
     [<ffffffff81074d62>] exit_to_usermode_loop+0x73/0xa2
     [<ffffffff81003b7a>] do_syscall_64+0xfa/0x110
     [<ffffffff817acf65>] entry_SYSCALL64_slow_path+0x25/0x25
    
    Call Trace:
     [<ffffffff817a9395>] schedule+0x35/0x80
     [<ffffffff81216dc3>] __wait_on_freeing_inode+0xa3/0xd0
     [<ffffffff810bc300>] ? autoremove_wake_function+0x40/0x4
     [<ffffffff8121771d>] find_inode_fast+0x7d/0xb0
     [<ffffffff8121794a>] ilookup+0x6a/0xd0
     [<ffffffffa02bc740>] sync_node_pages+0x210/0x650 [f2fs]
     [<ffffffff8122e690>] ? do_fsync+0x70/0x70
     [<ffffffffa02b085e>] block_operations+0x9e/0xf0 [f2fs]
     [<ffffffff8137b795>] ? bio_endio+0x55/0x60
     [<ffffffffa02b0942>] write_checkpoint+0x92/0xba0 [f2fs]
     [<ffffffff8117da57>] ? mempool_free_slab+0x17/0x20
     [<ffffffff8117de8b>] ? mempool_free+0x2b/0x80
     [<ffffffff8122e690>] ? do_fsync+0x70/0x70
     [<ffffffffa02a53e3>] f2fs_sync_fs+0x63/0xd0 [f2fs]
     [<ffffffff8129630f>] ? ext4_sync_fs+0xbf/0x190
     [<ffffffff8122e6b0>] sync_fs_one_sb+0x20/0x30
     [<ffffffff812002e9>] iterate_supers+0xb9/0x110
     [<ffffffff8122e7b5>] sys_sync+0x55/0x90
     [<ffffffff81003ae9>] do_syscall_64+0x69/0x110
     [<ffffffff817acf65>] entry_SYSCALL64_slow_path+0x25/0x25
    
    With following excuting serials, we will set inline_node in inode page
    after inode was unlinked, result in a deadloop described as below:
    1. open file
    2. write file
    3. unlink file
    4. write file
    5. close file
    
    Thread A                                Thread B
     - dput
      - iput_final
       - inode->i_state |= I_FREEING
       - evict
        - f2fs_evict_inode
                                             - f2fs_sync_fs
                                              - write_checkpoint
                                               - block_operations
                                                - f2fs_lock_all (down_write(cp_rwsem))
         - f2fs_lock_op (down_read(cp_rwsem))
                                                - sync_node_pages
                                                 - ilookup
                                                  - find_inode_fast
                                                   - __wait_on_freeing_inode
                                                     (wait on I_FREEING clear)
    
    Here, we change to set inline_node flag only for linked inode for fixing.
    
    Reported-by: Yunlei He <heyunlei@huawei.com>
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Tested-by: Jaegeuk Kim <jaegeuk@kernel.org>
    Cc: stable@vger.kernel.org # v4.6
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index b61e2d40cdfb..1013836c5b6b 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1546,7 +1546,8 @@ static int prepare_write_begin(struct f2fs_sb_info *sbi,
 		if (pos + len <= MAX_INLINE_DATA) {
 			read_inline_data(page, ipage);
 			set_inode_flag(F2FS_I(inode), FI_DATA_EXIST);
-			set_inline_node(ipage);
+			if (inode->i_nlink)
+				set_inline_node(ipage);
 		} else {
 			err = f2fs_convert_inline_page(&dn, page);
 			if (err)

commit 46008c6d42328710f9beaf5c2b47dc92b1cc1a75
Author: Chao Yu <yuchao0@huawei.com>
Date:   Mon May 9 19:56:30 2016 +0800

    f2fs: support in batch multi blocks preallocation
    
    This patch introduces reserve_new_blocks to make preallocation of multi
    blocks as in batch operation, so it can avoid lots of redundant
    operation, result in better performance.
    
    In virtual machine, with rotational device:
    
    time fallocate -l 32G /mnt/f2fs/file
    
    Before:
    real    0m4.584s
    user    0m0.000s
    sys     0m4.580s
    
    After:
    real    0m0.292s
    user    0m0.000s
    sys     0m0.272s
    
    In x86, with SSD:
    
    time fallocate -l 500G $MNT/testfile
    
    Before : 24.758 s
    After  :  1.604 s
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    [Jaegeuk Kim: fix bugs and add performance numbers measured in x86.]
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 369d953bd770..b61e2d40cdfb 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -278,6 +278,16 @@ void f2fs_submit_page_mbio(struct f2fs_io_info *fio)
 	trace_f2fs_submit_page_mbio(fio->page, fio);
 }
 
+static void __set_data_blkaddr(struct dnode_of_data *dn)
+{
+	struct f2fs_node *rn = F2FS_NODE(dn->node_page);
+	__le32 *addr_array;
+
+	/* Get physical address of data block */
+	addr_array = blkaddr_in_node(rn);
+	addr_array[dn->ofs_in_node] = cpu_to_le32(dn->data_blkaddr);
+}
+
 /*
  * Lock ordering for the change of data block address:
  * ->data_page
@@ -286,19 +296,9 @@ void f2fs_submit_page_mbio(struct f2fs_io_info *fio)
  */
 void set_data_blkaddr(struct dnode_of_data *dn)
 {
-	struct f2fs_node *rn;
-	__le32 *addr_array;
-	struct page *node_page = dn->node_page;
-	unsigned int ofs_in_node = dn->ofs_in_node;
-
-	f2fs_wait_on_page_writeback(node_page, NODE, true);
-
-	rn = F2FS_NODE(node_page);
-
-	/* Get physical address of data block */
-	addr_array = blkaddr_in_node(rn);
-	addr_array[ofs_in_node] = cpu_to_le32(dn->data_blkaddr);
-	if (set_page_dirty(node_page))
+	f2fs_wait_on_page_writeback(dn->node_page, NODE, true);
+	__set_data_blkaddr(dn);
+	if (set_page_dirty(dn->node_page))
 		dn->node_changed = true;
 }
 
@@ -309,24 +309,53 @@ void f2fs_update_data_blkaddr(struct dnode_of_data *dn, block_t blkaddr)
 	f2fs_update_extent_cache(dn);
 }
 
-int reserve_new_block(struct dnode_of_data *dn)
+/* dn->ofs_in_node will be returned with up-to-date last block pointer */
+int reserve_new_blocks(struct dnode_of_data *dn, blkcnt_t count)
 {
 	struct f2fs_sb_info *sbi = F2FS_I_SB(dn->inode);
 
+	if (!count)
+		return 0;
+
 	if (unlikely(is_inode_flag_set(F2FS_I(dn->inode), FI_NO_ALLOC)))
 		return -EPERM;
-	if (unlikely(!inc_valid_block_count(sbi, dn->inode, 1)))
+	if (unlikely(!inc_valid_block_count(sbi, dn->inode, &count)))
 		return -ENOSPC;
 
-	trace_f2fs_reserve_new_block(dn->inode, dn->nid, dn->ofs_in_node);
+	trace_f2fs_reserve_new_blocks(dn->inode, dn->nid,
+						dn->ofs_in_node, count);
+
+	f2fs_wait_on_page_writeback(dn->node_page, NODE, true);
+
+	for (; count > 0; dn->ofs_in_node++) {
+		block_t blkaddr =
+			datablock_addr(dn->node_page, dn->ofs_in_node);
+		if (blkaddr == NULL_ADDR) {
+			dn->data_blkaddr = NEW_ADDR;
+			__set_data_blkaddr(dn);
+			count--;
+		}
+	}
+
+	if (set_page_dirty(dn->node_page))
+		dn->node_changed = true;
 
-	dn->data_blkaddr = NEW_ADDR;
-	set_data_blkaddr(dn);
 	mark_inode_dirty(dn->inode);
 	sync_inode_page(dn);
 	return 0;
 }
 
+/* Should keep dn->ofs_in_node unchanged */
+int reserve_new_block(struct dnode_of_data *dn)
+{
+	unsigned int ofs_in_node = dn->ofs_in_node;
+	int ret;
+
+	ret = reserve_new_blocks(dn, 1);
+	dn->ofs_in_node = ofs_in_node;
+	return ret;
+}
+
 int f2fs_reserve_block(struct dnode_of_data *dn, pgoff_t index)
 {
 	bool need_put = dn->inode_page ? false : true;
@@ -545,6 +574,7 @@ static int __allocate_data_block(struct dnode_of_data *dn)
 	struct node_info ni;
 	int seg = CURSEG_WARM_DATA;
 	pgoff_t fofs;
+	blkcnt_t count = 1;
 
 	if (unlikely(is_inode_flag_set(F2FS_I(dn->inode), FI_NO_ALLOC)))
 		return -EPERM;
@@ -553,7 +583,7 @@ static int __allocate_data_block(struct dnode_of_data *dn)
 	if (dn->data_blkaddr == NEW_ADDR)
 		goto alloc;
 
-	if (unlikely(!inc_valid_block_count(sbi, dn->inode, 1)))
+	if (unlikely(!inc_valid_block_count(sbi, dn->inode, &count)))
 		return -ENOSPC;
 
 alloc:
@@ -621,8 +651,10 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 	struct dnode_of_data dn;
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
 	int mode = create ? ALLOC_NODE : LOOKUP_NODE_RA;
-	pgoff_t pgofs, end_offset;
+	pgoff_t pgofs, end_offset, end;
 	int err = 0, ofs = 1;
+	unsigned int ofs_in_node, last_ofs_in_node;
+	blkcnt_t prealloc;
 	struct extent_info ei;
 	bool allocated = false;
 	block_t blkaddr;
@@ -632,6 +664,7 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 
 	/* it only supports block size == page size */
 	pgofs =	(pgoff_t)map->m_lblk;
+	end = pgofs + maxblocks;
 
 	if (!create && f2fs_lookup_extent_cache(inode, pgofs, &ei)) {
 		map->m_pblk = ei.blk + pgofs - ei.fofs;
@@ -659,6 +692,8 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 		goto unlock_out;
 	}
 
+	prealloc = 0;
+	ofs_in_node = dn.ofs_in_node;
 	end_offset = ADDRS_PER_PAGE(dn.node_page, inode);
 
 next_block:
@@ -671,17 +706,20 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 				goto sync_out;
 			}
 			if (flag == F2FS_GET_BLOCK_PRE_AIO) {
-				if (blkaddr == NULL_ADDR)
-					err = reserve_new_block(&dn);
+				if (blkaddr == NULL_ADDR) {
+					prealloc++;
+					last_ofs_in_node = dn.ofs_in_node;
+				}
 			} else {
 				err = __allocate_data_block(&dn);
-				if (!err)
+				if (!err) {
 					set_inode_flag(F2FS_I(inode),
 							FI_APPEND_WRITE);
+					allocated = true;
+				}
 			}
 			if (err)
 				goto sync_out;
-			allocated = true;
 			map->m_flags = F2FS_MAP_NEW;
 			blkaddr = dn.data_blkaddr;
 		} else {
@@ -700,6 +738,9 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 		}
 	}
 
+	if (flag == F2FS_GET_BLOCK_PRE_AIO)
+		goto skip;
+
 	if (map->m_len == 0) {
 		/* preallocated unwritten block should be mapped for fiemap. */
 		if (blkaddr == NEW_ADDR)
@@ -711,32 +752,49 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 	} else if ((map->m_pblk != NEW_ADDR &&
 			blkaddr == (map->m_pblk + ofs)) ||
 			(map->m_pblk == NEW_ADDR && blkaddr == NEW_ADDR) ||
-			flag == F2FS_GET_BLOCK_PRE_DIO ||
-			flag == F2FS_GET_BLOCK_PRE_AIO) {
+			flag == F2FS_GET_BLOCK_PRE_DIO) {
 		ofs++;
 		map->m_len++;
 	} else {
 		goto sync_out;
 	}
 
+skip:
 	dn.ofs_in_node++;
 	pgofs++;
 
-	if (map->m_len < maxblocks) {
-		if (dn.ofs_in_node < end_offset)
-			goto next_block;
+	/* preallocate blocks in batch for one dnode page */
+	if (flag == F2FS_GET_BLOCK_PRE_AIO &&
+			(pgofs == end || dn.ofs_in_node == end_offset)) {
 
-		if (allocated)
-			sync_inode_page(&dn);
-		f2fs_put_dnode(&dn);
+		dn.ofs_in_node = ofs_in_node;
+		err = reserve_new_blocks(&dn, prealloc);
+		if (err)
+			goto sync_out;
 
-		if (create) {
-			f2fs_unlock_op(sbi);
-			f2fs_balance_fs(sbi, allocated);
+		map->m_len += dn.ofs_in_node - ofs_in_node;
+		if (prealloc && dn.ofs_in_node != last_ofs_in_node + 1) {
+			err = -ENOSPC;
+			goto sync_out;
 		}
-		allocated = false;
-		goto next_dnode;
+		dn.ofs_in_node = end_offset;
+	}
+
+	if (pgofs >= end)
+		goto sync_out;
+	else if (dn.ofs_in_node < end_offset)
+		goto next_block;
+
+	if (allocated)
+		sync_inode_page(&dn);
+	f2fs_put_dnode(&dn);
+
+	if (create) {
+		f2fs_unlock_op(sbi);
+		f2fs_balance_fs(sbi, allocated);
 	}
+	allocated = false;
+	goto next_dnode;
 
 sync_out:
 	if (allocated)

commit 0080c5076409d211fbe28d6f07966f7d39e58bad
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Sat May 7 08:52:57 2016 -0700

    f2fs: do not preallocate block unaligned to 4KB
    
    Previously f2fs_preallocate_blocks() tries to allocate unaligned blocks.
    In f2fs_write_begin(), however, prepare_write_begin() does not skip its
    allocation due to (len != 4KB).
    So, it needs locking node page twice unexpectedly.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 96b035319889..369d953bd770 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -582,8 +582,8 @@ ssize_t f2fs_preallocate_blocks(struct kiocb *iocb, struct iov_iter *from)
 	struct f2fs_map_blocks map;
 	ssize_t ret = 0;
 
-	map.m_lblk = F2FS_BYTES_TO_BLK(iocb->ki_pos);
-	map.m_len = F2FS_BLK_ALIGN(iov_iter_count(from));
+	map.m_lblk = F2FS_BLK_ALIGN(iocb->ki_pos);
+	map.m_len = F2FS_BYTES_TO_BLK(iov_iter_count(from));
 	map.m_next_pgofs = NULL;
 
 	if (f2fs_encrypted_inode(inode))

commit 43473f96453f0b075c480a26ec4fc846d5fb3bd4
Author: Chao Yu <yuchao0@huawei.com>
Date:   Thu May 5 19:13:02 2016 +0800

    f2fs: fix incorrect mapping in ->bmap
    
    Currently, generic_block_bmap is used in f2fs_bmap, its semantics is when
    the mapping is been found, return position of target physical block,
    otherwise return zero.
    
    But, previously, when there is no mapping info for specified logical block,
    f2fs_bmap will map target physical block to a uninitialized variable, which
    should be wrong. Fix it.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 888f1781ae0b..96b035319889 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -648,6 +648,8 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 	set_new_dnode(&dn, inode, NULL, NULL, 0);
 	err = get_dnode_of_data(&dn, pgofs, mode);
 	if (err) {
+		if (flag == F2FS_GET_BLOCK_BMAP)
+			map->m_pblk = 0;
 		if (err == -ENOENT) {
 			err = 0;
 			if (map->m_next_pgofs)
@@ -683,17 +685,18 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 			map->m_flags = F2FS_MAP_NEW;
 			blkaddr = dn.data_blkaddr;
 		} else {
+			if (flag == F2FS_GET_BLOCK_BMAP) {
+				map->m_pblk = 0;
+				goto sync_out;
+			}
 			if (flag == F2FS_GET_BLOCK_FIEMAP &&
 						blkaddr == NULL_ADDR) {
 				if (map->m_next_pgofs)
 					*map->m_next_pgofs = pgofs + 1;
 			}
 			if (flag != F2FS_GET_BLOCK_FIEMAP ||
-						blkaddr != NEW_ADDR) {
-				if (flag == F2FS_GET_BLOCK_BMAP)
-					err = -ENOENT;
+						blkaddr != NEW_ADDR)
 				goto sync_out;
-			}
 		}
 	}
 

commit 23dc974eed576b2464b222a892272073adf6a92c
Author: Chao Yu <yuchao0@huawei.com>
Date:   Fri Apr 29 20:09:15 2016 +0800

    f2fs: fix to clear private data in page
    
    Private data in page should be removed during ->releasepage or
    ->invalidatepage, otherwise garbage data would be remained in that page.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 38ce5d6f8583..888f1781ae0b 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1723,6 +1723,7 @@ void f2fs_invalidate_page(struct page *page, unsigned int offset,
 	if (IS_ATOMIC_WRITTEN_PAGE(page))
 		return;
 
+	set_page_private(page, 0);
 	ClearPagePrivate(page);
 }
 
@@ -1736,6 +1737,7 @@ int f2fs_release_page(struct page *page, gfp_t wait)
 	if (IS_ATOMIC_WRITTEN_PAGE(page))
 		return 0;
 
+	set_page_private(page, 0);
 	ClearPagePrivate(page);
 	return 1;
 }

commit c8b8e32d700fe943a935e435ae251364d016c497
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Apr 7 08:51:58 2016 -0700

    direct-io: eliminate the offset argument to ->direct_IO
    
    Including blkdev_direct_IO and dax_do_io.  It has to be ki_pos to actually
    work, so eliminate the superflous argument.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 53fec0872e60..a4c5da5bfe1e 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1655,12 +1655,12 @@ static int check_direct_IO(struct inode *inode, struct iov_iter *iter,
 	return 0;
 }
 
-static ssize_t f2fs_direct_IO(struct kiocb *iocb, struct iov_iter *iter,
-			      loff_t offset)
+static ssize_t f2fs_direct_IO(struct kiocb *iocb, struct iov_iter *iter)
 {
 	struct address_space *mapping = iocb->ki_filp->f_mapping;
 	struct inode *inode = mapping->host;
 	size_t count = iov_iter_count(iter);
+	loff_t offset = iocb->ki_pos;
 	int err;
 
 	err = check_direct_IO(inode, iter, offset);
@@ -1672,7 +1672,7 @@ static ssize_t f2fs_direct_IO(struct kiocb *iocb, struct iov_iter *iter,
 
 	trace_f2fs_direct_IO_enter(inode, offset, count, iov_iter_rw(iter));
 
-	err = blockdev_direct_IO(iocb, inode, iter, offset, get_data_block_dio);
+	err = blockdev_direct_IO(iocb, inode, iter, get_data_block_dio);
 	if (err < 0 && iov_iter_rw(iter) == WRITE)
 		f2fs_write_failed(mapping, offset + count);
 

commit 6bfc49197eba070b799ab4ca8755d3a9fd1700da
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Mon Apr 18 17:07:44 2016 -0400

    f2fs: issue cache flush on direct IO
    
    Under direct IO path with O_(D)SYNC, it needs to set proper APPEND or UPDATE
    flags, so taht f2fs_sync_file can make its data safe.
    
    Acked-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index e54489b970ae..38ce5d6f8583 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -673,6 +673,9 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 					err = reserve_new_block(&dn);
 			} else {
 				err = __allocate_data_block(&dn);
+				if (!err)
+					set_inode_flag(F2FS_I(inode),
+							FI_APPEND_WRITE);
 			}
 			if (err)
 				goto sync_out;
@@ -1685,8 +1688,12 @@ static ssize_t f2fs_direct_IO(struct kiocb *iocb, struct iov_iter *iter,
 	trace_f2fs_direct_IO_enter(inode, offset, count, iov_iter_rw(iter));
 
 	err = blockdev_direct_IO(iocb, inode, iter, offset, get_data_block_dio);
-	if (err < 0 && iov_iter_rw(iter) == WRITE)
-		f2fs_write_failed(mapping, offset + count);
+	if (iov_iter_rw(iter) == WRITE) {
+		if (err > 0)
+			set_inode_flag(F2FS_I(inode), FI_UPDATE_WRITE);
+		else if (err < 0)
+			f2fs_write_failed(mapping, offset + count);
+	}
 
 	trace_f2fs_direct_IO_exit(inode, offset, count, iov_iter_rw(iter), err);
 

commit e6e5f5610d585551785ec654b6db9277b19a0664
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Apr 14 16:48:52 2016 -0700

    f2fs: avoid writing 0'th page in volatile writes
    
    The first page of volatile writes usually contains a sort of header information
    which will be used for recovery.
    (e.g., journal header of sqlite)
    
    If this is written without other journal data, user needs to handle the stale
    journal information.
    
    Acked-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index c29bcf4cfca1..e54489b970ae 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1177,8 +1177,10 @@ static int f2fs_write_data_page(struct page *page,
 		goto redirty_out;
 	if (f2fs_is_drop_cache(inode))
 		goto out;
-	if (f2fs_is_volatile_file(inode) && !wbc->for_reclaim &&
-			available_free_memory(sbi, BASE_CHECK))
+	/* we should not write 0'th page having journal header */
+	if (f2fs_is_volatile_file(inode) && (!page->index ||
+			(!wbc->for_reclaim &&
+			available_free_memory(sbi, BASE_CHECK))))
 		goto redirty_out;
 
 	/* Dentry blocks are controlled by checkpoint */

commit 4da7bf5a4345f3ce9699476a8022f66cfb4a8ce9
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed Apr 6 11:27:03 2016 -0700

    f2fs: remove redundant condition check
    
    This patch resolves the redundant condition check reported by David.
    
    Reported-by: David Binderman <dcb314@hotmail.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 5dafb9cef12e..c29bcf4cfca1 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1496,7 +1496,7 @@ static int prepare_write_begin(struct f2fs_sb_info *sbi,
 		} else {
 			/* hole case */
 			err = get_dnode_of_data(&dn, index, LOOKUP_NODE);
-			if (err || (!err && dn.data_blkaddr == NULL_ADDR)) {
+			if (err || dn.data_blkaddr == NULL_ADDR) {
 				f2fs_put_dnode(&dn);
 				f2fs_lock_op(sbi);
 				locked = true;

commit b32e4482aadfd1322357f46d4ed8a990603664d9
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Mon Apr 11 15:51:57 2016 -0700

    fscrypto: don't let data integrity writebacks fail with ENOMEM
    
    This patch fixes the issue introduced by the ext4 crypto fix in a same manner.
    For F2FS, however, we flush the pending IOs and wait for a while to acquire free
    memory.
    
    Fixes: c9af28fdd4492 ("ext4 crypto: don't let data integrity writebacks fail with ENOMEM")
    Cc: Theodore Ts'o <tytso@mit.edu>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 53fec0872e60..5dafb9cef12e 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -992,7 +992,7 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 			if (f2fs_encrypted_inode(inode) &&
 					S_ISREG(inode->i_mode)) {
 
-				ctx = fscrypt_get_ctx(inode);
+				ctx = fscrypt_get_ctx(inode, GFP_NOFS);
 				if (IS_ERR(ctx))
 					goto set_error_page;
 
@@ -1092,14 +1092,24 @@ int do_write_data_page(struct f2fs_io_info *fio)
 	}
 
 	if (f2fs_encrypted_inode(inode) && S_ISREG(inode->i_mode)) {
+		gfp_t gfp_flags = GFP_NOFS;
 
 		/* wait for GCed encrypted page writeback */
 		f2fs_wait_on_encrypted_page_writeback(F2FS_I_SB(inode),
 							fio->old_blkaddr);
-
-		fio->encrypted_page = fscrypt_encrypt_page(inode, fio->page);
+retry_encrypt:
+		fio->encrypted_page = fscrypt_encrypt_page(inode, fio->page,
+								gfp_flags);
 		if (IS_ERR(fio->encrypted_page)) {
 			err = PTR_ERR(fio->encrypted_page);
+			if (err == -ENOMEM) {
+				/* flush pending ios and wait for a while */
+				f2fs_flush_merged_bios(F2FS_I_SB(inode));
+				congestion_wait(BLK_RW_ASYNC, HZ/50);
+				gfp_flags |= __GFP_NOFAIL;
+				err = 0;
+				goto retry_encrypt;
+			}
 			goto out_writepage;
 		}
 	}

commit 09cbfeaf1a5a67bfb3201e0c83c810cecb2efa5a
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Fri Apr 1 15:29:47 2016 +0300

    mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros
    
    PAGE_CACHE_{SIZE,SHIFT,MASK,ALIGN} macros were introduced *long* time
    ago with promise that one day it will be possible to implement page
    cache with bigger chunks than PAGE_SIZE.
    
    This promise never materialized.  And unlikely will.
    
    We have many places where PAGE_CACHE_SIZE assumed to be equal to
    PAGE_SIZE.  And it's constant source of confusion on whether
    PAGE_CACHE_* or PAGE_* constant should be used in a particular case,
    especially on the border between fs and mm.
    
    Global switching to PAGE_CACHE_SIZE != PAGE_SIZE would cause to much
    breakage to be doable.
    
    Let's stop pretending that pages in page cache are special.  They are
    not.
    
    The changes are pretty straight-forward:
    
     - <foo> << (PAGE_CACHE_SHIFT - PAGE_SHIFT) -> <foo>;
    
     - <foo> >> (PAGE_CACHE_SHIFT - PAGE_SHIFT) -> <foo>;
    
     - PAGE_CACHE_{SIZE,SHIFT,MASK,ALIGN} -> PAGE_{SIZE,SHIFT,MASK,ALIGN};
    
     - page_cache_get() -> get_page();
    
     - page_cache_release() -> put_page();
    
    This patch contains automated changes generated with coccinelle using
    script below.  For some reason, coccinelle doesn't patch header files.
    I've called spatch for them manually.
    
    The only adjustment after coccinelle is revert of changes to
    PAGE_CAHCE_ALIGN definition: we are going to drop it later.
    
    There are few places in the code where coccinelle didn't reach.  I'll
    fix them manually in a separate patch.  Comments and documentation also
    will be addressed with the separate patch.
    
    virtual patch
    
    @@
    expression E;
    @@
    - E << (PAGE_CACHE_SHIFT - PAGE_SHIFT)
    + E
    
    @@
    expression E;
    @@
    - E >> (PAGE_CACHE_SHIFT - PAGE_SHIFT)
    + E
    
    @@
    @@
    - PAGE_CACHE_SHIFT
    + PAGE_SHIFT
    
    @@
    @@
    - PAGE_CACHE_SIZE
    + PAGE_SIZE
    
    @@
    @@
    - PAGE_CACHE_MASK
    + PAGE_MASK
    
    @@
    expression E;
    @@
    - PAGE_CACHE_ALIGN(E)
    + PAGE_ALIGN(E)
    
    @@
    expression E;
    @@
    - page_cache_get(E)
    + get_page(E)
    
    @@
    expression E;
    @@
    - page_cache_release(E)
    + put_page(E)
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index e5c762b37239..53fec0872e60 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -223,7 +223,7 @@ int f2fs_submit_page_bio(struct f2fs_io_info *fio)
 	/* Allocate a new bio */
 	bio = __bio_alloc(fio->sbi, fio->new_blkaddr, 1, is_read_io(fio->rw));
 
-	if (bio_add_page(bio, page, PAGE_CACHE_SIZE, 0) < PAGE_CACHE_SIZE) {
+	if (bio_add_page(bio, page, PAGE_SIZE, 0) < PAGE_SIZE) {
 		bio_put(bio);
 		return -EFAULT;
 	}
@@ -265,8 +265,8 @@ void f2fs_submit_page_mbio(struct f2fs_io_info *fio)
 
 	bio_page = fio->encrypted_page ? fio->encrypted_page : fio->page;
 
-	if (bio_add_page(io->bio, bio_page, PAGE_CACHE_SIZE, 0) <
-							PAGE_CACHE_SIZE) {
+	if (bio_add_page(io->bio, bio_page, PAGE_SIZE, 0) <
+							PAGE_SIZE) {
 		__submit_merged_bio(io);
 		goto alloc_new;
 	}
@@ -406,7 +406,7 @@ struct page *get_read_data_page(struct inode *inode, pgoff_t index,
 	 * see, f2fs_add_link -> get_new_data_page -> init_inode_metadata.
 	 */
 	if (dn.data_blkaddr == NEW_ADDR) {
-		zero_user_segment(page, 0, PAGE_CACHE_SIZE);
+		zero_user_segment(page, 0, PAGE_SIZE);
 		SetPageUptodate(page);
 		unlock_page(page);
 		return page;
@@ -517,7 +517,7 @@ struct page *get_new_data_page(struct inode *inode,
 		goto got_it;
 
 	if (dn.data_blkaddr == NEW_ADDR) {
-		zero_user_segment(page, 0, PAGE_CACHE_SIZE);
+		zero_user_segment(page, 0, PAGE_SIZE);
 		SetPageUptodate(page);
 	} else {
 		f2fs_put_page(page, 1);
@@ -530,8 +530,8 @@ struct page *get_new_data_page(struct inode *inode,
 	}
 got_it:
 	if (new_i_size && i_size_read(inode) <
-				((loff_t)(index + 1) << PAGE_CACHE_SHIFT)) {
-		i_size_write(inode, ((loff_t)(index + 1) << PAGE_CACHE_SHIFT));
+				((loff_t)(index + 1) << PAGE_SHIFT)) {
+		i_size_write(inode, ((loff_t)(index + 1) << PAGE_SHIFT));
 		/* Only the directory inode sets new_i_size */
 		set_inode_flag(F2FS_I(inode), FI_UPDATE_DIR);
 	}
@@ -570,9 +570,9 @@ static int __allocate_data_block(struct dnode_of_data *dn)
 	/* update i_size */
 	fofs = start_bidx_of_node(ofs_of_node(dn->node_page), dn->inode) +
 							dn->ofs_in_node;
-	if (i_size_read(dn->inode) < ((loff_t)(fofs + 1) << PAGE_CACHE_SHIFT))
+	if (i_size_read(dn->inode) < ((loff_t)(fofs + 1) << PAGE_SHIFT))
 		i_size_write(dn->inode,
-				((loff_t)(fofs + 1) << PAGE_CACHE_SHIFT));
+				((loff_t)(fofs + 1) << PAGE_SHIFT));
 	return 0;
 }
 
@@ -971,7 +971,7 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 				goto confused;
 			}
 		} else {
-			zero_user_segment(page, 0, PAGE_CACHE_SIZE);
+			zero_user_segment(page, 0, PAGE_SIZE);
 			SetPageUptodate(page);
 			unlock_page(page);
 			goto next_page;
@@ -1021,7 +1021,7 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 		goto next_page;
 set_error_page:
 		SetPageError(page);
-		zero_user_segment(page, 0, PAGE_CACHE_SIZE);
+		zero_user_segment(page, 0, PAGE_SIZE);
 		unlock_page(page);
 		goto next_page;
 confused:
@@ -1032,7 +1032,7 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 		unlock_page(page);
 next_page:
 		if (pages)
-			page_cache_release(page);
+			put_page(page);
 	}
 	BUG_ON(pages && !list_empty(pages));
 	if (bio)
@@ -1136,7 +1136,7 @@ static int f2fs_write_data_page(struct page *page,
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
 	loff_t i_size = i_size_read(inode);
 	const pgoff_t end_index = ((unsigned long long) i_size)
-							>> PAGE_CACHE_SHIFT;
+							>> PAGE_SHIFT;
 	unsigned offset = 0;
 	bool need_balance_fs = false;
 	int err = 0;
@@ -1157,11 +1157,11 @@ static int f2fs_write_data_page(struct page *page,
 	 * If the offset is out-of-range of file size,
 	 * this page does not have to be written to disk.
 	 */
-	offset = i_size & (PAGE_CACHE_SIZE - 1);
+	offset = i_size & (PAGE_SIZE - 1);
 	if ((page->index >= end_index + 1) || !offset)
 		goto out;
 
-	zero_user_segment(page, offset, PAGE_CACHE_SIZE);
+	zero_user_segment(page, offset, PAGE_SIZE);
 write:
 	if (unlikely(is_sbi_flag_set(sbi, SBI_POR_DOING)))
 		goto redirty_out;
@@ -1267,8 +1267,8 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 			cycled = 0;
 		end = -1;
 	} else {
-		index = wbc->range_start >> PAGE_CACHE_SHIFT;
-		end = wbc->range_end >> PAGE_CACHE_SHIFT;
+		index = wbc->range_start >> PAGE_SHIFT;
+		end = wbc->range_end >> PAGE_SHIFT;
 		if (wbc->range_start == 0 && wbc->range_end == LLONG_MAX)
 			range_whole = 1;
 		cycled = 1; /* ignore range_cyclic tests */
@@ -1448,11 +1448,11 @@ static int prepare_write_begin(struct f2fs_sb_info *sbi,
 	 * the block addresses when there is no need to fill the page.
 	 */
 	if (!f2fs_has_inline_data(inode) && !f2fs_encrypted_inode(inode) &&
-					len == PAGE_CACHE_SIZE)
+					len == PAGE_SIZE)
 		return 0;
 
 	if (f2fs_has_inline_data(inode) ||
-			(pos & PAGE_CACHE_MASK) >= i_size_read(inode)) {
+			(pos & PAGE_MASK) >= i_size_read(inode)) {
 		f2fs_lock_op(sbi);
 		locked = true;
 	}
@@ -1513,7 +1513,7 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 	struct inode *inode = mapping->host;
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
 	struct page *page = NULL;
-	pgoff_t index = ((unsigned long long) pos) >> PAGE_CACHE_SHIFT;
+	pgoff_t index = ((unsigned long long) pos) >> PAGE_SHIFT;
 	bool need_balance = false;
 	block_t blkaddr = NULL_ADDR;
 	int err = 0;
@@ -1561,22 +1561,22 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 	if (f2fs_encrypted_inode(inode) && S_ISREG(inode->i_mode))
 		f2fs_wait_on_encrypted_page_writeback(sbi, blkaddr);
 
-	if (len == PAGE_CACHE_SIZE)
+	if (len == PAGE_SIZE)
 		goto out_update;
 	if (PageUptodate(page))
 		goto out_clear;
 
-	if ((pos & PAGE_CACHE_MASK) >= i_size_read(inode)) {
-		unsigned start = pos & (PAGE_CACHE_SIZE - 1);
+	if ((pos & PAGE_MASK) >= i_size_read(inode)) {
+		unsigned start = pos & (PAGE_SIZE - 1);
 		unsigned end = start + len;
 
 		/* Reading beyond i_size is simple: memset to zero */
-		zero_user_segments(page, 0, start, end, PAGE_CACHE_SIZE);
+		zero_user_segments(page, 0, start, end, PAGE_SIZE);
 		goto out_update;
 	}
 
 	if (blkaddr == NEW_ADDR) {
-		zero_user_segment(page, 0, PAGE_CACHE_SIZE);
+		zero_user_segment(page, 0, PAGE_SIZE);
 	} else {
 		struct f2fs_io_info fio = {
 			.sbi = sbi,
@@ -1688,7 +1688,7 @@ void f2fs_invalidate_page(struct page *page, unsigned int offset,
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
 
 	if (inode->i_ino >= F2FS_ROOT_INO(sbi) &&
-		(offset % PAGE_CACHE_SIZE || length != PAGE_CACHE_SIZE))
+		(offset % PAGE_SIZE || length != PAGE_SIZE))
 		return;
 
 	if (PageDirty(page)) {

commit 0b81d0779072696371822e5ed9e7c6292e547024
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri May 15 16:26:10 2015 -0700

    fs crypto: move per-file encryption from f2fs tree to fs/crypto
    
    This patch adds the renamed functions moved from the f2fs crypto files.
    
    1. definitions for per-file encryption used by ext4 and f2fs.
    
    2. crypto.c for encrypt/decrypt functions
     a. IO preparation:
      - fscrypt_get_ctx / fscrypt_release_ctx
     b. before IOs:
      - fscrypt_encrypt_page
      - fscrypt_decrypt_page
      - fscrypt_zeroout_range
     c. after IOs:
      - fscrypt_decrypt_bio_pages
      - fscrypt_pullback_bio_page
      - fscrypt_restore_control_page
    
    3. policy.c supporting context management.
     a. For ioctls:
      - fscrypt_process_policy
      - fscrypt_get_policy
     b. For context permission
      - fscrypt_has_permitted_context
      - fscrypt_inherit_context
    
    4. keyinfo.c to handle permissions
      - fscrypt_get_encryption_info
      - fscrypt_free_encryption_info
    
    5. fname.c to support filename encryption
     a. general wrapper functions
      - fscrypt_fname_disk_to_usr
      - fscrypt_fname_usr_to_disk
      - fscrypt_setup_filename
      - fscrypt_free_filename
    
     b. specific filename handling functions
      - fscrypt_fname_alloc_buffer
      - fscrypt_fname_free_buffer
    
    6. Makefile and Kconfig
    
    Cc: Al Viro <viro@ftp.linux.org.uk>
    Signed-off-by: Michael Halcrow <mhalcrow@google.com>
    Signed-off-by: Ildar Muslukhov <ildarm@google.com>
    Signed-off-by: Uday Savagaonkar <savagaon@google.com>
    Signed-off-by: Theodore Ts'o <tytso@mit.edu>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 9643d88a01af..e5c762b37239 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -34,9 +34,9 @@ static void f2fs_read_end_io(struct bio *bio)
 
 	if (f2fs_bio_encrypted(bio)) {
 		if (bio->bi_error) {
-			f2fs_release_crypto_ctx(bio->bi_private);
+			fscrypt_release_ctx(bio->bi_private);
 		} else {
-			f2fs_end_io_crypto_work(bio->bi_private, bio);
+			fscrypt_decrypt_bio_pages(bio->bi_private, bio);
 			return;
 		}
 	}
@@ -64,7 +64,7 @@ static void f2fs_write_end_io(struct bio *bio)
 	bio_for_each_segment_all(bvec, bio, i) {
 		struct page *page = bvec->bv_page;
 
-		f2fs_restore_and_release_control_page(&page);
+		fscrypt_pullback_bio_page(&page, true);
 
 		if (unlikely(bio->bi_error)) {
 			set_bit(AS_EIO, &page->mapping->flags);
@@ -129,16 +129,10 @@ static bool __has_merged_page(struct f2fs_bio_info *io, struct inode *inode,
 
 	bio_for_each_segment_all(bvec, io->bio, i) {
 
-		if (bvec->bv_page->mapping) {
+		if (bvec->bv_page->mapping)
 			target = bvec->bv_page;
-		} else {
-			struct f2fs_crypto_ctx *ctx;
-
-			/* encrypted page */
-			ctx = (struct f2fs_crypto_ctx *)page_private(
-								bvec->bv_page);
-			target = ctx->w.control_page;
-		}
+		else
+			target = fscrypt_control_page(bvec->bv_page);
 
 		if (inode && inode == target->mapping->host)
 			return true;
@@ -220,7 +214,8 @@ void f2fs_flush_merged_bios(struct f2fs_sb_info *sbi)
 int f2fs_submit_page_bio(struct f2fs_io_info *fio)
 {
 	struct bio *bio;
-	struct page *page = fio->encrypted_page ? fio->encrypted_page : fio->page;
+	struct page *page = fio->encrypted_page ?
+			fio->encrypted_page : fio->page;
 
 	trace_f2fs_submit_page_bio(page, fio);
 	f2fs_trace_ios(fio, 0);
@@ -992,12 +987,12 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 			bio = NULL;
 		}
 		if (bio == NULL) {
-			struct f2fs_crypto_ctx *ctx = NULL;
+			struct fscrypt_ctx *ctx = NULL;
 
 			if (f2fs_encrypted_inode(inode) &&
 					S_ISREG(inode->i_mode)) {
 
-				ctx = f2fs_get_crypto_ctx(inode);
+				ctx = fscrypt_get_ctx(inode);
 				if (IS_ERR(ctx))
 					goto set_error_page;
 
@@ -1010,7 +1005,7 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 				min_t(int, nr_pages, BIO_MAX_PAGES));
 			if (!bio) {
 				if (ctx)
-					f2fs_release_crypto_ctx(ctx);
+					fscrypt_release_ctx(ctx);
 				goto set_error_page;
 			}
 			bio->bi_bdev = bdev;
@@ -1102,7 +1097,7 @@ int do_write_data_page(struct f2fs_io_info *fio)
 		f2fs_wait_on_encrypted_page_writeback(F2FS_I_SB(inode),
 							fio->old_blkaddr);
 
-		fio->encrypted_page = f2fs_encrypt(inode, fio->page);
+		fio->encrypted_page = fscrypt_encrypt_page(inode, fio->page);
 		if (IS_ERR(fio->encrypted_page)) {
 			err = PTR_ERR(fio->encrypted_page);
 			goto out_writepage;
@@ -1608,7 +1603,7 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 
 		/* avoid symlink page */
 		if (f2fs_encrypted_inode(inode) && S_ISREG(inode->i_mode)) {
-			err = f2fs_decrypt(page);
+			err = fscrypt_decrypt_page(page);
 			if (err)
 				goto fail;
 		}

commit 406657dd1868a4d7b12be04ed769d9215ec9a7d1
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Wed Feb 24 17:17:55 2016 +0800

    f2fs: introduce f2fs_flush_merged_bios for cleanup
    
    Add a new helper f2fs_flush_merged_bios to clean up redundant codes.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index cc5ce9f5ad80..9643d88a01af 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -206,6 +206,13 @@ void f2fs_submit_merged_bio_cond(struct f2fs_sb_info *sbi,
 		__f2fs_submit_merged_bio(sbi, inode, page, ino, type, rw);
 }
 
+void f2fs_flush_merged_bios(struct f2fs_sb_info *sbi)
+{
+	f2fs_submit_merged_bio(sbi, DATA, WRITE);
+	f2fs_submit_merged_bio(sbi, NODE, WRITE);
+	f2fs_submit_merged_bio(sbi, META, WRITE);
+}
+
 /*
  * Fill the locked page with data located in the block address.
  * Return unlocked page.

commit f28b3434afb8bb586965970039e46ffb6a1be033
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Wed Feb 24 17:16:47 2016 +0800

    f2fs: introduce f2fs_update_data_blkaddr for cleanup
    
    Add a new help f2fs_update_data_blkaddr to clean up redundant codes.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 8c736838f13c..cc5ce9f5ad80 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -300,6 +300,13 @@ void set_data_blkaddr(struct dnode_of_data *dn)
 		dn->node_changed = true;
 }
 
+void f2fs_update_data_blkaddr(struct dnode_of_data *dn, block_t blkaddr)
+{
+	dn->data_blkaddr = blkaddr;
+	set_data_blkaddr(dn);
+	f2fs_update_extent_cache(dn);
+}
+
 int reserve_new_block(struct dnode_of_data *dn)
 {
 	struct f2fs_sb_info *sbi = F2FS_I_SB(dn->inode);
@@ -1110,8 +1117,6 @@ int do_write_data_page(struct f2fs_io_info *fio)
 		trace_f2fs_do_write_data_page(page, IPU);
 	} else {
 		write_data_page(&dn, fio);
-		set_data_blkaddr(&dn);
-		f2fs_update_extent_cache(&dn);
 		trace_f2fs_do_write_data_page(page, OPU);
 		set_inode_flag(F2FS_I(inode), FI_APPEND_WRITE);
 		if (page->index == 0)

commit 7a9d75481b85d59204d76097d41a28db663a7a43
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Mon Feb 22 18:36:38 2016 +0800

    f2fs: trace old block address for CoWed page
    
    This patch enables to trace old block address of CoWed page for better
    debugging.
    
    f2fs_submit_page_mbio: dev = (1,0), ino = 1, page_index = 0x1d4f0, oldaddr = 0xfe8ab, newaddr = 0xfee90 rw = WRITE_SYNC, type = NODE
    f2fs_submit_page_mbio: dev = (1,0), ino = 1, page_index = 0x1d4f8, oldaddr = 0xfe8b0, newaddr = 0xfee91 rw = WRITE_SYNC, type = NODE
    f2fs_submit_page_mbio: dev = (1,0), ino = 1, page_index = 0x1d4fa, oldaddr = 0xfe8ae, newaddr = 0xfee92 rw = WRITE_SYNC, type = NODE
    
    f2fs_submit_page_mbio: dev = (1,0), ino = 134824, page_index = 0x96, oldaddr = 0xf049b, newaddr = 0x2bbe rw = WRITE, type = DATA
    f2fs_submit_page_mbio: dev = (1,0), ino = 134824, page_index = 0x97, oldaddr = 0xf049c, newaddr = 0x2bbf rw = WRITE, type = DATA
    f2fs_submit_page_mbio: dev = (1,0), ino = 134824, page_index = 0x98, oldaddr = 0xf049d, newaddr = 0x2bc0 rw = WRITE, type = DATA
    
    f2fs_submit_page_mbio: dev = (1,0), ino = 135260, page_index = 0x47, oldaddr = 0xffffffff, newaddr = 0xf2631 rw = WRITE, type = DATA
    f2fs_submit_page_mbio: dev = (1,0), ino = 135260, page_index = 0x48, oldaddr = 0xffffffff, newaddr = 0xf2632 rw = WRITE, type = DATA
    f2fs_submit_page_mbio: dev = (1,0), ino = 135260, page_index = 0x49, oldaddr = 0xffffffff, newaddr = 0xf2633 rw = WRITE, type = DATA
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 8b46e5d9bcdd..8c736838f13c 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -219,7 +219,7 @@ int f2fs_submit_page_bio(struct f2fs_io_info *fio)
 	f2fs_trace_ios(fio, 0);
 
 	/* Allocate a new bio */
-	bio = __bio_alloc(fio->sbi, fio->blk_addr, 1, is_read_io(fio->rw));
+	bio = __bio_alloc(fio->sbi, fio->new_blkaddr, 1, is_read_io(fio->rw));
 
 	if (bio_add_page(bio, page, PAGE_CACHE_SIZE, 0) < PAGE_CACHE_SIZE) {
 		bio_put(bio);
@@ -240,21 +240,24 @@ void f2fs_submit_page_mbio(struct f2fs_io_info *fio)
 
 	io = is_read ? &sbi->read_io : &sbi->write_io[btype];
 
-	verify_block_addr(sbi, fio->blk_addr);
+	if (fio->old_blkaddr != NEW_ADDR)
+		verify_block_addr(sbi, fio->old_blkaddr);
+	verify_block_addr(sbi, fio->new_blkaddr);
 
 	down_write(&io->io_rwsem);
 
 	if (!is_read)
 		inc_page_count(sbi, F2FS_WRITEBACK);
 
-	if (io->bio && (io->last_block_in_bio != fio->blk_addr - 1 ||
+	if (io->bio && (io->last_block_in_bio != fio->new_blkaddr - 1 ||
 						io->fio.rw != fio->rw))
 		__submit_merged_bio(io);
 alloc_new:
 	if (io->bio == NULL) {
 		int bio_blocks = MAX_BIO_BLOCKS(sbi);
 
-		io->bio = __bio_alloc(sbi, fio->blk_addr, bio_blocks, is_read);
+		io->bio = __bio_alloc(sbi, fio->new_blkaddr,
+						bio_blocks, is_read);
 		io->fio = *fio;
 	}
 
@@ -266,7 +269,7 @@ void f2fs_submit_page_mbio(struct f2fs_io_info *fio)
 		goto alloc_new;
 	}
 
-	io->last_block_in_bio = fio->blk_addr;
+	io->last_block_in_bio = fio->new_blkaddr;
 	f2fs_trace_ios(fio, 0);
 
 	up_write(&io->io_rwsem);
@@ -400,7 +403,7 @@ struct page *get_read_data_page(struct inode *inode, pgoff_t index,
 		return page;
 	}
 
-	fio.blk_addr = dn.data_blkaddr;
+	fio.new_blkaddr = fio.old_blkaddr = dn.data_blkaddr;
 	fio.page = page;
 	err = f2fs_submit_page_bio(&fio);
 	if (err)
@@ -1071,11 +1074,10 @@ int do_write_data_page(struct f2fs_io_info *fio)
 	if (err)
 		return err;
 
-	fio->blk_addr = dn.data_blkaddr;
 	fio->old_blkaddr = dn.data_blkaddr;
 
 	/* This page is already truncated */
-	if (fio->blk_addr == NULL_ADDR) {
+	if (fio->old_blkaddr == NULL_ADDR) {
 		ClearPageUptodate(page);
 		goto out_writepage;
 	}
@@ -1084,7 +1086,7 @@ int do_write_data_page(struct f2fs_io_info *fio)
 
 		/* wait for GCed encrypted page writeback */
 		f2fs_wait_on_encrypted_page_writeback(F2FS_I_SB(inode),
-							fio->blk_addr);
+							fio->old_blkaddr);
 
 		fio->encrypted_page = f2fs_encrypt(inode, fio->page);
 		if (IS_ERR(fio->encrypted_page)) {
@@ -1099,7 +1101,7 @@ int do_write_data_page(struct f2fs_io_info *fio)
 	 * If current allocation needs SSR,
 	 * it had better in-place writes for updated data.
 	 */
-	if (unlikely(fio->blk_addr != NEW_ADDR &&
+	if (unlikely(fio->old_blkaddr != NEW_ADDR &&
 			!is_cold_data(page) &&
 			!IS_ATOMIC_WRITTEN_PAGE(page) &&
 			need_inplace_update(inode))) {
@@ -1573,7 +1575,8 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 			.sbi = sbi,
 			.type = DATA,
 			.rw = READ_SYNC,
-			.blk_addr = blkaddr,
+			.old_blkaddr = blkaddr,
+			.new_blkaddr = blkaddr,
 			.page = page,
 			.encrypted_page = NULL,
 		};

commit 28bc106b2346a7348706bf86d9efbe31920c69f3
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Sat Feb 6 14:40:34 2016 +0800

    f2fs: support revoking atomic written pages
    
    f2fs support atomic write with following semantics:
    1. open db file
    2. ioctl start atomic write
    3. (write db file) * n
    4. ioctl commit atomic write
    5. close db file
    
    With this flow we can avoid file becoming corrupted when abnormal power
    cut, because we hold data of transaction in referenced pages linked in
    inmem_pages list of inode, but without setting them dirty, so these data
    won't be persisted unless we commit them in step 4.
    
    But we should still hold journal db file in memory by using volatile
    write, because our semantics of 'atomic write support' is incomplete, in
    step 4, we could fail to submit all dirty data of transaction, once
    partial dirty data was committed in storage, then after a checkpoint &
    abnormal power-cut, db file will be corrupted forever.
    
    So this patch tries to improve atomic write flow by adding a revoking flow,
    once inner error occurs in committing, this gives another chance to try to
    revoke these partial submitted data of current transaction, it makes
    committing operation more like aotmical one.
    
    If we're not lucky, once revoking operation was failed, EAGAIN will be
    reported to user for suggesting doing the recovery with held journal file,
    or retrying current transaction again.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index ca99a2aca107..8b46e5d9bcdd 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1072,6 +1072,7 @@ int do_write_data_page(struct f2fs_io_info *fio)
 		return err;
 
 	fio->blk_addr = dn.data_blkaddr;
+	fio->old_blkaddr = dn.data_blkaddr;
 
 	/* This page is already truncated */
 	if (fio->blk_addr == NULL_ADDR) {

commit ce855a3bd0922f548a3c4937d6447c2ed4d4b1bc
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Feb 5 19:34:31 2016 -0800

    f2fs crypto: f2fs_page_crypto() doesn't need a encryption context
    
    This patch adopts:
            ext4 crypto: ext4_page_crypto() doesn't need a encryption context
    
    Since ext4_page_crypto() doesn't need an encryption context (at least
    not any more), this allows us to simplify a number function signature
    and also allows us to avoid needing to allocate a context in
    ext4_block_write_begin().  It also means we no longer need a separate
    ext4_decrypt_one() function.
    
    Signed-off-by: Theodore Ts'o <tytso@mit.edu>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 03f948e84115..ca99a2aca107 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1592,7 +1592,7 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 
 		/* avoid symlink page */
 		if (f2fs_encrypted_inode(inode) && S_ISREG(inode->i_mode)) {
-			err = f2fs_decrypt_one(inode, page);
+			err = f2fs_decrypt(page);
 			if (err)
 				goto fail;
 		}

commit 24b8491251cde66879e74092167cc0f27a1f11ce
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed Feb 3 13:49:44 2016 -0800

    f2fs: preallocate blocks for buffered aio writes
    
    This patch preallocates data blocks for buffered aio writes.
    With this patch, we can avoid redundant locking and unlocking of node pages
    given consecutive aio request.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index e7815ace6053..03f948e84115 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -571,16 +571,25 @@ ssize_t f2fs_preallocate_blocks(struct kiocb *iocb, struct iov_iter *from)
 	ssize_t ret = 0;
 
 	map.m_lblk = F2FS_BYTES_TO_BLK(iocb->ki_pos);
-	map.m_len = F2FS_BYTES_TO_BLK(iov_iter_count(from));
+	map.m_len = F2FS_BLK_ALIGN(iov_iter_count(from));
 	map.m_next_pgofs = NULL;
 
-	if (iocb->ki_flags & IOCB_DIRECT &&
-		!(f2fs_encrypted_inode(inode) && S_ISREG(inode->i_mode))) {
+	if (f2fs_encrypted_inode(inode))
+		return 0;
+
+	if (iocb->ki_flags & IOCB_DIRECT) {
+		ret = f2fs_convert_inline_inode(inode);
+		if (ret)
+			return ret;
+		return f2fs_map_blocks(inode, &map, 1, F2FS_GET_BLOCK_PRE_DIO);
+	}
+	if (iocb->ki_pos + iov_iter_count(from) > MAX_INLINE_DATA) {
 		ret = f2fs_convert_inline_inode(inode);
 		if (ret)
 			return ret;
-		ret = f2fs_map_blocks(inode, &map, 1, F2FS_GET_BLOCK_PRE_DIO);
 	}
+	if (!f2fs_has_inline_data(inode))
+		return f2fs_map_blocks(inode, &map, 1, F2FS_GET_BLOCK_PRE_AIO);
 	return ret;
 }
 
@@ -612,7 +621,7 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 	/* it only supports block size == page size */
 	pgofs =	(pgoff_t)map->m_lblk;
 
-	if (f2fs_lookup_extent_cache(inode, pgofs, &ei)) {
+	if (!create && f2fs_lookup_extent_cache(inode, pgofs, &ei)) {
 		map->m_pblk = ei.blk + pgofs - ei.fofs;
 		map->m_len = min((pgoff_t)maxblocks, ei.fofs + ei.len - pgofs);
 		map->m_flags = F2FS_MAP_MAPPED;
@@ -647,7 +656,12 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 				err = -EIO;
 				goto sync_out;
 			}
-			err = __allocate_data_block(&dn);
+			if (flag == F2FS_GET_BLOCK_PRE_AIO) {
+				if (blkaddr == NULL_ADDR)
+					err = reserve_new_block(&dn);
+			} else {
+				err = __allocate_data_block(&dn);
+			}
 			if (err)
 				goto sync_out;
 			allocated = true;
@@ -679,7 +693,8 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 	} else if ((map->m_pblk != NEW_ADDR &&
 			blkaddr == (map->m_pblk + ofs)) ||
 			(map->m_pblk == NEW_ADDR && blkaddr == NEW_ADDR) ||
-			flag == F2FS_GET_BLOCK_PRE_DIO) {
+			flag == F2FS_GET_BLOCK_PRE_DIO ||
+			flag == F2FS_GET_BLOCK_PRE_AIO) {
 		ofs++;
 		map->m_len++;
 	} else {
@@ -1418,6 +1433,14 @@ static int prepare_write_begin(struct f2fs_sb_info *sbi,
 	struct extent_info ei;
 	int err = 0;
 
+	/*
+	 * we already allocated all the blocks, so we don't need to get
+	 * the block addresses when there is no need to fill the page.
+	 */
+	if (!f2fs_has_inline_data(inode) && !f2fs_encrypted_inode(inode) &&
+					len == PAGE_CACHE_SIZE)
+		return 0;
+
 	if (f2fs_has_inline_data(inode) ||
 			(pos & PAGE_CACHE_MASK) >= i_size_read(inode)) {
 		f2fs_lock_op(sbi);

commit b439b103a6c9eb3417f34b4a609d4e00b4c59aca
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed Feb 3 13:09:09 2016 -0800

    f2fs: move dio preallocation into f2fs_file_write_iter
    
    This patch moves preallocation code for direct IOs into f2fs_file_write_iter.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 657ab8707b58..e7815ace6053 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -564,16 +564,24 @@ static int __allocate_data_block(struct dnode_of_data *dn)
 	return 0;
 }
 
-static int __allocate_data_blocks(struct inode *inode, loff_t offset,
-							size_t count)
+ssize_t f2fs_preallocate_blocks(struct kiocb *iocb, struct iov_iter *from)
 {
+	struct inode *inode = file_inode(iocb->ki_filp);
 	struct f2fs_map_blocks map;
+	ssize_t ret = 0;
 
-	map.m_lblk = F2FS_BYTES_TO_BLK(offset);
-	map.m_len = F2FS_BYTES_TO_BLK(count);
+	map.m_lblk = F2FS_BYTES_TO_BLK(iocb->ki_pos);
+	map.m_len = F2FS_BYTES_TO_BLK(iov_iter_count(from));
 	map.m_next_pgofs = NULL;
 
-	return f2fs_map_blocks(inode, &map, 1, F2FS_GET_BLOCK_DIO);
+	if (iocb->ki_flags & IOCB_DIRECT &&
+		!(f2fs_encrypted_inode(inode) && S_ISREG(inode->i_mode))) {
+		ret = f2fs_convert_inline_inode(inode);
+		if (ret)
+			return ret;
+		ret = f2fs_map_blocks(inode, &map, 1, F2FS_GET_BLOCK_PRE_DIO);
+	}
+	return ret;
 }
 
 /*
@@ -670,7 +678,8 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 		map->m_len = 1;
 	} else if ((map->m_pblk != NEW_ADDR &&
 			blkaddr == (map->m_pblk + ofs)) ||
-			(map->m_pblk == NEW_ADDR && blkaddr == NEW_ADDR)) {
+			(map->m_pblk == NEW_ADDR && blkaddr == NEW_ADDR) ||
+			flag == F2FS_GET_BLOCK_PRE_DIO) {
 		ofs++;
 		map->m_len++;
 	} else {
@@ -1615,34 +1624,21 @@ static int check_direct_IO(struct inode *inode, struct iov_iter *iter,
 static ssize_t f2fs_direct_IO(struct kiocb *iocb, struct iov_iter *iter,
 			      loff_t offset)
 {
-	struct file *file = iocb->ki_filp;
-	struct address_space *mapping = file->f_mapping;
+	struct address_space *mapping = iocb->ki_filp->f_mapping;
 	struct inode *inode = mapping->host;
 	size_t count = iov_iter_count(iter);
 	int err;
 
-	/* we don't need to use inline_data strictly */
-	err = f2fs_convert_inline_inode(inode);
+	err = check_direct_IO(inode, iter, offset);
 	if (err)
 		return err;
 
 	if (f2fs_encrypted_inode(inode) && S_ISREG(inode->i_mode))
 		return 0;
 
-	err = check_direct_IO(inode, iter, offset);
-	if (err)
-		return err;
-
 	trace_f2fs_direct_IO_enter(inode, offset, count, iov_iter_rw(iter));
 
-	if (iov_iter_rw(iter) == WRITE) {
-		err = __allocate_data_blocks(inode, offset, count);
-		if (err)
-			goto out;
-	}
-
 	err = blockdev_direct_IO(iocb, inode, iter, offset, get_data_block_dio);
-out:
 	if (err < 0 && iov_iter_rw(iter) == WRITE)
 		f2fs_write_failed(mapping, offset + count);
 

commit d31c7c3f0b003358a68c5c9a660ea2be13a3ca67
Author: Yunlei He <heyunlei@huawei.com>
Date:   Thu Feb 4 16:14:00 2016 +0800

    f2fs: fix missing skip pages info
    
    fix missing skip pages info in f2fs_writepages trace event.
    
    Signed-off-by: Yunlei He <heyunlei@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 9ae43a725d54..657ab8707b58 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1341,8 +1341,6 @@ static int f2fs_write_data_pages(struct address_space *mapping,
 	int ret;
 	long diff;
 
-	trace_f2fs_writepages(mapping->host, wbc, DATA);
-
 	/* deal with chardevs and other special file */
 	if (!mapping->a_ops->writepage)
 		return 0;
@@ -1364,6 +1362,8 @@ static int f2fs_write_data_pages(struct address_space *mapping,
 	if (unlikely(is_sbi_flag_set(sbi, SBI_POR_DOING)))
 		goto skip_write;
 
+	trace_f2fs_writepages(mapping->host, wbc, DATA);
+
 	diff = nr_pages_to_write(sbi, DATA, wbc);
 
 	if (!S_ISDIR(inode->i_mode) && wbc->sync_mode == WB_SYNC_ALL) {
@@ -1382,6 +1382,7 @@ static int f2fs_write_data_pages(struct address_space *mapping,
 
 skip_write:
 	wbc->pages_skipped += get_dirty_pages(inode);
+	trace_f2fs_writepages(mapping->host, wbc, DATA);
 	return 0;
 }
 

commit 0c3a579758362d5c713bb8ecc85ef82eccd56db0
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Mon Jan 18 18:28:11 2016 +0800

    f2fs: introduce f2fs_submit_merged_bio_cond
    
    f2fs use single bio buffer per type data (META/NODE/DATA) for caching
    writes locating in continuous block address as many as possible, after
    submitting, these writes may be still cached in bio buffer, so we have
    to flush cached writes in bio buffer by calling f2fs_submit_merged_bio.
    
    Unfortunately, in the scenario of high concurrency, bio buffer could be
    flushed by someone else before we submit it as below reasons:
    a) there is no space in bio buffer.
    b) add a request of different type (SYNC, ASYNC).
    c) add a discontinuous block address.
    
    For this condition, f2fs_submit_merged_bio will be devastating, because
    it could break the following merging of writes in bio buffer, split one
    big bio into two smaller one.
    
    This patch introduces f2fs_submit_merged_bio_cond which can do a
    conditional submitting with bio buffer, before submitting it will judge
    whether:
     - page in DATA type bio buffer is matching with specified page;
     - page in DATA type bio buffer is belong to specified inode;
     - page in NODE type bio buffer is belong to specified inode;
    If there is no eligible page in bio buffer, we will skip submitting step,
    result in gaining more chance to merge consecutive block IOs in bio cache.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index e0b09c33fc0f..9ae43a725d54 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -114,20 +114,18 @@ static void __submit_merged_bio(struct f2fs_bio_info *io)
 	io->bio = NULL;
 }
 
-bool is_merged_page(struct f2fs_sb_info *sbi, struct page *page,
-							enum page_type type)
+static bool __has_merged_page(struct f2fs_bio_info *io, struct inode *inode,
+						struct page *page, nid_t ino)
 {
-	enum page_type btype = PAGE_TYPE_OF_BIO(type);
-	struct f2fs_bio_info *io = &sbi->write_io[btype];
 	struct bio_vec *bvec;
 	struct page *target;
 	int i;
 
-	down_read(&io->io_rwsem);
-	if (!io->bio) {
-		up_read(&io->io_rwsem);
+	if (!io->bio)
 		return false;
-	}
+
+	if (!inode && !page && !ino)
+		return true;
 
 	bio_for_each_segment_all(bvec, io->bio, i) {
 
@@ -142,18 +140,34 @@ bool is_merged_page(struct f2fs_sb_info *sbi, struct page *page,
 			target = ctx->w.control_page;
 		}
 
-		if (page == target) {
-			up_read(&io->io_rwsem);
+		if (inode && inode == target->mapping->host)
+			return true;
+		if (page && page == target)
+			return true;
+		if (ino && ino == ino_of_node(target))
 			return true;
-		}
 	}
 
-	up_read(&io->io_rwsem);
 	return false;
 }
 
-void f2fs_submit_merged_bio(struct f2fs_sb_info *sbi,
-				enum page_type type, int rw)
+static bool has_merged_page(struct f2fs_sb_info *sbi, struct inode *inode,
+						struct page *page, nid_t ino,
+						enum page_type type)
+{
+	enum page_type btype = PAGE_TYPE_OF_BIO(type);
+	struct f2fs_bio_info *io = &sbi->write_io[btype];
+	bool ret;
+
+	down_read(&io->io_rwsem);
+	ret = __has_merged_page(io, inode, page, ino);
+	up_read(&io->io_rwsem);
+	return ret;
+}
+
+static void __f2fs_submit_merged_bio(struct f2fs_sb_info *sbi,
+				struct inode *inode, struct page *page,
+				nid_t ino, enum page_type type, int rw)
 {
 	enum page_type btype = PAGE_TYPE_OF_BIO(type);
 	struct f2fs_bio_info *io;
@@ -162,6 +176,9 @@ void f2fs_submit_merged_bio(struct f2fs_sb_info *sbi,
 
 	down_write(&io->io_rwsem);
 
+	if (!__has_merged_page(io, inode, page, ino))
+		goto out;
+
 	/* change META to META_FLUSH in the checkpoint procedure */
 	if (type >= META_FLUSH) {
 		io->fio.type = META_FLUSH;
@@ -171,9 +188,24 @@ void f2fs_submit_merged_bio(struct f2fs_sb_info *sbi,
 			io->fio.rw = WRITE_FLUSH_FUA | REQ_META | REQ_PRIO;
 	}
 	__submit_merged_bio(io);
+out:
 	up_write(&io->io_rwsem);
 }
 
+void f2fs_submit_merged_bio(struct f2fs_sb_info *sbi, enum page_type type,
+									int rw)
+{
+	__f2fs_submit_merged_bio(sbi, NULL, NULL, 0, type, rw);
+}
+
+void f2fs_submit_merged_bio_cond(struct f2fs_sb_info *sbi,
+				struct inode *inode, struct page *page,
+				nid_t ino, enum page_type type, int rw)
+{
+	if (has_merged_page(sbi, inode, page, ino, type))
+		__f2fs_submit_merged_bio(sbi, inode, page, ino, type, rw);
+}
+
 /*
  * Fill the locked page with data located in the block address.
  * Return unlocked page.
@@ -1140,12 +1172,18 @@ static int f2fs_write_data_page(struct page *page,
 	inode_dec_dirty_pages(inode);
 	if (err)
 		ClearPageUptodate(page);
+
+	if (wbc->for_reclaim) {
+		f2fs_submit_merged_bio_cond(sbi, NULL, page, 0, DATA, WRITE);
+		remove_dirty_inode(inode);
+	}
+
 	unlock_page(page);
 	f2fs_balance_fs(sbi, need_balance_fs);
-	if (wbc->for_reclaim || unlikely(f2fs_cp_error(sbi))) {
+
+	if (unlikely(f2fs_cp_error(sbi)))
 		f2fs_submit_merged_bio(sbi, DATA, WRITE);
-		remove_dirty_inode(inode);
-	}
+
 	return 0;
 
 redirty_out:
@@ -1333,7 +1371,7 @@ static int f2fs_write_data_pages(struct address_space *mapping,
 		locked = true;
 	}
 	ret = f2fs_write_cache_pages(mapping, wbc, __f2fs_writepage, mapping);
-	f2fs_submit_merged_bio(sbi, DATA, WRITE);
+	f2fs_submit_merged_bio_cond(sbi, inode, NULL, 0, DATA, WRITE);
 	if (locked)
 		mutex_unlock(&sbi->writepages);
 

commit da85985c6142decea67ee5ff67eadf3f13103a91
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Tue Jan 26 15:42:58 2016 +0800

    f2fs: speed up handling holes in fiemap
    
    This patch makes f2fs_map_blocks supporting returning next potential
    page offset which skips hole region in indirect tree of inode, and
    use it to speed up fiemap in handling big hole case.
    
    Test method:
    xfs_io -f /mnt/f2fs/file  -c "pwrite 1099511627776 4096"
    time xfs_io -f /mnt/f2fs/file -c "fiemap -v"
    
    Before:
    time xfs_io -f /mnt/f2fs/file -c "fiemap -v"
    /mnt/f2fs/file:
     EXT: FILE-OFFSET              BLOCK-RANGE      TOTAL FLAGS
       0: [0..2147483647]:         hole             2147483648
       1: [2147483648..2147483655]: 81920..81927         8   0x1
    
    real    3m3.518s
    user    0m0.000s
    sys     3m3.456s
    
    After:
    time xfs_io -f /mnt/f2fs/file -c "fiemap -v"
    /mnt/f2fs/file:
     EXT: FILE-OFFSET              BLOCK-RANGE      TOTAL FLAGS
       0: [0..2147483647]:         hole             2147483648
       1: [2147483648..2147483655]: 81920..81927         8   0x1
    
    real    0m0.008s
    user    0m0.000s
    sys     0m0.008s
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index bb60e6afbb72..e0b09c33fc0f 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -539,6 +539,7 @@ static int __allocate_data_blocks(struct inode *inode, loff_t offset,
 
 	map.m_lblk = F2FS_BYTES_TO_BLK(offset);
 	map.m_len = F2FS_BYTES_TO_BLK(count);
+	map.m_next_pgofs = NULL;
 
 	return f2fs_map_blocks(inode, &map, 1, F2FS_GET_BLOCK_DIO);
 }
@@ -586,8 +587,12 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 	set_new_dnode(&dn, inode, NULL, NULL, 0);
 	err = get_dnode_of_data(&dn, pgofs, mode);
 	if (err) {
-		if (err == -ENOENT)
+		if (err == -ENOENT) {
 			err = 0;
+			if (map->m_next_pgofs)
+				*map->m_next_pgofs =
+					get_next_page_offset(&dn, pgofs);
+		}
 		goto unlock_out;
 	}
 
@@ -609,6 +614,11 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 			map->m_flags = F2FS_MAP_NEW;
 			blkaddr = dn.data_blkaddr;
 		} else {
+			if (flag == F2FS_GET_BLOCK_FIEMAP &&
+						blkaddr == NULL_ADDR) {
+				if (map->m_next_pgofs)
+					*map->m_next_pgofs = pgofs + 1;
+			}
 			if (flag != F2FS_GET_BLOCK_FIEMAP ||
 						blkaddr != NEW_ADDR) {
 				if (flag == F2FS_GET_BLOCK_BMAP)
@@ -669,13 +679,15 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 }
 
 static int __get_data_block(struct inode *inode, sector_t iblock,
-			struct buffer_head *bh, int create, int flag)
+			struct buffer_head *bh, int create, int flag,
+			pgoff_t *next_pgofs)
 {
 	struct f2fs_map_blocks map;
 	int ret;
 
 	map.m_lblk = iblock;
 	map.m_len = bh->b_size >> inode->i_blkbits;
+	map.m_next_pgofs = next_pgofs;
 
 	ret = f2fs_map_blocks(inode, &map, create, flag);
 	if (!ret) {
@@ -687,16 +699,18 @@ static int __get_data_block(struct inode *inode, sector_t iblock,
 }
 
 static int get_data_block(struct inode *inode, sector_t iblock,
-			struct buffer_head *bh_result, int create, int flag)
+			struct buffer_head *bh_result, int create, int flag,
+			pgoff_t *next_pgofs)
 {
-	return __get_data_block(inode, iblock, bh_result, create, flag);
+	return __get_data_block(inode, iblock, bh_result, create,
+							flag, next_pgofs);
 }
 
 static int get_data_block_dio(struct inode *inode, sector_t iblock,
 			struct buffer_head *bh_result, int create)
 {
 	return __get_data_block(inode, iblock, bh_result, create,
-						F2FS_GET_BLOCK_DIO);
+						F2FS_GET_BLOCK_DIO, NULL);
 }
 
 static int get_data_block_bmap(struct inode *inode, sector_t iblock,
@@ -707,7 +721,7 @@ static int get_data_block_bmap(struct inode *inode, sector_t iblock,
 		return -EFBIG;
 
 	return __get_data_block(inode, iblock, bh_result, create,
-						F2FS_GET_BLOCK_BMAP);
+						F2FS_GET_BLOCK_BMAP, NULL);
 }
 
 static inline sector_t logical_to_blk(struct inode *inode, loff_t offset)
@@ -725,6 +739,7 @@ int f2fs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,
 {
 	struct buffer_head map_bh;
 	sector_t start_blk, last_blk;
+	pgoff_t next_pgofs;
 	loff_t isize;
 	u64 logical = 0, phys = 0, size = 0;
 	u32 flags = 0;
@@ -760,14 +775,15 @@ int f2fs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,
 	map_bh.b_size = len;
 
 	ret = get_data_block(inode, start_blk, &map_bh, 0,
-					F2FS_GET_BLOCK_FIEMAP);
+					F2FS_GET_BLOCK_FIEMAP, &next_pgofs);
 	if (ret)
 		goto out;
 
 	/* HOLE */
 	if (!buffer_mapped(&map_bh)) {
+		start_blk = next_pgofs;
 		/* Go through holes util pass the EOF */
-		if (blk_to_logical(inode, start_blk++) < isize)
+		if (blk_to_logical(inode, start_blk) < isize)
 			goto prep_next;
 		/* Found a hole beyond isize means no more extents.
 		 * Note that the premise is that filesystems don't
@@ -835,6 +851,7 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 	map.m_lblk = 0;
 	map.m_len = 0;
 	map.m_flags = 0;
+	map.m_next_pgofs = NULL;
 
 	for (page_idx = 0; nr_pages; page_idx++, nr_pages--) {
 
@@ -873,7 +890,7 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 			map.m_len = last_block - block_in_file;
 
 			if (f2fs_map_blocks(inode, &map, 0,
-							F2FS_GET_BLOCK_READ))
+						F2FS_GET_BLOCK_READ))
 				goto set_error_page;
 		}
 got_it:

commit 81ca7350ce5ed438547ea769b0c33cb0abbd74ba
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Tue Jan 26 15:39:35 2016 +0800

    f2fs: remove unneeded pointer conversion
    
    There are redundant pointer conversion in following call stack:
     - at position a, inode was been converted to f2fs_file_info.
     - at position b, f2fs_file_info was been converted to inode again.
    
     - truncate_blocks(inode,..)
      - fi = F2FS_I(inode)          ---a
      - ADDRS_PER_PAGE(node_page, fi)
       - addrs_per_inode(fi)
        - inode = &fi->vfs_inode    ---b
        - f2fs_has_inline_xattr(inode)
         - fi = F2FS_I(inode)
         - is_inode_flag_set(fi,..)
    
    In order to avoid unneeded conversion, alter ADDRS_PER_PAGE and
    addrs_per_inode to acept parameter with type of inode pointer.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index f89ef4e37510..bb60e6afbb72 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -497,7 +497,6 @@ struct page *get_new_data_page(struct inode *inode,
 static int __allocate_data_block(struct dnode_of_data *dn)
 {
 	struct f2fs_sb_info *sbi = F2FS_I_SB(dn->inode);
-	struct f2fs_inode_info *fi = F2FS_I(dn->inode);
 	struct f2fs_summary sum;
 	struct node_info ni;
 	int seg = CURSEG_WARM_DATA;
@@ -525,7 +524,7 @@ static int __allocate_data_block(struct dnode_of_data *dn)
 	set_data_blkaddr(dn);
 
 	/* update i_size */
-	fofs = start_bidx_of_node(ofs_of_node(dn->node_page), fi) +
+	fofs = start_bidx_of_node(ofs_of_node(dn->node_page), dn->inode) +
 							dn->ofs_in_node;
 	if (i_size_read(dn->inode) < ((loff_t)(fofs + 1) << PAGE_CACHE_SHIFT))
 		i_size_write(dn->inode,
@@ -592,7 +591,7 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 		goto unlock_out;
 	}
 
-	end_offset = ADDRS_PER_PAGE(dn.node_page, F2FS_I(inode));
+	end_offset = ADDRS_PER_PAGE(dn.node_page, inode);
 
 next_block:
 	blkaddr = datablock_addr(dn.node_page, dn.ofs_in_node);

commit 5b8db7fada07e67bb2075a39320f0abe7fe0ed77
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Tue Jan 26 15:38:29 2016 +0800

    f2fs: simplify __allocate_data_blocks
    
    This patch uses existing function f2fs_map_block to simplify implementation
    of __allocate_data_blocks.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index bd49a02dfa11..f89ef4e37510 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -536,64 +536,12 @@ static int __allocate_data_block(struct dnode_of_data *dn)
 static int __allocate_data_blocks(struct inode *inode, loff_t offset,
 							size_t count)
 {
-	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	struct dnode_of_data dn;
-	u64 start = F2FS_BYTES_TO_BLK(offset);
-	u64 len = F2FS_BYTES_TO_BLK(count);
-	bool allocated = false;
-	u64 end_offset;
-	int err = 0;
-
-	while (len) {
-		f2fs_lock_op(sbi);
-
-		/* When reading holes, we need its node page */
-		set_new_dnode(&dn, inode, NULL, NULL, 0);
-		err = get_dnode_of_data(&dn, start, ALLOC_NODE);
-		if (err)
-			goto out;
-
-		allocated = false;
-		end_offset = ADDRS_PER_PAGE(dn.node_page, F2FS_I(inode));
-
-		while (dn.ofs_in_node < end_offset && len) {
-			block_t blkaddr;
-
-			if (unlikely(f2fs_cp_error(sbi))) {
-				err = -EIO;
-				goto sync_out;
-			}
-
-			blkaddr = datablock_addr(dn.node_page, dn.ofs_in_node);
-			if (blkaddr == NULL_ADDR || blkaddr == NEW_ADDR) {
-				err = __allocate_data_block(&dn);
-				if (err)
-					goto sync_out;
-				allocated = true;
-			}
-			len--;
-			start++;
-			dn.ofs_in_node++;
-		}
-
-		if (allocated)
-			sync_inode_page(&dn);
-
-		f2fs_put_dnode(&dn);
-		f2fs_unlock_op(sbi);
+	struct f2fs_map_blocks map;
 
-		f2fs_balance_fs(sbi, allocated);
-	}
-	return err;
+	map.m_lblk = F2FS_BYTES_TO_BLK(offset);
+	map.m_len = F2FS_BYTES_TO_BLK(count);
 
-sync_out:
-	if (allocated)
-		sync_inode_page(&dn);
-	f2fs_put_dnode(&dn);
-out:
-	f2fs_unlock_op(sbi);
-	f2fs_balance_fs(sbi, allocated);
-	return err;
+	return f2fs_map_blocks(inode, &map, 1, F2FS_GET_BLOCK_DIO);
 }
 
 /*

commit 4fe71e88bf681359fdbc99dc74920b38f018f89d
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Tue Jan 26 15:37:38 2016 +0800

    f2fs: simplify f2fs_map_blocks
    
    In f2fs_map_blocks, we use duplicated codes to handle first block mapping
    and the following blocks mapping, it's unnecessary. This patch simplifies
    f2fs_map_blocks to avoid using copied codes.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 90b3a3726d80..bd49a02dfa11 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -631,6 +631,7 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 		goto out;
 	}
 
+next_dnode:
 	if (create)
 		f2fs_lock_op(sbi);
 
@@ -643,47 +644,57 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 		goto unlock_out;
 	}
 
-	if (dn.data_blkaddr == NEW_ADDR || dn.data_blkaddr == NULL_ADDR) {
+	end_offset = ADDRS_PER_PAGE(dn.node_page, F2FS_I(inode));
+
+next_block:
+	blkaddr = datablock_addr(dn.node_page, dn.ofs_in_node);
+
+	if (blkaddr == NEW_ADDR || blkaddr == NULL_ADDR) {
 		if (create) {
 			if (unlikely(f2fs_cp_error(sbi))) {
 				err = -EIO;
-				goto put_out;
+				goto sync_out;
 			}
 			err = __allocate_data_block(&dn);
 			if (err)
-				goto put_out;
+				goto sync_out;
 			allocated = true;
 			map->m_flags = F2FS_MAP_NEW;
+			blkaddr = dn.data_blkaddr;
 		} else {
 			if (flag != F2FS_GET_BLOCK_FIEMAP ||
-						dn.data_blkaddr != NEW_ADDR) {
+						blkaddr != NEW_ADDR) {
 				if (flag == F2FS_GET_BLOCK_BMAP)
 					err = -ENOENT;
-				goto put_out;
+				goto sync_out;
 			}
-
-			/*
-			 * preallocated unwritten block should be mapped
-			 * for fiemap.
-			 */
-			if (dn.data_blkaddr == NEW_ADDR)
-				map->m_flags = F2FS_MAP_UNWRITTEN;
 		}
 	}
 
-	map->m_flags |= F2FS_MAP_MAPPED;
-	map->m_pblk = dn.data_blkaddr;
-	map->m_len = 1;
+	if (map->m_len == 0) {
+		/* preallocated unwritten block should be mapped for fiemap. */
+		if (blkaddr == NEW_ADDR)
+			map->m_flags |= F2FS_MAP_UNWRITTEN;
+		map->m_flags |= F2FS_MAP_MAPPED;
+
+		map->m_pblk = blkaddr;
+		map->m_len = 1;
+	} else if ((map->m_pblk != NEW_ADDR &&
+			blkaddr == (map->m_pblk + ofs)) ||
+			(map->m_pblk == NEW_ADDR && blkaddr == NEW_ADDR)) {
+		ofs++;
+		map->m_len++;
+	} else {
+		goto sync_out;
+	}
 
-	end_offset = ADDRS_PER_PAGE(dn.node_page, F2FS_I(inode));
 	dn.ofs_in_node++;
 	pgofs++;
 
-get_next:
-	if (map->m_len >= maxblocks)
-		goto sync_out;
+	if (map->m_len < maxblocks) {
+		if (dn.ofs_in_node < end_offset)
+			goto next_block;
 
-	if (dn.ofs_in_node >= end_offset) {
 		if (allocated)
 			sync_inode_page(&dn);
 		f2fs_put_dnode(&dn);
@@ -691,62 +702,14 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 		if (create) {
 			f2fs_unlock_op(sbi);
 			f2fs_balance_fs(sbi, allocated);
-			f2fs_lock_op(sbi);
 		}
 		allocated = false;
-
-		set_new_dnode(&dn, inode, NULL, NULL, 0);
-		err = get_dnode_of_data(&dn, pgofs, mode);
-		if (err) {
-			if (err == -ENOENT)
-				err = 0;
-			goto unlock_out;
-		}
-
-		end_offset = ADDRS_PER_PAGE(dn.node_page, F2FS_I(inode));
-	}
-
-	blkaddr = datablock_addr(dn.node_page, dn.ofs_in_node);
-
-	if (blkaddr == NEW_ADDR || blkaddr == NULL_ADDR) {
-		if (create) {
-			if (unlikely(f2fs_cp_error(sbi))) {
-				err = -EIO;
-				goto sync_out;
-			}
-			err = __allocate_data_block(&dn);
-			if (err)
-				goto sync_out;
-			allocated = true;
-			map->m_flags |= F2FS_MAP_NEW;
-			blkaddr = dn.data_blkaddr;
-		} else {
-			/*
-			 * we only merge preallocated unwritten blocks
-			 * for fiemap.
-			 */
-			if (flag != F2FS_GET_BLOCK_FIEMAP ||
-					blkaddr != NEW_ADDR)
-				goto sync_out;
-		}
-	}
-
-	/* Give more consecutive addresses for the readahead */
-	if ((map->m_pblk != NEW_ADDR &&
-			blkaddr == (map->m_pblk + ofs)) ||
-			(map->m_pblk == NEW_ADDR &&
-			blkaddr == NEW_ADDR)) {
-		ofs++;
-		dn.ofs_in_node++;
-		pgofs++;
-		map->m_len++;
-		goto get_next;
+		goto next_dnode;
 	}
 
 sync_out:
 	if (allocated)
 		sync_inode_page(&dn);
-put_out:
 	f2fs_put_dnode(&dn);
 unlock_out:
 	if (create) {

commit 7c506896cf933a07460dd9777ac023382446beac
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Tue Jan 26 11:55:35 2016 -0800

    f2fs: use wq_has_sleeper for cp_wait wait_queue
    
    We need to use wq_has_sleeper including smp_mb to consider cp_wait concurrency.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 16cf8f0bc55f..90b3a3726d80 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -74,8 +74,7 @@ static void f2fs_write_end_io(struct bio *bio)
 		dec_page_count(sbi, F2FS_WRITEBACK);
 	}
 
-	if (!get_pages(sbi, F2FS_WRITEBACK) &&
-			!list_empty(&sbi->cp_wait.task_list))
+	if (!get_pages(sbi, F2FS_WRITEBACK) && wq_has_sleeper(&sbi->cp_wait))
 		wake_up(&sbi->cp_wait);
 
 	bio_put(bio);

commit fec1d6576cdf2ce13f84fcdf7b20d02a05f76fc6
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed Jan 20 23:43:51 2016 +0800

    f2fs: use wait_for_stable_page to avoid contention
    
    In write_begin, if storage supports stable_page, we don't need to wait for
    writeback to update its contents.
    This patch introduces to use wait_for_stable_page instead of
    wait_on_page_writeback.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index c1fa63f37053..16cf8f0bc55f 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -255,7 +255,7 @@ void set_data_blkaddr(struct dnode_of_data *dn)
 	struct page *node_page = dn->node_page;
 	unsigned int ofs_in_node = dn->ofs_in_node;
 
-	f2fs_wait_on_page_writeback(node_page, NODE);
+	f2fs_wait_on_page_writeback(node_page, NODE, true);
 
 	rn = F2FS_NODE(node_page);
 
@@ -1319,7 +1319,8 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 
 			if (PageWriteback(page)) {
 				if (wbc->sync_mode != WB_SYNC_NONE)
-					f2fs_wait_on_page_writeback(page, DATA);
+					f2fs_wait_on_page_writeback(page,
+								DATA, true);
 				else
 					goto continue_unlock;
 			}
@@ -1547,7 +1548,7 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 		}
 	}
 
-	f2fs_wait_on_page_writeback(page, DATA);
+	f2fs_wait_on_page_writeback(page, DATA, false);
 
 	/* wait for GCed encrypted page writeback */
 	if (f2fs_encrypted_inode(inode) && S_ISREG(inode->i_mode))

commit e3ef18762f5757d3fb86f75ca59315db6d17d719
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Mon Jan 25 14:31:58 2016 -0800

    f2fs: don't need to call set_page_dirty for io error
    
    If end_io gets an error, we don't need to set the page as dirty, since we
    already set f2fs_stop_checkpoint which will not flush any data.
    
    This will resolve the following warning.
    
    ======================================================
    [ INFO: HARDIRQ-safe -> HARDIRQ-unsafe lock order detected ]
    4.4.0+ #9 Tainted: G           O
    ------------------------------------------------------
    xfs_io/26773 [HC0[0]:SC0[0]:HE0:SE1] is trying to acquire:
     (&(&sbi->inode_lock[i])->rlock){+.+...}, at: [<ffffffffc025483f>] update_dirty_page+0x6f/0xd0 [f2fs]
    
    and this task is already holding:
     (&(&q->__queue_lock)->rlock){-.-.-.}, at: [<ffffffff81396ea2>] blk_queue_bio+0x422/0x490
    which would create a new lock dependency:
     (&(&q->__queue_lock)->rlock){-.-.-.} -> (&(&sbi->inode_lock[i])->rlock){+.+...}
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index f0ffb15af009..c1fa63f37053 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -67,7 +67,6 @@ static void f2fs_write_end_io(struct bio *bio)
 		f2fs_restore_and_release_control_page(&page);
 
 		if (unlikely(bio->bi_error)) {
-			set_page_dirty(page);
 			set_bit(AS_EIO, &page->mapping->flags);
 			f2fs_stop_checkpoint(sbi);
 		}

commit ae96e7bdd42739ef677a8a5d2d39d3eac5a4ff59
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Mon Jan 25 10:48:50 2016 -0800

    f2fs: avoid needless sync_inode_page when reading inline_data
    
    In write_begin, if there is an inline_data, f2fs loads it into 0'th data page.
    Since it's the read path, we don't need to sync its inode page.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 24c65e9823b7..f0ffb15af009 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1464,7 +1464,6 @@ static int prepare_write_begin(struct f2fs_sb_info *sbi,
 			read_inline_data(page, ipage);
 			set_inode_flag(F2FS_I(inode), FI_DATA_EXIST);
 			set_inline_node(ipage);
-			sync_inode_page(&dn);
 		} else {
 			err = f2fs_convert_inline_page(&dn, page);
 			if (err)

commit 52f803371200bc86d4d028693d06a627fe11f740
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Mon Jan 25 10:46:58 2016 -0800

    f2fs: don't need to sync node page at every time
    
    In write_end, we don't need to sync inode page at every time.
    Instead, we can expect f2fs_write_inode will update later.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index a11a8c789cd5..24c65e9823b7 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1627,7 +1627,6 @@ static int f2fs_write_end(struct file *file,
 	if (pos + copied > i_size_read(inode)) {
 		i_size_write(inode, pos + copied);
 		mark_inode_dirty(inode);
-		update_inode_page(inode);
 	}
 
 	f2fs_put_page(page, 1);

commit 2049d4fcb057c263929bec480f2db079d25fd601
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Mon Jan 25 05:57:05 2016 -0800

    f2fs: avoid multiple node page writes due to inline_data
    
    The sceanrio is:
    1. create fully node blocks
    2. flush node blocks
    3. write inline_data for all the node blocks again
    4. flush node blocks redundantly
    
    So, this patch tries to flush inline_data when flushing node blocks.
    
    Reviewed-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 1b08480c5e38..a11a8c789cd5 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1463,6 +1463,7 @@ static int prepare_write_begin(struct f2fs_sb_info *sbi,
 		if (pos + len <= MAX_INLINE_DATA) {
 			read_inline_data(page, ipage);
 			set_inode_flag(F2FS_I(inode), FI_DATA_EXIST);
+			set_inline_node(ipage);
 			sync_inode_page(&dn);
 		} else {
 			err = f2fs_convert_inline_page(&dn, page);

commit 3c082b7b5b28be606ed9ef11e4741df7c722c92e
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Sat Jan 23 13:35:18 2016 -0800

    f2fs: do f2fs_balance_fs when block is allocated
    
    We should consider data block allocation to trigger f2fs_balance_fs.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 0f0187804eda..1b08480c5e38 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -542,7 +542,7 @@ static int __allocate_data_blocks(struct inode *inode, loff_t offset,
 	struct dnode_of_data dn;
 	u64 start = F2FS_BYTES_TO_BLK(offset);
 	u64 len = F2FS_BYTES_TO_BLK(count);
-	bool allocated;
+	bool allocated = false;
 	u64 end_offset;
 	int err = 0;
 
@@ -584,7 +584,7 @@ static int __allocate_data_blocks(struct inode *inode, loff_t offset,
 		f2fs_put_dnode(&dn);
 		f2fs_unlock_op(sbi);
 
-		f2fs_balance_fs(sbi, dn.node_changed);
+		f2fs_balance_fs(sbi, allocated);
 	}
 	return err;
 
@@ -594,7 +594,7 @@ static int __allocate_data_blocks(struct inode *inode, loff_t offset,
 	f2fs_put_dnode(&dn);
 out:
 	f2fs_unlock_op(sbi);
-	f2fs_balance_fs(sbi, dn.node_changed);
+	f2fs_balance_fs(sbi, allocated);
 	return err;
 }
 
@@ -688,14 +688,14 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 	if (dn.ofs_in_node >= end_offset) {
 		if (allocated)
 			sync_inode_page(&dn);
-		allocated = false;
 		f2fs_put_dnode(&dn);
 
 		if (create) {
 			f2fs_unlock_op(sbi);
-			f2fs_balance_fs(sbi, dn.node_changed);
+			f2fs_balance_fs(sbi, allocated);
 			f2fs_lock_op(sbi);
 		}
+		allocated = false;
 
 		set_new_dnode(&dn, inode, NULL, NULL, 0);
 		err = get_dnode_of_data(&dn, pgofs, mode);
@@ -753,7 +753,7 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 unlock_out:
 	if (create) {
 		f2fs_unlock_op(sbi);
-		f2fs_balance_fs(sbi, dn.node_changed);
+		f2fs_balance_fs(sbi, allocated);
 	}
 out:
 	trace_f2fs_map_blocks(inode, map, err);

commit 25c13551519c66453737e038d104058f94b2e70a
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed Jan 20 23:46:05 2016 +0800

    f2fs: use writepages->lock for WB_SYNC_ALL
    
    If there are many writepages calls by multiple threads in background, we don't
    need to serialize to merge all the bios, since it's background.
    In such the case, it'd better to run writepages concurrently.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index e8d4158a3206..0f0187804eda 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1402,7 +1402,7 @@ static int f2fs_write_data_pages(struct address_space *mapping,
 
 	diff = nr_pages_to_write(sbi, DATA, wbc);
 
-	if (!S_ISDIR(inode->i_mode)) {
+	if (!S_ISDIR(inode->i_mode) && wbc->sync_mode == WB_SYNC_ALL) {
 		mutex_lock(&sbi->writepages);
 		locked = true;
 	}

commit b483fadf7e913be838e11e065175205aa17e429a
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed Jan 20 08:27:37 2016 +0800

    f2fs: remove needless condition check
    
    This patch removes needless condition variable.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 7fff6ac3d0cb..e8d4158a3206 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1477,13 +1477,9 @@ static int prepare_write_begin(struct f2fs_sb_info *sbi,
 		if (f2fs_lookup_extent_cache(inode, index, &ei)) {
 			dn.data_blkaddr = ei.blk + index - ei.fofs;
 		} else {
-			bool restart = false;
-
 			/* hole case */
 			err = get_dnode_of_data(&dn, index, LOOKUP_NODE);
-			if (err || (!err && dn.data_blkaddr == NULL_ADDR))
-				restart = true;
-			if (restart) {
+			if (err || (!err && dn.data_blkaddr == NULL_ADDR)) {
 				f2fs_put_dnode(&dn);
 				f2fs_lock_op(sbi);
 				locked = true;

commit 0fd785eb931d254a4ea4abd42f1c0c5a17f7132a
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Mon Jan 18 18:24:59 2016 +0800

    f2fs: relocate is_merged_page
    
    Operations in is_merged_page is related to inner bio cache, move it to
    data.c.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 5c06db17e41f..7fff6ac3d0cb 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -116,6 +116,44 @@ static void __submit_merged_bio(struct f2fs_bio_info *io)
 	io->bio = NULL;
 }
 
+bool is_merged_page(struct f2fs_sb_info *sbi, struct page *page,
+							enum page_type type)
+{
+	enum page_type btype = PAGE_TYPE_OF_BIO(type);
+	struct f2fs_bio_info *io = &sbi->write_io[btype];
+	struct bio_vec *bvec;
+	struct page *target;
+	int i;
+
+	down_read(&io->io_rwsem);
+	if (!io->bio) {
+		up_read(&io->io_rwsem);
+		return false;
+	}
+
+	bio_for_each_segment_all(bvec, io->bio, i) {
+
+		if (bvec->bv_page->mapping) {
+			target = bvec->bv_page;
+		} else {
+			struct f2fs_crypto_ctx *ctx;
+
+			/* encrypted page */
+			ctx = (struct f2fs_crypto_ctx *)page_private(
+								bvec->bv_page);
+			target = ctx->w.control_page;
+		}
+
+		if (page == target) {
+			up_read(&io->io_rwsem);
+			return true;
+		}
+	}
+
+	up_read(&io->io_rwsem);
+	return false;
+}
+
 void f2fs_submit_merged_bio(struct f2fs_sb_info *sbi,
 				enum page_type type, int rw)
 {

commit 5955102c9984fa081b2d570cfac75c97eecf8f3b
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Fri Jan 22 15:40:57 2016 -0500

    wrappers for ->i_mutex access
    
    parallel to mutex_{lock,unlock,trylock,is_locked,lock_nested},
    inode_foo(inode) being mutex_foo(&inode->i_mutex).
    
    Please, use those for access to ->i_mutex; over the coming cycle
    ->i_mutex will become rwsem, with ->lookup() done with it held
    only shared.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index ac9e7c6aac74..5c06db17e41f 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -794,7 +794,7 @@ int f2fs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,
 			return ret;
 	}
 
-	mutex_lock(&inode->i_mutex);
+	inode_lock(inode);
 
 	isize = i_size_read(inode);
 	if (start >= isize)
@@ -860,7 +860,7 @@ int f2fs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,
 	if (ret == 1)
 		ret = 0;
 
-	mutex_unlock(&inode->i_mutex);
+	inode_unlock(inode);
 	return ret;
 }
 

commit d0239e1bf5204d602281f93c01d46bcf3531098d
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Jan 8 16:57:48 2016 -0800

    f2fs: detect idle time depending on user behavior
    
    This patch adds last time that user requested filesystem operations.
    This information is used to detect whether system is idle or not later.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index a3bce12b0cce..ac9e7c6aac74 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1596,6 +1596,7 @@ static int f2fs_write_end(struct file *file,
 	}
 
 	f2fs_put_page(page, 1);
+	f2fs_update_time(F2FS_I_SB(inode), REQ_TIME);
 	return copied;
 }
 

commit da5af127a1a17bac121c6889c88cc90f8a278a84
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Fri Jan 8 20:19:27 2016 +0800

    f2fs: recognize encrypted data in f2fs_fiemap
    
    This patch fixes to teach f2fs_fiemap to recognize encrypted data.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 6fae75ddae6d..a3bce12b0cce 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -830,9 +830,13 @@ int f2fs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,
 		flags |= FIEMAP_EXTENT_LAST;
 	}
 
-	if (size)
+	if (size) {
+		if (f2fs_encrypted_inode(inode))
+			flags |= FIEMAP_EXTENT_DATA_ENCRYPTED;
+
 		ret = fiemap_fill_next_extent(fieinfo, logical,
 				phys, size, flags);
+	}
 
 	if (start_blk > last_blk || ret)
 		goto out;

commit 2c4db1a6f6b42e2a9fb611cbbeb71a3a9a358ee0
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Jan 7 14:15:04 2016 -0800

    f2fs: clean up f2fs_balance_fs
    
    This patch adds one parameter to clean up all the callers of f2fs_balance_fs.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 3cf86fda8138..6fae75ddae6d 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -546,8 +546,7 @@ static int __allocate_data_blocks(struct inode *inode, loff_t offset,
 		f2fs_put_dnode(&dn);
 		f2fs_unlock_op(sbi);
 
-		if (dn.node_changed)
-			f2fs_balance_fs(sbi);
+		f2fs_balance_fs(sbi, dn.node_changed);
 	}
 	return err;
 
@@ -557,8 +556,7 @@ static int __allocate_data_blocks(struct inode *inode, loff_t offset,
 	f2fs_put_dnode(&dn);
 out:
 	f2fs_unlock_op(sbi);
-	if (dn.node_changed)
-		f2fs_balance_fs(sbi);
+	f2fs_balance_fs(sbi, dn.node_changed);
 	return err;
 }
 
@@ -657,8 +655,7 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 
 		if (create) {
 			f2fs_unlock_op(sbi);
-			if (dn.node_changed)
-				f2fs_balance_fs(sbi);
+			f2fs_balance_fs(sbi, dn.node_changed);
 			f2fs_lock_op(sbi);
 		}
 
@@ -718,8 +715,7 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 unlock_out:
 	if (create) {
 		f2fs_unlock_op(sbi);
-		if (dn.node_changed)
-			f2fs_balance_fs(sbi);
+		f2fs_balance_fs(sbi, dn.node_changed);
 	}
 out:
 	trace_f2fs_map_blocks(inode, map, err);
@@ -1178,8 +1174,7 @@ static int f2fs_write_data_page(struct page *page,
 	if (err)
 		ClearPageUptodate(page);
 	unlock_page(page);
-	if (need_balance_fs)
-		f2fs_balance_fs(sbi);
+	f2fs_balance_fs(sbi, need_balance_fs);
 	if (wbc->for_reclaim || unlikely(f2fs_cp_error(sbi))) {
 		f2fs_submit_merged_bio(sbi, DATA, WRITE);
 		remove_dirty_inode(inode);
@@ -1506,7 +1501,7 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 
 	if (need_balance && has_not_enough_free_secs(sbi, 0)) {
 		unlock_page(page);
-		f2fs_balance_fs(sbi);
+		f2fs_balance_fs(sbi, true);
 		lock_page(page);
 		if (page->mapping != mapping) {
 			/* The page got truncated from under us */

commit 12719ae14e57980ebf0a7baa63bc80494c76b192
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Jan 7 13:23:12 2016 -0800

    f2fs: avoid unnecessary f2fs_balance_fs calls
    
    Only when node page is newly dirtied, it needs to check whether we need to do
    f2fs_gc.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 77c3bbb9bee0..3cf86fda8138 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -225,8 +225,8 @@ void set_data_blkaddr(struct dnode_of_data *dn)
 	/* Get physical address of data block */
 	addr_array = blkaddr_in_node(rn);
 	addr_array[ofs_in_node] = cpu_to_le32(dn->data_blkaddr);
-	set_page_dirty(node_page);
-	dn->node_changed = true;
+	if (set_page_dirty(node_page))
+		dn->node_changed = true;
 }
 
 int reserve_new_block(struct dnode_of_data *dn)

commit 7612118ae8cdd36cbd74d873855d70252d2d49e3
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Jan 1 22:03:47 2016 -0800

    f2fs: check the page status filled from disk
    
    After reading a page, we need to check whether there is any error.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index ac5bea0f5f09..77c3bbb9bee0 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -413,7 +413,7 @@ struct page *get_new_data_page(struct inode *inode,
 	struct page *page;
 	struct dnode_of_data dn;
 	int err;
-repeat:
+
 	page = f2fs_grab_cache_page(mapping, index, true);
 	if (!page) {
 		/*
@@ -442,12 +442,11 @@ struct page *get_new_data_page(struct inode *inode,
 	} else {
 		f2fs_put_page(page, 1);
 
-		page = get_read_data_page(inode, index, READ_SYNC, true);
+		/* if ipage exists, blkaddr should be NEW_ADDR */
+		f2fs_bug_on(F2FS_I_SB(inode), ipage);
+		page = get_lock_data_page(inode, index, true);
 		if (IS_ERR(page))
-			goto repeat;
-
-		/* wait for read completion */
-		lock_page(page);
+			return page;
 	}
 got_it:
 	if (new_i_size && i_size_read(inode) <

commit de1475cc53b2d6442443dcf5d66ed0fc50ed3c7e
Author: Fan Li <fanofcode.li@samsung.com>
Date:   Mon Jan 4 15:56:50 2016 +0800

    f2fs: read isize while holding i_mutex in fiemap
    
    make sure the isize we read doesn't change during the process.
    
    Signed-off-by: Fan li <fanofcode.li@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 89a978c57da9..ac5bea0f5f09 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -784,7 +784,7 @@ int f2fs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,
 {
 	struct buffer_head map_bh;
 	sector_t start_blk, last_blk;
-	loff_t isize = i_size_read(inode);
+	loff_t isize;
 	u64 logical = 0, phys = 0, size = 0;
 	u32 flags = 0;
 	int ret = 0;
@@ -800,6 +800,8 @@ int f2fs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,
 	}
 
 	mutex_lock(&inode->i_mutex);
+
+	isize = i_size_read(inode);
 	if (start >= isize)
 		goto out;
 

commit e0afc4d6d0d3e7e5a99f691bc64ae7c74bea790e
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Thu Dec 31 14:35:37 2015 +0800

    f2fs: introduce max_file_blocks in sbi
    
    Introduce max_file_blocks in sbi to store max block index of file in f2fs,
    it could be used to avoid unneeded calculation of max block index in
    runtime.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    [Jaegeuk Kim: fix overflow of sbi->max_file_blocks]
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 4851e84d0283..89a978c57da9 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -762,7 +762,7 @@ static int get_data_block_bmap(struct inode *inode, sector_t iblock,
 			struct buffer_head *bh_result, int create)
 {
 	/* Block number less than F2FS MAX BLOCKS */
-	if (unlikely(iblock >= max_file_size(0)))
+	if (unlikely(iblock >= F2FS_I_SB(inode)->max_file_blocks))
 		return -EFBIG;
 
 	return __get_data_block(inode, iblock, bh_result, create,

commit 8d4ea29b6426470456ee9daee64bac55a3b13289
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Dec 31 13:08:02 2015 -0800

    f2fs: write pending bios when cp_error is set
    
    When testing ioc_shutdown, put_super is able to be hanged by waiting for
    writebacking pages as follows.
    
    INFO: task umount:2723 blocked for more than 120 seconds.
          Tainted: G           O    4.4.0-rc3+ #8
    "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
    umount          D ffff88000859f9d8     0  2723   2110 0x00000000
     ffff88000859f9d8 0000000000000000 0000000000000000 ffffffff81e11540
     ffff880078c225c0 ffff8800085a0000 ffff88007fc17440 7fffffffffffffff
     ffffffff818239f0 ffff88000859fb48 ffff88000859f9f0 ffffffff8182310c
    Call Trace:
     [<ffffffff818239f0>] ? bit_wait+0x50/0x50
     [<ffffffff8182310c>] schedule+0x3c/0x90
     [<ffffffff81827fb9>] schedule_timeout+0x2d9/0x430
     [<ffffffff810e0f8f>] ? mark_held_locks+0x6f/0xa0
     [<ffffffff8111614d>] ? ktime_get+0x7d/0x140
     [<ffffffff818239f0>] ? bit_wait+0x50/0x50
     [<ffffffff8106a655>] ? kvm_clock_get_cycles+0x25/0x30
     [<ffffffff8111617c>] ? ktime_get+0xac/0x140
     [<ffffffff818239f0>] ? bit_wait+0x50/0x50
     [<ffffffff81822564>] io_schedule_timeout+0xa4/0x110
     [<ffffffff81823a25>] bit_wait_io+0x35/0x50
     [<ffffffff818235bd>] __wait_on_bit+0x5d/0x90
     [<ffffffff811b9e8b>] wait_on_page_bit+0xcb/0xf0
     [<ffffffff810d5f90>] ? autoremove_wake_function+0x40/0x40
     [<ffffffff811cf84c>] truncate_inode_pages_range+0x4bc/0x840
     [<ffffffff811cfc3d>] truncate_inode_pages_final+0x4d/0x60
     [<ffffffffc023ced5>] f2fs_evict_inode+0x75/0x400 [f2fs]
     [<ffffffff812639bc>] evict+0xbc/0x190
     [<ffffffff81263d19>] iput+0x229/0x2c0
     [<ffffffffc0241885>] f2fs_put_super+0x105/0x1a0 [f2fs]
     [<ffffffff8124756a>] generic_shutdown_super+0x6a/0xf0
     [<ffffffff812478f7>] kill_block_super+0x27/0x70
     [<ffffffffc0241290>] kill_f2fs_super+0x20/0x30 [f2fs]
     [<ffffffff81247b03>] deactivate_locked_super+0x43/0x70
     [<ffffffff81247f4c>] deactivate_super+0x5c/0x60
     [<ffffffff81268d2f>] cleanup_mnt+0x3f/0x90
     [<ffffffff81268dc2>] __cleanup_mnt+0x12/0x20
     [<ffffffff810ac463>] task_work_run+0x73/0xa0
     [<ffffffff810032ac>] exit_to_usermode_loop+0xcc/0xd0
     [<ffffffff81003e7c>] syscall_return_slowpath+0xcc/0xe0
     [<ffffffff81829ea2>] int_ret_from_sys_call+0x25/0x9f
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 14b40a9db5b3..4851e84d0283 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1179,7 +1179,7 @@ static int f2fs_write_data_page(struct page *page,
 	unlock_page(page);
 	if (need_balance_fs)
 		f2fs_balance_fs(sbi);
-	if (wbc->for_reclaim) {
+	if (wbc->for_reclaim || unlikely(f2fs_cp_error(sbi))) {
 		f2fs_submit_merged_bio(sbi, DATA, WRITE);
 		remove_dirty_inode(inode);
 	}

commit 819d9153d4c87329910a4cb01198610cd24ec62d
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Mon Dec 28 13:48:11 2015 -0800

    f2fs: use i_size_read to get i_size
    
    We need to use i_size_read() to get inode->i_size.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 6fbfc70ac8a0..14b40a9db5b3 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1386,10 +1386,11 @@ static int f2fs_write_data_pages(struct address_space *mapping,
 static void f2fs_write_failed(struct address_space *mapping, loff_t to)
 {
 	struct inode *inode = mapping->host;
+	loff_t i_size = i_size_read(inode);
 
-	if (to > inode->i_size) {
-		truncate_pagecache(inode, inode->i_size);
-		truncate_blocks(inode, inode->i_size, true);
+	if (to > i_size) {
+		truncate_pagecache(inode, i_size);
+		truncate_blocks(inode, i_size, true);
 	}
 }
 

commit 179448bfe4cd201e98e728391c6b01b25c849fe8
Author: Yunlei He <heyunlei@huawei.com>
Date:   Mon Dec 28 21:48:32 2015 +0800

    f2fs: add a max block check for get_data_block_bmap
    
    This patch adds a max block check for get_data_block_bmap.
    
    Trinity test program will send a block number as parameter into
    ioctl_fibmap, which will be used in get_node_path(), when the block
    number large than f2fs max blocks, it will trigger kernel bug.
    
    Signed-off-by: Yunlei He <heyunlei@huawei.com>
    Signed-off-by: Xue Liu <liuxueliu.liu@huawei.com>
    [Jaegeuk Kim: fix missing condition, pointed by Chao Yu]
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index d67c599510d9..6fbfc70ac8a0 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -761,6 +761,10 @@ static int get_data_block_dio(struct inode *inode, sector_t iblock,
 static int get_data_block_bmap(struct inode *inode, sector_t iblock,
 			struct buffer_head *bh_result, int create)
 {
+	/* Block number less than F2FS MAX BLOCKS */
+	if (unlikely(iblock >= max_file_size(0)))
+		return -EFBIG;
+
 	return __get_data_block(inode, iblock, bh_result, create,
 						F2FS_GET_BLOCK_BMAP);
 }

commit 9a950d52b7f0e1c64c2cc70d350562fb18c8b451
Author: Fan Li <fanofcode.li@samsung.com>
Date:   Sat Dec 26 18:07:41 2015 +0800

    f2fs: fix bugs and simplify codes of f2fs_fiemap
    
    fix bugs:
    1. len could be updated incorrectly when start+len is beyond isize.
    2. If there is a hole consisting of more than two blocks, it could
       fail to add FIEMAP_EXTENT_LAST flag for the last extent.
    3. If there is an extent beyond isize, when we search extents in a range
       that ends at isize, it will also return the extent beyond isize,
       which is outside the range.
    
    Signed-off-by: Fan li <fanofcode.li@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 5c43b2d606ec..d67c599510d9 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -783,7 +783,6 @@ int f2fs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,
 	loff_t isize = i_size_read(inode);
 	u64 logical = 0, phys = 0, size = 0;
 	u32 flags = 0;
-	bool past_eof = false, whole_file = false;
 	int ret = 0;
 
 	ret = fiemap_check_flags(fieinfo, FIEMAP_FLAG_SYNC);
@@ -797,17 +796,18 @@ int f2fs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,
 	}
 
 	mutex_lock(&inode->i_mutex);
+	if (start >= isize)
+		goto out;
 
-	if (len >= isize) {
-		whole_file = true;
-		len = isize;
-	}
+	if (start + len > isize)
+		len = isize - start;
 
 	if (logical_to_blk(inode, len) == 0)
 		len = blk_to_logical(inode, 1);
 
 	start_blk = logical_to_blk(inode, start);
 	last_blk = logical_to_blk(inode, start + len - 1);
+
 next:
 	memset(&map_bh, 0, sizeof(struct buffer_head));
 	map_bh.b_size = len;
@@ -819,59 +819,33 @@ int f2fs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,
 
 	/* HOLE */
 	if (!buffer_mapped(&map_bh)) {
-		start_blk++;
-
-		if (!past_eof && blk_to_logical(inode, start_blk) >= isize)
-			past_eof = 1;
-
-		if (past_eof && size) {
-			flags |= FIEMAP_EXTENT_LAST;
-			ret = fiemap_fill_next_extent(fieinfo, logical,
-					phys, size, flags);
-		} else if (size) {
-			ret = fiemap_fill_next_extent(fieinfo, logical,
-					phys, size, flags);
-			size = 0;
-		}
+		/* Go through holes util pass the EOF */
+		if (blk_to_logical(inode, start_blk++) < isize)
+			goto prep_next;
+		/* Found a hole beyond isize means no more extents.
+		 * Note that the premise is that filesystems don't
+		 * punch holes beyond isize and keep size unchanged.
+		 */
+		flags |= FIEMAP_EXTENT_LAST;
+	}
 
-		/* if we have holes up to/past EOF then we're done */
-		if (start_blk > last_blk || past_eof || ret)
-			goto out;
-	} else {
-		if (start_blk > last_blk && !whole_file) {
-			ret = fiemap_fill_next_extent(fieinfo, logical,
-					phys, size, flags);
-			goto out;
-		}
+	if (size)
+		ret = fiemap_fill_next_extent(fieinfo, logical,
+				phys, size, flags);
 
-		/*
-		 * if size != 0 then we know we already have an extent
-		 * to add, so add it.
-		 */
-		if (size) {
-			ret = fiemap_fill_next_extent(fieinfo, logical,
-					phys, size, flags);
-			if (ret)
-				goto out;
-		}
+	if (start_blk > last_blk || ret)
+		goto out;
 
-		logical = blk_to_logical(inode, start_blk);
-		phys = blk_to_logical(inode, map_bh.b_blocknr);
-		size = map_bh.b_size;
-		flags = 0;
-		if (buffer_unwritten(&map_bh))
-			flags = FIEMAP_EXTENT_UNWRITTEN;
+	logical = blk_to_logical(inode, start_blk);
+	phys = blk_to_logical(inode, map_bh.b_blocknr);
+	size = map_bh.b_size;
+	flags = 0;
+	if (buffer_unwritten(&map_bh))
+		flags = FIEMAP_EXTENT_UNWRITTEN;
 
-		start_blk += logical_to_blk(inode, size);
+	start_blk += logical_to_blk(inode, size);
 
-		/*
-		 * If we are past the EOF, then we need to make sure as
-		 * soon as we find a hole that the last extent we found
-		 * is marked with FIEMAP_EXTENT_LAST
-		 */
-		if (!past_eof && logical + size >= isize)
-			past_eof = true;
-	}
+prep_next:
 	cond_resched();
 	if (fatal_signal_pending(current))
 		ret = -EINTR;

commit 6d5a1495eebd441216dc96913a4270100b26e104
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Thu Dec 24 18:04:56 2015 +0800

    f2fs: let user being aware of IO error
    
    Sometimes we keep dumb when IO error occur in lower layer device, so user
    will not receive any error return value for some operation, but actually,
    the operation did not succeed.
    
    This sould be avoided, so this patch reports such kind of error to user.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index f2a023edfc1d..5c43b2d606ec 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -498,7 +498,7 @@ static int __allocate_data_block(struct dnode_of_data *dn)
 	return 0;
 }
 
-static void __allocate_data_blocks(struct inode *inode, loff_t offset,
+static int __allocate_data_blocks(struct inode *inode, loff_t offset,
 							size_t count)
 {
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
@@ -507,13 +507,15 @@ static void __allocate_data_blocks(struct inode *inode, loff_t offset,
 	u64 len = F2FS_BYTES_TO_BLK(count);
 	bool allocated;
 	u64 end_offset;
+	int err = 0;
 
 	while (len) {
 		f2fs_lock_op(sbi);
 
 		/* When reading holes, we need its node page */
 		set_new_dnode(&dn, inode, NULL, NULL, 0);
-		if (get_dnode_of_data(&dn, start, ALLOC_NODE))
+		err = get_dnode_of_data(&dn, start, ALLOC_NODE);
+		if (err)
 			goto out;
 
 		allocated = false;
@@ -522,12 +524,15 @@ static void __allocate_data_blocks(struct inode *inode, loff_t offset,
 		while (dn.ofs_in_node < end_offset && len) {
 			block_t blkaddr;
 
-			if (unlikely(f2fs_cp_error(sbi)))
+			if (unlikely(f2fs_cp_error(sbi))) {
+				err = -EIO;
 				goto sync_out;
+			}
 
 			blkaddr = datablock_addr(dn.node_page, dn.ofs_in_node);
 			if (blkaddr == NULL_ADDR || blkaddr == NEW_ADDR) {
-				if (__allocate_data_block(&dn))
+				err = __allocate_data_block(&dn);
+				if (err)
 					goto sync_out;
 				allocated = true;
 			}
@@ -545,7 +550,7 @@ static void __allocate_data_blocks(struct inode *inode, loff_t offset,
 		if (dn.node_changed)
 			f2fs_balance_fs(sbi);
 	}
-	return;
+	return err;
 
 sync_out:
 	if (allocated)
@@ -555,7 +560,7 @@ static void __allocate_data_blocks(struct inode *inode, loff_t offset,
 	f2fs_unlock_op(sbi);
 	if (dn.node_changed)
 		f2fs_balance_fs(sbi);
-	return;
+	return err;
 }
 
 /*
@@ -1653,11 +1658,9 @@ static ssize_t f2fs_direct_IO(struct kiocb *iocb, struct iov_iter *iter,
 	trace_f2fs_direct_IO_enter(inode, offset, count, iov_iter_rw(iter));
 
 	if (iov_iter_rw(iter) == WRITE) {
-		__allocate_data_blocks(inode, offset, count);
-		if (unlikely(f2fs_cp_error(F2FS_I_SB(inode)))) {
-			err = -EIO;
+		err = __allocate_data_blocks(inode, offset, count);
+		if (err)
 			goto out;
-		}
 	}
 
 	err = blockdev_direct_IO(iocb, inode, iter, offset, get_data_block_dio);

commit b4d07a3e1a6e783132be7506aeb171dc5728f077
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed Dec 23 13:48:58 2015 -0800

    f2fs: avoid f2fs_lock_op in f2fs_write_begin
    
    If f2fs_write_begin is to update data, we can bypass calling f2fs_lock_op() in
    order to avoid the checkpoint latency in the write syscall.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index d4839fc2b4ca..f2a023edfc1d 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1418,10 +1418,16 @@ static int prepare_write_begin(struct f2fs_sb_info *sbi,
 	pgoff_t index = page->index;
 	struct dnode_of_data dn;
 	struct page *ipage;
+	bool locked = false;
+	struct extent_info ei;
 	int err = 0;
 
-	f2fs_lock_op(sbi);
-
+	if (f2fs_has_inline_data(inode) ||
+			(pos & PAGE_CACHE_MASK) >= i_size_read(inode)) {
+		f2fs_lock_op(sbi);
+		locked = true;
+	}
+restart:
 	/* check inline_data */
 	ipage = get_node_page(sbi, inode->i_ino);
 	if (IS_ERR(ipage)) {
@@ -1436,22 +1442,42 @@ static int prepare_write_begin(struct f2fs_sb_info *sbi,
 			read_inline_data(page, ipage);
 			set_inode_flag(F2FS_I(inode), FI_DATA_EXIST);
 			sync_inode_page(&dn);
-			goto done;
 		} else {
 			err = f2fs_convert_inline_page(&dn, page);
 			if (err)
-				goto err_out;
+				goto out;
+			if (dn.data_blkaddr == NULL_ADDR)
+				err = f2fs_get_block(&dn, index);
+		}
+	} else if (locked) {
+		err = f2fs_get_block(&dn, index);
+	} else {
+		if (f2fs_lookup_extent_cache(inode, index, &ei)) {
+			dn.data_blkaddr = ei.blk + index - ei.fofs;
+		} else {
+			bool restart = false;
+
+			/* hole case */
+			err = get_dnode_of_data(&dn, index, LOOKUP_NODE);
+			if (err || (!err && dn.data_blkaddr == NULL_ADDR))
+				restart = true;
+			if (restart) {
+				f2fs_put_dnode(&dn);
+				f2fs_lock_op(sbi);
+				locked = true;
+				goto restart;
+			}
 		}
 	}
-	err = f2fs_get_block(&dn, index);
-done:
+
 	/* convert_inline_page can make node_changed */
 	*blk_addr = dn.data_blkaddr;
 	*node_changed = dn.node_changed;
-err_out:
+out:
 	f2fs_put_dnode(&dn);
 unlock_out:
-	f2fs_unlock_op(sbi);
+	if (locked)
+		f2fs_unlock_op(sbi);
 	return err;
 }
 

commit 2aadac085cf0ca3e0295988d4d1dbdeafc15a9f6
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed Dec 23 11:55:18 2015 -0800

    f2fs: introduce prepare_write_begin to clean up
    
    This patch adds prepare_write_begin to clean f2fs_write_begin.
    The major role of this function is to convert any inline_data and allocate
    or find block address.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 958d8261b258..d4839fc2b4ca 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1410,6 +1410,51 @@ static void f2fs_write_failed(struct address_space *mapping, loff_t to)
 	}
 }
 
+static int prepare_write_begin(struct f2fs_sb_info *sbi,
+			struct page *page, loff_t pos, unsigned len,
+			block_t *blk_addr, bool *node_changed)
+{
+	struct inode *inode = page->mapping->host;
+	pgoff_t index = page->index;
+	struct dnode_of_data dn;
+	struct page *ipage;
+	int err = 0;
+
+	f2fs_lock_op(sbi);
+
+	/* check inline_data */
+	ipage = get_node_page(sbi, inode->i_ino);
+	if (IS_ERR(ipage)) {
+		err = PTR_ERR(ipage);
+		goto unlock_out;
+	}
+
+	set_new_dnode(&dn, inode, ipage, ipage, 0);
+
+	if (f2fs_has_inline_data(inode)) {
+		if (pos + len <= MAX_INLINE_DATA) {
+			read_inline_data(page, ipage);
+			set_inode_flag(F2FS_I(inode), FI_DATA_EXIST);
+			sync_inode_page(&dn);
+			goto done;
+		} else {
+			err = f2fs_convert_inline_page(&dn, page);
+			if (err)
+				goto err_out;
+		}
+	}
+	err = f2fs_get_block(&dn, index);
+done:
+	/* convert_inline_page can make node_changed */
+	*blk_addr = dn.data_blkaddr;
+	*node_changed = dn.node_changed;
+err_out:
+	f2fs_put_dnode(&dn);
+unlock_out:
+	f2fs_unlock_op(sbi);
+	return err;
+}
+
 static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 		loff_t pos, unsigned len, unsigned flags,
 		struct page **pagep, void **fsdata)
@@ -1417,9 +1462,9 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 	struct inode *inode = mapping->host;
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
 	struct page *page = NULL;
-	struct page *ipage;
 	pgoff_t index = ((unsigned long long) pos) >> PAGE_CACHE_SHIFT;
-	struct dnode_of_data dn;
+	bool need_balance = false;
+	block_t blkaddr = NULL_ADDR;
 	int err = 0;
 
 	trace_f2fs_write_begin(inode, pos, len, flags);
@@ -1443,37 +1488,12 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 
 	*pagep = page;
 
-	f2fs_lock_op(sbi);
-
-	/* check inline_data */
-	ipage = get_node_page(sbi, inode->i_ino);
-	if (IS_ERR(ipage)) {
-		err = PTR_ERR(ipage);
-		goto unlock_fail;
-	}
-
-	set_new_dnode(&dn, inode, ipage, ipage, 0);
-
-	if (f2fs_has_inline_data(inode)) {
-		if (pos + len <= MAX_INLINE_DATA) {
-			read_inline_data(page, ipage);
-			set_inode_flag(F2FS_I(inode), FI_DATA_EXIST);
-			sync_inode_page(&dn);
-			goto put_next;
-		}
-		err = f2fs_convert_inline_page(&dn, page);
-		if (err)
-			goto put_fail;
-	}
-
-	err = f2fs_get_block(&dn, index);
+	err = prepare_write_begin(sbi, page, pos, len,
+					&blkaddr, &need_balance);
 	if (err)
-		goto put_fail;
-put_next:
-	f2fs_put_dnode(&dn);
-	f2fs_unlock_op(sbi);
+		goto fail;
 
-	if (dn.node_changed && has_not_enough_free_secs(sbi, 0)) {
+	if (need_balance && has_not_enough_free_secs(sbi, 0)) {
 		unlock_page(page);
 		f2fs_balance_fs(sbi);
 		lock_page(page);
@@ -1488,7 +1508,7 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 
 	/* wait for GCed encrypted page writeback */
 	if (f2fs_encrypted_inode(inode) && S_ISREG(inode->i_mode))
-		f2fs_wait_on_encrypted_page_writeback(sbi, dn.data_blkaddr);
+		f2fs_wait_on_encrypted_page_writeback(sbi, blkaddr);
 
 	if (len == PAGE_CACHE_SIZE)
 		goto out_update;
@@ -1504,14 +1524,14 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 		goto out_update;
 	}
 
-	if (dn.data_blkaddr == NEW_ADDR) {
+	if (blkaddr == NEW_ADDR) {
 		zero_user_segment(page, 0, PAGE_CACHE_SIZE);
 	} else {
 		struct f2fs_io_info fio = {
 			.sbi = sbi,
 			.type = DATA,
 			.rw = READ_SYNC,
-			.blk_addr = dn.data_blkaddr,
+			.blk_addr = blkaddr,
 			.page = page,
 			.encrypted_page = NULL,
 		};
@@ -1542,10 +1562,6 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 	clear_cold_data(page);
 	return 0;
 
-put_fail:
-	f2fs_put_dnode(&dn);
-unlock_fail:
-	f2fs_unlock_op(sbi);
 fail:
 	f2fs_put_page(page, 1);
 	f2fs_write_failed(mapping, pos + len);

commit 2a3407607028f7c780f1c20faa4e922bf631d340
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Tue Dec 22 13:23:35 2015 -0800

    f2fs: call f2fs_balance_fs only when node was changed
    
    If user tries to update or read data, we don't need to call f2fs_balance_fs
    which triggers f2fs_gc, which increases unnecessary long latency.
    
    Reviewed-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 82ecaa30fd77..958d8261b258 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -509,7 +509,6 @@ static void __allocate_data_blocks(struct inode *inode, loff_t offset,
 	u64 end_offset;
 
 	while (len) {
-		f2fs_balance_fs(sbi);
 		f2fs_lock_op(sbi);
 
 		/* When reading holes, we need its node page */
@@ -542,6 +541,9 @@ static void __allocate_data_blocks(struct inode *inode, loff_t offset,
 
 		f2fs_put_dnode(&dn);
 		f2fs_unlock_op(sbi);
+
+		if (dn.node_changed)
+			f2fs_balance_fs(sbi);
 	}
 	return;
 
@@ -551,6 +553,8 @@ static void __allocate_data_blocks(struct inode *inode, loff_t offset,
 	f2fs_put_dnode(&dn);
 out:
 	f2fs_unlock_op(sbi);
+	if (dn.node_changed)
+		f2fs_balance_fs(sbi);
 	return;
 }
 
@@ -649,6 +653,8 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 
 		if (create) {
 			f2fs_unlock_op(sbi);
+			if (dn.node_changed)
+				f2fs_balance_fs(sbi);
 			f2fs_lock_op(sbi);
 		}
 
@@ -706,8 +712,11 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 put_out:
 	f2fs_put_dnode(&dn);
 unlock_out:
-	if (create)
+	if (create) {
 		f2fs_unlock_op(sbi);
+		if (dn.node_changed)
+			f2fs_balance_fs(sbi);
+	}
 out:
 	trace_f2fs_map_blocks(inode, map, err);
 	return err;
@@ -1415,8 +1424,6 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 
 	trace_f2fs_write_begin(inode, pos, len, flags);
 
-	f2fs_balance_fs(sbi);
-
 	/*
 	 * We should check this at this moment to avoid deadlock on inode page
 	 * and #0 page. The locking rule for inline_data conversion should be:
@@ -1466,6 +1473,17 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 	f2fs_put_dnode(&dn);
 	f2fs_unlock_op(sbi);
 
+	if (dn.node_changed && has_not_enough_free_secs(sbi, 0)) {
+		unlock_page(page);
+		f2fs_balance_fs(sbi);
+		lock_page(page);
+		if (page->mapping != mapping) {
+			/* The page got truncated from under us */
+			f2fs_put_page(page, 1);
+			goto repeat;
+		}
+	}
+
 	f2fs_wait_on_page_writeback(page, DATA);
 
 	/* wait for GCed encrypted page writeback */

commit 3104af35eb6a2452ccc9912997e7728777100de2
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Wed Dec 23 17:11:43 2015 +0800

    f2fs: reduce covered region of sbi->cp_rwsem in f2fs_map_blocks
    
    Only cover sbi->cp_rwsem on one dnode page's allocation and modification
    instead of multiple's in f2fs_map_blocks, it can reduce the covered region
    of cp_rwsem, then we can avoid potential long time delay for concurrent
    checkpointer.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index a7a9a05d012a..82ecaa30fd77 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -590,7 +590,7 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 	}
 
 	if (create)
-		f2fs_lock_op(F2FS_I_SB(inode));
+		f2fs_lock_op(sbi);
 
 	/* When reading holes, we need its node page */
 	set_new_dnode(&dn, inode, NULL, NULL, 0);
@@ -647,6 +647,11 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 		allocated = false;
 		f2fs_put_dnode(&dn);
 
+		if (create) {
+			f2fs_unlock_op(sbi);
+			f2fs_lock_op(sbi);
+		}
+
 		set_new_dnode(&dn, inode, NULL, NULL, 0);
 		err = get_dnode_of_data(&dn, pgofs, mode);
 		if (err) {
@@ -702,7 +707,7 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 	f2fs_put_dnode(&dn);
 unlock_out:
 	if (create)
-		f2fs_unlock_op(F2FS_I_SB(inode));
+		f2fs_unlock_op(sbi);
 out:
 	trace_f2fs_map_blocks(inode, map, err);
 	return err;

commit 93bae099eaa0ae784fbe4d9eddcdc54fb5812466
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Tue Dec 22 12:59:54 2015 -0800

    f2fs: record node block allocation in dnode_of_data
    
    This patch introduces recording node block allocation in dnode_of_data.
    This information helps to figure out whether any node block is allocated during
    specific file operations.
    
    Reviewed-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index cf0c9dda0365..a7a9a05d012a 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -226,6 +226,7 @@ void set_data_blkaddr(struct dnode_of_data *dn)
 	addr_array = blkaddr_in_node(rn);
 	addr_array[ofs_in_node] = cpu_to_le32(dn->data_blkaddr);
 	set_page_dirty(node_page);
+	dn->node_changed = true;
 }
 
 int reserve_new_block(struct dnode_of_data *dn)

commit b9d777b85ff1ff79a1173190317b25bebc404ab4
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Tue Dec 22 11:09:35 2015 -0800

    f2fs: check inline_data flag at converting time
    
    We can check inode's inline_data flag  when calling to convert it.
    
    Reviewed-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index e34b1bdfc995..cf0c9dda0365 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1573,11 +1573,9 @@ static ssize_t f2fs_direct_IO(struct kiocb *iocb, struct iov_iter *iter,
 	int err;
 
 	/* we don't need to use inline_data strictly */
-	if (f2fs_has_inline_data(inode)) {
-		err = f2fs_convert_inline_inode(inode);
-		if (err)
-			return err;
-	}
+	err = f2fs_convert_inline_inode(inode);
+	if (err)
+		return err;
 
 	if (f2fs_encrypted_inode(inode) && S_ISREG(inode->i_mode))
 		return 0;

commit 7df3a4318d07ba520b4a8eddad29e9ac748b0a19
Author: Fan Li <fanofcode.li@samsung.com>
Date:   Thu Dec 17 13:20:59 2015 +0800

    f2fs: optimize the flow of f2fs_map_blocks
    
    check map->m_len right after it changes to avoid excess call
    to update dnode_of_data.
    
    Signed-off-by: Fan li <fanofcode.li@samsung.com>
    Reviewed-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 292a06cbea07..e34b1bdfc995 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -573,6 +573,7 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 	int err = 0, ofs = 1;
 	struct extent_info ei;
 	bool allocated = false;
+	block_t blkaddr;
 
 	map->m_len = 0;
 	map->m_flags = 0;
@@ -636,6 +637,9 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 	pgofs++;
 
 get_next:
+	if (map->m_len >= maxblocks)
+		goto sync_out;
+
 	if (dn.ofs_in_node >= end_offset) {
 		if (allocated)
 			sync_inode_page(&dn);
@@ -653,44 +657,43 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 		end_offset = ADDRS_PER_PAGE(dn.node_page, F2FS_I(inode));
 	}
 
-	if (maxblocks > map->m_len) {
-		block_t blkaddr = datablock_addr(dn.node_page, dn.ofs_in_node);
+	blkaddr = datablock_addr(dn.node_page, dn.ofs_in_node);
 
-		if (blkaddr == NEW_ADDR || blkaddr == NULL_ADDR) {
-			if (create) {
-				if (unlikely(f2fs_cp_error(sbi))) {
-					err = -EIO;
-					goto sync_out;
-				}
-				err = __allocate_data_block(&dn);
-				if (err)
-					goto sync_out;
-				allocated = true;
-				map->m_flags |= F2FS_MAP_NEW;
-				blkaddr = dn.data_blkaddr;
-			} else {
-				/*
-				 * we only merge preallocated unwritten blocks
-				 * for fiemap.
-				 */
-				if (flag != F2FS_GET_BLOCK_FIEMAP ||
-						blkaddr != NEW_ADDR)
-					goto sync_out;
+	if (blkaddr == NEW_ADDR || blkaddr == NULL_ADDR) {
+		if (create) {
+			if (unlikely(f2fs_cp_error(sbi))) {
+				err = -EIO;
+				goto sync_out;
 			}
+			err = __allocate_data_block(&dn);
+			if (err)
+				goto sync_out;
+			allocated = true;
+			map->m_flags |= F2FS_MAP_NEW;
+			blkaddr = dn.data_blkaddr;
+		} else {
+			/*
+			 * we only merge preallocated unwritten blocks
+			 * for fiemap.
+			 */
+			if (flag != F2FS_GET_BLOCK_FIEMAP ||
+					blkaddr != NEW_ADDR)
+				goto sync_out;
 		}
+	}
 
-		/* Give more consecutive addresses for the readahead */
-		if ((map->m_pblk != NEW_ADDR &&
-				blkaddr == (map->m_pblk + ofs)) ||
-				(map->m_pblk == NEW_ADDR &&
-				blkaddr == NEW_ADDR)) {
-			ofs++;
-			dn.ofs_in_node++;
-			pgofs++;
-			map->m_len++;
-			goto get_next;
-		}
+	/* Give more consecutive addresses for the readahead */
+	if ((map->m_pblk != NEW_ADDR &&
+			blkaddr == (map->m_pblk + ofs)) ||
+			(map->m_pblk == NEW_ADDR &&
+			blkaddr == NEW_ADDR)) {
+		ofs++;
+		dn.ofs_in_node++;
+		pgofs++;
+		map->m_len++;
+		goto get_next;
 	}
+
 sync_out:
 	if (allocated)
 		sync_inode_page(&dn);

commit c227f912732f204c0ec4a577ba812401ac4672af
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Wed Dec 16 13:09:20 2015 +0800

    f2fs: record dirty status of regular/symlink inode
    
    Maintain regular/symlink inode which has dirty pages in global dirty list
    and record their total dirty pages count like the way of handling directory
    inode.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 90a2ffea875b..292a06cbea07 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1180,7 +1180,7 @@ static int f2fs_write_data_page(struct page *page,
 		f2fs_balance_fs(sbi);
 	if (wbc->for_reclaim) {
 		f2fs_submit_merged_bio(sbi, DATA, WRITE);
-		remove_dirty_dir_inode(inode);
+		remove_dirty_inode(inode);
 	}
 	return 0;
 
@@ -1372,7 +1372,7 @@ static int f2fs_write_data_pages(struct address_space *mapping,
 	if (locked)
 		mutex_unlock(&sbi->writepages);
 
-	remove_dirty_dir_inode(inode);
+	remove_dirty_inode(inode);
 
 	wbc->nr_to_write = max((long)0, wbc->nr_to_write - diff);
 	return ret;

commit 9006f2c93fe5cc450bc0d3a4924b46393f165b4a
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Tue Dec 1 11:47:33 2015 +0800

    f2fs: kill f2fs_drop_largest_extent
    
    For direct IO, f2fs only allocate new address for the block which is not
    exist in the disk before, its mapping info should not exist in extent
    cache previously, so here we do not need to call f2fs_drop_largest_extent
    to drop related cache.
    
    Due to no more callers for f2fs_drop_largest_extent now, kill it.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 6c689e9a86a3..90a2ffea875b 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -494,10 +494,6 @@ static int __allocate_data_block(struct dnode_of_data *dn)
 	if (i_size_read(dn->inode) < ((loff_t)(fofs + 1) << PAGE_CACHE_SHIFT))
 		i_size_write(dn->inode,
 				((loff_t)(fofs + 1) << PAGE_CACHE_SHIFT));
-
-	/* direct IO doesn't use extent cache to maximize the performance */
-	f2fs_drop_largest_extent(dn->inode, fofs);
-
 	return 0;
 }
 

commit eb7e813cc791735f2428202d5249a8fd769df1f3
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Tue Nov 10 18:45:07 2015 +0800

    f2fs: fix to remove directory inode from dirty list
    
    If last dirty dentry page was writebacked in reclaim path, we should
    remove its directory inode from global dirty list to avoid unnecessary
    flush for this inode when doing checkpoint.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index c3e1ffa0c8d6..6c689e9a86a3 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1182,8 +1182,10 @@ static int f2fs_write_data_page(struct page *page,
 	unlock_page(page);
 	if (need_balance_fs)
 		f2fs_balance_fs(sbi);
-	if (wbc->for_reclaim)
+	if (wbc->for_reclaim) {
 		f2fs_submit_merged_bio(sbi, DATA, WRITE);
+		remove_dirty_dir_inode(inode);
+	}
 	return 0;
 
 redirty_out:

commit d323d005ac4a2d413128267af76bb9d71f7303da
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Tue Oct 27 09:53:45 2015 +0800

    f2fs: support file defragment
    
    This patch introduces a new ioctl F2FS_IOC_DEFRAGMENT to support file
    defragment in a specified range of regular file.
    
    This ioctl can be used in very limited workload: if user expects high
    sequential read performance in randomly written file, this interface
    can be used for defragmentation, after that file can be written as
    continuous as possible in the device.
    
    Meanwhile, it has side-effect, it will make holes in segments where
    blocks located originally, so it's better to trigger GC to eliminate
    fragment in segments.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 0cc8de2839c4..c3e1ffa0c8d6 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -566,7 +566,7 @@ static void __allocate_data_blocks(struct inode *inode, loff_t offset,
  *     b. do not use extent cache for better performance
  *     c. give the block addresses to blockdev
  */
-static int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
+int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 						int create, int flag)
 {
 	unsigned int maxblocks = map->m_len;
@@ -1355,6 +1355,10 @@ static int f2fs_write_data_pages(struct address_space *mapping,
 			available_free_memory(sbi, DIRTY_DENTS))
 		goto skip_write;
 
+	/* skip writing during file defragment */
+	if (is_inode_flag_set(F2FS_I(inode), FI_DO_DEFRAG))
+		goto skip_write;
+
 	/* during POR, we don't need to trigger writepage at all. */
 	if (unlikely(is_sbi_flag_set(sbi, SBI_POR_DOING)))
 		goto skip_write;

commit 2da3e027461ab0148384b02bd5905f1a7b335dff
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Wed Oct 28 17:56:14 2015 +0800

    f2fs: commit atomic written page in LFS mode
    
    We should always commit atomic written pages in LFS mode, otherwise data
    will become corrupted if we encounter suddent power cut after partial
    pages committed in IPU mode.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 972eab7ac071..0cc8de2839c4 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1083,6 +1083,7 @@ int do_write_data_page(struct f2fs_io_info *fio)
 	 */
 	if (unlikely(fio->blk_addr != NEW_ADDR &&
 			!is_cold_data(page) &&
+			!IS_ATOMIC_WRITTEN_PAGE(page) &&
 			need_inplace_update(inode))) {
 		rewrite_data_page(fio);
 		set_inode_flag(F2FS_I(inode), FI_UPDATE_WRITE);

commit 67f8cf3cee6f398d05de8333c04fea2ddb59c805
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Oct 15 11:34:49 2015 -0700

    f2fs: support fiemap for inline_data
    
    There is a FIEMAP_EXTENT_INLINE_DATA, pointed out by Marc.
    
    Reviewed-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index b052e7c262ac..972eab7ac071 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -771,6 +771,12 @@ int f2fs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,
 	if (ret)
 		return ret;
 
+	if (f2fs_has_inline_data(inode)) {
+		ret = f2fs_inline_data_fiemap(inode, fieinfo, start, len);
+		if (ret != -EAGAIN)
+			return ret;
+	}
+
 	mutex_lock(&inode->i_mutex);
 
 	if (len >= isize) {

commit 1d373a0ef7a7bc08f95ca820c627e961fb21e188
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Mon Oct 19 10:29:51 2015 -0700

    f2fs: flush dirty data for bmap
    
    Users expect bmap will give allocated block addresses.
    Let's play likewise ext4.
    
    Reviewed-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 77dfc9eee380..b052e7c262ac 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1667,12 +1667,13 @@ static sector_t f2fs_bmap(struct address_space *mapping, sector_t block)
 {
 	struct inode *inode = mapping->host;
 
-	/* we don't need to use inline_data strictly */
-	if (f2fs_has_inline_data(inode)) {
-		int err = f2fs_convert_inline_inode(inode);
-		if (err)
-			return err;
-	}
+	if (f2fs_has_inline_data(inode))
+		return 0;
+
+	/* make sure allocating whole blocks */
+	if (mapping_tagged(mapping, PAGECACHE_TAG_DIRTY))
+		filemap_write_and_wait(mapping);
+
 	return generic_block_bmap(mapping, block, get_data_block_bmap);
 }
 

commit 08b39fbd59781729da9fb6367decaf4804a22721
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Thu Oct 8 13:27:34 2015 +0800

    f2fs crypto: fix racing of accessing encrypted page among
    
     different competitors
    
    Since we use different page cache (normally inode's page cache for R/W
    and meta inode's page cache for GC) to cache the same physical block
    which is belong to an encrypted inode. Writeback of these two page
    cache should be exclusive, but now we didn't handle writeback state
    well, so there may be potential racing problem:
    
    a)
    kworker:                                f2fs_gc:
     - f2fs_write_data_pages
      - f2fs_write_data_page
       - do_write_data_page
        - write_data_page
         - f2fs_submit_page_mbio
    (page#1 in inode's page cache was queued
    in f2fs bio cache, and be ready to write
    to new blkaddr)
                                             - gc_data_segment
                                              - move_encrypted_block
                                               - pagecache_get_page
                                            (page#2 in meta inode's page cache
                                            was cached with the invalid datas
                                            of physical block located in new
                                            blkaddr)
                                               - f2fs_submit_page_mbio
                                            (page#1 was submitted, later, page#2
                                            with invalid data will be submitted)
    
    b)
    f2fs_gc:
     - gc_data_segment
      - move_encrypted_block
       - f2fs_submit_page_mbio
    (page#1 in meta inode's page cache was
    queued in f2fs bio cache, and be ready
    to write to new blkaddr)
                                            user thread:
                                             - f2fs_write_begin
                                              - f2fs_submit_page_bio
                                            (we submit the request to block layer
                                            to update page#2 in inode's page cache
                                            with physical block located in new
                                            blkaddr, so here we may read gabbage
                                            data from new blkaddr since GC hasn't
                                            writebacked the page#1 yet)
    
    This patch fixes above potential racing problem for encrypted inode.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 228537657723..77dfc9eee380 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -956,21 +956,14 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 
 			if (f2fs_encrypted_inode(inode) &&
 					S_ISREG(inode->i_mode)) {
-				struct page *cpage;
 
 				ctx = f2fs_get_crypto_ctx(inode);
 				if (IS_ERR(ctx))
 					goto set_error_page;
 
 				/* wait the page to be moved by cleaning */
-				cpage = find_lock_page(
-						META_MAPPING(F2FS_I_SB(inode)),
-						block_nr);
-				if (cpage) {
-					f2fs_wait_on_page_writeback(cpage,
-									DATA);
-					f2fs_put_page(cpage, 1);
-				}
+				f2fs_wait_on_encrypted_page_writeback(
+						F2FS_I_SB(inode), block_nr);
 			}
 
 			bio = bio_alloc(GFP_KERNEL,
@@ -1064,6 +1057,11 @@ int do_write_data_page(struct f2fs_io_info *fio)
 	}
 
 	if (f2fs_encrypted_inode(inode) && S_ISREG(inode->i_mode)) {
+
+		/* wait for GCed encrypted page writeback */
+		f2fs_wait_on_encrypted_page_writeback(F2FS_I_SB(inode),
+							fio->blk_addr);
+
 		fio->encrypted_page = f2fs_encrypt(inode, fio->page);
 		if (IS_ERR(fio->encrypted_page)) {
 			err = PTR_ERR(fio->encrypted_page);
@@ -1452,6 +1450,10 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 
 	f2fs_wait_on_page_writeback(page, DATA);
 
+	/* wait for GCed encrypted page writeback */
+	if (f2fs_encrypted_inode(inode) && S_ISREG(inode->i_mode))
+		f2fs_wait_on_encrypted_page_writeback(sbi, dn.data_blkaddr);
+
 	if (len == PAGE_CACHE_SIZE)
 		goto out_update;
 	if (PageUptodate(page))

commit b8c2940048adf4b2fcc5ae738f2bd4821ebf6a8a
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Mon Oct 12 17:02:26 2015 +0800

    f2fs: add a tracepoint for f2fs_read_data_pages
    
    This patch adds a tracepoint for f2fs_read_data_pages to trace when pages
    are readahead by VFS.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index d4f1c74bab26..228537657723 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1032,6 +1032,9 @@ static int f2fs_read_data_pages(struct file *file,
 			struct list_head *pages, unsigned nr_pages)
 {
 	struct inode *inode = file->f_mapping->host;
+	struct page *page = list_entry(pages->prev, struct page, lru);
+
+	trace_f2fs_readpages(inode, page, nr_pages);
 
 	/* If the file has inline data, skip readpages */
 	if (f2fs_has_inline_data(inode))

commit a56c7c6fb3c60857c1335bcb8b914e6f65655486
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Oct 9 15:11:38 2015 -0700

    f2fs: set GFP_NOFS for grab_cache_page
    
    For normal inodes, their pages are allocated with __GFP_FS, which can cause
    filesystem calls when reclaiming memory.
    This can incur a dead lock condition accordingly.
    
    So, this patch addresses this problem by introducing
    f2fs_grab_cache_page(.., bool for_write), which calls
    grab_cache_page_write_begin() with AOP_FLAG_NOFS.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index bc04e9201fd6..d4f1c74bab26 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -275,7 +275,8 @@ int f2fs_get_block(struct dnode_of_data *dn, pgoff_t index)
 	return f2fs_reserve_block(dn, index);
 }
 
-struct page *get_read_data_page(struct inode *inode, pgoff_t index, int rw)
+struct page *get_read_data_page(struct inode *inode, pgoff_t index,
+						int rw, bool for_write)
 {
 	struct address_space *mapping = inode->i_mapping;
 	struct dnode_of_data dn;
@@ -292,7 +293,7 @@ struct page *get_read_data_page(struct inode *inode, pgoff_t index, int rw)
 	if (f2fs_encrypted_inode(inode) && S_ISREG(inode->i_mode))
 		return read_mapping_page(mapping, index, NULL);
 
-	page = grab_cache_page(mapping, index);
+	page = f2fs_grab_cache_page(mapping, index, for_write);
 	if (!page)
 		return ERR_PTR(-ENOMEM);
 
@@ -352,7 +353,7 @@ struct page *find_data_page(struct inode *inode, pgoff_t index)
 		return page;
 	f2fs_put_page(page, 0);
 
-	page = get_read_data_page(inode, index, READ_SYNC);
+	page = get_read_data_page(inode, index, READ_SYNC, false);
 	if (IS_ERR(page))
 		return page;
 
@@ -372,12 +373,13 @@ struct page *find_data_page(struct inode *inode, pgoff_t index)
  * Because, the callers, functions in dir.c and GC, should be able to know
  * whether this page exists or not.
  */
-struct page *get_lock_data_page(struct inode *inode, pgoff_t index)
+struct page *get_lock_data_page(struct inode *inode, pgoff_t index,
+							bool for_write)
 {
 	struct address_space *mapping = inode->i_mapping;
 	struct page *page;
 repeat:
-	page = get_read_data_page(inode, index, READ_SYNC);
+	page = get_read_data_page(inode, index, READ_SYNC, for_write);
 	if (IS_ERR(page))
 		return page;
 
@@ -411,7 +413,7 @@ struct page *get_new_data_page(struct inode *inode,
 	struct dnode_of_data dn;
 	int err;
 repeat:
-	page = grab_cache_page(mapping, index);
+	page = f2fs_grab_cache_page(mapping, index, true);
 	if (!page) {
 		/*
 		 * before exiting, we should make sure ipage will be released
@@ -439,7 +441,7 @@ struct page *get_new_data_page(struct inode *inode,
 	} else {
 		f2fs_put_page(page, 1);
 
-		page = get_read_data_page(inode, index, READ_SYNC);
+		page = get_read_data_page(inode, index, READ_SYNC, true);
 		if (IS_ERR(page))
 			goto repeat;
 

commit a125702326d9c3b753fe9c9b9727d3b3dd1cba4a
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Oct 8 10:40:07 2015 -0700

    Revert "f2fs: do not skip dentry block writes"
    
    The periodic checkpoint can resolve the previous issue.
    So, now we can use this again to improve the reported performance regression:
    
    https://lkml.org/lkml/2015/10/8/20
    
    This reverts commit 15bec0ff5a9ba6d203178fa8772259df6207942a.

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index a903423e4cd5..bc04e9201fd6 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1340,6 +1340,11 @@ static int f2fs_write_data_pages(struct address_space *mapping,
 	if (!get_dirty_pages(inode) && wbc->sync_mode == WB_SYNC_NONE)
 		return 0;
 
+	if (S_ISDIR(inode->i_mode) && wbc->sync_mode == WB_SYNC_NONE &&
+			get_dirty_pages(inode) < nr_pages_to_skip(sbi, DATA) &&
+			available_free_memory(sbi, DIRTY_DENTS))
+		goto skip_write;
+
 	/* during POR, we don't need to trigger writepage at all. */
 	if (unlikely(is_sbi_flag_set(sbi, SBI_POR_DOING)))
 		goto skip_write;

commit 90b803e6fb6243922bff9ddd8a6205c17cb93b31
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Sep 25 19:34:50 2015 -0700

    f2fs: do not skip dentry block writes
    
    Previously, we skip dentry block writes when wbc is SYNC_NONE with no memory
    pressure and the number of dirty pages is pretty small.
    
    But, we didn't skip for normal data writes, which gives us not much big impact
    on overall performance.
    Moreover, by skipping some data writes, kworker falls into infinite loop to try
    to write blocks, when many dir inodes have only one dentry block.
    
    So, this patch removes skipping data writes.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index bc04e9201fd6..a903423e4cd5 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1340,11 +1340,6 @@ static int f2fs_write_data_pages(struct address_space *mapping,
 	if (!get_dirty_pages(inode) && wbc->sync_mode == WB_SYNC_NONE)
 		return 0;
 
-	if (S_ISDIR(inode->i_mode) && wbc->sync_mode == WB_SYNC_NONE &&
-			get_dirty_pages(inode) < nr_pages_to_skip(sbi, DATA) &&
-			available_free_memory(sbi, DIRTY_DENTS))
-		goto skip_write;
-
 	/* during POR, we don't need to trigger writepage at all. */
 	if (unlikely(is_sbi_flag_set(sbi, SBI_POR_DOING)))
 		goto skip_write;

commit 46c9e1413f3c4ecbbe775e545063e88feb1f9871
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Fri Sep 18 16:54:16 2015 +0800

    f2fs: use correct flag in f2fs_map_blocks()
    
    We introduce F2FS_GET_BLOCK_READ in commit e2b4e2bc8865 ("f2fs: fix
    incorrect mapping for bmap"), but forget to use this flag in the right
    place, fix it.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index d265401b5326..bc04e9201fd6 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -920,7 +920,8 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 			map.m_lblk = block_in_file;
 			map.m_len = last_block - block_in_file;
 
-			if (f2fs_map_blocks(inode, &map, 0, false))
+			if (f2fs_map_blocks(inode, &map, 0,
+							F2FS_GET_BLOCK_READ))
 				goto set_error_page;
 		}
 got_it:

commit f9811703fefbdb22abf723ec61368777cdad8508
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Mon Sep 21 20:17:52 2015 +0800

    f2fs: fix to handle io error in ->direct_IO
    
    Here is a oops reported as following message when testing generic/019 of
    xfstest:
    
     ------------[ cut here ]------------
     kernel BUG at /home/yuchao/git/f2fs-dev/segment.c:882!
     invalid opcode: 0000 [#1] SMP
     Modules linked in: zram lz4_compress lz4_decompress f2fs(O) ip6table_filter ip6_tables ebtable_nat ebtables nf_conntrack_ipv4
    nf_def
     CPU: 2 PID: 25441 Comm: fio Tainted: G           O    4.3.0-rc1+ #6
     Hardware name: Hewlett-Packard HP Z220 CMT Workstation/1790, BIOS K51 v01.61 05/16/2013
     task: ffff8803f4e85580 ti: ffff8803fd61c000 task.ti: ffff8803fd61c000
     RIP: 0010:[<ffffffffa0784981>]  [<ffffffffa0784981>] new_curseg+0x321/0x330 [f2fs]
     RSP: 0018:ffff8803fd61f918  EFLAGS: 00010246
     RAX: 00000000000007ed RBX: 0000000000000224 RCX: 000000000000001f
     RDX: 0000000000000800 RSI: ffffffffffffffff RDI: ffff8803f56f4300
     RBP: ffff8803fd61f978 R08: 0000000000000000 R09: 0000000000000000
     R10: 0000000000000024 R11: ffff8800d23bbd78 R12: ffff8800d0ef0000
     R13: 0000000000000224 R14: 0000000000000000 R15: 0000000000000001
     FS:  00007f827ff85700(0000) GS:ffff88041ea80000(0000) knlGS:0000000000000000
     CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
     CR2: ffffffffff600000 CR3: 00000003fef17000 CR4: 00000000001406e0
     Stack:
      000007ea00000002 0000000100000001 ffff8803f6456248 000007ed0000002b
      0000000000000224 ffff880404d1aa20 ffff8803fd61f9c8 ffff8800d0ef0000
      ffff8803f6456248 0000000000000001 00000000ffffffff ffffffffa078f358
     Call Trace:
      [<ffffffffa0785b87>] allocate_segment_by_default+0x1a7/0x1f0 [f2fs]
      [<ffffffffa078322c>] allocate_data_block+0x17c/0x360 [f2fs]
      [<ffffffffa0779521>] __allocate_data_block+0x131/0x1d0 [f2fs]
      [<ffffffffa077a995>] f2fs_direct_IO+0x4b5/0x580 [f2fs]
      [<ffffffff811510ae>] generic_file_direct_write+0xae/0x160
      [<ffffffff811518f5>] __generic_file_write_iter+0xd5/0x1f0
      [<ffffffff81151e07>] generic_file_write_iter+0xf7/0x200
      [<ffffffff81319e38>] ? apparmor_file_permission+0x18/0x20
      [<ffffffffa0768480>] ? f2fs_fallocate+0x1190/0x1190 [f2fs]
      [<ffffffffa07684c6>] f2fs_file_write_iter+0x46/0x90 [f2fs]
      [<ffffffff8120b4fe>] aio_run_iocb+0x1ee/0x290
      [<ffffffff81700f7e>] ? mutex_lock+0x1e/0x50
      [<ffffffff8120a1d7>] ? aio_read_events+0x207/0x2b0
      [<ffffffff8120b913>] do_io_submit+0x373/0x630
      [<ffffffff8120a4f6>] ? SyS_io_getevents+0x56/0xb0
      [<ffffffff8120bbe0>] SyS_io_submit+0x10/0x20
      [<ffffffff81703857>] entry_SYSCALL_64_fastpath+0x12/0x6a
     Code: 45 c8 48 8b 78 10 e8 9f 23 bf e0 41 8b 8c 24 cc 03 00 00 89 c7 31 d2 89 c6 89 d8 29 df f7 f1 29 d1 39 cf 0f 83 be fd ff ff eb
     RIP  [<ffffffffa0784981>] new_curseg+0x321/0x330 [f2fs]
      RSP <ffff8803fd61f918>
     ---[ end trace 2e577d7f711ddb86 ]---
    
    The reason is that: in the test of generic/019, we will trigger a manmade
    IO error in block layer through debugfs, after that, prefree segment will
    no longer be freed, because we always skip doing gc or checkpoint when
    there occurs an IO error.
    
    Meanwhile fio with aio engine generated a large number of direct IOs,
    which continue allocating spaces in free segment until we run out of them,
    eventually, results in panic in new_curseg as no more free segment was
    found.
    
    So, this patch changes to return EIO in direct_IO for this condition.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 883b6499841f..d265401b5326 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -524,6 +524,9 @@ static void __allocate_data_blocks(struct inode *inode, loff_t offset,
 		while (dn.ofs_in_node < end_offset && len) {
 			block_t blkaddr;
 
+			if (unlikely(f2fs_cp_error(sbi)))
+				goto sync_out;
+
 			blkaddr = datablock_addr(dn.node_page, dn.ofs_in_node);
 			if (blkaddr == NULL_ADDR || blkaddr == NEW_ADDR) {
 				if (__allocate_data_block(&dn))
@@ -566,6 +569,7 @@ static int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 {
 	unsigned int maxblocks = map->m_len;
 	struct dnode_of_data dn;
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
 	int mode = create ? ALLOC_NODE : LOOKUP_NODE_RA;
 	pgoff_t pgofs, end_offset;
 	int err = 0, ofs = 1;
@@ -599,6 +603,10 @@ static int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 
 	if (dn.data_blkaddr == NEW_ADDR || dn.data_blkaddr == NULL_ADDR) {
 		if (create) {
+			if (unlikely(f2fs_cp_error(sbi))) {
+				err = -EIO;
+				goto put_out;
+			}
 			err = __allocate_data_block(&dn);
 			if (err)
 				goto put_out;
@@ -652,6 +660,10 @@ static int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 
 		if (blkaddr == NEW_ADDR || blkaddr == NULL_ADDR) {
 			if (create) {
+				if (unlikely(f2fs_cp_error(sbi))) {
+					err = -EIO;
+					goto sync_out;
+				}
 				err = __allocate_data_block(&dn);
 				if (err)
 					goto sync_out;
@@ -1556,10 +1568,16 @@ static ssize_t f2fs_direct_IO(struct kiocb *iocb, struct iov_iter *iter,
 
 	trace_f2fs_direct_IO_enter(inode, offset, count, iov_iter_rw(iter));
 
-	if (iov_iter_rw(iter) == WRITE)
+	if (iov_iter_rw(iter) == WRITE) {
 		__allocate_data_blocks(inode, offset, count);
+		if (unlikely(f2fs_cp_error(F2FS_I_SB(inode)))) {
+			err = -EIO;
+			goto out;
+		}
+	}
 
 	err = blockdev_direct_IO(iocb, inode, iter, offset, get_data_block_dio);
+out:
 	if (err < 0 && iov_iter_rw(iter) == WRITE)
 		f2fs_write_failed(mapping, offset + count);
 

commit 973163fc0ce07761eae671e895dc6747a3410fe7
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Fri Sep 18 16:51:51 2015 +0800

    f2fs: reorganize f2fs_map_blocks
    
    In this patch, we try to reorganize f2fs_map_blocks to make block mapping
    flow more clear by using following structure:
    
    /* check status of mapping */
    
    if (unmapped) {
            /* blkaddr == NULL_ADDR || blkaddr == NEW_ADDR */
    
            if (create) {
                    /* write path, handle dio write case here */
                    alloc_and_map;
            } else {
                    /*
                     * handle read cases from all call paths:
                     *     1. generic read;
                     *     2. dio read;
                     *     3. fiemap;
                     *     4. bmap
                     */
            }
    }
    
    /* map buffer_header */
    
    Besides, this patch handles the missing case correctly for dio write:
    When we fail in __allocate_data_blocks, then in f2fs_map_blocks, we will
    not allocate blocks correctly for preallocated blocks, but returning with
    an unmapped buffer head, which will result in failure of dio write.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 681373e1da92..883b6499841f 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -596,40 +596,36 @@ static int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 			err = 0;
 		goto unlock_out;
 	}
-	if (dn.data_blkaddr == NEW_ADDR) {
-		if (flag == F2FS_GET_BLOCK_BMAP) {
-			err = -ENOENT;
-			goto put_out;
-		} else if (flag == F2FS_GET_BLOCK_READ ||
-				flag == F2FS_GET_BLOCK_DIO) {
-			goto put_out;
+
+	if (dn.data_blkaddr == NEW_ADDR || dn.data_blkaddr == NULL_ADDR) {
+		if (create) {
+			err = __allocate_data_block(&dn);
+			if (err)
+				goto put_out;
+			allocated = true;
+			map->m_flags = F2FS_MAP_NEW;
+		} else {
+			if (flag != F2FS_GET_BLOCK_FIEMAP ||
+						dn.data_blkaddr != NEW_ADDR) {
+				if (flag == F2FS_GET_BLOCK_BMAP)
+					err = -ENOENT;
+				goto put_out;
+			}
+
+			/*
+			 * preallocated unwritten block should be mapped
+			 * for fiemap.
+			 */
+			if (dn.data_blkaddr == NEW_ADDR)
+				map->m_flags = F2FS_MAP_UNWRITTEN;
 		}
-		/*
-		 * if it is in fiemap call path (flag = F2FS_GET_BLOCK_FIEMAP),
-		 * mark it as mapped and unwritten block.
-		 */
 	}
 
-	if (dn.data_blkaddr != NULL_ADDR) {
-		map->m_flags = F2FS_MAP_MAPPED;
-		map->m_pblk = dn.data_blkaddr;
-		if (dn.data_blkaddr == NEW_ADDR)
-			map->m_flags |= F2FS_MAP_UNWRITTEN;
-	} else if (create) {
-		err = __allocate_data_block(&dn);
-		if (err)
-			goto put_out;
-		allocated = true;
-		map->m_flags = F2FS_MAP_NEW | F2FS_MAP_MAPPED;
-		map->m_pblk = dn.data_blkaddr;
-	} else {
-		if (flag == F2FS_GET_BLOCK_BMAP)
-			err = -ENOENT;
-		goto put_out;
-	}
+	map->m_flags |= F2FS_MAP_MAPPED;
+	map->m_pblk = dn.data_blkaddr;
+	map->m_len = 1;
 
 	end_offset = ADDRS_PER_PAGE(dn.node_page, F2FS_I(inode));
-	map->m_len = 1;
 	dn.ofs_in_node++;
 	pgofs++;
 
@@ -648,23 +644,31 @@ static int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 			goto unlock_out;
 		}
 
-		if (dn.data_blkaddr == NEW_ADDR &&
-				flag != F2FS_GET_BLOCK_FIEMAP)
-			goto put_out;
-
 		end_offset = ADDRS_PER_PAGE(dn.node_page, F2FS_I(inode));
 	}
 
 	if (maxblocks > map->m_len) {
 		block_t blkaddr = datablock_addr(dn.node_page, dn.ofs_in_node);
-		if (blkaddr == NULL_ADDR && create) {
-			err = __allocate_data_block(&dn);
-			if (err)
-				goto sync_out;
-			allocated = true;
-			map->m_flags |= F2FS_MAP_NEW;
-			blkaddr = dn.data_blkaddr;
+
+		if (blkaddr == NEW_ADDR || blkaddr == NULL_ADDR) {
+			if (create) {
+				err = __allocate_data_block(&dn);
+				if (err)
+					goto sync_out;
+				allocated = true;
+				map->m_flags |= F2FS_MAP_NEW;
+				blkaddr = dn.data_blkaddr;
+			} else {
+				/*
+				 * we only merge preallocated unwritten blocks
+				 * for fiemap.
+				 */
+				if (flag != F2FS_GET_BLOCK_FIEMAP ||
+						blkaddr != NEW_ADDR)
+					goto sync_out;
+			}
 		}
+
 		/* Give more consecutive addresses for the readahead */
 		if ((map->m_pblk != NEW_ADDR &&
 				blkaddr == (map->m_pblk + ofs)) ||

commit 9edcdabf36422d15d01db73b0fa5487e418beff6
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Fri Sep 11 14:43:52 2015 +0800

    f2fs: fix overflow of size calculation
    
    We have potential overflow issue when calculating size of object, when
    we left shift index with PAGE_CACHE_SHIFT bits, if type of index has only
    32-bits space in 32-bit architecture, left shifting will incur overflow,
    i.e:
    
    pgoff_t index =  0xFFFFFFFF;
    loff_t size = index << PAGE_CACHE_SHIFT;
    size: 0xFFFFF000
    
    So we should cast index with 64-bits type to avoid this issue.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index a82abe921b89..681373e1da92 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -447,9 +447,9 @@ struct page *get_new_data_page(struct inode *inode,
 		lock_page(page);
 	}
 got_it:
-	if (new_i_size &&
-		i_size_read(inode) < ((index + 1) << PAGE_CACHE_SHIFT)) {
-		i_size_write(inode, ((index + 1) << PAGE_CACHE_SHIFT));
+	if (new_i_size && i_size_read(inode) <
+				((loff_t)(index + 1) << PAGE_CACHE_SHIFT)) {
+		i_size_write(inode, ((loff_t)(index + 1) << PAGE_CACHE_SHIFT));
 		/* Only the directory inode sets new_i_size */
 		set_inode_flag(F2FS_I(inode), FI_UPDATE_DIR);
 	}
@@ -489,8 +489,9 @@ static int __allocate_data_block(struct dnode_of_data *dn)
 	/* update i_size */
 	fofs = start_bidx_of_node(ofs_of_node(dn->node_page), fi) +
 							dn->ofs_in_node;
-	if (i_size_read(dn->inode) < ((fofs + 1) << PAGE_CACHE_SHIFT))
-		i_size_write(dn->inode, ((fofs + 1) << PAGE_CACHE_SHIFT));
+	if (i_size_read(dn->inode) < ((loff_t)(fofs + 1) << PAGE_CACHE_SHIFT))
+		i_size_write(dn->inode,
+				((loff_t)(fofs + 1) << PAGE_CACHE_SHIFT));
 
 	/* direct IO doesn't use extent cache to maximize the performance */
 	f2fs_drop_largest_extent(dn->inode, fofs);

commit 4c12ab7e5e2e892fa94df500f96001837918a281
Merge: 9cbf22b37ae0 01a5ad827a36
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Sep 3 13:10:22 2015 -0700

    Merge tag 'for-f2fs-4.3' of git://git.kernel.org/pub/scm/linux/kernel/git/jaegeuk/f2fs
    
    Pull f2fs updates from Jaegeuk Kim:
     "The major work includes fixing and enhancing the existing extent_cache
      feature, which has been well settling down so far and now it becomes a
      default mount option accordingly.
    
      Also, this version newly registers a f2fs memory shrinker to reclaim
      several objects consumed by a couple of data structures in order to
      avoid memory pressures.
    
      Another new feature is to add ioctl(F2FS_GARBAGE_COLLECT) which
      triggers a cleaning job explicitly by users.
    
      Most of the other patches are to fix bugs occurred in the corner cases
      across the whole code area"
    
    * tag 'for-f2fs-4.3' of git://git.kernel.org/pub/scm/linux/kernel/git/jaegeuk/f2fs: (85 commits)
      f2fs: upset segment_info repair
      f2fs: avoid accessing NULL pointer in f2fs_drop_largest_extent
      f2fs: update extent tree in batches
      f2fs: fix to release inode correctly
      f2fs: handle f2fs_truncate error correctly
      f2fs: avoid unneeded initializing when converting inline dentry
      f2fs: atomically set inode->i_flags
      f2fs: fix wrong pointer access during try_to_free_nids
      f2fs: use __GFP_NOFAIL to avoid infinite loop
      f2fs: lookup neighbor extent nodes for merging later
      f2fs: split __insert_extent_tree_ret for readability
      f2fs: kill dead code in __insert_extent_tree
      f2fs: adjust showing of extent cache stat
      f2fs: add largest/cached stat in extent cache
      f2fs: fix incorrect mapping for bmap
      f2fs: add annotation for space utilization of regular/inline dentry
      f2fs: fix to update cached_en of extent tree properly
      f2fs: fix typo
      f2fs: check the node block address of newly allocated nid
      f2fs: go out for insert_inode_locked failure
      ...

commit 1081230b748de8f03f37f80c53dfa89feda9b8de
Merge: df910390e2db 2ca495ac27d2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Sep 2 13:10:25 2015 -0700

    Merge branch 'for-4.3/core' of git://git.kernel.dk/linux-block
    
    Pull core block updates from Jens Axboe:
     "This first core part of the block IO changes contains:
    
       - Cleanup of the bio IO error signaling from Christoph.  We used to
         rely on the uptodate bit and passing around of an error, now we
         store the error in the bio itself.
    
       - Improvement of the above from myself, by shrinking the bio size
         down again to fit in two cachelines on x86-64.
    
       - Revert of the max_hw_sectors cap removal from a revision again,
         from Jeff Moyer.  This caused performance regressions in various
         tests.  Reinstate the limit, bump it to a more reasonable size
         instead.
    
       - Make /sys/block/<dev>/queue/discard_max_bytes writeable, by me.
         Most devices have huge trim limits, which can cause nasty latencies
         when deleting files.  Enable the admin to configure the size down.
         We will look into having a more sane default instead of UINT_MAX
         sectors.
    
       - Improvement of the SGP gaps logic from Keith Busch.
    
       - Enable the block core to handle arbitrarily sized bios, which
         enables a nice simplification of bio_add_page() (which is an IO hot
         path).  From Kent.
    
       - Improvements to the partition io stats accounting, making it
         faster.  From Ming Lei.
    
       - Also from Ming Lei, a basic fixup for overflow of the sysfs pending
         file in blk-mq, as well as a fix for a blk-mq timeout race
         condition.
    
       - Ming Lin has been carrying Kents above mentioned patches forward
         for a while, and testing them.  Ming also did a few fixes around
         that.
    
       - Sasha Levin found and fixed a use-after-free problem introduced by
         the bio->bi_error changes from Christoph.
    
       - Small blk cgroup cleanup from Viresh Kumar"
    
    * 'for-4.3/core' of git://git.kernel.dk/linux-block: (26 commits)
      blk: Fix bio_io_vec index when checking bvec gaps
      block: Replace SG_GAPS with new queue limits mask
      block: bump BLK_DEF_MAX_SECTORS to 2560
      Revert "block: remove artifical max_hw_sectors cap"
      blk-mq: fix race between timeout and freeing request
      blk-mq: fix buffer overflow when reading sysfs file of 'pending'
      Documentation: update notes in biovecs about arbitrarily sized bios
      block: remove bio_get_nr_vecs()
      fs: use helper bio_add_page() instead of open coding on bi_io_vec
      block: kill merge_bvec_fn() completely
      md/raid5: get rid of bio_fits_rdev()
      md/raid5: split bio for chunk_aligned_read
      block: remove split code in blkdev_issue_{discard,write_same}
      btrfs: remove bio splitting and merge_bvec_fn() calls
      bcache: remove driver private bio splitting code
      block: simplify bio_add_page()
      block: make generic_make_request handle arbitrarily sized bios
      blk-cgroup: Drop unlikely before IS_ERR(_OR_NULL)
      block: don't access bio->bi_error after bio_put()
      block: shrink struct bio down to 2 cache lines again
      ...

commit e2b4e2bc8865e03eecd49caa9713a2402a96bba9
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Wed Aug 19 19:11:19 2015 +0800

    f2fs: fix incorrect mapping for bmap
    
    The test step is like below:
    1. touch file
    2. truncate -s $((1024*1024)) file
    3. fallocate -o 0 -l $((1024*1024)) file
    4. fibmap.f2fs file
    
    Our result of fibmap.f2fs showed below is not correct:
    
    file_pos   start_blk     end_blk        blks
           0    -937166132    -937166132           1
        4096    -937166132    -937166132           1
        8192    -937166132    -937166132           1
       12288    -937166132    -937166132           1
       16384    -937166132    -937166132           1
       20480    -937166132    -937166132           1
    ...
     1040384    -937166132    -937166132           1
     1044480    -937166132    -937166132           1
    
    This is because f2fs_map_blocks will return with no error when meeting
    a hole or preallocated block, the caller __get_data_block will map the
    uninitialized variable value to bh->b_blocknr.
    
    Unfortunately generic_block_bmap will neither check the return value of
    get_data() nor check mapping info of buffer_head, result in returning
    the random block address.
    
    After fixing the issue, our result shows correctly:
    
    file_pos   start_blk     end_blk        blks
           0           0           0         256
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 726e58b76295..73713bbd4646 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -561,7 +561,7 @@ static void __allocate_data_blocks(struct inode *inode, loff_t offset,
  *     c. give the block addresses to blockdev
  */
 static int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
-			int create, bool fiemap)
+						int create, int flag)
 {
 	unsigned int maxblocks = map->m_len;
 	struct dnode_of_data dn;
@@ -595,8 +595,19 @@ static int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 			err = 0;
 		goto unlock_out;
 	}
-	if (dn.data_blkaddr == NEW_ADDR && !fiemap)
-		goto put_out;
+	if (dn.data_blkaddr == NEW_ADDR) {
+		if (flag == F2FS_GET_BLOCK_BMAP) {
+			err = -ENOENT;
+			goto put_out;
+		} else if (flag == F2FS_GET_BLOCK_READ ||
+				flag == F2FS_GET_BLOCK_DIO) {
+			goto put_out;
+		}
+		/*
+		 * if it is in fiemap call path (flag = F2FS_GET_BLOCK_FIEMAP),
+		 * mark it as mapped and unwritten block.
+		 */
+	}
 
 	if (dn.data_blkaddr != NULL_ADDR) {
 		map->m_flags = F2FS_MAP_MAPPED;
@@ -611,6 +622,8 @@ static int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 		map->m_flags = F2FS_MAP_NEW | F2FS_MAP_MAPPED;
 		map->m_pblk = dn.data_blkaddr;
 	} else {
+		if (flag == F2FS_GET_BLOCK_BMAP)
+			err = -ENOENT;
 		goto put_out;
 	}
 
@@ -633,7 +646,9 @@ static int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 				err = 0;
 			goto unlock_out;
 		}
-		if (dn.data_blkaddr == NEW_ADDR && !fiemap)
+
+		if (dn.data_blkaddr == NEW_ADDR &&
+				flag != F2FS_GET_BLOCK_FIEMAP)
 			goto put_out;
 
 		end_offset = ADDRS_PER_PAGE(dn.node_page, F2FS_I(inode));
@@ -675,7 +690,7 @@ static int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 }
 
 static int __get_data_block(struct inode *inode, sector_t iblock,
-			struct buffer_head *bh, int create, bool fiemap)
+			struct buffer_head *bh, int create, int flag)
 {
 	struct f2fs_map_blocks map;
 	int ret;
@@ -683,7 +698,7 @@ static int __get_data_block(struct inode *inode, sector_t iblock,
 	map.m_lblk = iblock;
 	map.m_len = bh->b_size >> inode->i_blkbits;
 
-	ret = f2fs_map_blocks(inode, &map, create, fiemap);
+	ret = f2fs_map_blocks(inode, &map, create, flag);
 	if (!ret) {
 		map_bh(bh, inode->i_sb, map.m_pblk);
 		bh->b_state = (bh->b_state & ~F2FS_MAP_FLAGS) | map.m_flags;
@@ -693,15 +708,23 @@ static int __get_data_block(struct inode *inode, sector_t iblock,
 }
 
 static int get_data_block(struct inode *inode, sector_t iblock,
+			struct buffer_head *bh_result, int create, int flag)
+{
+	return __get_data_block(inode, iblock, bh_result, create, flag);
+}
+
+static int get_data_block_dio(struct inode *inode, sector_t iblock,
 			struct buffer_head *bh_result, int create)
 {
-	return __get_data_block(inode, iblock, bh_result, create, false);
+	return __get_data_block(inode, iblock, bh_result, create,
+						F2FS_GET_BLOCK_DIO);
 }
 
-static int get_data_block_fiemap(struct inode *inode, sector_t iblock,
+static int get_data_block_bmap(struct inode *inode, sector_t iblock,
 			struct buffer_head *bh_result, int create)
 {
-	return __get_data_block(inode, iblock, bh_result, create, true);
+	return __get_data_block(inode, iblock, bh_result, create,
+						F2FS_GET_BLOCK_BMAP);
 }
 
 static inline sector_t logical_to_blk(struct inode *inode, loff_t offset)
@@ -745,7 +768,8 @@ int f2fs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,
 	memset(&map_bh, 0, sizeof(struct buffer_head));
 	map_bh.b_size = len;
 
-	ret = get_data_block_fiemap(inode, start_blk, &map_bh, 0);
+	ret = get_data_block(inode, start_blk, &map_bh, 0,
+					F2FS_GET_BLOCK_FIEMAP);
 	if (ret)
 		goto out;
 
@@ -1530,7 +1554,7 @@ static ssize_t f2fs_direct_IO(struct kiocb *iocb, struct iov_iter *iter,
 	if (iov_iter_rw(iter) == WRITE)
 		__allocate_data_blocks(inode, offset, count);
 
-	err = blockdev_direct_IO(iocb, inode, iter, offset, get_data_block);
+	err = blockdev_direct_IO(iocb, inode, iter, offset, get_data_block_dio);
 	if (err < 0 && iov_iter_rw(iter) == WRITE)
 		f2fs_write_failed(mapping, offset + count);
 
@@ -1618,7 +1642,7 @@ static sector_t f2fs_bmap(struct address_space *mapping, sector_t block)
 		if (err)
 			return err;
 	}
-	return generic_block_bmap(mapping, block, get_data_block);
+	return generic_block_bmap(mapping, block, get_data_block_bmap);
 }
 
 const struct address_space_operations f2fs_dblock_aops = {

commit 740432f835608d11b5386321ab5aa8f61e07fb27
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Aug 14 11:43:56 2015 -0700

    f2fs: handle failed bio allocation
    
    As the below comment of bio_alloc_bioset, f2fs can allocate multiple bios at the
    same time. So, we can't guarantee that bio is allocated all the time.
    
    "
     *   When @bs is not NULL, if %__GFP_WAIT is set then bio_alloc will always be
     *   able to allocate a bio. This is due to the mempool guarantees. To make this
     *   work, callers must never allocate more than 1 bio at a time from this pool.
     *   Callers that need to allocate more than 1 bio must always submit the
     *   previously allocated bio for IO before attempting to allocate a new one.
     *   Failure to do so can cause deadlocks under memory pressure.
    "
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index cad9ebe45692..726e58b76295 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -90,8 +90,7 @@ static struct bio *__bio_alloc(struct f2fs_sb_info *sbi, block_t blk_addr,
 {
 	struct bio *bio;
 
-	/* No failure on bio allocation */
-	bio = bio_alloc(GFP_NOIO, npages);
+	bio = f2fs_bio_alloc(npages);
 
 	bio->bi_bdev = sbi->sb->s_bdev;
 	bio->bi_iter.bi_sector = SECTOR_FROM_BLOCK(blk_addr);

commit b54ffb73cadcdcff9cc1ae0e11f502407e3e2e4c
Author: Kent Overstreet <kent.overstreet@gmail.com>
Date:   Tue May 19 14:31:01 2015 +0200

    block: remove bio_get_nr_vecs()
    
    We can always fill up the bio now, no need to estimate the possible
    size based on queue parameters.
    
    Acked-by: Steven Whitehouse <swhiteho@redhat.com>
    Signed-off-by: Kent Overstreet <kent.overstreet@gmail.com>
    [hch: rebased and wrote a changelog]
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Ming Lin <ming.l@ssi.samsung.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 8f0baa7ffb50..b478accb24d9 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1552,7 +1552,7 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 			}
 
 			bio = bio_alloc(GFP_KERNEL,
-				min_t(int, nr_pages, bio_get_nr_vecs(bdev)));
+				min_t(int, nr_pages, BIO_MAX_PAGES));
 			if (!bio) {
 				if (ctx)
 					f2fs_release_crypto_ctx(ctx);

commit decd36b6c43a1051bab97571cf4c0ec8450268b0
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Fri Aug 7 18:42:09 2015 +0800

    f2fs: remove inmem radix tree
    
    Previously, we use radix tree to index all registered page entries for
    atomic file, but now we only use radix tree to see whether current page
    is indexed or not, since the other user of radix tree is gone in commit
    042b7816aaeb ("f2fs: remove unnecessary call to invalidate inmemory pages").
    
    So in this patch, we try to use one more efficient way:
    Introducing a macro ATOMIC_WRITTEN_PAGE, and setting it as page private
    value to indicate page indexing status. By using this way, we can save
    memory and lookup time.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 7ea8eda8f137..cad9ebe45692 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1558,6 +1558,11 @@ void f2fs_invalidate_page(struct page *page, unsigned int offset,
 		else
 			inode_dec_dirty_pages(inode);
 	}
+
+	/* This is atomic written page, keep Private */
+	if (IS_ATOMIC_WRITTEN_PAGE(page))
+		return;
+
 	ClearPagePrivate(page);
 }
 
@@ -1567,6 +1572,10 @@ int f2fs_release_page(struct page *page, gfp_t wait)
 	if (PageDirty(page))
 		return 0;
 
+	/* This is atomic written page, keep Private */
+	if (IS_ATOMIC_WRITTEN_PAGE(page))
+		return 0;
+
 	ClearPagePrivate(page);
 	return 1;
 }
@@ -1581,8 +1590,15 @@ static int f2fs_set_data_page_dirty(struct page *page)
 	SetPageUptodate(page);
 
 	if (f2fs_is_atomic_file(inode)) {
-		register_inmem_page(inode, page);
-		return 1;
+		if (!IS_ATOMIC_WRITTEN_PAGE(page)) {
+			register_inmem_page(inode, page);
+			return 1;
+		}
+		/*
+		 * Previously, this page has been registered, we just
+		 * return here.
+		 */
+		return 0;
 	}
 
 	if (!PageDirty(page)) {

commit c15e8599ffe1b4f866691424d07037c467c23a2f
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Fri Aug 7 18:39:32 2015 +0800

    f2fs: report EINVAL for unalignment direct IO
    
    We run ltp testcase with f2fs and obtain a TFAIL in diotest4, the result in
    detail is as fallow:
    
    dio04
    
    <<<test_start>>>
    tag=dio04 stime=1432278894
    cmdline="diotest4"
    contacts=""
    analysis=exit
    <<<test_output>>>
    diotest4    1  TPASS  :  Negative Offset
    diotest4    2  TPASS  :  removed
    diotest4    3  TFAIL  :  diotest4.c:129: write allows odd count.returns 1: Success
    diotest4    4  TFAIL  :  diotest4.c:183: Odd count of read and write
    diotest4    5  TPASS  :  Read beyond the file size
    ......
    
    the result of ext4 with same environment:
    
    dio04
    
    <<<test_start>>>
    tag=dio04 stime=1432259643
    cmdline="diotest4"
    contacts=""
    analysis=exit
    <<<test_output>>>
    diotest4    1  TPASS  :  Negative Offset
    diotest4    2  TPASS  :  removed
    diotest4    3  TPASS  :  Odd count of read and write
    diotest4    4  TPASS  :  Read beyond the file size
    ......
    
    The reason is that when triggering DIO in f2fs, we will return zero value
    in ->direct_IO if writer's buffer offset, file offset and transfer size is
    not alignment to block size of filesystem, resulting in falling back into
    buffered write instead of returning -EINVAL.
    
    This patch fixes that problem by returning correct error number for above
    case, and removing the judgement condition in check_direct_IO to make sure
    the verification will be enabled for direct reader too.
    
    Besides, Jaegeuk Kim pointed out that there is expectional cases we should
    always make direct-io falling back into buffered write, such as dio in
    encrypted file.
    
    Signed-off-by: Yunlei He <heyunlei@huawei.com>
    [Chao Yu make small change and add detail description in commit message]
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 4fabdd47490a..7ea8eda8f137 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1494,9 +1494,6 @@ static int check_direct_IO(struct inode *inode, struct iov_iter *iter,
 {
 	unsigned blocksize_mask = inode->i_sb->s_blocksize - 1;
 
-	if (iov_iter_rw(iter) == READ)
-		return 0;
-
 	if (offset & blocksize_mask)
 		return -EINVAL;
 
@@ -1525,8 +1522,9 @@ static ssize_t f2fs_direct_IO(struct kiocb *iocb, struct iov_iter *iter,
 	if (f2fs_encrypted_inode(inode) && S_ISREG(inode->i_mode))
 		return 0;
 
-	if (check_direct_IO(inode, iter, offset))
-		return 0;
+	err = check_direct_IO(inode, iter, offset);
+	if (err)
+		return err;
 
 	trace_f2fs_direct_IO_enter(inode, offset, count, iov_iter_rw(iter));
 

commit 759af1c9c16fec5323111b799ce25a3d8864df7e
Author: Fan Li <fanofcode.li@samsung.com>
Date:   Wed Aug 5 15:52:16 2015 +0800

    f2fs: use extent cache to optimize f2fs_reserve_block
    
    In some cases, we only need the block address when we call
    f2fs_reserve_block,
    other fields of struct dnode_of_data aren't necessary.
    We can try extent cache first for such cases in order to speed up the
    process.
    
    Signed-off-by: Fan li <fanofcode.li@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index f8f93db437ce..4fabdd47490a 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -263,6 +263,19 @@ int f2fs_reserve_block(struct dnode_of_data *dn, pgoff_t index)
 	return err;
 }
 
+int f2fs_get_block(struct dnode_of_data *dn, pgoff_t index)
+{
+	struct extent_info ei;
+	struct inode *inode = dn->inode;
+
+	if (f2fs_lookup_extent_cache(inode, index, &ei)) {
+		dn->data_blkaddr = ei.blk + index - ei.fofs;
+		return 0;
+	}
+
+	return f2fs_reserve_block(dn, index);
+}
+
 struct page *get_read_data_page(struct inode *inode, pgoff_t index, int rw)
 {
 	struct address_space *mapping = inode->i_mapping;
@@ -1383,7 +1396,8 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 		if (err)
 			goto put_fail;
 	}
-	err = f2fs_reserve_block(&dn, index);
+
+	err = f2fs_get_block(&dn, index);
 	if (err)
 		goto put_fail;
 put_next:

commit 470f00e9686f0b338a457568229fe7b7d44b8e6a
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Tue Jul 14 18:14:06 2015 +0800

    f2fs: fix to release inode page correctly
    
    In following call path, we will pass a locked and referenced ipage
    pointer to get_new_data_page:
     - init_inode_metadata
      - make_empty_dir
       - get_new_data_page
    
    There are two exit paths in get_new_data_page when error occurs:
    1) grab_cache_page fails, ipage will not be released;
    2) f2fs_reserve_block fails, ipage will be released in callee.
    
    So, it's not consistent for error handling in get_new_data_page.
    
    For f2fs_reserve_block, it's not very easy to change the rule
    of error handling, since it's already complicated.
    
    Here we deside to choose an easy way to fix this issue:
    If any error occur in get_new_data_page, we will ensure releasing
    ipage in this function.
    
    The same issue is in f2fs_convert_inline_dir, fix that too.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 2692848e7f75..f8f93db437ce 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -388,7 +388,8 @@ struct page *get_lock_data_page(struct inode *inode, pgoff_t index)
  *
  * Also, caller should grab and release a rwsem by calling f2fs_lock_op() and
  * f2fs_unlock_op().
- * Note that, ipage is set only by make_empty_dir.
+ * Note that, ipage is set only by make_empty_dir, and if any error occur,
+ * ipage should be released by this function.
  */
 struct page *get_new_data_page(struct inode *inode,
 		struct page *ipage, pgoff_t index, bool new_i_size)
@@ -399,8 +400,14 @@ struct page *get_new_data_page(struct inode *inode,
 	int err;
 repeat:
 	page = grab_cache_page(mapping, index);
-	if (!page)
+	if (!page) {
+		/*
+		 * before exiting, we should make sure ipage will be released
+		 * if any error occur.
+		 */
+		f2fs_put_page(ipage, 1);
 		return ERR_PTR(-ENOMEM);
+	}
 
 	set_new_dnode(&dn, inode, ipage, NULL, 0);
 	err = f2fs_reserve_block(&dn, index);

commit 5768dcdd7f7675f9540e648428c8a1cd7208a0fe
Author: Fan Li <fanofcode.li@samsung.com>
Date:   Tue Aug 4 13:27:51 2015 +0800

    f2fs: change the timing of f2fs_wait_on_page_writeback
    
    some backing devices need pages to be stable during writeback. It doesn't
    matter if
    the page is completely overwritten or already uptodate, it needs to wait
    before write.
    
    Signed-off-by: Fan li <fanofcode.li@samsung.com>
    Reviewed-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index e4081fc91012..2692848e7f75 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1383,13 +1383,13 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 	f2fs_put_dnode(&dn);
 	f2fs_unlock_op(sbi);
 
+	f2fs_wait_on_page_writeback(page, DATA);
+
 	if (len == PAGE_CACHE_SIZE)
 		goto out_update;
 	if (PageUptodate(page))
 		goto out_clear;
 
-	f2fs_wait_on_page_writeback(page, DATA);
-
 	if ((pos & PAGE_CACHE_MASK) >= i_size_read(inode)) {
 		unsigned start = pos & (PAGE_CACHE_SIZE - 1);
 		unsigned end = start + len;

commit 6a2905443cf27f9c14889428f14fccfb98ed97f4
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Fri Jul 17 18:02:39 2015 +0800

    f2fs: skip writing in ->writepages when no dirty pages exist
    
    When flushing comes from background, if there is no dirty page in the
    mapping of inode, we'd better to skip seeking dirty page from mapping
    for writebacking.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 801b0b0b08f4..e4081fc91012 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1275,6 +1275,10 @@ static int f2fs_write_data_pages(struct address_space *mapping,
 	if (!mapping->a_ops->writepage)
 		return 0;
 
+	/* skip writing if there is no dirty page in this inode */
+	if (!get_dirty_pages(inode) && wbc->sync_mode == WB_SYNC_NONE)
+		return 0;
+
 	if (S_ISDIR(inode->i_mode) && wbc->sync_mode == WB_SYNC_NONE &&
 			get_dirty_pages(inode) < nr_pages_to_skip(sbi, DATA) &&
 			available_free_memory(sbi, DIRTY_DENTS))

commit 737f18992ee81cab897336e84c5c7f4e179dfd61
Author: Tiezhu Yang <kernelpatch@126.com>
Date:   Fri Jul 17 12:56:00 2015 +0800

    f2fs: optimize f2fs_write_cache_pages
    
    The if statement "goto continue_unlock" is exactly the same when
    each if condition is true that is depended on the value of both
    "step" and "is_cold_data(page)" are 0 or 1. That means when the
    value of "step" equals to "is_cold_data(page)", the if condition
    is true and the if statement "goto continue_unlock" appears only
    once, so it can be optimized to reduce the duplicated code.
    
    Signed-off-by: Tiezhu Yang <kernelpatch@126.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 7f51296fbbf6..801b0b0b08f4 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1207,9 +1207,7 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 				goto continue_unlock;
 			}
 
-			if (step == 0 && !is_cold_data(page))
-				goto continue_unlock;
-			if (step == 1 && is_cold_data(page))
+			if (step == is_cold_data(page))
 				goto continue_unlock;
 
 			if (PageWriteback(page)) {

commit 86531d6b84bc096d5d9dbc23333df0ab8d347763
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed Jul 15 13:08:21 2015 -0700

    f2fs: callers take care of the page from bio error
    
    This patch changes for a caller to handle the page after its bio gets an error.
    
    Reviewed-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index e58562e70da0..7f51296fbbf6 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -156,7 +156,6 @@ int f2fs_submit_page_bio(struct f2fs_io_info *fio)
 
 	if (bio_add_page(bio, page, PAGE_CACHE_SIZE, 0) < PAGE_CACHE_SIZE) {
 		bio_put(bio);
-		f2fs_put_page(page, 1);
 		return -EFAULT;
 	}
 
@@ -292,15 +291,13 @@ struct page *get_read_data_page(struct inode *inode, pgoff_t index, int rw)
 
 	set_new_dnode(&dn, inode, NULL, NULL, 0);
 	err = get_dnode_of_data(&dn, index, LOOKUP_NODE);
-	if (err) {
-		f2fs_put_page(page, 1);
-		return ERR_PTR(err);
-	}
+	if (err)
+		goto put_err;
 	f2fs_put_dnode(&dn);
 
 	if (unlikely(dn.data_blkaddr == NULL_ADDR)) {
-		f2fs_put_page(page, 1);
-		return ERR_PTR(-ENOENT);
+		err = -ENOENT;
+		goto put_err;
 	}
 got_it:
 	if (PageUptodate(page)) {
@@ -325,8 +322,12 @@ struct page *get_read_data_page(struct inode *inode, pgoff_t index, int rw)
 	fio.page = page;
 	err = f2fs_submit_page_bio(&fio);
 	if (err)
-		return ERR_PTR(err);
+		goto put_err;
 	return page;
+
+put_err:
+	f2fs_put_page(page, 1);
+	return ERR_PTR(err);
 }
 
 struct page *find_data_page(struct inode *inode, pgoff_t index)
@@ -1322,7 +1323,8 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 {
 	struct inode *inode = mapping->host;
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	struct page *page, *ipage;
+	struct page *page = NULL;
+	struct page *ipage;
 	pgoff_t index = ((unsigned long long) pos) >> PAGE_CACHE_SHIFT;
 	struct dnode_of_data dn;
 	int err = 0;
@@ -1412,7 +1414,6 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 
 		lock_page(page);
 		if (unlikely(!PageUptodate(page))) {
-			f2fs_put_page(page, 1);
 			err = -EIO;
 			goto fail;
 		}
@@ -1424,10 +1425,8 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 		/* avoid symlink page */
 		if (f2fs_encrypted_inode(inode) && S_ISREG(inode->i_mode)) {
 			err = f2fs_decrypt_one(inode, page);
-			if (err) {
-				f2fs_put_page(page, 1);
+			if (err)
 				goto fail;
-			}
 		}
 	}
 out_update:
@@ -1440,8 +1439,8 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 	f2fs_put_dnode(&dn);
 unlock_fail:
 	f2fs_unlock_op(sbi);
-	f2fs_put_page(page, 1);
 fail:
+	f2fs_put_page(page, 1);
 	f2fs_write_failed(mapping, pos + len);
 	return err;
 }

commit 8f46dcaea8d9d1552f4071f1ddeeca4427c1d83a
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Tue Jul 14 18:56:10 2015 +0800

    f2fs: expose f2fs_write_cache_pages
    
    If there are gced dirty pages and normal dirty pages in the mapping
    of one inode, we might writeback them alternately with discontinuous
    block address, resulting in low performance.
    
    This patch introduces f2fs_write_cache_pages with codes copied from
    write_cache_pages in mm/page-writeback.c.
    
    In this function, we refactor flow with two steps:
    1) writeback all cold type pages.
    2) writeback all non-cold type pages.
    
    By using this method, f2fs will writeback dirty pages with the same
    temperature in bunch mode, it makes writeouted block being with
    more continuous address, so they can be merged as much as possible
    in f2fs bio cache, and also it will reduce the chance of submiting
    small IO from block layer.
    
    Test environment: 8g nokia sd card (very old sd card, but it shows
    better effect when testing with this patch, and with a 32g kingston
    sd card, I didn't see much more improvement).
    
    Test step:
    1. touch testfile;
    2. truncate -s 512K testfile;
    3. write all pages with odd index;
    4. trigger gc by ioctl;
    5. write all pages with even index;
    6. time fsync testfile.
    
    before:
    real    0m0.402s
    user    0m0.000s
    sys     0m0.000s
    
    after:
    real    0m0.143s
    user    0m0.004s
    sys     0m0.004s
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index ef30b59756c6..e58562e70da0 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -14,6 +14,7 @@
 #include <linux/mpage.h>
 #include <linux/writeback.h>
 #include <linux/backing-dev.h>
+#include <linux/pagevec.h>
 #include <linux/blkdev.h>
 #include <linux/bio.h>
 #include <linux/prefetch.h>
@@ -1127,6 +1128,139 @@ static int __f2fs_writepage(struct page *page, struct writeback_control *wbc,
 	return ret;
 }
 
+/*
+ * This function was copied from write_cche_pages from mm/page-writeback.c.
+ * The major change is making write step of cold data page separately from
+ * warm/hot data page.
+ */
+static int f2fs_write_cache_pages(struct address_space *mapping,
+			struct writeback_control *wbc, writepage_t writepage,
+			void *data)
+{
+	int ret = 0;
+	int done = 0;
+	struct pagevec pvec;
+	int nr_pages;
+	pgoff_t uninitialized_var(writeback_index);
+	pgoff_t index;
+	pgoff_t end;		/* Inclusive */
+	pgoff_t done_index;
+	int cycled;
+	int range_whole = 0;
+	int tag;
+	int step = 0;
+
+	pagevec_init(&pvec, 0);
+next:
+	if (wbc->range_cyclic) {
+		writeback_index = mapping->writeback_index; /* prev offset */
+		index = writeback_index;
+		if (index == 0)
+			cycled = 1;
+		else
+			cycled = 0;
+		end = -1;
+	} else {
+		index = wbc->range_start >> PAGE_CACHE_SHIFT;
+		end = wbc->range_end >> PAGE_CACHE_SHIFT;
+		if (wbc->range_start == 0 && wbc->range_end == LLONG_MAX)
+			range_whole = 1;
+		cycled = 1; /* ignore range_cyclic tests */
+	}
+	if (wbc->sync_mode == WB_SYNC_ALL || wbc->tagged_writepages)
+		tag = PAGECACHE_TAG_TOWRITE;
+	else
+		tag = PAGECACHE_TAG_DIRTY;
+retry:
+	if (wbc->sync_mode == WB_SYNC_ALL || wbc->tagged_writepages)
+		tag_pages_for_writeback(mapping, index, end);
+	done_index = index;
+	while (!done && (index <= end)) {
+		int i;
+
+		nr_pages = pagevec_lookup_tag(&pvec, mapping, &index, tag,
+			      min(end - index, (pgoff_t)PAGEVEC_SIZE - 1) + 1);
+		if (nr_pages == 0)
+			break;
+
+		for (i = 0; i < nr_pages; i++) {
+			struct page *page = pvec.pages[i];
+
+			if (page->index > end) {
+				done = 1;
+				break;
+			}
+
+			done_index = page->index;
+
+			lock_page(page);
+
+			if (unlikely(page->mapping != mapping)) {
+continue_unlock:
+				unlock_page(page);
+				continue;
+			}
+
+			if (!PageDirty(page)) {
+				/* someone wrote it for us */
+				goto continue_unlock;
+			}
+
+			if (step == 0 && !is_cold_data(page))
+				goto continue_unlock;
+			if (step == 1 && is_cold_data(page))
+				goto continue_unlock;
+
+			if (PageWriteback(page)) {
+				if (wbc->sync_mode != WB_SYNC_NONE)
+					f2fs_wait_on_page_writeback(page, DATA);
+				else
+					goto continue_unlock;
+			}
+
+			BUG_ON(PageWriteback(page));
+			if (!clear_page_dirty_for_io(page))
+				goto continue_unlock;
+
+			ret = (*writepage)(page, wbc, data);
+			if (unlikely(ret)) {
+				if (ret == AOP_WRITEPAGE_ACTIVATE) {
+					unlock_page(page);
+					ret = 0;
+				} else {
+					done_index = page->index + 1;
+					done = 1;
+					break;
+				}
+			}
+
+			if (--wbc->nr_to_write <= 0 &&
+			    wbc->sync_mode == WB_SYNC_NONE) {
+				done = 1;
+				break;
+			}
+		}
+		pagevec_release(&pvec);
+		cond_resched();
+	}
+
+	if (step < 1) {
+		step++;
+		goto next;
+	}
+
+	if (!cycled && !done) {
+		cycled = 1;
+		index = 0;
+		end = writeback_index - 1;
+		goto retry;
+	}
+	if (wbc->range_cyclic || (range_whole && wbc->nr_to_write > 0))
+		mapping->writeback_index = done_index;
+
+	return ret;
+}
+
 static int f2fs_write_data_pages(struct address_space *mapping,
 			    struct writeback_control *wbc)
 {
@@ -1157,7 +1291,7 @@ static int f2fs_write_data_pages(struct address_space *mapping,
 		mutex_lock(&sbi->writepages);
 		locked = true;
 	}
-	ret = write_cache_pages(mapping, wbc, __f2fs_writepage, mapping);
+	ret = f2fs_write_cache_pages(mapping, wbc, __f2fs_writepage, mapping);
 	f2fs_submit_merged_bio(sbi, DATA, WRITE);
 	if (locked)
 		mutex_unlock(&sbi->writepages);

commit a28ef1f5aebe1068fc5fd65c4699c1c3b1e9094b
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Wed Jul 8 17:59:36 2015 +0800

    f2fs: maintain extent cache in separated file
    
    This patch moves extent cache related code from data.c into extent_cache.c
    since extent cache is independent feature, and its codes are not relate to
    others in data.c, it's better for us to maintain them in separated place.
    
    There is no functionality change, but several small coding style fixes
    including:
    * rename __drop_largest_extent to f2fs_drop_largest_extent for exporting;
    * rename misspelled word 'untill' to 'until';
    * remove unneeded 'return' in the end of f2fs_destroy_extent_tree().
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index ce0d5ec8e770..ef30b59756c6 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -26,9 +26,6 @@
 #include "trace.h"
 #include <trace/events/f2fs.h>
 
-static struct kmem_cache *extent_tree_slab;
-static struct kmem_cache *extent_node_slab;
-
 static void f2fs_read_end_io(struct bio *bio, int err)
 {
 	struct bio_vec *bvec;
@@ -266,548 +263,6 @@ int f2fs_reserve_block(struct dnode_of_data *dn, pgoff_t index)
 	return err;
 }
 
-static struct extent_node *__attach_extent_node(struct f2fs_sb_info *sbi,
-				struct extent_tree *et, struct extent_info *ei,
-				struct rb_node *parent, struct rb_node **p)
-{
-	struct extent_node *en;
-
-	en = kmem_cache_alloc(extent_node_slab, GFP_ATOMIC);
-	if (!en)
-		return NULL;
-
-	en->ei = *ei;
-	INIT_LIST_HEAD(&en->list);
-
-	rb_link_node(&en->rb_node, parent, p);
-	rb_insert_color(&en->rb_node, &et->root);
-	et->count++;
-	atomic_inc(&sbi->total_ext_node);
-	return en;
-}
-
-static void __detach_extent_node(struct f2fs_sb_info *sbi,
-				struct extent_tree *et, struct extent_node *en)
-{
-	rb_erase(&en->rb_node, &et->root);
-	et->count--;
-	atomic_dec(&sbi->total_ext_node);
-
-	if (et->cached_en == en)
-		et->cached_en = NULL;
-}
-
-static struct extent_tree *__grab_extent_tree(struct inode *inode)
-{
-	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	struct extent_tree *et;
-	nid_t ino = inode->i_ino;
-
-	down_write(&sbi->extent_tree_lock);
-	et = radix_tree_lookup(&sbi->extent_tree_root, ino);
-	if (!et) {
-		et = f2fs_kmem_cache_alloc(extent_tree_slab, GFP_NOFS);
-		f2fs_radix_tree_insert(&sbi->extent_tree_root, ino, et);
-		memset(et, 0, sizeof(struct extent_tree));
-		et->ino = ino;
-		et->root = RB_ROOT;
-		et->cached_en = NULL;
-		rwlock_init(&et->lock);
-		atomic_set(&et->refcount, 0);
-		et->count = 0;
-		sbi->total_ext_tree++;
-	}
-	atomic_inc(&et->refcount);
-	up_write(&sbi->extent_tree_lock);
-
-	/* never died untill evict_inode */
-	F2FS_I(inode)->extent_tree = et;
-
-	return et;
-}
-
-static struct extent_node *__lookup_extent_tree(struct extent_tree *et,
-							unsigned int fofs)
-{
-	struct rb_node *node = et->root.rb_node;
-	struct extent_node *en;
-
-	if (et->cached_en) {
-		struct extent_info *cei = &et->cached_en->ei;
-
-		if (cei->fofs <= fofs && cei->fofs + cei->len > fofs)
-			return et->cached_en;
-	}
-
-	while (node) {
-		en = rb_entry(node, struct extent_node, rb_node);
-
-		if (fofs < en->ei.fofs)
-			node = node->rb_left;
-		else if (fofs >= en->ei.fofs + en->ei.len)
-			node = node->rb_right;
-		else
-			return en;
-	}
-	return NULL;
-}
-
-static struct extent_node *__try_back_merge(struct f2fs_sb_info *sbi,
-				struct extent_tree *et, struct extent_node *en)
-{
-	struct extent_node *prev;
-	struct rb_node *node;
-
-	node = rb_prev(&en->rb_node);
-	if (!node)
-		return NULL;
-
-	prev = rb_entry(node, struct extent_node, rb_node);
-	if (__is_back_mergeable(&en->ei, &prev->ei)) {
-		en->ei.fofs = prev->ei.fofs;
-		en->ei.blk = prev->ei.blk;
-		en->ei.len += prev->ei.len;
-		__detach_extent_node(sbi, et, prev);
-		return prev;
-	}
-	return NULL;
-}
-
-static struct extent_node *__try_front_merge(struct f2fs_sb_info *sbi,
-				struct extent_tree *et, struct extent_node *en)
-{
-	struct extent_node *next;
-	struct rb_node *node;
-
-	node = rb_next(&en->rb_node);
-	if (!node)
-		return NULL;
-
-	next = rb_entry(node, struct extent_node, rb_node);
-	if (__is_front_mergeable(&en->ei, &next->ei)) {
-		en->ei.len += next->ei.len;
-		__detach_extent_node(sbi, et, next);
-		return next;
-	}
-	return NULL;
-}
-
-static struct extent_node *__insert_extent_tree(struct f2fs_sb_info *sbi,
-				struct extent_tree *et, struct extent_info *ei,
-				struct extent_node **den)
-{
-	struct rb_node **p = &et->root.rb_node;
-	struct rb_node *parent = NULL;
-	struct extent_node *en;
-
-	while (*p) {
-		parent = *p;
-		en = rb_entry(parent, struct extent_node, rb_node);
-
-		if (ei->fofs < en->ei.fofs) {
-			if (__is_front_mergeable(ei, &en->ei)) {
-				f2fs_bug_on(sbi, !den);
-				en->ei.fofs = ei->fofs;
-				en->ei.blk = ei->blk;
-				en->ei.len += ei->len;
-				*den = __try_back_merge(sbi, et, en);
-				goto update_out;
-			}
-			p = &(*p)->rb_left;
-		} else if (ei->fofs >= en->ei.fofs + en->ei.len) {
-			if (__is_back_mergeable(ei, &en->ei)) {
-				f2fs_bug_on(sbi, !den);
-				en->ei.len += ei->len;
-				*den = __try_front_merge(sbi, et, en);
-				goto update_out;
-			}
-			p = &(*p)->rb_right;
-		} else {
-			f2fs_bug_on(sbi, 1);
-		}
-	}
-
-	en = __attach_extent_node(sbi, et, ei, parent, p);
-	if (!en)
-		return NULL;
-update_out:
-	if (en->ei.len > et->largest.len)
-		et->largest = en->ei;
-	et->cached_en = en;
-	return en;
-}
-
-static unsigned int __free_extent_tree(struct f2fs_sb_info *sbi,
-					struct extent_tree *et, bool free_all)
-{
-	struct rb_node *node, *next;
-	struct extent_node *en;
-	unsigned int count = et->count;
-
-	node = rb_first(&et->root);
-	while (node) {
-		next = rb_next(node);
-		en = rb_entry(node, struct extent_node, rb_node);
-
-		if (free_all) {
-			spin_lock(&sbi->extent_lock);
-			if (!list_empty(&en->list))
-				list_del_init(&en->list);
-			spin_unlock(&sbi->extent_lock);
-		}
-
-		if (free_all || list_empty(&en->list)) {
-			__detach_extent_node(sbi, et, en);
-			kmem_cache_free(extent_node_slab, en);
-		}
-		node = next;
-	}
-
-	return count - et->count;
-}
-
-static void __drop_largest_extent(struct inode *inode, pgoff_t fofs)
-{
-	struct extent_info *largest = &F2FS_I(inode)->extent_tree->largest;
-
-	if (largest->fofs <= fofs && largest->fofs + largest->len > fofs)
-		largest->len = 0;
-}
-
-void f2fs_init_extent_tree(struct inode *inode, struct f2fs_extent *i_ext)
-{
-	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	struct extent_tree *et;
-	struct extent_node *en;
-	struct extent_info ei;
-
-	if (!f2fs_may_extent_tree(inode))
-		return;
-
-	et = __grab_extent_tree(inode);
-
-	if (!i_ext || le32_to_cpu(i_ext->len) < F2FS_MIN_EXTENT_LEN)
-		return;
-
-	set_extent_info(&ei, le32_to_cpu(i_ext->fofs),
-		le32_to_cpu(i_ext->blk), le32_to_cpu(i_ext->len));
-
-	write_lock(&et->lock);
-	if (et->count)
-		goto out;
-
-	en = __insert_extent_tree(sbi, et, &ei, NULL);
-	if (en) {
-		spin_lock(&sbi->extent_lock);
-		list_add_tail(&en->list, &sbi->extent_list);
-		spin_unlock(&sbi->extent_lock);
-	}
-out:
-	write_unlock(&et->lock);
-}
-
-static bool f2fs_lookup_extent_tree(struct inode *inode, pgoff_t pgofs,
-							struct extent_info *ei)
-{
-	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	struct extent_tree *et = F2FS_I(inode)->extent_tree;
-	struct extent_node *en;
-	bool ret = false;
-
-	f2fs_bug_on(sbi, !et);
-
-	trace_f2fs_lookup_extent_tree_start(inode, pgofs);
-
-	read_lock(&et->lock);
-
-	if (et->largest.fofs <= pgofs &&
-			et->largest.fofs + et->largest.len > pgofs) {
-		*ei = et->largest;
-		ret = true;
-		stat_inc_read_hit(sbi->sb);
-		goto out;
-	}
-
-	en = __lookup_extent_tree(et, pgofs);
-	if (en) {
-		*ei = en->ei;
-		spin_lock(&sbi->extent_lock);
-		if (!list_empty(&en->list))
-			list_move_tail(&en->list, &sbi->extent_list);
-		et->cached_en = en;
-		spin_unlock(&sbi->extent_lock);
-		ret = true;
-		stat_inc_read_hit(sbi->sb);
-	}
-out:
-	stat_inc_total_hit(sbi->sb);
-	read_unlock(&et->lock);
-
-	trace_f2fs_lookup_extent_tree_end(inode, pgofs, ei);
-	return ret;
-}
-
-/* return true, if on-disk extent should be updated */
-static bool f2fs_update_extent_tree(struct inode *inode, pgoff_t fofs,
-							block_t blkaddr)
-{
-	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	struct extent_tree *et = F2FS_I(inode)->extent_tree;
-	struct extent_node *en = NULL, *en1 = NULL, *en2 = NULL, *en3 = NULL;
-	struct extent_node *den = NULL;
-	struct extent_info ei, dei, prev;
-	unsigned int endofs;
-
-	if (!et)
-		return false;
-
-	trace_f2fs_update_extent_tree(inode, fofs, blkaddr);
-
-	write_lock(&et->lock);
-
-	if (is_inode_flag_set(F2FS_I(inode), FI_NO_EXTENT)) {
-		write_unlock(&et->lock);
-		return false;
-	}
-
-	prev = et->largest;
-	dei.len = 0;
-
-	/* we do not guarantee that the largest extent is cached all the time */
-	__drop_largest_extent(inode, fofs);
-
-	/* 1. lookup and remove existing extent info in cache */
-	en = __lookup_extent_tree(et, fofs);
-	if (!en)
-		goto update_extent;
-
-	dei = en->ei;
-	__detach_extent_node(sbi, et, en);
-
-	/* 2. if extent can be split more, split and insert the left part */
-	if (dei.len > F2FS_MIN_EXTENT_LEN) {
-		/*  insert left part of split extent into cache */
-		if (fofs - dei.fofs >= F2FS_MIN_EXTENT_LEN) {
-			set_extent_info(&ei, dei.fofs, dei.blk,
-							fofs - dei.fofs);
-			en1 = __insert_extent_tree(sbi, et, &ei, NULL);
-		}
-
-		/* insert right part of split extent into cache */
-		endofs = dei.fofs + dei.len - 1;
-		if (endofs - fofs >= F2FS_MIN_EXTENT_LEN) {
-			set_extent_info(&ei, fofs + 1,
-				fofs - dei.fofs + dei.blk + 1, endofs - fofs);
-			en2 = __insert_extent_tree(sbi, et, &ei, NULL);
-		}
-	}
-
-update_extent:
-	/* 3. update extent in extent cache */
-	if (blkaddr) {
-		set_extent_info(&ei, fofs, blkaddr, 1);
-		en3 = __insert_extent_tree(sbi, et, &ei, &den);
-
-		/* give up extent_cache, if split and small updates happen */
-		if (dei.len >= 1 &&
-				prev.len < F2FS_MIN_EXTENT_LEN &&
-				et->largest.len < F2FS_MIN_EXTENT_LEN) {
-			et->largest.len = 0;
-			set_inode_flag(F2FS_I(inode), FI_NO_EXTENT);
-		}
-	}
-
-	/* 4. update in global extent list */
-	spin_lock(&sbi->extent_lock);
-	if (en && !list_empty(&en->list))
-		list_del(&en->list);
-	/*
-	 * en1 and en2 split from en, they will become more and more smaller
-	 * fragments after splitting several times. So if the length is smaller
-	 * than F2FS_MIN_EXTENT_LEN, we will not add them into extent tree.
-	 */
-	if (en1)
-		list_add_tail(&en1->list, &sbi->extent_list);
-	if (en2)
-		list_add_tail(&en2->list, &sbi->extent_list);
-	if (en3) {
-		if (list_empty(&en3->list))
-			list_add_tail(&en3->list, &sbi->extent_list);
-		else
-			list_move_tail(&en3->list, &sbi->extent_list);
-	}
-	if (den && !list_empty(&den->list))
-		list_del(&den->list);
-	spin_unlock(&sbi->extent_lock);
-
-	/* 5. release extent node */
-	if (en)
-		kmem_cache_free(extent_node_slab, en);
-	if (den)
-		kmem_cache_free(extent_node_slab, den);
-
-	if (is_inode_flag_set(F2FS_I(inode), FI_NO_EXTENT))
-		__free_extent_tree(sbi, et, true);
-
-	write_unlock(&et->lock);
-
-	return !__is_extent_same(&prev, &et->largest);
-}
-
-unsigned int f2fs_shrink_extent_tree(struct f2fs_sb_info *sbi, int nr_shrink)
-{
-	struct extent_tree *treevec[EXT_TREE_VEC_SIZE];
-	struct extent_node *en, *tmp;
-	unsigned long ino = F2FS_ROOT_INO(sbi);
-	struct radix_tree_root *root = &sbi->extent_tree_root;
-	unsigned int found;
-	unsigned int node_cnt = 0, tree_cnt = 0;
-	int remained;
-
-	if (!test_opt(sbi, EXTENT_CACHE))
-		return 0;
-
-	if (!down_write_trylock(&sbi->extent_tree_lock))
-		goto out;
-
-	/* 1. remove unreferenced extent tree */
-	while ((found = radix_tree_gang_lookup(root,
-				(void **)treevec, ino, EXT_TREE_VEC_SIZE))) {
-		unsigned i;
-
-		ino = treevec[found - 1]->ino + 1;
-		for (i = 0; i < found; i++) {
-			struct extent_tree *et = treevec[i];
-
-			if (!atomic_read(&et->refcount)) {
-				write_lock(&et->lock);
-				node_cnt += __free_extent_tree(sbi, et, true);
-				write_unlock(&et->lock);
-
-				radix_tree_delete(root, et->ino);
-				kmem_cache_free(extent_tree_slab, et);
-				sbi->total_ext_tree--;
-				tree_cnt++;
-
-				if (node_cnt + tree_cnt >= nr_shrink)
-					goto unlock_out;
-			}
-		}
-	}
-	up_write(&sbi->extent_tree_lock);
-
-	/* 2. remove LRU extent entries */
-	if (!down_write_trylock(&sbi->extent_tree_lock))
-		goto out;
-
-	remained = nr_shrink - (node_cnt + tree_cnt);
-
-	spin_lock(&sbi->extent_lock);
-	list_for_each_entry_safe(en, tmp, &sbi->extent_list, list) {
-		if (!remained--)
-			break;
-		list_del_init(&en->list);
-	}
-	spin_unlock(&sbi->extent_lock);
-
-	while ((found = radix_tree_gang_lookup(root,
-				(void **)treevec, ino, EXT_TREE_VEC_SIZE))) {
-		unsigned i;
-
-		ino = treevec[found - 1]->ino + 1;
-		for (i = 0; i < found; i++) {
-			struct extent_tree *et = treevec[i];
-
-			write_lock(&et->lock);
-			node_cnt += __free_extent_tree(sbi, et, false);
-			write_unlock(&et->lock);
-
-			if (node_cnt + tree_cnt >= nr_shrink)
-				break;
-		}
-	}
-unlock_out:
-	up_write(&sbi->extent_tree_lock);
-out:
-	trace_f2fs_shrink_extent_tree(sbi, node_cnt, tree_cnt);
-
-	return node_cnt + tree_cnt;
-}
-
-unsigned int f2fs_destroy_extent_node(struct inode *inode)
-{
-	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	struct extent_tree *et = F2FS_I(inode)->extent_tree;
-	unsigned int node_cnt = 0;
-
-	if (!et)
-		return 0;
-
-	write_lock(&et->lock);
-	node_cnt = __free_extent_tree(sbi, et, true);
-	write_unlock(&et->lock);
-
-	return node_cnt;
-}
-
-void f2fs_destroy_extent_tree(struct inode *inode)
-{
-	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	struct extent_tree *et = F2FS_I(inode)->extent_tree;
-	unsigned int node_cnt = 0;
-
-	if (!et)
-		return;
-
-	if (inode->i_nlink && !is_bad_inode(inode) && et->count) {
-		atomic_dec(&et->refcount);
-		return;
-	}
-
-	/* free all extent info belong to this extent tree */
-	node_cnt = f2fs_destroy_extent_node(inode);
-
-	/* delete extent tree entry in radix tree */
-	down_write(&sbi->extent_tree_lock);
-	atomic_dec(&et->refcount);
-	f2fs_bug_on(sbi, atomic_read(&et->refcount) || et->count);
-	radix_tree_delete(&sbi->extent_tree_root, inode->i_ino);
-	kmem_cache_free(extent_tree_slab, et);
-	sbi->total_ext_tree--;
-	up_write(&sbi->extent_tree_lock);
-
-	F2FS_I(inode)->extent_tree = NULL;
-
-	trace_f2fs_destroy_extent_tree(inode, node_cnt);
-	return;
-}
-
-static bool f2fs_lookup_extent_cache(struct inode *inode, pgoff_t pgofs,
-							struct extent_info *ei)
-{
-	if (!f2fs_may_extent_tree(inode))
-		return false;
-
-	return f2fs_lookup_extent_tree(inode, pgofs, ei);
-}
-
-void f2fs_update_extent_cache(struct dnode_of_data *dn)
-{
-	struct f2fs_inode_info *fi = F2FS_I(dn->inode);
-	pgoff_t fofs;
-
-	if (!f2fs_may_extent_tree(dn->inode))
-		return;
-
-	f2fs_bug_on(F2FS_I_SB(dn->inode), dn->data_blkaddr == NEW_ADDR);
-
-	fofs = start_bidx_of_node(ofs_of_node(dn->node_page), fi) +
-							dn->ofs_in_node;
-
-	if (f2fs_update_extent_tree(dn->inode, fofs, dn->data_blkaddr))
-		sync_inode_page(dn);
-}
-
 struct page *get_read_data_page(struct inode *inode, pgoff_t index, int rw)
 {
 	struct address_space *mapping = inode->i_mapping;
@@ -1017,7 +472,7 @@ static int __allocate_data_block(struct dnode_of_data *dn)
 		i_size_write(dn->inode, ((fofs + 1) << PAGE_CACHE_SHIFT));
 
 	/* direct IO doesn't use extent cache to maximize the performance */
-	__drop_largest_extent(dn->inode, fofs);
+	f2fs_drop_largest_extent(dn->inode, fofs);
 
 	return 0;
 }
@@ -1997,37 +1452,6 @@ static sector_t f2fs_bmap(struct address_space *mapping, sector_t block)
 	return generic_block_bmap(mapping, block, get_data_block);
 }
 
-void init_extent_cache_info(struct f2fs_sb_info *sbi)
-{
-	INIT_RADIX_TREE(&sbi->extent_tree_root, GFP_NOIO);
-	init_rwsem(&sbi->extent_tree_lock);
-	INIT_LIST_HEAD(&sbi->extent_list);
-	spin_lock_init(&sbi->extent_lock);
-	sbi->total_ext_tree = 0;
-	atomic_set(&sbi->total_ext_node, 0);
-}
-
-int __init create_extent_cache(void)
-{
-	extent_tree_slab = f2fs_kmem_cache_create("f2fs_extent_tree",
-			sizeof(struct extent_tree));
-	if (!extent_tree_slab)
-		return -ENOMEM;
-	extent_node_slab = f2fs_kmem_cache_create("f2fs_extent_node",
-			sizeof(struct extent_node));
-	if (!extent_node_slab) {
-		kmem_cache_destroy(extent_tree_slab);
-		return -ENOMEM;
-	}
-	return 0;
-}
-
-void destroy_extent_cache(void)
-{
-	kmem_cache_destroy(extent_node_slab);
-	kmem_cache_destroy(extent_tree_slab);
-}
-
 const struct address_space_operations f2fs_dblock_aops = {
 	.readpage	= f2fs_read_data_page,
 	.readpages	= f2fs_read_data_pages,

commit 3c7df87dad065a4656b13115593c59c8a324a108
Author: Fan Li <fanofcode.li@samsung.com>
Date:   Wed Jul 8 16:02:54 2015 +0800

    f2fs: don't try to split extents shorter than F2FS_MIN_EXTENT_LEN
    
    Since only parts of extents longer than F2FS_MIN_EXTENT_LEN will
    be kept in extent cache after split, extents already shorter than
    F2FS_MIN_EXTENT_LEN don't need to try split at all.
    
    Signed-off-by: Fan Li <fanofcode.li@samsung.com>
    Reviewed-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index de55c088948f..ce0d5ec8e770 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -585,7 +585,7 @@ static bool f2fs_update_extent_tree(struct inode *inode, pgoff_t fofs,
 	__detach_extent_node(sbi, et, en);
 
 	/* 2. if extent can be split more, split and insert the left part */
-	if (dei.len > 1) {
+	if (dei.len > F2FS_MIN_EXTENT_LEN) {
 		/*  insert left part of split extent into cache */
 		if (fofs - dei.fofs >= F2FS_MIN_EXTENT_LEN) {
 			set_extent_info(&ei, dei.fofs, dei.blk,

commit 90d4388ac2cec0c83cad7315d3cd0065553430e1
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Wed Jul 8 18:24:38 2015 +0800

    f2fs: fix to update page flag
    
    This patch fixes to update page flag (e.g. Uptodate/cold flag) in
    ->write_begin.
    
    Otherwise, page will be non-uptodate when we try to write entire
    page, and cold data flag in page will not be clean when gced page
    is being rewritten.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index c9d0f8b06d15..de55c088948f 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1790,8 +1790,10 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 	f2fs_put_dnode(&dn);
 	f2fs_unlock_op(sbi);
 
-	if ((len == PAGE_CACHE_SIZE) || PageUptodate(page))
-		return 0;
+	if (len == PAGE_CACHE_SIZE)
+		goto out_update;
+	if (PageUptodate(page))
+		goto out_clear;
 
 	f2fs_wait_on_page_writeback(page, DATA);
 
@@ -1801,7 +1803,7 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 
 		/* Reading beyond i_size is simple: memset to zero */
 		zero_user_segments(page, 0, start, end, PAGE_CACHE_SIZE);
-		goto out;
+		goto out_update;
 	}
 
 	if (dn.data_blkaddr == NEW_ADDR) {
@@ -1839,8 +1841,9 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 			}
 		}
 	}
-out:
+out_update:
 	SetPageUptodate(page);
+out_clear:
 	clear_cold_data(page);
 	return 0;
 

commit 7023a1ad17f4e21acb74167ab647cd123d9eb801
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Mon Jun 29 16:34:39 2015 -0700

    f2fs: shrink unreferenced extent_caches first
    
    If an extent_tree entry has a zero reference count, we can drop it from the
    cache in higher priority rather than currently referencing entries.
    
    Reviewed-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 3e4402f661d7..c9d0f8b06d15 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -662,21 +662,54 @@ unsigned int f2fs_shrink_extent_tree(struct f2fs_sb_info *sbi, int nr_shrink)
 	struct radix_tree_root *root = &sbi->extent_tree_root;
 	unsigned int found;
 	unsigned int node_cnt = 0, tree_cnt = 0;
+	int remained;
 
 	if (!test_opt(sbi, EXTENT_CACHE))
 		return 0;
 
+	if (!down_write_trylock(&sbi->extent_tree_lock))
+		goto out;
+
+	/* 1. remove unreferenced extent tree */
+	while ((found = radix_tree_gang_lookup(root,
+				(void **)treevec, ino, EXT_TREE_VEC_SIZE))) {
+		unsigned i;
+
+		ino = treevec[found - 1]->ino + 1;
+		for (i = 0; i < found; i++) {
+			struct extent_tree *et = treevec[i];
+
+			if (!atomic_read(&et->refcount)) {
+				write_lock(&et->lock);
+				node_cnt += __free_extent_tree(sbi, et, true);
+				write_unlock(&et->lock);
+
+				radix_tree_delete(root, et->ino);
+				kmem_cache_free(extent_tree_slab, et);
+				sbi->total_ext_tree--;
+				tree_cnt++;
+
+				if (node_cnt + tree_cnt >= nr_shrink)
+					goto unlock_out;
+			}
+		}
+	}
+	up_write(&sbi->extent_tree_lock);
+
+	/* 2. remove LRU extent entries */
+	if (!down_write_trylock(&sbi->extent_tree_lock))
+		goto out;
+
+	remained = nr_shrink - (node_cnt + tree_cnt);
+
 	spin_lock(&sbi->extent_lock);
 	list_for_each_entry_safe(en, tmp, &sbi->extent_list, list) {
-		if (!nr_shrink--)
+		if (!remained--)
 			break;
 		list_del_init(&en->list);
 	}
 	spin_unlock(&sbi->extent_lock);
 
-	if (!down_write_trylock(&sbi->extent_tree_lock))
-		goto out;
-
 	while ((found = radix_tree_gang_lookup(root,
 				(void **)treevec, ino, EXT_TREE_VEC_SIZE))) {
 		unsigned i;
@@ -688,14 +721,12 @@ unsigned int f2fs_shrink_extent_tree(struct f2fs_sb_info *sbi, int nr_shrink)
 			write_lock(&et->lock);
 			node_cnt += __free_extent_tree(sbi, et, false);
 			write_unlock(&et->lock);
-			if (!atomic_read(&et->refcount) && !et->count) {
-				radix_tree_delete(root, et->ino);
-				kmem_cache_free(extent_tree_slab, et);
-				sbi->total_ext_tree--;
-				tree_cnt++;
-			}
+
+			if (node_cnt + tree_cnt >= nr_shrink)
+				break;
 		}
 	}
+unlock_out:
 	up_write(&sbi->extent_tree_lock);
 out:
 	trace_f2fs_shrink_extent_tree(sbi, node_cnt, tree_cnt);

commit bb96a8d51e523c162b436c4545eb1a4e9f9f530e
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Mon Jul 6 20:31:49 2015 +0800

    f2fs: enhance multithread performance
    
    In ->writepages, we use writepages mutex lock to serialize all block
    address allocation and page submitting pairs from different inodes.
    This method makes our delayed dirty pages of one inode being written
    continously as many as possible.
    
    But there is one problem that we did not submit current cached bio in
    protection region of writepages mutex lock, so there is a small chance
    that we submit the one of other thread's as below, resulting in
    splitting more bios.
    
    thread 1                        thread 2
    ->writepages
      lock(writepages)
      ->write_cache_pages
      unlock(writepages)
                                      lock(writepages)
                                      ->write_cache_pages
      ->f2fs_submit_merged_bio
                                        ->writepage
                                      unlock(writepages)
    
    fs_mark-6535  [002] ....  2242.270230: f2fs_submit_write_bio: dev = (1,0), WRITE_SYNC, DATA, sector = 5766152, size = 524288
    fs_mark-6536  [000] ....  2242.270361: f2fs_submit_write_bio: dev = (1,0), WRITE_SYNC, DATA, sector = 5767176, size = 4096
    fs_mark-6536  [000] ....  2242.270370: f2fs_submit_write_bio: dev = (1,0), WRITE_SYNC, NODE, sector = 8138112, size = 4096
    fs_mark-6535  [002] ....  2242.270776: f2fs_submit_write_bio: dev = (1,0), WRITE_SYNC, DATA, sector = 5767184, size = 516096
    
    This may really increase time of block layer works, and may cause
    larger IO lantency.
    
    This patch moves the submitting operation into region of writepages
    mutex lock to avoid bio splits when concurrently writebacking is
    intensive.
    
    my test environment: virtual machine,
    intel cpu i5 2500, 8GB size memory, 4GB size ramdisk
    
    time fs_mark  -t  16  -L  1  -s  524288  -S  1  -d  /mnt/f2fs/
    
    before:
    real    0m4.244s
    user    0m0.088s
    sys     0m12.336s
    
    after:
    real    0m3.822s
    user    0m0.072s
    sys     0m10.760s
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index cdc1c2b781b8..3e4402f661d7 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1672,11 +1672,10 @@ static int f2fs_write_data_pages(struct address_space *mapping,
 		locked = true;
 	}
 	ret = write_cache_pages(mapping, wbc, __f2fs_writepage, mapping);
+	f2fs_submit_merged_bio(sbi, DATA, WRITE);
 	if (locked)
 		mutex_unlock(&sbi->writepages);
 
-	f2fs_submit_merged_bio(sbi, DATA, WRITE);
-
 	remove_dirty_dir_inode(inode);
 
 	wbc->nr_to_write = max((long)0, wbc->nr_to_write - diff);

commit 84bc926c076963d5b992640f5c8d242754801fd6
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Mon Jun 29 16:01:14 2015 -0700

    f2fs: check the largest extent at look-up time
    
    Because of the extent shrinker or other -ENOMEM scenarios, it cannot guarantee
    that the largest extent would be cached in the tree all the time.
    
    Instead of relying on extent_tree, we can simply check the cached one in extent
    tree accordingly.
    
    Reviewed-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index be0945cd9808..cdc1c2b781b8 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -512,12 +512,22 @@ static bool f2fs_lookup_extent_tree(struct inode *inode, pgoff_t pgofs,
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
 	struct extent_tree *et = F2FS_I(inode)->extent_tree;
 	struct extent_node *en;
+	bool ret = false;
 
 	f2fs_bug_on(sbi, !et);
 
 	trace_f2fs_lookup_extent_tree_start(inode, pgofs);
 
 	read_lock(&et->lock);
+
+	if (et->largest.fofs <= pgofs &&
+			et->largest.fofs + et->largest.len > pgofs) {
+		*ei = et->largest;
+		ret = true;
+		stat_inc_read_hit(sbi->sb);
+		goto out;
+	}
+
 	en = __lookup_extent_tree(et, pgofs);
 	if (en) {
 		*ei = en->ei;
@@ -526,13 +536,15 @@ static bool f2fs_lookup_extent_tree(struct inode *inode, pgoff_t pgofs,
 			list_move_tail(&en->list, &sbi->extent_list);
 		et->cached_en = en;
 		spin_unlock(&sbi->extent_lock);
+		ret = true;
 		stat_inc_read_hit(sbi->sb);
 	}
+out:
 	stat_inc_total_hit(sbi->sb);
 	read_unlock(&et->lock);
 
-	trace_f2fs_lookup_extent_tree_end(inode, pgofs, en);
-	return en ? true : false;
+	trace_f2fs_lookup_extent_tree_end(inode, pgofs, ei);
+	return ret;
 }
 
 /* return true, if on-disk extent should be updated */

commit 3e72f721390dc14e7b33fda812843c0725810106
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Jun 19 17:53:26 2015 -0700

    f2fs: use extent_cache by default
    
    We don't need to handle the duplicate extent information.
    
    The integrated rule is:
     - update on-disk extent with largest one tracked by in-memory extent_cache
     - destroy extent_tree for the truncation case
     - drop per-inode extent_cache by shrinker
    
    Reviewed-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 55b2a79b3526..be0945cd9808 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -266,103 +266,6 @@ int f2fs_reserve_block(struct dnode_of_data *dn, pgoff_t index)
 	return err;
 }
 
-static bool lookup_extent_info(struct inode *inode, pgoff_t pgofs,
-							struct extent_info *ei)
-{
-	struct f2fs_inode_info *fi = F2FS_I(inode);
-	pgoff_t start_fofs, end_fofs;
-	block_t start_blkaddr;
-
-	read_lock(&fi->ext_lock);
-	if (fi->ext.len == 0) {
-		read_unlock(&fi->ext_lock);
-		return false;
-	}
-
-	stat_inc_total_hit(inode->i_sb);
-
-	start_fofs = fi->ext.fofs;
-	end_fofs = fi->ext.fofs + fi->ext.len - 1;
-	start_blkaddr = fi->ext.blk;
-
-	if (pgofs >= start_fofs && pgofs <= end_fofs) {
-		*ei = fi->ext;
-		stat_inc_read_hit(inode->i_sb);
-		read_unlock(&fi->ext_lock);
-		return true;
-	}
-	read_unlock(&fi->ext_lock);
-	return false;
-}
-
-static bool update_extent_info(struct inode *inode, pgoff_t fofs,
-								block_t blkaddr)
-{
-	struct f2fs_inode_info *fi = F2FS_I(inode);
-	pgoff_t start_fofs, end_fofs;
-	block_t start_blkaddr, end_blkaddr;
-	int need_update = true;
-
-	write_lock(&fi->ext_lock);
-
-	start_fofs = fi->ext.fofs;
-	end_fofs = fi->ext.fofs + fi->ext.len - 1;
-	start_blkaddr = fi->ext.blk;
-	end_blkaddr = fi->ext.blk + fi->ext.len - 1;
-
-	/* Drop and initialize the matched extent */
-	if (fi->ext.len == 1 && fofs == start_fofs)
-		fi->ext.len = 0;
-
-	/* Initial extent */
-	if (fi->ext.len == 0) {
-		if (blkaddr != NULL_ADDR) {
-			fi->ext.fofs = fofs;
-			fi->ext.blk = blkaddr;
-			fi->ext.len = 1;
-		}
-		goto end_update;
-	}
-
-	/* Front merge */
-	if (fofs == start_fofs - 1 && blkaddr == start_blkaddr - 1) {
-		fi->ext.fofs--;
-		fi->ext.blk--;
-		fi->ext.len++;
-		goto end_update;
-	}
-
-	/* Back merge */
-	if (fofs == end_fofs + 1 && blkaddr == end_blkaddr + 1) {
-		fi->ext.len++;
-		goto end_update;
-	}
-
-	/* Split the existing extent */
-	if (fi->ext.len > 1 &&
-		fofs >= start_fofs && fofs <= end_fofs) {
-		if ((end_fofs - fofs) < (fi->ext.len >> 1)) {
-			fi->ext.len = fofs - start_fofs;
-		} else {
-			fi->ext.fofs = fofs + 1;
-			fi->ext.blk = start_blkaddr + fofs - start_fofs + 1;
-			fi->ext.len -= fofs - start_fofs + 1;
-		}
-	} else {
-		need_update = false;
-	}
-
-	/* Finally, if the extent is very fragmented, let's drop the cache. */
-	if (fi->ext.len < F2FS_MIN_EXTENT_LEN) {
-		fi->ext.len = 0;
-		set_inode_flag(fi, FI_NO_EXTENT);
-		need_update = true;
-	}
-end_update:
-	write_unlock(&fi->ext_lock);
-	return need_update;
-}
-
 static struct extent_node *__attach_extent_node(struct f2fs_sb_info *sbi,
 				struct extent_tree *et, struct extent_info *ei,
 				struct rb_node *parent, struct rb_node **p)
@@ -394,23 +297,6 @@ static void __detach_extent_node(struct f2fs_sb_info *sbi,
 		et->cached_en = NULL;
 }
 
-static struct extent_tree *__find_extent_tree(struct f2fs_sb_info *sbi,
-							nid_t ino)
-{
-	struct extent_tree *et;
-
-	down_read(&sbi->extent_tree_lock);
-	et = radix_tree_lookup(&sbi->extent_tree_root, ino);
-	if (!et) {
-		up_read(&sbi->extent_tree_lock);
-		return NULL;
-	}
-	atomic_inc(&et->refcount);
-	up_read(&sbi->extent_tree_lock);
-
-	return et;
-}
-
 static struct extent_tree *__grab_extent_tree(struct inode *inode)
 {
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
@@ -434,6 +320,9 @@ static struct extent_tree *__grab_extent_tree(struct inode *inode)
 	atomic_inc(&et->refcount);
 	up_write(&sbi->extent_tree_lock);
 
+	/* never died untill evict_inode */
+	F2FS_I(inode)->extent_tree = et;
+
 	return et;
 }
 
@@ -522,7 +411,7 @@ static struct extent_node *__insert_extent_tree(struct f2fs_sb_info *sbi,
 				en->ei.blk = ei->blk;
 				en->ei.len += ei->len;
 				*den = __try_back_merge(sbi, et, en);
-				return en;
+				goto update_out;
 			}
 			p = &(*p)->rb_left;
 		} else if (ei->fofs >= en->ei.fofs + en->ei.len) {
@@ -530,7 +419,7 @@ static struct extent_node *__insert_extent_tree(struct f2fs_sb_info *sbi,
 				f2fs_bug_on(sbi, !den);
 				en->ei.len += ei->len;
 				*den = __try_front_merge(sbi, et, en);
-				return en;
+				goto update_out;
 			}
 			p = &(*p)->rb_right;
 		} else {
@@ -538,7 +427,14 @@ static struct extent_node *__insert_extent_tree(struct f2fs_sb_info *sbi,
 		}
 	}
 
-	return __attach_extent_node(sbi, et, ei, parent, p);
+	en = __attach_extent_node(sbi, et, ei, parent, p);
+	if (!en)
+		return NULL;
+update_out:
+	if (en->ei.len > et->largest.len)
+		et->largest = en->ei;
+	et->cached_en = en;
+	return en;
 }
 
 static unsigned int __free_extent_tree(struct f2fs_sb_info *sbi,
@@ -570,51 +466,56 @@ static unsigned int __free_extent_tree(struct f2fs_sb_info *sbi,
 	return count - et->count;
 }
 
-static void f2fs_init_extent_tree(struct inode *inode,
-						struct f2fs_extent *i_ext)
+static void __drop_largest_extent(struct inode *inode, pgoff_t fofs)
+{
+	struct extent_info *largest = &F2FS_I(inode)->extent_tree->largest;
+
+	if (largest->fofs <= fofs && largest->fofs + largest->len > fofs)
+		largest->len = 0;
+}
+
+void f2fs_init_extent_tree(struct inode *inode, struct f2fs_extent *i_ext)
 {
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
 	struct extent_tree *et;
 	struct extent_node *en;
 	struct extent_info ei;
 
-	if (le32_to_cpu(i_ext->len) < F2FS_MIN_EXTENT_LEN)
+	if (!f2fs_may_extent_tree(inode))
 		return;
 
 	et = __grab_extent_tree(inode);
 
-	write_lock(&et->lock);
-	if (et->count)
-		goto out;
+	if (!i_ext || le32_to_cpu(i_ext->len) < F2FS_MIN_EXTENT_LEN)
+		return;
 
 	set_extent_info(&ei, le32_to_cpu(i_ext->fofs),
 		le32_to_cpu(i_ext->blk), le32_to_cpu(i_ext->len));
 
+	write_lock(&et->lock);
+	if (et->count)
+		goto out;
+
 	en = __insert_extent_tree(sbi, et, &ei, NULL);
 	if (en) {
-		et->cached_en = en;
-
 		spin_lock(&sbi->extent_lock);
 		list_add_tail(&en->list, &sbi->extent_list);
 		spin_unlock(&sbi->extent_lock);
 	}
 out:
 	write_unlock(&et->lock);
-	atomic_dec(&et->refcount);
 }
 
 static bool f2fs_lookup_extent_tree(struct inode *inode, pgoff_t pgofs,
 							struct extent_info *ei)
 {
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	struct extent_tree *et;
+	struct extent_tree *et = F2FS_I(inode)->extent_tree;
 	struct extent_node *en;
 
-	trace_f2fs_lookup_extent_tree_start(inode, pgofs);
+	f2fs_bug_on(sbi, !et);
 
-	et = __find_extent_tree(sbi, inode->i_ino);
-	if (!et)
-		return false;
+	trace_f2fs_lookup_extent_tree_start(inode, pgofs);
 
 	read_lock(&et->lock);
 	en = __lookup_extent_tree(et, pgofs);
@@ -631,27 +532,38 @@ static bool f2fs_lookup_extent_tree(struct inode *inode, pgoff_t pgofs,
 	read_unlock(&et->lock);
 
 	trace_f2fs_lookup_extent_tree_end(inode, pgofs, en);
-
-	atomic_dec(&et->refcount);
 	return en ? true : false;
 }
 
-static void f2fs_update_extent_tree(struct inode *inode, pgoff_t fofs,
+/* return true, if on-disk extent should be updated */
+static bool f2fs_update_extent_tree(struct inode *inode, pgoff_t fofs,
 							block_t blkaddr)
 {
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	struct extent_tree *et;
+	struct extent_tree *et = F2FS_I(inode)->extent_tree;
 	struct extent_node *en = NULL, *en1 = NULL, *en2 = NULL, *en3 = NULL;
 	struct extent_node *den = NULL;
-	struct extent_info ei, dei;
+	struct extent_info ei, dei, prev;
 	unsigned int endofs;
 
-	trace_f2fs_update_extent_tree(inode, fofs, blkaddr);
+	if (!et)
+		return false;
 
-	et = __grab_extent_tree(inode);
+	trace_f2fs_update_extent_tree(inode, fofs, blkaddr);
 
 	write_lock(&et->lock);
 
+	if (is_inode_flag_set(F2FS_I(inode), FI_NO_EXTENT)) {
+		write_unlock(&et->lock);
+		return false;
+	}
+
+	prev = et->largest;
+	dei.len = 0;
+
+	/* we do not guarantee that the largest extent is cached all the time */
+	__drop_largest_extent(inode, fofs);
+
 	/* 1. lookup and remove existing extent info in cache */
 	en = __lookup_extent_tree(et, fofs);
 	if (!en)
@@ -683,6 +595,14 @@ static void f2fs_update_extent_tree(struct inode *inode, pgoff_t fofs,
 	if (blkaddr) {
 		set_extent_info(&ei, fofs, blkaddr, 1);
 		en3 = __insert_extent_tree(sbi, et, &ei, &den);
+
+		/* give up extent_cache, if split and small updates happen */
+		if (dei.len >= 1 &&
+				prev.len < F2FS_MIN_EXTENT_LEN &&
+				et->largest.len < F2FS_MIN_EXTENT_LEN) {
+			et->largest.len = 0;
+			set_inode_flag(F2FS_I(inode), FI_NO_EXTENT);
+		}
 	}
 
 	/* 4. update in global extent list */
@@ -714,57 +634,12 @@ static void f2fs_update_extent_tree(struct inode *inode, pgoff_t fofs,
 	if (den)
 		kmem_cache_free(extent_node_slab, den);
 
-	write_unlock(&et->lock);
-	atomic_dec(&et->refcount);
-}
-
-void f2fs_preserve_extent_tree(struct inode *inode)
-{
-	struct extent_tree *et;
-	struct extent_info *ext = &F2FS_I(inode)->ext;
-	bool sync = false;
-
-	if (!test_opt(F2FS_I_SB(inode), EXTENT_CACHE))
-		return;
-
-	et = __find_extent_tree(F2FS_I_SB(inode), inode->i_ino);
-	if (!et) {
-		if (ext->len) {
-			ext->len = 0;
-			update_inode_page(inode);
-		}
-		return;
-	}
-
-	read_lock(&et->lock);
-	if (et->count) {
-		struct extent_node *en;
-
-		if (et->cached_en) {
-			en = et->cached_en;
-		} else {
-			struct rb_node *node = rb_first(&et->root);
-
-			if (!node)
-				node = rb_last(&et->root);
-			en = rb_entry(node, struct extent_node, rb_node);
-		}
-
-		if (__is_extent_same(ext, &en->ei))
-			goto out;
+	if (is_inode_flag_set(F2FS_I(inode), FI_NO_EXTENT))
+		__free_extent_tree(sbi, et, true);
 
-		*ext = en->ei;
-		sync = true;
-	} else if (ext->len) {
-		ext->len = 0;
-		sync = true;
-	}
-out:
-	read_unlock(&et->lock);
-	atomic_dec(&et->refcount);
+	write_unlock(&et->lock);
 
-	if (sync)
-		update_inode_page(inode);
+	return !__is_extent_same(&prev, &et->largest);
 }
 
 unsigned int f2fs_shrink_extent_tree(struct f2fs_sb_info *sbi, int nr_shrink)
@@ -772,8 +647,7 @@ unsigned int f2fs_shrink_extent_tree(struct f2fs_sb_info *sbi, int nr_shrink)
 	struct extent_tree *treevec[EXT_TREE_VEC_SIZE];
 	struct extent_node *en, *tmp;
 	unsigned long ino = F2FS_ROOT_INO(sbi);
-	struct radix_tree_iter iter;
-	void **slot;
+	struct radix_tree_root *root = &sbi->extent_tree_root;
 	unsigned int found;
 	unsigned int node_cnt = 0, tree_cnt = 0;
 
@@ -788,10 +662,10 @@ unsigned int f2fs_shrink_extent_tree(struct f2fs_sb_info *sbi, int nr_shrink)
 	}
 	spin_unlock(&sbi->extent_lock);
 
-	if (!down_read_trylock(&sbi->extent_tree_lock))
+	if (!down_write_trylock(&sbi->extent_tree_lock))
 		goto out;
 
-	while ((found = radix_tree_gang_lookup(&sbi->extent_tree_root,
+	while ((found = radix_tree_gang_lookup(root,
 				(void **)treevec, ino, EXT_TREE_VEC_SIZE))) {
 		unsigned i;
 
@@ -799,27 +673,15 @@ unsigned int f2fs_shrink_extent_tree(struct f2fs_sb_info *sbi, int nr_shrink)
 		for (i = 0; i < found; i++) {
 			struct extent_tree *et = treevec[i];
 
-			atomic_inc(&et->refcount);
 			write_lock(&et->lock);
 			node_cnt += __free_extent_tree(sbi, et, false);
 			write_unlock(&et->lock);
-			atomic_dec(&et->refcount);
-		}
-	}
-	up_read(&sbi->extent_tree_lock);
-
-	if (!down_write_trylock(&sbi->extent_tree_lock))
-		goto out;
-
-	radix_tree_for_each_slot(slot, &sbi->extent_tree_root, &iter,
-							F2FS_ROOT_INO(sbi)) {
-		struct extent_tree *et = (struct extent_tree *)*slot;
-
-		if (!atomic_read(&et->refcount) && !et->count) {
-			radix_tree_delete(&sbi->extent_tree_root, et->ino);
-			kmem_cache_free(extent_tree_slab, et);
-			sbi->total_ext_tree--;
-			tree_cnt++;
+			if (!atomic_read(&et->refcount) && !et->count) {
+				radix_tree_delete(root, et->ino);
+				kmem_cache_free(extent_tree_slab, et);
+				sbi->total_ext_tree--;
+				tree_cnt++;
+			}
 		}
 	}
 	up_write(&sbi->extent_tree_lock);
@@ -829,63 +691,61 @@ unsigned int f2fs_shrink_extent_tree(struct f2fs_sb_info *sbi, int nr_shrink)
 	return node_cnt + tree_cnt;
 }
 
-void f2fs_destroy_extent_tree(struct inode *inode)
+unsigned int f2fs_destroy_extent_node(struct inode *inode)
 {
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	struct extent_tree *et;
+	struct extent_tree *et = F2FS_I(inode)->extent_tree;
 	unsigned int node_cnt = 0;
 
-	if (!test_opt(sbi, EXTENT_CACHE))
-		return;
-
-	et = __find_extent_tree(sbi, inode->i_ino);
 	if (!et)
-		goto out;
+		return 0;
 
-	/* free all extent info belong to this extent tree */
 	write_lock(&et->lock);
 	node_cnt = __free_extent_tree(sbi, et, true);
 	write_unlock(&et->lock);
 
-	atomic_dec(&et->refcount);
+	return node_cnt;
+}
 
-	/* try to find and delete extent tree entry in radix tree */
-	down_write(&sbi->extent_tree_lock);
-	et = radix_tree_lookup(&sbi->extent_tree_root, inode->i_ino);
-	if (!et) {
-		up_write(&sbi->extent_tree_lock);
-		goto out;
+void f2fs_destroy_extent_tree(struct inode *inode)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct extent_tree *et = F2FS_I(inode)->extent_tree;
+	unsigned int node_cnt = 0;
+
+	if (!et)
+		return;
+
+	if (inode->i_nlink && !is_bad_inode(inode) && et->count) {
+		atomic_dec(&et->refcount);
+		return;
 	}
+
+	/* free all extent info belong to this extent tree */
+	node_cnt = f2fs_destroy_extent_node(inode);
+
+	/* delete extent tree entry in radix tree */
+	down_write(&sbi->extent_tree_lock);
+	atomic_dec(&et->refcount);
 	f2fs_bug_on(sbi, atomic_read(&et->refcount) || et->count);
 	radix_tree_delete(&sbi->extent_tree_root, inode->i_ino);
 	kmem_cache_free(extent_tree_slab, et);
 	sbi->total_ext_tree--;
 	up_write(&sbi->extent_tree_lock);
-out:
-	trace_f2fs_destroy_extent_tree(inode, node_cnt);
-	return;
-}
 
-void f2fs_init_extent_cache(struct inode *inode, struct f2fs_extent *i_ext)
-{
-	if (test_opt(F2FS_I_SB(inode), EXTENT_CACHE))
-		f2fs_init_extent_tree(inode, i_ext);
+	F2FS_I(inode)->extent_tree = NULL;
 
-	write_lock(&F2FS_I(inode)->ext_lock);
-	get_extent_info(&F2FS_I(inode)->ext, *i_ext);
-	write_unlock(&F2FS_I(inode)->ext_lock);
+	trace_f2fs_destroy_extent_tree(inode, node_cnt);
+	return;
 }
 
 static bool f2fs_lookup_extent_cache(struct inode *inode, pgoff_t pgofs,
 							struct extent_info *ei)
 {
-	if (is_inode_flag_set(F2FS_I(inode), FI_NO_EXTENT))
+	if (!f2fs_may_extent_tree(inode))
 		return false;
 
-	if (test_opt(F2FS_I_SB(inode), EXTENT_CACHE))
-		return f2fs_lookup_extent_tree(inode, pgofs, ei);
-
-	return lookup_extent_info(inode, pgofs, ei);
+	return f2fs_lookup_extent_tree(inode, pgofs, ei);
 }
 
 void f2fs_update_extent_cache(struct dnode_of_data *dn)
@@ -893,19 +753,15 @@ void f2fs_update_extent_cache(struct dnode_of_data *dn)
 	struct f2fs_inode_info *fi = F2FS_I(dn->inode);
 	pgoff_t fofs;
 
-	f2fs_bug_on(F2FS_I_SB(dn->inode), dn->data_blkaddr == NEW_ADDR);
-
-	if (is_inode_flag_set(fi, FI_NO_EXTENT))
+	if (!f2fs_may_extent_tree(dn->inode))
 		return;
 
+	f2fs_bug_on(F2FS_I_SB(dn->inode), dn->data_blkaddr == NEW_ADDR);
+
 	fofs = start_bidx_of_node(ofs_of_node(dn->node_page), fi) +
 							dn->ofs_in_node;
 
-	/* we should call update_extent_info() to update on-disk extent */
-	if (test_opt(F2FS_I_SB(dn->inode), EXTENT_CACHE))
-		f2fs_update_extent_tree(dn->inode, fofs, dn->data_blkaddr);
-
-	if (update_extent_info(dn->inode, fofs, dn->data_blkaddr))
+	if (f2fs_update_extent_tree(dn->inode, fofs, dn->data_blkaddr))
 		sync_inode_page(dn);
 }
 
@@ -1109,8 +965,6 @@ static int __allocate_data_block(struct dnode_of_data *dn)
 
 	allocate_data_block(sbi, NULL, dn->data_blkaddr, &dn->data_blkaddr,
 								&sum, seg);
-
-	/* direct IO doesn't use extent cache to maximize the performance */
 	set_data_blkaddr(dn);
 
 	/* update i_size */
@@ -1119,6 +973,9 @@ static int __allocate_data_block(struct dnode_of_data *dn)
 	if (i_size_read(dn->inode) < ((fofs + 1) << PAGE_CACHE_SHIFT))
 		i_size_write(dn->inode, ((fofs + 1) << PAGE_CACHE_SHIFT));
 
+	/* direct IO doesn't use extent cache to maximize the performance */
+	__drop_largest_extent(dn->inode, fofs);
+
 	return 0;
 }
 

commit 554df79e523d14dab475eb6650cb96617256ceea
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Jun 19 13:41:23 2015 -0700

    f2fs: shrink extent_cache entries
    
    This patch registers shrinking extent_caches.
    
    Reviewed-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 982a1a58efd7..55b2a79b3526 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -767,7 +767,7 @@ void f2fs_preserve_extent_tree(struct inode *inode)
 		update_inode_page(inode);
 }
 
-void f2fs_shrink_extent_tree(struct f2fs_sb_info *sbi, int nr_shrink)
+unsigned int f2fs_shrink_extent_tree(struct f2fs_sb_info *sbi, int nr_shrink)
 {
 	struct extent_tree *treevec[EXT_TREE_VEC_SIZE];
 	struct extent_node *en, *tmp;
@@ -778,10 +778,7 @@ void f2fs_shrink_extent_tree(struct f2fs_sb_info *sbi, int nr_shrink)
 	unsigned int node_cnt = 0, tree_cnt = 0;
 
 	if (!test_opt(sbi, EXTENT_CACHE))
-		return;
-
-	if (available_free_memory(sbi, EXTENT_CACHE))
-		return;
+		return 0;
 
 	spin_lock(&sbi->extent_lock);
 	list_for_each_entry_safe(en, tmp, &sbi->extent_list, list) {
@@ -791,7 +788,9 @@ void f2fs_shrink_extent_tree(struct f2fs_sb_info *sbi, int nr_shrink)
 	}
 	spin_unlock(&sbi->extent_lock);
 
-	down_read(&sbi->extent_tree_lock);
+	if (!down_read_trylock(&sbi->extent_tree_lock))
+		goto out;
+
 	while ((found = radix_tree_gang_lookup(&sbi->extent_tree_root,
 				(void **)treevec, ino, EXT_TREE_VEC_SIZE))) {
 		unsigned i;
@@ -809,7 +808,9 @@ void f2fs_shrink_extent_tree(struct f2fs_sb_info *sbi, int nr_shrink)
 	}
 	up_read(&sbi->extent_tree_lock);
 
-	down_write(&sbi->extent_tree_lock);
+	if (!down_write_trylock(&sbi->extent_tree_lock))
+		goto out;
+
 	radix_tree_for_each_slot(slot, &sbi->extent_tree_root, &iter,
 							F2FS_ROOT_INO(sbi)) {
 		struct extent_tree *et = (struct extent_tree *)*slot;
@@ -822,8 +823,10 @@ void f2fs_shrink_extent_tree(struct f2fs_sb_info *sbi, int nr_shrink)
 		}
 	}
 	up_write(&sbi->extent_tree_lock);
-
+out:
 	trace_f2fs_shrink_extent_tree(sbi, node_cnt, tree_cnt);
+
+	return node_cnt + tree_cnt;
 }
 
 void f2fs_destroy_extent_tree(struct inode *inode)

commit 244f4fc1c530c4e486f0e4f0909c0514e4539ba2
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Mon Jun 22 18:22:38 2015 -0700

    f2fs: set cached_en after checking finally
    
    This patch relocates cached_en not only to be covered by spin_lock, but also
    to set once after checking out completely.
    
    Reviewed-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 176e4ad4e5ed..982a1a58efd7 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -453,14 +453,12 @@ static struct extent_node *__lookup_extent_tree(struct extent_tree *et,
 	while (node) {
 		en = rb_entry(node, struct extent_node, rb_node);
 
-		if (fofs < en->ei.fofs) {
+		if (fofs < en->ei.fofs)
 			node = node->rb_left;
-		} else if (fofs >= en->ei.fofs + en->ei.len) {
+		else if (fofs >= en->ei.fofs + en->ei.len)
 			node = node->rb_right;
-		} else {
-			et->cached_en = en;
+		else
 			return en;
-		}
 	}
 	return NULL;
 }
@@ -625,6 +623,7 @@ static bool f2fs_lookup_extent_tree(struct inode *inode, pgoff_t pgofs,
 		spin_lock(&sbi->extent_lock);
 		if (!list_empty(&en->list))
 			list_move_tail(&en->list, &sbi->extent_list);
+		et->cached_en = en;
 		spin_unlock(&sbi->extent_lock);
 		stat_inc_read_hit(sbi->sb);
 	}

commit cbe91923a97c96d6a931f4b5b7e32083218a0251
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Tue Jun 16 15:17:01 2015 -0700

    f2fs: update on-disk extents even under extent_cache
    
    Previously, f2fs_update_extent_cache() updates in-memory extent_cache all the
    time, and then finally preserves its up-to-date extent into on-disk one during
    f2fs_evict_inode.
    
    But, in the following scenario:
    
    1. mount
    2. open & write an extent X
    3. f2fs_evict_inode; on-disk extent is X
    4. open & update the extent X with Y
    5. sync; trigger checkpoint
    6. power-cut
    
    after power-on, f2fs should serve extent Y, but we have an on-disk extent X.
    
    This causes a failure on xfstests/311.
    
    Reviewed-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index d1d86d53d1dc..176e4ad4e5ed 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -899,9 +899,9 @@ void f2fs_update_extent_cache(struct dnode_of_data *dn)
 	fofs = start_bidx_of_node(ofs_of_node(dn->node_page), fi) +
 							dn->ofs_in_node;
 
+	/* we should call update_extent_info() to update on-disk extent */
 	if (test_opt(F2FS_I_SB(dn->inode), EXTENT_CACHE))
-		return f2fs_update_extent_tree(dn->inode, fofs,
-							dn->data_blkaddr);
+		f2fs_update_extent_tree(dn->inode, fofs, dn->data_blkaddr);
 
 	if (update_extent_info(dn->inode, fofs, dn->data_blkaddr))
 		sync_inode_page(dn);

commit 7a2cb67867b9a7f28a7c4d0fadd2f337a6d46ff7
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Jun 18 14:17:04 2015 -0700

    f2fs: fix wrong block address calculation for a split extent
    
    This patch fixes wrong calculation on block address field when an extent is
    split.
    
    Reviewed-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index f71e19a9dd3c..d1d86d53d1dc 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -674,7 +674,7 @@ static void f2fs_update_extent_tree(struct inode *inode, pgoff_t fofs,
 		endofs = dei.fofs + dei.len - 1;
 		if (endofs - fofs >= F2FS_MIN_EXTENT_LEN) {
 			set_extent_info(&ei, fofs + 1,
-				fofs - dei.fofs + dei.blk, endofs - fofs);
+				fofs - dei.fofs + dei.blk + 1, endofs - fofs);
 			en2 = __insert_extent_tree(sbi, et, &ei, NULL);
 		}
 	}

commit 4246a0b63bd8f56a1469b12eafeb875b1041a451
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Jul 20 15:29:37 2015 +0200

    block: add a bi_error field to struct bio
    
    Currently we have two different ways to signal an I/O error on a BIO:
    
     (1) by clearing the BIO_UPTODATE flag
     (2) by returning a Linux errno value to the bi_end_io callback
    
    The first one has the drawback of only communicating a single possible
    error (-EIO), and the second one has the drawback of not beeing persistent
    when bios are queued up, and are not passed along from child to parent
    bio in the ever more popular chaining scenario.  Having both mechanisms
    available has the additional drawback of utterly confusing driver authors
    and introducing bugs where various I/O submitters only deal with one of
    them, and the others have to add boilerplate code to deal with both kinds
    of error returns.
    
    So add a new bi_error field to store an errno value directly in struct
    bio and remove the existing mechanisms to clean all this up.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Hannes Reinecke <hare@suse.de>
    Reviewed-by: NeilBrown <neilb@suse.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 9bedfa8dd3a5..8f0baa7ffb50 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -29,13 +29,13 @@
 static struct kmem_cache *extent_tree_slab;
 static struct kmem_cache *extent_node_slab;
 
-static void f2fs_read_end_io(struct bio *bio, int err)
+static void f2fs_read_end_io(struct bio *bio)
 {
 	struct bio_vec *bvec;
 	int i;
 
 	if (f2fs_bio_encrypted(bio)) {
-		if (err) {
+		if (bio->bi_error) {
 			f2fs_release_crypto_ctx(bio->bi_private);
 		} else {
 			f2fs_end_io_crypto_work(bio->bi_private, bio);
@@ -46,7 +46,7 @@ static void f2fs_read_end_io(struct bio *bio, int err)
 	bio_for_each_segment_all(bvec, bio, i) {
 		struct page *page = bvec->bv_page;
 
-		if (!err) {
+		if (!bio->bi_error) {
 			SetPageUptodate(page);
 		} else {
 			ClearPageUptodate(page);
@@ -57,7 +57,7 @@ static void f2fs_read_end_io(struct bio *bio, int err)
 	bio_put(bio);
 }
 
-static void f2fs_write_end_io(struct bio *bio, int err)
+static void f2fs_write_end_io(struct bio *bio)
 {
 	struct f2fs_sb_info *sbi = bio->bi_private;
 	struct bio_vec *bvec;
@@ -68,7 +68,7 @@ static void f2fs_write_end_io(struct bio *bio, int err)
 
 		f2fs_restore_and_release_control_page(&page);
 
-		if (unlikely(err)) {
+		if (unlikely(bio->bi_error)) {
 			set_page_dirty(page);
 			set_bit(AS_EIO, &page->mapping->flags);
 			f2fs_stop_checkpoint(sbi);

commit 6282adbf932c226f76e1b83e074448c79976fe75
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Sat Jul 25 00:29:17 2015 -0700

    f2fs: call set_page_dirty to attach i_wb for cgroup
    
    The cgroup attaches inode->i_wb via mark_inode_dirty and when set_page_writeback
    is called, __inc_wb_stat() updates i_wb's stat.
    
    So, we need to explicitly call set_page_dirty->__mark_inode_dirty in prior to
    any writebacking pages.
    
    This patch should resolve the following kernel panic reported by Andreas Reis.
    
    https://bugzilla.kernel.org/show_bug.cgi?id=101801
    
    --- Comment #2 from Andreas Reis <andreas.reis@gmail.com> ---
    BUG: unable to handle kernel NULL pointer dereference at 00000000000000a8
    IP: [<ffffffff8149deea>] __percpu_counter_add+0x1a/0x90
    PGD 2951ff067 PUD 2df43f067 PMD 0
    Oops: 0000 [#1] PREEMPT SMP
    Modules linked in:
    CPU: 7 PID: 10356 Comm: gcc Tainted: G        W       4.2.0-1-cu #1
    Hardware name: Gigabyte Technology Co., Ltd. G1.Sniper M5/G1.Sniper M5, BIOS
    T01 02/03/2015
    task: ffff880295044f80 ti: ffff880295140000 task.ti: ffff880295140000
    RIP: 0010:[<ffffffff8149deea>]  [<ffffffff8149deea>]
    __percpu_counter_add+0x1a/0x90
    RSP: 0018:ffff880295143ac8  EFLAGS: 00010082
    RAX: 0000000000000003 RBX: ffffea000a526d40 RCX: 0000000000000001
    RDX: 0000000000000020 RSI: 0000000000000001 RDI: 0000000000000088
    RBP: ffff880295143ae8 R08: 0000000000000000 R09: ffff88008f69bb30
    R10: 00000000fffffffa R11: 0000000000000000 R12: 0000000000000088
    R13: 0000000000000001 R14: ffff88041d099000 R15: ffff880084a205d0
    FS:  00007f8549374700(0000) GS:ffff88042f3c0000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 00000000000000a8 CR3: 000000033e1d5000 CR4: 00000000001406e0
    DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
    Stack:
     0000000000000000 ffffea000a526d40 ffff880084a20738 ffff880084a20750
     ffff880295143b48 ffffffff811cc91e ffff880000000000 0000000000000296
     0000000000000000 ffff880417090198 0000000000000000 ffffea000a526d40
    Call Trace:
     [<ffffffff811cc91e>] __test_set_page_writeback+0xde/0x1d0
     [<ffffffff813fee87>] do_write_data_page+0xe7/0x3a0
     [<ffffffff813faeea>] gc_data_segment+0x5aa/0x640
     [<ffffffff813fb0b8>] do_garbage_collect+0x138/0x150
     [<ffffffff813fb3fe>] f2fs_gc+0x1be/0x3e0
     [<ffffffff81405541>] f2fs_balance_fs+0x81/0x90
     [<ffffffff813ee357>] f2fs_unlink+0x47/0x1d0
     [<ffffffff81239329>] vfs_unlink+0x109/0x1b0
     [<ffffffff8123e3d7>] do_unlinkat+0x287/0x2c0
     [<ffffffff8123ebc6>] SyS_unlink+0x16/0x20
     [<ffffffff81942e2e>] entry_SYSCALL_64_fastpath+0x12/0x71
    Code: 41 5e 5d c3 0f 1f 00 66 2e 0f 1f 84 00 00 00 00 00 55 48 89 e5 41 55 49
    89 f5 41 54 49 89 fc 53 48 83 ec 08 65 ff 05 e6 d9 b6 7e <48> 8b 47 20 48 63 ca
    65 8b 18 48 63 db 48 01 f3 48 39 cb 7d 0a
    RIP  [<ffffffff8149deea>] __percpu_counter_add+0x1a/0x90
     RSP <ffff880295143ac8>
    CR2: 00000000000000a8
    ---[ end trace 5132449a58ed93a3 ]---
    note: gcc[10356] exited with preempt_count 2
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 9bedfa8dd3a5..f71e19a9dd3c 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -2072,8 +2072,6 @@ static int f2fs_set_data_page_dirty(struct page *page)
 		return 1;
 	}
 
-	mark_inode_dirty(inode);
-
 	if (!PageDirty(page)) {
 		__set_page_dirty_nobuffers(page);
 		update_dirty_page(inode, page);

commit 12377024719f08b7411afe9fc0169b13808dfefa
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Mon May 25 18:03:38 2015 +0800

    f2fs: avoid duplicated code by reusing f2fs_read_end_io
    
    This patch tries to clean up code because part code of f2fs_read_end_io
    and mpage_end_io are the same, so it's better to merge and reuse them.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 8d04e24a889f..9bedfa8dd3a5 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -34,29 +34,6 @@ static void f2fs_read_end_io(struct bio *bio, int err)
 	struct bio_vec *bvec;
 	int i;
 
-	bio_for_each_segment_all(bvec, bio, i) {
-		struct page *page = bvec->bv_page;
-
-		if (!err) {
-			SetPageUptodate(page);
-		} else {
-			ClearPageUptodate(page);
-			SetPageError(page);
-		}
-		unlock_page(page);
-	}
-	bio_put(bio);
-}
-
-/*
- * I/O completion handler for multipage BIOs.
- * copied from fs/mpage.c
- */
-static void mpage_end_io(struct bio *bio, int err)
-{
-	struct bio_vec *bv;
-	int i;
-
 	if (f2fs_bio_encrypted(bio)) {
 		if (err) {
 			f2fs_release_crypto_ctx(bio->bi_private);
@@ -66,8 +43,8 @@ static void mpage_end_io(struct bio *bio, int err)
 		}
 	}
 
-	bio_for_each_segment_all(bv, bio, i) {
-		struct page *page = bv->bv_page;
+	bio_for_each_segment_all(bvec, bio, i) {
+		struct page *page = bvec->bv_page;
 
 		if (!err) {
 			SetPageUptodate(page);
@@ -77,7 +54,6 @@ static void mpage_end_io(struct bio *bio, int err)
 		}
 		unlock_page(page);
 	}
-
 	bio_put(bio);
 }
 
@@ -122,7 +98,7 @@ static struct bio *__bio_alloc(struct f2fs_sb_info *sbi, block_t blk_addr,
 	bio->bi_bdev = sbi->sb->s_bdev;
 	bio->bi_iter.bi_sector = SECTOR_FROM_BLOCK(blk_addr);
 	bio->bi_end_io = is_read ? f2fs_read_end_io : f2fs_write_end_io;
-	bio->bi_private = sbi;
+	bio->bi_private = is_read ? NULL : sbi;
 
 	return bio;
 }
@@ -1584,7 +1560,7 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 			}
 			bio->bi_bdev = bdev;
 			bio->bi_iter.bi_sector = SECTOR_FROM_BLOCK(block_nr);
-			bio->bi_end_io = mpage_end_io;
+			bio->bi_end_io = f2fs_read_end_io;
 			bio->bi_private = ctx;
 		}
 

commit 4375a33664de17af9032b5f491a49bd256670927
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Apr 23 12:04:33 2015 -0700

    f2fs crypto: add encryption support in read/write paths
    
    This patch adds encryption support in read and write paths.
    
    Note that, in f2fs, we need to consider cleaning operation.
    In cleaning procedure, we must avoid encrypting and decrypting written blocks.
    So, this patch implements move_encrypted_block().
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 473b4d41c0c8..8d04e24a889f 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -57,6 +57,15 @@ static void mpage_end_io(struct bio *bio, int err)
 	struct bio_vec *bv;
 	int i;
 
+	if (f2fs_bio_encrypted(bio)) {
+		if (err) {
+			f2fs_release_crypto_ctx(bio->bi_private);
+		} else {
+			f2fs_end_io_crypto_work(bio->bi_private, bio);
+			return;
+		}
+	}
+
 	bio_for_each_segment_all(bv, bio, i) {
 		struct page *page = bv->bv_page;
 
@@ -81,6 +90,8 @@ static void f2fs_write_end_io(struct bio *bio, int err)
 	bio_for_each_segment_all(bvec, bio, i) {
 		struct page *page = bvec->bv_page;
 
+		f2fs_restore_and_release_control_page(&page);
+
 		if (unlikely(err)) {
 			set_page_dirty(page);
 			set_bit(AS_EIO, &page->mapping->flags);
@@ -161,7 +172,7 @@ void f2fs_submit_merged_bio(struct f2fs_sb_info *sbi,
 int f2fs_submit_page_bio(struct f2fs_io_info *fio)
 {
 	struct bio *bio;
-	struct page *page = fio->page;
+	struct page *page = fio->encrypted_page ? fio->encrypted_page : fio->page;
 
 	trace_f2fs_submit_page_bio(page, fio);
 	f2fs_trace_ios(fio, 0);
@@ -185,6 +196,7 @@ void f2fs_submit_page_mbio(struct f2fs_io_info *fio)
 	enum page_type btype = PAGE_TYPE_OF_BIO(fio->type);
 	struct f2fs_bio_info *io;
 	bool is_read = is_read_io(fio->rw);
+	struct page *bio_page;
 
 	io = is_read ? &sbi->read_io : &sbi->write_io[btype];
 
@@ -206,7 +218,9 @@ void f2fs_submit_page_mbio(struct f2fs_io_info *fio)
 		io->fio = *fio;
 	}
 
-	if (bio_add_page(io->bio, fio->page, PAGE_CACHE_SIZE, 0) <
+	bio_page = fio->encrypted_page ? fio->encrypted_page : fio->page;
+
+	if (bio_add_page(io->bio, bio_page, PAGE_CACHE_SIZE, 0) <
 							PAGE_CACHE_SIZE) {
 		__submit_merged_bio(io);
 		goto alloc_new;
@@ -928,8 +942,12 @@ struct page *get_read_data_page(struct inode *inode, pgoff_t index, int rw)
 		.sbi = F2FS_I_SB(inode),
 		.type = DATA,
 		.rw = rw,
+		.encrypted_page = NULL,
 	};
 
+	if (f2fs_encrypted_inode(inode) && S_ISREG(inode->i_mode))
+		return read_mapping_page(mapping, index, NULL);
+
 	page = grab_cache_page(mapping, index);
 	if (!page)
 		return ERR_PTR(-ENOMEM);
@@ -1066,26 +1084,14 @@ struct page *get_new_data_page(struct inode *inode,
 		zero_user_segment(page, 0, PAGE_CACHE_SIZE);
 		SetPageUptodate(page);
 	} else {
-		struct f2fs_io_info fio = {
-			.sbi = F2FS_I_SB(inode),
-			.type = DATA,
-			.rw = READ_SYNC,
-			.blk_addr = dn.data_blkaddr,
-			.page = page,
-		};
-		err = f2fs_submit_page_bio(&fio);
-		if (err)
-			return ERR_PTR(err);
+		f2fs_put_page(page, 1);
 
-		lock_page(page);
-		if (unlikely(!PageUptodate(page))) {
-			f2fs_put_page(page, 1);
-			return ERR_PTR(-EIO);
-		}
-		if (unlikely(page->mapping != mapping)) {
-			f2fs_put_page(page, 1);
+		page = get_read_data_page(inode, index, READ_SYNC);
+		if (IS_ERR(page))
 			goto repeat;
-		}
+
+		/* wait for read completion */
+		lock_page(page);
 	}
 got_it:
 	if (new_i_size &&
@@ -1548,14 +1554,38 @@ static int f2fs_mpage_readpages(struct address_space *mapping,
 			bio = NULL;
 		}
 		if (bio == NULL) {
+			struct f2fs_crypto_ctx *ctx = NULL;
+
+			if (f2fs_encrypted_inode(inode) &&
+					S_ISREG(inode->i_mode)) {
+				struct page *cpage;
+
+				ctx = f2fs_get_crypto_ctx(inode);
+				if (IS_ERR(ctx))
+					goto set_error_page;
+
+				/* wait the page to be moved by cleaning */
+				cpage = find_lock_page(
+						META_MAPPING(F2FS_I_SB(inode)),
+						block_nr);
+				if (cpage) {
+					f2fs_wait_on_page_writeback(cpage,
+									DATA);
+					f2fs_put_page(cpage, 1);
+				}
+			}
+
 			bio = bio_alloc(GFP_KERNEL,
 				min_t(int, nr_pages, bio_get_nr_vecs(bdev)));
-			if (!bio)
+			if (!bio) {
+				if (ctx)
+					f2fs_release_crypto_ctx(ctx);
 				goto set_error_page;
+			}
 			bio->bi_bdev = bdev;
 			bio->bi_iter.bi_sector = SECTOR_FROM_BLOCK(block_nr);
 			bio->bi_end_io = mpage_end_io;
-			bio->bi_private = NULL;
+			bio->bi_private = ctx;
 		}
 
 		if (bio_add_page(bio, page, blocksize, 0) < blocksize)
@@ -1632,6 +1662,14 @@ int do_write_data_page(struct f2fs_io_info *fio)
 		goto out_writepage;
 	}
 
+	if (f2fs_encrypted_inode(inode) && S_ISREG(inode->i_mode)) {
+		fio->encrypted_page = f2fs_encrypt(inode, fio->page);
+		if (IS_ERR(fio->encrypted_page)) {
+			err = PTR_ERR(fio->encrypted_page);
+			goto out_writepage;
+		}
+	}
+
 	set_page_writeback(page);
 
 	/*
@@ -1674,6 +1712,7 @@ static int f2fs_write_data_page(struct page *page,
 		.type = DATA,
 		.rw = (wbc->sync_mode == WB_SYNC_ALL) ? WRITE_SYNC : WRITE,
 		.page = page,
+		.encrypted_page = NULL,
 	};
 
 	trace_f2fs_writepage(page, DATA);
@@ -1897,6 +1936,7 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 			.rw = READ_SYNC,
 			.blk_addr = dn.data_blkaddr,
 			.page = page,
+			.encrypted_page = NULL,
 		};
 		err = f2fs_submit_page_bio(&fio);
 		if (err)
@@ -1912,6 +1952,15 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 			f2fs_put_page(page, 1);
 			goto repeat;
 		}
+
+		/* avoid symlink page */
+		if (f2fs_encrypted_inode(inode) && S_ISREG(inode->i_mode)) {
+			err = f2fs_decrypt_one(inode, page);
+			if (err) {
+				f2fs_put_page(page, 1);
+				goto fail;
+			}
+		}
 	}
 out:
 	SetPageUptodate(page);

commit fcc85a4d86b5018f08717160c89c0eb50afd1dca
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Tue Apr 21 20:39:58 2015 -0700

    f2fs crypto: activate encryption support for fs APIs
    
    This patch activates the following APIs for encryption support.
    
    The rules quoted by ext4 are:
     - An unencrypted directory may contain encrypted or unencrypted files
       or directories.
     - All files or directories in a directory must be protected using the
       same key as their containing directory.
     - Encrypted inode for regular file should not have inline_data.
     - Encrypted symlink and directory may have inline_data and inline_dentry.
    
    This patch activates the following APIs.
    1. f2fs_link              : validate context
    2. f2fs_lookup            :      ''
    3. f2fs_rename            :      ''
    4. f2fs_create/f2fs_mkdir : inherit its dir's context
    5. f2fs_direct_IO         : do buffered io for regular files
    6. f2fs_open              : check encryption info
    7. f2fs_file_mmap         :      ''
    8. f2fs_setattr           :      ''
    9. f2fs_file_write_iter   :      ''           (Called by sys_io_submit)
    10. f2fs_fallocate        : do not support fcollapse
    11. f2fs_evict_inode      : free_encryption_info
    
    Signed-off-by: Michael Halcrow <mhalcrow@google.com>
    Signed-off-by: Theodore Ts'o <tytso@mit.edu>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 842fcdd9d226..473b4d41c0c8 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1982,6 +1982,9 @@ static ssize_t f2fs_direct_IO(struct kiocb *iocb, struct iov_iter *iter,
 			return err;
 	}
 
+	if (f2fs_encrypted_inode(inode) && S_ISREG(inode->i_mode))
+		return 0;
+
 	if (check_direct_IO(inode, iter, offset))
 		return 0;
 

commit 7f63eb77af7bd7580b27b0c3f249f7efaa5c63ae
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri May 8 19:30:32 2015 -0700

    f2fs: report unwritten area in f2fs_fiemap
    
    This patch slightly changes f2fs_fiemap function to report unwritten area.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 3b762611ff6d..842fcdd9d226 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1241,6 +1241,8 @@ static int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 	if (dn.data_blkaddr != NULL_ADDR) {
 		map->m_flags = F2FS_MAP_MAPPED;
 		map->m_pblk = dn.data_blkaddr;
+		if (dn.data_blkaddr == NEW_ADDR)
+			map->m_flags |= F2FS_MAP_UNWRITTEN;
 	} else if (create) {
 		err = __allocate_data_block(&dn);
 		if (err)
@@ -1288,7 +1290,10 @@ static int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 			blkaddr = dn.data_blkaddr;
 		}
 		/* Give more consecutive addresses for the readahead */
-		if (map->m_pblk != NEW_ADDR && blkaddr == (map->m_pblk + ofs)) {
+		if ((map->m_pblk != NEW_ADDR &&
+				blkaddr == (map->m_pblk + ofs)) ||
+				(map->m_pblk == NEW_ADDR &&
+				blkaddr == NEW_ADDR)) {
 			ofs++;
 			dn.ofs_in_node++;
 			pgofs++;
@@ -1339,11 +1344,117 @@ static int get_data_block_fiemap(struct inode *inode, sector_t iblock,
 	return __get_data_block(inode, iblock, bh_result, create, true);
 }
 
+static inline sector_t logical_to_blk(struct inode *inode, loff_t offset)
+{
+	return (offset >> inode->i_blkbits);
+}
+
+static inline loff_t blk_to_logical(struct inode *inode, sector_t blk)
+{
+	return (blk << inode->i_blkbits);
+}
+
 int f2fs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,
 		u64 start, u64 len)
 {
-	return generic_block_fiemap(inode, fieinfo,
-				start, len, get_data_block_fiemap);
+	struct buffer_head map_bh;
+	sector_t start_blk, last_blk;
+	loff_t isize = i_size_read(inode);
+	u64 logical = 0, phys = 0, size = 0;
+	u32 flags = 0;
+	bool past_eof = false, whole_file = false;
+	int ret = 0;
+
+	ret = fiemap_check_flags(fieinfo, FIEMAP_FLAG_SYNC);
+	if (ret)
+		return ret;
+
+	mutex_lock(&inode->i_mutex);
+
+	if (len >= isize) {
+		whole_file = true;
+		len = isize;
+	}
+
+	if (logical_to_blk(inode, len) == 0)
+		len = blk_to_logical(inode, 1);
+
+	start_blk = logical_to_blk(inode, start);
+	last_blk = logical_to_blk(inode, start + len - 1);
+next:
+	memset(&map_bh, 0, sizeof(struct buffer_head));
+	map_bh.b_size = len;
+
+	ret = get_data_block_fiemap(inode, start_blk, &map_bh, 0);
+	if (ret)
+		goto out;
+
+	/* HOLE */
+	if (!buffer_mapped(&map_bh)) {
+		start_blk++;
+
+		if (!past_eof && blk_to_logical(inode, start_blk) >= isize)
+			past_eof = 1;
+
+		if (past_eof && size) {
+			flags |= FIEMAP_EXTENT_LAST;
+			ret = fiemap_fill_next_extent(fieinfo, logical,
+					phys, size, flags);
+		} else if (size) {
+			ret = fiemap_fill_next_extent(fieinfo, logical,
+					phys, size, flags);
+			size = 0;
+		}
+
+		/* if we have holes up to/past EOF then we're done */
+		if (start_blk > last_blk || past_eof || ret)
+			goto out;
+	} else {
+		if (start_blk > last_blk && !whole_file) {
+			ret = fiemap_fill_next_extent(fieinfo, logical,
+					phys, size, flags);
+			goto out;
+		}
+
+		/*
+		 * if size != 0 then we know we already have an extent
+		 * to add, so add it.
+		 */
+		if (size) {
+			ret = fiemap_fill_next_extent(fieinfo, logical,
+					phys, size, flags);
+			if (ret)
+				goto out;
+		}
+
+		logical = blk_to_logical(inode, start_blk);
+		phys = blk_to_logical(inode, map_bh.b_blocknr);
+		size = map_bh.b_size;
+		flags = 0;
+		if (buffer_unwritten(&map_bh))
+			flags = FIEMAP_EXTENT_UNWRITTEN;
+
+		start_blk += logical_to_blk(inode, size);
+
+		/*
+		 * If we are past the EOF, then we need to make sure as
+		 * soon as we find a hole that the last extent we found
+		 * is marked with FIEMAP_EXTENT_LAST
+		 */
+		if (!past_eof && logical + size >= isize)
+			past_eof = true;
+	}
+	cond_resched();
+	if (fatal_signal_pending(current))
+		ret = -EINTR;
+	else
+		goto next;
+out:
+	if (ret == 1)
+		ret = 0;
+
+	mutex_unlock(&inode->i_mutex);
+	return ret;
 }
 
 /*

commit 43f3eae1d3b1de6a4f7e39ef9c363ec6f8b9c8d4
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Apr 30 17:00:33 2015 -0700

    f2fs: split find_data_page according to specific purposes
    
    This patch splits find_data_page as follows.
    
    1. f2fs_gc
     - use get_read_data_page() with read only
    
    2. find_in_level
     - use find_data_page without locked page
    
    3. truncate_partial_page
     - In the case cache_only mode, just drop cached page.
     - Ohterwise, use get_lock_data_page() and guarantee to truncate
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 9ba30b435a5a..3b762611ff6d 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -917,7 +917,7 @@ void f2fs_update_extent_cache(struct dnode_of_data *dn)
 		sync_inode_page(dn);
 }
 
-struct page *find_data_page(struct inode *inode, pgoff_t index, bool sync)
+struct page *get_read_data_page(struct inode *inode, pgoff_t index, int rw)
 {
 	struct address_space *mapping = inode->i_mapping;
 	struct dnode_of_data dn;
@@ -927,84 +927,9 @@ struct page *find_data_page(struct inode *inode, pgoff_t index, bool sync)
 	struct f2fs_io_info fio = {
 		.sbi = F2FS_I_SB(inode),
 		.type = DATA,
-		.rw = sync ? READ_SYNC : READA,
+		.rw = rw,
 	};
 
-	/*
-	 * If sync is false, it needs to check its block allocation.
-	 * This is need and triggered by two flows:
-	 *   gc and truncate_partial_data_page.
-	 */
-	if (!sync)
-		goto search;
-
-	page = find_get_page(mapping, index);
-	if (page && PageUptodate(page))
-		return page;
-	f2fs_put_page(page, 0);
-search:
-	if (f2fs_lookup_extent_cache(inode, index, &ei)) {
-		dn.data_blkaddr = ei.blk + index - ei.fofs;
-		goto got_it;
-	}
-
-	set_new_dnode(&dn, inode, NULL, NULL, 0);
-	err = get_dnode_of_data(&dn, index, LOOKUP_NODE);
-	if (err)
-		return ERR_PTR(err);
-	f2fs_put_dnode(&dn);
-
-	if (dn.data_blkaddr == NULL_ADDR)
-		return ERR_PTR(-ENOENT);
-
-	/* By fallocate(), there is no cached page, but with NEW_ADDR */
-	if (unlikely(dn.data_blkaddr == NEW_ADDR))
-		return ERR_PTR(-EINVAL);
-
-got_it:
-	page = grab_cache_page(mapping, index);
-	if (!page)
-		return ERR_PTR(-ENOMEM);
-
-	if (PageUptodate(page)) {
-		unlock_page(page);
-		return page;
-	}
-
-	fio.blk_addr = dn.data_blkaddr;
-	fio.page = page;
-	err = f2fs_submit_page_bio(&fio);
-	if (err)
-		return ERR_PTR(err);
-
-	if (sync) {
-		wait_on_page_locked(page);
-		if (unlikely(!PageUptodate(page))) {
-			f2fs_put_page(page, 0);
-			return ERR_PTR(-EIO);
-		}
-	}
-	return page;
-}
-
-/*
- * If it tries to access a hole, return an error.
- * Because, the callers, functions in dir.c and GC, should be able to know
- * whether this page exists or not.
- */
-struct page *get_lock_data_page(struct inode *inode, pgoff_t index)
-{
-	struct address_space *mapping = inode->i_mapping;
-	struct dnode_of_data dn;
-	struct page *page;
-	struct extent_info ei;
-	int err;
-	struct f2fs_io_info fio = {
-		.sbi = F2FS_I_SB(inode),
-		.type = DATA,
-		.rw = READ_SYNC,
-	};
-repeat:
 	page = grab_cache_page(mapping, index);
 	if (!page)
 		return ERR_PTR(-ENOMEM);
@@ -1026,10 +951,11 @@ struct page *get_lock_data_page(struct inode *inode, pgoff_t index)
 		f2fs_put_page(page, 1);
 		return ERR_PTR(-ENOENT);
 	}
-
 got_it:
-	if (PageUptodate(page))
+	if (PageUptodate(page)) {
+		unlock_page(page);
 		return page;
+	}
 
 	/*
 	 * A new dentry page is allocated but not able to be written, since its
@@ -1040,6 +966,7 @@ struct page *get_lock_data_page(struct inode *inode, pgoff_t index)
 	if (dn.data_blkaddr == NEW_ADDR) {
 		zero_user_segment(page, 0, PAGE_CACHE_SIZE);
 		SetPageUptodate(page);
+		unlock_page(page);
 		return page;
 	}
 
@@ -1048,7 +975,49 @@ struct page *get_lock_data_page(struct inode *inode, pgoff_t index)
 	err = f2fs_submit_page_bio(&fio);
 	if (err)
 		return ERR_PTR(err);
+	return page;
+}
+
+struct page *find_data_page(struct inode *inode, pgoff_t index)
+{
+	struct address_space *mapping = inode->i_mapping;
+	struct page *page;
+
+	page = find_get_page(mapping, index);
+	if (page && PageUptodate(page))
+		return page;
+	f2fs_put_page(page, 0);
+
+	page = get_read_data_page(inode, index, READ_SYNC);
+	if (IS_ERR(page))
+		return page;
+
+	if (PageUptodate(page))
+		return page;
+
+	wait_on_page_locked(page);
+	if (unlikely(!PageUptodate(page))) {
+		f2fs_put_page(page, 0);
+		return ERR_PTR(-EIO);
+	}
+	return page;
+}
+
+/*
+ * If it tries to access a hole, return an error.
+ * Because, the callers, functions in dir.c and GC, should be able to know
+ * whether this page exists or not.
+ */
+struct page *get_lock_data_page(struct inode *inode, pgoff_t index)
+{
+	struct address_space *mapping = inode->i_mapping;
+	struct page *page;
+repeat:
+	page = get_read_data_page(inode, index, READ_SYNC);
+	if (IS_ERR(page))
+		return page;
 
+	/* wait for read completion */
 	lock_page(page);
 	if (unlikely(!PageUptodate(page))) {
 		f2fs_put_page(page, 1);

commit 01f28610a1691078d0f7ba62b365567f8799f07c
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed Apr 29 11:18:42 2015 -0700

    f2fs: fix race on allocating and deallocating a dentry block
    
    There are two threads:
     f2fs_delete_entry()              get_new_data_page()
                                      f2fs_reserve_block()
                                      dn.blkaddr = XXX
     lock_page(dentry_block)
     truncate_hole()
     dn.blkaddr = NULL
     unlock_page(dentry_block)
                                      lock_page(dentry_block)
                                      fill the block from XXX address
                                      add new dentries
                                      unlock_page(dentry_block)
    
    Later, f2fs_write_data_page() will truncate the dentry_block, since
    its block address is NULL.
    
    The reason for this was due to the wrong lock order.
    In this case, we should do f2fs_reserve_block() after locking its dentry block.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 81d1fd581078..9ba30b435a5a 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1076,20 +1076,22 @@ struct page *get_new_data_page(struct inode *inode,
 	struct page *page;
 	struct dnode_of_data dn;
 	int err;
+repeat:
+	page = grab_cache_page(mapping, index);
+	if (!page)
+		return ERR_PTR(-ENOMEM);
 
 	set_new_dnode(&dn, inode, ipage, NULL, 0);
 	err = f2fs_reserve_block(&dn, index);
-	if (err)
+	if (err) {
+		f2fs_put_page(page, 1);
 		return ERR_PTR(err);
-repeat:
-	page = grab_cache_page(mapping, index);
-	if (!page) {
-		err = -ENOMEM;
-		goto put_err;
 	}
+	if (!ipage)
+		f2fs_put_dnode(&dn);
 
 	if (PageUptodate(page))
-		return page;
+		goto got_it;
 
 	if (dn.data_blkaddr == NEW_ADDR) {
 		zero_user_segment(page, 0, PAGE_CACHE_SIZE);
@@ -1104,20 +1106,19 @@ struct page *get_new_data_page(struct inode *inode,
 		};
 		err = f2fs_submit_page_bio(&fio);
 		if (err)
-			goto put_err;
+			return ERR_PTR(err);
 
 		lock_page(page);
 		if (unlikely(!PageUptodate(page))) {
 			f2fs_put_page(page, 1);
-			err = -EIO;
-			goto put_err;
+			return ERR_PTR(-EIO);
 		}
 		if (unlikely(page->mapping != mapping)) {
 			f2fs_put_page(page, 1);
 			goto repeat;
 		}
 	}
-
+got_it:
 	if (new_i_size &&
 		i_size_read(inode) < ((index + 1) << PAGE_CACHE_SHIFT)) {
 		i_size_write(inode, ((index + 1) << PAGE_CACHE_SHIFT));
@@ -1125,10 +1126,6 @@ struct page *get_new_data_page(struct inode *inode,
 		set_inode_flag(F2FS_I(inode), FI_UPDATE_DIR);
 	}
 	return page;
-
-put_err:
-	f2fs_put_dnode(&dn);
-	return ERR_PTR(err);
 }
 
 static int __allocate_data_block(struct dnode_of_data *dn)

commit 05ca3632e5a73b493b27ec3e2a337885563abff0
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Apr 23 14:38:15 2015 -0700

    f2fs: add sbi and page pointer in f2fs_io_info
    
    This patch adds f2fs_sb_info and page pointers in f2fs_io_info structure.
    With this change, we can reduce a lot of parameters for IO functions.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 2a3a9cd008da..81d1fd581078 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -158,16 +158,16 @@ void f2fs_submit_merged_bio(struct f2fs_sb_info *sbi,
  * Fill the locked page with data located in the block address.
  * Return unlocked page.
  */
-int f2fs_submit_page_bio(struct f2fs_sb_info *sbi, struct page *page,
-					struct f2fs_io_info *fio)
+int f2fs_submit_page_bio(struct f2fs_io_info *fio)
 {
 	struct bio *bio;
+	struct page *page = fio->page;
 
 	trace_f2fs_submit_page_bio(page, fio);
-	f2fs_trace_ios(page, fio, 0);
+	f2fs_trace_ios(fio, 0);
 
 	/* Allocate a new bio */
-	bio = __bio_alloc(sbi, fio->blk_addr, 1, is_read_io(fio->rw));
+	bio = __bio_alloc(fio->sbi, fio->blk_addr, 1, is_read_io(fio->rw));
 
 	if (bio_add_page(bio, page, PAGE_CACHE_SIZE, 0) < PAGE_CACHE_SIZE) {
 		bio_put(bio);
@@ -179,9 +179,9 @@ int f2fs_submit_page_bio(struct f2fs_sb_info *sbi, struct page *page,
 	return 0;
 }
 
-void f2fs_submit_page_mbio(struct f2fs_sb_info *sbi, struct page *page,
-					struct f2fs_io_info *fio)
+void f2fs_submit_page_mbio(struct f2fs_io_info *fio)
 {
+	struct f2fs_sb_info *sbi = fio->sbi;
 	enum page_type btype = PAGE_TYPE_OF_BIO(fio->type);
 	struct f2fs_bio_info *io;
 	bool is_read = is_read_io(fio->rw);
@@ -206,17 +206,17 @@ void f2fs_submit_page_mbio(struct f2fs_sb_info *sbi, struct page *page,
 		io->fio = *fio;
 	}
 
-	if (bio_add_page(io->bio, page, PAGE_CACHE_SIZE, 0) <
+	if (bio_add_page(io->bio, fio->page, PAGE_CACHE_SIZE, 0) <
 							PAGE_CACHE_SIZE) {
 		__submit_merged_bio(io);
 		goto alloc_new;
 	}
 
 	io->last_block_in_bio = fio->blk_addr;
-	f2fs_trace_ios(page, fio, 0);
+	f2fs_trace_ios(fio, 0);
 
 	up_write(&io->io_rwsem);
-	trace_f2fs_submit_page_mbio(page, fio);
+	trace_f2fs_submit_page_mbio(fio->page, fio);
 }
 
 /*
@@ -925,6 +925,7 @@ struct page *find_data_page(struct inode *inode, pgoff_t index, bool sync)
 	struct extent_info ei;
 	int err;
 	struct f2fs_io_info fio = {
+		.sbi = F2FS_I_SB(inode),
 		.type = DATA,
 		.rw = sync ? READ_SYNC : READA,
 	};
@@ -971,7 +972,8 @@ struct page *find_data_page(struct inode *inode, pgoff_t index, bool sync)
 	}
 
 	fio.blk_addr = dn.data_blkaddr;
-	err = f2fs_submit_page_bio(F2FS_I_SB(inode), page, &fio);
+	fio.page = page;
+	err = f2fs_submit_page_bio(&fio);
 	if (err)
 		return ERR_PTR(err);
 
@@ -998,6 +1000,7 @@ struct page *get_lock_data_page(struct inode *inode, pgoff_t index)
 	struct extent_info ei;
 	int err;
 	struct f2fs_io_info fio = {
+		.sbi = F2FS_I_SB(inode),
 		.type = DATA,
 		.rw = READ_SYNC,
 	};
@@ -1041,7 +1044,8 @@ struct page *get_lock_data_page(struct inode *inode, pgoff_t index)
 	}
 
 	fio.blk_addr = dn.data_blkaddr;
-	err = f2fs_submit_page_bio(F2FS_I_SB(inode), page, &fio);
+	fio.page = page;
+	err = f2fs_submit_page_bio(&fio);
 	if (err)
 		return ERR_PTR(err);
 
@@ -1092,11 +1096,13 @@ struct page *get_new_data_page(struct inode *inode,
 		SetPageUptodate(page);
 	} else {
 		struct f2fs_io_info fio = {
+			.sbi = F2FS_I_SB(inode),
 			.type = DATA,
 			.rw = READ_SYNC,
 			.blk_addr = dn.data_blkaddr,
+			.page = page,
 		};
-		err = f2fs_submit_page_bio(F2FS_I_SB(inode), page, &fio);
+		err = f2fs_submit_page_bio(&fio);
 		if (err)
 			goto put_err;
 
@@ -1529,8 +1535,9 @@ static int f2fs_read_data_pages(struct file *file,
 	return f2fs_mpage_readpages(mapping, pages, NULL, nr_pages);
 }
 
-int do_write_data_page(struct page *page, struct f2fs_io_info *fio)
+int do_write_data_page(struct f2fs_io_info *fio)
 {
+	struct page *page = fio->page;
 	struct inode *inode = page->mapping->host;
 	struct dnode_of_data dn;
 	int err = 0;
@@ -1557,11 +1564,11 @@ int do_write_data_page(struct page *page, struct f2fs_io_info *fio)
 	if (unlikely(fio->blk_addr != NEW_ADDR &&
 			!is_cold_data(page) &&
 			need_inplace_update(inode))) {
-		rewrite_data_page(page, fio);
+		rewrite_data_page(fio);
 		set_inode_flag(F2FS_I(inode), FI_UPDATE_WRITE);
 		trace_f2fs_do_write_data_page(page, IPU);
 	} else {
-		write_data_page(page, &dn, fio);
+		write_data_page(&dn, fio);
 		set_data_blkaddr(&dn);
 		f2fs_update_extent_cache(&dn);
 		trace_f2fs_do_write_data_page(page, OPU);
@@ -1586,8 +1593,10 @@ static int f2fs_write_data_page(struct page *page,
 	bool need_balance_fs = false;
 	int err = 0;
 	struct f2fs_io_info fio = {
+		.sbi = sbi,
 		.type = DATA,
 		.rw = (wbc->sync_mode == WB_SYNC_ALL) ? WRITE_SYNC : WRITE,
+		.page = page,
 	};
 
 	trace_f2fs_writepage(page, DATA);
@@ -1617,7 +1626,7 @@ static int f2fs_write_data_page(struct page *page,
 	if (S_ISDIR(inode->i_mode)) {
 		if (unlikely(f2fs_cp_error(sbi)))
 			goto redirty_out;
-		err = do_write_data_page(page, &fio);
+		err = do_write_data_page(&fio);
 		goto done;
 	}
 
@@ -1637,7 +1646,7 @@ static int f2fs_write_data_page(struct page *page,
 	if (f2fs_has_inline_data(inode))
 		err = f2fs_write_inline_data(inode, page);
 	if (err == -EAGAIN)
-		err = do_write_data_page(page, &fio);
+		err = do_write_data_page(&fio);
 	f2fs_unlock_op(sbi);
 done:
 	if (err && err != -ENOENT)
@@ -1806,11 +1815,13 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 		zero_user_segment(page, 0, PAGE_CACHE_SIZE);
 	} else {
 		struct f2fs_io_info fio = {
+			.sbi = sbi,
 			.type = DATA,
 			.rw = READ_SYNC,
 			.blk_addr = dn.data_blkaddr,
+			.page = page,
 		};
-		err = f2fs_submit_page_bio(sbi, page, &fio);
+		err = f2fs_submit_page_bio(&fio);
 		if (err)
 			goto fail;
 

commit f1e8866016b53bd0ea108ae9adbb52da1dd53dab
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Apr 9 11:20:42 2015 -0700

    f2fs: expose f2fs_mpage_readpages
    
    This patch implements f2fs_mpage_readpages for further optimization on
    encryption support.
    
    The basic code was taken from fs/mpage.c, and changed to be simple by adjusting
    that block_size is equal to page_size in f2fs.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index aa3c079ffd94..2a3a9cd008da 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -18,6 +18,7 @@
 #include <linux/bio.h>
 #include <linux/prefetch.h>
 #include <linux/uio.h>
+#include <linux/cleancache.h>
 
 #include "f2fs.h"
 #include "node.h"
@@ -47,6 +48,30 @@ static void f2fs_read_end_io(struct bio *bio, int err)
 	bio_put(bio);
 }
 
+/*
+ * I/O completion handler for multipage BIOs.
+ * copied from fs/mpage.c
+ */
+static void mpage_end_io(struct bio *bio, int err)
+{
+	struct bio_vec *bv;
+	int i;
+
+	bio_for_each_segment_all(bv, bio, i) {
+		struct page *page = bv->bv_page;
+
+		if (!err) {
+			SetPageUptodate(page);
+		} else {
+			ClearPageUptodate(page);
+			SetPageError(page);
+		}
+		unlock_page(page);
+	}
+
+	bio_put(bio);
+}
+
 static void f2fs_write_end_io(struct bio *bio, int err)
 {
 	struct f2fs_sb_info *sbi = bio->bi_private;
@@ -1349,6 +1374,133 @@ int f2fs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,
 				start, len, get_data_block_fiemap);
 }
 
+/*
+ * This function was originally taken from fs/mpage.c, and customized for f2fs.
+ * Major change was from block_size == page_size in f2fs by default.
+ */
+static int f2fs_mpage_readpages(struct address_space *mapping,
+			struct list_head *pages, struct page *page,
+			unsigned nr_pages)
+{
+	struct bio *bio = NULL;
+	unsigned page_idx;
+	sector_t last_block_in_bio = 0;
+	struct inode *inode = mapping->host;
+	const unsigned blkbits = inode->i_blkbits;
+	const unsigned blocksize = 1 << blkbits;
+	sector_t block_in_file;
+	sector_t last_block;
+	sector_t last_block_in_file;
+	sector_t block_nr;
+	struct block_device *bdev = inode->i_sb->s_bdev;
+	struct f2fs_map_blocks map;
+
+	map.m_pblk = 0;
+	map.m_lblk = 0;
+	map.m_len = 0;
+	map.m_flags = 0;
+
+	for (page_idx = 0; nr_pages; page_idx++, nr_pages--) {
+
+		prefetchw(&page->flags);
+		if (pages) {
+			page = list_entry(pages->prev, struct page, lru);
+			list_del(&page->lru);
+			if (add_to_page_cache_lru(page, mapping,
+						  page->index, GFP_KERNEL))
+				goto next_page;
+		}
+
+		block_in_file = (sector_t)page->index;
+		last_block = block_in_file + nr_pages;
+		last_block_in_file = (i_size_read(inode) + blocksize - 1) >>
+								blkbits;
+		if (last_block > last_block_in_file)
+			last_block = last_block_in_file;
+
+		/*
+		 * Map blocks using the previous result first.
+		 */
+		if ((map.m_flags & F2FS_MAP_MAPPED) &&
+				block_in_file > map.m_lblk &&
+				block_in_file < (map.m_lblk + map.m_len))
+			goto got_it;
+
+		/*
+		 * Then do more f2fs_map_blocks() calls until we are
+		 * done with this page.
+		 */
+		map.m_flags = 0;
+
+		if (block_in_file < last_block) {
+			map.m_lblk = block_in_file;
+			map.m_len = last_block - block_in_file;
+
+			if (f2fs_map_blocks(inode, &map, 0, false))
+				goto set_error_page;
+		}
+got_it:
+		if ((map.m_flags & F2FS_MAP_MAPPED)) {
+			block_nr = map.m_pblk + block_in_file - map.m_lblk;
+			SetPageMappedToDisk(page);
+
+			if (!PageUptodate(page) && !cleancache_get_page(page)) {
+				SetPageUptodate(page);
+				goto confused;
+			}
+		} else {
+			zero_user_segment(page, 0, PAGE_CACHE_SIZE);
+			SetPageUptodate(page);
+			unlock_page(page);
+			goto next_page;
+		}
+
+		/*
+		 * This page will go to BIO.  Do we need to send this
+		 * BIO off first?
+		 */
+		if (bio && (last_block_in_bio != block_nr - 1)) {
+submit_and_realloc:
+			submit_bio(READ, bio);
+			bio = NULL;
+		}
+		if (bio == NULL) {
+			bio = bio_alloc(GFP_KERNEL,
+				min_t(int, nr_pages, bio_get_nr_vecs(bdev)));
+			if (!bio)
+				goto set_error_page;
+			bio->bi_bdev = bdev;
+			bio->bi_iter.bi_sector = SECTOR_FROM_BLOCK(block_nr);
+			bio->bi_end_io = mpage_end_io;
+			bio->bi_private = NULL;
+		}
+
+		if (bio_add_page(bio, page, blocksize, 0) < blocksize)
+			goto submit_and_realloc;
+
+		last_block_in_bio = block_nr;
+		goto next_page;
+set_error_page:
+		SetPageError(page);
+		zero_user_segment(page, 0, PAGE_CACHE_SIZE);
+		unlock_page(page);
+		goto next_page;
+confused:
+		if (bio) {
+			submit_bio(READ, bio);
+			bio = NULL;
+		}
+		unlock_page(page);
+next_page:
+		if (pages)
+			page_cache_release(page);
+	}
+	BUG_ON(pages && !list_empty(pages));
+	if (bio)
+		submit_bio(READ, bio);
+	return 0;
+}
+
 static int f2fs_read_data_page(struct file *file, struct page *page)
 {
 	struct inode *inode = page->mapping->host;
@@ -1360,8 +1512,7 @@ static int f2fs_read_data_page(struct file *file, struct page *page)
 	if (f2fs_has_inline_data(inode))
 		ret = f2fs_read_inline_data(inode, page);
 	if (ret == -EAGAIN)
-		ret = mpage_readpage(page, get_data_block);
-
+		ret = f2fs_mpage_readpages(page->mapping, NULL, page, 1);
 	return ret;
 }
 
@@ -1375,7 +1526,7 @@ static int f2fs_read_data_pages(struct file *file,
 	if (f2fs_has_inline_data(inode))
 		return 0;
 
-	return mpage_readpages(mapping, pages, nr_pages, get_data_block);
+	return f2fs_mpage_readpages(mapping, pages, NULL, nr_pages);
 }
 
 int do_write_data_page(struct page *page, struct f2fs_io_info *fio)

commit 003a3e1d60b0bb5cfb4feffb05a2083db2346364
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Mon Apr 6 19:55:34 2015 -0700

    f2fs: add f2fs_map_blocks
    
    This patch introduces f2fs_map_blocks structure likewise ext4_map_blocks.
    Now, f2fs uses f2fs_map_blocks when handling get_block.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 1e1aae669fa8..aa3c079ffd94 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -251,19 +251,6 @@ int f2fs_reserve_block(struct dnode_of_data *dn, pgoff_t index)
 	return err;
 }
 
-static void f2fs_map_bh(struct super_block *sb, pgoff_t pgofs,
-			struct extent_info *ei, struct buffer_head *bh_result)
-{
-	unsigned int blkbits = sb->s_blocksize_bits;
-	size_t max_size = bh_result->b_size;
-	size_t mapped_size;
-
-	clear_buffer_new(bh_result);
-	map_bh(bh_result, sb, ei->blk + pgofs - ei->fofs);
-	mapped_size = (ei->fofs + ei->len - pgofs) << blkbits;
-	bh_result->b_size = min(max_size, mapped_size);
-}
-
 static bool lookup_extent_info(struct inode *inode, pgoff_t pgofs,
 							struct extent_info *ei)
 {
@@ -1208,18 +1195,18 @@ static void __allocate_data_blocks(struct inode *inode, loff_t offset,
 }
 
 /*
- * get_data_block() now supported readahead/bmap/rw direct_IO with mapped bh.
+ * f2fs_map_blocks() now supported readahead/bmap/rw direct_IO with
+ * f2fs_map_blocks structure.
  * If original data blocks are allocated, then give them to blockdev.
  * Otherwise,
  *     a. preallocate requested block addresses
  *     b. do not use extent cache for better performance
  *     c. give the block addresses to blockdev
  */
-static int __get_data_block(struct inode *inode, sector_t iblock,
-			struct buffer_head *bh_result, int create, bool fiemap)
+static int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
+			int create, bool fiemap)
 {
-	unsigned int blkbits = inode->i_sb->s_blocksize_bits;
-	unsigned maxblocks = bh_result->b_size >> blkbits;
+	unsigned int maxblocks = map->m_len;
 	struct dnode_of_data dn;
 	int mode = create ? ALLOC_NODE : LOOKUP_NODE_RA;
 	pgoff_t pgofs, end_offset;
@@ -1227,11 +1214,16 @@ static int __get_data_block(struct inode *inode, sector_t iblock,
 	struct extent_info ei;
 	bool allocated = false;
 
-	/* Get the page offset from the block offset(iblock) */
-	pgofs =	(pgoff_t)(iblock >> (PAGE_CACHE_SHIFT - blkbits));
+	map->m_len = 0;
+	map->m_flags = 0;
+
+	/* it only supports block size == page size */
+	pgofs =	(pgoff_t)map->m_lblk;
 
 	if (f2fs_lookup_extent_cache(inode, pgofs, &ei)) {
-		f2fs_map_bh(inode->i_sb, pgofs, &ei, bh_result);
+		map->m_pblk = ei.blk + pgofs - ei.fofs;
+		map->m_len = min((pgoff_t)maxblocks, ei.fofs + ei.len - pgofs);
+		map->m_flags = F2FS_MAP_MAPPED;
 		goto out;
 	}
 
@@ -1250,21 +1242,21 @@ static int __get_data_block(struct inode *inode, sector_t iblock,
 		goto put_out;
 
 	if (dn.data_blkaddr != NULL_ADDR) {
-		clear_buffer_new(bh_result);
-		map_bh(bh_result, inode->i_sb, dn.data_blkaddr);
+		map->m_flags = F2FS_MAP_MAPPED;
+		map->m_pblk = dn.data_blkaddr;
 	} else if (create) {
 		err = __allocate_data_block(&dn);
 		if (err)
 			goto put_out;
 		allocated = true;
-		set_buffer_new(bh_result);
-		map_bh(bh_result, inode->i_sb, dn.data_blkaddr);
+		map->m_flags = F2FS_MAP_NEW | F2FS_MAP_MAPPED;
+		map->m_pblk = dn.data_blkaddr;
 	} else {
 		goto put_out;
 	}
 
 	end_offset = ADDRS_PER_PAGE(dn.node_page, F2FS_I(inode));
-	bh_result->b_size = (((size_t)1) << blkbits);
+	map->m_len = 1;
 	dn.ofs_in_node++;
 	pgofs++;
 
@@ -1288,22 +1280,22 @@ static int __get_data_block(struct inode *inode, sector_t iblock,
 		end_offset = ADDRS_PER_PAGE(dn.node_page, F2FS_I(inode));
 	}
 
-	if (maxblocks > (bh_result->b_size >> blkbits)) {
+	if (maxblocks > map->m_len) {
 		block_t blkaddr = datablock_addr(dn.node_page, dn.ofs_in_node);
 		if (blkaddr == NULL_ADDR && create) {
 			err = __allocate_data_block(&dn);
 			if (err)
 				goto sync_out;
 			allocated = true;
-			set_buffer_new(bh_result);
+			map->m_flags |= F2FS_MAP_NEW;
 			blkaddr = dn.data_blkaddr;
 		}
 		/* Give more consecutive addresses for the readahead */
-		if (blkaddr == (bh_result->b_blocknr + ofs)) {
+		if (map->m_pblk != NEW_ADDR && blkaddr == (map->m_pblk + ofs)) {
 			ofs++;
 			dn.ofs_in_node++;
 			pgofs++;
-			bh_result->b_size += (((size_t)1) << blkbits);
+			map->m_len++;
 			goto get_next;
 		}
 	}
@@ -1316,10 +1308,28 @@ static int __get_data_block(struct inode *inode, sector_t iblock,
 	if (create)
 		f2fs_unlock_op(F2FS_I_SB(inode));
 out:
-	trace_f2fs_get_data_block(inode, iblock, bh_result, err);
+	trace_f2fs_map_blocks(inode, map, err);
 	return err;
 }
 
+static int __get_data_block(struct inode *inode, sector_t iblock,
+			struct buffer_head *bh, int create, bool fiemap)
+{
+	struct f2fs_map_blocks map;
+	int ret;
+
+	map.m_lblk = iblock;
+	map.m_len = bh->b_size >> inode->i_blkbits;
+
+	ret = f2fs_map_blocks(inode, &map, create, fiemap);
+	if (!ret) {
+		map_bh(bh, inode->i_sb, map.m_pblk);
+		bh->b_state = (bh->b_state & ~F2FS_MAP_FLAGS) | map.m_flags;
+		bh->b_size = map.m_len << inode->i_blkbits;
+	}
+	return ret;
+}
+
 static int get_data_block(struct inode *inode, sector_t iblock,
 			struct buffer_head *bh_result, int create)
 {

commit 5463e7c18e51152104aba9614e6abfc039a8b710
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Tue Apr 21 10:40:54 2015 -0700

    Revert "f2fs: enhance multi-threads performance"
    
    This reports performance regression by Yuanhan Liu.
    The basic idea was to reduce one-point mutex, but it turns out this causes
    another contention like context swithes.
    
    https://lkml.org/lkml/2015/4/21/11
    
    Until finishing the analysis on this issue, I'd like to revert this for a while.
    
    This reverts commit 78373b7319abdf15050af5b1632c4c8b8b398f33.

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index b91b0e10678e..1e1aae669fa8 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1513,6 +1513,7 @@ static int f2fs_write_data_pages(struct address_space *mapping,
 {
 	struct inode *inode = mapping->host;
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	bool locked = false;
 	int ret;
 	long diff;
 
@@ -1533,7 +1534,13 @@ static int f2fs_write_data_pages(struct address_space *mapping,
 
 	diff = nr_pages_to_write(sbi, DATA, wbc);
 
+	if (!S_ISDIR(inode->i_mode)) {
+		mutex_lock(&sbi->writepages);
+		locked = true;
+	}
 	ret = write_cache_pages(mapping, wbc, __f2fs_writepage, mapping);
+	if (locked)
+		mutex_unlock(&sbi->writepages);
 
 	f2fs_submit_merged_bio(sbi, DATA, WRITE);
 

commit 06a60deca87dba8e2c186ea7f12ea87d6785188e
Merge: d6a24d0640d6 10027551ccf5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Apr 18 11:17:20 2015 -0400

    Merge tag 'for-f2fs-4.1' of git://git.kernel.org/pub/scm/linux/kernel/git/jaegeuk/f2fs
    
    Pull f2fs updates from Jaegeuk Kim:
     "New features:
       - in-memory extent_cache
       - fs_shutdown to test power-off-recovery
       - use inline_data to store symlink path
       - show f2fs as a non-misc filesystem
    
      Major fixes:
       - avoid CPU stalls on sync_dirty_dir_inodes
       - fix some power-off-recovery procedure
       - fix handling of broken symlink correctly
       - fix missing dot and dotdot made by sudden power cuts
       - handle wrong data index during roll-forward recovery
       - preallocate data blocks for direct_io
    
      ... and a bunch of minor bug fixes and cleanups"
    
    * tag 'for-f2fs-4.1' of git://git.kernel.org/pub/scm/linux/kernel/git/jaegeuk/f2fs: (71 commits)
      f2fs: pass checkpoint reason on roll-forward recovery
      f2fs: avoid abnormal behavior on broken symlink
      f2fs: flush symlink path to avoid broken symlink after POR
      f2fs: change 0 to false for bool type
      f2fs: do not recover wrong data index
      f2fs: do not increase link count during recovery
      f2fs: assign parent's i_mode for empty dir
      f2fs: add F2FS_INLINE_DOTS to recover missing dot dentries
      f2fs: fix mismatching lock and unlock pages for roll-forward recovery
      f2fs: fix sparse warnings
      f2fs: limit b_size of mapped bh in f2fs_map_bh
      f2fs: persist system.advise into on-disk inode
      f2fs: avoid NULL pointer dereference in f2fs_xattr_advise_get
      f2fs: preallocate fallocated blocks for direct IO
      f2fs: enable inline data by default
      f2fs: preserve extent info for extent cache
      f2fs: initialize extent tree with on-disk extent info of inode
      f2fs: introduce __{find,grab}_extent_tree
      f2fs: split set_data_blkaddr from f2fs_update_extent_cache
      f2fs: enable fast symlink by utilizing inline data
      ...

commit 22c6186ecea0be9eff1c399298ad36e94a59995f
Author: Omar Sandoval <osandov@osandov.com>
Date:   Mon Mar 16 04:33:53 2015 -0700

    direct_IO: remove rw from a_ops->direct_IO()
    
    Now that no one is using rw, remove it completely.
    
    Signed-off-by: Omar Sandoval <osandov@osandov.com>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index ce25f62edfa7..319eda511c4f 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1135,8 +1135,8 @@ static int check_direct_IO(struct inode *inode, struct iov_iter *iter,
 	return 0;
 }
 
-static ssize_t f2fs_direct_IO(int rw, struct kiocb *iocb,
-		struct iov_iter *iter, loff_t offset)
+static ssize_t f2fs_direct_IO(struct kiocb *iocb, struct iov_iter *iter,
+			      loff_t offset)
 {
 	struct file *file = iocb->ki_filp;
 	struct address_space *mapping = file->f_mapping;

commit 6f67376318abea58589ebe6d69dffeabb6f6c26a
Author: Omar Sandoval <osandov@osandov.com>
Date:   Mon Mar 16 04:33:52 2015 -0700

    direct_IO: use iov_iter_rw() instead of rw everywhere
    
    The rw parameter to direct_IO is redundant with iov_iter->type, and
    treated slightly differently just about everywhere it's used: some users
    do rw & WRITE, and others do rw == WRITE where they should be doing a
    bitwise check. Simplify this with the new iov_iter_rw() helper, which
    always returns either READ or WRITE.
    
    Signed-off-by: Omar Sandoval <osandov@osandov.com>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index e16adebcb9b6..ce25f62edfa7 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1118,12 +1118,12 @@ static int f2fs_write_end(struct file *file,
 	return copied;
 }
 
-static int check_direct_IO(struct inode *inode, int rw,
-		struct iov_iter *iter, loff_t offset)
+static int check_direct_IO(struct inode *inode, struct iov_iter *iter,
+			   loff_t offset)
 {
 	unsigned blocksize_mask = inode->i_sb->s_blocksize - 1;
 
-	if (rw == READ)
+	if (iov_iter_rw(iter) == READ)
 		return 0;
 
 	if (offset & blocksize_mask)
@@ -1151,19 +1151,19 @@ static ssize_t f2fs_direct_IO(int rw, struct kiocb *iocb,
 			return err;
 	}
 
-	if (check_direct_IO(inode, rw, iter, offset))
+	if (check_direct_IO(inode, iter, offset))
 		return 0;
 
-	trace_f2fs_direct_IO_enter(inode, offset, count, rw);
+	trace_f2fs_direct_IO_enter(inode, offset, count, iov_iter_rw(iter));
 
-	if (rw & WRITE)
+	if (iov_iter_rw(iter) == WRITE)
 		__allocate_data_blocks(inode, offset, count);
 
 	err = blockdev_direct_IO(iocb, inode, iter, offset, get_data_block);
-	if (err < 0 && (rw & WRITE))
+	if (err < 0 && iov_iter_rw(iter) == WRITE)
 		f2fs_write_failed(mapping, offset + count);
 
-	trace_f2fs_direct_IO_exit(inode, offset, count, rw, err);
+	trace_f2fs_direct_IO_exit(inode, offset, count, iov_iter_rw(iter), err);
 
 	return err;
 }

commit 17f8c842d24ac054e4212c82b5bd6ae455a334f3
Author: Omar Sandoval <osandov@osandov.com>
Date:   Mon Mar 16 04:33:50 2015 -0700

    Remove rw from {,__,do_}blockdev_direct_IO()
    
    Most filesystems call through to these at some point, so we'll start
    here.
    
    Signed-off-by: Omar Sandoval <osandov@osandov.com>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 497f8515d205..e16adebcb9b6 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1159,7 +1159,7 @@ static ssize_t f2fs_direct_IO(int rw, struct kiocb *iocb,
 	if (rw & WRITE)
 		__allocate_data_blocks(inode, offset, count);
 
-	err = blockdev_direct_IO(rw, iocb, inode, iter, offset, get_data_block);
+	err = blockdev_direct_IO(iocb, inode, iter, offset, get_data_block);
 	if (err < 0 && (rw & WRITE))
 		f2fs_write_failed(mapping, offset + count);
 

commit 1b3e27a92ab60452b8fbb35e3ba691ac34f2c0fb
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Tue Mar 24 13:08:05 2015 +0800

    f2fs: limit b_size of mapped bh in f2fs_map_bh
    
    Map bh over max size which caller defined is not needed, limit it in
    f2fs_map_bh.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 614e444e5297..93aae548cf80 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -255,15 +255,13 @@ static void f2fs_map_bh(struct super_block *sb, pgoff_t pgofs,
 			struct extent_info *ei, struct buffer_head *bh_result)
 {
 	unsigned int blkbits = sb->s_blocksize_bits;
-	size_t count;
+	size_t max_size = bh_result->b_size;
+	size_t mapped_size;
 
 	clear_buffer_new(bh_result);
 	map_bh(bh_result, sb, ei->blk + pgofs - ei->fofs);
-	count = ei->fofs + ei->len - pgofs;
-	if (count < (UINT_MAX >> blkbits))
-		bh_result->b_size = (count << blkbits);
-	else
-		bh_result->b_size = UINT_MAX;
+	mapped_size = (ei->fofs + ei->len - pgofs) << blkbits;
+	bh_result->b_size = min(max_size, mapped_size);
 }
 
 static bool lookup_extent_info(struct inode *inode, pgoff_t pgofs,

commit df6136ef5533421e68ea7ff9c33d5b2ac9005ff9
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Mon Mar 23 10:33:37 2015 +0800

    f2fs: preallocate fallocated blocks for direct IO
    
    Normally, due to DIO_SKIP_HOLES flag is set by default, blockdev_direct_IO in
    f2fs_direct_IO tries to skip DIO in holes when writing inside i_size, this
    makes us falling back to buffered IO which shows lower performance.
    
    So in commit 59b802e5a453 ("f2fs: allocate data blocks in advance for
    f2fs_direct_IO"), we improve perfromance by allocating data blocks in advance
    if we meet holes no matter in i_size or not, since with it we can avoid falling
    back to buffered IO.
    
    But we forget to consider for unwritten fallocated block in this commit.
    This patch tries to fix it for fallocate case, this helps to improve
    performance.
    
    Test result:
    Storage info: sandisk ultra 64G micro sd card.
    
    touch /mnt/f2fs/file
    truncate -s 67108864 /mnt/f2fs/file
    fallocate -o 0 -l 67108864 /mnt/f2fs/file
    time dd if=/dev/zero of=/mnt/f2fs/file bs=1M count=64 conv=notrunc oflag=direct
    
    Time before applying the patch:
    67108864 bytes (67 MB) copied, 36.16 s, 1.9 MB/s
    real    0m36.162s
    user    0m0.000s
    sys     0m0.180s
    
    Time after applying the patch:
    67108864 bytes (67 MB) copied, 27.7776 s, 2.4 MB/s
    real    0m27.780s
    user    0m0.000s
    sys     0m0.036s
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 8a940e518be8..614e444e5297 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1126,16 +1126,23 @@ static int __allocate_data_block(struct dnode_of_data *dn)
 
 	if (unlikely(is_inode_flag_set(F2FS_I(dn->inode), FI_NO_ALLOC)))
 		return -EPERM;
+
+	dn->data_blkaddr = datablock_addr(dn->node_page, dn->ofs_in_node);
+	if (dn->data_blkaddr == NEW_ADDR)
+		goto alloc;
+
 	if (unlikely(!inc_valid_block_count(sbi, dn->inode, 1)))
 		return -ENOSPC;
 
+alloc:
 	get_node_info(sbi, dn->nid, &ni);
 	set_summary(&sum, dn->nid, dn->ofs_in_node, ni.version);
 
 	if (dn->ofs_in_node == 0 && dn->inode_page == dn->node_page)
 		seg = CURSEG_DIRECT_IO;
 
-	allocate_data_block(sbi, NULL, NULL_ADDR, &dn->data_blkaddr, &sum, seg);
+	allocate_data_block(sbi, NULL, dn->data_blkaddr, &dn->data_blkaddr,
+								&sum, seg);
 
 	/* direct IO doesn't use extent cache to maximize the performance */
 	set_data_blkaddr(dn);
@@ -1175,7 +1182,7 @@ static void __allocate_data_blocks(struct inode *inode, loff_t offset,
 			block_t blkaddr;
 
 			blkaddr = datablock_addr(dn.node_page, dn.ofs_in_node);
-			if (blkaddr == NULL_ADDR) {
+			if (blkaddr == NULL_ADDR || blkaddr == NEW_ADDR) {
 				if (__allocate_data_block(&dn))
 					goto sync_out;
 				allocated = true;

commit 0bdee482509fe8c3cf0e66231ed37b8e70954093
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Thu Mar 19 19:27:51 2015 +0800

    f2fs: preserve extent info for extent cache
    
    This patch tries to preserve last extent info in extent tree cache into on-disk
    inode, so this can help us to reuse the last extent info next time for
    performance.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index e3be4be3a6d8..8a940e518be8 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -719,6 +719,55 @@ static void f2fs_update_extent_tree(struct inode *inode, pgoff_t fofs,
 	atomic_dec(&et->refcount);
 }
 
+void f2fs_preserve_extent_tree(struct inode *inode)
+{
+	struct extent_tree *et;
+	struct extent_info *ext = &F2FS_I(inode)->ext;
+	bool sync = false;
+
+	if (!test_opt(F2FS_I_SB(inode), EXTENT_CACHE))
+		return;
+
+	et = __find_extent_tree(F2FS_I_SB(inode), inode->i_ino);
+	if (!et) {
+		if (ext->len) {
+			ext->len = 0;
+			update_inode_page(inode);
+		}
+		return;
+	}
+
+	read_lock(&et->lock);
+	if (et->count) {
+		struct extent_node *en;
+
+		if (et->cached_en) {
+			en = et->cached_en;
+		} else {
+			struct rb_node *node = rb_first(&et->root);
+
+			if (!node)
+				node = rb_last(&et->root);
+			en = rb_entry(node, struct extent_node, rb_node);
+		}
+
+		if (__is_extent_same(ext, &en->ei))
+			goto out;
+
+		*ext = en->ei;
+		sync = true;
+	} else if (ext->len) {
+		ext->len = 0;
+		sync = true;
+	}
+out:
+	read_unlock(&et->lock);
+	atomic_dec(&et->refcount);
+
+	if (sync)
+		update_inode_page(inode);
+}
+
 void f2fs_shrink_extent_tree(struct f2fs_sb_info *sbi, int nr_shrink)
 {
 	struct extent_tree *treevec[EXT_TREE_VEC_SIZE];

commit 028a41e89383e1208dff1afe3e260b8cb6d3431c
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Thu Mar 19 19:26:02 2015 +0800

    f2fs: initialize extent tree with on-disk extent info of inode
    
    With normal extent info cache, we records largest extent mapping between logical
    block and physical block into extent info, and we persist extent info in on-disk
    inode.
    
    When we enable extent tree cache, if extent info of on-disk inode is exist, and
    the extent is not a small fragmented mapping extent. We'd better to load the
    extent info into extent tree cache when inode is loaded. By this way we can have
    more chance to hit extent tree cache rather than taking more time to read dnode
    page for block address.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 67b3a99e03d5..e3be4be3a6d8 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -572,6 +572,39 @@ static unsigned int __free_extent_tree(struct f2fs_sb_info *sbi,
 	return count - et->count;
 }
 
+static void f2fs_init_extent_tree(struct inode *inode,
+						struct f2fs_extent *i_ext)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct extent_tree *et;
+	struct extent_node *en;
+	struct extent_info ei;
+
+	if (le32_to_cpu(i_ext->len) < F2FS_MIN_EXTENT_LEN)
+		return;
+
+	et = __grab_extent_tree(inode);
+
+	write_lock(&et->lock);
+	if (et->count)
+		goto out;
+
+	set_extent_info(&ei, le32_to_cpu(i_ext->fofs),
+		le32_to_cpu(i_ext->blk), le32_to_cpu(i_ext->len));
+
+	en = __insert_extent_tree(sbi, et, &ei, NULL);
+	if (en) {
+		et->cached_en = en;
+
+		spin_lock(&sbi->extent_lock);
+		list_add_tail(&en->list, &sbi->extent_list);
+		spin_unlock(&sbi->extent_lock);
+	}
+out:
+	write_unlock(&et->lock);
+	atomic_dec(&et->refcount);
+}
+
 static bool f2fs_lookup_extent_tree(struct inode *inode, pgoff_t pgofs,
 							struct extent_info *ei)
 {
@@ -782,6 +815,16 @@ void f2fs_destroy_extent_tree(struct inode *inode)
 	return;
 }
 
+void f2fs_init_extent_cache(struct inode *inode, struct f2fs_extent *i_ext)
+{
+	if (test_opt(F2FS_I_SB(inode), EXTENT_CACHE))
+		f2fs_init_extent_tree(inode, i_ext);
+
+	write_lock(&F2FS_I(inode)->ext_lock);
+	get_extent_info(&F2FS_I(inode)->ext, *i_ext);
+	write_unlock(&F2FS_I(inode)->ext_lock);
+}
+
 static bool f2fs_lookup_extent_cache(struct inode *inode, pgoff_t pgofs,
 							struct extent_info *ei)
 {

commit 93dfc52656e9ae1009ccba67e67ff69285b7743f
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Thu Mar 19 19:24:59 2015 +0800

    f2fs: introduce __{find,grab}_extent_tree
    
    This patch introduces __{find,grab}_extent_tree for reusing by following
    patches.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 6492f88a2cc4..67b3a99e03d5 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -394,6 +394,49 @@ static void __detach_extent_node(struct f2fs_sb_info *sbi,
 		et->cached_en = NULL;
 }
 
+static struct extent_tree *__find_extent_tree(struct f2fs_sb_info *sbi,
+							nid_t ino)
+{
+	struct extent_tree *et;
+
+	down_read(&sbi->extent_tree_lock);
+	et = radix_tree_lookup(&sbi->extent_tree_root, ino);
+	if (!et) {
+		up_read(&sbi->extent_tree_lock);
+		return NULL;
+	}
+	atomic_inc(&et->refcount);
+	up_read(&sbi->extent_tree_lock);
+
+	return et;
+}
+
+static struct extent_tree *__grab_extent_tree(struct inode *inode)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct extent_tree *et;
+	nid_t ino = inode->i_ino;
+
+	down_write(&sbi->extent_tree_lock);
+	et = radix_tree_lookup(&sbi->extent_tree_root, ino);
+	if (!et) {
+		et = f2fs_kmem_cache_alloc(extent_tree_slab, GFP_NOFS);
+		f2fs_radix_tree_insert(&sbi->extent_tree_root, ino, et);
+		memset(et, 0, sizeof(struct extent_tree));
+		et->ino = ino;
+		et->root = RB_ROOT;
+		et->cached_en = NULL;
+		rwlock_init(&et->lock);
+		atomic_set(&et->refcount, 0);
+		et->count = 0;
+		sbi->total_ext_tree++;
+	}
+	atomic_inc(&et->refcount);
+	up_write(&sbi->extent_tree_lock);
+
+	return et;
+}
+
 static struct extent_node *__lookup_extent_tree(struct extent_tree *et,
 							unsigned int fofs)
 {
@@ -538,14 +581,9 @@ static bool f2fs_lookup_extent_tree(struct inode *inode, pgoff_t pgofs,
 
 	trace_f2fs_lookup_extent_tree_start(inode, pgofs);
 
-	down_read(&sbi->extent_tree_lock);
-	et = radix_tree_lookup(&sbi->extent_tree_root, inode->i_ino);
-	if (!et) {
-		up_read(&sbi->extent_tree_lock);
+	et = __find_extent_tree(sbi, inode->i_ino);
+	if (!et)
 		return false;
-	}
-	atomic_inc(&et->refcount);
-	up_read(&sbi->extent_tree_lock);
 
 	read_lock(&et->lock);
 	en = __lookup_extent_tree(et, pgofs);
@@ -570,7 +608,6 @@ static void f2fs_update_extent_tree(struct inode *inode, pgoff_t fofs,
 							block_t blkaddr)
 {
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	nid_t ino = inode->i_ino;
 	struct extent_tree *et;
 	struct extent_node *en = NULL, *en1 = NULL, *en2 = NULL, *en3 = NULL;
 	struct extent_node *den = NULL;
@@ -579,22 +616,7 @@ static void f2fs_update_extent_tree(struct inode *inode, pgoff_t fofs,
 
 	trace_f2fs_update_extent_tree(inode, fofs, blkaddr);
 
-	down_write(&sbi->extent_tree_lock);
-	et = radix_tree_lookup(&sbi->extent_tree_root, ino);
-	if (!et) {
-		et = f2fs_kmem_cache_alloc(extent_tree_slab, GFP_NOFS);
-		f2fs_radix_tree_insert(&sbi->extent_tree_root, ino, et);
-		memset(et, 0, sizeof(struct extent_tree));
-		et->ino = ino;
-		et->root = RB_ROOT;
-		et->cached_en = NULL;
-		rwlock_init(&et->lock);
-		atomic_set(&et->refcount, 0);
-		et->count = 0;
-		sbi->total_ext_tree++;
-	}
-	atomic_inc(&et->refcount);
-	up_write(&sbi->extent_tree_lock);
+	et = __grab_extent_tree(inode);
 
 	write_lock(&et->lock);
 
@@ -732,14 +754,9 @@ void f2fs_destroy_extent_tree(struct inode *inode)
 	if (!test_opt(sbi, EXTENT_CACHE))
 		return;
 
-	down_read(&sbi->extent_tree_lock);
-	et = radix_tree_lookup(&sbi->extent_tree_root, inode->i_ino);
-	if (!et) {
-		up_read(&sbi->extent_tree_lock);
+	et = __find_extent_tree(sbi, inode->i_ino);
+	if (!et)
 		goto out;
-	}
-	atomic_inc(&et->refcount);
-	up_read(&sbi->extent_tree_lock);
 
 	/* free all extent info belong to this extent tree */
 	write_lock(&et->lock);

commit 216a620a7c3d35ae604ba519c99c5cd1ce4dad6e
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Thu Mar 19 19:23:32 2015 +0800

    f2fs: split set_data_blkaddr from f2fs_update_extent_cache
    
    Split __set_data_blkaddr from f2fs_update_extent_cache for readability.
    
    Additionally rename __set_data_blkaddr to set_data_blkaddr for exporting.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 241b98741df9..6492f88a2cc4 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -200,7 +200,7 @@ void f2fs_submit_page_mbio(struct f2fs_sb_info *sbi, struct page *page,
  *  ->node_page
  *    update block addresses in the node page
  */
-static void __set_data_blkaddr(struct dnode_of_data *dn)
+void set_data_blkaddr(struct dnode_of_data *dn)
 {
 	struct f2fs_node *rn;
 	__le32 *addr_array;
@@ -229,7 +229,7 @@ int reserve_new_block(struct dnode_of_data *dn)
 	trace_f2fs_reserve_new_block(dn->inode, dn->nid, dn->ofs_in_node);
 
 	dn->data_blkaddr = NEW_ADDR;
-	__set_data_blkaddr(dn);
+	set_data_blkaddr(dn);
 	mark_inode_dirty(dn->inode);
 	sync_inode_page(dn);
 	return 0;
@@ -784,9 +784,6 @@ void f2fs_update_extent_cache(struct dnode_of_data *dn)
 
 	f2fs_bug_on(F2FS_I_SB(dn->inode), dn->data_blkaddr == NEW_ADDR);
 
-	/* Update the page address in the parent node */
-	__set_data_blkaddr(dn);
-
 	if (is_inode_flag_set(fi, FI_NO_EXTENT))
 		return;
 
@@ -1032,7 +1029,7 @@ static int __allocate_data_block(struct dnode_of_data *dn)
 	allocate_data_block(sbi, NULL, NULL_ADDR, &dn->data_blkaddr, &sum, seg);
 
 	/* direct IO doesn't use extent cache to maximize the performance */
-	__set_data_blkaddr(dn);
+	set_data_blkaddr(dn);
 
 	/* update i_size */
 	fofs = start_bidx_of_node(ofs_of_node(dn->node_page), fi) +
@@ -1290,6 +1287,7 @@ int do_write_data_page(struct page *page, struct f2fs_io_info *fio)
 		trace_f2fs_do_write_data_page(page, IPU);
 	} else {
 		write_data_page(page, &dn, fio);
+		set_data_blkaddr(&dn);
 		f2fs_update_extent_cache(&dn);
 		trace_f2fs_do_write_data_page(page, OPU);
 		set_inode_flag(F2FS_I(inode), FI_APPEND_WRITE);

commit 8ce67cb07dbf6ba35aea1e07e8ad1ea004ced27b
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Tue Mar 17 17:58:08 2015 -0700

    f2fs: add some tracepoints to debug volatile and atomic writes
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 486113db97b1..241b98741df9 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1287,9 +1287,11 @@ int do_write_data_page(struct page *page, struct f2fs_io_info *fio)
 			need_inplace_update(inode))) {
 		rewrite_data_page(page, fio);
 		set_inode_flag(F2FS_I(inode), FI_UPDATE_WRITE);
+		trace_f2fs_do_write_data_page(page, IPU);
 	} else {
 		write_data_page(page, &dn, fio);
 		f2fs_update_extent_cache(&dn);
+		trace_f2fs_do_write_data_page(page, OPU);
 		set_inode_flag(F2FS_I(inode), FI_APPEND_WRITE);
 		if (page->index == 0)
 			set_inode_flag(F2FS_I(inode), FI_FIRST_BLOCK_WRITTEN);

commit 3c6c2bebef79999b1827041696dc1881e637e3af
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Tue Mar 17 17:16:35 2015 -0700

    f2fs: avoid punch_hole overhead when releasing volatile data
    
    This patch is to avoid some punch_hole overhead when releasing volatile data.
    If volatile data was not written yet, we just can make the first page as zero.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index f0a18a005bda..486113db97b1 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1291,6 +1291,8 @@ int do_write_data_page(struct page *page, struct f2fs_io_info *fio)
 		write_data_page(page, &dn, fio);
 		f2fs_update_extent_cache(&dn);
 		set_inode_flag(F2FS_I(inode), FI_APPEND_WRITE);
+		if (page->index == 0)
+			set_inode_flag(F2FS_I(inode), FI_FIRST_BLOCK_WRITTEN);
 	}
 out_writepage:
 	f2fs_put_dnode(&dn);

commit 78373b7319abdf15050af5b1632c4c8b8b398f33
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Mar 13 21:44:36 2015 -0700

    f2fs: enhance multi-threads performance
    
    Previously, f2fs_write_data_pages has a mutex, sbi->writepages, to serialize
    data writes to maximize write bandwidth, while sacrificing multi-threads
    performance.
    Practically, however, multi-threads environment is much more important for
    users. So this patch tries to remove the mutex.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 4a416e74bb2a..f0a18a005bda 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1397,7 +1397,6 @@ static int f2fs_write_data_pages(struct address_space *mapping,
 {
 	struct inode *inode = mapping->host;
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	bool locked = false;
 	int ret;
 	long diff;
 
@@ -1418,13 +1417,7 @@ static int f2fs_write_data_pages(struct address_space *mapping,
 
 	diff = nr_pages_to_write(sbi, DATA, wbc);
 
-	if (!S_ISDIR(inode->i_mode)) {
-		mutex_lock(&sbi->writepages);
-		locked = true;
-	}
 	ret = write_cache_pages(mapping, wbc, __f2fs_writepage, mapping);
-	if (locked)
-		mutex_unlock(&sbi->writepages);
 
 	f2fs_submit_merged_bio(sbi, DATA, WRITE);
 

commit 3402e87cfb5e762f9c95071bf4a2ad65fd9392a2
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed Mar 11 23:27:25 2015 -0400

    f2fs: set buffer_new when new blocks are allocated
    
    This patch modifies to call set_buffer_new, if new blocks are allocated.
    
    Reviewed-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index ba70a78a396f..4a416e74bb2a 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -257,7 +257,7 @@ static void f2fs_map_bh(struct super_block *sb, pgoff_t pgofs,
 	unsigned int blkbits = sb->s_blocksize_bits;
 	size_t count;
 
-	set_buffer_new(bh_result);
+	clear_buffer_new(bh_result);
 	map_bh(bh_result, sb, ei->blk + pgofs - ei->fofs);
 	count = ei->fofs + ei->len - pgofs;
 	if (count < (UINT_MAX >> blkbits))
@@ -1139,7 +1139,7 @@ static int __get_data_block(struct inode *inode, sector_t iblock,
 		goto put_out;
 
 	if (dn.data_blkaddr != NULL_ADDR) {
-		set_buffer_new(bh_result);
+		clear_buffer_new(bh_result);
 		map_bh(bh_result, inode->i_sb, dn.data_blkaddr);
 	} else if (create) {
 		err = __allocate_data_block(&dn);
@@ -1184,6 +1184,7 @@ static int __get_data_block(struct inode *inode, sector_t iblock,
 			if (err)
 				goto sync_out;
 			allocated = true;
+			set_buffer_new(bh_result);
 			blkaddr = dn.data_blkaddr;
 		}
 		/* Give more consecutive addresses for the readahead */

commit d6d4f1cb912d9bcf988a529d0934568d4550f7b5
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Thu Mar 12 17:04:24 2015 +0800

    f2fs: fix to check current blkaddr in __allocate_data_blocks
    
    In __allocate_data_blocks, we should check current blkaddr which is located at
    ofs_in_node of dnode page instead of checking first blkaddr all the time.
    Otherwise we can only allocate one blkaddr in each dnode page. Fix it.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 0057d4baad33..ba70a78a396f 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1066,7 +1066,10 @@ static void __allocate_data_blocks(struct inode *inode, loff_t offset,
 		end_offset = ADDRS_PER_PAGE(dn.node_page, F2FS_I(inode));
 
 		while (dn.ofs_in_node < end_offset && len) {
-			if (dn.data_blkaddr == NULL_ADDR) {
+			block_t blkaddr;
+
+			blkaddr = datablock_addr(dn.node_page, dn.ofs_in_node);
+			if (blkaddr == NULL_ADDR) {
 				if (__allocate_data_block(&dn))
 					goto sync_out;
 				allocated = true;

commit d5669f7b9b0344a7f1e874ac86b9403434381ba8
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Feb 27 13:37:39 2015 -0800

    f2fs: avoid to trigger writepage during POR
    
    This patch doesn't make any effect on previous behavior, since
    f2fs_write_data_page bypasses writing the page during POR.
    
    But, the difference is that this patch avoids holding writepages mutex.
    This is to avoid the following false warning, since this can happen only
    when mount and shutdown are triggered at the same time.
    
     ======================================================
     [ INFO: possible circular locking dependency detected ]
     4.0.0-rc1+ #3 Tainted: G           O
     -------------------------------------------------------
     kworker/u8:0/2270 is trying to acquire lock:
      (&sbi->gc_mutex){+.+.+.}, at: [<ffffffffa02bdd33>] f2fs_balance_fs+0x73/0x90 [f2fs]
    
     but task is already holding lock:
      (&sbi->writepages){+.+...}, at: [<ffffffffa02b261b>] f2fs_write_data_pages+0xcb/0x3a0 [f2fs]
    
     which lock already depends on the new lock.
    
     the existing dependency chain (in reverse order) is:
    
     -> #2 (&sbi->writepages){+.+...}:
            [<ffffffff810e2b11>] lock_acquire+0xe1/0x2f0
            [<ffffffff8185e1b3>] mutex_lock_nested+0x63/0x530
            [<ffffffffa02b261b>] f2fs_write_data_pages+0xcb/0x3a0 [f2fs]
            [<ffffffff811c38c1>] do_writepages+0x21/0x50
            [<ffffffff8126c5a6>] __writeback_single_inode+0x76/0xbf0
            [<ffffffff8126e23a>] writeback_single_inode+0xea/0x1c0
            [<ffffffff8126e425>] write_inode_now+0x95/0xa0
            [<ffffffff81259dab>] iput+0x20b/0x3f0
            [<ffffffffa02c1c8b>] recover_data.constprop.14+0x26b/0xa80 [f2fs]
            [<ffffffffa02c2776>] recover_fsync_data+0x2b6/0x5e0 [f2fs]
            [<ffffffffa02a9744>] f2fs_fill_super+0xb24/0xb90 [f2fs]
            [<ffffffff8123d7f4>] mount_bdev+0x1a4/0x1e0
            [<ffffffffa02a3c85>] f2fs_mount+0x15/0x20 [f2fs]
            [<ffffffff8123e159>] mount_fs+0x39/0x180
            [<ffffffff8125e51b>] vfs_kern_mount+0x6b/0x160
            [<ffffffff81261554>] do_mount+0x204/0xbe0
            [<ffffffff8126223b>] SyS_mount+0x8b/0xe0
            [<ffffffff81863e6d>] system_call_fastpath+0x16/0x1b
    
     -> #1 (&sbi->cp_mutex){+.+...}:
            [<ffffffff810e2b11>] lock_acquire+0xe1/0x2f0
            [<ffffffff8185e1b3>] mutex_lock_nested+0x63/0x530
            [<ffffffffa02acbf2>] write_checkpoint+0x42/0x1230 [f2fs]
            [<ffffffffa02a847d>] f2fs_sync_fs+0x9d/0x2a0 [f2fs]
            [<ffffffff81272f82>] sync_filesystem+0x82/0xb0
            [<ffffffff8123c214>] generic_shutdown_super+0x34/0x100
            [<ffffffff8123c5f7>] kill_block_super+0x27/0x70
            [<ffffffffa02a3c60>] kill_f2fs_super+0x20/0x30 [f2fs]
            [<ffffffff8123ca49>] deactivate_locked_super+0x49/0x80
            [<ffffffff8123d05e>] deactivate_super+0x4e/0x70
            [<ffffffff8125df63>] cleanup_mnt+0x43/0x90
            [<ffffffff8125e002>] __cleanup_mnt+0x12/0x20
            [<ffffffff810a82e4>] task_work_run+0xc4/0xf0
            [<ffffffff8101f0bd>] do_notify_resume+0x8d/0xa0
            [<ffffffff81864141>] int_signal+0x12/0x17
    
     -> #0 (&sbi->gc_mutex){+.+.+.}:
            [<ffffffff810e2866>] __lock_acquire+0x1ac6/0x1c90
            [<ffffffff810e2b11>] lock_acquire+0xe1/0x2f0
            [<ffffffff8185e1b3>] mutex_lock_nested+0x63/0x530
            [<ffffffffa02bdd33>] f2fs_balance_fs+0x73/0x90 [f2fs]
            [<ffffffffa02b5938>] f2fs_write_data_page+0x348/0x5b0 [f2fs]
            [<ffffffffa02af9da>] __f2fs_writepage+0x1a/0x50 [f2fs]
            [<ffffffff811c1b54>] write_cache_pages+0x274/0x6f0
            [<ffffffffa02b2630>] f2fs_write_data_pages+0xe0/0x3a0 [f2fs]
            [<ffffffff811c38c1>] do_writepages+0x21/0x50
            [<ffffffff8126c5a6>] __writeback_single_inode+0x76/0xbf0
            [<ffffffff8126d44a>] writeback_sb_inodes+0x32a/0x710
            [<ffffffff8126d8cf>] __writeback_inodes_wb+0x9f/0xd0
            [<ffffffff8126dcdb>] wb_writeback+0x3db/0x850
            [<ffffffff8126e848>] bdi_writeback_workfn+0x148/0x980
            [<ffffffff810a3782>] process_one_work+0x1e2/0x840
            [<ffffffff810a3f01>] worker_thread+0x121/0x460
            [<ffffffff810a9dc8>] kthread+0xf8/0x110
            [<ffffffff81863dbc>] ret_from_fork+0x7c/0xb0
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 389fda772e69..0057d4baad33 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1408,6 +1408,10 @@ static int f2fs_write_data_pages(struct address_space *mapping,
 			available_free_memory(sbi, DIRTY_DENTS))
 		goto skip_write;
 
+	/* during POR, we don't need to trigger writepage at all. */
+	if (unlikely(is_sbi_flag_set(sbi, SBI_POR_DOING)))
+		goto skip_write;
+
 	diff = nr_pages_to_write(sbi, DATA, wbc);
 
 	if (!S_ISDIR(inode->i_mode)) {

commit b7f204cca4b7155f47e64555614b1f534a53737e
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed Feb 25 19:54:48 2015 -0800

    f2fs: check its block allocation to avoid producing wrong dirty pages
    
    If a page is cached but its block was deallocated, we don't need to make
    the page dirty again by gc and truncate_partial_data_page.
    
    In that case, it needs to check its block allocation all the time instead
    of giving up-to-date page.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 62e31b2aa131..389fda772e69 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -813,11 +813,19 @@ struct page *find_data_page(struct inode *inode, pgoff_t index, bool sync)
 		.rw = sync ? READ_SYNC : READA,
 	};
 
+	/*
+	 * If sync is false, it needs to check its block allocation.
+	 * This is need and triggered by two flows:
+	 *   gc and truncate_partial_data_page.
+	 */
+	if (!sync)
+		goto search;
+
 	page = find_get_page(mapping, index);
 	if (page && PageUptodate(page))
 		return page;
 	f2fs_put_page(page, 0);
-
+search:
 	if (f2fs_lookup_extent_cache(inode, index, &ei)) {
 		dn.data_blkaddr = ei.blk + index - ei.fofs;
 		goto got_it;

commit 2bca1e2388a8a9e8a3db0daf54fcc124516a3e83
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed Feb 25 19:25:01 2015 -0800

    f2fs: clear page's up-to-date if block was deallocated
    
    If page's on-disk block was deallocated, let's remove up-to-date flag to avoid
    further access with wrong contents.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index ae5c41444e8f..62e31b2aa131 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1259,8 +1259,10 @@ int do_write_data_page(struct page *page, struct f2fs_io_info *fio)
 	fio->blk_addr = dn.data_blkaddr;
 
 	/* This page is already truncated */
-	if (fio->blk_addr == NULL_ADDR)
+	if (fio->blk_addr == NULL_ADDR) {
+		ClearPageUptodate(page);
 		goto out_writepage;
+	}
 
 	set_page_writeback(page);
 
@@ -1355,6 +1357,8 @@ static int f2fs_write_data_page(struct page *page,
 	clear_cold_data(page);
 out:
 	inode_dec_dirty_pages(inode);
+	if (err)
+		ClearPageUptodate(page);
 	unlock_page(page);
 	if (need_balance_fs)
 		f2fs_balance_fs(sbi);

commit e2e40f2c1ed433c5e224525c8c862fd32e5d3df2
Author: Christoph Hellwig <hch@lst.de>
Date:   Sun Feb 22 08:58:50 2015 -0800

    fs: move struct kiocb to fs.h
    
    struct kiocb now is a generic I/O container, so move it to fs.h.
    Also do a #include diet for aio.h while we're at it.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 985ed023a750..497f8515d205 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -12,12 +12,12 @@
 #include <linux/f2fs_fs.h>
 #include <linux/buffer_head.h>
 #include <linux/mpage.h>
-#include <linux/aio.h>
 #include <linux/writeback.h>
 #include <linux/backing-dev.h>
 #include <linux/blkdev.h>
 #include <linux/bio.h>
 #include <linux/prefetch.h>
+#include <linux/uio.h>
 
 #include "f2fs.h"
 #include "node.h"

commit cb3bc9ee06f708f3b615b6ce119b907d6a4c8a5d
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Thu Feb 5 18:03:40 2015 +0800

    f2fs: use extent cache for dir
    
    We update extent cache for all user inode of f2fs including dir inode, so this
    patch gives another chance to try to get physical address of page from extent
    cache for dir inode.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index ad99b7a295b4..ae5c41444e8f 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -806,6 +806,7 @@ struct page *find_data_page(struct inode *inode, pgoff_t index, bool sync)
 	struct address_space *mapping = inode->i_mapping;
 	struct dnode_of_data dn;
 	struct page *page;
+	struct extent_info ei;
 	int err;
 	struct f2fs_io_info fio = {
 		.type = DATA,
@@ -817,6 +818,11 @@ struct page *find_data_page(struct inode *inode, pgoff_t index, bool sync)
 		return page;
 	f2fs_put_page(page, 0);
 
+	if (f2fs_lookup_extent_cache(inode, index, &ei)) {
+		dn.data_blkaddr = ei.blk + index - ei.fofs;
+		goto got_it;
+	}
+
 	set_new_dnode(&dn, inode, NULL, NULL, 0);
 	err = get_dnode_of_data(&dn, index, LOOKUP_NODE);
 	if (err)
@@ -830,6 +836,7 @@ struct page *find_data_page(struct inode *inode, pgoff_t index, bool sync)
 	if (unlikely(dn.data_blkaddr == NEW_ADDR))
 		return ERR_PTR(-EINVAL);
 
+got_it:
 	page = grab_cache_page(mapping, index);
 	if (!page)
 		return ERR_PTR(-ENOMEM);
@@ -864,6 +871,7 @@ struct page *get_lock_data_page(struct inode *inode, pgoff_t index)
 	struct address_space *mapping = inode->i_mapping;
 	struct dnode_of_data dn;
 	struct page *page;
+	struct extent_info ei;
 	int err;
 	struct f2fs_io_info fio = {
 		.type = DATA,
@@ -874,6 +882,11 @@ struct page *get_lock_data_page(struct inode *inode, pgoff_t index)
 	if (!page)
 		return ERR_PTR(-ENOMEM);
 
+	if (f2fs_lookup_extent_cache(inode, index, &ei)) {
+		dn.data_blkaddr = ei.blk + index - ei.fofs;
+		goto got_it;
+	}
+
 	set_new_dnode(&dn, inode, NULL, NULL, 0);
 	err = get_dnode_of_data(&dn, index, LOOKUP_NODE);
 	if (err) {
@@ -887,6 +900,7 @@ struct page *get_lock_data_page(struct inode *inode, pgoff_t index)
 		return ERR_PTR(-ENOENT);
 	}
 
+got_it:
 	if (PageUptodate(page))
 		return page;
 

commit 91c5d9bce7fef638add2cd36676e157354784d0f
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Thu Feb 5 18:02:44 2015 +0800

    f2fs: switch to check FI_NO_EXTENT in f2fs_{lookup,update}_extent_cache
    
    This patch switch to check FI_NO_EXTENT in f2fs_{lookup,update}_extent_cache
    instead of f2fs_{lookup,update}_extent_tree or {lookup,update}_extent_info.
    
    No functionality modification in this patch.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 08a71ae3ab8d..ad99b7a295b4 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -273,9 +273,6 @@ static bool lookup_extent_info(struct inode *inode, pgoff_t pgofs,
 	pgoff_t start_fofs, end_fofs;
 	block_t start_blkaddr;
 
-	if (is_inode_flag_set(fi, FI_NO_EXTENT))
-		return false;
-
 	read_lock(&fi->ext_lock);
 	if (fi->ext.len == 0) {
 		read_unlock(&fi->ext_lock);
@@ -306,9 +303,6 @@ static bool update_extent_info(struct inode *inode, pgoff_t fofs,
 	block_t start_blkaddr, end_blkaddr;
 	int need_update = true;
 
-	if (is_inode_flag_set(fi, FI_NO_EXTENT))
-		return false;
-
 	write_lock(&fi->ext_lock);
 
 	start_fofs = fi->ext.fofs;
@@ -542,9 +536,6 @@ static bool f2fs_lookup_extent_tree(struct inode *inode, pgoff_t pgofs,
 	struct extent_tree *et;
 	struct extent_node *en;
 
-	if (is_inode_flag_set(F2FS_I(inode), FI_NO_EXTENT))
-		return false;
-
 	trace_f2fs_lookup_extent_tree_start(inode, pgofs);
 
 	down_read(&sbi->extent_tree_lock);
@@ -586,9 +577,6 @@ static void f2fs_update_extent_tree(struct inode *inode, pgoff_t fofs,
 	struct extent_info ei, dei;
 	unsigned int endofs;
 
-	if (is_inode_flag_set(F2FS_I(inode), FI_NO_EXTENT))
-		return;
-
 	trace_f2fs_update_extent_tree(inode, fofs, blkaddr);
 
 	down_write(&sbi->extent_tree_lock);
@@ -780,6 +768,9 @@ void f2fs_destroy_extent_tree(struct inode *inode)
 static bool f2fs_lookup_extent_cache(struct inode *inode, pgoff_t pgofs,
 							struct extent_info *ei)
 {
+	if (is_inode_flag_set(F2FS_I(inode), FI_NO_EXTENT))
+		return false;
+
 	if (test_opt(F2FS_I_SB(inode), EXTENT_CACHE))
 		return f2fs_lookup_extent_tree(inode, pgofs, ei);
 
@@ -796,6 +787,9 @@ void f2fs_update_extent_cache(struct dnode_of_data *dn)
 	/* Update the page address in the parent node */
 	__set_data_blkaddr(dn);
 
+	if (is_inode_flag_set(fi, FI_NO_EXTENT))
+		return;
+
 	fofs = start_bidx_of_node(ofs_of_node(dn->node_page), fi) +
 							dn->ofs_in_node;
 

commit 62c8af651b37490c18a42c02586fa6a4fb39320a
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Thu Feb 5 18:01:39 2015 +0800

    f2fs: support fast lookup in extent cache
    
    This patch adds a fast lookup path for rb-tree extent cache.
    
    In this patch we add a recently accessed extent node pointer 'cached_en' in
    extent tree. In lookup path of extent cache, we will firstly lookup the last
    accessed extent node which cached_en points, if we do not hit in this node,
    we will try to lookup extent node in rb-tree.
    
    By this way we can avoid unnecessary slow lookup in rb-tree sometimes.
    
    Note that, side-effect of this patch is that we will increase memory cost,
    because we will store a pointer variable in each struct extent tree
    additionally.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index d7ff4ca5be18..08a71ae3ab8d 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -395,6 +395,9 @@ static void __detach_extent_node(struct f2fs_sb_info *sbi,
 	rb_erase(&en->rb_node, &et->root);
 	et->count--;
 	atomic_dec(&sbi->total_ext_node);
+
+	if (et->cached_en == en)
+		et->cached_en = NULL;
 }
 
 static struct extent_node *__lookup_extent_tree(struct extent_tree *et,
@@ -403,15 +406,24 @@ static struct extent_node *__lookup_extent_tree(struct extent_tree *et,
 	struct rb_node *node = et->root.rb_node;
 	struct extent_node *en;
 
+	if (et->cached_en) {
+		struct extent_info *cei = &et->cached_en->ei;
+
+		if (cei->fofs <= fofs && cei->fofs + cei->len > fofs)
+			return et->cached_en;
+	}
+
 	while (node) {
 		en = rb_entry(node, struct extent_node, rb_node);
 
-		if (fofs < en->ei.fofs)
+		if (fofs < en->ei.fofs) {
 			node = node->rb_left;
-		else if (fofs >= en->ei.fofs + en->ei.len)
+		} else if (fofs >= en->ei.fofs + en->ei.len) {
 			node = node->rb_right;
-		else
+		} else {
+			et->cached_en = en;
 			return en;
+		}
 	}
 	return NULL;
 }
@@ -587,6 +599,7 @@ static void f2fs_update_extent_tree(struct inode *inode, pgoff_t fofs,
 		memset(et, 0, sizeof(struct extent_tree));
 		et->ino = ino;
 		et->root = RB_ROOT;
+		et->cached_en = NULL;
 		rwlock_init(&et->lock);
 		atomic_set(&et->refcount, 0);
 		et->count = 0;

commit 1ec4610c522cc51219cc022ef120a928828fa934
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Thu Feb 5 17:59:59 2015 +0800

    f2fs: add trace for rb-tree extent cache ops
    
    This patch adds trace for lookup/update/shrink/destroy ops in rb-tree extent cache.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index acdc0767f77c..d7ff4ca5be18 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -533,6 +533,8 @@ static bool f2fs_lookup_extent_tree(struct inode *inode, pgoff_t pgofs,
 	if (is_inode_flag_set(F2FS_I(inode), FI_NO_EXTENT))
 		return false;
 
+	trace_f2fs_lookup_extent_tree_start(inode, pgofs);
+
 	down_read(&sbi->extent_tree_lock);
 	et = radix_tree_lookup(&sbi->extent_tree_root, inode->i_ino);
 	if (!et) {
@@ -555,6 +557,8 @@ static bool f2fs_lookup_extent_tree(struct inode *inode, pgoff_t pgofs,
 	stat_inc_total_hit(sbi->sb);
 	read_unlock(&et->lock);
 
+	trace_f2fs_lookup_extent_tree_end(inode, pgofs, en);
+
 	atomic_dec(&et->refcount);
 	return en ? true : false;
 }
@@ -573,6 +577,8 @@ static void f2fs_update_extent_tree(struct inode *inode, pgoff_t fofs,
 	if (is_inode_flag_set(F2FS_I(inode), FI_NO_EXTENT))
 		return;
 
+	trace_f2fs_update_extent_tree(inode, fofs, blkaddr);
+
 	down_write(&sbi->extent_tree_lock);
 	et = radix_tree_lookup(&sbi->extent_tree_root, ino);
 	if (!et) {
@@ -665,6 +671,7 @@ void f2fs_shrink_extent_tree(struct f2fs_sb_info *sbi, int nr_shrink)
 	struct radix_tree_iter iter;
 	void **slot;
 	unsigned int found;
+	unsigned int node_cnt = 0, tree_cnt = 0;
 
 	if (!test_opt(sbi, EXTENT_CACHE))
 		return;
@@ -691,7 +698,7 @@ void f2fs_shrink_extent_tree(struct f2fs_sb_info *sbi, int nr_shrink)
 
 			atomic_inc(&et->refcount);
 			write_lock(&et->lock);
-			__free_extent_tree(sbi, et, false);
+			node_cnt += __free_extent_tree(sbi, et, false);
 			write_unlock(&et->lock);
 			atomic_dec(&et->refcount);
 		}
@@ -707,15 +714,19 @@ void f2fs_shrink_extent_tree(struct f2fs_sb_info *sbi, int nr_shrink)
 			radix_tree_delete(&sbi->extent_tree_root, et->ino);
 			kmem_cache_free(extent_tree_slab, et);
 			sbi->total_ext_tree--;
+			tree_cnt++;
 		}
 	}
 	up_write(&sbi->extent_tree_lock);
+
+	trace_f2fs_shrink_extent_tree(sbi, node_cnt, tree_cnt);
 }
 
 void f2fs_destroy_extent_tree(struct inode *inode)
 {
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
 	struct extent_tree *et;
+	unsigned int node_cnt = 0;
 
 	if (!test_opt(sbi, EXTENT_CACHE))
 		return;
@@ -731,7 +742,7 @@ void f2fs_destroy_extent_tree(struct inode *inode)
 
 	/* free all extent info belong to this extent tree */
 	write_lock(&et->lock);
-	__free_extent_tree(sbi, et, true);
+	node_cnt = __free_extent_tree(sbi, et, true);
 	write_unlock(&et->lock);
 
 	atomic_dec(&et->refcount);
@@ -749,6 +760,7 @@ void f2fs_destroy_extent_tree(struct inode *inode)
 	sbi->total_ext_tree--;
 	up_write(&sbi->extent_tree_lock);
 out:
+	trace_f2fs_destroy_extent_tree(inode, node_cnt);
 	return;
 }
 

commit 1dcc336b02bff3d38f173feac55a2b6c25a5fb54
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Thu Feb 5 17:57:31 2015 +0800

    f2fs: enable rb-tree extent cache
    
    This patch enables rb-tree based extent cache in f2fs.
    
    When we mount with "-o extent_cache", f2fs will try to add recently accessed
    page-block mappings into rb-tree based extent cache as much as possible, instead
    of original one extent info cache.
    
    By this way, f2fs can support more effective cache between dnode page cache and
    disk. It will supply high hit ratio in the cache with fewer memory when dnode
    page cache are reclaimed in environment of low memory.
    
    Storage: Sandisk sd card 64g
    1.append write file (offset: 0, size: 128M);
    2.override write file (offset: 2M, size: 1M);
    3.override write file (offset: 4M, size: 1M);
    ...
    4.override write file (offset: 48M, size: 1M);
    ...
    5.override write file (offset: 112M, size: 1M);
    6.sync
    7.echo 3 > /proc/sys/vm/drop_caches
    8.read file (size:128M, unit: 4k, count: 32768)
    (time dd if=/mnt/f2fs/128m bs=4k count=32768)
    
    Extent Hit Ratio:
                    before          patched
    Hit Ratio       121 / 1071      1071 / 1071
    
    Performance:
                    before          patched
    real            0m37.051s       0m35.556s
    user            0m0.040s        0m0.026s
    sys             0m2.990s        0m2.251s
    
    Memory Cost:
                    before          patched
    Tree Count:     0               1 (size: 24 bytes)
    Node Count:     0               45 (size: 1440 bytes)
    
    v3:
     o retest and given more details of test result.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index f52745346205..acdc0767f77c 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -666,6 +666,9 @@ void f2fs_shrink_extent_tree(struct f2fs_sb_info *sbi, int nr_shrink)
 	void **slot;
 	unsigned int found;
 
+	if (!test_opt(sbi, EXTENT_CACHE))
+		return;
+
 	if (available_free_memory(sbi, EXTENT_CACHE))
 		return;
 
@@ -714,6 +717,9 @@ void f2fs_destroy_extent_tree(struct inode *inode)
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
 	struct extent_tree *et;
 
+	if (!test_opt(sbi, EXTENT_CACHE))
+		return;
+
 	down_read(&sbi->extent_tree_lock);
 	et = radix_tree_lookup(&sbi->extent_tree_root, inode->i_ino);
 	if (!et) {
@@ -749,6 +755,9 @@ void f2fs_destroy_extent_tree(struct inode *inode)
 static bool f2fs_lookup_extent_cache(struct inode *inode, pgoff_t pgofs,
 							struct extent_info *ei)
 {
+	if (test_opt(F2FS_I_SB(inode), EXTENT_CACHE))
+		return f2fs_lookup_extent_tree(inode, pgofs, ei);
+
 	return lookup_extent_info(inode, pgofs, ei);
 }
 
@@ -765,6 +774,10 @@ void f2fs_update_extent_cache(struct dnode_of_data *dn)
 	fofs = start_bidx_of_node(ofs_of_node(dn->node_page), fi) +
 							dn->ofs_in_node;
 
+	if (test_opt(F2FS_I_SB(dn->inode), EXTENT_CACHE))
+		return f2fs_update_extent_tree(dn->inode, fofs,
+							dn->data_blkaddr);
+
 	if (update_extent_info(dn->inode, fofs, dn->data_blkaddr))
 		sync_inode_page(dn);
 }

commit 429511cdf8b3a9b894b914f282a9293df405a449
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Thu Feb 5 17:54:31 2015 +0800

    f2fs: add core functions for rb-tree extent cache
    
    This patch adds core functions including slab cache init function and
    init/lookup/update/shrink/destroy function for rb-tree based extent cache.
    
    Thank Jaegeuk Kim and Changman Lee as they gave much suggestion about detail
    design and implementation of extent cache.
    
    Todo:
     * register rb-based extent cache shrink with mm shrink interface.
    
    v2:
     o move set_extent_info and __is_{extent,back,front}_mergeable into f2fs.h.
     o introduce __{attach,detach}_extent_node for code readability.
     o add cond_resched() when fail to invoke kmem_cache_alloc/radix_tree_insert.
     o fix some coding style and typo issues.
    
    v3:
     o fix oops due to using an unassigned pointer.
     o use list_del to remove extent node in shrink list.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Changman Lee <cm224.lee@samsung.com>
    [Jaegeuk Kim: add static for some funcitons and declare in f2fs.h]
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 3dbaa475fce4..f52745346205 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -25,6 +25,9 @@
 #include "trace.h"
 #include <trace/events/f2fs.h>
 
+static struct kmem_cache *extent_tree_slab;
+static struct kmem_cache *extent_node_slab;
+
 static void f2fs_read_end_io(struct bio *bio, int err)
 {
 	struct bio_vec *bvec;
@@ -366,6 +369,383 @@ static bool update_extent_info(struct inode *inode, pgoff_t fofs,
 	return need_update;
 }
 
+static struct extent_node *__attach_extent_node(struct f2fs_sb_info *sbi,
+				struct extent_tree *et, struct extent_info *ei,
+				struct rb_node *parent, struct rb_node **p)
+{
+	struct extent_node *en;
+
+	en = kmem_cache_alloc(extent_node_slab, GFP_ATOMIC);
+	if (!en)
+		return NULL;
+
+	en->ei = *ei;
+	INIT_LIST_HEAD(&en->list);
+
+	rb_link_node(&en->rb_node, parent, p);
+	rb_insert_color(&en->rb_node, &et->root);
+	et->count++;
+	atomic_inc(&sbi->total_ext_node);
+	return en;
+}
+
+static void __detach_extent_node(struct f2fs_sb_info *sbi,
+				struct extent_tree *et, struct extent_node *en)
+{
+	rb_erase(&en->rb_node, &et->root);
+	et->count--;
+	atomic_dec(&sbi->total_ext_node);
+}
+
+static struct extent_node *__lookup_extent_tree(struct extent_tree *et,
+							unsigned int fofs)
+{
+	struct rb_node *node = et->root.rb_node;
+	struct extent_node *en;
+
+	while (node) {
+		en = rb_entry(node, struct extent_node, rb_node);
+
+		if (fofs < en->ei.fofs)
+			node = node->rb_left;
+		else if (fofs >= en->ei.fofs + en->ei.len)
+			node = node->rb_right;
+		else
+			return en;
+	}
+	return NULL;
+}
+
+static struct extent_node *__try_back_merge(struct f2fs_sb_info *sbi,
+				struct extent_tree *et, struct extent_node *en)
+{
+	struct extent_node *prev;
+	struct rb_node *node;
+
+	node = rb_prev(&en->rb_node);
+	if (!node)
+		return NULL;
+
+	prev = rb_entry(node, struct extent_node, rb_node);
+	if (__is_back_mergeable(&en->ei, &prev->ei)) {
+		en->ei.fofs = prev->ei.fofs;
+		en->ei.blk = prev->ei.blk;
+		en->ei.len += prev->ei.len;
+		__detach_extent_node(sbi, et, prev);
+		return prev;
+	}
+	return NULL;
+}
+
+static struct extent_node *__try_front_merge(struct f2fs_sb_info *sbi,
+				struct extent_tree *et, struct extent_node *en)
+{
+	struct extent_node *next;
+	struct rb_node *node;
+
+	node = rb_next(&en->rb_node);
+	if (!node)
+		return NULL;
+
+	next = rb_entry(node, struct extent_node, rb_node);
+	if (__is_front_mergeable(&en->ei, &next->ei)) {
+		en->ei.len += next->ei.len;
+		__detach_extent_node(sbi, et, next);
+		return next;
+	}
+	return NULL;
+}
+
+static struct extent_node *__insert_extent_tree(struct f2fs_sb_info *sbi,
+				struct extent_tree *et, struct extent_info *ei,
+				struct extent_node **den)
+{
+	struct rb_node **p = &et->root.rb_node;
+	struct rb_node *parent = NULL;
+	struct extent_node *en;
+
+	while (*p) {
+		parent = *p;
+		en = rb_entry(parent, struct extent_node, rb_node);
+
+		if (ei->fofs < en->ei.fofs) {
+			if (__is_front_mergeable(ei, &en->ei)) {
+				f2fs_bug_on(sbi, !den);
+				en->ei.fofs = ei->fofs;
+				en->ei.blk = ei->blk;
+				en->ei.len += ei->len;
+				*den = __try_back_merge(sbi, et, en);
+				return en;
+			}
+			p = &(*p)->rb_left;
+		} else if (ei->fofs >= en->ei.fofs + en->ei.len) {
+			if (__is_back_mergeable(ei, &en->ei)) {
+				f2fs_bug_on(sbi, !den);
+				en->ei.len += ei->len;
+				*den = __try_front_merge(sbi, et, en);
+				return en;
+			}
+			p = &(*p)->rb_right;
+		} else {
+			f2fs_bug_on(sbi, 1);
+		}
+	}
+
+	return __attach_extent_node(sbi, et, ei, parent, p);
+}
+
+static unsigned int __free_extent_tree(struct f2fs_sb_info *sbi,
+					struct extent_tree *et, bool free_all)
+{
+	struct rb_node *node, *next;
+	struct extent_node *en;
+	unsigned int count = et->count;
+
+	node = rb_first(&et->root);
+	while (node) {
+		next = rb_next(node);
+		en = rb_entry(node, struct extent_node, rb_node);
+
+		if (free_all) {
+			spin_lock(&sbi->extent_lock);
+			if (!list_empty(&en->list))
+				list_del_init(&en->list);
+			spin_unlock(&sbi->extent_lock);
+		}
+
+		if (free_all || list_empty(&en->list)) {
+			__detach_extent_node(sbi, et, en);
+			kmem_cache_free(extent_node_slab, en);
+		}
+		node = next;
+	}
+
+	return count - et->count;
+}
+
+static bool f2fs_lookup_extent_tree(struct inode *inode, pgoff_t pgofs,
+							struct extent_info *ei)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct extent_tree *et;
+	struct extent_node *en;
+
+	if (is_inode_flag_set(F2FS_I(inode), FI_NO_EXTENT))
+		return false;
+
+	down_read(&sbi->extent_tree_lock);
+	et = radix_tree_lookup(&sbi->extent_tree_root, inode->i_ino);
+	if (!et) {
+		up_read(&sbi->extent_tree_lock);
+		return false;
+	}
+	atomic_inc(&et->refcount);
+	up_read(&sbi->extent_tree_lock);
+
+	read_lock(&et->lock);
+	en = __lookup_extent_tree(et, pgofs);
+	if (en) {
+		*ei = en->ei;
+		spin_lock(&sbi->extent_lock);
+		if (!list_empty(&en->list))
+			list_move_tail(&en->list, &sbi->extent_list);
+		spin_unlock(&sbi->extent_lock);
+		stat_inc_read_hit(sbi->sb);
+	}
+	stat_inc_total_hit(sbi->sb);
+	read_unlock(&et->lock);
+
+	atomic_dec(&et->refcount);
+	return en ? true : false;
+}
+
+static void f2fs_update_extent_tree(struct inode *inode, pgoff_t fofs,
+							block_t blkaddr)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	nid_t ino = inode->i_ino;
+	struct extent_tree *et;
+	struct extent_node *en = NULL, *en1 = NULL, *en2 = NULL, *en3 = NULL;
+	struct extent_node *den = NULL;
+	struct extent_info ei, dei;
+	unsigned int endofs;
+
+	if (is_inode_flag_set(F2FS_I(inode), FI_NO_EXTENT))
+		return;
+
+	down_write(&sbi->extent_tree_lock);
+	et = radix_tree_lookup(&sbi->extent_tree_root, ino);
+	if (!et) {
+		et = f2fs_kmem_cache_alloc(extent_tree_slab, GFP_NOFS);
+		f2fs_radix_tree_insert(&sbi->extent_tree_root, ino, et);
+		memset(et, 0, sizeof(struct extent_tree));
+		et->ino = ino;
+		et->root = RB_ROOT;
+		rwlock_init(&et->lock);
+		atomic_set(&et->refcount, 0);
+		et->count = 0;
+		sbi->total_ext_tree++;
+	}
+	atomic_inc(&et->refcount);
+	up_write(&sbi->extent_tree_lock);
+
+	write_lock(&et->lock);
+
+	/* 1. lookup and remove existing extent info in cache */
+	en = __lookup_extent_tree(et, fofs);
+	if (!en)
+		goto update_extent;
+
+	dei = en->ei;
+	__detach_extent_node(sbi, et, en);
+
+	/* 2. if extent can be split more, split and insert the left part */
+	if (dei.len > 1) {
+		/*  insert left part of split extent into cache */
+		if (fofs - dei.fofs >= F2FS_MIN_EXTENT_LEN) {
+			set_extent_info(&ei, dei.fofs, dei.blk,
+							fofs - dei.fofs);
+			en1 = __insert_extent_tree(sbi, et, &ei, NULL);
+		}
+
+		/* insert right part of split extent into cache */
+		endofs = dei.fofs + dei.len - 1;
+		if (endofs - fofs >= F2FS_MIN_EXTENT_LEN) {
+			set_extent_info(&ei, fofs + 1,
+				fofs - dei.fofs + dei.blk, endofs - fofs);
+			en2 = __insert_extent_tree(sbi, et, &ei, NULL);
+		}
+	}
+
+update_extent:
+	/* 3. update extent in extent cache */
+	if (blkaddr) {
+		set_extent_info(&ei, fofs, blkaddr, 1);
+		en3 = __insert_extent_tree(sbi, et, &ei, &den);
+	}
+
+	/* 4. update in global extent list */
+	spin_lock(&sbi->extent_lock);
+	if (en && !list_empty(&en->list))
+		list_del(&en->list);
+	/*
+	 * en1 and en2 split from en, they will become more and more smaller
+	 * fragments after splitting several times. So if the length is smaller
+	 * than F2FS_MIN_EXTENT_LEN, we will not add them into extent tree.
+	 */
+	if (en1)
+		list_add_tail(&en1->list, &sbi->extent_list);
+	if (en2)
+		list_add_tail(&en2->list, &sbi->extent_list);
+	if (en3) {
+		if (list_empty(&en3->list))
+			list_add_tail(&en3->list, &sbi->extent_list);
+		else
+			list_move_tail(&en3->list, &sbi->extent_list);
+	}
+	if (den && !list_empty(&den->list))
+		list_del(&den->list);
+	spin_unlock(&sbi->extent_lock);
+
+	/* 5. release extent node */
+	if (en)
+		kmem_cache_free(extent_node_slab, en);
+	if (den)
+		kmem_cache_free(extent_node_slab, den);
+
+	write_unlock(&et->lock);
+	atomic_dec(&et->refcount);
+}
+
+void f2fs_shrink_extent_tree(struct f2fs_sb_info *sbi, int nr_shrink)
+{
+	struct extent_tree *treevec[EXT_TREE_VEC_SIZE];
+	struct extent_node *en, *tmp;
+	unsigned long ino = F2FS_ROOT_INO(sbi);
+	struct radix_tree_iter iter;
+	void **slot;
+	unsigned int found;
+
+	if (available_free_memory(sbi, EXTENT_CACHE))
+		return;
+
+	spin_lock(&sbi->extent_lock);
+	list_for_each_entry_safe(en, tmp, &sbi->extent_list, list) {
+		if (!nr_shrink--)
+			break;
+		list_del_init(&en->list);
+	}
+	spin_unlock(&sbi->extent_lock);
+
+	down_read(&sbi->extent_tree_lock);
+	while ((found = radix_tree_gang_lookup(&sbi->extent_tree_root,
+				(void **)treevec, ino, EXT_TREE_VEC_SIZE))) {
+		unsigned i;
+
+		ino = treevec[found - 1]->ino + 1;
+		for (i = 0; i < found; i++) {
+			struct extent_tree *et = treevec[i];
+
+			atomic_inc(&et->refcount);
+			write_lock(&et->lock);
+			__free_extent_tree(sbi, et, false);
+			write_unlock(&et->lock);
+			atomic_dec(&et->refcount);
+		}
+	}
+	up_read(&sbi->extent_tree_lock);
+
+	down_write(&sbi->extent_tree_lock);
+	radix_tree_for_each_slot(slot, &sbi->extent_tree_root, &iter,
+							F2FS_ROOT_INO(sbi)) {
+		struct extent_tree *et = (struct extent_tree *)*slot;
+
+		if (!atomic_read(&et->refcount) && !et->count) {
+			radix_tree_delete(&sbi->extent_tree_root, et->ino);
+			kmem_cache_free(extent_tree_slab, et);
+			sbi->total_ext_tree--;
+		}
+	}
+	up_write(&sbi->extent_tree_lock);
+}
+
+void f2fs_destroy_extent_tree(struct inode *inode)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct extent_tree *et;
+
+	down_read(&sbi->extent_tree_lock);
+	et = radix_tree_lookup(&sbi->extent_tree_root, inode->i_ino);
+	if (!et) {
+		up_read(&sbi->extent_tree_lock);
+		goto out;
+	}
+	atomic_inc(&et->refcount);
+	up_read(&sbi->extent_tree_lock);
+
+	/* free all extent info belong to this extent tree */
+	write_lock(&et->lock);
+	__free_extent_tree(sbi, et, true);
+	write_unlock(&et->lock);
+
+	atomic_dec(&et->refcount);
+
+	/* try to find and delete extent tree entry in radix tree */
+	down_write(&sbi->extent_tree_lock);
+	et = radix_tree_lookup(&sbi->extent_tree_root, inode->i_ino);
+	if (!et) {
+		up_write(&sbi->extent_tree_lock);
+		goto out;
+	}
+	f2fs_bug_on(sbi, atomic_read(&et->refcount) || et->count);
+	radix_tree_delete(&sbi->extent_tree_root, inode->i_ino);
+	kmem_cache_free(extent_tree_slab, et);
+	sbi->total_ext_tree--;
+	up_write(&sbi->extent_tree_lock);
+out:
+	return;
+}
+
 static bool f2fs_lookup_extent_cache(struct inode *inode, pgoff_t pgofs,
 							struct extent_info *ei)
 {
@@ -1256,6 +1636,37 @@ static sector_t f2fs_bmap(struct address_space *mapping, sector_t block)
 	return generic_block_bmap(mapping, block, get_data_block);
 }
 
+void init_extent_cache_info(struct f2fs_sb_info *sbi)
+{
+	INIT_RADIX_TREE(&sbi->extent_tree_root, GFP_NOIO);
+	init_rwsem(&sbi->extent_tree_lock);
+	INIT_LIST_HEAD(&sbi->extent_list);
+	spin_lock_init(&sbi->extent_lock);
+	sbi->total_ext_tree = 0;
+	atomic_set(&sbi->total_ext_node, 0);
+}
+
+int __init create_extent_cache(void)
+{
+	extent_tree_slab = f2fs_kmem_cache_create("f2fs_extent_tree",
+			sizeof(struct extent_tree));
+	if (!extent_tree_slab)
+		return -ENOMEM;
+	extent_node_slab = f2fs_kmem_cache_create("f2fs_extent_node",
+			sizeof(struct extent_node));
+	if (!extent_node_slab) {
+		kmem_cache_destroy(extent_tree_slab);
+		return -ENOMEM;
+	}
+	return 0;
+}
+
+void destroy_extent_cache(void)
+{
+	kmem_cache_destroy(extent_node_slab);
+	kmem_cache_destroy(extent_tree_slab);
+}
+
 const struct address_space_operations f2fs_dblock_aops = {
 	.readpage	= f2fs_read_data_page,
 	.readpages	= f2fs_read_data_pages,

commit 7e4dde79df7cdf8b40282857e030c7572ff04886
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Thu Feb 5 17:51:34 2015 +0800

    f2fs: introduce universal lookup/update interface for extent cache
    
    In this patch, we do these jobs:
    1. rename {check,update}_extent_cache to {lookup,update}_extent_info;
    2. introduce universal lookup/update interface of extent cache:
    f2fs_{lookup,update}_extent_cache including above two real functions, then
    export them to function callers.
    
    So after above cleanup, we can add new rb-tree based extent cache into exported
    interfaces.
    
    v2:
     o remove "f2fs_" for inner function {lookup,update}_extent_info suggested by
       Jaegeuk Kim.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index bce0372a90d4..3dbaa475fce4 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -263,20 +263,20 @@ static void f2fs_map_bh(struct super_block *sb, pgoff_t pgofs,
 		bh_result->b_size = UINT_MAX;
 }
 
-static int check_extent_cache(struct inode *inode, pgoff_t pgofs,
-					struct extent_info *ei)
+static bool lookup_extent_info(struct inode *inode, pgoff_t pgofs,
+							struct extent_info *ei)
 {
 	struct f2fs_inode_info *fi = F2FS_I(inode);
 	pgoff_t start_fofs, end_fofs;
 	block_t start_blkaddr;
 
 	if (is_inode_flag_set(fi, FI_NO_EXTENT))
-		return 0;
+		return false;
 
 	read_lock(&fi->ext_lock);
 	if (fi->ext.len == 0) {
 		read_unlock(&fi->ext_lock);
-		return 0;
+		return false;
 	}
 
 	stat_inc_total_hit(inode->i_sb);
@@ -289,29 +289,22 @@ static int check_extent_cache(struct inode *inode, pgoff_t pgofs,
 		*ei = fi->ext;
 		stat_inc_read_hit(inode->i_sb);
 		read_unlock(&fi->ext_lock);
-		return 1;
+		return true;
 	}
 	read_unlock(&fi->ext_lock);
-	return 0;
+	return false;
 }
 
-void update_extent_cache(struct dnode_of_data *dn)
+static bool update_extent_info(struct inode *inode, pgoff_t fofs,
+								block_t blkaddr)
 {
-	struct f2fs_inode_info *fi = F2FS_I(dn->inode);
-	pgoff_t fofs, start_fofs, end_fofs;
+	struct f2fs_inode_info *fi = F2FS_I(inode);
+	pgoff_t start_fofs, end_fofs;
 	block_t start_blkaddr, end_blkaddr;
 	int need_update = true;
 
-	f2fs_bug_on(F2FS_I_SB(dn->inode), dn->data_blkaddr == NEW_ADDR);
-
-	/* Update the page address in the parent node */
-	__set_data_blkaddr(dn);
-
 	if (is_inode_flag_set(fi, FI_NO_EXTENT))
-		return;
-
-	fofs = start_bidx_of_node(ofs_of_node(dn->node_page), fi) +
-							dn->ofs_in_node;
+		return false;
 
 	write_lock(&fi->ext_lock);
 
@@ -326,16 +319,16 @@ void update_extent_cache(struct dnode_of_data *dn)
 
 	/* Initial extent */
 	if (fi->ext.len == 0) {
-		if (dn->data_blkaddr != NULL_ADDR) {
+		if (blkaddr != NULL_ADDR) {
 			fi->ext.fofs = fofs;
-			fi->ext.blk = dn->data_blkaddr;
+			fi->ext.blk = blkaddr;
 			fi->ext.len = 1;
 		}
 		goto end_update;
 	}
 
 	/* Front merge */
-	if (fofs == start_fofs - 1 && dn->data_blkaddr == start_blkaddr - 1) {
+	if (fofs == start_fofs - 1 && blkaddr == start_blkaddr - 1) {
 		fi->ext.fofs--;
 		fi->ext.blk--;
 		fi->ext.len++;
@@ -343,7 +336,7 @@ void update_extent_cache(struct dnode_of_data *dn)
 	}
 
 	/* Back merge */
-	if (fofs == end_fofs + 1 && dn->data_blkaddr == end_blkaddr + 1) {
+	if (fofs == end_fofs + 1 && blkaddr == end_blkaddr + 1) {
 		fi->ext.len++;
 		goto end_update;
 	}
@@ -370,9 +363,30 @@ void update_extent_cache(struct dnode_of_data *dn)
 	}
 end_update:
 	write_unlock(&fi->ext_lock);
-	if (need_update)
+	return need_update;
+}
+
+static bool f2fs_lookup_extent_cache(struct inode *inode, pgoff_t pgofs,
+							struct extent_info *ei)
+{
+	return lookup_extent_info(inode, pgofs, ei);
+}
+
+void f2fs_update_extent_cache(struct dnode_of_data *dn)
+{
+	struct f2fs_inode_info *fi = F2FS_I(dn->inode);
+	pgoff_t fofs;
+
+	f2fs_bug_on(F2FS_I_SB(dn->inode), dn->data_blkaddr == NEW_ADDR);
+
+	/* Update the page address in the parent node */
+	__set_data_blkaddr(dn);
+
+	fofs = start_bidx_of_node(ofs_of_node(dn->node_page), fi) +
+							dn->ofs_in_node;
+
+	if (update_extent_info(dn->inode, fofs, dn->data_blkaddr))
 		sync_inode_page(dn);
-	return;
 }
 
 struct page *find_data_page(struct inode *inode, pgoff_t index, bool sync)
@@ -668,7 +682,7 @@ static int __get_data_block(struct inode *inode, sector_t iblock,
 	/* Get the page offset from the block offset(iblock) */
 	pgofs =	(pgoff_t)(iblock >> (PAGE_CACHE_SHIFT - blkbits));
 
-	if (check_extent_cache(inode, pgofs, &ei)) {
+	if (f2fs_lookup_extent_cache(inode, pgofs, &ei)) {
 		f2fs_map_bh(inode->i_sb, pgofs, &ei, bh_result);
 		goto out;
 	}
@@ -835,7 +849,7 @@ int do_write_data_page(struct page *page, struct f2fs_io_info *fio)
 		set_inode_flag(F2FS_I(inode), FI_UPDATE_WRITE);
 	} else {
 		write_data_page(page, &dn, fio);
-		update_extent_cache(&dn);
+		f2fs_update_extent_cache(&dn);
 		set_inode_flag(F2FS_I(inode), FI_APPEND_WRITE);
 	}
 out_writepage:

commit a2e7d1bfebe0bc349a3eb9d01caac026627f095e
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Thu Feb 5 17:50:30 2015 +0800

    f2fs: introduce f2fs_map_bh to clean codes of check_extent_cache
    
    This patch introduces f2fs_map_bh to clean codes of check_extent_cache.
    
    v2:
     o cleanup f2fs_map_bh pointed out by Jaegeuk Kim.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 64f9049a9f25..bce0372a90d4 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -248,8 +248,23 @@ int f2fs_reserve_block(struct dnode_of_data *dn, pgoff_t index)
 	return err;
 }
 
+static void f2fs_map_bh(struct super_block *sb, pgoff_t pgofs,
+			struct extent_info *ei, struct buffer_head *bh_result)
+{
+	unsigned int blkbits = sb->s_blocksize_bits;
+	size_t count;
+
+	set_buffer_new(bh_result);
+	map_bh(bh_result, sb, ei->blk + pgofs - ei->fofs);
+	count = ei->fofs + ei->len - pgofs;
+	if (count < (UINT_MAX >> blkbits))
+		bh_result->b_size = (count << blkbits);
+	else
+		bh_result->b_size = UINT_MAX;
+}
+
 static int check_extent_cache(struct inode *inode, pgoff_t pgofs,
-					struct buffer_head *bh_result)
+					struct extent_info *ei)
 {
 	struct f2fs_inode_info *fi = F2FS_I(inode);
 	pgoff_t start_fofs, end_fofs;
@@ -271,18 +286,7 @@ static int check_extent_cache(struct inode *inode, pgoff_t pgofs,
 	start_blkaddr = fi->ext.blk;
 
 	if (pgofs >= start_fofs && pgofs <= end_fofs) {
-		unsigned int blkbits = inode->i_sb->s_blocksize_bits;
-		size_t count;
-
-		set_buffer_new(bh_result);
-		map_bh(bh_result, inode->i_sb,
-				start_blkaddr + pgofs - start_fofs);
-		count = end_fofs - pgofs + 1;
-		if (count < (UINT_MAX >> blkbits))
-			bh_result->b_size = (count << blkbits);
-		else
-			bh_result->b_size = UINT_MAX;
-
+		*ei = fi->ext;
 		stat_inc_read_hit(inode->i_sb);
 		read_unlock(&fi->ext_lock);
 		return 1;
@@ -658,13 +662,16 @@ static int __get_data_block(struct inode *inode, sector_t iblock,
 	int mode = create ? ALLOC_NODE : LOOKUP_NODE_RA;
 	pgoff_t pgofs, end_offset;
 	int err = 0, ofs = 1;
+	struct extent_info ei;
 	bool allocated = false;
 
 	/* Get the page offset from the block offset(iblock) */
 	pgofs =	(pgoff_t)(iblock >> (PAGE_CACHE_SHIFT - blkbits));
 
-	if (check_extent_cache(inode, pgofs, bh_result))
+	if (check_extent_cache(inode, pgofs, &ei)) {
+		f2fs_map_bh(inode->i_sb, pgofs, &ei, bh_result);
 		goto out;
+	}
 
 	if (create)
 		f2fs_lock_op(F2FS_I_SB(inode));

commit 4d0b0bd4385f0ce8d3b430f9667c5e2ca1de10af
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Thu Feb 5 17:47:25 2015 +0800

    f2fs: simplfy a field name in struct f2fs_extent,extent_info
    
    Rename a filed name from 'blk_addr' to 'blk' in struct {f2fs_extent,extent_info}
    as annotation of this field descripts its meaning well to us.
    
    By this way, we can avoid long statement in code of following patches.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 0811d6509fce..64f9049a9f25 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -268,7 +268,7 @@ static int check_extent_cache(struct inode *inode, pgoff_t pgofs,
 
 	start_fofs = fi->ext.fofs;
 	end_fofs = fi->ext.fofs + fi->ext.len - 1;
-	start_blkaddr = fi->ext.blk_addr;
+	start_blkaddr = fi->ext.blk;
 
 	if (pgofs >= start_fofs && pgofs <= end_fofs) {
 		unsigned int blkbits = inode->i_sb->s_blocksize_bits;
@@ -313,8 +313,8 @@ void update_extent_cache(struct dnode_of_data *dn)
 
 	start_fofs = fi->ext.fofs;
 	end_fofs = fi->ext.fofs + fi->ext.len - 1;
-	start_blkaddr = fi->ext.blk_addr;
-	end_blkaddr = fi->ext.blk_addr + fi->ext.len - 1;
+	start_blkaddr = fi->ext.blk;
+	end_blkaddr = fi->ext.blk + fi->ext.len - 1;
 
 	/* Drop and initialize the matched extent */
 	if (fi->ext.len == 1 && fofs == start_fofs)
@@ -324,7 +324,7 @@ void update_extent_cache(struct dnode_of_data *dn)
 	if (fi->ext.len == 0) {
 		if (dn->data_blkaddr != NULL_ADDR) {
 			fi->ext.fofs = fofs;
-			fi->ext.blk_addr = dn->data_blkaddr;
+			fi->ext.blk = dn->data_blkaddr;
 			fi->ext.len = 1;
 		}
 		goto end_update;
@@ -333,7 +333,7 @@ void update_extent_cache(struct dnode_of_data *dn)
 	/* Front merge */
 	if (fofs == start_fofs - 1 && dn->data_blkaddr == start_blkaddr - 1) {
 		fi->ext.fofs--;
-		fi->ext.blk_addr--;
+		fi->ext.blk--;
 		fi->ext.len++;
 		goto end_update;
 	}
@@ -351,8 +351,7 @@ void update_extent_cache(struct dnode_of_data *dn)
 			fi->ext.len = fofs - start_fofs;
 		} else {
 			fi->ext.fofs = fofs + 1;
-			fi->ext.blk_addr = start_blkaddr +
-					fofs - start_fofs + 1;
+			fi->ext.blk = start_blkaddr + fofs - start_fofs + 1;
 			fi->ext.len -= fofs - start_fofs + 1;
 		}
 	} else {

commit 0c872e2dedfc09f41a5604d1c5010f800c0bd8f1
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Thu Feb 5 17:46:29 2015 +0800

    f2fs: move ext_lock out of struct extent_info
    
    Move ext_lock out of struct extent_info, then in the following patches we can
    use variables with struct extent_info type as a parameter to pass pure data.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 985ed023a750..0811d6509fce 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -258,9 +258,9 @@ static int check_extent_cache(struct inode *inode, pgoff_t pgofs,
 	if (is_inode_flag_set(fi, FI_NO_EXTENT))
 		return 0;
 
-	read_lock(&fi->ext.ext_lock);
+	read_lock(&fi->ext_lock);
 	if (fi->ext.len == 0) {
-		read_unlock(&fi->ext.ext_lock);
+		read_unlock(&fi->ext_lock);
 		return 0;
 	}
 
@@ -284,10 +284,10 @@ static int check_extent_cache(struct inode *inode, pgoff_t pgofs,
 			bh_result->b_size = UINT_MAX;
 
 		stat_inc_read_hit(inode->i_sb);
-		read_unlock(&fi->ext.ext_lock);
+		read_unlock(&fi->ext_lock);
 		return 1;
 	}
-	read_unlock(&fi->ext.ext_lock);
+	read_unlock(&fi->ext_lock);
 	return 0;
 }
 
@@ -309,7 +309,7 @@ void update_extent_cache(struct dnode_of_data *dn)
 	fofs = start_bidx_of_node(ofs_of_node(dn->node_page), fi) +
 							dn->ofs_in_node;
 
-	write_lock(&fi->ext.ext_lock);
+	write_lock(&fi->ext_lock);
 
 	start_fofs = fi->ext.fofs;
 	end_fofs = fi->ext.fofs + fi->ext.len - 1;
@@ -366,7 +366,7 @@ void update_extent_cache(struct dnode_of_data *dn)
 		need_update = true;
 	}
 end_update:
-	write_unlock(&fi->ext.ext_lock);
+	write_unlock(&fi->ext_lock);
 	if (need_update)
 		sync_inode_page(dn);
 	return;

commit 59b802e5a453d413e7222d179be405cbadca3a8e
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Mon Feb 9 12:09:53 2015 -0800

    f2fs: allocate data blocks in advance for f2fs_direct_IO
    
    This patch adds preallocation for data blocks to prepare f2fs_direct_IO.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 26e247697e58..985ed023a750 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -592,6 +592,56 @@ static int __allocate_data_block(struct dnode_of_data *dn)
 	return 0;
 }
 
+static void __allocate_data_blocks(struct inode *inode, loff_t offset,
+							size_t count)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct dnode_of_data dn;
+	u64 start = F2FS_BYTES_TO_BLK(offset);
+	u64 len = F2FS_BYTES_TO_BLK(count);
+	bool allocated;
+	u64 end_offset;
+
+	while (len) {
+		f2fs_balance_fs(sbi);
+		f2fs_lock_op(sbi);
+
+		/* When reading holes, we need its node page */
+		set_new_dnode(&dn, inode, NULL, NULL, 0);
+		if (get_dnode_of_data(&dn, start, ALLOC_NODE))
+			goto out;
+
+		allocated = false;
+		end_offset = ADDRS_PER_PAGE(dn.node_page, F2FS_I(inode));
+
+		while (dn.ofs_in_node < end_offset && len) {
+			if (dn.data_blkaddr == NULL_ADDR) {
+				if (__allocate_data_block(&dn))
+					goto sync_out;
+				allocated = true;
+			}
+			len--;
+			start++;
+			dn.ofs_in_node++;
+		}
+
+		if (allocated)
+			sync_inode_page(&dn);
+
+		f2fs_put_dnode(&dn);
+		f2fs_unlock_op(sbi);
+	}
+	return;
+
+sync_out:
+	if (allocated)
+		sync_inode_page(&dn);
+	f2fs_put_dnode(&dn);
+out:
+	f2fs_unlock_op(sbi);
+	return;
+}
+
 /*
  * get_data_block() now supported readahead/bmap/rw direct_IO with mapped bh.
  * If original data blocks are allocated, then give them to blockdev.
@@ -617,10 +667,8 @@ static int __get_data_block(struct inode *inode, sector_t iblock,
 	if (check_extent_cache(inode, pgofs, bh_result))
 		goto out;
 
-	if (create) {
-		f2fs_balance_fs(F2FS_I_SB(inode));
+	if (create)
 		f2fs_lock_op(F2FS_I_SB(inode));
-	}
 
 	/* When reading holes, we need its node page */
 	set_new_dnode(&dn, inode, NULL, NULL, 0);
@@ -1108,6 +1156,9 @@ static ssize_t f2fs_direct_IO(int rw, struct kiocb *iocb,
 
 	trace_f2fs_direct_IO_enter(inode, offset, count, rw);
 
+	if (rw & WRITE)
+		__allocate_data_blocks(inode, offset, count);
+
 	err = blockdev_direct_IO(rw, iocb, inode, iter, offset, get_data_block);
 	if (err < 0 && (rw & WRITE))
 		f2fs_write_failed(mapping, offset + count);

commit da17eece035d72cb50d48529744a490784f29d2f
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Mon Feb 9 10:34:38 2015 -0800

    f2fs: call set_buffer_new for get_block
    
    This patch fixes wrong handling of buffer_new flag in get_block.
    If f2fs allocates new blocks and mapped buffer_head, it needs to set buffer_new
    for the bh_result.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index d166557ca867..26e247697e58 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -274,7 +274,7 @@ static int check_extent_cache(struct inode *inode, pgoff_t pgofs,
 		unsigned int blkbits = inode->i_sb->s_blocksize_bits;
 		size_t count;
 
-		clear_buffer_new(bh_result);
+		set_buffer_new(bh_result);
 		map_bh(bh_result, inode->i_sb,
 				start_blkaddr + pgofs - start_fofs);
 		count = end_fofs - pgofs + 1;
@@ -634,12 +634,14 @@ static int __get_data_block(struct inode *inode, sector_t iblock,
 		goto put_out;
 
 	if (dn.data_blkaddr != NULL_ADDR) {
+		set_buffer_new(bh_result);
 		map_bh(bh_result, inode->i_sb, dn.data_blkaddr);
 	} else if (create) {
 		err = __allocate_data_block(&dn);
 		if (err)
 			goto put_out;
 		allocated = true;
+		set_buffer_new(bh_result);
 		map_bh(bh_result, inode->i_sb, dn.data_blkaddr);
 	} else {
 		goto put_out;

commit 487261f39bcd8983f55c611e299f70f34659674b
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Thu Feb 5 17:44:29 2015 +0800

    f2fs: merge {invalidate,release}page for meta/node/data pages
    
    This patch merges ->{invalidate,release}page function for meta/node/data pages.
    
    After this, duplication of codes could be removed.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 27dc3fd9fb60..d166557ca867 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1115,20 +1115,28 @@ static ssize_t f2fs_direct_IO(int rw, struct kiocb *iocb,
 	return err;
 }
 
-static void f2fs_invalidate_data_page(struct page *page, unsigned int offset,
-				      unsigned int length)
+void f2fs_invalidate_page(struct page *page, unsigned int offset,
+							unsigned int length)
 {
 	struct inode *inode = page->mapping->host;
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
 
-	if (offset % PAGE_CACHE_SIZE || length != PAGE_CACHE_SIZE)
+	if (inode->i_ino >= F2FS_ROOT_INO(sbi) &&
+		(offset % PAGE_CACHE_SIZE || length != PAGE_CACHE_SIZE))
 		return;
 
-	if (PageDirty(page))
-		inode_dec_dirty_pages(inode);
+	if (PageDirty(page)) {
+		if (inode->i_ino == F2FS_META_INO(sbi))
+			dec_page_count(sbi, F2FS_DIRTY_META);
+		else if (inode->i_ino == F2FS_NODE_INO(sbi))
+			dec_page_count(sbi, F2FS_DIRTY_NODES);
+		else
+			inode_dec_dirty_pages(inode);
+	}
 	ClearPagePrivate(page);
 }
 
-static int f2fs_release_data_page(struct page *page, gfp_t wait)
+int f2fs_release_page(struct page *page, gfp_t wait)
 {
 	/* If this is dirty page, keep PagePrivate */
 	if (PageDirty(page))
@@ -1183,8 +1191,8 @@ const struct address_space_operations f2fs_dblock_aops = {
 	.write_begin	= f2fs_write_begin,
 	.write_end	= f2fs_write_end,
 	.set_page_dirty	= f2fs_set_data_page_dirty,
-	.invalidatepage	= f2fs_invalidate_data_page,
-	.releasepage	= f2fs_release_data_page,
+	.invalidatepage	= f2fs_invalidate_page,
+	.releasepage	= f2fs_release_page,
 	.direct_IO	= f2fs_direct_IO,
 	.bmap		= f2fs_bmap,
 };

commit f68daeebba5a697f31f64c07b8693fa678981819
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Jan 30 11:39:08 2015 -0800

    f2fs: keep PagePrivate during releasepage
    
    If PagePrivate is removed by releasepage, f2fs loses counting dirty pages.
    
    e.g., try_to_release_page will not release page when the page is dirty,
    but our releasepage removes PagePrivate.
    
        [<ffffffff81188d75>] try_to_release_page+0x35/0x50
        [<ffffffff811996f9>] invalidate_inode_pages2_range+0x2f9/0x3b0
        [<ffffffffa02a7f54>] ? truncate_blocks+0x384/0x4d0 [f2fs]
        [<ffffffffa02b7583>] ? f2fs_direct_IO+0x283/0x290 [f2fs]
        [<ffffffffa02b7fb0>] ? get_data_block_fiemap+0x20/0x20 [f2fs]
        [<ffffffff8118aa53>] generic_file_direct_write+0x163/0x170
        [<ffffffff8118ad06>] __generic_file_write_iter+0x2a6/0x350
        [<ffffffff8118adef>] generic_file_write_iter+0x3f/0xb0
        [<ffffffff81203081>] new_sync_write+0x81/0xb0
        [<ffffffff81203837>] vfs_write+0xb7/0x1f0
        [<ffffffff81204459>] SyS_write+0x49/0xb0
        [<ffffffff817c286d>] system_call_fastpath+0x16/0x1b
    
    Reviewed-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 6d9e6e4ce439..27dc3fd9fb60 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1130,6 +1130,10 @@ static void f2fs_invalidate_data_page(struct page *page, unsigned int offset,
 
 static int f2fs_release_data_page(struct page *page, gfp_t wait)
 {
+	/* If this is dirty page, keep PagePrivate */
+	if (PageDirty(page))
+		return 0;
+
 	ClearPagePrivate(page);
 	return 1;
 }

commit caf0047e7e1e60a7ad1d655d3b81b32e2dfb6095
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Wed Jan 28 17:48:42 2015 +0800

    f2fs: merge flags in struct f2fs_sb_info
    
    Currently, there are several variables with Boolean type as below:
    
    struct f2fs_sb_info {
    ...
            int s_dirty;
            bool need_fsck;
            bool s_closing;
    ...
            bool por_doing;
    ...
    }
    
    For this there are some issues:
    1. there are some space of f2fs_sb_info is wasted due to aligning after Boolean
       type variables by compiler.
    2. if we continuously add new flag into f2fs_sb_info, structure will be messed
       up.
    
    So in this patch, we try to:
    1. switch s_dirty to Boolean type variable since it has two status 0/1.
    2. merge s_dirty/need_fsck/s_closing/por_doing variables into s_flag.
    3. introduce an enum type which can indicate different states of sbi.
    4. use new introduced universal interfaces is_sbi_flag_set/{set,clear}_sbi_flag
       to operate flags for sbi.
    
    After that, above issues will be fixed.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index a7b905cb05f8..6d9e6e4ce439 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -818,7 +818,7 @@ static int f2fs_write_data_page(struct page *page,
 
 	zero_user_segment(page, offset, PAGE_CACHE_SIZE);
 write:
-	if (unlikely(sbi->por_doing))
+	if (unlikely(is_sbi_flag_set(sbi, SBI_POR_DOING)))
 		goto redirty_out;
 	if (f2fs_is_drop_cache(inode))
 		goto out;

commit df1991391f9301efbd5e46fb776bf5103d4c59f8
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Tue Jan 6 16:00:03 2015 -0800

    f2fs: fix wrong unlock_page call
    
    This patch removes wrongly called unlock_page.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index b48b355104c3..a7b905cb05f8 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -837,7 +837,6 @@ static int f2fs_write_data_page(struct page *page,
 	/* we should bypass data pages to proceed the kworkder jobs */
 	if (unlikely(f2fs_cp_error(sbi))) {
 		SetPageError(page);
-		unlock_page(page);
 		goto out;
 	}
 

commit 38aa0889b2504bbe68e47f51cf73bf7f0a7246bd
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Mon Jan 5 16:02:20 2015 -0800

    f2fs: align direct_io'ed data to section
    
    This patch aligns the start block address of a file for direct io to the f2fs's
    section size.
    
    Some flash devices manage an over 4KB-sized page as a write unit, and if the
    direct_io'ed data are written but not aligned to that unit, the performance can
    be degraded due to the partial page copies.
    
    Thus, since f2fs has a section that is well aligned to FTL units, we can align
    the block address to the section size so that f2fs avoids this misalignment.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 3e0f5f303c97..b48b355104c3 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -564,6 +564,7 @@ static int __allocate_data_block(struct dnode_of_data *dn)
 	struct f2fs_inode_info *fi = F2FS_I(dn->inode);
 	struct f2fs_summary sum;
 	struct node_info ni;
+	int seg = CURSEG_WARM_DATA;
 	pgoff_t fofs;
 
 	if (unlikely(is_inode_flag_set(F2FS_I(dn->inode), FI_NO_ALLOC)))
@@ -574,8 +575,10 @@ static int __allocate_data_block(struct dnode_of_data *dn)
 	get_node_info(sbi, dn->nid, &ni);
 	set_summary(&sum, dn->nid, dn->ofs_in_node, ni.version);
 
-	allocate_data_block(sbi, NULL, NULL_ADDR, &dn->data_blkaddr, &sum,
-							CURSEG_WARM_DATA);
+	if (dn->ofs_in_node == 0 && dn->inode_page == dn->node_page)
+		seg = CURSEG_DIRECT_IO;
+
+	allocate_data_block(sbi, NULL, NULL_ADDR, &dn->data_blkaddr, &sum, seg);
 
 	/* direct IO doesn't use extent cache to maximize the performance */
 	__set_data_blkaddr(dn);

commit 41ef94b35c8df8e01780b4a3362246c51f3aa79b
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Tue Dec 30 23:12:11 2014 -0800

    f2fs: remove uncovered code path
    
    This patch removes unnecessary function calls.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 155885bf714c..3e0f5f303c97 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -563,31 +563,22 @@ static int __allocate_data_block(struct dnode_of_data *dn)
 	struct f2fs_sb_info *sbi = F2FS_I_SB(dn->inode);
 	struct f2fs_inode_info *fi = F2FS_I(dn->inode);
 	struct f2fs_summary sum;
-	block_t new_blkaddr;
 	struct node_info ni;
 	pgoff_t fofs;
-	int type;
 
 	if (unlikely(is_inode_flag_set(F2FS_I(dn->inode), FI_NO_ALLOC)))
 		return -EPERM;
 	if (unlikely(!inc_valid_block_count(sbi, dn->inode, 1)))
 		return -ENOSPC;
 
-	dn->data_blkaddr = NEW_ADDR;
-	__set_data_blkaddr(dn);
-
 	get_node_info(sbi, dn->nid, &ni);
 	set_summary(&sum, dn->nid, dn->ofs_in_node, ni.version);
 
-	type = CURSEG_WARM_DATA;
-
-	allocate_data_block(sbi, NULL, NULL_ADDR, &new_blkaddr, &sum, type);
+	allocate_data_block(sbi, NULL, NULL_ADDR, &dn->data_blkaddr, &sum,
+							CURSEG_WARM_DATA);
 
 	/* direct IO doesn't use extent cache to maximize the performance */
-	set_inode_flag(F2FS_I(dn->inode), FI_NO_EXTENT);
-	dn->data_blkaddr = new_blkaddr;
-	update_extent_cache(dn);
-	clear_inode_flag(F2FS_I(dn->inode), FI_NO_EXTENT);
+	__set_data_blkaddr(dn);
 
 	/* update i_size */
 	fofs = start_bidx_of_node(ofs_of_node(dn->node_page), fi) +
@@ -595,7 +586,6 @@ static int __allocate_data_block(struct dnode_of_data *dn)
 	if (i_size_read(dn->inode) < ((fofs + 1) << PAGE_CACHE_SHIFT))
 		i_size_write(dn->inode, ((fofs + 1) << PAGE_CACHE_SHIFT));
 
-	dn->data_blkaddr = new_blkaddr;
 	return 0;
 }
 

commit 3547ea961dd66a474c6f709c4f5e8a2472289df9
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Tue Dec 30 23:08:26 2014 -0800

    f2fs: avoid potential unnecessary codes
    
    This patch relocates some operations to avoid unnecessary execution.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 2c0cb6617918..155885bf714c 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -299,8 +299,6 @@ void update_extent_cache(struct dnode_of_data *dn)
 	int need_update = true;
 
 	f2fs_bug_on(F2FS_I_SB(dn->inode), dn->data_blkaddr == NEW_ADDR);
-	fofs = start_bidx_of_node(ofs_of_node(dn->node_page), fi) +
-							dn->ofs_in_node;
 
 	/* Update the page address in the parent node */
 	__set_data_blkaddr(dn);
@@ -308,6 +306,9 @@ void update_extent_cache(struct dnode_of_data *dn)
 	if (is_inode_flag_set(fi, FI_NO_EXTENT))
 		return;
 
+	fofs = start_bidx_of_node(ofs_of_node(dn->node_page), fi) +
+							dn->ofs_in_node;
+
 	write_lock(&fi->ext.ext_lock);
 
 	start_fofs = fi->ext.fofs;

commit e1509cf294cc670cda1fedd430f0ff175c42b591
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Tue Dec 30 22:57:55 2014 -0800

    f2fs: clean up to remove parameter
    
    This patch uses dn->data_blkaddr as a parameter for the destination block
    address.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 6308435028f6..2c0cb6617918 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -197,7 +197,7 @@ void f2fs_submit_page_mbio(struct f2fs_sb_info *sbi, struct page *page,
  *  ->node_page
  *    update block addresses in the node page
  */
-static void __set_data_blkaddr(struct dnode_of_data *dn, block_t new_addr)
+static void __set_data_blkaddr(struct dnode_of_data *dn)
 {
 	struct f2fs_node *rn;
 	__le32 *addr_array;
@@ -210,7 +210,7 @@ static void __set_data_blkaddr(struct dnode_of_data *dn, block_t new_addr)
 
 	/* Get physical address of data block */
 	addr_array = blkaddr_in_node(rn);
-	addr_array[ofs_in_node] = cpu_to_le32(new_addr);
+	addr_array[ofs_in_node] = cpu_to_le32(dn->data_blkaddr);
 	set_page_dirty(node_page);
 }
 
@@ -225,8 +225,8 @@ int reserve_new_block(struct dnode_of_data *dn)
 
 	trace_f2fs_reserve_new_block(dn->inode, dn->nid, dn->ofs_in_node);
 
-	__set_data_blkaddr(dn, NEW_ADDR);
 	dn->data_blkaddr = NEW_ADDR;
+	__set_data_blkaddr(dn);
 	mark_inode_dirty(dn->inode);
 	sync_inode_page(dn);
 	return 0;
@@ -291,19 +291,19 @@ static int check_extent_cache(struct inode *inode, pgoff_t pgofs,
 	return 0;
 }
 
-void update_extent_cache(block_t blk_addr, struct dnode_of_data *dn)
+void update_extent_cache(struct dnode_of_data *dn)
 {
 	struct f2fs_inode_info *fi = F2FS_I(dn->inode);
 	pgoff_t fofs, start_fofs, end_fofs;
 	block_t start_blkaddr, end_blkaddr;
 	int need_update = true;
 
-	f2fs_bug_on(F2FS_I_SB(dn->inode), blk_addr == NEW_ADDR);
+	f2fs_bug_on(F2FS_I_SB(dn->inode), dn->data_blkaddr == NEW_ADDR);
 	fofs = start_bidx_of_node(ofs_of_node(dn->node_page), fi) +
 							dn->ofs_in_node;
 
 	/* Update the page address in the parent node */
-	__set_data_blkaddr(dn, blk_addr);
+	__set_data_blkaddr(dn);
 
 	if (is_inode_flag_set(fi, FI_NO_EXTENT))
 		return;
@@ -321,16 +321,16 @@ void update_extent_cache(block_t blk_addr, struct dnode_of_data *dn)
 
 	/* Initial extent */
 	if (fi->ext.len == 0) {
-		if (blk_addr != NULL_ADDR) {
+		if (dn->data_blkaddr != NULL_ADDR) {
 			fi->ext.fofs = fofs;
-			fi->ext.blk_addr = blk_addr;
+			fi->ext.blk_addr = dn->data_blkaddr;
 			fi->ext.len = 1;
 		}
 		goto end_update;
 	}
 
 	/* Front merge */
-	if (fofs == start_fofs - 1 && blk_addr == start_blkaddr - 1) {
+	if (fofs == start_fofs - 1 && dn->data_blkaddr == start_blkaddr - 1) {
 		fi->ext.fofs--;
 		fi->ext.blk_addr--;
 		fi->ext.len++;
@@ -338,7 +338,7 @@ void update_extent_cache(block_t blk_addr, struct dnode_of_data *dn)
 	}
 
 	/* Back merge */
-	if (fofs == end_fofs + 1 && blk_addr == end_blkaddr + 1) {
+	if (fofs == end_fofs + 1 && dn->data_blkaddr == end_blkaddr + 1) {
 		fi->ext.len++;
 		goto end_update;
 	}
@@ -572,8 +572,8 @@ static int __allocate_data_block(struct dnode_of_data *dn)
 	if (unlikely(!inc_valid_block_count(sbi, dn->inode, 1)))
 		return -ENOSPC;
 
-	__set_data_blkaddr(dn, NEW_ADDR);
 	dn->data_blkaddr = NEW_ADDR;
+	__set_data_blkaddr(dn);
 
 	get_node_info(sbi, dn->nid, &ni);
 	set_summary(&sum, dn->nid, dn->ofs_in_node, ni.version);
@@ -584,7 +584,8 @@ static int __allocate_data_block(struct dnode_of_data *dn)
 
 	/* direct IO doesn't use extent cache to maximize the performance */
 	set_inode_flag(F2FS_I(dn->inode), FI_NO_EXTENT);
-	update_extent_cache(new_blkaddr, dn);
+	dn->data_blkaddr = new_blkaddr;
+	update_extent_cache(dn);
 	clear_inode_flag(F2FS_I(dn->inode), FI_NO_EXTENT);
 
 	/* update i_size */
@@ -784,7 +785,7 @@ int do_write_data_page(struct page *page, struct f2fs_io_info *fio)
 		set_inode_flag(F2FS_I(inode), FI_UPDATE_WRITE);
 	} else {
 		write_data_page(page, &dn, fio);
-		update_extent_cache(fio->blk_addr, &dn);
+		update_extent_cache(&dn);
 		set_inode_flag(F2FS_I(inode), FI_APPEND_WRITE);
 	}
 out_writepage:

commit 2ace38e00e54f5c722d8c5eba36d1172548a3466
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Wed Dec 24 16:08:14 2014 +0800

    f2fs: cleanup parameters for trace_f2fs_submit_{read_,write_,page_,page_m}bio with fio
    
    Cleanup parameters for trace_f2fs_submit_{read_,write_,page_,page_m}bio with fio
    as one parameter.
    
    Suggested-by: Jaegeuk Kim <jaegeuk@kernel.org>
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 7953bc279205..6308435028f6 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -96,11 +96,9 @@ static void __submit_merged_bio(struct f2fs_bio_info *io)
 		return;
 
 	if (is_read_io(fio->rw))
-		trace_f2fs_submit_read_bio(io->sbi->sb, fio->rw,
-							fio->type, io->bio);
+		trace_f2fs_submit_read_bio(io->sbi->sb, fio, io->bio);
 	else
-		trace_f2fs_submit_write_bio(io->sbi->sb, fio->rw,
-							fio->type, io->bio);
+		trace_f2fs_submit_write_bio(io->sbi->sb, fio, io->bio);
 
 	submit_bio(fio->rw, io->bio);
 	io->bio = NULL;
@@ -137,7 +135,7 @@ int f2fs_submit_page_bio(struct f2fs_sb_info *sbi, struct page *page,
 {
 	struct bio *bio;
 
-	trace_f2fs_submit_page_bio(page, fio->blk_addr, fio->rw, fio->type);
+	trace_f2fs_submit_page_bio(page, fio);
 	f2fs_trace_ios(page, fio, 0);
 
 	/* Allocate a new bio */
@@ -190,7 +188,7 @@ void f2fs_submit_page_mbio(struct f2fs_sb_info *sbi, struct page *page,
 	f2fs_trace_ios(page, fio, 0);
 
 	up_write(&io->io_rwsem);
-	trace_f2fs_submit_page_mbio(page, fio->blk_addr, fio->rw, fio->type);
+	trace_f2fs_submit_page_mbio(page, fio);
 }
 
 /*

commit 3e1c8f125eeea0f8111e2b9131162bfba32c6381
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Tue Dec 23 16:35:21 2014 +0800

    f2fs: cleanup trace event of f2fs_submit_page_{m,}bio with DECLARE_EVENT_CLASS
    
    This patch adds missing parameter _type_ for trace_f2fs_submit_page_bio, then
    use DECLARE_EVENT_CLASS/DEFINE_EVENT_CONDITION pair to cleanup some trace event
    code related to f2fs_submit_page_{m,}bio.
    
    Additionally, after we remove redundant code, size of code can be reduced:
       text    data     bss     dec     hex filename
     176787    8712      56  185555   2d4d3 f2fs.ko.org
     174408    8648      56  183112   2cb48 f2fs.ko
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 20aa3c355a0e..7953bc279205 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -137,7 +137,7 @@ int f2fs_submit_page_bio(struct f2fs_sb_info *sbi, struct page *page,
 {
 	struct bio *bio;
 
-	trace_f2fs_submit_page_bio(page, fio->blk_addr, fio->rw);
+	trace_f2fs_submit_page_bio(page, fio->blk_addr, fio->rw, fio->type);
 	f2fs_trace_ios(page, fio, 0);
 
 	/* Allocate a new bio */
@@ -190,7 +190,7 @@ void f2fs_submit_page_mbio(struct f2fs_sb_info *sbi, struct page *page,
 	f2fs_trace_ios(page, fio, 0);
 
 	up_write(&io->io_rwsem);
-	trace_f2fs_submit_page_mbio(page, fio->rw, fio->type, fio->blk_addr);
+	trace_f2fs_submit_page_mbio(page, fio->blk_addr, fio->rw, fio->type);
 }
 
 /*

commit db9f7c1a9561e998d6227bcc1c19bc4c1fbbca1b
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed Dec 17 20:04:08 2014 -0800

    f2fs: activate f2fs_trace_ios
    
    This patch activates f2fs_trace_ios.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index d86f8b1413f1..20aa3c355a0e 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -22,6 +22,7 @@
 #include "f2fs.h"
 #include "node.h"
 #include "segment.h"
+#include "trace.h"
 #include <trace/events/f2fs.h>
 
 static void f2fs_read_end_io(struct bio *bio, int err)
@@ -137,6 +138,7 @@ int f2fs_submit_page_bio(struct f2fs_sb_info *sbi, struct page *page,
 	struct bio *bio;
 
 	trace_f2fs_submit_page_bio(page, fio->blk_addr, fio->rw);
+	f2fs_trace_ios(page, fio, 0);
 
 	/* Allocate a new bio */
 	bio = __bio_alloc(sbi, fio->blk_addr, 1, is_read_io(fio->rw));
@@ -185,6 +187,7 @@ void f2fs_submit_page_mbio(struct f2fs_sb_info *sbi, struct page *page,
 	}
 
 	io->last_block_in_bio = fio->blk_addr;
+	f2fs_trace_ios(page, fio, 0);
 
 	up_write(&io->io_rwsem);
 	trace_f2fs_submit_page_mbio(page, fio->rw, fio->type, fio->blk_addr);

commit cf04e8eb55290c7b836c36f0b4e1a8d0fe8ee275
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed Dec 17 19:33:13 2014 -0800

    f2fs: use f2fs_io_info to clean up messy parameters during IO path
    
    This patch cleans up parameters on IO paths.
    The key idea is to use f2fs_io_info adding a parameter, block address, and then
    use this structure as parameters.
    
    Reviewed-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index caa08e46697a..d86f8b1413f1 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -132,14 +132,14 @@ void f2fs_submit_merged_bio(struct f2fs_sb_info *sbi,
  * Return unlocked page.
  */
 int f2fs_submit_page_bio(struct f2fs_sb_info *sbi, struct page *page,
-					block_t blk_addr, int rw)
+					struct f2fs_io_info *fio)
 {
 	struct bio *bio;
 
-	trace_f2fs_submit_page_bio(page, blk_addr, rw);
+	trace_f2fs_submit_page_bio(page, fio->blk_addr, fio->rw);
 
 	/* Allocate a new bio */
-	bio = __bio_alloc(sbi, blk_addr, 1, is_read_io(rw));
+	bio = __bio_alloc(sbi, fio->blk_addr, 1, is_read_io(fio->rw));
 
 	if (bio_add_page(bio, page, PAGE_CACHE_SIZE, 0) < PAGE_CACHE_SIZE) {
 		bio_put(bio);
@@ -147,12 +147,12 @@ int f2fs_submit_page_bio(struct f2fs_sb_info *sbi, struct page *page,
 		return -EFAULT;
 	}
 
-	submit_bio(rw, bio);
+	submit_bio(fio->rw, bio);
 	return 0;
 }
 
 void f2fs_submit_page_mbio(struct f2fs_sb_info *sbi, struct page *page,
-			block_t blk_addr, struct f2fs_io_info *fio)
+					struct f2fs_io_info *fio)
 {
 	enum page_type btype = PAGE_TYPE_OF_BIO(fio->type);
 	struct f2fs_bio_info *io;
@@ -160,21 +160,21 @@ void f2fs_submit_page_mbio(struct f2fs_sb_info *sbi, struct page *page,
 
 	io = is_read ? &sbi->read_io : &sbi->write_io[btype];
 
-	verify_block_addr(sbi, blk_addr);
+	verify_block_addr(sbi, fio->blk_addr);
 
 	down_write(&io->io_rwsem);
 
 	if (!is_read)
 		inc_page_count(sbi, F2FS_WRITEBACK);
 
-	if (io->bio && (io->last_block_in_bio != blk_addr - 1 ||
+	if (io->bio && (io->last_block_in_bio != fio->blk_addr - 1 ||
 						io->fio.rw != fio->rw))
 		__submit_merged_bio(io);
 alloc_new:
 	if (io->bio == NULL) {
 		int bio_blocks = MAX_BIO_BLOCKS(sbi);
 
-		io->bio = __bio_alloc(sbi, blk_addr, bio_blocks, is_read);
+		io->bio = __bio_alloc(sbi, fio->blk_addr, bio_blocks, is_read);
 		io->fio = *fio;
 	}
 
@@ -184,10 +184,10 @@ void f2fs_submit_page_mbio(struct f2fs_sb_info *sbi, struct page *page,
 		goto alloc_new;
 	}
 
-	io->last_block_in_bio = blk_addr;
+	io->last_block_in_bio = fio->blk_addr;
 
 	up_write(&io->io_rwsem);
-	trace_f2fs_submit_page_mbio(page, fio->rw, fio->type, blk_addr);
+	trace_f2fs_submit_page_mbio(page, fio->rw, fio->type, fio->blk_addr);
 }
 
 /*
@@ -376,6 +376,10 @@ struct page *find_data_page(struct inode *inode, pgoff_t index, bool sync)
 	struct dnode_of_data dn;
 	struct page *page;
 	int err;
+	struct f2fs_io_info fio = {
+		.type = DATA,
+		.rw = sync ? READ_SYNC : READA,
+	};
 
 	page = find_get_page(mapping, index);
 	if (page && PageUptodate(page))
@@ -404,8 +408,8 @@ struct page *find_data_page(struct inode *inode, pgoff_t index, bool sync)
 		return page;
 	}
 
-	err = f2fs_submit_page_bio(F2FS_I_SB(inode), page, dn.data_blkaddr,
-					sync ? READ_SYNC : READA);
+	fio.blk_addr = dn.data_blkaddr;
+	err = f2fs_submit_page_bio(F2FS_I_SB(inode), page, &fio);
 	if (err)
 		return ERR_PTR(err);
 
@@ -430,7 +434,10 @@ struct page *get_lock_data_page(struct inode *inode, pgoff_t index)
 	struct dnode_of_data dn;
 	struct page *page;
 	int err;
-
+	struct f2fs_io_info fio = {
+		.type = DATA,
+		.rw = READ_SYNC,
+	};
 repeat:
 	page = grab_cache_page(mapping, index);
 	if (!page)
@@ -464,8 +471,8 @@ struct page *get_lock_data_page(struct inode *inode, pgoff_t index)
 		return page;
 	}
 
-	err = f2fs_submit_page_bio(F2FS_I_SB(inode), page,
-					dn.data_blkaddr, READ_SYNC);
+	fio.blk_addr = dn.data_blkaddr;
+	err = f2fs_submit_page_bio(F2FS_I_SB(inode), page, &fio);
 	if (err)
 		return ERR_PTR(err);
 
@@ -515,8 +522,12 @@ struct page *get_new_data_page(struct inode *inode,
 		zero_user_segment(page, 0, PAGE_CACHE_SIZE);
 		SetPageUptodate(page);
 	} else {
-		err = f2fs_submit_page_bio(F2FS_I_SB(inode), page,
-						dn.data_blkaddr, READ_SYNC);
+		struct f2fs_io_info fio = {
+			.type = DATA,
+			.rw = READ_SYNC,
+			.blk_addr = dn.data_blkaddr,
+		};
+		err = f2fs_submit_page_bio(F2FS_I_SB(inode), page, &fio);
 		if (err)
 			goto put_err;
 
@@ -745,7 +756,6 @@ static int f2fs_read_data_pages(struct file *file,
 int do_write_data_page(struct page *page, struct f2fs_io_info *fio)
 {
 	struct inode *inode = page->mapping->host;
-	block_t old_blkaddr, new_blkaddr;
 	struct dnode_of_data dn;
 	int err = 0;
 
@@ -754,10 +764,10 @@ int do_write_data_page(struct page *page, struct f2fs_io_info *fio)
 	if (err)
 		return err;
 
-	old_blkaddr = dn.data_blkaddr;
+	fio->blk_addr = dn.data_blkaddr;
 
 	/* This page is already truncated */
-	if (old_blkaddr == NULL_ADDR)
+	if (fio->blk_addr == NULL_ADDR)
 		goto out_writepage;
 
 	set_page_writeback(page);
@@ -766,14 +776,14 @@ int do_write_data_page(struct page *page, struct f2fs_io_info *fio)
 	 * If current allocation needs SSR,
 	 * it had better in-place writes for updated data.
 	 */
-	if (unlikely(old_blkaddr != NEW_ADDR &&
+	if (unlikely(fio->blk_addr != NEW_ADDR &&
 			!is_cold_data(page) &&
 			need_inplace_update(inode))) {
-		rewrite_data_page(page, old_blkaddr, fio);
+		rewrite_data_page(page, fio);
 		set_inode_flag(F2FS_I(inode), FI_UPDATE_WRITE);
 	} else {
-		write_data_page(page, &dn, &new_blkaddr, fio);
-		update_extent_cache(new_blkaddr, &dn);
+		write_data_page(page, &dn, fio);
+		update_extent_cache(fio->blk_addr, &dn);
 		set_inode_flag(F2FS_I(inode), FI_APPEND_WRITE);
 	}
 out_writepage:
@@ -1007,8 +1017,12 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 	if (dn.data_blkaddr == NEW_ADDR) {
 		zero_user_segment(page, 0, PAGE_CACHE_SIZE);
 	} else {
-		err = f2fs_submit_page_bio(sbi, page, dn.data_blkaddr,
-					   READ_SYNC);
+		struct f2fs_io_info fio = {
+			.type = DATA,
+			.rw = READ_SYNC,
+			.blk_addr = dn.data_blkaddr,
+		};
+		err = f2fs_submit_page_bio(sbi, page, &fio);
 		if (err)
 			goto fail;
 

commit 042b7816aaebb1eb137b9889c20b595d951d15b7
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Dec 12 19:40:02 2014 -0800

    f2fs: remove unnecessary call to invalidate inmemory pages
    
    Now we use inmemory pages for atomic write only and provide abort procedure,
    we don't need to truncate them explicitly.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 32264e3d524f..caa08e46697a 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1114,9 +1114,6 @@ static void f2fs_invalidate_data_page(struct page *page, unsigned int offset,
 	if (offset % PAGE_CACHE_SIZE || length != PAGE_CACHE_SIZE)
 		return;
 
-	if (f2fs_is_atomic_file(inode))
-		invalidate_inmem_page(inode, page);
-
 	if (PageDirty(page))
 		inode_dec_dirty_pages(inode);
 	ClearPagePrivate(page);

commit 1e84371ffeef451e8532e0cd04c2fe59ff10c514
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Tue Dec 9 06:08:59 2014 -0800

    f2fs: change atomic and volatile write policies
    
    This patch adds two new ioctls to release inmemory pages grabbed by atomic
    writes.
     o f2fs_ioc_abort_volatile_write
      - If transaction was failed, all the grabbed pages and data should be written.
     o f2fs_ioc_release_volatile_write
      - This is to enhance the performance of PERSIST mode in sqlite.
    
    In order to avoid huge memory consumption which causes OOM, this patch changes
    volatile writes to use normal dirty pages, instead blocked flushing to the disk
    as long as system does not suffer from memory pressure.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 7ec697b37f19..32264e3d524f 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -814,6 +814,11 @@ static int f2fs_write_data_page(struct page *page,
 write:
 	if (unlikely(sbi->por_doing))
 		goto redirty_out;
+	if (f2fs_is_drop_cache(inode))
+		goto out;
+	if (f2fs_is_volatile_file(inode) && !wbc->for_reclaim &&
+			available_free_memory(sbi, BASE_CHECK))
+		goto redirty_out;
 
 	/* Dentry blocks are controlled by checkpoint */
 	if (S_ISDIR(inode->i_mode)) {
@@ -1109,7 +1114,7 @@ static void f2fs_invalidate_data_page(struct page *page, unsigned int offset,
 	if (offset % PAGE_CACHE_SIZE || length != PAGE_CACHE_SIZE)
 		return;
 
-	if (f2fs_is_atomic_file(inode) || f2fs_is_volatile_file(inode))
+	if (f2fs_is_atomic_file(inode))
 		invalidate_inmem_page(inode, page);
 
 	if (PageDirty(page))
@@ -1132,7 +1137,7 @@ static int f2fs_set_data_page_dirty(struct page *page)
 
 	SetPageUptodate(page);
 
-	if (f2fs_is_atomic_file(inode) || f2fs_is_volatile_file(inode)) {
+	if (f2fs_is_atomic_file(inode)) {
 		register_inmem_page(inode, page);
 		return 1;
 	}

commit cd34e2969b28de7685bb51d4b0fafeced8ef7f66
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Mon Dec 1 11:30:20 2014 +0800

    f2fs: fix to return correct error number in f2fs_write_begin
    
    Fix the wrong error number in error path of f2fs_write_begin.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index c7bc62641103..7ec697b37f19 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -960,8 +960,10 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 
 	/* check inline_data */
 	ipage = get_node_page(sbi, inode->i_ino);
-	if (IS_ERR(ipage))
+	if (IS_ERR(ipage)) {
+		err = PTR_ERR(ipage);
 		goto unlock_fail;
+	}
 
 	set_new_dnode(&dn, inode, ipage, ipage, 0);
 

commit 5f72739583a29bfaa57448ec2c9b122995d0ae4f
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Tue Nov 25 10:59:45 2014 -0800

    f2fs: fix deadlock during inline_data conversion
    
    A deadlock can be occurred:
    Thread 1]                             Thread 2]
     - f2fs_write_data_pages              - f2fs_write_begin
       - lock_page(page #0)
                                            - grab_cache_page(page #X)
                                            - get_node_page(inode_page)
                                            - grab_cache_page(page #0)
                                              : to convert inline_data
       - f2fs_write_data_page
         - f2fs_write_inline_data
           - get_node_page(inode_page)
    
    In this case, trying to lock inode_page and page #0 causes deadlock.
    In order to avoid this, this patch adds a rule for this locking policy,
    which is that page #0 should be locked followed by inode_page lock.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 12dd58aa569a..c7bc62641103 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -936,6 +936,17 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 	trace_f2fs_write_begin(inode, pos, len, flags);
 
 	f2fs_balance_fs(sbi);
+
+	/*
+	 * We should check this at this moment to avoid deadlock on inode page
+	 * and #0 page. The locking rule for inline_data conversion should be:
+	 * lock_page(page #0) -> lock_page(inode_page)
+	 */
+	if (index != 0) {
+		err = f2fs_convert_inline_inode(inode);
+		if (err)
+			goto fail;
+	}
 repeat:
 	page = grab_cache_page_write_begin(mapping, index, flags);
 	if (!page) {
@@ -960,21 +971,10 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 			set_inode_flag(F2FS_I(inode), FI_DATA_EXIST);
 			sync_inode_page(&dn);
 			goto put_next;
-		} else if (page->index == 0) {
-			err = f2fs_convert_inline_page(&dn, page);
-			if (err)
-				goto put_fail;
-		} else {
-			struct page *p = grab_cache_page(inode->i_mapping, 0);
-			if (!p) {
-				err = -ENOMEM;
-				goto put_fail;
-			}
-			err = f2fs_convert_inline_page(&dn, p);
-			f2fs_put_page(p, 1);
-			if (err)
-				goto put_fail;
 		}
+		err = f2fs_convert_inline_page(&dn, page);
+		if (err)
+			goto put_fail;
 	}
 	err = f2fs_reserve_block(&dn, index);
 	if (err)

commit 8cdcb71322ec21aaee90117b0c01d576851a8faa
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Mon Nov 17 16:14:11 2014 -0800

    f2fs: put the inode page when error was occurred
    
    We should put the inode page when error was occurred.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 8f164432408e..12dd58aa569a 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -963,22 +963,22 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 		} else if (page->index == 0) {
 			err = f2fs_convert_inline_page(&dn, page);
 			if (err)
-				goto unlock_fail;
+				goto put_fail;
 		} else {
 			struct page *p = grab_cache_page(inode->i_mapping, 0);
 			if (!p) {
 				err = -ENOMEM;
-				goto unlock_fail;
+				goto put_fail;
 			}
 			err = f2fs_convert_inline_page(&dn, p);
 			f2fs_put_page(p, 1);
 			if (err)
-				goto unlock_fail;
+				goto put_fail;
 		}
 	}
 	err = f2fs_reserve_block(&dn, index);
 	if (err)
-		goto unlock_fail;
+		goto put_fail;
 put_next:
 	f2fs_put_dnode(&dn);
 	f2fs_unlock_op(sbi);
@@ -1021,6 +1021,8 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 	clear_cold_data(page);
 	return 0;
 
+put_fail:
+	f2fs_put_dnode(&dn);
 unlock_fail:
 	f2fs_unlock_op(sbi);
 	f2fs_put_page(page, 1);

commit 6a8f8ca582a1bafe6b620e000316206c8719f1d0
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed Oct 29 14:37:22 2014 -0700

    f2fs: avoid race condition in handling wait_io
    
    __submit_merged_bio    f2fs_write_end_io        f2fs_write_end_io
                           wait_io = X              wait_io = x
                           complete(X)              complete(X)
                           wait_io = NULL
    wait_for_completion()
    free(X)
                                                     spin_lock(X)
                                                     kernel panic
    
    In order to avoid this, this patch removes the wait_io facility.
    Instead, we can use wait_on_all_pages_writeback(sbi) to wait for end_ios.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index ceee1a69c5aa..8f164432408e 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -61,11 +61,6 @@ static void f2fs_write_end_io(struct bio *bio, int err)
 		dec_page_count(sbi, F2FS_WRITEBACK);
 	}
 
-	if (sbi->wait_io) {
-		complete(sbi->wait_io);
-		sbi->wait_io = NULL;
-	}
-
 	if (!get_pages(sbi, F2FS_WRITEBACK) &&
 			!list_empty(&sbi->cp_wait.task_list))
 		wake_up(&sbi->cp_wait);
@@ -95,34 +90,18 @@ static struct bio *__bio_alloc(struct f2fs_sb_info *sbi, block_t blk_addr,
 static void __submit_merged_bio(struct f2fs_bio_info *io)
 {
 	struct f2fs_io_info *fio = &io->fio;
-	int rw;
 
 	if (!io->bio)
 		return;
 
-	rw = fio->rw;
-
-	if (is_read_io(rw)) {
-		trace_f2fs_submit_read_bio(io->sbi->sb, rw,
-						fio->type, io->bio);
-		submit_bio(rw, io->bio);
-	} else {
-		trace_f2fs_submit_write_bio(io->sbi->sb, rw,
-						fio->type, io->bio);
-		/*
-		 * META_FLUSH is only from the checkpoint procedure, and we
-		 * should wait this metadata bio for FS consistency.
-		 */
-		if (fio->type == META_FLUSH) {
-			DECLARE_COMPLETION_ONSTACK(wait);
-			io->sbi->wait_io = &wait;
-			submit_bio(rw, io->bio);
-			wait_for_completion(&wait);
-		} else {
-			submit_bio(rw, io->bio);
-		}
-	}
+	if (is_read_io(fio->rw))
+		trace_f2fs_submit_read_bio(io->sbi->sb, fio->rw,
+							fio->type, io->bio);
+	else
+		trace_f2fs_submit_write_bio(io->sbi->sb, fio->rw,
+							fio->type, io->bio);
 
+	submit_bio(fio->rw, io->bio);
 	io->bio = NULL;
 }
 

commit b3d208f96d6bb21247108a956dead6a028d5cdb2
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Oct 23 19:48:09 2014 -0700

    f2fs: revisit inline_data to avoid data races and potential bugs
    
    This patch simplifies the inline_data usage with the following rule.
    1. inline_data is set during the file creation.
    2. If new data is requested to be written ranges out of inline_data,
     f2fs converts that inode permanently.
    3. There is no cases which converts non-inline_data inode to inline_data.
    4. The inline_data flag should be changed under inode page lock.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index e3788bd206d8..ceee1a69c5aa 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -737,14 +737,14 @@ int f2fs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,
 static int f2fs_read_data_page(struct file *file, struct page *page)
 {
 	struct inode *inode = page->mapping->host;
-	int ret;
+	int ret = -EAGAIN;
 
 	trace_f2fs_readpage(page, DATA);
 
 	/* If the file has inline data, try to read it directly */
 	if (f2fs_has_inline_data(inode))
 		ret = f2fs_read_inline_data(inode, page);
-	else
+	if (ret == -EAGAIN)
 		ret = mpage_readpage(page, get_data_block);
 
 	return ret;
@@ -856,10 +856,11 @@ static int f2fs_write_data_page(struct page *page,
 	else if (has_not_enough_free_secs(sbi, 0))
 		goto redirty_out;
 
+	err = -EAGAIN;
 	f2fs_lock_op(sbi);
-	if (f2fs_has_inline_data(inode) || f2fs_may_inline(inode))
-		err = f2fs_write_inline_data(inode, page, offset);
-	else
+	if (f2fs_has_inline_data(inode))
+		err = f2fs_write_inline_data(inode, page);
+	if (err == -EAGAIN)
 		err = do_write_data_page(page, &fio);
 	f2fs_unlock_op(sbi);
 done:
@@ -957,24 +958,14 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 
 	f2fs_balance_fs(sbi);
 repeat:
-	err = f2fs_convert_inline_data(inode, pos + len, NULL);
-	if (err)
-		goto fail;
-
 	page = grab_cache_page_write_begin(mapping, index, flags);
 	if (!page) {
 		err = -ENOMEM;
 		goto fail;
 	}
 
-	/* to avoid latency during memory pressure */
-	unlock_page(page);
-
 	*pagep = page;
 
-	if (f2fs_has_inline_data(inode) && (pos + len) <= MAX_INLINE_DATA)
-		goto inline_data;
-
 	f2fs_lock_op(sbi);
 
 	/* check inline_data */
@@ -982,32 +973,42 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 	if (IS_ERR(ipage))
 		goto unlock_fail;
 
+	set_new_dnode(&dn, inode, ipage, ipage, 0);
+
 	if (f2fs_has_inline_data(inode)) {
-		f2fs_put_page(ipage, 1);
-		f2fs_unlock_op(sbi);
-		f2fs_put_page(page, 0);
-		goto repeat;
+		if (pos + len <= MAX_INLINE_DATA) {
+			read_inline_data(page, ipage);
+			set_inode_flag(F2FS_I(inode), FI_DATA_EXIST);
+			sync_inode_page(&dn);
+			goto put_next;
+		} else if (page->index == 0) {
+			err = f2fs_convert_inline_page(&dn, page);
+			if (err)
+				goto unlock_fail;
+		} else {
+			struct page *p = grab_cache_page(inode->i_mapping, 0);
+			if (!p) {
+				err = -ENOMEM;
+				goto unlock_fail;
+			}
+			err = f2fs_convert_inline_page(&dn, p);
+			f2fs_put_page(p, 1);
+			if (err)
+				goto unlock_fail;
+		}
 	}
-
-	set_new_dnode(&dn, inode, ipage, NULL, 0);
 	err = f2fs_reserve_block(&dn, index);
 	if (err)
 		goto unlock_fail;
+put_next:
 	f2fs_put_dnode(&dn);
 	f2fs_unlock_op(sbi);
 
-inline_data:
-	lock_page(page);
-	if (unlikely(page->mapping != mapping)) {
-		f2fs_put_page(page, 1);
-		goto repeat;
-	}
-
-	f2fs_wait_on_page_writeback(page, DATA);
-
 	if ((len == PAGE_CACHE_SIZE) || PageUptodate(page))
 		return 0;
 
+	f2fs_wait_on_page_writeback(page, DATA);
+
 	if ((pos & PAGE_CACHE_MASK) >= i_size_read(inode)) {
 		unsigned start = pos & (PAGE_CACHE_SIZE - 1);
 		unsigned end = start + len;
@@ -1017,13 +1018,7 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 		goto out;
 	}
 
-	if (f2fs_has_inline_data(inode)) {
-		err = f2fs_read_inline_data(inode, page);
-		if (err) {
-			page_cache_release(page);
-			goto fail;
-		}
-	} else if (dn.data_blkaddr == NEW_ADDR) {
+	if (dn.data_blkaddr == NEW_ADDR) {
 		zero_user_segment(page, 0, PAGE_CACHE_SIZE);
 	} else {
 		err = f2fs_submit_page_bio(sbi, page, dn.data_blkaddr,
@@ -1049,7 +1044,7 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 
 unlock_fail:
 	f2fs_unlock_op(sbi);
-	f2fs_put_page(page, 0);
+	f2fs_put_page(page, 1);
 fail:
 	f2fs_write_failed(mapping, pos + len);
 	return err;
@@ -1102,9 +1097,12 @@ static ssize_t f2fs_direct_IO(int rw, struct kiocb *iocb,
 	size_t count = iov_iter_count(iter);
 	int err;
 
-	/* Let buffer I/O handle the inline data case. */
-	if (f2fs_has_inline_data(inode))
-		return 0;
+	/* we don't need to use inline_data strictly */
+	if (f2fs_has_inline_data(inode)) {
+		err = f2fs_convert_inline_inode(inode);
+		if (err)
+			return err;
+	}
 
 	if (check_direct_IO(inode, rw, iter, offset))
 		return 0;
@@ -1170,9 +1168,12 @@ static sector_t f2fs_bmap(struct address_space *mapping, sector_t block)
 {
 	struct inode *inode = mapping->host;
 
-	if (f2fs_has_inline_data(inode))
-		return 0;
-
+	/* we don't need to use inline_data strictly */
+	if (f2fs_has_inline_data(inode)) {
+		int err = f2fs_convert_inline_inode(inode);
+		if (err)
+			return err;
+	}
 	return generic_block_bmap(mapping, block, get_data_block);
 }
 

commit 9234f3190bf8b25b11b105191d408ac50a107948
Author: Jan Kara <jack@suse.cz>
Date:   Wed Oct 22 15:21:47 2014 +0200

    f2fs: fix possible data corruption in f2fs_write_begin()
    
    f2fs_write_begin() doesn't initialize the 'dn' variable if the inode has
    inline data. However it uses its contents to decide whether it should
    just zero out the page or load data to it. Thus if we are unlucky we can
    zero out page contents instead of loading inline data into a page.
    
    CC: stable@vger.kernel.org
    CC: Changman Lee <cm224.lee@samsung.com>
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 973fd7770d56..e3788bd206d8 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1017,21 +1017,19 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 		goto out;
 	}
 
-	if (dn.data_blkaddr == NEW_ADDR) {
+	if (f2fs_has_inline_data(inode)) {
+		err = f2fs_read_inline_data(inode, page);
+		if (err) {
+			page_cache_release(page);
+			goto fail;
+		}
+	} else if (dn.data_blkaddr == NEW_ADDR) {
 		zero_user_segment(page, 0, PAGE_CACHE_SIZE);
 	} else {
-		if (f2fs_has_inline_data(inode)) {
-			err = f2fs_read_inline_data(inode, page);
-			if (err) {
-				page_cache_release(page);
-				goto fail;
-			}
-		} else {
-			err = f2fs_submit_page_bio(sbi, page, dn.data_blkaddr,
-							READ_SYNC);
-			if (err)
-				goto fail;
-		}
+		err = f2fs_submit_page_bio(sbi, page, dn.data_blkaddr,
+					   READ_SYNC);
+		if (err)
+			goto fail;
 
 		lock_page(page);
 		if (unlikely(!PageUptodate(page))) {

commit 9ba69cf9877384baebd16c6fb51ceccd13677b37
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Oct 17 20:33:55 2014 -0700

    f2fs: avoid to allocate when inline_data was written
    
    The sceanrio is like this.
    inline_data   i_size     page                 write_begin/vm_page_mkwrite
      X             30       dirty_page
      X             30                            write to #4096 position
      X             30       get_dnode_of_data    wait for get_dnode_of_data
      O             30       write inline_data
      O             30                            get_dnode_of_data
      O             30                            reserve data block
    ..
    
    In this case, we have #0 = NEW_ADDR and inline_data as well.
    We should not allow this condition for further access.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 5b80adac8985..973fd7770d56 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -257,9 +257,6 @@ int f2fs_reserve_block(struct dnode_of_data *dn, pgoff_t index)
 	bool need_put = dn->inode_page ? false : true;
 	int err;
 
-	/* if inode_page exists, index should be zero */
-	f2fs_bug_on(F2FS_I_SB(dn->inode), !need_put && index);
-
 	err = get_dnode_of_data(dn, index, ALLOC_NODE);
 	if (err)
 		return err;
@@ -951,7 +948,7 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 {
 	struct inode *inode = mapping->host;
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	struct page *page;
+	struct page *page, *ipage;
 	pgoff_t index = ((unsigned long long) pos) >> PAGE_CACHE_SHIFT;
 	struct dnode_of_data dn;
 	int err = 0;
@@ -979,13 +976,26 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 		goto inline_data;
 
 	f2fs_lock_op(sbi);
-	set_new_dnode(&dn, inode, NULL, NULL, 0);
-	err = f2fs_reserve_block(&dn, index);
-	f2fs_unlock_op(sbi);
-	if (err) {
+
+	/* check inline_data */
+	ipage = get_node_page(sbi, inode->i_ino);
+	if (IS_ERR(ipage))
+		goto unlock_fail;
+
+	if (f2fs_has_inline_data(inode)) {
+		f2fs_put_page(ipage, 1);
+		f2fs_unlock_op(sbi);
 		f2fs_put_page(page, 0);
-		goto fail;
+		goto repeat;
 	}
+
+	set_new_dnode(&dn, inode, ipage, NULL, 0);
+	err = f2fs_reserve_block(&dn, index);
+	if (err)
+		goto unlock_fail;
+	f2fs_put_dnode(&dn);
+	f2fs_unlock_op(sbi);
+
 inline_data:
 	lock_page(page);
 	if (unlikely(page->mapping != mapping)) {
@@ -1038,6 +1048,10 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 	SetPageUptodate(page);
 	clear_cold_data(page);
 	return 0;
+
+unlock_fail:
+	f2fs_unlock_op(sbi);
+	f2fs_put_page(page, 0);
 fail:
 	f2fs_write_failed(mapping, pos + len);
 	return err;

commit cbcb2872e37ba0511f21b3ab5d65973b2055440c
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Oct 9 13:39:06 2014 -0700

    f2fs: invalidate inmemory page
    
    If user truncates file's data, we should truncate inmemory pages too.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 84f20e932499..5b80adac8985 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1116,6 +1116,9 @@ static void f2fs_invalidate_data_page(struct page *page, unsigned int offset,
 	if (offset % PAGE_CACHE_SIZE || length != PAGE_CACHE_SIZE)
 		return;
 
+	if (f2fs_is_atomic_file(inode) || f2fs_is_volatile_file(inode))
+		invalidate_inmem_page(inode, page);
+
 	if (PageDirty(page))
 		inode_dec_dirty_pages(inode);
 	ClearPagePrivate(page);

commit 34ba94bac938be14ffe2a639a4688b81a37d0f58
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Oct 9 13:19:53 2014 -0700

    f2fs: do not make dirty any inmemory pages
    
    This patch let inmemory pages be clean all the time.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 8e58c4cc2cb9..84f20e932499 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1052,10 +1052,7 @@ static int f2fs_write_end(struct file *file,
 
 	trace_f2fs_write_end(inode, pos, len, copied);
 
-	if (f2fs_is_atomic_file(inode) || f2fs_is_volatile_file(inode))
-		register_inmem_page(inode, page);
-	else
-		set_page_dirty(page);
+	set_page_dirty(page);
 
 	if (pos + copied > i_size_read(inode)) {
 		i_size_write(inode, pos + copied);
@@ -1138,6 +1135,12 @@ static int f2fs_set_data_page_dirty(struct page *page)
 	trace_f2fs_set_page_dirty(page, DATA);
 
 	SetPageUptodate(page);
+
+	if (f2fs_is_atomic_file(inode) || f2fs_is_volatile_file(inode)) {
+		register_inmem_page(inode, page);
+		return 1;
+	}
+
 	mark_inode_dirty(inode);
 
 	if (!PageDirty(page)) {

commit 02a1335f25a386db9afc68f8315162f862aac93f
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Mon Oct 6 16:11:16 2014 -0700

    f2fs: support volatile operations for transient data
    
    This patch adds support for volatile writes which keep data pages in memory
    until f2fs_evict_inode is called by iput.
    
    For instance, we can use this feature for the sqlite database as follows.
    While supporting atomic writes for main database file, we can keep its journal
    data temporarily in the page cache by the following sequence.
    
    1. open
     -> ioctl(F2FS_IOC_START_VOLATILE_WRITE);
    2. writes
     : keep all the data in the page cache.
    3. flush to the database file with atomic writes
      a. ioctl(F2FS_IOC_START_ATOMIC_WRITE);
      b. writes
      c. ioctl(F2FS_IOC_COMMIT_ATOMIC_WRITE);
    4. close
     -> drop the cached data
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 8bbd60633f37..8e58c4cc2cb9 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1052,7 +1052,7 @@ static int f2fs_write_end(struct file *file,
 
 	trace_f2fs_write_end(inode, pos, len, copied);
 
-	if (f2fs_is_atomic_file(inode))
+	if (f2fs_is_atomic_file(inode) || f2fs_is_volatile_file(inode))
 		register_inmem_page(inode, page);
 	else
 		set_page_dirty(page);

commit 88b88a66797159949cec32eaab12b4968f6fae2d
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Mon Oct 6 17:39:50 2014 -0700

    f2fs: support atomic writes
    
    This patch introduces a very limited functionality for atomic write support.
    In order to support atomic write, this patch adds two ioctls:
     o F2FS_IOC_START_ATOMIC_WRITE
     o F2FS_IOC_COMMIT_ATOMIC_WRITE
    
    The database engine should be aware of the following sequence.
    1. open
     -> ioctl(F2FS_IOC_START_ATOMIC_WRITE);
    2. writes
      : all the written data will be treated as atomic pages.
    3. commit
     -> ioctl(F2FS_IOC_COMMIT_ATOMIC_WRITE);
      : this flushes all the data blocks to the disk, which will be shown all or
      nothing by f2fs recovery procedure.
    4. repeat to #2.
    
    The IO pattens should be:
    
      ,- START_ATOMIC_WRITE                  ,- COMMIT_ATOMIC_WRITE
     CP | D D D D D D | FSYNC | D D D D | FSYNC ...
                          `- COMMIT_ATOMIC_WRITE
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 13ab72084913..8bbd60633f37 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1052,7 +1052,10 @@ static int f2fs_write_end(struct file *file,
 
 	trace_f2fs_write_end(inode, pos, len, copied);
 
-	set_page_dirty(page);
+	if (f2fs_is_atomic_file(inode))
+		register_inmem_page(inode, page);
+	else
+		set_page_dirty(page);
 
 	if (pos + copied > i_size_read(inode)) {
 		i_size_write(inode, pos + copied);

commit 55cf9cb63f0e5439f208d78ed944de9a8df65011
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Mon Sep 15 18:01:10 2014 +0800

    f2fs: support large sector size
    
    Block size in f2fs is 4096 bytes, so theoretically, f2fs can support 4096 bytes
    sector device at maximum. But now f2fs only support 512 bytes size sector, so
    block device such as zRAM which uses page cache as its block storage space will
    not be mounted successfully as mismatch between sector size of zRAM and sector
    size of f2fs supported.
    
    In this patch we support large sector size in f2fs, so block device with sector
    size of 512/1024/2048/4096 bytes can be supported in f2fs.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index aaf22a912044..13ab72084913 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -85,7 +85,7 @@ static struct bio *__bio_alloc(struct f2fs_sb_info *sbi, block_t blk_addr,
 	bio = bio_alloc(GFP_NOIO, npages);
 
 	bio->bi_bdev = sbi->sb->s_bdev;
-	bio->bi_iter.bi_sector = SECTOR_FROM_BLOCK(sbi, blk_addr);
+	bio->bi_iter.bi_sector = SECTOR_FROM_BLOCK(blk_addr);
 	bio->bi_end_io = is_read ? f2fs_read_end_io : f2fs_write_end_io;
 	bio->bi_private = sbi;
 

commit 976e4c50aea111bc7193b48950a3b0c8bc0a25ff
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Mon Sep 15 19:32:16 2014 -0700

    f2fs: update i_size when __allocate_data_block
    
    The f2fs_direct_IO uses __allocate_data_block, but inside the allocation path,
    we should update i_size at the changed time to update its inode page.
    Otherwise, we can get wrong i_size after roll-forward recovery.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 7749f305b102..aaf22a912044 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -572,9 +572,11 @@ struct page *get_new_data_page(struct inode *inode,
 static int __allocate_data_block(struct dnode_of_data *dn)
 {
 	struct f2fs_sb_info *sbi = F2FS_I_SB(dn->inode);
+	struct f2fs_inode_info *fi = F2FS_I(dn->inode);
 	struct f2fs_summary sum;
 	block_t new_blkaddr;
 	struct node_info ni;
+	pgoff_t fofs;
 	int type;
 
 	if (unlikely(is_inode_flag_set(F2FS_I(dn->inode), FI_NO_ALLOC)))
@@ -597,6 +599,12 @@ static int __allocate_data_block(struct dnode_of_data *dn)
 	update_extent_cache(new_blkaddr, dn);
 	clear_inode_flag(F2FS_I(dn->inode), FI_NO_EXTENT);
 
+	/* update i_size */
+	fofs = start_bidx_of_node(ofs_of_node(dn->node_page), fi) +
+							dn->ofs_in_node;
+	if (i_size_read(dn->inode) < ((fofs + 1) << PAGE_CACHE_SHIFT))
+		i_size_write(dn->inode, ((fofs + 1) << PAGE_CACHE_SHIFT));
+
 	dn->data_blkaddr = new_blkaddr;
 	return 0;
 }

commit 90a893c749f4582f21e97639f4e85e7f2362c2f0
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Mon Sep 22 16:21:07 2014 -0700

    f2fs: use MAX_BIO_BLOCKS(sbi)
    
    This patch cleans up a simple macro.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index fdc3dbe677a1..7749f305b102 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -193,7 +193,7 @@ void f2fs_submit_page_mbio(struct f2fs_sb_info *sbi, struct page *page,
 		__submit_merged_bio(io);
 alloc_new:
 	if (io->bio == NULL) {
-		int bio_blocks = MAX_BIO_BLOCKS(max_hw_blocks(sbi));
+		int bio_blocks = MAX_BIO_BLOCKS(sbi);
 
 		io->bio = __bio_alloc(sbi, blk_addr, bio_blocks, is_read);
 		io->fio = *fio;

commit 88bd02c9472a166b706284a34a84f1243322d782
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Mon Sep 15 14:50:48 2014 -0700

    f2fs: fix conditions to remain recovery information in f2fs_sync_file
    
    This patch revisited whole the recovery information during the f2fs_sync_file.
    
    In this patch, there are three information to make a decision.
    
    a) IS_CHECKPOINTED,     /* is it checkpointed before? */
    b) HAS_FSYNCED_INODE,   /* is the inode fsynced before? */
    c) HAS_LAST_FSYNC,      /* has the latest node fsync mark? */
    
    And, the scenarios for our rule are based on:
    
    [Term] F: fsync_mark, D: dentry_mark
    
    1. inode(x) | CP | inode(x) | dnode(F)
    2. inode(x) | CP | inode(F) | dnode(F)
    3. inode(x) | CP | dnode(F) | inode(x) | inode(F)
    4. inode(x) | CP | dnode(F) | inode(F)
    5. CP | inode(x) | dnode(F) | inode(DF)
    6. CP | inode(DF) | dnode(F)
    7. CP | dnode(F) | inode(DF)
    8. CP | dnode(F) | inode(x) | inode(DF)
    
    For example, #3, the three conditions should be changed as follows.
    
       inode(x) | CP | dnode(F) | inode(x) | inode(F)
    a)    x       o      o          o          o
    b)    x       x      x          x          o
    c)    x       o      o          x          o
    
    If f2fs_sync_file stops   ------^,
     it should write inode(F)    --------------^
    
    So, the need_inode_block_update should return true, since
     c) get_nat_flag(e, HAS_LAST_FSYNC), is false.
    
    For example, #8,
          CP | alloc | dnode(F) | inode(x) | inode(DF)
    a)    o      x        x          x          x
    b)    x               x          x          o
    c)    o               o          x          o
    
    If f2fs_sync_file stops   -------^,
     it should write inode(DF)    --------------^
    
    Note that, the roll-forward policy should follow this rule, which means,
    if there are any missing blocks, we doesn't need to recover that inode.
    
    Signed-off-by: Huang Ying <ying.huang@intel.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 0e376585e29f..fdc3dbe677a1 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1089,9 +1089,6 @@ static ssize_t f2fs_direct_IO(int rw, struct kiocb *iocb,
 	if (check_direct_IO(inode, rw, iter, offset))
 		return 0;
 
-	/* clear fsync mark to recover these blocks */
-	fsync_mark_clear(F2FS_I_SB(inode), inode->i_ino);
-
 	trace_f2fs_direct_IO_enter(inode, offset, count, rw);
 
 	err = blockdev_direct_IO(rw, iocb, inode, iter, offset, get_data_block);

commit a7ffdbe22cecaed59b5d76a5f003d68907d64240
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Sep 12 15:53:45 2014 -0700

    f2fs: expand counting dirty pages in the inode page cache
    
    Previously f2fs only counts dirty dentry pages, but there is no reason not to
    expand the scope.
    
    This patch changes the names on the management of dirty pages and to count
    dirty pages in each inode info as well.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 64d855085edf..0e376585e29f 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -843,7 +843,7 @@ static int f2fs_write_data_page(struct page *page,
 	if (unlikely(f2fs_cp_error(sbi))) {
 		SetPageError(page);
 		unlock_page(page);
-		return 0;
+		goto out;
 	}
 
 	if (!wbc->for_reclaim)
@@ -863,7 +863,7 @@ static int f2fs_write_data_page(struct page *page,
 
 	clear_cold_data(page);
 out:
-	inode_dec_dirty_dents(inode);
+	inode_dec_dirty_pages(inode);
 	unlock_page(page);
 	if (need_balance_fs)
 		f2fs_balance_fs(sbi);
@@ -901,7 +901,7 @@ static int f2fs_write_data_pages(struct address_space *mapping,
 		return 0;
 
 	if (S_ISDIR(inode->i_mode) && wbc->sync_mode == WB_SYNC_NONE &&
-			get_dirty_dents(inode) < nr_pages_to_skip(sbi, DATA) &&
+			get_dirty_pages(inode) < nr_pages_to_skip(sbi, DATA) &&
 			available_free_memory(sbi, DIRTY_DENTS))
 		goto skip_write;
 
@@ -923,7 +923,7 @@ static int f2fs_write_data_pages(struct address_space *mapping,
 	return ret;
 
 skip_write:
-	wbc->pages_skipped += get_dirty_dents(inode);
+	wbc->pages_skipped += get_dirty_pages(inode);
 	return 0;
 }
 
@@ -1107,8 +1107,12 @@ static void f2fs_invalidate_data_page(struct page *page, unsigned int offset,
 				      unsigned int length)
 {
 	struct inode *inode = page->mapping->host;
+
+	if (offset % PAGE_CACHE_SIZE || length != PAGE_CACHE_SIZE)
+		return;
+
 	if (PageDirty(page))
-		inode_dec_dirty_dents(inode);
+		inode_dec_dirty_pages(inode);
 	ClearPagePrivate(page);
 }
 
@@ -1130,7 +1134,7 @@ static int f2fs_set_data_page_dirty(struct page *page)
 
 	if (!PageDirty(page)) {
 		__set_page_dirty_nobuffers(page);
-		set_dirty_dir_page(inode, page);
+		update_dirty_page(inode, page);
 		return 1;
 	}
 	return 0;

commit 9850cf4a8908886370b1f15aacf83d291f098c72
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Tue Sep 2 15:52:58 2014 -0700

    f2fs: need fsck.f2fs when f2fs_bug_on is triggered
    
    If any f2fs_bug_on is triggered, fsck.f2fs is needed.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 8eb6fcaa0a6b..64d855085edf 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -258,7 +258,7 @@ int f2fs_reserve_block(struct dnode_of_data *dn, pgoff_t index)
 	int err;
 
 	/* if inode_page exists, index should be zero */
-	f2fs_bug_on(!need_put && index);
+	f2fs_bug_on(F2FS_I_SB(dn->inode), !need_put && index);
 
 	err = get_dnode_of_data(dn, index, ALLOC_NODE);
 	if (err)
@@ -321,7 +321,7 @@ void update_extent_cache(block_t blk_addr, struct dnode_of_data *dn)
 	block_t start_blkaddr, end_blkaddr;
 	int need_update = true;
 
-	f2fs_bug_on(blk_addr == NEW_ADDR);
+	f2fs_bug_on(F2FS_I_SB(dn->inode), blk_addr == NEW_ADDR);
 	fofs = start_bidx_of_node(ofs_of_node(dn->node_page), fi) +
 							dn->ofs_in_node;
 

commit 4081363fbe84a7ebac6d3339dd2775df45d856d0
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Tue Sep 2 15:31:18 2014 -0700

    f2fs: introduce F2FS_I_SB, F2FS_M_SB, and F2FS_P_SB
    
    This patch adds three inline functions to clean up dirty casting codes.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 76de83e25a89..8eb6fcaa0a6b 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -236,7 +236,7 @@ static void __set_data_blkaddr(struct dnode_of_data *dn, block_t new_addr)
 
 int reserve_new_block(struct dnode_of_data *dn)
 {
-	struct f2fs_sb_info *sbi = F2FS_SB(dn->inode->i_sb);
+	struct f2fs_sb_info *sbi = F2FS_I_SB(dn->inode);
 
 	if (unlikely(is_inode_flag_set(F2FS_I(dn->inode), FI_NO_ALLOC)))
 		return -EPERM;
@@ -396,7 +396,6 @@ void update_extent_cache(block_t blk_addr, struct dnode_of_data *dn)
 
 struct page *find_data_page(struct inode *inode, pgoff_t index, bool sync)
 {
-	struct f2fs_sb_info *sbi = F2FS_SB(inode->i_sb);
 	struct address_space *mapping = inode->i_mapping;
 	struct dnode_of_data dn;
 	struct page *page;
@@ -429,7 +428,7 @@ struct page *find_data_page(struct inode *inode, pgoff_t index, bool sync)
 		return page;
 	}
 
-	err = f2fs_submit_page_bio(sbi, page, dn.data_blkaddr,
+	err = f2fs_submit_page_bio(F2FS_I_SB(inode), page, dn.data_blkaddr,
 					sync ? READ_SYNC : READA);
 	if (err)
 		return ERR_PTR(err);
@@ -451,7 +450,6 @@ struct page *find_data_page(struct inode *inode, pgoff_t index, bool sync)
  */
 struct page *get_lock_data_page(struct inode *inode, pgoff_t index)
 {
-	struct f2fs_sb_info *sbi = F2FS_SB(inode->i_sb);
 	struct address_space *mapping = inode->i_mapping;
 	struct dnode_of_data dn;
 	struct page *page;
@@ -490,7 +488,8 @@ struct page *get_lock_data_page(struct inode *inode, pgoff_t index)
 		return page;
 	}
 
-	err = f2fs_submit_page_bio(sbi, page, dn.data_blkaddr, READ_SYNC);
+	err = f2fs_submit_page_bio(F2FS_I_SB(inode), page,
+					dn.data_blkaddr, READ_SYNC);
 	if (err)
 		return ERR_PTR(err);
 
@@ -517,7 +516,6 @@ struct page *get_lock_data_page(struct inode *inode, pgoff_t index)
 struct page *get_new_data_page(struct inode *inode,
 		struct page *ipage, pgoff_t index, bool new_i_size)
 {
-	struct f2fs_sb_info *sbi = F2FS_SB(inode->i_sb);
 	struct address_space *mapping = inode->i_mapping;
 	struct page *page;
 	struct dnode_of_data dn;
@@ -541,8 +539,8 @@ struct page *get_new_data_page(struct inode *inode,
 		zero_user_segment(page, 0, PAGE_CACHE_SIZE);
 		SetPageUptodate(page);
 	} else {
-		err = f2fs_submit_page_bio(sbi, page, dn.data_blkaddr,
-								READ_SYNC);
+		err = f2fs_submit_page_bio(F2FS_I_SB(inode), page,
+						dn.data_blkaddr, READ_SYNC);
 		if (err)
 			goto put_err;
 
@@ -573,7 +571,7 @@ struct page *get_new_data_page(struct inode *inode,
 
 static int __allocate_data_block(struct dnode_of_data *dn)
 {
-	struct f2fs_sb_info *sbi = F2FS_SB(dn->inode->i_sb);
+	struct f2fs_sb_info *sbi = F2FS_I_SB(dn->inode);
 	struct f2fs_summary sum;
 	block_t new_blkaddr;
 	struct node_info ni;
@@ -614,7 +612,6 @@ static int __allocate_data_block(struct dnode_of_data *dn)
 static int __get_data_block(struct inode *inode, sector_t iblock,
 			struct buffer_head *bh_result, int create, bool fiemap)
 {
-	struct f2fs_sb_info *sbi = F2FS_SB(inode->i_sb);
 	unsigned int blkbits = inode->i_sb->s_blocksize_bits;
 	unsigned maxblocks = bh_result->b_size >> blkbits;
 	struct dnode_of_data dn;
@@ -630,8 +627,8 @@ static int __get_data_block(struct inode *inode, sector_t iblock,
 		goto out;
 
 	if (create) {
-		f2fs_balance_fs(sbi);
-		f2fs_lock_op(sbi);
+		f2fs_balance_fs(F2FS_I_SB(inode));
+		f2fs_lock_op(F2FS_I_SB(inode));
 	}
 
 	/* When reading holes, we need its node page */
@@ -707,7 +704,7 @@ static int __get_data_block(struct inode *inode, sector_t iblock,
 	f2fs_put_dnode(&dn);
 unlock_out:
 	if (create)
-		f2fs_unlock_op(sbi);
+		f2fs_unlock_op(F2FS_I_SB(inode));
 out:
 	trace_f2fs_get_data_block(inode, iblock, bh_result, err);
 	return err;
@@ -804,7 +801,7 @@ static int f2fs_write_data_page(struct page *page,
 					struct writeback_control *wbc)
 {
 	struct inode *inode = page->mapping->host;
-	struct f2fs_sb_info *sbi = F2FS_SB(inode->i_sb);
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
 	loff_t i_size = i_size_read(inode);
 	const pgoff_t end_index = ((unsigned long long) i_size)
 							>> PAGE_CACHE_SHIFT;
@@ -892,7 +889,7 @@ static int f2fs_write_data_pages(struct address_space *mapping,
 			    struct writeback_control *wbc)
 {
 	struct inode *inode = mapping->host;
-	struct f2fs_sb_info *sbi = F2FS_SB(inode->i_sb);
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
 	bool locked = false;
 	int ret;
 	long diff;
@@ -945,7 +942,7 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 		struct page **pagep, void **fsdata)
 {
 	struct inode *inode = mapping->host;
-	struct f2fs_sb_info *sbi = F2FS_SB(inode->i_sb);
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
 	struct page *page;
 	pgoff_t index = ((unsigned long long) pos) >> PAGE_CACHE_SHIFT;
 	struct dnode_of_data dn;
@@ -1093,7 +1090,7 @@ static ssize_t f2fs_direct_IO(int rw, struct kiocb *iocb,
 		return 0;
 
 	/* clear fsync mark to recover these blocks */
-	fsync_mark_clear(F2FS_SB(inode->i_sb), inode->i_ino);
+	fsync_mark_clear(F2FS_I_SB(inode), inode->i_ino);
 
 	trace_f2fs_direct_IO_enter(inode, offset, count, rw);
 

commit 764aa3e978020121cbb86111b5d8f42830015a06
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Aug 14 16:32:54 2014 -0700

    f2fs: avoid double lock in truncate_blocks
    
    The init_inode_metadata calls truncate_blocks when error is occurred.
    The callers holds f2fs_lock_op, so we should not call it again in
    truncate_blocks.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 6ba52a393063..76de83e25a89 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -936,7 +936,7 @@ static void f2fs_write_failed(struct address_space *mapping, loff_t to)
 
 	if (to > inode->i_size) {
 		truncate_pagecache(inode, inode->i_size);
-		truncate_blocks(inode, inode->i_size);
+		truncate_blocks(inode, inode->i_size, true);
 	}
 }
 

commit cf779cab14d50a84b61399f758da269654b863db
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Mon Aug 11 18:37:46 2014 -0700

    f2fs: handle EIO not to break fs consistency
    
    There are two rules when EIO is occurred.
    1. don't write any checkpoint data to preserve the previous checkpoint
    2. don't lose the cached dentry/node/meta pages
    
    So, at first, this patch adds set_page_dirty in f2fs_write_end_io's failure.
    Then, writing checkpoint/dentry/node blocks is not allowed.
    
    Note that, for the data pages, we can't just throw away by redirtying them.
    Otherwise, kworker can fall into infinite loop to flush them.
    (Ref. xfstests/019)
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index ac3ccc25386b..6ba52a393063 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -53,7 +53,7 @@ static void f2fs_write_end_io(struct bio *bio, int err)
 		struct page *page = bvec->bv_page;
 
 		if (unlikely(err)) {
-			SetPageError(page);
+			set_page_dirty(page);
 			set_bit(AS_EIO, &page->mapping->flags);
 			f2fs_stop_checkpoint(sbi);
 		}
@@ -836,10 +836,19 @@ static int f2fs_write_data_page(struct page *page,
 
 	/* Dentry blocks are controlled by checkpoint */
 	if (S_ISDIR(inode->i_mode)) {
+		if (unlikely(f2fs_cp_error(sbi)))
+			goto redirty_out;
 		err = do_write_data_page(page, &fio);
 		goto done;
 	}
 
+	/* we should bypass data pages to proceed the kworkder jobs */
+	if (unlikely(f2fs_cp_error(sbi))) {
+		SetPageError(page);
+		unlock_page(page);
+		return 0;
+	}
+
 	if (!wbc->for_reclaim)
 		need_balance_fs = true;
 	else if (has_not_enough_free_secs(sbi, 0))

commit b067ba1f1b3fa7ec798d35e12aed6cdba9cea905
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Aug 7 16:32:25 2014 -0700

    f2fs: should convert inline_data during the mkwrite
    
    If mkwrite is called to an inode having inline_data, it can overwrite the data
    index space as NEW_ADDR. (e.g., the first 4 bytes are coincidently zero)
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 7aef28d1fffa..ac3ccc25386b 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -946,7 +946,7 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 
 	f2fs_balance_fs(sbi);
 repeat:
-	err = f2fs_convert_inline_data(inode, pos + len);
+	err = f2fs_convert_inline_data(inode, pos + len, NULL);
 	if (err)
 		goto fail;
 

commit e1c42045203071c4634b89e696037357810d3083
Author: arter97 <qkrwngud825@gmail.com>
Date:   Wed Aug 6 23:22:50 2014 +0900

    f2fs: fix typo
    
    Fix typo and some grammatical errors.
    
    The words "filesystem" and "readahead" are being used without the space treewide.
    
    Signed-off-by: Park Ju Hyung <qkrwngud825@gmail.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 03313099c51c..7aef28d1fffa 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -691,7 +691,7 @@ static int __get_data_block(struct inode *inode, sector_t iblock,
 			allocated = true;
 			blkaddr = dn.data_blkaddr;
 		}
-		/* Give more consecutive addresses for the read ahead */
+		/* Give more consecutive addresses for the readahead */
 		if (blkaddr == (bh_result->b_blocknr + ofs)) {
 			ofs++;
 			dn.ofs_in_node++;
@@ -739,7 +739,7 @@ static int f2fs_read_data_page(struct file *file, struct page *page)
 
 	trace_f2fs_readpage(page, DATA);
 
-	/* If the file has inline data, try to read it directlly */
+	/* If the file has inline data, try to read it directly */
 	if (f2fs_has_inline_data(inode))
 		ret = f2fs_read_inline_data(inode, page);
 	else

commit 70407fad85f2ec87a0cf56057c3267cd3aa22768
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Thu Jul 31 21:11:22 2014 +0800

    f2fs: add tracepoint for f2fs_direct_IO
    
    This patch adds a tracepoint for f2fs_direct_IO.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index ec3c8860539e..03313099c51c 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1086,9 +1086,14 @@ static ssize_t f2fs_direct_IO(int rw, struct kiocb *iocb,
 	/* clear fsync mark to recover these blocks */
 	fsync_mark_clear(F2FS_SB(inode->i_sb), inode->i_ino);
 
+	trace_f2fs_direct_IO_enter(inode, offset, count, rw);
+
 	err = blockdev_direct_IO(rw, iocb, inode, iter, offset, get_data_block);
 	if (err < 0 && (rw & WRITE))
 		f2fs_write_failed(mapping, offset + count);
+
+	trace_f2fs_direct_IO_exit(inode, offset, count, rw, err);
+
 	return err;
 }
 

commit fff04f90c1b9f91b9c513a89702a4b9ffe5dc1c5
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Jul 25 07:40:59 2014 -0700

    f2fs: add info of appended or updated data writes
    
    This patch introduces a inode number list in which represents inodes having
    appended data writes or updated data writes after last checkpoint.
    This will be used at fsync to determine whether the recovery information
    should be written or not.
    
    Reviewed-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 482313dd9e24..ec3c8860539e 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -789,9 +789,11 @@ int do_write_data_page(struct page *page, struct f2fs_io_info *fio)
 			!is_cold_data(page) &&
 			need_inplace_update(inode))) {
 		rewrite_data_page(page, old_blkaddr, fio);
+		set_inode_flag(F2FS_I(inode), FI_UPDATE_WRITE);
 	} else {
 		write_data_page(page, &dn, &new_blkaddr, fio);
 		update_extent_cache(new_blkaddr, &dn);
+		set_inode_flag(F2FS_I(inode), FI_APPEND_WRITE);
 	}
 out_writepage:
 	f2fs_put_dnode(&dn);

commit 0f7b2abd188089a44f60e2bf8521d1363ada9e12
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed Jul 23 09:57:31 2014 -0700

    f2fs: add nobarrier mount option
    
    This patch adds a mount option, nobarrier, in f2fs.
    The assumption in here is that file system keeps the IO ordering, but
    doesn't care about cache flushes inside the storages.
    
    Reviewed-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index c77c66723d70..482313dd9e24 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -139,7 +139,10 @@ void f2fs_submit_merged_bio(struct f2fs_sb_info *sbi,
 	/* change META to META_FLUSH in the checkpoint procedure */
 	if (type >= META_FLUSH) {
 		io->fio.type = META_FLUSH;
-		io->fio.rw = WRITE_FLUSH_FUA | REQ_META | REQ_PRIO;
+		if (test_opt(sbi, NOBARRIER))
+			io->fio.rw = WRITE_FLUSH | REQ_META | REQ_PRIO;
+		else
+			io->fio.rw = WRITE_FLUSH_FUA | REQ_META | REQ_PRIO;
 	}
 	__submit_merged_bio(io);
 	up_write(&io->io_rwsem);

commit 79e35dc3c23dd2ac9f8681361026c82b71a0b006
Author: Huang Ying <ying.huang@intel.com>
Date:   Sat Jul 12 20:10:00 2014 +0800

    f2fs: add f2fs_balance_fs for direct IO
    
    Otherwise, if a large amount of direct IO writes were done, the
    segment allocation may be failed because no enough segments are gced.
    
    Changes:
    
    v2: add f2fs_balance_fs into __get_data_block instead of f2fs_direct_IO.
    
    Signed-off-by: Huang, Ying <ying.huang@intel.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 05154d6de49a..c77c66723d70 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -626,8 +626,10 @@ static int __get_data_block(struct inode *inode, sector_t iblock,
 	if (check_extent_cache(inode, pgofs, bh_result))
 		goto out;
 
-	if (create)
+	if (create) {
+		f2fs_balance_fs(sbi);
 		f2fs_lock_op(sbi);
+	}
 
 	/* When reading holes, we need its node page */
 	set_new_dnode(&dn, inode, NULL, NULL, 0);

commit 3aab8f828ef358ae92545294a14cd52d510cc585
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Wed Jul 2 13:25:04 2014 +0800

    f2fs: introduce f2fs_write_failed to handle error case when write
    
    When we fail in ->write_begin()/->direct_IO(), our allocated node block in disk
    and page cache are still kept, despite these may not be used again.
    
    This patch introduce f2fs_write_failed() to handle the error case of these two
    interfaces, it will truncate page cache and blocks of this file according to
    i_size.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 2eb2764173ca..05154d6de49a 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -914,6 +914,16 @@ static int f2fs_write_data_pages(struct address_space *mapping,
 	return 0;
 }
 
+static void f2fs_write_failed(struct address_space *mapping, loff_t to)
+{
+	struct inode *inode = mapping->host;
+
+	if (to > inode->i_size) {
+		truncate_pagecache(inode, inode->i_size);
+		truncate_blocks(inode, inode->i_size);
+	}
+}
+
 static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 		loff_t pos, unsigned len, unsigned flags,
 		struct page **pagep, void **fsdata)
@@ -931,11 +941,13 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 repeat:
 	err = f2fs_convert_inline_data(inode, pos + len);
 	if (err)
-		return err;
+		goto fail;
 
 	page = grab_cache_page_write_begin(mapping, index, flags);
-	if (!page)
-		return -ENOMEM;
+	if (!page) {
+		err = -ENOMEM;
+		goto fail;
+	}
 
 	/* to avoid latency during memory pressure */
 	unlock_page(page);
@@ -949,10 +961,9 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 	set_new_dnode(&dn, inode, NULL, NULL, 0);
 	err = f2fs_reserve_block(&dn, index);
 	f2fs_unlock_op(sbi);
-
 	if (err) {
 		f2fs_put_page(page, 0);
-		return err;
+		goto fail;
 	}
 inline_data:
 	lock_page(page);
@@ -982,19 +993,20 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 			err = f2fs_read_inline_data(inode, page);
 			if (err) {
 				page_cache_release(page);
-				return err;
+				goto fail;
 			}
 		} else {
 			err = f2fs_submit_page_bio(sbi, page, dn.data_blkaddr,
 							READ_SYNC);
 			if (err)
-				return err;
+				goto fail;
 		}
 
 		lock_page(page);
 		if (unlikely(!PageUptodate(page))) {
 			f2fs_put_page(page, 1);
-			return -EIO;
+			err = -EIO;
+			goto fail;
 		}
 		if (unlikely(page->mapping != mapping)) {
 			f2fs_put_page(page, 1);
@@ -1005,6 +1017,9 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 	SetPageUptodate(page);
 	clear_cold_data(page);
 	return 0;
+fail:
+	f2fs_write_failed(mapping, pos + len);
+	return err;
 }
 
 static int f2fs_write_end(struct file *file,
@@ -1049,7 +1064,10 @@ static ssize_t f2fs_direct_IO(int rw, struct kiocb *iocb,
 		struct iov_iter *iter, loff_t offset)
 {
 	struct file *file = iocb->ki_filp;
-	struct inode *inode = file->f_mapping->host;
+	struct address_space *mapping = file->f_mapping;
+	struct inode *inode = mapping->host;
+	size_t count = iov_iter_count(iter);
+	int err;
 
 	/* Let buffer I/O handle the inline data case. */
 	if (f2fs_has_inline_data(inode))
@@ -1061,8 +1079,10 @@ static ssize_t f2fs_direct_IO(int rw, struct kiocb *iocb,
 	/* clear fsync mark to recover these blocks */
 	fsync_mark_clear(F2FS_SB(inode->i_sb), inode->i_ino);
 
-	return blockdev_direct_IO(rw, iocb, inode, iter, offset,
-				  get_data_block);
+	err = blockdev_direct_IO(rw, iocb, inode, iter, offset, get_data_block);
+	if (err < 0 && (rw & WRITE))
+		f2fs_write_failed(mapping, offset + count);
+	return err;
 }
 
 static void f2fs_invalidate_data_page(struct page *page, unsigned int offset,

commit 5576cd6ca555bd79e76a2d798aec44b28851d369
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Thu Jun 12 13:25:01 2014 +0800

    f2fs: avoid unneeded SetPageUptodate in f2fs_write_end
    
    We have already set page update in ->write_begin, so we should remove redundant
    SetPageUptodate in ->write_end.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index f8cf619edb5f..2eb2764173ca 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1016,7 +1016,6 @@ static int f2fs_write_end(struct file *file,
 
 	trace_f2fs_write_end(inode, pos, len, copied);
 
-	SetPageUptodate(page);
 	set_page_dirty(page);
 
 	if (pos + copied > i_size_read(inode)) {

commit ccfb30001f37ace4690a74c27b4812cf054e123a
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Jun 13 13:02:11 2014 +0900

    f2fs: fix to report newly allocate region as extent
    
    Previous get_block in f2fs didn't report the newly allocated region which has
    NEW_ADDR.
    For reader, it should not report, but fiemap needs this.
    So, this patch introduces two get_block sharing core function.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 0924521306b4..f8cf619edb5f 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -608,8 +608,8 @@ static int __allocate_data_block(struct dnode_of_data *dn)
  *     b. do not use extent cache for better performance
  *     c. give the block addresses to blockdev
  */
-static int get_data_block(struct inode *inode, sector_t iblock,
-			struct buffer_head *bh_result, int create)
+static int __get_data_block(struct inode *inode, sector_t iblock,
+			struct buffer_head *bh_result, int create, bool fiemap)
 {
 	struct f2fs_sb_info *sbi = F2FS_SB(inode->i_sb);
 	unsigned int blkbits = inode->i_sb->s_blocksize_bits;
@@ -637,7 +637,7 @@ static int get_data_block(struct inode *inode, sector_t iblock,
 			err = 0;
 		goto unlock_out;
 	}
-	if (dn.data_blkaddr == NEW_ADDR)
+	if (dn.data_blkaddr == NEW_ADDR && !fiemap)
 		goto put_out;
 
 	if (dn.data_blkaddr != NULL_ADDR) {
@@ -671,7 +671,7 @@ static int get_data_block(struct inode *inode, sector_t iblock,
 				err = 0;
 			goto unlock_out;
 		}
-		if (dn.data_blkaddr == NEW_ADDR)
+		if (dn.data_blkaddr == NEW_ADDR && !fiemap)
 			goto put_out;
 
 		end_offset = ADDRS_PER_PAGE(dn.node_page, F2FS_I(inode));
@@ -708,10 +708,23 @@ static int get_data_block(struct inode *inode, sector_t iblock,
 	return err;
 }
 
+static int get_data_block(struct inode *inode, sector_t iblock,
+			struct buffer_head *bh_result, int create)
+{
+	return __get_data_block(inode, iblock, bh_result, create, false);
+}
+
+static int get_data_block_fiemap(struct inode *inode, sector_t iblock,
+			struct buffer_head *bh_result, int create)
+{
+	return __get_data_block(inode, iblock, bh_result, create, true);
+}
+
 int f2fs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,
 		u64 start, u64 len)
 {
-	return generic_block_fiemap(inode, fieinfo, start, len, get_data_block);
+	return generic_block_fiemap(inode, fieinfo,
+				start, len, get_data_block_fiemap);
 }
 
 static int f2fs_read_data_page(struct file *file, struct page *page)

commit 16b9057804c02e2d351e9c8f606e909b43cbd9e7
Merge: 5c02c392cd23 c2338f2dc7c1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jun 12 10:30:18 2014 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs
    
    Pull vfs updates from Al Viro:
     "This the bunch that sat in -next + lock_parent() fix.  This is the
      minimal set; there's more pending stuff.
    
      In particular, I really hope to get acct.c fixes merged this cycle -
      we need that to deal sanely with delayed-mntput stuff.  In the next
      pile, hopefully - that series is fairly short and localized
      (kernel/acct.c, fs/super.c and fs/namespace.c).  In this pile: more
      iov_iter work.  Most of prereqs for ->splice_write with sane locking
      order are there and Kent's dio rewrite would also fit nicely on top of
      this pile"
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs: (70 commits)
      lock_parent: don't step on stale ->d_parent of all-but-freed one
      kill generic_file_splice_write()
      ceph: switch to iter_file_splice_write()
      shmem: switch to iter_file_splice_write()
      nfs: switch to iter_splice_write_file()
      fs/splice.c: remove unneeded exports
      ocfs2: switch to iter_file_splice_write()
      ->splice_write() via ->write_iter()
      bio_vec-backed iov_iter
      optimize copy_page_{to,from}_iter()
      bury generic_file_aio_{read,write}
      lustre: get rid of messing with iovecs
      ceph: switch to ->write_iter()
      ceph_sync_direct_write: stop poking into iov_iter guts
      ceph_sync_read: stop poking into iov_iter guts
      new helper: copy_page_from_iter()
      fuse: switch to ->write_iter()
      btrfs: switch to ->write_iter()
      ocfs2: switch to ->write_iter()
      xfs: switch to ->write_iter()
      ...

commit 9ab701349247368f9d57a993b95a5bb05bb37e10
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Sun Jun 8 04:30:14 2014 +0900

    f2fs: support f2fs_fiemap
    
    This patch links f2fs_fiemap with generic function with get_block.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 39fe7d70791a..c1fb6dd10911 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -708,6 +708,12 @@ static int get_data_block(struct inode *inode, sector_t iblock,
 	return err;
 }
 
+int f2fs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,
+		u64 start, u64 len)
+{
+	return generic_block_fiemap(inode, fieinfo, start, len, get_data_block);
+}
+
 static int f2fs_read_data_page(struct file *file, struct page *page)
 {
 	struct inode *inode = page->mapping->host;

commit b6fe5873cb422417ae3fc914954bc5a10fd4e003
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed Jun 4 00:39:42 2014 +0900

    f2fs: fix to recover data written by dio
    
    If data are overwritten through dio, previous f2fs doesn't remain the fsync mark
    due to no additional node writes.
    
    Note that this patch should resolve the xfstests:311.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 8c250a5d6f26..39fe7d70791a 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1041,6 +1041,9 @@ static ssize_t f2fs_direct_IO(int rw, struct kiocb *iocb,
 	if (check_direct_IO(inode, rw, iov, offset, nr_segs))
 		return 0;
 
+	/* clear fsync mark to recover these blocks */
+	fsync_mark_clear(F2FS_SB(inode->i_sb), inode->i_ino);
+
 	return blockdev_direct_IO(rw, iocb, inode, iov, offset, nr_segs,
 							get_data_block);
 }

commit c20e89cde669799eff62bf8c00ca9a4819c4e11f
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Tue May 6 16:53:08 2014 +0800

    f2fs: add a tracepoint for f2fs_read_data_page
    
    This patch adds a tracepoint for f2fs_read_data_page to trace when page is
    readed by user.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 21bfafaafe83..8c250a5d6f26 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -713,6 +713,8 @@ static int f2fs_read_data_page(struct file *file, struct page *page)
 	struct inode *inode = page->mapping->host;
 	int ret;
 
+	trace_f2fs_readpage(page, DATA);
+
 	/* If the file has inline data, try to read it directlly */
 	if (f2fs_has_inline_data(inode))
 		ret = f2fs_read_inline_data(inode, page);

commit e57484343898094bb8f72a2aa1a50929d27aa027
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Tue May 6 16:51:24 2014 +0800

    f2fs: add a tracepoint for f2fs_write_{meta,node,data}_pages
    
    This patch adds a tracepoint for f2fs_write_{meta,node,data}_pages to trace when
    pages are fsyncing/flushing.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index b997d552880e..21bfafaafe83 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -860,6 +860,8 @@ static int f2fs_write_data_pages(struct address_space *mapping,
 	int ret;
 	long diff;
 
+	trace_f2fs_writepages(mapping->host, wbc, DATA);
+
 	/* deal with chardevs and other special file */
 	if (!mapping->a_ops->writepage)
 		return 0;

commit ecda0de3430455378f1c02523bf3ad71d91d613a
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Tue May 6 16:48:26 2014 +0800

    f2fs: add a tracepoint for f2fs_write_{meta,node,data}_page
    
    This patch adds a tracepoint for f2fs_write_{meta,node,data}_page to trace when
    page is writting out.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index ea58fb698ac6..b997d552880e 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -788,6 +788,8 @@ static int f2fs_write_data_page(struct page *page,
 		.rw = (wbc->sync_mode == WB_SYNC_ALL) ? WRITE_SYNC : WRITE,
 	};
 
+	trace_f2fs_writepage(page, DATA);
+
 	if (page->index < end_index)
 		goto write;
 

commit dfb2bf38bf89e15a65f9996566b2945921388a2f
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Tue May 6 16:47:23 2014 +0800

    f2fs: add a tracepoint for f2fs_write_end
    
    This patch adds a tracepoint for f2fs_write_end to trace write op of user.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index cde0d69969b9..ea58fb698ac6 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -989,6 +989,8 @@ static int f2fs_write_end(struct file *file,
 {
 	struct inode *inode = page->mapping->host;
 
+	trace_f2fs_write_end(inode, pos, len, copied);
+
 	SetPageUptodate(page);
 	set_page_dirty(page);
 

commit 62aed044ea4bef36ac3f6885611b013b11c9be2d
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Tue May 6 16:46:04 2014 +0800

    f2fs: add a tracepoint for f2fs_write_begin
    
    This patch adds a tracepoint for f2fs_write_begin to trace write op of user.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index a7bb98f5831a..cde0d69969b9 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -900,6 +900,8 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 	struct dnode_of_data dn;
 	int err = 0;
 
+	trace_f2fs_write_begin(inode, pos, len, flags);
+
 	f2fs_balance_fs(sbi);
 repeat:
 	err = f2fs_convert_inline_data(inode, pos + len);

commit d5f66990bb928e7490ba4da94d585f618adcee5e
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Wed Apr 30 09:22:45 2014 +0900

    f2fs: decrease the lock granularity during write_begin
    
    This patch reduces the lock granularity during write_begin.
    When the system is under memory pressure, it would be better to reduce
    the locking time for the data pages.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 91ff104b3849..a7bb98f5831a 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -909,6 +909,10 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 	page = grab_cache_page_write_begin(mapping, index, flags);
 	if (!page)
 		return -ENOMEM;
+
+	/* to avoid latency during memory pressure */
+	unlock_page(page);
+
 	*pagep = page;
 
 	if (f2fs_has_inline_data(inode) && (pos + len) <= MAX_INLINE_DATA)
@@ -920,10 +924,18 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 	f2fs_unlock_op(sbi);
 
 	if (err) {
-		f2fs_put_page(page, 1);
+		f2fs_put_page(page, 0);
 		return err;
 	}
 inline_data:
+	lock_page(page);
+	if (unlikely(page->mapping != mapping)) {
+		f2fs_put_page(page, 1);
+		goto repeat;
+	}
+
+	f2fs_wait_on_page_writeback(page, DATA);
+
 	if ((len == PAGE_CACHE_SIZE) || PageUptodate(page))
 		return 0;
 

commit 9ac1349ad7e57013d6aa171cb014ad4166a1b50c
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Tue Apr 29 17:35:10 2014 +0900

    f2fs: avoid grab_cache_page_write_begin for data pages
    
    We don't need to wait on page writeback for these cases.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 273fe1631af9..91ff104b3849 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -417,7 +417,7 @@ struct page *find_data_page(struct inode *inode, pgoff_t index, bool sync)
 	if (unlikely(dn.data_blkaddr == NEW_ADDR))
 		return ERR_PTR(-EINVAL);
 
-	page = grab_cache_page_write_begin(mapping, index, AOP_FLAG_NOFS);
+	page = grab_cache_page(mapping, index);
 	if (!page)
 		return ERR_PTR(-ENOMEM);
 
@@ -455,7 +455,7 @@ struct page *get_lock_data_page(struct inode *inode, pgoff_t index)
 	int err;
 
 repeat:
-	page = grab_cache_page_write_begin(mapping, index, AOP_FLAG_NOFS);
+	page = grab_cache_page(mapping, index);
 	if (!page)
 		return ERR_PTR(-ENOMEM);
 

commit 6403eb1f646a49cc92f25c08f8716f8870a4a865
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Sat Apr 26 19:59:52 2014 +0800

    f2fs: introduce help macro ADDRS_PER_PAGE()
    
    Introduce help macro ADDRS_PER_PAGE() to get the number of address pointers in
    direct node or inode.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 0147de7e3973..273fe1631af9 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -652,8 +652,7 @@ static int get_data_block(struct inode *inode, sector_t iblock,
 		goto put_out;
 	}
 
-	end_offset = IS_INODE(dn.node_page) ?
-			ADDRS_PER_INODE(F2FS_I(inode)) : ADDRS_PER_BLOCK;
+	end_offset = ADDRS_PER_PAGE(dn.node_page, F2FS_I(inode));
 	bh_result->b_size = (((size_t)1) << blkbits);
 	dn.ofs_in_node++;
 	pgofs++;
@@ -675,8 +674,7 @@ static int get_data_block(struct inode *inode, sector_t iblock,
 		if (dn.data_blkaddr == NEW_ADDR)
 			goto put_out;
 
-		end_offset = IS_INODE(dn.node_page) ?
-			ADDRS_PER_INODE(F2FS_I(inode)) : ADDRS_PER_BLOCK;
+		end_offset = ADDRS_PER_PAGE(dn.node_page, F2FS_I(inode));
 	}
 
 	if (maxblocks > (bh_result->b_size >> blkbits)) {

commit 2aea39eca6b68d6ae7eb545332df0695f56a3d3f
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Thu Apr 24 09:49:52 2014 +0900

    f2fs: submit bio at the reclaim path
    
    If f2fs_write_data_page is called through the reclaim path, we should submit
    the bio right away.
    
    This patch resolves the following issue that Marc Dietrich reported.
    "It took me a while to bisect a problem which causes my ARM (tegra2) netbook to
    frequently stall for 5-10 seconds when I enable EXA acceleration (opentegra
    experimental ddx)."
    And this patch fixes that.
    
    Reported-by: Marc Dietrich <marvin24@gmx.de>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 150c12ace4af..0147de7e3973 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -833,6 +833,8 @@ static int f2fs_write_data_page(struct page *page,
 	unlock_page(page);
 	if (need_balance_fs)
 		f2fs_balance_fs(sbi);
+	if (wbc->for_reclaim)
+		f2fs_submit_merged_bio(sbi, DATA, WRITE);
 	return 0;
 
 redirty_out:

commit 454ae7e519f90db11c0ae082df12a162d2e206ce
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Tue Apr 22 13:34:01 2014 +0800

    f2fs: handle inline data independently in f2fs_bmap
    
    We'd better handle inline data case independently in f2fs_bmap().
    It can reduce our handling time in f2fs_bmap().
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 6b89b2517edf..150c12ace4af 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1058,6 +1058,11 @@ static int f2fs_set_data_page_dirty(struct page *page)
 
 static sector_t f2fs_bmap(struct address_space *mapping, sector_t block)
 {
+	struct inode *inode = mapping->host;
+
+	if (f2fs_has_inline_data(inode))
+		return 0;
+
 	return generic_block_bmap(mapping, block, get_data_block);
 }
 

commit 6fb03f3a40805a412c9b285010ffdc2e7563f81b
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Wed Apr 16 10:47:06 2014 +0900

    f2fs: adjust free mem size to flush dentry blocks
    
    If so many dirty dentry blocks are cached, not reached to the flush condition,
    we should fall into livelock in balance_dirty_pages.
    So, let's consider the mem size for the condition.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index b5cd6d1c9320..6b89b2517edf 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -863,7 +863,8 @@ static int f2fs_write_data_pages(struct address_space *mapping,
 		return 0;
 
 	if (S_ISDIR(inode->i_mode) && wbc->sync_mode == WB_SYNC_NONE &&
-			get_dirty_dents(inode) < nr_pages_to_skip(sbi, DATA))
+			get_dirty_dents(inode) < nr_pages_to_skip(sbi, DATA) &&
+			available_free_memory(sbi, DIRTY_DENTS))
 		goto skip_write;
 
 	diff = nr_pages_to_write(sbi, DATA, wbc);

commit 76f60268e70a700c04c85e1b0d520c94062a40a2
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Tue Apr 15 16:04:15 2014 +0900

    f2fs: call redirty_page_for_writepage
    
    This patch replace some general codes with redirty_page_for_writepage, which
    can be enabled after consideration on additional procedure like counting dirty
    pages appropriately.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 45abd60e2bff..b5cd6d1c9320 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -798,10 +798,8 @@ static int f2fs_write_data_page(struct page *page,
 	 * this page does not have to be written to disk.
 	 */
 	offset = i_size & (PAGE_CACHE_SIZE - 1);
-	if ((page->index >= end_index + 1) || !offset) {
-		inode_dec_dirty_dents(inode);
+	if ((page->index >= end_index + 1) || !offset)
 		goto out;
-	}
 
 	zero_user_segment(page, offset, PAGE_CACHE_SIZE);
 write:
@@ -810,7 +808,6 @@ static int f2fs_write_data_page(struct page *page,
 
 	/* Dentry blocks are controlled by checkpoint */
 	if (S_ISDIR(inode->i_mode)) {
-		inode_dec_dirty_dents(inode);
 		err = do_write_data_page(page, &fio);
 		goto done;
 	}
@@ -832,15 +829,14 @@ static int f2fs_write_data_page(struct page *page,
 
 	clear_cold_data(page);
 out:
+	inode_dec_dirty_dents(inode);
 	unlock_page(page);
 	if (need_balance_fs)
 		f2fs_balance_fs(sbi);
 	return 0;
 
 redirty_out:
-	wbc->pages_skipped++;
-	account_page_redirty(page);
-	set_page_dirty(page);
+	redirty_page_for_writepage(wbc, page);
 	return AOP_WRITEPAGE_ACTIVATE;
 }
 

commit 5b46f25ddc6edf4adff1a137d453da542c27a640
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sun Mar 16 18:07:34 2014 -0400

    f2fs: switch to iov_iter_alignment()
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 151488f27755..1d2e7e9624d2 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -992,10 +992,9 @@ static int f2fs_write_end(struct file *file,
 }
 
 static int check_direct_IO(struct inode *inode, int rw,
-		const struct iovec *iov, loff_t offset, unsigned long nr_segs)
+		struct iov_iter *iter, loff_t offset)
 {
 	unsigned blocksize_mask = inode->i_sb->s_blocksize - 1;
-	int i;
 
 	if (rw == READ)
 		return 0;
@@ -1003,9 +1002,9 @@ static int check_direct_IO(struct inode *inode, int rw,
 	if (offset & blocksize_mask)
 		return -EINVAL;
 
-	for (i = 0; i < nr_segs; i++)
-		if (iov[i].iov_len & blocksize_mask)
-			return -EINVAL;
+	if (iov_iter_alignment(iter) & blocksize_mask)
+		return -EINVAL;
+
 	return 0;
 }
 
@@ -1019,7 +1018,7 @@ static ssize_t f2fs_direct_IO(int rw, struct kiocb *iocb,
 	if (f2fs_has_inline_data(inode))
 		return 0;
 
-	if (check_direct_IO(inode, rw, iter->iov, offset, iter->nr_segs))
+	if (check_direct_IO(inode, rw, iter, offset))
 		return 0;
 
 	return blockdev_direct_IO(rw, iocb, inode, iter, offset,

commit 31b140398ce56ab41646eda7f02bcb78d6a4c916
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Wed Mar 5 01:33:16 2014 -0500

    switch {__,}blockdev_direct_IO() to iov_iter
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 3a6ef121c095..151488f27755 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1022,8 +1022,8 @@ static ssize_t f2fs_direct_IO(int rw, struct kiocb *iocb,
 	if (check_direct_IO(inode, rw, iter->iov, offset, iter->nr_segs))
 		return 0;
 
-	return blockdev_direct_IO(rw, iocb, inode, iter->iov, offset,
-				  iter->nr_segs, get_data_block);
+	return blockdev_direct_IO(rw, iocb, inode, iter, offset,
+				  get_data_block);
 }
 
 static void f2fs_invalidate_data_page(struct page *page, unsigned int offset,

commit d8d3d94b80aa1a1c0ca75c58b8abdc7356f38418
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Tue Mar 4 21:27:34 2014 -0500

    pass iov_iter to ->direct_IO()
    
    unmodified, for now
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 45abd60e2bff..3a6ef121c095 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1010,7 +1010,7 @@ static int check_direct_IO(struct inode *inode, int rw,
 }
 
 static ssize_t f2fs_direct_IO(int rw, struct kiocb *iocb,
-		const struct iovec *iov, loff_t offset, unsigned long nr_segs)
+		struct iov_iter *iter, loff_t offset)
 {
 	struct file *file = iocb->ki_filp;
 	struct inode *inode = file->f_mapping->host;
@@ -1019,11 +1019,11 @@ static ssize_t f2fs_direct_IO(int rw, struct kiocb *iocb,
 	if (f2fs_has_inline_data(inode))
 		return 0;
 
-	if (check_direct_IO(inode, rw, iov, offset, nr_segs))
+	if (check_direct_IO(inode, rw, iter->iov, offset, iter->nr_segs))
 		return 0;
 
-	return blockdev_direct_IO(rw, iocb, inode, iov, offset, nr_segs,
-							get_data_block);
+	return blockdev_direct_IO(rw, iocb, inode, iter->iov, offset,
+				  iter->nr_segs, get_data_block);
 }
 
 static void f2fs_invalidate_data_page(struct page *page, unsigned int offset,

commit d54c795b495b9bd417e482286c832c9a8eb210ae
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Sat Mar 29 15:30:40 2014 +0800

    f2fs: fix error path when fail to read inline data
    
    We should unlock page in ->readpage() path and also should unlock & release page
    in error path of ->write_begin() to avoid deadlock or memory leak.
    So let's add release code to fix the problem when we fail to read inline data.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 598bfa617a7e..45abd60e2bff 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -942,13 +942,19 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 	if (dn.data_blkaddr == NEW_ADDR) {
 		zero_user_segment(page, 0, PAGE_CACHE_SIZE);
 	} else {
-		if (f2fs_has_inline_data(inode))
+		if (f2fs_has_inline_data(inode)) {
 			err = f2fs_read_inline_data(inode, page);
-		else
+			if (err) {
+				page_cache_release(page);
+				return err;
+			}
+		} else {
 			err = f2fs_submit_page_bio(sbi, page, dn.data_blkaddr,
 							READ_SYNC);
-		if (err)
-			return err;
+			if (err)
+				return err;
+		}
+
 		lock_page(page);
 		if (unlikely(!PageUptodate(page))) {
 			f2fs_put_page(page, 1);

commit df0f8dc0e154de13e3a54846f384b674dd557c85
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Sat Mar 22 14:57:23 2014 +0800

    f2fs: avoid unnecessary bio submit when wait page writeback
    
    This patch introduce is_merged_page() to check whether current page is merged
    in f2fs bio cache. When page is not in cache, we can avoid submitting bio cache,
    resulting in having more chance to merge pages.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index b0c923aef229..598bfa617a7e 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -134,7 +134,7 @@ void f2fs_submit_merged_bio(struct f2fs_sb_info *sbi,
 
 	io = is_read_io(rw) ? &sbi->read_io : &sbi->write_io[btype];
 
-	mutex_lock(&io->io_mutex);
+	down_write(&io->io_rwsem);
 
 	/* change META to META_FLUSH in the checkpoint procedure */
 	if (type >= META_FLUSH) {
@@ -142,7 +142,7 @@ void f2fs_submit_merged_bio(struct f2fs_sb_info *sbi,
 		io->fio.rw = WRITE_FLUSH_FUA | REQ_META | REQ_PRIO;
 	}
 	__submit_merged_bio(io);
-	mutex_unlock(&io->io_mutex);
+	up_write(&io->io_rwsem);
 }
 
 /*
@@ -180,7 +180,7 @@ void f2fs_submit_page_mbio(struct f2fs_sb_info *sbi, struct page *page,
 
 	verify_block_addr(sbi, blk_addr);
 
-	mutex_lock(&io->io_mutex);
+	down_write(&io->io_rwsem);
 
 	if (!is_read)
 		inc_page_count(sbi, F2FS_WRITEBACK);
@@ -204,7 +204,7 @@ void f2fs_submit_page_mbio(struct f2fs_sb_info *sbi, struct page *page,
 
 	io->last_block_in_bio = blk_addr;
 
-	mutex_unlock(&io->io_mutex);
+	up_write(&io->io_rwsem);
 	trace_f2fs_submit_page_mbio(page, fio->rw, fio->type, blk_addr);
 }
 

commit 50c8cdb35ad8016c52fb2326ef9d65542e3a3e1b
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Tue Mar 18 13:47:11 2014 +0900

    f2fs: introduce nr_pages_to_write for segment alignment
    
    This patch introduces nr_pages_to_write to align page writes to the segment
    or other operational unit size, which can be tuned according to the system
    environment.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 3bc380942068..b0c923aef229 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -844,8 +844,6 @@ static int f2fs_write_data_page(struct page *page,
 	return AOP_WRITEPAGE_ACTIVATE;
 }
 
-#define MAX_DESIRED_PAGES_WP	4096
-
 static int __f2fs_writepage(struct page *page, struct writeback_control *wbc,
 			void *data)
 {
@@ -862,7 +860,7 @@ static int f2fs_write_data_pages(struct address_space *mapping,
 	struct f2fs_sb_info *sbi = F2FS_SB(inode->i_sb);
 	bool locked = false;
 	int ret;
-	long excess_nrtw = 0, desired_nrtw;
+	long diff;
 
 	/* deal with chardevs and other special file */
 	if (!mapping->a_ops->writepage)
@@ -872,11 +870,7 @@ static int f2fs_write_data_pages(struct address_space *mapping,
 			get_dirty_dents(inode) < nr_pages_to_skip(sbi, DATA))
 		goto skip_write;
 
-	if (wbc->nr_to_write < MAX_DESIRED_PAGES_WP) {
-		desired_nrtw = MAX_DESIRED_PAGES_WP;
-		excess_nrtw = desired_nrtw - wbc->nr_to_write;
-		wbc->nr_to_write = desired_nrtw;
-	}
+	diff = nr_pages_to_write(sbi, DATA, wbc);
 
 	if (!S_ISDIR(inode->i_mode)) {
 		mutex_lock(&sbi->writepages);
@@ -890,7 +884,7 @@ static int f2fs_write_data_pages(struct address_space *mapping,
 
 	remove_dirty_dir_inode(inode);
 
-	wbc->nr_to_write -= excess_nrtw;
+	wbc->nr_to_write = max((long)0, wbc->nr_to_write - diff);
 	return ret;
 
 skip_write:

commit d3baf95da5b0bce9fe980eeff6140817d63fabdf
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Tue Mar 18 13:43:05 2014 +0900

    f2fs: increase pages_skipped when skipping writepages
    
    This patch increases pages_skipped when skipping writepages.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index e3b7cfa17b99..3bc380942068 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -870,7 +870,7 @@ static int f2fs_write_data_pages(struct address_space *mapping,
 
 	if (S_ISDIR(inode->i_mode) && wbc->sync_mode == WB_SYNC_NONE &&
 			get_dirty_dents(inode) < nr_pages_to_skip(sbi, DATA))
-		return 0;
+		goto skip_write;
 
 	if (wbc->nr_to_write < MAX_DESIRED_PAGES_WP) {
 		desired_nrtw = MAX_DESIRED_PAGES_WP;
@@ -892,6 +892,10 @@ static int f2fs_write_data_pages(struct address_space *mapping,
 
 	wbc->nr_to_write -= excess_nrtw;
 	return ret;
+
+skip_write:
+	wbc->pages_skipped += get_dirty_dents(inode);
+	return 0;
 }
 
 static int f2fs_write_begin(struct file *file, struct address_space *mapping,

commit 87d6f890944d092c4ef5b84053f0d0d5d8137b0b
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Tue Mar 18 12:40:49 2014 +0900

    f2fs: avoid small data writes by skipping writepages
    
    This patch introduces nr_pages_to_skip(sbi, type) to determine writepages can
    be skipped.
    The dentry, node, and meta pages can be conrolled by F2FS without breaking the
    FS consistency.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 101b4cd4170d..e3b7cfa17b99 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -868,6 +868,10 @@ static int f2fs_write_data_pages(struct address_space *mapping,
 	if (!mapping->a_ops->writepage)
 		return 0;
 
+	if (S_ISDIR(inode->i_mode) && wbc->sync_mode == WB_SYNC_NONE &&
+			get_dirty_dents(inode) < nr_pages_to_skip(sbi, DATA))
+		return 0;
+
 	if (wbc->nr_to_write < MAX_DESIRED_PAGES_WP) {
 		desired_nrtw = MAX_DESIRED_PAGES_WP;
 		excess_nrtw = desired_nrtw - wbc->nr_to_write;

commit 9cf3c3898a274ca637b88ad01b0830550ee2d318
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Fri Feb 28 10:12:05 2014 +0800

    f2fs: fix dirty page accounting when redirty
    
    We should de-account dirty counters for page when redirty in ->writepage().
    
    Wu Fengguang described in 'commit 971767caf632190f77a40b4011c19948232eed75':
    "writeback: fix dirtied pages accounting on redirty
    De-account the accumulative dirty counters on page redirty.
    
    Page redirties (very common in ext4) will introduce mismatch between
    counters (a) and (b)
    
    a) NR_DIRTIED, BDI_DIRTIED, tsk->nr_dirtied
    b) NR_WRITTEN, BDI_WRITTEN
    
    This will introduce systematic errors in balanced_rate and result in
    dirty page position errors (ie. the dirty pages are no longer balanced
    around the global/bdi setpoints)."
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 93d80ea4674b..101b4cd4170d 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -839,6 +839,7 @@ static int f2fs_write_data_page(struct page *page,
 
 redirty_out:
 	wbc->pages_skipped++;
+	account_page_redirty(page);
 	set_page_dirty(page);
 	return AOP_WRITEPAGE_ACTIVATE;
 }

commit 8618b881e9fd0e1817ff5cb7befadd3240d54830
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Mon Feb 17 19:29:27 2014 +0900

    f2fs: fix not to write data pages on the page reclaiming path
    
    Even if f2fs_write_data_page is called by the page reclaiming path, we should
    not write the page to provide enough free segments for the worst case scenario.
    Otherwise, f2fs can face with no free segment while gc is conducted, resulting
    in:
    
     ------------[ cut here ]------------
     kernel BUG at /home/zeus/f2fs_test/src/fs/f2fs/segment.c:565!
     RIP: 0010:[<ffffffffa02c3b11>]  [<ffffffffa02c3b11>] new_curseg+0x331/0x340 [f2fs]
     Call Trace:
      allocate_segment_by_default+0x204/0x280 [f2fs]
      allocate_data_block+0x108/0x210 [f2fs]
      write_data_page+0x8a/0xc0 [f2fs]
      do_write_data_page+0xe1/0x2a0 [f2fs]
      move_data_page+0x8a/0xf0 [f2fs]
      f2fs_gc+0x446/0x970 [f2fs]
      f2fs_balance_fs+0xb6/0xd0 [f2fs]
      f2fs_write_begin+0x50/0x350 [f2fs]
      ? unlock_page+0x27/0x30
      ? unlock_page+0x27/0x30
      generic_file_buffered_write+0x10a/0x280
      ? file_update_time+0xa3/0xf0
      __generic_file_aio_write+0x1c8/0x3d0
      ? generic_file_aio_write+0x52/0xb0
      ? generic_file_aio_write+0x52/0xb0
      generic_file_aio_write+0x65/0xb0
      do_sync_write+0x5a/0x90
      vfs_write+0xc5/0x1f0
      SyS_write+0x55/0xa0
      system_call_fastpath+0x16/0x1b
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index b401be71ecbd..93d80ea4674b 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -805,38 +805,30 @@ static int f2fs_write_data_page(struct page *page,
 
 	zero_user_segment(page, offset, PAGE_CACHE_SIZE);
 write:
-	if (unlikely(sbi->por_doing)) {
-		err = AOP_WRITEPAGE_ACTIVATE;
+	if (unlikely(sbi->por_doing))
 		goto redirty_out;
-	}
 
 	/* Dentry blocks are controlled by checkpoint */
 	if (S_ISDIR(inode->i_mode)) {
 		inode_dec_dirty_dents(inode);
 		err = do_write_data_page(page, &fio);
-	} else {
-		f2fs_lock_op(sbi);
-
-		if (f2fs_has_inline_data(inode) || f2fs_may_inline(inode)) {
-			err = f2fs_write_inline_data(inode, page, offset);
-			f2fs_unlock_op(sbi);
-			goto out;
-		} else {
-			err = do_write_data_page(page, &fio);
-		}
+		goto done;
+	}
 
-		f2fs_unlock_op(sbi);
+	if (!wbc->for_reclaim)
 		need_balance_fs = true;
-	}
-	if (err == -ENOENT)
-		goto out;
-	else if (err)
+	else if (has_not_enough_free_secs(sbi, 0))
 		goto redirty_out;
 
-	if (wbc->for_reclaim) {
-		f2fs_submit_merged_bio(sbi, DATA, WRITE);
-		need_balance_fs = false;
-	}
+	f2fs_lock_op(sbi);
+	if (f2fs_has_inline_data(inode) || f2fs_may_inline(inode))
+		err = f2fs_write_inline_data(inode, page, offset);
+	else
+		err = do_write_data_page(page, &fio);
+	f2fs_unlock_op(sbi);
+done:
+	if (err && err != -ENOENT)
+		goto redirty_out;
 
 	clear_cold_data(page);
 out:
@@ -848,7 +840,7 @@ static int f2fs_write_data_page(struct page *page,
 redirty_out:
 	wbc->pages_skipped++;
 	set_page_dirty(page);
-	return err;
+	return AOP_WRITEPAGE_ACTIVATE;
 }
 
 #define MAX_DESIRED_PAGES_WP	4096

commit 1fe54f9dd3acfaa3ed4e1d1e3278fd0f1d1e98cd
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Fri Feb 7 10:00:06 2014 +0900

    f2fs: clean up redundant function call
    
    This patch integrates inode_[inc|dec]_dirty_dents with inc_page_count to remove
    redundant calls.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index d175ae3b612a..b401be71ecbd 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -799,10 +799,7 @@ static int f2fs_write_data_page(struct page *page,
 	 */
 	offset = i_size & (PAGE_CACHE_SIZE - 1);
 	if ((page->index >= end_index + 1) || !offset) {
-		if (S_ISDIR(inode->i_mode)) {
-			dec_page_count(sbi, F2FS_DIRTY_DENTS);
-			inode_dec_dirty_dents(inode);
-		}
+		inode_dec_dirty_dents(inode);
 		goto out;
 	}
 
@@ -815,7 +812,6 @@ static int f2fs_write_data_page(struct page *page,
 
 	/* Dentry blocks are controlled by checkpoint */
 	if (S_ISDIR(inode->i_mode)) {
-		dec_page_count(sbi, F2FS_DIRTY_DENTS);
 		inode_dec_dirty_dents(inode);
 		err = do_write_data_page(page, &fio);
 	} else {
@@ -1033,11 +1029,8 @@ static void f2fs_invalidate_data_page(struct page *page, unsigned int offset,
 				      unsigned int length)
 {
 	struct inode *inode = page->mapping->host;
-	struct f2fs_sb_info *sbi = F2FS_SB(inode->i_sb);
-	if (S_ISDIR(inode->i_mode) && PageDirty(page)) {
-		dec_page_count(sbi, F2FS_DIRTY_DENTS);
+	if (PageDirty(page))
 		inode_dec_dirty_dents(inode);
-	}
 	ClearPagePrivate(page);
 }
 

commit 1b1f559fc362f96869b7e04ef9825b1039b9a67d
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Mon Feb 3 10:50:22 2014 +0900

    f2fs: remove the ugly pointer conversion
    
    This patch modifies the use of bi_private to remove pointer chasing for sbi.
    Previously, we had a bi_private structure, but it needs memory allocation.
    So this patch uses bi_private by the sbi pointer and adds a completion pointer
    into the sbi.
    This can achieve no memory allocation and nice use of the bi_private.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 20c3c648e56d..d175ae3b612a 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -45,7 +45,7 @@ static void f2fs_read_end_io(struct bio *bio, int err)
 
 static void f2fs_write_end_io(struct bio *bio, int err)
 {
-	struct f2fs_sb_info *sbi = F2FS_SB(bio->bi_io_vec->bv_page->mapping->host->i_sb);
+	struct f2fs_sb_info *sbi = bio->bi_private;
 	struct bio_vec *bvec;
 	int i;
 
@@ -61,8 +61,10 @@ static void f2fs_write_end_io(struct bio *bio, int err)
 		dec_page_count(sbi, F2FS_WRITEBACK);
 	}
 
-	if (bio->bi_private)
-		complete(bio->bi_private);
+	if (sbi->wait_io) {
+		complete(sbi->wait_io);
+		sbi->wait_io = NULL;
+	}
 
 	if (!get_pages(sbi, F2FS_WRITEBACK) &&
 			!list_empty(&sbi->cp_wait.task_list))
@@ -85,6 +87,7 @@ static struct bio *__bio_alloc(struct f2fs_sb_info *sbi, block_t blk_addr,
 	bio->bi_bdev = sbi->sb->s_bdev;
 	bio->bi_iter.bi_sector = SECTOR_FROM_BLOCK(sbi, blk_addr);
 	bio->bi_end_io = is_read ? f2fs_read_end_io : f2fs_write_end_io;
+	bio->bi_private = sbi;
 
 	return bio;
 }
@@ -112,7 +115,7 @@ static void __submit_merged_bio(struct f2fs_bio_info *io)
 		 */
 		if (fio->type == META_FLUSH) {
 			DECLARE_COMPLETION_ONSTACK(wait);
-			io->bio->bi_private = &wait;
+			io->sbi->wait_io = &wait;
 			submit_bio(rw, io->bio);
 			wait_for_completion(&wait);
 		} else {

commit 744602cf45ce35758b8637f76bc263c871abc6ea
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Fri Jan 24 09:42:16 2014 +0900

    f2fs: update_inode_page should be done all the time
    
    In order to make fs consistency, update_inode_page should not be failed all
    the time. Otherwise, it is possible to lose some metadata in the inode like
    a link count.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 2261ccdd0b5f..20c3c648e56d 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -55,8 +55,7 @@ static void f2fs_write_end_io(struct bio *bio, int err)
 		if (unlikely(err)) {
 			SetPageError(page);
 			set_bit(AS_EIO, &page->mapping->flags);
-			set_ckpt_flags(sbi->ckpt, CP_ERROR_FLAG);
-			sbi->sb->s_flags |= MS_RDONLY;
+			f2fs_stop_checkpoint(sbi);
 		}
 		end_page_writeback(page);
 		dec_page_count(sbi, F2FS_WRITEBACK);

commit f568849edac8611d603e00bd6cbbcfea09395ae6
Merge: d9894c228b11 675675ada486
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jan 30 11:19:05 2014 -0800

    Merge branch 'for-3.14/core' of git://git.kernel.dk/linux-block
    
    Pull core block IO changes from Jens Axboe:
     "The major piece in here is the immutable bio_ve series from Kent, the
      rest is fairly minor.  It was supposed to go in last round, but
      various issues pushed it to this release instead.  The pull request
      contains:
    
       - Various smaller blk-mq fixes from different folks.  Nothing major
         here, just minor fixes and cleanups.
    
       - Fix for a memory leak in the error path in the block ioctl code
         from Christian Engelmayer.
    
       - Header export fix from CaiZhiyong.
    
       - Finally the immutable biovec changes from Kent Overstreet.  This
         enables some nice future work on making arbitrarily sized bios
         possible, and splitting more efficient.  Related fixes to immutable
         bio_vecs:
    
            - dm-cache immutable fixup from Mike Snitzer.
            - btrfs immutable fixup from Muthu Kumar.
    
      - bio-integrity fix from Nic Bellinger, which is also going to stable"
    
    * 'for-3.14/core' of git://git.kernel.dk/linux-block: (44 commits)
      xtensa: fixup simdisk driver to work with immutable bio_vecs
      block/blk-mq-cpu.c: use hotcpu_notifier()
      blk-mq: for_each_* macro correctness
      block: Fix memory leak in rw_copy_check_uvector() handling
      bio-integrity: Fix bio_integrity_verify segment start bug
      block: remove unrelated header files and export symbol
      blk-mq: uses page->list incorrectly
      blk-mq: use __smp_call_function_single directly
      btrfs: fix missing increment of bi_remaining
      Revert "block: Warn and free bio if bi_end_io is not set"
      block: Warn and free bio if bi_end_io is not set
      blk-mq: fix initializing request's start time
      block: blk-mq: don't export blk_mq_free_queue()
      block: blk-mq: make blk_sync_queue support mq
      block: blk-mq: support draining mq queue
      dm cache: increment bi_remaining when bi_end_io is restored
      block: fixup for generic bio chaining
      block: Really silence spurious compiler warnings
      block: Silence spurious compiler warnings
      block: Kill bio_pair_split()
      ...

commit a18ff063406dd6aec41fda598eabe2691007a30d
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Tue Jan 21 13:32:12 2014 +0900

    f2fs: call mark_inode_dirty to flush dirty pages
    
    If a dentry page is updated, we should call mark_inode_dirty to add the inode
    into the dirty list, so that its dentry pages are flushed to the disk.
    Otherwise, the inode can be evicted without flush.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index f5fac16bc6e7..0ae558723506 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -249,6 +249,7 @@ int reserve_new_block(struct dnode_of_data *dn)
 
 	__set_data_blkaddr(dn, NEW_ADDR);
 	dn->data_blkaddr = NEW_ADDR;
+	mark_inode_dirty(dn->inode);
 	sync_inode_page(dn);
 	return 0;
 }
@@ -564,7 +565,6 @@ struct page *get_new_data_page(struct inode *inode,
 		i_size_write(inode, ((index + 1) << PAGE_CACHE_SHIFT));
 		/* Only the directory inode sets new_i_size */
 		set_inode_flag(F2FS_I(inode), FI_UPDATE_DIR);
-		mark_inode_dirty_sync(inode);
 	}
 	return page;
 
@@ -1060,6 +1060,8 @@ static int f2fs_set_data_page_dirty(struct page *page)
 	trace_f2fs_set_page_dirty(page, DATA);
 
 	SetPageUptodate(page);
+	mark_inode_dirty(inode);
+
 	if (!PageDirty(page)) {
 		__set_page_dirty_nobuffers(page);
 		set_dirty_dir_page(inode, page);

commit 6c311ec6c2d9e015d454b4e3fda8008b5bebf316
Author: Chris Fries <cfries@motorola.com>
Date:   Fri Jan 17 14:44:39 2014 -0600

    f2fs: clean checkpatch warnings
    
    Fixed a variety of trivial checkpatch warnings.  The only delta should
    be some minor formatting on log strings that were split / too long.
    
    Signed-off-by: Chris Fries <cfries@motorola.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index bda889e2ca63..f5fac16bc6e7 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -792,7 +792,7 @@ static int f2fs_write_data_page(struct page *page,
 	int err = 0;
 	struct f2fs_io_info fio = {
 		.type = DATA,
-		.rw = (wbc->sync_mode == WB_SYNC_ALL) ? WRITE_SYNC: WRITE,
+		.rw = (wbc->sync_mode == WB_SYNC_ALL) ? WRITE_SYNC : WRITE,
 	};
 
 	if (page->index < end_index)

commit c434cbc0edda6a7a59d22b9d5d4279989d0ab804
Author: Changman Lee <cm224.lee@samsung.com>
Date:   Thu Jan 16 11:58:54 2014 +0900

    f2fs: missing REQ_META and REQ_PRIO when sync_meta_pages(META_FLUSH)
    
    Doing sync_meta_pages with META_FLUSH when checkpoint, we overide rw
    using WRITE_FLUSH_FUA. At this time, we also should set
    REQ_META|REQ_PRIO.
    
    Signed-off-by: Changman Lee <cm224.lee@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index e57bde02e37f..bda889e2ca63 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -144,7 +144,7 @@ void f2fs_submit_merged_bio(struct f2fs_sb_info *sbi,
 	/* change META to META_FLUSH in the checkpoint procedure */
 	if (type >= META_FLUSH) {
 		io->fio.type = META_FLUSH;
-		io->fio.rw = WRITE_FLUSH_FUA;
+		io->fio.rw = WRITE_FLUSH_FUA | REQ_META | REQ_PRIO;
 	}
 	__submit_merged_bio(io);
 	mutex_unlock(&io->io_mutex);

commit c33ec32692e1f2f4650f7bf5bb1108bb346b82a4
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Thu Jan 16 16:20:40 2014 +0900

    f2fs: avoid f2fs_balance_fs call during pageout
    
    This patch should resolve the following bug.
    
    =========================================================
    [ INFO: possible irq lock inversion dependency detected ]
    3.13.0-rc5.f2fs+ #6 Not tainted
    ---------------------------------------------------------
    kswapd0/41 just changed the state of lock:
     (&sbi->gc_mutex){+.+.-.}, at: [<ffffffffa030503e>] f2fs_balance_fs+0xae/0xd0 [f2fs]
    but this lock took another, RECLAIM_FS-READ-unsafe lock in the past:
     (&sbi->cp_rwsem){++++.?}
    
    and interrupts could create inverse lock ordering between them.
    
    other info that might help us debug this:
    Chain exists of:
      &sbi->gc_mutex --> &sbi->cp_mutex --> &sbi->cp_rwsem
    
     Possible interrupt unsafe locking scenario:
    
           CPU0                    CPU1
           ----                    ----
      lock(&sbi->cp_rwsem);
                                   local_irq_disable();
                                   lock(&sbi->gc_mutex);
                                   lock(&sbi->cp_mutex);
      <Interrupt>
        lock(&sbi->gc_mutex);
    
     *** DEADLOCK ***
    
    This bug is due to the f2fs_balance_fs call in f2fs_write_data_page.
    If f2fs_write_data_page is triggered by wbc->for_reclaim via kswapd, it should
    not call f2fs_balance_fs which tries to get a mutex grabbed by original syscall
    flow.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 19ad06694478..e57bde02e37f 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -842,8 +842,10 @@ static int f2fs_write_data_page(struct page *page,
 	else if (err)
 		goto redirty_out;
 
-	if (wbc->for_reclaim)
+	if (wbc->for_reclaim) {
 		f2fs_submit_merged_bio(sbi, DATA, WRITE);
+		need_balance_fs = false;
+	}
 
 	clear_cold_data(page);
 out:

commit 5514f0aadddcdfaaaea697b60203f5402552eb7b
Author: Yuan Zhong <yuan.mark.zhong@samsung.com>
Date:   Fri Jan 10 07:26:14 2014 +0000

    f2fs: remove the needless parameter of f2fs_wait_on_page_writeback
    
    "boo sync" parameter is never referenced in f2fs_wait_on_page_writeback.
    We should remove this parameter.
    
    Signed-off-by: Yuan Zhong <yuan.mark.zhong@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 63d190264a36..19ad06694478 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -226,7 +226,7 @@ static void __set_data_blkaddr(struct dnode_of_data *dn, block_t new_addr)
 	struct page *node_page = dn->node_page;
 	unsigned int ofs_in_node = dn->ofs_in_node;
 
-	f2fs_wait_on_page_writeback(node_page, NODE, false);
+	f2fs_wait_on_page_writeback(node_page, NODE);
 
 	rn = F2FS_NODE(node_page);
 

commit a8865372a8414298982e07f4ac8d6dc0ab1e0a3d
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Fri Dec 27 17:04:17 2013 +0900

    f2fs: handle errors correctly during f2fs_reserve_block
    
    The get_dnode_of_data nullifies inode and node page when error is occurred.
    
    There are two cases that passes inode page into get_dnode_of_data().
    
    1. make_empty_dir()
        -> get_new_data_page()
          -> f2fs_reserve_block(ipage)
            -> get_dnode_of_data()
    
    2. f2fs_convert_inline_data()
        -> __f2fs_convert_inline_data()
          -> f2fs_reserve_block(ipage)
            -> get_dnode_of_data()
    
    This patch adds correct error handling codes when get_dnode_of_data() returns
    an error.
    
    At first, f2fs_reserve_block() calls f2fs_put_dnode() whenever reserve_new_block
    returns an error.
    So, the rule of f2fs_reserve_block() is to nullify inode page when there is any
    error internally.
    
    Finally, two callers of f2fs_reserve_block() should call f2fs_put_dnode()
    appropriately if they got an error since successful f2fs_reserve_block().
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index fc7a28c5ad23..63d190264a36 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -258,13 +258,16 @@ int f2fs_reserve_block(struct dnode_of_data *dn, pgoff_t index)
 	bool need_put = dn->inode_page ? false : true;
 	int err;
 
+	/* if inode_page exists, index should be zero */
+	f2fs_bug_on(!need_put && index);
+
 	err = get_dnode_of_data(dn, index, ALLOC_NODE);
 	if (err)
 		return err;
+
 	if (dn->data_blkaddr == NULL_ADDR)
 		err = reserve_new_block(dn);
-
-	if (need_put)
+	if (err || need_put)
 		f2fs_put_dnode(dn);
 	return err;
 }
@@ -510,10 +513,10 @@ struct page *get_lock_data_page(struct inode *inode, pgoff_t index)
  *
  * Also, caller should grab and release a rwsem by calling f2fs_lock_op() and
  * f2fs_unlock_op().
- * Note that, npage is set only by make_empty_dir.
+ * Note that, ipage is set only by make_empty_dir.
  */
 struct page *get_new_data_page(struct inode *inode,
-		struct page *npage, pgoff_t index, bool new_i_size)
+		struct page *ipage, pgoff_t index, bool new_i_size)
 {
 	struct f2fs_sb_info *sbi = F2FS_SB(inode->i_sb);
 	struct address_space *mapping = inode->i_mapping;
@@ -521,14 +524,16 @@ struct page *get_new_data_page(struct inode *inode,
 	struct dnode_of_data dn;
 	int err;
 
-	set_new_dnode(&dn, inode, npage, npage, 0);
+	set_new_dnode(&dn, inode, ipage, NULL, 0);
 	err = f2fs_reserve_block(&dn, index);
 	if (err)
 		return ERR_PTR(err);
 repeat:
 	page = grab_cache_page(mapping, index);
-	if (!page)
-		return ERR_PTR(-ENOMEM);
+	if (!page) {
+		err = -ENOMEM;
+		goto put_err;
+	}
 
 	if (PageUptodate(page))
 		return page;
@@ -540,11 +545,13 @@ struct page *get_new_data_page(struct inode *inode,
 		err = f2fs_submit_page_bio(sbi, page, dn.data_blkaddr,
 								READ_SYNC);
 		if (err)
-			return ERR_PTR(err);
+			goto put_err;
+
 		lock_page(page);
 		if (unlikely(!PageUptodate(page))) {
 			f2fs_put_page(page, 1);
-			return ERR_PTR(-EIO);
+			err = -EIO;
+			goto put_err;
 		}
 		if (unlikely(page->mapping != mapping)) {
 			f2fs_put_page(page, 1);
@@ -560,6 +567,10 @@ struct page *get_new_data_page(struct inode *inode,
 		mark_inode_dirty_sync(inode);
 	}
 	return page;
+
+put_err:
+	f2fs_put_dnode(&dn);
+	return ERR_PTR(err);
 }
 
 static int __allocate_data_block(struct dnode_of_data *dn)

commit 9e09fc855dd6f6ed510b3db7f3c3c1dd73631ac7
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Fri Dec 27 12:28:59 2013 +0900

    f2fs: refactor f2fs_convert_inline_data
    
    Change log from v1:
     o handle NULL pointer of grab_cache_page_write_begin() pointed by Chao Yu.
    
    This patch refactors f2fs_convert_inline_data to check a couple of conditions
    internally for deciding whether it needs to convert inline_data or not.
    
    So, the new f2fs_convert_inline_data initially checks:
    1) f2fs_has_inline_data(), and
    2) the data size to be changed.
    
    If the inode has inline_data but the size to fill is less than MAX_INLINE_DATA,
    then we don't need to convert the inline_data with data allocation.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 253e6633dbf6..fc7a28c5ad23 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -906,21 +906,17 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 
 	f2fs_balance_fs(sbi);
 repeat:
+	err = f2fs_convert_inline_data(inode, pos + len);
+	if (err)
+		return err;
+
 	page = grab_cache_page_write_begin(mapping, index, flags);
 	if (!page)
 		return -ENOMEM;
 	*pagep = page;
 
-	if ((pos + len) < MAX_INLINE_DATA) {
-		if (f2fs_has_inline_data(inode))
-			goto inline_data;
-	} else if (f2fs_has_inline_data(inode)) {
-		err = f2fs_convert_inline_data(inode, page, flags);
-		if (err) {
-			f2fs_put_page(page, 1);
-			return err;
-		}
-	}
+	if (f2fs_has_inline_data(inode) && (pos + len) <= MAX_INLINE_DATA)
+		goto inline_data;
 
 	f2fs_lock_op(sbi);
 	set_new_dnode(&dn, inode, NULL, NULL, 0);

commit 26f466f4a948ddc765f9b474ad6e0bdb94fb1a66
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Fri Dec 27 12:21:49 2013 +0900

    f2fs: call f2fs_put_page at the error case
    
    In f2fs_write_begin(), if f2fs_conver_inline_data() returns an error like
    -ENOSPC, f2fs should call f2fs_put_page().
    Otherwise, it is remained as a locked page, resulting in the following bug.
    
    [<ffffffff8114657e>] sleep_on_page+0xe/0x20
    [<ffffffff81146567>] __lock_page+0x67/0x70
    [<ffffffff81157d08>] truncate_inode_pages_range+0x368/0x5d0
    [<ffffffff81157ff5>] truncate_inode_pages+0x15/0x20
    [<ffffffff8115804b>] truncate_pagecache+0x4b/0x70
    [<ffffffff81158082>] truncate_setsize+0x12/0x20
    [<ffffffffa02a1842>] f2fs_setattr+0x72/0x270 [f2fs]
    [<ffffffff811cdae3>] notify_change+0x213/0x400
    [<ffffffff811ab376>] do_truncate+0x66/0xa0
    [<ffffffff811ab541>] vfs_truncate+0x191/0x1b0
    [<ffffffff811ab5bc>] do_sys_truncate+0x5c/0xa0
    [<ffffffff811ab78e>] SyS_truncate+0xe/0x10
    [<ffffffff81756052>] system_call_fastpath+0x16/0x1b
    [<ffffffffffffffff>] 0xffffffffffffffff
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index bf39eed2442f..253e6633dbf6 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -916,8 +916,10 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 			goto inline_data;
 	} else if (f2fs_has_inline_data(inode)) {
 		err = f2fs_convert_inline_data(inode, page, flags);
-		if (err)
+		if (err) {
+			f2fs_put_page(page, 1);
 			return err;
+		}
 	}
 
 	f2fs_lock_op(sbi);

commit 9ffe0fb5f3bbd01c766162bb17a62dee0df7e125
Author: Huajun Li <huajun.li@intel.com>
Date:   Sun Nov 10 23:13:20 2013 +0800

    f2fs: handle inline data operations
    
    Hook inline data read/write, truncate, fallocate, setattr, etc.
    
    Files need meet following 2 requirement to inline:
     1) file size is not greater than MAX_INLINE_DATA;
     2) file doesn't pre-allocate data blocks by fallocate().
    
    FI_INLINE_DATA will not be set while creating a new regular inode because
    most of the files are bigger than ~3.4K. Set FI_INLINE_DATA only when
    data is submitted to block layer, ranther than set it while creating a new
    inode, this also avoids converting data from inline to normal data block
    and vice versa.
    
    While writting inline data to inode block, the first data block should be
    released if the file has a block indexed by i_addr[0].
    
    On the other hand, when a file operation is appied to a file with inline
    data, we need to test if this file can remain inline by doing this
    operation, otherwise it should be convert into normal file by reserving
    a new data block, copying inline data to this new block and clear
    FI_INLINE_DATA flag. Because reserve a new data block here will make use
    of i_addr[0], if we save inline data in i_addr[0..872], then the first
    4 bytes would be overwriten. This problem can be avoided simply by
    not using i_addr[0] for inline data.
    
    Signed-off-by: Huajun Li <huajun.li@intel.com>
    Signed-off-by: Haicheng Li <haicheng.li@linux.intel.com>
    Signed-off-by: Weihong Xu <weihong.xu@intel.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index e0965b43d16e..bf39eed2442f 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -706,13 +706,28 @@ static int get_data_block(struct inode *inode, sector_t iblock,
 
 static int f2fs_read_data_page(struct file *file, struct page *page)
 {
-	return mpage_readpage(page, get_data_block);
+	struct inode *inode = page->mapping->host;
+	int ret;
+
+	/* If the file has inline data, try to read it directlly */
+	if (f2fs_has_inline_data(inode))
+		ret = f2fs_read_inline_data(inode, page);
+	else
+		ret = mpage_readpage(page, get_data_block);
+
+	return ret;
 }
 
 static int f2fs_read_data_pages(struct file *file,
 			struct address_space *mapping,
 			struct list_head *pages, unsigned nr_pages)
 {
+	struct inode *inode = file->f_mapping->host;
+
+	/* If the file has inline data, skip readpages */
+	if (f2fs_has_inline_data(inode))
+		return 0;
+
 	return mpage_readpages(mapping, pages, nr_pages, get_data_block);
 }
 
@@ -761,7 +776,7 @@ static int f2fs_write_data_page(struct page *page,
 	loff_t i_size = i_size_read(inode);
 	const pgoff_t end_index = ((unsigned long long) i_size)
 							>> PAGE_CACHE_SHIFT;
-	unsigned offset;
+	unsigned offset = 0;
 	bool need_balance_fs = false;
 	int err = 0;
 	struct f2fs_io_info fio = {
@@ -799,7 +814,15 @@ static int f2fs_write_data_page(struct page *page,
 		err = do_write_data_page(page, &fio);
 	} else {
 		f2fs_lock_op(sbi);
-		err = do_write_data_page(page, &fio);
+
+		if (f2fs_has_inline_data(inode) || f2fs_may_inline(inode)) {
+			err = f2fs_write_inline_data(inode, page, offset);
+			f2fs_unlock_op(sbi);
+			goto out;
+		} else {
+			err = do_write_data_page(page, &fio);
+		}
+
 		f2fs_unlock_op(sbi);
 		need_balance_fs = true;
 	}
@@ -888,6 +911,15 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 		return -ENOMEM;
 	*pagep = page;
 
+	if ((pos + len) < MAX_INLINE_DATA) {
+		if (f2fs_has_inline_data(inode))
+			goto inline_data;
+	} else if (f2fs_has_inline_data(inode)) {
+		err = f2fs_convert_inline_data(inode, page, flags);
+		if (err)
+			return err;
+	}
+
 	f2fs_lock_op(sbi);
 	set_new_dnode(&dn, inode, NULL, NULL, 0);
 	err = f2fs_reserve_block(&dn, index);
@@ -897,7 +929,7 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 		f2fs_put_page(page, 1);
 		return err;
 	}
-
+inline_data:
 	if ((len == PAGE_CACHE_SIZE) || PageUptodate(page))
 		return 0;
 
@@ -913,7 +945,10 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 	if (dn.data_blkaddr == NEW_ADDR) {
 		zero_user_segment(page, 0, PAGE_CACHE_SIZE);
 	} else {
-		err = f2fs_submit_page_bio(sbi, page, dn.data_blkaddr,
+		if (f2fs_has_inline_data(inode))
+			err = f2fs_read_inline_data(inode, page);
+		else
+			err = f2fs_submit_page_bio(sbi, page, dn.data_blkaddr,
 							READ_SYNC);
 		if (err)
 			return err;
@@ -977,6 +1012,10 @@ static ssize_t f2fs_direct_IO(int rw, struct kiocb *iocb,
 	struct file *file = iocb->ki_filp;
 	struct inode *inode = file->f_mapping->host;
 
+	/* Let buffer I/O handle the inline data case. */
+	if (f2fs_has_inline_data(inode))
+		return 0;
+
 	if (check_direct_IO(inode, rw, iov, offset, nr_segs))
 		return 0;
 

commit 944fcfc18445b22d7ad1953a6effcfbdc4ffec74
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Thu Dec 26 20:15:09 2013 +0900

    f2fs: check the blocksize before calling generic_direct_IO path
    
    The f2fs supports 4KB block size. If user requests dwrite with under 4KB data,
    it allocates a new 4KB data block.
    However, f2fs doesn't add zero data into the untouched data area inside the
    newly allocated data block.
    
    This incurs an error during the xfstest #263 test as follow.
    
    263 12s ... [failed, exit status 1] - output mismatch (see 263.out.bad)
            --- 263.out     2013-03-09 03:37:15.043967603 +0900
            +++ 263.out.bad 2013-12-27 04:20:39.230203114 +0900
            @@ -1,3 +1,976 @@
            QA output created by 263
            fsx -N 10000 -o 8192 -l 500000 -r PSIZE -t BSIZE -w BSIZE -Z
            -fsx -N 10000 -o 128000 -l 500000 -r PSIZE -t BSIZE -w BSIZE -Z
            +fsx -N 10000 -o 8192 -l 500000 -r PSIZE -t BSIZE -w BSIZE -Z
            +truncating to largest ever: 0x12a00
            +truncating to largest ever: 0x75400
            +fallocating to largest ever: 0x79cbf
            ...
            (Run 'diff -u 263.out 263.out.bad' to see the entire diff)
            Ran: 263
            Failures: 263
            Failed 1 of 1 tests
    
    It turns out that, when the test tries to write 2KB data with dio, the new dio
    path allocates 4KB data block without filling zero data inside the remained 2KB
    area. Finally, the output file contains a garbage data for that region.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 991e36835df8..e0965b43d16e 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -953,11 +953,33 @@ static int f2fs_write_end(struct file *file,
 	return copied;
 }
 
+static int check_direct_IO(struct inode *inode, int rw,
+		const struct iovec *iov, loff_t offset, unsigned long nr_segs)
+{
+	unsigned blocksize_mask = inode->i_sb->s_blocksize - 1;
+	int i;
+
+	if (rw == READ)
+		return 0;
+
+	if (offset & blocksize_mask)
+		return -EINVAL;
+
+	for (i = 0; i < nr_segs; i++)
+		if (iov[i].iov_len & blocksize_mask)
+			return -EINVAL;
+	return 0;
+}
+
 static ssize_t f2fs_direct_IO(int rw, struct kiocb *iocb,
 		const struct iovec *iov, loff_t offset, unsigned long nr_segs)
 {
 	struct file *file = iocb->ki_filp;
 	struct inode *inode = file->f_mapping->host;
+
+	if (check_direct_IO(inode, rw, iov, offset, nr_segs))
+		return 0;
+
 	return blockdev_direct_IO(rw, iocb, inode, iov, offset, nr_segs,
 							get_data_block);
 }

commit 1ec79083b2d4614d9dbaea67b5f55b60d7137a2d
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Thu Dec 26 16:55:22 2013 +0900

    f2fs: should put the dnode when NEW_ADDR is detected
    
    When get_dnode_of_data() in get_data_block() returns a successful dnode, we
    should put the dnode.
    But, previously, if its data block address is equal to NEW_ADDR, we didn't do
    that, resulting in a deadlock condition.
    So, this patch splits original error conditions with this case, and then calls
    f2fs_put_dnode before finishing the function.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 0879d2aa97e6..991e36835df8 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -626,11 +626,13 @@ static int get_data_block(struct inode *inode, sector_t iblock,
 	/* When reading holes, we need its node page */
 	set_new_dnode(&dn, inode, NULL, NULL, 0);
 	err = get_dnode_of_data(&dn, pgofs, mode);
-	if (err || dn.data_blkaddr == NEW_ADDR) {
+	if (err) {
 		if (err == -ENOENT)
 			err = 0;
 		goto unlock_out;
 	}
+	if (dn.data_blkaddr == NEW_ADDR)
+		goto put_out;
 
 	if (dn.data_blkaddr != NULL_ADDR) {
 		map_bh(bh_result, inode->i_sb, dn.data_blkaddr);
@@ -659,11 +661,14 @@ static int get_data_block(struct inode *inode, sector_t iblock,
 
 		set_new_dnode(&dn, inode, NULL, NULL, 0);
 		err = get_dnode_of_data(&dn, pgofs, mode);
-		if (err || dn.data_blkaddr == NEW_ADDR) {
+		if (err) {
 			if (err == -ENOENT)
 				err = 0;
 			goto unlock_out;
 		}
+		if (dn.data_blkaddr == NEW_ADDR)
+			goto put_out;
+
 		end_offset = IS_INODE(dn.node_page) ?
 			ADDRS_PER_INODE(F2FS_I(inode)) : ADDRS_PER_BLOCK;
 	}

commit 4f4124d0b99682efa7307191a28ec050872d2079
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Sat Dec 21 18:02:14 2013 +0800

    f2fs: update several comments
    
    Update several comments:
    1. use f2fs_{un}lock_op install of mutex_{un}lock_op.
    2. update comment of get_data_block().
    3. update description of node offset.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index e46b5c52d2ed..0879d2aa97e6 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -508,8 +508,8 @@ struct page *get_lock_data_page(struct inode *inode, pgoff_t index)
  * Caller ensures that this data page is never allocated.
  * A new zero-filled data page is allocated in the page cache.
  *
- * Also, caller should grab and release a mutex by calling mutex_lock_op() and
- * mutex_unlock_op().
+ * Also, caller should grab and release a rwsem by calling f2fs_lock_op() and
+ * f2fs_unlock_op().
  * Note that, npage is set only by make_empty_dir.
  */
 struct page *get_new_data_page(struct inode *inode,
@@ -595,10 +595,12 @@ static int __allocate_data_block(struct dnode_of_data *dn)
 }
 
 /*
- * This function should be used by the data read flow only where it
- * does not check the "create" flag that indicates block allocation.
- * The reason for this special functionality is to exploit VFS readahead
- * mechanism.
+ * get_data_block() now supported readahead/bmap/rw direct_IO with mapped bh.
+ * If original data blocks are allocated, then give them to blockdev.
+ * Otherwise,
+ *     a. preallocate requested block addresses
+ *     b. do not use extent cache for better performance
+ *     c. give the block addresses to blockdev
  */
 static int get_data_block(struct inode *inode, sector_t iblock,
 			struct buffer_head *bh_result, int create)

commit 7e8f23081ab3a11de90d7389f2c6fd44676c8df9
Author: Gu Zheng <guz.fnst@cn.fujitsu.com>
Date:   Fri Dec 20 18:17:49 2013 +0800

    f2fs: remove the rw_flag domain from f2fs_io_info
    
    When using the f2fs_io_info in the low level, we still need to merge the
    rw and rw_flag, so use the rw to hold all the io flags directly,
    and remove the rw_flag field.
    
    ps.It is based on the previous patch:
    f2fs: move all the bio initialization into __bio_alloc
    
    Signed-off-by: Gu Zheng <guz.fnst@cn.fujitsu.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 154a4f93a548..e46b5c52d2ed 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -105,7 +105,7 @@ static void __submit_merged_bio(struct f2fs_bio_info *io)
 	if (!io->bio)
 		return;
 
-	rw = fio->rw | fio->rw_flag;
+	rw = fio->rw;
 
 	if (is_read_io(rw)) {
 		trace_f2fs_submit_read_bio(io->sbi->sb, rw,
@@ -760,7 +760,6 @@ static int f2fs_write_data_page(struct page *page,
 	struct f2fs_io_info fio = {
 		.type = DATA,
 		.rw = (wbc->sync_mode == WB_SYNC_ALL) ? WRITE_SYNC: WRITE,
-		.rw_flag = 0,
 	};
 
 	if (page->index < end_index)

commit 940a6d34b31b96f0748a4b688a551a0890b2b229
Author: Gu Zheng <guz.fnst@cn.fujitsu.com>
Date:   Fri Dec 20 17:39:59 2013 +0800

    f2fs: move all the bio initialization into __bio_alloc
    
    Move all the bio initialization into __bio_alloc, and some minor cleanups are
    also added.
    
    v3:
      Use 'bool' rather than 'int' as Kim suggested.
    
    v2:
      Use 'is_read' rather than 'rw' as Yu Chao suggested.
      Remove the needless initialization of bio->bi_private.
    
    Signed-off-by: Gu Zheng <guz.fnst@cn.fujitsu.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index a0950bcbf568..154a4f93a548 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -24,20 +24,6 @@
 #include "segment.h"
 #include <trace/events/f2fs.h>
 
-/*
- * Low-level block read/write IO operations.
- */
-static struct bio *__bio_alloc(struct block_device *bdev, int npages)
-{
-	struct bio *bio;
-
-	/* No failure on bio allocation */
-	bio = bio_alloc(GFP_NOIO, npages);
-	bio->bi_bdev = bdev;
-	bio->bi_private = NULL;
-	return bio;
-}
-
 static void f2fs_read_end_io(struct bio *bio, int err)
 {
 	const int uptodate = test_bit(BIO_UPTODATE, &bio->bi_flags);
@@ -93,6 +79,24 @@ static void f2fs_write_end_io(struct bio *bio, int err)
 	bio_put(bio);
 }
 
+/*
+ * Low-level block read/write IO operations.
+ */
+static struct bio *__bio_alloc(struct f2fs_sb_info *sbi, block_t blk_addr,
+				int npages, bool is_read)
+{
+	struct bio *bio;
+
+	/* No failure on bio allocation */
+	bio = bio_alloc(GFP_NOIO, npages);
+
+	bio->bi_bdev = sbi->sb->s_bdev;
+	bio->bi_sector = SECTOR_FROM_BLOCK(sbi, blk_addr);
+	bio->bi_end_io = is_read ? f2fs_read_end_io : f2fs_write_end_io;
+
+	return bio;
+}
+
 static void __submit_merged_bio(struct f2fs_bio_info *io)
 {
 	struct f2fs_io_info *fio = &io->fio;
@@ -104,25 +108,26 @@ static void __submit_merged_bio(struct f2fs_bio_info *io)
 	rw = fio->rw | fio->rw_flag;
 
 	if (is_read_io(rw)) {
-		trace_f2fs_submit_read_bio(io->sbi->sb, rw, fio->type, io->bio);
+		trace_f2fs_submit_read_bio(io->sbi->sb, rw,
+						fio->type, io->bio);
 		submit_bio(rw, io->bio);
-		io->bio = NULL;
-		return;
-	}
-	trace_f2fs_submit_write_bio(io->sbi->sb, rw, fio->type, io->bio);
-
-	/*
-	 * META_FLUSH is only from the checkpoint procedure, and we should wait
-	 * this metadata bio for FS consistency.
-	 */
-	if (fio->type == META_FLUSH) {
-		DECLARE_COMPLETION_ONSTACK(wait);
-		io->bio->bi_private = &wait;
-		submit_bio(rw, io->bio);
-		wait_for_completion(&wait);
 	} else {
-		submit_bio(rw, io->bio);
+		trace_f2fs_submit_write_bio(io->sbi->sb, rw,
+						fio->type, io->bio);
+		/*
+		 * META_FLUSH is only from the checkpoint procedure, and we
+		 * should wait this metadata bio for FS consistency.
+		 */
+		if (fio->type == META_FLUSH) {
+			DECLARE_COMPLETION_ONSTACK(wait);
+			io->bio->bi_private = &wait;
+			submit_bio(rw, io->bio);
+			wait_for_completion(&wait);
+		} else {
+			submit_bio(rw, io->bio);
+		}
 	}
+
 	io->bio = NULL;
 }
 
@@ -152,17 +157,12 @@ void f2fs_submit_merged_bio(struct f2fs_sb_info *sbi,
 int f2fs_submit_page_bio(struct f2fs_sb_info *sbi, struct page *page,
 					block_t blk_addr, int rw)
 {
-	struct block_device *bdev = sbi->sb->s_bdev;
 	struct bio *bio;
 
 	trace_f2fs_submit_page_bio(page, blk_addr, rw);
 
 	/* Allocate a new bio */
-	bio = __bio_alloc(bdev, 1);
-
-	/* Initialize the bio */
-	bio->bi_sector = SECTOR_FROM_BLOCK(sbi, blk_addr);
-	bio->bi_end_io = is_read_io(rw) ? f2fs_read_end_io : f2fs_write_end_io;
+	bio = __bio_alloc(sbi, blk_addr, 1, is_read_io(rw));
 
 	if (bio_add_page(bio, page, PAGE_CACHE_SIZE, 0) < PAGE_CACHE_SIZE) {
 		bio_put(bio);
@@ -178,17 +178,16 @@ void f2fs_submit_page_mbio(struct f2fs_sb_info *sbi, struct page *page,
 			block_t blk_addr, struct f2fs_io_info *fio)
 {
 	enum page_type btype = PAGE_TYPE_OF_BIO(fio->type);
-	struct block_device *bdev = sbi->sb->s_bdev;
 	struct f2fs_bio_info *io;
-	int bio_blocks;
+	bool is_read = is_read_io(fio->rw);
 
-	io = is_read_io(fio->rw) ? &sbi->read_io : &sbi->write_io[btype];
+	io = is_read ? &sbi->read_io : &sbi->write_io[btype];
 
 	verify_block_addr(sbi, blk_addr);
 
 	mutex_lock(&io->io_mutex);
 
-	if (!is_read_io(fio->rw))
+	if (!is_read)
 		inc_page_count(sbi, F2FS_WRITEBACK);
 
 	if (io->bio && (io->last_block_in_bio != blk_addr - 1 ||
@@ -196,17 +195,10 @@ void f2fs_submit_page_mbio(struct f2fs_sb_info *sbi, struct page *page,
 		__submit_merged_bio(io);
 alloc_new:
 	if (io->bio == NULL) {
-		bio_blocks = MAX_BIO_BLOCKS(max_hw_blocks(sbi));
-		io->bio = __bio_alloc(bdev, bio_blocks);
-		io->bio->bi_sector = SECTOR_FROM_BLOCK(sbi, blk_addr);
-		io->bio->bi_end_io = is_read_io(fio->rw) ? f2fs_read_end_io :
-							f2fs_write_end_io;
+		int bio_blocks = MAX_BIO_BLOCKS(max_hw_blocks(sbi));
+
+		io->bio = __bio_alloc(sbi, blk_addr, bio_blocks, is_read);
 		io->fio = *fio;
-		/*
-		 * The end_io will be assigned at the sumbission phase.
-		 * Until then, let bio_add_page() merge consecutive IOs as much
-		 * as possible.
-		 */
 	}
 
 	if (bio_add_page(io->bio, page, PAGE_CACHE_SIZE, 0) <

commit bfad7c2d40332be6a1d7a89660bceb0f6ea1d73a
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Mon Dec 16 19:04:05 2013 +0900

    f2fs: introduce a new direct_IO write path
    
    Previously, f2fs doesn't support direct IOs with high performance, which throws
    every write requests via the buffered write path, resulting in highly
    performance degradation due to memory opeations like copy_from_user.
    
    This patch introduces a new direct IO path in which every write requests are
    processed by generic blockdev_direct_IO() with enhanced get_block function.
    
    The get_data_block() in f2fs handles:
    1. if original data blocks are allocates, then give them to blockdev.
    2. otherwise,
      a. preallocate requested block addresses
      b. do not use extent cache for better performance
      c. give the block addresses to blockdev
    
    This policy induces that:
    - new allocated data are sequentially written to the disk
    - updated data are randomly written to the disk.
    - f2fs gives consistency on its file meta, not file data.
    
    Reviewed-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 15956fa584de..a0950bcbf568 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -570,74 +570,151 @@ struct page *get_new_data_page(struct inode *inode,
 	return page;
 }
 
+static int __allocate_data_block(struct dnode_of_data *dn)
+{
+	struct f2fs_sb_info *sbi = F2FS_SB(dn->inode->i_sb);
+	struct f2fs_summary sum;
+	block_t new_blkaddr;
+	struct node_info ni;
+	int type;
+
+	if (unlikely(is_inode_flag_set(F2FS_I(dn->inode), FI_NO_ALLOC)))
+		return -EPERM;
+	if (unlikely(!inc_valid_block_count(sbi, dn->inode, 1)))
+		return -ENOSPC;
+
+	__set_data_blkaddr(dn, NEW_ADDR);
+	dn->data_blkaddr = NEW_ADDR;
+
+	get_node_info(sbi, dn->nid, &ni);
+	set_summary(&sum, dn->nid, dn->ofs_in_node, ni.version);
+
+	type = CURSEG_WARM_DATA;
+
+	allocate_data_block(sbi, NULL, NULL_ADDR, &new_blkaddr, &sum, type);
+
+	/* direct IO doesn't use extent cache to maximize the performance */
+	set_inode_flag(F2FS_I(dn->inode), FI_NO_EXTENT);
+	update_extent_cache(new_blkaddr, dn);
+	clear_inode_flag(F2FS_I(dn->inode), FI_NO_EXTENT);
+
+	dn->data_blkaddr = new_blkaddr;
+	return 0;
+}
+
 /*
  * This function should be used by the data read flow only where it
  * does not check the "create" flag that indicates block allocation.
  * The reason for this special functionality is to exploit VFS readahead
  * mechanism.
  */
-static int get_data_block_ro(struct inode *inode, sector_t iblock,
+static int get_data_block(struct inode *inode, sector_t iblock,
 			struct buffer_head *bh_result, int create)
 {
+	struct f2fs_sb_info *sbi = F2FS_SB(inode->i_sb);
 	unsigned int blkbits = inode->i_sb->s_blocksize_bits;
 	unsigned maxblocks = bh_result->b_size >> blkbits;
 	struct dnode_of_data dn;
-	pgoff_t pgofs;
-	int err;
+	int mode = create ? ALLOC_NODE : LOOKUP_NODE_RA;
+	pgoff_t pgofs, end_offset;
+	int err = 0, ofs = 1;
+	bool allocated = false;
 
 	/* Get the page offset from the block offset(iblock) */
 	pgofs =	(pgoff_t)(iblock >> (PAGE_CACHE_SHIFT - blkbits));
 
-	if (check_extent_cache(inode, pgofs, bh_result)) {
-		trace_f2fs_get_data_block(inode, iblock, bh_result, 0);
-		return 0;
-	}
+	if (check_extent_cache(inode, pgofs, bh_result))
+		goto out;
+
+	if (create)
+		f2fs_lock_op(sbi);
 
 	/* When reading holes, we need its node page */
 	set_new_dnode(&dn, inode, NULL, NULL, 0);
-	err = get_dnode_of_data(&dn, pgofs, LOOKUP_NODE_RA);
-	if (err) {
-		trace_f2fs_get_data_block(inode, iblock, bh_result, err);
-		return (err == -ENOENT) ? 0 : err;
+	err = get_dnode_of_data(&dn, pgofs, mode);
+	if (err || dn.data_blkaddr == NEW_ADDR) {
+		if (err == -ENOENT)
+			err = 0;
+		goto unlock_out;
 	}
 
-	/* It does not support data allocation */
-	f2fs_bug_on(create);
-
-	if (dn.data_blkaddr != NEW_ADDR && dn.data_blkaddr != NULL_ADDR) {
-		int i;
-		unsigned int end_offset;
-
+	if (dn.data_blkaddr != NULL_ADDR) {
+		map_bh(bh_result, inode->i_sb, dn.data_blkaddr);
+	} else if (create) {
+		err = __allocate_data_block(&dn);
+		if (err)
+			goto put_out;
+		allocated = true;
+		map_bh(bh_result, inode->i_sb, dn.data_blkaddr);
+	} else {
+		goto put_out;
+	}
+
+	end_offset = IS_INODE(dn.node_page) ?
+			ADDRS_PER_INODE(F2FS_I(inode)) : ADDRS_PER_BLOCK;
+	bh_result->b_size = (((size_t)1) << blkbits);
+	dn.ofs_in_node++;
+	pgofs++;
+
+get_next:
+	if (dn.ofs_in_node >= end_offset) {
+		if (allocated)
+			sync_inode_page(&dn);
+		allocated = false;
+		f2fs_put_dnode(&dn);
+
+		set_new_dnode(&dn, inode, NULL, NULL, 0);
+		err = get_dnode_of_data(&dn, pgofs, mode);
+		if (err || dn.data_blkaddr == NEW_ADDR) {
+			if (err == -ENOENT)
+				err = 0;
+			goto unlock_out;
+		}
 		end_offset = IS_INODE(dn.node_page) ?
-				ADDRS_PER_INODE(F2FS_I(inode)) :
-				ADDRS_PER_BLOCK;
-
-		clear_buffer_new(bh_result);
+			ADDRS_PER_INODE(F2FS_I(inode)) : ADDRS_PER_BLOCK;
+	}
 
+	if (maxblocks > (bh_result->b_size >> blkbits)) {
+		block_t blkaddr = datablock_addr(dn.node_page, dn.ofs_in_node);
+		if (blkaddr == NULL_ADDR && create) {
+			err = __allocate_data_block(&dn);
+			if (err)
+				goto sync_out;
+			allocated = true;
+			blkaddr = dn.data_blkaddr;
+		}
 		/* Give more consecutive addresses for the read ahead */
-		for (i = 0; i < end_offset - dn.ofs_in_node; i++)
-			if (((datablock_addr(dn.node_page,
-							dn.ofs_in_node + i))
-				!= (dn.data_blkaddr + i)) || maxblocks == i)
-				break;
-		map_bh(bh_result, inode->i_sb, dn.data_blkaddr);
-		bh_result->b_size = (((size_t)i) << blkbits);
+		if (blkaddr == (bh_result->b_blocknr + ofs)) {
+			ofs++;
+			dn.ofs_in_node++;
+			pgofs++;
+			bh_result->b_size += (((size_t)1) << blkbits);
+			goto get_next;
+		}
 	}
+sync_out:
+	if (allocated)
+		sync_inode_page(&dn);
+put_out:
 	f2fs_put_dnode(&dn);
-	trace_f2fs_get_data_block(inode, iblock, bh_result, 0);
-	return 0;
+unlock_out:
+	if (create)
+		f2fs_unlock_op(sbi);
+out:
+	trace_f2fs_get_data_block(inode, iblock, bh_result, err);
+	return err;
 }
 
 static int f2fs_read_data_page(struct file *file, struct page *page)
 {
-	return mpage_readpage(page, get_data_block_ro);
+	return mpage_readpage(page, get_data_block);
 }
 
 static int f2fs_read_data_pages(struct file *file,
 			struct address_space *mapping,
 			struct list_head *pages, unsigned nr_pages)
 {
-	return mpage_readpages(mapping, pages, nr_pages, get_data_block_ro);
+	return mpage_readpages(mapping, pages, nr_pages, get_data_block);
 }
 
 int do_write_data_page(struct page *page, struct f2fs_io_info *fio)
@@ -883,13 +960,8 @@ static ssize_t f2fs_direct_IO(int rw, struct kiocb *iocb,
 {
 	struct file *file = iocb->ki_filp;
 	struct inode *inode = file->f_mapping->host;
-
-	if (rw == WRITE)
-		return 0;
-
-	/* Needs synchronization with the cleaner */
 	return blockdev_direct_IO(rw, iocb, inode, iov, offset, nr_segs,
-						  get_data_block_ro);
+							get_data_block);
 }
 
 static void f2fs_invalidate_data_page(struct page *page, unsigned int offset,
@@ -928,7 +1000,7 @@ static int f2fs_set_data_page_dirty(struct page *page)
 
 static sector_t f2fs_bmap(struct address_space *mapping, sector_t block)
 {
-	return generic_block_bmap(mapping, block, get_data_block_ro);
+	return generic_block_bmap(mapping, block, get_data_block);
 }
 
 const struct address_space_operations f2fs_dblock_aops = {

commit 76130ccabcc39f0716691113f739f28a3088e253
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Wed Dec 11 14:29:39 2013 +0900

    f2fs: fix the location of tracepoint
    
    We need to get a trace before submit_bio, since its bi_sector is remapped during
    the submit_bio.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index ebc91778e815..15956fa584de 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -104,11 +104,12 @@ static void __submit_merged_bio(struct f2fs_bio_info *io)
 	rw = fio->rw | fio->rw_flag;
 
 	if (is_read_io(rw)) {
-		submit_bio(rw, io->bio);
 		trace_f2fs_submit_read_bio(io->sbi->sb, rw, fio->type, io->bio);
+		submit_bio(rw, io->bio);
 		io->bio = NULL;
 		return;
 	}
+	trace_f2fs_submit_write_bio(io->sbi->sb, rw, fio->type, io->bio);
 
 	/*
 	 * META_FLUSH is only from the checkpoint procedure, and we should wait
@@ -122,7 +123,6 @@ static void __submit_merged_bio(struct f2fs_bio_info *io)
 	} else {
 		submit_bio(rw, io->bio);
 	}
-	trace_f2fs_submit_write_bio(io->sbi->sb, rw, fio->type, io->bio);
 	io->bio = NULL;
 }
 

commit 458e6197c37de53f7be0a837644daabb900c3036
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Wed Dec 11 13:54:01 2013 +0900

    f2fs: refactor bio->rw handling
    
    This patch introduces f2fs_io_info to mitigate the complex parameter list.
    
    struct f2fs_io_info {
            enum page_type type;            /* contains DATA/NODE/META/META_FLUSH */
            int rw;                         /* contains R/RS/W/WS */
            int rw_flag;                    /* contains REQ_META/REQ_PRIO */
    }
    
    1. f2fs_write_data_pages
     - DATA
     - WRITE_SYNC is set when wbc->WB_SYNC_ALL.
    
    2. sync_node_pages
     - NODE
     - WRITE_SYNC all the time
    
    3. sync_meta_pages
     - META
     - WRITE_SYNC all the time
     - REQ_META | REQ_PRIO all the time
    
     ** f2fs_submit_merged_bio() handles META_FLUSH.
    
    4. ra_nat_pages, ra_sit_pages, ra_sum_pages
     - META
     - READ_SYNC
    
    Cc: Fan Li <fanofcode.li@samsung.com>
    Cc: Changman Lee <cm224.lee@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index fb5e5c2627e5..ebc91778e815 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -93,37 +93,28 @@ static void f2fs_write_end_io(struct bio *bio, int err)
 	bio_put(bio);
 }
 
-static void __submit_merged_bio(struct f2fs_sb_info *sbi,
-				struct f2fs_bio_info *io,
-				enum page_type type, bool sync, int rw)
+static void __submit_merged_bio(struct f2fs_bio_info *io)
 {
-	enum page_type btype = PAGE_TYPE_OF_BIO(type);
+	struct f2fs_io_info *fio = &io->fio;
+	int rw;
 
 	if (!io->bio)
 		return;
 
-	if (btype == META)
-		rw |= REQ_META;
+	rw = fio->rw | fio->rw_flag;
 
 	if (is_read_io(rw)) {
-		if (sync)
-			rw |= READ_SYNC;
 		submit_bio(rw, io->bio);
-		trace_f2fs_submit_read_bio(sbi->sb, rw, type, io->bio);
+		trace_f2fs_submit_read_bio(io->sbi->sb, rw, fio->type, io->bio);
 		io->bio = NULL;
 		return;
 	}
 
-	if (sync)
-		rw |= WRITE_SYNC;
-	if (type >= META_FLUSH)
-		rw |= WRITE_FLUSH_FUA;
-
 	/*
 	 * META_FLUSH is only from the checkpoint procedure, and we should wait
 	 * this metadata bio for FS consistency.
 	 */
-	if (type == META_FLUSH) {
+	if (fio->type == META_FLUSH) {
 		DECLARE_COMPLETION_ONSTACK(wait);
 		io->bio->bi_private = &wait;
 		submit_bio(rw, io->bio);
@@ -131,12 +122,12 @@ static void __submit_merged_bio(struct f2fs_sb_info *sbi,
 	} else {
 		submit_bio(rw, io->bio);
 	}
-	trace_f2fs_submit_write_bio(sbi->sb, rw, btype, io->bio);
+	trace_f2fs_submit_write_bio(io->sbi->sb, rw, fio->type, io->bio);
 	io->bio = NULL;
 }
 
 void f2fs_submit_merged_bio(struct f2fs_sb_info *sbi,
-				enum page_type type, bool sync, int rw)
+				enum page_type type, int rw)
 {
 	enum page_type btype = PAGE_TYPE_OF_BIO(type);
 	struct f2fs_bio_info *io;
@@ -144,7 +135,13 @@ void f2fs_submit_merged_bio(struct f2fs_sb_info *sbi,
 	io = is_read_io(rw) ? &sbi->read_io : &sbi->write_io[btype];
 
 	mutex_lock(&io->io_mutex);
-	__submit_merged_bio(sbi, io, type, sync, rw);
+
+	/* change META to META_FLUSH in the checkpoint procedure */
+	if (type >= META_FLUSH) {
+		io->fio.type = META_FLUSH;
+		io->fio.rw = WRITE_FLUSH_FUA;
+	}
+	__submit_merged_bio(io);
 	mutex_unlock(&io->io_mutex);
 }
 
@@ -178,33 +175,33 @@ int f2fs_submit_page_bio(struct f2fs_sb_info *sbi, struct page *page,
 }
 
 void f2fs_submit_page_mbio(struct f2fs_sb_info *sbi, struct page *page,
-			block_t blk_addr, enum page_type type, int rw)
+			block_t blk_addr, struct f2fs_io_info *fio)
 {
-	enum page_type btype = PAGE_TYPE_OF_BIO(type);
+	enum page_type btype = PAGE_TYPE_OF_BIO(fio->type);
 	struct block_device *bdev = sbi->sb->s_bdev;
 	struct f2fs_bio_info *io;
 	int bio_blocks;
 
-	io = is_read_io(rw) ? &sbi->read_io : &sbi->write_io[btype];
+	io = is_read_io(fio->rw) ? &sbi->read_io : &sbi->write_io[btype];
 
 	verify_block_addr(sbi, blk_addr);
 
 	mutex_lock(&io->io_mutex);
 
-	if (!is_read_io(rw))
+	if (!is_read_io(fio->rw))
 		inc_page_count(sbi, F2FS_WRITEBACK);
 
 	if (io->bio && (io->last_block_in_bio != blk_addr - 1 ||
-						io->rw_flag != rw))
-		__submit_merged_bio(sbi, io, type, false, io->rw_flag);
+						io->fio.rw != fio->rw))
+		__submit_merged_bio(io);
 alloc_new:
 	if (io->bio == NULL) {
 		bio_blocks = MAX_BIO_BLOCKS(max_hw_blocks(sbi));
 		io->bio = __bio_alloc(bdev, bio_blocks);
 		io->bio->bi_sector = SECTOR_FROM_BLOCK(sbi, blk_addr);
-		io->bio->bi_end_io = is_read_io(rw) ? f2fs_read_end_io :
+		io->bio->bi_end_io = is_read_io(fio->rw) ? f2fs_read_end_io :
 							f2fs_write_end_io;
-		io->rw_flag = rw;
+		io->fio = *fio;
 		/*
 		 * The end_io will be assigned at the sumbission phase.
 		 * Until then, let bio_add_page() merge consecutive IOs as much
@@ -214,14 +211,14 @@ void f2fs_submit_page_mbio(struct f2fs_sb_info *sbi, struct page *page,
 
 	if (bio_add_page(io->bio, page, PAGE_CACHE_SIZE, 0) <
 							PAGE_CACHE_SIZE) {
-		__submit_merged_bio(sbi, io, type, false, rw);
+		__submit_merged_bio(io);
 		goto alloc_new;
 	}
 
 	io->last_block_in_bio = blk_addr;
 
 	mutex_unlock(&io->io_mutex);
-	trace_f2fs_submit_page_mbio(page, rw, type, blk_addr);
+	trace_f2fs_submit_page_mbio(page, fio->rw, fio->type, blk_addr);
 }
 
 /*
@@ -643,10 +640,10 @@ static int f2fs_read_data_pages(struct file *file,
 	return mpage_readpages(mapping, pages, nr_pages, get_data_block_ro);
 }
 
-int do_write_data_page(struct page *page, struct writeback_control *wbc)
+int do_write_data_page(struct page *page, struct f2fs_io_info *fio)
 {
 	struct inode *inode = page->mapping->host;
-	block_t old_blk_addr, new_blk_addr;
+	block_t old_blkaddr, new_blkaddr;
 	struct dnode_of_data dn;
 	int err = 0;
 
@@ -655,10 +652,10 @@ int do_write_data_page(struct page *page, struct writeback_control *wbc)
 	if (err)
 		return err;
 
-	old_blk_addr = dn.data_blkaddr;
+	old_blkaddr = dn.data_blkaddr;
 
 	/* This page is already truncated */
-	if (old_blk_addr == NULL_ADDR)
+	if (old_blkaddr == NULL_ADDR)
 		goto out_writepage;
 
 	set_page_writeback(page);
@@ -667,15 +664,13 @@ int do_write_data_page(struct page *page, struct writeback_control *wbc)
 	 * If current allocation needs SSR,
 	 * it had better in-place writes for updated data.
 	 */
-	if (unlikely(old_blk_addr != NEW_ADDR &&
+	if (unlikely(old_blkaddr != NEW_ADDR &&
 			!is_cold_data(page) &&
 			need_inplace_update(inode))) {
-		rewrite_data_page(F2FS_SB(inode->i_sb), page,
-						old_blk_addr, wbc);
+		rewrite_data_page(page, old_blkaddr, fio);
 	} else {
-		write_data_page(inode, page, &dn,
-				old_blk_addr, &new_blk_addr, wbc);
-		update_extent_cache(new_blk_addr, &dn);
+		write_data_page(page, &dn, &new_blkaddr, fio);
+		update_extent_cache(new_blkaddr, &dn);
 	}
 out_writepage:
 	f2fs_put_dnode(&dn);
@@ -693,6 +688,11 @@ static int f2fs_write_data_page(struct page *page,
 	unsigned offset;
 	bool need_balance_fs = false;
 	int err = 0;
+	struct f2fs_io_info fio = {
+		.type = DATA,
+		.rw = (wbc->sync_mode == WB_SYNC_ALL) ? WRITE_SYNC: WRITE,
+		.rw_flag = 0,
+	};
 
 	if (page->index < end_index)
 		goto write;
@@ -721,10 +721,10 @@ static int f2fs_write_data_page(struct page *page,
 	if (S_ISDIR(inode->i_mode)) {
 		dec_page_count(sbi, F2FS_DIRTY_DENTS);
 		inode_dec_dirty_dents(inode);
-		err = do_write_data_page(page, wbc);
+		err = do_write_data_page(page, &fio);
 	} else {
 		f2fs_lock_op(sbi);
-		err = do_write_data_page(page, wbc);
+		err = do_write_data_page(page, &fio);
 		f2fs_unlock_op(sbi);
 		need_balance_fs = true;
 	}
@@ -734,7 +734,7 @@ static int f2fs_write_data_page(struct page *page,
 		goto redirty_out;
 
 	if (wbc->for_reclaim)
-		f2fs_submit_merged_bio(sbi, DATA, true, WRITE);
+		f2fs_submit_merged_bio(sbi, DATA, WRITE);
 
 	clear_cold_data(page);
 out:
@@ -786,7 +786,8 @@ static int f2fs_write_data_pages(struct address_space *mapping,
 	ret = write_cache_pages(mapping, wbc, __f2fs_writepage, mapping);
 	if (locked)
 		mutex_unlock(&sbi->writepages);
-	f2fs_submit_merged_bio(sbi, DATA, wbc->sync_mode == WB_SYNC_ALL, WRITE);
+
+	f2fs_submit_merged_bio(sbi, DATA, WRITE);
 
 	remove_dirty_dir_inode(inode);
 

commit 63a0b7cb33d85aeb0df39b984c08e234db4925d1
Author: Fan Li <fanofcode.li@samsung.com>
Date:   Mon Dec 9 16:09:00 2013 +0800

    f2fs: merge pages with the same sync_mode flag
    
    Previously f2fs submits most of write requests using WRITE_SYNC, but f2fs_write_data_pages
    submits last write requests by sync_mode flags callers pass.
    
    This causes a performance problem since continuous pages with different sync flags
    can't be merged in cfq IO scheduler(thanks yu chao for pointing it out), and synchronous
    requests often take more time.
    
    This patch makes the following modifies to DATA writebacks:
    
    1. every page will be written back using the sync mode caller pass.
    2. only pages with the same sync mode can be merged in one bio request.
    
    These changes are restricted to DATA pages.Other types of writebacks are modified
    To remain synchronous.
    
    In my test with tiotest, f2fs sequence write performance is improved by about 7%-10% ,
    and this patch has no obvious impact on other performance tests.
    
    Signed-off-by: Fan Li <fanofcode.li@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 5607393198df..fb5e5c2627e5 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -194,8 +194,9 @@ void f2fs_submit_page_mbio(struct f2fs_sb_info *sbi, struct page *page,
 	if (!is_read_io(rw))
 		inc_page_count(sbi, F2FS_WRITEBACK);
 
-	if (io->bio && io->last_block_in_bio != blk_addr - 1)
-		__submit_merged_bio(sbi, io, type, true, rw);
+	if (io->bio && (io->last_block_in_bio != blk_addr - 1 ||
+						io->rw_flag != rw))
+		__submit_merged_bio(sbi, io, type, false, io->rw_flag);
 alloc_new:
 	if (io->bio == NULL) {
 		bio_blocks = MAX_BIO_BLOCKS(max_hw_blocks(sbi));
@@ -203,6 +204,7 @@ void f2fs_submit_page_mbio(struct f2fs_sb_info *sbi, struct page *page,
 		io->bio->bi_sector = SECTOR_FROM_BLOCK(sbi, blk_addr);
 		io->bio->bi_end_io = is_read_io(rw) ? f2fs_read_end_io :
 							f2fs_write_end_io;
+		io->rw_flag = rw;
 		/*
 		 * The end_io will be assigned at the sumbission phase.
 		 * Until then, let bio_add_page() merge consecutive IOs as much
@@ -212,7 +214,7 @@ void f2fs_submit_page_mbio(struct f2fs_sb_info *sbi, struct page *page,
 
 	if (bio_add_page(io->bio, page, PAGE_CACHE_SIZE, 0) <
 							PAGE_CACHE_SIZE) {
-		__submit_merged_bio(sbi, io, type, true, rw);
+		__submit_merged_bio(sbi, io, type, false, rw);
 		goto alloc_new;
 	}
 
@@ -641,7 +643,7 @@ static int f2fs_read_data_pages(struct file *file,
 	return mpage_readpages(mapping, pages, nr_pages, get_data_block_ro);
 }
 
-int do_write_data_page(struct page *page)
+int do_write_data_page(struct page *page, struct writeback_control *wbc)
 {
 	struct inode *inode = page->mapping->host;
 	block_t old_blk_addr, new_blk_addr;
@@ -669,10 +671,10 @@ int do_write_data_page(struct page *page)
 			!is_cold_data(page) &&
 			need_inplace_update(inode))) {
 		rewrite_data_page(F2FS_SB(inode->i_sb), page,
-						old_blk_addr);
+						old_blk_addr, wbc);
 	} else {
 		write_data_page(inode, page, &dn,
-				old_blk_addr, &new_blk_addr);
+				old_blk_addr, &new_blk_addr, wbc);
 		update_extent_cache(new_blk_addr, &dn);
 	}
 out_writepage:
@@ -719,10 +721,10 @@ static int f2fs_write_data_page(struct page *page,
 	if (S_ISDIR(inode->i_mode)) {
 		dec_page_count(sbi, F2FS_DIRTY_DENTS);
 		inode_dec_dirty_dents(inode);
-		err = do_write_data_page(page);
+		err = do_write_data_page(page, wbc);
 	} else {
 		f2fs_lock_op(sbi);
-		err = do_write_data_page(page);
+		err = do_write_data_page(page, wbc);
 		f2fs_unlock_op(sbi);
 		need_balance_fs = true;
 	}

commit 6bacf52fb58aeb3e89d9a62970b85a5570aa8ace
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Fri Dec 6 15:00:58 2013 +0900

    f2fs: add unlikely() macro for compiler more aggressively
    
    This patch adds unlikely() macro into the most of codes.
    The basic rule is to add that when:
    - checking unusual errors,
    - checking page mappings,
    - and the other unlikely conditions.
    
    Change log from v1:
     - Don't add unlikely for the NULL test and error test: advised by Andi Kleen.
    
    Cc: Chao Yu <chao2.yu@samsung.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Reviewed-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 2ce5a9ef508b..5607393198df 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -49,11 +49,11 @@ static void f2fs_read_end_io(struct bio *bio, int err)
 		if (--bvec >= bio->bi_io_vec)
 			prefetchw(&bvec->bv_page->flags);
 
-		if (uptodate) {
-			SetPageUptodate(page);
-		} else {
+		if (unlikely(!uptodate)) {
 			ClearPageUptodate(page);
 			SetPageError(page);
+		} else {
+			SetPageUptodate(page);
 		}
 		unlock_page(page);
 	} while (bvec >= bio->bi_io_vec);
@@ -73,7 +73,7 @@ static void f2fs_write_end_io(struct bio *bio, int err)
 		if (--bvec >= bio->bi_io_vec)
 			prefetchw(&bvec->bv_page->flags);
 
-		if (!uptodate) {
+		if (unlikely(!uptodate)) {
 			SetPageError(page);
 			set_bit(AS_EIO, &page->mapping->flags);
 			set_ckpt_flags(sbi->ckpt, CP_ERROR_FLAG);
@@ -249,7 +249,7 @@ int reserve_new_block(struct dnode_of_data *dn)
 {
 	struct f2fs_sb_info *sbi = F2FS_SB(dn->inode->i_sb);
 
-	if (is_inode_flag_set(F2FS_I(dn->inode), FI_NO_ALLOC))
+	if (unlikely(is_inode_flag_set(F2FS_I(dn->inode), FI_NO_ALLOC)))
 		return -EPERM;
 	if (unlikely(!inc_valid_block_count(sbi, dn->inode, 1)))
 		return -ENOSPC;
@@ -424,7 +424,7 @@ struct page *find_data_page(struct inode *inode, pgoff_t index, bool sync)
 		return ERR_PTR(-ENOENT);
 
 	/* By fallocate(), there is no cached page, but with NEW_ADDR */
-	if (dn.data_blkaddr == NEW_ADDR)
+	if (unlikely(dn.data_blkaddr == NEW_ADDR))
 		return ERR_PTR(-EINVAL);
 
 	page = grab_cache_page_write_begin(mapping, index, AOP_FLAG_NOFS);
@@ -443,7 +443,7 @@ struct page *find_data_page(struct inode *inode, pgoff_t index, bool sync)
 
 	if (sync) {
 		wait_on_page_locked(page);
-		if (!PageUptodate(page)) {
+		if (unlikely(!PageUptodate(page))) {
 			f2fs_put_page(page, 0);
 			return ERR_PTR(-EIO);
 		}
@@ -477,7 +477,7 @@ struct page *get_lock_data_page(struct inode *inode, pgoff_t index)
 	}
 	f2fs_put_dnode(&dn);
 
-	if (dn.data_blkaddr == NULL_ADDR) {
+	if (unlikely(dn.data_blkaddr == NULL_ADDR)) {
 		f2fs_put_page(page, 1);
 		return ERR_PTR(-ENOENT);
 	}
@@ -502,11 +502,11 @@ struct page *get_lock_data_page(struct inode *inode, pgoff_t index)
 		return ERR_PTR(err);
 
 	lock_page(page);
-	if (!PageUptodate(page)) {
+	if (unlikely(!PageUptodate(page))) {
 		f2fs_put_page(page, 1);
 		return ERR_PTR(-EIO);
 	}
-	if (page->mapping != mapping) {
+	if (unlikely(page->mapping != mapping)) {
 		f2fs_put_page(page, 1);
 		goto repeat;
 	}
@@ -534,7 +534,6 @@ struct page *get_new_data_page(struct inode *inode,
 	err = f2fs_reserve_block(&dn, index);
 	if (err)
 		return ERR_PTR(err);
-
 repeat:
 	page = grab_cache_page(mapping, index);
 	if (!page)
@@ -552,11 +551,11 @@ struct page *get_new_data_page(struct inode *inode,
 		if (err)
 			return ERR_PTR(err);
 		lock_page(page);
-		if (!PageUptodate(page)) {
+		if (unlikely(!PageUptodate(page))) {
 			f2fs_put_page(page, 1);
 			return ERR_PTR(-EIO);
 		}
-		if (page->mapping != mapping) {
+		if (unlikely(page->mapping != mapping)) {
 			f2fs_put_page(page, 1);
 			goto repeat;
 		}
@@ -841,11 +840,11 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 		if (err)
 			return err;
 		lock_page(page);
-		if (!PageUptodate(page)) {
+		if (unlikely(!PageUptodate(page))) {
 			f2fs_put_page(page, 1);
 			return -EIO;
 		}
-		if (page->mapping != mapping) {
+		if (unlikely(page->mapping != mapping)) {
 			f2fs_put_page(page, 1);
 			goto repeat;
 		}

commit cfb271d485d0ec31eb92b51f4fbe54bf6542e8e6
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Thu Dec 5 17:15:22 2013 +0800

    f2fs: add unlikely() macro for compiler optimization
    
    As we know, some of our branch condition will rarely be true. So we could add
    'unlikely' to let compiler optimize these code, by this way we could drop
    unneeded 'jump' assemble code to improve performance.
    
    change log:
     o add *unlikely* as many as possible across the whole source files at once
       suggested by Jaegeuk Kim.
    
    Suggested-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 4e2fc09f0e4f..2ce5a9ef508b 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -251,7 +251,7 @@ int reserve_new_block(struct dnode_of_data *dn)
 
 	if (is_inode_flag_set(F2FS_I(dn->inode), FI_NO_ALLOC))
 		return -EPERM;
-	if (!inc_valid_block_count(sbi, dn->inode, 1))
+	if (unlikely(!inc_valid_block_count(sbi, dn->inode, 1)))
 		return -ENOSPC;
 
 	trace_f2fs_reserve_new_block(dn->inode, dn->nid, dn->ofs_in_node);
@@ -711,7 +711,7 @@ static int f2fs_write_data_page(struct page *page,
 
 	zero_user_segment(page, offset, PAGE_CACHE_SIZE);
 write:
-	if (sbi->por_doing) {
+	if (unlikely(sbi->por_doing)) {
 		err = AOP_WRITEPAGE_ACTIVATE;
 		goto redirty_out;
 	}

commit 93dfe2ac516250755f7d5edd438b0ce67c0e3aa6
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Sat Nov 30 12:51:14 2013 +0900

    f2fs: refactor bio-related operations
    
    This patch integrates redundant bio operations on read and write IOs.
    
    1. Move bio-related codes to the top of data.c.
    2. Replace f2fs_submit_bio with f2fs_submit_merged_bio, which handles read
       bios additionally.
    3. Introduce __submit_merged_bio to submit the merged bio.
    4. Change f2fs_readpage to f2fs_submit_page_bio.
    5. Introduce f2fs_submit_page_mbio to integrate previous submit_read_page and
       submit_write_page.
    
    Reviewed-by: Gu Zheng <guz.fnst@cn.fujitsu.com>
    Reviewed-by: Chao Yu <chao2.yu@samsung.com >
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index c9a76f8c1028..4e2fc09f0e4f 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -24,6 +24,204 @@
 #include "segment.h"
 #include <trace/events/f2fs.h>
 
+/*
+ * Low-level block read/write IO operations.
+ */
+static struct bio *__bio_alloc(struct block_device *bdev, int npages)
+{
+	struct bio *bio;
+
+	/* No failure on bio allocation */
+	bio = bio_alloc(GFP_NOIO, npages);
+	bio->bi_bdev = bdev;
+	bio->bi_private = NULL;
+	return bio;
+}
+
+static void f2fs_read_end_io(struct bio *bio, int err)
+{
+	const int uptodate = test_bit(BIO_UPTODATE, &bio->bi_flags);
+	struct bio_vec *bvec = bio->bi_io_vec + bio->bi_vcnt - 1;
+
+	do {
+		struct page *page = bvec->bv_page;
+
+		if (--bvec >= bio->bi_io_vec)
+			prefetchw(&bvec->bv_page->flags);
+
+		if (uptodate) {
+			SetPageUptodate(page);
+		} else {
+			ClearPageUptodate(page);
+			SetPageError(page);
+		}
+		unlock_page(page);
+	} while (bvec >= bio->bi_io_vec);
+
+	bio_put(bio);
+}
+
+static void f2fs_write_end_io(struct bio *bio, int err)
+{
+	const int uptodate = test_bit(BIO_UPTODATE, &bio->bi_flags);
+	struct bio_vec *bvec = bio->bi_io_vec + bio->bi_vcnt - 1;
+	struct f2fs_sb_info *sbi = F2FS_SB(bvec->bv_page->mapping->host->i_sb);
+
+	do {
+		struct page *page = bvec->bv_page;
+
+		if (--bvec >= bio->bi_io_vec)
+			prefetchw(&bvec->bv_page->flags);
+
+		if (!uptodate) {
+			SetPageError(page);
+			set_bit(AS_EIO, &page->mapping->flags);
+			set_ckpt_flags(sbi->ckpt, CP_ERROR_FLAG);
+			sbi->sb->s_flags |= MS_RDONLY;
+		}
+		end_page_writeback(page);
+		dec_page_count(sbi, F2FS_WRITEBACK);
+	} while (bvec >= bio->bi_io_vec);
+
+	if (bio->bi_private)
+		complete(bio->bi_private);
+
+	if (!get_pages(sbi, F2FS_WRITEBACK) &&
+			!list_empty(&sbi->cp_wait.task_list))
+		wake_up(&sbi->cp_wait);
+
+	bio_put(bio);
+}
+
+static void __submit_merged_bio(struct f2fs_sb_info *sbi,
+				struct f2fs_bio_info *io,
+				enum page_type type, bool sync, int rw)
+{
+	enum page_type btype = PAGE_TYPE_OF_BIO(type);
+
+	if (!io->bio)
+		return;
+
+	if (btype == META)
+		rw |= REQ_META;
+
+	if (is_read_io(rw)) {
+		if (sync)
+			rw |= READ_SYNC;
+		submit_bio(rw, io->bio);
+		trace_f2fs_submit_read_bio(sbi->sb, rw, type, io->bio);
+		io->bio = NULL;
+		return;
+	}
+
+	if (sync)
+		rw |= WRITE_SYNC;
+	if (type >= META_FLUSH)
+		rw |= WRITE_FLUSH_FUA;
+
+	/*
+	 * META_FLUSH is only from the checkpoint procedure, and we should wait
+	 * this metadata bio for FS consistency.
+	 */
+	if (type == META_FLUSH) {
+		DECLARE_COMPLETION_ONSTACK(wait);
+		io->bio->bi_private = &wait;
+		submit_bio(rw, io->bio);
+		wait_for_completion(&wait);
+	} else {
+		submit_bio(rw, io->bio);
+	}
+	trace_f2fs_submit_write_bio(sbi->sb, rw, btype, io->bio);
+	io->bio = NULL;
+}
+
+void f2fs_submit_merged_bio(struct f2fs_sb_info *sbi,
+				enum page_type type, bool sync, int rw)
+{
+	enum page_type btype = PAGE_TYPE_OF_BIO(type);
+	struct f2fs_bio_info *io;
+
+	io = is_read_io(rw) ? &sbi->read_io : &sbi->write_io[btype];
+
+	mutex_lock(&io->io_mutex);
+	__submit_merged_bio(sbi, io, type, sync, rw);
+	mutex_unlock(&io->io_mutex);
+}
+
+/*
+ * Fill the locked page with data located in the block address.
+ * Return unlocked page.
+ */
+int f2fs_submit_page_bio(struct f2fs_sb_info *sbi, struct page *page,
+					block_t blk_addr, int rw)
+{
+	struct block_device *bdev = sbi->sb->s_bdev;
+	struct bio *bio;
+
+	trace_f2fs_submit_page_bio(page, blk_addr, rw);
+
+	/* Allocate a new bio */
+	bio = __bio_alloc(bdev, 1);
+
+	/* Initialize the bio */
+	bio->bi_sector = SECTOR_FROM_BLOCK(sbi, blk_addr);
+	bio->bi_end_io = is_read_io(rw) ? f2fs_read_end_io : f2fs_write_end_io;
+
+	if (bio_add_page(bio, page, PAGE_CACHE_SIZE, 0) < PAGE_CACHE_SIZE) {
+		bio_put(bio);
+		f2fs_put_page(page, 1);
+		return -EFAULT;
+	}
+
+	submit_bio(rw, bio);
+	return 0;
+}
+
+void f2fs_submit_page_mbio(struct f2fs_sb_info *sbi, struct page *page,
+			block_t blk_addr, enum page_type type, int rw)
+{
+	enum page_type btype = PAGE_TYPE_OF_BIO(type);
+	struct block_device *bdev = sbi->sb->s_bdev;
+	struct f2fs_bio_info *io;
+	int bio_blocks;
+
+	io = is_read_io(rw) ? &sbi->read_io : &sbi->write_io[btype];
+
+	verify_block_addr(sbi, blk_addr);
+
+	mutex_lock(&io->io_mutex);
+
+	if (!is_read_io(rw))
+		inc_page_count(sbi, F2FS_WRITEBACK);
+
+	if (io->bio && io->last_block_in_bio != blk_addr - 1)
+		__submit_merged_bio(sbi, io, type, true, rw);
+alloc_new:
+	if (io->bio == NULL) {
+		bio_blocks = MAX_BIO_BLOCKS(max_hw_blocks(sbi));
+		io->bio = __bio_alloc(bdev, bio_blocks);
+		io->bio->bi_sector = SECTOR_FROM_BLOCK(sbi, blk_addr);
+		io->bio->bi_end_io = is_read_io(rw) ? f2fs_read_end_io :
+							f2fs_write_end_io;
+		/*
+		 * The end_io will be assigned at the sumbission phase.
+		 * Until then, let bio_add_page() merge consecutive IOs as much
+		 * as possible.
+		 */
+	}
+
+	if (bio_add_page(io->bio, page, PAGE_CACHE_SIZE, 0) <
+							PAGE_CACHE_SIZE) {
+		__submit_merged_bio(sbi, io, type, true, rw);
+		goto alloc_new;
+	}
+
+	io->last_block_in_bio = blk_addr;
+
+	mutex_unlock(&io->io_mutex);
+	trace_f2fs_submit_page_mbio(page, rw, type, blk_addr);
+}
+
 /*
  * Lock ordering for the change of data block address:
  * ->data_page
@@ -238,7 +436,7 @@ struct page *find_data_page(struct inode *inode, pgoff_t index, bool sync)
 		return page;
 	}
 
-	err = f2fs_readpage(sbi, page, dn.data_blkaddr,
+	err = f2fs_submit_page_bio(sbi, page, dn.data_blkaddr,
 					sync ? READ_SYNC : READA);
 	if (err)
 		return ERR_PTR(err);
@@ -299,7 +497,7 @@ struct page *get_lock_data_page(struct inode *inode, pgoff_t index)
 		return page;
 	}
 
-	err = f2fs_readpage(sbi, page, dn.data_blkaddr, READ_SYNC);
+	err = f2fs_submit_page_bio(sbi, page, dn.data_blkaddr, READ_SYNC);
 	if (err)
 		return ERR_PTR(err);
 
@@ -349,7 +547,8 @@ struct page *get_new_data_page(struct inode *inode,
 		zero_user_segment(page, 0, PAGE_CACHE_SIZE);
 		SetPageUptodate(page);
 	} else {
-		err = f2fs_readpage(sbi, page, dn.data_blkaddr, READ_SYNC);
+		err = f2fs_submit_page_bio(sbi, page, dn.data_blkaddr,
+								READ_SYNC);
 		if (err)
 			return ERR_PTR(err);
 		lock_page(page);
@@ -373,110 +572,6 @@ struct page *get_new_data_page(struct inode *inode,
 	return page;
 }
 
-static void read_end_io(struct bio *bio, int err)
-{
-	const int uptodate = test_bit(BIO_UPTODATE, &bio->bi_flags);
-	struct bio_vec *bvec = bio->bi_io_vec + bio->bi_vcnt - 1;
-
-	do {
-		struct page *page = bvec->bv_page;
-
-		if (--bvec >= bio->bi_io_vec)
-			prefetchw(&bvec->bv_page->flags);
-
-		if (uptodate) {
-			SetPageUptodate(page);
-		} else {
-			ClearPageUptodate(page);
-			SetPageError(page);
-		}
-		unlock_page(page);
-	} while (bvec >= bio->bi_io_vec);
-	bio_put(bio);
-}
-
-/*
- * Fill the locked page with data located in the block address.
- * Return unlocked page.
- */
-int f2fs_readpage(struct f2fs_sb_info *sbi, struct page *page,
-					block_t blk_addr, int type)
-{
-	struct block_device *bdev = sbi->sb->s_bdev;
-	struct bio *bio;
-
-	trace_f2fs_readpage(page, blk_addr, type);
-
-	/* Allocate a new bio */
-	bio = f2fs_bio_alloc(bdev, 1);
-
-	/* Initialize the bio */
-	bio->bi_sector = SECTOR_FROM_BLOCK(sbi, blk_addr);
-	bio->bi_end_io = read_end_io;
-
-	if (bio_add_page(bio, page, PAGE_CACHE_SIZE, 0) < PAGE_CACHE_SIZE) {
-		bio_put(bio);
-		f2fs_put_page(page, 1);
-		return -EFAULT;
-	}
-
-	submit_bio(type, bio);
-	return 0;
-}
-
-void f2fs_submit_read_bio(struct f2fs_sb_info *sbi, int rw)
-{
-	struct f2fs_bio_info *io = &sbi->read_io;
-
-	if (!io->bio)
-		return;
-
-	trace_f2fs_submit_read_bio(sbi->sb, rw, META, io->bio);
-
-	mutex_lock(&io->io_mutex);
-	if (io->bio) {
-		submit_bio(rw, io->bio);
-		io->bio = NULL;
-	}
-	mutex_unlock(&io->io_mutex);
-}
-
-void submit_read_page(struct f2fs_sb_info *sbi, struct page *page,
-					block_t blk_addr, int rw)
-{
-	struct block_device *bdev = sbi->sb->s_bdev;
-	struct f2fs_bio_info *io = &sbi->read_io;
-	int bio_blocks;
-
-	verify_block_addr(sbi, blk_addr);
-
-	mutex_lock(&io->io_mutex);
-
-	if (io->bio && io->last_block_in_bio != blk_addr - 1) {
-		submit_bio(rw, io->bio);
-		io->bio = NULL;
-	}
-alloc_new:
-	if (io->bio == NULL) {
-		bio_blocks = MAX_BIO_BLOCKS(max_hw_blocks(sbi));
-		io->bio = f2fs_bio_alloc(bdev, bio_blocks);
-		io->bio->bi_sector = SECTOR_FROM_BLOCK(sbi, blk_addr);
-		io->bio->bi_end_io = read_end_io;
-	}
-
-	if (bio_add_page(io->bio, page, PAGE_CACHE_SIZE, 0) <
-							PAGE_CACHE_SIZE) {
-		submit_bio(rw, io->bio);
-		io->bio = NULL;
-		goto alloc_new;
-	}
-
-	io->last_block_in_bio = blk_addr;
-
-	mutex_unlock(&io->io_mutex);
-	trace_f2fs_submit_read_page(page, rw, META, blk_addr);
-}
-
 /*
  * This function should be used by the data read flow only where it
  * does not check the "create" flag that indicates block allocation.
@@ -638,7 +733,7 @@ static int f2fs_write_data_page(struct page *page,
 		goto redirty_out;
 
 	if (wbc->for_reclaim)
-		f2fs_submit_bio(sbi, DATA, true);
+		f2fs_submit_merged_bio(sbi, DATA, true, WRITE);
 
 	clear_cold_data(page);
 out:
@@ -690,7 +785,7 @@ static int f2fs_write_data_pages(struct address_space *mapping,
 	ret = write_cache_pages(mapping, wbc, __f2fs_writepage, mapping);
 	if (locked)
 		mutex_unlock(&sbi->writepages);
-	f2fs_submit_bio(sbi, DATA, (wbc->sync_mode == WB_SYNC_ALL));
+	f2fs_submit_merged_bio(sbi, DATA, wbc->sync_mode == WB_SYNC_ALL, WRITE);
 
 	remove_dirty_dir_inode(inode);
 
@@ -741,7 +836,8 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 	if (dn.data_blkaddr == NEW_ADDR) {
 		zero_user_segment(page, 0, PAGE_CACHE_SIZE);
 	} else {
-		err = f2fs_readpage(sbi, page, dn.data_blkaddr, READ_SYNC);
+		err = f2fs_submit_page_bio(sbi, page, dn.data_blkaddr,
+							READ_SYNC);
 		if (err)
 			return err;
 		lock_page(page);

commit 1069bbf7b963d31d5532c36d43a02b0447e5bcfa
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Thu Nov 28 15:43:43 2013 +0800

    f2fs: check return value of f2fs_readpage in find_data_page
    
    We should return error if we do not get an updated page in find_date_page
    when f2fs_readpage failed.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 24f752de6a90..c9a76f8c1028 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -240,6 +240,9 @@ struct page *find_data_page(struct inode *inode, pgoff_t index, bool sync)
 
 	err = f2fs_readpage(sbi, page, dn.data_blkaddr,
 					sync ? READ_SYNC : READA);
+	if (err)
+		return ERR_PTR(err);
+
 	if (sync) {
 		wait_on_page_locked(page);
 		if (!PageUptodate(page)) {

commit f9a4e6df52edf8ce1040d1b8d340d31234a1bce3
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Thu Nov 28 12:44:05 2013 +0900

    f2fs: bug fix on bit overflow from 32bits to 64bits
    
    This patch fixes some bit overflows by the shift operations.
    
    Dan Carpenter reported potential bugs on bit overflows as follows.
    
    fs/f2fs/segment.c:910 submit_write_page()
            warn: should 'blk_addr << ((sbi)->log_blocksize - 9)' be a 64 bit type?
    fs/f2fs/checkpoint.c:429 get_valid_checkpoint()
            warn: should '1 << ()' be a 64 bit type?
    fs/f2fs/data.c:408 f2fs_readpage()
            warn: should 'blk_addr << ((sbi)->log_blocksize - 9)' be a 64 bit type?
    fs/f2fs/data.c:457 submit_read_page()
            warn: should 'blk_addr << ((sbi)->log_blocksize - 9)' be a 64 bit type?
    fs/f2fs/data.c:525 get_data_block_ro()
            warn: should 'i << blkbits' be a 64 bit type?
    
    Bug-Reported-by: Dan Carpenter <dan.carpenter@oracle.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 2d02cf36d806..24f752de6a90 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -525,7 +525,7 @@ static int get_data_block_ro(struct inode *inode, sector_t iblock,
 				!= (dn.data_blkaddr + i)) || maxblocks == i)
 				break;
 		map_bh(bh_result, inode->i_sb, dn.data_blkaddr);
-		bh_result->b_size = (i << blkbits);
+		bh_result->b_size = (((size_t)i) << blkbits);
 	}
 	f2fs_put_dnode(&dn);
 	trace_f2fs_get_data_block(inode, iblock, bh_result, 0);

commit b600965c43f9690eb481d0c19948e109b685bde7
Author: Huajun Li <huajun.li@intel.com>
Date:   Sun Nov 10 23:13:18 2013 +0800

    f2fs: add a new function: f2fs_reserve_block()
    
    Add the function f2fs_reserve_block() to easily reserve new blocks, and
    use it to clean up more codes.
    
    Signed-off-by: Huajun Li <huajun.li@intel.com>
    Signed-off-by: Haicheng Li <haicheng.li@linux.intel.com>
    Signed-off-by: Weihong Xu <weihong.xu@intel.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 711722018b8e..2d02cf36d806 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -64,6 +64,22 @@ int reserve_new_block(struct dnode_of_data *dn)
 	return 0;
 }
 
+int f2fs_reserve_block(struct dnode_of_data *dn, pgoff_t index)
+{
+	bool need_put = dn->inode_page ? false : true;
+	int err;
+
+	err = get_dnode_of_data(dn, index, ALLOC_NODE);
+	if (err)
+		return err;
+	if (dn->data_blkaddr == NULL_ADDR)
+		err = reserve_new_block(dn);
+
+	if (need_put)
+		f2fs_put_dnode(dn);
+	return err;
+}
+
 static int check_extent_cache(struct inode *inode, pgoff_t pgofs,
 					struct buffer_head *bh_result)
 {
@@ -314,19 +330,10 @@ struct page *get_new_data_page(struct inode *inode,
 	int err;
 
 	set_new_dnode(&dn, inode, npage, npage, 0);
-	err = get_dnode_of_data(&dn, index, ALLOC_NODE);
+	err = f2fs_reserve_block(&dn, index);
 	if (err)
 		return ERR_PTR(err);
 
-	if (dn.data_blkaddr == NULL_ADDR) {
-		if (reserve_new_block(&dn)) {
-			if (!npage)
-				f2fs_put_dnode(&dn);
-			return ERR_PTR(-ENOSPC);
-		}
-	}
-	if (!npage)
-		f2fs_put_dnode(&dn);
 repeat:
 	page = grab_cache_page(mapping, index);
 	if (!page)
@@ -707,21 +714,15 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 	*pagep = page;
 
 	f2fs_lock_op(sbi);
-
 	set_new_dnode(&dn, inode, NULL, NULL, 0);
-	err = get_dnode_of_data(&dn, index, ALLOC_NODE);
-	if (err)
-		goto err;
-
-	if (dn.data_blkaddr == NULL_ADDR)
-		err = reserve_new_block(&dn);
-
-	f2fs_put_dnode(&dn);
-	if (err)
-		goto err;
-
+	err = f2fs_reserve_block(&dn, index);
 	f2fs_unlock_op(sbi);
 
+	if (err) {
+		f2fs_put_page(page, 1);
+		return err;
+	}
+
 	if ((len == PAGE_CACHE_SIZE) || PageUptodate(page))
 		return 0;
 
@@ -754,11 +755,6 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 	SetPageUptodate(page);
 	clear_cold_data(page);
 	return 0;
-
-err:
-	f2fs_unlock_op(sbi);
-	f2fs_put_page(page, 1);
-	return err;
 }
 
 static int f2fs_write_end(struct file *file,

commit d4d288bc72c020d335868ce217695c4d5dfd74d0
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Sun Nov 24 12:36:42 2013 +0900

    f2fs: adds a tracepoint for f2fs_submit_read_bio
    
    This patch adds a tracepoint for f2fs_submit_read_bio.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    [Jaegeuk Kim: integrate tracepoints of f2fs_submit_read(_write)_bio]
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index cdb342eeb421..711722018b8e 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -421,6 +421,8 @@ void f2fs_submit_read_bio(struct f2fs_sb_info *sbi, int rw)
 	if (!io->bio)
 		return;
 
+	trace_f2fs_submit_read_bio(sbi->sb, rw, META, io->bio);
+
 	mutex_lock(&io->io_mutex);
 	if (io->bio) {
 		submit_bio(rw, io->bio);

commit 87b8872d5b4a8f9f61123ab913aff4f6047d8b53
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Wed Nov 20 16:40:10 2013 +0800

    f2fs: adds a tracepoint for submit_read_page
    
    This patch adds a tracepoint for submit_read_page.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    [Jaegeuk Kim: integrate tracepoints of f2fs_submit_read(_write)_page]
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index ce3cbd92585e..cdb342eeb421 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -462,6 +462,7 @@ void submit_read_page(struct f2fs_sb_info *sbi, struct page *page,
 	io->last_block_in_bio = blk_addr;
 
 	mutex_unlock(&io->io_mutex);
+	trace_f2fs_submit_read_page(page, rw, META, blk_addr);
 }
 
 /*

commit 924b720b589f91311657216c97edbb3337449270
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Wed Nov 20 14:46:39 2013 +0800

    f2fs: add a new function to support for merging contiguous read
    
    For better read performance, we add a new function to support for merging
    contiguous read as the one for write.
    
    v1-->v2:
     o add declarations here as Gu Zheng suggested.
     o use new structure f2fs_bio_info introduced by Jaegeuk Kim.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Acked-by: Gu Zheng <guz.fnst@cn.fujitsu.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 2e54522a8061..ce3cbd92585e 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -414,6 +414,56 @@ int f2fs_readpage(struct f2fs_sb_info *sbi, struct page *page,
 	return 0;
 }
 
+void f2fs_submit_read_bio(struct f2fs_sb_info *sbi, int rw)
+{
+	struct f2fs_bio_info *io = &sbi->read_io;
+
+	if (!io->bio)
+		return;
+
+	mutex_lock(&io->io_mutex);
+	if (io->bio) {
+		submit_bio(rw, io->bio);
+		io->bio = NULL;
+	}
+	mutex_unlock(&io->io_mutex);
+}
+
+void submit_read_page(struct f2fs_sb_info *sbi, struct page *page,
+					block_t blk_addr, int rw)
+{
+	struct block_device *bdev = sbi->sb->s_bdev;
+	struct f2fs_bio_info *io = &sbi->read_io;
+	int bio_blocks;
+
+	verify_block_addr(sbi, blk_addr);
+
+	mutex_lock(&io->io_mutex);
+
+	if (io->bio && io->last_block_in_bio != blk_addr - 1) {
+		submit_bio(rw, io->bio);
+		io->bio = NULL;
+	}
+alloc_new:
+	if (io->bio == NULL) {
+		bio_blocks = MAX_BIO_BLOCKS(max_hw_blocks(sbi));
+		io->bio = f2fs_bio_alloc(bdev, bio_blocks);
+		io->bio->bi_sector = SECTOR_FROM_BLOCK(sbi, blk_addr);
+		io->bio->bi_end_io = read_end_io;
+	}
+
+	if (bio_add_page(io->bio, page, PAGE_CACHE_SIZE, 0) <
+							PAGE_CACHE_SIZE) {
+		submit_bio(rw, io->bio);
+		io->bio = NULL;
+		goto alloc_new;
+	}
+
+	io->last_block_in_bio = blk_addr;
+
+	mutex_unlock(&io->io_mutex);
+}
+
 /*
  * This function should be used by the data read flow only where it
  * does not check the "create" flag that indicates block allocation.

commit c11abd1a8075e428ecf5303359513b48193b29cd
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Tue Nov 19 10:41:54 2013 +0900

    f2fs: disable the extent cache ops on high fragmented files
    
    The f2fs manages an extent cache to search a number of consecutive data blocks
    very quickly.
    
    However it conducts unnecessary cache operations if the file is highly
    fragmented with no valid extent cache.
    
    In such the case, we don't need to handle the extent cache, but just can disable
    the cache facility.
    
    Nevertheless, this patch gives one more chance to enable the extent cache.
    
    For example,
    1. create a file
    2. write data sequentially which produces a large valid extent cache
    3. update some data, resulting in a fragmented extent
    4. if the fragmented extent is too small, then drop extent cache
    5. close the file
    
    6. open the file again
    7. give another chance to make a new extent cache
    8. write data sequentially again which creates another big extent cache.
    ...
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 5920639ca377..2e54522a8061 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -71,6 +71,9 @@ static int check_extent_cache(struct inode *inode, pgoff_t pgofs,
 	pgoff_t start_fofs, end_fofs;
 	block_t start_blkaddr;
 
+	if (is_inode_flag_set(fi, FI_NO_EXTENT))
+		return 0;
+
 	read_lock(&fi->ext.ext_lock);
 	if (fi->ext.len == 0) {
 		read_unlock(&fi->ext.ext_lock);
@@ -109,6 +112,7 @@ void update_extent_cache(block_t blk_addr, struct dnode_of_data *dn)
 	struct f2fs_inode_info *fi = F2FS_I(dn->inode);
 	pgoff_t fofs, start_fofs, end_fofs;
 	block_t start_blkaddr, end_blkaddr;
+	int need_update = true;
 
 	f2fs_bug_on(blk_addr == NEW_ADDR);
 	fofs = start_bidx_of_node(ofs_of_node(dn->node_page), fi) +
@@ -117,6 +121,9 @@ void update_extent_cache(block_t blk_addr, struct dnode_of_data *dn)
 	/* Update the page address in the parent node */
 	__set_data_blkaddr(dn, blk_addr);
 
+	if (is_inode_flag_set(fi, FI_NO_EXTENT))
+		return;
+
 	write_lock(&fi->ext.ext_lock);
 
 	start_fofs = fi->ext.fofs;
@@ -163,14 +170,21 @@ void update_extent_cache(block_t blk_addr, struct dnode_of_data *dn)
 					fofs - start_fofs + 1;
 			fi->ext.len -= fofs - start_fofs + 1;
 		}
-		goto end_update;
+	} else {
+		need_update = false;
 	}
-	write_unlock(&fi->ext.ext_lock);
-	return;
 
+	/* Finally, if the extent is very fragmented, let's drop the cache. */
+	if (fi->ext.len < F2FS_MIN_EXTENT_LEN) {
+		fi->ext.len = 0;
+		set_inode_flag(fi, FI_NO_EXTENT);
+		need_update = true;
+	}
 end_update:
 	write_unlock(&fi->ext.ext_lock);
-	sync_inode_page(dn);
+	if (need_update)
+		sync_inode_page(dn);
+	return;
 }
 
 struct page *find_data_page(struct inode *inode, pgoff_t index, bool sync)

commit 971767caf632190f77a40b4011c19948232eed75
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Mon Nov 18 17:16:17 2013 +0900

    f2fs: use sbi->write_mutex for write bios
    
    This patch removes an unnecessary semaphore (i.e., sbi->bio_sem).
    There is no reason to use the semaphore when f2fs submits read and write IOs.
    Instead, let's use a write mutex and cover the sbi->bio[] by the lock.
    
    Change log from v1:
     o split write_mutex suggested by Chao Yu
    
    Chao described,
    "All DATA/NODE/META bio buffers in superblock is protected by
    'sbi->write_mutex', but each bio buffer area is independent, So we
    should split write_mutex to three for DATA/NODE/META."
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 076a60cb8dbb..5920639ca377 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -383,8 +383,6 @@ int f2fs_readpage(struct f2fs_sb_info *sbi, struct page *page,
 
 	trace_f2fs_readpage(page, blk_addr, type);
 
-	down_read(&sbi->bio_sem);
-
 	/* Allocate a new bio */
 	bio = f2fs_bio_alloc(bdev, 1);
 
@@ -394,13 +392,11 @@ int f2fs_readpage(struct f2fs_sb_info *sbi, struct page *page,
 
 	if (bio_add_page(bio, page, PAGE_CACHE_SIZE, 0) < PAGE_CACHE_SIZE) {
 		bio_put(bio);
-		up_read(&sbi->bio_sem);
 		f2fs_put_page(page, 1);
 		return -EFAULT;
 	}
 
 	submit_bio(type, bio);
-	up_read(&sbi->bio_sem);
 	return 0;
 }
 

commit 75c3c8bc88d573d6823fcf2576f5c47e4fc41710
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Sat Nov 16 14:15:59 2013 +0800

    f2fs: use f2fs_put_page to release page for uniform style
    
    We should use f2fs_put_page to release page for uniform style of f2fs code.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index aa3438c571fa..076a60cb8dbb 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -714,8 +714,7 @@ static int f2fs_write_end(struct file *file,
 		update_inode_page(inode);
 	}
 
-	unlock_page(page);
-	page_cache_release(page);
+	f2fs_put_page(page, 1);
 	return copied;
 }
 

commit 4f024f3797c43cb4b73cd2c50cec728842d0e49e
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Fri Oct 11 15:44:27 2013 -0700

    block: Abstract out bvec iterator
    
    Immutable biovecs are going to require an explicit iterator. To
    implement immutable bvecs, a later patch is going to add a bi_bvec_done
    member to this struct; for now, this patch effectively just renames
    things.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: "Ed L. Cashin" <ecashin@coraid.com>
    Cc: Nick Piggin <npiggin@kernel.dk>
    Cc: Lars Ellenberg <drbd-dev@lists.linbit.com>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Matthew Wilcox <willy@linux.intel.com>
    Cc: Geoff Levand <geoff@infradead.org>
    Cc: Yehuda Sadeh <yehuda@inktank.com>
    Cc: Sage Weil <sage@inktank.com>
    Cc: Alex Elder <elder@inktank.com>
    Cc: ceph-devel@vger.kernel.org
    Cc: Joshua Morris <josh.h.morris@us.ibm.com>
    Cc: Philip Kelleher <pjk1939@linux.vnet.ibm.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: "Michael S. Tsirkin" <mst@redhat.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Jeremy Fitzhardinge <jeremy@goop.org>
    Cc: Neil Brown <neilb@suse.de>
    Cc: Alasdair Kergon <agk@redhat.com>
    Cc: Mike Snitzer <snitzer@redhat.com>
    Cc: dm-devel@redhat.com
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: linux390@de.ibm.com
    Cc: Boaz Harrosh <bharrosh@panasas.com>
    Cc: Benny Halevy <bhalevy@tonian.com>
    Cc: "James E.J. Bottomley" <JBottomley@parallels.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: "Nicholas A. Bellinger" <nab@linux-iscsi.org>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: Chris Mason <chris.mason@fusionio.com>
    Cc: "Theodore Ts'o" <tytso@mit.edu>
    Cc: Andreas Dilger <adilger.kernel@dilger.ca>
    Cc: Jaegeuk Kim <jaegeuk.kim@samsung.com>
    Cc: Steven Whitehouse <swhiteho@redhat.com>
    Cc: Dave Kleikamp <shaggy@kernel.org>
    Cc: Joern Engel <joern@logfs.org>
    Cc: Prasad Joshi <prasadjoshi.linux@gmail.com>
    Cc: Trond Myklebust <Trond.Myklebust@netapp.com>
    Cc: KONISHI Ryusuke <konishi.ryusuke@lab.ntt.co.jp>
    Cc: Mark Fasheh <mfasheh@suse.com>
    Cc: Joel Becker <jlbec@evilplan.org>
    Cc: Ben Myers <bpm@sgi.com>
    Cc: xfs@oss.sgi.com
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Len Brown <len.brown@intel.com>
    Cc: Pavel Machek <pavel@ucw.cz>
    Cc: "Rafael J. Wysocki" <rjw@sisk.pl>
    Cc: Herton Ronaldo Krzesinski <herton.krzesinski@canonical.com>
    Cc: Ben Hutchings <ben@decadent.org.uk>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Guo Chao <yan@linux.vnet.ibm.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Asai Thambi S P <asamymuthupa@micron.com>
    Cc: Selvan Mani <smani@micron.com>
    Cc: Sam Bradshaw <sbradshaw@micron.com>
    Cc: Wei Yongjun <yongjun_wei@trendmicro.com.cn>
    Cc: "Roger Pau Monné" <roger.pau@citrix.com>
    Cc: Jan Beulich <jbeulich@suse.com>
    Cc: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Cc: Ian Campbell <Ian.Campbell@citrix.com>
    Cc: Sebastian Ott <sebott@linux.vnet.ibm.com>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Jiang Liu <jiang.liu@huawei.com>
    Cc: Nitin Gupta <ngupta@vflare.org>
    Cc: Jerome Marchand <jmarchand@redhat.com>
    Cc: Joe Perches <joe@perches.com>
    Cc: Peng Tao <tao.peng@emc.com>
    Cc: Andy Adamson <andros@netapp.com>
    Cc: fanchaoting <fanchaoting@cn.fujitsu.com>
    Cc: Jie Liu <jeff.liu@oracle.com>
    Cc: Sunil Mushran <sunil.mushran@gmail.com>
    Cc: "Martin K. Petersen" <martin.petersen@oracle.com>
    Cc: Namjae Jeon <namjae.jeon@samsung.com>
    Cc: Pankaj Kumar <pankaj.km@samsung.com>
    Cc: Dan Magenheimer <dan.magenheimer@oracle.com>
    Cc: Mel Gorman <mgorman@suse.de>6

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index a4949096cf4c..a2c8de8ba6ce 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -386,7 +386,7 @@ int f2fs_readpage(struct f2fs_sb_info *sbi, struct page *page,
 	bio = f2fs_bio_alloc(bdev, 1);
 
 	/* Initialize the bio */
-	bio->bi_sector = SECTOR_FROM_BLOCK(sbi, blk_addr);
+	bio->bi_iter.bi_sector = SECTOR_FROM_BLOCK(sbi, blk_addr);
 	bio->bi_end_io = read_end_io;
 
 	if (bio_add_page(bio, page, PAGE_CACHE_SIZE, 0) < PAGE_CACHE_SIZE) {

commit 2c30c71bd653afcbed7f6754e8fe3d16e0e708a1
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Thu Nov 7 12:20:26 2013 -0800

    block: Convert various code to bio_for_each_segment()
    
    With immutable biovecs we don't want code accessing bi_io_vec directly -
    the uses this patch changes weren't incorrect since they all own the
    bio, but it makes the code harder to audit for no good reason - also,
    this will help with multipage bvecs later.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: Chris Mason <chris.mason@fusionio.com>
    Cc: Jaegeuk Kim <jaegeuk.kim@samsung.com>
    Cc: Joern Engel <joern@logfs.org>
    Cc: Prasad Joshi <prasadjoshi.linux@gmail.com>
    Cc: Trond Myklebust <Trond.Myklebust@netapp.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index aa3438c571fa..a4949096cf4c 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -351,23 +351,20 @@ struct page *get_new_data_page(struct inode *inode,
 
 static void read_end_io(struct bio *bio, int err)
 {
-	const int uptodate = test_bit(BIO_UPTODATE, &bio->bi_flags);
-	struct bio_vec *bvec = bio->bi_io_vec + bio->bi_vcnt - 1;
+	struct bio_vec *bvec;
+	int i;
 
-	do {
+	bio_for_each_segment_all(bvec, bio, i) {
 		struct page *page = bvec->bv_page;
 
-		if (--bvec >= bio->bi_io_vec)
-			prefetchw(&bvec->bv_page->flags);
-
-		if (uptodate) {
+		if (!err) {
 			SetPageUptodate(page);
 		} else {
 			ClearPageUptodate(page);
 			SetPageError(page);
 		}
 		unlock_page(page);
-	} while (bvec >= bio->bi_io_vec);
+	}
 	bio_put(bio);
 }
 

commit 5d56b6718a0f4e5c58cdd3cb6b7a472d7c5671b9
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Tue Oct 29 15:14:54 2013 +0900

    f2fs: add an option to avoid unnecessary BUG_ONs
    
    If you want to remove unnecessary BUG_ONs, you can just turn off F2FS_CHECK_FS
    in your kernel config.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index c8887d847dcf..aa3438c571fa 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -110,7 +110,7 @@ void update_extent_cache(block_t blk_addr, struct dnode_of_data *dn)
 	pgoff_t fofs, start_fofs, end_fofs;
 	block_t start_blkaddr, end_blkaddr;
 
-	BUG_ON(blk_addr == NEW_ADDR);
+	f2fs_bug_on(blk_addr == NEW_ADDR);
 	fofs = start_bidx_of_node(ofs_of_node(dn->node_page), fi) +
 							dn->ofs_in_node;
 
@@ -436,7 +436,7 @@ static int get_data_block_ro(struct inode *inode, sector_t iblock,
 	}
 
 	/* It does not support data allocation */
-	BUG_ON(create);
+	f2fs_bug_on(create);
 
 	if (dn.data_blkaddr != NEW_ADDR && dn.data_blkaddr != NULL_ADDR) {
 		int i;

commit 26c6b8879911df991dc780c67eaeb84c7629949d
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Thu Oct 24 17:53:29 2013 +0900

    f2fs: add tracepoint for set_page_dirty
    
    This patch adds a tracepoint for set_page_dirty.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 1cccf98f0e4d..c8887d847dcf 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -756,6 +756,8 @@ static int f2fs_set_data_page_dirty(struct page *page)
 	struct address_space *mapping = page->mapping;
 	struct inode *inode = mapping->host;
 
+	trace_f2fs_set_page_dirty(page, DATA);
+
 	SetPageUptodate(page);
 	if (!PageDirty(page)) {
 		__set_page_dirty_nobuffers(page);

commit dcdfff65276fdc6dfe5eb1d0aff802dfa7a95e15
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Tue Oct 22 20:56:10 2013 +0900

    f2fs: clean up several status-related operations
    
    This patch cleans up improper definitions that update some status information.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 2535d3b1a763..1cccf98f0e4d 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -68,9 +68,6 @@ static int check_extent_cache(struct inode *inode, pgoff_t pgofs,
 					struct buffer_head *bh_result)
 {
 	struct f2fs_inode_info *fi = F2FS_I(inode);
-#ifdef CONFIG_F2FS_STAT_FS
-	struct f2fs_sb_info *sbi = F2FS_SB(inode->i_sb);
-#endif
 	pgoff_t start_fofs, end_fofs;
 	block_t start_blkaddr;
 
@@ -80,9 +77,8 @@ static int check_extent_cache(struct inode *inode, pgoff_t pgofs,
 		return 0;
 	}
 
-#ifdef CONFIG_F2FS_STAT_FS
-	sbi->total_hit_ext++;
-#endif
+	stat_inc_total_hit(inode->i_sb);
+
 	start_fofs = fi->ext.fofs;
 	end_fofs = fi->ext.fofs + fi->ext.len - 1;
 	start_blkaddr = fi->ext.blk_addr;
@@ -100,9 +96,7 @@ static int check_extent_cache(struct inode *inode, pgoff_t pgofs,
 		else
 			bh_result->b_size = UINT_MAX;
 
-#ifdef CONFIG_F2FS_STAT_FS
-		sbi->read_hit_ext++;
-#endif
+		stat_inc_read_hit(inode->i_sb);
 		read_unlock(&fi->ext.ext_lock);
 		return 1;
 	}

commit e479556bfdd136669854292eb57ed0139d7253d5
Author: Gu Zheng <guz.fnst@cn.fujitsu.com>
Date:   Fri Sep 27 18:08:30 2013 +0800

    f2fs: use rw_sem instead of fs_lock(locks mutex)
    
    The fs_locks is used to block other ops(ex, recovery) when doing checkpoint.
    And each other operate routine(besides checkpoint) needs to acquire a fs_lock,
    there is a terrible problem here, if these are too many concurrency threads acquiring
    fs_lock, so that they will block each other and may lead to some performance problem,
    but this is not the phenomenon we want to see.
    Though there are some optimization patches introduced to enhance the usage of fs_lock,
    but the thorough solution is using a *rw_sem* to replace the fs_lock.
    Checkpoint routine takes write_sem, and other ops take read_sem, so that we can block
    other ops(ex, recovery) when doing checkpoint, and other ops will not disturb each other,
    this can avoid the problem described above completely.
    Because of the weakness of rw_sem, the above change may introduce a potential problem
    that the checkpoint thread might get starved if other threads are intensively locking
    the read semaphore for I/O.(Pointed out by Xu Jin)
    In order to avoid this, a wait_list is introduced, the appending read semaphore ops
    will be dropped into the wait_list if checkpoint thread is waiting for write semaphore,
    and will be waked up when checkpoint thread gives up write semaphore.
    Thanks to Kim's previous review and test, and will be very glad to see other guys'
    performance tests about this patch.
    
    V2:
      -fix the potential starvation problem.
      -use more suitable func name suggested by Xu Jin.
    
    Signed-off-by: Gu Zheng <guz.fnst@cn.fujitsu.com>
    [Jaegeuk Kim: adjust minor coding standard]
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 941f9b9ca3a5..2535d3b1a763 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -560,9 +560,9 @@ static int f2fs_write_data_page(struct page *page,
 		inode_dec_dirty_dents(inode);
 		err = do_write_data_page(page);
 	} else {
-		int ilock = mutex_lock_op(sbi);
+		f2fs_lock_op(sbi);
 		err = do_write_data_page(page);
-		mutex_unlock_op(sbi, ilock);
+		f2fs_unlock_op(sbi);
 		need_balance_fs = true;
 	}
 	if (err == -ENOENT)
@@ -641,7 +641,6 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 	pgoff_t index = ((unsigned long long) pos) >> PAGE_CACHE_SHIFT;
 	struct dnode_of_data dn;
 	int err = 0;
-	int ilock;
 
 	f2fs_balance_fs(sbi);
 repeat:
@@ -650,7 +649,7 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 		return -ENOMEM;
 	*pagep = page;
 
-	ilock = mutex_lock_op(sbi);
+	f2fs_lock_op(sbi);
 
 	set_new_dnode(&dn, inode, NULL, NULL, 0);
 	err = get_dnode_of_data(&dn, index, ALLOC_NODE);
@@ -664,7 +663,7 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 	if (err)
 		goto err;
 
-	mutex_unlock_op(sbi, ilock);
+	f2fs_unlock_op(sbi);
 
 	if ((len == PAGE_CACHE_SIZE) || PageUptodate(page))
 		return 0;
@@ -700,7 +699,7 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 	return 0;
 
 err:
-	mutex_unlock_op(sbi, ilock);
+	f2fs_unlock_op(sbi);
 	f2fs_put_page(page, 1);
 	return err;
 }

commit de93653fe31fc9439971296842dcd0280f8ab5f4
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Mon Aug 12 21:08:03 2013 +0900

    f2fs: reserve the xattr space dynamically
    
    This patch enables the number of direct pointers inside on-disk inode block to
    be changed dynamically according to the size of inline xattr space.
    
    The number of direct pointers, ADDRS_PER_INODE, can be changed only if the file
    has inline xattr flag.
    
    The number of direct pointers that will be used by inline xattrs is defined as
    F2FS_INLINE_XATTR_ADDRS.
    Current patch assigns F2FS_INLINE_XATTR_ADDRS to 0 temporarily.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 6b328de41728..941f9b9ca3a5 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -117,7 +117,8 @@ void update_extent_cache(block_t blk_addr, struct dnode_of_data *dn)
 	block_t start_blkaddr, end_blkaddr;
 
 	BUG_ON(blk_addr == NEW_ADDR);
-	fofs = start_bidx_of_node(ofs_of_node(dn->node_page)) + dn->ofs_in_node;
+	fofs = start_bidx_of_node(ofs_of_node(dn->node_page), fi) +
+							dn->ofs_in_node;
 
 	/* Update the page address in the parent node */
 	__set_data_blkaddr(dn, blk_addr);
@@ -448,7 +449,7 @@ static int get_data_block_ro(struct inode *inode, sector_t iblock,
 		unsigned int end_offset;
 
 		end_offset = IS_INODE(dn.node_page) ?
-				ADDRS_PER_INODE :
+				ADDRS_PER_INODE(F2FS_I(inode)) :
 				ADDRS_PER_BLOCK;
 
 		clear_buffer_new(bh_result);

commit d59ff4df7b7ae39e6fb047db9e83cd899b5764f1
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Tue Aug 20 19:13:07 2013 +0900

    f2fs: fix wrong BUG_ON condition
    
    This patch removes a false-alaramed BUG_ON.
    The previous BUG_ON condition didn't cover the following true scenario.
    
    In f2fs_add_link, 1) get_new_data_page gives an uptodate page successfully,
    and then, 2) init_inode_metadata returns -ENOSPC.
    At this moment, a new clean data page is remained in the page cache, but its
    block address still indicates NEW_ADDR.
    After then, even if sync is called, this clean data page cannot be written to
    the disk due to the clean state.
    
    So this means that get_lock_data_page should make a new empty page when its
    block address is NEW_ADDR and its page is not uptodated.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index ea3cb29018e9..6b328de41728 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -259,8 +259,17 @@ struct page *get_lock_data_page(struct inode *inode, pgoff_t index)
 	if (PageUptodate(page))
 		return page;
 
-	BUG_ON(dn.data_blkaddr == NEW_ADDR);
-	BUG_ON(dn.data_blkaddr == NULL_ADDR);
+	/*
+	 * A new dentry page is allocated but not able to be written, since its
+	 * new inode page couldn't be allocated due to -ENOSPC.
+	 * In such the case, its blkaddr can be remained as NEW_ADDR.
+	 * see, f2fs_add_link -> get_new_data_page -> init_inode_metadata.
+	 */
+	if (dn.data_blkaddr == NEW_ADDR) {
+		zero_user_segment(page, 0, PAGE_CACHE_SIZE);
+		SetPageUptodate(page);
+		return page;
+	}
 
 	err = f2fs_readpage(sbi, page, dn.data_blkaddr, READ_SYNC);
 	if (err)

commit 41dfde135f9169948dd0c9bba948774f2e521210
Author: Gu Zheng <guz.fnst@cn.fujitsu.com>
Date:   Fri Aug 9 18:21:24 2013 +0800

    f2fs: clean up the needless end 'return' of void function
    
    Signed-off-by: Gu Zheng <guz.fnst@cn.fujitsu.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index a7eb52925723..ea3cb29018e9 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -176,7 +176,6 @@ void update_extent_cache(block_t blk_addr, struct dnode_of_data *dn)
 end_update:
 	write_unlock(&fi->ext.ext_lock);
 	sync_inode_page(dn);
-	return;
 }
 
 struct page *find_data_page(struct inode *inode, pgoff_t index, bool sync)

commit a569469e967022d9ceeaa4b73619f96614087d2d
Author: Jin Xu <jinuxstyle@gmail.com>
Date:   Mon Aug 5 20:02:04 2013 +0800

    f2fs: fix a deadlock in fsync
    
    This patch fixes a deadlock bug that occurs quite often when there are
    concurrent write and fsync on a same file.
    
    Following is the simplified call trace when tasks get hung.
    
    fsync thread:
    - f2fs_sync_file
     ...
     - f2fs_write_data_pages
     ...
      - update_extent_cache
      ...
       - update_inode
        - wait_on_page_writeback
    
    bdi writeback thread
    - __writeback_single_inode
     - f2fs_write_data_pages
      - mutex_lock(sbi->writepages)
    
    The deadlock happens when the fsync thread waits on a inode page that has
    been added to the f2fs' cached bio sbi->bio[NODE], and unfortunately,
    no one else could be able to submit the cached bio to block layer for
    writeback. This is because the fsync thread already hold a sbi->fs_lock and
    the sbi->writepages lock, causing the bdi thread being blocked when attempt
    to write data pages for the same inode. At the same time, f2fs_gc thread
    does not notice the situation and could not help. Even the sync syscall
    gets blocked.
    
    To fix it, we could submit the cached bio first before waiting on a inode page
    that is being written back.
    
    Signed-off-by: Jin Xu <jinuxstyle@gmail.com>
    [Jaegeuk Kim: add more cases to use f2fs_wait_on_page_writeback]
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index f458883af815..a7eb52925723 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -37,7 +37,7 @@ static void __set_data_blkaddr(struct dnode_of_data *dn, block_t new_addr)
 	struct page *node_page = dn->node_page;
 	unsigned int ofs_in_node = dn->ofs_in_node;
 
-	wait_on_page_writeback(node_page);
+	f2fs_wait_on_page_writeback(node_page, NODE, false);
 
 	rn = F2FS_NODE(node_page);
 

commit df273efc36024a9ea97fd687d638a70c3ea8c37a
Author: Namjae Jeon <namjae.jeon@samsung.com>
Date:   Sun Aug 4 23:10:28 2013 +0900

    f2fs: remove redundant code from f2fs_write_begin
    
    This code is being used for nobh_write_end() function.
    But since now f2fs_write_end function is added so
    there is no need for this code.
    
    Signed-off-by: Namjae Jeon <namjae.jeon@samsung.com>
    Signed-off-by: Pankaj Kumar <pankaj.km@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 027341cacd2b..f458883af815 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -634,9 +634,6 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 	int err = 0;
 	int ilock;
 
-	/* for nobh_write_end */
-	*fsdata = NULL;
-
 	f2fs_balance_fs(sbi);
 repeat:
 	page = grab_cache_page_write_begin(mapping, index, flags);

commit f0c5e565bb05a4cd6105bb197c56078462252e78
Author: Dan Carpenter <dan.carpenter@oracle.com>
Date:   Wed Jul 31 11:44:13 2013 +0300

    f2fs: remove an unneeded kfree(NULL)
    
    This kfree() is no longer needed after a79dc083d7 "f2fs: move
    bio_private allocation out of f2fs_bio_alloc()".  The "bio->bi_private"
    is NULL here so it's a no-op.
    
    Signed-off-by: Dan Carpenter <dan.carpenter@oracle.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 19cd7c6e964c..027341cacd2b 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -390,7 +390,6 @@ int f2fs_readpage(struct f2fs_sb_info *sbi, struct page *page,
 	bio->bi_end_io = read_end_io;
 
 	if (bio_add_page(bio, page, PAGE_CACHE_SIZE, 0) < PAGE_CACHE_SIZE) {
-		kfree(bio->bi_private);
 		bio_put(bio);
 		up_read(&sbi->bio_sem);
 		f2fs_put_page(page, 1);

commit d8207f69589c74037128ff6c9e1a44223fad3b7c
Author: Gu Zheng <guz.fnst@cn.fujitsu.com>
Date:   Thu Jul 25 11:30:01 2013 +0800

    f2fs: move bio_private allocation out of f2fs_bio_alloc()
    
    bio->bi_private is not always needed. As in the reading data path,
    end_read_io does not need bio_private for further using, so moving
    bio_private allocation out of f2fs_bio_alloc(). Alloc it in the
    submit_write_page(), and ignore it in the f2fs_readpage().
    
    Signed-off-by: Gu Zheng <guz.fnst@cn.fujitsu.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index c73c394c3d8c..19cd7c6e964c 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -365,7 +365,6 @@ static void read_end_io(struct bio *bio, int err)
 		}
 		unlock_page(page);
 	} while (bvec >= bio->bi_io_vec);
-	kfree(bio->bi_private);
 	bio_put(bio);
 }
 

commit 4559071063270999d016c92a0b9241692cbbb522
Author: Gu Zheng <guz.fnst@cn.fujitsu.com>
Date:   Mon Jul 15 17:57:38 2013 +0800

    f2fs: introduce help function F2FS_NODE()
    
    Introduce help function F2FS_NODE() to simplify the conversion of node_page to
    f2fs_node.
    
    Signed-off-by: Gu Zheng <guz.fnst@cn.fujitsu.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 035f9a345cdf..c73c394c3d8c 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -39,7 +39,7 @@ static void __set_data_blkaddr(struct dnode_of_data *dn, block_t new_addr)
 
 	wait_on_page_writeback(node_page);
 
-	rn = (struct f2fs_node *)page_address(node_page);
+	rn = F2FS_NODE(node_page);
 
 	/* Get physical address of data block */
 	addr_array = blkaddr_in_node(rn);

commit 3f490f7f99053288bd85563f8d9b5032b810e177
Merge: c4eb1b07303a a1dd3c13ce65
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jul 2 09:42:38 2013 -0700

    Merge tag 'for-f2fs-3.11' of git://git.kernel.org/pub/scm/linux/kernel/git/jaegeuk/f2fs
    
    Pull f2fs updates from Jaegeuk Kim:
     "This patch-set includes the following major enhancement patches:
       - remount_fs callback function
       - restore parent inode number to enhance the fsync performance
       - xattr security labels
       - reduce the number of redundant lock/unlock data pages
       - avoid frequent write_inode calls
    
      The other minor bug fixes are as follows.
       - endian conversion bugs
       - various bugs in the roll-forward recovery routine"
    
    * tag 'for-f2fs-3.11' of git://git.kernel.org/pub/scm/linux/kernel/git/jaegeuk/f2fs: (56 commits)
      f2fs: fix to recover i_size from roll-forward
      f2fs: remove the unused argument "sbi" of func destroy_fsync_dnodes()
      f2fs: remove reusing any prefree segments
      f2fs: code cleanup and simplify in func {find/add}_gc_inode
      f2fs: optimize the init_dirty_segmap function
      f2fs: fix an endian conversion bug detected by sparse
      f2fs: fix crc endian conversion
      f2fs: add remount_fs callback support
      f2fs: recover wrong pino after checkpoint during fsync
      f2fs: optimize do_write_data_page()
      f2fs: make locate_dirty_segment() as static
      f2fs: remove unnecessary parameter "offset" from __add_sum_entry()
      f2fs: avoid freqeunt write_inode calls
      f2fs: optimise the truncate_data_blocks_range() range
      f2fs: use the F2FS specific flags in f2fs_ioctl()
      f2fs: sync dir->i_size with its block allocation
      f2fs: fix i_blocks translation on various types of files
      f2fs: set sb->s_fs_info before calling parse_options()
      f2fs: support xattr security labels
      f2fs: fix iget/iput of dir during recovery
      ...

commit a1dd3c13ce65b726fddfe72b9d2f1009db983ce6
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Thu Jun 27 13:04:08 2013 +0900

    f2fs: fix to recover i_size from roll-forward
    
    If user requests many data writes and fsync together, the last updated i_size
    should be stored to the inode block consistently.
    
    But, previous write_end just marks the inode as dirty and doesn't update its
    metadata into its inode block.
    After that, fsync just writes the inode block with newly updated data index
    excluding inode metadata updates.
    
    So, this patch introduces write_end in which updates inode block too when the
    i_size is changed.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 6d4a743caf86..e88f46f122aa 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -701,6 +701,27 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 	return err;
 }
 
+static int f2fs_write_end(struct file *file,
+			struct address_space *mapping,
+			loff_t pos, unsigned len, unsigned copied,
+			struct page *page, void *fsdata)
+{
+	struct inode *inode = page->mapping->host;
+
+	SetPageUptodate(page);
+	set_page_dirty(page);
+
+	if (pos + copied > i_size_read(inode)) {
+		i_size_write(inode, pos + copied);
+		mark_inode_dirty(inode);
+		update_inode_page(inode);
+	}
+
+	unlock_page(page);
+	page_cache_release(page);
+	return copied;
+}
+
 static ssize_t f2fs_direct_IO(int rw, struct kiocb *iocb,
 		const struct iovec *iov, loff_t offset, unsigned long nr_segs)
 {
@@ -757,7 +778,7 @@ const struct address_space_operations f2fs_dblock_aops = {
 	.writepage	= f2fs_write_data_page,
 	.writepages	= f2fs_write_data_pages,
 	.write_begin	= f2fs_write_begin,
-	.write_end	= nobh_write_end,
+	.write_end	= f2fs_write_end,
 	.set_page_dirty	= f2fs_set_data_page_dirty,
 	.invalidatepage	= f2fs_invalidate_data_page,
 	.releasepage	= f2fs_release_data_page,

commit b25958b6ecf1dce087e62b9aa27cf8f2fe9b5c86
Author: Haicheng Li <haicheng.li@linux.intel.com>
Date:   Thu Jun 13 16:59:29 2013 +0800

    f2fs: optimize do_write_data_page()
    
    Since "need_inplace_update() == true" is a very rare case, using unlikely()
    to give compiler a chance to optimize the code.
    
    Signed-off-by: Haicheng Li <haicheng.li@linux.intel.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 5b145fcc2864..6d4a743caf86 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -497,8 +497,9 @@ int do_write_data_page(struct page *page)
 	 * If current allocation needs SSR,
 	 * it had better in-place writes for updated data.
 	 */
-	if (old_blk_addr != NEW_ADDR && !is_cold_data(page) &&
-				need_inplace_update(inode)) {
+	if (unlikely(old_blk_addr != NEW_ADDR &&
+			!is_cold_data(page) &&
+			need_inplace_update(inode))) {
 		rewrite_data_page(F2FS_SB(inode->i_sb), page,
 						old_blk_addr);
 	} else {

commit 699489bbbea4fc3b9b735d69941cf4fca91ce1d5
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Fri Jun 7 22:08:23 2013 +0900

    f2fs: sync dir->i_size with its block allocation
    
    If new dentry block is allocated and its i_size is updated, we should update
    its inode block together in order to sync i_size and its block allocation.
    Otherwise, we can loose additional dentry block due to the unconsistent i_size.
    
    Errorneous Scenario
    -------------------
    
    In the recovery routine,
     - recovery_dentry
     | - __f2fs_add_link
     | | - get_new_data_page
     | | | - i_size_write(new_i_size)
     | | | - mark_inode_dirty_sync(dir)
     | | - update_parent_metadata
     | | | - mark_inode_dirty(dir)
     |
     - write_checkpoint
       - sync_dirty_dir_inodes
         - filemap_flush(dentry_blocks)
           - f2fs_write_data_page
             - skip to write the last dentry block due to index < i_size
    
    In the above flow, new_i_size is not updated to its inode block so that the
    last dentry block will be lost accordingly.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 93917e31dbdf..5b145fcc2864 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -339,6 +339,8 @@ struct page *get_new_data_page(struct inode *inode,
 	if (new_i_size &&
 		i_size_read(inode) < ((index + 1) << PAGE_CACHE_SHIFT)) {
 		i_size_write(inode, ((index + 1) << PAGE_CACHE_SHIFT));
+		/* Only the directory inode sets new_i_size */
+		set_inode_flag(F2FS_I(inode), FI_UPDATE_DIR);
 		mark_inode_dirty_sync(inode);
 	}
 	return page;

commit 35b09d82c3cf3fc0b8b6d923e7fd82ff7926aafc
Author: Namjae Jeon <namjae.jeon@samsung.com>
Date:   Thu May 23 22:57:53 2013 +0900

    f2fs: push some variables to debug part
    
    Some, counters are needed only for the statistical information
    while debugging.
    So, those can be controlled using CONFIG_F2FS_STAT_FS,
    pushing the usage for few variables under this flag.
    
    Signed-off-by: Namjae Jeon <namjae.jeon@samsung.com>
    Signed-off-by: Amit Sahrawat <a.sahrawat@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 1644fffea251..93917e31dbdf 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -68,7 +68,9 @@ static int check_extent_cache(struct inode *inode, pgoff_t pgofs,
 					struct buffer_head *bh_result)
 {
 	struct f2fs_inode_info *fi = F2FS_I(inode);
+#ifdef CONFIG_F2FS_STAT_FS
 	struct f2fs_sb_info *sbi = F2FS_SB(inode->i_sb);
+#endif
 	pgoff_t start_fofs, end_fofs;
 	block_t start_blkaddr;
 
@@ -78,7 +80,9 @@ static int check_extent_cache(struct inode *inode, pgoff_t pgofs,
 		return 0;
 	}
 
+#ifdef CONFIG_F2FS_STAT_FS
 	sbi->total_hit_ext++;
+#endif
 	start_fofs = fi->ext.fofs;
 	end_fofs = fi->ext.fofs + fi->ext.len - 1;
 	start_blkaddr = fi->ext.blk_addr;
@@ -96,7 +100,9 @@ static int check_extent_cache(struct inode *inode, pgoff_t pgofs,
 		else
 			bh_result->b_size = UINT_MAX;
 
+#ifdef CONFIG_F2FS_STAT_FS
 		sbi->read_hit_ext++;
+#endif
 		read_unlock(&fi->ext.ext_lock);
 		return 1;
 	}

commit 6f85b3520325a67ee4ac33e75bbcdbc25c79ce69
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Mon May 20 16:15:22 2013 +0900

    f2fs: avoid RECLAIM_FS-ON-W: deadlock
    
    This patch tries to avoid the following deadlock condition of which the reclaim
    path can trigger f2fs_balance_fs again.
    
    =================================
    [ INFO: inconsistent lock state ]
    ---------------------------------
    inconsistent {RECLAIM_FS-ON-W} -> {IN-RECLAIM_FS-W} usage.
    kswapd0/41 [HC0[0]:SC0[0]:HE1:SE1] takes:
     (&sbi->gc_mutex){+.+.?.}, at: f2fs_balance_fs+0xe6/0x100 [f2fs]
    {RECLAIM_FS-ON-W} state was registered at:
      [<ffffffff810aa5a9>] mark_held_locks+0xb9/0x140
      [<ffffffff810aae85>] lockdep_trace_alloc+0x85/0xf0
      [<ffffffff8113ab2c>] __alloc_pages_nodemask+0x7c/0x9b0
      [<ffffffff81175aa8>] alloc_pages_current+0xb8/0x180
      [<ffffffff811319cf>] __page_cache_alloc+0xaf/0xd0
      [<ffffffff8113225c>] find_or_create_page+0x4c/0xb0
      [<ffffffffa021359e>] find_data_page+0x14e/0x210 [f2fs]
      [<ffffffffa021161b>] f2fs_gc+0x9eb/0xd90 [f2fs]
      [<ffffffffa0218fae>] f2fs_balance_fs+0xee/0x100 [f2fs]
      [<ffffffffa020848c>] f2fs_setattr+0x6c/0x200 [f2fs]
      [<ffffffff811ae51b>] notify_change+0x1db/0x3a0
      [<ffffffff8118fbd0>] do_truncate+0x60/0xa0
      [<ffffffff8118fd95>] vfs_truncate+0x185/0x1b0
      [<ffffffff8118fe1c>] do_sys_truncate+0x5c/0xa0
      [<ffffffff8118ffee>] SyS_truncate+0xe/0x10
      [<ffffffff816e2b42>] system_call_fastpath+0x16/0x1b
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index c320f7f31327..1644fffea251 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -199,7 +199,7 @@ struct page *find_data_page(struct inode *inode, pgoff_t index, bool sync)
 	if (dn.data_blkaddr == NEW_ADDR)
 		return ERR_PTR(-EINVAL);
 
-	page = grab_cache_page(mapping, index);
+	page = grab_cache_page_write_begin(mapping, index, AOP_FLAG_NOFS);
 	if (!page)
 		return ERR_PTR(-ENOMEM);
 
@@ -234,7 +234,7 @@ struct page *get_lock_data_page(struct inode *inode, pgoff_t index)
 	int err;
 
 repeat:
-	page = grab_cache_page(mapping, index);
+	page = grab_cache_page_write_begin(mapping, index, AOP_FLAG_NOFS);
 	if (!page)
 		return ERR_PTR(-ENOMEM);
 

commit 44a83ff6a81d84ab83bcb43a49ff1ba6c7e17cd1
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Mon May 20 10:10:29 2013 +0900

    f2fs: update inode page after creation
    
    I found a bug when testing power-off-recovery as follows.
    
    [Bug Scenario]
    1. create a file
    2. fsync the file
    3. reboot w/o any sync
    4. try to recover the file
     - found its fsync mark
     - found its dentry mark
       : try to recover its dentry
        - get its file name
        - get its parent inode number
         : here we got zero value
    
    The reason why we get the wrong parent inode number is that we didn't
    synchronize the inode page with its newly created inode information perfectly.
    
    Especially, previous f2fs stores fi->i_pino and writes it to the cached
    node page in a wrong order, which incurs the zero-valued i_pino during the
    recovery.
    
    So, this patch modifies the creation flow to fix the synchronization order of
    inode page with its inode.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index af7454939362..c320f7f31327 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -279,6 +279,7 @@ struct page *get_lock_data_page(struct inode *inode, pgoff_t index)
  *
  * Also, caller should grab and release a mutex by calling mutex_lock_op() and
  * mutex_unlock_op().
+ * Note that, npage is set only by make_empty_dir.
  */
 struct page *get_new_data_page(struct inode *inode,
 		struct page *npage, pgoff_t index, bool new_i_size)

commit 64aa7ed98db489d1c41ef140876ada38498678ab
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Mon May 20 09:55:50 2013 +0900

    f2fs: change get_new_data_page to pass a locked node page
    
    This patch is for passing a locked node page to get_dnode_of_data.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 05fb5c6077b8..af7454939362 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -280,8 +280,8 @@ struct page *get_lock_data_page(struct inode *inode, pgoff_t index)
  * Also, caller should grab and release a mutex by calling mutex_lock_op() and
  * mutex_unlock_op().
  */
-struct page *get_new_data_page(struct inode *inode, pgoff_t index,
-						bool new_i_size)
+struct page *get_new_data_page(struct inode *inode,
+		struct page *npage, pgoff_t index, bool new_i_size)
 {
 	struct f2fs_sb_info *sbi = F2FS_SB(inode->i_sb);
 	struct address_space *mapping = inode->i_mapping;
@@ -289,18 +289,20 @@ struct page *get_new_data_page(struct inode *inode, pgoff_t index,
 	struct dnode_of_data dn;
 	int err;
 
-	set_new_dnode(&dn, inode, NULL, NULL, 0);
+	set_new_dnode(&dn, inode, npage, npage, 0);
 	err = get_dnode_of_data(&dn, index, ALLOC_NODE);
 	if (err)
 		return ERR_PTR(err);
 
 	if (dn.data_blkaddr == NULL_ADDR) {
 		if (reserve_new_block(&dn)) {
-			f2fs_put_dnode(&dn);
+			if (!npage)
+				f2fs_put_dnode(&dn);
 			return ERR_PTR(-ENOSPC);
 		}
 	}
-	f2fs_put_dnode(&dn);
+	if (!npage)
+		f2fs_put_dnode(&dn);
 repeat:
 	page = grab_cache_page(mapping, index);
 	if (!page)

commit 650495dedc34daf8590c708a5b48f82ed2787b75
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Mon May 13 08:38:35 2013 +0900

    f2fs: fix the inconsistent state of data pages
    
    In get_lock_data_page, if there is a data race between get_dnode_of_data for
    node and grab_cache_page for data, f2fs is able to face with the following
    BUG_ON(dn.data_blkaddr == NEW_ADDR).
    
    kernel BUG at /home/zeus/f2fs_test/src/fs/f2fs/data.c:251!
     [<ffffffffa044966c>] get_lock_data_page+0x1ec/0x210 [f2fs]
    Call Trace:
     [<ffffffffa043b089>] f2fs_readdir+0x89/0x210 [f2fs]
     [<ffffffff811a0920>] ? fillonedir+0x100/0x100
     [<ffffffff811a0920>] ? fillonedir+0x100/0x100
     [<ffffffff811a07f8>] vfs_readdir+0xb8/0xe0
     [<ffffffff811a0b4f>] sys_getdents+0x8f/0x110
     [<ffffffff816d7999>] system_call_fastpath+0x16/0x1b
    
    This bug is able to be occurred when the block address of the data block is
    changed after f2fs_put_dnode().
    In order to avoid that, this patch fixes the lock order of node and data
    blocks in which the node block lock is covered by the data block lock.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 91ff93b0b0f4..05fb5c6077b8 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -233,18 +233,23 @@ struct page *get_lock_data_page(struct inode *inode, pgoff_t index)
 	struct page *page;
 	int err;
 
+repeat:
+	page = grab_cache_page(mapping, index);
+	if (!page)
+		return ERR_PTR(-ENOMEM);
+
 	set_new_dnode(&dn, inode, NULL, NULL, 0);
 	err = get_dnode_of_data(&dn, index, LOOKUP_NODE);
-	if (err)
+	if (err) {
+		f2fs_put_page(page, 1);
 		return ERR_PTR(err);
+	}
 	f2fs_put_dnode(&dn);
 
-	if (dn.data_blkaddr == NULL_ADDR)
+	if (dn.data_blkaddr == NULL_ADDR) {
+		f2fs_put_page(page, 1);
 		return ERR_PTR(-ENOENT);
-repeat:
-	page = grab_cache_page(mapping, index);
-	if (!page)
-		return ERR_PTR(-ENOMEM);
+	}
 
 	if (PageUptodate(page))
 		return page;

commit d47992f86b307985b3215bcf141d56d1849d71df
Author: Lukas Czerner <lczerner@redhat.com>
Date:   Tue May 21 23:17:23 2013 -0400

    mm: change invalidatepage prototype to accept length
    
    Currently there is no way to truncate partial page where the end
    truncate point is not at the end of the page. This is because it was not
    needed and the functionality was enough for file system truncate
    operation to work properly. However more file systems now support punch
    hole feature and it can benefit from mm supporting truncating page just
    up to the certain point.
    
    Specifically, with this functionality truncate_inode_pages_range() can
    be changed so it supports truncating partial page at the end of the
    range (currently it will BUG_ON() if 'end' is not at the end of the
    page).
    
    This commit changes the invalidatepage() address space operation
    prototype to accept range to be invalidated and update all the instances
    for it.
    
    We also change the block_invalidatepage() in the same way and actually
    make a use of the new length argument implementing range invalidation.
    
    Actual file system implementations will follow except the file systems
    where the changes are really simple and should not change the behaviour
    in any way .Implementation for truncate_page_range() which will be able
    to accept page unaligned ranges will follow as well.
    
    Signed-off-by: Lukas Czerner <lczerner@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Hugh Dickins <hughd@google.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 91ff93b0b0f4..ce11d9a92aed 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -698,7 +698,8 @@ static ssize_t f2fs_direct_IO(int rw, struct kiocb *iocb,
 						  get_data_block_ro);
 }
 
-static void f2fs_invalidate_data_page(struct page *page, unsigned long offset)
+static void f2fs_invalidate_data_page(struct page *page, unsigned int offset,
+				      unsigned int length)
 {
 	struct inode *inode = page->mapping->host;
 	struct f2fs_sb_info *sbi = F2FS_SB(inode->i_sb);

commit 942d33da999b86821c9aee9615fcb81207ee04c7
Merge: 246e6a0d7810 59bbd474abb9
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed May 8 15:11:48 2013 -0700

    Merge tag 'f2fs-for-v3.10' of git://git.kernel.org/pub/scm/linux/kernel/git/jaegeuk/f2fs
    
    Pull f2fs updates from Jaegeuk Kim:
     "This patch-set includes the following major enhancement patches.
       - introduce a new gloabl lock scheme
       - add tracepoints on several major functions
       - fix the overall cleaning process focused on victim selection
       - apply the block plugging to merge IOs as much as possible
       - enhance management of free nids and its list
       - enhance the readahead mode for node pages
       - address several cretical deadlock conditions
       - reduce lock_page calls
    
      The other minor bug fixes and enhancements are as follows.
       - calculation mistakes: overflow
       - bio types: READ, READA, and READ_SYNC
       - fix the recovery flow, data races, and null pointer errors"
    
    * tag 'f2fs-for-v3.10' of git://git.kernel.org/pub/scm/linux/kernel/git/jaegeuk/f2fs: (68 commits)
      f2fs: cover free_nid management with spin_lock
      f2fs: optimize scan_nat_page()
      f2fs: code cleanup for scan_nat_page() and build_free_nids()
      f2fs: bugfix for alloc_nid_failed()
      f2fs: recover when journal contains deleted files
      f2fs: continue to mount after failing recovery
      f2fs: avoid deadlock during evict after f2fs_gc
      f2fs: modify the number of issued pages to merge IOs
      f2fs: remove useless #include <linux/proc_fs.h> as we're now using sysfs as debug entry.
      f2fs: fix inconsistent using of NM_WOUT_THRESHOLD
      f2fs: check truncation of mapping after lock_page
      f2fs: enhance alloc_nid and build_free_nids flows
      f2fs: add a tracepoint on f2fs_new_inode
      f2fs: check nid == 0 in add_free_nid
      f2fs: add REQ_META about metadata requests for submit
      f2fs: give a chance to merge IOs by IO scheduler
      f2fs: avoid frequent background GC
      f2fs: add tracepoints to debug checkpoint request
      f2fs: add tracepoints for write page operations
      f2fs: add tracepoints to debug the block allocation
      ...

commit 531ad7d58c6476c5856653448b4c7d26427502b4
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Tue Apr 30 11:33:27 2013 +0900

    f2fs: avoid deadlock during evict after f2fs_gc
    
    o Deadlock case #1
    
    Thread 1:
    - writeback_sb_inodes
     - do_writepages
      - f2fs_write_data_pages
       - write_cache_pages
        - f2fs_write_data_page
         - f2fs_balance_fs
          - wait mutex_lock(gc_mutex)
    
    Thread 2:
    - f2fs_balance_fs
     - mutex_lock(gc_mutex)
     - f2fs_gc
      - f2fs_iget
       - wait iget_locked(inode->i_lock)
    
    Thread 3:
    - do_unlinkat
     - iput
      - lock(inode->i_lock)
       - evict
        - inode_wait_for_writeback
    
    o Deadlock case #2
    
    Thread 1:
    - __writeback_single_inode
     : set I_SYNC
      - do_writepages
       - f2fs_write_data_page
        - f2fs_balance_fs
         - f2fs_gc
          - iput
           - evict
            - inode_wait_for_writeback(I_SYNC)
    
    In order to avoid this, even though iput is called with the zero-reference
    count, we need to stop the eviction procedure if the inode is on writeback.
    So this patch links f2fs_drop_inode which checks the I_SYNC flag.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 2db9380f5dda..61b44542417c 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -577,6 +577,7 @@ static int f2fs_write_data_pages(struct address_space *mapping,
 {
 	struct inode *inode = mapping->host;
 	struct f2fs_sb_info *sbi = F2FS_SB(inode->i_sb);
+	bool locked = false;
 	int ret;
 	long excess_nrtw = 0, desired_nrtw;
 
@@ -590,10 +591,12 @@ static int f2fs_write_data_pages(struct address_space *mapping,
 		wbc->nr_to_write = desired_nrtw;
 	}
 
-	if (!S_ISDIR(inode->i_mode))
+	if (!S_ISDIR(inode->i_mode)) {
 		mutex_lock(&sbi->writepages);
+		locked = true;
+	}
 	ret = write_cache_pages(mapping, wbc, __f2fs_writepage, mapping);
-	if (!S_ISDIR(inode->i_mode))
+	if (locked)
 		mutex_unlock(&sbi->writepages);
 	f2fs_submit_bio(sbi, DATA, (wbc->sync_mode == WB_SYNC_ALL));
 

commit a27bb332c04cec8c4afd7912df0dc7890db27560
Author: Kent Overstreet <koverstreet@google.com>
Date:   Tue May 7 16:19:08 2013 -0700

    aio: don't include aio.h in sched.h
    
    Faster kernel compiles by way of fewer unnecessary includes.
    
    [akpm@linux-foundation.org: fix fallout]
    [akpm@linux-foundation.org: fix build]
    Signed-off-by: Kent Overstreet <koverstreet@google.com>
    Cc: Zach Brown <zab@redhat.com>
    Cc: Felipe Balbi <balbi@ti.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Mark Fasheh <mfasheh@suse.com>
    Cc: Joel Becker <jlbec@evilplan.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Asai Thambi S P <asamymuthupa@micron.com>
    Cc: Selvan Mani <smani@micron.com>
    Cc: Sam Bradshaw <sbradshaw@micron.com>
    Cc: Jeff Moyer <jmoyer@redhat.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Benjamin LaHaise <bcrl@kvack.org>
    Reviewed-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 7bd22a201125..d0ed4ba4b61b 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -12,6 +12,7 @@
 #include <linux/f2fs_fs.h>
 #include <linux/buffer_head.h>
 #include <linux/mpage.h>
+#include <linux/aio.h>
 #include <linux/writeback.h>
 #include <linux/backing-dev.h>
 #include <linux/blkdev.h>

commit afcb7ca01f47b0481e0b248d1542d0934fa70767
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Fri Apr 26 11:55:17 2013 +0900

    f2fs: check truncation of mapping after lock_page
    
    We call lock_page when we need to update a page after readpage.
    Between grab and lock page, the page can be truncated by other thread.
    So, we should check the page after lock_page whether it was truncated or not.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index eba7e84d1ffd..2db9380f5dda 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -240,7 +240,7 @@ struct page *get_lock_data_page(struct inode *inode, pgoff_t index)
 
 	if (dn.data_blkaddr == NULL_ADDR)
 		return ERR_PTR(-ENOENT);
-
+repeat:
 	page = grab_cache_page(mapping, index);
 	if (!page)
 		return ERR_PTR(-ENOMEM);
@@ -260,6 +260,10 @@ struct page *get_lock_data_page(struct inode *inode, pgoff_t index)
 		f2fs_put_page(page, 1);
 		return ERR_PTR(-EIO);
 	}
+	if (page->mapping != mapping) {
+		f2fs_put_page(page, 1);
+		goto repeat;
+	}
 	return page;
 }
 
@@ -291,7 +295,7 @@ struct page *get_new_data_page(struct inode *inode, pgoff_t index,
 		}
 	}
 	f2fs_put_dnode(&dn);
-
+repeat:
 	page = grab_cache_page(mapping, index);
 	if (!page)
 		return ERR_PTR(-ENOMEM);
@@ -311,6 +315,10 @@ struct page *get_new_data_page(struct inode *inode, pgoff_t index,
 			f2fs_put_page(page, 1);
 			return ERR_PTR(-EIO);
 		}
+		if (page->mapping != mapping) {
+			f2fs_put_page(page, 1);
+			goto repeat;
+		}
 	}
 
 	if (new_i_size &&
@@ -611,7 +619,7 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 	*fsdata = NULL;
 
 	f2fs_balance_fs(sbi);
-
+repeat:
 	page = grab_cache_page_write_begin(mapping, index, flags);
 	if (!page)
 		return -ENOMEM;
@@ -656,6 +664,10 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 			f2fs_put_page(page, 1);
 			return -EIO;
 		}
+		if (page->mapping != mapping) {
+			f2fs_put_page(page, 1);
+			goto repeat;
+		}
 	}
 out:
 	SetPageUptodate(page);

commit c718379b6b0954a04a153d7e5dc8b3136a301ee6
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Wed Apr 24 13:19:56 2013 +0900

    f2fs: give a chance to merge IOs by IO scheduler
    
    Previously, background GC submits many 4KB read requests to load victim blocks
    and/or its (i)node blocks.
    
    ...
    f2fs_gc : f2fs_readpage: ino = 1, page_index = 0xb61, blkaddr = 0x3b964ed
    f2fs_gc : block_rq_complete: 8,16 R () 499854968 + 8 [0]
    f2fs_gc : f2fs_readpage: ino = 1, page_index = 0xb6f, blkaddr = 0x3b964ee
    f2fs_gc : block_rq_complete: 8,16 R () 499854976 + 8 [0]
    f2fs_gc : f2fs_readpage: ino = 1, page_index = 0xb79, blkaddr = 0x3b964ef
    f2fs_gc : block_rq_complete: 8,16 R () 499854984 + 8 [0]
    ...
    
    However, by the fact that many IOs are sequential, we can give a chance to merge
    the IOs by IO scheduler.
    In order to do that, let's use blk_plug.
    
    ...
    f2fs_gc : f2fs_iget: ino = 143
    f2fs_gc : f2fs_readpage: ino = 143, page_index = 0x1c6, blkaddr = 0x2e6ee
    f2fs_gc : f2fs_iget: ino = 143
    f2fs_gc : f2fs_readpage: ino = 143, page_index = 0x1c7, blkaddr = 0x2e6ef
    <idle> : block_rq_complete: 8,16 R () 1519616 + 8 [0]
    <idle> : block_rq_complete: 8,16 R () 1519848 + 8 [0]
    <idle> : block_rq_complete: 8,16 R () 1520432 + 96 [0]
    <idle> : block_rq_complete: 8,16 R () 1520536 + 104 [0]
    <idle> : block_rq_complete: 8,16 R () 1521008 + 112 [0]
    <idle> : block_rq_complete: 8,16 R () 1521440 + 152 [0]
    <idle> : block_rq_complete: 8,16 R () 1521688 + 144 [0]
    <idle> : block_rq_complete: 8,16 R () 1522128 + 192 [0]
    <idle> : block_rq_complete: 8,16 R () 1523256 + 328 [0]
    ...
    
    Note that this issue should be addressed in checkpoint, and some readahead
    flows too.
    
    Reviewed-by: Namjae Jeon <namjae.jeon@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 1220b5c2ea21..eba7e84d1ffd 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -172,7 +172,7 @@ void update_extent_cache(block_t blk_addr, struct dnode_of_data *dn)
 	return;
 }
 
-struct page *find_data_page(struct inode *inode, pgoff_t index)
+struct page *find_data_page(struct inode *inode, pgoff_t index, bool sync)
 {
 	struct f2fs_sb_info *sbi = F2FS_SB(inode->i_sb);
 	struct address_space *mapping = inode->i_mapping;
@@ -207,11 +207,14 @@ struct page *find_data_page(struct inode *inode, pgoff_t index)
 		return page;
 	}
 
-	err = f2fs_readpage(sbi, page, dn.data_blkaddr, READ_SYNC);
-	wait_on_page_locked(page);
-	if (!PageUptodate(page)) {
-		f2fs_put_page(page, 0);
-		return ERR_PTR(-EIO);
+	err = f2fs_readpage(sbi, page, dn.data_blkaddr,
+					sync ? READ_SYNC : READA);
+	if (sync) {
+		wait_on_page_locked(page);
+		if (!PageUptodate(page)) {
+			f2fs_put_page(page, 0);
+			return ERR_PTR(-EIO);
+		}
 	}
 	return page;
 }

commit c01e285324793a86c2c90c8451ed6feb04b3d310
Author: Namjae Jeon <namjae.jeon@samsung.com>
Date:   Tue Apr 23 17:00:52 2013 +0900

    f2fs: add tracepoints to debug the block allocation
    
    Add tracepoints to debug the block allocation & fallocate.
    
    Signed-off-by: Namjae Jeon <namjae.jeon@samsung.com>
    Signed-off-by: Pankaj Kumar <pankaj.km@samsung.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    [Jaegeuk: enhance information]
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 8e8b14d714fc..1220b5c2ea21 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -55,6 +55,8 @@ int reserve_new_block(struct dnode_of_data *dn)
 	if (!inc_valid_block_count(sbi, dn->inode, 1))
 		return -ENOSPC;
 
+	trace_f2fs_reserve_new_block(dn->inode, dn->nid, dn->ofs_in_node);
+
 	__set_data_blkaddr(dn, NEW_ADDR);
 	dn->data_blkaddr = NEW_ADDR;
 	sync_inode_page(dn);

commit 848753aa3b19a6513315ca54f22ba1e2732ea94a
Author: Namjae Jeon <namjae.jeon@samsung.com>
Date:   Tue Apr 23 16:38:02 2013 +0900

    f2fs: add tracepoint for tracing the page i/o
    
    Add tracepoints for page i/o operations and block allocation
    tracing during page read operation.
    
    Signed-off-by: Namjae Jeon <namjae.jeon@samsung.com>
    Signed-off-by: Pankaj Kumar <pankaj.km@samsung.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    [Jaegeuk: combine and modify the tracepoint structures]
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 3c31ec7d633d..8e8b14d714fc 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -21,6 +21,7 @@
 #include "f2fs.h"
 #include "node.h"
 #include "segment.h"
+#include <trace/events/f2fs.h>
 
 /*
  * Lock ordering for the change of data block address:
@@ -348,6 +349,8 @@ int f2fs_readpage(struct f2fs_sb_info *sbi, struct page *page,
 	struct block_device *bdev = sbi->sb->s_bdev;
 	struct bio *bio;
 
+	trace_f2fs_readpage(page, blk_addr, type);
+
 	down_read(&sbi->bio_sem);
 
 	/* Allocate a new bio */
@@ -388,14 +391,18 @@ static int get_data_block_ro(struct inode *inode, sector_t iblock,
 	/* Get the page offset from the block offset(iblock) */
 	pgofs =	(pgoff_t)(iblock >> (PAGE_CACHE_SHIFT - blkbits));
 
-	if (check_extent_cache(inode, pgofs, bh_result))
+	if (check_extent_cache(inode, pgofs, bh_result)) {
+		trace_f2fs_get_data_block(inode, iblock, bh_result, 0);
 		return 0;
+	}
 
 	/* When reading holes, we need its node page */
 	set_new_dnode(&dn, inode, NULL, NULL, 0);
 	err = get_dnode_of_data(&dn, pgofs, LOOKUP_NODE_RA);
-	if (err)
+	if (err) {
+		trace_f2fs_get_data_block(inode, iblock, bh_result, err);
 		return (err == -ENOENT) ? 0 : err;
+	}
 
 	/* It does not support data allocation */
 	BUG_ON(create);
@@ -420,6 +427,7 @@ static int get_data_block_ro(struct inode *inode, sector_t iblock,
 		bh_result->b_size = (i << blkbits);
 	}
 	f2fs_put_dnode(&dn);
+	trace_f2fs_get_data_block(inode, iblock, bh_result, 0);
 	return 0;
 }
 

commit 6224da875ee0cb702c6ffe3456307701106dffb5
Author: Namjae Jeon <namjae.jeon@samsung.com>
Date:   Sat Apr 6 14:44:32 2013 +0900

    f2fs: fix typo mistakes
    
    Fix typo mistakes.
    1. I think that it should be 'L' instead of 'V'.
    2. and try to fix 'Front' instead of 'Frone'
    
    Signed-off-by: Namjae Jeon <namjae.jeon@samsung.com>
    Signed-off-by: Amit Sahrawat <a.sahrawat@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 72a1b30f720a..3c31ec7d633d 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -133,7 +133,7 @@ void update_extent_cache(block_t blk_addr, struct dnode_of_data *dn)
 		goto end_update;
 	}
 
-	/* Frone merge */
+	/* Front merge */
 	if (fofs == start_fofs - 1 && blk_addr == start_blkaddr - 1) {
 		fi->ext.fofs--;
 		fi->ext.blk_addr--;

commit 399368372ed9f3c396eadb5c2bbc98be8c774a39
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Thu Nov 22 16:21:29 2012 +0900

    f2fs: introduce a new global lock scheme
    
    In the previous version, f2fs uses global locks according to the usage types,
    such as directory operations, block allocation, block write, and so on.
    
    Reference the following lock types in f2fs.h.
    enum lock_type {
            RENAME,         /* for renaming operations */
            DENTRY_OPS,     /* for directory operations */
            DATA_WRITE,     /* for data write */
            DATA_NEW,       /* for data allocation */
            DATA_TRUNC,     /* for data truncate */
            NODE_NEW,       /* for node allocation */
            NODE_TRUNC,     /* for node truncate */
            NODE_WRITE,     /* for node write */
            NR_LOCK_TYPE,
    };
    
    In that case, we lose the performance under the multi-threading environment,
    since every types of operations must be conducted one at a time.
    
    In order to address the problem, let's share the locks globally with a mutex
    array regardless of any types.
    So, let users grab a mutex and perform their jobs in parallel as much as
    possbile.
    
    For this, I propose a new global lock scheme as follows.
    
    0. Data structure
     - f2fs_sb_info -> mutex_lock[NR_GLOBAL_LOCKS]
     - f2fs_sb_info -> node_write
    
    1. mutex_lock_op(sbi)
     - try to get an avaiable lock from the array.
     - returns the index of the gottern lock variable.
    
    2. mutex_unlock_op(sbi, index of the lock)
     - unlock the given index of the lock.
    
    3. mutex_lock_all(sbi)
     - grab all the locks in the array before the checkpoint.
    
    4. mutex_unlock_all(sbi)
     - release all the locks in the array after checkpoint.
    
    5. block_operations()
     - call mutex_lock_all()
     - sync_dirty_dir_inodes()
     - grab node_write
     - sync_node_pages()
    
    Note that,
     the pairs of mutex_lock_op()/mutex_unlock_op() and
     mutex_lock_all()/mutex_unlock_all() should be used together.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index cf9ff5f76134..72a1b30f720a 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -260,6 +260,9 @@ struct page *get_lock_data_page(struct inode *inode, pgoff_t index)
 /*
  * Caller ensures that this data page is never allocated.
  * A new zero-filled data page is allocated in the page cache.
+ *
+ * Also, caller should grab and release a mutex by calling mutex_lock_op() and
+ * mutex_unlock_op().
  */
 struct page *get_new_data_page(struct inode *inode, pgoff_t index,
 						bool new_i_size)
@@ -479,10 +482,11 @@ static int f2fs_write_data_page(struct page *page,
 	const pgoff_t end_index = ((unsigned long long) i_size)
 							>> PAGE_CACHE_SHIFT;
 	unsigned offset;
+	bool need_balance_fs = false;
 	int err = 0;
 
 	if (page->index < end_index)
-		goto out;
+		goto write;
 
 	/*
 	 * If the offset is out-of-range of file size,
@@ -494,50 +498,46 @@ static int f2fs_write_data_page(struct page *page,
 			dec_page_count(sbi, F2FS_DIRTY_DENTS);
 			inode_dec_dirty_dents(inode);
 		}
-		goto unlock_out;
+		goto out;
 	}
 
 	zero_user_segment(page, offset, PAGE_CACHE_SIZE);
-out:
-	if (sbi->por_doing)
-		goto redirty_out;
-
-	if (wbc->for_reclaim && !S_ISDIR(inode->i_mode) && !is_cold_data(page))
+write:
+	if (sbi->por_doing) {
+		err = AOP_WRITEPAGE_ACTIVATE;
 		goto redirty_out;
+	}
 
-	mutex_lock_op(sbi, DATA_WRITE);
+	/* Dentry blocks are controlled by checkpoint */
 	if (S_ISDIR(inode->i_mode)) {
 		dec_page_count(sbi, F2FS_DIRTY_DENTS);
 		inode_dec_dirty_dents(inode);
+		err = do_write_data_page(page);
+	} else {
+		int ilock = mutex_lock_op(sbi);
+		err = do_write_data_page(page);
+		mutex_unlock_op(sbi, ilock);
+		need_balance_fs = true;
 	}
-	err = do_write_data_page(page);
-	if (err && err != -ENOENT) {
-		wbc->pages_skipped++;
-		set_page_dirty(page);
-	}
-	mutex_unlock_op(sbi, DATA_WRITE);
+	if (err == -ENOENT)
+		goto out;
+	else if (err)
+		goto redirty_out;
 
 	if (wbc->for_reclaim)
 		f2fs_submit_bio(sbi, DATA, true);
 
-	if (err == -ENOENT)
-		goto unlock_out;
-
 	clear_cold_data(page);
+out:
 	unlock_page(page);
-
-	if (!wbc->for_reclaim && !S_ISDIR(inode->i_mode))
+	if (need_balance_fs)
 		f2fs_balance_fs(sbi);
 	return 0;
 
-unlock_out:
-	unlock_page(page);
-	return (err == -ENOENT) ? 0 : err;
-
 redirty_out:
 	wbc->pages_skipped++;
 	set_page_dirty(page);
-	return AOP_WRITEPAGE_ACTIVATE;
+	return err;
 }
 
 #define MAX_DESIRED_PAGES_WP	4096
@@ -592,6 +592,7 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 	pgoff_t index = ((unsigned long long) pos) >> PAGE_CACHE_SHIFT;
 	struct dnode_of_data dn;
 	int err = 0;
+	int ilock;
 
 	/* for nobh_write_end */
 	*fsdata = NULL;
@@ -603,28 +604,21 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 		return -ENOMEM;
 	*pagep = page;
 
-	mutex_lock_op(sbi, DATA_NEW);
+	ilock = mutex_lock_op(sbi);
 
 	set_new_dnode(&dn, inode, NULL, NULL, 0);
 	err = get_dnode_of_data(&dn, index, ALLOC_NODE);
-	if (err) {
-		mutex_unlock_op(sbi, DATA_NEW);
-		f2fs_put_page(page, 1);
-		return err;
-	}
+	if (err)
+		goto err;
 
-	if (dn.data_blkaddr == NULL_ADDR) {
+	if (dn.data_blkaddr == NULL_ADDR)
 		err = reserve_new_block(&dn);
-		if (err) {
-			f2fs_put_dnode(&dn);
-			mutex_unlock_op(sbi, DATA_NEW);
-			f2fs_put_page(page, 1);
-			return err;
-		}
-	}
+
 	f2fs_put_dnode(&dn);
+	if (err)
+		goto err;
 
-	mutex_unlock_op(sbi, DATA_NEW);
+	mutex_unlock_op(sbi, ilock);
 
 	if ((len == PAGE_CACHE_SIZE) || PageUptodate(page))
 		return 0;
@@ -654,6 +648,11 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 	SetPageUptodate(page);
 	clear_cold_data(page);
 	return 0;
+
+err:
+	mutex_unlock_op(sbi, ilock);
+	f2fs_put_page(page, 1);
+	return err;
 }
 
 static ssize_t f2fs_direct_IO(int rw, struct kiocb *iocb,

commit cfb185a1488810fbae9256c7d52f66c558c6ea04
Author: P J P <ppandit@redhat.com>
Date:   Wed Apr 3 11:38:00 2013 +0900

    f2fs: add NULL pointer check
    
    Commit - fa9150a84c - replaces a call to generic_writepages() in
    f2fs_write_data_pages() with write_cache_pages(), with a function pointer
    argument pointing to routine: __f2fs_writepage.
    
      -> https://git.kernel.org/linus/fa9150a84ca333f68127097c4fa1eda4b3913a22
    
      This patch adds a NULL pointer check in f2fs_write_data_pages() to avoid
      a possible NULL pointer dereference, in case if - mapping->a_ops->writepage -
      is NULL.
    
    Signed-off-by: P J P <ppandit@redhat.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 47a2d7c87ea9..cf9ff5f76134 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -559,6 +559,10 @@ static int f2fs_write_data_pages(struct address_space *mapping,
 	int ret;
 	long excess_nrtw = 0, desired_nrtw;
 
+	/* deal with chardevs and other special file */
+	if (!mapping->a_ops->writepage)
+		return 0;
+
 	if (wbc->nr_to_write < MAX_DESIRED_PAGES_WP) {
 		desired_nrtw = MAX_DESIRED_PAGES_WP;
 		excess_nrtw = desired_nrtw - wbc->nr_to_write;

commit 0ff153a2f1fa7ef31d6d9bc9ce6c3815dede55e6
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Wed Mar 20 14:58:38 2013 +0900

    f2fs: do not skip writing file meta during fsync
    
    This patch removes data_version check flow during the fsync call.
    The original purpose for the use of data_version was to avoid writng inode
    pages redundantly by the fsync calls repeatedly.
    However, when user can modify file meta and then call fsync, we should not
    skip fsync procedure.
    So, let's remove this condition check and hope that user triggers in right
    manner.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index ea8be6fc38f1..47a2d7c87ea9 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -435,7 +435,6 @@ static int f2fs_read_data_pages(struct file *file,
 int do_write_data_page(struct page *page)
 {
 	struct inode *inode = page->mapping->host;
-	struct f2fs_sb_info *sbi = F2FS_SB(inode->i_sb);
 	block_t old_blk_addr, new_blk_addr;
 	struct dnode_of_data dn;
 	int err = 0;
@@ -465,8 +464,6 @@ int do_write_data_page(struct page *page)
 		write_data_page(inode, page, &dn,
 				old_blk_addr, &new_blk_addr);
 		update_extent_cache(new_blk_addr, &dn);
-		F2FS_I(inode)->data_version =
-			le64_to_cpu(F2FS_CKPT(sbi)->checkpoint_ver);
 	}
 out_writepage:
 	f2fs_put_dnode(&dn);

commit c3850aa1cb25872fddacd7abd8dfb021411e92ee
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Thu Mar 14 09:24:32 2013 +0900

    f2fs: fix return value of releasepage for node and data
    
    If the return value of releasepage is equal to zero, the page cannot be reclaimed.
    Instead, we should return 1 in order to reclaim clean pages.
    
    Reviewed-by: Namjae Jeon <namjae.jeon@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index c8e20b618913..ea8be6fc38f1 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -683,7 +683,7 @@ static void f2fs_invalidate_data_page(struct page *page, unsigned long offset)
 static int f2fs_release_data_page(struct page *page, gfp_t wait)
 {
 	ClearPagePrivate(page);
-	return 0;
+	return 1;
 }
 
 static int f2fs_set_data_page_dirty(struct page *page)

commit 393ff91f57c87d48ffed30878be6e3e486d3a00a
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Fri Mar 8 21:29:23 2013 +0900

    f2fs: reduce unncessary locking pages during read
    
    This patch reduces redundant locking and unlocking pages during read operations.
    In f2fs_readpage, let's use wait_on_page_locked() instead of lock_page.
    And then, when we need to modify any data finally, let's lock the page so that
    we can avoid lock contention.
    
    [readpage rule]
    - The f2fs_readpage returns unlocked page, or released page too in error cases.
    - Its caller should handle read error, -EIO, after locking the page, which
      indicates read completion.
    - Its caller should check PageUptodate after grab_cache_page.
    
    Signed-off-by: Changman Lee <cm224.lee@samsung.com>
    Reviewed-by: Namjae Jeon <namjae.jeon@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 277966a8547a..c8e20b618913 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -199,12 +199,17 @@ struct page *find_data_page(struct inode *inode, pgoff_t index)
 	if (!page)
 		return ERR_PTR(-ENOMEM);
 
+	if (PageUptodate(page)) {
+		unlock_page(page);
+		return page;
+	}
+
 	err = f2fs_readpage(sbi, page, dn.data_blkaddr, READ_SYNC);
-	if (err) {
-		f2fs_put_page(page, 1);
-		return ERR_PTR(err);
+	wait_on_page_locked(page);
+	if (!PageUptodate(page)) {
+		f2fs_put_page(page, 0);
+		return ERR_PTR(-EIO);
 	}
-	unlock_page(page);
 	return page;
 }
 
@@ -241,9 +246,13 @@ struct page *get_lock_data_page(struct inode *inode, pgoff_t index)
 	BUG_ON(dn.data_blkaddr == NULL_ADDR);
 
 	err = f2fs_readpage(sbi, page, dn.data_blkaddr, READ_SYNC);
-	if (err) {
-		f2fs_put_page(page, 1);
+	if (err)
 		return ERR_PTR(err);
+
+	lock_page(page);
+	if (!PageUptodate(page)) {
+		f2fs_put_page(page, 1);
+		return ERR_PTR(-EIO);
 	}
 	return page;
 }
@@ -283,14 +292,17 @@ struct page *get_new_data_page(struct inode *inode, pgoff_t index,
 
 	if (dn.data_blkaddr == NEW_ADDR) {
 		zero_user_segment(page, 0, PAGE_CACHE_SIZE);
+		SetPageUptodate(page);
 	} else {
 		err = f2fs_readpage(sbi, page, dn.data_blkaddr, READ_SYNC);
-		if (err) {
-			f2fs_put_page(page, 1);
+		if (err)
 			return ERR_PTR(err);
+		lock_page(page);
+		if (!PageUptodate(page)) {
+			f2fs_put_page(page, 1);
+			return ERR_PTR(-EIO);
 		}
 	}
-	SetPageUptodate(page);
 
 	if (new_i_size &&
 		i_size_read(inode) < ((index + 1) << PAGE_CACHE_SHIFT)) {
@@ -325,22 +337,14 @@ static void read_end_io(struct bio *bio, int err)
 
 /*
  * Fill the locked page with data located in the block address.
- * Read operation is synchronous, and caller must unlock the page.
+ * Return unlocked page.
  */
 int f2fs_readpage(struct f2fs_sb_info *sbi, struct page *page,
 					block_t blk_addr, int type)
 {
 	struct block_device *bdev = sbi->sb->s_bdev;
-	bool sync = (type == READ_SYNC);
 	struct bio *bio;
 
-	/* This page can be already read by other threads */
-	if (PageUptodate(page)) {
-		if (!sync)
-			unlock_page(page);
-		return 0;
-	}
-
 	down_read(&sbi->bio_sem);
 
 	/* Allocate a new bio */
@@ -354,18 +358,12 @@ int f2fs_readpage(struct f2fs_sb_info *sbi, struct page *page,
 		kfree(bio->bi_private);
 		bio_put(bio);
 		up_read(&sbi->bio_sem);
+		f2fs_put_page(page, 1);
 		return -EFAULT;
 	}
 
 	submit_bio(type, bio);
 	up_read(&sbi->bio_sem);
-
-	/* wait for read completion if sync */
-	if (sync) {
-		lock_page(page);
-		if (PageError(page))
-			return -EIO;
-	}
 	return 0;
 }
 
@@ -636,18 +634,22 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 
 		/* Reading beyond i_size is simple: memset to zero */
 		zero_user_segments(page, 0, start, end, PAGE_CACHE_SIZE);
-		return 0;
+		goto out;
 	}
 
 	if (dn.data_blkaddr == NEW_ADDR) {
 		zero_user_segment(page, 0, PAGE_CACHE_SIZE);
 	} else {
 		err = f2fs_readpage(sbi, page, dn.data_blkaddr, READ_SYNC);
-		if (err) {
-			f2fs_put_page(page, 1);
+		if (err)
 			return err;
+		lock_page(page);
+		if (!PageUptodate(page)) {
+			f2fs_put_page(page, 1);
+			return -EIO;
 		}
 	}
+out:
 	SetPageUptodate(page);
 	clear_cold_data(page);
 	return 0;

commit 266e97a81cf73d1a0dac5f68391da382630a80b7
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Tue Feb 26 13:10:46 2013 +0900

    f2fs: introduce readahead mode of node pages
    
    Previously, f2fs reads several node pages ahead when get_dnode_of_data is called
    with RDONLY_NODE flag.
    And, this flag is set by the following functions.
    - get_data_block_ro
    - get_lock_data_page
    - do_write_data_page
    - truncate_blocks
    - truncate_hole
    
    However, this readahead mechanism is initially introduced for the use of
    get_data_block_ro to enhance the sequential read performance.
    
    So, let's clarify all the cases with the additional modes as follows.
    
    enum {
            ALLOC_NODE,     /* allocate a new node page if needed */
            LOOKUP_NODE,    /* look up a node without readahead */
            LOOKUP_NODE_RA, /*
                             * look up a node with readahead called
                             * by get_datablock_ro.
                             */
    }
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>
    Reviewed-by: Namjae Jeon <namjae.jeon@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 7bd22a201125..277966a8547a 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -183,7 +183,7 @@ struct page *find_data_page(struct inode *inode, pgoff_t index)
 	f2fs_put_page(page, 0);
 
 	set_new_dnode(&dn, inode, NULL, NULL, 0);
-	err = get_dnode_of_data(&dn, index, RDONLY_NODE);
+	err = get_dnode_of_data(&dn, index, LOOKUP_NODE);
 	if (err)
 		return ERR_PTR(err);
 	f2fs_put_dnode(&dn);
@@ -222,7 +222,7 @@ struct page *get_lock_data_page(struct inode *inode, pgoff_t index)
 	int err;
 
 	set_new_dnode(&dn, inode, NULL, NULL, 0);
-	err = get_dnode_of_data(&dn, index, RDONLY_NODE);
+	err = get_dnode_of_data(&dn, index, LOOKUP_NODE);
 	if (err)
 		return ERR_PTR(err);
 	f2fs_put_dnode(&dn);
@@ -262,7 +262,7 @@ struct page *get_new_data_page(struct inode *inode, pgoff_t index,
 	int err;
 
 	set_new_dnode(&dn, inode, NULL, NULL, 0);
-	err = get_dnode_of_data(&dn, index, 0);
+	err = get_dnode_of_data(&dn, index, ALLOC_NODE);
 	if (err)
 		return ERR_PTR(err);
 
@@ -392,7 +392,7 @@ static int get_data_block_ro(struct inode *inode, sector_t iblock,
 
 	/* When reading holes, we need its node page */
 	set_new_dnode(&dn, inode, NULL, NULL, 0);
-	err = get_dnode_of_data(&dn, pgofs, RDONLY_NODE);
+	err = get_dnode_of_data(&dn, pgofs, LOOKUP_NODE_RA);
 	if (err)
 		return (err == -ENOENT) ? 0 : err;
 
@@ -443,7 +443,7 @@ int do_write_data_page(struct page *page)
 	int err = 0;
 
 	set_new_dnode(&dn, inode, NULL, NULL, 0);
-	err = get_dnode_of_data(&dn, page->index, RDONLY_NODE);
+	err = get_dnode_of_data(&dn, page->index, LOOKUP_NODE);
 	if (err)
 		return err;
 
@@ -607,7 +607,7 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 	mutex_lock_op(sbi, DATA_NEW);
 
 	set_new_dnode(&dn, inode, NULL, NULL, 0);
-	err = get_dnode_of_data(&dn, index, 0);
+	err = get_dnode_of_data(&dn, index, ALLOC_NODE);
 	if (err) {
 		mutex_unlock_op(sbi, DATA_NEW);
 		f2fs_put_page(page, 1);

commit c01e54b770e69c65525295eb2668be3dc0822406
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Thu Jan 17 20:30:23 2013 +0900

    f2fs: support swapfile
    
    This patch adds f2fs_bmap operation to the data address space.
    This enables f2fs to support swapfile.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index b1347fc6d688..7bd22a201125 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -698,6 +698,11 @@ static int f2fs_set_data_page_dirty(struct page *page)
 	return 0;
 }
 
+static sector_t f2fs_bmap(struct address_space *mapping, sector_t block)
+{
+	return generic_block_bmap(mapping, block, get_data_block_ro);
+}
+
 const struct address_space_operations f2fs_dblock_aops = {
 	.readpage	= f2fs_read_data_page,
 	.readpages	= f2fs_read_data_pages,
@@ -709,4 +714,5 @@ const struct address_space_operations f2fs_dblock_aops = {
 	.invalidatepage	= f2fs_invalidate_data_page,
 	.releasepage	= f2fs_release_data_page,
 	.direct_IO	= f2fs_direct_IO,
+	.bmap		= f2fs_bmap,
 };

commit fa9150a84ca333f68127097c4fa1eda4b3913a22
Author: Namjae Jeon <namjae.jeon@samsung.com>
Date:   Tue Jan 15 16:45:24 2013 +0900

    f2fs: remove the blk_plug usage in f2fs_write_data_pages
    
    Let's consider the usage of blk_plug in f2fs_write_data_pages().
    We can come up with the two issues: lock contention and task awareness.
    
    1. Merging bios prior to grabing "queue lock"
     The f2fs merges consecutive IOs in the file system level before
     submitting any bios, which is similar with the back merge by the
     plugging mechanism in attempt_plug_merge(). Both of them need to acquire
     no queue lock.
    
    2. Merging policy with respect to tasks
     The f2fs merges IOs as much as possible regardless of tasks, while
     blk-plugging is conducted on a basis of tasks. As we can understand
     there are trade-offs, f2fs tries to maximize the write performance with
     well-merged bios.
    
    As a result, if f2fs produces many consecutive but separated bios in
    writepages(), it would be good to use blk-plugging since f2fs would be
    able to avoid queue lock contention in the block layer by merging them.
    But, f2fs merges IOs and submit one bio, which means that there are not
    much chances to merge bios by attempt_plug_merge().
    
    However, f2fs has already been used blk_plug by triggering generic_writepages()
    in f2fs_write_data_pages().
    So to make the overall code consistency, I'd like to remove blk_plug there.
    
    Signed-off-by: Namjae Jeon <namjae.jeon@samsung.com>
    Signed-off-by: Amit Sahrawat <a.sahrawat@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 3aa5ce7cab83..b1347fc6d688 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -547,6 +547,15 @@ static int f2fs_write_data_page(struct page *page,
 
 #define MAX_DESIRED_PAGES_WP	4096
 
+static int __f2fs_writepage(struct page *page, struct writeback_control *wbc,
+			void *data)
+{
+	struct address_space *mapping = data;
+	int ret = mapping->a_ops->writepage(page, wbc);
+	mapping_set_error(mapping, ret);
+	return ret;
+}
+
 static int f2fs_write_data_pages(struct address_space *mapping,
 			    struct writeback_control *wbc)
 {
@@ -563,7 +572,7 @@ static int f2fs_write_data_pages(struct address_space *mapping,
 
 	if (!S_ISDIR(inode->i_mode))
 		mutex_lock(&sbi->writepages);
-	ret = generic_writepages(mapping, wbc);
+	ret = write_cache_pages(mapping, wbc, __f2fs_writepage, mapping);
 	if (!S_ISDIR(inode->i_mode))
 		mutex_unlock(&sbi->writepages);
 	f2fs_submit_bio(sbi, DATA, (wbc->sync_mode == WB_SYNC_ALL));

commit 690e4a3ead5f88fc95f7650816d1376aa2e79db5
Author: Geert Uytterhoeven <geert@linux-m68k.org>
Date:   Wed Dec 19 22:19:30 2012 +0100

    f2fs: add missing #include <linux/prefetch.h>
    
    m68k allmodconfig:
    
    fs/f2fs/data.c: In function ‘read_end_io’:
    fs/f2fs/data.c:311: error: implicit declaration of function ‘prefetchw’
    
    fs/f2fs/segment.c: In function ‘f2fs_end_io_write’:
    fs/f2fs/segment.c:628: error: implicit declaration of function ‘prefetchw’
    
    Signed-off-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 655aeabc1dd4..3aa5ce7cab83 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -16,6 +16,7 @@
 #include <linux/backing-dev.h>
 #include <linux/blkdev.h>
 #include <linux/bio.h>
+#include <linux/prefetch.h>
 
 #include "f2fs.h"
 #include "node.h"

commit 3cd8a23948b29301f8f67b8d70c5c18fabbc05e1
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Mon Dec 10 09:26:05 2012 +0900

    f2fs: cleanup the f2fs_bio_alloc routine
    
    Do cleanup more for better code readability.
    
    - Change the parameter set of f2fs_bio_alloc()
      This function should allocate a bio only since it is not something like
      f2fs_bio_init(). Instead, the caller should initialize the allocated bio.
    
    - Introduce SECTOR_FROM_BLOCK
      This macro translates a block address to its sector address.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>
    Reviewed-by: Namjae Jeon <namjae.jeon@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 444c2a6fbaa0..655aeabc1dd4 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -343,11 +343,12 @@ int f2fs_readpage(struct f2fs_sb_info *sbi, struct page *page,
 	down_read(&sbi->bio_sem);
 
 	/* Allocate a new bio */
-	bio = f2fs_bio_alloc(bdev, blk_addr << (sbi->log_blocksize - 9),
-				1, GFP_NOFS | __GFP_HIGH);
+	bio = f2fs_bio_alloc(bdev, 1);
 
 	/* Initialize the bio */
+	bio->bi_sector = SECTOR_FROM_BLOCK(sbi, blk_addr);
 	bio->bi_end_io = read_end_io;
+
 	if (bio_add_page(bio, page, PAGE_CACHE_SIZE, 0) < PAGE_CACHE_SIZE) {
 		kfree(bio->bi_private);
 		bio_put(bio);

commit 0a8165d7c2cf1395059db20ab07665baf3758fcd
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Thu Nov 29 13:28:09 2012 +0900

    f2fs: adjust kernel coding style
    
    As pointed out by Randy Dunlap, this patch removes all usage of "/**" for comment
    blocks. Instead, just use "/*".
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 5635cc5a9d4d..444c2a6fbaa0 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -1,4 +1,4 @@
-/**
+/*
  * fs/f2fs/data.c
  *
  * Copyright (c) 2012 Samsung Electronics Co., Ltd.
@@ -21,7 +21,7 @@
 #include "node.h"
 #include "segment.h"
 
-/**
+/*
  * Lock ordering for the change of data block address:
  * ->data_page
  *  ->node_page
@@ -207,7 +207,7 @@ struct page *find_data_page(struct inode *inode, pgoff_t index)
 	return page;
 }
 
-/**
+/*
  * If it tries to access a hole, return an error.
  * Because, the callers, functions in dir.c and GC, should be able to know
  * whether this page exists or not.
@@ -247,7 +247,7 @@ struct page *get_lock_data_page(struct inode *inode, pgoff_t index)
 	return page;
 }
 
-/**
+/*
  * Caller ensures that this data page is never allocated.
  * A new zero-filled data page is allocated in the page cache.
  */
@@ -322,7 +322,7 @@ static void read_end_io(struct bio *bio, int err)
 	bio_put(bio);
 }
 
-/**
+/*
  * Fill the locked page with data located in the block address.
  * Read operation is synchronous, and caller must unlock the page.
  */
@@ -367,7 +367,7 @@ int f2fs_readpage(struct f2fs_sb_info *sbi, struct page *page,
 	return 0;
 }
 
-/**
+/*
  * This function should be used by the data read flow only where it
  * does not check the "create" flag that indicates block allocation.
  * The reason for this special functionality is to exploit VFS readahead

commit 25ca923b2a766b9c93b63777ead351137533a623
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Wed Nov 28 16:12:41 2012 +0900

    f2fs: fix endian conversion bugs reported by sparse
    
    This patch should resolve the bugs reported by the sparse tool.
    Initial reports were written by "kbuild test robot" managed by fengguang.wu.
    
    In my local machines, I've tested also by running:
    > make C=2 CF="-D__CHECK_ENDIAN__"
    
    Accordingly, I've found lots of warnings and bugs related to the endian
    conversion. And I've fixed all at this moment.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index c2fd0a80db16..5635cc5a9d4d 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -545,7 +545,7 @@ static int f2fs_write_data_page(struct page *page,
 
 #define MAX_DESIRED_PAGES_WP	4096
 
-int f2fs_write_data_pages(struct address_space *mapping,
+static int f2fs_write_data_pages(struct address_space *mapping,
 			    struct writeback_control *wbc)
 {
 	struct inode *inode = mapping->host;

commit eb47b8009dc969a3386c983bd5e798e9f690c5d9
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Fri Nov 2 17:10:12 2012 +0900

    f2fs: add address space operations for data
    
    This adds address space operations for data.
    
    - F2FS supports readpages(), writepages(), and direct_IO().
    
    - Because of out-of-place writes, f2fs_direct_IO() does not write data in place.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
new file mode 100644
index 000000000000..c2fd0a80db16
--- /dev/null
+++ b/fs/f2fs/data.c
@@ -0,0 +1,701 @@
+/**
+ * fs/f2fs/data.c
+ *
+ * Copyright (c) 2012 Samsung Electronics Co., Ltd.
+ *             http://www.samsung.com/
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+#include <linux/fs.h>
+#include <linux/f2fs_fs.h>
+#include <linux/buffer_head.h>
+#include <linux/mpage.h>
+#include <linux/writeback.h>
+#include <linux/backing-dev.h>
+#include <linux/blkdev.h>
+#include <linux/bio.h>
+
+#include "f2fs.h"
+#include "node.h"
+#include "segment.h"
+
+/**
+ * Lock ordering for the change of data block address:
+ * ->data_page
+ *  ->node_page
+ *    update block addresses in the node page
+ */
+static void __set_data_blkaddr(struct dnode_of_data *dn, block_t new_addr)
+{
+	struct f2fs_node *rn;
+	__le32 *addr_array;
+	struct page *node_page = dn->node_page;
+	unsigned int ofs_in_node = dn->ofs_in_node;
+
+	wait_on_page_writeback(node_page);
+
+	rn = (struct f2fs_node *)page_address(node_page);
+
+	/* Get physical address of data block */
+	addr_array = blkaddr_in_node(rn);
+	addr_array[ofs_in_node] = cpu_to_le32(new_addr);
+	set_page_dirty(node_page);
+}
+
+int reserve_new_block(struct dnode_of_data *dn)
+{
+	struct f2fs_sb_info *sbi = F2FS_SB(dn->inode->i_sb);
+
+	if (is_inode_flag_set(F2FS_I(dn->inode), FI_NO_ALLOC))
+		return -EPERM;
+	if (!inc_valid_block_count(sbi, dn->inode, 1))
+		return -ENOSPC;
+
+	__set_data_blkaddr(dn, NEW_ADDR);
+	dn->data_blkaddr = NEW_ADDR;
+	sync_inode_page(dn);
+	return 0;
+}
+
+static int check_extent_cache(struct inode *inode, pgoff_t pgofs,
+					struct buffer_head *bh_result)
+{
+	struct f2fs_inode_info *fi = F2FS_I(inode);
+	struct f2fs_sb_info *sbi = F2FS_SB(inode->i_sb);
+	pgoff_t start_fofs, end_fofs;
+	block_t start_blkaddr;
+
+	read_lock(&fi->ext.ext_lock);
+	if (fi->ext.len == 0) {
+		read_unlock(&fi->ext.ext_lock);
+		return 0;
+	}
+
+	sbi->total_hit_ext++;
+	start_fofs = fi->ext.fofs;
+	end_fofs = fi->ext.fofs + fi->ext.len - 1;
+	start_blkaddr = fi->ext.blk_addr;
+
+	if (pgofs >= start_fofs && pgofs <= end_fofs) {
+		unsigned int blkbits = inode->i_sb->s_blocksize_bits;
+		size_t count;
+
+		clear_buffer_new(bh_result);
+		map_bh(bh_result, inode->i_sb,
+				start_blkaddr + pgofs - start_fofs);
+		count = end_fofs - pgofs + 1;
+		if (count < (UINT_MAX >> blkbits))
+			bh_result->b_size = (count << blkbits);
+		else
+			bh_result->b_size = UINT_MAX;
+
+		sbi->read_hit_ext++;
+		read_unlock(&fi->ext.ext_lock);
+		return 1;
+	}
+	read_unlock(&fi->ext.ext_lock);
+	return 0;
+}
+
+void update_extent_cache(block_t blk_addr, struct dnode_of_data *dn)
+{
+	struct f2fs_inode_info *fi = F2FS_I(dn->inode);
+	pgoff_t fofs, start_fofs, end_fofs;
+	block_t start_blkaddr, end_blkaddr;
+
+	BUG_ON(blk_addr == NEW_ADDR);
+	fofs = start_bidx_of_node(ofs_of_node(dn->node_page)) + dn->ofs_in_node;
+
+	/* Update the page address in the parent node */
+	__set_data_blkaddr(dn, blk_addr);
+
+	write_lock(&fi->ext.ext_lock);
+
+	start_fofs = fi->ext.fofs;
+	end_fofs = fi->ext.fofs + fi->ext.len - 1;
+	start_blkaddr = fi->ext.blk_addr;
+	end_blkaddr = fi->ext.blk_addr + fi->ext.len - 1;
+
+	/* Drop and initialize the matched extent */
+	if (fi->ext.len == 1 && fofs == start_fofs)
+		fi->ext.len = 0;
+
+	/* Initial extent */
+	if (fi->ext.len == 0) {
+		if (blk_addr != NULL_ADDR) {
+			fi->ext.fofs = fofs;
+			fi->ext.blk_addr = blk_addr;
+			fi->ext.len = 1;
+		}
+		goto end_update;
+	}
+
+	/* Frone merge */
+	if (fofs == start_fofs - 1 && blk_addr == start_blkaddr - 1) {
+		fi->ext.fofs--;
+		fi->ext.blk_addr--;
+		fi->ext.len++;
+		goto end_update;
+	}
+
+	/* Back merge */
+	if (fofs == end_fofs + 1 && blk_addr == end_blkaddr + 1) {
+		fi->ext.len++;
+		goto end_update;
+	}
+
+	/* Split the existing extent */
+	if (fi->ext.len > 1 &&
+		fofs >= start_fofs && fofs <= end_fofs) {
+		if ((end_fofs - fofs) < (fi->ext.len >> 1)) {
+			fi->ext.len = fofs - start_fofs;
+		} else {
+			fi->ext.fofs = fofs + 1;
+			fi->ext.blk_addr = start_blkaddr +
+					fofs - start_fofs + 1;
+			fi->ext.len -= fofs - start_fofs + 1;
+		}
+		goto end_update;
+	}
+	write_unlock(&fi->ext.ext_lock);
+	return;
+
+end_update:
+	write_unlock(&fi->ext.ext_lock);
+	sync_inode_page(dn);
+	return;
+}
+
+struct page *find_data_page(struct inode *inode, pgoff_t index)
+{
+	struct f2fs_sb_info *sbi = F2FS_SB(inode->i_sb);
+	struct address_space *mapping = inode->i_mapping;
+	struct dnode_of_data dn;
+	struct page *page;
+	int err;
+
+	page = find_get_page(mapping, index);
+	if (page && PageUptodate(page))
+		return page;
+	f2fs_put_page(page, 0);
+
+	set_new_dnode(&dn, inode, NULL, NULL, 0);
+	err = get_dnode_of_data(&dn, index, RDONLY_NODE);
+	if (err)
+		return ERR_PTR(err);
+	f2fs_put_dnode(&dn);
+
+	if (dn.data_blkaddr == NULL_ADDR)
+		return ERR_PTR(-ENOENT);
+
+	/* By fallocate(), there is no cached page, but with NEW_ADDR */
+	if (dn.data_blkaddr == NEW_ADDR)
+		return ERR_PTR(-EINVAL);
+
+	page = grab_cache_page(mapping, index);
+	if (!page)
+		return ERR_PTR(-ENOMEM);
+
+	err = f2fs_readpage(sbi, page, dn.data_blkaddr, READ_SYNC);
+	if (err) {
+		f2fs_put_page(page, 1);
+		return ERR_PTR(err);
+	}
+	unlock_page(page);
+	return page;
+}
+
+/**
+ * If it tries to access a hole, return an error.
+ * Because, the callers, functions in dir.c and GC, should be able to know
+ * whether this page exists or not.
+ */
+struct page *get_lock_data_page(struct inode *inode, pgoff_t index)
+{
+	struct f2fs_sb_info *sbi = F2FS_SB(inode->i_sb);
+	struct address_space *mapping = inode->i_mapping;
+	struct dnode_of_data dn;
+	struct page *page;
+	int err;
+
+	set_new_dnode(&dn, inode, NULL, NULL, 0);
+	err = get_dnode_of_data(&dn, index, RDONLY_NODE);
+	if (err)
+		return ERR_PTR(err);
+	f2fs_put_dnode(&dn);
+
+	if (dn.data_blkaddr == NULL_ADDR)
+		return ERR_PTR(-ENOENT);
+
+	page = grab_cache_page(mapping, index);
+	if (!page)
+		return ERR_PTR(-ENOMEM);
+
+	if (PageUptodate(page))
+		return page;
+
+	BUG_ON(dn.data_blkaddr == NEW_ADDR);
+	BUG_ON(dn.data_blkaddr == NULL_ADDR);
+
+	err = f2fs_readpage(sbi, page, dn.data_blkaddr, READ_SYNC);
+	if (err) {
+		f2fs_put_page(page, 1);
+		return ERR_PTR(err);
+	}
+	return page;
+}
+
+/**
+ * Caller ensures that this data page is never allocated.
+ * A new zero-filled data page is allocated in the page cache.
+ */
+struct page *get_new_data_page(struct inode *inode, pgoff_t index,
+						bool new_i_size)
+{
+	struct f2fs_sb_info *sbi = F2FS_SB(inode->i_sb);
+	struct address_space *mapping = inode->i_mapping;
+	struct page *page;
+	struct dnode_of_data dn;
+	int err;
+
+	set_new_dnode(&dn, inode, NULL, NULL, 0);
+	err = get_dnode_of_data(&dn, index, 0);
+	if (err)
+		return ERR_PTR(err);
+
+	if (dn.data_blkaddr == NULL_ADDR) {
+		if (reserve_new_block(&dn)) {
+			f2fs_put_dnode(&dn);
+			return ERR_PTR(-ENOSPC);
+		}
+	}
+	f2fs_put_dnode(&dn);
+
+	page = grab_cache_page(mapping, index);
+	if (!page)
+		return ERR_PTR(-ENOMEM);
+
+	if (PageUptodate(page))
+		return page;
+
+	if (dn.data_blkaddr == NEW_ADDR) {
+		zero_user_segment(page, 0, PAGE_CACHE_SIZE);
+	} else {
+		err = f2fs_readpage(sbi, page, dn.data_blkaddr, READ_SYNC);
+		if (err) {
+			f2fs_put_page(page, 1);
+			return ERR_PTR(err);
+		}
+	}
+	SetPageUptodate(page);
+
+	if (new_i_size &&
+		i_size_read(inode) < ((index + 1) << PAGE_CACHE_SHIFT)) {
+		i_size_write(inode, ((index + 1) << PAGE_CACHE_SHIFT));
+		mark_inode_dirty_sync(inode);
+	}
+	return page;
+}
+
+static void read_end_io(struct bio *bio, int err)
+{
+	const int uptodate = test_bit(BIO_UPTODATE, &bio->bi_flags);
+	struct bio_vec *bvec = bio->bi_io_vec + bio->bi_vcnt - 1;
+
+	do {
+		struct page *page = bvec->bv_page;
+
+		if (--bvec >= bio->bi_io_vec)
+			prefetchw(&bvec->bv_page->flags);
+
+		if (uptodate) {
+			SetPageUptodate(page);
+		} else {
+			ClearPageUptodate(page);
+			SetPageError(page);
+		}
+		unlock_page(page);
+	} while (bvec >= bio->bi_io_vec);
+	kfree(bio->bi_private);
+	bio_put(bio);
+}
+
+/**
+ * Fill the locked page with data located in the block address.
+ * Read operation is synchronous, and caller must unlock the page.
+ */
+int f2fs_readpage(struct f2fs_sb_info *sbi, struct page *page,
+					block_t blk_addr, int type)
+{
+	struct block_device *bdev = sbi->sb->s_bdev;
+	bool sync = (type == READ_SYNC);
+	struct bio *bio;
+
+	/* This page can be already read by other threads */
+	if (PageUptodate(page)) {
+		if (!sync)
+			unlock_page(page);
+		return 0;
+	}
+
+	down_read(&sbi->bio_sem);
+
+	/* Allocate a new bio */
+	bio = f2fs_bio_alloc(bdev, blk_addr << (sbi->log_blocksize - 9),
+				1, GFP_NOFS | __GFP_HIGH);
+
+	/* Initialize the bio */
+	bio->bi_end_io = read_end_io;
+	if (bio_add_page(bio, page, PAGE_CACHE_SIZE, 0) < PAGE_CACHE_SIZE) {
+		kfree(bio->bi_private);
+		bio_put(bio);
+		up_read(&sbi->bio_sem);
+		return -EFAULT;
+	}
+
+	submit_bio(type, bio);
+	up_read(&sbi->bio_sem);
+
+	/* wait for read completion if sync */
+	if (sync) {
+		lock_page(page);
+		if (PageError(page))
+			return -EIO;
+	}
+	return 0;
+}
+
+/**
+ * This function should be used by the data read flow only where it
+ * does not check the "create" flag that indicates block allocation.
+ * The reason for this special functionality is to exploit VFS readahead
+ * mechanism.
+ */
+static int get_data_block_ro(struct inode *inode, sector_t iblock,
+			struct buffer_head *bh_result, int create)
+{
+	unsigned int blkbits = inode->i_sb->s_blocksize_bits;
+	unsigned maxblocks = bh_result->b_size >> blkbits;
+	struct dnode_of_data dn;
+	pgoff_t pgofs;
+	int err;
+
+	/* Get the page offset from the block offset(iblock) */
+	pgofs =	(pgoff_t)(iblock >> (PAGE_CACHE_SHIFT - blkbits));
+
+	if (check_extent_cache(inode, pgofs, bh_result))
+		return 0;
+
+	/* When reading holes, we need its node page */
+	set_new_dnode(&dn, inode, NULL, NULL, 0);
+	err = get_dnode_of_data(&dn, pgofs, RDONLY_NODE);
+	if (err)
+		return (err == -ENOENT) ? 0 : err;
+
+	/* It does not support data allocation */
+	BUG_ON(create);
+
+	if (dn.data_blkaddr != NEW_ADDR && dn.data_blkaddr != NULL_ADDR) {
+		int i;
+		unsigned int end_offset;
+
+		end_offset = IS_INODE(dn.node_page) ?
+				ADDRS_PER_INODE :
+				ADDRS_PER_BLOCK;
+
+		clear_buffer_new(bh_result);
+
+		/* Give more consecutive addresses for the read ahead */
+		for (i = 0; i < end_offset - dn.ofs_in_node; i++)
+			if (((datablock_addr(dn.node_page,
+							dn.ofs_in_node + i))
+				!= (dn.data_blkaddr + i)) || maxblocks == i)
+				break;
+		map_bh(bh_result, inode->i_sb, dn.data_blkaddr);
+		bh_result->b_size = (i << blkbits);
+	}
+	f2fs_put_dnode(&dn);
+	return 0;
+}
+
+static int f2fs_read_data_page(struct file *file, struct page *page)
+{
+	return mpage_readpage(page, get_data_block_ro);
+}
+
+static int f2fs_read_data_pages(struct file *file,
+			struct address_space *mapping,
+			struct list_head *pages, unsigned nr_pages)
+{
+	return mpage_readpages(mapping, pages, nr_pages, get_data_block_ro);
+}
+
+int do_write_data_page(struct page *page)
+{
+	struct inode *inode = page->mapping->host;
+	struct f2fs_sb_info *sbi = F2FS_SB(inode->i_sb);
+	block_t old_blk_addr, new_blk_addr;
+	struct dnode_of_data dn;
+	int err = 0;
+
+	set_new_dnode(&dn, inode, NULL, NULL, 0);
+	err = get_dnode_of_data(&dn, page->index, RDONLY_NODE);
+	if (err)
+		return err;
+
+	old_blk_addr = dn.data_blkaddr;
+
+	/* This page is already truncated */
+	if (old_blk_addr == NULL_ADDR)
+		goto out_writepage;
+
+	set_page_writeback(page);
+
+	/*
+	 * If current allocation needs SSR,
+	 * it had better in-place writes for updated data.
+	 */
+	if (old_blk_addr != NEW_ADDR && !is_cold_data(page) &&
+				need_inplace_update(inode)) {
+		rewrite_data_page(F2FS_SB(inode->i_sb), page,
+						old_blk_addr);
+	} else {
+		write_data_page(inode, page, &dn,
+				old_blk_addr, &new_blk_addr);
+		update_extent_cache(new_blk_addr, &dn);
+		F2FS_I(inode)->data_version =
+			le64_to_cpu(F2FS_CKPT(sbi)->checkpoint_ver);
+	}
+out_writepage:
+	f2fs_put_dnode(&dn);
+	return err;
+}
+
+static int f2fs_write_data_page(struct page *page,
+					struct writeback_control *wbc)
+{
+	struct inode *inode = page->mapping->host;
+	struct f2fs_sb_info *sbi = F2FS_SB(inode->i_sb);
+	loff_t i_size = i_size_read(inode);
+	const pgoff_t end_index = ((unsigned long long) i_size)
+							>> PAGE_CACHE_SHIFT;
+	unsigned offset;
+	int err = 0;
+
+	if (page->index < end_index)
+		goto out;
+
+	/*
+	 * If the offset is out-of-range of file size,
+	 * this page does not have to be written to disk.
+	 */
+	offset = i_size & (PAGE_CACHE_SIZE - 1);
+	if ((page->index >= end_index + 1) || !offset) {
+		if (S_ISDIR(inode->i_mode)) {
+			dec_page_count(sbi, F2FS_DIRTY_DENTS);
+			inode_dec_dirty_dents(inode);
+		}
+		goto unlock_out;
+	}
+
+	zero_user_segment(page, offset, PAGE_CACHE_SIZE);
+out:
+	if (sbi->por_doing)
+		goto redirty_out;
+
+	if (wbc->for_reclaim && !S_ISDIR(inode->i_mode) && !is_cold_data(page))
+		goto redirty_out;
+
+	mutex_lock_op(sbi, DATA_WRITE);
+	if (S_ISDIR(inode->i_mode)) {
+		dec_page_count(sbi, F2FS_DIRTY_DENTS);
+		inode_dec_dirty_dents(inode);
+	}
+	err = do_write_data_page(page);
+	if (err && err != -ENOENT) {
+		wbc->pages_skipped++;
+		set_page_dirty(page);
+	}
+	mutex_unlock_op(sbi, DATA_WRITE);
+
+	if (wbc->for_reclaim)
+		f2fs_submit_bio(sbi, DATA, true);
+
+	if (err == -ENOENT)
+		goto unlock_out;
+
+	clear_cold_data(page);
+	unlock_page(page);
+
+	if (!wbc->for_reclaim && !S_ISDIR(inode->i_mode))
+		f2fs_balance_fs(sbi);
+	return 0;
+
+unlock_out:
+	unlock_page(page);
+	return (err == -ENOENT) ? 0 : err;
+
+redirty_out:
+	wbc->pages_skipped++;
+	set_page_dirty(page);
+	return AOP_WRITEPAGE_ACTIVATE;
+}
+
+#define MAX_DESIRED_PAGES_WP	4096
+
+int f2fs_write_data_pages(struct address_space *mapping,
+			    struct writeback_control *wbc)
+{
+	struct inode *inode = mapping->host;
+	struct f2fs_sb_info *sbi = F2FS_SB(inode->i_sb);
+	int ret;
+	long excess_nrtw = 0, desired_nrtw;
+
+	if (wbc->nr_to_write < MAX_DESIRED_PAGES_WP) {
+		desired_nrtw = MAX_DESIRED_PAGES_WP;
+		excess_nrtw = desired_nrtw - wbc->nr_to_write;
+		wbc->nr_to_write = desired_nrtw;
+	}
+
+	if (!S_ISDIR(inode->i_mode))
+		mutex_lock(&sbi->writepages);
+	ret = generic_writepages(mapping, wbc);
+	if (!S_ISDIR(inode->i_mode))
+		mutex_unlock(&sbi->writepages);
+	f2fs_submit_bio(sbi, DATA, (wbc->sync_mode == WB_SYNC_ALL));
+
+	remove_dirty_dir_inode(inode);
+
+	wbc->nr_to_write -= excess_nrtw;
+	return ret;
+}
+
+static int f2fs_write_begin(struct file *file, struct address_space *mapping,
+		loff_t pos, unsigned len, unsigned flags,
+		struct page **pagep, void **fsdata)
+{
+	struct inode *inode = mapping->host;
+	struct f2fs_sb_info *sbi = F2FS_SB(inode->i_sb);
+	struct page *page;
+	pgoff_t index = ((unsigned long long) pos) >> PAGE_CACHE_SHIFT;
+	struct dnode_of_data dn;
+	int err = 0;
+
+	/* for nobh_write_end */
+	*fsdata = NULL;
+
+	f2fs_balance_fs(sbi);
+
+	page = grab_cache_page_write_begin(mapping, index, flags);
+	if (!page)
+		return -ENOMEM;
+	*pagep = page;
+
+	mutex_lock_op(sbi, DATA_NEW);
+
+	set_new_dnode(&dn, inode, NULL, NULL, 0);
+	err = get_dnode_of_data(&dn, index, 0);
+	if (err) {
+		mutex_unlock_op(sbi, DATA_NEW);
+		f2fs_put_page(page, 1);
+		return err;
+	}
+
+	if (dn.data_blkaddr == NULL_ADDR) {
+		err = reserve_new_block(&dn);
+		if (err) {
+			f2fs_put_dnode(&dn);
+			mutex_unlock_op(sbi, DATA_NEW);
+			f2fs_put_page(page, 1);
+			return err;
+		}
+	}
+	f2fs_put_dnode(&dn);
+
+	mutex_unlock_op(sbi, DATA_NEW);
+
+	if ((len == PAGE_CACHE_SIZE) || PageUptodate(page))
+		return 0;
+
+	if ((pos & PAGE_CACHE_MASK) >= i_size_read(inode)) {
+		unsigned start = pos & (PAGE_CACHE_SIZE - 1);
+		unsigned end = start + len;
+
+		/* Reading beyond i_size is simple: memset to zero */
+		zero_user_segments(page, 0, start, end, PAGE_CACHE_SIZE);
+		return 0;
+	}
+
+	if (dn.data_blkaddr == NEW_ADDR) {
+		zero_user_segment(page, 0, PAGE_CACHE_SIZE);
+	} else {
+		err = f2fs_readpage(sbi, page, dn.data_blkaddr, READ_SYNC);
+		if (err) {
+			f2fs_put_page(page, 1);
+			return err;
+		}
+	}
+	SetPageUptodate(page);
+	clear_cold_data(page);
+	return 0;
+}
+
+static ssize_t f2fs_direct_IO(int rw, struct kiocb *iocb,
+		const struct iovec *iov, loff_t offset, unsigned long nr_segs)
+{
+	struct file *file = iocb->ki_filp;
+	struct inode *inode = file->f_mapping->host;
+
+	if (rw == WRITE)
+		return 0;
+
+	/* Needs synchronization with the cleaner */
+	return blockdev_direct_IO(rw, iocb, inode, iov, offset, nr_segs,
+						  get_data_block_ro);
+}
+
+static void f2fs_invalidate_data_page(struct page *page, unsigned long offset)
+{
+	struct inode *inode = page->mapping->host;
+	struct f2fs_sb_info *sbi = F2FS_SB(inode->i_sb);
+	if (S_ISDIR(inode->i_mode) && PageDirty(page)) {
+		dec_page_count(sbi, F2FS_DIRTY_DENTS);
+		inode_dec_dirty_dents(inode);
+	}
+	ClearPagePrivate(page);
+}
+
+static int f2fs_release_data_page(struct page *page, gfp_t wait)
+{
+	ClearPagePrivate(page);
+	return 0;
+}
+
+static int f2fs_set_data_page_dirty(struct page *page)
+{
+	struct address_space *mapping = page->mapping;
+	struct inode *inode = mapping->host;
+
+	SetPageUptodate(page);
+	if (!PageDirty(page)) {
+		__set_page_dirty_nobuffers(page);
+		set_dirty_dir_page(inode, page);
+		return 1;
+	}
+	return 0;
+}
+
+const struct address_space_operations f2fs_dblock_aops = {
+	.readpage	= f2fs_read_data_page,
+	.readpages	= f2fs_read_data_pages,
+	.writepage	= f2fs_write_data_page,
+	.writepages	= f2fs_write_data_pages,
+	.write_begin	= f2fs_write_begin,
+	.write_end	= nobh_write_end,
+	.set_page_dirty	= f2fs_set_data_page_dirty,
+	.invalidatepage	= f2fs_invalidate_data_page,
+	.releasepage	= f2fs_release_data_page,
+	.direct_IO	= f2fs_direct_IO,
+};
