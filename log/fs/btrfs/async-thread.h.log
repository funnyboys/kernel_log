commit f0cc2cd70164efe8f75c5d99560f0f69969c72e4
Author: Filipe Manana <fdmanana@suse.com>
Date:   Fri Feb 28 13:04:36 2020 +0000

    Btrfs: fix crash during unmount due to race with delayed inode workers
    
    During unmount we can have a job from the delayed inode items work queue
    still running, that can lead to at least two bad things:
    
    1) A crash, because the worker can try to create a transaction just
       after the fs roots were freed;
    
    2) A transaction leak, because the worker can create a transaction
       before the fs roots are freed and just after we committed the last
       transaction and after we stopped the transaction kthread.
    
    A stack trace example of the crash:
    
     [79011.691214] kernel BUG at lib/radix-tree.c:982!
     [79011.692056] invalid opcode: 0000 [#1] PREEMPT SMP DEBUG_PAGEALLOC PTI
     [79011.693180] CPU: 3 PID: 1394 Comm: kworker/u8:2 Tainted: G        W         5.6.0-rc2-btrfs-next-54 #2
     (...)
     [79011.696789] Workqueue: btrfs-delayed-meta btrfs_work_helper [btrfs]
     [79011.697904] RIP: 0010:radix_tree_tag_set+0xe7/0x170
     (...)
     [79011.702014] RSP: 0018:ffffb3c84a317ca0 EFLAGS: 00010293
     [79011.702949] RAX: 0000000000000000 RBX: 0000000000000000 RCX: 0000000000000000
     [79011.704202] RDX: ffffb3c84a317cb0 RSI: ffffb3c84a317ca8 RDI: ffff8db3931340a0
     [79011.705463] RBP: 0000000000000005 R08: 0000000000000005 R09: ffffffff974629d0
     [79011.706756] R10: ffffb3c84a317bc0 R11: 0000000000000001 R12: ffff8db393134000
     [79011.708010] R13: ffff8db3931340a0 R14: ffff8db393134068 R15: 0000000000000001
     [79011.709270] FS:  0000000000000000(0000) GS:ffff8db3b6a00000(0000) knlGS:0000000000000000
     [79011.710699] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
     [79011.711710] CR2: 00007f22c2a0a000 CR3: 0000000232ad4005 CR4: 00000000003606e0
     [79011.712958] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
     [79011.714205] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
     [79011.715448] Call Trace:
     [79011.715925]  record_root_in_trans+0x72/0xf0 [btrfs]
     [79011.716819]  btrfs_record_root_in_trans+0x4b/0x70 [btrfs]
     [79011.717925]  start_transaction+0xdd/0x5c0 [btrfs]
     [79011.718829]  btrfs_async_run_delayed_root+0x17e/0x2b0 [btrfs]
     [79011.719915]  btrfs_work_helper+0xaa/0x720 [btrfs]
     [79011.720773]  process_one_work+0x26d/0x6a0
     [79011.721497]  worker_thread+0x4f/0x3e0
     [79011.722153]  ? process_one_work+0x6a0/0x6a0
     [79011.722901]  kthread+0x103/0x140
     [79011.723481]  ? kthread_create_worker_on_cpu+0x70/0x70
     [79011.724379]  ret_from_fork+0x3a/0x50
     (...)
    
    The following diagram shows a sequence of steps that lead to the crash
    during ummount of the filesystem:
    
            CPU 1                                             CPU 2                                CPU 3
    
     btrfs_punch_hole()
       btrfs_btree_balance_dirty()
         btrfs_balance_delayed_items()
           --> sees
               fs_info->delayed_root->items
               with value 200, which is greater
               than
               BTRFS_DELAYED_BACKGROUND (128)
               and smaller than
               BTRFS_DELAYED_WRITEBACK (512)
           btrfs_wq_run_delayed_node()
             --> queues a job for
                 fs_info->delayed_workers to run
                 btrfs_async_run_delayed_root()
    
                                                                                                btrfs_async_run_delayed_root()
                                                                                                  --> job queued by CPU 1
    
                                                                                                  --> starts picking and running
                                                                                                      delayed nodes from the
                                                                                                      prepare_list list
    
                                                     close_ctree()
    
                                                       btrfs_delete_unused_bgs()
    
                                                       btrfs_commit_super()
    
                                                         btrfs_join_transaction()
                                                           --> gets transaction N
    
                                                         btrfs_commit_transaction(N)
                                                           --> set transaction state
                                                            to TRANTS_STATE_COMMIT_START
    
                                                                                                 btrfs_first_prepared_delayed_node()
                                                                                                   --> picks delayed node X through
                                                                                                       the prepared_list list
    
                                                           btrfs_run_delayed_items()
    
                                                             btrfs_first_delayed_node()
                                                               --> also picks delayed node X
                                                                   but through the node_list
                                                                   list
    
                                                             __btrfs_commit_inode_delayed_items()
                                                                --> runs all delayed items from
                                                                    this node and drops the
                                                                    node's item count to 0
                                                                    through call to
                                                                    btrfs_release_delayed_inode()
    
                                                             --> finishes running any remaining
                                                                 delayed nodes
    
                                                           --> finishes transaction commit
    
                                                       --> stops cleaner and transaction threads
    
                                                       btrfs_free_fs_roots()
                                                         --> frees all roots and removes them
                                                             from the radix tree
                                                             fs_info->fs_roots_radix
    
                                                                                                 btrfs_join_transaction()
                                                                                                   start_transaction()
                                                                                                     btrfs_record_root_in_trans()
                                                                                                       record_root_in_trans()
                                                                                                         radix_tree_tag_set()
                                                                                                           --> crashes because
                                                                                                               the root is not in
                                                                                                               the radix tree
                                                                                                               anymore
    
    If the worker is able to call btrfs_join_transaction() before the unmount
    task frees the fs roots, we end up leaking a transaction and all its
    resources, since after the call to btrfs_commit_super() and stopping the
    transaction kthread, we don't expect to have any transaction open anymore.
    
    When this situation happens the worker has a delayed node that has no
    more items to run, since the task calling btrfs_run_delayed_items(),
    which is doing a transaction commit, picks the same node and runs all
    its items first.
    
    We can not wait for the worker to complete when running delayed items
    through btrfs_run_delayed_items(), because we call that function in
    several phases of a transaction commit, and that could cause a deadlock
    because the worker calls btrfs_join_transaction() and the task doing the
    transaction commit may have already set the transaction state to
    TRANS_STATE_COMMIT_DOING.
    
    Also it's not possible to get into a situation where only some of the
    items of a delayed node are added to the fs/subvolume tree in the current
    transaction and the remaining ones in the next transaction, because when
    running the items of a delayed inode we lock its mutex, effectively
    waiting for the worker if the worker is running the items of the delayed
    node already.
    
    Since this can only cause issues when unmounting a filesystem, fix it in
    a simple way by waiting for any jobs on the delayed workers queue before
    calling btrfs_commit_supper() at close_ctree(). This works because at this
    point no one can call btrfs_btree_balance_dirty() or
    btrfs_balance_delayed_items(), and if we end up waiting for any worker to
    complete, btrfs_commit_super() will commit the transaction created by the
    worker.
    
    CC: stable@vger.kernel.org # 4.4+
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/async-thread.h b/fs/btrfs/async-thread.h
index a4434301d84d..3204daa51b95 100644
--- a/fs/btrfs/async-thread.h
+++ b/fs/btrfs/async-thread.h
@@ -44,5 +44,6 @@ void btrfs_set_work_high_priority(struct btrfs_work *work);
 struct btrfs_fs_info * __pure btrfs_work_owner(const struct btrfs_work *work);
 struct btrfs_fs_info * __pure btrfs_workqueue_owner(const struct __btrfs_workqueue *wq);
 bool btrfs_workqueue_normal_congested(const struct btrfs_workqueue *wq);
+void btrfs_flush_workqueue(struct btrfs_workqueue *wq);
 
 #endif

commit e1f60a6580c04d0d2492bb6034e968b8c29c78cf
Author: David Sterba <dsterba@suse.com>
Date:   Tue Oct 1 19:57:39 2019 +0200

    btrfs: add __pure attribute to functions
    
    The attribute is more relaxed than const and the functions could
    dereference pointers, as long as the observable state is not changed. We
    do have such functions, based on -Wsuggest-attribute=pure .
    
    The visible effects of this patch are negligible, there are differences
    in the assembly but hard to summarize.
    
    Reviewed-by: Nikolay Borisov <nborisov@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/async-thread.h b/fs/btrfs/async-thread.h
index c5bf2b117c05..a4434301d84d 100644
--- a/fs/btrfs/async-thread.h
+++ b/fs/btrfs/async-thread.h
@@ -41,8 +41,8 @@ void btrfs_queue_work(struct btrfs_workqueue *wq,
 void btrfs_destroy_workqueue(struct btrfs_workqueue *wq);
 void btrfs_workqueue_set_max(struct btrfs_workqueue *wq, int max);
 void btrfs_set_work_high_priority(struct btrfs_work *work);
-struct btrfs_fs_info *btrfs_work_owner(const struct btrfs_work *work);
-struct btrfs_fs_info *btrfs_workqueue_owner(const struct __btrfs_workqueue *wq);
+struct btrfs_fs_info * __pure btrfs_work_owner(const struct btrfs_work *work);
+struct btrfs_fs_info * __pure btrfs_workqueue_owner(const struct __btrfs_workqueue *wq);
 bool btrfs_workqueue_normal_congested(const struct btrfs_workqueue *wq);
 
 #endif

commit a0cac0ec961f0d42828eeef196ac2246a2f07659
Author: Omar Sandoval <osandov@fb.com>
Date:   Mon Sep 16 11:30:57 2019 -0700

    btrfs: get rid of unique workqueue helper functions
    
    Commit 9e0af2376434 ("Btrfs: fix task hang under heavy compressed
    write") worked around the issue that a recycled work item could get a
    false dependency on the original work item due to how the workqueue code
    guarantees non-reentrancy. It did so by giving different work functions
    to different types of work.
    
    However, the fixes in the previous few patches are more complete, as
    they prevent a work item from being recycled at all (except for a tiny
    window that the kernel workqueue code handles for us). This obsoletes
    the previous fix, so we don't need the unique helpers for correctness.
    The only other reason to keep them would be so they show up in stack
    traces, but they always seem to be optimized to a tail call, so they
    don't show up anyways. So, let's just get rid of the extra indirection.
    
    While we're here, rename normal_work_helper() to the more informative
    btrfs_work_helper().
    
    Reviewed-by: Nikolay Borisov <nborisov@suse.com>
    Reviewed-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/async-thread.h b/fs/btrfs/async-thread.h
index 7861c9feba5f..c5bf2b117c05 100644
--- a/fs/btrfs/async-thread.h
+++ b/fs/btrfs/async-thread.h
@@ -29,42 +29,13 @@ struct btrfs_work {
 	unsigned long flags;
 };
 
-#define BTRFS_WORK_HELPER_PROTO(name)					\
-void btrfs_##name(struct work_struct *arg)
-
-BTRFS_WORK_HELPER_PROTO(worker_helper);
-BTRFS_WORK_HELPER_PROTO(delalloc_helper);
-BTRFS_WORK_HELPER_PROTO(flush_delalloc_helper);
-BTRFS_WORK_HELPER_PROTO(cache_helper);
-BTRFS_WORK_HELPER_PROTO(submit_helper);
-BTRFS_WORK_HELPER_PROTO(fixup_helper);
-BTRFS_WORK_HELPER_PROTO(endio_helper);
-BTRFS_WORK_HELPER_PROTO(endio_meta_helper);
-BTRFS_WORK_HELPER_PROTO(endio_meta_write_helper);
-BTRFS_WORK_HELPER_PROTO(endio_raid56_helper);
-BTRFS_WORK_HELPER_PROTO(endio_repair_helper);
-BTRFS_WORK_HELPER_PROTO(rmw_helper);
-BTRFS_WORK_HELPER_PROTO(endio_write_helper);
-BTRFS_WORK_HELPER_PROTO(freespace_write_helper);
-BTRFS_WORK_HELPER_PROTO(delayed_meta_helper);
-BTRFS_WORK_HELPER_PROTO(readahead_helper);
-BTRFS_WORK_HELPER_PROTO(qgroup_rescan_helper);
-BTRFS_WORK_HELPER_PROTO(extent_refs_helper);
-BTRFS_WORK_HELPER_PROTO(scrub_helper);
-BTRFS_WORK_HELPER_PROTO(scrubwrc_helper);
-BTRFS_WORK_HELPER_PROTO(scrubnc_helper);
-BTRFS_WORK_HELPER_PROTO(scrubparity_helper);
-
-
 struct btrfs_workqueue *btrfs_alloc_workqueue(struct btrfs_fs_info *fs_info,
 					      const char *name,
 					      unsigned int flags,
 					      int limit_active,
 					      int thresh);
-void btrfs_init_work(struct btrfs_work *work, btrfs_work_func_t helper,
-		     btrfs_func_t func,
-		     btrfs_func_t ordered_func,
-		     btrfs_func_t ordered_free);
+void btrfs_init_work(struct btrfs_work *work, btrfs_func_t func,
+		     btrfs_func_t ordered_func, btrfs_func_t ordered_free);
 void btrfs_queue_work(struct btrfs_workqueue *wq,
 		      struct btrfs_work *work);
 void btrfs_destroy_workqueue(struct btrfs_workqueue *wq);

commit 9888c3402c8567a977de37f61e9dd87792723064
Author: David Sterba <dsterba@suse.com>
Date:   Tue Apr 3 19:16:55 2018 +0200

    btrfs: replace GPL boilerplate by SPDX -- headers
    
    Remove GPL boilerplate text (long, short, one-line) and keep the rest,
    ie. personal, company or original source copyright statements. Add the
    SPDX header.
    
    Unify the include protection macros to match the file names.
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/async-thread.h b/fs/btrfs/async-thread.h
index fc957e00cef1..7861c9feba5f 100644
--- a/fs/btrfs/async-thread.h
+++ b/fs/btrfs/async-thread.h
@@ -1,24 +1,12 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 /*
  * Copyright (C) 2007 Oracle.  All rights reserved.
  * Copyright (C) 2014 Fujitsu.  All rights reserved.
- *
- * This program is free software; you can redistribute it and/or
- * modify it under the terms of the GNU General Public
- * License v2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * General Public License for more details.
- *
- * You should have received a copy of the GNU General Public
- * License along with this program; if not, write to the
- * Free Software Foundation, Inc., 59 Temple Place - Suite 330,
- * Boston, MA 021110-1307, USA.
  */
 
-#ifndef __BTRFS_ASYNC_THREAD_
-#define __BTRFS_ASYNC_THREAD_
+#ifndef BTRFS_ASYNC_THREAD_H
+#define BTRFS_ASYNC_THREAD_H
+
 #include <linux/workqueue.h>
 
 struct btrfs_fs_info;
@@ -85,4 +73,5 @@ void btrfs_set_work_high_priority(struct btrfs_work *work);
 struct btrfs_fs_info *btrfs_work_owner(const struct btrfs_work *work);
 struct btrfs_fs_info *btrfs_workqueue_owner(const struct __btrfs_workqueue *wq);
 bool btrfs_workqueue_normal_congested(const struct btrfs_workqueue *wq);
+
 #endif

commit 9a35b63728ceb8602c111260044451dd64952500
Author: Jeff Mahoney <jeffm@suse.com>
Date:   Wed Jun 28 21:56:54 2017 -0600

    btrfs: constify tracepoint arguments
    
    Tracepoint arguments are all read-only.  If we mark the arguments
    as const, we're able to keep or convert those arguments to const
    where appropriate.
    
    Signed-off-by: Jeff Mahoney <jeffm@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/async-thread.h b/fs/btrfs/async-thread.h
index 1f9597355c9d..fc957e00cef1 100644
--- a/fs/btrfs/async-thread.h
+++ b/fs/btrfs/async-thread.h
@@ -82,7 +82,7 @@ void btrfs_queue_work(struct btrfs_workqueue *wq,
 void btrfs_destroy_workqueue(struct btrfs_workqueue *wq);
 void btrfs_workqueue_set_max(struct btrfs_workqueue *wq, int max);
 void btrfs_set_work_high_priority(struct btrfs_work *work);
-struct btrfs_fs_info *btrfs_work_owner(struct btrfs_work *work);
-struct btrfs_fs_info *btrfs_workqueue_owner(struct __btrfs_workqueue *wq);
-bool btrfs_workqueue_normal_congested(struct btrfs_workqueue *wq);
+struct btrfs_fs_info *btrfs_work_owner(const struct btrfs_work *work);
+struct btrfs_fs_info *btrfs_workqueue_owner(const struct __btrfs_workqueue *wq);
+bool btrfs_workqueue_normal_congested(const struct btrfs_workqueue *wq);
 #endif

commit 2939e1a86f758b55cdba73e29397dd3d94df13bc
Author: Maxim Patlasov <mpatlasov@virtuozzo.com>
Date:   Mon Dec 12 14:32:44 2016 -0800

    btrfs: limit async_work allocation and worker func duration
    
    Problem statement: unprivileged user who has read-write access to more than
    one btrfs subvolume may easily consume all kernel memory (eventually
    triggering oom-killer).
    
    Reproducer (./mkrmdir below essentially loops over mkdir/rmdir):
    
    [root@kteam1 ~]# cat prep.sh
    
    DEV=/dev/sdb
    mkfs.btrfs -f $DEV
    mount $DEV /mnt
    for i in `seq 1 16`
    do
            mkdir /mnt/$i
            btrfs subvolume create /mnt/SV_$i
            ID=`btrfs subvolume list /mnt |grep "SV_$i$" |cut -d ' ' -f 2`
            mount -t btrfs -o subvolid=$ID $DEV /mnt/$i
            chmod a+rwx /mnt/$i
    done
    
    [root@kteam1 ~]# sh prep.sh
    
    [maxim@kteam1 ~]$ for i in `seq 1 16`; do ./mkrmdir /mnt/$i 2000 2000 & done
    
    [root@kteam1 ~]# for i in `seq 1 4`; do grep "kmalloc-128" /proc/slabinfo | grep -v dma; sleep 60; done
    kmalloc-128        10144  10144    128   32    1 : tunables    0    0    0 : slabdata    317    317      0
    kmalloc-128       9992352 9992352    128   32    1 : tunables    0    0    0 : slabdata 312261 312261      0
    kmalloc-128       24226752 24226752    128   32    1 : tunables    0    0    0 : slabdata 757086 757086      0
    kmalloc-128       42754240 42754240    128   32    1 : tunables    0    0    0 : slabdata 1336070 1336070      0
    
    The huge numbers above come from insane number of async_work-s allocated
    and queued by btrfs_wq_run_delayed_node.
    
    The problem is caused by btrfs_wq_run_delayed_node() queuing more and more
    works if the number of delayed items is above BTRFS_DELAYED_BACKGROUND. The
    worker func (btrfs_async_run_delayed_root) processes at least
    BTRFS_DELAYED_BATCH items (if they are present in the list). So, the machinery
    works as expected while the list is almost empty. As soon as it is getting
    bigger, worker func starts to process more than one item at a time, it takes
    longer, and the chances to have async_works queued more than needed is getting
    higher.
    
    The problem above is worsened by another flaw of delayed-inode implementation:
    if async_work was queued in a throttling branch (number of items >=
    BTRFS_DELAYED_WRITEBACK), corresponding worker func won't quit until
    the number of items < BTRFS_DELAYED_BACKGROUND / 2. So, it is possible that
    the func occupies CPU infinitely (up to 30sec in my experiments): while the
    func is trying to drain the list, the user activity may add more and more
    items to the list.
    
    The patch fixes both problems in straightforward way: refuse queuing too
    many works in btrfs_wq_run_delayed_node and bail out of worker func if
    at least BTRFS_DELAYED_WRITEBACK items are processed.
    
    Changed in v2: remove support of thresh == NO_THRESHOLD.
    
    Signed-off-by: Maxim Patlasov <mpatlasov@virtuozzo.com>
    Signed-off-by: Chris Mason <clm@fb.com>
    Cc: stable@vger.kernel.org # v3.15+

diff --git a/fs/btrfs/async-thread.h b/fs/btrfs/async-thread.h
index 8e52484cd461..1f9597355c9d 100644
--- a/fs/btrfs/async-thread.h
+++ b/fs/btrfs/async-thread.h
@@ -84,4 +84,5 @@ void btrfs_workqueue_set_max(struct btrfs_workqueue *wq, int max);
 void btrfs_set_work_high_priority(struct btrfs_work *work);
 struct btrfs_fs_info *btrfs_work_owner(struct btrfs_work *work);
 struct btrfs_fs_info *btrfs_workqueue_owner(struct __btrfs_workqueue *wq);
+bool btrfs_workqueue_normal_congested(struct btrfs_workqueue *wq);
 #endif

commit cb001095ca705dcd95f57fe98867e38a4889916d
Author: Jeff Mahoney <jeffm@suse.com>
Date:   Thu Jun 9 16:22:11 2016 -0400

    btrfs: plumb fs_info into btrfs_work
    
    In order to provide an fsid for trace events, we'll need a btrfs_fs_info
    pointer.  The most lightweight way to do that for btrfs_work structures
    is to associate it with the __btrfs_workqueue structure.  Each queued
    btrfs_work structure has a workqueue associated with it, so that's
    a natural fit.  It's a privately defined structures, so we add accessors
    to retrieve the fs_info pointer.
    
    Signed-off-by: Jeff Mahoney <jeffm@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/async-thread.h b/fs/btrfs/async-thread.h
index ad4d0647d1a6..8e52484cd461 100644
--- a/fs/btrfs/async-thread.h
+++ b/fs/btrfs/async-thread.h
@@ -21,6 +21,7 @@
 #define __BTRFS_ASYNC_THREAD_
 #include <linux/workqueue.h>
 
+struct btrfs_fs_info;
 struct btrfs_workqueue;
 /* Internal use only */
 struct __btrfs_workqueue;
@@ -67,7 +68,8 @@ BTRFS_WORK_HELPER_PROTO(scrubnc_helper);
 BTRFS_WORK_HELPER_PROTO(scrubparity_helper);
 
 
-struct btrfs_workqueue *btrfs_alloc_workqueue(const char *name,
+struct btrfs_workqueue *btrfs_alloc_workqueue(struct btrfs_fs_info *fs_info,
+					      const char *name,
 					      unsigned int flags,
 					      int limit_active,
 					      int thresh);
@@ -80,4 +82,6 @@ void btrfs_queue_work(struct btrfs_workqueue *wq,
 void btrfs_destroy_workqueue(struct btrfs_workqueue *wq);
 void btrfs_workqueue_set_max(struct btrfs_workqueue *wq, int max);
 void btrfs_set_work_high_priority(struct btrfs_work *work);
+struct btrfs_fs_info *btrfs_work_owner(struct btrfs_work *work);
+struct btrfs_fs_info *btrfs_workqueue_owner(struct __btrfs_workqueue *wq);
 #endif

commit c6dd6ea55758cf403bdc07a51a06c2a1d474f906
Author: Qu Wenruo <quwenruo@cn.fujitsu.com>
Date:   Thu Aug 20 09:30:39 2015 +0800

    btrfs: async_thread: Fix workqueue 'max_active' value when initializing
    
    At initializing time, for threshold-able workqueue, it's max_active
    of kernel workqueue should be 1 and grow if it hits threshold.
    
    But due to the bad naming, there is both 'max_active' for kernel
    workqueue and btrfs workqueue.
    So wrong value is given at workqueue initialization.
    
    This patch fixes it, and to avoid further misunderstanding, change the
    member name of btrfs_workqueue to 'current_active' and 'limit_active'.
    
    Also corresponding comment is added for readability.
    
    Reported-by: Alex Lyakas <alex.btrfs@zadarastorage.com>
    Signed-off-by: Qu Wenruo <quwenruo@cn.fujitsu.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/async-thread.h b/fs/btrfs/async-thread.h
index b0b093b6afec..ad4d0647d1a6 100644
--- a/fs/btrfs/async-thread.h
+++ b/fs/btrfs/async-thread.h
@@ -69,7 +69,7 @@ BTRFS_WORK_HELPER_PROTO(scrubparity_helper);
 
 struct btrfs_workqueue *btrfs_alloc_workqueue(const char *name,
 					      unsigned int flags,
-					      int max_active,
+					      int limit_active,
 					      int thresh);
 void btrfs_init_work(struct btrfs_work *work, btrfs_work_func_t helper,
 		     btrfs_func_t func,

commit 20b2e3029eef277cd93a46a991004260057e1a9e
Author: Zhao Lei <zhaolei@cn.fujitsu.com>
Date:   Thu Jun 4 20:09:15 2015 +0800

    btrfs: Fix lockdep warning of wr_ctx->wr_lock in scrub_free_wr_ctx()
    
    lockdep report following warning in test:
     [25176.843958] =================================
     [25176.844519] [ INFO: inconsistent lock state ]
     [25176.845047] 4.1.0-rc3 #22 Tainted: G        W
     [25176.845591] ---------------------------------
     [25176.846153] inconsistent {SOFTIRQ-ON-W} -> {IN-SOFTIRQ-W} usage.
     [25176.846713] fsstress/26661 [HC0[0]:SC1[1]:HE1:SE0] takes:
     [25176.847246]  (&wr_ctx->wr_lock){+.?...}, at: [<ffffffffa04cdc6d>] scrub_free_ctx+0x2d/0xf0 [btrfs]
     [25176.847838] {SOFTIRQ-ON-W} state was registered at:
     [25176.848396]   [<ffffffff810bf460>] __lock_acquire+0x6a0/0xe10
     [25176.848955]   [<ffffffff810bfd1e>] lock_acquire+0xce/0x2c0
     [25176.849491]   [<ffffffff816489af>] mutex_lock_nested+0x7f/0x410
     [25176.850029]   [<ffffffffa04d04ff>] scrub_stripe+0x4df/0x1080 [btrfs]
     [25176.850575]   [<ffffffffa04d11b1>] scrub_chunk.isra.19+0x111/0x130 [btrfs]
     [25176.851110]   [<ffffffffa04d144c>] scrub_enumerate_chunks+0x27c/0x510 [btrfs]
     [25176.851660]   [<ffffffffa04d3b87>] btrfs_scrub_dev+0x1c7/0x6c0 [btrfs]
     [25176.852189]   [<ffffffffa04e918e>] btrfs_dev_replace_start+0x36e/0x450 [btrfs]
     [25176.852771]   [<ffffffffa04a98e0>] btrfs_ioctl+0x1e10/0x2d20 [btrfs]
     [25176.853315]   [<ffffffff8121c5b8>] do_vfs_ioctl+0x318/0x570
     [25176.853868]   [<ffffffff8121c851>] SyS_ioctl+0x41/0x80
     [25176.854406]   [<ffffffff8164da17>] system_call_fastpath+0x12/0x6f
     [25176.854935] irq event stamp: 51506
     [25176.855511] hardirqs last  enabled at (51506): [<ffffffff810d4ce5>] vprintk_emit+0x225/0x5e0
     [25176.856059] hardirqs last disabled at (51505): [<ffffffff810d4b77>] vprintk_emit+0xb7/0x5e0
     [25176.856642] softirqs last  enabled at (50886): [<ffffffff81067a23>] __do_softirq+0x363/0x640
     [25176.857184] softirqs last disabled at (50949): [<ffffffff8106804d>] irq_exit+0x10d/0x120
     [25176.857746]
     other info that might help us debug this:
     [25176.858845]  Possible unsafe locking scenario:
     [25176.859981]        CPU0
     [25176.860537]        ----
     [25176.861059]   lock(&wr_ctx->wr_lock);
     [25176.861705]   <Interrupt>
     [25176.862272]     lock(&wr_ctx->wr_lock);
     [25176.862881]
      *** DEADLOCK ***
    
    Reason:
     Above warning is caused by:
     Interrupt
     -> bio_endio()
     -> ...
     -> scrub_put_ctx()
     -> scrub_free_ctx() *1
     -> ...
     -> mutex_lock(&wr_ctx->wr_lock);
    
     scrub_put_ctx() is allowed to be called in end_bio interrupt, but
     in code design, it will never call scrub_free_ctx(sctx) in interrupe
     context(above *1), because btrfs_scrub_dev() get one additional
     reference of sctx->refs, which makes scrub_free_ctx() only called
     withine btrfs_scrub_dev().
    
     Now the code runs out of our wish, because free sequence in
     scrub_pending_bio_dec() have a gap.
    
     Current code:
     -----------------------------------+-----------------------------------
     scrub_pending_bio_dec()            |  btrfs_scrub_dev
     -----------------------------------+-----------------------------------
     atomic_dec(&sctx->bios_in_flight); |
     wake_up(&sctx->list_wait);         |
                                        | scrub_put_ctx()
                                        | -> atomic_dec_and_test(&sctx->refs)
     scrub_put_ctx(sctx);               |
     -> atomic_dec_and_test(&sctx->refs)|
     -> scrub_free_ctx()                |
     -----------------------------------+-----------------------------------
    
     We expected:
     -----------------------------------+-----------------------------------
     scrub_pending_bio_dec()            |  btrfs_scrub_dev
     -----------------------------------+-----------------------------------
     atomic_dec(&sctx->bios_in_flight); |
     wake_up(&sctx->list_wait);         |
     scrub_put_ctx(sctx);               |
     -> atomic_dec_and_test(&sctx->refs)|
                                        | scrub_put_ctx()
                                        | -> atomic_dec_and_test(&sctx->refs)
                                        | -> scrub_free_ctx()
     -----------------------------------+-----------------------------------
    
    Fix:
     Move scrub_pending_bio_dec() to a workqueue, to avoid this function run
     in interrupt context.
     Tested by check tracelog in debug.
    
    Changelog v1->v2:
     Use workqueue instead of adjust function call sequence in v1,
     because v1 will introduce a bug pointed out by:
     Filipe David Manana <fdmanana@gmail.com>
    
    Reported-by: Qu Wenruo <quwenruo@cn.fujitsu.com>
    Signed-off-by: Zhao Lei <zhaolei@cn.fujitsu.com>
    Reviewed-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/async-thread.h b/fs/btrfs/async-thread.h
index ec2ee477f8ba..b0b093b6afec 100644
--- a/fs/btrfs/async-thread.h
+++ b/fs/btrfs/async-thread.h
@@ -64,6 +64,8 @@ BTRFS_WORK_HELPER_PROTO(extent_refs_helper);
 BTRFS_WORK_HELPER_PROTO(scrub_helper);
 BTRFS_WORK_HELPER_PROTO(scrubwrc_helper);
 BTRFS_WORK_HELPER_PROTO(scrubnc_helper);
+BTRFS_WORK_HELPER_PROTO(scrubparity_helper);
+
 
 struct btrfs_workqueue *btrfs_alloc_workqueue(const char *name,
 					      unsigned int flags,

commit 6f0110581911623df08bf4b65fdef4548ebcda0a
Author: David Sterba <dsterba@suse.cz>
Date:   Mon Feb 16 18:34:01 2015 +0100

    btrfs: use correct type for workqueue flags
    
    Through all the local wrappers to alloc_workqueue, __alloc_workqueue_key
    takes an unsigned int.
    
    Signed-off-by: David Sterba <dsterba@suse.cz>

diff --git a/fs/btrfs/async-thread.h b/fs/btrfs/async-thread.h
index e386c29ef1f6..ec2ee477f8ba 100644
--- a/fs/btrfs/async-thread.h
+++ b/fs/btrfs/async-thread.h
@@ -66,7 +66,7 @@ BTRFS_WORK_HELPER_PROTO(scrubwrc_helper);
 BTRFS_WORK_HELPER_PROTO(scrubnc_helper);
 
 struct btrfs_workqueue *btrfs_alloc_workqueue(const char *name,
-					      int flags,
+					      unsigned int flags,
 					      int max_active,
 					      int thresh);
 void btrfs_init_work(struct btrfs_work *work, btrfs_work_func_t helper,

commit 8b110e393c5a6e72d50fcdf9fa7ed8b647cfdfc9
Author: Miao Xie <miaox@cn.fujitsu.com>
Date:   Fri Sep 12 18:44:03 2014 +0800

    Btrfs: implement repair function when direct read fails
    
    This patch implement data repair function when direct read fails.
    
    The detail of the implementation is:
    - When we find the data is not right, we try to read the data from the other
      mirror.
    - When the io on the mirror ends, we will insert the endio work into the
      dedicated btrfs workqueue, not common read endio workqueue, because the
      original endio work is still blocked in the btrfs endio workqueue, if we
      insert the endio work of the io on the mirror into that workqueue, deadlock
      would happen.
    - After we get right data, we write it back to the corrupted mirror.
    - And if the data on the new mirror is still corrupted, we will try next
      mirror until we read right data or all the mirrors are traversed.
    - After the above work, we set the uptodate flag according to the result.
    
    Signed-off-by: Miao Xie <miaox@cn.fujitsu.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/async-thread.h b/fs/btrfs/async-thread.h
index e9e31c94758f..e386c29ef1f6 100644
--- a/fs/btrfs/async-thread.h
+++ b/fs/btrfs/async-thread.h
@@ -53,6 +53,7 @@ BTRFS_WORK_HELPER_PROTO(endio_helper);
 BTRFS_WORK_HELPER_PROTO(endio_meta_helper);
 BTRFS_WORK_HELPER_PROTO(endio_meta_write_helper);
 BTRFS_WORK_HELPER_PROTO(endio_raid56_helper);
+BTRFS_WORK_HELPER_PROTO(endio_repair_helper);
 BTRFS_WORK_HELPER_PROTO(rmw_helper);
 BTRFS_WORK_HELPER_PROTO(endio_write_helper);
 BTRFS_WORK_HELPER_PROTO(freespace_write_helper);

commit 9e0af23764344f7f1b68e4eefbe7dc865018b63d
Author: Liu Bo <bo.li.liu@oracle.com>
Date:   Fri Aug 15 23:36:53 2014 +0800

    Btrfs: fix task hang under heavy compressed write
    
    This has been reported and discussed for a long time, and this hang occurs in
    both 3.15 and 3.16.
    
    Btrfs now migrates to use kernel workqueue, but it introduces this hang problem.
    
    Btrfs has a kind of work queued as an ordered way, which means that its
    ordered_func() must be processed in the way of FIFO, so it usually looks like --
    
    normal_work_helper(arg)
        work = container_of(arg, struct btrfs_work, normal_work);
    
        work->func() <---- (we name it work X)
        for ordered_work in wq->ordered_list
                ordered_work->ordered_func()
                ordered_work->ordered_free()
    
    The hang is a rare case, first when we find free space, we get an uncached block
    group, then we go to read its free space cache inode for free space information,
    so it will
    
    file a readahead request
        btrfs_readpages()
             for page that is not in page cache
                    __do_readpage()
                         submit_extent_page()
                               btrfs_submit_bio_hook()
                                     btrfs_bio_wq_end_io()
                                     submit_bio()
                                     end_workqueue_bio() <--(ret by the 1st endio)
                                          queue a work(named work Y) for the 2nd
                                          also the real endio()
    
    So the hang occurs when work Y's work_struct and work X's work_struct happens
    to share the same address.
    
    A bit more explanation,
    
    A,B,C -- struct btrfs_work
    arg   -- struct work_struct
    
    kthread:
    worker_thread()
        pick up a work_struct from @worklist
        process_one_work(arg)
            worker->current_work = arg;  <-- arg is A->normal_work
            worker->current_func(arg)
                    normal_work_helper(arg)
                         A = container_of(arg, struct btrfs_work, normal_work);
    
                         A->func()
                         A->ordered_func()
                         A->ordered_free()  <-- A gets freed
    
                         B->ordered_func()
                              submit_compressed_extents()
                                  find_free_extent()
                                      load_free_space_inode()
                                          ...   <-- (the above readhead stack)
                                          end_workqueue_bio()
                                               btrfs_queue_work(work C)
                         B->ordered_free()
    
    As if work A has a high priority in wq->ordered_list and there are more ordered
    works queued after it, such as B->ordered_func(), its memory could have been
    freed before normal_work_helper() returns, which means that kernel workqueue
    code worker_thread() still has worker->current_work pointer to be work
    A->normal_work's, ie. arg's address.
    
    Meanwhile, work C is allocated after work A is freed, work C->normal_work
    and work A->normal_work are likely to share the same address(I confirmed this
    with ftrace output, so I'm not just guessing, it's rare though).
    
    When another kthread picks up work C->normal_work to process, and finds our
    kthread is processing it(see find_worker_executing_work()), it'll think
    work C as a collision and skip then, which ends up nobody processing work C.
    
    So the situation is that our kthread is waiting forever on work C.
    
    Besides, there're other cases that can lead to deadlock, but the real problem
    is that all btrfs workqueue shares one work->func, -- normal_work_helper,
    so this makes each workqueue to have its own helper function, but only a
    wraper pf normal_work_helper.
    
    With this patch, I no long hit the above hang.
    
    Signed-off-by: Liu Bo <bo.li.liu@oracle.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/async-thread.h b/fs/btrfs/async-thread.h
index 9c6b66d15fb0..e9e31c94758f 100644
--- a/fs/btrfs/async-thread.h
+++ b/fs/btrfs/async-thread.h
@@ -19,12 +19,14 @@
 
 #ifndef __BTRFS_ASYNC_THREAD_
 #define __BTRFS_ASYNC_THREAD_
+#include <linux/workqueue.h>
 
 struct btrfs_workqueue;
 /* Internal use only */
 struct __btrfs_workqueue;
 struct btrfs_work;
 typedef void (*btrfs_func_t)(struct btrfs_work *arg);
+typedef void (*btrfs_work_func_t)(struct work_struct *arg);
 
 struct btrfs_work {
 	btrfs_func_t func;
@@ -38,11 +40,35 @@ struct btrfs_work {
 	unsigned long flags;
 };
 
+#define BTRFS_WORK_HELPER_PROTO(name)					\
+void btrfs_##name(struct work_struct *arg)
+
+BTRFS_WORK_HELPER_PROTO(worker_helper);
+BTRFS_WORK_HELPER_PROTO(delalloc_helper);
+BTRFS_WORK_HELPER_PROTO(flush_delalloc_helper);
+BTRFS_WORK_HELPER_PROTO(cache_helper);
+BTRFS_WORK_HELPER_PROTO(submit_helper);
+BTRFS_WORK_HELPER_PROTO(fixup_helper);
+BTRFS_WORK_HELPER_PROTO(endio_helper);
+BTRFS_WORK_HELPER_PROTO(endio_meta_helper);
+BTRFS_WORK_HELPER_PROTO(endio_meta_write_helper);
+BTRFS_WORK_HELPER_PROTO(endio_raid56_helper);
+BTRFS_WORK_HELPER_PROTO(rmw_helper);
+BTRFS_WORK_HELPER_PROTO(endio_write_helper);
+BTRFS_WORK_HELPER_PROTO(freespace_write_helper);
+BTRFS_WORK_HELPER_PROTO(delayed_meta_helper);
+BTRFS_WORK_HELPER_PROTO(readahead_helper);
+BTRFS_WORK_HELPER_PROTO(qgroup_rescan_helper);
+BTRFS_WORK_HELPER_PROTO(extent_refs_helper);
+BTRFS_WORK_HELPER_PROTO(scrub_helper);
+BTRFS_WORK_HELPER_PROTO(scrubwrc_helper);
+BTRFS_WORK_HELPER_PROTO(scrubnc_helper);
+
 struct btrfs_workqueue *btrfs_alloc_workqueue(const char *name,
 					      int flags,
 					      int max_active,
 					      int thresh);
-void btrfs_init_work(struct btrfs_work *work,
+void btrfs_init_work(struct btrfs_work *work, btrfs_work_func_t helper,
 		     btrfs_func_t func,
 		     btrfs_func_t ordered_func,
 		     btrfs_func_t ordered_free);

commit c3a468915a384c0015263edd9b7263775599a323
Author: Qu Wenruo <quwenruo@cn.fujitsu.com>
Date:   Wed Mar 12 08:05:33 2014 +0000

    btrfs: Add trace for btrfs_workqueue alloc/destroy
    
    Since most of the btrfs_workqueue is printed as pointer address,
    for easier analysis, add trace for btrfs_workqueue alloc/destroy.
    So it is possible to determine the workqueue that a given work belongs
    to(by comparing the wq pointer address with alloc trace event).
    
    Signed-off-by: Qu Wenruo <quenruo@cn.fujitsu.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/async-thread.h b/fs/btrfs/async-thread.h
index 0a891cdc4c28..9c6b66d15fb0 100644
--- a/fs/btrfs/async-thread.h
+++ b/fs/btrfs/async-thread.h
@@ -38,7 +38,7 @@ struct btrfs_work {
 	unsigned long flags;
 };
 
-struct btrfs_workqueue *btrfs_alloc_workqueue(char *name,
+struct btrfs_workqueue *btrfs_alloc_workqueue(const char *name,
 					      int flags,
 					      int max_active,
 					      int thresh);

commit 6db8914f9763d3f0a7610b497d44f93a4c17e62e
Author: Qu Wenruo <quwenruo@cn.fujitsu.com>
Date:   Thu Mar 6 04:19:50 2014 +0000

    btrfs: Cleanup the btrfs_workqueue related function type
    
    The new btrfs_workqueue still use open-coded function defition,
    this patch will change them into btrfs_func_t type which is much the
    same as kernel workqueue.
    
    Signed-off-by: Qu Wenruo <quwenruo@cn.fujitsu.com>
    Signed-off-by: Josef Bacik <jbacik@fb.com>

diff --git a/fs/btrfs/async-thread.h b/fs/btrfs/async-thread.h
index 08d717476227..0a891cdc4c28 100644
--- a/fs/btrfs/async-thread.h
+++ b/fs/btrfs/async-thread.h
@@ -23,11 +23,13 @@
 struct btrfs_workqueue;
 /* Internal use only */
 struct __btrfs_workqueue;
+struct btrfs_work;
+typedef void (*btrfs_func_t)(struct btrfs_work *arg);
 
 struct btrfs_work {
-	void (*func)(struct btrfs_work *arg);
-	void (*ordered_func)(struct btrfs_work *arg);
-	void (*ordered_free)(struct btrfs_work *arg);
+	btrfs_func_t func;
+	btrfs_func_t ordered_func;
+	btrfs_func_t ordered_free;
 
 	/* Don't touch things below */
 	struct work_struct normal_work;
@@ -37,13 +39,13 @@ struct btrfs_work {
 };
 
 struct btrfs_workqueue *btrfs_alloc_workqueue(char *name,
-						     int flags,
-						     int max_active,
-						     int thresh);
+					      int flags,
+					      int max_active,
+					      int thresh);
 void btrfs_init_work(struct btrfs_work *work,
-		     void (*func)(struct btrfs_work *),
-		     void (*ordered_func)(struct btrfs_work *),
-		     void (*ordered_free)(struct btrfs_work *));
+		     btrfs_func_t func,
+		     btrfs_func_t ordered_func,
+		     btrfs_func_t ordered_free);
 void btrfs_queue_work(struct btrfs_workqueue *wq,
 		      struct btrfs_work *work);
 void btrfs_destroy_workqueue(struct btrfs_workqueue *wq);

commit d458b0540ebd728b4d6ef47cc5ef0dbfd4dd361a
Author: Qu Wenruo <quwenruo@cn.fujitsu.com>
Date:   Fri Feb 28 10:46:19 2014 +0800

    btrfs: Cleanup the "_struct" suffix in btrfs_workequeue
    
    Since the "_struct" suffix is mainly used for distinguish the differnt
    btrfs_work between the original and the newly created one,
    there is no need using the suffix since all btrfs_workers are changed
    into btrfs_workqueue.
    
    Also this patch fixed some codes whose code style is changed due to the
    too long "_struct" suffix.
    
    Signed-off-by: Qu Wenruo <quwenruo@cn.fujitsu.com>
    Tested-by: David Sterba <dsterba@suse.cz>
    Signed-off-by: Josef Bacik <jbacik@fb.com>

diff --git a/fs/btrfs/async-thread.h b/fs/btrfs/async-thread.h
index ab05904f791c..08d717476227 100644
--- a/fs/btrfs/async-thread.h
+++ b/fs/btrfs/async-thread.h
@@ -20,33 +20,33 @@
 #ifndef __BTRFS_ASYNC_THREAD_
 #define __BTRFS_ASYNC_THREAD_
 
-struct btrfs_workqueue_struct;
+struct btrfs_workqueue;
 /* Internal use only */
-struct __btrfs_workqueue_struct;
+struct __btrfs_workqueue;
 
-struct btrfs_work_struct {
-	void (*func)(struct btrfs_work_struct *arg);
-	void (*ordered_func)(struct btrfs_work_struct *arg);
-	void (*ordered_free)(struct btrfs_work_struct *arg);
+struct btrfs_work {
+	void (*func)(struct btrfs_work *arg);
+	void (*ordered_func)(struct btrfs_work *arg);
+	void (*ordered_free)(struct btrfs_work *arg);
 
 	/* Don't touch things below */
 	struct work_struct normal_work;
 	struct list_head ordered_list;
-	struct __btrfs_workqueue_struct *wq;
+	struct __btrfs_workqueue *wq;
 	unsigned long flags;
 };
 
-struct btrfs_workqueue_struct *btrfs_alloc_workqueue(char *name,
+struct btrfs_workqueue *btrfs_alloc_workqueue(char *name,
 						     int flags,
 						     int max_active,
 						     int thresh);
-void btrfs_init_work(struct btrfs_work_struct *work,
-		     void (*func)(struct btrfs_work_struct *),
-		     void (*ordered_func)(struct btrfs_work_struct *),
-		     void (*ordered_free)(struct btrfs_work_struct *));
-void btrfs_queue_work(struct btrfs_workqueue_struct *wq,
-		      struct btrfs_work_struct *work);
-void btrfs_destroy_workqueue(struct btrfs_workqueue_struct *wq);
-void btrfs_workqueue_set_max(struct btrfs_workqueue_struct *wq, int max);
-void btrfs_set_work_high_priority(struct btrfs_work_struct *work);
+void btrfs_init_work(struct btrfs_work *work,
+		     void (*func)(struct btrfs_work *),
+		     void (*ordered_func)(struct btrfs_work *),
+		     void (*ordered_free)(struct btrfs_work *));
+void btrfs_queue_work(struct btrfs_workqueue *wq,
+		      struct btrfs_work *work);
+void btrfs_destroy_workqueue(struct btrfs_workqueue *wq);
+void btrfs_workqueue_set_max(struct btrfs_workqueue *wq, int max);
+void btrfs_set_work_high_priority(struct btrfs_work *work);
 #endif

commit a046e9c88b0f46677923864295eac7c92cd962cb
Author: Qu Wenruo <quwenruo@cn.fujitsu.com>
Date:   Fri Feb 28 10:46:18 2014 +0800

    btrfs: Cleanup the old btrfs_worker.
    
    Since all the btrfs_worker is replaced with the newly created
    btrfs_workqueue, the old codes can be easily remove.
    
    Signed-off-by: Quwenruo <quwenruo@cn.fujitsu.com>
    Tested-by: David Sterba <dsterba@suse.cz>
    Signed-off-by: Josef Bacik <jbacik@fb.com>

diff --git a/fs/btrfs/async-thread.h b/fs/btrfs/async-thread.h
index 3129d8a6128b..ab05904f791c 100644
--- a/fs/btrfs/async-thread.h
+++ b/fs/btrfs/async-thread.h
@@ -20,106 +20,6 @@
 #ifndef __BTRFS_ASYNC_THREAD_
 #define __BTRFS_ASYNC_THREAD_
 
-struct btrfs_worker_thread;
-
-/*
- * This is similar to a workqueue, but it is meant to spread the operations
- * across all available cpus instead of just the CPU that was used to
- * queue the work.  There is also some batching introduced to try and
- * cut down on context switches.
- *
- * By default threads are added on demand up to 2 * the number of cpus.
- * Changing struct btrfs_workers->max_workers is one way to prevent
- * demand creation of kthreads.
- *
- * the basic model of these worker threads is to embed a btrfs_work
- * structure in your own data struct, and use container_of in a
- * work function to get back to your data struct.
- */
-struct btrfs_work {
-	/*
-	 * func should be set to the function you want called
-	 * your work struct is passed as the only arg
-	 *
-	 * ordered_func must be set for work sent to an ordered work queue,
-	 * and it is called to complete a given work item in the same
-	 * order they were sent to the queue.
-	 */
-	void (*func)(struct btrfs_work *work);
-	void (*ordered_func)(struct btrfs_work *work);
-	void (*ordered_free)(struct btrfs_work *work);
-
-	/*
-	 * flags should be set to zero.  It is used to make sure the
-	 * struct is only inserted once into the list.
-	 */
-	unsigned long flags;
-
-	/* don't touch these */
-	struct btrfs_worker_thread *worker;
-	struct list_head list;
-	struct list_head order_list;
-};
-
-struct btrfs_workers {
-	/* current number of running workers */
-	int num_workers;
-
-	int num_workers_starting;
-
-	/* max number of workers allowed.  changed by btrfs_start_workers */
-	int max_workers;
-
-	/* once a worker has this many requests or fewer, it is idle */
-	int idle_thresh;
-
-	/* force completions in the order they were queued */
-	int ordered;
-
-	/* more workers required, but in an interrupt handler */
-	int atomic_start_pending;
-
-	/*
-	 * are we allowed to sleep while starting workers or are we required
-	 * to start them at a later time?  If we can't sleep, this indicates
-	 * which queue we need to use to schedule thread creation.
-	 */
-	struct btrfs_workers *atomic_worker_start;
-
-	/* list with all the work threads.  The workers on the idle thread
-	 * may be actively servicing jobs, but they haven't yet hit the
-	 * idle thresh limit above.
-	 */
-	struct list_head worker_list;
-	struct list_head idle_list;
-
-	/*
-	 * when operating in ordered mode, this maintains the list
-	 * of work items waiting for completion
-	 */
-	struct list_head order_list;
-	struct list_head prio_order_list;
-
-	/* lock for finding the next worker thread to queue on */
-	spinlock_t lock;
-
-	/* lock for the ordered lists */
-	spinlock_t order_lock;
-
-	/* extra name for this worker, used for current->name */
-	char *name;
-
-	int stopping;
-};
-
-void btrfs_queue_worker(struct btrfs_workers *workers, struct btrfs_work *work);
-int btrfs_start_workers(struct btrfs_workers *workers);
-void btrfs_stop_workers(struct btrfs_workers *workers);
-void btrfs_init_workers(struct btrfs_workers *workers, char *name, int max,
-			struct btrfs_workers *async_starter);
-void btrfs_requeue_work(struct btrfs_work *work);
-void btrfs_set_work_high_prio(struct btrfs_work *work);
-
 struct btrfs_workqueue_struct;
 /* Internal use only */
 struct __btrfs_workqueue_struct;

commit 0bd9289c28c3b6a38f5a05a812afae0274674fa2
Author: Qu Wenruo <quwenruo@cn.fujitsu.com>
Date:   Fri Feb 28 10:46:05 2014 +0800

    btrfs: Add threshold workqueue based on kernel workqueue
    
    The original btrfs_workers has thresholding functions to dynamically
    create or destroy kthreads.
    
    Though there is no such function in kernel workqueue because the worker
    is not created manually, we can still use the workqueue_set_max_active
    to simulated the behavior, mainly to achieve a better HDD performance by
    setting a high threshold on submit_workers.
    (Sadly, no resource can be saved)
    
    So in this patch, extra workqueue pending counters are introduced to
    dynamically change the max active of each btrfs_workqueue_struct, hoping
    to restore the behavior of the original thresholding function.
    
    Also, workqueue_set_max_active use a mutex to protect workqueue_struct,
    which is not meant to be called too frequently, so a new interval
    mechanism is applied, that will only call workqueue_set_max_active after
    a count of work is queued. Hoping to balance both the random and
    sequence performance on HDD.
    
    Signed-off-by: Qu Wenruo <quwenruo@cn.fujitsu.com>
    Tested-by: David Sterba <dsterba@suse.cz>
    Signed-off-by: Josef Bacik <jbacik@fb.com>

diff --git a/fs/btrfs/async-thread.h b/fs/btrfs/async-thread.h
index fce623cfe3da..3129d8a6128b 100644
--- a/fs/btrfs/async-thread.h
+++ b/fs/btrfs/async-thread.h
@@ -138,7 +138,8 @@ struct btrfs_work_struct {
 
 struct btrfs_workqueue_struct *btrfs_alloc_workqueue(char *name,
 						     int flags,
-						     int max_active);
+						     int max_active,
+						     int thresh);
 void btrfs_init_work(struct btrfs_work_struct *work,
 		     void (*func)(struct btrfs_work_struct *),
 		     void (*ordered_func)(struct btrfs_work_struct *),

commit 1ca08976ae94f3594dd7303584581cf8099ce47e
Author: Qu Wenruo <quwenruo@cn.fujitsu.com>
Date:   Fri Feb 28 10:46:04 2014 +0800

    btrfs: Add high priority workqueue support for btrfs_workqueue_struct
    
    Add high priority function to btrfs_workqueue.
    
    This is implemented by embedding a btrfs_workqueue into a
    btrfs_workqueue and use some helper functions to differ the normal
    priority wq and high priority wq.
    So the high priority wq is completely independent from the normal
    workqueue.
    
    Signed-off-by: Qu Wenruo <quwenruo@cn.fujitsu.com>
    Tested-by: David Sterba <dsterba@suse.cz>
    Signed-off-by: Josef Bacik <jbacik@fb.com>

diff --git a/fs/btrfs/async-thread.h b/fs/btrfs/async-thread.h
index 9d8da53f6dd9..fce623cfe3da 100644
--- a/fs/btrfs/async-thread.h
+++ b/fs/btrfs/async-thread.h
@@ -121,6 +121,8 @@ void btrfs_requeue_work(struct btrfs_work *work);
 void btrfs_set_work_high_prio(struct btrfs_work *work);
 
 struct btrfs_workqueue_struct;
+/* Internal use only */
+struct __btrfs_workqueue_struct;
 
 struct btrfs_work_struct {
 	void (*func)(struct btrfs_work_struct *arg);
@@ -130,7 +132,7 @@ struct btrfs_work_struct {
 	/* Don't touch things below */
 	struct work_struct normal_work;
 	struct list_head ordered_list;
-	struct btrfs_workqueue_struct *wq;
+	struct __btrfs_workqueue_struct *wq;
 	unsigned long flags;
 };
 
@@ -145,4 +147,5 @@ void btrfs_queue_work(struct btrfs_workqueue_struct *wq,
 		      struct btrfs_work_struct *work);
 void btrfs_destroy_workqueue(struct btrfs_workqueue_struct *wq);
 void btrfs_workqueue_set_max(struct btrfs_workqueue_struct *wq, int max);
+void btrfs_set_work_high_priority(struct btrfs_work_struct *work);
 #endif

commit 08a9ff3264181986d1d692a4e6fce3669700c9f8
Author: Qu Wenruo <quwenruo@cn.fujitsu.com>
Date:   Fri Feb 28 10:46:03 2014 +0800

    btrfs: Added btrfs_workqueue_struct implemented ordered execution based on kernel workqueue
    
    Use kernel workqueue to implement a new btrfs_workqueue_struct, which
    has the ordering execution feature like the btrfs_worker.
    
    The func is executed in a concurrency way, and the
    ordred_func/ordered_free is executed in the sequence them are queued
    after the corresponding func is done.
    
    The new btrfs_workqueue works much like the original one, one workqueue
    for normal work and a list for ordered work.
    When a work is queued, ordered work will be added to the list and helper
    function will be queued into the workqueue.
    The helper function will execute a normal work and then check and execute as many
    ordered work as possible in the sequence they were queued.
    
    At this patch, high priority work queue or thresholding is not added yet.
    The high priority feature and thresholding will be added in the following patches.
    
    Signed-off-by: Qu Wenruo <quwenruo@cn.fujitsu.com>
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Tested-by: David Sterba <dsterba@suse.cz>
    Signed-off-by: Josef Bacik <jbacik@fb.com>

diff --git a/fs/btrfs/async-thread.h b/fs/btrfs/async-thread.h
index 1f26792683ed..9d8da53f6dd9 100644
--- a/fs/btrfs/async-thread.h
+++ b/fs/btrfs/async-thread.h
@@ -1,5 +1,6 @@
 /*
  * Copyright (C) 2007 Oracle.  All rights reserved.
+ * Copyright (C) 2014 Fujitsu.  All rights reserved.
  *
  * This program is free software; you can redistribute it and/or
  * modify it under the terms of the GNU General Public
@@ -118,4 +119,30 @@ void btrfs_init_workers(struct btrfs_workers *workers, char *name, int max,
 			struct btrfs_workers *async_starter);
 void btrfs_requeue_work(struct btrfs_work *work);
 void btrfs_set_work_high_prio(struct btrfs_work *work);
+
+struct btrfs_workqueue_struct;
+
+struct btrfs_work_struct {
+	void (*func)(struct btrfs_work_struct *arg);
+	void (*ordered_func)(struct btrfs_work_struct *arg);
+	void (*ordered_free)(struct btrfs_work_struct *arg);
+
+	/* Don't touch things below */
+	struct work_struct normal_work;
+	struct list_head ordered_list;
+	struct btrfs_workqueue_struct *wq;
+	unsigned long flags;
+};
+
+struct btrfs_workqueue_struct *btrfs_alloc_workqueue(char *name,
+						     int flags,
+						     int max_active);
+void btrfs_init_work(struct btrfs_work_struct *work,
+		     void (*func)(struct btrfs_work_struct *),
+		     void (*ordered_func)(struct btrfs_work_struct *),
+		     void (*ordered_free)(struct btrfs_work_struct *));
+void btrfs_queue_work(struct btrfs_workqueue_struct *wq,
+		      struct btrfs_work_struct *work);
+void btrfs_destroy_workqueue(struct btrfs_workqueue_struct *wq);
+void btrfs_workqueue_set_max(struct btrfs_workqueue_struct *wq, int max);
 #endif

commit 964fb15acfcd672ac691f04879b71f07ccc21e0c
Author: Ilya Dryomov <idryomov@gmail.com>
Date:   Wed Oct 2 19:39:50 2013 +0300

    Btrfs: eliminate races in worker stopping code
    
    The current implementation of worker threads in Btrfs has races in
    worker stopping code, which cause all kinds of panics and lockups when
    running btrfs/011 xfstest in a loop.  The problem is that
    btrfs_stop_workers is unsynchronized with respect to check_idle_worker,
    check_busy_worker and __btrfs_start_workers.
    
    E.g., check_idle_worker race flow:
    
           btrfs_stop_workers():            check_idle_worker(aworker):
    - grabs the lock
    - splices the idle list into the
      working list
    - removes the first worker from the
      working list
    - releases the lock to wait for
      its kthread's completion
                                      - grabs the lock
                                      - if aworker is on the working list,
                                        moves aworker from the working list
                                        to the idle list
                                      - releases the lock
    - grabs the lock
    - puts the worker
    - removes the second worker from the
      working list
                                  ......
            btrfs_stop_workers returns, aworker is on the idle list
                     FS is umounted, memory is freed
                                  ......
                  aworker is waken up, fireworks ensue
    
    With this applied, I wasn't able to trigger the problem in 48 hours,
    whereas previously I could reliably reproduce at least one of these
    races within an hour.
    
    Reported-by: David Sterba <dsterba@suse.cz>
    Signed-off-by: Ilya Dryomov <idryomov@gmail.com>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

diff --git a/fs/btrfs/async-thread.h b/fs/btrfs/async-thread.h
index 063698b90ce2..1f26792683ed 100644
--- a/fs/btrfs/async-thread.h
+++ b/fs/btrfs/async-thread.h
@@ -107,6 +107,8 @@ struct btrfs_workers {
 
 	/* extra name for this worker, used for current->name */
 	char *name;
+
+	int stopping;
 };
 
 void btrfs_queue_worker(struct btrfs_workers *workers, struct btrfs_work *work);

commit 143bede527b054a271053f41bfaca2b57baa9408
Author: Jeff Mahoney <jeffm@suse.com>
Date:   Thu Mar 1 14:56:26 2012 +0100

    btrfs: return void in functions without error conditions
    
    Signed-off-by: Jeff Mahoney <jeffm@suse.com>

diff --git a/fs/btrfs/async-thread.h b/fs/btrfs/async-thread.h
index f34cc31fa3c9..063698b90ce2 100644
--- a/fs/btrfs/async-thread.h
+++ b/fs/btrfs/async-thread.h
@@ -111,9 +111,9 @@ struct btrfs_workers {
 
 void btrfs_queue_worker(struct btrfs_workers *workers, struct btrfs_work *work);
 int btrfs_start_workers(struct btrfs_workers *workers);
-int btrfs_stop_workers(struct btrfs_workers *workers);
+void btrfs_stop_workers(struct btrfs_workers *workers);
 void btrfs_init_workers(struct btrfs_workers *workers, char *name, int max,
 			struct btrfs_workers *async_starter);
-int btrfs_requeue_work(struct btrfs_work *work);
+void btrfs_requeue_work(struct btrfs_work *work);
 void btrfs_set_work_high_prio(struct btrfs_work *work);
 #endif

commit 0dc3b84a73267f47a75468f924f5d58a840e3152
Author: Josef Bacik <josef@redhat.com>
Date:   Fri Nov 18 14:37:27 2011 -0500

    Btrfs: fix num_workers_starting bug and other bugs in async thread
    
    Al pointed out we have some random problems with the way we account for
    num_workers_starting in the async thread stuff.  First of all we need to make
    sure to decrement num_workers_starting if we fail to start the worker, so make
    __btrfs_start_workers do this.  Also fix __btrfs_start_workers so that it
    doesn't call btrfs_stop_workers(), there is no point in stopping everybody if we
    failed to create a worker.  Also check_pending_worker_creates needs to call
    __btrfs_start_work in it's work function since it already increments
    num_workers_starting.
    
    People only start one worker at a time, so get rid of the num_workers argument
    everywhere, and make btrfs_queue_worker a void since it will always succeed.
    Thanks,
    
    Signed-off-by: Josef Bacik <josef@redhat.com>

diff --git a/fs/btrfs/async-thread.h b/fs/btrfs/async-thread.h
index 5077746cf85e..f34cc31fa3c9 100644
--- a/fs/btrfs/async-thread.h
+++ b/fs/btrfs/async-thread.h
@@ -109,8 +109,8 @@ struct btrfs_workers {
 	char *name;
 };
 
-int btrfs_queue_worker(struct btrfs_workers *workers, struct btrfs_work *work);
-int btrfs_start_workers(struct btrfs_workers *workers, int num_workers);
+void btrfs_queue_worker(struct btrfs_workers *workers, struct btrfs_work *work);
+int btrfs_start_workers(struct btrfs_workers *workers);
 int btrfs_stop_workers(struct btrfs_workers *workers);
 void btrfs_init_workers(struct btrfs_workers *workers, char *name, int max,
 			struct btrfs_workers *async_starter);

commit 61d92c328c16419fc96dc50dd16f8b8c695409ec
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Oct 2 19:11:56 2009 -0400

    Btrfs: fix deadlock on async thread startup
    
    The btrfs async worker threads are used for a wide variety of things,
    including processing bio end_io functions.  This means that when
    the endio threads aren't running, the rest of the FS isn't
    able to do the final processing required to clear PageWriteback.
    
    The endio threads also try to exit as they become idle and
    start more as the work piles up.  The problem is that starting more
    threads means kthreadd may need to allocate ram, and that allocation
    may wait until the global number of writeback pages on the system is
    below a certain limit.
    
    The result of that throttling is that end IO threads wait on
    kthreadd, who is waiting on IO to end, which will never happen.
    
    This commit fixes the deadlock by handing off thread startup to a
    dedicated thread.  It also fixes a bug where the on-demand thread
    creation was creating far too many threads because it didn't take into
    account threads being started by other procs.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/async-thread.h b/fs/btrfs/async-thread.h
index fc089b95ec14..5077746cf85e 100644
--- a/fs/btrfs/async-thread.h
+++ b/fs/btrfs/async-thread.h
@@ -64,6 +64,8 @@ struct btrfs_workers {
 	/* current number of running workers */
 	int num_workers;
 
+	int num_workers_starting;
+
 	/* max number of workers allowed.  changed by btrfs_start_workers */
 	int max_workers;
 
@@ -78,9 +80,10 @@ struct btrfs_workers {
 
 	/*
 	 * are we allowed to sleep while starting workers or are we required
-	 * to start them at a later time?
+	 * to start them at a later time?  If we can't sleep, this indicates
+	 * which queue we need to use to schedule thread creation.
 	 */
-	int atomic_worker_start;
+	struct btrfs_workers *atomic_worker_start;
 
 	/* list with all the work threads.  The workers on the idle thread
 	 * may be actively servicing jobs, but they haven't yet hit the
@@ -109,7 +112,8 @@ struct btrfs_workers {
 int btrfs_queue_worker(struct btrfs_workers *workers, struct btrfs_work *work);
 int btrfs_start_workers(struct btrfs_workers *workers, int num_workers);
 int btrfs_stop_workers(struct btrfs_workers *workers);
-void btrfs_init_workers(struct btrfs_workers *workers, char *name, int max);
+void btrfs_init_workers(struct btrfs_workers *workers, char *name, int max,
+			struct btrfs_workers *async_starter);
 int btrfs_requeue_work(struct btrfs_work *work);
 void btrfs_set_work_high_prio(struct btrfs_work *work);
 #endif

commit 4e3f9c5042b43301d70781aee4a164a20878066b
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Aug 5 16:36:45 2009 -0400

    Btrfs: keep irqs on more often in the worker threads
    
    The btrfs worker thread spinlock was being used both for the
    queueing of IO and for the processing of ordered events.
    
    The ordered events never happen from end_io handlers, and so they
    don't need to use the _irq version of spinlocks.  This adds a
    dedicated lock to the ordered lists so they don't have to run
    with irqs off.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/async-thread.h b/fs/btrfs/async-thread.h
index a562ad8d83aa..fc089b95ec14 100644
--- a/fs/btrfs/async-thread.h
+++ b/fs/btrfs/async-thread.h
@@ -99,6 +99,9 @@ struct btrfs_workers {
 	/* lock for finding the next worker thread to queue on */
 	spinlock_t lock;
 
+	/* lock for the ordered lists */
+	spinlock_t order_lock;
+
 	/* extra name for this worker, used for current->name */
 	char *name;
 };

commit 9042846bc7ae69cc3288d85af6bad16208d93a95
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Aug 4 16:56:34 2009 -0400

    Btrfs: Allow worker threads to exit when idle
    
    The Btrfs worker threads don't currently die off after they have
    been idle for a while, leading to a lot of threads sitting around
    doing nothing for each mount.
    
    Also, they are unable to start atomically (from end_io hanlders).
    
    This commit reworks the worker threads so they can be started
    from end_io handlers (just setting a flag that asks for a thread
    to be added at a later date) and so they can exit if they
    have been idle for a long time.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/async-thread.h b/fs/btrfs/async-thread.h
index 1b511c109db6..a562ad8d83aa 100644
--- a/fs/btrfs/async-thread.h
+++ b/fs/btrfs/async-thread.h
@@ -73,6 +73,15 @@ struct btrfs_workers {
 	/* force completions in the order they were queued */
 	int ordered;
 
+	/* more workers required, but in an interrupt handler */
+	int atomic_start_pending;
+
+	/*
+	 * are we allowed to sleep while starting workers or are we required
+	 * to start them at a later time?
+	 */
+	int atomic_worker_start;
+
 	/* list with all the work threads.  The workers on the idle thread
 	 * may be actively servicing jobs, but they haven't yet hit the
 	 * idle thresh limit above.

commit d313d7a31a752c88f7288692bd98e66d0789779b
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Apr 20 15:50:09 2009 -0400

    Btrfs: add a priority queue to the async thread helpers
    
    Btrfs is using WRITE_SYNC_PLUG to send down synchronous IOs with a
    higher priority.  But, the checksumming helper threads prevent it
    from being fully effective.
    
    There are two problems.  First, a big queue of pending checksumming
    will delay the synchronous IO behind other lower priority writes.  Second,
    the checksumming uses an ordered async work queue.  The ordering makes sure
    that IOs are sent to the block layer in the same order they are sent
    to the checksumming threads.  Usually this gives us less seeky IO.
    
    But, when we start mixing IO priorities, the lower priority IO can delay
    the higher priority IO.
    
    This patch solves both problems by adding a high priority list to the async
    helper threads, and a new btrfs_set_work_high_prio(), which is used
    to make put a new async work item onto the higher priority list.
    
    The ordering is still done on high priority IO, but all of the high
    priority bios are ordered separately from the low priority bios.  This
    ordering is purely an IO optimization, it is not involved in data
    or metadata integrity.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/async-thread.h b/fs/btrfs/async-thread.h
index 31be4ed8b63e..1b511c109db6 100644
--- a/fs/btrfs/async-thread.h
+++ b/fs/btrfs/async-thread.h
@@ -85,6 +85,7 @@ struct btrfs_workers {
 	 * of work items waiting for completion
 	 */
 	struct list_head order_list;
+	struct list_head prio_order_list;
 
 	/* lock for finding the next worker thread to queue on */
 	spinlock_t lock;
@@ -98,4 +99,5 @@ int btrfs_start_workers(struct btrfs_workers *workers, int num_workers);
 int btrfs_stop_workers(struct btrfs_workers *workers);
 void btrfs_init_workers(struct btrfs_workers *workers, char *name, int max);
 int btrfs_requeue_work(struct btrfs_work *work);
+void btrfs_set_work_high_prio(struct btrfs_work *work);
 #endif

commit 4a69a41009c4ac691f7d9c289f5f37fabeddce46
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Nov 6 22:03:00 2008 -0500

    Btrfs: Add ordered async work queues
    
    Btrfs uses kernel threads to create async work queues for cpu intensive
    operations such as checksumming and decompression.  These work well,
    but they make it difficult to keep IO order intact.
    
    A single writepages call from pdflush or fsync will turn into a number
    of bios, and each bio is checksummed in parallel.  Once the checksum is
    computed, the bio is sent down to the disk, and since we don't control
    the order in which the parallel operations happen, they might go down to
    the disk in almost any order.
    
    The code deals with this somewhat by having deep work queues for a single
    kernel thread, making it very likely that a single thread will process all
    the bios for a single inode.
    
    This patch introduces an explicitly ordered work queue.  As work structs
    are placed into the queue they are put onto the tail of a list.  They have
    three callbacks:
    
    ->func (cpu intensive processing here)
    ->ordered_func (order sensitive processing here)
    ->ordered_free (free the work struct, all processing is done)
    
    The work struct has three callbacks.  The func callback does the cpu intensive
    work, and when it completes the work struct is marked as done.
    
    Every time a work struct completes, the list is checked to see if the head
    is marked as done.  If so the ordered_func callback is used to do the
    order sensitive processing and the ordered_free callback is used to do
    any cleanup.  Then we loop back and check the head of the list again.
    
    This patch also changes the checksumming code to use the ordered workqueues.
    One a 4 drive array, it increases streaming writes from 280MB/s to 350MB/s.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/async-thread.h b/fs/btrfs/async-thread.h
index 4ec9a2ee0f9d..31be4ed8b63e 100644
--- a/fs/btrfs/async-thread.h
+++ b/fs/btrfs/async-thread.h
@@ -37,10 +37,16 @@ struct btrfs_worker_thread;
  */
 struct btrfs_work {
 	/*
-	 * only func should be set to the function you want called
+	 * func should be set to the function you want called
 	 * your work struct is passed as the only arg
+	 *
+	 * ordered_func must be set for work sent to an ordered work queue,
+	 * and it is called to complete a given work item in the same
+	 * order they were sent to the queue.
 	 */
 	void (*func)(struct btrfs_work *work);
+	void (*ordered_func)(struct btrfs_work *work);
+	void (*ordered_free)(struct btrfs_work *work);
 
 	/*
 	 * flags should be set to zero.  It is used to make sure the
@@ -51,6 +57,7 @@ struct btrfs_work {
 	/* don't touch these */
 	struct btrfs_worker_thread *worker;
 	struct list_head list;
+	struct list_head order_list;
 };
 
 struct btrfs_workers {
@@ -63,6 +70,9 @@ struct btrfs_workers {
 	/* once a worker has this many requests or fewer, it is idle */
 	int idle_thresh;
 
+	/* force completions in the order they were queued */
+	int ordered;
+
 	/* list with all the work threads.  The workers on the idle thread
 	 * may be actively servicing jobs, but they haven't yet hit the
 	 * idle thresh limit above.
@@ -70,6 +80,12 @@ struct btrfs_workers {
 	struct list_head worker_list;
 	struct list_head idle_list;
 
+	/*
+	 * when operating in ordered mode, this maintains the list
+	 * of work items waiting for completion
+	 */
+	struct list_head order_list;
+
 	/* lock for finding the next worker thread to queue on */
 	spinlock_t lock;
 

commit d352ac68148b69937d39ca5d48bcc4478e118dbf
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Sep 29 15:18:18 2008 -0400

    Btrfs: add and improve comments
    
    This improves the comments at the top of many functions.  It didn't
    dive into the guts of functions because I was trying to
    avoid merging problems with the new allocator and back reference work.
    
    extent-tree.c and volumes.c were both skipped, and there is definitely
    more work todo in cleaning and commenting the code.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/async-thread.h b/fs/btrfs/async-thread.h
index 43e44d115dd1..4ec9a2ee0f9d 100644
--- a/fs/btrfs/async-thread.h
+++ b/fs/btrfs/async-thread.h
@@ -63,14 +63,17 @@ struct btrfs_workers {
 	/* once a worker has this many requests or fewer, it is idle */
 	int idle_thresh;
 
-	/* list with all the work threads */
+	/* list with all the work threads.  The workers on the idle thread
+	 * may be actively servicing jobs, but they haven't yet hit the
+	 * idle thresh limit above.
+	 */
 	struct list_head worker_list;
 	struct list_head idle_list;
 
 	/* lock for finding the next worker thread to queue on */
 	spinlock_t lock;
 
-	/* extra name for this worker */
+	/* extra name for this worker, used for current->name */
 	char *name;
 };
 

commit 5443be45f5cb57d02fd895a0bcaf7e7d9890b1df
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Aug 15 15:34:16 2008 -0400

    Btrfs: Give all the worker threads descriptive names
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/async-thread.h b/fs/btrfs/async-thread.h
index 3436ff897597..43e44d115dd1 100644
--- a/fs/btrfs/async-thread.h
+++ b/fs/btrfs/async-thread.h
@@ -69,11 +69,14 @@ struct btrfs_workers {
 
 	/* lock for finding the next worker thread to queue on */
 	spinlock_t lock;
+
+	/* extra name for this worker */
+	char *name;
 };
 
 int btrfs_queue_worker(struct btrfs_workers *workers, struct btrfs_work *work);
 int btrfs_start_workers(struct btrfs_workers *workers, int num_workers);
 int btrfs_stop_workers(struct btrfs_workers *workers);
-void btrfs_init_workers(struct btrfs_workers *workers, int max);
+void btrfs_init_workers(struct btrfs_workers *workers, char *name, int max);
 int btrfs_requeue_work(struct btrfs_work *work);
 #endif

commit 35d8ba66294ff2a53c17337a1aa1ff6739492f41
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Jun 11 20:21:24 2008 -0400

    Btrfs: Worker thread optimizations
    
    This changes the worker thread pool to maintain a list of idle threads,
    avoiding a complex search for a good thread to wake up.
    
    Threads have two states:
    
    idle - we try to reuse the last thread used in hopes of improving the batching
    ratios
    
    busy - each time a new work item is added to a busy task, the task is
    rotated to the end of the line.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/async-thread.h b/fs/btrfs/async-thread.h
index 52fc9da0f9e7..3436ff897597 100644
--- a/fs/btrfs/async-thread.h
+++ b/fs/btrfs/async-thread.h
@@ -60,11 +60,12 @@ struct btrfs_workers {
 	/* max number of workers allowed.  changed by btrfs_start_workers */
 	int max_workers;
 
+	/* once a worker has this many requests or fewer, it is idle */
+	int idle_thresh;
+
 	/* list with all the work threads */
 	struct list_head worker_list;
-
-	/* the last worker thread to have something queued */
-	struct btrfs_worker_thread *last;
+	struct list_head idle_list;
 
 	/* lock for finding the next worker thread to queue on */
 	spinlock_t lock;

commit 8b7128429235d9bd72cfd5ed20c77c4f3118f744
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Jun 11 16:50:36 2008 -0400

    Btrfs: Add async worker threads for pre and post IO checksumming
    
    Btrfs has been using workqueues to spread the checksumming load across
    other CPUs in the system.  But, workqueues only schedule work on the
    same CPU that queued the work, giving them a limited benefit for systems with
    higher CPU counts.
    
    This code adds a generic facility to schedule work with pools of kthreads,
    and changes the bio submission code to queue bios up.  The queueing is
    important to make sure large numbers of procs on the system don't
    turn streaming workloads into random workloads by sending IO down
    concurrently.
    
    The end result of all of this is much higher performance (and CPU usage) when
    doing checksumming on large machines.  Two worker pools are created,
    one for writes and one for endio processing.  The two could deadlock if
    we tried to service both from a single pool.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/async-thread.h b/fs/btrfs/async-thread.h
new file mode 100644
index 000000000000..52fc9da0f9e7
--- /dev/null
+++ b/fs/btrfs/async-thread.h
@@ -0,0 +1,78 @@
+/*
+ * Copyright (C) 2007 Oracle.  All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public
+ * License v2 as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public
+ * License along with this program; if not, write to the
+ * Free Software Foundation, Inc., 59 Temple Place - Suite 330,
+ * Boston, MA 021110-1307, USA.
+ */
+
+#ifndef __BTRFS_ASYNC_THREAD_
+#define __BTRFS_ASYNC_THREAD_
+
+struct btrfs_worker_thread;
+
+/*
+ * This is similar to a workqueue, but it is meant to spread the operations
+ * across all available cpus instead of just the CPU that was used to
+ * queue the work.  There is also some batching introduced to try and
+ * cut down on context switches.
+ *
+ * By default threads are added on demand up to 2 * the number of cpus.
+ * Changing struct btrfs_workers->max_workers is one way to prevent
+ * demand creation of kthreads.
+ *
+ * the basic model of these worker threads is to embed a btrfs_work
+ * structure in your own data struct, and use container_of in a
+ * work function to get back to your data struct.
+ */
+struct btrfs_work {
+	/*
+	 * only func should be set to the function you want called
+	 * your work struct is passed as the only arg
+	 */
+	void (*func)(struct btrfs_work *work);
+
+	/*
+	 * flags should be set to zero.  It is used to make sure the
+	 * struct is only inserted once into the list.
+	 */
+	unsigned long flags;
+
+	/* don't touch these */
+	struct btrfs_worker_thread *worker;
+	struct list_head list;
+};
+
+struct btrfs_workers {
+	/* current number of running workers */
+	int num_workers;
+
+	/* max number of workers allowed.  changed by btrfs_start_workers */
+	int max_workers;
+
+	/* list with all the work threads */
+	struct list_head worker_list;
+
+	/* the last worker thread to have something queued */
+	struct btrfs_worker_thread *last;
+
+	/* lock for finding the next worker thread to queue on */
+	spinlock_t lock;
+};
+
+int btrfs_queue_worker(struct btrfs_workers *workers, struct btrfs_work *work);
+int btrfs_start_workers(struct btrfs_workers *workers, int num_workers);
+int btrfs_stop_workers(struct btrfs_workers *workers);
+void btrfs_init_workers(struct btrfs_workers *workers, int max);
+int btrfs_requeue_work(struct btrfs_work *work);
+#endif
