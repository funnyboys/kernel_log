commit 6ef03debdb3d82d7deec65f96e143b9adcfb2cd4
Author: Josef Bacik <josef@toxicpanda.com>
Date:   Wed Jun 19 15:11:58 2019 -0400

    btrfs: migrate the delayed refs rsv code
    
    These belong with the delayed refs related code, not in extent-tree.c.
    
    Signed-off-by: Josef Bacik <josef@toxicpanda.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index c18f93ea88ed..1c977e6d45dc 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -364,6 +364,16 @@ struct btrfs_delayed_ref_head *btrfs_select_ref_head(
 
 int btrfs_check_delayed_seq(struct btrfs_fs_info *fs_info, u64 seq);
 
+void btrfs_delayed_refs_rsv_release(struct btrfs_fs_info *fs_info, int nr);
+void btrfs_update_delayed_refs_rsv(struct btrfs_trans_handle *trans);
+int btrfs_delayed_refs_rsv_refill(struct btrfs_fs_info *fs_info,
+				  enum btrfs_reserve_flush_enum flush);
+void btrfs_migrate_to_delayed_refs_rsv(struct btrfs_fs_info *fs_info,
+				       struct btrfs_block_rsv *src,
+				       u64 num_bytes);
+int btrfs_should_throttle_delayed_refs(struct btrfs_trans_handle *trans);
+bool btrfs_check_space_for_delayed_refs(struct btrfs_fs_info *fs_info);
+
 /*
  * helper functions to cast a node into its container
  */

commit c6e340bc1c9e3411c40aafca4c69b989530c9347
Author: David Sterba <dsterba@suse.com>
Date:   Wed Mar 20 11:42:34 2019 +0100

    btrfs: remove unused parameter fs_info from btrfs_add_delayed_extent_op
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index 7cbb52adc553..c18f93ea88ed 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -340,8 +340,7 @@ int btrfs_add_delayed_data_ref(struct btrfs_trans_handle *trans,
 			       struct btrfs_ref *generic_ref,
 			       u64 reserved, int *old_ref_mod,
 			       int *new_ref_mod);
-int btrfs_add_delayed_extent_op(struct btrfs_fs_info *fs_info,
-				struct btrfs_trans_handle *trans,
+int btrfs_add_delayed_extent_op(struct btrfs_trans_handle *trans,
 				u64 bytenr, u64 num_bytes,
 				struct btrfs_delayed_extent_op *extent_op);
 void btrfs_merge_delayed_refs(struct btrfs_trans_handle *trans,

commit 76675593b69f2fcd57e24d9dd2a9b278f0130d0b
Author: Qu Wenruo <wqu@suse.com>
Date:   Thu Apr 4 14:45:32 2019 +0800

    btrfs: delayed-ref: Use btrfs_ref to refactor btrfs_add_delayed_data_ref()
    
    Just like btrfs_add_delayed_tree_ref(), use btrfs_ref to refactor
    btrfs_add_delayed_data_ref().
    
    Signed-off-by: Qu Wenruo <wqu@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index 515d1a3d8080..7cbb52adc553 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -337,10 +337,9 @@ int btrfs_add_delayed_tree_ref(struct btrfs_trans_handle *trans,
 			       struct btrfs_delayed_extent_op *extent_op,
 			       int *old_ref_mod, int *new_ref_mod);
 int btrfs_add_delayed_data_ref(struct btrfs_trans_handle *trans,
-			       u64 bytenr, u64 num_bytes,
-			       u64 parent, u64 ref_root,
-			       u64 owner, u64 offset, u64 reserved, int action,
-			       int *old_ref_mod, int *new_ref_mod);
+			       struct btrfs_ref *generic_ref,
+			       u64 reserved, int *old_ref_mod,
+			       int *new_ref_mod);
 int btrfs_add_delayed_extent_op(struct btrfs_fs_info *fs_info,
 				struct btrfs_trans_handle *trans,
 				u64 bytenr, u64 num_bytes,

commit ed4f255b9bacb774c99ded17647f138c3f61546d
Author: Qu Wenruo <wqu@suse.com>
Date:   Thu Apr 4 14:45:31 2019 +0800

    btrfs: delayed-ref: Use btrfs_ref to refactor btrfs_add_delayed_tree_ref()
    
    btrfs_add_delayed_tree_ref() has a longer and longer parameter list, and
    some callers like btrfs_inc_extent_ref() are using @owner as level for
    delayed tree ref.
    
    Instead of making the parameter list longer, use btrfs_ref to refactor
    it, so each parameter assignment should be self-explaining without dirty
    level/owner trick, and provides the basis for later refactoring.
    
    Signed-off-by: Qu Wenruo <wqu@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index d05bb6e2e1a6..515d1a3d8080 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -333,8 +333,7 @@ static inline void btrfs_put_delayed_ref_head(struct btrfs_delayed_ref_head *hea
 }
 
 int btrfs_add_delayed_tree_ref(struct btrfs_trans_handle *trans,
-			       u64 bytenr, u64 num_bytes, u64 parent,
-			       u64 ref_root, int level, int action,
+			       struct btrfs_ref *generic_ref,
 			       struct btrfs_delayed_extent_op *extent_op,
 			       int *old_ref_mod, int *new_ref_mod);
 int btrfs_add_delayed_data_ref(struct btrfs_trans_handle *trans,

commit b28b1f0ce44c1b9ebc1c43e3eba18c1f1f5d9cec
Author: Qu Wenruo <wqu@suse.com>
Date:   Thu Apr 4 14:45:29 2019 +0800

    btrfs: delayed-ref: Introduce better documented delayed ref structures
    
    Current delayed ref interface has several problems:
    
    - Longer and longer parameter lists
      bytenr
      num_bytes
      parent
      ---------- so far so good
      ref_root
      owner
      offset
      ---------- I don't feel good now
    
    - Different interpretation of the same parameter
    
      Above @owner for data ref is inode number (u64),
      while for tree ref, it's level (int).
    
      They are even in different size range.
      For level we only need 0 ~ 8, while for ino it's
      BTRFS_FIRST_FREE_OBJECTID ~ BTRFS_LAST_FREE_OBJECTID.
    
      And @offset doesn't even make sense for tree ref.
    
      Such parameter reuse may look clever as an hidden union, but it
      destroys code readability.
    
    To solve both problems, we introduce a new structure, btrfs_ref to solve
    them:
    
    - Structure instead of long parameter list
      This makes later expansion easier, and is better documented.
    
    - Use btrfs_ref::type to distinguish data and tree ref
    
    - Use proper union to store data/tree ref specific structures.
    
    - Use separate functions to fill data/tree ref data, with a common generic
      function to fill common bytenr/num_bytes members.
    
    All parameters will find its place in btrfs_ref, and an extra member,
    @real_root, inspired by ref-verify code, is newly introduced for later
    qgroup code, to record which tree is triggered by this extent modification.
    
    This patch doesn't touch any code, but provides the basis for further
    refactoring.
    
    Signed-off-by: Qu Wenruo <wqu@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index 70606da440aa..d05bb6e2e1a6 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -176,6 +176,83 @@ struct btrfs_delayed_ref_root {
 	u64 qgroup_to_skip;
 };
 
+enum btrfs_ref_type {
+	BTRFS_REF_NOT_SET,
+	BTRFS_REF_DATA,
+	BTRFS_REF_METADATA,
+	BTRFS_REF_LAST,
+};
+
+struct btrfs_data_ref {
+	/* For EXTENT_DATA_REF */
+
+	/* Root which refers to this data extent */
+	u64 ref_root;
+
+	/* Inode which refers to this data extent */
+	u64 ino;
+
+	/*
+	 * file_offset - extent_offset
+	 *
+	 * file_offset is the key.offset of the EXTENT_DATA key.
+	 * extent_offset is btrfs_file_extent_offset() of the EXTENT_DATA data.
+	 */
+	u64 offset;
+};
+
+struct btrfs_tree_ref {
+	/*
+	 * Level of this tree block
+	 *
+	 * Shared for skinny (TREE_BLOCK_REF) and normal tree ref.
+	 */
+	int level;
+
+	/*
+	 * Root which refers to this tree block.
+	 *
+	 * For TREE_BLOCK_REF (skinny metadata, either inline or keyed)
+	 */
+	u64 root;
+
+	/* For non-skinny metadata, no special member needed */
+};
+
+struct btrfs_ref {
+	enum btrfs_ref_type type;
+	int action;
+
+	/*
+	 * Whether this extent should go through qgroup record.
+	 *
+	 * Normally false, but for certain cases like delayed subtree scan,
+	 * setting this flag can hugely reduce qgroup overhead.
+	 */
+	bool skip_qgroup;
+
+	/*
+	 * Optional. For which root is this modification.
+	 * Mostly used for qgroup optimization.
+	 *
+	 * When unset, data/tree ref init code will populate it.
+	 * In certain cases, we're modifying reference for a different root.
+	 * E.g. COW fs tree blocks for balance.
+	 * In that case, tree_ref::root will be fs tree, but we're doing this
+	 * for reloc tree, then we should set @real_root to reloc tree.
+	 */
+	u64 real_root;
+	u64 bytenr;
+	u64 len;
+
+	/* Bytenr of the parent tree block */
+	u64 parent;
+	union {
+		struct btrfs_data_ref data_ref;
+		struct btrfs_tree_ref tree_ref;
+	};
+};
+
 extern struct kmem_cache *btrfs_delayed_ref_head_cachep;
 extern struct kmem_cache *btrfs_delayed_tree_ref_cachep;
 extern struct kmem_cache *btrfs_delayed_data_ref_cachep;
@@ -184,6 +261,38 @@ extern struct kmem_cache *btrfs_delayed_extent_op_cachep;
 int __init btrfs_delayed_ref_init(void);
 void __cold btrfs_delayed_ref_exit(void);
 
+static inline void btrfs_init_generic_ref(struct btrfs_ref *generic_ref,
+				int action, u64 bytenr, u64 len, u64 parent)
+{
+	generic_ref->action = action;
+	generic_ref->bytenr = bytenr;
+	generic_ref->len = len;
+	generic_ref->parent = parent;
+}
+
+static inline void btrfs_init_tree_ref(struct btrfs_ref *generic_ref,
+				int level, u64 root)
+{
+	/* If @real_root not set, use @root as fallback */
+	if (!generic_ref->real_root)
+		generic_ref->real_root = root;
+	generic_ref->tree_ref.level = level;
+	generic_ref->tree_ref.root = root;
+	generic_ref->type = BTRFS_REF_METADATA;
+}
+
+static inline void btrfs_init_data_ref(struct btrfs_ref *generic_ref,
+				u64 ref_root, u64 ino, u64 offset)
+{
+	/* If @real_root not set, use @root as fallback */
+	if (!generic_ref->real_root)
+		generic_ref->real_root = ref_root;
+	generic_ref->data_ref.ref_root = ref_root;
+	generic_ref->data_ref.ino = ino;
+	generic_ref->data_ref.offset = offset;
+	generic_ref->type = BTRFS_REF_DATA;
+}
+
 static inline struct btrfs_delayed_extent_op *
 btrfs_alloc_delayed_extent_op(void)
 {

commit 1418bae1c22951aad9883bc8f8f4dccb272cce1e
Author: Qu Wenruo <wqu@suse.com>
Date:   Wed Jan 23 15:15:12 2019 +0800

    btrfs: qgroup: Move reserved data accounting from btrfs_delayed_ref_head to btrfs_qgroup_extent_record
    
    [BUG]
    Btrfs/139 will fail with a high probability if the testing machine (VM)
    has only 2G RAM.
    
    Resulting the final write success while it should fail due to EDQUOT,
    and the fs will have quota exceeding the limit by 16K.
    
    The simplified reproducer will be: (needs a 2G ram VM)
    
      $ mkfs.btrfs -f $dev
      $ mount $dev $mnt
    
      $ btrfs subv create $mnt/subv
      $ btrfs quota enable $mnt
      $ btrfs quota rescan -w $mnt
      $ btrfs qgroup limit -e 1G $mnt/subv
    
      $ for i in $(seq -w  1 8); do
            xfs_io -f -c "pwrite 0 128M" $mnt/subv/file_$i > /dev/null
            echo "file $i written" > /dev/kmsg
        done
      $ sync
      $ btrfs qgroup show -pcre --raw $mnt
    
    The last pwrite will not trigger EDQUOT and final 'qgroup show' will
    show something like:
    
      qgroupid         rfer         excl     max_rfer     max_excl parent  child
      --------         ----         ----     --------     -------- ------  -----
      0/5             16384        16384         none         none ---     ---
      0/256      1073758208   1073758208         none   1073741824 ---     ---
    
    And 1073758208 is larger than
      > 1073741824.
    
    [CAUSE]
    It's a bug in btrfs qgroup data reserved space management.
    
    For quota limit, we must ensure that:
      reserved (data + metadata) + rfer/excl <= limit
    
    Since rfer/excl is only updated at transaction commmit time, reserved
    space needs to be taken special care.
    
    One important part of reserved space is data, and for a new data extent
    written to disk, we still need to take the reserved space until
    rfer/excl numbers get updated.
    
    Originally when an ordered extent finishes, we migrate the reserved
    qgroup data space from extent_io tree to delayed ref head of the data
    extent, expecting delayed ref will only be cleaned up at commit
    transaction time.
    
    However for small RAM machine, due to memory pressure dirty pages can be
    flushed back to disk without committing a transaction.
    
    The related events will be something like:
    
      file 1 written
      btrfs_finish_ordered_io: ino=258 ordered offset=0 len=54947840
      btrfs_finish_ordered_io: ino=258 ordered offset=54947840 len=5636096
      btrfs_finish_ordered_io: ino=258 ordered offset=61153280 len=57344
      btrfs_finish_ordered_io: ino=258 ordered offset=61210624 len=8192
      btrfs_finish_ordered_io: ino=258 ordered offset=60583936 len=569344
      cleanup_ref_head: num_bytes=54947840
      cleanup_ref_head: num_bytes=5636096
      cleanup_ref_head: num_bytes=569344
      cleanup_ref_head: num_bytes=57344
      cleanup_ref_head: num_bytes=8192
      ^^^^^^^^^^^^^^^^ This will free qgroup data reserved space
      file 2 written
      ...
      file 8 written
      cleanup_ref_head: num_bytes=8192
      ...
      btrfs_commit_transaction  <<< the only transaction committed during
                                    the test
    
    When file 2 is written, we have already freed 128M reserved qgroup data
    space for ino 258. Thus later write won't trigger EDQUOT.
    
    This allows us to write more data beyond qgroup limit.
    
    In my 2G ram VM, it could reach about 1.2G before hitting EDQUOT.
    
    [FIX]
    By moving reserved qgroup data space from btrfs_delayed_ref_head to
    btrfs_qgroup_extent_record, we can ensure that reserved qgroup data
    space won't be freed half way before commit transaction, thus fix the
    problem.
    
    Fixes: f64d5ca86821 ("btrfs: delayed_ref: Add new function to record reserved space into delayed ref")
    Signed-off-by: Qu Wenruo <wqu@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index d2af974f68a1..70606da440aa 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -102,17 +102,6 @@ struct btrfs_delayed_ref_head {
 	 */
 	int ref_mod;
 
-	/*
-	 * For qgroup reserved space freeing.
-	 *
-	 * ref_root and reserved will be recorded after
-	 * BTRFS_ADD_DELAYED_EXTENT is called.
-	 * And will be used to free reserved qgroup space at
-	 * run_delayed_refs() time.
-	 */
-	u64 qgroup_ref_root;
-	u64 qgroup_reserved;
-
 	/*
 	 * when a new extent is allocated, it is just reserved in memory
 	 * The actual extent isn't inserted into the extent allocation tree

commit d7baffdaf9f9df8c9715aa507e3be2f409347c74
Author: Josef Bacik <jbacik@fb.com>
Date:   Mon Dec 3 10:20:29 2018 -0500

    btrfs: add btrfs_delete_ref_head helper
    
    We do this dance in cleanup_ref_head and check_ref_cleanup, unify it
    into a helper and cleanup the calling functions.
    
    Reviewed-by: Omar Sandoval <osandov@fb.com>
    Reviewed-by: Nikolay Borisov <nborisov@suse.com>
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index 8e20c5cb5404..d2af974f68a1 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -261,7 +261,8 @@ static inline void btrfs_delayed_ref_unlock(struct btrfs_delayed_ref_head *head)
 {
 	mutex_unlock(&head->mutex);
 }
-
+void btrfs_delete_ref_head(struct btrfs_delayed_ref_root *delayed_refs,
+			   struct btrfs_delayed_ref_head *head);
 
 struct btrfs_delayed_ref_head *btrfs_select_ref_head(
 		struct btrfs_delayed_ref_root *delayed_refs);

commit 9e920a6f03e40b1eb712f38b29ad5880153754e2
Author: Lu Fengqi <lufq.fnst@cn.fujitsu.com>
Date:   Thu Oct 11 13:40:34 2018 +0800

    btrfs: delayed-ref: pass delayed_refs directly to btrfs_delayed_ref_lock
    
    Since trans is only used for referring to delayed_refs, there is no need
    to pass it instead of delayed_refs to btrfs_delayed_ref_lock().
    
    No functional change.
    
    Reviewed-by: Nikolay Borisov <nborisov@suse.com>
    Signed-off-by: Lu Fengqi <lufq.fnst@cn.fujitsu.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index d664c99ce716..8e20c5cb5404 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -255,7 +255,7 @@ void btrfs_merge_delayed_refs(struct btrfs_trans_handle *trans,
 struct btrfs_delayed_ref_head *
 btrfs_find_delayed_ref_head(struct btrfs_delayed_ref_root *delayed_refs,
 			    u64 bytenr);
-int btrfs_delayed_ref_lock(struct btrfs_trans_handle *trans,
+int btrfs_delayed_ref_lock(struct btrfs_delayed_ref_root *delayed_refs,
 			   struct btrfs_delayed_ref_head *head);
 static inline void btrfs_delayed_ref_unlock(struct btrfs_delayed_ref_head *head)
 {

commit 5637c74b01458d4bc392c2bb721bd102f316ad2d
Author: Lu Fengqi <lufq.fnst@cn.fujitsu.com>
Date:   Thu Oct 11 13:40:33 2018 +0800

    btrfs: delayed-ref: pass delayed_refs directly to btrfs_select_ref_head
    
    Since trans is only used for referring to delayed_refs, there is no need
    to pass it instead of delayed_refs to btrfs_select_ref_head().  No
    functional change.
    
    Signed-off-by: Lu Fengqi <lufq.fnst@cn.fujitsu.com>
    Reviewed-by: Nikolay Borisov <nborisov@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index c3e3486a126c..d664c99ce716 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -263,8 +263,8 @@ static inline void btrfs_delayed_ref_unlock(struct btrfs_delayed_ref_head *head)
 }
 
 
-struct btrfs_delayed_ref_head *
-btrfs_select_ref_head(struct btrfs_trans_handle *trans);
+struct btrfs_delayed_ref_head *btrfs_select_ref_head(
+		struct btrfs_delayed_ref_root *delayed_refs);
 
 int btrfs_check_delayed_seq(struct btrfs_fs_info *fs_info, u64 seq);
 

commit e3d039656384288bbe952413d8d404b3035fe7d7
Author: Liu Bo <bo.liu@linux.alibaba.com>
Date:   Thu Aug 23 03:51:50 2018 +0800

    Btrfs: delayed-refs: use rb_first_cached for ref_tree
    
    rb_first_cached() trades an extra pointer "leftmost" for doing the same
    job as rb_first() but in O(1).
    
    Functions manipulating href->ref_tree need to get the first entry, this
    converts href->ref_tree to use rb_first_cached().
    
    For more details about the optimization see patch "Btrfs: delayed-refs:
    use rb_first_cached for href_root".
    
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Liu Bo <bo.liu@linux.alibaba.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index 88438b6cee45..c3e3486a126c 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -79,7 +79,7 @@ struct btrfs_delayed_ref_head {
 	struct mutex mutex;
 
 	spinlock_t lock;
-	struct rb_root ref_tree;
+	struct rb_root_cached ref_tree;
 	/* accumulate add BTRFS_ADD_DELAYED_REF nodes to this ref_add_list. */
 	struct list_head ref_add_list;
 

commit 5c9d028b3b174e5cf3678a7b0c14e21e51665793
Author: Liu Bo <bo.liu@linux.alibaba.com>
Date:   Thu Aug 23 03:51:49 2018 +0800

    Btrfs: delayed-refs: use rb_first_cached for href_root
    
    rb_first_cached() trades an extra pointer "leftmost" for doing the same
    job as rb_first() but in O(1).
    
    Functions manipulating href_root need to get the first entry, this
    converts href_root to use rb_first_cached().
    
    This patch is first in the sequenct of similar updates to other rbtrees
    and this is analysis of the expected behaviour and improvements.
    
    There's a common pattern:
    
    while (node = rb_first) {
            entry = rb_entry(node)
            next = rb_next(node)
            rb_erase(node)
            cleanup(entry)
    }
    
    rb_first needs to traverse the tree up to logN depth, rb_erase can
    completely reshuffle the tree. With the caching we'll skip the traversal
    in rb_first.  That's a cached memory access vs looped pointer
    dereference trade-off that IMHO has a clear winner.
    
    Measurements show there's not much difference in a sample tree with
    10000 nodes: 4.5s / rb_first and 4.8s / rb_first_cached. Real effects of
    caching and pointer chasing are unpredictable though.
    
    Further optimzations can be done to avoid the expensive rb_erase step.
    In some cases it's ok to process the nodes in any order, so the tree can
    be traversed in post-order, not rebalancing the children nodes and just
    calling free. Care must be taken regarding the next node.
    
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Liu Bo <bo.liu@linux.alibaba.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    [ update changelog from mail discussions ]
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index d9f2a4ebd5db..88438b6cee45 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -148,7 +148,7 @@ struct btrfs_delayed_data_ref {
 
 struct btrfs_delayed_ref_root {
 	/* head ref rbtree */
-	struct rb_root href_root;
+	struct rb_root_cached href_root;
 
 	/* dirty extent records */
 	struct rb_root dirty_extent_root;

commit 88a979c615d0d9da19498b3b7692e725fb2f387e
Author: Nikolay Borisov <nborisov@suse.com>
Date:   Wed Jun 20 15:48:54 2018 +0300

    btrfs: Remove fs_info from btrfs_add_delayed_data_ref
    
    This function is always called with a valid transaction handle from
    where fs_info can be referenced. No functional changes.
    
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Reviewed-by: Qu Wenruo <wqu@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index 31729302c827..d9f2a4ebd5db 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -239,8 +239,7 @@ int btrfs_add_delayed_tree_ref(struct btrfs_trans_handle *trans,
 			       u64 ref_root, int level, int action,
 			       struct btrfs_delayed_extent_op *extent_op,
 			       int *old_ref_mod, int *new_ref_mod);
-int btrfs_add_delayed_data_ref(struct btrfs_fs_info *fs_info,
-			       struct btrfs_trans_handle *trans,
+int btrfs_add_delayed_data_ref(struct btrfs_trans_handle *trans,
 			       u64 bytenr, u64 num_bytes,
 			       u64 parent, u64 ref_root,
 			       u64 owner, u64 offset, u64 reserved, int action,

commit 44e1c47d5c3f31a9f5c883834eb9e29d0b165ea8
Author: Nikolay Borisov <nborisov@suse.com>
Date:   Wed Jun 20 15:48:53 2018 +0300

    btrfs: Remove fs_info from btrfs_add_delayed_tree_ref
    
    This function is always called with a valid transaction handle from
    where fs_info can be referenced. No functional changes.
    
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Reviewed-by: Qu Wenruo <wqu@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index ea1aecb6a50d..31729302c827 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -234,8 +234,7 @@ static inline void btrfs_put_delayed_ref_head(struct btrfs_delayed_ref_head *hea
 		kmem_cache_free(btrfs_delayed_ref_head_cachep, head);
 }
 
-int btrfs_add_delayed_tree_ref(struct btrfs_fs_info *fs_info,
-			       struct btrfs_trans_handle *trans,
+int btrfs_add_delayed_tree_ref(struct btrfs_trans_handle *trans,
 			       u64 bytenr, u64 num_bytes, u64 parent,
 			       u64 ref_root, int level, int action,
 			       struct btrfs_delayed_extent_op *extent_op,

commit be97f133b374bd60b7f5f87a4e93ad408bd5fe03
Author: Nikolay Borisov <nborisov@suse.com>
Date:   Thu Apr 19 11:06:39 2018 +0300

    btrfs: Drop fs_info parameter from btrfs_merge_delayed_refs
    
    It's provided by the transaction handle.
    
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index 84cc007badd6..ea1aecb6a50d 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -251,7 +251,6 @@ int btrfs_add_delayed_extent_op(struct btrfs_fs_info *fs_info,
 				u64 bytenr, u64 num_bytes,
 				struct btrfs_delayed_extent_op *extent_op);
 void btrfs_merge_delayed_refs(struct btrfs_trans_handle *trans,
-			      struct btrfs_fs_info *fs_info,
 			      struct btrfs_delayed_ref_root *delayed_refs,
 			      struct btrfs_delayed_ref_head *head);
 

commit 41d0bd3b5e73afbcee3cd7dcb6f3f0ec936f54d9
Author: Nikolay Borisov <nborisov@suse.com>
Date:   Wed Apr 4 15:57:42 2018 +0300

    btrfs: Drop delayed_refs argument from btrfs_check_delayed_seq
    
    It's used to print its pointer in a debug statement but doesn't really
    bring any useful information to the error message.
    
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index 7f00db50bd24..84cc007badd6 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -269,9 +269,7 @@ static inline void btrfs_delayed_ref_unlock(struct btrfs_delayed_ref_head *head)
 struct btrfs_delayed_ref_head *
 btrfs_select_ref_head(struct btrfs_trans_handle *trans);
 
-int btrfs_check_delayed_seq(struct btrfs_fs_info *fs_info,
-			    struct btrfs_delayed_ref_root *delayed_refs,
-			    u64 seq);
+int btrfs_check_delayed_seq(struct btrfs_fs_info *fs_info, u64 seq);
 
 /*
  * helper functions to cast a node into its container

commit 5e388e95815408c27f3612190d089afc0774b870
Author: Nikolay Borisov <nborisov@suse.com>
Date:   Wed Apr 18 09:41:54 2018 +0300

    btrfs: Fix race condition between delayed refs and blockgroup removal
    
    When the delayed refs for a head are all run, eventually
    cleanup_ref_head is called which (in case of deletion) obtains a
    reference for the relevant btrfs_space_info struct by querying the bg
    for the range. This is problematic because when the last extent of a
    bg is deleted a race window emerges between removal of that bg and the
    subsequent invocation of cleanup_ref_head. This can result in cache being null
    and either a null pointer dereference or assertion failure.
    
            task: ffff8d04d31ed080 task.stack: ffff9e5dc10cc000
            RIP: 0010:assfail.constprop.78+0x18/0x1a [btrfs]
            RSP: 0018:ffff9e5dc10cfbe8 EFLAGS: 00010292
            RAX: 0000000000000044 RBX: 0000000000000000 RCX: 0000000000000000
            RDX: ffff8d04ffc1f868 RSI: ffff8d04ffc178c8 RDI: ffff8d04ffc178c8
            RBP: ffff8d04d29e5ea0 R08: 00000000000001f0 R09: 0000000000000001
            R10: ffff9e5dc0507d58 R11: 0000000000000001 R12: ffff8d04d29e5ea0
            R13: ffff8d04d29e5f08 R14: ffff8d04efe29b40 R15: ffff8d04efe203e0
            FS:  00007fbf58ead500(0000) GS:ffff8d04ffc00000(0000) knlGS:0000000000000000
            CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
            CR2: 00007fe6c6975648 CR3: 0000000013b2a000 CR4: 00000000000006f0
            DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
            DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
            Call Trace:
             __btrfs_run_delayed_refs+0x10e7/0x12c0 [btrfs]
             btrfs_run_delayed_refs+0x68/0x250 [btrfs]
             btrfs_should_end_transaction+0x42/0x60 [btrfs]
             btrfs_truncate_inode_items+0xaac/0xfc0 [btrfs]
             btrfs_evict_inode+0x4c6/0x5c0 [btrfs]
             evict+0xc6/0x190
             do_unlinkat+0x19c/0x300
             do_syscall_64+0x74/0x140
             entry_SYSCALL_64_after_hwframe+0x3d/0xa2
            RIP: 0033:0x7fbf589c57a7
    
    To fix this, introduce a new flag "is_system" to head_ref structs,
    which is populated at insertion time. This allows to decouple the
    querying for the spaceinfo from querying the possibly deleted bg.
    
    Fixes: d7eae3403f46 ("Btrfs: rework delayed ref total_bytes_pinned accounting")
    CC: stable@vger.kernel.org # 4.14+
    Suggested-by: Omar Sandoval <osandov@osandov.com>
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Reviewed-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index 741869dbc316..7f00db50bd24 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -127,6 +127,7 @@ struct btrfs_delayed_ref_head {
 	 */
 	unsigned int must_insert_reserved:1;
 	unsigned int is_data:1;
+	unsigned int is_system:1;
 	unsigned int processing:1;
 };
 

commit 9888c3402c8567a977de37f61e9dd87792723064
Author: David Sterba <dsterba@suse.com>
Date:   Tue Apr 3 19:16:55 2018 +0200

    btrfs: replace GPL boilerplate by SPDX -- headers
    
    Remove GPL boilerplate text (long, short, one-line) and keep the rest,
    ie. personal, company or original source copyright statements. Add the
    SPDX header.
    
    Unify the include protection macros to match the file names.
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index 9e3e5aff0937..741869dbc316 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -1,22 +1,10 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 /*
  * Copyright (C) 2008 Oracle.  All rights reserved.
- *
- * This program is free software; you can redistribute it and/or
- * modify it under the terms of the GNU General Public
- * License v2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * General Public License for more details.
- *
- * You should have received a copy of the GNU General Public
- * License along with this program; if not, write to the
- * Free Software Foundation, Inc., 59 Temple Place - Suite 330,
- * Boston, MA 021110-1307, USA.
  */
-#ifndef __DELAYED_REF__
-#define __DELAYED_REF__
+
+#ifndef BTRFS_DELAYED_REF_H
+#define BTRFS_DELAYED_REF_H
 
 #include <linux/refcount.h>
 
@@ -298,4 +286,5 @@ btrfs_delayed_node_to_data_ref(struct btrfs_delayed_ref_node *node)
 {
 	return container_of(node, struct btrfs_delayed_data_ref, node);
 }
+
 #endif

commit e67c718b5b9a306bde7e966be7b4ca48fa063d73
Author: David Sterba <dsterba@suse.com>
Date:   Mon Feb 19 17:24:18 2018 +0100

    btrfs: add more __cold annotations
    
    The __cold functions are placed to a special section, as they're
    expected to be called rarely. This could help i-cache prefetches or help
    compiler to decide which branches are more/less likely to be taken
    without any other annotations needed.
    
    Though we can't add more __exit annotations, it's still possible to add
    __cold (that's also added with __exit). That way the following function
    categories are tagged:
    
    - printf wrappers, error messages
    - exit helpers
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index c4f625e5a691..9e3e5aff0937 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -204,7 +204,7 @@ extern struct kmem_cache *btrfs_delayed_data_ref_cachep;
 extern struct kmem_cache *btrfs_delayed_extent_op_cachep;
 
 int __init btrfs_delayed_ref_init(void);
-void btrfs_delayed_ref_exit(void);
+void __cold btrfs_delayed_ref_exit(void);
 
 static inline struct btrfs_delayed_extent_op *
 btrfs_alloc_delayed_extent_op(void)

commit f5c29bd9dbd3e90e03ab7697ecc373b49394e62e
Author: Liu Bo <bo.li.liu@oracle.com>
Date:   Thu Nov 2 17:21:50 2017 -0600

    Btrfs: add __init macro to btrfs init functions
    
    Adding __init macro gives kernel a hint that this function is only used
    during the initialization phase and its memory resources can be freed up
    after.
    
    Signed-off-by: Liu Bo <bo.li.liu@oracle.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index a43af432f859..c4f625e5a691 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -203,7 +203,7 @@ extern struct kmem_cache *btrfs_delayed_tree_ref_cachep;
 extern struct kmem_cache *btrfs_delayed_data_ref_cachep;
 extern struct kmem_cache *btrfs_delayed_extent_op_cachep;
 
-int btrfs_delayed_ref_init(void);
+int __init btrfs_delayed_ref_init(void);
 void btrfs_delayed_ref_exit(void);
 
 static inline struct btrfs_delayed_extent_op *

commit 0e0adbcfdc908684317c99a9bf5e13383f03b7ec
Author: Josef Bacik <josef@toxicpanda.com>
Date:   Thu Oct 19 14:16:00 2017 -0400

    btrfs: track refs in a rb_tree instead of a list
    
    If we get a significant amount of delayed refs for a single block (think
    modifying multiple snapshots) we can end up spending an ungodly amount
    of time looping through all of the entries trying to see if they can be
    merged.  This is because we only add them to a list, so we have O(2n)
    for every ref head.  This doesn't make any sense as we likely have refs
    for different roots, and so they cannot be merged.  Tracking in a tree
    will allow us to break as soon as we hit an entry that doesn't match,
    making our worst case O(n).
    
    With this we can also merge entries more easily.  Before we had to hope
    that matching refs were on the ends of our list, but with the tree we
    can search down to exact matches and merge them at insert time.
    
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index 1ce11858d727..a43af432f859 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -27,8 +27,7 @@
 #define BTRFS_UPDATE_DELAYED_HEAD 4 /* not changing ref count on head ref */
 
 struct btrfs_delayed_ref_node {
-	/*data/tree ref use list, stored in ref_head->ref_list. */
-	struct list_head list;
+	struct rb_node ref_node;
 	/*
 	 * If action is BTRFS_ADD_DELAYED_REF, also link this node to
 	 * ref_head->ref_add_list, then we do not need to iterate the
@@ -92,7 +91,7 @@ struct btrfs_delayed_ref_head {
 	struct mutex mutex;
 
 	spinlock_t lock;
-	struct list_head ref_list;
+	struct rb_root ref_tree;
 	/* accumulate add BTRFS_ADD_DELAYED_REF nodes to this ref_add_list. */
 	struct list_head ref_add_list;
 

commit d278850eff3053ef166cf64c16f798dfe36278a2
Author: Josef Bacik <josef@toxicpanda.com>
Date:   Fri Sep 29 15:43:57 2017 -0400

    btrfs: remove delayed_ref_node from ref_head
    
    This is just excessive information in the ref_head, and makes the code
    complicated.  It is a relic from when we had the heads and the refs in
    the same tree, which is no longer the case.  With this removal I've
    cleaned up a bunch of the cruft around this old assumption as well.
    
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index ce88e4ac5276..1ce11858d727 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -26,15 +26,6 @@
 #define BTRFS_ADD_DELAYED_EXTENT 3 /* record a full extent allocation */
 #define BTRFS_UPDATE_DELAYED_HEAD 4 /* not changing ref count on head ref */
 
-/*
- * XXX: Qu: I really hate the design that ref_head and tree/data ref shares the
- * same ref_node structure.
- * Ref_head is in a higher logic level than tree/data ref, and duplicated
- * bytenr/num_bytes in ref_node is really a waste or memory, they should be
- * referred from ref_head.
- * This gets more disgusting after we use list to store tree/data ref in
- * ref_head. Must clean this mess up later.
- */
 struct btrfs_delayed_ref_node {
 	/*data/tree ref use list, stored in ref_head->ref_list. */
 	struct list_head list;
@@ -91,8 +82,9 @@ struct btrfs_delayed_extent_op {
  * reference count modifications we've queued up.
  */
 struct btrfs_delayed_ref_head {
-	struct btrfs_delayed_ref_node node;
-
+	u64 bytenr;
+	u64 num_bytes;
+	refcount_t refs;
 	/*
 	 * the mutex is held while running the refs, and it is also
 	 * held when checking the sum of reference modifications.
@@ -115,6 +107,14 @@ struct btrfs_delayed_ref_head {
 	 */
 	int total_ref_mod;
 
+	/*
+	 * This is the current outstanding mod references for this bytenr.  This
+	 * is used with lookup_extent_info to get an accurate reference count
+	 * for a bytenr, so it is adjusted as delayed refs are run so that any
+	 * on disk reference count + ref_mod is accurate.
+	 */
+	int ref_mod;
+
 	/*
 	 * For qgroup reserved space freeing.
 	 *
@@ -234,15 +234,18 @@ static inline void btrfs_put_delayed_ref(struct btrfs_delayed_ref_node *ref)
 		case BTRFS_SHARED_DATA_REF_KEY:
 			kmem_cache_free(btrfs_delayed_data_ref_cachep, ref);
 			break;
-		case 0:
-			kmem_cache_free(btrfs_delayed_ref_head_cachep, ref);
-			break;
 		default:
 			BUG();
 		}
 	}
 }
 
+static inline void btrfs_put_delayed_ref_head(struct btrfs_delayed_ref_head *head)
+{
+	if (refcount_dec_and_test(&head->refs))
+		kmem_cache_free(btrfs_delayed_ref_head_cachep, head);
+}
+
 int btrfs_add_delayed_tree_ref(struct btrfs_fs_info *fs_info,
 			       struct btrfs_trans_handle *trans,
 			       u64 bytenr, u64 num_bytes, u64 parent,
@@ -282,36 +285,18 @@ int btrfs_check_delayed_seq(struct btrfs_fs_info *fs_info,
 			    struct btrfs_delayed_ref_root *delayed_refs,
 			    u64 seq);
 
-/*
- * a node might live in a head or a regular ref, this lets you
- * test for the proper type to use.
- */
-static int btrfs_delayed_ref_is_head(struct btrfs_delayed_ref_node *node)
-{
-	return node->is_head;
-}
-
 /*
  * helper functions to cast a node into its container
  */
 static inline struct btrfs_delayed_tree_ref *
 btrfs_delayed_node_to_tree_ref(struct btrfs_delayed_ref_node *node)
 {
-	WARN_ON(btrfs_delayed_ref_is_head(node));
 	return container_of(node, struct btrfs_delayed_tree_ref, node);
 }
 
 static inline struct btrfs_delayed_data_ref *
 btrfs_delayed_node_to_data_ref(struct btrfs_delayed_ref_node *node)
 {
-	WARN_ON(btrfs_delayed_ref_is_head(node));
 	return container_of(node, struct btrfs_delayed_data_ref, node);
 }
-
-static inline struct btrfs_delayed_ref_head *
-btrfs_delayed_node_to_head(struct btrfs_delayed_ref_node *node)
-{
-	WARN_ON(!btrfs_delayed_ref_is_head(node));
-	return container_of(node, struct btrfs_delayed_ref_head, node);
-}
 #endif

commit 7be07912b32d103d9789082f27dd54b47c89c744
Author: Omar Sandoval <osandov@fb.com>
Date:   Tue Jun 6 16:45:30 2017 -0700

    Btrfs: return old and new total ref mods when adding delayed refs
    
    We need this to decide when to account pinned bytes.
    
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index c0264ff01b53..ce88e4ac5276 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -247,12 +247,14 @@ int btrfs_add_delayed_tree_ref(struct btrfs_fs_info *fs_info,
 			       struct btrfs_trans_handle *trans,
 			       u64 bytenr, u64 num_bytes, u64 parent,
 			       u64 ref_root, int level, int action,
-			       struct btrfs_delayed_extent_op *extent_op);
+			       struct btrfs_delayed_extent_op *extent_op,
+			       int *old_ref_mod, int *new_ref_mod);
 int btrfs_add_delayed_data_ref(struct btrfs_fs_info *fs_info,
 			       struct btrfs_trans_handle *trans,
 			       u64 bytenr, u64 num_bytes,
 			       u64 parent, u64 ref_root,
-			       u64 owner, u64 offset, u64 reserved, int action);
+			       u64 owner, u64 offset, u64 reserved, int action,
+			       int *old_ref_mod, int *new_ref_mod);
 int btrfs_add_delayed_extent_op(struct btrfs_fs_info *fs_info,
 				struct btrfs_trans_handle *trans,
 				u64 bytenr, u64 num_bytes,

commit 6df8cdf5bda221f268ac23940bce589ad176993d
Author: Elena Reshetova <elena.reshetova@intel.com>
Date:   Fri Mar 3 10:55:15 2017 +0200

    btrfs: convert btrfs_delayed_ref_node.refs from atomic_t to refcount_t
    
    refcount_t type and corresponding API should be
    used instead of atomic_t when the variable is used as
    a reference counter. This allows to avoid accidental
    refcounter overflows that might lead to use-after-free
    situations.
    
    Signed-off-by: Elena Reshetova <elena.reshetova@intel.com>
    Signed-off-by: Hans Liljestrand <ishkamiel@gmail.com>
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: David Windsor <dwindsor@gmail.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index 0e537f98f1a1..c0264ff01b53 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -18,6 +18,8 @@
 #ifndef __DELAYED_REF__
 #define __DELAYED_REF__
 
+#include <linux/refcount.h>
+
 /* these are the possible values of struct btrfs_delayed_ref_node->action */
 #define BTRFS_ADD_DELAYED_REF    1 /* add one backref to the tree */
 #define BTRFS_DROP_DELAYED_REF   2 /* delete one backref from the tree */
@@ -53,7 +55,7 @@ struct btrfs_delayed_ref_node {
 	u64 seq;
 
 	/* ref count on this data structure */
-	atomic_t refs;
+	refcount_t refs;
 
 	/*
 	 * how many refs is this entry adding or deleting.  For
@@ -220,8 +222,8 @@ btrfs_free_delayed_extent_op(struct btrfs_delayed_extent_op *op)
 
 static inline void btrfs_put_delayed_ref(struct btrfs_delayed_ref_node *ref)
 {
-	WARN_ON(atomic_read(&ref->refs) == 0);
-	if (atomic_dec_and_test(&ref->refs)) {
+	WARN_ON(refcount_read(&ref->refs) == 0);
+	if (refcount_dec_and_test(&ref->refs)) {
 		WARN_ON(ref->in_tree);
 		switch (ref->type) {
 		case BTRFS_TREE_BLOCK_REF_KEY:

commit f72ad18e996338a8033aeb981e645f8dbf4e22c1
Author: Liu Bo <bo.li.liu@oracle.com>
Date:   Mon Jan 30 12:24:37 2017 -0800

    Btrfs: pass delayed_refs directly to btrfs_find_delayed_ref_head
    
    All we need is @delayed_refs, all callers have get it ahead of calling
    btrfs_find_delayed_ref_head since lock needs to be acquired firstly,
    there is no reason to deference it again inside the function.
    
    Signed-off-by: Liu Bo <bo.li.liu@oracle.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index 8a0220fb703c..0e537f98f1a1 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -261,7 +261,8 @@ void btrfs_merge_delayed_refs(struct btrfs_trans_handle *trans,
 			      struct btrfs_delayed_ref_head *head);
 
 struct btrfs_delayed_ref_head *
-btrfs_find_delayed_ref_head(struct btrfs_trans_handle *trans, u64 bytenr);
+btrfs_find_delayed_ref_head(struct btrfs_delayed_ref_root *delayed_refs,
+			    u64 bytenr);
 int btrfs_delayed_ref_lock(struct btrfs_trans_handle *trans,
 			   struct btrfs_delayed_ref_head *head);
 static inline void btrfs_delayed_ref_unlock(struct btrfs_delayed_ref_head *head)

commit fef394f75bc17599c17287fbc437d4fb07a98583
Author: Jeff Mahoney <jeffm@suse.com>
Date:   Tue Dec 13 14:39:34 2016 -0500

    btrfs: drop unused extent_op arg from btrfs_add_delayed_data_ref
    
    btrfs_add_delayed_data_ref is always called with a NULL extent_op,
    so let's drop the argument.
    
    Signed-off-by: Jeff Mahoney <jeffm@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index 50947b5a9152..8a0220fb703c 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -250,8 +250,7 @@ int btrfs_add_delayed_data_ref(struct btrfs_fs_info *fs_info,
 			       struct btrfs_trans_handle *trans,
 			       u64 bytenr, u64 num_bytes,
 			       u64 parent, u64 ref_root,
-			       u64 owner, u64 offset, u64 reserved, int action,
-			       struct btrfs_delayed_extent_op *extent_op);
+			       u64 owner, u64 offset, u64 reserved, int action);
 int btrfs_add_delayed_extent_op(struct btrfs_fs_info *fs_info,
 				struct btrfs_trans_handle *trans,
 				u64 bytenr, u64 num_bytes,

commit 5f52a2c512a55500349aa261e469d099ede0f256
Merge: 7c4c71ac8a72 2a7bf53f577e
Author: Chris Mason <clm@fb.com>
Date:   Tue Dec 13 09:14:42 2016 -0800

    Merge branch 'for-chris-4.10' of git://git.kernel.org/pub/scm/linux/kernel/git/fdmanana/linux into for-linus-4.10
    
    Patches queued up by Filipe:
    
    The most important change is still the fix for the extent tree
    corruption that happens due to balance when qgroups are enabled (a
    regression introduced in 4.7 by a fix for a regression from the last
    qgroups rework). This has been hitting SLE and openSUSE users and QA
    very badly, where transactions keep getting aborted when running
    delayed references leaving the root filesystem in RO mode and nearly
    unusable.  There are fixes here that allow us to run xfstests again
    with the integrity checker enabled, which has been impossible since 4.8
    (apparently I'm the only one running xfstests with the integrity
    checker enabled, which is useful to validate dirtied leafs, like
    checking if there are keys out of order, etc).  The rest are just some
    trivial fixes, most of them tagged for stable, and two cleanups.
    
    Signed-off-by: Chris Mason <clm@fb.com>

commit 1d57ee941692d0cc928526e21a1557b2ae3e11db
Author: Wang Xiaoguang <wangxg.fnst@cn.fujitsu.com>
Date:   Wed Oct 26 18:07:33 2016 +0800

    btrfs: improve delayed refs iterations
    
    This issue was found when I tried to delete a heavily reflinked file,
    when deleting such files, other transaction operation will not have a
    chance to make progress, for example, start_transaction() will blocked
    in wait_current_trans(root) for long time, sometimes it even triggers
    soft lockups, and the time taken to delete such heavily reflinked file
    is also very large, often hundreds of seconds. Using perf top, it reports
    that:
    
    PerfTop:    7416 irqs/sec  kernel:99.8%  exact:  0.0% [4000Hz cpu-clock],  (all, 4 CPUs)
    ---------------------------------------------------------------------------------------
        84.37%  [btrfs]             [k] __btrfs_run_delayed_refs.constprop.80
        11.02%  [kernel]            [k] delay_tsc
         0.79%  [kernel]            [k] _raw_spin_unlock_irq
         0.78%  [kernel]            [k] _raw_spin_unlock_irqrestore
         0.45%  [kernel]            [k] do_raw_spin_lock
         0.18%  [kernel]            [k] __slab_alloc
    It seems __btrfs_run_delayed_refs() took most cpu time, after some debug
    work, I found it's select_delayed_ref() causing this issue, for a delayed
    head, in our case, it'll be full of BTRFS_DROP_DELAYED_REF nodes, but
    select_delayed_ref() will firstly try to iterate node list to find
    BTRFS_ADD_DELAYED_REF nodes, obviously it's a disaster in this case, and
    waste much time.
    
    To fix this issue, we introduce a new ref_add_list in struct btrfs_delayed_ref_head,
    then in select_delayed_ref(), if this list is not empty, we can directly use
    nodes in this list. With this patch, it just took about 10~15 seconds to
    delte the same file. Now using perf top, it reports that:
    
    PerfTop:    2734 irqs/sec  kernel:99.5%  exact:  0.0% [4000Hz cpu-clock],  (all, 4 CPUs)
    ----------------------------------------------------------------------------------------
    
        20.74%  [kernel]          [k] _raw_spin_unlock_irqrestore
        16.33%  [kernel]          [k] __slab_alloc
         5.41%  [kernel]          [k] lock_acquired
         4.42%  [kernel]          [k] lock_acquire
         4.05%  [kernel]          [k] lock_release
         3.37%  [kernel]          [k] _raw_spin_unlock_irq
    
    For normal files, this patch also gives help, at least we do not need to
    iterate whole list to found BTRFS_ADD_DELAYED_REF nodes.
    
    Signed-off-by: Wang Xiaoguang <wangxg.fnst@cn.fujitsu.com>
    Reviewed-by: Liu Bo <bo.li.liu@oracle.com>
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index 43f3629760e9..dba97842b47a 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -42,6 +42,12 @@ struct btrfs_delayed_ref_node {
 
 	/*data/tree ref use list, stored in ref_head->ref_list. */
 	struct list_head list;
+	/*
+	 * If action is BTRFS_ADD_DELAYED_REF, also link this node to
+	 * ref_head->ref_add_list, then we do not need to iterate the
+	 * whole ref_head->ref_list to find BTRFS_ADD_DELAYED_REF nodes.
+	 */
+	struct list_head add_list;
 
 	/* the starting bytenr of the extent */
 	u64 bytenr;
@@ -99,6 +105,8 @@ struct btrfs_delayed_ref_head {
 
 	spinlock_t lock;
 	struct list_head ref_list;
+	/* accumulate add BTRFS_ADD_DELAYED_REF nodes to this ref_add_list. */
+	struct list_head ref_add_list;
 
 	struct rb_node href_node;
 

commit 2a2a83de548f7afe2c27e51cbc9ff761cba2b61b
Author: Filipe Manana <fdmanana@suse.com>
Date:   Wed Nov 2 10:55:18 2016 +0000

    Btrfs: remove rb_node field from the delayed ref node structure
    
    After the last big change in the delayed references code that was needed
    for the last qgroups rework, the red black tree node field of struct
    btrfs_delayed_ref_node is no longer used, so just remove it, this helps
    us save some memory (since struct rb_node is 24 bytes on x86_64) for
    these structures.
    
    Signed-off-by: Filipe Manana <fdmanana@suse.com>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index 43f3629760e9..a72fbe7f4e09 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -34,12 +34,6 @@
  * ref_head. Must clean this mess up later.
  */
 struct btrfs_delayed_ref_node {
-	/*
-	 * ref_head use rb tree, stored in ref_root->href.
-	 * indexed by bytenr
-	 */
-	struct rb_node rb_node;
-
 	/*data/tree ref use list, stored in ref_head->ref_list. */
 	struct list_head list;
 

commit e6571499336e10f93a77c51a35fd1a96828eea71
Author: Filipe Manana <fdmanana@suse.com>
Date:   Wed Aug 3 11:01:32 2016 +0100

    Btrfs: remove unused function btrfs_add_delayed_qgroup_reserve()
    
    No longer used as of commit 5846a3c26873 ("btrfs: qgroup: Fix a race in
    delayed_ref which leads to abort trans").
    
    Signed-off-by: Filipe Manana <fdmanana@suse.com>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index 5fca9534a271..43f3629760e9 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -250,9 +250,6 @@ int btrfs_add_delayed_data_ref(struct btrfs_fs_info *fs_info,
 			       u64 parent, u64 ref_root,
 			       u64 owner, u64 offset, u64 reserved, int action,
 			       struct btrfs_delayed_extent_op *extent_op);
-int btrfs_add_delayed_qgroup_reserve(struct btrfs_fs_info *fs_info,
-				     struct btrfs_trans_handle *trans,
-				     u64 ref_root, u64 bytenr, u64 num_bytes);
 int btrfs_add_delayed_extent_op(struct btrfs_fs_info *fs_info,
 				struct btrfs_trans_handle *trans,
 				u64 bytenr, u64 num_bytes,

commit 0132761017e012ab4dc8584d679503f2ba26ca86
Author: Nicholas D Steeves <nsteeves@gmail.com>
Date:   Thu May 19 21:18:45 2016 -0400

    btrfs: fix string and comment grammatical issues and typos
    
    Signed-off-by: Nicholas D Steeves <nsteeves@gmail.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index c24b653c7343..5fca9534a271 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -188,7 +188,7 @@ struct btrfs_delayed_ref_root {
 
 	/*
 	 * To make qgroup to skip given root.
-	 * This is for snapshot, as btrfs_qgroup_inherit() will manully
+	 * This is for snapshot, as btrfs_qgroup_inherit() will manually
 	 * modify counters for snapshot and its source, so we should skip
 	 * the snapshot in new_root/old_roots or it will get calculated twice
 	 */

commit 35b3ad50baa4a5fc2ae616c0513d2987bfb52a85
Author: David Sterba <dsterba@suse.com>
Date:   Mon Nov 30 16:51:29 2015 +0100

    btrfs: better packing of btrfs_delayed_extent_op
    
    btrfs_delayed_extent_op can be packed in a better way, it's 40 bytes now
    and has 8 unused bytes. Reducing the level type to u8 makes it possible
    to squeeze it to the padding byte after key. The bitfields were switched
    to bool as there's space to store the full byte without increasing the
    whole structure, besides that the generated assembly is smaller.
    
    struct btrfs_delayed_extent_op {
            struct btrfs_disk_key      key;                  /*     0    17 */
            u8                         level;                /*    17     1 */
            bool                       update_key;           /*    18     1 */
            bool                       update_flags;         /*    19     1 */
            bool                       is_data;              /*    20     1 */
    
            /* XXX 3 bytes hole, try to pack */
    
            u64                        flags_to_set;         /*    24     8 */
    
            /* size: 32, cachelines: 1, members: 6 */
            /* sum members: 29, holes: 1, sum holes: 3 */
            /* last cacheline: 32 bytes */
    };
    
    The final size is 32 bytes which gives +26 object per slab page.
    
       text    data     bss     dec     hex filename
     938811   43670   23144 1005625   f5839 fs/btrfs/btrfs.ko.before
     938747   43670   23144 1005561   f57f9 fs/btrfs/btrfs.ko.after
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index 00ed02cbf3e9..c24b653c7343 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -75,11 +75,11 @@ struct btrfs_delayed_ref_node {
 
 struct btrfs_delayed_extent_op {
 	struct btrfs_disk_key key;
+	u8 level;
+	bool update_key;
+	bool update_flags;
+	bool is_data;
 	u64 flags_to_set;
-	int level;
-	unsigned int update_key:1;
-	unsigned int update_flags:1;
-	unsigned int is_data:1;
 };
 
 /*

commit 5846a3c26873e86b034c702a8bc202aa76082369
Author: Qu Wenruo <quwenruo@cn.fujitsu.com>
Date:   Mon Oct 26 14:11:18 2015 +0800

    btrfs: qgroup: Fix a race in delayed_ref which leads to abort trans
    
    Between btrfs_allocerved_file_extent() and
    btrfs_add_delayed_qgroup_reserve(), there is a window that delayed_refs
    are run and delayed ref head maybe freed before
    btrfs_add_delayed_qgroup_reserve().
    
    This will cause btrfs_dad_delayed_qgroup_reserve() to return -ENOENT,
    and cause transaction to be aborted.
    
    This patch will record qgroup reserve space info into delayed_ref_head
    at btrfs_add_delayed_ref(), to eliminate the race window.
    
    Reported-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: Qu Wenruo <quwenruo@cn.fujitsu.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index f9cf2345b864..00ed02cbf3e9 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -248,7 +248,7 @@ int btrfs_add_delayed_data_ref(struct btrfs_fs_info *fs_info,
 			       struct btrfs_trans_handle *trans,
 			       u64 bytenr, u64 num_bytes,
 			       u64 parent, u64 ref_root,
-			       u64 owner, u64 offset, int action,
+			       u64 owner, u64 offset, u64 reserved, int action,
 			       struct btrfs_delayed_extent_op *extent_op);
 int btrfs_add_delayed_qgroup_reserve(struct btrfs_fs_info *fs_info,
 				     struct btrfs_trans_handle *trans,

commit b06c4bf5c874a57254b197f53ddf588e7a24a2bf
Author: Filipe Manana <fdmanana@suse.com>
Date:   Fri Oct 23 07:52:54 2015 +0100

    Btrfs: fix regression running delayed references when using qgroups
    
    In the kernel 4.2 merge window we had a big changes to the implementation
    of delayed references and qgroups which made the no_quota field of delayed
    references not used anymore. More specifically the no_quota field is not
    used anymore as of:
    
      commit 0ed4792af0e8 ("btrfs: qgroup: Switch to new extent-oriented qgroup mechanism.")
    
    Leaving the no_quota field actually prevents delayed references from
    getting merged, which in turn cause the following BUG_ON(), at
    fs/btrfs/extent-tree.c, to be hit when qgroups are enabled:
    
      static int run_delayed_tree_ref(...)
      {
         (...)
         BUG_ON(node->ref_mod != 1);
         (...)
      }
    
    This happens on a scenario like the following:
    
      1) Ref1 bytenr X, action = BTRFS_ADD_DELAYED_REF, no_quota = 1, added.
    
      2) Ref2 bytenr X, action = BTRFS_DROP_DELAYED_REF, no_quota = 0, added.
         It's not merged with Ref1 because Ref1->no_quota != Ref2->no_quota.
    
      3) Ref3 bytenr X, action = BTRFS_ADD_DELAYED_REF, no_quota = 1, added.
         It's not merged with the reference at the tail of the list of refs
         for bytenr X because the reference at the tail, Ref2 is incompatible
         due to Ref2->no_quota != Ref3->no_quota.
    
      4) Ref4 bytenr X, action = BTRFS_DROP_DELAYED_REF, no_quota = 0, added.
         It's not merged with the reference at the tail of the list of refs
         for bytenr X because the reference at the tail, Ref3 is incompatible
         due to Ref3->no_quota != Ref4->no_quota.
    
      5) We run delayed references, trigger merging of delayed references,
         through __btrfs_run_delayed_refs() -> btrfs_merge_delayed_refs().
    
      6) Ref1 and Ref3 are merged as Ref1->no_quota = Ref3->no_quota and
         all other conditions are satisfied too. So Ref1 gets a ref_mod
         value of 2.
    
      7) Ref2 and Ref4 are merged as Ref2->no_quota = Ref4->no_quota and
         all other conditions are satisfied too. So Ref2 gets a ref_mod
         value of 2.
    
      8) Ref1 and Ref2 aren't merged, because they have different values
         for their no_quota field.
    
      9) Delayed reference Ref1 is picked for running (select_delayed_ref()
         always prefers references with an action == BTRFS_ADD_DELAYED_REF).
         So run_delayed_tree_ref() is called for Ref1 which triggers the
         BUG_ON because Ref1->red_mod != 1 (equals 2).
    
    So fix this by removing the no_quota field, as it's not used anymore as
    of commit 0ed4792af0e8 ("btrfs: qgroup: Switch to new extent-oriented
    qgroup mechanism.").
    
    The use of no_quota was also buggy in at least two places:
    
    1) At delayed-refs.c:btrfs_add_delayed_tree_ref() - we were setting
       no_quota to 0 instead of 1 when the following condition was true:
       is_fstree(ref_root) || !fs_info->quota_enabled
    
    2) At extent-tree.c:__btrfs_inc_extent_ref() - we were attempting to
       reset a node's no_quota when the condition "!is_fstree(root_objectid)
       || !root->fs_info->quota_enabled" was true but we did it only in
       an unused local stack variable, that is, we never reset the no_quota
       value in the node itself.
    
    This fixes the remainder of problems several people have been having when
    running delayed references, mostly while a balance is running in parallel,
    on a 4.2+ kernel.
    
    Very special thanks to Stéphane Lesimple for helping debugging this issue
    and testing this fix on his multi terabyte filesystem (which took more
    than one day to balance alone, plus fsck, etc).
    
    Also, this fixes deadlock issue when using the clone ioctl with qgroups
    enabled, as reported by Elias Probst in the mailing list. The deadlock
    happens because after calling btrfs_insert_empty_item we have our path
    holding a write lock on a leaf of the fs/subvol tree and then before
    releasing the path we called check_ref() which did backref walking, when
    qgroups are enabled, and tried to read lock the same leaf. The trace for
    this case is the following:
    
      INFO: task systemd-nspawn:6095 blocked for more than 120 seconds.
      (...)
      Call Trace:
        [<ffffffff86999201>] schedule+0x74/0x83
        [<ffffffff863ef64c>] btrfs_tree_read_lock+0xc0/0xea
        [<ffffffff86137ed7>] ? wait_woken+0x74/0x74
        [<ffffffff8639f0a7>] btrfs_search_old_slot+0x51a/0x810
        [<ffffffff863a129b>] btrfs_next_old_leaf+0xdf/0x3ce
        [<ffffffff86413a00>] ? ulist_add_merge+0x1b/0x127
        [<ffffffff86411688>] __resolve_indirect_refs+0x62a/0x667
        [<ffffffff863ef546>] ? btrfs_clear_lock_blocking_rw+0x78/0xbe
        [<ffffffff864122d3>] find_parent_nodes+0xaf3/0xfc6
        [<ffffffff86412838>] __btrfs_find_all_roots+0x92/0xf0
        [<ffffffff864128f2>] btrfs_find_all_roots+0x45/0x65
        [<ffffffff8639a75b>] ? btrfs_get_tree_mod_seq+0x2b/0x88
        [<ffffffff863e852e>] check_ref+0x64/0xc4
        [<ffffffff863e9e01>] btrfs_clone+0x66e/0xb5d
        [<ffffffff863ea77f>] btrfs_ioctl_clone+0x48f/0x5bb
        [<ffffffff86048a68>] ? native_sched_clock+0x28/0x77
        [<ffffffff863ed9b0>] btrfs_ioctl+0xabc/0x25cb
      (...)
    
    The problem goes away by eleminating check_ref(), which no longer is
    needed as its purpose was to get a value for the no_quota field of
    a delayed reference (this patch removes the no_quota field as mentioned
    earlier).
    
    Reported-by: Stéphane Lesimple <stephane_btrfs@lesimple.fr>
    Tested-by: Stéphane Lesimple <stephane_btrfs@lesimple.fr>
    Reported-by: Elias Probst <mail@eliasprobst.eu>
    Reported-by: Peter Becker <floyd.net@gmail.com>
    Reported-by: Malte Schröder <malte@tnxip.de>
    Reported-by: Derek Dongray <derek@valedon.co.uk>
    Reported-by: Erkki Seppala <flux-btrfs@inside.org>
    Cc: stable@vger.kernel.org  # 4.2+
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Reviewed-by: Qu Wenruo <quwenruo@cn.fujitsu.com>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index d4c41e26101c..f9cf2345b864 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -68,7 +68,6 @@ struct btrfs_delayed_ref_node {
 
 	unsigned int action:8;
 	unsigned int type:8;
-	unsigned int no_quota:1;
 	/* is this node still in the rbtree? */
 	unsigned int is_head:1;
 	unsigned int in_tree:1;
@@ -244,15 +243,13 @@ int btrfs_add_delayed_tree_ref(struct btrfs_fs_info *fs_info,
 			       struct btrfs_trans_handle *trans,
 			       u64 bytenr, u64 num_bytes, u64 parent,
 			       u64 ref_root, int level, int action,
-			       struct btrfs_delayed_extent_op *extent_op,
-			       int no_quota);
+			       struct btrfs_delayed_extent_op *extent_op);
 int btrfs_add_delayed_data_ref(struct btrfs_fs_info *fs_info,
 			       struct btrfs_trans_handle *trans,
 			       u64 bytenr, u64 num_bytes,
 			       u64 parent, u64 ref_root,
 			       u64 owner, u64 offset, int action,
-			       struct btrfs_delayed_extent_op *extent_op,
-			       int no_quota);
+			       struct btrfs_delayed_extent_op *extent_op);
 int btrfs_add_delayed_qgroup_reserve(struct btrfs_fs_info *fs_info,
 				     struct btrfs_trans_handle *trans,
 				     u64 ref_root, u64 bytenr, u64 num_bytes);

commit f64d5ca868214f9beb60108c82fcb96940132bfb
Author: Qu Wenruo <quwenruo@cn.fujitsu.com>
Date:   Tue Sep 8 17:08:36 2015 +0800

    btrfs: delayed_ref: Add new function to record reserved space into delayed ref
    
    Add new function btrfs_add_delayed_qgroup_reserve() function to record
    how much space is reserved for that extent.
    
    As btrfs only accounts qgroup at run_delayed_refs() time, so newly
    allocated extent should keep the reserved space until then.
    
    So add needed function with related members to do it.
    
    Signed-off-by: Qu Wenruo <quwenruo@cn.fujitsu.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index 13fb5e6090fe..d4c41e26101c 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -112,6 +112,17 @@ struct btrfs_delayed_ref_head {
 	 */
 	int total_ref_mod;
 
+	/*
+	 * For qgroup reserved space freeing.
+	 *
+	 * ref_root and reserved will be recorded after
+	 * BTRFS_ADD_DELAYED_EXTENT is called.
+	 * And will be used to free reserved qgroup space at
+	 * run_delayed_refs() time.
+	 */
+	u64 qgroup_ref_root;
+	u64 qgroup_reserved;
+
 	/*
 	 * when a new extent is allocated, it is just reserved in memory
 	 * The actual extent isn't inserted into the extent allocation tree
@@ -242,6 +253,9 @@ int btrfs_add_delayed_data_ref(struct btrfs_fs_info *fs_info,
 			       u64 owner, u64 offset, int action,
 			       struct btrfs_delayed_extent_op *extent_op,
 			       int no_quota);
+int btrfs_add_delayed_qgroup_reserve(struct btrfs_fs_info *fs_info,
+				     struct btrfs_trans_handle *trans,
+				     u64 ref_root, u64 bytenr, u64 num_bytes);
 int btrfs_add_delayed_extent_op(struct btrfs_fs_info *fs_info,
 				struct btrfs_trans_handle *trans,
 				u64 bytenr, u64 num_bytes,

commit 9086db86e0b09c39abead4d747119695553e3978
Author: Qu Wenruo <quwenruo@cn.fujitsu.com>
Date:   Mon Apr 20 09:53:50 2015 +0800

    btrfs: qgroup: Add the ability to skip given qgroup for old/new_roots.
    
    This is used by later qgroup fix patches for snapshot.
    
    As current snapshot accounting is done by btrfs_qgroup_inherit(), but
    new extent oriented quota mechanism will account extent from
    btrfs_copy_root() and other snapshot things, causing wrong result.
    
    So add this ability to handle snapshot accounting.
    
    Signed-off-by: Qu Wenruo <quwenruo@cn.fujitsu.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index 4016f963599e..13fb5e6090fe 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -175,6 +175,14 @@ struct btrfs_delayed_ref_root {
 	int flushing;
 
 	u64 run_delayed_start;
+
+	/*
+	 * To make qgroup to skip given root.
+	 * This is for snapshot, as btrfs_qgroup_inherit() will manully
+	 * modify counters for snapshot and its source, so we should skip
+	 * the snapshot in new_root/old_roots or it will get calculated twice
+	 */
+	u64 qgroup_to_skip;
 };
 
 extern struct kmem_cache *btrfs_delayed_ref_head_cachep;

commit 3368d001ba5df44930d986e82b1b497d4da285ba
Author: Qu Wenruo <quwenruo@cn.fujitsu.com>
Date:   Thu Apr 16 14:34:17 2015 +0800

    btrfs: qgroup: Record possible quota-related extent for qgroup.
    
    Add hook in add_delayed_ref_head() to record quota-related extent record
    into delayed_ref_root->dirty_extent_record rb-tree for later qgroup
    accounting.
    
    Signed-off-by: Qu Wenruo <quwenruo@cn.fujitsu.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index 362ca57cfeb7..4016f963599e 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -148,6 +148,9 @@ struct btrfs_delayed_ref_root {
 	/* head ref rbtree */
 	struct rb_root href_root;
 
+	/* dirty extent records */
+	struct rb_root dirty_extent_root;
+
 	/* this spin lock protects the rbtree and the entries inside */
 	spinlock_t lock;
 

commit c6fc24549960f26910cd0c6e4b5f48f2f306b11d
Author: Qu Wenruo <quwenruo@cn.fujitsu.com>
Date:   Mon Mar 30 17:03:00 2015 +0800

    btrfs: delayed-ref: Use list to replace the ref_root in ref_head.
    
    This patch replace the rbtree used in ref_head to list.
    This has the following advantage:
    1) Easier merge logic.
    With the new list implement, we only need to care merging the tail
    ref_node with the new ref_node.
    And this can be done quite easy at insert time, no need to do a
    indicated merge at run_delayed_refs().
    
    Signed-off-by: Qu Wenruo <quwenruo@cn.fujitsu.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index 5eb0892396d0..362ca57cfeb7 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -24,9 +24,25 @@
 #define BTRFS_ADD_DELAYED_EXTENT 3 /* record a full extent allocation */
 #define BTRFS_UPDATE_DELAYED_HEAD 4 /* not changing ref count on head ref */
 
+/*
+ * XXX: Qu: I really hate the design that ref_head and tree/data ref shares the
+ * same ref_node structure.
+ * Ref_head is in a higher logic level than tree/data ref, and duplicated
+ * bytenr/num_bytes in ref_node is really a waste or memory, they should be
+ * referred from ref_head.
+ * This gets more disgusting after we use list to store tree/data ref in
+ * ref_head. Must clean this mess up later.
+ */
 struct btrfs_delayed_ref_node {
+	/*
+	 * ref_head use rb tree, stored in ref_root->href.
+	 * indexed by bytenr
+	 */
 	struct rb_node rb_node;
 
+	/*data/tree ref use list, stored in ref_head->ref_list. */
+	struct list_head list;
+
 	/* the starting bytenr of the extent */
 	u64 bytenr;
 
@@ -83,7 +99,7 @@ struct btrfs_delayed_ref_head {
 	struct mutex mutex;
 
 	spinlock_t lock;
-	struct rb_root ref_root;
+	struct list_head ref_list;
 
 	struct rb_node href_node;
 

commit 1262133b8d6f10f5ca7621cd4cf65ddf6254126a
Author: Josef Bacik <jbacik@fb.com>
Date:   Tue Feb 3 07:50:16 2015 -0800

    Btrfs: account for crcs in delayed ref processing
    
    As we delete large extents, we end up doing huge amounts of COW in order
    to delete the corresponding crcs.  This adds accounting so that we keep
    track of that space and flushing of delayed refs so that we don't build
    up too much delayed crc work.
    
    This helps limit the delayed work that must be done at commit time and
    tries to avoid ENOSPC aborts because the crcs eat all the global
    reserves.
    
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index a764e2340d48..5eb0892396d0 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -88,6 +88,14 @@ struct btrfs_delayed_ref_head {
 	struct rb_node href_node;
 
 	struct btrfs_delayed_extent_op *extent_op;
+
+	/*
+	 * This is used to track the final ref_mod from all the refs associated
+	 * with this head ref, this is not adjusted as delayed refs are run,
+	 * this is meant to track if we need to do the csum accounting or not.
+	 */
+	int total_ref_mod;
+
 	/*
 	 * when a new extent is allocated, it is just reserved in memory
 	 * The actual extent isn't inserted into the extent allocation tree
@@ -138,6 +146,8 @@ struct btrfs_delayed_ref_root {
 	/* total number of head nodes ready for processing */
 	unsigned long num_heads_ready;
 
+	u64 pending_csums;
+
 	/*
 	 * set when the tree is flushing before a transaction commit,
 	 * used by the throttling code to decide if new updates need

commit fcebe4562dec83b3f8d3088d77584727b09130b2
Author: Josef Bacik <jbacik@fb.com>
Date:   Tue May 13 17:30:47 2014 -0700

    Btrfs: rework qgroup accounting
    
    Currently qgroups account for space by intercepting delayed ref updates to fs
    trees.  It does this by adding sequence numbers to delayed ref updates so that
    it can figure out how the tree looked before the update so we can adjust the
    counters properly.  The problem with this is that it does not allow delayed refs
    to be merged, so if you say are defragging an extent with 5k snapshots pointing
    to it we will thrash the delayed ref lock because we need to go back and
    manually merge these things together.  Instead we want to process quota changes
    when we know they are going to happen, like when we first allocate an extent, we
    free a reference for an extent, we add new references etc.  This patch
    accomplishes this by only adding qgroup operations for real ref changes.  We
    only modify the sequence number when we need to lookup roots for bytenrs, this
    reduces the amount of churn on the sequence number and allows us to merge
    delayed refs as we add them most of the time.  This patch encompasses a bunch of
    architectural changes
    
    1) qgroup ref operations: instead of tracking qgroup operations through the
    delayed refs we simply add new ref operations whenever we notice that we need to
    when we've modified the refs themselves.
    
    2) tree mod seq:  we no longer have this separation of major/minor counters.
    this makes the sequence number stuff much more sane and we can remove some
    locking that was needed to protect the counter.
    
    3) delayed ref seq: we now read the tree mod seq number and use that as our
    sequence.  This means each new delayed ref doesn't have it's own unique sequence
    number, rather whenever we go to lookup backrefs we inc the sequence number so
    we can make sure to keep any new operations from screwing up our world view at
    that given point.  This allows us to merge delayed refs during runtime.
    
    With all of these changes the delayed ref stuff is a little saner and the qgroup
    accounting stuff no longer goes negative in some cases like it was before.
    Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index 4ba9b93022ff..a764e2340d48 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -52,6 +52,7 @@ struct btrfs_delayed_ref_node {
 
 	unsigned int action:8;
 	unsigned int type:8;
+	unsigned int no_quota:1;
 	/* is this node still in the rbtree? */
 	unsigned int is_head:1;
 	unsigned int in_tree:1;
@@ -196,14 +197,14 @@ int btrfs_add_delayed_tree_ref(struct btrfs_fs_info *fs_info,
 			       u64 bytenr, u64 num_bytes, u64 parent,
 			       u64 ref_root, int level, int action,
 			       struct btrfs_delayed_extent_op *extent_op,
-			       int for_cow);
+			       int no_quota);
 int btrfs_add_delayed_data_ref(struct btrfs_fs_info *fs_info,
 			       struct btrfs_trans_handle *trans,
 			       u64 bytenr, u64 num_bytes,
 			       u64 parent, u64 ref_root,
 			       u64 owner, u64 offset, int action,
 			       struct btrfs_delayed_extent_op *extent_op,
-			       int for_cow);
+			       int no_quota);
 int btrfs_add_delayed_extent_op(struct btrfs_fs_info *fs_info,
 				struct btrfs_trans_handle *trans,
 				u64 bytenr, u64 num_bytes,
@@ -230,25 +231,6 @@ int btrfs_check_delayed_seq(struct btrfs_fs_info *fs_info,
 			    struct btrfs_delayed_ref_root *delayed_refs,
 			    u64 seq);
 
-/*
- * delayed refs with a ref_seq > 0 must be held back during backref walking.
- * this only applies to items in one of the fs-trees. for_cow items never need
- * to be held back, so they won't get a ref_seq number.
- */
-static inline int need_ref_seq(int for_cow, u64 rootid)
-{
-	if (for_cow)
-		return 0;
-
-	if (rootid == BTRFS_FS_TREE_OBJECTID)
-		return 1;
-
-	if ((s64)rootid >= (s64)BTRFS_FIRST_FREE_OBJECTID)
-		return 1;
-
-	return 0;
-}
-
 /*
  * a node might live in a head or a regular ref, this lets you
  * test for the proper type to use.

commit d7df2c796d7eedd72a334dc89c65e1fec8171431
Author: Josef Bacik <jbacik@fb.com>
Date:   Thu Jan 23 09:21:38 2014 -0500

    Btrfs: attach delayed ref updates to delayed ref heads
    
    Currently we have two rb-trees, one for delayed ref heads and one for all of the
    delayed refs, including the delayed ref heads.  When we process the delayed refs
    we have to hold onto the delayed ref lock for all of the selecting and merging
    and such, which results in quite a bit of lock contention.  This was solved by
    having a waitqueue and only one flusher at a time, however this hurts if we get
    a lot of delayed refs queued up.
    
    So instead just have an rb tree for the delayed ref heads, and then attach the
    delayed ref updates to an rb tree that is per delayed ref head.  Then we only
    need to take the delayed ref lock when adding new delayed refs and when
    selecting a delayed ref head to process, all the rest of the time we deal with a
    per delayed ref head lock which will be much less contentious.
    
    The locking rules for this get a little more complicated since we have to lock
    up to 3 things to properly process delayed refs, but I will address that problem
    later.  For now this passes all of xfstests and my overnight stress tests.
    Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index a54c9d47918f..4ba9b93022ff 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -81,7 +81,8 @@ struct btrfs_delayed_ref_head {
 	 */
 	struct mutex mutex;
 
-	struct list_head cluster;
+	spinlock_t lock;
+	struct rb_root ref_root;
 
 	struct rb_node href_node;
 
@@ -100,6 +101,7 @@ struct btrfs_delayed_ref_head {
 	 */
 	unsigned int must_insert_reserved:1;
 	unsigned int is_data:1;
+	unsigned int processing:1;
 };
 
 struct btrfs_delayed_tree_ref {
@@ -118,8 +120,6 @@ struct btrfs_delayed_data_ref {
 };
 
 struct btrfs_delayed_ref_root {
-	struct rb_root root;
-
 	/* head ref rbtree */
 	struct rb_root href_root;
 
@@ -129,7 +129,7 @@ struct btrfs_delayed_ref_root {
 	/* how many delayed ref updates we've queued, used by the
 	 * throttling code
 	 */
-	unsigned long num_entries;
+	atomic_t num_entries;
 
 	/* total number of head nodes in tree */
 	unsigned long num_heads;
@@ -137,15 +137,6 @@ struct btrfs_delayed_ref_root {
 	/* total number of head nodes ready for processing */
 	unsigned long num_heads_ready;
 
-	/*
-	 * bumped when someone is making progress on the delayed
-	 * refs, so that other procs know they are just adding to
-	 * contention intead of helping
-	 */
-	atomic_t procs_running_refs;
-	atomic_t ref_seq;
-	wait_queue_head_t wait;
-
 	/*
 	 * set when the tree is flushing before a transaction commit,
 	 * used by the throttling code to decide if new updates need
@@ -231,9 +222,9 @@ static inline void btrfs_delayed_ref_unlock(struct btrfs_delayed_ref_head *head)
 	mutex_unlock(&head->mutex);
 }
 
-int btrfs_find_ref_cluster(struct btrfs_trans_handle *trans,
-			   struct list_head *cluster, u64 search_start);
-void btrfs_release_ref_cluster(struct list_head *cluster);
+
+struct btrfs_delayed_ref_head *
+btrfs_select_ref_head(struct btrfs_trans_handle *trans);
 
 int btrfs_check_delayed_seq(struct btrfs_fs_info *fs_info,
 			    struct btrfs_delayed_ref_root *delayed_refs,

commit c46effa601f869f3d20a7386a745d9c002838eb8
Author: Liu Bo <bo.li.liu@oracle.com>
Date:   Mon Oct 14 12:59:45 2013 +0800

    Btrfs: introduce a head ref rbtree
    
    The way how we process delayed refs is
    1) get a bunch of head refs,
    2) pick up one head ref,
    3) go one node back for any delayed ref updates.
    
    The head ref is also linked in the same rbtree as the delayed ref is,
    so in 1) stage, we have to walk one by one including not only head refs, but
    delayed refs.
    
    When we have a great number of delayed refs pending to process,
    this'll cost time a lot.
    
    Here we introduce a head ref specific rbtree, it only has head refs, so troubles
    go away.
    
    Signed-off-by: Liu Bo <bo.li.liu@oracle.com>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index 70b962cc177d..a54c9d47918f 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -83,6 +83,8 @@ struct btrfs_delayed_ref_head {
 
 	struct list_head cluster;
 
+	struct rb_node href_node;
+
 	struct btrfs_delayed_extent_op *extent_op;
 	/*
 	 * when a new extent is allocated, it is just reserved in memory
@@ -118,6 +120,9 @@ struct btrfs_delayed_data_ref {
 struct btrfs_delayed_ref_root {
 	struct rb_root root;
 
+	/* head ref rbtree */
+	struct rb_root href_root;
+
 	/* this spin lock protects the rbtree and the entries inside */
 	spinlock_t lock;
 

commit b1c79e0947e0c190f865e2eb7b84a0fea0021cec
Author: Josef Bacik <jbacik@fusionio.com>
Date:   Thu May 9 13:49:30 2013 -0400

    Btrfs: handle running extent ops with skinny metadata
    
    Chris hit a bug where we weren't finding extent records when running extent ops.
    This is because we use the delayed_ref_head when running the extent op, which
    means we can't use the ->type checks to see if we are metadata.  We also lose
    the level of the metadata we are working on.  So to fix this we can just check
    the ->is_data section of the extent_op, and we can store the level of the buffer
    we were modifying in the extent_op.  Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index f75fcaf79aeb..70b962cc177d 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -60,6 +60,7 @@ struct btrfs_delayed_ref_node {
 struct btrfs_delayed_extent_op {
 	struct btrfs_disk_key key;
 	u64 flags_to_set;
+	int level;
 	unsigned int update_key:1;
 	unsigned int update_flags:1;
 	unsigned int is_data:1;

commit e942f883bc6651d50be139477baf6fb0eed3d5bb
Merge: b2c6b3e0611c 0e4e02636611
Author: Chris Mason <chris.mason@fusionio.com>
Date:   Wed Feb 20 14:06:05 2013 -0500

    Merge branch 'raid56-experimental' into for-linus-3.9
    
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>
    
    Conflicts:
            fs/btrfs/ctree.h
            fs/btrfs/extent-tree.c
            fs/btrfs/inode.c
            fs/btrfs/volumes.c

commit 093486c453a55230ccdad4b48863b872fe68c46e
Author: Miao Xie <miaox@cn.fujitsu.com>
Date:   Wed Dec 19 08:10:10 2012 +0000

    Btrfs: make delayed ref lock logic more readable
    
    Locking and unlocking delayed ref mutex are in the different functions,
    and the name of lock functions is not uniform, so the readability is not
    so good, this patch optimizes the lock logic and makes it more readable.
    
    Signed-off-by: Miao Xie <miaox@cn.fujitsu.com>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index fe50392cdf76..7939149f8f27 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -211,8 +211,14 @@ struct btrfs_delayed_ref_head *
 btrfs_find_delayed_ref_head(struct btrfs_trans_handle *trans, u64 bytenr);
 int btrfs_delayed_ref_lock(struct btrfs_trans_handle *trans,
 			   struct btrfs_delayed_ref_head *head);
+static inline void btrfs_delayed_ref_unlock(struct btrfs_delayed_ref_head *head)
+{
+	mutex_unlock(&head->mutex);
+}
+
 int btrfs_find_ref_cluster(struct btrfs_trans_handle *trans,
 			   struct list_head *cluster, u64 search_start);
+void btrfs_release_ref_cluster(struct list_head *cluster);
 
 int btrfs_check_delayed_seq(struct btrfs_fs_info *fs_info,
 			    struct btrfs_delayed_ref_root *delayed_refs,

commit 78a6184a3ff9041280ee56273c01e5679a831b39
Author: Miao Xie <miaox@cn.fujitsu.com>
Date:   Wed Nov 21 02:21:28 2012 +0000

    Btrfs: use slabs for delayed reference allocation
    
    The delayed reference allocation is in the fast path of the IO, so use slabs
    to improve the speed of the allocation.
    
    And besides that, it can do check for leaked objects when the module is removed.
    
    Signed-off-by: Miao Xie <miaox@cn.fujitsu.com>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index c9d703693df0..fe50392cdf76 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -141,12 +141,47 @@ struct btrfs_delayed_ref_root {
 	u64 run_delayed_start;
 };
 
+extern struct kmem_cache *btrfs_delayed_ref_head_cachep;
+extern struct kmem_cache *btrfs_delayed_tree_ref_cachep;
+extern struct kmem_cache *btrfs_delayed_data_ref_cachep;
+extern struct kmem_cache *btrfs_delayed_extent_op_cachep;
+
+int btrfs_delayed_ref_init(void);
+void btrfs_delayed_ref_exit(void);
+
+static inline struct btrfs_delayed_extent_op *
+btrfs_alloc_delayed_extent_op(void)
+{
+	return kmem_cache_alloc(btrfs_delayed_extent_op_cachep, GFP_NOFS);
+}
+
+static inline void
+btrfs_free_delayed_extent_op(struct btrfs_delayed_extent_op *op)
+{
+	if (op)
+		kmem_cache_free(btrfs_delayed_extent_op_cachep, op);
+}
+
 static inline void btrfs_put_delayed_ref(struct btrfs_delayed_ref_node *ref)
 {
 	WARN_ON(atomic_read(&ref->refs) == 0);
 	if (atomic_dec_and_test(&ref->refs)) {
 		WARN_ON(ref->in_tree);
-		kfree(ref);
+		switch (ref->type) {
+		case BTRFS_TREE_BLOCK_REF_KEY:
+		case BTRFS_SHARED_BLOCK_REF_KEY:
+			kmem_cache_free(btrfs_delayed_tree_ref_cachep, ref);
+			break;
+		case BTRFS_EXTENT_DATA_REF_KEY:
+		case BTRFS_SHARED_DATA_REF_KEY:
+			kmem_cache_free(btrfs_delayed_data_ref_cachep, ref);
+			break;
+		case 0:
+			kmem_cache_free(btrfs_delayed_ref_head_cachep, ref);
+			break;
+		default:
+			BUG();
+		}
 	}
 }
 

commit bb721703aa551e98dc5c7fb259cf90343408baf2
Author: Chris Mason <chris.mason@fusionio.com>
Date:   Tue Jan 29 18:44:12 2013 -0500

    Btrfs: reduce CPU contention while waiting for delayed extent operations
    
    We batch up operations to the extent allocation tree, which allows
    us to deal with the recursive nature of using the extent allocation
    tree to allocate extents to the extent allocation tree.
    
    It also provides a mechanism to sort and collect extent
    operations, which makes it much more efficient to record extents
    that are close together.
    
    The delayed extent operations must all be finished before the
    running transaction commits, so we have code to make sure and run a few
    of the batched operations when closing our transaction handles.
    
    This creates a great deal of contention for the locks in the
    delayed extent operation tree, and also contention for the lock on the
    extent allocation tree itself.  All the extra contention just slows
    down the operations and doesn't get things done any faster.
    
    This commit changes things to use a wait queue instead.  As procs
    want to run the delayed operations, one of them races in and gets
    permission to hit the tree, and the others step back and wait for
    progress to be made.
    
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index c9d703693df0..23bdeb8502a6 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -131,6 +131,15 @@ struct btrfs_delayed_ref_root {
 	/* total number of head nodes ready for processing */
 	unsigned long num_heads_ready;
 
+	/*
+	 * bumped when someone is making progress on the delayed
+	 * refs, so that other procs know they are just adding to
+	 * contention intead of helping
+	 */
+	atomic_t procs_running_refs;
+	atomic_t ref_seq;
+	wait_queue_head_t wait;
+
 	/*
 	 * set when the tree is flushing before a transaction commit,
 	 * used by the throttling code to decide if new updates need

commit 99dbb1632f1165c2726056ebfce6edde0e5a0208
Merge: aae6f989c6e9 9c33c512b2d3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 1 09:06:36 2012 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/trivial
    
    Pull the trivial tree from Jiri Kosina:
     "Tiny usual fixes all over the place"
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/trivial: (34 commits)
      doc: fix old config name of kprobetrace
      fs/fs-writeback.c: cleanup riteback_sb_inodes kerneldoc
      btrfs: fix the commment for the action flags in delayed-ref.h
      btrfs: fix trivial typo for the comment of BTRFS_FREE_INO_OBJECTID
      vfs: fix kerneldoc for generic_fh_to_parent()
      treewide: fix comment/printk/variable typos
      ipr: fix small coding style issues
      doc: fix broken utf8 encoding
      nfs: comment fix
      platform/x86: fix asus_laptop.wled_type module parameter
      mfd: printk/comment fixes
      doc: getdelays.c: remember to close() socket on error in create_nl_socket()
      doc: aliasing-test: close fd on write error
      mmc: fix comment typos
      dma: fix comments
      spi: fix comment/printk typos in spi
      Coccinelle: fix typo in memdup_user.cocci
      tmiofb: missing NULL pointer checks
      tools: perf: Fix typo in tools/perf
      tools/testing: fix comment / output typos
      ...

commit 44a075bde9ab32d13bb2ff89f3e72fd869f272c4
Author: Wang Sheng-Hui <shhuiw@gmail.com>
Date:   Fri Sep 21 08:21:01 2012 +0800

    btrfs: fix the commment for the action flags in delayed-ref.h
    
    The action field has been merged into struct btrfs_delayed_ref_node,
    and no struct btrfs_delayed_ref is available now.
    
    Signed-off-by: Wang Sheng-Hui <shhuiw@gmail.com>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index 413927fb9957..25589456d1c9 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -18,7 +18,7 @@
 #ifndef __DELAYED_REF__
 #define __DELAYED_REF__
 
-/* these are the possible values of struct btrfs_delayed_ref->action */
+/* these are the possible values of struct btrfs_delayed_ref_node->action */
 #define BTRFS_ADD_DELAYED_REF    1 /* add one backref to the tree */
 #define BTRFS_DROP_DELAYED_REF   2 /* delete one backref from the tree */
 #define BTRFS_ADD_DELAYED_EXTENT 3 /* record a full extent allocation */

commit ae1e206b806ccc490dadff59af8a7a2477b32884
Author: Josef Bacik <jbacik@fusionio.com>
Date:   Tue Aug 7 16:00:32 2012 -0400

    Btrfs: allow delayed refs to be merged
    
    Daniel Blueman reported a bug with fio+balance on a ramdisk setup.
    Basically what happens is the balance relocates a tree block which will drop
    the implicit refs for all of its children and adds a full backref.  Once the
    block is relocated we have to add the implicit refs back, so when we cow the
    block again we add the implicit refs for its children back.  The problem
    comes when the original drop ref doesn't get run before we add the implicit
    refs back.  The delayed ref stuff will specifically prefer ADD operations
    over DROP to keep us from freeing up an extent that will have references to
    it, so we try to add the implicit ref before it is actually removed and we
    panic.  This worked fine before because the add would have just canceled the
    drop out and we would have been fine.  But the backref walking work needs to
    be able to freeze the delayed ref stuff in time so we have this ever
    increasing sequence number that gets attached to all new delayed ref updates
    which makes us not merge refs and we run into this issue.
    
    So to fix this we need to merge delayed refs.  So everytime we run a
    clustered ref we need to try and merge all of its delayed refs.  The backref
    walking stuff locks the delayed ref head before processing, so if we have it
    locked we are safe to merge any refs inside of the sequence number.  If
    there is no sequence number we can merge all refs.  Doing this not only
    fixes our bug but keeps the delayed ref code from adding and removing
    useless refs and batching together multiple refs into one search instead of
    one search per delayed ref, which will really help our commit times.  I ran
    this with Daniels test and 276 and I haven't seen any problems.  Thanks,
    
    Reported-by: Daniel J Blueman <daniel@quora.org>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index 0d7c90c366b6..ab5300595847 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -167,6 +167,10 @@ int btrfs_add_delayed_extent_op(struct btrfs_fs_info *fs_info,
 				struct btrfs_trans_handle *trans,
 				u64 bytenr, u64 num_bytes,
 				struct btrfs_delayed_extent_op *extent_op);
+void btrfs_merge_delayed_refs(struct btrfs_trans_handle *trans,
+			      struct btrfs_fs_info *fs_info,
+			      struct btrfs_delayed_ref_root *delayed_refs,
+			      struct btrfs_delayed_ref_head *head);
 
 struct btrfs_delayed_ref_head *
 btrfs_find_delayed_ref_head(struct btrfs_trans_handle *trans, u64 bytenr);

commit 546adb0d817c34dc2be3a7cb5bba8771f837a562
Author: Jan Schmidt <list.btrfs@jan-o-sch.net>
Date:   Thu Jun 14 16:37:44 2012 +0200

    Btrfs: hooks for qgroup to record delayed refs
    
    Hooks into qgroup code to record refs and into transaction commit.
    This is the main entry point for qgroup. Basically every change in
    extent backrefs got accounted to the appropriate qgroups.
    
    Signed-off-by: Arne Jansen <sensille@gmx.net>
    Signed-off-by: Jan Schmidt <list.btrfs@jan-o-sch.net>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index 2b5cb27f9861..0d7c90c366b6 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -179,6 +179,25 @@ int btrfs_check_delayed_seq(struct btrfs_fs_info *fs_info,
 			    struct btrfs_delayed_ref_root *delayed_refs,
 			    u64 seq);
 
+/*
+ * delayed refs with a ref_seq > 0 must be held back during backref walking.
+ * this only applies to items in one of the fs-trees. for_cow items never need
+ * to be held back, so they won't get a ref_seq number.
+ */
+static inline int need_ref_seq(int for_cow, u64 rootid)
+{
+	if (for_cow)
+		return 0;
+
+	if (rootid == BTRFS_FS_TREE_OBJECTID)
+		return 1;
+
+	if ((s64)rootid >= (s64)BTRFS_FIRST_FREE_OBJECTID)
+		return 1;
+
+	return 0;
+}
+
 /*
  * a node might live in a head or a regular ref, this lets you
  * test for the proper type to use.

commit 097b8a7c9e48e2cb50fd0eb9315791921beaf484
Author: Jan Schmidt <list.btrfs@jan-o-sch.net>
Date:   Thu Jun 21 11:08:04 2012 +0200

    Btrfs: join tree mod log code with the code holding back delayed refs
    
    We've got two mechanisms both required for reliable backref resolving (tree
    mod log and holding back delayed refs). You cannot make use of one without
    the other. So instead of requiring the user of this mechanism to setup both
    correctly, we join them into a single interface.
    
    Additionally, we stop inserting non-blockers into fs_info->tree_mod_seq_list
    as we did before, which was of no value.
    
    Signed-off-by: Jan Schmidt <list.btrfs@jan-o-sch.net>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index 413927fb9957..2b5cb27f9861 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -139,26 +139,6 @@ struct btrfs_delayed_ref_root {
 	int flushing;
 
 	u64 run_delayed_start;
-
-	/*
-	 * seq number of delayed refs. We need to know if a backref was being
-	 * added before the currently processed ref or afterwards.
-	 */
-	u64 seq;
-
-	/*
-	 * seq_list holds a list of all seq numbers that are currently being
-	 * added to the list. While walking backrefs (btrfs_find_all_roots,
-	 * qgroups), which might take some time, no newer ref must be processed,
-	 * as it might influence the outcome of the walk.
-	 */
-	struct list_head seq_head;
-
-	/*
-	 * when the only refs we have in the list must not be processed, we want
-	 * to wait for more refs to show up or for the end of backref walking.
-	 */
-	wait_queue_head_t seq_wait;
 };
 
 static inline void btrfs_put_delayed_ref(struct btrfs_delayed_ref_node *ref)
@@ -195,33 +175,8 @@ int btrfs_delayed_ref_lock(struct btrfs_trans_handle *trans,
 int btrfs_find_ref_cluster(struct btrfs_trans_handle *trans,
 			   struct list_head *cluster, u64 search_start);
 
-static inline u64 inc_delayed_seq(struct btrfs_delayed_ref_root *delayed_refs)
-{
-	assert_spin_locked(&delayed_refs->lock);
-	++delayed_refs->seq;
-	return delayed_refs->seq;
-}
-
-static inline void
-btrfs_get_delayed_seq(struct btrfs_delayed_ref_root *delayed_refs,
-		      struct seq_list *elem)
-{
-	assert_spin_locked(&delayed_refs->lock);
-	elem->seq = delayed_refs->seq;
-	list_add_tail(&elem->list, &delayed_refs->seq_head);
-}
-
-static inline void
-btrfs_put_delayed_seq(struct btrfs_delayed_ref_root *delayed_refs,
-		      struct seq_list *elem)
-{
-	spin_lock(&delayed_refs->lock);
-	list_del(&elem->list);
-	wake_up(&delayed_refs->seq_wait);
-	spin_unlock(&delayed_refs->lock);
-}
-
-int btrfs_check_delayed_seq(struct btrfs_delayed_ref_root *delayed_refs,
+int btrfs_check_delayed_seq(struct btrfs_fs_info *fs_info,
+			    struct btrfs_delayed_ref_root *delayed_refs,
 			    u64 seq);
 
 /*

commit 95a06077f7edbd00d32612562be4d857a5b7df54
Author: Jan Schmidt <list.btrfs@jan-o-sch.net>
Date:   Tue May 29 17:06:54 2012 +0200

    Btrfs: use delayed ref sequence numbers for all fs-tree updates
    
    The sequence number for delayed refs is needed to postpone certain delayed
    refs for a very short period while walking backrefs. Before the tree
    modification log, we thought we'd only have to hold back those references
    that don't have a counter operation.
    
    While now we've the tree mod log, we're rewinding fs tree blocks to a
    defined consistent state. We cannot know in advance for which tree block
    we'll be doing rewind operations later. Therefore, we must postpone all the
    delayed refs for fs-tree blocks, even those having a counter operation.
    
    Signed-off-by: Jan Schmidt <list.btrfs@jan-o-sch.net>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index fd8244670212..413927fb9957 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -224,25 +224,6 @@ btrfs_put_delayed_seq(struct btrfs_delayed_ref_root *delayed_refs,
 int btrfs_check_delayed_seq(struct btrfs_delayed_ref_root *delayed_refs,
 			    u64 seq);
 
-/*
- * delayed refs with a ref_seq > 0 must be held back during backref walking.
- * this only applies to items in one of the fs-trees. for_cow items never need
- * to be held back, so they won't get a ref_seq number.
- */
-static inline int need_ref_seq(int for_cow, u64 rootid)
-{
-	if (for_cow)
-		return 0;
-
-	if (rootid == BTRFS_FS_TREE_OBJECTID)
-		return 1;
-
-	if ((s64)rootid >= (s64)BTRFS_FIRST_FREE_OBJECTID)
-		return 1;
-
-	return 0;
-}
-
 /*
  * a node might live in a head or a regular ref, this lets you
  * test for the proper type to use.

commit 64947ec0d16dd20d6542b58cf82c8d5f9678cabf
Author: Jan Schmidt <list.btrfs@jan-o-sch.net>
Date:   Wed May 16 16:57:09 2012 +0200

    Btrfs: move struct seq_list to ctree.h
    
    Signed-off-by: Jan Schmidt <list.btrfs@jan-o-sch.net>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index d8f244d94925..fd8244670212 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -195,11 +195,6 @@ int btrfs_delayed_ref_lock(struct btrfs_trans_handle *trans,
 int btrfs_find_ref_cluster(struct btrfs_trans_handle *trans,
 			   struct list_head *cluster, u64 search_start);
 
-struct seq_list {
-	struct list_head list;
-	u64 seq;
-};
-
 static inline u64 inc_delayed_seq(struct btrfs_delayed_ref_root *delayed_refs)
 {
 	assert_spin_locked(&delayed_refs->lock);

commit a168650c08300434e1456abe7b6451f1448230d3
Author: Jan Schmidt <list.btrfs@jan-o-sch.net>
Date:   Mon Dec 12 16:10:07 2011 +0100

    Btrfs: add waitqueue instead of doing busy waiting for more delayed refs
    
    Now that we may be holding back delayed refs for a limited period, we
    might end up having no runnable delayed refs. Without this commit, we'd
    do busy waiting in that thread until another (runnable) ref arives.
    Instead, we're detecting this situation and use a waitqueue, such that
    we only try to run more refs after
            a) another runnable ref was added  or
            b) delayed refs are no longer held back
    
    Signed-off-by: Jan Schmidt <list.btrfs@jan-o-sch.net>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index 174416f7882b..d8f244d94925 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -153,6 +153,12 @@ struct btrfs_delayed_ref_root {
 	 * as it might influence the outcome of the walk.
 	 */
 	struct list_head seq_head;
+
+	/*
+	 * when the only refs we have in the list must not be processed, we want
+	 * to wait for more refs to show up or for the end of backref walking.
+	 */
+	wait_queue_head_t seq_wait;
 };
 
 static inline void btrfs_put_delayed_ref(struct btrfs_delayed_ref_node *ref)
@@ -216,6 +222,7 @@ btrfs_put_delayed_seq(struct btrfs_delayed_ref_root *delayed_refs,
 {
 	spin_lock(&delayed_refs->lock);
 	list_del(&elem->list);
+	wake_up(&delayed_refs->seq_wait);
 	spin_unlock(&delayed_refs->lock);
 }
 

commit 00f04b88791ff49dc64ada18819d40a5b0671709
Author: Arne Jansen <sensille@gmx.net>
Date:   Wed Sep 14 12:37:00 2011 +0200

    Btrfs: add sequence numbers to delayed refs
    
    Sequence numbers are needed to reconstruct the backrefs of a given extent to
    a certain point in time. The total set of backrefs consist of the set of
    backrefs recorded on disk plus the enqueued delayed refs for it that existed
    at that moment.
    
    This patch also adds a list that records all delayed refs which are
    currently in the process of being added.
    
    When walking all refs of an extent in btrfs_find_all_roots(), we freeze the
    current state of delayed refs, honor anythinh up to this point and prevent
    processing newer delayed refs to assert consistency.
    
    Signed-off-by: Arne Jansen <sensille@gmx.net>
    Signed-off-by: Jan Schmidt <list.btrfs@jan-o-sch.net>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index a5fb2bc83732..174416f7882b 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -33,6 +33,9 @@ struct btrfs_delayed_ref_node {
 	/* the size of the extent */
 	u64 num_bytes;
 
+	/* seq number to keep track of insertion order */
+	u64 seq;
+
 	/* ref count on this data structure */
 	atomic_t refs;
 
@@ -136,6 +139,20 @@ struct btrfs_delayed_ref_root {
 	int flushing;
 
 	u64 run_delayed_start;
+
+	/*
+	 * seq number of delayed refs. We need to know if a backref was being
+	 * added before the currently processed ref or afterwards.
+	 */
+	u64 seq;
+
+	/*
+	 * seq_list holds a list of all seq numbers that are currently being
+	 * added to the list. While walking backrefs (btrfs_find_all_roots,
+	 * qgroups), which might take some time, no newer ref must be processed,
+	 * as it might influence the outcome of the walk.
+	 */
+	struct list_head seq_head;
 };
 
 static inline void btrfs_put_delayed_ref(struct btrfs_delayed_ref_node *ref)
@@ -171,6 +188,59 @@ int btrfs_delayed_ref_lock(struct btrfs_trans_handle *trans,
 			   struct btrfs_delayed_ref_head *head);
 int btrfs_find_ref_cluster(struct btrfs_trans_handle *trans,
 			   struct list_head *cluster, u64 search_start);
+
+struct seq_list {
+	struct list_head list;
+	u64 seq;
+};
+
+static inline u64 inc_delayed_seq(struct btrfs_delayed_ref_root *delayed_refs)
+{
+	assert_spin_locked(&delayed_refs->lock);
+	++delayed_refs->seq;
+	return delayed_refs->seq;
+}
+
+static inline void
+btrfs_get_delayed_seq(struct btrfs_delayed_ref_root *delayed_refs,
+		      struct seq_list *elem)
+{
+	assert_spin_locked(&delayed_refs->lock);
+	elem->seq = delayed_refs->seq;
+	list_add_tail(&elem->list, &delayed_refs->seq_head);
+}
+
+static inline void
+btrfs_put_delayed_seq(struct btrfs_delayed_ref_root *delayed_refs,
+		      struct seq_list *elem)
+{
+	spin_lock(&delayed_refs->lock);
+	list_del(&elem->list);
+	spin_unlock(&delayed_refs->lock);
+}
+
+int btrfs_check_delayed_seq(struct btrfs_delayed_ref_root *delayed_refs,
+			    u64 seq);
+
+/*
+ * delayed refs with a ref_seq > 0 must be held back during backref walking.
+ * this only applies to items in one of the fs-trees. for_cow items never need
+ * to be held back, so they won't get a ref_seq number.
+ */
+static inline int need_ref_seq(int for_cow, u64 rootid)
+{
+	if (for_cow)
+		return 0;
+
+	if (rootid == BTRFS_FS_TREE_OBJECTID)
+		return 1;
+
+	if ((s64)rootid >= (s64)BTRFS_FIRST_FREE_OBJECTID)
+		return 1;
+
+	return 0;
+}
+
 /*
  * a node might live in a head or a regular ref, this lets you
  * test for the proper type to use.

commit eebe063b7f916087cd5c61de57b20a3a30894a96
Author: Arne Jansen <sensille@gmx.net>
Date:   Wed Sep 14 14:01:24 2011 +0200

    Btrfs: always save ref_root in delayed refs
    
    For consistent backref walking and (later) qgroup calculation the
    information to which root a delayed ref belongs is useful even for shared
    refs.
    
    Signed-off-by: Arne Jansen <sensille@gmx.net>
    Signed-off-by: Jan Schmidt <list.btrfs@jan-o-sch.net>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index 8316bff18d30..a5fb2bc83732 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -98,19 +98,15 @@ struct btrfs_delayed_ref_head {
 
 struct btrfs_delayed_tree_ref {
 	struct btrfs_delayed_ref_node node;
-	union {
-		u64 root;
-		u64 parent;
-	};
+	u64 root;
+	u64 parent;
 	int level;
 };
 
 struct btrfs_delayed_data_ref {
 	struct btrfs_delayed_ref_node node;
-	union {
-		u64 root;
-		u64 parent;
-	};
+	u64 root;
+	u64 parent;
 	u64 objectid;
 	u64 offset;
 };

commit 66d7e7f09f77456fe68683247d77721032a00ee5
Author: Arne Jansen <sensille@gmx.net>
Date:   Mon Sep 12 15:26:38 2011 +0200

    Btrfs: mark delayed refs as for cow
    
    Add a for_cow parameter to add_delayed_*_ref and pass the appropriate value
    from every call site. The for_cow parameter will later on be used to
    determine if a ref will change anything with respect to qgroups.
    
    Delayed refs coming from relocation are always counted as for_cow, as they
    don't change subvol quota.
    
    Also pass in the fs_info for later use.
    
    btrfs_find_all_roots() will use this as an optimization, as changes that are
    for_cow will not change anything with respect to which root points to a
    certain leaf. Thus, we don't need to add the current sequence number to
    those delayed refs.
    
    Signed-off-by: Arne Jansen <sensille@gmx.net>
    Signed-off-by: Jan Schmidt <list.btrfs@jan-o-sch.net>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index e287e3b0eab0..8316bff18d30 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -151,16 +151,21 @@ static inline void btrfs_put_delayed_ref(struct btrfs_delayed_ref_node *ref)
 	}
 }
 
-int btrfs_add_delayed_tree_ref(struct btrfs_trans_handle *trans,
+int btrfs_add_delayed_tree_ref(struct btrfs_fs_info *fs_info,
+			       struct btrfs_trans_handle *trans,
 			       u64 bytenr, u64 num_bytes, u64 parent,
 			       u64 ref_root, int level, int action,
-			       struct btrfs_delayed_extent_op *extent_op);
-int btrfs_add_delayed_data_ref(struct btrfs_trans_handle *trans,
+			       struct btrfs_delayed_extent_op *extent_op,
+			       int for_cow);
+int btrfs_add_delayed_data_ref(struct btrfs_fs_info *fs_info,
+			       struct btrfs_trans_handle *trans,
 			       u64 bytenr, u64 num_bytes,
 			       u64 parent, u64 ref_root,
 			       u64 owner, u64 offset, int action,
-			       struct btrfs_delayed_extent_op *extent_op);
-int btrfs_add_delayed_extent_op(struct btrfs_trans_handle *trans,
+			       struct btrfs_delayed_extent_op *extent_op,
+			       int for_cow);
+int btrfs_add_delayed_extent_op(struct btrfs_fs_info *fs_info,
+				struct btrfs_trans_handle *trans,
 				u64 bytenr, u64 num_bytes,
 				struct btrfs_delayed_extent_op *extent_op);
 

commit f2a97a9dbd86eb1ef956bdf20e05c507b32beb96
Author: David Sterba <dsterba@suse.cz>
Date:   Thu May 5 12:44:41 2011 +0200

    btrfs: remove all unused functions
    
    Remove static and global declarations and/or definitions. Reduces size
    of btrfs.ko by ~3.4kB.
    
      text    data     bss     dec     hex filename
    402081    7464     200  409745   64091 btrfs.ko.base
    398620    7144     200  405964   631cc btrfs.ko.remove-all
    
    Signed-off-by: David Sterba <dsterba@suse.cz>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index 946ed71ab84f..e287e3b0eab0 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -166,7 +166,6 @@ int btrfs_add_delayed_extent_op(struct btrfs_trans_handle *trans,
 
 struct btrfs_delayed_ref_head *
 btrfs_find_delayed_ref_head(struct btrfs_trans_handle *trans, u64 bytenr);
-int btrfs_delayed_ref_pending(struct btrfs_trans_handle *trans, u64 bytenr);
 int btrfs_delayed_ref_lock(struct btrfs_trans_handle *trans,
 			   struct btrfs_delayed_ref_head *head);
 int btrfs_find_ref_cluster(struct btrfs_trans_handle *trans,

commit 621496f4fd56195b7b273521f467c2945165481f
Author: David Sterba <dsterba@suse.cz>
Date:   Wed May 4 12:56:49 2011 +0200

    btrfs: remove unused function prototypes
    
    function prototypes without a body
    
    Signed-off-by: David Sterba <dsterba@suse.cz>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index 50e3cf92fbda..946ed71ab84f 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -167,11 +167,6 @@ int btrfs_add_delayed_extent_op(struct btrfs_trans_handle *trans,
 struct btrfs_delayed_ref_head *
 btrfs_find_delayed_ref_head(struct btrfs_trans_handle *trans, u64 bytenr);
 int btrfs_delayed_ref_pending(struct btrfs_trans_handle *trans, u64 bytenr);
-int btrfs_update_delayed_ref(struct btrfs_trans_handle *trans,
-			  u64 bytenr, u64 num_bytes, u64 orig_parent,
-			  u64 parent, u64 orig_ref_root, u64 ref_root,
-			  u64 orig_ref_generation, u64 ref_generation,
-			  u64 owner_objectid, int pin);
 int btrfs_delayed_ref_lock(struct btrfs_trans_handle *trans,
 			   struct btrfs_delayed_ref_head *head);
 int btrfs_find_ref_cluster(struct btrfs_trans_handle *trans,

commit a22285a6a32390195235171b89d157ed1a1fe932
Author: Yan, Zheng <zheng.yan@oracle.com>
Date:   Sun May 16 10:48:46 2010 -0400

    Btrfs: Integrate metadata reservation with start_transaction
    
    Besides simplify the code, this change makes sure all metadata
    reservation for normal metadata operations are released after
    committing transaction.
    
    Changes since V1:
    
    Add code that check if unlink and rmdir will free space.
    
    Add ENOSPC handling for clone ioctl.
    
    Signed-off-by: Yan Zheng <zheng.yan@oracle.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index f6fc67ddad36..50e3cf92fbda 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -167,9 +167,6 @@ int btrfs_add_delayed_extent_op(struct btrfs_trans_handle *trans,
 struct btrfs_delayed_ref_head *
 btrfs_find_delayed_ref_head(struct btrfs_trans_handle *trans, u64 bytenr);
 int btrfs_delayed_ref_pending(struct btrfs_trans_handle *trans, u64 bytenr);
-int btrfs_lookup_extent_info(struct btrfs_trans_handle *trans,
-			     struct btrfs_root *root, u64 bytenr,
-			     u64 num_bytes, u64 *refs, u64 *flags);
 int btrfs_update_delayed_ref(struct btrfs_trans_handle *trans,
 			  u64 bytenr, u64 num_bytes, u64 orig_parent,
 			  u64 parent, u64 orig_ref_root, u64 ref_root,

commit 5d4f98a28c7d334091c1b7744f48a1acdd2a4ae0
Author: Yan Zheng <zheng.yan@oracle.com>
Date:   Wed Jun 10 10:45:14 2009 -0400

    Btrfs: Mixed back reference  (FORWARD ROLLING FORMAT CHANGE)
    
    This commit introduces a new kind of back reference for btrfs metadata.
    Once a filesystem has been mounted with this commit, IT WILL NO LONGER
    BE MOUNTABLE BY OLDER KERNELS.
    
    When a tree block in subvolume tree is cow'd, the reference counts of all
    extents it points to are increased by one.  At transaction commit time,
    the old root of the subvolume is recorded in a "dead root" data structure,
    and the btree it points to is later walked, dropping reference counts
    and freeing any blocks where the reference count goes to 0.
    
    The increments done during cow and decrements done after commit cancel out,
    and the walk is a very expensive way to go about freeing the blocks that
    are no longer referenced by the new btree root.  This commit reduces the
    transaction overhead by avoiding the need for dead root records.
    
    When a non-shared tree block is cow'd, we free the old block at once, and the
    new block inherits old block's references. When a tree block with reference
    count > 1 is cow'd, we increase the reference counts of all extents
    the new block points to by one, and decrease the old block's reference count by
    one.
    
    This dead tree avoidance code removes the need to modify the reference
    counts of lower level extents when a non-shared tree block is cow'd.
    But we still need to update back ref for all pointers in the block.
    This is because the location of the block is recorded in the back ref
    item.
    
    We can solve this by introducing a new type of back ref. The new
    back ref provides information about pointer's key, level and in which
    tree the pointer lives. This information allow us to find the pointer
    by searching the tree. The shortcoming of the new back ref is that it
    only works for pointers in tree blocks referenced by their owner trees.
    
    This is mostly a problem for snapshots, where resolving one of these
    fuzzy back references would be O(number_of_snapshots) and quite slow.
    The solution used here is to use the fuzzy back references in the common
    case where a given tree block is only referenced by one root,
    and use the full back references when multiple roots have a reference
    on a given block.
    
    This commit adds per subvolume red-black tree to keep trace of cached
    inodes. The red-black tree helps the balancing code to find cached
    inodes whose inode numbers within a given range.
    
    This commit improves the balancing code by introducing several data
    structures to keep the state of balancing. The most important one
    is the back ref cache. It caches how the upper level tree blocks are
    referenced. This greatly reduce the overhead of checking back ref.
    
    The improved balancing code scales significantly better with a large
    number of snapshots.
    
    This is a very large commit and was written in a number of
    pieces.  But, they depend heavily on the disk format change and were
    squashed together to make sure git bisect didn't end up in a
    bad state wrt space balancing or the format change.
    
    Signed-off-by: Yan Zheng <zheng.yan@oracle.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index 3bec2ff0b15c..f6fc67ddad36 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -30,9 +30,6 @@ struct btrfs_delayed_ref_node {
 	/* the starting bytenr of the extent */
 	u64 bytenr;
 
-	/* the parent our backref will point to */
-	u64 parent;
-
 	/* the size of the extent */
 	u64 num_bytes;
 
@@ -50,10 +47,21 @@ struct btrfs_delayed_ref_node {
 	 */
 	int ref_mod;
 
+	unsigned int action:8;
+	unsigned int type:8;
 	/* is this node still in the rbtree? */
+	unsigned int is_head:1;
 	unsigned int in_tree:1;
 };
 
+struct btrfs_delayed_extent_op {
+	struct btrfs_disk_key key;
+	u64 flags_to_set;
+	unsigned int update_key:1;
+	unsigned int update_flags:1;
+	unsigned int is_data:1;
+};
+
 /*
  * the head refs are used to hold a lock on a given extent, which allows us
  * to make sure that only one process is running the delayed refs
@@ -71,6 +79,7 @@ struct btrfs_delayed_ref_head {
 
 	struct list_head cluster;
 
+	struct btrfs_delayed_extent_op *extent_op;
 	/*
 	 * when a new extent is allocated, it is just reserved in memory
 	 * The actual extent isn't inserted into the extent allocation tree
@@ -84,27 +93,26 @@ struct btrfs_delayed_ref_head {
 	 * the free has happened.
 	 */
 	unsigned int must_insert_reserved:1;
+	unsigned int is_data:1;
 };
 
-struct btrfs_delayed_ref {
+struct btrfs_delayed_tree_ref {
 	struct btrfs_delayed_ref_node node;
+	union {
+		u64 root;
+		u64 parent;
+	};
+	int level;
+};
 
-	/* the root objectid our ref will point to */
-	u64 root;
-
-	/* the generation for the backref */
-	u64 generation;
-
-	/* owner_objectid of the backref  */
-	u64 owner_objectid;
-
-	/* operation done by this entry in the rbtree */
-	u8 action;
-
-	/* if pin == 1, when the extent is freed it will be pinned until
-	 * transaction commit
-	 */
-	unsigned int pin:1;
+struct btrfs_delayed_data_ref {
+	struct btrfs_delayed_ref_node node;
+	union {
+		u64 root;
+		u64 parent;
+	};
+	u64 objectid;
+	u64 offset;
 };
 
 struct btrfs_delayed_ref_root {
@@ -143,17 +151,25 @@ static inline void btrfs_put_delayed_ref(struct btrfs_delayed_ref_node *ref)
 	}
 }
 
-int btrfs_add_delayed_ref(struct btrfs_trans_handle *trans,
-			  u64 bytenr, u64 num_bytes, u64 parent, u64 ref_root,
-			  u64 ref_generation, u64 owner_objectid, int action,
-			  int pin);
+int btrfs_add_delayed_tree_ref(struct btrfs_trans_handle *trans,
+			       u64 bytenr, u64 num_bytes, u64 parent,
+			       u64 ref_root, int level, int action,
+			       struct btrfs_delayed_extent_op *extent_op);
+int btrfs_add_delayed_data_ref(struct btrfs_trans_handle *trans,
+			       u64 bytenr, u64 num_bytes,
+			       u64 parent, u64 ref_root,
+			       u64 owner, u64 offset, int action,
+			       struct btrfs_delayed_extent_op *extent_op);
+int btrfs_add_delayed_extent_op(struct btrfs_trans_handle *trans,
+				u64 bytenr, u64 num_bytes,
+				struct btrfs_delayed_extent_op *extent_op);
 
 struct btrfs_delayed_ref_head *
 btrfs_find_delayed_ref_head(struct btrfs_trans_handle *trans, u64 bytenr);
 int btrfs_delayed_ref_pending(struct btrfs_trans_handle *trans, u64 bytenr);
-int btrfs_lookup_extent_ref(struct btrfs_trans_handle *trans,
-			    struct btrfs_root *root, u64 bytenr,
-			    u64 num_bytes, u32 *refs);
+int btrfs_lookup_extent_info(struct btrfs_trans_handle *trans,
+			     struct btrfs_root *root, u64 bytenr,
+			     u64 num_bytes, u64 *refs, u64 *flags);
 int btrfs_update_delayed_ref(struct btrfs_trans_handle *trans,
 			  u64 bytenr, u64 num_bytes, u64 orig_parent,
 			  u64 parent, u64 orig_ref_root, u64 ref_root,
@@ -169,18 +185,24 @@ int btrfs_find_ref_cluster(struct btrfs_trans_handle *trans,
  */
 static int btrfs_delayed_ref_is_head(struct btrfs_delayed_ref_node *node)
 {
-	return node->parent == (u64)-1;
+	return node->is_head;
 }
 
 /*
  * helper functions to cast a node into its container
  */
-static inline struct btrfs_delayed_ref *
-btrfs_delayed_node_to_ref(struct btrfs_delayed_ref_node *node)
+static inline struct btrfs_delayed_tree_ref *
+btrfs_delayed_node_to_tree_ref(struct btrfs_delayed_ref_node *node)
 {
 	WARN_ON(btrfs_delayed_ref_is_head(node));
-	return container_of(node, struct btrfs_delayed_ref, node);
+	return container_of(node, struct btrfs_delayed_tree_ref, node);
+}
 
+static inline struct btrfs_delayed_data_ref *
+btrfs_delayed_node_to_data_ref(struct btrfs_delayed_ref_node *node)
+{
+	WARN_ON(btrfs_delayed_ref_is_head(node));
+	return container_of(node, struct btrfs_delayed_data_ref, node);
 }
 
 static inline struct btrfs_delayed_ref_head *
@@ -188,6 +210,5 @@ btrfs_delayed_node_to_head(struct btrfs_delayed_ref_node *node)
 {
 	WARN_ON(!btrfs_delayed_ref_is_head(node));
 	return container_of(node, struct btrfs_delayed_ref_head, node);
-
 }
 #endif

commit 1a81af4d1d9c60d4313309f937a1fc5567205a87
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Mar 25 09:55:11 2009 -0400

    Btrfs: make sure btrfs_update_delayed_ref doesn't increase ref_mod
    
    btrfs_update_delayed_ref is optimized to add and remove different
    references in one pass through the delayed ref tree.  It is a zero
    sum on the total number of refs on a given extent.
    
    But, the code was recording an extra ref in the head node.  This
    never made it down to the disk but was used when deciding if it was
    safe to free the extent while dropping snapshots.
    
    The fix used here is to make sure the ref_mod count is unchanged
    on the head ref when btrfs_update_delayed_ref is called.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index 57153fcc347b..3bec2ff0b15c 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -22,6 +22,7 @@
 #define BTRFS_ADD_DELAYED_REF    1 /* add one backref to the tree */
 #define BTRFS_DROP_DELAYED_REF   2 /* delete one backref from the tree */
 #define BTRFS_ADD_DELAYED_EXTENT 3 /* record a full extent allocation */
+#define BTRFS_UPDATE_DELAYED_HEAD 4 /* not changing ref count on head ref */
 
 struct btrfs_delayed_ref_node {
 	struct rb_node rb_node;

commit c3e69d58e86c3917ae4e9e31b4acf490a7cafe60
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Mar 13 10:17:05 2009 -0400

    Btrfs: process the delayed reference queue in clusters
    
    The delayed reference queue maintains pending operations that need to
    be done to the extent allocation tree.  These are processed by
    finding records in the tree that are not currently being processed one at
    a time.
    
    This is slow because it uses lots of time searching through the rbtree
    and because it creates lock contention on the extent allocation tree
    when lots of different procs are running delayed refs at the same time.
    
    This commit changes things to grab a cluster of refs for processing,
    using a cursor into the rbtree as the starting point of the next search.
    This way we walk smoothly through the rbtree.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index c345fee9f96b..57153fcc347b 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -68,6 +68,8 @@ struct btrfs_delayed_ref_head {
 	 */
 	struct mutex mutex;
 
+	struct list_head cluster;
+
 	/*
 	 * when a new extent is allocated, it is just reserved in memory
 	 * The actual extent isn't inserted into the extent allocation tree
@@ -115,12 +117,20 @@ struct btrfs_delayed_ref_root {
 	 */
 	unsigned long num_entries;
 
+	/* total number of head nodes in tree */
+	unsigned long num_heads;
+
+	/* total number of head nodes ready for processing */
+	unsigned long num_heads_ready;
+
 	/*
 	 * set when the tree is flushing before a transaction commit,
 	 * used by the throttling code to decide if new updates need
 	 * to be run right away
 	 */
 	int flushing;
+
+	u64 run_delayed_start;
 };
 
 static inline void btrfs_put_delayed_ref(struct btrfs_delayed_ref_node *ref)
@@ -140,9 +150,6 @@ int btrfs_add_delayed_ref(struct btrfs_trans_handle *trans,
 struct btrfs_delayed_ref_head *
 btrfs_find_delayed_ref_head(struct btrfs_trans_handle *trans, u64 bytenr);
 int btrfs_delayed_ref_pending(struct btrfs_trans_handle *trans, u64 bytenr);
-int btrfs_lock_delayed_ref(struct btrfs_trans_handle *trans,
-			   struct btrfs_delayed_ref_node *ref,
-			   struct btrfs_delayed_ref_head **next_ret);
 int btrfs_lookup_extent_ref(struct btrfs_trans_handle *trans,
 			    struct btrfs_root *root, u64 bytenr,
 			    u64 num_bytes, u32 *refs);
@@ -151,6 +158,10 @@ int btrfs_update_delayed_ref(struct btrfs_trans_handle *trans,
 			  u64 parent, u64 orig_ref_root, u64 ref_root,
 			  u64 orig_ref_generation, u64 ref_generation,
 			  u64 owner_objectid, int pin);
+int btrfs_delayed_ref_lock(struct btrfs_trans_handle *trans,
+			   struct btrfs_delayed_ref_head *head);
+int btrfs_find_ref_cluster(struct btrfs_trans_handle *trans,
+			   struct list_head *cluster, u64 search_start);
 /*
  * a node might live in a head or a regular ref, this lets you
  * test for the proper type to use.

commit 1887be66dcc3140a81d1299958a41fc0eedfa64f
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Mar 13 10:11:24 2009 -0400

    Btrfs: try to cleanup delayed refs while freeing extents
    
    When extents are freed, it is likely that we've removed the last
    delayed reference update for the extent.  This checks the delayed
    ref tree when things are freed, and if no ref updates area left it
    immediately processes the delayed ref.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
index 37919e5c007f..c345fee9f96b 100644
--- a/fs/btrfs/delayed-ref.h
+++ b/fs/btrfs/delayed-ref.h
@@ -137,9 +137,8 @@ int btrfs_add_delayed_ref(struct btrfs_trans_handle *trans,
 			  u64 ref_generation, u64 owner_objectid, int action,
 			  int pin);
 
-struct btrfs_delayed_ref *
-btrfs_find_delayed_ref(struct btrfs_trans_handle *trans, u64 bytenr,
-		       u64 parent);
+struct btrfs_delayed_ref_head *
+btrfs_find_delayed_ref_head(struct btrfs_trans_handle *trans, u64 bytenr);
 int btrfs_delayed_ref_pending(struct btrfs_trans_handle *trans, u64 bytenr);
 int btrfs_lock_delayed_ref(struct btrfs_trans_handle *trans,
 			   struct btrfs_delayed_ref_node *ref,

commit 56bec294dea971335d4466b30f2d959f28f6e36d
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Mar 13 10:10:06 2009 -0400

    Btrfs: do extent allocation and reference count updates in the background
    
    The extent allocation tree maintains a reference count and full
    back reference information for every extent allocated in the
    filesystem.  For subvolume and snapshot trees, every time
    a block goes through COW, the new copy of the block adds a reference
    on every block it points to.
    
    If a btree node points to 150 leaves, then the COW code needs to go
    and add backrefs on 150 different extents, which might be spread all
    over the extent allocation tree.
    
    These updates currently happen during btrfs_cow_block, and most COWs
    happen during btrfs_search_slot.  btrfs_search_slot has locks held
    on both the parent and the node we are COWing, and so we really want
    to avoid IO during the COW if we can.
    
    This commit adds an rbtree of pending reference count updates and extent
    allocations.  The tree is ordered by byte number of the extent and byte number
    of the parent for the back reference.  The tree allows us to:
    
    1) Modify back references in something close to disk order, reducing seeks
    2) Significantly reduce the number of modifications made as block pointers
    are balanced around
    3) Do all of the extent insertion and back reference modifications outside
    of the performance critical btrfs_search_slot code.
    
    #3 has the added benefit of greatly reducing the btrfs stack footprint.
    The extent allocation tree modifications are done without the deep
    (and somewhat recursive) call chains used in the past.
    
    These delayed back reference updates must be done before the transaction
    commits, and so the rbtree is tied to the transaction.  Throttling is
    implemented to help keep the queue of backrefs at a reasonable size.
    
    Since there was a similar mechanism in place for the extent tree
    extents, that is removed and replaced by the delayed reference tree.
    
    Yan Zheng <yan.zheng@oracle.com> helped review and fixup this code.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/delayed-ref.h b/fs/btrfs/delayed-ref.h
new file mode 100644
index 000000000000..37919e5c007f
--- /dev/null
+++ b/fs/btrfs/delayed-ref.h
@@ -0,0 +1,182 @@
+/*
+ * Copyright (C) 2008 Oracle.  All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public
+ * License v2 as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public
+ * License along with this program; if not, write to the
+ * Free Software Foundation, Inc., 59 Temple Place - Suite 330,
+ * Boston, MA 021110-1307, USA.
+ */
+#ifndef __DELAYED_REF__
+#define __DELAYED_REF__
+
+/* these are the possible values of struct btrfs_delayed_ref->action */
+#define BTRFS_ADD_DELAYED_REF    1 /* add one backref to the tree */
+#define BTRFS_DROP_DELAYED_REF   2 /* delete one backref from the tree */
+#define BTRFS_ADD_DELAYED_EXTENT 3 /* record a full extent allocation */
+
+struct btrfs_delayed_ref_node {
+	struct rb_node rb_node;
+
+	/* the starting bytenr of the extent */
+	u64 bytenr;
+
+	/* the parent our backref will point to */
+	u64 parent;
+
+	/* the size of the extent */
+	u64 num_bytes;
+
+	/* ref count on this data structure */
+	atomic_t refs;
+
+	/*
+	 * how many refs is this entry adding or deleting.  For
+	 * head refs, this may be a negative number because it is keeping
+	 * track of the total mods done to the reference count.
+	 * For individual refs, this will always be a positive number
+	 *
+	 * It may be more than one, since it is possible for a single
+	 * parent to have more than one ref on an extent
+	 */
+	int ref_mod;
+
+	/* is this node still in the rbtree? */
+	unsigned int in_tree:1;
+};
+
+/*
+ * the head refs are used to hold a lock on a given extent, which allows us
+ * to make sure that only one process is running the delayed refs
+ * at a time for a single extent.  They also store the sum of all the
+ * reference count modifications we've queued up.
+ */
+struct btrfs_delayed_ref_head {
+	struct btrfs_delayed_ref_node node;
+
+	/*
+	 * the mutex is held while running the refs, and it is also
+	 * held when checking the sum of reference modifications.
+	 */
+	struct mutex mutex;
+
+	/*
+	 * when a new extent is allocated, it is just reserved in memory
+	 * The actual extent isn't inserted into the extent allocation tree
+	 * until the delayed ref is processed.  must_insert_reserved is
+	 * used to flag a delayed ref so the accounting can be updated
+	 * when a full insert is done.
+	 *
+	 * It is possible the extent will be freed before it is ever
+	 * inserted into the extent allocation tree.  In this case
+	 * we need to update the in ram accounting to properly reflect
+	 * the free has happened.
+	 */
+	unsigned int must_insert_reserved:1;
+};
+
+struct btrfs_delayed_ref {
+	struct btrfs_delayed_ref_node node;
+
+	/* the root objectid our ref will point to */
+	u64 root;
+
+	/* the generation for the backref */
+	u64 generation;
+
+	/* owner_objectid of the backref  */
+	u64 owner_objectid;
+
+	/* operation done by this entry in the rbtree */
+	u8 action;
+
+	/* if pin == 1, when the extent is freed it will be pinned until
+	 * transaction commit
+	 */
+	unsigned int pin:1;
+};
+
+struct btrfs_delayed_ref_root {
+	struct rb_root root;
+
+	/* this spin lock protects the rbtree and the entries inside */
+	spinlock_t lock;
+
+	/* how many delayed ref updates we've queued, used by the
+	 * throttling code
+	 */
+	unsigned long num_entries;
+
+	/*
+	 * set when the tree is flushing before a transaction commit,
+	 * used by the throttling code to decide if new updates need
+	 * to be run right away
+	 */
+	int flushing;
+};
+
+static inline void btrfs_put_delayed_ref(struct btrfs_delayed_ref_node *ref)
+{
+	WARN_ON(atomic_read(&ref->refs) == 0);
+	if (atomic_dec_and_test(&ref->refs)) {
+		WARN_ON(ref->in_tree);
+		kfree(ref);
+	}
+}
+
+int btrfs_add_delayed_ref(struct btrfs_trans_handle *trans,
+			  u64 bytenr, u64 num_bytes, u64 parent, u64 ref_root,
+			  u64 ref_generation, u64 owner_objectid, int action,
+			  int pin);
+
+struct btrfs_delayed_ref *
+btrfs_find_delayed_ref(struct btrfs_trans_handle *trans, u64 bytenr,
+		       u64 parent);
+int btrfs_delayed_ref_pending(struct btrfs_trans_handle *trans, u64 bytenr);
+int btrfs_lock_delayed_ref(struct btrfs_trans_handle *trans,
+			   struct btrfs_delayed_ref_node *ref,
+			   struct btrfs_delayed_ref_head **next_ret);
+int btrfs_lookup_extent_ref(struct btrfs_trans_handle *trans,
+			    struct btrfs_root *root, u64 bytenr,
+			    u64 num_bytes, u32 *refs);
+int btrfs_update_delayed_ref(struct btrfs_trans_handle *trans,
+			  u64 bytenr, u64 num_bytes, u64 orig_parent,
+			  u64 parent, u64 orig_ref_root, u64 ref_root,
+			  u64 orig_ref_generation, u64 ref_generation,
+			  u64 owner_objectid, int pin);
+/*
+ * a node might live in a head or a regular ref, this lets you
+ * test for the proper type to use.
+ */
+static int btrfs_delayed_ref_is_head(struct btrfs_delayed_ref_node *node)
+{
+	return node->parent == (u64)-1;
+}
+
+/*
+ * helper functions to cast a node into its container
+ */
+static inline struct btrfs_delayed_ref *
+btrfs_delayed_node_to_ref(struct btrfs_delayed_ref_node *node)
+{
+	WARN_ON(btrfs_delayed_ref_is_head(node));
+	return container_of(node, struct btrfs_delayed_ref, node);
+
+}
+
+static inline struct btrfs_delayed_ref_head *
+btrfs_delayed_node_to_head(struct btrfs_delayed_ref_node *node)
+{
+	WARN_ON(!btrfs_delayed_ref_is_head(node));
+	return container_of(node, struct btrfs_delayed_ref_head, node);
+
+}
+#endif
