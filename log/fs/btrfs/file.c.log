commit d77765911385b65fc82d74ab71b8983cddfe0b58
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Jul 9 18:22:06 2020 +0200

    btrfs: wire up iter_file_splice_write
    
    btrfs implements the iter_write op and thus can use the more efficient
    iov_iter based splice implementation.  For now falling back to the less
    efficient default is pretty harmless, but I have a pending series that
    removes the default, and thus would cause btrfs to not support splice
    at all.
    
    Reported-by: Andy Lavr <andy.lavr@gmail.com>
    Tested-by: Andy Lavr <andy.lavr@gmail.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 2520605afc25..b0d2c976587e 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -3509,6 +3509,7 @@ const struct file_operations btrfs_file_operations = {
 	.read_iter      = generic_file_read_iter,
 	.splice_read	= generic_file_splice_read,
 	.write_iter	= btrfs_file_write_iter,
+	.splice_write	= iter_file_splice_write,
 	.mmap		= btrfs_file_mmap,
 	.open		= btrfs_file_open,
 	.release	= btrfs_release_file,

commit 5dbb75ed6900048e146247b6325742d92c892548
Author: Filipe Manana <fdmanana@suse.com>
Date:   Mon Jun 15 18:49:39 2020 +0100

    btrfs: fix RWF_NOWAIT writes blocking on extent locks and waiting for IO
    
    A RWF_NOWAIT write is not supposed to wait on filesystem locks that can be
    held for a long time or for ongoing IO to complete.
    
    However when calling check_can_nocow(), if the inode has prealloc extents
    or has the NOCOW flag set, we can block on extent (file range) locks
    through the call to btrfs_lock_and_flush_ordered_range(). Such lock can
    take a significant amount of time to be available. For example, a fiemap
    task may be running, and iterating through the entire file range checking
    all extents and doing backref walking to determine if they are shared,
    or a readpage operation may be in progress.
    
    Also at btrfs_lock_and_flush_ordered_range(), called by check_can_nocow(),
    after locking the file range we wait for any existing ordered extent that
    is in progress to complete. Another operation that can take a significant
    amount of time and defeat the purpose of RWF_NOWAIT.
    
    So fix this by trying to lock the file range and if it's currently locked
    return -EAGAIN to user space. If we are able to lock the file range without
    waiting and there is an ordered extent in the range, return -EAGAIN as
    well, instead of waiting for it to complete. Finally, don't bother trying
    to lock the snapshot lock of the root when attempting a RWF_NOWAIT write,
    as that is only important for buffered writes.
    
    Fixes: edf064e7c6fec3 ("btrfs: nowait aio support")
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 6d5d905281c6..2520605afc25 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1533,7 +1533,7 @@ lock_and_cleanup_extent_if_need(struct btrfs_inode *inode, struct page **pages,
 }
 
 static noinline int check_can_nocow(struct btrfs_inode *inode, loff_t pos,
-				    size_t *write_bytes)
+				    size_t *write_bytes, bool nowait)
 {
 	struct btrfs_fs_info *fs_info = inode->root->fs_info;
 	struct btrfs_root *root = inode->root;
@@ -1541,27 +1541,43 @@ static noinline int check_can_nocow(struct btrfs_inode *inode, loff_t pos,
 	u64 num_bytes;
 	int ret;
 
-	if (!btrfs_drew_try_write_lock(&root->snapshot_lock))
+	if (!nowait && !btrfs_drew_try_write_lock(&root->snapshot_lock))
 		return -EAGAIN;
 
 	lockstart = round_down(pos, fs_info->sectorsize);
 	lockend = round_up(pos + *write_bytes,
 			   fs_info->sectorsize) - 1;
+	num_bytes = lockend - lockstart + 1;
 
-	btrfs_lock_and_flush_ordered_range(inode, lockstart,
-					   lockend, NULL);
+	if (nowait) {
+		struct btrfs_ordered_extent *ordered;
+
+		if (!try_lock_extent(&inode->io_tree, lockstart, lockend))
+			return -EAGAIN;
+
+		ordered = btrfs_lookup_ordered_range(inode, lockstart,
+						     num_bytes);
+		if (ordered) {
+			btrfs_put_ordered_extent(ordered);
+			ret = -EAGAIN;
+			goto out_unlock;
+		}
+	} else {
+		btrfs_lock_and_flush_ordered_range(inode, lockstart,
+						   lockend, NULL);
+	}
 
-	num_bytes = lockend - lockstart + 1;
 	ret = can_nocow_extent(&inode->vfs_inode, lockstart, &num_bytes,
 			NULL, NULL, NULL);
 	if (ret <= 0) {
 		ret = 0;
-		btrfs_drew_write_unlock(&root->snapshot_lock);
+		if (!nowait)
+			btrfs_drew_write_unlock(&root->snapshot_lock);
 	} else {
 		*write_bytes = min_t(size_t, *write_bytes ,
 				     num_bytes - pos + lockstart);
 	}
-
+out_unlock:
 	unlock_extent(&inode->io_tree, lockstart, lockend);
 
 	return ret;
@@ -1633,7 +1649,7 @@ static noinline ssize_t btrfs_buffered_write(struct kiocb *iocb,
 			if ((BTRFS_I(inode)->flags & (BTRFS_INODE_NODATACOW |
 						      BTRFS_INODE_PREALLOC)) &&
 			    check_can_nocow(BTRFS_I(inode), pos,
-					&write_bytes) > 0) {
+					    &write_bytes, false) > 0) {
 				/*
 				 * For nodata cow case, no need to reserve
 				 * data space.
@@ -1912,12 +1928,11 @@ static ssize_t btrfs_file_write_iter(struct kiocb *iocb,
 		 */
 		if (!(BTRFS_I(inode)->flags & (BTRFS_INODE_NODATACOW |
 					      BTRFS_INODE_PREALLOC)) ||
-		    check_can_nocow(BTRFS_I(inode), pos, &nocow_bytes) <= 0) {
+		    check_can_nocow(BTRFS_I(inode), pos, &nocow_bytes,
+				    true) <= 0) {
 			inode_unlock(inode);
 			return -EAGAIN;
 		}
-		/* check_can_nocow() locks the snapshot lock on success */
-		btrfs_drew_write_unlock(&root->snapshot_lock);
 		/*
 		 * There are holes in the range or parts of the range that must
 		 * be COWed (shared extents, RO block groups, etc), so just bail

commit 260a63395f90f67d6ab89e4266af9e3dc34a77e9
Author: Filipe Manana <fdmanana@suse.com>
Date:   Mon Jun 15 18:49:13 2020 +0100

    btrfs: fix RWF_NOWAIT write not failling when we need to cow
    
    If we attempt to do a RWF_NOWAIT write against a file range for which we
    can only do NOCOW for a part of it, due to the existence of holes or
    shared extents for example, we proceed with the write as if it were
    possible to NOCOW the whole range.
    
    Example:
    
      $ mkfs.btrfs -f /dev/sdb
      $ mount /dev/sdb /mnt
    
      $ touch /mnt/sdj/bar
      $ chattr +C /mnt/sdj/bar
    
      $ xfs_io -d -c "pwrite -S 0xab -b 256K 0 256K" /mnt/bar
      wrote 262144/262144 bytes at offset 0
      256 KiB, 1 ops; 0.0003 sec (694.444 MiB/sec and 2777.7778 ops/sec)
    
      $ xfs_io -c "fpunch 64K 64K" /mnt/bar
      $ sync
    
      $ xfs_io -d -c "pwrite -N -V 1 -b 128K -S 0xfe 0 128K" /mnt/bar
      wrote 131072/131072 bytes at offset 0
      128 KiB, 1 ops; 0.0007 sec (160.051 MiB/sec and 1280.4097 ops/sec)
    
    This last write should fail with -EAGAIN since the file range from 64K to
    128K is a hole. On xfs it fails, as expected, but on ext4 it currently
    succeeds because apparently it is expensive to check if there are extents
    allocated for the whole range, but I'll check with the ext4 people.
    
    Fix the issue by checking if check_can_nocow() returns a number of
    NOCOW'able bytes smaller then the requested number of bytes, and if it
    does return -EAGAIN.
    
    Fixes: edf064e7c6fec3 ("btrfs: nowait aio support")
    CC: stable@vger.kernel.org # 4.14+
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 04faa04fccd1..6d5d905281c6 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1904,18 +1904,29 @@ static ssize_t btrfs_file_write_iter(struct kiocb *iocb,
 	pos = iocb->ki_pos;
 	count = iov_iter_count(from);
 	if (iocb->ki_flags & IOCB_NOWAIT) {
+		size_t nocow_bytes = count;
+
 		/*
 		 * We will allocate space in case nodatacow is not set,
 		 * so bail
 		 */
 		if (!(BTRFS_I(inode)->flags & (BTRFS_INODE_NODATACOW |
 					      BTRFS_INODE_PREALLOC)) ||
-		    check_can_nocow(BTRFS_I(inode), pos, &count) <= 0) {
+		    check_can_nocow(BTRFS_I(inode), pos, &nocow_bytes) <= 0) {
 			inode_unlock(inode);
 			return -EAGAIN;
 		}
 		/* check_can_nocow() locks the snapshot lock on success */
 		btrfs_drew_write_unlock(&root->snapshot_lock);
+		/*
+		 * There are holes in the range or parts of the range that must
+		 * be COWed (shared extents, RO block groups, etc), so just bail
+		 * out.
+		 */
+		if (nocow_bytes < count) {
+			inode_unlock(inode);
+			return -EAGAIN;
+		}
 	}
 
 	current->backing_dev_info = inode_to_bdi(inode);

commit f2cb2f39ccc30fa13d3ac078d461031a63960e5b
Author: Filipe Manana <fdmanana@suse.com>
Date:   Mon Jun 15 18:46:01 2020 +0100

    btrfs: fix hang on snapshot creation after RWF_NOWAIT write
    
    If we do a successful RWF_NOWAIT write we end up locking the snapshot lock
    of the inode, through a call to check_can_nocow(), but we never unlock it.
    
    This means the next attempt to create a snapshot on the subvolume will
    hang forever.
    
    Trivial reproducer:
    
      $ mkfs.btrfs -f /dev/sdb
      $ mount /dev/sdb /mnt
    
      $ touch /mnt/foobar
      $ chattr +C /mnt/foobar
      $ xfs_io -d -c "pwrite -S 0xab 0 64K" /mnt/foobar
      $ xfs_io -d -c "pwrite -N -V 1 -S 0xfe 0 64K" /mnt/foobar
    
      $ btrfs subvolume snapshot -r /mnt /mnt/snap
        --> hangs
    
    Fix this by unlocking the snapshot lock if check_can_nocow() returned
    success.
    
    Fixes: edf064e7c6fec3 ("btrfs: nowait aio support")
    CC: stable@vger.kernel.org # 4.14+
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 2c14312b05e8..04faa04fccd1 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1914,6 +1914,8 @@ static ssize_t btrfs_file_write_iter(struct kiocb *iocb,
 			inode_unlock(inode);
 			return -EAGAIN;
 		}
+		/* check_can_nocow() locks the snapshot lock on success */
+		btrfs_drew_write_unlock(&root->snapshot_lock);
 	}
 
 	current->backing_dev_info = inode_to_bdi(inode);

commit 55e20bd12a56e06c38b953177bb162cbbaa96004
Author: David Sterba <dsterba@suse.com>
Date:   Tue Jun 9 19:56:06 2020 +0200

    Revert "btrfs: switch to iomap_dio_rw() for dio"
    
    This reverts commit a43a67a2d715540c1368b9501a22b0373b5874c0.
    
    This patch reverts the main part of switching direct io implementation
    to iomap infrastructure. There's a problem in invalidate page that
    couldn't be solved as regression in this development cycle.
    
    The problem occurs when buffered and direct io are mixed, and the ranges
    overlap. Although this is not recommended, filesystems implement
    measures or fallbacks to make it somehow work. In this case, fallback to
    buffered IO would be an option for btrfs (this already happens when
    direct io is done on compressed data), but the change would be needed in
    the iomap code, bringing new semantics to other filesystems.
    
    Another problem arises when again the buffered and direct ios are mixed,
    invalidation fails, then -EIO is set on the mapping and fsync will fail,
    though there's no real error.
    
    There have been discussions how to fix that, but revert seems to be the
    least intrusive option.
    
    Link: https://lore.kernel.org/linux-btrfs/20200528192103.xm45qoxqmkw7i5yl@fiona/
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index dff89d04fd16..2c14312b05e8 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1819,7 +1819,7 @@ static ssize_t __btrfs_direct_write(struct kiocb *iocb, struct iov_iter *from)
 	loff_t endbyte;
 	int err;
 
-	written = btrfs_direct_IO(iocb, from);
+	written = generic_file_direct_write(iocb, from);
 
 	if (written < 0 || !iov_iter_count(from))
 		return written;
@@ -3476,26 +3476,9 @@ static int btrfs_file_open(struct inode *inode, struct file *filp)
 	return generic_file_open(inode, filp);
 }
 
-static ssize_t btrfs_file_read_iter(struct kiocb *iocb, struct iov_iter *to)
-{
-	ssize_t ret = 0;
-
-	if (iocb->ki_flags & IOCB_DIRECT) {
-		struct inode *inode = file_inode(iocb->ki_filp);
-
-		inode_lock_shared(inode);
-		ret = btrfs_direct_IO(iocb, to);
-		inode_unlock_shared(inode);
-		if (ret < 0)
-			return ret;
-	}
-
-	return generic_file_buffered_read(iocb, to, ret);
-}
-
 const struct file_operations btrfs_file_operations = {
 	.llseek		= btrfs_file_llseek,
-	.read_iter      = btrfs_file_read_iter,
+	.read_iter      = generic_file_read_iter,
 	.splice_read	= generic_file_splice_read,
 	.write_iter	= btrfs_file_write_iter,
 	.mmap		= btrfs_file_mmap,

commit f4c48b44080e18d86d7983c1a60ee5a7f9d07e3e
Author: David Sterba <dsterba@suse.com>
Date:   Tue Jun 9 19:19:27 2020 +0200

    Revert "btrfs: split btrfs_direct_IO to read and write part"
    
    This reverts commit d8f3e73587ce574f7a9bc165e0db69b0b148f6f8.
    
    The patch is a cleanup of direct IO port to iomap infrastructure,
    which gets reverted.
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index fde125616687..dff89d04fd16 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1809,61 +1809,21 @@ static noinline ssize_t btrfs_buffered_write(struct kiocb *iocb,
 	return num_written ? num_written : ret;
 }
 
-static ssize_t check_direct_IO(struct btrfs_fs_info *fs_info,
-                               const struct iov_iter *iter, loff_t offset)
-{
-        const unsigned int blocksize_mask = fs_info->sectorsize - 1;
-
-        if (offset & blocksize_mask)
-                return -EINVAL;
-
-        if (iov_iter_alignment(iter) & blocksize_mask)
-                return -EINVAL;
-
-	return 0;
-}
-
-static ssize_t btrfs_direct_write(struct kiocb *iocb, struct iov_iter *from)
+static ssize_t __btrfs_direct_write(struct kiocb *iocb, struct iov_iter *from)
 {
 	struct file *file = iocb->ki_filp;
 	struct inode *inode = file_inode(file);
-	struct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);
-	loff_t pos = iocb->ki_pos;
-	ssize_t written = 0;
+	loff_t pos;
+	ssize_t written;
 	ssize_t written_buffered;
 	loff_t endbyte;
 	int err;
-	size_t count = 0;
-	bool relock = false;
 
-	if (check_direct_IO(fs_info, from, pos))
-		goto buffered;
-
-	count = iov_iter_count(from);
-	/*
-	 * If the write DIO is beyond the EOF, we need update the isize, but it
-	 * is protected by i_mutex. So we can not unlock the i_mutex at this
-	 * case.
-	 */
-	if (pos + count <= inode->i_size) {
-		inode_unlock(inode);
-		relock = true;
-	} else if (iocb->ki_flags & IOCB_NOWAIT) {
-		return -EAGAIN;
-	}
-
-	down_read(&BTRFS_I(inode)->dio_sem);
-	written = iomap_dio_rw(iocb, from, &btrfs_dio_iomap_ops, &btrfs_dops,
-			       is_sync_kiocb(iocb));
-	up_read(&BTRFS_I(inode)->dio_sem);
-
-	if (relock)
-		inode_lock(inode);
+	written = btrfs_direct_IO(iocb, from);
 
 	if (written < 0 || !iov_iter_count(from))
 		return written;
 
-buffered:
 	pos = iocb->ki_pos;
 	written_buffered = btrfs_buffered_write(iocb, from);
 	if (written_buffered < 0) {
@@ -2002,7 +1962,7 @@ static ssize_t btrfs_file_write_iter(struct kiocb *iocb,
 		atomic_inc(&BTRFS_I(inode)->sync_writers);
 
 	if (iocb->ki_flags & IOCB_DIRECT) {
-		num_written = btrfs_direct_write(iocb, from);
+		num_written = __btrfs_direct_write(iocb, from);
 	} else {
 		num_written = btrfs_buffered_write(iocb, from);
 		if (num_written > 0)
@@ -3516,44 +3476,16 @@ static int btrfs_file_open(struct inode *inode, struct file *filp)
 	return generic_file_open(inode, filp);
 }
 
-static int check_direct_read(struct btrfs_fs_info *fs_info,
-                               const struct iov_iter *iter, loff_t offset)
-{
-	int ret;
-	int i, seg;
-
-	ret = check_direct_IO(fs_info, iter, offset);
-	if (ret < 0)
-		return ret;
-
-	for (seg = 0; seg < iter->nr_segs; seg++)
-		for (i = seg + 1; i < iter->nr_segs; i++)
-			if (iter->iov[seg].iov_base == iter->iov[i].iov_base)
-				return -EINVAL;
-	return 0;
-}
-
-static ssize_t btrfs_direct_read(struct kiocb *iocb, struct iov_iter *to)
-{
-	struct inode *inode = file_inode(iocb->ki_filp);
-	ssize_t ret;
-
-	if (check_direct_read(btrfs_sb(inode->i_sb), to, iocb->ki_pos))
-		return 0;
-
-	inode_lock_shared(inode);
-        ret = iomap_dio_rw(iocb, to, &btrfs_dio_iomap_ops, &btrfs_dops,
-			   is_sync_kiocb(iocb));
-	inode_unlock_shared(inode);
-	return ret;
-}
-
 static ssize_t btrfs_file_read_iter(struct kiocb *iocb, struct iov_iter *to)
 {
 	ssize_t ret = 0;
 
 	if (iocb->ki_flags & IOCB_DIRECT) {
-		ret = btrfs_direct_read(iocb, to);
+		struct inode *inode = file_inode(iocb->ki_filp);
+
+		inode_lock_shared(inode);
+		ret = btrfs_direct_IO(iocb, to);
+		inode_unlock_shared(inode);
 		if (ret < 0)
 			return ret;
 	}

commit d8f3e73587ce574f7a9bc165e0db69b0b148f6f8
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue May 19 09:46:29 2020 -0500

    btrfs: split btrfs_direct_IO to read and write part
    
    The read and write versions don't have anything in common except for the
    call to iomap_dio_rw.  So split this function, and merge each half into
    its only caller.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Goldwyn Rodrigues <rgoldwyn@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index dff89d04fd16..fde125616687 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1809,21 +1809,61 @@ static noinline ssize_t btrfs_buffered_write(struct kiocb *iocb,
 	return num_written ? num_written : ret;
 }
 
-static ssize_t __btrfs_direct_write(struct kiocb *iocb, struct iov_iter *from)
+static ssize_t check_direct_IO(struct btrfs_fs_info *fs_info,
+                               const struct iov_iter *iter, loff_t offset)
+{
+        const unsigned int blocksize_mask = fs_info->sectorsize - 1;
+
+        if (offset & blocksize_mask)
+                return -EINVAL;
+
+        if (iov_iter_alignment(iter) & blocksize_mask)
+                return -EINVAL;
+
+	return 0;
+}
+
+static ssize_t btrfs_direct_write(struct kiocb *iocb, struct iov_iter *from)
 {
 	struct file *file = iocb->ki_filp;
 	struct inode *inode = file_inode(file);
-	loff_t pos;
-	ssize_t written;
+	struct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);
+	loff_t pos = iocb->ki_pos;
+	ssize_t written = 0;
 	ssize_t written_buffered;
 	loff_t endbyte;
 	int err;
+	size_t count = 0;
+	bool relock = false;
 
-	written = btrfs_direct_IO(iocb, from);
+	if (check_direct_IO(fs_info, from, pos))
+		goto buffered;
+
+	count = iov_iter_count(from);
+	/*
+	 * If the write DIO is beyond the EOF, we need update the isize, but it
+	 * is protected by i_mutex. So we can not unlock the i_mutex at this
+	 * case.
+	 */
+	if (pos + count <= inode->i_size) {
+		inode_unlock(inode);
+		relock = true;
+	} else if (iocb->ki_flags & IOCB_NOWAIT) {
+		return -EAGAIN;
+	}
+
+	down_read(&BTRFS_I(inode)->dio_sem);
+	written = iomap_dio_rw(iocb, from, &btrfs_dio_iomap_ops, &btrfs_dops,
+			       is_sync_kiocb(iocb));
+	up_read(&BTRFS_I(inode)->dio_sem);
+
+	if (relock)
+		inode_lock(inode);
 
 	if (written < 0 || !iov_iter_count(from))
 		return written;
 
+buffered:
 	pos = iocb->ki_pos;
 	written_buffered = btrfs_buffered_write(iocb, from);
 	if (written_buffered < 0) {
@@ -1962,7 +2002,7 @@ static ssize_t btrfs_file_write_iter(struct kiocb *iocb,
 		atomic_inc(&BTRFS_I(inode)->sync_writers);
 
 	if (iocb->ki_flags & IOCB_DIRECT) {
-		num_written = __btrfs_direct_write(iocb, from);
+		num_written = btrfs_direct_write(iocb, from);
 	} else {
 		num_written = btrfs_buffered_write(iocb, from);
 		if (num_written > 0)
@@ -3476,16 +3516,44 @@ static int btrfs_file_open(struct inode *inode, struct file *filp)
 	return generic_file_open(inode, filp);
 }
 
+static int check_direct_read(struct btrfs_fs_info *fs_info,
+                               const struct iov_iter *iter, loff_t offset)
+{
+	int ret;
+	int i, seg;
+
+	ret = check_direct_IO(fs_info, iter, offset);
+	if (ret < 0)
+		return ret;
+
+	for (seg = 0; seg < iter->nr_segs; seg++)
+		for (i = seg + 1; i < iter->nr_segs; i++)
+			if (iter->iov[seg].iov_base == iter->iov[i].iov_base)
+				return -EINVAL;
+	return 0;
+}
+
+static ssize_t btrfs_direct_read(struct kiocb *iocb, struct iov_iter *to)
+{
+	struct inode *inode = file_inode(iocb->ki_filp);
+	ssize_t ret;
+
+	if (check_direct_read(btrfs_sb(inode->i_sb), to, iocb->ki_pos))
+		return 0;
+
+	inode_lock_shared(inode);
+        ret = iomap_dio_rw(iocb, to, &btrfs_dio_iomap_ops, &btrfs_dops,
+			   is_sync_kiocb(iocb));
+	inode_unlock_shared(inode);
+	return ret;
+}
+
 static ssize_t btrfs_file_read_iter(struct kiocb *iocb, struct iov_iter *to)
 {
 	ssize_t ret = 0;
 
 	if (iocb->ki_flags & IOCB_DIRECT) {
-		struct inode *inode = file_inode(iocb->ki_filp);
-
-		inode_lock_shared(inode);
-		ret = btrfs_direct_IO(iocb, to);
-		inode_unlock_shared(inode);
+		ret = btrfs_direct_read(iocb, to);
 		if (ret < 0)
 			return ret;
 	}

commit a43a67a2d715540c1368b9501a22b0373b5874c0
Author: Goldwyn Rodrigues <rgoldwyn@suse.com>
Date:   Tue May 19 09:14:18 2020 -0500

    btrfs: switch to iomap_dio_rw() for dio
    
    Switch from __blockdev_direct_IO() to iomap_dio_rw().
    Rename btrfs_get_blocks_direct() to btrfs_dio_iomap_begin() and use it
    as iomap_begin() for iomap direct I/O functions. This function
    allocates and locks all the blocks required for the I/O.
    btrfs_submit_direct() is used as the submit_io() hook for direct I/O
    ops.
    
    Since we need direct I/O reads to go through iomap_dio_rw(), we change
    file_operations.read_iter() to a btrfs_file_read_iter() which calls
    btrfs_direct_IO() for direct reads and falls back to
    generic_file_buffered_read() for incomplete reads and buffered reads.
    
    We don't need address_space.direct_IO() anymore so set it to noop.
    Similarly, we don't need flags used in __blockdev_direct_IO(). iomap is
    capable of direct I/O reads from a hole, so we don't need to return
    -ENOENT.
    
    BTRFS direct I/O is now done under i_rwsem, shared in case of reads and
    exclusive in case of writes. This guards against simultaneous truncates.
    
    Use iomap->iomap_end() to check for failed or incomplete direct I/O:
     - for writes, call __endio_write_update_ordered()
     - for reads, unlock extents
    
    btrfs_dio_data is now hooked in iomap->private and not
    current->journal_info. It carries the reservation variable and the
    amount of data submitted, so we can calculate the amount of data to call
    __endio_write_update_ordered in case of an error.
    
    This patch removes last use of struct buffer_head from btrfs.
    
    Signed-off-by: Goldwyn Rodrigues <rgoldwyn@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 2c14312b05e8..dff89d04fd16 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1819,7 +1819,7 @@ static ssize_t __btrfs_direct_write(struct kiocb *iocb, struct iov_iter *from)
 	loff_t endbyte;
 	int err;
 
-	written = generic_file_direct_write(iocb, from);
+	written = btrfs_direct_IO(iocb, from);
 
 	if (written < 0 || !iov_iter_count(from))
 		return written;
@@ -3476,9 +3476,26 @@ static int btrfs_file_open(struct inode *inode, struct file *filp)
 	return generic_file_open(inode, filp);
 }
 
+static ssize_t btrfs_file_read_iter(struct kiocb *iocb, struct iov_iter *to)
+{
+	ssize_t ret = 0;
+
+	if (iocb->ki_flags & IOCB_DIRECT) {
+		struct inode *inode = file_inode(iocb->ki_filp);
+
+		inode_lock_shared(inode);
+		ret = btrfs_direct_IO(iocb, to);
+		inode_unlock_shared(inode);
+		if (ret < 0)
+			return ret;
+	}
+
+	return generic_file_buffered_read(iocb, to, ret);
+}
+
 const struct file_operations btrfs_file_operations = {
 	.llseek		= btrfs_file_llseek,
-	.read_iter      = generic_file_read_iter,
+	.read_iter      = btrfs_file_read_iter,
 	.splice_read	= generic_file_splice_read,
 	.write_iter	= btrfs_file_write_iter,
 	.mmap		= btrfs_file_mmap,

commit 0202e83fdab05b3bf641804afea57a2bfcbcbd70
Author: David Sterba <dsterba@suse.com>
Date:   Fri May 15 19:35:59 2020 +0200

    btrfs: simplify iget helpers
    
    The inode lookup starting at btrfs_iget takes the full location key,
    while only the objectid is used to match the inode, because the lookup
    happens inside the given root thus the inode number is unique.
    The entire location key is properly set up in btrfs_init_locked_inode.
    
    Simplify the helpers and pass only inode number, renaming it to 'ino'
    instead of 'objectid'. This allows to remove temporary variables key,
    saving some stack space.
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 14e1464870ab..2c14312b05e8 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -275,7 +275,6 @@ static int __btrfs_run_defrag_inode(struct btrfs_fs_info *fs_info,
 {
 	struct btrfs_root *inode_root;
 	struct inode *inode;
-	struct btrfs_key key;
 	struct btrfs_ioctl_defrag_range_args range;
 	int num_defrag;
 	int ret;
@@ -287,10 +286,7 @@ static int __btrfs_run_defrag_inode(struct btrfs_fs_info *fs_info,
 		goto cleanup;
 	}
 
-	key.objectid = defrag->ino;
-	key.type = BTRFS_INODE_ITEM_KEY;
-	key.offset = 0;
-	inode = btrfs_iget(fs_info->sb, &key, inode_root);
+	inode = btrfs_iget(fs_info->sb, defrag->ino, inode_root);
 	btrfs_put_root(inode_root);
 	if (IS_ERR(inode)) {
 		ret = PTR_ERR(inode);

commit 56e9357a1e8167134388d4c70654795353765c7b
Author: David Sterba <dsterba@suse.com>
Date:   Fri May 15 19:35:55 2020 +0200

    btrfs: simplify root lookup by id
    
    The main function to lookup a root by its id btrfs_get_fs_root takes the
    whole key, while only using the objectid. The value of offset is preset
    to (u64)-1 but not actually used until btrfs_find_root that does the
    actual search.
    
    Switch btrfs_get_fs_root to use only objectid and remove all local
    variables that existed just for the lookup. The actual key for search is
    set up in btrfs_get_fs_root, reusing another key variable.
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 606c2f3c1a38..14e1464870ab 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -281,11 +281,7 @@ static int __btrfs_run_defrag_inode(struct btrfs_fs_info *fs_info,
 	int ret;
 
 	/* get the inode */
-	key.objectid = defrag->root;
-	key.type = BTRFS_ROOT_ITEM_KEY;
-	key.offset = (u64)-1;
-
-	inode_root = btrfs_get_fs_root(fs_info, &key, true);
+	inode_root = btrfs_get_fs_root(fs_info, defrag->root, true);
 	if (IS_ERR(inode_root)) {
 		ret = PTR_ERR(inode_root);
 		goto cleanup;

commit 92a7cc4252231d1641b36c38cf845cfc50308ab0
Author: Qu Wenruo <wqu@suse.com>
Date:   Fri May 15 14:01:40 2020 +0800

    btrfs: rename BTRFS_ROOT_REF_COWS to BTRFS_ROOT_SHAREABLE
    
    The name BTRFS_ROOT_REF_COWS is not very clear about the meaning.
    
    In fact, that bit can only be set to those trees:
    
    - Subvolume roots
    - Data reloc root
    - Reloc roots for above roots
    
    All other trees won't get this bit set.  So just by the result, it is
    obvious that, roots with this bit set can have tree blocks shared with
    other trees.  Either shared by snapshots, or by reloc roots (an special
    snapshot created by relocation).
    
    This patch will rename BTRFS_ROOT_REF_COWS to BTRFS_ROOT_SHAREABLE to
    make it easier to understand, and update all comment mentioning
    "reference counted" to follow the rename.
    
    Signed-off-by: Qu Wenruo <wqu@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 719e68ab552c..606c2f3c1a38 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -775,7 +775,7 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 	if (start >= BTRFS_I(inode)->disk_i_size && !replace_extent)
 		modify_tree = 0;
 
-	update_refs = (test_bit(BTRFS_ROOT_REF_COWS, &root->state) ||
+	update_refs = (test_bit(BTRFS_ROOT_SHAREABLE, &root->state) ||
 		       root == fs_info->tree_root);
 	while (1) {
 		recow = 0;

commit 7af597433d435b56b7c5c8260dad6f979153957b
Author: Filipe Manana <fdmanana@suse.com>
Date:   Tue Apr 7 11:37:44 2020 +0100

    btrfs: make full fsyncs always operate on the entire file again
    
    This is a revert of commit 0a8068a3dd4294 ("btrfs: make ranged full
    fsyncs more efficient"), with updated comment in btrfs_sync_file.
    
    Commit 0a8068a3dd4294 ("btrfs: make ranged full fsyncs more efficient")
    made full fsyncs operate on the given range only as it assumed it was safe
    when using the NO_HOLES feature, since the hole detection was simplified
    some time ago and no longer was a source for races with ordered extent
    completion of adjacent file ranges.
    
    However it's still not safe to have a full fsync only operate on the given
    range, because extent maps for new extents might not be present in memory
    due to inode eviction or extent cloning. Consider the following example:
    
    1) We are currently at transaction N;
    
    2) We write to the file range [0, 1MiB);
    
    3) Writeback finishes for the whole range and ordered extents complete,
       while we are still at transaction N;
    
    4) The inode is evicted;
    
    5) We open the file for writing, causing the inode to be loaded to
       memory again, which sets the 'full sync' bit on its flags. At this
       point the inode's list of modified extent maps is empty (figuring
       out which extents were created in the current transaction and were
       not yet logged by an fsync is expensive, that's why we set the
       'full sync' bit when loading an inode);
    
    6) We write to the file range [512KiB, 768KiB);
    
    7) We do a ranged fsync (such as msync()) for file range [512KiB, 768KiB).
       This correctly flushes this range and logs its extent into the log
       tree. When the writeback started an extent map for range [512KiB, 768KiB)
       was added to the inode's list of modified extents, and when the fsync()
       finishes logging it removes that extent map from the list of modified
       extent maps. This fsync also clears the 'full sync' bit;
    
    8) We do a regular fsync() (full ranged). This fsync() ends up doing
       nothing because the inode's list of modified extents is empty and
       no other changes happened since the previous ranged fsync(), so
       it just returns success (0) and we end up never logging extents for
       the file ranges [0, 512KiB) and [768KiB, 1MiB).
    
    Another scenario where this can happen is if we replace steps 2 to 4 with
    cloning from another file into our test file, as that sets the 'full sync'
    bit in our inode's flags and does not populate its list of modified extent
    maps.
    
    This was causing test case generic/457 to fail sporadically when using the
    NO_HOLES feature, as it exercised this later case where the inode has the
    'full sync' bit set and has no extent maps in memory to represent the new
    extents due to extent cloning.
    
    Fix this by reverting commit 0a8068a3dd4294 ("btrfs: make ranged full fsyncs
    more efficient") since there is no easy way to work around it.
    
    Fixes: 0a8068a3dd4294 ("btrfs: make ranged full fsyncs more efficient")
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 8a144f9cb7ac..719e68ab552c 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2097,6 +2097,21 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 
 	atomic_inc(&root->log_batch);
 
+	/*
+	 * If the inode needs a full sync, make sure we use a full range to
+	 * avoid log tree corruption, due to hole detection racing with ordered
+	 * extent completion for adjacent ranges and races between logging and
+	 * completion of ordered extents for adjancent ranges - both races
+	 * could lead to file extent items in the log with overlapping ranges.
+	 * Do this while holding the inode lock, to avoid races with other
+	 * tasks.
+	 */
+	if (test_bit(BTRFS_INODE_NEEDS_FULL_SYNC,
+		     &BTRFS_I(inode)->runtime_flags)) {
+		start = 0;
+		end = LLONG_MAX;
+	}
+
 	/*
 	 * Before we acquired the inode's lock, someone may have dirtied more
 	 * pages in the target range. We need to make sure that writeback for

commit 6ff06729c22ec0b7498d900d79cc88cfb8aceaeb
Author: Robbie Ko <robbieko@synology.com>
Date:   Tue Mar 17 14:31:02 2020 +0800

    btrfs: fix missing semaphore unlock in btrfs_sync_file
    
    Ordered ops are started twice in sync file, once outside of inode mutex
    and once inside, taking the dio semaphore. There was one error path
    missing the semaphore unlock.
    
    Fixes: aab15e8ec2576 ("Btrfs: fix rare chances for data loss when doing a fast fsync")
    CC: stable@vger.kernel.org # 4.19+
    Signed-off-by: Robbie Ko <robbieko@synology.com>
    Reviewed-by: Filipe Manana <fdmanana@suse.com>
    [ add changelog ]
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 20107f42a766..8a144f9cb7ac 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2117,6 +2117,7 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 	 */
 	ret = start_ordered_ops(inode, start, end);
 	if (ret) {
+		up_write(&BTRFS_I(inode)->dio_sem);
 		inode_unlock(inode);
 		goto out;
 	}

commit c75e839414d3610e6487ae3145199c500d55f7f7
Author: Josef Bacik <josef@toxicpanda.com>
Date:   Fri Feb 14 16:11:47 2020 -0500

    btrfs: kill the subvol_srcu
    
    Now that we have proper root ref counting everywhere we can kill the
    subvol_srcu.
    
    * removal of fs_info::subvol_srcu reduces size of fs_info by 1176 bytes
    
    * the refcount_t used for the references checks for accidental 0->1
      in cases where the root lifetime would not be properly protected
    
    * there's a leak detector for roots to catch unfreed roots at umount
      time
    
    * SRCU served us well over the years but is was not a proper
      synchronization mechanism for some cases
    
    Signed-off-by: Josef Bacik <josef@toxicpanda.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    [ update changelog ]
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 18c88f514a0d..20107f42a766 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -278,7 +278,6 @@ static int __btrfs_run_defrag_inode(struct btrfs_fs_info *fs_info,
 	struct btrfs_key key;
 	struct btrfs_ioctl_defrag_range_args range;
 	int num_defrag;
-	int index;
 	int ret;
 
 	/* get the inode */
@@ -286,8 +285,6 @@ static int __btrfs_run_defrag_inode(struct btrfs_fs_info *fs_info,
 	key.type = BTRFS_ROOT_ITEM_KEY;
 	key.offset = (u64)-1;
 
-	index = srcu_read_lock(&fs_info->subvol_srcu);
-
 	inode_root = btrfs_get_fs_root(fs_info, &key, true);
 	if (IS_ERR(inode_root)) {
 		ret = PTR_ERR(inode_root);
@@ -303,7 +300,6 @@ static int __btrfs_run_defrag_inode(struct btrfs_fs_info *fs_info,
 		ret = PTR_ERR(inode);
 		goto cleanup;
 	}
-	srcu_read_unlock(&fs_info->subvol_srcu, index);
 
 	/* do a chunk of defrag */
 	clear_bit(BTRFS_INODE_IN_DEFRAG, &BTRFS_I(inode)->runtime_flags);
@@ -339,7 +335,6 @@ static int __btrfs_run_defrag_inode(struct btrfs_fs_info *fs_info,
 	iput(inode);
 	return 0;
 cleanup:
-	srcu_read_unlock(&fs_info->subvol_srcu, index);
 	kmem_cache_free(btrfs_inode_defrag_cachep, defrag);
 	return ret;
 }

commit 0a8068a3dd4294ebd8b99969cdc6ccde0f944ba6
Author: Filipe Manana <fdmanana@suse.com>
Date:   Mon Mar 9 12:41:08 2020 +0000

    btrfs: make ranged full fsyncs more efficient
    
    Commit 0c713cbab6200b ("Btrfs: fix race between ranged fsync and writeback
    of adjacent ranges") fixed a bug where we could end up with file extent
    items in a log tree that represent file ranges that overlap due to a race
    between the hole detection of a ranged full fsync and writeback for a
    different file range.
    
    The problem was solved by forcing any ranged full fsync to become a
    non-ranged full fsync - setting the range start to 0 and the end offset to
    LLONG_MAX. This was a simple solution because the code that detected and
    marked holes was very complex, it used to be done at copy_items() and
    implied several searches on the fs/subvolume tree. The drawback of that
    solution was that we started to flush delalloc for the entire file and
    wait for all the ordered extents to complete for ranged full fsyncs
    (including ordered extents covering ranges completely outside the given
    range). Fortunatelly ranged full fsyncs are not the most common case
    (hopefully for most workloads).
    
    However a later fix for detecting and marking holes was made by commit
    0e56315ca147b3 ("Btrfs: fix missing hole after hole punching and fsync
    when using NO_HOLES") and it simplified a lot the detection of holes,
    and now copy_items() no longer does it and we do it in a much more simple
    way at btrfs_log_holes().
    
    This makes it now possible to simply make the code that detects holes to
    operate only on the initial range and no longer need to operate on the
    whole file, while also avoiding the need to flush delalloc for the entire
    file and wait for ordered extents that cover ranges that don't overlap the
    given range.
    
    Another special care is that we must skip file extent items that fall
    entirely outside the fsync range when copying inode items from the
    fs/subvolume tree into the log tree - this is to avoid races with ordered
    extent completion for extents falling outside the fsync range, which could
    cause us to end up with file extent items in the log tree that have
    overlapping ranges - for example if the fsync range is [1Mb, 2Mb], when
    we copy inode items we could copy an extent item for the range [0, 512K],
    then release the search path and before moving to the next leaf, an
    ordered extent for a range of [256Kb, 512Kb] completes - this would
    cause us to copy the new extent item for range [256Kb, 512Kb] into the
    log tree after we have copied one for the range [0, 512Kb] - the extents
    overlap, resulting in a corruption.
    
    So this change just does these steps:
    
    1) When the NO_HOLES feature is enabled it leaves the initial range
       intact - no longer sets it to [0, LLONG_MAX] when the full sync bit
       is set in the inode. If NO_HOLES is not enabled, always set the range
       to a full, just like before this change, to avoid missing file extent
       items representing holes after replaying the log (for both full and
       fast fsyncs);
    
    2) Make the hole detection code to operate only on the fsync range;
    
    3) Make the code that copies items from the fs/subvolume tree to skip
       copying file extent items that cover a range completely outside the
       range of the fsync.
    
    Reviewed-by: Josef Bacik <josef@toxicpanda.com>
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 4a536387e992..18c88f514a0d 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2102,19 +2102,6 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 
 	atomic_inc(&root->log_batch);
 
-	/*
-	 * If the inode needs a full sync, make sure we use a full range to
-	 * avoid log tree corruption, due to hole detection racing with ordered
-	 * extent completion for adjacent ranges, and assertion failures during
-	 * hole detection. Do this while holding the inode lock, to avoid races
-	 * with other tasks.
-	 */
-	if (test_bit(BTRFS_INODE_NEEDS_FULL_SYNC,
-		     &BTRFS_I(inode)->runtime_flags)) {
-		start = 0;
-		end = LLONG_MAX;
-	}
-
 	/*
 	 * Before we acquired the inode's lock, someone may have dirtied more
 	 * pages in the target range. We need to make sure that writeback for

commit 95418ed1d10774cd9a49af6f39e216c1256f1eeb
Author: Filipe Manana <fdmanana@suse.com>
Date:   Mon Mar 9 12:41:05 2020 +0000

    btrfs: fix missing file extent item for hole after ranged fsync
    
    When doing a fast fsync for a range that starts at an offset greater than
    zero, we can end up with a log that when replayed causes the respective
    inode miss a file extent item representing a hole if we are not using the
    NO_HOLES feature. This is because for fast fsyncs we don't log any extents
    that cover a range different from the one requested in the fsync.
    
    Example scenario to trigger it:
    
      $ mkfs.btrfs -O ^no-holes -f /dev/sdd
      $ mount /dev/sdd /mnt
    
      # Create a file with a single 256K and fsync it to clear to full sync
      # bit in the inode - we want the msync below to trigger a fast fsync.
      $ xfs_io -f -c "pwrite -S 0xab 0 256K" -c "fsync" /mnt/foo
    
      # Force a transaction commit and wipe out the log tree.
      $ sync
    
      # Dirty 768K of data, increasing the file size to 1Mb, and flush only
      # the range from 256K to 512K without updating the log tree
      # (sync_file_range() does not trigger fsync, it only starts writeback
      # and waits for it to finish).
    
      $ xfs_io -c "pwrite -S 0xcd 256K 768K" /mnt/foo
      $ xfs_io -c "sync_range -abw 256K 256K" /mnt/foo
    
      # Now dirty the range from 768K to 1M again and sync that range.
      $ xfs_io -c "mmap -w 768K 256K"        \
               -c "mwrite -S 0xef 768K 256K" \
               -c "msync -s 768K 256K"       \
               -c "munmap"                   \
               /mnt/foo
    
      <power fail>
    
      # Mount to replay the log.
      $ mount /dev/sdd /mnt
      $ umount /mnt
    
      $ btrfs check /dev/sdd
      Opening filesystem to check...
      Checking filesystem on /dev/sdd
      UUID: 482fb574-b288-478e-a190-a9c44a78fca6
      [1/7] checking root items
      [2/7] checking extents
      [3/7] checking free space cache
      [4/7] checking fs roots
      root 5 inode 257 errors 100, file extent discount
      Found file extent holes:
           start: 262144, len: 524288
      ERROR: errors found in fs roots
      found 720896 bytes used, error(s) found
      total csum bytes: 512
      total tree bytes: 131072
      total fs tree bytes: 32768
      total extent tree bytes: 16384
      btree space waste bytes: 123514
      file data blocks allocated: 589824
        referenced 589824
    
    Fix this issue by setting the range to full (0 to LLONG_MAX) when the
    NO_HOLES feature is not enabled. This results in extra work being done
    but it gives the guarantee we don't end up with missing holes after
    replaying the log.
    
    CC: stable@vger.kernel.org # 4.19+
    Reviewed-by: Josef Bacik <josef@toxicpanda.com>
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 31c72371a164..4a536387e992 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2071,6 +2071,16 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 
 	btrfs_init_log_ctx(&ctx, inode);
 
+	/*
+	 * Set the range to full if the NO_HOLES feature is not enabled.
+	 * This is to avoid missing file extent items representing holes after
+	 * replaying the log.
+	 */
+	if (!btrfs_fs_incompat(fs_info, NO_HOLES)) {
+		start = 0;
+		end = LLONG_MAX;
+	}
+
 	/*
 	 * We write the dirty pages in the range and wait until they complete
 	 * out of the ->i_mutex. If so, we can flush the dirty pages by

commit 6a177381007b463ad611375cce526c24f12ab081
Author: Filipe Manana <fdmanana@suse.com>
Date:   Fri Feb 28 13:04:17 2020 +0000

    Btrfs: move all reflink implementation code into its own file
    
    The reflink code is quite large and has been living in ioctl.c since ever.
    It has grown over the years after many bug fixes and improvements, and
    since I'm planning on making some further improvements on it, it's time
    to get it better organized by moving into its own file, reflink.c
    (similar to what xfs does for example).
    
    This change only moves the code out of ioctl.c into the new file, it
    doesn't do any other change.
    
    Reviewed-by: Josef Bacik <josef@toxicpanda.com>
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 8a974a82be51..31c72371a164 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -27,6 +27,7 @@
 #include "qgroup.h"
 #include "compression.h"
 #include "delalloc-space.h"
+#include "reflink.h"
 
 static struct kmem_cache *btrfs_inode_defrag_cachep;
 /*

commit dcc3eb9638c3c927f1597075e851d0a16300a876
Author: Nikolay Borisov <nborisov@suse.com>
Date:   Thu Jan 30 14:59:45 2020 +0200

    btrfs: convert snapshot/nocow exlcusion to drew lock
    
    This patch removes all haphazard code implementing nocow writers
    exclusion from pending snapshot creation and switches to using the drew
    lock to ensure this invariant still holds.
    
    'Readers' are snapshot creators from create_snapshot and 'writers' are
    nocow writers from buffered write path or btrfs_setsize. This locking
    scheme allows for multiple snapshots to happen while any nocow writers
    are blocked, since writes to page cache in the nocow path will make
    snapshots inconsistent.
    
    So for performance reasons we'd like to have the ability to run multiple
    concurrent snapshots and also favors readers in this case. And in case
    there aren't pending snapshots (which will be the majority of the cases)
    we rely on the percpu's writers counter to avoid cacheline contention.
    
    The main gain from using the drew lock is it's now a lot easier to
    reason about the guarantees of the locking scheme and whether there is
    some silent breakage lurking.
    
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index fd52ad00b6c8..8a974a82be51 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1553,8 +1553,7 @@ static noinline int check_can_nocow(struct btrfs_inode *inode, loff_t pos,
 	u64 num_bytes;
 	int ret;
 
-	ret = btrfs_start_write_no_snapshotting(root);
-	if (!ret)
+	if (!btrfs_drew_try_write_lock(&root->snapshot_lock))
 		return -EAGAIN;
 
 	lockstart = round_down(pos, fs_info->sectorsize);
@@ -1569,7 +1568,7 @@ static noinline int check_can_nocow(struct btrfs_inode *inode, loff_t pos,
 			NULL, NULL, NULL);
 	if (ret <= 0) {
 		ret = 0;
-		btrfs_end_write_no_snapshotting(root);
+		btrfs_drew_write_unlock(&root->snapshot_lock);
 	} else {
 		*write_bytes = min_t(size_t, *write_bytes ,
 				     num_bytes - pos + lockstart);
@@ -1675,7 +1674,7 @@ static noinline ssize_t btrfs_buffered_write(struct kiocb *iocb,
 						data_reserved, pos,
 						write_bytes);
 			else
-				btrfs_end_write_no_snapshotting(root);
+				btrfs_drew_write_unlock(&root->snapshot_lock);
 			break;
 		}
 
@@ -1779,7 +1778,7 @@ static noinline ssize_t btrfs_buffered_write(struct kiocb *iocb,
 
 		release_bytes = 0;
 		if (only_release_metadata)
-			btrfs_end_write_no_snapshotting(root);
+			btrfs_drew_write_unlock(&root->snapshot_lock);
 
 		if (only_release_metadata && copied > 0) {
 			lockstart = round_down(pos,
@@ -1808,7 +1807,7 @@ static noinline ssize_t btrfs_buffered_write(struct kiocb *iocb,
 
 	if (release_bytes) {
 		if (only_release_metadata) {
-			btrfs_end_write_no_snapshotting(root);
+			btrfs_drew_write_unlock(&root->snapshot_lock);
 			btrfs_delalloc_release_metadata(BTRFS_I(inode),
 					release_bytes, true);
 		} else {

commit b272ae22acd2ca688bbf9d94eea4b1da61fdc697
Author: David Sterba <dsterba@suse.com>
Date:   Wed Feb 5 19:09:33 2020 +0100

    btrfs: drop argument tree from btrfs_lock_and_flush_ordered_range
    
    The tree pointer can be safely read from the inode so we can drop the
    redundant argument from btrfs_lock_and_flush_ordered_range.
    
    Reviewed-by: Johannes Thumshirn <johannes.thumshirn@wdc.com>
    Reviewed-by: Nikolay Borisov <nborisov@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 3f7f5323aefa..fd52ad00b6c8 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1561,7 +1561,7 @@ static noinline int check_can_nocow(struct btrfs_inode *inode, loff_t pos,
 	lockend = round_up(pos + *write_bytes,
 			   fs_info->sectorsize) - 1;
 
-	btrfs_lock_and_flush_ordered_range(&inode->io_tree, inode, lockstart,
+	btrfs_lock_and_flush_ordered_range(inode, lockstart,
 					   lockend, NULL);
 
 	num_bytes = lockend - lockstart + 1;

commit 0024652895e3479cd0d372f63b57d9581a0bdd38
Author: Josef Bacik <josef@toxicpanda.com>
Date:   Fri Jan 24 09:33:01 2020 -0500

    btrfs: rename btrfs_put_fs_root and btrfs_grab_fs_root
    
    We are now using these for all roots, rename them to btrfs_put_root()
    and btrfs_grab_root();
    
    Reviewed-by: Nikolay Borisov <nborisov@suse.com>
    Signed-off-by: Josef Bacik <josef@toxicpanda.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index ba19b4debc61..3f7f5323aefa 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -297,7 +297,7 @@ static int __btrfs_run_defrag_inode(struct btrfs_fs_info *fs_info,
 	key.type = BTRFS_INODE_ITEM_KEY;
 	key.offset = 0;
 	inode = btrfs_iget(fs_info->sb, &key, inode_root);
-	btrfs_put_fs_root(inode_root);
+	btrfs_put_root(inode_root);
 	if (IS_ERR(inode)) {
 		ret = PTR_ERR(inode);
 		goto cleanup;

commit bc44d7c4b2b179c4b74fba208b9908e2ecbc1b4d
Author: Josef Bacik <josef@toxicpanda.com>
Date:   Fri Jan 24 09:32:56 2020 -0500

    btrfs: push btrfs_grab_fs_root into btrfs_get_fs_root
    
    Now that all callers of btrfs_get_fs_root are subsequently calling
    btrfs_grab_fs_root and handling dropping the ref when they are done
    appropriately, go ahead and push btrfs_grab_fs_root up into
    btrfs_get_fs_root.
    
    Signed-off-by: Josef Bacik <josef@toxicpanda.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 27237bbaad65..ba19b4debc61 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -292,10 +292,6 @@ static int __btrfs_run_defrag_inode(struct btrfs_fs_info *fs_info,
 		ret = PTR_ERR(inode_root);
 		goto cleanup;
 	}
-	if (!btrfs_grab_fs_root(inode_root)) {
-		ret = -ENOENT;
-		goto cleanup;
-	}
 
 	key.objectid = defrag->ino;
 	key.type = BTRFS_INODE_ITEM_KEY;

commit 02162a0265eb56742442ef42c58db8739ddd9b94
Author: Josef Bacik <josef@toxicpanda.com>
Date:   Fri Jan 24 09:32:30 2020 -0500

    btrfs: hold a ref on the root in __btrfs_run_defrag_inode
    
    We are looking up an arbitrary inode, we need to hold a ref on the root
    while we're doing this.
    
    Signed-off-by: Josef Bacik <josef@toxicpanda.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 1bc7665f8ff0..27237bbaad65 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -292,11 +292,16 @@ static int __btrfs_run_defrag_inode(struct btrfs_fs_info *fs_info,
 		ret = PTR_ERR(inode_root);
 		goto cleanup;
 	}
+	if (!btrfs_grab_fs_root(inode_root)) {
+		ret = -ENOENT;
+		goto cleanup;
+	}
 
 	key.objectid = defrag->ino;
 	key.type = BTRFS_INODE_ITEM_KEY;
 	key.offset = 0;
 	inode = btrfs_iget(fs_info->sb, &key, inode_root);
+	btrfs_put_fs_root(inode_root);
 	if (IS_ERR(inode)) {
 		ret = PTR_ERR(inode);
 		goto cleanup;

commit 3619c94f073e4e96bef4cc15e70adbc36f3cb203
Author: Josef Bacik <josef@toxicpanda.com>
Date:   Fri Jan 24 09:32:24 2020 -0500

    btrfs: open code btrfs_read_fs_root_no_name
    
    All this does is call btrfs_get_fs_root() with check_ref == true.  Just
    use btrfs_get_fs_root() so we don't have a bunch of different helpers
    that do the same thing.
    
    Reviewed-by: Nikolay Borisov <nborisov@suse.com>
    Signed-off-by: Josef Bacik <josef@toxicpanda.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 06a2f0f2e602..1bc7665f8ff0 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -287,7 +287,7 @@ static int __btrfs_run_defrag_inode(struct btrfs_fs_info *fs_info,
 
 	index = srcu_read_lock(&fs_info->subvol_srcu);
 
-	inode_root = btrfs_read_fs_root_no_name(fs_info, &key);
+	inode_root = btrfs_get_fs_root(fs_info, &key, true);
 	if (IS_ERR(inode_root)) {
 		ret = PTR_ERR(inode_root);
 		goto cleanup;

commit d923afe96d7eabbe868a478bae0997dfecb8a5a3
Author: Josef Bacik <josef@toxicpanda.com>
Date:   Fri Jan 17 09:02:23 2020 -0500

    btrfs: replace all uses of btrfs_ordered_update_i_size
    
    Now that we have a safe way to update the i_size, replace all uses of
    btrfs_ordered_update_i_size with btrfs_inode_safe_disk_i_size_write.
    
    Reviewed-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: Josef Bacik <josef@toxicpanda.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 200eb14b6721..06a2f0f2e602 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2967,7 +2967,7 @@ static int btrfs_fallocate_update_isize(struct inode *inode,
 
 	inode->i_ctime = current_time(inode);
 	i_size_write(inode, end);
-	btrfs_ordered_update_i_size(inode, end, NULL);
+	btrfs_inode_safe_disk_i_size_write(inode, 0);
 	ret = btrfs_update_inode(trans, root, inode);
 	ret2 = btrfs_end_transaction(trans);
 

commit 9ddc959e802bf7555a0be543205ddcba2bae98bf
Author: Josef Bacik <josef@toxicpanda.com>
Date:   Fri Jan 17 09:02:22 2020 -0500

    btrfs: use the file extent tree infrastructure
    
    We want to use this everywhere we modify the file extent items
    permanently.  These include:
    
      1) Inserting new file extents for writes and prealloc extents.
      2) Truncating inode items.
      3) btrfs_cont_expand().
      4) Insert inline extents.
      5) Insert new extents from log replay.
      6) Insert a new extent for clone, as it could be past i_size.
      7) Hole punching
    
    For hole punching in particular it might seem it's not necessary because
    anybody extending would use btrfs_cont_expand, however there is a corner
    that still can give us trouble.  Start with an empty file and
    
    fallocate KEEP_SIZE 1M-2M
    
    We now have a 0 length file, and a hole file extent from 0-1M, and a
    prealloc extent from 1M-2M.  Now
    
    punch 1M-1.5M
    
    Because this is past i_size we have
    
    [HOLE EXTENT][ NOTHING ][PREALLOC]
    [0        1M][1M   1.5M][1.5M  2M]
    
    with an i_size of 0.  Now if we pwrite 0-1.5M we'll increas our i_size
    to 1.5M, but our disk_i_size is still 0 until the ordered extent
    completes.
    
    However if we now immediately truncate 2M on the file we'll just call
    btrfs_cont_expand(inode, 1.5M, 2M), since our old i_size is 1.5M.  If we
    commit the transaction here and crash we'll expose the gap.
    
    To fix this we need to clear the file extent mapping for the range that
    we punched but didn't insert a corresponding file extent for.  This will
    mean the truncate will only get an disk_i_size set to 1M if we crash
    before the finish ordered io happens.
    
    I've written an xfstest to reproduce the problem and validate this fix.
    
    Reviewed-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: Josef Bacik <josef@toxicpanda.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index a16da274c9aa..200eb14b6721 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2486,6 +2486,11 @@ static int btrfs_insert_clone_extent(struct btrfs_trans_handle *trans,
 	btrfs_mark_buffer_dirty(leaf);
 	btrfs_release_path(path);
 
+	ret = btrfs_inode_set_file_extent_range(BTRFS_I(inode),
+			clone_info->file_offset, clone_len);
+	if (ret)
+		return ret;
+
 	/* If it's a hole, nothing more needs to be done. */
 	if (clone_info->disk_offset == 0)
 		return 0;
@@ -2596,6 +2601,24 @@ int btrfs_punch_hole_range(struct inode *inode, struct btrfs_path *path,
 				btrfs_abort_transaction(trans, ret);
 				break;
 			}
+		} else if (!clone_info && cur_offset < drop_end) {
+			/*
+			 * We are past the i_size here, but since we didn't
+			 * insert holes we need to clear the mapped area so we
+			 * know to not set disk_i_size in this area until a new
+			 * file extent is inserted here.
+			 */
+			ret = btrfs_inode_clear_file_extent_range(BTRFS_I(inode),
+					cur_offset, drop_end - cur_offset);
+			if (ret) {
+				/*
+				 * We couldn't clear our area, so we could
+				 * presumably adjust up and corrupt the fs, so
+				 * we need to abort.
+				 */
+				btrfs_abort_transaction(trans, ret);
+				break;
+			}
 		}
 
 		if (clone_info && drop_end > clone_info->file_offset) {
@@ -2686,6 +2709,15 @@ int btrfs_punch_hole_range(struct inode *inode, struct btrfs_path *path,
 			btrfs_abort_transaction(trans, ret);
 			goto out_trans;
 		}
+	} else if (!clone_info && cur_offset < drop_end) {
+		/* See the comment in the loop above for the reasoning here. */
+		ret = btrfs_inode_clear_file_extent_range(BTRFS_I(inode),
+					cur_offset, drop_end - cur_offset);
+		if (ret) {
+			btrfs_abort_transaction(trans, ret);
+			goto out_trans;
+		}
+
 	}
 	if (clone_info) {
 		ret = btrfs_insert_clone_extent(trans, inode, path, clone_info,

commit 39b07b5d7072f8e9fd8cc2f840d3749f86699bbb
Author: Omar Sandoval <osandov@fb.com>
Date:   Mon Dec 2 17:34:23 2019 -0800

    btrfs: drop create parameter to btrfs_get_extent()
    
    We only pass this as 1 from __extent_writepage_io(). The parameter
    basically means "pretend I didn't pass in a page". This is silly since
    we can simply not pass in the page. Get rid of the parameter from
    btrfs_get_extent(), and since it's used as a get_extent_t callback,
    remove it from get_extent_t and btree_get_extent(), neither of which
    need it.
    
    While we're here, let's document btrfs_get_extent().
    
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 76c68c70d3e2..a16da274c9aa 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -477,8 +477,7 @@ static int btrfs_find_new_delalloc_bytes(struct btrfs_inode *inode,
 		u64 em_len;
 		int ret = 0;
 
-		em = btrfs_get_extent(inode, NULL, 0, search_start,
-				      search_len, 0);
+		em = btrfs_get_extent(inode, NULL, 0, search_start, search_len);
 		if (IS_ERR(em))
 			return PTR_ERR(em);
 
@@ -2390,7 +2389,7 @@ static int find_first_non_hole(struct inode *inode, u64 *start, u64 *len)
 
 	em = btrfs_get_extent(BTRFS_I(inode), NULL, 0,
 			      round_down(*start, fs_info->sectorsize),
-			      round_up(*len, fs_info->sectorsize), 0);
+			      round_up(*len, fs_info->sectorsize));
 	if (IS_ERR(em))
 		return PTR_ERR(em);
 
@@ -2957,7 +2956,7 @@ static int btrfs_zero_range_check_range_boundary(struct inode *inode,
 	int ret;
 
 	offset = round_down(offset, sectorsize);
-	em = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);
+	em = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize);
 	if (IS_ERR(em))
 		return PTR_ERR(em);
 
@@ -2990,8 +2989,8 @@ static int btrfs_zero_range(struct inode *inode,
 
 	inode_dio_wait(inode);
 
-	em = btrfs_get_extent(BTRFS_I(inode), NULL, 0,
-			      alloc_start, alloc_end - alloc_start, 0);
+	em = btrfs_get_extent(BTRFS_I(inode), NULL, 0, alloc_start,
+			      alloc_end - alloc_start);
 	if (IS_ERR(em)) {
 		ret = PTR_ERR(em);
 		goto out;
@@ -3034,8 +3033,8 @@ static int btrfs_zero_range(struct inode *inode,
 
 	if (BTRFS_BYTES_TO_BLKS(fs_info, offset) ==
 	    BTRFS_BYTES_TO_BLKS(fs_info, offset + len - 1)) {
-		em = btrfs_get_extent(BTRFS_I(inode), NULL, 0,
-				      alloc_start, sectorsize, 0);
+		em = btrfs_get_extent(BTRFS_I(inode), NULL, 0, alloc_start,
+				      sectorsize);
 		if (IS_ERR(em)) {
 			ret = PTR_ERR(em);
 			goto out;
@@ -3273,7 +3272,7 @@ static long btrfs_fallocate(struct file *file, int mode,
 	INIT_LIST_HEAD(&reserve_list);
 	while (cur_offset < alloc_end) {
 		em = btrfs_get_extent(BTRFS_I(inode), NULL, 0, cur_offset,
-				      alloc_end - cur_offset, 0);
+				      alloc_end - cur_offset);
 		if (IS_ERR(em)) {
 			ret = PTR_ERR(em);
 			break;

commit bffe633e00fb6b904817137fc17a44b42efcd985
Author: Omar Sandoval <osandov@fb.com>
Date:   Mon Dec 2 17:34:19 2019 -0800

    btrfs: make btrfs_ordered_extent naming consistent with btrfs_file_extent_item
    
    ordered->start, ordered->len, and ordered->disk_len correspond to
    fi->disk_bytenr, fi->num_bytes, and fi->disk_num_bytes, respectively.
    It's confusing to translate between the two naming schemes. Since a
    btrfs_ordered_extent is basically a pending btrfs_file_extent_item,
    let's make the former use the naming from the latter.
    
    Note that I didn't touch the names in tracepoints just in case there are
    scripts depending on the current naming.
    
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 8d47c76b7bd1..76c68c70d3e2 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1501,7 +1501,7 @@ lock_and_cleanup_extent_if_need(struct btrfs_inode *inode, struct page **pages,
 		ordered = btrfs_lookup_ordered_range(inode, start_pos,
 						     last_pos - start_pos + 1);
 		if (ordered &&
-		    ordered->file_offset + ordered->len > start_pos &&
+		    ordered->file_offset + ordered->num_bytes > start_pos &&
 		    ordered->file_offset <= last_pos) {
 			unlock_extent_cached(&inode->io_tree, start_pos,
 					last_pos, cached_state);
@@ -2426,7 +2426,7 @@ static int btrfs_punch_hole_lock_range(struct inode *inode,
 		 * we need to try again.
 		 */
 		if ((!ordered ||
-		    (ordered->file_offset + ordered->len <= lockstart ||
+		    (ordered->file_offset + ordered->num_bytes <= lockstart ||
 		     ordered->file_offset > lockend)) &&
 		     !filemap_range_has_page(inode->i_mapping,
 					     lockstart, lockend)) {
@@ -3248,7 +3248,7 @@ static long btrfs_fallocate(struct file *file, int mode,
 		ordered = btrfs_lookup_first_ordered_extent(inode, locked_end);
 
 		if (ordered &&
-		    ordered->file_offset + ordered->len > alloc_start &&
+		    ordered->file_offset + ordered->num_bytes > alloc_start &&
 		    ordered->file_offset < alloc_end) {
 			btrfs_put_ordered_extent(ordered);
 			unlock_extent_cached(&BTRFS_I(inode)->io_tree,

commit fcb970581dd900675c4371c2b688a57924a8368c
Author: Filipe Manana <fdmanana@suse.com>
Date:   Thu Dec 5 16:57:39 2019 +0000

    Btrfs: fix cloning range with a hole when using the NO_HOLES feature
    
    When using the NO_HOLES feature if we clone a range that contains a hole
    and a temporary ENOSPC happens while dropping extents from the target
    inode's range, we can end up failing and aborting the transaction with
    -EEXIST or with a corrupt file extent item, that has a length greater
    than it should and overlaps with other extents. For example when cloning
    the following range from inode A to inode B:
    
      Inode A:
    
        extent A1                                          extent A2
      [ ----------- ]  [ hole, implicit, 4MB length ]  [ ------------- ]
      0            1MB                                 5MB            6MB
    
      Range to clone: [1MB, 6MB)
    
      Inode B:
    
        extent B1       extent B2        extent B3         extent B4
      [ ---------- ]  [ --------- ]    [ ---------- ]    [ ---------- ]
      0           1MB 1MB        2MB   2MB        5MB    5MB         6MB
    
      Target range: [1MB, 6MB) (same as source, to make it easier to explain)
    
    The following can happen:
    
    1) btrfs_punch_hole_range() gets -ENOSPC from __btrfs_drop_extents();
    
    2) At that point, 'cur_offset' is set to 1MB and __btrfs_drop_extents()
       set 'drop_end' to 2MB, meaning it was able to drop only extent B2;
    
    3) We then compute 'clone_len' as 'drop_end' - 'cur_offset' = 2MB - 1MB =
       1MB;
    
    4) We then attempt to insert a file extent item at inode B with a file
       offset of 5MB, which is the value of clone_info->file_offset. This
       fails with error -EEXIST because there's already an extent at that
       offset (extent B4);
    
    5) We abort the current transaction with -EEXIST and return that error
       to user space as well.
    
    Another example, for extent corruption:
    
      Inode A:
    
        extent A1                                           extent A2
      [ ----------- ]   [ hole, implicit, 10MB length ]  [ ------------- ]
      0            1MB                                  11MB            12MB
    
      Inode B:
    
        extent B1         extent B2
      [ ----------- ]   [ --------- ]    [ ----------------------------- ]
      0            1MB 1MB         5MB  5MB                             12MB
    
      Target range: [1MB, 12MB) (same as source, to make it easier to explain)
    
    1) btrfs_punch_hole_range() gets -ENOSPC from __btrfs_drop_extents();
    
    2) At that point, 'cur_offset' is set to 1MB and __btrfs_drop_extents()
       set 'drop_end' to 5MB, meaning it was able to drop only extent B2;
    
    3) We then compute 'clone_len' as 'drop_end' - 'cur_offset' = 5MB - 1MB =
       4MB;
    
    4) We then insert a file extent item at inode B with a file offset of 11MB
       which is the value of clone_info->file_offset, and a length of 4MB (the
       value of 'clone_len'). So we get 2 extents items with ranges that
       overlap and an extent length of 4MB, larger then the extent A2 from
       inode A (1MB length);
    
    5) After that we end the transaction, balance the btree dirty pages and
       then start another or join the previous transaction. It might happen
       that the transaction which inserted the incorrect extent was committed
       by another task so we end up with extent corruption if a power failure
       happens.
    
    So fix this by making sure we attempt to insert the extent to clone at
    the destination inode only if we are past dropping the sub-range that
    corresponds to a hole.
    
    Fixes: 690a5dbfc51315 ("Btrfs: fix ENOSPC errors, leading to transaction aborts, when cloning extents")
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 0cb43b682789..8d47c76b7bd1 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2599,8 +2599,8 @@ int btrfs_punch_hole_range(struct inode *inode, struct btrfs_path *path,
 			}
 		}
 
-		if (clone_info) {
-			u64 clone_len = drop_end - cur_offset;
+		if (clone_info && drop_end > clone_info->file_offset) {
+			u64 clone_len = drop_end - clone_info->file_offset;
 
 			ret = btrfs_insert_clone_extent(trans, inode, path,
 							clone_info, clone_len);

commit a019e9e197eaa68ffe2efeba00d685581b1a5416
Author: David Sterba <dsterba@suse.com>
Date:   Fri Aug 30 15:40:53 2019 +0200

    btrfs: remove extent_map::bdev
    
    We can now remove the bdev from extent_map. Previous patches made sure
    that bio_set_dev is correctly in all places and that we don't need to
    grab it from latest_bdev or pass it around inside the extent map.
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 32e620981485..0cb43b682789 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -667,7 +667,6 @@ void btrfs_drop_extent_cache(struct btrfs_inode *inode, u64 start, u64 end,
 			}
 
 			split->generation = gen;
-			split->bdev = em->bdev;
 			split->flags = flags;
 			split->compress_type = em->compress_type;
 			replace_extent_mapping(em_tree, em, split, modified);
@@ -680,7 +679,6 @@ void btrfs_drop_extent_cache(struct btrfs_inode *inode, u64 start, u64 end,
 
 			split->start = start + len;
 			split->len = em->start + em->len - (start + len);
-			split->bdev = em->bdev;
 			split->flags = flags;
 			split->compress_type = em->compress_type;
 			split->generation = gen;
@@ -2360,7 +2358,6 @@ static int fill_holes(struct btrfs_trans_handle *trans,
 		hole_em->block_start = EXTENT_MAP_HOLE;
 		hole_em->block_len = 0;
 		hole_em->orig_block_len = 0;
-		hole_em->bdev = fs_info->fs_devices->latest_bdev;
 		hole_em->compress_type = BTRFS_COMPRESS_NONE;
 		hole_em->generation = trans->transid;
 

commit bc80230e0e7bc779cd553c2326ea5b3a5bac303b
Author: Nikolay Borisov <nborisov@suse.com>
Date:   Fri Sep 27 13:23:18 2019 +0300

    btrfs: Return offset from find_desired_extent
    
    Instead of using an input pointer parameter as the return value and have
    an int as the return type of find_desired_extent, rework the function to
    directly return the found offset. Doing that the 'ret' variable in
    btrfs_llseek_file can be removed. Additional (subjective) benefit is
    that btrfs' llseek function now resemebles those of the other major
    filesystems.
    
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 949212289c89..32e620981485 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -3351,7 +3351,8 @@ static long btrfs_fallocate(struct file *file, int mode,
 	return ret;
 }
 
-static int find_desired_extent(struct inode *inode, loff_t *offset, int whence)
+static loff_t find_desired_extent(struct inode *inode, loff_t offset,
+				  int whence)
 {
 	struct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);
 	struct extent_map *em = NULL;
@@ -3363,14 +3364,14 @@ static int find_desired_extent(struct inode *inode, loff_t *offset, int whence)
 	u64 len;
 	int ret = 0;
 
-	if (i_size == 0 || *offset >= i_size)
+	if (i_size == 0 || offset >= i_size)
 		return -ENXIO;
 
 	/*
-	 * *offset can be negative, in this case we start finding DATA/HOLE from
+	 * offset can be negative, in this case we start finding DATA/HOLE from
 	 * the very start of the file.
 	 */
-	start = max_t(loff_t, 0, *offset);
+	start = max_t(loff_t, 0, offset);
 
 	lockstart = round_down(start, fs_info->sectorsize);
 	lockend = round_up(i_size, fs_info->sectorsize);
@@ -3405,21 +3406,23 @@ static int find_desired_extent(struct inode *inode, loff_t *offset, int whence)
 		cond_resched();
 	}
 	free_extent_map(em);
-	if (!ret) {
+	unlock_extent_cached(&BTRFS_I(inode)->io_tree, lockstart, lockend,
+			     &cached_state);
+	if (ret) {
+		offset = ret;
+	} else {
 		if (whence == SEEK_DATA && start >= i_size)
-			ret = -ENXIO;
+			offset = -ENXIO;
 		else
-			*offset = min_t(loff_t, start, i_size);
+			offset = min_t(loff_t, start, i_size);
 	}
-	unlock_extent_cached(&BTRFS_I(inode)->io_tree, lockstart, lockend,
-			     &cached_state);
-	return ret;
+
+	return offset;
 }
 
 static loff_t btrfs_file_llseek(struct file *file, loff_t offset, int whence)
 {
 	struct inode *inode = file->f_mapping->host;
-	int ret;
 
 	switch (whence) {
 	default:
@@ -3427,13 +3430,14 @@ static loff_t btrfs_file_llseek(struct file *file, loff_t offset, int whence)
 	case SEEK_DATA:
 	case SEEK_HOLE:
 		inode_lock_shared(inode);
-		ret = find_desired_extent(inode, &offset, whence);
+		offset = find_desired_extent(inode, offset, whence);
 		inode_unlock_shared(inode);
-
-		if (ret)
-			return ret;
+		break;
 	}
 
+	if (offset < 0)
+		return offset;
+
 	return vfs_setpos(file, offset, inode->i_sb->s_maxbytes);
 }
 

commit 2034f3b470ccfd4103737cf4d2a25a52ce9ad002
Author: Nikolay Borisov <nborisov@suse.com>
Date:   Fri Sep 27 13:23:17 2019 +0300

    btrfs: Simplify btrfs_file_llseek
    
    Handle SEEK_END/SEEK_CUR in a single 'default' case by directly
    returning from generic_file_llseek. This makes the 'out' label
    redundant.  Finally return directly the vale from vfs_setpos. No
    semantic changes.
    
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 35cfc738f6b7..949212289c89 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -3422,10 +3422,8 @@ static loff_t btrfs_file_llseek(struct file *file, loff_t offset, int whence)
 	int ret;
 
 	switch (whence) {
-	case SEEK_END:
-	case SEEK_CUR:
-		offset = generic_file_llseek(file, offset, whence);
-		goto out;
+	default:
+		return generic_file_llseek(file, offset, whence);
 	case SEEK_DATA:
 	case SEEK_HOLE:
 		inode_lock_shared(inode);
@@ -3436,9 +3434,7 @@ static loff_t btrfs_file_llseek(struct file *file, loff_t offset, int whence)
 			return ret;
 	}
 
-	offset = vfs_setpos(file, offset, inode->i_sb->s_maxbytes);
-out:
-	return offset;
+	return vfs_setpos(file, offset, inode->i_sb->s_maxbytes);
 }
 
 static int btrfs_file_open(struct inode *inode, struct file *filp)

commit d79b7c26b12279038b8b84bfcbe7bb3e53382b3f
Author: Nikolay Borisov <nborisov@suse.com>
Date:   Fri Sep 27 13:23:16 2019 +0300

    btrfs: Speed up btrfs_file_llseek
    
    Modifying the file position is done on a per-file basis. This renders
    holding the inode lock for writing useless and makes the performance of
    concurrent llseek's abysmal.
    
    Fix this by holding the inode for read. This provides protection against
    concurrent truncates and find_desired_extent already includes proper
    extent locking for the range which ensures proper locking against
    concurrent writes. SEEK_CUR and SEEK_END can be done lockessly.
    
    The former is synchronized by file::f_lock spinlock. SEEK_END is not
    synchronized but atomic, but that's OK since there is not guarantee that
    SEEK_END will always be at the end of the file in the face of tail
    modifications.
    
    This change brings ~82% performance improvement when doing a lot of
    parallel fseeks. The workload essentially does:
    
        for (d=0; d<num_seek_read; d++)
          {
            /* offset %= 16777216; */
            fseek (f, 256 * d % 16777216, SEEK_SET);
            fread (buffer, 64, 1, f);
          }
    
    Without patch:
    
    num workprocesses = 16
    num fseek/fread = 8000000
    step = 256
    fork 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
    
    real    0m41.412s
    user    0m28.777s
    sys     2m16.510s
    
    With patch:
    
    num workprocesses = 16
    num fseek/fread = 8000000
    step = 256
    fork 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
    
    real    0m11.479s
    user    0m27.629s
    sys     0m21.040s
    
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 91c98a6d1408..35cfc738f6b7 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -3356,13 +3356,14 @@ static int find_desired_extent(struct inode *inode, loff_t *offset, int whence)
 	struct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);
 	struct extent_map *em = NULL;
 	struct extent_state *cached_state = NULL;
+	loff_t i_size = inode->i_size;
 	u64 lockstart;
 	u64 lockend;
 	u64 start;
 	u64 len;
 	int ret = 0;
 
-	if (inode->i_size == 0)
+	if (i_size == 0 || *offset >= i_size)
 		return -ENXIO;
 
 	/*
@@ -3372,8 +3373,7 @@ static int find_desired_extent(struct inode *inode, loff_t *offset, int whence)
 	start = max_t(loff_t, 0, *offset);
 
 	lockstart = round_down(start, fs_info->sectorsize);
-	lockend = round_up(i_size_read(inode),
-			   fs_info->sectorsize);
+	lockend = round_up(i_size, fs_info->sectorsize);
 	if (lockend <= lockstart)
 		lockend = lockstart + fs_info->sectorsize;
 	lockend--;
@@ -3382,7 +3382,7 @@ static int find_desired_extent(struct inode *inode, loff_t *offset, int whence)
 	lock_extent_bits(&BTRFS_I(inode)->io_tree, lockstart, lockend,
 			 &cached_state);
 
-	while (start < inode->i_size) {
+	while (start < i_size) {
 		em = btrfs_get_extent_fiemap(BTRFS_I(inode), start, len);
 		if (IS_ERR(em)) {
 			ret = PTR_ERR(em);
@@ -3406,10 +3406,10 @@ static int find_desired_extent(struct inode *inode, loff_t *offset, int whence)
 	}
 	free_extent_map(em);
 	if (!ret) {
-		if (whence == SEEK_DATA && start >= inode->i_size)
+		if (whence == SEEK_DATA && start >= i_size)
 			ret = -ENXIO;
 		else
-			*offset = min_t(loff_t, start, inode->i_size);
+			*offset = min_t(loff_t, start, i_size);
 	}
 	unlock_extent_cached(&BTRFS_I(inode)->io_tree, lockstart, lockend,
 			     &cached_state);
@@ -3421,7 +3421,6 @@ static loff_t btrfs_file_llseek(struct file *file, loff_t offset, int whence)
 	struct inode *inode = file->f_mapping->host;
 	int ret;
 
-	inode_lock(inode);
 	switch (whence) {
 	case SEEK_END:
 	case SEEK_CUR:
@@ -3429,21 +3428,16 @@ static loff_t btrfs_file_llseek(struct file *file, loff_t offset, int whence)
 		goto out;
 	case SEEK_DATA:
 	case SEEK_HOLE:
-		if (offset >= i_size_read(inode)) {
-			inode_unlock(inode);
-			return -ENXIO;
-		}
-
+		inode_lock_shared(inode);
 		ret = find_desired_extent(inode, &offset, whence);
-		if (ret) {
-			inode_unlock(inode);
+		inode_unlock_shared(inode);
+
+		if (ret)
 			return ret;
-		}
 	}
 
 	offset = vfs_setpos(file, offset, inode->i_sb->s_maxbytes);
 out:
-	inode_unlock(inode);
 	return offset;
 }
 

commit a0e248bb502d5165b3314ac3819e888fdcdf7d9f
Author: Filipe Manana <fdmanana@suse.com>
Date:   Fri Oct 11 16:41:20 2019 +0100

    Btrfs: fix negative subv_writers counter and data space leak after buffered write
    
    When doing a buffered write it's possible to leave the subv_writers
    counter of the root, used for synchronization between buffered nocow
    writers and snapshotting. This happens in an exceptional case like the
    following:
    
    1) We fail to allocate data space for the write, since there's not
       enough available data space nor enough unallocated space for allocating
       a new data block group;
    
    2) Because of that failure, we try to go to NOCOW mode, which succeeds
       and therefore we set the local variable 'only_release_metadata' to true
       and set the root's sub_writers counter to 1 through the call to
       btrfs_start_write_no_snapshotting() made by check_can_nocow();
    
    3) The call to btrfs_copy_from_user() returns zero, which is very unlikely
       to happen but not impossible;
    
    4) No pages are copied because btrfs_copy_from_user() returned zero;
    
    5) We call btrfs_end_write_no_snapshotting() which decrements the root's
       subv_writers counter to 0;
    
    6) We don't set 'only_release_metadata' back to 'false' because we do
       it only if 'copied', the value returned by btrfs_copy_from_user(), is
       greater than zero;
    
    7) On the next iteration of the while loop, which processes the same
       page range, we are now able to allocate data space for the write (we
       got enough data space released in the meanwhile);
    
    8) After this if we fail at btrfs_delalloc_reserve_metadata(), because
       now there isn't enough free metadata space, or in some other place
       further below (prepare_pages(), lock_and_cleanup_extent_if_need(),
       btrfs_dirty_pages()), we break out of the while loop with
       'only_release_metadata' having a value of 'true';
    
    9) Because 'only_release_metadata' is 'true' we end up decrementing the
       root's subv_writers counter to -1 (through a call to
       btrfs_end_write_no_snapshotting()), and we also end up not releasing the
       data space previously reserved through btrfs_check_data_free_space().
       As a consequence the mechanism for synchronizing NOCOW buffered writes
       with snapshotting gets broken.
    
    Fix this by always setting 'only_release_metadata' to false at the start
    of each iteration.
    
    Fixes: 8257b2dc3c1a ("Btrfs: introduce btrfs_{start, end}_nocow_write() for each subvolume")
    Fixes: 7ee9e4405f26 ("Btrfs: check if we can nocow if we don't have data space")
    CC: stable@vger.kernel.org # 4.4+
    Reviewed-by: Josef Bacik <josef@toxicpanda.com>
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index f9434fa3e387..91c98a6d1408 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1636,6 +1636,7 @@ static noinline ssize_t btrfs_buffered_write(struct kiocb *iocb,
 			break;
 		}
 
+		only_release_metadata = false;
 		sector_offset = pos & (fs_info->sectorsize - 1);
 		reserve_bytes = round_up(write_bytes + sector_offset,
 				fs_info->sectorsize);
@@ -1791,7 +1792,6 @@ static noinline ssize_t btrfs_buffered_write(struct kiocb *iocb,
 			set_extent_bit(&BTRFS_I(inode)->io_tree, lockstart,
 				       lockend, EXTENT_NORESERVE, NULL,
 				       NULL, GFP_NOFS);
-			only_release_metadata = false;
 		}
 
 		btrfs_drop_pages(pages, num_pages);

commit 4c66e0d4243bb8829f2c936e966030d967726e90
Author: David Sterba <dsterba@suse.com>
Date:   Thu Oct 3 19:09:35 2019 +0200

    btrfs: drop unused parameter is_new from btrfs_iget
    
    The parameter is now always set to NULL and could be dropped. The last
    user was get_default_root but that got reworked in 05dbe6837b60 ("Btrfs:
    unify subvol= and subvolid= mounting") and the parameter became unused.
    
    Reviewed-by: Anand Jain <anand.jain@oracle.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 25ce1b6dbda9..f9434fa3e387 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -296,7 +296,7 @@ static int __btrfs_run_defrag_inode(struct btrfs_fs_info *fs_info,
 	key.objectid = defrag->ino;
 	key.type = BTRFS_INODE_ITEM_KEY;
 	key.offset = 0;
-	inode = btrfs_iget(fs_info->sb, &key, inode_root, NULL);
+	inode = btrfs_iget(fs_info->sb, &key, inode_root);
 	if (IS_ERR(inode)) {
 		ret = PTR_ERR(inode);
 		goto cleanup;

commit 9cf35f673583ccc9f3e2507498b3079d56614ad3
Author: Goldwyn Rodrigues <rgoldwyn@suse.com>
Date:   Wed Sep 11 11:45:15 2019 -0500

    btrfs: simplify inode locking for RWF_NOWAIT
    
    This is similar to 942491c9e6d6 ("xfs: fix AIM7 regression"). Apparently
    our current rwsem code doesn't like doing the trylock, then lock for
    real scheme. This causes extra contention on the lock and can be
    measured eg. by AIM7 benchmark.  So change our read/write methods to
    just do the trylock for the RWF_NOWAIT case.
    
    Fixes: edf064e7c6fe ("btrfs: nowait aio support")
    Signed-off-by: Goldwyn Rodrigues <rgoldwyn@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    [ update changelog ]
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 435a502a3226..25ce1b6dbda9 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1903,9 +1903,10 @@ static ssize_t btrfs_file_write_iter(struct kiocb *iocb,
 	    (iocb->ki_flags & IOCB_NOWAIT))
 		return -EOPNOTSUPP;
 
-	if (!inode_trylock(inode)) {
-		if (iocb->ki_flags & IOCB_NOWAIT)
+	if (iocb->ki_flags & IOCB_NOWAIT) {
+		if (!inode_trylock(inode))
 			return -EAGAIN;
+	} else {
 		inode_lock(inode);
 	}
 

commit ba0b084ac309283db6e329785c1dc4f45fdbd379
Author: Filipe Manana <fdmanana@suse.com>
Date:   Wed Oct 16 16:28:52 2019 +0100

    Btrfs: check for the full sync flag while holding the inode lock during fsync
    
    We were checking for the full fsync flag in the inode before locking the
    inode, which is racy, since at that that time it might not be set but
    after we acquire the inode lock some other task set it. One case where
    this can happen is on a system low on memory and some concurrent task
    failed to allocate an extent map and therefore set the full sync flag on
    the inode, to force the next fsync to work in full mode.
    
    A consequence of missing the full fsync flag set is hitting the problems
    fixed by commit 0c713cbab620 ("Btrfs: fix race between ranged fsync and
    writeback of adjacent ranges"), BUG_ON() when dropping extents from a log
    tree, hitting assertion failures at tree-log.c:copy_items() or all sorts
    of weird inconsistencies after replaying a log due to file extents items
    representing ranges that overlap.
    
    So just move the check such that it's done after locking the inode and
    before starting writeback again.
    
    Fixes: 0c713cbab620 ("Btrfs: fix race between ranged fsync and writeback of adjacent ranges")
    CC: stable@vger.kernel.org # 5.2+
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index e955e7fa9201..435a502a3226 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2067,25 +2067,7 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 	struct btrfs_trans_handle *trans;
 	struct btrfs_log_ctx ctx;
 	int ret = 0, err;
-	u64 len;
 
-	/*
-	 * If the inode needs a full sync, make sure we use a full range to
-	 * avoid log tree corruption, due to hole detection racing with ordered
-	 * extent completion for adjacent ranges, and assertion failures during
-	 * hole detection.
-	 */
-	if (test_bit(BTRFS_INODE_NEEDS_FULL_SYNC,
-		     &BTRFS_I(inode)->runtime_flags)) {
-		start = 0;
-		end = LLONG_MAX;
-	}
-
-	/*
-	 * The range length can be represented by u64, we have to do the typecasts
-	 * to avoid signed overflow if it's [0, LLONG_MAX] eg. from fsync()
-	 */
-	len = (u64)end - (u64)start + 1;
 	trace_btrfs_sync_file(file, datasync);
 
 	btrfs_init_log_ctx(&ctx, inode);
@@ -2111,6 +2093,19 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 
 	atomic_inc(&root->log_batch);
 
+	/*
+	 * If the inode needs a full sync, make sure we use a full range to
+	 * avoid log tree corruption, due to hole detection racing with ordered
+	 * extent completion for adjacent ranges, and assertion failures during
+	 * hole detection. Do this while holding the inode lock, to avoid races
+	 * with other tasks.
+	 */
+	if (test_bit(BTRFS_INODE_NEEDS_FULL_SYNC,
+		     &BTRFS_I(inode)->runtime_flags)) {
+		start = 0;
+		end = LLONG_MAX;
+	}
+
 	/*
 	 * Before we acquired the inode's lock, someone may have dirtied more
 	 * pages in the target range. We need to make sure that writeback for
@@ -2138,8 +2133,11 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 	/*
 	 * We have to do this here to avoid the priority inversion of waiting on
 	 * IO of a lower priority task while holding a transaction open.
+	 *
+	 * Also, the range length can be represented by u64, we have to do the
+	 * typecasts to avoid signed overflow if it's [0, LLONG_MAX].
 	 */
-	ret = btrfs_wait_ordered_range(inode, start, len);
+	ret = btrfs_wait_ordered_range(inode, start, (u64)end - (u64)start + 1);
 	if (ret) {
 		up_write(&BTRFS_I(inode)->dio_sem);
 		inode_unlock(inode);

commit 8702ba9396bf7bbae2ab93c94acd4bd37cfa4f09
Author: Qu Wenruo <wqu@suse.com>
Date:   Mon Oct 14 14:34:51 2019 +0800

    btrfs: qgroup: Always free PREALLOC META reserve in btrfs_delalloc_release_extents()
    
    [Background]
    Btrfs qgroup uses two types of reserved space for METADATA space,
    PERTRANS and PREALLOC.
    
    PERTRANS is metadata space reserved for each transaction started by
    btrfs_start_transaction().
    While PREALLOC is for delalloc, where we reserve space before joining a
    transaction, and finally it will be converted to PERTRANS after the
    writeback is done.
    
    [Inconsistency]
    However there is inconsistency in how we handle PREALLOC metadata space.
    
    The most obvious one is:
    In btrfs_buffered_write():
            btrfs_delalloc_release_extents(BTRFS_I(inode), reserve_bytes, true);
    
    We always free qgroup PREALLOC meta space.
    
    While in btrfs_truncate_block():
            btrfs_delalloc_release_extents(BTRFS_I(inode), blocksize, (ret != 0));
    
    We only free qgroup PREALLOC meta space when something went wrong.
    
    [The Correct Behavior]
    The correct behavior should be the one in btrfs_buffered_write(), we
    should always free PREALLOC metadata space.
    
    The reason is, the btrfs_delalloc_* mechanism works by:
    - Reserve metadata first, even it's not necessary
      In btrfs_delalloc_reserve_metadata()
    
    - Free the unused metadata space
      Normally in:
      btrfs_delalloc_release_extents()
      |- btrfs_inode_rsv_release()
         Here we do calculation on whether we should release or not.
    
    E.g. for 64K buffered write, the metadata rsv works like:
    
    /* The first page */
    reserve_meta:   num_bytes=calc_inode_reservations()
    free_meta:      num_bytes=0
    total:          num_bytes=calc_inode_reservations()
    /* The first page caused one outstanding extent, thus needs metadata
       rsv */
    
    /* The 2nd page */
    reserve_meta:   num_bytes=calc_inode_reservations()
    free_meta:      num_bytes=calc_inode_reservations()
    total:          not changed
    /* The 2nd page doesn't cause new outstanding extent, needs no new meta
       rsv, so we free what we have reserved */
    
    /* The 3rd~16th pages */
    reserve_meta:   num_bytes=calc_inode_reservations()
    free_meta:      num_bytes=calc_inode_reservations()
    total:          not changed (still space for one outstanding extent)
    
    This means, if btrfs_delalloc_release_extents() determines to free some
    space, then those space should be freed NOW.
    So for qgroup, we should call btrfs_qgroup_free_meta_prealloc() other
    than btrfs_qgroup_convert_reserved_meta().
    
    The good news is:
    - The callers are not that hot
      The hottest caller is in btrfs_buffered_write(), which is already
      fixed by commit 336a8bb8e36a ("btrfs: Fix wrong
      btrfs_delalloc_release_extents parameter"). Thus it's not that
      easy to cause false EDQUOT.
    
    - The trans commit in advance for qgroup would hide the bug
      Since commit f5fef4593653 ("btrfs: qgroup: Make qgroup async transaction
      commit more aggressive"), when btrfs qgroup metadata free space is slow,
      it will try to commit transaction and free the wrongly converted
      PERTRANS space, so it's not that easy to hit such bug.
    
    [FIX]
    So to fix the problem, remove the @qgroup_free parameter for
    btrfs_delalloc_release_extents(), and always pass true to
    btrfs_inode_rsv_release().
    
    Reported-by: Filipe Manana <fdmanana@suse.com>
    Fixes: 43b18595d660 ("btrfs: qgroup: Use separate meta reservation type for delalloc")
    CC: stable@vger.kernel.org # 4.19+
    Reviewed-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: Qu Wenruo <wqu@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 27e5b269e729..e955e7fa9201 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1692,7 +1692,7 @@ static noinline ssize_t btrfs_buffered_write(struct kiocb *iocb,
 				    force_page_uptodate);
 		if (ret) {
 			btrfs_delalloc_release_extents(BTRFS_I(inode),
-						       reserve_bytes, true);
+						       reserve_bytes);
 			break;
 		}
 
@@ -1704,7 +1704,7 @@ static noinline ssize_t btrfs_buffered_write(struct kiocb *iocb,
 			if (extents_locked == -EAGAIN)
 				goto again;
 			btrfs_delalloc_release_extents(BTRFS_I(inode),
-						       reserve_bytes, true);
+						       reserve_bytes);
 			ret = extents_locked;
 			break;
 		}
@@ -1772,8 +1772,7 @@ static noinline ssize_t btrfs_buffered_write(struct kiocb *iocb,
 		else
 			free_extent_state(cached_state);
 
-		btrfs_delalloc_release_extents(BTRFS_I(inode), reserve_bytes,
-					       true);
+		btrfs_delalloc_release_extents(BTRFS_I(inode), reserve_bytes);
 		if (ret) {
 			btrfs_drop_pages(pages, num_pages);
 			break;

commit c67d970f0ea8dcc423e112137d34334fa0abb8ec
Author: Filipe Manana <fdmanana@suse.com>
Date:   Mon Sep 30 10:20:25 2019 +0100

    Btrfs: fix memory leak due to concurrent append writes with fiemap
    
    When we have a buffered write that starts at an offset greater than or
    equals to the file's size happening concurrently with a full ranged
    fiemap, we can end up leaking an extent state structure.
    
    Suppose we have a file with a size of 1Mb, and before the buffered write
    and fiemap are performed, it has a single extent state in its io tree
    representing the range from 0 to 1Mb, with the EXTENT_DELALLOC bit set.
    
    The following sequence diagram shows how the memory leak happens if a
    fiemap a buffered write, starting at offset 1Mb and with a length of
    4Kb, are performed concurrently.
    
              CPU 1                                                  CPU 2
    
      extent_fiemap()
        --> it's a full ranged fiemap
            range from 0 to LLONG_MAX - 1
            (9223372036854775807)
    
        --> locks range in the inode's
            io tree
          --> after this we have 2 extent
              states in the io tree:
              --> 1 for range [0, 1Mb[ with
                  the bits EXTENT_LOCKED and
                  EXTENT_DELALLOC_BITS set
              --> 1 for the range
                  [1Mb, LLONG_MAX[ with
                  the EXTENT_LOCKED bit set
    
                                                      --> start buffered write at offset
                                                          1Mb with a length of 4Kb
    
                                                      btrfs_file_write_iter()
    
                                                        btrfs_buffered_write()
                                                          --> cached_state is NULL
    
                                                          lock_and_cleanup_extent_if_need()
                                                            --> returns 0 and does not lock
                                                                range because it starts
                                                                at current i_size / eof
    
                                                          --> cached_state remains NULL
    
                                                          btrfs_dirty_pages()
                                                            btrfs_set_extent_delalloc()
                                                              (...)
                                                              __set_extent_bit()
    
                                                                --> splits extent state for range
                                                                    [1Mb, LLONG_MAX[ and now we
                                                                    have 2 extent states:
    
                                                                    --> one for the range
                                                                        [1Mb, 1Mb + 4Kb[ with
                                                                        EXTENT_LOCKED set
                                                                    --> another one for the range
                                                                        [1Mb + 4Kb, LLONG_MAX[ with
                                                                        EXTENT_LOCKED set as well
    
                                                                --> sets EXTENT_DELALLOC on the
                                                                    extent state for the range
                                                                    [1Mb, 1Mb + 4Kb[
                                                                --> caches extent state
                                                                    [1Mb, 1Mb + 4Kb[ into
                                                                    @cached_state because it has
                                                                    the bit EXTENT_LOCKED set
    
                                                        --> btrfs_buffered_write() ends up
                                                            with a non-NULL cached_state and
                                                            never calls anything to release its
                                                            reference on it, resulting in a
                                                            memory leak
    
    Fix this by calling free_extent_state() on cached_state if the range was
    not locked by lock_and_cleanup_extent_if_need().
    
    The same issue can happen if anything else other than fiemap locks a range
    that covers eof and beyond.
    
    This could be triggered, sporadically, by test case generic/561 from the
    fstests suite, which makes duperemove run concurrently with fsstress, and
    duperemove does plenty of calls to fiemap. When CONFIG_BTRFS_DEBUG is set
    the leak is reported in dmesg/syslog when removing the btrfs module with
    a message like the following:
    
      [77100.039461] BTRFS: state leak: start 6574080 end 6582271 state 16402 in tree 0 refs 1
    
    Otherwise (CONFIG_BTRFS_DEBUG not set) detectable with kmemleak.
    
    CC: stable@vger.kernel.org # 4.16+
    Reviewed-by: Josef Bacik <josef@toxicpanda.com>
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 8fe4eb7e5045..27e5b269e729 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1591,7 +1591,6 @@ static noinline ssize_t btrfs_buffered_write(struct kiocb *iocb,
 	struct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);
 	struct btrfs_root *root = BTRFS_I(inode)->root;
 	struct page **pages = NULL;
-	struct extent_state *cached_state = NULL;
 	struct extent_changeset *data_reserved = NULL;
 	u64 release_bytes = 0;
 	u64 lockstart;
@@ -1611,6 +1610,7 @@ static noinline ssize_t btrfs_buffered_write(struct kiocb *iocb,
 		return -ENOMEM;
 
 	while (iov_iter_count(i) > 0) {
+		struct extent_state *cached_state = NULL;
 		size_t offset = offset_in_page(pos);
 		size_t sector_offset;
 		size_t write_bytes = min(iov_iter_count(i),
@@ -1758,9 +1758,20 @@ static noinline ssize_t btrfs_buffered_write(struct kiocb *iocb,
 		if (copied > 0)
 			ret = btrfs_dirty_pages(inode, pages, dirty_pages,
 						pos, copied, &cached_state);
+
+		/*
+		 * If we have not locked the extent range, because the range's
+		 * start offset is >= i_size, we might still have a non-NULL
+		 * cached extent state, acquired while marking the extent range
+		 * as delalloc through btrfs_dirty_pages(). Therefore free any
+		 * possible cached extent state to avoid a memory leak.
+		 */
 		if (extents_locked)
 			unlock_extent_cached(&BTRFS_I(inode)->io_tree,
 					     lockstart, lockend, &cached_state);
+		else
+			free_extent_state(cached_state);
+
 		btrfs_delalloc_release_extents(BTRFS_I(inode), reserve_bytes,
 					       true);
 		if (ret) {

commit e182163d9cbe86bc0f754068628df55e6dc073d3
Author: Omar Sandoval <osandov@fb.com>
Date:   Thu Aug 15 14:04:04 2019 -0700

    btrfs: stop clearing EXTENT_DIRTY in inode I/O tree
    
    Since commit fee187d9d9dd ("Btrfs: do not set EXTENT_DIRTY along with
    EXTENT_DELALLOC"), we never set EXTENT_DIRTY in inode->io_tree, so we
    can simplify and stop trying to clear it.
    
    Reviewed-by: Josef Bacik <josef@toxicpanda.com>
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 702c30a28a43..8fe4eb7e5045 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -537,8 +537,8 @@ int btrfs_dirty_pages(struct inode *inode, struct page **pages,
 	 * we can set things up properly
 	 */
 	clear_extent_bit(&BTRFS_I(inode)->io_tree, start_pos, end_of_last_block,
-			 EXTENT_DIRTY | EXTENT_DELALLOC |
-			 EXTENT_DO_ACCOUNTING | EXTENT_DEFRAG, 0, 0, cached);
+			 EXTENT_DELALLOC | EXTENT_DO_ACCOUNTING | EXTENT_DEFRAG,
+			 0, 0, cached);
 
 	if (!btrfs_is_free_space_inode(BTRFS_I(inode))) {
 		if (start_pos >= isize &&

commit f50cb7aff9645998b3269eac903a5e01cd6ba689
Author: Omar Sandoval <osandov@fb.com>
Date:   Thu Aug 15 14:04:03 2019 -0700

    btrfs: treat RWF_{,D}SYNC writes as sync for CRCs
    
    The VFS indicates a synchronous write to ->write_iter() via
    iocb->ki_flags. The IOCB_{,D}SYNC flags may be set based on the file
    (see iocb_flags()) or the RWF_* flags passed to a syscall like
    pwritev2() (see kiocb_set_rw_flags()).
    
    However, in btrfs_file_write_iter(), we're checking if a write is
    synchronous based only on the file; we use this to decide when to bump
    the sync_writers counter and thus do CRCs synchronously. Make sure we do
    this for all synchronous writes as determined by the VFS.
    
    Reviewed-by: Josef Bacik <josef@toxicpanda.com>
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    [ add const ]
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 25df5b03b591..702c30a28a43 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1882,7 +1882,7 @@ static ssize_t btrfs_file_write_iter(struct kiocb *iocb,
 	u64 start_pos;
 	u64 end_pos;
 	ssize_t num_written = 0;
-	bool sync = (file->f_flags & O_DSYNC) || IS_SYNC(file->f_mapping->host);
+	const bool sync = iocb->ki_flags & IOCB_DSYNC;
 	ssize_t err;
 	loff_t pos;
 	size_t count;

commit c09767a8960ca0500fb636bf73686723337debf4
Author: Omar Sandoval <osandov@fb.com>
Date:   Thu Aug 15 14:04:02 2019 -0700

    btrfs: use correct count in btrfs_file_write_iter()
    
    generic_write_checks() may modify iov_iter_count(), so we must get the
    count after the call, not before. Using the wrong one has a couple of
    consequences:
    
    1. We check a longer range in check_can_nocow() for nowait than we're
       actually writing.
    2. We create extra hole extent maps in btrfs_cont_expand(). As far as I
       can tell, this is harmless, but I might be missing something.
    
    These issues are pretty minor, but let's fix it before something more
    important trips on it.
    
    Fixes: edf064e7c6fe ("btrfs: nowait aio support")
    Reviewed-by: Josef Bacik <josef@toxicpanda.com>
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 1cb694c96500..25df5b03b591 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1885,7 +1885,7 @@ static ssize_t btrfs_file_write_iter(struct kiocb *iocb,
 	bool sync = (file->f_flags & O_DSYNC) || IS_SYNC(file->f_mapping->host);
 	ssize_t err;
 	loff_t pos;
-	size_t count = iov_iter_count(from);
+	size_t count;
 	loff_t oldsize;
 	int clean_page = 0;
 
@@ -1906,6 +1906,7 @@ static ssize_t btrfs_file_write_iter(struct kiocb *iocb,
 	}
 
 	pos = iocb->ki_pos;
+	count = iov_iter_count(from);
 	if (iocb->ki_flags & IOCB_NOWAIT) {
 		/*
 		 * We will allocate space in case nodatacow is not set,

commit 2bd36e7b4fd60d4ff5f9ba6a0ad84557ae4803c4
Author: Josef Bacik <josef@toxicpanda.com>
Date:   Thu Aug 22 15:14:33 2019 -0400

    btrfs: rename the btrfs_calc_*_metadata_size helpers
    
    btrfs_calc_trunc_metadata_size differs from trans_metadata_size in that
    it doesn't take into account any splitting at the levels, because
    truncate will never split nodes.  However truncate _and_ changing will
    never split nodes, so rename btrfs_calc_trunc_metadata_size to
    btrfs_calc_metadata_size.  Also btrfs_calc_trans_metadata_size is purely
    for inserting items, so rename this to btrfs_calc_insert_metadata_size.
    Making these clearer will help when I start using them differently in
    upcoming patches.
    
    Reviewed-by: Nikolay Borisov <nborisov@suse.com>
    Signed-off-by: Josef Bacik <josef@toxicpanda.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index b31991f0f440..1cb694c96500 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2511,7 +2511,7 @@ int btrfs_punch_hole_range(struct inode *inode, struct btrfs_path *path,
 			   struct btrfs_trans_handle **trans_out)
 {
 	struct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);
-	u64 min_size = btrfs_calc_trans_metadata_size(fs_info, 1);
+	u64 min_size = btrfs_calc_insert_metadata_size(fs_info, 1);
 	u64 ino_size = round_up(inode->i_size, fs_info->sectorsize);
 	struct btrfs_root *root = BTRFS_I(inode)->root;
 	struct btrfs_trans_handle *trans = NULL;
@@ -2530,7 +2530,7 @@ int btrfs_punch_hole_range(struct inode *inode, struct btrfs_path *path,
 		ret = -ENOMEM;
 		goto out;
 	}
-	rsv->size = btrfs_calc_trans_metadata_size(fs_info, 1);
+	rsv->size = btrfs_calc_insert_metadata_size(fs_info, 1);
 	rsv->failfast = 1;
 
 	/*

commit 330a582790452a159686c5dab8f4286babd9c00e
Author: Nikolay Borisov <nborisov@suse.com>
Date:   Wed Jul 17 16:18:17 2019 +0300

    btrfs: Remove leftover of in-band dedupe
    
    It's unlikely in-band dedupe is going to land so just remove any
    leftovers - dedupe.h header as well as the 'dedupe' parameter to
    btrfs_set_extent_delalloc.
    
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 474ff1cac640..b31991f0f440 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -559,7 +559,7 @@ int btrfs_dirty_pages(struct inode *inode, struct page **pages,
 	}
 
 	err = btrfs_set_extent_delalloc(inode, start_pos, end_of_last_block,
-					extra_bits, cached, 0);
+					extra_bits, cached);
 	if (err)
 		return err;
 

commit 690a5dbfc5131572910e6350d65d7b9d55439817
Author: Filipe Manana <fdmanana@suse.com>
Date:   Fri Jul 5 11:09:50 2019 +0100

    Btrfs: fix ENOSPC errors, leading to transaction aborts, when cloning extents
    
    When cloning extents (or deduplicating) we create a transaction with a
    space reservation that considers we will drop or update a single file
    extent item of the destination inode (that we modify a single leaf). That
    is fine for the vast majority of scenarios, however it might happen that
    we need to drop many file extent items, and adjust at most two file extent
    items, in the destination root, which can span multiple leafs. This will
    lead to either the call to btrfs_drop_extents() to fail with ENOSPC or
    the subsequent calls to btrfs_insert_empty_item() or btrfs_update_inode()
    (called through clone_finish_inode_update()) to fail with ENOSPC. Such
    failure results in a transaction abort, leaving the filesystem in a
    read-only mode.
    
    In order to fix this we need to follow the same approach as the hole
    punching code, where we create a local reservation with 1 unit and keep
    ending and starting transactions, after balancing the btree inode,
    when __btrfs_drop_extents() returns ENOSPC. So fix this by making the
    extent cloning call calls the recently added btrfs_punch_hole_range()
    helper, which is what does the mentioned work for hole punching, and
    make sure whenever we drop extent items in a transaction, we also add a
    replacing file extent item, to avoid corruption (a hole) if after ending
    a transaction and before starting a new one, the old transaction gets
    committed and a power failure happens before we finish cloning.
    
    A test case for fstests follows soon.
    
    Reported-by: David Goodwin <david@codepoets.co.uk>
    Link: https://lore.kernel.org/linux-btrfs/a4a4cf31-9cf4-e52c-1f86-c62d336c9cd1@codepoets.co.uk/
    Reported-by: Sam Tygier <sam@tygier.co.uk>
    Link: https://lore.kernel.org/linux-btrfs/82aace9f-a1e3-1f0b-055f-3ea75f7a41a0@tygier.co.uk/
    Fixes: b6f3409b2197e8f ("Btrfs: reserve sufficient space for ioctl clone")
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 16dc09736310..474ff1cac640 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2439,13 +2439,76 @@ static int btrfs_punch_hole_lock_range(struct inode *inode,
 	return 0;
 }
 
+static int btrfs_insert_clone_extent(struct btrfs_trans_handle *trans,
+				     struct inode *inode,
+				     struct btrfs_path *path,
+				     struct btrfs_clone_extent_info *clone_info,
+				     const u64 clone_len)
+{
+	struct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);
+	struct btrfs_root *root = BTRFS_I(inode)->root;
+	struct btrfs_file_extent_item *extent;
+	struct extent_buffer *leaf;
+	struct btrfs_key key;
+	int slot;
+	struct btrfs_ref ref = { 0 };
+	u64 ref_offset;
+	int ret;
+
+	if (clone_len == 0)
+		return 0;
+
+	if (clone_info->disk_offset == 0 &&
+	    btrfs_fs_incompat(fs_info, NO_HOLES))
+		return 0;
+
+	key.objectid = btrfs_ino(BTRFS_I(inode));
+	key.type = BTRFS_EXTENT_DATA_KEY;
+	key.offset = clone_info->file_offset;
+	ret = btrfs_insert_empty_item(trans, root, path, &key,
+				      clone_info->item_size);
+	if (ret)
+		return ret;
+	leaf = path->nodes[0];
+	slot = path->slots[0];
+	write_extent_buffer(leaf, clone_info->extent_buf,
+			    btrfs_item_ptr_offset(leaf, slot),
+			    clone_info->item_size);
+	extent = btrfs_item_ptr(leaf, slot, struct btrfs_file_extent_item);
+	btrfs_set_file_extent_offset(leaf, extent, clone_info->data_offset);
+	btrfs_set_file_extent_num_bytes(leaf, extent, clone_len);
+	btrfs_mark_buffer_dirty(leaf);
+	btrfs_release_path(path);
+
+	/* If it's a hole, nothing more needs to be done. */
+	if (clone_info->disk_offset == 0)
+		return 0;
+
+	inode_add_bytes(inode, clone_len);
+	btrfs_init_generic_ref(&ref, BTRFS_ADD_DELAYED_REF,
+			       clone_info->disk_offset,
+			       clone_info->disk_len, 0);
+	ref_offset = clone_info->file_offset - clone_info->data_offset;
+	btrfs_init_data_ref(&ref, root->root_key.objectid,
+			    btrfs_ino(BTRFS_I(inode)), ref_offset);
+	ret = btrfs_inc_extent_ref(trans, &ref);
+
+	return ret;
+}
+
 /*
  * The respective range must have been previously locked, as well as the inode.
  * The end offset is inclusive (last byte of the range).
+ * @clone_info is NULL for fallocate's hole punching and non-NULL for extent
+ * cloning.
+ * When cloning, we don't want to end up in a state where we dropped extents
+ * without inserting a new one, so we must abort the transaction to avoid a
+ * corruption.
  */
-static int btrfs_punch_hole_range(struct inode *inode, struct btrfs_path *path,
-				  const u64 start, const u64 end,
-				  struct btrfs_trans_handle **trans_out)
+int btrfs_punch_hole_range(struct inode *inode, struct btrfs_path *path,
+			   const u64 start, const u64 end,
+			   struct btrfs_clone_extent_info *clone_info,
+			   struct btrfs_trans_handle **trans_out)
 {
 	struct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);
 	u64 min_size = btrfs_calc_trans_metadata_size(fs_info, 1);
@@ -2473,9 +2536,14 @@ static int btrfs_punch_hole_range(struct inode *inode, struct btrfs_path *path,
 	/*
 	 * 1 - update the inode
 	 * 1 - removing the extents in the range
-	 * 1 - adding the hole extent if no_holes isn't set
+	 * 1 - adding the hole extent if no_holes isn't set or if we are cloning
+	 *     an extent
 	 */
-	rsv_count = btrfs_fs_incompat(fs_info, NO_HOLES) ? 2 : 3;
+	if (!btrfs_fs_incompat(fs_info, NO_HOLES) || clone_info)
+		rsv_count = 3;
+	else
+		rsv_count = 2;
+
 	trans = btrfs_start_transaction(root, rsv_count);
 	if (IS_ERR(trans)) {
 		ret = PTR_ERR(trans);
@@ -2493,12 +2561,23 @@ static int btrfs_punch_hole_range(struct inode *inode, struct btrfs_path *path,
 		ret = __btrfs_drop_extents(trans, root, inode, path,
 					   cur_offset, end + 1, &drop_end,
 					   1, 0, 0, NULL);
-		if (ret != -ENOSPC)
+		if (ret != -ENOSPC) {
+			/*
+			 * When cloning we want to avoid transaction aborts when
+			 * nothing was done and we are attempting to clone parts
+			 * of inline extents, in such cases -EOPNOTSUPP is
+			 * returned by __btrfs_drop_extents() without having
+			 * changed anything in the file.
+			 */
+			if (clone_info && ret && ret != -EOPNOTSUPP)
+				btrfs_abort_transaction(trans, ret);
 			break;
+		}
 
 		trans->block_rsv = &fs_info->trans_block_rsv;
 
-		if (cur_offset < drop_end && cur_offset < ino_size) {
+		if (!clone_info && cur_offset < drop_end &&
+		    cur_offset < ino_size) {
 			ret = fill_holes(trans, BTRFS_I(inode), path,
 					cur_offset, drop_end);
 			if (ret) {
@@ -2513,6 +2592,20 @@ static int btrfs_punch_hole_range(struct inode *inode, struct btrfs_path *path,
 			}
 		}
 
+		if (clone_info) {
+			u64 clone_len = drop_end - cur_offset;
+
+			ret = btrfs_insert_clone_extent(trans, inode, path,
+							clone_info, clone_len);
+			if (ret) {
+				btrfs_abort_transaction(trans, ret);
+				break;
+			}
+			clone_info->data_len -= clone_len;
+			clone_info->data_offset += clone_len;
+			clone_info->file_offset += clone_len;
+		}
+
 		cur_offset = drop_end;
 
 		ret = btrfs_update_inode(trans, root, inode);
@@ -2534,15 +2627,29 @@ static int btrfs_punch_hole_range(struct inode *inode, struct btrfs_path *path,
 		BUG_ON(ret);	/* shouldn't happen */
 		trans->block_rsv = rsv;
 
-		ret = find_first_non_hole(inode, &cur_offset, &len);
-		if (unlikely(ret < 0))
-			break;
-		if (ret && !len) {
-			ret = 0;
-			break;
+		if (!clone_info) {
+			ret = find_first_non_hole(inode, &cur_offset, &len);
+			if (unlikely(ret < 0))
+				break;
+			if (ret && !len) {
+				ret = 0;
+				break;
+			}
 		}
 	}
 
+	/*
+	 * If we were cloning, force the next fsync to be a full one since we
+	 * we replaced (or just dropped in the case of cloning holes when
+	 * NO_HOLES is enabled) extents and extent maps.
+	 * This is for the sake of simplicity, and cloning into files larger
+	 * than 16Mb would force the full fsync any way (when
+	 * try_release_extent_mapping() is invoked during page cache truncation.
+	 */
+	if (clone_info)
+		set_bit(BTRFS_INODE_NEEDS_FULL_SYNC,
+			&BTRFS_I(inode)->runtime_flags);
+
 	if (ret)
 		goto out_trans;
 
@@ -2565,7 +2672,7 @@ static int btrfs_punch_hole_range(struct inode *inode, struct btrfs_path *path,
 	 * (because it's useless) or if it represents a 0 bytes range (when
 	 * cur_offset == drop_end).
 	 */
-	if (cur_offset < ino_size && cur_offset < drop_end) {
+	if (!clone_info && cur_offset < ino_size && cur_offset < drop_end) {
 		ret = fill_holes(trans, BTRFS_I(inode), path,
 				cur_offset, drop_end);
 		if (ret) {
@@ -2574,6 +2681,14 @@ static int btrfs_punch_hole_range(struct inode *inode, struct btrfs_path *path,
 			goto out_trans;
 		}
 	}
+	if (clone_info) {
+		ret = btrfs_insert_clone_extent(trans, inode, path, clone_info,
+						clone_info->data_len);
+		if (ret) {
+			btrfs_abort_transaction(trans, ret);
+			goto out_trans;
+		}
+	}
 
 out_trans:
 	if (!trans)
@@ -2710,7 +2825,8 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 		goto out;
 	}
 
-	ret = btrfs_punch_hole_range(inode, path, lockstart, lockend, &trans);
+	ret = btrfs_punch_hole_range(inode, path, lockstart, lockend, NULL,
+				     &trans);
 	btrfs_free_path(path);
 	if (ret)
 		goto out;

commit 9cba40a693e69badb567d6ce0eaa0150f25c3d39
Author: Filipe Manana <fdmanana@suse.com>
Date:   Fri Jun 28 23:11:26 2019 +0100

    Btrfs: factor out extent dropping code from hole punch handler
    
    Move the code that is responsible for dropping extents in a range out of
    btrfs_punch_hole() into a new helper function, btrfs_punch_hole_range(),
    so that later it can be used by the reflinking (extent cloning and dedup)
    code to fix a ENOSPC bug.
    
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 58a18ed11546..16dc09736310 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2439,27 +2439,171 @@ static int btrfs_punch_hole_lock_range(struct inode *inode,
 	return 0;
 }
 
+/*
+ * The respective range must have been previously locked, as well as the inode.
+ * The end offset is inclusive (last byte of the range).
+ */
+static int btrfs_punch_hole_range(struct inode *inode, struct btrfs_path *path,
+				  const u64 start, const u64 end,
+				  struct btrfs_trans_handle **trans_out)
+{
+	struct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);
+	u64 min_size = btrfs_calc_trans_metadata_size(fs_info, 1);
+	u64 ino_size = round_up(inode->i_size, fs_info->sectorsize);
+	struct btrfs_root *root = BTRFS_I(inode)->root;
+	struct btrfs_trans_handle *trans = NULL;
+	struct btrfs_block_rsv *rsv;
+	unsigned int rsv_count;
+	u64 cur_offset;
+	u64 drop_end;
+	u64 len = end - start;
+	int ret = 0;
+
+	if (end <= start)
+		return -EINVAL;
+
+	rsv = btrfs_alloc_block_rsv(fs_info, BTRFS_BLOCK_RSV_TEMP);
+	if (!rsv) {
+		ret = -ENOMEM;
+		goto out;
+	}
+	rsv->size = btrfs_calc_trans_metadata_size(fs_info, 1);
+	rsv->failfast = 1;
+
+	/*
+	 * 1 - update the inode
+	 * 1 - removing the extents in the range
+	 * 1 - adding the hole extent if no_holes isn't set
+	 */
+	rsv_count = btrfs_fs_incompat(fs_info, NO_HOLES) ? 2 : 3;
+	trans = btrfs_start_transaction(root, rsv_count);
+	if (IS_ERR(trans)) {
+		ret = PTR_ERR(trans);
+		trans = NULL;
+		goto out_free;
+	}
+
+	ret = btrfs_block_rsv_migrate(&fs_info->trans_block_rsv, rsv,
+				      min_size, false);
+	BUG_ON(ret);
+	trans->block_rsv = rsv;
+
+	cur_offset = start;
+	while (cur_offset < end) {
+		ret = __btrfs_drop_extents(trans, root, inode, path,
+					   cur_offset, end + 1, &drop_end,
+					   1, 0, 0, NULL);
+		if (ret != -ENOSPC)
+			break;
+
+		trans->block_rsv = &fs_info->trans_block_rsv;
+
+		if (cur_offset < drop_end && cur_offset < ino_size) {
+			ret = fill_holes(trans, BTRFS_I(inode), path,
+					cur_offset, drop_end);
+			if (ret) {
+				/*
+				 * If we failed then we didn't insert our hole
+				 * entries for the area we dropped, so now the
+				 * fs is corrupted, so we must abort the
+				 * transaction.
+				 */
+				btrfs_abort_transaction(trans, ret);
+				break;
+			}
+		}
+
+		cur_offset = drop_end;
+
+		ret = btrfs_update_inode(trans, root, inode);
+		if (ret)
+			break;
+
+		btrfs_end_transaction(trans);
+		btrfs_btree_balance_dirty(fs_info);
+
+		trans = btrfs_start_transaction(root, rsv_count);
+		if (IS_ERR(trans)) {
+			ret = PTR_ERR(trans);
+			trans = NULL;
+			break;
+		}
+
+		ret = btrfs_block_rsv_migrate(&fs_info->trans_block_rsv,
+					      rsv, min_size, false);
+		BUG_ON(ret);	/* shouldn't happen */
+		trans->block_rsv = rsv;
+
+		ret = find_first_non_hole(inode, &cur_offset, &len);
+		if (unlikely(ret < 0))
+			break;
+		if (ret && !len) {
+			ret = 0;
+			break;
+		}
+	}
+
+	if (ret)
+		goto out_trans;
+
+	trans->block_rsv = &fs_info->trans_block_rsv;
+	/*
+	 * If we are using the NO_HOLES feature we might have had already an
+	 * hole that overlaps a part of the region [lockstart, lockend] and
+	 * ends at (or beyond) lockend. Since we have no file extent items to
+	 * represent holes, drop_end can be less than lockend and so we must
+	 * make sure we have an extent map representing the existing hole (the
+	 * call to __btrfs_drop_extents() might have dropped the existing extent
+	 * map representing the existing hole), otherwise the fast fsync path
+	 * will not record the existence of the hole region
+	 * [existing_hole_start, lockend].
+	 */
+	if (drop_end <= end)
+		drop_end = end + 1;
+	/*
+	 * Don't insert file hole extent item if it's for a range beyond eof
+	 * (because it's useless) or if it represents a 0 bytes range (when
+	 * cur_offset == drop_end).
+	 */
+	if (cur_offset < ino_size && cur_offset < drop_end) {
+		ret = fill_holes(trans, BTRFS_I(inode), path,
+				cur_offset, drop_end);
+		if (ret) {
+			/* Same comment as above. */
+			btrfs_abort_transaction(trans, ret);
+			goto out_trans;
+		}
+	}
+
+out_trans:
+	if (!trans)
+		goto out_free;
+
+	trans->block_rsv = &fs_info->trans_block_rsv;
+	if (ret)
+		btrfs_end_transaction(trans);
+	else
+		*trans_out = trans;
+out_free:
+	btrfs_free_block_rsv(fs_info, rsv);
+out:
+	return ret;
+}
+
 static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 {
 	struct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);
 	struct btrfs_root *root = BTRFS_I(inode)->root;
 	struct extent_state *cached_state = NULL;
 	struct btrfs_path *path;
-	struct btrfs_block_rsv *rsv;
-	struct btrfs_trans_handle *trans;
+	struct btrfs_trans_handle *trans = NULL;
 	u64 lockstart;
 	u64 lockend;
 	u64 tail_start;
 	u64 tail_len;
 	u64 orig_start = offset;
-	u64 cur_offset;
-	u64 min_size = btrfs_calc_trans_metadata_size(fs_info, 1);
-	u64 drop_end;
 	int ret = 0;
-	int err = 0;
-	unsigned int rsv_count;
 	bool same_block;
-	bool no_holes = btrfs_fs_incompat(fs_info, NO_HOLES);
 	u64 ino_size;
 	bool truncated_block = false;
 	bool updated_inode = false;
@@ -2566,145 +2710,23 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 		goto out;
 	}
 
-	rsv = btrfs_alloc_block_rsv(fs_info, BTRFS_BLOCK_RSV_TEMP);
-	if (!rsv) {
-		ret = -ENOMEM;
-		goto out_free;
-	}
-	rsv->size = btrfs_calc_trans_metadata_size(fs_info, 1);
-	rsv->failfast = 1;
-
-	/*
-	 * 1 - update the inode
-	 * 1 - removing the extents in the range
-	 * 1 - adding the hole extent if no_holes isn't set
-	 */
-	rsv_count = no_holes ? 2 : 3;
-	trans = btrfs_start_transaction(root, rsv_count);
-	if (IS_ERR(trans)) {
-		err = PTR_ERR(trans);
-		goto out_free;
-	}
-
-	ret = btrfs_block_rsv_migrate(&fs_info->trans_block_rsv, rsv,
-				      min_size, false);
-	BUG_ON(ret);
-	trans->block_rsv = rsv;
-
-	cur_offset = lockstart;
-	len = lockend - cur_offset;
-	while (cur_offset < lockend) {
-		ret = __btrfs_drop_extents(trans, root, inode, path,
-					   cur_offset, lockend + 1,
-					   &drop_end, 1, 0, 0, NULL);
-		if (ret != -ENOSPC)
-			break;
-
-		trans->block_rsv = &fs_info->trans_block_rsv;
-
-		if (cur_offset < drop_end && cur_offset < ino_size) {
-			ret = fill_holes(trans, BTRFS_I(inode), path,
-					cur_offset, drop_end);
-			if (ret) {
-				/*
-				 * If we failed then we didn't insert our hole
-				 * entries for the area we dropped, so now the
-				 * fs is corrupted, so we must abort the
-				 * transaction.
-				 */
-				btrfs_abort_transaction(trans, ret);
-				err = ret;
-				break;
-			}
-		}
-
-		cur_offset = drop_end;
-
-		ret = btrfs_update_inode(trans, root, inode);
-		if (ret) {
-			err = ret;
-			break;
-		}
-
-		btrfs_end_transaction(trans);
-		btrfs_btree_balance_dirty(fs_info);
-
-		trans = btrfs_start_transaction(root, rsv_count);
-		if (IS_ERR(trans)) {
-			ret = PTR_ERR(trans);
-			trans = NULL;
-			break;
-		}
-
-		ret = btrfs_block_rsv_migrate(&fs_info->trans_block_rsv,
-					      rsv, min_size, false);
-		BUG_ON(ret);	/* shouldn't happen */
-		trans->block_rsv = rsv;
-
-		ret = find_first_non_hole(inode, &cur_offset, &len);
-		if (unlikely(ret < 0))
-			break;
-		if (ret && !len) {
-			ret = 0;
-			break;
-		}
-	}
-
-	if (ret) {
-		err = ret;
-		goto out_trans;
-	}
-
-	trans->block_rsv = &fs_info->trans_block_rsv;
-	/*
-	 * If we are using the NO_HOLES feature we might have had already an
-	 * hole that overlaps a part of the region [lockstart, lockend] and
-	 * ends at (or beyond) lockend. Since we have no file extent items to
-	 * represent holes, drop_end can be less than lockend and so we must
-	 * make sure we have an extent map representing the existing hole (the
-	 * call to __btrfs_drop_extents() might have dropped the existing extent
-	 * map representing the existing hole), otherwise the fast fsync path
-	 * will not record the existence of the hole region
-	 * [existing_hole_start, lockend].
-	 */
-	if (drop_end <= lockend)
-		drop_end = lockend + 1;
-	/*
-	 * Don't insert file hole extent item if it's for a range beyond eof
-	 * (because it's useless) or if it represents a 0 bytes range (when
-	 * cur_offset == drop_end).
-	 */
-	if (cur_offset < ino_size && cur_offset < drop_end) {
-		ret = fill_holes(trans, BTRFS_I(inode), path,
-				cur_offset, drop_end);
-		if (ret) {
-			/* Same comment as above. */
-			btrfs_abort_transaction(trans, ret);
-			err = ret;
-			goto out_trans;
-		}
-	}
-
-out_trans:
-	if (!trans)
-		goto out_free;
+	ret = btrfs_punch_hole_range(inode, path, lockstart, lockend, &trans);
+	btrfs_free_path(path);
+	if (ret)
+		goto out;
 
+	ASSERT(trans != NULL);
 	inode_inc_iversion(inode);
 	inode->i_mtime = inode->i_ctime = current_time(inode);
-
-	trans->block_rsv = &fs_info->trans_block_rsv;
 	ret = btrfs_update_inode(trans, root, inode);
 	updated_inode = true;
 	btrfs_end_transaction(trans);
 	btrfs_btree_balance_dirty(fs_info);
-out_free:
-	btrfs_free_path(path);
-	btrfs_free_block_rsv(fs_info, rsv);
 out:
 	unlock_extent_cached(&BTRFS_I(inode)->io_tree, lockstart, lockend,
 			     &cached_state);
 out_only_mutex:
-	if (!updated_inode && truncated_block && !ret && !err) {
+	if (!updated_inode && truncated_block && !ret) {
 		/*
 		 * If we only end up zeroing part of a page, we still need to
 		 * update the inode item, so that all the time fields are
@@ -2719,16 +2741,18 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 		inode->i_ctime = now;
 		trans = btrfs_start_transaction(root, 1);
 		if (IS_ERR(trans)) {
-			err = PTR_ERR(trans);
+			ret = PTR_ERR(trans);
 		} else {
-			err = btrfs_update_inode(trans, root, inode);
-			ret = btrfs_end_transaction(trans);
+			int ret2;
+
+			ret = btrfs_update_inode(trans, root, inode);
+			ret2 = btrfs_end_transaction(trans);
+			if (!ret)
+				ret = ret2;
 		}
 	}
 	inode_unlock(inode);
-	if (ret && !err)
-		err = ret;
-	return err;
+	return ret;
 }
 
 /* Helper structure to record which range is already reserved */

commit 867363429d706984915cb4b1f299ce05f8413e23
Author: Josef Bacik <josef@toxicpanda.com>
Date:   Wed Jun 19 15:12:00 2019 -0400

    btrfs: migrate the delalloc space stuff to it's own home
    
    We have code for data and metadata reservations for delalloc.  There's
    quite a bit of code here, and it's used in a lot of places so I've
    separated it out to it's own file.  inode.c and file.c are already
    pretty large, and this code is complicated enough to live in its own
    space.
    
    Signed-off-by: Josef Bacik <josef@toxicpanda.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index a8e0fc2d1c86..58a18ed11546 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -26,6 +26,7 @@
 #include "volumes.h"
 #include "qgroup.h"
 #include "compression.h"
+#include "delalloc-space.h"
 
 static struct kmem_cache *btrfs_inode_defrag_cachep;
 /*

commit f262fa8de6a2510e13a59ad3ecdcb0f2d468bb98
Author: David Sterba <dsterba@suse.com>
Date:   Tue Jun 18 20:00:08 2019 +0200

    btrfs: drop default value assignments in enums
    
    A few more instances whre we don't need to specify the values as long as
    they are the same that enum assigns automatically. All of the enums are
    in-memory only and nothing relies on the exact values.
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index b455bdf46faa..a8e0fc2d1c86 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2796,9 +2796,9 @@ static int btrfs_fallocate_update_isize(struct inode *inode,
 }
 
 enum {
-	RANGE_BOUNDARY_WRITTEN_EXTENT = 0,
-	RANGE_BOUNDARY_PREALLOC_EXTENT = 1,
-	RANGE_BOUNDARY_HOLE = 2,
+	RANGE_BOUNDARY_WRITTEN_EXTENT,
+	RANGE_BOUNDARY_PREALLOC_EXTENT,
+	RANGE_BOUNDARY_HOLE,
 };
 
 static int btrfs_zero_range_check_range_boundary(struct inode *inode,

commit 179006688a7e888cbff39577189f2e034786d06a
Author: Filipe Manana <fdmanana@suse.com>
Date:   Wed Jun 19 13:05:50 2019 +0100

    Btrfs: add missing inode version, ctime and mtime updates when punching hole
    
    If the range for which we are punching a hole covers only part of a page,
    we end up updating the inode item but we skip the update of the inode's
    iversion, mtime and ctime. Fix that by ensuring we update those properties
    of the inode.
    
    A patch for fstests test case generic/059 that tests this as been sent
    along with this fix.
    
    Fixes: 2aaa66558172b0 ("Btrfs: add hole punching")
    Fixes: e8c1c76e804b18 ("Btrfs: add missing inode update when punching hole")
    CC: stable@vger.kernel.org # 4.4+
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 5370152ea7e3..b455bdf46faa 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2711,6 +2711,11 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 		 * for detecting, at fsync time, if the inode isn't yet in the
 		 * log tree or it's there but not up to date.
 		 */
+		struct timespec64 now = current_time(inode);
+
+		inode_inc_iversion(inode);
+		inode->i_mtime = now;
+		inode->i_ctime = now;
 		trans = btrfs_start_transaction(root, 1);
 		if (IS_ERR(trans)) {
 			err = PTR_ERR(trans);

commit 5f791ec31f535f62a2dc06b37c3a69e5e268b5db
Author: Nikolay Borisov <nborisov@suse.com>
Date:   Tue May 7 10:23:46 2019 +0300

    btrfs: Return EAGAIN if we can't start no snpashot write in check_can_nocow
    
    The first thing code does in check_can_nocow is trying to block
    concurrent snapshots. If this fails (due to snpashot already being in
    progress) the function returns ENOSPC which makes no sense. Instead
    return EAGAIN. Despite this return value not being propagated to callers
    it's good practice to return the closest in terms of semantics error
    code. No functional changes.
    
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 007652ff5cb6..5370152ea7e3 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1556,7 +1556,7 @@ static noinline int check_can_nocow(struct btrfs_inode *inode, loff_t pos,
 
 	ret = btrfs_start_write_no_snapshotting(root);
 	if (!ret)
-		return -ENOSPC;
+		return -EAGAIN;
 
 	lockstart = round_down(pos, fs_info->sectorsize);
 	lockend = round_up(pos + *write_bytes,

commit 23d31bd476d1d096d0f073483547872ec155ab34
Author: Nikolay Borisov <nborisov@suse.com>
Date:   Tue May 7 10:19:23 2019 +0300

    btrfs: Use newly introduced btrfs_lock_and_flush_ordered_range
    
    There several functions which open code
    btrfs_lock_and_flush_ordered_range, just replace them with a call to the
    function. No functional changes.
    
    Reviewed-by: Josef Bacik <josef@toxicpanda.com>
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 89f5be2bfb43..007652ff5cb6 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1550,7 +1550,6 @@ static noinline int check_can_nocow(struct btrfs_inode *inode, loff_t pos,
 {
 	struct btrfs_fs_info *fs_info = inode->root->fs_info;
 	struct btrfs_root *root = inode->root;
-	struct btrfs_ordered_extent *ordered;
 	u64 lockstart, lockend;
 	u64 num_bytes;
 	int ret;
@@ -1563,17 +1562,8 @@ static noinline int check_can_nocow(struct btrfs_inode *inode, loff_t pos,
 	lockend = round_up(pos + *write_bytes,
 			   fs_info->sectorsize) - 1;
 
-	while (1) {
-		lock_extent(&inode->io_tree, lockstart, lockend);
-		ordered = btrfs_lookup_ordered_range(inode, lockstart,
-						     lockend - lockstart + 1);
-		if (!ordered) {
-			break;
-		}
-		unlock_extent(&inode->io_tree, lockstart, lockend);
-		btrfs_start_ordered_extent(&inode->vfs_inode, ordered, 1);
-		btrfs_put_ordered_extent(ordered);
-	}
+	btrfs_lock_and_flush_ordered_range(&inode->io_tree, inode, lockstart,
+					   lockend, NULL);
 
 	num_bytes = lockend - lockstart + 1;
 	ret = can_nocow_extent(&inode->vfs_inode, lockstart, &num_bytes,

commit 0c713cbab6200b0ab6473b50435e450a6e1de85d
Author: Filipe Manana <fdmanana@suse.com>
Date:   Mon May 6 16:44:02 2019 +0100

    Btrfs: fix race between ranged fsync and writeback of adjacent ranges
    
    When we do a full fsync (the bit BTRFS_INODE_NEEDS_FULL_SYNC is set in the
    inode) that happens to be ranged, which happens during a msync() or writes
    for files opened with O_SYNC for example, we can end up with a corrupt log,
    due to different file extent items representing ranges that overlap with
    each other, or hit some assertion failures.
    
    When doing a ranged fsync we only flush delalloc and wait for ordered
    exents within that range. If while we are logging items from our inode
    ordered extents for adjacent ranges complete, we end up in a race that can
    make us insert the file extent items that overlap with others we logged
    previously and the assertion failures.
    
    For example, if tree-log.c:copy_items() receives a leaf that has the
    following file extents items, all with a length of 4K and therefore there
    is an implicit hole in the range 68K to 72K - 1:
    
      (257 EXTENT_ITEM 64K), (257 EXTENT_ITEM 72K), (257 EXTENT_ITEM 76K), ...
    
    It copies them to the log tree. However due to the need to detect implicit
    holes, it may release the path, in order to look at the previous leaf to
    detect an implicit hole, and then later it will search again in the tree
    for the first file extent item key, with the goal of locking again the
    leaf (which might have changed due to concurrent changes to other inodes).
    
    However when it locks again the leaf containing the first key, the key
    corresponding to the extent at offset 72K may not be there anymore since
    there is an ordered extent for that range that is finishing (that is,
    somewhere in the middle of btrfs_finish_ordered_io()), and it just
    removed the file extent item but has not yet replaced it with a new file
    extent item, so the part of copy_items() that does hole detection will
    decide that there is a hole in the range starting from 68K to 76K - 1,
    and therefore insert a file extent item to represent that hole, having
    a key offset of 68K. After that we now have a log tree with 2 different
    extent items that have overlapping ranges:
    
     1) The file extent item copied before copy_items() released the path,
        which has a key offset of 72K and a length of 4K, representing the
        file range 72K to 76K - 1.
    
     2) And a file extent item representing a hole that has a key offset of
        68K and a length of 8K, representing the range 68K to 76K - 1. This
        item was inserted after releasing the path, and overlaps with the
        extent item inserted before.
    
    The overlapping extent items can cause all sorts of unpredictable and
    incorrect behaviour, either when replayed or if a fast (non full) fsync
    happens later, which can trigger a BUG_ON() when calling
    btrfs_set_item_key_safe() through __btrfs_drop_extents(), producing a
    trace like the following:
    
      [61666.783269] ------------[ cut here ]------------
      [61666.783943] kernel BUG at fs/btrfs/ctree.c:3182!
      [61666.784644] invalid opcode: 0000 [#1] PREEMPT SMP
      (...)
      [61666.786253] task: ffff880117b88c40 task.stack: ffffc90008168000
      [61666.786253] RIP: 0010:btrfs_set_item_key_safe+0x7c/0xd2 [btrfs]
      [61666.786253] RSP: 0018:ffffc9000816b958 EFLAGS: 00010246
      [61666.786253] RAX: 0000000000000000 RBX: 000000000000000f RCX: 0000000000030000
      [61666.786253] RDX: 0000000000000000 RSI: ffffc9000816ba4f RDI: ffffc9000816b937
      [61666.786253] RBP: ffffc9000816b998 R08: ffff88011dae2428 R09: 0000000000001000
      [61666.786253] R10: 0000160000000000 R11: 6db6db6db6db6db7 R12: ffff88011dae2418
      [61666.786253] R13: ffffc9000816ba4f R14: ffff8801e10c4118 R15: ffff8801e715c000
      [61666.786253] FS:  00007f6060a18700(0000) GS:ffff88023f5c0000(0000) knlGS:0000000000000000
      [61666.786253] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
      [61666.786253] CR2: 00007f6060a28000 CR3: 0000000213e69000 CR4: 00000000000006e0
      [61666.786253] Call Trace:
      [61666.786253]  __btrfs_drop_extents+0x5e3/0xaad [btrfs]
      [61666.786253]  ? time_hardirqs_on+0x9/0x14
      [61666.786253]  btrfs_log_changed_extents+0x294/0x4e0 [btrfs]
      [61666.786253]  ? release_extent_buffer+0x38/0xb4 [btrfs]
      [61666.786253]  btrfs_log_inode+0xb6e/0xcdc [btrfs]
      [61666.786253]  ? lock_acquire+0x131/0x1c5
      [61666.786253]  ? btrfs_log_inode_parent+0xee/0x659 [btrfs]
      [61666.786253]  ? arch_local_irq_save+0x9/0xc
      [61666.786253]  ? btrfs_log_inode_parent+0x1f5/0x659 [btrfs]
      [61666.786253]  btrfs_log_inode_parent+0x223/0x659 [btrfs]
      [61666.786253]  ? arch_local_irq_save+0x9/0xc
      [61666.786253]  ? lockref_get_not_zero+0x2c/0x34
      [61666.786253]  ? rcu_read_unlock+0x3e/0x5d
      [61666.786253]  btrfs_log_dentry_safe+0x60/0x7b [btrfs]
      [61666.786253]  btrfs_sync_file+0x317/0x42c [btrfs]
      [61666.786253]  vfs_fsync_range+0x8c/0x9e
      [61666.786253]  SyS_msync+0x13c/0x1c9
      [61666.786253]  entry_SYSCALL_64_fastpath+0x18/0xad
    
    A sample of a corrupt log tree leaf with overlapping extents I got from
    running btrfs/072:
    
          item 14 key (295 108 200704) itemoff 2599 itemsize 53
                  extent data disk bytenr 0 nr 0
                  extent data offset 0 nr 458752 ram 458752
          item 15 key (295 108 659456) itemoff 2546 itemsize 53
                  extent data disk bytenr 4343541760 nr 770048
                  extent data offset 606208 nr 163840 ram 770048
          item 16 key (295 108 663552) itemoff 2493 itemsize 53
                  extent data disk bytenr 4343541760 nr 770048
                  extent data offset 610304 nr 155648 ram 770048
          item 17 key (295 108 819200) itemoff 2440 itemsize 53
                  extent data disk bytenr 4334788608 nr 4096
                  extent data offset 0 nr 4096 ram 4096
    
    The file extent item at offset 659456 (item 15) ends at offset 823296
    (659456 + 163840) while the next file extent item (item 16) starts at
    offset 663552.
    
    Another different problem that the race can trigger is a failure in the
    assertions at tree-log.c:copy_items(), which expect that the first file
    extent item key we found before releasing the path exists after we have
    released path and that the last key we found before releasing the path
    also exists after releasing the path:
    
      $ cat -n fs/btrfs/tree-log.c
      4080          if (need_find_last_extent) {
      4081                  /* btrfs_prev_leaf could return 1 without releasing the path */
      4082                  btrfs_release_path(src_path);
      4083                  ret = btrfs_search_slot(NULL, inode->root, &first_key,
      4084                                  src_path, 0, 0);
      4085                  if (ret < 0)
      4086                          return ret;
      4087                  ASSERT(ret == 0);
      (...)
      4103                  if (i >= btrfs_header_nritems(src_path->nodes[0])) {
      4104                          ret = btrfs_next_leaf(inode->root, src_path);
      4105                          if (ret < 0)
      4106                                  return ret;
      4107                          ASSERT(ret == 0);
      4108                          src = src_path->nodes[0];
      4109                          i = 0;
      4110                          need_find_last_extent = true;
      4111                  }
      (...)
    
    The second assertion implicitly expects that the last key before the path
    release still exists, because the surrounding while loop only stops after
    we have found that key. When this assertion fails it produces a stack like
    this:
    
      [139590.037075] assertion failed: ret == 0, file: fs/btrfs/tree-log.c, line: 4107
      [139590.037406] ------------[ cut here ]------------
      [139590.037707] kernel BUG at fs/btrfs/ctree.h:3546!
      [139590.038034] invalid opcode: 0000 [#1] SMP DEBUG_PAGEALLOC PTI
      [139590.038340] CPU: 1 PID: 31841 Comm: fsstress Tainted: G        W         5.0.0-btrfs-next-46 #1
      (...)
      [139590.039354] RIP: 0010:assfail.constprop.24+0x18/0x1a [btrfs]
      (...)
      [139590.040397] RSP: 0018:ffffa27f48f2b9b0 EFLAGS: 00010282
      [139590.040730] RAX: 0000000000000041 RBX: ffff897c635d92c8 RCX: 0000000000000000
      [139590.041105] RDX: 0000000000000000 RSI: ffff897d36a96868 RDI: ffff897d36a96868
      [139590.041470] RBP: ffff897d1b9a0708 R08: 0000000000000000 R09: 0000000000000000
      [139590.041815] R10: 0000000000000008 R11: 0000000000000000 R12: 0000000000000013
      [139590.042159] R13: 0000000000000227 R14: ffff897cffcbba88 R15: 0000000000000001
      [139590.042501] FS:  00007f2efc8dee80(0000) GS:ffff897d36a80000(0000) knlGS:0000000000000000
      [139590.042847] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
      [139590.043199] CR2: 00007f8c064935e0 CR3: 0000000232252002 CR4: 00000000003606e0
      [139590.043547] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
      [139590.043899] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
      [139590.044250] Call Trace:
      [139590.044631]  copy_items+0xa3f/0x1000 [btrfs]
      [139590.045009]  ? generic_bin_search.constprop.32+0x61/0x200 [btrfs]
      [139590.045396]  btrfs_log_inode+0x7b3/0xd70 [btrfs]
      [139590.045773]  btrfs_log_inode_parent+0x2b3/0xce0 [btrfs]
      [139590.046143]  ? do_raw_spin_unlock+0x49/0xc0
      [139590.046510]  btrfs_log_dentry_safe+0x4a/0x70 [btrfs]
      [139590.046872]  btrfs_sync_file+0x3b6/0x440 [btrfs]
      [139590.047243]  btrfs_file_write_iter+0x45b/0x5c0 [btrfs]
      [139590.047592]  __vfs_write+0x129/0x1c0
      [139590.047932]  vfs_write+0xc2/0x1b0
      [139590.048270]  ksys_write+0x55/0xc0
      [139590.048608]  do_syscall_64+0x60/0x1b0
      [139590.048946]  entry_SYSCALL_64_after_hwframe+0x49/0xbe
      [139590.049287] RIP: 0033:0x7f2efc4be190
      (...)
      [139590.050342] RSP: 002b:00007ffe743243a8 EFLAGS: 00000246 ORIG_RAX: 0000000000000001
      [139590.050701] RAX: ffffffffffffffda RBX: 0000000000008d58 RCX: 00007f2efc4be190
      [139590.051067] RDX: 0000000000008d58 RSI: 00005567eca0f370 RDI: 0000000000000003
      [139590.051459] RBP: 0000000000000024 R08: 0000000000000003 R09: 0000000000008d60
      [139590.051863] R10: 0000000000000078 R11: 0000000000000246 R12: 0000000000000003
      [139590.052252] R13: 00000000003d3507 R14: 00005567eca0f370 R15: 0000000000000000
      (...)
      [139590.055128] ---[ end trace 193f35d0215cdeeb ]---
    
    So fix this race between a full ranged fsync and writeback of adjacent
    ranges by flushing all delalloc and waiting for all ordered extents to
    complete before logging the inode. This is the simplest way to solve the
    problem because currently the full fsync path does not deal with ranges
    at all (it assumes a full range from 0 to LLONG_MAX) and it always needs
    to look at adjacent ranges for hole detection. For use cases of ranged
    fsyncs this can make a few fsyncs slower but on the other hand it can
    make some following fsyncs to other ranges do less work or no need to do
    anything at all. A full fsync is rare anyway and happens only once after
    loading/creating an inode and once after less common operations such as a
    shrinking truncate.
    
    This is an issue that exists for a long time, and was often triggered by
    generic/127, because it does mmap'ed writes and msync (which triggers a
    ranged fsync). Adding support for the tree checker to detect overlapping
    extents (next patch in the series) and trigger a WARN() when such cases
    are found, and then calling btrfs_check_leaf_full() at the end of
    btrfs_insert_file_extent() made the issue much easier to detect. Running
    btrfs/072 with that change to the tree checker and making fsstress open
    files always with O_SYNC made it much easier to trigger the issue (as
    triggering it with generic/127 is very rare).
    
    CC: stable@vger.kernel.org # 3.16+
    Reviewed-by: Josef Bacik <josef@toxicpanda.com>
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 9dbea72a61fe..89f5be2bfb43 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2067,6 +2067,18 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 	int ret = 0, err;
 	u64 len;
 
+	/*
+	 * If the inode needs a full sync, make sure we use a full range to
+	 * avoid log tree corruption, due to hole detection racing with ordered
+	 * extent completion for adjacent ranges, and assertion failures during
+	 * hole detection.
+	 */
+	if (test_bit(BTRFS_INODE_NEEDS_FULL_SYNC,
+		     &BTRFS_I(inode)->runtime_flags)) {
+		start = 0;
+		end = LLONG_MAX;
+	}
+
 	/*
 	 * The range length can be represented by u64, we have to do the typecasts
 	 * to avoid signed overflow if it's [0, LLONG_MAX] eg. from fsync()

commit 8fca955057b9c58467d1b231e43f19c4cf26ae8c
Author: Josef Bacik <josef@toxicpanda.com>
Date:   Fri May 3 11:10:06 2019 -0400

    btrfs: don't double unlock on error in btrfs_punch_hole
    
    If we have an error writing out a delalloc range in
    btrfs_punch_hole_lock_range we'll unlock the inode and then goto
    out_only_mutex, where we will again unlock the inode.  This is bad,
    don't do this.
    
    Fixes: f27451f22996 ("Btrfs: add support for fallocate's zero range operation")
    CC: stable@vger.kernel.org # 4.19+
    Reviewed-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: Josef Bacik <josef@toxicpanda.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 7e85dca0e6f2..9dbea72a61fe 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2554,10 +2554,8 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 
 	ret = btrfs_punch_hole_lock_range(inode, lockstart, lockend,
 					  &cached_state);
-	if (ret) {
-		inode_unlock(inode);
+	if (ret)
 		goto out_only_mutex;
-	}
 
 	path = btrfs_alloc_path();
 	if (!path) {

commit ffd4bb2a19cd29681f5b70a200654ab92619de8a
Author: Qu Wenruo <wqu@suse.com>
Date:   Thu Apr 4 14:45:36 2019 +0800

    btrfs: extent-tree: Use btrfs_ref to refactor btrfs_free_extent()
    
    Similar to btrfs_inc_extent_ref(), use btrfs_ref to replace the long
    parameter list and the confusing @owner parameter.
    
    Signed-off-by: Qu Wenruo <wqu@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index a4fc89a84baf..7e85dca0e6f2 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -997,11 +997,14 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 				extent_end = ALIGN(extent_end,
 						   fs_info->sectorsize);
 			} else if (update_refs && disk_bytenr > 0) {
-				ret = btrfs_free_extent(trans, root,
-						disk_bytenr, num_bytes, 0,
+				btrfs_init_generic_ref(&ref,
+						BTRFS_DROP_DELAYED_REF,
+						disk_bytenr, num_bytes, 0);
+				btrfs_init_data_ref(&ref,
 						root->root_key.objectid,
-						key.objectid, key.offset -
-						extent_offset);
+						key.objectid,
+						key.offset - extent_offset);
+				ret = btrfs_free_extent(trans, &ref);
 				BUG_ON(ret); /* -ENOMEM */
 				inode_sub_bytes(inode,
 						extent_end - key.offset);
@@ -1318,6 +1321,9 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 
 	other_start = end;
 	other_end = 0;
+	btrfs_init_generic_ref(&ref, BTRFS_DROP_DELAYED_REF, bytenr,
+			       num_bytes, 0);
+	btrfs_init_data_ref(&ref, root->root_key.objectid, ino, orig_offset);
 	if (extent_mergeable(leaf, path->slots[0] + 1,
 			     ino, bytenr, orig_offset,
 			     &other_start, &other_end)) {
@@ -1328,9 +1334,7 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 		extent_end = other_end;
 		del_slot = path->slots[0] + 1;
 		del_nr++;
-		ret = btrfs_free_extent(trans, root, bytenr, num_bytes,
-					0, root->root_key.objectid,
-					ino, orig_offset);
+		ret = btrfs_free_extent(trans, &ref);
 		if (ret) {
 			btrfs_abort_transaction(trans, ret);
 			goto out;
@@ -1348,9 +1352,7 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 		key.offset = other_start;
 		del_slot = path->slots[0];
 		del_nr++;
-		ret = btrfs_free_extent(trans, root, bytenr, num_bytes,
-					0, root->root_key.objectid,
-					ino, orig_offset);
+		ret = btrfs_free_extent(trans, &ref);
 		if (ret) {
 			btrfs_abort_transaction(trans, ret);
 			goto out;

commit 82fa113fccc41fe5204b4ce35341d69ebde0020f
Author: Qu Wenruo <wqu@suse.com>
Date:   Thu Apr 4 14:45:35 2019 +0800

    btrfs: extent-tree: Use btrfs_ref to refactor btrfs_inc_extent_ref()
    
    Use the new btrfs_ref structure and replace parameter list to clean up
    the usage of owner and level to distinguish the extent types.
    
    Signed-off-by: Qu Wenruo <wqu@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index c857a884a90f..a4fc89a84baf 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -754,6 +754,7 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 	struct btrfs_fs_info *fs_info = root->fs_info;
 	struct extent_buffer *leaf;
 	struct btrfs_file_extent_item *fi;
+	struct btrfs_ref ref = { 0 };
 	struct btrfs_key key;
 	struct btrfs_key new_key;
 	u64 ino = btrfs_ino(BTRFS_I(inode));
@@ -909,11 +910,14 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 			btrfs_mark_buffer_dirty(leaf);
 
 			if (update_refs && disk_bytenr > 0) {
-				ret = btrfs_inc_extent_ref(trans, root,
-						disk_bytenr, num_bytes, 0,
+				btrfs_init_generic_ref(&ref,
+						BTRFS_ADD_DELAYED_REF,
+						disk_bytenr, num_bytes, 0);
+				btrfs_init_data_ref(&ref,
 						root->root_key.objectid,
 						new_key.objectid,
 						start - extent_offset);
+				ret = btrfs_inc_extent_ref(trans, &ref);
 				BUG_ON(ret); /* -ENOMEM */
 			}
 			key.offset = start;
@@ -1142,6 +1146,7 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 	struct extent_buffer *leaf;
 	struct btrfs_path *path;
 	struct btrfs_file_extent_item *fi;
+	struct btrfs_ref ref = { 0 };
 	struct btrfs_key key;
 	struct btrfs_key new_key;
 	u64 bytenr;
@@ -1287,9 +1292,11 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 						extent_end - split);
 		btrfs_mark_buffer_dirty(leaf);
 
-		ret = btrfs_inc_extent_ref(trans, root, bytenr, num_bytes,
-					   0, root->root_key.objectid,
-					   ino, orig_offset);
+		btrfs_init_generic_ref(&ref, BTRFS_ADD_DELAYED_REF, bytenr,
+				       num_bytes, 0);
+		btrfs_init_data_ref(&ref, root->root_key.objectid, ino,
+				    orig_offset);
+		ret = btrfs_inc_extent_ref(trans, &ref);
 		if (ret) {
 			btrfs_abort_transaction(trans, ret);
 			goto out;

commit 39ad317315887c2cb9a4347a93a8859326ddf136
Author: Robbie Ko <robbieko@synology.com>
Date:   Tue Mar 26 11:56:11 2019 +0800

    Btrfs: fix data bytes_may_use underflow with fallocate due to failed quota reserve
    
    When doing fallocate, we first add the range to the reserve_list and
    then reserve the quota.  If quota reservation fails, we'll release all
    reserved parts of reserve_list.
    
    However, cur_offset is not updated to indicate that this range is
    already been inserted into the list.  Therefore, the same range is freed
    twice.  Once at list_for_each_entry loop, and once at the end of the
    function.  This will result in WARN_ON on bytes_may_use when we free the
    remaining space.
    
    At the end, under the 'out' label we have a call to:
    
       btrfs_free_reserved_data_space(inode, data_reserved, alloc_start, alloc_end - cur_offset);
    
    The start offset, third argument, should be cur_offset.
    
    Everything from alloc_start to cur_offset was freed by the
    list_for_each_entry_safe_loop.
    
    Fixes: 18513091af94 ("btrfs: update btrfs_space_info's bytes_may_use timely")
    Reviewed-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: Robbie Ko <robbieko@synology.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 15cc3b861346..c857a884a90f 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -3131,6 +3131,7 @@ static long btrfs_fallocate(struct file *file, int mode,
 			ret = btrfs_qgroup_reserve_data(inode, &data_reserved,
 					cur_offset, last_byte - cur_offset);
 			if (ret < 0) {
+				cur_offset = last_byte;
 				free_extent_map(em);
 				break;
 			}
@@ -3180,7 +3181,7 @@ static long btrfs_fallocate(struct file *file, int mode,
 	/* Let go of our reservation. */
 	if (ret != 0 && !(mode & FALLOC_FL_ZERO_RANGE))
 		btrfs_free_reserved_data_space(inode, data_reserved,
-				alloc_start, alloc_end - cur_offset);
+				cur_offset, alloc_end - cur_offset);
 	extent_changeset_free(data_reserved);
 	return ret;
 }

commit e902baac656479bdb956224ed693578424cf9e96
Author: David Sterba <dsterba@suse.com>
Date:   Wed Mar 20 14:36:46 2019 +0100

    btrfs: get fs_info from eb in btrfs_leaf_free_space
    
    We can read fs_info from extent buffer and can drop it from the
    parameters.
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 5e6aee84daee..15cc3b861346 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1050,7 +1050,7 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 	if (!ret && replace_extent && leafs_visited == 1 &&
 	    (path->locks[0] == BTRFS_WRITE_LOCK_BLOCKING ||
 	     path->locks[0] == BTRFS_WRITE_LOCK) &&
-	    btrfs_leaf_free_space(fs_info, leaf) >=
+	    btrfs_leaf_free_space(leaf) >=
 	    sizeof(struct btrfs_item) + extent_item_size) {
 
 		key.objectid = ino;

commit 290342f66108638048997b71393f0dd88e771352
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Mon Mar 25 14:02:25 2019 +0100

    btrfs: use BUG() instead of BUG_ON(1)
    
    BUG_ON(1) leads to bogus warnings from clang when
    CONFIG_PROFILE_ANNOTATED_BRANCHES is set:
    
    fs/btrfs/volumes.c:5041:3: error: variable 'max_chunk_size' is used uninitialized whenever 'if' condition is false
          [-Werror,-Wsometimes-uninitialized]
                    BUG_ON(1);
                    ^~~~~~~~~
    include/asm-generic/bug.h:61:36: note: expanded from macro 'BUG_ON'
     #define BUG_ON(condition) do { if (unlikely(condition)) BUG(); } while (0)
                                       ^~~~~~~~~~~~~~~~~~~
    include/linux/compiler.h:48:23: note: expanded from macro 'unlikely'
     #  define unlikely(x)   (__branch_check__(x, 0, __builtin_constant_p(x)))
                            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    fs/btrfs/volumes.c:5046:9: note: uninitialized use occurs here
                                 max_chunk_size);
                                 ^~~~~~~~~~~~~~
    include/linux/kernel.h:860:36: note: expanded from macro 'min'
     #define min(x, y)       __careful_cmp(x, y, <)
                                             ^
    include/linux/kernel.h:853:17: note: expanded from macro '__careful_cmp'
                    __cmp_once(x, y, __UNIQUE_ID(__x), __UNIQUE_ID(__y), op))
                                  ^
    include/linux/kernel.h:847:25: note: expanded from macro '__cmp_once'
                    typeof(y) unique_y = (y);               \
                                          ^
    fs/btrfs/volumes.c:5041:3: note: remove the 'if' if its condition is always true
                    BUG_ON(1);
                    ^
    include/asm-generic/bug.h:61:32: note: expanded from macro 'BUG_ON'
     #define BUG_ON(condition) do { if (unlikely(condition)) BUG(); } while (0)
                                   ^
    fs/btrfs/volumes.c:4993:20: note: initialize the variable 'max_chunk_size' to silence this warning
            u64 max_chunk_size;
                              ^
                               = 0
    
    Change it to BUG() so clang can see that this code path can never
    continue.
    
    Reviewed-by: Nikolay Borisov <nborisov@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 94c1c86fd18a..5e6aee84daee 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1025,7 +1025,7 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 			continue;
 		}
 
-		BUG_ON(1);
+		BUG();
 	}
 
 	if (!ret && del_nr > 0) {

commit 3b1da515c64e18bdd6a13348313f1168396b3722
Author: Filipe Manana <fdmanana@suse.com>
Date:   Mon Mar 11 13:10:56 2019 +0000

    Btrfs: remove no longer used 'sync' member from transaction handle
    
    Commit db2462a6ad3d ("btrfs: don't run delayed refs in the end transaction
    logic") removed its last use, so now it does absolutely nothing, therefore
    remove it.
    
    Reviewed-by: Nikolay Borisov <nborisov@suse.com>
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 34fe8a58b0e9..94c1c86fd18a 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2165,7 +2165,6 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 		inode_unlock(inode);
 		goto out;
 	}
-	trans->sync = true;
 
 	ret = btrfs_log_dentry_safe(trans, dentry, start, end, &ctx);
 	if (ret < 0) {

commit 4ab47a8d9ce2d0d6b73dedbda7a5ee0c545af526
Author: Nikolay Borisov <nborisov@suse.com>
Date:   Wed Dec 12 09:42:32 2018 +0200

    btrfs: Remove unused arguments from btrfs_get_extent_fiemap
    
    This function is a simple wrapper over btrfs_get_extent that returns
    either:
    
    a) A real extent in the passed range or
    b) Adjusted extent based on whether delalloc bytes are found backing up
       a hole.
    
    To support these semantics it doesn't need the page/pg_offset/create
    arguments which are passed to btrfs_get_extent in case an extent is to
    be created. So simplify the function by removing the unused arguments.
    No functional changes.
    
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index d38dc8c31533..34fe8a58b0e9 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -3218,8 +3218,7 @@ static int find_desired_extent(struct inode *inode, loff_t *offset, int whence)
 			 &cached_state);
 
 	while (start < inode->i_size) {
-		em = btrfs_get_extent_fiemap(BTRFS_I(inode), NULL, 0,
-				start, len, 0);
+		em = btrfs_get_extent_fiemap(BTRFS_I(inode), start, len);
 		if (IS_ERR(em)) {
 			ret = PTR_ERR(em);
 			em = NULL;

commit 52042d8e82ff50d40e76a275ac0b97aa663328b0
Author: Andrea Gelmini <andrea.gelmini@gelma.net>
Date:   Wed Nov 28 12:05:13 2018 +0100

    btrfs: Fix typos in comments and strings
    
    The typos accumulate over time so once in a while time they get fixed in
    a large patch.
    
    Signed-off-by: Andrea Gelmini <andrea.gelmini@gelma.net>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 81aae230d1a5..d38dc8c31533 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2005,7 +2005,7 @@ int btrfs_release_file(struct inode *inode, struct file *filp)
 	filp->private_data = NULL;
 
 	/*
-	 * ordered_data_close is set by settattr when we are about to truncate
+	 * ordered_data_close is set by setattr when we are about to truncate
 	 * a file from a non-zero size to a zero size.  This tries to
 	 * flush down new bytes that may have been written if the
 	 * application were using truncate to replace a file in place.
@@ -2114,7 +2114,7 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 
 	/*
 	 * We have to do this here to avoid the priority inversion of waiting on
-	 * IO of a lower priority task while holding a transaciton open.
+	 * IO of a lower priority task while holding a transaction open.
 	 */
 	ret = btrfs_wait_ordered_range(inode, start, len);
 	if (ret) {
@@ -2154,7 +2154,7 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 	 * here we could get into a situation where we're waiting on IO to
 	 * happen that is blocked on a transaction trying to commit.  With start
 	 * we inc the extwriter counter, so we wait for all extwriters to exit
-	 * before we start blocking join'ers.  This comment is to keep somebody
+	 * before we start blocking joiners.  This comment is to keep somebody
 	 * from thinking they are super smart and changing this to
 	 * btrfs_join_transaction *cough*Josef*cough*.
 	 */

commit 7073017aeb98db311ca407f0f552f2bfc1af3015
Author: Johannes Thumshirn <jthumshirn@suse.de>
Date:   Wed Dec 5 15:23:03 2018 +0100

    btrfs: use offset_in_page instead of open-coding it
    
    Constructs like 'var & (PAGE_SIZE - 1)' or 'var & ~PAGE_MASK' can denote an
    offset into a page.
    
    So replace them by the offset_in_page() macro instead of open-coding it if
    they're not used as an alignment check.
    
    Reviewed-by: Nikolay Borisov <nborisov@suse.com>
    Signed-off-by: Johannes Thumshirn <jthumshirn@suse.de>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 3835bb8c146d..81aae230d1a5 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -399,7 +399,7 @@ static noinline int btrfs_copy_from_user(loff_t pos, size_t write_bytes,
 	size_t copied = 0;
 	size_t total_copied = 0;
 	int pg = 0;
-	int offset = pos & (PAGE_SIZE - 1);
+	int offset = offset_in_page(pos);
 
 	while (write_bytes > 0) {
 		size_t count = min_t(size_t,
@@ -1611,7 +1611,7 @@ static noinline ssize_t btrfs_buffered_write(struct kiocb *iocb,
 		return -ENOMEM;
 
 	while (iov_iter_count(i) > 0) {
-		size_t offset = pos & (PAGE_SIZE - 1);
+		size_t offset = offset_in_page(pos);
 		size_t sector_offset;
 		size_t write_bytes = min(iov_iter_count(i),
 					 nrptrs * (size_t)PAGE_SIZE -

commit 6d4cbf790307515dcd91eb2165e36e9b83d0fab6
Author: Filipe Manana <fdmanana@suse.com>
Date:   Mon Nov 12 10:24:30 2018 +0000

    Btrfs: remove no longer used io_err from btrfs_log_ctx
    
    The io_err field of struct btrfs_log_ctx is no longer used after the
    recent simplification of the fast fsync path, where we now wait for
    ordered extents to complete before logging the inode. We did this in
    commit b5e6c3e170b7 ("btrfs: always wait on ordered extents at fsync
    time") and commit a2120a473a80 ("btrfs: clean up the left over
    logged_list usage") removed its last use.
    
    Reviewed-by: Nikolay Borisov <nborisov@suse.com>
    Reviewed-by: Josef Bacik <josef@toxicpanda.com>
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 58e93bce3036..3835bb8c146d 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2186,25 +2186,6 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 	up_write(&BTRFS_I(inode)->dio_sem);
 	inode_unlock(inode);
 
-	/*
-	 * If any of the ordered extents had an error, just return it to user
-	 * space, so that the application knows some writes didn't succeed and
-	 * can take proper action (retry for e.g.). Blindly committing the
-	 * transaction in this case, would fool userspace that everything was
-	 * successful. And we also want to make sure our log doesn't contain
-	 * file extent items pointing to extents that weren't fully written to -
-	 * just like in the non fast fsync path, where we check for the ordered
-	 * operation's error flag before writing to the log tree and return -EIO
-	 * if any of them had this flag set (btrfs_wait_ordered_range) -
-	 * therefore we need to check for errors in the ordered operations,
-	 * which are indicated by ctx.io_err.
-	 */
-	if (ctx.io_err) {
-		btrfs_end_transaction(trans);
-		ret = ctx.io_err;
-		goto out;
-	}
-
 	if (ret != BTRFS_NO_LOG_SYNC) {
 		if (!ret) {
 			ret = btrfs_sync_log(trans, root, &ctx);

commit 121b018f8c74b4e0ba81b4b8ee73a82db3f24b7b
Merge: 5b26f7180cdb 42a657f57628
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Nov 28 08:38:20 2018 -0800

    Merge tag 'for-4.20-rc4-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/kdave/linux
    
    Pull btrfs fixes from David Sterba:
     "Some of these bugs are being hit during testing so we'd like to get
      them merged, otherwise there are usual stability fixes for stable
      trees"
    
    * tag 'for-4.20-rc4-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/kdave/linux:
      btrfs: relocation: set trans to be NULL after ending transaction
      Btrfs: fix race between enabling quotas and subvolume creation
      Btrfs: send, fix infinite loop due to directory rename dependencies
      Btrfs: ensure path name is null terminated at btrfs_control_ioctl
      Btrfs: fix rare chances for data loss when doing a fast fsync
      btrfs: Always try all copies when reading extent buffers

commit aab15e8ec25765cf7968c72cbec7583acf99d8a4
Author: Filipe Manana <fdmanana@suse.com>
Date:   Mon Nov 12 10:23:58 2018 +0000

    Btrfs: fix rare chances for data loss when doing a fast fsync
    
    After the simplification of the fast fsync patch done recently by commit
    b5e6c3e170b7 ("btrfs: always wait on ordered extents at fsync time") and
    commit e7175a692765 ("btrfs: remove the wait ordered logic in the
    log_one_extent path"), we got a very short time window where we can get
    extents logged without writeback completing first or extents logged
    without logging the respective data checksums. Both issues can only happen
    when doing a non-full (fast) fsync.
    
    As soon as we enter btrfs_sync_file() we trigger writeback, then lock the
    inode and then wait for the writeback to complete before starting to log
    the inode. However before we acquire the inode's lock and after we started
    writeback, it's possible that more writes happened and dirtied more pages.
    If that happened and those pages get writeback triggered while we are
    logging the inode (for example, the VM subsystem triggering it due to
    memory pressure, or another concurrent fsync), we end up seeing the
    respective extent maps in the inode's list of modified extents and will
    log matching file extent items without waiting for the respective
    ordered extents to complete, meaning that either of the following will
    happen:
    
    1) We log an extent after its writeback finishes but before its checksums
       are added to the csum tree, leading to -EIO errors when attempting to
       read the extent after a log replay.
    
    2) We log an extent before its writeback finishes.
       Therefore after the log replay we will have a file extent item pointing
       to an unwritten extent (and without the respective data checksums as
       well).
    
    This could not happen before the fast fsync patch simplification, because
    for any extent we found in the list of modified extents, we would wait for
    its respective ordered extent to finish writeback or collect its checksums
    for logging if it did not complete yet.
    
    Fix this by triggering writeback again after acquiring the inode's lock
    and before waiting for ordered extents to complete.
    
    Fixes: e7175a692765 ("btrfs: remove the wait ordered logic in the log_one_extent path")
    Fixes: b5e6c3e170b7 ("btrfs: always wait on ordered extents at fsync time")
    CC: stable@vger.kernel.org # 4.19+
    Reviewed-by: Josef Bacik <josef@toxicpanda.com>
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 97c7a086f7bd..b92b7f05c3d5 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2088,6 +2088,30 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 
 	atomic_inc(&root->log_batch);
 
+	/*
+	 * Before we acquired the inode's lock, someone may have dirtied more
+	 * pages in the target range. We need to make sure that writeback for
+	 * any such pages does not start while we are logging the inode, because
+	 * if it does, any of the following might happen when we are not doing a
+	 * full inode sync:
+	 *
+	 * 1) We log an extent after its writeback finishes but before its
+	 *    checksums are added to the csum tree, leading to -EIO errors
+	 *    when attempting to read the extent after a log replay.
+	 *
+	 * 2) We can end up logging an extent before its writeback finishes.
+	 *    Therefore after the log replay we will have a file extent item
+	 *    pointing to an unwritten extent (and no data checksums as well).
+	 *
+	 * So trigger writeback for any eventual new dirty pages and then we
+	 * wait for all ordered extents to complete below.
+	 */
+	ret = start_ordered_ops(inode, start, end);
+	if (ret) {
+		inode_unlock(inode);
+		goto out;
+	}
+
 	/*
 	 * We have to do this here to avoid the priority inversion of waiting on
 	 * IO of a lower priority task while holding a transaciton open.

commit c2aa1a444cab2c673650ada80a7dffc4345ce2e6
Merge: b69f9e17a57a bf4a1fcf0bc1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Nov 2 09:33:08 2018 -0700

    Merge tag 'xfs-4.20-merge-2' of git://git.kernel.org/pub/scm/fs/xfs/xfs-linux
    
    Pull vfs dedup fixes from Dave Chinner:
     "This reworks the vfs data cloning infrastructure.
    
      We discovered many issues with these interfaces late in the 4.19 cycle
      - the worst of them (data corruption, setuid stripping) were fixed for
      XFS in 4.19-rc8, but a larger rework of the infrastructure fixing all
      the problems was needed. That rework is the contents of this pull
      request.
    
      Rework the vfs_clone_file_range and vfs_dedupe_file_range
      infrastructure to use a common .remap_file_range method and supply
      generic bounds and sanity checking functions that are shared with the
      data write path. The current VFS infrastructure has problems with
      rlimit, LFS file sizes, file time stamps, maximum filesystem file
      sizes, stripping setuid bits, etc and so they are addressed in these
      commits.
    
      We also introduce the ability for the ->remap_file_range methods to
      return short clones so that clones for vfs_copy_file_range() don't get
      rejected if the entire range can't be cloned. It also allows
      filesystems to sliently skip deduplication of partial EOF blocks if
      they are not capable of doing so without requiring errors to be thrown
      to userspace.
    
      Existing filesystems are converted to user the new remap_file_range
      method, and both XFS and ocfs2 are modified to make use of the new
      generic checking infrastructure"
    
    * tag 'xfs-4.20-merge-2' of git://git.kernel.org/pub/scm/fs/xfs/xfs-linux: (28 commits)
      xfs: remove [cm]time update from reflink calls
      xfs: remove xfs_reflink_remap_range
      xfs: remove redundant remap partial EOF block checks
      xfs: support returning partial reflink results
      xfs: clean up xfs_reflink_remap_blocks call site
      xfs: fix pagecache truncation prior to reflink
      ocfs2: remove ocfs2_reflink_remap_range
      ocfs2: support partial clone range and dedupe range
      ocfs2: fix pagecache truncation prior to reflink
      ocfs2: truncate page cache for clone destination file before remapping
      vfs: clean up generic_remap_file_range_prep return value
      vfs: hide file range comparison function
      vfs: enable remap callers that can handle short operations
      vfs: plumb remap flags through the vfs dedupe functions
      vfs: plumb remap flags through the vfs clone functions
      vfs: make remap_file_range functions take and return bytes completed
      vfs: remap helper should update destination inode metadata
      vfs: pass remap flags to generic_remap_checks
      vfs: pass remap flags to generic_remap_file_range_prep
      vfs: combine the clone and dedupe into a single remap_file_range
      ...

commit 2e5dfc99f2e61c42083ba742395e7a7b353513d1
Author: Darrick J. Wong <darrick.wong@oracle.com>
Date:   Tue Oct 30 10:41:21 2018 +1100

    vfs: combine the clone and dedupe into a single remap_file_range
    
    Combine the clone_file_range and dedupe_file_range operations into a
    single remap_file_range file operation dispatch since they're
    fundamentally the same operation.  The differences between the two can
    be made in the prep functions.
    
    Signed-off-by: Darrick J. Wong <darrick.wong@oracle.com>
    Reviewed-by: Amir Goldstein <amir73il@gmail.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Dave Chinner <david@fromorbit.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 2be00e873e92..9a963f061393 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -3269,8 +3269,7 @@ const struct file_operations btrfs_file_operations = {
 #ifdef CONFIG_COMPAT
 	.compat_ioctl	= btrfs_compat_ioctl,
 #endif
-	.clone_file_range = btrfs_clone_file_range,
-	.dedupe_file_range = btrfs_dedupe_file_range,
+	.remap_file_range = btrfs_remap_file_range,
 };
 
 void __cold btrfs_auto_defrag_exit(void)

commit c495144bc6962186feae31d687596d2472000e45
Author: Josef Bacik <josef@toxicpanda.com>
Date:   Fri Oct 12 15:32:32 2018 -0400

    btrfs: move the dio_sem higher up the callchain
    
    We're getting a lockdep splat because we take the dio_sem under the
    log_mutex.  What we really need is to protect fsync() from logging an
    extent map for an extent we never waited on higher up, so just guard the
    whole thing with dio_sem.
    
    ======================================================
    WARNING: possible circular locking dependency detected
    4.18.0-rc4-xfstests-00025-g5de5edbaf1d4 #411 Not tainted
    ------------------------------------------------------
    aio-dio-invalid/30928 is trying to acquire lock:
    0000000092621cfd (&mm->mmap_sem){++++}, at: get_user_pages_unlocked+0x5a/0x1e0
    
    but task is already holding lock:
    00000000cefe6b35 (&ei->dio_sem){++++}, at: btrfs_direct_IO+0x3be/0x400
    
    which lock already depends on the new lock.
    
    the existing dependency chain (in reverse order) is:
    
    -> #5 (&ei->dio_sem){++++}:
           lock_acquire+0xbd/0x220
           down_write+0x51/0xb0
           btrfs_log_changed_extents+0x80/0xa40
           btrfs_log_inode+0xbaf/0x1000
           btrfs_log_inode_parent+0x26f/0xa80
           btrfs_log_dentry_safe+0x50/0x70
           btrfs_sync_file+0x357/0x540
           do_fsync+0x38/0x60
           __ia32_sys_fdatasync+0x12/0x20
           do_fast_syscall_32+0x9a/0x2f0
           entry_SYSENTER_compat+0x84/0x96
    
    -> #4 (&ei->log_mutex){+.+.}:
           lock_acquire+0xbd/0x220
           __mutex_lock+0x86/0xa10
           btrfs_record_unlink_dir+0x2a/0xa0
           btrfs_unlink+0x5a/0xc0
           vfs_unlink+0xb1/0x1a0
           do_unlinkat+0x264/0x2b0
           do_fast_syscall_32+0x9a/0x2f0
           entry_SYSENTER_compat+0x84/0x96
    
    -> #3 (sb_internal#2){.+.+}:
           lock_acquire+0xbd/0x220
           __sb_start_write+0x14d/0x230
           start_transaction+0x3e6/0x590
           btrfs_evict_inode+0x475/0x640
           evict+0xbf/0x1b0
           btrfs_run_delayed_iputs+0x6c/0x90
           cleaner_kthread+0x124/0x1a0
           kthread+0x106/0x140
           ret_from_fork+0x3a/0x50
    
    -> #2 (&fs_info->cleaner_delayed_iput_mutex){+.+.}:
           lock_acquire+0xbd/0x220
           __mutex_lock+0x86/0xa10
           btrfs_alloc_data_chunk_ondemand+0x197/0x530
           btrfs_check_data_free_space+0x4c/0x90
           btrfs_delalloc_reserve_space+0x20/0x60
           btrfs_page_mkwrite+0x87/0x520
           do_page_mkwrite+0x31/0xa0
           __handle_mm_fault+0x799/0xb00
           handle_mm_fault+0x7c/0xe0
           __do_page_fault+0x1d3/0x4a0
           async_page_fault+0x1e/0x30
    
    -> #1 (sb_pagefaults){.+.+}:
           lock_acquire+0xbd/0x220
           __sb_start_write+0x14d/0x230
           btrfs_page_mkwrite+0x6a/0x520
           do_page_mkwrite+0x31/0xa0
           __handle_mm_fault+0x799/0xb00
           handle_mm_fault+0x7c/0xe0
           __do_page_fault+0x1d3/0x4a0
           async_page_fault+0x1e/0x30
    
    -> #0 (&mm->mmap_sem){++++}:
           __lock_acquire+0x42e/0x7a0
           lock_acquire+0xbd/0x220
           down_read+0x48/0xb0
           get_user_pages_unlocked+0x5a/0x1e0
           get_user_pages_fast+0xa4/0x150
           iov_iter_get_pages+0xc3/0x340
           do_direct_IO+0xf93/0x1d70
           __blockdev_direct_IO+0x32d/0x1c20
           btrfs_direct_IO+0x227/0x400
           generic_file_direct_write+0xcf/0x180
           btrfs_file_write_iter+0x308/0x58c
           aio_write+0xf8/0x1d0
           io_submit_one+0x3a9/0x620
           __ia32_compat_sys_io_submit+0xb2/0x270
           do_int80_syscall_32+0x5b/0x1a0
           entry_INT80_compat+0x88/0xa0
    
    other info that might help us debug this:
    
    Chain exists of:
      &mm->mmap_sem --> &ei->log_mutex --> &ei->dio_sem
    
     Possible unsafe locking scenario:
    
           CPU0                    CPU1
           ----                    ----
      lock(&ei->dio_sem);
                                   lock(&ei->log_mutex);
                                   lock(&ei->dio_sem);
      lock(&mm->mmap_sem);
    
     *** DEADLOCK ***
    
    1 lock held by aio-dio-invalid/30928:
     #0: 00000000cefe6b35 (&ei->dio_sem){++++}, at: btrfs_direct_IO+0x3be/0x400
    
    stack backtrace:
    CPU: 0 PID: 30928 Comm: aio-dio-invalid Not tainted 4.18.0-rc4-xfstests-00025-g5de5edbaf1d4 #411
    Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.11.0-2.el7 04/01/2014
    Call Trace:
     dump_stack+0x7c/0xbb
     print_circular_bug.isra.37+0x297/0x2a4
     check_prev_add.constprop.45+0x781/0x7a0
     ? __lock_acquire+0x42e/0x7a0
     validate_chain.isra.41+0x7f0/0xb00
     __lock_acquire+0x42e/0x7a0
     lock_acquire+0xbd/0x220
     ? get_user_pages_unlocked+0x5a/0x1e0
     down_read+0x48/0xb0
     ? get_user_pages_unlocked+0x5a/0x1e0
     get_user_pages_unlocked+0x5a/0x1e0
     get_user_pages_fast+0xa4/0x150
     iov_iter_get_pages+0xc3/0x340
     do_direct_IO+0xf93/0x1d70
     ? __alloc_workqueue_key+0x358/0x490
     ? __blockdev_direct_IO+0x14b/0x1c20
     __blockdev_direct_IO+0x32d/0x1c20
     ? btrfs_run_delalloc_work+0x40/0x40
     ? can_nocow_extent+0x490/0x490
     ? kvm_clock_read+0x1f/0x30
     ? can_nocow_extent+0x490/0x490
     ? btrfs_run_delalloc_work+0x40/0x40
     btrfs_direct_IO+0x227/0x400
     ? btrfs_run_delalloc_work+0x40/0x40
     generic_file_direct_write+0xcf/0x180
     btrfs_file_write_iter+0x308/0x58c
     aio_write+0xf8/0x1d0
     ? kvm_clock_read+0x1f/0x30
     ? __might_fault+0x3e/0x90
     io_submit_one+0x3a9/0x620
     ? io_submit_one+0xe5/0x620
     __ia32_compat_sys_io_submit+0xb2/0x270
     do_int80_syscall_32+0x5b/0x1a0
     entry_INT80_compat+0x88/0xa0
    
    CC: stable@vger.kernel.org # 4.14+
    Reviewed-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: Josef Bacik <josef@toxicpanda.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 15b925142793..97c7a086f7bd 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2078,6 +2078,14 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 		goto out;
 
 	inode_lock(inode);
+
+	/*
+	 * We take the dio_sem here because the tree log stuff can race with
+	 * lockless dio writes and get an extent map logged for an extent we
+	 * never waited on.  We need it this high up for lockdep reasons.
+	 */
+	down_write(&BTRFS_I(inode)->dio_sem);
+
 	atomic_inc(&root->log_batch);
 
 	/*
@@ -2086,6 +2094,7 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 	 */
 	ret = btrfs_wait_ordered_range(inode, start, len);
 	if (ret) {
+		up_write(&BTRFS_I(inode)->dio_sem);
 		inode_unlock(inode);
 		goto out;
 	}
@@ -2109,6 +2118,7 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 		 * checked called fsync.
 		 */
 		ret = filemap_check_wb_err(inode->i_mapping, file->f_wb_err);
+		up_write(&BTRFS_I(inode)->dio_sem);
 		inode_unlock(inode);
 		goto out;
 	}
@@ -2127,6 +2137,7 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 	trans = btrfs_start_transaction(root, 0);
 	if (IS_ERR(trans)) {
 		ret = PTR_ERR(trans);
+		up_write(&BTRFS_I(inode)->dio_sem);
 		inode_unlock(inode);
 		goto out;
 	}
@@ -2148,6 +2159,7 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 	 * file again, but that will end up using the synchronization
 	 * inside btrfs_sync_log to keep things safe.
 	 */
+	up_write(&BTRFS_I(inode)->dio_sem);
 	inode_unlock(inode);
 
 	/*

commit 7703bdd8d23e6ef057af3253958a793ec6066b28
Author: Chris Mason <clm@fb.com>
Date:   Wed Jun 20 07:56:11 2018 -0700

    Btrfs: don't clean dirty pages during buffered writes
    
    During buffered writes, we follow this basic series of steps:
    
    again:
            lock all the pages
            wait for writeback on all the pages
            Take the extent range lock
            wait for ordered extents on the whole range
            clean all the pages
    
            if (copy_from_user_in_atomic() hits a fault) {
                    drop our locks
                    goto again;
            }
    
            dirty all the pages
            release all the locks
    
    The extra waiting, cleaning and locking are there to make sure we don't
    modify pages in flight to the drive, after they've been crc'd.
    
    If some of the pages in the range were already dirty when the write
    began, and we need to goto again, we create a window where a dirty page
    has been cleaned and unlocked.  It may be reclaimed before we're able to
    lock it again, which means we'll read the old contents off the drive and
    lose any modifications that had been pending writeback.
    
    We don't actually need to clean the pages.  All of the other locking in
    place makes sure we don't start IO on the pages, so we can just leave
    them dirty for the duration of the write.
    
    Fixes: 73d59314e6ed (the original btrfs merge)
    CC: stable@vger.kernel.org # v4.4+
    Signed-off-by: Chris Mason <clm@fb.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index d254cf94545f..15b925142793 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -531,6 +531,14 @@ int btrfs_dirty_pages(struct inode *inode, struct page **pages,
 
 	end_of_last_block = start_pos + num_bytes - 1;
 
+	/*
+	 * The pages may have already been dirty, clear out old accounting so
+	 * we can set things up properly
+	 */
+	clear_extent_bit(&BTRFS_I(inode)->io_tree, start_pos, end_of_last_block,
+			 EXTENT_DIRTY | EXTENT_DELALLOC |
+			 EXTENT_DO_ACCOUNTING | EXTENT_DEFRAG, 0, 0, cached);
+
 	if (!btrfs_is_free_space_inode(BTRFS_I(inode))) {
 		if (start_pos >= isize &&
 		    !(BTRFS_I(inode)->flags & BTRFS_INODE_PREALLOC)) {
@@ -1500,18 +1508,27 @@ lock_and_cleanup_extent_if_need(struct btrfs_inode *inode, struct page **pages,
 		}
 		if (ordered)
 			btrfs_put_ordered_extent(ordered);
-		clear_extent_bit(&inode->io_tree, start_pos, last_pos,
-				 EXTENT_DIRTY | EXTENT_DELALLOC |
-				 EXTENT_DO_ACCOUNTING | EXTENT_DEFRAG,
-				 0, 0, cached_state);
+
 		*lockstart = start_pos;
 		*lockend = last_pos;
 		ret = 1;
 	}
 
+	/*
+	 * It's possible the pages are dirty right now, but we don't want
+	 * to clean them yet because copy_from_user may catch a page fault
+	 * and we might have to fall back to one page at a time.  If that
+	 * happens, we'll unlock these pages and we'd have a window where
+	 * reclaim could sneak in and drop the once-dirty page on the floor
+	 * without writing it.
+	 *
+	 * We have the pages locked and the extent range locked, so there's
+	 * no way someone can start IO on any dirty pages in this range.
+	 *
+	 * We'll call btrfs_dirty_pages() later on, and that will flip around
+	 * delalloc bits and dirty the pages as required.
+	 */
 	for (i = 0; i < num_pages; i++) {
-		if (clear_page_dirty_for_io(pages[i]))
-			account_page_redirty(pages[i]);
 		set_page_extent_mapped(pages[i]);
 		WARN_ON(!PageLocked(pages[i]));
 	}

commit 3a58417486ca99a3bfef40e309f38adb45a5171d
Author: Lu Fengqi <lufq.fnst@cn.fujitsu.com>
Date:   Sat Aug 4 21:10:55 2018 +0800

    btrfs: switch update_size to bool in btrfs_block_rsv_migrate and btrfs_rsv_add_bytes
    
    Using true and false here is closer to the expected semantic than using
    0 and 1.  No functional change.
    
    Signed-off-by: Lu Fengqi <lufq.fnst@cn.fujitsu.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 2be00e873e92..d254cf94545f 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2544,7 +2544,7 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 	}
 
 	ret = btrfs_block_rsv_migrate(&fs_info->trans_block_rsv, rsv,
-				      min_size, 0);
+				      min_size, false);
 	BUG_ON(ret);
 	trans->block_rsv = rsv;
 
@@ -2594,7 +2594,7 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 		}
 
 		ret = btrfs_block_rsv_migrate(&fs_info->trans_block_rsv,
-					      rsv, min_size, 0);
+					      rsv, min_size, false);
 		BUG_ON(ret);	/* shouldn't happen */
 		trans->block_rsv = rsv;
 

commit d7f663fa3ff906247a979c1115bc92cbabfb19ba
Author: David Sterba <dsterba@suse.com>
Date:   Fri Jun 29 10:56:47 2018 +0200

    btrfs: prune unused includes
    
    Remove includes if none of the interfaces and exports is used in the
    given source file.
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index da53e45705ba..2be00e873e92 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -5,14 +5,11 @@
 
 #include <linux/fs.h>
 #include <linux/pagemap.h>
-#include <linux/highmem.h>
 #include <linux/time.h>
 #include <linux/init.h>
 #include <linux/string.h>
 #include <linux/backing-dev.h>
-#include <linux/mpage.h>
 #include <linux/falloc.h>
-#include <linux/swap.h>
 #include <linux/writeback.h>
 #include <linux/compat.h>
 #include <linux/slab.h>

commit 3ffbd68c48320730ef64ebfb5e639220f1f65483
Author: David Sterba <dsterba@suse.com>
Date:   Fri Jun 29 10:56:42 2018 +0200

    btrfs: simplify pointer chasing of local fs_info variables
    
    Functions that get btrfs inode can simply reach the fs_info by
    dereferencing the root and this looks a bit more straightforward
    compared to the btrfs_sb(...) indirection.
    
    If the transaction handle is available and not NULL it's used instead.
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 89c9404fee9a..da53e45705ba 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -83,7 +83,7 @@ static int __compare_inode_defrag(struct inode_defrag *defrag1,
 static int __btrfs_add_inode_defrag(struct btrfs_inode *inode,
 				    struct inode_defrag *defrag)
 {
-	struct btrfs_fs_info *fs_info = btrfs_sb(inode->vfs_inode.i_sb);
+	struct btrfs_fs_info *fs_info = inode->root->fs_info;
 	struct inode_defrag *entry;
 	struct rb_node **p;
 	struct rb_node *parent = NULL;
@@ -135,8 +135,8 @@ static inline int __need_auto_defrag(struct btrfs_fs_info *fs_info)
 int btrfs_add_inode_defrag(struct btrfs_trans_handle *trans,
 			   struct btrfs_inode *inode)
 {
-	struct btrfs_fs_info *fs_info = btrfs_sb(inode->vfs_inode.i_sb);
 	struct btrfs_root *root = inode->root;
+	struct btrfs_fs_info *fs_info = root->fs_info;
 	struct inode_defrag *defrag;
 	u64 transid;
 	int ret;
@@ -185,7 +185,7 @@ int btrfs_add_inode_defrag(struct btrfs_trans_handle *trans,
 static void btrfs_requeue_inode_defrag(struct btrfs_inode *inode,
 				       struct inode_defrag *defrag)
 {
-	struct btrfs_fs_info *fs_info = btrfs_sb(inode->vfs_inode.i_sb);
+	struct btrfs_fs_info *fs_info = inode->root->fs_info;
 	int ret;
 
 	if (!__need_auto_defrag(fs_info))
@@ -1132,7 +1132,7 @@ static int extent_mergeable(struct extent_buffer *leaf, int slot,
 int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 			      struct btrfs_inode *inode, u64 start, u64 end)
 {
-	struct btrfs_fs_info *fs_info = btrfs_sb(inode->vfs_inode.i_sb);
+	struct btrfs_fs_info *fs_info = trans->fs_info;
 	struct btrfs_root *root = inode->root;
 	struct extent_buffer *leaf;
 	struct btrfs_path *path;
@@ -1469,7 +1469,7 @@ lock_and_cleanup_extent_if_need(struct btrfs_inode *inode, struct page **pages,
 				u64 *lockstart, u64 *lockend,
 				struct extent_state **cached_state)
 {
-	struct btrfs_fs_info *fs_info = btrfs_sb(inode->vfs_inode.i_sb);
+	struct btrfs_fs_info *fs_info = inode->root->fs_info;
 	u64 start_pos;
 	u64 last_pos;
 	int i;
@@ -1525,7 +1525,7 @@ lock_and_cleanup_extent_if_need(struct btrfs_inode *inode, struct page **pages,
 static noinline int check_can_nocow(struct btrfs_inode *inode, loff_t pos,
 				    size_t *write_bytes)
 {
-	struct btrfs_fs_info *fs_info = btrfs_sb(inode->vfs_inode.i_sb);
+	struct btrfs_fs_info *fs_info = inode->root->fs_info;
 	struct btrfs_root *root = inode->root;
 	struct btrfs_ordered_extent *ordered;
 	u64 lockstart, lockend;
@@ -2227,7 +2227,7 @@ static int fill_holes(struct btrfs_trans_handle *trans,
 		struct btrfs_inode *inode,
 		struct btrfs_path *path, u64 offset, u64 end)
 {
-	struct btrfs_fs_info *fs_info = btrfs_sb(inode->vfs_inode.i_sb);
+	struct btrfs_fs_info *fs_info = trans->fs_info;
 	struct btrfs_root *root = inode->root;
 	struct extent_buffer *leaf;
 	struct btrfs_file_extent_item *fi;

commit e4af400a9c5081e7d2b703e4c479cb9831cc1117
Author: Goldwyn Rodrigues <rgoldwyn@suse.com>
Date:   Sun Jun 17 12:39:47 2018 -0500

    btrfs: Use iocb to derive pos instead of passing a separate parameter
    
    struct kiocb carries the ki_pos, so there is no need to pass it as
    a separate function parameter.
    
    generic_file_direct_write() increments ki_pos, so we now assign pos
    after the function.
    
    Signed-off-by: Goldwyn Rodrigues <rgoldwyn@suse.com>
    Reviewed-by: Misono Tomohiro <misono.tomohiro@jp.fujitsu.com>
    [ rename to btrfs_buffered_write ]
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 4cd8af14f915..89c9404fee9a 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1568,10 +1568,11 @@ static noinline int check_can_nocow(struct btrfs_inode *inode, loff_t pos,
 	return ret;
 }
 
-static noinline ssize_t __btrfs_buffered_write(struct file *file,
-					       struct iov_iter *i,
-					       loff_t pos)
+static noinline ssize_t btrfs_buffered_write(struct kiocb *iocb,
+					       struct iov_iter *i)
 {
+	struct file *file = iocb->ki_filp;
+	loff_t pos = iocb->ki_pos;
 	struct inode *inode = file_inode(file);
 	struct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);
 	struct btrfs_root *root = BTRFS_I(inode)->root;
@@ -1803,7 +1804,7 @@ static ssize_t __btrfs_direct_write(struct kiocb *iocb, struct iov_iter *from)
 {
 	struct file *file = iocb->ki_filp;
 	struct inode *inode = file_inode(file);
-	loff_t pos = iocb->ki_pos;
+	loff_t pos;
 	ssize_t written;
 	ssize_t written_buffered;
 	loff_t endbyte;
@@ -1814,8 +1815,8 @@ static ssize_t __btrfs_direct_write(struct kiocb *iocb, struct iov_iter *from)
 	if (written < 0 || !iov_iter_count(from))
 		return written;
 
-	pos += written;
-	written_buffered = __btrfs_buffered_write(file, from, pos);
+	pos = iocb->ki_pos;
+	written_buffered = btrfs_buffered_write(iocb, from);
 	if (written_buffered < 0) {
 		err = written_buffered;
 		goto out;
@@ -1952,7 +1953,7 @@ static ssize_t btrfs_file_write_iter(struct kiocb *iocb,
 	if (iocb->ki_flags & IOCB_DIRECT) {
 		num_written = __btrfs_direct_write(iocb, from);
 	} else {
-		num_written = __btrfs_buffered_write(file, from, pos);
+		num_written = btrfs_buffered_write(iocb, from);
 		if (num_written > 0)
 			iocb->ki_pos = pos + num_written;
 		if (clean_page)

commit e41ca5897489b1c18af75ff0cc8f5c80260b3281
Author: Qu Wenruo <wqu@suse.com>
Date:   Wed Jun 6 15:41:49 2018 +0800

    btrfs: Get rid of the confusing btrfs_file_extent_inline_len
    
    We used to call btrfs_file_extent_inline_len() to get the uncompressed
    data size of an inlined extent.
    
    However this function is hiding evil, for compressed extent, it has no
    choice but to directly read out ram_bytes from btrfs_file_extent_item.
    While for uncompressed extent, it uses item size to calculate the real
    data size, and ignoring ram_bytes completely.
    
    In fact, for corrupted ram_bytes, due to above behavior kernel
    btrfs_print_leaf() can't even print correct ram_bytes to expose the bug.
    
    Since we have the tree-checker to verify all EXTENT_DATA, such mismatch
    can be detected pretty easily, thus we can trust ram_bytes without the
    evil btrfs_file_extent_inline_len().
    
    Signed-off-by: Qu Wenruo <wqu@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 975c590c50d8..4cd8af14f915 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -833,8 +833,7 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 				btrfs_file_extent_num_bytes(leaf, fi);
 		} else if (extent_type == BTRFS_FILE_EXTENT_INLINE) {
 			extent_end = key.offset +
-				btrfs_file_extent_inline_len(leaf,
-						     path->slots[0], fi);
+				btrfs_file_extent_ram_bytes(leaf, fi);
 		} else {
 			/* can't happen */
 			BUG();

commit ca5788aba3e8153da38cf99ca3ce2294f032fb51
Author: David Sterba <dsterba@suse.com>
Date:   Thu Jul 19 15:27:46 2018 +0200

    btrfs: remove remaing full_sync logic from btrfs_sync_file
    
    The logic to check if the inode is already in the log can now be
    simplified since we always wait for the ordered extents to complete
    before deciding whether the inode needs to be logged. The big comment
    about it can go away too.
    
    CC: Filipe Manana <fdmanana@suse.com>
    Suggested-by: Filipe Manana <fdmanana@suse.com>
    [ code and changelog copied from mail discussion ]
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index d5be80fb427c..975c590c50d8 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2042,7 +2042,6 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 	struct btrfs_trans_handle *trans;
 	struct btrfs_log_ctx ctx;
 	int ret = 0, err;
-	bool full_sync = false;
 	u64 len;
 
 	/*
@@ -2066,8 +2065,6 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 
 	inode_lock(inode);
 	atomic_inc(&root->log_batch);
-	full_sync = test_bit(BTRFS_INODE_NEEDS_FULL_SYNC,
-			     &BTRFS_I(inode)->runtime_flags);
 
 	/*
 	 * We have to do this here to avoid the priority inversion of waiting on
@@ -2080,41 +2077,9 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 	}
 	atomic_inc(&root->log_batch);
 
-	/*
-	 * If the last transaction that changed this file was before the current
-	 * transaction and we have the full sync flag set in our inode, we can
-	 * bail out now without any syncing.
-	 *
-	 * Note that we can't bail out if the full sync flag isn't set. This is
-	 * because when the full sync flag is set we start all ordered extents
-	 * and wait for them to fully complete - when they complete they update
-	 * the inode's last_trans field through:
-	 *
-	 *     btrfs_finish_ordered_io() ->
-	 *         btrfs_update_inode_fallback() ->
-	 *             btrfs_update_inode() ->
-	 *                 btrfs_set_inode_last_trans()
-	 *
-	 * So we are sure that last_trans is up to date and can do this check to
-	 * bail out safely. For the fast path, when the full sync flag is not
-	 * set in our inode, we can not do it because we start only our ordered
-	 * extents and don't wait for them to complete (that is when
-	 * btrfs_finish_ordered_io runs), so here at this point their last_trans
-	 * value might be less than or equals to fs_info->last_trans_committed,
-	 * and setting a speculative last_trans for an inode when a buffered
-	 * write is made (such as fs_info->generation + 1 for example) would not
-	 * be reliable since after setting the value and before fsync is called
-	 * any number of transactions can start and commit (transaction kthread
-	 * commits the current transaction periodically), and a transaction
-	 * commit does not start nor waits for ordered extents to complete.
-	 */
 	smp_mb();
 	if (btrfs_inode_in_log(BTRFS_I(inode), fs_info->generation) ||
-	    (full_sync && BTRFS_I(inode)->last_trans <=
-	     fs_info->last_trans_committed) ||
-	    (!btrfs_have_ordered_extents_in_range(inode, start, len) &&
-	     BTRFS_I(inode)->last_trans
-	     <= fs_info->last_trans_committed)) {
+	    BTRFS_I(inode)->last_trans <= fs_info->last_trans_committed) {
 		/*
 		 * We've had everything committed since the last time we were
 		 * modified so clear this flag in case it was set for whatever

commit b5e6c3e170b77025b5f6174258c7ad71eed2d4de
Author: Josef Bacik <jbacik@fb.com>
Date:   Wed May 23 11:58:33 2018 -0400

    btrfs: always wait on ordered extents at fsync time
    
    There's a priority inversion that exists currently with btrfs fsync.  In
    some cases we will collect outstanding ordered extents onto a list and
    only wait on them at the very last second.  However this "very last
    second" falls inside of a transaction handle, so if we are in a lower
    priority cgroup we can end up holding the transaction open for longer
    than needed, so if a high priority cgroup is also trying to fsync()
    it'll see latency.
    
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Reviewed-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 51e77d72068a..d5be80fb427c 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2068,53 +2068,12 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 	atomic_inc(&root->log_batch);
 	full_sync = test_bit(BTRFS_INODE_NEEDS_FULL_SYNC,
 			     &BTRFS_I(inode)->runtime_flags);
+
 	/*
-	 * We might have have had more pages made dirty after calling
-	 * start_ordered_ops and before acquiring the inode's i_mutex.
+	 * We have to do this here to avoid the priority inversion of waiting on
+	 * IO of a lower priority task while holding a transaciton open.
 	 */
-	if (full_sync) {
-		/*
-		 * For a full sync, we need to make sure any ordered operations
-		 * start and finish before we start logging the inode, so that
-		 * all extents are persisted and the respective file extent
-		 * items are in the fs/subvol btree.
-		 */
-		ret = btrfs_wait_ordered_range(inode, start, len);
-	} else {
-		/*
-		 * Start any new ordered operations before starting to log the
-		 * inode. We will wait for them to finish in btrfs_sync_log().
-		 *
-		 * Right before acquiring the inode's mutex, we might have new
-		 * writes dirtying pages, which won't immediately start the
-		 * respective ordered operations - that is done through the
-		 * fill_delalloc callbacks invoked from the writepage and
-		 * writepages address space operations. So make sure we start
-		 * all ordered operations before starting to log our inode. Not
-		 * doing this means that while logging the inode, writeback
-		 * could start and invoke writepage/writepages, which would call
-		 * the fill_delalloc callbacks (cow_file_range,
-		 * submit_compressed_extents). These callbacks add first an
-		 * extent map to the modified list of extents and then create
-		 * the respective ordered operation, which means in
-		 * tree-log.c:btrfs_log_inode() we might capture all existing
-		 * ordered operations (with btrfs_get_logged_extents()) before
-		 * the fill_delalloc callback adds its ordered operation, and by
-		 * the time we visit the modified list of extent maps (with
-		 * btrfs_log_changed_extents()), we see and process the extent
-		 * map they created. We then use the extent map to construct a
-		 * file extent item for logging without waiting for the
-		 * respective ordered operation to finish - this file extent
-		 * item points to a disk location that might not have yet been
-		 * written to, containing random data - so after a crash a log
-		 * replay will make our inode have file extent items that point
-		 * to disk locations containing invalid data, as we returned
-		 * success to userspace without waiting for the respective
-		 * ordered operation to finish, because it wasn't captured by
-		 * btrfs_get_logged_extents().
-		 */
-		ret = start_ordered_ops(inode, start, end);
-	}
+	ret = btrfs_wait_ordered_range(inode, start, len);
 	if (ret) {
 		inode_unlock(inode);
 		goto out;
@@ -2239,13 +2198,6 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 				goto out;
 			}
 		}
-		if (!full_sync) {
-			ret = btrfs_wait_ordered_range(inode, start, len);
-			if (ret) {
-				btrfs_end_transaction(trans);
-				goto out;
-			}
-		}
 		ret = btrfs_commit_transaction(trans);
 	} else {
 		ret = btrfs_end_transaction(trans);

commit 95582b00838837fc07e042979320caf917ce3fe6
Author: Deepa Dinamani <deepa.kernel@gmail.com>
Date:   Tue May 8 19:36:02 2018 -0700

    vfs: change inode times to use struct timespec64
    
    struct timespec is not y2038 safe. Transition vfs to use
    y2038 safe struct timespec64 instead.
    
    The change was made with the help of the following cocinelle
    script. This catches about 80% of the changes.
    All the header file and logic changes are included in the
    first 5 rules. The rest are trivial substitutions.
    I avoid changing any of the function signatures or any other
    filesystem specific data structures to keep the patch simple
    for review.
    
    The script can be a little shorter by combining different cases.
    But, this version was sufficient for my usecase.
    
    virtual patch
    
    @ depends on patch @
    identifier now;
    @@
    - struct timespec
    + struct timespec64
      current_time ( ... )
      {
    - struct timespec now = current_kernel_time();
    + struct timespec64 now = current_kernel_time64();
      ...
    - return timespec_trunc(
    + return timespec64_trunc(
      ... );
      }
    
    @ depends on patch @
    identifier xtime;
    @@
     struct \( iattr \| inode \| kstat \) {
     ...
    -       struct timespec xtime;
    +       struct timespec64 xtime;
     ...
     }
    
    @ depends on patch @
    identifier t;
    @@
     struct inode_operations {
     ...
    int (*update_time) (...,
    -       struct timespec t,
    +       struct timespec64 t,
    ...);
     ...
     }
    
    @ depends on patch @
    identifier t;
    identifier fn_update_time =~ "update_time$";
    @@
     fn_update_time (...,
    - struct timespec *t,
    + struct timespec64 *t,
     ...) { ... }
    
    @ depends on patch @
    identifier t;
    @@
    lease_get_mtime( ... ,
    - struct timespec *t
    + struct timespec64 *t
      ) { ... }
    
    @te depends on patch forall@
    identifier ts;
    local idexpression struct inode *inode_node;
    identifier i_xtime =~ "^i_[acm]time$";
    identifier ia_xtime =~ "^ia_[acm]time$";
    identifier fn_update_time =~ "update_time$";
    identifier fn;
    expression e, E3;
    local idexpression struct inode *node1;
    local idexpression struct inode *node2;
    local idexpression struct iattr *attr1;
    local idexpression struct iattr *attr2;
    local idexpression struct iattr attr;
    identifier i_xtime1 =~ "^i_[acm]time$";
    identifier i_xtime2 =~ "^i_[acm]time$";
    identifier ia_xtime1 =~ "^ia_[acm]time$";
    identifier ia_xtime2 =~ "^ia_[acm]time$";
    @@
    (
    (
    - struct timespec ts;
    + struct timespec64 ts;
    |
    - struct timespec ts = current_time(inode_node);
    + struct timespec64 ts = current_time(inode_node);
    )
    
    <+... when != ts
    (
    - timespec_equal(&inode_node->i_xtime, &ts)
    + timespec64_equal(&inode_node->i_xtime, &ts)
    |
    - timespec_equal(&ts, &inode_node->i_xtime)
    + timespec64_equal(&ts, &inode_node->i_xtime)
    |
    - timespec_compare(&inode_node->i_xtime, &ts)
    + timespec64_compare(&inode_node->i_xtime, &ts)
    |
    - timespec_compare(&ts, &inode_node->i_xtime)
    + timespec64_compare(&ts, &inode_node->i_xtime)
    |
    ts = current_time(e)
    |
    fn_update_time(..., &ts,...)
    |
    inode_node->i_xtime = ts
    |
    node1->i_xtime = ts
    |
    ts = inode_node->i_xtime
    |
    <+... attr1->ia_xtime ...+> = ts
    |
    ts = attr1->ia_xtime
    |
    ts.tv_sec
    |
    ts.tv_nsec
    |
    btrfs_set_stack_timespec_sec(..., ts.tv_sec)
    |
    btrfs_set_stack_timespec_nsec(..., ts.tv_nsec)
    |
    - ts = timespec64_to_timespec(
    + ts =
    ...
    -)
    |
    - ts = ktime_to_timespec(
    + ts = ktime_to_timespec64(
    ...)
    |
    - ts = E3
    + ts = timespec_to_timespec64(E3)
    |
    - ktime_get_real_ts(&ts)
    + ktime_get_real_ts64(&ts)
    |
    fn(...,
    - ts
    + timespec64_to_timespec(ts)
    ,...)
    )
    ...+>
    (
    <... when != ts
    - return ts;
    + return timespec64_to_timespec(ts);
    ...>
    )
    |
    - timespec_equal(&node1->i_xtime1, &node2->i_xtime2)
    + timespec64_equal(&node1->i_xtime2, &node2->i_xtime2)
    |
    - timespec_equal(&node1->i_xtime1, &attr2->ia_xtime2)
    + timespec64_equal(&node1->i_xtime2, &attr2->ia_xtime2)
    |
    - timespec_compare(&node1->i_xtime1, &node2->i_xtime2)
    + timespec64_compare(&node1->i_xtime1, &node2->i_xtime2)
    |
    node1->i_xtime1 =
    - timespec_trunc(attr1->ia_xtime1,
    + timespec64_trunc(attr1->ia_xtime1,
    ...)
    |
    - attr1->ia_xtime1 = timespec_trunc(attr2->ia_xtime2,
    + attr1->ia_xtime1 =  timespec64_trunc(attr2->ia_xtime2,
    ...)
    |
    - ktime_get_real_ts(&attr1->ia_xtime1)
    + ktime_get_real_ts64(&attr1->ia_xtime1)
    |
    - ktime_get_real_ts(&attr.ia_xtime1)
    + ktime_get_real_ts64(&attr.ia_xtime1)
    )
    
    @ depends on patch @
    struct inode *node;
    struct iattr *attr;
    identifier fn;
    identifier i_xtime =~ "^i_[acm]time$";
    identifier ia_xtime =~ "^ia_[acm]time$";
    expression e;
    @@
    (
    - fn(node->i_xtime);
    + fn(timespec64_to_timespec(node->i_xtime));
    |
     fn(...,
    - node->i_xtime);
    + timespec64_to_timespec(node->i_xtime));
    |
    - e = fn(attr->ia_xtime);
    + e = fn(timespec64_to_timespec(attr->ia_xtime));
    )
    
    @ depends on patch forall @
    struct inode *node;
    struct iattr *attr;
    identifier i_xtime =~ "^i_[acm]time$";
    identifier ia_xtime =~ "^ia_[acm]time$";
    identifier fn;
    @@
    {
    + struct timespec ts;
    <+...
    (
    + ts = timespec64_to_timespec(node->i_xtime);
    fn (...,
    - &node->i_xtime,
    + &ts,
    ...);
    |
    + ts = timespec64_to_timespec(attr->ia_xtime);
    fn (...,
    - &attr->ia_xtime,
    + &ts,
    ...);
    )
    ...+>
    }
    
    @ depends on patch forall @
    struct inode *node;
    struct iattr *attr;
    struct kstat *stat;
    identifier ia_xtime =~ "^ia_[acm]time$";
    identifier i_xtime =~ "^i_[acm]time$";
    identifier xtime =~ "^[acm]time$";
    identifier fn, ret;
    @@
    {
    + struct timespec ts;
    <+...
    (
    + ts = timespec64_to_timespec(node->i_xtime);
    ret = fn (...,
    - &node->i_xtime,
    + &ts,
    ...);
    |
    + ts = timespec64_to_timespec(node->i_xtime);
    ret = fn (...,
    - &node->i_xtime);
    + &ts);
    |
    + ts = timespec64_to_timespec(attr->ia_xtime);
    ret = fn (...,
    - &attr->ia_xtime,
    + &ts,
    ...);
    |
    + ts = timespec64_to_timespec(attr->ia_xtime);
    ret = fn (...,
    - &attr->ia_xtime);
    + &ts);
    |
    + ts = timespec64_to_timespec(stat->xtime);
    ret = fn (...,
    - &stat->xtime);
    + &ts);
    )
    ...+>
    }
    
    @ depends on patch @
    struct inode *node;
    struct inode *node2;
    identifier i_xtime1 =~ "^i_[acm]time$";
    identifier i_xtime2 =~ "^i_[acm]time$";
    identifier i_xtime3 =~ "^i_[acm]time$";
    struct iattr *attrp;
    struct iattr *attrp2;
    struct iattr attr ;
    identifier ia_xtime1 =~ "^ia_[acm]time$";
    identifier ia_xtime2 =~ "^ia_[acm]time$";
    struct kstat *stat;
    struct kstat stat1;
    struct timespec64 ts;
    identifier xtime =~ "^[acmb]time$";
    expression e;
    @@
    (
    ( node->i_xtime2 \| attrp->ia_xtime2 \| attr.ia_xtime2 \) = node->i_xtime1  ;
    |
     node->i_xtime2 = \( node2->i_xtime1 \| timespec64_trunc(...) \);
    |
     node->i_xtime2 = node->i_xtime1 = node->i_xtime3 = \(ts \| current_time(...) \);
    |
     node->i_xtime1 = node->i_xtime3 = \(ts \| current_time(...) \);
    |
     stat->xtime = node2->i_xtime1;
    |
     stat1.xtime = node2->i_xtime1;
    |
    ( node->i_xtime2 \| attrp->ia_xtime2 \) = attrp->ia_xtime1  ;
    |
    ( attrp->ia_xtime1 \| attr.ia_xtime1 \) = attrp2->ia_xtime2;
    |
    - e = node->i_xtime1;
    + e = timespec64_to_timespec( node->i_xtime1 );
    |
    - e = attrp->ia_xtime1;
    + e = timespec64_to_timespec( attrp->ia_xtime1 );
    |
    node->i_xtime1 = current_time(...);
    |
     node->i_xtime2 = node->i_xtime1 = node->i_xtime3 =
    - e;
    + timespec_to_timespec64(e);
    |
     node->i_xtime1 = node->i_xtime3 =
    - e;
    + timespec_to_timespec64(e);
    |
    - node->i_xtime1 = e;
    + node->i_xtime1 = timespec_to_timespec64(e);
    )
    
    Signed-off-by: Deepa Dinamani <deepa.kernel@gmail.com>
    Cc: <anton@tuxera.com>
    Cc: <balbi@kernel.org>
    Cc: <bfields@fieldses.org>
    Cc: <darrick.wong@oracle.com>
    Cc: <dhowells@redhat.com>
    Cc: <dsterba@suse.com>
    Cc: <dwmw2@infradead.org>
    Cc: <hch@lst.de>
    Cc: <hirofumi@mail.parknet.co.jp>
    Cc: <hubcap@omnibond.com>
    Cc: <jack@suse.com>
    Cc: <jaegeuk@kernel.org>
    Cc: <jaharkes@cs.cmu.edu>
    Cc: <jslaby@suse.com>
    Cc: <keescook@chromium.org>
    Cc: <mark@fasheh.com>
    Cc: <miklos@szeredi.hu>
    Cc: <nico@linaro.org>
    Cc: <reiserfs-devel@vger.kernel.org>
    Cc: <richard@nod.at>
    Cc: <sage@redhat.com>
    Cc: <sfrench@samba.org>
    Cc: <swhiteho@redhat.com>
    Cc: <tj@kernel.org>
    Cc: <trond.myklebust@primarydata.com>
    Cc: <tytso@mit.edu>
    Cc: <viro@zeniv.linux.org.uk>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index f660ba1e5e58..51e77d72068a 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1842,16 +1842,16 @@ static ssize_t __btrfs_direct_write(struct kiocb *iocb, struct iov_iter *from)
 
 static void update_time_for_write(struct inode *inode)
 {
-	struct timespec now;
+	struct timespec64 now;
 
 	if (IS_NOCMTIME(inode))
 		return;
 
 	now = current_time(inode);
-	if (!timespec_equal(&inode->i_mtime, &now))
+	if (!timespec64_equal(&inode->i_mtime, &now))
 		inode->i_mtime = now;
 
-	if (!timespec_equal(&inode->i_ctime, &now))
+	if (!timespec64_equal(&inode->i_ctime, &now))
 		inode->i_ctime = now;
 
 	if (IS_I_VERSION(inode))

commit 336a8bb8e36a273a802a54b2e673c777c9c62fb1
Author: Qu Wenruo <wqu@suse.com>
Date:   Tue Apr 17 18:43:58 2018 +0800

    btrfs: Fix wrong btrfs_delalloc_release_extents parameter
    
    Commit 43b18595d660 ("btrfs: qgroup: Use separate meta reservation type
    for delalloc") merged into mainline is not the latest version submitted
    to mail list in Dec 2017.
    
    It has a fatal wrong @qgroup_free parameter, which results increasing
    qgroup metadata pertrans reserved space, and causing a lot of early EDQUOT.
    
    Fix it by applying the correct diff on top of current branch.
    
    Fixes: 43b18595d660 ("btrfs: qgroup: Use separate meta reservation type for delalloc")
    Signed-off-by: Qu Wenruo <wqu@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 0167a9c97c9c..f660ba1e5e58 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1748,7 +1748,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 			unlock_extent_cached(&BTRFS_I(inode)->io_tree,
 					     lockstart, lockend, &cached_state);
 		btrfs_delalloc_release_extents(BTRFS_I(inode), reserve_bytes,
-					       (ret != 0));
+					       true);
 		if (ret) {
 			btrfs_drop_pages(pages, num_pages);
 			break;

commit c1d7c514f745628eb096c5cbb10737855879ae25
Author: David Sterba <dsterba@suse.com>
Date:   Tue Apr 3 19:23:33 2018 +0200

    btrfs: replace GPL boilerplate by SPDX -- sources
    
    Remove GPL boilerplate text (long, short, one-line) and keep the rest,
    ie. personal, company or original source copyright statements. Add the
    SPDX header.
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index f247300170e5..0167a9c97c9c 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1,19 +1,6 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * Copyright (C) 2007 Oracle.  All rights reserved.
- *
- * This program is free software; you can redistribute it and/or
- * modify it under the terms of the GNU General Public
- * License v2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * General Public License for more details.
- *
- * You should have received a copy of the GNU General Public
- * License along with this program; if not, write to the
- * Free Software Foundation, Inc., 59 Temple Place - Suite 330,
- * Boston, MA 021110-1307, USA.
  */
 
 #include <linux/fs.h>

commit 43b18595d6603cb4197fb9b063915cd7802141a6
Author: Qu Wenruo <wqu@suse.com>
Date:   Tue Dec 12 15:34:32 2017 +0800

    btrfs: qgroup: Use separate meta reservation type for delalloc
    
    Before this patch, btrfs qgroup is mixing per-transcation meta rsv with
    preallocated meta rsv, making it quite easy to underflow qgroup meta
    reservation.
    
    Since we have the new qgroup meta rsv types, apply it to delalloc
    reservation.
    
    Now for delalloc, most of its reserved space will use META_PREALLOC qgroup
    rsv type.
    
    And for callers reducing outstanding extent like btrfs_finish_ordered_io(),
    they will convert corresponding META_PREALLOC reservation to
    META_PERTRANS.
    
    This is mainly due to the fact that current qgroup numbers will only be
    updated in btrfs_commit_transaction(), that's to say if we don't keep
    such placeholder reservation, we can exceed qgroup limitation.
    
    And for callers freeing outstanding extent in error handler, we will
    just free META_PREALLOC bytes.
    
    This behavior makes callers of btrfs_qgroup_release_meta() or
    btrfs_qgroup_convert_meta() to be aware of which type they are.
    So in this patch, btrfs_delalloc_release_metadata() and its callers get
    an extra parameter to info qgroup to do correct meta convert/release.
    
    The good news is, even we use the wrong type (convert or free), it won't
    cause obvious bug, as prealloc type is always in good shape, and the
    type only affects how per-trans meta is increased or not.
    
    So the worst case will be at most metadata limitation can be sometimes
    exceeded (no convert at all) or metadata limitation is reached too soon
    (no free at all).
    
    Signed-off-by: Qu Wenruo <wqu@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 6d878f1d1082..f247300170e5 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1691,7 +1691,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 				    force_page_uptodate);
 		if (ret) {
 			btrfs_delalloc_release_extents(BTRFS_I(inode),
-						       reserve_bytes);
+						       reserve_bytes, true);
 			break;
 		}
 
@@ -1703,7 +1703,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 			if (extents_locked == -EAGAIN)
 				goto again;
 			btrfs_delalloc_release_extents(BTRFS_I(inode),
-						       reserve_bytes);
+						       reserve_bytes, true);
 			ret = extents_locked;
 			break;
 		}
@@ -1738,7 +1738,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 						fs_info->sb->s_blocksize_bits;
 			if (only_release_metadata) {
 				btrfs_delalloc_release_metadata(BTRFS_I(inode),
-								release_bytes);
+							release_bytes, true);
 			} else {
 				u64 __pos;
 
@@ -1747,7 +1747,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 					(dirty_pages << PAGE_SHIFT);
 				btrfs_delalloc_release_space(inode,
 						data_reserved, __pos,
-						release_bytes);
+						release_bytes, true);
 			}
 		}
 
@@ -1760,7 +1760,8 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 		if (extents_locked)
 			unlock_extent_cached(&BTRFS_I(inode)->io_tree,
 					     lockstart, lockend, &cached_state);
-		btrfs_delalloc_release_extents(BTRFS_I(inode), reserve_bytes);
+		btrfs_delalloc_release_extents(BTRFS_I(inode), reserve_bytes,
+					       (ret != 0));
 		if (ret) {
 			btrfs_drop_pages(pages, num_pages);
 			break;
@@ -1800,11 +1801,11 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 		if (only_release_metadata) {
 			btrfs_end_write_no_snapshotting(root);
 			btrfs_delalloc_release_metadata(BTRFS_I(inode),
-					release_bytes);
+					release_bytes, true);
 		} else {
 			btrfs_delalloc_release_space(inode, data_reserved,
 					round_down(pos, fs_info->sectorsize),
-					release_bytes);
+					release_bytes, true);
 		}
 	}
 

commit 7a5a07a81062915c65ce27e80608b1c819b1f936
Author: Nikolay Borisov <nborisov@suse.com>
Date:   Mon Feb 5 10:41:13 2018 +0200

    btrfs: Remove userspace transaction ioctls
    
    Commit 3558d4f88ec8 ("btrfs: Deprecate userspace transaction ioctls")
    marked the beginning of the end of userspace transaction. This commit
    finishes the job! There are no known users and ceph does not use the
    ioctl anymore.
    
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Acked-by: Sage Weil <sage@redhat.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 8cac40005e6c..6d878f1d1082 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1997,8 +1997,6 @@ int btrfs_release_file(struct inode *inode, struct file *filp)
 {
 	struct btrfs_file_private *private = filp->private_data;
 
-	if (private && private->trans)
-		btrfs_ioctl_trans_end(filp);
 	if (private && private->filldir_buf)
 		kfree(private->filldir_buf);
 	kfree(private);
@@ -2189,12 +2187,6 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 		goto out;
 	}
 
-	/*
-	 * ok we haven't committed the transaction yet, lets do a commit
-	 */
-	if (file->private_data)
-		btrfs_ioctl_trans_end(file);
-
 	/*
 	 * We use start here because we will need to wait on the IO to complete
 	 * in btrfs_sync_log, which could require joining a transaction (for

commit 051c98eb11e6fd64a8306851c34ee485b5817955
Author: David Sterba <dsterba@suse.com>
Date:   Wed Mar 7 15:33:22 2018 +0100

    btrfs: open code trivial helper btrfs_page_exists_in_range
    
    The called function name is self explanatory.
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 8f425c64d75f..8cac40005e6c 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2482,7 +2482,8 @@ static int btrfs_punch_hole_lock_range(struct inode *inode,
 		if ((!ordered ||
 		    (ordered->file_offset + ordered->len <= lockstart ||
 		     ordered->file_offset > lockend)) &&
-		     !btrfs_page_exists_in_range(inode, lockstart, lockend)) {
+		     !filemap_range_has_page(inode->i_mapping,
+					     lockstart, lockend)) {
 			if (ordered)
 				btrfs_put_ordered_extent(ordered);
 			break;

commit e5b84f7a258a8344947e8b955b8c2648008d8ccc
Author: Nikolay Borisov <nborisov@suse.com>
Date:   Tue Feb 27 17:37:18 2018 +0200

    btrfs: Remove root argument from btrfs_log_dentry_safe
    
    Now that nothing uses the root arg of btrfs_log_dentry_safe it can be
    safely removed. No functional changes.
    
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index a335e2e6c84d..8f425c64d75f 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2214,7 +2214,7 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 	}
 	trans->sync = true;
 
-	ret = btrfs_log_dentry_safe(trans, root, dentry, start, end, &ctx);
+	ret = btrfs_log_dentry_safe(trans, dentry, start, end, &ctx);
 	if (ret < 0) {
 		/* Fallthrough and commit/free transaction. */
 		ret = 1;

commit e67c718b5b9a306bde7e966be7b4ca48fa063d73
Author: David Sterba <dsterba@suse.com>
Date:   Mon Feb 19 17:24:18 2018 +0100

    btrfs: add more __cold annotations
    
    The __cold functions are placed to a special section, as they're
    expected to be called rarely. This could help i-cache prefetches or help
    compiler to decide which branches are more/less likely to be taken
    without any other annotations needed.
    
    Though we can't add more __exit annotations, it's still possible to add
    __cold (that's also added with __exit). That way the following function
    categories are tagged:
    
    - printf wrappers, error messages
    - exit helpers
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 41ab9073d1d4..a335e2e6c84d 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -3378,7 +3378,7 @@ const struct file_operations btrfs_file_operations = {
 	.dedupe_file_range = btrfs_dedupe_file_range,
 };
 
-void btrfs_auto_defrag_exit(void)
+void __cold btrfs_auto_defrag_exit(void)
 {
 	kmem_cache_destroy(btrfs_inode_defrag_cachep);
 }

commit 31466f3ed710e5761077190809e694f55aed5deb
Merge: 6787dc24b72b 3acbcbfc8f06
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jan 29 14:04:23 2018 -0800

    Merge tag 'for-4.16-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/kdave/linux
    
    Pull btrfs updates from David Sterba:
     "Features or user visible changes:
    
       - fallocate: implement zero range mode
    
       - avoid losing data raid profile when deleting a device
    
       - tree item checker: more checks for directory items and xattrs
    
      Notable fixes:
    
       - raid56 recovery: don't use cached stripes, that could be
         potentially changed and a later RMW or recovery would lead to
         corruptions or failures
    
       - let raid56 try harder to rebuild damaged data, reading from all
         stripes if necessary
    
       - fix scrub to repair raid56 in a similar way as in the case above
    
      Other:
    
       - cleanups: device freeing, removed some call indirections, redundant
         bio_put/_get, unused parameters, refactorings and renames
    
       - RCU list traversal fixups
    
       - simplify mount callchain, remove recursing back when mounting a
         subvolume
    
       - plug for fsync, may improve bio merging on multiple devices
    
       - compression heurisic: replace heap sort with radix sort, gains some
         performance
    
       - add extent map selftests, buffered write vs dio"
    
    * tag 'for-4.16-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/kdave/linux: (155 commits)
      btrfs: drop devid as device_list_add() arg
      btrfs: get device pointer from device_list_add()
      btrfs: set the total_devices in device_list_add()
      btrfs: move pr_info into device_list_add
      btrfs: make btrfs_free_stale_devices() to match the path
      btrfs: rename btrfs_free_stale_devices() arg to skip_dev
      btrfs: make btrfs_free_stale_devices() argument optional
      btrfs: make btrfs_free_stale_device() to iterate all stales
      btrfs: no need to check for btrfs_fs_devices::seeding
      btrfs: Use IS_ALIGNED in btrfs_truncate_block instead of opencoding it
      Btrfs: noinline merge_extent_mapping
      Btrfs: add WARN_ONCE to detect unexpected error from merge_extent_mapping
      Btrfs: extent map selftest: dio write vs dio read
      Btrfs: extent map selftest: buffered write vs dio read
      Btrfs: add extent map selftests
      Btrfs: move extent map specific code to extent_map.c
      Btrfs: add helper for em merge logic
      Btrfs: fix unexpected EEXIST from btrfs_get_extent
      Btrfs: fix incorrect block_len in merge_extent_mapping
      btrfs: Remove unused readahead spinlock
      ...

commit ae5e165d855dd978a461b22175531b07f54fb61f
Author: Jeff Layton <jlayton@redhat.com>
Date:   Mon Jan 29 06:41:30 2018 -0500

    fs: new API for handling inode->i_version
    
    Add a documentation blob that explains what the i_version field is, how
    it is expected to work, and how it is currently implemented by various
    filesystems.
    
    We already have inode_inc_iversion. Add several other functions for
    manipulating and accessing the i_version counter. For now, the
    implementation is trivial and basically works the way that all of the
    open-coded i_version accesses work today.
    
    Future patches will convert existing users of i_version to use the new
    API, and then convert the backend implementation to do things more
    efficiently.
    
    Signed-off-by: Jeff Layton <jlayton@redhat.com>
    Reviewed-by: Jan Kara <jack@suse.cz>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index eb1bac7c8553..c95d7b2efefb 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -31,6 +31,7 @@
 #include <linux/slab.h>
 #include <linux/btrfs.h>
 #include <linux/uio.h>
+#include <linux/iversion.h>
 #include "ctree.h"
 #include "disk-io.h"
 #include "transaction.h"

commit 81fdf6382b3b92f6fc5f34f9c8cd9074b25f6c0e
Author: Filipe Manana <fdmanana@suse.com>
Date:   Thu Jan 18 11:34:31 2018 +0000

    Btrfs: fix space leak after fallocate and zero range operations
    
    If we do a buffered write after a zero range operation that has an
    unaligned (with the filesystem's sector size) end which also falls within
    an unwritten (prealloc) extent that is currently beyond the inode's
    i_size, and the zero range operation has the flag FALLOC_FL_KEEP_SIZE,
    we end up leaking data and metadata space. This happens because when
    zeroing a range we call btrfs_truncate_block(), which does delalloc
    (loads the page and partially zeroes its content), and in the buffered
    write path we only clear existing delalloc space reservation for the
    range we are writing into if that range starts at an offset smaller then
    the inode's i_size, which makes sense since we can not have delalloc
    extents beyond the i_size, only unwritten extents are allowed.
    
    Example reproducer:
    
     $ mkfs.btrfs -f /dev/sdb
     $ mount /dev/sdb /mnt
     $ xfs_io -f -c "falloc -k 428K 4K" /mnt/foobar
     $ xfs_io -c "fzero -k 0 430K" /mnt/foobar
     $ xfs_io -c "pwrite -S 0xaa 428K 4K" /mnt/foobar
     $ umount /mnt
    
    After the unmount we get the metadata and data space leaks reported in
    dmesg/syslog:
    
     [95794.602253] ------------[ cut here ]------------
     [95794.603322] WARNING: CPU: 0 PID: 31496 at fs/btrfs/inode.c:9561 btrfs_destroy_inode+0x4e/0x206 [btrfs]
     [95794.605167] Modules linked in: btrfs xfs ppdev ghash_clmulni_intel pcbc aesni_intel aes_x86_64 crypto_simd cryptd glue_helper parport_pc psmouse sg i2c_piix4 parport i2c_core evdev pcspkr button serio_raw sunrpc loop autofs4 ext4 crc16 mbcache jbd2 zstd_decompress zstd_compress xxhash raid10 raid456 async_raid6_recov async_memcpy async_pq async_xor async_tx xor raid6_pq libcrc32c crc32c_generic raid1 raid0 multipath linear md_mod sd_mod virtio_scsi ata_generic crc32c_intel ata_piix floppy virtio_pci virtio_ring virtio libata scsi_mod e1000 [last unloaded: btrfs]
     [95794.613000] CPU: 0 PID: 31496 Comm: umount Tainted: G        W       4.14.0-rc6-btrfs-next-54+ #1
     [95794.614448] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS rel-1.10.2-0-g5f4c7b1-prebuilt.qemu-project.org 04/01/2014
     [95794.615972] task: ffff880075aa0240 task.stack: ffffc90001734000
     [95794.617114] RIP: 0010:btrfs_destroy_inode+0x4e/0x206 [btrfs]
     [95794.618001] RSP: 0018:ffffc90001737d00 EFLAGS: 00010202
     [95794.618721] RAX: 0000000000000000 RBX: ffff880070fa1418 RCX: ffffc90001737c7c
     [95794.619645] RDX: 0000000175aa0240 RSI: 0000000000000001 RDI: ffff880070fa1418
     [95794.620711] RBP: ffffc90001737d38 R08: 0000000000000000 R09: 0000000000000000
     [95794.621932] R10: ffffc90001737c48 R11: ffff88007123e158 R12: ffff880075b6a000
     [95794.623124] R13: ffff88006145c000 R14: ffff880070fa1418 R15: ffff880070c3b4a0
     [95794.624188] FS:  00007fa6793c92c0(0000) GS:ffff88023fc00000(0000) knlGS:0000000000000000
     [95794.625578] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
     [95794.626522] CR2: 000056338670d048 CR3: 00000000610dc005 CR4: 00000000001606f0
     [95794.627647] Call Trace:
     [95794.628128]  destroy_inode+0x3d/0x55
     [95794.628573]  evict+0x177/0x17e
     [95794.629010]  dispose_list+0x50/0x71
     [95794.629478]  evict_inodes+0x132/0x141
     [95794.630289]  generic_shutdown_super+0x3f/0x10b
     [95794.630864]  kill_anon_super+0x12/0x1c
     [95794.631383]  btrfs_kill_super+0x16/0x21 [btrfs]
     [95794.631930]  deactivate_locked_super+0x30/0x68
     [95794.632539]  deactivate_super+0x36/0x39
     [95794.633200]  cleanup_mnt+0x49/0x67
     [95794.633818]  __cleanup_mnt+0x12/0x14
     [95794.634416]  task_work_run+0x82/0xa6
     [95794.634902]  prepare_exit_to_usermode+0xe1/0x10c
     [95794.635525]  syscall_return_slowpath+0x18c/0x1af
     [95794.636122]  entry_SYSCALL_64_fastpath+0xab/0xad
     [95794.636834] RIP: 0033:0x7fa678cb99a7
     [95794.637370] RSP: 002b:00007ffccf0aaed8 EFLAGS: 00000246 ORIG_RAX: 00000000000000a6
     [95794.638672] RAX: 0000000000000000 RBX: 0000563386706030 RCX: 00007fa678cb99a7
     [95794.639596] RDX: 0000000000000001 RSI: 0000000000000000 RDI: 000056338670ca90
     [95794.640703] RBP: 000056338670ca90 R08: 000056338670c740 R09: 0000000000000015
     [95794.641773] R10: 00000000000006b4 R11: 0000000000000246 R12: 00007fa6791bae64
     [95794.643150] R13: 0000000000000000 R14: 0000563386706210 R15: 00007ffccf0ab160
     [95794.644249] Code: ff 4c 8b a8 80 06 00 00 48 8b 87 c0 01 00 00 48 85 c0 74 02 0f ff 48 83 bb e0 02 00 00 00 74 02 0f ff 83 bb 3c ff ff ff 00 74 02 <0f> ff 83 bb 40 ff ff ff 00 74 02 0f ff 48 83 bb f8 fe ff ff 00
     [95794.646929] ---[ end trace e95877675c6ec007 ]---
     [95794.647751] ------------[ cut here ]------------
     [95794.648509] WARNING: CPU: 0 PID: 31496 at fs/btrfs/inode.c:9562 btrfs_destroy_inode+0x59/0x206 [btrfs]
     [95794.649842] Modules linked in: btrfs xfs ppdev ghash_clmulni_intel pcbc aesni_intel aes_x86_64 crypto_simd cryptd glue_helper parport_pc psmouse sg i2c_piix4 parport i2c_core evdev pcspkr button serio_raw sunrpc loop autofs4 ext4 crc16 mbcache jbd2 zstd_decompress zstd_compress xxhash raid10 raid456 async_raid6_recov async_memcpy async_pq async_xor async_tx xor raid6_pq libcrc32c crc32c_generic raid1 raid0 multipath linear md_mod sd_mod virtio_scsi ata_generic crc32c_intel ata_piix floppy virtio_pci virtio_ring virtio libata scsi_mod e1000 [last unloaded: btrfs]
     [95794.654659] CPU: 0 PID: 31496 Comm: umount Tainted: G        W       4.14.0-rc6-btrfs-next-54+ #1
     [95794.655894] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS rel-1.10.2-0-g5f4c7b1-prebuilt.qemu-project.org 04/01/2014
     [95794.657546] task: ffff880075aa0240 task.stack: ffffc90001734000
     [95794.658433] RIP: 0010:btrfs_destroy_inode+0x59/0x206 [btrfs]
     [95794.659279] RSP: 0018:ffffc90001737d00 EFLAGS: 00010202
     [95794.660054] RAX: 0000000000000000 RBX: ffff880070fa1418 RCX: ffffc90001737c7c
     [95794.660753] RDX: 0000000175aa0240 RSI: 0000000000000001 RDI: ffff880070fa1418
     [95794.661513] RBP: ffffc90001737d38 R08: 0000000000000000 R09: 0000000000000000
     [95794.662289] R10: ffffc90001737c48 R11: ffff88007123e158 R12: ffff880075b6a000
     [95794.663393] R13: ffff88006145c000 R14: ffff880070fa1418 R15: ffff880070c3b4a0
     [95794.664342] FS:  00007fa6793c92c0(0000) GS:ffff88023fc00000(0000) knlGS:0000000000000000
     [95794.665673] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
     [95794.666593] CR2: 000056338670d048 CR3: 00000000610dc005 CR4: 00000000001606f0
     [95794.667629] Call Trace:
     [95794.668065]  destroy_inode+0x3d/0x55
     [95794.668637]  evict+0x177/0x17e
     [95794.669179]  dispose_list+0x50/0x71
     [95794.669830]  evict_inodes+0x132/0x141
     [95794.670416]  generic_shutdown_super+0x3f/0x10b
     [95794.671103]  kill_anon_super+0x12/0x1c
     [95794.671786]  btrfs_kill_super+0x16/0x21 [btrfs]
     [95794.672552]  deactivate_locked_super+0x30/0x68
     [95794.673393]  deactivate_super+0x36/0x39
     [95794.674107]  cleanup_mnt+0x49/0x67
     [95794.674706]  __cleanup_mnt+0x12/0x14
     [95794.675279]  task_work_run+0x82/0xa6
     [95794.675795]  prepare_exit_to_usermode+0xe1/0x10c
     [95794.676507]  syscall_return_slowpath+0x18c/0x1af
     [95794.677275]  entry_SYSCALL_64_fastpath+0xab/0xad
     [95794.678006] RIP: 0033:0x7fa678cb99a7
     [95794.678600] RSP: 002b:00007ffccf0aaed8 EFLAGS: 00000246 ORIG_RAX: 00000000000000a6
     [95794.679739] RAX: 0000000000000000 RBX: 0000563386706030 RCX: 00007fa678cb99a7
     [95794.680779] RDX: 0000000000000001 RSI: 0000000000000000 RDI: 000056338670ca90
     [95794.681837] RBP: 000056338670ca90 R08: 000056338670c740 R09: 0000000000000015
     [95794.682867] R10: 00000000000006b4 R11: 0000000000000246 R12: 00007fa6791bae64
     [95794.683891] R13: 0000000000000000 R14: 0000563386706210 R15: 00007ffccf0ab160
     [95794.684843] Code: c0 01 00 00 48 85 c0 74 02 0f ff 48 83 bb e0 02 00 00 00 74 02 0f ff 83 bb 3c ff ff ff 00 74 02 0f ff 83 bb 40 ff ff ff 00 74 02 <0f> ff 48 83 bb f8 fe ff ff 00 74 02 0f ff 48 83 bb 00 ff ff ff
     [95794.687156] ---[ end trace e95877675c6ec008 ]---
     [95794.687876] ------------[ cut here ]------------
     [95794.688579] WARNING: CPU: 0 PID: 31496 at fs/btrfs/inode.c:9565 btrfs_destroy_inode+0x7d/0x206 [btrfs]
     [95794.689735] Modules linked in: btrfs xfs ppdev ghash_clmulni_intel pcbc aesni_intel aes_x86_64 crypto_simd cryptd glue_helper parport_pc psmouse sg i2c_piix4 parport i2c_core evdev pcspkr button serio_raw sunrpc loop autofs4 ext4 crc16 mbcache jbd2 zstd_decompress zstd_compress xxhash raid10 raid456 async_raid6_recov async_memcpy async_pq async_xor async_tx xor raid6_pq libcrc32c crc32c_generic raid1 raid0 multipath linear md_mod sd_mod virtio_scsi ata_generic crc32c_intel ata_piix floppy virtio_pci virtio_ring virtio libata scsi_mod e1000 [last unloaded: btrfs]
     [95794.695015] CPU: 0 PID: 31496 Comm: umount Tainted: G        W       4.14.0-rc6-btrfs-next-54+ #1
     [95794.696396] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS rel-1.10.2-0-g5f4c7b1-prebuilt.qemu-project.org 04/01/2014
     [95794.697956] task: ffff880075aa0240 task.stack: ffffc90001734000
     [95794.698925] RIP: 0010:btrfs_destroy_inode+0x7d/0x206 [btrfs]
     [95794.699763] RSP: 0018:ffffc90001737d00 EFLAGS: 00010206
     [95794.700434] RAX: 0000000000000000 RBX: ffff880070fa1418 RCX: ffffc90001737c7c
     [95794.701445] RDX: 0000000175aa0240 RSI: 0000000000000001 RDI: ffff880070fa1418
     [95794.702448] RBP: ffffc90001737d38 R08: 0000000000000000 R09: 0000000000000000
     [95794.703557] R10: ffffc90001737c48 R11: ffff88007123e158 R12: ffff880075b6a000
     [95794.704441] R13: ffff88006145c000 R14: ffff880070fa1418 R15: ffff880070c3b4a0
     [95794.705270] FS:  00007fa6793c92c0(0000) GS:ffff88023fc00000(0000) knlGS:0000000000000000
     [95794.706341] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
     [95794.707001] CR2: 000056338670d048 CR3: 00000000610dc005 CR4: 00000000001606f0
     [95794.708030] Call Trace:
     [95794.708466]  destroy_inode+0x3d/0x55
     [95794.709071]  evict+0x177/0x17e
     [95794.709497]  dispose_list+0x50/0x71
     [95794.709973]  evict_inodes+0x132/0x141
     [95794.710564]  generic_shutdown_super+0x3f/0x10b
     [95794.711200]  kill_anon_super+0x12/0x1c
     [95794.711633]  btrfs_kill_super+0x16/0x21 [btrfs]
     [95794.712139]  deactivate_locked_super+0x30/0x68
     [95794.712608]  deactivate_super+0x36/0x39
     [95794.713093]  cleanup_mnt+0x49/0x67
     [95794.713514]  __cleanup_mnt+0x12/0x14
     [95794.713933]  task_work_run+0x82/0xa6
     [95794.714543]  prepare_exit_to_usermode+0xe1/0x10c
     [95794.715247]  syscall_return_slowpath+0x18c/0x1af
     [95794.715952]  entry_SYSCALL_64_fastpath+0xab/0xad
     [95794.716653] RIP: 0033:0x7fa678cb99a7
     [95794.721100] RSP: 002b:00007ffccf0aaed8 EFLAGS: 00000246 ORIG_RAX: 00000000000000a6
     [95794.722052] RAX: 0000000000000000 RBX: 0000563386706030 RCX: 00007fa678cb99a7
     [95794.722856] RDX: 0000000000000001 RSI: 0000000000000000 RDI: 000056338670ca90
     [95794.723698] RBP: 000056338670ca90 R08: 000056338670c740 R09: 0000000000000015
     [95794.724736] R10: 00000000000006b4 R11: 0000000000000246 R12: 00007fa6791bae64
     [95794.725928] R13: 0000000000000000 R14: 0000563386706210 R15: 00007ffccf0ab160
     [95794.726728] Code: 40 ff ff ff 00 74 02 0f ff 48 83 bb f8 fe ff ff 00 74 02 0f ff 48 83 bb 00 ff ff ff 00 74 02 0f ff 48 83 bb 30 ff ff ff 00 74 02 <0f> ff 48 83 bb 08 ff ff ff 00 74 02 0f ff 4d 85 e4 0f 84 52 01
     [95794.729203] ---[ end trace e95877675c6ec009 ]---
     [95794.841054] ------------[ cut here ]------------
     [95794.841829] WARNING: CPU: 0 PID: 31496 at fs/btrfs/extent-tree.c:5831 btrfs_free_block_groups+0x235/0x36a [btrfs]
     [95794.843425] Modules linked in: btrfs xfs ppdev ghash_clmulni_intel pcbc aesni_intel aes_x86_64 crypto_simd cryptd glue_helper parport_pc psmouse sg i2c_piix4 parport i2c_core evdev pcspkr button serio_raw sunrpc loop autofs4 ext4 crc16 mbcache jbd2 zstd_decompress zstd_compress xxhash raid10 raid456 async_raid6_recov async_memcpy async_pq async_xor async_tx xor raid6_pq libcrc32c crc32c_generic raid1 raid0 multipath linear md_mod sd_mod virtio_scsi ata_generic crc32c_intel ata_piix floppy virtio_pci virtio_ring virtio libata scsi_mod e1000 [last unloaded: btrfs]
     [95794.850658] CPU: 0 PID: 31496 Comm: umount Tainted: G        W       4.14.0-rc6-btrfs-next-54+ #1
     [95794.852590] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS rel-1.10.2-0-g5f4c7b1-prebuilt.qemu-project.org 04/01/2014
     [95794.854752] task: ffff880075aa0240 task.stack: ffffc90001734000
     [95794.855812] RIP: 0010:btrfs_free_block_groups+0x235/0x36a [btrfs]
     [95794.856811] RSP: 0018:ffffc90001737d70 EFLAGS: 00010206
     [95794.857805] RAX: 0000000080000000 RBX: ffff88006145c000 RCX: 0000000000000001
     [95794.859014] RDX: 00000001810af668 RSI: 0000000000000002 RDI: 00000000ffffffff
     [95794.860270] RBP: ffffc90001737d98 R08: 0000000000000000 R09: ffffffff817e22b9
     [95794.861525] R10: ffffc90001737c80 R11: 00000000000337fd R12: 0000000000000000
     [95794.862700] R13: ffff88006145c0c0 R14: ffff88021b61a800 R15: ffff88006145c100
     [95794.863810] FS:  00007fa6793c92c0(0000) GS:ffff88023fc00000(0000) knlGS:0000000000000000
     [95794.865149] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
     [95794.866099] CR2: 000056338670d048 CR3: 00000000610dc005 CR4: 00000000001606f0
     [95794.867198] Call Trace:
     [95794.867626]  close_ctree+0x1db/0x2b8 [btrfs]
     [95794.868188]  ? evict_inodes+0x132/0x141
     [95794.869037]  btrfs_put_super+0x15/0x17 [btrfs]
     [95794.870400]  generic_shutdown_super+0x6a/0x10b
     [95794.871262]  kill_anon_super+0x12/0x1c
     [95794.872046]  btrfs_kill_super+0x16/0x21 [btrfs]
     [95794.872746]  deactivate_locked_super+0x30/0x68
     [95794.873687]  deactivate_super+0x36/0x39
     [95794.874639]  cleanup_mnt+0x49/0x67
     [95794.875504]  __cleanup_mnt+0x12/0x14
     [95794.876126]  task_work_run+0x82/0xa6
     [95794.876788]  prepare_exit_to_usermode+0xe1/0x10c
     [95794.877777]  syscall_return_slowpath+0x18c/0x1af
     [95794.878381]  entry_SYSCALL_64_fastpath+0xab/0xad
     [95794.878888] RIP: 0033:0x7fa678cb99a7
     [95794.879307] RSP: 002b:00007ffccf0aaed8 EFLAGS: 00000246 ORIG_RAX: 00000000000000a6
     [95794.880204] RAX: 0000000000000000 RBX: 0000563386706030 RCX: 00007fa678cb99a7
     [95794.881640] RDX: 0000000000000001 RSI: 0000000000000000 RDI: 000056338670ca90
     [95794.882690] RBP: 000056338670ca90 R08: 000056338670c740 R09: 0000000000000015
     [95794.883538] R10: 00000000000006b4 R11: 0000000000000246 R12: 00007fa6791bae64
     [95794.884562] R13: 0000000000000000 R14: 0000563386706210 R15: 00007ffccf0ab160
     [95794.885664] Code: 89 ef e8 07 ec 32 e1 e8 9d c0 ea e0 48 8d b3 28 02 00 00 48 83 c9 ff 31 d2 48 89 df e8 29 c5 ff ff 48 83 bb 80 02 00 00 00 74 02 <0f> ff 48 83 bb 88 02 00 00 00 74 02 0f ff 48 83 bb d8 02 00 00
     [95794.887980] ---[ end trace e95877675c6ec00a ]---
     [95794.888739] ------------[ cut here ]------------
     [95794.889405] WARNING: CPU: 0 PID: 31496 at fs/btrfs/extent-tree.c:5832 btrfs_free_block_groups+0x241/0x36a [btrfs]
     [95794.891020] Modules linked in: btrfs xfs ppdev ghash_clmulni_intel pcbc aesni_intel aes_x86_64 crypto_simd cryptd glue_helper parport_pc psmouse sg i2c_piix4 parport i2c_core evdev pcspkr button serio_raw sunrpc loop autofs4 ext4 crc16 mbcache jbd2 zstd_decompress zstd_compress xxhash raid10 raid456 async_raid6_recov async_memcpy async_pq async_xor async_tx xor raid6_pq libcrc32c crc32c_generic raid1 raid0 multipath linear md_mod sd_mod virtio_scsi ata_generic crc32c_intel ata_piix floppy virtio_pci virtio_ring virtio libata scsi_mod e1000 [last unloaded: btrfs]
     [95794.897551] CPU: 0 PID: 31496 Comm: umount Tainted: G        W       4.14.0-rc6-btrfs-next-54+ #1
     [95794.898509] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS rel-1.10.2-0-g5f4c7b1-prebuilt.qemu-project.org 04/01/2014
     [95794.899685] task: ffff880075aa0240 task.stack: ffffc90001734000
     [95794.900592] RIP: 0010:btrfs_free_block_groups+0x241/0x36a [btrfs]
     [95794.901387] RSP: 0018:ffffc90001737d70 EFLAGS: 00010206
     [95794.902300] RAX: 0000000080000000 RBX: ffff88006145c000 RCX: 0000000000000001
     [95794.903260] RDX: 00000001810af668 RSI: 0000000000000002 RDI: 00000000ffffffff
     [95794.904332] RBP: ffffc90001737d98 R08: 0000000000000000 R09: ffffffff817e22b9
     [95794.905300] R10: ffffc90001737c80 R11: 00000000000337fd R12: 0000000000000000
     [95794.906439] R13: ffff88006145c0c0 R14: ffff88021b61a800 R15: ffff88006145c100
     [95794.907459] FS:  00007fa6793c92c0(0000) GS:ffff88023fc00000(0000) knlGS:0000000000000000
     [95794.908625] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
     [95794.909511] CR2: 000056338670d048 CR3: 00000000610dc005 CR4: 00000000001606f0
     [95794.910630] Call Trace:
     [95794.911153]  close_ctree+0x1db/0x2b8 [btrfs]
     [95794.911837]  ? evict_inodes+0x132/0x141
     [95794.912344]  btrfs_put_super+0x15/0x17 [btrfs]
     [95794.912975]  generic_shutdown_super+0x6a/0x10b
     [95794.913788]  kill_anon_super+0x12/0x1c
     [95794.914424]  btrfs_kill_super+0x16/0x21 [btrfs]
     [95794.915142]  deactivate_locked_super+0x30/0x68
     [95794.915831]  deactivate_super+0x36/0x39
     [95794.916433]  cleanup_mnt+0x49/0x67
     [95794.917045]  __cleanup_mnt+0x12/0x14
     [95794.917665]  task_work_run+0x82/0xa6
     [95794.918309]  prepare_exit_to_usermode+0xe1/0x10c
     [95794.919021]  syscall_return_slowpath+0x18c/0x1af
     [95794.919722]  entry_SYSCALL_64_fastpath+0xab/0xad
     [95794.920426] RIP: 0033:0x7fa678cb99a7
     [95794.921039] RSP: 002b:00007ffccf0aaed8 EFLAGS: 00000246 ORIG_RAX: 00000000000000a6
     [95794.922303] RAX: 0000000000000000 RBX: 0000563386706030 RCX: 00007fa678cb99a7
     [95794.923335] RDX: 0000000000000001 RSI: 0000000000000000 RDI: 000056338670ca90
     [95794.924364] RBP: 000056338670ca90 R08: 000056338670c740 R09: 0000000000000015
     [95794.925435] R10: 00000000000006b4 R11: 0000000000000246 R12: 00007fa6791bae64
     [95794.926533] R13: 0000000000000000 R14: 0000563386706210 R15: 00007ffccf0ab160
     [95794.927557] Code: 48 8d b3 28 02 00 00 48 83 c9 ff 31 d2 48 89 df e8 29 c5 ff ff 48 83 bb 80 02 00 00 00 74 02 0f ff 48 83 bb 88 02 00 00 00 74 02 <0f> ff 48 83 bb d8 02 00 00 00 74 02 0f ff 48 83 bb e0 02 00 00
     [95794.930166] ---[ end trace e95877675c6ec00b ]---
     [95794.930961] ------------[ cut here ]------------
     [95794.931727] WARNING: CPU: 0 PID: 31496 at fs/btrfs/extent-tree.c:9953 btrfs_free_block_groups+0x2bc/0x36a [btrfs]
     [95794.932729] Modules linked in: btrfs xfs ppdev ghash_clmulni_intel pcbc aesni_intel aes_x86_64 crypto_simd cryptd glue_helper parport_pc psmouse sg i2c_piix4 parport i2c_core evdev pcspkr button serio_raw sunrpc loop autofs4 ext4 crc16 mbcache jbd2 zstd_decompress zstd_compress xxhash raid10 raid456 async_raid6_recov async_memcpy async_pq async_xor async_tx xor raid6_pq libcrc32c crc32c_generic raid1 raid0 multipath linear md_mod sd_mod virtio_scsi ata_generic crc32c_intel ata_piix floppy virtio_pci virtio_ring virtio libata scsi_mod e1000 [last unloaded: btrfs]
     [95794.938394] CPU: 0 PID: 31496 Comm: umount Tainted: G        W       4.14.0-rc6-btrfs-next-54+ #1
     [95794.939842] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS rel-1.10.2-0-g5f4c7b1-prebuilt.qemu-project.org 04/01/2014
     [95794.941455] task: ffff880075aa0240 task.stack: ffffc90001734000
     [95794.942336] RIP: 0010:btrfs_free_block_groups+0x2bc/0x36a [btrfs]
     [95794.943268] RSP: 0018:ffffc90001737d70 EFLAGS: 00010206
     [95794.944127] RAX: ffff8802004fd0e8 RBX: ffff88006145c000 RCX: 0000000000000001
     [95794.945211] RDX: 00000001810af668 RSI: 0000000000000002 RDI: 00000000ffffffff
     [95794.946316] RBP: ffffc90001737d98 R08: 0000000000000000 R09: ffffffff817e22b9
     [95794.947271] R10: ffffc90001737c80 R11: 00000000000337fd R12: ffff8802004fd0e8
     [95794.948219] R13: ffff88006145c0c0 R14: ffff88006145e598 R15: ffff88006145c100
     [95794.949193] FS:  00007fa6793c92c0(0000) GS:ffff88023fc00000(0000) knlGS:0000000000000000
     [95794.950495] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
     [95794.951338] CR2: 000056338670d048 CR3: 00000000610dc005 CR4: 00000000001606f0
     [95794.952361] Call Trace:
     [95794.952811]  close_ctree+0x1db/0x2b8 [btrfs]
     [95794.953522]  ? evict_inodes+0x132/0x141
     [95794.954543]  btrfs_put_super+0x15/0x17 [btrfs]
     [95794.955231]  generic_shutdown_super+0x6a/0x10b
     [95794.955916]  kill_anon_super+0x12/0x1c
     [95794.956414]  btrfs_kill_super+0x16/0x21 [btrfs]
     [95794.956953]  deactivate_locked_super+0x30/0x68
     [95794.957635]  deactivate_super+0x36/0x39
     [95794.958256]  cleanup_mnt+0x49/0x67
     [95794.958701]  __cleanup_mnt+0x12/0x14
     [95794.959181]  task_work_run+0x82/0xa6
     [95794.959635]  prepare_exit_to_usermode+0xe1/0x10c
     [95794.960182]  syscall_return_slowpath+0x18c/0x1af
     [95794.960731]  entry_SYSCALL_64_fastpath+0xab/0xad
     [95794.961438] RIP: 0033:0x7fa678cb99a7
     [95794.961990] RSP: 002b:00007ffccf0aaed8 EFLAGS: 00000246 ORIG_RAX: 00000000000000a6
     [95794.963111] RAX: 0000000000000000 RBX: 0000563386706030 RCX: 00007fa678cb99a7
     [95794.963975] RDX: 0000000000000001 RSI: 0000000000000000 RDI: 000056338670ca90
     [95794.964680] RBP: 000056338670ca90 R08: 000056338670c740 R09: 0000000000000015
     [95794.965763] R10: 00000000000006b4 R11: 0000000000000246 R12: 00007fa6791bae64
     [95794.966868] R13: 0000000000000000 R14: 0000563386706210 R15: 00007ffccf0ab160
     [95794.967800] Code: 00 00 00 4c 8b a3 98 25 00 00 49 83 bc 24 60 ff ff ff 00 75 16 49 83 bc 24 68 ff ff ff 00 75 0b 49 83 bc 24 70 ff ff ff 00 74 16 <0f> ff 49 8d b4 24 18 ff ff ff 31 c9 31 d2 48 89 df e8 93 7a ff
     [95794.970629] ---[ end trace e95877675c6ec00c ]---
     [95794.971451] BTRFS info (device sdi): space_info 1 has 7680000 free, is not full
     [95794.972351] BTRFS info (device sdi): space_info total=8388608, used=704512, pinned=0, reserved=0, may_use=4096, readonly=0
     [95794.973595] ------------[ cut here ]------------
     [95794.974353] WARNING: CPU: 0 PID: 31496 at fs/btrfs/extent-tree.c:9953 btrfs_free_block_groups+0x2bc/0x36a [btrfs]
     [95794.980163] Modules linked in: btrfs xfs ppdev ghash_clmulni_intel pcbc aesni_intel aes_x86_64 crypto_simd cryptd glue_helper parport_pc psmouse sg i2c_piix4 parport i2c_core evdev pcspkr button serio_raw sunrpc loop autofs4 ext4 crc16 mbcache jbd2 zstd_decompress zstd_compress xxhash raid10 raid456 async_raid6_recov async_memcpy async_pq async_xor async_tx xor raid6_pq libcrc32c crc32c_generic raid1 raid0 multipath linear md_mod sd_mod virtio_scsi ata_generic crc32c_intel ata_piix floppy virtio_pci virtio_ring virtio libata scsi_mod e1000 [last unloaded: btrfs]
     [95794.986461] CPU: 0 PID: 31496 Comm: umount Tainted: G        W       4.14.0-rc6-btrfs-next-54+ #1
     [95794.987591] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS rel-1.10.2-0-g5f4c7b1-prebuilt.qemu-project.org 04/01/2014
     [95794.988929] task: ffff880075aa0240 task.stack: ffffc90001734000
     [95794.989922] RIP: 0010:btrfs_free_block_groups+0x2bc/0x36a [btrfs]
     [95794.990715] RSP: 0018:ffffc90001737d70 EFLAGS: 00010206
     [95794.991431] RAX: ffff88020f6e70e8 RBX: ffff88006145c000 RCX: ffffffff8115a906
     [95794.992455] RDX: ffffffff8115a902 RSI: ffff880075aa0b40 RDI: ffff880075aa0b40
     [95794.993535] RBP: ffffc90001737d98 R08: 0000000000000020 R09: fffffffffffffff7
     [95794.994573] R10: 00000000ffffffc4 R11: ffff8800633b1bc0 R12: ffff88020f6e70e8
     [95794.996250] R13: 0000000000000038 R14: ffff88006145e598 R15: 0000000000000000
     [95794.997233] FS:  00007fa6793c92c0(0000) GS:ffff88023fc00000(0000) knlGS:0000000000000000
     [95794.998592] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
     [95794.999484] CR2: 000056338670d048 CR3: 00000000610dc005 CR4: 00000000001606f0
     [95795.000542] Call Trace:
     [95795.001138]  close_ctree+0x1db/0x2b8 [btrfs]
     [95795.001885]  ? evict_inodes+0x132/0x141
     [95795.002407]  btrfs_put_super+0x15/0x17 [btrfs]
     [95795.003093]  generic_shutdown_super+0x6a/0x10b
     [95795.003720]  kill_anon_super+0x12/0x1c
     [95795.004353]  btrfs_kill_super+0x16/0x21 [btrfs]
     [95795.005095]  deactivate_locked_super+0x30/0x68
     [95795.005716]  deactivate_super+0x36/0x39
     [95795.006388]  cleanup_mnt+0x49/0x67
     [95795.006939]  __cleanup_mnt+0x12/0x14
     [95795.007512]  task_work_run+0x82/0xa6
     [95795.008124]  prepare_exit_to_usermode+0xe1/0x10c
     [95795.008994]  syscall_return_slowpath+0x18c/0x1af
     [95795.009831]  entry_SYSCALL_64_fastpath+0xab/0xad
     [95795.010610] RIP: 0033:0x7fa678cb99a7
     [95795.011193] RSP: 002b:00007ffccf0aaed8 EFLAGS: 00000246 ORIG_RAX: 00000000000000a6
     [95795.012327] RAX: 0000000000000000 RBX: 0000563386706030 RCX: 00007fa678cb99a7
     [95795.013432] RDX: 0000000000000001 RSI: 0000000000000000 RDI: 000056338670ca90
     [95795.014558] RBP: 000056338670ca90 R08: 000056338670c740 R09: 0000000000000015
     [95795.015577] R10: 00000000000006b4 R11: 0000000000000246 R12: 00007fa6791bae64
     [95795.016569] R13: 0000000000000000 R14: 0000563386706210 R15: 00007ffccf0ab160
     [95795.017662] Code: 00 00 00 4c 8b a3 98 25 00 00 49 83 bc 24 60 ff ff ff 00 75 16 49 83 bc 24 68 ff ff ff 00 75 0b 49 83 bc 24 70 ff ff ff 00 74 16 <0f> ff 49 8d b4 24 18 ff ff ff 31 c9 31 d2 48 89 df e8 93 7a ff
     [95795.020538] ---[ end trace e95877675c6ec00d ]---
     [95795.021259] BTRFS info (device sdi): space_info 4 has 1072775168 free, is not full
     [95795.022390] BTRFS info (device sdi): space_info total=1073741824, used=114688, pinned=0, reserved=0, may_use=786432, readonly=65536
    
    Fix this by ensuring the zero range operation does not call
    btrfs_truncate_block() if the corresponding extent is an unwritten one
    (it's pointless anyway, since reading from an unwritten extent yields
    zeroes).
    
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Tested-by: Nikolay Borisov <nborisov@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index baad81c1f9a3..06a631f89b1e 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2852,12 +2852,18 @@ static int btrfs_fallocate_update_isize(struct inode *inode,
 	return ret ? ret : ret2;
 }
 
+enum {
+	RANGE_BOUNDARY_WRITTEN_EXTENT = 0,
+	RANGE_BOUNDARY_PREALLOC_EXTENT = 1,
+	RANGE_BOUNDARY_HOLE = 2,
+};
+
 static int btrfs_zero_range_check_range_boundary(struct inode *inode,
 						 u64 offset)
 {
 	const u64 sectorsize = btrfs_inode_sectorsize(inode);
 	struct extent_map *em;
-	int ret = 0;
+	int ret;
 
 	offset = round_down(offset, sectorsize);
 	em = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);
@@ -2865,7 +2871,11 @@ static int btrfs_zero_range_check_range_boundary(struct inode *inode,
 		return PTR_ERR(em);
 
 	if (em->block_start == EXTENT_MAP_HOLE)
-		ret = 1;
+		ret = RANGE_BOUNDARY_HOLE;
+	else if (test_bit(EXTENT_FLAG_PREALLOC, &em->flags))
+		ret = RANGE_BOUNDARY_PREALLOC_EXTENT;
+	else
+		ret = RANGE_BOUNDARY_WRITTEN_EXTENT;
 
 	free_extent_map(em);
 	return ret;
@@ -2974,13 +2984,15 @@ static int btrfs_zero_range(struct inode *inode,
 		ret = btrfs_zero_range_check_range_boundary(inode, offset);
 		if (ret < 0)
 			goto out;
-		if (ret) {
+		if (ret == RANGE_BOUNDARY_HOLE) {
 			alloc_start = round_down(offset, sectorsize);
 			ret = 0;
-		} else {
+		} else if (ret == RANGE_BOUNDARY_WRITTEN_EXTENT) {
 			ret = btrfs_truncate_block(inode, offset, 0, 0);
 			if (ret)
 				goto out;
+		} else {
+			ret = 0;
 		}
 	}
 
@@ -2989,13 +3001,15 @@ static int btrfs_zero_range(struct inode *inode,
 							    offset + len);
 		if (ret < 0)
 			goto out;
-		if (ret) {
+		if (ret == RANGE_BOUNDARY_HOLE) {
 			alloc_end = round_up(offset + len, sectorsize);
 			ret = 0;
-		} else {
+		} else if (ret == RANGE_BOUNDARY_WRITTEN_EXTENT) {
 			ret = btrfs_truncate_block(inode, offset + len, 0, 1);
 			if (ret)
 				goto out;
+		} else {
+			ret = 0;
 		}
 	}
 

commit 9f13ce743b1bd4e764193980e6311bfcdf424bb2
Author: Filipe Manana <fdmanana@suse.com>
Date:   Thu Jan 18 11:34:20 2018 +0000

    Btrfs: fix missing inode i_size update after zero range operation
    
    For a fallocate's zero range operation that targets a range with an end
    that is not aligned to the sector size, we can end up not updating the
    inode's i_size. This happens when the last page of the range maps to an
    unwritten (prealloc) extent and before that last page we have either a
    hole or a written extent. This is because in this scenario we relied
    on a call to btrfs_prealloc_file_range() to update the inode's i_size,
    however it can only update the i_size to the "down aligned" end of the
    range.
    
    Example:
    
     $ mkfs.btrfs -f /dev/sdc
     $ mount /dev/sdc /mnt
     $ xfs_io -f -c "pwrite -S 0xff 0 428K" /mnt/foobar
     $ xfs_io -c "falloc -k 428K 4K" /mnt/foobar
     $ xfs_io -c "fzero 0 430K" /mnt/foobar
     $ du --bytes /mnt/foobar
     438272 /mnt/foobar
    
    The inode's i_size was left as 428Kb (438272 bytes) when it should have
    been updated to 430Kb (440320 bytes).
    Fix this by always updating the inode's i_size explicitly after zeroing
    the range.
    
    Fixes: ba6d5887946ff86d93dc ("Btrfs: add support for fallocate's zero range operation")
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index cba2ac371ce0..baad81c1f9a3 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -3026,9 +3026,12 @@ static int btrfs_zero_range(struct inode *inode,
 		unlock_extent_cached(&BTRFS_I(inode)->io_tree, lockstart,
 				     lockend, &cached_state);
 		/* btrfs_prealloc_file_range releases reserved space on error */
-		if (ret)
+		if (ret) {
 			space_reserved = false;
+			goto out;
+		}
 	}
+	ret = btrfs_fallocate_update_isize(inode, offset + len, mode);
  out:
 	if (ret && space_reserved)
 		btrfs_free_reserved_data_space(inode, data_reserved,

commit 94f450712ac9cb4e165b5115e5eb0ab10055a64b
Author: Filipe Manana <fdmanana@suse.com>
Date:   Tue Oct 31 17:59:54 2017 +0000

    Btrfs: use cached state when dirtying pages during buffered write
    
    During a buffered IO write, we can have an extent state that we got when
    we locked the range (if the range starts at an offset lower than eof), so
    always pass it to btrfs_dirty_pages() so that setting the delalloc bit
    in the range does not need to do a full search in the inode's io tree,
    saving time and reducing the amount of time we hold the io tree's lock.
    
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 16c8031db645..cba2ac371ce0 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1755,7 +1755,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 
 		if (copied > 0)
 			ret = btrfs_dirty_pages(inode, pages, dirty_pages,
-						pos, copied, NULL);
+						pos, copied, &cached_state);
 		if (extents_locked)
 			unlock_extent_cached(&BTRFS_I(inode)->io_tree,
 					     lockstart, lockend, &cached_state);

commit f27451f229966874a8793995b8e6b74326d125df
Author: Filipe Manana <fdmanana@suse.com>
Date:   Wed Oct 25 11:55:28 2017 +0100

    Btrfs: add support for fallocate's zero range operation
    
    This implements support the zero range operation of fallocate. For now
    at least it's as simple as possible while reusing most of the existing
    fallocate and hole punching infrastructure.
    
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 1ed2e6e9e204..16c8031db645 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2458,6 +2458,46 @@ static int find_first_non_hole(struct inode *inode, u64 *start, u64 *len)
 	return ret;
 }
 
+static int btrfs_punch_hole_lock_range(struct inode *inode,
+				       const u64 lockstart,
+				       const u64 lockend,
+				       struct extent_state **cached_state)
+{
+	while (1) {
+		struct btrfs_ordered_extent *ordered;
+		int ret;
+
+		truncate_pagecache_range(inode, lockstart, lockend);
+
+		lock_extent_bits(&BTRFS_I(inode)->io_tree, lockstart, lockend,
+				 cached_state);
+		ordered = btrfs_lookup_first_ordered_extent(inode, lockend);
+
+		/*
+		 * We need to make sure we have no ordered extents in this range
+		 * and nobody raced in and read a page in this range, if we did
+		 * we need to try again.
+		 */
+		if ((!ordered ||
+		    (ordered->file_offset + ordered->len <= lockstart ||
+		     ordered->file_offset > lockend)) &&
+		     !btrfs_page_exists_in_range(inode, lockstart, lockend)) {
+			if (ordered)
+				btrfs_put_ordered_extent(ordered);
+			break;
+		}
+		if (ordered)
+			btrfs_put_ordered_extent(ordered);
+		unlock_extent_cached(&BTRFS_I(inode)->io_tree, lockstart,
+				     lockend, cached_state);
+		ret = btrfs_wait_ordered_range(inode, lockstart,
+					       lockend - lockstart + 1);
+		if (ret)
+			return ret;
+	}
+	return 0;
+}
+
 static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 {
 	struct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);
@@ -2574,38 +2614,11 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 		goto out_only_mutex;
 	}
 
-	while (1) {
-		struct btrfs_ordered_extent *ordered;
-
-		truncate_pagecache_range(inode, lockstart, lockend);
-
-		lock_extent_bits(&BTRFS_I(inode)->io_tree, lockstart, lockend,
-				 &cached_state);
-		ordered = btrfs_lookup_first_ordered_extent(inode, lockend);
-
-		/*
-		 * We need to make sure we have no ordered extents in this range
-		 * and nobody raced in and read a page in this range, if we did
-		 * we need to try again.
-		 */
-		if ((!ordered ||
-		    (ordered->file_offset + ordered->len <= lockstart ||
-		     ordered->file_offset > lockend)) &&
-		     !btrfs_page_exists_in_range(inode, lockstart, lockend)) {
-			if (ordered)
-				btrfs_put_ordered_extent(ordered);
-			break;
-		}
-		if (ordered)
-			btrfs_put_ordered_extent(ordered);
-		unlock_extent_cached(&BTRFS_I(inode)->io_tree, lockstart,
-				     lockend, &cached_state);
-		ret = btrfs_wait_ordered_range(inode, lockstart,
-					       lockend - lockstart + 1);
-		if (ret) {
-			inode_unlock(inode);
-			return ret;
-		}
+	ret = btrfs_punch_hole_lock_range(inode, lockstart, lockend,
+					  &cached_state);
+	if (ret) {
+		inode_unlock(inode);
+		goto out_only_mutex;
 	}
 
 	path = btrfs_alloc_path();
@@ -2814,6 +2827,217 @@ static int add_falloc_range(struct list_head *head, u64 start, u64 len)
 	return 0;
 }
 
+static int btrfs_fallocate_update_isize(struct inode *inode,
+					const u64 end,
+					const int mode)
+{
+	struct btrfs_trans_handle *trans;
+	struct btrfs_root *root = BTRFS_I(inode)->root;
+	int ret;
+	int ret2;
+
+	if (mode & FALLOC_FL_KEEP_SIZE || end <= i_size_read(inode))
+		return 0;
+
+	trans = btrfs_start_transaction(root, 1);
+	if (IS_ERR(trans))
+		return PTR_ERR(trans);
+
+	inode->i_ctime = current_time(inode);
+	i_size_write(inode, end);
+	btrfs_ordered_update_i_size(inode, end, NULL);
+	ret = btrfs_update_inode(trans, root, inode);
+	ret2 = btrfs_end_transaction(trans);
+
+	return ret ? ret : ret2;
+}
+
+static int btrfs_zero_range_check_range_boundary(struct inode *inode,
+						 u64 offset)
+{
+	const u64 sectorsize = btrfs_inode_sectorsize(inode);
+	struct extent_map *em;
+	int ret = 0;
+
+	offset = round_down(offset, sectorsize);
+	em = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);
+	if (IS_ERR(em))
+		return PTR_ERR(em);
+
+	if (em->block_start == EXTENT_MAP_HOLE)
+		ret = 1;
+
+	free_extent_map(em);
+	return ret;
+}
+
+static int btrfs_zero_range(struct inode *inode,
+			    loff_t offset,
+			    loff_t len,
+			    const int mode)
+{
+	struct btrfs_fs_info *fs_info = BTRFS_I(inode)->root->fs_info;
+	struct extent_map *em;
+	struct extent_changeset *data_reserved = NULL;
+	int ret;
+	u64 alloc_hint = 0;
+	const u64 sectorsize = btrfs_inode_sectorsize(inode);
+	u64 alloc_start = round_down(offset, sectorsize);
+	u64 alloc_end = round_up(offset + len, sectorsize);
+	u64 bytes_to_reserve = 0;
+	bool space_reserved = false;
+
+	inode_dio_wait(inode);
+
+	em = btrfs_get_extent(BTRFS_I(inode), NULL, 0,
+			      alloc_start, alloc_end - alloc_start, 0);
+	if (IS_ERR(em)) {
+		ret = PTR_ERR(em);
+		goto out;
+	}
+
+	/*
+	 * Avoid hole punching and extent allocation for some cases. More cases
+	 * could be considered, but these are unlikely common and we keep things
+	 * as simple as possible for now. Also, intentionally, if the target
+	 * range contains one or more prealloc extents together with regular
+	 * extents and holes, we drop all the existing extents and allocate a
+	 * new prealloc extent, so that we get a larger contiguous disk extent.
+	 */
+	if (em->start <= alloc_start &&
+	    test_bit(EXTENT_FLAG_PREALLOC, &em->flags)) {
+		const u64 em_end = em->start + em->len;
+
+		if (em_end >= offset + len) {
+			/*
+			 * The whole range is already a prealloc extent,
+			 * do nothing except updating the inode's i_size if
+			 * needed.
+			 */
+			free_extent_map(em);
+			ret = btrfs_fallocate_update_isize(inode, offset + len,
+							   mode);
+			goto out;
+		}
+		/*
+		 * Part of the range is already a prealloc extent, so operate
+		 * only on the remaining part of the range.
+		 */
+		alloc_start = em_end;
+		ASSERT(IS_ALIGNED(alloc_start, sectorsize));
+		len = offset + len - alloc_start;
+		offset = alloc_start;
+		alloc_hint = em->block_start + em->len;
+	}
+	free_extent_map(em);
+
+	if (BTRFS_BYTES_TO_BLKS(fs_info, offset) ==
+	    BTRFS_BYTES_TO_BLKS(fs_info, offset + len - 1)) {
+		em = btrfs_get_extent(BTRFS_I(inode), NULL, 0,
+				      alloc_start, sectorsize, 0);
+		if (IS_ERR(em)) {
+			ret = PTR_ERR(em);
+			goto out;
+		}
+
+		if (test_bit(EXTENT_FLAG_PREALLOC, &em->flags)) {
+			free_extent_map(em);
+			ret = btrfs_fallocate_update_isize(inode, offset + len,
+							   mode);
+			goto out;
+		}
+		if (len < sectorsize && em->block_start != EXTENT_MAP_HOLE) {
+			free_extent_map(em);
+			ret = btrfs_truncate_block(inode, offset, len, 0);
+			if (!ret)
+				ret = btrfs_fallocate_update_isize(inode,
+								   offset + len,
+								   mode);
+			return ret;
+		}
+		free_extent_map(em);
+		alloc_start = round_down(offset, sectorsize);
+		alloc_end = alloc_start + sectorsize;
+		goto reserve_space;
+	}
+
+	alloc_start = round_up(offset, sectorsize);
+	alloc_end = round_down(offset + len, sectorsize);
+
+	/*
+	 * For unaligned ranges, check the pages at the boundaries, they might
+	 * map to an extent, in which case we need to partially zero them, or
+	 * they might map to a hole, in which case we need our allocation range
+	 * to cover them.
+	 */
+	if (!IS_ALIGNED(offset, sectorsize)) {
+		ret = btrfs_zero_range_check_range_boundary(inode, offset);
+		if (ret < 0)
+			goto out;
+		if (ret) {
+			alloc_start = round_down(offset, sectorsize);
+			ret = 0;
+		} else {
+			ret = btrfs_truncate_block(inode, offset, 0, 0);
+			if (ret)
+				goto out;
+		}
+	}
+
+	if (!IS_ALIGNED(offset + len, sectorsize)) {
+		ret = btrfs_zero_range_check_range_boundary(inode,
+							    offset + len);
+		if (ret < 0)
+			goto out;
+		if (ret) {
+			alloc_end = round_up(offset + len, sectorsize);
+			ret = 0;
+		} else {
+			ret = btrfs_truncate_block(inode, offset + len, 0, 1);
+			if (ret)
+				goto out;
+		}
+	}
+
+reserve_space:
+	if (alloc_start < alloc_end) {
+		struct extent_state *cached_state = NULL;
+		const u64 lockstart = alloc_start;
+		const u64 lockend = alloc_end - 1;
+
+		bytes_to_reserve = alloc_end - alloc_start;
+		ret = btrfs_alloc_data_chunk_ondemand(BTRFS_I(inode),
+						      bytes_to_reserve);
+		if (ret < 0)
+			goto out;
+		space_reserved = true;
+		ret = btrfs_qgroup_reserve_data(inode, &data_reserved,
+						alloc_start, bytes_to_reserve);
+		if (ret)
+			goto out;
+		ret = btrfs_punch_hole_lock_range(inode, lockstart, lockend,
+						  &cached_state);
+		if (ret)
+			goto out;
+		ret = btrfs_prealloc_file_range(inode, mode, alloc_start,
+						alloc_end - alloc_start,
+						i_blocksize(inode),
+						offset + len, &alloc_hint);
+		unlock_extent_cached(&BTRFS_I(inode)->io_tree, lockstart,
+				     lockend, &cached_state);
+		/* btrfs_prealloc_file_range releases reserved space on error */
+		if (ret)
+			space_reserved = false;
+	}
+ out:
+	if (ret && space_reserved)
+		btrfs_free_reserved_data_space(inode, data_reserved,
+					       alloc_start, bytes_to_reserve);
+	extent_changeset_free(data_reserved);
+
+	return ret;
+}
+
 static long btrfs_fallocate(struct file *file, int mode,
 			    loff_t offset, loff_t len)
 {
@@ -2839,7 +3063,8 @@ static long btrfs_fallocate(struct file *file, int mode,
 	cur_offset = alloc_start;
 
 	/* Make sure we aren't being give some crap mode */
-	if (mode & ~(FALLOC_FL_KEEP_SIZE | FALLOC_FL_PUNCH_HOLE))
+	if (mode & ~(FALLOC_FL_KEEP_SIZE | FALLOC_FL_PUNCH_HOLE |
+		     FALLOC_FL_ZERO_RANGE))
 		return -EOPNOTSUPP;
 
 	if (mode & FALLOC_FL_PUNCH_HOLE)
@@ -2850,10 +3075,12 @@ static long btrfs_fallocate(struct file *file, int mode,
 	 *
 	 * For qgroup space, it will be checked later.
 	 */
-	ret = btrfs_alloc_data_chunk_ondemand(BTRFS_I(inode),
-			alloc_end - alloc_start);
-	if (ret < 0)
-		return ret;
+	if (!(mode & FALLOC_FL_ZERO_RANGE)) {
+		ret = btrfs_alloc_data_chunk_ondemand(BTRFS_I(inode),
+						      alloc_end - alloc_start);
+		if (ret < 0)
+			return ret;
+	}
 
 	inode_lock(inode);
 
@@ -2895,6 +3122,12 @@ static long btrfs_fallocate(struct file *file, int mode,
 	if (ret)
 		goto out;
 
+	if (mode & FALLOC_FL_ZERO_RANGE) {
+		ret = btrfs_zero_range(inode, offset, len, mode);
+		inode_unlock(inode);
+		return ret;
+	}
+
 	locked_end = alloc_end - 1;
 	while (1) {
 		struct btrfs_ordered_extent *ordered;
@@ -2988,37 +3221,18 @@ static long btrfs_fallocate(struct file *file, int mode,
 	if (ret < 0)
 		goto out_unlock;
 
-	if (actual_end > inode->i_size &&
-	    !(mode & FALLOC_FL_KEEP_SIZE)) {
-		struct btrfs_trans_handle *trans;
-		struct btrfs_root *root = BTRFS_I(inode)->root;
-
-		/*
-		 * We didn't need to allocate any more space, but we
-		 * still extended the size of the file so we need to
-		 * update i_size and the inode item.
-		 */
-		trans = btrfs_start_transaction(root, 1);
-		if (IS_ERR(trans)) {
-			ret = PTR_ERR(trans);
-		} else {
-			inode->i_ctime = current_time(inode);
-			i_size_write(inode, actual_end);
-			btrfs_ordered_update_i_size(inode, actual_end, NULL);
-			ret = btrfs_update_inode(trans, root, inode);
-			if (ret)
-				btrfs_end_transaction(trans);
-			else
-				ret = btrfs_end_transaction(trans);
-		}
-	}
+	/*
+	 * We didn't need to allocate any more space, but we still extended the
+	 * size of the file so we need to update i_size and the inode item.
+	 */
+	ret = btrfs_fallocate_update_isize(inode, actual_end, mode);
 out_unlock:
 	unlock_extent_cached(&BTRFS_I(inode)->io_tree, alloc_start, locked_end,
 			     &cached_state);
 out:
 	inode_unlock(inode);
 	/* Let go of our reservation. */
-	if (ret != 0)
+	if (ret != 0 && !(mode & FALLOC_FL_ZERO_RANGE))
 		btrfs_free_reserved_data_space(inode, data_reserved,
 				alloc_start, alloc_end - cur_offset);
 	extent_changeset_free(data_reserved);

commit e43bbe5e16d87b40f3b382b3a43b0142d6d1193d
Author: David Sterba <dsterba@suse.com>
Date:   Tue Dec 12 21:43:52 2017 +0100

    btrfs: sink unlock_extent parameter gfp_flags
    
    All callers pass either GFP_NOFS or GFP_KERNEL now, so we can sink the
    parameter to the function, though we lose some of the slightly better
    semantics of GFP_KERNEL in some places, it's worth cleaning up the
    callchains.
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 1096398e1351..1ed2e6e9e204 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1504,7 +1504,7 @@ lock_and_cleanup_extent_if_need(struct btrfs_inode *inode, struct page **pages,
 		    ordered->file_offset + ordered->len > start_pos &&
 		    ordered->file_offset <= last_pos) {
 			unlock_extent_cached(&inode->io_tree, start_pos,
-					last_pos, cached_state, GFP_NOFS);
+					last_pos, cached_state);
 			for (i = 0; i < num_pages; i++) {
 				unlock_page(pages[i]);
 				put_page(pages[i]);
@@ -1758,8 +1758,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 						pos, copied, NULL);
 		if (extents_locked)
 			unlock_extent_cached(&BTRFS_I(inode)->io_tree,
-					     lockstart, lockend, &cached_state,
-					     GFP_NOFS);
+					     lockstart, lockend, &cached_state);
 		btrfs_delalloc_release_extents(BTRFS_I(inode), reserve_bytes);
 		if (ret) {
 			btrfs_drop_pages(pages, num_pages);
@@ -2600,7 +2599,7 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 		if (ordered)
 			btrfs_put_ordered_extent(ordered);
 		unlock_extent_cached(&BTRFS_I(inode)->io_tree, lockstart,
-				     lockend, &cached_state, GFP_NOFS);
+				     lockend, &cached_state);
 		ret = btrfs_wait_ordered_range(inode, lockstart,
 					       lockend - lockstart + 1);
 		if (ret) {
@@ -2751,7 +2750,7 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 	btrfs_free_block_rsv(fs_info, rsv);
 out:
 	unlock_extent_cached(&BTRFS_I(inode)->io_tree, lockstart, lockend,
-			     &cached_state, GFP_NOFS);
+			     &cached_state);
 out_only_mutex:
 	if (!updated_inode && truncated_block && !ret && !err) {
 		/*
@@ -2913,7 +2912,7 @@ static long btrfs_fallocate(struct file *file, int mode,
 			btrfs_put_ordered_extent(ordered);
 			unlock_extent_cached(&BTRFS_I(inode)->io_tree,
 					     alloc_start, locked_end,
-					     &cached_state, GFP_KERNEL);
+					     &cached_state);
 			/*
 			 * we can't wait on the range with the transaction
 			 * running or with the extent lock held
@@ -3015,7 +3014,7 @@ static long btrfs_fallocate(struct file *file, int mode,
 	}
 out_unlock:
 	unlock_extent_cached(&BTRFS_I(inode)->io_tree, alloc_start, locked_end,
-			     &cached_state, GFP_KERNEL);
+			     &cached_state);
 out:
 	inode_unlock(inode);
 	/* Let go of our reservation. */
@@ -3088,7 +3087,7 @@ static int find_desired_extent(struct inode *inode, loff_t *offset, int whence)
 			*offset = min_t(loff_t, start, inode->i_size);
 	}
 	unlock_extent_cached(&BTRFS_I(inode)->io_tree, lockstart, lockend,
-			     &cached_state, GFP_NOFS);
+			     &cached_state);
 	return ret;
 }
 

commit 343e4fc1c60971b0734de26dbbd475d433950982
Author: Liu Bo <bo.li.liu@oracle.com>
Date:   Wed Nov 15 16:10:28 2017 -0700

    Btrfs: set plug for fsync
    
    Setting plug can merge adjacent IOs before dispatching IOs to the disk
    driver.
    
    Without plug, it'd not be a problem for single disk usecases, but for
    multiple disks using raid profile, a large IO can be split to several
    IOs of stripe length, and plug can be helpful to bring them together
    for each disk so that we can save several disk access.
    
    Moreover, fsync issues synchronous writes, so plug can really take
    effect.
    
    Signed-off-by: Liu Bo <bo.li.liu@oracle.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index b85b6d7d0ccd..1096398e1351 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2019,10 +2019,19 @@ int btrfs_release_file(struct inode *inode, struct file *filp)
 static int start_ordered_ops(struct inode *inode, loff_t start, loff_t end)
 {
 	int ret;
+	struct blk_plug plug;
 
+	/*
+	 * This is only called in fsync, which would do synchronous writes, so
+	 * a plug can merge adjacent IOs as much as possible.  Esp. in case of
+	 * multiple disks using raid profile, a large IO can be split to
+	 * several segments of stripe length (currently 64K).
+	 */
+	blk_start_plug(&plug);
 	atomic_inc(&BTRFS_I(inode)->sync_writers);
 	ret = btrfs_fdatawrite_range(inode, start, end);
 	atomic_dec(&BTRFS_I(inode)->sync_writers);
+	blk_finish_plug(&plug);
 
 	return ret;
 }

commit ae0f162534e98afccc7d055cfaa3d3e920a928f0
Author: David Sterba <dsterba@suse.com>
Date:   Tue Oct 31 16:37:52 2017 +0100

    btrfs: sink gfp parameter to clear_extent_bit
    
    All callers use GFP_NOFS, we don't have to pass it as an argument. The
    built-in tests pass GFP_KERNEL, but they run only at module load time
    and NOFS works there as well.
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index d1eba3394660..b85b6d7d0ccd 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1519,7 +1519,7 @@ lock_and_cleanup_extent_if_need(struct btrfs_inode *inode, struct page **pages,
 		clear_extent_bit(&inode->io_tree, start_pos, last_pos,
 				 EXTENT_DIRTY | EXTENT_DELALLOC |
 				 EXTENT_DO_ACCOUNTING | EXTENT_DEFRAG,
-				 0, 0, cached_state, GFP_NOFS);
+				 0, 0, cached_state);
 		*lockstart = start_pos;
 		*lockend = last_pos;
 		ret = 1;

commit f5c29bd9dbd3e90e03ab7697ecc373b49394e62e
Author: Liu Bo <bo.li.liu@oracle.com>
Date:   Thu Nov 2 17:21:50 2017 -0600

    Btrfs: add __init macro to btrfs init functions
    
    Adding __init macro gives kernel a hint that this function is only used
    during the initialization phase and its memory resources can be freed up
    after.
    
    Signed-off-by: Liu Bo <bo.li.liu@oracle.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 559d716221df..d1eba3394660 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -3143,7 +3143,7 @@ void btrfs_auto_defrag_exit(void)
 	kmem_cache_destroy(btrfs_inode_defrag_cachep);
 }
 
-int btrfs_auto_defrag_init(void)
+int __init btrfs_auto_defrag_init(void)
 {
 	btrfs_inode_defrag_cachep = kmem_cache_create("btrfs_inode_defrag",
 					sizeof(struct inode_defrag), 0,

commit 96b09dde92515956c992c1b330f00399487f47b3
Author: Nikolay Borisov <nborisov@suse.com>
Date:   Wed Nov 1 11:36:05 2017 +0200

    btrfs: Use locked_end rather than open coding it
    
    Right before we go into this loop locked_end is set to alloc_end - 1 and
    is being used in nearby functions, no need to have exceptions. This just
    makes the code consistent, no functional changes.
    
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 89fb9eff714f..559d716221df 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2896,8 +2896,8 @@ static long btrfs_fallocate(struct file *file, int mode,
 		 */
 		lock_extent_bits(&BTRFS_I(inode)->io_tree, alloc_start,
 				 locked_end, &cached_state);
-		ordered = btrfs_lookup_first_ordered_extent(inode,
-							    alloc_end - 1);
+		ordered = btrfs_lookup_first_ordered_extent(inode, locked_end);
+
 		if (ordered &&
 		    ordered->file_offset + ordered->len > alloc_start &&
 		    ordered->file_offset < alloc_end) {

commit 6b7d6e933433a43062ce9355a4126b59bd2519e3
Author: Nikolay Borisov <nborisov@suse.com>
Date:   Wed Nov 1 11:32:18 2017 +0200

    btrfs: Move loop termination condition in while()
    
    Fallocating a file in btrfs goes through several stages. The one before
    actually inserting the fallocated extents is to create a qgroup
    reservation, covering the desired range. To this end there is a loop in
    btrfs_fallocate which checks to see if there are holes in the fallocated
    range or !PREALLOC extents past EOF and if so create qgroup reservations
    for them. Unfortunately, the main condition of the loop is burried right
    at the end of its body rather than in the actual while statement which
    makes it non-obvious. Fix this by moving the condition in the while
    statement where it belongs. No functional changes.
    
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index eb1bac7c8553..89fb9eff714f 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2922,7 +2922,7 @@ static long btrfs_fallocate(struct file *file, int mode,
 
 	/* First, check if we exceed the qgroup limit */
 	INIT_LIST_HEAD(&reserve_list);
-	while (1) {
+	while (cur_offset < alloc_end) {
 		em = btrfs_get_extent(BTRFS_I(inode), NULL, 0, cur_offset,
 				      alloc_end - cur_offset, 0);
 		if (IS_ERR(em)) {
@@ -2958,8 +2958,6 @@ static long btrfs_fallocate(struct file *file, int mode,
 		}
 		free_extent_map(em);
 		cur_offset = last_byte;
-		if (cur_offset >= alloc_end)
-			break;
 	}
 
 	/*

commit ebb70442cdd4872260c2415929c456be3562da82
Author: Liu Bo <bo.li.liu@oracle.com>
Date:   Tue Nov 21 14:35:40 2017 -0700

    Btrfs: fix list_add corruption and soft lockups in fsync
    
    Xfstests btrfs/146 revealed this corruption,
    
    [   58.138831] Buffer I/O error on dev dm-0, logical block 2621424, async page read
    [   58.151233] BTRFS error (device sdf): bdev /dev/mapper/error-test errs: wr 1, rd 0, flush 0, corrupt 0, gen 0
    [   58.152403] list_add corruption. prev->next should be next (ffff88005e6775d8), but was ffffc9000189be88. (prev=ffffc9000189be88).
    [   58.153518] ------------[ cut here ]------------
    [   58.153892] WARNING: CPU: 1 PID: 1287 at lib/list_debug.c:31 __list_add_valid+0x169/0x1f0
    ...
    [   58.157379] RIP: 0010:__list_add_valid+0x169/0x1f0
    ...
    [   58.161956] Call Trace:
    [   58.162264]  btrfs_log_inode_parent+0x5bd/0xfb0 [btrfs]
    [   58.163583]  btrfs_log_dentry_safe+0x60/0x80 [btrfs]
    [   58.164003]  btrfs_sync_file+0x4c2/0x6f0 [btrfs]
    [   58.164393]  vfs_fsync_range+0x5f/0xd0
    [   58.164898]  do_fsync+0x5a/0x90
    [   58.165170]  SyS_fsync+0x10/0x20
    [   58.165395]  entry_SYSCALL_64_fastpath+0x1f/0xbe
    ...
    
    It turns out that we could record btrfs_log_ctx:io_err in
    log_one_extents when IO fails, but make log_one_extents() return '0'
    instead of -EIO, so the IO error is not acknowledged by the callers,
    i.e.  btrfs_log_inode_parent(), which would remove btrfs_log_ctx:list
    from list head 'root->log_ctxs'.  Since btrfs_log_ctx is allocated
    from stack memory, it'd get freed with a object alive on the
    list. then a future list_add will throw the above warning.
    
    This returns the correct error in the above case.
    
    Jeff also reported this while testing against his fsync error
    patch set[1].
    
    [1]: https://www.spinics.net/lists/linux-btrfs/msg65308.html
    "btrfs list corruption and soft lockups while testing writeback error handling"
    
    Fixes: 8407f553268a4611f254 ("Btrfs: fix data corruption after fast fsync and writeback error")
    Signed-off-by: Liu Bo <bo.li.liu@oracle.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 2de7b4f4ca3c..eb1bac7c8553 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2057,6 +2057,8 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 	len = (u64)end - (u64)start + 1;
 	trace_btrfs_sync_file(file, datasync);
 
+	btrfs_init_log_ctx(&ctx, inode);
+
 	/*
 	 * We write the dirty pages in the range and wait until they complete
 	 * out of the ->i_mutex. If so, we can flush the dirty pages by
@@ -2203,8 +2205,6 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 	}
 	trans->sync = true;
 
-	btrfs_init_log_ctx(&ctx, inode);
-
 	ret = btrfs_log_dentry_safe(trans, root, dentry, start, end, &ctx);
 	if (ret < 0) {
 		/* Fallthrough and commit/free transaction. */
@@ -2262,6 +2262,7 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 		ret = btrfs_end_transaction(trans);
 	}
 out:
+	ASSERT(list_empty(&ctx.list));
 	err = file_check_and_advance_wb_err(file);
 	if (!ret)
 		ret = err;

commit e3b8a4858566a6cc25422fbfdfdd760b13b79280
Author: Filipe Manana <fdmanana@suse.com>
Date:   Sat Nov 4 00:16:59 2017 +0000

    Btrfs: fix reported number of inode blocks after buffered append writes
    
    The patch from commit a7e3b975a0f9 ("Btrfs: fix reported number of inode
    blocks") introduced a regression where if we do a buffered write starting
    at position equal to or greater than the file's size and then stat(2) the
    file before writeback is triggered, the number of used blocks does not
    change (unless there's a prealloc/unwritten extent). Example:
    
      $ xfs_io -f -c "pwrite -S 0xab 0 64K" foobar
      $ du -h foobar
      0     foobar
      $ sync
      $ du -h foobar
      64K   foobar
    
    The first version of that patch didn't had this regression and the second
    version, which was the one committed, was made only to address some
    performance regression detected by the intel test robots using fs_mark.
    
    This fixes the regression by setting the new delaloc bit in the range, and
    doing it at btrfs_dirty_pages() while setting the regular dealloc bit as
    well, so that this way we set both bits at once avoiding navigation of the
    inode's io tree twice. Doing it at btrfs_dirty_pages() is also the most
    meaninful place, as we should set the new dellaloc bit when if we set the
    delalloc bit, which happens only if we copied bytes into the pages at
    __btrfs_buffered_write().
    
    This was making some of LTP's du tests fail, which can be quickly run
    using a command line like the following:
    
      $ ./runltp -q -p -l /ltp.log -f commands -s du -d /mnt
    
    Fixes: a7e3b975a0f9 ("Btrfs: fix reported number of inode blocks")
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 47e6ebad78c3..2de7b4f4ca3c 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -538,14 +538,34 @@ int btrfs_dirty_pages(struct inode *inode, struct page **pages,
 	u64 end_of_last_block;
 	u64 end_pos = pos + write_bytes;
 	loff_t isize = i_size_read(inode);
+	unsigned int extra_bits = 0;
 
 	start_pos = pos & ~((u64) fs_info->sectorsize - 1);
 	num_bytes = round_up(write_bytes + pos - start_pos,
 			     fs_info->sectorsize);
 
 	end_of_last_block = start_pos + num_bytes - 1;
+
+	if (!btrfs_is_free_space_inode(BTRFS_I(inode))) {
+		if (start_pos >= isize &&
+		    !(BTRFS_I(inode)->flags & BTRFS_INODE_PREALLOC)) {
+			/*
+			 * There can't be any extents following eof in this case
+			 * so just set the delalloc new bit for the range
+			 * directly.
+			 */
+			extra_bits |= EXTENT_DELALLOC_NEW;
+		} else {
+			err = btrfs_find_new_delalloc_bytes(BTRFS_I(inode),
+							    start_pos,
+							    num_bytes, cached);
+			if (err)
+				return err;
+		}
+	}
+
 	err = btrfs_set_extent_delalloc(inode, start_pos, end_of_last_block,
-					cached, 0);
+					extra_bits, cached, 0);
 	if (err)
 		return err;
 
@@ -1473,10 +1493,8 @@ lock_and_cleanup_extent_if_need(struct btrfs_inode *inode, struct page **pages,
 		+ round_up(pos + write_bytes - start_pos,
 			   fs_info->sectorsize) - 1;
 
-	if (start_pos < inode->vfs_inode.i_size ||
-	    (inode->flags & BTRFS_INODE_PREALLOC)) {
+	if (start_pos < inode->vfs_inode.i_size) {
 		struct btrfs_ordered_extent *ordered;
-		unsigned int clear_bits;
 
 		lock_extent_bits(&inode->io_tree, start_pos, last_pos,
 				cached_state);
@@ -1498,19 +1516,10 @@ lock_and_cleanup_extent_if_need(struct btrfs_inode *inode, struct page **pages,
 		}
 		if (ordered)
 			btrfs_put_ordered_extent(ordered);
-		ret = btrfs_find_new_delalloc_bytes(inode, start_pos,
-						    last_pos - start_pos + 1,
-						    cached_state);
-		clear_bits = EXTENT_DIRTY | EXTENT_DELALLOC |
-			EXTENT_DO_ACCOUNTING | EXTENT_DEFRAG;
-		if (ret)
-			clear_bits |= EXTENT_DELALLOC_NEW | EXTENT_LOCKED;
-		clear_extent_bit(&inode->io_tree, start_pos,
-				 last_pos, clear_bits,
-				 (clear_bits & EXTENT_LOCKED) ? 1 : 0,
-				 0, cached_state, GFP_NOFS);
-		if (ret)
-			return ret;
+		clear_extent_bit(&inode->io_tree, start_pos, last_pos,
+				 EXTENT_DIRTY | EXTENT_DELALLOC |
+				 EXTENT_DO_ACCOUNTING | EXTENT_DEFRAG,
+				 0, 0, cached_state, GFP_NOFS);
 		*lockstart = start_pos;
 		*lockend = last_pos;
 		ret = 1;

commit f48bf66b662e7acd6a32dbc28c4fa38931f8f0a6
Author: Filipe Manana <fdmanana@suse.com>
Date:   Fri Nov 3 22:26:44 2017 +0000

    Btrfs: move definition of the function btrfs_find_new_delalloc_bytes
    
    Move the definition of the function btrfs_find_new_delalloc_bytes() closer
    to the function btrfs_dirty_pages(), because in a future commit it will be
    used exclusively by btrfs_dirty_pages(). This just moves the function's
    definition, with no functional changes at all.
    
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index f80254d82f40..47e6ebad78c3 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -477,6 +477,47 @@ static void btrfs_drop_pages(struct page **pages, size_t num_pages)
 	}
 }
 
+static int btrfs_find_new_delalloc_bytes(struct btrfs_inode *inode,
+					 const u64 start,
+					 const u64 len,
+					 struct extent_state **cached_state)
+{
+	u64 search_start = start;
+	const u64 end = start + len - 1;
+
+	while (search_start < end) {
+		const u64 search_len = end - search_start + 1;
+		struct extent_map *em;
+		u64 em_len;
+		int ret = 0;
+
+		em = btrfs_get_extent(inode, NULL, 0, search_start,
+				      search_len, 0);
+		if (IS_ERR(em))
+			return PTR_ERR(em);
+
+		if (em->block_start != EXTENT_MAP_HOLE)
+			goto next;
+
+		em_len = em->len;
+		if (em->start < search_start)
+			em_len -= search_start - em->start;
+		if (em_len > search_len)
+			em_len = search_len;
+
+		ret = set_extent_bit(&inode->io_tree, search_start,
+				     search_start + em_len - 1,
+				     EXTENT_DELALLOC_NEW,
+				     NULL, cached_state, GFP_NOFS);
+next:
+		search_start = extent_map_end(em);
+		free_extent_map(em);
+		if (ret)
+			return ret;
+	}
+	return 0;
+}
+
 /*
  * after copy_from_user, pages need to be dirtied and we need to make
  * sure holes are created between the current EOF and the start of
@@ -1404,47 +1445,6 @@ static noinline int prepare_pages(struct inode *inode, struct page **pages,
 
 }
 
-static int btrfs_find_new_delalloc_bytes(struct btrfs_inode *inode,
-					 const u64 start,
-					 const u64 len,
-					 struct extent_state **cached_state)
-{
-	u64 search_start = start;
-	const u64 end = start + len - 1;
-
-	while (search_start < end) {
-		const u64 search_len = end - search_start + 1;
-		struct extent_map *em;
-		u64 em_len;
-		int ret = 0;
-
-		em = btrfs_get_extent(inode, NULL, 0, search_start,
-				      search_len, 0);
-		if (IS_ERR(em))
-			return PTR_ERR(em);
-
-		if (em->block_start != EXTENT_MAP_HOLE)
-			goto next;
-
-		em_len = em->len;
-		if (em->start < search_start)
-			em_len -= search_start - em->start;
-		if (em_len > search_len)
-			em_len = search_len;
-
-		ret = set_extent_bit(&inode->io_tree, search_start,
-				     search_start + em_len - 1,
-				     EXTENT_DELALLOC_NEW,
-				     NULL, cached_state, GFP_NOFS);
-next:
-		search_start = extent_map_end(em);
-		free_extent_map(em);
-		if (ret)
-			return ret;
-	}
-	return 0;
-}
-
 /*
  * This function locks the extent and properly waits for data=ordered extents
  * to finish before allowing the pages to be modified if need.

commit 8b62f87bad9cf06e536799bf8cb942ab95f6bfa4
Author: Josef Bacik <josef@toxicpanda.com>
Date:   Thu Oct 19 14:15:55 2017 -0400

    Btrfs: rework outstanding_extents
    
    Right now we do a lot of weird hoops around outstanding_extents in order
    to keep the extent count consistent.  This is because we logically
    transfer the outstanding_extent count from the initial reservation
    through the set_delalloc_bits.  This makes it pretty difficult to get a
    handle on how and when we need to mess with outstanding_extents.
    
    Fix this by revamping the rules of how we deal with outstanding_extents.
    Now instead everybody that is holding on to a delalloc extent is
    required to increase the outstanding extents count for itself.  This
    means we'll have something like this
    
    btrfs_delalloc_reserve_metadata - outstanding_extents = 1
     btrfs_set_extent_delalloc      - outstanding_extents = 2
    btrfs_release_delalloc_extents  - outstanding_extents = 1
    
    for an initial file write.  Now take the append write where we extend an
    existing delalloc range but still under the maximum extent size
    
    btrfs_delalloc_reserve_metadata - outstanding_extents = 2
      btrfs_set_extent_delalloc
        btrfs_set_bit_hook          - outstanding_extents = 3
        btrfs_merge_extent_hook     - outstanding_extents = 2
    btrfs_delalloc_release_extents  - outstanding_extnets = 1
    
    In order to make the ordered extent transition we of course must now
    make ordered extents carry their own outstanding_extent reservation, so
    for cow_file_range we end up with
    
    btrfs_add_ordered_extent        - outstanding_extents = 2
    clear_extent_bit                - outstanding_extents = 1
    btrfs_remove_ordered_extent     - outstanding_extents = 0
    
    This makes all manipulations of outstanding_extents much more explicit.
    Every successful call to btrfs_delalloc_reserve_metadata _must_ now be
    combined with btrfs_release_delalloc_extents, even in the error case, as
    that is the only function that actually modifies the
    outstanding_extents counter.
    
    The drawback to this is now we are much more likely to have transient
    cases where outstanding_extents is much larger than it actually should
    be.  This could happen before as we manipulated the delalloc bits, but
    now it happens basically at every write.  This may put more pressure on
    the ENOSPC flushing code, but I think making this code simpler is worth
    the cost.  I have another change coming to mitigate this side-effect
    somewhat.
    
    I also added trace points for the counter manipulation.  These were used
    by a bpf script I wrote to help track down leak issues.
    
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 4de174b664ff..f80254d82f40 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1656,6 +1656,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 			}
 		}
 
+		WARN_ON(reserve_bytes == 0);
 		ret = btrfs_delalloc_reserve_metadata(BTRFS_I(inode),
 				reserve_bytes);
 		if (ret) {
@@ -1678,8 +1679,11 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 		ret = prepare_pages(inode, pages, num_pages,
 				    pos, write_bytes,
 				    force_page_uptodate);
-		if (ret)
+		if (ret) {
+			btrfs_delalloc_release_extents(BTRFS_I(inode),
+						       reserve_bytes);
 			break;
+		}
 
 		extents_locked = lock_and_cleanup_extent_if_need(
 				BTRFS_I(inode), pages,
@@ -1688,6 +1692,8 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 		if (extents_locked < 0) {
 			if (extents_locked == -EAGAIN)
 				goto again;
+			btrfs_delalloc_release_extents(BTRFS_I(inode),
+						       reserve_bytes);
 			ret = extents_locked;
 			break;
 		}
@@ -1716,23 +1722,10 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 						   PAGE_SIZE);
 		}
 
-		/*
-		 * If we had a short copy we need to release the excess delaloc
-		 * bytes we reserved.  We need to increment outstanding_extents
-		 * because btrfs_delalloc_release_space and
-		 * btrfs_delalloc_release_metadata will decrement it, but
-		 * we still have an outstanding extent for the chunk we actually
-		 * managed to copy.
-		 */
 		if (num_sectors > dirty_sectors) {
 			/* release everything except the sectors we dirtied */
 			release_bytes -= dirty_sectors <<
 						fs_info->sb->s_blocksize_bits;
-			if (copied > 0) {
-				spin_lock(&BTRFS_I(inode)->lock);
-				BTRFS_I(inode)->outstanding_extents++;
-				spin_unlock(&BTRFS_I(inode)->lock);
-			}
 			if (only_release_metadata) {
 				btrfs_delalloc_release_metadata(BTRFS_I(inode),
 								release_bytes);
@@ -1758,6 +1751,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 			unlock_extent_cached(&BTRFS_I(inode)->io_tree,
 					     lockstart, lockend, &cached_state,
 					     GFP_NOFS);
+		btrfs_delalloc_release_extents(BTRFS_I(inode), reserve_bytes);
 		if (ret) {
 			btrfs_drop_pages(pages, num_pages);
 			break;

commit 79f015f216539dfdf0d3ef0b3bbb9dc7754364cd
Author: Goldwyn Rodrigues <rgoldwyn@suse.com>
Date:   Mon Oct 16 05:43:21 2017 -0500

    btrfs: cleanup extent locking sequence
    
    Code cleanup for better understanding:
    Variable needs_unlock to be called extent_locked to show state as
    opposed to action. Changed the type to int, to reduce code in the
    critical path.
    
    Signed-off-by: Goldwyn Rodrigues <rgoldwyn@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index d3c1725f03e4..4de174b664ff 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1590,7 +1590,6 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 	int ret = 0;
 	bool only_release_metadata = false;
 	bool force_page_uptodate = false;
-	bool need_unlock;
 
 	nrptrs = min(DIV_ROUND_UP(iov_iter_count(i), PAGE_SIZE),
 			PAGE_SIZE / (sizeof(struct page *)));
@@ -1613,6 +1612,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 		size_t copied;
 		size_t dirty_sectors;
 		size_t num_sectors;
+		int extents_locked;
 
 		WARN_ON(num_pages > nrptrs);
 
@@ -1669,7 +1669,6 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 		}
 
 		release_bytes = reserve_bytes;
-		need_unlock = false;
 again:
 		/*
 		 * This is going to setup the pages array with the number of
@@ -1682,16 +1681,15 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 		if (ret)
 			break;
 
-		ret = lock_and_cleanup_extent_if_need(BTRFS_I(inode), pages,
+		extents_locked = lock_and_cleanup_extent_if_need(
+				BTRFS_I(inode), pages,
 				num_pages, pos, write_bytes, &lockstart,
 				&lockend, &cached_state);
-		if (ret < 0) {
-			if (ret == -EAGAIN)
+		if (extents_locked < 0) {
+			if (extents_locked == -EAGAIN)
 				goto again;
+			ret = extents_locked;
 			break;
-		} else if (ret > 0) {
-			need_unlock = true;
-			ret = 0;
 		}
 
 		copied = btrfs_copy_from_user(pos, write_bytes, pages, i);
@@ -1756,7 +1754,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 		if (copied > 0)
 			ret = btrfs_dirty_pages(inode, pages, dirty_pages,
 						pos, copied, NULL);
-		if (need_unlock)
+		if (extents_locked)
 			unlock_extent_cached(&BTRFS_I(inode)->io_tree,
 					     lockstart, lockend, &cached_state,
 					     GFP_NOFS);

commit 84f7d8e6242ceb377c7af10a7133c653cc7fea5f
Author: Josef Bacik <josef@toxicpanda.com>
Date:   Fri Sep 29 15:43:49 2017 -0400

    btrfs: pass root to various extent ref mod functions
    
    We need the actual root for the ref verifier tool to work, so change
    these functions to pass the root around instead.  This will be used in
    a subsequent patch.
    
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 79945053c7e7..d3c1725f03e4 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -856,7 +856,7 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 			btrfs_mark_buffer_dirty(leaf);
 
 			if (update_refs && disk_bytenr > 0) {
-				ret = btrfs_inc_extent_ref(trans, fs_info,
+				ret = btrfs_inc_extent_ref(trans, root,
 						disk_bytenr, num_bytes, 0,
 						root->root_key.objectid,
 						new_key.objectid,
@@ -940,7 +940,7 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 				extent_end = ALIGN(extent_end,
 						   fs_info->sectorsize);
 			} else if (update_refs && disk_bytenr > 0) {
-				ret = btrfs_free_extent(trans, fs_info,
+				ret = btrfs_free_extent(trans, root,
 						disk_bytenr, num_bytes, 0,
 						root->root_key.objectid,
 						key.objectid, key.offset -
@@ -1234,7 +1234,7 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 						extent_end - split);
 		btrfs_mark_buffer_dirty(leaf);
 
-		ret = btrfs_inc_extent_ref(trans, fs_info, bytenr, num_bytes,
+		ret = btrfs_inc_extent_ref(trans, root, bytenr, num_bytes,
 					   0, root->root_key.objectid,
 					   ino, orig_offset);
 		if (ret) {
@@ -1268,7 +1268,7 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 		extent_end = other_end;
 		del_slot = path->slots[0] + 1;
 		del_nr++;
-		ret = btrfs_free_extent(trans, fs_info, bytenr, num_bytes,
+		ret = btrfs_free_extent(trans, root, bytenr, num_bytes,
 					0, root->root_key.objectid,
 					ino, orig_offset);
 		if (ret) {
@@ -1288,7 +1288,7 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 		key.offset = other_start;
 		del_slot = path->slots[0];
 		del_nr++;
-		ret = btrfs_free_extent(trans, fs_info, bytenr, num_bytes,
+		ret = btrfs_free_extent(trans, root, bytenr, num_bytes,
 					0, root->root_key.objectid,
 					ino, orig_offset);
 		if (ret) {

commit 897ca8194cd1b287bc5e7d8a5edc2b9a041e15ba
Author: Thomas Meyer <thomas@m3y3r.de>
Date:   Sat Oct 7 16:02:21 2017 +0200

    btrfs: Fix bool initialization/comparison
    
    Bool initializations should use true and false. Bool tests don't need
    comparisons.
    
    Signed-off-by: Thomas Meyer <thomas@m3y3r.de>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index aafcc785f840..79945053c7e7 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2046,7 +2046,7 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 	struct btrfs_trans_handle *trans;
 	struct btrfs_log_ctx ctx;
 	int ret = 0, err;
-	bool full_sync = 0;
+	bool full_sync = false;
 	u64 len;
 
 	/*

commit e253d98f5babbec7e6ced810f7335b265a7f7e83
Merge: 0f0d12728e56 c35fc7a5abae
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Sep 14 19:29:55 2017 -0700

    Merge branch 'work.read_write' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs
    
    Pull nowait read support from Al Viro:
     "Support IOCB_NOWAIT for buffered reads and block devices"
    
    * 'work.read_write' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs:
      block_dev: support RFW_NOWAIT on block device nodes
      fs: support RWF_NOWAIT for buffered reads
      fs: support IOCB_NOWAIT in generic_file_buffered_read
      fs: pass iocb to do_generic_file_read

commit 91f9943e1c7b6638f27312d03fe71fcc67b23571
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Aug 29 16:13:20 2017 +0200

    fs: support RWF_NOWAIT for buffered reads
    
    This is based on the old idea and code from Milosz Tanski.  With the aio
    nowait code it becomes mostly trivial now.  Buffered writes continue to
    return -EOPNOTSUPP if RWF_NOWAIT is passed.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 9e75d8a39aac..e62dd55b4079 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1886,6 +1886,10 @@ static ssize_t btrfs_file_write_iter(struct kiocb *iocb,
 	loff_t oldsize;
 	int clean_page = 0;
 
+	if (!(iocb->ki_flags & IOCB_DIRECT) &&
+	    (iocb->ki_flags & IOCB_NOWAIT))
+		return -EOPNOTSUPP;
+
 	if (!inode_trylock(inode)) {
 		if (iocb->ki_flags & IOCB_NOWAIT)
 			return -EAGAIN;
@@ -3105,7 +3109,7 @@ static loff_t btrfs_file_llseek(struct file *file, loff_t offset, int whence)
 
 static int btrfs_file_open(struct inode *inode, struct file *filp)
 {
-	filp->f_mode |= FMODE_AIO_NOWAIT;
+	filp->f_mode |= FMODE_NOWAIT;
 	return generic_file_open(inode, filp);
 }
 

commit 23b5ec74943f44378b68c0edd8e210a86318ea5e
Author: Josef Bacik <jbacik@fb.com>
Date:   Mon Jul 24 15:14:25 2017 -0400

    btrfs: fix readdir deadlock with pagefault
    
    Readdir does dir_emit while under the btree lock.  dir_emit can trigger
    the page fault which means we can deadlock.  Fix this by allocating a
    buffer on opening a directory and copying the readdir into this buffer
    and doing dir_emit from outside of the tree lock.
    
    Thread A
    readdir  <holding tree lock>
      dir_emit
        <page fault>
          down_read(mmap_sem)
    
    Thread B
    mmap write
      down_write(mmap_sem)
        page_mkwrite
          wait_ordered_extents
    
    Process C
    finish_ordered_extent
      insert_reserved_file_extent
       try to lock leaf <hang>
    
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    [ copy the deadlock scenario to changelog ]
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 58818cf7f82d..74fd7756cff3 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1990,8 +1990,15 @@ static ssize_t btrfs_file_write_iter(struct kiocb *iocb,
 
 int btrfs_release_file(struct inode *inode, struct file *filp)
 {
-	if (filp->private_data)
+	struct btrfs_file_private *private = filp->private_data;
+
+	if (private && private->trans)
 		btrfs_ioctl_trans_end(filp);
+	if (private && private->filldir_buf)
+		kfree(private->filldir_buf);
+	kfree(private);
+	filp->private_data = NULL;
+
 	/*
 	 * ordered_data_close is set by settattr when we are about to truncate
 	 * a file from a non-zero size to a zero size.  This tries to

commit ea14b57fd1954fa3193e025224bbbeab7415c490
Author: David Sterba <dsterba@suse.com>
Date:   Thu Jun 22 02:19:11 2017 +0200

    btrfs: fix spelling of snapshotting
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 9e75d8a39aac..58818cf7f82d 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1536,7 +1536,7 @@ static noinline int check_can_nocow(struct btrfs_inode *inode, loff_t pos,
 	u64 num_bytes;
 	int ret;
 
-	ret = btrfs_start_write_no_snapshoting(root);
+	ret = btrfs_start_write_no_snapshotting(root);
 	if (!ret)
 		return -ENOSPC;
 
@@ -1561,7 +1561,7 @@ static noinline int check_can_nocow(struct btrfs_inode *inode, loff_t pos,
 			NULL, NULL, NULL);
 	if (ret <= 0) {
 		ret = 0;
-		btrfs_end_write_no_snapshoting(root);
+		btrfs_end_write_no_snapshotting(root);
 	} else {
 		*write_bytes = min_t(size_t, *write_bytes ,
 				     num_bytes - pos + lockstart);
@@ -1664,7 +1664,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 						data_reserved, pos,
 						write_bytes);
 			else
-				btrfs_end_write_no_snapshoting(root);
+				btrfs_end_write_no_snapshotting(root);
 			break;
 		}
 
@@ -1767,7 +1767,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 
 		release_bytes = 0;
 		if (only_release_metadata)
-			btrfs_end_write_no_snapshoting(root);
+			btrfs_end_write_no_snapshotting(root);
 
 		if (only_release_metadata && copied > 0) {
 			lockstart = round_down(pos,
@@ -1797,7 +1797,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 
 	if (release_bytes) {
 		if (only_release_metadata) {
-			btrfs_end_write_no_snapshoting(root);
+			btrfs_end_write_no_snapshotting(root);
 			btrfs_delalloc_release_metadata(BTRFS_I(inode),
 					release_bytes);
 		} else {

commit 6618a24ab2313309e9df1cc06cc1f6786a6b6a9c
Merge: 1d07b6cb96bc ff0fa73247e4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 10 10:27:48 2017 -0700

    Merge branch 'nowait-aio-btrfs-fixup' of git://git.kernel.org/pub/scm/linux/kernel/git/kdave/linux
    
    Pull btrfs fix from David Sterba:
     "This fixes a user-visible bug introduced by the nowait-aio patches
      merged in this cycle"
    
    * 'nowait-aio-btrfs-fixup' of git://git.kernel.org/pub/scm/linux/kernel/git/kdave/linux:
      btrfs: nowait aio: Correct assignment of pos

commit ff0fa73247e442518936baa43c3f037b17f10fa7
Author: Goldwyn Rodrigues <rgoldwyn@suse.com>
Date:   Tue Jul 4 22:33:07 2017 -0500

    btrfs: nowait aio: Correct assignment of pos
    
    Assigning pos for usage early messes up in append mode, where the pos is
    re-assigned in generic_write_checks(). Assign pos later to get the
    correct position to write from iocb->ki_pos.
    
    Since check_can_nocow also uses the value of pos, we shift
    generic_write_checks() before check_can_nocow(). Checks with IOCB_DIRECT
    are present in generic_write_checks(), so checking for IOCB_NOWAIT is
    enough.
    
    Also, put locking sequence in the fast path.
    
    This fixes a user visible bug, as reported:
    
    "apparently breaks several shell related features on my system.
    In zsh history stopped working, because no new entries are added
    anymore.
    I fist noticed the issue when I tried to build mplayer. It uses a shell
    script to generate a help_mp.h file:
    [...]
    
    Here is a simple testcase:
    
     % echo "foo" >> test
     % echo "foo" >> test
     % cat test
     foo
     %
    "
    
    Fixes: edf064e7c6fe ("btrfs: nowait aio support")
    CC: Jens Axboe <axboe@kernel.dk>
    Reported-by: Markus Trippelsdorf <markus@trippelsdorf.de>
    Link: https://lkml.kernel.org/r/20170704042306.GA274@x4
    Signed-off-by: Goldwyn Rodrigues <rgoldwyn@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 59e2dccdf75b..ad53832838b5 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1875,16 +1875,25 @@ static ssize_t btrfs_file_write_iter(struct kiocb *iocb,
 	ssize_t num_written = 0;
 	bool sync = (file->f_flags & O_DSYNC) || IS_SYNC(file->f_mapping->host);
 	ssize_t err;
-	loff_t pos = iocb->ki_pos;
+	loff_t pos;
 	size_t count = iov_iter_count(from);
 	loff_t oldsize;
 	int clean_page = 0;
 
-	if ((iocb->ki_flags & IOCB_NOWAIT) &&
-			(iocb->ki_flags & IOCB_DIRECT)) {
-		/* Don't sleep on inode rwsem */
-		if (!inode_trylock(inode))
+	if (!inode_trylock(inode)) {
+		if (iocb->ki_flags & IOCB_NOWAIT)
 			return -EAGAIN;
+		inode_lock(inode);
+	}
+
+	err = generic_write_checks(iocb, from);
+	if (err <= 0) {
+		inode_unlock(inode);
+		return err;
+	}
+
+	pos = iocb->ki_pos;
+	if (iocb->ki_flags & IOCB_NOWAIT) {
 		/*
 		 * We will allocate space in case nodatacow is not set,
 		 * so bail
@@ -1895,13 +1904,6 @@ static ssize_t btrfs_file_write_iter(struct kiocb *iocb,
 			inode_unlock(inode);
 			return -EAGAIN;
 		}
-	} else
-		inode_lock(inode);
-
-	err = generic_write_checks(iocb, from);
-	if (err <= 0) {
-		inode_unlock(inode);
-		return err;
 	}
 
 	current->backing_dev_info = inode_to_bdi(inode);

commit 088737f44bbf6378745f5b57b035e57ee3dc4750
Merge: 33198c165b7a 333427a505be
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jul 7 19:38:17 2017 -0700

    Merge tag 'for-linus-v4.13-2' of git://git.kernel.org/pub/scm/linux/kernel/git/jlayton/linux
    
    Pull Writeback error handling updates from Jeff Layton:
     "This pile represents the bulk of the writeback error handling fixes
      that I have for this cycle. Some of the earlier patches in this pile
      may look trivial but they are prerequisites for later patches in the
      series.
    
      The aim of this set is to improve how we track and report writeback
      errors to userland. Most applications that care about data integrity
      will periodically call fsync/fdatasync/msync to ensure that their
      writes have made it to the backing store.
    
      For a very long time, we have tracked writeback errors using two flags
      in the address_space: AS_EIO and AS_ENOSPC. Those flags are set when a
      writeback error occurs (via mapping_set_error) and are cleared as a
      side-effect of filemap_check_errors (as you noted yesterday). This
      model really sucks for userland.
    
      Only the first task to call fsync (or msync or fdatasync) will see the
      error. Any subsequent task calling fsync on a file will get back 0
      (unless another writeback error occurs in the interim). If I have
      several tasks writing to a file and calling fsync to ensure that their
      writes got stored, then I need to have them coordinate with one
      another. That's difficult enough, but in a world of containerized
      setups that coordination may even not be possible.
    
      But wait...it gets worse!
    
      The calls to filemap_check_errors can be buried pretty far down in the
      call stack, and there are internal callers of filemap_write_and_wait
      and the like that also end up clearing those errors. Many of those
      callers ignore the error return from that function or return it to
      userland at nonsensical times (e.g. truncate() or stat()). If I get
      back -EIO on a truncate, there is no reason to think that it was
      because some previous writeback failed, and a subsequent fsync() will
      (incorrectly) return 0.
    
      This pile aims to do three things:
    
       1) ensure that when a writeback error occurs that that error will be
          reported to userland on a subsequent fsync/fdatasync/msync call,
          regardless of what internal callers are doing
    
       2) report writeback errors on all file descriptions that were open at
          the time that the error occurred. This is a user-visible change,
          but I think most applications are written to assume this behavior
          anyway. Those that aren't are unlikely to be hurt by it.
    
       3) document what filesystems should do when there is a writeback
          error. Today, there is very little consistency between them, and a
          lot of cargo-cult copying. We need to make it very clear what
          filesystems should do in this situation.
    
      To achieve this, the set adds a new data type (errseq_t) and then
      builds new writeback error tracking infrastructure around that. Once
      all of that is in place, we change the filesystems to use the new
      infrastructure for reporting wb errors to userland.
    
      Note that this is just the initial foray into cleaning up this mess.
      There is a lot of work remaining here:
    
       1) convert the rest of the filesystems in a similar fashion. Once the
          initial set is in, then I think most other fs' will be fairly
          simple to convert. Hopefully most of those can in via individual
          filesystem trees.
    
       2) convert internal waiters on writeback to use errseq_t for
          detecting errors instead of relying on the AS_* flags. I have some
          draft patches for this for ext4, but they are not quite ready for
          prime time yet.
    
      This was a discussion topic this year at LSF/MM too. If you're
      interested in the gory details, LWN has some good articles about this:
    
          https://lwn.net/Articles/718734/
          https://lwn.net/Articles/724307/"
    
    * tag 'for-linus-v4.13-2' of git://git.kernel.org/pub/scm/linux/kernel/git/jlayton/linux:
      btrfs: minimal conversion to errseq_t writeback error reporting on fsync
      xfs: minimal conversion to errseq_t writeback error reporting
      ext4: use errseq_t based error handling for reporting data writeback errors
      fs: convert __generic_file_fsync to use errseq_t based reporting
      block: convert to errseq_t based writeback error tracking
      dax: set errors in mapping when writeback fails
      Documentation: flesh out the section in vfs.txt on storing and reporting writeback errors
      mm: set both AS_EIO/AS_ENOSPC and errseq_t in mapping_set_error
      fs: new infrastructure for writeback error handling and reporting
      lib: add errseq_t type and infrastructure for handling it
      mm: don't TestClearPageError in __filemap_fdatawait_range
      mm: clear AS_EIO/AS_ENOSPC when writeback initiation fails
      jbd2: don't clear and reset errors after waiting on writeback
      buffer: set errors in mapping at the time that the error occurs
      fs: check for writeback errors after syncing out buffers in generic_file_fsync
      buffer: use mapping_set_error instead of setting the flag
      mm: fix mapping_set_error call in me_pagecache_dirty

commit 333427a505be1e10d8da13427dc0c33ec1976b99
Author: Jeff Layton <jlayton@redhat.com>
Date:   Thu Jul 6 07:02:31 2017 -0400

    btrfs: minimal conversion to errseq_t writeback error reporting on fsync
    
    Just check and advance the errseq_t in the file before returning, and
    use an errseq_t based check for writeback errors.
    
    Other internal callers of filemap_* functions are left as-is.
    
    Signed-off-by: Jeff Layton <jlayton@redhat.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index da1096eb1a40..deeb4799da5c 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2011,7 +2011,7 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 	struct btrfs_root *root = BTRFS_I(inode)->root;
 	struct btrfs_trans_handle *trans;
 	struct btrfs_log_ctx ctx;
-	int ret = 0;
+	int ret = 0, err;
 	bool full_sync = 0;
 	u64 len;
 
@@ -2030,7 +2030,7 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 	 */
 	ret = start_ordered_ops(inode, start, end);
 	if (ret)
-		return ret;
+		goto out;
 
 	inode_lock(inode);
 	atomic_inc(&root->log_batch);
@@ -2135,10 +2135,10 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 		 * An ordered extent might have started before and completed
 		 * already with io errors, in which case the inode was not
 		 * updated and we end up here. So check the inode's mapping
-		 * flags for any errors that might have happened while doing
-		 * writeback of file data.
+		 * for any errors that might have happened since we last
+		 * checked called fsync.
 		 */
-		ret = filemap_check_errors(inode->i_mapping);
+		ret = filemap_check_wb_err(inode->i_mapping, file->f_wb_err);
 		inode_unlock(inode);
 		goto out;
 	}
@@ -2227,6 +2227,9 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 		ret = btrfs_end_transaction(trans);
 	}
 out:
+	err = file_check_and_advance_wb_err(file);
+	if (!ret)
+		ret = err;
 	return ret > 0 ? -EIO : ret;
 }
 

commit 8c27cb3566762613a23c080e3db7d0501af9a787
Merge: 7114f51fcb97 848c23b78faf
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jul 5 16:41:23 2017 -0700

    Merge branch 'for-4.13-part1' of git://git.kernel.org/pub/scm/linux/kernel/git/kdave/linux
    
    Pull btrfs updates from David Sterba:
     "The core updates improve error handling (mostly related to bios), with
      the usual incremental work on the GFP_NOFS (mis)use removal,
      refactoring or cleanups. Except the two top patches, all have been in
      for-next for an extensive amount of time.
    
      User visible changes:
    
       - statx support
    
       - quota override tunable
    
       - improved compression thresholds
    
       - obsoleted mount option alloc_start
    
      Core updates:
    
       - bio-related updates:
           - faster bio cloning
           - no allocation failures
           - preallocated flush bios
    
       - more kvzalloc use, memalloc_nofs protections, GFP_NOFS updates
    
       - prep work for btree_inode removal
    
       - dir-item validation
    
       - qgoup fixes and updates
    
       - cleanups:
           - removed unused struct members, unused code, refactoring
           - argument refactoring (fs_info/root, caller -> callee sink)
           - SEARCH_TREE ioctl docs"
    
    * 'for-4.13-part1' of git://git.kernel.org/pub/scm/linux/kernel/git/kdave/linux: (115 commits)
      btrfs: Remove false alert when fiemap range is smaller than on-disk extent
      btrfs: Don't clear SGID when inheriting ACLs
      btrfs: fix integer overflow in calc_reclaim_items_nr
      btrfs: scrub: fix target device intialization while setting up scrub context
      btrfs: qgroup: Fix qgroup reserved space underflow by only freeing reserved ranges
      btrfs: qgroup: Introduce extent changeset for qgroup reserve functions
      btrfs: qgroup: Fix qgroup reserved space underflow caused by buffered write and quotas being enabled
      btrfs: qgroup: Return actually freed bytes for qgroup release or free data
      btrfs: qgroup: Cleanup btrfs_qgroup_prepare_account_extents function
      btrfs: qgroup: Add quick exit for non-fs extents
      Btrfs: rework delayed ref total_bytes_pinned accounting
      Btrfs: return old and new total ref mods when adding delayed refs
      Btrfs: always account pinned bytes when dropping a tree block ref
      Btrfs: update total_bytes_pinned when pinning down extents
      Btrfs: make BUG_ON() in add_pinned_bytes() an ASSERT()
      Btrfs: make add_pinned_bytes() take an s64 num_bytes instead of u64
      btrfs: fix validation of XATTR_ITEM dir items
      btrfs: Verify dir_item in iterate_object_props
      btrfs: Check name_len before in btrfs_del_root_ref
      btrfs: Check name_len before reading btrfs_get_name
      ...

commit bc42bda22345efdb5d8b578d1b4df2c6eaa85c58
Author: Qu Wenruo <quwenruo@cn.fujitsu.com>
Date:   Mon Feb 27 15:10:39 2017 +0800

    btrfs: qgroup: Fix qgroup reserved space underflow by only freeing reserved ranges
    
    [BUG]
    For the following case, btrfs can underflow qgroup reserved space
    at an error path:
    (Page size 4K, function name without "btrfs_" prefix)
    
             Task A                  |             Task B
    ----------------------------------------------------------------------
    Buffered_write [0, 2K)           |
    |- check_data_free_space()       |
    |  |- qgroup_reserve_data()      |
    |     Range aligned to page      |
    |     range [0, 4K)          <<< |
    |     4K bytes reserved      <<< |
    |- copy pages to page cache      |
                                     | Buffered_write [2K, 4K)
                                     | |- check_data_free_space()
                                     | |  |- qgroup_reserved_data()
                                     | |     Range alinged to page
                                     | |     range [0, 4K)
                                     | |     Already reserved by A <<<
                                     | |     0 bytes reserved      <<<
                                     | |- delalloc_reserve_metadata()
                                     | |  And it *FAILED* (Maybe EQUOTA)
                                     | |- free_reserved_data_space()
                                          |- qgroup_free_data()
                                             Range aligned to page range
                                             [0, 4K)
                                             Freeing 4K
    (Special thanks to Chandan for the detailed report and analyse)
    
    [CAUSE]
    Above Task B is freeing reserved data range [0, 4K) which is actually
    reserved by Task A.
    
    And at writeback time, page dirty by Task A will go through writeback
    routine, which will free 4K reserved data space at file extent insert
    time, causing the qgroup underflow.
    
    [FIX]
    For btrfs_qgroup_free_data(), add @reserved parameter to only free
    data ranges reserved by previous btrfs_qgroup_reserve_data().
    So in above case, Task B will try to free 0 byte, so no underflow.
    
    Reported-by: Chandan Rajendra <chandan@linux.vnet.ibm.com>
    Signed-off-by: Qu Wenruo <quwenruo@cn.fujitsu.com>
    Reviewed-by: Chandan Rajendra <chandan@linux.vnet.ibm.com>
    Tested-by: Chandan Rajendra <chandan@linux.vnet.ibm.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 1b5cce51728b..0f102a1b851f 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1660,8 +1660,9 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 				reserve_bytes);
 		if (ret) {
 			if (!only_release_metadata)
-				btrfs_free_reserved_data_space(inode, pos,
-							       write_bytes);
+				btrfs_free_reserved_data_space(inode,
+						data_reserved, pos,
+						write_bytes);
 			else
 				btrfs_end_write_no_snapshoting(root);
 			break;
@@ -1743,8 +1744,9 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 				__pos = round_down(pos,
 						   fs_info->sectorsize) +
 					(dirty_pages << PAGE_SHIFT);
-				btrfs_delalloc_release_space(inode, __pos,
-							     release_bytes);
+				btrfs_delalloc_release_space(inode,
+						data_reserved, __pos,
+						release_bytes);
 			}
 		}
 
@@ -1799,9 +1801,9 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 			btrfs_delalloc_release_metadata(BTRFS_I(inode),
 					release_bytes);
 		} else {
-			btrfs_delalloc_release_space(inode,
-						round_down(pos, fs_info->sectorsize),
-						release_bytes);
+			btrfs_delalloc_release_space(inode, data_reserved,
+					round_down(pos, fs_info->sectorsize),
+					release_bytes);
 		}
 	}
 
@@ -2918,8 +2920,8 @@ static long btrfs_fallocate(struct file *file, int mode,
 			 * range, free reserved data space first, otherwise
 			 * it'll result in false ENOSPC error.
 			 */
-			btrfs_free_reserved_data_space(inode, cur_offset,
-				last_byte - cur_offset);
+			btrfs_free_reserved_data_space(inode, data_reserved,
+					cur_offset, last_byte - cur_offset);
 		}
 		free_extent_map(em);
 		cur_offset = last_byte;
@@ -2938,8 +2940,9 @@ static long btrfs_fallocate(struct file *file, int mode,
 					range->len, i_blocksize(inode),
 					offset + len, &alloc_hint);
 		else
-			btrfs_free_reserved_data_space(inode, range->start,
-						       range->len);
+			btrfs_free_reserved_data_space(inode,
+					data_reserved, range->start,
+					range->len);
 		list_del(&range->list);
 		kfree(range);
 	}
@@ -2977,8 +2980,8 @@ static long btrfs_fallocate(struct file *file, int mode,
 	inode_unlock(inode);
 	/* Let go of our reservation. */
 	if (ret != 0)
-		btrfs_free_reserved_data_space(inode, alloc_start,
-				       alloc_end - cur_offset);
+		btrfs_free_reserved_data_space(inode, data_reserved,
+				alloc_start, alloc_end - cur_offset);
 	extent_changeset_free(data_reserved);
 	return ret;
 }

commit 364ecf3651e0862152c8b340d7cb3021dc0122c7
Author: Qu Wenruo <quwenruo@cn.fujitsu.com>
Date:   Mon Feb 27 15:10:38 2017 +0800

    btrfs: qgroup: Introduce extent changeset for qgroup reserve functions
    
    Introduce a new parameter, struct extent_changeset for
    btrfs_qgroup_reserved_data() and its callers.
    
    Such extent_changeset was used in btrfs_qgroup_reserve_data() to record
    which range it reserved in current reserve, so it can free it in error
    paths.
    
    The reason we need to export it to callers is, at buffered write error
    path, without knowing what exactly which range we reserved in current
    allocation, we can free space which is not reserved by us.
    
    This will lead to qgroup reserved space underflow.
    
    Reviewed-by: Chandan Rajendra <chandan@linux.vnet.ibm.com>
    Signed-off-by: Qu Wenruo <quwenruo@cn.fujitsu.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 5da85b080368..1b5cce51728b 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1581,6 +1581,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 	struct btrfs_root *root = BTRFS_I(inode)->root;
 	struct page **pages = NULL;
 	struct extent_state *cached_state = NULL;
+	struct extent_changeset *data_reserved = NULL;
 	u64 release_bytes = 0;
 	u64 lockstart;
 	u64 lockend;
@@ -1628,7 +1629,9 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 		reserve_bytes = round_up(write_bytes + sector_offset,
 				fs_info->sectorsize);
 
-		ret = btrfs_check_data_free_space(inode, pos, write_bytes);
+		extent_changeset_release(data_reserved);
+		ret = btrfs_check_data_free_space(inode, &data_reserved, pos,
+						  write_bytes);
 		if (ret < 0) {
 			if ((BTRFS_I(inode)->flags & (BTRFS_INODE_NODATACOW |
 						      BTRFS_INODE_PREALLOC)) &&
@@ -1802,6 +1805,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 		}
 	}
 
+	extent_changeset_free(data_reserved);
 	return num_written ? num_written : ret;
 }
 
@@ -2772,6 +2776,7 @@ static long btrfs_fallocate(struct file *file, int mode,
 {
 	struct inode *inode = file_inode(file);
 	struct extent_state *cached_state = NULL;
+	struct extent_changeset *data_reserved = NULL;
 	struct falloc_range *range;
 	struct falloc_range *tmp;
 	struct list_head reserve_list;
@@ -2901,8 +2906,8 @@ static long btrfs_fallocate(struct file *file, int mode,
 				free_extent_map(em);
 				break;
 			}
-			ret = btrfs_qgroup_reserve_data(inode, cur_offset,
-					last_byte - cur_offset);
+			ret = btrfs_qgroup_reserve_data(inode, &data_reserved,
+					cur_offset, last_byte - cur_offset);
 			if (ret < 0) {
 				free_extent_map(em);
 				break;
@@ -2974,6 +2979,7 @@ static long btrfs_fallocate(struct file *file, int mode,
 	if (ret != 0)
 		btrfs_free_reserved_data_space(inode, alloc_start,
 				       alloc_end - cur_offset);
+	extent_changeset_free(data_reserved);
 	return ret;
 }
 

commit 609805d809733d0c669f21f710bdac308cc63cba
Author: Filipe Manana <fdmanana@suse.com>
Date:   Tue May 30 05:29:09 2017 +0100

    Btrfs: fix invalid extent maps due to hole punching
    
    While punching a hole in a range that is not aligned with the sector size
    (currently the same as the page size) we can end up leaving an extent map
    in memory with a length that is smaller then the sector size or with a
    start offset that is not aligned to the sector size. Both cases are not
    expected and can lead to problems. This issue is easily detected
    after the patch from commit a7e3b975a0f9 ("Btrfs: fix reported number of
    inode blocks"), introduced in kernel 4.12-rc1, in a scenario like the
    following for example:
    
      $ mkfs.btrfs -f /dev/sdb
      $ mount /dev/sdb /mnt
      $ xfs_io -c "pwrite -S 0xaa -b 100K 0 100K" /mnt/foo
      $ xfs_io -c "fpunch 60K 90K" /mnt/foo
      $ xfs_io -c "pwrite -S 0xbb -b 100K 50K 100K" /mnt/foo
      $ xfs_io -c "pwrite -S 0xcc -b 50K 100K 50K" /mnt/foo
      $ umount /mnt
    
    After the unmount operation we can see several warnings emmitted due to
    underflows related to space reservation counters:
    
    [ 2837.443299] ------------[ cut here ]------------
    [ 2837.447395] WARNING: CPU: 8 PID: 2474 at fs/btrfs/inode.c:9444 btrfs_destroy_inode+0xe8/0x27e [btrfs]
    [ 2837.452108] Modules linked in: dm_flakey dm_mod ppdev parport_pc psmouse parport sg pcspkr acpi_cpufreq tpm_tis tpm_tis_core i2c_piix4 i2c_core evdev tpm button se
    rio_raw sunrpc loop autofs4 ext4 crc16 jbd2 mbcache btrfs raid10 raid456 async_raid6_recov async_memcpy async_pq async_xor async_tx xor raid6_pq libcrc32c crc32c_gene
    ric raid1 raid0 multipath linear md_mod sr_mod cdrom sd_mod ata_generic virtio_scsi ata_piix libata virtio_pci virtio_ring virtio e1000 scsi_mod floppy
    [ 2837.458389] CPU: 8 PID: 2474 Comm: umount Tainted: G        W       4.10.0-rc8-btrfs-next-43+ #1
    [ 2837.459754] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS rel-1.9.1-0-gb3ef39f-prebuilt.qemu-project.org 04/01/2014
    [ 2837.462379] Call Trace:
    [ 2837.462379]  dump_stack+0x68/0x92
    [ 2837.462379]  __warn+0xc2/0xdd
    [ 2837.462379]  warn_slowpath_null+0x1d/0x1f
    [ 2837.462379]  btrfs_destroy_inode+0xe8/0x27e [btrfs]
    [ 2837.462379]  destroy_inode+0x3d/0x55
    [ 2837.462379]  evict+0x177/0x17e
    [ 2837.462379]  dispose_list+0x50/0x71
    [ 2837.462379]  evict_inodes+0x132/0x141
    [ 2837.462379]  generic_shutdown_super+0x3f/0xeb
    [ 2837.462379]  kill_anon_super+0x12/0x1c
    [ 2837.462379]  btrfs_kill_super+0x16/0x21 [btrfs]
    [ 2837.462379]  deactivate_locked_super+0x30/0x68
    [ 2837.462379]  deactivate_super+0x36/0x39
    [ 2837.462379]  cleanup_mnt+0x58/0x76
    [ 2837.462379]  __cleanup_mnt+0x12/0x14
    [ 2837.462379]  task_work_run+0x77/0x9b
    [ 2837.462379]  prepare_exit_to_usermode+0x9d/0xc5
    [ 2837.462379]  syscall_return_slowpath+0x196/0x1b9
    [ 2837.462379]  entry_SYSCALL_64_fastpath+0xab/0xad
    [ 2837.462379] RIP: 0033:0x7f3ef3e6b9a7
    [ 2837.462379] RSP: 002b:00007ffdd0d8de58 EFLAGS: 00000246 ORIG_RAX: 00000000000000a6
    [ 2837.462379] RAX: 0000000000000000 RBX: 0000556f76a39060 RCX: 00007f3ef3e6b9a7
    [ 2837.462379] RDX: 0000000000000001 RSI: 0000000000000000 RDI: 0000556f76a3f910
    [ 2837.462379] RBP: 0000556f76a3f910 R08: 0000556f76a3e670 R09: 0000000000000015
    [ 2837.462379] R10: 00000000000006b4 R11: 0000000000000246 R12: 00007f3ef436ce64
    [ 2837.462379] R13: 0000000000000000 R14: 0000556f76a39240 R15: 00007ffdd0d8e0e0
    [ 2837.519355] ---[ end trace e79345fe24b30b8d ]---
    [ 2837.596256] ------------[ cut here ]------------
    [ 2837.597625] WARNING: CPU: 8 PID: 2474 at fs/btrfs/extent-tree.c:5699 btrfs_free_block_groups+0x246/0x3eb [btrfs]
    [ 2837.603547] Modules linked in: dm_flakey dm_mod ppdev parport_pc psmouse parport sg pcspkr acpi_cpufreq tpm_tis tpm_tis_core i2c_piix4 i2c_core evdev tpm button serio_raw sunrpc loop autofs4 ext4 crc16 jbd2 mbcache btrfs raid10 raid456 async_raid6_recov async_memcpy async_pq async_xor async_tx xor raid6_pq libcrc32c crc32c_generic raid1 raid0 multipath linear md_mod sr_mod cdrom sd_mod ata_generic virtio_scsi ata_piix libata virtio_pci virtio_ring virtio e1000 scsi_mod floppy
    [ 2837.659372] CPU: 8 PID: 2474 Comm: umount Tainted: G        W       4.10.0-rc8-btrfs-next-43+ #1
    [ 2837.663359] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS rel-1.9.1-0-gb3ef39f-prebuilt.qemu-project.org 04/01/2014
    [ 2837.663359] Call Trace:
    [ 2837.663359]  dump_stack+0x68/0x92
    [ 2837.663359]  __warn+0xc2/0xdd
    [ 2837.663359]  warn_slowpath_null+0x1d/0x1f
    [ 2837.663359]  btrfs_free_block_groups+0x246/0x3eb [btrfs]
    [ 2837.663359]  close_ctree+0x1dd/0x2e1 [btrfs]
    [ 2837.663359]  ? evict_inodes+0x132/0x141
    [ 2837.663359]  btrfs_put_super+0x15/0x17 [btrfs]
    [ 2837.663359]  generic_shutdown_super+0x6a/0xeb
    [ 2837.663359]  kill_anon_super+0x12/0x1c
    [ 2837.663359]  btrfs_kill_super+0x16/0x21 [btrfs]
    [ 2837.663359]  deactivate_locked_super+0x30/0x68
    [ 2837.663359]  deactivate_super+0x36/0x39
    [ 2837.663359]  cleanup_mnt+0x58/0x76
    [ 2837.663359]  __cleanup_mnt+0x12/0x14
    [ 2837.663359]  task_work_run+0x77/0x9b
    [ 2837.663359]  prepare_exit_to_usermode+0x9d/0xc5
    [ 2837.663359]  syscall_return_slowpath+0x196/0x1b9
    [ 2837.663359]  entry_SYSCALL_64_fastpath+0xab/0xad
    [ 2837.663359] RIP: 0033:0x7f3ef3e6b9a7
    [ 2837.663359] RSP: 002b:00007ffdd0d8de58 EFLAGS: 00000246 ORIG_RAX: 00000000000000a6
    [ 2837.663359] RAX: 0000000000000000 RBX: 0000556f76a39060 RCX: 00007f3ef3e6b9a7
    [ 2837.663359] RDX: 0000000000000001 RSI: 0000000000000000 RDI: 0000556f76a3f910
    [ 2837.663359] RBP: 0000556f76a3f910 R08: 0000556f76a3e670 R09: 0000000000000015
    [ 2837.663359] R10: 00000000000006b4 R11: 0000000000000246 R12: 00007f3ef436ce64
    [ 2837.663359] R13: 0000000000000000 R14: 0000556f76a39240 R15: 00007ffdd0d8e0e0
    [ 2837.739445] ---[ end trace e79345fe24b30b8e ]---
    [ 2837.745595] ------------[ cut here ]------------
    [ 2837.746412] WARNING: CPU: 8 PID: 2474 at fs/btrfs/extent-tree.c:5700 btrfs_free_block_groups+0x261/0x3eb [btrfs]
    [ 2837.747955] Modules linked in: dm_flakey dm_mod ppdev parport_pc psmouse parport sg pcspkr acpi_cpufreq tpm_tis tpm_tis_core i2c_piix4 i2c_core evdev tpm button serio_raw sunrpc loop autofs4 ext4 crc16 jbd2 mbcache btrfs raid10 raid456 async_raid6_recov async_memcpy async_pq async_xor async_tx xor raid6_pq libcrc32c crc32c_generic raid1 raid0 multipath linear md_mod sr_mod cdrom sd_mod ata_generic virtio_scsi ata_piix libata virtio_pci virtio_ring virtio e1000 scsi_mod floppy
    [ 2837.755395] CPU: 8 PID: 2474 Comm: umount Tainted: G        W       4.10.0-rc8-btrfs-next-43+ #1
    [ 2837.756769] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS rel-1.9.1-0-gb3ef39f-prebuilt.qemu-project.org 04/01/2014
    [ 2837.758526] Call Trace:
    [ 2837.758925]  dump_stack+0x68/0x92
    [ 2837.759383]  __warn+0xc2/0xdd
    [ 2837.759383]  warn_slowpath_null+0x1d/0x1f
    [ 2837.759383]  btrfs_free_block_groups+0x261/0x3eb [btrfs]
    [ 2837.759383]  close_ctree+0x1dd/0x2e1 [btrfs]
    [ 2837.759383]  ? evict_inodes+0x132/0x141
    [ 2837.759383]  btrfs_put_super+0x15/0x17 [btrfs]
    [ 2837.759383]  generic_shutdown_super+0x6a/0xeb
    [ 2837.759383]  kill_anon_super+0x12/0x1c
    [ 2837.759383]  btrfs_kill_super+0x16/0x21 [btrfs]
    [ 2837.759383]  deactivate_locked_super+0x30/0x68
    [ 2837.759383]  deactivate_super+0x36/0x39
    [ 2837.759383]  cleanup_mnt+0x58/0x76
    [ 2837.759383]  __cleanup_mnt+0x12/0x14
    [ 2837.759383]  task_work_run+0x77/0x9b
    [ 2837.759383]  prepare_exit_to_usermode+0x9d/0xc5
    [ 2837.759383]  syscall_return_slowpath+0x196/0x1b9
    [ 2837.759383]  entry_SYSCALL_64_fastpath+0xab/0xad
    [ 2837.759383] RIP: 0033:0x7f3ef3e6b9a7
    [ 2837.759383] RSP: 002b:00007ffdd0d8de58 EFLAGS: 00000246 ORIG_RAX: 00000000000000a6
    [ 2837.759383] RAX: 0000000000000000 RBX: 0000556f76a39060 RCX: 00007f3ef3e6b9a7
    [ 2837.759383] RDX: 0000000000000001 RSI: 0000000000000000 RDI: 0000556f76a3f910
    [ 2837.759383] RBP: 0000556f76a3f910 R08: 0000556f76a3e670 R09: 0000000000000015
    [ 2837.759383] R10: 00000000000006b4 R11: 0000000000000246 R12: 00007f3ef436ce64
    [ 2837.759383] R13: 0000000000000000 R14: 0000556f76a39240 R15: 00007ffdd0d8e0e0
    [ 2837.777063] ---[ end trace e79345fe24b30b8f ]---
    [ 2837.778235] ------------[ cut here ]------------
    [ 2837.778856] WARNING: CPU: 8 PID: 2474 at fs/btrfs/extent-tree.c:9825 btrfs_free_block_groups+0x348/0x3eb [btrfs]
    [ 2837.791385] Modules linked in: dm_flakey dm_mod ppdev parport_pc psmouse parport sg pcspkr acpi_cpufreq tpm_tis tpm_tis_core i2c_piix4 i2c_core evdev tpm button serio_raw sunrpc loop autofs4 ext4 crc16 jbd2 mbcache btrfs raid10 raid456 async_raid6_recov async_memcpy async_pq async_xor async_tx xor raid6_pq libcrc32c crc32c_generic raid1 raid0 multipath linear md_mod sr_mod cdrom sd_mod ata_generic virtio_scsi ata_piix libata virtio_pci virtio_ring virtio e1000 scsi_mod floppy
    [ 2837.797711] CPU: 8 PID: 2474 Comm: umount Tainted: G        W       4.10.0-rc8-btrfs-next-43+ #1
    [ 2837.798594] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS rel-1.9.1-0-gb3ef39f-prebuilt.qemu-project.org 04/01/2014
    [ 2837.800118] Call Trace:
    [ 2837.800515]  dump_stack+0x68/0x92
    [ 2837.801015]  __warn+0xc2/0xdd
    [ 2837.801471]  warn_slowpath_null+0x1d/0x1f
    [ 2837.801698]  btrfs_free_block_groups+0x348/0x3eb [btrfs]
    [ 2837.801698]  close_ctree+0x1dd/0x2e1 [btrfs]
    [ 2837.801698]  ? evict_inodes+0x132/0x141
    [ 2837.801698]  btrfs_put_super+0x15/0x17 [btrfs]
    [ 2837.801698]  generic_shutdown_super+0x6a/0xeb
    [ 2837.801698]  kill_anon_super+0x12/0x1c
    [ 2837.801698]  btrfs_kill_super+0x16/0x21 [btrfs]
    [ 2837.801698]  deactivate_locked_super+0x30/0x68
    [ 2837.801698]  deactivate_super+0x36/0x39
    [ 2837.801698]  cleanup_mnt+0x58/0x76
    [ 2837.801698]  __cleanup_mnt+0x12/0x14
    [ 2837.801698]  task_work_run+0x77/0x9b
    [ 2837.801698]  prepare_exit_to_usermode+0x9d/0xc5
    [ 2837.801698]  syscall_return_slowpath+0x196/0x1b9
    [ 2837.801698]  entry_SYSCALL_64_fastpath+0xab/0xad
    [ 2837.801698] RIP: 0033:0x7f3ef3e6b9a7
    [ 2837.801698] RSP: 002b:00007ffdd0d8de58 EFLAGS: 00000246 ORIG_RAX: 00000000000000a6
    [ 2837.801698] RAX: 0000000000000000 RBX: 0000556f76a39060 RCX: 00007f3ef3e6b9a7
    [ 2837.801698] RDX: 0000000000000001 RSI: 0000000000000000 RDI: 0000556f76a3f910
    [ 2837.801698] RBP: 0000556f76a3f910 R08: 0000556f76a3e670 R09: 0000000000000015
    [ 2837.801698] R10: 00000000000006b4 R11: 0000000000000246 R12: 00007f3ef436ce64
    [ 2837.801698] R13: 0000000000000000 R14: 0000556f76a39240 R15: 00007ffdd0d8e0e0
    [ 2837.818441] ---[ end trace e79345fe24b30b90 ]---
    [ 2837.818991] BTRFS info (device sdc): space_info 1 has 7974912 free, is not full
    [ 2837.819830] BTRFS info (device sdc): space_info total=8388608, used=417792, pinned=0, reserved=0, may_use=18446744073709547520, readonly=0
    
    What happens in the above example is the following:
    
    1) When punching the hole, at btrfs_punch_hole(), the variable tail_len
       is set to 2048 (as tail_start is 148Kb + 1 and offset + len is 150Kb).
       This results in the creation of an extent map with a length of 2Kb
       starting at file offset 148Kb, through find_first_non_hole() ->
       btrfs_get_extent().
    
    2) The second write (first write after the hole punch operation), sets
       the range [50Kb, 152Kb[ to delalloc.
    
    3) The third write, at btrfs_find_new_delalloc_bytes(), sees the extent
       map covering the range [148Kb, 150Kb[ and ends up calling
       set_extent_bit() for the same range, which results in splitting an
       existing extent state record, covering the range [148Kb, 152Kb[ into
       two 2Kb extent state records, covering the ranges [148Kb, 150Kb[ and
       [150Kb, 152Kb[.
    
    4) Finally at lock_and_cleanup_extent_if_need(), immediately after calling
       btrfs_find_new_delalloc_bytes() we clear the delalloc bit from the
       range [100Kb, 152Kb[ which results in the btrfs_clear_bit_hook()
       callback being invoked against the two 2Kb extent state records that
       cover the ranges [148Kb, 150Kb[ and [150Kb, 152Kb[. When called against
       the first 2Kb extent state, it calls btrfs_delalloc_release_metadata()
       with a length argument of 2048 bytes. That function rounds up the length
       to a sector size aligned length, so it ends up considering a length of
       4096 bytes, and then calls calc_csum_metadata_size() which results in
       decrementing the inode's csum_bytes counter by 4096 bytes, so after
       it stays a value of 0 bytes. Then the same happens when
       btrfs_clear_bit_hook() is called against the second extent state that
       has a length of 2Kb, covering the range [150Kb, 152Kb[, the length is
       rounded up to 4096 and calc_csum_metadata_size() ends up being called
       to decrement 4096 bytes from the inode's csum_bytes counter, which
       at that time has a value of 0, leading to an underflow, which is
       exactly what triggers the first warning, at btrfs_destroy_inode().
       All the other warnings relate to several space accounting counters
       that underflow as well due to similar reasons.
    
    A similar case but where the hole punching operation creates an extent map
    with a start offset not aligned to the sector size is the following:
    
      $ mkfs.btrfs -f /dev/sdb
      $ mount /dev/sdb /mnt
      $ xfs_io -f -c "fpunch 695K 820K" $SCRATCH_MNT/bar
      $ xfs_io -c "pwrite -S 0xaa 1008K 307K" $SCRATCH_MNT/bar
      $ xfs_io -c "pwrite -S 0xbb -b 630K 1073K 630K" $SCRATCH_MNT/bar
      $ xfs_io -c "pwrite -S 0xcc -b 459K 1068K 459K" $SCRATCH_MNT/bar
      $ umount /mnt
    
    During the unmount operation we get similar traces for the same reasons as
    in the first example.
    
    So fix the hole punching operation to make sure it never creates extent
    maps with a length that is not aligned to the sector size nor with a start
    offset that is not aligned to the sector size, as this breaks all
    assumptions and it's a land mine.
    
    Fixes: d77815461f04 ("btrfs: Avoid trucating page or punching hole in a already existed hole.")
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Reviewed-by: Liu Bo <bo.li.liu@oracle.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index da1096eb1a40..5da85b080368 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2390,10 +2390,13 @@ static int fill_holes(struct btrfs_trans_handle *trans,
  */
 static int find_first_non_hole(struct inode *inode, u64 *start, u64 *len)
 {
+	struct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);
 	struct extent_map *em;
 	int ret = 0;
 
-	em = btrfs_get_extent(BTRFS_I(inode), NULL, 0, *start, *len, 0);
+	em = btrfs_get_extent(BTRFS_I(inode), NULL, 0,
+			      round_down(*start, fs_info->sectorsize),
+			      round_up(*len, fs_info->sectorsize), 0);
 	if (IS_ERR(em))
 		return PTR_ERR(em);
 

commit edf064e7c6fec3646b06c944a8e35d1a3de5c2c3
Author: Goldwyn Rodrigues <rgoldwyn@suse.com>
Date:   Tue Jun 20 07:05:49 2017 -0500

    btrfs: nowait aio support
    
    Return EAGAIN if any of the following checks fail
     + i_rwsem is not lockable
     + NODATACOW or PREALLOC is not set
     + Cannot nocow at the desired location
     + Writing beyond end of file which is not allocated
    
    Acked-by: David Sterba <dsterba@suse.com>
    Signed-off-by: Goldwyn Rodrigues <rgoldwyn@suse.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index da1096eb1a40..59e2dccdf75b 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1875,12 +1875,29 @@ static ssize_t btrfs_file_write_iter(struct kiocb *iocb,
 	ssize_t num_written = 0;
 	bool sync = (file->f_flags & O_DSYNC) || IS_SYNC(file->f_mapping->host);
 	ssize_t err;
-	loff_t pos;
-	size_t count;
+	loff_t pos = iocb->ki_pos;
+	size_t count = iov_iter_count(from);
 	loff_t oldsize;
 	int clean_page = 0;
 
-	inode_lock(inode);
+	if ((iocb->ki_flags & IOCB_NOWAIT) &&
+			(iocb->ki_flags & IOCB_DIRECT)) {
+		/* Don't sleep on inode rwsem */
+		if (!inode_trylock(inode))
+			return -EAGAIN;
+		/*
+		 * We will allocate space in case nodatacow is not set,
+		 * so bail
+		 */
+		if (!(BTRFS_I(inode)->flags & (BTRFS_INODE_NODATACOW |
+					      BTRFS_INODE_PREALLOC)) ||
+		    check_can_nocow(BTRFS_I(inode), pos, &count) <= 0) {
+			inode_unlock(inode);
+			return -EAGAIN;
+		}
+	} else
+		inode_lock(inode);
+
 	err = generic_write_checks(iocb, from);
 	if (err <= 0) {
 		inode_unlock(inode);
@@ -1914,8 +1931,6 @@ static ssize_t btrfs_file_write_iter(struct kiocb *iocb,
 	 */
 	update_time_for_write(inode);
 
-	pos = iocb->ki_pos;
-	count = iov_iter_count(from);
 	start_pos = round_down(pos, fs_info->sectorsize);
 	oldsize = i_size_read(inode);
 	if (start_pos > oldsize) {
@@ -3071,13 +3086,19 @@ static loff_t btrfs_file_llseek(struct file *file, loff_t offset, int whence)
 	return offset;
 }
 
+static int btrfs_file_open(struct inode *inode, struct file *filp)
+{
+	filp->f_mode |= FMODE_AIO_NOWAIT;
+	return generic_file_open(inode, filp);
+}
+
 const struct file_operations btrfs_file_operations = {
 	.llseek		= btrfs_file_llseek,
 	.read_iter      = generic_file_read_iter,
 	.splice_read	= generic_file_splice_read,
 	.write_iter	= btrfs_file_write_iter,
 	.mmap		= btrfs_file_mmap,
-	.open		= generic_file_open,
+	.open		= btrfs_file_open,
 	.release	= btrfs_release_file,
 	.fsync		= btrfs_sync_file,
 	.fallocate	= btrfs_fallocate,

commit bce19f9d232b71c4eef9ca7d0947035bbb922cef
Merge: c2a9c7ab475b a7e3b975a0f9
Author: Chris Mason <clm@fb.com>
Date:   Thu Apr 27 14:13:09 2017 -0700

    Merge branch 'for-chris-4.12' of git://git.kernel.org/pub/scm/linux/kernel/git/fdmanana/linux into for-linus-4.12

commit a7e3b975a0f9296162b72ac6ab7fad9631a07630
Author: Filipe Manana <fdmanana@suse.com>
Date:   Mon Apr 3 10:45:46 2017 +0100

    Btrfs: fix reported number of inode blocks
    
    Currently when there are buffered writes that were not yet flushed and
    they fall within allocated ranges of the file (that is, not in holes or
    beyond eof assuming there are no prealloc extents beyond eof), btrfs
    simply reports an incorrect number of used blocks through the stat(2)
    system call (or any of its variants), regardless of mount options or
    inode flags (compress, compress-force, nodatacow). This is because the
    number of blocks used that is reported is based on the current number
    of bytes in the vfs inode plus the number of dealloc bytes in the btrfs
    inode. The later covers bytes that both fall within allocated regions
    of the file and holes.
    
    Example scenarios where the number of reported blocks is wrong while the
    buffered writes are not flushed:
    
      $ mkfs.btrfs -f /dev/sdc
      $ mount /dev/sdc /mnt/sdc
    
      $ xfs_io -f -c "pwrite -S 0xaa 0 64K" /mnt/sdc/foo1
      wrote 65536/65536 bytes at offset 0
      64 KiB, 16 ops; 0.0000 sec (259.336 MiB/sec and 66390.0415 ops/sec)
    
      $ sync
    
      $ xfs_io -c "pwrite -S 0xbb 0 64K" /mnt/sdc/foo1
      wrote 65536/65536 bytes at offset 0
      64 KiB, 16 ops; 0.0000 sec (192.308 MiB/sec and 49230.7692 ops/sec)
    
      # The following should have reported 64K...
      $ du -h /mnt/sdc/foo1
      128K  /mnt/sdc/foo1
    
      $ sync
    
      # After flushing the buffered write, it now reports the correct value.
      $ du -h /mnt/sdc/foo1
      64K   /mnt/sdc/foo1
    
      $ xfs_io -f -c "falloc -k 0 128K" -c "pwrite -S 0xaa 0 64K" /mnt/sdc/foo2
      wrote 65536/65536 bytes at offset 0
      64 KiB, 16 ops; 0.0000 sec (520.833 MiB/sec and 133333.3333 ops/sec)
    
      $ sync
    
      $ xfs_io -c "pwrite -S 0xbb 64K 64K" /mnt/sdc/foo2
      wrote 65536/65536 bytes at offset 65536
      64 KiB, 16 ops; 0.0000 sec (260.417 MiB/sec and 66666.6667 ops/sec)
    
      # The following should have reported 128K...
      $ du -h /mnt/sdc/foo2
      192K  /mnt/sdc/foo2
    
      $ sync
    
      # After flushing the buffered write, it now reports the correct value.
      $ du -h /mnt/sdc/foo2
      128K  /mnt/sdc/foo2
    
    So the number of used file blocks is simply incorrect, unlike in other
    filesystems such as ext4 and xfs for example, but only while the buffered
    writes are not flushed.
    
    Fix this by tracking the number of delalloc bytes that fall within holes
    and beyond eof of a file, and use instead this new counter when reporting
    the number of used blocks for an inode.
    
    Another different problem that exists is that the delalloc bytes counter
    is reset when writeback starts (by clearing the EXTENT_DEALLOC flag from
    the respective range in the inode's iotree) and the vfs inode's bytes
    counter is only incremented when writeback finishes (through
    insert_reserved_file_extent()). Therefore while writeback is ongoing we
    simply report a wrong number of blocks used by an inode if the write
    operation covers a range previously unallocated. While this change does
    not fix this problem, it does minimizes it a lot by shortening that time
    window, as the new dealloc bytes counter (new_delalloc_bytes) is only
    decremented when writeback finishes right before updating the vfs inode's
    bytes counter. Fully fixing this second problem is not trivial and will
    be addressed later by a different patch.
    
    Signed-off-by: Filipe Manana <fdmanana@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 56304c400db5..f7d022bc7998 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1404,6 +1404,47 @@ static noinline int prepare_pages(struct inode *inode, struct page **pages,
 
 }
 
+static int btrfs_find_new_delalloc_bytes(struct btrfs_inode *inode,
+					 const u64 start,
+					 const u64 len,
+					 struct extent_state **cached_state)
+{
+	u64 search_start = start;
+	const u64 end = start + len - 1;
+
+	while (search_start < end) {
+		const u64 search_len = end - search_start + 1;
+		struct extent_map *em;
+		u64 em_len;
+		int ret = 0;
+
+		em = btrfs_get_extent(inode, NULL, 0, search_start,
+				      search_len, 0);
+		if (IS_ERR(em))
+			return PTR_ERR(em);
+
+		if (em->block_start != EXTENT_MAP_HOLE)
+			goto next;
+
+		em_len = em->len;
+		if (em->start < search_start)
+			em_len -= search_start - em->start;
+		if (em_len > search_len)
+			em_len = search_len;
+
+		ret = set_extent_bit(&inode->io_tree, search_start,
+				     search_start + em_len - 1,
+				     EXTENT_DELALLOC_NEW,
+				     NULL, cached_state, GFP_NOFS);
+next:
+		search_start = extent_map_end(em);
+		free_extent_map(em);
+		if (ret)
+			return ret;
+	}
+	return 0;
+}
+
 /*
  * This function locks the extent and properly waits for data=ordered extents
  * to finish before allowing the pages to be modified if need.
@@ -1432,8 +1473,11 @@ lock_and_cleanup_extent_if_need(struct btrfs_inode *inode, struct page **pages,
 		+ round_up(pos + write_bytes - start_pos,
 			   fs_info->sectorsize) - 1;
 
-	if (start_pos < inode->vfs_inode.i_size) {
+	if (start_pos < inode->vfs_inode.i_size ||
+	    (inode->flags & BTRFS_INODE_PREALLOC)) {
 		struct btrfs_ordered_extent *ordered;
+		unsigned int clear_bits;
+
 		lock_extent_bits(&inode->io_tree, start_pos, last_pos,
 				cached_state);
 		ordered = btrfs_lookup_ordered_range(inode, start_pos,
@@ -1454,11 +1498,19 @@ lock_and_cleanup_extent_if_need(struct btrfs_inode *inode, struct page **pages,
 		}
 		if (ordered)
 			btrfs_put_ordered_extent(ordered);
-
+		ret = btrfs_find_new_delalloc_bytes(inode, start_pos,
+						    last_pos - start_pos + 1,
+						    cached_state);
+		clear_bits = EXTENT_DIRTY | EXTENT_DELALLOC |
+			EXTENT_DO_ACCOUNTING | EXTENT_DEFRAG;
+		if (ret)
+			clear_bits |= EXTENT_DELALLOC_NEW | EXTENT_LOCKED;
 		clear_extent_bit(&inode->io_tree, start_pos,
-				  last_pos, EXTENT_DIRTY | EXTENT_DELALLOC |
-				  EXTENT_DO_ACCOUNTING | EXTENT_DEFRAG,
-				  0, 0, cached_state, GFP_NOFS);
+				 last_pos, clear_bits,
+				 (clear_bits & EXTENT_LOCKED) ? 1 : 0,
+				 0, cached_state, GFP_NOFS);
+		if (ret)
+			return ret;
 		*lockstart = start_pos;
 		*lockend = last_pos;
 		ret = 1;

commit be2d253cc98244765323a7c94cc1ac5cd5a17072
Author: Filipe Manana <fdmanana@suse.com>
Date:   Mon Apr 3 15:57:17 2017 +0100

    Btrfs: fix extent map leak during fallocate error path
    
    If the call to btrfs_qgroup_reserve_data() failed, we were leaking an
    extent map structure. The failure can happen either due to an -ENOMEM
    condition or, when quotas are enabled, due to -EDQUOT for example.
    
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 48dfb8e4baf2..56304c400db5 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2856,8 +2856,10 @@ static long btrfs_fallocate(struct file *file, int mode,
 			}
 			ret = btrfs_qgroup_reserve_data(inode, cur_offset,
 					last_byte - cur_offset);
-			if (ret < 0)
+			if (ret < 0) {
+				free_extent_map(em);
 				break;
+			}
 		} else {
 			/*
 			 * Do not need to reserve unwritten extent for this

commit 9986277e0e4ce10fd37bf853fe22ba429a967a45
Author: Dan Carpenter <dan.carpenter@oracle.com>
Date:   Tue Apr 11 11:57:15 2017 +0300

    Btrfs: handle only applicable errors returned by btrfs_get_extent
    
    btrfs_get_extent() never returns NULL pointers, so this code introduces
    a static checker warning.
    
    The btrfs_get_extent() is a bit complex, but trust me that it doesn't
    return NULLs and also if it did we would trigger the BUG_ON(!em) before
    the last return statement.
    
    Signed-off-by: Dan Carpenter <dan.carpenter@oracle.com>
    [ updated subject ]
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 520cb7230b2d..3cedbfd08a3a 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2342,13 +2342,8 @@ static int find_first_non_hole(struct inode *inode, u64 *start, u64 *len)
 	int ret = 0;
 
 	em = btrfs_get_extent(BTRFS_I(inode), NULL, 0, *start, *len, 0);
-	if (IS_ERR_OR_NULL(em)) {
-		if (!em)
-			ret = -ENOMEM;
-		else
-			ret = PTR_ERR(em);
-		return ret;
-	}
+	if (IS_ERR(em))
+		return PTR_ERR(em);
 
 	/* Hole or vacuum extent(only exists in no-hole mode) */
 	if (em->block_start == EXTENT_MAP_HOLE) {
@@ -2835,11 +2830,8 @@ static long btrfs_fallocate(struct file *file, int mode,
 	while (1) {
 		em = btrfs_get_extent(BTRFS_I(inode), NULL, 0, cur_offset,
 				      alloc_end - cur_offset, 0);
-		if (IS_ERR_OR_NULL(em)) {
-			if (!em)
-				ret = -ENOMEM;
-			else
-				ret = PTR_ERR(em);
+		if (IS_ERR(em)) {
+			ret = PTR_ERR(em);
 			break;
 		}
 		last_byte = min(extent_map_end(em), alloc_end);

commit bbe08c0a43e2c5ee3a00de68c0e867a08a9aa990
Merge: 94e877d0fb43 e9f467d028cd
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 2 16:03:00 2017 -0800

    Merge branch 'for-linus-4.11' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs
    
    Pull more btrfs updates from Chris Mason:
     "Btrfs round two.
    
      These are mostly a continuation of Dave Sterba's collection of
      cleanups, but Filipe also has some bug fixes and performance
      improvements"
    
    * 'for-linus-4.11' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs: (69 commits)
      btrfs: add dummy callback for readpage_io_failed and drop checks
      btrfs: drop checks for mandatory extent_io_ops callbacks
      btrfs: document existence of extent_io ops callbacks
      btrfs: let writepage_end_io_hook return void
      btrfs: do proper error handling in btrfs_insert_xattr_item
      btrfs: handle allocation error in update_dev_stat_item
      btrfs: remove BUG_ON from __tree_mod_log_insert
      btrfs: derive maximum output size in the compression implementation
      btrfs: use predefined limits for calculating maximum number of pages for compression
      btrfs: export compression buffer limits in a header
      btrfs: merge nr_pages input and output parameter in compress_pages
      btrfs: merge length input and output parameter in compress_pages
      btrfs: constify name of subvolume in creation helpers
      btrfs: constify buffers used by compression helpers
      btrfs: constify input buffer of btrfs_csum_data
      btrfs: constify device path passed to relevant helpers
      btrfs: make btrfs_inode_resume_unlocked_dio take btrfs_inode
      btrfs: make btrfs_inode_block_unlocked_dio take btrfs_inode
      btrfs: Make btrfs_add_nondir take btrfs_inode
      btrfs: Make btrfs_add_link take btrfs_inode
      ...

commit fc4f21b1d8d023cf0a2b1b050ae18e15dbe7068e
Author: Nikolay Borisov <nborisov@suse.com>
Date:   Mon Feb 20 13:51:06 2017 +0200

    btrfs: Make get_extent_t take btrfs_inode
    
    In addition to changing the signature, this patch also switches
    all the functions which are used as an argument to also take btrfs_inode.
    Namely those are: btrfs_get_extent and btrfs_get_extent_filemap.
    
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index dff7ec1770c1..48dfb8e4baf2 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2341,7 +2341,7 @@ static int find_first_non_hole(struct inode *inode, u64 *start, u64 *len)
 	struct extent_map *em;
 	int ret = 0;
 
-	em = btrfs_get_extent(inode, NULL, 0, *start, *len, 0);
+	em = btrfs_get_extent(BTRFS_I(inode), NULL, 0, *start, *len, 0);
 	if (IS_ERR_OR_NULL(em)) {
 		if (!em)
 			ret = -ENOMEM;
@@ -2833,7 +2833,7 @@ static long btrfs_fallocate(struct file *file, int mode,
 	/* First, check if we exceed the qgroup limit */
 	INIT_LIST_HEAD(&reserve_list);
 	while (1) {
-		em = btrfs_get_extent(inode, NULL, 0, cur_offset,
+		em = btrfs_get_extent(BTRFS_I(inode), NULL, 0, cur_offset,
 				      alloc_end - cur_offset, 0);
 		if (IS_ERR_OR_NULL(em)) {
 			if (!em)
@@ -2960,7 +2960,8 @@ static int find_desired_extent(struct inode *inode, loff_t *offset, int whence)
 			 &cached_state);
 
 	while (start < inode->i_size) {
-		em = btrfs_get_extent_fiemap(inode, NULL, 0, start, len, 0);
+		em = btrfs_get_extent_fiemap(BTRFS_I(inode), NULL, 0,
+				start, len, 0);
 		if (IS_ERR(em)) {
 			ret = PTR_ERR(em);
 			em = NULL;

commit 2cff578cfceba883eef199c52674a301a8f91d19
Author: Nikolay Borisov <n.borisov.lkml@gmail.com>
Date:   Mon Feb 20 13:50:51 2017 +0200

    btrfs: Make lock_and_cleanup_extent_if_need take btrfs_inode
    
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 1d09eccec477..dff7ec1770c1 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1415,13 +1415,13 @@ static noinline int prepare_pages(struct inode *inode, struct page **pages,
  * the other < 0 number - Something wrong happens
  */
 static noinline int
-lock_and_cleanup_extent_if_need(struct inode *inode, struct page **pages,
+lock_and_cleanup_extent_if_need(struct btrfs_inode *inode, struct page **pages,
 				size_t num_pages, loff_t pos,
 				size_t write_bytes,
 				u64 *lockstart, u64 *lockend,
 				struct extent_state **cached_state)
 {
-	struct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);
+	struct btrfs_fs_info *fs_info = btrfs_sb(inode->vfs_inode.i_sb);
 	u64 start_pos;
 	u64 last_pos;
 	int i;
@@ -1432,30 +1432,30 @@ lock_and_cleanup_extent_if_need(struct inode *inode, struct page **pages,
 		+ round_up(pos + write_bytes - start_pos,
 			   fs_info->sectorsize) - 1;
 
-	if (start_pos < inode->i_size) {
+	if (start_pos < inode->vfs_inode.i_size) {
 		struct btrfs_ordered_extent *ordered;
-		lock_extent_bits(&BTRFS_I(inode)->io_tree,
-				 start_pos, last_pos, cached_state);
-		ordered = btrfs_lookup_ordered_range(BTRFS_I(inode), start_pos,
+		lock_extent_bits(&inode->io_tree, start_pos, last_pos,
+				cached_state);
+		ordered = btrfs_lookup_ordered_range(inode, start_pos,
 						     last_pos - start_pos + 1);
 		if (ordered &&
 		    ordered->file_offset + ordered->len > start_pos &&
 		    ordered->file_offset <= last_pos) {
-			unlock_extent_cached(&BTRFS_I(inode)->io_tree,
-					     start_pos, last_pos,
-					     cached_state, GFP_NOFS);
+			unlock_extent_cached(&inode->io_tree, start_pos,
+					last_pos, cached_state, GFP_NOFS);
 			for (i = 0; i < num_pages; i++) {
 				unlock_page(pages[i]);
 				put_page(pages[i]);
 			}
-			btrfs_start_ordered_extent(inode, ordered, 1);
+			btrfs_start_ordered_extent(&inode->vfs_inode,
+					ordered, 1);
 			btrfs_put_ordered_extent(ordered);
 			return -EAGAIN;
 		}
 		if (ordered)
 			btrfs_put_ordered_extent(ordered);
 
-		clear_extent_bit(&BTRFS_I(inode)->io_tree, start_pos,
+		clear_extent_bit(&inode->io_tree, start_pos,
 				  last_pos, EXTENT_DIRTY | EXTENT_DELALLOC |
 				  EXTENT_DO_ACCOUNTING | EXTENT_DEFRAG,
 				  0, 0, cached_state, GFP_NOFS);
@@ -1626,9 +1626,9 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 		if (ret)
 			break;
 
-		ret = lock_and_cleanup_extent_if_need(inode, pages, num_pages,
-						pos, write_bytes, &lockstart,
-						&lockend, &cached_state);
+		ret = lock_and_cleanup_extent_if_need(BTRFS_I(inode), pages,
+				num_pages, pos, write_bytes, &lockstart,
+				&lockend, &cached_state);
 		if (ret < 0) {
 			if (ret == -EAGAIN)
 				goto again;

commit 85b7ab6705d9a2e6173361f5da7cbf8f9eb07864
Author: Nikolay Borisov <n.borisov.lkml@gmail.com>
Date:   Mon Feb 20 13:50:50 2017 +0200

    btrfs: Make check_can_nocow take btrfs_inode
    
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index f3648e9bfa01..1d09eccec477 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1474,11 +1474,11 @@ lock_and_cleanup_extent_if_need(struct inode *inode, struct page **pages,
 	return ret;
 }
 
-static noinline int check_can_nocow(struct inode *inode, loff_t pos,
+static noinline int check_can_nocow(struct btrfs_inode *inode, loff_t pos,
 				    size_t *write_bytes)
 {
-	struct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);
-	struct btrfs_root *root = BTRFS_I(inode)->root;
+	struct btrfs_fs_info *fs_info = btrfs_sb(inode->vfs_inode.i_sb);
+	struct btrfs_root *root = inode->root;
 	struct btrfs_ordered_extent *ordered;
 	u64 lockstart, lockend;
 	u64 num_bytes;
@@ -1493,19 +1493,20 @@ static noinline int check_can_nocow(struct inode *inode, loff_t pos,
 			   fs_info->sectorsize) - 1;
 
 	while (1) {
-		lock_extent(&BTRFS_I(inode)->io_tree, lockstart, lockend);
-		ordered = btrfs_lookup_ordered_range(BTRFS_I(inode), lockstart,
+		lock_extent(&inode->io_tree, lockstart, lockend);
+		ordered = btrfs_lookup_ordered_range(inode, lockstart,
 						     lockend - lockstart + 1);
 		if (!ordered) {
 			break;
 		}
-		unlock_extent(&BTRFS_I(inode)->io_tree, lockstart, lockend);
-		btrfs_start_ordered_extent(inode, ordered, 1);
+		unlock_extent(&inode->io_tree, lockstart, lockend);
+		btrfs_start_ordered_extent(&inode->vfs_inode, ordered, 1);
 		btrfs_put_ordered_extent(ordered);
 	}
 
 	num_bytes = lockend - lockstart + 1;
-	ret = can_nocow_extent(inode, lockstart, &num_bytes, NULL, NULL, NULL);
+	ret = can_nocow_extent(&inode->vfs_inode, lockstart, &num_bytes,
+			NULL, NULL, NULL);
 	if (ret <= 0) {
 		ret = 0;
 		btrfs_end_write_no_snapshoting(root);
@@ -1514,7 +1515,7 @@ static noinline int check_can_nocow(struct inode *inode, loff_t pos,
 				     num_bytes - pos + lockstart);
 	}
 
-	unlock_extent(&BTRFS_I(inode)->io_tree, lockstart, lockend);
+	unlock_extent(&inode->io_tree, lockstart, lockend);
 
 	return ret;
 }
@@ -1579,7 +1580,8 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 		if (ret < 0) {
 			if ((BTRFS_I(inode)->flags & (BTRFS_INODE_NODATACOW |
 						      BTRFS_INODE_PREALLOC)) &&
-			    check_can_nocow(inode, pos, &write_bytes) > 0) {
+			    check_can_nocow(BTRFS_I(inode), pos,
+					&write_bytes) > 0) {
 				/*
 				 * For nodata cow case, no need to reserve
 				 * data space.

commit a776c6fa1feba7a84519170ebdb7f4a4155b89d6
Author: Nikolay Borisov <n.borisov.lkml@gmail.com>
Date:   Mon Feb 20 13:50:49 2017 +0200

    btrfs: Make btrfs_lookup_ordered_range take btrfs_inode
    
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index e704b6ce2058..f3648e9bfa01 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1436,7 +1436,7 @@ lock_and_cleanup_extent_if_need(struct inode *inode, struct page **pages,
 		struct btrfs_ordered_extent *ordered;
 		lock_extent_bits(&BTRFS_I(inode)->io_tree,
 				 start_pos, last_pos, cached_state);
-		ordered = btrfs_lookup_ordered_range(inode, start_pos,
+		ordered = btrfs_lookup_ordered_range(BTRFS_I(inode), start_pos,
 						     last_pos - start_pos + 1);
 		if (ordered &&
 		    ordered->file_offset + ordered->len > start_pos &&
@@ -1494,7 +1494,7 @@ static noinline int check_can_nocow(struct inode *inode, loff_t pos,
 
 	while (1) {
 		lock_extent(&BTRFS_I(inode)->io_tree, lockstart, lockend);
-		ordered = btrfs_lookup_ordered_range(inode, lockstart,
+		ordered = btrfs_lookup_ordered_range(BTRFS_I(inode), lockstart,
 						     lockend - lockstart + 1);
 		if (!ordered) {
 			break;

commit 7a6d7067958c32d1c76e0bfd1a0d46be06340b2e
Author: Nikolay Borisov <n.borisov.lkml@gmail.com>
Date:   Mon Feb 20 13:50:48 2017 +0200

    btrfs: Make btrfs_mark_extent_written take btrfs_inode
    
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index ef4ecd003742..e704b6ce2058 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1082,10 +1082,10 @@ static int extent_mergeable(struct extent_buffer *leaf, int slot,
  * two or three.
  */
 int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
-			      struct inode *inode, u64 start, u64 end)
+			      struct btrfs_inode *inode, u64 start, u64 end)
 {
-	struct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);
-	struct btrfs_root *root = BTRFS_I(inode)->root;
+	struct btrfs_fs_info *fs_info = btrfs_sb(inode->vfs_inode.i_sb);
+	struct btrfs_root *root = inode->root;
 	struct extent_buffer *leaf;
 	struct btrfs_path *path;
 	struct btrfs_file_extent_item *fi;
@@ -1102,7 +1102,7 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 	int del_slot = 0;
 	int recow;
 	int ret;
-	u64 ino = btrfs_ino(BTRFS_I(inode));
+	u64 ino = btrfs_ino(inode);
 
 	path = btrfs_alloc_path();
 	if (!path)

commit a012a74e78d99aa27f8487c050e9ac3183bc3785
Author: Nikolay Borisov <n.borisov.lkml@gmail.com>
Date:   Mon Feb 20 13:50:47 2017 +0200

    btrfs: Make fill_holes take btrfs_inode
    
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 27dfdfb7ff19..ef4ecd003742 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2224,22 +2224,23 @@ static int hole_mergeable(struct btrfs_inode *inode, struct extent_buffer *leaf,
 	return 0;
 }
 
-static int fill_holes(struct btrfs_trans_handle *trans, struct inode *inode,
-		      struct btrfs_path *path, u64 offset, u64 end)
+static int fill_holes(struct btrfs_trans_handle *trans,
+		struct btrfs_inode *inode,
+		struct btrfs_path *path, u64 offset, u64 end)
 {
-	struct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);
-	struct btrfs_root *root = BTRFS_I(inode)->root;
+	struct btrfs_fs_info *fs_info = btrfs_sb(inode->vfs_inode.i_sb);
+	struct btrfs_root *root = inode->root;
 	struct extent_buffer *leaf;
 	struct btrfs_file_extent_item *fi;
 	struct extent_map *hole_em;
-	struct extent_map_tree *em_tree = &BTRFS_I(inode)->extent_tree;
+	struct extent_map_tree *em_tree = &inode->extent_tree;
 	struct btrfs_key key;
 	int ret;
 
 	if (btrfs_fs_incompat(fs_info, NO_HOLES))
 		goto out;
 
-	key.objectid = btrfs_ino(BTRFS_I(inode));
+	key.objectid = btrfs_ino(inode);
 	key.type = BTRFS_EXTENT_DATA_KEY;
 	key.offset = offset;
 
@@ -2255,8 +2256,7 @@ static int fill_holes(struct btrfs_trans_handle *trans, struct inode *inode,
 	}
 
 	leaf = path->nodes[0];
-	if (hole_mergeable(BTRFS_I(inode), leaf, path->slots[0] - 1,
-				offset, end)) {
+	if (hole_mergeable(inode, leaf, path->slots[0] - 1, offset, end)) {
 		u64 num_bytes;
 
 		path->slots[0]--;
@@ -2271,7 +2271,7 @@ static int fill_holes(struct btrfs_trans_handle *trans, struct inode *inode,
 		goto out;
 	}
 
-	if (hole_mergeable(BTRFS_I(inode), leaf, path->slots[0], offset, end)) {
+	if (hole_mergeable(inode, leaf, path->slots[0], offset, end)) {
 		u64 num_bytes;
 
 		key.offset = offset;
@@ -2288,7 +2288,7 @@ static int fill_holes(struct btrfs_trans_handle *trans, struct inode *inode,
 	}
 	btrfs_release_path(path);
 
-	ret = btrfs_insert_file_extent(trans, root, btrfs_ino(BTRFS_I(inode)),
+	ret = btrfs_insert_file_extent(trans, root, btrfs_ino(inode),
 			offset, 0, 0, end - offset, 0, end - offset, 0, 0, 0);
 	if (ret)
 		return ret;
@@ -2298,9 +2298,8 @@ static int fill_holes(struct btrfs_trans_handle *trans, struct inode *inode,
 
 	hole_em = alloc_extent_map();
 	if (!hole_em) {
-		btrfs_drop_extent_cache(BTRFS_I(inode), offset, end - 1, 0);
-		set_bit(BTRFS_INODE_NEEDS_FULL_SYNC,
-			&BTRFS_I(inode)->runtime_flags);
+		btrfs_drop_extent_cache(inode, offset, end - 1, 0);
+		set_bit(BTRFS_INODE_NEEDS_FULL_SYNC, &inode->runtime_flags);
 	} else {
 		hole_em->start = offset;
 		hole_em->len = end - offset;
@@ -2315,8 +2314,7 @@ static int fill_holes(struct btrfs_trans_handle *trans, struct inode *inode,
 		hole_em->generation = trans->transid;
 
 		do {
-			btrfs_drop_extent_cache(BTRFS_I(inode), offset,
-					end - 1, 0);
+			btrfs_drop_extent_cache(inode, offset, end - 1, 0);
 			write_lock(&em_tree->lock);
 			ret = add_extent_mapping(em_tree, hole_em, 1);
 			write_unlock(&em_tree->lock);
@@ -2324,7 +2322,7 @@ static int fill_holes(struct btrfs_trans_handle *trans, struct inode *inode,
 		free_extent_map(hole_em);
 		if (ret)
 			set_bit(BTRFS_INODE_NEEDS_FULL_SYNC,
-				&BTRFS_I(inode)->runtime_flags);
+					&inode->runtime_flags);
 	}
 
 	return 0;
@@ -2554,8 +2552,8 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 		trans->block_rsv = &fs_info->trans_block_rsv;
 
 		if (cur_offset < drop_end && cur_offset < ino_size) {
-			ret = fill_holes(trans, inode, path, cur_offset,
-					 drop_end);
+			ret = fill_holes(trans, BTRFS_I(inode), path,
+					cur_offset, drop_end);
 			if (ret) {
 				/*
 				 * If we failed then we didn't insert our hole
@@ -2626,7 +2624,8 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 	 * cur_offset == drop_end).
 	 */
 	if (cur_offset < ino_size && cur_offset < drop_end) {
-		ret = fill_holes(trans, inode, path, cur_offset, drop_end);
+		ret = fill_holes(trans, BTRFS_I(inode), path,
+				cur_offset, drop_end);
 		if (ret) {
 			/* Same comment as above. */
 			btrfs_abort_transaction(trans, ret);

commit 35339c245b5939315e8763deb7f9b68fffe54912
Author: Nikolay Borisov <n.borisov.lkml@gmail.com>
Date:   Mon Feb 20 13:50:46 2017 +0200

    btrfs: Make hole_mergeable take btrfs_inode
    
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 5df1de43aace..27dfdfb7ff19 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2195,7 +2195,7 @@ static int btrfs_file_mmap(struct file	*filp, struct vm_area_struct *vma)
 	return 0;
 }
 
-static int hole_mergeable(struct inode *inode, struct extent_buffer *leaf,
+static int hole_mergeable(struct btrfs_inode *inode, struct extent_buffer *leaf,
 			  int slot, u64 start, u64 end)
 {
 	struct btrfs_file_extent_item *fi;
@@ -2205,7 +2205,7 @@ static int hole_mergeable(struct inode *inode, struct extent_buffer *leaf,
 		return 0;
 
 	btrfs_item_key_to_cpu(leaf, &key, slot);
-	if (key.objectid != btrfs_ino(BTRFS_I(inode)) ||
+	if (key.objectid != btrfs_ino(inode) ||
 	    key.type != BTRFS_EXTENT_DATA_KEY)
 		return 0;
 
@@ -2255,7 +2255,8 @@ static int fill_holes(struct btrfs_trans_handle *trans, struct inode *inode,
 	}
 
 	leaf = path->nodes[0];
-	if (hole_mergeable(inode, leaf, path->slots[0]-1, offset, end)) {
+	if (hole_mergeable(BTRFS_I(inode), leaf, path->slots[0] - 1,
+				offset, end)) {
 		u64 num_bytes;
 
 		path->slots[0]--;
@@ -2270,7 +2271,7 @@ static int fill_holes(struct btrfs_trans_handle *trans, struct inode *inode,
 		goto out;
 	}
 
-	if (hole_mergeable(inode, leaf, path->slots[0], offset, end)) {
+	if (hole_mergeable(BTRFS_I(inode), leaf, path->slots[0], offset, end)) {
 		u64 num_bytes;
 
 		key.offset = offset;

commit dcdbc059f01e242f92e3239654a1a57d15b0da5a
Author: Nikolay Borisov <n.borisov.lkml@gmail.com>
Date:   Mon Feb 20 13:50:45 2017 +0200

    btrfs: Make btrfs_drop_extent_cache take btrfs_inode
    
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index dd1b56504e10..5df1de43aace 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -529,13 +529,13 @@ int btrfs_dirty_pages(struct inode *inode, struct page **pages,
  * this drops all the extents in the cache that intersect the range
  * [start, end].  Existing extents are split as required.
  */
-void btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
+void btrfs_drop_extent_cache(struct btrfs_inode *inode, u64 start, u64 end,
 			     int skip_pinned)
 {
 	struct extent_map *em;
 	struct extent_map *split = NULL;
 	struct extent_map *split2 = NULL;
-	struct extent_map_tree *em_tree = &BTRFS_I(inode)->extent_tree;
+	struct extent_map_tree *em_tree = &inode->extent_tree;
 	u64 len = end - start + 1;
 	u64 gen;
 	int ret;
@@ -720,7 +720,7 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 	int leafs_visited = 0;
 
 	if (drop_cache)
-		btrfs_drop_extent_cache(inode, start, end - 1, 0);
+		btrfs_drop_extent_cache(BTRFS_I(inode), start, end - 1, 0);
 
 	if (start >= BTRFS_I(inode)->disk_i_size && !replace_extent)
 		modify_tree = 0;
@@ -2297,7 +2297,7 @@ static int fill_holes(struct btrfs_trans_handle *trans, struct inode *inode,
 
 	hole_em = alloc_extent_map();
 	if (!hole_em) {
-		btrfs_drop_extent_cache(inode, offset, end - 1, 0);
+		btrfs_drop_extent_cache(BTRFS_I(inode), offset, end - 1, 0);
 		set_bit(BTRFS_INODE_NEEDS_FULL_SYNC,
 			&BTRFS_I(inode)->runtime_flags);
 	} else {
@@ -2314,7 +2314,8 @@ static int fill_holes(struct btrfs_trans_handle *trans, struct inode *inode,
 		hole_em->generation = trans->transid;
 
 		do {
-			btrfs_drop_extent_cache(inode, offset, end - 1, 0);
+			btrfs_drop_extent_cache(BTRFS_I(inode), offset,
+					end - 1, 0);
 			write_lock(&em_tree->lock);
 			ret = add_extent_mapping(em_tree, hole_em, 1);
 			write_unlock(&em_tree->lock);

commit 46e59791836c75210bdf1f715592b49836fad848
Author: Nikolay Borisov <n.borisov.lkml@gmail.com>
Date:   Mon Feb 20 13:50:44 2017 +0200

    btrfs: Make btrfs_requeue_inode_defrag take btrfs_inode
    
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 63645e86ad95..dd1b56504e10 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -194,10 +194,10 @@ int btrfs_add_inode_defrag(struct btrfs_trans_handle *trans,
  * the same inode in the tree, we will merge them together (by
  * __btrfs_add_inode_defrag()) and free the one that we want to requeue.
  */
-static void btrfs_requeue_inode_defrag(struct inode *inode,
+static void btrfs_requeue_inode_defrag(struct btrfs_inode *inode,
 				       struct inode_defrag *defrag)
 {
-	struct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);
+	struct btrfs_fs_info *fs_info = btrfs_sb(inode->vfs_inode.i_sb);
 	int ret;
 
 	if (!__need_auto_defrag(fs_info))
@@ -208,7 +208,7 @@ static void btrfs_requeue_inode_defrag(struct inode *inode,
 	 * them together.
 	 */
 	spin_lock(&fs_info->defrag_inodes_lock);
-	ret = __btrfs_add_inode_defrag(BTRFS_I(inode), defrag);
+	ret = __btrfs_add_inode_defrag(inode, defrag);
 	spin_unlock(&fs_info->defrag_inodes_lock);
 	if (ret)
 		goto out;
@@ -334,7 +334,7 @@ static int __btrfs_run_defrag_inode(struct btrfs_fs_info *fs_info,
 	 */
 	if (num_defrag == BTRFS_DEFRAG_BATCH) {
 		defrag->last_offset = range.start;
-		btrfs_requeue_inode_defrag(inode, defrag);
+		btrfs_requeue_inode_defrag(BTRFS_I(inode), defrag);
 	} else if (defrag->last_offset && !defrag->cycled) {
 		/*
 		 * we didn't fill our defrag batch, but
@@ -343,7 +343,7 @@ static int __btrfs_run_defrag_inode(struct btrfs_fs_info *fs_info,
 		 */
 		defrag->last_offset = 0;
 		defrag->cycled = 1;
-		btrfs_requeue_inode_defrag(inode, defrag);
+		btrfs_requeue_inode_defrag(BTRFS_I(inode), defrag);
 	} else {
 		kmem_cache_free(btrfs_inode_defrag_cachep, defrag);
 	}

commit 6158e1ce1cc620df650ebdcfb3cc08a3d86f5a4c
Author: Nikolay Borisov <n.borisov.lkml@gmail.com>
Date:   Mon Feb 20 13:50:43 2017 +0200

    btrfs: Make (__)btrfs_add_inode_defrag take btrfs_inode
    
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 0e30d14b4916..63645e86ad95 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -92,10 +92,10 @@ static int __compare_inode_defrag(struct inode_defrag *defrag1,
  * If an existing record is found the defrag item you
  * pass in is freed
  */
-static int __btrfs_add_inode_defrag(struct inode *inode,
+static int __btrfs_add_inode_defrag(struct btrfs_inode *inode,
 				    struct inode_defrag *defrag)
 {
-	struct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);
+	struct btrfs_fs_info *fs_info = btrfs_sb(inode->vfs_inode.i_sb);
 	struct inode_defrag *entry;
 	struct rb_node **p;
 	struct rb_node *parent = NULL;
@@ -123,7 +123,7 @@ static int __btrfs_add_inode_defrag(struct inode *inode,
 			return -EEXIST;
 		}
 	}
-	set_bit(BTRFS_INODE_IN_DEFRAG, &BTRFS_I(inode)->runtime_flags);
+	set_bit(BTRFS_INODE_IN_DEFRAG, &inode->runtime_flags);
 	rb_link_node(&defrag->rb_node, parent, p);
 	rb_insert_color(&defrag->rb_node, &fs_info->defrag_inodes);
 	return 0;
@@ -145,10 +145,10 @@ static inline int __need_auto_defrag(struct btrfs_fs_info *fs_info)
  * enabled
  */
 int btrfs_add_inode_defrag(struct btrfs_trans_handle *trans,
-			   struct inode *inode)
+			   struct btrfs_inode *inode)
 {
-	struct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);
-	struct btrfs_root *root = BTRFS_I(inode)->root;
+	struct btrfs_fs_info *fs_info = btrfs_sb(inode->vfs_inode.i_sb);
+	struct btrfs_root *root = inode->root;
 	struct inode_defrag *defrag;
 	u64 transid;
 	int ret;
@@ -156,24 +156,24 @@ int btrfs_add_inode_defrag(struct btrfs_trans_handle *trans,
 	if (!__need_auto_defrag(fs_info))
 		return 0;
 
-	if (test_bit(BTRFS_INODE_IN_DEFRAG, &BTRFS_I(inode)->runtime_flags))
+	if (test_bit(BTRFS_INODE_IN_DEFRAG, &inode->runtime_flags))
 		return 0;
 
 	if (trans)
 		transid = trans->transid;
 	else
-		transid = BTRFS_I(inode)->root->last_trans;
+		transid = inode->root->last_trans;
 
 	defrag = kmem_cache_zalloc(btrfs_inode_defrag_cachep, GFP_NOFS);
 	if (!defrag)
 		return -ENOMEM;
 
-	defrag->ino = btrfs_ino(BTRFS_I(inode));
+	defrag->ino = btrfs_ino(inode);
 	defrag->transid = transid;
 	defrag->root = root->root_key.objectid;
 
 	spin_lock(&fs_info->defrag_inodes_lock);
-	if (!test_bit(BTRFS_INODE_IN_DEFRAG, &BTRFS_I(inode)->runtime_flags)) {
+	if (!test_bit(BTRFS_INODE_IN_DEFRAG, &inode->runtime_flags)) {
 		/*
 		 * If we set IN_DEFRAG flag and evict the inode from memory,
 		 * and then re-read this inode, this new inode doesn't have
@@ -208,7 +208,7 @@ static void btrfs_requeue_inode_defrag(struct inode *inode,
 	 * them together.
 	 */
 	spin_lock(&fs_info->defrag_inodes_lock);
-	ret = __btrfs_add_inode_defrag(inode, defrag);
+	ret = __btrfs_add_inode_defrag(BTRFS_I(inode), defrag);
 	spin_unlock(&fs_info->defrag_inodes_lock);
 	if (ret)
 		goto out;

commit 691fa059673b3b33c25d7925acb0a58e8204dbd6
Author: Nikolay Borisov <nborisov@suse.com>
Date:   Mon Feb 20 13:50:42 2017 +0200

    btrfs: all btrfs_delalloc_release_metadata take btrfs_inode
    
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index e32a92081547..0e30d14b4916 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1678,7 +1678,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 				spin_unlock(&BTRFS_I(inode)->lock);
 			}
 			if (only_release_metadata) {
-				btrfs_delalloc_release_metadata(inode,
+				btrfs_delalloc_release_metadata(BTRFS_I(inode),
 								release_bytes);
 			} else {
 				u64 __pos;
@@ -1739,7 +1739,8 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 	if (release_bytes) {
 		if (only_release_metadata) {
 			btrfs_end_write_no_snapshoting(root);
-			btrfs_delalloc_release_metadata(inode, release_bytes);
+			btrfs_delalloc_release_metadata(BTRFS_I(inode),
+					release_bytes);
 		} else {
 			btrfs_delalloc_release_space(inode,
 						round_down(pos, fs_info->sectorsize),

commit 9f3db423f98c5c6c53b47f4bb2729901500bc330
Author: Nikolay Borisov <n.borisov.lkml@gmail.com>
Date:   Mon Feb 20 13:50:41 2017 +0200

    btrfs: Make btrfs_delalloc_reserve_metadata take btrfs_inode
    
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 073bc975bbb6..e32a92081547 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1599,7 +1599,8 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 			}
 		}
 
-		ret = btrfs_delalloc_reserve_metadata(inode, reserve_bytes);
+		ret = btrfs_delalloc_reserve_metadata(BTRFS_I(inode),
+				reserve_bytes);
 		if (ret) {
 			if (!only_release_metadata)
 				btrfs_free_reserved_data_space(inode, pos,

commit 04f4f916531adc7d2ca6fdb16a68b6f2ff2a8a3b
Author: Nikolay Borisov <nborisov@suse.com>
Date:   Mon Feb 20 13:50:36 2017 +0200

    btrfs: make btrfs_alloc_data_chunk_ondemand take btrfs_inode
    
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 18e5146df864..073bc975bbb6 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2747,7 +2747,8 @@ static long btrfs_fallocate(struct file *file, int mode,
 	 *
 	 * For qgroup space, it will be checked later.
 	 */
-	ret = btrfs_alloc_data_chunk_ondemand(inode, alloc_end - alloc_start);
+	ret = btrfs_alloc_data_chunk_ondemand(BTRFS_I(inode),
+			alloc_end - alloc_start);
 	if (ret < 0)
 		return ret;
 

commit 93407472a21b82f39c955ea7787e5bc7da100642
Author: Fabian Frederick <fabf@skynet.be>
Date:   Mon Feb 27 14:28:32 2017 -0800

    fs: add i_blocksize()
    
    Replace all 1 << inode->i_blkbits and (1 << inode->i_blkbits) in fs
    branch.
    
    This patch also fixes multiple checkpatch warnings: WARNING: Prefer
    'unsigned int' to bare use of 'unsigned'
    
    Thanks to Andrew Morton for suggesting more appropriate function instead
    of macro.
    
    [geliangtang@gmail.com: truncate: use i_blocksize()]
      Link: http://lkml.kernel.org/r/9c8b2cd83c8f5653805d43debde9fa8817e02fc4.1484895804.git.geliangtang@gmail.com
    Link: http://lkml.kernel.org/r/1481319905-10126-1-git-send-email-fabf@skynet.be
    Signed-off-by: Fabian Frederick <fabf@skynet.be>
    Signed-off-by: Geliang Tang <geliangtang@gmail.com>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: Ross Zwisler <ross.zwisler@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 18e5146df864..c1d2a07205da 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2875,7 +2875,7 @@ static long btrfs_fallocate(struct file *file, int mode,
 		if (!ret)
 			ret = btrfs_prealloc_file_range(inode, mode,
 					range->start,
-					range->len, 1 << inode->i_blkbits,
+					range->len, i_blocksize(inode),
 					offset + len, &alloc_hint);
 		else
 			btrfs_free_reserved_data_space(inode, range->start,

commit f85b7379cd76ad25590c4059299b018eac6fbc50
Author: David Sterba <dsterba@suse.com>
Date:   Fri Jan 20 14:54:07 2017 +0100

    btrfs: fix over-80 lines introduced by previous cleanups
    
    This goes as a separate patch because fixing that inside the patches
    caused too many many conflicts.
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 149b79b3aaf8..18e5146df864 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2285,9 +2285,8 @@ static int fill_holes(struct btrfs_trans_handle *trans, struct inode *inode,
 	}
 	btrfs_release_path(path);
 
-	ret = btrfs_insert_file_extent(trans, root, btrfs_ino(BTRFS_I(inode)), offset,
-				       0, 0, end - offset, 0, end - offset,
-				       0, 0, 0);
+	ret = btrfs_insert_file_extent(trans, root, btrfs_ino(BTRFS_I(inode)),
+			offset, 0, 0, end - offset, 0, end - offset, 0, 0, 0);
 	if (ret)
 		return ret;
 

commit 0f8939b8ac8623760c078d41282526de143ee623
Author: Nikolay Borisov <n.borisov.lkml@gmail.com>
Date:   Wed Jan 18 00:31:30 2017 +0200

    btrfs: Make btrfs_inode_in_log take btrfs_inode
    
    Signed-off-by: Nikolay Borisov <n.borisov.lkml@gmail.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 0d32f45cef28..149b79b3aaf8 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2062,7 +2062,7 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 	 * commit does not start nor waits for ordered extents to complete.
 	 */
 	smp_mb();
-	if (btrfs_inode_in_log(inode, fs_info->generation) ||
+	if (btrfs_inode_in_log(BTRFS_I(inode), fs_info->generation) ||
 	    (full_sync && BTRFS_I(inode)->last_trans <=
 	     fs_info->last_trans_committed) ||
 	    (!btrfs_have_ordered_extents_in_range(inode, start, len) &&

commit 4a0cc7ca6c40b607b8aaa0bf6e97ffd74d64c2d8
Author: Nikolay Borisov <n.borisov.lkml@gmail.com>
Date:   Tue Jan 10 20:35:31 2017 +0200

    btrfs: Make btrfs_ino take a struct btrfs_inode
    
    Currently btrfs_ino takes a struct inode and this causes a lot of
    internal btrfs functions which consume this ino to take a VFS inode,
    rather than btrfs' own struct btrfs_inode. In order to fix this "leak"
    of VFS structs into the internals of btrfs first it's necessary to
    eliminate all uses of struct inode for the purpose of inode. This patch
    does that by using BTRFS_I to convert an inode to btrfs_inode. With
    this problem eliminated subsequent patches will start eliminating the
    passing of struct inode altogether, eventually resulting in a lot cleaner
    code.
    
    Signed-off-by: Nikolay Borisov <n.borisov.lkml@gmail.com>
    [ fix btrfs_get_extent tracepoint prototype ]
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index b5c5da215d05..0d32f45cef28 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -168,7 +168,7 @@ int btrfs_add_inode_defrag(struct btrfs_trans_handle *trans,
 	if (!defrag)
 		return -ENOMEM;
 
-	defrag->ino = btrfs_ino(inode);
+	defrag->ino = btrfs_ino(BTRFS_I(inode));
 	defrag->transid = transid;
 	defrag->root = root->root_key.objectid;
 
@@ -702,7 +702,7 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 	struct btrfs_file_extent_item *fi;
 	struct btrfs_key key;
 	struct btrfs_key new_key;
-	u64 ino = btrfs_ino(inode);
+	u64 ino = btrfs_ino(BTRFS_I(inode));
 	u64 search_start = start;
 	u64 disk_bytenr = 0;
 	u64 num_bytes = 0;
@@ -1102,7 +1102,7 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 	int del_slot = 0;
 	int recow;
 	int ret;
-	u64 ino = btrfs_ino(inode);
+	u64 ino = btrfs_ino(BTRFS_I(inode));
 
 	path = btrfs_alloc_path();
 	if (!path)
@@ -2203,7 +2203,7 @@ static int hole_mergeable(struct inode *inode, struct extent_buffer *leaf,
 		return 0;
 
 	btrfs_item_key_to_cpu(leaf, &key, slot);
-	if (key.objectid != btrfs_ino(inode) ||
+	if (key.objectid != btrfs_ino(BTRFS_I(inode)) ||
 	    key.type != BTRFS_EXTENT_DATA_KEY)
 		return 0;
 
@@ -2237,7 +2237,7 @@ static int fill_holes(struct btrfs_trans_handle *trans, struct inode *inode,
 	if (btrfs_fs_incompat(fs_info, NO_HOLES))
 		goto out;
 
-	key.objectid = btrfs_ino(inode);
+	key.objectid = btrfs_ino(BTRFS_I(inode));
 	key.type = BTRFS_EXTENT_DATA_KEY;
 	key.offset = offset;
 
@@ -2285,7 +2285,7 @@ static int fill_holes(struct btrfs_trans_handle *trans, struct inode *inode,
 	}
 	btrfs_release_path(path);
 
-	ret = btrfs_insert_file_extent(trans, root, btrfs_ino(inode), offset,
+	ret = btrfs_insert_file_extent(trans, root, btrfs_ino(BTRFS_I(inode)), offset,
 				       0, 0, end - offset, 0, end - offset,
 				       0, 0, 0);
 	if (ret)

commit 0110c350c86d511be2130cb2a30dcbb76c4af750
Merge: d9cb5bfcc333 9763f7a4a5f7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Dec 17 18:44:00 2016 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs
    
    Pull more vfs updates from Al Viro:
     "In this pile:
    
       - autofs-namespace series
       - dedupe stuff
       - more struct path constification"
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs: (40 commits)
      ocfs2: implement the VFS clone_range, copy_range, and dedupe_range features
      ocfs2: charge quota for reflinked blocks
      ocfs2: fix bad pointer cast
      ocfs2: always unlock when completing dio writes
      ocfs2: don't eat io errors during _dio_end_io_write
      ocfs2: budget for extent tree splits when adding refcount flag
      ocfs2: prohibit refcounted swapfiles
      ocfs2: add newlines to some error messages
      ocfs2: convert inode refcount test to a helper
      simple_write_end(): don't zero in short copy into uptodate
      exofs: don't mess with simple_write_{begin,end}
      9p: saner ->write_end() on failing copy into non-uptodate page
      fix gfs2_stuffed_write_end() on short copies
      fix ceph_write_end()
      nfs_write_end(): fix handling of short copies
      vfs: refactor clone/dedupe_file_range common functions
      fs: try to clone files first in vfs_copy_file_range
      vfs: misc struct path constification
      namespace.c: constify struct path passed to a bunch of primitives
      quota: constify struct path in quota_on
      ...

commit 5f52a2c512a55500349aa261e469d099ede0f256
Merge: 7c4c71ac8a72 2a7bf53f577e
Author: Chris Mason <clm@fb.com>
Date:   Tue Dec 13 09:14:42 2016 -0800

    Merge branch 'for-chris-4.10' of git://git.kernel.org/pub/scm/linux/kernel/git/fdmanana/linux into for-linus-4.10
    
    Patches queued up by Filipe:
    
    The most important change is still the fix for the extent tree
    corruption that happens due to balance when qgroups are enabled (a
    regression introduced in 4.7 by a fix for a regression from the last
    qgroups rework). This has been hitting SLE and openSUSE users and QA
    very badly, where transactions keep getting aborted when running
    delayed references leaving the root filesystem in RO mode and nearly
    unusable.  There are fixes here that allow us to run xfstests again
    with the integrity checker enabled, which has been impossible since 4.8
    (apparently I'm the only one running xfstests with the integrity
    checker enabled, which is useful to validate dirtied leafs, like
    checking if there are keys out of order, etc).  The rest are just some
    trivial fixes, most of them tagged for stable, and two cleanups.
    
    Signed-off-by: Chris Mason <clm@fb.com>

commit a76b5b04375f974579c83433b06466758c0c552c
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Dec 9 16:17:19 2016 -0800

    fs: try to clone files first in vfs_copy_file_range
    
    A clone is a perfectly fine implementation of a file copy, so most
    file systems just implement the copy that way.  Instead of duplicating
    this logic move it to the VFS.  Currently btrfs and XFS implement copies
    the same way as clones and there is no behavior change for them, cifs
    only implements clones and grow support for copy_file_range with this
    patch.  NFS implements both, so this will allow copy_file_range to work
    on servers that only implement CLONE and be lot more efficient on servers
    that implements CLONE and COPY.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 3a14c87d9c92..991cc991fd29 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2998,7 +2998,6 @@ const struct file_operations btrfs_file_operations = {
 #ifdef CONFIG_COMPAT
 	.compat_ioctl	= btrfs_compat_ioctl,
 #endif
-	.copy_file_range = btrfs_copy_file_range,
 	.clone_file_range = btrfs_clone_file_range,
 	.dedupe_file_range = btrfs_dedupe_file_range,
 };

commit 3a45bb207ee2c5548ebf6f5fcc7d249e141f15e8
Author: Jeff Mahoney <jeffm@suse.com>
Date:   Fri Sep 9 21:39:03 2016 -0400

    btrfs: remove root parameter from transaction commit/end routines
    
    Now we only use the root parameter to print the root objectid in
    a tracepoint.  We can use the root parameter from the transaction
    handle for that.  It's also used to join the transaction with
    async commits, so we remove the comment that it's just for checking.
    
    Signed-off-by: Jeff Mahoney <jeffm@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 2d3b93dd9c2c..140271b1ea2e 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2146,7 +2146,7 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 	 * which are indicated by ctx.io_err.
 	 */
 	if (ctx.io_err) {
-		btrfs_end_transaction(trans, root);
+		btrfs_end_transaction(trans);
 		ret = ctx.io_err;
 		goto out;
 	}
@@ -2155,20 +2155,20 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 		if (!ret) {
 			ret = btrfs_sync_log(trans, root, &ctx);
 			if (!ret) {
-				ret = btrfs_end_transaction(trans, root);
+				ret = btrfs_end_transaction(trans);
 				goto out;
 			}
 		}
 		if (!full_sync) {
 			ret = btrfs_wait_ordered_range(inode, start, len);
 			if (ret) {
-				btrfs_end_transaction(trans, root);
+				btrfs_end_transaction(trans);
 				goto out;
 			}
 		}
-		ret = btrfs_commit_transaction(trans, root);
+		ret = btrfs_commit_transaction(trans);
 	} else {
-		ret = btrfs_end_transaction(trans, root);
+		ret = btrfs_end_transaction(trans);
 	}
 out:
 	return ret > 0 ? -EIO : ret;
@@ -2574,7 +2574,7 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 			break;
 		}
 
-		btrfs_end_transaction(trans, root);
+		btrfs_end_transaction(trans);
 		btrfs_btree_balance_dirty(fs_info);
 
 		trans = btrfs_start_transaction(root, rsv_count);
@@ -2642,7 +2642,7 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 	trans->block_rsv = &fs_info->trans_block_rsv;
 	ret = btrfs_update_inode(trans, root, inode);
 	updated_inode = true;
-	btrfs_end_transaction(trans, root);
+	btrfs_end_transaction(trans);
 	btrfs_btree_balance_dirty(fs_info);
 out_free:
 	btrfs_free_path(path);
@@ -2664,7 +2664,7 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 			err = PTR_ERR(trans);
 		} else {
 			err = btrfs_update_inode(trans, root, inode);
-			ret = btrfs_end_transaction(trans, root);
+			ret = btrfs_end_transaction(trans);
 		}
 	}
 	inode_unlock(inode);
@@ -2906,9 +2906,9 @@ static long btrfs_fallocate(struct file *file, int mode,
 			btrfs_ordered_update_i_size(inode, actual_end, NULL);
 			ret = btrfs_update_inode(trans, root, inode);
 			if (ret)
-				btrfs_end_transaction(trans, root);
+				btrfs_end_transaction(trans);
 			else
-				ret = btrfs_end_transaction(trans, root);
+				ret = btrfs_end_transaction(trans);
 		}
 	}
 out_unlock:

commit 2ff7e61e0d30ff166a2ae94575526bffe11fd1a8
Author: Jeff Mahoney <jeffm@suse.com>
Date:   Wed Jun 22 18:54:24 2016 -0400

    btrfs: take an fs_info directly when the root is not used otherwise
    
    There are loads of functions in btrfs that accept a root parameter
    but only use it to obtain an fs_info pointer.  Let's convert those to
    just accept an fs_info pointer directly.
    
    Signed-off-by: Jeff Mahoney <jeffm@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index d49d8eadf517..2d3b93dd9c2c 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -129,10 +129,8 @@ static int __btrfs_add_inode_defrag(struct inode *inode,
 	return 0;
 }
 
-static inline int __need_auto_defrag(struct btrfs_root *root)
+static inline int __need_auto_defrag(struct btrfs_fs_info *fs_info)
 {
-	struct btrfs_fs_info *fs_info = root->fs_info;
-
 	if (!btrfs_test_opt(fs_info, AUTO_DEFRAG))
 		return 0;
 
@@ -155,7 +153,7 @@ int btrfs_add_inode_defrag(struct btrfs_trans_handle *trans,
 	u64 transid;
 	int ret;
 
-	if (!__need_auto_defrag(root))
+	if (!__need_auto_defrag(fs_info))
 		return 0;
 
 	if (test_bit(BTRFS_INODE_IN_DEFRAG, &BTRFS_I(inode)->runtime_flags))
@@ -200,10 +198,9 @@ static void btrfs_requeue_inode_defrag(struct inode *inode,
 				       struct inode_defrag *defrag)
 {
 	struct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);
-	struct btrfs_root *root = BTRFS_I(inode)->root;
 	int ret;
 
-	if (!__need_auto_defrag(root))
+	if (!__need_auto_defrag(fs_info))
 		goto out;
 
 	/*
@@ -376,7 +373,7 @@ int btrfs_run_defrag_inodes(struct btrfs_fs_info *fs_info)
 			     &fs_info->fs_state))
 			break;
 
-		if (!__need_auto_defrag(fs_info->tree_root))
+		if (!__need_auto_defrag(fs_info))
 			break;
 
 		/* find an inode to defrag */
@@ -488,10 +485,9 @@ static void btrfs_drop_pages(struct page **pages, size_t num_pages)
  * this also makes the decision about creating an inline extent vs
  * doing real data extents, marking pages dirty and delalloc as required.
  */
-int btrfs_dirty_pages(struct btrfs_root *root, struct inode *inode,
-			     struct page **pages, size_t num_pages,
-			     loff_t pos, size_t write_bytes,
-			     struct extent_state **cached)
+int btrfs_dirty_pages(struct inode *inode, struct page **pages,
+		      size_t num_pages, loff_t pos, size_t write_bytes,
+		      struct extent_state **cached)
 {
 	struct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);
 	int err = 0;
@@ -860,7 +856,7 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 			btrfs_mark_buffer_dirty(leaf);
 
 			if (update_refs && disk_bytenr > 0) {
-				ret = btrfs_inc_extent_ref(trans, root,
+				ret = btrfs_inc_extent_ref(trans, fs_info,
 						disk_bytenr, num_bytes, 0,
 						root->root_key.objectid,
 						new_key.objectid,
@@ -944,7 +940,7 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 				extent_end = ALIGN(extent_end,
 						   fs_info->sectorsize);
 			} else if (update_refs && disk_bytenr > 0) {
-				ret = btrfs_free_extent(trans, root,
+				ret = btrfs_free_extent(trans, fs_info,
 						disk_bytenr, num_bytes, 0,
 						root->root_key.objectid,
 						key.objectid, key.offset -
@@ -1001,7 +997,7 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 	if (!ret && replace_extent && leafs_visited == 1 &&
 	    (path->locks[0] == BTRFS_WRITE_LOCK_BLOCKING ||
 	     path->locks[0] == BTRFS_WRITE_LOCK) &&
-	    btrfs_leaf_free_space(root, leaf) >=
+	    btrfs_leaf_free_space(fs_info, leaf) >=
 	    sizeof(struct btrfs_item) + extent_item_size) {
 
 		key.objectid = ino;
@@ -1238,8 +1234,8 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 						extent_end - split);
 		btrfs_mark_buffer_dirty(leaf);
 
-		ret = btrfs_inc_extent_ref(trans, root, bytenr, num_bytes, 0,
-					   root->root_key.objectid,
+		ret = btrfs_inc_extent_ref(trans, fs_info, bytenr, num_bytes,
+					   0, root->root_key.objectid,
 					   ino, orig_offset);
 		if (ret) {
 			btrfs_abort_transaction(trans, ret);
@@ -1272,7 +1268,7 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 		extent_end = other_end;
 		del_slot = path->slots[0] + 1;
 		del_nr++;
-		ret = btrfs_free_extent(trans, root, bytenr, num_bytes,
+		ret = btrfs_free_extent(trans, fs_info, bytenr, num_bytes,
 					0, root->root_key.objectid,
 					ino, orig_offset);
 		if (ret) {
@@ -1292,7 +1288,7 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 		key.offset = other_start;
 		del_slot = path->slots[0];
 		del_nr++;
-		ret = btrfs_free_extent(trans, root, bytenr, num_bytes,
+		ret = btrfs_free_extent(trans, fs_info, bytenr, num_bytes,
 					0, root->root_key.objectid,
 					ino, orig_offset);
 		if (ret) {
@@ -1698,9 +1694,8 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 					fs_info->sectorsize);
 
 		if (copied > 0)
-			ret = btrfs_dirty_pages(root, inode, pages,
-						dirty_pages, pos, copied,
-						NULL);
+			ret = btrfs_dirty_pages(inode, pages, dirty_pages,
+						pos, copied, NULL);
 		if (need_unlock)
 			unlock_extent_cached(&BTRFS_I(inode)->io_tree,
 					     lockstart, lockend, &cached_state,
@@ -1732,7 +1727,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 
 		balance_dirty_pages_ratelimited(inode->i_mapping);
 		if (dirty_pages < (fs_info->nodesize >> PAGE_SHIFT) + 1)
-			btrfs_btree_balance_dirty(root);
+			btrfs_btree_balance_dirty(fs_info);
 
 		pos += copied;
 		num_written += copied;
@@ -2519,7 +2514,7 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 		goto out;
 	}
 
-	rsv = btrfs_alloc_block_rsv(root, BTRFS_BLOCK_RSV_TEMP);
+	rsv = btrfs_alloc_block_rsv(fs_info, BTRFS_BLOCK_RSV_TEMP);
 	if (!rsv) {
 		ret = -ENOMEM;
 		goto out_free;
@@ -2580,7 +2575,7 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 		}
 
 		btrfs_end_transaction(trans, root);
-		btrfs_btree_balance_dirty(root);
+		btrfs_btree_balance_dirty(fs_info);
 
 		trans = btrfs_start_transaction(root, rsv_count);
 		if (IS_ERR(trans)) {
@@ -2648,10 +2643,10 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 	ret = btrfs_update_inode(trans, root, inode);
 	updated_inode = true;
 	btrfs_end_transaction(trans, root);
-	btrfs_btree_balance_dirty(root);
+	btrfs_btree_balance_dirty(fs_info);
 out_free:
 	btrfs_free_path(path);
-	btrfs_free_block_rsv(root, rsv);
+	btrfs_free_block_rsv(fs_info, rsv);
 out:
 	unlock_extent_cached(&BTRFS_I(inode)->io_tree, lockstart, lockend,
 			     &cached_state, GFP_NOFS);

commit 0b246afa62b0cf5b09d078121f543135f28492ad
Author: Jeff Mahoney <jeffm@suse.com>
Date:   Wed Jun 22 18:54:23 2016 -0400

    btrfs: root->fs_info cleanup, add fs_info convenience variables
    
    In routines where someptr->fs_info is referenced multiple times, we
    introduce a convenience variable.  This makes the code considerably
    more readable.
    
    Signed-off-by: Jeff Mahoney <jeffm@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 008670e3c98a..d49d8eadf517 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -95,13 +95,13 @@ static int __compare_inode_defrag(struct inode_defrag *defrag1,
 static int __btrfs_add_inode_defrag(struct inode *inode,
 				    struct inode_defrag *defrag)
 {
-	struct btrfs_root *root = BTRFS_I(inode)->root;
+	struct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);
 	struct inode_defrag *entry;
 	struct rb_node **p;
 	struct rb_node *parent = NULL;
 	int ret;
 
-	p = &root->fs_info->defrag_inodes.rb_node;
+	p = &fs_info->defrag_inodes.rb_node;
 	while (*p) {
 		parent = *p;
 		entry = rb_entry(parent, struct inode_defrag, rb_node);
@@ -125,16 +125,18 @@ static int __btrfs_add_inode_defrag(struct inode *inode,
 	}
 	set_bit(BTRFS_INODE_IN_DEFRAG, &BTRFS_I(inode)->runtime_flags);
 	rb_link_node(&defrag->rb_node, parent, p);
-	rb_insert_color(&defrag->rb_node, &root->fs_info->defrag_inodes);
+	rb_insert_color(&defrag->rb_node, &fs_info->defrag_inodes);
 	return 0;
 }
 
 static inline int __need_auto_defrag(struct btrfs_root *root)
 {
-	if (!btrfs_test_opt(root->fs_info, AUTO_DEFRAG))
+	struct btrfs_fs_info *fs_info = root->fs_info;
+
+	if (!btrfs_test_opt(fs_info, AUTO_DEFRAG))
 		return 0;
 
-	if (btrfs_fs_closing(root->fs_info))
+	if (btrfs_fs_closing(fs_info))
 		return 0;
 
 	return 1;
@@ -147,6 +149,7 @@ static inline int __need_auto_defrag(struct btrfs_root *root)
 int btrfs_add_inode_defrag(struct btrfs_trans_handle *trans,
 			   struct inode *inode)
 {
+	struct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);
 	struct btrfs_root *root = BTRFS_I(inode)->root;
 	struct inode_defrag *defrag;
 	u64 transid;
@@ -171,7 +174,7 @@ int btrfs_add_inode_defrag(struct btrfs_trans_handle *trans,
 	defrag->transid = transid;
 	defrag->root = root->root_key.objectid;
 
-	spin_lock(&root->fs_info->defrag_inodes_lock);
+	spin_lock(&fs_info->defrag_inodes_lock);
 	if (!test_bit(BTRFS_INODE_IN_DEFRAG, &BTRFS_I(inode)->runtime_flags)) {
 		/*
 		 * If we set IN_DEFRAG flag and evict the inode from memory,
@@ -184,7 +187,7 @@ int btrfs_add_inode_defrag(struct btrfs_trans_handle *trans,
 	} else {
 		kmem_cache_free(btrfs_inode_defrag_cachep, defrag);
 	}
-	spin_unlock(&root->fs_info->defrag_inodes_lock);
+	spin_unlock(&fs_info->defrag_inodes_lock);
 	return 0;
 }
 
@@ -196,6 +199,7 @@ int btrfs_add_inode_defrag(struct btrfs_trans_handle *trans,
 static void btrfs_requeue_inode_defrag(struct inode *inode,
 				       struct inode_defrag *defrag)
 {
+	struct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);
 	struct btrfs_root *root = BTRFS_I(inode)->root;
 	int ret;
 
@@ -206,9 +210,9 @@ static void btrfs_requeue_inode_defrag(struct inode *inode,
 	 * Here we don't check the IN_DEFRAG flag, because we need merge
 	 * them together.
 	 */
-	spin_lock(&root->fs_info->defrag_inodes_lock);
+	spin_lock(&fs_info->defrag_inodes_lock);
 	ret = __btrfs_add_inode_defrag(inode, defrag);
-	spin_unlock(&root->fs_info->defrag_inodes_lock);
+	spin_unlock(&fs_info->defrag_inodes_lock);
 	if (ret)
 		goto out;
 	return;
@@ -489,6 +493,7 @@ int btrfs_dirty_pages(struct btrfs_root *root, struct inode *inode,
 			     loff_t pos, size_t write_bytes,
 			     struct extent_state **cached)
 {
+	struct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);
 	int err = 0;
 	int i;
 	u64 num_bytes;
@@ -497,9 +502,9 @@ int btrfs_dirty_pages(struct btrfs_root *root, struct inode *inode,
 	u64 end_pos = pos + write_bytes;
 	loff_t isize = i_size_read(inode);
 
-	start_pos = pos & ~((u64) root->fs_info->sectorsize - 1);
+	start_pos = pos & ~((u64) fs_info->sectorsize - 1);
 	num_bytes = round_up(write_bytes + pos - start_pos,
-			     root->fs_info->sectorsize);
+			     fs_info->sectorsize);
 
 	end_of_last_block = start_pos + num_bytes - 1;
 	err = btrfs_set_extent_delalloc(inode, start_pos, end_of_last_block,
@@ -696,6 +701,7 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 			 u32 extent_item_size,
 			 int *key_inserted)
 {
+	struct btrfs_fs_info *fs_info = root->fs_info;
 	struct extent_buffer *leaf;
 	struct btrfs_file_extent_item *fi;
 	struct btrfs_key key;
@@ -724,7 +730,7 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 		modify_tree = 0;
 
 	update_refs = (test_bit(BTRFS_ROOT_REF_COWS, &root->state) ||
-		       root == root->fs_info->tree_root);
+		       root == fs_info->tree_root);
 	while (1) {
 		recow = 0;
 		ret = btrfs_lookup_file_extent(trans, root, path, ino,
@@ -881,7 +887,7 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 
 			memcpy(&new_key, &key, sizeof(new_key));
 			new_key.offset = end;
-			btrfs_set_item_key_safe(root->fs_info, path, &new_key);
+			btrfs_set_item_key_safe(fs_info, path, &new_key);
 
 			extent_offset += end - key.offset;
 			btrfs_set_file_extent_offset(leaf, fi, extent_offset);
@@ -936,7 +942,7 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 				inode_sub_bytes(inode,
 						extent_end - key.offset);
 				extent_end = ALIGN(extent_end,
-						   root->fs_info->sectorsize);
+						   fs_info->sectorsize);
 			} else if (update_refs && disk_bytenr > 0) {
 				ret = btrfs_free_extent(trans, root,
 						disk_bytenr, num_bytes, 0,
@@ -1082,6 +1088,7 @@ static int extent_mergeable(struct extent_buffer *leaf, int slot,
 int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 			      struct inode *inode, u64 start, u64 end)
 {
+	struct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);
 	struct btrfs_root *root = BTRFS_I(inode)->root;
 	struct extent_buffer *leaf;
 	struct btrfs_path *path;
@@ -1151,7 +1158,7 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 				     ino, bytenr, orig_offset,
 				     &other_start, &other_end)) {
 			new_key.offset = end;
-			btrfs_set_item_key_safe(root->fs_info, path, &new_key);
+			btrfs_set_item_key_safe(fs_info, path, &new_key);
 			fi = btrfs_item_ptr(leaf, path->slots[0],
 					    struct btrfs_file_extent_item);
 			btrfs_set_file_extent_generation(leaf, fi,
@@ -1185,7 +1192,7 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 							 trans->transid);
 			path->slots[0]++;
 			new_key.offset = start;
-			btrfs_set_item_key_safe(root->fs_info, path, &new_key);
+			btrfs_set_item_key_safe(fs_info, path, &new_key);
 
 			fi = btrfs_item_ptr(leaf, path->slots[0],
 					    struct btrfs_file_extent_item);
@@ -1418,16 +1425,16 @@ lock_and_cleanup_extent_if_need(struct inode *inode, struct page **pages,
 				u64 *lockstart, u64 *lockend,
 				struct extent_state **cached_state)
 {
-	struct btrfs_root *root = BTRFS_I(inode)->root;
+	struct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);
 	u64 start_pos;
 	u64 last_pos;
 	int i;
 	int ret = 0;
 
-	start_pos = round_down(pos, root->fs_info->sectorsize);
+	start_pos = round_down(pos, fs_info->sectorsize);
 	last_pos = start_pos
 		+ round_up(pos + write_bytes - start_pos,
-			   root->fs_info->sectorsize) - 1;
+			   fs_info->sectorsize) - 1;
 
 	if (start_pos < inode->i_size) {
 		struct btrfs_ordered_extent *ordered;
@@ -1474,6 +1481,7 @@ lock_and_cleanup_extent_if_need(struct inode *inode, struct page **pages,
 static noinline int check_can_nocow(struct inode *inode, loff_t pos,
 				    size_t *write_bytes)
 {
+	struct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);
 	struct btrfs_root *root = BTRFS_I(inode)->root;
 	struct btrfs_ordered_extent *ordered;
 	u64 lockstart, lockend;
@@ -1484,9 +1492,9 @@ static noinline int check_can_nocow(struct inode *inode, loff_t pos,
 	if (!ret)
 		return -ENOSPC;
 
-	lockstart = round_down(pos, root->fs_info->sectorsize);
+	lockstart = round_down(pos, fs_info->sectorsize);
 	lockend = round_up(pos + *write_bytes,
-			   root->fs_info->sectorsize) - 1;
+			   fs_info->sectorsize) - 1;
 
 	while (1) {
 		lock_extent(&BTRFS_I(inode)->io_tree, lockstart, lockend);
@@ -1520,8 +1528,8 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 					       loff_t pos)
 {
 	struct inode *inode = file_inode(file);
+	struct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);
 	struct btrfs_root *root = BTRFS_I(inode)->root;
-	struct btrfs_fs_info *fs_info = root->fs_info;
 	struct page **pages = NULL;
 	struct extent_state *cached_state = NULL;
 	u64 release_bytes = 0;
@@ -1633,12 +1641,10 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 
 		copied = btrfs_copy_from_user(pos, write_bytes, pages, i);
 
-		num_sectors = BTRFS_BYTES_TO_BLKS(root->fs_info,
-						reserve_bytes);
+		num_sectors = BTRFS_BYTES_TO_BLKS(fs_info, reserve_bytes);
 		dirty_sectors = round_up(copied + sector_offset,
-					root->fs_info->sectorsize);
-		dirty_sectors = BTRFS_BYTES_TO_BLKS(root->fs_info,
-						dirty_sectors);
+					fs_info->sectorsize);
+		dirty_sectors = BTRFS_BYTES_TO_BLKS(fs_info, dirty_sectors);
 
 		/*
 		 * if we have trouble faulting in the pages, fall
@@ -1666,11 +1672,9 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 		 * managed to copy.
 		 */
 		if (num_sectors > dirty_sectors) {
-
 			/* release everything except the sectors we dirtied */
 			release_bytes -= dirty_sectors <<
-				root->fs_info->sb->s_blocksize_bits;
-
+						fs_info->sb->s_blocksize_bits;
 			if (copied > 0) {
 				spin_lock(&BTRFS_I(inode)->lock);
 				BTRFS_I(inode)->outstanding_extents++;
@@ -1683,7 +1687,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 				u64 __pos;
 
 				__pos = round_down(pos,
-						   root->fs_info->sectorsize) +
+						   fs_info->sectorsize) +
 					(dirty_pages << PAGE_SHIFT);
 				btrfs_delalloc_release_space(inode, __pos,
 							     release_bytes);
@@ -1691,7 +1695,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 		}
 
 		release_bytes = round_up(copied + sector_offset,
-					root->fs_info->sectorsize);
+					fs_info->sectorsize);
 
 		if (copied > 0)
 			ret = btrfs_dirty_pages(root, inode, pages,
@@ -1712,9 +1716,9 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 
 		if (only_release_metadata && copied > 0) {
 			lockstart = round_down(pos,
-					       root->fs_info->sectorsize);
+					       fs_info->sectorsize);
 			lockend = round_up(pos + copied,
-					   root->fs_info->sectorsize) - 1;
+					   fs_info->sectorsize) - 1;
 
 			set_extent_bit(&BTRFS_I(inode)->io_tree, lockstart,
 				       lockend, EXTENT_NORESERVE, NULL,
@@ -1727,7 +1731,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 		cond_resched();
 
 		balance_dirty_pages_ratelimited(inode->i_mapping);
-		if (dirty_pages < (root->fs_info->nodesize >> PAGE_SHIFT) + 1)
+		if (dirty_pages < (fs_info->nodesize >> PAGE_SHIFT) + 1)
 			btrfs_btree_balance_dirty(root);
 
 		pos += copied;
@@ -1742,7 +1746,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 			btrfs_delalloc_release_metadata(inode, release_bytes);
 		} else {
 			btrfs_delalloc_release_space(inode,
-						round_down(pos, root->fs_info->sectorsize),
+						round_down(pos, fs_info->sectorsize),
 						release_bytes);
 		}
 	}
@@ -1813,6 +1817,7 @@ static ssize_t btrfs_file_write_iter(struct kiocb *iocb,
 {
 	struct file *file = iocb->ki_filp;
 	struct inode *inode = file_inode(file);
+	struct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);
 	struct btrfs_root *root = BTRFS_I(inode)->root;
 	u64 start_pos;
 	u64 end_pos;
@@ -1844,7 +1849,7 @@ static ssize_t btrfs_file_write_iter(struct kiocb *iocb,
 	 * although we have opened a file as writable, we have
 	 * to stop this write operation to ensure FS consistency.
 	 */
-	if (test_bit(BTRFS_FS_STATE_ERROR, &root->fs_info->fs_state)) {
+	if (test_bit(BTRFS_FS_STATE_ERROR, &fs_info->fs_state)) {
 		inode_unlock(inode);
 		err = -EROFS;
 		goto out;
@@ -1860,18 +1865,18 @@ static ssize_t btrfs_file_write_iter(struct kiocb *iocb,
 
 	pos = iocb->ki_pos;
 	count = iov_iter_count(from);
-	start_pos = round_down(pos, root->fs_info->sectorsize);
+	start_pos = round_down(pos, fs_info->sectorsize);
 	oldsize = i_size_read(inode);
 	if (start_pos > oldsize) {
 		/* Expand hole size to cover write data, preventing empty gap */
 		end_pos = round_up(pos + count,
-				   root->fs_info->sectorsize);
+				   fs_info->sectorsize);
 		err = btrfs_cont_expand(inode, oldsize, end_pos);
 		if (err) {
 			inode_unlock(inode);
 			goto out;
 		}
-		if (start_pos > round_up(oldsize, root->fs_info->sectorsize))
+		if (start_pos > round_up(oldsize, fs_info->sectorsize))
 			clean_page = 1;
 	}
 
@@ -1951,6 +1956,7 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 {
 	struct dentry *dentry = file_dentry(file);
 	struct inode *inode = d_inode(dentry);
+	struct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);
 	struct btrfs_root *root = BTRFS_I(inode)->root;
 	struct btrfs_trans_handle *trans;
 	struct btrfs_log_ctx ctx;
@@ -2061,12 +2067,12 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 	 * commit does not start nor waits for ordered extents to complete.
 	 */
 	smp_mb();
-	if (btrfs_inode_in_log(inode, root->fs_info->generation) ||
+	if (btrfs_inode_in_log(inode, fs_info->generation) ||
 	    (full_sync && BTRFS_I(inode)->last_trans <=
-	     root->fs_info->last_trans_committed) ||
+	     fs_info->last_trans_committed) ||
 	    (!btrfs_have_ordered_extents_in_range(inode, start, len) &&
 	     BTRFS_I(inode)->last_trans
-	     <= root->fs_info->last_trans_committed)) {
+	     <= fs_info->last_trans_committed)) {
 		/*
 		 * We've had everything committed since the last time we were
 		 * modified so clear this flag in case it was set for whatever
@@ -2224,6 +2230,7 @@ static int hole_mergeable(struct inode *inode, struct extent_buffer *leaf,
 static int fill_holes(struct btrfs_trans_handle *trans, struct inode *inode,
 		      struct btrfs_path *path, u64 offset, u64 end)
 {
+	struct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);
 	struct btrfs_root *root = BTRFS_I(inode)->root;
 	struct extent_buffer *leaf;
 	struct btrfs_file_extent_item *fi;
@@ -2232,7 +2239,7 @@ static int fill_holes(struct btrfs_trans_handle *trans, struct inode *inode,
 	struct btrfs_key key;
 	int ret;
 
-	if (btrfs_fs_incompat(root->fs_info, NO_HOLES))
+	if (btrfs_fs_incompat(fs_info, NO_HOLES))
 		goto out;
 
 	key.objectid = btrfs_ino(inode);
@@ -2270,7 +2277,7 @@ static int fill_holes(struct btrfs_trans_handle *trans, struct inode *inode,
 		u64 num_bytes;
 
 		key.offset = offset;
-		btrfs_set_item_key_safe(root->fs_info, path, &key);
+		btrfs_set_item_key_safe(fs_info, path, &key);
 		fi = btrfs_item_ptr(leaf, path->slots[0],
 				    struct btrfs_file_extent_item);
 		num_bytes = btrfs_file_extent_num_bytes(leaf, fi) + end -
@@ -2306,7 +2313,7 @@ static int fill_holes(struct btrfs_trans_handle *trans, struct inode *inode,
 		hole_em->block_start = EXTENT_MAP_HOLE;
 		hole_em->block_len = 0;
 		hole_em->orig_block_len = 0;
-		hole_em->bdev = root->fs_info->fs_devices->latest_bdev;
+		hole_em->bdev = fs_info->fs_devices->latest_bdev;
 		hole_em->compress_type = BTRFS_COMPRESS_NONE;
 		hole_em->generation = trans->transid;
 
@@ -2358,6 +2365,7 @@ static int find_first_non_hole(struct inode *inode, u64 *start, u64 *len)
 
 static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 {
+	struct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);
 	struct btrfs_root *root = BTRFS_I(inode)->root;
 	struct extent_state *cached_state = NULL;
 	struct btrfs_path *path;
@@ -2369,13 +2377,13 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 	u64 tail_len;
 	u64 orig_start = offset;
 	u64 cur_offset;
-	u64 min_size = btrfs_calc_trunc_metadata_size(root->fs_info, 1);
+	u64 min_size = btrfs_calc_trunc_metadata_size(fs_info, 1);
 	u64 drop_end;
 	int ret = 0;
 	int err = 0;
 	unsigned int rsv_count;
 	bool same_block;
-	bool no_holes = btrfs_fs_incompat(root->fs_info, NO_HOLES);
+	bool no_holes = btrfs_fs_incompat(fs_info, NO_HOLES);
 	u64 ino_size;
 	bool truncated_block = false;
 	bool updated_inode = false;
@@ -2385,7 +2393,7 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 		return ret;
 
 	inode_lock(inode);
-	ino_size = round_up(inode->i_size, root->fs_info->sectorsize);
+	ino_size = round_up(inode->i_size, fs_info->sectorsize);
 	ret = find_first_non_hole(inode, &offset, &len);
 	if (ret < 0)
 		goto out_only_mutex;
@@ -2398,8 +2406,8 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 	lockstart = round_up(offset, btrfs_inode_sectorsize(inode));
 	lockend = round_down(offset + len,
 			     btrfs_inode_sectorsize(inode)) - 1;
-	same_block = (BTRFS_BYTES_TO_BLKS(root->fs_info, offset))
-		== (BTRFS_BYTES_TO_BLKS(root->fs_info, offset + len - 1));
+	same_block = (BTRFS_BYTES_TO_BLKS(fs_info, offset))
+		== (BTRFS_BYTES_TO_BLKS(fs_info, offset + len - 1));
 	/*
 	 * We needn't truncate any block which is beyond the end of the file
 	 * because we are sure there is no data there.
@@ -2408,7 +2416,7 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 	 * Only do this if we are in the same block and we aren't doing the
 	 * entire block.
 	 */
-	if (same_block && len < root->fs_info->sectorsize) {
+	if (same_block && len < fs_info->sectorsize) {
 		if (offset < ino_size) {
 			truncated_block = true;
 			ret = btrfs_truncate_block(inode, offset, len, 0);
@@ -2516,7 +2524,7 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 		ret = -ENOMEM;
 		goto out_free;
 	}
-	rsv->size = btrfs_calc_trunc_metadata_size(root->fs_info, 1);
+	rsv->size = btrfs_calc_trunc_metadata_size(fs_info, 1);
 	rsv->failfast = 1;
 
 	/*
@@ -2531,7 +2539,7 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 		goto out_free;
 	}
 
-	ret = btrfs_block_rsv_migrate(&root->fs_info->trans_block_rsv, rsv,
+	ret = btrfs_block_rsv_migrate(&fs_info->trans_block_rsv, rsv,
 				      min_size, 0);
 	BUG_ON(ret);
 	trans->block_rsv = rsv;
@@ -2545,7 +2553,7 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 		if (ret != -ENOSPC)
 			break;
 
-		trans->block_rsv = &root->fs_info->trans_block_rsv;
+		trans->block_rsv = &fs_info->trans_block_rsv;
 
 		if (cur_offset < drop_end && cur_offset < ino_size) {
 			ret = fill_holes(trans, inode, path, cur_offset,
@@ -2581,7 +2589,7 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 			break;
 		}
 
-		ret = btrfs_block_rsv_migrate(&root->fs_info->trans_block_rsv,
+		ret = btrfs_block_rsv_migrate(&fs_info->trans_block_rsv,
 					      rsv, min_size, 0);
 		BUG_ON(ret);	/* shouldn't happen */
 		trans->block_rsv = rsv;
@@ -2600,7 +2608,7 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 		goto out_trans;
 	}
 
-	trans->block_rsv = &root->fs_info->trans_block_rsv;
+	trans->block_rsv = &fs_info->trans_block_rsv;
 	/*
 	 * If we are using the NO_HOLES feature we might have had already an
 	 * hole that overlaps a part of the region [lockstart, lockend] and
@@ -2636,7 +2644,7 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 	inode_inc_iversion(inode);
 	inode->i_mtime = inode->i_ctime = current_time(inode);
 
-	trans->block_rsv = &root->fs_info->trans_block_rsv;
+	trans->block_rsv = &fs_info->trans_block_rsv;
 	ret = btrfs_update_inode(trans, root, inode);
 	updated_inode = true;
 	btrfs_end_transaction(trans, root);
@@ -2922,7 +2930,7 @@ static long btrfs_fallocate(struct file *file, int mode,
 
 static int find_desired_extent(struct inode *inode, loff_t *offset, int whence)
 {
-	struct btrfs_root *root = BTRFS_I(inode)->root;
+	struct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);
 	struct extent_map *em = NULL;
 	struct extent_state *cached_state = NULL;
 	u64 lockstart;
@@ -2940,11 +2948,11 @@ static int find_desired_extent(struct inode *inode, loff_t *offset, int whence)
 	 */
 	start = max_t(loff_t, 0, *offset);
 
-	lockstart = round_down(start, root->fs_info->sectorsize);
+	lockstart = round_down(start, fs_info->sectorsize);
 	lockend = round_up(i_size_read(inode),
-			   root->fs_info->sectorsize);
+			   fs_info->sectorsize);
 	if (lockend <= lockstart)
-		lockend = lockstart + root->fs_info->sectorsize;
+		lockend = lockstart + fs_info->sectorsize;
 	lockend--;
 	len = lockend - lockstart + 1;
 

commit 27965b6c2cad220f6c512334665808bf3d895e5e
Author: Jeff Mahoney <jeffm@suse.com>
Date:   Thu Jun 16 11:07:27 2016 -0400

    btrfs: root->fs_info cleanup, btrfs_calc_{trans,trunc}_metadata_size
    
    Signed-off-by: Jeff Mahoney <jeffm@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 1e0af55e619e..008670e3c98a 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2369,7 +2369,7 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 	u64 tail_len;
 	u64 orig_start = offset;
 	u64 cur_offset;
-	u64 min_size = btrfs_calc_trunc_metadata_size(root, 1);
+	u64 min_size = btrfs_calc_trunc_metadata_size(root->fs_info, 1);
 	u64 drop_end;
 	int ret = 0;
 	int err = 0;
@@ -2516,7 +2516,7 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 		ret = -ENOMEM;
 		goto out_free;
 	}
-	rsv->size = btrfs_calc_trunc_metadata_size(root, 1);
+	rsv->size = btrfs_calc_trunc_metadata_size(root->fs_info, 1);
 	rsv->failfast = 1;
 
 	/*

commit da17066c40472c2d6a1aab7bb0090c3d285531c9
Author: Jeff Mahoney <jeffm@suse.com>
Date:   Wed Jun 15 09:22:56 2016 -0400

    btrfs: pull node/sector/stripe sizes out of root and into fs_info
    
    We track the node sizes per-root, but they never vary from the values
    in the superblock.  This patch messes with the 80-column style a bit,
    but subsequent patches to factor out root->fs_info into a convenience
    variable fix it up again.
    
    Signed-off-by: Jeff Mahoney <jeffm@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 3c1f4be36f16..1e0af55e619e 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -497,8 +497,9 @@ int btrfs_dirty_pages(struct btrfs_root *root, struct inode *inode,
 	u64 end_pos = pos + write_bytes;
 	loff_t isize = i_size_read(inode);
 
-	start_pos = pos & ~((u64)root->sectorsize - 1);
-	num_bytes = round_up(write_bytes + pos - start_pos, root->sectorsize);
+	start_pos = pos & ~((u64) root->fs_info->sectorsize - 1);
+	num_bytes = round_up(write_bytes + pos - start_pos,
+			     root->fs_info->sectorsize);
 
 	end_of_last_block = start_pos + num_bytes - 1;
 	err = btrfs_set_extent_delalloc(inode, start_pos, end_of_last_block,
@@ -935,7 +936,7 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 				inode_sub_bytes(inode,
 						extent_end - key.offset);
 				extent_end = ALIGN(extent_end,
-						   root->sectorsize);
+						   root->fs_info->sectorsize);
 			} else if (update_refs && disk_bytenr > 0) {
 				ret = btrfs_free_extent(trans, root,
 						disk_bytenr, num_bytes, 0,
@@ -1423,9 +1424,10 @@ lock_and_cleanup_extent_if_need(struct inode *inode, struct page **pages,
 	int i;
 	int ret = 0;
 
-	start_pos = round_down(pos, root->sectorsize);
+	start_pos = round_down(pos, root->fs_info->sectorsize);
 	last_pos = start_pos
-		+ round_up(pos + write_bytes - start_pos, root->sectorsize) - 1;
+		+ round_up(pos + write_bytes - start_pos,
+			   root->fs_info->sectorsize) - 1;
 
 	if (start_pos < inode->i_size) {
 		struct btrfs_ordered_extent *ordered;
@@ -1482,8 +1484,9 @@ static noinline int check_can_nocow(struct inode *inode, loff_t pos,
 	if (!ret)
 		return -ENOSPC;
 
-	lockstart = round_down(pos, root->sectorsize);
-	lockend = round_up(pos + *write_bytes, root->sectorsize) - 1;
+	lockstart = round_down(pos, root->fs_info->sectorsize);
+	lockend = round_up(pos + *write_bytes,
+			   root->fs_info->sectorsize) - 1;
 
 	while (1) {
 		lock_extent(&BTRFS_I(inode)->io_tree, lockstart, lockend);
@@ -1518,6 +1521,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 {
 	struct inode *inode = file_inode(file);
 	struct btrfs_root *root = BTRFS_I(inode)->root;
+	struct btrfs_fs_info *fs_info = root->fs_info;
 	struct page **pages = NULL;
 	struct extent_state *cached_state = NULL;
 	u64 release_bytes = 0;
@@ -1563,9 +1567,9 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 			break;
 		}
 
-		sector_offset = pos & (root->sectorsize - 1);
+		sector_offset = pos & (fs_info->sectorsize - 1);
 		reserve_bytes = round_up(write_bytes + sector_offset,
-				root->sectorsize);
+				fs_info->sectorsize);
 
 		ret = btrfs_check_data_free_space(inode, pos, write_bytes);
 		if (ret < 0) {
@@ -1585,7 +1589,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 							 PAGE_SIZE);
 				reserve_bytes = round_up(write_bytes +
 							 sector_offset,
-							 root->sectorsize);
+							 fs_info->sectorsize);
 			} else {
 				break;
 			}
@@ -1632,7 +1636,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 		num_sectors = BTRFS_BYTES_TO_BLKS(root->fs_info,
 						reserve_bytes);
 		dirty_sectors = round_up(copied + sector_offset,
-					root->sectorsize);
+					root->fs_info->sectorsize);
 		dirty_sectors = BTRFS_BYTES_TO_BLKS(root->fs_info,
 						dirty_sectors);
 
@@ -1678,7 +1682,8 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 			} else {
 				u64 __pos;
 
-				__pos = round_down(pos, root->sectorsize) +
+				__pos = round_down(pos,
+						   root->fs_info->sectorsize) +
 					(dirty_pages << PAGE_SHIFT);
 				btrfs_delalloc_release_space(inode, __pos,
 							     release_bytes);
@@ -1686,7 +1691,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 		}
 
 		release_bytes = round_up(copied + sector_offset,
-					root->sectorsize);
+					root->fs_info->sectorsize);
 
 		if (copied > 0)
 			ret = btrfs_dirty_pages(root, inode, pages,
@@ -1706,8 +1711,10 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 			btrfs_end_write_no_snapshoting(root);
 
 		if (only_release_metadata && copied > 0) {
-			lockstart = round_down(pos, root->sectorsize);
-			lockend = round_up(pos + copied, root->sectorsize) - 1;
+			lockstart = round_down(pos,
+					       root->fs_info->sectorsize);
+			lockend = round_up(pos + copied,
+					   root->fs_info->sectorsize) - 1;
 
 			set_extent_bit(&BTRFS_I(inode)->io_tree, lockstart,
 				       lockend, EXTENT_NORESERVE, NULL,
@@ -1720,7 +1727,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 		cond_resched();
 
 		balance_dirty_pages_ratelimited(inode->i_mapping);
-		if (dirty_pages < (root->nodesize >> PAGE_SHIFT) + 1)
+		if (dirty_pages < (root->fs_info->nodesize >> PAGE_SHIFT) + 1)
 			btrfs_btree_balance_dirty(root);
 
 		pos += copied;
@@ -1735,7 +1742,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 			btrfs_delalloc_release_metadata(inode, release_bytes);
 		} else {
 			btrfs_delalloc_release_space(inode,
-						round_down(pos, root->sectorsize),
+						round_down(pos, root->fs_info->sectorsize),
 						release_bytes);
 		}
 	}
@@ -1853,17 +1860,18 @@ static ssize_t btrfs_file_write_iter(struct kiocb *iocb,
 
 	pos = iocb->ki_pos;
 	count = iov_iter_count(from);
-	start_pos = round_down(pos, root->sectorsize);
+	start_pos = round_down(pos, root->fs_info->sectorsize);
 	oldsize = i_size_read(inode);
 	if (start_pos > oldsize) {
 		/* Expand hole size to cover write data, preventing empty gap */
-		end_pos = round_up(pos + count, root->sectorsize);
+		end_pos = round_up(pos + count,
+				   root->fs_info->sectorsize);
 		err = btrfs_cont_expand(inode, oldsize, end_pos);
 		if (err) {
 			inode_unlock(inode);
 			goto out;
 		}
-		if (start_pos > round_up(oldsize, root->sectorsize))
+		if (start_pos > round_up(oldsize, root->fs_info->sectorsize))
 			clean_page = 1;
 	}
 
@@ -2377,7 +2385,7 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 		return ret;
 
 	inode_lock(inode);
-	ino_size = round_up(inode->i_size, root->sectorsize);
+	ino_size = round_up(inode->i_size, root->fs_info->sectorsize);
 	ret = find_first_non_hole(inode, &offset, &len);
 	if (ret < 0)
 		goto out_only_mutex;
@@ -2387,9 +2395,9 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 		goto out_only_mutex;
 	}
 
-	lockstart = round_up(offset, BTRFS_I(inode)->root->sectorsize);
+	lockstart = round_up(offset, btrfs_inode_sectorsize(inode));
 	lockend = round_down(offset + len,
-			     BTRFS_I(inode)->root->sectorsize) - 1;
+			     btrfs_inode_sectorsize(inode)) - 1;
 	same_block = (BTRFS_BYTES_TO_BLKS(root->fs_info, offset))
 		== (BTRFS_BYTES_TO_BLKS(root->fs_info, offset + len - 1));
 	/*
@@ -2400,7 +2408,7 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 	 * Only do this if we are in the same block and we aren't doing the
 	 * entire block.
 	 */
-	if (same_block && len < root->sectorsize) {
+	if (same_block && len < root->fs_info->sectorsize) {
 		if (offset < ino_size) {
 			truncated_block = true;
 			ret = btrfs_truncate_block(inode, offset, len, 0);
@@ -2718,7 +2726,7 @@ static long btrfs_fallocate(struct file *file, int mode,
 	u64 locked_end;
 	u64 actual_end = 0;
 	struct extent_map *em;
-	int blocksize = BTRFS_I(inode)->root->sectorsize;
+	int blocksize = btrfs_inode_sectorsize(inode);
 	int ret;
 
 	alloc_start = round_down(offset, blocksize);
@@ -2932,10 +2940,11 @@ static int find_desired_extent(struct inode *inode, loff_t *offset, int whence)
 	 */
 	start = max_t(loff_t, 0, *offset);
 
-	lockstart = round_down(start, root->sectorsize);
-	lockend = round_up(i_size_read(inode), root->sectorsize);
+	lockstart = round_down(start, root->fs_info->sectorsize);
+	lockend = round_up(i_size_read(inode),
+			   root->fs_info->sectorsize);
 	if (lockend <= lockstart)
-		lockend = lockstart + root->sectorsize;
+		lockend = lockstart + root->fs_info->sectorsize;
 	lockend--;
 	len = lockend - lockstart + 1;
 

commit 2cdaf447e8c411bb61d3d1c91fafbbdd59ef0db2
Author: Robbie Ko <robbieko@synology.com>
Date:   Fri Oct 28 10:32:54 2016 +0800

    Btrfs: fix enospc in hole punching
    
    The hole punching can result in adding new leafs (and as a consequence
    new nodes) to the tree because when we find file extent items that span
    beyond the hole range we may end up not deleting them (just adjusting
    them, reducing their range by reducing their length or increasing their
    offset field) and add new file extent items representing holes.
    
    So after splitting a leaf (therefore creating a new one) to insert a new
    file extent item representing a hole, a new node might be added to each
    level of the tree in the worst case scenario (since there's a new key
    and every parent node was full).
    
    For example if a file has an extent item representing the range 0 to 64Mb
    and we punch a hole in the range 1Mb to 20Mb, the existing extent item is
    duplicated and one of the copies is adjusted to represent the range 0 to
    1Mb, the other copy adjusted to represent the range 20Mb to 64Mb, and a
    new file extent item representing a hole in the range 1Mb to 20Mb is
    inserted.
    
    Fix this by using btrfs_calc_trans_metadata_size() instead of
    btrfs_calc_trunc_metadata_size(), so that enough metadata space is
    reserved for the worst possible case.
    
    Signed-off-by: Robbie Ko <robbieko@synology.com>
    Reviewed-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    [Modified changelog for clarity and correctness]

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 72a180d3503e..4129de52d986 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2347,7 +2347,7 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 	u64 tail_len;
 	u64 orig_start = offset;
 	u64 cur_offset;
-	u64 min_size = btrfs_calc_trunc_metadata_size(root, 1);
+	u64 min_size = btrfs_calc_trans_metadata_size(root, 1);
 	u64 drop_end;
 	int ret = 0;
 	int err = 0;
@@ -2494,7 +2494,7 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 		ret = -ENOMEM;
 		goto out_free;
 	}
-	rsv->size = btrfs_calc_trunc_metadata_size(root, 1);
+	rsv->size = btrfs_calc_trans_metadata_size(root, 1);
 	rsv->failfast = 1;
 
 	/*

commit f94480bd7be6bb1b0823d1036f3ee4ebe7450172
Author: Josef Bacik <jbacik@fb.com>
Date:   Mon Nov 14 14:06:22 2016 -0500

    Btrfs: abort transaction if fill_holes() fails
    
    At this point we will have dropped extent entries from the file, so if we fail
    to insert the new hole entries then we are leaving the fs in a corrupt state
    (albeit an easily fixed one).  Abort the transaciton if this happens so we can
    avoid corrupting the fs.  Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index f5288fa0aad0..3c1f4be36f16 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2232,9 +2232,15 @@ static int fill_holes(struct btrfs_trans_handle *trans, struct inode *inode,
 	key.offset = offset;
 
 	ret = btrfs_search_slot(trans, root, &key, path, 0, 1);
-	if (ret < 0)
+	if (ret <= 0) {
+		/*
+		 * We should have dropped this offset, so if we find it then
+		 * something has gone horribly wrong.
+		 */
+		if (ret == 0)
+			ret = -EINVAL;
 		return ret;
-	BUG_ON(!ret);
+	}
 
 	leaf = path->nodes[0];
 	if (hole_mergeable(inode, leaf, path->slots[0]-1, offset, end)) {
@@ -2537,6 +2543,13 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 			ret = fill_holes(trans, inode, path, cur_offset,
 					 drop_end);
 			if (ret) {
+				/*
+				 * If we failed then we didn't insert our hole
+				 * entries for the area we dropped, so now the
+				 * fs is corrupted, so we must abort the
+				 * transaction.
+				 */
+				btrfs_abort_transaction(trans, ret);
 				err = ret;
 				break;
 			}
@@ -2601,6 +2614,8 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 	if (cur_offset < ino_size && cur_offset < drop_end) {
 		ret = fill_holes(trans, inode, path, cur_offset, drop_end);
 		if (ret) {
+			/* Same comment as above. */
+			btrfs_abort_transaction(trans, ret);
 			err = ret;
 			goto out_trans;
 		}

commit 62fe51c1d0100ff07a761cd077872e01f2a2b8ca
Author: Josef Bacik <jbacik@fb.com>
Date:   Wed Nov 16 09:13:39 2016 -0500

    Btrfs: fix file extent corruption
    
    In order to do hole punching we have a block reserve to hold the reservation we
    need to drop the extents in our range.  Since we could end up dropping a lot of
    extents we set rsv->failfast so we can just loop around again and drop the
    remaining of the range.  Unfortunately we unconditionally fill the hole extents
    in and start from the last extent we encountered, which we may or may not have
    dropped.  So this can result in overlapping file extent entries, which can be
    tripped over in a variety of ways, either by hitting BUG_ON(!ret) in
    fill_holes() after the search, or in btrfs_set_item_key_safe() in
    btrfs_drop_extent() at a later time by an unrelated task.  Fix this by only
    setting drop_end to the last extent we did actually drop.  This way our holes
    are filled in properly for the range that we did drop, and the rest of the range
    that remains to be dropped is actually dropped.  Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 5b1f90af3db6..f5288fa0aad0 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -705,6 +705,7 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 	u64 num_bytes = 0;
 	u64 extent_offset = 0;
 	u64 extent_end = 0;
+	u64 last_end = start;
 	int del_nr = 0;
 	int del_slot = 0;
 	int extent_type;
@@ -796,8 +797,10 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 		 * extent item in the call to setup_items_for_insert() later
 		 * in this function.
 		 */
-		if (extent_end == key.offset && extent_end >= search_start)
+		if (extent_end == key.offset && extent_end >= search_start) {
+			last_end = extent_end;
 			goto delete_extent_item;
+		}
 
 		if (extent_end <= search_start) {
 			path->slots[0]++;
@@ -859,6 +862,12 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 			}
 			key.offset = start;
 		}
+		/*
+		 * From here on out we will have actually dropped something, so
+		 * last_end can be updated.
+		 */
+		last_end = extent_end;
+
 		/*
 		 *  | ---- range to drop ----- |
 		 *      | -------- extent -------- |
@@ -1009,7 +1018,7 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 	if (!replace_extent || !(*key_inserted))
 		btrfs_release_path(path);
 	if (drop_end)
-		*drop_end = found ? min(end, extent_end) : end;
+		*drop_end = found ? min(end, last_end) : end;
 	return ret;
 }
 
@@ -2524,7 +2533,7 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 
 		trans->block_rsv = &root->fs_info->trans_block_rsv;
 
-		if (cur_offset < ino_size) {
+		if (cur_offset < drop_end && cur_offset < ino_size) {
 			ret = fill_holes(trans, inode, path, cur_offset,
 					 drop_end);
 			if (ret) {

commit 926b92335a607e787d8d111d872f82de6d5988d5
Author: David Sterba <dsterba@suse.com>
Date:   Wed Oct 5 14:23:05 2016 +0200

    btrfs: remove unused headers, statfs.h
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 3a14c87d9c92..5b1f90af3db6 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -27,7 +27,6 @@
 #include <linux/falloc.h>
 #include <linux/swap.h>
 #include <linux/writeback.h>
-#include <linux/statfs.h>
 #include <linux/compat.h>
 #include <linux/slab.h>
 #include <linux/btrfs.h>

commit f29135b54bcbfe1fea97d94e2ae860bade1d5a31
Merge: 4c609922a3ae 19c4d2f99478
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 11 11:23:06 2016 -0700

    Merge branch 'for-linus-4.9' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs
    
    Pull btrfs updates from Chris Mason:
     "This is a big variety of fixes and cleanups.
    
      Liu Bo continues to fixup fuzzer related problems, and some of Josef's
      cleanups are prep for his bigger extent buffer changes (slated for
      v4.10)"
    
    * 'for-linus-4.9' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs: (39 commits)
      Revert "btrfs: let btrfs_delete_unused_bgs() to clean relocated bgs"
      Btrfs: remove unnecessary btrfs_mark_buffer_dirty in split_leaf
      Btrfs: don't BUG() during drop snapshot
      btrfs: fix btrfs_no_printk stub helper
      Btrfs: memset to avoid stale content in btree leaf
      btrfs: parent_start initialization cleanup
      btrfs: Remove already completed TODO comment
      btrfs: Do not reassign count in btrfs_run_delayed_refs
      btrfs: fix a possible umount deadlock
      Btrfs: fix memory leak in do_walk_down
      btrfs: btrfs_debug should consume fs_info when DEBUG is not defined
      btrfs: convert send's verbose_printk to btrfs_debug
      btrfs: convert pr_* to btrfs_* where possible
      btrfs: convert printk(KERN_* to use pr_* calls
      btrfs: unsplit printed strings
      btrfs: clean the old superblocks before freeing the device
      Btrfs: kill BUG_ON in run_delayed_tree_ref
      Btrfs: don't leak reloc root nodes on error
      btrfs: squash lines for simple wrapper functions
      Btrfs: improve check_node to avoid reading corrupted nodes
      ...

commit 101105b1717f536ca741f940033996302d4ef191
Merge: 35ff96dfd3c9 3873691e5ab3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 10 20:16:43 2016 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs
    
    Pull more vfs updates from Al Viro:
     ">rename2() work from Miklos + current_time() from Deepa"
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs:
      fs: Replace current_fs_time() with current_time()
      fs: Replace CURRENT_TIME_SEC with current_time() for inode timestamps
      fs: Replace CURRENT_TIME with current_time() for inode timestamps
      fs: proc: Delete inode time initializations in proc_alloc_inode()
      vfs: Add current_time() api
      vfs: add note about i_op->rename changes to porting
      fs: rename "rename2" i_op to "rename"
      vfs: remove unused i_op->rename
      fs: make remaining filesystems use .rename2
      libfs: support RENAME_NOREPLACE in simple_rename()
      fs: support RENAME_NOREPLACE for local filesystems
      ncpfs: fix unused variable warning

commit c2050a454c7f123d7a57fa1d76ff61bd43643abb
Author: Deepa Dinamani <deepa.kernel@gmail.com>
Date:   Wed Sep 14 07:48:06 2016 -0700

    fs: Replace current_fs_time() with current_time()
    
    current_fs_time() uses struct super_block* as an argument.
    As per Linus's suggestion, this is changed to take struct
    inode* as a parameter instead. This is because the function
    is primarily meant for vfs inode timestamps.
    Also the function was renamed as per Arnd's suggestion.
    
    Change all calls to current_fs_time() to use the new
    current_time() function instead. current_fs_time() will be
    deleted.
    
    Signed-off-by: Deepa Dinamani <deepa.kernel@gmail.com>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index fea31a4a6e36..dad53ce54d91 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1757,7 +1757,7 @@ static void update_time_for_write(struct inode *inode)
 	if (IS_NOCMTIME(inode))
 		return;
 
-	now = current_fs_time(inode->i_sb);
+	now = current_time(inode);
 	if (!timespec_equal(&inode->i_mtime, &now))
 		inode->i_mtime = now;
 
@@ -2578,7 +2578,7 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 		goto out_free;
 
 	inode_inc_iversion(inode);
-	inode->i_mtime = inode->i_ctime = current_fs_time(inode->i_sb);
+	inode->i_mtime = inode->i_ctime = current_time(inode);
 
 	trans->block_rsv = &root->fs_info->trans_block_rsv;
 	ret = btrfs_update_inode(trans, root, inode);
@@ -2842,7 +2842,7 @@ static long btrfs_fallocate(struct file *file, int mode,
 		if (IS_ERR(trans)) {
 			ret = PTR_ERR(trans);
 		} else {
-			inode->i_ctime = current_fs_time(inode->i_sb);
+			inode->i_ctime = current_time(inode);
 			i_size_write(inode, actual_end);
 			btrfs_ordered_update_i_size(inode, actual_end, NULL);
 			ret = btrfs_update_inode(trans, root, inode);

commit 9c8e63db1de98c5cc3c6fb32d11b5cf55f228601
Author: Josef Bacik <jbacik@fb.com>
Date:   Fri Sep 2 15:40:06 2016 -0400

    Btrfs: kill BUG_ON()'s in btrfs_mark_extent_written
    
    No reason to bug on in here, fs corruption could easily cause these things to
    happen.
    
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index f1f1ae6ff08b..72a180d3503e 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1110,13 +1110,25 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 
 	leaf = path->nodes[0];
 	btrfs_item_key_to_cpu(leaf, &key, path->slots[0]);
-	BUG_ON(key.objectid != ino || key.type != BTRFS_EXTENT_DATA_KEY);
+	if (key.objectid != ino ||
+	    key.type != BTRFS_EXTENT_DATA_KEY) {
+		ret = -EINVAL;
+		btrfs_abort_transaction(trans, ret);
+		goto out;
+	}
 	fi = btrfs_item_ptr(leaf, path->slots[0],
 			    struct btrfs_file_extent_item);
-	BUG_ON(btrfs_file_extent_type(leaf, fi) !=
-	       BTRFS_FILE_EXTENT_PREALLOC);
+	if (btrfs_file_extent_type(leaf, fi) != BTRFS_FILE_EXTENT_PREALLOC) {
+		ret = -EINVAL;
+		btrfs_abort_transaction(trans, ret);
+		goto out;
+	}
 	extent_end = key.offset + btrfs_file_extent_num_bytes(leaf, fi);
-	BUG_ON(key.offset > start || extent_end < end);
+	if (key.offset > start || extent_end < end) {
+		ret = -EINVAL;
+		btrfs_abort_transaction(trans, ret);
+		goto out;
+	}
 
 	bytenr = btrfs_file_extent_disk_bytenr(leaf, fi);
 	num_bytes = btrfs_file_extent_disk_num_bytes(leaf, fi);
@@ -1213,12 +1225,19 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 		ret = btrfs_inc_extent_ref(trans, root, bytenr, num_bytes, 0,
 					   root->root_key.objectid,
 					   ino, orig_offset);
-		BUG_ON(ret); /* -ENOMEM */
+		if (ret) {
+			btrfs_abort_transaction(trans, ret);
+			goto out;
+		}
 
 		if (split == start) {
 			key.offset = start;
 		} else {
-			BUG_ON(start != key.offset);
+			if (start != key.offset) {
+				ret = -EINVAL;
+				btrfs_abort_transaction(trans, ret);
+				goto out;
+			}
 			path->slots[0]--;
 			extent_end = end;
 		}
@@ -1240,7 +1259,10 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 		ret = btrfs_free_extent(trans, root, bytenr, num_bytes,
 					0, root->root_key.objectid,
 					ino, orig_offset);
-		BUG_ON(ret); /* -ENOMEM */
+		if (ret) {
+			btrfs_abort_transaction(trans, ret);
+			goto out;
+		}
 	}
 	other_start = 0;
 	other_end = start;
@@ -1257,7 +1279,10 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 		ret = btrfs_free_extent(trans, root, bytenr, num_bytes,
 					0, root->root_key.objectid,
 					ino, orig_offset);
-		BUG_ON(ret); /* -ENOMEM */
+		if (ret) {
+			btrfs_abort_transaction(trans, ret);
+			goto out;
+		}
 	}
 	if (del_nr == 0) {
 		fi = btrfs_item_ptr(leaf, path->slots[0],

commit ba8b04c1d4adbc66f3653e3de5bd6c74a9a003bf
Author: Qu Wenruo <quwenruo@cn.fujitsu.com>
Date:   Tue Jul 19 16:50:36 2016 +0800

    btrfs: extend btrfs_set_extent_delalloc and its friends to support in-band dedupe and subpage size patchset
    
    Extend btrfs_set_extent_delalloc() and extent_clear_unlock_delalloc()
    parameters for both in-band dedupe and subpage sector size patchset.
    
    This should reduce conflict of both patchset and the effort to rebase
    them.
    
    Cc: Chandan Rajendra <chandan@linux.vnet.ibm.com>
    Cc: David Sterba <dsterba@suse.cz>
    Signed-off-by: Qu Wenruo <quwenruo@cn.fujitsu.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index fea31a4a6e36..f1f1ae6ff08b 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -503,7 +503,7 @@ int btrfs_dirty_pages(struct btrfs_root *root, struct inode *inode,
 
 	end_of_last_block = start_pos + num_bytes - 1;
 	err = btrfs_set_extent_delalloc(inode, start_pos, end_of_last_block,
-					cached);
+					cached, 0);
 	if (err)
 		return err;
 

commit f0312210010bf063c29efe112b0d9accbc9191b3
Author: Miklos Szeredi <mszeredi@redhat.com>
Date:   Fri Sep 16 12:44:21 2016 +0200

    btrfs: use filemap_check_errors()
    
    Signed-off-by: Miklos Szeredi <mszeredi@redhat.com>
    Reviewed-by: Omar Sandoval <osandov@fb.com>
    Cc: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index fea31a4a6e36..4843cb994835 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2040,7 +2040,7 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 		 * flags for any errors that might have happened while doing
 		 * writeback of file data.
 		 */
-		ret = btrfs_inode_check_errors(inode);
+		ret = filemap_check_errors(inode->i_mapping);
 		inode_unlock(inode);
 		goto out;
 	}

commit 28a235931b56d4e7bdd51f6733daf95f2b269da8
Author: Filipe Manana <fdmanana@suse.com>
Date:   Tue Aug 23 21:13:51 2016 +0100

    Btrfs: fix lockdep warning on deadlock against an inode's log mutex
    
    Commit 44f714dae50a ("Btrfs: improve performance on fsync against new
    inode after rename/unlink"), which landed in 4.8-rc2, introduced a
    possibility for a deadlock due to double locking of an inode's log mutex
    by the same task, which lockdep reports with:
    
    [23045.433975] =============================================
    [23045.434748] [ INFO: possible recursive locking detected ]
    [23045.435426] 4.7.0-rc6-btrfs-next-34+ #1 Not tainted
    [23045.436044] ---------------------------------------------
    [23045.436044] xfs_io/3688 is trying to acquire lock:
    [23045.436044]  (&ei->log_mutex){+.+...}, at: [<ffffffffa038552d>] btrfs_log_inode+0x13a/0xc95 [btrfs]
    [23045.436044]
                   but task is already holding lock:
    [23045.436044]  (&ei->log_mutex){+.+...}, at: [<ffffffffa038552d>] btrfs_log_inode+0x13a/0xc95 [btrfs]
    [23045.436044]
                   other info that might help us debug this:
    [23045.436044]  Possible unsafe locking scenario:
    
    [23045.436044]        CPU0
    [23045.436044]        ----
    [23045.436044]   lock(&ei->log_mutex);
    [23045.436044]   lock(&ei->log_mutex);
    [23045.436044]
                    *** DEADLOCK ***
    
    [23045.436044]  May be due to missing lock nesting notation
    
    [23045.436044] 3 locks held by xfs_io/3688:
    [23045.436044]  #0:  (&sb->s_type->i_mutex_key#15){+.+...}, at: [<ffffffffa035f2ae>] btrfs_sync_file+0x14e/0x425 [btrfs]
    [23045.436044]  #1:  (sb_internal#2){.+.+.+}, at: [<ffffffff8118446b>] __sb_start_write+0x5f/0xb0
    [23045.436044]  #2:  (&ei->log_mutex){+.+...}, at: [<ffffffffa038552d>] btrfs_log_inode+0x13a/0xc95 [btrfs]
    [23045.436044]
                   stack backtrace:
    [23045.436044] CPU: 4 PID: 3688 Comm: xfs_io Not tainted 4.7.0-rc6-btrfs-next-34+ #1
    [23045.436044] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS rel-1.9.1-0-gb3ef39f-prebuilt.qemu-project.org 04/01/2014
    [23045.436044]  0000000000000000 ffff88022f5f7860 ffffffff8127074d ffffffff82a54b70
    [23045.436044]  ffffffff82a54b70 ffff88022f5f7920 ffffffff81092897 ffff880228015d68
    [23045.436044]  0000000000000000 ffffffff82a54b70 ffffffff829c3f00 ffff880228015d68
    [23045.436044] Call Trace:
    [23045.436044]  [<ffffffff8127074d>] dump_stack+0x67/0x90
    [23045.436044]  [<ffffffff81092897>] __lock_acquire+0xcbb/0xe4e
    [23045.436044]  [<ffffffff8109155f>] ? mark_lock+0x24/0x201
    [23045.436044]  [<ffffffff8109179a>] ? mark_held_locks+0x5e/0x74
    [23045.436044]  [<ffffffff81092de0>] lock_acquire+0x12f/0x1c3
    [23045.436044]  [<ffffffff81092de0>] ? lock_acquire+0x12f/0x1c3
    [23045.436044]  [<ffffffffa038552d>] ? btrfs_log_inode+0x13a/0xc95 [btrfs]
    [23045.436044]  [<ffffffffa038552d>] ? btrfs_log_inode+0x13a/0xc95 [btrfs]
    [23045.436044]  [<ffffffff814a51a4>] mutex_lock_nested+0x77/0x3a7
    [23045.436044]  [<ffffffffa038552d>] ? btrfs_log_inode+0x13a/0xc95 [btrfs]
    [23045.436044]  [<ffffffffa039705e>] ? btrfs_release_delayed_node+0xb/0xd [btrfs]
    [23045.436044]  [<ffffffffa038552d>] btrfs_log_inode+0x13a/0xc95 [btrfs]
    [23045.436044]  [<ffffffffa038552d>] ? btrfs_log_inode+0x13a/0xc95 [btrfs]
    [23045.436044]  [<ffffffff810a0ed1>] ? vprintk_emit+0x453/0x465
    [23045.436044]  [<ffffffffa0385a61>] btrfs_log_inode+0x66e/0xc95 [btrfs]
    [23045.436044]  [<ffffffffa03c084d>] log_new_dir_dentries+0x26c/0x359 [btrfs]
    [23045.436044]  [<ffffffffa03865aa>] btrfs_log_inode_parent+0x4a6/0x628 [btrfs]
    [23045.436044]  [<ffffffffa0387552>] btrfs_log_dentry_safe+0x5a/0x75 [btrfs]
    [23045.436044]  [<ffffffffa035f464>] btrfs_sync_file+0x304/0x425 [btrfs]
    [23045.436044]  [<ffffffff811acaf4>] vfs_fsync_range+0x8c/0x9e
    [23045.436044]  [<ffffffff811acb22>] vfs_fsync+0x1c/0x1e
    [23045.436044]  [<ffffffff811acc79>] do_fsync+0x31/0x4a
    [23045.436044]  [<ffffffff811ace99>] SyS_fsync+0x10/0x14
    [23045.436044]  [<ffffffff814a88e5>] entry_SYSCALL_64_fastpath+0x18/0xa8
    [23045.436044]  [<ffffffff8108f039>] ? trace_hardirqs_off_caller+0x3f/0xaa
    
    An example reproducer for this is:
    
       $ mkfs.btrfs -f /dev/sdb
       $ mount /dev/sdb /mnt
       $ mkdir /mnt/dir
       $ touch /mnt/dir/foo
       $ sync
       $ mv /mnt/dir/foo /mnt/dir/bar
       $ touch /mnt/dir/foo
       $ xfs_io -c "fsync" /mnt/dir/bar
    
    This is because while logging the inode of file bar we end up logging its
    parent directory (since its inode has an unlink_trans field matching the
    current transaction id due to the rename operation), which in turn logs
    the inodes for all its new dentries, so that the new inode for the new
    file named foo gets logged which in turn triggered another logging attempt
    for the inode we are fsync'ing, since that inode had an old name that
    corresponds to the name of the new inode.
    
    So fix this by ensuring that when logging the inode for a new dentry that
    has a name matching an old name of some other inode, we don't log again
    the original inode that we are fsync'ing.
    
    Fixes: 44f714dae50a ("Btrfs: improve performance on fsync against new inode after rename/unlink")
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 3391f2adf0c8..fea31a4a6e36 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2070,7 +2070,7 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 	}
 	trans->sync = true;
 
-	btrfs_init_log_ctx(&ctx);
+	btrfs_init_log_ctx(&ctx, inode);
 
 	ret = btrfs_log_dentry_safe(trans, root, dentry, start, end, &ctx);
 	if (ret < 0) {

commit 18513091af9483ba84328d42092bd4d42a3c958f
Author: Wang Xiaoguang <wangxg.fnst@cn.fujitsu.com>
Date:   Mon Jul 25 15:51:40 2016 +0800

    btrfs: update btrfs_space_info's bytes_may_use timely
    
    This patch can fix some false ENOSPC errors, below test script can
    reproduce one false ENOSPC error:
            #!/bin/bash
            dd if=/dev/zero of=fs.img bs=$((1024*1024)) count=128
            dev=$(losetup --show -f fs.img)
            mkfs.btrfs -f -M $dev
            mkdir /tmp/mntpoint
            mount $dev /tmp/mntpoint
            cd /tmp/mntpoint
            xfs_io -f -c "falloc 0 $((64*1024*1024))" testfile
    
    Above script will fail for ENOSPC reason, but indeed fs still has free
    space to satisfy this request. Please see call graph:
    btrfs_fallocate()
    |-> btrfs_alloc_data_chunk_ondemand()
    |   bytes_may_use += 64M
    |-> btrfs_prealloc_file_range()
        |-> btrfs_reserve_extent()
            |-> btrfs_add_reserved_bytes()
            |   alloc_type is RESERVE_ALLOC_NO_ACCOUNT, so it does not
            |   change bytes_may_use, and bytes_reserved += 64M. Now
            |   bytes_may_use + bytes_reserved == 128M, which is greater
            |   than btrfs_space_info's total_bytes, false enospc occurs.
            |   Note, the bytes_may_use decrease operation will be done in
            |   end of btrfs_fallocate(), which is too late.
    
    Here is another simple case for buffered write:
                        CPU 1              |              CPU 2
                                           |
    |-> cow_file_range()                   |-> __btrfs_buffered_write()
        |-> btrfs_reserve_extent()         |   |
        |                                  |   |
        |                                  |   |
        |    .....                         |   |-> btrfs_check_data_free_space()
        |                                  |
        |                                  |
        |-> extent_clear_unlock_delalloc() |
    
    In CPU 1, btrfs_reserve_extent()->find_free_extent()->
    btrfs_add_reserved_bytes() do not decrease bytes_may_use, the decrease
    operation will be delayed to be done in extent_clear_unlock_delalloc().
    Assume in this case, btrfs_reserve_extent() reserved 128MB data, CPU2's
    btrfs_check_data_free_space() tries to reserve 100MB data space.
    If
            100MB > data_sinfo->total_bytes - data_sinfo->bytes_used -
                    data_sinfo->bytes_reserved - data_sinfo->bytes_pinned -
                    data_sinfo->bytes_readonly - data_sinfo->bytes_may_use
    btrfs_check_data_free_space() will try to allcate new data chunk or call
    btrfs_start_delalloc_roots(), or commit current transaction in order to
    reserve some free space, obviously a lot of work. But indeed it's not
    necessary as long as decreasing bytes_may_use timely, we still have
    free space, decreasing 128M from bytes_may_use.
    
    To fix this issue, this patch chooses to update bytes_may_use for both
    data and metadata in btrfs_add_reserved_bytes(). For compress path, real
    extent length may not be equal to file content length, so introduce a
    ram_bytes argument for btrfs_reserve_extent(), find_free_extent() and
    btrfs_add_reserved_bytes(), it's becasue bytes_may_use is increased by
    file content length. Then compress path can update bytes_may_use
    correctly. Also now we can discard RESERVE_ALLOC_NO_ACCOUNT, RESERVE_ALLOC
    and RESERVE_FREE.
    
    As we know, usually EXTENT_DO_ACCOUNTING is used for error path. In
    run_delalloc_nocow(), for inode marked as NODATACOW or extent marked as
    PREALLOC, we also need to update bytes_may_use, but can not pass
    EXTENT_DO_ACCOUNTING, because it also clears metadata reservation, so
    here we introduce EXTENT_CLEAR_DATA_RESV flag to indicate btrfs_clear_bit_hook()
    to update btrfs_space_info's bytes_may_use.
    
    Meanwhile __btrfs_prealloc_file_range() will call
    btrfs_free_reserved_data_space() internally for both sucessful and failed
    path, btrfs_prealloc_file_range()'s callers does not need to call
    btrfs_free_reserved_data_space() any more.
    
    Signed-off-by: Wang Xiaoguang <wangxg.fnst@cn.fujitsu.com>
    Reviewed-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: David Sterba <dsterba@suse.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 5842423f8f47..3391f2adf0c8 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2675,6 +2675,7 @@ static long btrfs_fallocate(struct file *file, int mode,
 
 	alloc_start = round_down(offset, blocksize);
 	alloc_end = round_up(offset + len, blocksize);
+	cur_offset = alloc_start;
 
 	/* Make sure we aren't being give some crap mode */
 	if (mode & ~(FALLOC_FL_KEEP_SIZE | FALLOC_FL_PUNCH_HOLE))
@@ -2767,7 +2768,6 @@ static long btrfs_fallocate(struct file *file, int mode,
 
 	/* First, check if we exceed the qgroup limit */
 	INIT_LIST_HEAD(&reserve_list);
-	cur_offset = alloc_start;
 	while (1) {
 		em = btrfs_get_extent(inode, NULL, 0, cur_offset,
 				      alloc_end - cur_offset, 0);
@@ -2794,6 +2794,14 @@ static long btrfs_fallocate(struct file *file, int mode,
 					last_byte - cur_offset);
 			if (ret < 0)
 				break;
+		} else {
+			/*
+			 * Do not need to reserve unwritten extent for this
+			 * range, free reserved data space first, otherwise
+			 * it'll result in false ENOSPC error.
+			 */
+			btrfs_free_reserved_data_space(inode, cur_offset,
+				last_byte - cur_offset);
 		}
 		free_extent_map(em);
 		cur_offset = last_byte;
@@ -2811,6 +2819,9 @@ static long btrfs_fallocate(struct file *file, int mode,
 					range->start,
 					range->len, 1 << inode->i_blkbits,
 					offset + len, &alloc_hint);
+		else
+			btrfs_free_reserved_data_space(inode, range->start,
+						       range->len);
 		list_del(&range->list);
 		kfree(range);
 	}
@@ -2845,18 +2856,11 @@ static long btrfs_fallocate(struct file *file, int mode,
 	unlock_extent_cached(&BTRFS_I(inode)->io_tree, alloc_start, locked_end,
 			     &cached_state, GFP_KERNEL);
 out:
-	/*
-	 * As we waited the extent range, the data_rsv_map must be empty
-	 * in the range, as written data range will be released from it.
-	 * And for prealloacted extent, it will also be released when
-	 * its metadata is written.
-	 * So this is completely used as cleanup.
-	 */
-	btrfs_qgroup_free_data(inode, alloc_start, alloc_end - alloc_start);
 	inode_unlock(inode);
 	/* Let go of our reservation. */
-	btrfs_free_reserved_data_space(inode, alloc_start,
-				       alloc_end - alloc_start);
+	if (ret != 0)
+		btrfs_free_reserved_data_space(inode, alloc_start,
+				       alloc_end - cur_offset);
 	return ret;
 }
 

commit 10838816547a28696ca10e038b3b32f2efec5a42
Merge: 42049bf60db4 e6571499336e
Author: Chris Mason <clm@fb.com>
Date:   Fri Aug 5 12:25:05 2016 -0700

    Merge branch 'integration-4.8' of git://git.kernel.org/pub/scm/linux/kernel/git/fdmanana/linux into for-linus-4.8

commit 0596a9048bf2aca2a74b312493f39e4d5ac3b653
Author: Filipe Manana <fdmanana@suse.com>
Date:   Tue Jun 14 14:18:27 2016 +0100

    Btrfs: add missing check for writeback errors on fsync
    
    When we start an fsync we start ordered extents for all delalloc ranges.
    However before attempting to log the inode, we only wait for those ordered
    extents if we are not doing a full sync (bit BTRFS_INODE_NEEDS_FULL_SYNC
    is set in the inode's flags). This means that if an ordered extent
    completes with an IO error before we check if we can skip logging the
    inode, we will not catch and report the IO error to user space. This is
    because on an IO error, when the ordered extent completes we do not
    update the inode, so if the inode was not previously updated by the
    current transaction we end up not logging it through calls to fsync and
    therefore not check its mapping flags for the presence of IO errors.
    
    Fix this by checking for errors in the flags of the inode's mapping when
    we notice we can skip logging the inode.
    
    This caused sporadic failures in the test generic/331 (which explicitly
    tests for IO errors during an fsync call).
    
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Reviewed-by: Liu Bo <bo.li.liu@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index bcfb4a27ddd4..6f049109fffe 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2033,6 +2033,14 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 		 */
 		clear_bit(BTRFS_INODE_NEEDS_FULL_SYNC,
 			  &BTRFS_I(inode)->runtime_flags);
+		/*
+		 * An ordered extent might have started before and completed
+		 * already with io errors, in which case the inode was not
+		 * updated and we end up here. So check the inode's mapping
+		 * flags for any errors that might have happened while doing
+		 * writeback of file data.
+		 */
+		ret = btrfs_inode_check_errors(inode);
 		inode_unlock(inode);
 		goto out;
 	}

commit 66642832f06a4351e23cea6cf254967c227f8224
Author: Jeff Mahoney <jeffm@suse.com>
Date:   Fri Jun 10 18:19:25 2016 -0400

    btrfs: btrfs_abort_transaction, drop root parameter
    
    __btrfs_abort_transaction doesn't use its root parameter except to
    obtain an fs_info pointer.  We can obtain that from trans->root->fs_info
    for now and from trans->fs_info in a later patch.
    
    Signed-off-by: Jeff Mahoney <jeffm@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 48b94904a519..9404121fd5f7 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -950,7 +950,7 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 			ret = btrfs_del_items(trans, root, path, del_slot,
 					      del_nr);
 			if (ret) {
-				btrfs_abort_transaction(trans, root, ret);
+				btrfs_abort_transaction(trans, ret);
 				break;
 			}
 
@@ -974,7 +974,7 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 		path->slots[0] = del_slot;
 		ret = btrfs_del_items(trans, root, path, del_slot, del_nr);
 		if (ret)
-			btrfs_abort_transaction(trans, root, ret);
+			btrfs_abort_transaction(trans, ret);
 	}
 
 	leaf = path->nodes[0];
@@ -1190,7 +1190,7 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 			goto again;
 		}
 		if (ret < 0) {
-			btrfs_abort_transaction(trans, root, ret);
+			btrfs_abort_transaction(trans, ret);
 			goto out;
 		}
 
@@ -1278,7 +1278,7 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 
 		ret = btrfs_del_items(trans, root, path, del_slot, del_nr);
 		if (ret < 0) {
-			btrfs_abort_transaction(trans, root, ret);
+			btrfs_abort_transaction(trans, ret);
 			goto out;
 		}
 	}

commit 3cdde2240d4533ff71fbb8dc9c32d5d57d3cdeed
Author: Jeff Mahoney <jeffm@suse.com>
Date:   Thu Jun 9 21:38:35 2016 -0400

    btrfs: btrfs_test_opt and friends should take a btrfs_fs_info
    
    btrfs_test_opt and friends only use the root pointer to access
    the fs_info.  Let's pass the fs_info directly in preparation to
    eliminate similar patterns all over btrfs.
    
    Signed-off-by: Jeff Mahoney <jeffm@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 38bea7bb0987..48b94904a519 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -132,7 +132,7 @@ static int __btrfs_add_inode_defrag(struct inode *inode,
 
 static inline int __need_auto_defrag(struct btrfs_root *root)
 {
-	if (!btrfs_test_opt(root, AUTO_DEFRAG))
+	if (!btrfs_test_opt(root->fs_info, AUTO_DEFRAG))
 		return 0;
 
 	if (btrfs_fs_closing(root->fs_info))

commit fba4b697710eb2a4bee456b9d39e9239c66f8bee
Author: Nikolay Borisov <n.borisov.lkml@gmail.com>
Date:   Thu Jun 23 21:17:08 2016 +0300

    btrfs: Fix slab accounting flags
    
    BTRFS is using a variety of slab caches to satisfy internal needs.
    Those slab caches are always allocated with the SLAB_RECLAIM_ACCOUNT,
    meaning allocations from the caches are going to be accounted as
    SReclaimable. At the same time btrfs is not registering any shrinkers
    whatsoever, thus preventing memory from the slabs to be shrunk. This
    means those caches are not in fact reclaimable.
    
    To fix this remove the SLAB_RECLAIM_ACCOUNT on all caches apart from the
    inode cache, since this one is being freed by the generic VFS super_block
    shrinker. Also set the transaction related caches as SLAB_TEMPORARY,
    to better document the lifetime of the objects (it just translates
    to SLAB_RECLAIM_ACCOUNT).
    
    Signed-off-by: Nikolay Borisov <n.borisov.lkml@gmail.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index bcfb4a27ddd4..38bea7bb0987 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2975,7 +2975,7 @@ int btrfs_auto_defrag_init(void)
 {
 	btrfs_inode_defrag_cachep = kmem_cache_create("btrfs_inode_defrag",
 					sizeof(struct inode_defrag), 0,
-					SLAB_RECLAIM_ACCOUNT | SLAB_MEM_SPREAD,
+					SLAB_MEM_SPREAD,
 					NULL);
 	if (!btrfs_inode_defrag_cachep)
 		return -ENOMEM;

commit 8b8b08cbfb9021af4b54b4175fc4c51d655aac8c
Author: Chris Mason <clm@fb.com>
Date:   Tue Jul 19 05:52:36 2016 -0700

    Btrfs: fix delalloc accounting after copy_from_user faults
    
    Commit 56244ef151c3cd11 was almost but not quite enough to fix the
    reservation math after btrfs_copy_from_user returned partial copies.
    
    Some users are still seeing warnings in btrfs_destroy_inode, and with a
    long enough test run I'm able to trigger them as well.
    
    This patch fixes the accounting math again, bringing it much closer to
    the way it was before the sectorsize conversion Chandan did.  The
    problem is accounting for the offset into the page/sector when we do a
    partial copy.  This one just uses the dirty_sectors variable which
    should already be updated properly.
    
    Signed-off-by: Chris Mason <clm@fb.com>
    cc: stable@vger.kernel.org # v4.6+

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index f3f61d1ad18a..bcfb4a27ddd4 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1629,13 +1629,11 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 		 * managed to copy.
 		 */
 		if (num_sectors > dirty_sectors) {
-			/*
-			 * we round down because we don't want to count
-			 * any partial blocks actually sent through the
-			 * IO machines
-			 */
-			release_bytes = round_down(release_bytes - copied,
-				      root->sectorsize);
+
+			/* release everything except the sectors we dirtied */
+			release_bytes -= dirty_sectors <<
+				root->fs_info->sb->s_blocksize_bits;
+
 			if (copied > 0) {
 				spin_lock(&BTRFS_I(inode)->lock);
 				BTRFS_I(inode)->outstanding_extents++;

commit 25d609f86d6808eb1f0e8a6cafc3edb4a2b5ae35
Author: Josef Bacik <jbacik@fb.com>
Date:   Fri Mar 25 13:25:48 2016 -0400

    Btrfs: fix callers of btrfs_block_rsv_migrate
    
    So btrfs_block_rsv_migrate just unconditionally calls block_rsv_migrate_bytes.
    Not only this but it unconditionally changes the size of the block_rsv.  This
    isn't a bug strictly speaking, but it makes truncate block rsv's look funny
    because every time we migrate bytes over its size grows, even though we only
    want it to be a specific size.  So collapse this into one function that takes an
    update_size argument and make truncate and evict not update the size for
    consistency sake.  Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 2234e88cf674..f3f61d1ad18a 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2479,7 +2479,7 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 	}
 
 	ret = btrfs_block_rsv_migrate(&root->fs_info->trans_block_rsv, rsv,
-				      min_size);
+				      min_size, 0);
 	BUG_ON(ret);
 	trans->block_rsv = rsv;
 
@@ -2522,7 +2522,7 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 		}
 
 		ret = btrfs_block_rsv_migrate(&root->fs_info->trans_block_rsv,
-					      rsv, min_size);
+					      rsv, min_size, 0);
 		BUG_ON(ret);	/* shouldn't happen */
 		trans->block_rsv = rsv;
 

commit b971712afc8dd0de38e943ea9d904fe8c8aff956
Merge: ca83a55c9fe2 b7f67055d2df
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jun 25 08:42:31 2016 -0700

    Merge branch 'for-linus-4.7' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs
    
    Pull btrfs fixes from Chris Mason:
     "I have a two part pull this time because one of the patches Dave
      Sterba collected needed to be against v4.7-rc2 or higher (we used
      rc4).  I try to make my for-linus-xx branch testable on top of the
      last major so we can hand fixes to people on the list more easily, so
      I've split this pull in two.
    
      This first part has some fixes and two performance improvements that
      we've been testing for some time.
    
      Josef's two performance fixes are most notable.  The transid tracking
      patch makes a big improvement on pretty much every workload"
    
    * 'for-linus-4.7' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs:
      Btrfs: Force stripesize to the value of sectorsize
      btrfs: fix disk_i_size update bug when fallocate() fails
      Btrfs: fix error handling in map_private_extent_buffer
      Btrfs: fix error return code in btrfs_init_test_fs()
      Btrfs: don't do nocow check unless we have to
      btrfs: fix deadlock in delayed_ref_async_start
      Btrfs: track transid for delayed ref flushing

commit c6887cd11149d7325328749f06719071e6c725c6
Author: Josef Bacik <jbacik@fb.com>
Date:   Fri Mar 25 13:26:00 2016 -0400

    Btrfs: don't do nocow check unless we have to
    
    Before we write into prealloc/nocow space we have to make sure that there are no
    references to the extents we are writing into, which means checking the extent
    tree and csum tree in the case of nocow.  So we don't want to do the nocow dance
    unless we can't reserve data space, since it's a serious drag on performance.
    With the following sequence
    
    fallocate -l10737418240 /mnt/btrfs-test/file
    cp --reflink /mnt/btrfs-test/file /mnt/btrfs-test/link
    fio --name=randwrite --rw=randwrite --bs=4k --filename=/mnt/btrfs-test/file \
            --end_fsync=1
    
    we get the worst case scenario where we have to fall back on to doing the check
    anyway.
    
    Without this patch
    lat (usec): min=5, max=111598, avg=27.65, stdev=124.51
    write: io=10240MB, bw=126876KB/s, iops=31718, runt= 82646msec
    
    With this patch
    lat (usec): min=3, max=91210, avg=14.09, stdev=110.62
    write: io=10240MB, bw=212753KB/s, iops=53188, runt= 49286msec
    
    We get twice the throughput, half of the runtime, and half of the average
    latency.  Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    [ PAGE_CACHE_ removal related fixups ]
    Signed-off-by: David Sterba <dsterba@suse.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 159a93450e26..8a538abb597a 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1534,30 +1534,30 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 		reserve_bytes = round_up(write_bytes + sector_offset,
 				root->sectorsize);
 
-		if ((BTRFS_I(inode)->flags & (BTRFS_INODE_NODATACOW |
-					      BTRFS_INODE_PREALLOC)) &&
-		    check_can_nocow(inode, pos, &write_bytes) > 0) {
-			/*
-			 * For nodata cow case, no need to reserve
-			 * data space.
-			 */
-			only_release_metadata = true;
-			/*
-			 * our prealloc extent may be smaller than
-			 * write_bytes, so scale down.
-			 */
-			num_pages = DIV_ROUND_UP(write_bytes + offset,
-						 PAGE_SIZE);
-			reserve_bytes = round_up(write_bytes + sector_offset,
-					root->sectorsize);
-			goto reserve_metadata;
-		}
-
 		ret = btrfs_check_data_free_space(inode, pos, write_bytes);
-		if (ret < 0)
-			break;
+		if (ret < 0) {
+			if ((BTRFS_I(inode)->flags & (BTRFS_INODE_NODATACOW |
+						      BTRFS_INODE_PREALLOC)) &&
+			    check_can_nocow(inode, pos, &write_bytes) > 0) {
+				/*
+				 * For nodata cow case, no need to reserve
+				 * data space.
+				 */
+				only_release_metadata = true;
+				/*
+				 * our prealloc extent may be smaller than
+				 * write_bytes, so scale down.
+				 */
+				num_pages = DIV_ROUND_UP(write_bytes + offset,
+							 PAGE_SIZE);
+				reserve_bytes = round_up(write_bytes +
+							 sector_offset,
+							 root->sectorsize);
+			} else {
+				break;
+			}
+		}
 
-reserve_metadata:
 		ret = btrfs_delalloc_reserve_metadata(inode, reserve_bytes);
 		if (ret) {
 			if (!only_release_metadata)

commit 559b6d90a0beb375c46dffe18133012bfa29f441
Merge: aa00edc1287a 56244ef151c3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri May 27 16:37:36 2016 -0700

    Merge branch 'for-linus-4.7' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs
    
    Pull btrfs cleanups and fixes from Chris Mason:
     "We have another round of fixes and a few cleanups.
    
      I have a fix for short returns from btrfs_copy_from_user, which
      finally nails down a very hard to find regression we added in v4.6.
    
      Dave is pushing around gfp parameters, mostly to cleanup internal apis
      and make it a little more consistent.
    
      The rest are smaller fixes, and one speelling fixup patch"
    
    * 'for-linus-4.7' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs: (22 commits)
      Btrfs: fix handling of faults from btrfs_copy_from_user
      btrfs: fix string and comment grammatical issues and typos
      btrfs: scrub: Set bbio to NULL before calling btrfs_map_block
      Btrfs: fix unexpected return value of fiemap
      Btrfs: free sys_array eb as soon as possible
      btrfs: sink gfp parameter to convert_extent_bit
      btrfs: make state preallocation more speculative in __set_extent_bit
      btrfs: untangle gotos a bit in convert_extent_bit
      btrfs: untangle gotos a bit in __clear_extent_bit
      btrfs: untangle gotos a bit in __set_extent_bit
      btrfs: sink gfp parameter to set_record_extent_bits
      btrfs: sink gfp parameter to set_extent_new
      btrfs: sink gfp parameter to set_extent_defrag
      btrfs: sink gfp parameter to set_extent_delalloc
      btrfs: sink gfp parameter to clear_extent_dirty
      btrfs: sink gfp parameter to clear_record_extent_bits
      btrfs: sink gfp parameter to clear_extent_bits
      btrfs: sink gfp parameter to set_extent_bits
      btrfs: make find_workspace warn if there are no workspaces
      btrfs: make find_workspace always succeed
      ...

commit 56244ef151c3cd11f505020ab0b3f45454363bcc
Author: Chris Mason <clm@fb.com>
Date:   Mon May 16 09:21:01 2016 -0700

    Btrfs: fix handling of faults from btrfs_copy_from_user
    
    When btrfs_copy_from_user isn't able to copy all of the pages, we need
    to adjust our accounting to reflect the work that was actually done.
    
    Commit 2e78c927d79 changed around the decisions a little and we ended up
    skipping the accounting adjustments some of the time.  This commit makes
    sure that when we don't copy anything at all, we still hop into
    the adjustments, and switches to release_bytes instead of write_bytes,
    since write_bytes isn't aligned.
    
    The accounting errors led to warnings during btrfs_destroy_inode:
    
    [   70.847532] WARNING: CPU: 10 PID: 514 at fs/btrfs/inode.c:9350 btrfs_destroy_inode+0x2b3/0x2c0
    [   70.847536] Modules linked in: i2c_piix4 virtio_net i2c_core input_leds button led_class serio_raw acpi_cpufreq sch_fq_codel autofs4 virtio_blk
    [   70.847538] CPU: 10 PID: 514 Comm: umount Tainted: G        W 4.6.0-rc6_00062_g2997da1-dirty #23
    [   70.847539] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.9.0-1.fc24 04/01/2014
    [   70.847542]  0000000000000000 ffff880ff5cafab8 ffffffff8149d5e9 0000000000000202
    [   70.847543]  0000000000000000 0000000000000000 0000000000000000 ffff880ff5cafb08
    [   70.847547]  ffffffff8107bdfd ffff880ff5cafaf8 000024868120013d ffff880ff5cafb28
    [   70.847547] Call Trace:
    [   70.847550]  [<ffffffff8149d5e9>] dump_stack+0x51/0x78
    [   70.847551]  [<ffffffff8107bdfd>] __warn+0xfd/0x120
    [   70.847553]  [<ffffffff8107be3d>] warn_slowpath_null+0x1d/0x20
    [   70.847555]  [<ffffffff8139c9e3>] btrfs_destroy_inode+0x2b3/0x2c0
    [   70.847556]  [<ffffffff812003a1>] ? __destroy_inode+0x71/0x140
    [   70.847558]  [<ffffffff812004b3>] destroy_inode+0x43/0x70
    [   70.847559]  [<ffffffff810b7b5f>] ? wake_up_bit+0x2f/0x40
    [   70.847560]  [<ffffffff81200c68>] evict+0x148/0x1d0
    [   70.847562]  [<ffffffff81398ade>] ? start_transaction+0x3de/0x460
    [   70.847564]  [<ffffffff81200d49>] dispose_list+0x59/0x80
    [   70.847565]  [<ffffffff81201ba0>] evict_inodes+0x180/0x190
    [   70.847566]  [<ffffffff812191ff>] ? __sync_filesystem+0x3f/0x50
    [   70.847568]  [<ffffffff811e95f8>] generic_shutdown_super+0x48/0x100
    [   70.847569]  [<ffffffff810b75c0>] ? woken_wake_function+0x20/0x20
    [   70.847571]  [<ffffffff811e9796>] kill_anon_super+0x16/0x30
    [   70.847573]  [<ffffffff81365cde>] btrfs_kill_super+0x1e/0x130
    [   70.847574]  [<ffffffff811e99be>] deactivate_locked_super+0x4e/0x90
    [   70.847576]  [<ffffffff811e9e61>] deactivate_super+0x51/0x70
    [   70.847577]  [<ffffffff8120536f>] cleanup_mnt+0x3f/0x80
    [   70.847579]  [<ffffffff81205402>] __cleanup_mnt+0x12/0x20
    [   70.847581]  [<ffffffff81098358>] task_work_run+0x68/0xa0
    [   70.847582]  [<ffffffff810022b6>] exit_to_usermode_loop+0xd6/0xe0
    [   70.847583]  [<ffffffff81002e1d>] do_syscall_64+0xbd/0x170
    [   70.847586]  [<ffffffff817d4dbc>] entry_SYSCALL64_slow_path+0x25/0x25
    
    This is the test program I used to force short returns from
    btrfs_copy_from_user
    
    void *dontneed(void *arg)
    {
            char *p = arg;
            int ret;
    
            while(1) {
                    ret = madvise(p, BUFSIZE/4, MADV_DONTNEED);
                    if (ret) {
                            perror("madvise");
                            exit(1);
                    }
            }
    }
    
    int main(int ac, char **av) {
            int ret;
            int fd;
            char *filename;
            unsigned long offset;
            char *buf;
            int i;
            pthread_t tid;
    
            if (ac != 2) {
                    fprintf(stderr, "usage: dammitdave filename\n");
                    exit(1);
            }
    
            buf = mmap(NULL, BUFSIZE, PROT_READ|PROT_WRITE,
                       MAP_PRIVATE|MAP_ANONYMOUS, -1, 0);
            if (buf == MAP_FAILED) {
                    perror("mmap");
                    exit(1);
            }
            memset(buf, 'a', BUFSIZE);
            filename = av[1];
    
            ret = pthread_create(&tid, NULL, dontneed, buf);
            if (ret) {
                    fprintf(stderr, "error %d from pthread_create\n", ret);
                    exit(1);
            }
    
            ret = pthread_detach(tid);
            if (ret) {
                    fprintf(stderr, "pthread detach failed %d\n", ret);
                    exit(1);
            }
    
            while (1) {
                    fd = open(filename, O_RDWR | O_CREAT, 0600);
                    if (fd < 0) {
                            perror("open");
                            exit(1);
                    }
    
                    for (i = 0; i < ROUNDS; i++) {
                            int this_write = BUFSIZE;
    
                            offset = rand() % MAXSIZE;
                            ret = pwrite(fd, buf, this_write, offset);
                            if (ret < 0) {
                                    perror("pwrite");
                                    exit(1);
                            } else if (ret != this_write) {
                                    fprintf(stderr, "short write to %s offset %lu ret %d\n",
                                            filename, offset, ret);
                                    exit(1);
                            }
                            if (i == ROUNDS - 1) {
                                    ret = sync_file_range(fd, offset, 4096,
                                        SYNC_FILE_RANGE_WRITE);
                                    if (ret < 0) {
                                            perror("sync_file_range");
                                            exit(1);
                                    }
                            }
                    }
                    ret = ftruncate(fd, 0);
                    if (ret < 0) {
                            perror("ftruncate");
                            exit(1);
                    }
                    ret = close(fd);
                    if (ret) {
                            perror("close");
                            exit(1);
                    }
                    ret = unlink(filename);
                    if (ret) {
                            perror("unlink");
                            exit(1);
                    }
    
            }
            return 0;
    }
    
    Signed-off-by: Chris Mason <clm@fb.com>
    Reported-by: Dave Jones <dsj@fb.com>
    Fixes: 2e78c927d79333f299a8ac81c2fd2952caeef335
    cc: stable@vger.kernel.org # v4.6
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 1933e8b07ed7..159a93450e26 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1596,6 +1596,13 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 
 		copied = btrfs_copy_from_user(pos, write_bytes, pages, i);
 
+		num_sectors = BTRFS_BYTES_TO_BLKS(root->fs_info,
+						reserve_bytes);
+		dirty_sectors = round_up(copied + sector_offset,
+					root->sectorsize);
+		dirty_sectors = BTRFS_BYTES_TO_BLKS(root->fs_info,
+						dirty_sectors);
+
 		/*
 		 * if we have trouble faulting in the pages, fall
 		 * back to one page at a time
@@ -1605,6 +1612,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 
 		if (copied == 0) {
 			force_page_uptodate = true;
+			dirty_sectors = 0;
 			dirty_pages = 0;
 		} else {
 			force_page_uptodate = false;
@@ -1615,20 +1623,19 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 		/*
 		 * If we had a short copy we need to release the excess delaloc
 		 * bytes we reserved.  We need to increment outstanding_extents
-		 * because btrfs_delalloc_release_space will decrement it, but
+		 * because btrfs_delalloc_release_space and
+		 * btrfs_delalloc_release_metadata will decrement it, but
 		 * we still have an outstanding extent for the chunk we actually
 		 * managed to copy.
 		 */
-		num_sectors = BTRFS_BYTES_TO_BLKS(root->fs_info,
-						reserve_bytes);
-		dirty_sectors = round_up(copied + sector_offset,
-					root->sectorsize);
-		dirty_sectors = BTRFS_BYTES_TO_BLKS(root->fs_info,
-						dirty_sectors);
-
 		if (num_sectors > dirty_sectors) {
-			release_bytes = (write_bytes - copied)
-				& ~((u64)root->sectorsize - 1);
+			/*
+			 * we round down because we don't want to count
+			 * any partial blocks actually sent through the
+			 * IO machines
+			 */
+			release_bytes = round_down(release_bytes - copied,
+				      root->sectorsize);
 			if (copied > 0) {
 				spin_lock(&BTRFS_I(inode)->lock);
 				BTRFS_I(inode)->outstanding_extents++;

commit 42f31734eb7658fd01fb186d56312be869450a42
Merge: e73440868fde 0132761017e0
Author: David Sterba <dsterba@suse.com>
Date:   Wed May 25 22:51:03 2016 +0200

    Merge branch 'cleanups-4.7' into for-chris-4.7-20160525

commit 0132761017e012ab4dc8584d679503f2ba26ca86
Author: Nicholas D Steeves <nsteeves@gmail.com>
Date:   Thu May 19 21:18:45 2016 -0400

    btrfs: fix string and comment grammatical issues and typos
    
    Signed-off-by: Nicholas D Steeves <nsteeves@gmail.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 8d7b5a45c005..50dac40d9561 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2024,7 +2024,7 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 	     BTRFS_I(inode)->last_trans
 	     <= root->fs_info->last_trans_committed)) {
 		/*
-		 * We'v had everything committed since the last time we were
+		 * We've had everything committed since the last time we were
 		 * modified so clear this flag in case it was set for whatever
 		 * reason, it's no longer relevant.
 		 */
@@ -2372,7 +2372,7 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 
 	/* Check the aligned pages after the first unaligned page,
 	 * if offset != orig_start, which means the first unaligned page
-	 * including serveral following pages are already in holes,
+	 * including several following pages are already in holes,
 	 * the extra check can be skipped */
 	if (offset == orig_start) {
 		/* after truncate page, check hole again */

commit 07be1337b9e8bfcd855c6e9175b5066a30ac609b
Merge: 63d222b9d277 c315ef8d9db7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat May 21 10:49:22 2016 -0700

    Merge branch 'for-linus-4.7' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs
    
    Pull btrfs updates from Chris Mason:
     "This has our merge window series of cleanups and fixes.  These target
      a wide range of issues, but do include some important fixes for
      qgroups, O_DIRECT, and fsync handling.  Jeff Mahoney moved around a
      few definitions to make them easier for userland to consume.
    
      Also whiteout support is included now that issues with overlayfs have
      been cleared up.
    
      I have one more fix pending for page faults during btrfs_copy_from_user,
      but I wanted to get this bulk out the door first"
    
    * 'for-linus-4.7' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs: (90 commits)
      btrfs: fix memory leak during RAID 5/6 device replacement
      Btrfs: add semaphore to synchronize direct IO writes with fsync
      Btrfs: fix race between block group relocation and nocow writes
      Btrfs: fix race between fsync and direct IO writes for prealloc extents
      Btrfs: fix number of transaction units for renames with whiteout
      Btrfs: pin logs earlier when doing a rename exchange operation
      Btrfs: unpin logs if rename exchange operation fails
      Btrfs: fix inode leak on failure to setup whiteout inode in rename
      btrfs: add support for RENAME_EXCHANGE and RENAME_WHITEOUT
      Btrfs: pin log earlier when renaming
      Btrfs: unpin log if rename operation fails
      Btrfs: don't do unnecessary delalloc flushes when relocating
      Btrfs: don't wait for unrelated IO to finish before relocation
      Btrfs: fix empty symlink after creating symlink and fsync parent dir
      Btrfs: fix for incorrect directory entries after fsync log replay
      btrfs: build fixup for qgroup_account_snapshot
      btrfs: qgroup: Fix qgroup accounting when creating snapshot
      Btrfs: fix fspath error deallocation
      btrfs: make find_workspace warn if there are no workspaces
      btrfs: make find_workspace always succeed
      ...

commit e259221763a40403d5bb232209998e8c45804ab8
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Apr 7 08:52:01 2016 -0700

    fs: simplify the generic_write_sync prototype
    
    The kiocb already has the new position, so use that.  The only interesting
    case is AIO, where we currently don't bother updating ki_pos.  We're about
    to free the kiocb after we're done, so we might as well update it to make
    everyone's life simpler.
    
    While we're at it also return the bytes written argument passed in if
    we were successful so that the boilerplate error switch code in the
    callers can go away.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 35ce146cceec..ea9f10bb089c 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1851,11 +1851,8 @@ static ssize_t btrfs_file_write_iter(struct kiocb *iocb,
 	spin_lock(&BTRFS_I(inode)->lock);
 	BTRFS_I(inode)->last_sub_trans = root->log_transid;
 	spin_unlock(&BTRFS_I(inode)->lock);
-	if (num_written > 0) {
-		err = generic_write_sync(iocb, pos, num_written);
-		if (err < 0)
-			num_written = err;
-	}
+	if (num_written > 0)
+		num_written = generic_write_sync(iocb, num_written);
 
 	if (sync)
 		atomic_dec(&BTRFS_I(inode)->sync_writers);

commit dde0c2e79848298cc25621ad080d47f94dbd7cce
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Apr 7 08:52:00 2016 -0700

    fs: add IOCB_SYNC and IOCB_DSYNC
    
    This will allow us to do per-I/O sync file writes, as required by a lot
    of fileservers or storage targets.
    
    XXX: Will need a few additional audits for O_DSYNC
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 6c376311a9d7..35ce146cceec 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1852,7 +1852,7 @@ static ssize_t btrfs_file_write_iter(struct kiocb *iocb,
 	BTRFS_I(inode)->last_sub_trans = root->log_transid;
 	spin_unlock(&BTRFS_I(inode)->lock);
 	if (num_written > 0) {
-		err = generic_write_sync(file, pos, num_written);
+		err = generic_write_sync(iocb, pos, num_written);
 		if (err < 0)
 			num_written = err;
 	}

commit 1af5bb491fbb41c8dab9d728a92758dd6a28afd4
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Apr 7 08:51:56 2016 -0700

    filemap: remove the pos argument to generic_file_direct_write
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 8d7b5a45c005..6c376311a9d7 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1703,18 +1703,17 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 	return num_written ? num_written : ret;
 }
 
-static ssize_t __btrfs_direct_write(struct kiocb *iocb,
-				    struct iov_iter *from,
-				    loff_t pos)
+static ssize_t __btrfs_direct_write(struct kiocb *iocb, struct iov_iter *from)
 {
 	struct file *file = iocb->ki_filp;
 	struct inode *inode = file_inode(file);
+	loff_t pos = iocb->ki_pos;
 	ssize_t written;
 	ssize_t written_buffered;
 	loff_t endbyte;
 	int err;
 
-	written = generic_file_direct_write(iocb, from, pos);
+	written = generic_file_direct_write(iocb, from);
 
 	if (written < 0 || !iov_iter_count(from))
 		return written;
@@ -1832,7 +1831,7 @@ static ssize_t btrfs_file_write_iter(struct kiocb *iocb,
 		atomic_inc(&BTRFS_I(inode)->sync_writers);
 
 	if (iocb->ki_flags & IOCB_DIRECT) {
-		num_written = __btrfs_direct_write(iocb, from, pos);
+		num_written = __btrfs_direct_write(iocb, from);
 	} else {
 		num_written = __btrfs_buffered_write(file, from, pos);
 		if (num_written > 0)

commit a2af23b7d7cb0de89570e97da84f6fb642e990a4
Author: Chandan Rajendra <chandan@linux.vnet.ibm.com>
Date:   Mon Apr 4 02:53:06 2016 +0530

    Btrfs: __btrfs_buffered_write: Pass valid file offset when releasing delalloc space
    
    The delalloc reserved space is calculated in terms of number of bytes
    used by an integral number of blocks. This is done by rounding down the
    value of 'pos' to the nearest multiple of sectorsize.
    
    The file offset value held by 'pos' variable may not be aligned to
    sectorsize and hence when passing it as an argument to
    btrfs_delalloc_release_space(), we may end up releasing larger delalloc
    space than we originally had reserved.
    
    Signed-off-by: Chandan Rajendra <chandan@linux.vnet.ibm.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 751daacd268d..af059c44684d 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1696,7 +1696,9 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 			btrfs_end_write_no_snapshoting(root);
 			btrfs_delalloc_release_metadata(inode, release_bytes);
 		} else {
-			btrfs_delalloc_release_space(inode, pos, release_bytes);
+			btrfs_delalloc_release_space(inode,
+						round_down(pos, root->sectorsize),
+						release_bytes);
 		}
 	}
 

commit 4c63c2454eff996c5e27991221106eb511f7db38
Author: Luke Dashjr <luke@dashjr.org>
Date:   Thu Oct 29 08:22:21 2015 +0000

    btrfs: bugfix: handle FS_IOC32_{GETFLAGS,SETFLAGS,GETVERSION} in btrfs_ioctl
    
    32-bit ioctl uses these rather than the regular FS_IOC_* versions. They can
    be handled in btrfs using the same code. Without this, 32-bit {ch,ls}attr
    fail.
    
    Signed-off-by: Luke Dashjr <luke-jr+git@utopios.org>
    Cc: stable@vger.kernel.org
    Reviewed-by: Josef Bacik <jbacik@fb.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 8d7b5a45c005..751daacd268d 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2956,7 +2956,7 @@ const struct file_operations btrfs_file_operations = {
 	.fallocate	= btrfs_fallocate,
 	.unlocked_ioctl	= btrfs_ioctl,
 #ifdef CONFIG_COMPAT
-	.compat_ioctl	= btrfs_ioctl,
+	.compat_ioctl	= btrfs_compat_ioctl,
 #endif
 	.copy_file_range = btrfs_copy_file_range,
 	.clone_file_range = btrfs_clone_file_range,

commit 839a3f765728cdca0057a12e2dc0bf669ac1c22e
Merge: 6759212640fd 56f23fdbb600
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Apr 9 10:41:34 2016 -0700

    Merge branch 'for-linus-4.6' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs
    
    Pull btrfs fixes from Chris Mason:
     "These are bug fixes, including a really old fsync bug, and a few trace
      points to help us track down problems in the quota code"
    
    * 'for-linus-4.6' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs:
      Btrfs: fix file/data loss caused by fsync after rename and new inode
      btrfs: Reset IO error counters before start of device replacing
      btrfs: Add qgroup tracing
      Btrfs: don't use src fd for printk
      btrfs: fallback to vmalloc in btrfs_compare_tree
      btrfs: handle non-fatal errors in btrfs_qgroup_inherit()
      btrfs: Output more info for enospc_debug mount option
      Btrfs: fix invalid reference in replace_path
      Btrfs: Improve FL_KEEP_SIZE handling in fallocate

commit 93061f390f107c37bad7e3bf9eb07bda58a4a99f
Merge: 1c915b3ac4ec c325a67c7290
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Apr 7 17:22:20 2016 -0700

    Merge tag 'ext4_for_linus_stable' of git://git.kernel.org/pub/scm/linux/kernel/git/tytso/ext4
    
    Pull ext4 bugfixes from Ted Ts'o:
     "These changes contains a fix for overlayfs interacting with some
      (badly behaved) dentry code in various file systems.  These have been
      reviewed by Al and the respective file system mtinainers and are going
      through the ext4 tree for convenience.
    
      This also has a few ext4 encryption bug fixes that were discovered in
      Android testing (yes, we will need to get these sync'ed up with the
      fs/crypto code; I'll take care of that).  It also has some bug fixes
      and a change to ignore the legacy quota options to allow for xfstests
      regression testing of ext4's internal quota feature and to be more
      consistent with how xfs handles this case"
    
    * tag 'ext4_for_linus_stable' of git://git.kernel.org/pub/scm/linux/kernel/git/tytso/ext4:
      ext4: ignore quota mount options if the quota feature is enabled
      ext4 crypto: fix some error handling
      ext4: avoid calling dquot_get_next_id() if quota is not enabled
      ext4: retry block allocation for failed DIO and DAX writes
      ext4: add lockdep annotations for i_data_sem
      ext4: allow readdir()'s of large empty directories to be interrupted
      btrfs: fix crash/invalid memory access on fsync when using overlayfs
      ext4 crypto: use dget_parent() in ext4_d_revalidate()
      ext4: use file_dentry()
      ext4: use dget_parent() in ext4_file_open()
      nfs: use file_dentry()
      fs: add file_dentry()
      ext4 crypto: don't let data integrity writebacks fail with ENOMEM
      ext4: check if in-inode xattr is corrupted in ext4_expand_extra_isize_ea()

commit 09cbfeaf1a5a67bfb3201e0c83c810cecb2efa5a
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Fri Apr 1 15:29:47 2016 +0300

    mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros
    
    PAGE_CACHE_{SIZE,SHIFT,MASK,ALIGN} macros were introduced *long* time
    ago with promise that one day it will be possible to implement page
    cache with bigger chunks than PAGE_SIZE.
    
    This promise never materialized.  And unlikely will.
    
    We have many places where PAGE_CACHE_SIZE assumed to be equal to
    PAGE_SIZE.  And it's constant source of confusion on whether
    PAGE_CACHE_* or PAGE_* constant should be used in a particular case,
    especially on the border between fs and mm.
    
    Global switching to PAGE_CACHE_SIZE != PAGE_SIZE would cause to much
    breakage to be doable.
    
    Let's stop pretending that pages in page cache are special.  They are
    not.
    
    The changes are pretty straight-forward:
    
     - <foo> << (PAGE_CACHE_SHIFT - PAGE_SHIFT) -> <foo>;
    
     - <foo> >> (PAGE_CACHE_SHIFT - PAGE_SHIFT) -> <foo>;
    
     - PAGE_CACHE_{SIZE,SHIFT,MASK,ALIGN} -> PAGE_{SIZE,SHIFT,MASK,ALIGN};
    
     - page_cache_get() -> get_page();
    
     - page_cache_release() -> put_page();
    
    This patch contains automated changes generated with coccinelle using
    script below.  For some reason, coccinelle doesn't patch header files.
    I've called spatch for them manually.
    
    The only adjustment after coccinelle is revert of changes to
    PAGE_CAHCE_ALIGN definition: we are going to drop it later.
    
    There are few places in the code where coccinelle didn't reach.  I'll
    fix them manually in a separate patch.  Comments and documentation also
    will be addressed with the separate patch.
    
    virtual patch
    
    @@
    expression E;
    @@
    - E << (PAGE_CACHE_SHIFT - PAGE_SHIFT)
    + E
    
    @@
    expression E;
    @@
    - E >> (PAGE_CACHE_SHIFT - PAGE_SHIFT)
    + E
    
    @@
    @@
    - PAGE_CACHE_SHIFT
    + PAGE_SHIFT
    
    @@
    @@
    - PAGE_CACHE_SIZE
    + PAGE_SIZE
    
    @@
    @@
    - PAGE_CACHE_MASK
    + PAGE_MASK
    
    @@
    expression E;
    @@
    - PAGE_CACHE_ALIGN(E)
    + PAGE_ALIGN(E)
    
    @@
    expression E;
    @@
    - page_cache_get(E)
    + get_page(E)
    
    @@
    expression E;
    @@
    - page_cache_release(E)
    + put_page(E)
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 15a09cb156ce..cf31a60c6284 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -414,11 +414,11 @@ static noinline int btrfs_copy_from_user(loff_t pos, size_t write_bytes,
 	size_t copied = 0;
 	size_t total_copied = 0;
 	int pg = 0;
-	int offset = pos & (PAGE_CACHE_SIZE - 1);
+	int offset = pos & (PAGE_SIZE - 1);
 
 	while (write_bytes > 0) {
 		size_t count = min_t(size_t,
-				     PAGE_CACHE_SIZE - offset, write_bytes);
+				     PAGE_SIZE - offset, write_bytes);
 		struct page *page = prepared_pages[pg];
 		/*
 		 * Copy data from userspace to the current page
@@ -448,7 +448,7 @@ static noinline int btrfs_copy_from_user(loff_t pos, size_t write_bytes,
 		if (unlikely(copied == 0))
 			break;
 
-		if (copied < PAGE_CACHE_SIZE - offset) {
+		if (copied < PAGE_SIZE - offset) {
 			offset += copied;
 		} else {
 			pg++;
@@ -473,7 +473,7 @@ static void btrfs_drop_pages(struct page **pages, size_t num_pages)
 		 */
 		ClearPageChecked(pages[i]);
 		unlock_page(pages[i]);
-		page_cache_release(pages[i]);
+		put_page(pages[i]);
 	}
 }
 
@@ -1297,7 +1297,7 @@ static int prepare_uptodate_page(struct inode *inode,
 {
 	int ret = 0;
 
-	if (((pos & (PAGE_CACHE_SIZE - 1)) || force_uptodate) &&
+	if (((pos & (PAGE_SIZE - 1)) || force_uptodate) &&
 	    !PageUptodate(page)) {
 		ret = btrfs_readpage(NULL, page);
 		if (ret)
@@ -1323,7 +1323,7 @@ static noinline int prepare_pages(struct inode *inode, struct page **pages,
 				  size_t write_bytes, bool force_uptodate)
 {
 	int i;
-	unsigned long index = pos >> PAGE_CACHE_SHIFT;
+	unsigned long index = pos >> PAGE_SHIFT;
 	gfp_t mask = btrfs_alloc_write_mask(inode->i_mapping);
 	int err = 0;
 	int faili;
@@ -1345,7 +1345,7 @@ static noinline int prepare_pages(struct inode *inode, struct page **pages,
 			err = prepare_uptodate_page(inode, pages[i],
 						    pos + write_bytes, false);
 		if (err) {
-			page_cache_release(pages[i]);
+			put_page(pages[i]);
 			if (err == -EAGAIN) {
 				err = 0;
 				goto again;
@@ -1360,7 +1360,7 @@ static noinline int prepare_pages(struct inode *inode, struct page **pages,
 fail:
 	while (faili >= 0) {
 		unlock_page(pages[faili]);
-		page_cache_release(pages[faili]);
+		put_page(pages[faili]);
 		faili--;
 	}
 	return err;
@@ -1408,7 +1408,7 @@ lock_and_cleanup_extent_if_need(struct inode *inode, struct page **pages,
 					     cached_state, GFP_NOFS);
 			for (i = 0; i < num_pages; i++) {
 				unlock_page(pages[i]);
-				page_cache_release(pages[i]);
+				put_page(pages[i]);
 			}
 			btrfs_start_ordered_extent(inode, ordered, 1);
 			btrfs_put_ordered_extent(ordered);
@@ -1497,8 +1497,8 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 	bool force_page_uptodate = false;
 	bool need_unlock;
 
-	nrptrs = min(DIV_ROUND_UP(iov_iter_count(i), PAGE_CACHE_SIZE),
-			PAGE_CACHE_SIZE / (sizeof(struct page *)));
+	nrptrs = min(DIV_ROUND_UP(iov_iter_count(i), PAGE_SIZE),
+			PAGE_SIZE / (sizeof(struct page *)));
 	nrptrs = min(nrptrs, current->nr_dirtied_pause - current->nr_dirtied);
 	nrptrs = max(nrptrs, 8);
 	pages = kmalloc_array(nrptrs, sizeof(struct page *), GFP_KERNEL);
@@ -1506,13 +1506,13 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 		return -ENOMEM;
 
 	while (iov_iter_count(i) > 0) {
-		size_t offset = pos & (PAGE_CACHE_SIZE - 1);
+		size_t offset = pos & (PAGE_SIZE - 1);
 		size_t sector_offset;
 		size_t write_bytes = min(iov_iter_count(i),
-					 nrptrs * (size_t)PAGE_CACHE_SIZE -
+					 nrptrs * (size_t)PAGE_SIZE -
 					 offset);
 		size_t num_pages = DIV_ROUND_UP(write_bytes + offset,
-						PAGE_CACHE_SIZE);
+						PAGE_SIZE);
 		size_t reserve_bytes;
 		size_t dirty_pages;
 		size_t copied;
@@ -1547,7 +1547,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 			 * write_bytes, so scale down.
 			 */
 			num_pages = DIV_ROUND_UP(write_bytes + offset,
-						 PAGE_CACHE_SIZE);
+						 PAGE_SIZE);
 			reserve_bytes = round_up(write_bytes + sector_offset,
 					root->sectorsize);
 			goto reserve_metadata;
@@ -1609,7 +1609,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 		} else {
 			force_page_uptodate = false;
 			dirty_pages = DIV_ROUND_UP(copied + offset,
-						   PAGE_CACHE_SIZE);
+						   PAGE_SIZE);
 		}
 
 		/*
@@ -1641,7 +1641,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 				u64 __pos;
 
 				__pos = round_down(pos, root->sectorsize) +
-					(dirty_pages << PAGE_CACHE_SHIFT);
+					(dirty_pages << PAGE_SHIFT);
 				btrfs_delalloc_release_space(inode, __pos,
 							     release_bytes);
 			}
@@ -1682,7 +1682,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 		cond_resched();
 
 		balance_dirty_pages_ratelimited(inode->i_mapping);
-		if (dirty_pages < (root->nodesize >> PAGE_CACHE_SHIFT) + 1)
+		if (dirty_pages < (root->nodesize >> PAGE_SHIFT) + 1)
 			btrfs_btree_balance_dirty(root);
 
 		pos += copied;
@@ -1738,8 +1738,8 @@ static ssize_t __btrfs_direct_write(struct kiocb *iocb,
 		goto out;
 	written += written_buffered;
 	iocb->ki_pos = pos + written_buffered;
-	invalidate_mapping_pages(file->f_mapping, pos >> PAGE_CACHE_SHIFT,
-				 endbyte >> PAGE_CACHE_SHIFT);
+	invalidate_mapping_pages(file->f_mapping, pos >> PAGE_SHIFT,
+				 endbyte >> PAGE_SHIFT);
 out:
 	return written ? written : err;
 }

commit 2a162ce93232eb78124601996744f8eafec845ab
Author: Davide Italiano <dccitaliano@gmail.com>
Date:   Mon Apr 6 22:09:15 2015 -0700

    Btrfs: Improve FL_KEEP_SIZE handling in fallocate
    
    - We call inode_size_ok() only if FL_KEEP_SIZE isn't specified.
    - As an optimisation we can skip the call if (off + len)
      isn't greater than the current size of the file. This operation
      is called under the lock so the less work we do, the better.
    - If we call inode_size_ok() pass to it the correct value rather
      than a more conservative estimation.
    
    Signed-off-by: Davide Italiano <dccitaliano@gmail.com>
    Reviewed-by: Liu Bo <bo.li.liu@oracle.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 15a09cb156ce..7af1abda68d0 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2682,9 +2682,12 @@ static long btrfs_fallocate(struct file *file, int mode,
 		return ret;
 
 	inode_lock(inode);
-	ret = inode_newsize_ok(inode, alloc_end);
-	if (ret)
-		goto out;
+
+	if (!(mode & FALLOC_FL_KEEP_SIZE) && offset + len > inode->i_size) {
+		ret = inode_newsize_ok(inode, offset + len);
+		if (ret)
+			goto out;
+	}
 
 	/*
 	 * TODO: Move these two operations after we have checked

commit de17e793b104d690e1d007dfc5cb6b4f649598ca
Author: Filipe Manana <fdmanana@suse.com>
Date:   Wed Mar 30 19:03:13 2016 -0400

    btrfs: fix crash/invalid memory access on fsync when using overlayfs
    
    If the lower or upper directory of an overlayfs mount belong to a btrfs
    file system and we fsync the file through the overlayfs' merged directory
    we ended up accessing an inode that didn't belong to btrfs as if it were
    a btrfs inode at btrfs_sync_file() resulting in a crash like the following:
    
    [ 7782.588845] BUG: unable to handle kernel NULL pointer dereference at 0000000000000544
    [ 7782.590624] IP: [<ffffffffa030b7ab>] btrfs_sync_file+0x11b/0x3e9 [btrfs]
    [ 7782.591931] PGD 4d954067 PUD 1e878067 PMD 0
    [ 7782.592016] Oops: 0002 [#6] PREEMPT SMP DEBUG_PAGEALLOC
    [ 7782.592016] Modules linked in: btrfs overlay ppdev crc32c_generic evdev xor raid6_pq psmouse pcspkr sg serio_raw acpi_cpufreq parport_pc parport tpm_tis i2c_piix4 tpm i2c_core processor button loop autofs4 ext4 crc16 mbcache jbd2 sr_mod cdrom sd_mod ata_generic virtio_scsi ata_piix virtio_pci libata virtio_ring virtio scsi_mod e1000 floppy [last unloaded: btrfs]
    [ 7782.592016] CPU: 10 PID: 16437 Comm: xfs_io Tainted: G      D         4.5.0-rc6-btrfs-next-26+ #1
    [ 7782.592016] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS by qemu-project.org 04/01/2014
    [ 7782.592016] task: ffff88001b8d40c0 ti: ffff880137488000 task.ti: ffff880137488000
    [ 7782.592016] RIP: 0010:[<ffffffffa030b7ab>]  [<ffffffffa030b7ab>] btrfs_sync_file+0x11b/0x3e9 [btrfs]
    [ 7782.592016] RSP: 0018:ffff88013748be40  EFLAGS: 00010286
    [ 7782.592016] RAX: 0000000080000000 RBX: ffff880133b30c88 RCX: 0000000000000001
    [ 7782.592016] RDX: 0000000000000001 RSI: ffffffff8148fec0 RDI: 00000000ffffffff
    [ 7782.592016] RBP: ffff88013748bec0 R08: 0000000000000001 R09: 0000000000000000
    [ 7782.624248] R10: ffff88013748be40 R11: 0000000000000246 R12: 0000000000000000
    [ 7782.624248] R13: 0000000000000000 R14: 00000000009305a0 R15: ffff880015e3be40
    [ 7782.624248] FS:  00007fa83b9cb700(0000) GS:ffff88023ed40000(0000) knlGS:0000000000000000
    [ 7782.624248] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [ 7782.624248] CR2: 0000000000000544 CR3: 00000001fa652000 CR4: 00000000000006e0
    [ 7782.624248] Stack:
    [ 7782.624248]  ffffffff8108b5cc ffff88013748bec0 0000000000000246 ffff8800b005ded0
    [ 7782.624248]  ffff880133b30d60 8000000000000000 7fffffffffffffff 0000000000000246
    [ 7782.624248]  0000000000000246 ffffffff81074f9b ffffffff8104357c ffff880015e3be40
    [ 7782.624248] Call Trace:
    [ 7782.624248]  [<ffffffff8108b5cc>] ? arch_local_irq_save+0x9/0xc
    [ 7782.624248]  [<ffffffff81074f9b>] ? ___might_sleep+0xce/0x217
    [ 7782.624248]  [<ffffffff8104357c>] ? __do_page_fault+0x3c0/0x43a
    [ 7782.624248]  [<ffffffff811a2351>] vfs_fsync_range+0x8c/0x9e
    [ 7782.624248]  [<ffffffff811a237f>] vfs_fsync+0x1c/0x1e
    [ 7782.624248]  [<ffffffff811a24d6>] do_fsync+0x31/0x4a
    [ 7782.624248]  [<ffffffff811a2700>] SyS_fsync+0x10/0x14
    [ 7782.624248]  [<ffffffff81493617>] entry_SYSCALL_64_fastpath+0x12/0x6b
    [ 7782.624248] Code: 85 c0 0f 85 e2 02 00 00 48 8b 45 b0 31 f6 4c 29 e8 48 ff c0 48 89 45 a8 48 8d 83 d8 00 00 00 48 89 c7 48 89 45 a0 e8 fc 43 18 e1 <f0> 41 ff 84 24 44 05 00 00 48 8b 83 58 ff ff ff 48 c1 e8 07 83
    [ 7782.624248] RIP  [<ffffffffa030b7ab>] btrfs_sync_file+0x11b/0x3e9 [btrfs]
    [ 7782.624248]  RSP <ffff88013748be40>
    [ 7782.624248] CR2: 0000000000000544
    [ 7782.661994] ---[ end trace 721e14960eb939bc ]---
    
    This started happening since commit 4bacc9c9234 (overlayfs: Make f_path
    always point to the overlay and f_inode to the underlay) and even though
    after this change we could still access the btrfs inode through
    struct file->f_mapping->host or struct file->f_inode, we would end up
    resulting in more similar issues later on at check_parent_dirs_for_sync()
    because the dentry we got (from struct file->f_path.dentry) was from
    overlayfs and not from btrfs, that is, we had no way of getting the dentry
    that belonged to btrfs (we always got the dentry that belonged to
    overlayfs).
    
    The new patch from Miklos Szeredi, titled "vfs: add file_dentry()" and
    recently submitted to linux-fsdevel, adds a file_dentry() API that allows
    us to get the btrfs dentry from the input file and therefore being able
    to fsync when the upper and lower directories belong to btrfs filesystems.
    
    This issue has been reported several times by users in the mailing list
    and bugzilla. A test case for xfstests is being submitted as well.
    
    Fixes: 4bacc9c9234c ("overlayfs: Make f_path always point to the overlay and f_inode to the underlay")
    Bugzilla: https://bugzilla.kernel.org/show_bug.cgi?id=101951
    Bugzilla: https://bugzilla.kernel.org/show_bug.cgi?id=109791
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: Chris Mason <clm@fb.com>
    Cc: stable@vger.kernel.org

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 15a09cb156ce..2f40482347af 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1905,7 +1905,7 @@ static int start_ordered_ops(struct inode *inode, loff_t start, loff_t end)
  */
 int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 {
-	struct dentry *dentry = file->f_path.dentry;
+	struct dentry *dentry = file_dentry(file);
 	struct inode *inode = d_inode(dentry);
 	struct btrfs_root *root = BTRFS_I(inode)->root;
 	struct btrfs_trans_handle *trans;

commit bb7ab3b92e46da06b580c6f83abe7894dc449cca
Author: Adam Buchbinder <adam.buchbinder@gmail.com>
Date:   Fri Mar 4 11:23:12 2016 -0800

    btrfs: Fix misspellings in comments.
    
    Signed-off-by: Adam Buchbinder <adam.buchbinder@gmail.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index dba5de6cdc8a..15a09cb156ce 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1847,7 +1847,7 @@ static ssize_t btrfs_file_write_iter(struct kiocb *iocb,
 	/*
 	 * We also have to set last_sub_trans to the current log transid,
 	 * otherwise subsequent syncs to a file that's been synced in this
-	 * transaction will appear to have already occured.
+	 * transaction will appear to have already occurred.
 	 */
 	spin_lock(&BTRFS_I(inode)->lock);
 	BTRFS_I(inode)->last_sub_trans = root->log_transid;

commit ebb8765b2ded869b75bf5154b048119eb52571f7
Author: Anand Jain <anand.jain@oracle.com>
Date:   Thu Mar 10 17:26:59 2016 +0800

    btrfs: move btrfs_compression_type to compression.h
    
    So that its better organized.
    
    Signed-off-by: Anand Jain <anand.jain@oracle.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index d23d10024cf0..dba5de6cdc8a 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -41,6 +41,7 @@
 #include "locking.h"
 #include "volumes.h"
 #include "qgroup.h"
+#include "compression.h"
 
 static struct kmem_cache *btrfs_inode_defrag_cachep;
 /*

commit affc0ff902d539ebe9bba405d330410314f46e9f
Author: Filipe Manana <fdmanana@suse.com>
Date:   Wed Feb 24 07:35:05 2016 +0000

    Btrfs: fix race when checking if we can skip fsync'ing an inode
    
    If we're about to do a fast fsync for an inode and btrfs_inode_in_log()
    returns false, it's possible that we had an ordered extent in progress
    (btrfs_finish_ordered_io() not run yet) when we noticed that the inode's
    last_trans field was not greater than the id of the last committed
    transaction, but shortly after, before we checked if there were any
    ongoing ordered extents, the ordered extent had just completed and
    removed itself from the inode's ordered tree, in which case we end up not
    logging the inode, losing some data if a power failure or crash happens
    after the fsync handler returns and before the transaction is committed.
    
    Fix this by checking first if there are any ongoing ordered extents
    before comparing the inode's last_trans with the id of the last committed
    transaction - when it completes, an ordered extent always updates the
    inode's last_trans before it removes itself from the inode's ordered
    tree (at btrfs_finish_ordered_io()).
    
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 03de2466db23..d23d10024cf0 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2017,10 +2017,11 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 	 */
 	smp_mb();
 	if (btrfs_inode_in_log(inode, root->fs_info->generation) ||
-	    (BTRFS_I(inode)->last_trans <=
-	     root->fs_info->last_trans_committed &&
-	     (full_sync ||
-	      !btrfs_have_ordered_extents_in_range(inode, start, len)))) {
+	    (full_sync && BTRFS_I(inode)->last_trans <=
+	     root->fs_info->last_trans_committed) ||
+	    (!btrfs_have_ordered_extents_in_range(inode, start, len) &&
+	     BTRFS_I(inode)->last_trans
+	     <= root->fs_info->last_trans_committed)) {
 		/*
 		 * We'v had everything committed since the last time we were
 		 * modified so clear this flag in case it was set for whatever

commit fa695b01bceabc40be3267d309ca8a663de53d7a
Merge: f004fae0cfeb f7e98a7fff86
Author: David Sterba <dsterba@suse.com>
Date:   Fri Feb 26 15:38:34 2016 +0100

    Merge branch 'misc-4.6' into for-chris-4.6
    
    # Conflicts:
    #       fs/btrfs/file.c

commit f004fae0cfeb96d33240eb5471f14cb6fbbd4eea
Merge: 675d276b322b f827ba9a641b
Author: David Sterba <dsterba@suse.com>
Date:   Fri Feb 26 15:38:33 2016 +0100

    Merge branch 'cleanups-4.6' into for-chris-4.6

commit e22b3d1fbe596c7feba6782dab2e11c7b99f1d90
Merge: 5f1b5664d978 66722f7c0590
Author: David Sterba <dsterba@suse.com>
Date:   Fri Feb 26 15:38:28 2016 +0100

    Merge branch 'dev/gfp-flags' into for-chris-4.6

commit 5f1b5664d97842bc5dba40c2053bf95270b6ff7a
Merge: 388f7b1d6e8c 65bfa6580791
Author: David Sterba <dsterba@suse.com>
Date:   Fri Feb 26 15:38:28 2016 +0100

    Merge branch 'chandan/prep-subpage-blocksize' into for-chris-4.6
    
    # Conflicts:
    #       fs/btrfs/file.c

commit 4da2e26a2a32b174878744bd0f07db180c875f26
Author: Zhao Lei <zhaolei@cn.fujitsu.com>
Date:   Wed Jan 6 18:24:43 2016 +0800

    btrfs: Continue write in case of can_not_nocow
    
    btrfs failed in xfstests btrfs/080 with -o nodatacow.
    
    Can be reproduced by following script:
      DEV=/dev/vdg
      MNT=/mnt/tmp
    
      umount $DEV &>/dev/null
      mkfs.btrfs -f $DEV
      mount -o nodatacow $DEV $MNT
    
      dd if=/dev/zero of=$MNT/test bs=1 count=2048 &
      btrfs subvolume snapshot -r $MNT $MNT/test_snap &
      wait
      --
      We can see dd failed on NO_SPACE.
    
    Reason:
      __btrfs_buffered_write should run cow write when no_cow impossible,
      and current code is designed with above logic.
      But check_can_nocow() have 2 type of return value(0 and <0) on
      can_not_no_cow, and current code only continue write on first case,
      the second case happened in doing subvolume.
    
    Fix:
      Continue write when check_can_nocow() return 0 and <0.
    
    Reviewed-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: Zhao Lei <zhaolei@cn.fujitsu.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 098bb8f690c9..cadfebaaf8c4 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1525,27 +1525,24 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 
 		reserve_bytes = num_pages << PAGE_CACHE_SHIFT;
 
-		if (BTRFS_I(inode)->flags & (BTRFS_INODE_NODATACOW |
-					     BTRFS_INODE_PREALLOC)) {
-			ret = check_can_nocow(inode, pos, &write_bytes);
-			if (ret < 0)
-				break;
-			if (ret > 0) {
-				/*
-				 * For nodata cow case, no need to reserve
-				 * data space.
-				 */
-				only_release_metadata = true;
-				/*
-				 * our prealloc extent may be smaller than
-				 * write_bytes, so scale down.
-				 */
-				num_pages = DIV_ROUND_UP(write_bytes + offset,
-							 PAGE_CACHE_SIZE);
-				reserve_bytes = num_pages << PAGE_CACHE_SHIFT;
-				goto reserve_metadata;
-			}
+		if ((BTRFS_I(inode)->flags & (BTRFS_INODE_NODATACOW |
+					      BTRFS_INODE_PREALLOC)) &&
+		    check_can_nocow(inode, pos, &write_bytes) > 0) {
+			/*
+			 * For nodata cow case, no need to reserve
+			 * data space.
+			 */
+			only_release_metadata = true;
+			/*
+			 * our prealloc extent may be smaller than
+			 * write_bytes, so scale down.
+			 */
+			num_pages = DIV_ROUND_UP(write_bytes + offset,
+						 PAGE_CACHE_SIZE);
+			reserve_bytes = num_pages << PAGE_CACHE_SHIFT;
+			goto reserve_metadata;
 		}
+
 		ret = btrfs_check_data_free_space(inode, pos, write_bytes);
 		if (ret < 0)
 			break;

commit 5598e9005a4076d6700bbd89d0cdbe5b2922a846
Author: Kinglong Mee <kinglongmee@gmail.com>
Date:   Fri Jan 29 21:36:35 2016 +0800

    btrfs: drop null testing before destroy functions
    
    Cleanup.
    
    kmem_cache_destroy has support NULL argument checking,
    so drop the double null testing before calling it.
    
    Signed-off-by: Kinglong Mee <kinglongmee@gmail.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 610f56992464..b387fb53a880 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2939,8 +2939,7 @@ const struct file_operations btrfs_file_operations = {
 
 void btrfs_auto_defrag_exit(void)
 {
-	if (btrfs_inode_defrag_cachep)
-		kmem_cache_destroy(btrfs_inode_defrag_cachep);
+	kmem_cache_destroy(btrfs_inode_defrag_cachep);
 }
 
 int btrfs_auto_defrag_init(void)

commit 04b285f35e2086b69682c7ed054aa35eebea9f72
Author: Deepa Dinamani <deepa.kernel@gmail.com>
Date:   Sat Feb 6 23:57:21 2016 -0800

    btrfs: Replace CURRENT_TIME by current_fs_time()
    
    CURRENT_TIME macro is not appropriate for filesystems as it
    doesn't use the right granularity for filesystem timestamps.
    Use current_fs_time() instead.
    
    Signed-off-by: Deepa Dinamani <deepa.kernel@gmail.com>
    Cc: Chris Mason <clm@fb.com>
    Cc: Josef Bacik <jbacik@fb.com>
    Cc: linux-btrfs@vger.kernel.org
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 098bb8f690c9..610f56992464 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2544,7 +2544,7 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 		goto out_free;
 
 	inode_inc_iversion(inode);
-	inode->i_mtime = inode->i_ctime = CURRENT_TIME;
+	inode->i_mtime = inode->i_ctime = current_fs_time(inode->i_sb);
 
 	trans->block_rsv = &root->fs_info->trans_block_rsv;
 	ret = btrfs_update_inode(trans, root, inode);
@@ -2794,7 +2794,7 @@ static long btrfs_fallocate(struct file *file, int mode,
 		if (IS_ERR(trans)) {
 			ret = PTR_ERR(trans);
 		} else {
-			inode->i_ctime = CURRENT_TIME;
+			inode->i_ctime = current_fs_time(inode->i_sb);
 			i_size_write(inode, actual_end);
 			btrfs_ordered_update_i_size(inode, actual_end, NULL);
 			ret = btrfs_update_inode(trans, root, inode);

commit 32fc932e30c4b6a03847426f52e23459c9fb906f
Author: David Sterba <dsterba@suse.com>
Date:   Thu Feb 11 14:25:38 2016 +0100

    btrfs: fallocate: use GFP_KERNEL
    
    Fallocate is initiated from userspace and is not on the critical
    writeback path, we don't need to use GFP_NOFS for allocations.
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 098bb8f690c9..5d46fdc4651c 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2611,7 +2611,7 @@ static int add_falloc_range(struct list_head *head, u64 start, u64 len)
 		return 0;
 	}
 insert:
-	range = kmalloc(sizeof(*range), GFP_NOFS);
+	range = kmalloc(sizeof(*range), GFP_KERNEL);
 	if (!range)
 		return -ENOMEM;
 	range->start = start;
@@ -2712,7 +2712,7 @@ static long btrfs_fallocate(struct file *file, int mode,
 			btrfs_put_ordered_extent(ordered);
 			unlock_extent_cached(&BTRFS_I(inode)->io_tree,
 					     alloc_start, locked_end,
-					     &cached_state, GFP_NOFS);
+					     &cached_state, GFP_KERNEL);
 			/*
 			 * we can't wait on the range with the transaction
 			 * running or with the extent lock held
@@ -2806,7 +2806,7 @@ static long btrfs_fallocate(struct file *file, int mode,
 	}
 out_unlock:
 	unlock_extent_cached(&BTRFS_I(inode)->io_tree, alloc_start, locked_end,
-			     &cached_state, GFP_NOFS);
+			     &cached_state, GFP_KERNEL);
 out:
 	/*
 	 * As we waited the extent range, the data_rsv_map must be empty

commit 27772b68f6994f0011690899c31717b7cbec51c9
Author: Chandan Rajendra <chandan@linux.vnet.ibm.com>
Date:   Thu Jan 21 15:56:03 2016 +0530

    Btrfs: Clean pte corresponding to page straddling i_size
    
    When extending a file by either "truncate up" or by writing beyond i_size, the
    page which had i_size needs to be marked "read only" so that future writes to
    the page via mmap interface causes btrfs_page_mkwrite() to be invoked. If not,
    a write performed after extending the file via the mmap interface will find
    the page to be writaeable and continue writing to the page without invoking
    btrfs_page_mkwrite() i.e. we end up writing to a file without reserving disk
    space.
    
    Signed-off-by: Chandan Rajendra <chandan@linux.vnet.ibm.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 58bb29788f5f..953f0ad17802 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1778,6 +1778,8 @@ static ssize_t btrfs_file_write_iter(struct kiocb *iocb,
 	ssize_t err;
 	loff_t pos;
 	size_t count;
+	loff_t oldsize;
+	int clean_page = 0;
 
 	mutex_lock(&inode->i_mutex);
 	err = generic_write_checks(iocb, from);
@@ -1816,14 +1818,17 @@ static ssize_t btrfs_file_write_iter(struct kiocb *iocb,
 	pos = iocb->ki_pos;
 	count = iov_iter_count(from);
 	start_pos = round_down(pos, root->sectorsize);
-	if (start_pos > i_size_read(inode)) {
+	oldsize = i_size_read(inode);
+	if (start_pos > oldsize) {
 		/* Expand hole size to cover write data, preventing empty gap */
 		end_pos = round_up(pos + count, root->sectorsize);
-		err = btrfs_cont_expand(inode, i_size_read(inode), end_pos);
+		err = btrfs_cont_expand(inode, oldsize, end_pos);
 		if (err) {
 			mutex_unlock(&inode->i_mutex);
 			goto out;
 		}
+		if (start_pos > round_up(oldsize, root->sectorsize))
+			clean_page = 1;
 	}
 
 	if (sync)
@@ -1835,6 +1840,9 @@ static ssize_t btrfs_file_write_iter(struct kiocb *iocb,
 		num_written = __btrfs_buffered_write(file, from, pos);
 		if (num_written > 0)
 			iocb->ki_pos = pos + num_written;
+		if (clean_page)
+			pagecache_isize_extended(inode, oldsize,
+						i_size_read(inode));
 	}
 
 	mutex_unlock(&inode->i_mutex);

commit 9703fefe0b137bb4475187b5d82ec5823445616b
Author: Chandan Rajendra <chandan@linux.vnet.ibm.com>
Date:   Thu Jan 21 15:55:56 2016 +0530

    Btrfs: fallocate: Work with sectorsized blocks
    
    While at it, this commit changes btrfs_truncate_page() to truncate sectorsized
    blocks instead of pages. Hence the function has been renamed to
    btrfs_truncate_block().
    
    Signed-off-by: Chandan Rajendra <chandan@linux.vnet.ibm.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 9809557213d4..58bb29788f5f 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2310,10 +2310,10 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 	int ret = 0;
 	int err = 0;
 	unsigned int rsv_count;
-	bool same_page;
+	bool same_block;
 	bool no_holes = btrfs_fs_incompat(root->fs_info, NO_HOLES);
 	u64 ino_size;
-	bool truncated_page = false;
+	bool truncated_block = false;
 	bool updated_inode = false;
 
 	ret = btrfs_wait_ordered_range(inode, offset, len);
@@ -2321,7 +2321,7 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 		return ret;
 
 	mutex_lock(&inode->i_mutex);
-	ino_size = round_up(inode->i_size, PAGE_CACHE_SIZE);
+	ino_size = round_up(inode->i_size, root->sectorsize);
 	ret = find_first_non_hole(inode, &offset, &len);
 	if (ret < 0)
 		goto out_only_mutex;
@@ -2334,31 +2334,30 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 	lockstart = round_up(offset, BTRFS_I(inode)->root->sectorsize);
 	lockend = round_down(offset + len,
 			     BTRFS_I(inode)->root->sectorsize) - 1;
-	same_page = ((offset >> PAGE_CACHE_SHIFT) ==
-		    ((offset + len - 1) >> PAGE_CACHE_SHIFT));
-
+	same_block = (BTRFS_BYTES_TO_BLKS(root->fs_info, offset))
+		== (BTRFS_BYTES_TO_BLKS(root->fs_info, offset + len - 1));
 	/*
-	 * We needn't truncate any page which is beyond the end of the file
+	 * We needn't truncate any block which is beyond the end of the file
 	 * because we are sure there is no data there.
 	 */
 	/*
-	 * Only do this if we are in the same page and we aren't doing the
-	 * entire page.
+	 * Only do this if we are in the same block and we aren't doing the
+	 * entire block.
 	 */
-	if (same_page && len < PAGE_CACHE_SIZE) {
+	if (same_block && len < root->sectorsize) {
 		if (offset < ino_size) {
-			truncated_page = true;
-			ret = btrfs_truncate_page(inode, offset, len, 0);
+			truncated_block = true;
+			ret = btrfs_truncate_block(inode, offset, len, 0);
 		} else {
 			ret = 0;
 		}
 		goto out_only_mutex;
 	}
 
-	/* zero back part of the first page */
+	/* zero back part of the first block */
 	if (offset < ino_size) {
-		truncated_page = true;
-		ret = btrfs_truncate_page(inode, offset, 0, 0);
+		truncated_block = true;
+		ret = btrfs_truncate_block(inode, offset, 0, 0);
 		if (ret) {
 			mutex_unlock(&inode->i_mutex);
 			return ret;
@@ -2393,9 +2392,10 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 		if (!ret) {
 			/* zero the front end of the last page */
 			if (tail_start + tail_len < ino_size) {
-				truncated_page = true;
-				ret = btrfs_truncate_page(inode,
-						tail_start + tail_len, 0, 1);
+				truncated_block = true;
+				ret = btrfs_truncate_block(inode,
+							tail_start + tail_len,
+							0, 1);
 				if (ret)
 					goto out_only_mutex;
 			}
@@ -2575,7 +2575,7 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 	unlock_extent_cached(&BTRFS_I(inode)->io_tree, lockstart, lockend,
 			     &cached_state, GFP_NOFS);
 out_only_mutex:
-	if (!updated_inode && truncated_page && !ret && !err) {
+	if (!updated_inode && truncated_block && !ret && !err) {
 		/*
 		 * If we only end up zeroing part of a page, we still need to
 		 * update the inode item, so that all the time fields are
@@ -2695,10 +2695,10 @@ static long btrfs_fallocate(struct file *file, int mode,
 	} else if (offset + len > inode->i_size) {
 		/*
 		 * If we are fallocating from the end of the file onward we
-		 * need to zero out the end of the page if i_size lands in the
-		 * middle of a page.
+		 * need to zero out the end of the block if i_size lands in the
+		 * middle of a block.
 		 */
-		ret = btrfs_truncate_page(inode, inode->i_size, 0, 0);
+		ret = btrfs_truncate_block(inode, inode->i_size, 0, 0);
 		if (ret)
 			goto out;
 	}

commit 2e78c927d79333f299a8ac81c2fd2952caeef335
Author: Chandan Rajendra <chandan@linux.vnet.ibm.com>
Date:   Thu Jan 21 15:55:53 2016 +0530

    Btrfs: __btrfs_buffered_write: Reserve/release extents aligned to block size
    
    Currently, the code reserves/releases extents in multiples of PAGE_CACHE_SIZE
    units. Fix this by doing reservation/releases in block size units.
    
    Signed-off-by: Chandan Rajendra <chandan@linux.vnet.ibm.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index af782fdd4fca..9809557213d4 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -498,7 +498,7 @@ int btrfs_dirty_pages(struct btrfs_root *root, struct inode *inode,
 	loff_t isize = i_size_read(inode);
 
 	start_pos = pos & ~((u64)root->sectorsize - 1);
-	num_bytes = ALIGN(write_bytes + pos - start_pos, root->sectorsize);
+	num_bytes = round_up(write_bytes + pos - start_pos, root->sectorsize);
 
 	end_of_last_block = start_pos + num_bytes - 1;
 	err = btrfs_set_extent_delalloc(inode, start_pos, end_of_last_block,
@@ -1379,16 +1379,19 @@ static noinline int prepare_pages(struct inode *inode, struct page **pages,
 static noinline int
 lock_and_cleanup_extent_if_need(struct inode *inode, struct page **pages,
 				size_t num_pages, loff_t pos,
+				size_t write_bytes,
 				u64 *lockstart, u64 *lockend,
 				struct extent_state **cached_state)
 {
+	struct btrfs_root *root = BTRFS_I(inode)->root;
 	u64 start_pos;
 	u64 last_pos;
 	int i;
 	int ret = 0;
 
-	start_pos = pos & ~((u64)PAGE_CACHE_SIZE - 1);
-	last_pos = start_pos + ((u64)num_pages << PAGE_CACHE_SHIFT) - 1;
+	start_pos = round_down(pos, root->sectorsize);
+	last_pos = start_pos
+		+ round_up(pos + write_bytes - start_pos, root->sectorsize) - 1;
 
 	if (start_pos < inode->i_size) {
 		struct btrfs_ordered_extent *ordered;
@@ -1503,6 +1506,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 
 	while (iov_iter_count(i) > 0) {
 		size_t offset = pos & (PAGE_CACHE_SIZE - 1);
+		size_t sector_offset;
 		size_t write_bytes = min(iov_iter_count(i),
 					 nrptrs * (size_t)PAGE_CACHE_SIZE -
 					 offset);
@@ -1511,6 +1515,8 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 		size_t reserve_bytes;
 		size_t dirty_pages;
 		size_t copied;
+		size_t dirty_sectors;
+		size_t num_sectors;
 
 		WARN_ON(num_pages > nrptrs);
 
@@ -1523,7 +1529,9 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 			break;
 		}
 
-		reserve_bytes = num_pages << PAGE_CACHE_SHIFT;
+		sector_offset = pos & (root->sectorsize - 1);
+		reserve_bytes = round_up(write_bytes + sector_offset,
+				root->sectorsize);
 
 		if (BTRFS_I(inode)->flags & (BTRFS_INODE_NODATACOW |
 					     BTRFS_INODE_PREALLOC)) {
@@ -1542,7 +1550,9 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 				 */
 				num_pages = DIV_ROUND_UP(write_bytes + offset,
 							 PAGE_CACHE_SIZE);
-				reserve_bytes = num_pages << PAGE_CACHE_SHIFT;
+				reserve_bytes = round_up(write_bytes
+							+ sector_offset,
+							root->sectorsize);
 				goto reserve_metadata;
 			}
 		}
@@ -1576,8 +1586,8 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 			break;
 
 		ret = lock_and_cleanup_extent_if_need(inode, pages, num_pages,
-						      pos, &lockstart, &lockend,
-						      &cached_state);
+						pos, write_bytes, &lockstart,
+						&lockend, &cached_state);
 		if (ret < 0) {
 			if (ret == -EAGAIN)
 				goto again;
@@ -1612,9 +1622,16 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 		 * we still have an outstanding extent for the chunk we actually
 		 * managed to copy.
 		 */
-		if (num_pages > dirty_pages) {
-			release_bytes = (num_pages - dirty_pages) <<
-				PAGE_CACHE_SHIFT;
+		num_sectors = BTRFS_BYTES_TO_BLKS(root->fs_info,
+						reserve_bytes);
+		dirty_sectors = round_up(copied + sector_offset,
+					root->sectorsize);
+		dirty_sectors = BTRFS_BYTES_TO_BLKS(root->fs_info,
+						dirty_sectors);
+
+		if (num_sectors > dirty_sectors) {
+			release_bytes = (write_bytes - copied)
+				& ~((u64)root->sectorsize - 1);
 			if (copied > 0) {
 				spin_lock(&BTRFS_I(inode)->lock);
 				BTRFS_I(inode)->outstanding_extents++;
@@ -1633,7 +1650,8 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 			}
 		}
 
-		release_bytes = dirty_pages << PAGE_CACHE_SHIFT;
+		release_bytes = round_up(copied + sector_offset,
+					root->sectorsize);
 
 		if (copied > 0)
 			ret = btrfs_dirty_pages(root, inode, pages,
@@ -1654,8 +1672,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 
 		if (only_release_metadata && copied > 0) {
 			lockstart = round_down(pos, root->sectorsize);
-			lockend = lockstart +
-				(dirty_pages << PAGE_CACHE_SHIFT) - 1;
+			lockend = round_up(pos + copied, root->sectorsize) - 1;
 
 			set_extent_bit(&BTRFS_I(inode)->io_tree, lockstart,
 				       lockend, EXTENT_NORESERVE, NULL,

commit 5955102c9984fa081b2d570cfac75c97eecf8f3b
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Fri Jan 22 15:40:57 2016 -0500

    wrappers for ->i_mutex access
    
    parallel to mutex_{lock,unlock,trylock,is_locked,lock_nested},
    inode_foo(inode) being mutex_foo(&inode->i_mutex).
    
    Please, use those for access to ->i_mutex; over the coming cycle
    ->i_mutex will become rwsem, with ->lookup() done with it held
    only shared.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 9f5cc1e8e126..098bb8f690c9 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1762,17 +1762,17 @@ static ssize_t btrfs_file_write_iter(struct kiocb *iocb,
 	loff_t pos;
 	size_t count;
 
-	mutex_lock(&inode->i_mutex);
+	inode_lock(inode);
 	err = generic_write_checks(iocb, from);
 	if (err <= 0) {
-		mutex_unlock(&inode->i_mutex);
+		inode_unlock(inode);
 		return err;
 	}
 
 	current->backing_dev_info = inode_to_bdi(inode);
 	err = file_remove_privs(file);
 	if (err) {
-		mutex_unlock(&inode->i_mutex);
+		inode_unlock(inode);
 		goto out;
 	}
 
@@ -1783,7 +1783,7 @@ static ssize_t btrfs_file_write_iter(struct kiocb *iocb,
 	 * to stop this write operation to ensure FS consistency.
 	 */
 	if (test_bit(BTRFS_FS_STATE_ERROR, &root->fs_info->fs_state)) {
-		mutex_unlock(&inode->i_mutex);
+		inode_unlock(inode);
 		err = -EROFS;
 		goto out;
 	}
@@ -1804,7 +1804,7 @@ static ssize_t btrfs_file_write_iter(struct kiocb *iocb,
 		end_pos = round_up(pos + count, root->sectorsize);
 		err = btrfs_cont_expand(inode, i_size_read(inode), end_pos);
 		if (err) {
-			mutex_unlock(&inode->i_mutex);
+			inode_unlock(inode);
 			goto out;
 		}
 	}
@@ -1820,7 +1820,7 @@ static ssize_t btrfs_file_write_iter(struct kiocb *iocb,
 			iocb->ki_pos = pos + num_written;
 	}
 
-	mutex_unlock(&inode->i_mutex);
+	inode_unlock(inode);
 
 	/*
 	 * We also have to set last_sub_trans to the current log transid,
@@ -1909,7 +1909,7 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 	if (ret)
 		return ret;
 
-	mutex_lock(&inode->i_mutex);
+	inode_lock(inode);
 	atomic_inc(&root->log_batch);
 	full_sync = test_bit(BTRFS_INODE_NEEDS_FULL_SYNC,
 			     &BTRFS_I(inode)->runtime_flags);
@@ -1961,7 +1961,7 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 		ret = start_ordered_ops(inode, start, end);
 	}
 	if (ret) {
-		mutex_unlock(&inode->i_mutex);
+		inode_unlock(inode);
 		goto out;
 	}
 	atomic_inc(&root->log_batch);
@@ -2007,7 +2007,7 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 		 */
 		clear_bit(BTRFS_INODE_NEEDS_FULL_SYNC,
 			  &BTRFS_I(inode)->runtime_flags);
-		mutex_unlock(&inode->i_mutex);
+		inode_unlock(inode);
 		goto out;
 	}
 
@@ -2031,7 +2031,7 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 	trans = btrfs_start_transaction(root, 0);
 	if (IS_ERR(trans)) {
 		ret = PTR_ERR(trans);
-		mutex_unlock(&inode->i_mutex);
+		inode_unlock(inode);
 		goto out;
 	}
 	trans->sync = true;
@@ -2054,7 +2054,7 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 	 * file again, but that will end up using the synchronization
 	 * inside btrfs_sync_log to keep things safe.
 	 */
-	mutex_unlock(&inode->i_mutex);
+	inode_unlock(inode);
 
 	/*
 	 * If any of the ordered extents had an error, just return it to user
@@ -2303,7 +2303,7 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 	if (ret)
 		return ret;
 
-	mutex_lock(&inode->i_mutex);
+	inode_lock(inode);
 	ino_size = round_up(inode->i_size, PAGE_CACHE_SIZE);
 	ret = find_first_non_hole(inode, &offset, &len);
 	if (ret < 0)
@@ -2343,7 +2343,7 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 		truncated_page = true;
 		ret = btrfs_truncate_page(inode, offset, 0, 0);
 		if (ret) {
-			mutex_unlock(&inode->i_mutex);
+			inode_unlock(inode);
 			return ret;
 		}
 	}
@@ -2419,7 +2419,7 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 		ret = btrfs_wait_ordered_range(inode, lockstart,
 					       lockend - lockstart + 1);
 		if (ret) {
-			mutex_unlock(&inode->i_mutex);
+			inode_unlock(inode);
 			return ret;
 		}
 	}
@@ -2574,7 +2574,7 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 			ret = btrfs_end_transaction(trans, root);
 		}
 	}
-	mutex_unlock(&inode->i_mutex);
+	inode_unlock(inode);
 	if (ret && !err)
 		err = ret;
 	return err;
@@ -2658,7 +2658,7 @@ static long btrfs_fallocate(struct file *file, int mode,
 	if (ret < 0)
 		return ret;
 
-	mutex_lock(&inode->i_mutex);
+	inode_lock(inode);
 	ret = inode_newsize_ok(inode, alloc_end);
 	if (ret)
 		goto out;
@@ -2816,7 +2816,7 @@ static long btrfs_fallocate(struct file *file, int mode,
 	 * So this is completely used as cleanup.
 	 */
 	btrfs_qgroup_free_data(inode, alloc_start, alloc_end - alloc_start);
-	mutex_unlock(&inode->i_mutex);
+	inode_unlock(inode);
 	/* Let go of our reservation. */
 	btrfs_free_reserved_data_space(inode, alloc_start,
 				       alloc_end - alloc_start);
@@ -2892,7 +2892,7 @@ static loff_t btrfs_file_llseek(struct file *file, loff_t offset, int whence)
 	struct inode *inode = file->f_mapping->host;
 	int ret;
 
-	mutex_lock(&inode->i_mutex);
+	inode_lock(inode);
 	switch (whence) {
 	case SEEK_END:
 	case SEEK_CUR:
@@ -2901,20 +2901,20 @@ static loff_t btrfs_file_llseek(struct file *file, loff_t offset, int whence)
 	case SEEK_DATA:
 	case SEEK_HOLE:
 		if (offset >= i_size_read(inode)) {
-			mutex_unlock(&inode->i_mutex);
+			inode_unlock(inode);
 			return -ENXIO;
 		}
 
 		ret = find_desired_extent(inode, &offset, whence);
 		if (ret) {
-			mutex_unlock(&inode->i_mutex);
+			inode_unlock(inode);
 			return ret;
 		}
 	}
 
 	offset = vfs_setpos(file, offset, inode->i_sb->s_maxbytes);
 out:
-	mutex_unlock(&inode->i_mutex);
+	inode_unlock(inode);
 	return offset;
 }
 

commit 2101ae42899a14fe7caa73114e2161e778328661
Merge: 391f2a16b74b a6111d11b8b5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jan 22 11:49:21 2016 -0800

    Merge branch 'for-linus-4.5' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs
    
    Pull more btrfs updates from Chris Mason:
     "These are mostly fixes that we've been testing, but also we grabbed
      and tested a few small cleanups that had been on the list for a while.
    
      Zhao Lei's patchset also fixes some early ENOSPC buglets"
    
    * 'for-linus-4.5' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs: (21 commits)
      btrfs: raid56: Use raid_write_end_io for scrub
      btrfs: Remove unnecessary ClearPageUptodate for raid56
      btrfs: use rbio->nr_pages to reduce calculation
      btrfs: Use unified stripe_page's index calculation
      btrfs: Fix calculation of rbio->dbitmap's size calculation
      btrfs: Fix no_space in write and rm loop
      btrfs: merge functions for wait snapshot creation
      btrfs: delete unused argument in btrfs_copy_from_user
      btrfs: Use direct way to determine raid56 write/recover mode
      btrfs: Small cleanup for get index_srcdev loop
      btrfs: Enhance chunk validation check
      btrfs: Enhance super validation check
      Btrfs: fix deadlock running delayed iputs at transaction commit time
      Btrfs: fix typo in log message when starting a balance
      btrfs: remove duplicate const specifier
      btrfs: initialize the seq counter in struct btrfs_device
      Btrfs: clean up an error code in btrfs_init_space_info()
      btrfs: fix iterator with update error in backref.c
      Btrfs: fix output of compression message in btrfs_parse_options()
      Btrfs: Initialize btrfs_root->highest_objectid when loading tree root and subvolume roots
      ...

commit ee22f0c4ec428e7f16d3c5fa1d55fd7860cb304a
Author: Zhao Lei <zhaolei@cn.fujitsu.com>
Date:   Wed Jan 6 18:47:31 2016 +0800

    btrfs: delete unused argument in btrfs_copy_from_user
    
    size_t write_bytes is not necessary for btrfs_copy_from_user(),
    delete it.
    
    Signed-off-by: Zhao Lei <zhaolei@cn.fujitsu.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 364e0f1f61f6..af782fdd4fca 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -406,8 +406,7 @@ int btrfs_run_defrag_inodes(struct btrfs_fs_info *fs_info)
 /* simple helper to fault in pages and copy.  This should go away
  * and be replaced with calls into generic code.
  */
-static noinline int btrfs_copy_from_user(loff_t pos, int num_pages,
-					 size_t write_bytes,
+static noinline int btrfs_copy_from_user(loff_t pos, size_t write_bytes,
 					 struct page **prepared_pages,
 					 struct iov_iter *i)
 {
@@ -1588,8 +1587,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 			ret = 0;
 		}
 
-		copied = btrfs_copy_from_user(pos, num_pages,
-					   write_bytes, pages, i);
+		copied = btrfs_copy_from_user(pos, write_bytes, pages, i);
 
 		/*
 		 * if we have trouble faulting in the pages, fall

commit c1a198d9235b9e7d6942027374e44f78ebdcb455
Merge: 48f58ba9cbff 988f1f576d4f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jan 18 12:44:40 2016 -0800

    Merge branch 'for-linus-4.5' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs
    
    Pull btrfs updates from Chris Mason:
     "This has our usual assortment of fixes and cleanups, but the biggest
      change included is Omar Sandoval's free space tree.  It's not the
      default yet, mounting -o space_cache=v2 enables it and sets a readonly
      compat bit.  The tree can actually be deleted and regenerated if there
      are any problems, but it has held up really well in testing so far.
    
      For very large filesystems (30T+) our existing free space caching code
      can end up taking a huge amount of time during commits.  The new tree
      based code is faster and less work overall to update as the commit
      progresses.
    
      Omar worked on this during the summer and we'll hammer on it in
      production here at FB over the next few months"
    
    * 'for-linus-4.5' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs: (73 commits)
      Btrfs: fix fitrim discarding device area reserved for boot loader's use
      Btrfs: Check metadata redundancy on balance
      btrfs: statfs: report zero available if metadata are exhausted
      btrfs: preallocate path for snapshot creation at ioctl time
      btrfs: allocate root item at snapshot ioctl time
      btrfs: do an allocation earlier during snapshot creation
      btrfs: use smaller type for btrfs_path locks
      btrfs: use smaller type for btrfs_path lowest_level
      btrfs: use smaller type for btrfs_path reada
      btrfs: cleanup, use enum values for btrfs_path reada
      btrfs: constify static arrays
      btrfs: constify remaining structs with function pointers
      btrfs tests: replace whole ops structure for free space tests
      btrfs: use list_for_each_entry* in backref.c
      btrfs: use list_for_each_entry_safe in free-space-cache.c
      btrfs: use list_for_each_entry* in check-integrity.c
      Btrfs: use linux/sizes.h to represent constants
      btrfs: cleanup, remove stray return statements
      btrfs: zero out delayed node upon allocation
      btrfs: pass proper enum type to start_transaction()
      ...

commit fce205e9da8e063aa1cf3d6583c1a9ed2b82f3f0
Merge: 065019a38fea 2b3909f8a7fe
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jan 12 16:30:34 2016 -0800

    Merge branch 'work.copy_file_range' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs
    
    Pull vfs copy_file_range updates from Al Viro:
     "Several series around copy_file_range/CLONE"
    
    * 'work.copy_file_range' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs:
      btrfs: use new dedupe data function pointer
      vfs: hoist the btrfs deduplication ioctl to the vfs
      vfs: wire up compat ioctl for CLONE/CLONE_RANGE
      cifs: avoid unused variable and label
      nfsd: implement the NFSv4.2 CLONE operation
      nfsd: Pass filehandle to nfs4_preprocess_stateid_op()
      vfs: pull btrfs clone API to vfs layer
      locks: new locks_mandatory_area calling convention
      vfs: Add vfs_copy_file_range() support for pagecache copies
      btrfs: add .copy_file_range file operation
      x86: add sys_copy_file_range to syscall tables
      vfs: add copy_file_range syscall and vfs helper

commit 2b3909f8a7fe94e0234850aa9d120cca15b6e1f7
Author: Darrick J. Wong <darrick.wong@oracle.com>
Date:   Sat Dec 19 00:56:05 2015 -0800

    btrfs: use new dedupe data function pointer
    
    Now that the VFS encapsulates the dedupe ioctl, wire up btrfs to it.
    
    Signed-off-by: Darrick J. Wong <darrick.wong@oracle.com>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 232e300a6c93..d012e0a96ec3 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2926,6 +2926,7 @@ const struct file_operations btrfs_file_operations = {
 #endif
 	.copy_file_range = btrfs_copy_file_range,
 	.clone_file_range = btrfs_clone_file_range,
+	.dedupe_file_range = btrfs_dedupe_file_range,
 };
 
 void btrfs_auto_defrag_exit(void)

commit bb9d687618695e8291f1e6209eb3211d231f97bb
Merge: 13d5d15d6301 cd716d8fea12
Author: Chris Mason <clm@fb.com>
Date:   Wed Dec 23 13:17:42 2015 -0800

    Merge branch 'dev/simplify-set-bit' of git://git.kernel.org/pub/scm/linux/kernel/git/kdave/linux into for-linus-4.5
    
    Signed-off-by: Chris Mason <clm@fb.com>

commit fc315e3e5c9418df6ce5cee97fd4adcce9dcf24e
Merge: 8b4414f51d09 1d3a5a82fe72
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Dec 18 15:35:08 2015 -0800

    Merge branch 'for-linus-4.4' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs
    
    Pull btrfs fixes from Chris Mason:
     "A couple of small fixes"
    
    * 'for-linus-4.4' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs:
      Btrfs: check prepare_uptodate_page() error code earlier
      Btrfs: check for empty bitmap list in setup_cluster_bitmaps
      btrfs: fix misleading warning when space cache failed to load
      Btrfs: fix transaction handle leak in balance
      Btrfs: fix unprotected list move from unused_bgs to deleted_bgs list

commit bb1591b4ea1a1485ebc79be4e4748e94f96c670b
Author: Chris Mason <clm@fb.com>
Date:   Mon Dec 14 15:40:44 2015 -0800

    Btrfs: check prepare_uptodate_page() error code earlier
    
    prepare_pages() may end up calling prepare_uptodate_page() twice if our
    write only spans a single page.  But if the first call returns an error,
    our page will be unlocked and its not safe to call it again.
    
    This bug goes all the way back to 2011, and it's not something commonly
    hit.
    
    While we're here, add a more explicit check for the page being truncated
    away.  The bare lock_page() alone is protected only by good thoughts and
    i_mutex, which we're sure to regret eventually.
    
    Reported-by: Dave Jones <dsj@fb.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 8eb1f3c1b647..870150547f5c 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1291,7 +1291,8 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
  * on error we return an unlocked page and the error value
  * on success we return a locked page and 0
  */
-static int prepare_uptodate_page(struct page *page, u64 pos,
+static int prepare_uptodate_page(struct inode *inode,
+				 struct page *page, u64 pos,
 				 bool force_uptodate)
 {
 	int ret = 0;
@@ -1306,6 +1307,10 @@ static int prepare_uptodate_page(struct page *page, u64 pos,
 			unlock_page(page);
 			return -EIO;
 		}
+		if (page->mapping != inode->i_mapping) {
+			unlock_page(page);
+			return -EAGAIN;
+		}
 	}
 	return 0;
 }
@@ -1324,6 +1329,7 @@ static noinline int prepare_pages(struct inode *inode, struct page **pages,
 	int faili;
 
 	for (i = 0; i < num_pages; i++) {
+again:
 		pages[i] = find_or_create_page(inode->i_mapping, index + i,
 					       mask | __GFP_WRITE);
 		if (!pages[i]) {
@@ -1333,13 +1339,17 @@ static noinline int prepare_pages(struct inode *inode, struct page **pages,
 		}
 
 		if (i == 0)
-			err = prepare_uptodate_page(pages[i], pos,
+			err = prepare_uptodate_page(inode, pages[i], pos,
 						    force_uptodate);
-		if (i == num_pages - 1)
-			err = prepare_uptodate_page(pages[i],
+		if (!err && i == num_pages - 1)
+			err = prepare_uptodate_page(inode, pages[i],
 						    pos + write_bytes, false);
 		if (err) {
 			page_cache_release(pages[i]);
+			if (err == -EAGAIN) {
+				err = 0;
+				goto again;
+			}
 			faili = i - 1;
 			goto fail;
 		}

commit 04b38d601239b4d9be641b412cf4b7456a041c67
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Dec 3 12:59:50 2015 +0100

    vfs: pull btrfs clone API to vfs layer
    
    The btrfs clone ioctls are now adopted by other file systems, with NFS
    and CIFS already having support for them, and XFS being under active
    development.  To avoid growth of various slightly incompatible
    implementations, add one to the VFS.  Note that clones are different from
    file copies in several ways:
    
     - they are atomic vs other writers
     - they support whole file clones
     - they support 64-bit legth clones
     - they do not allow partial success (aka short writes)
     - clones are expected to be a fast metadata operation
    
    Because of that it would be rather cumbersome to try to piggyback them on
    top of the recent clone_file_range infrastructure.  The converse isn't
    true and the clone_file_range system call could try clone file range as
    a first attempt to copy, something that further patches will enable.
    
    Based on earlier work from Peng Tao.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index e67fe6ab8c9e..232e300a6c93 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2925,6 +2925,7 @@ const struct file_operations btrfs_file_operations = {
 	.compat_ioctl	= btrfs_ioctl,
 #endif
 	.copy_file_range = btrfs_copy_file_range,
+	.clone_file_range = btrfs_clone_file_range,
 };
 
 void btrfs_auto_defrag_exit(void)

commit ff13db41f184f8222aca0cb653347ccdd48a057a
Author: David Sterba <dsterba@suse.com>
Date:   Thu Dec 3 14:30:40 2015 +0100

    btrfs: drop unused parameter from lock_extent_bits
    
    We've always passed 0. Stack usage will slightly decrease.
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 977e715f0bf2..8a00298f8094 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1384,7 +1384,7 @@ lock_and_cleanup_extent_if_need(struct inode *inode, struct page **pages,
 	if (start_pos < inode->i_size) {
 		struct btrfs_ordered_extent *ordered;
 		lock_extent_bits(&BTRFS_I(inode)->io_tree,
-				 start_pos, last_pos, 0, cached_state);
+				 start_pos, last_pos, cached_state);
 		ordered = btrfs_lookup_ordered_range(inode, start_pos,
 						     last_pos - start_pos + 1);
 		if (ordered &&
@@ -2384,7 +2384,7 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 		truncate_pagecache_range(inode, lockstart, lockend);
 
 		lock_extent_bits(&BTRFS_I(inode)->io_tree, lockstart, lockend,
-				 0, &cached_state);
+				 &cached_state);
 		ordered = btrfs_lookup_first_ordered_extent(inode, lockend);
 
 		/*
@@ -2691,7 +2691,7 @@ static long btrfs_fallocate(struct file *file, int mode,
 		 * transaction
 		 */
 		lock_extent_bits(&BTRFS_I(inode)->io_tree, alloc_start,
-				 locked_end, 0, &cached_state);
+				 locked_end, &cached_state);
 		ordered = btrfs_lookup_first_ordered_extent(inode,
 							    alloc_end - 1);
 		if (ordered &&
@@ -2838,7 +2838,7 @@ static int find_desired_extent(struct inode *inode, loff_t *offset, int whence)
 	lockend--;
 	len = lockend - lockstart + 1;
 
-	lock_extent_bits(&BTRFS_I(inode)->io_tree, lockstart, lockend, 0,
+	lock_extent_bits(&BTRFS_I(inode)->io_tree, lockstart, lockend,
 			 &cached_state);
 
 	while (start < inode->i_size) {

commit 3db11b2eecc02dc0eee943e71822c6d929281aa7
Author: Zach Brown <zab@redhat.com>
Date:   Tue Nov 10 16:53:32 2015 -0500

    btrfs: add .copy_file_range file operation
    
    This rearranges the existing COPY_RANGE ioctl implementation so that the
    .copy_file_range file operation can call the core loop that copies file
    data extent items.
    
    The extent copying loop is lifted up into its own function.  It retains
    the core btrfs error checks that should be shared.
    
    Signed-off-by: Zach Brown <zab@redhat.com>
    [Anna Schumaker: Make flags an unsigned int,
                     Check for COPY_FR_REFLINK]
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>
    Reviewed-by: Josef Bacik <jbacik@fb.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 72e73461c064..e67fe6ab8c9e 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2924,6 +2924,7 @@ const struct file_operations btrfs_file_operations = {
 #ifdef CONFIG_COMPAT
 	.compat_ioctl	= btrfs_ioctl,
 #endif
+	.copy_file_range = btrfs_copy_file_range,
 };
 
 void btrfs_auto_defrag_exit(void)

commit 80e0c505b22e64ab1787285b182a1e8395e53991
Merge: 7e4b9359f483 dba72cb30b6a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Nov 27 15:45:45 2015 -0800

    Merge branch 'for-linus-4.4' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs
    
    Pull btrfs fixes from Chris Mason:
     "This has Mark Fasheh's patches to fix quota accounting during subvol
      deletion, which we've been working on for a while now.  The patch is
      pretty small but it's a key fix.
    
      Otherwise it's a random assortment"
    
    * 'for-linus-4.4' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs:
      btrfs: fix balance range usage filters in 4.4-rc
      btrfs: qgroup: account shared subtree during snapshot delete
      Btrfs: use btrfs_get_fs_root in resolve_indirect_ref
      btrfs: qgroup: fix quota disable during rescan
      Btrfs: fix race between cleaner kthread and space cache writeout
      Btrfs: fix scrub preventing unused block groups from being deleted
      Btrfs: fix race between scrub and block group deletion
      btrfs: fix rcu warning during device replace
      btrfs: Continue replace when set_block_ro failed
      btrfs: fix clashing number of the enhanced balance usage filter
      Btrfs: fix the number of transaction units needed to remove a block group
      Btrfs: use global reserve when deleting unused block group after ENOSPC
      Btrfs: tests: checking for NULL instead of IS_ERR()
      btrfs: fix signed overflows in btrfs_sync_file

commit 9dcbeed4d7e11e1dcf5e55475de3754f0855d1c2
Author: David Sterba <dsterba@suse.com>
Date:   Mon Nov 9 11:44:45 2015 +0100

    btrfs: fix signed overflows in btrfs_sync_file
    
    The calculation of range length in btrfs_sync_file leads to signed
    overflow. This was caught by PaX gcc SIZE_OVERFLOW plugin.
    
    https://forums.grsecurity.net/viewtopic.php?f=1&t=4284
    
    The fsync call passes 0 and LLONG_MAX, the range length does not fit to
    loff_t and overflows, but the value is converted to u64 so it silently
    works as expected.
    
    The minimal fix is a typecast to u64, switching functions to take
    (start, end) instead of (start, len) would be more intrusive.
    
    Coccinelle script found that there's one more opencoded calculation of
    the length.
    
    <smpl>
    @@
    loff_t start, end;
    @@
    * end - start
    </smpl>
    
    CC: stable@vger.kernel.org
    Signed-off-by: David Sterba <dsterba@suse.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 3009d4536d38..8eb1f3c1b647 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1882,8 +1882,13 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 	struct btrfs_log_ctx ctx;
 	int ret = 0;
 	bool full_sync = 0;
-	const u64 len = end - start + 1;
+	u64 len;
 
+	/*
+	 * The range length can be represented by u64, we have to do the typecasts
+	 * to avoid signed overflow if it's [0, LLONG_MAX] eg. from fsync()
+	 */
+	len = (u64)end - (u64)start + 1;
 	trace_btrfs_sync_file(file, datasync);
 
 	/*
@@ -2071,8 +2076,7 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 			}
 		}
 		if (!full_sync) {
-			ret = btrfs_wait_ordered_range(inode, start,
-						       end - start + 1);
+			ret = btrfs_wait_ordered_range(inode, start, len);
 			if (ret) {
 				btrfs_end_transaction(trans, root);
 				goto out;

commit e75cdf9898132f521df98a3ce1c280a2f85d360a
Merge: ca4ba96e02e9 d5f2e33b92b1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Nov 13 16:30:29 2015 -0800

    Merge branch 'for-linus-4.4' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs
    
    Pull btrfs fixes and cleanups from Chris Mason:
     "Some of this got cherry-picked from a github repo this week, but I
      verified the patches.
    
      We have three small scrub cleanups and a collection of fixes"
    
    * 'for-linus-4.4' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs:
      btrfs: Use fs_info directly in btrfs_delete_unused_bgs
      btrfs: Fix lost-data-profile caused by balance bg
      btrfs: Fix lost-data-profile caused by auto removing bg
      btrfs: Remove len argument from scrub_find_csum
      btrfs: Reduce unnecessary arguments in scrub_recheck_block
      btrfs: Use scrub_checksum_data and scrub_checksum_tree_block for scrub_recheck_block_checksum
      btrfs: Reset sblock->xxx_error stats before calling scrub_recheck_block_checksum
      btrfs: scrub: setup all fields for sblock_to_check
      btrfs: scrub: set error stats when tree block spanning stripes
      Btrfs: fix race when listing an inode's xattrs
      Btrfs: fix race leading to BUG_ON when running delalloc for nodatacow
      Btrfs: fix race leading to incorrect item deletion when dropping extents
      Btrfs: fix sleeping inside atomic context in qgroup rescan worker
      Btrfs: fix race waiting for qgroup rescan worker
      btrfs: qgroup: exit the rescan worker during umount
      Btrfs: fix extent accounting for partial direct IO writes

commit aeafbf8486c9e2bd53f5cc3c10c0b7fd7149d69c
Author: Filipe Manana <fdmanana@suse.com>
Date:   Fri Nov 6 13:33:33 2015 +0000

    Btrfs: fix race leading to incorrect item deletion when dropping extents
    
    While running a stress test I got the following warning triggered:
    
      [191627.672810] ------------[ cut here ]------------
      [191627.673949] WARNING: CPU: 8 PID: 8447 at fs/btrfs/file.c:779 __btrfs_drop_extents+0x391/0xa50 [btrfs]()
      (...)
      [191627.701485] Call Trace:
      [191627.702037]  [<ffffffff8145f077>] dump_stack+0x4f/0x7b
      [191627.702992]  [<ffffffff81095de5>] ? console_unlock+0x356/0x3a2
      [191627.704091]  [<ffffffff8104b3b0>] warn_slowpath_common+0xa1/0xbb
      [191627.705380]  [<ffffffffa0664499>] ? __btrfs_drop_extents+0x391/0xa50 [btrfs]
      [191627.706637]  [<ffffffff8104b46d>] warn_slowpath_null+0x1a/0x1c
      [191627.707789]  [<ffffffffa0664499>] __btrfs_drop_extents+0x391/0xa50 [btrfs]
      [191627.709155]  [<ffffffff8115663c>] ? cache_alloc_debugcheck_after.isra.32+0x171/0x1d0
      [191627.712444]  [<ffffffff81155007>] ? kmemleak_alloc_recursive.constprop.40+0x16/0x18
      [191627.714162]  [<ffffffffa06570c9>] insert_reserved_file_extent.constprop.40+0x83/0x24e [btrfs]
      [191627.715887]  [<ffffffffa065422b>] ? start_transaction+0x3bb/0x610 [btrfs]
      [191627.717287]  [<ffffffffa065b604>] btrfs_finish_ordered_io+0x273/0x4e2 [btrfs]
      [191627.728865]  [<ffffffffa065b888>] finish_ordered_fn+0x15/0x17 [btrfs]
      [191627.730045]  [<ffffffffa067d688>] normal_work_helper+0x14c/0x32c [btrfs]
      [191627.731256]  [<ffffffffa067d96a>] btrfs_endio_write_helper+0x12/0x14 [btrfs]
      [191627.732661]  [<ffffffff81061119>] process_one_work+0x24c/0x4ae
      [191627.733822]  [<ffffffff810615b0>] worker_thread+0x206/0x2c2
      [191627.734857]  [<ffffffff810613aa>] ? process_scheduled_works+0x2f/0x2f
      [191627.736052]  [<ffffffff810613aa>] ? process_scheduled_works+0x2f/0x2f
      [191627.737349]  [<ffffffff810669a6>] kthread+0xef/0xf7
      [191627.738267]  [<ffffffff810f3b3a>] ? time_hardirqs_on+0x15/0x28
      [191627.739330]  [<ffffffff810668b7>] ? __kthread_parkme+0xad/0xad
      [191627.741976]  [<ffffffff81465592>] ret_from_fork+0x42/0x70
      [191627.743080]  [<ffffffff810668b7>] ? __kthread_parkme+0xad/0xad
      [191627.744206] ---[ end trace bbfddacb7aaada8d ]---
    
      $ cat -n fs/btrfs/file.c
      691  int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
      (...)
      758                  btrfs_item_key_to_cpu(leaf, &key, path->slots[0]);
      759                  if (key.objectid > ino ||
      760                      key.type > BTRFS_EXTENT_DATA_KEY || key.offset >= end)
      761                          break;
      762
      763                  fi = btrfs_item_ptr(leaf, path->slots[0],
      764                                      struct btrfs_file_extent_item);
      765                  extent_type = btrfs_file_extent_type(leaf, fi);
      766
      767                  if (extent_type == BTRFS_FILE_EXTENT_REG ||
      768                      extent_type == BTRFS_FILE_EXTENT_PREALLOC) {
      (...)
      774                  } else if (extent_type == BTRFS_FILE_EXTENT_INLINE) {
      (...)
      778                  } else {
      779                          WARN_ON(1);
      780                          extent_end = search_start;
      781                  }
      (...)
    
    This happened because the item we were processing did not match a file
    extent item (its key type != BTRFS_EXTENT_DATA_KEY), and even on this
    case we cast the item to a struct btrfs_file_extent_item pointer and
    then find a type field value that does not match any of the expected
    values (BTRFS_FILE_EXTENT_[REG|PREALLOC|INLINE]). This scenario happens
    due to a tiny time window where a race can happen as exemplified below.
    For example, consider the following scenario where we're using the
    NO_HOLES feature and we have the following two neighbour leafs:
    
                   Leaf X (has N items)                    Leaf Y
    
    [ ... (257 INODE_ITEM 0) (257 INODE_REF 256) ]  [ (257 EXTENT_DATA 8192), ... ]
              slot N - 2         slot N - 1              slot 0
    
    Our inode 257 has an implicit hole in the range [0, 8K[ (implicit rather
    than explicit because NO_HOLES is enabled). Now if our inode has an
    ordered extent for the range [4K, 8K[ that is finishing, the following
    can happen:
    
              CPU 1                                       CPU 2
    
      btrfs_finish_ordered_io()
        insert_reserved_file_extent()
          __btrfs_drop_extents()
             Searches for the key
              (257 EXTENT_DATA 4096) through
              btrfs_lookup_file_extent()
    
             Key not found and we get a path where
             path->nodes[0] == leaf X and
             path->slots[0] == N
    
             Because path->slots[0] is >=
             btrfs_header_nritems(leaf X), we call
             btrfs_next_leaf()
    
             btrfs_next_leaf() releases the path
    
                                                      inserts key
                                                      (257 INODE_REF 4096)
                                                      at the end of leaf X,
                                                      leaf X now has N + 1 keys,
                                                      and the new key is at
                                                      slot N
    
             btrfs_next_leaf() searches for
             key (257 INODE_REF 256), with
             path->keep_locks set to 1,
             because it was the last key it
             saw in leaf X
    
               finds it in leaf X again and
               notices it's no longer the last
               key of the leaf, so it returns 0
               with path->nodes[0] == leaf X and
               path->slots[0] == N (which is now
               < btrfs_header_nritems(leaf X)),
               pointing to the new key
               (257 INODE_REF 4096)
    
             __btrfs_drop_extents() casts the
             item at path->nodes[0], slot
             path->slots[0], to a struct
             btrfs_file_extent_item - it does
             not skip keys for the target
             inode with a type less than
             BTRFS_EXTENT_DATA_KEY
             (BTRFS_INODE_REF_KEY < BTRFS_EXTENT_DATA_KEY)
    
             sees a bogus value for the type
             field triggering the WARN_ON in
             the trace shown above, and sets
             extent_end = search_start (4096)
    
             does the if-then-else logic to
             fixup 0 length extent items created
             by a past bug from hole punching:
    
               if (extent_end == key.offset &&
                   extent_end >= search_start)
                   goto delete_extent_item;
    
             that evaluates to true and it ends
             up deleting the key pointed to by
             path->slots[0], (257 INODE_REF 4096),
             from leaf X
    
    The same could happen for example for a xattr that ends up having a key
    with an offset value that matches search_start (very unlikely but not
    impossible).
    
    So fix this by ensuring that keys smaller than BTRFS_EXTENT_DATA_KEY are
    skipped, never casted to struct btrfs_file_extent_item and never deleted
    by accident. Also protect against the unexpected case of getting a key
    for a lower inode number by skipping that key and issuing a warning.
    
    Cc: stable@vger.kernel.org
    Signed-off-by: Filipe Manana <fdmanana@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index cfa243c3ad13..3009d4536d38 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -756,8 +756,16 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 		}
 
 		btrfs_item_key_to_cpu(leaf, &key, path->slots[0]);
-		if (key.objectid > ino ||
-		    key.type > BTRFS_EXTENT_DATA_KEY || key.offset >= end)
+
+		if (key.objectid > ino)
+			break;
+		if (WARN_ON_ONCE(key.objectid < ino) ||
+		    key.type < BTRFS_EXTENT_DATA_KEY) {
+			ASSERT(del_nr == 0);
+			path->slots[0]++;
+			goto next_slot;
+		}
+		if (key.type > BTRFS_EXTENT_DATA_KEY || key.offset >= end)
 			break;
 
 		fi = btrfs_item_ptr(leaf, path->slots[0],
@@ -776,8 +784,8 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 				btrfs_file_extent_inline_len(leaf,
 						     path->slots[0], fi);
 		} else {
-			WARN_ON(1);
-			extent_end = search_start;
+			/* can't happen */
+			BUG();
 		}
 
 		/*

commit 27eb427bdc0960ad64b72da03e3596c801e7a9e9
Merge: 713009809681 2959a32a858a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Nov 6 17:17:13 2015 -0800

    Merge branch 'for-linus-4.4' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs
    
    Pull btrfs updates from Chris Mason:
     "We have a lot of subvolume quota improvements in here, along with big
      piles of cleanups from Dave Sterba and Anand Jain and others.
    
      Josef pitched in a batch of allocator fixes based on production use
      here at FB.  We found that mount -o ssd_spread greatly improved our
      performance on hardware raid5/6, but it exposed some CPU bottlenecks
      in the allocator.  These patches make a huge difference"
    
    * 'for-linus-4.4' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs: (100 commits)
      Btrfs: fix hole punching when using the no-holes feature
      Btrfs: find_free_extent: Do not erroneously skip LOOP_CACHING_WAIT state
      btrfs: Fix a data space underflow warning
      btrfs: qgroup: Fix a rebase bug which will cause qgroup double free
      btrfs: qgroup: Fix a race in delayed_ref which leads to abort trans
      btrfs: clear PF_NOFREEZE in cleaner_kthread()
      btrfs: qgroup: Don't copy extent buffer to do qgroup rescan
      btrfs: add balance filters limits, stripes and usage to supported mask
      btrfs: extend balance filter usage to take minimum and maximum
      btrfs: add balance filter for stripes
      btrfs: extend balance filter limit to take minimum and maximum
      btrfs: fix use after free iterating extrefs
      btrfs: check unsupported filters in balance arguments
      Btrfs: fix regression running delayed references when using qgroups
      Btrfs: fix regression when running delayed references
      Btrfs: don't do extra bitmap search in one bit case
      Btrfs: keep track of largest extent in bitmaps
      Btrfs: don't keep trying to build clusters if we are fragmented
      Btrfs: cut down on loops through the allocator
      Btrfs: don't continue setting up space cache when enospc
      ...

commit 2959a32a858a2c44bbbce83d19c158d54cc5998a
Author: Filipe Manana <fdmanana@suse.com>
Date:   Mon Nov 2 12:32:44 2015 +0000

    Btrfs: fix hole punching when using the no-holes feature
    
    When we are using the no-holes feature, if we punch a hole into a file
    range that already contains a hole which overlaps the range we are passing
    to fallocate(), we end up removing the extent map that represents the
    existing hole without adding a new one. This happens because with the
    no-holes feature we do not have explicit extent items to represent holes
    and therefore the call to __btrfs_drop_extents(), made from
    btrfs_punch_hole(), returns an end offset to the variable drop_end that
    is smaller than the end of the range passed to fallocate(), while it
    drops all existing extent maps in that range.
    Normally having a missing extent map is not a problem, for example for
    a readpages() operation we just end up building the extent map by
    looking at the fs/subvol tree for a matching extent item (or a lack of
    one for implicit holes). However for an fsync that uses the fast path,
    which needs to look at the list of modified extent maps, this means
    the fsync will not record information about the complete hole we had
    before the fallocate() call into the log tree, resulting in a file with
    content/layout that does not match what we had neither before nor after
    the hole punch operation.
    
    The following test case for fstests reproduces the issue. It fails without
    this change because we get a file with a different digest after the fsync
    log replay and also with a different extent/hole layout.
    
      seq=`basename $0`
      seqres=$RESULT_DIR/$seq
      echo "QA output created by $seq"
      tmp=/tmp/$$
      status=1      # failure is the default!
      trap "_cleanup; exit \$status" 0 1 2 3 15
    
      _cleanup()
      {
         _cleanup_flakey
         rm -f $tmp.*
      }
    
      # get standard environment, filters and checks
      . ./common/rc
      . ./common/filter
      . ./common/punch
      . ./common/dmflakey
    
      # real QA test starts here
      _need_to_be_root
      _supported_fs generic
      _supported_os Linux
      _require_scratch
      _require_xfs_io_command "fpunch"
      _require_xfs_io_command "fiemap"
      _require_dm_target flakey
      _require_metadata_journaling $SCRATCH_DEV
    
      # This test was motivated by an issue found in btrfs when the btrfs
      # no-holes feature is enabled (introduced in kernel 3.14). So enable
      # the feature if the fs being tested is btrfs.
      if [ $FSTYP == "btrfs" ]; then
          _require_btrfs_fs_feature "no_holes"
          _require_btrfs_mkfs_feature "no-holes"
          MKFS_OPTIONS="$MKFS_OPTIONS -O no-holes"
      fi
    
      rm -f $seqres.full
    
      _scratch_mkfs >>$seqres.full 2>&1
      _init_flakey
      _mount_flakey
    
      # Create out test file with some data and then fsync it.
      # We do the fsync only to make sure the last fsync we do in this test
      # triggers the fast code path of btrfs' fsync implementation, a
      # condition necessary to trigger the bug btrfs had.
      $XFS_IO_PROG -f -c "pwrite -S 0xaa 0K 128K" \
                      -c "fsync"                  \
                      $SCRATCH_MNT/foobar | _filter_xfs_io
    
      # Now punch a hole against the range [96K, 128K[.
      $XFS_IO_PROG -c "fpunch 96K 32K" $SCRATCH_MNT/foobar
    
      # Punch another hole against a range that overlaps the previous range
      # and ends beyond eof.
      $XFS_IO_PROG -c "fpunch 64K 128K" $SCRATCH_MNT/foobar
    
      # Punch another hole against a range that overlaps the first range
      # ([96K, 128K[) and ends at eof.
      $XFS_IO_PROG -c "fpunch 32K 96K" $SCRATCH_MNT/foobar
    
      # Fsync our file. We want to verify that, after a power failure and
      # mounting the filesystem again, the file content reflects all the hole
      # punch operations.
      $XFS_IO_PROG -c "fsync" $SCRATCH_MNT/foobar
    
      echo "File digest before power failure:"
      md5sum $SCRATCH_MNT/foobar | _filter_scratch
    
      echo "Fiemap before power failure:"
      $XFS_IO_PROG -c "fiemap -v" $SCRATCH_MNT/foobar | _filter_fiemap
    
      # Silently drop all writes and umount to simulate a crash/power failure.
      _load_flakey_table $FLAKEY_DROP_WRITES
      _unmount_flakey
    
      # Allow writes again, mount to trigger log replay and validate file
      # contents.
      _load_flakey_table $FLAKEY_ALLOW_WRITES
      _mount_flakey
    
      echo "File digest after log replay:"
      # Must match the same digest we got before the power failure.
      md5sum $SCRATCH_MNT/foobar | _filter_scratch
    
      echo "Fiemap after log replay:"
      # Must match the same extent listing we got before the power failure.
      $XFS_IO_PROG -c "fiemap -v" $SCRATCH_MNT/foobar | _filter_fiemap
    
      _unmount_flakey
    
      status=0
      exit
    
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index e6df6f1b13dd..cfa243c3ad13 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2493,6 +2493,19 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 	}
 
 	trans->block_rsv = &root->fs_info->trans_block_rsv;
+	/*
+	 * If we are using the NO_HOLES feature we might have had already an
+	 * hole that overlaps a part of the region [lockstart, lockend] and
+	 * ends at (or beyond) lockend. Since we have no file extent items to
+	 * represent holes, drop_end can be less than lockend and so we must
+	 * make sure we have an extent map representing the existing hole (the
+	 * call to __btrfs_drop_extents() might have dropped the existing extent
+	 * map representing the existing hole), otherwise the fast fsync path
+	 * will not record the existence of the hole region
+	 * [existing_hole_start, lockend].
+	 */
+	if (drop_end <= lockend)
+		drop_end = lockend + 1;
 	/*
 	 * Don't insert file hole extent item if it's for a range beyond eof
 	 * (because it's useless) or if it represents a 0 bytes range (when

commit 485290a734f14279fa9376b3d6021a2dc1f82356
Author: Qu Wenruo <quwenruo@cn.fujitsu.com>
Date:   Thu Oct 29 17:28:46 2015 +0800

    btrfs: Fix a data space underflow warning
    
    Even with quota disabled, generic/127 will trigger a kernel warning by
    underflow data space info.
    
    The bug is caused by buffered write, which in case of short copy, the
    start parameter for btrfs_delalloc_release_space() is wrong, and
    round_up/down() in btrfs_delalloc_release() extents the range to page
    aligned, decreasing one more page than expected.
    
    This patch will fix it by passing correct start.
    
    Signed-off-by: Qu Wenruo <quwenruo@cn.fujitsu.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 381be79f779a..e6df6f1b13dd 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1604,12 +1604,17 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 				BTRFS_I(inode)->outstanding_extents++;
 				spin_unlock(&BTRFS_I(inode)->lock);
 			}
-			if (only_release_metadata)
+			if (only_release_metadata) {
 				btrfs_delalloc_release_metadata(inode,
 								release_bytes);
-			else
-				btrfs_delalloc_release_space(inode, pos,
+			} else {
+				u64 __pos;
+
+				__pos = round_down(pos, root->sectorsize) +
+					(dirty_pages << PAGE_CACHE_SHIFT);
+				btrfs_delalloc_release_space(inode, __pos,
 							     release_bytes);
+			}
 		}
 
 		release_bytes = dirty_pages << PAGE_CACHE_SHIFT;

commit b06c4bf5c874a57254b197f53ddf588e7a24a2bf
Author: Filipe Manana <fdmanana@suse.com>
Date:   Fri Oct 23 07:52:54 2015 +0100

    Btrfs: fix regression running delayed references when using qgroups
    
    In the kernel 4.2 merge window we had a big changes to the implementation
    of delayed references and qgroups which made the no_quota field of delayed
    references not used anymore. More specifically the no_quota field is not
    used anymore as of:
    
      commit 0ed4792af0e8 ("btrfs: qgroup: Switch to new extent-oriented qgroup mechanism.")
    
    Leaving the no_quota field actually prevents delayed references from
    getting merged, which in turn cause the following BUG_ON(), at
    fs/btrfs/extent-tree.c, to be hit when qgroups are enabled:
    
      static int run_delayed_tree_ref(...)
      {
         (...)
         BUG_ON(node->ref_mod != 1);
         (...)
      }
    
    This happens on a scenario like the following:
    
      1) Ref1 bytenr X, action = BTRFS_ADD_DELAYED_REF, no_quota = 1, added.
    
      2) Ref2 bytenr X, action = BTRFS_DROP_DELAYED_REF, no_quota = 0, added.
         It's not merged with Ref1 because Ref1->no_quota != Ref2->no_quota.
    
      3) Ref3 bytenr X, action = BTRFS_ADD_DELAYED_REF, no_quota = 1, added.
         It's not merged with the reference at the tail of the list of refs
         for bytenr X because the reference at the tail, Ref2 is incompatible
         due to Ref2->no_quota != Ref3->no_quota.
    
      4) Ref4 bytenr X, action = BTRFS_DROP_DELAYED_REF, no_quota = 0, added.
         It's not merged with the reference at the tail of the list of refs
         for bytenr X because the reference at the tail, Ref3 is incompatible
         due to Ref3->no_quota != Ref4->no_quota.
    
      5) We run delayed references, trigger merging of delayed references,
         through __btrfs_run_delayed_refs() -> btrfs_merge_delayed_refs().
    
      6) Ref1 and Ref3 are merged as Ref1->no_quota = Ref3->no_quota and
         all other conditions are satisfied too. So Ref1 gets a ref_mod
         value of 2.
    
      7) Ref2 and Ref4 are merged as Ref2->no_quota = Ref4->no_quota and
         all other conditions are satisfied too. So Ref2 gets a ref_mod
         value of 2.
    
      8) Ref1 and Ref2 aren't merged, because they have different values
         for their no_quota field.
    
      9) Delayed reference Ref1 is picked for running (select_delayed_ref()
         always prefers references with an action == BTRFS_ADD_DELAYED_REF).
         So run_delayed_tree_ref() is called for Ref1 which triggers the
         BUG_ON because Ref1->red_mod != 1 (equals 2).
    
    So fix this by removing the no_quota field, as it's not used anymore as
    of commit 0ed4792af0e8 ("btrfs: qgroup: Switch to new extent-oriented
    qgroup mechanism.").
    
    The use of no_quota was also buggy in at least two places:
    
    1) At delayed-refs.c:btrfs_add_delayed_tree_ref() - we were setting
       no_quota to 0 instead of 1 when the following condition was true:
       is_fstree(ref_root) || !fs_info->quota_enabled
    
    2) At extent-tree.c:__btrfs_inc_extent_ref() - we were attempting to
       reset a node's no_quota when the condition "!is_fstree(root_objectid)
       || !root->fs_info->quota_enabled" was true but we did it only in
       an unused local stack variable, that is, we never reset the no_quota
       value in the node itself.
    
    This fixes the remainder of problems several people have been having when
    running delayed references, mostly while a balance is running in parallel,
    on a 4.2+ kernel.
    
    Very special thanks to Stphane Lesimple for helping debugging this issue
    and testing this fix on his multi terabyte filesystem (which took more
    than one day to balance alone, plus fsck, etc).
    
    Also, this fixes deadlock issue when using the clone ioctl with qgroups
    enabled, as reported by Elias Probst in the mailing list. The deadlock
    happens because after calling btrfs_insert_empty_item we have our path
    holding a write lock on a leaf of the fs/subvol tree and then before
    releasing the path we called check_ref() which did backref walking, when
    qgroups are enabled, and tried to read lock the same leaf. The trace for
    this case is the following:
    
      INFO: task systemd-nspawn:6095 blocked for more than 120 seconds.
      (...)
      Call Trace:
        [<ffffffff86999201>] schedule+0x74/0x83
        [<ffffffff863ef64c>] btrfs_tree_read_lock+0xc0/0xea
        [<ffffffff86137ed7>] ? wait_woken+0x74/0x74
        [<ffffffff8639f0a7>] btrfs_search_old_slot+0x51a/0x810
        [<ffffffff863a129b>] btrfs_next_old_leaf+0xdf/0x3ce
        [<ffffffff86413a00>] ? ulist_add_merge+0x1b/0x127
        [<ffffffff86411688>] __resolve_indirect_refs+0x62a/0x667
        [<ffffffff863ef546>] ? btrfs_clear_lock_blocking_rw+0x78/0xbe
        [<ffffffff864122d3>] find_parent_nodes+0xaf3/0xfc6
        [<ffffffff86412838>] __btrfs_find_all_roots+0x92/0xf0
        [<ffffffff864128f2>] btrfs_find_all_roots+0x45/0x65
        [<ffffffff8639a75b>] ? btrfs_get_tree_mod_seq+0x2b/0x88
        [<ffffffff863e852e>] check_ref+0x64/0xc4
        [<ffffffff863e9e01>] btrfs_clone+0x66e/0xb5d
        [<ffffffff863ea77f>] btrfs_ioctl_clone+0x48f/0x5bb
        [<ffffffff86048a68>] ? native_sched_clock+0x28/0x77
        [<ffffffff863ed9b0>] btrfs_ioctl+0xabc/0x25cb
      (...)
    
    The problem goes away by eleminating check_ref(), which no longer is
    needed as its purpose was to get a value for the no_quota field of
    a delayed reference (this patch removes the no_quota field as mentioned
    earlier).
    
    Reported-by: Stphane Lesimple <stephane_btrfs@lesimple.fr>
    Tested-by: Stphane Lesimple <stephane_btrfs@lesimple.fr>
    Reported-by: Elias Probst <mail@eliasprobst.eu>
    Reported-by: Peter Becker <floyd.net@gmail.com>
    Reported-by: Malte Schrder <malte@tnxip.de>
    Reported-by: Derek Dongray <derek@valedon.co.uk>
    Reported-by: Erkki Seppala <flux-btrfs@inside.org>
    Cc: stable@vger.kernel.org  # 4.2+
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Reviewed-by: Qu Wenruo <quwenruo@cn.fujitsu.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 12432057a12e..381be79f779a 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -847,7 +847,7 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 						disk_bytenr, num_bytes, 0,
 						root->root_key.objectid,
 						new_key.objectid,
-						start - extent_offset, 1);
+						start - extent_offset);
 				BUG_ON(ret); /* -ENOMEM */
 			}
 			key.offset = start;
@@ -925,7 +925,7 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 						disk_bytenr, num_bytes, 0,
 						root->root_key.objectid,
 						key.objectid, key.offset -
-						extent_offset, 0);
+						extent_offset);
 				BUG_ON(ret); /* -ENOMEM */
 				inode_sub_bytes(inode,
 						extent_end - key.offset);
@@ -1204,7 +1204,7 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 
 		ret = btrfs_inc_extent_ref(trans, root, bytenr, num_bytes, 0,
 					   root->root_key.objectid,
-					   ino, orig_offset, 1);
+					   ino, orig_offset);
 		BUG_ON(ret); /* -ENOMEM */
 
 		if (split == start) {
@@ -1231,7 +1231,7 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 		del_nr++;
 		ret = btrfs_free_extent(trans, root, bytenr, num_bytes,
 					0, root->root_key.objectid,
-					ino, orig_offset, 0);
+					ino, orig_offset);
 		BUG_ON(ret); /* -ENOMEM */
 	}
 	other_start = 0;
@@ -1248,7 +1248,7 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 		del_nr++;
 		ret = btrfs_free_extent(trans, root, bytenr, num_bytes,
 					0, root->root_key.objectid,
-					ino, orig_offset, 0);
+					ino, orig_offset);
 		BUG_ON(ret); /* -ENOMEM */
 	}
 	if (del_nr == 0) {

commit 14524a846eb52c18438e9bd5eb8cf1431fd57b44
Author: Qu Wenruo <quwenruo@cn.fujitsu.com>
Date:   Tue Sep 8 17:22:44 2015 +0800

    btrfs: fallocate: Add support to accurate qgroup reserve
    
    Now fallocate will do accurate qgroup reserve space check, unlike old
    method, which will always reserve the whole length of the range.
    
    With this patch, fallocate will:
    1) Iterate the desired range and mark in data rsv map
       Only range which is going to be allocated will be recorded in data
       rsv map and reserve the space.
       For already allocated range (normal/prealloc extent) they will be
       skipped.
       Also, record the marked range into a new list for later use.
    
    2) If 1) succeeded, do real file extent allocate.
       And at file extent allocation time, corresponding range will be
       removed from the range in data rsv map.
    
    Signed-off-by: Qu Wenruo <quwenruo@cn.fujitsu.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 47785c8a7d39..12432057a12e 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2542,17 +2542,61 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 	return err;
 }
 
+/* Helper structure to record which range is already reserved */
+struct falloc_range {
+	struct list_head list;
+	u64 start;
+	u64 len;
+};
+
+/*
+ * Helper function to add falloc range
+ *
+ * Caller should have locked the larger range of extent containing
+ * [start, len)
+ */
+static int add_falloc_range(struct list_head *head, u64 start, u64 len)
+{
+	struct falloc_range *prev = NULL;
+	struct falloc_range *range = NULL;
+
+	if (list_empty(head))
+		goto insert;
+
+	/*
+	 * As fallocate iterate by bytenr order, we only need to check
+	 * the last range.
+	 */
+	prev = list_entry(head->prev, struct falloc_range, list);
+	if (prev->start + prev->len == start) {
+		prev->len += len;
+		return 0;
+	}
+insert:
+	range = kmalloc(sizeof(*range), GFP_NOFS);
+	if (!range)
+		return -ENOMEM;
+	range->start = start;
+	range->len = len;
+	list_add_tail(&range->list, head);
+	return 0;
+}
+
 static long btrfs_fallocate(struct file *file, int mode,
 			    loff_t offset, loff_t len)
 {
 	struct inode *inode = file_inode(file);
 	struct extent_state *cached_state = NULL;
+	struct falloc_range *range;
+	struct falloc_range *tmp;
+	struct list_head reserve_list;
 	u64 cur_offset;
 	u64 last_byte;
 	u64 alloc_start;
 	u64 alloc_end;
 	u64 alloc_hint = 0;
 	u64 locked_end;
+	u64 actual_end = 0;
 	struct extent_map *em;
 	int blocksize = BTRFS_I(inode)->root->sectorsize;
 	int ret;
@@ -2568,14 +2612,12 @@ static long btrfs_fallocate(struct file *file, int mode,
 		return btrfs_punch_hole(inode, offset, len);
 
 	/*
-	 * Make sure we have enough space before we do the
-	 * allocation.
-	 * XXX: The behavior must be changed to do accurate check first
-	 * and then check data reserved space.
+	 * Only trigger disk allocation, don't trigger qgroup reserve
+	 *
+	 * For qgroup space, it will be checked later.
 	 */
-	ret = btrfs_check_data_free_space(inode, alloc_start,
-					  alloc_end - alloc_start);
-	if (ret)
+	ret = btrfs_alloc_data_chunk_ondemand(inode, alloc_end - alloc_start);
+	if (ret < 0)
 		return ret;
 
 	mutex_lock(&inode->i_mutex);
@@ -2583,6 +2625,13 @@ static long btrfs_fallocate(struct file *file, int mode,
 	if (ret)
 		goto out;
 
+	/*
+	 * TODO: Move these two operations after we have checked
+	 * accurate reserved space, or fallocate can still fail but
+	 * with page truncated or size expanded.
+	 *
+	 * But that's a minor problem and won't do much harm BTW.
+	 */
 	if (alloc_start > inode->i_size) {
 		ret = btrfs_cont_expand(inode, i_size_read(inode),
 					alloc_start);
@@ -2641,10 +2690,10 @@ static long btrfs_fallocate(struct file *file, int mode,
 		}
 	}
 
+	/* First, check if we exceed the qgroup limit */
+	INIT_LIST_HEAD(&reserve_list);
 	cur_offset = alloc_start;
 	while (1) {
-		u64 actual_end;
-
 		em = btrfs_get_extent(inode, NULL, 0, cur_offset,
 				      alloc_end - cur_offset, 0);
 		if (IS_ERR_OR_NULL(em)) {
@@ -2657,54 +2706,78 @@ static long btrfs_fallocate(struct file *file, int mode,
 		last_byte = min(extent_map_end(em), alloc_end);
 		actual_end = min_t(u64, extent_map_end(em), offset + len);
 		last_byte = ALIGN(last_byte, blocksize);
-
 		if (em->block_start == EXTENT_MAP_HOLE ||
 		    (cur_offset >= inode->i_size &&
 		     !test_bit(EXTENT_FLAG_PREALLOC, &em->flags))) {
-			ret = btrfs_prealloc_file_range(inode, mode, cur_offset,
-							last_byte - cur_offset,
-							1 << inode->i_blkbits,
-							offset + len,
-							&alloc_hint);
-		} else if (actual_end > inode->i_size &&
-			   !(mode & FALLOC_FL_KEEP_SIZE)) {
-			struct btrfs_trans_handle *trans;
-			struct btrfs_root *root = BTRFS_I(inode)->root;
-
-			/*
-			 * We didn't need to allocate any more space, but we
-			 * still extended the size of the file so we need to
-			 * update i_size and the inode item.
-			 */
-			trans = btrfs_start_transaction(root, 1);
-			if (IS_ERR(trans)) {
-				ret = PTR_ERR(trans);
-			} else {
-				inode->i_ctime = CURRENT_TIME;
-				i_size_write(inode, actual_end);
-				btrfs_ordered_update_i_size(inode, actual_end,
-							    NULL);
-				ret = btrfs_update_inode(trans, root, inode);
-				if (ret)
-					btrfs_end_transaction(trans, root);
-				else
-					ret = btrfs_end_transaction(trans,
-								    root);
+			ret = add_falloc_range(&reserve_list, cur_offset,
+					       last_byte - cur_offset);
+			if (ret < 0) {
+				free_extent_map(em);
+				break;
 			}
+			ret = btrfs_qgroup_reserve_data(inode, cur_offset,
+					last_byte - cur_offset);
+			if (ret < 0)
+				break;
 		}
 		free_extent_map(em);
-		if (ret < 0)
-			break;
-
 		cur_offset = last_byte;
-		if (cur_offset >= alloc_end) {
-			ret = 0;
+		if (cur_offset >= alloc_end)
 			break;
+	}
+
+	/*
+	 * If ret is still 0, means we're OK to fallocate.
+	 * Or just cleanup the list and exit.
+	 */
+	list_for_each_entry_safe(range, tmp, &reserve_list, list) {
+		if (!ret)
+			ret = btrfs_prealloc_file_range(inode, mode,
+					range->start,
+					range->len, 1 << inode->i_blkbits,
+					offset + len, &alloc_hint);
+		list_del(&range->list);
+		kfree(range);
+	}
+	if (ret < 0)
+		goto out_unlock;
+
+	if (actual_end > inode->i_size &&
+	    !(mode & FALLOC_FL_KEEP_SIZE)) {
+		struct btrfs_trans_handle *trans;
+		struct btrfs_root *root = BTRFS_I(inode)->root;
+
+		/*
+		 * We didn't need to allocate any more space, but we
+		 * still extended the size of the file so we need to
+		 * update i_size and the inode item.
+		 */
+		trans = btrfs_start_transaction(root, 1);
+		if (IS_ERR(trans)) {
+			ret = PTR_ERR(trans);
+		} else {
+			inode->i_ctime = CURRENT_TIME;
+			i_size_write(inode, actual_end);
+			btrfs_ordered_update_i_size(inode, actual_end, NULL);
+			ret = btrfs_update_inode(trans, root, inode);
+			if (ret)
+				btrfs_end_transaction(trans, root);
+			else
+				ret = btrfs_end_transaction(trans, root);
 		}
 	}
+out_unlock:
 	unlock_extent_cached(&BTRFS_I(inode)->io_tree, alloc_start, locked_end,
 			     &cached_state, GFP_NOFS);
 out:
+	/*
+	 * As we waited the extent range, the data_rsv_map must be empty
+	 * in the range, as written data range will be released from it.
+	 * And for prealloacted extent, it will also be released when
+	 * its metadata is written.
+	 * So this is completely used as cleanup.
+	 */
+	btrfs_qgroup_free_data(inode, alloc_start, alloc_end - alloc_start);
 	mutex_unlock(&inode->i_mutex);
 	/* Let go of our reservation. */
 	btrfs_free_reserved_data_space(inode, alloc_start,

commit 7cf5b97650f2ecefbd5afa2d58b61b289b6e3750
Author: Qu Wenruo <quwenruo@cn.fujitsu.com>
Date:   Tue Sep 8 17:25:55 2015 +0800

    btrfs: qgroup: Cleanup old inaccurate facilities
    
    Cleanup the old facilities which use old btrfs_qgroup_reserve() function
    call, replace them with the newer version, and remove the "__" prefix in
    them.
    
    Also, make btrfs_qgroup_reserve/free() functions private, as they are
    now only used inside qgroup codes.
    
    Now, the whole btrfs qgroup is swithed to use the new reserve facilities.
    
    Signed-off-by: Qu Wenruo <quwenruo@cn.fujitsu.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 29b3702fe10d..47785c8a7d39 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1529,7 +1529,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 				goto reserve_metadata;
 			}
 		}
-		ret = __btrfs_check_data_free_space(inode, pos, write_bytes);
+		ret = btrfs_check_data_free_space(inode, pos, write_bytes);
 		if (ret < 0)
 			break;
 
@@ -1537,8 +1537,8 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 		ret = btrfs_delalloc_reserve_metadata(inode, reserve_bytes);
 		if (ret) {
 			if (!only_release_metadata)
-				__btrfs_free_reserved_data_space(inode, pos,
-							         write_bytes);
+				btrfs_free_reserved_data_space(inode, pos,
+							       write_bytes);
 			else
 				btrfs_end_write_no_snapshoting(root);
 			break;
@@ -1608,7 +1608,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 				btrfs_delalloc_release_metadata(inode,
 								release_bytes);
 			else
-				__btrfs_delalloc_release_space(inode, pos,
+				btrfs_delalloc_release_space(inode, pos,
 							     release_bytes);
 		}
 
@@ -1661,8 +1661,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 			btrfs_end_write_no_snapshoting(root);
 			btrfs_delalloc_release_metadata(inode, release_bytes);
 		} else {
-			__btrfs_delalloc_release_space(inode, pos,
-						       release_bytes);
+			btrfs_delalloc_release_space(inode, pos, release_bytes);
 		}
 	}
 
@@ -2708,8 +2707,8 @@ static long btrfs_fallocate(struct file *file, int mode,
 out:
 	mutex_unlock(&inode->i_mutex);
 	/* Let go of our reservation. */
-	__btrfs_free_reserved_data_space(inode, alloc_start,
-					 alloc_end - alloc_start);
+	btrfs_free_reserved_data_space(inode, alloc_start,
+				       alloc_end - alloc_start);
 	return ret;
 }
 

commit df480633b891cf03301d87e56024a8ec3251da5b
Author: Qu Wenruo <quwenruo@cn.fujitsu.com>
Date:   Tue Sep 8 17:25:54 2015 +0800

    btrfs: extent-tree: Switch to new delalloc space reserve and release
    
    Use new __btrfs_delalloc_reserve_space() and
    __btrfs_delalloc_release_space() to reserve and release space for
    delalloc.
    
    Signed-off-by: Qu Wenruo <quwenruo@cn.fujitsu.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 61fb78f1ea5c..29b3702fe10d 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1608,7 +1608,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 				btrfs_delalloc_release_metadata(inode,
 								release_bytes);
 			else
-				btrfs_delalloc_release_space(inode,
+				__btrfs_delalloc_release_space(inode, pos,
 							     release_bytes);
 		}
 
@@ -1661,7 +1661,8 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 			btrfs_end_write_no_snapshoting(root);
 			btrfs_delalloc_release_metadata(inode, release_bytes);
 		} else {
-			btrfs_delalloc_release_space(inode, release_bytes);
+			__btrfs_delalloc_release_space(inode, pos,
+						       release_bytes);
 		}
 	}
 

commit d9d8b2a51a404c2d45b9dc4c755f62cb3ddb7c79
Author: Qu Wenruo <quwenruo@cn.fujitsu.com>
Date:   Tue Sep 8 17:22:43 2015 +0800

    btrfs: extent-tree: Switch to new check_data_free_space and free_reserved_data_space
    
    Use new reserve/free for buffered write and inode cache.
    
    For buffered write case, as nodatacow write won't increase quota account,
    so unlike old behavior which does reserve before check nocow, now we
    check nocow first and then only reserve data if we can't do nocow write.
    
    Signed-off-by: Qu Wenruo <quwenruo@cn.fujitsu.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 7031e9631519..61fb78f1ea5c 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1507,12 +1507,17 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 		}
 
 		reserve_bytes = num_pages << PAGE_CACHE_SHIFT;
-		ret = btrfs_check_data_free_space(inode, reserve_bytes, write_bytes);
-		if (ret == -ENOSPC &&
-		    (BTRFS_I(inode)->flags & (BTRFS_INODE_NODATACOW |
-					      BTRFS_INODE_PREALLOC))) {
+
+		if (BTRFS_I(inode)->flags & (BTRFS_INODE_NODATACOW |
+					     BTRFS_INODE_PREALLOC)) {
 			ret = check_can_nocow(inode, pos, &write_bytes);
+			if (ret < 0)
+				break;
 			if (ret > 0) {
+				/*
+				 * For nodata cow case, no need to reserve
+				 * data space.
+				 */
 				only_release_metadata = true;
 				/*
 				 * our prealloc extent may be smaller than
@@ -1521,20 +1526,19 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 				num_pages = DIV_ROUND_UP(write_bytes + offset,
 							 PAGE_CACHE_SIZE);
 				reserve_bytes = num_pages << PAGE_CACHE_SHIFT;
-				ret = 0;
-			} else {
-				ret = -ENOSPC;
+				goto reserve_metadata;
 			}
 		}
-
-		if (ret)
+		ret = __btrfs_check_data_free_space(inode, pos, write_bytes);
+		if (ret < 0)
 			break;
 
+reserve_metadata:
 		ret = btrfs_delalloc_reserve_metadata(inode, reserve_bytes);
 		if (ret) {
 			if (!only_release_metadata)
-				btrfs_free_reserved_data_space(inode,
-							       reserve_bytes);
+				__btrfs_free_reserved_data_space(inode, pos,
+							         write_bytes);
 			else
 				btrfs_end_write_no_snapshoting(root);
 			break;
@@ -2566,8 +2570,11 @@ static long btrfs_fallocate(struct file *file, int mode,
 	/*
 	 * Make sure we have enough space before we do the
 	 * allocation.
+	 * XXX: The behavior must be changed to do accurate check first
+	 * and then check data reserved space.
 	 */
-	ret = btrfs_check_data_free_space(inode, alloc_end - alloc_start, alloc_end - alloc_start);
+	ret = btrfs_check_data_free_space(inode, alloc_start,
+					  alloc_end - alloc_start);
 	if (ret)
 		return ret;
 
@@ -2700,7 +2707,8 @@ static long btrfs_fallocate(struct file *file, int mode,
 out:
 	mutex_unlock(&inode->i_mutex);
 	/* Let go of our reservation. */
-	btrfs_free_reserved_data_space(inode, alloc_end - alloc_start);
+	__btrfs_free_reserved_data_space(inode, alloc_start,
+					 alloc_end - alloc_start);
 	return ret;
 }
 

commit 6e4d6fa12ceb22f87d49e043e5a7636abdac970f
Author: Alexandru Moise <00moses.alexander00@gmail.com>
Date:   Tue Sep 22 21:00:07 2015 +0000

    btrfs: declare rsv_count as unsigned int instead of int
    
    rsv_count ultimately gets passed to start_transaction() which
    now takes an unsigned int as its num_items parameter.
    The value of rsv_count should always be positive so declare it
    as being unsigned.
    
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: Alexandru Moise <00moses.alexander00@gmail.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index b6695c43859d..7031e9631519 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2263,7 +2263,7 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 	u64 drop_end;
 	int ret = 0;
 	int err = 0;
-	int rsv_count;
+	unsigned int rsv_count;
 	bool same_page;
 	bool no_holes = btrfs_fs_incompat(root->fs_info, NO_HOLES);
 	u64 ino_size;

commit bb78915203c0c1b07096e50d12f6e8fe54666f4f
Author: Shan Hai <shan.hai@windriver.com>
Date:   Mon Sep 21 11:40:57 2015 +0800

    btrfs/file.c: remove an unsed varialbe first_index
    
    The commit b37392ea86761 ("Btrfs: cleanup unnecessary parameter
    and variant of prepare_pages()") makes it redundant.
    
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: Shan Hai <haishan.bai@hotmail.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index b823fac91c92..b6695c43859d 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1469,7 +1469,6 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 	u64 release_bytes = 0;
 	u64 lockstart;
 	u64 lockend;
-	unsigned long first_index;
 	size_t num_written = 0;
 	int nrptrs;
 	int ret = 0;
@@ -1485,8 +1484,6 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 	if (!pages)
 		return -ENOMEM;
 
-	first_index = pos >> PAGE_CACHE_SHIFT;
-
 	while (iov_iter_count(i) > 0) {
 		size_t offset = pos & (PAGE_CACHE_SIZE - 1);
 		size_t write_bytes = min(iov_iter_count(i),

commit 0f6925fa2907df58496cabc33fa4677c635e2223
Author: Qu Wenruo <quwenruo@cn.fujitsu.com>
Date:   Wed Oct 14 15:26:13 2015 +0800

    btrfs: Avoid truncate tailing page if fallocate range doesn't exceed inode size
    
    Current code will always truncate tailing page if its alloc_start is
    smaller than inode size.
    
    For example, the file extent layout is like:
    0       4K      8K      16K     32K
    |<-----Extent A---------------->|
    |<--Inode size: 18K---------->|
    
    But if calling fallocate even for range [0,4K), it will cause btrfs to
    re-truncate the range [16,32K), causing COW and a new extent.
    
    0       4K      8K      16K     32K
    |///////|       <- Fallocate call range
    |<-----Extent A-------->|<--B-->|
    
    The cause is quite easy, just a careless btrfs_truncate_inode() in a
    else branch without extra judgment.
    Fix it by add judgment on whether the fallocate range is beyond isize.
    
    Signed-off-by: Qu Wenruo <quwenruo@cn.fujitsu.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index b823fac91c92..8c6f247ba81d 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2584,7 +2584,7 @@ static long btrfs_fallocate(struct file *file, int mode,
 					alloc_start);
 		if (ret)
 			goto out;
-	} else {
+	} else if (offset + len > inode->i_size) {
 		/*
 		 * If we are fallocating from the end of the file onward we
 		 * need to zero out the end of the page if i_size lands in the

commit 1dc51b8288007753ad7cd7d08bb8fa930fc8bb10
Merge: 9b284cbdb5de 0f1db7dee200
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jul 4 19:36:06 2015 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs
    
    Pull more vfs updates from Al Viro:
     "Assorted VFS fixes and related cleanups (IMO the most interesting in
      that part are f_path-related things and Eric's descriptor-related
      stuff).  UFS regression fixes (it got broken last cycle).  9P fixes.
      fs-cache series, DAX patches, Jan's file_remove_suid() work"
    
    [ I'd say this is much more than "fixes and related cleanups".  The
      file_table locking rule change by Eric Dumazet is a rather big and
      fundamental update even if the patch isn't huge.   - Linus ]
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs: (49 commits)
      9p: cope with bogus responses from server in p9_client_{read,write}
      p9_client_write(): avoid double p9_free_req()
      9p: forgetting to cancel request on interrupted zero-copy RPC
      dax: bdev_direct_access() may sleep
      block: Add support for DAX reads/writes to block devices
      dax: Use copy_from_iter_nocache
      dax: Add block size note to documentation
      fs/file.c: __fget() and dup2() atomicity rules
      fs/file.c: don't acquire files->file_lock in fd_install()
      fs:super:get_anon_bdev: fix race condition could cause dev exceed its upper limitation
      vfs: avoid creation of inode number 0 in get_next_ino
      namei: make set_root_rcu() return void
      make simple_positive() public
      ufs: use dir_pages instead of ufs_dir_pages()
      pagemap.h: move dir_pages() over there
      remove the pointless include of lglock.h
      fs: cleanup slight list_entry abuse
      xfs: Correctly lock inode when removing suid and file capabilities
      fs: Call security_ops->inode_killpriv on truncate
      fs: Provide function telling whether file_remove_privs() will do anything
      ...

commit 5fa8e0a1c6a762857ae67d1628c58b9a02362003
Author: Jan Kara <jack@suse.cz>
Date:   Thu May 21 16:05:53 2015 +0200

    fs: Rename file_remove_suid() to file_remove_privs()
    
    file_remove_suid() is a misnomer since it removes also file capabilities
    stored in xattrs and sets S_NOSEC flag. Also should_remove_suid() tells
    something else than whether file_remove_suid() call is necessary which
    leads to bugs.
    
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index b072e17479aa..86f97282779a 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1748,7 +1748,7 @@ static ssize_t btrfs_file_write_iter(struct kiocb *iocb,
 	}
 
 	current->backing_dev_info = inode_to_bdi(inode);
-	err = file_remove_suid(file);
+	err = file_remove_privs(file);
 	if (err) {
 		mutex_unlock(&inode->i_mutex);
 		goto out;

commit b659ef027792219b590d67a2baf1643a93727d29
Author: Filipe Manana <fdmanana@suse.com>
Date:   Tue Mar 31 14:16:52 2015 +0100

    Btrfs: avoid syncing log in the fast fsync path when not necessary
    
    Commit 3a8b36f37806 ("Btrfs: fix data loss in the fast fsync path") added
    a performance regression for that causes an unnecessary sync of the log
    trees (fs/subvol and root log trees) when 2 consecutive fsyncs are done
    against a file, without no writes or any metadata updates to the inode in
    between them and if a transaction is committed before the second fsync is
    called.
    
    Huang Ying reported this to lkml (https://lkml.org/lkml/2015/3/18/99)
    after a test sysbench test that measured a -62% decrease of file io
    requests per second for that tests' workload.
    
    The test is:
    
      echo performance > /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor
      echo performance > /sys/devices/system/cpu/cpu1/cpufreq/scaling_governor
      echo performance > /sys/devices/system/cpu/cpu2/cpufreq/scaling_governor
      echo performance > /sys/devices/system/cpu/cpu3/cpufreq/scaling_governor
      mkfs -t btrfs /dev/sda2
      mount -t btrfs /dev/sda2 /fs/sda2
      cd /fs/sda2
      for ((i = 0; i < 1024; i++)); do fallocate -l 67108864 testfile.$i; done
      sysbench --test=fileio --max-requests=0 --num-threads=4 --max-time=600 \
        --file-test-mode=rndwr --file-total-size=68719476736 --file-io-mode=sync \
        --file-num=1024 run
    
    A test on kvm guest, running a debug kernel gave me the following results:
    
    Without 3a8b36f378060d:             16.01 reqs/sec
    With 3a8b36f378060d:                 3.39 reqs/sec
    With 3a8b36f378060d and this patch: 16.04 reqs/sec
    
    Reported-by: Huang Ying <ying.huang@intel.com>
    Tested-by: Huang, Ying <ying.huang@intel.com>
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index b072e17479aa..795d754327a7 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1868,6 +1868,7 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 	struct btrfs_log_ctx ctx;
 	int ret = 0;
 	bool full_sync = 0;
+	const u64 len = end - start + 1;
 
 	trace_btrfs_sync_file(file, datasync);
 
@@ -1896,7 +1897,7 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 		 * all extents are persisted and the respective file extent
 		 * items are in the fs/subvol btree.
 		 */
-		ret = btrfs_wait_ordered_range(inode, start, end - start + 1);
+		ret = btrfs_wait_ordered_range(inode, start, len);
 	} else {
 		/*
 		 * Start any new ordered operations before starting to log the
@@ -1968,8 +1969,10 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 	 */
 	smp_mb();
 	if (btrfs_inode_in_log(inode, root->fs_info->generation) ||
-	    (full_sync && BTRFS_I(inode)->last_trans <=
-	     root->fs_info->last_trans_committed)) {
+	    (BTRFS_I(inode)->last_trans <=
+	     root->fs_info->last_trans_committed &&
+	     (full_sync ||
+	      !btrfs_have_ordered_extents_in_range(inode, start, len)))) {
 		/*
 		 * We'v had everything committed since the last time we were
 		 * modified so clear this flag in case it was set for whatever

commit 9ec3a646fe09970f801ab15e0f1694060b9f19af
Merge: c8b3fd0ce313 3cab989afd8d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Apr 26 15:48:49 2015 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs
    
    Pull fourth vfs update from Al Viro:
     "d_inode() annotations from David Howells (sat in for-next since before
      the beginning of merge window) + four assorted fixes"
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs:
      RCU pathwalk breakage when running into a symlink overmounting something
      fix I_DIO_WAKEUP definition
      direct-io: only inc/dec inode->i_dio_count for file systems
      fs/9p: fix readdir()
      VFS: assorted d_backing_inode() annotations
      VFS: fs/inode.c helpers: d_inode() annotations
      VFS: fs/cachefiles: d_backing_inode() annotations
      VFS: fs library helpers: d_inode() annotations
      VFS: assorted weird filesystems: d_inode() annotations
      VFS: normal filesystems (and lustre): d_inode() annotations
      VFS: security/: d_inode() annotations
      VFS: security/: d_backing_inode() annotations
      VFS: net/: d_inode() annotations
      VFS: net/unix: d_backing_inode() annotations
      VFS: kernel/: d_inode() annotations
      VFS: audit: d_backing_inode() annotations
      VFS: Fix up some ->d_inode accesses in the chelsio driver
      VFS: Cachefiles should perform fs modifications on the top layer only
      VFS: AF_UNIX sockets should call mknod on the top layer only

commit ba0e4ae88f0f71b42ad8734e0c371d321554f13b
Merge: 1aef882f023e e082f56313f3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Apr 24 07:40:02 2015 -0700

    Merge branch 'for-linus-4.1' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs
    
    Pull btrfs updates from Chris Mason:
     "I've been running these through a longer set of load tests because my
      commits change the free space cache writeout.  It fixes commit stalls
      on large filesystems (~20T space used and up) that we have been
      triggering here.  We were seeing new writers blocked for 10 seconds or
      more during commits, which is far from good.
    
      Josef and I fixed up ENOSPC aborts when deleting huge files (3T or
      more), that are triggered because our metadata reservations were not
      properly accounting for crcs and were not replenishing during the
      truncate.
    
      Also in this series, a number of qgroup fixes from Fujitsu and Dave
      Sterba collected most of the pending cleanups from the list"
    
    * 'for-linus-4.1' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs: (93 commits)
      btrfs: quota: Update quota tree after qgroup relationship change.
      btrfs: quota: Automatically update related qgroups or mark INCONSISTENT flags when assigning/deleting a qgroup relations.
      btrfs: qgroup: clear STATUS_FLAG_ON in disabling quota.
      btrfs: Update btrfs qgroup status item when rescan is done.
      btrfs: qgroup: Fix dead judgement on qgroup_rescan_leaf() return value.
      btrfs: Don't allow subvolid >= (1 << BTRFS_QGROUP_LEVEL_SHIFT) to be created
      btrfs: Check qgroup level in kernel qgroup assign.
      btrfs: qgroup: allow to remove qgroup which has parent but no child.
      btrfs: qgroup: return EINVAL if level of parent is not higher than child's.
      btrfs: qgroup: do a reservation in a higher level.
      Btrfs: qgroup, Account data space in more proper timings.
      Btrfs: qgroup: Introduce a may_use to account space_info->bytes_may_use.
      Btrfs: qgroup: free reserved in exceeding quota.
      Btrfs: qgroup: cleanup, remove an unsued parameter in btrfs_create_qgroup().
      btrfs: qgroup: fix limit args override whole limit struct
      btrfs: qgroup: update limit info in function btrfs_run_qgroups().
      btrfs: qgroup: consolidate the parameter of fucntion update_qgroup_limit_item().
      btrfs: qgroup: update qgroup in memory at the same time when we update it in btree.
      btrfs: qgroup: inherit limit info from srcgroup in creating snapshot.
      btrfs: Support busy loop of write and delete
      ...

commit 2b0143b5c986be1ce8408b3aadc4709e0a94429d
Author: David Howells <dhowells@redhat.com>
Date:   Tue Mar 17 22:25:59 2015 +0000

    VFS: normal filesystems (and lustre): d_inode() annotations
    
    that's the bulk of filesystem drivers dealing with inodes of their own
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index faa7d390841b..2e57064c3cdf 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1864,7 +1864,7 @@ static int start_ordered_ops(struct inode *inode, loff_t start, loff_t end)
 int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 {
 	struct dentry *dentry = file->f_path.dentry;
-	struct inode *inode = dentry->d_inode;
+	struct inode *inode = d_inode(dentry);
 	struct btrfs_root *root = BTRFS_I(inode)->root;
 	struct btrfs_trans_handle *trans;
 	struct btrfs_log_ctx ctx;

commit e2d1f92399afb6ec518b68867ed10db2585b283a
Author: Dongsheng Yang <yangds.fnst@cn.fujitsu.com>
Date:   Fri Feb 6 10:26:52 2015 -0500

    btrfs: qgroup: do a reservation in a higher level.
    
    There are two problems in qgroup:
    
    a). The PAGE_CACHE is 4K, even when we are writing a data of 1K,
    qgroup will reserve a 4K size. It will cause the last 3K in a qgroup
    is not available to user.
    
    b). When user is writing a inline data, qgroup will not reserve it,
    it means this is a window we can exceed the limit of a qgroup.
    
    The main idea of this patch is reserving the data size of write_bytes
    rather than the reserve_bytes. It means qgroup will not care about
    the data size btrfs will reserve for user, but only care about the
    data size user is going to write. Then reserve it when user want to
    write and release it in transaction committed.
    
    In this way, qgroup can be released from the complex procedure in
    btrfs and only do the reserve when user want to write and account
    when the data is written in commit_transaction().
    
    Signed-off-by: Dongsheng Yang <yangds.fnst@cn.fujitsu.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index faef1d64394d..23b6e03f8465 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1510,7 +1510,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 		}
 
 		reserve_bytes = num_pages << PAGE_CACHE_SHIFT;
-		ret = btrfs_check_data_free_space(inode, reserve_bytes);
+		ret = btrfs_check_data_free_space(inode, reserve_bytes, write_bytes);
 		if (ret == -ENOSPC &&
 		    (BTRFS_I(inode)->flags & (BTRFS_INODE_NODATACOW |
 					      BTRFS_INODE_PREALLOC))) {
@@ -2573,7 +2573,7 @@ static long btrfs_fallocate(struct file *file, int mode,
 	 * Make sure we have enough space before we do the
 	 * allocation.
 	 */
-	ret = btrfs_check_data_free_space(inode, alloc_end - alloc_start);
+	ret = btrfs_check_data_free_space(inode, alloc_end - alloc_start, alloc_end - alloc_start);
 	if (ret)
 		return ret;
 

commit 237c0e9f1fbfdca7287f3539f1fa73e5063156b5
Author: Dongsheng Yang <yangds.fnst@cn.fujitsu.com>
Date:   Mon Dec 29 06:23:05 2014 -0500

    Btrfs: qgroup, Account data space in more proper timings.
    
    Currenly, in data writing, ->reserved is accounted in
    fill_delalloc(), but ->may_use is released in clear_bit_hook()
    which is called by btrfs_finish_ordered_io(). That's too late,
    that said, between fill_delalloc() and btrfs_finish_ordered_io(),
    the data is doublely accounted by qgroup. It will cause some
    unexpected -EDQUOT.
    
    Example:
            # btrfs quota enable /root/btrfs-auto-test/
            # btrfs subvolume create /root/btrfs-auto-test//sub
            Create subvolume '/root/btrfs-auto-test/sub'
            # btrfs qgroup limit 1G /root/btrfs-auto-test//sub
            dd if=/dev/zero of=/root/btrfs-auto-test//sub/file bs=1024 count=1500000
            dd: error writing '/root/btrfs-auto-test//sub/file': Disk quota exceeded
            681353+0 records in
            681352+0 records out
            697704448 bytes (698 MB) copied, 8.15563 s, 85.5 MB/s
    It's (698 MB) when we got an -EDQUOT, but we limit it by 1G.
    
    This patch move the btrfs_qgroup_reserve/free() for data from
    btrfs_delalloc_reserve/release_metadata() to btrfs_check_data_free_space()
    and btrfs_free_reserved_data_space(). Then the accounter in qgroup
    will be updated at the same time with the accounter in space_info updated.
    In this way, the unexpected -EDQUOT will be killed.
    
    Reported-by: Satoru Takeuchi <takeuchi_satoru@jp.fujitsu.com>
    Signed-off-by: Dongsheng Yang <yangds.fnst@cn.fujitsu.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index fd105c172c8b..faef1d64394d 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2549,7 +2549,6 @@ static long btrfs_fallocate(struct file *file, int mode,
 {
 	struct inode *inode = file_inode(file);
 	struct extent_state *cached_state = NULL;
-	struct btrfs_root *root = BTRFS_I(inode)->root;
 	u64 cur_offset;
 	u64 last_byte;
 	u64 alloc_start;
@@ -2577,11 +2576,6 @@ static long btrfs_fallocate(struct file *file, int mode,
 	ret = btrfs_check_data_free_space(inode, alloc_end - alloc_start);
 	if (ret)
 		return ret;
-	if (root->fs_info->quota_enabled) {
-		ret = btrfs_qgroup_reserve(root, alloc_end - alloc_start);
-		if (ret)
-			goto out_reserve_fail;
-	}
 
 	mutex_lock(&inode->i_mutex);
 	ret = inode_newsize_ok(inode, alloc_end);
@@ -2674,6 +2668,7 @@ static long btrfs_fallocate(struct file *file, int mode,
 		} else if (actual_end > inode->i_size &&
 			   !(mode & FALLOC_FL_KEEP_SIZE)) {
 			struct btrfs_trans_handle *trans;
+			struct btrfs_root *root = BTRFS_I(inode)->root;
 
 			/*
 			 * We didn't need to allocate any more space, but we
@@ -2710,9 +2705,6 @@ static long btrfs_fallocate(struct file *file, int mode,
 			     &cached_state, GFP_NOFS);
 out:
 	mutex_unlock(&inode->i_mutex);
-	if (root->fs_info->quota_enabled)
-		btrfs_qgroup_free(root, alloc_end - alloc_start);
-out_reserve_fail:
 	/* Let go of our reservation. */
 	btrfs_free_reserved_data_space(inode, alloc_end - alloc_start);
 	return ret;

commit 2ba48ce513c4e545318d22b138861d5876edf906
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Thu Apr 9 13:52:01 2015 -0400

    mirror O_APPEND and O_DIRECT into iocb->ki_flags
    
    ... avoiding write_iter/fcntl races.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index c64d11c41eeb..faa7d390841b 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1794,7 +1794,7 @@ static ssize_t btrfs_file_write_iter(struct kiocb *iocb,
 	if (sync)
 		atomic_inc(&BTRFS_I(inode)->sync_writers);
 
-	if (file->f_flags & O_DIRECT) {
+	if (iocb->ki_flags & IOCB_DIRECT) {
 		num_written = __btrfs_direct_write(iocb, from, pos);
 	} else {
 		num_written = __btrfs_buffered_write(file, from, pos);

commit 3309dd04cbcd2cdad168485af5cf3576b5051e49
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Thu Apr 9 12:55:47 2015 -0400

    switch generic_write_checks() to iocb and iter
    
    ... returning -E... upon error and amount of data left in iter after
    (possible) truncation upon success.  Note, that normal case gives
    a non-zero (positive) return value, so any tests for != 0 _must_ be
    updated.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    
    Conflicts:
            fs/ext4/file.c

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 691a84a81e09..c64d11c41eeb 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1739,27 +1739,19 @@ static ssize_t btrfs_file_write_iter(struct kiocb *iocb,
 	u64 start_pos;
 	u64 end_pos;
 	ssize_t num_written = 0;
-	ssize_t err = 0;
-	size_t count = iov_iter_count(from);
 	bool sync = (file->f_flags & O_DSYNC) || IS_SYNC(file->f_mapping->host);
-	loff_t pos = iocb->ki_pos;
+	ssize_t err;
+	loff_t pos;
+	size_t count;
 
 	mutex_lock(&inode->i_mutex);
-
-	current->backing_dev_info = inode_to_bdi(inode);
-	err = generic_write_checks(file, &pos, &count);
-	if (err) {
+	err = generic_write_checks(iocb, from);
+	if (err <= 0) {
 		mutex_unlock(&inode->i_mutex);
-		goto out;
-	}
-
-	if (count == 0) {
-		mutex_unlock(&inode->i_mutex);
-		goto out;
+		return err;
 	}
 
-	iov_iter_truncate(from, count);
-
+	current->backing_dev_info = inode_to_bdi(inode);
 	err = file_remove_suid(file);
 	if (err) {
 		mutex_unlock(&inode->i_mutex);
@@ -1786,6 +1778,8 @@ static ssize_t btrfs_file_write_iter(struct kiocb *iocb,
 	 */
 	update_time_for_write(inode);
 
+	pos = iocb->ki_pos;
+	count = iov_iter_count(from);
 	start_pos = round_down(pos, root->sectorsize);
 	if (start_pos > i_size_read(inode)) {
 		/* Expand hole size to cover write data, preventing empty gap */

commit 0fa6b005afdb3152ce85df963302e59b61115f9b
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sat Apr 4 04:05:48 2015 -0400

    generic_write_checks(): drop isblk argument
    
    all remaining callers are passing 0; some just obscure that fact.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index cdc801c85105..691a84a81e09 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1747,7 +1747,7 @@ static ssize_t btrfs_file_write_iter(struct kiocb *iocb,
 	mutex_lock(&inode->i_mutex);
 
 	current->backing_dev_info = inode_to_bdi(inode);
-	err = generic_write_checks(file, &pos, &count, S_ISBLK(inode->i_mode));
+	err = generic_write_checks(file, &pos, &count);
 	if (err) {
 		mutex_unlock(&inode->i_mutex);
 		goto out;

commit 5d5d568975307877e9195f5305f4240e506a2807
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Fri Apr 3 15:41:18 2015 -0400

    make new_sync_{read,write}() static
    
    All places outside of core VFS that checked ->read and ->write for being NULL or
    called the methods directly are gone now, so NULL {read,write} with non-NULL
    {read,write}_iter will do the right thing in all cases.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index aee18f84e315..cdc801c85105 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2806,8 +2806,6 @@ static loff_t btrfs_file_llseek(struct file *file, loff_t offset, int whence)
 
 const struct file_operations btrfs_file_operations = {
 	.llseek		= btrfs_file_llseek,
-	.read		= new_sync_read,
-	.write		= new_sync_write,
 	.read_iter      = generic_file_read_iter,
 	.splice_read	= generic_file_splice_read,
 	.write_iter	= btrfs_file_write_iter,

commit c0fec3a98bd6c4d992f191ee1aa0b3599213f3d4
Merge: c1b8940b42bb e2e40f2c1ed4
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sat Apr 11 22:24:41 2015 -0400

    Merge branch 'iocb' into for-next

commit 2f2ff0ee5e4303e727cfd7abd4133d1a8ee68394
Author: Filipe Manana <fdmanana@suse.com>
Date:   Fri Mar 20 17:19:46 2015 +0000

    Btrfs: fix metadata inconsistencies after directory fsync
    
    We can get into inconsistency between inodes and directory entries
    after fsyncing a directory. The issue is that while a directory gets
    the new dentries persisted in the fsync log and replayed at mount time,
    the link count of the inode that directory entries point to doesn't
    get updated, staying with an incorrect link count (smaller then the
    correct value). This later leads to stale file handle errors when
    accessing (including attempt to delete) some of the links if all the
    other ones are removed, which also implies impossibility to delete the
    parent directories, since the dentries can not be removed.
    
    Another issue is that (unlike ext3/4, xfs, f2fs, reiserfs, nilfs2),
    when fsyncing a directory, new files aren't logged (their metadata and
    dentries) nor any child directories. So this patch fixes this issue too,
    since it has the same resolution as the incorrect inode link count issue
    mentioned before.
    
    This is very easy to reproduce, and the following excerpt from my test
    case for xfstests shows how:
    
      _scratch_mkfs >> $seqres.full 2>&1
      _init_flakey
      _mount_flakey
    
      # Create our main test file and directory.
      $XFS_IO_PROG -f -c "pwrite -S 0xaa 0 8K" $SCRATCH_MNT/foo | _filter_xfs_io
      mkdir $SCRATCH_MNT/mydir
    
      # Make sure all metadata and data are durably persisted.
      sync
    
      # Add a hard link to 'foo' inside our test directory and fsync only the
      # directory. The btrfs fsync implementation had a bug that caused the new
      # directory entry to be visible after the fsync log replay but, the inode
      # of our file remained with a link count of 1.
      ln $SCRATCH_MNT/foo $SCRATCH_MNT/mydir/foo_2
    
      # Add a few more links and new files.
      # This is just to verify nothing breaks or gives incorrect results after the
      # fsync log is replayed.
      ln $SCRATCH_MNT/foo $SCRATCH_MNT/mydir/foo_3
      $XFS_IO_PROG -f -c "pwrite -S 0xff 0 64K" $SCRATCH_MNT/hello | _filter_xfs_io
      ln $SCRATCH_MNT/hello $SCRATCH_MNT/mydir/hello_2
    
      # Add some subdirectories and new files and links to them. This is to verify
      # that after fsyncing our top level directory 'mydir', all the subdirectories
      # and their files/links are registered in the fsync log and exist after the
      # fsync log is replayed.
      mkdir -p $SCRATCH_MNT/mydir/x/y/z
      ln $SCRATCH_MNT/foo $SCRATCH_MNT/mydir/x/y/foo_y_link
      ln $SCRATCH_MNT/foo $SCRATCH_MNT/mydir/x/y/z/foo_z_link
      touch $SCRATCH_MNT/mydir/x/y/z/qwerty
    
      # Now fsync only our top directory.
      $XFS_IO_PROG -c "fsync" $SCRATCH_MNT/mydir
    
      # And fsync now our new file named 'hello', just to verify later that it has
      # the expected content and that the previous fsync on the directory 'mydir' had
      # no bad influence on this fsync.
      $XFS_IO_PROG -c "fsync" $SCRATCH_MNT/hello
    
      # Simulate a crash/power loss.
      _load_flakey_table $FLAKEY_DROP_WRITES
      _unmount_flakey
    
      _load_flakey_table $FLAKEY_ALLOW_WRITES
      _mount_flakey
    
      # Verify the content of our file 'foo' remains the same as before, 8192 bytes,
      # all with the value 0xaa.
      echo "File 'foo' content after log replay:"
      od -t x1 $SCRATCH_MNT/foo
    
      # Remove the first name of our inode. Because of the directory fsync bug, the
      # inode's link count was 1 instead of 5, so removing the 'foo' name ended up
      # deleting the inode and the other names became stale directory entries (still
      # visible to applications). Attempting to remove or access the remaining
      # dentries pointing to that inode resulted in stale file handle errors and
      # made it impossible to remove the parent directories since it was impossible
      # for them to become empty.
      echo "file 'foo' link count after log replay: $(stat -c %h $SCRATCH_MNT/foo)"
      rm -f $SCRATCH_MNT/foo
    
      # Now verify that all files, links and directories created before fsyncing our
      # directory exist after the fsync log was replayed.
      [ -f $SCRATCH_MNT/mydir/foo_2 ] || echo "Link mydir/foo_2 is missing"
      [ -f $SCRATCH_MNT/mydir/foo_3 ] || echo "Link mydir/foo_3 is missing"
      [ -f $SCRATCH_MNT/hello ] || echo "File hello is missing"
      [ -f $SCRATCH_MNT/mydir/hello_2 ] || echo "Link mydir/hello_2 is missing"
      [ -f $SCRATCH_MNT/mydir/x/y/foo_y_link ] || \
          echo "Link mydir/x/y/foo_y_link is missing"
      [ -f $SCRATCH_MNT/mydir/x/y/z/foo_z_link ] || \
          echo "Link mydir/x/y/z/foo_z_link is missing"
      [ -f $SCRATCH_MNT/mydir/x/y/z/qwerty ] || \
          echo "File mydir/x/y/z/qwerty is missing"
    
      # We expect our file here to have a size of 64Kb and all the bytes having the
      # value 0xff.
      echo "file 'hello' content after log replay:"
      od -t x1 $SCRATCH_MNT/hello
    
      # Now remove all files/links, under our test directory 'mydir', and verify we
      # can remove all the directories.
      rm -f $SCRATCH_MNT/mydir/x/y/z/*
      rmdir $SCRATCH_MNT/mydir/x/y/z
      rm -f $SCRATCH_MNT/mydir/x/y/*
      rmdir $SCRATCH_MNT/mydir/x/y
      rmdir $SCRATCH_MNT/mydir/x
      rm -f $SCRATCH_MNT/mydir/*
      rmdir $SCRATCH_MNT/mydir
    
      # An fsck, run by the fstests framework everytime a test finishes, also detected
      # the inconsistency and printed the following error message:
      #
      # root 5 inode 257 errors 2001, no inode item, link count wrong
      #    unresolved ref dir 258 index 2 namelen 5 name foo_2 filetype 1 errors 4, no inode ref
      #    unresolved ref dir 258 index 3 namelen 5 name foo_3 filetype 1 errors 4, no inode ref
    
      status=0
      exit
    
    The expected golden output for the test is:
    
      wrote 8192/8192 bytes at offset 0
      XXX Bytes, X ops; XX:XX:XX.X (XXX YYY/sec and XXX ops/sec)
      wrote 65536/65536 bytes at offset 0
      XXX Bytes, X ops; XX:XX:XX.X (XXX YYY/sec and XXX ops/sec)
      File 'foo' content after log replay:
      0000000 aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa
      *
      0020000
      file 'foo' link count after log replay: 5
      file 'hello' content after log replay:
      0000000 ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff
      *
      0200000
    
    Which is the output after this patch and when running the test against
    ext3/4, xfs, f2fs, reiserfs or nilfs2. Without this patch, the test's
    output is:
    
      wrote 8192/8192 bytes at offset 0
      XXX Bytes, X ops; XX:XX:XX.X (XXX YYY/sec and XXX ops/sec)
      wrote 65536/65536 bytes at offset 0
      XXX Bytes, X ops; XX:XX:XX.X (XXX YYY/sec and XXX ops/sec)
      File 'foo' content after log replay:
      0000000 aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa
      *
      0020000
      file 'foo' link count after log replay: 1
      Link mydir/foo_2 is missing
      Link mydir/foo_3 is missing
      Link mydir/x/y/foo_y_link is missing
      Link mydir/x/y/z/foo_z_link is missing
      File mydir/x/y/z/qwerty is missing
      file 'hello' content after log replay:
      0000000 ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff
      *
      0200000
      rmdir: failed to remove '/home/fdmanana/btrfs-tests/scratch_1/mydir/x/y/z': No such file or directory
      rmdir: failed to remove '/home/fdmanana/btrfs-tests/scratch_1/mydir/x/y': No such file or directory
      rmdir: failed to remove '/home/fdmanana/btrfs-tests/scratch_1/mydir/x': No such file or directory
      rm: cannot remove '/home/fdmanana/btrfs-tests/scratch_1/mydir/foo_2': Stale file handle
      rm: cannot remove '/home/fdmanana/btrfs-tests/scratch_1/mydir/foo_3': Stale file handle
      rmdir: failed to remove '/home/fdmanana/btrfs-tests/scratch_1/mydir': Directory not empty
    
    Fsck, without this fix, also complains about the wrong link count:
    
      root 5 inode 257 errors 2001, no inode item, link count wrong
          unresolved ref dir 258 index 2 namelen 5 name foo_2 filetype 1 errors 4, no inode ref
          unresolved ref dir 258 index 3 namelen 5 name foo_3 filetype 1 errors 4, no inode ref
    
    So fix this by logging the inodes that the dentries point to when
    fsyncing a directory.
    
    A test case for xfstests follows.
    
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 150db5e50c2d..fd105c172c8b 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1811,7 +1811,9 @@ static ssize_t btrfs_file_write_iter(struct kiocb *iocb,
 	 * otherwise subsequent syncs to a file that's been synced in this
 	 * transaction will appear to have already occured.
 	 */
+	spin_lock(&BTRFS_I(inode)->lock);
 	BTRFS_I(inode)->last_sub_trans = root->log_transid;
+	spin_unlock(&BTRFS_I(inode)->lock);
 	if (num_written > 0) {
 		err = generic_write_sync(file, pos, num_written);
 		if (err < 0)

commit 3d850dd44889d3aa67d0b8007c2cdd259bff7da4
Author: Filipe Manana <fdmanana@suse.com>
Date:   Thu Mar 12 23:23:13 2015 +0000

    Btrfs: add missing inode item update in fallocate()
    
    If we fallocate(), without the keep size flag, into an area already covered
    by an extent previously fallocated, we were updating the inode's i_size but
    we weren't updating the inode item in the fs/subvol tree. A following umount
    + mount would result in a loss of the inode's size (and an fsync would miss
    too the fact that the inode changed).
    
    Reproducer:
    
      $ mkfs.btrfs -f /dev/sdd
      $ mount /dev/sdd /mnt
      $ fallocate -n -l 1M /mnt/foobar
      $ fallocate -l 512K /mnt/foobar
      $ umount /mnt
      $ mount /dev/sdd /mnt
      $ od -t x1 /mnt/foobar
      0000000
    
    The expected result is:
    
      $ od -t x1 /mnt/foobar
      0000000 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
      *
      2000000
    
    A test case for fstests follows soon.
    
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Reviewed-by: Liu Bo <bo.li.liu@oracle.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 7d4bb3b6fbc2..150db5e50c2d 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2669,23 +2669,34 @@ static long btrfs_fallocate(struct file *file, int mode,
 							1 << inode->i_blkbits,
 							offset + len,
 							&alloc_hint);
-
-			if (ret < 0) {
-				free_extent_map(em);
-				break;
-			}
 		} else if (actual_end > inode->i_size &&
 			   !(mode & FALLOC_FL_KEEP_SIZE)) {
+			struct btrfs_trans_handle *trans;
+
 			/*
 			 * We didn't need to allocate any more space, but we
 			 * still extended the size of the file so we need to
-			 * update i_size.
+			 * update i_size and the inode item.
 			 */
-			inode->i_ctime = CURRENT_TIME;
-			i_size_write(inode, actual_end);
-			btrfs_ordered_update_i_size(inode, actual_end, NULL);
+			trans = btrfs_start_transaction(root, 1);
+			if (IS_ERR(trans)) {
+				ret = PTR_ERR(trans);
+			} else {
+				inode->i_ctime = CURRENT_TIME;
+				i_size_write(inode, actual_end);
+				btrfs_ordered_update_i_size(inode, actual_end,
+							    NULL);
+				ret = btrfs_update_inode(trans, root, inode);
+				if (ret)
+					btrfs_end_transaction(trans, root);
+				else
+					ret = btrfs_end_transaction(trans,
+								    root);
+			}
 		}
 		free_extent_map(em);
+		if (ret < 0)
+			break;
 
 		cur_offset = last_byte;
 		if (cur_offset >= alloc_end) {

commit e2e40f2c1ed433c5e224525c8c862fd32e5d3df2
Author: Christoph Hellwig <hch@lst.de>
Date:   Sun Feb 22 08:58:50 2015 -0800

    fs: move struct kiocb to fs.h
    
    struct kiocb now is a generic I/O container, so move it to fs.h.
    Also do a #include diet for aio.h while we're at it.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index b78bbbac900d..69c9508d2c7e 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -24,7 +24,6 @@
 #include <linux/string.h>
 #include <linux/backing-dev.h>
 #include <linux/mpage.h>
-#include <linux/aio.h>
 #include <linux/falloc.h>
 #include <linux/swap.h>
 #include <linux/writeback.h>
@@ -32,6 +31,7 @@
 #include <linux/compat.h>
 #include <linux/slab.h>
 #include <linux/btrfs.h>
+#include <linux/uio.h>
 #include "ctree.h"
 #include "disk-io.h"
 #include "transaction.h"

commit fc4c3c872f44bf425963feba57eb9c3f8ac2d7eb
Merge: 9deed229fa8a a4f3d2c4efe2
Author: Chris Mason <clm@fb.com>
Date:   Wed Mar 25 10:52:48 2015 -0700

    Merge branch 'cleanups-post-3.19' of git://git.kernel.org/pub/scm/linux/kernel/git/kdave/linux into for-linus-4.1
    
    Signed-off-by: Chris Mason <clm@fb.com>
    
    Conflicts:
            fs/btrfs/disk-io.c

commit 9deed229fa8a83bb5cd713b2d2a8e5c022a4b45b
Merge: bc465aa9d045 258ece02126a
Author: Chris Mason <clm@fb.com>
Date:   Wed Mar 25 10:43:16 2015 -0700

    Merge branch 'cleanups-for-4.1-v2' of git://git.kernel.org/pub/scm/linux/kernel/git/kdave/linux into for-linus-4.1

commit 84399bb075a6fe320d4221970dc36314e46229fe
Merge: 0d9b9c1674fa dd9ef135e354
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Mar 6 13:52:54 2015 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs
    
    Pull btrfs fixes from Chris Mason:
     "Outside of misc fixes, Filipe has a few fsync corners and we're
      pulling in one more of Josef's fixes from production use here"
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs:
      Btrfs:__add_inode_ref: out of bounds memory read when looking for extended ref.
      Btrfs: fix data loss in the fast fsync path
      Btrfs: remove extra run_delayed_refs in update_cowonly_root
      Btrfs: incremental send, don't rename a directory too soon
      btrfs: fix lost return value due to variable shadowing
      Btrfs: do not ignore errors from btrfs_lookup_xattr in do_setxattr
      Btrfs: fix off-by-one logic error in btrfs_realloc_node
      Btrfs: add missing inode update when punching hole
      Btrfs: abort the transaction if we fail to update the free space cache inode
      Btrfs: fix fsync race leading to ordered extent memory leaks

commit 3a8b36f378060d20062a0918e99fae39ff077bf0
Author: Filipe Manana <fdmanana@suse.com>
Date:   Sun Mar 1 20:36:00 2015 +0000

    Btrfs: fix data loss in the fast fsync path
    
    When using the fast file fsync code path we can miss the fact that new
    writes happened since the last file fsync and therefore return without
    waiting for the IO to finish and write the new extents to the fsync log.
    
    Here's an example scenario where the fsync will miss the fact that new
    file data exists that wasn't yet durably persisted:
    
    1. fs_info->last_trans_committed == N - 1 and current transaction is
       transaction N (fs_info->generation == N);
    
    2. do a buffered write;
    
    3. fsync our inode, this clears our inode's full sync flag, starts
       an ordered extent and waits for it to complete - when it completes
       at btrfs_finish_ordered_io(), the inode's last_trans is set to the
       value N (via btrfs_update_inode_fallback -> btrfs_update_inode ->
       btrfs_set_inode_last_trans);
    
    4. transaction N is committed, so fs_info->last_trans_committed is now
       set to the value N and fs_info->generation remains with the value N;
    
    5. do another buffered write, when this happens btrfs_file_write_iter
       sets our inode's last_trans to the value N + 1 (that is
       fs_info->generation + 1 == N + 1);
    
    6. transaction N + 1 is started and fs_info->generation now has the
       value N + 1;
    
    7. transaction N + 1 is committed, so fs_info->last_trans_committed
       is set to the value N + 1;
    
    8. fsync our inode - because it doesn't have the full sync flag set,
       we only start the ordered extent, we don't wait for it to complete
       (only in a later phase) therefore its last_trans field has the
       value N + 1 set previously by btrfs_file_write_iter(), and so we
       have:
    
           inode->last_trans <= fs_info->last_trans_committed
               (N + 1)              (N + 1)
    
       Which made us not log the last buffered write and exit the fsync
       handler immediately, returning success (0) to user space and resulting
       in data loss after a crash.
    
    This can actually be triggered deterministically and the following excerpt
    from a testcase I made for xfstests triggers the issue. It moves a dummy
    file across directories and then fsyncs the old parent directory - this
    is just to trigger a transaction commit, so moving files around isn't
    directly related to the issue but it was chosen because running 'sync' for
    example does more than just committing the current transaction, as it
    flushes/waits for all file data to be persisted. The issue can also happen
    at random periods, since the transaction kthread periodicaly commits the
    current transaction (about every 30 seconds by default).
    The body of the test is:
    
      _scratch_mkfs >> $seqres.full 2>&1
      _init_flakey
      _mount_flakey
    
      # Create our main test file 'foo', the one we check for data loss.
      # By doing an fsync against our file, it makes btrfs clear the 'needs_full_sync'
      # bit from its flags (btrfs inode specific flags).
      $XFS_IO_PROG -f -c "pwrite -S 0xaa 0 8K" \
                      -c "fsync" $SCRATCH_MNT/foo | _filter_xfs_io
    
      # Now create one other file and 2 directories. We will move this second file
      # from one directory to the other later because it forces btrfs to commit its
      # currently open transaction if we fsync the old parent directory. This is
      # necessary to trigger the data loss bug that affected btrfs.
      mkdir $SCRATCH_MNT/testdir_1
      touch $SCRATCH_MNT/testdir_1/bar
      mkdir $SCRATCH_MNT/testdir_2
    
      # Make sure everything is durably persisted.
      sync
    
      # Write more 8Kb of data to our file.
      $XFS_IO_PROG -c "pwrite -S 0xbb 8K 8K" $SCRATCH_MNT/foo | _filter_xfs_io
    
      # Move our 'bar' file into a new directory.
      mv $SCRATCH_MNT/testdir_1/bar $SCRATCH_MNT/testdir_2/bar
    
      # Fsync our first directory. Because it had a file moved into some other
      # directory, this made btrfs commit the currently open transaction. This is
      # a condition necessary to trigger the data loss bug.
      $XFS_IO_PROG -c "fsync" $SCRATCH_MNT/testdir_1
    
      # Now fsync our main test file. If the fsync succeeds, we expect the 8Kb of
      # data we wrote previously to be persisted and available if a crash happens.
      # This did not happen with btrfs, because of the transaction commit that
      # happened when we fsynced the parent directory.
      $XFS_IO_PROG -c "fsync" $SCRATCH_MNT/foo
    
      # Simulate a crash/power loss.
      _load_flakey_table $FLAKEY_DROP_WRITES
      _unmount_flakey
    
      _load_flakey_table $FLAKEY_ALLOW_WRITES
      _mount_flakey
    
      # Now check that all data we wrote before are available.
      echo "File content after log replay:"
      od -t x1 $SCRATCH_MNT/foo
    
      status=0
      exit
    
    The expected golden output for the test, which is what we get with this
    fix applied (or when running against ext3/4 and xfs), is:
    
      wrote 8192/8192 bytes at offset 0
      XXX Bytes, X ops; XX:XX:XX.X (XXX YYY/sec and XXX ops/sec)
      wrote 8192/8192 bytes at offset 8192
      XXX Bytes, X ops; XX:XX:XX.X (XXX YYY/sec and XXX ops/sec)
      File content after log replay:
      0000000 aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa
      *
      0020000 bb bb bb bb bb bb bb bb bb bb bb bb bb bb bb bb
      *
      0040000
    
    Without this fix applied, the output shows the test file does not have
    the second 8Kb extent that we successfully fsynced:
    
      wrote 8192/8192 bytes at offset 0
      XXX Bytes, X ops; XX:XX:XX.X (XXX YYY/sec and XXX ops/sec)
      wrote 8192/8192 bytes at offset 8192
      XXX Bytes, X ops; XX:XX:XX.X (XXX YYY/sec and XXX ops/sec)
      File content after log replay:
      0000000 aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa
      *
      0020000
    
    So fix this by skipping the fsync only if we're doing a full sync and
    if the inode's last_trans is <= fs_info->last_trans_committed, or if
    the inode is already in the log. Also remove setting the inode's
    last_trans in btrfs_file_write_iter since it's useless/unreliable.
    
    Also because btrfs_file_write_iter no longer sets inode->last_trans to
    fs_info->generation + 1, don't set last_trans to 0 if we bail out and don't
    bail out if last_trans is 0, otherwise something as simple as the following
    example wouldn't log the second write on the last fsync:
    
      1. write to file
    
      2. fsync file
    
      3. fsync file
           |--> btrfs_inode_in_log() returns true and it set last_trans to 0
    
      4. write to file
           |--> btrfs_file_write_iter() no longers sets last_trans, so it
                remained with a value of 0
      5. fsync
           |--> inode->last_trans == 0, so it bails out without logging the
                second write
    
    A test case for xfstests will be sent soon.
    
    CC: <stable@vger.kernel.org>
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index b476e5645034..6351947c9bb0 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1811,22 +1811,10 @@ static ssize_t btrfs_file_write_iter(struct kiocb *iocb,
 	mutex_unlock(&inode->i_mutex);
 
 	/*
-	 * we want to make sure fsync finds this change
-	 * but we haven't joined a transaction running right now.
-	 *
-	 * Later on, someone is sure to update the inode and get the
-	 * real transid recorded.
-	 *
-	 * We set last_trans now to the fs_info generation + 1,
-	 * this will either be one more than the running transaction
-	 * or the generation used for the next transaction if there isn't
-	 * one running right now.
-	 *
 	 * We also have to set last_sub_trans to the current log transid,
 	 * otherwise subsequent syncs to a file that's been synced in this
 	 * transaction will appear to have already occured.
 	 */
-	BTRFS_I(inode)->last_trans = root->fs_info->generation + 1;
 	BTRFS_I(inode)->last_sub_trans = root->log_transid;
 	if (num_written > 0) {
 		err = generic_write_sync(file, pos, num_written);
@@ -1959,25 +1947,37 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 	atomic_inc(&root->log_batch);
 
 	/*
-	 * check the transaction that last modified this inode
-	 * and see if its already been committed
-	 */
-	if (!BTRFS_I(inode)->last_trans) {
-		mutex_unlock(&inode->i_mutex);
-		goto out;
-	}
-
-	/*
-	 * if the last transaction that changed this file was before
-	 * the current transaction, we can bail out now without any
-	 * syncing
+	 * If the last transaction that changed this file was before the current
+	 * transaction and we have the full sync flag set in our inode, we can
+	 * bail out now without any syncing.
+	 *
+	 * Note that we can't bail out if the full sync flag isn't set. This is
+	 * because when the full sync flag is set we start all ordered extents
+	 * and wait for them to fully complete - when they complete they update
+	 * the inode's last_trans field through:
+	 *
+	 *     btrfs_finish_ordered_io() ->
+	 *         btrfs_update_inode_fallback() ->
+	 *             btrfs_update_inode() ->
+	 *                 btrfs_set_inode_last_trans()
+	 *
+	 * So we are sure that last_trans is up to date and can do this check to
+	 * bail out safely. For the fast path, when the full sync flag is not
+	 * set in our inode, we can not do it because we start only our ordered
+	 * extents and don't wait for them to complete (that is when
+	 * btrfs_finish_ordered_io runs), so here at this point their last_trans
+	 * value might be less than or equals to fs_info->last_trans_committed,
+	 * and setting a speculative last_trans for an inode when a buffered
+	 * write is made (such as fs_info->generation + 1 for example) would not
+	 * be reliable since after setting the value and before fsync is called
+	 * any number of transactions can start and commit (transaction kthread
+	 * commits the current transaction periodically), and a transaction
+	 * commit does not start nor waits for ordered extents to complete.
 	 */
 	smp_mb();
 	if (btrfs_inode_in_log(inode, root->fs_info->generation) ||
-	    BTRFS_I(inode)->last_trans <=
-	    root->fs_info->last_trans_committed) {
-		BTRFS_I(inode)->last_trans = 0;
-
+	    (full_sync && BTRFS_I(inode)->last_trans <=
+	     root->fs_info->last_trans_committed)) {
 		/*
 		 * We'v had everything committed since the last time we were
 		 * modified so clear this flag in case it was set for whatever

commit f64c7b12f86c638f13e19de08eeb8cf888dff8f6
Author: David Sterba <dsterba@suse.cz>
Date:   Tue Feb 24 19:07:26 2015 +0100

    btrfs: remove shadowing variables in __btrfs_buffered_write
    
    There are lockstart and lockend defined in the function and not used
    after their duplicate definition scope ends, it's safe to reuse them.
    
    Signed-off-by: David Sterba <dsterba@suse.cz>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index e74abb3018d4..a3c2bd77c74b 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1631,8 +1631,8 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 			btrfs_end_write_no_snapshoting(root);
 
 		if (only_release_metadata && copied > 0) {
-			u64 lockstart = round_down(pos, root->sectorsize);
-			u64 lockend = lockstart +
+			lockstart = round_down(pos, root->sectorsize);
+			lockend = lockstart +
 				(dirty_pages << PAGE_CACHE_SHIFT) - 1;
 
 			set_extent_bit(&BTRFS_I(inode)->io_tree, lockstart,

commit 31e818fe7375d60de9953051f7bd1615cebc3681
Author: David Sterba <dsterba@suse.cz>
Date:   Fri Feb 20 18:00:26 2015 +0100

    btrfs: cleanup, use kmalloc_array/kcalloc array helpers
    
    Convert kmalloc(nr * size, ..) to kmalloc_array that does additional
    overflow checks, the zeroing variant is kcalloc.
    
    Signed-off-by: David Sterba <dsterba@suse.cz>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 1e34bc00249f..e74abb3018d4 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1481,7 +1481,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 			PAGE_CACHE_SIZE / (sizeof(struct page *)));
 	nrptrs = min(nrptrs, current->nr_dirtied_pause - current->nr_dirtied);
 	nrptrs = max(nrptrs, 8);
-	pages = kmalloc(nrptrs * sizeof(struct page *), GFP_KERNEL);
+	pages = kmalloc_array(nrptrs, sizeof(struct page *), GFP_KERNEL);
 	if (!pages)
 		return -ENOMEM;
 

commit 351810c1d2aafa288af61844d877941d516fb031
Author: David Sterba <dsterba@suse.cz>
Date:   Thu Jan 8 15:20:54 2015 +0100

    btrfs: use cond_resched_lock where possible
    
    Clean the opencoded variant, cond_resched_lock also checks the lock for
    contention so it might help in some cases that were not covered by
    simple need_resched().
    
    Signed-off-by: David Sterba <dsterba@suse.cz>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index e4090259569b..1e34bc00249f 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -273,11 +273,7 @@ void btrfs_cleanup_defrag_inodes(struct btrfs_fs_info *fs_info)
 		defrag = rb_entry(node, struct inode_defrag, rb_node);
 		kmem_cache_free(btrfs_inode_defrag_cachep, defrag);
 
-		if (need_resched()) {
-			spin_unlock(&fs_info->defrag_inodes_lock);
-			cond_resched();
-			spin_lock(&fs_info->defrag_inodes_lock);
-		}
+		cond_resched_lock(&fs_info->defrag_inodes_lock);
 
 		node = rb_first(&fs_info->defrag_inodes);
 	}

commit e8c1c76e804b18120e6977fc092769c043876212
Author: Filipe Manana <fdmanana@suse.com>
Date:   Sun Feb 15 22:38:54 2015 +0000

    Btrfs: add missing inode update when punching hole
    
    When punching a file hole if we endup only zeroing parts of a page,
    because the start offset isn't a multiple of the sector size or the
    start offset and length fall within the same page, we were not updating
    the inode item. This prevented an fsync from doing anything, if no other
    file changes happened in the current transaction, because the fields
    in btrfs_inode used to check if the inode needs to be fsync'ed weren't
    updated.
    
    This issue is easy to reproduce and the following excerpt from the
    xfstest case I made shows how to trigger it:
    
      _scratch_mkfs >> $seqres.full 2>&1
      _init_flakey
      _mount_flakey
    
      # Create our test file.
      $XFS_IO_PROG -f -c "pwrite -S 0x22 -b 16K 0 16K" \
          $SCRATCH_MNT/foo | _filter_xfs_io
    
      # Fsync the file, this makes btrfs update some btrfs inode specific fields
      # that are used to track if the inode needs to be written/updated to the fsync
      # log or not. After this fsync, the new values for those fields indicate that
      # a subsequent fsync does not need to touch the fsync log.
      $XFS_IO_PROG -c "fsync" $SCRATCH_MNT/foo
    
      # Force a commit of the current transaction. After this point, any operation
      # that modifies the data or metadata of our file, should update those fields in
      # the btrfs inode with values that make the next fsync operation write to the
      # fsync log.
      sync
    
      # Punch a hole in our file. This small range affects only 1 page.
      # This made the btrfs hole punching implementation write only some zeroes in
      # one page, but it did not update the btrfs inode fields used to determine if
      # the next fsync needs to write to the fsync log.
      $XFS_IO_PROG -c "fpunch 8000 4K" $SCRATCH_MNT/foo
    
      # Another variation of the previously mentioned case.
      $XFS_IO_PROG -c "fpunch 15000 100" $SCRATCH_MNT/foo
    
      # Now fsync the file. This was a no-operation because the previous hole punch
      # operation didn't update the inode's fields mentioned before, so they remained
      # with the values they had after the first fsync - that is, they indicate that
      # it is not needed to write to fsync log.
      $XFS_IO_PROG -c "fsync" $SCRATCH_MNT/foo
    
      echo "File content before:"
      od -t x1 $SCRATCH_MNT/foo
    
      # Simulate a crash/power loss.
      _load_flakey_table $FLAKEY_DROP_WRITES
      _unmount_flakey
    
      # Enable writes and mount the fs. This makes the fsync log replay code run.
      _load_flakey_table $FLAKEY_ALLOW_WRITES
      _mount_flakey
    
      # Because the last fsync didn't do anything, here the file content matched what
      # it was after the first fsync, before the holes were punched, and not what it
      # was after the holes were punched.
      echo "File content after:"
      od -t x1 $SCRATCH_MNT/foo
    
    This issue has been around since 2012, when the punch hole implementation
    was added, commit 2aaa66558172 ("Btrfs: add hole punching").
    
    A test case for xfstests follows soon.
    
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Reviewed-by: Liu Bo <bo.li.liu@oracle.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index e4090259569b..b476e5645034 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2276,6 +2276,8 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 	bool same_page;
 	bool no_holes = btrfs_fs_incompat(root->fs_info, NO_HOLES);
 	u64 ino_size;
+	bool truncated_page = false;
+	bool updated_inode = false;
 
 	ret = btrfs_wait_ordered_range(inode, offset, len);
 	if (ret)
@@ -2307,13 +2309,18 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 	 * entire page.
 	 */
 	if (same_page && len < PAGE_CACHE_SIZE) {
-		if (offset < ino_size)
+		if (offset < ino_size) {
+			truncated_page = true;
 			ret = btrfs_truncate_page(inode, offset, len, 0);
+		} else {
+			ret = 0;
+		}
 		goto out_only_mutex;
 	}
 
 	/* zero back part of the first page */
 	if (offset < ino_size) {
+		truncated_page = true;
 		ret = btrfs_truncate_page(inode, offset, 0, 0);
 		if (ret) {
 			mutex_unlock(&inode->i_mutex);
@@ -2349,6 +2356,7 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 		if (!ret) {
 			/* zero the front end of the last page */
 			if (tail_start + tail_len < ino_size) {
+				truncated_page = true;
 				ret = btrfs_truncate_page(inode,
 						tail_start + tail_len, 0, 1);
 				if (ret)
@@ -2358,8 +2366,8 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 	}
 
 	if (lockend < lockstart) {
-		mutex_unlock(&inode->i_mutex);
-		return 0;
+		ret = 0;
+		goto out_only_mutex;
 	}
 
 	while (1) {
@@ -2507,6 +2515,7 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 
 	trans->block_rsv = &root->fs_info->trans_block_rsv;
 	ret = btrfs_update_inode(trans, root, inode);
+	updated_inode = true;
 	btrfs_end_transaction(trans, root);
 	btrfs_btree_balance_dirty(root);
 out_free:
@@ -2516,6 +2525,22 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 	unlock_extent_cached(&BTRFS_I(inode)->io_tree, lockstart, lockend,
 			     &cached_state, GFP_NOFS);
 out_only_mutex:
+	if (!updated_inode && truncated_page && !ret && !err) {
+		/*
+		 * If we only end up zeroing part of a page, we still need to
+		 * update the inode item, so that all the time fields are
+		 * updated as well as the necessary btrfs inode in memory fields
+		 * for detecting, at fsync time, if the inode isn't yet in the
+		 * log tree or it's there but not up to date.
+		 */
+		trans = btrfs_start_transaction(root, 1);
+		if (IS_ERR(trans)) {
+			err = PTR_ERR(trans);
+		} else {
+			err = btrfs_update_inode(trans, root, inode);
+			ret = btrfs_end_transaction(trans, root);
+		}
+	}
 	mutex_unlock(&inode->i_mutex);
 	if (ret && !err)
 		err = ret;

commit b7a0365ec7a0fb1d39113846fd34038af68ebd01
Author: Daniel Dressler <danieru.dressler@gmail.com>
Date:   Wed Nov 12 13:43:09 2014 +0900

    Btrfs: ctree: reduce args where only fs_info used
    
    This patch is part of a larger project to cleanup btrfs's internal usage
    of struct btrfs_root. Many functions take btrfs_root only to grab a
    pointer to fs_info.
    
    This causes programmers to ponder which root can be passed. Since only
    the fs_info is read affected functions can accept any root, except this
    is only obvious upon inspection.
    
    This patch reduces the specificty of such functions to accept the
    fs_info directly.
    
    This patch does not address the two functions in ctree.c (insert_ptr,
    and split_item) which only use root for BUG_ONs in ctree.c
    
    This patch affects the following functions:
      1) fixup_low_keys
      2) btrfs_set_item_key_safe
    
    Signed-off-by: Daniel Dressler <danieru.dressler@gmail.com>
    Signed-off-by: David Sterba <dsterba@suse.cz>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index e4090259569b..6b796f03de10 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -868,7 +868,7 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 
 			memcpy(&new_key, &key, sizeof(new_key));
 			new_key.offset = end;
-			btrfs_set_item_key_safe(root, path, &new_key);
+			btrfs_set_item_key_safe(root->fs_info, path, &new_key);
 
 			extent_offset += end - key.offset;
 			btrfs_set_file_extent_offset(leaf, fi, extent_offset);
@@ -1126,7 +1126,7 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 				     ino, bytenr, orig_offset,
 				     &other_start, &other_end)) {
 			new_key.offset = end;
-			btrfs_set_item_key_safe(root, path, &new_key);
+			btrfs_set_item_key_safe(root->fs_info, path, &new_key);
 			fi = btrfs_item_ptr(leaf, path->slots[0],
 					    struct btrfs_file_extent_item);
 			btrfs_set_file_extent_generation(leaf, fi,
@@ -1160,7 +1160,7 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 							 trans->transid);
 			path->slots[0]++;
 			new_key.offset = start;
-			btrfs_set_item_key_safe(root, path, &new_key);
+			btrfs_set_item_key_safe(root->fs_info, path, &new_key);
 
 			fi = btrfs_item_ptr(leaf, path->slots[0],
 					    struct btrfs_file_extent_item);
@@ -2169,7 +2169,7 @@ static int fill_holes(struct btrfs_trans_handle *trans, struct inode *inode,
 		u64 num_bytes;
 
 		key.offset = offset;
-		btrfs_set_item_key_safe(root, path, &key);
+		btrfs_set_item_key_safe(root->fs_info, path, &key);
 		fi = btrfs_item_ptr(leaf, path->slots[0],
 				    struct btrfs_file_extent_item);
 		num_bytes = btrfs_file_extent_num_bytes(leaf, fi) + end -

commit 6bec0035286119eefc32a5b1102127e6a4032cb2
Merge: 5d8e7fb69165 15d0f5ea348b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Feb 12 13:50:21 2015 -0800

    Merge branch 'for-3.20/bdi' of git://git.kernel.dk/linux-block
    
    Pull backing device changes from Jens Axboe:
     "This contains a cleanup of how the backing device is handled, in
      preparation for a rework of the life time rules.  In this part, the
      most important change is to split the unrelated nommu mmap flags from
      it, but also removing a backing_dev_info pointer from the
      address_space (and inode), and a cleanup of other various minor bits.
    
      Christoph did all the work here, I just fixed an oops with pages that
      have a swap backing.  Arnd fixed a missing export, and Oleg killed the
      lustre backing_dev_info from staging.  Last patch was from Al,
      unexporting parts that are now no longer needed outside"
    
    * 'for-3.20/bdi' of git://git.kernel.dk/linux-block:
      Make super_blocks and sb_lock static
      mtd: export new mtd_mmap_capabilities
      fs: make inode_to_bdi() handle NULL inode
      staging/lustre/llite: get rid of backing_dev_info
      fs: remove default_backing_dev_info
      fs: don't reassign dirty inodes to default_backing_dev_info
      nfs: don't call bdi_unregister
      ceph: remove call to bdi_unregister
      fs: remove mapping->backing_dev_info
      fs: export inode_to_bdi and use it in favor of mapping->backing_dev_info
      nilfs2: set up s_bdi like the generic mount_bdev code
      block_dev: get bdev inode bdi directly from the block device
      block_dev: only write bdev inode on close
      fs: introduce f_op->mmap_capabilities for nommu mmap support
      fs: kill BDI_CAP_SWAP_BACKED
      fs: deduplicate noop_backing_dev_info

commit d83a08db5ba6072caa658745881f4baa9bad6a08
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Tue Feb 10 14:09:54 2015 -0800

    mm: drop vm_ops->remap_pages and generic_file_remap_pages() stub
    
    Nobody uses it anymore.
    
    [akpm@linux-foundation.org: fix filemap_xip.c]
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index e4090259569b..a606ab551296 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2081,7 +2081,6 @@ static const struct vm_operations_struct btrfs_file_vm_ops = {
 	.fault		= filemap_fault,
 	.map_pages	= filemap_map_pages,
 	.page_mkwrite	= btrfs_page_mkwrite,
-	.remap_pages	= generic_file_remap_pages,
 };
 
 static int btrfs_file_mmap(struct file	*filp, struct vm_area_struct *vma)

commit de1414a654e66b81b5348dbc5259ecf2fb61655e
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Jan 14 10:42:36 2015 +0100

    fs: export inode_to_bdi and use it in favor of mapping->backing_dev_info
    
    Now that we got rid of the bdi abuse on character devices we can always use
    sb->s_bdi to get at the backing_dev_info for a file, except for the block
    device special case.  Export inode_to_bdi and replace uses of
    mapping->backing_dev_info with it to prepare for the removal of
    mapping->backing_dev_info.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index e4090259569b..835c04a874fd 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1746,7 +1746,7 @@ static ssize_t btrfs_file_write_iter(struct kiocb *iocb,
 
 	mutex_lock(&inode->i_mutex);
 
-	current->backing_dev_info = inode->i_mapping->backing_dev_info;
+	current->backing_dev_info = inode_to_bdi(inode);
 	err = generic_write_checks(file, &pos, &count, S_ISBLK(inode->i_mode));
 	if (err) {
 		mutex_unlock(&inode->i_mutex);

commit 9ea24bbe17a29f937e7f48e4b15fd52e89e9d386
Author: Filipe Manana <fdmanana@suse.com>
Date:   Wed Oct 29 11:57:59 2014 +0000

    Btrfs: fix snapshot inconsistency after a file write followed by truncate
    
    If right after starting the snapshot creation ioctl we perform a write against a
    file followed by a truncate, with both operations increasing the file's size, we
    can get a snapshot tree that reflects a state of the source subvolume's tree where
    the file truncation happened but the write operation didn't. This leaves a gap
    between 2 file extent items of the inode, which makes btrfs' fsck complain about it.
    
    For example, if we perform the following file operations:
    
        $ mkfs.btrfs -f /dev/vdd
        $ mount /dev/vdd /mnt
        $ xfs_io -f \
              -c "pwrite -S 0xaa -b 32K 0 32K" \
              -c "fsync" \
              -c "pwrite -S 0xbb -b 32770 16K 32770" \
              -c "truncate 90123" \
              /mnt/foobar
    
    and the snapshot creation ioctl was just called before the second write, we often
    can get the following inode items in the snapshot's btree:
    
            item 120 key (257 INODE_ITEM 0) itemoff 7987 itemsize 160
                    inode generation 146 transid 7 size 90123 block group 0 mode 100600 links 1 uid 0 gid 0 rdev 0 flags 0x0
            item 121 key (257 INODE_REF 256) itemoff 7967 itemsize 20
                    inode ref index 282 namelen 10 name: foobar
            item 122 key (257 EXTENT_DATA 0) itemoff 7914 itemsize 53
                    extent data disk byte 1104855040 nr 32768
                    extent data offset 0 nr 32768 ram 32768
                    extent compression 0
            item 123 key (257 EXTENT_DATA 53248) itemoff 7861 itemsize 53
                    extent data disk byte 0 nr 0
                    extent data offset 0 nr 40960 ram 40960
                    extent compression 0
    
    There's a file range, corresponding to the interval [32K; ALIGN(16K + 32770, 4096)[
    for which there's no file extent item covering it. This is because the file write
    and file truncate operations happened both right after the snapshot creation ioctl
    called btrfs_start_delalloc_inodes(), which means we didn't start and wait for the
    ordered extent that matches the write and, in btrfs_setsize(), we were able to call
    btrfs_cont_expand() before being able to commit the current transaction in the
    snapshot creation ioctl. So this made it possibe to insert the hole file extent
    item in the source subvolume (which represents the region added by the truncate)
    right before the transaction commit from the snapshot creation ioctl.
    
    Btrfs' fsck tool complains about such cases with a message like the following:
    
        "root 331 inode 257 errors 100, file extent discount"
    
    >From a user perspective, the expectation when a snapshot is created while those
    file operations are being performed is that the snapshot will have a file that
    either:
    
    1) is empty
    2) only the first write was captured
    3) only the 2 writes were captured
    4) both writes and the truncation were captured
    
    But never capture a state where only the first write and the truncation were
    captured (since the second write was performed before the truncation).
    
    A test case for xfstests follows.
    
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 0fbf0e7bc606..e4090259569b 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1428,7 +1428,7 @@ static noinline int check_can_nocow(struct inode *inode, loff_t pos,
 	u64 num_bytes;
 	int ret;
 
-	ret = btrfs_start_nocow_write(root);
+	ret = btrfs_start_write_no_snapshoting(root);
 	if (!ret)
 		return -ENOSPC;
 
@@ -1451,7 +1451,7 @@ static noinline int check_can_nocow(struct inode *inode, loff_t pos,
 	ret = can_nocow_extent(inode, lockstart, &num_bytes, NULL, NULL, NULL);
 	if (ret <= 0) {
 		ret = 0;
-		btrfs_end_nocow_write(root);
+		btrfs_end_write_no_snapshoting(root);
 	} else {
 		*write_bytes = min_t(size_t, *write_bytes ,
 				     num_bytes - pos + lockstart);
@@ -1543,7 +1543,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 				btrfs_free_reserved_data_space(inode,
 							       reserve_bytes);
 			else
-				btrfs_end_nocow_write(root);
+				btrfs_end_write_no_snapshoting(root);
 			break;
 		}
 
@@ -1632,7 +1632,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 
 		release_bytes = 0;
 		if (only_release_metadata)
-			btrfs_end_nocow_write(root);
+			btrfs_end_write_no_snapshoting(root);
 
 		if (only_release_metadata && copied > 0) {
 			u64 lockstart = round_down(pos, root->sectorsize);
@@ -1661,7 +1661,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 
 	if (release_bytes) {
 		if (only_release_metadata) {
-			btrfs_end_nocow_write(root);
+			btrfs_end_write_no_snapshoting(root);
 			btrfs_delalloc_release_metadata(inode, release_bytes);
 		} else {
 			btrfs_delalloc_release_space(inode, release_bytes);

commit 728404dacfddb5364d7256d821a2ea482159cbe7
Author: Filipe Manana <fdmanana@suse.com>
Date:   Fri Oct 10 09:43:11 2014 +0100

    Btrfs: add helper btrfs_fdatawrite_range
    
    To avoid duplicating this double filemap_fdatawrite_range() call for
    inodes with async extents (compressed writes) so often.
    
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index f5a868ab60f3..0fbf0e7bc606 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1676,6 +1676,7 @@ static ssize_t __btrfs_direct_write(struct kiocb *iocb,
 				    loff_t pos)
 {
 	struct file *file = iocb->ki_filp;
+	struct inode *inode = file_inode(file);
 	ssize_t written;
 	ssize_t written_buffered;
 	loff_t endbyte;
@@ -1697,13 +1698,10 @@ static ssize_t __btrfs_direct_write(struct kiocb *iocb,
 	 * able to read what was just written.
 	 */
 	endbyte = pos + written_buffered - 1;
-	err = filemap_fdatawrite_range(file->f_mapping, pos, endbyte);
-	if (!err && test_bit(BTRFS_INODE_HAS_ASYNC_EXTENT,
-			     &BTRFS_I(file_inode(file))->runtime_flags))
-		err = filemap_fdatawrite_range(file->f_mapping, pos, endbyte);
+	err = btrfs_fdatawrite_range(inode, pos, endbyte);
 	if (err)
 		goto out;
-	err = filemap_fdatawait_range(file->f_mapping, pos, endbyte);
+	err = filemap_fdatawait_range(inode->i_mapping, pos, endbyte);
 	if (err)
 		goto out;
 	written += written_buffered;
@@ -1864,10 +1862,7 @@ static int start_ordered_ops(struct inode *inode, loff_t start, loff_t end)
 	int ret;
 
 	atomic_inc(&BTRFS_I(inode)->sync_writers);
-	ret = filemap_fdatawrite_range(inode->i_mapping, start, end);
-	if (!ret && test_bit(BTRFS_INODE_HAS_ASYNC_EXTENT,
-			     &BTRFS_I(inode)->runtime_flags))
-		ret = filemap_fdatawrite_range(inode->i_mapping, start, end);
+	ret = btrfs_fdatawrite_range(inode, start, end);
 	atomic_dec(&BTRFS_I(inode)->sync_writers);
 
 	return ret;
@@ -2820,3 +2815,29 @@ int btrfs_auto_defrag_init(void)
 
 	return 0;
 }
+
+int btrfs_fdatawrite_range(struct inode *inode, loff_t start, loff_t end)
+{
+	int ret;
+
+	/*
+	 * So with compression we will find and lock a dirty page and clear the
+	 * first one as dirty, setup an async extent, and immediately return
+	 * with the entire range locked but with nobody actually marked with
+	 * writeback.  So we can't just filemap_write_and_wait_range() and
+	 * expect it to work since it will just kick off a thread to do the
+	 * actual work.  So we need to call filemap_fdatawrite_range _again_
+	 * since it will wait on the page lock, which won't be unlocked until
+	 * after the pages have been marked as writeback and so we're good to go
+	 * from there.  We have to do this otherwise we'll miss the ordered
+	 * extents and that results in badness.  Please Josef, do not think you
+	 * know better and pull this out at some point in the future, it is
+	 * right and you are wrong.
+	 */
+	ret = filemap_fdatawrite_range(inode->i_mapping, start, end);
+	if (!ret && test_bit(BTRFS_INODE_HAS_ASYNC_EXTENT,
+			     &BTRFS_I(inode)->runtime_flags))
+		ret = filemap_fdatawrite_range(inode->i_mapping, start, end);
+
+	return ret;
+}

commit 075bdbdbe9f21d68950ba5b187f80a4a23105365
Author: Filipe Manana <fdmanana@suse.com>
Date:   Thu Oct 9 21:18:55 2014 +0100

    Btrfs: correctly flush compressed data before/after direct IO
    
    For compressed writes, after doing the first filemap_fdatawrite_range() we
    don't get the pages tagged for writeback immediately. Instead we create
    a workqueue task, which is run by other kthread, and keep the pages locked.
    That other kthread compresses data, creates the respective ordered extent/s,
    tags the pages for writeback and unlocks them. Therefore we need a second
    call to filemap_fdatawrite_range() if we have compressed writes, as this
    second call will wait for the pages to become unlocked, then see they became
    tagged for writeback and finally wait for the writeback to finish.
    
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index a18ceabd99a8..f5a868ab60f3 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1692,8 +1692,18 @@ static ssize_t __btrfs_direct_write(struct kiocb *iocb,
 		err = written_buffered;
 		goto out;
 	}
+	/*
+	 * Ensure all data is persisted. We want the next direct IO read to be
+	 * able to read what was just written.
+	 */
 	endbyte = pos + written_buffered - 1;
-	err = filemap_write_and_wait_range(file->f_mapping, pos, endbyte);
+	err = filemap_fdatawrite_range(file->f_mapping, pos, endbyte);
+	if (!err && test_bit(BTRFS_INODE_HAS_ASYNC_EXTENT,
+			     &BTRFS_I(file_inode(file))->runtime_flags))
+		err = filemap_fdatawrite_range(file->f_mapping, pos, endbyte);
+	if (err)
+		goto out;
+	err = filemap_fdatawait_range(file->f_mapping, pos, endbyte);
 	if (err)
 		goto out;
 	written += written_buffered;

commit ee39b432b4ac083acdafd7b4f156283722e3bf14
Author: David Sterba <dsterba@suse.cz>
Date:   Tue Sep 30 01:33:33 2014 +0200

    btrfs: remove unlikely from data-dependent branches and slow paths
    
    There are the branch hints that obviously depend on the data being
    processed, the CPU predictor will do better job according to the actual
    load. It also does not make sense to use the hints in slow paths that do
    a lot of other operations like locking, waiting or IO.
    
    Signed-off-by: David Sterba <dsterba@suse.cz>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 29b147d46b0a..a18ceabd99a8 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -452,7 +452,7 @@ static noinline int btrfs_copy_from_user(loff_t pos, int num_pages,
 		if (unlikely(copied == 0))
 			break;
 
-		if (unlikely(copied < PAGE_CACHE_SIZE - offset)) {
+		if (copied < PAGE_CACHE_SIZE - offset) {
 			offset += copied;
 		} else {
 			pg++;
@@ -1792,7 +1792,7 @@ static ssize_t btrfs_file_write_iter(struct kiocb *iocb,
 	if (sync)
 		atomic_inc(&BTRFS_I(inode)->sync_writers);
 
-	if (unlikely(file->f_flags & O_DIRECT)) {
+	if (file->f_flags & O_DIRECT) {
 		num_written = __btrfs_direct_write(iocb, from, pos);
 	} else {
 		num_written = __btrfs_buffered_write(file, from, pos);

commit 8407f553268a4611f2542ed90677f0edfaa2c9c4
Author: Filipe Manana <fdmanana@suse.com>
Date:   Fri Sep 5 15:14:39 2014 +0100

    Btrfs: fix data corruption after fast fsync and writeback error
    
    When we do a fast fsync, we start all ordered operations and then while
    they're running in parallel we visit the list of modified extent maps
    and construct their matching file extent items and write them to the
    log btree. After that, in btrfs_sync_log() we wait for all the ordered
    operations to finish (via btrfs_wait_logged_extents).
    
    The problem with this is that we were completely ignoring errors that
    can happen in the extent write path, such as -ENOSPC, a temporary -ENOMEM
    or -EIO errors for example. When such error happens, it means we have parts
    of the on disk extent that weren't written to, and so we end up logging
    file extent items that point to these extents that contain garbage/random
    data - so after a crash/reboot plus log replay, we get our inode's metadata
    pointing to those extents.
    
    This worked in contrast with the full (non-fast) fsync path, where we
    start all ordered operations, wait for them to finish and then write
    to the log btree. In this path, after each ordered operation completes
    we check if it's flagged with an error (BTRFS_ORDERED_IOERR) and return
    -EIO if so (via btrfs_wait_ordered_range).
    
    So if an error happens with any ordered operation, just return a -EIO
    error to userspace, so that it knows that not all of its previous writes
    were durably persisted and the application can take proper action (like
    redo the writes for e.g.) - and definitely not leave any file extent items
    in the log refer to non fully written extents.
    
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index cdb71461e0fe..29b147d46b0a 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2029,6 +2029,25 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 	 */
 	mutex_unlock(&inode->i_mutex);
 
+	/*
+	 * If any of the ordered extents had an error, just return it to user
+	 * space, so that the application knows some writes didn't succeed and
+	 * can take proper action (retry for e.g.). Blindly committing the
+	 * transaction in this case, would fool userspace that everything was
+	 * successful. And we also want to make sure our log doesn't contain
+	 * file extent items pointing to extents that weren't fully written to -
+	 * just like in the non fast fsync path, where we check for the ordered
+	 * operation's error flag before writing to the log tree and return -EIO
+	 * if any of them had this flag set (btrfs_wait_ordered_range) -
+	 * therefore we need to check for errors in the ordered operations,
+	 * which are indicated by ctx.io_err.
+	 */
+	if (ctx.io_err) {
+		btrfs_end_transaction(trans, root);
+		ret = ctx.io_err;
+		goto out;
+	}
+
 	if (ret != BTRFS_NO_LOG_SYNC) {
 		if (!ret) {
 			ret = btrfs_sync_log(trans, root, &ctx);

commit 669249eea365dd32b793b58891c74281c0aac47e
Author: Filipe Manana <fdmanana@suse.com>
Date:   Tue Sep 2 11:09:58 2014 +0100

    Btrfs: fix fsync race leading to invalid data after log replay
    
    When the fsync callback (btrfs_sync_file) starts, it first waits for
    the writeback of any dirty pages to start and finish without holding
    the inode's mutex (to reduce contention). After this it acquires the
    inode's mutex and repeats that process via btrfs_wait_ordered_range
    only if we're doing a full sync (BTRFS_INODE_NEEDS_FULL_SYNC flag
    is set on the inode).
    
    This is not safe for a non full sync - we need to start and wait for
    writeback to finish for any pages that might have been made dirty
    before acquiring the inode's mutex and after that first step mentioned
    before. Why this is needed is explained by the following comment added
    to btrfs_sync_file:
    
      "Right before acquiring the inode's mutex, we might have new
       writes dirtying pages, which won't immediately start the
       respective ordered operations - that is done through the
       fill_delalloc callbacks invoked from the writepage and
       writepages address space operations. So make sure we start
       all ordered operations before starting to log our inode. Not
       doing this means that while logging the inode, writeback
       could start and invoke writepage/writepages, which would call
       the fill_delalloc callbacks (cow_file_range,
       submit_compressed_extents). These callbacks add first an
       extent map to the modified list of extents and then create
       the respective ordered operation, which means in
       tree-log.c:btrfs_log_inode() we might capture all existing
       ordered operations (with btrfs_get_logged_extents()) before
       the fill_delalloc callback adds its ordered operation, and by
       the time we visit the modified list of extent maps (with
       btrfs_log_changed_extents()), we see and process the extent
       map they created. We then use the extent map to construct a
       file extent item for logging without waiting for the
       respective ordered operation to finish - this file extent
       item points to a disk location that might not have yet been
       written to, containing random data - so after a crash a log
       replay will make our inode have file extent items that point
       to disk locations containing invalid data, as we returned
       success to userspace without waiting for the respective
       ordered operation to finish, because it wasn't captured by
       btrfs_get_logged_extents()."
    
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index d5d5060fe891..cdb71461e0fe 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1849,6 +1849,20 @@ int btrfs_release_file(struct inode *inode, struct file *filp)
 	return 0;
 }
 
+static int start_ordered_ops(struct inode *inode, loff_t start, loff_t end)
+{
+	int ret;
+
+	atomic_inc(&BTRFS_I(inode)->sync_writers);
+	ret = filemap_fdatawrite_range(inode->i_mapping, start, end);
+	if (!ret && test_bit(BTRFS_INODE_HAS_ASYNC_EXTENT,
+			     &BTRFS_I(inode)->runtime_flags))
+		ret = filemap_fdatawrite_range(inode->i_mapping, start, end);
+	atomic_dec(&BTRFS_I(inode)->sync_writers);
+
+	return ret;
+}
+
 /*
  * fsync call for both files and directories.  This logs the inode into
  * the tree log instead of forcing full commits whenever possible.
@@ -1878,30 +1892,64 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 	 * multi-task, and make the performance up.  See
 	 * btrfs_wait_ordered_range for an explanation of the ASYNC check.
 	 */
-	atomic_inc(&BTRFS_I(inode)->sync_writers);
-	ret = filemap_fdatawrite_range(inode->i_mapping, start, end);
-	if (!ret && test_bit(BTRFS_INODE_HAS_ASYNC_EXTENT,
-			     &BTRFS_I(inode)->runtime_flags))
-		ret = filemap_fdatawrite_range(inode->i_mapping, start, end);
-	atomic_dec(&BTRFS_I(inode)->sync_writers);
+	ret = start_ordered_ops(inode, start, end);
 	if (ret)
 		return ret;
 
 	mutex_lock(&inode->i_mutex);
-
-	/*
-	 * We flush the dirty pages again to avoid some dirty pages in the
-	 * range being left.
-	 */
 	atomic_inc(&root->log_batch);
 	full_sync = test_bit(BTRFS_INODE_NEEDS_FULL_SYNC,
 			     &BTRFS_I(inode)->runtime_flags);
+	/*
+	 * We might have have had more pages made dirty after calling
+	 * start_ordered_ops and before acquiring the inode's i_mutex.
+	 */
 	if (full_sync) {
+		/*
+		 * For a full sync, we need to make sure any ordered operations
+		 * start and finish before we start logging the inode, so that
+		 * all extents are persisted and the respective file extent
+		 * items are in the fs/subvol btree.
+		 */
 		ret = btrfs_wait_ordered_range(inode, start, end - start + 1);
-		if (ret) {
-			mutex_unlock(&inode->i_mutex);
-			goto out;
-		}
+	} else {
+		/*
+		 * Start any new ordered operations before starting to log the
+		 * inode. We will wait for them to finish in btrfs_sync_log().
+		 *
+		 * Right before acquiring the inode's mutex, we might have new
+		 * writes dirtying pages, which won't immediately start the
+		 * respective ordered operations - that is done through the
+		 * fill_delalloc callbacks invoked from the writepage and
+		 * writepages address space operations. So make sure we start
+		 * all ordered operations before starting to log our inode. Not
+		 * doing this means that while logging the inode, writeback
+		 * could start and invoke writepage/writepages, which would call
+		 * the fill_delalloc callbacks (cow_file_range,
+		 * submit_compressed_extents). These callbacks add first an
+		 * extent map to the modified list of extents and then create
+		 * the respective ordered operation, which means in
+		 * tree-log.c:btrfs_log_inode() we might capture all existing
+		 * ordered operations (with btrfs_get_logged_extents()) before
+		 * the fill_delalloc callback adds its ordered operation, and by
+		 * the time we visit the modified list of extent maps (with
+		 * btrfs_log_changed_extents()), we see and process the extent
+		 * map they created. We then use the extent map to construct a
+		 * file extent item for logging without waiting for the
+		 * respective ordered operation to finish - this file extent
+		 * item points to a disk location that might not have yet been
+		 * written to, containing random data - so after a crash a log
+		 * replay will make our inode have file extent items that point
+		 * to disk locations containing invalid data, as we returned
+		 * success to userspace without waiting for the respective
+		 * ordered operation to finish, because it wasn't captured by
+		 * btrfs_get_logged_extents().
+		 */
+		ret = start_ordered_ops(inode, start, end);
+	}
+	if (ret) {
+		mutex_unlock(&inode->i_mutex);
+		goto out;
 	}
 	atomic_inc(&root->log_batch);
 

commit 4d1a40c66bed0b3fa43b9da5fbd5cbe332e4eccf
Author: Liu Bo <bo.li.liu@oracle.com>
Date:   Tue Sep 16 17:49:30 2014 +0800

    Btrfs: fix up bounds checking in lseek
    
    An user reported this, it is because that lseek's SEEK_SET/SEEK_CUR/SEEK_END
    allow a negative value for @offset, but btrfs's SEEK_DATA/SEEK_HOLE don't
    prepare for that and convert the negative @offset into unsigned type,
    so we get (end < start) warning.
    
    [ 1269.835374] ------------[ cut here ]------------
    [ 1269.836809] WARNING: CPU: 0 PID: 1241 at fs/btrfs/extent_io.c:430 insert_state+0x11d/0x140()
    [ 1269.838816] BTRFS: end < start 4094 18446744073709551615
    [ 1269.840334] CPU: 0 PID: 1241 Comm: a.out Tainted: G        W      3.16.0+ #306
    [ 1269.858229] Call Trace:
    [ 1269.858612]  [<ffffffff81801a69>] dump_stack+0x4e/0x68
    [ 1269.858952]  [<ffffffff8107894c>] warn_slowpath_common+0x8c/0xc0
    [ 1269.859416]  [<ffffffff81078a36>] warn_slowpath_fmt+0x46/0x50
    [ 1269.859929]  [<ffffffff813b0fbd>] insert_state+0x11d/0x140
    [ 1269.860409]  [<ffffffff813b1396>] __set_extent_bit+0x3b6/0x4e0
    [ 1269.860805]  [<ffffffff813b21c7>] lock_extent_bits+0x87/0x200
    [ 1269.861697]  [<ffffffff813a5b28>] btrfs_file_llseek+0x148/0x2a0
    [ 1269.862168]  [<ffffffff811f201e>] SyS_lseek+0xae/0xc0
    [ 1269.862620]  [<ffffffff8180b212>] system_call_fastpath+0x16/0x1b
    [ 1269.862970] ---[ end trace 4d33ea885832054b ]---
    
    This assumes that btrfs starts finding DATA/HOLE from the beginning of file
    if the assigned @offset is negative.
    
    Also we add alignment for lock_extent_bits 's range.
    
    Reported-by: Toralf Frster <toralf.foerster@gmx.de>
    Signed-off-by: Liu Bo <bo.li.liu@oracle.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 2287545c5498..d5d5060fe891 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2618,23 +2618,28 @@ static int find_desired_extent(struct inode *inode, loff_t *offset, int whence)
 	struct btrfs_root *root = BTRFS_I(inode)->root;
 	struct extent_map *em = NULL;
 	struct extent_state *cached_state = NULL;
-	u64 lockstart = *offset;
-	u64 lockend = i_size_read(inode);
-	u64 start = *offset;
-	u64 len = i_size_read(inode);
+	u64 lockstart;
+	u64 lockend;
+	u64 start;
+	u64 len;
 	int ret = 0;
 
-	lockend = max_t(u64, root->sectorsize, lockend);
+	if (inode->i_size == 0)
+		return -ENXIO;
+
+	/*
+	 * *offset can be negative, in this case we start finding DATA/HOLE from
+	 * the very start of the file.
+	 */
+	start = max_t(loff_t, 0, *offset);
+
+	lockstart = round_down(start, root->sectorsize);
+	lockend = round_up(i_size_read(inode), root->sectorsize);
 	if (lockend <= lockstart)
 		lockend = lockstart + root->sectorsize;
-
 	lockend--;
 	len = lockend - lockstart + 1;
 
-	len = max_t(u64, len, root->sectorsize);
-	if (inode->i_size == 0)
-		return -ENXIO;
-
 	lock_extent_bits(&BTRFS_I(inode)->io_tree, lockstart, lockend, 0,
 			 &cached_state);
 

commit ed6078f70335f158ca79790a0d0708ce558a6e9a
Author: David Sterba <dsterba@suse.cz>
Date:   Thu Jun 5 01:59:57 2014 +0200

    btrfs: use DIV_ROUND_UP instead of open-coded variants
    
    The form
    
      (value + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT
    
    is equivalent to
    
      (value + PAGE_CACHE_SIZE - 1) / PAGE_CACHE_SIZE
    
    The rest is a simple subsitution, no difference in the generated
    assembly code.
    
    Signed-off-by: David Sterba <dsterba@suse.cz>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 033f04bac85b..2287545c5498 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1481,9 +1481,8 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 	bool force_page_uptodate = false;
 	bool need_unlock;
 
-	nrptrs = min((iov_iter_count(i) + PAGE_CACHE_SIZE - 1) /
-		     PAGE_CACHE_SIZE, PAGE_CACHE_SIZE /
-		     (sizeof(struct page *)));
+	nrptrs = min(DIV_ROUND_UP(iov_iter_count(i), PAGE_CACHE_SIZE),
+			PAGE_CACHE_SIZE / (sizeof(struct page *)));
 	nrptrs = min(nrptrs, current->nr_dirtied_pause - current->nr_dirtied);
 	nrptrs = max(nrptrs, 8);
 	pages = kmalloc(nrptrs * sizeof(struct page *), GFP_KERNEL);
@@ -1497,8 +1496,8 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 		size_t write_bytes = min(iov_iter_count(i),
 					 nrptrs * (size_t)PAGE_CACHE_SIZE -
 					 offset);
-		size_t num_pages = (write_bytes + offset +
-				    PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT;
+		size_t num_pages = DIV_ROUND_UP(write_bytes + offset,
+						PAGE_CACHE_SIZE);
 		size_t reserve_bytes;
 		size_t dirty_pages;
 		size_t copied;
@@ -1526,9 +1525,8 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 				 * our prealloc extent may be smaller than
 				 * write_bytes, so scale down.
 				 */
-				num_pages = (write_bytes + offset +
-					     PAGE_CACHE_SIZE - 1) >>
-					PAGE_CACHE_SHIFT;
+				num_pages = DIV_ROUND_UP(write_bytes + offset,
+							 PAGE_CACHE_SIZE);
 				reserve_bytes = num_pages << PAGE_CACHE_SHIFT;
 				ret = 0;
 			} else {
@@ -1590,9 +1588,8 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 			dirty_pages = 0;
 		} else {
 			force_page_uptodate = false;
-			dirty_pages = (copied + offset +
-				       PAGE_CACHE_SIZE - 1) >>
-				       PAGE_CACHE_SHIFT;
+			dirty_pages = DIV_ROUND_UP(copied + offset,
+						   PAGE_CACHE_SIZE);
 		}
 
 		/*

commit 707e8a071528385a87b63a72a37c2322e463c7b8
Author: David Sterba <dsterba@suse.cz>
Date:   Wed Jun 4 19:22:26 2014 +0200

    btrfs: use nodesize everywhere, kill leafsize
    
    The nodesize and leafsize were never of different values. Unify the
    usage and make nodesize the one. Cleanup the redundant checks and
    helpers.
    
    Shaves a few bytes from .text:
    
      text    data     bss     dec     hex filename
    852418   24560   23112  900090   dbbfa btrfs.ko.before
    851074   24584   23112  898770   db6d2 btrfs.ko.after
    
    Signed-off-by: David Sterba <dsterba@suse.cz>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index a9b56e32dd8d..033f04bac85b 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1653,7 +1653,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 		cond_resched();
 
 		balance_dirty_pages_ratelimited(inode->i_mapping);
-		if (dirty_pages < (root->leafsize >> PAGE_CACHE_SHIFT) + 1)
+		if (dirty_pages < (root->nodesize >> PAGE_CACHE_SHIFT) + 1)
 			btrfs_btree_balance_dirty(root);
 
 		pos += copied;

commit 962a298f35110edd8f326814ae41a3dd306ecb64
Author: David Sterba <dsterba@suse.cz>
Date:   Wed Jun 4 18:41:45 2014 +0200

    btrfs: kill the key type accessor helpers
    
    btrfs_set_key_type and btrfs_key_type are used inconsistently along with
    open coded variants. Other members of btrfs_key are accessed directly
    without any helpers anyway.
    
    Signed-off-by: David Sterba <dsterba@suse.cz>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index ff1cc0399b9a..a9b56e32dd8d 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -299,7 +299,7 @@ static int __btrfs_run_defrag_inode(struct btrfs_fs_info *fs_info,
 
 	/* get the inode */
 	key.objectid = defrag->root;
-	btrfs_set_key_type(&key, BTRFS_ROOT_ITEM_KEY);
+	key.type = BTRFS_ROOT_ITEM_KEY;
 	key.offset = (u64)-1;
 
 	index = srcu_read_lock(&fs_info->subvol_srcu);
@@ -311,7 +311,7 @@ static int __btrfs_run_defrag_inode(struct btrfs_fs_info *fs_info,
 	}
 
 	key.objectid = defrag->ino;
-	btrfs_set_key_type(&key, BTRFS_INODE_ITEM_KEY);
+	key.type = BTRFS_INODE_ITEM_KEY;
 	key.offset = 0;
 	inode = btrfs_iget(fs_info->sb, &key, inode_root, NULL);
 	if (IS_ERR(inode)) {

commit 49dae1bc1c665817e434d01eefaa11967f618243
Author: Filipe Manana <fdmanana@suse.com>
Date:   Sat Sep 6 22:34:39 2014 +0100

    Btrfs: fix fsync data loss after a ranged fsync
    
    While we're doing a full fsync (when the inode has the flag
    BTRFS_INODE_NEEDS_FULL_SYNC set) that is ranged too (covers only a
    portion of the file), we might have ordered operations that are started
    before or while we're logging the inode and that fall outside the fsync
    range.
    
    Therefore when a full ranged fsync finishes don't remove every extent
    map from the list of modified extent maps - as for some of them, that
    fall outside our fsync range, their respective ordered operation hasn't
    finished yet, meaning the corresponding file extent item wasn't inserted
    into the fs/subvol tree yet and therefore we didn't log it, and we must
    let the next fast fsync (one that checks only the modified list) see this
    extent map and log a matching file extent item to the log btree and wait
    for its ordered operation to finish (if it's still ongoing).
    
    A test case for xfstests follows.
    
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 36861b7a6757..ff1cc0399b9a 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1966,7 +1966,7 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 
 	btrfs_init_log_ctx(&ctx);
 
-	ret = btrfs_log_dentry_safe(trans, root, dentry, &ctx);
+	ret = btrfs_log_dentry_safe(trans, root, dentry, start, end, &ctx);
 	if (ret < 0) {
 		/* Fallthrough and commit/free transaction. */
 		ret = 1;

commit f6dc45c7a93a011dff6eb9b2ffda59c390c7705a
Author: Chris Mason <clm@fb.com>
Date:   Wed Aug 20 07:15:33 2014 -0700

    Btrfs: fix filemap_flush call in btrfs_file_release
    
    We should only be flushing on close if the file was flagged as needing
    it during truncate.  I broke this with my ordered data vs transaction
    commit deadlock fix.
    
    Thanks to Miao Xie for catching this.
    
    Signed-off-by: Chris Mason <clm@fb.com>
    Reported-by: Miao Xie <miaox@cn.fujitsu.com>
    Reported-by: Fengguang Wu <fengguang.wu@intel.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index f15c13f97018..36861b7a6757 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1840,7 +1840,15 @@ int btrfs_release_file(struct inode *inode, struct file *filp)
 {
 	if (filp->private_data)
 		btrfs_ioctl_trans_end(filp);
-	filemap_flush(inode->i_mapping);
+	/*
+	 * ordered_data_close is set by settattr when we are about to truncate
+	 * a file from a non-zero size to a zero size.  This tries to
+	 * flush down new bytes that may have been written if the
+	 * application were using truncate to replace a file in place.
+	 */
+	if (test_and_clear_bit(BTRFS_INODE_ORDERED_DATA_CLOSE,
+			       &BTRFS_I(inode)->runtime_flags))
+			filemap_flush(inode->i_mapping);
 	return 0;
 }
 

commit 51f395ad4058883e4273b02fdebe98072dbdc0d2
Author: Qu Wenruo <quwenruo@cn.fujitsu.com>
Date:   Fri Aug 8 13:06:20 2014 +0800

    btrfs: Use right extent length when inserting overlap extent map.
    
    When current btrfs finds that a new extent map is going to be insereted
    but failed with -EEXIST, it will try again to insert the extent map
    but with the length of sectorsize.
    This is OK if we don't enable 'no-holes' feature since all extent space
    is continuous, we will not go into the not found->insert routine.
    
    But if we enable 'no-holes' feature, it will make things out of control.
    e.g. in 4K sectorsize, we pass the following args to btrfs_get_extent():
    btrfs_get_extent() args: start:  27874 len 4100
    28672             27874         28672   27874+4100      32768
                        |-----------------------|
    |---------hole--------------------|---------data----------|
    
    1) not found and insert
    Since no extent map containing the range, btrfs_get_extent() will go
    into the not_found and insert routine, which will try to insert the
    extent map (27874, 27847 + 4100).
    
    2) first overlap
    But it overlaps with (28672, 32768) extent, so -EEXIST will be returned
    by add_extent_mapping().
    
    3) retry but still overlap
    After catching the -EEXIST, then btrfs_get_extent() will try insert it
    again but with 4K length, which still overlaps, so -EEXIST will be
    returned.
    
    This makes the following patch fail to punch hole.
    d77815461f047e561f77a07754ae923ade597d4e btrfs: Avoid trucating page or punching hole in a already existed hole.
    
    This patch will use the right length, which is the (exsisting->start -
    em->start) to insert, making the above patch works in 'no-holes' mode.
    Also, some small code style problems in above patch is fixed too.
    
    Reported-by: Filipe David Manana <fdmanana@gmail.com>
    Signed-off-by: Qu Wenruo <quwenruo@cn.fujitsu.com>
    Reviewed-by: Filipe David Manana <fdmanana@suse.com>
    Tested-by: Filipe David Manana <fdmanana@suse.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 77e33534c7d6..f15c13f97018 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2215,7 +2215,7 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 		goto out_only_mutex;
 	}
 
-	lockstart = round_up(offset , BTRFS_I(inode)->root->sectorsize);
+	lockstart = round_up(offset, BTRFS_I(inode)->root->sectorsize);
 	lockend = round_down(offset + len,
 			     BTRFS_I(inode)->root->sectorsize) - 1;
 	same_page = ((offset >> PAGE_CACHE_SHIFT) ==
@@ -2276,7 +2276,7 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 						tail_start + tail_len, 0, 1);
 				if (ret)
 					goto out_only_mutex;
-				}
+			}
 		}
 	}
 

commit 1707e26d6ab05c477a91d260e31fda7c6c38588e
Author: chandan <chandan@linux.vnet.ibm.com>
Date:   Tue Jul 1 12:04:28 2014 +0530

    Btrfs: fill_holes: Fix slot number passed to hole_mergeable() call.
    
    For a non-existent key, btrfs_search_slot() sets path->slots[0] to the slot
    where the key could have been present, which in this case would be the slot
    containing the extent item which would be the next neighbor of the file range
    being punched. The current code passes an incremented path->slots[0] and we
    skip to the wrong file extent item. This would mean that we would fail to
    merge the "yet to be created" hole with the next neighboring hole (if one
    exists). Fix this.
    
    Signed-off-by: Chandan Rajendra <chandan@linux.vnet.ibm.com>
    Reviewed-by: Wang Shilong <wangsl.fnst@cn.fujitsu.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index d3afac292d67..77e33534c7d6 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2088,10 +2088,9 @@ static int fill_holes(struct btrfs_trans_handle *trans, struct inode *inode,
 		goto out;
 	}
 
-	if (hole_mergeable(inode, leaf, path->slots[0]+1, offset, end)) {
+	if (hole_mergeable(inode, leaf, path->slots[0], offset, end)) {
 		u64 num_bytes;
 
-		path->slots[0]++;
 		key.offset = offset;
 		btrfs_set_item_key_safe(root, path, &key);
 		fi = btrfs_item_ptr(leaf, path->slots[0],

commit 8d875f95da43c6a8f18f77869f2ef26e9594fecc
Author: Chris Mason <clm@fb.com>
Date:   Tue Aug 12 10:47:42 2014 -0700

    btrfs: disable strict file flushes for renames and truncates
    
    Truncates and renames are often used to replace old versions of a file
    with new versions.  Applications often expect this to be an atomic
    replacement, even if they haven't done anything to make sure the new
    version is fully on disk.
    
    Btrfs has strict flushing in place to make sure that renaming over an
    old file with a new file will fully flush out the new file before
    allowing the transaction commit with the rename to complete.
    
    This ordering means the commit code needs to be able to lock file pages,
    and there are a few paths in the filesystem where we will try to end a
    transaction with the page lock held.  It's rare, but these things can
    deadlock.
    
    This patch removes the ordered flushes and switches to a best effort
    filemap_flush like ext4 uses. It's not perfect, but it should fix the
    deadlocks.
    
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 1f2b99cb55ea..d3afac292d67 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1838,33 +1838,9 @@ static ssize_t btrfs_file_write_iter(struct kiocb *iocb,
 
 int btrfs_release_file(struct inode *inode, struct file *filp)
 {
-	/*
-	 * ordered_data_close is set by settattr when we are about to truncate
-	 * a file from a non-zero size to a zero size.  This tries to
-	 * flush down new bytes that may have been written if the
-	 * application were using truncate to replace a file in place.
-	 */
-	if (test_and_clear_bit(BTRFS_INODE_ORDERED_DATA_CLOSE,
-			       &BTRFS_I(inode)->runtime_flags)) {
-		struct btrfs_trans_handle *trans;
-		struct btrfs_root *root = BTRFS_I(inode)->root;
-
-		/*
-		 * We need to block on a committing transaction to keep us from
-		 * throwing a ordered operation on to the list and causing
-		 * something like sync to deadlock trying to flush out this
-		 * inode.
-		 */
-		trans = btrfs_start_transaction(root, 0);
-		if (IS_ERR(trans))
-			return PTR_ERR(trans);
-		btrfs_add_ordered_operation(trans, BTRFS_I(inode)->root, inode);
-		btrfs_end_transaction(trans, root);
-		if (inode->i_size > BTRFS_ORDERED_OPERATIONS_FLUSH_LIMIT)
-			filemap_flush(inode->i_mapping);
-	}
 	if (filp->private_data)
 		btrfs_ioctl_trans_end(filp);
+	filemap_flush(inode->i_mapping);
 	return 0;
 }
 

commit 16b9057804c02e2d351e9c8f606e909b43cbd9e7
Merge: 5c02c392cd23 c2338f2dc7c1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jun 12 10:30:18 2014 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs
    
    Pull vfs updates from Al Viro:
     "This the bunch that sat in -next + lock_parent() fix.  This is the
      minimal set; there's more pending stuff.
    
      In particular, I really hope to get acct.c fixes merged this cycle -
      we need that to deal sanely with delayed-mntput stuff.  In the next
      pile, hopefully - that series is fairly short and localized
      (kernel/acct.c, fs/super.c and fs/namespace.c).  In this pile: more
      iov_iter work.  Most of prereqs for ->splice_write with sane locking
      order are there and Kent's dio rewrite would also fit nicely on top of
      this pile"
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs: (70 commits)
      lock_parent: don't step on stale ->d_parent of all-but-freed one
      kill generic_file_splice_write()
      ceph: switch to iter_file_splice_write()
      shmem: switch to iter_file_splice_write()
      nfs: switch to iter_splice_write_file()
      fs/splice.c: remove unneeded exports
      ocfs2: switch to iter_file_splice_write()
      ->splice_write() via ->write_iter()
      bio_vec-backed iov_iter
      optimize copy_page_{to,from}_iter()
      bury generic_file_aio_{read,write}
      lustre: get rid of messing with iovecs
      ceph: switch to ->write_iter()
      ceph_sync_direct_write: stop poking into iov_iter guts
      ceph_sync_read: stop poking into iov_iter guts
      new helper: copy_page_from_iter()
      fuse: switch to ->write_iter()
      btrfs: switch to ->write_iter()
      ocfs2: switch to ->write_iter()
      xfs: switch to ->write_iter()
      ...

commit 859862ddd2b6b8dee00498c015ab37f02474b442
Merge: 412dd3a6daf0 c7548af69d9e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 11 09:22:21 2014 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs
    
    Pull btrfs updates from Chris Mason:
     "The biggest change here is Josef's rework of the btrfs quota
      accounting, which improves the in-memory tracking of delayed extent
      operations.
    
      I had been working on Btrfs stack usage for a while, mostly because it
      had become impossible to do long stress runs with slab, lockdep and
      pagealloc debugging turned on without blowing the stack.  Even though
      you upgraded us to a nice king sized stack, I kept most of the
      patches.
    
      We also have some very hard to find corruption fixes, an awesome sysfs
      use after free, and the usual assortment of optimizations, cleanups
      and other fixes"
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs: (80 commits)
      Btrfs: convert smp_mb__{before,after}_clear_bit
      Btrfs: fix scrub_print_warning to handle skinny metadata extents
      Btrfs: make fsync work after cloning into a file
      Btrfs: use right type to get real comparison
      Btrfs: don't check nodes for extent items
      Btrfs: don't release invalid page in btrfs_page_exists_in_range()
      Btrfs: make sure we retry if page is a retriable exception
      Btrfs: make sure we retry if we couldn't get the page
      btrfs: replace EINVAL with EOPNOTSUPP for dev_replace raid56
      trivial: fs/btrfs/ioctl.c: fix typo s/substract/subtract/
      Btrfs: fix leaf corruption after __btrfs_drop_extents
      Btrfs: ensure btrfs_prev_leaf doesn't miss 1 item
      Btrfs: fix clone to deal with holes when NO_HOLES feature is enabled
      btrfs: free delayed node outside of root->inode_lock
      btrfs: replace EINVAL with ERANGE for resize when ULLONG_MAX
      Btrfs: fix transaction leak during fsync call
      btrfs: Avoid trucating page or punching hole in a already existed hole.
      Btrfs: update commit root on snapshot creation after orphan cleanup
      Btrfs: ioctl, don't re-lock extent range when not necessary
      Btrfs: avoid visiting all extent items when cloning a range
      ...

commit b05fd8742f6291b67571ad0fdad4da6b6eb98025
Author: Filipe Manana <fdmanana@gmail.com>
Date:   Thu May 29 23:31:39 2014 +0100

    Btrfs: fix transaction leak during fsync call
    
    If btrfs_log_dentry_safe() returns an error, we set ret to 1 and
    fall through with the goal of committing the transaction. However,
    in the case where the inode doesn't need a full sync, we would call
    btrfs_wait_ordered_range() against the target range for our inode,
    and if it returned an error, we would return without commiting or
    ending the transaction.
    
    Signed-off-by: Filipe David Borba Manana <fdmanana@gmail.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index eb3f2708a01d..ad7c05909a49 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2025,8 +2025,10 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 		if (!full_sync) {
 			ret = btrfs_wait_ordered_range(inode, start,
 						       end - start + 1);
-			if (ret)
+			if (ret) {
+				btrfs_end_transaction(trans, root);
 				goto out;
+			}
 		}
 		ret = btrfs_commit_transaction(trans, root);
 	} else {

commit d77815461f047e561f77a07754ae923ade597d4e
Author: Qu Wenruo <quwenruo@cn.fujitsu.com>
Date:   Fri May 30 15:16:10 2014 +0800

    btrfs: Avoid trucating page or punching hole in a already existed hole.
    
    btrfs_punch_hole() will truncate unaligned pages or punch hole on a
    already existed hole.
    This will cause unneeded zero page or holes splitting the original huge
    hole.
    
    This patch will skip already existed holes before any page truncating or
    hole punching.
    
    Signed-off-by: Qu Wenruo <quwenruo@cn.fujitsu.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index e46bfaf6cde2..eb3f2708a01d 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2184,6 +2184,37 @@ static int fill_holes(struct btrfs_trans_handle *trans, struct inode *inode,
 	return 0;
 }
 
+/*
+ * Find a hole extent on given inode and change start/len to the end of hole
+ * extent.(hole/vacuum extent whose em->start <= start &&
+ *	   em->start + em->len > start)
+ * When a hole extent is found, return 1 and modify start/len.
+ */
+static int find_first_non_hole(struct inode *inode, u64 *start, u64 *len)
+{
+	struct extent_map *em;
+	int ret = 0;
+
+	em = btrfs_get_extent(inode, NULL, 0, *start, *len, 0);
+	if (IS_ERR_OR_NULL(em)) {
+		if (!em)
+			ret = -ENOMEM;
+		else
+			ret = PTR_ERR(em);
+		return ret;
+	}
+
+	/* Hole or vacuum extent(only exists in no-hole mode) */
+	if (em->block_start == EXTENT_MAP_HOLE) {
+		ret = 1;
+		*len = em->start + em->len > *start + *len ?
+		       0 : *start + *len - em->start - em->len;
+		*start = em->start + em->len;
+	}
+	free_extent_map(em);
+	return ret;
+}
+
 static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 {
 	struct btrfs_root *root = BTRFS_I(inode)->root;
@@ -2191,17 +2222,18 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 	struct btrfs_path *path;
 	struct btrfs_block_rsv *rsv;
 	struct btrfs_trans_handle *trans;
-	u64 lockstart = round_up(offset, BTRFS_I(inode)->root->sectorsize);
-	u64 lockend = round_down(offset + len,
-				 BTRFS_I(inode)->root->sectorsize) - 1;
-	u64 cur_offset = lockstart;
+	u64 lockstart;
+	u64 lockend;
+	u64 tail_start;
+	u64 tail_len;
+	u64 orig_start = offset;
+	u64 cur_offset;
 	u64 min_size = btrfs_calc_trunc_metadata_size(root, 1);
 	u64 drop_end;
 	int ret = 0;
 	int err = 0;
 	int rsv_count;
-	bool same_page = ((offset >> PAGE_CACHE_SHIFT) ==
-			  ((offset + len - 1) >> PAGE_CACHE_SHIFT));
+	bool same_page;
 	bool no_holes = btrfs_fs_incompat(root->fs_info, NO_HOLES);
 	u64 ino_size;
 
@@ -2211,6 +2243,21 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 
 	mutex_lock(&inode->i_mutex);
 	ino_size = round_up(inode->i_size, PAGE_CACHE_SIZE);
+	ret = find_first_non_hole(inode, &offset, &len);
+	if (ret < 0)
+		goto out_only_mutex;
+	if (ret && !len) {
+		/* Already in a large hole */
+		ret = 0;
+		goto out_only_mutex;
+	}
+
+	lockstart = round_up(offset , BTRFS_I(inode)->root->sectorsize);
+	lockend = round_down(offset + len,
+			     BTRFS_I(inode)->root->sectorsize) - 1;
+	same_page = ((offset >> PAGE_CACHE_SHIFT) ==
+		    ((offset + len - 1) >> PAGE_CACHE_SHIFT));
+
 	/*
 	 * We needn't truncate any page which is beyond the end of the file
 	 * because we are sure there is no data there.
@@ -2222,8 +2269,7 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 	if (same_page && len < PAGE_CACHE_SIZE) {
 		if (offset < ino_size)
 			ret = btrfs_truncate_page(inode, offset, len, 0);
-		mutex_unlock(&inode->i_mutex);
-		return ret;
+		goto out_only_mutex;
 	}
 
 	/* zero back part of the first page */
@@ -2235,12 +2281,39 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 		}
 	}
 
-	/* zero the front end of the last page */
-	if (offset + len < ino_size) {
-		ret = btrfs_truncate_page(inode, offset + len, 0, 1);
-		if (ret) {
-			mutex_unlock(&inode->i_mutex);
-			return ret;
+	/* Check the aligned pages after the first unaligned page,
+	 * if offset != orig_start, which means the first unaligned page
+	 * including serveral following pages are already in holes,
+	 * the extra check can be skipped */
+	if (offset == orig_start) {
+		/* after truncate page, check hole again */
+		len = offset + len - lockstart;
+		offset = lockstart;
+		ret = find_first_non_hole(inode, &offset, &len);
+		if (ret < 0)
+			goto out_only_mutex;
+		if (ret && !len) {
+			ret = 0;
+			goto out_only_mutex;
+		}
+		lockstart = offset;
+	}
+
+	/* Check the tail unaligned part is in a hole */
+	tail_start = lockend + 1;
+	tail_len = offset + len - tail_start;
+	if (tail_len) {
+		ret = find_first_non_hole(inode, &tail_start, &tail_len);
+		if (unlikely(ret < 0))
+			goto out_only_mutex;
+		if (!ret) {
+			/* zero the front end of the last page */
+			if (tail_start + tail_len < ino_size) {
+				ret = btrfs_truncate_page(inode,
+						tail_start + tail_len, 0, 1);
+				if (ret)
+					goto out_only_mutex;
+				}
 		}
 	}
 
@@ -2314,6 +2387,8 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 	BUG_ON(ret);
 	trans->block_rsv = rsv;
 
+	cur_offset = lockstart;
+	len = lockend - cur_offset;
 	while (cur_offset < lockend) {
 		ret = __btrfs_drop_extents(trans, root, inode, path,
 					   cur_offset, lockend + 1,
@@ -2354,6 +2429,14 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 					      rsv, min_size);
 		BUG_ON(ret);	/* shouldn't happen */
 		trans->block_rsv = rsv;
+
+		ret = find_first_non_hole(inode, &cur_offset, &len);
+		if (unlikely(ret < 0))
+			break;
+		if (ret && !len) {
+			ret = 0;
+			break;
+		}
 	}
 
 	if (ret) {
@@ -2392,6 +2475,7 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 out:
 	unlock_extent_cached(&BTRFS_I(inode)->io_tree, lockstart, lockend,
 			     &cached_state, GFP_NOFS);
+out_only_mutex:
 	mutex_unlock(&inode->i_mutex);
 	if (ret && !err)
 		err = ret;

commit fc4adbff823f76577ece26dcb88bf6f8392dbd43
Author: Alex Gartrell <agartrell@fb.com>
Date:   Tue May 20 13:07:56 2014 -0700

    btrfs: Drop EXTENT_UPTODATE check in hole punching and direct locking
    
    In these instances, we are trying to determine if a page has been accessed
    since we began the operation for the sake of retry.  This is easily
    accomplished by doing a gang lookup in the page mapping radix tree, and it
    saves us the dependency on the flag (so that we might eventually delete
    it).
    
    btrfs_page_exists_in_range borrows heavily from find_get_page, replacing
    the radix tree look up with a gang lookup of 1, so that we can find the
    next highest page >= index and see if it falls into our lock range.
    
    Signed-off-by: Chris Mason <clm@fb.com>
    Signed-off-by: Alex Gartrell <agartrell@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 8accf94ef220..e46bfaf6cde2 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2266,9 +2266,7 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 		if ((!ordered ||
 		    (ordered->file_offset + ordered->len <= lockstart ||
 		     ordered->file_offset > lockend)) &&
-		     !test_range_bit(&BTRFS_I(inode)->io_tree, lockstart,
-				     lockend, EXTENT_UPTODATE, 0,
-				     cached_state)) {
+		     !btrfs_page_exists_in_range(inode, lockstart, lockend)) {
 			if (ordered)
 				btrfs_put_ordered_extent(ordered);
 			break;

commit fcebe4562dec83b3f8d3088d77584727b09130b2
Author: Josef Bacik <jbacik@fb.com>
Date:   Tue May 13 17:30:47 2014 -0700

    Btrfs: rework qgroup accounting
    
    Currently qgroups account for space by intercepting delayed ref updates to fs
    trees.  It does this by adding sequence numbers to delayed ref updates so that
    it can figure out how the tree looked before the update so we can adjust the
    counters properly.  The problem with this is that it does not allow delayed refs
    to be merged, so if you say are defragging an extent with 5k snapshots pointing
    to it we will thrash the delayed ref lock because we need to go back and
    manually merge these things together.  Instead we want to process quota changes
    when we know they are going to happen, like when we first allocate an extent, we
    free a reference for an extent, we add new references etc.  This patch
    accomplishes this by only adding qgroup operations for real ref changes.  We
    only modify the sequence number when we need to lookup roots for bytenrs, this
    reduces the amount of churn on the sequence number and allows us to merge
    delayed refs as we add them most of the time.  This patch encompasses a bunch of
    architectural changes
    
    1) qgroup ref operations: instead of tracking qgroup operations through the
    delayed refs we simply add new ref operations whenever we notice that we need to
    when we've modified the refs themselves.
    
    2) tree mod seq:  we no longer have this separation of major/minor counters.
    this makes the sequence number stuff much more sane and we can remove some
    locking that was needed to protect the counter.
    
    3) delayed ref seq: we now read the tree mod seq number and use that as our
    sequence.  This means each new delayed ref doesn't have it's own unique sequence
    number, rather whenever we go to lookup backrefs we inc the sequence number so
    we can make sure to keep any new operations from screwing up our world view at
    that given point.  This allows us to merge delayed refs during runtime.
    
    With all of these changes the delayed ref stuff is a little saner and the qgroup
    accounting stuff no longer goes negative in some cases like it was before.
    Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 5c6947dbc948..8accf94ef220 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -40,6 +40,7 @@
 #include "tree-log.h"
 #include "locking.h"
 #include "volumes.h"
+#include "qgroup.h"
 
 static struct kmem_cache *btrfs_inode_defrag_cachep;
 /*
@@ -849,7 +850,7 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 						disk_bytenr, num_bytes, 0,
 						root->root_key.objectid,
 						new_key.objectid,
-						start - extent_offset, 0);
+						start - extent_offset, 1);
 				BUG_ON(ret); /* -ENOMEM */
 			}
 			key.offset = start;
@@ -1206,7 +1207,7 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 
 		ret = btrfs_inc_extent_ref(trans, root, bytenr, num_bytes, 0,
 					   root->root_key.objectid,
-					   ino, orig_offset, 0);
+					   ino, orig_offset, 1);
 		BUG_ON(ret); /* -ENOMEM */
 
 		if (split == start) {

commit 27cdeb7096b86f05ad018a24cdb63acdf0850a5d
Author: Miao Xie <miaox@cn.fujitsu.com>
Date:   Wed Apr 2 19:51:05 2014 +0800

    Btrfs: use bitfield instead of integer data type for the some variants in btrfs_root
    
    Signed-off-by: Miao Xie <miaox@cn.fujitsu.com>
    Signed-off-by: Wang Shilong <wangsl.fnst@cn.fujitsu.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 3029925e96d7..5c6947dbc948 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -714,7 +714,7 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 	int recow;
 	int ret;
 	int modify_tree = -1;
-	int update_refs = (root->ref_cows || root == root->fs_info->tree_root);
+	int update_refs;
 	int found = 0;
 	int leafs_visited = 0;
 
@@ -724,6 +724,8 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 	if (start >= BTRFS_I(inode)->disk_i_size && !replace_extent)
 		modify_tree = 0;
 
+	update_refs = (test_bit(BTRFS_ROOT_REF_COWS, &root->state) ||
+		       root == root->fs_info->tree_root);
 	while (1) {
 		recow = 0;
 		ret = btrfs_lookup_file_extent(trans, root, path, ino,

commit fc19c5e73645f95d3eca12b4e91e7b56faf1e4a4
Author: Filipe Manana <fdmanana@gmail.com>
Date:   Tue Apr 29 13:18:40 2014 +0100

    Btrfs: fix leaf corruption caused by ENOSPC while hole punching
    
    While running a stress test with multiple threads writing to the same btrfs
    file system, I ended up with a situation where a leaf was corrupted in that
    it had 2 file extent item keys that had the same exact key. I was able to
    detect this quickly thanks to the following patch which triggers an assertion
    as soon as a leaf is marked dirty if there are duplicated keys or out of order
    keys:
    
        Btrfs: check if items are ordered when a leaf is marked dirty
        (https://patchwork.kernel.org/patch/3955431/)
    
    Basically while running the test, I got the following in dmesg:
    
        [28877.415877] WARNING: CPU: 2 PID: 10706 at fs/btrfs/file.c:553 btrfs_drop_extent_cache+0x435/0x440 [btrfs]()
        (...)
        [28877.415917] Call Trace:
        [28877.415922]  [<ffffffff816f1189>] dump_stack+0x4e/0x68
        [28877.415926]  [<ffffffff8104a32c>] warn_slowpath_common+0x8c/0xc0
        [28877.415929]  [<ffffffff8104a37a>] warn_slowpath_null+0x1a/0x20
        [28877.415944]  [<ffffffffa03775a5>] btrfs_drop_extent_cache+0x435/0x440 [btrfs]
        [28877.415949]  [<ffffffff8118e7be>] ? kmem_cache_alloc+0xfe/0x1c0
        [28877.415962]  [<ffffffffa03777d9>] fill_holes+0x229/0x3e0 [btrfs]
        [28877.415972]  [<ffffffffa0345865>] ? block_rsv_add_bytes+0x55/0x80 [btrfs]
        [28877.415984]  [<ffffffffa03792cb>] btrfs_fallocate+0xb6b/0xc20 [btrfs]
        (...)
        [29854.132560] BTRFS critical (device sdc): corrupt leaf, bad key order: block=955232256,root=1, slot=24
        [29854.132565] BTRFS info (device sdc): leaf 955232256 total ptrs 40 free space 778
        (...)
        [29854.132637]      item 23 key (3486 108 667648) itemoff 2694 itemsize 53
        [29854.132638]              extent data disk bytenr 14574411776 nr 286720
        [29854.132639]              extent data offset 0 nr 286720 ram 286720
        [29854.132640]      item 24 key (3486 108 954368) itemoff 2641 itemsize 53
        [29854.132641]              extent data disk bytenr 0 nr 0
        [29854.132643]              extent data offset 0 nr 0 ram 0
        [29854.132644]      item 25 key (3486 108 954368) itemoff 2588 itemsize 53
        [29854.132645]              extent data disk bytenr 8699670528 nr 77824
        [29854.132646]              extent data offset 0 nr 77824 ram 77824
        [29854.132647]      item 26 key (3486 108 1146880) itemoff 2535 itemsize 53
        [29854.132648]              extent data disk bytenr 8699670528 nr 77824
        [29854.132649]              extent data offset 0 nr 77824 ram 77824
        (...)
        [29854.132707] kernel BUG at fs/btrfs/ctree.h:3901!
        (...)
        [29854.132771] Call Trace:
        [29854.132779]  [<ffffffffa0342b5c>] setup_items_for_insert+0x2dc/0x400 [btrfs]
        [29854.132791]  [<ffffffffa0378537>] __btrfs_drop_extents+0xba7/0xdd0 [btrfs]
        [29854.132794]  [<ffffffff8109c0d6>] ? trace_hardirqs_on_caller+0x16/0x1d0
        [29854.132797]  [<ffffffff8109c29d>] ? trace_hardirqs_on+0xd/0x10
        [29854.132800]  [<ffffffff8118e7be>] ? kmem_cache_alloc+0xfe/0x1c0
        [29854.132810]  [<ffffffffa036783b>] insert_reserved_file_extent.constprop.66+0xab/0x310 [btrfs]
        [29854.132820]  [<ffffffffa036a6c6>] __btrfs_prealloc_file_range+0x116/0x340 [btrfs]
        [29854.132830]  [<ffffffffa0374d53>] btrfs_prealloc_file_range+0x23/0x30 [btrfs]
        (...)
    
    So this is caused by getting an -ENOSPC error while punching a file hole, more
    specifically, we get -ENOSPC error from __btrfs_drop_extents in the while loop
    of file.c:btrfs_punch_hole() when it's unable to modify the btree to delete one
    or more file extent items due to lack of enough free space. When this happens,
    in btrfs_punch_hole(), we attempt to reclaim free space by switching our transaction
    block reservation object to root->fs_info->trans_block_rsv, end our transaction and
    start a new transaction basically - and, we keep increasing our current offset
    (cur_offset) as long as it's smaller than the end of the target range (lockend) -
    this makes use leave the loop with cur_offset == drop_end which in turn makes us
    call fill_holes() for inserting a file extent item that represents a 0 bytes range
    hole (and this insertion succeeds, as in the meanwhile more space became available).
    
    This 0 bytes file hole extent item is a problem because any subsequent caller of
    __btrfs_drop_extents (regular file writes, or fallocate calls for e.g.), with a
    start file offset that is equal to the offset of the hole, will not remove this
    extent item due to the following conditional in the while loop of
    __btrfs_drop_extents:
    
        if (extent_end <= search_start) {
                path->slots[0]++;
                goto next_slot;
        }
    
    This later makes the call to setup_items_for_insert() (at the very end of
    __btrfs_drop_extents), insert a new file extent item with the same offset as
    the 0 bytes file hole extent item that follows it. Needless is to say that this
    causes chaos, either when reading the leaf from disk (btree_readpage_end_io_hook),
    where we perform leaf sanity checks or in subsequent operations that manipulate
    file extent items, as in the fallocate call as shown by the dmesg trace above.
    
    Without my other patch to perform the leaf sanity checks once a leaf is marked
    as dirty (if the integrity checker is enabled), it would have been much harder
    to debug this issue.
    
    This change might fix a few similar issues reported by users in the mailing
    list regarding assertion failures in btrfs_set_item_key_safe calls performed
    by __btrfs_drop_extents, such as the following report:
    
        http://comments.gmane.org/gmane.comp.file-systems.btrfs/32938
    
    Asking fill_holes() to create a 0 bytes wide file hole item also produced the
    first warning in the trace above, as we passed a range to btrfs_drop_extent_cache
    that has an end smaller (by -1) than its start.
    
    On 3.14 kernels this issue manifests itself through leaf corruption, as we get
    duplicated file extent item keys in a leaf when calling setup_items_for_insert(),
    but on older kernels, setup_items_for_insert() isn't called by __btrfs_drop_extents(),
    instead we have callers of __btrfs_drop_extents(), namely the functions
    inode.c:insert_inline_extent() and inode.c:insert_reserved_file_extent(), calling
    btrfs_insert_empty_item() to insert the new file extent item, which would fail with
    error -EEXIST, instead of inserting a duplicated key - which is still a serious
    issue as it would make all similar file extent item replace operations keep
    failing if they target the same file range.
    
    Cc: stable@vger.kernel.org
    Signed-off-by: Filipe David Borba Manana <fdmanana@gmail.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index e4ff2d52ea23..3029925e96d7 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -780,6 +780,18 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 			extent_end = search_start;
 		}
 
+		/*
+		 * Don't skip extent items representing 0 byte lengths. They
+		 * used to be created (bug) if while punching holes we hit
+		 * -ENOSPC condition. So if we find one here, just ensure we
+		 * delete it, otherwise we would insert a new file extent item
+		 * with the same key (offset) as that 0 bytes length file
+		 * extent item in the call to setup_items_for_insert() later
+		 * in this function.
+		 */
+		if (extent_end == key.offset && extent_end >= search_start)
+			goto delete_extent_item;
+
 		if (extent_end <= search_start) {
 			path->slots[0]++;
 			goto next_slot;
@@ -893,6 +905,7 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 		 *    | ------ extent ------ |
 		 */
 		if (start <= key.offset && end >= extent_end) {
+delete_extent_item:
 			if (del_nr == 0) {
 				del_slot = path->slots[0];
 				del_nr = 1;
@@ -2348,7 +2361,12 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 	}
 
 	trans->block_rsv = &root->fs_info->trans_block_rsv;
-	if (cur_offset < ino_size) {
+	/*
+	 * Don't insert file hole extent item if it's for a range beyond eof
+	 * (because it's useless) or if it represents a 0 bytes range (when
+	 * cur_offset == drop_end).
+	 */
+	if (cur_offset < ino_size && cur_offset < drop_end) {
 		ret = fill_holes(trans, inode, path, cur_offset, drop_end);
 		if (ret) {
 			err = ret;

commit a1a50f60a6bf4f861eb94793420274bc1ccd409a
Author: Filipe Manana <fdmanana@gmail.com>
Date:   Sat Apr 26 01:35:31 2014 +0100

    Btrfs: read inode size after acquiring the mutex when punching a hole
    
    In a previous change, commit 12870f1c9b2de7d475d22e73fd7db1b418599725,
    I accidentally moved the roundup of inode->i_size to outside of the
    critical section delimited by the inode mutex, which is not atomic and
    not correct since the size can be changed by other task before we acquire
    the mutex. Therefore fix it.
    
    Signed-off-by: Filipe David Borba Manana <fdmanana@gmail.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index ae6af072b635..e4ff2d52ea23 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2187,13 +2187,14 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 	bool same_page = ((offset >> PAGE_CACHE_SHIFT) ==
 			  ((offset + len - 1) >> PAGE_CACHE_SHIFT));
 	bool no_holes = btrfs_fs_incompat(root->fs_info, NO_HOLES);
-	u64 ino_size = round_up(inode->i_size, PAGE_CACHE_SIZE);
+	u64 ino_size;
 
 	ret = btrfs_wait_ordered_range(inode, offset, len);
 	if (ret)
 		return ret;
 
 	mutex_lock(&inode->i_mutex);
+	ino_size = round_up(inode->i_size, PAGE_CACHE_SIZE);
 	/*
 	 * We needn't truncate any page which is beyond the end of the file
 	 * because we are sure there is no data there.

commit 2457aec63745e235bcafb7ef312b182d8682f0fc
Author: Mel Gorman <mgorman@suse.de>
Date:   Wed Jun 4 16:10:31 2014 -0700

    mm: non-atomically mark page accessed during page cache allocation where possible
    
    aops->write_begin may allocate a new page and make it visible only to have
    mark_page_accessed called almost immediately after.  Once the page is
    visible the atomic operations are necessary which is noticable overhead
    when writing to an in-memory filesystem like tmpfs but should also be
    noticable with fast storage.  The objective of the patch is to initialse
    the accessed information with non-atomic operations before the page is
    visible.
    
    The bulk of filesystems directly or indirectly use
    grab_cache_page_write_begin or find_or_create_page for the initial
    allocation of a page cache page.  This patch adds an init_page_accessed()
    helper which behaves like the first call to mark_page_accessed() but may
    called before the page is visible and can be done non-atomically.
    
    The primary APIs of concern in this care are the following and are used
    by most filesystems.
    
            find_get_page
            find_lock_page
            find_or_create_page
            grab_cache_page_nowait
            grab_cache_page_write_begin
    
    All of them are very similar in detail to the patch creates a core helper
    pagecache_get_page() which takes a flags parameter that affects its
    behavior such as whether the page should be marked accessed or not.  Then
    old API is preserved but is basically a thin wrapper around this core
    function.
    
    Each of the filesystems are then updated to avoid calling
    mark_page_accessed when it is known that the VM interfaces have already
    done the job.  There is a slight snag in that the timing of the
    mark_page_accessed() has now changed so in rare cases it's possible a page
    gets to the end of the LRU as PageReferenced where as previously it might
    have been repromoted.  This is expected to be rare but it's worth the
    filesystem people thinking about it in case they see a problem with the
    timing change.  It is also the case that some filesystems may be marking
    pages accessed that previously did not but it makes sense that filesystems
    have consistent behaviour in this regard.
    
    The test case used to evaulate this is a simple dd of a large file done
    multiple times with the file deleted on each iterations.  The size of the
    file is 1/10th physical memory to avoid dirty page balancing.  In the
    async case it will be possible that the workload completes without even
    hitting the disk and will have variable results but highlight the impact
    of mark_page_accessed for async IO.  The sync results are expected to be
    more stable.  The exception is tmpfs where the normal case is for the "IO"
    to not hit the disk.
    
    The test machine was single socket and UMA to avoid any scheduling or NUMA
    artifacts.  Throughput and wall times are presented for sync IO, only wall
    times are shown for async as the granularity reported by dd and the
    variability is unsuitable for comparison.  As async results were variable
    do to writback timings, I'm only reporting the maximum figures.  The sync
    results were stable enough to make the mean and stddev uninteresting.
    
    The performance results are reported based on a run with no profiling.
    Profile data is based on a separate run with oprofile running.
    
    async dd
                                        3.15.0-rc3            3.15.0-rc3
                                           vanilla           accessed-v2
    ext3    Max      elapsed     13.9900 (  0.00%)     11.5900 ( 17.16%)
    tmpfs   Max      elapsed      0.5100 (  0.00%)      0.4900 (  3.92%)
    btrfs   Max      elapsed     12.8100 (  0.00%)     12.7800 (  0.23%)
    ext4    Max      elapsed     18.6000 (  0.00%)     13.3400 ( 28.28%)
    xfs     Max      elapsed     12.5600 (  0.00%)      2.0900 ( 83.36%)
    
    The XFS figure is a bit strange as it managed to avoid a worst case by
    sheer luck but the average figures looked reasonable.
    
            samples percentage
    ext3       86107    0.9783  vmlinux-3.15.0-rc4-vanilla        mark_page_accessed
    ext3       23833    0.2710  vmlinux-3.15.0-rc4-accessed-v3r25 mark_page_accessed
    ext3        5036    0.0573  vmlinux-3.15.0-rc4-accessed-v3r25 init_page_accessed
    ext4       64566    0.8961  vmlinux-3.15.0-rc4-vanilla        mark_page_accessed
    ext4        5322    0.0713  vmlinux-3.15.0-rc4-accessed-v3r25 mark_page_accessed
    ext4        2869    0.0384  vmlinux-3.15.0-rc4-accessed-v3r25 init_page_accessed
    xfs        62126    1.7675  vmlinux-3.15.0-rc4-vanilla        mark_page_accessed
    xfs         1904    0.0554  vmlinux-3.15.0-rc4-accessed-v3r25 init_page_accessed
    xfs          103    0.0030  vmlinux-3.15.0-rc4-accessed-v3r25 mark_page_accessed
    btrfs      10655    0.1338  vmlinux-3.15.0-rc4-vanilla        mark_page_accessed
    btrfs       2020    0.0273  vmlinux-3.15.0-rc4-accessed-v3r25 init_page_accessed
    btrfs        587    0.0079  vmlinux-3.15.0-rc4-accessed-v3r25 mark_page_accessed
    tmpfs      59562    3.2628  vmlinux-3.15.0-rc4-vanilla        mark_page_accessed
    tmpfs       1210    0.0696  vmlinux-3.15.0-rc4-accessed-v3r25 init_page_accessed
    tmpfs         94    0.0054  vmlinux-3.15.0-rc4-accessed-v3r25 mark_page_accessed
    
    [akpm@linux-foundation.org: don't run init_page_accessed() against an uninitialised pointer]
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Theodore Ts'o <tytso@mit.edu>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Tested-by: Prabhakar Lad <prabhakar.csengg@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index ae6af072b635..74272a3f9d9b 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -470,11 +470,12 @@ static void btrfs_drop_pages(struct page **pages, size_t num_pages)
 	for (i = 0; i < num_pages; i++) {
 		/* page checked is some magic around finding pages that
 		 * have been modified without going through btrfs_set_page_dirty
-		 * clear it here
+		 * clear it here. There should be no need to mark the pages
+		 * accessed as prepare_pages should have marked them accessed
+		 * in prepare_pages via find_or_create_page()
 		 */
 		ClearPageChecked(pages[i]);
 		unlock_page(pages[i]);
-		mark_page_accessed(pages[i]);
 		page_cache_release(pages[i]);
 	}
 }

commit b30ac0fc4109701fc122d41ee085c65b52dc44a3
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Thu Apr 3 14:29:04 2014 -0400

    btrfs: switch to ->write_iter()
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 39014d5db9d5..17e7393c50f0 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -447,7 +447,7 @@ static noinline int btrfs_copy_from_user(loff_t pos, int num_pages,
 		write_bytes -= copied;
 		total_copied += copied;
 
-		/* Return to btrfs_file_aio_write to fault page */
+		/* Return to btrfs_file_write_iter to fault page */
 		if (unlikely(copied == 0))
 			break;
 
@@ -1708,9 +1708,8 @@ static void update_time_for_write(struct inode *inode)
 		inode_inc_iversion(inode);
 }
 
-static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
-				    const struct iovec *iov,
-				    unsigned long nr_segs, loff_t pos)
+static ssize_t btrfs_file_write_iter(struct kiocb *iocb,
+				    struct iov_iter *from)
 {
 	struct file *file = iocb->ki_filp;
 	struct inode *inode = file_inode(file);
@@ -1719,15 +1718,12 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 	u64 end_pos;
 	ssize_t num_written = 0;
 	ssize_t err = 0;
-	size_t count;
+	size_t count = iov_iter_count(from);
 	bool sync = (file->f_flags & O_DSYNC) || IS_SYNC(file->f_mapping->host);
-	struct iov_iter i;
+	loff_t pos = iocb->ki_pos;
 
 	mutex_lock(&inode->i_mutex);
 
-	count = iov_length(iov, nr_segs);
-	iov_iter_init(&i, WRITE, iov, nr_segs, count);
-
 	current->backing_dev_info = inode->i_mapping->backing_dev_info;
 	err = generic_write_checks(file, &pos, &count, S_ISBLK(inode->i_mode));
 	if (err) {
@@ -1740,7 +1736,7 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 		goto out;
 	}
 
-	iov_iter_truncate(&i, count);
+	iov_iter_truncate(from, count);
 
 	err = file_remove_suid(file);
 	if (err) {
@@ -1783,9 +1779,9 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 		atomic_inc(&BTRFS_I(inode)->sync_writers);
 
 	if (unlikely(file->f_flags & O_DIRECT)) {
-		num_written = __btrfs_direct_write(iocb, &i, pos);
+		num_written = __btrfs_direct_write(iocb, from, pos);
 	} else {
-		num_written = __btrfs_buffered_write(file, &i, pos);
+		num_written = __btrfs_buffered_write(file, from, pos);
 		if (num_written > 0)
 			iocb->ki_pos = pos + num_written;
 	}
@@ -2623,10 +2619,10 @@ static loff_t btrfs_file_llseek(struct file *file, loff_t offset, int whence)
 const struct file_operations btrfs_file_operations = {
 	.llseek		= btrfs_file_llseek,
 	.read		= new_sync_read,
-	.write		= do_sync_write,
+	.write		= new_sync_write,
 	.read_iter      = generic_file_read_iter,
 	.splice_read	= generic_file_splice_read,
-	.aio_write	= btrfs_file_aio_write,
+	.write_iter	= btrfs_file_write_iter,
 	.mmap		= btrfs_file_mmap,
 	.open		= generic_file_open,
 	.release	= btrfs_release_file,

commit aad4f8bb42af06371aa0e85bf0cd9d52c0494985
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Wed Apr 2 14:33:16 2014 -0400

    switch simple generic_file_aio_read() users to ->read_iter()
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index ea63a51c148c..39014d5db9d5 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2622,9 +2622,9 @@ static loff_t btrfs_file_llseek(struct file *file, loff_t offset, int whence)
 
 const struct file_operations btrfs_file_operations = {
 	.llseek		= btrfs_file_llseek,
-	.read		= do_sync_read,
+	.read		= new_sync_read,
 	.write		= do_sync_write,
-	.aio_read       = generic_file_aio_read,
+	.read_iter      = generic_file_read_iter,
 	.splice_read	= generic_file_splice_read,
 	.aio_write	= btrfs_file_aio_write,
 	.mmap		= btrfs_file_mmap,

commit 0c949334a9e2581646c6ff0d1470a805b1e5be99
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sat Mar 22 06:51:37 2014 -0400

    iov_iter_truncate()
    
    Now It Can Be Done(tm) - we don't need to do iov_shorten() in
    generic_file_direct_write() anymore, now that all ->direct_IO()
    instances are converted to proper iov_iter methods and honour
    iter->count and iter->iov_offset properly.
    
    Get rid of count/ocount arguments of generic_file_direct_write(),
    while we are at it.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index f8cee205618a..ea63a51c148c 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1659,8 +1659,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 
 static ssize_t __btrfs_direct_write(struct kiocb *iocb,
 				    struct iov_iter *from,
-				    loff_t pos,
-				    size_t count, size_t ocount)
+				    loff_t pos)
 {
 	struct file *file = iocb->ki_filp;
 	ssize_t written;
@@ -1668,9 +1667,9 @@ static ssize_t __btrfs_direct_write(struct kiocb *iocb,
 	loff_t endbyte;
 	int err;
 
-	written = generic_file_direct_write(iocb, from, pos, count, ocount);
+	written = generic_file_direct_write(iocb, from, pos);
 
-	if (written < 0 || written == count)
+	if (written < 0 || !iov_iter_count(from))
 		return written;
 
 	pos += written;
@@ -1720,13 +1719,14 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 	u64 end_pos;
 	ssize_t num_written = 0;
 	ssize_t err = 0;
-	size_t count, ocount;
+	size_t count;
 	bool sync = (file->f_flags & O_DSYNC) || IS_SYNC(file->f_mapping->host);
 	struct iov_iter i;
 
 	mutex_lock(&inode->i_mutex);
 
-	count = ocount = iov_length(iov, nr_segs);
+	count = iov_length(iov, nr_segs);
+	iov_iter_init(&i, WRITE, iov, nr_segs, count);
 
 	current->backing_dev_info = inode->i_mapping->backing_dev_info;
 	err = generic_write_checks(file, &pos, &count, S_ISBLK(inode->i_mode));
@@ -1740,7 +1740,7 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 		goto out;
 	}
 
-	iov_iter_init(&i, WRITE, iov, nr_segs, count);
+	iov_iter_truncate(&i, count);
 
 	err = file_remove_suid(file);
 	if (err) {
@@ -1783,8 +1783,7 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 		atomic_inc(&BTRFS_I(inode)->sync_writers);
 
 	if (unlikely(file->f_flags & O_DIRECT)) {
-		num_written = __btrfs_direct_write(iocb, &i,
-						   pos, count, ocount);
+		num_written = __btrfs_direct_write(iocb, &i, pos);
 	} else {
 		num_written = __btrfs_buffered_write(file, &i, pos);
 		if (num_written > 0)

commit 71d8e532b1549a478e6a6a8a44f309d050294d00
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Wed Mar 5 19:28:09 2014 -0500

    start adding the tag to iov_iter
    
    For now, just use the same thing we pass to ->direct_IO() - it's all
    iovec-based at the moment.  Pass it explicitly to iov_iter_init() and
    account for kvec vs. iovec in there, by the same kludge NFS ->direct_IO()
    uses.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index a0a94a30d85a..f8cee205618a 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1740,7 +1740,7 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 		goto out;
 	}
 
-	iov_iter_init(&i, iov, nr_segs, count, 0);
+	iov_iter_init(&i, WRITE, iov, nr_segs, count);
 
 	err = file_remove_suid(file);
 	if (err) {

commit cb66a7a1f149ff705fa37cad6d1252b046e0ad4f
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Tue Mar 4 15:24:06 2014 -0500

    kill generic_segment_checks()
    
    all callers of ->aio_read() and ->aio_write() have iov/nr_segs already
    checked - generic_segment_checks() done after that is just an odd way
    to spell iov_length().
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 1dafe0701daf..a0a94a30d85a 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1726,12 +1726,7 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 
 	mutex_lock(&inode->i_mutex);
 
-	err = generic_segment_checks(iov, &nr_segs, &ocount, VERIFY_READ);
-	if (err) {
-		mutex_unlock(&inode->i_mutex);
-		goto out;
-	}
-	count = ocount;
+	count = ocount = iov_length(iov, nr_segs);
 
 	current->backing_dev_info = inode->i_mapping->backing_dev_info;
 	err = generic_write_checks(file, &pos, &count, S_ISBLK(inode->i_mode));

commit 0ae5e4d370599592eab845527b31708a4f3411be
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Mon Mar 3 22:09:39 2014 -0500

    __btrfs_direct_write(): switch to iov_iter
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 9fe20c2052af..1dafe0701daf 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1658,25 +1658,23 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 }
 
 static ssize_t __btrfs_direct_write(struct kiocb *iocb,
-				    const struct iovec *iov,
-				    unsigned long nr_segs, loff_t pos,
+				    struct iov_iter *from,
+				    loff_t pos,
 				    size_t count, size_t ocount)
 {
 	struct file *file = iocb->ki_filp;
-	struct iov_iter i;
 	ssize_t written;
 	ssize_t written_buffered;
 	loff_t endbyte;
 	int err;
 
-	iov_iter_init(&i, iov, nr_segs, count, 0);
-	written = generic_file_direct_write(iocb, &i, pos, count, ocount);
+	written = generic_file_direct_write(iocb, from, pos, count, ocount);
 
 	if (written < 0 || written == count)
 		return written;
 
 	pos += written;
-	written_buffered = __btrfs_buffered_write(file, &i, pos);
+	written_buffered = __btrfs_buffered_write(file, from, pos);
 	if (written_buffered < 0) {
 		err = written_buffered;
 		goto out;
@@ -1724,6 +1722,7 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 	ssize_t err = 0;
 	size_t count, ocount;
 	bool sync = (file->f_flags & O_DSYNC) || IS_SYNC(file->f_mapping->host);
+	struct iov_iter i;
 
 	mutex_lock(&inode->i_mutex);
 
@@ -1746,6 +1745,8 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 		goto out;
 	}
 
+	iov_iter_init(&i, iov, nr_segs, count, 0);
+
 	err = file_remove_suid(file);
 	if (err) {
 		mutex_unlock(&inode->i_mutex);
@@ -1787,13 +1788,9 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 		atomic_inc(&BTRFS_I(inode)->sync_writers);
 
 	if (unlikely(file->f_flags & O_DIRECT)) {
-		num_written = __btrfs_direct_write(iocb, iov, nr_segs,
+		num_written = __btrfs_direct_write(iocb, &i,
 						   pos, count, ocount);
 	} else {
-		struct iov_iter i;
-
-		iov_iter_init(&i, iov, nr_segs, count, num_written);
-
 		num_written = __btrfs_buffered_write(file, &i, pos);
 		if (num_written > 0)
 			iocb->ki_pos = pos + num_written;

commit f8579f8673b7ecdb7a81d5d5bb1d981093d9aa94
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Mon Mar 3 22:03:20 2014 -0500

    generic_file_direct_write(): switch to iov_iter
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index ae6af072b635..9fe20c2052af 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1669,15 +1669,13 @@ static ssize_t __btrfs_direct_write(struct kiocb *iocb,
 	loff_t endbyte;
 	int err;
 
-	written = generic_file_direct_write(iocb, iov, &nr_segs, pos,
-					    count, ocount);
+	iov_iter_init(&i, iov, nr_segs, count, 0);
+	written = generic_file_direct_write(iocb, &i, pos, count, ocount);
 
 	if (written < 0 || written == count)
 		return written;
 
 	pos += written;
-	count -= written;
-	iov_iter_init(&i, iov, nr_segs, count, written);
 	written_buffered = __btrfs_buffered_write(file, &i, pos);
 	if (written_buffered < 0) {
 		err = written_buffered;

commit 33c0022f0e687b0161a9bb84a5671df932551e3a
Merge: 2b9d1c050d29 cfd4a535b68f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Apr 27 13:26:28 2014 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs
    
    Pull btrfs fixes from Chris Mason.
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs:
      Btrfs: limit the path size in send to PATH_MAX
      Btrfs: correctly set profile flags on seqlock retry
      Btrfs: use correct key when repeating search for extent item
      Btrfs: fix inode caching vs tree log
      Btrfs: fix possible memory leaks in open_ctree()
      Btrfs: avoid triggering bug_on() when we fail to start inode caching task
      Btrfs: move btrfs_{set,clear}_and_info() to ctree.h
      btrfs: replace error code from btrfs_drop_extents
      btrfs: Change the hole range to a more accurate value.
      btrfs: fix use-after-free in mount_subvol()

commit 3f9e3df8da3c51649c15db249978a10f7374236a
Author: David Sterba <dsterba@suse.cz>
Date:   Tue Apr 15 18:50:17 2014 +0200

    btrfs: replace error code from btrfs_drop_extents
    
    There's a case which clone does not handle and used to BUG_ON instead,
    (testcase xfstests/btrfs/035), now returns EINVAL. This error code is
    confusing to the ioctl caller, as it normally signifies errorneous
    arguments.
    
    Change it to ENOPNOTSUPP which allows a fall back to copy instead of
    clone. This does not affect the common reflink operation.
    
    Signed-off-by: David Sterba <dsterba@suse.cz>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index e7e78fa9085e..1eee3f79d75f 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -805,7 +805,7 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 		if (start > key.offset && end < extent_end) {
 			BUG_ON(del_nr > 0);
 			if (extent_type == BTRFS_FILE_EXTENT_INLINE) {
-				ret = -EINVAL;
+				ret = -EOPNOTSUPP;
 				break;
 			}
 
@@ -851,7 +851,7 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 		 */
 		if (start <= key.offset && end < extent_end) {
 			if (extent_type == BTRFS_FILE_EXTENT_INLINE) {
-				ret = -EINVAL;
+				ret = -EOPNOTSUPP;
 				break;
 			}
 
@@ -877,7 +877,7 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 		if (start > key.offset && end >= extent_end) {
 			BUG_ON(del_nr > 0);
 			if (extent_type == BTRFS_FILE_EXTENT_INLINE) {
-				ret = -EINVAL;
+				ret = -EOPNOTSUPP;
 				break;
 			}
 

commit c5f7d0bb29df2e1848a236e58e201daf5b4e0f21
Author: Qu Wenruo <quwenruo@cn.fujitsu.com>
Date:   Tue Apr 15 10:41:00 2014 +0800

    btrfs: Change the hole range to a more accurate value.
    
    Commit 3ac0d7b96a268a98bd474cab8bce3a9f125aaccf fixed the btrfs expanding
    write problem but the hole punched is sometimes too large for some
    iovec, which has unmapped data ranges.
    This patch will change to hole range to a more accurate value using the
    counts checked by the write check routines.
    
    Reported-by: Al Viro <viro@ZenIV.linux.org.uk>
    Signed-off-by: Qu Wenruo <quwenruo@cn.fujitsu.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 23f6a9d9f104..e7e78fa9085e 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1783,7 +1783,7 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 	start_pos = round_down(pos, root->sectorsize);
 	if (start_pos > i_size_read(inode)) {
 		/* Expand hole size to cover write data, preventing empty gap */
-		end_pos = round_up(pos + iov->iov_len, root->sectorsize);
+		end_pos = round_up(pos + count, root->sectorsize);
 		err = btrfs_cont_expand(inode, i_size_read(inode), end_pos);
 		if (err) {
 			mutex_unlock(&inode->i_mutex);

commit 5166701b368caea89d57b14bf41cf39e819dad51
Merge: 0a7418f5f569 a786c06d9f27
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Apr 12 14:49:50 2014 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs
    
    Pull vfs updates from Al Viro:
     "The first vfs pile, with deep apologies for being very late in this
      window.
    
      Assorted cleanups and fixes, plus a large preparatory part of iov_iter
      work.  There's a lot more of that, but it'll probably go into the next
      merge window - it *does* shape up nicely, removes a lot of
      boilerplate, gets rid of locking inconsistencie between aio_write and
      splice_write and I hope to get Kent's direct-io rewrite merged into
      the same queue, but some of the stuff after this point is having
      (mostly trivial) conflicts with the things already merged into
      mainline and with some I want more testing.
    
      This one passes LTP and xfstests without regressions, in addition to
      usual beating.  BTW, readahead02 in ltp syscalls testsuite has started
      giving failures since "mm/readahead.c: fix readahead failure for
      memoryless NUMA nodes and limit readahead pages" - might be a false
      positive, might be a real regression..."
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs: (63 commits)
      missing bits of "splice: fix racy pipe->buffers uses"
      cifs: fix the race in cifs_writev()
      ceph_sync_{,direct_}write: fix an oops on ceph_osdc_new_request() failure
      kill generic_file_buffered_write()
      ocfs2_file_aio_write(): switch to generic_perform_write()
      ceph_aio_write(): switch to generic_perform_write()
      xfs_file_buffered_aio_write(): switch to generic_perform_write()
      export generic_perform_write(), start getting rid of generic_file_buffer_write()
      generic_file_direct_write(): get rid of ppos argument
      btrfs_file_aio_write(): get rid of ppos
      kill the 5th argument of generic_file_buffered_write()
      kill the 4th argument of __generic_file_aio_write()
      lustre: don't open-code kernel_recvmsg()
      ocfs2: don't open-code kernel_recvmsg()
      drbd: don't open-code kernel_recvmsg()
      constify blk_rq_map_user_iov() and friends
      lustre: switch to kernel_sendmsg()
      ocfs2: don't open-code kernel_sendmsg()
      take iov_iter stuff to mm/iov_iter.c
      process_vm_access: tidy up a bit
      ...

commit 3123bca71993c2346a458875488863772c1d5dc4
Merge: 582076ab1677 e4fbaee29272
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Apr 11 14:16:53 2014 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs
    
    Pull second set of btrfs updates from Chris Mason:
     "The most important changes here are from Josef, fixing a btrfs
      regression in 3.14 that can cause corruptions in the extent allocation
      tree when snapshots are in use.
    
      Josef also fixed some deadlocks in send/recv and other assorted races
      when balance is running"
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs: (23 commits)
      Btrfs: fix compile warnings on on avr32 platform
      btrfs: allow mounting btrfs subvolumes with different ro/rw options
      btrfs: export global block reserve size as space_info
      btrfs: fix crash in remount(thread_pool=) case
      Btrfs: abort the transaction when we don't find our extent ref
      Btrfs: fix EINVAL checks in btrfs_clone
      Btrfs: fix unlock in __start_delalloc_inodes()
      Btrfs: scrub raid56 stripes in the right way
      Btrfs: don't compress for a small write
      Btrfs: more efficient io tree navigation on wait_extent_bit
      Btrfs: send, build path string only once in send_hole
      btrfs: filter invalid arg for btrfs resize
      Btrfs: send, fix data corruption due to incorrect hole detection
      Btrfs: kmalloc() doesn't return an ERR_PTR
      Btrfs: fix snapshot vs nocow writting
      btrfs: Change the expanding write sequence to fix snapshot related bug.
      btrfs: make device scan less noisy
      btrfs: fix lockdep warning with reclaim lock inversion
      Btrfs: hold the commit_root_sem when getting the commit root during send
      Btrfs: remove transaction from send
      ...

commit f1820361f83d556a7f0a9f629100f3825e594328
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Mon Apr 7 15:37:19 2014 -0700

    mm: implement ->map_pages for page cache
    
    filemap_map_pages() is generic implementation of ->map_pages() for
    filesystems who uses page cache.
    
    It should be safe to use filemap_map_pages() for ->map_pages() if
    filesystem use filemap_fault() for ->fault().
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Matthew Wilcox <matthew.r.wilcox@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: Ning Qu <quning@gmail.com>
    Cc: Hugh Dickins <hughd@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index e1ffb1e22898..c660527af838 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2025,6 +2025,7 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 
 static const struct vm_operations_struct btrfs_file_vm_ops = {
 	.fault		= filemap_fault,
+	.map_pages	= filemap_map_pages,
 	.page_mkwrite	= btrfs_page_mkwrite,
 	.remap_pages	= generic_file_remap_pages,
 };

commit 3ac0d7b96a268a98bd474cab8bce3a9f125aaccf
Author: Qu Wenruo <quwenruo@cn.fujitsu.com>
Date:   Thu Mar 27 02:51:58 2014 +0000

    btrfs: Change the expanding write sequence to fix snapshot related bug.
    
    When testing fsstress with snapshot making background, some snapshot
    following problem.
    
    Snapshot 270:
    inode 323: size 0
    
    Snapshot 271:
    inode 323: size 349145
    |-------Hole---|---------Empty gap-------|-------Hole-----|
    0           122880                      172032        349145
    
    Snapshot 272:
    inode 323: size 349145
    |-------Hole---|------------Data---------|-------Hole-----|
    0           122880                      172032        349145
    
    The fsstress operation on inode 323 is the following:
    write:          offset  126832  len 43124
    truncate:       size    349145
    
    Since the write with offset is consist of 2 operations:
    1. punch hole
    2. write data
    Hole punching is faster than data write, so hole punching in write
    and truncate is done first and then buffered write, so the snapshot 271 got
    empty gap, which will not pass btrfsck.
    
    To fix the bug, this patch will change the write sequence which will
    first punch a hole covering the write end if a hole is needed.
    
    Reported-by: Gui Hecheng <guihc.fnst@cn.fujitsu.com>
    Signed-off-by: Qu Wenruo <quwenruo@cn.fujitsu.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 036f506cabd8..23f6a9d9f104 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1727,6 +1727,7 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 	struct btrfs_root *root = BTRFS_I(inode)->root;
 	loff_t *ppos = &iocb->ki_pos;
 	u64 start_pos;
+	u64 end_pos;
 	ssize_t num_written = 0;
 	ssize_t err = 0;
 	size_t count, ocount;
@@ -1781,7 +1782,9 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 
 	start_pos = round_down(pos, root->sectorsize);
 	if (start_pos > i_size_read(inode)) {
-		err = btrfs_cont_expand(inode, i_size_read(inode), start_pos);
+		/* Expand hole size to cover write data, preventing empty gap */
+		end_pos = round_up(pos + iov->iov_len, root->sectorsize);
+		err = btrfs_cont_expand(inode, i_size_read(inode), end_pos);
 		if (err) {
 			mutex_unlock(&inode->i_mutex);
 			goto out;

commit 53c566625fb872e7826a237f0f5c21458028e94a
Merge: 34917f971390 00fdf13a2e9f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Apr 4 15:31:36 2014 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs
    
    Pull btrfs changes from Chris Mason:
     "This is a pretty long stream of bug fixes and performance fixes.
    
      Qu Wenruo has replaced the btrfs async threads with regular kernel
      workqueues.  We'll keep an eye out for performance differences, but
      it's nice to be using more generic code for this.
    
      We still have some corruption fixes and other patches coming in for
      the merge window, but this batch is tested and ready to go"
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs: (108 commits)
      Btrfs: fix a crash of clone with inline extents's split
      btrfs: fix uninit variable warning
      Btrfs: take into account total references when doing backref lookup
      Btrfs: part 2, fix incremental send's decision to delay a dir move/rename
      Btrfs: fix incremental send's decision to delay a dir move/rename
      Btrfs: remove unnecessary inode generation lookup in send
      Btrfs: fix race when updating existing ref head
      btrfs: Add trace for btrfs_workqueue alloc/destroy
      Btrfs: less fs tree lock contention when using autodefrag
      Btrfs: return EPERM when deleting a default subvolume
      Btrfs: add missing kfree in btrfs_destroy_workqueue
      Btrfs: cache extent states in defrag code path
      Btrfs: fix deadlock with nested trans handles
      Btrfs: fix possible empty list access when flushing the delalloc inodes
      Btrfs: split the global ordered extents mutex
      Btrfs: don't flush all delalloc inodes when we doesn't get s_umount lock
      Btrfs: reclaim delalloc metadata more aggressively
      Btrfs: remove unnecessary lock in may_commit_transaction()
      Btrfs: remove the unnecessary flush when preparing the pages
      Btrfs: just do dirty page flush for the inode with compression before direct IO
      ...

commit 45d4f855046631d63dab8832ba8a8369ed8e04bd
Author: Dan Carpenter <dan.carpenter@oracle.com>
Date:   Thu Apr 3 14:47:17 2014 -0700

    fs/direct-io.c: remove some left over checks
    
    We know that "ret > 0" is true here.  These tests were left over from
    commit 02afc27faec9 ('direct-io: Handle O_(D)SYNC AIO') and aren't
    needed any more.
    
    Signed-off-by: Dan Carpenter <dan.carpenter@oracle.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 0165b8672f09..7331a230e30b 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1797,7 +1797,7 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 	BTRFS_I(inode)->last_sub_trans = root->log_transid;
 	if (num_written > 0) {
 		err = generic_write_sync(file, pos, num_written);
-		if (err < 0 && num_written > 0)
+		if (err < 0)
 			num_written = err;
 	}
 

commit 5cb6c6c7eb1ed24744b41fad47d9a25b72207098
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Tue Feb 11 20:58:20 2014 -0500

    generic_file_direct_write(): get rid of ppos argument
    
    always equal to &iocb->ki_pos.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index f6032a2bfab9..8ed4b165abbd 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1640,7 +1640,7 @@ static ssize_t __btrfs_direct_write(struct kiocb *iocb,
 	loff_t endbyte;
 	int err;
 
-	written = generic_file_direct_write(iocb, iov, &nr_segs, pos, &iocb->ki_pos,
+	written = generic_file_direct_write(iocb, iov, &nr_segs, pos,
 					    count, ocount);
 
 	if (written < 0 || written == count)

commit 867c4f9329e1bf7d0967bec761f033373f72b55e
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Tue Feb 11 19:31:06 2014 -0500

    btrfs_file_aio_write(): get rid of ppos
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 34e096201da1..f6032a2bfab9 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1631,7 +1631,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 static ssize_t __btrfs_direct_write(struct kiocb *iocb,
 				    const struct iovec *iov,
 				    unsigned long nr_segs, loff_t pos,
-				    loff_t *ppos, size_t count, size_t ocount)
+				    size_t count, size_t ocount)
 {
 	struct file *file = iocb->ki_filp;
 	struct iov_iter i;
@@ -1640,7 +1640,7 @@ static ssize_t __btrfs_direct_write(struct kiocb *iocb,
 	loff_t endbyte;
 	int err;
 
-	written = generic_file_direct_write(iocb, iov, &nr_segs, pos, ppos,
+	written = generic_file_direct_write(iocb, iov, &nr_segs, pos, &iocb->ki_pos,
 					    count, ocount);
 
 	if (written < 0 || written == count)
@@ -1659,7 +1659,7 @@ static ssize_t __btrfs_direct_write(struct kiocb *iocb,
 	if (err)
 		goto out;
 	written += written_buffered;
-	*ppos = pos + written_buffered;
+	iocb->ki_pos = pos + written_buffered;
 	invalidate_mapping_pages(file->f_mapping, pos >> PAGE_CACHE_SHIFT,
 				 endbyte >> PAGE_CACHE_SHIFT);
 out:
@@ -1691,7 +1691,6 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 	struct file *file = iocb->ki_filp;
 	struct inode *inode = file_inode(file);
 	struct btrfs_root *root = BTRFS_I(inode)->root;
-	loff_t *ppos = &iocb->ki_pos;
 	u64 start_pos;
 	ssize_t num_written = 0;
 	ssize_t err = 0;
@@ -1759,7 +1758,7 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 
 	if (unlikely(file->f_flags & O_DIRECT)) {
 		num_written = __btrfs_direct_write(iocb, iov, nr_segs,
-						   pos, ppos, count, ocount);
+						   pos, count, ocount);
 	} else {
 		struct iov_iter i;
 
@@ -1767,7 +1766,7 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 
 		num_written = __btrfs_buffered_write(file, &i, pos);
 		if (num_written > 0)
-			*ppos = pos + num_written;
+			iocb->ki_pos = pos + num_written;
 	}
 
 	mutex_unlock(&inode->i_mutex);

commit 9e8c2af96e0d2d5fe298dd796fb6bc16e888a48d
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sun Feb 2 22:10:25 2014 -0500

    callers of iov_copy_from_user_atomic() don't need pagecache_disable()
    
    ... it does that itself (via kmap_atomic())
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 0165b8672f09..34e096201da1 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -425,13 +425,8 @@ static noinline int btrfs_copy_from_user(loff_t pos, int num_pages,
 		struct page *page = prepared_pages[pg];
 		/*
 		 * Copy data from userspace to the current page
-		 *
-		 * Disable pagefault to avoid recursive lock since
-		 * the pages are already locked
 		 */
-		pagefault_disable();
 		copied = iov_iter_copy_from_user_atomic(page, i, offset, count);
-		pagefault_enable();
 
 		/* Flush processor's dcache for this page */
 		flush_dcache_page(page);

commit 00fdf13a2e9f313a044288aa59d3b8ec29ff904a
Author: Liu Bo <bo.li.liu@oracle.com>
Date:   Mon Mar 10 18:56:07 2014 +0800

    Btrfs: fix a crash of clone with inline extents's split
    
    xfstests's btrfs/035 triggers a BUG_ON, which we use to detect the split
    of inline extents in __btrfs_drop_extents().
    
    For inline extents, we cannot duplicate another EXTENT_DATA item, because
    it breaks the rule of inline extents, that is, 'start offset' needs to be 0.
    
    We have set limitations for the source inode's compressed inline extents,
    because it needs to decompress and recompress.  Now the destination inode's
    inline extents also need similar limitations.
    
    With this, xfstests btrfs/035 doesn't run into panic.
    
    Signed-off-by: Liu Bo <bo.li.liu@oracle.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index b2143b8c33c5..036f506cabd8 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -804,7 +804,10 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 		 */
 		if (start > key.offset && end < extent_end) {
 			BUG_ON(del_nr > 0);
-			BUG_ON(extent_type == BTRFS_FILE_EXTENT_INLINE);
+			if (extent_type == BTRFS_FILE_EXTENT_INLINE) {
+				ret = -EINVAL;
+				break;
+			}
 
 			memcpy(&new_key, &key, sizeof(new_key));
 			new_key.offset = start;
@@ -847,7 +850,10 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 		 *      | -------- extent -------- |
 		 */
 		if (start <= key.offset && end < extent_end) {
-			BUG_ON(extent_type == BTRFS_FILE_EXTENT_INLINE);
+			if (extent_type == BTRFS_FILE_EXTENT_INLINE) {
+				ret = -EINVAL;
+				break;
+			}
 
 			memcpy(&new_key, &key, sizeof(new_key));
 			new_key.offset = end;
@@ -870,7 +876,10 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 		 */
 		if (start > key.offset && end >= extent_end) {
 			BUG_ON(del_nr > 0);
-			BUG_ON(extent_type == BTRFS_FILE_EXTENT_INLINE);
+			if (extent_type == BTRFS_FILE_EXTENT_INLINE) {
+				ret = -EINVAL;
+				break;
+			}
 
 			btrfs_set_file_extent_num_bytes(leaf, fi,
 							start - key.offset);

commit b88935bf9822cda58fd70dffe8e016d448757d40
Author: Miao Xie <miaox@cn.fujitsu.com>
Date:   Thu Mar 6 13:54:58 2014 +0800

    Btrfs: remove the unnecessary flush when preparing the pages
    
    Signed-off-by: Miao Xie <miaox@cn.fujitsu.com>
    Signed-off-by: Josef Bacik <jbacik@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 006cf9f0e852..b2143b8c33c5 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1360,11 +1360,11 @@ lock_and_cleanup_extent_if_need(struct inode *inode, struct page **pages,
 		struct btrfs_ordered_extent *ordered;
 		lock_extent_bits(&BTRFS_I(inode)->io_tree,
 				 start_pos, last_pos, 0, cached_state);
-		ordered = btrfs_lookup_first_ordered_extent(inode, last_pos);
+		ordered = btrfs_lookup_ordered_range(inode, start_pos,
+						     last_pos - start_pos + 1);
 		if (ordered &&
 		    ordered->file_offset + ordered->len > start_pos &&
 		    ordered->file_offset <= last_pos) {
-			btrfs_put_ordered_extent(ordered);
 			unlock_extent_cached(&BTRFS_I(inode)->io_tree,
 					     start_pos, last_pos,
 					     cached_state, GFP_NOFS);
@@ -1372,12 +1372,9 @@ lock_and_cleanup_extent_if_need(struct inode *inode, struct page **pages,
 				unlock_page(pages[i]);
 				page_cache_release(pages[i]);
 			}
-			ret = btrfs_wait_ordered_range(inode, start_pos,
-						last_pos - start_pos + 1);
-			if (ret)
-				return ret;
-			else
-				return -EAGAIN;
+			btrfs_start_ordered_extent(inode, ordered, 1);
+			btrfs_put_ordered_extent(ordered);
+			return -EAGAIN;
 		}
 		if (ordered)
 			btrfs_put_ordered_extent(ordered);

commit 8257b2dc3c1a1057b84a589827354abdc4c767fd
Author: Miao Xie <miaox@cn.fujitsu.com>
Date:   Thu Mar 6 13:38:19 2014 +0800

    Btrfs: introduce btrfs_{start, end}_nocow_write() for each subvolume
    
    If the snapshot creation happened after the nocow write but before the dirty
    data flush, we would fail to flush the dirty data because of no space.
    
    So we must keep track of when those nocow write operations start and when they
    end, if there are nocow writers, the snapshot creators must wait. In order
    to implement this function, I introduce btrfs_{start, end}_nocow_write(),
    which is similar to mnt_{want,drop}_write().
    
    These two functions are only used for nocow file write operations.
    
    Signed-off-by: Miao Xie <miaox@cn.fujitsu.com>
    Signed-off-by: Josef Bacik <jbacik@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index d88f2dc4d045..006cf9f0e852 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1410,6 +1410,10 @@ static noinline int check_can_nocow(struct inode *inode, loff_t pos,
 	u64 num_bytes;
 	int ret;
 
+	ret = btrfs_start_nocow_write(root);
+	if (!ret)
+		return -ENOSPC;
+
 	lockstart = round_down(pos, root->sectorsize);
 	lockend = round_up(pos + *write_bytes, root->sectorsize) - 1;
 
@@ -1427,11 +1431,13 @@ static noinline int check_can_nocow(struct inode *inode, loff_t pos,
 
 	num_bytes = lockend - lockstart + 1;
 	ret = can_nocow_extent(inode, lockstart, &num_bytes, NULL, NULL, NULL);
-	if (ret <= 0)
+	if (ret <= 0) {
 		ret = 0;
-	else
+		btrfs_end_nocow_write(root);
+	} else {
 		*write_bytes = min_t(size_t, *write_bytes ,
 				     num_bytes - pos + lockstart);
+	}
 
 	unlock_extent(&BTRFS_I(inode)->io_tree, lockstart, lockend);
 
@@ -1520,6 +1526,8 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 			if (!only_release_metadata)
 				btrfs_free_reserved_data_space(inode,
 							       reserve_bytes);
+			else
+				btrfs_end_nocow_write(root);
 			break;
 		}
 
@@ -1608,6 +1616,9 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 		}
 
 		release_bytes = 0;
+		if (only_release_metadata)
+			btrfs_end_nocow_write(root);
+
 		if (only_release_metadata && copied > 0) {
 			u64 lockstart = round_down(pos, root->sectorsize);
 			u64 lockend = lockstart +
@@ -1634,10 +1645,12 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 	kfree(pages);
 
 	if (release_bytes) {
-		if (only_release_metadata)
+		if (only_release_metadata) {
+			btrfs_end_nocow_write(root);
 			btrfs_delalloc_release_metadata(inode, release_bytes);
-		else
+		} else {
 			btrfs_delalloc_release_space(inode, release_bytes);
+		}
 	}
 
 	return num_written ? num_written : ret;

commit 7b2b70851f862b68714f357d2926adbb6c574fdd
Author: Miao Xie <miaox@cn.fujitsu.com>
Date:   Thu Feb 27 13:58:05 2014 +0800

    Btrfs: fix preallocate vs double nocow write
    
    We can not release the reserved metadata space for the first write if we
    find the write position is pre-allocated. Because the kernel might write
    the data on the disk before we do the second write but after the can-nocow
    check, if we release the space for the first write, we might fail to update
    the metadata because of no space.
    
    Fix this problem by end nocow write if there is dirty data in the range whose
    space is pre-allocated.
    
    Signed-off-by: Miao Xie <miaox@cn.fujitsu.com>
    Signed-off-by: Josef Bacik <jbacik@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index fc2d21b0a022..d88f2dc4d045 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1427,16 +1427,11 @@ static noinline int check_can_nocow(struct inode *inode, loff_t pos,
 
 	num_bytes = lockend - lockstart + 1;
 	ret = can_nocow_extent(inode, lockstart, &num_bytes, NULL, NULL, NULL);
-	if (ret <= 0) {
+	if (ret <= 0)
 		ret = 0;
-	} else {
-		clear_extent_bit(&BTRFS_I(inode)->io_tree, lockstart, lockend,
-				 EXTENT_DIRTY | EXTENT_DELALLOC |
-				 EXTENT_DO_ACCOUNTING | EXTENT_DEFRAG, 0, 0,
-				 NULL, GFP_NOFS);
+	else
 		*write_bytes = min_t(size_t, *write_bytes ,
 				     num_bytes - pos + lockstart);
-	}
 
 	unlock_extent(&BTRFS_I(inode)->io_tree, lockstart, lockend);
 

commit c933956ddf80bc455d33cbcf39d35d935daf45a9
Author: Miao Xie <miaox@cn.fujitsu.com>
Date:   Thu Feb 27 13:58:04 2014 +0800

    Btrfs: fix wrong lock range and write size in check_can_nocow()
    
    The write range may not be sector-aligned, for example:
    
           |--------|--------|      <- write range, sector-unaligned, size: 2blocks
      |--------|--------|--------|  <- correct lock range, size: 3blocks
    
    But according to the old code, we used the size of write range to calculate
    the lock range directly, not considered the offset, we would get a wrong lock
    range:
    
           |--------|--------|      <- write range, sector-unaligned, size: 2blocks
      |--------|--------|           <- wrong lock range, size: 2blocks
    
    And besides that, the old code also had the same problem when calculating
    the real write size. Correct them.
    
    Signed-off-by: Miao Xie <miaox@cn.fujitsu.com>
    Signed-off-by: Josef Bacik <jbacik@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 31e48b947060..fc2d21b0a022 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1411,7 +1411,7 @@ static noinline int check_can_nocow(struct inode *inode, loff_t pos,
 	int ret;
 
 	lockstart = round_down(pos, root->sectorsize);
-	lockend = lockstart + round_up(*write_bytes, root->sectorsize) - 1;
+	lockend = round_up(pos + *write_bytes, root->sectorsize) - 1;
 
 	while (1) {
 		lock_extent(&BTRFS_I(inode)->io_tree, lockstart, lockend);
@@ -1434,7 +1434,8 @@ static noinline int check_can_nocow(struct inode *inode, loff_t pos,
 				 EXTENT_DIRTY | EXTENT_DELALLOC |
 				 EXTENT_DO_ACCOUNTING | EXTENT_DEFRAG, 0, 0,
 				 NULL, GFP_NOFS);
-		*write_bytes = min_t(size_t, *write_bytes, num_bytes);
+		*write_bytes = min_t(size_t, *write_bytes ,
+				     num_bytes - pos + lockstart);
 	}
 
 	unlock_extent(&BTRFS_I(inode)->io_tree, lockstart, lockend);

commit 176840b3aa3cb795ddec4fc665ffbd707abff906
Author: Filipe Manana <fdmanana@gmail.com>
Date:   Tue Feb 25 14:15:13 2014 +0000

    Btrfs: more efficient btrfs_drop_extent_cache
    
    While droping extent map structures from the extent cache that cover our
    target range, we would remove each extent map structure from the red black
    tree and then add either 1 or 2 new extent map structures if the former
    extent map covered sections outside our target range.
    
    This change simply attempts to replace the existing extent map structure
    with a new one that covers the subsection we're not interested in, instead
    of doing a red black remove operation followed by an insertion operation.
    
    The number of elements in an inode's extent map tree can get very high for large
    files under random writes. For example, while running the following test:
    
        sysbench --test=fileio --file-num=1 --file-total-size=10G \
            --file-test-mode=rndrw --num-threads=32 --file-block-size=32768 \
            --max-requests=500000 --file-rw-ratio=2 [prepare|run]
    
    I captured the following histogram capturing the number of extent_map items
    in the red black tree while that test was running:
    
        Count: 122462
        Range:  1.000 - 172231.000; Mean: 96415.831; Median: 101855.000; Stddev: 49700.981
        Percentiles:  90th: 160120.000; 95th: 166335.000; 99th: 171070.000
           1.000 -    5.231:   452 |
           5.231 -  187.392:    87 |
         187.392 -  585.911:   206 |
         585.911 - 1827.438:   623 |
        1827.438 - 5695.245:  1962 #
        5695.245 - 17744.861:  6204 ####
       17744.861 - 55283.764: 21115 ############
       55283.764 - 172231.000: 91813 #####################################################
    
    Benchmark:
    
        sysbench --test=fileio --file-num=1 --file-total-size=10G --file-test-mode=rndwr \
            --num-threads=64 --file-block-size=32768 --max-requests=0 --max-time=60 \
            --file-io-mode=sync --file-fsync-freq=0 [prepare|run]
    
    Before this change: 122.1Mb/sec
    After this change:  125.07Mb/sec
    (averages of 5 test runs)
    
    Test machine: quad core intel i5-3570K, 32Gb of ram, SSD
    
    Signed-off-by: Filipe David Borba Manana <fdmanana@gmail.com>
    Signed-off-by: Josef Bacik <jbacik@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 762ca32bd988..31e48b947060 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -591,7 +591,6 @@ void btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
 		clear_bit(EXTENT_FLAG_PINNED, &em->flags);
 		clear_bit(EXTENT_FLAG_LOGGING, &flags);
 		modified = !list_empty(&em->list);
-		remove_extent_mapping(em_tree, em);
 		if (no_splits)
 			goto next;
 
@@ -622,8 +621,7 @@ void btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
 			split->bdev = em->bdev;
 			split->flags = flags;
 			split->compress_type = em->compress_type;
-			ret = add_extent_mapping(em_tree, split, modified);
-			BUG_ON(ret); /* Logic error */
+			replace_extent_mapping(em_tree, em, split, modified);
 			free_extent_map(split);
 			split = split2;
 			split2 = NULL;
@@ -661,12 +659,20 @@ void btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
 				split->orig_block_len = 0;
 			}
 
-			ret = add_extent_mapping(em_tree, split, modified);
-			BUG_ON(ret); /* Logic error */
+			if (extent_map_in_tree(em)) {
+				replace_extent_mapping(em_tree, em, split,
+						       modified);
+			} else {
+				ret = add_extent_mapping(em_tree, split,
+							 modified);
+				ASSERT(ret == 0); /* Logic error */
+			}
 			free_extent_map(split);
 			split = NULL;
 		}
 next:
+		if (extent_map_in_tree(em))
+			remove_extent_mapping(em_tree, em);
 		write_unlock(&em_tree->lock);
 
 		/* once for us */

commit 12870f1c9b2de7d475d22e73fd7db1b418599725
Author: Filipe Manana <fdmanana@gmail.com>
Date:   Sat Feb 15 15:55:58 2014 +0000

    Btrfs: don't insert useless holes when punching beyond the inode's size
    
    If we punch beyond the size of an inode, we'll correctly remove any prealloc extents,
    but we'll also insert file extent items representing holes (disk bytenr == 0) that start
    with a key offset that lies beyond the inode's size and are not contiguous with the last
    file extent item.
    
    Example:
    
      $XFS_IO_PROG -f -c "truncate 118811" $SCRATCH_MNT/foo
      $XFS_IO_PROG -c "fpunch 582007 864596" $SCRATCH_MNT/foo
      $XFS_IO_PROG -c "pwrite -S 0x0d -b 39987 92267 39987" $SCRATCH_MNT/foo
    
    btrfs-debug-tree output:
    
      item 4 key (257 INODE_ITEM 0) itemoff 15885 itemsize 160
            inode generation 6 transid 6 size 132254 block group 0 mode 100600 links 1
      item 5 key (257 INODE_REF 256) itemoff 15872 itemsize 13
            inode ref index 2 namelen 3 name: foo
      item 6 key (257 EXTENT_DATA 0) itemoff 15819 itemsize 53
            extent data disk byte 0 nr 0 gen 6
            extent data offset 0 nr 90112 ram 122880
            extent compression 0
      item 7 key (257 EXTENT_DATA 90112) itemoff 15766 itemsize 53
            extent data disk byte 12845056 nr 4096 gen 6
            extent data offset 0 nr 45056 ram 45056
            extent compression 2
      item 8 key (257 EXTENT_DATA 585728) itemoff 15713 itemsize 53
            extent data disk byte 0 nr 0 gen 6
            extent data offset 0 nr 860160 ram 860160
            extent compression 0
    
    The last extent item, which represents a hole, is useless as it lies beyond the inode's
    size.
    
    Signed-off-by: Filipe David Borba Manana <fdmanana@gmail.com>
    Signed-off-by: Josef Bacik <jbacik@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 6acccc4a7f2a..762ca32bd988 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2168,6 +2168,7 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 	bool same_page = ((offset >> PAGE_CACHE_SHIFT) ==
 			  ((offset + len - 1) >> PAGE_CACHE_SHIFT));
 	bool no_holes = btrfs_fs_incompat(root->fs_info, NO_HOLES);
+	u64 ino_size = round_up(inode->i_size, PAGE_CACHE_SIZE);
 
 	ret = btrfs_wait_ordered_range(inode, offset, len);
 	if (ret)
@@ -2183,14 +2184,14 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 	 * entire page.
 	 */
 	if (same_page && len < PAGE_CACHE_SIZE) {
-		if (offset < round_up(inode->i_size, PAGE_CACHE_SIZE))
+		if (offset < ino_size)
 			ret = btrfs_truncate_page(inode, offset, len, 0);
 		mutex_unlock(&inode->i_mutex);
 		return ret;
 	}
 
 	/* zero back part of the first page */
-	if (offset < round_up(inode->i_size, PAGE_CACHE_SIZE)) {
+	if (offset < ino_size) {
 		ret = btrfs_truncate_page(inode, offset, 0, 0);
 		if (ret) {
 			mutex_unlock(&inode->i_mutex);
@@ -2199,7 +2200,7 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 	}
 
 	/* zero the front end of the last page */
-	if (offset + len < round_up(inode->i_size, PAGE_CACHE_SIZE)) {
+	if (offset + len < ino_size) {
 		ret = btrfs_truncate_page(inode, offset + len, 0, 1);
 		if (ret) {
 			mutex_unlock(&inode->i_mutex);
@@ -2288,10 +2289,13 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 
 		trans->block_rsv = &root->fs_info->trans_block_rsv;
 
-		ret = fill_holes(trans, inode, path, cur_offset, drop_end);
-		if (ret) {
-			err = ret;
-			break;
+		if (cur_offset < ino_size) {
+			ret = fill_holes(trans, inode, path, cur_offset,
+					 drop_end);
+			if (ret) {
+				err = ret;
+				break;
+			}
 		}
 
 		cur_offset = drop_end;
@@ -2324,10 +2328,12 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 	}
 
 	trans->block_rsv = &root->fs_info->trans_block_rsv;
-	ret = fill_holes(trans, inode, path, cur_offset, drop_end);
-	if (ret) {
-		err = ret;
-		goto out_trans;
+	if (cur_offset < ino_size) {
+		ret = fill_holes(trans, inode, path, cur_offset, drop_end);
+		if (ret) {
+			err = ret;
+			goto out_trans;
+		}
 	}
 
 out_trans:

commit 8b050d350c7846462a21e9e054c9154ede9b43cf
Author: Miao Xie <miaox@cn.fujitsu.com>
Date:   Thu Feb 20 18:08:58 2014 +0800

    Btrfs: fix skipped error handle when log sync failed
    
    It is possible that many tasks sync the log tree at the same time, but
    only one task can do the sync work, the others will wait for it. But those
    wait tasks didn't get the result of the log sync, and returned 0 when they
    ended the wait. It caused those tasks skipped the error handle, and the
    serious problem was they told the users the file sync succeeded but in
    fact they failed.
    
    This patch fixes this problem by introducing a log context structure,
    we insert it into the a global list. When the sync fails, we will set
    the error number of every log context in the list, then the waiting tasks
    get the error number of the log context and handle the error if need.
    
    Signed-off-by: Miao Xie <miaox@cn.fujitsu.com>
    Signed-off-by: Josef Bacik <jbacik@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 006af2f4dd98..6acccc4a7f2a 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1864,8 +1864,9 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 	struct dentry *dentry = file->f_path.dentry;
 	struct inode *inode = dentry->d_inode;
 	struct btrfs_root *root = BTRFS_I(inode)->root;
-	int ret = 0;
 	struct btrfs_trans_handle *trans;
+	struct btrfs_log_ctx ctx;
+	int ret = 0;
 	bool full_sync = 0;
 
 	trace_btrfs_sync_file(file, datasync);
@@ -1959,7 +1960,9 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 	}
 	trans->sync = true;
 
-	ret = btrfs_log_dentry_safe(trans, root, dentry);
+	btrfs_init_log_ctx(&ctx);
+
+	ret = btrfs_log_dentry_safe(trans, root, dentry, &ctx);
 	if (ret < 0) {
 		/* Fallthrough and commit/free transaction. */
 		ret = 1;
@@ -1979,7 +1982,7 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 
 	if (ret != BTRFS_NO_LOG_SYNC) {
 		if (!ret) {
-			ret = btrfs_sync_log(trans, root);
+			ret = btrfs_sync_log(trans, root, &ctx);
 			if (!ret) {
 				ret = btrfs_end_transaction(trans, root);
 				goto out;

commit d5f375270aa55794f4a7196b5247469f86278a8f
Author: Filipe David Borba Manana <fdmanana@gmail.com>
Date:   Sun Feb 9 23:45:12 2014 +0000

    Btrfs: faster/more efficient insertion of file extent items
    
    This is an extension to my previous commit titled:
    
      "Btrfs: faster file extent item replace operations"
      (hash 1acae57b161ef1282f565ef907f72aeed0eb71d9)
    
    Instead of inserting the new file extent item if we deleted existing
    file extent items covering our target file range, also allow to insert
    the new file extent item if we didn't find any existing items to delete
    and replace_extent != 0, since in this case our caller would do another
    tree search to insert the new file extent item anyway, therefore just
    combine the two tree searches into a single one, saving cpu time, reducing
    lock contention and reducing btree node/leaf COW operations.
    
    This covers the case where applications keep doing tail append writes to
    files, which for example is the case of Apache CouchDB (its database and
    view index files are always open with O_APPEND).
    
    Signed-off-by: Filipe David Borba Manana <fdmanana@gmail.com>
    Signed-off-by: Josef Bacik <jbacik@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 0165b8672f09..006af2f4dd98 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -720,7 +720,7 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 	if (drop_cache)
 		btrfs_drop_extent_cache(inode, start, end - 1, 0);
 
-	if (start >= BTRFS_I(inode)->disk_i_size)
+	if (start >= BTRFS_I(inode)->disk_i_size && !replace_extent)
 		modify_tree = 0;
 
 	while (1) {
@@ -938,34 +938,42 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 		 * Set path->slots[0] to first slot, so that after the delete
 		 * if items are move off from our leaf to its immediate left or
 		 * right neighbor leafs, we end up with a correct and adjusted
-		 * path->slots[0] for our insertion.
+		 * path->slots[0] for our insertion (if replace_extent != 0).
 		 */
 		path->slots[0] = del_slot;
 		ret = btrfs_del_items(trans, root, path, del_slot, del_nr);
 		if (ret)
 			btrfs_abort_transaction(trans, root, ret);
+	}
 
-		leaf = path->nodes[0];
-		/*
-		 * leaf eb has flag EXTENT_BUFFER_STALE if it was deleted (that
-		 * is, its contents got pushed to its neighbors), in which case
-		 * it means path->locks[0] == 0
-		 */
-		if (!ret && replace_extent && leafs_visited == 1 &&
-		    path->locks[0] &&
-		    btrfs_leaf_free_space(root, leaf) >=
-		    sizeof(struct btrfs_item) + extent_item_size) {
-
-			key.objectid = ino;
-			key.type = BTRFS_EXTENT_DATA_KEY;
-			key.offset = start;
-			setup_items_for_insert(root, path, &key,
-					       &extent_item_size,
-					       extent_item_size,
-					       sizeof(struct btrfs_item) +
-					       extent_item_size, 1);
-			*key_inserted = 1;
+	leaf = path->nodes[0];
+	/*
+	 * If btrfs_del_items() was called, it might have deleted a leaf, in
+	 * which case it unlocked our path, so check path->locks[0] matches a
+	 * write lock.
+	 */
+	if (!ret && replace_extent && leafs_visited == 1 &&
+	    (path->locks[0] == BTRFS_WRITE_LOCK_BLOCKING ||
+	     path->locks[0] == BTRFS_WRITE_LOCK) &&
+	    btrfs_leaf_free_space(root, leaf) >=
+	    sizeof(struct btrfs_item) + extent_item_size) {
+
+		key.objectid = ino;
+		key.type = BTRFS_EXTENT_DATA_KEY;
+		key.offset = start;
+		if (!del_nr && path->slots[0] < btrfs_header_nritems(leaf)) {
+			struct btrfs_key slot_key;
+
+			btrfs_item_key_to_cpu(leaf, &slot_key, path->slots[0]);
+			if (btrfs_comp_cpu_keys(&key, &slot_key) > 0)
+				path->slots[0]++;
 		}
+		setup_items_for_insert(root, path, &key,
+				       &extent_item_size,
+				       extent_item_size,
+				       sizeof(struct btrfs_item) +
+				       extent_item_size, 1);
+		*key_inserted = 1;
 	}
 
 	if (!replace_extent || !(*key_inserted))

commit 514ac8ad8793a097c0c9d89202c642479d6dfa34
Author: Chris Mason <clm@fb.com>
Date:   Fri Jan 3 21:07:00 2014 -0800

    Btrfs: don't use ram_bytes for uncompressed inline items
    
    If we truncate an uncompressed inline item, ram_bytes isn't updated to reflect
    the new size.  The fixe uses the size directly from the item header when
    reading uncompressed inlines, and also fixes truncate to update the
    size as it goes.
    
    Reported-by: Jens Axboe <axboe@fb.com>
    Signed-off-by: Chris Mason <clm@fb.com>
    CC: stable@vger.kernel.org

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 3dfd8db0e243..0165b8672f09 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -772,7 +772,8 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 				btrfs_file_extent_num_bytes(leaf, fi);
 		} else if (extent_type == BTRFS_FILE_EXTENT_INLINE) {
 			extent_end = key.offset +
-				btrfs_file_extent_inline_len(leaf, fi);
+				btrfs_file_extent_inline_len(leaf,
+						     path->slots[0], fi);
 		} else {
 			WARN_ON(1);
 			extent_end = search_start;

commit f1de968376340c97ac2d7acd25fa3107c398e0e5
Author: Miao Xie <miaox@cn.fujitsu.com>
Date:   Thu Jan 9 10:06:10 2014 +0800

    Btrfs: fix the race between write back and nocow buffered write
    
    When we ran the 274th case of xfstests with nodatacow mount option,
    We met the following warning message:
    WARNING: CPU: 1 PID: 14185 at fs/btrfs/extent-tree.c:3734 btrfs_free_reserved_data_space+0xa6/0xd0
    
    It is caused by the race between the write back and nocow buffered
    write:
      Task1                         Task2
      __btrfs_buffered_write()
        skip data reservation
        reserve the metadata space
        copy the data
        dirty the pages
        unlock the pages
                                    write back the pages
                                    release the data space
                                      becasue there is no
                                      noreserve flag
       set the noreserve flag
    
    This patch fixes this problem by unlocking the pages after
    the noreserve flag is set.
    
    Reported-by: Tsutomu Itoh <t-itoh@jp.fujitsu.com>
    Signed-off-by: Miao Xie <miaox@cn.fujitsu.com>
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 72df63b0c799..3dfd8db0e243 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1591,9 +1591,10 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 			unlock_extent_cached(&BTRFS_I(inode)->io_tree,
 					     lockstart, lockend, &cached_state,
 					     GFP_NOFS);
-		btrfs_drop_pages(pages, num_pages);
-		if (ret)
+		if (ret) {
+			btrfs_drop_pages(pages, num_pages);
 			break;
+		}
 
 		release_bytes = 0;
 		if (only_release_metadata && copied > 0) {
@@ -1607,6 +1608,8 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 			only_release_metadata = false;
 		}
 
+		btrfs_drop_pages(pages, num_pages);
+
 		cond_resched();
 
 		balance_dirty_pages_ratelimited(inode->i_mapping);

commit 5039eddc19aee8c894191c24f2dde4e645ca1bbb
Author: Josef Bacik <jbacik@fb.com>
Date:   Wed Jan 15 13:34:13 2014 -0500

    Btrfs: make fsync latency less sucky
    
    Looking into some performance related issues with large amounts of metadata
    revealed that we can have some pretty huge swings in fsync() performance.  If we
    have a lot of delayed refs backed up (as you will tend to do with lots of
    metadata) fsync() will wander off and try to run some of those delayed refs
    which can result in reading from disk and such.  Since the actual act of fsync()
    doesn't create any delayed refs there is no need to make it throttle on delayed
    ref stuff, that will be handled by other people.  With this patch we get much
    smoother fsync performance with large amounts of metadata.  Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 030012e1710c..72df63b0c799 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1928,12 +1928,24 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 	if (file->private_data)
 		btrfs_ioctl_trans_end(file);
 
+	/*
+	 * We use start here because we will need to wait on the IO to complete
+	 * in btrfs_sync_log, which could require joining a transaction (for
+	 * example checking cross references in the nocow path).  If we use join
+	 * here we could get into a situation where we're waiting on IO to
+	 * happen that is blocked on a transaction trying to commit.  With start
+	 * we inc the extwriter counter, so we wait for all extwriters to exit
+	 * before we start blocking join'ers.  This comment is to keep somebody
+	 * from thinking they are super smart and changing this to
+	 * btrfs_join_transaction *cough*Josef*cough*.
+	 */
 	trans = btrfs_start_transaction(root, 0);
 	if (IS_ERR(trans)) {
 		ret = PTR_ERR(trans);
 		mutex_unlock(&inode->i_mutex);
 		goto out;
 	}
+	trans->sync = true;
 
 	ret = btrfs_log_dentry_safe(trans, root, dentry);
 	if (ret < 0) {

commit 1acae57b161ef1282f565ef907f72aeed0eb71d9
Author: Filipe David Borba Manana <fdmanana@gmail.com>
Date:   Tue Jan 7 11:42:27 2014 +0000

    Btrfs: faster file extent item replace operations
    
    When writing to a file we drop existing file extent items that cover the
    write range and then add a new file extent item that represents that write
    range.
    
    Before this change we were doing a tree lookup to remove the file extent
    items, and then after we did another tree lookup to insert the new file
    extent item.
    Most of the time all the file extent items we need to drop are located
    within a single leaf - this is the leaf where our new file extent item ends
    up at. Therefore, in this common case just combine these 2 operations into
    a single one.
    
    By avoiding the second btree navigation for insertion of the new file extent
    item, we reduce btree node/leaf lock acquisitions/releases, btree block/leaf
    COW operations, CPU time on btree node/leaf key binary searches, etc.
    
    Besides for file writes, this is an operation that happens for file fsync's
    as well. However log btrees are much less likely to big as big as regular
    fs btrees, therefore the impact of this change is smaller.
    
    The following benchmark was performed against an SSD drive and a
    HDD drive, both for random and sequential writes:
    
      sysbench --test=fileio --file-num=4096 --file-total-size=8G \
         --file-test-mode=[rndwr|seqwr] --num-threads=512 \
         --file-block-size=8192 \ --max-requests=1000000 \
         --file-fsync-freq=0 --file-io-mode=sync [prepare|run]
    
    All results below are averages of 10 runs of the respective test.
    
    ** SSD sequential writes
    
    Before this change: 225.88 Mb/sec
    After this change:  277.26 Mb/sec
    
    ** SSD random writes
    
    Before this change: 49.91 Mb/sec
    After this change:  56.39 Mb/sec
    
    ** HDD sequential writes
    
    Before this change: 68.53 Mb/sec
    After this change:  69.87 Mb/sec
    
    ** HDD random writes
    
    Before this change: 13.04 Mb/sec
    After this change:  14.39 Mb/sec
    
    Signed-off-by: Filipe David Borba Manana <fdmanana@gmail.com>
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 35bf83876f3f..030012e1710c 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -692,7 +692,10 @@ void btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
 int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 			 struct btrfs_root *root, struct inode *inode,
 			 struct btrfs_path *path, u64 start, u64 end,
-			 u64 *drop_end, int drop_cache)
+			 u64 *drop_end, int drop_cache,
+			 int replace_extent,
+			 u32 extent_item_size,
+			 int *key_inserted)
 {
 	struct extent_buffer *leaf;
 	struct btrfs_file_extent_item *fi;
@@ -712,6 +715,7 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 	int modify_tree = -1;
 	int update_refs = (root->ref_cows || root == root->fs_info->tree_root);
 	int found = 0;
+	int leafs_visited = 0;
 
 	if (drop_cache)
 		btrfs_drop_extent_cache(inode, start, end - 1, 0);
@@ -733,6 +737,7 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 				path->slots[0]--;
 		}
 		ret = 0;
+		leafs_visited++;
 next_slot:
 		leaf = path->nodes[0];
 		if (path->slots[0] >= btrfs_header_nritems(leaf)) {
@@ -744,6 +749,7 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 				ret = 0;
 				break;
 			}
+			leafs_visited++;
 			leaf = path->nodes[0];
 			recow = 1;
 		}
@@ -927,14 +933,44 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 	}
 
 	if (!ret && del_nr > 0) {
+		/*
+		 * Set path->slots[0] to first slot, so that after the delete
+		 * if items are move off from our leaf to its immediate left or
+		 * right neighbor leafs, we end up with a correct and adjusted
+		 * path->slots[0] for our insertion.
+		 */
+		path->slots[0] = del_slot;
 		ret = btrfs_del_items(trans, root, path, del_slot, del_nr);
 		if (ret)
 			btrfs_abort_transaction(trans, root, ret);
+
+		leaf = path->nodes[0];
+		/*
+		 * leaf eb has flag EXTENT_BUFFER_STALE if it was deleted (that
+		 * is, its contents got pushed to its neighbors), in which case
+		 * it means path->locks[0] == 0
+		 */
+		if (!ret && replace_extent && leafs_visited == 1 &&
+		    path->locks[0] &&
+		    btrfs_leaf_free_space(root, leaf) >=
+		    sizeof(struct btrfs_item) + extent_item_size) {
+
+			key.objectid = ino;
+			key.type = BTRFS_EXTENT_DATA_KEY;
+			key.offset = start;
+			setup_items_for_insert(root, path, &key,
+					       &extent_item_size,
+					       extent_item_size,
+					       sizeof(struct btrfs_item) +
+					       extent_item_size, 1);
+			*key_inserted = 1;
+		}
 	}
 
+	if (!replace_extent || !(*key_inserted))
+		btrfs_release_path(path);
 	if (drop_end)
 		*drop_end = found ? min(end, extent_end) : end;
-	btrfs_release_path(path);
 	return ret;
 }
 
@@ -949,7 +985,7 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 	if (!path)
 		return -ENOMEM;
 	ret = __btrfs_drop_extents(trans, root, inode, path, start, end, NULL,
-				   drop_cache);
+				   drop_cache, 0, 0, NULL);
 	btrfs_free_path(path);
 	return ret;
 }
@@ -2219,7 +2255,7 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 	while (cur_offset < lockend) {
 		ret = __btrfs_drop_extents(trans, root, inode, path,
 					   cur_offset, lockend + 1,
-					   &drop_end, 1);
+					   &drop_end, 1, 0, 0, NULL);
 		if (ret != -ENOSPC)
 			break;
 

commit fc28b62d648dde3cfbec6584c0fa19ea7350e7e9
Author: Filipe David Borba Manana <fdmanana@gmail.com>
Date:   Fri Dec 13 19:39:34 2013 +0000

    Btrfs: fix use of uninitialized err variable
    
    fs/btrfs/file.c: In function prepare_pages.isra.18:
    fs/btrfs/file.c:1265:6: warning: err may be used uninitialized in this function [-Wuninitialized]
    
    Signed-off-by: Filipe David Borba Manana <fdmanana@gmail.com>
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 740ae8c71701..35bf83876f3f 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1244,7 +1244,7 @@ static noinline int prepare_pages(struct inode *inode, struct page **pages,
 	int i;
 	unsigned long index = pos >> PAGE_CACHE_SHIFT;
 	gfp_t mask = btrfs_alloc_write_mask(inode->i_mapping);
-	int err;
+	int err = 0;
 	int faili;
 
 	for (i = 0; i < num_pages; i++) {

commit 6126e3caf7468a07cc1a8239d9e95090acedd3ca
Author: Filipe David Borba Manana <fdmanana@gmail.com>
Date:   Tue Nov 19 16:19:24 2013 +0000

    Btrfs: fix ordered extent check in btrfs_punch_hole
    
    If the ordered extent's last byte was 1 less than our region's
    start byte, we would unnecessarily wait for the completion of
    that ordered extent, because it doesn't intersect our target
    range.
    
    Signed-off-by: Filipe David Borba Manana <fdmanana@gmail.com>
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 6cd003c3f05e..740ae8c71701 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2164,7 +2164,7 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 		 * we need to try again.
 		 */
 		if ((!ordered ||
-		    (ordered->file_offset + ordered->len < lockstart ||
+		    (ordered->file_offset + ordered->len <= lockstart ||
 		     ordered->file_offset > lockend)) &&
 		     !test_range_bit(&BTRFS_I(inode)->io_tree, lockstart,
 				     lockend, EXTENT_UPTODATE, 0,

commit 376cc685cb3b43a6509de0b6a343c4079f23c239
Author: Miao Xie <miaox@cn.fujitsu.com>
Date:   Tue Dec 10 19:25:04 2013 +0800

    Btrfs: fix the reserved space leak caused by the race between nonlock dio and buffered io
    
    When we ran sysbench on the fs with compression, the following WARN_ONs were
    triggered:
     fs/btrfs/inode.c:7829  WARN_ON(BTRFS_I(inode)->outstanding_extents);
     fs/btrfs/inode.c:7830  WARN_ON(BTRFS_I(inode)->reserved_extents);
     fs/btrfs/inode.c:7832  WARN_ON(BTRFS_I(inode)->csum_bytes);
    
    Steps to reproduce:
     # mkfs.btrfs -f <dev>
     # mount -o compress <dev> <mnt>
     # cd <mnt>
     # sysbench --test=fileio --num-threads=8 --file-total-size=8G \
     > --file-block-size=32K --file-io-mode=rndwr --file-fsync-freq=0 \
     > --file-fsync-end=no --max-requests=300000 --file-extra-flags=direct \
     > --file-test-mode=sync prepare
     # cd -
     # umount <mnt>
     # mount -o compress <dev> <mnt>
     # cd <mnt>
     # sysbench --test=fileio --num-threads=8 --file-total-size=8G \
     > --file-block-size=32K --file-io-mode=rndwr --file-fsync-freq=0 \
     > --file-fsync-end=no --max-requests=300000 --file-extra-flags=direct \
     > --file-test-mode=sync run
     # cd -
     # umount <mnt>
    
    The reason of this problem is:
    Task0                           Task1
    btrfs_direct_IO
      unlock(&inode->i_mutex)
                                    lock(&inode->i_mutex)
                                    reserve_space()
                                    prepare_pages()
                                      lock_extent()
                                      clear_extent()
                                      unlock_extent()
      lock_extent()
      test_extent(uptodate)
        return false
                                    copy_data()
                                    set_delalloc_extent()
      extent need compress
        go back to buffered write
      clear_extent(DELALLOC | DIRTY)
      unlock_extent()
    
    Task 0 and 1 wrote the same place, and task0 cleared the delalloc flag which
    was set by task1, it made the dirty pages in that extents couldn't be flushed
    into the disk, so the reserved space for that extent was not released at
    the end.
    
    This patch fixes the above bug by unlocking the extent after the delalloc.
    
    Signed-off-by: Miao Xie <miaox@cn.fujitsu.com>
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 5591c67b1b4b..6cd003c3f05e 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1235,27 +1235,18 @@ static int prepare_uptodate_page(struct page *page, u64 pos,
 }
 
 /*
- * this gets pages into the page cache and locks them down, it also properly
- * waits for data=ordered extents to finish before allowing the pages to be
- * modified.
+ * this just gets pages into the page cache and locks them down.
  */
 static noinline int prepare_pages(struct inode *inode, struct page **pages,
 				  size_t num_pages, loff_t pos,
 				  size_t write_bytes, bool force_uptodate)
 {
-	struct extent_state *cached_state = NULL;
 	int i;
 	unsigned long index = pos >> PAGE_CACHE_SHIFT;
 	gfp_t mask = btrfs_alloc_write_mask(inode->i_mapping);
-	int err = 0;
-	int faili = 0;
-	u64 start_pos;
-	u64 last_pos;
-
-	start_pos = pos & ~((u64)PAGE_CACHE_SIZE - 1);
-	last_pos = ((u64)index + num_pages) << PAGE_CACHE_SHIFT;
+	int err;
+	int faili;
 
-again:
 	for (i = 0; i < num_pages; i++) {
 		pages[i] = find_or_create_page(inode->i_mapping, index + i,
 					       mask | __GFP_WRITE);
@@ -1278,57 +1269,85 @@ static noinline int prepare_pages(struct inode *inode, struct page **pages,
 		}
 		wait_on_page_writeback(pages[i]);
 	}
-	faili = num_pages - 1;
-	err = 0;
+
+	return 0;
+fail:
+	while (faili >= 0) {
+		unlock_page(pages[faili]);
+		page_cache_release(pages[faili]);
+		faili--;
+	}
+	return err;
+
+}
+
+/*
+ * This function locks the extent and properly waits for data=ordered extents
+ * to finish before allowing the pages to be modified if need.
+ *
+ * The return value:
+ * 1 - the extent is locked
+ * 0 - the extent is not locked, and everything is OK
+ * -EAGAIN - need re-prepare the pages
+ * the other < 0 number - Something wrong happens
+ */
+static noinline int
+lock_and_cleanup_extent_if_need(struct inode *inode, struct page **pages,
+				size_t num_pages, loff_t pos,
+				u64 *lockstart, u64 *lockend,
+				struct extent_state **cached_state)
+{
+	u64 start_pos;
+	u64 last_pos;
+	int i;
+	int ret = 0;
+
+	start_pos = pos & ~((u64)PAGE_CACHE_SIZE - 1);
+	last_pos = start_pos + ((u64)num_pages << PAGE_CACHE_SHIFT) - 1;
+
 	if (start_pos < inode->i_size) {
 		struct btrfs_ordered_extent *ordered;
 		lock_extent_bits(&BTRFS_I(inode)->io_tree,
-				 start_pos, last_pos - 1, 0, &cached_state);
-		ordered = btrfs_lookup_first_ordered_extent(inode,
-							    last_pos - 1);
+				 start_pos, last_pos, 0, cached_state);
+		ordered = btrfs_lookup_first_ordered_extent(inode, last_pos);
 		if (ordered &&
 		    ordered->file_offset + ordered->len > start_pos &&
-		    ordered->file_offset < last_pos) {
+		    ordered->file_offset <= last_pos) {
 			btrfs_put_ordered_extent(ordered);
 			unlock_extent_cached(&BTRFS_I(inode)->io_tree,
-					     start_pos, last_pos - 1,
-					     &cached_state, GFP_NOFS);
+					     start_pos, last_pos,
+					     cached_state, GFP_NOFS);
 			for (i = 0; i < num_pages; i++) {
 				unlock_page(pages[i]);
 				page_cache_release(pages[i]);
 			}
-			err = btrfs_wait_ordered_range(inode, start_pos,
-						       last_pos - start_pos);
-			if (err)
-				goto fail;
-			goto again;
+			ret = btrfs_wait_ordered_range(inode, start_pos,
+						last_pos - start_pos + 1);
+			if (ret)
+				return ret;
+			else
+				return -EAGAIN;
 		}
 		if (ordered)
 			btrfs_put_ordered_extent(ordered);
 
 		clear_extent_bit(&BTRFS_I(inode)->io_tree, start_pos,
-				  last_pos - 1, EXTENT_DIRTY | EXTENT_DELALLOC |
+				  last_pos, EXTENT_DIRTY | EXTENT_DELALLOC |
 				  EXTENT_DO_ACCOUNTING | EXTENT_DEFRAG,
-				  0, 0, &cached_state, GFP_NOFS);
-		unlock_extent_cached(&BTRFS_I(inode)->io_tree,
-				     start_pos, last_pos - 1, &cached_state,
-				     GFP_NOFS);
+				  0, 0, cached_state, GFP_NOFS);
+		*lockstart = start_pos;
+		*lockend = last_pos;
+		ret = 1;
 	}
+
 	for (i = 0; i < num_pages; i++) {
 		if (clear_page_dirty_for_io(pages[i]))
 			account_page_redirty(pages[i]);
 		set_page_extent_mapped(pages[i]);
 		WARN_ON(!PageLocked(pages[i]));
 	}
-	return 0;
-fail:
-	while (faili >= 0) {
-		unlock_page(pages[faili]);
-		page_cache_release(pages[faili]);
-		faili--;
-	}
-	return err;
 
+	return ret;
 }
 
 static noinline int check_can_nocow(struct inode *inode, loff_t pos,
@@ -1379,13 +1398,17 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 	struct inode *inode = file_inode(file);
 	struct btrfs_root *root = BTRFS_I(inode)->root;
 	struct page **pages = NULL;
+	struct extent_state *cached_state = NULL;
 	u64 release_bytes = 0;
+	u64 lockstart;
+	u64 lockend;
 	unsigned long first_index;
 	size_t num_written = 0;
 	int nrptrs;
 	int ret = 0;
 	bool only_release_metadata = false;
 	bool force_page_uptodate = false;
+	bool need_unlock;
 
 	nrptrs = min((iov_iter_count(i) + PAGE_CACHE_SIZE - 1) /
 		     PAGE_CACHE_SIZE, PAGE_CACHE_SIZE /
@@ -1454,7 +1477,8 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 		}
 
 		release_bytes = reserve_bytes;
-
+		need_unlock = false;
+again:
 		/*
 		 * This is going to setup the pages array with the number of
 		 * pages we want, so we don't really need to worry about the
@@ -1466,6 +1490,18 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 		if (ret)
 			break;
 
+		ret = lock_and_cleanup_extent_if_need(inode, pages, num_pages,
+						      pos, &lockstart, &lockend,
+						      &cached_state);
+		if (ret < 0) {
+			if (ret == -EAGAIN)
+				goto again;
+			break;
+		} else if (ret > 0) {
+			need_unlock = true;
+			ret = 0;
+		}
+
 		copied = btrfs_copy_from_user(pos, num_pages,
 					   write_bytes, pages, i);
 
@@ -1510,19 +1546,20 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 		}
 
 		release_bytes = dirty_pages << PAGE_CACHE_SHIFT;
-		if (copied > 0) {
+
+		if (copied > 0)
 			ret = btrfs_dirty_pages(root, inode, pages,
 						dirty_pages, pos, copied,
 						NULL);
-			if (ret) {
-				btrfs_drop_pages(pages, num_pages);
-				break;
-			}
-		}
-
-		release_bytes = 0;
+		if (need_unlock)
+			unlock_extent_cached(&BTRFS_I(inode)->io_tree,
+					     lockstart, lockend, &cached_state,
+					     GFP_NOFS);
 		btrfs_drop_pages(pages, num_pages);
+		if (ret)
+			break;
 
+		release_bytes = 0;
 		if (only_release_metadata && copied > 0) {
 			u64 lockstart = round_down(pos, root->sectorsize);
 			u64 lockend = lockstart +

commit b37392ea86761e97d46b567667ff158e8bb67b72
Author: Miao Xie <miaox@cn.fujitsu.com>
Date:   Tue Dec 10 19:25:03 2013 +0800

    Btrfs: cleanup unnecessary parameter and variant of prepare_pages()
    
    - the caller has gotten the inode object, needn't pass the file object.
      And if so, we needn't define a inode pointer variant.
    - the position should be aligned by the page size not sector size, so
      we also needn't pass the root object into prepare_pages().
    
    Signed-off-by: Miao Xie <miaox@cn.fujitsu.com>
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index c77da440146a..5591c67b1b4b 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1239,22 +1239,20 @@ static int prepare_uptodate_page(struct page *page, u64 pos,
  * waits for data=ordered extents to finish before allowing the pages to be
  * modified.
  */
-static noinline int prepare_pages(struct btrfs_root *root, struct file *file,
-			 struct page **pages, size_t num_pages,
-			 loff_t pos, unsigned long first_index,
-			 size_t write_bytes, bool force_uptodate)
+static noinline int prepare_pages(struct inode *inode, struct page **pages,
+				  size_t num_pages, loff_t pos,
+				  size_t write_bytes, bool force_uptodate)
 {
 	struct extent_state *cached_state = NULL;
 	int i;
 	unsigned long index = pos >> PAGE_CACHE_SHIFT;
-	struct inode *inode = file_inode(file);
 	gfp_t mask = btrfs_alloc_write_mask(inode->i_mapping);
 	int err = 0;
 	int faili = 0;
 	u64 start_pos;
 	u64 last_pos;
 
-	start_pos = pos & ~((u64)root->sectorsize - 1);
+	start_pos = pos & ~((u64)PAGE_CACHE_SIZE - 1);
 	last_pos = ((u64)index + num_pages) << PAGE_CACHE_SHIFT;
 
 again:
@@ -1462,8 +1460,8 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 		 * pages we want, so we don't really need to worry about the
 		 * contents of pages from loop to loop
 		 */
-		ret = prepare_pages(root, file, pages, num_pages,
-				    pos, first_index, write_bytes,
+		ret = prepare_pages(inode, pages, num_pages,
+				    pos, write_bytes,
 				    force_page_uptodate);
 		if (ret)
 			break;

commit 16e7549f045d33b0c5b0ebf19d08439e9221d40c
Author: Josef Bacik <jbacik@fusionio.com>
Date:   Tue Oct 22 12:18:51 2013 -0400

    Btrfs: incompatible format change to remove hole extents
    
    Btrfs has always had these filler extent data items for holes in inodes.  This
    has made somethings very easy, like logging hole punches and sending hole
    punches.  However for large holey files these extent data items are pure
    overhead.  So add an incompatible feature to no longer add hole extents to
    reduce the amount of metadata used by these sort of files.  This has a few
    changes for logging and send obviously since they will need to detect holes and
    log/send the holes if there are any.  I've tested this thoroughly with xfstests
    and it doesn't cause any issues with and without the incompat format set.
    Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 82d0342763c5..c77da440146a 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1963,11 +1963,13 @@ static int fill_holes(struct btrfs_trans_handle *trans, struct inode *inode,
 	struct btrfs_key key;
 	int ret;
 
+	if (btrfs_fs_incompat(root->fs_info, NO_HOLES))
+		goto out;
+
 	key.objectid = btrfs_ino(inode);
 	key.type = BTRFS_EXTENT_DATA_KEY;
 	key.offset = offset;
 
-
 	ret = btrfs_search_slot(trans, root, &key, path, 0, 1);
 	if (ret < 0)
 		return ret;
@@ -2064,8 +2066,10 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 	u64 drop_end;
 	int ret = 0;
 	int err = 0;
+	int rsv_count;
 	bool same_page = ((offset >> PAGE_CACHE_SHIFT) ==
 			  ((offset + len - 1) >> PAGE_CACHE_SHIFT));
+	bool no_holes = btrfs_fs_incompat(root->fs_info, NO_HOLES);
 
 	ret = btrfs_wait_ordered_range(inode, offset, len);
 	if (ret)
@@ -2163,9 +2167,10 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 	/*
 	 * 1 - update the inode
 	 * 1 - removing the extents in the range
-	 * 1 - adding the hole extent
+	 * 1 - adding the hole extent if no_holes isn't set
 	 */
-	trans = btrfs_start_transaction(root, 3);
+	rsv_count = no_holes ? 2 : 3;
+	trans = btrfs_start_transaction(root, rsv_count);
 	if (IS_ERR(trans)) {
 		err = PTR_ERR(trans);
 		goto out_free;
@@ -2202,7 +2207,7 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 		btrfs_end_transaction(trans, root);
 		btrfs_btree_balance_dirty(root);
 
-		trans = btrfs_start_transaction(root, 3);
+		trans = btrfs_start_transaction(root, rsv_count);
 		if (IS_ERR(trans)) {
 			ret = PTR_ERR(trans);
 			trans = NULL;

commit 678712545b62715a6c867471320ff5f60a521f3a
Author: Dulshani Gunawardhana <dulshani.gunawardhana89@gmail.com>
Date:   Thu Oct 31 10:33:04 2013 +0530

    btrfs: Fix checkpatch.pl warning of spacing issues
    
    Fix spacing issues detected via checkpatch.pl in accordance with the
    kernel style guidelines.
    
    Signed-off-by: Dulshani Gunawardhana <dulshani.gunawardhana89@gmail.com>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 3a20a12513b2..82d0342763c5 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -369,7 +369,7 @@ int btrfs_run_defrag_inodes(struct btrfs_fs_info *fs_info)
 	u64 root_objectid = 0;
 
 	atomic_inc(&fs_info->defrag_running);
-	while(1) {
+	while (1) {
 		/* Pause the auto defragger. */
 		if (test_bit(BTRFS_FS_STATE_REMOUNTING,
 			     &fs_info->fs_state))

commit 0ef8b726075aa6931ddf1c16f5bae043eef184f9
Author: Josef Bacik <jbacik@fusionio.com>
Date:   Fri Oct 25 16:13:35 2013 -0400

    Btrfs: return an error from btrfs_wait_ordered_range
    
    I noticed that if the free space cache has an error writing out it's data it
    won't actually error out, it will just carry on.  This is because it doesn't
    check the return value of btrfs_wait_ordered_range, which didn't actually return
    anything.  So fix this in order to keep us from making free space cache look
    valid when it really isnt.  Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 14b41d569a3e..3a20a12513b2 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1280,6 +1280,7 @@ static noinline int prepare_pages(struct btrfs_root *root, struct file *file,
 		}
 		wait_on_page_writeback(pages[i]);
 	}
+	faili = num_pages - 1;
 	err = 0;
 	if (start_pos < inode->i_size) {
 		struct btrfs_ordered_extent *ordered;
@@ -1298,8 +1299,10 @@ static noinline int prepare_pages(struct btrfs_root *root, struct file *file,
 				unlock_page(pages[i]);
 				page_cache_release(pages[i]);
 			}
-			btrfs_wait_ordered_range(inode, start_pos,
-						 last_pos - start_pos);
+			err = btrfs_wait_ordered_range(inode, start_pos,
+						       last_pos - start_pos);
+			if (err)
+				goto fail;
 			goto again;
 		}
 		if (ordered)
@@ -1808,8 +1811,13 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 	atomic_inc(&root->log_batch);
 	full_sync = test_bit(BTRFS_INODE_NEEDS_FULL_SYNC,
 			     &BTRFS_I(inode)->runtime_flags);
-	if (full_sync)
-		btrfs_wait_ordered_range(inode, start, end - start + 1);
+	if (full_sync) {
+		ret = btrfs_wait_ordered_range(inode, start, end - start + 1);
+		if (ret) {
+			mutex_unlock(&inode->i_mutex);
+			goto out;
+		}
+	}
 	atomic_inc(&root->log_batch);
 
 	/*
@@ -1875,27 +1883,20 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 	mutex_unlock(&inode->i_mutex);
 
 	if (ret != BTRFS_NO_LOG_SYNC) {
-		if (ret > 0) {
-			/*
-			 * If we didn't already wait for ordered extents we need
-			 * to do that now.
-			 */
-			if (!full_sync)
-				btrfs_wait_ordered_range(inode, start,
-							 end - start + 1);
-			ret = btrfs_commit_transaction(trans, root);
-		} else {
+		if (!ret) {
 			ret = btrfs_sync_log(trans, root);
-			if (ret == 0) {
+			if (!ret) {
 				ret = btrfs_end_transaction(trans, root);
-			} else {
-				if (!full_sync)
-					btrfs_wait_ordered_range(inode, start,
-								 end -
-								 start + 1);
-				ret = btrfs_commit_transaction(trans, root);
+				goto out;
 			}
 		}
+		if (!full_sync) {
+			ret = btrfs_wait_ordered_range(inode, start,
+						       end - start + 1);
+			if (ret)
+				goto out;
+		}
+		ret = btrfs_commit_transaction(trans, root);
 	} else {
 		ret = btrfs_end_transaction(trans, root);
 	}
@@ -2066,7 +2067,9 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 	bool same_page = ((offset >> PAGE_CACHE_SHIFT) ==
 			  ((offset + len - 1) >> PAGE_CACHE_SHIFT));
 
-	btrfs_wait_ordered_range(inode, offset, len);
+	ret = btrfs_wait_ordered_range(inode, offset, len);
+	if (ret)
+		return ret;
 
 	mutex_lock(&inode->i_mutex);
 	/*
@@ -2135,8 +2138,12 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 			btrfs_put_ordered_extent(ordered);
 		unlock_extent_cached(&BTRFS_I(inode)->io_tree, lockstart,
 				     lockend, &cached_state, GFP_NOFS);
-		btrfs_wait_ordered_range(inode, lockstart,
-					 lockend - lockstart + 1);
+		ret = btrfs_wait_ordered_range(inode, lockstart,
+					       lockend - lockstart + 1);
+		if (ret) {
+			mutex_unlock(&inode->i_mutex);
+			return ret;
+		}
 	}
 
 	path = btrfs_alloc_path();
@@ -2307,7 +2314,10 @@ static long btrfs_fallocate(struct file *file, int mode,
 	 * wait for ordered IO before we have any locks.  We'll loop again
 	 * below with the locks held.
 	 */
-	btrfs_wait_ordered_range(inode, alloc_start, alloc_end - alloc_start);
+	ret = btrfs_wait_ordered_range(inode, alloc_start,
+				       alloc_end - alloc_start);
+	if (ret)
+		goto out;
 
 	locked_end = alloc_end - 1;
 	while (1) {
@@ -2331,8 +2341,10 @@ static long btrfs_fallocate(struct file *file, int mode,
 			 * we can't wait on the range with the transaction
 			 * running or with the extent lock held
 			 */
-			btrfs_wait_ordered_range(inode, alloc_start,
-						 alloc_end - alloc_start);
+			ret = btrfs_wait_ordered_range(inode, alloc_start,
+						       alloc_end - alloc_start);
+			if (ret)
+				goto out;
 		} else {
 			if (ordered)
 				btrfs_put_ordered_extent(ordered);

commit 8b558c5f097b636209b654f4d7775ac96054d6e3
Author: Zach Brown <zab@redhat.com>
Date:   Wed Oct 16 12:10:34 2013 -0700

    btrfs: remove fs/btrfs/compat.h
    
    fs/btrfs/compat.h only contained trivial macro wrappers of drop_nlink()
    and inc_nlink().  This doesn't belong in mainline.
    
    Signed-off-by: Zach Brown <zab@redhat.com>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index bf3465cf455a..14b41d569a3e 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -39,7 +39,6 @@
 #include "print-tree.h"
 #include "tree-log.h"
 #include "locking.h"
-#include "compat.h"
 #include "volumes.h"
 
 static struct kmem_cache *btrfs_inode_defrag_cachep;

commit 7f4ca37c486733da008778a1f4058fbc194a4fdd
Author: Josef Bacik <jbacik@fusionio.com>
Date:   Fri Oct 18 11:44:46 2013 -0400

    Btrfs: fix up seek_hole/seek_data handling
    
    Whoever wrote this was braindead.  Also it doesn't work right if you have
    VACANCY's since we assumed you would only have that at the end of the file,
    which won't be the case in the near future.  I tested this with generic/285 and
    generic/286 as well as the btrfs tests that use fssum since it uses
    seek_hole/seek_data to verify things are ok.  Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 72da4df53c9a..bf3465cf455a 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2405,14 +2405,12 @@ static long btrfs_fallocate(struct file *file, int mode,
 static int find_desired_extent(struct inode *inode, loff_t *offset, int whence)
 {
 	struct btrfs_root *root = BTRFS_I(inode)->root;
-	struct extent_map *em;
+	struct extent_map *em = NULL;
 	struct extent_state *cached_state = NULL;
 	u64 lockstart = *offset;
 	u64 lockend = i_size_read(inode);
 	u64 start = *offset;
-	u64 orig_start = *offset;
 	u64 len = i_size_read(inode);
-	u64 last_end = 0;
 	int ret = 0;
 
 	lockend = max_t(u64, root->sectorsize, lockend);
@@ -2429,89 +2427,35 @@ static int find_desired_extent(struct inode *inode, loff_t *offset, int whence)
 	lock_extent_bits(&BTRFS_I(inode)->io_tree, lockstart, lockend, 0,
 			 &cached_state);
 
-	/*
-	 * Delalloc is such a pain.  If we have a hole and we have pending
-	 * delalloc for a portion of the hole we will get back a hole that
-	 * exists for the entire range since it hasn't been actually written
-	 * yet.  So to take care of this case we need to look for an extent just
-	 * before the position we want in case there is outstanding delalloc
-	 * going on here.
-	 */
-	if (whence == SEEK_HOLE && start != 0) {
-		if (start <= root->sectorsize)
-			em = btrfs_get_extent_fiemap(inode, NULL, 0, 0,
-						     root->sectorsize, 0);
-		else
-			em = btrfs_get_extent_fiemap(inode, NULL, 0,
-						     start - root->sectorsize,
-						     root->sectorsize, 0);
-		if (IS_ERR(em)) {
-			ret = PTR_ERR(em);
-			goto out;
-		}
-		last_end = em->start + em->len;
-		if (em->block_start == EXTENT_MAP_DELALLOC)
-			last_end = min_t(u64, last_end, inode->i_size);
-		free_extent_map(em);
-	}
-
-	while (1) {
+	while (start < inode->i_size) {
 		em = btrfs_get_extent_fiemap(inode, NULL, 0, start, len, 0);
 		if (IS_ERR(em)) {
 			ret = PTR_ERR(em);
+			em = NULL;
 			break;
 		}
 
-		if (em->block_start == EXTENT_MAP_HOLE) {
-			if (test_bit(EXTENT_FLAG_VACANCY, &em->flags)) {
-				if (last_end <= orig_start) {
-					free_extent_map(em);
-					ret = -ENXIO;
-					break;
-				}
-			}
-
-			if (whence == SEEK_HOLE) {
-				*offset = start;
-				free_extent_map(em);
-				break;
-			}
-		} else {
-			if (whence == SEEK_DATA) {
-				if (em->block_start == EXTENT_MAP_DELALLOC) {
-					if (start >= inode->i_size) {
-						free_extent_map(em);
-						ret = -ENXIO;
-						break;
-					}
-				}
-
-				if (!test_bit(EXTENT_FLAG_PREALLOC,
-					      &em->flags)) {
-					*offset = start;
-					free_extent_map(em);
-					break;
-				}
-			}
-		}
+		if (whence == SEEK_HOLE &&
+		    (em->block_start == EXTENT_MAP_HOLE ||
+		     test_bit(EXTENT_FLAG_PREALLOC, &em->flags)))
+			break;
+		else if (whence == SEEK_DATA &&
+			   (em->block_start != EXTENT_MAP_HOLE &&
+			    !test_bit(EXTENT_FLAG_PREALLOC, &em->flags)))
+			break;
 
 		start = em->start + em->len;
-		last_end = em->start + em->len;
-
-		if (em->block_start == EXTENT_MAP_DELALLOC)
-			last_end = min_t(u64, last_end, inode->i_size);
-
-		if (test_bit(EXTENT_FLAG_VACANCY, &em->flags)) {
-			free_extent_map(em);
-			ret = -ENXIO;
-			break;
-		}
 		free_extent_map(em);
+		em = NULL;
 		cond_resched();
 	}
-	if (!ret)
-		*offset = min(*offset, inode->i_size);
-out:
+	free_extent_map(em);
+	if (!ret) {
+		if (whence == SEEK_DATA && start >= inode->i_size)
+			ret = -ENXIO;
+		else
+			*offset = min_t(loff_t, start, inode->i_size);
+	}
 	unlock_extent_cached(&BTRFS_I(inode)->io_tree, lockstart, lockend,
 			     &cached_state, GFP_NOFS);
 	return ret;

commit 0fbf2cc983ca15208545010863c6536d36a25f3a
Merge: c43a3855f41a 94aebfb2e7d8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Sep 22 14:58:49 2013 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs
    
    Pull btrfs fixes from Chris Mason:
     "These are mostly bug fixes and a two small performance fixes.  The
      most important of the bunch are Josef's fix for a snapshotting
      regression and Mark's update to fix compile problems on arm"
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs: (25 commits)
      Btrfs: create the uuid tree on remount rw
      btrfs: change extent-same to copy entire argument struct
      Btrfs: dir_inode_operations should use btrfs_update_time also
      btrfs: Add btrfs: prefix to kernel log output
      btrfs: refuse to remount read-write after abort
      Btrfs: btrfs_ioctl_default_subvol: Revert back to toplevel subvolume when arg is 0
      Btrfs: don't leak transaction in btrfs_sync_file()
      Btrfs: add the missing mutex unlock in write_all_supers()
      Btrfs: iput inode on allocation failure
      Btrfs: remove space_info->reservation_progress
      Btrfs: kill delay_iput arg to the wait_ordered functions
      Btrfs: fix worst case calculator for space usage
      Revert "Btrfs: rework the overcommit logic to be based on the total size"
      Btrfs: improve replacing nocow extents
      Btrfs: drop dir i_size when adding new names on replay
      Btrfs: replay dir_index items before other items
      Btrfs: check roots last log commit when checking if an inode has been logged
      Btrfs: actually log directory we are fsync()'ing
      Btrfs: actually limit the size of delalloc range
      Btrfs: allocate the free space by the existed max extent size when ENOSPC
      ...

commit a0634be562c27ddee7c94bce4f765f8071244e07
Author: Filipe David Borba Manana <fdmanana@gmail.com>
Date:   Wed Sep 11 20:36:44 2013 +0100

    Btrfs: don't leak transaction in btrfs_sync_file()
    
    In btrfs_sync_file(), if the call to btrfs_log_dentry_safe() returns
    a negative error (for e.g. -ENOMEM via btrfs_log_inode()), we would
    return without ending/freeing the transaction.
    
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 5ba87b0d2ef8..d12107e90987 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1859,8 +1859,8 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 
 	ret = btrfs_log_dentry_safe(trans, root, dentry);
 	if (ret < 0) {
-		mutex_unlock(&inode->i_mutex);
-		goto out;
+		/* Fallthrough and commit/free transaction. */
+		ret = 1;
 	}
 
 	/* we've logged all the items and now have a consistent

commit b7c09ad4014e3678e8cc01fdf663c9f43b272dc6
Merge: 1812997720ab d7396f07358a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Sep 12 09:58:51 2013 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs
    
    Pull btrfs updates from Chris Mason:
     "This is against 3.11-rc7, but was pulled and tested against your tree
      as of yesterday.  We do have two small incrementals queued up, but I
      wanted to get this bunch out the door before I hop on an airplane.
    
      This is a fairly large batch of fixes, performance improvements, and
      cleanups from the usual Btrfs suspects.
    
      We've included Stefan Behren's work to index subvolume UUIDs, which is
      targeted at speeding up send/receive with many subvolumes or snapshots
      in place.  It closes a long standing performance issue that was built
      in to the disk format.
    
      Mark Fasheh's offline dedup work is also here.  In this case offline
      means the FS is mounted and active, but the dedup work is not done
      inline during file IO.  This is a building block where utilities are
      able to ask the FS to dedup a series of extents.  The kernel takes
      care of verifying the data involved really is the same.  Today this
      involves reading both extents, but we'll continue to evolve the
      patches"
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs: (118 commits)
      Btrfs: optimize key searches in btrfs_search_slot
      Btrfs: don't use an async starter for most of our workers
      Btrfs: only update disk_i_size as we remove extents
      Btrfs: fix deadlock in uuid scan kthread
      Btrfs: stop refusing the relocation of chunk 0
      Btrfs: fix memory leak of uuid_root in free_fs_info
      btrfs: reuse kbasename helper
      btrfs: return btrfs error code for dev excl ops err
      Btrfs: allow partial ordered extent completion
      Btrfs: convert all bug_ons in free-space-cache.c
      Btrfs: add support for asserts
      Btrfs: adjust the fs_devices->missing count on unmount
      Btrf: cleanup: don't check for root_refs == 0 twice
      Btrfs: fix for patch "cleanup: don't check the same thing twice"
      Btrfs: get rid of one BUG() in write_all_supers()
      Btrfs: allocate prelim_ref with a slab allocater
      Btrfs: pass gfp_t to __add_prelim_ref() to avoid always using GFP_ATOMIC
      Btrfs: fix race conditions in BTRFS_IOC_FS_INFO ioctl
      Btrfs: fix race between removing a dev and writing sbs
      Btrfs: remove ourselves from the cluster list under lock
      ...

commit 02afc27faec94c9e068517a22acf55400976c698
Author: Christoph Hellwig <hch@infradead.org>
Date:   Wed Sep 4 15:04:40 2013 +0200

    direct-io: Handle O_(D)SYNC AIO
    
    Call generic_write_sync() from the deferred I/O completion handler if
    O_DSYNC is set for a write request.  Also make sure various callers
    don't call generic_write_sync if the direct I/O code returns
    -EIOCBQUEUED.
    
    Based on an earlier patch from Jan Kara <jack@suse.cz> with updates from
    Jeff Moyer <jmoyer@redhat.com> and Darrick J. Wong <darrick.wong@oracle.com>.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 8e686a427ce2..4d2eb6417145 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1727,7 +1727,7 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 	 */
 	BTRFS_I(inode)->last_trans = root->fs_info->generation + 1;
 	BTRFS_I(inode)->last_sub_trans = root->log_transid;
-	if (num_written > 0 || num_written == -EIOCBQUEUED) {
+	if (num_written > 0) {
 		err = generic_write_sync(file, pos, num_written);
 		if (err < 0 && num_written > 0)
 			num_written = err;

commit 23fa76b0ba78b7d84708d9ee683587d8a5bbceef
Author: Stefan Behrens <sbehrens@giantdisaster.de>
Date:   Fri Aug 23 10:34:43 2013 +0200

    Btrf: cleanup: don't check for root_refs == 0 twice
    
    btrfs_read_fs_root_no_name() already checks if btrfs_root_refs()
    is zero and returns ENOENT in this case. There is no need to do
    it again in three more places.
    
    Signed-off-by: Stefan Behrens <sbehrens@giantdisaster.de>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 5e7ea996f105..5ba87b0d2ef8 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -310,11 +310,6 @@ static int __btrfs_run_defrag_inode(struct btrfs_fs_info *fs_info,
 		goto cleanup;
 	}
 
-	if (btrfs_root_refs(&inode_root->root_item) == 0) {
-		ret = -ENOENT;
-		goto cleanup;
-	}
-
 	key.objectid = defrag->ino;
 	btrfs_set_key_type(&key, BTRFS_INODE_ITEM_KEY);
 	key.offset = 0;

commit 00361589d2eebd90fca022148c763e40d3e90871
Author: Josef Bacik <jbacik@fusionio.com>
Date:   Wed Aug 14 14:02:47 2013 -0400

    Btrfs: avoid starting a transaction in the write path
    
    I noticed while looking at a deadlock that we are always starting a transaction
    in cow_file_range().  This isn't really needed since we only need a transaction
    if we are doing an inline extent, or if the allocator needs to allocate a chunk.
    So push down all the transaction start stuff to be closer to where we actually
    need a transaction in all of these cases.  This will hopefully reduce our write
    latency when we are committing often.  Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 8cc941e5b3e1..5e7ea996f105 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1339,7 +1339,6 @@ static noinline int prepare_pages(struct btrfs_root *root, struct file *file,
 static noinline int check_can_nocow(struct inode *inode, loff_t pos,
 				    size_t *write_bytes)
 {
-	struct btrfs_trans_handle *trans;
 	struct btrfs_root *root = BTRFS_I(inode)->root;
 	struct btrfs_ordered_extent *ordered;
 	u64 lockstart, lockend;
@@ -1361,16 +1360,8 @@ static noinline int check_can_nocow(struct inode *inode, loff_t pos,
 		btrfs_put_ordered_extent(ordered);
 	}
 
-	trans = btrfs_join_transaction(root);
-	if (IS_ERR(trans)) {
-		unlock_extent(&BTRFS_I(inode)->io_tree, lockstart, lockend);
-		return PTR_ERR(trans);
-	}
-
 	num_bytes = lockend - lockstart + 1;
-	ret = can_nocow_extent(trans, inode, lockstart, &num_bytes, NULL, NULL,
-			       NULL);
-	btrfs_end_transaction(trans, root);
+	ret = can_nocow_extent(inode, lockstart, &num_bytes, NULL, NULL, NULL);
 	if (ret <= 0) {
 		ret = 0;
 	} else {

commit 2112ac800d43e795323bc18558e43fd3641da4ed
Author: Josef Bacik <jbacik@fusionio.com>
Date:   Fri Jul 26 09:13:15 2013 -0400

    Btrfs: don't bother autodefragging if our root is going away
    
    We can end up with inodes on the auto defrag list that exist on roots that are
    going to be deleted.  This is extra work we don't need to do, so just bail if
    our root has 0 root refs.  Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 8e686a427ce2..8cc941e5b3e1 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -310,6 +310,11 @@ static int __btrfs_run_defrag_inode(struct btrfs_fs_info *fs_info,
 		goto cleanup;
 	}
 
+	if (btrfs_root_refs(&inode_root->root_item) == 0) {
+		ret = -ENOENT;
+		goto cleanup;
+	}
+
 	key.objectid = defrag->ino;
 	btrfs_set_key_type(&key, BTRFS_INODE_ITEM_KEY);
 	key.offset = 0;

commit ee20a98314e52a6675e94d1a07ca205ffdf09a72
Author: Josef Bacik <jbacik@fusionio.com>
Date:   Thu Jul 11 10:34:59 2013 -0400

    Btrfs: allow splitting of hole em's when dropping extent cache
    
    I noticed while running multi-threaded fsync tests that sometimes fsck would
    complain about an improper gap.  This happens because we fail to add a hole
    extent to the file, which was happening when we'd split a hole EM because
    btrfs_drop_extent_cache was just discarding the whole em instead of splitting
    it.  So this patch fixes this by allowing us to split a hole em properly, which
    means that added holes actually get logged properly and we no longer see this
    fsck error.  Thankfully we're tolerant of these sort of problems so a user would
    not see any adverse effects of this bug, other than fsck complaining.  Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index a005fe2c072a..8e686a427ce2 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -596,20 +596,29 @@ void btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
 		if (no_splits)
 			goto next;
 
-		if (em->block_start < EXTENT_MAP_LAST_BYTE &&
-		    em->start < start) {
+		if (em->start < start) {
 			split->start = em->start;
 			split->len = start - em->start;
-			split->orig_start = em->orig_start;
-			split->block_start = em->block_start;
 
-			if (compressed)
-				split->block_len = em->block_len;
-			else
-				split->block_len = split->len;
-			split->ram_bytes = em->ram_bytes;
-			split->orig_block_len = max(split->block_len,
-						    em->orig_block_len);
+			if (em->block_start < EXTENT_MAP_LAST_BYTE) {
+				split->orig_start = em->orig_start;
+				split->block_start = em->block_start;
+
+				if (compressed)
+					split->block_len = em->block_len;
+				else
+					split->block_len = split->len;
+				split->orig_block_len = max(split->block_len,
+						em->orig_block_len);
+				split->ram_bytes = em->ram_bytes;
+			} else {
+				split->orig_start = split->start;
+				split->block_len = 0;
+				split->block_start = em->block_start;
+				split->orig_block_len = 0;
+				split->ram_bytes = split->len;
+			}
+
 			split->generation = gen;
 			split->bdev = em->bdev;
 			split->flags = flags;
@@ -620,8 +629,7 @@ void btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
 			split = split2;
 			split2 = NULL;
 		}
-		if (em->block_start < EXTENT_MAP_LAST_BYTE &&
-		    testend && em->start + em->len > start + len) {
+		if (testend && em->start + em->len > start + len) {
 			u64 diff = start + len - em->start;
 
 			split->start = start + len;
@@ -630,18 +638,28 @@ void btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
 			split->flags = flags;
 			split->compress_type = em->compress_type;
 			split->generation = gen;
-			split->orig_block_len = max(em->block_len,
+
+			if (em->block_start < EXTENT_MAP_LAST_BYTE) {
+				split->orig_block_len = max(em->block_len,
 						    em->orig_block_len);
-			split->ram_bytes = em->ram_bytes;
 
-			if (compressed) {
-				split->block_len = em->block_len;
-				split->block_start = em->block_start;
-				split->orig_start = em->orig_start;
+				split->ram_bytes = em->ram_bytes;
+				if (compressed) {
+					split->block_len = em->block_len;
+					split->block_start = em->block_start;
+					split->orig_start = em->orig_start;
+				} else {
+					split->block_len = split->len;
+					split->block_start = em->block_start
+						+ diff;
+					split->orig_start = em->orig_start;
+				}
 			} else {
-				split->block_len = split->len;
-				split->block_start = em->block_start + diff;
-				split->orig_start = em->orig_start;
+				split->ram_bytes = split->len;
+				split->orig_start = split->start;
+				split->block_len = 0;
+				split->block_start = em->block_start;
+				split->orig_block_len = 0;
 			}
 
 			ret = add_extent_mapping(em_tree, split, modified);

commit e3a0dd98e1ddfd135b7ef889fcc0269e8c2ca445
Merge: da89bd213fe7 0e267c44c3a4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jul 9 12:33:09 2013 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs
    
    Pull btrfs update from Chris Mason:
     "These are the usual mixture of bugs, cleanups and performance fixes.
      Miao has some really nice tuning of our crc code as well as our
      transaction commits.
    
      Josef is peeling off more and more problems related to early enospc,
      and has a number of important bug fixes in here too"
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs: (81 commits)
      Btrfs: wait ordered range before doing direct io
      Btrfs: only do the tree_mod_log_free_eb if this is our last ref
      Btrfs: hold the tree mod lock in __tree_mod_log_rewind
      Btrfs: make backref walking code handle skinny metadata
      Btrfs: fix crash regarding to ulist_add_merge
      Btrfs: fix several potential problems in copy_nocow_pages_for_inode
      Btrfs: cleanup the code of copy_nocow_pages_for_inode()
      Btrfs: fix oops when recovering the file data by scrub function
      Btrfs: make the chunk allocator completely tree lockless
      Btrfs: cleanup orphaned root orphan item
      Btrfs: fix wrong mirror number tuning
      Btrfs: cleanup redundant code in btrfs_submit_direct()
      Btrfs: remove btrfs_sector_sum structure
      Btrfs: check if we can nocow if we don't have data space
      Btrfs: stop using try_to_writeback_inodes_sb_nr to flush delalloc
      Btrfs: use a percpu to keep track of possibly pinned bytes
      Btrfs: check for actual acls rather than just xattrs when caching no acl
      Btrfs: move btrfs_truncate_page to btrfs_cont_expand instead of btrfs_truncate
      Btrfs: optimize reada_for_balance
      Btrfs: optimize read_block_for_search
      ...

commit 46a1c2c7ae53de2a5676754b54a73c591a3951d2
Author: Jie Liu <jeff.liu@oracle.com>
Date:   Tue Jun 25 12:02:13 2013 +0800

    vfs: export lseek_execute() to modules
    
    For those file systems(btrfs/ext4/ocfs2/tmpfs) that support
    SEEK_DATA/SEEK_HOLE functions, we end up handling the similar
    matter in lseek_execute() to update the current file offset
    to the desired offset if it is valid, ceph also does the
    simliar things at ceph_llseek().
    
    To reduce the duplications, this patch make lseek_execute()
    public accessible so that we can call it directly from the
    underlying file systems.
    
    Thanks Dave Chinner for this suggestion.
    
    [AV: call it vfs_setpos(), don't bring the removed 'inode' argument back]
    
    v2->v1:
    - Add kernel-doc comments for lseek_execute()
    - Call lseek_execute() in ceph->llseek()
    
    Signed-off-by: Jie Liu <jeff.liu@oracle.com>
    Cc: Dave Chinner <dchinner@redhat.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Chris Mason <chris.mason@fusionio.com>
    Cc: Josef Bacik <jbacik@fusionio.com>
    Cc: Ben Myers <bpm@sgi.com>
    Cc: Ted Tso <tytso@mit.edu>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Mark Fasheh <mfasheh@suse.com>
    Cc: Joel Becker <jlbec@evilplan.org>
    Cc: Sage Weil <sage@inktank.com>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 4205ba752d40..89da56a58b63 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2425,20 +2425,7 @@ static loff_t btrfs_file_llseek(struct file *file, loff_t offset, int whence)
 		}
 	}
 
-	if (offset < 0 && !(file->f_mode & FMODE_UNSIGNED_OFFSET)) {
-		offset = -EINVAL;
-		goto out;
-	}
-	if (offset > inode->i_sb->s_maxbytes) {
-		offset = -EINVAL;
-		goto out;
-	}
-
-	/* Special lock needed here? */
-	if (offset != file->f_pos) {
-		file->f_pos = offset;
-		file->f_version = 0;
-	}
+	offset = vfs_setpos(file, offset, inode->i_sb->s_maxbytes);
 out:
 	mutex_unlock(&inode->i_mutex);
 	return offset;

commit 7ee9e4405f264e9eda808aa5ca4522746a1af9c1
Author: Josef Bacik <jbacik@fusionio.com>
Date:   Fri Jun 21 16:37:03 2013 -0400

    Btrfs: check if we can nocow if we don't have data space
    
    We always just try and reserve data space when we write, but if we are out of
    space but have prealloc'ed extents we should still successfully write.  This
    patch will try and see if we can write to prealloc'ed space and if we can go
    ahead and allow the write to continue.  With this patch we now pass xfstests
    generic/274.  Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 5ffde5603686..2d70849cec92 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1312,6 +1312,56 @@ static noinline int prepare_pages(struct btrfs_root *root, struct file *file,
 
 }
 
+static noinline int check_can_nocow(struct inode *inode, loff_t pos,
+				    size_t *write_bytes)
+{
+	struct btrfs_trans_handle *trans;
+	struct btrfs_root *root = BTRFS_I(inode)->root;
+	struct btrfs_ordered_extent *ordered;
+	u64 lockstart, lockend;
+	u64 num_bytes;
+	int ret;
+
+	lockstart = round_down(pos, root->sectorsize);
+	lockend = lockstart + round_up(*write_bytes, root->sectorsize) - 1;
+
+	while (1) {
+		lock_extent(&BTRFS_I(inode)->io_tree, lockstart, lockend);
+		ordered = btrfs_lookup_ordered_range(inode, lockstart,
+						     lockend - lockstart + 1);
+		if (!ordered) {
+			break;
+		}
+		unlock_extent(&BTRFS_I(inode)->io_tree, lockstart, lockend);
+		btrfs_start_ordered_extent(inode, ordered, 1);
+		btrfs_put_ordered_extent(ordered);
+	}
+
+	trans = btrfs_join_transaction(root);
+	if (IS_ERR(trans)) {
+		unlock_extent(&BTRFS_I(inode)->io_tree, lockstart, lockend);
+		return PTR_ERR(trans);
+	}
+
+	num_bytes = lockend - lockstart + 1;
+	ret = can_nocow_extent(trans, inode, lockstart, &num_bytes, NULL, NULL,
+			       NULL);
+	btrfs_end_transaction(trans, root);
+	if (ret <= 0) {
+		ret = 0;
+	} else {
+		clear_extent_bit(&BTRFS_I(inode)->io_tree, lockstart, lockend,
+				 EXTENT_DIRTY | EXTENT_DELALLOC |
+				 EXTENT_DO_ACCOUNTING | EXTENT_DEFRAG, 0, 0,
+				 NULL, GFP_NOFS);
+		*write_bytes = min_t(size_t, *write_bytes, num_bytes);
+	}
+
+	unlock_extent(&BTRFS_I(inode)->io_tree, lockstart, lockend);
+
+	return ret;
+}
+
 static noinline ssize_t __btrfs_buffered_write(struct file *file,
 					       struct iov_iter *i,
 					       loff_t pos)
@@ -1319,10 +1369,12 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 	struct inode *inode = file_inode(file);
 	struct btrfs_root *root = BTRFS_I(inode)->root;
 	struct page **pages = NULL;
+	u64 release_bytes = 0;
 	unsigned long first_index;
 	size_t num_written = 0;
 	int nrptrs;
 	int ret = 0;
+	bool only_release_metadata = false;
 	bool force_page_uptodate = false;
 
 	nrptrs = min((iov_iter_count(i) + PAGE_CACHE_SIZE - 1) /
@@ -1343,6 +1395,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 					 offset);
 		size_t num_pages = (write_bytes + offset +
 				    PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT;
+		size_t reserve_bytes;
 		size_t dirty_pages;
 		size_t copied;
 
@@ -1357,11 +1410,41 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 			break;
 		}
 
-		ret = btrfs_delalloc_reserve_space(inode,
-					num_pages << PAGE_CACHE_SHIFT);
+		reserve_bytes = num_pages << PAGE_CACHE_SHIFT;
+		ret = btrfs_check_data_free_space(inode, reserve_bytes);
+		if (ret == -ENOSPC &&
+		    (BTRFS_I(inode)->flags & (BTRFS_INODE_NODATACOW |
+					      BTRFS_INODE_PREALLOC))) {
+			ret = check_can_nocow(inode, pos, &write_bytes);
+			if (ret > 0) {
+				only_release_metadata = true;
+				/*
+				 * our prealloc extent may be smaller than
+				 * write_bytes, so scale down.
+				 */
+				num_pages = (write_bytes + offset +
+					     PAGE_CACHE_SIZE - 1) >>
+					PAGE_CACHE_SHIFT;
+				reserve_bytes = num_pages << PAGE_CACHE_SHIFT;
+				ret = 0;
+			} else {
+				ret = -ENOSPC;
+			}
+		}
+
 		if (ret)
 			break;
 
+		ret = btrfs_delalloc_reserve_metadata(inode, reserve_bytes);
+		if (ret) {
+			if (!only_release_metadata)
+				btrfs_free_reserved_data_space(inode,
+							       reserve_bytes);
+			break;
+		}
+
+		release_bytes = reserve_bytes;
+
 		/*
 		 * This is going to setup the pages array with the number of
 		 * pages we want, so we don't really need to worry about the
@@ -1370,11 +1453,8 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 		ret = prepare_pages(root, file, pages, num_pages,
 				    pos, first_index, write_bytes,
 				    force_page_uptodate);
-		if (ret) {
-			btrfs_delalloc_release_space(inode,
-					num_pages << PAGE_CACHE_SHIFT);
+		if (ret)
 			break;
-		}
 
 		copied = btrfs_copy_from_user(pos, num_pages,
 					   write_bytes, pages, i);
@@ -1404,30 +1484,46 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 		 * managed to copy.
 		 */
 		if (num_pages > dirty_pages) {
+			release_bytes = (num_pages - dirty_pages) <<
+				PAGE_CACHE_SHIFT;
 			if (copied > 0) {
 				spin_lock(&BTRFS_I(inode)->lock);
 				BTRFS_I(inode)->outstanding_extents++;
 				spin_unlock(&BTRFS_I(inode)->lock);
 			}
-			btrfs_delalloc_release_space(inode,
-					(num_pages - dirty_pages) <<
-					PAGE_CACHE_SHIFT);
+			if (only_release_metadata)
+				btrfs_delalloc_release_metadata(inode,
+								release_bytes);
+			else
+				btrfs_delalloc_release_space(inode,
+							     release_bytes);
 		}
 
+		release_bytes = dirty_pages << PAGE_CACHE_SHIFT;
 		if (copied > 0) {
 			ret = btrfs_dirty_pages(root, inode, pages,
 						dirty_pages, pos, copied,
 						NULL);
 			if (ret) {
-				btrfs_delalloc_release_space(inode,
-					dirty_pages << PAGE_CACHE_SHIFT);
 				btrfs_drop_pages(pages, num_pages);
 				break;
 			}
 		}
 
+		release_bytes = 0;
 		btrfs_drop_pages(pages, num_pages);
 
+		if (only_release_metadata && copied > 0) {
+			u64 lockstart = round_down(pos, root->sectorsize);
+			u64 lockend = lockstart +
+				(dirty_pages << PAGE_CACHE_SHIFT) - 1;
+
+			set_extent_bit(&BTRFS_I(inode)->io_tree, lockstart,
+				       lockend, EXTENT_NORESERVE, NULL,
+				       NULL, GFP_NOFS);
+			only_release_metadata = false;
+		}
+
 		cond_resched();
 
 		balance_dirty_pages_ratelimited(inode->i_mapping);
@@ -1440,6 +1536,13 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 
 	kfree(pages);
 
+	if (release_bytes) {
+		if (only_release_metadata)
+			btrfs_delalloc_release_metadata(inode, release_bytes);
+		else
+			btrfs_delalloc_release_space(inode, release_bytes);
+	}
+
 	return num_written ? num_written : ret;
 }
 

commit a71754fc68f740b7ed46bb83123c63fbbc130611
Author: Josef Bacik <jbacik@fusionio.com>
Date:   Mon Jun 17 17:14:39 2013 -0400

    Btrfs: move btrfs_truncate_page to btrfs_cont_expand instead of btrfs_truncate
    
    This has plagued us forever and I'm so over working around it.  When we truncate
    down to a non-page aligned offset we will call btrfs_truncate_page to zero out
    the end of the page and write it back to disk, this will keep us from exposing
    stale data if we truncate back up from that point.  The problem with this is it
    requires data space to do this, and people don't really expect to get ENOSPC
    from truncate() for these sort of things.  This also tends to bite the orphan
    cleanup stuff too which keeps people from mounting.  To get around this we can
    just move this into btrfs_cont_expand() to make sure if we are truncating up
    from a non-page size aligned i_size we will zero out the rest of this page so
    that we don't expose stale data.  This will give ENOSPC if you try to truncate()
    up or if you try to write past the end of isize, which is much more reasonable.
    This fixes xfstests generic/083 failing to mount because of the orphan cleanup
    failing.  Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 185af15ad9e4..5ffde5603686 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2173,12 +2173,6 @@ static long btrfs_fallocate(struct file *file, int mode,
 			goto out_reserve_fail;
 	}
 
-	/*
-	 * wait for ordered IO before we have any locks.  We'll loop again
-	 * below with the locks held.
-	 */
-	btrfs_wait_ordered_range(inode, alloc_start, alloc_end - alloc_start);
-
 	mutex_lock(&inode->i_mutex);
 	ret = inode_newsize_ok(inode, alloc_end);
 	if (ret)
@@ -2189,8 +2183,23 @@ static long btrfs_fallocate(struct file *file, int mode,
 					alloc_start);
 		if (ret)
 			goto out;
+	} else {
+		/*
+		 * If we are fallocating from the end of the file onward we
+		 * need to zero out the end of the page if i_size lands in the
+		 * middle of a page.
+		 */
+		ret = btrfs_truncate_page(inode, inode->i_size, 0, 0);
+		if (ret)
+			goto out;
 	}
 
+	/*
+	 * wait for ordered IO before we have any locks.  We'll loop again
+	 * below with the locks held.
+	 */
+	btrfs_wait_ordered_range(inode, alloc_start, alloc_end - alloc_start);
+
 	locked_end = alloc_end - 1;
 	while (1) {
 		struct btrfs_ordered_extent *ordered;

commit 3c64a1aba7cfcb04f79e76f859b3d66660275d59
Author: Stefan Behrens <sbehrens@giantdisaster.de>
Date:   Mon May 13 13:53:35 2013 +0000

    Btrfs: cleanup: don't check the same thing twice
    
    btrfs_read_fs_root_no_name() already checks if btrfs_root_refs()
    is zero and returns ENOENT in this case. There is no need to do
    it again in six places.
    
    Signed-off-by: Stefan Behrens <sbehrens@giantdisaster.de>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index b3e359bc8e68..185af15ad9e4 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -308,10 +308,6 @@ static int __btrfs_run_defrag_inode(struct btrfs_fs_info *fs_info,
 		ret = PTR_ERR(inode_root);
 		goto cleanup;
 	}
-	if (btrfs_root_refs(&inode_root->root_item) == 0) {
-		ret = -ENOENT;
-		goto cleanup;
-	}
 
 	key.objectid = defrag->ino;
 	btrfs_set_key_type(&key, BTRFS_INODE_ITEM_KEY);

commit 983a5f84a4a11c8706ca70615125db711336b684
Merge: 8769e078a9a2 667e7d94a168
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu May 9 13:07:40 2013 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs
    
    Pull btrfs update from Chris Mason:
     "These are mostly fixes.  The biggest exceptions are Josef's skinny
      extents and Jan Schmidt's code to rebuild our quota indexes if they
      get out of sync (or you enable quotas on an existing filesystem).
    
      The skinny extents are off by default because they are a new variation
      on the extent allocation tree format.  btrfstune -x enables them, and
      the new format makes the extent allocation tree about 30% smaller.
    
      I rebased this a few days ago to rework Dave Sterba's crc checks on
      the super block, but almost all of these go back to rc6, since I
      though 3.9 was due any minute.
    
      The biggest missing fix is the tracepoint bug that was hit late in
      3.9.  I ran into problems with that in overnight testing and I'm still
      tracking it down.  I'll definitely have that fixed for rc2."
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs: (101 commits)
      Btrfs: allow superblock mismatch from older mkfs
      btrfs: enhance superblock checks
      btrfs: fix misleading variable name for flags
      btrfs: use unsigned long type for extent state bits
      Btrfs: improve the loop of scrub_stripe
      btrfs: read entire device info under lock
      btrfs: remove unused gfp mask parameter from release_extent_buffer callchain
      btrfs: handle errors returned from get_tree_block_key
      btrfs: make static code static & remove dead code
      Btrfs: deal with errors in write_dev_supers
      Btrfs: remove almost all of the BUG()'s from tree-log.c
      Btrfs: deal with free space cache errors while replaying log
      Btrfs: automatic rescan after "quota enable" command
      Btrfs: rescan for qgroups
      Btrfs: split btrfs_qgroup_account_ref into four functions
      Btrfs: allocate new chunks if the space is not enough for global rsv
      Btrfs: separate sequence numbers for delayed ref tracking and tree mod log
      btrfs: move leak debug code to functions
      Btrfs: return free space in cow error path
      Btrfs: set UUID in root_item for created trees
      ...

commit a27bb332c04cec8c4afd7912df0dc7890db27560
Author: Kent Overstreet <koverstreet@google.com>
Date:   Tue May 7 16:19:08 2013 -0700

    aio: don't include aio.h in sched.h
    
    Faster kernel compiles by way of fewer unnecessary includes.
    
    [akpm@linux-foundation.org: fix fallout]
    [akpm@linux-foundation.org: fix build]
    Signed-off-by: Kent Overstreet <koverstreet@google.com>
    Cc: Zach Brown <zab@redhat.com>
    Cc: Felipe Balbi <balbi@ti.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Mark Fasheh <mfasheh@suse.com>
    Cc: Joel Becker <jlbec@evilplan.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Asai Thambi S P <asamymuthupa@micron.com>
    Cc: Selvan Mani <smani@micron.com>
    Cc: Sam Bradshaw <sbradshaw@micron.com>
    Cc: Jeff Moyer <jmoyer@redhat.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Benjamin LaHaise <bcrl@kvack.org>
    Reviewed-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index bb8b7a0e28a6..bc4d54c465a0 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -24,6 +24,7 @@
 #include <linux/string.h>
 #include <linux/backing-dev.h>
 #include <linux/mpage.h>
+#include <linux/aio.h>
 #include <linux/falloc.h>
 #include <linux/swap.h>
 #include <linux/writeback.h>

commit 48a3b6366f6913683563d934eb16fea67dead9c1
Author: Eric Sandeen <sandeen@redhat.com>
Date:   Thu Apr 25 20:41:01 2013 +0000

    btrfs: make static code static & remove dead code
    
    Big patch, but all it does is add statics to functions which
    are in fact static, then remove the associated dead-code fallout.
    
    removed functions:
    
    btrfs_iref_to_path()
    __btrfs_lookup_delayed_deletion_item()
    __btrfs_search_delayed_insertion_item()
    __btrfs_search_delayed_deletion_item()
    find_eb_for_page()
    btrfs_find_block_group()
    range_straddles_pages()
    extent_range_uptodate()
    btrfs_file_extent_length()
    btrfs_scrub_cancel_devid()
    btrfs_start_transaction_lflush()
    
    btrfs_print_tree() is left because it is used for debugging.
    btrfs_start_transaction_lflush() and btrfs_reada_detach() are
    left for symmetry.
    
    ulist.c functions are left, another patch will take care of those.
    
    Signed-off-by: Eric Sandeen <sandeen@redhat.com>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index bef15c3ef41d..b3e359bc8e68 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -192,8 +192,8 @@ int btrfs_add_inode_defrag(struct btrfs_trans_handle *trans,
  * the same inode in the tree, we will merge them together (by
  * __btrfs_add_inode_defrag()) and free the one that we want to requeue.
  */
-void btrfs_requeue_inode_defrag(struct inode *inode,
-				struct inode_defrag *defrag)
+static void btrfs_requeue_inode_defrag(struct inode *inode,
+				       struct inode_defrag *defrag)
 {
 	struct btrfs_root *root = BTRFS_I(inode)->root;
 	int ret;
@@ -473,7 +473,7 @@ static noinline int btrfs_copy_from_user(loff_t pos, int num_pages,
 /*
  * unlocks pages after btrfs_file_write is done with them
  */
-void btrfs_drop_pages(struct page **pages, size_t num_pages)
+static void btrfs_drop_pages(struct page **pages, size_t num_pages)
 {
 	size_t i;
 	for (i = 0; i < num_pages; i++) {
@@ -497,9 +497,9 @@ void btrfs_drop_pages(struct page **pages, size_t num_pages)
  * doing real data extents, marking pages dirty and delalloc as required.
  */
 int btrfs_dirty_pages(struct btrfs_root *root, struct inode *inode,
-		      struct page **pages, size_t num_pages,
-		      loff_t pos, size_t write_bytes,
-		      struct extent_state **cached)
+			     struct page **pages, size_t num_pages,
+			     loff_t pos, size_t write_bytes,
+			     struct extent_state **cached)
 {
 	int err = 0;
 	int i;

commit afe5fea72bd50b1df2e6a721ef50559427d42f2b
Author: Tsutomu Itoh <t-itoh@jp.fujitsu.com>
Date:   Tue Apr 16 05:18:22 2013 +0000

    Btrfs: cleanup of function where fixup_low_keys() is called
    
    If argument 'trans' is unnecessary in the function where
    fixup_low_keys() is called, 'trans' is deleted.
    
    Signed-off-by: Tsutomu Itoh <t-itoh@jp.fujitsu.com>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index a56abed78104..bef15c3ef41d 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -824,7 +824,7 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 
 			memcpy(&new_key, &key, sizeof(new_key));
 			new_key.offset = end;
-			btrfs_set_item_key_safe(trans, root, path, &new_key);
+			btrfs_set_item_key_safe(root, path, &new_key);
 
 			extent_offset += end - key.offset;
 			btrfs_set_file_extent_offset(leaf, fi, extent_offset);
@@ -1040,7 +1040,7 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 				     ino, bytenr, orig_offset,
 				     &other_start, &other_end)) {
 			new_key.offset = end;
-			btrfs_set_item_key_safe(trans, root, path, &new_key);
+			btrfs_set_item_key_safe(root, path, &new_key);
 			fi = btrfs_item_ptr(leaf, path->slots[0],
 					    struct btrfs_file_extent_item);
 			btrfs_set_file_extent_generation(leaf, fi,
@@ -1074,7 +1074,7 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 							 trans->transid);
 			path->slots[0]++;
 			new_key.offset = start;
-			btrfs_set_item_key_safe(trans, root, path, &new_key);
+			btrfs_set_item_key_safe(root, path, &new_key);
 
 			fi = btrfs_item_ptr(leaf, path->slots[0],
 					    struct btrfs_file_extent_item);
@@ -1888,7 +1888,7 @@ static int fill_holes(struct btrfs_trans_handle *trans, struct inode *inode,
 
 		path->slots[0]++;
 		key.offset = offset;
-		btrfs_set_item_key_safe(trans, root, path, &key);
+		btrfs_set_item_key_safe(root, path, &key);
 		fi = btrfs_item_ptr(leaf, path->slots[0],
 				    struct btrfs_file_extent_item);
 		num_bytes = btrfs_file_extent_num_bytes(leaf, fi) + end -

commit 09a2a8f96e3009273bed1833b3f210e2c68728a5
Author: Josef Bacik <jbacik@fusionio.com>
Date:   Fri Apr 5 16:51:15 2013 -0400

    Btrfs: fix bad extent logging
    
    A user sent me a btrfs-image of a file system that was panicing on mount during
    the log recovery.  I had originally thought these problems were from a bug in
    the free space cache code, but that was just a symptom of the problem.  The
    problem is if your application does something like this
    
    [prealloc][prealloc][prealloc]
    
    the internal extent maps will merge those all together into one extent map, even
    though on disk they are 3 separate extents.  So if you go to write into one of
    these ranges the extent map will be right since we use the physical extent when
    doing the write, but when we log the extents they will use the wrong sizes for
    the remainder prealloc space.  If this doesn't happen to trip up the free space
    cache (which it won't in a lot of cases) then you will get bogus entries in your
    extent tree which will screw stuff up later.  The data and such will still work,
    but everything else is broken.  This patch fixes this by not allowing extents
    that are on the modified list to be merged.  This has the side effect that we
    are no longer adding everything to the modified list all the time, which means
    we now have to call btrfs_drop_extents every time we log an extent into the
    tree.  So this allows me to drop all this speciality code I was using to get
    around calling btrfs_drop_extents.  With this patch the testcase I've created no
    longer creates a bogus file system after replaying the log.  Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index e81e428a8840..a56abed78104 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -552,6 +552,7 @@ void btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
 	int testend = 1;
 	unsigned long flags;
 	int compressed = 0;
+	bool modified;
 
 	WARN_ON(end < start);
 	if (end == (u64)-1) {
@@ -561,6 +562,7 @@ void btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
 	while (1) {
 		int no_splits = 0;
 
+		modified = false;
 		if (!split)
 			split = alloc_extent_map();
 		if (!split2)
@@ -592,6 +594,7 @@ void btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
 		compressed = test_bit(EXTENT_FLAG_COMPRESSED, &em->flags);
 		clear_bit(EXTENT_FLAG_PINNED, &em->flags);
 		clear_bit(EXTENT_FLAG_LOGGING, &flags);
+		modified = !list_empty(&em->list);
 		remove_extent_mapping(em_tree, em);
 		if (no_splits)
 			goto next;
@@ -614,9 +617,8 @@ void btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
 			split->bdev = em->bdev;
 			split->flags = flags;
 			split->compress_type = em->compress_type;
-			ret = add_extent_mapping(em_tree, split);
+			ret = add_extent_mapping(em_tree, split, modified);
 			BUG_ON(ret); /* Logic error */
-			list_move(&split->list, &em_tree->modified_extents);
 			free_extent_map(split);
 			split = split2;
 			split2 = NULL;
@@ -645,9 +647,8 @@ void btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
 				split->orig_start = em->orig_start;
 			}
 
-			ret = add_extent_mapping(em_tree, split);
+			ret = add_extent_mapping(em_tree, split, modified);
 			BUG_ON(ret); /* Logic error */
-			list_move(&split->list, &em_tree->modified_extents);
 			free_extent_map(split);
 			split = NULL;
 		}
@@ -1930,10 +1931,7 @@ static int fill_holes(struct btrfs_trans_handle *trans, struct inode *inode,
 		do {
 			btrfs_drop_extent_cache(inode, offset, end - 1, 0);
 			write_lock(&em_tree->lock);
-			ret = add_extent_mapping(em_tree, hole_em);
-			if (!ret)
-				list_move(&hole_em->list,
-					  &em_tree->modified_extents);
+			ret = add_extent_mapping(em_tree, hole_em, 1);
 			write_unlock(&em_tree->lock);
 		} while (ret == -EEXIST);
 		free_extent_map(hole_em);

commit cc95bef635a649d595cf8d1cd4fcff5b6bf13023
Author: Josef Bacik <jbacik@fusionio.com>
Date:   Thu Apr 4 14:31:27 2013 -0400

    Btrfs: log ram bytes properly
    
    When logging changed extents I was logging ram_bytes as the current length,
    which isn't correct, it's supposed to be the ram bytes of the original extent.
    This is for compression where even if we split the extent we need to know the
    ram bytes so when we uncompress the extent we know how big it will be.  This was
    still working out right with compression for some reason but I think we were
    getting lucky.  It was definitely off for prealloc which is why I noticed it,
    btrfsck was complaining about it.  With this patch btrfsck no longer complains
    after a log replay.  Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index ade03e6f7bd2..e81e428a8840 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -607,6 +607,7 @@ void btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
 				split->block_len = em->block_len;
 			else
 				split->block_len = split->len;
+			split->ram_bytes = em->ram_bytes;
 			split->orig_block_len = max(split->block_len,
 						    em->orig_block_len);
 			split->generation = gen;
@@ -632,6 +633,7 @@ void btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
 			split->generation = gen;
 			split->orig_block_len = max(em->block_len,
 						    em->orig_block_len);
+			split->ram_bytes = em->ram_bytes;
 
 			if (compressed) {
 				split->block_len = em->block_len;
@@ -1915,6 +1917,7 @@ static int fill_holes(struct btrfs_trans_handle *trans, struct inode *inode,
 	} else {
 		hole_em->start = offset;
 		hole_em->len = end - offset;
+		hole_em->ram_bytes = hole_em->len;
 		hole_em->orig_start = offset;
 
 		hole_em->block_start = EXTENT_MAP_HOLE;

commit 20b4fb485227404329e41ad15588afad3df23050
Merge: b9394d8a657c ac3e3c5b1164
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed May 1 17:51:54 2013 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs
    
    Pull VFS updates from Al Viro,
    
    Misc cleanups all over the place, mainly wrt /proc interfaces (switch
    create_proc_entry to proc_create(), get rid of the deprecated
    create_proc_read_entry() in favor of using proc_create_data() and
    seq_file etc).
    
    7kloc removed.
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs: (204 commits)
      don't bother with deferred freeing of fdtables
      proc: Move non-public stuff from linux/proc_fs.h to fs/proc/internal.h
      proc: Make the PROC_I() and PDE() macros internal to procfs
      proc: Supply a function to remove a proc entry by PDE
      take cgroup_open() and cpuset_open() to fs/proc/base.c
      ppc: Clean up scanlog
      ppc: Clean up rtas_flash driver somewhat
      hostap: proc: Use remove_proc_subtree()
      drm: proc: Use remove_proc_subtree()
      drm: proc: Use minor->index to label things, not PDE->name
      drm: Constify drm_proc_list[]
      zoran: Don't print proc_dir_entry data in debug
      reiserfs: Don't access the proc_dir_entry in r_open(), r_start() r_show()
      proc: Supply an accessor for getting the data from a PDE's parent
      airo: Use remove_proc_subtree()
      rtl8192u: Don't need to save device proc dir PDE
      rtl8187se: Use a dir under /proc/net/r8180/
      proc: Add proc_mkdir_data()
      proc: Move some bits from linux/proc_fs.h to linux/{of.h,signal.h,tty.h}
      proc: Move PDE_NET() to fs/proc/proc_net.c
      ...

commit 8d71db4f0890605d44815a2b2da4ca003f1bb142
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Tue Mar 19 21:01:03 2013 -0400

    lift sb_start_write/sb_end_write out of ->aio_write()
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 5b4ea5f55b8f..254aeb72915f 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1514,8 +1514,6 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 	size_t count, ocount;
 	bool sync = (file->f_flags & O_DSYNC) || IS_SYNC(file->f_mapping->host);
 
-	sb_start_write(inode->i_sb);
-
 	mutex_lock(&inode->i_mutex);
 
 	err = generic_segment_checks(iov, &nr_segs, &ocount, VERIFY_READ);
@@ -1617,7 +1615,6 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 	if (sync)
 		atomic_dec(&BTRFS_I(inode)->sync_writers);
 out:
-	sb_end_write(inode->i_sb);
 	current->backing_dev_info = NULL;
 	return num_written ? num_written : err;
 }

commit 3615db41c4b82896de450b6b4e3dab2420dcae51
Merge: ed176886b68f d8fe29e9dea8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Mar 29 11:13:25 2013 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs
    
    Pull btrfs fixes from Chris Mason:
     "We've had a busy two weeks of bug fixing.  The biggest patches in here
      are some long standing early-enospc problems (Josef) and a very old
      race where compression and mmap combine forces to lose writes (me).
      I'm fairly sure the mmap bug goes all the way back to the introduction
      of the compression code, which is proof that fsx doesn't trigger every
      possible mmap corner after all.
    
      I'm sure you'll notice one of these is from this morning, it's a small
      and isolated use-after-free fix in our scrub error reporting.  I
      double checked it here."
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs:
      Btrfs: don't drop path when printing out tree errors in scrub
      Btrfs: fix wrong return value of btrfs_lookup_csum()
      Btrfs: fix wrong reservation of csums
      Btrfs: fix double free in the btrfs_qgroup_account_ref()
      Btrfs: limit the global reserve to 512mb
      Btrfs: hold the ordered operations mutex when waiting on ordered extents
      Btrfs: fix space accounting for unlink and rename
      Btrfs: fix space leak when we fail to reserve metadata space
      Btrfs: fix EIO from btrfs send in is_extent_unchanged for punched holes
      Btrfs: fix race between mmap writes and compression
      Btrfs: fix memory leak in btrfs_create_tree()
      Btrfs: fix locking on ROOT_REPLACE operations in tree mod log
      Btrfs: fix missing qgroup reservation before fallocating
      Btrfs: handle a bogus chunk tree nicely
      Btrfs: update to use fs_state bit

commit 6113077cd319e747875ec71227d2b5cb54e08c76
Author: Wang Shilong <wangsl-fnst@cn.fujitsu.com>
Date:   Tue Mar 19 10:57:14 2013 +0000

    Btrfs: fix missing qgroup reservation before fallocating
    
    Steps to reproduce:
            mkfs.btrfs <disk>
            mount <disk> <mnt>
            btrfs quota enable <mnt>
            btrfs sub create <mnt>/subv
            btrfs qgroup limit 10M <mnt>/subv
            fallocate --length 20M <mnt>/subv/data
    
    For the above example, fallocating will return successfully which
    is not expected, we try to fix it by doing qgroup reservation before
    fallocating.
    
    Signed-off-by: Wang Shilong <wangsl-fnst@cn.fujitsu.com>
    Reviewed-by: Miao Xie <miaox@cn.fujitsu.com>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 7bdb47faa12e..1be25b92d63c 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2142,6 +2142,7 @@ static long btrfs_fallocate(struct file *file, int mode,
 {
 	struct inode *inode = file->f_path.dentry->d_inode;
 	struct extent_state *cached_state = NULL;
+	struct btrfs_root *root = BTRFS_I(inode)->root;
 	u64 cur_offset;
 	u64 last_byte;
 	u64 alloc_start;
@@ -2169,6 +2170,11 @@ static long btrfs_fallocate(struct file *file, int mode,
 	ret = btrfs_check_data_free_space(inode, alloc_end - alloc_start);
 	if (ret)
 		return ret;
+	if (root->fs_info->quota_enabled) {
+		ret = btrfs_qgroup_reserve(root, alloc_end - alloc_start);
+		if (ret)
+			goto out_reserve_fail;
+	}
 
 	/*
 	 * wait for ordered IO before we have any locks.  We'll loop again
@@ -2272,6 +2278,9 @@ static long btrfs_fallocate(struct file *file, int mode,
 			     &cached_state, GFP_NOFS);
 out:
 	mutex_unlock(&inode->i_mutex);
+	if (root->fs_info->quota_enabled)
+		btrfs_qgroup_free(root, alloc_end - alloc_start);
+out_reserve_fail:
 	/* Let go of our reservation. */
 	btrfs_free_reserved_data_space(inode, alloc_end - alloc_start);
 	return ret;

commit 08637024ab77f7defff1627cc8aedc2c6679ad8a
Merge: e20437852de4 3b2775942d6c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Mar 17 11:04:14 2013 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs
    
    Pull btrfs fixes from Chris Mason:
     "Eric's rcu barrier patch fixes a long standing problem with our
      unmount code hanging on to devices in workqueue helpers.  Liu Bo
      nailed down a difficult assertion for in-memory extent mappings."
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs:
      Btrfs: fix warning of free_extent_map
      Btrfs: fix warning when creating snapshots
      Btrfs: return as soon as possible when edquot happens
      Btrfs: return EIO if we have extent tree corruption
      btrfs: use rcu_barrier() to wait for bdev puts at unmount
      Btrfs: remove btrfs_try_spin_lock
      Btrfs: get better concurrency for snapshot-aware defrag work

commit 3b2775942d6ccb14342f3aae55f22fbbfea8db14
Author: Liu Bo <bo.li.liu@oracle.com>
Date:   Fri Mar 15 08:46:39 2013 -0600

    Btrfs: fix warning of free_extent_map
    
    Users report that an extent map's list is still linked when it's actually
    going to be freed from cache.
    
    The story is that
    
    a) when we're going to drop an extent map and may split this large one into
    smaller ems, and if this large one is flagged as EXTENT_FLAG_LOGGING which means
    that it's on the list to be logged, then the smaller ems split from it will also
    be flagged as EXTENT_FLAG_LOGGING, and this is _not_ expected.
    
    b) we'll keep ems from unlinking the list and freeing when they are flagged with
    EXTENT_FLAG_LOGGING, because the log code holds one reference.
    
    The end result is the warning, but the truth is that we set the flag
    EXTENT_FLAG_LOGGING only during fsync.
    
    So clear flag EXTENT_FLAG_LOGGING for extent maps split from a large one.
    
    Reported-by: Johannes Hirte <johannes.hirte@fem.tu-ilmenau.de>
    Reported-by: Darrick J. Wong <darrick.wong@oracle.com>
    Signed-off-by: Liu Bo <bo.li.liu@oracle.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 83c790d84038..7bdb47faa12e 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -591,6 +591,7 @@ void btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
 		}
 		compressed = test_bit(EXTENT_FLAG_COMPRESSED, &em->flags);
 		clear_bit(EXTENT_FLAG_PINNED, &em->flags);
+		clear_bit(EXTENT_FLAG_LOGGING, &flags);
 		remove_extent_mapping(em_tree, em);
 		if (no_splits)
 			goto next;

commit b695188dd39162a1a6bff11fdbcc4c0b65b933ab
Merge: 48476df99894 180e001cd5fc
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Mar 2 16:41:54 2013 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs
    
    Pull btrfs update from Chris Mason:
     "The biggest feature in the pull is the new (and still experimental)
      raid56 code that David Woodhouse started long ago.  I'm still working
      on the parity logging setup that will avoid inconsistent parity after
      a crash, so this is only for testing right now.  But, I'd really like
      to get it out to a broader audience to hammer out any performance
      issues or other problems.
    
      scrub does not yet correct errors on raid5/6 either.
    
      Josef has another pass at fsync performance.  The big change here is
      to combine waiting for metadata with waiting for data, which is a big
      latency win.  It is also step one toward using atomics from the
      hardware during a commit.
    
      Mark Fasheh has a new way to use btrfs send/receive to send only the
      metadata changes.  SUSE is using this to make snapper more efficient
      at finding changes between snapshosts.
    
      Snapshot-aware defrag is also included.
    
      Otherwise we have a large number of fixes and cleanups.  Eric Sandeen
      wins the award for removing the most lines, and I'm hoping we steal
      this idea from XFS over and over again."
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs: (118 commits)
      btrfs: fixup/remove module.h usage as required
      Btrfs: delete inline extents when we find them during logging
      btrfs: try harder to allocate raid56 stripe cache
      Btrfs: cleanup to make the function btrfs_delalloc_reserve_metadata more logic
      Btrfs: don't call btrfs_qgroup_free if just btrfs_qgroup_reserve fails
      Btrfs: remove reduplicate check about root in the function btrfs_clean_quota_tree
      Btrfs: return ENOMEM rather than use BUG_ON when btrfs_alloc_path fails
      Btrfs: fix missing deleted items in btrfs_clean_quota_tree
      btrfs: use only inline_pages from extent buffer
      Btrfs: fix wrong reserved space when deleting a snapshot/subvolume
      Btrfs: fix wrong reserved space in qgroup during snap/subv creation
      Btrfs: remove unnecessary dget_parent/dput when creating the pending snapshot
      btrfs: remove a printk from scan_one_device
      Btrfs: fix NULL pointer after aborting a transaction
      Btrfs: fix memory leak of log roots
      Btrfs: copy everything if we've created an inline extent
      btrfs: cleanup for open-coded alignment
      Btrfs: do not change inode flags in rename
      Btrfs: use reserved space for creating a snapshot
      clear chunk_alloc flag on retryable failure
      ...

commit d895cb1af15c04c522a25c79cc429076987c089b
Merge: 9626357371b5 d3d009cb965e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Feb 26 20:16:07 2013 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs
    
    Pull vfs pile (part one) from Al Viro:
     "Assorted stuff - cleaning namei.c up a bit, fixing ->d_name/->d_parent
      locking violations, etc.
    
      The most visible changes here are death of FS_REVAL_DOT (replaced with
      "has ->d_weak_revalidate()") and a new helper getting from struct file
      to inode.  Some bits of preparation to xattr method interface changes.
    
      Misc patches by various people sent this cycle *and* ocfs2 fixes from
      several cycles ago that should've been upstream right then.
    
      PS: the next vfs pile will be xattr stuff."
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs: (46 commits)
      saner proc_get_inode() calling conventions
      proc: avoid extra pde_put() in proc_fill_super()
      fs: change return values from -EACCES to -EPERM
      fs/exec.c: make bprm_mm_init() static
      ocfs2/dlm: use GFP_ATOMIC inside a spin_lock
      ocfs2: fix possible use-after-free with AIO
      ocfs2: Fix oops in ocfs2_fast_symlink_readpage() code path
      get_empty_filp()/alloc_file() leave both ->f_pos and ->f_version zero
      target: writev() on single-element vector is pointless
      export kernel_write(), convert open-coded instances
      fs: encode_fh: return FILEID_INVALID if invalid fid_type
      kill f_vfsmnt
      vfs: kill FS_REVAL_DOT by adding a d_weak_revalidate dentry op
      nfsd: handle vfs_getattr errors in acl protocol
      switch vfs_getattr() to struct path
      default SET_PERSONALITY() in linux/elf.h
      ceph: prepopulate inodes only when request is aborted
      d_hash_and_lookup(): export, switch open-coded instances
      9p: switch v9fs_set_create_acl() to inode+fid, do it before d_instantiate()
      9p: split dropping the acls from v9fs_set_create_acl()
      ...

commit fda2832febb1928da0625b2c5d15559b29d7e740
Author: Qu Wenruo <quwenruo@cn.fujitsu.com>
Date:   Tue Feb 26 08:10:22 2013 +0000

    btrfs: cleanup for open-coded alignment
    
    Though most of the btrfs codes are using ALIGN macro for page alignment,
    there are still some codes using open-coded alignment like the
    following:
    ------
            u64 mask = ((u64)root->stripesize - 1);
            u64 ret = (val + mask) & ~mask;
    ------
    Or even hidden one:
    ------
            num_bytes = (end - start + blocksize) & ~(blocksize - 1);
    ------
    
    Sometimes these open-coded alignment is not so easy to understand for
    newbie like me.
    
    This commit changes the open-coded alignment to the ALIGN macro for a
    better readability.
    
    Also there is a previous patch from David Sterba with similar changes,
    but the patch is for 3.2 kernel and seems not merged.
    http://www.spinics.net/lists/linux-btrfs/msg12747.html
    
    Cc: David Sterba <dave@jikos.cz>
    Signed-off-by: Qu Wenruo <quwenruo@cn.fujitsu.com>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 6e6dd8cdad92..83c790d84038 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -510,8 +510,7 @@ int btrfs_dirty_pages(struct btrfs_root *root, struct inode *inode,
 	loff_t isize = i_size_read(inode);
 
 	start_pos = pos & ~((u64)root->sectorsize - 1);
-	num_bytes = (write_bytes + pos - start_pos +
-		    root->sectorsize - 1) & ~((u64)root->sectorsize - 1);
+	num_bytes = ALIGN(write_bytes + pos - start_pos, root->sectorsize);
 
 	end_of_last_block = start_pos + num_bytes - 1;
 	err = btrfs_set_extent_delalloc(inode, start_pos, end_of_last_block,

commit 496ad9aa8ef448058e36ca7a787c61f2e63f0f54
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Wed Jan 23 17:07:38 2013 -0500

    new helper: file_inode(file)
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 77061bf43edb..4118e0b6e339 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1211,7 +1211,7 @@ static noinline int prepare_pages(struct btrfs_root *root, struct file *file,
 	struct extent_state *cached_state = NULL;
 	int i;
 	unsigned long index = pos >> PAGE_CACHE_SHIFT;
-	struct inode *inode = fdentry(file)->d_inode;
+	struct inode *inode = file_inode(file);
 	gfp_t mask = btrfs_alloc_write_mask(inode->i_mapping);
 	int err = 0;
 	int faili = 0;
@@ -1298,7 +1298,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 					       struct iov_iter *i,
 					       loff_t pos)
 {
-	struct inode *inode = fdentry(file)->d_inode;
+	struct inode *inode = file_inode(file);
 	struct btrfs_root *root = BTRFS_I(inode)->root;
 	struct page **pages = NULL;
 	unsigned long first_index;
@@ -1486,7 +1486,7 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 				    unsigned long nr_segs, loff_t pos)
 {
 	struct file *file = iocb->ki_filp;
-	struct inode *inode = fdentry(file)->d_inode;
+	struct inode *inode = file_inode(file);
 	struct btrfs_root *root = BTRFS_I(inode)->root;
 	loff_t *ppos = &iocb->ki_pos;
 	u64 start_pos;
@@ -2087,7 +2087,7 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 static long btrfs_fallocate(struct file *file, int mode,
 			    loff_t offset, loff_t len)
 {
-	struct inode *inode = file->f_path.dentry->d_inode;
+	struct inode *inode = file_inode(file);
 	struct extent_state *cached_state = NULL;
 	u64 cur_offset;
 	u64 last_byte;

commit dc81cdc58ad2f413b96b9004f8d681e5dc554473
Author: Miao Xie <miaox@cn.fujitsu.com>
Date:   Wed Feb 20 23:32:52 2013 -0700

    Btrfs: fix remount vs autodefrag
    
    If we remount the fs to close the auto defragment or make the fs R/O,
    we should stop the auto defragment.
    
    Signed-off-by: Miao Xie <miaox@cn.fujitsu.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 9f67e623206d..6e6dd8cdad92 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -374,6 +374,11 @@ int btrfs_run_defrag_inodes(struct btrfs_fs_info *fs_info)
 
 	atomic_inc(&fs_info->defrag_running);
 	while(1) {
+		/* Pause the auto defragger. */
+		if (test_bit(BTRFS_FS_STATE_REMOUNTING,
+			     &fs_info->fs_state))
+			break;
+
 		if (!__need_auto_defrag(fs_info->tree_root))
 			break;
 

commit b2c6b3e0611c58fbeb6b9c0892b6249f7bdfaf6b
Merge: 19f949f52599 272d26d0ad8c
Author: Chris Mason <chris.mason@fusionio.com>
Date:   Wed Feb 20 14:05:45 2013 -0500

    Merge branch 'master' of git://git.kernel.org/pub/scm/linux/kernel/git/josef/btrfs-next into for-linus-3.9
    
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>
    
    Conflicts:
            fs/btrfs/disk-io.c

commit 569e0f358c0c37f6733702d4a5d2c412860f7169
Author: Josef Bacik <jbacik@fusionio.com>
Date:   Wed Feb 13 11:09:14 2013 -0500

    Btrfs: place ordered operations on a per transaction list
    
    Miao made the ordered operations stuff run async, which introduced a
    deadlock where we could get somebody (sync) racing in and committing the
    transaction while a commit was already happening.  The new committer would
    try and flush ordered operations which would hang waiting for the commit to
    finish because it is done asynchronously and no longer inherits the callers
    trans handle.  To fix this we need to make the ordered operations list a per
    transaction list.  We can get new inodes added to the ordered operation list
    by truncating them and then having another process writing to them, so this
    makes it so that anybody trying to add an ordered operation _must_ start a
    transaction in order to add itself to the list, which will keep new inodes
    from getting added to the ordered operations list after we start committing.
    This should fix the deadlock and also keeps us from doing a lot more work
    than we need to during commit.  Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 75d0fe134be3..b12ba52c4505 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1628,7 +1628,20 @@ int btrfs_release_file(struct inode *inode, struct file *filp)
 	 */
 	if (test_and_clear_bit(BTRFS_INODE_ORDERED_DATA_CLOSE,
 			       &BTRFS_I(inode)->runtime_flags)) {
-		btrfs_add_ordered_operation(NULL, BTRFS_I(inode)->root, inode);
+		struct btrfs_trans_handle *trans;
+		struct btrfs_root *root = BTRFS_I(inode)->root;
+
+		/*
+		 * We need to block on a committing transaction to keep us from
+		 * throwing a ordered operation on to the list and causing
+		 * something like sync to deadlock trying to flush out this
+		 * inode.
+		 */
+		trans = btrfs_start_transaction(root, 0);
+		if (IS_ERR(trans))
+			return PTR_ERR(trans);
+		btrfs_add_ordered_operation(trans, BTRFS_I(inode)->root, inode);
+		btrfs_end_transaction(trans, root);
 		if (inode->i_size > BTRFS_ORDERED_OPERATIONS_FLUSH_LIMIT)
 			filemap_flush(inode->i_mapping);
 	}

commit 87533c475187c1420794a2e164bc67a7974f1327
Author: Miao Xie <miaox@cn.fujitsu.com>
Date:   Tue Jan 29 10:14:48 2013 +0000

    Btrfs: use bit operation for ->fs_state
    
    There is no lock to protect fs_info->fs_state, it will introduce
    some problems, such as the value may be covered by the other task
    when several tasks modify it. For example:
            Task0 - CPU0            Task1 - CPU1
            mov %fs_state rax
            or $0x1 rax
                                    mov %fs_state rax
                                    or $0x2 rax
            mov rax %fs_state
                                    mov rax %fs_state
    The expected value is 3, but in fact, it is 2.
    
    Though this problem doesn't happen now (because there is only one
    flag currently), the code is error prone, if we add other flags,
    the above problem will happen to a certainty.
    
    Now we use bit operation for it to fix the above problem.
    In this way, we can make the code more robust and be easy to
    add new flags.
    
    Signed-off-by: Miao Xie <miaox@cn.fujitsu.com>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 13c78ea3ebce..75d0fe134be3 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1545,7 +1545,7 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 	 * although we have opened a file as writable, we have
 	 * to stop this write operation to ensure FS consistency.
 	 */
-	if (root->fs_info->fs_state & BTRFS_SUPER_FLAG_ERROR) {
+	if (test_bit(BTRFS_FS_STATE_ERROR, &root->fs_info->fs_state)) {
 		mutex_unlock(&inode->i_mutex);
 		err = -EROFS;
 		goto out;

commit 55e301fd57a6239ec14b91a1cf2e70b3dd135194
Author: Filipe Brandenburger <filbranden@google.com>
Date:   Tue Jan 29 06:04:50 2013 +0000

    Btrfs: move fs/btrfs/ioctl.h to include/uapi/linux/btrfs.h
    
    The header file will then be installed under /usr/include/linux so that
    userspace applications can refer to Btrfs ioctls by name and use the same
    structs used internally in the kernel.
    
    Signed-off-by: Filipe Brandenburger <filbranden@google.com>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 083abca56055..13c78ea3ebce 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -30,11 +30,11 @@
 #include <linux/statfs.h>
 #include <linux/compat.h>
 #include <linux/slab.h>
+#include <linux/btrfs.h>
 #include "ctree.h"
 #include "disk-io.h"
 #include "transaction.h"
 #include "btrfs_inode.h"
-#include "ioctl.h"
 #include "print-tree.h"
 #include "tree-log.h"
 #include "locking.h"

commit 2ab28f322f9896782da904f5942f3873432addc8
Author: Josef Bacik <jbacik@fusionio.com>
Date:   Fri Oct 12 15:27:49 2012 -0400

    Btrfs: wait on ordered extents at the last possible moment
    
    Since we don't actually copy the extent information from the source tree in
    the fast case we don't need to wait for ordered io to be completed in order
    to fsync, we just need to wait for the io to be completed.  So when we're
    logging our file just attach all of the ordered extents to the log, and then
    when the log syncs just wait for IO_DONE on the ordered extents and then
    write the super.  Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index b06d289f998f..083abca56055 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1655,16 +1655,21 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 	struct btrfs_root *root = BTRFS_I(inode)->root;
 	int ret = 0;
 	struct btrfs_trans_handle *trans;
+	bool full_sync = 0;
 
 	trace_btrfs_sync_file(file, datasync);
 
 	/*
 	 * We write the dirty pages in the range and wait until they complete
 	 * out of the ->i_mutex. If so, we can flush the dirty pages by
-	 * multi-task, and make the performance up.
+	 * multi-task, and make the performance up.  See
+	 * btrfs_wait_ordered_range for an explanation of the ASYNC check.
 	 */
 	atomic_inc(&BTRFS_I(inode)->sync_writers);
-	ret = filemap_write_and_wait_range(inode->i_mapping, start, end);
+	ret = filemap_fdatawrite_range(inode->i_mapping, start, end);
+	if (!ret && test_bit(BTRFS_INODE_HAS_ASYNC_EXTENT,
+			     &BTRFS_I(inode)->runtime_flags))
+		ret = filemap_fdatawrite_range(inode->i_mapping, start, end);
 	atomic_dec(&BTRFS_I(inode)->sync_writers);
 	if (ret)
 		return ret;
@@ -1676,7 +1681,10 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 	 * range being left.
 	 */
 	atomic_inc(&root->log_batch);
-	btrfs_wait_ordered_range(inode, start, end - start + 1);
+	full_sync = test_bit(BTRFS_INODE_NEEDS_FULL_SYNC,
+			     &BTRFS_I(inode)->runtime_flags);
+	if (full_sync)
+		btrfs_wait_ordered_range(inode, start, end - start + 1);
 	atomic_inc(&root->log_batch);
 
 	/*
@@ -1743,13 +1751,25 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 
 	if (ret != BTRFS_NO_LOG_SYNC) {
 		if (ret > 0) {
+			/*
+			 * If we didn't already wait for ordered extents we need
+			 * to do that now.
+			 */
+			if (!full_sync)
+				btrfs_wait_ordered_range(inode, start,
+							 end - start + 1);
 			ret = btrfs_commit_transaction(trans, root);
 		} else {
 			ret = btrfs_sync_log(trans, root);
-			if (ret == 0)
+			if (ret == 0) {
 				ret = btrfs_end_transaction(trans, root);
-			else
+			} else {
+				if (!full_sync)
+					btrfs_wait_ordered_range(inode, start,
+								 end -
+								 start + 1);
 				ret = btrfs_commit_transaction(trans, root);
+			}
 		}
 	} else {
 		ret = btrfs_end_transaction(trans, root);

commit 8d19514fade54798106a60059c539501eda31b47
Merge: 95436adaa0f9 1a65e24b0bb7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Feb 8 12:06:46 2013 +1100

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs
    
    Pull btrfs fixes from Chris Mason:
     "We've got corner cases for updating i_size that ceph was hitting,
      error handling for quotas when we run out of space, a very subtle
      snapshot deletion race, a crash while removing devices, and one
      deadlock between subvolume creation and the sb_internal code (thanks
      lockdep)."
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs:
      Btrfs: move d_instantiate outside the transaction during mksubvol
      Btrfs: fix EDQUOT handling in btrfs_delalloc_reserve_metadata
      Btrfs: fix possible stale data exposure
      Btrfs: fix missing i_size update
      Btrfs: fix race between snapshot deletion and getting inode
      Btrfs: fix missing release of the space/qgroup reservation in start_transaction()
      Btrfs: fix wrong sync_writers decrement in btrfs_file_aio_write()
      Btrfs: do not merge logged extents if we've removed them from the tree
      btrfs: don't try to notify udev about missing devices

commit 6f1c36055f96e80031c7fdda3fd5be826b8d7782
Author: Liu Bo <bo.li.liu@oracle.com>
Date:   Tue Jan 29 03:22:10 2013 +0000

    Btrfs: fix race between snapshot deletion and getting inode
    
    While running snapshot testscript created by Mitch and David,
    the race between autodefrag and snapshot deletion can lead to
    corruption of dead_root list so that we can get crash on
    btrfs_clean_old_snapshots().
    
    And besides autodefrag, scrub also does the same thing, ie. read
    root first and get inode.
    
    Here is the story(take autodefrag as an example):
    (1) when we delete a snapshot or subvolume, it will set its root's
    refs to zero and do a iput() on its own inode, and if this inode happens
    to be the only active in-meory one in root's inode rbtree, it will add
    itself to the global dead_roots list for later cleanup.
    
    (2) after (1), the autodefrag thread may read another inode for defrag
    and the inode is just in the deleted snapshot/subvolume, but all of these
    are without checking if the root is still valid(refs > 0).  So the end up
    result is adding the deleted snapshot/subvolume's root to the global
    dead_roots list AGAIN.
    
    Fortunately, we already have a srcu lock to avoid the race, ie. subvol_srcu.
    
    So all we need to do is to take the lock to protect 'read root and get inode',
    since we synchronize to wait for the rcu grace period before adding something
    to the global dead_roots list.
    
    Reported-by: Mitch Harder <mitch.harder@sabayonlinux.org>
    Signed-off-by: Liu Bo <bo.li.liu@oracle.com>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index a902faab7161..b06d289f998f 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -293,15 +293,24 @@ static int __btrfs_run_defrag_inode(struct btrfs_fs_info *fs_info,
 	struct btrfs_key key;
 	struct btrfs_ioctl_defrag_range_args range;
 	int num_defrag;
+	int index;
+	int ret;
 
 	/* get the inode */
 	key.objectid = defrag->root;
 	btrfs_set_key_type(&key, BTRFS_ROOT_ITEM_KEY);
 	key.offset = (u64)-1;
+
+	index = srcu_read_lock(&fs_info->subvol_srcu);
+
 	inode_root = btrfs_read_fs_root_no_name(fs_info, &key);
 	if (IS_ERR(inode_root)) {
-		kmem_cache_free(btrfs_inode_defrag_cachep, defrag);
-		return PTR_ERR(inode_root);
+		ret = PTR_ERR(inode_root);
+		goto cleanup;
+	}
+	if (btrfs_root_refs(&inode_root->root_item) == 0) {
+		ret = -ENOENT;
+		goto cleanup;
 	}
 
 	key.objectid = defrag->ino;
@@ -309,9 +318,10 @@ static int __btrfs_run_defrag_inode(struct btrfs_fs_info *fs_info,
 	key.offset = 0;
 	inode = btrfs_iget(fs_info->sb, &key, inode_root, NULL);
 	if (IS_ERR(inode)) {
-		kmem_cache_free(btrfs_inode_defrag_cachep, defrag);
-		return PTR_ERR(inode);
+		ret = PTR_ERR(inode);
+		goto cleanup;
 	}
+	srcu_read_unlock(&fs_info->subvol_srcu, index);
 
 	/* do a chunk of defrag */
 	clear_bit(BTRFS_INODE_IN_DEFRAG, &BTRFS_I(inode)->runtime_flags);
@@ -346,6 +356,10 @@ static int __btrfs_run_defrag_inode(struct btrfs_fs_info *fs_info,
 
 	iput(inode);
 	return 0;
+cleanup:
+	srcu_read_unlock(&fs_info->subvol_srcu, index);
+	kmem_cache_free(btrfs_inode_defrag_cachep, defrag);
+	return ret;
 }
 
 /*

commit 0a3404dcff29a7589e8d6ce8c36f97c5411f85b4
Author: Miao Xie <miaox@cn.fujitsu.com>
Date:   Mon Jan 28 12:34:55 2013 +0000

    Btrfs: fix wrong sync_writers decrement in btrfs_file_aio_write()
    
    If the checks at the beginning of btrfs_file_aio_write() fail, we needn't
    decrease ->sync_writers, because we have not increased it. Fix it.
    
    Signed-off-by: Miao Xie <miaox@cn.fujitsu.com>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 841cfe3be0e0..a902faab7161 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1595,9 +1595,10 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 		if (err < 0 && num_written > 0)
 			num_written = err;
 	}
-out:
+
 	if (sync)
 		atomic_dec(&BTRFS_I(inode)->sync_writers);
+out:
 	sb_end_write(inode->i_sb);
 	current->backing_dev_info = NULL;
 	return num_written ? num_written : err;

commit d7df025eb4c3c571532326b01e007be52c75e5c0
Merge: 66e2d3e8c229 1eafa6c73791
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jan 25 10:55:21 2013 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs
    
    Pull btrfs fixes from Chris Mason:
     "It turns out that we had two crc bugs when running fsx-linux in a
      loop.  Many thanks to Josef, Miao Xie, and Dave Sterba for nailing it
      all down.  Miao also has a new OOM fix in this v2 pull as well.
    
      Ilya fixed a regression Liu Bo found in the balance ioctls for pausing
      and resuming a running balance across drives.
    
      Josef's orphan truncate patch fixes an obscure corruption we'd see
      during xfstests.
    
      Arne's patches address problems with subvolume quotas.  If the user
      destroys quota groups incorrectly the FS will refuse to mount.
    
      The rest are smaller fixes and plugs for memory leaks."
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs: (30 commits)
      Btrfs: fix repeated delalloc work allocation
      Btrfs: fix wrong max device number for single profile
      Btrfs: fix missed transaction->aborted check
      Btrfs: Add ACCESS_ONCE() to transaction->abort accesses
      Btrfs: put csums on the right ordered extent
      Btrfs: use right range to find checksum for compressed extents
      Btrfs: fix panic when recovering tree log
      Btrfs: do not allow logged extents to be merged or removed
      Btrfs: fix a regression in balance usage filter
      Btrfs: prevent qgroup destroy when there are still relations
      Btrfs: ignore orphan qgroup relations
      Btrfs: reorder locks and sanity checks in btrfs_ioctl_defrag
      Btrfs: fix unlock order in btrfs_ioctl_rm_dev
      Btrfs: fix unlock order in btrfs_ioctl_resize
      Btrfs: fix "mutually exclusive op is running" error code
      Btrfs: bring back balance pause/resume logic
      btrfs: update timestamps on truncate()
      btrfs: fix btrfs_cont_expand() freeing IS_ERR em
      Btrfs: fix a bug when llseek for delalloc bytes behind prealloc extents
      Btrfs: fix off-by-one in lseek
      ...

commit f9e4fb53938de5db01950c9dfe479703b2f5c964
Author: Liu Bo <bo.li.liu@oracle.com>
Date:   Mon Jan 7 10:10:12 2013 +0000

    Btrfs: fix a bug when llseek for delalloc bytes behind prealloc extents
    
    xfstests case 285 complains.
    
    It it because btrfs did not try to find unwritten delalloc
    bytes(only dirty pages, not yet writeback) behind prealloc
    extents, it ends up finding nothing while we're with SEEK_DATA.
    
    Signed-off-by: Liu Bo <bo.li.liu@oracle.com>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index fa48051484b8..841cfe3be0e0 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2309,9 +2309,12 @@ static int find_desired_extent(struct inode *inode, loff_t *offset, int origin)
 					}
 				}
 
-				*offset = start;
-				free_extent_map(em);
-				break;
+				if (!test_bit(EXTENT_FLAG_PREALLOC,
+					      &em->flags)) {
+					*offset = start;
+					free_extent_map(em);
+					break;
+				}
 			}
 		}
 

commit 1214b53f90131fee1f950010c43e92455fe598ab
Author: Liu Bo <bo.li.liu@oracle.com>
Date:   Mon Jan 7 03:53:08 2013 +0000

    Btrfs: fix off-by-one in lseek
    
    Lock end is inclusive.
    
    Signed-off-by: Liu Bo <bo.li.liu@oracle.com>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 20452c110d7d..fa48051484b8 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2242,6 +2242,7 @@ static int find_desired_extent(struct inode *inode, loff_t *offset, int origin)
 	if (lockend <= lockstart)
 		lockend = lockstart + root->sectorsize;
 
+	lockend--;
 	len = lockend - lockstart + 1;
 
 	len = max_t(u64, len, root->sectorsize);

commit a22180d2666c018f4fef6818074d78bb76ff2bda
Merge: 2d4dce007044 213490b30177
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Dec 18 09:42:05 2012 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs
    
    Pull btrfs update from Chris Mason:
     "A big set of fixes and features.
    
      In terms of line count, most of the code comes from Stefan, who added
      the ability to replace a single drive in place.  This is different
      from how btrfs normally replaces drives, and is much much much faster.
    
      Josef is plowing through our synchronous write performance.  This pull
      request does not include the DIO_OWN_WAITING patch that was discussed
      on the list, but it has a number of other improvements to cut down our
      latencies and CPU time during fsync/O_DIRECT writes.
    
      Miao Xie has a big series of fixes and is spreading out ordered
      operations over more CPUs.  This improves performance and reduces
      contention.
    
      I've put in fixes for error handling around hash collisions.  These
      are going back to individual stable kernels as I test against them.
    
      Otherwise we have a lot of fixes and cleanups, thanks everyone!
      raid5/6 is being rebased against the device replacement code.  I'll
      have it posted this Friday along with a nice series of benchmarks."
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs: (115 commits)
      Btrfs: fix a bug of per-file nocow
      Btrfs: fix hash overflow handling
      Btrfs: don't take inode delalloc mutex if we're a free space inode
      Btrfs: fix autodefrag and umount lockup
      Btrfs: fix permissions of empty files not affected by umask
      Btrfs: put raid properties into global table
      Btrfs: fix BUG() in scrub when first superblock reading gives EIO
      Btrfs: do not call file_update_time in aio_write
      Btrfs: only unlock and relock if we have to
      Btrfs: use tokens where we can in the tree log
      Btrfs: optimize leaf_space_used
      Btrfs: don't memset new tokens
      Btrfs: only clear dirty on the buffer if it is marked as dirty
      Btrfs: move checks in set_page_dirty under DEBUG
      Btrfs: log changed inodes based on the extent map tree
      Btrfs: add path->really_keep_locks
      Btrfs: do not mark ems as prealloc if we are writing to them
      Btrfs: keep track of the extents original block length
      Btrfs: inline csums if we're fsyncing
      Btrfs: don't bother copying if we're only logging the inode
      ...

commit 965c8e59cfcf845ecde2265a1d1bfee5f011d302
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Mon Dec 17 15:59:39 2012 -0800

    lseek: the "whence" argument is called "whence"
    
    But the kernel decided to call it "origin" instead.  Fix most of the
    sites.
    
    Acked-by: Hugh Dickins <hughd@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index a8ee75cb96ee..9c6673a9231f 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2120,7 +2120,7 @@ static long btrfs_fallocate(struct file *file, int mode,
 	return ret;
 }
 
-static int find_desired_extent(struct inode *inode, loff_t *offset, int origin)
+static int find_desired_extent(struct inode *inode, loff_t *offset, int whence)
 {
 	struct btrfs_root *root = BTRFS_I(inode)->root;
 	struct extent_map *em;
@@ -2154,7 +2154,7 @@ static int find_desired_extent(struct inode *inode, loff_t *offset, int origin)
 	 * before the position we want in case there is outstanding delalloc
 	 * going on here.
 	 */
-	if (origin == SEEK_HOLE && start != 0) {
+	if (whence == SEEK_HOLE && start != 0) {
 		if (start <= root->sectorsize)
 			em = btrfs_get_extent_fiemap(inode, NULL, 0, 0,
 						     root->sectorsize, 0);
@@ -2188,13 +2188,13 @@ static int find_desired_extent(struct inode *inode, loff_t *offset, int origin)
 				}
 			}
 
-			if (origin == SEEK_HOLE) {
+			if (whence == SEEK_HOLE) {
 				*offset = start;
 				free_extent_map(em);
 				break;
 			}
 		} else {
-			if (origin == SEEK_DATA) {
+			if (whence == SEEK_DATA) {
 				if (em->block_start == EXTENT_MAP_DELALLOC) {
 					if (start >= inode->i_size) {
 						free_extent_map(em);
@@ -2231,16 +2231,16 @@ static int find_desired_extent(struct inode *inode, loff_t *offset, int origin)
 	return ret;
 }
 
-static loff_t btrfs_file_llseek(struct file *file, loff_t offset, int origin)
+static loff_t btrfs_file_llseek(struct file *file, loff_t offset, int whence)
 {
 	struct inode *inode = file->f_mapping->host;
 	int ret;
 
 	mutex_lock(&inode->i_mutex);
-	switch (origin) {
+	switch (whence) {
 	case SEEK_END:
 	case SEEK_CUR:
-		offset = generic_file_llseek(file, offset, origin);
+		offset = generic_file_llseek(file, offset, whence);
 		goto out;
 	case SEEK_DATA:
 	case SEEK_HOLE:
@@ -2249,7 +2249,7 @@ static loff_t btrfs_file_llseek(struct file *file, loff_t offset, int origin)
 			return -ENXIO;
 		}
 
-		ret = find_desired_extent(inode, &offset, origin);
+		ret = find_desired_extent(inode, &offset, whence);
 		if (ret) {
 			mutex_unlock(&inode->i_mutex);
 			return ret;

commit 6c760c072403f446ff829ec9e89568943a3c2ef2
Author: Josef Bacik <jbacik@fusionio.com>
Date:   Fri Nov 9 10:53:21 2012 -0500

    Btrfs: do not call file_update_time in aio_write
    
    This starts a transaction and dirties the inode everytime we call it, which
    is super expensive if you have a write heavy workload.  We will be updating
    the inode when the IO completes and we reserve the space for the inode
    update when we reserve space for the write, so there is no chance of loss of
    information or enospc issues.  Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index c56088ece500..20452c110d7d 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1464,6 +1464,24 @@ static ssize_t __btrfs_direct_write(struct kiocb *iocb,
 	return written ? written : err;
 }
 
+static void update_time_for_write(struct inode *inode)
+{
+	struct timespec now;
+
+	if (IS_NOCMTIME(inode))
+		return;
+
+	now = current_fs_time(inode->i_sb);
+	if (!timespec_equal(&inode->i_mtime, &now))
+		inode->i_mtime = now;
+
+	if (!timespec_equal(&inode->i_ctime, &now))
+		inode->i_ctime = now;
+
+	if (IS_I_VERSION(inode))
+		inode_inc_iversion(inode);
+}
+
 static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 				    const struct iovec *iov,
 				    unsigned long nr_segs, loff_t pos)
@@ -1519,11 +1537,13 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 		goto out;
 	}
 
-	err = file_update_time(file);
-	if (err) {
-		mutex_unlock(&inode->i_mutex);
-		goto out;
-	}
+	/*
+	 * We reserve space for updating the inode when we reserve space for the
+	 * extent we are going to write, so we will enospc out there.  We don't
+	 * need to start yet another transaction to update the inode as we will
+	 * update the inode when we finish writing whatever data we write.
+	 */
+	update_time_for_write(inode);
 
 	start_pos = round_down(pos, root->sectorsize);
 	if (start_pos > i_size_read(inode)) {
@@ -1563,8 +1583,13 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 	 * this will either be one more than the running transaction
 	 * or the generation used for the next transaction if there isn't
 	 * one running right now.
+	 *
+	 * We also have to set last_sub_trans to the current log transid,
+	 * otherwise subsequent syncs to a file that's been synced in this
+	 * transaction will appear to have already occured.
 	 */
 	BTRFS_I(inode)->last_trans = root->fs_info->generation + 1;
+	BTRFS_I(inode)->last_sub_trans = root->log_transid;
 	if (num_written > 0 || num_written == -EIOCBQUEUED) {
 		err = generic_write_sync(file, pos, num_written);
 		if (err < 0 && num_written > 0)

commit 70c8a91ce21b83ccd2d9e7c968775430ead4353d
Author: Josef Bacik <jbacik@fusionio.com>
Date:   Thu Oct 11 16:54:30 2012 -0400

    Btrfs: log changed inodes based on the extent map tree
    
    We don't really need to copy extents from the source tree since we have all
    of the information already available to us in the extent_map tree.  So
    instead just write the extents straight to the log tree and don't bother to
    copy the extent items from the source tree.
    
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 6810145f4e97..c56088ece500 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -621,7 +621,7 @@ void btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
 			} else {
 				split->block_len = split->len;
 				split->block_start = em->block_start + diff;
-				split->orig_start = split->start;
+				split->orig_start = em->orig_start;
 			}
 
 			ret = add_extent_mapping(em_tree, split);

commit b493968096944a11422c4d80fb87af537ca1cac7
Author: Josef Bacik <jbacik@fusionio.com>
Date:   Mon Dec 3 10:31:19 2012 -0500

    Btrfs: keep track of the extents original block length
    
    If we've written to a prealloc extent we need to know the original block len
    for the extent.  We can't figure this out currently since ->block_len is
    just set to the extent length.  So introduce ->orig_block_len so that we
    know how many bytes were in the original extent for proper extent logging
    that future patches will need.  Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 7f4654a15207..6810145f4e97 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -588,6 +588,8 @@ void btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
 				split->block_len = em->block_len;
 			else
 				split->block_len = split->len;
+			split->orig_block_len = max(split->block_len,
+						    em->orig_block_len);
 			split->generation = gen;
 			split->bdev = em->bdev;
 			split->flags = flags;
@@ -609,6 +611,8 @@ void btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
 			split->flags = flags;
 			split->compress_type = em->compress_type;
 			split->generation = gen;
+			split->orig_block_len = max(em->block_len,
+						    em->orig_block_len);
 
 			if (compressed) {
 				split->block_len = em->block_len;
@@ -1838,6 +1842,7 @@ static int fill_holes(struct btrfs_trans_handle *trans, struct inode *inode,
 
 		hole_em->block_start = EXTENT_MAP_HOLE;
 		hole_em->block_len = 0;
+		hole_em->orig_block_len = 0;
 		hole_em->bdev = root->fs_info->fs_devices->latest_bdev;
 		hole_em->compress_type = BTRFS_COMPRESS_NONE;
 		hole_em->generation = trans->transid;

commit b812ce28796f746f14ba6cc451250c422db447b2
Author: Josef Bacik <jbacik@fusionio.com>
Date:   Fri Nov 16 13:56:32 2012 -0500

    Btrfs: inline csums if we're fsyncing
    
    The tree logging stuff needs the csums to be on the ordered extents in order
    to log them properly, so mark that we're sync and inline the csum creation
    so we don't have to wait on the csumming to be done when logging extents
    that are still in flight.  Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 71c2dc1ea15a..7f4654a15207 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1472,6 +1472,7 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 	ssize_t num_written = 0;
 	ssize_t err = 0;
 	size_t count, ocount;
+	bool sync = (file->f_flags & O_DSYNC) || IS_SYNC(file->f_mapping->host);
 
 	sb_start_write(inode->i_sb);
 
@@ -1529,6 +1530,9 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 		}
 	}
 
+	if (sync)
+		atomic_inc(&BTRFS_I(inode)->sync_writers);
+
 	if (unlikely(file->f_flags & O_DIRECT)) {
 		num_written = __btrfs_direct_write(iocb, iov, nr_segs,
 						   pos, ppos, count, ocount);
@@ -1563,6 +1567,8 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 			num_written = err;
 	}
 out:
+	if (sync)
+		atomic_dec(&BTRFS_I(inode)->sync_writers);
 	sb_end_write(inode->i_sb);
 	current->backing_dev_info = NULL;
 	return num_written ? num_written : err;
@@ -1613,7 +1619,9 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 	 * out of the ->i_mutex. If so, we can flush the dirty pages by
 	 * multi-task, and make the performance up.
 	 */
+	atomic_inc(&BTRFS_I(inode)->sync_writers);
 	ret = filemap_write_and_wait_range(inode->i_mapping, start, end);
+	atomic_dec(&BTRFS_I(inode)->sync_writers);
 	if (ret)
 		return ret;
 

commit 7426cc04d407621773af3a0403e57642e40c36bf
Author: Miao Xie <miaox@cn.fujitsu.com>
Date:   Wed Dec 5 10:54:52 2012 +0000

    Btrfs: punch hole past the end of the file
    
    Since we can pre-allocate the space past EOF, we should be able to reclaim
    that space if we need. This patch implements it by removing the EOF check.
    
    Though the manual of fallocate command says we can use truncate command to
    reclaim the pre-allocated space which past EOF, but because truncate command
    changes the file size, we must run several commands to reclaim the space if we
    don't want to change the file size, so it is not a good choice.
    
    Signed-off-by: Miao Xie <miaox@cn.fujitsu.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 700ffd266da3..71c2dc1ea15a 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1873,26 +1873,28 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 	btrfs_wait_ordered_range(inode, offset, len);
 
 	mutex_lock(&inode->i_mutex);
-	if (offset >= inode->i_size) {
-		mutex_unlock(&inode->i_mutex);
-		return 0;
-	}
-
+	/*
+	 * We needn't truncate any page which is beyond the end of the file
+	 * because we are sure there is no data there.
+	 */
 	/*
 	 * Only do this if we are in the same page and we aren't doing the
 	 * entire page.
 	 */
 	if (same_page && len < PAGE_CACHE_SIZE) {
-		ret = btrfs_truncate_page(inode, offset, len, 0);
+		if (offset < round_up(inode->i_size, PAGE_CACHE_SIZE))
+			ret = btrfs_truncate_page(inode, offset, len, 0);
 		mutex_unlock(&inode->i_mutex);
 		return ret;
 	}
 
 	/* zero back part of the first page */
-	ret = btrfs_truncate_page(inode, offset, 0, 0);
-	if (ret) {
-		mutex_unlock(&inode->i_mutex);
-		return ret;
+	if (offset < round_up(inode->i_size, PAGE_CACHE_SIZE)) {
+		ret = btrfs_truncate_page(inode, offset, 0, 0);
+		if (ret) {
+			mutex_unlock(&inode->i_mutex);
+			return ret;
+		}
 	}
 
 	/* zero the front end of the last page */

commit 0061280d2c7240805cfd7b1f493da967c97c2f34
Author: Miao Xie <miaox@cn.fujitsu.com>
Date:   Wed Dec 5 10:54:12 2012 +0000

    Btrfs: fix the page that is beyond EOF
    
    Steps to reproduce:
     # mkfs.btrfs <disk>
     # mount <disk> <mnt>
     # dd if=/dev/zero of=<mnt>/<file> bs=512 seek=5 count=8
     # fallocate -p -o 2048 -l 16384 <mnt>/<file>
     # dd if=/dev/zero of=<mnt>/<file> bs=4096 seek=3 count=8 conv=notrunc,nocreat
     # umount <mnt>
     # dmesg
     WARNING: at fs/btrfs/inode.c:7140 btrfs_destroy_inode+0x2eb/0x330
    
    The reason is that we inputed a range which is beyond the end of the file. And
    because the end of this range was not page-aligned, we had to truncate the last
    page in this range, this operation is similar to a buffered file write. In other
    words, we reserved enough space and clear the data which was in the hole range
    on that page. But when we expanded that test file, write the data into the same
    page, we forgot that we have reserved enough space for the buffered write of
    that page because in most cases there is no page that is beyond the end of
    the file. As a result, we reserved the space twice.
    
    In fact, we needn't truncate the page if it is beyond the end of the file, just
    release the allocated space in that range. Fix the above problem by this way.
    
    Signed-off-by: Miao Xie <miaox@cn.fujitsu.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index d75412bf7c4a..700ffd266da3 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1859,9 +1859,9 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 	struct btrfs_path *path;
 	struct btrfs_block_rsv *rsv;
 	struct btrfs_trans_handle *trans;
-	u64 mask = BTRFS_I(inode)->root->sectorsize - 1;
-	u64 lockstart = (offset + mask) & ~mask;
-	u64 lockend = ((offset + len) & ~mask) - 1;
+	u64 lockstart = round_up(offset, BTRFS_I(inode)->root->sectorsize);
+	u64 lockend = round_down(offset + len,
+				 BTRFS_I(inode)->root->sectorsize) - 1;
 	u64 cur_offset = lockstart;
 	u64 min_size = btrfs_calc_trunc_metadata_size(root, 1);
 	u64 drop_end;
@@ -1896,10 +1896,12 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 	}
 
 	/* zero the front end of the last page */
-	ret = btrfs_truncate_page(inode, offset + len, 0, 1);
-	if (ret) {
-		mutex_unlock(&inode->i_mutex);
-		return ret;
+	if (offset + len < round_up(inode->i_size, PAGE_CACHE_SIZE)) {
+		ret = btrfs_truncate_page(inode, offset + len, 0, 1);
+		if (ret) {
+			mutex_unlock(&inode->i_mutex);
+			return ret;
+		}
 	}
 
 	if (lockend < lockstart) {

commit 6347b3c433a4cff00eb2299c7f2c7d1d8b24c1fc
Author: Miao Xie <miaox@cn.fujitsu.com>
Date:   Wed Dec 5 10:53:45 2012 +0000

    Btrfs: fix off-by-one error of the same page check in btrfs_punch_hole()
    
    (start + len) is the start of the adjacent extent, not the end of the current
    extent, so we should not use it to check the hole is on the same page or not.
    
    Signed-off-by: Miao Xie <miaox@cn.fujitsu.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 8e3d6788d6dd..d75412bf7c4a 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1867,8 +1867,8 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 	u64 drop_end;
 	int ret = 0;
 	int err = 0;
-	bool same_page = (offset >> PAGE_CACHE_SHIFT) ==
-		((offset + len) >> PAGE_CACHE_SHIFT);
+	bool same_page = ((offset >> PAGE_CACHE_SHIFT) ==
+			  ((offset + len - 1) >> PAGE_CACHE_SHIFT));
 
 	btrfs_wait_ordered_range(inode, offset, len);
 

commit 0ff6fabdb0a862b22df4dd75873578392478e64d
Author: Miao Xie <miaox@cn.fujitsu.com>
Date:   Wed Nov 28 10:28:54 2012 +0000

    Btrfs: fix off-by-one error of the reserved size of btrfs_allocate()
    
    alloc_end is not the real end of the current extent, it is the start of the
    next adjoining extent. So we needn't +1 when calculating the size the space
    that is about to be reserved.
    
    Signed-off-by: Miao Xie <miaox@cn.fujitsu.com>
    Reviewed-by: Liu Bo <bo.li.liu@oracle.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index a43d0aef6ee1..8e3d6788d6dd 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2072,7 +2072,7 @@ static long btrfs_fallocate(struct file *file, int mode,
 	 * Make sure we have enough space before we do the
 	 * allocation.
 	 */
-	ret = btrfs_check_data_free_space(inode, alloc_end - alloc_start + 1);
+	ret = btrfs_check_data_free_space(inode, alloc_end - alloc_start);
 	if (ret)
 		return ret;
 
@@ -2179,7 +2179,7 @@ static long btrfs_fallocate(struct file *file, int mode,
 out:
 	mutex_unlock(&inode->i_mutex);
 	/* Let go of our reservation. */
-	btrfs_free_reserved_data_space(inode, alloc_end - alloc_start + 1);
+	btrfs_free_reserved_data_space(inode, alloc_end - alloc_start);
 	return ret;
 }
 

commit 797f4277113bff142b6c64a55abaef64d7d67d5c
Author: Miao Xie <miaox@cn.fujitsu.com>
Date:   Wed Nov 28 10:28:07 2012 +0000

    Btrfs: use existing align macros in btrfs_allocate()
    
    The kernel developers have implemented some often-used align macros, we should
    use them instead of the complex code.
    
    Signed-off-by: Miao Xie <miaox@cn.fujitsu.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index d415a052ca9a..a43d0aef6ee1 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2054,12 +2054,12 @@ static long btrfs_fallocate(struct file *file, int mode,
 	u64 alloc_end;
 	u64 alloc_hint = 0;
 	u64 locked_end;
-	u64 mask = BTRFS_I(inode)->root->sectorsize - 1;
 	struct extent_map *em;
+	int blocksize = BTRFS_I(inode)->root->sectorsize;
 	int ret;
 
-	alloc_start = offset & ~mask;
-	alloc_end =  (offset + len + mask) & ~mask;
+	alloc_start = round_down(offset, blocksize);
+	alloc_end = round_up(offset + len, blocksize);
 
 	/* Make sure we aren't being give some crap mode */
 	if (mode & ~(FALLOC_FL_KEEP_SIZE | FALLOC_FL_PUNCH_HOLE))
@@ -2140,7 +2140,7 @@ static long btrfs_fallocate(struct file *file, int mode,
 		}
 		last_byte = min(extent_map_end(em), alloc_end);
 		actual_end = min_t(u64, extent_map_end(em), offset + len);
-		last_byte = (last_byte + mask) & ~mask;
+		last_byte = ALIGN(last_byte, blocksize);
 
 		if (em->block_start == EXTENT_MAP_HOLE ||
 		    (cur_offset >= inode->i_size &&

commit b66f00da0cfceb856c17706b77906b63437f6fda
Author: Miao Xie <miaox@cn.fujitsu.com>
Date:   Mon Nov 26 09:27:29 2012 +0000

    Btrfs: fix freeze vs auto defrag
    
    If we freeze the fs, the auto defragment should not run. Fix it.
    
    Signed-off-by: Miao Xie <miaox@cn.fujitsu.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 3c6f7479cd5b..d415a052ca9a 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -318,8 +318,11 @@ static int __btrfs_run_defrag_inode(struct btrfs_fs_info *fs_info,
 	memset(&range, 0, sizeof(range));
 	range.len = (u64)-1;
 	range.start = defrag->last_offset;
+
+	sb_start_write(fs_info->sb);
 	num_defrag = btrfs_defrag_file(inode, NULL, &range, defrag->transid,
 				       BTRFS_DEFRAG_BATCH);
+	sb_end_write(fs_info->sb);
 	/*
 	 * if we filled the whole defrag batch, there
 	 * must be more work to do.  Queue this defrag

commit 26176e7c2aa923327becdc25b5aca2cb907ac932
Author: Miao Xie <miaox@cn.fujitsu.com>
Date:   Mon Nov 26 09:26:20 2012 +0000

    Btrfs: restructure btrfs_run_defrag_inodes()
    
    This patch restructure btrfs_run_defrag_inodes() and make the code of the auto
    defragment more readable.
    
    Signed-off-by: Miao Xie <miaox@cn.fujitsu.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 00918321e390..3c6f7479cd5b 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -216,11 +216,11 @@ void btrfs_requeue_inode_defrag(struct inode *inode,
 }
 
 /*
- * must be called with the defrag_inodes lock held
+ * pick the defragable inode that we want, if it doesn't exist, we will get
+ * the next one.
  */
-struct inode_defrag *btrfs_find_defrag_inode(struct btrfs_fs_info *info,
-					     u64 root, u64 ino,
-					     struct rb_node **next)
+static struct inode_defrag *
+btrfs_pick_defrag_inode(struct btrfs_fs_info *fs_info, u64 root, u64 ino)
 {
 	struct inode_defrag *entry = NULL;
 	struct inode_defrag tmp;
@@ -231,7 +231,8 @@ struct inode_defrag *btrfs_find_defrag_inode(struct btrfs_fs_info *info,
 	tmp.ino = ino;
 	tmp.root = root;
 
-	p = info->defrag_inodes.rb_node;
+	spin_lock(&fs_info->defrag_inodes_lock);
+	p = fs_info->defrag_inodes.rb_node;
 	while (p) {
 		parent = p;
 		entry = rb_entry(parent, struct inode_defrag, rb_node);
@@ -242,52 +243,128 @@ struct inode_defrag *btrfs_find_defrag_inode(struct btrfs_fs_info *info,
 		else if (ret > 0)
 			p = parent->rb_right;
 		else
-			return entry;
+			goto out;
 	}
 
-	if (next) {
-		while (parent && __compare_inode_defrag(&tmp, entry) > 0) {
-			parent = rb_next(parent);
+	if (parent && __compare_inode_defrag(&tmp, entry) > 0) {
+		parent = rb_next(parent);
+		if (parent)
 			entry = rb_entry(parent, struct inode_defrag, rb_node);
-		}
-		*next = parent;
+		else
+			entry = NULL;
 	}
-	return NULL;
+out:
+	if (entry)
+		rb_erase(parent, &fs_info->defrag_inodes);
+	spin_unlock(&fs_info->defrag_inodes_lock);
+	return entry;
 }
 
-/*
- * run through the list of inodes in the FS that need
- * defragging
- */
-int btrfs_run_defrag_inodes(struct btrfs_fs_info *fs_info)
+void btrfs_cleanup_defrag_inodes(struct btrfs_fs_info *fs_info)
 {
 	struct inode_defrag *defrag;
+	struct rb_node *node;
+
+	spin_lock(&fs_info->defrag_inodes_lock);
+	node = rb_first(&fs_info->defrag_inodes);
+	while (node) {
+		rb_erase(node, &fs_info->defrag_inodes);
+		defrag = rb_entry(node, struct inode_defrag, rb_node);
+		kmem_cache_free(btrfs_inode_defrag_cachep, defrag);
+
+		if (need_resched()) {
+			spin_unlock(&fs_info->defrag_inodes_lock);
+			cond_resched();
+			spin_lock(&fs_info->defrag_inodes_lock);
+		}
+
+		node = rb_first(&fs_info->defrag_inodes);
+	}
+	spin_unlock(&fs_info->defrag_inodes_lock);
+}
+
+#define BTRFS_DEFRAG_BATCH	1024
+
+static int __btrfs_run_defrag_inode(struct btrfs_fs_info *fs_info,
+				    struct inode_defrag *defrag)
+{
 	struct btrfs_root *inode_root;
 	struct inode *inode;
-	struct rb_node *n;
 	struct btrfs_key key;
 	struct btrfs_ioctl_defrag_range_args range;
-	u64 first_ino = 0;
-	u64 root_objectid = 0;
 	int num_defrag;
-	int defrag_batch = 1024;
 
+	/* get the inode */
+	key.objectid = defrag->root;
+	btrfs_set_key_type(&key, BTRFS_ROOT_ITEM_KEY);
+	key.offset = (u64)-1;
+	inode_root = btrfs_read_fs_root_no_name(fs_info, &key);
+	if (IS_ERR(inode_root)) {
+		kmem_cache_free(btrfs_inode_defrag_cachep, defrag);
+		return PTR_ERR(inode_root);
+	}
+
+	key.objectid = defrag->ino;
+	btrfs_set_key_type(&key, BTRFS_INODE_ITEM_KEY);
+	key.offset = 0;
+	inode = btrfs_iget(fs_info->sb, &key, inode_root, NULL);
+	if (IS_ERR(inode)) {
+		kmem_cache_free(btrfs_inode_defrag_cachep, defrag);
+		return PTR_ERR(inode);
+	}
+
+	/* do a chunk of defrag */
+	clear_bit(BTRFS_INODE_IN_DEFRAG, &BTRFS_I(inode)->runtime_flags);
 	memset(&range, 0, sizeof(range));
 	range.len = (u64)-1;
+	range.start = defrag->last_offset;
+	num_defrag = btrfs_defrag_file(inode, NULL, &range, defrag->transid,
+				       BTRFS_DEFRAG_BATCH);
+	/*
+	 * if we filled the whole defrag batch, there
+	 * must be more work to do.  Queue this defrag
+	 * again
+	 */
+	if (num_defrag == BTRFS_DEFRAG_BATCH) {
+		defrag->last_offset = range.start;
+		btrfs_requeue_inode_defrag(inode, defrag);
+	} else if (defrag->last_offset && !defrag->cycled) {
+		/*
+		 * we didn't fill our defrag batch, but
+		 * we didn't start at zero.  Make sure we loop
+		 * around to the start of the file.
+		 */
+		defrag->last_offset = 0;
+		defrag->cycled = 1;
+		btrfs_requeue_inode_defrag(inode, defrag);
+	} else {
+		kmem_cache_free(btrfs_inode_defrag_cachep, defrag);
+	}
+
+	iput(inode);
+	return 0;
+}
+
+/*
+ * run through the list of inodes in the FS that need
+ * defragging
+ */
+int btrfs_run_defrag_inodes(struct btrfs_fs_info *fs_info)
+{
+	struct inode_defrag *defrag;
+	u64 first_ino = 0;
+	u64 root_objectid = 0;
 
 	atomic_inc(&fs_info->defrag_running);
-	spin_lock(&fs_info->defrag_inodes_lock);
 	while(1) {
-		n = NULL;
+		if (!__need_auto_defrag(fs_info->tree_root))
+			break;
 
 		/* find an inode to defrag */
-		defrag = btrfs_find_defrag_inode(fs_info, root_objectid,
-						 first_ino, &n);
+		defrag = btrfs_pick_defrag_inode(fs_info, root_objectid,
+						 first_ino);
 		if (!defrag) {
-			if (n) {
-				defrag = rb_entry(n, struct inode_defrag,
-						  rb_node);
-			} else if (root_objectid || first_ino) {
+			if (root_objectid || first_ino) {
 				root_objectid = 0;
 				first_ino = 0;
 				continue;
@@ -296,71 +373,11 @@ int btrfs_run_defrag_inodes(struct btrfs_fs_info *fs_info)
 			}
 		}
 
-		/* remove it from the rbtree */
 		first_ino = defrag->ino + 1;
 		root_objectid = defrag->root;
-		rb_erase(&defrag->rb_node, &fs_info->defrag_inodes);
-
-		if (btrfs_fs_closing(fs_info))
-			goto next_free;
 
-		spin_unlock(&fs_info->defrag_inodes_lock);
-
-		/* get the inode */
-		key.objectid = defrag->root;
-		btrfs_set_key_type(&key, BTRFS_ROOT_ITEM_KEY);
-		key.offset = (u64)-1;
-		inode_root = btrfs_read_fs_root_no_name(fs_info, &key);
-		if (IS_ERR(inode_root))
-			goto next;
-
-		key.objectid = defrag->ino;
-		btrfs_set_key_type(&key, BTRFS_INODE_ITEM_KEY);
-		key.offset = 0;
-
-		inode = btrfs_iget(fs_info->sb, &key, inode_root, NULL);
-		if (IS_ERR(inode))
-			goto next;
-
-		/* do a chunk of defrag */
-		clear_bit(BTRFS_INODE_IN_DEFRAG, &BTRFS_I(inode)->runtime_flags);
-		range.start = defrag->last_offset;
-		num_defrag = btrfs_defrag_file(inode, NULL, &range, defrag->transid,
-					       defrag_batch);
-		/*
-		 * if we filled the whole defrag batch, there
-		 * must be more work to do.  Queue this defrag
-		 * again
-		 */
-		if (num_defrag == defrag_batch) {
-			defrag->last_offset = range.start;
-			btrfs_requeue_inode_defrag(inode, defrag);
-			/*
-			 * we don't want to kfree defrag, we added it back to
-			 * the rbtree
-			 */
-			defrag = NULL;
-		} else if (defrag->last_offset && !defrag->cycled) {
-			/*
-			 * we didn't fill our defrag batch, but
-			 * we didn't start at zero.  Make sure we loop
-			 * around to the start of the file.
-			 */
-			defrag->last_offset = 0;
-			defrag->cycled = 1;
-			btrfs_requeue_inode_defrag(inode, defrag);
-			defrag = NULL;
-		}
-
-		iput(inode);
-next:
-		spin_lock(&fs_info->defrag_inodes_lock);
-next_free:
-		if (defrag)
-			kmem_cache_free(btrfs_inode_defrag_cachep, defrag);
+		__btrfs_run_defrag_inode(fs_info, defrag);
 	}
-	spin_unlock(&fs_info->defrag_inodes_lock);
-
 	atomic_dec(&fs_info->defrag_running);
 
 	/*

commit 8ddc473433b5e8ce8693db9f6e251f5a28267528
Author: Miao Xie <miaox@cn.fujitsu.com>
Date:   Mon Nov 26 09:25:38 2012 +0000

    Btrfs: fix unprotected defragable inode insertion
    
    We forget to get the defrag lock when we re-add the defragable inode,
    Fix it.
    
    Signed-off-by: Miao Xie <miaox@cn.fujitsu.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 15117eae85c4..00918321e390 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -91,7 +91,7 @@ static int __compare_inode_defrag(struct inode_defrag *defrag1,
  * If an existing record is found the defrag item you
  * pass in is freed
  */
-static void __btrfs_add_inode_defrag(struct inode *inode,
+static int __btrfs_add_inode_defrag(struct inode *inode,
 				    struct inode_defrag *defrag)
 {
 	struct btrfs_root *root = BTRFS_I(inode)->root;
@@ -119,18 +119,24 @@ static void __btrfs_add_inode_defrag(struct inode *inode,
 				entry->transid = defrag->transid;
 			if (defrag->last_offset > entry->last_offset)
 				entry->last_offset = defrag->last_offset;
-			goto exists;
+			return -EEXIST;
 		}
 	}
 	set_bit(BTRFS_INODE_IN_DEFRAG, &BTRFS_I(inode)->runtime_flags);
 	rb_link_node(&defrag->rb_node, parent, p);
 	rb_insert_color(&defrag->rb_node, &root->fs_info->defrag_inodes);
-	return;
+	return 0;
+}
 
-exists:
-	kmem_cache_free(btrfs_inode_defrag_cachep, defrag);
-	return;
+static inline int __need_auto_defrag(struct btrfs_root *root)
+{
+	if (!btrfs_test_opt(root, AUTO_DEFRAG))
+		return 0;
+
+	if (btrfs_fs_closing(root->fs_info))
+		return 0;
 
+	return 1;
 }
 
 /*
@@ -143,11 +149,9 @@ int btrfs_add_inode_defrag(struct btrfs_trans_handle *trans,
 	struct btrfs_root *root = BTRFS_I(inode)->root;
 	struct inode_defrag *defrag;
 	u64 transid;
+	int ret;
 
-	if (!btrfs_test_opt(root, AUTO_DEFRAG))
-		return 0;
-
-	if (btrfs_fs_closing(root->fs_info))
+	if (!__need_auto_defrag(root))
 		return 0;
 
 	if (test_bit(BTRFS_INODE_IN_DEFRAG, &BTRFS_I(inode)->runtime_flags))
@@ -167,14 +171,50 @@ int btrfs_add_inode_defrag(struct btrfs_trans_handle *trans,
 	defrag->root = root->root_key.objectid;
 
 	spin_lock(&root->fs_info->defrag_inodes_lock);
-	if (!test_bit(BTRFS_INODE_IN_DEFRAG, &BTRFS_I(inode)->runtime_flags))
-		__btrfs_add_inode_defrag(inode, defrag);
-	else
+	if (!test_bit(BTRFS_INODE_IN_DEFRAG, &BTRFS_I(inode)->runtime_flags)) {
+		/*
+		 * If we set IN_DEFRAG flag and evict the inode from memory,
+		 * and then re-read this inode, this new inode doesn't have
+		 * IN_DEFRAG flag. At the case, we may find the existed defrag.
+		 */
+		ret = __btrfs_add_inode_defrag(inode, defrag);
+		if (ret)
+			kmem_cache_free(btrfs_inode_defrag_cachep, defrag);
+	} else {
 		kmem_cache_free(btrfs_inode_defrag_cachep, defrag);
+	}
 	spin_unlock(&root->fs_info->defrag_inodes_lock);
 	return 0;
 }
 
+/*
+ * Requeue the defrag object. If there is a defrag object that points to
+ * the same inode in the tree, we will merge them together (by
+ * __btrfs_add_inode_defrag()) and free the one that we want to requeue.
+ */
+void btrfs_requeue_inode_defrag(struct inode *inode,
+				struct inode_defrag *defrag)
+{
+	struct btrfs_root *root = BTRFS_I(inode)->root;
+	int ret;
+
+	if (!__need_auto_defrag(root))
+		goto out;
+
+	/*
+	 * Here we don't check the IN_DEFRAG flag, because we need merge
+	 * them together.
+	 */
+	spin_lock(&root->fs_info->defrag_inodes_lock);
+	ret = __btrfs_add_inode_defrag(inode, defrag);
+	spin_unlock(&root->fs_info->defrag_inodes_lock);
+	if (ret)
+		goto out;
+	return;
+out:
+	kmem_cache_free(btrfs_inode_defrag_cachep, defrag);
+}
+
 /*
  * must be called with the defrag_inodes lock held
  */
@@ -294,7 +334,7 @@ int btrfs_run_defrag_inodes(struct btrfs_fs_info *fs_info)
 		 */
 		if (num_defrag == defrag_batch) {
 			defrag->last_offset = range.start;
-			__btrfs_add_inode_defrag(inode, defrag);
+			btrfs_requeue_inode_defrag(inode, defrag);
 			/*
 			 * we don't want to kfree defrag, we added it back to
 			 * the rbtree
@@ -308,7 +348,7 @@ int btrfs_run_defrag_inodes(struct btrfs_fs_info *fs_info)
 			 */
 			defrag->last_offset = 0;
 			defrag->cycled = 1;
-			__btrfs_add_inode_defrag(inode, defrag);
+			btrfs_requeue_inode_defrag(inode, defrag);
 			defrag = NULL;
 		}
 

commit 9247f3170b2c3d648707c93bbebcd763fac17c06
Author: Miao Xie <miaox@cn.fujitsu.com>
Date:   Mon Nov 26 09:24:43 2012 +0000

    Btrfs: use slabs for auto defrag allocation
    
    The auto defrag allocation is in the fast path of the IO, so use slabs
    to improve the speed of the allocation.
    
    And besides that, it can do check for leaked objects when the module is removed.
    
    Signed-off-by: Miao Xie <miaox@cn.fujitsu.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index bd7f1b01e051..15117eae85c4 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -41,6 +41,7 @@
 #include "compat.h"
 #include "volumes.h"
 
+static struct kmem_cache *btrfs_inode_defrag_cachep;
 /*
  * when auto defrag is enabled we
  * queue up these defrag structs to remember which
@@ -127,7 +128,7 @@ static void __btrfs_add_inode_defrag(struct inode *inode,
 	return;
 
 exists:
-	kfree(defrag);
+	kmem_cache_free(btrfs_inode_defrag_cachep, defrag);
 	return;
 
 }
@@ -157,7 +158,7 @@ int btrfs_add_inode_defrag(struct btrfs_trans_handle *trans,
 	else
 		transid = BTRFS_I(inode)->root->last_trans;
 
-	defrag = kzalloc(sizeof(*defrag), GFP_NOFS);
+	defrag = kmem_cache_zalloc(btrfs_inode_defrag_cachep, GFP_NOFS);
 	if (!defrag)
 		return -ENOMEM;
 
@@ -169,7 +170,7 @@ int btrfs_add_inode_defrag(struct btrfs_trans_handle *trans,
 	if (!test_bit(BTRFS_INODE_IN_DEFRAG, &BTRFS_I(inode)->runtime_flags))
 		__btrfs_add_inode_defrag(inode, defrag);
 	else
-		kfree(defrag);
+		kmem_cache_free(btrfs_inode_defrag_cachep, defrag);
 	spin_unlock(&root->fs_info->defrag_inodes_lock);
 	return 0;
 }
@@ -315,7 +316,8 @@ int btrfs_run_defrag_inodes(struct btrfs_fs_info *fs_info)
 next:
 		spin_lock(&fs_info->defrag_inodes_lock);
 next_free:
-		kfree(defrag);
+		if (defrag)
+			kmem_cache_free(btrfs_inode_defrag_cachep, defrag);
 	}
 	spin_unlock(&fs_info->defrag_inodes_lock);
 
@@ -2293,3 +2295,21 @@ const struct file_operations btrfs_file_operations = {
 	.compat_ioctl	= btrfs_ioctl,
 #endif
 };
+
+void btrfs_auto_defrag_exit(void)
+{
+	if (btrfs_inode_defrag_cachep)
+		kmem_cache_destroy(btrfs_inode_defrag_cachep);
+}
+
+int btrfs_auto_defrag_init(void)
+{
+	btrfs_inode_defrag_cachep = kmem_cache_create("btrfs_inode_defrag",
+					sizeof(struct inode_defrag), 0,
+					SLAB_RECLAIM_ACCOUNT | SLAB_MEM_SPREAD,
+					NULL);
+	if (!btrfs_inode_defrag_cachep)
+		return -ENOMEM;
+
+	return 0;
+}

commit b53d3f5db2b79637acadc06a330db6c2c60863f5
Author: Liu Bo <bo.li.liu@oracle.com>
Date:   Wed Nov 14 14:34:34 2012 +0000

    Btrfs: cleanup for btrfs_btree_balance_dirty
    
    - 'nr' is no more used.
    - btrfs_btree_balance_dirty() and __btrfs_btree_balance_dirty() can share
      a bunch of code.
    
    Signed-off-by: Liu Bo <bo.li.liu@oracle.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 883cf826cf25..bd7f1b01e051 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1349,7 +1349,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 		balance_dirty_pages_ratelimited_nr(inode->i_mapping,
 						   dirty_pages);
 		if (dirty_pages < (root->leafsize >> PAGE_CACHE_SHIFT) + 1)
-			btrfs_btree_balance_dirty(root, 1);
+			btrfs_btree_balance_dirty(root);
 
 		pos += copied;
 		num_written += copied;
@@ -1803,7 +1803,6 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 	u64 cur_offset = lockstart;
 	u64 min_size = btrfs_calc_trunc_metadata_size(root, 1);
 	u64 drop_end;
-	unsigned long nr;
 	int ret = 0;
 	int err = 0;
 	bool same_page = (offset >> PAGE_CACHE_SHIFT) ==
@@ -1931,9 +1930,8 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 			break;
 		}
 
-		nr = trans->blocks_used;
 		btrfs_end_transaction(trans, root);
-		btrfs_btree_balance_dirty(root, nr);
+		btrfs_btree_balance_dirty(root);
 
 		trans = btrfs_start_transaction(root, 3);
 		if (IS_ERR(trans)) {
@@ -1969,9 +1967,8 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 
 	trans->block_rsv = &root->fs_info->trans_block_rsv;
 	ret = btrfs_update_inode(trans, root, inode);
-	nr = trans->blocks_used;
 	btrfs_end_transaction(trans, root);
-	btrfs_btree_balance_dirty(root, nr);
+	btrfs_btree_balance_dirty(root);
 out_free:
 	btrfs_free_path(path);
 	btrfs_free_block_rsv(root, rsv);

commit e1f5790e0588bc5b11eb57f95bfde8702049dd0d
Author: Tsutomu Itoh <t-itoh@jp.fujitsu.com>
Date:   Thu Nov 8 04:47:33 2012 +0000

    Btrfs: set hole punching time properly
    
    Even if the hole punching is executed, the modification time of the
    file is not updated.
    So, current time is set to inode.
    
    Signed-off-by: Tsutomu Itoh <t-itoh@jp.fujitsu.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index d2df98124d0f..883cf826cf25 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1964,6 +1964,9 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 	if (!trans)
 		goto out_free;
 
+	inode_inc_iversion(inode);
+	inode->i_mtime = inode->i_ctime = CURRENT_TIME;
+
 	trans->block_rsv = &root->fs_info->trans_block_rsv;
 	ret = btrfs_update_inode(trans, root, inode);
 	nr = trans->blocks_used;

commit 9f3959c53d57d010ae6f4205fbd0159cb7976a83
Author: Liu Bo <bo.li.liu@oracle.com>
Date:   Thu Nov 1 06:38:48 2012 +0000

    Btrfs: get right arguments for btrfs_wait_ordered_range
    
    btrfs_wait_ordered_range expects for 'len' instead of 'end'.
    
    Signed-off-by: Liu Bo <bo.li.liu@oracle.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 9ab1bed88116..d2df98124d0f 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1562,7 +1562,7 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 	 * range being left.
 	 */
 	atomic_inc(&root->log_batch);
-	btrfs_wait_ordered_range(inode, start, end);
+	btrfs_wait_ordered_range(inode, start, end - start + 1);
 	atomic_inc(&root->log_batch);
 
 	/*

commit d0e1d66b5aa1ec9f556f951aa9a114cc192cd01c
Author: Namjae Jeon <linkinjeon@gmail.com>
Date:   Tue Dec 11 16:00:21 2012 -0800

    writeback: remove nr_pages_dirtied arg from balance_dirty_pages_ratelimited_nr()
    
    There is no reason to pass the nr_pages_dirtied argument, because
    nr_pages_dirtied value from the caller is unused in
    balance_dirty_pages_ratelimited_nr().
    
    Signed-off-by: Namjae Jeon <linkinjeon@gmail.com>
    Signed-off-by: Vivek Trivedi <vtrivedi018@gmail.com>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 9ab1bed88116..a8ee75cb96ee 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1346,8 +1346,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 
 		cond_resched();
 
-		balance_dirty_pages_ratelimited_nr(inode->i_mapping,
-						   dirty_pages);
+		balance_dirty_pages_ratelimited(inode->i_mapping);
 		if (dirty_pages < (root->leafsize >> PAGE_CACHE_SHIFT) + 1)
 			btrfs_btree_balance_dirty(root, 1);
 

commit 72055425e53540d9d0e59a57ac8c9b8ce77b62d5
Merge: fc81c038c2d6 f46dbe3dee85
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Oct 10 10:49:20 2012 +0900

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs
    
    Pull btrfs update from Chris Mason:
     "This is a large pull, with the bulk of the updates coming from:
    
       - Hole punching
    
       - send/receive fixes
    
       - fsync performance
    
       - Disk format extension allowing more hardlinks inside a single
         directory (btrfs-progs patch required to enable the compat bit for
         this one)
    
      I'm cooking more unrelated RAID code, but I wanted to make sure this
      original batch makes it in.  The largest updates here are relatively
      old and have been in testing for some time."
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs: (121 commits)
      btrfs: init ref_index to zero in add_inode_ref
      Btrfs: remove repeated eb->pages check in, disk-io.c/csum_dirty_buffer
      Btrfs: fix page leakage
      Btrfs: do not warn_on when we cannot alloc a page for an extent buffer
      Btrfs: don't bug on enomem in readpage
      Btrfs: cleanup pages properly when ENOMEM in compression
      Btrfs: make filesystem read-only when submitting barrier fails
      Btrfs: detect corrupted filesystem after write I/O errors
      Btrfs: make compress and nodatacow mount options mutually exclusive
      btrfs: fix message printing
      Btrfs: don't bother committing delayed inode updates when fsyncing
      btrfs: move inline function code to header file
      Btrfs: remove unnecessary IS_ERR in bio_readpage_error()
      btrfs: remove unused function btrfs_insert_some_items()
      Btrfs: don't commit instead of overcommitting
      Btrfs: confirmation of value is added before trace_btrfs_get_extent() is called
      Btrfs: be smarter about dropping things from the tree log
      Btrfs: don't lookup csums for prealloc extents
      Btrfs: cache extent state when writing out dirty metadata pages
      Btrfs: do not hold the file extent leaf locked when adding extent item
      ...

commit 0b173bc4daa8f8ec03a85abf5e47b23502ff80af
Author: Konstantin Khlebnikov <khlebnikov@openvz.org>
Date:   Mon Oct 8 16:28:46 2012 -0700

    mm: kill vma flag VM_CAN_NONLINEAR
    
    Move actual pte filling for non-linear file mappings into the new special
    vma operation: ->remap_pages().
    
    Filesystems must implement this method to get non-linear mapping support,
    if it uses filemap_fault() then generic_file_remap_pages() can be used.
    
    Now device drivers can implement this method and obtain nonlinear vma support.
    
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: Carsten Otte <cotte@de.ibm.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com> #arch/tile
    Cc: Cyrill Gorcunov <gorcunov@openvz.org>
    Cc: Eric Paris <eparis@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: James Morris <james.l.morris@oracle.com>
    Cc: Jason Baron <jbaron@redhat.com>
    Cc: Kentaro Takeda <takedakn@nttdata.co.jp>
    Cc: Matt Helsley <matthltc@us.ibm.com>
    Cc: Nick Piggin <npiggin@kernel.dk>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Robert Richter <robert.richter@amd.com>
    Cc: Suresh Siddha <suresh.b.siddha@intel.com>
    Cc: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Cc: Venkatesh Pallipadi <venki@google.com>
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 5caf285c6e4d..f6b40e86121b 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1599,6 +1599,7 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 static const struct vm_operations_struct btrfs_file_vm_ops = {
 	.fault		= filemap_fault,
 	.page_mkwrite	= btrfs_page_mkwrite,
+	.remap_pages	= generic_file_remap_pages,
 };
 
 static int btrfs_file_mmap(struct file	*filp, struct vm_area_struct *vma)
@@ -1610,7 +1611,6 @@ static int btrfs_file_mmap(struct file	*filp, struct vm_area_struct *vma)
 
 	file_accessed(filp);
 	vma->vm_ops = &btrfs_file_vm_ops;
-	vma->vm_flags |= VM_CAN_NONLINEAR;
 
 	return 0;
 }

commit c3308f84c1743eabb91f4976a314d118d5ea2342
Author: Josef Bacik <jbacik@fusionio.com>
Date:   Fri Sep 14 14:51:22 2012 -0400

    Btrfs: fix punch hole when no extent exists
    
    I saw the warning in btrfs_drop_extent_cache where our end is less than our
    start while running xfstests 68 in a loop.  This is because we
    unconditionally do drop_end = min(end, extent_end) in
    __btrfs_drop_extents(), even though we may not have found an extent in the
    range we were looking to drop.  So keep track of wether or not we found
    something, and if we didn't just use our end.  Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index d0fc4c5aaf15..110d3cb7b6fe 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -609,6 +609,7 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 	int ret;
 	int modify_tree = -1;
 	int update_refs = (root->ref_cows || root == root->fs_info->tree_root);
+	int found = 0;
 
 	if (drop_cache)
 		btrfs_drop_extent_cache(inode, start, end - 1, 0);
@@ -674,6 +675,7 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 			goto next_slot;
 		}
 
+		found = 1;
 		search_start = max(key.offset, start);
 		if (recow || !modify_tree) {
 			modify_tree = -1;
@@ -829,7 +831,7 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 	}
 
 	if (drop_end)
-		*drop_end = min(end, extent_end);
+		*drop_end = found ? min(end, extent_end) : end;
 	btrfs_release_path(path);
 	return ret;
 }

commit 90abccf2c6e6e9c5a5d519eaed95292afa30aa11
Author: Miao Xie <miaox@cn.fujitsu.com>
Date:   Thu Sep 13 04:53:47 2012 -0600

    Revert "Btrfs: do not do filemap_write_and_wait_range in fsync"
    
    This reverts commit 0885ef5b5601e9b007c383e77c172769b1f214fd
    
    After applying the above patch, the performance slowed down because the dirty
    page flush can only be done by one task, so revert it.
    
    The following is the test result of sysbench:
            Before          After
            24MB/s          39MB/s
    
    Signed-off-by: Miao Xie <miaox@cn.fujitsu.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 0a4b03d8fcd6..d0fc4c5aaf15 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1544,12 +1544,20 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 
 	trace_btrfs_sync_file(file, datasync);
 
+	/*
+	 * We write the dirty pages in the range and wait until they complete
+	 * out of the ->i_mutex. If so, we can flush the dirty pages by
+	 * multi-task, and make the performance up.
+	 */
+	ret = filemap_write_and_wait_range(inode->i_mapping, start, end);
+	if (ret)
+		return ret;
+
 	mutex_lock(&inode->i_mutex);
 
 	/*
-	 * we wait first, since the writeback may change the inode, also wait
-	 * ordered range does a filemape_write_and_wait_range which is why we
-	 * don't do it above like other file systems.
+	 * We flush the dirty pages again to avoid some dirty pages in the
+	 * range being left.
 	 */
 	atomic_inc(&root->log_batch);
 	btrfs_wait_ordered_range(inode, start, end);

commit 9e8a4a8b0b9484e8d14674fc62c9ad8ac9dbce5b
Author: Liu Bo <bo.li.liu@oracle.com>
Date:   Wed Sep 5 19:10:51 2012 -0600

    Btrfs: use flag EXTENT_DEFRAG for snapshot-aware defrag
    
    We're going to use this flag EXTENT_DEFRAG to indicate which range
    belongs to defragment so that we can implement snapshow-aware defrag:
    
    We set the EXTENT_DEFRAG flag when dirtying the extents that need
    defragmented, so later on writeback thread can differentiate between
    normal writeback and writeback started by defragmentation.
    
    Original-Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Signed-off-by: Liu Bo <bo.li.liu@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 00279c9a7f35..0a4b03d8fcd6 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1203,8 +1203,8 @@ static noinline int prepare_pages(struct btrfs_root *root, struct file *file,
 
 		clear_extent_bit(&BTRFS_I(inode)->io_tree, start_pos,
 				  last_pos - 1, EXTENT_DIRTY | EXTENT_DELALLOC |
-				  EXTENT_DO_ACCOUNTING, 0, 0, &cached_state,
-				  GFP_NOFS);
+				  EXTENT_DO_ACCOUNTING | EXTENT_DEFRAG,
+				  0, 0, &cached_state, GFP_NOFS);
 		unlock_extent_cached(&BTRFS_I(inode)->io_tree,
 				     start_pos, last_pos - 1, &cached_state,
 				     GFP_NOFS);

commit 903889f462409c816893abd02d88636f7b4a7774
Author: Miao Xie <miaox@cn.fujitsu.com>
Date:   Thu Sep 6 04:04:57 2012 -0600

    Btrfs: fix wrong size for the reservation when doing, file pre-allocation.
    
    When we ran fsstress(a program in xfstests), the filesystem hung up when it
    is full. It was because the space reserved in btrfs_fallocate() was wrong,
    btrfs_fallocate() just used the size of the pre-allocation to reserve the
    space, didn't took the block size aligning into account, so the size of
    the reserved space was less than the allocated space, it caused the over
    reserve problem and made the filesystem hung up when invoking cow_file_range().
    Fix it.
    
    Signed-off-by: Miao Xie <miaox@cn.fujitsu.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 793bc89c660f..00279c9a7f35 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -2000,7 +2000,7 @@ static long btrfs_fallocate(struct file *file, int mode,
 	 * Make sure we have enough space before we do the
 	 * allocation.
 	 */
-	ret = btrfs_check_data_free_space(inode, len);
+	ret = btrfs_check_data_free_space(inode, alloc_end - alloc_start + 1);
 	if (ret)
 		return ret;
 
@@ -2107,7 +2107,7 @@ static long btrfs_fallocate(struct file *file, int mode,
 out:
 	mutex_unlock(&inode->i_mutex);
 	/* Let go of our reservation. */
-	btrfs_free_reserved_data_space(inode, len);
+	btrfs_free_reserved_data_space(inode, alloc_end - alloc_start + 1);
 	return ret;
 }
 

commit 2ecb79239bcd04c9d410f4cdce16adb6840b19da
Author: Miao Xie <miaox@cn.fujitsu.com>
Date:   Thu Sep 6 04:04:27 2012 -0600

    Btrfs: fix unprotected ->log_batch
    
    We forget to protect ->log_batch when syncing a file, this patch fix
    this problem by atomic operation. And ->log_batch is used to check
    if there are parallel sync operations or not, so it is unnecessary to
    reset it to 0 after the sync operation of the current log tree complete.
    
    Signed-off-by: Miao Xie <miaox@cn.fujitsu.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index a9d7815cf58e..793bc89c660f 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1551,9 +1551,9 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 	 * ordered range does a filemape_write_and_wait_range which is why we
 	 * don't do it above like other file systems.
 	 */
-	root->log_batch++;
+	atomic_inc(&root->log_batch);
 	btrfs_wait_ordered_range(inode, start, end);
-	root->log_batch++;
+	atomic_inc(&root->log_batch);
 
 	/*
 	 * check the transaction that last modified this inode

commit 66d8f3dd1c87813d7f1cf8b774cb03e9b8d7e87e
Author: Miao Xie <miaox@cn.fujitsu.com>
Date:   Thu Sep 6 04:02:28 2012 -0600

    Btrfs: add a new "type" field into the block reservation structure
    
    Sometimes we need choose the method of the reservation according to the type
    of the block reservation, such as the reservation for the delayed inode update.
    Now we identify the type just by comparing the address of the reservation
    variants, it is very ugly if it is a temporary one because we need compare it
    with all the common reservation variants. So we add a new "type" field to keep
    the type the reservation variants.
    
    Signed-off-by: Miao Xie <miaox@cn.fujitsu.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index a50e98733e28..a9d7815cf58e 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1874,7 +1874,7 @@ static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 		goto out;
 	}
 
-	rsv = btrfs_alloc_block_rsv(root);
+	rsv = btrfs_alloc_block_rsv(root, BTRFS_BLOCK_RSV_TEMP);
 	if (!rsv) {
 		ret = -ENOMEM;
 		goto out_free;

commit 7014cdb49305eda0767d2ae6136f8c191ea8fd81
Author: Josef Bacik <jbacik@fusionio.com>
Date:   Thu Aug 30 20:06:49 2012 -0400

    Btrfs: btrfs_drop_extent_cache should never fail
    
    I noticed this when I was doing the fsync stuff, we allocate split extents if we
    drop an extent range that is in the middle of an existing extent.  This BUG()'s
    if we fail to allocate memory, but the fact is this is just a cache, we will
    just regenerate the cache if we need it, the important part is that we free the
    range we are given.  This can be done without allocations, so if we fail to
    allocate splits just skip the splitting stage and free our em and look for more
    extents to drop.  This also makes btrfs_drop_extent_cache a void since nobody
    was checking the return value anyway.  Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 57026a6e9c94..a50e98733e28 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -459,8 +459,8 @@ int btrfs_dirty_pages(struct btrfs_root *root, struct inode *inode,
  * this drops all the extents in the cache that intersect the range
  * [start, end].  Existing extents are split as required.
  */
-int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
-			       int skip_pinned)
+void btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
+			     int skip_pinned)
 {
 	struct extent_map *em;
 	struct extent_map *split = NULL;
@@ -479,11 +479,14 @@ int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
 		testend = 0;
 	}
 	while (1) {
+		int no_splits = 0;
+
 		if (!split)
 			split = alloc_extent_map();
 		if (!split2)
 			split2 = alloc_extent_map();
-		BUG_ON(!split || !split2); /* -ENOMEM */
+		if (!split || !split2)
+			no_splits = 1;
 
 		write_lock(&em_tree->lock);
 		em = lookup_extent_mapping(em_tree, start, len);
@@ -509,6 +512,8 @@ int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
 		compressed = test_bit(EXTENT_FLAG_COMPRESSED, &em->flags);
 		clear_bit(EXTENT_FLAG_PINNED, &em->flags);
 		remove_extent_mapping(em_tree, em);
+		if (no_splits)
+			goto next;
 
 		if (em->block_start < EXTENT_MAP_LAST_BYTE &&
 		    em->start < start) {
@@ -559,6 +564,7 @@ int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
 			free_extent_map(split);
 			split = NULL;
 		}
+next:
 		write_unlock(&em_tree->lock);
 
 		/* once for us */
@@ -570,7 +576,6 @@ int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
 		free_extent_map(split);
 	if (split2)
 		free_extent_map(split2);
-	return 0;
 }
 
 /*

commit 2aaa66558172b017f36bf38ae69372813dedee9d
Author: Josef Bacik <jbacik@fusionio.com>
Date:   Wed Aug 29 14:27:18 2012 -0400

    Btrfs: add hole punching
    
    This patch adds hole punching via fallocate.  Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 58598c249951..57026a6e9c94 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -39,6 +39,7 @@
 #include "tree-log.h"
 #include "locking.h"
 #include "compat.h"
+#include "volumes.h"
 
 /*
  * when auto defrag is enabled we
@@ -584,7 +585,7 @@ int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
 int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 			 struct btrfs_root *root, struct inode *inode,
 			 struct btrfs_path *path, u64 start, u64 end,
-			 int drop_cache)
+			 u64 *drop_end, int drop_cache)
 {
 	struct extent_buffer *leaf;
 	struct btrfs_file_extent_item *fi;
@@ -822,6 +823,8 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 			btrfs_abort_transaction(trans, root, ret);
 	}
 
+	if (drop_end)
+		*drop_end = min(end, extent_end);
 	btrfs_release_path(path);
 	return ret;
 }
@@ -836,7 +839,7 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 	path = btrfs_alloc_path();
 	if (!path)
 		return -ENOMEM;
-	ret = __btrfs_drop_extents(trans, root, inode, path, start, end,
+	ret = __btrfs_drop_extents(trans, root, inode, path, start, end, NULL,
 				   drop_cache);
 	btrfs_free_path(path);
 	return ret;
@@ -1645,6 +1648,324 @@ static int btrfs_file_mmap(struct file	*filp, struct vm_area_struct *vma)
 	return 0;
 }
 
+static int hole_mergeable(struct inode *inode, struct extent_buffer *leaf,
+			  int slot, u64 start, u64 end)
+{
+	struct btrfs_file_extent_item *fi;
+	struct btrfs_key key;
+
+	if (slot < 0 || slot >= btrfs_header_nritems(leaf))
+		return 0;
+
+	btrfs_item_key_to_cpu(leaf, &key, slot);
+	if (key.objectid != btrfs_ino(inode) ||
+	    key.type != BTRFS_EXTENT_DATA_KEY)
+		return 0;
+
+	fi = btrfs_item_ptr(leaf, slot, struct btrfs_file_extent_item);
+
+	if (btrfs_file_extent_type(leaf, fi) != BTRFS_FILE_EXTENT_REG)
+		return 0;
+
+	if (btrfs_file_extent_disk_bytenr(leaf, fi))
+		return 0;
+
+	if (key.offset == end)
+		return 1;
+	if (key.offset + btrfs_file_extent_num_bytes(leaf, fi) == start)
+		return 1;
+	return 0;
+}
+
+static int fill_holes(struct btrfs_trans_handle *trans, struct inode *inode,
+		      struct btrfs_path *path, u64 offset, u64 end)
+{
+	struct btrfs_root *root = BTRFS_I(inode)->root;
+	struct extent_buffer *leaf;
+	struct btrfs_file_extent_item *fi;
+	struct extent_map *hole_em;
+	struct extent_map_tree *em_tree = &BTRFS_I(inode)->extent_tree;
+	struct btrfs_key key;
+	int ret;
+
+	key.objectid = btrfs_ino(inode);
+	key.type = BTRFS_EXTENT_DATA_KEY;
+	key.offset = offset;
+
+
+	ret = btrfs_search_slot(trans, root, &key, path, 0, 1);
+	if (ret < 0)
+		return ret;
+	BUG_ON(!ret);
+
+	leaf = path->nodes[0];
+	if (hole_mergeable(inode, leaf, path->slots[0]-1, offset, end)) {
+		u64 num_bytes;
+
+		path->slots[0]--;
+		fi = btrfs_item_ptr(leaf, path->slots[0],
+				    struct btrfs_file_extent_item);
+		num_bytes = btrfs_file_extent_num_bytes(leaf, fi) +
+			end - offset;
+		btrfs_set_file_extent_num_bytes(leaf, fi, num_bytes);
+		btrfs_set_file_extent_ram_bytes(leaf, fi, num_bytes);
+		btrfs_set_file_extent_offset(leaf, fi, 0);
+		btrfs_mark_buffer_dirty(leaf);
+		goto out;
+	}
+
+	if (hole_mergeable(inode, leaf, path->slots[0]+1, offset, end)) {
+		u64 num_bytes;
+
+		path->slots[0]++;
+		key.offset = offset;
+		btrfs_set_item_key_safe(trans, root, path, &key);
+		fi = btrfs_item_ptr(leaf, path->slots[0],
+				    struct btrfs_file_extent_item);
+		num_bytes = btrfs_file_extent_num_bytes(leaf, fi) + end -
+			offset;
+		btrfs_set_file_extent_num_bytes(leaf, fi, num_bytes);
+		btrfs_set_file_extent_ram_bytes(leaf, fi, num_bytes);
+		btrfs_set_file_extent_offset(leaf, fi, 0);
+		btrfs_mark_buffer_dirty(leaf);
+		goto out;
+	}
+	btrfs_release_path(path);
+
+	ret = btrfs_insert_file_extent(trans, root, btrfs_ino(inode), offset,
+				       0, 0, end - offset, 0, end - offset,
+				       0, 0, 0);
+	if (ret)
+		return ret;
+
+out:
+	btrfs_release_path(path);
+
+	hole_em = alloc_extent_map();
+	if (!hole_em) {
+		btrfs_drop_extent_cache(inode, offset, end - 1, 0);
+		set_bit(BTRFS_INODE_NEEDS_FULL_SYNC,
+			&BTRFS_I(inode)->runtime_flags);
+	} else {
+		hole_em->start = offset;
+		hole_em->len = end - offset;
+		hole_em->orig_start = offset;
+
+		hole_em->block_start = EXTENT_MAP_HOLE;
+		hole_em->block_len = 0;
+		hole_em->bdev = root->fs_info->fs_devices->latest_bdev;
+		hole_em->compress_type = BTRFS_COMPRESS_NONE;
+		hole_em->generation = trans->transid;
+
+		do {
+			btrfs_drop_extent_cache(inode, offset, end - 1, 0);
+			write_lock(&em_tree->lock);
+			ret = add_extent_mapping(em_tree, hole_em);
+			if (!ret)
+				list_move(&hole_em->list,
+					  &em_tree->modified_extents);
+			write_unlock(&em_tree->lock);
+		} while (ret == -EEXIST);
+		free_extent_map(hole_em);
+		if (ret)
+			set_bit(BTRFS_INODE_NEEDS_FULL_SYNC,
+				&BTRFS_I(inode)->runtime_flags);
+	}
+
+	return 0;
+}
+
+static int btrfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
+{
+	struct btrfs_root *root = BTRFS_I(inode)->root;
+	struct extent_state *cached_state = NULL;
+	struct btrfs_path *path;
+	struct btrfs_block_rsv *rsv;
+	struct btrfs_trans_handle *trans;
+	u64 mask = BTRFS_I(inode)->root->sectorsize - 1;
+	u64 lockstart = (offset + mask) & ~mask;
+	u64 lockend = ((offset + len) & ~mask) - 1;
+	u64 cur_offset = lockstart;
+	u64 min_size = btrfs_calc_trunc_metadata_size(root, 1);
+	u64 drop_end;
+	unsigned long nr;
+	int ret = 0;
+	int err = 0;
+	bool same_page = (offset >> PAGE_CACHE_SHIFT) ==
+		((offset + len) >> PAGE_CACHE_SHIFT);
+
+	btrfs_wait_ordered_range(inode, offset, len);
+
+	mutex_lock(&inode->i_mutex);
+	if (offset >= inode->i_size) {
+		mutex_unlock(&inode->i_mutex);
+		return 0;
+	}
+
+	/*
+	 * Only do this if we are in the same page and we aren't doing the
+	 * entire page.
+	 */
+	if (same_page && len < PAGE_CACHE_SIZE) {
+		ret = btrfs_truncate_page(inode, offset, len, 0);
+		mutex_unlock(&inode->i_mutex);
+		return ret;
+	}
+
+	/* zero back part of the first page */
+	ret = btrfs_truncate_page(inode, offset, 0, 0);
+	if (ret) {
+		mutex_unlock(&inode->i_mutex);
+		return ret;
+	}
+
+	/* zero the front end of the last page */
+	ret = btrfs_truncate_page(inode, offset + len, 0, 1);
+	if (ret) {
+		mutex_unlock(&inode->i_mutex);
+		return ret;
+	}
+
+	if (lockend < lockstart) {
+		mutex_unlock(&inode->i_mutex);
+		return 0;
+	}
+
+	while (1) {
+		struct btrfs_ordered_extent *ordered;
+
+		truncate_pagecache_range(inode, lockstart, lockend);
+
+		lock_extent_bits(&BTRFS_I(inode)->io_tree, lockstart, lockend,
+				 0, &cached_state);
+		ordered = btrfs_lookup_first_ordered_extent(inode, lockend);
+
+		/*
+		 * We need to make sure we have no ordered extents in this range
+		 * and nobody raced in and read a page in this range, if we did
+		 * we need to try again.
+		 */
+		if ((!ordered ||
+		    (ordered->file_offset + ordered->len < lockstart ||
+		     ordered->file_offset > lockend)) &&
+		     !test_range_bit(&BTRFS_I(inode)->io_tree, lockstart,
+				     lockend, EXTENT_UPTODATE, 0,
+				     cached_state)) {
+			if (ordered)
+				btrfs_put_ordered_extent(ordered);
+			break;
+		}
+		if (ordered)
+			btrfs_put_ordered_extent(ordered);
+		unlock_extent_cached(&BTRFS_I(inode)->io_tree, lockstart,
+				     lockend, &cached_state, GFP_NOFS);
+		btrfs_wait_ordered_range(inode, lockstart,
+					 lockend - lockstart + 1);
+	}
+
+	path = btrfs_alloc_path();
+	if (!path) {
+		ret = -ENOMEM;
+		goto out;
+	}
+
+	rsv = btrfs_alloc_block_rsv(root);
+	if (!rsv) {
+		ret = -ENOMEM;
+		goto out_free;
+	}
+	rsv->size = btrfs_calc_trunc_metadata_size(root, 1);
+	rsv->failfast = 1;
+
+	/*
+	 * 1 - update the inode
+	 * 1 - removing the extents in the range
+	 * 1 - adding the hole extent
+	 */
+	trans = btrfs_start_transaction(root, 3);
+	if (IS_ERR(trans)) {
+		err = PTR_ERR(trans);
+		goto out_free;
+	}
+
+	ret = btrfs_block_rsv_migrate(&root->fs_info->trans_block_rsv, rsv,
+				      min_size);
+	BUG_ON(ret);
+	trans->block_rsv = rsv;
+
+	while (cur_offset < lockend) {
+		ret = __btrfs_drop_extents(trans, root, inode, path,
+					   cur_offset, lockend + 1,
+					   &drop_end, 1);
+		if (ret != -ENOSPC)
+			break;
+
+		trans->block_rsv = &root->fs_info->trans_block_rsv;
+
+		ret = fill_holes(trans, inode, path, cur_offset, drop_end);
+		if (ret) {
+			err = ret;
+			break;
+		}
+
+		cur_offset = drop_end;
+
+		ret = btrfs_update_inode(trans, root, inode);
+		if (ret) {
+			err = ret;
+			break;
+		}
+
+		nr = trans->blocks_used;
+		btrfs_end_transaction(trans, root);
+		btrfs_btree_balance_dirty(root, nr);
+
+		trans = btrfs_start_transaction(root, 3);
+		if (IS_ERR(trans)) {
+			ret = PTR_ERR(trans);
+			trans = NULL;
+			break;
+		}
+
+		ret = btrfs_block_rsv_migrate(&root->fs_info->trans_block_rsv,
+					      rsv, min_size);
+		BUG_ON(ret);	/* shouldn't happen */
+		trans->block_rsv = rsv;
+	}
+
+	if (ret) {
+		err = ret;
+		goto out_trans;
+	}
+
+	trans->block_rsv = &root->fs_info->trans_block_rsv;
+	ret = fill_holes(trans, inode, path, cur_offset, drop_end);
+	if (ret) {
+		err = ret;
+		goto out_trans;
+	}
+
+out_trans:
+	if (!trans)
+		goto out_free;
+
+	trans->block_rsv = &root->fs_info->trans_block_rsv;
+	ret = btrfs_update_inode(trans, root, inode);
+	nr = trans->blocks_used;
+	btrfs_end_transaction(trans, root);
+	btrfs_btree_balance_dirty(root, nr);
+out_free:
+	btrfs_free_path(path);
+	btrfs_free_block_rsv(root, rsv);
+out:
+	unlock_extent_cached(&BTRFS_I(inode)->io_tree, lockstart, lockend,
+			     &cached_state, GFP_NOFS);
+	mutex_unlock(&inode->i_mutex);
+	if (ret && !err)
+		err = ret;
+	return err;
+}
+
 static long btrfs_fallocate(struct file *file, int mode,
 			    loff_t offset, loff_t len)
 {
@@ -1663,10 +1984,13 @@ static long btrfs_fallocate(struct file *file, int mode,
 	alloc_start = offset & ~mask;
 	alloc_end =  (offset + len + mask) & ~mask;
 
-	/* We only support the FALLOC_FL_KEEP_SIZE mode */
-	if (mode & ~FALLOC_FL_KEEP_SIZE)
+	/* Make sure we aren't being give some crap mode */
+	if (mode & ~(FALLOC_FL_KEEP_SIZE | FALLOC_FL_PUNCH_HOLE))
 		return -EOPNOTSUPP;
 
+	if (mode & FALLOC_FL_PUNCH_HOLE)
+		return btrfs_punch_hole(inode, offset, len);
+
 	/*
 	 * Make sure we have enough space before we do the
 	 * allocation.

commit 2671485d395c07fca104c972785898d7c52fc942
Author: Josef Bacik <jbacik@fusionio.com>
Date:   Wed Aug 29 12:24:27 2012 -0400

    Btrfs: remove unused hint byte argument for btrfs_drop_extents
    
    I audited all users of btrfs_drop_extents and found that nobody actually uses
    the hint_byte argument.  I'm sure it was used for something at some point but
    it's not used now, and the way the pinning works the disk bytenr would never be
    immediately useful anyway so lets just remove it.  Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 399f9d71a926..58598c249951 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -584,7 +584,7 @@ int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
 int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 			 struct btrfs_root *root, struct inode *inode,
 			 struct btrfs_path *path, u64 start, u64 end,
-			 u64 *hint_byte, int drop_cache)
+			 int drop_cache)
 {
 	struct extent_buffer *leaf;
 	struct btrfs_file_extent_item *fi;
@@ -716,7 +716,6 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 						new_key.objectid,
 						start - extent_offset, 0);
 				BUG_ON(ret); /* -ENOMEM */
-				*hint_byte = disk_bytenr;
 			}
 			key.offset = start;
 		}
@@ -736,10 +735,8 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 			btrfs_set_file_extent_num_bytes(leaf, fi,
 							extent_end - end);
 			btrfs_mark_buffer_dirty(leaf);
-			if (update_refs && disk_bytenr > 0) {
+			if (update_refs && disk_bytenr > 0)
 				inode_sub_bytes(inode, end - key.offset);
-				*hint_byte = disk_bytenr;
-			}
 			break;
 		}
 
@@ -755,10 +752,8 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 			btrfs_set_file_extent_num_bytes(leaf, fi,
 							start - key.offset);
 			btrfs_mark_buffer_dirty(leaf);
-			if (update_refs && disk_bytenr > 0) {
+			if (update_refs && disk_bytenr > 0)
 				inode_sub_bytes(inode, extent_end - start);
-				*hint_byte = disk_bytenr;
-			}
 			if (end == extent_end)
 				break;
 
@@ -794,7 +789,6 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 				BUG_ON(ret); /* -ENOMEM */
 				inode_sub_bytes(inode,
 						extent_end - key.offset);
-				*hint_byte = disk_bytenr;
 			}
 
 			if (end == extent_end)
@@ -834,7 +828,7 @@ int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
 
 int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 		       struct btrfs_root *root, struct inode *inode, u64 start,
-		       u64 end, u64 *hint_byte, int drop_cache)
+		       u64 end, int drop_cache)
 {
 	struct btrfs_path *path;
 	int ret;
@@ -843,7 +837,7 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 	if (!path)
 		return -ENOMEM;
 	ret = __btrfs_drop_extents(trans, root, inode, path, start, end,
-				   hint_byte, drop_cache);
+				   drop_cache);
 	btrfs_free_path(path);
 	return ret;
 }

commit 5dc562c541e1026df9d43913c2f6b91156e22d32
Author: Josef Bacik <jbacik@fusionio.com>
Date:   Fri Aug 17 13:14:17 2012 -0400

    Btrfs: turbo charge fsync
    
    At least for the vm workload.  Currently on fsync we will
    
    1) Truncate all items in the log tree for the given inode if they exist
    
    and
    
    2) Copy all items for a given inode into the log
    
    The problem with this is that for things like VMs you can have lots of
    extents from the fragmented writing behavior, and worst yet you may have
    only modified a few extents, not the entire thing.  This patch fixes this
    problem by tracking which transid modified our extent, and then when we do
    the tree logging we find all of the extents we've modified in our current
    transaction, sort them and commit them.  We also only truncate up to the
    xattrs of the inode and copy that stuff in normally, and then just drop any
    extents in the range we have that exist in the log already.  Here are some
    numbers of a 50 meg fio job that does random writes and fsync()s after every
    write
    
                    Original        Patched
    SATA drive      82KB/s          140KB/s
    Fusion drive    431KB/s         2532KB/s
    
    So around 2-6 times faster depending on your hardware.  There are a few
    corner cases, for example if you truncate at all we have to do it the old
    way since there is no way to be sure what is in the log is ok.  This
    probably could be done smarter, but if you write-fsync-truncate-write-fsync
    you deserve what you get.  All this work is in RAM of course so if your
    inode gets evicted from cache and you read it in and fsync it we'll do it
    the slow way if we are still in the same transaction that we last modified
    the inode in.
    
    The biggest cool part of this is that it requires no changes to the recovery
    code, so if you fsync with this patch and crash and load an old kernel, it
    will run the recovery and be a-ok.  I have tested this pretty thoroughly
    with an fsync tester and everything comes back fine, as well as xfstests.
    Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index b7c885c8423f..399f9d71a926 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -459,13 +459,14 @@ int btrfs_dirty_pages(struct btrfs_root *root, struct inode *inode,
  * [start, end].  Existing extents are split as required.
  */
 int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
-			    int skip_pinned)
+			       int skip_pinned)
 {
 	struct extent_map *em;
 	struct extent_map *split = NULL;
 	struct extent_map *split2 = NULL;
 	struct extent_map_tree *em_tree = &BTRFS_I(inode)->extent_tree;
 	u64 len = end - start + 1;
+	u64 gen;
 	int ret;
 	int testend = 1;
 	unsigned long flags;
@@ -490,6 +491,7 @@ int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
 			break;
 		}
 		flags = em->flags;
+		gen = em->generation;
 		if (skip_pinned && test_bit(EXTENT_FLAG_PINNED, &em->flags)) {
 			if (testend && em->start + em->len >= start + len) {
 				free_extent_map(em);
@@ -518,12 +520,13 @@ int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
 				split->block_len = em->block_len;
 			else
 				split->block_len = split->len;
-
+			split->generation = gen;
 			split->bdev = em->bdev;
 			split->flags = flags;
 			split->compress_type = em->compress_type;
 			ret = add_extent_mapping(em_tree, split);
 			BUG_ON(ret); /* Logic error */
+			list_move(&split->list, &em_tree->modified_extents);
 			free_extent_map(split);
 			split = split2;
 			split2 = NULL;
@@ -537,6 +540,7 @@ int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
 			split->bdev = em->bdev;
 			split->flags = flags;
 			split->compress_type = em->compress_type;
+			split->generation = gen;
 
 			if (compressed) {
 				split->block_len = em->block_len;
@@ -550,6 +554,7 @@ int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
 
 			ret = add_extent_mapping(em_tree, split);
 			BUG_ON(ret); /* Logic error */
+			list_move(&split->list, &em_tree->modified_extents);
 			free_extent_map(split);
 			split = NULL;
 		}
@@ -576,13 +581,13 @@ int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
  * it is either truncated or split.  Anything entirely inside the range
  * is deleted from the tree.
  */
-int btrfs_drop_extents(struct btrfs_trans_handle *trans, struct inode *inode,
-		       u64 start, u64 end, u64 *hint_byte, int drop_cache)
+int __btrfs_drop_extents(struct btrfs_trans_handle *trans,
+			 struct btrfs_root *root, struct inode *inode,
+			 struct btrfs_path *path, u64 start, u64 end,
+			 u64 *hint_byte, int drop_cache)
 {
-	struct btrfs_root *root = BTRFS_I(inode)->root;
 	struct extent_buffer *leaf;
 	struct btrfs_file_extent_item *fi;
-	struct btrfs_path *path;
 	struct btrfs_key key;
 	struct btrfs_key new_key;
 	u64 ino = btrfs_ino(inode);
@@ -597,14 +602,11 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans, struct inode *inode,
 	int recow;
 	int ret;
 	int modify_tree = -1;
+	int update_refs = (root->ref_cows || root == root->fs_info->tree_root);
 
 	if (drop_cache)
 		btrfs_drop_extent_cache(inode, start, end - 1, 0);
 
-	path = btrfs_alloc_path();
-	if (!path)
-		return -ENOMEM;
-
 	if (start >= BTRFS_I(inode)->disk_i_size)
 		modify_tree = 0;
 
@@ -707,7 +709,7 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans, struct inode *inode,
 							extent_end - start);
 			btrfs_mark_buffer_dirty(leaf);
 
-			if (disk_bytenr > 0) {
+			if (update_refs && disk_bytenr > 0) {
 				ret = btrfs_inc_extent_ref(trans, root,
 						disk_bytenr, num_bytes, 0,
 						root->root_key.objectid,
@@ -734,7 +736,7 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans, struct inode *inode,
 			btrfs_set_file_extent_num_bytes(leaf, fi,
 							extent_end - end);
 			btrfs_mark_buffer_dirty(leaf);
-			if (disk_bytenr > 0) {
+			if (update_refs && disk_bytenr > 0) {
 				inode_sub_bytes(inode, end - key.offset);
 				*hint_byte = disk_bytenr;
 			}
@@ -753,7 +755,7 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans, struct inode *inode,
 			btrfs_set_file_extent_num_bytes(leaf, fi,
 							start - key.offset);
 			btrfs_mark_buffer_dirty(leaf);
-			if (disk_bytenr > 0) {
+			if (update_refs && disk_bytenr > 0) {
 				inode_sub_bytes(inode, extent_end - start);
 				*hint_byte = disk_bytenr;
 			}
@@ -777,12 +779,13 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans, struct inode *inode,
 				del_nr++;
 			}
 
-			if (extent_type == BTRFS_FILE_EXTENT_INLINE) {
+			if (update_refs &&
+			    extent_type == BTRFS_FILE_EXTENT_INLINE) {
 				inode_sub_bytes(inode,
 						extent_end - key.offset);
 				extent_end = ALIGN(extent_end,
 						   root->sectorsize);
-			} else if (disk_bytenr > 0) {
+			} else if (update_refs && disk_bytenr > 0) {
 				ret = btrfs_free_extent(trans, root,
 						disk_bytenr, num_bytes, 0,
 						root->root_key.objectid,
@@ -806,7 +809,7 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans, struct inode *inode,
 					      del_nr);
 			if (ret) {
 				btrfs_abort_transaction(trans, root, ret);
-				goto out;
+				break;
 			}
 
 			del_nr = 0;
@@ -825,7 +828,22 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans, struct inode *inode,
 			btrfs_abort_transaction(trans, root, ret);
 	}
 
-out:
+	btrfs_release_path(path);
+	return ret;
+}
+
+int btrfs_drop_extents(struct btrfs_trans_handle *trans,
+		       struct btrfs_root *root, struct inode *inode, u64 start,
+		       u64 end, u64 *hint_byte, int drop_cache)
+{
+	struct btrfs_path *path;
+	int ret;
+
+	path = btrfs_alloc_path();
+	if (!path)
+		return -ENOMEM;
+	ret = __btrfs_drop_extents(trans, root, inode, path, start, end,
+				   hint_byte, drop_cache);
 	btrfs_free_path(path);
 	return ret;
 }
@@ -892,8 +910,6 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 	int ret;
 	u64 ino = btrfs_ino(inode);
 
-	btrfs_drop_extent_cache(inode, start, end - 1, 0);
-
 	path = btrfs_alloc_path();
 	if (!path)
 		return -ENOMEM;
@@ -1556,6 +1572,14 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 	    BTRFS_I(inode)->last_trans <=
 	    root->fs_info->last_trans_committed) {
 		BTRFS_I(inode)->last_trans = 0;
+
+		/*
+		 * We'v had everything committed since the last time we were
+		 * modified so clear this flag in case it was set for whatever
+		 * reason, it's no longer relevant.
+		 */
+		clear_bit(BTRFS_INODE_NEEDS_FULL_SYNC,
+			  &BTRFS_I(inode)->runtime_flags);
 		mutex_unlock(&inode->i_mutex);
 		goto out;
 	}

commit 224ecce517af3a952321202cdf304c12e138caca
Author: Josef Bacik <jbacik@fusionio.com>
Date:   Thu Aug 16 16:32:06 2012 -0400

    Btrfs: fix possible corruption when fsyncing written prealloced extents
    
    While working on my fsync patch my fsync tester kept hitting mismatching
    md5sums when I would randomly write to a prealloc'ed region, syncfs() and
    then write to the prealloced region some more and then fsync() and then
    immediately reboot.  This is because the tree logging code will skip writing
    csums for file extents who's generation is less than the current running
    transaction.  When we mark extents as written we haven't been updating their
    generation so they were always being skipped.  This wouldn't happen if you
    were to preallocate and then write in the same transaction, but if you for
    example prealloced a VM you could definitely run into this problem.  This
    patch makes my fsync tester happy again.  Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 5caf285c6e4d..b7c885c8423f 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -935,12 +935,16 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 			btrfs_set_item_key_safe(trans, root, path, &new_key);
 			fi = btrfs_item_ptr(leaf, path->slots[0],
 					    struct btrfs_file_extent_item);
+			btrfs_set_file_extent_generation(leaf, fi,
+							 trans->transid);
 			btrfs_set_file_extent_num_bytes(leaf, fi,
 							extent_end - end);
 			btrfs_set_file_extent_offset(leaf, fi,
 						     end - orig_offset);
 			fi = btrfs_item_ptr(leaf, path->slots[0] - 1,
 					    struct btrfs_file_extent_item);
+			btrfs_set_file_extent_generation(leaf, fi,
+							 trans->transid);
 			btrfs_set_file_extent_num_bytes(leaf, fi,
 							end - other_start);
 			btrfs_mark_buffer_dirty(leaf);
@@ -958,12 +962,16 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 					    struct btrfs_file_extent_item);
 			btrfs_set_file_extent_num_bytes(leaf, fi,
 							start - key.offset);
+			btrfs_set_file_extent_generation(leaf, fi,
+							 trans->transid);
 			path->slots[0]++;
 			new_key.offset = start;
 			btrfs_set_item_key_safe(trans, root, path, &new_key);
 
 			fi = btrfs_item_ptr(leaf, path->slots[0],
 					    struct btrfs_file_extent_item);
+			btrfs_set_file_extent_generation(leaf, fi,
+							 trans->transid);
 			btrfs_set_file_extent_num_bytes(leaf, fi,
 							other_end - start);
 			btrfs_set_file_extent_offset(leaf, fi,
@@ -991,12 +999,14 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 		leaf = path->nodes[0];
 		fi = btrfs_item_ptr(leaf, path->slots[0] - 1,
 				    struct btrfs_file_extent_item);
+		btrfs_set_file_extent_generation(leaf, fi, trans->transid);
 		btrfs_set_file_extent_num_bytes(leaf, fi,
 						split - key.offset);
 
 		fi = btrfs_item_ptr(leaf, path->slots[0],
 				    struct btrfs_file_extent_item);
 
+		btrfs_set_file_extent_generation(leaf, fi, trans->transid);
 		btrfs_set_file_extent_offset(leaf, fi, split - orig_offset);
 		btrfs_set_file_extent_num_bytes(leaf, fi,
 						extent_end - split);
@@ -1056,12 +1066,14 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 			   struct btrfs_file_extent_item);
 		btrfs_set_file_extent_type(leaf, fi,
 					   BTRFS_FILE_EXTENT_REG);
+		btrfs_set_file_extent_generation(leaf, fi, trans->transid);
 		btrfs_mark_buffer_dirty(leaf);
 	} else {
 		fi = btrfs_item_ptr(leaf, del_slot - 1,
 			   struct btrfs_file_extent_item);
 		btrfs_set_file_extent_type(leaf, fi,
 					   BTRFS_FILE_EXTENT_REG);
+		btrfs_set_file_extent_generation(leaf, fi, trans->transid);
 		btrfs_set_file_extent_num_bytes(leaf, fi,
 						extent_end - key.offset);
 		btrfs_mark_buffer_dirty(leaf);

commit b2b5ef5c8e89f19b68c174bf246f3ca212dbf0bc
Author: Jan Kara <jack@suse.cz>
Date:   Tue Jun 12 16:20:45 2012 +0200

    btrfs: Convert to new freezing mechanism
    
    We convert btrfs_file_aio_write() to use new freeze check.  We also add proper
    freeze protection to btrfs_page_mkwrite(). We also add freeze protection to
    the transaction mechanism to avoid starting transactions on frozen filesystem.
    At minimum this is necessary to stop iput() of unlinked file to change frozen
    filesystem during truncation.
    
    Checks in cleaner_kthread() and transaction_kthread() can be safely removed
    since btrfs_freeze() will lock the mutexes and thus block the threads (and they
    shouldn't have anything to do anyway).
    
    CC: linux-btrfs@vger.kernel.org
    CC: Chris Mason <chris.mason@oracle.com>
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 9aa01ec2138d..5caf285c6e4d 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1379,7 +1379,7 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 	ssize_t err = 0;
 	size_t count, ocount;
 
-	vfs_check_frozen(inode->i_sb, SB_FREEZE_WRITE);
+	sb_start_write(inode->i_sb);
 
 	mutex_lock(&inode->i_mutex);
 
@@ -1469,6 +1469,7 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 			num_written = err;
 	}
 out:
+	sb_end_write(inode->i_sb);
 	current->backing_dev_info = NULL;
 	return num_written ? num_written : err;
 }

commit 5eecb9cc9029aef3c308fc94149c4a3065d40d9a
Merge: 62ad64498a2e b6305567e7d3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jul 5 13:06:25 2012 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs
    
    Pull btrfs updates from Chris Mason:
     "I held off on my rc5 pull because I hit an oops during log recovery
      after a crash.  I wanted to make sure it wasn't a regression because
      we have some logging fixes in here.
    
      It turns out that a commit during the merge window just made it much
      more likely to trigger directory logging instead of full commits,
      which exposed an old bug.
    
      The new backref walking code got some additional fixes.  This should
      be the final set of them.
    
      Josef fixed up a corner where our O_DIRECT writes and buffered reads
      could expose old file contents (not stale, just not the most recent).
      He and Liu Bo fixed crashes during tree log recover as well.
    
      Ilya fixed errors while we resume disk balancing operations on
      readonly mounts."
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs:
      Btrfs: run delayed directory updates during log replay
      Btrfs: hold a ref on the inode during writepages
      Btrfs: fix tree log remove space corner case
      Btrfs: fix wrong check during log recovery
      Btrfs: use _IOR for BTRFS_IOC_SUBVOL_GETFLAGS
      Btrfs: resume balance on rw (re)mounts properly
      Btrfs: restore restriper state on all mounts
      Btrfs: fix dio write vs buffered read race
      Btrfs: don't count I/O statistic read errors for missing devices
      Btrfs: resolve tree mod log locking issue in btrfs_next_leaf
      Btrfs: fix tree mod log rewind of ADD operations
      Btrfs: leave critical region in btrfs_find_all_roots as soon as possible
      Btrfs: always put insert_ptr modifications into the tree mod log
      Btrfs: fix tree mod log for root replacements at leaf level
      Btrfs: support root level changes in __resolve_indirect_ref
      Btrfs: avoid waiting for delayed refs when we must not

commit c3473e830074ef04f974f2829690942dd8580619
Author: Josef Bacik <josef@redhat.com>
Date:   Tue Jun 19 10:59:00 2012 -0400

    Btrfs: fix dio write vs buffered read race
    
    Miao pointed out there's a problem with mixing dio writes and buffered
    reads.  If the read happens between us invalidating the page range and
    actually locking the extent we can bring in pages into page cache.  Then
    once the write finishes if somebody tries to read again it will just find
    uptodate pages and we'll read stale data.  So we need to lock the extent and
    check for uptodate bits in the range.  If there are uptodate bits we need to
    unlock and invalidate again.  This will keep this race from happening since
    we will hold the extent locked until we create the ordered extent, and then
    teh read side always waits for ordered extents.  There was also a race in
    how we updated i_size, previously we were relying on the generic DIO stuff
    to adjust the i_size after the DIO had completed, but this happens outside
    of the extent lock which means reads could come in and not see the updated
    i_size.  So instead move this work into where we create the extents, and
    then this way the update ordered i_size stuff works properly in the endio
    handlers.  Thanks,
    
    Signed-off-by: Josef Bacik <josef@redhat.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 876cddd6b2f0..248d20265249 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1334,7 +1334,6 @@ static ssize_t __btrfs_direct_write(struct kiocb *iocb,
 				    loff_t *ppos, size_t count, size_t ocount)
 {
 	struct file *file = iocb->ki_filp;
-	struct inode *inode = fdentry(file)->d_inode;
 	struct iov_iter i;
 	ssize_t written;
 	ssize_t written_buffered;
@@ -1344,18 +1343,6 @@ static ssize_t __btrfs_direct_write(struct kiocb *iocb,
 	written = generic_file_direct_write(iocb, iov, &nr_segs, pos, ppos,
 					    count, ocount);
 
-	/*
-	 * the generic O_DIRECT will update in-memory i_size after the
-	 * DIOs are done.  But our endio handlers that update the on
-	 * disk i_size never update past the in memory i_size.  So we
-	 * need one more update here to catch any additions to the
-	 * file
-	 */
-	if (inode->i_size != BTRFS_I(inode)->disk_i_size) {
-		btrfs_ordered_update_i_size(inode, inode->i_size, NULL);
-		mark_inode_dirty(inode);
-	}
-
 	if (written < 0 || written == count)
 		return written;
 

commit 1193755ac6328ad240ba987e6ec41d5e8baf0680
Merge: 4edebed86690 0ef97dcfce41
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jun 1 10:34:35 2012 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs
    
    Pull vfs changes from Al Viro.
     "A lot of misc stuff.  The obvious groups:
       * Miklos' atomic_open series; kills the damn abuse of
         ->d_revalidate() by NFS, which was the major stumbling block for
         all work in that area.
       * ripping security_file_mmap() and dealing with deadlocks in the
         area; sanitizing the neighborhood of vm_mmap()/vm_munmap() in
         general.
       * ->encode_fh() switched to saner API; insane fake dentry in
         mm/cleancache.c gone.
       * assorted annotations in fs (endianness, __user)
       * parts of Artem's ->s_dirty work (jff2 and reiserfs parts)
       * ->update_time() work from Josef.
       * other bits and pieces all over the place.
    
      Normally it would've been in two or three pull requests, but
      signal.git stuff had eaten a lot of time during this cycle ;-/"
    
    Fix up trivial conflicts in Documentation/filesystems/vfs.txt (the
    'truncate_range' inode method was removed by the VM changes, the VFS
    update adds an 'update_time()' method), and in fs/btrfs/ulist.[ch] (due
    to sparse fix added twice, with other changes nearby).
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs: (95 commits)
      nfs: don't open in ->d_revalidate
      vfs: retry last component if opening stale dentry
      vfs: nameidata_to_filp(): don't throw away file on error
      vfs: nameidata_to_filp(): inline __dentry_open()
      vfs: do_dentry_open(): don't put filp
      vfs: split __dentry_open()
      vfs: do_last() common post lookup
      vfs: do_last(): add audit_inode before open
      vfs: do_last(): only return EISDIR for O_CREAT
      vfs: do_last(): check LOOKUP_DIRECTORY
      vfs: do_last(): make ENOENT exit RCU safe
      vfs: make follow_link check RCU safe
      vfs: do_last(): use inode variable
      vfs: do_last(): inline walk_component()
      vfs: do_last(): make exit RCU safe
      vfs: split do_lookup()
      Btrfs: move over to use ->update_time
      fs: introduce inode operation ->update_time
      reiserfs: get rid of resierfs_sync_super
      reiserfs: mark the superblock as dirty a bit later
      ...

commit e41f941a23115e84a8550b3d901a13a14b2edc2f
Author: Josef Bacik <josef@redhat.com>
Date:   Mon Mar 26 09:46:47 2012 -0400

    Btrfs: move over to use ->update_time
    
    Btrfs had been doing it's own file_update_time so we could catch ENOSPC
    properly, so just update our btrfs_update_time to work with the new stuff and
    then we'll be fancy later.  Thanks,
    
    Signed-off-by: Josef Bacik <josef@redhat.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 53bf2d764bbc..974beb84ed61 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1404,7 +1404,7 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 		goto out;
 	}
 
-	err = btrfs_update_time(file);
+	err = file_update_time(file);
 	if (err) {
 		mutex_unlock(&inode->i_mutex);
 		goto out;

commit 22ee6985de7d3e81ec0cef9c6ba01b45ad1bafeb
Author: Josef Bacik <josef@redhat.com>
Date:   Tue May 29 16:57:49 2012 -0400

    Btrfs: check to see if the inode is in the log before fsyncing
    
    We have this check down in the actual logging code, but this is after we
    start a transaction and all that good stuff.  So move the helper
    inode_in_log() out so we can call it in fsync() and avoid starting a
    transaction altogether and just exit if we've already fsync()'ed this file
    recently.  You would notice this issue if you fsync()'ed a file over and
    over again until the transaction committed.  Thanks,
    
    Signed-off-by: Josef Bacik <josef@redhat.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 2e63cdc2b093..876cddd6b2f0 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1552,7 +1552,8 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 	 * syncing
 	 */
 	smp_mb();
-	if (BTRFS_I(inode)->last_trans <=
+	if (btrfs_inode_in_log(inode, root->fs_info->generation) ||
+	    BTRFS_I(inode)->last_trans <=
 	    root->fs_info->last_trans_committed) {
 		BTRFS_I(inode)->last_trans = 0;
 		mutex_unlock(&inode->i_mutex);

commit 762f2263260d576504aeb23d20f90120acdb025f
Author: Miao Xie <miaox@cn.fujitsu.com>
Date:   Thu May 24 18:58:27 2012 +0800

    Btrfs: fix the same inode id problem when doing auto defragment
    
    Two files in the different subvolumes may have the same inode id, so
    The rb-tree which is used to manage the defragment object must take it
    into account. This patch fix this problem.
    
    Signed-off-by: Miao Xie <miaox@cn.fujitsu.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index c9005f216975..2e63cdc2b093 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -65,6 +65,21 @@ struct inode_defrag {
 	int cycled;
 };
 
+static int __compare_inode_defrag(struct inode_defrag *defrag1,
+				  struct inode_defrag *defrag2)
+{
+	if (defrag1->root > defrag2->root)
+		return 1;
+	else if (defrag1->root < defrag2->root)
+		return -1;
+	else if (defrag1->ino > defrag2->ino)
+		return 1;
+	else if (defrag1->ino < defrag2->ino)
+		return -1;
+	else
+		return 0;
+}
+
 /* pop a record for an inode into the defrag tree.  The lock
  * must be held already
  *
@@ -81,15 +96,17 @@ static void __btrfs_add_inode_defrag(struct inode *inode,
 	struct inode_defrag *entry;
 	struct rb_node **p;
 	struct rb_node *parent = NULL;
+	int ret;
 
 	p = &root->fs_info->defrag_inodes.rb_node;
 	while (*p) {
 		parent = *p;
 		entry = rb_entry(parent, struct inode_defrag, rb_node);
 
-		if (defrag->ino < entry->ino)
+		ret = __compare_inode_defrag(defrag, entry);
+		if (ret < 0)
 			p = &parent->rb_left;
-		else if (defrag->ino > entry->ino)
+		else if (ret > 0)
 			p = &parent->rb_right;
 		else {
 			/* if we're reinserting an entry for
@@ -159,28 +176,35 @@ int btrfs_add_inode_defrag(struct btrfs_trans_handle *trans,
 /*
  * must be called with the defrag_inodes lock held
  */
-struct inode_defrag *btrfs_find_defrag_inode(struct btrfs_fs_info *info, u64 ino,
+struct inode_defrag *btrfs_find_defrag_inode(struct btrfs_fs_info *info,
+					     u64 root, u64 ino,
 					     struct rb_node **next)
 {
 	struct inode_defrag *entry = NULL;
+	struct inode_defrag tmp;
 	struct rb_node *p;
 	struct rb_node *parent = NULL;
+	int ret;
+
+	tmp.ino = ino;
+	tmp.root = root;
 
 	p = info->defrag_inodes.rb_node;
 	while (p) {
 		parent = p;
 		entry = rb_entry(parent, struct inode_defrag, rb_node);
 
-		if (ino < entry->ino)
+		ret = __compare_inode_defrag(&tmp, entry);
+		if (ret < 0)
 			p = parent->rb_left;
-		else if (ino > entry->ino)
+		else if (ret > 0)
 			p = parent->rb_right;
 		else
 			return entry;
 	}
 
 	if (next) {
-		while (parent && ino > entry->ino) {
+		while (parent && __compare_inode_defrag(&tmp, entry) > 0) {
 			parent = rb_next(parent);
 			entry = rb_entry(parent, struct inode_defrag, rb_node);
 		}
@@ -202,6 +226,7 @@ int btrfs_run_defrag_inodes(struct btrfs_fs_info *fs_info)
 	struct btrfs_key key;
 	struct btrfs_ioctl_defrag_range_args range;
 	u64 first_ino = 0;
+	u64 root_objectid = 0;
 	int num_defrag;
 	int defrag_batch = 1024;
 
@@ -214,11 +239,14 @@ int btrfs_run_defrag_inodes(struct btrfs_fs_info *fs_info)
 		n = NULL;
 
 		/* find an inode to defrag */
-		defrag = btrfs_find_defrag_inode(fs_info, first_ino, &n);
+		defrag = btrfs_find_defrag_inode(fs_info, root_objectid,
+						 first_ino, &n);
 		if (!defrag) {
-			if (n)
-				defrag = rb_entry(n, struct inode_defrag, rb_node);
-			else if (first_ino) {
+			if (n) {
+				defrag = rb_entry(n, struct inode_defrag,
+						  rb_node);
+			} else if (root_objectid || first_ino) {
+				root_objectid = 0;
 				first_ino = 0;
 				continue;
 			} else {
@@ -228,6 +256,7 @@ int btrfs_run_defrag_inodes(struct btrfs_fs_info *fs_info)
 
 		/* remove it from the rbtree */
 		first_ino = defrag->ino + 1;
+		root_objectid = defrag->root;
 		rb_erase(&defrag->rb_node, &fs_info->defrag_inodes);
 
 		if (btrfs_fs_closing(fs_info))

commit 72ac3c0d7921f943d92d1ef42a549fb52e56817d
Author: Josef Bacik <josef@redhat.com>
Date:   Wed May 23 14:13:11 2012 -0400

    Btrfs: convert the inode bit field to use the actual bit operations
    
    Miao pointed this out while I was working on an orphan problem that messing
    with a bitfield where different ranges are protected by different locks
    doesn't work out right.  Turns out we've been doing this forever where we
    have different parts of the bit field protected by either no lock at all or
    different locks which could cause all sorts of weird problems including the
    issue I was hitting.  So instead make a runtime_flags thing that we use the
    normal bit operations on that are all atomic so we can keep having our
    no/different locking for the different flags and then make force_compress
    it's own thing so it can be treated normally.  Thanks,
    
    Signed-off-by: Josef Bacik <josef@redhat.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index cfc0ab915d03..c9005f216975 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -103,7 +103,7 @@ static void __btrfs_add_inode_defrag(struct inode *inode,
 			goto exists;
 		}
 	}
-	BTRFS_I(inode)->in_defrag = 1;
+	set_bit(BTRFS_INODE_IN_DEFRAG, &BTRFS_I(inode)->runtime_flags);
 	rb_link_node(&defrag->rb_node, parent, p);
 	rb_insert_color(&defrag->rb_node, &root->fs_info->defrag_inodes);
 	return;
@@ -131,7 +131,7 @@ int btrfs_add_inode_defrag(struct btrfs_trans_handle *trans,
 	if (btrfs_fs_closing(root->fs_info))
 		return 0;
 
-	if (BTRFS_I(inode)->in_defrag)
+	if (test_bit(BTRFS_INODE_IN_DEFRAG, &BTRFS_I(inode)->runtime_flags))
 		return 0;
 
 	if (trans)
@@ -148,7 +148,7 @@ int btrfs_add_inode_defrag(struct btrfs_trans_handle *trans,
 	defrag->root = root->root_key.objectid;
 
 	spin_lock(&root->fs_info->defrag_inodes_lock);
-	if (!BTRFS_I(inode)->in_defrag)
+	if (!test_bit(BTRFS_INODE_IN_DEFRAG, &BTRFS_I(inode)->runtime_flags))
 		__btrfs_add_inode_defrag(inode, defrag);
 	else
 		kfree(defrag);
@@ -252,7 +252,7 @@ int btrfs_run_defrag_inodes(struct btrfs_fs_info *fs_info)
 			goto next;
 
 		/* do a chunk of defrag */
-		BTRFS_I(inode)->in_defrag = 0;
+		clear_bit(BTRFS_INODE_IN_DEFRAG, &BTRFS_I(inode)->runtime_flags);
 		range.start = defrag->last_offset;
 		num_defrag = btrfs_defrag_file(inode, NULL, &range, defrag->transid,
 					       defrag_batch);
@@ -1465,8 +1465,8 @@ int btrfs_release_file(struct inode *inode, struct file *filp)
 	 * flush down new bytes that may have been written if the
 	 * application were using truncate to replace a file in place.
 	 */
-	if (BTRFS_I(inode)->ordered_data_close) {
-		BTRFS_I(inode)->ordered_data_close = 0;
+	if (test_and_clear_bit(BTRFS_INODE_ORDERED_DATA_CLOSE,
+			       &BTRFS_I(inode)->runtime_flags)) {
 		btrfs_add_ordered_operation(NULL, BTRFS_I(inode)->root, inode);
 		if (inode->i_size > BTRFS_ORDERED_OPERATIONS_FLUSH_LIMIT)
 			filemap_flush(inode->i_mapping);

commit 0885ef5b5601e9b007c383e77c172769b1f214fd
Author: Josef Bacik <josef@redhat.com>
Date:   Mon Apr 23 15:09:39 2012 -0400

    Btrfs: do not do filemap_write_and_wait_range in fsync
    
    We already do the btrfs_wait_ordered_range which will do this for us, so
    just remove this call so we don't call it twice.  Thanks,
    
    Signed-off-by: Josef Bacik <josef@redhat.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 8aa8d7fe74d7..cfc0ab915d03 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1497,14 +1497,15 @@ int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 
 	trace_btrfs_sync_file(file, datasync);
 
-	ret = filemap_write_and_wait_range(inode->i_mapping, start, end);
-	if (ret)
-		return ret;
 	mutex_lock(&inode->i_mutex);
 
-	/* we wait first, since the writeback may change the inode */
+	/*
+	 * we wait first, since the writeback may change the inode, also wait
+	 * ordered range does a filemape_write_and_wait_range which is why we
+	 * don't do it above like other file systems.
+	 */
 	root->log_batch++;
-	btrfs_wait_ordered_range(inode, 0, (u64)-1);
+	btrfs_wait_ordered_range(inode, start, end);
 	root->log_batch++;
 
 	/*

commit 0c4d2d95d06e920e0c61707e62c7fffc9c57f63a
Author: Josef Bacik <josef@redhat.com>
Date:   Thu Apr 5 15:03:02 2012 -0400

    Btrfs: use i_version instead of our own sequence
    
    We've been keeping around the inode sequence number in hopes that somebody
    would use it, but nobody uses it and people actually use i_version which
    serves the same purpose, so use i_version where we used the incore inode's
    sequence number and that way the sequence is updated properly across the
    board, and not just in file write.  Thanks,
    
    Signed-off-by: Josef Bacik <josef@redhat.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 53bf2d764bbc..8aa8d7fe74d7 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1409,7 +1409,6 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 		mutex_unlock(&inode->i_mutex);
 		goto out;
 	}
-	BTRFS_I(inode)->sequence++;
 
 	start_pos = round_down(pos, root->sectorsize);
 	if (start_pos > i_size_read(inode)) {

commit dc7fdde39e4962b1a88741f7eba2a6b3be1285d8
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Apr 27 14:31:29 2012 -0400

    Btrfs: reduce lock contention during extent insertion
    
    We're spending huge amounts of time on lock contention during
    end_io processing because we unconditionally assume we are overwriting
    an existing extent in the file for each IO.
    
    This checks to see if we are outside i_size, and if so, it uses a
    less expensive readonly search of the btree to look for existing
    extents.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index d83260d7498f..53bf2d764bbc 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -567,6 +567,7 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans, struct inode *inode,
 	int extent_type;
 	int recow;
 	int ret;
+	int modify_tree = -1;
 
 	if (drop_cache)
 		btrfs_drop_extent_cache(inode, start, end - 1, 0);
@@ -575,10 +576,13 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans, struct inode *inode,
 	if (!path)
 		return -ENOMEM;
 
+	if (start >= BTRFS_I(inode)->disk_i_size)
+		modify_tree = 0;
+
 	while (1) {
 		recow = 0;
 		ret = btrfs_lookup_file_extent(trans, root, path, ino,
-					       search_start, -1);
+					       search_start, modify_tree);
 		if (ret < 0)
 			break;
 		if (ret > 0 && path->slots[0] > 0 && search_start == start) {
@@ -634,7 +638,8 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans, struct inode *inode,
 		}
 
 		search_start = max(key.offset, start);
-		if (recow) {
+		if (recow || !modify_tree) {
+			modify_tree = -1;
 			btrfs_release_path(path);
 			continue;
 		}

commit 79787eaab46121d4713ed03c8fc63b9ec3eaec76
Author: Jeff Mahoney <jeffm@suse.com>
Date:   Mon Mar 12 16:03:00 2012 +0100

    btrfs: replace many BUG_ONs with proper error handling
    
     btrfs currently handles most errors with BUG_ON. This patch is a work-in-
     progress but aims to handle most errors other than internal logic
     errors and ENOMEM more gracefully.
    
     This iteration prevents most crashes but can run into lockups with
     the page lock on occasion when the timing "works out."
    
    Signed-off-by: Jeff Mahoney <jeffm@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 0eb80cc4ec81..d83260d7498f 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -452,7 +452,7 @@ int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
 			split = alloc_extent_map();
 		if (!split2)
 			split2 = alloc_extent_map();
-		BUG_ON(!split || !split2);
+		BUG_ON(!split || !split2); /* -ENOMEM */
 
 		write_lock(&em_tree->lock);
 		em = lookup_extent_mapping(em_tree, start, len);
@@ -494,7 +494,7 @@ int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
 			split->flags = flags;
 			split->compress_type = em->compress_type;
 			ret = add_extent_mapping(em_tree, split);
-			BUG_ON(ret);
+			BUG_ON(ret); /* Logic error */
 			free_extent_map(split);
 			split = split2;
 			split2 = NULL;
@@ -520,7 +520,7 @@ int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
 			}
 
 			ret = add_extent_mapping(em_tree, split);
-			BUG_ON(ret);
+			BUG_ON(ret); /* Logic error */
 			free_extent_map(split);
 			split = NULL;
 		}
@@ -679,7 +679,7 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans, struct inode *inode,
 						root->root_key.objectid,
 						new_key.objectid,
 						start - extent_offset, 0);
-				BUG_ON(ret);
+				BUG_ON(ret); /* -ENOMEM */
 				*hint_byte = disk_bytenr;
 			}
 			key.offset = start;
@@ -754,7 +754,7 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans, struct inode *inode,
 						root->root_key.objectid,
 						key.objectid, key.offset -
 						extent_offset, 0);
-				BUG_ON(ret);
+				BUG_ON(ret); /* -ENOMEM */
 				inode_sub_bytes(inode,
 						extent_end - key.offset);
 				*hint_byte = disk_bytenr;
@@ -770,7 +770,10 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans, struct inode *inode,
 
 			ret = btrfs_del_items(trans, root, path, del_slot,
 					      del_nr);
-			BUG_ON(ret);
+			if (ret) {
+				btrfs_abort_transaction(trans, root, ret);
+				goto out;
+			}
 
 			del_nr = 0;
 			del_slot = 0;
@@ -782,11 +785,13 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans, struct inode *inode,
 		BUG_ON(1);
 	}
 
-	if (del_nr > 0) {
+	if (!ret && del_nr > 0) {
 		ret = btrfs_del_items(trans, root, path, del_slot, del_nr);
-		BUG_ON(ret);
+		if (ret)
+			btrfs_abort_transaction(trans, root, ret);
 	}
 
+out:
 	btrfs_free_path(path);
 	return ret;
 }
@@ -944,7 +949,10 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 			btrfs_release_path(path);
 			goto again;
 		}
-		BUG_ON(ret < 0);
+		if (ret < 0) {
+			btrfs_abort_transaction(trans, root, ret);
+			goto out;
+		}
 
 		leaf = path->nodes[0];
 		fi = btrfs_item_ptr(leaf, path->slots[0] - 1,
@@ -963,7 +971,7 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 		ret = btrfs_inc_extent_ref(trans, root, bytenr, num_bytes, 0,
 					   root->root_key.objectid,
 					   ino, orig_offset, 0);
-		BUG_ON(ret);
+		BUG_ON(ret); /* -ENOMEM */
 
 		if (split == start) {
 			key.offset = start;
@@ -990,7 +998,7 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 		ret = btrfs_free_extent(trans, root, bytenr, num_bytes,
 					0, root->root_key.objectid,
 					ino, orig_offset, 0);
-		BUG_ON(ret);
+		BUG_ON(ret); /* -ENOMEM */
 	}
 	other_start = 0;
 	other_end = start;
@@ -1007,7 +1015,7 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 		ret = btrfs_free_extent(trans, root, bytenr, num_bytes,
 					0, root->root_key.objectid,
 					ino, orig_offset, 0);
-		BUG_ON(ret);
+		BUG_ON(ret); /* -ENOMEM */
 	}
 	if (del_nr == 0) {
 		fi = btrfs_item_ptr(leaf, path->slots[0],
@@ -1025,7 +1033,10 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 		btrfs_mark_buffer_dirty(leaf);
 
 		ret = btrfs_del_items(trans, root, path, del_slot, del_nr);
-		BUG_ON(ret);
+		if (ret < 0) {
+			btrfs_abort_transaction(trans, root, ret);
+			goto out;
+		}
 	}
 out:
 	btrfs_free_path(path);
@@ -1666,7 +1677,13 @@ static long btrfs_fallocate(struct file *file, int mode,
 
 		em = btrfs_get_extent(inode, NULL, 0, cur_offset,
 				      alloc_end - cur_offset, 0);
-		BUG_ON(IS_ERR_OR_NULL(em));
+		if (IS_ERR_OR_NULL(em)) {
+			if (!em)
+				ret = -ENOMEM;
+			else
+				ret = PTR_ERR(em);
+			break;
+		}
 		last_byte = min(extent_map_end(em), alloc_end);
 		actual_end = min_t(u64, extent_map_end(em), offset + len);
 		last_byte = (last_byte + mask) & ~mask;

commit d0082371cf086e0ba2bbd0367b2c9920532df24f
Author: Jeff Mahoney <jeffm@suse.com>
Date:   Thu Mar 1 14:57:19 2012 +0100

    btrfs: drop gfp_t from lock_extent
    
     lock_extent and unlock_extent are always called with GFP_NOFS, drop the
     argument and use GFP_NOFS consistently.
    
    Signed-off-by: Jeff Mahoney <jeffm@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index e8d06b6b9194..0eb80cc4ec81 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1105,8 +1105,7 @@ static noinline int prepare_pages(struct btrfs_root *root, struct file *file,
 	if (start_pos < inode->i_size) {
 		struct btrfs_ordered_extent *ordered;
 		lock_extent_bits(&BTRFS_I(inode)->io_tree,
-				 start_pos, last_pos - 1, 0, &cached_state,
-				 GFP_NOFS);
+				 start_pos, last_pos - 1, 0, &cached_state);
 		ordered = btrfs_lookup_first_ordered_extent(inode,
 							    last_pos - 1);
 		if (ordered &&
@@ -1638,7 +1637,7 @@ static long btrfs_fallocate(struct file *file, int mode,
 		 * transaction
 		 */
 		lock_extent_bits(&BTRFS_I(inode)->io_tree, alloc_start,
-				 locked_end, 0, &cached_state, GFP_NOFS);
+				 locked_end, 0, &cached_state);
 		ordered = btrfs_lookup_first_ordered_extent(inode,
 							    alloc_end - 1);
 		if (ordered &&
@@ -1737,7 +1736,7 @@ static int find_desired_extent(struct inode *inode, loff_t *offset, int origin)
 		return -ENXIO;
 
 	lock_extent_bits(&BTRFS_I(inode)->io_tree, lockstart, lockend, 0,
-			 &cached_state, GFP_NOFS);
+			 &cached_state);
 
 	/*
 	 * Delalloc is such a pain.  If we have a hole and we have pending

commit 855a85f704026d5fe7de94fb1b765fe03404507f
Merge: ee3253241a92 e77266e4c4be
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Feb 24 09:02:53 2012 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs
    
    Quoth Chris:
     "This is later than I wanted because I got backed up running through
      btrfs bugs from the Oracle QA teams.  But they are all bug fixes that
      we've queued and tested since rc1.
    
      Nothing in particular stands out, this just reflects bug fixing and QA
      done in parallel by all the btrfs developers.  The most user visible
      of these is:
    
        Btrfs: clear the extent uptodate bits during parent transid failures
    
      Because that helps deal with out of date drives (say an iscsi disk
      that has gone away and come back).  The old code wasn't always
      properly retrying the other mirror for this type of failure."
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs: (24 commits)
      Btrfs: fix compiler warnings on 32 bit systems
      Btrfs: increase the global block reserve estimates
      Btrfs: clear the extent uptodate bits during parent transid failures
      Btrfs: add extra sanity checks on the path names in btrfs_mksubvol
      Btrfs: make sure we update latest_bdev
      Btrfs: improve error handling for btrfs_insert_dir_item callers
      Btrfs: be less strict on finding next node in clear_extent_bit
      Btrfs: fix a bug on overcommit stuff
      Btrfs: kick out redundant stuff in convert_extent_bit
      Btrfs: skip states when they does not contain bits to clear
      Btrfs: check return value of lookup_extent_mapping() correctly
      Btrfs: fix deadlock on page lock when doing auto-defragment
      Btrfs: fix return value check of extent_io_ops
      btrfs: honor umask when creating subvol root
      btrfs: silence warning in raid array setup
      btrfs: fix structs where bitfields and spinlock/atomic share 8B word
      btrfs: delalloc for page dirtied out-of-band in fixup worker
      Btrfs: fix memory leak in load_free_space_cache()
      btrfs: don't check DUP chunks twice
      Btrfs: fix trim 0 bytes after a device delete
      ...

commit 6af021d8fc3bcce790e7fbb391e39c5920fa3f71
Author: Jeff Liu <jeff.liu@oracle.com>
Date:   Thu Feb 9 14:25:50 2012 +0800

    Btrfs: return the internal error unchanged if btrfs_get_extent_fiemap() call failed for SEEK_DATA/SEEK_HOLE inquiry
    
    Given that ENXIO only means "offset beyond EOF" for either SEEK_DATA or SEEK_HOLE inquiry
    in a desired file range, so we should return the internal error unchanged if btrfs_get_extent_fiemap()
    call failed, rather than ENXIO.
    
    Cc: Dave Chinner <david@fromorbit.com>
    Signed-off-by: Jie Liu <jeff.liu@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 0621a3a7d5d1..3a520899b024 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1755,7 +1755,7 @@ static int find_desired_extent(struct inode *inode, loff_t *offset, int origin)
 						     start - root->sectorsize,
 						     root->sectorsize, 0);
 		if (IS_ERR(em)) {
-			ret = -ENXIO;
+			ret = PTR_ERR(em);
 			goto out;
 		}
 		last_end = em->start + em->len;
@@ -1767,7 +1767,7 @@ static int find_desired_extent(struct inode *inode, loff_t *offset, int origin)
 	while (1) {
 		em = btrfs_get_extent_fiemap(inode, NULL, 0, start, len, 0);
 		if (IS_ERR(em)) {
-			ret = -ENXIO;
+			ret = PTR_ERR(em);
 			break;
 		}
 

commit d98456fcafa6f3fd1985f9b7429aaa3531c6bfa0
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Jan 31 20:27:41 2012 -0500

    Btrfs: don't reserve data with extents locked in btrfs_fallocate
    
    btrfs_fallocate tries to allocate space only if ranges in the file don't
    already exist.  But the enospc checks it does are not allowed with
    extents locked.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 0f61e11a2998..0621a3a7d5d1 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1603,6 +1603,14 @@ static long btrfs_fallocate(struct file *file, int mode,
 	if (mode & ~FALLOC_FL_KEEP_SIZE)
 		return -EOPNOTSUPP;
 
+	/*
+	 * Make sure we have enough space before we do the
+	 * allocation.
+	 */
+	ret = btrfs_check_data_free_space(inode, len);
+	if (ret)
+		return ret;
+
 	/*
 	 * wait for ordered IO before we have any locks.  We'll loop again
 	 * below with the locks held.
@@ -1666,27 +1674,12 @@ static long btrfs_fallocate(struct file *file, int mode,
 		if (em->block_start == EXTENT_MAP_HOLE ||
 		    (cur_offset >= inode->i_size &&
 		     !test_bit(EXTENT_FLAG_PREALLOC, &em->flags))) {
-
-			/*
-			 * Make sure we have enough space before we do the
-			 * allocation.
-			 */
-			ret = btrfs_check_data_free_space(inode, last_byte -
-							  cur_offset);
-			if (ret) {
-				free_extent_map(em);
-				break;
-			}
-
 			ret = btrfs_prealloc_file_range(inode, mode, cur_offset,
 							last_byte - cur_offset,
 							1 << inode->i_blkbits,
 							offset + len,
 							&alloc_hint);
 
-			/* Let go of our reservation. */
-			btrfs_free_reserved_data_space(inode, last_byte -
-						       cur_offset);
 			if (ret < 0) {
 				free_extent_map(em);
 				break;
@@ -1714,6 +1707,8 @@ static long btrfs_fallocate(struct file *file, int mode,
 			     &cached_state, GFP_NOFS);
 out:
 	mutex_unlock(&inode->i_mutex);
+	/* Let go of our reservation. */
+	btrfs_free_reserved_data_space(inode, len);
 	return ret;
 }
 

commit f9156c7288e2d11501ded4d7fe6d9a3a41ee4057
Merge: 67175b855bfd 96bdc7dc61fb
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jan 17 15:49:54 2012 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs: (62 commits)
      Btrfs: use larger system chunks
      Btrfs: add a delalloc mutex to inodes for delalloc reservations
      Btrfs: space leak tracepoints
      Btrfs: protect orphan block rsv with spin_lock
      Btrfs: add allocator tracepoints
      Btrfs: don't call btrfs_throttle in file write
      Btrfs: release space on error in page_mkwrite
      Btrfs: fix btrfsck error 400 when truncating a compressed
      Btrfs: do not use btrfs_end_transaction_throttle everywhere
      Btrfs: add balance progress reporting
      Btrfs: allow for resuming restriper after it was paused
      Btrfs: allow for canceling restriper
      Btrfs: allow for pausing restriper
      Btrfs: add skip_balance mount option
      Btrfs: recover balance on mount
      Btrfs: save balance parameters to disk
      Btrfs: soft profile changing mode (aka soft convert)
      Btrfs: implement online profile changing
      Btrfs: do not reduce profile in do_chunk_alloc()
      Btrfs: virtual address space subset filter
      ...
    
    Fix up trivial conflict in fs/btrfs/ioctl.c due to the use of the new
    mnt_drop_write_file() helper.

commit 45a8090e626ab470c91142954431a93846030b0d
Author: Josef Bacik <josef@redhat.com>
Date:   Thu Jan 12 19:10:12 2012 -0500

    Btrfs: don't call btrfs_throttle in file write
    
    Btrfs_throttle will make us wait if there is a currently committing transaction
    until we can open new transactions, which is ridiculous since we don't actually
    start any transactions within the file write path anyway, so all this does is
    introduce big latencies if we have a sync/fsync heavy workload going on while
    somebody else is trying to do work.  Thanks,
    
    Signed-off-by: Josef Bacik <josef@redhat.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index fc97b00bd871..0f61e11a2998 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1273,7 +1273,6 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 						   dirty_pages);
 		if (dirty_pages < (root->leafsize >> PAGE_CACHE_SHIFT) + 1)
 			btrfs_btree_balance_dirty(root, 1);
-		btrfs_throttle(root);
 
 		pos += copied;
 		num_written += copied;

commit 9785dbdf265ddc47d5c88267d89a97648c0dc14b
Merge: d756bd2d9339 6bf7e080d5bc
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Jan 16 15:26:31 2012 -0500

    Merge branch 'for-chris' of git://git.jan-o-sch.net/btrfs-unstable into integration

commit 001a541ea9163ace5e8243ee0e907ad80a4c0ec2
Merge: 40ba587923ae bc31b86a5923
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jan 10 16:59:59 2012 -0800

    Merge branch 'writeback-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/wfg/linux
    
    * 'writeback-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/wfg/linux:
      writeback: move MIN_WRITEBACK_PAGES to fs-writeback.c
      writeback: balanced_rate cannot exceed write bandwidth
      writeback: do strict bdi dirty_exceeded
      writeback: avoid tiny dirty poll intervals
      writeback: max, min and target dirty pause time
      writeback: dirty ratelimit - think time compensation
      btrfs: fix dirtied pages accounting on sub-page writes
      writeback: fix dirtied pages accounting on redirty
      writeback: fix dirtied pages accounting on sub-page writes
      writeback: charge leaked page dirties to active tasks
      writeback: Include all dirty inodes in background writeback

commit e3a41a5ba9c2ab988b9f1442925109dca2382fd9
Author: Johannes Weiner <jweiner@redhat.com>
Date:   Tue Jan 10 15:07:55 2012 -0800

    btrfs: pass __GFP_WRITE for buffered write page allocations
    
    Tell the page allocator that pages allocated for a buffered write are
    expected to become dirty soon.
    
    Signed-off-by: Johannes Weiner <jweiner@redhat.com>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Cc: Minchan Kim <minchan.kim@gmail.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Shaohua Li <shaohua.li@intel.com>
    Cc: Chris Mason <chris.mason@oracle.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 97fbe939c050..20375e6691c3 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1081,7 +1081,7 @@ static noinline int prepare_pages(struct btrfs_root *root, struct file *file,
 again:
 	for (i = 0; i < num_pages; i++) {
 		pages[i] = find_or_create_page(inode->i_mapping, index + i,
-					       mask);
+					       mask | __GFP_WRITE);
 		if (!pages[i]) {
 			faili = i - 1;
 			err = -ENOMEM;

commit 66d7e7f09f77456fe68683247d77721032a00ee5
Author: Arne Jansen <sensille@gmx.net>
Date:   Mon Sep 12 15:26:38 2011 +0200

    Btrfs: mark delayed refs as for cow
    
    Add a for_cow parameter to add_delayed_*_ref and pass the appropriate value
    from every call site. The for_cow parameter will later on be used to
    determine if a ref will change anything with respect to qgroups.
    
    Delayed refs coming from relocation are always counted as for_cow, as they
    don't change subvol quota.
    
    Also pass in the fs_info for later use.
    
    btrfs_find_all_roots() will use this as an optimization, as changes that are
    for_cow will not change anything with respect to which root points to a
    certain leaf. Thus, we don't need to add the current sequence number to
    those delayed refs.
    
    Signed-off-by: Arne Jansen <sensille@gmx.net>
    Signed-off-by: Jan Schmidt <list.btrfs@jan-o-sch.net>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index f2e928289600..d2b60ed6c33f 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -678,7 +678,7 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans, struct inode *inode,
 						disk_bytenr, num_bytes, 0,
 						root->root_key.objectid,
 						new_key.objectid,
-						start - extent_offset);
+						start - extent_offset, 0);
 				BUG_ON(ret);
 				*hint_byte = disk_bytenr;
 			}
@@ -753,7 +753,7 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans, struct inode *inode,
 						disk_bytenr, num_bytes, 0,
 						root->root_key.objectid,
 						key.objectid, key.offset -
-						extent_offset);
+						extent_offset, 0);
 				BUG_ON(ret);
 				inode_sub_bytes(inode,
 						extent_end - key.offset);
@@ -962,7 +962,7 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 
 		ret = btrfs_inc_extent_ref(trans, root, bytenr, num_bytes, 0,
 					   root->root_key.objectid,
-					   ino, orig_offset);
+					   ino, orig_offset, 0);
 		BUG_ON(ret);
 
 		if (split == start) {
@@ -989,7 +989,7 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 		del_nr++;
 		ret = btrfs_free_extent(trans, root, bytenr, num_bytes,
 					0, root->root_key.objectid,
-					ino, orig_offset);
+					ino, orig_offset, 0);
 		BUG_ON(ret);
 	}
 	other_start = 0;
@@ -1006,7 +1006,7 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 		del_nr++;
 		ret = btrfs_free_extent(trans, root, bytenr, num_bytes,
 					0, root->root_key.objectid,
-					ino, orig_offset);
+					ino, orig_offset, 0);
 		BUG_ON(ret);
 	}
 	if (del_nr == 0) {

commit 32c7f202a4801252a0f3578807b75a961f792870
Author: Wu Fengguang <fengguang.wu@intel.com>
Date:   Mon Aug 8 15:19:47 2011 -0600

    btrfs: fix dirtied pages accounting on sub-page writes
    
    When doing 1KB sequential writes to the same page,
    balance_dirty_pages_ratelimited_nr() should be called once instead of 4
    times, the latter makes the dirtier tasks be throttled much too heavy.
    
    Fix it with proper de-accounting on clear_page_dirty_for_io().
    
    CC: Chris Mason <chris.mason@oracle.com>
    Signed-off-by: Wu Fengguang <fengguang.wu@intel.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 97fbe939c050..bfb620ead295 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1136,7 +1136,8 @@ static noinline int prepare_pages(struct btrfs_root *root, struct file *file,
 				     GFP_NOFS);
 	}
 	for (i = 0; i < num_pages; i++) {
-		clear_page_dirty_for_io(pages[i]);
+		if (clear_page_dirty_for_io(pages[i]))
+			account_page_redirty(pages[i]);
 		set_page_extent_mapped(pages[i]);
 		WARN_ON(!PageLocked(pages[i]));
 	}

commit c9a7fe9672612c0b595633d2945f52257ad92b20
Merge: 2cfab8d74ebf d85c8a6f1bc0 142349f541d0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Dec 16 12:15:50 2011 -0800

    Merge branches 'for-linus' and 'for-linus-3.2' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs:
      Btrfs: unplug every once and a while
      Btrfs: deal with NULL srv_rsv in the delalloc inode reservation code
      Btrfs: only set cache_generation if we setup the block group
      Btrfs: don't panic if orphan item already exists
      Btrfs: fix leaked space in truncate
      Btrfs: fix how we do delalloc reservations and how we free reservations on error
      Btrfs: deal with enospc from dirtying inodes properly
      Btrfs: fix num_workers_starting bug and other bugs in async thread
      BTRFS: Establish i_ops before calling d_instantiate
      Btrfs: add a cond_resched() into the worker loop
      Btrfs: fix ctime update of on-disk inode
      btrfs: keep orphans for subvolume deletion
      Btrfs: fix inaccurate available space on raid0 profile
      Btrfs: fix wrong disk space information of the files
      Btrfs: fix wrong i_size when truncating a file to a larger size
      Btrfs: fix btrfs_end_bio to deal with write errors to a single mirror
    
    * 'for-linus-3.2' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs:
      btrfs: lower the dirty balance poll interval

commit 142349f541d0bb6bc3e0d4563268105aada42b0b
Author: Wu Fengguang <fengguang.wu@intel.com>
Date:   Fri Dec 16 12:32:57 2011 -0500

    btrfs: lower the dirty balance poll interval
    
    Tests show that the original large intervals can easily make the dirty
    limit exceeded on 100 concurrent dd's. So adapt to as large as the
    next check point selected by the dirty throttling algorithm.
    
    Signed-off-by: Wu Fengguang <fengguang.wu@intel.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index dafdfa059bf6..52305a885c3f 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1167,6 +1167,8 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 	nrptrs = min((iov_iter_count(i) + PAGE_CACHE_SIZE - 1) /
 		     PAGE_CACHE_SIZE, PAGE_CACHE_SIZE /
 		     (sizeof(struct page *)));
+	nrptrs = min(nrptrs, current->nr_dirtied_pause - current->nr_dirtied);
+	nrptrs = max(nrptrs, 8);
 	pages = kmalloc(nrptrs * sizeof(struct page *), GFP_KERNEL);
 	if (!pages)
 		return -ENOMEM;

commit 22c44fe65adacd20a174f3f54686509ee94ef7be
Author: Josef Bacik <josef@redhat.com>
Date:   Wed Nov 30 10:45:38 2011 -0500

    Btrfs: deal with enospc from dirtying inodes properly
    
    Now that we're properly keeping track of delayed inode space we've been getting
    a lot of warnings out of btrfs_dirty_inode() when running xfstest 83.  This is
    because a bunch of people call mark_inode_dirty, which is void so we can't
    return ENOSPC.  This needs to be fixed in a few areas
    
    1) file_update_time - this updates the mtime and such when writing to a file,
    which will call mark_inode_dirty.  So copy file_update_time into btrfs so we can
    call btrfs_dirty_inode directly and return an error if we get one appropriately.
    
    2) fix symlinks to use btrfs_setattr for ->setattr.  For some reason we weren't
    setting ->setattr for symlinks, even though we should have been.  This catches
    one of the cases where we were getting errors in mark_inode_dirty.
    
    3) Fix btrfs_setattr and btrfs_setsize to call btrfs_dirty_inode directly
    instead of mark_inode_dirty.  This lets us return errors properly for truncate
    and chown/anything related to setattr.
    
    4) Add a new btrfs_fs_dirty_inode which will just call btrfs_dirty_inode and
    print an error if we have one.  The only remaining user we can't control for
    this is touch_atime(), but we don't really want to keep people from walking
    down the tree if we don't have space to save the atime update, so just complain
    but don't worry about it.
    
    With this patch xfstests 83 complains a handful of times instead of hundreds of
    times.  Thanks,
    
    Signed-off-by: Josef Bacik <josef@redhat.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index f2e928289600..cc7492c823f3 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1387,7 +1387,11 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 		goto out;
 	}
 
-	file_update_time(file);
+	err = btrfs_update_time(file);
+	if (err) {
+		mutex_unlock(&inode->i_mutex);
+		goto out;
+	}
 	BTRFS_I(inode)->sequence++;
 
 	start_pos = round_down(pos, root->sectorsize);

commit 6a6662ced4153f6dbcfc40d7225c3cc45416039c
Merge: 32aaeffbd4a7 7c7e82a77fe3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Nov 6 20:03:41 2011 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs: (114 commits)
      Btrfs: check for a null fs root when writing to the backup root log
      Btrfs: fix race during transaction joins
      Btrfs: fix a potential btrfs_bio leak on scrub fixups
      Btrfs: rename btrfs_bio multi -> bbio for consistency
      Btrfs: stop leaking btrfs_bios on readahead
      Btrfs: stop the readahead threads on failed mount
      Btrfs: fix extent_buffer leak in the metadata IO error handling
      Btrfs: fix the new inspection ioctls for 32 bit compat
      Btrfs: fix delayed insertion reservation
      Btrfs: ClearPageError during writepage and clean_tree_block
      Btrfs: be smarter about committing the transaction in reserve_metadata_bytes
      Btrfs: make a delayed_block_rsv for the delayed item insertion
      Btrfs: add a log of past tree roots
      btrfs: separate superblock items out of fs_info
      Btrfs: use the global reserve when truncating the free space cache inode
      Btrfs: release metadata from global reserve if we have to fallback for unlink
      Btrfs: make sure to flush queued bios if write_cache_pages waits
      Btrfs: fix extent pinning bugs in the tree log
      Btrfs: make sure btrfs_remove_free_space doesn't leak EAGAIN
      Btrfs: don't wait as long for more batches during SSD log commit
      ...

commit ef3d0fd27e90f67e35da516dafc1482c82939a60
Author: Andi Kleen <ak@linux.intel.com>
Date:   Thu Sep 15 16:06:48 2011 -0700

    vfs: do (nearly) lockless generic_file_llseek
    
    The i_mutex lock use of generic _file_llseek hurts.  Independent processes
    accessing the same file synchronize over a single lock, even though
    they have no need for synchronization at all.
    
    Under high utilization this can cause llseek to scale very poorly on larger
    systems.
    
    This patch does some rethinking of the llseek locking model:
    
    First the 64bit f_pos is not necessarily atomic without locks
    on 32bit systems. This can already cause races with read() today.
    This was discussed on linux-kernel in the past and deemed acceptable.
    The patch does not change that.
    
    Let's look at the different seek variants:
    
    SEEK_SET: Doesn't really need any locking.
    If there's a race one writer wins, the other loses.
    
    For 32bit the non atomic update races against read()
    stay the same. Without a lock they can also happen
    against write() now.  The read() race was deemed
    acceptable in past discussions, and I think if it's
    ok for read it's ok for write too.
    
    => Don't need a lock.
    
    SEEK_END: This behaves like SEEK_SET plus it reads
    the maximum size too. Reading the maximum size would have the
    32bit atomic problem. But luckily we already have a way to read
    the maximum size without locking (i_size_read), so we
    can just use that instead.
    
    Without i_mutex there is no synchronization with write() anymore,
    however since the write() update is atomic on 64bit it just behaves
    like another racy SEEK_SET.  On non atomic 32bit it's the same
    as SEEK_SET.
    
    => Don't need a lock, but need to use i_size_read()
    
    SEEK_CUR: This has a read-modify-write race window
    on the same file. One could argue that any application
    doing unsynchronized seeks on the same file is already broken.
    But for the sake of not adding a regression here I'm
    using the file->f_lock to synchronize this. Using this
    lock is much better than the inode mutex because it doesn't
    synchronize between processes.
    
    => So still need a lock, but can use a f_lock.
    
    This patch implements this new scheme in generic_file_llseek.
    I dropped generic_file_llseek_unlocked and changed all callers.
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index e4e57d59edb7..1266f6e9cdb2 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1821,7 +1821,7 @@ static loff_t btrfs_file_llseek(struct file *file, loff_t offset, int origin)
 	switch (origin) {
 	case SEEK_END:
 	case SEEK_CUR:
-		offset = generic_file_llseek_unlocked(file, offset, origin);
+		offset = generic_file_llseek(file, offset, origin);
 		goto out;
 	case SEEK_DATA:
 	case SEEK_HOLE:

commit 3b16a4e3c355ee3c790473decfcf83d4faeb8ce0
Author: Josef Bacik <josef@redhat.com>
Date:   Wed Sep 21 15:05:58 2011 -0400

    Btrfs: use the inode's mapping mask for allocating pages
    
    Johannes pointed out we were allocating only kernel pages for doing writes,
    which is kind of a big deal if you are on 32bit and have more than a gig of ram.
    So fix our allocations to use the mapping's gfp but still clear __GFP_FS so we
    don't re-enter.  Thanks,
    
    Reported-by: Johannes Weiner <jweiner@redhat.com>
    Signed-off-by: Josef Bacik <josef@redhat.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index de569af766fe..f2e928289600 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1069,6 +1069,7 @@ static noinline int prepare_pages(struct btrfs_root *root, struct file *file,
 	int i;
 	unsigned long index = pos >> PAGE_CACHE_SHIFT;
 	struct inode *inode = fdentry(file)->d_inode;
+	gfp_t mask = btrfs_alloc_write_mask(inode->i_mapping);
 	int err = 0;
 	int faili = 0;
 	u64 start_pos;
@@ -1080,7 +1081,7 @@ static noinline int prepare_pages(struct btrfs_root *root, struct file *file,
 again:
 	for (i = 0; i < num_pages; i++) {
 		pages[i] = find_or_create_page(inode->i_mapping, index + i,
-					       GFP_NOFS);
+					       mask);
 		if (!pages[i]) {
 			faili = i - 1;
 			err = -ENOMEM;

commit 1b9c332b6c92e992b1971a08412c6f460a54b514
Author: Josef Bacik <josef@redhat.com>
Date:   Wed Aug 17 10:19:52 2011 -0400

    Btrfs: only reserve space in fallocate if we have to do a preallocate
    
    Lukas found a problem where if he tries to fallocate over the same region twice
    and the first fallocate took up all the space we would fail with ENOSPC.  This
    is because we reserve the total space we want to use for fallocate, regardless
    of wether or not we will have to actually preallocate.  So instead move the
    check into the loop where we actually have to do the preallocate.  Thanks,
    
    Tested-by: Lukas Czerner <lczerner@redhat.com>
    Signed-off-by: Josef Bacik <josef@redhat.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index e4e57d59edb7..de569af766fe 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1615,10 +1615,6 @@ static long btrfs_fallocate(struct file *file, int mode,
 			goto out;
 	}
 
-	ret = btrfs_check_data_free_space(inode, alloc_end - alloc_start);
-	if (ret)
-		goto out;
-
 	locked_end = alloc_end - 1;
 	while (1) {
 		struct btrfs_ordered_extent *ordered;
@@ -1664,11 +1660,27 @@ static long btrfs_fallocate(struct file *file, int mode,
 		if (em->block_start == EXTENT_MAP_HOLE ||
 		    (cur_offset >= inode->i_size &&
 		     !test_bit(EXTENT_FLAG_PREALLOC, &em->flags))) {
+
+			/*
+			 * Make sure we have enough space before we do the
+			 * allocation.
+			 */
+			ret = btrfs_check_data_free_space(inode, last_byte -
+							  cur_offset);
+			if (ret) {
+				free_extent_map(em);
+				break;
+			}
+
 			ret = btrfs_prealloc_file_range(inode, mode, cur_offset,
 							last_byte - cur_offset,
 							1 << inode->i_blkbits,
 							offset + len,
 							&alloc_hint);
+
+			/* Let go of our reservation. */
+			btrfs_free_reserved_data_space(inode, last_byte -
+						       cur_offset);
 			if (ret < 0) {
 				free_extent_map(em);
 				break;
@@ -1694,8 +1706,6 @@ static long btrfs_fallocate(struct file *file, int mode,
 	}
 	unlock_extent_cached(&BTRFS_I(inode)->io_tree, alloc_start, locked_end,
 			     &cached_state, GFP_NOFS);
-
-	btrfs_free_reserved_data_space(inode, alloc_end - alloc_start);
 out:
 	mutex_unlock(&inode->i_mutex);
 	return ret;

commit 7fd21be75dce605e7cf273bd64b6d733d422fb04
Merge: 9b13776977d4 b6316429af7f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 3 12:17:44 2011 -0700

    Merge branch 'btrfs-3.0' of git://github.com/chrismason/linux
    
    * 'btrfs-3.0' of git://github.com/chrismason/linux:
      Btrfs: force a page fault if we have a shorty copy on a page boundary

commit b6316429af7f365f307dfd2b6a7a42f2563aef19
Author: Josef Bacik <josef@redhat.com>
Date:   Fri Sep 30 15:23:54 2011 -0400

    Btrfs: force a page fault if we have a shorty copy on a page boundary
    
    A user reported a problem where ceph was getting into 100% cpu usage while doing
    some writing.  It turns out it's because we were doing a short write on a not
    uptodate page, which means we'd fall back at one page at a time and fault the
    page in.  The problem is our position is on the page boundary, so our fault in
    logic wasn't actually reading the page, so we'd just spin forever or until the
    page got read in by somebody else.  This will force a readpage if we end up
    doing a short copy.  Alexandre could reproduce this easily with ceph and reports
    it fixes his problem.  I also wrote a reproducer that no longer hangs my box
    with this patch.  Thanks,
    
    Reported-and-tested-by: Alexandre Oliva <aoliva@redhat.com>
    Signed-off-by: Josef Bacik <josef@redhat.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 98d95bb5f253..e73051099368 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1036,11 +1036,13 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
  * on error we return an unlocked page and the error value
  * on success we return a locked page and 0
  */
-static int prepare_uptodate_page(struct page *page, u64 pos)
+static int prepare_uptodate_page(struct page *page, u64 pos,
+				 bool force_uptodate)
 {
 	int ret = 0;
 
-	if ((pos & (PAGE_CACHE_SIZE - 1)) && !PageUptodate(page)) {
+	if (((pos & (PAGE_CACHE_SIZE - 1)) || force_uptodate) &&
+	    !PageUptodate(page)) {
 		ret = btrfs_readpage(NULL, page);
 		if (ret)
 			return ret;
@@ -1061,7 +1063,7 @@ static int prepare_uptodate_page(struct page *page, u64 pos)
 static noinline int prepare_pages(struct btrfs_root *root, struct file *file,
 			 struct page **pages, size_t num_pages,
 			 loff_t pos, unsigned long first_index,
-			 size_t write_bytes)
+			 size_t write_bytes, bool force_uptodate)
 {
 	struct extent_state *cached_state = NULL;
 	int i;
@@ -1086,10 +1088,11 @@ static noinline int prepare_pages(struct btrfs_root *root, struct file *file,
 		}
 
 		if (i == 0)
-			err = prepare_uptodate_page(pages[i], pos);
+			err = prepare_uptodate_page(pages[i], pos,
+						    force_uptodate);
 		if (i == num_pages - 1)
 			err = prepare_uptodate_page(pages[i],
-						    pos + write_bytes);
+						    pos + write_bytes, false);
 		if (err) {
 			page_cache_release(pages[i]);
 			faili = i - 1;
@@ -1158,6 +1161,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 	size_t num_written = 0;
 	int nrptrs;
 	int ret = 0;
+	bool force_page_uptodate = false;
 
 	nrptrs = min((iov_iter_count(i) + PAGE_CACHE_SIZE - 1) /
 		     PAGE_CACHE_SIZE, PAGE_CACHE_SIZE /
@@ -1200,7 +1204,8 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 		 * contents of pages from loop to loop
 		 */
 		ret = prepare_pages(root, file, pages, num_pages,
-				    pos, first_index, write_bytes);
+				    pos, first_index, write_bytes,
+				    force_page_uptodate);
 		if (ret) {
 			btrfs_delalloc_release_space(inode,
 					num_pages << PAGE_CACHE_SHIFT);
@@ -1217,12 +1222,15 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 		if (copied < write_bytes)
 			nrptrs = 1;
 
-		if (copied == 0)
+		if (copied == 0) {
+			force_page_uptodate = true;
 			dirty_pages = 0;
-		else
+		} else {
+			force_page_uptodate = false;
 			dirty_pages = (copied + offset +
 				       PAGE_CACHE_SIZE - 1) >>
 				       PAGE_CACHE_SHIFT;
+		}
 
 		/*
 		 * If we had a short copy we need to release the excess delaloc

commit 48802c8ae2a9d618ec734a61283d645ad527e06c
Author: Jeff Liu <jeff.liu@oracle.com>
Date:   Sun Sep 18 10:34:02 2011 -0400

    BTRFS: Fix lseek return value for error
    
    The recent reworking of btrfs' lseek lead to incorrect
    values being returned.  This adds checks for seeking
    beyond EOF in SEEK_HOLE and makes sure the error
    values come back correct.
    
    Andi Kleen also sent in similar patches.
    
    Signed-off-by: Jie Liu <jeff.liu@oracle.com>
    Reported-by: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 3c3abff731a7..a381cd22f518 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1817,6 +1817,11 @@ static loff_t btrfs_file_llseek(struct file *file, loff_t offset, int origin)
 		goto out;
 	case SEEK_DATA:
 	case SEEK_HOLE:
+		if (offset >= i_size_read(inode)) {
+			mutex_unlock(&inode->i_mutex);
+			return -ENXIO;
+		}
+
 		ret = find_desired_extent(inode, &offset, origin);
 		if (ret) {
 			mutex_unlock(&inode->i_mutex);
@@ -1825,11 +1830,11 @@ static loff_t btrfs_file_llseek(struct file *file, loff_t offset, int origin)
 	}
 
 	if (offset < 0 && !(file->f_mode & FMODE_UNSIGNED_OFFSET)) {
-		ret = -EINVAL;
+		offset = -EINVAL;
 		goto out;
 	}
 	if (offset > inode->i_sb->s_maxbytes) {
-		ret = -EINVAL;
+		offset = -EINVAL;
 		goto out;
 	}
 

commit 0b001b2edaead6fd906b1f87967ae05f082189c4
Merge: 5dfcc87fd79d d525e8ab022c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 12 11:47:49 2011 -0700

    Merge branch 'for-linus' of git://github.com/chrismason/linux
    
    * 'for-linus' of git://github.com/chrismason/linux:
      Btrfs: add dummy extent if dst offset excceeds file end in
      Btrfs: calc file extent num_bytes correctly in file clone
      btrfs: xattr: fix attribute removal
      Btrfs: fix wrong nbytes information of the inode
      Btrfs: fix the file extent gap when doing direct IO
      Btrfs: fix unclosed transaction handle in btrfs_cont_expand
      Btrfs: fix misuse of trans block rsv
      Btrfs: reset to appropriate block rsv after orphan operations
      Btrfs: skip locking if searching the commit root in csum lookup
      btrfs: fix warning in iput for bad-inode
      Btrfs: fix an oops when deleting snapshots

commit 0c1a98c81413e00a6c379d898e06a09350d31926
Author: Miao Xie <miaox@cn.fujitsu.com>
Date:   Sun Sep 11 10:52:24 2011 -0400

    Btrfs: fix the file extent gap when doing direct IO
    
    When we write some data to the place that is beyond the end of the file
    in direct I/O mode, a data hole will be created. And Btrfs should insert
    a file extent item that point to this hole into the fs tree. But unfortunately
    Btrfs forgets doing it.
    
    The following is a simple way to reproduce it:
     # mkfs.btrfs /dev/sdc2
     # mount /dev/sdc2 /test4
     # touch /test4/a
     # dd if=/dev/zero of=/test4/a seek=8 count=1 bs=4K oflag=direct conv=nocreat,notrunc
     # umount /test4
     # btrfsck /dev/sdc2
     root 5 inode 257 errors 100
    
    Reported-by: Tsutomu Itoh <t-itoh@jp.fujitsu.com>
    Signed-off-by: Miao Xie <miaox@cn.fujitsu.com>
    Tested-by: Tsutomu Itoh <t-itoh@jp.fujitsu.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 15e5a1cd8764..98d95bb5f253 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1075,12 +1075,6 @@ static noinline int prepare_pages(struct btrfs_root *root, struct file *file,
 	start_pos = pos & ~((u64)root->sectorsize - 1);
 	last_pos = ((u64)index + num_pages) << PAGE_CACHE_SHIFT;
 
-	if (start_pos > inode->i_size) {
-		err = btrfs_cont_expand(inode, i_size_read(inode), start_pos);
-		if (err)
-			return err;
-	}
-
 again:
 	for (i = 0; i < num_pages; i++) {
 		pages[i] = find_or_create_page(inode->i_mapping, index + i,
@@ -1338,6 +1332,7 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 	struct inode *inode = fdentry(file)->d_inode;
 	struct btrfs_root *root = BTRFS_I(inode)->root;
 	loff_t *ppos = &iocb->ki_pos;
+	u64 start_pos;
 	ssize_t num_written = 0;
 	ssize_t err = 0;
 	size_t count, ocount;
@@ -1386,6 +1381,15 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 	file_update_time(file);
 	BTRFS_I(inode)->sequence++;
 
+	start_pos = round_down(pos, root->sectorsize);
+	if (start_pos > i_size_read(inode)) {
+		err = btrfs_cont_expand(inode, i_size_read(inode), start_pos);
+		if (err) {
+			mutex_unlock(&inode->i_mutex);
+			goto out;
+		}
+	}
+
 	if (unlikely(file->f_flags & O_DIRECT)) {
 		num_written = __btrfs_direct_write(iocb, iov, nr_segs,
 						   pos, ppos, count, ocount);

commit 81d86e1b70961f4816f961875e0c706b0954acad
Merge: 9a4327ca1f45 f1e490a7ebe4
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Aug 18 10:38:03 2011 -0400

    Merge branch 'btrfs-3.0' into for-linus

commit f1e490a7ebe41e06324abbbcd86005b0af02a375
Author: Josef Bacik <josef@redhat.com>
Date:   Thu Aug 18 10:36:39 2011 -0400

    Btrfs: set i_size properly when fallocating and we already
    
    xfstests exposed a problem with preallocate when it fallocates a range that
    already has an extent.  We don't set the new i_size properly because we see that
    we already have an extent.  This isn't right and we should update i_size if the
    space already exists.  With this patch we now pass xfstests 075.  Thanks,
    
    Signed-off-by: Josef Bacik <josef@redhat.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 0705d15542c6..15e5a1cd8764 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1631,11 +1631,15 @@ static long btrfs_fallocate(struct file *file, int mode,
 
 	cur_offset = alloc_start;
 	while (1) {
+		u64 actual_end;
+
 		em = btrfs_get_extent(inode, NULL, 0, cur_offset,
 				      alloc_end - cur_offset, 0);
 		BUG_ON(IS_ERR_OR_NULL(em));
 		last_byte = min(extent_map_end(em), alloc_end);
+		actual_end = min_t(u64, extent_map_end(em), offset + len);
 		last_byte = (last_byte + mask) & ~mask;
+
 		if (em->block_start == EXTENT_MAP_HOLE ||
 		    (cur_offset >= inode->i_size &&
 		     !test_bit(EXTENT_FLAG_PREALLOC, &em->flags))) {
@@ -1648,6 +1652,16 @@ static long btrfs_fallocate(struct file *file, int mode,
 				free_extent_map(em);
 				break;
 			}
+		} else if (actual_end > inode->i_size &&
+			   !(mode & FALLOC_FL_KEEP_SIZE)) {
+			/*
+			 * We didn't need to allocate any more space, but we
+			 * still extended the size of the file so we need to
+			 * update i_size.
+			 */
+			inode->i_ctime = CURRENT_TIME;
+			i_size_write(inode, actual_end);
+			btrfs_ordered_update_i_size(inode, actual_end, NULL);
 		}
 		free_extent_map(em);
 

commit 9a4327ca1f45f82edad7dc0a4e52ce9316e0950c
Author: Dan Carpenter <error27@gmail.com>
Date:   Thu Aug 18 10:16:05 2011 -0400

    btrfs: unlock on error in btrfs_file_llseek()
    
    There were some unlocks on error missing in a recent patch to
    btrfs_file_llseek().
    
    Signed-off-by: Dan Carpenter <error27@gmail.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 658d66959abe..f7d9df7f3fdd 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1804,10 +1804,14 @@ static loff_t btrfs_file_llseek(struct file *file, loff_t offset, int origin)
 		}
 	}
 
-	if (offset < 0 && !(file->f_mode & FMODE_UNSIGNED_OFFSET))
-		return -EINVAL;
-	if (offset > inode->i_sb->s_maxbytes)
-		return -EINVAL;
+	if (offset < 0 && !(file->f_mode & FMODE_UNSIGNED_OFFSET)) {
+		ret = -EINVAL;
+		goto out;
+	}
+	if (offset > inode->i_sb->s_maxbytes) {
+		ret = -EINVAL;
+		goto out;
+	}
 
 	/* Special lock needed here? */
 	if (offset != file->f_pos) {

commit f4ac904c411b55e58bb240f332f93db2455f0010
Author: Dan Carpenter <error27@gmail.com>
Date:   Fri Aug 5 14:19:00 2011 +0000

    btrfs: memory leak in btrfs_add_inode_defrag()
    
    We don't use the defrag struct on this path.
    
    Signed-off-by: Dan Carpenter <error27@gmail.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 010aec8be824..0705d15542c6 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -150,6 +150,8 @@ int btrfs_add_inode_defrag(struct btrfs_trans_handle *trans,
 	spin_lock(&root->fs_info->defrag_inodes_lock);
 	if (!BTRFS_I(inode)->in_defrag)
 		__btrfs_add_inode_defrag(inode, defrag);
+	else
+		kfree(defrag);
 	spin_unlock(&root->fs_info->defrag_inodes_lock);
 	return 0;
 }

commit ed8f37370d83e695c0a4fa5d5fc7a83ecb947526
Merge: a6b11f533889 0d10ee2e6deb
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Aug 2 21:14:05 2011 -1000

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/btrfs-unstable
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/btrfs-unstable: (31 commits)
      Btrfs: don't call writepages from within write_full_page
      Btrfs: Remove unused variable 'last_index' in file.c
      Btrfs: clean up for find_first_extent_bit()
      Btrfs: clean up for wait_extent_bit()
      Btrfs: clean up for insert_state()
      Btrfs: remove unused members from struct extent_state
      Btrfs: clean up code for merging extent maps
      Btrfs: clean up code for extent_map lookup
      Btrfs: clean up search_extent_mapping()
      Btrfs: remove redundant code for dir item lookup
      Btrfs: make acl functions really no-op if acl is not enabled
      Btrfs: remove remaining ref-cache code
      Btrfs: remove a BUG_ON() in btrfs_commit_transaction()
      Btrfs: use wait_event()
      Btrfs: check the nodatasum flag when writing compressed files
      Btrfs: copy string correctly in INO_LOOKUP ioctl
      Btrfs: don't print the leaf if we had an error
      btrfs: make btrfs_set_root_node void
      Btrfs: fix oops while writing data to SSD partitions
      Btrfs: Protect the readonly flag of block group
      ...
    
    Fix up trivial conflicts (due to acl and writeback cleanups) in
     - fs/btrfs/acl.c
     - fs/btrfs/ctree.h
     - fs/btrfs/extent_io.c

commit 341d14f161a475ebdbc9adff1f7e681e1185dee9
Author: Mitch Harder <mitch.harder@sabayonlinux.org>
Date:   Tue Jul 12 19:43:45 2011 +0000

    Btrfs: Remove unused variable 'last_index' in file.c
    
    The variable 'last_index' is calculated in the __btrfs_buffered_write
    function and passed as a parameter to the prepare_pages function,
    but is not used anywhere in the prepare_pages function.
    
    Remove instances of 'last_index' in these functions.
    
    Signed-off-by: Mitch Harder <mitch.harder@sabayonlinux.org>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 7f134a730efb..010aec8be824 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1059,7 +1059,7 @@ static int prepare_uptodate_page(struct page *page, u64 pos)
 static noinline int prepare_pages(struct btrfs_root *root, struct file *file,
 			 struct page **pages, size_t num_pages,
 			 loff_t pos, unsigned long first_index,
-			 unsigned long last_index, size_t write_bytes)
+			 size_t write_bytes)
 {
 	struct extent_state *cached_state = NULL;
 	int i;
@@ -1159,7 +1159,6 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 	struct btrfs_root *root = BTRFS_I(inode)->root;
 	struct page **pages = NULL;
 	unsigned long first_index;
-	unsigned long last_index;
 	size_t num_written = 0;
 	int nrptrs;
 	int ret = 0;
@@ -1172,7 +1171,6 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 		return -ENOMEM;
 
 	first_index = pos >> PAGE_CACHE_SHIFT;
-	last_index = (pos + iov_iter_count(i)) >> PAGE_CACHE_SHIFT;
 
 	while (iov_iter_count(i) > 0) {
 		size_t offset = pos & (PAGE_CACHE_SIZE - 1);
@@ -1206,8 +1204,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 		 * contents of pages from loop to loop
 		 */
 		ret = prepare_pages(root, file, pages, num_pages,
-				    pos, first_index, last_index,
-				    write_bytes);
+				    pos, first_index, write_bytes);
 		if (ret) {
 			btrfs_delalloc_release_space(inode,
 					num_pages << PAGE_CACHE_SHIFT);

commit a0f98dde11a1afe9fbf5c98f57968e086e98b6f5
Author: Wanlong Gao <wanlong.gao@gmail.com>
Date:   Mon Jul 18 12:19:35 2011 +0000

    Btrfs:don't check the return value of __btrfs_add_inode_defrag
    
    Don't need to check the return value of __btrfs_add_inode_defrag(),
    since it will always return 0.
    
    Signed-off-by: Wanlong Gao <gaowanlong@cn.fujitsu.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 41ca5fdaee6c..7f134a730efb 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -74,7 +74,7 @@ struct inode_defrag {
  * If an existing record is found the defrag item you
  * pass in is freed
  */
-static int __btrfs_add_inode_defrag(struct inode *inode,
+static void __btrfs_add_inode_defrag(struct inode *inode,
 				    struct inode_defrag *defrag)
 {
 	struct btrfs_root *root = BTRFS_I(inode)->root;
@@ -106,11 +106,11 @@ static int __btrfs_add_inode_defrag(struct inode *inode,
 	BTRFS_I(inode)->in_defrag = 1;
 	rb_link_node(&defrag->rb_node, parent, p);
 	rb_insert_color(&defrag->rb_node, &root->fs_info->defrag_inodes);
-	return 0;
+	return;
 
 exists:
 	kfree(defrag);
-	return 0;
+	return;
 
 }
 
@@ -123,7 +123,6 @@ int btrfs_add_inode_defrag(struct btrfs_trans_handle *trans,
 {
 	struct btrfs_root *root = BTRFS_I(inode)->root;
 	struct inode_defrag *defrag;
-	int ret = 0;
 	u64 transid;
 
 	if (!btrfs_test_opt(root, AUTO_DEFRAG))
@@ -150,9 +149,9 @@ int btrfs_add_inode_defrag(struct btrfs_trans_handle *trans,
 
 	spin_lock(&root->fs_info->defrag_inodes_lock);
 	if (!BTRFS_I(inode)->in_defrag)
-		ret = __btrfs_add_inode_defrag(inode, defrag);
+		__btrfs_add_inode_defrag(inode, defrag);
 	spin_unlock(&root->fs_info->defrag_inodes_lock);
-	return ret;
+	return 0;
 }
 
 /*

commit b43b31bdf2e662006c27cc4dcccf863312d62bc1
Merge: ff95acb6733d 38a1a9195357
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Aug 1 14:27:34 2011 -0400

    Merge branch 'alloc_path' of git://git.kernel.org/pub/scm/linux/kernel/git/mfasheh/btrfs-error-handling into for-linus

commit 22712200e175e0df5c7f9edfe6c6bf5c94c23b83
Merge: 597a67e0ba75 ff95acb6733d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jul 27 16:43:52 2011 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/btrfs-unstable
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/btrfs-unstable:
      Btrfs: make sure reserve_metadata_bytes doesn't leak out strange errors
      Btrfs: use the commit_root for reading free_space_inode crcs
      Btrfs: reduce extent_state lock contention for metadata
      Btrfs: remove lockdep magic from btrfs_next_leaf
      Btrfs: make a lockdep class for each root
      Btrfs: switch the btrfs tree locks to reader/writer
      Btrfs: fix deadlock when throttling transactions
      Btrfs: stop using highmem for extent_buffers
      Btrfs: fix BUG_ON() caused by ENOSPC when relocating space
      Btrfs: tag pages for writeback in sync
      Btrfs: fix enospc problems with delalloc
      Btrfs: don't flush delalloc arbitrarily
      Btrfs: use find_or_create_page instead of grab_cache_page
      Btrfs: use a worker thread to do caching
      Btrfs: fix how we merge extent states and deal with cached states
      Btrfs: use the normal checksumming infrastructure for free space cache
      Btrfs: serialize flushers in reserve_metadata_bytes
      Btrfs: do transaction space reservation before joining the transaction
      Btrfs: try to only do one btrfs_search_slot in do_setxattr

commit 9e0baf60dea69f31ac3b1adeb35b03b02a53e8e1
Author: Josef Bacik <josef@redhat.com>
Date:   Fri Jul 15 15:16:44 2011 +0000

    Btrfs: fix enospc problems with delalloc
    
    So I had this brilliant idea to use atomic counters for outstanding and reserved
    extents, but this turned out to be a bad idea.  Consider this where we have 1
    outstanding extent and 1 reserved extent
    
    Reserver                                Releaser
                                            atomic_dec(outstanding) now 0
    atomic_read(outstanding)+1 get 1
    atomic_read(reserved) get 1
    don't actually reserve anything because
    they are the same
                                            atomic_cmpxchg(reserved, 1, 0)
    atomic_inc(outstanding)
    atomic_add(0, reserved)
                                            free reserved space for 1 extent
    
    Then the reserver now has no actual space reserved for it, and when it goes to
    finish the ordered IO it won't have enough space to do it's allocation and you
    get those lovely warnings.
    
    Signed-off-by: Josef Bacik <josef@redhat.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index bd6bbb877ff2..6e56a468d1f5 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1239,9 +1239,11 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 		 * managed to copy.
 		 */
 		if (num_pages > dirty_pages) {
-			if (copied > 0)
-				atomic_inc(
-					&BTRFS_I(inode)->outstanding_extents);
+			if (copied > 0) {
+				spin_lock(&BTRFS_I(inode)->lock);
+				BTRFS_I(inode)->outstanding_extents++;
+				spin_unlock(&BTRFS_I(inode)->lock);
+			}
 			btrfs_delalloc_release_space(inode,
 					(num_pages - dirty_pages) <<
 					PAGE_CACHE_SHIFT);

commit a94733d0bc630edaedc6ca156752dd5a7cb82521
Author: Josef Bacik <josef@redhat.com>
Date:   Mon Jul 11 10:47:06 2011 -0400

    Btrfs: use find_or_create_page instead of grab_cache_page
    
    grab_cache_page will use mapping_gfp_mask(), which for all inodes is set to
    GFP_HIGHUSER_MOVABLE.  So instead use find_or_create_page in all cases where we
    need GFP_NOFS so we don't deadlock.  Thanks,
    
    Signed-off-by: Josef Bacik <josef@redhat.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index fa4ef18b66b1..bd6bbb877ff2 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1081,7 +1081,8 @@ static noinline int prepare_pages(struct btrfs_root *root, struct file *file,
 
 again:
 	for (i = 0; i < num_pages; i++) {
-		pages[i] = grab_cache_page(inode->i_mapping, index + i);
+		pages[i] = find_or_create_page(inode->i_mapping, index + i,
+					       GFP_NOFS);
 		if (!pages[i]) {
 			faili = i - 1;
 			err = -ENOMEM;

commit 02c24a82187d5a628c68edfe71ae60dc135cd178
Author: Josef Bacik <josef@redhat.com>
Date:   Sat Jul 16 20:44:56 2011 -0400

    fs: push i_mutex and filemap_write_and_wait down into ->fsync() handlers
    
    Btrfs needs to be able to control how filemap_write_and_wait_range() is called
    in fsync to make it less of a painful operation, so push down taking i_mutex and
    the calling of filemap_write_and_wait() down into the ->fsync() handlers.  Some
    file systems can drop taking the i_mutex altogether it seems, like ext3 and
    ocfs2.  For correctness sake I just pushed everything down in all cases to make
    sure that we keep the current behavior the same for everybody, and then each
    individual fs maintainer can make up their mind about what to do from there.
    Thanks,
    
    Acked-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Josef Bacik <josef@redhat.com>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index bd4d061c6e4d..59cbdb120ad0 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1452,7 +1452,7 @@ int btrfs_release_file(struct inode *inode, struct file *filp)
  * important optimization for directories because holding the mutex prevents
  * new operations on the dir while we write to disk.
  */
-int btrfs_sync_file(struct file *file, int datasync)
+int btrfs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
 {
 	struct dentry *dentry = file->f_path.dentry;
 	struct inode *inode = dentry->d_inode;
@@ -1462,9 +1462,13 @@ int btrfs_sync_file(struct file *file, int datasync)
 
 	trace_btrfs_sync_file(file, datasync);
 
+	ret = filemap_write_and_wait_range(inode->i_mapping, start, end);
+	if (ret)
+		return ret;
+	mutex_lock(&inode->i_mutex);
+
 	/* we wait first, since the writeback may change the inode */
 	root->log_batch++;
-	/* the VFS called filemap_fdatawrite for us */
 	btrfs_wait_ordered_range(inode, 0, (u64)-1);
 	root->log_batch++;
 
@@ -1472,8 +1476,10 @@ int btrfs_sync_file(struct file *file, int datasync)
 	 * check the transaction that last modified this inode
 	 * and see if its already been committed
 	 */
-	if (!BTRFS_I(inode)->last_trans)
+	if (!BTRFS_I(inode)->last_trans) {
+		mutex_unlock(&inode->i_mutex);
 		goto out;
+	}
 
 	/*
 	 * if the last transaction that changed this file was before
@@ -1484,6 +1490,7 @@ int btrfs_sync_file(struct file *file, int datasync)
 	if (BTRFS_I(inode)->last_trans <=
 	    root->fs_info->last_trans_committed) {
 		BTRFS_I(inode)->last_trans = 0;
+		mutex_unlock(&inode->i_mutex);
 		goto out;
 	}
 
@@ -1496,12 +1503,15 @@ int btrfs_sync_file(struct file *file, int datasync)
 	trans = btrfs_start_transaction(root, 0);
 	if (IS_ERR(trans)) {
 		ret = PTR_ERR(trans);
+		mutex_unlock(&inode->i_mutex);
 		goto out;
 	}
 
 	ret = btrfs_log_dentry_safe(trans, root, dentry);
-	if (ret < 0)
+	if (ret < 0) {
+		mutex_unlock(&inode->i_mutex);
 		goto out;
+	}
 
 	/* we've logged all the items and now have a consistent
 	 * version of the file in the log.  It is possible that
@@ -1513,7 +1523,7 @@ int btrfs_sync_file(struct file *file, int datasync)
 	 * file again, but that will end up using the synchronization
 	 * inside btrfs_sync_log to keep things safe.
 	 */
-	mutex_unlock(&dentry->d_inode->i_mutex);
+	mutex_unlock(&inode->i_mutex);
 
 	if (ret != BTRFS_NO_LOG_SYNC) {
 		if (ret > 0) {
@@ -1528,7 +1538,6 @@ int btrfs_sync_file(struct file *file, int datasync)
 	} else {
 		ret = btrfs_end_transaction(trans, root);
 	}
-	mutex_lock(&dentry->d_inode->i_mutex);
 out:
 	return ret > 0 ? -EIO : ret;
 }

commit b26751575a9aa55fd6dbf3febde3ff06dfadc44f
Author: Josef Bacik <josef@redhat.com>
Date:   Mon Jul 18 13:21:36 2011 -0400

    Btrfs: implement our own ->llseek
    
    In order to handle SEEK_HOLE/SEEK_DATA we need to implement our own llseek.
    Basically for the normal SEEK_*'s we will just defer to the generic helper, and
    for SEEK_HOLE/SEEK_DATA we will use our fiemap helper to figure out the nearest
    hole or data.  Currently this helper doesn't check for delalloc bytes for
    prealloc space, so for now treat prealloc as data until that is fixed.  Thanks,
    
    Signed-off-by: Josef Bacik <josef@redhat.com>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index fa4ef18b66b1..bd4d061c6e4d 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1664,8 +1664,154 @@ static long btrfs_fallocate(struct file *file, int mode,
 	return ret;
 }
 
+static int find_desired_extent(struct inode *inode, loff_t *offset, int origin)
+{
+	struct btrfs_root *root = BTRFS_I(inode)->root;
+	struct extent_map *em;
+	struct extent_state *cached_state = NULL;
+	u64 lockstart = *offset;
+	u64 lockend = i_size_read(inode);
+	u64 start = *offset;
+	u64 orig_start = *offset;
+	u64 len = i_size_read(inode);
+	u64 last_end = 0;
+	int ret = 0;
+
+	lockend = max_t(u64, root->sectorsize, lockend);
+	if (lockend <= lockstart)
+		lockend = lockstart + root->sectorsize;
+
+	len = lockend - lockstart + 1;
+
+	len = max_t(u64, len, root->sectorsize);
+	if (inode->i_size == 0)
+		return -ENXIO;
+
+	lock_extent_bits(&BTRFS_I(inode)->io_tree, lockstart, lockend, 0,
+			 &cached_state, GFP_NOFS);
+
+	/*
+	 * Delalloc is such a pain.  If we have a hole and we have pending
+	 * delalloc for a portion of the hole we will get back a hole that
+	 * exists for the entire range since it hasn't been actually written
+	 * yet.  So to take care of this case we need to look for an extent just
+	 * before the position we want in case there is outstanding delalloc
+	 * going on here.
+	 */
+	if (origin == SEEK_HOLE && start != 0) {
+		if (start <= root->sectorsize)
+			em = btrfs_get_extent_fiemap(inode, NULL, 0, 0,
+						     root->sectorsize, 0);
+		else
+			em = btrfs_get_extent_fiemap(inode, NULL, 0,
+						     start - root->sectorsize,
+						     root->sectorsize, 0);
+		if (IS_ERR(em)) {
+			ret = -ENXIO;
+			goto out;
+		}
+		last_end = em->start + em->len;
+		if (em->block_start == EXTENT_MAP_DELALLOC)
+			last_end = min_t(u64, last_end, inode->i_size);
+		free_extent_map(em);
+	}
+
+	while (1) {
+		em = btrfs_get_extent_fiemap(inode, NULL, 0, start, len, 0);
+		if (IS_ERR(em)) {
+			ret = -ENXIO;
+			break;
+		}
+
+		if (em->block_start == EXTENT_MAP_HOLE) {
+			if (test_bit(EXTENT_FLAG_VACANCY, &em->flags)) {
+				if (last_end <= orig_start) {
+					free_extent_map(em);
+					ret = -ENXIO;
+					break;
+				}
+			}
+
+			if (origin == SEEK_HOLE) {
+				*offset = start;
+				free_extent_map(em);
+				break;
+			}
+		} else {
+			if (origin == SEEK_DATA) {
+				if (em->block_start == EXTENT_MAP_DELALLOC) {
+					if (start >= inode->i_size) {
+						free_extent_map(em);
+						ret = -ENXIO;
+						break;
+					}
+				}
+
+				*offset = start;
+				free_extent_map(em);
+				break;
+			}
+		}
+
+		start = em->start + em->len;
+		last_end = em->start + em->len;
+
+		if (em->block_start == EXTENT_MAP_DELALLOC)
+			last_end = min_t(u64, last_end, inode->i_size);
+
+		if (test_bit(EXTENT_FLAG_VACANCY, &em->flags)) {
+			free_extent_map(em);
+			ret = -ENXIO;
+			break;
+		}
+		free_extent_map(em);
+		cond_resched();
+	}
+	if (!ret)
+		*offset = min(*offset, inode->i_size);
+out:
+	unlock_extent_cached(&BTRFS_I(inode)->io_tree, lockstart, lockend,
+			     &cached_state, GFP_NOFS);
+	return ret;
+}
+
+static loff_t btrfs_file_llseek(struct file *file, loff_t offset, int origin)
+{
+	struct inode *inode = file->f_mapping->host;
+	int ret;
+
+	mutex_lock(&inode->i_mutex);
+	switch (origin) {
+	case SEEK_END:
+	case SEEK_CUR:
+		offset = generic_file_llseek_unlocked(file, offset, origin);
+		goto out;
+	case SEEK_DATA:
+	case SEEK_HOLE:
+		ret = find_desired_extent(inode, &offset, origin);
+		if (ret) {
+			mutex_unlock(&inode->i_mutex);
+			return ret;
+		}
+	}
+
+	if (offset < 0 && !(file->f_mode & FMODE_UNSIGNED_OFFSET))
+		return -EINVAL;
+	if (offset > inode->i_sb->s_maxbytes)
+		return -EINVAL;
+
+	/* Special lock needed here? */
+	if (offset != file->f_pos) {
+		file->f_pos = offset;
+		file->f_version = 0;
+	}
+out:
+	mutex_unlock(&inode->i_mutex);
+	return offset;
+}
+
 const struct file_operations btrfs_file_operations = {
-	.llseek		= generic_file_llseek,
+	.llseek		= btrfs_file_llseek,
 	.read		= do_sync_read,
 	.write		= do_sync_write,
 	.aio_read       = generic_file_aio_read,

commit d8926bb3badd36670fecf2de4a062c78bc37430b
Author: Mark Fasheh <mfasheh@suse.com>
Date:   Wed Jul 13 10:38:47 2011 -0700

    btrfs: don't BUG_ON btrfs_alloc_path() errors
    
    This patch fixes many callers of btrfs_alloc_path() which BUG_ON allocation
    failure. All the sites that are fixed in this patch were checked by me to
    be fairly trivial to fix because of at least one of two criteria:
    
     - Callers of the function catch errors from it already so bubbling the
       error up will be handled.
     - Callers of the function might BUG_ON any nonzero return code in which
       case there is no behavior changed (but we still got to remove a BUG_ON)
    
    The following functions were updated:
    
    btrfs_lookup_extent, alloc_reserved_tree_block, btrfs_remove_block_group,
    btrfs_lookup_csums_range, btrfs_csum_file_blocks, btrfs_mark_extent_written,
    btrfs_inode_by_name, btrfs_new_inode, btrfs_symlink,
    insert_reserved_file_extent, and run_delalloc_nocow
    
    Signed-off-by: Mark Fasheh <mfasheh@suse.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index fa4ef18b66b1..23d1d811e2b2 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -855,7 +855,8 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 	btrfs_drop_extent_cache(inode, start, end - 1, 0);
 
 	path = btrfs_alloc_path();
-	BUG_ON(!path);
+	if (!path)
+		return -ENOMEM;
 again:
 	recow = 0;
 	split = start;

commit 7841cb2898f66a73062c64d0ef5733dde7279e46
Author: David Sterba <dsterba@suse.cz>
Date:   Tue May 31 18:07:27 2011 +0200

    btrfs: add helper for fs_info->closing
    
    wrap checking of filesystem 'closing' flag and fix a few missing memory
    barriers.
    
    Signed-off-by: David Sterba <dsterba@suse.cz>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 982b5ea9762f..fa4ef18b66b1 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -129,7 +129,7 @@ int btrfs_add_inode_defrag(struct btrfs_trans_handle *trans,
 	if (!btrfs_test_opt(root, AUTO_DEFRAG))
 		return 0;
 
-	if (root->fs_info->closing)
+	if (btrfs_fs_closing(root->fs_info))
 		return 0;
 
 	if (BTRFS_I(inode)->in_defrag)
@@ -229,7 +229,7 @@ int btrfs_run_defrag_inodes(struct btrfs_fs_info *fs_info)
 		first_ino = defrag->ino + 1;
 		rb_erase(&defrag->rb_node, &fs_info->defrag_inodes);
 
-		if (fs_info->closing)
+		if (btrfs_fs_closing(fs_info))
 			goto next_free;
 
 		spin_unlock(&fs_info->defrag_inodes_lock);

commit a4689d2bd3b00dcf5c4320f06e0ab88810fbff9c
Author: David Sterba <dsterba@suse.cz>
Date:   Tue May 31 17:08:14 2011 +0000

    btrfs: use btrfs_ino to access inode number
    
    commit 4cb5300bc ("Btrfs: add mount -o auto_defrag") accesses inode
    number directly while it should use the helper with the new inode
    number allocator.
    
    Signed-off-by: David Sterba <dsterba@suse.cz>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index e3a1b0c2394c..982b5ea9762f 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -144,7 +144,7 @@ int btrfs_add_inode_defrag(struct btrfs_trans_handle *trans,
 	if (!defrag)
 		return -ENOMEM;
 
-	defrag->ino = inode->i_ino;
+	defrag->ino = btrfs_ino(inode);
 	defrag->transid = transid;
 	defrag->root = root->root_key.objectid;
 

commit ff5714cca971848963b87d6b477c16ca8abbaa54
Merge: 174ba50915b0 d90c732122a1
Author: Chris Mason <chris.mason@oracle.com>
Date:   Sat May 28 07:00:39 2011 -0400

    Merge branch 'for-chris' of
    git://git.kernel.org/pub/scm/linux/kernel/git/josef/btrfs-work into for-linus
    
    Conflicts:
            fs/btrfs/disk-io.c
            fs/btrfs/extent-tree.c
            fs/btrfs/free-space-cache.c
            fs/btrfs/inode.c
            fs/btrfs/transaction.c
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

commit 4cb5300bc839b8a943eb19c9f27f25470e22d0ca
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue May 24 15:35:30 2011 -0400

    Btrfs: add mount -o auto_defrag
    
    This will detect small random writes into files and
    queue the up for an auto defrag process.  It isn't well suited to
    database workloads yet, but works for smaller files such as rpm, sqlite
    or bdb databases.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 58ddc4442159..c6a22d783c35 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -40,6 +40,263 @@
 #include "locking.h"
 #include "compat.h"
 
+/*
+ * when auto defrag is enabled we
+ * queue up these defrag structs to remember which
+ * inodes need defragging passes
+ */
+struct inode_defrag {
+	struct rb_node rb_node;
+	/* objectid */
+	u64 ino;
+	/*
+	 * transid where the defrag was added, we search for
+	 * extents newer than this
+	 */
+	u64 transid;
+
+	/* root objectid */
+	u64 root;
+
+	/* last offset we were able to defrag */
+	u64 last_offset;
+
+	/* if we've wrapped around back to zero once already */
+	int cycled;
+};
+
+/* pop a record for an inode into the defrag tree.  The lock
+ * must be held already
+ *
+ * If you're inserting a record for an older transid than an
+ * existing record, the transid already in the tree is lowered
+ *
+ * If an existing record is found the defrag item you
+ * pass in is freed
+ */
+static int __btrfs_add_inode_defrag(struct inode *inode,
+				    struct inode_defrag *defrag)
+{
+	struct btrfs_root *root = BTRFS_I(inode)->root;
+	struct inode_defrag *entry;
+	struct rb_node **p;
+	struct rb_node *parent = NULL;
+
+	p = &root->fs_info->defrag_inodes.rb_node;
+	while (*p) {
+		parent = *p;
+		entry = rb_entry(parent, struct inode_defrag, rb_node);
+
+		if (defrag->ino < entry->ino)
+			p = &parent->rb_left;
+		else if (defrag->ino > entry->ino)
+			p = &parent->rb_right;
+		else {
+			/* if we're reinserting an entry for
+			 * an old defrag run, make sure to
+			 * lower the transid of our existing record
+			 */
+			if (defrag->transid < entry->transid)
+				entry->transid = defrag->transid;
+			if (defrag->last_offset > entry->last_offset)
+				entry->last_offset = defrag->last_offset;
+			goto exists;
+		}
+	}
+	BTRFS_I(inode)->in_defrag = 1;
+	rb_link_node(&defrag->rb_node, parent, p);
+	rb_insert_color(&defrag->rb_node, &root->fs_info->defrag_inodes);
+	return 0;
+
+exists:
+	kfree(defrag);
+	return 0;
+
+}
+
+/*
+ * insert a defrag record for this inode if auto defrag is
+ * enabled
+ */
+int btrfs_add_inode_defrag(struct btrfs_trans_handle *trans,
+			   struct inode *inode)
+{
+	struct btrfs_root *root = BTRFS_I(inode)->root;
+	struct inode_defrag *defrag;
+	int ret = 0;
+	u64 transid;
+
+	if (!btrfs_test_opt(root, AUTO_DEFRAG))
+		return 0;
+
+	if (root->fs_info->closing)
+		return 0;
+
+	if (BTRFS_I(inode)->in_defrag)
+		return 0;
+
+	if (trans)
+		transid = trans->transid;
+	else
+		transid = BTRFS_I(inode)->root->last_trans;
+
+	defrag = kzalloc(sizeof(*defrag), GFP_NOFS);
+	if (!defrag)
+		return -ENOMEM;
+
+	defrag->ino = inode->i_ino;
+	defrag->transid = transid;
+	defrag->root = root->root_key.objectid;
+
+	spin_lock(&root->fs_info->defrag_inodes_lock);
+	if (!BTRFS_I(inode)->in_defrag)
+		ret = __btrfs_add_inode_defrag(inode, defrag);
+	spin_unlock(&root->fs_info->defrag_inodes_lock);
+	return ret;
+}
+
+/*
+ * must be called with the defrag_inodes lock held
+ */
+struct inode_defrag *btrfs_find_defrag_inode(struct btrfs_fs_info *info, u64 ino,
+					     struct rb_node **next)
+{
+	struct inode_defrag *entry = NULL;
+	struct rb_node *p;
+	struct rb_node *parent = NULL;
+
+	p = info->defrag_inodes.rb_node;
+	while (p) {
+		parent = p;
+		entry = rb_entry(parent, struct inode_defrag, rb_node);
+
+		if (ino < entry->ino)
+			p = parent->rb_left;
+		else if (ino > entry->ino)
+			p = parent->rb_right;
+		else
+			return entry;
+	}
+
+	if (next) {
+		while (parent && ino > entry->ino) {
+			parent = rb_next(parent);
+			entry = rb_entry(parent, struct inode_defrag, rb_node);
+		}
+		*next = parent;
+	}
+	return NULL;
+}
+
+/*
+ * run through the list of inodes in the FS that need
+ * defragging
+ */
+int btrfs_run_defrag_inodes(struct btrfs_fs_info *fs_info)
+{
+	struct inode_defrag *defrag;
+	struct btrfs_root *inode_root;
+	struct inode *inode;
+	struct rb_node *n;
+	struct btrfs_key key;
+	struct btrfs_ioctl_defrag_range_args range;
+	u64 first_ino = 0;
+	int num_defrag;
+	int defrag_batch = 1024;
+
+	memset(&range, 0, sizeof(range));
+	range.len = (u64)-1;
+
+	atomic_inc(&fs_info->defrag_running);
+	spin_lock(&fs_info->defrag_inodes_lock);
+	while(1) {
+		n = NULL;
+
+		/* find an inode to defrag */
+		defrag = btrfs_find_defrag_inode(fs_info, first_ino, &n);
+		if (!defrag) {
+			if (n)
+				defrag = rb_entry(n, struct inode_defrag, rb_node);
+			else if (first_ino) {
+				first_ino = 0;
+				continue;
+			} else {
+				break;
+			}
+		}
+
+		/* remove it from the rbtree */
+		first_ino = defrag->ino + 1;
+		rb_erase(&defrag->rb_node, &fs_info->defrag_inodes);
+
+		if (fs_info->closing)
+			goto next_free;
+
+		spin_unlock(&fs_info->defrag_inodes_lock);
+
+		/* get the inode */
+		key.objectid = defrag->root;
+		btrfs_set_key_type(&key, BTRFS_ROOT_ITEM_KEY);
+		key.offset = (u64)-1;
+		inode_root = btrfs_read_fs_root_no_name(fs_info, &key);
+		if (IS_ERR(inode_root))
+			goto next;
+
+		key.objectid = defrag->ino;
+		btrfs_set_key_type(&key, BTRFS_INODE_ITEM_KEY);
+		key.offset = 0;
+
+		inode = btrfs_iget(fs_info->sb, &key, inode_root, NULL);
+		if (IS_ERR(inode))
+			goto next;
+
+		/* do a chunk of defrag */
+		BTRFS_I(inode)->in_defrag = 0;
+		range.start = defrag->last_offset;
+		num_defrag = btrfs_defrag_file(inode, NULL, &range, defrag->transid,
+					       defrag_batch);
+		/*
+		 * if we filled the whole defrag batch, there
+		 * must be more work to do.  Queue this defrag
+		 * again
+		 */
+		if (num_defrag == defrag_batch) {
+			defrag->last_offset = range.start;
+			__btrfs_add_inode_defrag(inode, defrag);
+			/*
+			 * we don't want to kfree defrag, we added it back to
+			 * the rbtree
+			 */
+			defrag = NULL;
+		} else if (defrag->last_offset && !defrag->cycled) {
+			/*
+			 * we didn't fill our defrag batch, but
+			 * we didn't start at zero.  Make sure we loop
+			 * around to the start of the file.
+			 */
+			defrag->last_offset = 0;
+			defrag->cycled = 1;
+			__btrfs_add_inode_defrag(inode, defrag);
+			defrag = NULL;
+		}
+
+		iput(inode);
+next:
+		spin_lock(&fs_info->defrag_inodes_lock);
+next_free:
+		kfree(defrag);
+	}
+	spin_unlock(&fs_info->defrag_inodes_lock);
+
+	atomic_dec(&fs_info->defrag_running);
+
+	/*
+	 * during unmount, we use the transaction_wait queue to
+	 * wait for the defragger to stop
+	 */
+	wake_up(&fs_info->transaction_wait);
+	return 0;
+}
 
 /* simple helper to fault in pages and copy.  This should go away
  * and be replaced with calls into generic code.

commit a4abeea41adfa3c143c289045f4625dfaeba2212
Author: Josef Bacik <josef@redhat.com>
Date:   Mon Apr 11 17:25:13 2011 -0400

    Btrfs: kill trans_mutex
    
    We use trans_mutex for lots of things, here's a basic list
    
    1) To serialize trans_handles joining the currently running transaction
    2) To make sure that no new trans handles are started while we are committing
    3) To protect the dead_roots list and the transaction lists
    
    Really the serializing trans_handles joining is not too hard, and can really get
    bogged down in acquiring a reference to the transaction.  So replace the
    trans_mutex with a trans_lock spinlock and use it to do the following
    
    1) Protect fs_info->running_transaction.  All trans handles have to do is check
    this, and then take a reference of the transaction and keep on going.
    2) Protect the fs_info->trans_list.  This doesn't get used too much, basically
    it just holds the current transactions, which will usually just be the currently
    committing transaction and the currently running transaction at most.
    3) Protect the dead roots list.  This is only ever processed by splicing the
    list so this is relatively simple.
    4) Protect the fs_info->reloc_ctl stuff.  This is very lightweight and was using
    the trans_mutex before, so this is a pretty straightforward change.
    5) Protect fs_info->no_trans_join.  Because we don't hold the trans_lock over
    the entirety of the commit we need to have a way to block new people from
    creating a new transaction while we're doing our work.  So we set no_trans_join
    and in join_transaction we test to see if that is set, and if it is we do a
    wait_on_commit.
    6) Make the transaction use count atomic so we don't need to take locks to
    modify it when we're dropping references.
    7) Add a commit_lock to the transaction to make sure multiple people trying to
    commit the same transaction don't race and commit at the same time.
    8) Make open_ioctl_trans an atomic so we don't have to take any locks for ioctl
    trans.
    
    I have tested this with xfstests, but obviously it is a pretty hairy change so
    lots of testing is greatly appreciated.  Thanks,
    
    Signed-off-by: Josef Bacik <josef@redhat.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 75899a01dded..cd5e82e500cf 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1222,14 +1222,12 @@ int btrfs_sync_file(struct file *file, int datasync)
 	 * the current transaction, we can bail out now without any
 	 * syncing
 	 */
-	mutex_lock(&root->fs_info->trans_mutex);
+	smp_mb();
 	if (BTRFS_I(inode)->last_trans <=
 	    root->fs_info->last_trans_committed) {
 		BTRFS_I(inode)->last_trans = 0;
-		mutex_unlock(&root->fs_info->trans_mutex);
 		goto out;
 	}
-	mutex_unlock(&root->fs_info->trans_mutex);
 
 	/*
 	 * ok we haven't committed the transaction yet, lets do a commit

commit 945d8962ceee6bb273365d0bdf42f763225b290f
Merge: 0d0ca30f1809 4ea028859bbd
Author: Chris Mason <chris.mason@oracle.com>
Date:   Sun May 22 12:33:42 2011 -0400

    Merge branch 'cleanups' of git://repo.or.cz/linux-2.6/btrfs-unstable into inode_numbers
    
    Conflicts:
            fs/btrfs/extent-tree.c
            fs/btrfs/free-space-cache.c
            fs/btrfs/inode.c
            fs/btrfs/tree-log.c
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

commit b3b4aa74b58bded927f579fff787fb6fa1c0393c
Author: David Sterba <dsterba@suse.cz>
Date:   Thu Apr 21 01:20:15 2011 +0200

    btrfs: drop unused parameter from btrfs_release_path
    
    parameter tree root it's not used since commit
    5f39d397dfbe140a14edecd4e73c34ce23c4f9ee ("Btrfs: Create extent_buffer
    interface for large blocksizes")
    
    Signed-off-by: David Sterba <dsterba@suse.cz>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 80eabe85409a..566bdf298ea8 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -376,7 +376,7 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans, struct inode *inode,
 
 		search_start = max(key.offset, start);
 		if (recow) {
-			btrfs_release_path(root, path);
+			btrfs_release_path(path);
 			continue;
 		}
 
@@ -393,7 +393,7 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans, struct inode *inode,
 			ret = btrfs_duplicate_item(trans, root, path,
 						   &new_key);
 			if (ret == -EAGAIN) {
-				btrfs_release_path(root, path);
+				btrfs_release_path(path);
 				continue;
 			}
 			if (ret < 0)
@@ -516,7 +516,7 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans, struct inode *inode,
 			del_nr = 0;
 			del_slot = 0;
 
-			btrfs_release_path(root, path);
+			btrfs_release_path(path);
 			continue;
 		}
 
@@ -681,7 +681,7 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 		new_key.offset = split;
 		ret = btrfs_duplicate_item(trans, root, path, &new_key);
 		if (ret == -EAGAIN) {
-			btrfs_release_path(root, path);
+			btrfs_release_path(path);
 			goto again;
 		}
 		BUG_ON(ret < 0);
@@ -721,7 +721,7 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 			     inode->i_ino, bytenr, orig_offset,
 			     &other_start, &other_end)) {
 		if (recow) {
-			btrfs_release_path(root, path);
+			btrfs_release_path(path);
 			goto again;
 		}
 		extent_end = other_end;
@@ -738,7 +738,7 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 			     inode->i_ino, bytenr, orig_offset,
 			     &other_start, &other_end)) {
 		if (recow) {
-			btrfs_release_path(root, path);
+			btrfs_release_path(path);
 			goto again;
 		}
 		key.offset = other_start;

commit 172ddd60a662c4d8bf2809462866ddddd6431ea5
Author: David Sterba <dsterba@suse.cz>
Date:   Thu Apr 21 00:48:27 2011 +0200

    btrfs: drop gfp parameter from alloc_extent_map
    
    pass GFP_NOFS directly to kmem_cache_alloc
    
    Signed-off-by: David Sterba <dsterba@suse.cz>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 83abd274370b..80eabe85409a 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -191,9 +191,9 @@ int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
 	}
 	while (1) {
 		if (!split)
-			split = alloc_extent_map(GFP_NOFS);
+			split = alloc_extent_map();
 		if (!split2)
-			split2 = alloc_extent_map(GFP_NOFS);
+			split2 = alloc_extent_map();
 		BUG_ON(!split || !split2);
 
 		write_lock(&em_tree->lock);

commit c704005d886cf0bc9bc3974eb009b22fe0da32c7
Author: David Sterba <dsterba@suse.cz>
Date:   Tue Apr 19 18:00:01 2011 +0200

    btrfs: unify checking of IS_ERR and null
    
    use IS_ERR_OR_NULL when possible, done by this coccinelle script:
    
    @ match @
    identifier id;
    @@
    (
    - BUG_ON(IS_ERR(id) || !id);
    + BUG_ON(IS_ERR_OR_NULL(id));
    |
    - IS_ERR(id) || !id
    + IS_ERR_OR_NULL(id)
    |
    - !id || IS_ERR(id)
    + IS_ERR_OR_NULL(id)
    )
    
    Signed-off-by: David Sterba <dsterba@suse.cz>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 75899a01dded..83abd274370b 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1375,7 +1375,7 @@ static long btrfs_fallocate(struct file *file, int mode,
 	while (1) {
 		em = btrfs_get_extent(inode, NULL, 0, cur_offset,
 				      alloc_end - cur_offset, 0);
-		BUG_ON(IS_ERR(em) || !em);
+		BUG_ON(IS_ERR_OR_NULL(em));
 		last_byte = min(extent_map_end(em), alloc_end);
 		last_byte = (last_byte + mask) & ~mask;
 		if (em->block_start == EXTENT_MAP_HOLE ||

commit 33345d01522f8152f99dc84a3e7a1a45707f387f
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Wed Apr 20 10:31:50 2011 +0800

    Btrfs: Always use 64bit inode number
    
    There's a potential problem in 32bit system when we exhaust 32bit inode
    numbers and start to allocate big inode numbers, because btrfs uses
    inode->i_ino in many places.
    
    So here we always use BTRFS_I(inode)->location.objectid, which is an
    u64 variable.
    
    There are 2 exceptions that BTRFS_I(inode)->location.objectid !=
    inode->i_ino: the btree inode (0 vs 1) and empty subvol dirs (256 vs 2),
    and inode->i_ino will be used in those cases.
    
    Another reason to make this change is I'm going to use a special inode
    to save free ino cache, and the inode number must be > (u64)-256.
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 75899a01dded..bef020451525 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -298,6 +298,7 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans, struct inode *inode,
 	struct btrfs_path *path;
 	struct btrfs_key key;
 	struct btrfs_key new_key;
+	u64 ino = btrfs_ino(inode);
 	u64 search_start = start;
 	u64 disk_bytenr = 0;
 	u64 num_bytes = 0;
@@ -318,14 +319,14 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans, struct inode *inode,
 
 	while (1) {
 		recow = 0;
-		ret = btrfs_lookup_file_extent(trans, root, path, inode->i_ino,
+		ret = btrfs_lookup_file_extent(trans, root, path, ino,
 					       search_start, -1);
 		if (ret < 0)
 			break;
 		if (ret > 0 && path->slots[0] > 0 && search_start == start) {
 			leaf = path->nodes[0];
 			btrfs_item_key_to_cpu(leaf, &key, path->slots[0] - 1);
-			if (key.objectid == inode->i_ino &&
+			if (key.objectid == ino &&
 			    key.type == BTRFS_EXTENT_DATA_KEY)
 				path->slots[0]--;
 		}
@@ -346,7 +347,7 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans, struct inode *inode,
 		}
 
 		btrfs_item_key_to_cpu(leaf, &key, path->slots[0]);
-		if (key.objectid > inode->i_ino ||
+		if (key.objectid > ino ||
 		    key.type > BTRFS_EXTENT_DATA_KEY || key.offset >= end)
 			break;
 
@@ -592,6 +593,7 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 	int del_slot = 0;
 	int recow;
 	int ret;
+	u64 ino = btrfs_ino(inode);
 
 	btrfs_drop_extent_cache(inode, start, end - 1, 0);
 
@@ -600,7 +602,7 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 again:
 	recow = 0;
 	split = start;
-	key.objectid = inode->i_ino;
+	key.objectid = ino;
 	key.type = BTRFS_EXTENT_DATA_KEY;
 	key.offset = split;
 
@@ -612,8 +614,7 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 
 	leaf = path->nodes[0];
 	btrfs_item_key_to_cpu(leaf, &key, path->slots[0]);
-	BUG_ON(key.objectid != inode->i_ino ||
-	       key.type != BTRFS_EXTENT_DATA_KEY);
+	BUG_ON(key.objectid != ino || key.type != BTRFS_EXTENT_DATA_KEY);
 	fi = btrfs_item_ptr(leaf, path->slots[0],
 			    struct btrfs_file_extent_item);
 	BUG_ON(btrfs_file_extent_type(leaf, fi) !=
@@ -630,7 +631,7 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 		other_start = 0;
 		other_end = start;
 		if (extent_mergeable(leaf, path->slots[0] - 1,
-				     inode->i_ino, bytenr, orig_offset,
+				     ino, bytenr, orig_offset,
 				     &other_start, &other_end)) {
 			new_key.offset = end;
 			btrfs_set_item_key_safe(trans, root, path, &new_key);
@@ -653,7 +654,7 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 		other_start = end;
 		other_end = 0;
 		if (extent_mergeable(leaf, path->slots[0] + 1,
-				     inode->i_ino, bytenr, orig_offset,
+				     ino, bytenr, orig_offset,
 				     &other_start, &other_end)) {
 			fi = btrfs_item_ptr(leaf, path->slots[0],
 					    struct btrfs_file_extent_item);
@@ -702,7 +703,7 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 
 		ret = btrfs_inc_extent_ref(trans, root, bytenr, num_bytes, 0,
 					   root->root_key.objectid,
-					   inode->i_ino, orig_offset);
+					   ino, orig_offset);
 		BUG_ON(ret);
 
 		if (split == start) {
@@ -718,7 +719,7 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 	other_start = end;
 	other_end = 0;
 	if (extent_mergeable(leaf, path->slots[0] + 1,
-			     inode->i_ino, bytenr, orig_offset,
+			     ino, bytenr, orig_offset,
 			     &other_start, &other_end)) {
 		if (recow) {
 			btrfs_release_path(root, path);
@@ -729,13 +730,13 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 		del_nr++;
 		ret = btrfs_free_extent(trans, root, bytenr, num_bytes,
 					0, root->root_key.objectid,
-					inode->i_ino, orig_offset);
+					ino, orig_offset);
 		BUG_ON(ret);
 	}
 	other_start = 0;
 	other_end = start;
 	if (extent_mergeable(leaf, path->slots[0] - 1,
-			     inode->i_ino, bytenr, orig_offset,
+			     ino, bytenr, orig_offset,
 			     &other_start, &other_end)) {
 		if (recow) {
 			btrfs_release_path(root, path);
@@ -746,7 +747,7 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 		del_nr++;
 		ret = btrfs_free_extent(trans, root, bytenr, num_bytes,
 					0, root->root_key.objectid,
-					inode->i_ino, orig_offset);
+					ino, orig_offset);
 		BUG_ON(ret);
 	}
 	if (del_nr == 0) {

commit be1a12a0dfed06cf1e62e35bf91620dc610a451a
Author: Josef Bacik <josef@redhat.com>
Date:   Wed Apr 6 13:05:22 2011 -0400

    Btrfs: deal with the case that we run out of space in the cache
    
    Currently we don't handle running out of space in the cache, so to fix this we
    keep track of how far in the cache we are.  Then we only dirty the pages if we
    successfully modify all of them, otherwise if we have an error or run out of
    space we can just drop them and not worry about the vm writing them out.
    Thanks,
    
    Tested-by Johannes Hirte <johannes.hirte@fem.tu-ilmenau.de>
    Signed-off-by: Josef Bacik <josef@redhat.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index e621ea54a3fd..75899a01dded 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -104,7 +104,7 @@ static noinline int btrfs_copy_from_user(loff_t pos, int num_pages,
 /*
  * unlocks pages after btrfs_file_write is done with them
  */
-static noinline void btrfs_drop_pages(struct page **pages, size_t num_pages)
+void btrfs_drop_pages(struct page **pages, size_t num_pages)
 {
 	size_t i;
 	for (i = 0; i < num_pages; i++) {
@@ -127,16 +127,13 @@ static noinline void btrfs_drop_pages(struct page **pages, size_t num_pages)
  * this also makes the decision about creating an inline extent vs
  * doing real data extents, marking pages dirty and delalloc as required.
  */
-static noinline int dirty_and_release_pages(struct btrfs_root *root,
-					    struct file *file,
-					    struct page **pages,
-					    size_t num_pages,
-					    loff_t pos,
-					    size_t write_bytes)
+int btrfs_dirty_pages(struct btrfs_root *root, struct inode *inode,
+		      struct page **pages, size_t num_pages,
+		      loff_t pos, size_t write_bytes,
+		      struct extent_state **cached)
 {
 	int err = 0;
 	int i;
-	struct inode *inode = fdentry(file)->d_inode;
 	u64 num_bytes;
 	u64 start_pos;
 	u64 end_of_last_block;
@@ -149,7 +146,7 @@ static noinline int dirty_and_release_pages(struct btrfs_root *root,
 
 	end_of_last_block = start_pos + num_bytes - 1;
 	err = btrfs_set_extent_delalloc(inode, start_pos, end_of_last_block,
-					NULL);
+					cached);
 	if (err)
 		return err;
 
@@ -992,9 +989,9 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 		}
 
 		if (copied > 0) {
-			ret = dirty_and_release_pages(root, file, pages,
-						      dirty_pages, pos,
-						      copied);
+			ret = btrfs_dirty_pages(root, inode, pages,
+						dirty_pages, pos, copied,
+						NULL);
 			if (ret) {
 				btrfs_delalloc_release_space(inode,
 					dirty_pages << PAGE_CACHE_SHIFT);

commit c9149235a42ab93914434fff45c44b45023363f3
Author: Tsutomu Itoh <t-itoh@jp.fujitsu.com>
Date:   Wed Mar 30 00:57:23 2011 +0000

    Btrfs: fix compiler warning in file.c
    
    While compiling Btrfs, I got following messages:
    
      CC [M]  fs/btrfs/file.o
    fs/btrfs/file.c: In function '__btrfs_buffered_write':
    fs/btrfs/file.c:909: warning: 'ret' may be used uninitialized in this function
      CC [M]  fs/btrfs/tree-defrag.o
    
    This patch fixes compiler warning.
    
    Signed-off-by: Tsutomu Itoh <t-itoh@jp.fujitsu.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 656bc0a892b1..e621ea54a3fd 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -906,7 +906,7 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 	unsigned long last_index;
 	size_t num_written = 0;
 	int nrptrs;
-	int ret;
+	int ret = 0;
 
 	nrptrs = min((iov_iter_count(i) + PAGE_CACHE_SIZE - 1) /
 		     PAGE_CACHE_SIZE, PAGE_CACHE_SIZE /

commit 1abe9b8a138c9988ba8f7bfded6453649a31541f
Author: liubo <liubo2009@cn.fujitsu.com>
Date:   Thu Mar 24 11:18:59 2011 +0000

    Btrfs: add initial tracepoint support for btrfs
    
    Tracepoints can provide insight into why btrfs hits bugs and be greatly
    helpful for debugging, e.g
                  dd-7822  [000]  2121.641088: btrfs_inode_request: root = 5(FS_TREE), gen = 4, ino = 256, blocks = 8, disk_i_size = 0, last_trans = 8, logged_trans = 0
                  dd-7822  [000]  2121.641100: btrfs_inode_new: root = 5(FS_TREE), gen = 8, ino = 257, blocks = 0, disk_i_size = 0, last_trans = 0, logged_trans = 0
     btrfs-transacti-7804  [001]  2146.935420: btrfs_cow_block: root = 2(EXTENT_TREE), refs = 2, orig_buf = 29368320 (orig_level = 0), cow_buf = 29388800 (cow_level = 0)
     btrfs-transacti-7804  [001]  2146.935473: btrfs_cow_block: root = 1(ROOT_TREE), refs = 2, orig_buf = 29364224 (orig_level = 0), cow_buf = 29392896 (cow_level = 0)
     btrfs-transacti-7804  [001]  2146.972221: btrfs_transaction_commit: root = 1(ROOT_TREE), gen = 8
       flush-btrfs-2-7821  [001]  2155.824210: btrfs_chunk_alloc: root = 3(CHUNK_TREE), offset = 1103101952, size = 1073741824, num_stripes = 1, sub_stripes = 0, type = DATA
       flush-btrfs-2-7821  [001]  2155.824241: btrfs_cow_block: root = 2(EXTENT_TREE), refs = 2, orig_buf = 29388800 (orig_level = 0), cow_buf = 29396992 (cow_level = 0)
       flush-btrfs-2-7821  [001]  2155.824255: btrfs_cow_block: root = 4(DEV_TREE), refs = 2, orig_buf = 29372416 (orig_level = 0), cow_buf = 29401088 (cow_level = 0)
       flush-btrfs-2-7821  [000]  2155.824329: btrfs_cow_block: root = 3(CHUNK_TREE), refs = 2, orig_buf = 20971520 (orig_level = 0), cow_buf = 20975616 (cow_level = 0)
     btrfs-endio-wri-7800  [001]  2155.898019: btrfs_cow_block: root = 5(FS_TREE), refs = 2, orig_buf = 29384704 (orig_level = 0), cow_buf = 29405184 (cow_level = 0)
     btrfs-endio-wri-7800  [001]  2155.898043: btrfs_cow_block: root = 7(CSUM_TREE), refs = 2, orig_buf = 29376512 (orig_level = 0), cow_buf = 29409280 (cow_level = 0)
    
    Here is what I have added:
    
    1) ordere_extent:
            btrfs_ordered_extent_add
            btrfs_ordered_extent_remove
            btrfs_ordered_extent_start
            btrfs_ordered_extent_put
    
    These provide critical information to understand how ordered_extents are
    updated.
    
    2) extent_map:
            btrfs_get_extent
    
    extent_map is used in both read and write cases, and it is useful for tracking
    how btrfs specific IO is running.
    
    3) writepage:
            __extent_writepage
            btrfs_writepage_end_io_hook
    
    Pages are cirtical resourses and produce a lot of corner cases during writeback,
    so it is valuable to know how page is written to disk.
    
    4) inode:
            btrfs_inode_new
            btrfs_inode_request
            btrfs_inode_evict
    
    These can show where and when a inode is created, when a inode is evicted.
    
    5) sync:
            btrfs_sync_file
            btrfs_sync_fs
    
    These show sync arguments.
    
    6) transaction:
            btrfs_transaction_commit
    
    In transaction based filesystem, it will be useful to know the generation and
    who does commit.
    
    7) back reference and cow:
            btrfs_delayed_tree_ref
            btrfs_delayed_data_ref
            btrfs_delayed_ref_head
            btrfs_cow_block
    
    Btrfs natively supports back references, these tracepoints are helpful on
    understanding btrfs's COW mechanism.
    
    8) chunk:
            btrfs_chunk_alloc
            btrfs_chunk_free
    
    Chunk is a link between physical offset and logical offset, and stands for space
    infomation in btrfs, and these are helpful on tracing space things.
    
    9) reserved_extent:
            btrfs_reserved_extent_alloc
            btrfs_reserved_extent_free
    
    These can show how btrfs uses its space.
    
    Signed-off-by: Liu Bo <liubo2009@cn.fujitsu.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index a85b044cf39e..656bc0a892b1 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1205,6 +1205,7 @@ int btrfs_sync_file(struct file *file, int datasync)
 	int ret = 0;
 	struct btrfs_trans_handle *trans;
 
+	trace_btrfs_sync_file(file, datasync);
 
 	/* we wait first, since the writeback may change the inode */
 	root->log_batch++;

commit 41415730a1050499fbd63b3f7dd59b3a4c3bb91a
Author: Josef Bacik <josef@redhat.com>
Date:   Wed Mar 16 13:59:32 2011 -0400

    Btrfs: check return value of btrfs_search_slot properly
    
    Doing an audit of where we use btrfs_search_slot only showed one place where we
    don't check the return value of btrfs_search_slot properly.  Just fix
    mark_extent_written to see if btrfs_search_slot failed and act accordingly.
    Thanks,
    
    Signed-off-by: Josef Bacik <josef@redhat.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 3786eca2a905..a85b044cf39e 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -608,6 +608,8 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 	key.offset = split;
 
 	ret = btrfs_search_slot(trans, root, &key, path, -1, 1);
+	if (ret < 0)
+		goto out;
 	if (ret > 0 && path->slots[0] > 0)
 		path->slots[0]--;
 

commit a41ad394a03b802497958d7c98a9dcf607266645
Author: Josef Bacik <josef@redhat.com>
Date:   Mon Jan 31 15:30:16 2011 -0500

    Btrfs: convert to the new truncate sequence
    
    ->truncate() is going away, instead all of the work needs to be done in
    ->setattr().  So this converts us over to do this.  It's fairly straightforward,
    just get rid of our .truncate inode operation and call btrfs_truncate() directly
    from btrfs_setsize.  This works out better for us since truncate can technically
    return ENOSPC, and before we had no way of letting anybody know.  Thanks,
    
    Signed-off-by: Josef Bacik <josef@redhat.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 24a19c2743ca..3786eca2a905 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -817,7 +817,7 @@ static noinline int prepare_pages(struct btrfs_root *root, struct file *file,
 	last_pos = ((u64)index + num_pages) << PAGE_CACHE_SHIFT;
 
 	if (start_pos > inode->i_size) {
-		err = btrfs_cont_expand(inode, start_pos);
+		err = btrfs_cont_expand(inode, i_size_read(inode), start_pos);
 		if (err)
 			return err;
 	}
@@ -1330,7 +1330,8 @@ static long btrfs_fallocate(struct file *file, int mode,
 		goto out;
 
 	if (alloc_start > inode->i_size) {
-		ret = btrfs_cont_expand(inode, alloc_start);
+		ret = btrfs_cont_expand(inode, i_size_read(inode),
+					alloc_start);
 		if (ret)
 			goto out;
 	}

commit 4a64001f0047956e283f7ada9843dfc3f3b5d8c8
Author: Josef Bacik <josef@redhat.com>
Date:   Tue Jan 25 15:10:08 2011 -0500

    Btrfs: fix how we deal with the pages array in the write path
    
    Really we don't need to memset the pages array at all, since we know how many
    pages we're going to use in the array and pass that around.  So don't memset,
    just trust we're not idiots and we pass num_pages around properly.
    
    Signed-off-by: Josef Bacik <josef@redhat.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index f2a80e570a6c..24a19c2743ca 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -108,8 +108,6 @@ static noinline void btrfs_drop_pages(struct page **pages, size_t num_pages)
 {
 	size_t i;
 	for (i = 0; i < num_pages; i++) {
-		if (!pages[i])
-			break;
 		/* page checked is some magic around finding pages that
 		 * have been modified without going through btrfs_set_page_dirty
 		 * clear it here
@@ -824,7 +822,6 @@ static noinline int prepare_pages(struct btrfs_root *root, struct file *file,
 			return err;
 	}
 
-	memset(pages, 0, num_pages * sizeof(struct page *));
 again:
 	for (i = 0; i < num_pages; i++) {
 		pages[i] = grab_cache_page(inode->i_mapping, index + i);
@@ -930,7 +927,6 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 		size_t copied;
 
 		WARN_ON(num_pages > nrptrs);
-		memset(pages, 0, sizeof(struct page *) * nrptrs);
 
 		/*
 		 * Fault pages before locking them in prepare_pages
@@ -946,6 +942,11 @@ static noinline ssize_t __btrfs_buffered_write(struct file *file,
 		if (ret)
 			break;
 
+		/*
+		 * This is going to setup the pages array with the number of
+		 * pages we want, so we don't really need to worry about the
+		 * contents of pages from loop to loop
+		 */
 		ret = prepare_pages(root, file, pages, num_pages,
 				    pos, first_index, last_index,
 				    write_bytes);

commit d0215f3e5ebb5803cd6ec067b10c5e00a3ad7cfc
Author: Josef Bacik <josef@redhat.com>
Date:   Tue Jan 25 14:57:24 2011 -0500

    Btrfs: simplify our write path
    
    Our aio_write function is huge and kind of hard to follow at times.  So this
    patch fixes this by breaking out the buffered and direct write paths out into
    seperate functions so it's a little clearer what's going on.  I've also fixed
    some wrong typing that we had and added the ability to handle getting an error
    back from btrfs_set_extent_delalloc.  Tested this with xfstests and everything
    came out fine.  Thanks,
    
    Signed-off-by: Josef Bacik <josef@redhat.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 4d4975592668..f2a80e570a6c 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -45,14 +45,14 @@
  * and be replaced with calls into generic code.
  */
 static noinline int btrfs_copy_from_user(loff_t pos, int num_pages,
-					 int write_bytes,
+					 size_t write_bytes,
 					 struct page **prepared_pages,
 					 struct iov_iter *i)
 {
 	size_t copied = 0;
+	size_t total_copied = 0;
 	int pg = 0;
 	int offset = pos & (PAGE_CACHE_SIZE - 1);
-	int total_copied = 0;
 
 	while (write_bytes > 0) {
 		size_t count = min_t(size_t,
@@ -129,13 +129,12 @@ static noinline void btrfs_drop_pages(struct page **pages, size_t num_pages)
  * this also makes the decision about creating an inline extent vs
  * doing real data extents, marking pages dirty and delalloc as required.
  */
-static noinline int dirty_and_release_pages(struct btrfs_trans_handle *trans,
-				   struct btrfs_root *root,
-				   struct file *file,
-				   struct page **pages,
-				   size_t num_pages,
-				   loff_t pos,
-				   size_t write_bytes)
+static noinline int dirty_and_release_pages(struct btrfs_root *root,
+					    struct file *file,
+					    struct page **pages,
+					    size_t num_pages,
+					    loff_t pos,
+					    size_t write_bytes)
 {
 	int err = 0;
 	int i;
@@ -153,7 +152,8 @@ static noinline int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 	end_of_last_block = start_pos + num_bytes - 1;
 	err = btrfs_set_extent_delalloc(inode, start_pos, end_of_last_block,
 					NULL);
-	BUG_ON(err);
+	if (err)
+		return err;
 
 	for (i = 0; i < num_pages; i++) {
 		struct page *p = pages[i];
@@ -896,127 +896,38 @@ static noinline int prepare_pages(struct btrfs_root *root, struct file *file,
 
 }
 
-static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
-				    const struct iovec *iov,
-				    unsigned long nr_segs, loff_t pos)
+static noinline ssize_t __btrfs_buffered_write(struct file *file,
+					       struct iov_iter *i,
+					       loff_t pos)
 {
-	struct file *file = iocb->ki_filp;
 	struct inode *inode = fdentry(file)->d_inode;
 	struct btrfs_root *root = BTRFS_I(inode)->root;
 	struct page **pages = NULL;
-	struct iov_iter i;
-	loff_t *ppos = &iocb->ki_pos;
-	loff_t start_pos;
-	ssize_t num_written = 0;
-	ssize_t err = 0;
-	size_t count;
-	size_t ocount;
-	int ret = 0;
-	int nrptrs;
 	unsigned long first_index;
 	unsigned long last_index;
-	int will_write;
-	int buffered = 0;
-	int copied = 0;
-	int dirty_pages = 0;
-
-	will_write = ((file->f_flags & O_DSYNC) || IS_SYNC(inode) ||
-		      (file->f_flags & O_DIRECT));
-
-	start_pos = pos;
-
-	vfs_check_frozen(inode->i_sb, SB_FREEZE_WRITE);
-
-	mutex_lock(&inode->i_mutex);
-
-	err = generic_segment_checks(iov, &nr_segs, &ocount, VERIFY_READ);
-	if (err)
-		goto out;
-	count = ocount;
-
-	current->backing_dev_info = inode->i_mapping->backing_dev_info;
-	err = generic_write_checks(file, &pos, &count, S_ISBLK(inode->i_mode));
-	if (err)
-		goto out;
-
-	if (count == 0)
-		goto out;
-
-	err = file_remove_suid(file);
-	if (err)
-		goto out;
-
-	/*
-	 * If BTRFS flips readonly due to some impossible error
-	 * (fs_info->fs_state now has BTRFS_SUPER_FLAG_ERROR),
-	 * although we have opened a file as writable, we have
-	 * to stop this write operation to ensure FS consistency.
-	 */
-	if (root->fs_info->fs_state & BTRFS_SUPER_FLAG_ERROR) {
-		err = -EROFS;
-		goto out;
-	}
-
-	file_update_time(file);
-	BTRFS_I(inode)->sequence++;
-
-	if (unlikely(file->f_flags & O_DIRECT)) {
-		num_written = generic_file_direct_write(iocb, iov, &nr_segs,
-							pos, ppos, count,
-							ocount);
-		/*
-		 * the generic O_DIRECT will update in-memory i_size after the
-		 * DIOs are done.  But our endio handlers that update the on
-		 * disk i_size never update past the in memory i_size.  So we
-		 * need one more update here to catch any additions to the
-		 * file
-		 */
-		if (inode->i_size != BTRFS_I(inode)->disk_i_size) {
-			btrfs_ordered_update_i_size(inode, inode->i_size, NULL);
-			mark_inode_dirty(inode);
-		}
-
-		if (num_written < 0) {
-			ret = num_written;
-			num_written = 0;
-			goto out;
-		} else if (num_written == count) {
-			/* pick up pos changes done by the generic code */
-			pos = *ppos;
-			goto out;
-		}
-		/*
-		 * We are going to do buffered for the rest of the range, so we
-		 * need to make sure to invalidate the buffered pages when we're
-		 * done.
-		 */
-		buffered = 1;
-		pos += num_written;
-	}
+	size_t num_written = 0;
+	int nrptrs;
+	int ret;
 
-	iov_iter_init(&i, iov, nr_segs, count, num_written);
-	nrptrs = min((iov_iter_count(&i) + PAGE_CACHE_SIZE - 1) /
+	nrptrs = min((iov_iter_count(i) + PAGE_CACHE_SIZE - 1) /
 		     PAGE_CACHE_SIZE, PAGE_CACHE_SIZE /
 		     (sizeof(struct page *)));
 	pages = kmalloc(nrptrs * sizeof(struct page *), GFP_KERNEL);
-	if (!pages) {
-		ret = -ENOMEM;
-		goto out;
-	}
-
-	/* generic_write_checks can change our pos */
-	start_pos = pos;
+	if (!pages)
+		return -ENOMEM;
 
 	first_index = pos >> PAGE_CACHE_SHIFT;
-	last_index = (pos + iov_iter_count(&i)) >> PAGE_CACHE_SHIFT;
+	last_index = (pos + iov_iter_count(i)) >> PAGE_CACHE_SHIFT;
 
-	while (iov_iter_count(&i) > 0) {
+	while (iov_iter_count(i) > 0) {
 		size_t offset = pos & (PAGE_CACHE_SIZE - 1);
-		size_t write_bytes = min(iov_iter_count(&i),
+		size_t write_bytes = min(iov_iter_count(i),
 					 nrptrs * (size_t)PAGE_CACHE_SIZE -
 					 offset);
 		size_t num_pages = (write_bytes + offset +
 				    PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT;
+		size_t dirty_pages;
+		size_t copied;
 
 		WARN_ON(num_pages > nrptrs);
 		memset(pages, 0, sizeof(struct page *) * nrptrs);
@@ -1025,15 +936,15 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 		 * Fault pages before locking them in prepare_pages
 		 * to avoid recursive lock
 		 */
-		if (unlikely(iov_iter_fault_in_readable(&i, write_bytes))) {
+		if (unlikely(iov_iter_fault_in_readable(i, write_bytes))) {
 			ret = -EFAULT;
-			goto out;
+			break;
 		}
 
 		ret = btrfs_delalloc_reserve_space(inode,
 					num_pages << PAGE_CACHE_SHIFT);
 		if (ret)
-			goto out;
+			break;
 
 		ret = prepare_pages(root, file, pages, num_pages,
 				    pos, first_index, last_index,
@@ -1041,11 +952,11 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 		if (ret) {
 			btrfs_delalloc_release_space(inode,
 					num_pages << PAGE_CACHE_SHIFT);
-			goto out;
+			break;
 		}
 
 		copied = btrfs_copy_from_user(pos, num_pages,
-					   write_bytes, pages, &i);
+					   write_bytes, pages, i);
 
 		/*
 		 * if we have trouble faulting in the pages, fall
@@ -1061,6 +972,13 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 				       PAGE_CACHE_SIZE - 1) >>
 				       PAGE_CACHE_SHIFT;
 
+		/*
+		 * If we had a short copy we need to release the excess delaloc
+		 * bytes we reserved.  We need to increment outstanding_extents
+		 * because btrfs_delalloc_release_space will decrement it, but
+		 * we still have an outstanding extent for the chunk we actually
+		 * managed to copy.
+		 */
 		if (num_pages > dirty_pages) {
 			if (copied > 0)
 				atomic_inc(
@@ -1071,39 +989,157 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 		}
 
 		if (copied > 0) {
-			dirty_and_release_pages(NULL, root, file, pages,
-						dirty_pages, pos, copied);
+			ret = dirty_and_release_pages(root, file, pages,
+						      dirty_pages, pos,
+						      copied);
+			if (ret) {
+				btrfs_delalloc_release_space(inode,
+					dirty_pages << PAGE_CACHE_SHIFT);
+				btrfs_drop_pages(pages, num_pages);
+				break;
+			}
 		}
 
 		btrfs_drop_pages(pages, num_pages);
 
-		if (copied > 0) {
-			if (will_write) {
-				filemap_fdatawrite_range(inode->i_mapping, pos,
-							 pos + copied - 1);
-			} else {
-				balance_dirty_pages_ratelimited_nr(
-							inode->i_mapping,
-							dirty_pages);
-				if (dirty_pages <
-				(root->leafsize >> PAGE_CACHE_SHIFT) + 1)
-					btrfs_btree_balance_dirty(root, 1);
-				btrfs_throttle(root);
-			}
-		}
+		cond_resched();
+
+		balance_dirty_pages_ratelimited_nr(inode->i_mapping,
+						   dirty_pages);
+		if (dirty_pages < (root->leafsize >> PAGE_CACHE_SHIFT) + 1)
+			btrfs_btree_balance_dirty(root, 1);
+		btrfs_throttle(root);
 
 		pos += copied;
 		num_written += copied;
+	}
 
-		cond_resched();
+	kfree(pages);
+
+	return num_written ? num_written : ret;
+}
+
+static ssize_t __btrfs_direct_write(struct kiocb *iocb,
+				    const struct iovec *iov,
+				    unsigned long nr_segs, loff_t pos,
+				    loff_t *ppos, size_t count, size_t ocount)
+{
+	struct file *file = iocb->ki_filp;
+	struct inode *inode = fdentry(file)->d_inode;
+	struct iov_iter i;
+	ssize_t written;
+	ssize_t written_buffered;
+	loff_t endbyte;
+	int err;
+
+	written = generic_file_direct_write(iocb, iov, &nr_segs, pos, ppos,
+					    count, ocount);
+
+	/*
+	 * the generic O_DIRECT will update in-memory i_size after the
+	 * DIOs are done.  But our endio handlers that update the on
+	 * disk i_size never update past the in memory i_size.  So we
+	 * need one more update here to catch any additions to the
+	 * file
+	 */
+	if (inode->i_size != BTRFS_I(inode)->disk_i_size) {
+		btrfs_ordered_update_i_size(inode, inode->i_size, NULL);
+		mark_inode_dirty(inode);
 	}
+
+	if (written < 0 || written == count)
+		return written;
+
+	pos += written;
+	count -= written;
+	iov_iter_init(&i, iov, nr_segs, count, written);
+	written_buffered = __btrfs_buffered_write(file, &i, pos);
+	if (written_buffered < 0) {
+		err = written_buffered;
+		goto out;
+	}
+	endbyte = pos + written_buffered - 1;
+	err = filemap_write_and_wait_range(file->f_mapping, pos, endbyte);
+	if (err)
+		goto out;
+	written += written_buffered;
+	*ppos = pos + written_buffered;
+	invalidate_mapping_pages(file->f_mapping, pos >> PAGE_CACHE_SHIFT,
+				 endbyte >> PAGE_CACHE_SHIFT);
 out:
-	mutex_unlock(&inode->i_mutex);
-	if (ret)
-		err = ret;
+	return written ? written : err;
+}
 
-	kfree(pages);
-	*ppos = pos;
+static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
+				    const struct iovec *iov,
+				    unsigned long nr_segs, loff_t pos)
+{
+	struct file *file = iocb->ki_filp;
+	struct inode *inode = fdentry(file)->d_inode;
+	struct btrfs_root *root = BTRFS_I(inode)->root;
+	loff_t *ppos = &iocb->ki_pos;
+	ssize_t num_written = 0;
+	ssize_t err = 0;
+	size_t count, ocount;
+
+	vfs_check_frozen(inode->i_sb, SB_FREEZE_WRITE);
+
+	mutex_lock(&inode->i_mutex);
+
+	err = generic_segment_checks(iov, &nr_segs, &ocount, VERIFY_READ);
+	if (err) {
+		mutex_unlock(&inode->i_mutex);
+		goto out;
+	}
+	count = ocount;
+
+	current->backing_dev_info = inode->i_mapping->backing_dev_info;
+	err = generic_write_checks(file, &pos, &count, S_ISBLK(inode->i_mode));
+	if (err) {
+		mutex_unlock(&inode->i_mutex);
+		goto out;
+	}
+
+	if (count == 0) {
+		mutex_unlock(&inode->i_mutex);
+		goto out;
+	}
+
+	err = file_remove_suid(file);
+	if (err) {
+		mutex_unlock(&inode->i_mutex);
+		goto out;
+	}
+
+	/*
+	 * If BTRFS flips readonly due to some impossible error
+	 * (fs_info->fs_state now has BTRFS_SUPER_FLAG_ERROR),
+	 * although we have opened a file as writable, we have
+	 * to stop this write operation to ensure FS consistency.
+	 */
+	if (root->fs_info->fs_state & BTRFS_SUPER_FLAG_ERROR) {
+		mutex_unlock(&inode->i_mutex);
+		err = -EROFS;
+		goto out;
+	}
+
+	file_update_time(file);
+	BTRFS_I(inode)->sequence++;
+
+	if (unlikely(file->f_flags & O_DIRECT)) {
+		num_written = __btrfs_direct_write(iocb, iov, nr_segs,
+						   pos, ppos, count, ocount);
+	} else {
+		struct iov_iter i;
+
+		iov_iter_init(&i, iov, nr_segs, count, num_written);
+
+		num_written = __btrfs_buffered_write(file, &i, pos);
+		if (num_written > 0)
+			*ppos = pos + num_written;
+	}
+
+	mutex_unlock(&inode->i_mutex);
 
 	/*
 	 * we want to make sure fsync finds this change
@@ -1118,43 +1154,12 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 	 * one running right now.
 	 */
 	BTRFS_I(inode)->last_trans = root->fs_info->generation + 1;
-
-	if (num_written > 0 && will_write) {
-		struct btrfs_trans_handle *trans;
-
-		err = btrfs_wait_ordered_range(inode, start_pos, num_written);
-		if (err)
+	if (num_written > 0 || num_written == -EIOCBQUEUED) {
+		err = generic_write_sync(file, pos, num_written);
+		if (err < 0 && num_written > 0)
 			num_written = err;
-
-		if ((file->f_flags & O_DSYNC) || IS_SYNC(inode)) {
-			trans = btrfs_start_transaction(root, 0);
-			if (IS_ERR(trans)) {
-				num_written = PTR_ERR(trans);
-				goto done;
-			}
-			mutex_lock(&inode->i_mutex);
-			ret = btrfs_log_dentry_safe(trans, root,
-						    file->f_dentry);
-			mutex_unlock(&inode->i_mutex);
-			if (ret == 0) {
-				ret = btrfs_sync_log(trans, root);
-				if (ret == 0)
-					btrfs_end_transaction(trans, root);
-				else
-					btrfs_commit_transaction(trans, root);
-			} else if (ret != BTRFS_NO_LOG_SYNC) {
-				btrfs_commit_transaction(trans, root);
-			} else {
-				btrfs_end_transaction(trans, root);
-			}
-		}
-		if (file->f_flags & O_DIRECT && buffered) {
-			invalidate_mapping_pages(inode->i_mapping,
-			      start_pos >> PAGE_CACHE_SHIFT,
-			     (start_pos + num_written - 1) >> PAGE_CACHE_SHIFT);
-		}
 	}
-done:
+out:
 	current->backing_dev_info = NULL;
 	return num_written ? num_written : err;
 }

commit 9f570b8d48b6677b5557d86fb3ca148215e295f2
Author: Josef Bacik <josef@redhat.com>
Date:   Tue Jan 25 12:42:37 2011 -0500

    Btrfs: fix formatting in file.c
    
    Sorry, but these were bugging me.  Just cleanup some of the formatting in
    file.c.
    
    Signed-off-by: Josef Bacik <josef@redhat.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index f447b783bb84..4d4975592668 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -88,9 +88,8 @@ static noinline int btrfs_copy_from_user(loff_t pos, int num_pages,
 		total_copied += copied;
 
 		/* Return to btrfs_file_aio_write to fault page */
-		if (unlikely(copied == 0)) {
+		if (unlikely(copied == 0))
 			break;
-		}
 
 		if (unlikely(copied < PAGE_CACHE_SIZE - offset)) {
 			offset += copied;
@@ -162,13 +161,14 @@ static noinline int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 		ClearPageChecked(p);
 		set_page_dirty(p);
 	}
-	if (end_pos > isize) {
+
+	/*
+	 * we've only changed i_size in ram, and we haven't updated
+	 * the disk i_size.  There is no need to log the inode
+	 * at this time.
+	 */
+	if (end_pos > isize)
 		i_size_write(inode, end_pos);
-		/* we've only changed i_size in ram, and we haven't updated
-		 * the disk i_size.  There is no need to log the inode
-		 * at this time.
-		 */
-	}
 	return 0;
 }
 

commit 0e5b88cd9975dca6c191cc9bd11f233fac4ca882
Merge: eebea5d13d39 36e39c40b3fa
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Mar 13 16:00:49 2011 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/mason/btrfs-unstable
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/mason/btrfs-unstable:
      Btrfs: break out of shrink_delalloc earlier
      btrfs: fix not enough reserved space
      btrfs: fix dip leak
      Btrfs: make sure not to return overlapping extents to fiemap
      Btrfs: deal with short returns from copy_from_user
      Btrfs: fix regressions in copy_from_user handling

commit 31339acd07b4ba687906702085127895a56eb920
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Mar 7 11:10:24 2011 -0500

    Btrfs: deal with short returns from copy_from_user
    
    When copy_from_user is only able to copy some of the bytes we requested,
    we may end up creating a partially up to date page.  To avoid garbage in
    the page, we need to treat a partial copy as a zero length copy.
    
    This makes the rest of the file_write code drop the page and
    retry the whole copy instead of marking the partially up to
    date page as dirty.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>
    cc: stable@kernel.org

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 13664b315fe2..ab22ca4f237f 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -69,6 +69,19 @@ static noinline int btrfs_copy_from_user(loff_t pos, int num_pages,
 
 		/* Flush processor's dcache for this page */
 		flush_dcache_page(page);
+
+		/*
+		 * if we get a partial write, we can end up with
+		 * partially up to date pages.  These add
+		 * a lot of complexity, so make sure they don't
+		 * happen by forcing this copy to be retried.
+		 *
+		 * The rest of the btrfs_file_write code will fall
+		 * back to page at a time copies after we return 0.
+		 */
+		if (!PageUptodate(page) && copied < count)
+			copied = 0;
+
 		iov_iter_advance(i, copied);
 		write_bytes -= copied;
 		total_copied += copied;

commit b1bf862e9dad431175a1174379476299dbfdc017
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Feb 28 09:52:08 2011 -0500

    Btrfs: fix regressions in copy_from_user handling
    
    Commit 914ee295af418e936ec20a08c1663eaabe4cd07a fixed deadlocks in
    btrfs_file_write where we would catch page faults on pages we had
    locked.
    
    But, there were a few problems:
    
    1) The x86-32 iov_iter_copy_from_user_atomic code always fails to copy
    data when the amount to copy is more than 4K and the offset to start
    copying from is not page aligned.  The result was btrfs_file_write
    looping forever retrying the iov_iter_copy_from_user_atomic
    
    We deal with this by changing btrfs_file_write to drop down to single
    page copies when iov_iter_copy_from_user_atomic starts returning failure.
    
    2) The btrfs_file_write code was leaking delalloc reservations when
    iov_iter_copy_from_user_atomic returned zero.  The looping above would
    result in the entire filesystem running out of delalloc reservations and
    constantly trying to flush things to disk.
    
    3) btrfs_file_write will lock down page cache pages, make sure
    any writeback is finished, do the copy_from_user and then release them.
    Before the loop runs we check the first and last pages in the write to
    see if they are only being partially modified.  If the start or end of
    the write isn't aligned, we make sure the corresponding pages are
    up to date so that we don't introduce garbage into the file.
    
    With the copy_from_user changes, we're allowing the VM to reclaim the
    pages after a partial update from copy_from_user, but we're not
    making sure the page cache page is up to date when we loop around to
    resume the write.
    
    We deal with this by pushing the up to date checks down into the page
    prep code.  This fits better with how the rest of file_write works.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>
    Reported-by: Mitch Harder <mitch.harder@sabayonlinux.org>
    cc: stable@kernel.org

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 65338a1d14ad..13664b315fe2 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -761,6 +761,27 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 	return 0;
 }
 
+/*
+ * on error we return an unlocked page and the error value
+ * on success we return a locked page and 0
+ */
+static int prepare_uptodate_page(struct page *page, u64 pos)
+{
+	int ret = 0;
+
+	if ((pos & (PAGE_CACHE_SIZE - 1)) && !PageUptodate(page)) {
+		ret = btrfs_readpage(NULL, page);
+		if (ret)
+			return ret;
+		lock_page(page);
+		if (!PageUptodate(page)) {
+			unlock_page(page);
+			return -EIO;
+		}
+	}
+	return 0;
+}
+
 /*
  * this gets pages into the page cache and locks them down, it also properly
  * waits for data=ordered extents to finish before allowing the pages to be
@@ -776,6 +797,7 @@ static noinline int prepare_pages(struct btrfs_root *root, struct file *file,
 	unsigned long index = pos >> PAGE_CACHE_SHIFT;
 	struct inode *inode = fdentry(file)->d_inode;
 	int err = 0;
+	int faili = 0;
 	u64 start_pos;
 	u64 last_pos;
 
@@ -793,15 +815,24 @@ static noinline int prepare_pages(struct btrfs_root *root, struct file *file,
 	for (i = 0; i < num_pages; i++) {
 		pages[i] = grab_cache_page(inode->i_mapping, index + i);
 		if (!pages[i]) {
-			int c;
-			for (c = i - 1; c >= 0; c--) {
-				unlock_page(pages[c]);
-				page_cache_release(pages[c]);
-			}
-			return -ENOMEM;
+			faili = i - 1;
+			err = -ENOMEM;
+			goto fail;
+		}
+
+		if (i == 0)
+			err = prepare_uptodate_page(pages[i], pos);
+		if (i == num_pages - 1)
+			err = prepare_uptodate_page(pages[i],
+						    pos + write_bytes);
+		if (err) {
+			page_cache_release(pages[i]);
+			faili = i - 1;
+			goto fail;
 		}
 		wait_on_page_writeback(pages[i]);
 	}
+	err = 0;
 	if (start_pos < inode->i_size) {
 		struct btrfs_ordered_extent *ordered;
 		lock_extent_bits(&BTRFS_I(inode)->io_tree,
@@ -841,6 +872,14 @@ static noinline int prepare_pages(struct btrfs_root *root, struct file *file,
 		WARN_ON(!PageLocked(pages[i]));
 	}
 	return 0;
+fail:
+	while (faili >= 0) {
+		unlock_page(pages[faili]);
+		page_cache_release(pages[faili]);
+		faili--;
+	}
+	return err;
+
 }
 
 static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
@@ -850,7 +889,6 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 	struct file *file = iocb->ki_filp;
 	struct inode *inode = fdentry(file)->d_inode;
 	struct btrfs_root *root = BTRFS_I(inode)->root;
-	struct page *pinned[2];
 	struct page **pages = NULL;
 	struct iov_iter i;
 	loff_t *ppos = &iocb->ki_pos;
@@ -871,9 +909,6 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 	will_write = ((file->f_flags & O_DSYNC) || IS_SYNC(inode) ||
 		      (file->f_flags & O_DIRECT));
 
-	pinned[0] = NULL;
-	pinned[1] = NULL;
-
 	start_pos = pos;
 
 	vfs_check_frozen(inode->i_sb, SB_FREEZE_WRITE);
@@ -961,32 +996,6 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 	first_index = pos >> PAGE_CACHE_SHIFT;
 	last_index = (pos + iov_iter_count(&i)) >> PAGE_CACHE_SHIFT;
 
-	/*
-	 * there are lots of better ways to do this, but this code
-	 * makes sure the first and last page in the file range are
-	 * up to date and ready for cow
-	 */
-	if ((pos & (PAGE_CACHE_SIZE - 1))) {
-		pinned[0] = grab_cache_page(inode->i_mapping, first_index);
-		if (!PageUptodate(pinned[0])) {
-			ret = btrfs_readpage(NULL, pinned[0]);
-			BUG_ON(ret);
-			wait_on_page_locked(pinned[0]);
-		} else {
-			unlock_page(pinned[0]);
-		}
-	}
-	if ((pos + iov_iter_count(&i)) & (PAGE_CACHE_SIZE - 1)) {
-		pinned[1] = grab_cache_page(inode->i_mapping, last_index);
-		if (!PageUptodate(pinned[1])) {
-			ret = btrfs_readpage(NULL, pinned[1]);
-			BUG_ON(ret);
-			wait_on_page_locked(pinned[1]);
-		} else {
-			unlock_page(pinned[1]);
-		}
-	}
-
 	while (iov_iter_count(&i) > 0) {
 		size_t offset = pos & (PAGE_CACHE_SIZE - 1);
 		size_t write_bytes = min(iov_iter_count(&i),
@@ -1023,8 +1032,20 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 
 		copied = btrfs_copy_from_user(pos, num_pages,
 					   write_bytes, pages, &i);
-		dirty_pages = (copied + offset + PAGE_CACHE_SIZE - 1) >>
-				PAGE_CACHE_SHIFT;
+
+		/*
+		 * if we have trouble faulting in the pages, fall
+		 * back to one page at a time
+		 */
+		if (copied < write_bytes)
+			nrptrs = 1;
+
+		if (copied == 0)
+			dirty_pages = 0;
+		else
+			dirty_pages = (copied + offset +
+				       PAGE_CACHE_SIZE - 1) >>
+				       PAGE_CACHE_SHIFT;
 
 		if (num_pages > dirty_pages) {
 			if (copied > 0)
@@ -1068,10 +1089,6 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 		err = ret;
 
 	kfree(pages);
-	if (pinned[0])
-		page_cache_release(pinned[0]);
-	if (pinned[1])
-		page_cache_release(pinned[1]);
 	*ppos = pos;
 
 	/*

commit 007a14af2649c9ac77f38cd23469518ffb8b355a
Merge: 261cd298a8c3 c26a920373a9
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Feb 15 08:00:35 2011 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/mason/btrfs-unstable
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/mason/btrfs-unstable:
      Btrfs: check return value of alloc_extent_map()
      Btrfs - Fix memory leak in btrfs_init_new_device()
      btrfs: prevent heap corruption in btrfs_ioctl_space_info()
      Btrfs: Fix balance panic
      Btrfs: don't release pages when we can't clear the uptodate bits
      Btrfs: fix page->private races

commit c26a920373a983b52223eed5a13b97404d8b4158
Author: Tsutomu Itoh <t-itoh@jp.fujitsu.com>
Date:   Mon Feb 14 00:45:29 2011 +0000

    Btrfs: check return value of alloc_extent_map()
    
    I add the check on the return value of alloc_extent_map() to several places.
    In addition, alloc_extent_map() returns only the address or NULL.
    Therefore, check by IS_ERR() is unnecessary. So, I remove IS_ERR() checking.
    
    Signed-off-by: Tsutomu Itoh <t-itoh@jp.fujitsu.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index b0ff34b96607..65338a1d14ad 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -185,6 +185,7 @@ int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
 			split = alloc_extent_map(GFP_NOFS);
 		if (!split2)
 			split2 = alloc_extent_map(GFP_NOFS);
+		BUG_ON(!split || !split2);
 
 		write_lock(&em_tree->lock);
 		em = lookup_extent_mapping(em_tree, start, len);

commit cb5520f02c010e3cb974b9ac06f30aafa2eebc38
Merge: eee4da2cef8e 3a90983dbdcb
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Feb 7 14:06:18 2011 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/mason/btrfs-unstable
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/mason/btrfs-unstable: (33 commits)
      Btrfs: Fix page count calculation
      btrfs: Drop __exit attribute on btrfs_exit_compress
      btrfs: cleanup error handling in btrfs_unlink_inode()
      Btrfs: exclude super blocks when we read in block groups
      Btrfs: make sure search_bitmap finds something in remove_from_bitmap
      btrfs: fix return value check of btrfs_start_transaction()
      btrfs: checking NULL or not in some functions
      Btrfs: avoid uninit variable warnings in ordered-data.c
      Btrfs: catch errors from btrfs_sync_log
      Btrfs: make shrink_delalloc a little friendlier
      Btrfs: handle no memory properly in prepare_pages
      Btrfs: do error checking in btrfs_del_csums
      Btrfs: use the global block reserve if we cannot reserve space
      Btrfs: do not release more reserved bytes to the global_block_rsv than we need
      Btrfs: fix check_path_shared so it returns the right value
      btrfs: check return value of btrfs_start_ioctl_transaction() properly
      btrfs: fix return value check of btrfs_join_transaction()
      fs/btrfs/inode.c: Add missing IS_ERR test
      btrfs: fix missing break in switch phrase
      btrfs: fix several uncheck memory allocations
      ...

commit 3a90983dbdcb2f4f48c0d771d8e5b4d88f27fae6
Author: Yan, Zheng <zheng.z.yan@linux.intel.com>
Date:   Tue Jan 18 13:34:40 2011 +0800

    Btrfs: Fix page count calculation
    
    take offset of start position into account when calculating page count.
    
    Signed-off-by: Yan, Zheng <zheng.z.yan@intel.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 9e097fbfc78d..b0ff34b96607 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -991,8 +991,8 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 		size_t write_bytes = min(iov_iter_count(&i),
 					 nrptrs * (size_t)PAGE_CACHE_SIZE -
 					 offset);
-		size_t num_pages = (write_bytes + PAGE_CACHE_SIZE - 1) >>
-					PAGE_CACHE_SHIFT;
+		size_t num_pages = (write_bytes + offset +
+				    PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT;
 
 		WARN_ON(num_pages > nrptrs);
 		memset(pages, 0, sizeof(struct page *) * nrptrs);
@@ -1022,8 +1022,8 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 
 		copied = btrfs_copy_from_user(pos, num_pages,
 					   write_bytes, pages, &i);
-		dirty_pages = (copied + PAGE_CACHE_SIZE - 1) >>
-					PAGE_CACHE_SHIFT;
+		dirty_pages = (copied + offset + PAGE_CACHE_SIZE - 1) >>
+				PAGE_CACHE_SHIFT;
 
 		if (num_pages > dirty_pages) {
 			if (copied > 0)

commit 7adf5dfbb3af65a00e20b3ead224c3a1b40e4ec4
Author: Josef Bacik <josef@redhat.com>
Date:   Tue Jan 25 22:11:54 2011 +0000

    Btrfs: handle no memory properly in prepare_pages
    
    Instead of doing a BUG_ON(1) in prepare_pages if grab_cache_page() fails, just
    loop through the pages we've already grabbed and unlock and release them, then
    return -ENOMEM like we should.  Thanks,
    
    Signed-off-by: Josef Bacik <josef@redhat.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 65b2424a4116..9e097fbfc78d 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -792,8 +792,12 @@ static noinline int prepare_pages(struct btrfs_root *root, struct file *file,
 	for (i = 0; i < num_pages; i++) {
 		pages[i] = grab_cache_page(inode->i_mapping, index + i);
 		if (!pages[i]) {
-			err = -ENOMEM;
-			BUG_ON(1);
+			int c;
+			for (c = i - 1; c >= 0; c--) {
+				unlock_page(pages[c]);
+				page_cache_release(pages[c]);
+			}
+			return -ENOMEM;
 		}
 		wait_on_page_writeback(pages[i]);
 	}

commit 2a29edc6b60a5248ccab588e7ba7dad38cef0235
Author: liubo <liubo2009@cn.fujitsu.com>
Date:   Wed Jan 26 06:22:08 2011 +0000

    btrfs: fix several uncheck memory allocations
    
    To make btrfs more stable, add several missing necessary memory allocation
    checks, and when no memory, return proper errno.
    
    We've checked that some of those -ENOMEM errors will be returned to
    userspace, and some will be catched by BUG_ON() in the upper callers,
    and none will be ignored silently.
    
    Signed-off-by: Liu Bo <liubo2009@cn.fujitsu.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index f903433f5bdf..65b2424a4116 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -945,6 +945,10 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 		     PAGE_CACHE_SIZE, PAGE_CACHE_SIZE /
 		     (sizeof(struct page *)));
 	pages = kmalloc(nrptrs * sizeof(struct page *), GFP_KERNEL);
+	if (!pages) {
+		ret = -ENOMEM;
+		goto out;
+	}
 
 	/* generic_write_checks can change our pos */
 	start_pos = pos;

commit eee2a817df7c5a6e569f353f8be78cc1b3604bb6
Merge: 83896fb5e515 acce952b0263
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jan 17 14:43:43 2011 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/btrfs-unstable
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/btrfs-unstable: (25 commits)
      Btrfs: forced readonly mounts on errors
      btrfs: Require CAP_SYS_ADMIN for filesystem rebalance
      Btrfs: don't warn if we get ENOSPC in btrfs_block_rsv_check
      btrfs: Fix memory leak in btrfs_read_fs_root_no_radix()
      btrfs: check NULL or not
      btrfs: Don't pass NULL ptr to func that may deref it.
      btrfs: mount failure return value fix
      btrfs: Mem leak in btrfs_get_acl()
      btrfs: fix wrong free space information of btrfs
      btrfs: make the chunk allocator utilize the devices better
      btrfs: restructure find_free_dev_extent()
      btrfs: fix wrong calculation of stripe size
      btrfs: try to reclaim some space when chunk allocation fails
      btrfs: fix wrong data space statistics
      fs/btrfs: Fix build of ctree
      Btrfs: fix off by one while setting block groups readonly
      Btrfs: Add BTRFS_IOC_SUBVOL_GETFLAGS/SETFLAGS ioctls
      Btrfs: Add readonly snapshots support
      Btrfs: Refactor btrfs_ioctl_snap_create()
      btrfs: Extract duplicate decompress code
      ...

commit acce952b0263825da32cf10489413dec78053347
Author: liubo <liubo2009@cn.fujitsu.com>
Date:   Thu Jan 6 19:30:25 2011 +0800

    Btrfs: forced readonly mounts on errors
    
    This patch comes from "Forced readonly mounts on errors" ideas.
    
    As we know, this is the first step in being more fault tolerant of disk
    corruptions instead of just using BUG() statements.
    
    The major content:
    - add a framework for generating errors that should result in filesystems
      going readonly.
    - keep FS state in disk super block.
    - make sure that all of resource will be freed and released at umount time.
    - make sure that fter FS is forced readonly on error, there will be no more
      disk change before FS is corrected. For this, we should stop write operation.
    
    After this patch is applied, the conversion from BUG() to such a framework can
    happen incrementally.
    
    Signed-off-by: Liu Bo <liubo2009@cn.fujitsu.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 05df688c96f4..f903433f5bdf 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -892,6 +892,17 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 	if (err)
 		goto out;
 
+	/*
+	 * If BTRFS flips readonly due to some impossible error
+	 * (fs_info->fs_state now has BTRFS_SUPER_FLAG_ERROR),
+	 * although we have opened a file as writable, we have
+	 * to stop this write operation to ensure FS consistency.
+	 */
+	if (root->fs_info->fs_state & BTRFS_SUPER_FLAG_ERROR) {
+		err = -EROFS;
+		goto out;
+	}
+
 	file_update_time(file);
 	BTRFS_I(inode)->sequence++;
 

commit 2fe17c1075836b66678ed2a305fd09b6773883aa
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Jan 14 13:07:43 2011 +0100

    fallocate should be a file operation
    
    Currently all filesystems except XFS implement fallocate asynchronously,
    while XFS forced a commit.  Both of these are suboptimal - in case of O_SYNC
    I/O we really want our allocation on disk, especially for the !KEEP_SIZE
    case where we actually grow the file with user-visible zeroes.  On the
    other hand always commiting the transaction is a bad idea for fast-path
    uses of fallocate like for example in recent Samba versions.   Given
    that block allocation is a data plane operation anyway change it from
    an inode operation to a file operation so that we have the file structure
    available that lets us check for O_SYNC.
    
    This also includes moving the code around for a few of the filesystems,
    and remove the already unnedded S_ISDIR checks given that we only wire
    up fallocate for regular files.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 66836d85763b..a9e0a4eaf3d9 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -24,6 +24,7 @@
 #include <linux/string.h>
 #include <linux/backing-dev.h>
 #include <linux/mpage.h>
+#include <linux/falloc.h>
 #include <linux/swap.h>
 #include <linux/writeback.h>
 #include <linux/statfs.h>
@@ -1237,6 +1238,117 @@ static int btrfs_file_mmap(struct file	*filp, struct vm_area_struct *vma)
 	return 0;
 }
 
+static long btrfs_fallocate(struct file *file, int mode,
+			    loff_t offset, loff_t len)
+{
+	struct inode *inode = file->f_path.dentry->d_inode;
+	struct extent_state *cached_state = NULL;
+	u64 cur_offset;
+	u64 last_byte;
+	u64 alloc_start;
+	u64 alloc_end;
+	u64 alloc_hint = 0;
+	u64 locked_end;
+	u64 mask = BTRFS_I(inode)->root->sectorsize - 1;
+	struct extent_map *em;
+	int ret;
+
+	alloc_start = offset & ~mask;
+	alloc_end =  (offset + len + mask) & ~mask;
+
+	/* We only support the FALLOC_FL_KEEP_SIZE mode */
+	if (mode & ~FALLOC_FL_KEEP_SIZE)
+		return -EOPNOTSUPP;
+
+	/*
+	 * wait for ordered IO before we have any locks.  We'll loop again
+	 * below with the locks held.
+	 */
+	btrfs_wait_ordered_range(inode, alloc_start, alloc_end - alloc_start);
+
+	mutex_lock(&inode->i_mutex);
+	ret = inode_newsize_ok(inode, alloc_end);
+	if (ret)
+		goto out;
+
+	if (alloc_start > inode->i_size) {
+		ret = btrfs_cont_expand(inode, alloc_start);
+		if (ret)
+			goto out;
+	}
+
+	ret = btrfs_check_data_free_space(inode, alloc_end - alloc_start);
+	if (ret)
+		goto out;
+
+	locked_end = alloc_end - 1;
+	while (1) {
+		struct btrfs_ordered_extent *ordered;
+
+		/* the extent lock is ordered inside the running
+		 * transaction
+		 */
+		lock_extent_bits(&BTRFS_I(inode)->io_tree, alloc_start,
+				 locked_end, 0, &cached_state, GFP_NOFS);
+		ordered = btrfs_lookup_first_ordered_extent(inode,
+							    alloc_end - 1);
+		if (ordered &&
+		    ordered->file_offset + ordered->len > alloc_start &&
+		    ordered->file_offset < alloc_end) {
+			btrfs_put_ordered_extent(ordered);
+			unlock_extent_cached(&BTRFS_I(inode)->io_tree,
+					     alloc_start, locked_end,
+					     &cached_state, GFP_NOFS);
+			/*
+			 * we can't wait on the range with the transaction
+			 * running or with the extent lock held
+			 */
+			btrfs_wait_ordered_range(inode, alloc_start,
+						 alloc_end - alloc_start);
+		} else {
+			if (ordered)
+				btrfs_put_ordered_extent(ordered);
+			break;
+		}
+	}
+
+	cur_offset = alloc_start;
+	while (1) {
+		em = btrfs_get_extent(inode, NULL, 0, cur_offset,
+				      alloc_end - cur_offset, 0);
+		BUG_ON(IS_ERR(em) || !em);
+		last_byte = min(extent_map_end(em), alloc_end);
+		last_byte = (last_byte + mask) & ~mask;
+		if (em->block_start == EXTENT_MAP_HOLE ||
+		    (cur_offset >= inode->i_size &&
+		     !test_bit(EXTENT_FLAG_PREALLOC, &em->flags))) {
+			ret = btrfs_prealloc_file_range(inode, mode, cur_offset,
+							last_byte - cur_offset,
+							1 << inode->i_blkbits,
+							offset + len,
+							&alloc_hint);
+			if (ret < 0) {
+				free_extent_map(em);
+				break;
+			}
+		}
+		free_extent_map(em);
+
+		cur_offset = last_byte;
+		if (cur_offset >= alloc_end) {
+			ret = 0;
+			break;
+		}
+	}
+	unlock_extent_cached(&BTRFS_I(inode)->io_tree, alloc_start, locked_end,
+			     &cached_state, GFP_NOFS);
+
+	btrfs_free_reserved_data_space(inode, alloc_end - alloc_start);
+out:
+	mutex_unlock(&inode->i_mutex);
+	return ret;
+}
+
 const struct file_operations btrfs_file_operations = {
 	.llseek		= generic_file_llseek,
 	.read		= do_sync_read,
@@ -1248,6 +1360,7 @@ const struct file_operations btrfs_file_operations = {
 	.open		= generic_file_open,
 	.release	= btrfs_release_file,
 	.fsync		= btrfs_sync_file,
+	.fallocate	= btrfs_fallocate,
 	.unlocked_ioctl	= btrfs_ioctl,
 #ifdef CONFIG_COMPAT
 	.compat_ioctl	= btrfs_ioctl,

commit 261507a02ccba9afda919852263b6bc1581ce1ef
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Fri Dec 17 14:21:50 2010 +0800

    btrfs: Allow to add new compression algorithm
    
    Make the code aware of compression type, instead of always assuming
    zlib compression.
    
    Also make the zlib workspace function as common code for all
    compression types.
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 66836d85763b..05df688c96f4 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -224,6 +224,7 @@ int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
 
 			split->bdev = em->bdev;
 			split->flags = flags;
+			split->compress_type = em->compress_type;
 			ret = add_extent_mapping(em_tree, split);
 			BUG_ON(ret);
 			free_extent_map(split);
@@ -238,6 +239,7 @@ int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
 			split->len = em->start + em->len - (start + len);
 			split->bdev = em->bdev;
 			split->flags = flags;
+			split->compress_type = em->compress_type;
 
 			if (compressed) {
 				split->block_len = em->block_len;

commit 914ee295af418e936ec20a08c1663eaabe4cd07a
Author: Xin Zhong <xin.zhong@intel.com>
Date:   Thu Dec 9 09:30:14 2010 +0000

    Btrfs: pwrite blocked when writing from the mmaped buffer of the same page
    
    This problem is found in meego testing:
    http://bugs.meego.com/show_bug.cgi?id=6672
    A file in btrfs is mmaped and the mmaped buffer is passed to pwrite to write to the same page
    of the same file. In btrfs_file_aio_write(), the pages is locked by prepare_pages(). So when
    btrfs_copy_from_user() is called, page fault happens and the same page needs to be locked again
    in filemap_fault(). The fix is to move iov_iter_fault_in_readable() before prepage_pages() to make page
    fault happen before pages are locked. And also disable page fault in critical region in
    btrfs_copy_from_user().
    
    Reviewed-by: Yan, Zheng<zheng.z.yan@intel.com>
    Signed-off-by: Zhong, Xin <xin.zhong@intel.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index c1faded5fca0..66836d85763b 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -48,30 +48,34 @@ static noinline int btrfs_copy_from_user(loff_t pos, int num_pages,
 					 struct page **prepared_pages,
 					 struct iov_iter *i)
 {
-	size_t copied;
+	size_t copied = 0;
 	int pg = 0;
 	int offset = pos & (PAGE_CACHE_SIZE - 1);
+	int total_copied = 0;
 
 	while (write_bytes > 0) {
 		size_t count = min_t(size_t,
 				     PAGE_CACHE_SIZE - offset, write_bytes);
 		struct page *page = prepared_pages[pg];
-again:
-		if (unlikely(iov_iter_fault_in_readable(i, count)))
-			return -EFAULT;
-
-		/* Copy data from userspace to the current page */
-		copied = iov_iter_copy_from_user(page, i, offset, count);
+		/*
+		 * Copy data from userspace to the current page
+		 *
+		 * Disable pagefault to avoid recursive lock since
+		 * the pages are already locked
+		 */
+		pagefault_disable();
+		copied = iov_iter_copy_from_user_atomic(page, i, offset, count);
+		pagefault_enable();
 
 		/* Flush processor's dcache for this page */
 		flush_dcache_page(page);
 		iov_iter_advance(i, copied);
 		write_bytes -= copied;
+		total_copied += copied;
 
+		/* Return to btrfs_file_aio_write to fault page */
 		if (unlikely(copied == 0)) {
-			count = min_t(size_t, PAGE_CACHE_SIZE - offset,
-				      iov_iter_single_seg_count(i));
-			goto again;
+			break;
 		}
 
 		if (unlikely(copied < PAGE_CACHE_SIZE - offset)) {
@@ -81,7 +85,7 @@ static noinline int btrfs_copy_from_user(loff_t pos, int num_pages,
 			offset = 0;
 		}
 	}
-	return 0;
+	return total_copied;
 }
 
 /*
@@ -854,6 +858,8 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 	unsigned long last_index;
 	int will_write;
 	int buffered = 0;
+	int copied = 0;
+	int dirty_pages = 0;
 
 	will_write = ((file->f_flags & O_DSYNC) || IS_SYNC(inode) ||
 		      (file->f_flags & O_DIRECT));
@@ -970,7 +976,17 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 		WARN_ON(num_pages > nrptrs);
 		memset(pages, 0, sizeof(struct page *) * nrptrs);
 
-		ret = btrfs_delalloc_reserve_space(inode, write_bytes);
+		/*
+		 * Fault pages before locking them in prepare_pages
+		 * to avoid recursive lock
+		 */
+		if (unlikely(iov_iter_fault_in_readable(&i, write_bytes))) {
+			ret = -EFAULT;
+			goto out;
+		}
+
+		ret = btrfs_delalloc_reserve_space(inode,
+					num_pages << PAGE_CACHE_SHIFT);
 		if (ret)
 			goto out;
 
@@ -978,37 +994,49 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 				    pos, first_index, last_index,
 				    write_bytes);
 		if (ret) {
-			btrfs_delalloc_release_space(inode, write_bytes);
+			btrfs_delalloc_release_space(inode,
+					num_pages << PAGE_CACHE_SHIFT);
 			goto out;
 		}
 
-		ret = btrfs_copy_from_user(pos, num_pages,
+		copied = btrfs_copy_from_user(pos, num_pages,
 					   write_bytes, pages, &i);
-		if (ret == 0) {
+		dirty_pages = (copied + PAGE_CACHE_SIZE - 1) >>
+					PAGE_CACHE_SHIFT;
+
+		if (num_pages > dirty_pages) {
+			if (copied > 0)
+				atomic_inc(
+					&BTRFS_I(inode)->outstanding_extents);
+			btrfs_delalloc_release_space(inode,
+					(num_pages - dirty_pages) <<
+					PAGE_CACHE_SHIFT);
+		}
+
+		if (copied > 0) {
 			dirty_and_release_pages(NULL, root, file, pages,
-						num_pages, pos, write_bytes);
+						dirty_pages, pos, copied);
 		}
 
 		btrfs_drop_pages(pages, num_pages);
-		if (ret) {
-			btrfs_delalloc_release_space(inode, write_bytes);
-			goto out;
-		}
 
-		if (will_write) {
-			filemap_fdatawrite_range(inode->i_mapping, pos,
-						 pos + write_bytes - 1);
-		} else {
-			balance_dirty_pages_ratelimited_nr(inode->i_mapping,
-							   num_pages);
-			if (num_pages <
-			    (root->leafsize >> PAGE_CACHE_SHIFT) + 1)
-				btrfs_btree_balance_dirty(root, 1);
-			btrfs_throttle(root);
+		if (copied > 0) {
+			if (will_write) {
+				filemap_fdatawrite_range(inode->i_mapping, pos,
+							 pos + copied - 1);
+			} else {
+				balance_dirty_pages_ratelimited_nr(
+							inode->i_mapping,
+							dirty_pages);
+				if (dirty_pages <
+				(root->leafsize >> PAGE_CACHE_SHIFT) + 1)
+					btrfs_btree_balance_dirty(root, 1);
+				btrfs_throttle(root);
+			}
 		}
 
-		pos += write_bytes;
-		num_written += write_bytes;
+		pos += copied;
+		num_written += copied;
 
 		cond_resched();
 	}

commit 495e86779f4f319828bc10dfc0c9ac2161868077
Author: Josef Bacik <josef@redhat.com>
Date:   Fri Nov 19 20:36:10 2010 +0000

    Btrfs: hold i_mutex when calling btrfs_log_dentry_safe
    
    Since we walk up the path logging all of the parts of the inode's path, we need
    to hold i_mutex to make sure that the inode is not renamed while we're logging
    everything.  btrfs_log_dentry_safe does dget_parent and all of that jazz, but we
    may get unexpected results if the rename changes the inode's location while
    we're higher up the path logging those dentries, so do this for safety reasons.
    Thanks,
    
    Signed-off-by: Josef Bacik <josef@redhat.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index e354c33df082..c1faded5fca0 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1047,8 +1047,14 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 
 		if ((file->f_flags & O_DSYNC) || IS_SYNC(inode)) {
 			trans = btrfs_start_transaction(root, 0);
+			if (IS_ERR(trans)) {
+				num_written = PTR_ERR(trans);
+				goto done;
+			}
+			mutex_lock(&inode->i_mutex);
 			ret = btrfs_log_dentry_safe(trans, root,
 						    file->f_dentry);
+			mutex_unlock(&inode->i_mutex);
 			if (ret == 0) {
 				ret = btrfs_sync_log(trans, root);
 				if (ret == 0)
@@ -1067,6 +1073,7 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 			     (start_pos + num_written - 1) >> PAGE_CACHE_SHIFT);
 		}
 	}
+done:
 	current->backing_dev_info = NULL;
 	return num_written ? num_written : err;
 }

commit b25b550bb153626df6a48eb8583e923e3dfcf64a
Merge: eda054770e5c 6f902af400b2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jun 11 14:18:47 2010 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/btrfs-unstable
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/btrfs-unstable:
      Btrfs: The file argument for fsync() is never null
      Btrfs: handle ERR_PTR from posix_acl_from_xattr()
      Btrfs: avoid BUG when dropping root and reference in same transaction
      Btrfs: prohibit a operation of changing acl's mask when noacl mount option used
      Btrfs: should add a permission check for setfacl
      Btrfs: btrfs_lookup_dir_item() can return ERR_PTR
      Btrfs: btrfs_read_fs_root_no_name() returns ERR_PTRs
      Btrfs: unwind after btrfs_start_transaction() errors
      Btrfs: btrfs_iget() returns ERR_PTR
      Btrfs: handle kzalloc() failure in open_ctree()
      Btrfs: handle error returns from btrfs_lookup_dir_item()
      Btrfs: Fix BUG_ON for fs converted from extN
      Btrfs: Fix null dereference in relocation.c
      Btrfs: fix remap_file_pages error
      Btrfs: uninitialized data is check_path_shared()
      Btrfs: fix fallocate regression
      Btrfs: fix loop device on top of btrfs

commit 6f902af400b2499c80865c62a06fbbd15cf804fd
Author: Dan Carpenter <error27@gmail.com>
Date:   Sat May 29 09:49:07 2010 +0000

    Btrfs: The file argument for fsync() is never null
    
    The "file" argument for fsync is never null so we can remove this check.
    
    What drew my attention here is that 7ea8085910e: "drop unused dentry
    argument to ->fsync" introduced an unconditional dereference at the
    start of the function and that generated a smatch warning.
    
    Signed-off-by: Dan Carpenter <error27@gmail.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index ce0cd29efa9e..7f29464c0ebf 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1139,7 +1139,7 @@ int btrfs_sync_file(struct file *file, struct dentry *dentry, int datasync)
 	/*
 	 * ok we haven't committed the transaction yet, lets do a commit
 	 */
-	if (file && file->private_data)
+	if (file->private_data)
 		btrfs_ioctl_trans_end(file);
 
 	trans = btrfs_start_transaction(root, 0);

commit 058a457ef0ce28d595af53d6103db73332383cbc
Author: Miao Xie <miaox@cn.fujitsu.com>
Date:   Thu May 20 07:21:50 2010 +0000

    Btrfs: fix remap_file_pages error
    
    when we use remap_file_pages() to remap a file, remap_file_pages always return
    error. It is because btrfs didn't set VM_CAN_NONLINEAR for vma.
    
    Signed-off-by: Miao Xie <miaox@cn.fujitsu.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index abcb91867b56..ce0cd29efa9e 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1189,8 +1189,15 @@ static const struct vm_operations_struct btrfs_file_vm_ops = {
 
 static int btrfs_file_mmap(struct file	*filp, struct vm_area_struct *vma)
 {
-	vma->vm_ops = &btrfs_file_vm_ops;
+	struct address_space *mapping = filp->f_mapping;
+
+	if (!mapping->a_ops->readpage)
+		return -ENOEXEC;
+
 	file_accessed(filp);
+	vma->vm_ops = &btrfs_file_vm_ops;
+	vma->vm_flags |= VM_CAN_NONLINEAR;
+
 	return 0;
 }
 

commit 4a001071d3549f596c7c3736c5dda8a3a4aba9ed
Author: Miao Xie <miaox@cn.fujitsu.com>
Date:   Mon Jun 7 03:38:51 2010 +0000

    Btrfs: fix loop device on top of btrfs
    
    We cannot use the loop device which has been connected to a file in the btrf
    
    The reproduce steps is following:
     # dd if=/dev/zero of=vdev0 bs=1M count=1024
     # losetup /dev/loop0 vdev0
     # mkfs.btrfs /dev/loop0
     ...
     failed to zero device start -5
    
    The reason is that the btrfs don't implement either ->write_begin or ->write
    the VFS API, so we fix it by setting ->write to do_sync_write().
    
    Signed-off-by: Miao Xie <miaox@cn.fujitsu.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 79437c5eeb1e..abcb91867b56 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1197,6 +1197,7 @@ static int btrfs_file_mmap(struct file	*filp, struct vm_area_struct *vma)
 const struct file_operations btrfs_file_operations = {
 	.llseek		= generic_file_llseek,
 	.read		= do_sync_read,
+	.write		= do_sync_write,
 	.aio_read       = generic_file_aio_read,
 	.splice_read	= generic_file_splice_read,
 	.aio_write	= btrfs_file_aio_write,

commit 7ea8085910ef3dd4f3cad6845aaa2b580d39b115
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed May 26 17:53:25 2010 +0200

    drop unused dentry argument to ->fsync
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 79437c5eeb1e..787b50a16a14 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1101,8 +1101,9 @@ int btrfs_release_file(struct inode *inode, struct file *filp)
  * important optimization for directories because holding the mutex prevents
  * new operations on the dir while we write to disk.
  */
-int btrfs_sync_file(struct file *file, struct dentry *dentry, int datasync)
+int btrfs_sync_file(struct file *file, int datasync)
 {
+	struct dentry *dentry = file->f_path.dentry;
 	struct inode *inode = dentry->d_inode;
 	struct btrfs_root *root = BTRFS_I(inode)->root;
 	int ret = 0;

commit 3f7c579c41a3d20af76fd6ff1f6b949edf105fd1
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed May 26 10:59:53 2010 -0400

    Btrfs: move O_DIRECT space reservation to btrfs_direct_IO
    
    This moves the delalloc space reservation done for O_DIRECT
    into btrfs_direct_IO.  This way we don't leak reserved space
    if the generic O_DIRECT write code errors out before it
    calls into btrfs_direct_IO.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 54556cae4497..79437c5eeb1e 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -888,14 +888,9 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 	BTRFS_I(inode)->sequence++;
 
 	if (unlikely(file->f_flags & O_DIRECT)) {
-		ret = btrfs_delalloc_reserve_space(inode, count);
-		if (ret)
-			goto out;
-
 		num_written = generic_file_direct_write(iocb, iov, &nr_segs,
 							pos, ppos, count,
 							ocount);
-
 		/*
 		 * the generic O_DIRECT will update in-memory i_size after the
 		 * DIOs are done.  But our endio handlers that update the on

commit 4845e44ffdb26be9b25610664228e8ecaf949a0d
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue May 25 20:56:50 2010 -0400

    Btrfs: rework O_DIRECT enospc handling
    
    This changes O_DIRECT write code to mark extents as delalloc
    while it is processing them.  Yan Zheng has reworked the
    enospc accounting based on tracking delalloc extents and
    this makes it much easier to track enospc in the O_DIRECT code.
    
    There are a few space cases with the O_DIRECT code though,
    it only sets the EXTENT_DELALLOC bits, instead of doing
    EXTENT_DELALLOC | EXTENT_DIRTY | EXTENT_UPTODATE, because
    we don't want to mess with clearing the dirty and uptodate
    bits when things go wrong.  This is important because there
    are no pages in the page cache, so any extent state structs
    that we put in the tree won't get freed by releasepage.  We have
    to clear them ourselves as the DIO ends.
    
    With this commit, we reserve space at in btrfs_file_aio_write,
    and then as each btrfs_direct_IO call progresses it sets
    EXTENT_DELALLOC on the range.
    
    btrfs_get_blocks_direct is responsible for clearing the delalloc
    at the same time it drops the extent lock.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 233aea2e5ef2..54556cae4497 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -909,13 +909,6 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 		}
 
 		if (num_written < 0) {
-			if (num_written != -EIOCBQUEUED) {
-				/*
-				 * aio land will take care of releasing the
-				 * delalloc
-				 */
-				btrfs_delalloc_release_space(inode, count);
-			}
 			ret = num_written;
 			num_written = 0;
 			goto out;
@@ -924,13 +917,6 @@ static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
 			pos = *ppos;
 			goto out;
 		}
-
-		/*
-		 * the buffered IO will reserve bytes for the rest of the
-		 * range, don't double count them here
-		 */
-		btrfs_delalloc_release_space(inode, count - num_written);
-
 		/*
 		 * We are going to do buffered for the rest of the range, so we
 		 * need to make sure to invalidate the buffered pages when we're

commit 11c65dccf70be9ace5dbd3906778e1a099b1fee1
Author: Josef Bacik <josef@redhat.com>
Date:   Sun May 23 11:07:21 2010 -0400

    Btrfs: do aio_write instead of write
    
    In order for AIO to work, we need to implement aio_write.  This patch converts
    our btrfs_file_write to btrfs_aio_write.  I've tested this with xfstests and
    nothing broke, and the AIO stuff magically started working.  Thanks,
    
    Signed-off-by: Josef Bacik <josef@redhat.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index a28810abfb98..233aea2e5ef2 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -46,32 +46,42 @@
 static noinline int btrfs_copy_from_user(loff_t pos, int num_pages,
 					 int write_bytes,
 					 struct page **prepared_pages,
-					 const char __user *buf)
+					 struct iov_iter *i)
 {
-	long page_fault = 0;
-	int i;
+	size_t copied;
+	int pg = 0;
 	int offset = pos & (PAGE_CACHE_SIZE - 1);
 
-	for (i = 0; i < num_pages && write_bytes > 0; i++, offset = 0) {
+	while (write_bytes > 0) {
 		size_t count = min_t(size_t,
 				     PAGE_CACHE_SIZE - offset, write_bytes);
-		struct page *page = prepared_pages[i];
-		fault_in_pages_readable(buf, count);
+		struct page *page = prepared_pages[pg];
+again:
+		if (unlikely(iov_iter_fault_in_readable(i, count)))
+			return -EFAULT;
 
 		/* Copy data from userspace to the current page */
-		kmap(page);
-		page_fault = __copy_from_user(page_address(page) + offset,
-					      buf, count);
+		copied = iov_iter_copy_from_user(page, i, offset, count);
+
 		/* Flush processor's dcache for this page */
 		flush_dcache_page(page);
-		kunmap(page);
-		buf += count;
-		write_bytes -= count;
+		iov_iter_advance(i, copied);
+		write_bytes -= copied;
 
-		if (page_fault)
-			break;
+		if (unlikely(copied == 0)) {
+			count = min_t(size_t, PAGE_CACHE_SIZE - offset,
+				      iov_iter_single_seg_count(i));
+			goto again;
+		}
+
+		if (unlikely(copied < PAGE_CACHE_SIZE - offset)) {
+			offset += copied;
+		} else {
+			pg++;
+			offset = 0;
+		}
 	}
-	return page_fault ? -EFAULT : 0;
+	return 0;
 }
 
 /*
@@ -822,60 +832,24 @@ static noinline int prepare_pages(struct btrfs_root *root, struct file *file,
 	return 0;
 }
 
-/* Copied from read-write.c */
-static void wait_on_retry_sync_kiocb(struct kiocb *iocb)
-{
-	set_current_state(TASK_UNINTERRUPTIBLE);
-	if (!kiocbIsKicked(iocb))
-		schedule();
-	else
-		kiocbClearKicked(iocb);
-	__set_current_state(TASK_RUNNING);
-}
-
-/*
- * Just a copy of what do_sync_write does.
- */
-static ssize_t __btrfs_direct_write(struct file *file, const char __user *buf,
-				    size_t count, loff_t pos, loff_t *ppos)
+static ssize_t btrfs_file_aio_write(struct kiocb *iocb,
+				    const struct iovec *iov,
+				    unsigned long nr_segs, loff_t pos)
 {
-	struct iovec iov = { .iov_base = (void __user *)buf, .iov_len = count };
-	unsigned long nr_segs = 1;
-	struct kiocb kiocb;
-	ssize_t ret;
-
-	init_sync_kiocb(&kiocb, file);
-	kiocb.ki_pos = pos;
-	kiocb.ki_left = count;
-	kiocb.ki_nbytes = count;
-
-	while (1) {
-		ret = generic_file_direct_write(&kiocb, &iov, &nr_segs, pos,
-						ppos, count, count);
-		if (ret != -EIOCBRETRY)
-			break;
-		wait_on_retry_sync_kiocb(&kiocb);
-	}
-
-	if (ret == -EIOCBQUEUED)
-		ret = wait_on_sync_kiocb(&kiocb);
-	*ppos = kiocb.ki_pos;
-	return ret;
-}
-
-static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
-				size_t count, loff_t *ppos)
-{
-	loff_t pos;
+	struct file *file = iocb->ki_filp;
+	struct inode *inode = fdentry(file)->d_inode;
+	struct btrfs_root *root = BTRFS_I(inode)->root;
+	struct page *pinned[2];
+	struct page **pages = NULL;
+	struct iov_iter i;
+	loff_t *ppos = &iocb->ki_pos;
 	loff_t start_pos;
 	ssize_t num_written = 0;
 	ssize_t err = 0;
+	size_t count;
+	size_t ocount;
 	int ret = 0;
-	struct inode *inode = fdentry(file)->d_inode;
-	struct btrfs_root *root = BTRFS_I(inode)->root;
-	struct page **pages = NULL;
 	int nrptrs;
-	struct page *pinned[2];
 	unsigned long first_index;
 	unsigned long last_index;
 	int will_write;
@@ -887,13 +861,17 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 	pinned[0] = NULL;
 	pinned[1] = NULL;
 
-	pos = *ppos;
 	start_pos = pos;
 
 	vfs_check_frozen(inode->i_sb, SB_FREEZE_WRITE);
 
 	mutex_lock(&inode->i_mutex);
 
+	err = generic_segment_checks(iov, &nr_segs, &ocount, VERIFY_READ);
+	if (err)
+		goto out;
+	count = ocount;
+
 	current->backing_dev_info = inode->i_mapping->backing_dev_info;
 	err = generic_write_checks(file, &pos, &count, S_ISBLK(inode->i_mode));
 	if (err)
@@ -910,14 +888,48 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 	BTRFS_I(inode)->sequence++;
 
 	if (unlikely(file->f_flags & O_DIRECT)) {
-		num_written = __btrfs_direct_write(file, buf, count, pos,
-						   ppos);
-		pos += num_written;
-		count -= num_written;
+		ret = btrfs_delalloc_reserve_space(inode, count);
+		if (ret)
+			goto out;
 
-		/* We've written everything we wanted to, exit */
-		if (num_written < 0 || !count)
+		num_written = generic_file_direct_write(iocb, iov, &nr_segs,
+							pos, ppos, count,
+							ocount);
+
+		/*
+		 * the generic O_DIRECT will update in-memory i_size after the
+		 * DIOs are done.  But our endio handlers that update the on
+		 * disk i_size never update past the in memory i_size.  So we
+		 * need one more update here to catch any additions to the
+		 * file
+		 */
+		if (inode->i_size != BTRFS_I(inode)->disk_i_size) {
+			btrfs_ordered_update_i_size(inode, inode->i_size, NULL);
+			mark_inode_dirty(inode);
+		}
+
+		if (num_written < 0) {
+			if (num_written != -EIOCBQUEUED) {
+				/*
+				 * aio land will take care of releasing the
+				 * delalloc
+				 */
+				btrfs_delalloc_release_space(inode, count);
+			}
+			ret = num_written;
+			num_written = 0;
 			goto out;
+		} else if (num_written == count) {
+			/* pick up pos changes done by the generic code */
+			pos = *ppos;
+			goto out;
+		}
+
+		/*
+		 * the buffered IO will reserve bytes for the rest of the
+		 * range, don't double count them here
+		 */
+		btrfs_delalloc_release_space(inode, count - num_written);
 
 		/*
 		 * We are going to do buffered for the rest of the range, so we
@@ -925,18 +937,20 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 		 * done.
 		 */
 		buffered = 1;
-		buf += num_written;
+		pos += num_written;
 	}
 
-	nrptrs = min((count + PAGE_CACHE_SIZE - 1) / PAGE_CACHE_SIZE,
-		     PAGE_CACHE_SIZE / (sizeof(struct page *)));
+	iov_iter_init(&i, iov, nr_segs, count, num_written);
+	nrptrs = min((iov_iter_count(&i) + PAGE_CACHE_SIZE - 1) /
+		     PAGE_CACHE_SIZE, PAGE_CACHE_SIZE /
+		     (sizeof(struct page *)));
 	pages = kmalloc(nrptrs * sizeof(struct page *), GFP_KERNEL);
 
 	/* generic_write_checks can change our pos */
 	start_pos = pos;
 
 	first_index = pos >> PAGE_CACHE_SHIFT;
-	last_index = (pos + count) >> PAGE_CACHE_SHIFT;
+	last_index = (pos + iov_iter_count(&i)) >> PAGE_CACHE_SHIFT;
 
 	/*
 	 * there are lots of better ways to do this, but this code
@@ -953,7 +967,7 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 			unlock_page(pinned[0]);
 		}
 	}
-	if ((pos + count) & (PAGE_CACHE_SIZE - 1)) {
+	if ((pos + iov_iter_count(&i)) & (PAGE_CACHE_SIZE - 1)) {
 		pinned[1] = grab_cache_page(inode->i_mapping, last_index);
 		if (!PageUptodate(pinned[1])) {
 			ret = btrfs_readpage(NULL, pinned[1]);
@@ -964,10 +978,10 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 		}
 	}
 
-	while (count > 0) {
+	while (iov_iter_count(&i) > 0) {
 		size_t offset = pos & (PAGE_CACHE_SIZE - 1);
-		size_t write_bytes = min(count, nrptrs *
-					(size_t)PAGE_CACHE_SIZE -
+		size_t write_bytes = min(iov_iter_count(&i),
+					 nrptrs * (size_t)PAGE_CACHE_SIZE -
 					 offset);
 		size_t num_pages = (write_bytes + PAGE_CACHE_SIZE - 1) >>
 					PAGE_CACHE_SHIFT;
@@ -988,7 +1002,7 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 		}
 
 		ret = btrfs_copy_from_user(pos, num_pages,
-					   write_bytes, pages, buf);
+					   write_bytes, pages, &i);
 		if (ret == 0) {
 			dirty_and_release_pages(NULL, root, file, pages,
 						num_pages, pos, write_bytes);
@@ -1012,8 +1026,6 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 			btrfs_throttle(root);
 		}
 
-		buf += write_bytes;
-		count -= write_bytes;
 		pos += write_bytes;
 		num_written += write_bytes;
 
@@ -1206,7 +1218,7 @@ const struct file_operations btrfs_file_operations = {
 	.read		= do_sync_read,
 	.aio_read       = generic_file_aio_read,
 	.splice_read	= generic_file_splice_read,
-	.write		= btrfs_file_write,
+	.aio_write	= btrfs_file_aio_write,
 	.mmap		= btrfs_file_mmap,
 	.open		= generic_file_open,
 	.release	= btrfs_release_file,

commit 4b46fce23349bfca781a32e2707a18328ca5ae22
Author: Josef Bacik <josef@redhat.com>
Date:   Sun May 23 11:00:55 2010 -0400

    Btrfs: add basic DIO read/write support
    
    This provides basic DIO support for reading and writing.  It does not do the
    work to recover from mismatching checksums, that will come later.  A few design
    changes have been made from Jim's code (sorry Jim!)
    
    1) Use the generic direct-io code.  Jim originally re-wrote all the generic DIO
    code in order to account for all of BTRFS's oddities, but thanks to that work it
    seems like the best bet is to just ignore compression and such and just opt to
    fallback on buffered IO.
    
    2) Fallback on buffered IO for compressed or inline extents.  Jim's code did
    it's own buffering to make dio with compressed extents work.  Now we just
    fallback onto normal buffered IO.
    
    3) Use ordered extents for the writes so that all of the
    
    lock_extent()
    lookup_ordered()
    
    type checks continue to work.
    
    4) Do the lock_extent() lookup_ordered() loop in readpage so we don't race with
    DIO writes.
    
    I've tested this with fsx and everything works great.  This patch depends on my
    dio and filemap.c patches to work.  Thanks,
    
    Signed-off-by: Josef Bacik <josef@redhat.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 6d8f817eadb5..a28810abfb98 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -822,6 +822,47 @@ static noinline int prepare_pages(struct btrfs_root *root, struct file *file,
 	return 0;
 }
 
+/* Copied from read-write.c */
+static void wait_on_retry_sync_kiocb(struct kiocb *iocb)
+{
+	set_current_state(TASK_UNINTERRUPTIBLE);
+	if (!kiocbIsKicked(iocb))
+		schedule();
+	else
+		kiocbClearKicked(iocb);
+	__set_current_state(TASK_RUNNING);
+}
+
+/*
+ * Just a copy of what do_sync_write does.
+ */
+static ssize_t __btrfs_direct_write(struct file *file, const char __user *buf,
+				    size_t count, loff_t pos, loff_t *ppos)
+{
+	struct iovec iov = { .iov_base = (void __user *)buf, .iov_len = count };
+	unsigned long nr_segs = 1;
+	struct kiocb kiocb;
+	ssize_t ret;
+
+	init_sync_kiocb(&kiocb, file);
+	kiocb.ki_pos = pos;
+	kiocb.ki_left = count;
+	kiocb.ki_nbytes = count;
+
+	while (1) {
+		ret = generic_file_direct_write(&kiocb, &iov, &nr_segs, pos,
+						ppos, count, count);
+		if (ret != -EIOCBRETRY)
+			break;
+		wait_on_retry_sync_kiocb(&kiocb);
+	}
+
+	if (ret == -EIOCBQUEUED)
+		ret = wait_on_sync_kiocb(&kiocb);
+	*ppos = kiocb.ki_pos;
+	return ret;
+}
+
 static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 				size_t count, loff_t *ppos)
 {
@@ -838,12 +879,11 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 	unsigned long first_index;
 	unsigned long last_index;
 	int will_write;
+	int buffered = 0;
 
 	will_write = ((file->f_flags & O_DSYNC) || IS_SYNC(inode) ||
 		      (file->f_flags & O_DIRECT));
 
-	nrptrs = min((count + PAGE_CACHE_SIZE - 1) / PAGE_CACHE_SIZE,
-		     PAGE_CACHE_SIZE / (sizeof(struct page *)));
 	pinned[0] = NULL;
 	pinned[1] = NULL;
 
@@ -867,13 +907,34 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 		goto out;
 
 	file_update_time(file);
+	BTRFS_I(inode)->sequence++;
+
+	if (unlikely(file->f_flags & O_DIRECT)) {
+		num_written = __btrfs_direct_write(file, buf, count, pos,
+						   ppos);
+		pos += num_written;
+		count -= num_written;
+
+		/* We've written everything we wanted to, exit */
+		if (num_written < 0 || !count)
+			goto out;
 
+		/*
+		 * We are going to do buffered for the rest of the range, so we
+		 * need to make sure to invalidate the buffered pages when we're
+		 * done.
+		 */
+		buffered = 1;
+		buf += num_written;
+	}
+
+	nrptrs = min((count + PAGE_CACHE_SIZE - 1) / PAGE_CACHE_SIZE,
+		     PAGE_CACHE_SIZE / (sizeof(struct page *)));
 	pages = kmalloc(nrptrs * sizeof(struct page *), GFP_KERNEL);
 
 	/* generic_write_checks can change our pos */
 	start_pos = pos;
 
-	BTRFS_I(inode)->sequence++;
 	first_index = pos >> PAGE_CACHE_SHIFT;
 	last_index = (pos + count) >> PAGE_CACHE_SHIFT;
 
@@ -1007,7 +1068,7 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 				btrfs_end_transaction(trans, root);
 			}
 		}
-		if (file->f_flags & O_DIRECT) {
+		if (file->f_flags & O_DIRECT && buffered) {
 			invalidate_mapping_pages(inode->i_mapping,
 			      start_pos >> PAGE_CACHE_SHIFT,
 			     (start_pos + num_written - 1) >> PAGE_CACHE_SHIFT);

commit 0ca1f7ceb1991099ed5273885ebcf4323948c72e
Author: Yan, Zheng <zheng.yan@oracle.com>
Date:   Sun May 16 10:48:47 2010 -0400

    Btrfs: Update metadata reservation for delayed allocation
    
    Introduce metadata reservation context for delayed allocation
    and update various related functions.
    
    This patch also introduces EXTENT_FIRST_DELALLOC control bit for
    set/clear_extent_bit. It tells set/clear_bit_hook whether they
    are processing the first extent_state with EXTENT_DELALLOC bit
    set. This change is important if set/clear_extent_bit involves
    multiple extent_state.
    
    Signed-off-by: Yan Zheng <zheng.yan@oracle.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 41e09e24e295..6d8f817eadb5 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -852,13 +852,6 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 
 	vfs_check_frozen(inode->i_sb, SB_FREEZE_WRITE);
 
-	/* do the reserve before the mutex lock in case we have to do some
-	 * flushing.  We wouldn't deadlock, but this is more polite.
-	 */
-	err = btrfs_reserve_metadata_for_delalloc(root, inode, 1);
-	if (err)
-		goto out_nolock;
-
 	mutex_lock(&inode->i_mutex);
 
 	current->backing_dev_info = inode->i_mapping->backing_dev_info;
@@ -921,7 +914,7 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 		WARN_ON(num_pages > nrptrs);
 		memset(pages, 0, sizeof(struct page *) * nrptrs);
 
-		ret = btrfs_check_data_free_space(root, inode, write_bytes);
+		ret = btrfs_delalloc_reserve_space(inode, write_bytes);
 		if (ret)
 			goto out;
 
@@ -929,26 +922,20 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 				    pos, first_index, last_index,
 				    write_bytes);
 		if (ret) {
-			btrfs_free_reserved_data_space(root, inode,
-						       write_bytes);
+			btrfs_delalloc_release_space(inode, write_bytes);
 			goto out;
 		}
 
 		ret = btrfs_copy_from_user(pos, num_pages,
 					   write_bytes, pages, buf);
-		if (ret) {
-			btrfs_free_reserved_data_space(root, inode,
-						       write_bytes);
-			btrfs_drop_pages(pages, num_pages);
-			goto out;
+		if (ret == 0) {
+			dirty_and_release_pages(NULL, root, file, pages,
+						num_pages, pos, write_bytes);
 		}
 
-		ret = dirty_and_release_pages(NULL, root, file, pages,
-					      num_pages, pos, write_bytes);
 		btrfs_drop_pages(pages, num_pages);
 		if (ret) {
-			btrfs_free_reserved_data_space(root, inode,
-						       write_bytes);
+			btrfs_delalloc_release_space(inode, write_bytes);
 			goto out;
 		}
 
@@ -975,9 +962,7 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 	mutex_unlock(&inode->i_mutex);
 	if (ret)
 		err = ret;
-	btrfs_unreserve_metadata_for_delalloc(root, inode, 1);
 
-out_nolock:
 	kfree(pages);
 	if (pinned[0])
 		page_cache_release(pinned[0]);

commit a22285a6a32390195235171b89d157ed1a1fe932
Author: Yan, Zheng <zheng.yan@oracle.com>
Date:   Sun May 16 10:48:46 2010 -0400

    Btrfs: Integrate metadata reservation with start_transaction
    
    Besides simplify the code, this change makes sure all metadata
    reservation for normal metadata operations are released after
    committing transaction.
    
    Changes since V1:
    
    Add code that check if unlink and rmdir will free space.
    
    Add ENOSPC handling for clone ioctl.
    
    Signed-off-by: Yan Zheng <zheng.yan@oracle.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 29ff749ff4ca..41e09e24e295 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -126,8 +126,7 @@ static noinline int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 	end_of_last_block = start_pos + num_bytes - 1;
 	err = btrfs_set_extent_delalloc(inode, start_pos, end_of_last_block,
 					NULL);
-	if (err)
-		return err;
+	BUG_ON(err);
 
 	for (i = 0; i < num_pages; i++) {
 		struct page *p = pages[i];
@@ -142,7 +141,7 @@ static noinline int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 		 * at this time.
 		 */
 	}
-	return err;
+	return 0;
 }
 
 /*
@@ -1008,7 +1007,7 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 			num_written = err;
 
 		if ((file->f_flags & O_DSYNC) || IS_SYNC(inode)) {
-			trans = btrfs_start_transaction(root, 1);
+			trans = btrfs_start_transaction(root, 0);
 			ret = btrfs_log_dentry_safe(trans, root,
 						    file->f_dentry);
 			if (ret == 0) {
@@ -1104,9 +1103,9 @@ int btrfs_sync_file(struct file *file, struct dentry *dentry, int datasync)
 	if (file && file->private_data)
 		btrfs_ioctl_trans_end(file);
 
-	trans = btrfs_start_transaction(root, 1);
-	if (!trans) {
-		ret = -ENOMEM;
+	trans = btrfs_start_transaction(root, 0);
+	if (IS_ERR(trans)) {
+		ret = PTR_ERR(trans);
 		goto out;
 	}
 

commit 5a0e3ad6af8660be21ca98a971cd00f331318c05
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Mar 24 17:04:11 2010 +0900

    include cleanup: Update gfp.h and slab.h includes to prepare for breaking implicit slab.h inclusion from percpu.h
    
    percpu.h is included by sched.h and module.h and thus ends up being
    included when building most .c files.  percpu.h includes slab.h which
    in turn includes gfp.h making everything defined by the two files
    universally available and complicating inclusion dependencies.
    
    percpu.h -> slab.h dependency is about to be removed.  Prepare for
    this change by updating users of gfp and slab facilities include those
    headers directly instead of assuming availability.  As this conversion
    needs to touch large number of source files, the following script is
    used as the basis of conversion.
    
      http://userweb.kernel.org/~tj/misc/slabh-sweep.py
    
    The script does the followings.
    
    * Scan files for gfp and slab usages and update includes such that
      only the necessary includes are there.  ie. if only gfp is used,
      gfp.h, if slab is used, slab.h.
    
    * When the script inserts a new include, it looks at the include
      blocks and try to put the new include such that its order conforms
      to its surrounding.  It's put in the include block which contains
      core kernel includes, in the same order that the rest are ordered -
      alphabetical, Christmas tree, rev-Xmas-tree or at the end if there
      doesn't seem to be any matching order.
    
    * If the script can't find a place to put a new include (mostly
      because the file doesn't have fitting include block), it prints out
      an error message indicating which .h file needs to be added to the
      file.
    
    The conversion was done in the following steps.
    
    1. The initial automatic conversion of all .c files updated slightly
       over 4000 files, deleting around 700 includes and adding ~480 gfp.h
       and ~3000 slab.h inclusions.  The script emitted errors for ~400
       files.
    
    2. Each error was manually checked.  Some didn't need the inclusion,
       some needed manual addition while adding it to implementation .h or
       embedding .c file was more appropriate for others.  This step added
       inclusions to around 150 files.
    
    3. The script was run again and the output was compared to the edits
       from #2 to make sure no file was left behind.
    
    4. Several build tests were done and a couple of problems were fixed.
       e.g. lib/decompress_*.c used malloc/free() wrappers around slab
       APIs requiring slab.h to be added manually.
    
    5. The script was run on all .h files but without automatically
       editing them as sprinkling gfp.h and slab.h inclusions around .h
       files could easily lead to inclusion dependency hell.  Most gfp.h
       inclusion directives were ignored as stuff from gfp.h was usually
       wildly available and often used in preprocessor macros.  Each
       slab.h inclusion directive was examined and added manually as
       necessary.
    
    6. percpu.h was updated not to include slab.h.
    
    7. Build test were done on the following configurations and failures
       were fixed.  CONFIG_GCOV_KERNEL was turned off for all tests (as my
       distributed build env didn't work with gcov compiles) and a few
       more options had to be turned off depending on archs to make things
       build (like ipr on powerpc/64 which failed due to missing writeq).
    
       * x86 and x86_64 UP and SMP allmodconfig and a custom test config.
       * powerpc and powerpc64 SMP allmodconfig
       * sparc and sparc64 SMP allmodconfig
       * ia64 SMP allmodconfig
       * s390 SMP allmodconfig
       * alpha SMP allmodconfig
       * um on x86_64 SMP allmodconfig
    
    8. percpu.h modifications were reverted so that it could be applied as
       a separate patch and serve as bisection point.
    
    Given the fact that I had only a couple of failures from tests on step
    6, I'm fairly confident about the coverage of this conversion patch.
    If there is a breakage, it's likely to be something in one of the arch
    headers which should be easily discoverable easily on most builds of
    the specific arch.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Guess-its-ok-by: Christoph Lameter <cl@linux-foundation.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Lee Schermerhorn <Lee.Schermerhorn@hp.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index ee3323c7fc1c..29ff749ff4ca 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -28,6 +28,7 @@
 #include <linux/writeback.h>
 #include <linux/statfs.h>
 #include <linux/compat.h>
+#include <linux/slab.h>
 #include "ctree.h"
 #include "disk-io.h"
 #include "transaction.h"

commit 441f4058a04b2943685ff94e0f5f1992b0b3649e
Merge: 7c34691abe23 8ad6fcab564c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 18 16:50:55 2010 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/mason/btrfs-unstable
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/mason/btrfs-unstable: (30 commits)
      Btrfs: fix the inode ref searches done by btrfs_search_path_in_tree
      Btrfs: allow treeid==0 in the inode lookup ioctl
      Btrfs: return keys for large items to the search ioctl
      Btrfs: fix key checks and advance in the search ioctl
      Btrfs: buffer results in the space_info ioctl
      Btrfs: use __u64 types in ioctl.h
      Btrfs: fix search_ioctl key advance
      Btrfs: fix gfp flags masking in the compression code
      Btrfs: don't look at bio flags after submit_bio
      btrfs: using btrfs_stack_device_id() get devid
      btrfs: use memparse
      Btrfs: add a "df" ioctl for btrfs
      Btrfs: cache the extent state everywhere we possibly can V2
      Btrfs: cache ordered extent when completing io
      Btrfs: cache extent state in find_delalloc_range
      Btrfs: change the ordered tree to use a spinlock instead of a mutex
      Btrfs: finish read pages in the order they are submitted
      btrfs: fix btrfs_mkdir goto for no free objectids
      Btrfs: flush data on snapshot creation
      Btrfs: make df be a little bit more understandable
      ...

commit 2ac55d41b5d6bf49e76bc85db5431240617e2f8f
Author: Josef Bacik <josef@redhat.com>
Date:   Wed Feb 3 19:33:23 2010 +0000

    Btrfs: cache the extent state everywhere we possibly can V2
    
    This patch just goes through and fixes everybody that does
    
    lock_extent()
    blah
    unlock_extent()
    
    to use
    
    lock_extent_bits()
    blah
    unlock_extent_cached()
    
    and pass around a extent_state so we only have to do the searches once per
    function.  This gives me about a 3 mb/s boots on my random write test.  I have
    not converted some things, like the relocation and ioctl's, since they aren't
    heavily used and the relocation stuff is in the middle of being re-written.  I
    also changed the clear_extent_bit() to only unset the cached state if we are
    clearing EXTENT_LOCKED and related stuff, so we can do things like this
    
    lock_extent_bits()
    clear delalloc bits
    unlock_extent_cached()
    
    without losing our cached state.  I tested this thoroughly and turned on
    LEAK_DEBUG to make sure we weren't leaking extent states, everything worked out
    fine.
    
    Signed-off-by: Josef Bacik <josef@redhat.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index a7fd9f3a750a..d146dde7efb6 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -123,7 +123,8 @@ static noinline int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 		    root->sectorsize - 1) & ~((u64)root->sectorsize - 1);
 
 	end_of_last_block = start_pos + num_bytes - 1;
-	err = btrfs_set_extent_delalloc(inode, start_pos, end_of_last_block);
+	err = btrfs_set_extent_delalloc(inode, start_pos, end_of_last_block,
+					NULL);
 	if (err)
 		return err;
 
@@ -753,6 +754,7 @@ static noinline int prepare_pages(struct btrfs_root *root, struct file *file,
 			 loff_t pos, unsigned long first_index,
 			 unsigned long last_index, size_t write_bytes)
 {
+	struct extent_state *cached_state = NULL;
 	int i;
 	unsigned long index = pos >> PAGE_CACHE_SHIFT;
 	struct inode *inode = fdentry(file)->d_inode;
@@ -781,16 +783,18 @@ static noinline int prepare_pages(struct btrfs_root *root, struct file *file,
 	}
 	if (start_pos < inode->i_size) {
 		struct btrfs_ordered_extent *ordered;
-		lock_extent(&BTRFS_I(inode)->io_tree,
-			    start_pos, last_pos - 1, GFP_NOFS);
+		lock_extent_bits(&BTRFS_I(inode)->io_tree,
+				 start_pos, last_pos - 1, 0, &cached_state,
+				 GFP_NOFS);
 		ordered = btrfs_lookup_first_ordered_extent(inode,
 							    last_pos - 1);
 		if (ordered &&
 		    ordered->file_offset + ordered->len > start_pos &&
 		    ordered->file_offset < last_pos) {
 			btrfs_put_ordered_extent(ordered);
-			unlock_extent(&BTRFS_I(inode)->io_tree,
-				      start_pos, last_pos - 1, GFP_NOFS);
+			unlock_extent_cached(&BTRFS_I(inode)->io_tree,
+					     start_pos, last_pos - 1,
+					     &cached_state, GFP_NOFS);
 			for (i = 0; i < num_pages; i++) {
 				unlock_page(pages[i]);
 				page_cache_release(pages[i]);
@@ -802,12 +806,13 @@ static noinline int prepare_pages(struct btrfs_root *root, struct file *file,
 		if (ordered)
 			btrfs_put_ordered_extent(ordered);
 
-		clear_extent_bits(&BTRFS_I(inode)->io_tree, start_pos,
+		clear_extent_bit(&BTRFS_I(inode)->io_tree, start_pos,
 				  last_pos - 1, EXTENT_DIRTY | EXTENT_DELALLOC |
-				  EXTENT_DO_ACCOUNTING,
+				  EXTENT_DO_ACCOUNTING, 0, 0, &cached_state,
 				  GFP_NOFS);
-		unlock_extent(&BTRFS_I(inode)->io_tree,
-			      start_pos, last_pos - 1, GFP_NOFS);
+		unlock_extent_cached(&BTRFS_I(inode)->io_tree,
+				     start_pos, last_pos - 1, &cached_state,
+				     GFP_NOFS);
 	}
 	for (i = 0; i < num_pages; i++) {
 		clear_page_dirty_for_io(pages[i]);

commit 0813e22d4e0d618eac9b47bec942bf856adca4c5
Merge: 382640b33724 3f6fae955922
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Feb 15 19:56:21 2010 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/mason/btrfs-unstable
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/mason/btrfs-unstable:
      Btrfs: btrfs_mark_extent_written uses the wrong slot

commit 3f6fae9559225741c91f1320090b285da1413290
Author: Shaohua Li <shaohua.li@intel.com>
Date:   Thu Feb 11 07:43:00 2010 +0000

    Btrfs: btrfs_mark_extent_written uses the wrong slot
    
    My test do: fallocate a big file and do write. The file is 512M, but
    after file write is done btrfs-debug-tree shows:
    item 6 key (257 EXTENT_DATA 0) itemoff 3516 itemsize 53
                    extent data disk byte 1103101952 nr 536870912
                    extent data offset 0 nr 399634432 ram 536870912
                    extent compression 0
    Looks like a regression introducted by
    6c7d54ac87f338c479d9729e8392eca3f76e11e1, where we set wrong slot.
    
    Signed-off-by: Shaohua Li <shaohua.li@intel.com>
    Acked-by: Yan Zheng <zheng.yan@oracle.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 413a30dafcda..a7fd9f3a750a 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -720,13 +720,15 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 					inode->i_ino, orig_offset);
 		BUG_ON(ret);
 	}
-	fi = btrfs_item_ptr(leaf, path->slots[0],
-			   struct btrfs_file_extent_item);
 	if (del_nr == 0) {
+		fi = btrfs_item_ptr(leaf, path->slots[0],
+			   struct btrfs_file_extent_item);
 		btrfs_set_file_extent_type(leaf, fi,
 					   BTRFS_FILE_EXTENT_REG);
 		btrfs_mark_buffer_dirty(leaf);
 	} else {
+		fi = btrfs_item_ptr(leaf, del_slot - 1,
+			   struct btrfs_file_extent_item);
 		btrfs_set_file_extent_type(leaf, fi,
 					   BTRFS_FILE_EXTENT_REG);
 		btrfs_set_file_extent_num_bytes(leaf, fi,

commit adbfbcd12af3d183957622a99ca009b665639b81
Merge: fc76be434d90 23b5c50945f2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Feb 5 07:23:03 2010 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/mason/btrfs-unstable
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/mason/btrfs-unstable:
      Btrfs: apply updated fallocate i_size fix
      Btrfs: do not try and lookup the file extent when finishing ordered io
      Btrfs: Fix oopsen when dropping empty tree.
      Btrfs: remove BUG_ON() due to mounting bad filesystem
      Btrfs: make error return negative in btrfs_sync_file()
      Btrfs: fix race between allocate and release extent buffer.

commit 014e4ac4f7d9c981750491fa40ea35efadc9ed49
Author: Roel Kluin <roel.kluin@gmail.com>
Date:   Fri Jan 29 10:42:11 2010 +0000

    Btrfs: make error return negative in btrfs_sync_file()
    
    It appears the error return should be negative
    
    Signed-off-by: Roel Kluin <roel.kluin@gmail.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index ae96fdae1f7d..413a30dafcda 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1133,7 +1133,7 @@ int btrfs_sync_file(struct file *file, struct dentry *dentry, int datasync)
 	}
 	mutex_lock(&dentry->d_inode->i_mutex);
 out:
-	return ret > 0 ? EIO : ret;
+	return ret > 0 ? -EIO : ret;
 }
 
 static const struct vm_operations_struct btrfs_file_vm_ops = {

commit 30a0f5e1fb510f17c25ff159a9fffbe01ae0f34e
Merge: 88f5004430ba 11dfe35a0108
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jan 21 07:28:05 2010 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/mason/btrfs-unstable
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/mason/btrfs-unstable:
      Btrfs: fix possible panic on unmount
      Btrfs: deal with NULL acl sent to btrfs_set_acl
      Btrfs: fix regression in orphan cleanup
      Btrfs: Fix race in btrfs_mark_extent_written
      Btrfs, fix memory leaks in error paths
      Btrfs: align offsets for btrfs_ordered_update_i_size
      btrfs: fix missing last-entry in readdir(3)

commit 6c7d54ac87f338c479d9729e8392eca3f76e11e1
Author: Yan, Zheng <zheng.yan@oracle.com>
Date:   Fri Jan 15 08:43:09 2010 +0000

    Btrfs: Fix race in btrfs_mark_extent_written
    
    Fix bug reported by Johannes Hirte. The reason of that bug
    is btrfs_del_items is called after btrfs_duplicate_item and
    btrfs_del_items triggers tree balance. The fix is check that
    case and call btrfs_search_slot when needed.
    
    Signed-off-by: Yan Zheng <zheng.yan@oracle.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 3bfe9f03990b..ae96fdae1f7d 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -506,7 +506,8 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans, struct inode *inode,
 }
 
 static int extent_mergeable(struct extent_buffer *leaf, int slot,
-			    u64 objectid, u64 bytenr, u64 *start, u64 *end)
+			    u64 objectid, u64 bytenr, u64 orig_offset,
+			    u64 *start, u64 *end)
 {
 	struct btrfs_file_extent_item *fi;
 	struct btrfs_key key;
@@ -522,6 +523,7 @@ static int extent_mergeable(struct extent_buffer *leaf, int slot,
 	fi = btrfs_item_ptr(leaf, slot, struct btrfs_file_extent_item);
 	if (btrfs_file_extent_type(leaf, fi) != BTRFS_FILE_EXTENT_REG ||
 	    btrfs_file_extent_disk_bytenr(leaf, fi) != bytenr ||
+	    btrfs_file_extent_offset(leaf, fi) != key.offset - orig_offset ||
 	    btrfs_file_extent_compression(leaf, fi) ||
 	    btrfs_file_extent_encryption(leaf, fi) ||
 	    btrfs_file_extent_other_encoding(leaf, fi))
@@ -561,6 +563,7 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 	u64 split;
 	int del_nr = 0;
 	int del_slot = 0;
+	int recow;
 	int ret;
 
 	btrfs_drop_extent_cache(inode, start, end - 1, 0);
@@ -568,6 +571,7 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 	path = btrfs_alloc_path();
 	BUG_ON(!path);
 again:
+	recow = 0;
 	split = start;
 	key.objectid = inode->i_ino;
 	key.type = BTRFS_EXTENT_DATA_KEY;
@@ -591,12 +595,60 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 	bytenr = btrfs_file_extent_disk_bytenr(leaf, fi);
 	num_bytes = btrfs_file_extent_disk_num_bytes(leaf, fi);
 	orig_offset = key.offset - btrfs_file_extent_offset(leaf, fi);
+	memcpy(&new_key, &key, sizeof(new_key));
+
+	if (start == key.offset && end < extent_end) {
+		other_start = 0;
+		other_end = start;
+		if (extent_mergeable(leaf, path->slots[0] - 1,
+				     inode->i_ino, bytenr, orig_offset,
+				     &other_start, &other_end)) {
+			new_key.offset = end;
+			btrfs_set_item_key_safe(trans, root, path, &new_key);
+			fi = btrfs_item_ptr(leaf, path->slots[0],
+					    struct btrfs_file_extent_item);
+			btrfs_set_file_extent_num_bytes(leaf, fi,
+							extent_end - end);
+			btrfs_set_file_extent_offset(leaf, fi,
+						     end - orig_offset);
+			fi = btrfs_item_ptr(leaf, path->slots[0] - 1,
+					    struct btrfs_file_extent_item);
+			btrfs_set_file_extent_num_bytes(leaf, fi,
+							end - other_start);
+			btrfs_mark_buffer_dirty(leaf);
+			goto out;
+		}
+	}
+
+	if (start > key.offset && end == extent_end) {
+		other_start = end;
+		other_end = 0;
+		if (extent_mergeable(leaf, path->slots[0] + 1,
+				     inode->i_ino, bytenr, orig_offset,
+				     &other_start, &other_end)) {
+			fi = btrfs_item_ptr(leaf, path->slots[0],
+					    struct btrfs_file_extent_item);
+			btrfs_set_file_extent_num_bytes(leaf, fi,
+							start - key.offset);
+			path->slots[0]++;
+			new_key.offset = start;
+			btrfs_set_item_key_safe(trans, root, path, &new_key);
+
+			fi = btrfs_item_ptr(leaf, path->slots[0],
+					    struct btrfs_file_extent_item);
+			btrfs_set_file_extent_num_bytes(leaf, fi,
+							other_end - start);
+			btrfs_set_file_extent_offset(leaf, fi,
+						     start - orig_offset);
+			btrfs_mark_buffer_dirty(leaf);
+			goto out;
+		}
+	}
 
 	while (start > key.offset || end < extent_end) {
 		if (key.offset == start)
 			split = end;
 
-		memcpy(&new_key, &key, sizeof(new_key));
 		new_key.offset = split;
 		ret = btrfs_duplicate_item(trans, root, path, &new_key);
 		if (ret == -EAGAIN) {
@@ -631,15 +683,18 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 			path->slots[0]--;
 			extent_end = end;
 		}
+		recow = 1;
 	}
 
-	fi = btrfs_item_ptr(leaf, path->slots[0],
-			    struct btrfs_file_extent_item);
-
 	other_start = end;
 	other_end = 0;
-	if (extent_mergeable(leaf, path->slots[0] + 1, inode->i_ino,
-			     bytenr, &other_start, &other_end)) {
+	if (extent_mergeable(leaf, path->slots[0] + 1,
+			     inode->i_ino, bytenr, orig_offset,
+			     &other_start, &other_end)) {
+		if (recow) {
+			btrfs_release_path(root, path);
+			goto again;
+		}
 		extent_end = other_end;
 		del_slot = path->slots[0] + 1;
 		del_nr++;
@@ -650,8 +705,13 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 	}
 	other_start = 0;
 	other_end = start;
-	if (extent_mergeable(leaf, path->slots[0] - 1, inode->i_ino,
-			     bytenr, &other_start, &other_end)) {
+	if (extent_mergeable(leaf, path->slots[0] - 1,
+			     inode->i_ino, bytenr, orig_offset,
+			     &other_start, &other_end)) {
+		if (recow) {
+			btrfs_release_path(root, path);
+			goto again;
+		}
 		key.offset = other_start;
 		del_slot = path->slots[0];
 		del_nr++;
@@ -660,22 +720,22 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 					inode->i_ino, orig_offset);
 		BUG_ON(ret);
 	}
+	fi = btrfs_item_ptr(leaf, path->slots[0],
+			   struct btrfs_file_extent_item);
 	if (del_nr == 0) {
 		btrfs_set_file_extent_type(leaf, fi,
 					   BTRFS_FILE_EXTENT_REG);
 		btrfs_mark_buffer_dirty(leaf);
-		goto out;
-	}
-
-	fi = btrfs_item_ptr(leaf, del_slot - 1,
-			    struct btrfs_file_extent_item);
-	btrfs_set_file_extent_type(leaf, fi, BTRFS_FILE_EXTENT_REG);
-	btrfs_set_file_extent_num_bytes(leaf, fi,
-					extent_end - key.offset);
-	btrfs_mark_buffer_dirty(leaf);
+	} else {
+		btrfs_set_file_extent_type(leaf, fi,
+					   BTRFS_FILE_EXTENT_REG);
+		btrfs_set_file_extent_num_bytes(leaf, fi,
+						extent_end - key.offset);
+		btrfs_mark_buffer_dirty(leaf);
 
-	ret = btrfs_del_items(trans, root, path, del_slot, del_nr);
-	BUG_ON(ret);
+		ret = btrfs_del_items(trans, root, path, del_slot, del_nr);
+		BUG_ON(ret);
+	}
 out:
 	btrfs_free_path(path);
 	return 0;

commit ebfee3d71d5a29102aac1fb2e756b8258f753592
Merge: b8a7f3cd7e82 83d3c9696fed
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Dec 17 15:02:22 2009 -0500

    Merge branch btrfs-master into for-linus
    
    Conflicts:
            fs/btrfs/acl.c

commit 55ef68990029fcd8d04d42fc184aa7fb18cf309e
Author: Yan, Zheng <zheng.yan@oracle.com>
Date:   Thu Nov 12 09:36:44 2009 +0000

    Btrfs: Fix btrfs_drop_extent_cache for skip pinned case
    
    The check for skip pinned case is wrong, it may breaks the
    while loop too soon.
    
    Signed-off-by: Yan Zheng <zheng.yan@oracle.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 3d2e45ce5d25..3bfe9f03990b 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -179,18 +179,14 @@ int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
 		}
 		flags = em->flags;
 		if (skip_pinned && test_bit(EXTENT_FLAG_PINNED, &em->flags)) {
-			if (em->start <= start &&
-			    (!testend || em->start + em->len >= start + len)) {
+			if (testend && em->start + em->len >= start + len) {
 				free_extent_map(em);
 				write_unlock(&em_tree->lock);
 				break;
 			}
-			if (start < em->start) {
-				len = em->start - start;
-			} else {
+			start = em->start + em->len;
+			if (testend)
 				len = start + len - (em->start + em->len);
-				start = em->start + em->len;
-			}
 			free_extent_map(em);
 			write_unlock(&em_tree->lock);
 			continue;

commit 920bbbfb05c9fce22e088d20eb9dcb8f96342de9
Author: Yan, Zheng <zheng.yan@oracle.com>
Date:   Thu Nov 12 09:34:08 2009 +0000

    Btrfs: Rewrite btrfs_drop_extents
    
    Rewrite btrfs_drop_extents by using btrfs_duplicate_item, so we can
    avoid calling lock_extent within transaction.
    
    Signed-off-by: Yan Zheng <zheng.yan@oracle.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 06550affbd27..3d2e45ce5d25 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -265,319 +265,247 @@ int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
  * If an extent intersects the range but is not entirely inside the range
  * it is either truncated or split.  Anything entirely inside the range
  * is deleted from the tree.
- *
- * inline_limit is used to tell this code which offsets in the file to keep
- * if they contain inline extents.
  */
-noinline int btrfs_drop_extents(struct btrfs_trans_handle *trans,
-		       struct btrfs_root *root, struct inode *inode,
-		       u64 start, u64 end, u64 locked_end,
-		       u64 inline_limit, u64 *hint_byte, int drop_cache)
+int btrfs_drop_extents(struct btrfs_trans_handle *trans, struct inode *inode,
+		       u64 start, u64 end, u64 *hint_byte, int drop_cache)
 {
-	u64 extent_end = 0;
-	u64 search_start = start;
-	u64 ram_bytes = 0;
-	u64 disk_bytenr = 0;
-	u64 orig_locked_end = locked_end;
-	u8 compression;
-	u8 encryption;
-	u16 other_encoding = 0;
+	struct btrfs_root *root = BTRFS_I(inode)->root;
 	struct extent_buffer *leaf;
-	struct btrfs_file_extent_item *extent;
+	struct btrfs_file_extent_item *fi;
 	struct btrfs_path *path;
 	struct btrfs_key key;
-	struct btrfs_file_extent_item old;
-	int keep;
-	int slot;
-	int bookend;
-	int found_type = 0;
-	int found_extent;
-	int found_inline;
+	struct btrfs_key new_key;
+	u64 search_start = start;
+	u64 disk_bytenr = 0;
+	u64 num_bytes = 0;
+	u64 extent_offset = 0;
+	u64 extent_end = 0;
+	int del_nr = 0;
+	int del_slot = 0;
+	int extent_type;
 	int recow;
 	int ret;
 
-	inline_limit = 0;
 	if (drop_cache)
 		btrfs_drop_extent_cache(inode, start, end - 1, 0);
 
 	path = btrfs_alloc_path();
 	if (!path)
 		return -ENOMEM;
+
 	while (1) {
 		recow = 0;
-		btrfs_release_path(root, path);
 		ret = btrfs_lookup_file_extent(trans, root, path, inode->i_ino,
 					       search_start, -1);
 		if (ret < 0)
-			goto out;
-		if (ret > 0) {
-			if (path->slots[0] == 0) {
-				ret = 0;
-				goto out;
-			}
-			path->slots[0]--;
+			break;
+		if (ret > 0 && path->slots[0] > 0 && search_start == start) {
+			leaf = path->nodes[0];
+			btrfs_item_key_to_cpu(leaf, &key, path->slots[0] - 1);
+			if (key.objectid == inode->i_ino &&
+			    key.type == BTRFS_EXTENT_DATA_KEY)
+				path->slots[0]--;
 		}
+		ret = 0;
 next_slot:
-		keep = 0;
-		bookend = 0;
-		found_extent = 0;
-		found_inline = 0;
-		compression = 0;
-		encryption = 0;
-		extent = NULL;
 		leaf = path->nodes[0];
-		slot = path->slots[0];
-		ret = 0;
-		btrfs_item_key_to_cpu(leaf, &key, slot);
-		if (btrfs_key_type(&key) == BTRFS_EXTENT_DATA_KEY &&
-		    key.offset >= end) {
-			goto out;
-		}
-		if (btrfs_key_type(&key) > BTRFS_EXTENT_DATA_KEY ||
-		    key.objectid != inode->i_ino) {
-			goto out;
-		}
-		if (recow) {
-			search_start = max(key.offset, start);
-			continue;
-		}
-		if (btrfs_key_type(&key) == BTRFS_EXTENT_DATA_KEY) {
-			extent = btrfs_item_ptr(leaf, slot,
-						struct btrfs_file_extent_item);
-			found_type = btrfs_file_extent_type(leaf, extent);
-			compression = btrfs_file_extent_compression(leaf,
-								    extent);
-			encryption = btrfs_file_extent_encryption(leaf,
-								  extent);
-			other_encoding = btrfs_file_extent_other_encoding(leaf,
-								  extent);
-			if (found_type == BTRFS_FILE_EXTENT_REG ||
-			    found_type == BTRFS_FILE_EXTENT_PREALLOC) {
-				extent_end =
-				     btrfs_file_extent_disk_bytenr(leaf,
-								   extent);
-				if (extent_end)
-					*hint_byte = extent_end;
-
-				extent_end = key.offset +
-				     btrfs_file_extent_num_bytes(leaf, extent);
-				ram_bytes = btrfs_file_extent_ram_bytes(leaf,
-								extent);
-				found_extent = 1;
-			} else if (found_type == BTRFS_FILE_EXTENT_INLINE) {
-				found_inline = 1;
-				extent_end = key.offset +
-				     btrfs_file_extent_inline_len(leaf, extent);
+		if (path->slots[0] >= btrfs_header_nritems(leaf)) {
+			BUG_ON(del_nr > 0);
+			ret = btrfs_next_leaf(root, path);
+			if (ret < 0)
+				break;
+			if (ret > 0) {
+				ret = 0;
+				break;
 			}
+			leaf = path->nodes[0];
+			recow = 1;
+		}
+
+		btrfs_item_key_to_cpu(leaf, &key, path->slots[0]);
+		if (key.objectid > inode->i_ino ||
+		    key.type > BTRFS_EXTENT_DATA_KEY || key.offset >= end)
+			break;
+
+		fi = btrfs_item_ptr(leaf, path->slots[0],
+				    struct btrfs_file_extent_item);
+		extent_type = btrfs_file_extent_type(leaf, fi);
+
+		if (extent_type == BTRFS_FILE_EXTENT_REG ||
+		    extent_type == BTRFS_FILE_EXTENT_PREALLOC) {
+			disk_bytenr = btrfs_file_extent_disk_bytenr(leaf, fi);
+			num_bytes = btrfs_file_extent_disk_num_bytes(leaf, fi);
+			extent_offset = btrfs_file_extent_offset(leaf, fi);
+			extent_end = key.offset +
+				btrfs_file_extent_num_bytes(leaf, fi);
+		} else if (extent_type == BTRFS_FILE_EXTENT_INLINE) {
+			extent_end = key.offset +
+				btrfs_file_extent_inline_len(leaf, fi);
 		} else {
+			WARN_ON(1);
 			extent_end = search_start;
 		}
 
-		/* we found nothing we can drop */
-		if ((!found_extent && !found_inline) ||
-		    search_start >= extent_end) {
-			int nextret;
-			u32 nritems;
-			nritems = btrfs_header_nritems(leaf);
-			if (slot >= nritems - 1) {
-				nextret = btrfs_next_leaf(root, path);
-				if (nextret)
-					goto out;
-				recow = 1;
-			} else {
-				path->slots[0]++;
-			}
+		if (extent_end <= search_start) {
+			path->slots[0]++;
 			goto next_slot;
 		}
 
-		if (end <= extent_end && start >= key.offset && found_inline)
-			*hint_byte = EXTENT_MAP_INLINE;
-
-		if (found_extent) {
-			read_extent_buffer(leaf, &old, (unsigned long)extent,
-					   sizeof(old));
-		}
-
-		if (end < extent_end && end >= key.offset) {
-			bookend = 1;
-			if (found_inline && start <= key.offset)
-				keep = 1;
+		search_start = max(key.offset, start);
+		if (recow) {
+			btrfs_release_path(root, path);
+			continue;
 		}
 
-		if (bookend && found_extent) {
-			if (locked_end < extent_end) {
-				ret = try_lock_extent(&BTRFS_I(inode)->io_tree,
-						locked_end, extent_end - 1,
-						GFP_NOFS);
-				if (!ret) {
-					btrfs_release_path(root, path);
-					lock_extent(&BTRFS_I(inode)->io_tree,
-						locked_end, extent_end - 1,
-						GFP_NOFS);
-					locked_end = extent_end;
-					continue;
-				}
-				locked_end = extent_end;
+		/*
+		 *     | - range to drop - |
+		 *  | -------- extent -------- |
+		 */
+		if (start > key.offset && end < extent_end) {
+			BUG_ON(del_nr > 0);
+			BUG_ON(extent_type == BTRFS_FILE_EXTENT_INLINE);
+
+			memcpy(&new_key, &key, sizeof(new_key));
+			new_key.offset = start;
+			ret = btrfs_duplicate_item(trans, root, path,
+						   &new_key);
+			if (ret == -EAGAIN) {
+				btrfs_release_path(root, path);
+				continue;
 			}
-			disk_bytenr = le64_to_cpu(old.disk_bytenr);
-			if (disk_bytenr != 0) {
+			if (ret < 0)
+				break;
+
+			leaf = path->nodes[0];
+			fi = btrfs_item_ptr(leaf, path->slots[0] - 1,
+					    struct btrfs_file_extent_item);
+			btrfs_set_file_extent_num_bytes(leaf, fi,
+							start - key.offset);
+
+			fi = btrfs_item_ptr(leaf, path->slots[0],
+					    struct btrfs_file_extent_item);
+
+			extent_offset += start - key.offset;
+			btrfs_set_file_extent_offset(leaf, fi, extent_offset);
+			btrfs_set_file_extent_num_bytes(leaf, fi,
+							extent_end - start);
+			btrfs_mark_buffer_dirty(leaf);
+
+			if (disk_bytenr > 0) {
 				ret = btrfs_inc_extent_ref(trans, root,
-					   disk_bytenr,
-					   le64_to_cpu(old.disk_num_bytes), 0,
-					   root->root_key.objectid,
-					   key.objectid, key.offset -
-					   le64_to_cpu(old.offset));
+						disk_bytenr, num_bytes, 0,
+						root->root_key.objectid,
+						new_key.objectid,
+						start - extent_offset);
 				BUG_ON(ret);
+				*hint_byte = disk_bytenr;
 			}
+			key.offset = start;
 		}
+		/*
+		 *  | ---- range to drop ----- |
+		 *      | -------- extent -------- |
+		 */
+		if (start <= key.offset && end < extent_end) {
+			BUG_ON(extent_type == BTRFS_FILE_EXTENT_INLINE);
+
+			memcpy(&new_key, &key, sizeof(new_key));
+			new_key.offset = end;
+			btrfs_set_item_key_safe(trans, root, path, &new_key);
 
-		if (found_inline) {
-			u64 mask = root->sectorsize - 1;
-			search_start = (extent_end + mask) & ~mask;
-		} else
-			search_start = extent_end;
-
-		/* truncate existing extent */
-		if (start > key.offset) {
-			u64 new_num;
-			u64 old_num;
-			keep = 1;
-			WARN_ON(start & (root->sectorsize - 1));
-			if (found_extent) {
-				new_num = start - key.offset;
-				old_num = btrfs_file_extent_num_bytes(leaf,
-								      extent);
-				*hint_byte =
-					btrfs_file_extent_disk_bytenr(leaf,
-								      extent);
-				if (btrfs_file_extent_disk_bytenr(leaf,
-								  extent)) {
-					inode_sub_bytes(inode, old_num -
-							new_num);
-				}
-				btrfs_set_file_extent_num_bytes(leaf,
-							extent, new_num);
-				btrfs_mark_buffer_dirty(leaf);
-			} else if (key.offset < inline_limit &&
-				   (end > extent_end) &&
-				   (inline_limit < extent_end)) {
-				u32 new_size;
-				new_size = btrfs_file_extent_calc_inline_size(
-						   inline_limit - key.offset);
-				inode_sub_bytes(inode, extent_end -
-						inline_limit);
-				btrfs_set_file_extent_ram_bytes(leaf, extent,
-							new_size);
-				if (!compression && !encryption) {
-					btrfs_truncate_item(trans, root, path,
-							    new_size, 1);
-				}
+			extent_offset += end - key.offset;
+			btrfs_set_file_extent_offset(leaf, fi, extent_offset);
+			btrfs_set_file_extent_num_bytes(leaf, fi,
+							extent_end - end);
+			btrfs_mark_buffer_dirty(leaf);
+			if (disk_bytenr > 0) {
+				inode_sub_bytes(inode, end - key.offset);
+				*hint_byte = disk_bytenr;
 			}
+			break;
 		}
-		/* delete the entire extent */
-		if (!keep) {
-			if (found_inline)
-				inode_sub_bytes(inode, extent_end -
-						key.offset);
-			ret = btrfs_del_item(trans, root, path);
-			/* TODO update progress marker and return */
-			BUG_ON(ret);
-			extent = NULL;
-			btrfs_release_path(root, path);
-			/* the extent will be freed later */
-		}
-		if (bookend && found_inline && start <= key.offset) {
-			u32 new_size;
-			new_size = btrfs_file_extent_calc_inline_size(
-						   extent_end - end);
-			inode_sub_bytes(inode, end - key.offset);
-			btrfs_set_file_extent_ram_bytes(leaf, extent,
-							new_size);
-			if (!compression && !encryption)
-				ret = btrfs_truncate_item(trans, root, path,
-							  new_size, 0);
-			BUG_ON(ret);
-		}
-		/* create bookend, splitting the extent in two */
-		if (bookend && found_extent) {
-			struct btrfs_key ins;
-			ins.objectid = inode->i_ino;
-			ins.offset = end;
-			btrfs_set_key_type(&ins, BTRFS_EXTENT_DATA_KEY);
 
-			btrfs_release_path(root, path);
-			path->leave_spinning = 1;
-			ret = btrfs_insert_empty_item(trans, root, path, &ins,
-						      sizeof(*extent));
-			BUG_ON(ret);
+		search_start = extent_end;
+		/*
+		 *       | ---- range to drop ----- |
+		 *  | -------- extent -------- |
+		 */
+		if (start > key.offset && end >= extent_end) {
+			BUG_ON(del_nr > 0);
+			BUG_ON(extent_type == BTRFS_FILE_EXTENT_INLINE);
 
-			leaf = path->nodes[0];
-			extent = btrfs_item_ptr(leaf, path->slots[0],
-						struct btrfs_file_extent_item);
-			write_extent_buffer(leaf, &old,
-					    (unsigned long)extent, sizeof(old));
-
-			btrfs_set_file_extent_compression(leaf, extent,
-							  compression);
-			btrfs_set_file_extent_encryption(leaf, extent,
-							 encryption);
-			btrfs_set_file_extent_other_encoding(leaf, extent,
-							     other_encoding);
-			btrfs_set_file_extent_offset(leaf, extent,
-				    le64_to_cpu(old.offset) + end - key.offset);
-			WARN_ON(le64_to_cpu(old.num_bytes) <
-				(extent_end - end));
-			btrfs_set_file_extent_num_bytes(leaf, extent,
-							extent_end - end);
+			btrfs_set_file_extent_num_bytes(leaf, fi,
+							start - key.offset);
+			btrfs_mark_buffer_dirty(leaf);
+			if (disk_bytenr > 0) {
+				inode_sub_bytes(inode, extent_end - start);
+				*hint_byte = disk_bytenr;
+			}
+			if (end == extent_end)
+				break;
 
-			/*
-			 * set the ram bytes to the size of the full extent
-			 * before splitting.  This is a worst case flag,
-			 * but its the best we can do because we don't know
-			 * how splitting affects compression
-			 */
-			btrfs_set_file_extent_ram_bytes(leaf, extent,
-							ram_bytes);
-			btrfs_set_file_extent_type(leaf, extent, found_type);
-
-			btrfs_unlock_up_safe(path, 1);
-			btrfs_mark_buffer_dirty(path->nodes[0]);
-			btrfs_set_lock_blocking(path->nodes[0]);
-
-			path->leave_spinning = 0;
-			btrfs_release_path(root, path);
-			if (disk_bytenr != 0)
-				inode_add_bytes(inode, extent_end - end);
+			path->slots[0]++;
+			goto next_slot;
 		}
 
-		if (found_extent && !keep) {
-			u64 old_disk_bytenr = le64_to_cpu(old.disk_bytenr);
+		/*
+		 *  | ---- range to drop ----- |
+		 *    | ------ extent ------ |
+		 */
+		if (start <= key.offset && end >= extent_end) {
+			if (del_nr == 0) {
+				del_slot = path->slots[0];
+				del_nr = 1;
+			} else {
+				BUG_ON(del_slot + del_nr != path->slots[0]);
+				del_nr++;
+			}
 
-			if (old_disk_bytenr != 0) {
+			if (extent_type == BTRFS_FILE_EXTENT_INLINE) {
 				inode_sub_bytes(inode,
-						le64_to_cpu(old.num_bytes));
+						extent_end - key.offset);
+				extent_end = ALIGN(extent_end,
+						   root->sectorsize);
+			} else if (disk_bytenr > 0) {
 				ret = btrfs_free_extent(trans, root,
-						old_disk_bytenr,
-						le64_to_cpu(old.disk_num_bytes),
-						0, root->root_key.objectid,
+						disk_bytenr, num_bytes, 0,
+						root->root_key.objectid,
 						key.objectid, key.offset -
-						le64_to_cpu(old.offset));
+						extent_offset);
 				BUG_ON(ret);
-				*hint_byte = old_disk_bytenr;
+				inode_sub_bytes(inode,
+						extent_end - key.offset);
+				*hint_byte = disk_bytenr;
 			}
-		}
 
-		if (search_start >= end) {
-			ret = 0;
-			goto out;
+			if (end == extent_end)
+				break;
+
+			if (path->slots[0] + 1 < btrfs_header_nritems(leaf)) {
+				path->slots[0]++;
+				goto next_slot;
+			}
+
+			ret = btrfs_del_items(trans, root, path, del_slot,
+					      del_nr);
+			BUG_ON(ret);
+
+			del_nr = 0;
+			del_slot = 0;
+
+			btrfs_release_path(root, path);
+			continue;
 		}
+
+		BUG_ON(1);
 	}
-out:
-	btrfs_free_path(path);
-	if (locked_end > orig_locked_end) {
-		unlock_extent(&BTRFS_I(inode)->io_tree, orig_locked_end,
-			      locked_end - 1, GFP_NOFS);
+
+	if (del_nr > 0) {
+		ret = btrfs_del_items(trans, root, path, del_slot, del_nr);
+		BUG_ON(ret);
 	}
+
+	btrfs_free_path(path);
 	return ret;
 }
 
@@ -620,23 +548,23 @@ static int extent_mergeable(struct extent_buffer *leaf, int slot,
  * two or three.
  */
 int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
-			      struct btrfs_root *root,
 			      struct inode *inode, u64 start, u64 end)
 {
+	struct btrfs_root *root = BTRFS_I(inode)->root;
 	struct extent_buffer *leaf;
 	struct btrfs_path *path;
 	struct btrfs_file_extent_item *fi;
 	struct btrfs_key key;
+	struct btrfs_key new_key;
 	u64 bytenr;
 	u64 num_bytes;
 	u64 extent_end;
 	u64 orig_offset;
 	u64 other_start;
 	u64 other_end;
-	u64 split = start;
-	u64 locked_end = end;
-	int extent_type;
-	int split_end = 1;
+	u64 split;
+	int del_nr = 0;
+	int del_slot = 0;
 	int ret;
 
 	btrfs_drop_extent_cache(inode, start, end - 1, 0);
@@ -644,12 +572,10 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 	path = btrfs_alloc_path();
 	BUG_ON(!path);
 again:
+	split = start;
 	key.objectid = inode->i_ino;
 	key.type = BTRFS_EXTENT_DATA_KEY;
-	if (split == start)
-		key.offset = split;
-	else
-		key.offset = split - 1;
+	key.offset = split;
 
 	ret = btrfs_search_slot(trans, root, &key, path, -1, 1);
 	if (ret > 0 && path->slots[0] > 0)
@@ -661,8 +587,8 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 	       key.type != BTRFS_EXTENT_DATA_KEY);
 	fi = btrfs_item_ptr(leaf, path->slots[0],
 			    struct btrfs_file_extent_item);
-	extent_type = btrfs_file_extent_type(leaf, fi);
-	BUG_ON(extent_type != BTRFS_FILE_EXTENT_PREALLOC);
+	BUG_ON(btrfs_file_extent_type(leaf, fi) !=
+	       BTRFS_FILE_EXTENT_PREALLOC);
 	extent_end = key.offset + btrfs_file_extent_num_bytes(leaf, fi);
 	BUG_ON(key.offset > start || extent_end < end);
 
@@ -670,150 +596,91 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 	num_bytes = btrfs_file_extent_disk_num_bytes(leaf, fi);
 	orig_offset = key.offset - btrfs_file_extent_offset(leaf, fi);
 
-	if (key.offset == start)
-		split = end;
-
-	if (key.offset == start && extent_end == end) {
-		int del_nr = 0;
-		int del_slot = 0;
-		other_start = end;
-		other_end = 0;
-		if (extent_mergeable(leaf, path->slots[0] + 1, inode->i_ino,
-				     bytenr, &other_start, &other_end)) {
-			extent_end = other_end;
-			del_slot = path->slots[0] + 1;
-			del_nr++;
-			ret = btrfs_free_extent(trans, root, bytenr, num_bytes,
-						0, root->root_key.objectid,
-						inode->i_ino, orig_offset);
-			BUG_ON(ret);
-		}
-		other_start = 0;
-		other_end = start;
-		if (extent_mergeable(leaf, path->slots[0] - 1, inode->i_ino,
-				     bytenr, &other_start, &other_end)) {
-			key.offset = other_start;
-			del_slot = path->slots[0];
-			del_nr++;
-			ret = btrfs_free_extent(trans, root, bytenr, num_bytes,
-						0, root->root_key.objectid,
-						inode->i_ino, orig_offset);
-			BUG_ON(ret);
-		}
-		split_end = 0;
-		if (del_nr == 0) {
-			btrfs_set_file_extent_type(leaf, fi,
-						   BTRFS_FILE_EXTENT_REG);
-			goto done;
+	while (start > key.offset || end < extent_end) {
+		if (key.offset == start)
+			split = end;
+
+		memcpy(&new_key, &key, sizeof(new_key));
+		new_key.offset = split;
+		ret = btrfs_duplicate_item(trans, root, path, &new_key);
+		if (ret == -EAGAIN) {
+			btrfs_release_path(root, path);
+			goto again;
 		}
+		BUG_ON(ret < 0);
 
-		fi = btrfs_item_ptr(leaf, del_slot - 1,
+		leaf = path->nodes[0];
+		fi = btrfs_item_ptr(leaf, path->slots[0] - 1,
 				    struct btrfs_file_extent_item);
-		btrfs_set_file_extent_type(leaf, fi, BTRFS_FILE_EXTENT_REG);
 		btrfs_set_file_extent_num_bytes(leaf, fi,
-						extent_end - key.offset);
+						split - key.offset);
+
+		fi = btrfs_item_ptr(leaf, path->slots[0],
+				    struct btrfs_file_extent_item);
+
+		btrfs_set_file_extent_offset(leaf, fi, split - orig_offset);
+		btrfs_set_file_extent_num_bytes(leaf, fi,
+						extent_end - split);
 		btrfs_mark_buffer_dirty(leaf);
 
-		ret = btrfs_del_items(trans, root, path, del_slot, del_nr);
+		ret = btrfs_inc_extent_ref(trans, root, bytenr, num_bytes, 0,
+					   root->root_key.objectid,
+					   inode->i_ino, orig_offset);
 		BUG_ON(ret);
-		goto release;
-	} else if (split == start) {
-		if (locked_end < extent_end) {
-			ret = try_lock_extent(&BTRFS_I(inode)->io_tree,
-					locked_end, extent_end - 1, GFP_NOFS);
-			if (!ret) {
-				btrfs_release_path(root, path);
-				lock_extent(&BTRFS_I(inode)->io_tree,
-					locked_end, extent_end - 1, GFP_NOFS);
-				locked_end = extent_end;
-				goto again;
-			}
-			locked_end = extent_end;
-		}
-		btrfs_set_file_extent_num_bytes(leaf, fi, split - key.offset);
-	} else  {
-		BUG_ON(key.offset != start);
-		key.offset = split;
-		btrfs_set_file_extent_offset(leaf, fi, key.offset -
-					     orig_offset);
-		btrfs_set_file_extent_num_bytes(leaf, fi, extent_end - split);
-		btrfs_set_item_key_safe(trans, root, path, &key);
-		extent_end = split;
-	}
 
-	if (extent_end == end) {
-		split_end = 0;
-		extent_type = BTRFS_FILE_EXTENT_REG;
-	}
-	if (extent_end == end && split == start) {
-		other_start = end;
-		other_end = 0;
-		if (extent_mergeable(leaf, path->slots[0] + 1, inode->i_ino,
-				     bytenr, &other_start, &other_end)) {
-			path->slots[0]++;
-			fi = btrfs_item_ptr(leaf, path->slots[0],
-					    struct btrfs_file_extent_item);
-			key.offset = split;
-			btrfs_set_item_key_safe(trans, root, path, &key);
-			btrfs_set_file_extent_offset(leaf, fi, key.offset -
-						     orig_offset);
-			btrfs_set_file_extent_num_bytes(leaf, fi,
-							other_end - split);
-			goto done;
-		}
-	}
-	if (extent_end == end && split == end) {
-		other_start = 0;
-		other_end = start;
-		if (extent_mergeable(leaf, path->slots[0] - 1 , inode->i_ino,
-				     bytenr, &other_start, &other_end)) {
+		if (split == start) {
+			key.offset = start;
+		} else {
+			BUG_ON(start != key.offset);
 			path->slots[0]--;
-			fi = btrfs_item_ptr(leaf, path->slots[0],
-					    struct btrfs_file_extent_item);
-			btrfs_set_file_extent_num_bytes(leaf, fi, extent_end -
-							other_start);
-			goto done;
+			extent_end = end;
 		}
 	}
 
-	btrfs_mark_buffer_dirty(leaf);
-
-	ret = btrfs_inc_extent_ref(trans, root, bytenr, num_bytes, 0,
-				   root->root_key.objectid,
-				   inode->i_ino, orig_offset);
-	BUG_ON(ret);
-	btrfs_release_path(root, path);
-
-	key.offset = start;
-	ret = btrfs_insert_empty_item(trans, root, path, &key, sizeof(*fi));
-	BUG_ON(ret);
-
-	leaf = path->nodes[0];
 	fi = btrfs_item_ptr(leaf, path->slots[0],
 			    struct btrfs_file_extent_item);
-	btrfs_set_file_extent_generation(leaf, fi, trans->transid);
-	btrfs_set_file_extent_type(leaf, fi, extent_type);
-	btrfs_set_file_extent_disk_bytenr(leaf, fi, bytenr);
-	btrfs_set_file_extent_disk_num_bytes(leaf, fi, num_bytes);
-	btrfs_set_file_extent_offset(leaf, fi, key.offset - orig_offset);
-	btrfs_set_file_extent_num_bytes(leaf, fi, extent_end - key.offset);
-	btrfs_set_file_extent_ram_bytes(leaf, fi, num_bytes);
-	btrfs_set_file_extent_compression(leaf, fi, 0);
-	btrfs_set_file_extent_encryption(leaf, fi, 0);
-	btrfs_set_file_extent_other_encoding(leaf, fi, 0);
-done:
-	btrfs_mark_buffer_dirty(leaf);
 
-release:
-	btrfs_release_path(root, path);
-	if (split_end && split == start) {
-		split = end;
-		goto again;
+	other_start = end;
+	other_end = 0;
+	if (extent_mergeable(leaf, path->slots[0] + 1, inode->i_ino,
+			     bytenr, &other_start, &other_end)) {
+		extent_end = other_end;
+		del_slot = path->slots[0] + 1;
+		del_nr++;
+		ret = btrfs_free_extent(trans, root, bytenr, num_bytes,
+					0, root->root_key.objectid,
+					inode->i_ino, orig_offset);
+		BUG_ON(ret);
 	}
-	if (locked_end > end) {
-		unlock_extent(&BTRFS_I(inode)->io_tree, end, locked_end - 1,
-			      GFP_NOFS);
+	other_start = 0;
+	other_end = start;
+	if (extent_mergeable(leaf, path->slots[0] - 1, inode->i_ino,
+			     bytenr, &other_start, &other_end)) {
+		key.offset = other_start;
+		del_slot = path->slots[0];
+		del_nr++;
+		ret = btrfs_free_extent(trans, root, bytenr, num_bytes,
+					0, root->root_key.objectid,
+					inode->i_ino, orig_offset);
+		BUG_ON(ret);
 	}
+	if (del_nr == 0) {
+		btrfs_set_file_extent_type(leaf, fi,
+					   BTRFS_FILE_EXTENT_REG);
+		btrfs_mark_buffer_dirty(leaf);
+		goto out;
+	}
+
+	fi = btrfs_item_ptr(leaf, del_slot - 1,
+			    struct btrfs_file_extent_item);
+	btrfs_set_file_extent_type(leaf, fi, BTRFS_FILE_EXTENT_REG);
+	btrfs_set_file_extent_num_bytes(leaf, fi,
+					extent_end - key.offset);
+	btrfs_mark_buffer_dirty(leaf);
+
+	ret = btrfs_del_items(trans, root, path, del_slot, del_nr);
+	BUG_ON(ret);
+out:
 	btrfs_free_path(path);
 	return 0;
 }

commit 6b2f3d1f769be5779b479c37800229d9a4809fc3
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Oct 27 11:05:28 2009 +0100

    vfs: Implement proper O_SYNC semantics
    
    While Linux provided an O_SYNC flag basically since day 1, it took until
    Linux 2.4.0-test12pre2 to actually get it implemented for filesystems,
    since that day we had generic_osync_around with only minor changes and the
    great "For now, when the user asks for O_SYNC, we'll actually give
    O_DSYNC" comment.  This patch intends to actually give us real O_SYNC
    semantics in addition to the O_DSYNC semantics.  After Jan's O_SYNC
    patches which are required before this patch it's actually surprisingly
    simple, we just need to figure out when to set the datasync flag to
    vfs_fsync_range and when not.
    
    This patch renames the existing O_SYNC flag to O_DSYNC while keeping it's
    numerical value to keep binary compatibility, and adds a new real O_SYNC
    flag.  To guarantee backwards compatiblity it is defined as expanding to
    both the O_DSYNC and the new additional binary flag (__O_SYNC) to make
    sure we are backwards-compatible when compiled against the new headers.
    
    This also means that all places that don't care about the differences can
    just check O_DSYNC and get the right behaviour for O_SYNC, too - only
    places that actuall care need to check __O_SYNC in addition.  Drivers and
    network filesystems have been updated in a fail safe way to always do the
    full sync magic if O_DSYNC is set.  The few places setting O_SYNC for
    lower layers are kept that way for now to stay failsafe.
    
    We enforce that O_DSYNC is set when __O_SYNC is set early in the open path
    to make sure we always get these sane options.
    
    Note that parisc really screwed up their headers as they already define a
    O_DSYNC that has always been a no-op.  We try to repair it by using it for
    the new O_DSYNC and redefinining O_SYNC to send both the traditional
    O_SYNC numerical value _and_ the O_DSYNC one.
    
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    Cc: Grant Grundler <grundler@parisc-linux.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Andreas Dilger <adilger@sun.com>
    Acked-by: Trond Myklebust <Trond.Myklebust@netapp.com>
    Acked-by: Kyle McMartin <kyle@mcmartin.ca>
    Acked-by: Ulrich Drepper <drepper@redhat.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Jan Kara <jack@suse.cz>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 06550affbd27..77f759302e12 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -909,7 +909,7 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 	unsigned long last_index;
 	int will_write;
 
-	will_write = ((file->f_flags & O_SYNC) || IS_SYNC(inode) ||
+	will_write = ((file->f_flags & O_DSYNC) || IS_SYNC(inode) ||
 		      (file->f_flags & O_DIRECT));
 
 	nrptrs = min((count + PAGE_CACHE_SIZE - 1) / PAGE_CACHE_SIZE,
@@ -1076,7 +1076,7 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 		if (err)
 			num_written = err;
 
-		if ((file->f_flags & O_SYNC) || IS_SYNC(inode)) {
+		if ((file->f_flags & O_DSYNC) || IS_SYNC(inode)) {
 			trans = btrfs_start_transaction(root, 1);
 			ret = btrfs_log_dentry_safe(trans, root,
 						    file->f_dentry);

commit dcbeb0bec5f2695c3ff53f174efb8e03c209f3f3
Merge: 2b650df2cea9 444528b3e614
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 15 15:06:37 2009 -0700

    Merge branch 'master' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/btrfs-unstable
    
    * 'master' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/btrfs-unstable:
      Btrfs: always pin metadata in discard mode
      Btrfs: enable discard support
      Btrfs: add -o discard option
      Btrfs: properly wait log writers during log sync
      Btrfs: fix possible ENOSPC problems with truncate
      Btrfs: fix btrfs acl #ifdef checks
      Btrfs: streamline tree-log btree block writeout
      Btrfs: avoid tree log commit when there are no changes
      Btrfs: only write one super copy during fsync

commit 257c62e1bce03e5b9f3f069fd52ad73a56de71fd
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Oct 13 13:21:08 2009 -0400

    Btrfs: avoid tree log commit when there are no changes
    
    rpm has a habit of running fdatasync when the file hasn't
    changed.  We already detect if a file hasn't been changed
    in the current transaction but it might have been sent to
    the tree-log in this transaction and not changed since
    the last call to fsync.
    
    In this case, we want to avoid a tree log sync, which includes
    a number of synchronous writes and barriers.  This commit
    extends the existing tracking of the last transaction to change
    a file to also track the last sub-transaction.
    
    The end result is that rpm -ivh and -Uvh are roughly twice as fast,
    and on par with ext3.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 53fb1c997f0e..4599113ed72e 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1087,8 +1087,10 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 					btrfs_end_transaction(trans, root);
 				else
 					btrfs_commit_transaction(trans, root);
-			} else {
+			} else if (ret != BTRFS_NO_LOG_SYNC) {
 				btrfs_commit_transaction(trans, root);
+			} else {
+				btrfs_end_transaction(trans, root);
 			}
 		}
 		if (file->f_flags & O_DIRECT) {
@@ -1138,6 +1140,13 @@ int btrfs_sync_file(struct file *file, struct dentry *dentry, int datasync)
 	int ret = 0;
 	struct btrfs_trans_handle *trans;
 
+
+	/* we wait first, since the writeback may change the inode */
+	root->log_batch++;
+	/* the VFS called filemap_fdatawrite for us */
+	btrfs_wait_ordered_range(inode, 0, (u64)-1);
+	root->log_batch++;
+
 	/*
 	 * check the transaction that last modified this inode
 	 * and see if its already been committed
@@ -1145,6 +1154,11 @@ int btrfs_sync_file(struct file *file, struct dentry *dentry, int datasync)
 	if (!BTRFS_I(inode)->last_trans)
 		goto out;
 
+	/*
+	 * if the last transaction that changed this file was before
+	 * the current transaction, we can bail out now without any
+	 * syncing
+	 */
 	mutex_lock(&root->fs_info->trans_mutex);
 	if (BTRFS_I(inode)->last_trans <=
 	    root->fs_info->last_trans_committed) {
@@ -1154,13 +1168,6 @@ int btrfs_sync_file(struct file *file, struct dentry *dentry, int datasync)
 	}
 	mutex_unlock(&root->fs_info->trans_mutex);
 
-	root->log_batch++;
-	filemap_fdatawrite(inode->i_mapping);
-	btrfs_wait_ordered_range(inode, 0, (u64)-1);
-	root->log_batch++;
-
-	if (datasync && !(inode->i_state & I_DIRTY_PAGES))
-		goto out;
 	/*
 	 * ok we haven't committed the transaction yet, lets do a commit
 	 */
@@ -1189,14 +1196,18 @@ int btrfs_sync_file(struct file *file, struct dentry *dentry, int datasync)
 	 */
 	mutex_unlock(&dentry->d_inode->i_mutex);
 
-	if (ret > 0) {
-		ret = btrfs_commit_transaction(trans, root);
-	} else {
-		ret = btrfs_sync_log(trans, root);
-		if (ret == 0)
-			ret = btrfs_end_transaction(trans, root);
-		else
+	if (ret != BTRFS_NO_LOG_SYNC) {
+		if (ret > 0) {
 			ret = btrfs_commit_transaction(trans, root);
+		} else {
+			ret = btrfs_sync_log(trans, root);
+			if (ret == 0)
+				ret = btrfs_end_transaction(trans, root);
+			else
+				ret = btrfs_commit_transaction(trans, root);
+		}
+	} else {
+		ret = btrfs_end_transaction(trans, root);
 	}
 	mutex_lock(&dentry->d_inode->i_mutex);
 out:

commit 474a503d4bf77ae0cbe484dd0842a2648c0b1c28
Merge: d43c36dc6b35 ac6889cbb254
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Oct 11 11:23:13 2009 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/mason/btrfs-unstable
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/mason/btrfs-unstable:
      Btrfs: fix file clone ioctl for bookend extents
      Btrfs: fix uninit compiler warning in cow_file_range_nocow
      Btrfs: constify dentry_operations
      Btrfs: optimize back reference update during btrfs_drop_snapshot
      Btrfs: remove negative dentry when deleting subvolumne
      Btrfs: optimize fsync for the single writer case
      Btrfs: async delalloc flushing under space pressure
      Btrfs: release delalloc reservations on extent item insertion
      Btrfs: delay clearing EXTENT_DELALLOC for compressed extents
      Btrfs: cleanup extent_clear_unlock_delalloc flags
      Btrfs: fix possible softlockup in the allocator
      Btrfs: fix deadlock on async thread startup

commit 32c00aff718bb54a214b39146bdd9ac01511cd25
Author: Josef Bacik <josef@redhat.com>
Date:   Thu Oct 8 13:34:05 2009 -0400

    Btrfs: release delalloc reservations on extent item insertion
    
    This patch fixes an issue with the delalloc metadata space reservation
    code.  The problem is we used to free the reservation as soon as we
    allocated the delalloc region.  The problem with this is if we are not
    inserting an inline extent, we don't actually insert the extent item until
    after the ordered extent is written out.  This patch does 3 things,
    
    1) It moves the reservation clearing stuff into the ordered code, so when
    we remove the ordered extent we remove the reservation.
    2) It adds a EXTENT_DO_ACCOUNTING flag that gets passed when we clear
    delalloc bits in the cases where we want to clear the metadata reservation
    when we clear the delalloc extent, in the case that we do an inline extent
    or we invalidate the page.
    3) It adds another waitqueue to the space info so that when we start a fs
    wide delalloc flush, anybody else who also hits that area will simply wait
    for the flush to finish and then try to make their allocation.
    
    This has been tested thoroughly to make sure we did not regress on
    performance.
    
    Signed-off-by: Josef Bacik <jbacik@redhat.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index f155179877a6..53fb1c997f0e 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -878,7 +878,8 @@ static noinline int prepare_pages(struct btrfs_root *root, struct file *file,
 			btrfs_put_ordered_extent(ordered);
 
 		clear_extent_bits(&BTRFS_I(inode)->io_tree, start_pos,
-				  last_pos - 1, EXTENT_DIRTY | EXTENT_DELALLOC,
+				  last_pos - 1, EXTENT_DIRTY | EXTENT_DELALLOC |
+				  EXTENT_DO_ACCOUNTING,
 				  GFP_NOFS);
 		unlock_extent(&BTRFS_I(inode)->io_tree,
 			      start_pos, last_pos - 1, GFP_NOFS);

commit 0efe5e32c8729ef44b00d9a7203e4c99a6378b27
Merge: e6a0a8bfef10 9c2693c9243b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 1 20:23:15 2009 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/btrfs-unstable
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/btrfs-unstable:
      Btrfs: fix data space leak fix
      Btrfs: remove duplicates of filemap_ helpers
      Btrfs: take i_mutex before generic_write_checks
      Btrfs: fix arguments to btrfs_wait_on_page_writeback_range
      Btrfs: fix deadlock with free space handling and user transactions
      Btrfs: fix error cases for ioctl transactions
      Btrfs: Use CONFIG_BTRFS_POSIX_ACL to enable ACL code
      Btrfs: introduce missing kfree
      Btrfs: Fix setting umask when POSIX ACLs are not enabled
      Btrfs: proper -ENOSPC handling

commit 828c09509b9695271bcbdc53e9fc9a6a737148d2
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Thu Oct 1 15:43:56 2009 -0700

    const: constify remaining file_operations
    
    [akpm@linux-foundation.org: fix KVM]
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Acked-by: Mike Frysinger <vapier@gentoo.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index a3492a3ad96b..9ed17dbe5c6e 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1196,7 +1196,7 @@ static int btrfs_file_mmap(struct file	*filp, struct vm_area_struct *vma)
 	return 0;
 }
 
-struct file_operations btrfs_file_operations = {
+const struct file_operations btrfs_file_operations = {
 	.llseek		= generic_file_llseek,
 	.read		= do_sync_read,
 	.aio_read       = generic_file_aio_read,

commit 8aa38c31b7659e338fee4d9af4c3805acbd9806f
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Oct 1 12:58:30 2009 -0400

    Btrfs: remove duplicates of filemap_ helpers
    
    Use filemap_fdatawrite_range and filemap_fdatawait_range instead of
    local copies of the functions.  For filemap_fdatawait_range that
    also means replacing the awkward old wait_on_page_writeback_range
    calling convention with the regular filemap byte offsets.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 7351bdbca26f..ca784a7fbeba 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1022,9 +1022,8 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 		}
 
 		if (will_write) {
-			btrfs_fdatawrite_range(inode->i_mapping, pos,
-					       pos + write_bytes - 1,
-					       WB_SYNC_ALL);
+			filemap_fdatawrite_range(inode->i_mapping, pos,
+						 pos + write_bytes - 1);
 		} else {
 			balance_dirty_pages_ratelimited_nr(inode->i_mapping,
 							   num_pages);

commit 25472b880c69c0daa485c4f80a6550437ed1149f
Merge: 17d857be649a ab93dbecfba7
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Oct 1 12:58:13 2009 -0400

    Merge branch 'master' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/btrfs-unstable into for-linus

commit ab93dbecfba72bbc04b7036343d180aaff1b61a3
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Oct 1 12:29:10 2009 -0400

    Btrfs: take i_mutex before generic_write_checks
    
    btrfs_file_write was incorrectly calling generic_write_checks without
    taking i_mutex.  This lead to problems with racing around i_size when
    doing O_APPEND writes.
    
    The fix here is to move i_mutex higher.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 1be96ba6f6bb..f155179877a6 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -920,26 +920,35 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 	start_pos = pos;
 
 	vfs_check_frozen(inode->i_sb, SB_FREEZE_WRITE);
+
+	/* do the reserve before the mutex lock in case we have to do some
+	 * flushing.  We wouldn't deadlock, but this is more polite.
+	 */
+	err = btrfs_reserve_metadata_for_delalloc(root, inode, 1);
+	if (err)
+		goto out_nolock;
+
+	mutex_lock(&inode->i_mutex);
+
 	current->backing_dev_info = inode->i_mapping->backing_dev_info;
 	err = generic_write_checks(file, &pos, &count, S_ISBLK(inode->i_mode));
 	if (err)
-		goto out_nolock;
+		goto out;
+
 	if (count == 0)
-		goto out_nolock;
+		goto out;
 
 	err = file_remove_suid(file);
 	if (err)
-		goto out_nolock;
-
-	err = btrfs_reserve_metadata_for_delalloc(root, inode, 1);
-	if (err)
-		goto out_nolock;
+		goto out;
 
 	file_update_time(file);
 
 	pages = kmalloc(nrptrs * sizeof(struct page *), GFP_KERNEL);
 
-	mutex_lock(&inode->i_mutex);
+	/* generic_write_checks can change our pos */
+	start_pos = pos;
+
 	BTRFS_I(inode)->sequence++;
 	first_index = pos >> PAGE_CACHE_SHIFT;
 	last_index = (pos + count) >> PAGE_CACHE_SHIFT;

commit 9ed74f2dba6ebf9f30b80554290bfc73cc3ef083
Author: Josef Bacik <josef@redhat.com>
Date:   Fri Sep 11 16:12:44 2009 -0400

    Btrfs: proper -ENOSPC handling
    
    At the start of a transaction we do a btrfs_reserve_metadata_space() and
    specify how many items we plan on modifying.  Then once we've done our
    modifications and such, just call btrfs_unreserve_metadata_space() for
    the same number of items we reserved.
    
    For keeping track of metadata needed for data I've had to add an extent_io op
    for when we merge extents.  This lets us track space properly when we are doing
    sequential writes, so we don't end up reserving way more metadata space than
    what we need.
    
    The only place where the metadata space accounting is not done is in the
    relocation code.  This is because Yan is going to be reworking that code in the
    near future, so running btrfs-vol -b could still possibly result in a ENOSPC
    related panic.  This patch also turns off the metadata_ratio stuff in order to
    allow users to more efficiently use their disk space.
    
    This patch makes it so we track how much metadata we need for an inode's
    delayed allocation extents by tracking how many extents are currently
    waiting for allocation.  It introduces two new callbacks for the
    extent_io tree's, merge_extent_hook and split_extent_hook.  These help
    us keep track of when we merge delalloc extents together and split them
    up.  Reservations are handled prior to any actually dirty'ing occurs,
    and then we unreserve after we dirty.
    
    btrfs_unreserve_metadata_for_delalloc() will make the appropriate
    unreservations as needed based on the number of reservations we
    currently have and the number of extents we currently have.  Doing the
    reservation outside of doing any of the actual dirty'ing lets us do
    things like filemap_flush() the inode to try and force delalloc to
    happen, or as a last resort actually start allocation on all delalloc
    inodes in the fs.  This has survived dbench, fs_mark and an fsx torture
    test.
    
    Signed-off-by: Josef Bacik <jbacik@redhat.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 571ad3c13b47..1be96ba6f6bb 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -123,7 +123,10 @@ static noinline int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 		    root->sectorsize - 1) & ~((u64)root->sectorsize - 1);
 
 	end_of_last_block = start_pos + num_bytes - 1;
-	btrfs_set_extent_delalloc(inode, start_pos, end_of_last_block);
+	err = btrfs_set_extent_delalloc(inode, start_pos, end_of_last_block);
+	if (err)
+		return err;
+
 	for (i = 0; i < num_pages; i++) {
 		struct page *p = pages[i];
 		SetPageUptodate(p);
@@ -927,6 +930,11 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 	err = file_remove_suid(file);
 	if (err)
 		goto out_nolock;
+
+	err = btrfs_reserve_metadata_for_delalloc(root, inode, 1);
+	if (err)
+		goto out_nolock;
+
 	file_update_time(file);
 
 	pages = kmalloc(nrptrs * sizeof(struct page *), GFP_KERNEL);
@@ -1028,6 +1036,7 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 	mutex_unlock(&inode->i_mutex);
 	if (ret)
 		err = ret;
+	btrfs_unreserve_metadata_for_delalloc(root, inode, 1);
 
 out_nolock:
 	kfree(pages);

commit f0f37e2f77731b3473fa6bd5ee53255d9a9cdb40
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Sun Sep 27 22:29:37 2009 +0400

    const: mark struct vm_struct_operations
    
    * mark struct vm_area_struct::vm_ops as const
    * mark vm_ops in AGP code
    
    But leave TTM code alone, something is fishy there with global vm_ops
    being used.
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 571ad3c13b47..a3492a3ad96b 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1184,7 +1184,7 @@ int btrfs_sync_file(struct file *file, struct dentry *dentry, int datasync)
 	return ret > 0 ? EIO : ret;
 }
 
-static struct vm_operations_struct btrfs_file_vm_ops = {
+static const struct vm_operations_struct btrfs_file_vm_ops = {
 	.fault		= filemap_fault,
 	.page_mkwrite	= btrfs_page_mkwrite,
 };

commit 83ebade34bc1a90d0c3f77b87b940f336d075fda
Merge: 74fca6a42863 93c82d575055
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Sep 11 19:07:25 2009 -0400

    Merge branch 'master' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/btrfs-unstable

commit a1ed835e1ab5795f91b198d08c43e2f56848dcf3
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Sep 11 12:27:37 2009 -0400

    Btrfs: Fix extent replacment race
    
    Data COW means that whenever we write to a file, we replace any old
    extent pointers with new ones.  There was a window where a readpage
    might find the old extent pointers on disk and cache them in the
    extent_map tree in ram in the middle of a given write replacing them.
    
    Even though both the readpage and the write had their respective bytes
    in the file locked, the extent readpage inserts may cover more bytes than
    it had locked down.
    
    This commit closes the race by keeping the new extent pinned in the extent
    map tree until after the on-disk btree is properly setup with the new
    extent pointers.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index ef66c3d989b9..4123db9d5141 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -177,10 +177,10 @@ int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
 		}
 		flags = em->flags;
 		if (skip_pinned && test_bit(EXTENT_FLAG_PINNED, &em->flags)) {
-			write_unlock(&em_tree->lock);
 			if (em->start <= start &&
 			    (!testend || em->start + em->len >= start + len)) {
 				free_extent_map(em);
+				write_unlock(&em_tree->lock);
 				break;
 			}
 			if (start < em->start) {
@@ -190,6 +190,7 @@ int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
 				start = em->start + em->len;
 			}
 			free_extent_map(em);
+			write_unlock(&em_tree->lock);
 			continue;
 		}
 		compressed = test_bit(EXTENT_FLAG_COMPRESSED, &em->flags);
@@ -269,7 +270,7 @@ int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
 noinline int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 		       struct btrfs_root *root, struct inode *inode,
 		       u64 start, u64 end, u64 locked_end,
-		       u64 inline_limit, u64 *hint_byte)
+		       u64 inline_limit, u64 *hint_byte, int drop_cache)
 {
 	u64 extent_end = 0;
 	u64 search_start = start;
@@ -294,7 +295,8 @@ noinline int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 	int ret;
 
 	inline_limit = 0;
-	btrfs_drop_extent_cache(inode, start, end - 1, 0);
+	if (drop_cache)
+		btrfs_drop_extent_cache(inode, start, end - 1, 0);
 
 	path = btrfs_alloc_path();
 	if (!path)

commit 1edbb734b4e010974c41d2859d22a43d04f5f1cf
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Sep 2 13:24:36 2009 -0400

    Btrfs: reduce CPU usage in the extent_state tree
    
    Btrfs is currently mirroring some of the page state bits into
    its extent state tree.  The goal behind this was to use it in supporting
    blocksizes other than the page size.
    
    But, we don't currently support that, and we're using quite a lot of CPU
    on the rb tree and its spin lock.  This commit starts a series of
    cleanups to reduce the amount of work done in the extent state tree as
    part of each IO.
    
    This commit:
    
    * Adds the ability to lock an extent in the state tree and also set
    other bits.  The idea is to do locking and delalloc in one call
    
    * Removes the EXTENT_WRITEBACK and EXTENT_DIRTY bits.  Btrfs is using
    a combination of the page bits and the ordered write code for this
    instead.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 8a9c76aecdf3..ef66c3d989b9 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -113,8 +113,6 @@ static noinline int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 	int err = 0;
 	int i;
 	struct inode *inode = fdentry(file)->d_inode;
-	struct extent_io_tree *io_tree = &BTRFS_I(inode)->io_tree;
-	u64 hint_byte;
 	u64 num_bytes;
 	u64 start_pos;
 	u64 end_of_last_block;
@@ -126,20 +124,6 @@ static noinline int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 		    root->sectorsize - 1) & ~((u64)root->sectorsize - 1);
 
 	end_of_last_block = start_pos + num_bytes - 1;
-
-	lock_extent(io_tree, start_pos, end_of_last_block, GFP_NOFS);
-	trans = btrfs_join_transaction(root, 1);
-	if (!trans) {
-		err = -ENOMEM;
-		goto out_unlock;
-	}
-	btrfs_set_trans_block_group(trans, inode);
-	hint_byte = 0;
-
-	/* check for reserved extents on each page, we don't want
-	 * to reset the delalloc bit on things that already have
-	 * extents reserved.
-	 */
 	btrfs_set_extent_delalloc(inode, start_pos, end_of_last_block);
 	for (i = 0; i < num_pages; i++) {
 		struct page *p = pages[i];
@@ -154,9 +138,6 @@ static noinline int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 		 * at this time.
 		 */
 	}
-	err = btrfs_end_transaction(trans, root);
-out_unlock:
-	unlock_extent(io_tree, start_pos, end_of_last_block, GFP_NOFS);
 	return err;
 }
 

commit 890871be854b5f5e43e7ba2475f706209906cc24
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Sep 2 16:24:52 2009 -0400

    Btrfs: switch extent_map to a rw lock
    
    There are two main users of the extent_map tree.  The
    first is regular file inodes, where it is evenly spread
    between readers and writers.
    
    The second is the chunk allocation tree, which maps blocks from
    logical addresses to phyiscal ones, and it is 99.99% reads.
    
    The mapping tree is a point of lock contention during heavy IO
    workloads, so this commit switches things to a rw lock.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index a760d97279ac..8a9c76aecdf3 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -188,15 +188,15 @@ int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
 		if (!split2)
 			split2 = alloc_extent_map(GFP_NOFS);
 
-		spin_lock(&em_tree->lock);
+		write_lock(&em_tree->lock);
 		em = lookup_extent_mapping(em_tree, start, len);
 		if (!em) {
-			spin_unlock(&em_tree->lock);
+			write_unlock(&em_tree->lock);
 			break;
 		}
 		flags = em->flags;
 		if (skip_pinned && test_bit(EXTENT_FLAG_PINNED, &em->flags)) {
-			spin_unlock(&em_tree->lock);
+			write_unlock(&em_tree->lock);
 			if (em->start <= start &&
 			    (!testend || em->start + em->len >= start + len)) {
 				free_extent_map(em);
@@ -259,7 +259,7 @@ int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
 			free_extent_map(split);
 			split = NULL;
 		}
-		spin_unlock(&em_tree->lock);
+		write_unlock(&em_tree->lock);
 
 		/* once for us */
 		free_extent_map(em);

commit 40431d6c1288793a682fc6f5e5b5c9d5cac34608
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Aug 5 12:57:59 2009 -0400

    Btrfs: optimize set extent bit
    
    The Btrfs set_extent_bit call currently searches the rbtree
    every time it needs to find more extent_state objects to fill
    the requested operation.
    
    This adds a simple test with rb_next to see if the next object
    in the tree was adjacent to the one we just found.  If so,
    we skip the search and just use the next object.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 7c3cd248d8d6..a760d97279ac 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -136,8 +136,6 @@ static noinline int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 	btrfs_set_trans_block_group(trans, inode);
 	hint_byte = 0;
 
-	set_extent_uptodate(io_tree, start_pos, end_of_last_block, GFP_NOFS);
-
 	/* check for reserved extents on each page, we don't want
 	 * to reset the delalloc bit on things that already have
 	 * extents reserved.

commit 405f55712dfe464b3240d7816cc4fe4174831be2
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Sat Jul 11 22:08:37 2009 +0400

    headers: smp_lock.h redux
    
    * Remove smp_lock.h from files which don't need it (including some headers!)
    * Add smp_lock.h to files which do need it
    * Make smp_lock.h include conditional in hardirq.h
      It's needed only for one kernel_locked() usage which is under CONFIG_PREEMPT
    
      This will make hardirq.h inclusion cheaper for every PREEMPT=n config
      (which includes allmodconfig/allyesconfig, BTW)
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 7c3cd248d8d6..4b833972273a 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -22,7 +22,6 @@
 #include <linux/time.h>
 #include <linux/init.h>
 #include <linux/string.h>
-#include <linux/smp_lock.h>
 #include <linux/backing-dev.h>
 #include <linux/mpage.h>
 #include <linux/swap.h>

commit f597bb19ccd034cbcf05e1194238e2c8d9505a8a
Author: Chris Mason <chris.mason@oracle.com>
Date:   Sat Jun 27 21:06:22 2009 -0400

    Btrfs: don't log the inode in file_write while growing the file

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 126477eaecf5..7c3cd248d8d6 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -151,7 +151,10 @@ static noinline int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 	}
 	if (end_pos > isize) {
 		i_size_write(inode, end_pos);
-		btrfs_update_inode(trans, root, inode);
+		/* we've only changed i_size in ram, and we haven't updated
+		 * the disk i_size.  There is no need to log the inode
+		 * at this time.
+		 */
 	}
 	err = btrfs_end_transaction(trans, root);
 out_unlock:

commit 524724ed1f224875a117be593540591ed050c73d
Author: Hisashi Hifumi <hifumi.hisashi@oss.ntt.co.jp>
Date:   Wed Jun 10 11:13:17 2009 -0400

    Btrfs: fdatasync should skip metadata writeout
    
    In btrfs, fdatasync and fsync are identical, but
    fdatasync should skip committing transaction when
    inode->i_state is set just I_DIRTY_SYNC and this indicates
    only atime or/and mtime updates.
    Following patch improves fdatasync throughput.
    
    --file-block-size=4K --file-total-size=16G --file-test-mode=rndwr
    --file-fsync-mode=fdatasync run
    
    Results:
    -2.6.30-rc8
    Test execution summary:
        total time:                          1980.6540s
        total number of events:              10001
        total time taken by event execution: 1192.9804
        per-request statistics:
             min:                            0.0000s
             avg:                            0.1193s
             max:                            15.3720s
             approx.  95 percentile:         0.7257s
    
    Threads fairness:
        events (avg/stddev):           625.0625/151.32
        execution time (avg/stddev):   74.5613/9.46
    
    -2.6.30-rc8-patched
    Test execution summary:
        total time:                          1695.9118s
        total number of events:              10000
        total time taken by event execution: 871.3214
        per-request statistics:
             min:                            0.0000s
             avg:                            0.0871s
             max:                            10.4644s
             approx.  95 percentile:         0.4787s
    
    Threads fairness:
        events (avg/stddev):           625.0000/131.86
        execution time (avg/stddev):   54.4576/8.98
    
    Signed-off-by: Hisashi Hifumi <hifumi.hisashi@oss.ntt.co.jp>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 0726a734ee38..126477eaecf5 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1157,6 +1157,8 @@ int btrfs_sync_file(struct file *file, struct dentry *dentry, int datasync)
 	btrfs_wait_ordered_range(inode, 0, (u64)-1);
 	root->log_batch++;
 
+	if (datasync && !(inode->i_state & I_DIRTY_PAGES))
+		goto out;
 	/*
 	 * ok we haven't committed the transaction yet, lets do a commit
 	 */

commit 5d4f98a28c7d334091c1b7744f48a1acdd2a4ae0
Author: Yan Zheng <zheng.yan@oracle.com>
Date:   Wed Jun 10 10:45:14 2009 -0400

    Btrfs: Mixed back reference  (FORWARD ROLLING FORMAT CHANGE)
    
    This commit introduces a new kind of back reference for btrfs metadata.
    Once a filesystem has been mounted with this commit, IT WILL NO LONGER
    BE MOUNTABLE BY OLDER KERNELS.
    
    When a tree block in subvolume tree is cow'd, the reference counts of all
    extents it points to are increased by one.  At transaction commit time,
    the old root of the subvolume is recorded in a "dead root" data structure,
    and the btree it points to is later walked, dropping reference counts
    and freeing any blocks where the reference count goes to 0.
    
    The increments done during cow and decrements done after commit cancel out,
    and the walk is a very expensive way to go about freeing the blocks that
    are no longer referenced by the new btree root.  This commit reduces the
    transaction overhead by avoiding the need for dead root records.
    
    When a non-shared tree block is cow'd, we free the old block at once, and the
    new block inherits old block's references. When a tree block with reference
    count > 1 is cow'd, we increase the reference counts of all extents
    the new block points to by one, and decrease the old block's reference count by
    one.
    
    This dead tree avoidance code removes the need to modify the reference
    counts of lower level extents when a non-shared tree block is cow'd.
    But we still need to update back ref for all pointers in the block.
    This is because the location of the block is recorded in the back ref
    item.
    
    We can solve this by introducing a new type of back ref. The new
    back ref provides information about pointer's key, level and in which
    tree the pointer lives. This information allow us to find the pointer
    by searching the tree. The shortcoming of the new back ref is that it
    only works for pointers in tree blocks referenced by their owner trees.
    
    This is mostly a problem for snapshots, where resolving one of these
    fuzzy back references would be O(number_of_snapshots) and quite slow.
    The solution used here is to use the fuzzy back references in the common
    case where a given tree block is only referenced by one root,
    and use the full back references when multiple roots have a reference
    on a given block.
    
    This commit adds per subvolume red-black tree to keep trace of cached
    inodes. The red-black tree helps the balancing code to find cached
    inodes whose inode numbers within a given range.
    
    This commit improves the balancing code by introducing several data
    structures to keep the state of balancing. The most important one
    is the back ref cache. It caches how the upper level tree blocks are
    referenced. This greatly reduce the overhead of checking back ref.
    
    The improved balancing code scales significantly better with a large
    number of snapshots.
    
    This is a very large commit and was written in a number of
    pieces.  But, they depend heavily on the disk format change and were
    squashed together to make sure git bisect didn't end up in a
    bad state wrt space balancing or the format change.
    
    Signed-off-by: Yan Zheng <zheng.yan@oracle.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 1d51dc38bb49..0726a734ee38 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -291,16 +291,12 @@ noinline int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 {
 	u64 extent_end = 0;
 	u64 search_start = start;
-	u64 leaf_start;
 	u64 ram_bytes = 0;
-	u64 orig_parent = 0;
 	u64 disk_bytenr = 0;
 	u64 orig_locked_end = locked_end;
 	u8 compression;
 	u8 encryption;
 	u16 other_encoding = 0;
-	u64 root_gen;
-	u64 root_owner;
 	struct extent_buffer *leaf;
 	struct btrfs_file_extent_item *extent;
 	struct btrfs_path *path;
@@ -340,9 +336,6 @@ noinline int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 		bookend = 0;
 		found_extent = 0;
 		found_inline = 0;
-		leaf_start = 0;
-		root_gen = 0;
-		root_owner = 0;
 		compression = 0;
 		encryption = 0;
 		extent = NULL;
@@ -417,9 +410,6 @@ noinline int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 		if (found_extent) {
 			read_extent_buffer(leaf, &old, (unsigned long)extent,
 					   sizeof(old));
-			root_gen = btrfs_header_generation(leaf);
-			root_owner = btrfs_header_owner(leaf);
-			leaf_start = leaf->start;
 		}
 
 		if (end < extent_end && end >= key.offset) {
@@ -443,14 +433,14 @@ noinline int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 				}
 				locked_end = extent_end;
 			}
-			orig_parent = path->nodes[0]->start;
 			disk_bytenr = le64_to_cpu(old.disk_bytenr);
 			if (disk_bytenr != 0) {
 				ret = btrfs_inc_extent_ref(trans, root,
 					   disk_bytenr,
-					   le64_to_cpu(old.disk_num_bytes),
-					   orig_parent, root->root_key.objectid,
-					   trans->transid, inode->i_ino);
+					   le64_to_cpu(old.disk_num_bytes), 0,
+					   root->root_key.objectid,
+					   key.objectid, key.offset -
+					   le64_to_cpu(old.offset));
 				BUG_ON(ret);
 			}
 		}
@@ -568,17 +558,6 @@ noinline int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 			btrfs_mark_buffer_dirty(path->nodes[0]);
 			btrfs_set_lock_blocking(path->nodes[0]);
 
-			if (disk_bytenr != 0) {
-				ret = btrfs_update_extent_ref(trans, root,
-						disk_bytenr,
-						le64_to_cpu(old.disk_num_bytes),
-						orig_parent,
-						leaf->start,
-						root->root_key.objectid,
-						trans->transid, ins.objectid);
-
-				BUG_ON(ret);
-			}
 			path->leave_spinning = 0;
 			btrfs_release_path(root, path);
 			if (disk_bytenr != 0)
@@ -594,8 +573,9 @@ noinline int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 				ret = btrfs_free_extent(trans, root,
 						old_disk_bytenr,
 						le64_to_cpu(old.disk_num_bytes),
-						leaf_start, root_owner,
-						root_gen, key.objectid, 0);
+						0, root->root_key.objectid,
+						key.objectid, key.offset -
+						le64_to_cpu(old.offset));
 				BUG_ON(ret);
 				*hint_byte = old_disk_bytenr;
 			}
@@ -664,12 +644,11 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 	u64 bytenr;
 	u64 num_bytes;
 	u64 extent_end;
-	u64 extent_offset;
+	u64 orig_offset;
 	u64 other_start;
 	u64 other_end;
 	u64 split = start;
 	u64 locked_end = end;
-	u64 orig_parent;
 	int extent_type;
 	int split_end = 1;
 	int ret;
@@ -703,7 +682,7 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 
 	bytenr = btrfs_file_extent_disk_bytenr(leaf, fi);
 	num_bytes = btrfs_file_extent_disk_num_bytes(leaf, fi);
-	extent_offset = btrfs_file_extent_offset(leaf, fi);
+	orig_offset = key.offset - btrfs_file_extent_offset(leaf, fi);
 
 	if (key.offset == start)
 		split = end;
@@ -711,8 +690,6 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 	if (key.offset == start && extent_end == end) {
 		int del_nr = 0;
 		int del_slot = 0;
-		u64 leaf_owner = btrfs_header_owner(leaf);
-		u64 leaf_gen = btrfs_header_generation(leaf);
 		other_start = end;
 		other_end = 0;
 		if (extent_mergeable(leaf, path->slots[0] + 1, inode->i_ino,
@@ -721,8 +698,8 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 			del_slot = path->slots[0] + 1;
 			del_nr++;
 			ret = btrfs_free_extent(trans, root, bytenr, num_bytes,
-						leaf->start, leaf_owner,
-						leaf_gen, inode->i_ino, 0);
+						0, root->root_key.objectid,
+						inode->i_ino, orig_offset);
 			BUG_ON(ret);
 		}
 		other_start = 0;
@@ -733,8 +710,8 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 			del_slot = path->slots[0];
 			del_nr++;
 			ret = btrfs_free_extent(trans, root, bytenr, num_bytes,
-						leaf->start, leaf_owner,
-						leaf_gen, inode->i_ino, 0);
+						0, root->root_key.objectid,
+						inode->i_ino, orig_offset);
 			BUG_ON(ret);
 		}
 		split_end = 0;
@@ -768,13 +745,12 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 			locked_end = extent_end;
 		}
 		btrfs_set_file_extent_num_bytes(leaf, fi, split - key.offset);
-		extent_offset += split - key.offset;
 	} else  {
 		BUG_ON(key.offset != start);
-		btrfs_set_file_extent_offset(leaf, fi, extent_offset +
-					     split - key.offset);
-		btrfs_set_file_extent_num_bytes(leaf, fi, extent_end - split);
 		key.offset = split;
+		btrfs_set_file_extent_offset(leaf, fi, key.offset -
+					     orig_offset);
+		btrfs_set_file_extent_num_bytes(leaf, fi, extent_end - split);
 		btrfs_set_item_key_safe(trans, root, path, &key);
 		extent_end = split;
 	}
@@ -793,7 +769,8 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 					    struct btrfs_file_extent_item);
 			key.offset = split;
 			btrfs_set_item_key_safe(trans, root, path, &key);
-			btrfs_set_file_extent_offset(leaf, fi, extent_offset);
+			btrfs_set_file_extent_offset(leaf, fi, key.offset -
+						     orig_offset);
 			btrfs_set_file_extent_num_bytes(leaf, fi,
 							other_end - split);
 			goto done;
@@ -815,10 +792,9 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 
 	btrfs_mark_buffer_dirty(leaf);
 
-	orig_parent = leaf->start;
-	ret = btrfs_inc_extent_ref(trans, root, bytenr, num_bytes,
-				   orig_parent, root->root_key.objectid,
-				   trans->transid, inode->i_ino);
+	ret = btrfs_inc_extent_ref(trans, root, bytenr, num_bytes, 0,
+				   root->root_key.objectid,
+				   inode->i_ino, orig_offset);
 	BUG_ON(ret);
 	btrfs_release_path(root, path);
 
@@ -833,20 +809,12 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 	btrfs_set_file_extent_type(leaf, fi, extent_type);
 	btrfs_set_file_extent_disk_bytenr(leaf, fi, bytenr);
 	btrfs_set_file_extent_disk_num_bytes(leaf, fi, num_bytes);
-	btrfs_set_file_extent_offset(leaf, fi, extent_offset);
+	btrfs_set_file_extent_offset(leaf, fi, key.offset - orig_offset);
 	btrfs_set_file_extent_num_bytes(leaf, fi, extent_end - key.offset);
 	btrfs_set_file_extent_ram_bytes(leaf, fi, num_bytes);
 	btrfs_set_file_extent_compression(leaf, fi, 0);
 	btrfs_set_file_extent_encryption(leaf, fi, 0);
 	btrfs_set_file_extent_other_encoding(leaf, fi, 0);
-
-	if (orig_parent != leaf->start) {
-		ret = btrfs_update_extent_ref(trans, root, bytenr, num_bytes,
-					      orig_parent, leaf->start,
-					      root->root_key.objectid,
-					      trans->transid, inode->i_ino);
-		BUG_ON(ret);
-	}
 done:
 	btrfs_mark_buffer_dirty(leaf);
 

commit b7967db75a38df4891b22efe1b0969b9357eb946
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Apr 27 07:29:04 2009 -0400

    Btrfs: remove #if 0 code
    
    Btrfs had some old code sitting around under #if 0, this drops it.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index da3ed965c956..1d51dc38bb49 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -272,83 +272,6 @@ int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
 	return 0;
 }
 
-int btrfs_check_file(struct btrfs_root *root, struct inode *inode)
-{
-	return 0;
-#if 0
-	struct btrfs_path *path;
-	struct btrfs_key found_key;
-	struct extent_buffer *leaf;
-	struct btrfs_file_extent_item *extent;
-	u64 last_offset = 0;
-	int nritems;
-	int slot;
-	int found_type;
-	int ret;
-	int err = 0;
-	u64 extent_end = 0;
-
-	path = btrfs_alloc_path();
-	ret = btrfs_lookup_file_extent(NULL, root, path, inode->i_ino,
-				       last_offset, 0);
-	while (1) {
-		nritems = btrfs_header_nritems(path->nodes[0]);
-		if (path->slots[0] >= nritems) {
-			ret = btrfs_next_leaf(root, path);
-			if (ret)
-				goto out;
-			nritems = btrfs_header_nritems(path->nodes[0]);
-		}
-		slot = path->slots[0];
-		leaf = path->nodes[0];
-		btrfs_item_key_to_cpu(leaf, &found_key, slot);
-		if (found_key.objectid != inode->i_ino)
-			break;
-		if (found_key.type != BTRFS_EXTENT_DATA_KEY)
-			goto out;
-
-		if (found_key.offset < last_offset) {
-			WARN_ON(1);
-			btrfs_print_leaf(root, leaf);
-			printk(KERN_ERR "inode %lu found offset %llu "
-			       "expected %llu\n", inode->i_ino,
-			       (unsigned long long)found_key.offset,
-			       (unsigned long long)last_offset);
-			err = 1;
-			goto out;
-		}
-		extent = btrfs_item_ptr(leaf, slot,
-					struct btrfs_file_extent_item);
-		found_type = btrfs_file_extent_type(leaf, extent);
-		if (found_type == BTRFS_FILE_EXTENT_REG) {
-			extent_end = found_key.offset +
-			     btrfs_file_extent_num_bytes(leaf, extent);
-		} else if (found_type == BTRFS_FILE_EXTENT_INLINE) {
-			struct btrfs_item *item;
-			item = btrfs_item_nr(leaf, slot);
-			extent_end = found_key.offset +
-			     btrfs_file_extent_inline_len(leaf, extent);
-			extent_end = (extent_end + root->sectorsize - 1) &
-				~((u64)root->sectorsize - 1);
-		}
-		last_offset = extent_end;
-		path->slots[0]++;
-	}
-	if (0 && last_offset < inode->i_size) {
-		WARN_ON(1);
-		btrfs_print_leaf(root, leaf);
-		printk(KERN_ERR "inode %lu found offset %llu size %llu\n",
-		       inode->i_ino, (unsigned long long)last_offset,
-		       (unsigned long long)inode->i_size);
-		err = 1;
-
-	}
-out:
-	btrfs_free_path(path);
-	return err;
-#endif
-}
-
 /*
  * this is very complex, but the basic idea is to drop all extents
  * in the range start - end.  hint_block is filled in with a block number
@@ -689,7 +612,6 @@ noinline int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 		unlock_extent(&BTRFS_I(inode)->io_tree, orig_locked_end,
 			      locked_end - 1, GFP_NOFS);
 	}
-	btrfs_check_file(root, inode);
 	return ret;
 }
 

commit e980b50cda1610f1c17978d9b7fd311a9dd93877
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Apr 24 14:39:24 2009 -0400

    Btrfs: fix fallocate deadlock on inode extent lock
    
    The btrfs fallocate call takes an extent lock on the entire range
    being fallocated, and then runs through insert_reserved_extent on each
    extent as they are allocated.
    
    The problem with this is that btrfs_drop_extents may decide to try
    and take the same extent lock fallocate was already holding.  The solution
    used here is to push down knowledge of the range that is already locked
    going into btrfs_drop_extents.
    
    It turns out that at least one other caller had the same bug.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 482f8db2cfd0..da3ed965c956 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -363,15 +363,16 @@ int btrfs_check_file(struct btrfs_root *root, struct inode *inode)
  */
 noinline int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 		       struct btrfs_root *root, struct inode *inode,
-		       u64 start, u64 end, u64 inline_limit, u64 *hint_byte)
+		       u64 start, u64 end, u64 locked_end,
+		       u64 inline_limit, u64 *hint_byte)
 {
 	u64 extent_end = 0;
-	u64 locked_end = end;
 	u64 search_start = start;
 	u64 leaf_start;
 	u64 ram_bytes = 0;
 	u64 orig_parent = 0;
 	u64 disk_bytenr = 0;
+	u64 orig_locked_end = locked_end;
 	u8 compression;
 	u8 encryption;
 	u16 other_encoding = 0;
@@ -684,9 +685,9 @@ noinline int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 	}
 out:
 	btrfs_free_path(path);
-	if (locked_end > end) {
-		unlock_extent(&BTRFS_I(inode)->io_tree, end, locked_end - 1,
-			      GFP_NOFS);
+	if (locked_end > orig_locked_end) {
+		unlock_extent(&BTRFS_I(inode)->io_tree, orig_locked_end,
+			      locked_end - 1, GFP_NOFS);
 	}
 	btrfs_check_file(root, inode);
 	return ret;

commit 546888da82082555a56528730a83f0afd12f33bf
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Apr 21 11:53:38 2009 -0400

    Btrfs: fix btrfs fallocate oops and deadlock
    
    Btrfs fallocate was incorrectly starting a transaction with a lock held
    on the extent_io tree for the file, which could deadlock.  Strictly
    speaking it was using join_transaction which would be safe, but it is better
    to move the transaction outside of the lock.
    
    When preallocated extents are overwritten, btrfs_mark_buffer_dirty was
    being called on an unlocked buffer.  This was triggering an assertion and
    oops because the lock is supposed to be held.
    
    The bug was calling btrfs_mark_buffer_dirty on a leaf after btrfs_del_item had
    been run.  btrfs_del_item takes care of dirtying things, so the solution is a
    to skip the btrfs_mark_buffer_dirty call in this case.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index e21c0060ee73..482f8db2cfd0 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -830,7 +830,7 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 
 		ret = btrfs_del_items(trans, root, path, del_slot, del_nr);
 		BUG_ON(ret);
-		goto done;
+		goto release;
 	} else if (split == start) {
 		if (locked_end < extent_end) {
 			ret = try_lock_extent(&BTRFS_I(inode)->io_tree,
@@ -926,6 +926,8 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 	}
 done:
 	btrfs_mark_buffer_dirty(leaf);
+
+release:
 	btrfs_release_path(root, path);
 	if (split_end && split == start) {
 		split = end;

commit d313d7a31a752c88f7288692bd98e66d0789779b
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Apr 20 15:50:09 2009 -0400

    Btrfs: add a priority queue to the async thread helpers
    
    Btrfs is using WRITE_SYNC_PLUG to send down synchronous IOs with a
    higher priority.  But, the checksumming helper threads prevent it
    from being fully effective.
    
    There are two problems.  First, a big queue of pending checksumming
    will delay the synchronous IO behind other lower priority writes.  Second,
    the checksumming uses an ordered async work queue.  The ordering makes sure
    that IOs are sent to the block layer in the same order they are sent
    to the checksumming threads.  Usually this gives us less seeky IO.
    
    But, when we start mixing IO priorities, the lower priority IO can delay
    the higher priority IO.
    
    This patch solves both problems by adding a high priority list to the async
    helper threads, and a new btrfs_set_work_high_prio(), which is used
    to make put a new async work item onto the higher priority list.
    
    The ordering is still done on high priority IO, but all of the high
    priority bios are ordered separately from the low priority bios.  This
    ordering is purely an IO optimization, it is not involved in data
    or metadata integrity.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 9c9fb46ccd08..e21c0060ee73 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1131,7 +1131,7 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 		if (will_write) {
 			btrfs_fdatawrite_range(inode->i_mapping, pos,
 					       pos + write_bytes - 1,
-					       WB_SYNC_NONE);
+					       WB_SYNC_ALL);
 		} else {
 			balance_dirty_pages_ratelimited_nr(inode->i_mapping,
 							   num_pages);

commit 5a3f23d515a2ebf0c750db80579ca57b28cbce6d
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Mar 31 13:27:11 2009 -0400

    Btrfs: add extra flushing for renames and truncates
    
    Renames and truncates are both common ways to replace old data with new
    data.  The filesystem can make an effort to make sure the new data is
    on disk before actually replacing the old data.
    
    This is especially important for rename, which many application use as
    though it were atomic for both the data and the metadata involved.  The
    current btrfs code will happily replace a file that is fully on disk
    with one that was just created and still has pending IO.
    
    If we crash after transaction commit but before the IO is done, we'll end
    up replacing a good file with a zero length file.  The solution used
    here is to create a list of inodes that need special ordering and force
    them to disk before the commit is done.  This is similar to the
    ext3 style data=ordering, except it is only done on selected files.
    
    Btrfs is able to get away with this because it does not wait on commits
    very often, even for fsync (which use a sub-commit).
    
    For renames, we order the file when it wasn't already
    on disk and when it is replacing an existing file.  Larger files
    are sent to filemap_flush right away (before the transaction handle is
    opened).
    
    For truncates, we order if the file goes from non-zero size down to
    zero size.  This is a little different, because at the time of the
    truncate the file has no dirty bytes to order.  But, we flag the inode
    so that it is added to the ordered list on close (via release method).  We
    also immediately add it to the ordered list of the current transaction
    so that we can try to flush down any writes the application sneaks in
    before commit.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 32d10a617613..9c9fb46ccd08 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1161,6 +1161,20 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 		page_cache_release(pinned[1]);
 	*ppos = pos;
 
+	/*
+	 * we want to make sure fsync finds this change
+	 * but we haven't joined a transaction running right now.
+	 *
+	 * Later on, someone is sure to update the inode and get the
+	 * real transid recorded.
+	 *
+	 * We set last_trans now to the fs_info generation + 1,
+	 * this will either be one more than the running transaction
+	 * or the generation used for the next transaction if there isn't
+	 * one running right now.
+	 */
+	BTRFS_I(inode)->last_trans = root->fs_info->generation + 1;
+
 	if (num_written > 0 && will_write) {
 		struct btrfs_trans_handle *trans;
 
@@ -1194,6 +1208,18 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 
 int btrfs_release_file(struct inode *inode, struct file *filp)
 {
+	/*
+	 * ordered_data_close is set by settattr when we are about to truncate
+	 * a file from a non-zero size to a zero size.  This tries to
+	 * flush down new bytes that may have been written if the
+	 * application were using truncate to replace a file in place.
+	 */
+	if (BTRFS_I(inode)->ordered_data_close) {
+		BTRFS_I(inode)->ordered_data_close = 0;
+		btrfs_add_ordered_operation(NULL, BTRFS_I(inode)->root, inode);
+		if (inode->i_size > BTRFS_ORDERED_OPERATIONS_FLUSH_LIMIT)
+			filemap_flush(inode->i_mapping);
+	}
 	if (filp->private_data)
 		btrfs_ioctl_trans_end(filp);
 	return 0;

commit 12fcfd22fe5bf4fe74710232098bc101af497995
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Mar 24 10:24:20 2009 -0400

    Btrfs: tree logging unlink/rename fixes
    
    The tree logging code allows individual files or directories to be logged
    without including operations on other files and directories in the FS.
    It tries to commit the minimal set of changes to disk in order to
    fsync the single file or directory that was sent to fsync or O_SYNC.
    
    The tree logging code was allowing files and directories to be unlinked
    if they were part of a rename operation where only one directory
    in the rename was in the fsync log.  This patch adds a few new rules
    to the tree logging.
    
    1) on rename or unlink, if the inode being unlinked isn't in the fsync
    log, we must force a full commit before doing an fsync of the directory
    where the unlink was done.  The commit isn't done during the unlink,
    but it is forced the next time we try to log the parent directory.
    
    Solution: record transid of last unlink/rename per directory when the
    directory wasn't already logged.  For renames this is only done when
    renaming to a different directory.
    
    mkdir foo/some_dir
    normal commit
    rename foo/some_dir foo2/some_dir
    mkdir foo/some_dir
    fsync foo/some_dir/some_file
    
    The fsync above will unlink the original some_dir without recording
    it in its new location (foo2).  After a crash, some_dir will be gone
    unless the fsync of some_file forces a full commit
    
    2) we must log any new names for any file or dir that is in the fsync
    log.  This way we make sure not to lose files that are unlinked during
    the same transaction.
    
    2a) we must log any new names for any file or dir during rename
    when the directory they are being removed from was logged.
    
    2a is actually the more important variant.  Without the extra logging
    a crash might unlink the old name without recreating the new one
    
    3) after a crash, we must go through any directories with a link count
    of zero and redo the rm -rf
    
    mkdir f1/foo
    normal commit
    rm -rf f1/foo
    fsync(f1)
    
    The directory f1 was fully removed from the FS, but fsync was never
    called on f1, only its parent dir.  After a crash the rm -rf must
    be replayed.  This must be able to recurse down the entire
    directory tree.  The inode link count fixup code takes care of the
    ugly details.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index f06c275644b7..32d10a617613 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1173,8 +1173,11 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 			ret = btrfs_log_dentry_safe(trans, root,
 						    file->f_dentry);
 			if (ret == 0) {
-				btrfs_sync_log(trans, root);
-				btrfs_end_transaction(trans, root);
+				ret = btrfs_sync_log(trans, root);
+				if (ret == 0)
+					btrfs_end_transaction(trans, root);
+				else
+					btrfs_commit_transaction(trans, root);
 			} else {
 				btrfs_commit_transaction(trans, root);
 			}
@@ -1266,8 +1269,11 @@ int btrfs_sync_file(struct file *file, struct dentry *dentry, int datasync)
 	if (ret > 0) {
 		ret = btrfs_commit_transaction(trans, root);
 	} else {
-		btrfs_sync_log(trans, root);
-		ret = btrfs_end_transaction(trans, root);
+		ret = btrfs_sync_log(trans, root);
+		if (ret == 0)
+			ret = btrfs_end_transaction(trans, root);
+		else
+			ret = btrfs_commit_transaction(trans, root);
 	}
 	mutex_lock(&dentry->d_inode->i_mutex);
 out:

commit b9473439d3e84d9fc1a0a83faca69cc1b7566341
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Mar 13 11:00:37 2009 -0400

    Btrfs: leave btree locks spinning more often
    
    btrfs_mark_buffer dirty would set dirty bits in the extent_io tree
    for the buffers it was dirtying.  This may require a kmalloc and it
    was not atomic.  So, anyone who called btrfs_mark_buffer_dirty had to
    set any btree locks they were holding to blocking first.
    
    This commit changes dirty tracking for extent buffers to just use a flag
    in the extent buffer.  Now that we have one and only one extent buffer
    per page, this can be safely done without losing dirty bits along the way.
    
    This also introduces a path->leave_spinning flag that callers of
    btrfs_search_slot can use to indicate they will properly deal with a
    path returned where all the locks are spinning instead of blocking.
    
    Many of the btree search callers now expect spinning paths,
    resulting in better btree concurrency overall.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index c80075497645..f06c275644b7 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -606,6 +606,7 @@ noinline int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 			btrfs_set_key_type(&ins, BTRFS_EXTENT_DATA_KEY);
 
 			btrfs_release_path(root, path);
+			path->leave_spinning = 1;
 			ret = btrfs_insert_empty_item(trans, root, path, &ins,
 						      sizeof(*extent));
 			BUG_ON(ret);
@@ -639,7 +640,9 @@ noinline int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 							ram_bytes);
 			btrfs_set_file_extent_type(leaf, extent, found_type);
 
+			btrfs_unlock_up_safe(path, 1);
 			btrfs_mark_buffer_dirty(path->nodes[0]);
+			btrfs_set_lock_blocking(path->nodes[0]);
 
 			if (disk_bytenr != 0) {
 				ret = btrfs_update_extent_ref(trans, root,
@@ -652,6 +655,7 @@ noinline int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 
 				BUG_ON(ret);
 			}
+			path->leave_spinning = 0;
 			btrfs_release_path(root, path);
 			if (disk_bytenr != 0)
 				inode_add_bytes(inode, extent_end - end);

commit 56bec294dea971335d4466b30f2d959f28f6e36d
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Mar 13 10:10:06 2009 -0400

    Btrfs: do extent allocation and reference count updates in the background
    
    The extent allocation tree maintains a reference count and full
    back reference information for every extent allocated in the
    filesystem.  For subvolume and snapshot trees, every time
    a block goes through COW, the new copy of the block adds a reference
    on every block it points to.
    
    If a btree node points to 150 leaves, then the COW code needs to go
    and add backrefs on 150 different extents, which might be spread all
    over the extent allocation tree.
    
    These updates currently happen during btrfs_cow_block, and most COWs
    happen during btrfs_search_slot.  btrfs_search_slot has locks held
    on both the parent and the node we are COWing, and so we really want
    to avoid IO during the COW if we can.
    
    This commit adds an rbtree of pending reference count updates and extent
    allocations.  The tree is ordered by byte number of the extent and byte number
    of the parent for the back reference.  The tree allows us to:
    
    1) Modify back references in something close to disk order, reducing seeks
    2) Significantly reduce the number of modifications made as block pointers
    are balanced around
    3) Do all of the extent insertion and back reference modifications outside
    of the performance critical btrfs_search_slot code.
    
    #3 has the added benefit of greatly reducing the btrfs stack footprint.
    The extent allocation tree modifications are done without the deep
    (and somewhat recursive) call chains used in the past.
    
    These delayed back reference updates must be done before the transaction
    commits, and so the rbtree is tied to the transaction.  Throttling is
    implemented to help keep the queue of backrefs at a reasonable size.
    
    Since there was a similar mechanism in place for the extent tree
    extents, that is removed and replaced by the delayed reference tree.
    
    Yan Zheng <yan.zheng@oracle.com> helped review and fixup this code.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index dc78954861b3..c80075497645 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -643,7 +643,9 @@ noinline int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 
 			if (disk_bytenr != 0) {
 				ret = btrfs_update_extent_ref(trans, root,
-						disk_bytenr, orig_parent,
+						disk_bytenr,
+						le64_to_cpu(old.disk_num_bytes),
+						orig_parent,
 						leaf->start,
 						root->root_key.objectid,
 						trans->transid, ins.objectid);
@@ -912,7 +914,7 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 	btrfs_set_file_extent_other_encoding(leaf, fi, 0);
 
 	if (orig_parent != leaf->start) {
-		ret = btrfs_update_extent_ref(trans, root, bytenr,
+		ret = btrfs_update_extent_ref(trans, root, bytenr, num_bytes,
 					      orig_parent, leaf->start,
 					      root->root_key.objectid,
 					      trans->transid, inode->i_ino);

commit 6a63209fc02d5483371f07e4913ee8abad608051
Author: Josef Bacik <jbacik@redhat.com>
Date:   Fri Feb 20 11:00:09 2009 -0500

    Btrfs: add better -ENOSPC handling
    
    This is a step in the direction of better -ENOSPC handling.  Instead of
    checking the global bytes counter we check the space_info bytes counters to
    make sure we have enough space.
    
    If we don't we go ahead and try to allocate a new chunk, and then if that fails
    we return -ENOSPC.  This patch adds two counters to btrfs_space_info,
    bytes_delalloc and bytes_may_use.
    
    bytes_delalloc account for extents we've actually setup for delalloc and will
    be allocated at some point down the line.
    
    bytes_may_use is to keep track of how many bytes we may use for delalloc at
    some point.  When we actually set the extent_bit for the delalloc bytes we
    subtract the reserved bytes from the bytes_may_use counter.  This keeps us from
    not actually being able to allocate space for any delalloc bytes.
    
    Signed-off-by: Josef Bacik <jbacik@redhat.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 872f104576e5..dc78954861b3 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1091,19 +1091,24 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 		WARN_ON(num_pages > nrptrs);
 		memset(pages, 0, sizeof(struct page *) * nrptrs);
 
-		ret = btrfs_check_free_space(root, write_bytes, 0);
+		ret = btrfs_check_data_free_space(root, inode, write_bytes);
 		if (ret)
 			goto out;
 
 		ret = prepare_pages(root, file, pages, num_pages,
 				    pos, first_index, last_index,
 				    write_bytes);
-		if (ret)
+		if (ret) {
+			btrfs_free_reserved_data_space(root, inode,
+						       write_bytes);
 			goto out;
+		}
 
 		ret = btrfs_copy_from_user(pos, num_pages,
 					   write_bytes, pages, buf);
 		if (ret) {
+			btrfs_free_reserved_data_space(root, inode,
+						       write_bytes);
 			btrfs_drop_pages(pages, num_pages);
 			goto out;
 		}
@@ -1111,8 +1116,11 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 		ret = dirty_and_release_pages(NULL, root, file, pages,
 					      num_pages, pos, write_bytes);
 		btrfs_drop_pages(pages, num_pages);
-		if (ret)
+		if (ret) {
+			btrfs_free_reserved_data_space(root, inode,
+						       write_bytes);
 			goto out;
+		}
 
 		if (will_write) {
 			btrfs_fdatawrite_range(inode->i_mapping, pos,
@@ -1136,6 +1144,8 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 	}
 out:
 	mutex_unlock(&inode->i_mutex);
+	if (ret)
+		err = ret;
 
 out_nolock:
 	kfree(pages);

commit 2cfbd50b536c878e58ab3681c4e944fa3d99b415
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Feb 20 10:55:10 2009 -0500

    Btrfs: check file pointer in btrfs_sync_file
    
    fsync can be called by NFS with a null file pointer, and btrfs was
    oopsing in this case.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 3e8023efaff7..872f104576e5 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1222,7 +1222,7 @@ int btrfs_sync_file(struct file *file, struct dentry *dentry, int datasync)
 	/*
 	 * ok we haven't committed the transaction yet, lets do a commit
 	 */
-	if (file->private_data)
+	if (file && file->private_data)
 		btrfs_ioctl_trans_end(file);
 
 	trans = btrfs_start_transaction(root, 1);
@@ -1231,7 +1231,7 @@ int btrfs_sync_file(struct file *file, struct dentry *dentry, int datasync)
 		goto out;
 	}
 
-	ret = btrfs_log_dentry_safe(trans, root, file->f_dentry);
+	ret = btrfs_log_dentry_safe(trans, root, dentry);
 	if (ret < 0)
 		goto out;
 
@@ -1245,7 +1245,7 @@ int btrfs_sync_file(struct file *file, struct dentry *dentry, int datasync)
 	 * file again, but that will end up using the synchronization
 	 * inside btrfs_sync_log to keep things safe.
 	 */
-	mutex_unlock(&file->f_dentry->d_inode->i_mutex);
+	mutex_unlock(&dentry->d_inode->i_mutex);
 
 	if (ret > 0) {
 		ret = btrfs_commit_transaction(trans, root);
@@ -1253,7 +1253,7 @@ int btrfs_sync_file(struct file *file, struct dentry *dentry, int datasync)
 		btrfs_sync_log(trans, root);
 		ret = btrfs_end_transaction(trans, root);
 	}
-	mutex_lock(&file->f_dentry->d_inode->i_mutex);
+	mutex_lock(&dentry->d_inode->i_mutex);
 out:
 	return ret > 0 ? EIO : ret;
 }

commit 7237f1833601dcc435a64176c2c347ec4bd959f9
Author: Yan Zheng <zheng.yan@oracle.com>
Date:   Wed Jan 21 12:54:03 2009 -0500

    Btrfs: fix tree logs parallel sync
    
    To improve performance, btrfs_sync_log merges tree log sync
    requests. But it wrongly merges sync requests for different
    tree logs. If multiple tree logs are synced at the same time,
    only one of them actually gets synced.
    
    This patch has following changes to fix the bug:
    
    Move most tree log related fields in btrfs_fs_info to
    btrfs_root. This allows merging sync requests separately
    for each tree log.
    
    Don't insert root item into the log root tree immediately
    after log tree is allocated. Root item for log tree is
    inserted when log tree get synced for the first time. This
    allows syncing the log root tree without first syncing all
    log trees.
    
    At tree-log sync, btrfs_sync_log first sync the log tree;
    then updates corresponding root item in the log root tree;
    sync the log root tree; then update the super block.
    
    Signed-off-by: Yan Zheng <zheng.yan@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index fbcbf43f5114..3e8023efaff7 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1214,10 +1214,10 @@ int btrfs_sync_file(struct file *file, struct dentry *dentry, int datasync)
 	}
 	mutex_unlock(&root->fs_info->trans_mutex);
 
-	root->fs_info->tree_log_batch++;
+	root->log_batch++;
 	filemap_fdatawrite(inode->i_mapping);
 	btrfs_wait_ordered_range(inode, 0, (u64)-1);
-	root->fs_info->tree_log_batch++;
+	root->log_batch++;
 
 	/*
 	 * ok we haven't committed the transaction yet, lets do a commit

commit 7eaebe7d503c3ef240ac7b3efc5433fe647c0298
Author: Huang Weiyi <weiyi.huang@gmail.com>
Date:   Wed Jan 21 10:49:16 2009 -0500

    Btrfs: removed unused #include <version.h>'s
    
    Removed unused #include <version.h>'s in btrfs
    
    Signed-off-by: Huang Weiyi <weiyi.huang@gmail.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 90268334145e..fbcbf43f5114 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -29,7 +29,6 @@
 #include <linux/writeback.h>
 #include <linux/statfs.h>
 #include <linux/compat.h>
-#include <linux/version.h>
 #include "ctree.h"
 #include "disk-io.h"
 #include "transaction.h"

commit 1ba12553f3600ffebad226c5204ab0e46df98161
Author: Yan Zheng <zheng.yan@oracle.com>
Date:   Tue Jan 6 09:58:02 2009 -0500

    Btrfs: don't change file extent's ram_bytes in btrfs_drop_extents
    
    btrfs_drop_extents doesn't change file extent's ram_bytes
    in the case of booked extent. To be consistent, we should
    also not change ram_bytes when truncating existing extent.
    
    Signed-off-by: Yan Zheng <zheng.yan@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 0e3a13a45653..90268334145e 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -556,10 +556,6 @@ noinline int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 					inode_sub_bytes(inode, old_num -
 							new_num);
 				}
-				if (!compression && !encryption) {
-					btrfs_set_file_extent_ram_bytes(leaf,
-							extent, new_num);
-				}
 				btrfs_set_file_extent_num_bytes(leaf,
 							extent, new_num);
 				btrfs_mark_buffer_dirty(leaf);

commit d397712bcc6a759a560fd247e6053ecae091f958
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Jan 5 21:25:51 2009 -0500

    Btrfs: Fix checkpatch.pl warnings
    
    There were many, most are fixed now.  struct-funcs.c generates some warnings
    but these are bogus.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 5908521922fb..0e3a13a45653 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -44,10 +44,10 @@
 /* simple helper to fault in pages and copy.  This should go away
  * and be replaced with calls into generic code.
  */
-static int noinline btrfs_copy_from_user(loff_t pos, int num_pages,
+static noinline int btrfs_copy_from_user(loff_t pos, int num_pages,
 					 int write_bytes,
 					 struct page **prepared_pages,
-					 const char __user * buf)
+					 const char __user *buf)
 {
 	long page_fault = 0;
 	int i;
@@ -78,7 +78,7 @@ static int noinline btrfs_copy_from_user(loff_t pos, int num_pages,
 /*
  * unlocks pages after btrfs_file_write is done with them
  */
-static void noinline btrfs_drop_pages(struct page **pages, size_t num_pages)
+static noinline void btrfs_drop_pages(struct page **pages, size_t num_pages)
 {
 	size_t i;
 	for (i = 0; i < num_pages; i++) {
@@ -103,7 +103,7 @@ static void noinline btrfs_drop_pages(struct page **pages, size_t num_pages)
  * this also makes the decision about creating an inline extent vs
  * doing real data extents, marking pages dirty and delalloc as required.
  */
-static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
+static noinline int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 				   struct btrfs_root *root,
 				   struct file *file,
 				   struct page **pages,
@@ -137,9 +137,6 @@ static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
 	btrfs_set_trans_block_group(trans, inode);
 	hint_byte = 0;
 
-	if ((end_of_last_block & 4095) == 0) {
-		printk("strange end of last %Lu %zu %Lu\n", start_pos, write_bytes, end_of_last_block);
-	}
 	set_extent_uptodate(io_tree, start_pos, end_of_last_block, GFP_NOFS);
 
 	/* check for reserved extents on each page, we don't want
@@ -185,7 +182,7 @@ int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
 		len = (u64)-1;
 		testend = 0;
 	}
-	while(1) {
+	while (1) {
 		if (!split)
 			split = alloc_extent_map(GFP_NOFS);
 		if (!split2)
@@ -295,7 +292,7 @@ int btrfs_check_file(struct btrfs_root *root, struct inode *inode)
 	path = btrfs_alloc_path();
 	ret = btrfs_lookup_file_extent(NULL, root, path, inode->i_ino,
 				       last_offset, 0);
-	while(1) {
+	while (1) {
 		nritems = btrfs_header_nritems(path->nodes[0]);
 		if (path->slots[0] >= nritems) {
 			ret = btrfs_next_leaf(root, path);
@@ -314,8 +311,10 @@ int btrfs_check_file(struct btrfs_root *root, struct inode *inode)
 		if (found_key.offset < last_offset) {
 			WARN_ON(1);
 			btrfs_print_leaf(root, leaf);
-			printk("inode %lu found offset %Lu expected %Lu\n",
-			       inode->i_ino, found_key.offset, last_offset);
+			printk(KERN_ERR "inode %lu found offset %llu "
+			       "expected %llu\n", inode->i_ino,
+			       (unsigned long long)found_key.offset,
+			       (unsigned long long)last_offset);
 			err = 1;
 			goto out;
 		}
@@ -331,7 +330,7 @@ int btrfs_check_file(struct btrfs_root *root, struct inode *inode)
 			extent_end = found_key.offset +
 			     btrfs_file_extent_inline_len(leaf, extent);
 			extent_end = (extent_end + root->sectorsize - 1) &
-				~((u64)root->sectorsize -1 );
+				~((u64)root->sectorsize - 1);
 		}
 		last_offset = extent_end;
 		path->slots[0]++;
@@ -339,8 +338,9 @@ int btrfs_check_file(struct btrfs_root *root, struct inode *inode)
 	if (0 && last_offset < inode->i_size) {
 		WARN_ON(1);
 		btrfs_print_leaf(root, leaf);
-		printk("inode %lu found offset %Lu size %Lu\n", inode->i_ino,
-		       last_offset, inode->i_size);
+		printk(KERN_ERR "inode %lu found offset %llu size %llu\n",
+		       inode->i_ino, (unsigned long long)last_offset,
+		       (unsigned long long)inode->i_size);
 		err = 1;
 
 	}
@@ -362,7 +362,7 @@ int btrfs_check_file(struct btrfs_root *root, struct inode *inode)
  * inline_limit is used to tell this code which offsets in the file to keep
  * if they contain inline extents.
  */
-int noinline btrfs_drop_extents(struct btrfs_trans_handle *trans,
+noinline int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 		       struct btrfs_root *root, struct inode *inode,
 		       u64 start, u64 end, u64 inline_limit, u64 *hint_byte)
 {
@@ -398,7 +398,7 @@ int noinline btrfs_drop_extents(struct btrfs_trans_handle *trans,
 	path = btrfs_alloc_path();
 	if (!path)
 		return -ENOMEM;
-	while(1) {
+	while (1) {
 		recow = 0;
 		btrfs_release_path(root, path);
 		ret = btrfs_lookup_file_extent(trans, root, path, inode->i_ino,
@@ -649,16 +649,15 @@ int noinline btrfs_drop_extents(struct btrfs_trans_handle *trans,
 			if (disk_bytenr != 0) {
 				ret = btrfs_update_extent_ref(trans, root,
 						disk_bytenr, orig_parent,
-					        leaf->start,
+						leaf->start,
 						root->root_key.objectid,
 						trans->transid, ins.objectid);
 
 				BUG_ON(ret);
 			}
 			btrfs_release_path(root, path);
-			if (disk_bytenr != 0) {
+			if (disk_bytenr != 0)
 				inode_add_bytes(inode, extent_end - end);
-			}
 		}
 
 		if (found_extent && !keep) {
@@ -944,7 +943,7 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
  * waits for data=ordered extents to finish before allowing the pages to be
  * modified.
  */
-static int noinline prepare_pages(struct btrfs_root *root, struct file *file,
+static noinline int prepare_pages(struct btrfs_root *root, struct file *file,
 			 struct page **pages, size_t num_pages,
 			 loff_t pos, unsigned long first_index,
 			 unsigned long last_index, size_t write_bytes)
@@ -979,7 +978,8 @@ static int noinline prepare_pages(struct btrfs_root *root, struct file *file,
 		struct btrfs_ordered_extent *ordered;
 		lock_extent(&BTRFS_I(inode)->io_tree,
 			    start_pos, last_pos - 1, GFP_NOFS);
-		ordered = btrfs_lookup_first_ordered_extent(inode, last_pos -1);
+		ordered = btrfs_lookup_first_ordered_extent(inode,
+							    last_pos - 1);
 		if (ordered &&
 		    ordered->file_offset + ordered->len > start_pos &&
 		    ordered->file_offset < last_pos) {
@@ -1085,7 +1085,7 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 		}
 	}
 
-	while(count > 0) {
+	while (count > 0) {
 		size_t offset = pos & (PAGE_CACHE_SIZE - 1);
 		size_t write_bytes = min(count, nrptrs *
 					(size_t)PAGE_CACHE_SIZE -
@@ -1178,7 +1178,7 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 	return num_written ? num_written : err;
 }
 
-int btrfs_release_file(struct inode * inode, struct file * filp)
+int btrfs_release_file(struct inode *inode, struct file *filp)
 {
 	if (filp->private_data)
 		btrfs_ioctl_trans_end(filp);
@@ -1237,9 +1237,8 @@ int btrfs_sync_file(struct file *file, struct dentry *dentry, int datasync)
 	}
 
 	ret = btrfs_log_dentry_safe(trans, root, file->f_dentry);
-	if (ret < 0) {
+	if (ret < 0)
 		goto out;
-	}
 
 	/* we've logged all the items and now have a consistent
 	 * version of the file in the log.  It is possible that

commit 9aead43588f4bdb1bb61e348ad0f33794bbddc0f
Author: yanhai zhu <zhu.yanhai@gmail.com>
Date:   Mon Jan 5 15:49:11 2009 -0500

    Btrfs: Fix memset length in btrfs_file_write
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 507081059d97..5908521922fb 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1094,7 +1094,7 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 					PAGE_CACHE_SHIFT;
 
 		WARN_ON(num_pages > nrptrs);
-		memset(pages, 0, sizeof(pages));
+		memset(pages, 0, sizeof(struct page *) * nrptrs);
 
 		ret = btrfs_check_free_space(root, write_bytes, 0);
 		if (ret)

commit 17d217fe970d34720f4f1633dca73a6aa2f3d9d1
Author: Yan Zheng <zheng.yan@oracle.com>
Date:   Fri Dec 12 10:03:38 2008 -0500

    Btrfs: fix nodatasum handling in balancing code
    
    Checksums on data can be disabled by mount option, so it's
    possible some data extents don't have checksums or have
    invalid checksums. This causes trouble for data relocation.
    This patch contains following things to make data relocation
    work.
    
    1) make nodatasum/nodatacow mount option only affects new
    files. Checksums and COW on data are only controlled by the
    inode flags.
    
    2) check the existence of checksum in the nodatacow checker.
    If checksums exist, force COW the data extent. This ensure that
    checksum for a given block is either valid or does not exist.
    
    3) update data relocation code to properly handle the case
    of checksum missing.
    
    Signed-off-by: Yan Zheng <zheng.yan@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 71bfe3a6a444..507081059d97 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1059,14 +1059,6 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 	first_index = pos >> PAGE_CACHE_SHIFT;
 	last_index = (pos + count) >> PAGE_CACHE_SHIFT;
 
-	/*
-	 * if this is a nodatasum mount, force summing off for the inode
-	 * all the time.  That way a later mount with summing on won't
-	 * get confused
-	 */
-	if (btrfs_test_opt(root, NODATASUM))
-		btrfs_set_flag(inode, NODATASUM);
-
 	/*
 	 * there are lots of better ways to do this, but this code
 	 * makes sure the first and last page in the file range are

commit 580afd76e451deb6772d0507de580fb1df14da6c
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Dec 8 19:15:39 2008 -0500

    Btrfs: Fix compressed checksum fsync log copies
    
    The fsync logging code makes sure to onl copy the relevant checksum for each
    extent based on the file extent pointers it finds.
    
    But for compressed extents, it needs to copy the checksum for the
    entire extent.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index b5a6a2b6f668..71bfe3a6a444 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1228,7 +1228,8 @@ int btrfs_sync_file(struct file *file, struct dentry *dentry, int datasync)
 	mutex_unlock(&root->fs_info->trans_mutex);
 
 	root->fs_info->tree_log_batch++;
-	filemap_fdatawait(inode->i_mapping);
+	filemap_fdatawrite(inode->i_mapping);
+	btrfs_wait_ordered_range(inode, 0, (u64)-1);
 	root->fs_info->tree_log_batch++;
 
 	/*

commit c3027eb5523d6983f12628f3fe13d8a7576db701
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Dec 8 16:40:21 2008 -0500

    Btrfs: Add inode sequence number for NFS and reserved space in a few structs
    
    This adds a sequence number to the btrfs inode that is increased on
    every update.  NFS will be able to use that to detect when an inode has
    changed, without relying on inaccurate time fields.
    
    While we're here, this also:
    
    Puts reserved space into the super block and inode
    
    Adds a log root transid to the super so we can pick the newest super
    based on the fsync log as well as the main transaction ID.  For now
    the log root transid is always zero, but that'll get fixed.
    
    Adds a starting offset to the dev_item.  This will let us do better
    alignment calculations if we know the start of a partition on the disk.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 1c9243560eab..b5a6a2b6f668 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1055,6 +1055,7 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 	pages = kmalloc(nrptrs * sizeof(struct page *), GFP_KERNEL);
 
 	mutex_lock(&inode->i_mutex);
+	BTRFS_I(inode)->sequence++;
 	first_index = pos >> PAGE_CACHE_SHIFT;
 	last_index = (pos + count) >> PAGE_CACHE_SHIFT;
 

commit 6e430f94e508fee1aefd1dfec88da3c24ce64433
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Dec 2 06:36:09 2008 -0500

    Btrfs: fix shadowed variable declarations
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 1e8c024c69c3..1c9243560eab 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -662,18 +662,18 @@ int noinline btrfs_drop_extents(struct btrfs_trans_handle *trans,
 		}
 
 		if (found_extent && !keep) {
-			u64 disk_bytenr = le64_to_cpu(old.disk_bytenr);
+			u64 old_disk_bytenr = le64_to_cpu(old.disk_bytenr);
 
-			if (disk_bytenr != 0) {
+			if (old_disk_bytenr != 0) {
 				inode_sub_bytes(inode,
 						le64_to_cpu(old.num_bytes));
 				ret = btrfs_free_extent(trans, root,
-						disk_bytenr,
+						old_disk_bytenr,
 						le64_to_cpu(old.disk_num_bytes),
 						leaf_start, root_owner,
 						root_gen, key.objectid, 0);
 				BUG_ON(ret);
-				*hint_byte = disk_bytenr;
+				*hint_byte = old_disk_bytenr;
 			}
 		}
 

commit c36047d729a3fa080dd194b20b684cc9fe73e90c
Author: Yan Zheng <zheng.yan@oracle.com>
Date:   Wed Nov 12 14:19:50 2008 -0500

    Btrfs: Fix race in btrfs_mark_extent_written
    
    When extent needs to be split, btrfs_mark_extent_written truncates the extent
    first, then inserts a new extent and increases the reference count.
    
    The race happens if someone else deletes the old extent before the new extent
    is inserted. The fix here is increase the reference count in advance. This race
    is similar to the race in btrfs_drop_extents that was recently fixed.
    
    Signed-off-by: Yan Zheng <zheng.yan@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 934bc094bf17..1e8c024c69c3 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -746,6 +746,7 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 	u64 other_end;
 	u64 split = start;
 	u64 locked_end = end;
+	u64 orig_parent;
 	int extent_type;
 	int split_end = 1;
 	int ret;
@@ -890,6 +891,12 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 	}
 
 	btrfs_mark_buffer_dirty(leaf);
+
+	orig_parent = leaf->start;
+	ret = btrfs_inc_extent_ref(trans, root, bytenr, num_bytes,
+				   orig_parent, root->root_key.objectid,
+				   trans->transid, inode->i_ino);
+	BUG_ON(ret);
 	btrfs_release_path(root, path);
 
 	key.offset = start;
@@ -910,10 +917,13 @@ int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
 	btrfs_set_file_extent_encryption(leaf, fi, 0);
 	btrfs_set_file_extent_other_encoding(leaf, fi, 0);
 
-	ret = btrfs_inc_extent_ref(trans, root, bytenr, num_bytes,
-				   leaf->start, root->root_key.objectid,
-				   trans->transid, inode->i_ino);
-	BUG_ON(ret);
+	if (orig_parent != leaf->start) {
+		ret = btrfs_update_extent_ref(trans, root, bytenr,
+					      orig_parent, leaf->start,
+					      root->root_key.objectid,
+					      trans->transid, inode->i_ino);
+		BUG_ON(ret);
+	}
 done:
 	btrfs_mark_buffer_dirty(leaf);
 	btrfs_release_path(root, path);

commit 8247b41ac980d125de8aeba6f33f381056ac0ecb
Author: Yan Zheng <zheng.yan@oracle.com>
Date:   Tue Nov 11 09:33:29 2008 -0500

    Btrfs: Fix starting search offset inside btrfs_drop_extents
    
    btrfs_drop_extents will drop paths and search again when it needs to
    force COW of higher nodes.  It was using the key it found during the last
    search as the offset for the next search.
    
    But, this wasn't always correct.  The key could be from before our desired
    range, and because we're dropping the path, it is possible for file's items
    to change while we do the search again.
    
    The fix here is to make sure we don't search for something smaller than
    the offset btrfs_drop_extents was called with.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 4119f9a95320..934bc094bf17 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -436,7 +436,7 @@ int noinline btrfs_drop_extents(struct btrfs_trans_handle *trans,
 			goto out;
 		}
 		if (recow) {
-			search_start = key.offset;
+			search_start = max(key.offset, start);
 			continue;
 		}
 		if (btrfs_key_type(&key) == BTRFS_EXTENT_DATA_KEY) {

commit 445a69449994a37615cd47e47bcab2e42a070adf
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Nov 10 11:53:33 2008 -0500

    Btrfs: Fix usage of struct extent_map->orig_start
    
    This makes sure the orig_start field in struct extent_map gets set
    everywhere the extent_map structs are created or modified.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 85841c538805..4119f9a95320 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -244,16 +244,17 @@ int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
 
 			split->start = start + len;
 			split->len = em->start + em->len - (start + len);
-			split->orig_start = em->orig_start;
 			split->bdev = em->bdev;
 			split->flags = flags;
 
 			if (compressed) {
 				split->block_len = em->block_len;
 				split->block_start = em->block_start;
+				split->orig_start = em->orig_start;
 			} else {
 				split->block_len = split->len;
 				split->block_start = em->block_start + diff;
+				split->orig_start = split->start;
 			}
 
 			ret = add_extent_mapping(em_tree, split);

commit ff5b7ee33d82414bf4baf299c21fb703bcc89629
Author: Yan Zheng <zheng.yan@oracle.com>
Date:   Mon Nov 10 07:34:43 2008 -0500

    Btrfs: Fix csum error for compressed data
    
    The decompress code doesn't take the logical offset in extent
    pointer into account. If the logical offset isn't zero, data
    will be decompressed into wrong pages.
    
    The solution used here is to record the starting offset of the extent
    in the file separately from the logical start of the extent_map struct.
    This allows us to avoid problems inserting overlapping extents.
    
    Signed-off-by: Yan Zheng <zheng.yan@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 337221ecca27..85841c538805 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -222,6 +222,7 @@ int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
 		    em->start < start) {
 			split->start = em->start;
 			split->len = start - em->start;
+			split->orig_start = em->orig_start;
 			split->block_start = em->block_start;
 
 			if (compressed)
@@ -243,6 +244,7 @@ int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
 
 			split->start = start + len;
 			split->len = em->start + em->len - (start + len);
+			split->orig_start = em->orig_start;
 			split->bdev = em->bdev;
 			split->flags = flags;
 

commit 771ed689d2cd53439e28e095bc38fbe40a71429e
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Nov 6 22:02:51 2008 -0500

    Btrfs: Optimize compressed writeback and reads
    
    When reading compressed extents, try to put pages into the page cache
    for any pages covered by the compressed extent that readpages didn't already
    preload.
    
    Add an async work queue to handle transformations at delayed allocation processing
    time.  Right now this is just compression.  The workflow is:
    
    1) Find offsets in the file marked for delayed allocation
    2) Lock the pages
    3) Lock the state bits
    4) Call the async delalloc code
    
    The async delalloc code clears the state lock bits and delalloc bits.  It is
    important this happens before the range goes into the work queue because
    otherwise it might deadlock with other work queue items that try to lock
    those extent bits.
    
    The file pages are compressed, and if the compression doesn't work the
    pages are written back directly.
    
    An ordered work queue is used to make sure the inodes are written in the same
    order that pdflush or writepages sent them down.
    
    This changes extent_write_cache_pages to let the writepage function
    update the wbc nr_written count.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 0c8cc35a8b97..337221ecca27 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -368,6 +368,8 @@ int noinline btrfs_drop_extents(struct btrfs_trans_handle *trans,
 	u64 search_start = start;
 	u64 leaf_start;
 	u64 ram_bytes = 0;
+	u64 orig_parent = 0;
+	u64 disk_bytenr = 0;
 	u8 compression;
 	u8 encryption;
 	u16 other_encoding = 0;
@@ -500,17 +502,31 @@ int noinline btrfs_drop_extents(struct btrfs_trans_handle *trans,
 				keep = 1;
 		}
 
-		if (bookend && found_extent && locked_end < extent_end) {
-			ret = try_lock_extent(&BTRFS_I(inode)->io_tree,
-					locked_end, extent_end - 1, GFP_NOFS);
-			if (!ret) {
-				btrfs_release_path(root, path);
-				lock_extent(&BTRFS_I(inode)->io_tree,
-					locked_end, extent_end - 1, GFP_NOFS);
+		if (bookend && found_extent) {
+			if (locked_end < extent_end) {
+				ret = try_lock_extent(&BTRFS_I(inode)->io_tree,
+						locked_end, extent_end - 1,
+						GFP_NOFS);
+				if (!ret) {
+					btrfs_release_path(root, path);
+					lock_extent(&BTRFS_I(inode)->io_tree,
+						locked_end, extent_end - 1,
+						GFP_NOFS);
+					locked_end = extent_end;
+					continue;
+				}
 				locked_end = extent_end;
-				continue;
 			}
-			locked_end = extent_end;
+			orig_parent = path->nodes[0]->start;
+			disk_bytenr = le64_to_cpu(old.disk_bytenr);
+			if (disk_bytenr != 0) {
+				ret = btrfs_inc_extent_ref(trans, root,
+					   disk_bytenr,
+					   le64_to_cpu(old.disk_num_bytes),
+					   orig_parent, root->root_key.objectid,
+					   trans->transid, inode->i_ino);
+				BUG_ON(ret);
+			}
 		}
 
 		if (found_inline) {
@@ -537,8 +553,12 @@ int noinline btrfs_drop_extents(struct btrfs_trans_handle *trans,
 					inode_sub_bytes(inode, old_num -
 							new_num);
 				}
-				btrfs_set_file_extent_num_bytes(leaf, extent,
-								new_num);
+				if (!compression && !encryption) {
+					btrfs_set_file_extent_ram_bytes(leaf,
+							extent, new_num);
+				}
+				btrfs_set_file_extent_num_bytes(leaf,
+							extent, new_num);
 				btrfs_mark_buffer_dirty(leaf);
 			} else if (key.offset < inline_limit &&
 				   (end > extent_end) &&
@@ -582,11 +602,11 @@ int noinline btrfs_drop_extents(struct btrfs_trans_handle *trans,
 		}
 		/* create bookend, splitting the extent in two */
 		if (bookend && found_extent) {
-			u64 disk_bytenr;
 			struct btrfs_key ins;
 			ins.objectid = inode->i_ino;
 			ins.offset = end;
 			btrfs_set_key_type(&ins, BTRFS_EXTENT_DATA_KEY);
+
 			btrfs_release_path(root, path);
 			ret = btrfs_insert_empty_item(trans, root, path, &ins,
 						      sizeof(*extent));
@@ -623,14 +643,13 @@ int noinline btrfs_drop_extents(struct btrfs_trans_handle *trans,
 
 			btrfs_mark_buffer_dirty(path->nodes[0]);
 
-			disk_bytenr = le64_to_cpu(old.disk_bytenr);
 			if (disk_bytenr != 0) {
-				ret = btrfs_inc_extent_ref(trans, root,
-						disk_bytenr,
-						le64_to_cpu(old.disk_num_bytes),
-						leaf->start,
+				ret = btrfs_update_extent_ref(trans, root,
+						disk_bytenr, orig_parent,
+					        leaf->start,
 						root->root_key.objectid,
 						trans->transid, ins.objectid);
+
 				BUG_ON(ret);
 			}
 			btrfs_release_path(root, path);

commit 70b99e6959a4c28ae1b314985eca731f3db72f1d
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Oct 31 12:46:39 2008 -0400

    Btrfs: Compression corner fixes
    
    Make sure we keep page->mapping NULL on the pages we're getting
    via alloc_page.  It gets set so a few of the callbacks can do the right
    thing, but in general these pages don't have a mapping.
    
    Don't try to truncate compressed inline items in btrfs_drop_extents.
    The whole compressed item must be preserved.
    
    Don't try to create multipage inline compressed items.  When we try to
    overwrite just the first page of the file, we would have to read in and recow
    all the pages after it in the same compressed inline items.  For now, only
    create single page inline items.
    
    Make sure we lock pages in the correct order during delalloc.  The
    search into the state tree for delalloc bytes can return bytes before
    the page we already have locked.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 238a8e215eb9..0c8cc35a8b97 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -368,8 +368,8 @@ int noinline btrfs_drop_extents(struct btrfs_trans_handle *trans,
 	u64 search_start = start;
 	u64 leaf_start;
 	u64 ram_bytes = 0;
-	u8 compression = 0;
-	u8 encryption = 0;
+	u8 compression;
+	u8 encryption;
 	u16 other_encoding = 0;
 	u64 root_gen;
 	u64 root_owner;
@@ -415,6 +415,8 @@ int noinline btrfs_drop_extents(struct btrfs_trans_handle *trans,
 		leaf_start = 0;
 		root_gen = 0;
 		root_owner = 0;
+		compression = 0;
+		encryption = 0;
 		extent = NULL;
 		leaf = path->nodes[0];
 		slot = path->slots[0];
@@ -546,8 +548,12 @@ int noinline btrfs_drop_extents(struct btrfs_trans_handle *trans,
 						   inline_limit - key.offset);
 				inode_sub_bytes(inode, extent_end -
 						inline_limit);
-				btrfs_truncate_item(trans, root, path,
-						    new_size, 1);
+				btrfs_set_file_extent_ram_bytes(leaf, extent,
+							new_size);
+				if (!compression && !encryption) {
+					btrfs_truncate_item(trans, root, path,
+							    new_size, 1);
+				}
 			}
 		}
 		/* delete the entire extent */
@@ -567,8 +573,11 @@ int noinline btrfs_drop_extents(struct btrfs_trans_handle *trans,
 			new_size = btrfs_file_extent_calc_inline_size(
 						   extent_end - end);
 			inode_sub_bytes(inode, end - key.offset);
-			ret = btrfs_truncate_item(trans, root, path,
-						  new_size, 0);
+			btrfs_set_file_extent_ram_bytes(leaf, extent,
+							new_size);
+			if (!compression && !encryption)
+				ret = btrfs_truncate_item(trans, root, path,
+							  new_size, 0);
 			BUG_ON(ret);
 		}
 		/* create bookend, splitting the extent in two */

commit d899e05215178fed903ad0e7fc1cb4d8e0cc0a88
Author: Yan Zheng <zheng.yan@oracle.com>
Date:   Thu Oct 30 14:25:28 2008 -0400

    Btrfs: Add fallocate support v2
    This patch updates btrfs-progs for fallocate support.
    
    fallocate is a little different in Btrfs because we need to tell the
    COW system that a given preallocated extent doesn't need to be
    cow'd as long as there are no snapshots of it.  This leverages the
    -o nodatacow checks.
    
    Signed-off-by: Yan Zheng <zheng.yan@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 1a0510ad030c..238a8e215eb9 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -381,7 +381,7 @@ int noinline btrfs_drop_extents(struct btrfs_trans_handle *trans,
 	int keep;
 	int slot;
 	int bookend;
-	int found_type;
+	int found_type = 0;
 	int found_extent;
 	int found_inline;
 	int recow;
@@ -442,7 +442,8 @@ int noinline btrfs_drop_extents(struct btrfs_trans_handle *trans,
 								  extent);
 			other_encoding = btrfs_file_extent_other_encoding(leaf,
 								  extent);
-			if (found_type == BTRFS_FILE_EXTENT_REG) {
+			if (found_type == BTRFS_FILE_EXTENT_REG ||
+			    found_type == BTRFS_FILE_EXTENT_PREALLOC) {
 				extent_end =
 				     btrfs_file_extent_disk_bytenr(leaf,
 								   extent);
@@ -609,8 +610,7 @@ int noinline btrfs_drop_extents(struct btrfs_trans_handle *trans,
 			 */
 			btrfs_set_file_extent_ram_bytes(leaf, extent,
 							ram_bytes);
-			btrfs_set_file_extent_type(leaf, extent,
-						   BTRFS_FILE_EXTENT_REG);
+			btrfs_set_file_extent_type(leaf, extent, found_type);
 
 			btrfs_mark_buffer_dirty(path->nodes[0]);
 
@@ -661,6 +661,243 @@ int noinline btrfs_drop_extents(struct btrfs_trans_handle *trans,
 	return ret;
 }
 
+static int extent_mergeable(struct extent_buffer *leaf, int slot,
+			    u64 objectid, u64 bytenr, u64 *start, u64 *end)
+{
+	struct btrfs_file_extent_item *fi;
+	struct btrfs_key key;
+	u64 extent_end;
+
+	if (slot < 0 || slot >= btrfs_header_nritems(leaf))
+		return 0;
+
+	btrfs_item_key_to_cpu(leaf, &key, slot);
+	if (key.objectid != objectid || key.type != BTRFS_EXTENT_DATA_KEY)
+		return 0;
+
+	fi = btrfs_item_ptr(leaf, slot, struct btrfs_file_extent_item);
+	if (btrfs_file_extent_type(leaf, fi) != BTRFS_FILE_EXTENT_REG ||
+	    btrfs_file_extent_disk_bytenr(leaf, fi) != bytenr ||
+	    btrfs_file_extent_compression(leaf, fi) ||
+	    btrfs_file_extent_encryption(leaf, fi) ||
+	    btrfs_file_extent_other_encoding(leaf, fi))
+		return 0;
+
+	extent_end = key.offset + btrfs_file_extent_num_bytes(leaf, fi);
+	if ((*start && *start != key.offset) || (*end && *end != extent_end))
+		return 0;
+
+	*start = key.offset;
+	*end = extent_end;
+	return 1;
+}
+
+/*
+ * Mark extent in the range start - end as written.
+ *
+ * This changes extent type from 'pre-allocated' to 'regular'. If only
+ * part of extent is marked as written, the extent will be split into
+ * two or three.
+ */
+int btrfs_mark_extent_written(struct btrfs_trans_handle *trans,
+			      struct btrfs_root *root,
+			      struct inode *inode, u64 start, u64 end)
+{
+	struct extent_buffer *leaf;
+	struct btrfs_path *path;
+	struct btrfs_file_extent_item *fi;
+	struct btrfs_key key;
+	u64 bytenr;
+	u64 num_bytes;
+	u64 extent_end;
+	u64 extent_offset;
+	u64 other_start;
+	u64 other_end;
+	u64 split = start;
+	u64 locked_end = end;
+	int extent_type;
+	int split_end = 1;
+	int ret;
+
+	btrfs_drop_extent_cache(inode, start, end - 1, 0);
+
+	path = btrfs_alloc_path();
+	BUG_ON(!path);
+again:
+	key.objectid = inode->i_ino;
+	key.type = BTRFS_EXTENT_DATA_KEY;
+	if (split == start)
+		key.offset = split;
+	else
+		key.offset = split - 1;
+
+	ret = btrfs_search_slot(trans, root, &key, path, -1, 1);
+	if (ret > 0 && path->slots[0] > 0)
+		path->slots[0]--;
+
+	leaf = path->nodes[0];
+	btrfs_item_key_to_cpu(leaf, &key, path->slots[0]);
+	BUG_ON(key.objectid != inode->i_ino ||
+	       key.type != BTRFS_EXTENT_DATA_KEY);
+	fi = btrfs_item_ptr(leaf, path->slots[0],
+			    struct btrfs_file_extent_item);
+	extent_type = btrfs_file_extent_type(leaf, fi);
+	BUG_ON(extent_type != BTRFS_FILE_EXTENT_PREALLOC);
+	extent_end = key.offset + btrfs_file_extent_num_bytes(leaf, fi);
+	BUG_ON(key.offset > start || extent_end < end);
+
+	bytenr = btrfs_file_extent_disk_bytenr(leaf, fi);
+	num_bytes = btrfs_file_extent_disk_num_bytes(leaf, fi);
+	extent_offset = btrfs_file_extent_offset(leaf, fi);
+
+	if (key.offset == start)
+		split = end;
+
+	if (key.offset == start && extent_end == end) {
+		int del_nr = 0;
+		int del_slot = 0;
+		u64 leaf_owner = btrfs_header_owner(leaf);
+		u64 leaf_gen = btrfs_header_generation(leaf);
+		other_start = end;
+		other_end = 0;
+		if (extent_mergeable(leaf, path->slots[0] + 1, inode->i_ino,
+				     bytenr, &other_start, &other_end)) {
+			extent_end = other_end;
+			del_slot = path->slots[0] + 1;
+			del_nr++;
+			ret = btrfs_free_extent(trans, root, bytenr, num_bytes,
+						leaf->start, leaf_owner,
+						leaf_gen, inode->i_ino, 0);
+			BUG_ON(ret);
+		}
+		other_start = 0;
+		other_end = start;
+		if (extent_mergeable(leaf, path->slots[0] - 1, inode->i_ino,
+				     bytenr, &other_start, &other_end)) {
+			key.offset = other_start;
+			del_slot = path->slots[0];
+			del_nr++;
+			ret = btrfs_free_extent(trans, root, bytenr, num_bytes,
+						leaf->start, leaf_owner,
+						leaf_gen, inode->i_ino, 0);
+			BUG_ON(ret);
+		}
+		split_end = 0;
+		if (del_nr == 0) {
+			btrfs_set_file_extent_type(leaf, fi,
+						   BTRFS_FILE_EXTENT_REG);
+			goto done;
+		}
+
+		fi = btrfs_item_ptr(leaf, del_slot - 1,
+				    struct btrfs_file_extent_item);
+		btrfs_set_file_extent_type(leaf, fi, BTRFS_FILE_EXTENT_REG);
+		btrfs_set_file_extent_num_bytes(leaf, fi,
+						extent_end - key.offset);
+		btrfs_mark_buffer_dirty(leaf);
+
+		ret = btrfs_del_items(trans, root, path, del_slot, del_nr);
+		BUG_ON(ret);
+		goto done;
+	} else if (split == start) {
+		if (locked_end < extent_end) {
+			ret = try_lock_extent(&BTRFS_I(inode)->io_tree,
+					locked_end, extent_end - 1, GFP_NOFS);
+			if (!ret) {
+				btrfs_release_path(root, path);
+				lock_extent(&BTRFS_I(inode)->io_tree,
+					locked_end, extent_end - 1, GFP_NOFS);
+				locked_end = extent_end;
+				goto again;
+			}
+			locked_end = extent_end;
+		}
+		btrfs_set_file_extent_num_bytes(leaf, fi, split - key.offset);
+		extent_offset += split - key.offset;
+	} else  {
+		BUG_ON(key.offset != start);
+		btrfs_set_file_extent_offset(leaf, fi, extent_offset +
+					     split - key.offset);
+		btrfs_set_file_extent_num_bytes(leaf, fi, extent_end - split);
+		key.offset = split;
+		btrfs_set_item_key_safe(trans, root, path, &key);
+		extent_end = split;
+	}
+
+	if (extent_end == end) {
+		split_end = 0;
+		extent_type = BTRFS_FILE_EXTENT_REG;
+	}
+	if (extent_end == end && split == start) {
+		other_start = end;
+		other_end = 0;
+		if (extent_mergeable(leaf, path->slots[0] + 1, inode->i_ino,
+				     bytenr, &other_start, &other_end)) {
+			path->slots[0]++;
+			fi = btrfs_item_ptr(leaf, path->slots[0],
+					    struct btrfs_file_extent_item);
+			key.offset = split;
+			btrfs_set_item_key_safe(trans, root, path, &key);
+			btrfs_set_file_extent_offset(leaf, fi, extent_offset);
+			btrfs_set_file_extent_num_bytes(leaf, fi,
+							other_end - split);
+			goto done;
+		}
+	}
+	if (extent_end == end && split == end) {
+		other_start = 0;
+		other_end = start;
+		if (extent_mergeable(leaf, path->slots[0] - 1 , inode->i_ino,
+				     bytenr, &other_start, &other_end)) {
+			path->slots[0]--;
+			fi = btrfs_item_ptr(leaf, path->slots[0],
+					    struct btrfs_file_extent_item);
+			btrfs_set_file_extent_num_bytes(leaf, fi, extent_end -
+							other_start);
+			goto done;
+		}
+	}
+
+	btrfs_mark_buffer_dirty(leaf);
+	btrfs_release_path(root, path);
+
+	key.offset = start;
+	ret = btrfs_insert_empty_item(trans, root, path, &key, sizeof(*fi));
+	BUG_ON(ret);
+
+	leaf = path->nodes[0];
+	fi = btrfs_item_ptr(leaf, path->slots[0],
+			    struct btrfs_file_extent_item);
+	btrfs_set_file_extent_generation(leaf, fi, trans->transid);
+	btrfs_set_file_extent_type(leaf, fi, extent_type);
+	btrfs_set_file_extent_disk_bytenr(leaf, fi, bytenr);
+	btrfs_set_file_extent_disk_num_bytes(leaf, fi, num_bytes);
+	btrfs_set_file_extent_offset(leaf, fi, extent_offset);
+	btrfs_set_file_extent_num_bytes(leaf, fi, extent_end - key.offset);
+	btrfs_set_file_extent_ram_bytes(leaf, fi, num_bytes);
+	btrfs_set_file_extent_compression(leaf, fi, 0);
+	btrfs_set_file_extent_encryption(leaf, fi, 0);
+	btrfs_set_file_extent_other_encoding(leaf, fi, 0);
+
+	ret = btrfs_inc_extent_ref(trans, root, bytenr, num_bytes,
+				   leaf->start, root->root_key.objectid,
+				   trans->transid, inode->i_ino);
+	BUG_ON(ret);
+done:
+	btrfs_mark_buffer_dirty(leaf);
+	btrfs_release_path(root, path);
+	if (split_end && split == start) {
+		split = end;
+		goto again;
+	}
+	if (locked_end > end) {
+		unlock_extent(&BTRFS_I(inode)->io_tree, end, locked_end - 1,
+			      GFP_NOFS);
+	}
+	btrfs_free_path(path);
+	return 0;
+}
+
 /*
  * this gets pages into the page cache and locks them down, it also properly
  * waits for data=ordered extents to finish before allowing the pages to be

commit 6643558db29006825dbb10012b3f8890aca4bcd5
Author: Yan Zheng <zheng.yan@oracle.com>
Date:   Thu Oct 30 14:19:50 2008 -0400

    Btrfs: Fix bookend extent race v2
    
    When dropping middle part of an extent, btrfs_drop_extents truncates
    the extent at first, then inserts a bookend extent.
    
    Since truncation and insertion can't be done atomically, there is a small
    period that the bookend extent isn't in the tree. This causes problem for
    functions that search the tree for file extent item. The way to fix this is
    lock the range of the bookend extent before truncation.
    
    Signed-off-by: Yan Zheng <zheng.yan@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index b8a7637e14a1..1a0510ad030c 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -364,6 +364,7 @@ int noinline btrfs_drop_extents(struct btrfs_trans_handle *trans,
 		       u64 start, u64 end, u64 inline_limit, u64 *hint_byte)
 {
 	u64 extent_end = 0;
+	u64 locked_end = end;
 	u64 search_start = start;
 	u64 leaf_start;
 	u64 ram_bytes = 0;
@@ -479,12 +480,6 @@ int noinline btrfs_drop_extents(struct btrfs_trans_handle *trans,
 			goto next_slot;
 		}
 
-		if (found_inline) {
-			u64 mask = root->sectorsize - 1;
-			search_start = (extent_end + mask) & ~mask;
-		} else
-			search_start = extent_end;
-
 		if (end <= extent_end && start >= key.offset && found_inline)
 			*hint_byte = EXTENT_MAP_INLINE;
 
@@ -501,6 +496,26 @@ int noinline btrfs_drop_extents(struct btrfs_trans_handle *trans,
 			if (found_inline && start <= key.offset)
 				keep = 1;
 		}
+
+		if (bookend && found_extent && locked_end < extent_end) {
+			ret = try_lock_extent(&BTRFS_I(inode)->io_tree,
+					locked_end, extent_end - 1, GFP_NOFS);
+			if (!ret) {
+				btrfs_release_path(root, path);
+				lock_extent(&BTRFS_I(inode)->io_tree,
+					locked_end, extent_end - 1, GFP_NOFS);
+				locked_end = extent_end;
+				continue;
+			}
+			locked_end = extent_end;
+		}
+
+		if (found_inline) {
+			u64 mask = root->sectorsize - 1;
+			search_start = (extent_end + mask) & ~mask;
+		} else
+			search_start = extent_end;
+
 		/* truncate existing extent */
 		if (start > key.offset) {
 			u64 new_num;
@@ -638,6 +653,10 @@ int noinline btrfs_drop_extents(struct btrfs_trans_handle *trans,
 	}
 out:
 	btrfs_free_path(path);
+	if (locked_end > end) {
+		unlock_extent(&BTRFS_I(inode)->io_tree, end, locked_end - 1,
+			      GFP_NOFS);
+	}
 	btrfs_check_file(root, inode);
 	return ret;
 }

commit 9036c10208e1fc496cef7692ba66a78699b360dc
Author: Yan Zheng <zheng.yan@oracle.com>
Date:   Thu Oct 30 14:19:41 2008 -0400

    Btrfs: update hole handling v2
    
    This patch splits the hole insertion code out of btrfs_setattr
    into btrfs_cont_expand and updates btrfs_get_extent to properly
    handle the case that file extent items are not continuous.
    
    Signed-off-by: Yan Zheng <zheng.yan@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 0aa15436590e..b8a7637e14a1 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -142,40 +142,6 @@ static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
 	}
 	set_extent_uptodate(io_tree, start_pos, end_of_last_block, GFP_NOFS);
 
-	/* FIXME...EIEIO, ENOSPC and more */
-	/* insert any holes we need to create */
-	if (isize < start_pos) {
-		u64 last_pos_in_file;
-		u64 hole_size;
-		u64 mask = root->sectorsize - 1;
-		last_pos_in_file = (isize + mask) & ~mask;
-		hole_size = (start_pos - last_pos_in_file + mask) & ~mask;
-		if (hole_size > 0) {
-			btrfs_wait_ordered_range(inode, last_pos_in_file,
-						 last_pos_in_file + hole_size);
-			mutex_lock(&BTRFS_I(inode)->extent_mutex);
-			err = btrfs_drop_extents(trans, root, inode,
-						 last_pos_in_file,
-						 last_pos_in_file + hole_size,
-						 last_pos_in_file,
-						 &hint_byte);
-			if (err)
-				goto failed;
-
-			err = btrfs_insert_file_extent(trans, root,
-						       inode->i_ino,
-						       last_pos_in_file,
-						       0, 0, hole_size, 0,
-						       hole_size, 0, 0, 0);
-			btrfs_drop_extent_cache(inode, last_pos_in_file,
-					last_pos_in_file + hole_size - 1, 0);
-			mutex_unlock(&BTRFS_I(inode)->extent_mutex);
-			btrfs_check_file(root, inode);
-		}
-		if (err)
-			goto failed;
-	}
-
 	/* check for reserved extents on each page, we don't want
 	 * to reset the delalloc bit on things that already have
 	 * extents reserved.
@@ -191,7 +157,6 @@ static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
 		i_size_write(inode, end_pos);
 		btrfs_update_inode(trans, root, inode);
 	}
-failed:
 	err = btrfs_end_transaction(trans, root);
 out_unlock:
 	unlock_extent(io_tree, start_pos, end_of_last_block, GFP_NOFS);
@@ -697,6 +662,12 @@ static int noinline prepare_pages(struct btrfs_root *root, struct file *file,
 	start_pos = pos & ~((u64)root->sectorsize - 1);
 	last_pos = ((u64)index + num_pages) << PAGE_CACHE_SHIFT;
 
+	if (start_pos > inode->i_size) {
+		err = btrfs_cont_expand(inode, start_pos);
+		if (err)
+			return err;
+	}
+
 	memset(pages, 0, num_pages * sizeof(struct page *));
 again:
 	for (i = 0; i < num_pages; i++) {

commit c8b978188c9a0fd3d535c13debd19d522b726f1f
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Oct 29 14:49:59 2008 -0400

    Btrfs: Add zlib compression support
    
    This is a large change for adding compression on reading and writing,
    both for inline and regular extents.  It does some fairly large
    surgery to the writeback paths.
    
    Compression is off by default and enabled by mount -o compress.  Even
    when the -o compress mount option is not used, it is possible to read
    compressed extents off the disk.
    
    If compression for a given set of pages fails to make them smaller, the
    file is flagged to avoid future compression attempts later.
    
    * While finding delalloc extents, the pages are locked before being sent down
    to the delalloc handler.  This allows the delalloc handler to do complex things
    such as cleaning the pages, marking them writeback and starting IO on their
    behalf.
    
    * Inline extents are inserted at delalloc time now.  This allows us to compress
    the data before inserting the inline extent, and it allows us to insert
    an inline extent that spans multiple pages.
    
    * All of the in-memory extent representations (extent_map.c, ordered-data.c etc)
    are changed to record both an in-memory size and an on disk size, as well
    as a flag for compression.
    
    From a disk format point of view, the extent pointers in the file are changed
    to record the on disk size of a given extent and some encoding flags.
    Space in the disk format is allocated for compression encoding, as well
    as encryption and a generic 'other' field.  Neither the encryption or the
    'other' field are currently used.
    
    In order to limit the amount of data read for a single random read in the
    file, the size of a compressed extent is limited to 128k.  This is a
    software only limit, the disk format supports u64 sized compressed extents.
    
    In order to limit the ram consumed while processing extents, the uncompressed
    size of a compressed extent is limited to 256k.  This is a software only limit
    and will be subject to tuning later.
    
    Checksumming is still done on compressed extents, and it is done on the
    uncompressed version of the data.  This way additional encodings can be
    layered on without having to figure out which encoding to checksum.
    
    Compression happens at delalloc time, which is basically singled threaded because
    it is usually done by a single pdflush thread.  This makes it tricky to
    spread the compression load across all the cpus on the box.  We'll have to
    look at parallel pdflush walks of dirty inodes at a later time.
    
    Decompression is hooked into readpages and it does spread across CPUs nicely.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 69abbe19add2..0aa15436590e 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -95,153 +95,6 @@ static void noinline btrfs_drop_pages(struct page **pages, size_t num_pages)
 	}
 }
 
-/* this does all the hard work for inserting an inline extent into
- * the btree.  Any existing inline extent is extended as required to make room,
- * otherwise things are inserted as required into the btree
- */
-static int noinline insert_inline_extent(struct btrfs_trans_handle *trans,
-				struct btrfs_root *root, struct inode *inode,
-				u64 offset, size_t size,
-				struct page **pages, size_t page_offset,
-				int num_pages)
-{
-	struct btrfs_key key;
-	struct btrfs_path *path;
-	struct extent_buffer *leaf;
-	char *kaddr;
-	unsigned long ptr;
-	struct btrfs_file_extent_item *ei;
-	struct page *page;
-	u32 datasize;
-	int err = 0;
-	int ret;
-	int i;
-	ssize_t cur_size;
-
-	path = btrfs_alloc_path();
-	if (!path)
-		return -ENOMEM;
-
-	btrfs_set_trans_block_group(trans, inode);
-
-	key.objectid = inode->i_ino;
-	key.offset = offset;
-	btrfs_set_key_type(&key, BTRFS_EXTENT_DATA_KEY);
-
-	ret = btrfs_search_slot(trans, root, &key, path, 0, 1);
-	if (ret < 0) {
-		err = ret;
-		goto fail;
-	}
-	if (ret == 1) {
-		struct btrfs_key found_key;
-
-		if (path->slots[0] == 0)
-			goto insert;
-
-		path->slots[0]--;
-		leaf = path->nodes[0];
-		btrfs_item_key_to_cpu(leaf, &found_key, path->slots[0]);
-
-		if (found_key.objectid != inode->i_ino)
-			goto insert;
-
-		if (found_key.type != BTRFS_EXTENT_DATA_KEY)
-			goto insert;
-		ei = btrfs_item_ptr(leaf, path->slots[0],
-				    struct btrfs_file_extent_item);
-
-		if (btrfs_file_extent_type(leaf, ei) !=
-		    BTRFS_FILE_EXTENT_INLINE) {
-			goto insert;
-		}
-		btrfs_item_key_to_cpu(leaf, &key, path->slots[0]);
-		ret = 0;
-	}
-	if (ret == 0) {
-		u32 found_size;
-		u64 found_end;
-
-		leaf = path->nodes[0];
-		ei = btrfs_item_ptr(leaf, path->slots[0],
-				    struct btrfs_file_extent_item);
-
-		if (btrfs_file_extent_type(leaf, ei) !=
-		    BTRFS_FILE_EXTENT_INLINE) {
-			err = ret;
-			btrfs_print_leaf(root, leaf);
-			printk("found wasn't inline offset %Lu inode %lu\n",
-			       offset, inode->i_ino);
-			goto fail;
-		}
-		found_size = btrfs_file_extent_inline_len(leaf,
-					  btrfs_item_nr(leaf, path->slots[0]));
-		found_end = key.offset + found_size;
-
-		if (found_end < offset + size) {
-			btrfs_release_path(root, path);
-			ret = btrfs_search_slot(trans, root, &key, path,
-						offset + size - found_end, 1);
-			BUG_ON(ret != 0);
-
-			ret = btrfs_extend_item(trans, root, path,
-						offset + size - found_end);
-			if (ret) {
-				err = ret;
-				goto fail;
-			}
-			leaf = path->nodes[0];
-			ei = btrfs_item_ptr(leaf, path->slots[0],
-					    struct btrfs_file_extent_item);
-			inode_add_bytes(inode, offset + size - found_end);
-		}
-		if (found_end < offset) {
-			ptr = btrfs_file_extent_inline_start(ei) + found_size;
-			memset_extent_buffer(leaf, 0, ptr, offset - found_end);
-		}
-	} else {
-insert:
-		btrfs_release_path(root, path);
-		datasize = offset + size - key.offset;
-		inode_add_bytes(inode, datasize);
-		datasize = btrfs_file_extent_calc_inline_size(datasize);
-		ret = btrfs_insert_empty_item(trans, root, path, &key,
-					      datasize);
-		if (ret) {
-			err = ret;
-			printk("got bad ret %d\n", ret);
-			goto fail;
-		}
-		leaf = path->nodes[0];
-		ei = btrfs_item_ptr(leaf, path->slots[0],
-				    struct btrfs_file_extent_item);
-		btrfs_set_file_extent_generation(leaf, ei, trans->transid);
-		btrfs_set_file_extent_type(leaf, ei, BTRFS_FILE_EXTENT_INLINE);
-	}
-	ptr = btrfs_file_extent_inline_start(ei) + offset - key.offset;
-
-	cur_size = size;
-	i = 0;
-	while (size > 0) {
-		page = pages[i];
-		kaddr = kmap_atomic(page, KM_USER0);
-		cur_size = min_t(size_t, PAGE_CACHE_SIZE - page_offset, size);
-		write_extent_buffer(leaf, kaddr + page_offset, ptr, cur_size);
-		kunmap_atomic(kaddr, KM_USER0);
-		page_offset = 0;
-		ptr += cur_size;
-		size -= cur_size;
-		if (i >= num_pages) {
-			printk("i %d num_pages %d\n", i, num_pages);
-		}
-		i++;
-	}
-	btrfs_mark_buffer_dirty(leaf);
-fail:
-	btrfs_free_path(path);
-	return err;
-}
-
 /*
  * after copy_from_user, pages need to be dirtied and we need to make
  * sure holes are created between the current EOF and the start of
@@ -267,8 +120,6 @@ static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
 	u64 start_pos;
 	u64 end_of_last_block;
 	u64 end_pos = pos + write_bytes;
-	u64 inline_size;
-	int did_inline = 0;
 	loff_t isize = i_size_read(inode);
 
 	start_pos = pos & ~((u64)root->sectorsize - 1);
@@ -314,7 +165,8 @@ static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
 			err = btrfs_insert_file_extent(trans, root,
 						       inode->i_ino,
 						       last_pos_in_file,
-						       0, 0, hole_size, 0);
+						       0, 0, hole_size, 0,
+						       hole_size, 0, 0, 0);
 			btrfs_drop_extent_cache(inode, last_pos_in_file,
 					last_pos_in_file + hole_size - 1, 0);
 			mutex_unlock(&BTRFS_I(inode)->extent_mutex);
@@ -324,57 +176,19 @@ static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
 			goto failed;
 	}
 
-	/*
-	 * either allocate an extent for the new bytes or setup the key
-	 * to show we are doing inline data in the extent
+	/* check for reserved extents on each page, we don't want
+	 * to reset the delalloc bit on things that already have
+	 * extents reserved.
 	 */
-	inline_size = end_pos;
-	if (isize >= BTRFS_MAX_INLINE_DATA_SIZE(root) ||
-	    inline_size > root->fs_info->max_inline ||
-	    (inline_size & (root->sectorsize -1)) == 0 ||
-	    inline_size >= BTRFS_MAX_INLINE_DATA_SIZE(root)) {
-		/* check for reserved extents on each page, we don't want
-		 * to reset the delalloc bit on things that already have
-		 * extents reserved.
-		 */
-		btrfs_set_extent_delalloc(inode, start_pos, end_of_last_block);
-		for (i = 0; i < num_pages; i++) {
-			struct page *p = pages[i];
-			SetPageUptodate(p);
-			ClearPageChecked(p);
-			set_page_dirty(p);
-		}
-	} else {
-		u64 aligned_end;
-		/* step one, delete the existing extents in this range */
-		aligned_end = (pos + write_bytes + root->sectorsize - 1) &
-			~((u64)root->sectorsize - 1);
-		mutex_lock(&BTRFS_I(inode)->extent_mutex);
-		err = btrfs_drop_extents(trans, root, inode, start_pos,
-					 aligned_end, aligned_end, &hint_byte);
-		if (err)
-			goto failed;
-		if (isize > inline_size)
-			inline_size = min_t(u64, isize, aligned_end);
-		inline_size -= start_pos;
-		err = insert_inline_extent(trans, root, inode, start_pos,
-					   inline_size, pages, 0, num_pages);
-		btrfs_drop_extent_cache(inode, start_pos, aligned_end - 1, 0);
-		BUG_ON(err);
-		mutex_unlock(&BTRFS_I(inode)->extent_mutex);
-
-		/*
-		 * an ugly way to do all the prop accounting around
-		 * the page bits and mapping tags
-		 */
-		set_page_writeback(pages[0]);
-		end_page_writeback(pages[0]);
-		did_inline = 1;
+	btrfs_set_extent_delalloc(inode, start_pos, end_of_last_block);
+	for (i = 0; i < num_pages; i++) {
+		struct page *p = pages[i];
+		SetPageUptodate(p);
+		ClearPageChecked(p);
+		set_page_dirty(p);
 	}
 	if (end_pos > isize) {
 		i_size_write(inode, end_pos);
-		if (did_inline)
-			BTRFS_I(inode)->disk_i_size = end_pos;
 		btrfs_update_inode(trans, root, inode);
 	}
 failed:
@@ -399,6 +213,7 @@ int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
 	int ret;
 	int testend = 1;
 	unsigned long flags;
+	int compressed = 0;
 
 	WARN_ON(end < start);
 	if (end == (u64)-1) {
@@ -434,6 +249,7 @@ int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
 			free_extent_map(em);
 			continue;
 		}
+		compressed = test_bit(EXTENT_FLAG_COMPRESSED, &em->flags);
 		clear_bit(EXTENT_FLAG_PINNED, &em->flags);
 		remove_extent_mapping(em_tree, em);
 
@@ -442,6 +258,12 @@ int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
 			split->start = em->start;
 			split->len = start - em->start;
 			split->block_start = em->block_start;
+
+			if (compressed)
+				split->block_len = em->block_len;
+			else
+				split->block_len = split->len;
+
 			split->bdev = em->bdev;
 			split->flags = flags;
 			ret = add_extent_mapping(em_tree, split);
@@ -459,7 +281,13 @@ int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
 			split->bdev = em->bdev;
 			split->flags = flags;
 
-			split->block_start = em->block_start + diff;
+			if (compressed) {
+				split->block_len = em->block_len;
+				split->block_start = em->block_start;
+			} else {
+				split->block_len = split->len;
+				split->block_start = em->block_start + diff;
+			}
 
 			ret = add_extent_mapping(em_tree, split);
 			BUG_ON(ret);
@@ -533,7 +361,7 @@ int btrfs_check_file(struct btrfs_root *root, struct inode *inode)
 			struct btrfs_item *item;
 			item = btrfs_item_nr(leaf, slot);
 			extent_end = found_key.offset +
-			     btrfs_file_extent_inline_len(leaf, item);
+			     btrfs_file_extent_inline_len(leaf, extent);
 			extent_end = (extent_end + root->sectorsize - 1) &
 				~((u64)root->sectorsize -1 );
 		}
@@ -573,6 +401,10 @@ int noinline btrfs_drop_extents(struct btrfs_trans_handle *trans,
 	u64 extent_end = 0;
 	u64 search_start = start;
 	u64 leaf_start;
+	u64 ram_bytes = 0;
+	u8 compression = 0;
+	u8 encryption = 0;
+	u16 other_encoding = 0;
 	u64 root_gen;
 	u64 root_owner;
 	struct extent_buffer *leaf;
@@ -589,6 +421,7 @@ int noinline btrfs_drop_extents(struct btrfs_trans_handle *trans,
 	int recow;
 	int ret;
 
+	inline_limit = 0;
 	btrfs_drop_extent_cache(inode, start, end - 1, 0);
 
 	path = btrfs_alloc_path();
@@ -637,6 +470,12 @@ int noinline btrfs_drop_extents(struct btrfs_trans_handle *trans,
 			extent = btrfs_item_ptr(leaf, slot,
 						struct btrfs_file_extent_item);
 			found_type = btrfs_file_extent_type(leaf, extent);
+			compression = btrfs_file_extent_compression(leaf,
+								    extent);
+			encryption = btrfs_file_extent_encryption(leaf,
+								  extent);
+			other_encoding = btrfs_file_extent_other_encoding(leaf,
+								  extent);
 			if (found_type == BTRFS_FILE_EXTENT_REG) {
 				extent_end =
 				     btrfs_file_extent_disk_bytenr(leaf,
@@ -646,13 +485,13 @@ int noinline btrfs_drop_extents(struct btrfs_trans_handle *trans,
 
 				extent_end = key.offset +
 				     btrfs_file_extent_num_bytes(leaf, extent);
+				ram_bytes = btrfs_file_extent_ram_bytes(leaf,
+								extent);
 				found_extent = 1;
 			} else if (found_type == BTRFS_FILE_EXTENT_INLINE) {
-				struct btrfs_item *item;
-				item = btrfs_item_nr(leaf, slot);
 				found_inline = 1;
 				extent_end = key.offset +
-				     btrfs_file_extent_inline_len(leaf, item);
+				     btrfs_file_extent_inline_len(leaf, extent);
 			}
 		} else {
 			extent_end = search_start;
@@ -680,10 +519,9 @@ int noinline btrfs_drop_extents(struct btrfs_trans_handle *trans,
 			search_start = (extent_end + mask) & ~mask;
 		} else
 			search_start = extent_end;
-		if (end <= extent_end && start >= key.offset && found_inline) {
+
+		if (end <= extent_end && start >= key.offset && found_inline)
 			*hint_byte = EXTENT_MAP_INLINE;
-			goto out;
-		}
 
 		if (found_extent) {
 			read_extent_buffer(leaf, &old, (unsigned long)extent,
@@ -770,12 +608,27 @@ int noinline btrfs_drop_extents(struct btrfs_trans_handle *trans,
 			write_extent_buffer(leaf, &old,
 					    (unsigned long)extent, sizeof(old));
 
+			btrfs_set_file_extent_compression(leaf, extent,
+							  compression);
+			btrfs_set_file_extent_encryption(leaf, extent,
+							 encryption);
+			btrfs_set_file_extent_other_encoding(leaf, extent,
+							     other_encoding);
 			btrfs_set_file_extent_offset(leaf, extent,
 				    le64_to_cpu(old.offset) + end - key.offset);
 			WARN_ON(le64_to_cpu(old.num_bytes) <
 				(extent_end - end));
 			btrfs_set_file_extent_num_bytes(leaf, extent,
 							extent_end - end);
+
+			/*
+			 * set the ram bytes to the size of the full extent
+			 * before splitting.  This is a worst case flag,
+			 * but its the best we can do because we don't know
+			 * how splitting affects compression
+			 */
+			btrfs_set_file_extent_ram_bytes(leaf, extent,
+							ram_bytes);
 			btrfs_set_file_extent_type(leaf, extent,
 						   BTRFS_FILE_EXTENT_REG);
 

commit 3bb1a1bc42f2ae9582c28adf620484efcd4da38d
Author: Yan Zheng <zheng.yan@oracle.com>
Date:   Thu Oct 9 11:46:24 2008 -0400

    Btrfs: Remove offset field from struct btrfs_extent_ref
    
    The offset field in struct btrfs_extent_ref records the position
    inside file that file extent is referenced by. In the new back
    reference system, tree leaves holding references to file extent
    are recorded explicitly. We can scan these tree leaves very quickly, so the
    offset field is not required.
    
    This patch also makes the back reference system check the objectid
    when extents are in deleting.
    
    Signed-off-by: Yan Zheng <zheng.yan@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 18dfdf5f91d1..69abbe19add2 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -788,8 +788,7 @@ int noinline btrfs_drop_extents(struct btrfs_trans_handle *trans,
 						le64_to_cpu(old.disk_num_bytes),
 						leaf->start,
 						root->root_key.objectid,
-						trans->transid,
-						ins.objectid, ins.offset);
+						trans->transid, ins.objectid);
 				BUG_ON(ret);
 			}
 			btrfs_release_path(root, path);
@@ -808,8 +807,7 @@ int noinline btrfs_drop_extents(struct btrfs_trans_handle *trans,
 						disk_bytenr,
 						le64_to_cpu(old.disk_num_bytes),
 						leaf_start, root_owner,
-						root_gen, key.objectid,
-						key.offset, 0);
+						root_gen, key.objectid, 0);
 				BUG_ON(ret);
 				*hint_byte = disk_bytenr;
 			}

commit a76a3cd40c1127ca199d4f7f37bf0d541bf44eb2
Author: Yan Zheng <zheng.yan@oracle.com>
Date:   Thu Oct 9 11:46:29 2008 -0400

    Btrfs: Count space allocated to file in bytes
    
    This patch makes btrfs count space allocated to file in bytes instead
    of 512 byte sectors.
    
    Everything else in btrfs uses a byte count instead of sector sizes or
    blocks sizes, so this fits better.
    
    Signed-off-by: Yan Zheng <zheng.yan@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index a03d1bbb19ad..18dfdf5f91d1 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -193,7 +193,7 @@ static int noinline insert_inline_extent(struct btrfs_trans_handle *trans,
 			leaf = path->nodes[0];
 			ei = btrfs_item_ptr(leaf, path->slots[0],
 					    struct btrfs_file_extent_item);
-			inode->i_blocks += (offset + size - found_end) >> 9;
+			inode_add_bytes(inode, offset + size - found_end);
 		}
 		if (found_end < offset) {
 			ptr = btrfs_file_extent_inline_start(ei) + found_size;
@@ -203,7 +203,7 @@ static int noinline insert_inline_extent(struct btrfs_trans_handle *trans,
 insert:
 		btrfs_release_path(root, path);
 		datasize = offset + size - key.offset;
-		inode->i_blocks += datasize >> 9;
+		inode_add_bytes(inode, datasize);
 		datasize = btrfs_file_extent_calc_inline_size(datasize);
 		ret = btrfs_insert_empty_item(trans, root, path, &key,
 					      datasize);
@@ -713,7 +713,8 @@ int noinline btrfs_drop_extents(struct btrfs_trans_handle *trans,
 								      extent);
 				if (btrfs_file_extent_disk_bytenr(leaf,
 								  extent)) {
-					dec_i_blocks(inode, old_num - new_num);
+					inode_sub_bytes(inode, old_num -
+							new_num);
 				}
 				btrfs_set_file_extent_num_bytes(leaf, extent,
 								new_num);
@@ -724,14 +725,17 @@ int noinline btrfs_drop_extents(struct btrfs_trans_handle *trans,
 				u32 new_size;
 				new_size = btrfs_file_extent_calc_inline_size(
 						   inline_limit - key.offset);
-				dec_i_blocks(inode, (extent_end - key.offset) -
-					(inline_limit - key.offset));
+				inode_sub_bytes(inode, extent_end -
+						inline_limit);
 				btrfs_truncate_item(trans, root, path,
 						    new_size, 1);
 			}
 		}
 		/* delete the entire extent */
 		if (!keep) {
+			if (found_inline)
+				inode_sub_bytes(inode, extent_end -
+						key.offset);
 			ret = btrfs_del_item(trans, root, path);
 			/* TODO update progress marker and return */
 			BUG_ON(ret);
@@ -743,8 +747,7 @@ int noinline btrfs_drop_extents(struct btrfs_trans_handle *trans,
 			u32 new_size;
 			new_size = btrfs_file_extent_calc_inline_size(
 						   extent_end - end);
-			dec_i_blocks(inode, (extent_end - key.offset) -
-					(extent_end - end));
+			inode_sub_bytes(inode, end - key.offset);
 			ret = btrfs_truncate_item(trans, root, path,
 						  new_size, 0);
 			BUG_ON(ret);
@@ -791,9 +794,7 @@ int noinline btrfs_drop_extents(struct btrfs_trans_handle *trans,
 			}
 			btrfs_release_path(root, path);
 			if (disk_bytenr != 0) {
-				inode->i_blocks +=
-				      btrfs_file_extent_num_bytes(leaf,
-								  extent) >> 9;
+				inode_add_bytes(inode, extent_end - end);
 			}
 		}
 
@@ -801,7 +802,8 @@ int noinline btrfs_drop_extents(struct btrfs_trans_handle *trans,
 			u64 disk_bytenr = le64_to_cpu(old.disk_bytenr);
 
 			if (disk_bytenr != 0) {
-				dec_i_blocks(inode, le64_to_cpu(old.num_bytes));
+				inode_sub_bytes(inode,
+						le64_to_cpu(old.num_bytes));
 				ret = btrfs_free_extent(trans, root,
 						disk_bytenr,
 						le64_to_cpu(old.disk_num_bytes),

commit cb843a6f513a1a91c54951005e60bd9b95bdf973
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Oct 3 12:30:02 2008 -0400

    Btrfs: O_DIRECT writes via buffered writes + invaldiate
    
    This reworks the btrfs O_DIRECT write code a bit.  It had always fallen
    back to buffered IO and done an invalidate, but needed to be updated
    for the data=ordered code.  The invalidate wasn't actually removing pages
    because they were still inside an ordered extent.
    
    This also combines the O_DIRECT/O_SYNC paths where possible, and kicks
    off IO in the main btrfs_file_write loop to keep the pipe down the the
    disk full as we process long writes.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 3088a1184483..a03d1bbb19ad 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -905,6 +905,10 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 	struct page *pinned[2];
 	unsigned long first_index;
 	unsigned long last_index;
+	int will_write;
+
+	will_write = ((file->f_flags & O_SYNC) || IS_SYNC(inode) ||
+		      (file->f_flags & O_DIRECT));
 
 	nrptrs = min((count + PAGE_CACHE_SIZE - 1) / PAGE_CACHE_SIZE,
 		     PAGE_CACHE_SIZE / (sizeof(struct page *)));
@@ -1001,15 +1005,24 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 		if (ret)
 			goto out;
 
+		if (will_write) {
+			btrfs_fdatawrite_range(inode->i_mapping, pos,
+					       pos + write_bytes - 1,
+					       WB_SYNC_NONE);
+		} else {
+			balance_dirty_pages_ratelimited_nr(inode->i_mapping,
+							   num_pages);
+			if (num_pages <
+			    (root->leafsize >> PAGE_CACHE_SHIFT) + 1)
+				btrfs_btree_balance_dirty(root, 1);
+			btrfs_throttle(root);
+		}
+
 		buf += write_bytes;
 		count -= write_bytes;
 		pos += write_bytes;
 		num_written += write_bytes;
 
-		balance_dirty_pages_ratelimited_nr(inode->i_mapping, num_pages);
-		if (num_pages < (root->leafsize >> PAGE_CACHE_SHIFT) + 1)
-			btrfs_btree_balance_dirty(root, 1);
-		btrfs_throttle(root);
 		cond_resched();
 	}
 out:
@@ -1023,36 +1036,29 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 		page_cache_release(pinned[1]);
 	*ppos = pos;
 
-	if (num_written > 0 && ((file->f_flags & O_SYNC) || IS_SYNC(inode))) {
+	if (num_written > 0 && will_write) {
 		struct btrfs_trans_handle *trans;
 
-		err = btrfs_fdatawrite_range(inode->i_mapping, start_pos,
-					     start_pos + num_written -1,
-					     WB_SYNC_NONE);
-		if (err < 0)
-			num_written = err;
-
-		err = btrfs_wait_on_page_writeback_range(inode->i_mapping,
-				 start_pos, start_pos + num_written - 1);
-		if (err < 0)
+		err = btrfs_wait_ordered_range(inode, start_pos, num_written);
+		if (err)
 			num_written = err;
 
-		trans = btrfs_start_transaction(root, 1);
-		ret = btrfs_log_dentry_safe(trans, root, file->f_dentry);
-		if (ret == 0) {
-			btrfs_sync_log(trans, root);
-			btrfs_end_transaction(trans, root);
-		} else {
-			btrfs_commit_transaction(trans, root);
+		if ((file->f_flags & O_SYNC) || IS_SYNC(inode)) {
+			trans = btrfs_start_transaction(root, 1);
+			ret = btrfs_log_dentry_safe(trans, root,
+						    file->f_dentry);
+			if (ret == 0) {
+				btrfs_sync_log(trans, root);
+				btrfs_end_transaction(trans, root);
+			} else {
+				btrfs_commit_transaction(trans, root);
+			}
+		}
+		if (file->f_flags & O_DIRECT) {
+			invalidate_mapping_pages(inode->i_mapping,
+			      start_pos >> PAGE_CACHE_SHIFT,
+			     (start_pos + num_written - 1) >> PAGE_CACHE_SHIFT);
 		}
-	} else if (num_written > 0 && (file->f_flags & O_DIRECT)) {
-		do_sync_mapping_range(inode->i_mapping, start_pos,
-				      start_pos + num_written - 1,
-				      SYNC_FILE_RANGE_WRITE |
-				      SYNC_FILE_RANGE_WAIT_AFTER);
-		invalidate_mapping_pages(inode->i_mapping,
-		      start_pos >> PAGE_CACHE_SHIFT,
-		     (start_pos + num_written - 1) >> PAGE_CACHE_SHIFT);
 	}
 	current->backing_dev_info = NULL;
 	return num_written ? num_written : err;

commit d352ac68148b69937d39ca5d48bcc4478e118dbf
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Sep 29 15:18:18 2008 -0400

    Btrfs: add and improve comments
    
    This improves the comments at the top of many functions.  It didn't
    dive into the guts of functions because I was trying to
    avoid merging problems with the new allocator and back reference work.
    
    extent-tree.c and volumes.c were both skipped, and there is definitely
    more work todo in cleaning and commenting the code.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 1b7e51a9db0f..3088a1184483 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -41,6 +41,9 @@
 #include "compat.h"
 
 
+/* simple helper to fault in pages and copy.  This should go away
+ * and be replaced with calls into generic code.
+ */
 static int noinline btrfs_copy_from_user(loff_t pos, int num_pages,
 					 int write_bytes,
 					 struct page **prepared_pages,
@@ -72,12 +75,19 @@ static int noinline btrfs_copy_from_user(loff_t pos, int num_pages,
 	return page_fault ? -EFAULT : 0;
 }
 
+/*
+ * unlocks pages after btrfs_file_write is done with them
+ */
 static void noinline btrfs_drop_pages(struct page **pages, size_t num_pages)
 {
 	size_t i;
 	for (i = 0; i < num_pages; i++) {
 		if (!pages[i])
 			break;
+		/* page checked is some magic around finding pages that
+		 * have been modified without going through btrfs_set_page_dirty
+		 * clear it here
+		 */
 		ClearPageChecked(pages[i]);
 		unlock_page(pages[i]);
 		mark_page_accessed(pages[i]);
@@ -85,6 +95,10 @@ static void noinline btrfs_drop_pages(struct page **pages, size_t num_pages)
 	}
 }
 
+/* this does all the hard work for inserting an inline extent into
+ * the btree.  Any existing inline extent is extended as required to make room,
+ * otherwise things are inserted as required into the btree
+ */
 static int noinline insert_inline_extent(struct btrfs_trans_handle *trans,
 				struct btrfs_root *root, struct inode *inode,
 				u64 offset, size_t size,
@@ -228,6 +242,14 @@ static int noinline insert_inline_extent(struct btrfs_trans_handle *trans,
 	return err;
 }
 
+/*
+ * after copy_from_user, pages need to be dirtied and we need to make
+ * sure holes are created between the current EOF and the start of
+ * any next extents (if required).
+ *
+ * this also makes the decision about creating an inline extent vs
+ * doing real data extents, marking pages dirty and delalloc as required.
+ */
 static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
 				   struct btrfs_root *root,
 				   struct file *file,
@@ -362,6 +384,10 @@ static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
 	return err;
 }
 
+/*
+ * this drops all the extents in the cache that intersect the range
+ * [start, end].  Existing extents are split as required.
+ */
 int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
 			    int skip_pinned)
 {
@@ -536,6 +562,9 @@ int btrfs_check_file(struct btrfs_root *root, struct inode *inode)
  * If an extent intersects the range but is not entirely inside the range
  * it is either truncated or split.  Anything entirely inside the range
  * is deleted from the tree.
+ *
+ * inline_limit is used to tell this code which offsets in the file to keep
+ * if they contain inline extents.
  */
 int noinline btrfs_drop_extents(struct btrfs_trans_handle *trans,
 		       struct btrfs_root *root, struct inode *inode,
@@ -796,7 +825,9 @@ int noinline btrfs_drop_extents(struct btrfs_trans_handle *trans,
 }
 
 /*
- * this gets pages into the page cache and locks them down
+ * this gets pages into the page cache and locks them down, it also properly
+ * waits for data=ordered extents to finish before allowing the pages to be
+ * modified.
  */
 static int noinline prepare_pages(struct btrfs_root *root, struct file *file,
 			 struct page **pages, size_t num_pages,
@@ -1034,6 +1065,17 @@ int btrfs_release_file(struct inode * inode, struct file * filp)
 	return 0;
 }
 
+/*
+ * fsync call for both files and directories.  This logs the inode into
+ * the tree log instead of forcing full commits whenever possible.
+ *
+ * It needs to call filemap_fdatawait so that all ordered extent updates are
+ * in the metadata btree are up to date for copying to the log.
+ *
+ * It drops the inode mutex before doing the tree log commit.  This is an
+ * important optimization for directories because holding the mutex prevents
+ * new operations on the dir while we write to disk.
+ */
 int btrfs_sync_file(struct file *file, struct dentry *dentry, int datasync)
 {
 	struct inode *inode = dentry->d_inode;

commit 5b21f2ed3f2947b5195b65c9fdbdd9e52904cc03
Author: Zheng Yan <zheng.yan@oracle.com>
Date:   Fri Sep 26 10:05:38 2008 -0400

    Btrfs: extent_map and data=ordered fixes for space balancing
    
    * Add an EXTENT_BOUNDARY state bit to keep the writepage code
    from merging data extents that are in the process of being
    relocated.  This allows us to do accounting for them properly.
    
    * The balancing code relocates data extents indepdent of the underlying
    inode.  The extent_map code was modified to properly account for
    things moving around (invalidating extent_map caches in the inode).
    
    * Don't take the drop_mutex in the create_subvol ioctl.  It isn't
    required.
    
    * Fix walking of the ordered extent list to avoid races with sys_unlink
    
    * Change the lock ordering rules.  Transaction start goes outside
    the drop_mutex.  This allows btrfs_commit_transaction to directly
    drop the relocation trees.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 8856570a0ebd..1b7e51a9db0f 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -294,7 +294,7 @@ static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
 						       last_pos_in_file,
 						       0, 0, hole_size, 0);
 			btrfs_drop_extent_cache(inode, last_pos_in_file,
-					last_pos_in_file + hole_size -1);
+					last_pos_in_file + hole_size - 1, 0);
 			mutex_unlock(&BTRFS_I(inode)->extent_mutex);
 			btrfs_check_file(root, inode);
 		}
@@ -337,7 +337,7 @@ static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
 		inline_size -= start_pos;
 		err = insert_inline_extent(trans, root, inode, start_pos,
 					   inline_size, pages, 0, num_pages);
-		btrfs_drop_extent_cache(inode, start_pos, aligned_end - 1);
+		btrfs_drop_extent_cache(inode, start_pos, aligned_end - 1, 0);
 		BUG_ON(err);
 		mutex_unlock(&BTRFS_I(inode)->extent_mutex);
 
@@ -362,7 +362,8 @@ static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
 	return err;
 }
 
-int noinline btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end)
+int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end,
+			    int skip_pinned)
 {
 	struct extent_map *em;
 	struct extent_map *split = NULL;
@@ -371,6 +372,7 @@ int noinline btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end)
 	u64 len = end - start + 1;
 	int ret;
 	int testend = 1;
+	unsigned long flags;
 
 	WARN_ON(end < start);
 	if (end == (u64)-1) {
@@ -389,6 +391,23 @@ int noinline btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end)
 			spin_unlock(&em_tree->lock);
 			break;
 		}
+		flags = em->flags;
+		if (skip_pinned && test_bit(EXTENT_FLAG_PINNED, &em->flags)) {
+			spin_unlock(&em_tree->lock);
+			if (em->start <= start &&
+			    (!testend || em->start + em->len >= start + len)) {
+				free_extent_map(em);
+				break;
+			}
+			if (start < em->start) {
+				len = em->start - start;
+			} else {
+				len = start + len - (em->start + em->len);
+				start = em->start + em->len;
+			}
+			free_extent_map(em);
+			continue;
+		}
 		clear_bit(EXTENT_FLAG_PINNED, &em->flags);
 		remove_extent_mapping(em_tree, em);
 
@@ -398,7 +417,7 @@ int noinline btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end)
 			split->len = start - em->start;
 			split->block_start = em->block_start;
 			split->bdev = em->bdev;
-			split->flags = em->flags;
+			split->flags = flags;
 			ret = add_extent_mapping(em_tree, split);
 			BUG_ON(ret);
 			free_extent_map(split);
@@ -412,7 +431,7 @@ int noinline btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end)
 			split->start = start + len;
 			split->len = em->start + em->len - (start + len);
 			split->bdev = em->bdev;
-			split->flags = em->flags;
+			split->flags = flags;
 
 			split->block_start = em->block_start + diff;
 
@@ -541,7 +560,7 @@ int noinline btrfs_drop_extents(struct btrfs_trans_handle *trans,
 	int recow;
 	int ret;
 
-	btrfs_drop_extent_cache(inode, start, end - 1);
+	btrfs_drop_extent_cache(inode, start, end - 1, 0);
 
 	path = btrfs_alloc_path();
 	if (!path)

commit 2b1f55b0f0d0d1a66470ef4ea2696cd5dd741a12
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Sep 24 11:48:04 2008 -0400

    Remove Btrfs compat code for older kernels
    
    Btrfs had compatibility code for kernels back to 2.6.18.  These have
    been removed, and will be maintained in a separate backport
    git tree from now on.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 48a702d41c8c..8856570a0ebd 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -871,15 +871,8 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 		goto out_nolock;
 	if (count == 0)
 		goto out_nolock;
-#ifdef REMOVE_SUID_PATH
-	err = remove_suid(&file->f_path);
-#else
-# if LINUX_VERSION_CODE > KERNEL_VERSION(2,6,26)
+
 	err = file_remove_suid(file);
-# else
-	err = remove_suid(fdentry(file));
-# endif
-#endif
 	if (err)
 		goto out_nolock;
 	file_update_time(file);
@@ -1003,17 +996,10 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 			btrfs_commit_transaction(trans, root);
 		}
 	} else if (num_written > 0 && (file->f_flags & O_DIRECT)) {
-#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,22)
-		do_sync_file_range(file, start_pos,
-				      start_pos + num_written - 1,
-				      SYNC_FILE_RANGE_WRITE |
-				      SYNC_FILE_RANGE_WAIT_AFTER);
-#else
 		do_sync_mapping_range(inode->i_mapping, start_pos,
 				      start_pos + num_written - 1,
 				      SYNC_FILE_RANGE_WRITE |
 				      SYNC_FILE_RANGE_WAIT_AFTER);
-#endif
 		invalidate_mapping_pages(inode->i_mapping,
 		      start_pos >> PAGE_CACHE_SHIFT,
 		     (start_pos + num_written - 1) >> PAGE_CACHE_SHIFT);
@@ -1097,12 +1083,7 @@ int btrfs_sync_file(struct file *file, struct dentry *dentry, int datasync)
 }
 
 static struct vm_operations_struct btrfs_file_vm_ops = {
-#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,23)
-	.nopage         = filemap_nopage,
-	.populate       = filemap_populate,
-#else
 	.fault		= filemap_fault,
-#endif
 	.page_mkwrite	= btrfs_page_mkwrite,
 };
 
@@ -1118,9 +1099,6 @@ struct file_operations btrfs_file_operations = {
 	.read		= do_sync_read,
 	.aio_read       = generic_file_aio_read,
 	.splice_read	= generic_file_splice_read,
-#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,18)
-	.sendfile	= generic_file_sendfile,
-#endif
 	.write		= btrfs_file_write,
 	.mmap		= btrfs_file_mmap,
 	.open		= generic_file_open,

commit 31840ae1a6b433ca0e6a8d341756ff478bbf959e
Author: Zheng Yan <zheng.yan@oracle.com>
Date:   Tue Sep 23 13:14:14 2008 -0400

    Btrfs: Full back reference support
    
    This patch makes the back reference system to explicit record the
    location of parent node for all types of extents. The location of
    parent node is placed into the offset field of backref key. Every
    time a tree block is balanced, the back references for the affected
    lower level extents are updated.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 58b329ddb426..48a702d41c8c 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -524,6 +524,9 @@ int noinline btrfs_drop_extents(struct btrfs_trans_handle *trans,
 {
 	u64 extent_end = 0;
 	u64 search_start = start;
+	u64 leaf_start;
+	u64 root_gen;
+	u64 root_owner;
 	struct extent_buffer *leaf;
 	struct btrfs_file_extent_item *extent;
 	struct btrfs_path *path;
@@ -562,6 +565,9 @@ int noinline btrfs_drop_extents(struct btrfs_trans_handle *trans,
 		bookend = 0;
 		found_extent = 0;
 		found_inline = 0;
+		leaf_start = 0;
+		root_gen = 0;
+		root_owner = 0;
 		extent = NULL;
 		leaf = path->nodes[0];
 		slot = path->slots[0];
@@ -628,27 +634,18 @@ int noinline btrfs_drop_extents(struct btrfs_trans_handle *trans,
 			search_start = extent_end;
 		if (end <= extent_end && start >= key.offset && found_inline) {
 			*hint_byte = EXTENT_MAP_INLINE;
-			continue;
+			goto out;
+		}
+
+		if (found_extent) {
+			read_extent_buffer(leaf, &old, (unsigned long)extent,
+					   sizeof(old));
+			root_gen = btrfs_header_generation(leaf);
+			root_owner = btrfs_header_owner(leaf);
+			leaf_start = leaf->start;
 		}
+
 		if (end < extent_end && end >= key.offset) {
-			if (found_extent) {
-				u64 disk_bytenr =
-				    btrfs_file_extent_disk_bytenr(leaf, extent);
-				u64 disk_num_bytes =
-				    btrfs_file_extent_disk_num_bytes(leaf,
-								      extent);
-				read_extent_buffer(leaf, &old,
-						   (unsigned long)extent,
-						   sizeof(old));
-				if (disk_bytenr != 0) {
-					ret = btrfs_inc_extent_ref(trans, root,
-					         disk_bytenr, disk_num_bytes,
-						 root->root_key.objectid,
-						 trans->transid,
-						 key.objectid, end);
-					BUG_ON(ret);
-				}
-			}
 			bookend = 1;
 			if (found_inline && start <= key.offset)
 				keep = 1;
@@ -687,49 +684,12 @@ int noinline btrfs_drop_extents(struct btrfs_trans_handle *trans,
 		}
 		/* delete the entire extent */
 		if (!keep) {
-			u64 disk_bytenr = 0;
-			u64 disk_num_bytes = 0;
-			u64 extent_num_bytes = 0;
-			u64 root_gen;
-			u64 root_owner;
-
-			root_gen = btrfs_header_generation(leaf);
-			root_owner = btrfs_header_owner(leaf);
-			if (found_extent) {
-				disk_bytenr =
-				      btrfs_file_extent_disk_bytenr(leaf,
-								     extent);
-				disk_num_bytes =
-				      btrfs_file_extent_disk_num_bytes(leaf,
-								       extent);
-				extent_num_bytes =
-				      btrfs_file_extent_num_bytes(leaf, extent);
-				*hint_byte =
-					btrfs_file_extent_disk_bytenr(leaf,
-								      extent);
-			}
 			ret = btrfs_del_item(trans, root, path);
 			/* TODO update progress marker and return */
 			BUG_ON(ret);
-			btrfs_release_path(root, path);
 			extent = NULL;
-			if (found_extent && disk_bytenr != 0) {
-				dec_i_blocks(inode, extent_num_bytes);
-				ret = btrfs_free_extent(trans, root,
-						disk_bytenr,
-						disk_num_bytes,
-						root_owner,
-						root_gen, inode->i_ino,
-						key.offset, 0);
-			}
-
-			BUG_ON(ret);
-			if (!bookend && search_start >= end) {
-				ret = 0;
-				goto out;
-			}
-			if (!bookend)
-				continue;
+			btrfs_release_path(root, path);
+			/* the extent will be freed later */
 		}
 		if (bookend && found_inline && start <= key.offset) {
 			u32 new_size;
@@ -737,10 +697,13 @@ int noinline btrfs_drop_extents(struct btrfs_trans_handle *trans,
 						   extent_end - end);
 			dec_i_blocks(inode, (extent_end - key.offset) -
 					(extent_end - end));
-			btrfs_truncate_item(trans, root, path, new_size, 0);
+			ret = btrfs_truncate_item(trans, root, path,
+						  new_size, 0);
+			BUG_ON(ret);
 		}
 		/* create bookend, splitting the extent in two */
 		if (bookend && found_extent) {
+			u64 disk_bytenr;
 			struct btrfs_key ins;
 			ins.objectid = inode->i_ino;
 			ins.offset = end;
@@ -748,13 +711,9 @@ int noinline btrfs_drop_extents(struct btrfs_trans_handle *trans,
 			btrfs_release_path(root, path);
 			ret = btrfs_insert_empty_item(trans, root, path, &ins,
 						      sizeof(*extent));
+			BUG_ON(ret);
 
 			leaf = path->nodes[0];
-			if (ret) {
-				btrfs_print_leaf(root, leaf);
-				printk("got %d on inserting %Lu %u %Lu start %Lu end %Lu found %Lu %Lu keep was %d\n", ret , ins.objectid, ins.type, ins.offset, start, end, key.offset, extent_end, keep);
-			}
-			BUG_ON(ret);
 			extent = btrfs_item_ptr(leaf, path->slots[0],
 						struct btrfs_file_extent_item);
 			write_extent_buffer(leaf, &old,
@@ -770,11 +729,43 @@ int noinline btrfs_drop_extents(struct btrfs_trans_handle *trans,
 						   BTRFS_FILE_EXTENT_REG);
 
 			btrfs_mark_buffer_dirty(path->nodes[0]);
-			if (le64_to_cpu(old.disk_bytenr) != 0) {
+
+			disk_bytenr = le64_to_cpu(old.disk_bytenr);
+			if (disk_bytenr != 0) {
+				ret = btrfs_inc_extent_ref(trans, root,
+						disk_bytenr,
+						le64_to_cpu(old.disk_num_bytes),
+						leaf->start,
+						root->root_key.objectid,
+						trans->transid,
+						ins.objectid, ins.offset);
+				BUG_ON(ret);
+			}
+			btrfs_release_path(root, path);
+			if (disk_bytenr != 0) {
 				inode->i_blocks +=
 				      btrfs_file_extent_num_bytes(leaf,
 								  extent) >> 9;
 			}
+		}
+
+		if (found_extent && !keep) {
+			u64 disk_bytenr = le64_to_cpu(old.disk_bytenr);
+
+			if (disk_bytenr != 0) {
+				dec_i_blocks(inode, le64_to_cpu(old.num_bytes));
+				ret = btrfs_free_extent(trans, root,
+						disk_bytenr,
+						le64_to_cpu(old.disk_num_bytes),
+						leaf_start, root_owner,
+						root_gen, key.objectid,
+						key.offset, 0);
+				BUG_ON(ret);
+				*hint_byte = disk_bytenr;
+			}
+		}
+
+		if (search_start >= end) {
 			ret = 0;
 			goto out;
 		}

commit 49eb7e46d47ea72a9bd2a5f8cedb04f5159cc277
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Sep 11 15:53:12 2008 -0400

    Btrfs: Dir fsync optimizations
    
    Drop i_mutex during the commit
    
    Don't bother doing the fsync at all unless the dir is marked as dirtied
    and needing fsync in this transaction.  For directories, this means
    that someone has unlinked a file from the dir without fsyncing the
    file.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 84ecf3ab8511..58b329ddb426 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1061,7 +1061,9 @@ int btrfs_sync_file(struct file *file, struct dentry *dentry, int datasync)
 	}
 	mutex_unlock(&root->fs_info->trans_mutex);
 
+	root->fs_info->tree_log_batch++;
 	filemap_fdatawait(inode->i_mapping);
+	root->fs_info->tree_log_batch++;
 
 	/*
 	 * ok we haven't committed the transaction yet, lets do a commit
@@ -1076,14 +1078,29 @@ int btrfs_sync_file(struct file *file, struct dentry *dentry, int datasync)
 	}
 
 	ret = btrfs_log_dentry_safe(trans, root, file->f_dentry);
-	if (ret < 0)
+	if (ret < 0) {
 		goto out;
+	}
+
+	/* we've logged all the items and now have a consistent
+	 * version of the file in the log.  It is possible that
+	 * someone will come in and modify the file, but that's
+	 * fine because the log is consistent on disk, and we
+	 * have references to all of the file's extents
+	 *
+	 * It is possible that someone will come in and log the
+	 * file again, but that will end up using the synchronization
+	 * inside btrfs_sync_log to keep things safe.
+	 */
+	mutex_unlock(&file->f_dentry->d_inode->i_mutex);
+
 	if (ret > 0) {
 		ret = btrfs_commit_transaction(trans, root);
 	} else {
 		btrfs_sync_log(trans, root);
 		ret = btrfs_end_transaction(trans, root);
 	}
+	mutex_lock(&file->f_dentry->d_inode->i_mutex);
 out:
 	return ret > 0 ? EIO : ret;
 }

commit e02119d5a7b4396c5a872582fddc8bd6d305a70a
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Sep 5 16:13:11 2008 -0400

    Btrfs: Add a write ahead tree log to optimize synchronous operations
    
    File syncs and directory syncs are optimized by copying their
    items into a special (copy-on-write) log tree.  There is one log tree per
    subvolume and the btrfs super block points to a tree of log tree roots.
    
    After a crash, items are copied out of the log tree and back into the
    subvolume.  See tree-log.c for all the details.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index e9e86fbaa243..84ecf3ab8511 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -36,6 +36,8 @@
 #include "btrfs_inode.h"
 #include "ioctl.h"
 #include "print-tree.h"
+#include "tree-log.h"
+#include "locking.h"
 #include "compat.h"
 
 
@@ -988,10 +990,27 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 	*ppos = pos;
 
 	if (num_written > 0 && ((file->f_flags & O_SYNC) || IS_SYNC(inode))) {
-		err = sync_page_range(inode, inode->i_mapping,
-				      start_pos, num_written);
+		struct btrfs_trans_handle *trans;
+
+		err = btrfs_fdatawrite_range(inode->i_mapping, start_pos,
+					     start_pos + num_written -1,
+					     WB_SYNC_NONE);
+		if (err < 0)
+			num_written = err;
+
+		err = btrfs_wait_on_page_writeback_range(inode->i_mapping,
+				 start_pos, start_pos + num_written - 1);
 		if (err < 0)
 			num_written = err;
+
+		trans = btrfs_start_transaction(root, 1);
+		ret = btrfs_log_dentry_safe(trans, root, file->f_dentry);
+		if (ret == 0) {
+			btrfs_sync_log(trans, root);
+			btrfs_end_transaction(trans, root);
+		} else {
+			btrfs_commit_transaction(trans, root);
+		}
 	} else if (num_written > 0 && (file->f_flags & O_DIRECT)) {
 #if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,22)
 		do_sync_file_range(file, start_pos,
@@ -1019,8 +1038,7 @@ int btrfs_release_file(struct inode * inode, struct file * filp)
 	return 0;
 }
 
-static int btrfs_sync_file(struct file *file,
-			   struct dentry *dentry, int datasync)
+int btrfs_sync_file(struct file *file, struct dentry *dentry, int datasync)
 {
 	struct inode *inode = dentry->d_inode;
 	struct btrfs_root *root = BTRFS_I(inode)->root;
@@ -1043,6 +1061,8 @@ static int btrfs_sync_file(struct file *file,
 	}
 	mutex_unlock(&root->fs_info->trans_mutex);
 
+	filemap_fdatawait(inode->i_mapping);
+
 	/*
 	 * ok we haven't committed the transaction yet, lets do a commit
 	 */
@@ -1054,7 +1074,16 @@ static int btrfs_sync_file(struct file *file,
 		ret = -ENOMEM;
 		goto out;
 	}
-	ret = btrfs_commit_transaction(trans, root);
+
+	ret = btrfs_log_dentry_safe(trans, root, file->f_dentry);
+	if (ret < 0)
+		goto out;
+	if (ret > 0) {
+		ret = btrfs_commit_transaction(trans, root);
+	} else {
+		btrfs_sync_log(trans, root);
+		ret = btrfs_end_transaction(trans, root);
+	}
 out:
 	return ret > 0 ? EIO : ret;
 }

commit a1b32a5932cfac7c38b442582285f3da2a09dfd8
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Sep 5 16:09:51 2008 -0400

    Btrfs: Add debugging checks to track down corrupted metadata
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index eb8e4556fa71..e9e86fbaa243 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -39,9 +39,10 @@
 #include "compat.h"
 
 
-static int btrfs_copy_from_user(loff_t pos, int num_pages, int write_bytes,
-				struct page **prepared_pages,
-				const char __user * buf)
+static int noinline btrfs_copy_from_user(loff_t pos, int num_pages,
+					 int write_bytes,
+					 struct page **prepared_pages,
+					 const char __user * buf)
 {
 	long page_fault = 0;
 	int i;
@@ -69,7 +70,7 @@ static int btrfs_copy_from_user(loff_t pos, int num_pages, int write_bytes,
 	return page_fault ? -EFAULT : 0;
 }
 
-static void btrfs_drop_pages(struct page **pages, size_t num_pages)
+static void noinline btrfs_drop_pages(struct page **pages, size_t num_pages)
 {
 	size_t i;
 	for (i = 0; i < num_pages; i++) {
@@ -359,7 +360,7 @@ static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
 	return err;
 }
 
-int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end)
+int noinline btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end)
 {
 	struct extent_map *em;
 	struct extent_map *split = NULL;
@@ -515,7 +516,7 @@ int btrfs_check_file(struct btrfs_root *root, struct inode *inode)
  * it is either truncated or split.  Anything entirely inside the range
  * is deleted from the tree.
  */
-int btrfs_drop_extents(struct btrfs_trans_handle *trans,
+int noinline btrfs_drop_extents(struct btrfs_trans_handle *trans,
 		       struct btrfs_root *root, struct inode *inode,
 		       u64 start, u64 end, u64 inline_limit, u64 *hint_byte)
 {
@@ -785,7 +786,7 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 /*
  * this gets pages into the page cache and locks them down
  */
-static int prepare_pages(struct btrfs_root *root, struct file *file,
+static int noinline prepare_pages(struct btrfs_root *root, struct file *file,
 			 struct page **pages, size_t num_pages,
 			 loff_t pos, unsigned long first_index,
 			 unsigned long last_index, size_t write_bytes)

commit ea8c281947950fac5f78818b767821d696c9512a
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Aug 4 23:17:27 2008 -0400

    Btrfs: Maintain a list of inodes that are delalloc and a way to wait on them
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 8915f2dc1bce..eb8e4556fa71 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -312,8 +312,7 @@ static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
 		 * to reset the delalloc bit on things that already have
 		 * extents reserved.
 		 */
-		set_extent_delalloc(io_tree, start_pos,
-				    end_of_last_block, GFP_NOFS);
+		btrfs_set_extent_delalloc(inode, start_pos, end_of_last_block);
 		for (i = 0; i < num_pages; i++) {
 			struct page *p = pages[i];
 			SetPageUptodate(p);

commit f87f057b49ee52cf5c627ab27a706e3252767c9f
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Aug 1 11:27:23 2008 -0400

    Btrfs: Improve and cleanup locking done by walk_down_tree
    
    While dropping snapshots, walk_down_tree does most of the work of checking
    reference counts and limiting tree traversal to just the blocks that
    we are freeing.
    
    It dropped and held the allocation mutex in strange and confusing ways,
    this commit changes it to only hold the mutex while actually freeing a block.
    
    The rest of the checks around reference counts should be safe without the lock
    because we only allow one process in btrfs_drop_snapshot at a time.  Other
    processes dropping reference counts should not drop it to 1 because
    their tree roots already have an extra ref on the block.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index c78f184ee5cc..8915f2dc1bce 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -338,6 +338,13 @@ static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
 		btrfs_drop_extent_cache(inode, start_pos, aligned_end - 1);
 		BUG_ON(err);
 		mutex_unlock(&BTRFS_I(inode)->extent_mutex);
+
+		/*
+		 * an ugly way to do all the prop accounting around
+		 * the page bits and mapping tags
+		 */
+		set_page_writeback(pages[0]);
+		end_page_writeback(pages[0]);
 		did_inline = 1;
 	}
 	if (end_pos > isize) {
@@ -833,11 +840,7 @@ static int prepare_pages(struct btrfs_root *root, struct file *file,
 			      start_pos, last_pos - 1, GFP_NOFS);
 	}
 	for (i = 0; i < num_pages; i++) {
-#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,18)
-		ClearPageDirty(pages[i]);
-#else
-		cancel_dirty_page(pages[i], PAGE_CACHE_SIZE);
-#endif
+		clear_page_dirty_for_io(pages[i]);
 		set_page_extent_mapped(pages[i]);
 		WARN_ON(!PageLocked(pages[i]));
 	}

commit 3ce7e67a069b919be774a341b82fc20978b7f69d
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Jul 31 15:42:54 2008 -0400

    Btrfs: Drop some debugging around the extent_map pinned flag
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index d3f2fe0b7c6c..c78f184ee5cc 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -380,15 +380,7 @@ int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end)
 			spin_unlock(&em_tree->lock);
 			break;
 		}
-		if (test_bit(EXTENT_FLAG_PINNED, &em->flags)) {
-			printk(KERN_CRIT "inode %lu trying to drop pinned "
-			       "extent start %llu end %llu, em [%llu %llu]\n",
-			       inode->i_ino,
-			       (unsigned long long)start,
-			       (unsigned long long)end,
-			       (unsigned long long)em->start,
-			       (unsigned long long)em->len);
-		}
+		clear_bit(EXTENT_FLAG_PINNED, &em->flags);
 		remove_extent_mapping(em_tree, em);
 
 		if (em->block_start < EXTENT_MAP_LAST_BYTE &&

commit 37d1aeee3990385e9bb436c50c2f7e120a668df6
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Jul 31 10:48:37 2008 -0400

    Btrfs: Throttle tuning
    
    This avoids waiting for transactions with pages locked by breaking out
    the code to wait for the current transaction to close into a function
    called by btrfs_throttle.
    
    It also lowers the limits for where we start throttling.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 388ac397c2c8..d3f2fe0b7c6c 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -253,7 +253,7 @@ static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
 	end_of_last_block = start_pos + num_bytes - 1;
 
 	lock_extent(io_tree, start_pos, end_of_last_block, GFP_NOFS);
-	trans = btrfs_start_transaction(root, 1);
+	trans = btrfs_join_transaction(root, 1);
 	if (!trans) {
 		err = -ENOMEM;
 		goto out_unlock;

commit 0ee0fda06b943d7ef65f3cec50ab9d427a2baf97
Author: Sven Wegener <sven.wegener@stealer.net>
Date:   Wed Jul 30 16:54:26 2008 -0400

    Btrfs: Add compatibility for kernels >= 2.6.27-rc1
    
    Add a couple of #if's to follow API changes.
    
    Signed-off-by: Sven Wegener <sven.wegener@stealer.net>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 412ab4a26382..388ac397c2c8 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -886,7 +886,11 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 #ifdef REMOVE_SUID_PATH
 	err = remove_suid(&file->f_path);
 #else
+# if LINUX_VERSION_CODE > KERNEL_VERSION(2,6,26)
+	err = file_remove_suid(file);
+# else
 	err = remove_suid(fdentry(file));
+# endif
 #endif
 	if (err)
 		goto out_nolock;

commit bcc63abbf3e9bf948a1b0129b3e6120ec7d7f698
Author: Yan <zheng.yan@oracle.com>
Date:   Wed Jul 30 16:29:20 2008 -0400

    Btrfs: implement memory reclaim for leaf reference cache
    
    The memory reclaiming issue happens when snapshot exists. In that
    case, some cache entries may not be used during old snapshot dropping,
    so they will remain in the cache until umount.
    
    The patch adds a field to struct btrfs_leaf_ref to record create time. Besides,
    the patch makes all dead roots of a given snapshot linked together in order of
    create time. After a old snapshot was completely dropped, we check the dead
    root list and remove all cache entries created before the oldest dead root in
    the list.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index ded5281f8463..412ab4a26382 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1095,4 +1095,3 @@ struct file_operations btrfs_file_operations = {
 	.compat_ioctl	= btrfs_ioctl,
 #endif
 };
-

commit ab78c84de1ce4db1b2a2cef361625ad80abbab3f
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Jul 29 16:15:18 2008 -0400

    Btrfs: Throttle operations if the reference cache gets too large
    
    A large reference cache is directly related to a lot of work pending
    for the cleaner thread.  This throttles back new operations based on
    the size of the reference cache so the cleaner thread will be able to keep
    up.
    
    Overall, this actually makes the FS faster because the cleaner thread will
    be more likely to find things in cache.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 3efec25e34b0..ded5281f8463 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -974,6 +974,7 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 		balance_dirty_pages_ratelimited_nr(inode->i_mapping, num_pages);
 		if (num_pages < (root->leafsize >> PAGE_CACHE_SHIFT) + 1)
 			btrfs_btree_balance_dirty(root, 1);
+		btrfs_throttle(root);
 		cond_resched();
 	}
 out:

commit 017e5369eb353559d68a11d4a718faa634533821
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Jul 28 15:32:51 2008 -0400

    Btrfs: Leaf reference cache update
    
    This changes the reference cache to make a single cache per root
    instead of one cache per transaction, and to key by the byte number
    of the disk block instead of the keys inside.
    
    This makes it much less likely to have cache misses if a snapshot
    or something has an extra reference on a higher node or a leaf while
    the first transaction that added the leaf into the cache is dropping.
    
    Some throttling is added to functions that free blocks heavily so they
    wait for old transactions to drop.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index e5ffb66ad320..3efec25e34b0 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -347,7 +347,7 @@ static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
 		btrfs_update_inode(trans, root, inode);
 	}
 failed:
-	err = btrfs_end_transaction_throttle(trans, root);
+	err = btrfs_end_transaction(trans, root);
 out_unlock:
 	unlock_extent(io_tree, start_pos, end_of_last_block, GFP_NOFS);
 	return err;

commit f421950f86bf96a11fef932e167ab2e70d4c43a0
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Jul 22 11:18:09 2008 -0400

    Btrfs: Fix some data=ordered related data corruptions
    
    Stress testing was showing data checksum errors, most of which were caused
    by a lookup bug in the extent_map tree.  The tree was caching the last
    pointer returned, and searches would check the last pointer first.
    
    But, search callers also expect the search to return the very first
    matching extent in the range, which wasn't always true with the last
    pointer usage.
    
    For now, the code to cache the last return value is just removed.  It is
    easy to fix, but I think lookups are rare enough that it isn't required anymore.
    
    This commit also replaces do_sync_mapping_range with a local copy of the
    related functions.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 591a30208acd..e5ffb66ad320 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -381,14 +381,13 @@ int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end)
 			break;
 		}
 		if (test_bit(EXTENT_FLAG_PINNED, &em->flags)) {
-			start = em->start + em->len;
-			free_extent_map(em);
-			spin_unlock(&em_tree->lock);
-			if (start < end) {
-				len = end - start + 1;
-				continue;
-			}
-			break;
+			printk(KERN_CRIT "inode %lu trying to drop pinned "
+			       "extent start %llu end %llu, em [%llu %llu]\n",
+			       inode->i_ino,
+			       (unsigned long long)start,
+			       (unsigned long long)end,
+			       (unsigned long long)em->start,
+			       (unsigned long long)em->len);
 		}
 		remove_extent_mapping(em_tree, em);
 

commit 4a09675279674041862d2210635b0cc1f60be28e
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Jul 21 10:29:44 2008 -0400

    Btrfs: Data ordered fixes
    
    * In btrfs_delete_inode, wait for ordered extents after calling
    truncate_inode_pages.  This is much faster, and more correct
    
    * Properly clear our the PageChecked bit everywhere we redirty the page.
    
    * Change the writepage fixup handler to lock the page range and check to
    see if an ordered extent had been inserted since the improperly dirtied
    page was discovered
    
    * Wait for ordered extents outside the transaction.  This isn't required
    for locking rules but does improve transaction latencies
    
    * Reduce contention on the alloc_mutex by dropping it while incrementing
    refs on a node/leaf and while dropping refs on a leaf.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index eccdb9562ba8..591a30208acd 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -75,6 +75,7 @@ static void btrfs_drop_pages(struct page **pages, size_t num_pages)
 	for (i = 0; i < num_pages; i++) {
 		if (!pages[i])
 			break;
+		ClearPageChecked(pages[i]);
 		unlock_page(pages[i]);
 		mark_page_accessed(pages[i]);
 		page_cache_release(pages[i]);

commit 7f3c74fb831fa19bafe087e817c0a5ff3883f1ea
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Jul 18 12:01:11 2008 -0400

    Btrfs: Keep extent mappings in ram until pending ordered extents are done
    
    It was possible for stale mappings from disk to be used instead of the
    new pending ordered extent.  This adds a flag to the extent map struct
    to keep it pinned until the pending ordered extent is actually on disk.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 40ad1b2958cb..eccdb9562ba8 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -358,9 +358,7 @@ int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end)
 	struct extent_map *split = NULL;
 	struct extent_map *split2 = NULL;
 	struct extent_map_tree *em_tree = &BTRFS_I(inode)->extent_tree;
-	struct extent_map *tmp;
 	u64 len = end - start + 1;
-	u64 next_start;
 	int ret;
 	int testend = 1;
 
@@ -381,8 +379,16 @@ int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end)
 			spin_unlock(&em_tree->lock);
 			break;
 		}
-		tmp = rb_entry(&em->rb_node, struct extent_map, rb_node);
-		next_start = tmp->start;
+		if (test_bit(EXTENT_FLAG_PINNED, &em->flags)) {
+			start = em->start + em->len;
+			free_extent_map(em);
+			spin_unlock(&em_tree->lock);
+			if (start < end) {
+				len = end - start + 1;
+				continue;
+			}
+			break;
+		}
 		remove_extent_mapping(em_tree, em);
 
 		if (em->block_start < EXTENT_MAP_LAST_BYTE &&

commit ee6e6504e147a59a9f4d582662c105e9d72ae638
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Jul 17 12:54:40 2008 -0400

    Add a per-inode lock around btrfs_drop_extents
    
    btrfs_drop_extents is always called with a range lock held on the inode.
    But, it may operate on extents outside that range as it drops and splits
    them.
    
    This patch adds a per-inode mutex that is held while calling
    btrfs_drop_extents and while inserting new extents into the tree.  It
    prevents races from two procs working against adjacent ranges in the tree.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 3e4e5c227c0c..40ad1b2958cb 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -242,6 +242,7 @@ static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
 	u64 end_of_last_block;
 	u64 end_pos = pos + write_bytes;
 	u64 inline_size;
+	int did_inline = 0;
 	loff_t isize = i_size_read(inode);
 
 	start_pos = pos & ~((u64)root->sectorsize - 1);
@@ -275,6 +276,7 @@ static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
 		if (hole_size > 0) {
 			btrfs_wait_ordered_range(inode, last_pos_in_file,
 						 last_pos_in_file + hole_size);
+			mutex_lock(&BTRFS_I(inode)->extent_mutex);
 			err = btrfs_drop_extents(trans, root, inode,
 						 last_pos_in_file,
 						 last_pos_in_file + hole_size,
@@ -289,6 +291,7 @@ static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
 						       0, 0, hole_size, 0);
 			btrfs_drop_extent_cache(inode, last_pos_in_file,
 					last_pos_in_file + hole_size -1);
+			mutex_unlock(&BTRFS_I(inode)->extent_mutex);
 			btrfs_check_file(root, inode);
 		}
 		if (err)
@@ -321,6 +324,7 @@ static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
 		/* step one, delete the existing extents in this range */
 		aligned_end = (pos + write_bytes + root->sectorsize - 1) &
 			~((u64)root->sectorsize - 1);
+		mutex_lock(&BTRFS_I(inode)->extent_mutex);
 		err = btrfs_drop_extents(trans, root, inode, start_pos,
 					 aligned_end, aligned_end, &hint_byte);
 		if (err)
@@ -332,9 +336,13 @@ static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
 					   inline_size, pages, 0, num_pages);
 		btrfs_drop_extent_cache(inode, start_pos, aligned_end - 1);
 		BUG_ON(err);
+		mutex_unlock(&BTRFS_I(inode)->extent_mutex);
+		did_inline = 1;
 	}
 	if (end_pos > isize) {
 		i_size_write(inode, end_pos);
+		if (did_inline)
+			BTRFS_I(inode)->disk_i_size = end_pos;
 		btrfs_update_inode(trans, root, inode);
 	}
 failed:

commit ba1da2f442ec91a1534afa893f9bef7e33056ace
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Jul 17 12:54:15 2008 -0400

    Btrfs: Don't pin pages in ram until the entire ordered extent is on disk.
    
    Checksum items are not inserted until the entire ordered extent is on disk,
    but individual pages might be clean and available for reclaim long before
    the whole extent is on disk.
    
    In order to allow those pages to be freed, we need to be able to search
    the list of ordered extents to find the checksum that is going to be inserted
    in the tree.  This way if the page needs to be read back in before
    the checksums are in the btree, we'll be able to verify the checksum on
    the page.
    
    This commit adds the ability to search the pending ordered extents for
    a given offset in the file, and changes btrfs_releasepage to allow
    ordered pages to be freed.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index d6505892cd52..3e4e5c227c0c 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -251,7 +251,7 @@ static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
 	end_of_last_block = start_pos + num_bytes - 1;
 
 	lock_extent(io_tree, start_pos, end_of_last_block, GFP_NOFS);
-	trans = btrfs_join_transaction(root, 1);
+	trans = btrfs_start_transaction(root, 1);
 	if (!trans) {
 		err = -ENOMEM;
 		goto out_unlock;

commit f9295749388f82c8d2f485e99c72cd7c7876a99b
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Jul 17 12:54:14 2008 -0400

    btrfs_start_transaction: wait for commits in progress to finish
    
    btrfs_commit_transaction has to loop waiting for any writers in the
    transaction to finish before it can proceed.  btrfs_start_transaction
    should be polite and not join a transaction that is in the process
    of being finished off.
    
    There are a few places that can't wait, basically the ones doing IO that
    might be needed to finish the transaction.  For them, btrfs_join_transaction
    is added.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 3e4e5c227c0c..d6505892cd52 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -251,7 +251,7 @@ static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
 	end_of_last_block = start_pos + num_bytes - 1;
 
 	lock_extent(io_tree, start_pos, end_of_last_block, GFP_NOFS);
-	trans = btrfs_start_transaction(root, 1);
+	trans = btrfs_join_transaction(root, 1);
 	if (!trans) {
 		err = -ENOMEM;
 		goto out_unlock;

commit dbe674a99c8af088faa4c95eddaeb271a3140ab6
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Jul 17 12:54:05 2008 -0400

    Btrfs: Update on disk i_size only after pending ordered extents are done
    
    This changes the ordered data code to update i_size after the extent
    is on disk.  An on disk i_size is maintained in the in-memory btrfs inode
    structures, and this is updated as extents finish.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 20928639d173..3e4e5c227c0c 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -338,7 +338,7 @@ static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
 		btrfs_update_inode(trans, root, inode);
 	}
 failed:
-	err = btrfs_end_transaction(trans, root);
+	err = btrfs_end_transaction_throttle(trans, root);
 out_unlock:
 	unlock_extent(io_tree, start_pos, end_of_last_block, GFP_NOFS);
 	return err;

commit 247e743cbe6e655768c3679f84821e03c1577902
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Jul 17 12:53:51 2008 -0400

    Btrfs: Use async helpers to deal with pages that have been improperly dirtied
    
    Higher layers sometimes call set_page_dirty without asking the filesystem
    to help.  This causes many problems for the data=ordered and cow code.
    This commit detects pages that haven't been properly setup for IO and
    kicks off an async helper to deal with them.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 12e765f7e0d4..20928639d173 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -313,6 +313,7 @@ static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
 		for (i = 0; i < num_pages; i++) {
 			struct page *p = pages[i];
 			SetPageUptodate(p);
+			ClearPageChecked(p);
 			set_page_dirty(p);
 		}
 	} else {

commit e6dcd2dc9c489108648e2ed543315dd134d50a9a
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Jul 17 12:53:50 2008 -0400

    Btrfs: New data=ordered implementation
    
    The old data=ordered code would force commit to wait until
    all the data extents from the transaction were fully on disk.  This
    introduced large latencies into the commit and stalled new writers
    in the transaction for a long time.
    
    The new code changes the way data allocations and extents work:
    
    * When delayed allocation is filled, data extents are reserved, and
      the extent bit EXTENT_ORDERED is set on the entire range of the extent.
      A struct btrfs_ordered_extent is allocated an inserted into a per-inode
      rbtree to track the pending extents.
    
    * As each page is written EXTENT_ORDERED is cleared on the bytes corresponding
      to that page.
    
    * When all of the bytes corresponding to a single struct btrfs_ordered_extent
      are written, The previously reserved extent is inserted into the FS
      btree and into the extent allocation trees.  The checksums for the file
      data are also updated.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 8037792f8789..12e765f7e0d4 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -34,7 +34,6 @@
 #include "disk-io.h"
 #include "transaction.h"
 #include "btrfs_inode.h"
-#include "ordered-data.h"
 #include "ioctl.h"
 #include "print-tree.h"
 #include "compat.h"
@@ -273,7 +272,9 @@ static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
 		u64 mask = root->sectorsize - 1;
 		last_pos_in_file = (isize + mask) & ~mask;
 		hole_size = (start_pos - last_pos_in_file + mask) & ~mask;
-		if (last_pos_in_file < start_pos) {
+		if (hole_size > 0) {
+			btrfs_wait_ordered_range(inode, last_pos_in_file,
+						 last_pos_in_file + hole_size);
 			err = btrfs_drop_extents(trans, root, inode,
 						 last_pos_in_file,
 						 last_pos_in_file + hole_size,
@@ -303,19 +304,17 @@ static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
 	    inline_size > root->fs_info->max_inline ||
 	    (inline_size & (root->sectorsize -1)) == 0 ||
 	    inline_size >= BTRFS_MAX_INLINE_DATA_SIZE(root)) {
-		u64 last_end;
-
+		/* check for reserved extents on each page, we don't want
+		 * to reset the delalloc bit on things that already have
+		 * extents reserved.
+		 */
+		set_extent_delalloc(io_tree, start_pos,
+				    end_of_last_block, GFP_NOFS);
 		for (i = 0; i < num_pages; i++) {
 			struct page *p = pages[i];
 			SetPageUptodate(p);
 			set_page_dirty(p);
 		}
-		last_end = (u64)(pages[num_pages -1]->index) <<
-				PAGE_CACHE_SHIFT;
-		last_end += PAGE_CACHE_SIZE - 1;
-		set_extent_delalloc(io_tree, start_pos, end_of_last_block,
-				 GFP_NOFS);
-		btrfs_add_ordered_inode(inode);
 	} else {
 		u64 aligned_end;
 		/* step one, delete the existing extents in this range */
@@ -350,10 +349,13 @@ int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end)
 	struct extent_map *split = NULL;
 	struct extent_map *split2 = NULL;
 	struct extent_map_tree *em_tree = &BTRFS_I(inode)->extent_tree;
+	struct extent_map *tmp;
 	u64 len = end - start + 1;
+	u64 next_start;
 	int ret;
 	int testend = 1;
 
+	WARN_ON(end < start);
 	if (end == (u64)-1) {
 		len = (u64)-1;
 		testend = 0;
@@ -370,6 +372,8 @@ int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end)
 			spin_unlock(&em_tree->lock);
 			break;
 		}
+		tmp = rb_entry(&em->rb_node, struct extent_map, rb_node);
+		next_start = tmp->start;
 		remove_extent_mapping(em_tree, em);
 
 		if (em->block_start < EXTENT_MAP_LAST_BYTE &&
@@ -778,37 +782,58 @@ static int prepare_pages(struct btrfs_root *root, struct file *file,
 	struct inode *inode = fdentry(file)->d_inode;
 	int err = 0;
 	u64 start_pos;
+	u64 last_pos;
 
 	start_pos = pos & ~((u64)root->sectorsize - 1);
+	last_pos = ((u64)index + num_pages) << PAGE_CACHE_SHIFT;
 
 	memset(pages, 0, num_pages * sizeof(struct page *));
-
+again:
 	for (i = 0; i < num_pages; i++) {
 		pages[i] = grab_cache_page(inode->i_mapping, index + i);
 		if (!pages[i]) {
 			err = -ENOMEM;
 			BUG_ON(1);
 		}
-#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,18)
-		ClearPageDirty(pages[i]);
-#else
-		cancel_dirty_page(pages[i], PAGE_CACHE_SIZE);
-#endif
 		wait_on_page_writeback(pages[i]);
-		set_page_extent_mapped(pages[i]);
-		WARN_ON(!PageLocked(pages[i]));
 	}
 	if (start_pos < inode->i_size) {
-		u64 last_pos;
-		last_pos = ((u64)index + num_pages) << PAGE_CACHE_SHIFT;
+		struct btrfs_ordered_extent *ordered;
 		lock_extent(&BTRFS_I(inode)->io_tree,
 			    start_pos, last_pos - 1, GFP_NOFS);
+		ordered = btrfs_lookup_first_ordered_extent(inode, last_pos -1);
+		if (ordered &&
+		    ordered->file_offset + ordered->len > start_pos &&
+		    ordered->file_offset < last_pos) {
+			btrfs_put_ordered_extent(ordered);
+			unlock_extent(&BTRFS_I(inode)->io_tree,
+				      start_pos, last_pos - 1, GFP_NOFS);
+			for (i = 0; i < num_pages; i++) {
+				unlock_page(pages[i]);
+				page_cache_release(pages[i]);
+			}
+			btrfs_wait_ordered_range(inode, start_pos,
+						 last_pos - start_pos);
+			goto again;
+		}
+		if (ordered)
+			btrfs_put_ordered_extent(ordered);
+
 		clear_extent_bits(&BTRFS_I(inode)->io_tree, start_pos,
 				  last_pos - 1, EXTENT_DIRTY | EXTENT_DELALLOC,
 				  GFP_NOFS);
 		unlock_extent(&BTRFS_I(inode)->io_tree,
 			      start_pos, last_pos - 1, GFP_NOFS);
 	}
+	for (i = 0; i < num_pages; i++) {
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,18)
+		ClearPageDirty(pages[i]);
+#else
+		cancel_dirty_page(pages[i], PAGE_CACHE_SIZE);
+#endif
+		set_page_extent_mapped(pages[i]);
+		WARN_ON(!PageLocked(pages[i]));
+	}
 	return 0;
 }
 
@@ -969,13 +994,11 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 		     (start_pos + num_written - 1) >> PAGE_CACHE_SHIFT);
 	}
 	current->backing_dev_info = NULL;
-	btrfs_ordered_throttle(root, inode);
 	return num_written ? num_written : err;
 }
 
 int btrfs_release_file(struct inode * inode, struct file * filp)
 {
-	btrfs_del_ordered_inode(inode, 0);
 	if (filp->private_data)
 		btrfs_ioctl_trans_end(filp);
 	return 0;

commit 1b1e2135dc1e4efbcf25ac9ac9979316d4e1193e
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Jun 25 16:01:31 2008 -0400

    Btrfs: Add a per-inode csum mutex to avoid races creating csum items
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index ece221cba90c..8037792f8789 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -267,13 +267,13 @@ static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
 
 	/* FIXME...EIEIO, ENOSPC and more */
 	/* insert any holes we need to create */
-	if (isize < end_pos) {
+	if (isize < start_pos) {
 		u64 last_pos_in_file;
 		u64 hole_size;
 		u64 mask = root->sectorsize - 1;
 		last_pos_in_file = (isize + mask) & ~mask;
-		hole_size = (end_pos - last_pos_in_file + mask) & ~mask;
-		if (last_pos_in_file < end_pos) {
+		hole_size = (start_pos - last_pos_in_file + mask) & ~mask;
+		if (last_pos_in_file < start_pos) {
 			err = btrfs_drop_extents(trans, root, inode,
 						 last_pos_in_file,
 						 last_pos_in_file + hole_size,

commit 89ce8a63d0c761fbb02089850605360f389477d8
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Jun 25 16:01:31 2008 -0400

    Add btrfs_end_transaction_throttle to force writers to wait for pending commits
    
    The existing throttle mechanism was often not sufficient to prevent
    new writers from coming in and making a given transaction run forever.
    This adds an explicit wait at the end of most operations so they will
    allow the current transaction to close.
    
    There is no wait inside file_write, inode updates, or cow filling, all which
    have different deadlock possibilities.
    
    This is a temporary measure until better asynchronous commit support is
    added.  This code leads to stalls as it waits for data=ordered
    writeback, and it really needs to be fixed.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index b7f8f92daf8a..ece221cba90c 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -934,7 +934,6 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 		balance_dirty_pages_ratelimited_nr(inode->i_mapping, num_pages);
 		if (num_pages < (root->leafsize >> PAGE_CACHE_SHIFT) + 1)
 			btrfs_btree_balance_dirty(root, 1);
-		btrfs_throttle(root);
 		cond_resched();
 	}
 out:

commit 594a24eb0e7fa8413f8b443863be4b7c72bfde9f
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Jun 25 16:01:30 2008 -0400

    Fix btrfs_del_ordered_inode to allow forcing the drop during unlinks
    
    This allows us to delete an unlinked inode with dirty pages from the list
    instead of forcing commit to write these out before deleting the inode.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 18bbe108a0e6..b7f8f92daf8a 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -976,7 +976,7 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 
 int btrfs_release_file(struct inode * inode, struct file * filp)
 {
-	btrfs_del_ordered_inode(inode);
+	btrfs_del_ordered_inode(inode, 0);
 	if (filp->private_data)
 		btrfs_ioctl_trans_end(filp);
 	return 0;

commit a213501153fd66e2359e091b1612841305ba6551
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Jun 25 16:01:30 2008 -0400

    Btrfs: Replace the big fs_mutex with a collection of other locks
    
    Extent alloctions are still protected by a large alloc_mutex.
    Objectid allocations are covered by a objectid mutex
    Other btree operations are protected by a lock on individual btree nodes
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 73c6d085bd90..18bbe108a0e6 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -252,7 +252,6 @@ static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
 	end_of_last_block = start_pos + num_bytes - 1;
 
 	lock_extent(io_tree, start_pos, end_of_last_block, GFP_NOFS);
-	mutex_lock(&root->fs_info->fs_mutex);
 	trans = btrfs_start_transaction(root, 1);
 	if (!trans) {
 		err = -ENOMEM;
@@ -341,7 +340,6 @@ static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
 failed:
 	err = btrfs_end_transaction(trans, root);
 out_unlock:
-	mutex_unlock(&root->fs_info->fs_mutex);
 	unlock_extent(io_tree, start_pos, end_of_last_block, GFP_NOFS);
 	return err;
 }
@@ -905,9 +903,7 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 		WARN_ON(num_pages > nrptrs);
 		memset(pages, 0, sizeof(pages));
 
-		mutex_lock(&root->fs_info->fs_mutex);
 		ret = btrfs_check_free_space(root, write_bytes, 0);
-		mutex_unlock(&root->fs_info->fs_mutex);
 		if (ret)
 			goto out;
 
@@ -998,9 +994,9 @@ static int btrfs_sync_file(struct file *file,
 	 * check the transaction that last modified this inode
 	 * and see if its already been committed
 	 */
-	mutex_lock(&root->fs_info->fs_mutex);
 	if (!BTRFS_I(inode)->last_trans)
 		goto out;
+
 	mutex_lock(&root->fs_info->trans_mutex);
 	if (BTRFS_I(inode)->last_trans <=
 	    root->fs_info->last_trans_committed) {
@@ -1023,7 +1019,6 @@ static int btrfs_sync_file(struct file *file,
 	}
 	ret = btrfs_commit_transaction(trans, root);
 out:
-	mutex_unlock(&root->fs_info->fs_mutex);
 	return ret > 0 ? EIO : ret;
 }
 

commit 6bf13c0cc833bf5ba013d6aa60379484bf48c4e6
Author: Sage Weil <sage@newdream.net>
Date:   Tue Jun 10 10:07:39 2008 -0400

    Btrfs: transaction ioctls
    
    These ioctls let a user application hold a transaction open while it
    performs a series of operations.  A final ioctl does a sync on the fs
    (closing the current transaction).  This is the main requirement for
    Ceph's OSD to be able to keep the data it's storing in a btrfs volume
    consistent, and AFAICS it works just fine.  The application would do
    something like
    
            fd = ::open("some/file", O_RDONLY);
            ::ioctl(fd, BTRFS_IOC_TRANS_START);
            /* do a bunch of stuff */
            ::ioctl(fd, BTRFS_IOC_TRANS_END);
    or just
            ::close(fd);
    
    And to ensure it commits to disk,
    
            ::ioctl(fd, BTRFS_IOC_SYNC);
    
    When a transaction is held open, the trans_handle is attached to the
    struct file (via private_data) so that it will get cleaned up if the
    process dies unexpectedly.  A held transaction is also ended on fsync() to
    avoid a deadlock.
    
    A misbehaving application could also deliberately hold a transaction open,
    effectively locking up the FS, so it may make sense to restrict something
    like this to root or something.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index c4fa4664a456..73c6d085bd90 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -978,9 +978,11 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 	return num_written ? num_written : err;
 }
 
-static int btrfs_release_file (struct inode * inode, struct file * filp)
+int btrfs_release_file(struct inode * inode, struct file * filp)
 {
 	btrfs_del_ordered_inode(inode);
+	if (filp->private_data)
+		btrfs_ioctl_trans_end(filp);
 	return 0;
 }
 
@@ -1011,6 +1013,9 @@ static int btrfs_sync_file(struct file *file,
 	/*
 	 * ok we haven't committed the transaction yet, lets do a commit
 	 */
+	if (file->private_data)
+		btrfs_ioctl_trans_end(file);
+
 	trans = btrfs_start_transaction(root, 1);
 	if (!trans) {
 		ret = -ENOMEM;

commit e1b81e6761bd8419146d4bbe0aadd8b2d348a01c
Author: Mingming <cmm@us.ibm.com>
Date:   Tue May 27 10:55:43 2008 -0400

    btrfs delete ordered inode handling fix
    
    Use btrfs_release_file instead of a put_inode call
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index d968900c6973..c4fa4664a456 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -978,6 +978,12 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 	return num_written ? num_written : err;
 }
 
+static int btrfs_release_file (struct inode * inode, struct file * filp)
+{
+	btrfs_del_ordered_inode(inode);
+	return 0;
+}
+
 static int btrfs_sync_file(struct file *file,
 			   struct dentry *dentry, int datasync)
 {
@@ -1044,6 +1050,7 @@ struct file_operations btrfs_file_operations = {
 	.write		= btrfs_file_write,
 	.mmap		= btrfs_file_mmap,
 	.open		= generic_file_open,
+	.release	= btrfs_release_file,
 	.fsync		= btrfs_sync_file,
 	.unlocked_ioctl	= btrfs_ioctl,
 #ifdef CONFIG_COMPAT

commit 211c17f51f46dc6c308c742098273dd46b5ca59c
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu May 15 09:13:45 2008 -0400

    Fix corners in writepage and btrfs_truncate_page
    
    The extent_io writepage calls needed an extra check for discarding
    pages that started on th last byte in the file.
    
    btrfs_truncate_page needed checks to make sure the page was still part
    of the file after reading it, and most importantly, needed to wait for
    all IO to the page to finish before freeing the corresponding extents on
    disk.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index df97d470812a..d968900c6973 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -305,7 +305,6 @@ static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
 	    (inline_size & (root->sectorsize -1)) == 0 ||
 	    inline_size >= BTRFS_MAX_INLINE_DATA_SIZE(root)) {
 		u64 last_end;
-		u64 existing_delalloc = 0;
 
 		for (i = 0; i < num_pages; i++) {
 			struct page *p = pages[i];
@@ -315,13 +314,6 @@ static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
 		last_end = (u64)(pages[num_pages -1]->index) <<
 				PAGE_CACHE_SHIFT;
 		last_end += PAGE_CACHE_SIZE - 1;
-		if (start_pos < isize) {
-			u64 delalloc_start = start_pos;
-			existing_delalloc = count_range_bits(io_tree,
-					     &delalloc_start,
-					     end_of_last_block, (u64)-1,
-					     EXTENT_DELALLOC);
-		}
 		set_extent_delalloc(io_tree, start_pos, end_of_last_block,
 				 GFP_NOFS);
 		btrfs_add_ordered_inode(inode);

commit 12fa8ec64f445aa932ba154053fe95432f30f2c6
Author: Jeff Mahoney <jeffm@suse.com>
Date:   Fri May 2 15:03:58 2008 -0400

    Btrfs: Add workaround for AppArmor changing remove_suid()
    
    In openSUSE 10.3, AppArmor modifies remove_suid to take a struct path
    rather than just a dentry. This patch tests that the kernel is openSUSE
    10.3 or newer and adjusts the call accordingly.
    
    Debian/Ubuntu with AppArmor applied will also need a similar patch.
    Maintainers of btrfs under those distributions should build on this
    patch or, alternatively, alter their package descriptions to add
    -DREMOVE_SUID_PATH to the compiler command line.
    
    Signed-off-by: Jeff Mahoney <jeffm@suse.com>
    - --- /dev/null 1970-01-01 00:00:00.000000000 +0000
    +++ b/compat.h  2008-02-06 16:46:13.000000000 -0500
    @@ -0,0 +1,15 @@
    +#ifndef _COMPAT_H_
    +#define _COMPAT_H_
    +
    +
    +/*
    + * Even if AppArmor isn't enabled, it still has different prototypes.
    + * Add more distro/version pairs here to declare which has AppArmor applied.
    + */
    +#if defined(CONFIG_SUSE_KERNEL)
    +# if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,22)
    +# define REMOVE_SUID_PATH 1
    +# endif
    +#endif
    +
    +#endif /* _COMPAT_H_ */
    - --- a/file.c  2008-02-06 11:37:39.000000000 -0500
    +++ b/file.c    2008-02-06 16:46:23.000000000 -0500
    @@ -37,6 +37,7 @@
     #include "ordered-data.h"
     #include "ioctl.h"
     #include "print-tree.h"
    +#include "compat.h"
    
     static int btrfs_copy_from_user(loff_t pos, int num_pages, int write_bytes,
    @@ -790,7 +791,11 @@ static ssize_t btrfs_file_write(struct f
                    goto out_nolock;
            if (count == 0)
                    goto out_nolock;
    +#ifdef REMOVE_SUID_PATH
    +       err = remove_suid(&file->f_path);
    +#else
            err = remove_suid(fdentry(file));
    +#endif
            if (err)
                    goto out_nolock;
            file_update_time(file);
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index c89c3ac4b23b..df97d470812a 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -37,6 +37,7 @@
 #include "ordered-data.h"
 #include "ioctl.h"
 #include "print-tree.h"
+#include "compat.h"
 
 
 static int btrfs_copy_from_user(loff_t pos, int num_pages, int write_bytes,
@@ -852,7 +853,11 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 		goto out_nolock;
 	if (count == 0)
 		goto out_nolock;
+#ifdef REMOVE_SUID_PATH
+	err = remove_suid(&file->f_path);
+#else
 	err = remove_suid(fdentry(file));
+#endif
 	if (err)
 		goto out_nolock;
 	file_update_time(file);

commit bb8885cc0aa6b1606143d92c70762b16026016a5
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri May 2 14:49:33 2008 -0400

    Btrfs: Fix do_sync_file_range ifdefs (2.6.22)
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index a50507f3056d..c89c3ac4b23b 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -961,7 +961,7 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 		if (err < 0)
 			num_written = err;
 	} else if (num_written > 0 && (file->f_flags & O_DIRECT)) {
-#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,23)
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,22)
 		do_sync_file_range(file, start_pos,
 				      start_pos + num_written - 1,
 				      SYNC_FILE_RANGE_WRITE |

commit f2eb0a241f0e5c135d93243b0236cb1f14c305e0
Author: Sage Weil <sage@newdream.net>
Date:   Fri May 2 14:43:14 2008 -0400

    Btrfs: Clone file data ioctl
    
    Add a new ioctl to clone file data
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 8effdf4f5d6f..a50507f3056d 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -285,7 +285,7 @@ static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
 			err = btrfs_insert_file_extent(trans, root,
 						       inode->i_ino,
 						       last_pos_in_file,
-						       0, 0, hole_size);
+						       0, 0, hole_size, 0);
 			btrfs_drop_extent_cache(inode, last_pos_in_file,
 					last_pos_in_file + hole_size -1);
 			btrfs_check_file(root, inode);

commit 81d7ed29ff6bdec903c36c26b386e16c014993b2
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Apr 25 08:51:48 2008 -0400

    Btrfs: Throttle file_write when data=ordered is flushing the inode
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 5d537f26dc83..8effdf4f5d6f 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -977,6 +977,7 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 		     (start_pos + num_written - 1) >> PAGE_CACHE_SHIFT);
 	}
 	current->backing_dev_info = NULL;
+	btrfs_ordered_throttle(root, inode);
 	return num_written ? num_written : err;
 }
 

commit 409c6118d39cb2d8666bee3d61a1a9ae5bbd4f5d
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Apr 22 09:24:20 2008 -0400

    Btrfs: Set nodatasum on the inode when written by a nodatasum mount
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 3f5525f0834c..5d537f26dc83 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -863,6 +863,14 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 	first_index = pos >> PAGE_CACHE_SHIFT;
 	last_index = (pos + count) >> PAGE_CACHE_SHIFT;
 
+	/*
+	 * if this is a nodatasum mount, force summing off for the inode
+	 * all the time.  That way a later mount with summing on won't
+	 * get confused
+	 */
+	if (btrfs_test_opt(root, NODATASUM))
+		btrfs_set_flag(inode, NODATASUM);
+
 	/*
 	 * there are lots of better ways to do this, but this code
 	 * makes sure the first and last page in the file range are

commit 3b951516ed703af0f6d82053937655ad69b60864
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Apr 17 11:29:12 2008 -0400

    Btrfs: Use the extent map cache to find the logical disk block during data retries
    
    The data read retry code needs to find the logical disk block before it
    can resubmit new bios.  But, finding this block isn't allowed to take
    the fs_mutex because that will deadlock with a number of different callers.
    
    This changes the retry code to use the extent map cache instead, but
    that requires the extent map cache to have the extent we're looking for.
    This is a problem because btrfs_drop_extent_cache just drops the entire
    extent instead of the little tiny part it is invalidating.
    
    The bulk of the code in this patch changes btrfs_drop_extent_cache to
    invalidate only a portion of the extent cache, and changes btrfs_get_extent
    to deal with the results.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 9fbda6552069..3f5525f0834c 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -356,12 +356,23 @@ static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
 int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end)
 {
 	struct extent_map *em;
+	struct extent_map *split = NULL;
+	struct extent_map *split2 = NULL;
 	struct extent_map_tree *em_tree = &BTRFS_I(inode)->extent_tree;
 	u64 len = end - start + 1;
+	int ret;
+	int testend = 1;
 
-	if (end == (u64)-1)
+	if (end == (u64)-1) {
 		len = (u64)-1;
+		testend = 0;
+	}
 	while(1) {
+		if (!split)
+			split = alloc_extent_map(GFP_NOFS);
+		if (!split2)
+			split2 = alloc_extent_map(GFP_NOFS);
+
 		spin_lock(&em_tree->lock);
 		em = lookup_extent_mapping(em_tree, start, len);
 		if (!em) {
@@ -369,6 +380,36 @@ int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end)
 			break;
 		}
 		remove_extent_mapping(em_tree, em);
+
+		if (em->block_start < EXTENT_MAP_LAST_BYTE &&
+		    em->start < start) {
+			split->start = em->start;
+			split->len = start - em->start;
+			split->block_start = em->block_start;
+			split->bdev = em->bdev;
+			split->flags = em->flags;
+			ret = add_extent_mapping(em_tree, split);
+			BUG_ON(ret);
+			free_extent_map(split);
+			split = split2;
+			split2 = NULL;
+		}
+		if (em->block_start < EXTENT_MAP_LAST_BYTE &&
+		    testend && em->start + em->len > start + len) {
+			u64 diff = start + len - em->start;
+
+			split->start = start + len;
+			split->len = em->start + em->len - (start + len);
+			split->bdev = em->bdev;
+			split->flags = em->flags;
+
+			split->block_start = em->block_start + diff;
+
+			ret = add_extent_mapping(em_tree, split);
+			BUG_ON(ret);
+			free_extent_map(split);
+			split = NULL;
+		}
 		spin_unlock(&em_tree->lock);
 
 		/* once for us */
@@ -376,6 +417,10 @@ int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end)
 		/* once for the tree*/
 		free_extent_map(em);
 	}
+	if (split)
+		free_extent_map(split);
+	if (split2)
+		free_extent_map(split2);
 	return 0;
 }
 

commit b248a4152956cbae1b23f4c70ef5b51d6ea2ecfb
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Apr 14 09:48:18 2008 -0400

    Btrfs: A few updates for 2.6.18 and versions older than 2.6.25
    
    This includes fixing a missing spinlock init call that caused oops on mount
    for most kernels other than 2.6.25.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 5b9386452218..9fbda6552069 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -908,11 +908,17 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 		if (err < 0)
 			num_written = err;
 	} else if (num_written > 0 && (file->f_flags & O_DIRECT)) {
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,23)
+		do_sync_file_range(file, start_pos,
+				      start_pos + num_written - 1,
+				      SYNC_FILE_RANGE_WRITE |
+				      SYNC_FILE_RANGE_WAIT_AFTER);
+#else
 		do_sync_mapping_range(inode->i_mapping, start_pos,
 				      start_pos + num_written - 1,
 				      SYNC_FILE_RANGE_WRITE |
 				      SYNC_FILE_RANGE_WAIT_AFTER);
-
+#endif
 		invalidate_mapping_pages(inode->i_mapping,
 		      start_pos >> PAGE_CACHE_SHIFT,
 		     (start_pos + num_written - 1) >> PAGE_CACHE_SHIFT);

commit 16432985920f3c45af82da214e2498f3e2f9066b
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Apr 10 10:23:21 2008 -0400

    Btrfs: Add O_DIRECT read and write (writes == buffered + cache flush)
    
    This adds basic O_DIRECT read and write support.  In the write case, we
    just do a normal buffered write followed by a cache flush.  O_DIRECT +
    O_SYNC are required to trigger metadata syncs.
    
    In the read case, there is a basic btrfs_get_block call for use by
    the generic O_DIRECT code.  This does honor multi-volume mapping rules
    but it skips all checksumming.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 5fa450452f9b..5b9386452218 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -796,8 +796,6 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 		     PAGE_CACHE_SIZE / (sizeof(struct page *)));
 	pinned[0] = NULL;
 	pinned[1] = NULL;
-	if (file->f_flags & O_DIRECT)
-		return -EINVAL;
 
 	pos = *ppos;
 	start_pos = pos;
@@ -909,6 +907,15 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 				      start_pos, num_written);
 		if (err < 0)
 			num_written = err;
+	} else if (num_written > 0 && (file->f_flags & O_DIRECT)) {
+		do_sync_mapping_range(inode->i_mapping, start_pos,
+				      start_pos + num_written - 1,
+				      SYNC_FILE_RANGE_WRITE |
+				      SYNC_FILE_RANGE_WAIT_AFTER);
+
+		invalidate_mapping_pages(inode->i_mapping,
+		      start_pos >> PAGE_CACHE_SHIFT,
+		     (start_pos + num_written - 1) >> PAGE_CACHE_SHIFT);
 	}
 	current->backing_dev_info = NULL;
 	return num_written ? num_written : err;

commit 0740c82bc7585a23e100c9a59d5e2abbf99459fb
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Feb 19 16:24:18 2008 -0500

    Btrfs: Properly cast before shifting
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 51466dcd44e0..5fa450452f9b 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -764,7 +764,7 @@ static int prepare_pages(struct btrfs_root *root, struct file *file,
 	}
 	if (start_pos < inode->i_size) {
 		u64 last_pos;
-		last_pos = (index + num_pages) << PAGE_CACHE_SHIFT;
+		last_pos = ((u64)index + num_pages) << PAGE_CACHE_SHIFT;
 		lock_extent(&BTRFS_I(inode)->io_tree,
 			    start_pos, last_pos - 1, GFP_NOFS);
 		clear_extent_bits(&BTRFS_I(inode)->io_tree, start_pos,

commit d99cb30a11ea117ad0f58cda09aa43438ebbf81c
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Feb 19 12:55:05 2008 -0500

    Btrfs: Take the extent lock before dropping the delalloc bits
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 1a47251a9d4d..51466dcd44e0 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -765,9 +765,13 @@ static int prepare_pages(struct btrfs_root *root, struct file *file,
 	if (start_pos < inode->i_size) {
 		u64 last_pos;
 		last_pos = (index + num_pages) << PAGE_CACHE_SHIFT;
+		lock_extent(&BTRFS_I(inode)->io_tree,
+			    start_pos, last_pos - 1, GFP_NOFS);
 		clear_extent_bits(&BTRFS_I(inode)->io_tree, start_pos,
 				  last_pos - 1, EXTENT_DIRTY | EXTENT_DELALLOC,
 				  GFP_NOFS);
+		unlock_extent(&BTRFS_I(inode)->io_tree,
+			      start_pos, last_pos - 1, GFP_NOFS);
 	}
 	return 0;
 }

commit 0762704b196d41941a9b439e9165efaf85c6609e
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Feb 19 11:29:24 2008 -0500

    Btrfs: Properly clear dirty and delalloc extent bits while preparing the file for write
    
    Yan Zheng noticed that we don't clear the extent state tree dirty and delalloc
    bits when we clear the dirty bits on the page during file write.
    
    This leads to csum errors later on.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index b0352b5958a5..1a47251a9d4d 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -762,6 +762,13 @@ static int prepare_pages(struct btrfs_root *root, struct file *file,
 		set_page_extent_mapped(pages[i]);
 		WARN_ON(!PageLocked(pages[i]));
 	}
+	if (start_pos < inode->i_size) {
+		u64 last_pos;
+		last_pos = (index + num_pages) << PAGE_CACHE_SHIFT;
+		clear_extent_bits(&BTRFS_I(inode)->io_tree, start_pos,
+				  last_pos - 1, EXTENT_DIRTY | EXTENT_DELALLOC,
+				  GFP_NOFS);
+	}
 	return 0;
 }
 

commit 39b5637f6f195852259004bb27b58e2dcf9fb378
Author: Yan <yanzheng@21cn.com>
Date:   Fri Feb 15 10:40:50 2008 -0500

    Btrfs: Fix "no csum found for inode" issue.
    
    A few codes were not properly updated for changes of extent map.  This
    may be the causes of "no csum found for inode" issue.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index f89396082544..b0352b5958a5 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -357,10 +357,13 @@ int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end)
 {
 	struct extent_map *em;
 	struct extent_map_tree *em_tree = &BTRFS_I(inode)->extent_tree;
+	u64 len = end - start + 1;
 
+	if (end == (u64)-1)
+		len = (u64)-1;
 	while(1) {
 		spin_lock(&em_tree->lock);
-		em = lookup_extent_mapping(em_tree, start, end);
+		em = lookup_extent_mapping(em_tree, start, len);
 		if (!em) {
 			spin_unlock(&em_tree->lock);
 			break;

commit 9069218d448ea547dbad5f1cbd537e88d6519d66
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Feb 8 13:49:28 2008 -0500

    Btrfs: Fix i_blocks accounting
    
    Now that delayed allocation accounting works, i_blocks accounting is changed
    to only modify i_blocks when extents inserted or removed.
    
    The fillattr call is changed to include the delayed allocation byte count
    in the i_blocks result.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index bfa4149c053d..f89396082544 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -175,6 +175,7 @@ static int noinline insert_inline_extent(struct btrfs_trans_handle *trans,
 			leaf = path->nodes[0];
 			ei = btrfs_item_ptr(leaf, path->slots[0],
 					    struct btrfs_file_extent_item);
+			inode->i_blocks += (offset + size - found_end) >> 9;
 		}
 		if (found_end < offset) {
 			ptr = btrfs_file_extent_inline_start(ei) + found_size;
@@ -184,6 +185,7 @@ static int noinline insert_inline_extent(struct btrfs_trans_handle *trans,
 insert:
 		btrfs_release_path(root, path);
 		datasize = offset + size - key.offset;
+		inode->i_blocks += datasize >> 9;
 		datasize = btrfs_file_extent_calc_inline_size(datasize);
 		ret = btrfs_insert_empty_item(trans, root, path, &key,
 					      datasize);
@@ -256,7 +258,6 @@ static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
 		goto out_unlock;
 	}
 	btrfs_set_trans_block_group(trans, inode);
-	inode->i_blocks += num_bytes >> 9;
 	hint_byte = 0;
 
 	if ((end_of_last_block & 4095) == 0) {
@@ -410,7 +411,7 @@ int btrfs_check_file(struct btrfs_root *root, struct inode *inode)
 		if (found_key.type != BTRFS_EXTENT_DATA_KEY)
 			goto out;
 
-		if (found_key.offset != last_offset) {
+		if (found_key.offset < last_offset) {
 			WARN_ON(1);
 			btrfs_print_leaf(root, leaf);
 			printk("inode %lu found offset %Lu expected %Lu\n",
@@ -435,7 +436,7 @@ int btrfs_check_file(struct btrfs_root *root, struct inode *inode)
 		last_offset = extent_end;
 		path->slots[0]++;
 	}
-	if (last_offset < inode->i_size) {
+	if (0 && last_offset < inode->i_size) {
 		WARN_ON(1);
 		btrfs_print_leaf(root, leaf);
 		printk("inode %lu found offset %Lu size %Lu\n", inode->i_ino,
@@ -608,8 +609,7 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 								      extent);
 				if (btrfs_file_extent_disk_bytenr(leaf,
 								  extent)) {
-					inode->i_blocks -=
-						(old_num - new_num) >> 9;
+					dec_i_blocks(inode, old_num - new_num);
 				}
 				btrfs_set_file_extent_num_bytes(leaf, extent,
 								new_num);
@@ -620,6 +620,8 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 				u32 new_size;
 				new_size = btrfs_file_extent_calc_inline_size(
 						   inline_limit - key.offset);
+				dec_i_blocks(inode, (extent_end - key.offset) -
+					(inline_limit - key.offset));
 				btrfs_truncate_item(trans, root, path,
 						    new_size, 1);
 			}
@@ -653,7 +655,7 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 			btrfs_release_path(root, path);
 			extent = NULL;
 			if (found_extent && disk_bytenr != 0) {
-				inode->i_blocks -= extent_num_bytes >> 9;
+				dec_i_blocks(inode, extent_num_bytes);
 				ret = btrfs_free_extent(trans, root,
 						disk_bytenr,
 						disk_num_bytes,
@@ -674,6 +676,8 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 			u32 new_size;
 			new_size = btrfs_file_extent_calc_inline_size(
 						   extent_end - end);
+			dec_i_blocks(inode, (extent_end - key.offset) -
+					(extent_end - end));
 			btrfs_truncate_item(trans, root, path, new_size, 0);
 		}
 		/* create bookend, splitting the extent in two */
@@ -718,6 +722,7 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 	}
 out:
 	btrfs_free_path(path);
+	btrfs_check_file(root, inode);
 	return ret;
 }
 

commit 7261009ceaee65e4683be6e474d1ea85a1b1b26c
Author: Yan <yanzheng@21cn.com>
Date:   Tue Feb 5 15:40:36 2008 -0500

    btrfs_drop_extents: handle BTRFS_INODE_REF_KEY types
    It's possible "key.type == BTRFS_INODE_REF_KEY" and "key.offset >= end".
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 184bde1cfc3d..bfa4149c053d 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -507,11 +507,12 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 		slot = path->slots[0];
 		ret = 0;
 		btrfs_item_key_to_cpu(leaf, &key, slot);
-
-		if (key.offset >= end || key.objectid != inode->i_ino) {
+		if (btrfs_key_type(&key) == BTRFS_EXTENT_DATA_KEY &&
+		    key.offset >= end) {
 			goto out;
 		}
-		if (btrfs_key_type(&key) > BTRFS_EXTENT_DATA_KEY) {
+		if (btrfs_key_type(&key) > BTRFS_EXTENT_DATA_KEY ||
+		    key.objectid != inode->i_ino) {
 			goto out;
 		}
 		if (recow) {

commit dacb473ad81d5c1650cb73d1cb9b866202f6bab0
Author: Yan <yanzheng@21cn.com>
Date:   Tue Feb 5 09:07:49 2008 -0500

    Btrfs: Fix hole creation in file_write
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 7cbf8d8d7e27..184bde1cfc3d 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -265,16 +265,14 @@ static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
 	set_extent_uptodate(io_tree, start_pos, end_of_last_block, GFP_NOFS);
 
 	/* FIXME...EIEIO, ENOSPC and more */
-
 	/* insert any holes we need to create */
-	if (inode->i_size < start_pos) {
+	if (isize < end_pos) {
 		u64 last_pos_in_file;
 		u64 hole_size;
 		u64 mask = root->sectorsize - 1;
 		last_pos_in_file = (isize + mask) & ~mask;
 		hole_size = (end_pos - last_pos_in_file + mask) & ~mask;
-
-		if (last_pos_in_file < start_pos) {
+		if (last_pos_in_file < end_pos) {
 			err = btrfs_drop_extents(trans, root, inode,
 						 last_pos_in_file,
 						 last_pos_in_file + hole_size,

commit 0181e58f91c1edbf835edb7a87d6dfe81374709d
Author: Yan <yanzheng@21cn.com>
Date:   Wed Jan 30 14:39:54 2008 -0500

    btrfs_drop_extent fixe for inline items > 8K
    
    When truncating a inline extent, btrfs_drop_extents doesn't properly
    handle the case "key.offset > inline_limit". This bug can only happen
    when max line size is larger than 8K.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 7c10a90362ff..7cbf8d8d7e27 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -591,8 +591,7 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 				}
 			}
 			bookend = 1;
-			if (found_inline && start <= key.offset &&
-			    inline_limit < extent_end)
+			if (found_inline && start <= key.offset)
 				keep = 1;
 		}
 		/* truncate existing extent */
@@ -672,11 +671,10 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 			if (!bookend)
 				continue;
 		}
-		if (bookend && found_inline && start <= key.offset &&
-		    inline_limit < extent_end && key.offset <= inline_limit) {
+		if (bookend && found_inline && start <= key.offset) {
 			u32 new_size;
 			new_size = btrfs_file_extent_calc_inline_size(
-						   extent_end - inline_limit);
+						   extent_end - end);
 			btrfs_truncate_item(trans, root, path, new_size, 0);
 		}
 		/* create bookend, splitting the extent in two */

commit 6f568d35a045dbb8a13fe71bfc32e85e39a986cb
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Jan 29 16:03:38 2008 -0500

    Btrfs: mount -o max_inline=size to control the maximum inline extent size
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 8e210616d702..7c10a90362ff 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -301,7 +301,8 @@ static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
 	 */
 	inline_size = end_pos;
 	if (isize >= BTRFS_MAX_INLINE_DATA_SIZE(root) ||
-	    inline_size > 8192 ||
+	    inline_size > root->fs_info->max_inline ||
+	    (inline_size & (root->sectorsize -1)) == 0 ||
 	    inline_size >= BTRFS_MAX_INLINE_DATA_SIZE(root)) {
 		u64 last_end;
 		u64 existing_delalloc = 0;

commit 291d673e6a22d9c6834e939f66c7cfef90669021
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Jan 29 15:55:23 2008 -0500

    Btrfs: Do delalloc accounting via hooks in the extent_state code
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index c5bb00f92396..8e210616d702 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -323,10 +323,6 @@ static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
 		}
 		set_extent_delalloc(io_tree, start_pos, end_of_last_block,
 				 GFP_NOFS);
-		spin_lock(&root->fs_info->delalloc_lock);
-		root->fs_info->delalloc_bytes += (end_of_last_block + 1 -
-					  start_pos) - existing_delalloc;
-		spin_unlock(&root->fs_info->delalloc_lock);
 		btrfs_add_ordered_inode(inode);
 	} else {
 		u64 aligned_end;

commit d1310b2e0cd98eb1348553e69b73827b436dca7b
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Jan 24 16:13:08 2008 -0500

    Btrfs: Split the extent_map code into two parts
    
    There is now extent_map for mapping offsets in the file to disk and
    extent_io for state tracking, IO submission and extent_bufers.
    
    The new extent_map code shifts from [start,end] pairs to [start,len], and
    pushes the locking out into the caller.  This allows a few performance
    optimizations and is easier to use.
    
    A number of extent_map usage bugs were fixed, mostly with failing
    to remove extent_map entries when changing the file.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 1cd8c908811e..c5bb00f92396 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -233,8 +233,7 @@ static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
 	int err = 0;
 	int i;
 	struct inode *inode = fdentry(file)->d_inode;
-	struct extent_map *em;
-	struct extent_map_tree *em_tree = &BTRFS_I(inode)->extent_tree;
+	struct extent_io_tree *io_tree = &BTRFS_I(inode)->io_tree;
 	u64 hint_byte;
 	u64 num_bytes;
 	u64 start_pos;
@@ -242,11 +241,6 @@ static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
 	u64 end_pos = pos + write_bytes;
 	u64 inline_size;
 	loff_t isize = i_size_read(inode);
-	em = alloc_extent_map(GFP_NOFS);
-	if (!em)
-		return -ENOMEM;
-
-	em->bdev = inode->i_sb->s_bdev;
 
 	start_pos = pos & ~((u64)root->sectorsize - 1);
 	num_bytes = (write_bytes + pos - start_pos +
@@ -254,7 +248,7 @@ static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
 
 	end_of_last_block = start_pos + num_bytes - 1;
 
-	lock_extent(em_tree, start_pos, end_of_last_block, GFP_NOFS);
+	lock_extent(io_tree, start_pos, end_of_last_block, GFP_NOFS);
 	mutex_lock(&root->fs_info->fs_mutex);
 	trans = btrfs_start_transaction(root, 1);
 	if (!trans) {
@@ -268,7 +262,7 @@ static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
 	if ((end_of_last_block & 4095) == 0) {
 		printk("strange end of last %Lu %zu %Lu\n", start_pos, write_bytes, end_of_last_block);
 	}
-	set_extent_uptodate(em_tree, start_pos, end_of_last_block, GFP_NOFS);
+	set_extent_uptodate(io_tree, start_pos, end_of_last_block, GFP_NOFS);
 
 	/* FIXME...EIEIO, ENOSPC and more */
 
@@ -293,6 +287,8 @@ static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
 						       inode->i_ino,
 						       last_pos_in_file,
 						       0, 0, hole_size);
+			btrfs_drop_extent_cache(inode, last_pos_in_file,
+					last_pos_in_file + hole_size -1);
 			btrfs_check_file(root, inode);
 		}
 		if (err)
@@ -320,12 +316,12 @@ static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
 		last_end += PAGE_CACHE_SIZE - 1;
 		if (start_pos < isize) {
 			u64 delalloc_start = start_pos;
-			existing_delalloc = count_range_bits(em_tree,
+			existing_delalloc = count_range_bits(io_tree,
 					     &delalloc_start,
 					     end_of_last_block, (u64)-1,
 					     EXTENT_DELALLOC);
 		}
-		set_extent_delalloc(em_tree, start_pos, end_of_last_block,
+		set_extent_delalloc(io_tree, start_pos, end_of_last_block,
 				 GFP_NOFS);
 		spin_lock(&root->fs_info->delalloc_lock);
 		root->fs_info->delalloc_bytes += (end_of_last_block + 1 -
@@ -346,6 +342,7 @@ static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
 		inline_size -= start_pos;
 		err = insert_inline_extent(trans, root, inode, start_pos,
 					   inline_size, pages, 0, num_pages);
+		btrfs_drop_extent_cache(inode, start_pos, aligned_end - 1);
 		BUG_ON(err);
 	}
 	if (end_pos > isize) {
@@ -356,8 +353,7 @@ static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
 	err = btrfs_end_transaction(trans, root);
 out_unlock:
 	mutex_unlock(&root->fs_info->fs_mutex);
-	unlock_extent(em_tree, start_pos, end_of_last_block, GFP_NOFS);
-	free_extent_map(em);
+	unlock_extent(io_tree, start_pos, end_of_last_block, GFP_NOFS);
 	return err;
 }
 
@@ -367,10 +363,15 @@ int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end)
 	struct extent_map_tree *em_tree = &BTRFS_I(inode)->extent_tree;
 
 	while(1) {
+		spin_lock(&em_tree->lock);
 		em = lookup_extent_mapping(em_tree, start, end);
-		if (!em)
+		if (!em) {
+			spin_unlock(&em_tree->lock);
 			break;
+		}
 		remove_extent_mapping(em_tree, em);
+		spin_unlock(&em_tree->lock);
+
 		/* once for us */
 		free_extent_map(em);
 		/* once for the tree*/

commit 5f56406aabdf5444d040c5955effc665b1d0dbaf
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Jan 22 16:47:59 2008 -0500

    Btrfs: Fix hole insertion corner cases
    
    There were a few places that could cause duplicate extent insertion,
    this adjusts the code that creates holes to avoid it.
    
    lookup_extent_map is changed to correctly return all of the extents in a
    range, even when there are none matching at the start of the range.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 897242e87fa7..1cd8c908811e 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -278,7 +278,7 @@ static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
 		u64 hole_size;
 		u64 mask = root->sectorsize - 1;
 		last_pos_in_file = (isize + mask) & ~mask;
-		hole_size = (start_pos - last_pos_in_file + mask) & ~mask;
+		hole_size = (end_pos - last_pos_in_file + mask) & ~mask;
 
 		if (last_pos_in_file < start_pos) {
 			err = btrfs_drop_extents(trans, root, inode,
@@ -293,6 +293,7 @@ static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
 						       inode->i_ino,
 						       last_pos_in_file,
 						       0, 0, hole_size);
+			btrfs_check_file(root, inode);
 		}
 		if (err)
 			goto failed;
@@ -378,6 +379,80 @@ int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end)
 	return 0;
 }
 
+int btrfs_check_file(struct btrfs_root *root, struct inode *inode)
+{
+	return 0;
+#if 0
+	struct btrfs_path *path;
+	struct btrfs_key found_key;
+	struct extent_buffer *leaf;
+	struct btrfs_file_extent_item *extent;
+	u64 last_offset = 0;
+	int nritems;
+	int slot;
+	int found_type;
+	int ret;
+	int err = 0;
+	u64 extent_end = 0;
+
+	path = btrfs_alloc_path();
+	ret = btrfs_lookup_file_extent(NULL, root, path, inode->i_ino,
+				       last_offset, 0);
+	while(1) {
+		nritems = btrfs_header_nritems(path->nodes[0]);
+		if (path->slots[0] >= nritems) {
+			ret = btrfs_next_leaf(root, path);
+			if (ret)
+				goto out;
+			nritems = btrfs_header_nritems(path->nodes[0]);
+		}
+		slot = path->slots[0];
+		leaf = path->nodes[0];
+		btrfs_item_key_to_cpu(leaf, &found_key, slot);
+		if (found_key.objectid != inode->i_ino)
+			break;
+		if (found_key.type != BTRFS_EXTENT_DATA_KEY)
+			goto out;
+
+		if (found_key.offset != last_offset) {
+			WARN_ON(1);
+			btrfs_print_leaf(root, leaf);
+			printk("inode %lu found offset %Lu expected %Lu\n",
+			       inode->i_ino, found_key.offset, last_offset);
+			err = 1;
+			goto out;
+		}
+		extent = btrfs_item_ptr(leaf, slot,
+					struct btrfs_file_extent_item);
+		found_type = btrfs_file_extent_type(leaf, extent);
+		if (found_type == BTRFS_FILE_EXTENT_REG) {
+			extent_end = found_key.offset +
+			     btrfs_file_extent_num_bytes(leaf, extent);
+		} else if (found_type == BTRFS_FILE_EXTENT_INLINE) {
+			struct btrfs_item *item;
+			item = btrfs_item_nr(leaf, slot);
+			extent_end = found_key.offset +
+			     btrfs_file_extent_inline_len(leaf, item);
+			extent_end = (extent_end + root->sectorsize - 1) &
+				~((u64)root->sectorsize -1 );
+		}
+		last_offset = extent_end;
+		path->slots[0]++;
+	}
+	if (last_offset < inode->i_size) {
+		WARN_ON(1);
+		btrfs_print_leaf(root, leaf);
+		printk("inode %lu found offset %Lu size %Lu\n", inode->i_ino,
+		       last_offset, inode->i_size);
+		err = 1;
+
+	}
+out:
+	btrfs_free_path(path);
+	return err;
+#endif
+}
+
 /*
  * this is very complex, but the basic idea is to drop all extents
  * in the range start - end.  hint_block is filled in with a block number
@@ -436,6 +511,7 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 		slot = path->slots[0];
 		ret = 0;
 		btrfs_item_key_to_cpu(leaf, &key, slot);
+
 		if (key.offset >= end || key.objectid != inode->i_ino) {
 			goto out;
 		}

commit e2008b61401ecb467a8ce1788fcd2116ae1cfbc1
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Jan 8 15:46:30 2008 -0500

    Btrfs: Add some simple throttling to wait for data=ordered and snapshot deletion
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 0a5f4defe59b..897242e87fa7 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -800,6 +800,7 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 		balance_dirty_pages_ratelimited_nr(inode->i_mapping, num_pages);
 		if (num_pages < (root->leafsize >> PAGE_CACHE_SHIFT) + 1)
 			btrfs_btree_balance_dirty(root, 1);
+		btrfs_throttle(root);
 		cond_resched();
 	}
 out:

commit dc17ff8f11d129db9e83ab7244769e4eae05e14d
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Jan 8 15:46:30 2008 -0500

    Btrfs: Add data=ordered support
    
    This forces file data extents down the disk along with the metadata that
    references them.  The current implementation is fairly simple, and just
    writes out all of the dirty pages in an inode before the commit.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 94c93373cb7d..0a5f4defe59b 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -34,6 +34,7 @@
 #include "disk-io.h"
 #include "transaction.h"
 #include "btrfs_inode.h"
+#include "ordered-data.h"
 #include "ioctl.h"
 #include "print-tree.h"
 
@@ -329,6 +330,7 @@ static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
 		root->fs_info->delalloc_bytes += (end_of_last_block + 1 -
 					  start_pos) - existing_delalloc;
 		spin_unlock(&root->fs_info->delalloc_lock);
+		btrfs_add_ordered_inode(inode);
 	} else {
 		u64 aligned_end;
 		/* step one, delete the existing extents in this range */
@@ -724,8 +726,6 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 
 	pages = kmalloc(nrptrs * sizeof(struct page *), GFP_KERNEL);
 
-	down_read(&BTRFS_I(inode)->root->snap_sem);
-
 	mutex_lock(&inode->i_mutex);
 	first_index = pos >> PAGE_CACHE_SHIFT;
 	last_index = (pos + count) >> PAGE_CACHE_SHIFT;
@@ -804,7 +804,6 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 	}
 out:
 	mutex_unlock(&inode->i_mutex);
-	up_read(&BTRFS_I(inode)->root->snap_sem);
 
 out_nolock:
 	kfree(pages);

commit 2552488345ea0933711010ee2c844792130a4252
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Jan 3 15:44:57 2008 -0500

    Btrfs: Lower the max inline size to 8k
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index e862292bdfc6..94c93373cb7d 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -303,7 +303,7 @@ static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
 	 */
 	inline_size = end_pos;
 	if (isize >= BTRFS_MAX_INLINE_DATA_SIZE(root) ||
-	    inline_size > 32768 ||
+	    inline_size > 8192 ||
 	    inline_size >= BTRFS_MAX_INLINE_DATA_SIZE(root)) {
 		u64 last_end;
 		u64 existing_delalloc = 0;

commit 5b92ee7204a5fb6542b204831202adbc1a7a851a
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Jan 3 13:46:11 2008 -0500

    Btrfs: Fix lock ordering of the snapshot semaphore against the page lock
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index c9ebd4fe8f7e..e862292bdfc6 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -251,7 +251,6 @@ static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
 	num_bytes = (write_bytes + pos - start_pos +
 		    root->sectorsize - 1) & ~((u64)root->sectorsize - 1);
 
-	down_read(&BTRFS_I(inode)->root->snap_sem);
 	end_of_last_block = start_pos + num_bytes - 1;
 
 	lock_extent(em_tree, start_pos, end_of_last_block, GFP_NOFS);
@@ -356,7 +355,6 @@ static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
 	mutex_unlock(&root->fs_info->fs_mutex);
 	unlock_extent(em_tree, start_pos, end_of_last_block, GFP_NOFS);
 	free_extent_map(em);
-	up_read(&BTRFS_I(inode)->root->snap_sem);
 	return err;
 }
 
@@ -726,6 +724,8 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 
 	pages = kmalloc(nrptrs * sizeof(struct page *), GFP_KERNEL);
 
+	down_read(&BTRFS_I(inode)->root->snap_sem);
+
 	mutex_lock(&inode->i_mutex);
 	first_index = pos >> PAGE_CACHE_SHIFT;
 	last_index = (pos + count) >> PAGE_CACHE_SHIFT;
@@ -804,6 +804,8 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 	}
 out:
 	mutex_unlock(&inode->i_mutex);
+	up_read(&BTRFS_I(inode)->root->snap_sem);
+
 out_nolock:
 	kfree(pages);
 	if (pinned[0])

commit 98ed51747b63435b9987ef12692a75c223818bbe
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Jan 3 10:01:48 2008 -0500

    Btrfs: Force inlining off in a few places to save stack usage
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 71dc2d33b6c6..c9ebd4fe8f7e 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -80,7 +80,7 @@ static void btrfs_drop_pages(struct page **pages, size_t num_pages)
 	}
 }
 
-static int insert_inline_extent(struct btrfs_trans_handle *trans,
+static int noinline insert_inline_extent(struct btrfs_trans_handle *trans,
 				struct btrfs_root *root, struct inode *inode,
 				u64 offset, size_t size,
 				struct page **pages, size_t page_offset,
@@ -221,7 +221,7 @@ static int insert_inline_extent(struct btrfs_trans_handle *trans,
 	return err;
 }
 
-static int dirty_and_release_pages(struct btrfs_trans_handle *trans,
+static int noinline dirty_and_release_pages(struct btrfs_trans_handle *trans,
 				   struct btrfs_root *root,
 				   struct file *file,
 				   struct page **pages,
@@ -653,14 +653,10 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 /*
  * this gets pages into the page cache and locks them down
  */
-static int prepare_pages(struct btrfs_root *root,
-			 struct file *file,
-			 struct page **pages,
-			 size_t num_pages,
-			 loff_t pos,
-			 unsigned long first_index,
-			 unsigned long last_index,
-			 size_t write_bytes)
+static int prepare_pages(struct btrfs_root *root, struct file *file,
+			 struct page **pages, size_t num_pages,
+			 loff_t pos, unsigned long first_index,
+			 unsigned long last_index, size_t write_bytes)
 {
 	int i;
 	unsigned long index = pos >> PAGE_CACHE_SHIFT;

commit 1832a6d5ee3b1af61001cadba9e10da9e91af4a4
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Dec 21 16:27:21 2007 -0500

    Btrfs: Implement basic support for -ENOSPC
    
    This is intended to prevent accidentally filling the drive.  A determined
    user can still make things oops.
    
    It includes some accounting of the current bytes under delayed allocation,
    but this will change as things get optimized
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 461b09663fed..71dc2d33b6c6 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -307,6 +307,7 @@ static int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 	    inline_size > 32768 ||
 	    inline_size >= BTRFS_MAX_INLINE_DATA_SIZE(root)) {
 		u64 last_end;
+		u64 existing_delalloc = 0;
 
 		for (i = 0; i < num_pages; i++) {
 			struct page *p = pages[i];
@@ -316,8 +317,19 @@ static int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 		last_end = (u64)(pages[num_pages -1]->index) <<
 				PAGE_CACHE_SHIFT;
 		last_end += PAGE_CACHE_SIZE - 1;
+		if (start_pos < isize) {
+			u64 delalloc_start = start_pos;
+			existing_delalloc = count_range_bits(em_tree,
+					     &delalloc_start,
+					     end_of_last_block, (u64)-1,
+					     EXTENT_DELALLOC);
+		}
 		set_extent_delalloc(em_tree, start_pos, end_of_last_block,
 				 GFP_NOFS);
+		spin_lock(&root->fs_info->delalloc_lock);
+		root->fs_info->delalloc_bytes += (end_of_last_block + 1 -
+					  start_pos) - existing_delalloc;
+		spin_unlock(&root->fs_info->delalloc_lock);
 	} else {
 		u64 aligned_end;
 		/* step one, delete the existing extents in this range */
@@ -708,12 +720,12 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 	current->backing_dev_info = inode->i_mapping->backing_dev_info;
 	err = generic_write_checks(file, &pos, &count, S_ISBLK(inode->i_mode));
 	if (err)
-		goto out;
+		goto out_nolock;
 	if (count == 0)
-		goto out;
+		goto out_nolock;
 	err = remove_suid(fdentry(file));
 	if (err)
-		goto out;
+		goto out_nolock;
 	file_update_time(file);
 
 	pages = kmalloc(nrptrs * sizeof(struct page *), GFP_KERNEL);
@@ -758,6 +770,13 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 
 		WARN_ON(num_pages > nrptrs);
 		memset(pages, 0, sizeof(pages));
+
+		mutex_lock(&root->fs_info->fs_mutex);
+		ret = btrfs_check_free_space(root, write_bytes, 0);
+		mutex_unlock(&root->fs_info->fs_mutex);
+		if (ret)
+			goto out;
+
 		ret = prepare_pages(root, file, pages, num_pages,
 				    pos, first_index, last_index,
 				    write_bytes);
@@ -787,8 +806,9 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 			btrfs_btree_balance_dirty(root, 1);
 		cond_resched();
 	}
-	mutex_unlock(&inode->i_mutex);
 out:
+	mutex_unlock(&inode->i_mutex);
+out_nolock:
 	kfree(pages);
 	if (pinned[0])
 		page_cache_release(pinned[0]);

commit 6da6abae027e2dbc59bca5f4168b0760f25068c7
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Dec 18 16:15:09 2007 -0500

    Btrfs: Back port to 2.6.18-el kernels
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 62fcd79d8ab3..461b09663fed 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -231,7 +231,7 @@ static int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 {
 	int err = 0;
 	int i;
-	struct inode *inode = file->f_path.dentry->d_inode;
+	struct inode *inode = fdentry(file)->d_inode;
 	struct extent_map *em;
 	struct extent_map_tree *em_tree = &BTRFS_I(inode)->extent_tree;
 	u64 hint_byte;
@@ -652,7 +652,7 @@ static int prepare_pages(struct btrfs_root *root,
 {
 	int i;
 	unsigned long index = pos >> PAGE_CACHE_SHIFT;
-	struct inode *inode = file->f_path.dentry->d_inode;
+	struct inode *inode = fdentry(file)->d_inode;
 	int err = 0;
 	u64 start_pos;
 
@@ -666,7 +666,11 @@ static int prepare_pages(struct btrfs_root *root,
 			err = -ENOMEM;
 			BUG_ON(1);
 		}
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,18)
+		ClearPageDirty(pages[i]);
+#else
 		cancel_dirty_page(pages[i], PAGE_CACHE_SIZE);
+#endif
 		wait_on_page_writeback(pages[i]);
 		set_page_extent_mapped(pages[i]);
 		WARN_ON(!PageLocked(pages[i]));
@@ -682,7 +686,7 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 	ssize_t num_written = 0;
 	ssize_t err = 0;
 	int ret = 0;
-	struct inode *inode = file->f_path.dentry->d_inode;
+	struct inode *inode = fdentry(file)->d_inode;
 	struct btrfs_root *root = BTRFS_I(inode)->root;
 	struct page **pages = NULL;
 	int nrptrs;
@@ -707,7 +711,7 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 		goto out;
 	if (count == 0)
 		goto out;
-	err = remove_suid(file->f_path.dentry);
+	err = remove_suid(fdentry(file));
 	if (err)
 		goto out;
 	file_update_time(file);
@@ -862,6 +866,9 @@ struct file_operations btrfs_file_operations = {
 	.read		= do_sync_read,
 	.aio_read       = generic_file_aio_read,
 	.splice_read	= generic_file_splice_read,
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,18)
+	.sendfile	= generic_file_sendfile,
+#endif
 	.write		= btrfs_file_write,
 	.mmap		= btrfs_file_mmap,
 	.open		= generic_file_open,

commit e9906a98497c26a3c1a7532b51b81cc1fbdf9e1d
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Dec 14 12:56:58 2007 -0500

    Fixes for loopback files in btrfs
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index a280b9562221..62fcd79d8ab3 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -861,6 +861,7 @@ struct file_operations btrfs_file_operations = {
 	.llseek		= generic_file_llseek,
 	.read		= do_sync_read,
 	.aio_read       = generic_file_aio_read,
+	.splice_read	= generic_file_splice_read,
 	.write		= btrfs_file_write,
 	.mmap		= btrfs_file_mmap,
 	.open		= generic_file_open,

commit 6e3b96668eb34da6060a5189e6a73e9d62a3b178
Author: Yan <yanzheng@21cn.com>
Date:   Fri Dec 14 11:14:42 2007 -0500

    Off by one fix for btrfs_drop_extents
    
    One of my old patches introduces a new bug to
    btrfs_drop_extents(changeset 275). Inline extents are not truncated
    properly when "extent_end == end", it can trigger the BUG_ON at
    file.c:600.  I hope I don't introduce new bug this time.
    
    ---
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 1cc4d285951c..a280b9562221 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -481,8 +481,9 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 			search_start = (extent_end + mask) & ~mask;
 		} else
 			search_start = extent_end;
-		if (end < extent_end && start > key.offset && found_inline) {
+		if (end <= extent_end && start >= key.offset && found_inline) {
 			*hint_byte = EXTENT_MAP_INLINE;
+			continue;
 		}
 		if (end < extent_end && end >= key.offset) {
 			if (found_extent) {

commit d8d5f3e16d1ae4fe9b93312e083f2c04a95520f0
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Dec 11 12:42:00 2007 -0500

    Btrfs: Add lowest key information to back refs for extent tree blocks as well.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 5b1f90f06e03..1cc4d285951c 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -545,13 +545,10 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 			u64 disk_num_bytes = 0;
 			u64 extent_num_bytes = 0;
 			u64 root_gen;
+			u64 root_owner;
 
-			if (leaf != root->node) {
-				root_gen =
-					btrfs_header_generation(path->nodes[1]);
-			} else {
-				root_gen = btrfs_header_generation(leaf);
-			}
+			root_gen = btrfs_header_generation(leaf);
+			root_owner = btrfs_header_owner(leaf);
 			if (found_extent) {
 				disk_bytenr =
 				      btrfs_file_extent_disk_bytenr(leaf,
@@ -575,7 +572,7 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 				ret = btrfs_free_extent(trans, root,
 						disk_bytenr,
 						disk_num_bytes,
-						root->root_key.objectid,
+						root_owner,
 						root_gen, inode->i_ino,
 						key.offset, 0);
 			}

commit 7bb86316c3961d1bc401ef184fd996f999556c7f
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Dec 11 09:25:06 2007 -0500

    Btrfs: Add back pointers from extents to the btree or file referencing them
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index b0d637787264..5b1f90f06e03 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -496,7 +496,10 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 						   sizeof(old));
 				if (disk_bytenr != 0) {
 					ret = btrfs_inc_extent_ref(trans, root,
-					         disk_bytenr, disk_num_bytes);
+					         disk_bytenr, disk_num_bytes,
+						 root->root_key.objectid,
+						 trans->transid,
+						 key.objectid, end);
 					BUG_ON(ret);
 				}
 			}
@@ -541,6 +544,14 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 			u64 disk_bytenr = 0;
 			u64 disk_num_bytes = 0;
 			u64 extent_num_bytes = 0;
+			u64 root_gen;
+
+			if (leaf != root->node) {
+				root_gen =
+					btrfs_header_generation(path->nodes[1]);
+			} else {
+				root_gen = btrfs_header_generation(leaf);
+			}
 			if (found_extent) {
 				disk_bytenr =
 				      btrfs_file_extent_disk_bytenr(leaf,
@@ -562,8 +573,11 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 			if (found_extent && disk_bytenr != 0) {
 				inode->i_blocks -= extent_num_bytes >> 9;
 				ret = btrfs_free_extent(trans, root,
-							disk_bytenr,
-							disk_num_bytes, 0);
+						disk_bytenr,
+						disk_num_bytes,
+						root->root_key.objectid,
+						root_gen, inode->i_ino,
+						key.offset, 0);
 			}
 
 			BUG_ON(ret);

commit 00f5c795fca47d038fedd3f0c9311da3be710c9f
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Nov 30 10:09:33 2007 -0500

    btrfs_drop_extents: make sure the item is getting smaller before truncate
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index ba624ae16e61..b0d637787264 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -377,23 +377,23 @@ int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end)
  */
 int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 		       struct btrfs_root *root, struct inode *inode,
-		       u64 start, u64 end, u64 inline_end, u64 *hint_byte)
+		       u64 start, u64 end, u64 inline_limit, u64 *hint_byte)
 {
-	int ret;
-	struct btrfs_key key;
+	u64 extent_end = 0;
+	u64 search_start = start;
 	struct extent_buffer *leaf;
-	int slot;
 	struct btrfs_file_extent_item *extent;
-	u64 extent_end = 0;
-	int keep;
-	struct btrfs_file_extent_item old;
 	struct btrfs_path *path;
-	u64 search_start = start;
+	struct btrfs_key key;
+	struct btrfs_file_extent_item old;
+	int keep;
+	int slot;
 	int bookend;
 	int found_type;
 	int found_extent;
 	int found_inline;
 	int recow;
+	int ret;
 
 	btrfs_drop_extent_cache(inode, start, end - 1);
 
@@ -502,7 +502,7 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 			}
 			bookend = 1;
 			if (found_inline && start <= key.offset &&
-			    inline_end < extent_end)
+			    inline_limit < extent_end)
 				keep = 1;
 		}
 		/* truncate existing extent */
@@ -526,12 +526,12 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 				btrfs_set_file_extent_num_bytes(leaf, extent,
 								new_num);
 				btrfs_mark_buffer_dirty(leaf);
-			} else if (end > extent_end &&
-				   key.offset < inline_end &&
-				   inline_end < extent_end) {
+			} else if (key.offset < inline_limit &&
+				   (end > extent_end) &&
+				   (inline_limit < extent_end)) {
 				u32 new_size;
 				new_size = btrfs_file_extent_calc_inline_size(
-						   inline_end - key.offset);
+						   inline_limit - key.offset);
 				btrfs_truncate_item(trans, root, path,
 						    new_size, 1);
 			}
@@ -575,10 +575,10 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 				continue;
 		}
 		if (bookend && found_inline && start <= key.offset &&
-		    inline_end < extent_end) {
+		    inline_limit < extent_end && key.offset <= inline_limit) {
 			u32 new_size;
 			new_size = btrfs_file_extent_calc_inline_size(
-						   extent_end - inline_end);
+						   extent_end - inline_limit);
 			btrfs_truncate_item(trans, root, path, new_size, 0);
 		}
 		/* create bookend, splitting the extent in two */

commit 448d640b668dae3928591e83f2bf4ca9f4c06e52
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Nov 27 07:52:01 2007 -0800

    Btrfs: Fine tune the btree writeback exclusion some more
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 9dd2c5c23b56..ba624ae16e61 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -767,7 +767,8 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 		num_written += write_bytes;
 
 		balance_dirty_pages_ratelimited_nr(inode->i_mapping, num_pages);
-		btrfs_btree_balance_dirty(root, 1);
+		if (num_pages < (root->leafsize >> PAGE_CACHE_SHIFT) + 1)
+			btrfs_btree_balance_dirty(root, 1);
 		cond_resched();
 	}
 	mutex_unlock(&inode->i_mutex);

commit 257d0ce36f215937602eb5c1b328b69afaf3bcd8
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Nov 7 21:08:16 2007 -0500

    Btrfs: Allow large data extents in a single file to span into metadata block groups
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 5ceaed25dce1..9dd2c5c23b56 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -439,6 +439,12 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 						struct btrfs_file_extent_item);
 			found_type = btrfs_file_extent_type(leaf, extent);
 			if (found_type == BTRFS_FILE_EXTENT_REG) {
+				extent_end =
+				     btrfs_file_extent_disk_bytenr(leaf,
+								   extent);
+				if (extent_end)
+					*hint_byte = extent_end;
+
 				extent_end = key.offset +
 				     btrfs_file_extent_num_bytes(leaf, extent);
 				found_extent = 1;

commit a273208edd55463b3bcd8b77a6fe8ba54afc6940
Author: Yan <yanzheng@21cn.com>
Date:   Tue Nov 6 10:26:28 2007 -0500

    Fix EXTENT_MAP_INLINE off by one in btrfs_drop_extents
    
    Don't set hint_byte to EXTENT_MAP_INLINE when 'end == extent_end' or
    'start == key.offset' . The inline extent will be truncated in these
    cases.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index bb98f52f4ea4..5ceaed25dce1 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -475,8 +475,7 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 			search_start = (extent_end + mask) & ~mask;
 		} else
 			search_start = extent_end;
-
-		if (end <= extent_end && start >= key.offset && found_inline) {
+		if (end < extent_end && start > key.offset && found_inline) {
 			*hint_byte = EXTENT_MAP_INLINE;
 		}
 		if (end < extent_end && end >= key.offset) {

commit dcfec0dcb1b1a037fb26177789e8f108bc429cb3
Author: Yan <yanzheng@21cn.com>
Date:   Tue Nov 6 10:26:26 2007 -0500

    Btrfs: Fix u32 overflow in dirty_and_release_pages.
    
    When calculating the size of inline extent,  inode->i_size should also
    be take into consideration, otherwise sys_write may drop some data
    silently.  You can test this bug by:
    
    #dd if=/dev/zero bs=4k count=1 of=test_file
    #dd if=/dev/zero bs=2k count=1 of=test_file conv=notrunc
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 4e52f7ec1cbe..bb98f52f4ea4 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -239,9 +239,8 @@ static int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 	u64 start_pos;
 	u64 end_of_last_block;
 	u64 end_pos = pos + write_bytes;
-	u32 inline_size;
+	u64 inline_size;
 	loff_t isize = i_size_read(inode);
-
 	em = alloc_extent_map(GFP_NOFS);
 	if (!em)
 		return -ENOMEM;
@@ -328,9 +327,11 @@ static int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 					 aligned_end, aligned_end, &hint_byte);
 		if (err)
 			goto failed;
+		if (isize > inline_size)
+			inline_size = min_t(u64, isize, aligned_end);
+		inline_size -= start_pos;
 		err = insert_inline_extent(trans, root, inode, start_pos,
-					   end_pos - start_pos, pages, 0,
-					   num_pages);
+					   inline_size, pages, 0, num_pages);
 		BUG_ON(err);
 	}
 	if (end_pos > isize) {

commit 179e29e488cc74f1e9bd67bc45f70b832740e9ec
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Nov 1 11:28:41 2007 -0400

    Btrfs: Fix a number of inline extent problems that Yan Zheng reported.
    
    The fixes do a number of things:
    
    1) Most btrfs_drop_extent callers will try to leave the inline extents in
    place.  It can truncate bytes off the beginning of the inline extent if
    required.
    
    2) writepage can now update the inline extent, allowing mmap writes to
    go directly into the inline extent.
    
    3) btrfs_truncate_in_transaction truncates inline extents
    
    4) extent_map.c fixed to not merge inline extent mappings and hole
    mappings together
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index abdd9caad94e..4e52f7ec1cbe 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -115,8 +115,20 @@ static int insert_inline_extent(struct btrfs_trans_handle *trans,
 		goto fail;
 	}
 	if (ret == 1) {
+		struct btrfs_key found_key;
+
+		if (path->slots[0] == 0)
+			goto insert;
+
 		path->slots[0]--;
 		leaf = path->nodes[0];
+		btrfs_item_key_to_cpu(leaf, &found_key, path->slots[0]);
+
+		if (found_key.objectid != inode->i_ino)
+			goto insert;
+
+		if (found_key.type != BTRFS_EXTENT_DATA_KEY)
+			goto insert;
 		ei = btrfs_item_ptr(leaf, path->slots[0],
 				    struct btrfs_file_extent_item);
 
@@ -152,6 +164,7 @@ static int insert_inline_extent(struct btrfs_trans_handle *trans,
 			ret = btrfs_search_slot(trans, root, &key, path,
 						offset + size - found_end, 1);
 			BUG_ON(ret != 0);
+
 			ret = btrfs_extend_item(trans, root, path,
 						offset + size - found_end);
 			if (ret) {
@@ -292,7 +305,7 @@ static int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 	 */
 	inline_size = end_pos;
 	if (isize >= BTRFS_MAX_INLINE_DATA_SIZE(root) ||
-	    inline_size > 8192 ||
+	    inline_size > 32768 ||
 	    inline_size >= BTRFS_MAX_INLINE_DATA_SIZE(root)) {
 		u64 last_end;
 
@@ -312,7 +325,7 @@ static int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 		aligned_end = (pos + write_bytes + root->sectorsize - 1) &
 			~((u64)root->sectorsize - 1);
 		err = btrfs_drop_extents(trans, root, inode, start_pos,
-					 aligned_end, end_pos, &hint_byte);
+					 aligned_end, aligned_end, &hint_byte);
 		if (err)
 			goto failed;
 		err = insert_inline_extent(trans, root, inode, start_pos,
@@ -456,13 +469,15 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 			goto next_slot;
 		}
 
-		/* FIXME, there's only one inline extent allowed right now */
 		if (found_inline) {
 			u64 mask = root->sectorsize - 1;
 			search_start = (extent_end + mask) & ~mask;
 		} else
 			search_start = extent_end;
 
+		if (end <= extent_end && start >= key.offset && found_inline) {
+			*hint_byte = EXTENT_MAP_INLINE;
+		}
 		if (end < extent_end && end >= key.offset) {
 			if (found_extent) {
 				u64 disk_bytenr =
@@ -479,8 +494,10 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 					BUG_ON(ret);
 				}
 			}
-			if (!found_inline)
-				bookend = 1;
+			bookend = 1;
+			if (found_inline && start <= key.offset &&
+			    inline_end < extent_end)
+				keep = 1;
 		}
 		/* truncate existing extent */
 		if (start > key.offset) {
@@ -510,7 +527,7 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 				new_size = btrfs_file_extent_calc_inline_size(
 						   inline_end - key.offset);
 				btrfs_truncate_item(trans, root, path,
-						    new_size);
+						    new_size, 1);
 			}
 		}
 		/* delete the entire extent */
@@ -551,6 +568,13 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 			if (!bookend)
 				continue;
 		}
+		if (bookend && found_inline && start <= key.offset &&
+		    inline_end < extent_end) {
+			u32 new_size;
+			new_size = btrfs_file_extent_calc_inline_size(
+						   extent_end - inline_end);
+			btrfs_truncate_item(trans, root, path, new_size, 0);
+		}
 		/* create bookend, splitting the extent in two */
 		if (bookend && found_extent) {
 			struct btrfs_key ins;

commit 35ebb934bd7fcc7ca991b155b7980c3c4ff9f1a5
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Oct 30 16:56:53 2007 -0400

    Btrfs: Fix PAGE_CACHE_SHIFT shifts on 32 bit machines
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 843e920388ed..abdd9caad94e 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -301,7 +301,8 @@ static int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 			SetPageUptodate(p);
 			set_page_dirty(p);
 		}
-		last_end = pages[num_pages -1]->index << PAGE_CACHE_SHIFT;
+		last_end = (u64)(pages[num_pages -1]->index) <<
+				PAGE_CACHE_SHIFT;
 		last_end += PAGE_CACHE_SIZE - 1;
 		set_extent_delalloc(em_tree, start_pos, end_of_last_block,
 				 GFP_NOFS);

commit 2ff3e9b61d02b03e3157f7d43ba20ee1452814de
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Oct 29 14:36:41 2007 -0400

    Add O_SYNC support to btrfs_file_write
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 9260d3478aad..843e920388ed 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -635,8 +635,9 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 				size_t count, loff_t *ppos)
 {
 	loff_t pos;
-	size_t num_written = 0;
-	int err = 0;
+	loff_t start_pos;
+	ssize_t num_written = 0;
+	ssize_t err = 0;
 	int ret = 0;
 	struct inode *inode = file->f_path.dentry->d_inode;
 	struct btrfs_root *root = BTRFS_I(inode)->root;
@@ -652,7 +653,10 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 	pinned[1] = NULL;
 	if (file->f_flags & O_DIRECT)
 		return -EINVAL;
+
 	pos = *ppos;
+	start_pos = pos;
+
 	vfs_check_frozen(inode->i_sb, SB_FREEZE_WRITE);
 	current->backing_dev_info = inode->i_mapping->backing_dev_info;
 	err = generic_write_checks(file, &pos, &count, S_ISBLK(inode->i_mode));
@@ -743,6 +747,13 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 	if (pinned[1])
 		page_cache_release(pinned[1]);
 	*ppos = pos;
+
+	if (num_written > 0 && ((file->f_flags & O_SYNC) || IS_SYNC(inode))) {
+		err = sync_page_range(inode, inode->i_mapping,
+				      start_pos, num_written);
+		if (err < 0)
+			num_written = err;
+	}
 	current->backing_dev_info = NULL;
 	return num_written ? num_written : err;
 }

commit 18f16f7ba62a01c29e09b40ac6ad6d92a8955859
Author: Yan <yanzheng@21cn.com>
Date:   Thu Oct 25 15:42:57 2007 -0400

    Btrfs: Fix for insert_inline_extent to handle offset != 0
    
    This modifies inline extent size calculation, so that
    insert_inline_extent can handle the case that parameter 'offset' is
    not zero; it also a few codes to zero uninitialized area in inline
    extent.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 96df1b10cb60..9260d3478aad 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -108,7 +108,6 @@ static int insert_inline_extent(struct btrfs_trans_handle *trans,
 	key.objectid = inode->i_ino;
 	key.offset = offset;
 	btrfs_set_key_type(&key, BTRFS_EXTENT_DATA_KEY);
-	datasize = btrfs_file_extent_calc_inline_size(offset + size);
 
 	ret = btrfs_search_slot(trans, root, &key, path, 0, 1);
 	if (ret < 0) {
@@ -130,7 +129,7 @@ static int insert_inline_extent(struct btrfs_trans_handle *trans,
 	}
 	if (ret == 0) {
 		u32 found_size;
-		u64 found_start;
+		u64 found_end;
 
 		leaf = path->nodes[0];
 		ei = btrfs_item_ptr(leaf, path->slots[0],
@@ -144,19 +143,17 @@ static int insert_inline_extent(struct btrfs_trans_handle *trans,
 			       offset, inode->i_ino);
 			goto fail;
 		}
-		found_start = key.offset;
 		found_size = btrfs_file_extent_inline_len(leaf,
 					  btrfs_item_nr(leaf, path->slots[0]));
+		found_end = key.offset + found_size;
 
-		if (found_size < offset + size) {
+		if (found_end < offset + size) {
 			btrfs_release_path(root, path);
 			ret = btrfs_search_slot(trans, root, &key, path,
-						offset + size - found_size -
-						found_start, 1);
+						offset + size - found_end, 1);
 			BUG_ON(ret != 0);
 			ret = btrfs_extend_item(trans, root, path,
-						offset + size - found_size -
-						found_start);
+						offset + size - found_end);
 			if (ret) {
 				err = ret;
 				goto fail;
@@ -165,9 +162,15 @@ static int insert_inline_extent(struct btrfs_trans_handle *trans,
 			ei = btrfs_item_ptr(leaf, path->slots[0],
 					    struct btrfs_file_extent_item);
 		}
+		if (found_end < offset) {
+			ptr = btrfs_file_extent_inline_start(ei) + found_size;
+			memset_extent_buffer(leaf, 0, ptr, offset - found_end);
+		}
 	} else {
 insert:
 		btrfs_release_path(root, path);
+		datasize = offset + size - key.offset;
+		datasize = btrfs_file_extent_calc_inline_size(datasize);
 		ret = btrfs_insert_empty_item(trans, root, path, &key,
 					      datasize);
 		if (ret) {
@@ -181,7 +184,7 @@ static int insert_inline_extent(struct btrfs_trans_handle *trans,
 		btrfs_set_file_extent_generation(leaf, ei, trans->transid);
 		btrfs_set_file_extent_type(leaf, ei, BTRFS_FILE_EXTENT_INLINE);
 	}
-	ptr = btrfs_file_extent_inline_start(ei) + offset;
+	ptr = btrfs_file_extent_inline_start(ei) + offset - key.offset;
 
 	cur_size = size;
 	i = 0;

commit ae2f5411c4ce7180cca8418853db50c8e52d40db
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Fri Oct 19 09:22:59 2007 -0400

    btrfs: 32-bit type problems
    
    An assorted set of casts to get rid of the warnings on 32-bit archs.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 4aacf99bd97f..96df1b10cb60 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -188,7 +188,7 @@ static int insert_inline_extent(struct btrfs_trans_handle *trans,
 	while (size > 0) {
 		page = pages[i];
 		kaddr = kmap_atomic(page, KM_USER0);
-		cur_size = min(PAGE_CACHE_SIZE - page_offset, size);
+		cur_size = min_t(size_t, PAGE_CACHE_SIZE - page_offset, size);
 		write_extent_buffer(leaf, kaddr + page_offset, ptr, cur_size);
 		kunmap_atomic(kaddr, KM_USER0);
 		page_offset = 0;

commit 7936ca3883e5fef8ce5cc367a4356ad5fed67180
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Oct 19 09:22:41 2007 -0400

    Btrfs: Default to 8k max packed tails
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 1af2b6534dad..4aacf99bd97f 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -289,7 +289,7 @@ static int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 	 */
 	inline_size = end_pos;
 	if (isize >= BTRFS_MAX_INLINE_DATA_SIZE(root) ||
-	    inline_size > 16384 ||
+	    inline_size > 8192 ||
 	    inline_size >= BTRFS_MAX_INLINE_DATA_SIZE(root)) {
 		u64 last_end;
 

commit ff79f8190b6e955ff7a71faf804a3017d526e657
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Oct 15 16:22:25 2007 -0400

    Btrfs: Add back file data checksumming
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index fe28404ae7f4..1af2b6534dad 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -289,6 +289,7 @@ static int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 	 */
 	inline_size = end_pos;
 	if (isize >= BTRFS_MAX_INLINE_DATA_SIZE(root) ||
+	    inline_size > 16384 ||
 	    inline_size >= BTRFS_MAX_INLINE_DATA_SIZE(root)) {
 		u64 last_end;
 

commit 810191ff3087e8143b41a944fcf4fd8c693f00e3
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Oct 15 16:18:55 2007 -0400

    Btrfs: extent_map optimizations to cut down on CPU usage
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 1af2b6534dad..fe28404ae7f4 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -289,7 +289,6 @@ static int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 	 */
 	inline_size = end_pos;
 	if (isize >= BTRFS_MAX_INLINE_DATA_SIZE(root) ||
-	    inline_size > 16384 ||
 	    inline_size >= BTRFS_MAX_INLINE_DATA_SIZE(root)) {
 		u64 last_end;
 

commit 3326d1b07c0cb6a2ff5b835b7a2cffa54124d074
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Oct 15 16:18:25 2007 -0400

    Btrfs: Allow tails larger than one page
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 844d8807e44a..1af2b6534dad 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -82,8 +82,9 @@ static void btrfs_drop_pages(struct page **pages, size_t num_pages)
 
 static int insert_inline_extent(struct btrfs_trans_handle *trans,
 				struct btrfs_root *root, struct inode *inode,
-				u64 offset, ssize_t size,
-				struct page *page, size_t page_offset)
+				u64 offset, size_t size,
+				struct page **pages, size_t page_offset,
+				int num_pages)
 {
 	struct btrfs_key key;
 	struct btrfs_path *path;
@@ -91,9 +92,12 @@ static int insert_inline_extent(struct btrfs_trans_handle *trans,
 	char *kaddr;
 	unsigned long ptr;
 	struct btrfs_file_extent_item *ei;
+	struct page *page;
 	u32 datasize;
 	int err = 0;
 	int ret;
+	int i;
+	ssize_t cur_size;
 
 	path = btrfs_alloc_path();
 	if (!path)
@@ -104,25 +108,97 @@ static int insert_inline_extent(struct btrfs_trans_handle *trans,
 	key.objectid = inode->i_ino;
 	key.offset = offset;
 	btrfs_set_key_type(&key, BTRFS_EXTENT_DATA_KEY);
-	BUG_ON(size >= PAGE_CACHE_SIZE);
-	datasize = btrfs_file_extent_calc_inline_size(size);
+	datasize = btrfs_file_extent_calc_inline_size(offset + size);
 
-	ret = btrfs_insert_empty_item(trans, root, path, &key,
-				      datasize);
-	if (ret) {
+	ret = btrfs_search_slot(trans, root, &key, path, 0, 1);
+	if (ret < 0) {
 		err = ret;
 		goto fail;
 	}
-	leaf = path->nodes[0];
-	ei = btrfs_item_ptr(leaf, path->slots[0],
-			    struct btrfs_file_extent_item);
-	btrfs_set_file_extent_generation(leaf, ei, trans->transid);
-	btrfs_set_file_extent_type(leaf, ei, BTRFS_FILE_EXTENT_INLINE);
-	ptr = btrfs_file_extent_inline_start(ei);
-
-	kaddr = kmap_atomic(page, KM_USER1);
-	write_extent_buffer(leaf, kaddr + page_offset, ptr, size);
-	kunmap_atomic(kaddr, KM_USER1);
+	if (ret == 1) {
+		path->slots[0]--;
+		leaf = path->nodes[0];
+		ei = btrfs_item_ptr(leaf, path->slots[0],
+				    struct btrfs_file_extent_item);
+
+		if (btrfs_file_extent_type(leaf, ei) !=
+		    BTRFS_FILE_EXTENT_INLINE) {
+			goto insert;
+		}
+		btrfs_item_key_to_cpu(leaf, &key, path->slots[0]);
+		ret = 0;
+	}
+	if (ret == 0) {
+		u32 found_size;
+		u64 found_start;
+
+		leaf = path->nodes[0];
+		ei = btrfs_item_ptr(leaf, path->slots[0],
+				    struct btrfs_file_extent_item);
+
+		if (btrfs_file_extent_type(leaf, ei) !=
+		    BTRFS_FILE_EXTENT_INLINE) {
+			err = ret;
+			btrfs_print_leaf(root, leaf);
+			printk("found wasn't inline offset %Lu inode %lu\n",
+			       offset, inode->i_ino);
+			goto fail;
+		}
+		found_start = key.offset;
+		found_size = btrfs_file_extent_inline_len(leaf,
+					  btrfs_item_nr(leaf, path->slots[0]));
+
+		if (found_size < offset + size) {
+			btrfs_release_path(root, path);
+			ret = btrfs_search_slot(trans, root, &key, path,
+						offset + size - found_size -
+						found_start, 1);
+			BUG_ON(ret != 0);
+			ret = btrfs_extend_item(trans, root, path,
+						offset + size - found_size -
+						found_start);
+			if (ret) {
+				err = ret;
+				goto fail;
+			}
+			leaf = path->nodes[0];
+			ei = btrfs_item_ptr(leaf, path->slots[0],
+					    struct btrfs_file_extent_item);
+		}
+	} else {
+insert:
+		btrfs_release_path(root, path);
+		ret = btrfs_insert_empty_item(trans, root, path, &key,
+					      datasize);
+		if (ret) {
+			err = ret;
+			printk("got bad ret %d\n", ret);
+			goto fail;
+		}
+		leaf = path->nodes[0];
+		ei = btrfs_item_ptr(leaf, path->slots[0],
+				    struct btrfs_file_extent_item);
+		btrfs_set_file_extent_generation(leaf, ei, trans->transid);
+		btrfs_set_file_extent_type(leaf, ei, BTRFS_FILE_EXTENT_INLINE);
+	}
+	ptr = btrfs_file_extent_inline_start(ei) + offset;
+
+	cur_size = size;
+	i = 0;
+	while (size > 0) {
+		page = pages[i];
+		kaddr = kmap_atomic(page, KM_USER0);
+		cur_size = min(PAGE_CACHE_SIZE - page_offset, size);
+		write_extent_buffer(leaf, kaddr + page_offset, ptr, cur_size);
+		kunmap_atomic(kaddr, KM_USER0);
+		page_offset = 0;
+		ptr += cur_size;
+		size -= cur_size;
+		if (i >= num_pages) {
+			printk("i %d num_pages %d\n", i, num_pages);
+		}
+		i++;
+	}
 	btrfs_mark_buffer_dirty(leaf);
 fail:
 	btrfs_free_path(path);
@@ -193,6 +269,7 @@ static int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 			err = btrfs_drop_extents(trans, root, inode,
 						 last_pos_in_file,
 						 last_pos_in_file + hole_size,
+						 last_pos_in_file,
 						 &hint_byte);
 			if (err)
 				goto failed;
@@ -210,11 +287,12 @@ static int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 	 * either allocate an extent for the new bytes or setup the key
 	 * to show we are doing inline data in the extent
 	 */
-	inline_size = end_pos - start_pos;
-	if (isize >= PAGE_CACHE_SIZE || pos + write_bytes < inode->i_size ||
-	    inline_size >= BTRFS_MAX_INLINE_DATA_SIZE(root) ||
-	    inline_size >= PAGE_CACHE_SIZE) {
+	inline_size = end_pos;
+	if (isize >= BTRFS_MAX_INLINE_DATA_SIZE(root) ||
+	    inline_size > 16384 ||
+	    inline_size >= BTRFS_MAX_INLINE_DATA_SIZE(root)) {
 		u64 last_end;
+
 		for (i = 0; i < num_pages; i++) {
 			struct page *p = pages[i];
 			SetPageUptodate(p);
@@ -225,22 +303,18 @@ static int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 		set_extent_delalloc(em_tree, start_pos, end_of_last_block,
 				 GFP_NOFS);
 	} else {
-		struct page *p = pages[0];
+		u64 aligned_end;
 		/* step one, delete the existing extents in this range */
+		aligned_end = (pos + write_bytes + root->sectorsize - 1) &
+			~((u64)root->sectorsize - 1);
 		err = btrfs_drop_extents(trans, root, inode, start_pos,
-			 (pos + write_bytes + root->sectorsize -1) &
-			 ~((u64)root->sectorsize - 1), &hint_byte);
+					 aligned_end, end_pos, &hint_byte);
 		if (err)
 			goto failed;
-
 		err = insert_inline_extent(trans, root, inode, start_pos,
-					   end_pos - start_pos, p, 0);
+					   end_pos - start_pos, pages, 0,
+					   num_pages);
 		BUG_ON(err);
-		em->start = start_pos;
-		em->end = end_pos - 1;
-		em->block_start = EXTENT_MAP_INLINE;
-		em->block_end = EXTENT_MAP_INLINE;
-		add_extent_mapping(em_tree, em);
 	}
 	if (end_pos > isize) {
 		i_size_write(inode, end_pos);
@@ -285,7 +359,7 @@ int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end)
  */
 int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 		       struct btrfs_root *root, struct inode *inode,
-		       u64 start, u64 end, u64 *hint_byte)
+		       u64 start, u64 end, u64 inline_end, u64 *hint_byte)
 {
 	int ret;
 	struct btrfs_key key;
@@ -401,8 +475,8 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 					BUG_ON(ret);
 				}
 			}
-			WARN_ON(found_inline);
-			bookend = 1;
+			if (!found_inline)
+				bookend = 1;
 		}
 		/* truncate existing extent */
 		if (start > key.offset) {
@@ -425,8 +499,14 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 				btrfs_set_file_extent_num_bytes(leaf, extent,
 								new_num);
 				btrfs_mark_buffer_dirty(leaf);
-			} else {
-				WARN_ON(1);
+			} else if (end > extent_end &&
+				   key.offset < inline_end &&
+				   inline_end < extent_end) {
+				u32 new_size;
+				new_size = btrfs_file_extent_calc_inline_size(
+						   inline_end - key.offset);
+				btrfs_truncate_item(trans, root, path,
+						    new_size);
 			}
 		}
 		/* delete the entire extent */

commit db94535db75e67fab12ccbb7f5ee548e33fed891
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Oct 15 16:15:53 2007 -0400

    Btrfs: Allow tree blocks larger than the page size
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 1734ca695555..844d8807e44a 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -120,9 +120,9 @@ static int insert_inline_extent(struct btrfs_trans_handle *trans,
 	btrfs_set_file_extent_type(leaf, ei, BTRFS_FILE_EXTENT_INLINE);
 	ptr = btrfs_file_extent_inline_start(ei);
 
-	kaddr = kmap_atomic(page, KM_USER0);
+	kaddr = kmap_atomic(page, KM_USER1);
 	write_extent_buffer(leaf, kaddr + page_offset, ptr, size);
-	kunmap_atomic(kaddr, KM_USER0);
+	kunmap_atomic(kaddr, KM_USER1);
 	btrfs_mark_buffer_dirty(leaf);
 fail:
 	btrfs_free_path(path);
@@ -142,11 +142,12 @@ static int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 	struct inode *inode = file->f_path.dentry->d_inode;
 	struct extent_map *em;
 	struct extent_map_tree *em_tree = &BTRFS_I(inode)->extent_tree;
-	u64 hint_block;
-	u64 num_blocks;
+	u64 hint_byte;
+	u64 num_bytes;
 	u64 start_pos;
 	u64 end_of_last_block;
 	u64 end_pos = pos + write_bytes;
+	u32 inline_size;
 	loff_t isize = i_size_read(inode);
 
 	em = alloc_extent_map(GFP_NOFS);
@@ -156,11 +157,12 @@ static int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 	em->bdev = inode->i_sb->s_bdev;
 
 	start_pos = pos & ~((u64)root->sectorsize - 1);
-	num_blocks = (write_bytes + pos - start_pos + root->sectorsize - 1) >>
-			inode->i_blkbits;
+	num_bytes = (write_bytes + pos - start_pos +
+		    root->sectorsize - 1) & ~((u64)root->sectorsize - 1);
 
 	down_read(&BTRFS_I(inode)->root->snap_sem);
-	end_of_last_block = start_pos + (num_blocks << inode->i_blkbits) - 1;
+	end_of_last_block = start_pos + num_bytes - 1;
+
 	lock_extent(em_tree, start_pos, end_of_last_block, GFP_NOFS);
 	mutex_lock(&root->fs_info->fs_mutex);
 	trans = btrfs_start_transaction(root, 1);
@@ -169,8 +171,8 @@ static int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 		goto out_unlock;
 	}
 	btrfs_set_trans_block_group(trans, inode);
-	inode->i_blocks += num_blocks << 3;
-	hint_block = 0;
+	inode->i_blocks += num_bytes >> 9;
+	hint_byte = 0;
 
 	if ((end_of_last_block & 4095) == 0) {
 		printk("strange end of last %Lu %zu %Lu\n", start_pos, write_bytes, end_of_last_block);
@@ -191,11 +193,10 @@ static int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 			err = btrfs_drop_extents(trans, root, inode,
 						 last_pos_in_file,
 						 last_pos_in_file + hole_size,
-						 &hint_block);
+						 &hint_byte);
 			if (err)
 				goto failed;
 
-			hole_size >>= inode->i_blkbits;
 			err = btrfs_insert_file_extent(trans, root,
 						       inode->i_ino,
 						       last_pos_in_file,
@@ -209,8 +210,10 @@ static int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 	 * either allocate an extent for the new bytes or setup the key
 	 * to show we are doing inline data in the extent
 	 */
+	inline_size = end_pos - start_pos;
 	if (isize >= PAGE_CACHE_SIZE || pos + write_bytes < inode->i_size ||
-	    pos + write_bytes - start_pos > BTRFS_MAX_INLINE_DATA_SIZE(root)) {
+	    inline_size >= BTRFS_MAX_INLINE_DATA_SIZE(root) ||
+	    inline_size >= PAGE_CACHE_SIZE) {
 		u64 last_end;
 		for (i = 0; i < num_pages; i++) {
 			struct page *p = pages[i];
@@ -224,10 +227,9 @@ static int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 	} else {
 		struct page *p = pages[0];
 		/* step one, delete the existing extents in this range */
-		/* FIXME blocksize != pagesize */
 		err = btrfs_drop_extents(trans, root, inode, start_pos,
 			 (pos + write_bytes + root->sectorsize -1) &
-			 ~((u64)root->sectorsize - 1), &hint_block);
+			 ~((u64)root->sectorsize - 1), &hint_byte);
 		if (err)
 			goto failed;
 
@@ -283,7 +285,7 @@ int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end)
  */
 int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 		       struct btrfs_root *root, struct inode *inode,
-		       u64 start, u64 end, u64 *hint_block)
+		       u64 start, u64 end, u64 *hint_byte)
 {
 	int ret;
 	struct btrfs_key key;
@@ -346,8 +348,7 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 			found_type = btrfs_file_extent_type(leaf, extent);
 			if (found_type == BTRFS_FILE_EXTENT_REG) {
 				extent_end = key.offset +
-				 (btrfs_file_extent_num_blocks(leaf, extent) <<
-					 inode->i_blkbits);
+				     btrfs_file_extent_num_bytes(leaf, extent);
 				found_extent = 1;
 			} else if (found_type == BTRFS_FILE_EXTENT_INLINE) {
 				struct btrfs_item *item;
@@ -386,17 +387,17 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 
 		if (end < extent_end && end >= key.offset) {
 			if (found_extent) {
-				u64 disk_blocknr =
-				    btrfs_file_extent_disk_blocknr(leaf,extent);
-				u64 disk_num_blocks =
-				    btrfs_file_extent_disk_num_blocks(leaf,
+				u64 disk_bytenr =
+				    btrfs_file_extent_disk_bytenr(leaf, extent);
+				u64 disk_num_bytes =
+				    btrfs_file_extent_disk_num_bytes(leaf,
 								      extent);
 				read_extent_buffer(leaf, &old,
 						   (unsigned long)extent,
 						   sizeof(old));
-				if (disk_blocknr != 0) {
+				if (disk_bytenr != 0) {
 					ret = btrfs_inc_extent_ref(trans, root,
-					         disk_blocknr, disk_num_blocks);
+					         disk_bytenr, disk_num_bytes);
 					BUG_ON(ret);
 				}
 			}
@@ -410,21 +411,19 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 			keep = 1;
 			WARN_ON(start & (root->sectorsize - 1));
 			if (found_extent) {
-				new_num = (start - key.offset) >>
-					inode->i_blkbits;
-				old_num = btrfs_file_extent_num_blocks(leaf,
-								       extent);
-				*hint_block =
-					btrfs_file_extent_disk_blocknr(leaf,
-								       extent);
-				if (btrfs_file_extent_disk_blocknr(leaf,
-								   extent)) {
+				new_num = start - key.offset;
+				old_num = btrfs_file_extent_num_bytes(leaf,
+								      extent);
+				*hint_byte =
+					btrfs_file_extent_disk_bytenr(leaf,
+								      extent);
+				if (btrfs_file_extent_disk_bytenr(leaf,
+								  extent)) {
 					inode->i_blocks -=
-						(old_num - new_num) << 3;
+						(old_num - new_num) >> 9;
 				}
-				btrfs_set_file_extent_num_blocks(leaf,
-								 extent,
-								 new_num);
+				btrfs_set_file_extent_num_bytes(leaf, extent,
+								new_num);
 				btrfs_mark_buffer_dirty(leaf);
 			} else {
 				WARN_ON(1);
@@ -432,33 +431,32 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 		}
 		/* delete the entire extent */
 		if (!keep) {
-			u64 disk_blocknr = 0;
-			u64 disk_num_blocks = 0;
-			u64 extent_num_blocks = 0;
+			u64 disk_bytenr = 0;
+			u64 disk_num_bytes = 0;
+			u64 extent_num_bytes = 0;
 			if (found_extent) {
-				disk_blocknr =
-				      btrfs_file_extent_disk_blocknr(leaf,
+				disk_bytenr =
+				      btrfs_file_extent_disk_bytenr(leaf,
 								     extent);
-				disk_num_blocks =
-				      btrfs_file_extent_disk_num_blocks(leaf,
-									extent);
-				extent_num_blocks =
-				      btrfs_file_extent_num_blocks(leaf,
-								   extent);
-				*hint_block =
-					btrfs_file_extent_disk_blocknr(leaf,
+				disk_num_bytes =
+				      btrfs_file_extent_disk_num_bytes(leaf,
 								       extent);
+				extent_num_bytes =
+				      btrfs_file_extent_num_bytes(leaf, extent);
+				*hint_byte =
+					btrfs_file_extent_disk_bytenr(leaf,
+								      extent);
 			}
 			ret = btrfs_del_item(trans, root, path);
 			/* TODO update progress marker and return */
 			BUG_ON(ret);
 			btrfs_release_path(root, path);
 			extent = NULL;
-			if (found_extent && disk_blocknr != 0) {
-				inode->i_blocks -= extent_num_blocks << 3;
+			if (found_extent && disk_bytenr != 0) {
+				inode->i_blocks -= extent_num_bytes >> 9;
 				ret = btrfs_free_extent(trans, root,
-							disk_blocknr,
-							disk_num_blocks, 0);
+							disk_bytenr,
+							disk_num_bytes, 0);
 			}
 
 			BUG_ON(ret);
@@ -491,20 +489,19 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 					    (unsigned long)extent, sizeof(old));
 
 			btrfs_set_file_extent_offset(leaf, extent,
-				    le64_to_cpu(old.offset) +
-				    ((end - key.offset) >> inode->i_blkbits));
-			WARN_ON(le64_to_cpu(old.num_blocks) <
-				(extent_end - end) >> inode->i_blkbits);
-			btrfs_set_file_extent_num_blocks(leaf, extent,
-				    (extent_end - end) >> inode->i_blkbits);
-
+				    le64_to_cpu(old.offset) + end - key.offset);
+			WARN_ON(le64_to_cpu(old.num_bytes) <
+				(extent_end - end));
+			btrfs_set_file_extent_num_bytes(leaf, extent,
+							extent_end - end);
 			btrfs_set_file_extent_type(leaf, extent,
 						   BTRFS_FILE_EXTENT_REG);
+
 			btrfs_mark_buffer_dirty(path->nodes[0]);
-			if (le64_to_cpu(old.disk_blocknr) != 0) {
+			if (le64_to_cpu(old.disk_bytenr) != 0) {
 				inode->i_blocks +=
-				      btrfs_file_extent_num_blocks(leaf,
-								   extent) << 3;
+				      btrfs_file_extent_num_bytes(leaf,
+								  extent) >> 9;
 			}
 			ret = 0;
 			goto out;
@@ -531,12 +528,9 @@ static int prepare_pages(struct btrfs_root *root,
 	unsigned long index = pos >> PAGE_CACHE_SHIFT;
 	struct inode *inode = file->f_path.dentry->d_inode;
 	int err = 0;
-	u64 num_blocks;
 	u64 start_pos;
 
 	start_pos = pos & ~((u64)root->sectorsize - 1);
-	num_blocks = (write_bytes + pos - start_pos + root->sectorsize - 1) >>
-			inode->i_blkbits;
 
 	memset(pages, 0, num_pages * sizeof(struct page *));
 

commit 5f39d397dfbe140a14edecd4e73c34ce23c4f9ee
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Oct 15 16:14:19 2007 -0400

    Btrfs: Create extent_buffer interface for large blocksizes
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 4cc459c943ec..1734ca695555 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -16,7 +16,6 @@
  * Boston, MA 021110-1307, USA.
  */
 
-#include <linux/buffer_head.h>
 #include <linux/fs.h>
 #include <linux/pagemap.h>
 #include <linux/highmem.h>
@@ -88,7 +87,9 @@ static int insert_inline_extent(struct btrfs_trans_handle *trans,
 {
 	struct btrfs_key key;
 	struct btrfs_path *path;
-	char *ptr, *kaddr;
+	struct extent_buffer *leaf;
+	char *kaddr;
+	unsigned long ptr;
 	struct btrfs_file_extent_item *ei;
 	u32 datasize;
 	int err = 0;
@@ -102,7 +103,6 @@ static int insert_inline_extent(struct btrfs_trans_handle *trans,
 
 	key.objectid = inode->i_ino;
 	key.offset = offset;
-	key.flags = 0;
 	btrfs_set_key_type(&key, BTRFS_EXTENT_DATA_KEY);
 	BUG_ON(size >= PAGE_CACHE_SIZE);
 	datasize = btrfs_file_extent_calc_inline_size(size);
@@ -113,18 +113,17 @@ static int insert_inline_extent(struct btrfs_trans_handle *trans,
 		err = ret;
 		goto fail;
 	}
-	ei = btrfs_item_ptr(btrfs_buffer_leaf(path->nodes[0]),
-	       path->slots[0], struct btrfs_file_extent_item);
-	btrfs_set_file_extent_generation(ei, trans->transid);
-	btrfs_set_file_extent_type(ei,
-				   BTRFS_FILE_EXTENT_INLINE);
+	leaf = path->nodes[0];
+	ei = btrfs_item_ptr(leaf, path->slots[0],
+			    struct btrfs_file_extent_item);
+	btrfs_set_file_extent_generation(leaf, ei, trans->transid);
+	btrfs_set_file_extent_type(leaf, ei, BTRFS_FILE_EXTENT_INLINE);
 	ptr = btrfs_file_extent_inline_start(ei);
 
 	kaddr = kmap_atomic(page, KM_USER0);
-	btrfs_memcpy(root, path->nodes[0]->b_data,
-		     ptr, kaddr + page_offset, size);
+	write_extent_buffer(leaf, kaddr + page_offset, ptr, size);
 	kunmap_atomic(kaddr, KM_USER0);
-	btrfs_mark_buffer_dirty(path->nodes[0]);
+	btrfs_mark_buffer_dirty(leaf);
 fail:
 	btrfs_free_path(path);
 	return err;
@@ -156,8 +155,8 @@ static int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 
 	em->bdev = inode->i_sb->s_bdev;
 
-	start_pos = pos & ~((u64)root->blocksize - 1);
-	num_blocks = (write_bytes + pos - start_pos + root->blocksize - 1) >>
+	start_pos = pos & ~((u64)root->sectorsize - 1);
+	num_blocks = (write_bytes + pos - start_pos + root->sectorsize - 1) >>
 			inode->i_blkbits;
 
 	down_read(&BTRFS_I(inode)->root->snap_sem);
@@ -184,7 +183,7 @@ static int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 	if (inode->i_size < start_pos) {
 		u64 last_pos_in_file;
 		u64 hole_size;
-		u64 mask = root->blocksize - 1;
+		u64 mask = root->sectorsize - 1;
 		last_pos_in_file = (isize + mask) & ~mask;
 		hole_size = (start_pos - last_pos_in_file + mask) & ~mask;
 
@@ -227,8 +226,8 @@ static int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 		/* step one, delete the existing extents in this range */
 		/* FIXME blocksize != pagesize */
 		err = btrfs_drop_extents(trans, root, inode, start_pos,
-			 (pos + write_bytes + root->blocksize -1) &
-			 ~((u64)root->blocksize - 1), &hint_block);
+			 (pos + write_bytes + root->sectorsize -1) &
+			 ~((u64)root->sectorsize - 1), &hint_block);
 		if (err)
 			goto failed;
 
@@ -288,7 +287,7 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 {
 	int ret;
 	struct btrfs_key key;
-	struct btrfs_leaf *leaf;
+	struct extent_buffer *leaf;
 	int slot;
 	struct btrfs_file_extent_item *extent;
 	u64 extent_end = 0;
@@ -327,10 +326,10 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 		found_extent = 0;
 		found_inline = 0;
 		extent = NULL;
-		leaf = btrfs_buffer_leaf(path->nodes[0]);
+		leaf = path->nodes[0];
 		slot = path->slots[0];
 		ret = 0;
-		btrfs_disk_key_to_cpu(&key, &leaf->items[slot].key);
+		btrfs_item_key_to_cpu(leaf, &key, slot);
 		if (key.offset >= end || key.objectid != inode->i_ino) {
 			goto out;
 		}
@@ -344,17 +343,18 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 		if (btrfs_key_type(&key) == BTRFS_EXTENT_DATA_KEY) {
 			extent = btrfs_item_ptr(leaf, slot,
 						struct btrfs_file_extent_item);
-			found_type = btrfs_file_extent_type(extent);
+			found_type = btrfs_file_extent_type(leaf, extent);
 			if (found_type == BTRFS_FILE_EXTENT_REG) {
 				extent_end = key.offset +
-					(btrfs_file_extent_num_blocks(extent) <<
+				 (btrfs_file_extent_num_blocks(leaf, extent) <<
 					 inode->i_blkbits);
 				found_extent = 1;
 			} else if (found_type == BTRFS_FILE_EXTENT_INLINE) {
+				struct btrfs_item *item;
+				item = btrfs_item_nr(leaf, slot);
 				found_inline = 1;
 				extent_end = key.offset +
-				     btrfs_file_extent_inline_len(leaf->items +
-								  slot);
+				     btrfs_file_extent_inline_len(leaf, item);
 			}
 		} else {
 			extent_end = search_start;
@@ -365,8 +365,7 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 		    search_start >= extent_end) {
 			int nextret;
 			u32 nritems;
-			nritems = btrfs_header_nritems(
-					btrfs_buffer_header(path->nodes[0]));
+			nritems = btrfs_header_nritems(leaf);
 			if (slot >= nritems - 1) {
 				nextret = btrfs_next_leaf(root, path);
 				if (nextret)
@@ -380,7 +379,7 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 
 		/* FIXME, there's only one inline extent allowed right now */
 		if (found_inline) {
-			u64 mask = root->blocksize - 1;
+			u64 mask = root->sectorsize - 1;
 			search_start = (extent_end + mask) & ~mask;
 		} else
 			search_start = extent_end;
@@ -388,10 +387,13 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 		if (end < extent_end && end >= key.offset) {
 			if (found_extent) {
 				u64 disk_blocknr =
-					btrfs_file_extent_disk_blocknr(extent);
+				    btrfs_file_extent_disk_blocknr(leaf,extent);
 				u64 disk_num_blocks =
-				      btrfs_file_extent_disk_num_blocks(extent);
-				memcpy(&old, extent, sizeof(old));
+				    btrfs_file_extent_disk_num_blocks(leaf,
+								      extent);
+				read_extent_buffer(leaf, &old,
+						   (unsigned long)extent,
+						   sizeof(old));
 				if (disk_blocknr != 0) {
 					ret = btrfs_inc_extent_ref(trans, root,
 					         disk_blocknr, disk_num_blocks);
@@ -406,20 +408,24 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 			u64 new_num;
 			u64 old_num;
 			keep = 1;
-			WARN_ON(start & (root->blocksize - 1));
+			WARN_ON(start & (root->sectorsize - 1));
 			if (found_extent) {
 				new_num = (start - key.offset) >>
 					inode->i_blkbits;
-				old_num = btrfs_file_extent_num_blocks(extent);
+				old_num = btrfs_file_extent_num_blocks(leaf,
+								       extent);
 				*hint_block =
-					btrfs_file_extent_disk_blocknr(extent);
-				if (btrfs_file_extent_disk_blocknr(extent)) {
+					btrfs_file_extent_disk_blocknr(leaf,
+								       extent);
+				if (btrfs_file_extent_disk_blocknr(leaf,
+								   extent)) {
 					inode->i_blocks -=
 						(old_num - new_num) << 3;
 				}
-				btrfs_set_file_extent_num_blocks(extent,
+				btrfs_set_file_extent_num_blocks(leaf,
+								 extent,
 								 new_num);
-				btrfs_mark_buffer_dirty(path->nodes[0]);
+				btrfs_mark_buffer_dirty(leaf);
 			} else {
 				WARN_ON(1);
 			}
@@ -431,13 +437,17 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 			u64 extent_num_blocks = 0;
 			if (found_extent) {
 				disk_blocknr =
-				      btrfs_file_extent_disk_blocknr(extent);
+				      btrfs_file_extent_disk_blocknr(leaf,
+								     extent);
 				disk_num_blocks =
-				      btrfs_file_extent_disk_num_blocks(extent);
+				      btrfs_file_extent_disk_num_blocks(leaf,
+									extent);
 				extent_num_blocks =
-				      btrfs_file_extent_num_blocks(extent);
+				      btrfs_file_extent_num_blocks(leaf,
+								   extent);
 				*hint_block =
-					btrfs_file_extent_disk_blocknr(extent);
+					btrfs_file_extent_disk_blocknr(leaf,
+								       extent);
 			}
 			ret = btrfs_del_item(trans, root, path);
 			/* TODO update progress marker and return */
@@ -464,42 +474,37 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 			struct btrfs_key ins;
 			ins.objectid = inode->i_ino;
 			ins.offset = end;
-			ins.flags = 0;
 			btrfs_set_key_type(&ins, BTRFS_EXTENT_DATA_KEY);
 			btrfs_release_path(root, path);
 			ret = btrfs_insert_empty_item(trans, root, path, &ins,
 						      sizeof(*extent));
 
+			leaf = path->nodes[0];
 			if (ret) {
-				btrfs_print_leaf(root, btrfs_buffer_leaf(path->nodes[0]));
-				printk("got %d on inserting %Lu %u %Lu start %Lu end %Lu found %Lu %Lu keep was %d\n", ret , ins.objectid, ins.flags, ins.offset, start, end, key.offset, extent_end, keep);
+				btrfs_print_leaf(root, leaf);
+				printk("got %d on inserting %Lu %u %Lu start %Lu end %Lu found %Lu %Lu keep was %d\n", ret , ins.objectid, ins.type, ins.offset, start, end, key.offset, extent_end, keep);
 			}
 			BUG_ON(ret);
-			extent = btrfs_item_ptr(
-				    btrfs_buffer_leaf(path->nodes[0]),
-				    path->slots[0],
-				    struct btrfs_file_extent_item);
-			btrfs_set_file_extent_disk_blocknr(extent,
-				    btrfs_file_extent_disk_blocknr(&old));
-			btrfs_set_file_extent_disk_num_blocks(extent,
-				    btrfs_file_extent_disk_num_blocks(&old));
-
-			btrfs_set_file_extent_offset(extent,
-				    btrfs_file_extent_offset(&old) +
+			extent = btrfs_item_ptr(leaf, path->slots[0],
+						struct btrfs_file_extent_item);
+			write_extent_buffer(leaf, &old,
+					    (unsigned long)extent, sizeof(old));
+
+			btrfs_set_file_extent_offset(leaf, extent,
+				    le64_to_cpu(old.offset) +
 				    ((end - key.offset) >> inode->i_blkbits));
-			WARN_ON(btrfs_file_extent_num_blocks(&old) <
+			WARN_ON(le64_to_cpu(old.num_blocks) <
 				(extent_end - end) >> inode->i_blkbits);
-			btrfs_set_file_extent_num_blocks(extent,
+			btrfs_set_file_extent_num_blocks(leaf, extent,
 				    (extent_end - end) >> inode->i_blkbits);
 
-			btrfs_set_file_extent_type(extent,
+			btrfs_set_file_extent_type(leaf, extent,
 						   BTRFS_FILE_EXTENT_REG);
-			btrfs_set_file_extent_generation(extent,
-				    btrfs_file_extent_generation(&old));
 			btrfs_mark_buffer_dirty(path->nodes[0]);
-			if (btrfs_file_extent_disk_blocknr(&old) != 0) {
+			if (le64_to_cpu(old.disk_blocknr) != 0) {
 				inode->i_blocks +=
-				      btrfs_file_extent_num_blocks(extent) << 3;
+				      btrfs_file_extent_num_blocks(leaf,
+								   extent) << 3;
 			}
 			ret = 0;
 			goto out;
@@ -529,8 +534,8 @@ static int prepare_pages(struct btrfs_root *root,
 	u64 num_blocks;
 	u64 start_pos;
 
-	start_pos = pos & ~((u64)root->blocksize - 1);
-	num_blocks = (write_bytes + pos - start_pos + root->blocksize - 1) >>
+	start_pos = pos & ~((u64)root->sectorsize - 1);
+	num_blocks = (write_bytes + pos - start_pos + root->sectorsize - 1) >>
 			inode->i_blkbits;
 
 	memset(pages, 0, num_pages * sizeof(struct page *));

commit b3cfa35a493866cffd00952438b6c77364d42d3c
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Sep 17 11:25:58 2007 -0400

    Btrfs: factor page private preparations into a helper
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 698eaea612f1..4cc459c943ec 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -543,12 +543,7 @@ static int prepare_pages(struct btrfs_root *root,
 		}
 		cancel_dirty_page(pages[i], PAGE_CACHE_SIZE);
 		wait_on_page_writeback(pages[i]);
-		if (!PagePrivate(pages[i])) {
-			SetPagePrivate(pages[i]);
-			set_page_private(pages[i], 1);
-			WARN_ON(!pages[i]->mapping->a_ops->invalidatepage);
-			page_cache_get(pages[i]);
-		}
+		set_page_extent_mapped(pages[i]);
 		WARN_ON(!PageLocked(pages[i]));
 	}
 	return 0;

commit d3c2fdcf7b79079f60ac64e61d886964d4647910
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Sep 17 10:58:06 2007 -0400

    Btrfs: Use balance_dirty_pages_nr on btree blocks
    
    btrfs_btree_balance_dirty is changed to pass the number of pages dirtied
    for more accurate dirty throttling.  This lets the VM make better decisions
    about when to force some writeback.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 26826a543355..698eaea612f1 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -655,7 +655,7 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 		num_written += write_bytes;
 
 		balance_dirty_pages_ratelimited_nr(inode->i_mapping, num_pages);
-		btrfs_btree_balance_dirty(root);
+		btrfs_btree_balance_dirty(root, 1);
 		cond_resched();
 	}
 	mutex_unlock(&inode->i_mutex);

commit 6af858b24c0c4e91ce0b40b5dd5fdfc1fe2dda70
Author: Yan <yanzheng@21cn.com>
Date:   Fri Sep 14 10:23:29 2007 -0400

    Btrfs: Fix off by one error in dirty_and_release_pages
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index ca7e5d4474e0..26826a543355 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -236,7 +236,7 @@ static int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 					   end_pos - start_pos, p, 0);
 		BUG_ON(err);
 		em->start = start_pos;
-		em->end = end_pos;
+		em->end = end_pos - 1;
 		em->block_start = EXTENT_MAP_INLINE;
 		em->block_end = EXTENT_MAP_INLINE;
 		add_extent_mapping(em_tree, em);

commit 34287aa3605d52ef5a4b6e5e2cca98667166508b
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Sep 14 10:22:47 2007 -0400

    Btrfs: use unlocked_ioctl
    
    No reason to grab the BKL before calling into the btrfs ioctl code.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index c26ca54ed4de..ca7e5d4474e0 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -732,10 +732,10 @@ struct file_operations btrfs_file_operations = {
 	.write		= btrfs_file_write,
 	.mmap		= btrfs_file_mmap,
 	.open		= generic_file_open,
-	.ioctl		= btrfs_ioctl,
 	.fsync		= btrfs_sync_file,
+	.unlocked_ioctl	= btrfs_ioctl,
 #ifdef CONFIG_COMPAT
-	.compat_ioctl	= btrfs_compat_ioctl,
+	.compat_ioctl	= btrfs_ioctl,
 #endif
 };
 

commit 9433063bcb4b3c0c9e5ef3c24a811b63084dedb4
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Sep 10 20:02:22 2007 -0400

    [PATCH] btrfs: fix printk format warning
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 77f1950b0aa0..c26ca54ed4de 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -174,7 +174,7 @@ static int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 	hint_block = 0;
 
 	if ((end_of_last_block & 4095) == 0) {
-		printk("strange end of last %Lu %lu %Lu\n", start_pos, write_bytes, end_of_last_block);
+		printk("strange end of last %Lu %zu %Lu\n", start_pos, write_bytes, end_of_last_block);
 	}
 	set_extent_uptodate(em_tree, start_pos, end_of_last_block, GFP_NOFS);
 

commit 011410bd859a481a335d7db1fb559542c5663fd5
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Sep 10 19:58:36 2007 -0400

    Btrfs: Add more synchronization before creating a snapshot
    
    File data checksums are only done during writepage, so we have to make sure
    all pages are written when the snapshot is taken.  This also adds some
    locking so that new writes don't race in and add new dirty pages.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index acef8cb7fb79..77f1950b0aa0 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -160,6 +160,7 @@ static int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 	num_blocks = (write_bytes + pos - start_pos + root->blocksize - 1) >>
 			inode->i_blkbits;
 
+	down_read(&BTRFS_I(inode)->root->snap_sem);
 	end_of_last_block = start_pos + (num_blocks << inode->i_blkbits) - 1;
 	lock_extent(em_tree, start_pos, end_of_last_block, GFP_NOFS);
 	mutex_lock(&root->fs_info->fs_mutex);
@@ -250,6 +251,7 @@ static int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 	mutex_unlock(&root->fs_info->fs_mutex);
 	unlock_extent(em_tree, start_pos, end_of_last_block, GFP_NOFS);
 	free_extent_map(em);
+	up_read(&BTRFS_I(inode)->root->snap_sem);
 	return err;
 }
 

commit 8e21528f87854314792aaef4d279bc9e5a9be997
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Aug 30 12:16:51 2007 -0400

    Btrfs: remove extra drop_extent_cache call
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 07b121d4bd93..acef8cb7fb79 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -406,8 +406,6 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 			keep = 1;
 			WARN_ON(start & (root->blocksize - 1));
 			if (found_extent) {
-				btrfs_drop_extent_cache(inode, key.offset,
-							start - 1 );
 				new_num = (start - key.offset) >>
 					inode->i_blkbits;
 				old_num = btrfs_file_extent_num_blocks(extent);

commit 2bf5a725a3b82efeaf7b292c085e69a9388a89ea
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Aug 30 11:54:02 2007 -0400

    Btrfs: fsx delalloc fixes
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index d3d39e4a2797..07b121d4bd93 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -186,8 +186,16 @@ static int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 		u64 mask = root->blocksize - 1;
 		last_pos_in_file = (isize + mask) & ~mask;
 		hole_size = (start_pos - last_pos_in_file + mask) & ~mask;
-		hole_size >>= inode->i_blkbits;
+
 		if (last_pos_in_file < start_pos) {
+			err = btrfs_drop_extents(trans, root, inode,
+						 last_pos_in_file,
+						 last_pos_in_file + hole_size,
+						 &hint_block);
+			if (err)
+				goto failed;
+
+			hole_size >>= inode->i_blkbits;
 			err = btrfs_insert_file_extent(trans, root,
 						       inode->i_ino,
 						       last_pos_in_file,
@@ -217,13 +225,11 @@ static int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 		struct page *p = pages[0];
 		/* step one, delete the existing extents in this range */
 		/* FIXME blocksize != pagesize */
-		if (start_pos < inode->i_size) {
-			err = btrfs_drop_extents(trans, root, inode, start_pos,
-				 (pos + write_bytes + root->blocksize -1) &
-				 ~((u64)root->blocksize - 1), &hint_block);
-			if (err)
-				goto failed;
-		}
+		err = btrfs_drop_extents(trans, root, inode, start_pos,
+			 (pos + write_bytes + root->blocksize -1) &
+			 ~((u64)root->blocksize - 1), &hint_block);
+		if (err)
+			goto failed;
 
 		err = insert_inline_extent(trans, root, inode, start_pos,
 					   end_pos - start_pos, p, 0);
@@ -400,6 +406,8 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 			keep = 1;
 			WARN_ON(start & (root->blocksize - 1));
 			if (found_extent) {
+				btrfs_drop_extent_cache(inode, key.offset,
+							start - 1 );
 				new_num = (start - key.offset) >>
 					inode->i_blkbits;
 				old_num = btrfs_file_extent_num_blocks(extent);
@@ -464,7 +472,7 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 
 			if (ret) {
 				btrfs_print_leaf(root, btrfs_buffer_leaf(path->nodes[0]));
-				printk("got %d on inserting %Lu %u %Lu start %Lu end %Lu found %Lu %Lu\n", ret , ins.objectid, ins.flags, ins.offset, start, end, key.offset, extent_end);
+				printk("got %d on inserting %Lu %u %Lu start %Lu end %Lu found %Lu %Lu keep was %d\n", ret , ins.objectid, ins.flags, ins.offset, start, end, key.offset, extent_end, keep);
 			}
 			BUG_ON(ret);
 			extent = btrfs_item_ptr(

commit b888db2bd7b67f190b32934e6a86181f262ac3ec
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Aug 27 16:49:44 2007 -0400

    Btrfs: Add delayed allocation to the extent based page tree code
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 71a481894ab6..d3d39e4a2797 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -143,7 +143,6 @@ static int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 	struct inode *inode = file->f_path.dentry->d_inode;
 	struct extent_map *em;
 	struct extent_map_tree *em_tree = &BTRFS_I(inode)->extent_tree;
-	struct btrfs_key ins;
 	u64 hint_block;
 	u64 num_blocks;
 	u64 start_pos;
@@ -162,6 +161,7 @@ static int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 			inode->i_blkbits;
 
 	end_of_last_block = start_pos + (num_blocks << inode->i_blkbits) - 1;
+	lock_extent(em_tree, start_pos, end_of_last_block, GFP_NOFS);
 	mutex_lock(&root->fs_info->fs_mutex);
 	trans = btrfs_start_transaction(root, 1);
 	if (!trans) {
@@ -179,16 +179,6 @@ static int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 
 	/* FIXME...EIEIO, ENOSPC and more */
 
-	/* step one, delete the existing extents in this range */
-	/* FIXME blocksize != pagesize */
-	if (start_pos < inode->i_size) {
-		err = btrfs_drop_extents(trans, root, inode,
-			 start_pos, (pos + write_bytes + root->blocksize -1) &
-			 ~((u64)root->blocksize - 1), &hint_block);
-		if (err)
-			goto failed;
-	}
-
 	/* insert any holes we need to create */
 	if (inode->i_size < start_pos) {
 		u64 last_pos_in_file;
@@ -213,29 +203,28 @@ static int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 	 */
 	if (isize >= PAGE_CACHE_SIZE || pos + write_bytes < inode->i_size ||
 	    pos + write_bytes - start_pos > BTRFS_MAX_INLINE_DATA_SIZE(root)) {
-		err = btrfs_alloc_extent(trans, root, inode->i_ino,
-					 num_blocks, 0, hint_block, (u64)-1,
-					 &ins, 1);
-		BUG_ON(err);
-		err = btrfs_insert_file_extent(trans, root, inode->i_ino,
-				       start_pos, ins.objectid, ins.offset,
-				       ins.offset);
-		BUG_ON(err);
-		em->start = start_pos;
-		em->end = end_of_last_block;
-		em->block_start = ins.objectid << inode->i_blkbits;
-		em->block_end = em->block_start +
-			(ins.offset << inode->i_blkbits) - 1;
-		set_extent_dirty(em_tree, start_pos, end_of_last_block,
-				 GFP_NOFS);
-		err = add_extent_mapping(em_tree, em);
+		u64 last_end;
 		for (i = 0; i < num_pages; i++) {
 			struct page *p = pages[i];
 			SetPageUptodate(p);
-			__set_page_dirty_nobuffers(p);
+			set_page_dirty(p);
 		}
+		last_end = pages[num_pages -1]->index << PAGE_CACHE_SHIFT;
+		last_end += PAGE_CACHE_SIZE - 1;
+		set_extent_delalloc(em_tree, start_pos, end_of_last_block,
+				 GFP_NOFS);
 	} else {
 		struct page *p = pages[0];
+		/* step one, delete the existing extents in this range */
+		/* FIXME blocksize != pagesize */
+		if (start_pos < inode->i_size) {
+			err = btrfs_drop_extents(trans, root, inode, start_pos,
+				 (pos + write_bytes + root->blocksize -1) &
+				 ~((u64)root->blocksize - 1), &hint_block);
+			if (err)
+				goto failed;
+		}
+
 		err = insert_inline_extent(trans, root, inode, start_pos,
 					   end_pos - start_pos, p, 0);
 		BUG_ON(err);
@@ -253,6 +242,7 @@ static int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 	err = btrfs_end_transaction(trans, root);
 out_unlock:
 	mutex_unlock(&root->fs_info->fs_mutex);
+	unlock_extent(em_tree, start_pos, end_of_last_block, GFP_NOFS);
 	free_extent_map(em);
 	return err;
 }
@@ -531,7 +521,7 @@ static int prepare_pages(struct btrfs_root *root,
 	u64 num_blocks;
 	u64 start_pos;
 
-	start_pos = pos & ~((u64)PAGE_CACHE_SIZE - 1);
+	start_pos = pos & ~((u64)root->blocksize - 1);
 	num_blocks = (write_bytes + pos - start_pos + root->blocksize - 1) >>
 			inode->i_blkbits;
 
@@ -548,8 +538,10 @@ static int prepare_pages(struct btrfs_root *root,
 		if (!PagePrivate(pages[i])) {
 			SetPagePrivate(pages[i]);
 			set_page_private(pages[i], 1);
+			WARN_ON(!pages[i]->mapping->a_ops->invalidatepage);
 			page_cache_get(pages[i]);
 		}
+		WARN_ON(!PageLocked(pages[i]));
 	}
 	return 0;
 }

commit a52d9a8033c454cd9b4697cfafb467509fc1693f
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Aug 27 16:49:44 2007 -0400

    Btrfs: Extent based page cache code.  This uses an rbtree of extents and tests
    instead of buffer heads.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 6933ab11a5cd..71a481894ab6 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -81,14 +81,14 @@ static void btrfs_drop_pages(struct page **pages, size_t num_pages)
 	}
 }
 
-static int insert_inline_extent(struct btrfs_root *root, struct inode *inode,
+static int insert_inline_extent(struct btrfs_trans_handle *trans,
+				struct btrfs_root *root, struct inode *inode,
 				u64 offset, ssize_t size,
-				struct buffer_head *bh)
+				struct page *page, size_t page_offset)
 {
 	struct btrfs_key key;
 	struct btrfs_path *path;
 	char *ptr, *kaddr;
-	struct btrfs_trans_handle *trans;
 	struct btrfs_file_extent_item *ei;
 	u32 datasize;
 	int err = 0;
@@ -98,8 +98,6 @@ static int insert_inline_extent(struct btrfs_root *root, struct inode *inode,
 	if (!path)
 		return -ENOMEM;
 
-	mutex_lock(&root->fs_info->fs_mutex);
-	trans = btrfs_start_transaction(root, 1);
 	btrfs_set_trans_block_group(trans, inode);
 
 	key.objectid = inode->i_ino;
@@ -122,18 +120,13 @@ static int insert_inline_extent(struct btrfs_root *root, struct inode *inode,
 				   BTRFS_FILE_EXTENT_INLINE);
 	ptr = btrfs_file_extent_inline_start(ei);
 
-	kaddr = kmap_atomic(bh->b_page, KM_USER0);
+	kaddr = kmap_atomic(page, KM_USER0);
 	btrfs_memcpy(root, path->nodes[0]->b_data,
-		     ptr, kaddr + bh_offset(bh),
-		     size);
+		     ptr, kaddr + page_offset, size);
 	kunmap_atomic(kaddr, KM_USER0);
 	btrfs_mark_buffer_dirty(path->nodes[0]);
 fail:
 	btrfs_free_path(path);
-	ret = btrfs_end_transaction(trans, root);
-	if (ret && !err)
-		err = ret;
-	mutex_unlock(&root->fs_info->fs_mutex);
 	return err;
 }
 
@@ -145,45 +138,143 @@ static int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 				   loff_t pos,
 				   size_t write_bytes)
 {
-	int i;
-	int offset;
 	int err = 0;
-	int ret;
-	int this_write;
+	int i;
 	struct inode *inode = file->f_path.dentry->d_inode;
-	struct buffer_head *bh;
+	struct extent_map *em;
+	struct extent_map_tree *em_tree = &BTRFS_I(inode)->extent_tree;
+	struct btrfs_key ins;
+	u64 hint_block;
+	u64 num_blocks;
+	u64 start_pos;
+	u64 end_of_last_block;
+	u64 end_pos = pos + write_bytes;
+	loff_t isize = i_size_read(inode);
 
-	for (i = 0; i < num_pages; i++) {
-		offset = pos & (PAGE_CACHE_SIZE -1);
-		this_write = min((size_t)PAGE_CACHE_SIZE - offset, write_bytes);
+	em = alloc_extent_map(GFP_NOFS);
+	if (!em)
+		return -ENOMEM;
 
-		/* FIXME, one block at a time */
-		bh = page_buffers(pages[i]);
+	em->bdev = inode->i_sb->s_bdev;
 
-		if (buffer_mapped(bh) && bh->b_blocknr == 0) {
-			ret = insert_inline_extent(root, inode,
-					pages[i]->index << PAGE_CACHE_SHIFT,
-					offset + this_write, bh);
-			if (ret) {
-				err = ret;
-				goto failed;
-			}
-		}
+	start_pos = pos & ~((u64)root->blocksize - 1);
+	num_blocks = (write_bytes + pos - start_pos + root->blocksize - 1) >>
+			inode->i_blkbits;
 
-		ret = btrfs_commit_write(file, pages[i], offset,
-					 offset + this_write);
-		pos += this_write;
-		if (ret) {
-			err = ret;
+	end_of_last_block = start_pos + (num_blocks << inode->i_blkbits) - 1;
+	mutex_lock(&root->fs_info->fs_mutex);
+	trans = btrfs_start_transaction(root, 1);
+	if (!trans) {
+		err = -ENOMEM;
+		goto out_unlock;
+	}
+	btrfs_set_trans_block_group(trans, inode);
+	inode->i_blocks += num_blocks << 3;
+	hint_block = 0;
+
+	if ((end_of_last_block & 4095) == 0) {
+		printk("strange end of last %Lu %lu %Lu\n", start_pos, write_bytes, end_of_last_block);
+	}
+	set_extent_uptodate(em_tree, start_pos, end_of_last_block, GFP_NOFS);
+
+	/* FIXME...EIEIO, ENOSPC and more */
+
+	/* step one, delete the existing extents in this range */
+	/* FIXME blocksize != pagesize */
+	if (start_pos < inode->i_size) {
+		err = btrfs_drop_extents(trans, root, inode,
+			 start_pos, (pos + write_bytes + root->blocksize -1) &
+			 ~((u64)root->blocksize - 1), &hint_block);
+		if (err)
+			goto failed;
+	}
+
+	/* insert any holes we need to create */
+	if (inode->i_size < start_pos) {
+		u64 last_pos_in_file;
+		u64 hole_size;
+		u64 mask = root->blocksize - 1;
+		last_pos_in_file = (isize + mask) & ~mask;
+		hole_size = (start_pos - last_pos_in_file + mask) & ~mask;
+		hole_size >>= inode->i_blkbits;
+		if (last_pos_in_file < start_pos) {
+			err = btrfs_insert_file_extent(trans, root,
+						       inode->i_ino,
+						       last_pos_in_file,
+						       0, 0, hole_size);
+		}
+		if (err)
 			goto failed;
+	}
+
+	/*
+	 * either allocate an extent for the new bytes or setup the key
+	 * to show we are doing inline data in the extent
+	 */
+	if (isize >= PAGE_CACHE_SIZE || pos + write_bytes < inode->i_size ||
+	    pos + write_bytes - start_pos > BTRFS_MAX_INLINE_DATA_SIZE(root)) {
+		err = btrfs_alloc_extent(trans, root, inode->i_ino,
+					 num_blocks, 0, hint_block, (u64)-1,
+					 &ins, 1);
+		BUG_ON(err);
+		err = btrfs_insert_file_extent(trans, root, inode->i_ino,
+				       start_pos, ins.objectid, ins.offset,
+				       ins.offset);
+		BUG_ON(err);
+		em->start = start_pos;
+		em->end = end_of_last_block;
+		em->block_start = ins.objectid << inode->i_blkbits;
+		em->block_end = em->block_start +
+			(ins.offset << inode->i_blkbits) - 1;
+		set_extent_dirty(em_tree, start_pos, end_of_last_block,
+				 GFP_NOFS);
+		err = add_extent_mapping(em_tree, em);
+		for (i = 0; i < num_pages; i++) {
+			struct page *p = pages[i];
+			SetPageUptodate(p);
+			__set_page_dirty_nobuffers(p);
 		}
-		WARN_ON(this_write > write_bytes);
-		write_bytes -= this_write;
+	} else {
+		struct page *p = pages[0];
+		err = insert_inline_extent(trans, root, inode, start_pos,
+					   end_pos - start_pos, p, 0);
+		BUG_ON(err);
+		em->start = start_pos;
+		em->end = end_pos;
+		em->block_start = EXTENT_MAP_INLINE;
+		em->block_end = EXTENT_MAP_INLINE;
+		add_extent_mapping(em_tree, em);
+	}
+	if (end_pos > isize) {
+		i_size_write(inode, end_pos);
+		btrfs_update_inode(trans, root, inode);
 	}
 failed:
+	err = btrfs_end_transaction(trans, root);
+out_unlock:
+	mutex_unlock(&root->fs_info->fs_mutex);
+	free_extent_map(em);
 	return err;
 }
 
+int btrfs_drop_extent_cache(struct inode *inode, u64 start, u64 end)
+{
+	struct extent_map *em;
+	struct extent_map_tree *em_tree = &BTRFS_I(inode)->extent_tree;
+
+	while(1) {
+		em = lookup_extent_mapping(em_tree, start, end);
+		if (!em)
+			break;
+		remove_extent_mapping(em_tree, em);
+		/* once for us */
+		free_extent_map(em);
+		/* once for the tree*/
+		free_extent_map(em);
+	}
+	return 0;
+}
+
 /*
  * this is very complex, but the basic idea is to drop all extents
  * in the range start - end.  hint_block is filled in with a block number
@@ -213,6 +304,8 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 	int found_inline;
 	int recow;
 
+	btrfs_drop_extent_cache(inode, start, end - 1);
+
 	path = btrfs_alloc_path();
 	if (!path)
 		return -ENOMEM;
@@ -434,18 +527,9 @@ static int prepare_pages(struct btrfs_root *root,
 	int i;
 	unsigned long index = pos >> PAGE_CACHE_SHIFT;
 	struct inode *inode = file->f_path.dentry->d_inode;
-	int offset;
 	int err = 0;
-	int this_write;
-	struct buffer_head *bh;
-	struct buffer_head *head;
-	loff_t isize = i_size_read(inode);
-	struct btrfs_trans_handle *trans;
-	u64 hint_block;
 	u64 num_blocks;
-	u64 alloc_extent_start;
 	u64 start_pos;
-	struct btrfs_key ins;
 
 	start_pos = pos & ~((u64)PAGE_CACHE_SIZE - 1);
 	num_blocks = (write_bytes + pos - start_pos + root->blocksize - 1) >>
@@ -457,119 +541,17 @@ static int prepare_pages(struct btrfs_root *root,
 		pages[i] = grab_cache_page(inode->i_mapping, index + i);
 		if (!pages[i]) {
 			err = -ENOMEM;
-			goto failed_release;
+			BUG_ON(1);
 		}
 		cancel_dirty_page(pages[i], PAGE_CACHE_SIZE);
 		wait_on_page_writeback(pages[i]);
-	}
-
-	mutex_lock(&root->fs_info->fs_mutex);
-	trans = btrfs_start_transaction(root, 1);
-	if (!trans) {
-		err = -ENOMEM;
-		mutex_unlock(&root->fs_info->fs_mutex);
-		goto out_unlock;
-	}
-	btrfs_set_trans_block_group(trans, inode);
-	/* FIXME blocksize != 4096 */
-	inode->i_blocks += num_blocks << 3;
-	hint_block = 0;
-
-	/* FIXME...EIEIO, ENOSPC and more */
-
-	/* step one, delete the existing extents in this range */
-	/* FIXME blocksize != pagesize */
-	if (start_pos < inode->i_size) {
-		err = btrfs_drop_extents(trans, root, inode,
-			 start_pos, (pos + write_bytes + root->blocksize -1) &
-			 ~((u64)root->blocksize - 1), &hint_block);
-		if (err)
-			goto failed_release;
-	}
-
-	/* insert any holes we need to create */
-	if (inode->i_size < start_pos) {
-		u64 last_pos_in_file;
-		u64 hole_size;
-		u64 mask = root->blocksize - 1;
-		last_pos_in_file = (isize + mask) & ~mask;
-		hole_size = (start_pos - last_pos_in_file + mask) & ~mask;
-		hole_size >>= inode->i_blkbits;
-		if (last_pos_in_file < start_pos) {
-			err = btrfs_insert_file_extent(trans, root,
-						       inode->i_ino,
-						       last_pos_in_file,
-						       0, 0, hole_size);
-		}
-		if (err)
-			goto failed_release;
-	}
-
-	/*
-	 * either allocate an extent for the new bytes or setup the key
-	 * to show we are doing inline data in the extent
-	 */
-	if (isize >= PAGE_CACHE_SIZE || pos + write_bytes < inode->i_size ||
-	    pos + write_bytes - start_pos > BTRFS_MAX_INLINE_DATA_SIZE(root)) {
-		err = btrfs_alloc_extent(trans, root, inode->i_ino,
-					 num_blocks, 0, hint_block, (u64)-1,
-					 &ins, 1);
-		if (err)
-			goto failed_truncate;
-		err = btrfs_insert_file_extent(trans, root, inode->i_ino,
-				       start_pos, ins.objectid, ins.offset,
-				       ins.offset);
-		if (err)
-			goto failed_truncate;
-	} else {
-		ins.offset = 0;
-		ins.objectid = 0;
-	}
-	BUG_ON(err);
-	alloc_extent_start = ins.objectid;
-	err = btrfs_end_transaction(trans, root);
-	mutex_unlock(&root->fs_info->fs_mutex);
-
-	for (i = 0; i < num_pages; i++) {
-		offset = pos & (PAGE_CACHE_SIZE -1);
-		this_write = min((size_t)PAGE_CACHE_SIZE - offset, write_bytes);
-		if (!page_has_buffers(pages[i])) {
-			create_empty_buffers(pages[i],
-					     root->fs_info->sb->s_blocksize,
-					     (1 << BH_Uptodate));
+		if (!PagePrivate(pages[i])) {
+			SetPagePrivate(pages[i]);
+			set_page_private(pages[i], 1);
+			page_cache_get(pages[i]);
 		}
-		head = page_buffers(pages[i]);
-		bh = head;
-		do {
-			err = btrfs_map_bh_to_logical(root, bh,
-						      alloc_extent_start);
-			BUG_ON(err);
-			if (err)
-				goto failed_truncate;
-			bh = bh->b_this_page;
-			if (alloc_extent_start)
-				alloc_extent_start++;
-		} while (bh != head);
-		pos += this_write;
-		WARN_ON(this_write > write_bytes);
-		write_bytes -= this_write;
 	}
 	return 0;
-
-failed_release:
-	btrfs_drop_pages(pages, num_pages);
-	return err;
-
-failed_truncate:
-	btrfs_drop_pages(pages, num_pages);
-	if (pos > isize)
-		vmtruncate(inode, isize);
-	return err;
-
-out_unlock:
-	mutex_unlock(&root->fs_info->fs_mutex);
-	goto failed_release;
-
 }
 
 static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
@@ -685,7 +667,6 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 		page_cache_release(pinned[1]);
 	*ppos = pos;
 	current->backing_dev_info = NULL;
-	mark_inode_dirty(inode);
 	return num_written ? num_written : err;
 }
 
@@ -714,8 +695,8 @@ static int btrfs_sync_file(struct file *file,
 	mutex_unlock(&root->fs_info->trans_mutex);
 
 	/*
- 	 * ok we haven't committed the transaction yet, lets do a commit
- 	 */
+	 * ok we haven't committed the transaction yet, lets do a commit
+	 */
 	trans = btrfs_start_transaction(root, 1);
 	if (!trans) {
 		ret = -ENOMEM;

commit 15ee9bc7ed171248d1405df5854da5fa91bfdc39
Author: Josef Bacik <jwhiter@redhat.com>
Date:   Fri Aug 10 16:22:09 2007 -0400

    Btrfs: delay commits during fsync to allow more writers
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 00b118a2db69..6933ab11a5cd 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -694,22 +694,36 @@ static int btrfs_sync_file(struct file *file,
 {
 	struct inode *inode = dentry->d_inode;
 	struct btrfs_root *root = BTRFS_I(inode)->root;
-	int ret;
+	int ret = 0;
 	struct btrfs_trans_handle *trans;
 
 	/*
-	 * FIXME, use inode generation number to check if we can skip the
-	 * commit
+	 * check the transaction that last modified this inode
+	 * and see if its already been committed
 	 */
 	mutex_lock(&root->fs_info->fs_mutex);
+	if (!BTRFS_I(inode)->last_trans)
+		goto out;
+	mutex_lock(&root->fs_info->trans_mutex);
+	if (BTRFS_I(inode)->last_trans <=
+	    root->fs_info->last_trans_committed) {
+		BTRFS_I(inode)->last_trans = 0;
+		mutex_unlock(&root->fs_info->trans_mutex);
+		goto out;
+	}
+	mutex_unlock(&root->fs_info->trans_mutex);
+
+	/*
+ 	 * ok we haven't committed the transaction yet, lets do a commit
+ 	 */
 	trans = btrfs_start_transaction(root, 1);
 	if (!trans) {
 		ret = -ENOMEM;
 		goto out;
 	}
 	ret = btrfs_commit_transaction(trans, root);
-	mutex_unlock(&root->fs_info->fs_mutex);
 out:
+	mutex_unlock(&root->fs_info->fs_mutex);
 	return ret > 0 ? EIO : ret;
 }
 

commit 6702ed490ca0bb44e17131818a5a18b773957c5a
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Aug 7 16:15:09 2007 -0400

    Btrfs: Add run time btree defrag, and an ioctl to force btree defrag
    
    This adds two types of btree defrag, a run time form that tries to
    defrag recently allocated blocks in the btree when they are still in ram,
    and an ioctl that forces defrag of all btree blocks.
    
    File data blocks are not defragged yet, but this can make a huge difference
    in sequential btree reads.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 1fe38fe84150..00b118a2db69 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -512,7 +512,7 @@ static int prepare_pages(struct btrfs_root *root,
 	if (isize >= PAGE_CACHE_SIZE || pos + write_bytes < inode->i_size ||
 	    pos + write_bytes - start_pos > BTRFS_MAX_INLINE_DATA_SIZE(root)) {
 		err = btrfs_alloc_extent(trans, root, inode->i_ino,
-					 num_blocks, hint_block, (u64)-1,
+					 num_blocks, 0, hint_block, (u64)-1,
 					 &ins, 1);
 		if (err)
 			goto failed_truncate;

commit 92fee66d49b812ea57eae53c5baaf8942ac9af06
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Jul 25 12:31:35 2007 -0400

    Btrfs: deal with api changes in 2.6.23-rc1
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index f327525f541d..1fe38fe84150 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -30,6 +30,7 @@
 #include <linux/writeback.h>
 #include <linux/statfs.h>
 #include <linux/compat.h>
+#include <linux/version.h>
 #include "ctree.h"
 #include "disk-io.h"
 #include "transaction.h"
@@ -713,8 +714,12 @@ static int btrfs_sync_file(struct file *file,
 }
 
 static struct vm_operations_struct btrfs_file_vm_ops = {
-	.nopage		= filemap_nopage,
-	.populate	= filemap_populate,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,23)
+	.nopage         = filemap_nopage,
+	.populate       = filemap_populate,
+#else
+	.fault		= filemap_fault,
+#endif
 	.page_mkwrite	= btrfs_page_mkwrite,
 };
 

commit ec6b910fb330f29e169c9f502c3ac209515af8d1
Author: Zach Brown <zach.brown@oracle.com>
Date:   Wed Jul 11 10:00:37 2007 -0400

    Btrfs: trivial include fixups
    
    Almost none of the files including module.h need to do so,
    remove them.
    
    Include sched.h in extent-tree.c to silence a warning about cond_resched()
    being undeclared.
    
    Signed-off-by: Zach Brown <zach.brown@oracle.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 2456cc3e1cfd..f327525f541d 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -16,7 +16,6 @@
  * Boston, MA 021110-1307, USA.
  */
 
-#include <linux/module.h>
 #include <linux/buffer_head.h>
 #include <linux/fs.h>
 #include <linux/pagemap.h>

commit ccd467d60e81b48cdbecae93532b66bcdedca91d
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Jun 28 15:57:36 2007 -0400

    Btrfs: crash recovery fixes
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index fef7ba1e707f..2456cc3e1cfd 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -127,7 +127,7 @@ static int insert_inline_extent(struct btrfs_root *root, struct inode *inode,
 		     ptr, kaddr + bh_offset(bh),
 		     size);
 	kunmap_atomic(kaddr, KM_USER0);
-	mark_buffer_dirty(path->nodes[0]);
+	btrfs_mark_buffer_dirty(path->nodes[0]);
 fail:
 	btrfs_free_path(path);
 	ret = btrfs_end_transaction(trans, root);
@@ -211,11 +211,13 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 	int found_type;
 	int found_extent;
 	int found_inline;
+	int recow;
 
 	path = btrfs_alloc_path();
 	if (!path)
 		return -ENOMEM;
 	while(1) {
+		recow = 0;
 		btrfs_release_path(root, path);
 		ret = btrfs_lookup_file_extent(trans, root, path, inode->i_ino,
 					       search_start, -1);
@@ -244,6 +246,10 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 		if (btrfs_key_type(&key) > BTRFS_EXTENT_DATA_KEY) {
 			goto out;
 		}
+		if (recow) {
+			search_start = key.offset;
+			continue;
+		}
 		if (btrfs_key_type(&key) == BTRFS_EXTENT_DATA_KEY) {
 			extent = btrfs_item_ptr(leaf, slot,
 						struct btrfs_file_extent_item);
@@ -274,6 +280,7 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 				nextret = btrfs_next_leaf(root, path);
 				if (nextret)
 					goto out;
+				recow = 1;
 			} else {
 				path->slots[0]++;
 			}
@@ -321,7 +328,7 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 				}
 				btrfs_set_file_extent_num_blocks(extent,
 								 new_num);
-				mark_buffer_dirty(path->nodes[0]);
+				btrfs_mark_buffer_dirty(path->nodes[0]);
 			} else {
 				WARN_ON(1);
 			}
@@ -452,6 +459,8 @@ static int prepare_pages(struct btrfs_root *root,
 			err = -ENOMEM;
 			goto failed_release;
 		}
+		cancel_dirty_page(pages[i], PAGE_CACHE_SIZE);
+		wait_on_page_writeback(pages[i]);
 	}
 
 	mutex_lock(&root->fs_info->fs_mutex);
@@ -522,8 +531,6 @@ static int prepare_pages(struct btrfs_root *root,
 	mutex_unlock(&root->fs_info->fs_mutex);
 
 	for (i = 0; i < num_pages; i++) {
-		cancel_dirty_page(pages[i], PAGE_CACHE_SIZE);
-		wait_on_page_writeback(pages[i]);
 		offset = pos & (PAGE_CACHE_SIZE -1);
 		this_write = min((size_t)PAGE_CACHE_SIZE - offset, write_bytes);
 		if (!page_has_buffers(pages[i])) {

commit 54aa1f4dfdacd60a19c4471220b24e581be6f774
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Jun 22 14:16:25 2007 -0400

    Btrfs: Audit callers and return codes to make sure -ENOSPC gets up the stack
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 23f029359688..fef7ba1e707f 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -81,6 +81,62 @@ static void btrfs_drop_pages(struct page **pages, size_t num_pages)
 	}
 }
 
+static int insert_inline_extent(struct btrfs_root *root, struct inode *inode,
+				u64 offset, ssize_t size,
+				struct buffer_head *bh)
+{
+	struct btrfs_key key;
+	struct btrfs_path *path;
+	char *ptr, *kaddr;
+	struct btrfs_trans_handle *trans;
+	struct btrfs_file_extent_item *ei;
+	u32 datasize;
+	int err = 0;
+	int ret;
+
+	path = btrfs_alloc_path();
+	if (!path)
+		return -ENOMEM;
+
+	mutex_lock(&root->fs_info->fs_mutex);
+	trans = btrfs_start_transaction(root, 1);
+	btrfs_set_trans_block_group(trans, inode);
+
+	key.objectid = inode->i_ino;
+	key.offset = offset;
+	key.flags = 0;
+	btrfs_set_key_type(&key, BTRFS_EXTENT_DATA_KEY);
+	BUG_ON(size >= PAGE_CACHE_SIZE);
+	datasize = btrfs_file_extent_calc_inline_size(size);
+
+	ret = btrfs_insert_empty_item(trans, root, path, &key,
+				      datasize);
+	if (ret) {
+		err = ret;
+		goto fail;
+	}
+	ei = btrfs_item_ptr(btrfs_buffer_leaf(path->nodes[0]),
+	       path->slots[0], struct btrfs_file_extent_item);
+	btrfs_set_file_extent_generation(ei, trans->transid);
+	btrfs_set_file_extent_type(ei,
+				   BTRFS_FILE_EXTENT_INLINE);
+	ptr = btrfs_file_extent_inline_start(ei);
+
+	kaddr = kmap_atomic(bh->b_page, KM_USER0);
+	btrfs_memcpy(root, path->nodes[0]->b_data,
+		     ptr, kaddr + bh_offset(bh),
+		     size);
+	kunmap_atomic(kaddr, KM_USER0);
+	mark_buffer_dirty(path->nodes[0]);
+fail:
+	btrfs_free_path(path);
+	ret = btrfs_end_transaction(trans, root);
+	if (ret && !err)
+		err = ret;
+	mutex_unlock(&root->fs_info->fs_mutex);
+	return err;
+}
+
 static int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 				   struct btrfs_root *root,
 				   struct file *file,
@@ -96,57 +152,22 @@ static int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 	int this_write;
 	struct inode *inode = file->f_path.dentry->d_inode;
 	struct buffer_head *bh;
-	struct btrfs_file_extent_item *ei;
 
 	for (i = 0; i < num_pages; i++) {
 		offset = pos & (PAGE_CACHE_SIZE -1);
 		this_write = min((size_t)PAGE_CACHE_SIZE - offset, write_bytes);
-		/* FIXME, one block at a time */
 
+		/* FIXME, one block at a time */
 		bh = page_buffers(pages[i]);
 
 		if (buffer_mapped(bh) && bh->b_blocknr == 0) {
-			struct btrfs_key key;
-			struct btrfs_path *path;
-			char *ptr, *kaddr;
-			u32 datasize;
-
-			mutex_lock(&root->fs_info->fs_mutex);
-			trans = btrfs_start_transaction(root, 1);
-			btrfs_set_trans_block_group(trans, inode);
-
-			/* create an inline extent, and copy the data in */
-			path = btrfs_alloc_path();
-			BUG_ON(!path);
-			key.objectid = inode->i_ino;
-			key.offset = pages[i]->index << PAGE_CACHE_SHIFT;
-			key.flags = 0;
-			btrfs_set_key_type(&key, BTRFS_EXTENT_DATA_KEY);
-			BUG_ON(write_bytes >= PAGE_CACHE_SIZE);
-			datasize = offset +
-				btrfs_file_extent_calc_inline_size(write_bytes);
-
-			ret = btrfs_insert_empty_item(trans, root, path, &key,
-						      datasize);
-			BUG_ON(ret);
-			ei = btrfs_item_ptr(btrfs_buffer_leaf(path->nodes[0]),
-			       path->slots[0], struct btrfs_file_extent_item);
-			btrfs_set_file_extent_generation(ei, trans->transid);
-			btrfs_set_file_extent_type(ei,
-						   BTRFS_FILE_EXTENT_INLINE);
-			ptr = btrfs_file_extent_inline_start(ei);
-
-			kaddr = kmap_atomic(bh->b_page, KM_USER0);
-			btrfs_memcpy(root, path->nodes[0]->b_data,
-				     ptr, kaddr + bh_offset(bh),
-				     offset + write_bytes);
-			kunmap_atomic(kaddr, KM_USER0);
-
-			mark_buffer_dirty(path->nodes[0]);
-			btrfs_free_path(path);
-			ret = btrfs_end_transaction(trans, root);
-			BUG_ON(ret);
-			mutex_unlock(&root->fs_info->fs_mutex);
+			ret = insert_inline_extent(root, inode,
+					pages[i]->index << PAGE_CACHE_SHIFT,
+					offset + this_write, bh);
+			if (ret) {
+				err = ret;
+				goto failed;
+			}
 		}
 
 		ret = btrfs_commit_write(file, pages[i], offset,
@@ -321,6 +342,7 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 					btrfs_file_extent_disk_blocknr(extent);
 			}
 			ret = btrfs_del_item(trans, root, path);
+			/* TODO update progress marker and return */
 			BUG_ON(ret);
 			btrfs_release_path(root, path);
 			extent = NULL;
@@ -452,7 +474,8 @@ static int prepare_pages(struct btrfs_root *root,
 		err = btrfs_drop_extents(trans, root, inode,
 			 start_pos, (pos + write_bytes + root->blocksize -1) &
 			 ~((u64)root->blocksize - 1), &hint_block);
-		BUG_ON(err);
+		if (err)
+			goto failed_release;
 	}
 
 	/* insert any holes we need to create */
@@ -469,7 +492,8 @@ static int prepare_pages(struct btrfs_root *root,
 						       last_pos_in_file,
 						       0, 0, hole_size);
 		}
-		BUG_ON(err);
+		if (err)
+			goto failed_release;
 	}
 
 	/*
@@ -481,11 +505,13 @@ static int prepare_pages(struct btrfs_root *root,
 		err = btrfs_alloc_extent(trans, root, inode->i_ino,
 					 num_blocks, hint_block, (u64)-1,
 					 &ins, 1);
-		BUG_ON(err);
+		if (err)
+			goto failed_truncate;
 		err = btrfs_insert_file_extent(trans, root, inode->i_ino,
 				       start_pos, ins.objectid, ins.offset,
 				       ins.offset);
-		BUG_ON(err);
+		if (err)
+			goto failed_truncate;
 	} else {
 		ins.offset = 0;
 		ins.objectid = 0;
@@ -618,16 +644,21 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 		ret = prepare_pages(root, file, pages, num_pages,
 				    pos, first_index, last_index,
 				    write_bytes);
-		BUG_ON(ret);
+		if (ret)
+			goto out;
 
 		ret = btrfs_copy_from_user(pos, num_pages,
 					   write_bytes, pages, buf);
-		BUG_ON(ret);
+		if (ret) {
+			btrfs_drop_pages(pages, num_pages);
+			goto out;
+		}
 
 		ret = dirty_and_release_pages(NULL, root, file, pages,
 					      num_pages, pos, write_bytes);
-		BUG_ON(ret);
 		btrfs_drop_pages(pages, num_pages);
+		if (ret)
+			goto out;
 
 		buf += write_bytes;
 		count -= write_bytes;

commit 11bd143fc8243cf48c934dc1c4479a5aacf58ce3
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Jun 22 14:16:24 2007 -0400

    Btrfs: Switch to libcrc32c to avoid problems with cryptomgr on highmem machines
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 6b455c2b3f03..23f029359688 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -607,7 +607,8 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 
 	while(count > 0) {
 		size_t offset = pos & (PAGE_CACHE_SIZE - 1);
-		size_t write_bytes = min(count, nrptrs * PAGE_CACHE_SIZE -
+		size_t write_bytes = min(count, nrptrs *
+					(size_t)PAGE_CACHE_SIZE -
 					 offset);
 		size_t num_pages = (write_bytes + PAGE_CACHE_SIZE - 1) >>
 					PAGE_CACHE_SHIFT;

commit 8c2383c3dd2cb5bb39598ce4fa97154bc591020a
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Jun 18 09:57:58 2007 -0400

    Subject: Rework btrfs_file_write to only allocate while page locks are held
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index de8d47b44e12..6b455c2b3f03 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -207,6 +207,7 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 			}
 			path->slots[0]--;
 		}
+next_slot:
 		keep = 0;
 		bookend = 0;
 		found_extent = 0;
@@ -214,39 +215,48 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 		extent = NULL;
 		leaf = btrfs_buffer_leaf(path->nodes[0]);
 		slot = path->slots[0];
+		ret = 0;
 		btrfs_disk_key_to_cpu(&key, &leaf->items[slot].key);
 		if (key.offset >= end || key.objectid != inode->i_ino) {
-			ret = 0;
 			goto out;
 		}
-		if (btrfs_key_type(&key) != BTRFS_EXTENT_DATA_KEY) {
-			ret = 0;
+		if (btrfs_key_type(&key) > BTRFS_EXTENT_DATA_KEY) {
 			goto out;
 		}
-		extent = btrfs_item_ptr(leaf, slot,
-					struct btrfs_file_extent_item);
-		found_type = btrfs_file_extent_type(extent);
-		if (found_type == BTRFS_FILE_EXTENT_REG) {
-			extent_end = key.offset +
-				(btrfs_file_extent_num_blocks(extent) <<
-				 inode->i_blkbits);
-			found_extent = 1;
-		} else if (found_type == BTRFS_FILE_EXTENT_INLINE) {
-			found_inline = 1;
-			extent_end = key.offset +
-			     btrfs_file_extent_inline_len(leaf->items + slot);
+		if (btrfs_key_type(&key) == BTRFS_EXTENT_DATA_KEY) {
+			extent = btrfs_item_ptr(leaf, slot,
+						struct btrfs_file_extent_item);
+			found_type = btrfs_file_extent_type(extent);
+			if (found_type == BTRFS_FILE_EXTENT_REG) {
+				extent_end = key.offset +
+					(btrfs_file_extent_num_blocks(extent) <<
+					 inode->i_blkbits);
+				found_extent = 1;
+			} else if (found_type == BTRFS_FILE_EXTENT_INLINE) {
+				found_inline = 1;
+				extent_end = key.offset +
+				     btrfs_file_extent_inline_len(leaf->items +
+								  slot);
+			}
+		} else {
+			extent_end = search_start;
 		}
 
 		/* we found nothing we can drop */
-		if (!found_extent && !found_inline) {
-			ret = 0;
-			goto out;
-		}
-
-		/* we found nothing inside the range */
-		if (search_start >= extent_end) {
-			ret = 0;
-			goto out;
+		if ((!found_extent && !found_inline) ||
+		    search_start >= extent_end) {
+			int nextret;
+			u32 nritems;
+			nritems = btrfs_header_nritems(
+					btrfs_buffer_header(path->nodes[0]));
+			if (slot >= nritems - 1) {
+				nextret = btrfs_next_leaf(root, path);
+				if (nextret)
+					goto out;
+			} else {
+				path->slots[0]++;
+			}
+			goto next_slot;
 		}
 
 		/* FIXME, there's only one inline extent allowed right now */
@@ -272,7 +282,6 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 			WARN_ON(found_inline);
 			bookend = 1;
 		}
-
 		/* truncate existing extent */
 		if (start > key.offset) {
 			u64 new_num;
@@ -337,10 +346,14 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 			ins.offset = end;
 			ins.flags = 0;
 			btrfs_set_key_type(&ins, BTRFS_EXTENT_DATA_KEY);
-
 			btrfs_release_path(root, path);
 			ret = btrfs_insert_empty_item(trans, root, path, &ins,
 						      sizeof(*extent));
+
+			if (ret) {
+				btrfs_print_leaf(root, btrfs_buffer_leaf(path->nodes[0]));
+				printk("got %d on inserting %Lu %u %Lu start %Lu end %Lu found %Lu %Lu\n", ret , ins.objectid, ins.flags, ins.offset, start, end, key.offset, extent_end);
+			}
 			BUG_ON(ret);
 			extent = btrfs_item_ptr(
 				    btrfs_buffer_leaf(path->nodes[0]),
@@ -387,8 +400,7 @@ static int prepare_pages(struct btrfs_root *root,
 			 loff_t pos,
 			 unsigned long first_index,
 			 unsigned long last_index,
-			 size_t write_bytes,
-			 u64 alloc_extent_start)
+			 size_t write_bytes)
 {
 	int i;
 	unsigned long index = pos >> PAGE_CACHE_SHIFT;
@@ -399,6 +411,16 @@ static int prepare_pages(struct btrfs_root *root,
 	struct buffer_head *bh;
 	struct buffer_head *head;
 	loff_t isize = i_size_read(inode);
+	struct btrfs_trans_handle *trans;
+	u64 hint_block;
+	u64 num_blocks;
+	u64 alloc_extent_start;
+	u64 start_pos;
+	struct btrfs_key ins;
+
+	start_pos = pos & ~((u64)PAGE_CACHE_SIZE - 1);
+	num_blocks = (write_bytes + pos - start_pos + root->blocksize - 1) >>
+			inode->i_blkbits;
 
 	memset(pages, 0, num_pages * sizeof(struct page *));
 
@@ -408,6 +430,72 @@ static int prepare_pages(struct btrfs_root *root,
 			err = -ENOMEM;
 			goto failed_release;
 		}
+	}
+
+	mutex_lock(&root->fs_info->fs_mutex);
+	trans = btrfs_start_transaction(root, 1);
+	if (!trans) {
+		err = -ENOMEM;
+		mutex_unlock(&root->fs_info->fs_mutex);
+		goto out_unlock;
+	}
+	btrfs_set_trans_block_group(trans, inode);
+	/* FIXME blocksize != 4096 */
+	inode->i_blocks += num_blocks << 3;
+	hint_block = 0;
+
+	/* FIXME...EIEIO, ENOSPC and more */
+
+	/* step one, delete the existing extents in this range */
+	/* FIXME blocksize != pagesize */
+	if (start_pos < inode->i_size) {
+		err = btrfs_drop_extents(trans, root, inode,
+			 start_pos, (pos + write_bytes + root->blocksize -1) &
+			 ~((u64)root->blocksize - 1), &hint_block);
+		BUG_ON(err);
+	}
+
+	/* insert any holes we need to create */
+	if (inode->i_size < start_pos) {
+		u64 last_pos_in_file;
+		u64 hole_size;
+		u64 mask = root->blocksize - 1;
+		last_pos_in_file = (isize + mask) & ~mask;
+		hole_size = (start_pos - last_pos_in_file + mask) & ~mask;
+		hole_size >>= inode->i_blkbits;
+		if (last_pos_in_file < start_pos) {
+			err = btrfs_insert_file_extent(trans, root,
+						       inode->i_ino,
+						       last_pos_in_file,
+						       0, 0, hole_size);
+		}
+		BUG_ON(err);
+	}
+
+	/*
+	 * either allocate an extent for the new bytes or setup the key
+	 * to show we are doing inline data in the extent
+	 */
+	if (isize >= PAGE_CACHE_SIZE || pos + write_bytes < inode->i_size ||
+	    pos + write_bytes - start_pos > BTRFS_MAX_INLINE_DATA_SIZE(root)) {
+		err = btrfs_alloc_extent(trans, root, inode->i_ino,
+					 num_blocks, hint_block, (u64)-1,
+					 &ins, 1);
+		BUG_ON(err);
+		err = btrfs_insert_file_extent(trans, root, inode->i_ino,
+				       start_pos, ins.objectid, ins.offset,
+				       ins.offset);
+		BUG_ON(err);
+	} else {
+		ins.offset = 0;
+		ins.objectid = 0;
+	}
+	BUG_ON(err);
+	alloc_extent_start = ins.objectid;
+	err = btrfs_end_transaction(trans, root);
+	mutex_unlock(&root->fs_info->fs_mutex);
+
+	for (i = 0; i < num_pages; i++) {
 		cancel_dirty_page(pages[i], PAGE_CACHE_SIZE);
 		wait_on_page_writeback(pages[i]);
 		offset = pos & (PAGE_CACHE_SIZE -1);
@@ -444,6 +532,11 @@ static int prepare_pages(struct btrfs_root *root,
 	if (pos > isize)
 		vmtruncate(inode, isize);
 	return err;
+
+out_unlock:
+	mutex_unlock(&root->fs_info->fs_mutex);
+	goto failed_release;
+
 }
 
 static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
@@ -455,16 +548,14 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 	int ret = 0;
 	struct inode *inode = file->f_path.dentry->d_inode;
 	struct btrfs_root *root = BTRFS_I(inode)->root;
-	struct page *pages[8];
+	struct page **pages = NULL;
+	int nrptrs;
 	struct page *pinned[2];
 	unsigned long first_index;
 	unsigned long last_index;
-	u64 start_pos;
-	u64 num_blocks;
-	u64 alloc_extent_start;
-	u64 hint_block;
-	struct btrfs_trans_handle *trans;
-	struct btrfs_key ins;
+
+	nrptrs = min((count + PAGE_CACHE_SIZE - 1) / PAGE_CACHE_SIZE,
+		     PAGE_CACHE_SIZE / (sizeof(struct page *)));
 	pinned[0] = NULL;
 	pinned[1] = NULL;
 	if (file->f_flags & O_DIRECT)
@@ -482,9 +573,7 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 		goto out;
 	file_update_time(file);
 
-	start_pos = pos & ~((u64)PAGE_CACHE_SIZE - 1);
-	num_blocks = (count + pos - start_pos + root->blocksize - 1) >>
-			inode->i_blkbits;
+	pages = kmalloc(nrptrs * sizeof(struct page *), GFP_KERNEL);
 
 	mutex_lock(&inode->i_mutex);
 	first_index = pos >> PAGE_CACHE_SHIFT;
@@ -516,87 +605,20 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 		}
 	}
 
-	mutex_lock(&root->fs_info->fs_mutex);
-	trans = btrfs_start_transaction(root, 1);
-	if (!trans) {
-		err = -ENOMEM;
-		mutex_unlock(&root->fs_info->fs_mutex);
-		goto out_unlock;
-	}
-	btrfs_set_trans_block_group(trans, inode);
-	/* FIXME blocksize != 4096 */
-	inode->i_blocks += num_blocks << 3;
-	hint_block = 0;
-
-	/* FIXME...EIEIO, ENOSPC and more */
-
-	/* step one, delete the existing extents in this range */
-	if (start_pos < inode->i_size) {
-		/* FIXME blocksize != pagesize */
-		ret = btrfs_drop_extents(trans, root, inode,
-					 start_pos,
-					 (pos + count + root->blocksize -1) &
-					 ~((u64)root->blocksize - 1),
-					 &hint_block);
-		BUG_ON(ret);
-	}
-
-	/* insert any holes we need to create */
-	if (inode->i_size < start_pos) {
-		u64 last_pos_in_file;
-		u64 hole_size;
-		u64 mask = root->blocksize - 1;
-		last_pos_in_file = (inode->i_size + mask) & ~mask;
-		hole_size = (start_pos - last_pos_in_file + mask) & ~mask;
-		hole_size >>= inode->i_blkbits;
-		if (last_pos_in_file < start_pos) {
-			ret = btrfs_insert_file_extent(trans, root,
-						       inode->i_ino,
-						       last_pos_in_file,
-						       0, 0, hole_size);
-		}
-		BUG_ON(ret);
-	}
-
-	/*
-	 * either allocate an extent for the new bytes or setup the key
-	 * to show we are doing inline data in the extent
-	 */
-	if (inode->i_size >= PAGE_CACHE_SIZE || pos + count < inode->i_size ||
-	    pos + count - start_pos > BTRFS_MAX_INLINE_DATA_SIZE(root)) {
-		ret = btrfs_alloc_extent(trans, root, inode->i_ino,
-					 num_blocks, hint_block, (u64)-1,
-					 &ins, 1);
-		BUG_ON(ret);
-		ret = btrfs_insert_file_extent(trans, root, inode->i_ino,
-				       start_pos, ins.objectid, ins.offset,
-				       ins.offset);
-		BUG_ON(ret);
-	} else {
-		ins.offset = 0;
-		ins.objectid = 0;
-	}
-	BUG_ON(ret);
-	alloc_extent_start = ins.objectid;
-	ret = btrfs_end_transaction(trans, root);
-	mutex_unlock(&root->fs_info->fs_mutex);
-
 	while(count > 0) {
 		size_t offset = pos & (PAGE_CACHE_SIZE - 1);
-		size_t write_bytes = min(count,
-					 (size_t)PAGE_CACHE_SIZE - offset);
+		size_t write_bytes = min(count, nrptrs * PAGE_CACHE_SIZE -
+					 offset);
 		size_t num_pages = (write_bytes + PAGE_CACHE_SIZE - 1) >>
 					PAGE_CACHE_SHIFT;
 
+		WARN_ON(num_pages > nrptrs);
 		memset(pages, 0, sizeof(pages));
 		ret = prepare_pages(root, file, pages, num_pages,
 				    pos, first_index, last_index,
-				    write_bytes, alloc_extent_start);
+				    write_bytes);
 		BUG_ON(ret);
 
-		/* FIXME blocks != pagesize */
-		if (alloc_extent_start)
-			alloc_extent_start += num_pages;
 		ret = btrfs_copy_from_user(pos, num_pages,
 					   write_bytes, pages, buf);
 		BUG_ON(ret);
@@ -611,13 +633,13 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 		pos += write_bytes;
 		num_written += write_bytes;
 
-		balance_dirty_pages_ratelimited(inode->i_mapping);
+		balance_dirty_pages_ratelimited_nr(inode->i_mapping, num_pages);
 		btrfs_btree_balance_dirty(root);
 		cond_resched();
 	}
-out_unlock:
 	mutex_unlock(&inode->i_mutex);
 out:
+	kfree(pages);
 	if (pinned[0])
 		page_cache_release(pinned[0]);
 	if (pinned[1])

commit 9ebefb180bad4914a31c4e1748ba187a30e1e990
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Jun 15 13:50:00 2007 -0400

    Btrfs: patch queue: page_mkwrite
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 83836fb3d4e9..de8d47b44e12 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -103,10 +103,6 @@ static int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 		this_write = min((size_t)PAGE_CACHE_SIZE - offset, write_bytes);
 		/* FIXME, one block at a time */
 
-		mutex_lock(&root->fs_info->fs_mutex);
-		trans = btrfs_start_transaction(root, 1);
-		btrfs_set_trans_block_group(trans, inode);
-
 		bh = page_buffers(pages[i]);
 
 		if (buffer_mapped(bh) && bh->b_blocknr == 0) {
@@ -115,6 +111,10 @@ static int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 			char *ptr, *kaddr;
 			u32 datasize;
 
+			mutex_lock(&root->fs_info->fs_mutex);
+			trans = btrfs_start_transaction(root, 1);
+			btrfs_set_trans_block_group(trans, inode);
+
 			/* create an inline extent, and copy the data in */
 			path = btrfs_alloc_path();
 			BUG_ON(!path);
@@ -135,24 +135,19 @@ static int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 			btrfs_set_file_extent_type(ei,
 						   BTRFS_FILE_EXTENT_INLINE);
 			ptr = btrfs_file_extent_inline_start(ei);
+
 			kaddr = kmap_atomic(bh->b_page, KM_USER0);
 			btrfs_memcpy(root, path->nodes[0]->b_data,
 				     ptr, kaddr + bh_offset(bh),
 				     offset + write_bytes);
 			kunmap_atomic(kaddr, KM_USER0);
+
 			mark_buffer_dirty(path->nodes[0]);
 			btrfs_free_path(path);
-		} else if (buffer_mapped(bh)) {
-			/* csum the file data */
-			btrfs_csum_file_block(trans, root, inode->i_ino,
-				      pages[i]->index << PAGE_CACHE_SHIFT,
-				      kmap(pages[i]), PAGE_CACHE_SIZE);
-			kunmap(pages[i]);
+			ret = btrfs_end_transaction(trans, root);
+			BUG_ON(ret);
+			mutex_unlock(&root->fs_info->fs_mutex);
 		}
-		SetPageChecked(pages[i]);
-		ret = btrfs_end_transaction(trans, root);
-		BUG_ON(ret);
-		mutex_unlock(&root->fs_info->fs_mutex);
 
 		ret = btrfs_commit_write(file, pages[i], offset,
 					 offset + this_write);
@@ -503,7 +498,7 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 	if ((pos & (PAGE_CACHE_SIZE - 1))) {
 		pinned[0] = grab_cache_page(inode->i_mapping, first_index);
 		if (!PageUptodate(pinned[0])) {
-			ret = mpage_readpage(pinned[0], btrfs_get_block);
+			ret = btrfs_readpage(NULL, pinned[0]);
 			BUG_ON(ret);
 			wait_on_page_locked(pinned[0]);
 		} else {
@@ -513,7 +508,7 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 	if ((pos + count) & (PAGE_CACHE_SIZE - 1)) {
 		pinned[1] = grab_cache_page(inode->i_mapping, last_index);
 		if (!PageUptodate(pinned[1])) {
-			ret = mpage_readpage(pinned[1], btrfs_get_block);
+			ret = btrfs_readpage(NULL, pinned[1]);
 			BUG_ON(ret);
 			wait_on_page_locked(pinned[1]);
 		} else {
@@ -633,138 +628,6 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 	return num_written ? num_written : err;
 }
 
-/*
- * FIXME, do this by stuffing the csum we want in the info hanging off
- * page->private.  For now, verify file csums on read
- */
-static int btrfs_read_actor(read_descriptor_t *desc, struct page *page,
-			unsigned long offset, unsigned long size)
-{
-	char *kaddr;
-	unsigned long left, count = desc->count;
-	struct inode *inode = page->mapping->host;
-
-	if (size > count)
-		size = count;
-
-	if (!PageChecked(page)) {
-		/* FIXME, do it per block */
-		struct btrfs_root *root = BTRFS_I(inode)->root;
-		int ret;
-		struct buffer_head *bh;
-
-		if (page_has_buffers(page)) {
-			bh = page_buffers(page);
-			if (!buffer_mapped(bh)) {
-				SetPageChecked(page);
-				goto checked;
-			}
-		}
-
-		ret = btrfs_csum_verify_file_block(root,
-				  page->mapping->host->i_ino,
-				  page->index << PAGE_CACHE_SHIFT,
-				  kmap(page), PAGE_CACHE_SIZE);
-		if (ret) {
-			if (ret != -ENOENT) {
-				printk("failed to verify ino %lu page %lu ret %d\n",
-				       page->mapping->host->i_ino,
-				       page->index, ret);
-				memset(page_address(page), 1, PAGE_CACHE_SIZE);
-				flush_dcache_page(page);
-			}
-		}
-		SetPageChecked(page);
-		kunmap(page);
-	}
-checked:
-	/*
-	 * Faults on the destination of a read are common, so do it before
-	 * taking the kmap.
-	 */
-	if (!fault_in_pages_writeable(desc->arg.buf, size)) {
-		kaddr = kmap_atomic(page, KM_USER0);
-		left = __copy_to_user_inatomic(desc->arg.buf,
-						kaddr + offset, size);
-		kunmap_atomic(kaddr, KM_USER0);
-		if (left == 0)
-			goto success;
-	}
-
-	/* Do it the slow way */
-	kaddr = kmap(page);
-	left = __copy_to_user(desc->arg.buf, kaddr + offset, size);
-	kunmap(page);
-
-	if (left) {
-		size -= left;
-		desc->error = -EFAULT;
-	}
-success:
-	desc->count = count - size;
-	desc->written += size;
-	desc->arg.buf += size;
-	return size;
-}
-
-/**
- * btrfs_file_aio_read - filesystem read routine, with a mod to csum verify
- * @iocb:	kernel I/O control block
- * @iov:	io vector request
- * @nr_segs:	number of segments in the iovec
- * @pos:	current file position
- */
-static ssize_t btrfs_file_aio_read(struct kiocb *iocb, const struct iovec *iov,
-				   unsigned long nr_segs, loff_t pos)
-{
-	struct file *filp = iocb->ki_filp;
-	ssize_t retval;
-	unsigned long seg;
-	size_t count;
-	loff_t *ppos = &iocb->ki_pos;
-
-	count = 0;
-	for (seg = 0; seg < nr_segs; seg++) {
-		const struct iovec *iv = &iov[seg];
-
-		/*
-		 * If any segment has a negative length, or the cumulative
-		 * length ever wraps negative then return -EINVAL.
-		 */
-		count += iv->iov_len;
-		if (unlikely((ssize_t)(count|iv->iov_len) < 0))
-			return -EINVAL;
-		if (access_ok(VERIFY_WRITE, iv->iov_base, iv->iov_len))
-			continue;
-		if (seg == 0)
-			return -EFAULT;
-		nr_segs = seg;
-		count -= iv->iov_len;	/* This segment is no good */
-		break;
-	}
-	retval = 0;
-	if (count) {
-		for (seg = 0; seg < nr_segs; seg++) {
-			read_descriptor_t desc;
-
-			desc.written = 0;
-			desc.arg.buf = iov[seg].iov_base;
-			desc.count = iov[seg].iov_len;
-			if (desc.count == 0)
-				continue;
-			desc.error = 0;
-			do_generic_file_read(filp, ppos, &desc,
-					     btrfs_read_actor);
-			retval += desc.written;
-			if (desc.error) {
-				retval = retval ?: desc.error;
-				break;
-			}
-		}
-	}
-	return retval;
-}
-
 static int btrfs_sync_file(struct file *file,
 			   struct dentry *dentry, int datasync)
 {
@@ -789,12 +652,25 @@ static int btrfs_sync_file(struct file *file,
 	return ret > 0 ? EIO : ret;
 }
 
+static struct vm_operations_struct btrfs_file_vm_ops = {
+	.nopage		= filemap_nopage,
+	.populate	= filemap_populate,
+	.page_mkwrite	= btrfs_page_mkwrite,
+};
+
+static int btrfs_file_mmap(struct file	*filp, struct vm_area_struct *vma)
+{
+	vma->vm_ops = &btrfs_file_vm_ops;
+	file_accessed(filp);
+	return 0;
+}
+
 struct file_operations btrfs_file_operations = {
 	.llseek		= generic_file_llseek,
 	.read		= do_sync_read,
-	.aio_read       = btrfs_file_aio_read,
+	.aio_read       = generic_file_aio_read,
 	.write		= btrfs_file_write,
-	.mmap		= generic_file_mmap,
+	.mmap		= btrfs_file_mmap,
 	.open		= generic_file_open,
 	.ioctl		= btrfs_ioctl,
 	.fsync		= btrfs_sync_file,

commit 340887809d92a4dd6c71caa97d658cef32c098c3
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Jun 12 11:36:58 2007 -0400

    Btrfs: i386 fixes from axboe
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 566e526b60ff..83836fb3d4e9 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -112,7 +112,7 @@ static int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 		if (buffer_mapped(bh) && bh->b_blocknr == 0) {
 			struct btrfs_key key;
 			struct btrfs_path *path;
-			char *ptr;
+			char *ptr, *kaddr;
 			u32 datasize;
 
 			/* create an inline extent, and copy the data in */
@@ -135,8 +135,11 @@ static int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 			btrfs_set_file_extent_type(ei,
 						   BTRFS_FILE_EXTENT_INLINE);
 			ptr = btrfs_file_extent_inline_start(ei);
+			kaddr = kmap_atomic(bh->b_page, KM_USER0);
 			btrfs_memcpy(root, path->nodes[0]->b_data,
-				     ptr, bh->b_data, offset + write_bytes);
+				     ptr, kaddr + bh_offset(bh),
+				     offset + write_bytes);
+			kunmap_atomic(kaddr, KM_USER0);
 			mark_buffer_dirty(path->nodes[0]);
 			btrfs_free_path(path);
 		} else if (buffer_mapped(bh)) {

commit 6cbd55707802b98843f953d1ae6d8f5bcd9a76c0
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Jun 12 09:07:21 2007 -0400

    Btrfs: add GPLv2
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 0325dc038592..566e526b60ff 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -1,3 +1,21 @@
+/*
+ * Copyright (C) 2007 Oracle.  All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public
+ * License v2 as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public
+ * License along with this program; if not, write to the
+ * Free Software Foundation, Inc., 59 Temple Place - Suite 330,
+ * Boston, MA 021110-1307, USA.
+ */
+
 #include <linux/module.h>
 #include <linux/buffer_head.h>
 #include <linux/fs.h>

commit 84f54cfa78c81991e087309a9b379f25f1ffdb10
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Jun 12 07:43:08 2007 -0400

    Btrfs: 64 bit div fixes
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 7f8e3035d99e..0325dc038592 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -82,7 +82,7 @@ static int dirty_and_release_pages(struct btrfs_trans_handle *trans,
 
 	for (i = 0; i < num_pages; i++) {
 		offset = pos & (PAGE_CACHE_SIZE -1);
-		this_write = min(PAGE_CACHE_SIZE - offset, write_bytes);
+		this_write = min((size_t)PAGE_CACHE_SIZE - offset, write_bytes);
 		/* FIXME, one block at a time */
 
 		mutex_lock(&root->fs_info->fs_mutex);
@@ -395,7 +395,7 @@ static int prepare_pages(struct btrfs_root *root,
 		cancel_dirty_page(pages[i], PAGE_CACHE_SIZE);
 		wait_on_page_writeback(pages[i]);
 		offset = pos & (PAGE_CACHE_SIZE -1);
-		this_write = min(PAGE_CACHE_SIZE - offset, write_bytes);
+		this_write = min((size_t)PAGE_CACHE_SIZE - offset, write_bytes);
 		if (!page_has_buffers(pages[i])) {
 			create_empty_buffers(pages[i],
 					     root->fs_info->sb->s_blocksize,
@@ -567,7 +567,8 @@ static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
 
 	while(count > 0) {
 		size_t offset = pos & (PAGE_CACHE_SIZE - 1);
-		size_t write_bytes = min(count, PAGE_CACHE_SIZE - offset);
+		size_t write_bytes = min(count,
+					 (size_t)PAGE_CACHE_SIZE - offset);
 		size_t num_pages = (write_bytes + PAGE_CACHE_SIZE - 1) >>
 					PAGE_CACHE_SHIFT;
 

commit 39279cc3d2704cfbf9c35dcb5bdd392159ae4625
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Jun 12 06:35:45 2007 -0400

    Btrfs: split up super.c
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
new file mode 100644
index 000000000000..7f8e3035d99e
--- /dev/null
+++ b/fs/btrfs/file.c
@@ -0,0 +1,783 @@
+#include <linux/module.h>
+#include <linux/buffer_head.h>
+#include <linux/fs.h>
+#include <linux/pagemap.h>
+#include <linux/highmem.h>
+#include <linux/time.h>
+#include <linux/init.h>
+#include <linux/string.h>
+#include <linux/smp_lock.h>
+#include <linux/backing-dev.h>
+#include <linux/mpage.h>
+#include <linux/swap.h>
+#include <linux/writeback.h>
+#include <linux/statfs.h>
+#include <linux/compat.h>
+#include "ctree.h"
+#include "disk-io.h"
+#include "transaction.h"
+#include "btrfs_inode.h"
+#include "ioctl.h"
+#include "print-tree.h"
+
+
+static int btrfs_copy_from_user(loff_t pos, int num_pages, int write_bytes,
+				struct page **prepared_pages,
+				const char __user * buf)
+{
+	long page_fault = 0;
+	int i;
+	int offset = pos & (PAGE_CACHE_SIZE - 1);
+
+	for (i = 0; i < num_pages && write_bytes > 0; i++, offset = 0) {
+		size_t count = min_t(size_t,
+				     PAGE_CACHE_SIZE - offset, write_bytes);
+		struct page *page = prepared_pages[i];
+		fault_in_pages_readable(buf, count);
+
+		/* Copy data from userspace to the current page */
+		kmap(page);
+		page_fault = __copy_from_user(page_address(page) + offset,
+					      buf, count);
+		/* Flush processor's dcache for this page */
+		flush_dcache_page(page);
+		kunmap(page);
+		buf += count;
+		write_bytes -= count;
+
+		if (page_fault)
+			break;
+	}
+	return page_fault ? -EFAULT : 0;
+}
+
+static void btrfs_drop_pages(struct page **pages, size_t num_pages)
+{
+	size_t i;
+	for (i = 0; i < num_pages; i++) {
+		if (!pages[i])
+			break;
+		unlock_page(pages[i]);
+		mark_page_accessed(pages[i]);
+		page_cache_release(pages[i]);
+	}
+}
+
+static int dirty_and_release_pages(struct btrfs_trans_handle *trans,
+				   struct btrfs_root *root,
+				   struct file *file,
+				   struct page **pages,
+				   size_t num_pages,
+				   loff_t pos,
+				   size_t write_bytes)
+{
+	int i;
+	int offset;
+	int err = 0;
+	int ret;
+	int this_write;
+	struct inode *inode = file->f_path.dentry->d_inode;
+	struct buffer_head *bh;
+	struct btrfs_file_extent_item *ei;
+
+	for (i = 0; i < num_pages; i++) {
+		offset = pos & (PAGE_CACHE_SIZE -1);
+		this_write = min(PAGE_CACHE_SIZE - offset, write_bytes);
+		/* FIXME, one block at a time */
+
+		mutex_lock(&root->fs_info->fs_mutex);
+		trans = btrfs_start_transaction(root, 1);
+		btrfs_set_trans_block_group(trans, inode);
+
+		bh = page_buffers(pages[i]);
+
+		if (buffer_mapped(bh) && bh->b_blocknr == 0) {
+			struct btrfs_key key;
+			struct btrfs_path *path;
+			char *ptr;
+			u32 datasize;
+
+			/* create an inline extent, and copy the data in */
+			path = btrfs_alloc_path();
+			BUG_ON(!path);
+			key.objectid = inode->i_ino;
+			key.offset = pages[i]->index << PAGE_CACHE_SHIFT;
+			key.flags = 0;
+			btrfs_set_key_type(&key, BTRFS_EXTENT_DATA_KEY);
+			BUG_ON(write_bytes >= PAGE_CACHE_SIZE);
+			datasize = offset +
+				btrfs_file_extent_calc_inline_size(write_bytes);
+
+			ret = btrfs_insert_empty_item(trans, root, path, &key,
+						      datasize);
+			BUG_ON(ret);
+			ei = btrfs_item_ptr(btrfs_buffer_leaf(path->nodes[0]),
+			       path->slots[0], struct btrfs_file_extent_item);
+			btrfs_set_file_extent_generation(ei, trans->transid);
+			btrfs_set_file_extent_type(ei,
+						   BTRFS_FILE_EXTENT_INLINE);
+			ptr = btrfs_file_extent_inline_start(ei);
+			btrfs_memcpy(root, path->nodes[0]->b_data,
+				     ptr, bh->b_data, offset + write_bytes);
+			mark_buffer_dirty(path->nodes[0]);
+			btrfs_free_path(path);
+		} else if (buffer_mapped(bh)) {
+			/* csum the file data */
+			btrfs_csum_file_block(trans, root, inode->i_ino,
+				      pages[i]->index << PAGE_CACHE_SHIFT,
+				      kmap(pages[i]), PAGE_CACHE_SIZE);
+			kunmap(pages[i]);
+		}
+		SetPageChecked(pages[i]);
+		ret = btrfs_end_transaction(trans, root);
+		BUG_ON(ret);
+		mutex_unlock(&root->fs_info->fs_mutex);
+
+		ret = btrfs_commit_write(file, pages[i], offset,
+					 offset + this_write);
+		pos += this_write;
+		if (ret) {
+			err = ret;
+			goto failed;
+		}
+		WARN_ON(this_write > write_bytes);
+		write_bytes -= this_write;
+	}
+failed:
+	return err;
+}
+
+/*
+ * this is very complex, but the basic idea is to drop all extents
+ * in the range start - end.  hint_block is filled in with a block number
+ * that would be a good hint to the block allocator for this file.
+ *
+ * If an extent intersects the range but is not entirely inside the range
+ * it is either truncated or split.  Anything entirely inside the range
+ * is deleted from the tree.
+ */
+int btrfs_drop_extents(struct btrfs_trans_handle *trans,
+		       struct btrfs_root *root, struct inode *inode,
+		       u64 start, u64 end, u64 *hint_block)
+{
+	int ret;
+	struct btrfs_key key;
+	struct btrfs_leaf *leaf;
+	int slot;
+	struct btrfs_file_extent_item *extent;
+	u64 extent_end = 0;
+	int keep;
+	struct btrfs_file_extent_item old;
+	struct btrfs_path *path;
+	u64 search_start = start;
+	int bookend;
+	int found_type;
+	int found_extent;
+	int found_inline;
+
+	path = btrfs_alloc_path();
+	if (!path)
+		return -ENOMEM;
+	while(1) {
+		btrfs_release_path(root, path);
+		ret = btrfs_lookup_file_extent(trans, root, path, inode->i_ino,
+					       search_start, -1);
+		if (ret < 0)
+			goto out;
+		if (ret > 0) {
+			if (path->slots[0] == 0) {
+				ret = 0;
+				goto out;
+			}
+			path->slots[0]--;
+		}
+		keep = 0;
+		bookend = 0;
+		found_extent = 0;
+		found_inline = 0;
+		extent = NULL;
+		leaf = btrfs_buffer_leaf(path->nodes[0]);
+		slot = path->slots[0];
+		btrfs_disk_key_to_cpu(&key, &leaf->items[slot].key);
+		if (key.offset >= end || key.objectid != inode->i_ino) {
+			ret = 0;
+			goto out;
+		}
+		if (btrfs_key_type(&key) != BTRFS_EXTENT_DATA_KEY) {
+			ret = 0;
+			goto out;
+		}
+		extent = btrfs_item_ptr(leaf, slot,
+					struct btrfs_file_extent_item);
+		found_type = btrfs_file_extent_type(extent);
+		if (found_type == BTRFS_FILE_EXTENT_REG) {
+			extent_end = key.offset +
+				(btrfs_file_extent_num_blocks(extent) <<
+				 inode->i_blkbits);
+			found_extent = 1;
+		} else if (found_type == BTRFS_FILE_EXTENT_INLINE) {
+			found_inline = 1;
+			extent_end = key.offset +
+			     btrfs_file_extent_inline_len(leaf->items + slot);
+		}
+
+		/* we found nothing we can drop */
+		if (!found_extent && !found_inline) {
+			ret = 0;
+			goto out;
+		}
+
+		/* we found nothing inside the range */
+		if (search_start >= extent_end) {
+			ret = 0;
+			goto out;
+		}
+
+		/* FIXME, there's only one inline extent allowed right now */
+		if (found_inline) {
+			u64 mask = root->blocksize - 1;
+			search_start = (extent_end + mask) & ~mask;
+		} else
+			search_start = extent_end;
+
+		if (end < extent_end && end >= key.offset) {
+			if (found_extent) {
+				u64 disk_blocknr =
+					btrfs_file_extent_disk_blocknr(extent);
+				u64 disk_num_blocks =
+				      btrfs_file_extent_disk_num_blocks(extent);
+				memcpy(&old, extent, sizeof(old));
+				if (disk_blocknr != 0) {
+					ret = btrfs_inc_extent_ref(trans, root,
+					         disk_blocknr, disk_num_blocks);
+					BUG_ON(ret);
+				}
+			}
+			WARN_ON(found_inline);
+			bookend = 1;
+		}
+
+		/* truncate existing extent */
+		if (start > key.offset) {
+			u64 new_num;
+			u64 old_num;
+			keep = 1;
+			WARN_ON(start & (root->blocksize - 1));
+			if (found_extent) {
+				new_num = (start - key.offset) >>
+					inode->i_blkbits;
+				old_num = btrfs_file_extent_num_blocks(extent);
+				*hint_block =
+					btrfs_file_extent_disk_blocknr(extent);
+				if (btrfs_file_extent_disk_blocknr(extent)) {
+					inode->i_blocks -=
+						(old_num - new_num) << 3;
+				}
+				btrfs_set_file_extent_num_blocks(extent,
+								 new_num);
+				mark_buffer_dirty(path->nodes[0]);
+			} else {
+				WARN_ON(1);
+			}
+		}
+		/* delete the entire extent */
+		if (!keep) {
+			u64 disk_blocknr = 0;
+			u64 disk_num_blocks = 0;
+			u64 extent_num_blocks = 0;
+			if (found_extent) {
+				disk_blocknr =
+				      btrfs_file_extent_disk_blocknr(extent);
+				disk_num_blocks =
+				      btrfs_file_extent_disk_num_blocks(extent);
+				extent_num_blocks =
+				      btrfs_file_extent_num_blocks(extent);
+				*hint_block =
+					btrfs_file_extent_disk_blocknr(extent);
+			}
+			ret = btrfs_del_item(trans, root, path);
+			BUG_ON(ret);
+			btrfs_release_path(root, path);
+			extent = NULL;
+			if (found_extent && disk_blocknr != 0) {
+				inode->i_blocks -= extent_num_blocks << 3;
+				ret = btrfs_free_extent(trans, root,
+							disk_blocknr,
+							disk_num_blocks, 0);
+			}
+
+			BUG_ON(ret);
+			if (!bookend && search_start >= end) {
+				ret = 0;
+				goto out;
+			}
+			if (!bookend)
+				continue;
+		}
+		/* create bookend, splitting the extent in two */
+		if (bookend && found_extent) {
+			struct btrfs_key ins;
+			ins.objectid = inode->i_ino;
+			ins.offset = end;
+			ins.flags = 0;
+			btrfs_set_key_type(&ins, BTRFS_EXTENT_DATA_KEY);
+
+			btrfs_release_path(root, path);
+			ret = btrfs_insert_empty_item(trans, root, path, &ins,
+						      sizeof(*extent));
+			BUG_ON(ret);
+			extent = btrfs_item_ptr(
+				    btrfs_buffer_leaf(path->nodes[0]),
+				    path->slots[0],
+				    struct btrfs_file_extent_item);
+			btrfs_set_file_extent_disk_blocknr(extent,
+				    btrfs_file_extent_disk_blocknr(&old));
+			btrfs_set_file_extent_disk_num_blocks(extent,
+				    btrfs_file_extent_disk_num_blocks(&old));
+
+			btrfs_set_file_extent_offset(extent,
+				    btrfs_file_extent_offset(&old) +
+				    ((end - key.offset) >> inode->i_blkbits));
+			WARN_ON(btrfs_file_extent_num_blocks(&old) <
+				(extent_end - end) >> inode->i_blkbits);
+			btrfs_set_file_extent_num_blocks(extent,
+				    (extent_end - end) >> inode->i_blkbits);
+
+			btrfs_set_file_extent_type(extent,
+						   BTRFS_FILE_EXTENT_REG);
+			btrfs_set_file_extent_generation(extent,
+				    btrfs_file_extent_generation(&old));
+			btrfs_mark_buffer_dirty(path->nodes[0]);
+			if (btrfs_file_extent_disk_blocknr(&old) != 0) {
+				inode->i_blocks +=
+				      btrfs_file_extent_num_blocks(extent) << 3;
+			}
+			ret = 0;
+			goto out;
+		}
+	}
+out:
+	btrfs_free_path(path);
+	return ret;
+}
+
+/*
+ * this gets pages into the page cache and locks them down
+ */
+static int prepare_pages(struct btrfs_root *root,
+			 struct file *file,
+			 struct page **pages,
+			 size_t num_pages,
+			 loff_t pos,
+			 unsigned long first_index,
+			 unsigned long last_index,
+			 size_t write_bytes,
+			 u64 alloc_extent_start)
+{
+	int i;
+	unsigned long index = pos >> PAGE_CACHE_SHIFT;
+	struct inode *inode = file->f_path.dentry->d_inode;
+	int offset;
+	int err = 0;
+	int this_write;
+	struct buffer_head *bh;
+	struct buffer_head *head;
+	loff_t isize = i_size_read(inode);
+
+	memset(pages, 0, num_pages * sizeof(struct page *));
+
+	for (i = 0; i < num_pages; i++) {
+		pages[i] = grab_cache_page(inode->i_mapping, index + i);
+		if (!pages[i]) {
+			err = -ENOMEM;
+			goto failed_release;
+		}
+		cancel_dirty_page(pages[i], PAGE_CACHE_SIZE);
+		wait_on_page_writeback(pages[i]);
+		offset = pos & (PAGE_CACHE_SIZE -1);
+		this_write = min(PAGE_CACHE_SIZE - offset, write_bytes);
+		if (!page_has_buffers(pages[i])) {
+			create_empty_buffers(pages[i],
+					     root->fs_info->sb->s_blocksize,
+					     (1 << BH_Uptodate));
+		}
+		head = page_buffers(pages[i]);
+		bh = head;
+		do {
+			err = btrfs_map_bh_to_logical(root, bh,
+						      alloc_extent_start);
+			BUG_ON(err);
+			if (err)
+				goto failed_truncate;
+			bh = bh->b_this_page;
+			if (alloc_extent_start)
+				alloc_extent_start++;
+		} while (bh != head);
+		pos += this_write;
+		WARN_ON(this_write > write_bytes);
+		write_bytes -= this_write;
+	}
+	return 0;
+
+failed_release:
+	btrfs_drop_pages(pages, num_pages);
+	return err;
+
+failed_truncate:
+	btrfs_drop_pages(pages, num_pages);
+	if (pos > isize)
+		vmtruncate(inode, isize);
+	return err;
+}
+
+static ssize_t btrfs_file_write(struct file *file, const char __user *buf,
+				size_t count, loff_t *ppos)
+{
+	loff_t pos;
+	size_t num_written = 0;
+	int err = 0;
+	int ret = 0;
+	struct inode *inode = file->f_path.dentry->d_inode;
+	struct btrfs_root *root = BTRFS_I(inode)->root;
+	struct page *pages[8];
+	struct page *pinned[2];
+	unsigned long first_index;
+	unsigned long last_index;
+	u64 start_pos;
+	u64 num_blocks;
+	u64 alloc_extent_start;
+	u64 hint_block;
+	struct btrfs_trans_handle *trans;
+	struct btrfs_key ins;
+	pinned[0] = NULL;
+	pinned[1] = NULL;
+	if (file->f_flags & O_DIRECT)
+		return -EINVAL;
+	pos = *ppos;
+	vfs_check_frozen(inode->i_sb, SB_FREEZE_WRITE);
+	current->backing_dev_info = inode->i_mapping->backing_dev_info;
+	err = generic_write_checks(file, &pos, &count, S_ISBLK(inode->i_mode));
+	if (err)
+		goto out;
+	if (count == 0)
+		goto out;
+	err = remove_suid(file->f_path.dentry);
+	if (err)
+		goto out;
+	file_update_time(file);
+
+	start_pos = pos & ~((u64)PAGE_CACHE_SIZE - 1);
+	num_blocks = (count + pos - start_pos + root->blocksize - 1) >>
+			inode->i_blkbits;
+
+	mutex_lock(&inode->i_mutex);
+	first_index = pos >> PAGE_CACHE_SHIFT;
+	last_index = (pos + count) >> PAGE_CACHE_SHIFT;
+
+	/*
+	 * there are lots of better ways to do this, but this code
+	 * makes sure the first and last page in the file range are
+	 * up to date and ready for cow
+	 */
+	if ((pos & (PAGE_CACHE_SIZE - 1))) {
+		pinned[0] = grab_cache_page(inode->i_mapping, first_index);
+		if (!PageUptodate(pinned[0])) {
+			ret = mpage_readpage(pinned[0], btrfs_get_block);
+			BUG_ON(ret);
+			wait_on_page_locked(pinned[0]);
+		} else {
+			unlock_page(pinned[0]);
+		}
+	}
+	if ((pos + count) & (PAGE_CACHE_SIZE - 1)) {
+		pinned[1] = grab_cache_page(inode->i_mapping, last_index);
+		if (!PageUptodate(pinned[1])) {
+			ret = mpage_readpage(pinned[1], btrfs_get_block);
+			BUG_ON(ret);
+			wait_on_page_locked(pinned[1]);
+		} else {
+			unlock_page(pinned[1]);
+		}
+	}
+
+	mutex_lock(&root->fs_info->fs_mutex);
+	trans = btrfs_start_transaction(root, 1);
+	if (!trans) {
+		err = -ENOMEM;
+		mutex_unlock(&root->fs_info->fs_mutex);
+		goto out_unlock;
+	}
+	btrfs_set_trans_block_group(trans, inode);
+	/* FIXME blocksize != 4096 */
+	inode->i_blocks += num_blocks << 3;
+	hint_block = 0;
+
+	/* FIXME...EIEIO, ENOSPC and more */
+
+	/* step one, delete the existing extents in this range */
+	if (start_pos < inode->i_size) {
+		/* FIXME blocksize != pagesize */
+		ret = btrfs_drop_extents(trans, root, inode,
+					 start_pos,
+					 (pos + count + root->blocksize -1) &
+					 ~((u64)root->blocksize - 1),
+					 &hint_block);
+		BUG_ON(ret);
+	}
+
+	/* insert any holes we need to create */
+	if (inode->i_size < start_pos) {
+		u64 last_pos_in_file;
+		u64 hole_size;
+		u64 mask = root->blocksize - 1;
+		last_pos_in_file = (inode->i_size + mask) & ~mask;
+		hole_size = (start_pos - last_pos_in_file + mask) & ~mask;
+		hole_size >>= inode->i_blkbits;
+		if (last_pos_in_file < start_pos) {
+			ret = btrfs_insert_file_extent(trans, root,
+						       inode->i_ino,
+						       last_pos_in_file,
+						       0, 0, hole_size);
+		}
+		BUG_ON(ret);
+	}
+
+	/*
+	 * either allocate an extent for the new bytes or setup the key
+	 * to show we are doing inline data in the extent
+	 */
+	if (inode->i_size >= PAGE_CACHE_SIZE || pos + count < inode->i_size ||
+	    pos + count - start_pos > BTRFS_MAX_INLINE_DATA_SIZE(root)) {
+		ret = btrfs_alloc_extent(trans, root, inode->i_ino,
+					 num_blocks, hint_block, (u64)-1,
+					 &ins, 1);
+		BUG_ON(ret);
+		ret = btrfs_insert_file_extent(trans, root, inode->i_ino,
+				       start_pos, ins.objectid, ins.offset,
+				       ins.offset);
+		BUG_ON(ret);
+	} else {
+		ins.offset = 0;
+		ins.objectid = 0;
+	}
+	BUG_ON(ret);
+	alloc_extent_start = ins.objectid;
+	ret = btrfs_end_transaction(trans, root);
+	mutex_unlock(&root->fs_info->fs_mutex);
+
+	while(count > 0) {
+		size_t offset = pos & (PAGE_CACHE_SIZE - 1);
+		size_t write_bytes = min(count, PAGE_CACHE_SIZE - offset);
+		size_t num_pages = (write_bytes + PAGE_CACHE_SIZE - 1) >>
+					PAGE_CACHE_SHIFT;
+
+		memset(pages, 0, sizeof(pages));
+		ret = prepare_pages(root, file, pages, num_pages,
+				    pos, first_index, last_index,
+				    write_bytes, alloc_extent_start);
+		BUG_ON(ret);
+
+		/* FIXME blocks != pagesize */
+		if (alloc_extent_start)
+			alloc_extent_start += num_pages;
+		ret = btrfs_copy_from_user(pos, num_pages,
+					   write_bytes, pages, buf);
+		BUG_ON(ret);
+
+		ret = dirty_and_release_pages(NULL, root, file, pages,
+					      num_pages, pos, write_bytes);
+		BUG_ON(ret);
+		btrfs_drop_pages(pages, num_pages);
+
+		buf += write_bytes;
+		count -= write_bytes;
+		pos += write_bytes;
+		num_written += write_bytes;
+
+		balance_dirty_pages_ratelimited(inode->i_mapping);
+		btrfs_btree_balance_dirty(root);
+		cond_resched();
+	}
+out_unlock:
+	mutex_unlock(&inode->i_mutex);
+out:
+	if (pinned[0])
+		page_cache_release(pinned[0]);
+	if (pinned[1])
+		page_cache_release(pinned[1]);
+	*ppos = pos;
+	current->backing_dev_info = NULL;
+	mark_inode_dirty(inode);
+	return num_written ? num_written : err;
+}
+
+/*
+ * FIXME, do this by stuffing the csum we want in the info hanging off
+ * page->private.  For now, verify file csums on read
+ */
+static int btrfs_read_actor(read_descriptor_t *desc, struct page *page,
+			unsigned long offset, unsigned long size)
+{
+	char *kaddr;
+	unsigned long left, count = desc->count;
+	struct inode *inode = page->mapping->host;
+
+	if (size > count)
+		size = count;
+
+	if (!PageChecked(page)) {
+		/* FIXME, do it per block */
+		struct btrfs_root *root = BTRFS_I(inode)->root;
+		int ret;
+		struct buffer_head *bh;
+
+		if (page_has_buffers(page)) {
+			bh = page_buffers(page);
+			if (!buffer_mapped(bh)) {
+				SetPageChecked(page);
+				goto checked;
+			}
+		}
+
+		ret = btrfs_csum_verify_file_block(root,
+				  page->mapping->host->i_ino,
+				  page->index << PAGE_CACHE_SHIFT,
+				  kmap(page), PAGE_CACHE_SIZE);
+		if (ret) {
+			if (ret != -ENOENT) {
+				printk("failed to verify ino %lu page %lu ret %d\n",
+				       page->mapping->host->i_ino,
+				       page->index, ret);
+				memset(page_address(page), 1, PAGE_CACHE_SIZE);
+				flush_dcache_page(page);
+			}
+		}
+		SetPageChecked(page);
+		kunmap(page);
+	}
+checked:
+	/*
+	 * Faults on the destination of a read are common, so do it before
+	 * taking the kmap.
+	 */
+	if (!fault_in_pages_writeable(desc->arg.buf, size)) {
+		kaddr = kmap_atomic(page, KM_USER0);
+		left = __copy_to_user_inatomic(desc->arg.buf,
+						kaddr + offset, size);
+		kunmap_atomic(kaddr, KM_USER0);
+		if (left == 0)
+			goto success;
+	}
+
+	/* Do it the slow way */
+	kaddr = kmap(page);
+	left = __copy_to_user(desc->arg.buf, kaddr + offset, size);
+	kunmap(page);
+
+	if (left) {
+		size -= left;
+		desc->error = -EFAULT;
+	}
+success:
+	desc->count = count - size;
+	desc->written += size;
+	desc->arg.buf += size;
+	return size;
+}
+
+/**
+ * btrfs_file_aio_read - filesystem read routine, with a mod to csum verify
+ * @iocb:	kernel I/O control block
+ * @iov:	io vector request
+ * @nr_segs:	number of segments in the iovec
+ * @pos:	current file position
+ */
+static ssize_t btrfs_file_aio_read(struct kiocb *iocb, const struct iovec *iov,
+				   unsigned long nr_segs, loff_t pos)
+{
+	struct file *filp = iocb->ki_filp;
+	ssize_t retval;
+	unsigned long seg;
+	size_t count;
+	loff_t *ppos = &iocb->ki_pos;
+
+	count = 0;
+	for (seg = 0; seg < nr_segs; seg++) {
+		const struct iovec *iv = &iov[seg];
+
+		/*
+		 * If any segment has a negative length, or the cumulative
+		 * length ever wraps negative then return -EINVAL.
+		 */
+		count += iv->iov_len;
+		if (unlikely((ssize_t)(count|iv->iov_len) < 0))
+			return -EINVAL;
+		if (access_ok(VERIFY_WRITE, iv->iov_base, iv->iov_len))
+			continue;
+		if (seg == 0)
+			return -EFAULT;
+		nr_segs = seg;
+		count -= iv->iov_len;	/* This segment is no good */
+		break;
+	}
+	retval = 0;
+	if (count) {
+		for (seg = 0; seg < nr_segs; seg++) {
+			read_descriptor_t desc;
+
+			desc.written = 0;
+			desc.arg.buf = iov[seg].iov_base;
+			desc.count = iov[seg].iov_len;
+			if (desc.count == 0)
+				continue;
+			desc.error = 0;
+			do_generic_file_read(filp, ppos, &desc,
+					     btrfs_read_actor);
+			retval += desc.written;
+			if (desc.error) {
+				retval = retval ?: desc.error;
+				break;
+			}
+		}
+	}
+	return retval;
+}
+
+static int btrfs_sync_file(struct file *file,
+			   struct dentry *dentry, int datasync)
+{
+	struct inode *inode = dentry->d_inode;
+	struct btrfs_root *root = BTRFS_I(inode)->root;
+	int ret;
+	struct btrfs_trans_handle *trans;
+
+	/*
+	 * FIXME, use inode generation number to check if we can skip the
+	 * commit
+	 */
+	mutex_lock(&root->fs_info->fs_mutex);
+	trans = btrfs_start_transaction(root, 1);
+	if (!trans) {
+		ret = -ENOMEM;
+		goto out;
+	}
+	ret = btrfs_commit_transaction(trans, root);
+	mutex_unlock(&root->fs_info->fs_mutex);
+out:
+	return ret > 0 ? EIO : ret;
+}
+
+struct file_operations btrfs_file_operations = {
+	.llseek		= generic_file_llseek,
+	.read		= do_sync_read,
+	.aio_read       = btrfs_file_aio_read,
+	.write		= btrfs_file_write,
+	.mmap		= generic_file_mmap,
+	.open		= generic_file_open,
+	.ioctl		= btrfs_ioctl,
+	.fsync		= btrfs_sync_file,
+#ifdef CONFIG_COMPAT
+	.compat_ioctl	= btrfs_compat_ioctl,
+#endif
+};
+
