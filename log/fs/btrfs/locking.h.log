commit 2992df73268f78ec9281692b9b44ae92f3933b54
Author: Nikolay Borisov <nborisov@suse.com>
Date:   Thu Jan 30 14:59:44 2020 +0200

    btrfs: Implement DREW lock
    
    A (D)ouble (R)eader (W)riter (E)xclustion lock is a locking primitive
    that allows to have multiple readers or multiple writers but not
    multiple readers and writers holding it concurrently.
    
    The code is factored out from the existing open-coded locking scheme
    used to exclude pending snapshots from nocow writers and vice-versa.
    Current implementation actually favors Readers (that is snapshot
    creaters) to writers (nocow writers of the filesystem).
    
    The API provides lock/unlock/trylock for reads and writes.
    
    Formal specification for TLA+ provided by Valentin Schneider is at
    https://lore.kernel.org/linux-btrfs/2dcaf81c-f0d3-409e-cb29-733d8b3b4cc9@arm.com/
    
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/locking.h b/fs/btrfs/locking.h
index 21a285883e89..d715846c10b8 100644
--- a/fs/btrfs/locking.h
+++ b/fs/btrfs/locking.h
@@ -6,6 +6,9 @@
 #ifndef BTRFS_LOCKING_H
 #define BTRFS_LOCKING_H
 
+#include <linux/atomic.h>
+#include <linux/wait.h>
+#include <linux/percpu_counter.h>
 #include "extent_io.h"
 
 #define BTRFS_WRITE_LOCK 1
@@ -13,6 +16,8 @@
 #define BTRFS_WRITE_LOCK_BLOCKING 3
 #define BTRFS_READ_LOCK_BLOCKING 4
 
+struct btrfs_path;
+
 void btrfs_tree_lock(struct extent_buffer *eb);
 void btrfs_tree_unlock(struct extent_buffer *eb);
 
@@ -48,4 +53,19 @@ static inline void btrfs_tree_unlock_rw(struct extent_buffer *eb, int rw)
 		BUG();
 }
 
+struct btrfs_drew_lock {
+	atomic_t readers;
+	struct percpu_counter writers;
+	wait_queue_head_t pending_writers;
+	wait_queue_head_t pending_readers;
+};
+
+int btrfs_drew_lock_init(struct btrfs_drew_lock *lock);
+void btrfs_drew_lock_destroy(struct btrfs_drew_lock *lock);
+void btrfs_drew_write_lock(struct btrfs_drew_lock *lock);
+bool btrfs_drew_try_write_lock(struct btrfs_drew_lock *lock);
+void btrfs_drew_write_unlock(struct btrfs_drew_lock *lock);
+void btrfs_drew_read_lock(struct btrfs_drew_lock *lock);
+void btrfs_drew_read_unlock(struct btrfs_drew_lock *lock);
+
 #endif

commit 1f95ec012cb4a3fabfef3efd9ba0b59e14ce48ce
Author: David Sterba <dsterba@suse.com>
Date:   Tue Sep 24 19:17:17 2019 +0200

    btrfs: move btrfs_unlock_up_safe to other locking functions
    
    The function belongs to the family of locking functions, so move it
    there. The 'noinline' keyword is dropped as it's now an exported
    function that does not need it.
    
    Reviewed-by: Josef Bacik <josef@toxicpanda.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/locking.h b/fs/btrfs/locking.h
index 98c92222eaf0..21a285883e89 100644
--- a/fs/btrfs/locking.h
+++ b/fs/btrfs/locking.h
@@ -34,6 +34,7 @@ static inline void btrfs_assert_tree_locked(struct extent_buffer *eb) { }
 #endif
 
 void btrfs_set_path_blocking(struct btrfs_path *p);
+void btrfs_unlock_up_safe(struct btrfs_path *path, int level);
 
 static inline void btrfs_tree_unlock_rw(struct extent_buffer *eb, int rw)
 {

commit ed2b1d36a9d027f9b841be5bfc9d61011462d447
Author: David Sterba <dsterba@suse.com>
Date:   Tue Sep 24 19:17:17 2019 +0200

    btrfs: move btrfs_set_path_blocking to other locking functions
    
    The function belongs to the family of locking functions, so move it
    there. The 'noinline' keyword is dropped as it's now an exported
    function that does not need it.
    
    Reviewed-by: Josef Bacik <josef@toxicpanda.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/locking.h b/fs/btrfs/locking.h
index ab4020de25e7..98c92222eaf0 100644
--- a/fs/btrfs/locking.h
+++ b/fs/btrfs/locking.h
@@ -33,6 +33,8 @@ static inline void btrfs_assert_tree_locked(struct extent_buffer *eb) {
 static inline void btrfs_assert_tree_locked(struct extent_buffer *eb) { }
 #endif
 
+void btrfs_set_path_blocking(struct btrfs_path *p);
+
 static inline void btrfs_tree_unlock_rw(struct extent_buffer *eb, int rw)
 {
 	if (rw == BTRFS_WRITE_LOCK || rw == BTRFS_WRITE_LOCK_BLOCKING)

commit 31f6e769ce4145fdfaba44ad7c5b30ad21f58c11
Author: David Sterba <dsterba@suse.com>
Date:   Tue Sep 24 18:44:24 2019 +0200

    btrfs: make btrfs_assert_tree_locked static inline
    
    The function btrfs_assert_tree_locked is used outside of the locking
    code so it is exported, however we can make it static inine as it's
    fairly trivial.
    
    This is the only locking assertion used in release builds, inlining
    improves the text size by 174 bytes and reduces stack consumption in the
    callers.
    
    Reviewed-by: Josef Bacik <josef@toxicpanda.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/locking.h b/fs/btrfs/locking.h
index b775a4207ed9..ab4020de25e7 100644
--- a/fs/btrfs/locking.h
+++ b/fs/btrfs/locking.h
@@ -6,6 +6,8 @@
 #ifndef BTRFS_LOCKING_H
 #define BTRFS_LOCKING_H
 
+#include "extent_io.h"
+
 #define BTRFS_WRITE_LOCK 1
 #define BTRFS_READ_LOCK 2
 #define BTRFS_WRITE_LOCK_BLOCKING 3
@@ -19,11 +21,17 @@ void btrfs_tree_read_unlock(struct extent_buffer *eb);
 void btrfs_tree_read_unlock_blocking(struct extent_buffer *eb);
 void btrfs_set_lock_blocking_read(struct extent_buffer *eb);
 void btrfs_set_lock_blocking_write(struct extent_buffer *eb);
-void btrfs_assert_tree_locked(struct extent_buffer *eb);
 int btrfs_try_tree_read_lock(struct extent_buffer *eb);
 int btrfs_try_tree_write_lock(struct extent_buffer *eb);
 int btrfs_tree_read_lock_atomic(struct extent_buffer *eb);
 
+#ifdef CONFIG_BTRFS_DEBUG
+static inline void btrfs_assert_tree_locked(struct extent_buffer *eb) {
+	BUG_ON(!eb->write_locks);
+}
+#else
+static inline void btrfs_assert_tree_locked(struct extent_buffer *eb) { }
+#endif
 
 static inline void btrfs_tree_unlock_rw(struct extent_buffer *eb, int rw)
 {

commit 5044ed4f394cb371fb85db3e3ec0296487b7d324
Author: Nikolay Borisov <nborisov@suse.com>
Date:   Thu Jul 25 11:27:28 2019 +0300

    btrfs: Remove unused locking functions
    
    Those were split out of btrfs_clear_lock_blocking_rw by
    aa12c02778a9 ("btrfs: split btrfs_clear_lock_blocking_rw to read and write helpers")
    however at that time this function was unused due to commit
    523983401644 ("Btrfs: kill btrfs_clear_path_blocking"). Put the final
    nail in the coffin of those 2 functions.
    
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/locking.h b/fs/btrfs/locking.h
index 595014f64830..b775a4207ed9 100644
--- a/fs/btrfs/locking.h
+++ b/fs/btrfs/locking.h
@@ -19,8 +19,6 @@ void btrfs_tree_read_unlock(struct extent_buffer *eb);
 void btrfs_tree_read_unlock_blocking(struct extent_buffer *eb);
 void btrfs_set_lock_blocking_read(struct extent_buffer *eb);
 void btrfs_set_lock_blocking_write(struct extent_buffer *eb);
-void btrfs_clear_lock_blocking_read(struct extent_buffer *eb);
-void btrfs_clear_lock_blocking_write(struct extent_buffer *eb);
 void btrfs_assert_tree_locked(struct extent_buffer *eb);
 int btrfs_try_tree_read_lock(struct extent_buffer *eb);
 int btrfs_try_tree_write_lock(struct extent_buffer *eb);

commit 766ece54f4c9c29b25eabd091a2ee939feb1669e
Author: David Sterba <dsterba@suse.com>
Date:   Wed Jan 23 18:07:14 2019 +0100

    btrfs: merge btrfs_set_lock_blocking_rw with it's caller
    
    The last caller that does not have a fixed value of lock is
    btrfs_set_path_blocking, that actually does the same conditional swtich
    by the lock type so we can merge the branches together and remove the
    helper.
    
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/locking.h b/fs/btrfs/locking.h
index 84ea6ed60047..595014f64830 100644
--- a/fs/btrfs/locking.h
+++ b/fs/btrfs/locking.h
@@ -39,16 +39,4 @@ static inline void btrfs_tree_unlock_rw(struct extent_buffer *eb, int rw)
 		BUG();
 }
 
-/*
- * If we currently have a spinning reader or writer lock (indicated by the rw
- * flag) this will bump the count of blocking holders and drop the spinlock.
- */
-static inline void btrfs_set_lock_blocking_rw(struct extent_buffer *eb, int rw)
-{
-	if (rw == BTRFS_WRITE_LOCK)
-		btrfs_set_lock_blocking_write(eb);
-	else if (rw == BTRFS_READ_LOCK)
-		btrfs_set_lock_blocking_read(eb);
-}
-
 #endif

commit 8bead258206f4d4f485ad55bc1e39d23bbfe2fdd
Author: David Sterba <dsterba@suse.com>
Date:   Wed Apr 4 02:03:48 2018 +0200

    btrfs: open code now trivial btrfs_set_lock_blocking
    
    btrfs_set_lock_blocking is now only a simple wrapper around
    btrfs_set_lock_blocking_write. The name does not bring any semantic
    value that could not be inferred from the new function so there's no
    point keeping it.
    
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/locking.h b/fs/btrfs/locking.h
index 9d9f649e35cc..84ea6ed60047 100644
--- a/fs/btrfs/locking.h
+++ b/fs/btrfs/locking.h
@@ -51,9 +51,4 @@ static inline void btrfs_set_lock_blocking_rw(struct extent_buffer *eb, int rw)
 		btrfs_set_lock_blocking_read(eb);
 }
 
-static inline void btrfs_set_lock_blocking(struct extent_buffer *eb)
-{
-	btrfs_set_lock_blocking_write(eb);
-}
-
 #endif

commit 300aa896e1199bcd0dfb61aae86356714e017355
Author: David Sterba <dsterba@suse.com>
Date:   Wed Apr 4 02:00:17 2018 +0200

    btrfs: replace btrfs_set_lock_blocking_rw with appropriate helpers
    
    We can use the right helper where the lock type is a fixed parameter.
    
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/locking.h b/fs/btrfs/locking.h
index 3f81d6900c71..9d9f649e35cc 100644
--- a/fs/btrfs/locking.h
+++ b/fs/btrfs/locking.h
@@ -53,7 +53,7 @@ static inline void btrfs_set_lock_blocking_rw(struct extent_buffer *eb, int rw)
 
 static inline void btrfs_set_lock_blocking(struct extent_buffer *eb)
 {
-	btrfs_set_lock_blocking_rw(eb, BTRFS_WRITE_LOCK);
+	btrfs_set_lock_blocking_write(eb);
 }
 
 #endif

commit aa12c02778a9719283fc3c32cfe5cffb902a7685
Author: David Sterba <dsterba@suse.com>
Date:   Wed Apr 4 01:52:31 2018 +0200

    btrfs: split btrfs_clear_lock_blocking_rw to read and write helpers
    
    There are many callers that hardcode the desired lock type so we can
    avoid the switch and call them directly. Split the current function to
    two. There are no remaining users of btrfs_clear_lock_blocking_rw so
    it's removed.  The call sites will be converted in followup patches.
    
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/locking.h b/fs/btrfs/locking.h
index 0453a4797693..3f81d6900c71 100644
--- a/fs/btrfs/locking.h
+++ b/fs/btrfs/locking.h
@@ -19,7 +19,8 @@ void btrfs_tree_read_unlock(struct extent_buffer *eb);
 void btrfs_tree_read_unlock_blocking(struct extent_buffer *eb);
 void btrfs_set_lock_blocking_read(struct extent_buffer *eb);
 void btrfs_set_lock_blocking_write(struct extent_buffer *eb);
-void btrfs_clear_lock_blocking_rw(struct extent_buffer *eb, int rw);
+void btrfs_clear_lock_blocking_read(struct extent_buffer *eb);
+void btrfs_clear_lock_blocking_write(struct extent_buffer *eb);
 void btrfs_assert_tree_locked(struct extent_buffer *eb);
 int btrfs_try_tree_read_lock(struct extent_buffer *eb);
 int btrfs_try_tree_write_lock(struct extent_buffer *eb);
@@ -55,8 +56,4 @@ static inline void btrfs_set_lock_blocking(struct extent_buffer *eb)
 	btrfs_set_lock_blocking_rw(eb, BTRFS_WRITE_LOCK);
 }
 
-static inline void btrfs_clear_lock_blocking(struct extent_buffer *eb)
-{
-	btrfs_clear_lock_blocking_rw(eb, BTRFS_WRITE_LOCK_BLOCKING);
-}
 #endif

commit b95be2d9fb2a6120958b777e13d2328f9770bc2d
Author: David Sterba <dsterba@suse.com>
Date:   Wed Apr 4 01:43:05 2018 +0200

    btrfs: split btrfs_set_lock_blocking_rw to read and write helpers
    
    There are many callers that hardcode the desired lock type so we can
    avoid the switch and call them directly. Split the current function to
    two but leave a helper that still takes the variable lock type to make
    current code compile.  The call sites will be converted in followup
    patches.
    
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/locking.h b/fs/btrfs/locking.h
index 29135def468e..0453a4797693 100644
--- a/fs/btrfs/locking.h
+++ b/fs/btrfs/locking.h
@@ -17,7 +17,8 @@ void btrfs_tree_unlock(struct extent_buffer *eb);
 void btrfs_tree_read_lock(struct extent_buffer *eb);
 void btrfs_tree_read_unlock(struct extent_buffer *eb);
 void btrfs_tree_read_unlock_blocking(struct extent_buffer *eb);
-void btrfs_set_lock_blocking_rw(struct extent_buffer *eb, int rw);
+void btrfs_set_lock_blocking_read(struct extent_buffer *eb);
+void btrfs_set_lock_blocking_write(struct extent_buffer *eb);
 void btrfs_clear_lock_blocking_rw(struct extent_buffer *eb, int rw);
 void btrfs_assert_tree_locked(struct extent_buffer *eb);
 int btrfs_try_tree_read_lock(struct extent_buffer *eb);
@@ -37,6 +38,18 @@ static inline void btrfs_tree_unlock_rw(struct extent_buffer *eb, int rw)
 		BUG();
 }
 
+/*
+ * If we currently have a spinning reader or writer lock (indicated by the rw
+ * flag) this will bump the count of blocking holders and drop the spinlock.
+ */
+static inline void btrfs_set_lock_blocking_rw(struct extent_buffer *eb, int rw)
+{
+	if (rw == BTRFS_WRITE_LOCK)
+		btrfs_set_lock_blocking_write(eb);
+	else if (rw == BTRFS_READ_LOCK)
+		btrfs_set_lock_blocking_read(eb);
+}
+
 static inline void btrfs_set_lock_blocking(struct extent_buffer *eb)
 {
 	btrfs_set_lock_blocking_rw(eb, BTRFS_WRITE_LOCK);

commit 9888c3402c8567a977de37f61e9dd87792723064
Author: David Sterba <dsterba@suse.com>
Date:   Tue Apr 3 19:16:55 2018 +0200

    btrfs: replace GPL boilerplate by SPDX -- headers
    
    Remove GPL boilerplate text (long, short, one-line) and keep the rest,
    ie. personal, company or original source copyright statements. Add the
    SPDX header.
    
    Unify the include protection macros to match the file names.
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/locking.h b/fs/btrfs/locking.h
index c44a9d5f5362..29135def468e 100644
--- a/fs/btrfs/locking.h
+++ b/fs/btrfs/locking.h
@@ -1,23 +1,10 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 /*
  * Copyright (C) 2008 Oracle.  All rights reserved.
- *
- * This program is free software; you can redistribute it and/or
- * modify it under the terms of the GNU General Public
- * License v2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * General Public License for more details.
- *
- * You should have received a copy of the GNU General Public
- * License along with this program; if not, write to the
- * Free Software Foundation, Inc., 59 Temple Place - Suite 330,
- * Boston, MA 021110-1307, USA.
  */
 
-#ifndef __BTRFS_LOCKING_
-#define __BTRFS_LOCKING_
+#ifndef BTRFS_LOCKING_H
+#define BTRFS_LOCKING_H
 
 #define BTRFS_WRITE_LOCK 1
 #define BTRFS_READ_LOCK 2

commit f82c458a2c3ffb94b431fc6ad791a79df1b3713e
Author: Chris Mason <clm@fb.com>
Date:   Wed Nov 19 10:25:09 2014 -0800

    btrfs: fix lockups from btrfs_clear_path_blocking
    
    The fair reader/writer locks mean that btrfs_clear_path_blocking needs
    to strictly follow lock ordering rules even when we already have
    blocking locks on a given path.
    
    Before we can clear a blocking lock on the path, we need to make sure
    all of the locks have been converted to blocking.  This will remove lock
    inversions against anyone spinning in write_lock() against the buffers
    we're trying to get read locks on.  These inversions didn't exist before
    the fair read/writer locks, but now we need to be more careful.
    
    We papered over this deadlock in the past by changing
    btrfs_try_read_lock() to be a true trylock against both the spinlock and
    the blocking lock.  This was slower, and not sufficient to fix all the
    deadlocks.  This patch adds a btrfs_tree_read_lock_atomic(), which
    basically means get the spinlock but trylock on the blocking lock.
    
    Signed-off-by: Chris Mason <clm@fb.com>
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Reported-by: Patrick Schmid <schmid@phys.ethz.ch>
    cc: stable@vger.kernel.org #v3.15+

diff --git a/fs/btrfs/locking.h b/fs/btrfs/locking.h
index b81e0e9a4894..c44a9d5f5362 100644
--- a/fs/btrfs/locking.h
+++ b/fs/btrfs/locking.h
@@ -35,6 +35,8 @@ void btrfs_clear_lock_blocking_rw(struct extent_buffer *eb, int rw);
 void btrfs_assert_tree_locked(struct extent_buffer *eb);
 int btrfs_try_tree_read_lock(struct extent_buffer *eb);
 int btrfs_try_tree_write_lock(struct extent_buffer *eb);
+int btrfs_tree_read_lock_atomic(struct extent_buffer *eb);
+
 
 static inline void btrfs_tree_unlock_rw(struct extent_buffer *eb, int rw)
 {

commit d340d2475c6e394013325f83f499594628a9e558
Author: Liu Bo <bo.li.liu@oracle.com>
Date:   Mon Mar 11 09:37:45 2013 +0000

    Btrfs: remove btrfs_try_spin_lock
    
    Remove a useless function declaration
    
    Signed-off-by: Liu Bo <bo.li.liu@oracle.com>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/locking.h b/fs/btrfs/locking.h
index ca52681e5f40..b81e0e9a4894 100644
--- a/fs/btrfs/locking.h
+++ b/fs/btrfs/locking.h
@@ -26,7 +26,6 @@
 
 void btrfs_tree_lock(struct extent_buffer *eb);
 void btrfs_tree_unlock(struct extent_buffer *eb);
-int btrfs_try_spin_lock(struct extent_buffer *eb);
 
 void btrfs_tree_read_lock(struct extent_buffer *eb);
 void btrfs_tree_read_unlock(struct extent_buffer *eb);

commit 143bede527b054a271053f41bfaca2b57baa9408
Author: Jeff Mahoney <jeffm@suse.com>
Date:   Thu Mar 1 14:56:26 2012 +0100

    btrfs: return void in functions without error conditions
    
    Signed-off-by: Jeff Mahoney <jeffm@suse.com>

diff --git a/fs/btrfs/locking.h b/fs/btrfs/locking.h
index 17247ddb81a0..ca52681e5f40 100644
--- a/fs/btrfs/locking.h
+++ b/fs/btrfs/locking.h
@@ -24,8 +24,8 @@
 #define BTRFS_WRITE_LOCK_BLOCKING 3
 #define BTRFS_READ_LOCK_BLOCKING 4
 
-int btrfs_tree_lock(struct extent_buffer *eb);
-int btrfs_tree_unlock(struct extent_buffer *eb);
+void btrfs_tree_lock(struct extent_buffer *eb);
+void btrfs_tree_unlock(struct extent_buffer *eb);
 int btrfs_try_spin_lock(struct extent_buffer *eb);
 
 void btrfs_tree_read_lock(struct extent_buffer *eb);

commit bd681513fa6f2ff29aa391f01e413a2d1c59fd77
Author: Chris Mason <chris.mason@oracle.com>
Date:   Sat Jul 16 15:23:14 2011 -0400

    Btrfs: switch the btrfs tree locks to reader/writer
    
    The btrfs metadata btree is the source of significant
    lock contention, especially in the root node.   This
    commit changes our locking to use a reader/writer
    lock.
    
    The lock is built on top of rw spinlocks, and it
    extends the lock tracking to remember if we have a
    read lock or a write lock when we go to blocking.  Atomics
    count the number of blocking readers or writers at any
    given time.
    
    It removes all of the adaptive spinning from the old code
    and uses only the spinning/blocking hints inside of btrfs
    to decide when it should continue spinning.
    
    In read heavy workloads this is dramatically faster.  In write
    heavy workloads we're still faster because of less contention
    on the root node lock.
    
    We suffer slightly in dbench because we schedule more often
    during write locks, but all other benchmarks so far are improved.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/locking.h b/fs/btrfs/locking.h
index 5c33a560a2f1..17247ddb81a0 100644
--- a/fs/btrfs/locking.h
+++ b/fs/btrfs/locking.h
@@ -19,11 +19,43 @@
 #ifndef __BTRFS_LOCKING_
 #define __BTRFS_LOCKING_
 
+#define BTRFS_WRITE_LOCK 1
+#define BTRFS_READ_LOCK 2
+#define BTRFS_WRITE_LOCK_BLOCKING 3
+#define BTRFS_READ_LOCK_BLOCKING 4
+
 int btrfs_tree_lock(struct extent_buffer *eb);
 int btrfs_tree_unlock(struct extent_buffer *eb);
 int btrfs_try_spin_lock(struct extent_buffer *eb);
 
-void btrfs_set_lock_blocking(struct extent_buffer *eb);
-void btrfs_clear_lock_blocking(struct extent_buffer *eb);
+void btrfs_tree_read_lock(struct extent_buffer *eb);
+void btrfs_tree_read_unlock(struct extent_buffer *eb);
+void btrfs_tree_read_unlock_blocking(struct extent_buffer *eb);
+void btrfs_set_lock_blocking_rw(struct extent_buffer *eb, int rw);
+void btrfs_clear_lock_blocking_rw(struct extent_buffer *eb, int rw);
 void btrfs_assert_tree_locked(struct extent_buffer *eb);
+int btrfs_try_tree_read_lock(struct extent_buffer *eb);
+int btrfs_try_tree_write_lock(struct extent_buffer *eb);
+
+static inline void btrfs_tree_unlock_rw(struct extent_buffer *eb, int rw)
+{
+	if (rw == BTRFS_WRITE_LOCK || rw == BTRFS_WRITE_LOCK_BLOCKING)
+		btrfs_tree_unlock(eb);
+	else if (rw == BTRFS_READ_LOCK_BLOCKING)
+		btrfs_tree_read_unlock_blocking(eb);
+	else if (rw == BTRFS_READ_LOCK)
+		btrfs_tree_read_unlock(eb);
+	else
+		BUG();
+}
+
+static inline void btrfs_set_lock_blocking(struct extent_buffer *eb)
+{
+	btrfs_set_lock_blocking_rw(eb, BTRFS_WRITE_LOCK);
+}
+
+static inline void btrfs_clear_lock_blocking(struct extent_buffer *eb)
+{
+	btrfs_clear_lock_blocking_rw(eb, BTRFS_WRITE_LOCK_BLOCKING);
+}
 #endif

commit f2a97a9dbd86eb1ef956bdf20e05c507b32beb96
Author: David Sterba <dsterba@suse.cz>
Date:   Thu May 5 12:44:41 2011 +0200

    btrfs: remove all unused functions
    
    Remove static and global declarations and/or definitions. Reduces size
    of btrfs.ko by ~3.4kB.
    
      text    data     bss     dec     hex filename
    402081    7464     200  409745   64091 btrfs.ko.base
    398620    7144     200  405964   631cc btrfs.ko.remove-all
    
    Signed-off-by: David Sterba <dsterba@suse.cz>

diff --git a/fs/btrfs/locking.h b/fs/btrfs/locking.h
index 6c4ce457168c..5c33a560a2f1 100644
--- a/fs/btrfs/locking.h
+++ b/fs/btrfs/locking.h
@@ -21,8 +21,6 @@
 
 int btrfs_tree_lock(struct extent_buffer *eb);
 int btrfs_tree_unlock(struct extent_buffer *eb);
-
-int btrfs_try_tree_lock(struct extent_buffer *eb);
 int btrfs_try_spin_lock(struct extent_buffer *eb);
 
 void btrfs_set_lock_blocking(struct extent_buffer *eb);

commit b9447ef80bd301b932ac4d85c9622e929de5fd62
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Mar 9 11:45:38 2009 -0400

    Btrfs: fix spinlock assertions on UP systems
    
    btrfs_tree_locked was being used to make sure a given extent_buffer was
    properly locked in a few places.  But, it wasn't correct for UP compiled
    kernels.
    
    This switches it to using assert_spin_locked instead, and renames it to
    btrfs_assert_tree_locked to better reflect how it was really being used.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/locking.h b/fs/btrfs/locking.h
index 6bb0afbff928..6c4ce457168c 100644
--- a/fs/btrfs/locking.h
+++ b/fs/btrfs/locking.h
@@ -21,11 +21,11 @@
 
 int btrfs_tree_lock(struct extent_buffer *eb);
 int btrfs_tree_unlock(struct extent_buffer *eb);
-int btrfs_tree_locked(struct extent_buffer *eb);
 
 int btrfs_try_tree_lock(struct extent_buffer *eb);
 int btrfs_try_spin_lock(struct extent_buffer *eb);
 
 void btrfs_set_lock_blocking(struct extent_buffer *eb);
 void btrfs_clear_lock_blocking(struct extent_buffer *eb);
+void btrfs_assert_tree_locked(struct extent_buffer *eb);
 #endif

commit 284b066af41579f62649048fdec5c5e7091703e6
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Feb 9 16:22:03 2009 -0500

    Btrfs: don't use spin_is_contended
    
    Btrfs was using spin_is_contended to see if it should drop locks before
    doing extent allocations during btrfs_search_slot.  The idea was to avoid
    expensive searches in the tree unless the lock was actually contended.
    
    But, spin_is_contended is specific to the ticket spinlocks on x86, so this
    is causing compile errors everywhere else.
    
    In practice, the contention could easily appear some time after we started
    doing the extent allocation, and it makes more sense to always drop the lock
    instead.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/locking.h b/fs/btrfs/locking.h
index d92e707f5870..6bb0afbff928 100644
--- a/fs/btrfs/locking.h
+++ b/fs/btrfs/locking.h
@@ -26,8 +26,6 @@ int btrfs_tree_locked(struct extent_buffer *eb);
 int btrfs_try_tree_lock(struct extent_buffer *eb);
 int btrfs_try_spin_lock(struct extent_buffer *eb);
 
-int btrfs_path_lock_waiting(struct btrfs_path *path, int level);
-
 void btrfs_set_lock_blocking(struct extent_buffer *eb);
 void btrfs_clear_lock_blocking(struct extent_buffer *eb);
 #endif

commit b4ce94de9b4d64e8ab3cf155d13653c666e22b9b
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Feb 4 09:25:08 2009 -0500

    Btrfs: Change btree locking to use explicit blocking points
    
    Most of the btrfs metadata operations can be protected by a spinlock,
    but some operations still need to schedule.
    
    So far, btrfs has been using a mutex along with a trylock loop,
    most of the time it is able to avoid going for the full mutex, so
    the trylock loop is a big performance gain.
    
    This commit is step one for getting rid of the blocking locks entirely.
    btrfs_tree_lock takes a spinlock, and the code explicitly switches
    to a blocking lock when it starts an operation that can schedule.
    
    We'll be able get rid of the blocking locks in smaller pieces over time.
    Tracing allows us to find the most common cause of blocking, so we
    can start with the hot spots first.
    
    The basic idea is:
    
    btrfs_tree_lock() returns with the spin lock held
    
    btrfs_set_lock_blocking() sets the EXTENT_BUFFER_BLOCKING bit in
    the extent buffer flags, and then drops the spin lock.  The buffer is
    still considered locked by all of the btrfs code.
    
    If btrfs_tree_lock gets the spinlock but finds the blocking bit set, it drops
    the spin lock and waits on a wait queue for the blocking bit to go away.
    
    Much of the code that needs to set the blocking bit finishes without actually
    blocking a good percentage of the time.  So, an adaptive spin is still
    used against the blocking bit to avoid very high context switch rates.
    
    btrfs_clear_lock_blocking() clears the blocking bit and returns
    with the spinlock held again.
    
    btrfs_tree_unlock() can be called on either blocking or spinning locks,
    it does the right thing based on the blocking bit.
    
    ctree.c has a helper function to set/clear all the locked buffers in a
    path as blocking.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/locking.h b/fs/btrfs/locking.h
index bc1faef12519..d92e707f5870 100644
--- a/fs/btrfs/locking.h
+++ b/fs/btrfs/locking.h
@@ -22,6 +22,12 @@
 int btrfs_tree_lock(struct extent_buffer *eb);
 int btrfs_tree_unlock(struct extent_buffer *eb);
 int btrfs_tree_locked(struct extent_buffer *eb);
+
 int btrfs_try_tree_lock(struct extent_buffer *eb);
+int btrfs_try_spin_lock(struct extent_buffer *eb);
+
 int btrfs_path_lock_waiting(struct btrfs_path *path, int level);
+
+void btrfs_set_lock_blocking(struct extent_buffer *eb);
+void btrfs_clear_lock_blocking(struct extent_buffer *eb);
 #endif

commit 65b51a009e29e64c0951f21ea17fdc66bbb0fbd7
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Aug 1 15:11:20 2008 -0400

    btrfs_search_slot: reduce lock contention by cowing in two stages
    
    A btree block cow has two parts, the first is to allocate a destination
    block and the second is to copy the old bock over.
    
    The first part needs locks in the extent allocation tree, and may need to
    do IO.  This changeset splits that into a separate function that can be
    called without any tree locks held.
    
    btrfs_search_slot is changed to drop its path and start over if it has
    to COW a contended block.  This often means that many writers will
    pre-alloc a new destination for a the same contended block, but they
    cache their prealloc for later use on lower levels in the tree.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/locking.h b/fs/btrfs/locking.h
index 2dab96d8280e..bc1faef12519 100644
--- a/fs/btrfs/locking.h
+++ b/fs/btrfs/locking.h
@@ -23,4 +23,5 @@ int btrfs_tree_lock(struct extent_buffer *eb);
 int btrfs_tree_unlock(struct extent_buffer *eb);
 int btrfs_tree_locked(struct extent_buffer *eb);
 int btrfs_try_tree_lock(struct extent_buffer *eb);
+int btrfs_path_lock_waiting(struct btrfs_path *path, int level);
 #endif

commit 925baeddc5b0764a53f2214a1253251bab0e0324
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Jun 25 16:01:30 2008 -0400

    Btrfs: Start btree concurrency work.
    
    The allocation trees and the chunk trees are serialized via their own
    dedicated mutexes.  This means allocation location is still not very
    fine grained.
    
    The main FS btree is protected by locks on each block in the btree.  Locks
    are taken top / down, and as processing finishes on a given level of the
    tree, the lock is released after locking the lower level.
    
    The end result of a search is now a path where only the lowest level
    is locked.  Releasing or freeing the path drops any locks held.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/locking.h b/fs/btrfs/locking.h
new file mode 100644
index 000000000000..2dab96d8280e
--- /dev/null
+++ b/fs/btrfs/locking.h
@@ -0,0 +1,26 @@
+/*
+ * Copyright (C) 2008 Oracle.  All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public
+ * License v2 as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public
+ * License along with this program; if not, write to the
+ * Free Software Foundation, Inc., 59 Temple Place - Suite 330,
+ * Boston, MA 021110-1307, USA.
+ */
+
+#ifndef __BTRFS_LOCKING_
+#define __BTRFS_LOCKING_
+
+int btrfs_tree_lock(struct extent_buffer *eb);
+int btrfs_tree_unlock(struct extent_buffer *eb);
+int btrfs_tree_locked(struct extent_buffer *eb);
+int btrfs_try_tree_lock(struct extent_buffer *eb);
+#endif
