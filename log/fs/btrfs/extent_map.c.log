commit ac05ca913e9f3871126d61da275bfe8516ff01ca
Author: Filipe Manana <fdmanana@suse.com>
Date:   Fri Jan 31 14:06:07 2020 +0000

    Btrfs: fix race between using extent maps and merging them
    
    We have a few cases where we allow an extent map that is in an extent map
    tree to be merged with other extents in the tree. Such cases include the
    unpinning of an extent after the respective ordered extent completed or
    after logging an extent during a fast fsync. This can lead to subtle and
    dangerous problems because when doing the merge some other task might be
    using the same extent map and as consequence see an inconsistent state of
    the extent map - for example sees the new length but has seen the old start
    offset.
    
    With luck this triggers a BUG_ON(), and not some silent bug, such as the
    following one in __do_readpage():
    
      $ cat -n fs/btrfs/extent_io.c
      3061  static int __do_readpage(struct extent_io_tree *tree,
      3062                           struct page *page,
      (...)
      3127                  em = __get_extent_map(inode, page, pg_offset, cur,
      3128                                        end - cur + 1, get_extent, em_cached);
      3129                  if (IS_ERR_OR_NULL(em)) {
      3130                          SetPageError(page);
      3131                          unlock_extent(tree, cur, end);
      3132                          break;
      3133                  }
      3134                  extent_offset = cur - em->start;
      3135                  BUG_ON(extent_map_end(em) <= cur);
      (...)
    
    Consider the following example scenario, where we end up hitting the
    BUG_ON() in __do_readpage().
    
    We have an inode with a size of 8KiB and 2 extent maps:
    
      extent A: file offset 0, length 4KiB, disk_bytenr = X, persisted on disk by
                a previous transaction
    
      extent B: file offset 4KiB, length 4KiB, disk_bytenr = X + 4KiB, not yet
                persisted but writeback started for it already. The extent map
                is pinned since there's writeback and an ordered extent in
                progress, so it can not be merged with extent map A yet
    
    The following sequence of steps leads to the BUG_ON():
    
    1) The ordered extent for extent B completes, the respective page gets its
       writeback bit cleared and the extent map is unpinned, at that point it
       is not yet merged with extent map A because it's in the list of modified
       extents;
    
    2) Due to memory pressure, or some other reason, the MM subsystem releases
       the page corresponding to extent B - btrfs_releasepage() is called and
       returns 1, meaning the page can be released as it's not dirty, not under
       writeback anymore and the extent range is not locked in the inode's
       iotree. However the extent map is not released, either because we are
       not in a context that allows memory allocations to block or because the
       inode's size is smaller than 16MiB - in this case our inode has a size
       of 8KiB;
    
    3) Task B needs to read extent B and ends up __do_readpage() through the
       btrfs_readpage() callback. At __do_readpage() it gets a reference to
       extent map B;
    
    4) Task A, doing a fast fsync, calls clear_em_loggin() against extent map B
       while holding the write lock on the inode's extent map tree - this
       results in try_merge_map() being called and since it's possible to merge
       extent map B with extent map A now (the extent map B was removed from
       the list of modified extents), the merging begins - it sets extent map
       B's start offset to 0 (was 4KiB), but before it increments the map's
       length to 8KiB (4kb + 4KiB), task A is at:
    
       BUG_ON(extent_map_end(em) <= cur);
    
       The call to extent_map_end() sees the extent map has a start of 0
       and a length still at 4KiB, so it returns 4KiB and 'cur' is 4KiB, so
       the BUG_ON() is triggered.
    
    So it's dangerous to modify an extent map that is in the tree, because some
    other task might have got a reference to it before and still using it, and
    needs to see a consistent map while using it. Generally this is very rare
    since most paths that lookup and use extent maps also have the file range
    locked in the inode's iotree. The fsync path is pretty much the only
    exception where we don't do it to avoid serialization with concurrent
    reads.
    
    Fix this by not allowing an extent map do be merged if if it's being used
    by tasks other then the one attempting to merge the extent map (when the
    reference count of the extent map is greater than 2).
    
    Reported-by: ryusuke1925 <st13s20@gm.ibaraki-ct.ac.jp>
    Reported-by: Koki Mitani <koki.mitani.xg@hco.ntt.co.jp>
    Bugzilla: https://bugzilla.kernel.org/show_bug.cgi?id=206211
    CC: stable@vger.kernel.org # 4.4+
    Reviewed-by: Josef Bacik <josef@toxicpanda.com>
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 6f417ff68980..bd6229fb2b6f 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -237,6 +237,17 @@ static void try_merge_map(struct extent_map_tree *tree, struct extent_map *em)
 	struct extent_map *merge = NULL;
 	struct rb_node *rb;
 
+	/*
+	 * We can't modify an extent map that is in the tree and that is being
+	 * used by another task, as it can cause that other task to see it in
+	 * inconsistent state during the merging. We always have 1 reference for
+	 * the tree and 1 for this task (which is unpinning the extent map or
+	 * clearing the logging flag), so anything > 2 means it's being used by
+	 * other tasks too.
+	 */
+	if (refcount_read(&em->refs) > 2)
+		return;
+
 	if (em->start != 0) {
 		rb = rb_prev(&em->rb_node);
 		if (rb)

commit a019e9e197eaa68ffe2efeba00d685581b1a5416
Author: David Sterba <dsterba@suse.com>
Date:   Fri Aug 30 15:40:53 2019 +0200

    btrfs: remove extent_map::bdev
    
    We can now remove the bdev from extent_map. Previous patches made sure
    that bio_set_dev is correctly in all places and that we don't need to
    grab it from latest_bdev or pass it around inside the extent map.
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 9f99dccbc3ca..6f417ff68980 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -218,9 +218,6 @@ static int mergable_maps(struct extent_map *prev, struct extent_map *next)
 		ASSERT(test_bit(EXTENT_FLAG_FS_MAPPING, &prev->flags) &&
 		       test_bit(EXTENT_FLAG_FS_MAPPING, &next->flags));
 
-	if (prev->bdev || next->bdev)
-		ASSERT(prev->bdev == next->bdev);
-
 	if (extent_map_end(prev) == next->start &&
 	    prev->flags == next->flags &&
 	    prev->map_lookup == next->map_lookup &&

commit c3e14909d3b399d49d8dfcf3edbcd5463a7eed6f
Author: David Sterba <dsterba@suse.com>
Date:   Fri May 10 17:48:30 2019 +0200

    btrfs: assert extent_map bdevs and lookup_map and split
    
    This is a preparatory patch for removing extent_map::bdev. There's some
    history behind the code so this is only precaution to catch if things
    break before the actual removal happens.
    
    Logically, comparing a raw low-level block device (bdev) does not make
    sense for extent maps (high-level objects). This had no effect in
    practice but was quite confusing in the code.  The lookup_map is set iff
    EXTENT_FLAG_FS_MAPPING is set.
    
    The two pointers were stored in the same bytes and used potentially in
    two meanings. Now they're split, so the asserts are in place to check
    that the condition will not change.
    
    The lookup map pointer misused bdev, this has been changed in commit
    95617d69326c ("btrfs: cleanup, stop casting for extent_map->lookup
    everywhere") to the explicit type. But the semantics hasn't changed and
    bdev was not actually used to decide if maps are mergeable.
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 9d30acca55e1..9f99dccbc3ca 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -214,9 +214,16 @@ static int mergable_maps(struct extent_map *prev, struct extent_map *next)
 	ASSERT(next->block_start != EXTENT_MAP_DELALLOC &&
 	       prev->block_start != EXTENT_MAP_DELALLOC);
 
+	if (prev->map_lookup || next->map_lookup)
+		ASSERT(test_bit(EXTENT_FLAG_FS_MAPPING, &prev->flags) &&
+		       test_bit(EXTENT_FLAG_FS_MAPPING, &next->flags));
+
+	if (prev->bdev || next->bdev)
+		ASSERT(prev->bdev == next->bdev);
+
 	if (extent_map_end(prev) == next->start &&
 	    prev->flags == next->flags &&
-	    prev->bdev == next->bdev &&
+	    prev->map_lookup == next->map_lookup &&
 	    ((next->block_start == EXTENT_MAP_HOLE &&
 	      prev->block_start == EXTENT_MAP_HOLE) ||
 	     (next->block_start == EXTENT_MAP_INLINE &&

commit d23ea3fa7dcb0d4a2c405de0879bc4ddcf521d7d
Author: David Sterba <dsterba@suse.com>
Date:   Wed Mar 27 16:19:55 2019 +0100

    btrfs: assert extent map tree lock in add_extent_mapping
    
    As add_extent_mapping is called from several functions, let's add the
    lock annotation. The tree is going to be modified so it must be the
    exclusive lock.
    
    Reviewed-by: Nikolay Borisov <nborisov@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 9558d79faf1e..9d30acca55e1 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -384,6 +384,8 @@ int add_extent_mapping(struct extent_map_tree *tree,
 {
 	int ret = 0;
 
+	lockdep_assert_held_write(&tree->lock);
+
 	ret = tree_insert(&tree->map, em);
 	if (ret)
 		goto out;

commit 8811133d8a982d3cef5d25eef54a8dca9e8e6ded
Author: Nikolay Borisov <nborisov@suse.com>
Date:   Wed Mar 27 14:24:16 2019 +0200

    btrfs: Optimize unallocated chunks discard
    
    Currently unallocated chunks are always trimmed. For example
    2 consecutive trims on large storage would trim freespace twice
    irrespective of whether the space was actually allocated or not between
    those trims.
    
    Optimise this behavior by exploiting the newly introduced alloc_state
    tree of btrfs_device. A new CHUNK_TRIMMED bit is used to mark
    those unallocated chunks which have been trimmed and have not been
    allocated afterwards. On chunk allocation the respective underlying devices'
    physical space will have its CHUNK_TRIMMED flag cleared. This avoids
    submitting discards for space which hasn't been changed since the last
    time discard was issued.
    
    This applies to the single mount period of the filesystem as the
    information is not stored permanently.
    
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 5a79a656dfa6..9558d79faf1e 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -389,8 +389,10 @@ int add_extent_mapping(struct extent_map_tree *tree,
 		goto out;
 
 	setup_extent_mapping(tree, em, modified);
-	if (test_bit(EXTENT_FLAG_FS_MAPPING, &em->flags))
+	if (test_bit(EXTENT_FLAG_FS_MAPPING, &em->flags)) {
 		extent_map_device_set_bits(em, CHUNK_ALLOCATED);
+		extent_map_device_clear_bits(em, CHUNK_TRIMMED);
+	}
 out:
 	return ret;
 }

commit 1c11b63eff2a67906cb9137bc6b2ee27767f313b
Author: Jeff Mahoney <jeffm@suse.com>
Date:   Wed Mar 27 14:24:12 2019 +0200

    btrfs: replace pending/pinned chunks lists with io tree
    
    The pending chunks list contains chunks that are allocated in the
    current transaction but haven't been created yet. The pinned chunks
    list contains chunks that are being released in the current transaction.
    Both describe chunks that are not reflected on disk as in use but are
    unavailable just the same.
    
    The pending chunks list is anchored by the transaction handle, which
    means that we need to hold a reference to a transaction when working
    with the list.
    
    The way we use them is by iterating over both lists to perform
    comparisons on the stripes they describe for each device. This is
    backwards and requires that we keep a transaction handle open while
    we're trimming.
    
    This patchset adds an extent_io_tree to btrfs_device that maintains
    the allocation state of the device.  Extents are set dirty when
    chunks are first allocated -- when the extent maps are added to the
    mapping tree. They're cleared when last removed -- when the extent
    maps are removed from the mapping tree. This matches the lifespan
    of the pending and pinned chunks list and allows us to do trims
    on unallocated space safely without pinning the transaction for what
    may be a lengthy operation. We can also use this io tree to mark
    which chunks have already been trimmed so we don't repeat the operation.
    
    Signed-off-by: Jeff Mahoney <jeffm@suse.com>
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 928f729c55ba..5a79a656dfa6 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -4,6 +4,7 @@
 #include <linux/slab.h>
 #include <linux/spinlock.h>
 #include "ctree.h"
+#include "volumes.h"
 #include "extent_map.h"
 #include "compression.h"
 
@@ -337,6 +338,37 @@ static inline void setup_extent_mapping(struct extent_map_tree *tree,
 		try_merge_map(tree, em);
 }
 
+static void extent_map_device_set_bits(struct extent_map *em, unsigned bits)
+{
+	struct map_lookup *map = em->map_lookup;
+	u64 stripe_size = em->orig_block_len;
+	int i;
+
+	for (i = 0; i < map->num_stripes; i++) {
+		struct btrfs_bio_stripe *stripe = &map->stripes[i];
+		struct btrfs_device *device = stripe->dev;
+
+		set_extent_bits_nowait(&device->alloc_state, stripe->physical,
+				 stripe->physical + stripe_size - 1, bits);
+	}
+}
+
+static void extent_map_device_clear_bits(struct extent_map *em, unsigned bits)
+{
+	struct map_lookup *map = em->map_lookup;
+	u64 stripe_size = em->orig_block_len;
+	int i;
+
+	for (i = 0; i < map->num_stripes; i++) {
+		struct btrfs_bio_stripe *stripe = &map->stripes[i];
+		struct btrfs_device *device = stripe->dev;
+
+		__clear_extent_bit(&device->alloc_state, stripe->physical,
+				   stripe->physical + stripe_size - 1, bits,
+				   0, 0, NULL, GFP_NOWAIT, NULL);
+	}
+}
+
 /**
  * add_extent_mapping - add new extent map to the extent tree
  * @tree:	tree to insert new map in
@@ -357,6 +389,8 @@ int add_extent_mapping(struct extent_map_tree *tree,
 		goto out;
 
 	setup_extent_mapping(tree, em, modified);
+	if (test_bit(EXTENT_FLAG_FS_MAPPING, &em->flags))
+		extent_map_device_set_bits(em, CHUNK_ALLOCATED);
 out:
 	return ret;
 }
@@ -438,6 +472,8 @@ void remove_extent_mapping(struct extent_map_tree *tree, struct extent_map *em)
 	rb_erase_cached(&em->rb_node, &tree->map);
 	if (!test_bit(EXTENT_FLAG_LOGGING, &em->flags))
 		list_del_init(&em->list);
+	if (test_bit(EXTENT_FLAG_FS_MAPPING, &em->flags))
+		extent_map_device_clear_bits(em, CHUNK_ALLOCATED);
 	RB_CLEAR_NODE(&em->rb_node);
 }
 

commit 951e05a90469e29326715e7e3aefd4680748b4fa
Author: Nikolay Borisov <nborisov@suse.com>
Date:   Tue Jan 8 16:53:46 2019 +0200

    btrfs: Remove impossible condition from mergable_maps
    
    We can never have extents marked as EXTENT_MAP_DELALLOC since this
    value is only ever used by btrfs_get_extent_fiemap. In this case the
    extent map is created by btrfs_get_extent_fiemap and is never really
    published, this flag is used to return the corresponding userspace one.
    Considering this, it's pointless having a check for EXTENT_MAP_DELALLOC
    in mergable_maps. Just remove it.
    
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index a042a193c120..928f729c55ba 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -210,6 +210,9 @@ static int mergable_maps(struct extent_map *prev, struct extent_map *next)
 	if (!list_empty(&prev->list) || !list_empty(&next->list))
 		return 0;
 
+	ASSERT(next->block_start != EXTENT_MAP_DELALLOC &&
+	       prev->block_start != EXTENT_MAP_DELALLOC);
+
 	if (extent_map_end(prev) == next->start &&
 	    prev->flags == next->flags &&
 	    prev->bdev == next->bdev &&
@@ -217,8 +220,6 @@ static int mergable_maps(struct extent_map *prev, struct extent_map *next)
 	      prev->block_start == EXTENT_MAP_HOLE) ||
 	     (next->block_start == EXTENT_MAP_INLINE &&
 	      prev->block_start == EXTENT_MAP_INLINE) ||
-	     (next->block_start == EXTENT_MAP_DELALLOC &&
-	      prev->block_start == EXTENT_MAP_DELALLOC) ||
 	     (next->block_start < EXTENT_MAP_LAST_BYTE - 1 &&
 	      next->block_start == extent_map_block_end(prev)))) {
 		return 1;

commit 52042d8e82ff50d40e76a275ac0b97aa663328b0
Author: Andrea Gelmini <andrea.gelmini@gelma.net>
Date:   Wed Nov 28 12:05:13 2018 +0100

    btrfs: Fix typos in comments and strings
    
    The typos accumulate over time so once in a while time they get fixed in
    a large patch.
    
    Signed-off-by: Andrea Gelmini <andrea.gelmini@gelma.net>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 7eea8b6e2cd3..a042a193c120 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -475,7 +475,8 @@ static struct extent_map *prev_extent_map(struct extent_map *em)
 	return container_of(prev, struct extent_map, rb_node);
 }
 
-/* helper for btfs_get_extent.  Given an existing extent in the tree,
+/*
+ * Helper for btrfs_get_extent.  Given an existing extent in the tree,
  * the existing extent is the nearest extent to map_start,
  * and an extent that you want to insert, deal with overlap and insert
  * the best fitted new extent into the tree.

commit 07e1ce096db3605f3e0c98695df66a51e2be9f05
Author: Liu Bo <bo.liu@linux.alibaba.com>
Date:   Thu Aug 23 03:51:52 2018 +0800

    Btrfs: extent_map: use rb_first_cached
    
    rb_first_cached() trades an extra pointer "leftmost" for doing the
    same job as rb_first() but in O(1).
    
    As evict_inode_truncate_pages() removes all extent mapping by always
    looking for the first rb entry, it's helpful to use rb_first_cached
    instead.
    
    For more details about the optimization see patch "Btrfs: delayed-refs:
    use rb_first_cached for href_root".
    
    Tested-by: Holger Hoffst√§tte <holger@applied-asynchrony.com>
    Signed-off-by: Liu Bo <bo.liu@linux.alibaba.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 2382b949cef8..7eea8b6e2cd3 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -34,7 +34,7 @@ void __cold extent_map_exit(void)
  */
 void extent_map_tree_init(struct extent_map_tree *tree)
 {
-	tree->map = RB_ROOT;
+	tree->map = RB_ROOT_CACHED;
 	INIT_LIST_HEAD(&tree->modified_extents);
 	rwlock_init(&tree->lock);
 }
@@ -90,24 +90,27 @@ static u64 range_end(u64 start, u64 len)
 	return start + len;
 }
 
-static int tree_insert(struct rb_root *root, struct extent_map *em)
+static int tree_insert(struct rb_root_cached *root, struct extent_map *em)
 {
-	struct rb_node **p = &root->rb_node;
+	struct rb_node **p = &root->rb_root.rb_node;
 	struct rb_node *parent = NULL;
 	struct extent_map *entry = NULL;
 	struct rb_node *orig_parent = NULL;
 	u64 end = range_end(em->start, em->len);
+	bool leftmost = true;
 
 	while (*p) {
 		parent = *p;
 		entry = rb_entry(parent, struct extent_map, rb_node);
 
-		if (em->start < entry->start)
+		if (em->start < entry->start) {
 			p = &(*p)->rb_left;
-		else if (em->start >= extent_map_end(entry))
+		} else if (em->start >= extent_map_end(entry)) {
 			p = &(*p)->rb_right;
-		else
+			leftmost = false;
+		} else {
 			return -EEXIST;
+		}
 	}
 
 	orig_parent = parent;
@@ -130,7 +133,7 @@ static int tree_insert(struct rb_root *root, struct extent_map *em)
 			return -EEXIST;
 
 	rb_link_node(&em->rb_node, orig_parent, p);
-	rb_insert_color(&em->rb_node, root);
+	rb_insert_color_cached(&em->rb_node, root, leftmost);
 	return 0;
 }
 
@@ -242,7 +245,7 @@ static void try_merge_map(struct extent_map_tree *tree, struct extent_map *em)
 			em->mod_start = merge->mod_start;
 			em->generation = max(em->generation, merge->generation);
 
-			rb_erase(&merge->rb_node, &tree->map);
+			rb_erase_cached(&merge->rb_node, &tree->map);
 			RB_CLEAR_NODE(&merge->rb_node);
 			free_extent_map(merge);
 		}
@@ -254,7 +257,7 @@ static void try_merge_map(struct extent_map_tree *tree, struct extent_map *em)
 	if (rb && mergable_maps(em, merge)) {
 		em->len += merge->len;
 		em->block_len += merge->block_len;
-		rb_erase(&merge->rb_node, &tree->map);
+		rb_erase_cached(&merge->rb_node, &tree->map);
 		RB_CLEAR_NODE(&merge->rb_node);
 		em->mod_len = (merge->mod_start + merge->mod_len) - em->mod_start;
 		em->generation = max(em->generation, merge->generation);
@@ -367,7 +370,7 @@ __lookup_extent_mapping(struct extent_map_tree *tree,
 	struct rb_node *next = NULL;
 	u64 end = range_end(start, len);
 
-	rb_node = __tree_search(&tree->map, start, &prev, &next);
+	rb_node = __tree_search(&tree->map.rb_root, start, &prev, &next);
 	if (!rb_node) {
 		if (prev)
 			rb_node = prev;
@@ -431,7 +434,7 @@ struct extent_map *search_extent_mapping(struct extent_map_tree *tree,
 void remove_extent_mapping(struct extent_map_tree *tree, struct extent_map *em)
 {
 	WARN_ON(test_bit(EXTENT_FLAG_PINNED, &em->flags));
-	rb_erase(&em->rb_node, &tree->map);
+	rb_erase_cached(&em->rb_node, &tree->map);
 	if (!test_bit(EXTENT_FLAG_LOGGING, &em->flags))
 		list_del_init(&em->list);
 	RB_CLEAR_NODE(&em->rb_node);
@@ -446,7 +449,7 @@ void replace_extent_mapping(struct extent_map_tree *tree,
 	ASSERT(extent_map_in_tree(cur));
 	if (!test_bit(EXTENT_FLAG_LOGGING, &cur->flags))
 		list_del_init(&cur->list);
-	rb_replace_node(&cur->rb_node, &new->rb_node, &tree->map);
+	rb_replace_node_cached(&cur->rb_node, &new->rb_node, &tree->map);
 	RB_CLEAR_NODE(&cur->rb_node);
 
 	setup_extent_mapping(tree, new, modified);

commit c1766dd7829802b3a451682234497a8539763bd1
Author: zhong jiang <zhongjiang@huawei.com>
Date:   Thu Sep 13 11:01:15 2018 +0800

    btrfs: change remove_extent_mapping to return void
    
    remove_extent_mapping uses the variable "ret" for return value, but it
    is not modified after initialzation. Further, I find that any of the
    callers do not handle the return value and the callees are only simple
    functions so the return values does not need to be passed.
    
    Signed-off-by: zhong jiang <zhongjiang@huawei.com>
    Reviewed-by: Nikolay Borisov <nborisov@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 6648d55e5339..2382b949cef8 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -428,16 +428,13 @@ struct extent_map *search_extent_mapping(struct extent_map_tree *tree,
  * Removes @em from @tree.  No reference counts are dropped, and no checks
  * are done to see if the range is in use
  */
-int remove_extent_mapping(struct extent_map_tree *tree, struct extent_map *em)
+void remove_extent_mapping(struct extent_map_tree *tree, struct extent_map *em)
 {
-	int ret = 0;
-
 	WARN_ON(test_bit(EXTENT_FLAG_PINNED, &em->flags));
 	rb_erase(&em->rb_node, &tree->map);
 	if (!test_bit(EXTENT_FLAG_LOGGING, &em->flags))
 		list_del_init(&em->list);
 	RB_CLEAR_NODE(&em->rb_node);
-	return ret;
 }
 
 void replace_extent_mapping(struct extent_map_tree *tree,

commit f46b24c9457143a367c6707eac82d546e2bcf280
Author: David Sterba <dsterba@suse.com>
Date:   Tue Apr 3 21:45:57 2018 +0200

    btrfs: use fs_info for btrfs_handle_em_exist tracepoint
    
    We really want to know to which filesystem the extent map events belong,
    but as it cannot be reached from the extent_map pointers, we need to
    pass it down the callchain.
    
    Reviewed-by: Nikolay Borisov <nborisov@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 1b8a078f92eb..6648d55e5339 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -518,6 +518,7 @@ static noinline int merge_extent_mapping(struct extent_map_tree *em_tree,
 
 /**
  * btrfs_add_extent_mapping - add extent mapping into em_tree
+ * @fs_info - used for tracepoint
  * @em_tree - the extent tree into which we want to insert the extent mapping
  * @em_in   - extent we are inserting
  * @start   - start of the logical range btrfs_get_extent() is requesting
@@ -535,7 +536,8 @@ static noinline int merge_extent_mapping(struct extent_map_tree *em_tree,
  * Return 0 on success, otherwise -EEXIST.
  *
  */
-int btrfs_add_extent_mapping(struct extent_map_tree *em_tree,
+int btrfs_add_extent_mapping(struct btrfs_fs_info *fs_info,
+			     struct extent_map_tree *em_tree,
 			     struct extent_map **em_in, u64 start, u64 len)
 {
 	int ret;
@@ -553,7 +555,7 @@ int btrfs_add_extent_mapping(struct extent_map_tree *em_tree,
 
 		existing = search_extent_mapping(em_tree, start, len);
 
-		trace_btrfs_handle_em_exist(existing, em, start, len);
+		trace_btrfs_handle_em_exist(fs_info, existing, em, start, len);
 
 		/*
 		 * existing will always be non-NULL, since there must be

commit c1d7c514f745628eb096c5cbb10737855879ae25
Author: David Sterba <dsterba@suse.com>
Date:   Tue Apr 3 19:23:33 2018 +0200

    btrfs: replace GPL boilerplate by SPDX -- sources
    
    Remove GPL boilerplate text (long, short, one-line) and keep the rest,
    ie. personal, company or original source copyright statements. Add the
    SPDX header.
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 53a0633c6ef7..1b8a078f92eb 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -1,4 +1,5 @@
 // SPDX-License-Identifier: GPL-2.0
+
 #include <linux/err.h>
 #include <linux/slab.h>
 #include <linux/spinlock.h>

commit e67c718b5b9a306bde7e966be7b4ca48fa063d73
Author: David Sterba <dsterba@suse.com>
Date:   Mon Feb 19 17:24:18 2018 +0100

    btrfs: add more __cold annotations
    
    The __cold functions are placed to a special section, as they're
    expected to be called rarely. This could help i-cache prefetches or help
    compiler to decide which branches are more/less likely to be taken
    without any other annotations needed.
    
    Though we can't add more __exit annotations, it's still possible to add
    __cold (that's also added with __exit). That way the following function
    categories are tagged:
    
    - printf wrappers, error messages
    - exit helpers
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index b8ead8dc2ebe..53a0633c6ef7 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -19,7 +19,7 @@ int __init extent_map_init(void)
 	return 0;
 }
 
-void extent_map_exit(void)
+void __cold extent_map_exit(void)
 {
 	kmem_cache_destroy(extent_map_cache);
 }

commit 393da91819e35af538ef97c7c6a04899e2fbfe0e
Author: Liu Bo <bo.li.liu@oracle.com>
Date:   Fri Jan 5 12:51:16 2018 -0700

    Btrfs: add tracepoint for em's EEXIST case
    
    This is adding a tracepoint 'btrfs_handle_em_exist' to help debug the
    subtle bugs around merge_extent_mapping.
    
    Signed-off-by: Liu Bo <bo.li.liu@oracle.com>
    Reviewed-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index c80dea7c69af..b8ead8dc2ebe 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -551,6 +551,9 @@ int btrfs_add_extent_mapping(struct extent_map_tree *em_tree,
 		ret = 0;
 
 		existing = search_extent_mapping(em_tree, start, len);
+
+		trace_btrfs_handle_em_exist(existing, em, start, len);
+
 		/*
 		 * existing will always be non-NULL, since there must be
 		 * extent causing the -EEXIST.

commit 86d750a48a6d22435265abf525d8a27fa636db26
Author: Yang Shi <yang.s@alibaba-inc.com>
Date:   Sat Nov 18 07:02:16 2017 +0800

    btrfs: remove unused hardirq.h
    
    Preempt counter APIs have been split out, currently, hardirq.h just
    includes irq_enter/exit APIs which are not used by btrfs at all.
    
    So, remove the unused hardirq.h.
    
    Signed-off-by: Yang Shi <yang.s@alibaba-inc.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index d3bd02105d1c..c80dea7c69af 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -2,7 +2,6 @@
 #include <linux/err.h>
 #include <linux/slab.h>
 #include <linux/spinlock.h>
-#include <linux/hardirq.h>
 #include "ctree.h"
 #include "extent_map.h"
 #include "compression.h"

commit 5f4791f4a6479cdeb8caae1a3e5895c8a6e99c09
Author: Liu Bo <bo.li.liu@oracle.com>
Date:   Fri Jan 5 12:51:17 2018 -0700

    Btrfs: noinline merge_extent_mapping
    
    In order to debug subtle bugs around merge_extent_mapping(), perf probe
    can be used to check the arguments, but sometimes merge_extent_mapping()
    got inlined by compiler and couldn't be probed.
    
    This is adding noinline attribute to merge_extent_mapping().
    
    Signed-off-by: Liu Bo <bo.li.liu@oracle.com>
    Reviewed-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 914662428dbd..d3bd02105d1c 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -480,10 +480,10 @@ static struct extent_map *prev_extent_map(struct extent_map *em)
  * and an extent that you want to insert, deal with overlap and insert
  * the best fitted new extent into the tree.
  */
-static int merge_extent_mapping(struct extent_map_tree *em_tree,
-				struct extent_map *existing,
-				struct extent_map *em,
-				u64 map_start)
+static noinline int merge_extent_mapping(struct extent_map_tree *em_tree,
+					 struct extent_map *existing,
+					 struct extent_map *em,
+					 u64 map_start)
 {
 	struct extent_map *prev;
 	struct extent_map *next;

commit 9a7e10e7ba66ce23c8fdc1cac18cade7a0f6840d
Author: Liu Bo <bo.li.liu@oracle.com>
Date:   Fri Jan 5 12:51:15 2018 -0700

    Btrfs: add WARN_ONCE to detect unexpected error from merge_extent_mapping
    
    This is a subtle case, so in order to understand the problem, it'd be good
    to know the content of existing and em when any error occurs.
    
    Signed-off-by: Liu Bo <bo.li.liu@oracle.com>
    Reviewed-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 6fe8b14e11cf..914662428dbd 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -562,17 +562,24 @@ int btrfs_add_extent_mapping(struct extent_map_tree *em_tree,
 			*em_in = existing;
 			ret = 0;
 		} else {
+			u64 orig_start = em->start;
+			u64 orig_len = em->len;
+
 			/*
 			 * The existing extent map is the one nearest to
 			 * the [start, start + len) range which overlaps
 			 */
 			ret = merge_extent_mapping(em_tree, existing,
 						   em, start);
-			free_extent_map(existing);
 			if (ret) {
 				free_extent_map(em);
 				*em_in = NULL;
+				WARN_ONCE(ret,
+"unexpected error %d: merge existing(start %llu len %llu) with em(start %llu len %llu)\n",
+					  ret, existing->start, existing->len,
+					  orig_start, orig_len);
 			}
+			free_extent_map(existing);
 		}
 	}
 

commit c04e61b5e41b0e8ace4aa4b67685fbe68ac37a46
Author: Liu Bo <bo.li.liu@oracle.com>
Date:   Fri Jan 5 12:51:11 2018 -0700

    Btrfs: move extent map specific code to extent_map.c
    
    These helpers are extent map specific, move them to extent_map.c.
    
    Signed-off-by: Liu Bo <bo.li.liu@oracle.com>
    Reviewed-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 2e348fb0b280..6fe8b14e11cf 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -454,3 +454,128 @@ void replace_extent_mapping(struct extent_map_tree *tree,
 
 	setup_extent_mapping(tree, new, modified);
 }
+
+static struct extent_map *next_extent_map(struct extent_map *em)
+{
+	struct rb_node *next;
+
+	next = rb_next(&em->rb_node);
+	if (!next)
+		return NULL;
+	return container_of(next, struct extent_map, rb_node);
+}
+
+static struct extent_map *prev_extent_map(struct extent_map *em)
+{
+	struct rb_node *prev;
+
+	prev = rb_prev(&em->rb_node);
+	if (!prev)
+		return NULL;
+	return container_of(prev, struct extent_map, rb_node);
+}
+
+/* helper for btfs_get_extent.  Given an existing extent in the tree,
+ * the existing extent is the nearest extent to map_start,
+ * and an extent that you want to insert, deal with overlap and insert
+ * the best fitted new extent into the tree.
+ */
+static int merge_extent_mapping(struct extent_map_tree *em_tree,
+				struct extent_map *existing,
+				struct extent_map *em,
+				u64 map_start)
+{
+	struct extent_map *prev;
+	struct extent_map *next;
+	u64 start;
+	u64 end;
+	u64 start_diff;
+
+	BUG_ON(map_start < em->start || map_start >= extent_map_end(em));
+
+	if (existing->start > map_start) {
+		next = existing;
+		prev = prev_extent_map(next);
+	} else {
+		prev = existing;
+		next = next_extent_map(prev);
+	}
+
+	start = prev ? extent_map_end(prev) : em->start;
+	start = max_t(u64, start, em->start);
+	end = next ? next->start : extent_map_end(em);
+	end = min_t(u64, end, extent_map_end(em));
+	start_diff = start - em->start;
+	em->start = start;
+	em->len = end - start;
+	if (em->block_start < EXTENT_MAP_LAST_BYTE &&
+	    !test_bit(EXTENT_FLAG_COMPRESSED, &em->flags)) {
+		em->block_start += start_diff;
+		em->block_len = em->len;
+	}
+	return add_extent_mapping(em_tree, em, 0);
+}
+
+/**
+ * btrfs_add_extent_mapping - add extent mapping into em_tree
+ * @em_tree - the extent tree into which we want to insert the extent mapping
+ * @em_in   - extent we are inserting
+ * @start   - start of the logical range btrfs_get_extent() is requesting
+ * @len     - length of the logical range btrfs_get_extent() is requesting
+ *
+ * Note that @em_in's range may be different from [start, start+len),
+ * but they must be overlapped.
+ *
+ * Insert @em_in into @em_tree. In case there is an overlapping range, handle
+ * the -EEXIST by either:
+ * a) Returning the existing extent in @em_in if @start is within the
+ *    existing em.
+ * b) Merge the existing extent with @em_in passed in.
+ *
+ * Return 0 on success, otherwise -EEXIST.
+ *
+ */
+int btrfs_add_extent_mapping(struct extent_map_tree *em_tree,
+			     struct extent_map **em_in, u64 start, u64 len)
+{
+	int ret;
+	struct extent_map *em = *em_in;
+
+	ret = add_extent_mapping(em_tree, em, 0);
+	/* it is possible that someone inserted the extent into the tree
+	 * while we had the lock dropped.  It is also possible that
+	 * an overlapping map exists in the tree
+	 */
+	if (ret == -EEXIST) {
+		struct extent_map *existing;
+
+		ret = 0;
+
+		existing = search_extent_mapping(em_tree, start, len);
+		/*
+		 * existing will always be non-NULL, since there must be
+		 * extent causing the -EEXIST.
+		 */
+		if (start >= existing->start &&
+		    start < extent_map_end(existing)) {
+			free_extent_map(em);
+			*em_in = existing;
+			ret = 0;
+		} else {
+			/*
+			 * The existing extent map is the one nearest to
+			 * the [start, start + len) range which overlaps
+			 */
+			ret = merge_extent_mapping(em_tree, existing,
+						   em, start);
+			free_extent_map(existing);
+			if (ret) {
+				free_extent_map(em);
+				*em_in = NULL;
+			}
+		}
+	}
+
+	ASSERT(ret == 0 || ret == -EEXIST);
+	return ret;
+}

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 69850155870c..2e348fb0b280 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 #include <linux/err.h>
 #include <linux/slab.h>
 #include <linux/spinlock.h>

commit 490b54d6fb75f6ffd0471ec58bb38a992e2b40cd
Author: Elena Reshetova <elena.reshetova@intel.com>
Date:   Fri Mar 3 10:55:12 2017 +0200

    btrfs: convert extent_map.refs from atomic_t to refcount_t
    
    refcount_t type and corresponding API should be
    used instead of atomic_t when the variable is used as
    a reference counter. This allows to avoid accidental
    refcounter overflows that might lead to use-after-free
    situations.
    
    Signed-off-by: Elena Reshetova <elena.reshetova@intel.com>
    Signed-off-by: Hans Liljestrand <ishkamiel@gmail.com>
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: David Windsor <dwindsor@gmail.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 26f9ac719d20..69850155870c 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -55,7 +55,7 @@ struct extent_map *alloc_extent_map(void)
 	em->flags = 0;
 	em->compress_type = BTRFS_COMPRESS_NONE;
 	em->generation = 0;
-	atomic_set(&em->refs, 1);
+	refcount_set(&em->refs, 1);
 	INIT_LIST_HEAD(&em->list);
 	return em;
 }
@@ -71,8 +71,8 @@ void free_extent_map(struct extent_map *em)
 {
 	if (!em)
 		return;
-	WARN_ON(atomic_read(&em->refs) == 0);
-	if (atomic_dec_and_test(&em->refs)) {
+	WARN_ON(refcount_read(&em->refs) == 0);
+	if (refcount_dec_and_test(&em->refs)) {
 		WARN_ON(extent_map_in_tree(em));
 		WARN_ON(!list_empty(&em->list));
 		if (test_bit(EXTENT_FLAG_FS_MAPPING, &em->flags))
@@ -322,7 +322,7 @@ static inline void setup_extent_mapping(struct extent_map_tree *tree,
 					struct extent_map *em,
 					int modified)
 {
-	atomic_inc(&em->refs);
+	refcount_inc(&em->refs);
 	em->mod_start = em->start;
 	em->mod_len = em->len;
 
@@ -381,7 +381,7 @@ __lookup_extent_mapping(struct extent_map_tree *tree,
 	if (strict && !(end > em->start && start < extent_map_end(em)))
 		return NULL;
 
-	atomic_inc(&em->refs);
+	refcount_inc(&em->refs);
 	return em;
 }
 

commit fba4b697710eb2a4bee456b9d39e9239c66f8bee
Author: Nikolay Borisov <n.borisov.lkml@gmail.com>
Date:   Thu Jun 23 21:17:08 2016 +0300

    btrfs: Fix slab accounting flags
    
    BTRFS is using a variety of slab caches to satisfy internal needs.
    Those slab caches are always allocated with the SLAB_RECLAIM_ACCOUNT,
    meaning allocations from the caches are going to be accounted as
    SReclaimable. At the same time btrfs is not registering any shrinkers
    whatsoever, thus preventing memory from the slabs to be shrunk. This
    means those caches are not in fact reclaimable.
    
    To fix this remove the SLAB_RECLAIM_ACCOUNT on all caches apart from the
    inode cache, since this one is being freed by the generic VFS super_block
    shrinker. Also set the transaction related caches as SLAB_TEMPORARY,
    to better document the lifetime of the objects (it just translates
    to SLAB_RECLAIM_ACCOUNT).
    
    Signed-off-by: Nikolay Borisov <n.borisov.lkml@gmail.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index e0715fcfb11e..26f9ac719d20 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -13,7 +13,7 @@ int __init extent_map_init(void)
 {
 	extent_map_cache = kmem_cache_create("btrfs_extent_map",
 			sizeof(struct extent_map), 0,
-			SLAB_RECLAIM_ACCOUNT | SLAB_MEM_SPREAD, NULL);
+			SLAB_MEM_SPREAD, NULL);
 	if (!extent_map_cache)
 		return -ENOMEM;
 	return 0;

commit 0132761017e012ab4dc8584d679503f2ba26ca86
Author: Nicholas D Steeves <nsteeves@gmail.com>
Date:   Thu May 19 21:18:45 2016 -0400

    btrfs: fix string and comment grammatical issues and typos
    
    Signed-off-by: Nicholas D Steeves <nsteeves@gmail.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 318b048eb254..e0715fcfb11e 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -62,7 +62,7 @@ struct extent_map *alloc_extent_map(void)
 
 /**
  * free_extent_map - drop reference count of an extent_map
- * @em:		extent map being releasead
+ * @em:		extent map being released
  *
  * Drops the reference out on @em by one and free the structure
  * if the reference count hits zero.

commit bb7ab3b92e46da06b580c6f83abe7894dc449cca
Author: Adam Buchbinder <adam.buchbinder@gmail.com>
Date:   Fri Mar 4 11:23:12 2016 -0800

    btrfs: Fix misspellings in comments.
    
    Signed-off-by: Adam Buchbinder <adam.buchbinder@gmail.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index cca21ff6553b..318b048eb254 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -62,7 +62,7 @@ struct extent_map *alloc_extent_map(void)
 
 /**
  * free_extent_map - drop reference count of an extent_map
- * @em:		extent map beeing releasead
+ * @em:		extent map being releasead
  *
  * Drops the reference out on @em by one and free the structure
  * if the reference count hits zero.
@@ -422,7 +422,7 @@ struct extent_map *search_extent_mapping(struct extent_map_tree *tree,
 /**
  * remove_extent_mapping - removes an extent_map from the extent tree
  * @tree:	extent tree to remove from
- * @em:		extent map beeing removed
+ * @em:		extent map being removed
  *
  * Removes @em from @tree.  No reference counts are dropped, and no checks
  * are done to see if the range is in use

commit ebb8765b2ded869b75bf5154b048119eb52571f7
Author: Anand Jain <anand.jain@oracle.com>
Date:   Thu Mar 10 17:26:59 2016 +0800

    btrfs: move btrfs_compression_type to compression.h
    
    So that its better organized.
    
    Signed-off-by: Anand Jain <anand.jain@oracle.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index cdbadeaef202..cca21ff6553b 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -4,6 +4,7 @@
 #include <linux/hardirq.h>
 #include "ctree.h"
 #include "extent_map.h"
+#include "compression.h"
 
 
 static struct kmem_cache *extent_map_cache;

commit 5598e9005a4076d6700bbd89d0cdbe5b2922a846
Author: Kinglong Mee <kinglongmee@gmail.com>
Date:   Fri Jan 29 21:36:35 2016 +0800

    btrfs: drop null testing before destroy functions
    
    Cleanup.
    
    kmem_cache_destroy has support NULL argument checking,
    so drop the double null testing before calling it.
    
    Signed-off-by: Kinglong Mee <kinglongmee@gmail.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 84fb56d5c018..cdbadeaef202 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -20,8 +20,7 @@ int __init extent_map_init(void)
 
 void extent_map_exit(void)
 {
-	if (extent_map_cache)
-		kmem_cache_destroy(extent_map_cache);
+	kmem_cache_destroy(extent_map_cache);
 }
 
 /**

commit 95617d69326ce386c95e33db7aeb832b45ee9f8f
Author: Jeff Mahoney <jeffm@suse.com>
Date:   Wed Jun 3 10:55:48 2015 -0400

    btrfs: cleanup, stop casting for extent_map->lookup everywhere
    
    Overloading extent_map->bdev to struct map_lookup * might have started out
    as a means to an end, but it's a pattern that's used all over the place
    now. Let's get rid of the casting and just add a union instead.
    
    Signed-off-by: Jeff Mahoney <jeffm@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 6a98bddd8f33..84fb56d5c018 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -76,7 +76,7 @@ void free_extent_map(struct extent_map *em)
 		WARN_ON(extent_map_in_tree(em));
 		WARN_ON(!list_empty(&em->list));
 		if (test_bit(EXTENT_FLAG_FS_MAPPING, &em->flags))
-			kfree(em->bdev);
+			kfree(em->map_lookup);
 		kmem_cache_free(extent_map_cache, em);
 	}
 }

commit a28046956c71985046474283fa3bcd256915fb72
Author: Josef Bacik <jbacik@fb.com>
Date:   Fri Nov 14 16:16:30 2014 -0500

    Btrfs: do not move em to modified list when unpinning
    
    We use the modified list to keep track of which extents have been modified so we
    know which ones are candidates for logging at fsync() time.  Newly modified
    extents are added to the list at modification time, around the same time the
    ordered extent is created.  We do this so that we don't have to wait for ordered
    extents to complete before we know what we need to log.  The problem is when
    something like this happens
    
    log extent 0-4k on inode 1
    copy csum for 0-4k from ordered extent into log
    sync log
    commit transaction
    log some other extent on inode 1
    ordered extent for 0-4k completes and adds itself onto modified list again
    log changed extents
    see ordered extent for 0-4k has already been logged
            at this point we assume the csum has been copied
    sync log
    crash
    
    On replay we will see the extent 0-4k in the log, drop the original 0-4k extent
    which is the same one that we are replaying which also drops the csum, and then
    we won't find the csum in the log for that bytenr.  This of course causes us to
    have errors about not having csums for certain ranges of our inode.  So remove
    the modified list manipulation in unpin_extent_cache, any modified extents
    should have been added well before now, and we don't want them re-logged.  This
    fixes my test that I could reliably reproduce this problem with.  Thanks,
    
    cc: stable@vger.kernel.org
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 225302b39afb..6a98bddd8f33 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -287,8 +287,6 @@ int unpin_extent_cache(struct extent_map_tree *tree, u64 start, u64 len,
 	if (!em)
 		goto out;
 
-	if (!test_bit(EXTENT_FLAG_LOGGING, &em->flags))
-		list_move(&em->list, &tree->modified_extents);
 	em->generation = gen;
 	clear_bit(EXTENT_FLAG_PINNED, &em->flags);
 	em->mod_start = em->start;

commit 298a8f9cf17d2f2e1ffc41e5e247fa3695a8a76f
Author: Wang Shilong <wangsl.fnst@cn.fujitsu.com>
Date:   Thu Jun 19 10:42:52 2014 +0800

    Btrfs: fix NULL pointer crash when running balance and scrub concurrently
    
    While running balance, scrub, fsstress concurrently we hit the
    following kernel crash:
    
    [56561.448845] BTRFS info (device sde): relocating block group 11005853696 flags 132
    [56561.524077] BUG: unable to handle kernel NULL pointer dereference at 0000000000000078
    [56561.524237] IP: [<ffffffffa038956d>] scrub_chunk.isra.12+0xdd/0x130 [btrfs]
    [56561.524297] PGD 9be28067 PUD 7f3dd067 PMD 0
    [56561.524325] Oops: 0000 [#1] SMP
    [....]
    [56561.527237] Call Trace:
    [56561.527309]  [<ffffffffa038980e>] scrub_enumerate_chunks+0x24e/0x490 [btrfs]
    [56561.527392]  [<ffffffff810abe00>] ? abort_exclusive_wait+0x50/0xb0
    [56561.527476]  [<ffffffffa038add4>] btrfs_scrub_dev+0x1a4/0x530 [btrfs]
    [56561.527561]  [<ffffffffa0368107>] btrfs_ioctl+0x13f7/0x2a90 [btrfs]
    [56561.527639]  [<ffffffff811c82f0>] do_vfs_ioctl+0x2e0/0x4c0
    [56561.527712]  [<ffffffff8109c384>] ? vtime_account_user+0x54/0x60
    [56561.527788]  [<ffffffff810f768c>] ? __audit_syscall_entry+0x9c/0xf0
    [56561.527870]  [<ffffffff811c8551>] SyS_ioctl+0x81/0xa0
    [56561.527941]  [<ffffffff815707f7>] tracesys+0xdd/0xe2
    [...]
    [56561.528304] RIP  [<ffffffffa038956d>] scrub_chunk.isra.12+0xdd/0x130 [btrfs]
    [56561.528395]  RSP <ffff88004c0f5be8>
    [56561.528454] CR2: 0000000000000078
    
    This is because in btrfs_relocate_chunk(), we will free @bdev directly while
    scrub may still hold extent mapping, and may access freed memory.
    
    Fix this problem by wrapping freeing @bdev work into free_extent_map() which
    is based on reference count.
    
    Reported-by: Qu Wenruo <quwenruo@cn.fujitsu.com>
    Signed-off-by: Wang Shilong <wangsl.fnst@cn.fujitsu.com>
    Signed-off-by: Miao Xie <miaox@cn.fujitsu.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 1874aee69c86..225302b39afb 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -75,6 +75,8 @@ void free_extent_map(struct extent_map *em)
 	if (atomic_dec_and_test(&em->refs)) {
 		WARN_ON(extent_map_in_tree(em));
 		WARN_ON(!list_empty(&em->list));
+		if (test_bit(EXTENT_FLAG_FS_MAPPING, &em->flags))
+			kfree(em->bdev);
 		kmem_cache_free(extent_map_cache, em);
 	}
 }

commit 176840b3aa3cb795ddec4fc665ffbd707abff906
Author: Filipe Manana <fdmanana@gmail.com>
Date:   Tue Feb 25 14:15:13 2014 +0000

    Btrfs: more efficient btrfs_drop_extent_cache
    
    While droping extent map structures from the extent cache that cover our
    target range, we would remove each extent map structure from the red black
    tree and then add either 1 or 2 new extent map structures if the former
    extent map covered sections outside our target range.
    
    This change simply attempts to replace the existing extent map structure
    with a new one that covers the subsection we're not interested in, instead
    of doing a red black remove operation followed by an insertion operation.
    
    The number of elements in an inode's extent map tree can get very high for large
    files under random writes. For example, while running the following test:
    
        sysbench --test=fileio --file-num=1 --file-total-size=10G \
            --file-test-mode=rndrw --num-threads=32 --file-block-size=32768 \
            --max-requests=500000 --file-rw-ratio=2 [prepare|run]
    
    I captured the following histogram capturing the number of extent_map items
    in the red black tree while that test was running:
    
        Count: 122462
        Range:  1.000 - 172231.000; Mean: 96415.831; Median: 101855.000; Stddev: 49700.981
        Percentiles:  90th: 160120.000; 95th: 166335.000; 99th: 171070.000
           1.000 -    5.231:   452 |
           5.231 -  187.392:    87 |
         187.392 -  585.911:   206 |
         585.911 - 1827.438:   623 |
        1827.438 - 5695.245:  1962 #
        5695.245 - 17744.861:  6204 ####
       17744.861 - 55283.764: 21115 ############
       55283.764 - 172231.000: 91813 #####################################################
    
    Benchmark:
    
        sysbench --test=fileio --file-num=1 --file-total-size=10G --file-test-mode=rndwr \
            --num-threads=64 --file-block-size=32768 --max-requests=0 --max-time=60 \
            --file-io-mode=sync --file-fsync-freq=0 [prepare|run]
    
    Before this change: 122.1Mb/sec
    After this change:  125.07Mb/sec
    (averages of 5 test runs)
    
    Test machine: quad core intel i5-3570K, 32Gb of ram, SSD
    
    Signed-off-by: Filipe David Borba Manana <fdmanana@gmail.com>
    Signed-off-by: Josef Bacik <jbacik@fb.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 64d08f94485d..1874aee69c86 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -318,6 +318,20 @@ void clear_em_logging(struct extent_map_tree *tree, struct extent_map *em)
 		try_merge_map(tree, em);
 }
 
+static inline void setup_extent_mapping(struct extent_map_tree *tree,
+					struct extent_map *em,
+					int modified)
+{
+	atomic_inc(&em->refs);
+	em->mod_start = em->start;
+	em->mod_len = em->len;
+
+	if (modified)
+		list_move(&em->list, &tree->modified_extents);
+	else
+		try_merge_map(tree, em);
+}
+
 /**
  * add_extent_mapping - add new extent map to the extent tree
  * @tree:	tree to insert new map in
@@ -337,15 +351,7 @@ int add_extent_mapping(struct extent_map_tree *tree,
 	if (ret)
 		goto out;
 
-	atomic_inc(&em->refs);
-
-	em->mod_start = em->start;
-	em->mod_len = em->len;
-
-	if (modified)
-		list_move(&em->list, &tree->modified_extents);
-	else
-		try_merge_map(tree, em);
+	setup_extent_mapping(tree, em, modified);
 out:
 	return ret;
 }
@@ -432,3 +438,18 @@ int remove_extent_mapping(struct extent_map_tree *tree, struct extent_map *em)
 	RB_CLEAR_NODE(&em->rb_node);
 	return ret;
 }
+
+void replace_extent_mapping(struct extent_map_tree *tree,
+			    struct extent_map *cur,
+			    struct extent_map *new,
+			    int modified)
+{
+	WARN_ON(test_bit(EXTENT_FLAG_PINNED, &cur->flags));
+	ASSERT(extent_map_in_tree(cur));
+	if (!test_bit(EXTENT_FLAG_LOGGING, &cur->flags))
+		list_del_init(&cur->list);
+	rb_replace_node(&cur->rb_node, &new->rb_node, &tree->map);
+	RB_CLEAR_NODE(&cur->rb_node);
+
+	setup_extent_mapping(tree, new, modified);
+}

commit cbc0e9287d710ce7dce5f8daf667729e83316c45
Author: Filipe Manana <fdmanana@gmail.com>
Date:   Tue Feb 25 14:15:12 2014 +0000

    Btrfs: remove unneeded field / smaller extent_map structure
    
    We don't need to have an unsigned int field in the extent_map struct
    to tell us whether the extent map is in the inode's extent_map tree or
    not. We can use the rb_node struct field and the RB_CLEAR_NODE and
    RB_EMPTY_NODE macros to achieve the same task.
    
    This reduces sizeof(struct extent_map) from 152 bytes to 144 bytes (on a
    64 bits system).
    
    Signed-off-by: Filipe David Borba Manana <fdmanana@gmail.com>
    Reviewed-by: David Sterba <dsterba@suse.cz>
    Signed-off-by: Josef Bacik <jbacik@fb.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 996ad56b57db..64d08f94485d 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -51,7 +51,7 @@ struct extent_map *alloc_extent_map(void)
 	em = kmem_cache_zalloc(extent_map_cache, GFP_NOFS);
 	if (!em)
 		return NULL;
-	em->in_tree = 0;
+	RB_CLEAR_NODE(&em->rb_node);
 	em->flags = 0;
 	em->compress_type = BTRFS_COMPRESS_NONE;
 	em->generation = 0;
@@ -73,7 +73,7 @@ void free_extent_map(struct extent_map *em)
 		return;
 	WARN_ON(atomic_read(&em->refs) == 0);
 	if (atomic_dec_and_test(&em->refs)) {
-		WARN_ON(em->in_tree);
+		WARN_ON(extent_map_in_tree(em));
 		WARN_ON(!list_empty(&em->list));
 		kmem_cache_free(extent_map_cache, em);
 	}
@@ -99,8 +99,6 @@ static int tree_insert(struct rb_root *root, struct extent_map *em)
 		parent = *p;
 		entry = rb_entry(parent, struct extent_map, rb_node);
 
-		WARN_ON(!entry->in_tree);
-
 		if (em->start < entry->start)
 			p = &(*p)->rb_left;
 		else if (em->start >= extent_map_end(entry))
@@ -128,7 +126,6 @@ static int tree_insert(struct rb_root *root, struct extent_map *em)
 		if (end > entry->start && em->start < extent_map_end(entry))
 			return -EEXIST;
 
-	em->in_tree = 1;
 	rb_link_node(&em->rb_node, orig_parent, p);
 	rb_insert_color(&em->rb_node, root);
 	return 0;
@@ -153,8 +150,6 @@ static struct rb_node *__tree_search(struct rb_root *root, u64 offset,
 		prev = n;
 		prev_entry = entry;
 
-		WARN_ON(!entry->in_tree);
-
 		if (offset < entry->start)
 			n = n->rb_left;
 		else if (offset >= extent_map_end(entry))
@@ -240,12 +235,12 @@ static void try_merge_map(struct extent_map_tree *tree, struct extent_map *em)
 			em->len += merge->len;
 			em->block_len += merge->block_len;
 			em->block_start = merge->block_start;
-			merge->in_tree = 0;
 			em->mod_len = (em->mod_len + em->mod_start) - merge->mod_start;
 			em->mod_start = merge->mod_start;
 			em->generation = max(em->generation, merge->generation);
 
 			rb_erase(&merge->rb_node, &tree->map);
+			RB_CLEAR_NODE(&merge->rb_node);
 			free_extent_map(merge);
 		}
 	}
@@ -257,7 +252,7 @@ static void try_merge_map(struct extent_map_tree *tree, struct extent_map *em)
 		em->len += merge->len;
 		em->block_len += merge->block_len;
 		rb_erase(&merge->rb_node, &tree->map);
-		merge->in_tree = 0;
+		RB_CLEAR_NODE(&merge->rb_node);
 		em->mod_len = (merge->mod_start + merge->mod_len) - em->mod_start;
 		em->generation = max(em->generation, merge->generation);
 		free_extent_map(merge);
@@ -319,7 +314,7 @@ int unpin_extent_cache(struct extent_map_tree *tree, u64 start, u64 len,
 void clear_em_logging(struct extent_map_tree *tree, struct extent_map *em)
 {
 	clear_bit(EXTENT_FLAG_LOGGING, &em->flags);
-	if (em->in_tree)
+	if (extent_map_in_tree(em))
 		try_merge_map(tree, em);
 }
 
@@ -434,6 +429,6 @@ int remove_extent_mapping(struct extent_map_tree *tree, struct extent_map *em)
 	rb_erase(&em->rb_node, &tree->map);
 	if (!test_bit(EXTENT_FLAG_LOGGING, &em->flags))
 		list_del_init(&em->list);
-	em->in_tree = 0;
+	RB_CLEAR_NODE(&em->rb_node);
 	return ret;
 }

commit d527afe1e5865d25a336f6c4157f1086cfbddc81
Author: Filipe David Borba Manana <fdmanana@gmail.com>
Date:   Sat Nov 30 11:28:35 2013 +0000

    Btrfs: fix extent_map block_len after merging
    
    When merging an extent_map with its right neighbor, increment
    its block_len with the neighbor's block_len.
    
    Signed-off-by: Filipe David Borba Manana <fdmanana@gmail.com>
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index b60955d4f9bf..996ad56b57db 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -255,7 +255,7 @@ static void try_merge_map(struct extent_map_tree *tree, struct extent_map *em)
 		merge = rb_entry(rb, struct extent_map, rb_node);
 	if (rb && mergable_maps(em, merge)) {
 		em->len += merge->len;
-		em->block_len += merge->len;
+		em->block_len += merge->block_len;
 		rb_erase(&merge->rb_node, &tree->map);
 		merge->in_tree = 0;
 		em->mod_len = (merge->mod_start + merge->mod_len) - em->mod_start;

commit 32193c147f451652c6c089b5fa1c9852d53d65ee
Author: Filipe David Borba Manana <fdmanana@gmail.com>
Date:   Mon Nov 25 03:23:51 2013 +0000

    Btrfs: faster and more efficient extent map insertion
    
    Before this change, adding an extent map to the extent map tree of an
    inode required 2 tree nevigations:
    
    1) doing a tree navigation to search for an existing extent map starting
       at the same offset or an extent map that overlaps the extent map we
       want to insert;
    
    2) Another tree navigation to add the extent map to the tree (if the
       former tree search didn't found anything).
    
    This change just merges these 2 steps into a single one.
    While running first few btrfs xfstests I had noticed these trees easily
    had a few hundred elements, and then with the following sysbench test it
    reached over 1100 elements very often.
    
    Test:
    
      sysbench --test=fileio --file-num=32 --file-total-size=10G \
        --file-test-mode=seqwr --num-threads=512 --file-block-size=8192 \
        --max-requests=1000000 --file-io-mode=sync [prepare|run]
    
    (fs created with mkfs.btrfs -l 4096 -f /dev/sdb3 before each sysbench
    prepare phase)
    
    Before this patch:
    
    run 1 - 41.894Mb/sec
    run 2 - 40.527Mb/sec
    run 3 - 40.922Mb/sec
    run 4 - 49.433Mb/sec
    run 5 - 40.959Mb/sec
    
    average - 42.75Mb/sec
    
    After this patch:
    
    run 1 - 48.036Mb/sec
    run 2 - 50.21Mb/sec
    run 3 - 50.929Mb/sec
    run 4 - 46.881Mb/sec
    run 5 - 53.192Mb/sec
    
    average - 49.85Mb/sec
    
    Signed-off-by: Filipe David Borba Manana <fdmanana@gmail.com>
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index a4a7a1a8da95..b60955d4f9bf 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -79,12 +79,21 @@ void free_extent_map(struct extent_map *em)
 	}
 }
 
-static struct rb_node *tree_insert(struct rb_root *root, u64 offset,
-				   struct rb_node *node)
+/* simple helper to do math around the end of an extent, handling wrap */
+static u64 range_end(u64 start, u64 len)
+{
+	if (start + len < start)
+		return (u64)-1;
+	return start + len;
+}
+
+static int tree_insert(struct rb_root *root, struct extent_map *em)
 {
 	struct rb_node **p = &root->rb_node;
 	struct rb_node *parent = NULL;
-	struct extent_map *entry;
+	struct extent_map *entry = NULL;
+	struct rb_node *orig_parent = NULL;
+	u64 end = range_end(em->start, em->len);
 
 	while (*p) {
 		parent = *p;
@@ -92,19 +101,37 @@ static struct rb_node *tree_insert(struct rb_root *root, u64 offset,
 
 		WARN_ON(!entry->in_tree);
 
-		if (offset < entry->start)
+		if (em->start < entry->start)
 			p = &(*p)->rb_left;
-		else if (offset >= extent_map_end(entry))
+		else if (em->start >= extent_map_end(entry))
 			p = &(*p)->rb_right;
 		else
-			return parent;
+			return -EEXIST;
 	}
 
-	entry = rb_entry(node, struct extent_map, rb_node);
-	entry->in_tree = 1;
-	rb_link_node(node, parent, p);
-	rb_insert_color(node, root);
-	return NULL;
+	orig_parent = parent;
+	while (parent && em->start >= extent_map_end(entry)) {
+		parent = rb_next(parent);
+		entry = rb_entry(parent, struct extent_map, rb_node);
+	}
+	if (parent)
+		if (end > entry->start && em->start < extent_map_end(entry))
+			return -EEXIST;
+
+	parent = orig_parent;
+	entry = rb_entry(parent, struct extent_map, rb_node);
+	while (parent && em->start < entry->start) {
+		parent = rb_prev(parent);
+		entry = rb_entry(parent, struct extent_map, rb_node);
+	}
+	if (parent)
+		if (end > entry->start && em->start < extent_map_end(entry))
+			return -EEXIST;
+
+	em->in_tree = 1;
+	rb_link_node(&em->rb_node, orig_parent, p);
+	rb_insert_color(&em->rb_node, root);
+	return 0;
 }
 
 /*
@@ -310,20 +337,11 @@ int add_extent_mapping(struct extent_map_tree *tree,
 		       struct extent_map *em, int modified)
 {
 	int ret = 0;
-	struct rb_node *rb;
-	struct extent_map *exist;
 
-	exist = lookup_extent_mapping(tree, em->start, em->len);
-	if (exist) {
-		free_extent_map(exist);
-		ret = -EEXIST;
-		goto out;
-	}
-	rb = tree_insert(&tree->map, em->start, &em->rb_node);
-	if (rb) {
-		ret = -EEXIST;
+	ret = tree_insert(&tree->map, em);
+	if (ret)
 		goto out;
-	}
+
 	atomic_inc(&em->refs);
 
 	em->mod_start = em->start;
@@ -337,14 +355,6 @@ int add_extent_mapping(struct extent_map_tree *tree,
 	return ret;
 }
 
-/* simple helper to do math around the end of an extent, handling wrap */
-static u64 range_end(u64 start, u64 len)
-{
-	if (start + len < start)
-		return (u64)-1;
-	return start + len;
-}
-
 static struct extent_map *
 __lookup_extent_mapping(struct extent_map_tree *tree,
 			u64 start, u64 len, int strict)

commit 48a3b6366f6913683563d934eb16fea67dead9c1
Author: Eric Sandeen <sandeen@redhat.com>
Date:   Thu Apr 25 20:41:01 2013 +0000

    btrfs: make static code static & remove dead code
    
    Big patch, but all it does is add statics to functions which
    are in fact static, then remove the associated dead-code fallout.
    
    removed functions:
    
    btrfs_iref_to_path()
    __btrfs_lookup_delayed_deletion_item()
    __btrfs_search_delayed_insertion_item()
    __btrfs_search_delayed_deletion_item()
    find_eb_for_page()
    btrfs_find_block_group()
    range_straddles_pages()
    extent_range_uptodate()
    btrfs_file_extent_length()
    btrfs_scrub_cancel_devid()
    btrfs_start_transaction_lflush()
    
    btrfs_print_tree() is left because it is used for debugging.
    btrfs_start_transaction_lflush() and btrfs_reada_detach() are
    left for symmetry.
    
    ulist.c functions are left, another patch will take care of those.
    
    Signed-off-by: Eric Sandeen <sandeen@redhat.com>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index ca968742c3db..a4a7a1a8da95 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -345,8 +345,9 @@ static u64 range_end(u64 start, u64 len)
 	return start + len;
 }
 
-struct extent_map *__lookup_extent_mapping(struct extent_map_tree *tree,
-					   u64 start, u64 len, int strict)
+static struct extent_map *
+__lookup_extent_mapping(struct extent_map_tree *tree,
+			u64 start, u64 len, int strict)
 {
 	struct extent_map *em;
 	struct rb_node *rb_node;

commit 09a2a8f96e3009273bed1833b3f210e2c68728a5
Author: Josef Bacik <jbacik@fusionio.com>
Date:   Fri Apr 5 16:51:15 2013 -0400

    Btrfs: fix bad extent logging
    
    A user sent me a btrfs-image of a file system that was panicing on mount during
    the log recovery.  I had originally thought these problems were from a bug in
    the free space cache code, but that was just a symptom of the problem.  The
    problem is if your application does something like this
    
    [prealloc][prealloc][prealloc]
    
    the internal extent maps will merge those all together into one extent map, even
    though on disk they are 3 separate extents.  So if you go to write into one of
    these ranges the extent map will be right since we use the physical extent when
    doing the write, but when we log the extents they will use the wrong sizes for
    the remainder prealloc space.  If this doesn't happen to trip up the free space
    cache (which it won't in a lot of cases) then you will get bogus entries in your
    extent tree which will screw stuff up later.  The data and such will still work,
    but everything else is broken.  This patch fixes this by not allowing extents
    that are on the modified list to be merged.  This has the side effect that we
    are no longer adding everything to the modified list all the time, which means
    we now have to call btrfs_drop_extents every time we log an extent into the
    tree.  So this allows me to drop all this speciality code I was using to get
    around calling btrfs_drop_extents.  With this patch the testcase I've created no
    longer creates a bogus file system after replaying the log.  Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 2834ca5768ea..ca968742c3db 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -174,6 +174,14 @@ static int mergable_maps(struct extent_map *prev, struct extent_map *next)
 	    test_bit(EXTENT_FLAG_LOGGING, &next->flags))
 		return 0;
 
+	/*
+	 * We don't want to merge stuff that hasn't been written to the log yet
+	 * since it may not reflect exactly what is on disk, and that would be
+	 * bad.
+	 */
+	if (!list_empty(&prev->list) || !list_empty(&next->list))
+		return 0;
+
 	if (extent_map_end(prev) == next->start &&
 	    prev->flags == next->flags &&
 	    prev->bdev == next->bdev &&
@@ -209,9 +217,7 @@ static void try_merge_map(struct extent_map_tree *tree, struct extent_map *em)
 			em->mod_len = (em->mod_len + em->mod_start) - merge->mod_start;
 			em->mod_start = merge->mod_start;
 			em->generation = max(em->generation, merge->generation);
-			list_move(&em->list, &tree->modified_extents);
 
-			list_del_init(&merge->list);
 			rb_erase(&merge->rb_node, &tree->map);
 			free_extent_map(merge);
 		}
@@ -227,7 +233,6 @@ static void try_merge_map(struct extent_map_tree *tree, struct extent_map *em)
 		merge->in_tree = 0;
 		em->mod_len = (merge->mod_start + merge->mod_len) - em->mod_start;
 		em->generation = max(em->generation, merge->generation);
-		list_del_init(&merge->list);
 		free_extent_map(merge);
 	}
 }
@@ -302,7 +307,7 @@ void clear_em_logging(struct extent_map_tree *tree, struct extent_map *em)
  * reference dropped if the merge attempt was successful.
  */
 int add_extent_mapping(struct extent_map_tree *tree,
-		       struct extent_map *em)
+		       struct extent_map *em, int modified)
 {
 	int ret = 0;
 	struct rb_node *rb;
@@ -324,7 +329,10 @@ int add_extent_mapping(struct extent_map_tree *tree,
 	em->mod_start = em->start;
 	em->mod_len = em->len;
 
-	try_merge_map(tree, em);
+	if (modified)
+		list_move(&em->list, &tree->modified_extents);
+	else
+		try_merge_map(tree, em);
 out:
 	return ret;
 }

commit 180e001cd5fc2950dc6a7997dde5b65c954d0e79
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Thu Feb 14 13:50:15 2013 -0700

    btrfs: fixup/remove module.h usage as required
    
    We want to avoid module.h where posible, since it in turn includes
    nearly all of header space.  This means removing it where it is not
    required, and using export.h where we are only exporting symbols via
    EXPORT_SYMBOL and friends.
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index fdb7a8db3b57..2834ca5768ea 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -1,6 +1,5 @@
 #include <linux/err.h>
 #include <linux/slab.h>
-#include <linux/module.h>
 #include <linux/spinlock.h>
 #include <linux/hardirq.h>
 #include "ctree.h"

commit 8d19514fade54798106a60059c539501eda31b47
Merge: 95436adaa0f9 1a65e24b0bb7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Feb 8 12:06:46 2013 +1100

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs
    
    Pull btrfs fixes from Chris Mason:
     "We've got corner cases for updating i_size that ceph was hitting,
      error handling for quotas when we run out of space, a very subtle
      snapshot deletion race, a crash while removing devices, and one
      deadlock between subvolume creation and the sb_internal code (thanks
      lockdep)."
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs:
      Btrfs: move d_instantiate outside the transaction during mksubvol
      Btrfs: fix EDQUOT handling in btrfs_delalloc_reserve_metadata
      Btrfs: fix possible stale data exposure
      Btrfs: fix missing i_size update
      Btrfs: fix race between snapshot deletion and getting inode
      Btrfs: fix missing release of the space/qgroup reservation in start_transaction()
      Btrfs: fix wrong sync_writers decrement in btrfs_file_aio_write()
      Btrfs: do not merge logged extents if we've removed them from the tree
      btrfs: don't try to notify udev about missing devices

commit 222c81dc3874b4fe98371be665d0447a36447653
Author: Josef Bacik <jbacik@fusionio.com>
Date:   Mon Jan 28 09:45:20 2013 -0500

    Btrfs: do not merge logged extents if we've removed them from the tree
    
    You can run into this problem where if somebody is fsyncing and writing out
    the existing extents you will have removed the extent map from the em tree,
    but it's still valid for the current fsync so we go ahead and write it.  The
    problem is we unconditionally try to merge it back into the em tree, but if
    we've removed it from the em tree that will cause use after free problems.
    Fix this to only merge if we are still a part of the tree.  Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index ed88f5ee4bea..9759911dd34e 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -289,7 +289,8 @@ int unpin_extent_cache(struct extent_map_tree *tree, u64 start, u64 len,
 void clear_em_logging(struct extent_map_tree *tree, struct extent_map *em)
 {
 	clear_bit(EXTENT_FLAG_LOGGING, &em->flags);
-	try_merge_map(tree, em);
+	if (em->in_tree)
+		try_merge_map(tree, em);
 }
 
 /**

commit d7df025eb4c3c571532326b01e007be52c75e5c0
Merge: 66e2d3e8c229 1eafa6c73791
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jan 25 10:55:21 2013 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs
    
    Pull btrfs fixes from Chris Mason:
     "It turns out that we had two crc bugs when running fsx-linux in a
      loop.  Many thanks to Josef, Miao Xie, and Dave Sterba for nailing it
      all down.  Miao also has a new OOM fix in this v2 pull as well.
    
      Ilya fixed a regression Liu Bo found in the balance ioctls for pausing
      and resuming a running balance across drives.
    
      Josef's orphan truncate patch fixes an obscure corruption we'd see
      during xfstests.
    
      Arne's patches address problems with subvolume quotas.  If the user
      destroys quota groups incorrectly the FS will refuse to mount.
    
      The rest are smaller fixes and plugs for memory leaks."
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs: (30 commits)
      Btrfs: fix repeated delalloc work allocation
      Btrfs: fix wrong max device number for single profile
      Btrfs: fix missed transaction->aborted check
      Btrfs: Add ACCESS_ONCE() to transaction->abort accesses
      Btrfs: put csums on the right ordered extent
      Btrfs: use right range to find checksum for compressed extents
      Btrfs: fix panic when recovering tree log
      Btrfs: do not allow logged extents to be merged or removed
      Btrfs: fix a regression in balance usage filter
      Btrfs: prevent qgroup destroy when there are still relations
      Btrfs: ignore orphan qgroup relations
      Btrfs: reorder locks and sanity checks in btrfs_ioctl_defrag
      Btrfs: fix unlock order in btrfs_ioctl_rm_dev
      Btrfs: fix unlock order in btrfs_ioctl_resize
      Btrfs: fix "mutually exclusive op is running" error code
      Btrfs: bring back balance pause/resume logic
      btrfs: update timestamps on truncate()
      btrfs: fix btrfs_cont_expand() freeing IS_ERR em
      Btrfs: fix a bug when llseek for delalloc bytes behind prealloc extents
      Btrfs: fix off-by-one in lseek
      ...

commit 201a90389424d6771d24fc5d72f7e34cb4a8f967
Author: Josef Bacik <jbacik@fusionio.com>
Date:   Thu Jan 24 12:02:07 2013 -0500

    Btrfs: do not allow logged extents to be merged or removed
    
    We drop the extent map tree lock while we're logging extents, so somebody
    could come in and merge another extent into this one and screw up our
    logging, or they could even remove us from the list which would keep us from
    logging the extent or freeing our ref on it, so we need to make sure to not
    clear LOGGING until after the extent is logged, and then we can merge it to
    adjacent extents.  Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index fff2c28497b6..ed88f5ee4bea 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -171,6 +171,10 @@ static int mergable_maps(struct extent_map *prev, struct extent_map *next)
 	if (test_bit(EXTENT_FLAG_COMPRESSED, &prev->flags))
 		return 0;
 
+	if (test_bit(EXTENT_FLAG_LOGGING, &prev->flags) ||
+	    test_bit(EXTENT_FLAG_LOGGING, &next->flags))
+		return 0;
+
 	if (extent_map_end(prev) == next->start &&
 	    prev->flags == next->flags &&
 	    prev->bdev == next->bdev &&
@@ -256,7 +260,8 @@ int unpin_extent_cache(struct extent_map_tree *tree, u64 start, u64 len,
 	if (!em)
 		goto out;
 
-	list_move(&em->list, &tree->modified_extents);
+	if (!test_bit(EXTENT_FLAG_LOGGING, &em->flags))
+		list_move(&em->list, &tree->modified_extents);
 	em->generation = gen;
 	clear_bit(EXTENT_FLAG_PINNED, &em->flags);
 	em->mod_start = em->start;
@@ -281,6 +286,12 @@ int unpin_extent_cache(struct extent_map_tree *tree, u64 start, u64 len,
 
 }
 
+void clear_em_logging(struct extent_map_tree *tree, struct extent_map *em)
+{
+	clear_bit(EXTENT_FLAG_LOGGING, &em->flags);
+	try_merge_map(tree, em);
+}
+
 /**
  * add_extent_mapping - add new extent map to the extent tree
  * @tree:	tree to insert new map in

commit a22180d2666c018f4fef6818074d78bb76ff2bda
Merge: 2d4dce007044 213490b30177
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Dec 18 09:42:05 2012 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs
    
    Pull btrfs update from Chris Mason:
     "A big set of fixes and features.
    
      In terms of line count, most of the code comes from Stefan, who added
      the ability to replace a single drive in place.  This is different
      from how btrfs normally replaces drives, and is much much much faster.
    
      Josef is plowing through our synchronous write performance.  This pull
      request does not include the DIO_OWN_WAITING patch that was discussed
      on the list, but it has a number of other improvements to cut down our
      latencies and CPU time during fsync/O_DIRECT writes.
    
      Miao Xie has a big series of fixes and is spreading out ordered
      operations over more CPUs.  This improves performance and reduces
      contention.
    
      I've put in fixes for error handling around hash collisions.  These
      are going back to individual stable kernels as I test against them.
    
      Otherwise we have a lot of fixes and cleanups, thanks everyone!
      raid5/6 is being rebased against the device replacement code.  I'll
      have it posted this Friday along with a nice series of benchmarks."
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs: (115 commits)
      Btrfs: fix a bug of per-file nocow
      Btrfs: fix hash overflow handling
      Btrfs: don't take inode delalloc mutex if we're a free space inode
      Btrfs: fix autodefrag and umount lockup
      Btrfs: fix permissions of empty files not affected by umask
      Btrfs: put raid properties into global table
      Btrfs: fix BUG() in scrub when first superblock reading gives EIO
      Btrfs: do not call file_update_time in aio_write
      Btrfs: only unlock and relock if we have to
      Btrfs: use tokens where we can in the tree log
      Btrfs: optimize leaf_space_used
      Btrfs: don't memset new tokens
      Btrfs: only clear dirty on the buffer if it is marked as dirty
      Btrfs: move checks in set_page_dirty under DEBUG
      Btrfs: log changed inodes based on the extent map tree
      Btrfs: add path->really_keep_locks
      Btrfs: do not mark ems as prealloc if we are writing to them
      Btrfs: keep track of the extents original block length
      Btrfs: inline csums if we're fsyncing
      Btrfs: don't bother copying if we're only logging the inode
      ...

commit 70c8a91ce21b83ccd2d9e7c968775430ead4353d
Author: Josef Bacik <jbacik@fusionio.com>
Date:   Thu Oct 11 16:54:30 2012 -0400

    Btrfs: log changed inodes based on the extent map tree
    
    We don't really need to copy extents from the source tree since we have all
    of the information already available to us in the extent_map tree.  So
    instead just write the extents straight to the log tree and don't bother to
    copy the extent items from the source tree.
    
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 85ae2b6fe03b..fff2c28497b6 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -49,7 +49,7 @@ void extent_map_tree_init(struct extent_map_tree *tree)
 struct extent_map *alloc_extent_map(void)
 {
 	struct extent_map *em;
-	em = kmem_cache_alloc(extent_map_cache, GFP_NOFS);
+	em = kmem_cache_zalloc(extent_map_cache, GFP_NOFS);
 	if (!em)
 		return NULL;
 	em->in_tree = 0;
@@ -198,16 +198,15 @@ static void try_merge_map(struct extent_map_tree *tree, struct extent_map *em)
 			merge = rb_entry(rb, struct extent_map, rb_node);
 		if (rb && mergable_maps(merge, em)) {
 			em->start = merge->start;
+			em->orig_start = merge->orig_start;
 			em->len += merge->len;
 			em->block_len += merge->block_len;
 			em->block_start = merge->block_start;
 			merge->in_tree = 0;
-			if (merge->generation > em->generation) {
-				em->mod_start = em->start;
-				em->mod_len = em->len;
-				em->generation = merge->generation;
-				list_move(&em->list, &tree->modified_extents);
-			}
+			em->mod_len = (em->mod_len + em->mod_start) - merge->mod_start;
+			em->mod_start = merge->mod_start;
+			em->generation = max(em->generation, merge->generation);
+			list_move(&em->list, &tree->modified_extents);
 
 			list_del_init(&merge->list);
 			rb_erase(&merge->rb_node, &tree->map);
@@ -223,11 +222,8 @@ static void try_merge_map(struct extent_map_tree *tree, struct extent_map *em)
 		em->block_len += merge->len;
 		rb_erase(&merge->rb_node, &tree->map);
 		merge->in_tree = 0;
-		if (merge->generation > em->generation) {
-			em->mod_len = em->len;
-			em->generation = merge->generation;
-			list_move(&em->list, &tree->modified_extents);
-		}
+		em->mod_len = (merge->mod_start + merge->mod_len) - em->mod_start;
+		em->generation = max(em->generation, merge->generation);
 		list_del_init(&merge->list);
 		free_extent_map(merge);
 	}

commit b11e234d21e73df94099e473a080bca502b9a496
Author: Josef Bacik <jbacik@fusionio.com>
Date:   Mon Dec 3 10:58:15 2012 -0500

    Btrfs: do not mark ems as prealloc if we are writing to them
    
    We are going to use EM's to log extents in the future, so we need to not
    mark them as prealloc if they aren't actually prealloc extents.  Instead
    mark them with FILLING so we know to ammend mod_start/mod_len and that way
    we don't confuse the extent logging code.  Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index b8cbc8d5c7f7..85ae2b6fe03b 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -266,9 +266,9 @@ int unpin_extent_cache(struct extent_map_tree *tree, u64 start, u64 len,
 	em->mod_start = em->start;
 	em->mod_len = em->len;
 
-	if (test_bit(EXTENT_FLAG_PREALLOC, &em->flags)) {
+	if (test_bit(EXTENT_FLAG_FILLING, &em->flags)) {
 		prealloc = true;
-		clear_bit(EXTENT_FLAG_PREALLOC, &em->flags);
+		clear_bit(EXTENT_FLAG_FILLING, &em->flags);
 	}
 
 	try_merge_map(tree, em);

commit 52b1de91ea07d1f5600b3cae789d5be9a1f61787
Author: Liu Bo <liub.liubo@gmail.com>
Date:   Tue Oct 30 17:13:52 2012 +0800

    btrfs: unpin_extent_cache: fix the typo and unnecessary arguements
    
    - unpint->unpin
    - prealloc is no more used
    
    Signed-off-by: Liu Bo <liub.liubo@gmail.com>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index b8cbc8d5c7f7..ce9f79216723 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -234,12 +234,11 @@ static void try_merge_map(struct extent_map_tree *tree, struct extent_map *em)
 }
 
 /**
- * unpint_extent_cache - unpin an extent from the cache
+ * unpin_extent_cache - unpin an extent from the cache
  * @tree:	tree to unpin the extent in
  * @start:	logical offset in the file
  * @len:	length of the extent
  * @gen:	generation that this extent has been modified in
- * @prealloc:	if this is set we need to clear the prealloc flag
  *
  * Called after an extent has been written to disk properly.  Set the generation
  * to the generation that actually added the file item to the inode so we know

commit ff44c6e36dc9dcc02652a1105b120bdf08cea9f7
Author: Josef Bacik <jbacik@fusionio.com>
Date:   Fri Sep 14 12:59:20 2012 -0400

    Btrfs: do not hold the write_lock on the extent tree while logging
    
    Dave Sterba pointed out a sleeping while atomic bug while doing fsync.  This
    is because I'm an idiot and didn't realize that rwlock's were spin locks, so
    we've been holding this thing while doing allocations and such which is not
    good.  This patch fixes this by dropping the write lock before we do
    anything heavy and re-acquire it when it is done.  We also need to take a
    ref on the em's in case their corresponding pages are evicted and mark them
    as being logged so that releasepage does not remove them and doesn't remove
    them from our local list.  Thanks,
    
    Reported-by: Dave Sterba <dave@jikos.cz>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 8d1364d385d6..b8cbc8d5c7f7 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -407,7 +407,8 @@ int remove_extent_mapping(struct extent_map_tree *tree, struct extent_map *em)
 
 	WARN_ON(test_bit(EXTENT_FLAG_PINNED, &em->flags));
 	rb_erase(&em->rb_node, &tree->map);
-	list_del_init(&em->list);
+	if (!test_bit(EXTENT_FLAG_LOGGING, &em->flags))
+		list_del_init(&em->list);
 	em->in_tree = 0;
 	return ret;
 }

commit 837e197283199de640857192ca32767cb6e24fe8
Author: David Sterba <dsterba@suse.cz>
Date:   Fri Sep 7 03:00:48 2012 -0600

    btrfs: polish names of kmem caches
    
    Usecase:
    
      watch 'grep btrfs < /proc/slabinfo'
    
    easy to watch all caches in one go.
    
    Signed-off-by: David Sterba <dsterba@suse.cz>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index ac606f076eb7..8d1364d385d6 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -11,7 +11,7 @@ static struct kmem_cache *extent_map_cache;
 
 int __init extent_map_init(void)
 {
-	extent_map_cache = kmem_cache_create("extent_map",
+	extent_map_cache = kmem_cache_create("btrfs_extent_map",
 			sizeof(struct extent_map), 0,
 			SLAB_RECLAIM_ACCOUNT | SLAB_MEM_SPREAD, NULL);
 	if (!extent_map_cache)

commit 4e2f84e63dc138eca91e89ccbc34f37732ce58f7
Author: Liu Bo <bo.li.liu@oracle.com>
Date:   Mon Aug 27 10:52:20 2012 -0600

    Btrfs: improve fsync by filtering extents that we want
    
    This is based on Josef's "Btrfs: turbo charge fsync".
    
    The above Josef's patch performs very good in random sync write test,
    because we won't have too much extents to merge.
    
    However, it does not performs good on the test:
    dd if=/dev/zero of=foobar bs=4k count=12500 oflag=sync
    
    The reason is when we do sequencial sync write, we need to merge the
    current extent just with the previous one, so that we can get accumulated
    extents to log:
    
    A(4k) --> AA(8k) --> AAA(12k) --> AAAA(16k) ...
    
    So we'll have to flush more and more checksum into log tree, which is the
    bottleneck according to my tests.
    
    But we can avoid this by telling fsync the real extents that are needed
    to be logged.
    
    With this, I did the above dd sync write test (size=50m),
    
             w/o (orig)   w/ (josef's)   w/ (this)
    SATA      104KB/s       109KB/s       121KB/s
    ramdisk   1.5MB/s       1.5MB/s       10.7MB/s (613%)
    
    Signed-off-by: Liu Bo <bo.li.liu@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 1fe82cfc1d93..ac606f076eb7 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -203,6 +203,8 @@ static void try_merge_map(struct extent_map_tree *tree, struct extent_map *em)
 			em->block_start = merge->block_start;
 			merge->in_tree = 0;
 			if (merge->generation > em->generation) {
+				em->mod_start = em->start;
+				em->mod_len = em->len;
 				em->generation = merge->generation;
 				list_move(&em->list, &tree->modified_extents);
 			}
@@ -222,6 +224,7 @@ static void try_merge_map(struct extent_map_tree *tree, struct extent_map *em)
 		rb_erase(&merge->rb_node, &tree->map);
 		merge->in_tree = 0;
 		if (merge->generation > em->generation) {
+			em->mod_len = em->len;
 			em->generation = merge->generation;
 			list_move(&em->list, &tree->modified_extents);
 		}
@@ -247,6 +250,7 @@ int unpin_extent_cache(struct extent_map_tree *tree, u64 start, u64 len,
 {
 	int ret = 0;
 	struct extent_map *em;
+	bool prealloc = false;
 
 	write_lock(&tree->lock);
 	em = lookup_extent_mapping(tree, start, len);
@@ -259,8 +263,21 @@ int unpin_extent_cache(struct extent_map_tree *tree, u64 start, u64 len,
 	list_move(&em->list, &tree->modified_extents);
 	em->generation = gen;
 	clear_bit(EXTENT_FLAG_PINNED, &em->flags);
+	em->mod_start = em->start;
+	em->mod_len = em->len;
+
+	if (test_bit(EXTENT_FLAG_PREALLOC, &em->flags)) {
+		prealloc = true;
+		clear_bit(EXTENT_FLAG_PREALLOC, &em->flags);
+	}
 
 	try_merge_map(tree, em);
+
+	if (prealloc) {
+		em->mod_start = em->start;
+		em->mod_len = em->len;
+	}
+
 	free_extent_map(em);
 out:
 	write_unlock(&tree->lock);
@@ -298,6 +315,9 @@ int add_extent_mapping(struct extent_map_tree *tree,
 	}
 	atomic_inc(&em->refs);
 
+	em->mod_start = em->start;
+	em->mod_len = em->len;
+
 	try_merge_map(tree, em);
 out:
 	return ret;

commit 5dc562c541e1026df9d43913c2f6b91156e22d32
Author: Josef Bacik <jbacik@fusionio.com>
Date:   Fri Aug 17 13:14:17 2012 -0400

    Btrfs: turbo charge fsync
    
    At least for the vm workload.  Currently on fsync we will
    
    1) Truncate all items in the log tree for the given inode if they exist
    
    and
    
    2) Copy all items for a given inode into the log
    
    The problem with this is that for things like VMs you can have lots of
    extents from the fragmented writing behavior, and worst yet you may have
    only modified a few extents, not the entire thing.  This patch fixes this
    problem by tracking which transid modified our extent, and then when we do
    the tree logging we find all of the extents we've modified in our current
    transaction, sort them and commit them.  We also only truncate up to the
    xattrs of the inode and copy that stuff in normally, and then just drop any
    extents in the range we have that exist in the log already.  Here are some
    numbers of a 50 meg fio job that does random writes and fsync()s after every
    write
    
                    Original        Patched
    SATA drive      82KB/s          140KB/s
    Fusion drive    431KB/s         2532KB/s
    
    So around 2-6 times faster depending on your hardware.  There are a few
    corner cases, for example if you truncate at all we have to do it the old
    way since there is no way to be sure what is in the log is ok.  This
    probably could be done smarter, but if you write-fsync-truncate-write-fsync
    you deserve what you get.  All this work is in RAM of course so if your
    inode gets evicted from cache and you read it in and fsync it we'll do it
    the slow way if we are still in the same transaction that we last modified
    the inode in.
    
    The biggest cool part of this is that it requires no changes to the recovery
    code, so if you fsync with this patch and crash and load an old kernel, it
    will run the recovery and be a-ok.  I have tested this pretty thoroughly
    with an fsync tester and everything comes back fine, as well as xfstests.
    Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 7c97b3301459..1fe82cfc1d93 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -35,6 +35,7 @@ void extent_map_exit(void)
 void extent_map_tree_init(struct extent_map_tree *tree)
 {
 	tree->map = RB_ROOT;
+	INIT_LIST_HEAD(&tree->modified_extents);
 	rwlock_init(&tree->lock);
 }
 
@@ -54,7 +55,9 @@ struct extent_map *alloc_extent_map(void)
 	em->in_tree = 0;
 	em->flags = 0;
 	em->compress_type = BTRFS_COMPRESS_NONE;
+	em->generation = 0;
 	atomic_set(&em->refs, 1);
+	INIT_LIST_HEAD(&em->list);
 	return em;
 }
 
@@ -72,6 +75,7 @@ void free_extent_map(struct extent_map *em)
 	WARN_ON(atomic_read(&em->refs) == 0);
 	if (atomic_dec_and_test(&em->refs)) {
 		WARN_ON(em->in_tree);
+		WARN_ON(!list_empty(&em->list));
 		kmem_cache_free(extent_map_cache, em);
 	}
 }
@@ -198,6 +202,12 @@ static void try_merge_map(struct extent_map_tree *tree, struct extent_map *em)
 			em->block_len += merge->block_len;
 			em->block_start = merge->block_start;
 			merge->in_tree = 0;
+			if (merge->generation > em->generation) {
+				em->generation = merge->generation;
+				list_move(&em->list, &tree->modified_extents);
+			}
+
+			list_del_init(&merge->list);
 			rb_erase(&merge->rb_node, &tree->map);
 			free_extent_map(merge);
 		}
@@ -211,11 +221,29 @@ static void try_merge_map(struct extent_map_tree *tree, struct extent_map *em)
 		em->block_len += merge->len;
 		rb_erase(&merge->rb_node, &tree->map);
 		merge->in_tree = 0;
+		if (merge->generation > em->generation) {
+			em->generation = merge->generation;
+			list_move(&em->list, &tree->modified_extents);
+		}
+		list_del_init(&merge->list);
 		free_extent_map(merge);
 	}
 }
 
-int unpin_extent_cache(struct extent_map_tree *tree, u64 start, u64 len)
+/**
+ * unpint_extent_cache - unpin an extent from the cache
+ * @tree:	tree to unpin the extent in
+ * @start:	logical offset in the file
+ * @len:	length of the extent
+ * @gen:	generation that this extent has been modified in
+ * @prealloc:	if this is set we need to clear the prealloc flag
+ *
+ * Called after an extent has been written to disk properly.  Set the generation
+ * to the generation that actually added the file item to the inode so we know
+ * we need to sync this extent when we call fsync().
+ */
+int unpin_extent_cache(struct extent_map_tree *tree, u64 start, u64 len,
+		       u64 gen)
 {
 	int ret = 0;
 	struct extent_map *em;
@@ -228,10 +256,11 @@ int unpin_extent_cache(struct extent_map_tree *tree, u64 start, u64 len)
 	if (!em)
 		goto out;
 
+	list_move(&em->list, &tree->modified_extents);
+	em->generation = gen;
 	clear_bit(EXTENT_FLAG_PINNED, &em->flags);
 
 	try_merge_map(tree, em);
-
 	free_extent_map(em);
 out:
 	write_unlock(&tree->lock);
@@ -358,6 +387,7 @@ int remove_extent_mapping(struct extent_map_tree *tree, struct extent_map *em)
 
 	WARN_ON(test_bit(EXTENT_FLAG_PINNED, &em->flags));
 	rb_erase(&em->rb_node, &tree->map);
+	list_del_init(&em->list);
 	em->in_tree = 0;
 	return ret;
 }

commit 4d2c8f62f12a6652db67cc0c1f4a4a498b05ddbc
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Thu Jul 14 03:18:33 2011 +0000

    Btrfs: clean up code for merging extent maps
    
    unpin_extent_cache() and add_extent_mapping() shares the same code
    that merges extent maps.
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index df7a803005fb..7c97b3301459 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -183,22 +183,10 @@ static int mergable_maps(struct extent_map *prev, struct extent_map *next)
 	return 0;
 }
 
-int unpin_extent_cache(struct extent_map_tree *tree, u64 start, u64 len)
+static void try_merge_map(struct extent_map_tree *tree, struct extent_map *em)
 {
-	int ret = 0;
 	struct extent_map *merge = NULL;
 	struct rb_node *rb;
-	struct extent_map *em;
-
-	write_lock(&tree->lock);
-	em = lookup_extent_mapping(tree, start, len);
-
-	WARN_ON(!em || em->start != start);
-
-	if (!em)
-		goto out;
-
-	clear_bit(EXTENT_FLAG_PINNED, &em->flags);
 
 	if (em->start != 0) {
 		rb = rb_prev(&em->rb_node);
@@ -225,6 +213,24 @@ int unpin_extent_cache(struct extent_map_tree *tree, u64 start, u64 len)
 		merge->in_tree = 0;
 		free_extent_map(merge);
 	}
+}
+
+int unpin_extent_cache(struct extent_map_tree *tree, u64 start, u64 len)
+{
+	int ret = 0;
+	struct extent_map *em;
+
+	write_lock(&tree->lock);
+	em = lookup_extent_mapping(tree, start, len);
+
+	WARN_ON(!em || em->start != start);
+
+	if (!em)
+		goto out;
+
+	clear_bit(EXTENT_FLAG_PINNED, &em->flags);
+
+	try_merge_map(tree, em);
 
 	free_extent_map(em);
 out:
@@ -247,7 +253,6 @@ int add_extent_mapping(struct extent_map_tree *tree,
 		       struct extent_map *em)
 {
 	int ret = 0;
-	struct extent_map *merge = NULL;
 	struct rb_node *rb;
 	struct extent_map *exist;
 
@@ -263,30 +268,8 @@ int add_extent_mapping(struct extent_map_tree *tree,
 		goto out;
 	}
 	atomic_inc(&em->refs);
-	if (em->start != 0) {
-		rb = rb_prev(&em->rb_node);
-		if (rb)
-			merge = rb_entry(rb, struct extent_map, rb_node);
-		if (rb && mergable_maps(merge, em)) {
-			em->start = merge->start;
-			em->len += merge->len;
-			em->block_len += merge->block_len;
-			em->block_start = merge->block_start;
-			merge->in_tree = 0;
-			rb_erase(&merge->rb_node, &tree->map);
-			free_extent_map(merge);
-		}
-	 }
-	rb = rb_next(&em->rb_node);
-	if (rb)
-		merge = rb_entry(rb, struct extent_map, rb_node);
-	if (rb && mergable_maps(em, merge)) {
-		em->len += merge->len;
-		em->block_len += merge->len;
-		rb_erase(&merge->rb_node, &tree->map);
-		merge->in_tree = 0;
-		free_extent_map(merge);
-	}
+
+	try_merge_map(tree, em);
 out:
 	return ret;
 }

commit ed64f06652210b4a52fe0ea65ac43f9c6af1d988
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Thu Jul 14 03:18:15 2011 +0000

    Btrfs: clean up code for extent_map lookup
    
    lookup_extent_map() and search_extent_map() can share most of code.
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 911a9db801e0..df7a803005fb 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -299,19 +299,8 @@ static u64 range_end(u64 start, u64 len)
 	return start + len;
 }
 
-/**
- * lookup_extent_mapping - lookup extent_map
- * @tree:	tree to lookup in
- * @start:	byte offset to start the search
- * @len:	length of the lookup range
- *
- * Find and return the first extent_map struct in @tree that intersects the
- * [start, len] range.  There may be additional objects in the tree that
- * intersect, so check the object returned carefully to make sure that no
- * additional lookups are needed.
- */
-struct extent_map *lookup_extent_mapping(struct extent_map_tree *tree,
-					 u64 start, u64 len)
+struct extent_map *__lookup_extent_mapping(struct extent_map_tree *tree,
+					   u64 start, u64 len, int strict)
 {
 	struct extent_map *em;
 	struct rb_node *rb_node;
@@ -320,37 +309,41 @@ struct extent_map *lookup_extent_mapping(struct extent_map_tree *tree,
 	u64 end = range_end(start, len);
 
 	rb_node = __tree_search(&tree->map, start, &prev, &next);
-	if (!rb_node && prev) {
-		em = rb_entry(prev, struct extent_map, rb_node);
-		if (end > em->start && start < extent_map_end(em))
-			goto found;
-	}
-	if (!rb_node && next) {
-		em = rb_entry(next, struct extent_map, rb_node);
-		if (end > em->start && start < extent_map_end(em))
-			goto found;
-	}
 	if (!rb_node) {
-		em = NULL;
-		goto out;
-	}
-	if (IS_ERR(rb_node)) {
-		em = ERR_CAST(rb_node);
-		goto out;
+		if (prev)
+			rb_node = prev;
+		else if (next)
+			rb_node = next;
+		else
+			return NULL;
 	}
+
 	em = rb_entry(rb_node, struct extent_map, rb_node);
-	if (end > em->start && start < extent_map_end(em))
-		goto found;
 
-	em = NULL;
-	goto out;
+	if (strict && !(end > em->start && start < extent_map_end(em)))
+		return NULL;
 
-found:
 	atomic_inc(&em->refs);
-out:
 	return em;
 }
 
+/**
+ * lookup_extent_mapping - lookup extent_map
+ * @tree:	tree to lookup in
+ * @start:	byte offset to start the search
+ * @len:	length of the lookup range
+ *
+ * Find and return the first extent_map struct in @tree that intersects the
+ * [start, len] range.  There may be additional objects in the tree that
+ * intersect, so check the object returned carefully to make sure that no
+ * additional lookups are needed.
+ */
+struct extent_map *lookup_extent_mapping(struct extent_map_tree *tree,
+					 u64 start, u64 len)
+{
+	return __lookup_extent_mapping(tree, start, len, 1);
+}
+
 /**
  * search_extent_mapping - find a nearby extent map
  * @tree:	tree to lookup in
@@ -365,27 +358,7 @@ struct extent_map *lookup_extent_mapping(struct extent_map_tree *tree,
 struct extent_map *search_extent_mapping(struct extent_map_tree *tree,
 					 u64 start, u64 len)
 {
-	struct extent_map *em;
-	struct rb_node *rb_node;
-	struct rb_node *prev = NULL;
-	struct rb_node *next = NULL;
-
-	rb_node = __tree_search(&tree->map, start, &prev, &next);
-	if (!rb_node && prev) {
-		em = rb_entry(prev, struct extent_map, rb_node);
-		goto found;
-	}
-	if (!rb_node && next) {
-		em = rb_entry(next, struct extent_map, rb_node);
-		goto found;
-	}
-	if (!rb_node)
-		return NULL;
-
-	em = rb_entry(rb_node, struct extent_map, rb_node);
-found:
-	atomic_inc(&em->refs);
-	return em;
+	return __lookup_extent_mapping(tree, start, len, 0);
 }
 
 /**

commit 7e016a038e829c7d1271e1d57b8002860bbdf0db
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Thu Jul 14 03:18:03 2011 +0000

    Btrfs: clean up search_extent_mapping()
    
    rb_node returned by __tree_search() can be a valid pointer or NULL,
    but won't be some errno.
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 2d0410344ea3..911a9db801e0 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -379,23 +379,12 @@ struct extent_map *search_extent_mapping(struct extent_map_tree *tree,
 		em = rb_entry(next, struct extent_map, rb_node);
 		goto found;
 	}
-	if (!rb_node) {
-		em = NULL;
-		goto out;
-	}
-	if (IS_ERR(rb_node)) {
-		em = ERR_CAST(rb_node);
-		goto out;
-	}
-	em = rb_entry(rb_node, struct extent_map, rb_node);
-	goto found;
-
-	em = NULL;
-	goto out;
+	if (!rb_node)
+		return NULL;
 
+	em = rb_entry(rb_node, struct extent_map, rb_node);
 found:
 	atomic_inc(&em->refs);
-out:
 	return em;
 }
 

commit 172ddd60a662c4d8bf2809462866ddddd6431ea5
Author: David Sterba <dsterba@suse.cz>
Date:   Thu Apr 21 00:48:27 2011 +0200

    btrfs: drop gfp parameter from alloc_extent_map
    
    pass GFP_NOFS directly to kmem_cache_alloc
    
    Signed-off-by: David Sterba <dsterba@suse.cz>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 3c8f374a8e2d..2d0410344ea3 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -40,16 +40,15 @@ void extent_map_tree_init(struct extent_map_tree *tree)
 
 /**
  * alloc_extent_map - allocate new extent map structure
- * @mask:	memory allocation flags
  *
  * Allocate a new extent_map structure.  The new structure is
  * returned with a reference count of one and needs to be
  * freed using free_extent_map()
  */
-struct extent_map *alloc_extent_map(gfp_t mask)
+struct extent_map *alloc_extent_map(void)
 {
 	struct extent_map *em;
-	em = kmem_cache_alloc(extent_map_cache, mask);
+	em = kmem_cache_alloc(extent_map_cache, GFP_NOFS);
 	if (!em)
 		return NULL;
 	em->in_tree = 0;

commit a8067e022ab54fde8953880a64572c3acca644dc
Author: David Sterba <dsterba@suse.cz>
Date:   Thu Apr 21 00:34:43 2011 +0200

    btrfs: drop unused parameter from extent_map_tree_init
    
    the GFP flags are not stored anywhere and all allocations are done via
    alloc_extent_map(GFP_NOFS).
    
    Signed-off-by: David Sterba <dsterba@suse.cz>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index a24a3f2fa13e..3c8f374a8e2d 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -28,12 +28,11 @@ void extent_map_exit(void)
 /**
  * extent_map_tree_init - initialize extent map tree
  * @tree:		tree to initialize
- * @mask:		flags for memory allocations during tree operations
  *
  * Initialize the extent tree @tree.  Should be called for each new inode
  * or other user of the extent_map interface.
  */
-void extent_map_tree_init(struct extent_map_tree *tree, gfp_t mask)
+void extent_map_tree_init(struct extent_map_tree *tree)
 {
 	tree->map = RB_ROOT;
 	rwlock_init(&tree->lock);

commit 25985edcedea6396277003854657b5f3cb31a628
Author: Lucas De Marchi <lucas.demarchi@profusion.mobi>
Date:   Wed Mar 30 22:57:33 2011 -0300

    Fix common misspellings
    
    Fixes generated by 'codespell' and manually reviewed.
    
    Signed-off-by: Lucas De Marchi <lucas.demarchi@profusion.mobi>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 2b6c12e983b3..a24a3f2fa13e 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -243,7 +243,7 @@ int unpin_extent_cache(struct extent_map_tree *tree, u64 start, u64 len)
  * Insert @em into @tree or perform a simple forward/backward merge with
  * existing mappings.  The extent_map struct passed in will be inserted
  * into the tree directly, with an additional reference taken, or a
- * reference dropped if the merge attempt was successfull.
+ * reference dropped if the merge attempt was successful.
  */
 int add_extent_mapping(struct extent_map_tree *tree,
 		       struct extent_map *em)

commit c26a920373a983b52223eed5a13b97404d8b4158
Author: Tsutomu Itoh <t-itoh@jp.fujitsu.com>
Date:   Mon Feb 14 00:45:29 2011 +0000

    Btrfs: check return value of alloc_extent_map()
    
    I add the check on the return value of alloc_extent_map() to several places.
    In addition, alloc_extent_map() returns only the address or NULL.
    Therefore, check by IS_ERR() is unnecessary. So, I remove IS_ERR() checking.
    
    Signed-off-by: Tsutomu Itoh <t-itoh@jp.fujitsu.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index b0e1fce12530..2b6c12e983b3 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -51,8 +51,8 @@ struct extent_map *alloc_extent_map(gfp_t mask)
 {
 	struct extent_map *em;
 	em = kmem_cache_alloc(extent_map_cache, mask);
-	if (!em || IS_ERR(em))
-		return em;
+	if (!em)
+		return NULL;
 	em->in_tree = 0;
 	em->flags = 0;
 	em->compress_type = BTRFS_COMPRESS_NONE;

commit 261507a02ccba9afda919852263b6bc1581ce1ef
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Fri Dec 17 14:21:50 2010 +0800

    btrfs: Allow to add new compression algorithm
    
    Make the code aware of compression type, instead of always assuming
    zlib compression.
    
    Also make the zlib workspace function as common code for all
    compression types.
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 23cb8da3ff66..b0e1fce12530 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -3,6 +3,7 @@
 #include <linux/module.h>
 #include <linux/spinlock.h>
 #include <linux/hardirq.h>
+#include "ctree.h"
 #include "extent_map.h"
 
 
@@ -54,6 +55,7 @@ struct extent_map *alloc_extent_map(gfp_t mask)
 		return em;
 	em->in_tree = 0;
 	em->flags = 0;
+	em->compress_type = BTRFS_COMPRESS_NONE;
 	atomic_set(&em->refs, 1);
 	return em;
 }

commit d0b678cb0a26783ab7238784f1e7e608e5caafa3
Author: Julia Lawall <julia@diku.dk>
Date:   Fri Oct 29 15:14:23 2010 -0400

    Btrfs: Use ERR_CAST helpers
    
    Use ERR_CAST(x) rather than ERR_PTR(PTR_ERR(x)).  The former makes more
    clear what is the purpose of the operation, which otherwise looks like a
    no-op.
    
    The semantic patch that makes this change is as follows:
    (http://coccinelle.lip6.fr/)
    
    // <smpl>
    @@
    type T;
    T x;
    identifier f;
    @@
    
    T f (...) { <+...
    - ERR_PTR(PTR_ERR(x))
    + x
     ...+> }
    
    @@
    expression x;
    @@
    
    - ERR_PTR(PTR_ERR(x))
    + ERR_CAST(x)
    // </smpl>
    
    Signed-off-by: Julia Lawall <julia@diku.dk>
    Cc: Chris Mason <chris.mason@oracle.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 454ca52d6451..23cb8da3ff66 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -335,7 +335,7 @@ struct extent_map *lookup_extent_mapping(struct extent_map_tree *tree,
 		goto out;
 	}
 	if (IS_ERR(rb_node)) {
-		em = ERR_PTR(PTR_ERR(rb_node));
+		em = ERR_CAST(rb_node);
 		goto out;
 	}
 	em = rb_entry(rb_node, struct extent_map, rb_node);
@@ -384,7 +384,7 @@ struct extent_map *search_extent_mapping(struct extent_map_tree *tree,
 		goto out;
 	}
 	if (IS_ERR(rb_node)) {
-		em = ERR_PTR(PTR_ERR(rb_node));
+		em = ERR_CAST(rb_node);
 		goto out;
 	}
 	em = rb_entry(rb_node, struct extent_map, rb_node);

commit 5a0e3ad6af8660be21ca98a971cd00f331318c05
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Mar 24 17:04:11 2010 +0900

    include cleanup: Update gfp.h and slab.h includes to prepare for breaking implicit slab.h inclusion from percpu.h
    
    percpu.h is included by sched.h and module.h and thus ends up being
    included when building most .c files.  percpu.h includes slab.h which
    in turn includes gfp.h making everything defined by the two files
    universally available and complicating inclusion dependencies.
    
    percpu.h -> slab.h dependency is about to be removed.  Prepare for
    this change by updating users of gfp and slab facilities include those
    headers directly instead of assuming availability.  As this conversion
    needs to touch large number of source files, the following script is
    used as the basis of conversion.
    
      http://userweb.kernel.org/~tj/misc/slabh-sweep.py
    
    The script does the followings.
    
    * Scan files for gfp and slab usages and update includes such that
      only the necessary includes are there.  ie. if only gfp is used,
      gfp.h, if slab is used, slab.h.
    
    * When the script inserts a new include, it looks at the include
      blocks and try to put the new include such that its order conforms
      to its surrounding.  It's put in the include block which contains
      core kernel includes, in the same order that the rest are ordered -
      alphabetical, Christmas tree, rev-Xmas-tree or at the end if there
      doesn't seem to be any matching order.
    
    * If the script can't find a place to put a new include (mostly
      because the file doesn't have fitting include block), it prints out
      an error message indicating which .h file needs to be added to the
      file.
    
    The conversion was done in the following steps.
    
    1. The initial automatic conversion of all .c files updated slightly
       over 4000 files, deleting around 700 includes and adding ~480 gfp.h
       and ~3000 slab.h inclusions.  The script emitted errors for ~400
       files.
    
    2. Each error was manually checked.  Some didn't need the inclusion,
       some needed manual addition while adding it to implementation .h or
       embedding .c file was more appropriate for others.  This step added
       inclusions to around 150 files.
    
    3. The script was run again and the output was compared to the edits
       from #2 to make sure no file was left behind.
    
    4. Several build tests were done and a couple of problems were fixed.
       e.g. lib/decompress_*.c used malloc/free() wrappers around slab
       APIs requiring slab.h to be added manually.
    
    5. The script was run on all .h files but without automatically
       editing them as sprinkling gfp.h and slab.h inclusions around .h
       files could easily lead to inclusion dependency hell.  Most gfp.h
       inclusion directives were ignored as stuff from gfp.h was usually
       wildly available and often used in preprocessor macros.  Each
       slab.h inclusion directive was examined and added manually as
       necessary.
    
    6. percpu.h was updated not to include slab.h.
    
    7. Build test were done on the following configurations and failures
       were fixed.  CONFIG_GCOV_KERNEL was turned off for all tests (as my
       distributed build env didn't work with gcov compiles) and a few
       more options had to be turned off depending on archs to make things
       build (like ipr on powerpc/64 which failed due to missing writeq).
    
       * x86 and x86_64 UP and SMP allmodconfig and a custom test config.
       * powerpc and powerpc64 SMP allmodconfig
       * sparc and sparc64 SMP allmodconfig
       * ia64 SMP allmodconfig
       * s390 SMP allmodconfig
       * alpha SMP allmodconfig
       * um on x86_64 SMP allmodconfig
    
    8. percpu.h modifications were reverted so that it could be applied as
       a separate patch and serve as bisection point.
    
    Given the fact that I had only a couple of failures from tests on step
    6, I'm fairly confident about the coverage of this conversion patch.
    If there is a breakage, it's likely to be something in one of the arch
    headers which should be easily discoverable easily on most builds of
    the specific arch.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Guess-its-ok-by: Christoph Lameter <cl@linux-foundation.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Lee Schermerhorn <Lee.Schermerhorn@hp.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 28d87ba60ce8..454ca52d6451 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -1,5 +1,4 @@
 #include <linux/err.h>
-#include <linux/gfp.h>
 #include <linux/slab.h>
 #include <linux/module.h>
 #include <linux/spinlock.h>

commit 51d0f6d1f50349579f007adf5c0b51aaedd93b94
Merge: 57d54889cd00 da495ecc0fb0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Mar 8 14:07:53 2010 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/mason/btrfs-unstable
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/mason/btrfs-unstable:
      Btrfs: kfree correct pointer during mount option parsing
      Btrfs: use RB_ROOT to intialize rb_trees instead of setting rb_node to NULL

commit 6bef4d317193d3badbbfa3f3c593758ace84a629
Author: Eric Paris <eparis@redhat.com>
Date:   Tue Feb 23 19:43:04 2010 +0000

    Btrfs: use RB_ROOT to intialize rb_trees instead of setting rb_node to NULL
    
    btrfs inialize rb trees in quite a number of places by settin rb_node =
    NULL;  The problem with this is that 17d9ddc72fb8bba0d4f678 in the
    linux-next tree adds a new field to that struct which needs to be NULL for
    the new rbtree library code to work properly.  This patch uses RB_ROOT as
    the intializer so all of the relevant fields will be NULL'd.  Without the
    patch I get a panic.
    
    Signed-off-by: Eric Paris <eparis@redhat.com>
    Acked-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 5a4f73b79b75..5a01f35507dd 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -35,7 +35,7 @@ void extent_map_exit(void)
  */
 void extent_map_tree_init(struct extent_map_tree *tree, gfp_t mask)
 {
-	tree->map.rb_node = NULL;
+	tree->map = RB_ROOT;
 	rwlock_init(&tree->lock);
 }
 

commit 67f15b06c1a7e5417b7042b515ca2695de30beda
Merge: 94673e968cbc 035fe03a7ad5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jan 29 10:27:37 2010 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/mason/btrfs-unstable
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/mason/btrfs-unstable:
      Btrfs: check total number of devices when removing missing
      Btrfs: check return value of open_bdev_exclusive properly
      Btrfs: do not mark the chunk as readonly if in degraded mode
      Btrfs: run orphan cleanup on default fs root
      Btrfs: fix a memory leak in btrfs_init_acl
      Btrfs: Use correct values when updating inode i_size on fallocate
      Btrfs: remove tree_search() in extent_map.c
      Btrfs: Add mount -o compress-force

commit b8d9bfeb18f9af794020d96e9bee984d18a8d737
Author: Miao Xie <miaox@cn.fujitsu.com>
Date:   Tue Dec 15 06:54:17 2009 +0000

    Btrfs: remove tree_search() in extent_map.c
    
    This patch removes tree_search() in extent_map.c because it is not called by
    anything.
    
    Signed-off-by: Miao Xie <miaox@cn.fujitsu.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index ccbdcb54ec5d..5a4f73b79b75 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -155,20 +155,6 @@ static struct rb_node *__tree_search(struct rb_root *root, u64 offset,
 	return NULL;
 }
 
-/*
- * look for an offset in the tree, and if it can't be found, return
- * the first offset we can find smaller than 'offset'.
- */
-static inline struct rb_node *tree_search(struct rb_root *root, u64 offset)
-{
-	struct rb_node *prev;
-	struct rb_node *ret;
-	ret = __tree_search(root, offset, &prev, NULL);
-	if (!ret)
-		return prev;
-	return ret;
-}
-
 /* check to see if two extent_map structs are adjacent and safe to merge */
 static int mergable_maps(struct extent_map *prev, struct extent_map *next)
 {

commit d014d043869cdc591f3a33243d3481fa4479c2d0
Merge: 6ec22f9b037f 6070d81eb5f2
Author: Jiri Kosina <jkosina@suse.cz>
Date:   Mon Dec 7 18:36:35 2009 +0100

    Merge branch 'for-next' into for-linus
    
    Conflicts:
    
            kernel/irq/chip.c

commit af901ca181d92aac3a7dc265144a9081a86d8f39
Author: Andr√© Goddard Rosa <andre.goddard@gmail.com>
Date:   Sat Nov 14 13:09:05 2009 -0200

    tree-wide: fix assorted typos all over the place
    
    That is "success", "unknown", "through", "performance", "[re|un]mapping"
    , "access", "default", "reasonable", "[con]currently", "temperature"
    , "channel", "[un]used", "application", "example","hierarchy", "therefore"
    , "[over|under]flow", "contiguous", "threshold", "enough" and others.
    
    Signed-off-by: Andr√© Goddard Rosa <andre.goddard@gmail.com>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 2c726b7b9faa..6815d2a84b94 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -256,7 +256,7 @@ int unpin_extent_cache(struct extent_map_tree *tree, u64 start, u64 len)
  * Insert @em into @tree or perform a simple forward/backward merge with
  * existing mappings.  The extent_map struct passed in will be inserted
  * into the tree directly, with an additional reference taken, or a
- * reference dropped if the merge attempt was sucessfull.
+ * reference dropped if the merge attempt was successfull.
  */
 int add_extent_mapping(struct extent_map_tree *tree,
 		       struct extent_map *em)

commit 4eb3991c5def39bcf553c14ebe2618fcb47b627f
Author: Dan Carpenter <error27@gmail.com>
Date:   Tue Nov 10 09:01:43 2009 +0000

    Btrfs: avoid null deref in unpin_extent_cache()
    
    I re-orderred the checks to avoid dereferencing "em" if it was null.
    
    Found by smatch static checker.
    
    Signed-off-by: Dan Carpenter <error27@gmail.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 2c726b7b9faa..ccbdcb54ec5d 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -208,7 +208,7 @@ int unpin_extent_cache(struct extent_map_tree *tree, u64 start, u64 len)
 	write_lock(&tree->lock);
 	em = lookup_extent_mapping(tree, start, len);
 
-	WARN_ON(em->start != start || !em);
+	WARN_ON(!em || em->start != start);
 
 	if (!em)
 		goto out;

commit b917b7c3be50435fa8257591b964934e917f2d45
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Sep 18 16:07:03 2009 -0400

    Btrfs: search for an allocation hint while filling file COW
    
    The allocator has some nice knobs for sending hints about where
    to try and allocate new blocks, but when we're doing file allocations
    we're not sending any hint at all.
    
    This commit adds a simple extent map search to see if we can
    quickly and easily find a hint for the allocator.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 5bc7a0d325e7..2c726b7b9faa 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -366,6 +366,54 @@ struct extent_map *lookup_extent_mapping(struct extent_map_tree *tree,
 	return em;
 }
 
+/**
+ * search_extent_mapping - find a nearby extent map
+ * @tree:	tree to lookup in
+ * @start:	byte offset to start the search
+ * @len:	length of the lookup range
+ *
+ * Find and return the first extent_map struct in @tree that intersects the
+ * [start, len] range.
+ *
+ * If one can't be found, any nearby extent may be returned
+ */
+struct extent_map *search_extent_mapping(struct extent_map_tree *tree,
+					 u64 start, u64 len)
+{
+	struct extent_map *em;
+	struct rb_node *rb_node;
+	struct rb_node *prev = NULL;
+	struct rb_node *next = NULL;
+
+	rb_node = __tree_search(&tree->map, start, &prev, &next);
+	if (!rb_node && prev) {
+		em = rb_entry(prev, struct extent_map, rb_node);
+		goto found;
+	}
+	if (!rb_node && next) {
+		em = rb_entry(next, struct extent_map, rb_node);
+		goto found;
+	}
+	if (!rb_node) {
+		em = NULL;
+		goto out;
+	}
+	if (IS_ERR(rb_node)) {
+		em = ERR_PTR(PTR_ERR(rb_node));
+		goto out;
+	}
+	em = rb_entry(rb_node, struct extent_map, rb_node);
+	goto found;
+
+	em = NULL;
+	goto out;
+
+found:
+	atomic_inc(&em->refs);
+out:
+	return em;
+}
+
 /**
  * remove_extent_mapping - removes an extent_map from the extent tree
  * @tree:	extent tree to remove from

commit a1ed835e1ab5795f91b198d08c43e2f56848dcf3
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Sep 11 12:27:37 2009 -0400

    Btrfs: Fix extent replacment race
    
    Data COW means that whenever we write to a file, we replace any old
    extent pointers with new ones.  There was a window where a readpage
    might find the old extent pointers on disk and cache them in the
    extent_map tree in ram in the middle of a given write replacing them.
    
    Even though both the readpage and the write had their respective bytes
    in the file locked, the extent readpage inserts may cover more bytes than
    it had locked down.
    
    This commit closes the race by keeping the new extent pinned in the extent
    map tree until after the on-disk btree is properly setup with the new
    extent pointers.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 72e9fa3c31f5..5bc7a0d325e7 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -198,6 +198,56 @@ static int mergable_maps(struct extent_map *prev, struct extent_map *next)
 	return 0;
 }
 
+int unpin_extent_cache(struct extent_map_tree *tree, u64 start, u64 len)
+{
+	int ret = 0;
+	struct extent_map *merge = NULL;
+	struct rb_node *rb;
+	struct extent_map *em;
+
+	write_lock(&tree->lock);
+	em = lookup_extent_mapping(tree, start, len);
+
+	WARN_ON(em->start != start || !em);
+
+	if (!em)
+		goto out;
+
+	clear_bit(EXTENT_FLAG_PINNED, &em->flags);
+
+	if (em->start != 0) {
+		rb = rb_prev(&em->rb_node);
+		if (rb)
+			merge = rb_entry(rb, struct extent_map, rb_node);
+		if (rb && mergable_maps(merge, em)) {
+			em->start = merge->start;
+			em->len += merge->len;
+			em->block_len += merge->block_len;
+			em->block_start = merge->block_start;
+			merge->in_tree = 0;
+			rb_erase(&merge->rb_node, &tree->map);
+			free_extent_map(merge);
+		}
+	}
+
+	rb = rb_next(&em->rb_node);
+	if (rb)
+		merge = rb_entry(rb, struct extent_map, rb_node);
+	if (rb && mergable_maps(em, merge)) {
+		em->len += merge->len;
+		em->block_len += merge->len;
+		rb_erase(&merge->rb_node, &tree->map);
+		merge->in_tree = 0;
+		free_extent_map(merge);
+	}
+
+	free_extent_map(em);
+out:
+	write_unlock(&tree->lock);
+	return ret;
+
+}
+
 /**
  * add_extent_mapping - add new extent map to the extent tree
  * @tree:	tree to insert new map in

commit 890871be854b5f5e43e7ba2475f706209906cc24
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Sep 2 16:24:52 2009 -0400

    Btrfs: switch extent_map to a rw lock
    
    There are two main users of the extent_map tree.  The
    first is regular file inodes, where it is evenly spread
    between readers and writers.
    
    The second is the chunk allocation tree, which maps blocks from
    logical addresses to phyiscal ones, and it is 99.99% reads.
    
    The mapping tree is a point of lock contention during heavy IO
    workloads, so this commit switches things to a rw lock.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 30c9365861e6..72e9fa3c31f5 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -36,7 +36,7 @@ void extent_map_exit(void)
 void extent_map_tree_init(struct extent_map_tree *tree, gfp_t mask)
 {
 	tree->map.rb_node = NULL;
-	spin_lock_init(&tree->lock);
+	rwlock_init(&tree->lock);
 }
 
 /**
@@ -222,7 +222,6 @@ int add_extent_mapping(struct extent_map_tree *tree,
 		ret = -EEXIST;
 		goto out;
 	}
-	assert_spin_locked(&tree->lock);
 	rb = tree_insert(&tree->map, em->start, &em->rb_node);
 	if (rb) {
 		ret = -EEXIST;
@@ -285,7 +284,6 @@ struct extent_map *lookup_extent_mapping(struct extent_map_tree *tree,
 	struct rb_node *next = NULL;
 	u64 end = range_end(start, len);
 
-	assert_spin_locked(&tree->lock);
 	rb_node = __tree_search(&tree->map, start, &prev, &next);
 	if (!rb_node && prev) {
 		em = rb_entry(prev, struct extent_map, rb_node);
@@ -331,7 +329,6 @@ int remove_extent_mapping(struct extent_map_tree *tree, struct extent_map *em)
 	int ret = 0;
 
 	WARN_ON(test_bit(EXTENT_FLAG_PINNED, &em->flags));
-	assert_spin_locked(&tree->lock);
 	rb_erase(&em->rb_node, &tree->map);
 	em->in_tree = 0;
 	return ret;

commit 9601e3f6336f6ca66929f451b1f66085e68e36e3
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Apr 13 15:33:09 2009 +0200

    Btrfs: kill btrfs_cache_create
    
    Just use kmem_cache_create directly.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 9827fa1de4e1..30c9365861e6 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -6,19 +6,14 @@
 #include <linux/hardirq.h>
 #include "extent_map.h"
 
-/* temporary define until extent_map moves out of btrfs */
-struct kmem_cache *btrfs_cache_create(const char *name, size_t size,
-				       unsigned long extra_flags,
-				       void (*ctor)(void *, struct kmem_cache *,
-						    unsigned long));
 
 static struct kmem_cache *extent_map_cache;
 
 int __init extent_map_init(void)
 {
-	extent_map_cache = btrfs_cache_create("extent_map",
-					    sizeof(struct extent_map), 0,
-					    NULL);
+	extent_map_cache = kmem_cache_create("extent_map",
+			sizeof(struct extent_map), 0,
+			SLAB_RECLAIM_ACCOUNT | SLAB_MEM_SPREAD, NULL);
 	if (!extent_map_cache)
 		return -ENOMEM;
 	return 0;

commit 0d4bf11e5309eff64272a49e1ea55658372abc56
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Apr 13 15:33:56 2009 +0200

    Btrfs: don't export symbols
    
    Currently the extent_map code is only for btrfs so don't export it's
    symbols.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index b187917b36fa..9827fa1de4e1 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -43,7 +43,6 @@ void extent_map_tree_init(struct extent_map_tree *tree, gfp_t mask)
 	tree->map.rb_node = NULL;
 	spin_lock_init(&tree->lock);
 }
-EXPORT_SYMBOL(extent_map_tree_init);
 
 /**
  * alloc_extent_map - allocate new extent map structure
@@ -64,7 +63,6 @@ struct extent_map *alloc_extent_map(gfp_t mask)
 	atomic_set(&em->refs, 1);
 	return em;
 }
-EXPORT_SYMBOL(alloc_extent_map);
 
 /**
  * free_extent_map - drop reference count of an extent_map
@@ -83,7 +81,6 @@ void free_extent_map(struct extent_map *em)
 		kmem_cache_free(extent_map_cache, em);
 	}
 }
-EXPORT_SYMBOL(free_extent_map);
 
 static struct rb_node *tree_insert(struct rb_root *root, u64 offset,
 				   struct rb_node *node)
@@ -264,7 +261,6 @@ int add_extent_mapping(struct extent_map_tree *tree,
 out:
 	return ret;
 }
-EXPORT_SYMBOL(add_extent_mapping);
 
 /* simple helper to do math around the end of an extent, handling wrap */
 static u64 range_end(u64 start, u64 len)
@@ -326,7 +322,6 @@ struct extent_map *lookup_extent_mapping(struct extent_map_tree *tree,
 out:
 	return em;
 }
-EXPORT_SYMBOL(lookup_extent_mapping);
 
 /**
  * remove_extent_mapping - removes an extent_map from the extent tree
@@ -346,4 +341,3 @@ int remove_extent_mapping(struct extent_map_tree *tree, struct extent_map *em)
 	em->in_tree = 0;
 	return ret;
 }
-EXPORT_SYMBOL(remove_extent_mapping);

commit ff0a5836ac48554b9f3b9d3c5f5bce4ddea11f1f
Author: Dan Carpenter <error27@gmail.com>
Date:   Thu Apr 2 16:46:06 2009 -0400

    Btrfs: remove dead code
    
    merge is always NULL at this point.
    
    Signed-off-by: Dan Carpenter <error27@gmail.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 50da69da20ce..b187917b36fa 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -234,7 +234,6 @@ int add_extent_mapping(struct extent_map_tree *tree,
 	rb = tree_insert(&tree->map, em->start, &em->rb_node);
 	if (rb) {
 		ret = -EEXIST;
-		free_extent_map(merge);
 		goto out;
 	}
 	atomic_inc(&em->refs);

commit 7eaebe7d503c3ef240ac7b3efc5433fe647c0298
Author: Huang Weiyi <weiyi.huang@gmail.com>
Date:   Wed Jan 21 10:49:16 2009 -0500

    Btrfs: removed unused #include <version.h>'s
    
    Removed unused #include <version.h>'s in btrfs
    
    Signed-off-by: Huang Weiyi <weiyi.huang@gmail.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 4a83e33ada32..50da69da20ce 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -3,7 +3,6 @@
 #include <linux/slab.h>
 #include <linux/module.h>
 #include <linux/spinlock.h>
-#include <linux/version.h>
 #include <linux/hardirq.h>
 #include "extent_map.h"
 

commit d397712bcc6a759a560fd247e6053ecae091f958
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Jan 5 21:25:51 2009 -0500

    Btrfs: Fix checkpatch.pl warnings
    
    There were many, most are fixed now.  struct-funcs.c generates some warnings
    but these are bogus.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index fd3ebfb8c3c5..4a83e33ada32 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -89,11 +89,11 @@ EXPORT_SYMBOL(free_extent_map);
 static struct rb_node *tree_insert(struct rb_root *root, u64 offset,
 				   struct rb_node *node)
 {
-	struct rb_node ** p = &root->rb_node;
-	struct rb_node * parent = NULL;
+	struct rb_node **p = &root->rb_node;
+	struct rb_node *parent = NULL;
 	struct extent_map *entry;
 
-	while(*p) {
+	while (*p) {
 		parent = *p;
 		entry = rb_entry(parent, struct extent_map, rb_node);
 
@@ -122,13 +122,13 @@ static struct rb_node *__tree_search(struct rb_root *root, u64 offset,
 				     struct rb_node **prev_ret,
 				     struct rb_node **next_ret)
 {
-	struct rb_node * n = root->rb_node;
+	struct rb_node *n = root->rb_node;
 	struct rb_node *prev = NULL;
 	struct rb_node *orig_prev = NULL;
 	struct extent_map *entry;
 	struct extent_map *prev_entry = NULL;
 
-	while(n) {
+	while (n) {
 		entry = rb_entry(n, struct extent_map, rb_node);
 		prev = n;
 		prev_entry = entry;
@@ -145,7 +145,7 @@ static struct rb_node *__tree_search(struct rb_root *root, u64 offset,
 
 	if (prev_ret) {
 		orig_prev = prev;
-		while(prev && offset >= extent_map_end(prev_entry)) {
+		while (prev && offset >= extent_map_end(prev_entry)) {
 			prev = rb_next(prev);
 			prev_entry = rb_entry(prev, struct extent_map, rb_node);
 		}
@@ -155,7 +155,7 @@ static struct rb_node *__tree_search(struct rb_root *root, u64 offset,
 
 	if (next_ret) {
 		prev_entry = rb_entry(prev, struct extent_map, rb_node);
-		while(prev && offset < prev_entry->start) {
+		while (prev && offset < prev_entry->start) {
 			prev = rb_prev(prev);
 			prev_entry = rb_entry(prev, struct extent_map, rb_node);
 		}

commit c8b978188c9a0fd3d535c13debd19d522b726f1f
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Oct 29 14:49:59 2008 -0400

    Btrfs: Add zlib compression support
    
    This is a large change for adding compression on reading and writing,
    both for inline and regular extents.  It does some fairly large
    surgery to the writeback paths.
    
    Compression is off by default and enabled by mount -o compress.  Even
    when the -o compress mount option is not used, it is possible to read
    compressed extents off the disk.
    
    If compression for a given set of pages fails to make them smaller, the
    file is flagged to avoid future compression attempts later.
    
    * While finding delalloc extents, the pages are locked before being sent down
    to the delalloc handler.  This allows the delalloc handler to do complex things
    such as cleaning the pages, marking them writeback and starting IO on their
    behalf.
    
    * Inline extents are inserted at delalloc time now.  This allows us to compress
    the data before inserting the inline extent, and it allows us to insert
    an inline extent that spans multiple pages.
    
    * All of the in-memory extent representations (extent_map.c, ordered-data.c etc)
    are changed to record both an in-memory size and an on disk size, as well
    as a flag for compression.
    
    From a disk format point of view, the extent pointers in the file are changed
    to record the on disk size of a given extent and some encoding flags.
    Space in the disk format is allocated for compression encoding, as well
    as encryption and a generic 'other' field.  Neither the encryption or the
    'other' field are currently used.
    
    In order to limit the amount of data read for a single random read in the
    file, the size of a compressed extent is limited to 128k.  This is a
    software only limit, the disk format supports u64 sized compressed extents.
    
    In order to limit the ram consumed while processing extents, the uncompressed
    size of a compressed extent is limited to 256k.  This is a software only limit
    and will be subject to tuning later.
    
    Checksumming is still done on compressed extents, and it is done on the
    uncompressed version of the data.  This way additional encodings can be
    layered on without having to figure out which encoding to checksum.
    
    Compression happens at delalloc time, which is basically singled threaded because
    it is usually done by a single pdflush thread.  This makes it tricky to
    spread the compression load across all the cpus on the box.  We'll have to
    look at parallel pdflush walks of dirty inodes at a later time.
    
    Decompression is hooked into readpages and it does spread across CPUs nicely.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 74b2a29880d3..fd3ebfb8c3c5 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -184,6 +184,13 @@ static int mergable_maps(struct extent_map *prev, struct extent_map *next)
 	if (test_bit(EXTENT_FLAG_PINNED, &prev->flags))
 		return 0;
 
+	/*
+	 * don't merge compressed extents, we need to know their
+	 * actual size
+	 */
+	if (test_bit(EXTENT_FLAG_COMPRESSED, &prev->flags))
+		return 0;
+
 	if (extent_map_end(prev) == next->start &&
 	    prev->flags == next->flags &&
 	    prev->bdev == next->bdev &&
@@ -239,6 +246,7 @@ int add_extent_mapping(struct extent_map_tree *tree,
 		if (rb && mergable_maps(merge, em)) {
 			em->start = merge->start;
 			em->len += merge->len;
+			em->block_len += merge->block_len;
 			em->block_start = merge->block_start;
 			merge->in_tree = 0;
 			rb_erase(&merge->rb_node, &tree->map);
@@ -250,6 +258,7 @@ int add_extent_mapping(struct extent_map_tree *tree,
 		merge = rb_entry(rb, struct extent_map, rb_node);
 	if (rb && mergable_maps(em, merge)) {
 		em->len += merge->len;
+		em->block_len += merge->len;
 		rb_erase(&merge->rb_node, &tree->map);
 		merge->in_tree = 0;
 		free_extent_map(merge);

commit d352ac68148b69937d39ca5d48bcc4478e118dbf
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Sep 29 15:18:18 2008 -0400

    Btrfs: add and improve comments
    
    This improves the comments at the top of many functions.  It didn't
    dive into the guts of functions because I was trying to
    avoid merging problems with the new allocator and back reference work.
    
    extent-tree.c and volumes.c were both skipped, and there is definitely
    more work todo in cleaning and commenting the code.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 78ced11d18c7..74b2a29880d3 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -114,6 +114,10 @@ static struct rb_node *tree_insert(struct rb_root *root, u64 offset,
 	return NULL;
 }
 
+/*
+ * search through the tree for an extent_map with a given offset.  If
+ * it can't be found, try to find some neighboring extents
+ */
 static struct rb_node *__tree_search(struct rb_root *root, u64 offset,
 				     struct rb_node **prev_ret,
 				     struct rb_node **next_ret)
@@ -160,6 +164,10 @@ static struct rb_node *__tree_search(struct rb_root *root, u64 offset,
 	return NULL;
 }
 
+/*
+ * look for an offset in the tree, and if it can't be found, return
+ * the first offset we can find smaller than 'offset'.
+ */
 static inline struct rb_node *tree_search(struct rb_root *root, u64 offset)
 {
 	struct rb_node *prev;
@@ -170,6 +178,7 @@ static inline struct rb_node *tree_search(struct rb_root *root, u64 offset)
 	return ret;
 }
 
+/* check to see if two extent_map structs are adjacent and safe to merge */
 static int mergable_maps(struct extent_map *prev, struct extent_map *next)
 {
 	if (test_bit(EXTENT_FLAG_PINNED, &prev->flags))
@@ -250,6 +259,7 @@ int add_extent_mapping(struct extent_map_tree *tree,
 }
 EXPORT_SYMBOL(add_extent_mapping);
 
+/* simple helper to do math around the end of an extent, handling wrap */
 static u64 range_end(u64 start, u64 len)
 {
 	if (start + len < start)

commit 7c2fe32a238eb12422beca5cbd5194a594baa559
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Aug 20 08:51:50 2008 -0400

    Btrfs: Fix add_extent_mapping to check for duplicates across the whole range
    
    add_extent_mapping was allowing the insertion of overlapping extents.
    This never used to happen because it only inserted the extents from disk
    and those were never overlapping.
    
    But, with the data=ordered code, the disk and memory representations of the
    file are not the same.  add_extent_mapping needs to ensure a new extent
    does not overlap before it inserts.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 954b047639ab..78ced11d18c7 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -207,7 +207,14 @@ int add_extent_mapping(struct extent_map_tree *tree,
 	int ret = 0;
 	struct extent_map *merge = NULL;
 	struct rb_node *rb;
+	struct extent_map *exist;
 
+	exist = lookup_extent_mapping(tree, em->start, em->len);
+	if (exist) {
+		free_extent_map(exist);
+		ret = -EEXIST;
+		goto out;
+	}
 	assert_spin_locked(&tree->lock);
 	rb = tree_insert(&tree->map, em->start, &em->rb_node);
 	if (rb) {

commit 64f26f745084872b916cd1bef6054e21b15c5784
Author: David Woodhouse <dwmw2@infradead.org>
Date:   Thu Jul 24 10:09:43 2008 -0400

    Btrfs: Use assert_spin_locked instead of spin_trylock
    
    On UP systems spin_trylock always succeeds
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 8a502ee2f231..954b047639ab 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -208,7 +208,7 @@ int add_extent_mapping(struct extent_map_tree *tree,
 	struct extent_map *merge = NULL;
 	struct rb_node *rb;
 
-	BUG_ON(spin_trylock(&tree->lock));
+	assert_spin_locked(&tree->lock);
 	rb = tree_insert(&tree->map, em->start, &em->rb_node);
 	if (rb) {
 		ret = -EEXIST;
@@ -270,7 +270,7 @@ struct extent_map *lookup_extent_mapping(struct extent_map_tree *tree,
 	struct rb_node *next = NULL;
 	u64 end = range_end(start, len);
 
-	BUG_ON(spin_trylock(&tree->lock));
+	assert_spin_locked(&tree->lock);
 	rb_node = __tree_search(&tree->map, start, &prev, &next);
 	if (!rb_node && prev) {
 		em = rb_entry(prev, struct extent_map, rb_node);
@@ -317,7 +317,7 @@ int remove_extent_mapping(struct extent_map_tree *tree, struct extent_map *em)
 	int ret = 0;
 
 	WARN_ON(test_bit(EXTENT_FLAG_PINNED, &em->flags));
-	BUG_ON(spin_trylock(&tree->lock));
+	assert_spin_locked(&tree->lock);
 	rb_erase(&em->rb_node, &tree->map);
 	em->in_tree = 0;
 	return ret;

commit f421950f86bf96a11fef932e167ab2e70d4c43a0
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Jul 22 11:18:09 2008 -0400

    Btrfs: Fix some data=ordered related data corruptions
    
    Stress testing was showing data checksum errors, most of which were caused
    by a lookup bug in the extent_map tree.  The tree was caching the last
    pointer returned, and searches would check the last pointer first.
    
    But, search callers also expect the search to return the very first
    matching extent in the range, which wasn't always true with the last
    pointer usage.
    
    For now, the code to cache the last return value is just removed.  It is
    easy to fix, but I think lookups are rare enough that it isn't required anymore.
    
    This commit also replaces do_sync_mapping_range with a local copy of the
    related functions.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 71b1ac155355..8a502ee2f231 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -42,7 +42,6 @@ void extent_map_exit(void)
 void extent_map_tree_init(struct extent_map_tree *tree, gfp_t mask)
 {
 	tree->map.rb_node = NULL;
-	tree->last = NULL;
 	spin_lock_init(&tree->lock);
 }
 EXPORT_SYMBOL(extent_map_tree_init);
@@ -239,7 +238,6 @@ int add_extent_mapping(struct extent_map_tree *tree,
 		merge->in_tree = 0;
 		free_extent_map(merge);
 	}
-	tree->last = em;
 out:
 	return ret;
 }
@@ -273,10 +271,6 @@ struct extent_map *lookup_extent_mapping(struct extent_map_tree *tree,
 	u64 end = range_end(start, len);
 
 	BUG_ON(spin_trylock(&tree->lock));
-	em = tree->last;
-	if (em && end > em->start && start < extent_map_end(em))
-		goto found;
-
 	rb_node = __tree_search(&tree->map, start, &prev, &next);
 	if (!rb_node && prev) {
 		em = rb_entry(prev, struct extent_map, rb_node);
@@ -305,7 +299,6 @@ struct extent_map *lookup_extent_mapping(struct extent_map_tree *tree,
 
 found:
 	atomic_inc(&em->refs);
-	tree->last = em;
 out:
 	return em;
 }
@@ -327,8 +320,6 @@ int remove_extent_mapping(struct extent_map_tree *tree, struct extent_map *em)
 	BUG_ON(spin_trylock(&tree->lock));
 	rb_erase(&em->rb_node, &tree->map);
 	em->in_tree = 0;
-	if (tree->last == em)
-		tree->last = NULL;
 	return ret;
 }
 EXPORT_SYMBOL(remove_extent_mapping);

commit 7f3c74fb831fa19bafe087e817c0a5ff3883f1ea
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Jul 18 12:01:11 2008 -0400

    Btrfs: Keep extent mappings in ram until pending ordered extents are done
    
    It was possible for stale mappings from disk to be used instead of the
    new pending ordered extent.  This adds a flag to the extent map struct
    to keep it pinned until the pending ordered extent is actually on disk.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 81123277c2b8..71b1ac155355 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -173,6 +173,9 @@ static inline struct rb_node *tree_search(struct rb_root *root, u64 offset)
 
 static int mergable_maps(struct extent_map *prev, struct extent_map *next)
 {
+	if (test_bit(EXTENT_FLAG_PINNED, &prev->flags))
+		return 0;
+
 	if (extent_map_end(prev) == next->start &&
 	    prev->flags == next->flags &&
 	    prev->bdev == next->bdev &&
@@ -320,6 +323,7 @@ int remove_extent_mapping(struct extent_map_tree *tree, struct extent_map *em)
 {
 	int ret = 0;
 
+	WARN_ON(test_bit(EXTENT_FLAG_PINNED, &em->flags));
 	BUG_ON(spin_trylock(&tree->lock));
 	rb_erase(&em->rb_node, &tree->map);
 	em->in_tree = 0;

commit e6dcd2dc9c489108648e2ed543315dd134d50a9a
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Jul 17 12:53:50 2008 -0400

    Btrfs: New data=ordered implementation
    
    The old data=ordered code would force commit to wait until
    all the data extents from the transaction were fully on disk.  This
    introduced large latencies into the commit and stalled new writers
    in the transaction for a long time.
    
    The new code changes the way data allocations and extents work:
    
    * When delayed allocation is filled, data extents are reserved, and
      the extent bit EXTENT_ORDERED is set on the entire range of the extent.
      A struct btrfs_ordered_extent is allocated an inserted into a per-inode
      rbtree to track the pending extents.
    
    * As each page is written EXTENT_ORDERED is cleared on the bytes corresponding
      to that page.
    
    * When all of the bytes corresponding to a single struct btrfs_ordered_extent
      are written, The previously reserved extent is inserted into the FS
      btree and into the extent allocation trees.  The checksums for the file
      data are also updated.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index f5a04eb9a2ac..81123277c2b8 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -206,10 +206,11 @@ int add_extent_mapping(struct extent_map_tree *tree,
 	struct extent_map *merge = NULL;
 	struct rb_node *rb;
 
+	BUG_ON(spin_trylock(&tree->lock));
 	rb = tree_insert(&tree->map, em->start, &em->rb_node);
 	if (rb) {
-		merge = rb_entry(rb, struct extent_map, rb_node);
 		ret = -EEXIST;
+		free_extent_map(merge);
 		goto out;
 	}
 	atomic_inc(&em->refs);
@@ -268,6 +269,7 @@ struct extent_map *lookup_extent_mapping(struct extent_map_tree *tree,
 	struct rb_node *next = NULL;
 	u64 end = range_end(start, len);
 
+	BUG_ON(spin_trylock(&tree->lock));
 	em = tree->last;
 	if (em && end > em->start && start < extent_map_end(em))
 		goto found;
@@ -318,6 +320,7 @@ int remove_extent_mapping(struct extent_map_tree *tree, struct extent_map *em)
 {
 	int ret = 0;
 
+	BUG_ON(spin_trylock(&tree->lock));
 	rb_erase(&em->rb_node, &tree->map);
 	em->in_tree = 0;
 	if (tree->last == em)

commit 9d2423c5c3fbb0f110ac0b6cdc5a8e4d64729483
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Jun 11 21:52:17 2008 -0400

    Btrfs: kerneldoc comments for extent_map.c
    
    Add kerneldoc comments for all exported functions.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index ba46f7911d99..f5a04eb9a2ac 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -31,6 +31,14 @@ void extent_map_exit(void)
 		kmem_cache_destroy(extent_map_cache);
 }
 
+/**
+ * extent_map_tree_init - initialize extent map tree
+ * @tree:		tree to initialize
+ * @mask:		flags for memory allocations during tree operations
+ *
+ * Initialize the extent tree @tree.  Should be called for each new inode
+ * or other user of the extent_map interface.
+ */
 void extent_map_tree_init(struct extent_map_tree *tree, gfp_t mask)
 {
 	tree->map.rb_node = NULL;
@@ -39,6 +47,14 @@ void extent_map_tree_init(struct extent_map_tree *tree, gfp_t mask)
 }
 EXPORT_SYMBOL(extent_map_tree_init);
 
+/**
+ * alloc_extent_map - allocate new extent map structure
+ * @mask:	memory allocation flags
+ *
+ * Allocate a new extent_map structure.  The new structure is
+ * returned with a reference count of one and needs to be
+ * freed using free_extent_map()
+ */
 struct extent_map *alloc_extent_map(gfp_t mask)
 {
 	struct extent_map *em;
@@ -52,6 +68,13 @@ struct extent_map *alloc_extent_map(gfp_t mask)
 }
 EXPORT_SYMBOL(alloc_extent_map);
 
+/**
+ * free_extent_map - drop reference count of an extent_map
+ * @em:		extent map beeing releasead
+ *
+ * Drops the reference out on @em by one and free the structure
+ * if the reference count hits zero.
+ */
 void free_extent_map(struct extent_map *em)
 {
 	if (!em)
@@ -166,10 +189,15 @@ static int mergable_maps(struct extent_map *prev, struct extent_map *next)
 	return 0;
 }
 
-/*
- * add_extent_mapping tries a simple forward/backward merge with existing
- * mappings.  The extent_map struct passed in will be inserted into
- * the tree directly (no copies made, just a reference taken).
+/**
+ * add_extent_mapping - add new extent map to the extent tree
+ * @tree:	tree to insert new map in
+ * @em:		map to insert
+ *
+ * Insert @em into @tree or perform a simple forward/backward merge with
+ * existing mappings.  The extent_map struct passed in will be inserted
+ * into the tree directly, with an additional reference taken, or a
+ * reference dropped if the merge attempt was sucessfull.
  */
 int add_extent_mapping(struct extent_map_tree *tree,
 		       struct extent_map *em)
@@ -220,11 +248,16 @@ static u64 range_end(u64 start, u64 len)
 	return start + len;
 }
 
-/*
- * lookup_extent_mapping returns the first extent_map struct in the
- * tree that intersects the [start, len] range.  There may
- * be additional objects in the tree that intersect, so check the object
- * returned carefully to make sure you don't need additional lookups.
+/**
+ * lookup_extent_mapping - lookup extent_map
+ * @tree:	tree to lookup in
+ * @start:	byte offset to start the search
+ * @len:	length of the lookup range
+ *
+ * Find and return the first extent_map struct in @tree that intersects the
+ * [start, len] range.  There may be additional objects in the tree that
+ * intersect, so check the object returned carefully to make sure that no
+ * additional lookups are needed.
  */
 struct extent_map *lookup_extent_mapping(struct extent_map_tree *tree,
 					 u64 start, u64 len)
@@ -273,9 +306,13 @@ struct extent_map *lookup_extent_mapping(struct extent_map_tree *tree,
 }
 EXPORT_SYMBOL(lookup_extent_mapping);
 
-/*
- * removes an extent_map struct from the tree.  No reference counts are
- * dropped, and no checks are done to  see if the range is in use
+/**
+ * remove_extent_mapping - removes an extent_map from the extent tree
+ * @tree:	extent tree to remove from
+ * @em:		extent map beeing removed
+ *
+ * Removes @em from @tree.  No reference counts are dropped, and no checks
+ * are done to see if the range is in use
  */
 int remove_extent_mapping(struct extent_map_tree *tree, struct extent_map *em)
 {

commit 306929f364b993581c91596230807fa1c022268a
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Jun 10 10:21:04 2008 -0400

    btrfs: fix strange indentation in lookup_extent_mapping
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 268ad8facf6e..ba46f7911d99 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -231,7 +231,13 @@ struct extent_map *lookup_extent_mapping(struct extent_map_tree *tree,
 {
 	struct extent_map *em;
 	struct rb_node *rb_node;
-	struct rb_node *prev = NULL; struct rb_node *next = NULL; u64 end = range_end(start, len); em = tree->last; if (em && end > em->start && start < extent_map_end(em)) goto found;
+	struct rb_node *prev = NULL;
+	struct rb_node *next = NULL;
+	u64 end = range_end(start, len);
+
+	em = tree->last;
+	if (em && end > em->start && start < extent_map_end(em))
+		goto found;
 
 	rb_node = __tree_search(&tree->map, start, &prev, &next);
 	if (!rb_node && prev) {

commit d1310b2e0cd98eb1348553e69b73827b436dca7b
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Jan 24 16:13:08 2008 -0500

    Btrfs: Split the extent_map code into two parts
    
    There is now extent_map for mapping offsets in the file to disk and
    extent_io for state tracking, IO submission and extent_bufers.
    
    The new extent_map code shifts from [start,end] pairs to [start,len], and
    pushes the locking out into the caller.  This allows a few performance
    optimizations and is easier to use.
    
    A number of extent_map usage bugs were fixed, mostly with failing
    to remove extent_map entries when changing the file.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 010a287fbd71..268ad8facf6e 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -1,17 +1,10 @@
-#include <linux/bitops.h>
-#include <linux/slab.h>
-#include <linux/bio.h>
-#include <linux/mm.h>
+#include <linux/err.h>
 #include <linux/gfp.h>
-#include <linux/pagemap.h>
-#include <linux/page-flags.h>
+#include <linux/slab.h>
 #include <linux/module.h>
 #include <linux/spinlock.h>
-#include <linux/blkdev.h>
-#include <linux/swap.h>
 #include <linux/version.h>
-#include <linux/writeback.h>
-#include <linux/pagevec.h>
+#include <linux/hardirq.h>
 #include "extent_map.h"
 
 /* temporary define until extent_map moves out of btrfs */
@@ -21,27 +14,6 @@ struct kmem_cache *btrfs_cache_create(const char *name, size_t size,
 						    unsigned long));
 
 static struct kmem_cache *extent_map_cache;
-static struct kmem_cache *extent_state_cache;
-static struct kmem_cache *extent_buffer_cache;
-
-static LIST_HEAD(buffers);
-static LIST_HEAD(states);
-
-static spinlock_t state_lock = SPIN_LOCK_UNLOCKED;
-#define BUFFER_LRU_MAX 64
-
-struct tree_entry {
-	u64 start;
-	u64 end;
-	int in_tree;
-	struct rb_node rb_node;
-};
-
-struct extent_page_data {
-	struct bio *bio;
-	struct extent_map_tree *tree;
-	get_extent_t *get_extent;
-};
 
 int __init extent_map_init(void)
 {
@@ -50,72 +22,23 @@ int __init extent_map_init(void)
 					    NULL);
 	if (!extent_map_cache)
 		return -ENOMEM;
-	extent_state_cache = btrfs_cache_create("extent_state",
-					    sizeof(struct extent_state), 0,
-					    NULL);
-	if (!extent_state_cache)
-		goto free_map_cache;
-	extent_buffer_cache = btrfs_cache_create("extent_buffers",
-					    sizeof(struct extent_buffer), 0,
-					    NULL);
-	if (!extent_buffer_cache)
-		goto free_state_cache;
 	return 0;
-
-free_state_cache:
-	kmem_cache_destroy(extent_state_cache);
-free_map_cache:
-	kmem_cache_destroy(extent_map_cache);
-	return -ENOMEM;
 }
 
 void extent_map_exit(void)
 {
-	struct extent_state *state;
-
-	while (!list_empty(&states)) {
-		state = list_entry(states.next, struct extent_state, list);
-		printk("state leak: start %Lu end %Lu state %lu in tree %d refs %d\n", state->start, state->end, state->state, state->in_tree, atomic_read(&state->refs));
-		list_del(&state->list);
-		kmem_cache_free(extent_state_cache, state);
-
-	}
-
 	if (extent_map_cache)
 		kmem_cache_destroy(extent_map_cache);
-	if (extent_state_cache)
-		kmem_cache_destroy(extent_state_cache);
-	if (extent_buffer_cache)
-		kmem_cache_destroy(extent_buffer_cache);
 }
 
-void extent_map_tree_init(struct extent_map_tree *tree,
-			  struct address_space *mapping, gfp_t mask)
+void extent_map_tree_init(struct extent_map_tree *tree, gfp_t mask)
 {
 	tree->map.rb_node = NULL;
-	tree->state.rb_node = NULL;
-	tree->ops = NULL;
-	tree->dirty_bytes = 0;
-	rwlock_init(&tree->lock);
-	spin_lock_init(&tree->lru_lock);
-	tree->mapping = mapping;
-	INIT_LIST_HEAD(&tree->buffer_lru);
-	tree->lru_size = 0;
+	tree->last = NULL;
+	spin_lock_init(&tree->lock);
 }
 EXPORT_SYMBOL(extent_map_tree_init);
 
-void extent_map_tree_empty_lru(struct extent_map_tree *tree)
-{
-	struct extent_buffer *eb;
-	while(!list_empty(&tree->buffer_lru)) {
-		eb = list_entry(tree->buffer_lru.next, struct extent_buffer,
-				lru);
-		list_del_init(&eb->lru);
-		free_extent_buffer(eb);
-	}
-}
-EXPORT_SYMBOL(extent_map_tree_empty_lru);
-
 struct extent_map *alloc_extent_map(gfp_t mask)
 {
 	struct extent_map *em;
@@ -123,6 +46,7 @@ struct extent_map *alloc_extent_map(gfp_t mask)
 	if (!em || IS_ERR(em))
 		return em;
 	em->in_tree = 0;
+	em->flags = 0;
 	atomic_set(&em->refs, 1);
 	return em;
 }
@@ -132,6 +56,7 @@ void free_extent_map(struct extent_map *em)
 {
 	if (!em)
 		return;
+	WARN_ON(atomic_read(&em->refs) == 0);
 	if (atomic_dec_and_test(&em->refs)) {
 		WARN_ON(em->in_tree);
 		kmem_cache_free(extent_map_cache, em);
@@ -139,64 +64,28 @@ void free_extent_map(struct extent_map *em)
 }
 EXPORT_SYMBOL(free_extent_map);
 
-
-struct extent_state *alloc_extent_state(gfp_t mask)
-{
-	struct extent_state *state;
-	unsigned long flags;
-
-	state = kmem_cache_alloc(extent_state_cache, mask);
-	if (!state || IS_ERR(state))
-		return state;
-	state->state = 0;
-	state->in_tree = 0;
-	state->private = 0;
-
-	spin_lock_irqsave(&state_lock, flags);
-	list_add(&state->list, &states);
-	spin_unlock_irqrestore(&state_lock, flags);
-
-	atomic_set(&state->refs, 1);
-	init_waitqueue_head(&state->wq);
-	return state;
-}
-EXPORT_SYMBOL(alloc_extent_state);
-
-void free_extent_state(struct extent_state *state)
-{
-	unsigned long flags;
-	if (!state)
-		return;
-	if (atomic_dec_and_test(&state->refs)) {
-		WARN_ON(state->in_tree);
-		spin_lock_irqsave(&state_lock, flags);
-		list_del(&state->list);
-		spin_unlock_irqrestore(&state_lock, flags);
-		kmem_cache_free(extent_state_cache, state);
-	}
-}
-EXPORT_SYMBOL(free_extent_state);
-
 static struct rb_node *tree_insert(struct rb_root *root, u64 offset,
 				   struct rb_node *node)
 {
 	struct rb_node ** p = &root->rb_node;
 	struct rb_node * parent = NULL;
-	struct tree_entry *entry;
+	struct extent_map *entry;
 
 	while(*p) {
 		parent = *p;
-		entry = rb_entry(parent, struct tree_entry, rb_node);
+		entry = rb_entry(parent, struct extent_map, rb_node);
+
+		WARN_ON(!entry->in_tree);
 
 		if (offset < entry->start)
 			p = &(*p)->rb_left;
-		else if (offset > entry->end)
+		else if (offset >= extent_map_end(entry))
 			p = &(*p)->rb_right;
 		else
 			return parent;
 	}
 
-	entry = rb_entry(node, struct tree_entry, rb_node);
+	entry = rb_entry(node, struct extent_map, rb_node);
 	entry->in_tree = 1;
 	rb_link_node(node, parent, p);
 	rb_insert_color(node, root);
@@ -210,17 +99,19 @@ static struct rb_node *__tree_search(struct rb_root *root, u64 offset,
 	struct rb_node * n = root->rb_node;
 	struct rb_node *prev = NULL;
 	struct rb_node *orig_prev = NULL;
-	struct tree_entry *entry;
-	struct tree_entry *prev_entry = NULL;
+	struct extent_map *entry;
+	struct extent_map *prev_entry = NULL;
 
 	while(n) {
-		entry = rb_entry(n, struct tree_entry, rb_node);
+		entry = rb_entry(n, struct extent_map, rb_node);
 		prev = n;
 		prev_entry = entry;
 
+		WARN_ON(!entry->in_tree);
+
 		if (offset < entry->start)
 			n = n->rb_left;
-		else if (offset > entry->end)
+		else if (offset >= extent_map_end(entry))
 			n = n->rb_right;
 		else
 			return n;
@@ -228,19 +119,19 @@ static struct rb_node *__tree_search(struct rb_root *root, u64 offset,
 
 	if (prev_ret) {
 		orig_prev = prev;
-		while(prev && offset > prev_entry->end) {
+		while(prev && offset >= extent_map_end(prev_entry)) {
 			prev = rb_next(prev);
-			prev_entry = rb_entry(prev, struct tree_entry, rb_node);
+			prev_entry = rb_entry(prev, struct extent_map, rb_node);
 		}
 		*prev_ret = prev;
 		prev = orig_prev;
 	}
 
 	if (next_ret) {
-		prev_entry = rb_entry(prev, struct tree_entry, rb_node);
+		prev_entry = rb_entry(prev, struct extent_map, rb_node);
 		while(prev && offset < prev_entry->start) {
 			prev = rb_prev(prev);
-			prev_entry = rb_entry(prev, struct tree_entry, rb_node);
+			prev_entry = rb_entry(prev, struct extent_map, rb_node);
 		}
 		*next_ret = prev;
 	}
@@ -257,22 +148,26 @@ static inline struct rb_node *tree_search(struct rb_root *root, u64 offset)
 	return ret;
 }
 
-static int tree_delete(struct rb_root *root, u64 offset)
+static int mergable_maps(struct extent_map *prev, struct extent_map *next)
 {
-	struct rb_node *node;
-	struct tree_entry *entry;
-
-	node = __tree_search(root, offset, NULL, NULL);
-	if (!node)
-		return -ENOENT;
-	entry = rb_entry(node, struct tree_entry, rb_node);
-	entry->in_tree = 0;
-	rb_erase(node, root);
+	if (extent_map_end(prev) == next->start &&
+	    prev->flags == next->flags &&
+	    prev->bdev == next->bdev &&
+	    ((next->block_start == EXTENT_MAP_HOLE &&
+	      prev->block_start == EXTENT_MAP_HOLE) ||
+	     (next->block_start == EXTENT_MAP_INLINE &&
+	      prev->block_start == EXTENT_MAP_INLINE) ||
+	     (next->block_start == EXTENT_MAP_DELALLOC &&
+	      prev->block_start == EXTENT_MAP_DELALLOC) ||
+	     (next->block_start < EXTENT_MAP_LAST_BYTE - 1 &&
+	      next->block_start == extent_map_block_end(prev)))) {
+		return 1;
+	}
 	return 0;
 }
 
 /*
- * add_extent_mapping tries a simple backward merge with existing
+ * add_extent_mapping tries a simple forward/backward merge with existing
  * mappings.  The extent_map struct passed in will be inserted into
  * the tree directly (no copies made, just a reference taken).
  */
@@ -280,13 +175,12 @@ int add_extent_mapping(struct extent_map_tree *tree,
 		       struct extent_map *em)
 {
 	int ret = 0;
-	struct extent_map *prev = NULL;
+	struct extent_map *merge = NULL;
 	struct rb_node *rb;
 
-	write_lock_irq(&tree->lock);
-	rb = tree_insert(&tree->map, em->end, &em->rb_node);
+	rb = tree_insert(&tree->map, em->start, &em->rb_node);
 	if (rb) {
-		prev = rb_entry(rb, struct extent_map, rb_node);
+		merge = rb_entry(rb, struct extent_map, rb_node);
 		ret = -EEXIST;
 		goto out;
 	}
@@ -294,53 +188,60 @@ int add_extent_mapping(struct extent_map_tree *tree,
 	if (em->start != 0) {
 		rb = rb_prev(&em->rb_node);
 		if (rb)
-			prev = rb_entry(rb, struct extent_map, rb_node);
-		if (prev && prev->end + 1 == em->start &&
-		    ((em->block_start == EXTENT_MAP_HOLE &&
-		      prev->block_start == EXTENT_MAP_HOLE) ||
-		     (em->block_start == EXTENT_MAP_INLINE &&
-		      prev->block_start == EXTENT_MAP_INLINE) ||
-		     (em->block_start == EXTENT_MAP_DELALLOC &&
-		      prev->block_start == EXTENT_MAP_DELALLOC) ||
-		     (em->block_start < EXTENT_MAP_DELALLOC - 1 &&
-		      em->block_start == prev->block_end + 1))) {
-			em->start = prev->start;
-			em->block_start = prev->block_start;
-			rb_erase(&prev->rb_node, &tree->map);
-			prev->in_tree = 0;
-			free_extent_map(prev);
+			merge = rb_entry(rb, struct extent_map, rb_node);
+		if (rb && mergable_maps(merge, em)) {
+			em->start = merge->start;
+			em->len += merge->len;
+			em->block_start = merge->block_start;
+			merge->in_tree = 0;
+			rb_erase(&merge->rb_node, &tree->map);
+			free_extent_map(merge);
 		}
 	 }
+	rb = rb_next(&em->rb_node);
+	if (rb)
+		merge = rb_entry(rb, struct extent_map, rb_node);
+	if (rb && mergable_maps(em, merge)) {
+		em->len += merge->len;
+		rb_erase(&merge->rb_node, &tree->map);
+		merge->in_tree = 0;
+		free_extent_map(merge);
+	}
+	tree->last = em;
 out:
-	write_unlock_irq(&tree->lock);
 	return ret;
 }
 EXPORT_SYMBOL(add_extent_mapping);
 
+static u64 range_end(u64 start, u64 len)
+{
+	if (start + len < start)
+		return (u64)-1;
+	return start + len;
+}
+
 /*
  * lookup_extent_mapping returns the first extent_map struct in the
- * tree that intersects the [start, end] (inclusive) range.  There may
+ * tree that intersects the [start, len] range.  There may
  * be additional objects in the tree that intersect, so check the object
  * returned carefully to make sure you don't need additional lookups.
  */
 struct extent_map *lookup_extent_mapping(struct extent_map_tree *tree,
-					 u64 start, u64 end)
+					 u64 start, u64 len)
 {
 	struct extent_map *em;
 	struct rb_node *rb_node;
-	struct rb_node *prev = NULL;
-	struct rb_node *next = NULL;
+	struct rb_node *prev = NULL; struct rb_node *next = NULL; u64 end = range_end(start, len); em = tree->last; if (em && end > em->start && start < extent_map_end(em)) goto found;
 
-	read_lock_irq(&tree->lock);
 	rb_node = __tree_search(&tree->map, start, &prev, &next);
 	if (!rb_node && prev) {
 		em = rb_entry(prev, struct extent_map, rb_node);
-		if (em->start <= end && em->end >= start)
+		if (end > em->start && start < extent_map_end(em))
 			goto found;
 	}
 	if (!rb_node && next) {
 		em = rb_entry(next, struct extent_map, rb_node);
-		if (em->start <= end && em->end >= start)
+		if (end > em->start && start < extent_map_end(em))
 			goto found;
 	}
 	if (!rb_node) {
@@ -352,14 +253,16 @@ struct extent_map *lookup_extent_mapping(struct extent_map_tree *tree,
 		goto out;
 	}
 	em = rb_entry(rb_node, struct extent_map, rb_node);
-	if (em->end < start || em->start > end) {
-		em = NULL;
-		goto out;
-	}
+	if (end > em->start && start < extent_map_end(em))
+		goto found;
+
+	em = NULL;
+	goto out;
+
 found:
 	atomic_inc(&em->refs);
+	tree->last = em;
 out:
-	read_unlock_irq(&tree->lock);
 	return em;
 }
 EXPORT_SYMBOL(lookup_extent_mapping);
@@ -370,2866 +273,12 @@ EXPORT_SYMBOL(lookup_extent_mapping);
  */
 int remove_extent_mapping(struct extent_map_tree *tree, struct extent_map *em)
 {
-	int ret;
+	int ret = 0;
 
-	write_lock_irq(&tree->lock);
-	ret = tree_delete(&tree->map, em->end);
-	write_unlock_irq(&tree->lock);
+	rb_erase(&em->rb_node, &tree->map);
+	em->in_tree = 0;
+	if (tree->last == em)
+		tree->last = NULL;
 	return ret;
 }
 EXPORT_SYMBOL(remove_extent_mapping);
-
-/*
- * utility function to look for merge candidates inside a given range.
- * Any extents with matching state are merged together into a single
- * extent in the tree.  Extents with EXTENT_IO in their state field
- * are not merged because the end_io handlers need to be able to do
- * operations on them without sleeping (or doing allocations/splits).
- *
- * This should be called with the tree lock held.
- */
-static int merge_state(struct extent_map_tree *tree,
-		       struct extent_state *state)
-{
-	struct extent_state *other;
-	struct rb_node *other_node;
-
-	if (state->state & EXTENT_IOBITS)
-		return 0;
-
-	other_node = rb_prev(&state->rb_node);
-	if (other_node) {
-		other = rb_entry(other_node, struct extent_state, rb_node);
-		if (other->end == state->start - 1 &&
-		    other->state == state->state) {
-			state->start = other->start;
-			other->in_tree = 0;
-			rb_erase(&other->rb_node, &tree->state);
-			free_extent_state(other);
-		}
-	}
-	other_node = rb_next(&state->rb_node);
-	if (other_node) {
-		other = rb_entry(other_node, struct extent_state, rb_node);
-		if (other->start == state->end + 1 &&
-		    other->state == state->state) {
-			other->start = state->start;
-			state->in_tree = 0;
-			rb_erase(&state->rb_node, &tree->state);
-			free_extent_state(state);
-		}
-	}
-	return 0;
-}
-
-/*
- * insert an extent_state struct into the tree.  'bits' are set on the
- * struct before it is inserted.
- *
- * This may return -EEXIST if the extent is already there, in which case the
- * state struct is freed.
- *
- * The tree lock is not taken internally.  This is a utility function and
- * probably isn't what you want to call (see set/clear_extent_bit).
- */
-static int insert_state(struct extent_map_tree *tree,
-			struct extent_state *state, u64 start, u64 end,
-			int bits)
-{
-	struct rb_node *node;
-
-	if (end < start) {
-		printk("end < start %Lu %Lu\n", end, start);
-		WARN_ON(1);
-	}
-	if (bits & EXTENT_DIRTY)
-		tree->dirty_bytes += end - start + 1;
-	state->state |= bits;
-	state->start = start;
-	state->end = end;
-	node = tree_insert(&tree->state, end, &state->rb_node);
-	if (node) {
-		struct extent_state *found;
-		found = rb_entry(node, struct extent_state, rb_node);
-		printk("found node %Lu %Lu on insert of %Lu %Lu\n", found->start, found->end, start, end);
-		free_extent_state(state);
-		return -EEXIST;
-	}
-	merge_state(tree, state);
-	return 0;
-}
-
-/*
- * split a given extent state struct in two, inserting the preallocated
- * struct 'prealloc' as the newly created second half.  'split' indicates an
- * offset inside 'orig' where it should be split.
- *
- * Before calling,
- * the tree has 'orig' at [orig->start, orig->end].  After calling, there
- * are two extent state structs in the tree:
- * prealloc: [orig->start, split - 1]
- * orig: [ split, orig->end ]
- *
- * The tree locks are not taken by this function. They need to be held
- * by the caller.
- */
-static int split_state(struct extent_map_tree *tree, struct extent_state *orig,
-		       struct extent_state *prealloc, u64 split)
-{
-	struct rb_node *node;
-	prealloc->start = orig->start;
-	prealloc->end = split - 1;
-	prealloc->state = orig->state;
-	orig->start = split;
-
-	node = tree_insert(&tree->state, prealloc->end, &prealloc->rb_node);
-	if (node) {
-		struct extent_state *found;
-		found = rb_entry(node, struct extent_state, rb_node);
-		printk("found node %Lu %Lu on insert of %Lu %Lu\n", found->start, found->end, prealloc->start, prealloc->end);
-		free_extent_state(prealloc);
-		return -EEXIST;
-	}
-	return 0;
-}
-
-/*
- * utility function to clear some bits in an extent state struct.
- * it will optionally wake up any one waiting on this state (wake == 1), or
- * forcibly remove the state from the tree (delete == 1).
- *
- * If no bits are set on the state struct after clearing things, the
- * struct is freed and removed from the tree
- */
-static int clear_state_bit(struct extent_map_tree *tree,
-			    struct extent_state *state, int bits, int wake,
-			    int delete)
-{
-	int ret = state->state & bits;
-
-	if ((bits & EXTENT_DIRTY) && (state->state & EXTENT_DIRTY)) {
-		u64 range = state->end - state->start + 1;
-		WARN_ON(range > tree->dirty_bytes);
-		tree->dirty_bytes -= range;
-	}
-	state->state &= ~bits;
-	if (wake)
-		wake_up(&state->wq);
-	if (delete || state->state == 0) {
-		if (state->in_tree) {
-			rb_erase(&state->rb_node, &tree->state);
-			state->in_tree = 0;
-			free_extent_state(state);
-		} else {
-			WARN_ON(1);
-		}
-	} else {
-		merge_state(tree, state);
-	}
-	return ret;
-}
-
-/*
- * clear some bits on a range in the tree.  This may require splitting
- * or inserting elements in the tree, so the gfp mask is used to
- * indicate which allocations or sleeping are allowed.
- *
- * pass 'wake' == 1 to kick any sleepers, and 'delete' == 1 to remove
- * the given range from the tree regardless of state (ie for truncate).
- *
- * the range [start, end] is inclusive.
- *
- * This takes the tree lock, and returns < 0 on error, > 0 if any of the
- * bits were already set, or zero if none of the bits were already set.
- */
-int clear_extent_bit(struct extent_map_tree *tree, u64 start, u64 end,
-		     int bits, int wake, int delete, gfp_t mask)
-{
-	struct extent_state *state;
-	struct extent_state *prealloc = NULL;
-	struct rb_node *node;
-	unsigned long flags;
-	int err;
-	int set = 0;
-
-again:
-	if (!prealloc && (mask & __GFP_WAIT)) {
-		prealloc = alloc_extent_state(mask);
-		if (!prealloc)
-			return -ENOMEM;
-	}
-
-	write_lock_irqsave(&tree->lock, flags);
-	/*
-	 * this search will find the extents that end after
-	 * our range starts
-	 */
-	node = tree_search(&tree->state, start);
-	if (!node)
-		goto out;
-	state = rb_entry(node, struct extent_state, rb_node);
-	if (state->start > end)
-		goto out;
-	WARN_ON(state->end < start);
-
-	/*
-	 *     | ---- desired range ---- |
-	 *  | state | or
-	 *  | ------------- state -------------- |
-	 *
-	 * We need to split the extent we found, and may flip
-	 * bits on second half.
-	 *
-	 * If the extent we found extends past our range, we
-	 * just split and search again.  It'll get split again
-	 * the next time though.
-	 *
-	 * If the extent we found is inside our range, we clear
-	 * the desired bit on it.
-	 */
-
-	if (state->start < start) {
-		err = split_state(tree, state, prealloc, start);
-		BUG_ON(err == -EEXIST);
-		prealloc = NULL;
-		if (err)
-			goto out;
-		if (state->end <= end) {
-			start = state->end + 1;
-			set |= clear_state_bit(tree, state, bits,
-					wake, delete);
-		} else {
-			start = state->start;
-		}
-		goto search_again;
-	}
-	/*
-	 * | ---- desired range ---- |
-	 *                        | state |
-	 * We need to split the extent, and clear the bit
-	 * on the first half
-	 */
-	if (state->start <= end && state->end > end) {
-		err = split_state(tree, state, prealloc, end + 1);
-		BUG_ON(err == -EEXIST);
-
-		if (wake)
-			wake_up(&state->wq);
-		set |= clear_state_bit(tree, prealloc, bits,
-				       wake, delete);
-		prealloc = NULL;
-		goto out;
-	}
-
-	start = state->end + 1;
-	set |= clear_state_bit(tree, state, bits, wake, delete);
-	goto search_again;
-
-out:
-	write_unlock_irqrestore(&tree->lock, flags);
-	if (prealloc)
-		free_extent_state(prealloc);
-
-	return set;
-
-search_again:
-	if (start > end)
-		goto out;
-	write_unlock_irqrestore(&tree->lock, flags);
-	if (mask & __GFP_WAIT)
-		cond_resched();
-	goto again;
-}
-EXPORT_SYMBOL(clear_extent_bit);
-
-static int wait_on_state(struct extent_map_tree *tree,
-			 struct extent_state *state)
-{
-	DEFINE_WAIT(wait);
-	prepare_to_wait(&state->wq, &wait, TASK_UNINTERRUPTIBLE);
-	read_unlock_irq(&tree->lock);
-	schedule();
-	read_lock_irq(&tree->lock);
-	finish_wait(&state->wq, &wait);
-	return 0;
-}
-
-/*
- * waits for one or more bits to clear on a range in the state tree.
- * The range [start, end] is inclusive.
- * The tree lock is taken by this function
- */
-int wait_extent_bit(struct extent_map_tree *tree, u64 start, u64 end, int bits)
-{
-	struct extent_state *state;
-	struct rb_node *node;
-
-	read_lock_irq(&tree->lock);
-again:
-	while (1) {
-		/*
-		 * this search will find all the extents that end after
-		 * our range starts
-		 */
-		node = tree_search(&tree->state, start);
-		if (!node)
-			break;
-
-		state = rb_entry(node, struct extent_state, rb_node);
-
-		if (state->start > end)
-			goto out;
-
-		if (state->state & bits) {
-			start = state->start;
-			atomic_inc(&state->refs);
-			wait_on_state(tree, state);
-			free_extent_state(state);
-			goto again;
-		}
-		start = state->end + 1;
-
-		if (start > end)
-			break;
-
-		if (need_resched()) {
-			read_unlock_irq(&tree->lock);
-			cond_resched();
-			read_lock_irq(&tree->lock);
-		}
-	}
-out:
-	read_unlock_irq(&tree->lock);
-	return 0;
-}
-EXPORT_SYMBOL(wait_extent_bit);
-
-static void set_state_bits(struct extent_map_tree *tree,
-			   struct extent_state *state,
-			   int bits)
-{
-	if ((bits & EXTENT_DIRTY) && !(state->state & EXTENT_DIRTY)) {
-		u64 range = state->end - state->start + 1;
-		tree->dirty_bytes += range;
-	}
-	state->state |= bits;
-}
-
-/*
- * set some bits on a range in the tree.  This may require allocations
- * or sleeping, so the gfp mask is used to indicate what is allowed.
- *
- * If 'exclusive' == 1, this will fail with -EEXIST if some part of the
- * range already has the desired bits set.  The start of the existing
- * range is returned in failed_start in this case.
- *
- * [start, end] is inclusive
- * This takes the tree lock.
- */
-int set_extent_bit(struct extent_map_tree *tree, u64 start, u64 end, int bits,
-		   int exclusive, u64 *failed_start, gfp_t mask)
-{
-	struct extent_state *state;
-	struct extent_state *prealloc = NULL;
-	struct rb_node *node;
-	unsigned long flags;
-	int err = 0;
-	int set;
-	u64 last_start;
-	u64 last_end;
-again:
-	if (!prealloc && (mask & __GFP_WAIT)) {
-		prealloc = alloc_extent_state(mask);
-		if (!prealloc)
-			return -ENOMEM;
-	}
-
-	write_lock_irqsave(&tree->lock, flags);
-	/*
-	 * this search will find all the extents that end after
-	 * our range starts.
-	 */
-	node = tree_search(&tree->state, start);
-	if (!node) {
-		err = insert_state(tree, prealloc, start, end, bits);
-		prealloc = NULL;
-		BUG_ON(err == -EEXIST);
-		goto out;
-	}
-
-	state = rb_entry(node, struct extent_state, rb_node);
-	last_start = state->start;
-	last_end = state->end;
-
-	/*
-	 * | ---- desired range ---- |
-	 * | state |
-	 *
-	 * Just lock what we found and keep going
-	 */
-	if (state->start == start && state->end <= end) {
-		set = state->state & bits;
-		if (set && exclusive) {
-			*failed_start = state->start;
-			err = -EEXIST;
-			goto out;
-		}
-		set_state_bits(tree, state, bits);
-		start = state->end + 1;
-		merge_state(tree, state);
-		goto search_again;
-	}
-
-	/*
-	 *     | ---- desired range ---- |
-	 * | state |
-	 *   or
-	 * | ------------- state -------------- |
-	 *
-	 * We need to split the extent we found, and may flip bits on
-	 * second half.
-	 *
-	 * If the extent we found extends past our
-	 * range, we just split and search again.  It'll get split
-	 * again the next time though.
-	 *
-	 * If the extent we found is inside our range, we set the
-	 * desired bit on it.
-	 */
-	if (state->start < start) {
-		set = state->state & bits;
-		if (exclusive && set) {
-			*failed_start = start;
-			err = -EEXIST;
-			goto out;
-		}
-		err = split_state(tree, state, prealloc, start);
-		BUG_ON(err == -EEXIST);
-		prealloc = NULL;
-		if (err)
-			goto out;
-		if (state->end <= end) {
-			set_state_bits(tree, state, bits);
-			start = state->end + 1;
-			merge_state(tree, state);
-		} else {
-			start = state->start;
-		}
-		goto search_again;
-	}
-	/*
-	 * | ---- desired range ---- |
-	 *     | state | or               | state |
-	 *
-	 * There's a hole, we need to insert something in it and
-	 * ignore the extent we found.
-	 */
-	if (state->start > start) {
-		u64 this_end;
-		if (end < last_start)
-			this_end = end;
-		else
-			this_end = last_start -1;
-		err = insert_state(tree, prealloc, start, this_end,
-				   bits);
-		prealloc = NULL;
-		BUG_ON(err == -EEXIST);
-		if (err)
-			goto out;
-		start = this_end + 1;
-		goto search_again;
-	}
-	/*
-	 * | ---- desired range ---- |
-	 *                        | state |
-	 * We need to split the extent, and set the bit
-	 * on the first half
-	 */
-	if (state->start <= end && state->end > end) {
-		set = state->state & bits;
-		if (exclusive && set) {
-			*failed_start = start;
-			err = -EEXIST;
-			goto out;
-		}
-		err = split_state(tree, state, prealloc, end + 1);
-		BUG_ON(err == -EEXIST);
-
-		set_state_bits(tree, prealloc, bits);
-		merge_state(tree, prealloc);
-		prealloc = NULL;
-		goto out;
-	}
-
-	goto search_again;
-
-out:
-	write_unlock_irqrestore(&tree->lock, flags);
-	if (prealloc)
-		free_extent_state(prealloc);
-
-	return err;
-
-search_again:
-	if (start > end)
-		goto out;
-	write_unlock_irqrestore(&tree->lock, flags);
-	if (mask & __GFP_WAIT)
-		cond_resched();
-	goto again;
-}
-EXPORT_SYMBOL(set_extent_bit);
-
-/* wrappers around set/clear extent bit */
-int set_extent_dirty(struct extent_map_tree *tree, u64 start, u64 end,
-		     gfp_t mask)
-{
-	return set_extent_bit(tree, start, end, EXTENT_DIRTY, 0, NULL,
-			      mask);
-}
-EXPORT_SYMBOL(set_extent_dirty);
-
-int set_extent_bits(struct extent_map_tree *tree, u64 start, u64 end,
-		    int bits, gfp_t mask)
-{
-	return set_extent_bit(tree, start, end, bits, 0, NULL,
-			      mask);
-}
-EXPORT_SYMBOL(set_extent_bits);
-
-int clear_extent_bits(struct extent_map_tree *tree, u64 start, u64 end,
-		      int bits, gfp_t mask)
-{
-	return clear_extent_bit(tree, start, end, bits, 0, 0, mask);
-}
-EXPORT_SYMBOL(clear_extent_bits);
-
-int set_extent_delalloc(struct extent_map_tree *tree, u64 start, u64 end,
-		     gfp_t mask)
-{
-	return set_extent_bit(tree, start, end,
-			      EXTENT_DELALLOC | EXTENT_DIRTY, 0, NULL,
-			      mask);
-}
-EXPORT_SYMBOL(set_extent_delalloc);
-
-int clear_extent_dirty(struct extent_map_tree *tree, u64 start, u64 end,
-		       gfp_t mask)
-{
-	return clear_extent_bit(tree, start, end,
-				EXTENT_DIRTY | EXTENT_DELALLOC, 0, 0, mask);
-}
-EXPORT_SYMBOL(clear_extent_dirty);
-
-int set_extent_new(struct extent_map_tree *tree, u64 start, u64 end,
-		     gfp_t mask)
-{
-	return set_extent_bit(tree, start, end, EXTENT_NEW, 0, NULL,
-			      mask);
-}
-EXPORT_SYMBOL(set_extent_new);
-
-int clear_extent_new(struct extent_map_tree *tree, u64 start, u64 end,
-		       gfp_t mask)
-{
-	return clear_extent_bit(tree, start, end, EXTENT_NEW, 0, 0, mask);
-}
-EXPORT_SYMBOL(clear_extent_new);
-
-int set_extent_uptodate(struct extent_map_tree *tree, u64 start, u64 end,
-			gfp_t mask)
-{
-	return set_extent_bit(tree, start, end, EXTENT_UPTODATE, 0, NULL,
-			      mask);
-}
-EXPORT_SYMBOL(set_extent_uptodate);
-
-int clear_extent_uptodate(struct extent_map_tree *tree, u64 start, u64 end,
-			  gfp_t mask)
-{
-	return clear_extent_bit(tree, start, end, EXTENT_UPTODATE, 0, 0, mask);
-}
-EXPORT_SYMBOL(clear_extent_uptodate);
-
-int set_extent_writeback(struct extent_map_tree *tree, u64 start, u64 end,
-			 gfp_t mask)
-{
-	return set_extent_bit(tree, start, end, EXTENT_WRITEBACK,
-			      0, NULL, mask);
-}
-EXPORT_SYMBOL(set_extent_writeback);
-
-int clear_extent_writeback(struct extent_map_tree *tree, u64 start, u64 end,
-			   gfp_t mask)
-{
-	return clear_extent_bit(tree, start, end, EXTENT_WRITEBACK, 1, 0, mask);
-}
-EXPORT_SYMBOL(clear_extent_writeback);
-
-int wait_on_extent_writeback(struct extent_map_tree *tree, u64 start, u64 end)
-{
-	return wait_extent_bit(tree, start, end, EXTENT_WRITEBACK);
-}
-EXPORT_SYMBOL(wait_on_extent_writeback);
-
-/*
- * locks a range in ascending order, waiting for any locked regions
- * it hits on the way.  [start,end] are inclusive, and this will sleep.
- */
-int lock_extent(struct extent_map_tree *tree, u64 start, u64 end, gfp_t mask)
-{
-	int err;
-	u64 failed_start;
-	while (1) {
-		err = set_extent_bit(tree, start, end, EXTENT_LOCKED, 1,
-				     &failed_start, mask);
-		if (err == -EEXIST && (mask & __GFP_WAIT)) {
-			wait_extent_bit(tree, failed_start, end, EXTENT_LOCKED);
-			start = failed_start;
-		} else {
-			break;
-		}
-		WARN_ON(start > end);
-	}
-	return err;
-}
-EXPORT_SYMBOL(lock_extent);
-
-int unlock_extent(struct extent_map_tree *tree, u64 start, u64 end,
-		  gfp_t mask)
-{
-	return clear_extent_bit(tree, start, end, EXTENT_LOCKED, 1, 0, mask);
-}
-EXPORT_SYMBOL(unlock_extent);
-
-/*
- * helper function to set pages and extents in the tree dirty
- */
-int set_range_dirty(struct extent_map_tree *tree, u64 start, u64 end)
-{
-	unsigned long index = start >> PAGE_CACHE_SHIFT;
-	unsigned long end_index = end >> PAGE_CACHE_SHIFT;
-	struct page *page;
-
-	while (index <= end_index) {
-		page = find_get_page(tree->mapping, index);
-		BUG_ON(!page);
-		__set_page_dirty_nobuffers(page);
-		page_cache_release(page);
-		index++;
-	}
-	set_extent_dirty(tree, start, end, GFP_NOFS);
-	return 0;
-}
-EXPORT_SYMBOL(set_range_dirty);
-
-/*
- * helper function to set both pages and extents in the tree writeback
- */
-int set_range_writeback(struct extent_map_tree *tree, u64 start, u64 end)
-{
-	unsigned long index = start >> PAGE_CACHE_SHIFT;
-	unsigned long end_index = end >> PAGE_CACHE_SHIFT;
-	struct page *page;
-
-	while (index <= end_index) {
-		page = find_get_page(tree->mapping, index);
-		BUG_ON(!page);
-		set_page_writeback(page);
-		page_cache_release(page);
-		index++;
-	}
-	set_extent_writeback(tree, start, end, GFP_NOFS);
-	return 0;
-}
-EXPORT_SYMBOL(set_range_writeback);
-
-int find_first_extent_bit(struct extent_map_tree *tree, u64 start,
-			  u64 *start_ret, u64 *end_ret, int bits)
-{
-	struct rb_node *node;
-	struct extent_state *state;
-	int ret = 1;
-
-	read_lock_irq(&tree->lock);
-	/*
-	 * this search will find all the extents that end after
-	 * our range starts.
-	 */
-	node = tree_search(&tree->state, start);
-	if (!node || IS_ERR(node)) {
-		goto out;
-	}
-
-	while(1) {
-		state = rb_entry(node, struct extent_state, rb_node);
-		if (state->end >= start && (state->state & bits)) {
-			*start_ret = state->start;
-			*end_ret = state->end;
-			ret = 0;
-			break;
-		}
-		node = rb_next(node);
-		if (!node)
-			break;
-	}
-out:
-	read_unlock_irq(&tree->lock);
-	return ret;
-}
-EXPORT_SYMBOL(find_first_extent_bit);
-
-u64 find_lock_delalloc_range(struct extent_map_tree *tree,
-			     u64 *start, u64 *end, u64 max_bytes)
-{
-	struct rb_node *node;
-	struct extent_state *state;
-	u64 cur_start = *start;
-	u64 found = 0;
-	u64 total_bytes = 0;
-
-	write_lock_irq(&tree->lock);
-	/*
-	 * this search will find all the extents that end after
-	 * our range starts.
-	 */
-search_again:
-	node = tree_search(&tree->state, cur_start);
-	if (!node || IS_ERR(node)) {
-		*end = (u64)-1;
-		goto out;
-	}
-
-	while(1) {
-		state = rb_entry(node, struct extent_state, rb_node);
-		if (found && state->start != cur_start) {
-			goto out;
-		}
-		if (!(state->state & EXTENT_DELALLOC)) {
-			if (!found)
-				*end = state->end;
-			goto out;
-		}
-		if (!found) {
-			struct extent_state *prev_state;
-			struct rb_node *prev_node = node;
-			while(1) {
-				prev_node = rb_prev(prev_node);
-				if (!prev_node)
-					break;
-				prev_state = rb_entry(prev_node,
-						      struct extent_state,
-						      rb_node);
-				if (!(prev_state->state & EXTENT_DELALLOC))
-					break;
-				state = prev_state;
-				node = prev_node;
-			}
-		}
-		if (state->state & EXTENT_LOCKED) {
-			DEFINE_WAIT(wait);
-			atomic_inc(&state->refs);
-			prepare_to_wait(&state->wq, &wait,
-					TASK_UNINTERRUPTIBLE);
-			write_unlock_irq(&tree->lock);
-			schedule();
-			write_lock_irq(&tree->lock);
-			finish_wait(&state->wq, &wait);
-			free_extent_state(state);
-			goto search_again;
-		}
-		state->state |= EXTENT_LOCKED;
-		if (!found)
-			*start = state->start;
-		found++;
-		*end = state->end;
-		cur_start = state->end + 1;
-		node = rb_next(node);
-		if (!node)
-			break;
-		total_bytes += state->end - state->start + 1;
-		if (total_bytes >= max_bytes)
-			break;
-	}
-out:
-	write_unlock_irq(&tree->lock);
-	return found;
-}
-
-u64 count_range_bits(struct extent_map_tree *tree,
-		     u64 *start, u64 search_end, u64 max_bytes,
-		     unsigned long bits)
-{
-	struct rb_node *node;
-	struct extent_state *state;
-	u64 cur_start = *start;
-	u64 total_bytes = 0;
-	int found = 0;
-
-	if (search_end <= cur_start) {
-		printk("search_end %Lu start %Lu\n", search_end, cur_start);
-		WARN_ON(1);
-		return 0;
-	}
-
-	write_lock_irq(&tree->lock);
-	if (cur_start == 0 && bits == EXTENT_DIRTY) {
-		total_bytes = tree->dirty_bytes;
-		goto out;
-	}
-	/*
-	 * this search will find all the extents that end after
-	 * our range starts.
-	 */
-	node = tree_search(&tree->state, cur_start);
-	if (!node || IS_ERR(node)) {
-		goto out;
-	}
-
-	while(1) {
-		state = rb_entry(node, struct extent_state, rb_node);
-		if (state->start > search_end)
-			break;
-		if (state->end >= cur_start && (state->state & bits)) {
-			total_bytes += min(search_end, state->end) + 1 -
-				       max(cur_start, state->start);
-			if (total_bytes >= max_bytes)
-				break;
-			if (!found) {
-				*start = state->start;
-				found = 1;
-			}
-		}
-		node = rb_next(node);
-		if (!node)
-			break;
-	}
-out:
-	write_unlock_irq(&tree->lock);
-	return total_bytes;
-}
-/*
- * helper function to lock both pages and extents in the tree.
- * pages must be locked first.
- */
-int lock_range(struct extent_map_tree *tree, u64 start, u64 end)
-{
-	unsigned long index = start >> PAGE_CACHE_SHIFT;
-	unsigned long end_index = end >> PAGE_CACHE_SHIFT;
-	struct page *page;
-	int err;
-
-	while (index <= end_index) {
-		page = grab_cache_page(tree->mapping, index);
-		if (!page) {
-			err = -ENOMEM;
-			goto failed;
-		}
-		if (IS_ERR(page)) {
-			err = PTR_ERR(page);
-			goto failed;
-		}
-		index++;
-	}
-	lock_extent(tree, start, end, GFP_NOFS);
-	return 0;
-
-failed:
-	/*
-	 * we failed above in getting the page at 'index', so we undo here
-	 * up to but not including the page at 'index'
-	 */
-	end_index = index;
-	index = start >> PAGE_CACHE_SHIFT;
-	while (index < end_index) {
-		page = find_get_page(tree->mapping, index);
-		unlock_page(page);
-		page_cache_release(page);
-		index++;
-	}
-	return err;
-}
-EXPORT_SYMBOL(lock_range);
-
-/*
- * helper function to unlock both pages and extents in the tree.
- */
-int unlock_range(struct extent_map_tree *tree, u64 start, u64 end)
-{
-	unsigned long index = start >> PAGE_CACHE_SHIFT;
-	unsigned long end_index = end >> PAGE_CACHE_SHIFT;
-	struct page *page;
-
-	while (index <= end_index) {
-		page = find_get_page(tree->mapping, index);
-		unlock_page(page);
-		page_cache_release(page);
-		index++;
-	}
-	unlock_extent(tree, start, end, GFP_NOFS);
-	return 0;
-}
-EXPORT_SYMBOL(unlock_range);
-
-int set_state_private(struct extent_map_tree *tree, u64 start, u64 private)
-{
-	struct rb_node *node;
-	struct extent_state *state;
-	int ret = 0;
-
-	write_lock_irq(&tree->lock);
-	/*
-	 * this search will find all the extents that end after
-	 * our range starts.
-	 */
-	node = tree_search(&tree->state, start);
-	if (!node || IS_ERR(node)) {
-		ret = -ENOENT;
-		goto out;
-	}
-	state = rb_entry(node, struct extent_state, rb_node);
-	if (state->start != start) {
-		ret = -ENOENT;
-		goto out;
-	}
-	state->private = private;
-out:
-	write_unlock_irq(&tree->lock);
-	return ret;
-}
-
-int get_state_private(struct extent_map_tree *tree, u64 start, u64 *private)
-{
-	struct rb_node *node;
-	struct extent_state *state;
-	int ret = 0;
-
-	read_lock_irq(&tree->lock);
-	/*
-	 * this search will find all the extents that end after
-	 * our range starts.
-	 */
-	node = tree_search(&tree->state, start);
-	if (!node || IS_ERR(node)) {
-		ret = -ENOENT;
-		goto out;
-	}
-	state = rb_entry(node, struct extent_state, rb_node);
-	if (state->start != start) {
-		ret = -ENOENT;
-		goto out;
-	}
-	*private = state->private;
-out:
-	read_unlock_irq(&tree->lock);
-	return ret;
-}
-
-/*
- * searches a range in the state tree for a given mask.
- * If 'filled' == 1, this returns 1 only if ever extent in the tree
- * has the bits set.  Otherwise, 1 is returned if any bit in the
- * range is found set.
- */
-int test_range_bit(struct extent_map_tree *tree, u64 start, u64 end,
-		   int bits, int filled)
-{
-	struct extent_state *state = NULL;
-	struct rb_node *node;
-	int bitset = 0;
-
-	read_lock_irq(&tree->lock);
-	node = tree_search(&tree->state, start);
-	while (node && start <= end) {
-		state = rb_entry(node, struct extent_state, rb_node);
-
-		if (filled && state->start > start) {
-			bitset = 0;
-			break;
-		}
-
-		if (state->start > end)
-			break;
-
-		if (state->state & bits) {
-			bitset = 1;
-			if (!filled)
-				break;
-		} else if (filled) {
-			bitset = 0;
-			break;
-		}
-		start = state->end + 1;
-		if (start > end)
-			break;
-		node = rb_next(node);
-		if (!node) {
-			if (filled)
-				bitset = 0;
-			break;
-		}
-	}
-	read_unlock_irq(&tree->lock);
-	return bitset;
-}
-EXPORT_SYMBOL(test_range_bit);
-
-/*
- * helper function to set a given page up to date if all the
- * extents in the tree for that page are up to date
- */
-static int check_page_uptodate(struct extent_map_tree *tree,
-			       struct page *page)
-{
-	u64 start = (u64)page->index << PAGE_CACHE_SHIFT;
-	u64 end = start + PAGE_CACHE_SIZE - 1;
-	if (test_range_bit(tree, start, end, EXTENT_UPTODATE, 1))
-		SetPageUptodate(page);
-	return 0;
-}
-
-/*
- * helper function to unlock a page if all the extents in the tree
- * for that page are unlocked
- */
-static int check_page_locked(struct extent_map_tree *tree,
-			     struct page *page)
-{
-	u64 start = (u64)page->index << PAGE_CACHE_SHIFT;
-	u64 end = start + PAGE_CACHE_SIZE - 1;
-	if (!test_range_bit(tree, start, end, EXTENT_LOCKED, 0))
-		unlock_page(page);
-	return 0;
-}
-
-/*
- * helper function to end page writeback if all the extents
- * in the tree for that page are done with writeback
- */
-static int check_page_writeback(struct extent_map_tree *tree,
-			     struct page *page)
-{
-	u64 start = (u64)page->index << PAGE_CACHE_SHIFT;
-	u64 end = start + PAGE_CACHE_SIZE - 1;
-	if (!test_range_bit(tree, start, end, EXTENT_WRITEBACK, 0))
-		end_page_writeback(page);
-	return 0;
-}
-
-/* lots and lots of room for performance fixes in the end_bio funcs */
-
-/*
- * after a writepage IO is done, we need to:
- * clear the uptodate bits on error
- * clear the writeback bits in the extent tree for this IO
- * end_page_writeback if the page has no more pending IO
- *
- * Scheduling is not allowed, so the extent state tree is expected
- * to have one and only one object corresponding to this IO.
- */
-#if LINUX_VERSION_CODE > KERNEL_VERSION(2,6,23)
-static void end_bio_extent_writepage(struct bio *bio, int err)
-#else
-static int end_bio_extent_writepage(struct bio *bio,
-				   unsigned int bytes_done, int err)
-#endif
-{
-	const int uptodate = test_bit(BIO_UPTODATE, &bio->bi_flags);
-	struct bio_vec *bvec = bio->bi_io_vec + bio->bi_vcnt - 1;
-	struct extent_map_tree *tree = bio->bi_private;
-	u64 start;
-	u64 end;
-	int whole_page;
-
-#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,23)
-	if (bio->bi_size)
-		return 1;
-#endif
-
-	do {
-		struct page *page = bvec->bv_page;
-		start = ((u64)page->index << PAGE_CACHE_SHIFT) +
-			 bvec->bv_offset;
-		end = start + bvec->bv_len - 1;
-
-		if (bvec->bv_offset == 0 && bvec->bv_len == PAGE_CACHE_SIZE)
-			whole_page = 1;
-		else
-			whole_page = 0;
-
-		if (--bvec >= bio->bi_io_vec)
-			prefetchw(&bvec->bv_page->flags);
-
-		if (!uptodate) {
-			clear_extent_uptodate(tree, start, end, GFP_ATOMIC);
-			ClearPageUptodate(page);
-			SetPageError(page);
-		}
-		clear_extent_writeback(tree, start, end, GFP_ATOMIC);
-
-		if (whole_page)
-			end_page_writeback(page);
-		else
-			check_page_writeback(tree, page);
-		if (tree->ops && tree->ops->writepage_end_io_hook)
-			tree->ops->writepage_end_io_hook(page, start, end);
-	} while (bvec >= bio->bi_io_vec);
-
-	bio_put(bio);
-#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,23)
-	return 0;
-#endif
-}
-
-/*
- * after a readpage IO is done, we need to:
- * clear the uptodate bits on error
- * set the uptodate bits if things worked
- * set the page up to date if all extents in the tree are uptodate
- * clear the lock bit in the extent tree
- * unlock the page if there are no other extents locked for it
- *
- * Scheduling is not allowed, so the extent state tree is expected
- * to have one and only one object corresponding to this IO.
- */
-#if LINUX_VERSION_CODE > KERNEL_VERSION(2,6,23)
-static void end_bio_extent_readpage(struct bio *bio, int err)
-#else
-static int end_bio_extent_readpage(struct bio *bio,
-				   unsigned int bytes_done, int err)
-#endif
-{
-	int uptodate = test_bit(BIO_UPTODATE, &bio->bi_flags);
-	struct bio_vec *bvec = bio->bi_io_vec + bio->bi_vcnt - 1;
-	struct extent_map_tree *tree = bio->bi_private;
-	u64 start;
-	u64 end;
-	int whole_page;
-	int ret;
-
-#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,23)
-	if (bio->bi_size)
-		return 1;
-#endif
-
-	do {
-		struct page *page = bvec->bv_page;
-		start = ((u64)page->index << PAGE_CACHE_SHIFT) +
-			bvec->bv_offset;
-		end = start + bvec->bv_len - 1;
-
-		if (bvec->bv_offset == 0 && bvec->bv_len == PAGE_CACHE_SIZE)
-			whole_page = 1;
-		else
-			whole_page = 0;
-
-		if (--bvec >= bio->bi_io_vec)
-			prefetchw(&bvec->bv_page->flags);
-
-		if (uptodate && tree->ops && tree->ops->readpage_end_io_hook) {
-			ret = tree->ops->readpage_end_io_hook(page, start, end);
-			if (ret)
-				uptodate = 0;
-		}
-		if (uptodate) {
-			set_extent_uptodate(tree, start, end, GFP_ATOMIC);
-			if (whole_page)
-				SetPageUptodate(page);
-			else
-				check_page_uptodate(tree, page);
-		} else {
-			ClearPageUptodate(page);
-			SetPageError(page);
-		}
-
-		unlock_extent(tree, start, end, GFP_ATOMIC);
-
-		if (whole_page)
-			unlock_page(page);
-		else
-			check_page_locked(tree, page);
-	} while (bvec >= bio->bi_io_vec);
-
-	bio_put(bio);
-#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,23)
-	return 0;
-#endif
-}
-
-/*
- * IO done from prepare_write is pretty simple, we just unlock
- * the structs in the extent tree when done, and set the uptodate bits
- * as appropriate.
- */
-#if LINUX_VERSION_CODE > KERNEL_VERSION(2,6,23)
-static void end_bio_extent_preparewrite(struct bio *bio, int err)
-#else
-static int end_bio_extent_preparewrite(struct bio *bio,
-				       unsigned int bytes_done, int err)
-#endif
-{
-	const int uptodate = test_bit(BIO_UPTODATE, &bio->bi_flags);
-	struct bio_vec *bvec = bio->bi_io_vec + bio->bi_vcnt - 1;
-	struct extent_map_tree *tree = bio->bi_private;
-	u64 start;
-	u64 end;
-
-#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,23)
-	if (bio->bi_size)
-		return 1;
-#endif
-
-	do {
-		struct page *page = bvec->bv_page;
-		start = ((u64)page->index << PAGE_CACHE_SHIFT) +
-			bvec->bv_offset;
-		end = start + bvec->bv_len - 1;
-
-		if (--bvec >= bio->bi_io_vec)
-			prefetchw(&bvec->bv_page->flags);
-
-		if (uptodate) {
-			set_extent_uptodate(tree, start, end, GFP_ATOMIC);
-		} else {
-			ClearPageUptodate(page);
-			SetPageError(page);
-		}
-
-		unlock_extent(tree, start, end, GFP_ATOMIC);
-
-	} while (bvec >= bio->bi_io_vec);
-
-	bio_put(bio);
-#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,23)
-	return 0;
-#endif
-}
-
-static struct bio *
-extent_bio_alloc(struct block_device *bdev, u64 first_sector, int nr_vecs,
-		 gfp_t gfp_flags)
-{
-	struct bio *bio;
-
-	bio = bio_alloc(gfp_flags, nr_vecs);
-
-	if (bio == NULL && (current->flags & PF_MEMALLOC)) {
-		while (!bio && (nr_vecs /= 2))
-			bio = bio_alloc(gfp_flags, nr_vecs);
-	}
-
-	if (bio) {
-		bio->bi_bdev = bdev;
-		bio->bi_sector = first_sector;
-	}
-	return bio;
-}
-
-static int submit_one_bio(int rw, struct bio *bio)
-{
-	u64 maxsector;
-	int ret = 0;
-
-	bio_get(bio);
-
-        maxsector = bio->bi_bdev->bd_inode->i_size >> 9;
-	if (maxsector < bio->bi_sector) {
-		printk("sector too large max %Lu got %llu\n", maxsector,
-			(unsigned long long)bio->bi_sector);
-		WARN_ON(1);
-	}
-
-	submit_bio(rw, bio);
-	if (bio_flagged(bio, BIO_EOPNOTSUPP))
-		ret = -EOPNOTSUPP;
-	bio_put(bio);
-	return ret;
-}
-
-static int submit_extent_page(int rw, struct extent_map_tree *tree,
-			      struct page *page, sector_t sector,
-			      size_t size, unsigned long offset,
-			      struct block_device *bdev,
-			      struct bio **bio_ret,
-			      unsigned long max_pages,
-			      bio_end_io_t end_io_func)
-{
-	int ret = 0;
-	struct bio *bio;
-	int nr;
-
-	if (bio_ret && *bio_ret) {
-		bio = *bio_ret;
-		if (bio->bi_sector + (bio->bi_size >> 9) != sector ||
-		    bio_add_page(bio, page, size, offset) < size) {
-			ret = submit_one_bio(rw, bio);
-			bio = NULL;
-		} else {
-			return 0;
-		}
-	}
-	nr = min_t(int, max_pages, bio_get_nr_vecs(bdev));
-	bio = extent_bio_alloc(bdev, sector, nr, GFP_NOFS | __GFP_HIGH);
-	if (!bio) {
-		printk("failed to allocate bio nr %d\n", nr);
-	}
-	bio_add_page(bio, page, size, offset);
-	bio->bi_end_io = end_io_func;
-	bio->bi_private = tree;
-	if (bio_ret) {
-		*bio_ret = bio;
-	} else {
-		ret = submit_one_bio(rw, bio);
-	}
-
-	return ret;
-}
-
-void set_page_extent_mapped(struct page *page)
-{
-	if (!PagePrivate(page)) {
-		SetPagePrivate(page);
-		WARN_ON(!page->mapping->a_ops->invalidatepage);
-		set_page_private(page, EXTENT_PAGE_PRIVATE);
-		page_cache_get(page);
-	}
-}
-
-void set_page_extent_head(struct page *page, unsigned long len)
-{
-	set_page_private(page, EXTENT_PAGE_PRIVATE_FIRST_PAGE | len << 2);
-}
-
-/*
- * basic readpage implementation.  Locked extent state structs are inserted
- * into the tree that are removed when the IO is done (by the end_io
- * handlers)
- */
-static int __extent_read_full_page(struct extent_map_tree *tree,
-				   struct page *page,
-				   get_extent_t *get_extent,
-				   struct bio **bio)
-{
-	struct inode *inode = page->mapping->host;
-	u64 start = (u64)page->index << PAGE_CACHE_SHIFT;
-	u64 page_end = start + PAGE_CACHE_SIZE - 1;
-	u64 end;
-	u64 cur = start;
-	u64 extent_offset;
-	u64 last_byte = i_size_read(inode);
-	u64 block_start;
-	u64 cur_end;
-	sector_t sector;
-	struct extent_map *em;
-	struct block_device *bdev;
-	int ret;
-	int nr = 0;
-	size_t page_offset = 0;
-	size_t iosize;
-	size_t blocksize = inode->i_sb->s_blocksize;
-
-	set_page_extent_mapped(page);
-
-	end = page_end;
-	lock_extent(tree, start, end, GFP_NOFS);
-
-	while (cur <= end) {
-		if (cur >= last_byte) {
-			char *userpage;
-			iosize = PAGE_CACHE_SIZE - page_offset;
-			userpage = kmap_atomic(page, KM_USER0);
-			memset(userpage + page_offset, 0, iosize);
-			flush_dcache_page(page);
-			kunmap_atomic(userpage, KM_USER0);
-			set_extent_uptodate(tree, cur, cur + iosize - 1,
-					    GFP_NOFS);
-			unlock_extent(tree, cur, cur + iosize - 1, GFP_NOFS);
-			break;
-		}
-		em = get_extent(inode, page, page_offset, cur, end, 0);
-		if (IS_ERR(em) || !em) {
-			SetPageError(page);
-			unlock_extent(tree, cur, end, GFP_NOFS);
-			break;
-		}
-
-		extent_offset = cur - em->start;
-		BUG_ON(em->end < cur);
-		BUG_ON(end < cur);
-
-		iosize = min(em->end - cur, end - cur) + 1;
-		cur_end = min(em->end, end);
-		iosize = (iosize + blocksize - 1) & ~((u64)blocksize - 1);
-		sector = (em->block_start + extent_offset) >> 9;
-		bdev = em->bdev;
-		block_start = em->block_start;
-		free_extent_map(em);
-		em = NULL;
-
-		/* we've found a hole, just zero and go on */
-		if (block_start == EXTENT_MAP_HOLE) {
-			char *userpage;
-			userpage = kmap_atomic(page, KM_USER0);
-			memset(userpage + page_offset, 0, iosize);
-			flush_dcache_page(page);
-			kunmap_atomic(userpage, KM_USER0);
-
-			set_extent_uptodate(tree, cur, cur + iosize - 1,
-					    GFP_NOFS);
-			unlock_extent(tree, cur, cur + iosize - 1, GFP_NOFS);
-			cur = cur + iosize;
-			page_offset += iosize;
-			continue;
-		}
-		/* the get_extent function already copied into the page */
-		if (test_range_bit(tree, cur, cur_end, EXTENT_UPTODATE, 1)) {
-			unlock_extent(tree, cur, cur + iosize - 1, GFP_NOFS);
-			cur = cur + iosize;
-			page_offset += iosize;
-			continue;
-		}
-
-		ret = 0;
-		if (tree->ops && tree->ops->readpage_io_hook) {
-			ret = tree->ops->readpage_io_hook(page, cur,
-							  cur + iosize - 1);
-		}
-		if (!ret) {
-			unsigned long nr = (last_byte >> PAGE_CACHE_SHIFT) + 1;
-			nr -= page->index;
-			ret = submit_extent_page(READ, tree, page,
-					 sector, iosize, page_offset,
-					 bdev, bio, nr,
-					 end_bio_extent_readpage);
-		}
-		if (ret)
-			SetPageError(page);
-		cur = cur + iosize;
-		page_offset += iosize;
-		nr++;
-	}
-	if (!nr) {
-		if (!PageError(page))
-			SetPageUptodate(page);
-		unlock_page(page);
-	}
-	return 0;
-}
-
-int extent_read_full_page(struct extent_map_tree *tree, struct page *page,
-			    get_extent_t *get_extent)
-{
-	struct bio *bio = NULL;
-	int ret;
-
-	ret = __extent_read_full_page(tree, page, get_extent, &bio);
-	if (bio)
-		submit_one_bio(READ, bio);
-	return ret;
-}
-EXPORT_SYMBOL(extent_read_full_page);
-
-/*
- * the writepage semantics are similar to regular writepage.  extent
- * records are inserted to lock ranges in the tree, and as dirty areas
- * are found, they are marked writeback.  Then the lock bits are removed
- * and the end_io handler clears the writeback ranges
- */
-static int __extent_writepage(struct page *page, struct writeback_control *wbc,
-			      void *data)
-{
-	struct inode *inode = page->mapping->host;
-	struct extent_page_data *epd = data;
-	struct extent_map_tree *tree = epd->tree;
-	u64 start = (u64)page->index << PAGE_CACHE_SHIFT;
-	u64 delalloc_start;
-	u64 page_end = start + PAGE_CACHE_SIZE - 1;
-	u64 end;
-	u64 cur = start;
-	u64 extent_offset;
-	u64 last_byte = i_size_read(inode);
-	u64 block_start;
-	u64 iosize;
-	sector_t sector;
-	struct extent_map *em;
-	struct block_device *bdev;
-	int ret;
-	int nr = 0;
-	size_t page_offset = 0;
-	size_t blocksize;
-	loff_t i_size = i_size_read(inode);
-	unsigned long end_index = i_size >> PAGE_CACHE_SHIFT;
-	u64 nr_delalloc;
-	u64 delalloc_end;
-
-	WARN_ON(!PageLocked(page));
-	if (page->index > end_index) {
-		clear_extent_dirty(tree, start, page_end, GFP_NOFS);
-		unlock_page(page);
-		return 0;
-	}
-
-	if (page->index == end_index) {
-		char *userpage;
-
-		size_t offset = i_size & (PAGE_CACHE_SIZE - 1);
-
-		userpage = kmap_atomic(page, KM_USER0);
-		memset(userpage + offset, 0, PAGE_CACHE_SIZE - offset);
-		flush_dcache_page(page);
-		kunmap_atomic(userpage, KM_USER0);
-	}
-
-	set_page_extent_mapped(page);
-
-	delalloc_start = start;
-	delalloc_end = 0;
-	while(delalloc_end < page_end) {
-		nr_delalloc = find_lock_delalloc_range(tree, &delalloc_start,
-						       &delalloc_end,
-						       128 * 1024 * 1024);
-		if (nr_delalloc == 0) {
-			delalloc_start = delalloc_end + 1;
-			continue;
-		}
-		tree->ops->fill_delalloc(inode, delalloc_start,
-					 delalloc_end);
-		clear_extent_bit(tree, delalloc_start,
-				 delalloc_end,
-				 EXTENT_LOCKED | EXTENT_DELALLOC,
-				 1, 0, GFP_NOFS);
-		delalloc_start = delalloc_end + 1;
-	}
-	lock_extent(tree, start, page_end, GFP_NOFS);
-
-	end = page_end;
-	if (test_range_bit(tree, start, page_end, EXTENT_DELALLOC, 0)) {
-		printk("found delalloc bits after lock_extent\n");
-	}
-
-	if (last_byte <= start) {
-		clear_extent_dirty(tree, start, page_end, GFP_NOFS);
-		goto done;
-	}
-
-	set_extent_uptodate(tree, start, page_end, GFP_NOFS);
-	blocksize = inode->i_sb->s_blocksize;
-
-	while (cur <= end) {
-		if (cur >= last_byte) {
-			clear_extent_dirty(tree, cur, page_end, GFP_NOFS);
-			break;
-		}
-		em = epd->get_extent(inode, page, page_offset, cur, end, 1);
-		if (IS_ERR(em) || !em) {
-			SetPageError(page);
-			break;
-		}
-
-		extent_offset = cur - em->start;
-		BUG_ON(em->end < cur);
-		BUG_ON(end < cur);
-		iosize = min(em->end - cur, end - cur) + 1;
-		iosize = (iosize + blocksize - 1) & ~((u64)blocksize - 1);
-		sector = (em->block_start + extent_offset) >> 9;
-		bdev = em->bdev;
-		block_start = em->block_start;
-		free_extent_map(em);
-		em = NULL;
-
-		if (block_start == EXTENT_MAP_HOLE ||
-		    block_start == EXTENT_MAP_INLINE) {
-			clear_extent_dirty(tree, cur,
-					   cur + iosize - 1, GFP_NOFS);
-			cur = cur + iosize;
-			page_offset += iosize;
-			continue;
-		}
-
-		/* leave this out until we have a page_mkwrite call */
-		if (0 && !test_range_bit(tree, cur, cur + iosize - 1,
-				   EXTENT_DIRTY, 0)) {
-			cur = cur + iosize;
-			page_offset += iosize;
-			continue;
-		}
-		clear_extent_dirty(tree, cur, cur + iosize - 1, GFP_NOFS);
-		if (tree->ops && tree->ops->writepage_io_hook) {
-			ret = tree->ops->writepage_io_hook(page, cur,
-						cur + iosize - 1);
-		} else {
-			ret = 0;
-		}
-		if (ret)
-			SetPageError(page);
-		else {
-			unsigned long max_nr = end_index + 1;
-			set_range_writeback(tree, cur, cur + iosize - 1);
-			if (!PageWriteback(page)) {
-				printk("warning page %lu not writeback, "
-				       "cur %llu end %llu\n", page->index,
-				       (unsigned long long)cur,
-				       (unsigned long long)end);
-			}
-
-			ret = submit_extent_page(WRITE, tree, page, sector,
-						 iosize, page_offset, bdev,
-						 &epd->bio, max_nr,
-						 end_bio_extent_writepage);
-			if (ret)
-				SetPageError(page);
-		}
-		cur = cur + iosize;
-		page_offset += iosize;
-		nr++;
-	}
-done:
-	if (nr == 0) {
-		/* make sure the mapping tag for page dirty gets cleared */
-		set_page_writeback(page);
-		end_page_writeback(page);
-	}
-	unlock_extent(tree, start, page_end, GFP_NOFS);
-	unlock_page(page);
-	return 0;
-}
-
-#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,18)
-
-/* Taken directly from 2.6.23 for 2.6.18 back port */
-typedef int (*writepage_t)(struct page *page, struct writeback_control *wbc,
-                                void *data);
-
-/**
- * write_cache_pages - walk the list of dirty pages of the given address space
- * and write all of them.
- * @mapping: address space structure to write
- * @wbc: subtract the number of written pages from *@wbc->nr_to_write
- * @writepage: function called for each page
- * @data: data passed to writepage function
- *
- * If a page is already under I/O, write_cache_pages() skips it, even
- * if it's dirty.  This is desirable behaviour for memory-cleaning writeback,
- * but it is INCORRECT for data-integrity system calls such as fsync().  fsync()
- * and msync() need to guarantee that all the data which was dirty at the time
- * the call was made get new I/O started against them.  If wbc->sync_mode is
- * WB_SYNC_ALL then we were called for data integrity and we must wait for
- * existing IO to complete.
- */
-static int write_cache_pages(struct address_space *mapping,
-		      struct writeback_control *wbc, writepage_t writepage,
-		      void *data)
-{
-	struct backing_dev_info *bdi = mapping->backing_dev_info;
-	int ret = 0;
-	int done = 0;
-	struct pagevec pvec;
-	int nr_pages;
-	pgoff_t index;
-	pgoff_t end;		/* Inclusive */
-	int scanned = 0;
-	int range_whole = 0;
-
-	if (wbc->nonblocking && bdi_write_congested(bdi)) {
-		wbc->encountered_congestion = 1;
-		return 0;
-	}
-
-	pagevec_init(&pvec, 0);
-	if (wbc->range_cyclic) {
-		index = mapping->writeback_index; /* Start from prev offset */
-		end = -1;
-	} else {
-		index = wbc->range_start >> PAGE_CACHE_SHIFT;
-		end = wbc->range_end >> PAGE_CACHE_SHIFT;
-		if (wbc->range_start == 0 && wbc->range_end == LLONG_MAX)
-			range_whole = 1;
-		scanned = 1;
-	}
-retry:
-	while (!done && (index <= end) &&
-	       (nr_pages = pagevec_lookup_tag(&pvec, mapping, &index,
-					      PAGECACHE_TAG_DIRTY,
-					      min(end - index, (pgoff_t)PAGEVEC_SIZE-1) + 1))) {
-		unsigned i;
-
-		scanned = 1;
-		for (i = 0; i < nr_pages; i++) {
-			struct page *page = pvec.pages[i];
-
-			/*
-			 * At this point we hold neither mapping->tree_lock nor
-			 * lock on the page itself: the page may be truncated or
-			 * invalidated (changing page->mapping to NULL), or even
-			 * swizzled back from swapper_space to tmpfs file
-			 * mapping
-			 */
-			lock_page(page);
-
-			if (unlikely(page->mapping != mapping)) {
-				unlock_page(page);
-				continue;
-			}
-
-			if (!wbc->range_cyclic && page->index > end) {
-				done = 1;
-				unlock_page(page);
-				continue;
-			}
-
-			if (wbc->sync_mode != WB_SYNC_NONE)
-				wait_on_page_writeback(page);
-
-			if (PageWriteback(page) ||
-			    !clear_page_dirty_for_io(page)) {
-				unlock_page(page);
-				continue;
-			}
-
-			ret = (*writepage)(page, wbc, data);
-
-			if (unlikely(ret == AOP_WRITEPAGE_ACTIVATE)) {
-				unlock_page(page);
-				ret = 0;
-			}
-			if (ret || (--(wbc->nr_to_write) <= 0))
-				done = 1;
-			if (wbc->nonblocking && bdi_write_congested(bdi)) {
-				wbc->encountered_congestion = 1;
-				done = 1;
-			}
-		}
-		pagevec_release(&pvec);
-		cond_resched();
-	}
-	if (!scanned && !done) {
-		/*
-		 * We hit the last page and there is more work to be done: wrap
-		 * back to the start of the file
-		 */
-		scanned = 1;
-		index = 0;
-		goto retry;
-	}
-	if (wbc->range_cyclic || (range_whole && wbc->nr_to_write > 0))
-		mapping->writeback_index = index;
-	return ret;
-}
-#endif
-
-int extent_write_full_page(struct extent_map_tree *tree, struct page *page,
-			  get_extent_t *get_extent,
-			  struct writeback_control *wbc)
-{
-	int ret;
-	struct address_space *mapping = page->mapping;
-	struct extent_page_data epd = {
-		.bio = NULL,
-		.tree = tree,
-		.get_extent = get_extent,
-	};
-	struct writeback_control wbc_writepages = {
-		.bdi		= wbc->bdi,
-		.sync_mode	= WB_SYNC_NONE,
-		.older_than_this = NULL,
-		.nr_to_write	= 64,
-		.range_start	= page_offset(page) + PAGE_CACHE_SIZE,
-		.range_end	= (loff_t)-1,
-	};
-
-
-	ret = __extent_writepage(page, wbc, &epd);
-
-	write_cache_pages(mapping, &wbc_writepages, __extent_writepage, &epd);
-	if (epd.bio) {
-		submit_one_bio(WRITE, epd.bio);
-	}
-	return ret;
-}
-EXPORT_SYMBOL(extent_write_full_page);
-
-
-int extent_writepages(struct extent_map_tree *tree,
-		      struct address_space *mapping,
-		      get_extent_t *get_extent,
-		      struct writeback_control *wbc)
-{
-	int ret = 0;
-	struct extent_page_data epd = {
-		.bio = NULL,
-		.tree = tree,
-		.get_extent = get_extent,
-	};
-
-	ret = write_cache_pages(mapping, wbc, __extent_writepage, &epd);
-	if (epd.bio) {
-		submit_one_bio(WRITE, epd.bio);
-	}
-	return ret;
-}
-EXPORT_SYMBOL(extent_writepages);
-
-int extent_readpages(struct extent_map_tree *tree,
-		     struct address_space *mapping,
-		     struct list_head *pages, unsigned nr_pages,
-		     get_extent_t get_extent)
-{
-	struct bio *bio = NULL;
-	unsigned page_idx;
-	struct pagevec pvec;
-
-	pagevec_init(&pvec, 0);
-	for (page_idx = 0; page_idx < nr_pages; page_idx++) {
-		struct page *page = list_entry(pages->prev, struct page, lru);
-
-		prefetchw(&page->flags);
-		list_del(&page->lru);
-		/*
-		 * what we want to do here is call add_to_page_cache_lru,
-		 * but that isn't exported, so we reproduce it here
-		 */
-		if (!add_to_page_cache(page, mapping,
-					page->index, GFP_KERNEL)) {
-
-			/* open coding of lru_cache_add, also not exported */
-			page_cache_get(page);
-			if (!pagevec_add(&pvec, page))
-				__pagevec_lru_add(&pvec);
-			__extent_read_full_page(tree, page, get_extent, &bio);
-		}
-		page_cache_release(page);
-	}
-	if (pagevec_count(&pvec))
-		__pagevec_lru_add(&pvec);
-	BUG_ON(!list_empty(pages));
-	if (bio)
-		submit_one_bio(READ, bio);
-	return 0;
-}
-EXPORT_SYMBOL(extent_readpages);
-
-/*
- * basic invalidatepage code, this waits on any locked or writeback
- * ranges corresponding to the page, and then deletes any extent state
- * records from the tree
- */
-int extent_invalidatepage(struct extent_map_tree *tree,
-			  struct page *page, unsigned long offset)
-{
-	u64 start = ((u64)page->index << PAGE_CACHE_SHIFT);
-	u64 end = start + PAGE_CACHE_SIZE - 1;
-	size_t blocksize = page->mapping->host->i_sb->s_blocksize;
-
-	start += (offset + blocksize -1) & ~(blocksize - 1);
-	if (start > end)
-		return 0;
-
-	lock_extent(tree, start, end, GFP_NOFS);
-	wait_on_extent_writeback(tree, start, end);
-	clear_extent_bit(tree, start, end,
-			 EXTENT_LOCKED | EXTENT_DIRTY | EXTENT_DELALLOC,
-			 1, 1, GFP_NOFS);
-	return 0;
-}
-EXPORT_SYMBOL(extent_invalidatepage);
-
-/*
- * simple commit_write call, set_range_dirty is used to mark both
- * the pages and the extent records as dirty
- */
-int extent_commit_write(struct extent_map_tree *tree,
-			struct inode *inode, struct page *page,
-			unsigned from, unsigned to)
-{
-	loff_t pos = ((loff_t)page->index << PAGE_CACHE_SHIFT) + to;
-
-	set_page_extent_mapped(page);
-	set_page_dirty(page);
-
-	if (pos > inode->i_size) {
-		i_size_write(inode, pos);
-		mark_inode_dirty(inode);
-	}
-	return 0;
-}
-EXPORT_SYMBOL(extent_commit_write);
-
-int extent_prepare_write(struct extent_map_tree *tree,
-			 struct inode *inode, struct page *page,
-			 unsigned from, unsigned to, get_extent_t *get_extent)
-{
-	u64 page_start = (u64)page->index << PAGE_CACHE_SHIFT;
-	u64 page_end = page_start + PAGE_CACHE_SIZE - 1;
-	u64 block_start;
-	u64 orig_block_start;
-	u64 block_end;
-	u64 cur_end;
-	struct extent_map *em;
-	unsigned blocksize = 1 << inode->i_blkbits;
-	size_t page_offset = 0;
-	size_t block_off_start;
-	size_t block_off_end;
-	int err = 0;
-	int iocount = 0;
-	int ret = 0;
-	int isnew;
-
-	set_page_extent_mapped(page);
-
-	block_start = (page_start + from) & ~((u64)blocksize - 1);
-	block_end = (page_start + to - 1) | (blocksize - 1);
-	orig_block_start = block_start;
-
-	lock_extent(tree, page_start, page_end, GFP_NOFS);
-	while(block_start <= block_end) {
-		em = get_extent(inode, page, page_offset, block_start,
-				block_end, 1);
-		if (IS_ERR(em) || !em) {
-			goto err;
-		}
-		cur_end = min(block_end, em->end);
-		block_off_start = block_start & (PAGE_CACHE_SIZE - 1);
-		block_off_end = block_off_start + blocksize;
-		isnew = clear_extent_new(tree, block_start, cur_end, GFP_NOFS);
-
-		if (!PageUptodate(page) && isnew &&
-		    (block_off_end > to || block_off_start < from)) {
-			void *kaddr;
-
-			kaddr = kmap_atomic(page, KM_USER0);
-			if (block_off_end > to)
-				memset(kaddr + to, 0, block_off_end - to);
-			if (block_off_start < from)
-				memset(kaddr + block_off_start, 0,
-				       from - block_off_start);
-			flush_dcache_page(page);
-			kunmap_atomic(kaddr, KM_USER0);
-		}
-		if ((em->block_start != EXTENT_MAP_HOLE &&
-		     em->block_start != EXTENT_MAP_INLINE) &&
-		    !isnew && !PageUptodate(page) &&
-		    (block_off_end > to || block_off_start < from) &&
-		    !test_range_bit(tree, block_start, cur_end,
-				    EXTENT_UPTODATE, 1)) {
-			u64 sector;
-			u64 extent_offset = block_start - em->start;
-			size_t iosize;
-			sector = (em->block_start + extent_offset) >> 9;
-			iosize = (cur_end - block_start + blocksize) &
-				~((u64)blocksize - 1);
-			/*
-			 * we've already got the extent locked, but we
-			 * need to split the state such that our end_bio
-			 * handler can clear the lock.
-			 */
-			set_extent_bit(tree, block_start,
-				       block_start + iosize - 1,
-				       EXTENT_LOCKED, 0, NULL, GFP_NOFS);
-			ret = submit_extent_page(READ, tree, page,
-					 sector, iosize, page_offset, em->bdev,
-					 NULL, 1,
-					 end_bio_extent_preparewrite);
-			iocount++;
-			block_start = block_start + iosize;
-		} else {
-			set_extent_uptodate(tree, block_start, cur_end,
-					    GFP_NOFS);
-			unlock_extent(tree, block_start, cur_end, GFP_NOFS);
-			block_start = cur_end + 1;
-		}
-		page_offset = block_start & (PAGE_CACHE_SIZE - 1);
-		free_extent_map(em);
-	}
-	if (iocount) {
-		wait_extent_bit(tree, orig_block_start,
-				block_end, EXTENT_LOCKED);
-	}
-	check_page_uptodate(tree, page);
-err:
-	/* FIXME, zero out newly allocated blocks on error */
-	return err;
-}
-EXPORT_SYMBOL(extent_prepare_write);
-
-/*
- * a helper for releasepage.  As long as there are no locked extents
- * in the range corresponding to the page, both state records and extent
- * map records are removed
- */
-int try_release_extent_mapping(struct extent_map_tree *tree, struct page *page)
-{
-	struct extent_map *em;
-	u64 start = (u64)page->index << PAGE_CACHE_SHIFT;
-	u64 end = start + PAGE_CACHE_SIZE - 1;
-	u64 orig_start = start;
-	int ret = 1;
-
-	while (start <= end) {
-		em = lookup_extent_mapping(tree, start, end);
-		if (!em || IS_ERR(em))
-			break;
-		if (!test_range_bit(tree, em->start, em->end,
-				    EXTENT_LOCKED, 0)) {
-			remove_extent_mapping(tree, em);
-			/* once for the rb tree */
-			free_extent_map(em);
-		}
-		start = em->end + 1;
-		/* once for us */
-		free_extent_map(em);
-	}
-	if (test_range_bit(tree, orig_start, end, EXTENT_LOCKED, 0))
-		ret = 0;
-	else
-		clear_extent_bit(tree, orig_start, end, EXTENT_UPTODATE,
-				 1, 1, GFP_NOFS);
-	return ret;
-}
-EXPORT_SYMBOL(try_release_extent_mapping);
-
-sector_t extent_bmap(struct address_space *mapping, sector_t iblock,
-		get_extent_t *get_extent)
-{
-	struct inode *inode = mapping->host;
-	u64 start = iblock << inode->i_blkbits;
-	u64 end = start + (1 << inode->i_blkbits) - 1;
-	sector_t sector = 0;
-	struct extent_map *em;
-
-	em = get_extent(inode, NULL, 0, start, end, 0);
-	if (!em || IS_ERR(em))
-		return 0;
-
-	if (em->block_start == EXTENT_MAP_INLINE ||
-	    em->block_start == EXTENT_MAP_HOLE)
-		goto out;
-
-	sector = (em->block_start + start - em->start) >> inode->i_blkbits;
-out:
-	free_extent_map(em);
-	return sector;
-}
-
-static int add_lru(struct extent_map_tree *tree, struct extent_buffer *eb)
-{
-	if (list_empty(&eb->lru)) {
-		extent_buffer_get(eb);
-		list_add(&eb->lru, &tree->buffer_lru);
-		tree->lru_size++;
-		if (tree->lru_size >= BUFFER_LRU_MAX) {
-			struct extent_buffer *rm;
-			rm = list_entry(tree->buffer_lru.prev,
-					struct extent_buffer, lru);
-			tree->lru_size--;
-			list_del_init(&rm->lru);
-			free_extent_buffer(rm);
-		}
-	} else
-		list_move(&eb->lru, &tree->buffer_lru);
-	return 0;
-}
-static struct extent_buffer *find_lru(struct extent_map_tree *tree,
-				      u64 start, unsigned long len)
-{
-	struct list_head *lru = &tree->buffer_lru;
-	struct list_head *cur = lru->next;
-	struct extent_buffer *eb;
-
-	if (list_empty(lru))
-		return NULL;
-
-	do {
-		eb = list_entry(cur, struct extent_buffer, lru);
-		if (eb->start == start && eb->len == len) {
-			extent_buffer_get(eb);
-			return eb;
-		}
-		cur = cur->next;
-	} while (cur != lru);
-	return NULL;
-}
-
-static inline unsigned long num_extent_pages(u64 start, u64 len)
-{
-	return ((start + len + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT) -
-		(start >> PAGE_CACHE_SHIFT);
-}
-
-static inline struct page *extent_buffer_page(struct extent_buffer *eb,
-					      unsigned long i)
-{
-	struct page *p;
-	struct address_space *mapping;
-
-	if (i == 0)
-		return eb->first_page;
-	i += eb->start >> PAGE_CACHE_SHIFT;
-	mapping = eb->first_page->mapping;
-	read_lock_irq(&mapping->tree_lock);
-	p = radix_tree_lookup(&mapping->page_tree, i);
-	read_unlock_irq(&mapping->tree_lock);
-	return p;
-}
-
-static struct extent_buffer *__alloc_extent_buffer(struct extent_map_tree *tree,
-						   u64 start,
-						   unsigned long len,
-						   gfp_t mask)
-{
-	struct extent_buffer *eb = NULL;
-
-	spin_lock(&tree->lru_lock);
-	eb = find_lru(tree, start, len);
-	spin_unlock(&tree->lru_lock);
-	if (eb) {
-		return eb;
-	}
-
-	eb = kmem_cache_zalloc(extent_buffer_cache, mask);
-	INIT_LIST_HEAD(&eb->lru);
-	eb->start = start;
-	eb->len = len;
-	atomic_set(&eb->refs, 1);
-
-	return eb;
-}
-
-static void __free_extent_buffer(struct extent_buffer *eb)
-{
-	kmem_cache_free(extent_buffer_cache, eb);
-}
-
-struct extent_buffer *alloc_extent_buffer(struct extent_map_tree *tree,
-					  u64 start, unsigned long len,
-					  struct page *page0,
-					  gfp_t mask)
-{
-	unsigned long num_pages = num_extent_pages(start, len);
-	unsigned long i;
-	unsigned long index = start >> PAGE_CACHE_SHIFT;
-	struct extent_buffer *eb;
-	struct page *p;
-	struct address_space *mapping = tree->mapping;
-	int uptodate = 1;
-
-	eb = __alloc_extent_buffer(tree, start, len, mask);
-	if (!eb || IS_ERR(eb))
-		return NULL;
-
-	if (eb->flags & EXTENT_BUFFER_FILLED)
-		goto lru_add;
-
-	if (page0) {
-		eb->first_page = page0;
-		i = 1;
-		index++;
-		page_cache_get(page0);
-		mark_page_accessed(page0);
-		set_page_extent_mapped(page0);
-		WARN_ON(!PageUptodate(page0));
-		set_page_extent_head(page0, len);
-	} else {
-		i = 0;
-	}
-	for (; i < num_pages; i++, index++) {
-		p = find_or_create_page(mapping, index, mask | __GFP_HIGHMEM);
-		if (!p) {
-			WARN_ON(1);
-			goto fail;
-		}
-		set_page_extent_mapped(p);
-		mark_page_accessed(p);
-		if (i == 0) {
-			eb->first_page = p;
-			set_page_extent_head(p, len);
-		} else {
-			set_page_private(p, EXTENT_PAGE_PRIVATE);
-		}
-		if (!PageUptodate(p))
-			uptodate = 0;
-		unlock_page(p);
-	}
-	if (uptodate)
-		eb->flags |= EXTENT_UPTODATE;
-	eb->flags |= EXTENT_BUFFER_FILLED;
-
-lru_add:
-	spin_lock(&tree->lru_lock);
-	add_lru(tree, eb);
-	spin_unlock(&tree->lru_lock);
-	return eb;
-
-fail:
-	spin_lock(&tree->lru_lock);
-	list_del_init(&eb->lru);
-	spin_unlock(&tree->lru_lock);
-	if (!atomic_dec_and_test(&eb->refs))
-		return NULL;
-	for (index = 1; index < i; index++) {
-		page_cache_release(extent_buffer_page(eb, index));
-	}
-	if (i > 0)
-		page_cache_release(extent_buffer_page(eb, 0));
-	__free_extent_buffer(eb);
-	return NULL;
-}
-EXPORT_SYMBOL(alloc_extent_buffer);
-
-struct extent_buffer *find_extent_buffer(struct extent_map_tree *tree,
-					 u64 start, unsigned long len,
-					  gfp_t mask)
-{
-	unsigned long num_pages = num_extent_pages(start, len);
-	unsigned long i;
-	unsigned long index = start >> PAGE_CACHE_SHIFT;
-	struct extent_buffer *eb;
-	struct page *p;
-	struct address_space *mapping = tree->mapping;
-	int uptodate = 1;
-
-	eb = __alloc_extent_buffer(tree, start, len, mask);
-	if (!eb || IS_ERR(eb))
-		return NULL;
-
-	if (eb->flags & EXTENT_BUFFER_FILLED)
-		goto lru_add;
-
-	for (i = 0; i < num_pages; i++, index++) {
-		p = find_lock_page(mapping, index);
-		if (!p) {
-			goto fail;
-		}
-		set_page_extent_mapped(p);
-		mark_page_accessed(p);
-
-		if (i == 0) {
-			eb->first_page = p;
-			set_page_extent_head(p, len);
-		} else {
-			set_page_private(p, EXTENT_PAGE_PRIVATE);
-		}
-
-		if (!PageUptodate(p))
-			uptodate = 0;
-		unlock_page(p);
-	}
-	if (uptodate)
-		eb->flags |= EXTENT_UPTODATE;
-	eb->flags |= EXTENT_BUFFER_FILLED;
-
-lru_add:
-	spin_lock(&tree->lru_lock);
-	add_lru(tree, eb);
-	spin_unlock(&tree->lru_lock);
-	return eb;
-fail:
-	spin_lock(&tree->lru_lock);
-	list_del_init(&eb->lru);
-	spin_unlock(&tree->lru_lock);
-	if (!atomic_dec_and_test(&eb->refs))
-		return NULL;
-	for (index = 1; index < i; index++) {
-		page_cache_release(extent_buffer_page(eb, index));
-	}
-	if (i > 0)
-		page_cache_release(extent_buffer_page(eb, 0));
-	__free_extent_buffer(eb);
-	return NULL;
-}
-EXPORT_SYMBOL(find_extent_buffer);
-
-void free_extent_buffer(struct extent_buffer *eb)
-{
-	unsigned long i;
-	unsigned long num_pages;
-
-	if (!eb)
-		return;
-
-	if (!atomic_dec_and_test(&eb->refs))
-		return;
-
-	WARN_ON(!list_empty(&eb->lru));
-	num_pages = num_extent_pages(eb->start, eb->len);
-
-	for (i = 1; i < num_pages; i++) {
-		page_cache_release(extent_buffer_page(eb, i));
-	}
-	page_cache_release(extent_buffer_page(eb, 0));
-	__free_extent_buffer(eb);
-}
-EXPORT_SYMBOL(free_extent_buffer);
-
-int clear_extent_buffer_dirty(struct extent_map_tree *tree,
-			      struct extent_buffer *eb)
-{
-	int set;
-	unsigned long i;
-	unsigned long num_pages;
-	struct page *page;
-
-	u64 start = eb->start;
-	u64 end = start + eb->len - 1;
-
-	set = clear_extent_dirty(tree, start, end, GFP_NOFS);
-	num_pages = num_extent_pages(eb->start, eb->len);
-
-	for (i = 0; i < num_pages; i++) {
-		page = extent_buffer_page(eb, i);
-		lock_page(page);
-		if (i == 0)
-			set_page_extent_head(page, eb->len);
-		else
-			set_page_private(page, EXTENT_PAGE_PRIVATE);
-
-		/*
-		 * if we're on the last page or the first page and the
-		 * block isn't aligned on a page boundary, do extra checks
-		 * to make sure we don't clean page that is partially dirty
-		 */
-		if ((i == 0 && (eb->start & (PAGE_CACHE_SIZE - 1))) ||
-		    ((i == num_pages - 1) &&
-		     ((eb->start + eb->len) & (PAGE_CACHE_SIZE - 1)))) {
-			start = (u64)page->index << PAGE_CACHE_SHIFT;
-			end  = start + PAGE_CACHE_SIZE - 1;
-			if (test_range_bit(tree, start, end,
-					   EXTENT_DIRTY, 0)) {
-				unlock_page(page);
-				continue;
-			}
-		}
-		clear_page_dirty_for_io(page);
-		write_lock_irq(&page->mapping->tree_lock);
-		if (!PageDirty(page)) {
-			radix_tree_tag_clear(&page->mapping->page_tree,
-						page_index(page),
-						PAGECACHE_TAG_DIRTY);
-		}
-		write_unlock_irq(&page->mapping->tree_lock);
-		unlock_page(page);
-	}
-	return 0;
-}
-EXPORT_SYMBOL(clear_extent_buffer_dirty);
-
-int wait_on_extent_buffer_writeback(struct extent_map_tree *tree,
-				    struct extent_buffer *eb)
-{
-	return wait_on_extent_writeback(tree, eb->start,
-					eb->start + eb->len - 1);
-}
-EXPORT_SYMBOL(wait_on_extent_buffer_writeback);
-
-int set_extent_buffer_dirty(struct extent_map_tree *tree,
-			     struct extent_buffer *eb)
-{
-	unsigned long i;
-	unsigned long num_pages;
-
-	num_pages = num_extent_pages(eb->start, eb->len);
-	for (i = 0; i < num_pages; i++) {
-		struct page *page = extent_buffer_page(eb, i);
-		/* writepage may need to do something special for the
-		 * first page, we have to make sure page->private is
-		 * properly set.  releasepage may drop page->private
-		 * on us if the page isn't already dirty.
-		 */
-		if (i == 0) {
-			lock_page(page);
-			set_page_extent_head(page, eb->len);
-		} else if (PagePrivate(page) &&
-			   page->private != EXTENT_PAGE_PRIVATE) {
-			lock_page(page);
-			set_page_extent_mapped(page);
-			unlock_page(page);
-		}
-		__set_page_dirty_nobuffers(extent_buffer_page(eb, i));
-		if (i == 0)
-			unlock_page(page);
-	}
-	return set_extent_dirty(tree, eb->start,
-				eb->start + eb->len - 1, GFP_NOFS);
-}
-EXPORT_SYMBOL(set_extent_buffer_dirty);
-
-int set_extent_buffer_uptodate(struct extent_map_tree *tree,
-				struct extent_buffer *eb)
-{
-	unsigned long i;
-	struct page *page;
-	unsigned long num_pages;
-
-	num_pages = num_extent_pages(eb->start, eb->len);
-
-	set_extent_uptodate(tree, eb->start, eb->start + eb->len - 1,
-			    GFP_NOFS);
-	for (i = 0; i < num_pages; i++) {
-		page = extent_buffer_page(eb, i);
-		if ((i == 0 && (eb->start & (PAGE_CACHE_SIZE - 1))) ||
-		    ((i == num_pages - 1) &&
-		     ((eb->start + eb->len) & (PAGE_CACHE_SIZE - 1)))) {
-			check_page_uptodate(tree, page);
-			continue;
-		}
-		SetPageUptodate(page);
-	}
-	return 0;
-}
-EXPORT_SYMBOL(set_extent_buffer_uptodate);
-
-int extent_buffer_uptodate(struct extent_map_tree *tree,
-			     struct extent_buffer *eb)
-{
-	if (eb->flags & EXTENT_UPTODATE)
-		return 1;
-	return test_range_bit(tree, eb->start, eb->start + eb->len - 1,
-			   EXTENT_UPTODATE, 1);
-}
-EXPORT_SYMBOL(extent_buffer_uptodate);
-
-int read_extent_buffer_pages(struct extent_map_tree *tree,
-			     struct extent_buffer *eb,
-			     u64 start,
-			     int wait)
-{
-	unsigned long i;
-	unsigned long start_i;
-	struct page *page;
-	int err;
-	int ret = 0;
-	unsigned long num_pages;
-
-	if (eb->flags & EXTENT_UPTODATE)
-		return 0;
-
-	if (0 && test_range_bit(tree, eb->start, eb->start + eb->len - 1,
-			   EXTENT_UPTODATE, 1)) {
-		return 0;
-	}
-
-	if (start) {
-		WARN_ON(start < eb->start);
-		start_i = (start >> PAGE_CACHE_SHIFT) -
-			(eb->start >> PAGE_CACHE_SHIFT);
-	} else {
-		start_i = 0;
-	}
-
-	num_pages = num_extent_pages(eb->start, eb->len);
-	for (i = start_i; i < num_pages; i++) {
-		page = extent_buffer_page(eb, i);
-		if (PageUptodate(page)) {
-			continue;
-		}
-		if (!wait) {
-			if (TestSetPageLocked(page)) {
-				continue;
-			}
-		} else {
-			lock_page(page);
-		}
-		if (!PageUptodate(page)) {
-			err = page->mapping->a_ops->readpage(NULL, page);
-			if (err) {
-				ret = err;
-			}
-		} else {
-			unlock_page(page);
-		}
-	}
-
-	if (ret || !wait) {
-		return ret;
-	}
-
-	for (i = start_i; i < num_pages; i++) {
-		page = extent_buffer_page(eb, i);
-		wait_on_page_locked(page);
-		if (!PageUptodate(page)) {
-			ret = -EIO;
-		}
-	}
-	if (!ret)
-		eb->flags |= EXTENT_UPTODATE;
-	return ret;
-}
-EXPORT_SYMBOL(read_extent_buffer_pages);
-
-void read_extent_buffer(struct extent_buffer *eb, void *dstv,
-			unsigned long start,
-			unsigned long len)
-{
-	size_t cur;
-	size_t offset;
-	struct page *page;
-	char *kaddr;
-	char *dst = (char *)dstv;
-	size_t start_offset = eb->start & ((u64)PAGE_CACHE_SIZE - 1);
-	unsigned long i = (start_offset + start) >> PAGE_CACHE_SHIFT;
-	unsigned long num_pages = num_extent_pages(eb->start, eb->len);
-
-	WARN_ON(start > eb->len);
-	WARN_ON(start + len > eb->start + eb->len);
-
-	offset = (start_offset + start) & ((unsigned long)PAGE_CACHE_SIZE - 1);
-
-	while(len > 0) {
-		page = extent_buffer_page(eb, i);
-		if (!PageUptodate(page)) {
-			printk("page %lu not up to date i %lu, total %lu, len %lu\n", page->index, i, num_pages, eb->len);
-			WARN_ON(1);
-		}
-		WARN_ON(!PageUptodate(page));
-
-		cur = min(len, (PAGE_CACHE_SIZE - offset));
-		kaddr = kmap_atomic(page, KM_USER1);
-		memcpy(dst, kaddr + offset, cur);
-		kunmap_atomic(kaddr, KM_USER1);
-
-		dst += cur;
-		len -= cur;
-		offset = 0;
-		i++;
-	}
-}
-EXPORT_SYMBOL(read_extent_buffer);
-
-int map_private_extent_buffer(struct extent_buffer *eb, unsigned long start,
-			       unsigned long min_len, char **token, char **map,
-			       unsigned long *map_start,
-			       unsigned long *map_len, int km)
-{
-	size_t offset = start & (PAGE_CACHE_SIZE - 1);
-	char *kaddr;
-	struct page *p;
-	size_t start_offset = eb->start & ((u64)PAGE_CACHE_SIZE - 1);
-	unsigned long i = (start_offset + start) >> PAGE_CACHE_SHIFT;
-	unsigned long end_i = (start_offset + start + min_len - 1) >>
-		PAGE_CACHE_SHIFT;
-
-	if (i != end_i)
-		return -EINVAL;
-
-	if (i == 0) {
-		offset = start_offset;
-		*map_start = 0;
-	} else {
-		offset = 0;
-		*map_start = ((u64)i << PAGE_CACHE_SHIFT) - start_offset;
-	}
-	if (start + min_len > eb->len) {
-printk("bad mapping eb start %Lu len %lu, wanted %lu %lu\n", eb->start, eb->len, start, min_len);
-		WARN_ON(1);
-	}
-
-	p = extent_buffer_page(eb, i);
-	WARN_ON(!PageUptodate(p));
-	kaddr = kmap_atomic(p, km);
-	*token = kaddr;
-	*map = kaddr + offset;
-	*map_len = PAGE_CACHE_SIZE - offset;
-	return 0;
-}
-EXPORT_SYMBOL(map_private_extent_buffer);
-
-int map_extent_buffer(struct extent_buffer *eb, unsigned long start,
-		      unsigned long min_len,
-		      char **token, char **map,
-		      unsigned long *map_start,
-		      unsigned long *map_len, int km)
-{
-	int err;
-	int save = 0;
-	if (eb->map_token) {
-		unmap_extent_buffer(eb, eb->map_token, km);
-		eb->map_token = NULL;
-		save = 1;
-	}
-	err = map_private_extent_buffer(eb, start, min_len, token, map,
-				       map_start, map_len, km);
-	if (!err && save) {
-		eb->map_token = *token;
-		eb->kaddr = *map;
-		eb->map_start = *map_start;
-		eb->map_len = *map_len;
-	}
-	return err;
-}
-EXPORT_SYMBOL(map_extent_buffer);
-
-void unmap_extent_buffer(struct extent_buffer *eb, char *token, int km)
-{
-	kunmap_atomic(token, km);
-}
-EXPORT_SYMBOL(unmap_extent_buffer);
-
-int memcmp_extent_buffer(struct extent_buffer *eb, const void *ptrv,
-			  unsigned long start,
-			  unsigned long len)
-{
-	size_t cur;
-	size_t offset;
-	struct page *page;
-	char *kaddr;
-	char *ptr = (char *)ptrv;
-	size_t start_offset = eb->start & ((u64)PAGE_CACHE_SIZE - 1);
-	unsigned long i = (start_offset + start) >> PAGE_CACHE_SHIFT;
-	int ret = 0;
-
-	WARN_ON(start > eb->len);
-	WARN_ON(start + len > eb->start + eb->len);
-
-	offset = (start_offset + start) & ((unsigned long)PAGE_CACHE_SIZE - 1);
-
-	while(len > 0) {
-		page = extent_buffer_page(eb, i);
-		WARN_ON(!PageUptodate(page));
-
-		cur = min(len, (PAGE_CACHE_SIZE - offset));
-
-		kaddr = kmap_atomic(page, KM_USER0);
-		ret = memcmp(ptr, kaddr + offset, cur);
-		kunmap_atomic(kaddr, KM_USER0);
-		if (ret)
-			break;
-
-		ptr += cur;
-		len -= cur;
-		offset = 0;
-		i++;
-	}
-	return ret;
-}
-EXPORT_SYMBOL(memcmp_extent_buffer);
-
-void write_extent_buffer(struct extent_buffer *eb, const void *srcv,
-			 unsigned long start, unsigned long len)
-{
-	size_t cur;
-	size_t offset;
-	struct page *page;
-	char *kaddr;
-	char *src = (char *)srcv;
-	size_t start_offset = eb->start & ((u64)PAGE_CACHE_SIZE - 1);
-	unsigned long i = (start_offset + start) >> PAGE_CACHE_SHIFT;
-
-	WARN_ON(start > eb->len);
-	WARN_ON(start + len > eb->start + eb->len);
-
-	offset = (start_offset + start) & ((unsigned long)PAGE_CACHE_SIZE - 1);
-
-	while(len > 0) {
-		page = extent_buffer_page(eb, i);
-		WARN_ON(!PageUptodate(page));
-
-		cur = min(len, PAGE_CACHE_SIZE - offset);
-		kaddr = kmap_atomic(page, KM_USER1);
-		memcpy(kaddr + offset, src, cur);
-		kunmap_atomic(kaddr, KM_USER1);
-
-		src += cur;
-		len -= cur;
-		offset = 0;
-		i++;
-	}
-}
-EXPORT_SYMBOL(write_extent_buffer);
-
-void memset_extent_buffer(struct extent_buffer *eb, char c,
-			  unsigned long start, unsigned long len)
-{
-	size_t cur;
-	size_t offset;
-	struct page *page;
-	char *kaddr;
-	size_t start_offset = eb->start & ((u64)PAGE_CACHE_SIZE - 1);
-	unsigned long i = (start_offset + start) >> PAGE_CACHE_SHIFT;
-
-	WARN_ON(start > eb->len);
-	WARN_ON(start + len > eb->start + eb->len);
-
-	offset = (start_offset + start) & ((unsigned long)PAGE_CACHE_SIZE - 1);
-
-	while(len > 0) {
-		page = extent_buffer_page(eb, i);
-		WARN_ON(!PageUptodate(page));
-
-		cur = min(len, PAGE_CACHE_SIZE - offset);
-		kaddr = kmap_atomic(page, KM_USER0);
-		memset(kaddr + offset, c, cur);
-		kunmap_atomic(kaddr, KM_USER0);
-
-		len -= cur;
-		offset = 0;
-		i++;
-	}
-}
-EXPORT_SYMBOL(memset_extent_buffer);
-
-void copy_extent_buffer(struct extent_buffer *dst, struct extent_buffer *src,
-			unsigned long dst_offset, unsigned long src_offset,
-			unsigned long len)
-{
-	u64 dst_len = dst->len;
-	size_t cur;
-	size_t offset;
-	struct page *page;
-	char *kaddr;
-	size_t start_offset = dst->start & ((u64)PAGE_CACHE_SIZE - 1);
-	unsigned long i = (start_offset + dst_offset) >> PAGE_CACHE_SHIFT;
-
-	WARN_ON(src->len != dst_len);
-
-	offset = (start_offset + dst_offset) &
-		((unsigned long)PAGE_CACHE_SIZE - 1);
-
-	while(len > 0) {
-		page = extent_buffer_page(dst, i);
-		WARN_ON(!PageUptodate(page));
-
-		cur = min(len, (unsigned long)(PAGE_CACHE_SIZE - offset));
-
-		kaddr = kmap_atomic(page, KM_USER0);
-		read_extent_buffer(src, kaddr + offset, src_offset, cur);
-		kunmap_atomic(kaddr, KM_USER0);
-
-		src_offset += cur;
-		len -= cur;
-		offset = 0;
-		i++;
-	}
-}
-EXPORT_SYMBOL(copy_extent_buffer);
-
-static void move_pages(struct page *dst_page, struct page *src_page,
-		       unsigned long dst_off, unsigned long src_off,
-		       unsigned long len)
-{
-	char *dst_kaddr = kmap_atomic(dst_page, KM_USER0);
-	if (dst_page == src_page) {
-		memmove(dst_kaddr + dst_off, dst_kaddr + src_off, len);
-	} else {
-		char *src_kaddr = kmap_atomic(src_page, KM_USER1);
-		char *p = dst_kaddr + dst_off + len;
-		char *s = src_kaddr + src_off + len;
-
-		while (len--)
-			*--p = *--s;
-
-		kunmap_atomic(src_kaddr, KM_USER1);
-	}
-	kunmap_atomic(dst_kaddr, KM_USER0);
-}
-
-static void copy_pages(struct page *dst_page, struct page *src_page,
-		       unsigned long dst_off, unsigned long src_off,
-		       unsigned long len)
-{
-	char *dst_kaddr = kmap_atomic(dst_page, KM_USER0);
-	char *src_kaddr;
-
-	if (dst_page != src_page)
-		src_kaddr = kmap_atomic(src_page, KM_USER1);
-	else
-		src_kaddr = dst_kaddr;
-
-	memcpy(dst_kaddr + dst_off, src_kaddr + src_off, len);
-	kunmap_atomic(dst_kaddr, KM_USER0);
-	if (dst_page != src_page)
-		kunmap_atomic(src_kaddr, KM_USER1);
-}
-
-void memcpy_extent_buffer(struct extent_buffer *dst, unsigned long dst_offset,
-			   unsigned long src_offset, unsigned long len)
-{
-	size_t cur;
-	size_t dst_off_in_page;
-	size_t src_off_in_page;
-	size_t start_offset = dst->start & ((u64)PAGE_CACHE_SIZE - 1);
-	unsigned long dst_i;
-	unsigned long src_i;
-
-	if (src_offset + len > dst->len) {
-		printk("memmove bogus src_offset %lu move len %lu len %lu\n",
-		       src_offset, len, dst->len);
-		BUG_ON(1);
-	}
-	if (dst_offset + len > dst->len) {
-		printk("memmove bogus dst_offset %lu move len %lu len %lu\n",
-		       dst_offset, len, dst->len);
-		BUG_ON(1);
-	}
-
-	while(len > 0) {
-		dst_off_in_page = (start_offset + dst_offset) &
-			((unsigned long)PAGE_CACHE_SIZE - 1);
-		src_off_in_page = (start_offset + src_offset) &
-			((unsigned long)PAGE_CACHE_SIZE - 1);
-
-		dst_i = (start_offset + dst_offset) >> PAGE_CACHE_SHIFT;
-		src_i = (start_offset + src_offset) >> PAGE_CACHE_SHIFT;
-
-		cur = min(len, (unsigned long)(PAGE_CACHE_SIZE -
-					       src_off_in_page));
-		cur = min_t(unsigned long, cur,
-			(unsigned long)(PAGE_CACHE_SIZE - dst_off_in_page));
-
-		copy_pages(extent_buffer_page(dst, dst_i),
-			   extent_buffer_page(dst, src_i),
-			   dst_off_in_page, src_off_in_page, cur);
-
-		src_offset += cur;
-		dst_offset += cur;
-		len -= cur;
-	}
-}
-EXPORT_SYMBOL(memcpy_extent_buffer);
-
-void memmove_extent_buffer(struct extent_buffer *dst, unsigned long dst_offset,
-			   unsigned long src_offset, unsigned long len)
-{
-	size_t cur;
-	size_t dst_off_in_page;
-	size_t src_off_in_page;
-	unsigned long dst_end = dst_offset + len - 1;
-	unsigned long src_end = src_offset + len - 1;
-	size_t start_offset = dst->start & ((u64)PAGE_CACHE_SIZE - 1);
-	unsigned long dst_i;
-	unsigned long src_i;
-
-	if (src_offset + len > dst->len) {
-		printk("memmove bogus src_offset %lu move len %lu len %lu\n",
-		       src_offset, len, dst->len);
-		BUG_ON(1);
-	}
-	if (dst_offset + len > dst->len) {
-		printk("memmove bogus dst_offset %lu move len %lu len %lu\n",
-		       dst_offset, len, dst->len);
-		BUG_ON(1);
-	}
-	if (dst_offset < src_offset) {
-		memcpy_extent_buffer(dst, dst_offset, src_offset, len);
-		return;
-	}
-	while(len > 0) {
-		dst_i = (start_offset + dst_end) >> PAGE_CACHE_SHIFT;
-		src_i = (start_offset + src_end) >> PAGE_CACHE_SHIFT;
-
-		dst_off_in_page = (start_offset + dst_end) &
-			((unsigned long)PAGE_CACHE_SIZE - 1);
-		src_off_in_page = (start_offset + src_end) &
-			((unsigned long)PAGE_CACHE_SIZE - 1);
-
-		cur = min_t(unsigned long, len, src_off_in_page + 1);
-		cur = min(cur, dst_off_in_page + 1);
-		move_pages(extent_buffer_page(dst, dst_i),
-			   extent_buffer_page(dst, src_i),
-			   dst_off_in_page - cur + 1,
-			   src_off_in_page - cur + 1, cur);
-
-		dst_end -= cur;
-		src_end -= cur;
-		len -= cur;
-	}
-}
-EXPORT_SYMBOL(memmove_extent_buffer);

commit 5f56406aabdf5444d040c5955effc665b1d0dbaf
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Jan 22 16:47:59 2008 -0500

    Btrfs: Fix hole insertion corner cases
    
    There were a few places that could cause duplicate extent insertion,
    this adjusts the code that creates holes to avoid it.
    
    lookup_extent_map is changed to correctly return all of the extents in a
    range, even when there are none matching at the start of the range.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 485cf0719b3c..010a287fbd71 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -204,10 +204,12 @@ static struct rb_node *tree_insert(struct rb_root *root, u64 offset,
 }
 
 static struct rb_node *__tree_search(struct rb_root *root, u64 offset,
-				   struct rb_node **prev_ret)
+				     struct rb_node **prev_ret,
+				     struct rb_node **next_ret)
 {
 	struct rb_node * n = root->rb_node;
 	struct rb_node *prev = NULL;
+	struct rb_node *orig_prev = NULL;
 	struct tree_entry *entry;
 	struct tree_entry *prev_entry = NULL;
 
@@ -223,13 +225,25 @@ static struct rb_node *__tree_search(struct rb_root *root, u64 offset,
 		else
 			return n;
 	}
-	if (!prev_ret)
-		return NULL;
-	while(prev && offset > prev_entry->end) {
-		prev = rb_next(prev);
+
+	if (prev_ret) {
+		orig_prev = prev;
+		while(prev && offset > prev_entry->end) {
+			prev = rb_next(prev);
+			prev_entry = rb_entry(prev, struct tree_entry, rb_node);
+		}
+		*prev_ret = prev;
+		prev = orig_prev;
+	}
+
+	if (next_ret) {
 		prev_entry = rb_entry(prev, struct tree_entry, rb_node);
+		while(prev && offset < prev_entry->start) {
+			prev = rb_prev(prev);
+			prev_entry = rb_entry(prev, struct tree_entry, rb_node);
+		}
+		*next_ret = prev;
 	}
-	*prev_ret = prev;
 	return NULL;
 }
 
@@ -237,7 +251,7 @@ static inline struct rb_node *tree_search(struct rb_root *root, u64 offset)
 {
 	struct rb_node *prev;
 	struct rb_node *ret;
-	ret = __tree_search(root, offset, &prev);
+	ret = __tree_search(root, offset, &prev, NULL);
 	if (!ret)
 		return prev;
 	return ret;
@@ -248,7 +262,7 @@ static int tree_delete(struct rb_root *root, u64 offset)
 	struct rb_node *node;
 	struct tree_entry *entry;
 
-	node = __tree_search(root, offset, NULL);
+	node = __tree_search(root, offset, NULL, NULL);
 	if (!node)
 		return -ENOENT;
 	entry = rb_entry(node, struct tree_entry, rb_node);
@@ -314,9 +328,21 @@ struct extent_map *lookup_extent_mapping(struct extent_map_tree *tree,
 {
 	struct extent_map *em;
 	struct rb_node *rb_node;
+	struct rb_node *prev = NULL;
+	struct rb_node *next = NULL;
 
 	read_lock_irq(&tree->lock);
-	rb_node = tree_search(&tree->map, start);
+	rb_node = __tree_search(&tree->map, start, &prev, &next);
+	if (!rb_node && prev) {
+		em = rb_entry(prev, struct extent_map, rb_node);
+		if (em->start <= end && em->end >= start)
+			goto found;
+	}
+	if (!rb_node && next) {
+		em = rb_entry(next, struct extent_map, rb_node);
+		if (em->start <= end && em->end >= start)
+			goto found;
+	}
 	if (!rb_node) {
 		em = NULL;
 		goto out;
@@ -330,6 +356,7 @@ struct extent_map *lookup_extent_mapping(struct extent_map_tree *tree,
 		em = NULL;
 		goto out;
 	}
+found:
 	atomic_inc(&em->refs);
 out:
 	read_unlock_irq(&tree->lock);

commit f0c5da1446cc500856a5e31c9a0e2a7bdd30e663
Author: Yan <yanzheng@21cn.com>
Date:   Tue Jan 22 12:46:56 2008 -0500

    Btrfs: Fix for test_range_bit
    
    test_range_bit doesn't properly handle the case: there's a hole at the
    end of the range and there's no other extent_state after the range.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index d71aed341abd..485cf0719b3c 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -1336,6 +1336,11 @@ int test_range_bit(struct extent_map_tree *tree, u64 start, u64 end,
 		if (start > end)
 			break;
 		node = rb_next(node);
+		if (!node) {
+			if (filled)
+				bitset = 0;
+			break;
+		}
 	}
 	read_unlock_irq(&tree->lock);
 	return bitset;

commit b3a0d8d28c607cb2531a68742afc2b967b1f6083
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Jan 14 13:22:53 2008 -0500

    Btrfs: Remove verbose WARN_ON
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index f3a384ed700c..d71aed341abd 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -1665,8 +1665,6 @@ void set_page_extent_mapped(struct page *page)
 
 void set_page_extent_head(struct page *page, unsigned long len)
 {
-	WARN_ON(page->private && page->private == EXTENT_PAGE_PRIVATE &&
-		PageDirty(page));
 	set_page_private(page, EXTENT_PAGE_PRIVATE_FIRST_PAGE | len << 2);
 }
 

commit 55c69072d6bd5be170a85467f64a20963cddf490
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Jan 9 15:55:33 2008 -0500

    Btrfs: Fix extent_buffer usage when nodesize != leafsize
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 9d6aefa937c4..f3a384ed700c 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -1663,6 +1663,13 @@ void set_page_extent_mapped(struct page *page)
 	}
 }
 
+void set_page_extent_head(struct page *page, unsigned long len)
+{
+	WARN_ON(page->private && page->private == EXTENT_PAGE_PRIVATE &&
+		PageDirty(page));
+	set_page_private(page, EXTENT_PAGE_PRIVATE_FIRST_PAGE | len << 2);
+}
+
 /*
  * basic readpage implementation.  Locked extent state structs are inserted
  * into the tree that are removed when the IO is done (by the end_io
@@ -2490,8 +2497,7 @@ struct extent_buffer *alloc_extent_buffer(struct extent_map_tree *tree,
 		mark_page_accessed(page0);
 		set_page_extent_mapped(page0);
 		WARN_ON(!PageUptodate(page0));
-		set_page_private(page0, EXTENT_PAGE_PRIVATE_FIRST_PAGE |
-				 len << 2);
+		set_page_extent_head(page0, len);
 	} else {
 		i = 0;
 	}
@@ -2505,8 +2511,7 @@ struct extent_buffer *alloc_extent_buffer(struct extent_map_tree *tree,
 		mark_page_accessed(p);
 		if (i == 0) {
 			eb->first_page = p;
-			set_page_private(p, EXTENT_PAGE_PRIVATE_FIRST_PAGE |
-					 len << 2);
+			set_page_extent_head(p, len);
 		} else {
 			set_page_private(p, EXTENT_PAGE_PRIVATE);
 		}
@@ -2569,8 +2574,7 @@ struct extent_buffer *find_extent_buffer(struct extent_map_tree *tree,
 
 		if (i == 0) {
 			eb->first_page = p;
-			set_page_private(p, EXTENT_PAGE_PRIVATE_FIRST_PAGE |
-					 len << 2);
+			set_page_extent_head(p, len);
 		} else {
 			set_page_private(p, EXTENT_PAGE_PRIVATE);
 		}
@@ -2643,6 +2647,11 @@ int clear_extent_buffer_dirty(struct extent_map_tree *tree,
 	for (i = 0; i < num_pages; i++) {
 		page = extent_buffer_page(eb, i);
 		lock_page(page);
+		if (i == 0)
+			set_page_extent_head(page, eb->len);
+		else
+			set_page_private(page, EXTENT_PAGE_PRIVATE);
+
 		/*
 		 * if we're on the last page or the first page and the
 		 * block isn't aligned on a page boundary, do extra checks
@@ -2697,9 +2706,12 @@ int set_extent_buffer_dirty(struct extent_map_tree *tree,
 		 */
 		if (i == 0) {
 			lock_page(page);
-			set_page_private(page,
-					 EXTENT_PAGE_PRIVATE_FIRST_PAGE |
-					 eb->len << 2);
+			set_page_extent_head(page, eb->len);
+		} else if (PagePrivate(page) &&
+			   page->private != EXTENT_PAGE_PRIVATE) {
+			lock_page(page);
+			set_page_extent_mapped(page);
+			unlock_page(page);
 		}
 		__set_page_dirty_nobuffers(extent_buffer_page(eb, i));
 		if (i == 0)

commit bcd987feefe8da66bc59b4e6bd51761a9820588c
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Jan 9 06:28:28 2008 -0500

    Btrfs: Remove extent_map debugging message
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index f53d8c976219..9d6aefa937c4 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -273,7 +273,6 @@ int add_extent_mapping(struct extent_map_tree *tree,
 	rb = tree_insert(&tree->map, em->end, &em->rb_node);
 	if (rb) {
 		prev = rb_entry(rb, struct extent_map, rb_node);
-		printk("found extent map %Lu %Lu on insert of %Lu %Lu\n", prev->start, prev->end, em->start, em->end);
 		ret = -EEXIST;
 		goto out;
 	}

commit 5d4fb734b4cb89a3902d6de309af0f6f3e63fe73
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Dec 21 16:27:23 2007 -0500

    Btrfs: Fix an off by one in the extent_map prepare write code
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 2b92f1070274..f53d8c976219 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -2270,7 +2270,7 @@ int extent_prepare_write(struct extent_map_tree *tree,
 			kunmap_atomic(kaddr, KM_USER0);
 		}
 		if ((em->block_start != EXTENT_MAP_HOLE &&
-		     em->block_start != EXTENT_MAP_INLINE) && 
+		     em->block_start != EXTENT_MAP_INLINE) &&
 		    !isnew && !PageUptodate(page) &&
 		    (block_off_end > to || block_off_start < from) &&
 		    !test_range_bit(tree, block_start, cur_end,
@@ -2279,7 +2279,7 @@ int extent_prepare_write(struct extent_map_tree *tree,
 			u64 extent_offset = block_start - em->start;
 			size_t iosize;
 			sector = (em->block_start + extent_offset) >> 9;
-			iosize = (cur_end - block_start + blocksize - 1) &
+			iosize = (cur_end - block_start + blocksize) &
 				~((u64)blocksize - 1);
 			/*
 			 * we've already got the extent locked, but we

commit 1832a6d5ee3b1af61001cadba9e10da9e91af4a4
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Dec 21 16:27:21 2007 -0500

    Btrfs: Implement basic support for -ENOSPC
    
    This is intended to prevent accidentally filling the drive.  A determined
    user can still make things oops.
    
    It includes some accounting of the current bytes under delayed allocation,
    but this will change as things get optimized
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index a0dff34dd437..2b92f1070274 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -1131,7 +1131,8 @@ u64 find_lock_delalloc_range(struct extent_map_tree *tree,
 }
 
 u64 count_range_bits(struct extent_map_tree *tree,
-		     u64 *start, u64 max_bytes, unsigned long bits)
+		     u64 *start, u64 search_end, u64 max_bytes,
+		     unsigned long bits)
 {
 	struct rb_node *node;
 	struct extent_state *state;
@@ -1139,9 +1140,14 @@ u64 count_range_bits(struct extent_map_tree *tree,
 	u64 total_bytes = 0;
 	int found = 0;
 
+	if (search_end <= cur_start) {
+		printk("search_end %Lu start %Lu\n", search_end, cur_start);
+		WARN_ON(1);
+		return 0;
+	}
+
 	write_lock_irq(&tree->lock);
-	if (bits == EXTENT_DIRTY) {
-		*start = 0;
+	if (cur_start == 0 && bits == EXTENT_DIRTY) {
 		total_bytes = tree->dirty_bytes;
 		goto out;
 	}
@@ -1156,8 +1162,11 @@ u64 count_range_bits(struct extent_map_tree *tree,
 
 	while(1) {
 		state = rb_entry(node, struct extent_state, rb_node);
-		if ((state->state & bits)) {
-			total_bytes += state->end - state->start + 1;
+		if (state->start > search_end)
+			break;
+		if (state->end >= cur_start && (state->state & bits)) {
+			total_bytes += min(search_end, state->end) + 1 -
+				       max(cur_start, state->start);
 			if (total_bytes >= max_bytes)
 				break;
 			if (!found) {
@@ -1173,7 +1182,6 @@ u64 count_range_bits(struct extent_map_tree *tree,
 	write_unlock_irq(&tree->lock);
 	return total_bytes;
 }
-
 /*
  * helper function to lock both pages and extents in the tree.
  * pages must be locked first.

commit 190662b2128dd648749e197f5563e9f6bbb5e05c
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Dec 18 16:25:45 2007 -0500

    Btrfs: Fix delayed allocation to avoid missing delalloc extents
    
    find_lock_delalloc_range could exit out too early
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 0d1e59a86e49..a0dff34dd437 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -1070,6 +1070,7 @@ u64 find_lock_delalloc_range(struct extent_map_tree *tree,
 search_again:
 	node = tree_search(&tree->state, cur_start);
 	if (!node || IS_ERR(node)) {
+		*end = (u64)-1;
 		goto out;
 	}
 
@@ -1079,6 +1080,8 @@ u64 find_lock_delalloc_range(struct extent_map_tree *tree,
 			goto out;
 		}
 		if (!(state->state & EXTENT_DELALLOC)) {
+			if (!found)
+				*end = state->end;
 			goto out;
 		}
 		if (!found) {
@@ -1841,8 +1844,10 @@ static int __extent_writepage(struct page *page, struct writeback_control *wbc,
 		nr_delalloc = find_lock_delalloc_range(tree, &delalloc_start,
 						       &delalloc_end,
 						       128 * 1024 * 1024);
-		if (nr_delalloc <= 0)
-			break;
+		if (nr_delalloc == 0) {
+			delalloc_start = delalloc_end + 1;
+			continue;
+		}
 		tree->ops->fill_delalloc(inode, delalloc_start,
 					 delalloc_end);
 		clear_extent_bit(tree, delalloc_start,

commit 6da6abae027e2dbc59bca5f4168b0760f25068c7
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Dec 18 16:15:09 2007 -0500

    Btrfs: Back port to 2.6.18-el kernels
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 8326a18413ff..0d1e59a86e49 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -1585,8 +1585,18 @@ extent_bio_alloc(struct block_device *bdev, u64 first_sector, int nr_vecs,
 
 static int submit_one_bio(int rw, struct bio *bio)
 {
+	u64 maxsector;
 	int ret = 0;
+
 	bio_get(bio);
+
+        maxsector = bio->bi_bdev->bd_inode->i_size >> 9;
+	if (maxsector < bio->bi_sector) {
+		printk("sector too large max %Lu got %llu\n", maxsector,
+			(unsigned long long)bio->bi_sector);
+		WARN_ON(1);
+	}
+
 	submit_bio(rw, bio);
 	if (bio_flagged(bio, BIO_EOPNOTSUPP))
 		ret = -EOPNOTSUPP;
@@ -1678,8 +1688,12 @@ static int __extent_read_full_page(struct extent_map_tree *tree,
 
 	while (cur <= end) {
 		if (cur >= last_byte) {
+			char *userpage;
 			iosize = PAGE_CACHE_SIZE - page_offset;
-			zero_user_page(page, page_offset, iosize, KM_USER0);
+			userpage = kmap_atomic(page, KM_USER0);
+			memset(userpage + page_offset, 0, iosize);
+			flush_dcache_page(page);
+			kunmap_atomic(userpage, KM_USER0);
 			set_extent_uptodate(tree, cur, cur + iosize - 1,
 					    GFP_NOFS);
 			unlock_extent(tree, cur, cur + iosize - 1, GFP_NOFS);
@@ -1707,7 +1721,12 @@ static int __extent_read_full_page(struct extent_map_tree *tree,
 
 		/* we've found a hole, just zero and go on */
 		if (block_start == EXTENT_MAP_HOLE) {
-			zero_user_page(page, page_offset, iosize, KM_USER0);
+			char *userpage;
+			userpage = kmap_atomic(page, KM_USER0);
+			memset(userpage + page_offset, 0, iosize);
+			flush_dcache_page(page);
+			kunmap_atomic(userpage, KM_USER0);
+
 			set_extent_uptodate(tree, cur, cur + iosize - 1,
 					    GFP_NOFS);
 			unlock_extent(tree, cur, cur + iosize - 1, GFP_NOFS);
@@ -1804,9 +1823,14 @@ static int __extent_writepage(struct page *page, struct writeback_control *wbc,
 	}
 
 	if (page->index == end_index) {
+		char *userpage;
+
 		size_t offset = i_size & (PAGE_CACHE_SIZE - 1);
-		zero_user_page(page, offset,
-			       PAGE_CACHE_SIZE - offset, KM_USER0);
+
+		userpage = kmap_atomic(page, KM_USER0);
+		memset(userpage + offset, 0, PAGE_CACHE_SIZE - offset);
+		flush_dcache_page(page);
+		kunmap_atomic(userpage, KM_USER0);
 	}
 
 	set_page_extent_mapped(page);
@@ -1921,6 +1945,129 @@ static int __extent_writepage(struct page *page, struct writeback_control *wbc,
 	return 0;
 }
 
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,18)
+
+/* Taken directly from 2.6.23 for 2.6.18 back port */
+typedef int (*writepage_t)(struct page *page, struct writeback_control *wbc,
+                                void *data);
+
+/**
+ * write_cache_pages - walk the list of dirty pages of the given address space
+ * and write all of them.
+ * @mapping: address space structure to write
+ * @wbc: subtract the number of written pages from *@wbc->nr_to_write
+ * @writepage: function called for each page
+ * @data: data passed to writepage function
+ *
+ * If a page is already under I/O, write_cache_pages() skips it, even
+ * if it's dirty.  This is desirable behaviour for memory-cleaning writeback,
+ * but it is INCORRECT for data-integrity system calls such as fsync().  fsync()
+ * and msync() need to guarantee that all the data which was dirty at the time
+ * the call was made get new I/O started against them.  If wbc->sync_mode is
+ * WB_SYNC_ALL then we were called for data integrity and we must wait for
+ * existing IO to complete.
+ */
+static int write_cache_pages(struct address_space *mapping,
+		      struct writeback_control *wbc, writepage_t writepage,
+		      void *data)
+{
+	struct backing_dev_info *bdi = mapping->backing_dev_info;
+	int ret = 0;
+	int done = 0;
+	struct pagevec pvec;
+	int nr_pages;
+	pgoff_t index;
+	pgoff_t end;		/* Inclusive */
+	int scanned = 0;
+	int range_whole = 0;
+
+	if (wbc->nonblocking && bdi_write_congested(bdi)) {
+		wbc->encountered_congestion = 1;
+		return 0;
+	}
+
+	pagevec_init(&pvec, 0);
+	if (wbc->range_cyclic) {
+		index = mapping->writeback_index; /* Start from prev offset */
+		end = -1;
+	} else {
+		index = wbc->range_start >> PAGE_CACHE_SHIFT;
+		end = wbc->range_end >> PAGE_CACHE_SHIFT;
+		if (wbc->range_start == 0 && wbc->range_end == LLONG_MAX)
+			range_whole = 1;
+		scanned = 1;
+	}
+retry:
+	while (!done && (index <= end) &&
+	       (nr_pages = pagevec_lookup_tag(&pvec, mapping, &index,
+					      PAGECACHE_TAG_DIRTY,
+					      min(end - index, (pgoff_t)PAGEVEC_SIZE-1) + 1))) {
+		unsigned i;
+
+		scanned = 1;
+		for (i = 0; i < nr_pages; i++) {
+			struct page *page = pvec.pages[i];
+
+			/*
+			 * At this point we hold neither mapping->tree_lock nor
+			 * lock on the page itself: the page may be truncated or
+			 * invalidated (changing page->mapping to NULL), or even
+			 * swizzled back from swapper_space to tmpfs file
+			 * mapping
+			 */
+			lock_page(page);
+
+			if (unlikely(page->mapping != mapping)) {
+				unlock_page(page);
+				continue;
+			}
+
+			if (!wbc->range_cyclic && page->index > end) {
+				done = 1;
+				unlock_page(page);
+				continue;
+			}
+
+			if (wbc->sync_mode != WB_SYNC_NONE)
+				wait_on_page_writeback(page);
+
+			if (PageWriteback(page) ||
+			    !clear_page_dirty_for_io(page)) {
+				unlock_page(page);
+				continue;
+			}
+
+			ret = (*writepage)(page, wbc, data);
+
+			if (unlikely(ret == AOP_WRITEPAGE_ACTIVATE)) {
+				unlock_page(page);
+				ret = 0;
+			}
+			if (ret || (--(wbc->nr_to_write) <= 0))
+				done = 1;
+			if (wbc->nonblocking && bdi_write_congested(bdi)) {
+				wbc->encountered_congestion = 1;
+				done = 1;
+			}
+		}
+		pagevec_release(&pvec);
+		cond_resched();
+	}
+	if (!scanned && !done) {
+		/*
+		 * We hit the last page and there is more work to be done: wrap
+		 * back to the start of the file
+		 */
+		scanned = 1;
+		index = 0;
+		goto retry;
+	}
+	if (wbc->range_cyclic || (range_whole && wbc->nr_to_write > 0))
+		mapping->writeback_index = index;
+	return ret;
+}
+#endif
+
 int extent_write_full_page(struct extent_map_tree *tree, struct page *page,
 			  get_extent_t *get_extent,
 			  struct writeback_control *wbc)
@@ -1945,18 +2092,20 @@ int extent_write_full_page(struct extent_map_tree *tree, struct page *page,
 	ret = __extent_writepage(page, wbc, &epd);
 
 	write_cache_pages(mapping, &wbc_writepages, __extent_writepage, &epd);
-	if (epd.bio)
+	if (epd.bio) {
 		submit_one_bio(WRITE, epd.bio);
+	}
 	return ret;
 }
 EXPORT_SYMBOL(extent_write_full_page);
 
+
 int extent_writepages(struct extent_map_tree *tree,
 		      struct address_space *mapping,
 		      get_extent_t *get_extent,
 		      struct writeback_control *wbc)
 {
-	int ret;
+	int ret = 0;
 	struct extent_page_data epd = {
 		.bio = NULL,
 		.tree = tree,
@@ -1964,8 +2113,9 @@ int extent_writepages(struct extent_map_tree *tree,
 	};
 
 	ret = write_cache_pages(mapping, wbc, __extent_writepage, &epd);
-	if (epd.bio)
+	if (epd.bio) {
 		submit_one_bio(WRITE, epd.bio);
+	}
 	return ret;
 }
 EXPORT_SYMBOL(extent_writepages);
@@ -2106,7 +2256,9 @@ int extent_prepare_write(struct extent_map_tree *tree,
 			flush_dcache_page(page);
 			kunmap_atomic(kaddr, KM_USER0);
 		}
-		if (!isnew && !PageUptodate(page) &&
+		if ((em->block_start != EXTENT_MAP_HOLE &&
+		     em->block_start != EXTENT_MAP_INLINE) && 
+		    !isnew && !PageUptodate(page) &&
 		    (block_off_end > to || block_off_start < from) &&
 		    !test_range_bit(tree, block_start, cur_end,
 				    EXTENT_UPTODATE, 1)) {

commit 17636e03f42a1a42fed3834859de4702bd655fd1
Author: Christian Hesse <list@eworm.de>
Date:   Tue Dec 11 09:25:06 2007 -0500

    Btrfs: section mismatch warnings
    
    --Boundary-00=_CcOWHFYK4T+JwSj
    Content-Type: text/plain;
      charset="iso-8859-1"
    Content-Transfer-Encoding: 7bit
    Content-Disposition: inline
    
    Hello everybody,
    
    compiling btrfs into the kernel results in section mismatch warnings. __exit
    functions are called where they are not allowed to. The attached patch fixes
    this for me. Not sure if it is correct though.
    
    Signed-off-by: Christian Hesse <mail@earthworm.de>
    --
    Regards,
    Chris
    
    --Boundary-00=_CcOWHFYK4T+JwSj
    Content-Type: text/x-diff; charset="iso-8859-1";
            name="btrfs-section_mismatches.patch"
    Content-Transfer-Encoding: 7bit
    Content-Disposition: attachment;
            filename="btrfs-section_mismatches.patch"
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 06e437723dc3..8326a18413ff 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -69,7 +69,7 @@ int __init extent_map_init(void)
 	return -ENOMEM;
 }
 
-void __exit extent_map_exit(void)
+void extent_map_exit(void)
 {
 	struct extent_state *state;
 

commit ca6646264b7dab662d84435441164bb2a8e8885a
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Nov 27 11:16:35 2007 -0500

    Btrfs: Add efficient dirty accounting to the extent_map tree
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index b6a4974ecc23..06e437723dc3 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -42,6 +42,7 @@ struct extent_page_data {
 	struct extent_map_tree *tree;
 	get_extent_t *get_extent;
 };
+
 int __init extent_map_init(void)
 {
 	extent_map_cache = btrfs_cache_create("extent_map",
@@ -94,6 +95,7 @@ void extent_map_tree_init(struct extent_map_tree *tree,
 	tree->map.rb_node = NULL;
 	tree->state.rb_node = NULL;
 	tree->ops = NULL;
+	tree->dirty_bytes = 0;
 	rwlock_init(&tree->lock);
 	spin_lock_init(&tree->lru_lock);
 	tree->mapping = mapping;
@@ -414,6 +416,8 @@ static int insert_state(struct extent_map_tree *tree,
 		printk("end < start %Lu %Lu\n", end, start);
 		WARN_ON(1);
 	}
+	if (bits & EXTENT_DIRTY)
+		tree->dirty_bytes += end - start + 1;
 	state->state |= bits;
 	state->start = start;
 	state->end = end;
@@ -476,6 +480,12 @@ static int clear_state_bit(struct extent_map_tree *tree,
 			    int delete)
 {
 	int ret = state->state & bits;
+
+	if ((bits & EXTENT_DIRTY) && (state->state & EXTENT_DIRTY)) {
+		u64 range = state->end - state->start + 1;
+		WARN_ON(range > tree->dirty_bytes);
+		tree->dirty_bytes -= range;
+	}
 	state->state &= ~bits;
 	if (wake)
 		wake_up(&state->wq);
@@ -668,6 +678,17 @@ int wait_extent_bit(struct extent_map_tree *tree, u64 start, u64 end, int bits)
 }
 EXPORT_SYMBOL(wait_extent_bit);
 
+static void set_state_bits(struct extent_map_tree *tree,
+			   struct extent_state *state,
+			   int bits)
+{
+	if ((bits & EXTENT_DIRTY) && !(state->state & EXTENT_DIRTY)) {
+		u64 range = state->end - state->start + 1;
+		tree->dirty_bytes += range;
+	}
+	state->state |= bits;
+}
+
 /*
  * set some bits on a range in the tree.  This may require allocations
  * or sleeping, so the gfp mask is used to indicate what is allowed.
@@ -727,7 +748,7 @@ int set_extent_bit(struct extent_map_tree *tree, u64 start, u64 end, int bits,
 			err = -EEXIST;
 			goto out;
 		}
-		state->state |= bits;
+		set_state_bits(tree, state, bits);
 		start = state->end + 1;
 		merge_state(tree, state);
 		goto search_again;
@@ -762,7 +783,7 @@ int set_extent_bit(struct extent_map_tree *tree, u64 start, u64 end, int bits,
 		if (err)
 			goto out;
 		if (state->end <= end) {
-			state->state |= bits;
+			set_state_bits(tree, state, bits);
 			start = state->end + 1;
 			merge_state(tree, state);
 		} else {
@@ -808,7 +829,7 @@ int set_extent_bit(struct extent_map_tree *tree, u64 start, u64 end, int bits,
 		err = split_state(tree, state, prealloc, end + 1);
 		BUG_ON(err == -EEXIST);
 
-		prealloc->state |= bits;
+		set_state_bits(tree, prealloc, bits);
 		merge_state(tree, prealloc);
 		prealloc = NULL;
 		goto out;
@@ -1116,6 +1137,11 @@ u64 count_range_bits(struct extent_map_tree *tree,
 	int found = 0;
 
 	write_lock_irq(&tree->lock);
+	if (bits == EXTENT_DIRTY) {
+		*start = 0;
+		total_bytes = tree->dirty_bytes;
+		goto out;
+	}
 	/*
 	 * this search will find all the extents that end after
 	 * our range starts.

commit 793955bca66c99defdffc857ae6eb7e8431d6bbe
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Nov 26 16:34:41 2007 -0800

    Btrfs: Limit btree writeback to prevent seeks
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 55f272c335c6..b6a4974ecc23 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -1106,6 +1106,45 @@ u64 find_lock_delalloc_range(struct extent_map_tree *tree,
 	return found;
 }
 
+u64 count_range_bits(struct extent_map_tree *tree,
+		     u64 *start, u64 max_bytes, unsigned long bits)
+{
+	struct rb_node *node;
+	struct extent_state *state;
+	u64 cur_start = *start;
+	u64 total_bytes = 0;
+	int found = 0;
+
+	write_lock_irq(&tree->lock);
+	/*
+	 * this search will find all the extents that end after
+	 * our range starts.
+	 */
+	node = tree_search(&tree->state, cur_start);
+	if (!node || IS_ERR(node)) {
+		goto out;
+	}
+
+	while(1) {
+		state = rb_entry(node, struct extent_state, rb_node);
+		if ((state->state & bits)) {
+			total_bytes += state->end - state->start + 1;
+			if (total_bytes >= max_bytes)
+				break;
+			if (!found) {
+				*start = state->start;
+				found = 1;
+			}
+		}
+		node = rb_next(node);
+		if (!node)
+			break;
+	}
+out:
+	write_unlock_irq(&tree->lock);
+	return total_bytes;
+}
+
 /*
  * helper function to lock both pages and extents in the tree.
  * pages must be locked first.

commit 015a739c7c238768fbfa4eea8ea2ebc1a35e7bb1
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Nov 26 16:15:16 2007 -0800

    Btrfs: Handle writeback under high memory pressure better
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index a4e9096754fc..55f272c335c6 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -1861,13 +1861,25 @@ int extent_write_full_page(struct extent_map_tree *tree, struct page *page,
 			  struct writeback_control *wbc)
 {
 	int ret;
+	struct address_space *mapping = page->mapping;
 	struct extent_page_data epd = {
 		.bio = NULL,
 		.tree = tree,
 		.get_extent = get_extent,
 	};
+	struct writeback_control wbc_writepages = {
+		.bdi		= wbc->bdi,
+		.sync_mode	= WB_SYNC_NONE,
+		.older_than_this = NULL,
+		.nr_to_write	= 64,
+		.range_start	= page_offset(page) + PAGE_CACHE_SIZE,
+		.range_end	= (loff_t)-1,
+	};
+
 
 	ret = __extent_writepage(page, wbc, &epd);
+
+	write_cache_pages(mapping, &wbc_writepages, __extent_writepage, &epd);
 	if (epd.bio)
 		submit_one_bio(WRITE, epd.bio);
 	return ret;

commit 7073c8e852946274e4d50fdf072438612f9dc845
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Nov 20 13:44:45 2007 -0500

    Btrfs: Make sure page mapping dirty tag is properly cleared
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 7fd4eb7a8f03..a4e9096754fc 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -1825,12 +1825,18 @@ static int __extent_writepage(struct page *page, struct writeback_control *wbc,
 		if (ret)
 			SetPageError(page);
 		else {
-			unsigned long nr = end_index + 1;
+			unsigned long max_nr = end_index + 1;
 			set_range_writeback(tree, cur, cur + iosize - 1);
+			if (!PageWriteback(page)) {
+				printk("warning page %lu not writeback, "
+				       "cur %llu end %llu\n", page->index,
+				       (unsigned long long)cur,
+				       (unsigned long long)end);
+			}
 
 			ret = submit_extent_page(WRITE, tree, page, sector,
 						 iosize, page_offset, bdev,
-						 &epd->bio, nr,
+						 &epd->bio, max_nr,
 						 end_bio_extent_writepage);
 			if (ret)
 				SetPageError(page);
@@ -1840,6 +1846,11 @@ static int __extent_writepage(struct page *page, struct writeback_control *wbc,
 		nr++;
 	}
 done:
+	if (nr == 0) {
+		/* make sure the mapping tag for page dirty gets cleared */
+		set_page_writeback(page);
+		end_page_writeback(page);
+	}
 	unlock_extent(tree, start, page_end, GFP_NOFS);
 	unlock_page(page);
 	return 0;
@@ -2408,6 +2419,13 @@ int clear_extent_buffer_dirty(struct extent_map_tree *tree,
 			}
 		}
 		clear_page_dirty_for_io(page);
+		write_lock_irq(&page->mapping->tree_lock);
+		if (!PageDirty(page)) {
+			radix_tree_tag_clear(&page->mapping->page_tree,
+						page_index(page),
+						PAGECACHE_TAG_DIRTY);
+		}
+		write_unlock_irq(&page->mapping->tree_lock);
 		unlock_page(page);
 	}
 	return 0;

commit 3e9fd94ff0028a044d55690eb0a801fd1472e3c6
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Nov 20 10:47:25 2007 -0500

    Btrfs: Avoid fragmentation from parallel delalloc filling
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index f91f28efdb59..7fd4eb7a8f03 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -1033,11 +1033,11 @@ int find_first_extent_bit(struct extent_map_tree *tree, u64 start,
 EXPORT_SYMBOL(find_first_extent_bit);
 
 u64 find_lock_delalloc_range(struct extent_map_tree *tree,
-			     u64 start, u64 lock_start, u64 *end, u64 max_bytes)
+			     u64 *start, u64 *end, u64 max_bytes)
 {
 	struct rb_node *node;
 	struct extent_state *state;
-	u64 cur_start = start;
+	u64 cur_start = *start;
 	u64 found = 0;
 	u64 total_bytes = 0;
 
@@ -1054,27 +1054,43 @@ u64 find_lock_delalloc_range(struct extent_map_tree *tree,
 
 	while(1) {
 		state = rb_entry(node, struct extent_state, rb_node);
-		if (state->start != cur_start) {
+		if (found && state->start != cur_start) {
 			goto out;
 		}
 		if (!(state->state & EXTENT_DELALLOC)) {
 			goto out;
 		}
-		if (state->start >= lock_start) {
-			if (state->state & EXTENT_LOCKED) {
-				DEFINE_WAIT(wait);
-				atomic_inc(&state->refs);
-				prepare_to_wait(&state->wq, &wait,
-						TASK_UNINTERRUPTIBLE);
-				write_unlock_irq(&tree->lock);
-				schedule();
-				write_lock_irq(&tree->lock);
-				finish_wait(&state->wq, &wait);
-				free_extent_state(state);
-				goto search_again;
+		if (!found) {
+			struct extent_state *prev_state;
+			struct rb_node *prev_node = node;
+			while(1) {
+				prev_node = rb_prev(prev_node);
+				if (!prev_node)
+					break;
+				prev_state = rb_entry(prev_node,
+						      struct extent_state,
+						      rb_node);
+				if (!(prev_state->state & EXTENT_DELALLOC))
+					break;
+				state = prev_state;
+				node = prev_node;
 			}
-			state->state |= EXTENT_LOCKED;
 		}
+		if (state->state & EXTENT_LOCKED) {
+			DEFINE_WAIT(wait);
+			atomic_inc(&state->refs);
+			prepare_to_wait(&state->wq, &wait,
+					TASK_UNINTERRUPTIBLE);
+			write_unlock_irq(&tree->lock);
+			schedule();
+			write_lock_irq(&tree->lock);
+			finish_wait(&state->wq, &wait);
+			free_extent_state(state);
+			goto search_again;
+		}
+		state->state |= EXTENT_LOCKED;
+		if (!found)
+			*start = state->start;
 		found++;
 		*end = state->end;
 		cur_start = state->end + 1;
@@ -1695,6 +1711,7 @@ static int __extent_writepage(struct page *page, struct writeback_control *wbc,
 	struct extent_page_data *epd = data;
 	struct extent_map_tree *tree = epd->tree;
 	u64 start = (u64)page->index << PAGE_CACHE_SHIFT;
+	u64 delalloc_start;
 	u64 page_end = start + PAGE_CACHE_SIZE - 1;
 	u64 end;
 	u64 cur = start;
@@ -1729,25 +1746,23 @@ static int __extent_writepage(struct page *page, struct writeback_control *wbc,
 
 	set_page_extent_mapped(page);
 
-	lock_extent(tree, start, page_end, GFP_NOFS);
-	nr_delalloc = find_lock_delalloc_range(tree, start, page_end + 1,
-					       &delalloc_end,
-					       128 * 1024 * 1024);
-	if (nr_delalloc) {
-		tree->ops->fill_delalloc(inode, start, delalloc_end);
-		if (delalloc_end >= page_end + 1) {
-			clear_extent_bit(tree, page_end + 1, delalloc_end,
-					 EXTENT_LOCKED | EXTENT_DELALLOC,
-					 1, 0, GFP_NOFS);
-		}
-		clear_extent_bit(tree, start, page_end, EXTENT_DELALLOC,
-				 0, 0, GFP_NOFS);
-		if (test_range_bit(tree, start, page_end, EXTENT_DELALLOC, 0)) {
-			printk("found delalloc bits after clear extent_bit\n");
-		}
-	} else if (test_range_bit(tree, start, page_end, EXTENT_DELALLOC, 0)) {
-		printk("found delalloc bits after find_delalloc_range returns 0\n");
+	delalloc_start = start;
+	delalloc_end = 0;
+	while(delalloc_end < page_end) {
+		nr_delalloc = find_lock_delalloc_range(tree, &delalloc_start,
+						       &delalloc_end,
+						       128 * 1024 * 1024);
+		if (nr_delalloc <= 0)
+			break;
+		tree->ops->fill_delalloc(inode, delalloc_start,
+					 delalloc_end);
+		clear_extent_bit(tree, delalloc_start,
+				 delalloc_end,
+				 EXTENT_LOCKED | EXTENT_DELALLOC,
+				 1, 0, GFP_NOFS);
+		delalloc_start = delalloc_end + 1;
 	}
+	lock_extent(tree, start, page_end, GFP_NOFS);
 
 	end = page_end;
 	if (test_range_bit(tree, start, page_end, EXTENT_DELALLOC, 0)) {

commit 2f4cbe6442d3be7b5e4cf1607a5ab33995fe2d25
Author: Wyatt Banks <wyatt@banksresearch.com>
Date:   Mon Nov 19 10:22:33 2007 -0500

    Btrfs: Return value checking in module init
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index b3ff63c4bf2a..f91f28efdb59 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -42,18 +42,30 @@ struct extent_page_data {
 	struct extent_map_tree *tree;
 	get_extent_t *get_extent;
 };
-
-void __init extent_map_init(void)
+int __init extent_map_init(void)
 {
 	extent_map_cache = btrfs_cache_create("extent_map",
 					    sizeof(struct extent_map), 0,
 					    NULL);
+	if (!extent_map_cache)
+		return -ENOMEM;
 	extent_state_cache = btrfs_cache_create("extent_state",
 					    sizeof(struct extent_state), 0,
 					    NULL);
+	if (!extent_state_cache)
+		goto free_map_cache;
 	extent_buffer_cache = btrfs_cache_create("extent_buffers",
 					    sizeof(struct extent_buffer), 0,
 					    NULL);
+	if (!extent_buffer_cache)
+		goto free_state_cache;
+	return 0;
+
+free_state_cache:
+	kmem_cache_destroy(extent_state_cache);
+free_map_cache:
+	kmem_cache_destroy(extent_map_cache);
+	return -ENOMEM;
 }
 
 void __exit extent_map_exit(void)

commit 0591fb56fb732c7bf500d1446d0bf927046d7002
Author: Chris Mason <chris.mason@oracle.com>
Date:   Sun Nov 11 08:22:00 2007 -0500

    Btrfs: Fix extent bit range testing
    
    It could return the bit as set when there was actually a hole at the
    very end of the range.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 0077c6c1d9f9..b3ff63c4bf2a 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -96,7 +96,7 @@ void extent_map_tree_empty_lru(struct extent_map_tree *tree)
 	while(!list_empty(&tree->buffer_lru)) {
 		eb = list_entry(tree->buffer_lru.next, struct extent_buffer,
 				lru);
-		list_del(&eb->lru);
+		list_del_init(&eb->lru);
 		free_extent_buffer(eb);
 	}
 }
@@ -1212,13 +1212,15 @@ int test_range_bit(struct extent_map_tree *tree, u64 start, u64 end,
 	node = tree_search(&tree->state, start);
 	while (node && start <= end) {
 		state = rb_entry(node, struct extent_state, rb_node);
-		if (state->start > end)
-			break;
 
 		if (filled && state->start > start) {
 			bitset = 0;
 			break;
 		}
+
+		if (state->start > end)
+			break;
+
 		if (state->state & bits) {
 			bitset = 1;
 			if (!filled)
@@ -2208,6 +2210,7 @@ struct extent_buffer *alloc_extent_buffer(struct extent_map_tree *tree,
 		page_cache_get(page0);
 		mark_page_accessed(page0);
 		set_page_extent_mapped(page0);
+		WARN_ON(!PageUptodate(page0));
 		set_page_private(page0, EXTENT_PAGE_PRIVATE_FIRST_PAGE |
 				 len << 2);
 	} else {
@@ -2248,9 +2251,11 @@ struct extent_buffer *alloc_extent_buffer(struct extent_map_tree *tree,
 	spin_unlock(&tree->lru_lock);
 	if (!atomic_dec_and_test(&eb->refs))
 		return NULL;
-	for (index = 0; index < i; index++) {
+	for (index = 1; index < i; index++) {
 		page_cache_release(extent_buffer_page(eb, index));
 	}
+	if (i > 0)
+		page_cache_release(extent_buffer_page(eb, 0));
 	__free_extent_buffer(eb);
 	return NULL;
 }
@@ -2310,9 +2315,11 @@ struct extent_buffer *find_extent_buffer(struct extent_map_tree *tree,
 	spin_unlock(&tree->lru_lock);
 	if (!atomic_dec_and_test(&eb->refs))
 		return NULL;
-	for (index = 0; index < i; index++) {
+	for (index = 1; index < i; index++) {
 		page_cache_release(extent_buffer_page(eb, index));
 	}
+	if (i > 0)
+		page_cache_release(extent_buffer_page(eb, 0));
 	__free_extent_buffer(eb);
 	return NULL;
 }
@@ -2329,11 +2336,13 @@ void free_extent_buffer(struct extent_buffer *eb)
 	if (!atomic_dec_and_test(&eb->refs))
 		return;
 
+	WARN_ON(!list_empty(&eb->lru));
 	num_pages = num_extent_pages(eb->start, eb->len);
 
-	for (i = 0; i < num_pages; i++) {
+	for (i = 1; i < num_pages; i++) {
 		page_cache_release(extent_buffer_page(eb, i));
 	}
+	page_cache_release(extent_buffer_page(eb, 0));
 	__free_extent_buffer(eb);
 }
 EXPORT_SYMBOL(free_extent_buffer);
@@ -2469,6 +2478,7 @@ int read_extent_buffer_pages(struct extent_map_tree *tree,
 			   EXTENT_UPTODATE, 1)) {
 		return 0;
 	}
+
 	if (start) {
 		WARN_ON(start < eb->start);
 		start_i = (start >> PAGE_CACHE_SHIFT) -
@@ -2577,7 +2587,7 @@ int map_private_extent_buffer(struct extent_buffer *eb, unsigned long start,
 		*map_start = 0;
 	} else {
 		offset = 0;
-		*map_start = (i << PAGE_CACHE_SHIFT) - start_offset;
+		*map_start = ((u64)i << PAGE_CACHE_SHIFT) - start_offset;
 	}
 	if (start + min_len > eb->len) {
 printk("bad mapping eb start %Lu len %lu, wanted %lu %lu\n", eb->start, eb->len, start, min_len);

commit 3ab2fb5a8cb003897016b6eb38ddad916226c1b2
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Nov 8 10:59:22 2007 -0500

    Btrfs: Add readpages support
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 754bc42c162a..0077c6c1d9f9 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -11,6 +11,7 @@
 #include <linux/swap.h>
 #include <linux/version.h>
 #include <linux/writeback.h>
+#include <linux/pagevec.h>
 #include "extent_map.h"
 
 /* temporary define until extent_map moves out of btrfs */
@@ -1503,7 +1504,7 @@ static int submit_extent_page(int rw, struct extent_map_tree *tree,
 			      size_t size, unsigned long offset,
 			      struct block_device *bdev,
 			      struct bio **bio_ret,
-			      int max_pages,
+			      unsigned long max_pages,
 			      bio_end_io_t end_io_func)
 {
 	int ret = 0;
@@ -1520,7 +1521,7 @@ static int submit_extent_page(int rw, struct extent_map_tree *tree,
 			return 0;
 		}
 	}
-	nr = min(max_pages, bio_get_nr_vecs(bdev));
+	nr = min_t(int, max_pages, bio_get_nr_vecs(bdev));
 	bio = extent_bio_alloc(bdev, sector, nr, GFP_NOFS | __GFP_HIGH);
 	if (!bio) {
 		printk("failed to allocate bio nr %d\n", nr);
@@ -1552,8 +1553,10 @@ void set_page_extent_mapped(struct page *page)
  * into the tree that are removed when the IO is done (by the end_io
  * handlers)
  */
-int extent_read_full_page(struct extent_map_tree *tree, struct page *page,
-			  get_extent_t *get_extent)
+static int __extent_read_full_page(struct extent_map_tree *tree,
+				   struct page *page,
+				   get_extent_t *get_extent,
+				   struct bio **bio)
 {
 	struct inode *inode = page->mapping->host;
 	u64 start = (u64)page->index << PAGE_CACHE_SHIFT;
@@ -1631,10 +1634,12 @@ int extent_read_full_page(struct extent_map_tree *tree, struct page *page,
 							  cur + iosize - 1);
 		}
 		if (!ret) {
+			unsigned long nr = (last_byte >> PAGE_CACHE_SHIFT) + 1;
+			nr -= page->index;
 			ret = submit_extent_page(READ, tree, page,
-						 sector, iosize, page_offset,
-						 bdev, NULL, 1,
-						 end_bio_extent_readpage);
+					 sector, iosize, page_offset,
+					 bdev, bio, nr,
+					 end_bio_extent_readpage);
 		}
 		if (ret)
 			SetPageError(page);
@@ -1649,6 +1654,18 @@ int extent_read_full_page(struct extent_map_tree *tree, struct page *page,
 	}
 	return 0;
 }
+
+int extent_read_full_page(struct extent_map_tree *tree, struct page *page,
+			    get_extent_t *get_extent)
+{
+	struct bio *bio = NULL;
+	int ret;
+
+	ret = __extent_read_full_page(tree, page, get_extent, &bio);
+	if (bio)
+		submit_one_bio(READ, bio);
+	return ret;
+}
 EXPORT_SYMBOL(extent_read_full_page);
 
 /*
@@ -1836,6 +1853,45 @@ int extent_writepages(struct extent_map_tree *tree,
 }
 EXPORT_SYMBOL(extent_writepages);
 
+int extent_readpages(struct extent_map_tree *tree,
+		     struct address_space *mapping,
+		     struct list_head *pages, unsigned nr_pages,
+		     get_extent_t get_extent)
+{
+	struct bio *bio = NULL;
+	unsigned page_idx;
+	struct pagevec pvec;
+
+	pagevec_init(&pvec, 0);
+	for (page_idx = 0; page_idx < nr_pages; page_idx++) {
+		struct page *page = list_entry(pages->prev, struct page, lru);
+
+		prefetchw(&page->flags);
+		list_del(&page->lru);
+		/*
+		 * what we want to do here is call add_to_page_cache_lru,
+		 * but that isn't exported, so we reproduce it here
+		 */
+		if (!add_to_page_cache(page, mapping,
+					page->index, GFP_KERNEL)) {
+
+			/* open coding of lru_cache_add, also not exported */
+			page_cache_get(page);
+			if (!pagevec_add(&pvec, page))
+				__pagevec_lru_add(&pvec);
+			__extent_read_full_page(tree, page, get_extent, &bio);
+		}
+		page_cache_release(page);
+	}
+	if (pagevec_count(&pvec))
+		__pagevec_lru_add(&pvec);
+	BUG_ON(!list_empty(pages));
+	if (bio)
+		submit_one_bio(READ, bio);
+	return 0;
+}
+EXPORT_SYMBOL(extent_readpages);
+
 /*
  * basic invalidatepage code, this waits on any locked or writeback
  * ranges corresponding to the page, and then deletes any extent state

commit 856bf3e592f917e7d663cb7fa93e83fe795e8f4e
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Nov 8 10:59:05 2007 -0500

    Btrfs: Avoid extent_buffer lru corruption
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index c976615dcda3..754bc42c162a 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -2047,7 +2047,7 @@ static int add_lru(struct extent_map_tree *tree, struct extent_buffer *eb)
 			rm = list_entry(tree->buffer_lru.prev,
 					struct extent_buffer, lru);
 			tree->lru_size--;
-			list_del(&rm->lru);
+			list_del_init(&rm->lru);
 			free_extent_buffer(rm);
 		}
 	} else
@@ -2187,6 +2187,9 @@ struct extent_buffer *alloc_extent_buffer(struct extent_map_tree *tree,
 	return eb;
 
 fail:
+	spin_lock(&tree->lru_lock);
+	list_del_init(&eb->lru);
+	spin_unlock(&tree->lru_lock);
 	if (!atomic_dec_and_test(&eb->refs))
 		return NULL;
 	for (index = 0; index < i; index++) {
@@ -2246,6 +2249,9 @@ struct extent_buffer *find_extent_buffer(struct extent_map_tree *tree,
 	spin_unlock(&tree->lru_lock);
 	return eb;
 fail:
+	spin_lock(&tree->lru_lock);
+	list_del_init(&eb->lru);
+	spin_unlock(&tree->lru_lock);
 	if (!atomic_dec_and_test(&eb->refs))
 		return NULL;
 	for (index = 0; index < i; index++) {

commit 09be207d1ba224531a61de9afdc07a125e45318c
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Nov 7 21:08:16 2007 -0500

    Btrfs: Fix failure cleanups when allocating extent buffers fail
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index b0677c84bb75..c976615dcda3 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -2106,25 +2106,17 @@ static struct extent_buffer *__alloc_extent_buffer(struct extent_map_tree *tree,
 
 	spin_lock(&tree->lru_lock);
 	eb = find_lru(tree, start, len);
-	if (eb) {
-		goto lru_add;
-	}
 	spin_unlock(&tree->lru_lock);
-
 	if (eb) {
-		memset(eb, 0, sizeof(*eb));
-	} else {
-		eb = kmem_cache_zalloc(extent_buffer_cache, mask);
+		return eb;
 	}
+
+	eb = kmem_cache_zalloc(extent_buffer_cache, mask);
 	INIT_LIST_HEAD(&eb->lru);
 	eb->start = start;
 	eb->len = len;
 	atomic_set(&eb->refs, 1);
 
-	spin_lock(&tree->lru_lock);
-lru_add:
-	add_lru(tree, eb);
-	spin_unlock(&tree->lru_lock);
 	return eb;
 }
 
@@ -2151,7 +2143,7 @@ struct extent_buffer *alloc_extent_buffer(struct extent_map_tree *tree,
 		return NULL;
 
 	if (eb->flags & EXTENT_BUFFER_FILLED)
-		return eb;
+		goto lru_add;
 
 	if (page0) {
 		eb->first_page = page0;
@@ -2169,11 +2161,6 @@ struct extent_buffer *alloc_extent_buffer(struct extent_map_tree *tree,
 		p = find_or_create_page(mapping, index, mask | __GFP_HIGHMEM);
 		if (!p) {
 			WARN_ON(1);
-			/* make sure the free only frees the pages we've
-			 * grabbed a reference on
-			 */
-			eb->len = i << PAGE_CACHE_SHIFT;
-			eb->start &= ~((u64)PAGE_CACHE_SIZE - 1);
 			goto fail;
 		}
 		set_page_extent_mapped(p);
@@ -2192,9 +2179,20 @@ struct extent_buffer *alloc_extent_buffer(struct extent_map_tree *tree,
 	if (uptodate)
 		eb->flags |= EXTENT_UPTODATE;
 	eb->flags |= EXTENT_BUFFER_FILLED;
+
+lru_add:
+	spin_lock(&tree->lru_lock);
+	add_lru(tree, eb);
+	spin_unlock(&tree->lru_lock);
 	return eb;
+
 fail:
-	free_extent_buffer(eb);
+	if (!atomic_dec_and_test(&eb->refs))
+		return NULL;
+	for (index = 0; index < i; index++) {
+		page_cache_release(extent_buffer_page(eb, index));
+	}
+	__free_extent_buffer(eb);
 	return NULL;
 }
 EXPORT_SYMBOL(alloc_extent_buffer);
@@ -2204,7 +2202,8 @@ struct extent_buffer *find_extent_buffer(struct extent_map_tree *tree,
 					  gfp_t mask)
 {
 	unsigned long num_pages = num_extent_pages(start, len);
-	unsigned long i; unsigned long index = start >> PAGE_CACHE_SHIFT;
+	unsigned long i;
+	unsigned long index = start >> PAGE_CACHE_SHIFT;
 	struct extent_buffer *eb;
 	struct page *p;
 	struct address_space *mapping = tree->mapping;
@@ -2215,16 +2214,11 @@ struct extent_buffer *find_extent_buffer(struct extent_map_tree *tree,
 		return NULL;
 
 	if (eb->flags & EXTENT_BUFFER_FILLED)
-		return eb;
+		goto lru_add;
 
 	for (i = 0; i < num_pages; i++, index++) {
 		p = find_lock_page(mapping, index);
 		if (!p) {
-			/* make sure the free only frees the pages we've
-			 * grabbed a reference on
-			 */
-			eb->len = i << PAGE_CACHE_SHIFT;
-			eb->start &= ~((u64)PAGE_CACHE_SIZE - 1);
 			goto fail;
 		}
 		set_page_extent_mapped(p);
@@ -2245,9 +2239,19 @@ struct extent_buffer *find_extent_buffer(struct extent_map_tree *tree,
 	if (uptodate)
 		eb->flags |= EXTENT_UPTODATE;
 	eb->flags |= EXTENT_BUFFER_FILLED;
+
+lru_add:
+	spin_lock(&tree->lru_lock);
+	add_lru(tree, eb);
+	spin_unlock(&tree->lru_lock);
 	return eb;
 fail:
-	free_extent_buffer(eb);
+	if (!atomic_dec_and_test(&eb->refs))
+		return NULL;
+	for (index = 0; index < i; index++) {
+		page_cache_release(extent_buffer_page(eb, index));
+	}
+	__free_extent_buffer(eb);
 	return NULL;
 }
 EXPORT_SYMBOL(find_extent_buffer);

commit b293f02e1423f2099744f3ade23ddd83b65321fc
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Nov 1 19:45:34 2007 -0400

    Btrfs: Add writepages support
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index ff8881fb56d6..b0677c84bb75 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -10,6 +10,7 @@
 #include <linux/blkdev.h>
 #include <linux/swap.h>
 #include <linux/version.h>
+#include <linux/writeback.h>
 #include "extent_map.h"
 
 /* temporary define until extent_map moves out of btrfs */
@@ -35,6 +36,12 @@ struct tree_entry {
 	struct rb_node rb_node;
 };
 
+struct extent_page_data {
+	struct bio *bio;
+	struct extent_map_tree *tree;
+	get_extent_t *get_extent;
+};
+
 void __init extent_map_init(void)
 {
 	extent_map_cache = btrfs_cache_create("extent_map",
@@ -1460,40 +1467,76 @@ static int end_bio_extent_preparewrite(struct bio *bio,
 #endif
 }
 
-static int submit_extent_page(int rw, struct extent_map_tree *tree,
-			      struct page *page, sector_t sector,
-			      size_t size, unsigned long offset,
-			      struct block_device *bdev,
-			      bio_end_io_t end_io_func)
+static struct bio *
+extent_bio_alloc(struct block_device *bdev, u64 first_sector, int nr_vecs,
+		 gfp_t gfp_flags)
 {
 	struct bio *bio;
-	int ret = 0;
 
-	bio = bio_alloc(GFP_NOIO, 1);
+	bio = bio_alloc(gfp_flags, nr_vecs);
 
-	bio->bi_sector = sector;
-	bio->bi_bdev = bdev;
-	bio->bi_io_vec[0].bv_page = page;
-	bio->bi_io_vec[0].bv_len = size;
-	bio->bi_io_vec[0].bv_offset = offset;
-
-	bio->bi_vcnt = 1;
-	bio->bi_idx = 0;
-	bio->bi_size = size;
+	if (bio == NULL && (current->flags & PF_MEMALLOC)) {
+		while (!bio && (nr_vecs /= 2))
+			bio = bio_alloc(gfp_flags, nr_vecs);
+	}
 
-	bio->bi_end_io = end_io_func;
-	bio->bi_private = tree;
+	if (bio) {
+		bio->bi_bdev = bdev;
+		bio->bi_sector = first_sector;
+	}
+	return bio;
+}
 
+static int submit_one_bio(int rw, struct bio *bio)
+{
+	int ret = 0;
 	bio_get(bio);
 	submit_bio(rw, bio);
-
 	if (bio_flagged(bio, BIO_EOPNOTSUPP))
 		ret = -EOPNOTSUPP;
-
 	bio_put(bio);
 	return ret;
 }
 
+static int submit_extent_page(int rw, struct extent_map_tree *tree,
+			      struct page *page, sector_t sector,
+			      size_t size, unsigned long offset,
+			      struct block_device *bdev,
+			      struct bio **bio_ret,
+			      int max_pages,
+			      bio_end_io_t end_io_func)
+{
+	int ret = 0;
+	struct bio *bio;
+	int nr;
+
+	if (bio_ret && *bio_ret) {
+		bio = *bio_ret;
+		if (bio->bi_sector + (bio->bi_size >> 9) != sector ||
+		    bio_add_page(bio, page, size, offset) < size) {
+			ret = submit_one_bio(rw, bio);
+			bio = NULL;
+		} else {
+			return 0;
+		}
+	}
+	nr = min(max_pages, bio_get_nr_vecs(bdev));
+	bio = extent_bio_alloc(bdev, sector, nr, GFP_NOFS | __GFP_HIGH);
+	if (!bio) {
+		printk("failed to allocate bio nr %d\n", nr);
+	}
+	bio_add_page(bio, page, size, offset);
+	bio->bi_end_io = end_io_func;
+	bio->bi_private = tree;
+	if (bio_ret) {
+		*bio_ret = bio;
+	} else {
+		ret = submit_one_bio(rw, bio);
+	}
+
+	return ret;
+}
+
 void set_page_extent_mapped(struct page *page)
 {
 	if (!PagePrivate(page)) {
@@ -1590,7 +1633,8 @@ int extent_read_full_page(struct extent_map_tree *tree, struct page *page,
 		if (!ret) {
 			ret = submit_extent_page(READ, tree, page,
 						 sector, iosize, page_offset,
-						 bdev, end_bio_extent_readpage);
+						 bdev, NULL, 1,
+						 end_bio_extent_readpage);
 		}
 		if (ret)
 			SetPageError(page);
@@ -1613,11 +1657,12 @@ EXPORT_SYMBOL(extent_read_full_page);
  * are found, they are marked writeback.  Then the lock bits are removed
  * and the end_io handler clears the writeback ranges
  */
-int extent_write_full_page(struct extent_map_tree *tree, struct page *page,
-			  get_extent_t *get_extent,
-			  struct writeback_control *wbc)
+static int __extent_writepage(struct page *page, struct writeback_control *wbc,
+			      void *data)
 {
 	struct inode *inode = page->mapping->host;
+	struct extent_page_data *epd = data;
+	struct extent_map_tree *tree = epd->tree;
 	u64 start = (u64)page->index << PAGE_CACHE_SHIFT;
 	u64 page_end = start + PAGE_CACHE_SIZE - 1;
 	u64 end;
@@ -1691,7 +1736,7 @@ int extent_write_full_page(struct extent_map_tree *tree, struct page *page,
 			clear_extent_dirty(tree, cur, page_end, GFP_NOFS);
 			break;
 		}
-		em = get_extent(inode, page, page_offset, cur, end, 1);
+		em = epd->get_extent(inode, page, page_offset, cur, end, 1);
 		if (IS_ERR(em) || !em) {
 			SetPageError(page);
 			break;
@@ -1734,9 +1779,12 @@ int extent_write_full_page(struct extent_map_tree *tree, struct page *page,
 		if (ret)
 			SetPageError(page);
 		else {
+			unsigned long nr = end_index + 1;
 			set_range_writeback(tree, cur, cur + iosize - 1);
+
 			ret = submit_extent_page(WRITE, tree, page, sector,
 						 iosize, page_offset, bdev,
+						 &epd->bio, nr,
 						 end_bio_extent_writepage);
 			if (ret)
 				SetPageError(page);
@@ -1750,8 +1798,44 @@ int extent_write_full_page(struct extent_map_tree *tree, struct page *page,
 	unlock_page(page);
 	return 0;
 }
+
+int extent_write_full_page(struct extent_map_tree *tree, struct page *page,
+			  get_extent_t *get_extent,
+			  struct writeback_control *wbc)
+{
+	int ret;
+	struct extent_page_data epd = {
+		.bio = NULL,
+		.tree = tree,
+		.get_extent = get_extent,
+	};
+
+	ret = __extent_writepage(page, wbc, &epd);
+	if (epd.bio)
+		submit_one_bio(WRITE, epd.bio);
+	return ret;
+}
 EXPORT_SYMBOL(extent_write_full_page);
 
+int extent_writepages(struct extent_map_tree *tree,
+		      struct address_space *mapping,
+		      get_extent_t *get_extent,
+		      struct writeback_control *wbc)
+{
+	int ret;
+	struct extent_page_data epd = {
+		.bio = NULL,
+		.tree = tree,
+		.get_extent = get_extent,
+	};
+
+	ret = write_cache_pages(mapping, wbc, __extent_writepage, &epd);
+	if (epd.bio)
+		submit_one_bio(WRITE, epd.bio);
+	return ret;
+}
+EXPORT_SYMBOL(extent_writepages);
+
 /*
  * basic invalidatepage code, this waits on any locked or writeback
  * ranges corresponding to the page, and then deletes any extent state
@@ -1869,6 +1953,7 @@ int extent_prepare_write(struct extent_map_tree *tree,
 				       EXTENT_LOCKED, 0, NULL, GFP_NOFS);
 			ret = submit_extent_page(READ, tree, page,
 					 sector, iosize, page_offset, em->bdev,
+					 NULL, 1,
 					 end_bio_extent_preparewrite);
 			iocount++;
 			block_start = block_start + iosize;

commit 944746ec7575258475958d858cdf3a0e4c8f7b9e
Author: Yan <yanzheng@21cn.com>
Date:   Thu Nov 1 11:28:42 2007 -0400

    Btrfs: small fixes for find_lock_delalloc_range.
    
    There is a 'finish_wait', but no 'prepare_to_wait' . So I think that
    the 'prepare_to_wait' is missing. The second change is  according to
    the name of variable.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 44be9cfd30ee..ff8881fb56d6 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -1044,6 +1044,8 @@ u64 find_lock_delalloc_range(struct extent_map_tree *tree,
 			if (state->state & EXTENT_LOCKED) {
 				DEFINE_WAIT(wait);
 				atomic_inc(&state->refs);
+				prepare_to_wait(&state->wq, &wait,
+						TASK_UNINTERRUPTIBLE);
 				write_unlock_irq(&tree->lock);
 				schedule();
 				write_lock_irq(&tree->lock);
@@ -1059,7 +1061,7 @@ u64 find_lock_delalloc_range(struct extent_map_tree *tree,
 		node = rb_next(node);
 		if (!node)
 			break;
-		total_bytes = state->end - state->start + 1;
+		total_bytes += state->end - state->start + 1;
 		if (total_bytes >= max_bytes)
 			break;
 	}

commit 179e29e488cc74f1e9bd67bc45f70b832740e9ec
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Nov 1 11:28:41 2007 -0400

    Btrfs: Fix a number of inline extent problems that Yan Zheng reported.
    
    The fixes do a number of things:
    
    1) Most btrfs_drop_extent callers will try to leave the inline extents in
    place.  It can truncate bytes off the beginning of the inline extent if
    required.
    
    2) writepage can now update the inline extent, allowing mmap writes to
    go directly into the inline extent.
    
    3) btrfs_truncate_in_transaction truncates inline extents
    
    4) extent_map.c fixed to not merge inline extent mappings and hole
    mappings together
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 238cb1d81d56..44be9cfd30ee 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -263,7 +263,12 @@ int add_extent_mapping(struct extent_map_tree *tree,
 		if (prev && prev->end + 1 == em->start &&
 		    ((em->block_start == EXTENT_MAP_HOLE &&
 		      prev->block_start == EXTENT_MAP_HOLE) ||
-			     (em->block_start == prev->block_end + 1))) {
+		     (em->block_start == EXTENT_MAP_INLINE &&
+		      prev->block_start == EXTENT_MAP_INLINE) ||
+		     (em->block_start == EXTENT_MAP_DELALLOC &&
+		      prev->block_start == EXTENT_MAP_DELALLOC) ||
+		     (em->block_start < EXTENT_MAP_DELALLOC - 1 &&
+		      em->block_start == prev->block_end + 1))) {
 			em->start = prev->start;
 			em->block_start = prev->block_start;
 			rb_erase(&prev->rb_node, &tree->map);
@@ -1618,13 +1623,13 @@ int extent_write_full_page(struct extent_map_tree *tree, struct page *page,
 	u64 extent_offset;
 	u64 last_byte = i_size_read(inode);
 	u64 block_start;
+	u64 iosize;
 	sector_t sector;
 	struct extent_map *em;
 	struct block_device *bdev;
 	int ret;
 	int nr = 0;
 	size_t page_offset = 0;
-	size_t iosize;
 	size_t blocksize;
 	loff_t i_size = i_size_read(inode);
 	unsigned long end_index = i_size >> PAGE_CACHE_SHIFT;
@@ -1684,7 +1689,7 @@ int extent_write_full_page(struct extent_map_tree *tree, struct page *page,
 			clear_extent_dirty(tree, cur, page_end, GFP_NOFS);
 			break;
 		}
-		em = get_extent(inode, page, page_offset, cur, end, 0);
+		em = get_extent(inode, page, page_offset, cur, end, 1);
 		if (IS_ERR(em) || !em) {
 			SetPageError(page);
 			break;

commit 35ebb934bd7fcc7ca991b155b7980c3c4ff9f1a5
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Oct 30 16:56:53 2007 -0400

    Btrfs: Fix PAGE_CACHE_SHIFT shifts on 32 bit machines
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index e4ee980cc138..238cb1d81d56 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -1229,7 +1229,7 @@ EXPORT_SYMBOL(test_range_bit);
 static int check_page_uptodate(struct extent_map_tree *tree,
 			       struct page *page)
 {
-	u64 start = page->index << PAGE_CACHE_SHIFT;
+	u64 start = (u64)page->index << PAGE_CACHE_SHIFT;
 	u64 end = start + PAGE_CACHE_SIZE - 1;
 	if (test_range_bit(tree, start, end, EXTENT_UPTODATE, 1))
 		SetPageUptodate(page);
@@ -1243,7 +1243,7 @@ static int check_page_uptodate(struct extent_map_tree *tree,
 static int check_page_locked(struct extent_map_tree *tree,
 			     struct page *page)
 {
-	u64 start = page->index << PAGE_CACHE_SHIFT;
+	u64 start = (u64)page->index << PAGE_CACHE_SHIFT;
 	u64 end = start + PAGE_CACHE_SIZE - 1;
 	if (!test_range_bit(tree, start, end, EXTENT_LOCKED, 0))
 		unlock_page(page);
@@ -1257,7 +1257,7 @@ static int check_page_locked(struct extent_map_tree *tree,
 static int check_page_writeback(struct extent_map_tree *tree,
 			     struct page *page)
 {
-	u64 start = page->index << PAGE_CACHE_SHIFT;
+	u64 start = (u64)page->index << PAGE_CACHE_SHIFT;
 	u64 end = start + PAGE_CACHE_SIZE - 1;
 	if (!test_range_bit(tree, start, end, EXTENT_WRITEBACK, 0))
 		end_page_writeback(page);
@@ -1296,7 +1296,8 @@ static int end_bio_extent_writepage(struct bio *bio,
 
 	do {
 		struct page *page = bvec->bv_page;
-		start = (page->index << PAGE_CACHE_SHIFT) + bvec->bv_offset;
+		start = ((u64)page->index << PAGE_CACHE_SHIFT) +
+			 bvec->bv_offset;
 		end = start + bvec->bv_len - 1;
 
 		if (bvec->bv_offset == 0 && bvec->bv_len == PAGE_CACHE_SIZE)
@@ -1361,7 +1362,8 @@ static int end_bio_extent_readpage(struct bio *bio,
 
 	do {
 		struct page *page = bvec->bv_page;
-		start = (page->index << PAGE_CACHE_SHIFT) + bvec->bv_offset;
+		start = ((u64)page->index << PAGE_CACHE_SHIFT) +
+			bvec->bv_offset;
 		end = start + bvec->bv_len - 1;
 
 		if (bvec->bv_offset == 0 && bvec->bv_len == PAGE_CACHE_SIZE)
@@ -1427,7 +1429,8 @@ static int end_bio_extent_preparewrite(struct bio *bio,
 
 	do {
 		struct page *page = bvec->bv_page;
-		start = (page->index << PAGE_CACHE_SHIFT) + bvec->bv_offset;
+		start = ((u64)page->index << PAGE_CACHE_SHIFT) +
+			bvec->bv_offset;
 		end = start + bvec->bv_len - 1;
 
 		if (--bvec >= bio->bi_io_vec)
@@ -1503,7 +1506,7 @@ int extent_read_full_page(struct extent_map_tree *tree, struct page *page,
 			  get_extent_t *get_extent)
 {
 	struct inode *inode = page->mapping->host;
-	u64 start = page->index << PAGE_CACHE_SHIFT;
+	u64 start = (u64)page->index << PAGE_CACHE_SHIFT;
 	u64 page_end = start + PAGE_CACHE_SIZE - 1;
 	u64 end;
 	u64 cur = start;
@@ -1608,7 +1611,7 @@ int extent_write_full_page(struct extent_map_tree *tree, struct page *page,
 			  struct writeback_control *wbc)
 {
 	struct inode *inode = page->mapping->host;
-	u64 start = page->index << PAGE_CACHE_SHIFT;
+	u64 start = (u64)page->index << PAGE_CACHE_SHIFT;
 	u64 page_end = start + PAGE_CACHE_SIZE - 1;
 	u64 end;
 	u64 cur = start;
@@ -1750,7 +1753,7 @@ EXPORT_SYMBOL(extent_write_full_page);
 int extent_invalidatepage(struct extent_map_tree *tree,
 			  struct page *page, unsigned long offset)
 {
-	u64 start = (page->index << PAGE_CACHE_SHIFT);
+	u64 start = ((u64)page->index << PAGE_CACHE_SHIFT);
 	u64 end = start + PAGE_CACHE_SIZE - 1;
 	size_t blocksize = page->mapping->host->i_sb->s_blocksize;
 
@@ -1792,7 +1795,7 @@ int extent_prepare_write(struct extent_map_tree *tree,
 			 struct inode *inode, struct page *page,
 			 unsigned from, unsigned to, get_extent_t *get_extent)
 {
-	u64 page_start = page->index << PAGE_CACHE_SHIFT;
+	u64 page_start = (u64)page->index << PAGE_CACHE_SHIFT;
 	u64 page_end = page_start + PAGE_CACHE_SIZE - 1;
 	u64 block_start;
 	u64 orig_block_start;
@@ -1890,7 +1893,7 @@ EXPORT_SYMBOL(extent_prepare_write);
 int try_release_extent_mapping(struct extent_map_tree *tree, struct page *page)
 {
 	struct extent_map *em;
-	u64 start = page->index << PAGE_CACHE_SHIFT;
+	u64 start = (u64)page->index << PAGE_CACHE_SHIFT;
 	u64 end = start + PAGE_CACHE_SIZE - 1;
 	u64 orig_start = start;
 	int ret = 1;
@@ -2202,7 +2205,7 @@ int clear_extent_buffer_dirty(struct extent_map_tree *tree,
 		if ((i == 0 && (eb->start & (PAGE_CACHE_SIZE - 1))) ||
 		    ((i == num_pages - 1) &&
 		     ((eb->start + eb->len) & (PAGE_CACHE_SIZE - 1)))) {
-			start = page->index << PAGE_CACHE_SHIFT;
+			start = (u64)page->index << PAGE_CACHE_SHIFT;
 			end  = start + PAGE_CACHE_SIZE - 1;
 			if (test_range_bit(tree, start, end,
 					   EXTENT_DIRTY, 0)) {

commit c67cda1758130f3e437744c4c2ec4a7cc25564e3
Author: Yan <yanzheng@21cn.com>
Date:   Mon Oct 29 11:41:05 2007 -0400

    Btrfs: Fix extent_map leak in extent_bmap
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index c44989a1e520..e4ee980cc138 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -1924,6 +1924,7 @@ sector_t extent_bmap(struct address_space *mapping, sector_t iblock,
 	struct inode *inode = mapping->host;
 	u64 start = iblock << inode->i_blkbits;
 	u64 end = start + (1 << inode->i_blkbits) - 1;
+	sector_t sector = 0;
 	struct extent_map *em;
 
 	em = get_extent(inode, NULL, 0, start, end, 0);
@@ -1932,9 +1933,12 @@ sector_t extent_bmap(struct address_space *mapping, sector_t iblock,
 
 	if (em->block_start == EXTENT_MAP_INLINE ||
 	    em->block_start == EXTENT_MAP_HOLE)
-		return 0;
+		goto out;
 
-	return (em->block_start + start - em->start) >> inode->i_blkbits;
+	sector = (em->block_start + start - em->start) >> inode->i_blkbits;
+out:
+	free_extent_map(em);
+	return sector;
 }
 
 static int add_lru(struct extent_map_tree *tree, struct extent_buffer *eb)

commit 65555a06b4d1ae116ce223dc4b82d6068b36df96
Author: Yan <yanzheng@21cn.com>
Date:   Thu Oct 25 15:42:57 2007 -0400

    Btrfs: Off by one fixes in extent_map.c
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index a61379230dd2..c44989a1e520 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -2045,7 +2045,7 @@ struct extent_buffer *alloc_extent_buffer(struct extent_map_tree *tree,
 	struct extent_buffer *eb;
 	struct page *p;
 	struct address_space *mapping = tree->mapping;
-	int uptodate = 0;
+	int uptodate = 1;
 
 	eb = __alloc_extent_buffer(tree, start, len, mask);
 	if (!eb || IS_ERR(eb))
@@ -2197,7 +2197,7 @@ int clear_extent_buffer_dirty(struct extent_map_tree *tree,
 		 */
 		if ((i == 0 && (eb->start & (PAGE_CACHE_SIZE - 1))) ||
 		    ((i == num_pages - 1) &&
-		     ((eb->start + eb->len - 1) & (PAGE_CACHE_SIZE - 1)))) {
+		     ((eb->start + eb->len) & (PAGE_CACHE_SIZE - 1)))) {
 			start = page->index << PAGE_CACHE_SHIFT;
 			end  = start + PAGE_CACHE_SIZE - 1;
 			if (test_range_bit(tree, start, end,
@@ -2265,7 +2265,7 @@ int set_extent_buffer_uptodate(struct extent_map_tree *tree,
 		page = extent_buffer_page(eb, i);
 		if ((i == 0 && (eb->start & (PAGE_CACHE_SIZE - 1))) ||
 		    ((i == num_pages - 1) &&
-		     ((eb->start + eb->len - 1) & (PAGE_CACHE_SIZE - 1)))) {
+		     ((eb->start + eb->len) & (PAGE_CACHE_SIZE - 1)))) {
 			check_page_uptodate(tree, page);
 			continue;
 		}
@@ -2401,7 +2401,7 @@ int map_private_extent_buffer(struct extent_buffer *eb, unsigned long start,
 	struct page *p;
 	size_t start_offset = eb->start & ((u64)PAGE_CACHE_SIZE - 1);
 	unsigned long i = (start_offset + start) >> PAGE_CACHE_SHIFT;
-	unsigned long end_i = (start_offset + start + min_len) >>
+	unsigned long end_i = (start_offset + start + min_len - 1) >>
 		PAGE_CACHE_SHIFT;
 
 	if (i != end_i)
@@ -2414,7 +2414,7 @@ int map_private_extent_buffer(struct extent_buffer *eb, unsigned long start,
 		offset = 0;
 		*map_start = (i << PAGE_CACHE_SHIFT) - start_offset;
 	}
-	if (start + min_len >= eb->len) {
+	if (start + min_len > eb->len) {
 printk("bad mapping eb start %Lu len %lu, wanted %lu %lu\n", eb->start, eb->len, start, min_len);
 		WARN_ON(1);
 	}

commit ff190c0c004d8e51195c7bcf5a8490aeefccbce0
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Oct 19 10:39:41 2007 -0400

    Btrfs: Avoid recursive KM_USER1 mappings in copy_extent_buffer
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index caaf0bf0e059..a61379230dd2 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -2586,9 +2586,9 @@ void copy_extent_buffer(struct extent_buffer *dst, struct extent_buffer *src,
 
 		cur = min(len, (unsigned long)(PAGE_CACHE_SIZE - offset));
 
-		kaddr = kmap_atomic(page, KM_USER1);
+		kaddr = kmap_atomic(page, KM_USER0);
 		read_extent_buffer(src, kaddr + offset, src_offset, cur);
-		kunmap_atomic(kaddr, KM_USER1);
+		kunmap_atomic(kaddr, KM_USER0);
 
 		src_offset += cur;
 		len -= cur;

commit 3685f791659c9f21b763ee1702ac8ca58bc20f81
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Oct 19 09:23:27 2007 -0400

    Btrfs: CPU usage optimizations in push and the extent_map code
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 3c81f5eab155..caaf0bf0e059 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -1986,12 +1986,15 @@ static inline struct page *extent_buffer_page(struct extent_buffer *eb,
 					      unsigned long i)
 {
 	struct page *p;
+	struct address_space *mapping;
 
 	if (i == 0)
 		return eb->first_page;
 	i += eb->start >> PAGE_CACHE_SHIFT;
-	p = find_get_page(eb->first_page->mapping, i);
-	page_cache_release(p);
+	mapping = eb->first_page->mapping;
+	read_lock_irq(&mapping->tree_lock);
+	p = radix_tree_lookup(&mapping->page_tree, i);
+	read_unlock_irq(&mapping->tree_lock);
 	return p;
 }
 
@@ -2365,9 +2368,7 @@ void read_extent_buffer(struct extent_buffer *eb, void *dstv,
 	WARN_ON(start > eb->len);
 	WARN_ON(start + len > eb->start + eb->len);
 
-	offset = start & ((unsigned long)PAGE_CACHE_SIZE - 1);
-	if (i == 0)
-		offset += start_offset;
+	offset = (start_offset + start) & ((unsigned long)PAGE_CACHE_SIZE - 1);
 
 	while(len > 0) {
 		page = extent_buffer_page(eb, i);
@@ -2475,9 +2476,7 @@ int memcmp_extent_buffer(struct extent_buffer *eb, const void *ptrv,
 	WARN_ON(start > eb->len);
 	WARN_ON(start + len > eb->start + eb->len);
 
-	offset = start & ((unsigned long)PAGE_CACHE_SIZE - 1);
-	if (i == 0)
-		offset += start_offset;
+	offset = (start_offset + start) & ((unsigned long)PAGE_CACHE_SIZE - 1);
 
 	while(len > 0) {
 		page = extent_buffer_page(eb, i);
@@ -2514,9 +2513,7 @@ void write_extent_buffer(struct extent_buffer *eb, const void *srcv,
 	WARN_ON(start > eb->len);
 	WARN_ON(start + len > eb->start + eb->len);
 
-	offset = start & ((unsigned long)PAGE_CACHE_SIZE - 1);
-	if (i == 0)
-		offset += start_offset;
+	offset = (start_offset + start) & ((unsigned long)PAGE_CACHE_SIZE - 1);
 
 	while(len > 0) {
 		page = extent_buffer_page(eb, i);
@@ -2548,9 +2545,7 @@ void memset_extent_buffer(struct extent_buffer *eb, char c,
 	WARN_ON(start > eb->len);
 	WARN_ON(start + len > eb->start + eb->len);
 
-	offset = start & ((unsigned long)PAGE_CACHE_SIZE - 1);
-	if (i == 0)
-		offset += start_offset;
+	offset = (start_offset + start) & ((unsigned long)PAGE_CACHE_SIZE - 1);
 
 	while(len > 0) {
 		page = extent_buffer_page(eb, i);
@@ -2582,9 +2577,8 @@ void copy_extent_buffer(struct extent_buffer *dst, struct extent_buffer *src,
 
 	WARN_ON(src->len != dst_len);
 
-	offset = dst_offset & ((unsigned long)PAGE_CACHE_SIZE - 1);
-	if (i == 0)
-		offset += start_offset;
+	offset = (start_offset + dst_offset) &
+		((unsigned long)PAGE_CACHE_SIZE - 1);
 
 	while(len > 0) {
 		page = extent_buffer_page(dst, i);
@@ -2664,19 +2658,14 @@ void memcpy_extent_buffer(struct extent_buffer *dst, unsigned long dst_offset,
 	}
 
 	while(len > 0) {
-		dst_off_in_page = dst_offset &
+		dst_off_in_page = (start_offset + dst_offset) &
 			((unsigned long)PAGE_CACHE_SIZE - 1);
-		src_off_in_page = src_offset &
+		src_off_in_page = (start_offset + src_offset) &
 			((unsigned long)PAGE_CACHE_SIZE - 1);
 
 		dst_i = (start_offset + dst_offset) >> PAGE_CACHE_SHIFT;
 		src_i = (start_offset + src_offset) >> PAGE_CACHE_SHIFT;
 
-		if (src_i == 0)
-			src_off_in_page += start_offset;
-		if (dst_i == 0)
-			dst_off_in_page += start_offset;
-
 		cur = min(len, (unsigned long)(PAGE_CACHE_SIZE -
 					       src_off_in_page));
 		cur = min_t(unsigned long, cur,
@@ -2723,14 +2712,10 @@ void memmove_extent_buffer(struct extent_buffer *dst, unsigned long dst_offset,
 		dst_i = (start_offset + dst_end) >> PAGE_CACHE_SHIFT;
 		src_i = (start_offset + src_end) >> PAGE_CACHE_SHIFT;
 
-		dst_off_in_page = dst_end &
+		dst_off_in_page = (start_offset + dst_end) &
 			((unsigned long)PAGE_CACHE_SIZE - 1);
-		src_off_in_page = src_end &
+		src_off_in_page = (start_offset + src_end) &
 			((unsigned long)PAGE_CACHE_SIZE - 1);
-		if (src_i == 0)
-			src_off_in_page += start_offset;
-		if (dst_i == 0)
-			dst_off_in_page += start_offset;
 
 		cur = min_t(unsigned long, len, src_off_in_page + 1);
 		cur = min(cur, dst_off_in_page + 1);

commit 59d169e2b3d69ba0beca0fa0936789080c8d61a9
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Oct 19 09:23:09 2007 -0400

    Btrfs: Fix read/write_extent_buffer to use KM_USER1 instead of KM_USER0
    
    This avoids recursive use of KM_USER0 during btrfs_file_write
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index c0f834563ced..3c81f5eab155 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -2378,9 +2378,9 @@ void read_extent_buffer(struct extent_buffer *eb, void *dstv,
 		WARN_ON(!PageUptodate(page));
 
 		cur = min(len, (PAGE_CACHE_SIZE - offset));
-		kaddr = kmap_atomic(page, KM_USER0);
+		kaddr = kmap_atomic(page, KM_USER1);
 		memcpy(dst, kaddr + offset, cur);
-		kunmap_atomic(kaddr, KM_USER0);
+		kunmap_atomic(kaddr, KM_USER1);
 
 		dst += cur;
 		len -= cur;
@@ -2523,9 +2523,9 @@ void write_extent_buffer(struct extent_buffer *eb, const void *srcv,
 		WARN_ON(!PageUptodate(page));
 
 		cur = min(len, PAGE_CACHE_SIZE - offset);
-		kaddr = kmap_atomic(page, KM_USER0);
+		kaddr = kmap_atomic(page, KM_USER1);
 		memcpy(kaddr + offset, src, cur);
-		kunmap_atomic(kaddr, KM_USER0);
+		kunmap_atomic(kaddr, KM_USER1);
 
 		src += cur;
 		len -= cur;

commit 0a2118dfd4c328e8823cdcfebf839dff73ac21e9
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Fri Oct 19 09:23:05 2007 -0400

    Btrfs: Fix bi_end_io() functions on > 2.6.23 kernels
    
    It now returns void and it is never called for partial completions, so
    the bio->bi_size check must go.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 89d0ebd0ea83..c0f834563ced 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -9,6 +9,7 @@
 #include <linux/spinlock.h>
 #include <linux/blkdev.h>
 #include <linux/swap.h>
+#include <linux/version.h>
 #include "extent_map.h"
 
 /* temporary define until extent_map moves out of btrfs */
@@ -1274,8 +1275,12 @@ static int check_page_writeback(struct extent_map_tree *tree,
  * Scheduling is not allowed, so the extent state tree is expected
  * to have one and only one object corresponding to this IO.
  */
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2,6,23)
+static void end_bio_extent_writepage(struct bio *bio, int err)
+#else
 static int end_bio_extent_writepage(struct bio *bio,
 				   unsigned int bytes_done, int err)
+#endif
 {
 	const int uptodate = test_bit(BIO_UPTODATE, &bio->bi_flags);
 	struct bio_vec *bvec = bio->bi_io_vec + bio->bi_vcnt - 1;
@@ -1284,8 +1289,10 @@ static int end_bio_extent_writepage(struct bio *bio,
 	u64 end;
 	int whole_page;
 
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,23)
 	if (bio->bi_size)
 		return 1;
+#endif
 
 	do {
 		struct page *page = bvec->bv_page;
@@ -1316,7 +1323,9 @@ static int end_bio_extent_writepage(struct bio *bio,
 	} while (bvec >= bio->bi_io_vec);
 
 	bio_put(bio);
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,23)
 	return 0;
+#endif
 }
 
 /*
@@ -1330,8 +1339,12 @@ static int end_bio_extent_writepage(struct bio *bio,
  * Scheduling is not allowed, so the extent state tree is expected
  * to have one and only one object corresponding to this IO.
  */
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2,6,23)
+static void end_bio_extent_readpage(struct bio *bio, int err)
+#else
 static int end_bio_extent_readpage(struct bio *bio,
 				   unsigned int bytes_done, int err)
+#endif
 {
 	int uptodate = test_bit(BIO_UPTODATE, &bio->bi_flags);
 	struct bio_vec *bvec = bio->bi_io_vec + bio->bi_vcnt - 1;
@@ -1341,8 +1354,10 @@ static int end_bio_extent_readpage(struct bio *bio,
 	int whole_page;
 	int ret;
 
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,23)
 	if (bio->bi_size)
 		return 1;
+#endif
 
 	do {
 		struct page *page = bvec->bv_page;
@@ -1382,7 +1397,9 @@ static int end_bio_extent_readpage(struct bio *bio,
 	} while (bvec >= bio->bi_io_vec);
 
 	bio_put(bio);
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,23)
 	return 0;
+#endif
 }
 
 /*
@@ -1390,8 +1407,12 @@ static int end_bio_extent_readpage(struct bio *bio,
  * the structs in the extent tree when done, and set the uptodate bits
  * as appropriate.
  */
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2,6,23)
+static void end_bio_extent_preparewrite(struct bio *bio, int err)
+#else
 static int end_bio_extent_preparewrite(struct bio *bio,
 				       unsigned int bytes_done, int err)
+#endif
 {
 	const int uptodate = test_bit(BIO_UPTODATE, &bio->bi_flags);
 	struct bio_vec *bvec = bio->bi_io_vec + bio->bi_vcnt - 1;
@@ -1399,8 +1420,10 @@ static int end_bio_extent_preparewrite(struct bio *bio,
 	u64 start;
 	u64 end;
 
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,23)
 	if (bio->bi_size)
 		return 1;
+#endif
 
 	do {
 		struct page *page = bvec->bv_page;
@@ -1422,7 +1445,9 @@ static int end_bio_extent_preparewrite(struct bio *bio,
 	} while (bvec >= bio->bi_io_vec);
 
 	bio_put(bio);
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,23)
 	return 0;
+#endif
 }
 
 static int submit_extent_page(int rw, struct extent_map_tree *tree,

commit ae2f5411c4ce7180cca8418853db50c8e52d40db
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Fri Oct 19 09:22:59 2007 -0400

    btrfs: 32-bit type problems
    
    An assorted set of casts to get rid of the warnings on 32-bit archs.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index e87e476dca92..89d0ebd0ea83 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -2654,8 +2654,8 @@ void memcpy_extent_buffer(struct extent_buffer *dst, unsigned long dst_offset,
 
 		cur = min(len, (unsigned long)(PAGE_CACHE_SIZE -
 					       src_off_in_page));
-		cur = min(cur, (unsigned long)(PAGE_CACHE_SIZE -
-					       dst_off_in_page));
+		cur = min_t(unsigned long, cur,
+			(unsigned long)(PAGE_CACHE_SIZE - dst_off_in_page));
 
 		copy_pages(extent_buffer_page(dst, dst_i),
 			   extent_buffer_page(dst, src_i),
@@ -2707,7 +2707,7 @@ void memmove_extent_buffer(struct extent_buffer *dst, unsigned long dst_offset,
 		if (dst_i == 0)
 			dst_off_in_page += start_offset;
 
-		cur = min(len, src_off_in_page + 1);
+		cur = min_t(unsigned long, len, src_off_in_page + 1);
 		cur = min(cur, dst_off_in_page + 1);
 		move_pages(extent_buffer_page(dst, dst_i),
 			   extent_buffer_page(dst, src_i),

commit ff79f8190b6e955ff7a71faf804a3017d526e657
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Oct 15 16:22:25 2007 -0400

    Btrfs: Add back file data checksumming
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 2a8bc4bd43a9..e87e476dca92 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -2031,6 +2031,7 @@ struct extent_buffer *alloc_extent_buffer(struct extent_map_tree *tree,
 		i = 1;
 		index++;
 		page_cache_get(page0);
+		mark_page_accessed(page0);
 		set_page_extent_mapped(page0);
 		set_page_private(page0, EXTENT_PAGE_PRIVATE_FIRST_PAGE |
 				 len << 2);
@@ -2049,6 +2050,7 @@ struct extent_buffer *alloc_extent_buffer(struct extent_map_tree *tree,
 			goto fail;
 		}
 		set_page_extent_mapped(p);
+		mark_page_accessed(p);
 		if (i == 0) {
 			eb->first_page = p;
 			set_page_private(p, EXTENT_PAGE_PRIVATE_FIRST_PAGE |
@@ -2099,6 +2101,7 @@ struct extent_buffer *find_extent_buffer(struct extent_map_tree *tree,
 			goto fail;
 		}
 		set_page_extent_mapped(p);
+		mark_page_accessed(p);
 
 		if (i == 0) {
 			eb->first_page = p;

commit 19c00ddcc31ad4bdfb86b57085e06d6135b9b1d7
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Oct 15 16:19:22 2007 -0400

    Btrfs: Add back metadata checksumming
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index f8aaba8a30a2..2a8bc4bd43a9 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -81,7 +81,7 @@ void extent_map_tree_init(struct extent_map_tree *tree,
 }
 EXPORT_SYMBOL(extent_map_tree_init);
 
-void extent_map_tree_cleanup(struct extent_map_tree *tree)
+void extent_map_tree_empty_lru(struct extent_map_tree *tree)
 {
 	struct extent_buffer *eb;
 	while(!list_empty(&tree->buffer_lru)) {
@@ -91,7 +91,7 @@ void extent_map_tree_cleanup(struct extent_map_tree *tree)
 		free_extent_buffer(eb);
 	}
 }
-EXPORT_SYMBOL(extent_map_tree_cleanup);
+EXPORT_SYMBOL(extent_map_tree_empty_lru);
 
 struct extent_map *alloc_extent_map(gfp_t mask)
 {
@@ -1464,7 +1464,7 @@ void set_page_extent_mapped(struct page *page)
 	if (!PagePrivate(page)) {
 		SetPagePrivate(page);
 		WARN_ON(!page->mapping->a_ops->invalidatepage);
-		set_page_private(page, 1);
+		set_page_private(page, EXTENT_PAGE_PRIVATE);
 		page_cache_get(page);
 	}
 }
@@ -1979,8 +1979,9 @@ static struct extent_buffer *__alloc_extent_buffer(struct extent_map_tree *tree,
 
 	spin_lock(&tree->lru_lock);
 	eb = find_lru(tree, start, len);
-	if (eb)
+	if (eb) {
 		goto lru_add;
+	}
 	spin_unlock(&tree->lru_lock);
 
 	if (eb) {
@@ -2007,6 +2008,7 @@ static void __free_extent_buffer(struct extent_buffer *eb)
 
 struct extent_buffer *alloc_extent_buffer(struct extent_map_tree *tree,
 					  u64 start, unsigned long len,
+					  struct page *page0,
 					  gfp_t mask)
 {
 	unsigned long num_pages = num_extent_pages(start, len);
@@ -2024,7 +2026,18 @@ struct extent_buffer *alloc_extent_buffer(struct extent_map_tree *tree,
 	if (eb->flags & EXTENT_BUFFER_FILLED)
 		return eb;
 
-	for (i = 0; i < num_pages; i++, index++) {
+	if (page0) {
+		eb->first_page = page0;
+		i = 1;
+		index++;
+		page_cache_get(page0);
+		set_page_extent_mapped(page0);
+		set_page_private(page0, EXTENT_PAGE_PRIVATE_FIRST_PAGE |
+				 len << 2);
+	} else {
+		i = 0;
+	}
+	for (; i < num_pages; i++, index++) {
 		p = find_or_create_page(mapping, index, mask | __GFP_HIGHMEM);
 		if (!p) {
 			WARN_ON(1);
@@ -2036,8 +2049,13 @@ struct extent_buffer *alloc_extent_buffer(struct extent_map_tree *tree,
 			goto fail;
 		}
 		set_page_extent_mapped(p);
-		if (i == 0)
+		if (i == 0) {
 			eb->first_page = p;
+			set_page_private(p, EXTENT_PAGE_PRIVATE_FIRST_PAGE |
+					 len << 2);
+		} else {
+			set_page_private(p, EXTENT_PAGE_PRIVATE);
+		}
 		if (!PageUptodate(p))
 			uptodate = 0;
 		unlock_page(p);
@@ -2057,8 +2075,7 @@ struct extent_buffer *find_extent_buffer(struct extent_map_tree *tree,
 					  gfp_t mask)
 {
 	unsigned long num_pages = num_extent_pages(start, len);
-	unsigned long i;
-	unsigned long index = start >> PAGE_CACHE_SHIFT;
+	unsigned long i; unsigned long index = start >> PAGE_CACHE_SHIFT;
 	struct extent_buffer *eb;
 	struct page *p;
 	struct address_space *mapping = tree->mapping;
@@ -2082,8 +2099,15 @@ struct extent_buffer *find_extent_buffer(struct extent_map_tree *tree,
 			goto fail;
 		}
 		set_page_extent_mapped(p);
-		if (i == 0)
+
+		if (i == 0) {
 			eb->first_page = p;
+			set_page_private(p, EXTENT_PAGE_PRIVATE_FIRST_PAGE |
+					 len << 2);
+		} else {
+			set_page_private(p, EXTENT_PAGE_PRIVATE);
+		}
+
 		if (!PageUptodate(p))
 			uptodate = 0;
 		unlock_page(p);
@@ -2174,7 +2198,21 @@ int set_extent_buffer_dirty(struct extent_map_tree *tree,
 
 	num_pages = num_extent_pages(eb->start, eb->len);
 	for (i = 0; i < num_pages; i++) {
+		struct page *page = extent_buffer_page(eb, i);
+		/* writepage may need to do something special for the
+		 * first page, we have to make sure page->private is
+		 * properly set.  releasepage may drop page->private
+		 * on us if the page isn't already dirty.
+		 */
+		if (i == 0) {
+			lock_page(page);
+			set_page_private(page,
+					 EXTENT_PAGE_PRIVATE_FIRST_PAGE |
+					 eb->len << 2);
+		}
 		__set_page_dirty_nobuffers(extent_buffer_page(eb, i));
+		if (i == 0)
+			unlock_page(page);
 	}
 	return set_extent_dirty(tree, eb->start,
 				eb->start + eb->len - 1, GFP_NOFS);
@@ -2217,9 +2255,12 @@ int extent_buffer_uptodate(struct extent_map_tree *tree,
 EXPORT_SYMBOL(extent_buffer_uptodate);
 
 int read_extent_buffer_pages(struct extent_map_tree *tree,
-			     struct extent_buffer *eb, int wait)
+			     struct extent_buffer *eb,
+			     u64 start,
+			     int wait)
 {
 	unsigned long i;
+	unsigned long start_i;
 	struct page *page;
 	int err;
 	int ret = 0;
@@ -2232,9 +2273,16 @@ int read_extent_buffer_pages(struct extent_map_tree *tree,
 			   EXTENT_UPTODATE, 1)) {
 		return 0;
 	}
+	if (start) {
+		WARN_ON(start < eb->start);
+		start_i = (start >> PAGE_CACHE_SHIFT) -
+			(eb->start >> PAGE_CACHE_SHIFT);
+	} else {
+		start_i = 0;
+	}
 
 	num_pages = num_extent_pages(eb->start, eb->len);
-	for (i = 0; i < num_pages; i++) {
+	for (i = start_i; i < num_pages; i++) {
 		page = extent_buffer_page(eb, i);
 		if (PageUptodate(page)) {
 			continue;
@@ -2260,7 +2308,7 @@ int read_extent_buffer_pages(struct extent_map_tree *tree,
 		return ret;
 	}
 
-	for (i = 0; i < num_pages; i++) {
+	for (i = start_i; i < num_pages; i++) {
 		page = extent_buffer_page(eb, i);
 		wait_on_page_locked(page);
 		if (!PageUptodate(page)) {
@@ -2314,7 +2362,7 @@ void read_extent_buffer(struct extent_buffer *eb, void *dstv,
 }
 EXPORT_SYMBOL(read_extent_buffer);
 
-static int __map_extent_buffer(struct extent_buffer *eb, unsigned long start,
+int map_private_extent_buffer(struct extent_buffer *eb, unsigned long start,
 			       unsigned long min_len, char **token, char **map,
 			       unsigned long *map_start,
 			       unsigned long *map_len, int km)
@@ -2337,6 +2385,10 @@ static int __map_extent_buffer(struct extent_buffer *eb, unsigned long start,
 		offset = 0;
 		*map_start = (i << PAGE_CACHE_SHIFT) - start_offset;
 	}
+	if (start + min_len >= eb->len) {
+printk("bad mapping eb start %Lu len %lu, wanted %lu %lu\n", eb->start, eb->len, start, min_len);
+		WARN_ON(1);
+	}
 
 	p = extent_buffer_page(eb, i);
 	WARN_ON(!PageUptodate(p));
@@ -2346,6 +2398,7 @@ static int __map_extent_buffer(struct extent_buffer *eb, unsigned long start,
 	*map_len = PAGE_CACHE_SIZE - offset;
 	return 0;
 }
+EXPORT_SYMBOL(map_private_extent_buffer);
 
 int map_extent_buffer(struct extent_buffer *eb, unsigned long start,
 		      unsigned long min_len,
@@ -2360,8 +2413,8 @@ int map_extent_buffer(struct extent_buffer *eb, unsigned long start,
 		eb->map_token = NULL;
 		save = 1;
 	}
-	err = __map_extent_buffer(eb, start, min_len, token, map,
-				   map_start, map_len, km);
+	err = map_private_extent_buffer(eb, start, min_len, token, map,
+				       map_start, map_len, km);
 	if (!err && save) {
 		eb->map_token = *token;
 		eb->kaddr = *map;

commit 810191ff3087e8143b41a944fcf4fd8c693f00e3
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Oct 15 16:18:55 2007 -0400

    Btrfs: extent_map optimizations to cut down on CPU usage
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 85b28a6a4e05..f8aaba8a30a2 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -1963,9 +1963,9 @@ static inline struct page *extent_buffer_page(struct extent_buffer *eb,
 	struct page *p;
 
 	if (i == 0)
-		return eb->last_page;
+		return eb->first_page;
 	i += eb->start >> PAGE_CACHE_SHIFT;
-	p = find_get_page(eb->last_page->mapping, i);
+	p = find_get_page(eb->first_page->mapping, i);
 	page_cache_release(p);
 	return p;
 }
@@ -2037,7 +2037,7 @@ struct extent_buffer *alloc_extent_buffer(struct extent_map_tree *tree,
 		}
 		set_page_extent_mapped(p);
 		if (i == 0)
-			eb->last_page = p;
+			eb->first_page = p;
 		if (!PageUptodate(p))
 			uptodate = 0;
 		unlock_page(p);
@@ -2083,7 +2083,7 @@ struct extent_buffer *find_extent_buffer(struct extent_map_tree *tree,
 		}
 		set_page_extent_mapped(p);
 		if (i == 0)
-			eb->last_page = p;
+			eb->first_page = p;
 		if (!PageUptodate(p))
 			uptodate = 0;
 		unlock_page(p);
@@ -2169,7 +2169,15 @@ EXPORT_SYMBOL(wait_on_extent_buffer_writeback);
 int set_extent_buffer_dirty(struct extent_map_tree *tree,
 			     struct extent_buffer *eb)
 {
-	return set_range_dirty(tree, eb->start, eb->start + eb->len - 1);
+	unsigned long i;
+	unsigned long num_pages;
+
+	num_pages = num_extent_pages(eb->start, eb->len);
+	for (i = 0; i < num_pages; i++) {
+		__set_page_dirty_nobuffers(extent_buffer_page(eb, i));
+	}
+	return set_extent_dirty(tree, eb->start,
+				eb->start + eb->len - 1, GFP_NOFS);
 }
 EXPORT_SYMBOL(set_extent_buffer_dirty);
 
@@ -2317,16 +2325,11 @@ static int __map_extent_buffer(struct extent_buffer *eb, unsigned long start,
 	size_t start_offset = eb->start & ((u64)PAGE_CACHE_SIZE - 1);
 	unsigned long i = (start_offset + start) >> PAGE_CACHE_SHIFT;
 	unsigned long end_i = (start_offset + start + min_len) >>
-				PAGE_CACHE_SHIFT;
+		PAGE_CACHE_SHIFT;
 
 	if (i != end_i)
 		return -EINVAL;
 
-	if (start >= eb->len) {
-		printk("bad start in map eb start %Lu len %lu caller start %lu min %lu\n", eb->start, eb->len, start, min_len);
-		WARN_ON(1);
-	}
-
 	if (i == 0) {
 		offset = start_offset;
 		*map_start = 0;
@@ -2353,14 +2356,6 @@ int map_extent_buffer(struct extent_buffer *eb, unsigned long start,
 	int err;
 	int save = 0;
 	if (eb->map_token) {
-		if (start >= eb->map_start &&
-		    start + min_len <= eb->map_start + eb->map_len) {
-			*token = eb->map_token;
-			*map = eb->kaddr;
-			*map_start = eb->map_start;
-			*map_len = eb->map_len;
-			return 0;
-		}
 		unmap_extent_buffer(eb, eb->map_token, km);
 		eb->map_token = NULL;
 		save = 1;

commit 4dc119046d0d8501afa4346472917fb05586ad9c
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Oct 15 16:18:14 2007 -0400

    Btrfs: Add an extent buffer LRU to reduce radix tree hits
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index e241699024da..85b28a6a4e05 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -8,6 +8,7 @@
 #include <linux/module.h>
 #include <linux/spinlock.h>
 #include <linux/blkdev.h>
+#include <linux/swap.h>
 #include "extent_map.h"
 
 /* temporary define until extent_map moves out of btrfs */
@@ -20,14 +21,11 @@ static struct kmem_cache *extent_map_cache;
 static struct kmem_cache *extent_state_cache;
 static struct kmem_cache *extent_buffer_cache;
 
-static LIST_HEAD(extent_buffers);
 static LIST_HEAD(buffers);
 static LIST_HEAD(states);
 
-static spinlock_t extent_buffers_lock;
 static spinlock_t state_lock = SPIN_LOCK_UNLOCKED;
-static int nr_extent_buffers;
-#define MAX_EXTENT_BUFFER_CACHE 128
+#define BUFFER_LRU_MAX 64
 
 struct tree_entry {
 	u64 start;
@@ -47,20 +45,12 @@ void __init extent_map_init(void)
 	extent_buffer_cache = btrfs_cache_create("extent_buffers",
 					    sizeof(struct extent_buffer), 0,
 					    NULL);
-	spin_lock_init(&extent_buffers_lock);
 }
 
 void __exit extent_map_exit(void)
 {
-	struct extent_buffer *eb;
 	struct extent_state *state;
 
-	while (!list_empty(&extent_buffers)) {
-		eb = list_entry(extent_buffers.next,
-				struct extent_buffer, list);
-		list_del(&eb->list);
-		kmem_cache_free(extent_buffer_cache, eb);
-	}
 	while (!list_empty(&states)) {
 		state = list_entry(states.next, struct extent_state, list);
 		printk("state leak: start %Lu end %Lu state %lu in tree %d refs %d\n", state->start, state->end, state->state, state->in_tree, atomic_read(&state->refs));
@@ -68,14 +58,6 @@ void __exit extent_map_exit(void)
 		kmem_cache_free(extent_state_cache, state);
 
 	}
-	while (!list_empty(&buffers)) {
-		eb = list_entry(buffers.next,
-				struct extent_buffer, leak_list);
-		printk("buffer leak start %Lu len %lu return %lX\n", eb->start, eb->len, eb->alloc_addr);
-		list_del(&eb->leak_list);
-		kmem_cache_free(extent_buffer_cache, eb);
-	}
-
 
 	if (extent_map_cache)
 		kmem_cache_destroy(extent_map_cache);
@@ -92,10 +74,25 @@ void extent_map_tree_init(struct extent_map_tree *tree,
 	tree->state.rb_node = NULL;
 	tree->ops = NULL;
 	rwlock_init(&tree->lock);
+	spin_lock_init(&tree->lru_lock);
 	tree->mapping = mapping;
+	INIT_LIST_HEAD(&tree->buffer_lru);
+	tree->lru_size = 0;
 }
 EXPORT_SYMBOL(extent_map_tree_init);
 
+void extent_map_tree_cleanup(struct extent_map_tree *tree)
+{
+	struct extent_buffer *eb;
+	while(!list_empty(&tree->buffer_lru)) {
+		eb = list_entry(tree->buffer_lru.next, struct extent_buffer,
+				lru);
+		list_del(&eb->lru);
+		free_extent_buffer(eb);
+	}
+}
+EXPORT_SYMBOL(extent_map_tree_cleanup);
+
 struct extent_map *alloc_extent_map(gfp_t mask)
 {
 	struct extent_map *em;
@@ -1915,66 +1912,99 @@ sector_t extent_bmap(struct address_space *mapping, sector_t iblock,
 	return (em->block_start + start - em->start) >> inode->i_blkbits;
 }
 
-static struct extent_buffer *__alloc_extent_buffer(gfp_t mask)
+static int add_lru(struct extent_map_tree *tree, struct extent_buffer *eb)
 {
-	struct extent_buffer *eb = NULL;
-
-	spin_lock(&extent_buffers_lock);
-	if (!list_empty(&extent_buffers)) {
-		eb = list_entry(extent_buffers.next, struct extent_buffer,
-				list);
-		list_del(&eb->list);
-		WARN_ON(nr_extent_buffers == 0);
-		nr_extent_buffers--;
-	}
-	spin_unlock(&extent_buffers_lock);
+	if (list_empty(&eb->lru)) {
+		extent_buffer_get(eb);
+		list_add(&eb->lru, &tree->buffer_lru);
+		tree->lru_size++;
+		if (tree->lru_size >= BUFFER_LRU_MAX) {
+			struct extent_buffer *rm;
+			rm = list_entry(tree->buffer_lru.prev,
+					struct extent_buffer, lru);
+			tree->lru_size--;
+			list_del(&rm->lru);
+			free_extent_buffer(rm);
+		}
+	} else
+		list_move(&eb->lru, &tree->buffer_lru);
+	return 0;
+}
+static struct extent_buffer *find_lru(struct extent_map_tree *tree,
+				      u64 start, unsigned long len)
+{
+	struct list_head *lru = &tree->buffer_lru;
+	struct list_head *cur = lru->next;
+	struct extent_buffer *eb;
 
-	if (eb) {
-		memset(eb, 0, sizeof(*eb));
-	} else {
-		eb = kmem_cache_zalloc(extent_buffer_cache, mask);
-	}
-	spin_lock(&extent_buffers_lock);
-	list_add(&eb->leak_list, &buffers);
-	spin_unlock(&extent_buffers_lock);
+	if (list_empty(lru))
+		return NULL;
 
-	return eb;
+	do {
+		eb = list_entry(cur, struct extent_buffer, lru);
+		if (eb->start == start && eb->len == len) {
+			extent_buffer_get(eb);
+			return eb;
+		}
+		cur = cur->next;
+	} while (cur != lru);
+	return NULL;
 }
 
-static void __free_extent_buffer(struct extent_buffer *eb)
+static inline unsigned long num_extent_pages(u64 start, u64 len)
 {
-
-	spin_lock(&extent_buffers_lock);
-	list_del_init(&eb->leak_list);
-	spin_unlock(&extent_buffers_lock);
-
-	if (nr_extent_buffers >= MAX_EXTENT_BUFFER_CACHE) {
-		kmem_cache_free(extent_buffer_cache, eb);
-	} else {
-		spin_lock(&extent_buffers_lock);
-		list_add(&eb->list, &extent_buffers);
-		nr_extent_buffers++;
-		spin_unlock(&extent_buffers_lock);
-	}
+	return ((start + len + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT) -
+		(start >> PAGE_CACHE_SHIFT);
 }
 
-static inline struct page *extent_buffer_page(struct extent_buffer *eb, int i)
+static inline struct page *extent_buffer_page(struct extent_buffer *eb,
+					      unsigned long i)
 {
 	struct page *p;
 
-	if (i < EXTENT_INLINE_PAGES)
-		return eb->pages[i];
+	if (i == 0)
+		return eb->last_page;
 	i += eb->start >> PAGE_CACHE_SHIFT;
-	p = find_get_page(eb->pages[0]->mapping, i);
+	p = find_get_page(eb->last_page->mapping, i);
 	page_cache_release(p);
 	return p;
 }
 
-static inline unsigned long num_extent_pages(u64 start, u64 len)
+static struct extent_buffer *__alloc_extent_buffer(struct extent_map_tree *tree,
+						   u64 start,
+						   unsigned long len,
+						   gfp_t mask)
 {
-	return ((start + len + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT) -
-		(start >> PAGE_CACHE_SHIFT);
+	struct extent_buffer *eb = NULL;
+
+	spin_lock(&tree->lru_lock);
+	eb = find_lru(tree, start, len);
+	if (eb)
+		goto lru_add;
+	spin_unlock(&tree->lru_lock);
+
+	if (eb) {
+		memset(eb, 0, sizeof(*eb));
+	} else {
+		eb = kmem_cache_zalloc(extent_buffer_cache, mask);
+	}
+	INIT_LIST_HEAD(&eb->lru);
+	eb->start = start;
+	eb->len = len;
+	atomic_set(&eb->refs, 1);
+
+	spin_lock(&tree->lru_lock);
+lru_add:
+	add_lru(tree, eb);
+	spin_unlock(&tree->lru_lock);
+	return eb;
+}
+
+static void __free_extent_buffer(struct extent_buffer *eb)
+{
+	kmem_cache_free(extent_buffer_cache, eb);
 }
+
 struct extent_buffer *alloc_extent_buffer(struct extent_map_tree *tree,
 					  u64 start, unsigned long len,
 					  gfp_t mask)
@@ -1987,14 +2017,12 @@ struct extent_buffer *alloc_extent_buffer(struct extent_map_tree *tree,
 	struct address_space *mapping = tree->mapping;
 	int uptodate = 0;
 
-	eb = __alloc_extent_buffer(mask);
+	eb = __alloc_extent_buffer(tree, start, len, mask);
 	if (!eb || IS_ERR(eb))
 		return NULL;
 
-	eb->alloc_addr = (unsigned long)__builtin_return_address(0);
-	eb->start = start;
-	eb->len = len;
-	atomic_set(&eb->refs, 1);
+	if (eb->flags & EXTENT_BUFFER_FILLED)
+		return eb;
 
 	for (i = 0; i < num_pages; i++, index++) {
 		p = find_or_create_page(mapping, index, mask | __GFP_HIGHMEM);
@@ -2008,14 +2036,15 @@ struct extent_buffer *alloc_extent_buffer(struct extent_map_tree *tree,
 			goto fail;
 		}
 		set_page_extent_mapped(p);
-		if (i < EXTENT_INLINE_PAGES)
-			eb->pages[i] = p;
+		if (i == 0)
+			eb->last_page = p;
 		if (!PageUptodate(p))
 			uptodate = 0;
 		unlock_page(p);
 	}
 	if (uptodate)
 		eb->flags |= EXTENT_UPTODATE;
+	eb->flags |= EXTENT_BUFFER_FILLED;
 	return eb;
 fail:
 	free_extent_buffer(eb);
@@ -2035,14 +2064,12 @@ struct extent_buffer *find_extent_buffer(struct extent_map_tree *tree,
 	struct address_space *mapping = tree->mapping;
 	int uptodate = 1;
 
-	eb = __alloc_extent_buffer(mask);
+	eb = __alloc_extent_buffer(tree, start, len, mask);
 	if (!eb || IS_ERR(eb))
 		return NULL;
 
-	eb->alloc_addr = (unsigned long)__builtin_return_address(0);
-	eb->start = start;
-	eb->len = len;
-	atomic_set(&eb->refs, 1);
+	if (eb->flags & EXTENT_BUFFER_FILLED)
+		return eb;
 
 	for (i = 0; i < num_pages; i++, index++) {
 		p = find_lock_page(mapping, index);
@@ -2055,14 +2082,15 @@ struct extent_buffer *find_extent_buffer(struct extent_map_tree *tree,
 			goto fail;
 		}
 		set_page_extent_mapped(p);
-		if (i < EXTENT_INLINE_PAGES)
-			eb->pages[i] = p;
+		if (i == 0)
+			eb->last_page = p;
 		if (!PageUptodate(p))
 			uptodate = 0;
 		unlock_page(p);
 	}
 	if (uptodate)
 		eb->flags |= EXTENT_UPTODATE;
+	eb->flags |= EXTENT_BUFFER_FILLED;
 	return eb;
 fail:
 	free_extent_buffer(eb);
@@ -2231,7 +2259,8 @@ int read_extent_buffer_pages(struct extent_map_tree *tree,
 			ret = -EIO;
 		}
 	}
-	eb->flags |= EXTENT_UPTODATE;
+	if (!ret)
+		eb->flags |= EXTENT_UPTODATE;
 	return ret;
 }
 EXPORT_SYMBOL(read_extent_buffer_pages);

commit e19caa5f0e34b571ed0c2617554af5c43cb124d1
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Oct 15 16:17:44 2007 -0400

    Btrfs: Fix allocation routines to avoid intermixing data and metadata allocations
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 7ef3397a266d..e241699024da 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -981,7 +981,7 @@ int find_first_extent_bit(struct extent_map_tree *tree, u64 start,
 	struct extent_state *state;
 	int ret = 1;
 
-	write_lock_irq(&tree->lock);
+	read_lock_irq(&tree->lock);
 	/*
 	 * this search will find all the extents that end after
 	 * our range starts.
@@ -993,7 +993,7 @@ int find_first_extent_bit(struct extent_map_tree *tree, u64 start,
 
 	while(1) {
 		state = rb_entry(node, struct extent_state, rb_node);
-		if (state->state & bits) {
+		if (state->end >= start && (state->state & bits)) {
 			*start_ret = state->start;
 			*end_ret = state->end;
 			ret = 0;
@@ -1004,7 +1004,7 @@ int find_first_extent_bit(struct extent_map_tree *tree, u64 start,
 			break;
 	}
 out:
-	write_unlock_irq(&tree->lock);
+	read_unlock_irq(&tree->lock);
 	return ret;
 }
 EXPORT_SYMBOL(find_first_extent_bit);

commit 09e71a326341f40111400c88aaf0498ef622824b
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Oct 15 16:17:04 2007 -0400

    Btrfs: Use an array of pages in the extent buffers to reduce the cost of find_get_page
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index ea6ee68ef53c..7ef3397a266d 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -1961,16 +1961,12 @@ static void __free_extent_buffer(struct extent_buffer *eb)
 static inline struct page *extent_buffer_page(struct extent_buffer *eb, int i)
 {
 	struct page *p;
-	if (i == 0)
-		return eb->first_page;
 
+	if (i < EXTENT_INLINE_PAGES)
+		return eb->pages[i];
 	i += eb->start >> PAGE_CACHE_SHIFT;
-	if (eb->last_page && eb->last_page->index == i)
-		return eb->last_page;
-
-	p = find_get_page(eb->first_page->mapping, i);
+	p = find_get_page(eb->pages[0]->mapping, i);
 	page_cache_release(p);
-	eb->last_page = p;
 	return p;
 }
 
@@ -2012,8 +2008,8 @@ struct extent_buffer *alloc_extent_buffer(struct extent_map_tree *tree,
 			goto fail;
 		}
 		set_page_extent_mapped(p);
-		if (i == 0)
-			eb->first_page = p;
+		if (i < EXTENT_INLINE_PAGES)
+			eb->pages[i] = p;
 		if (!PageUptodate(p))
 			uptodate = 0;
 		unlock_page(p);
@@ -2059,8 +2055,8 @@ struct extent_buffer *find_extent_buffer(struct extent_map_tree *tree,
 			goto fail;
 		}
 		set_page_extent_mapped(p);
-		if (i == 0)
-			eb->first_page = p;
+		if (i < EXTENT_INLINE_PAGES)
+			eb->pages[i] = p;
 		if (!PageUptodate(p))
 			uptodate = 0;
 		unlock_page(p);
@@ -2087,9 +2083,7 @@ void free_extent_buffer(struct extent_buffer *eb)
 
 	num_pages = num_extent_pages(eb->start, eb->len);
 
-	if (eb->first_page)
-		page_cache_release(eb->first_page);
-	for (i = 1; i < num_pages; i++) {
+	for (i = 0; i < num_pages; i++) {
 		page_cache_release(extent_buffer_page(eb, i));
 	}
 	__free_extent_buffer(eb);

commit 14048ed0c415b8729b194e92c16d31c61628d216
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Oct 15 16:16:28 2007 -0400

    Btrfs: Cache extent buffer mappings
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index f658703c42e6..ea6ee68ef53c 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -2037,6 +2037,7 @@ struct extent_buffer *find_extent_buffer(struct extent_map_tree *tree,
 	struct extent_buffer *eb;
 	struct page *p;
 	struct address_space *mapping = tree->mapping;
+	int uptodate = 1;
 
 	eb = __alloc_extent_buffer(mask);
 	if (!eb || IS_ERR(eb))
@@ -2048,7 +2049,7 @@ struct extent_buffer *find_extent_buffer(struct extent_map_tree *tree,
 	atomic_set(&eb->refs, 1);
 
 	for (i = 0; i < num_pages; i++, index++) {
-		p = find_get_page(mapping, index);
+		p = find_lock_page(mapping, index);
 		if (!p) {
 			/* make sure the free only frees the pages we've
 			 * grabbed a reference on
@@ -2060,7 +2061,12 @@ struct extent_buffer *find_extent_buffer(struct extent_map_tree *tree,
 		set_page_extent_mapped(p);
 		if (i == 0)
 			eb->first_page = p;
+		if (!PageUptodate(p))
+			uptodate = 0;
+		unlock_page(p);
 	}
+	if (uptodate)
+		eb->flags |= EXTENT_UPTODATE;
 	return eb;
 fail:
 	free_extent_buffer(eb);
@@ -2192,7 +2198,7 @@ int read_extent_buffer_pages(struct extent_map_tree *tree,
 	if (eb->flags & EXTENT_UPTODATE)
 		return 0;
 
-	if (test_range_bit(tree, eb->start, eb->start + eb->len - 1,
+	if (0 && test_range_bit(tree, eb->start, eb->start + eb->len - 1,
 			   EXTENT_UPTODATE, 1)) {
 		return 0;
 	}
@@ -2247,6 +2253,7 @@ void read_extent_buffer(struct extent_buffer *eb, void *dstv,
 	char *dst = (char *)dstv;
 	size_t start_offset = eb->start & ((u64)PAGE_CACHE_SIZE - 1);
 	unsigned long i = (start_offset + start) >> PAGE_CACHE_SHIFT;
+	unsigned long num_pages = num_extent_pages(eb->start, eb->len);
 
 	WARN_ON(start > eb->len);
 	WARN_ON(start + len > eb->start + eb->len);
@@ -2257,6 +2264,10 @@ void read_extent_buffer(struct extent_buffer *eb, void *dstv,
 
 	while(len > 0) {
 		page = extent_buffer_page(eb, i);
+		if (!PageUptodate(page)) {
+			printk("page %lu not up to date i %lu, total %lu, len %lu\n", page->index, i, num_pages, eb->len);
+			WARN_ON(1);
+		}
 		WARN_ON(!PageUptodate(page));
 
 		cur = min(len, (PAGE_CACHE_SIZE - offset));

commit db94535db75e67fab12ccbb7f5ee548e33fed891
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Oct 15 16:15:53 2007 -0400

    Btrfs: Allow tree blocks larger than the page size
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index e081558d52ff..f658703c42e6 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -1963,18 +1963,27 @@ static inline struct page *extent_buffer_page(struct extent_buffer *eb, int i)
 	struct page *p;
 	if (i == 0)
 		return eb->first_page;
+
 	i += eb->start >> PAGE_CACHE_SHIFT;
+	if (eb->last_page && eb->last_page->index == i)
+		return eb->last_page;
+
 	p = find_get_page(eb->first_page->mapping, i);
 	page_cache_release(p);
+	eb->last_page = p;
 	return p;
 }
 
+static inline unsigned long num_extent_pages(u64 start, u64 len)
+{
+	return ((start + len + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT) -
+		(start >> PAGE_CACHE_SHIFT);
+}
 struct extent_buffer *alloc_extent_buffer(struct extent_map_tree *tree,
 					  u64 start, unsigned long len,
 					  gfp_t mask)
 {
-	unsigned long num_pages = ((start + len - 1) >> PAGE_CACHE_SHIFT) -
-				  (start >> PAGE_CACHE_SHIFT) + 1;
+	unsigned long num_pages = num_extent_pages(start, len);
 	unsigned long i;
 	unsigned long index = start >> PAGE_CACHE_SHIFT;
 	struct extent_buffer *eb;
@@ -1986,7 +1995,7 @@ struct extent_buffer *alloc_extent_buffer(struct extent_map_tree *tree,
 	if (!eb || IS_ERR(eb))
 		return NULL;
 
-	eb->alloc_addr = __builtin_return_address(0);
+	eb->alloc_addr = (unsigned long)__builtin_return_address(0);
 	eb->start = start;
 	eb->len = len;
 	atomic_set(&eb->refs, 1);
@@ -1994,6 +2003,7 @@ struct extent_buffer *alloc_extent_buffer(struct extent_map_tree *tree,
 	for (i = 0; i < num_pages; i++, index++) {
 		p = find_or_create_page(mapping, index, mask | __GFP_HIGHMEM);
 		if (!p) {
+			WARN_ON(1);
 			/* make sure the free only frees the pages we've
 			 * grabbed a reference on
 			 */
@@ -2021,8 +2031,7 @@ struct extent_buffer *find_extent_buffer(struct extent_map_tree *tree,
 					 u64 start, unsigned long len,
 					  gfp_t mask)
 {
-	unsigned long num_pages = ((start + len - 1) >> PAGE_CACHE_SHIFT) -
-				  (start >> PAGE_CACHE_SHIFT) + 1;
+	unsigned long num_pages = num_extent_pages(start, len);
 	unsigned long i;
 	unsigned long index = start >> PAGE_CACHE_SHIFT;
 	struct extent_buffer *eb;
@@ -2033,7 +2042,7 @@ struct extent_buffer *find_extent_buffer(struct extent_map_tree *tree,
 	if (!eb || IS_ERR(eb))
 		return NULL;
 
-	eb->alloc_addr = __builtin_return_address(0);
+	eb->alloc_addr = (unsigned long)__builtin_return_address(0);
 	eb->start = start;
 	eb->len = len;
 	atomic_set(&eb->refs, 1);
@@ -2070,8 +2079,7 @@ void free_extent_buffer(struct extent_buffer *eb)
 	if (!atomic_dec_and_test(&eb->refs))
 		return;
 
-	num_pages = ((eb->start + eb->len - 1) >> PAGE_CACHE_SHIFT) -
-		(eb->start >> PAGE_CACHE_SHIFT) + 1;
+	num_pages = num_extent_pages(eb->start, eb->len);
 
 	if (eb->first_page)
 		page_cache_release(eb->first_page);
@@ -2094,8 +2102,7 @@ int clear_extent_buffer_dirty(struct extent_map_tree *tree,
 	u64 end = start + eb->len - 1;
 
 	set = clear_extent_dirty(tree, start, end, GFP_NOFS);
-	num_pages = ((eb->start + eb->len - 1) >> PAGE_CACHE_SHIFT) -
-		(eb->start >> PAGE_CACHE_SHIFT) + 1;
+	num_pages = num_extent_pages(eb->start, eb->len);
 
 	for (i = 0; i < num_pages; i++) {
 		page = extent_buffer_page(eb, i);
@@ -2145,8 +2152,7 @@ int set_extent_buffer_uptodate(struct extent_map_tree *tree,
 	struct page *page;
 	unsigned long num_pages;
 
-	num_pages = ((eb->start + eb->len - 1) >> PAGE_CACHE_SHIFT) -
-		(eb->start >> PAGE_CACHE_SHIFT) + 1;
+	num_pages = num_extent_pages(eb->start, eb->len);
 
 	set_extent_uptodate(tree, eb->start, eb->start + eb->len - 1,
 			    GFP_NOFS);
@@ -2191,8 +2197,7 @@ int read_extent_buffer_pages(struct extent_map_tree *tree,
 		return 0;
 	}
 
-	num_pages = ((eb->start + eb->len - 1) >> PAGE_CACHE_SHIFT) -
-		(eb->start >> PAGE_CACHE_SHIFT) + 1;
+	num_pages = num_extent_pages(eb->start, eb->len);
 	for (i = 0; i < num_pages; i++) {
 		page = extent_buffer_page(eb, i);
 		if (PageUptodate(page)) {
@@ -2267,14 +2272,14 @@ void read_extent_buffer(struct extent_buffer *eb, void *dstv,
 }
 EXPORT_SYMBOL(read_extent_buffer);
 
-int map_extent_buffer(struct extent_buffer *eb, unsigned long start,
-		      unsigned long min_len,
-		      char **token, char **map,
-		      unsigned long *map_start,
-		      unsigned long *map_len, int km)
+static int __map_extent_buffer(struct extent_buffer *eb, unsigned long start,
+			       unsigned long min_len, char **token, char **map,
+			       unsigned long *map_start,
+			       unsigned long *map_len, int km)
 {
 	size_t offset = start & (PAGE_CACHE_SIZE - 1);
 	char *kaddr;
+	struct page *p;
 	size_t start_offset = eb->start & ((u64)PAGE_CACHE_SIZE - 1);
 	unsigned long i = (start_offset + start) >> PAGE_CACHE_SHIFT;
 	unsigned long end_i = (start_offset + start + min_len) >>
@@ -2283,21 +2288,59 @@ int map_extent_buffer(struct extent_buffer *eb, unsigned long start,
 	if (i != end_i)
 		return -EINVAL;
 
-	WARN_ON(start > eb->len);
+	if (start >= eb->len) {
+		printk("bad start in map eb start %Lu len %lu caller start %lu min %lu\n", eb->start, eb->len, start, min_len);
+		WARN_ON(1);
+	}
 
 	if (i == 0) {
 		offset = start_offset;
 		*map_start = 0;
 	} else {
+		offset = 0;
 		*map_start = (i << PAGE_CACHE_SHIFT) - start_offset;
 	}
 
-	kaddr = kmap_atomic(extent_buffer_page(eb, i), km);
+	p = extent_buffer_page(eb, i);
+	WARN_ON(!PageUptodate(p));
+	kaddr = kmap_atomic(p, km);
 	*token = kaddr;
 	*map = kaddr + offset;
 	*map_len = PAGE_CACHE_SIZE - offset;
 	return 0;
 }
+
+int map_extent_buffer(struct extent_buffer *eb, unsigned long start,
+		      unsigned long min_len,
+		      char **token, char **map,
+		      unsigned long *map_start,
+		      unsigned long *map_len, int km)
+{
+	int err;
+	int save = 0;
+	if (eb->map_token) {
+		if (start >= eb->map_start &&
+		    start + min_len <= eb->map_start + eb->map_len) {
+			*token = eb->map_token;
+			*map = eb->kaddr;
+			*map_start = eb->map_start;
+			*map_len = eb->map_len;
+			return 0;
+		}
+		unmap_extent_buffer(eb, eb->map_token, km);
+		eb->map_token = NULL;
+		save = 1;
+	}
+	err = __map_extent_buffer(eb, start, min_len, token, map,
+				   map_start, map_len, km);
+	if (!err && save) {
+		eb->map_token = *token;
+		eb->kaddr = *map;
+		eb->map_start = *map_start;
+		eb->map_len = *map_len;
+	}
+	return err;
+}
 EXPORT_SYMBOL(map_extent_buffer);
 
 void unmap_extent_buffer(struct extent_buffer *eb, char *token, int km)
@@ -2574,7 +2617,6 @@ void memmove_extent_buffer(struct extent_buffer *dst, unsigned long dst_offset,
 			((unsigned long)PAGE_CACHE_SIZE - 1);
 		src_off_in_page = src_end &
 			((unsigned long)PAGE_CACHE_SIZE - 1);
-
 		if (src_i == 0)
 			src_off_in_page += start_offset;
 		if (dst_i == 0)
@@ -2582,14 +2624,13 @@ void memmove_extent_buffer(struct extent_buffer *dst, unsigned long dst_offset,
 
 		cur = min(len, src_off_in_page + 1);
 		cur = min(cur, dst_off_in_page + 1);
-
 		move_pages(extent_buffer_page(dst, dst_i),
 			   extent_buffer_page(dst, src_i),
 			   dst_off_in_page - cur + 1,
 			   src_off_in_page - cur + 1, cur);
 
-		dst_end -= cur - 1;
-		src_end -= cur - 1;
+		dst_end -= cur;
+		src_end -= cur;
 		len -= cur;
 	}
 }

commit 1a5bc167f6707542b79a55452075525620ed43f5
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Oct 15 16:15:26 2007 -0400

    Btrfs: Change the remaining radix trees used by extent-tree.c to extent_map trees
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 1b2f9e059de6..e081558d52ff 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -1188,8 +1188,8 @@ int get_state_private(struct extent_map_tree *tree, u64 start, u64 *private)
  * has the bits set.  Otherwise, 1 is returned if any bit in the
  * range is found set.
  */
-static int test_range_bit(struct extent_map_tree *tree, u64 start, u64 end,
-			  int bits, int filled)
+int test_range_bit(struct extent_map_tree *tree, u64 start, u64 end,
+		   int bits, int filled)
 {
 	struct extent_state *state = NULL;
 	struct rb_node *node;
@@ -1222,6 +1222,7 @@ static int test_range_bit(struct extent_map_tree *tree, u64 start, u64 end,
 	read_unlock_irq(&tree->lock);
 	return bitset;
 }
+EXPORT_SYMBOL(test_range_bit);
 
 /*
  * helper function to set a given page up to date if all the

commit 96b5179d0d9b6368c203856f2ad6e8e12a8b2a2c
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Oct 15 16:15:19 2007 -0400

    Btrfs: Stop using radix trees for the block group cache
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 5b7dbcaacd11..1b2f9e059de6 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -574,7 +574,7 @@ int clear_extent_bit(struct extent_map_tree *tree, u64 start, u64 end,
 	return set;
 
 search_again:
-	if (start >= end)
+	if (start > end)
 		goto out;
 	write_unlock_irqrestore(&tree->lock, flags);
 	if (mask & __GFP_WAIT)
@@ -819,6 +819,21 @@ int set_extent_dirty(struct extent_map_tree *tree, u64 start, u64 end,
 }
 EXPORT_SYMBOL(set_extent_dirty);
 
+int set_extent_bits(struct extent_map_tree *tree, u64 start, u64 end,
+		    int bits, gfp_t mask)
+{
+	return set_extent_bit(tree, start, end, bits, 0, NULL,
+			      mask);
+}
+EXPORT_SYMBOL(set_extent_bits);
+
+int clear_extent_bits(struct extent_map_tree *tree, u64 start, u64 end,
+		      int bits, gfp_t mask)
+{
+	return clear_extent_bit(tree, start, end, bits, 0, 0, mask);
+}
+EXPORT_SYMBOL(clear_extent_bits);
+
 int set_extent_delalloc(struct extent_map_tree *tree, u64 start, u64 end,
 		     gfp_t mask)
 {
@@ -1138,7 +1153,6 @@ int set_state_private(struct extent_map_tree *tree, u64 start, u64 private)
 out:
 	write_unlock_irq(&tree->lock);
 	return ret;
-
 }
 
 int get_state_private(struct extent_map_tree *tree, u64 start, u64 *private)

commit f510cfecfc98759d75283823cfccf0cc0d59a4c6
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Oct 15 16:14:48 2007 -0400

    Btrfs: Fix extent_buffer and extent_state leaks
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index f150188f621c..5b7dbcaacd11 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -19,8 +19,13 @@ struct kmem_cache *btrfs_cache_create(const char *name, size_t size,
 static struct kmem_cache *extent_map_cache;
 static struct kmem_cache *extent_state_cache;
 static struct kmem_cache *extent_buffer_cache;
+
 static LIST_HEAD(extent_buffers);
+static LIST_HEAD(buffers);
+static LIST_HEAD(states);
+
 static spinlock_t extent_buffers_lock;
+static spinlock_t state_lock = SPIN_LOCK_UNLOCKED;
 static int nr_extent_buffers;
 #define MAX_EXTENT_BUFFER_CACHE 128
 
@@ -48,6 +53,7 @@ void __init extent_map_init(void)
 void __exit extent_map_exit(void)
 {
 	struct extent_buffer *eb;
+	struct extent_state *state;
 
 	while (!list_empty(&extent_buffers)) {
 		eb = list_entry(extent_buffers.next,
@@ -55,6 +61,22 @@ void __exit extent_map_exit(void)
 		list_del(&eb->list);
 		kmem_cache_free(extent_buffer_cache, eb);
 	}
+	while (!list_empty(&states)) {
+		state = list_entry(states.next, struct extent_state, list);
+		printk("state leak: start %Lu end %Lu state %lu in tree %d refs %d\n", state->start, state->end, state->state, state->in_tree, atomic_read(&state->refs));
+		list_del(&state->list);
+		kmem_cache_free(extent_state_cache, state);
+
+	}
+	while (!list_empty(&buffers)) {
+		eb = list_entry(buffers.next,
+				struct extent_buffer, leak_list);
+		printk("buffer leak start %Lu len %lu return %lX\n", eb->start, eb->len, eb->alloc_addr);
+		list_del(&eb->leak_list);
+		kmem_cache_free(extent_buffer_cache, eb);
+	}
+
+
 	if (extent_map_cache)
 		kmem_cache_destroy(extent_map_cache);
 	if (extent_state_cache)
@@ -101,12 +123,19 @@ EXPORT_SYMBOL(free_extent_map);
 struct extent_state *alloc_extent_state(gfp_t mask)
 {
 	struct extent_state *state;
+	unsigned long flags;
+
 	state = kmem_cache_alloc(extent_state_cache, mask);
 	if (!state || IS_ERR(state))
 		return state;
 	state->state = 0;
 	state->in_tree = 0;
 	state->private = 0;
+
+	spin_lock_irqsave(&state_lock, flags);
+	list_add(&state->list, &states);
+	spin_unlock_irqrestore(&state_lock, flags);
+
 	atomic_set(&state->refs, 1);
 	init_waitqueue_head(&state->wq);
 	return state;
@@ -115,10 +144,14 @@ EXPORT_SYMBOL(alloc_extent_state);
 
 void free_extent_state(struct extent_state *state)
 {
+	unsigned long flags;
 	if (!state)
 		return;
 	if (atomic_dec_and_test(&state->refs)) {
 		WARN_ON(state->in_tree);
+		spin_lock_irqsave(&state_lock, flags);
+		list_del(&state->list);
+		spin_unlock_irqrestore(&state_lock, flags);
 		kmem_cache_free(extent_state_cache, state);
 	}
 }
@@ -361,10 +394,6 @@ static int insert_state(struct extent_map_tree *tree,
 	state->state |= bits;
 	state->start = start;
 	state->end = end;
-	if ((end & 4095) == 0) {
-		printk("insert state %Lu %Lu strange end\n", start, end);
-		WARN_ON(1);
-	}
 	node = tree_insert(&tree->state, end, &state->rb_node);
 	if (node) {
 		struct extent_state *found;
@@ -399,11 +428,7 @@ static int split_state(struct extent_map_tree *tree, struct extent_state *orig,
 	prealloc->end = split - 1;
 	prealloc->state = orig->state;
 	orig->start = split;
-	if ((prealloc->end & 4095) == 0) {
-		printk("insert state %Lu %Lu strange end\n", prealloc->start,
-		       prealloc->end);
-		WARN_ON(1);
-	}
+
 	node = tree_insert(&tree->state, prealloc->end, &prealloc->rb_node);
 	if (node) {
 		struct extent_state *found;
@@ -957,6 +982,7 @@ int find_first_extent_bit(struct extent_map_tree *tree, u64 start,
 			*start_ret = state->start;
 			*end_ret = state->end;
 			ret = 0;
+			break;
 		}
 		node = rb_next(node);
 		if (!node)
@@ -1877,6 +1903,7 @@ sector_t extent_bmap(struct address_space *mapping, sector_t iblock,
 static struct extent_buffer *__alloc_extent_buffer(gfp_t mask)
 {
 	struct extent_buffer *eb = NULL;
+
 	spin_lock(&extent_buffers_lock);
 	if (!list_empty(&extent_buffers)) {
 		eb = list_entry(extent_buffers.next, struct extent_buffer,
@@ -1886,15 +1913,26 @@ static struct extent_buffer *__alloc_extent_buffer(gfp_t mask)
 		nr_extent_buffers--;
 	}
 	spin_unlock(&extent_buffers_lock);
+
 	if (eb) {
 		memset(eb, 0, sizeof(*eb));
-		return eb;
+	} else {
+		eb = kmem_cache_zalloc(extent_buffer_cache, mask);
 	}
-	return kmem_cache_zalloc(extent_buffer_cache, mask);
+	spin_lock(&extent_buffers_lock);
+	list_add(&eb->leak_list, &buffers);
+	spin_unlock(&extent_buffers_lock);
+
+	return eb;
 }
 
 static void __free_extent_buffer(struct extent_buffer *eb)
 {
+
+	spin_lock(&extent_buffers_lock);
+	list_del_init(&eb->leak_list);
+	spin_unlock(&extent_buffers_lock);
+
 	if (nr_extent_buffers >= MAX_EXTENT_BUFFER_CACHE) {
 		kmem_cache_free(extent_buffer_cache, eb);
 	} else {
@@ -1933,6 +1971,7 @@ struct extent_buffer *alloc_extent_buffer(struct extent_map_tree *tree,
 	if (!eb || IS_ERR(eb))
 		return NULL;
 
+	eb->alloc_addr = __builtin_return_address(0);
 	eb->start = start;
 	eb->len = len;
 	atomic_set(&eb->refs, 1);
@@ -1947,6 +1986,7 @@ struct extent_buffer *alloc_extent_buffer(struct extent_map_tree *tree,
 			eb->start &= ~((u64)PAGE_CACHE_SIZE - 1);
 			goto fail;
 		}
+		set_page_extent_mapped(p);
 		if (i == 0)
 			eb->first_page = p;
 		if (!PageUptodate(p))
@@ -1978,6 +2018,7 @@ struct extent_buffer *find_extent_buffer(struct extent_map_tree *tree,
 	if (!eb || IS_ERR(eb))
 		return NULL;
 
+	eb->alloc_addr = __builtin_return_address(0);
 	eb->start = start;
 	eb->len = len;
 	atomic_set(&eb->refs, 1);
@@ -1992,6 +2033,7 @@ struct extent_buffer *find_extent_buffer(struct extent_map_tree *tree,
 			eb->start &= ~((u64)PAGE_CACHE_SIZE - 1);
 			goto fail;
 		}
+		set_page_extent_mapped(p);
 		if (i == 0)
 			eb->first_page = p;
 	}

commit ae5252bd51a252b7b8b02289337c36774835101c
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Oct 15 16:14:41 2007 -0400

    Btrfs: Go back to kmaps instead of page_address in extent_buffers
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index d2c733c68b4c..f150188f621c 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -1867,7 +1867,6 @@ sector_t extent_bmap(struct address_space *mapping, sector_t iblock,
 	if (!em || IS_ERR(em))
 		return 0;
 
-	// XXX(hch): block 0 is valid in some cases, e.g. XFS RT device
 	if (em->block_start == EXTENT_MAP_INLINE ||
 	    em->block_start == EXTENT_MAP_HOLE)
 		return 0;
@@ -2199,10 +2198,9 @@ void read_extent_buffer(struct extent_buffer *eb, void *dstv,
 		WARN_ON(!PageUptodate(page));
 
 		cur = min(len, (PAGE_CACHE_SIZE - offset));
-		// kaddr = kmap_atomic(page, KM_USER0);
-		kaddr = page_address(page);
+		kaddr = kmap_atomic(page, KM_USER0);
 		memcpy(dst, kaddr + offset, cur);
-		// kunmap_atomic(kaddr, KM_USER0);
+		kunmap_atomic(kaddr, KM_USER0);
 
 		dst += cur;
 		len -= cur;
@@ -2237,8 +2235,7 @@ int map_extent_buffer(struct extent_buffer *eb, unsigned long start,
 		*map_start = (i << PAGE_CACHE_SHIFT) - start_offset;
 	}
 
-	// kaddr = kmap_atomic(eb->pages[i], km);
-	kaddr = page_address(extent_buffer_page(eb, i));
+	kaddr = kmap_atomic(extent_buffer_page(eb, i), km);
 	*token = kaddr;
 	*map = kaddr + offset;
 	*map_len = PAGE_CACHE_SIZE - offset;
@@ -2248,7 +2245,7 @@ EXPORT_SYMBOL(map_extent_buffer);
 
 void unmap_extent_buffer(struct extent_buffer *eb, char *token, int km)
 {
-	// kunmap_atomic(token, km);
+	kunmap_atomic(token, km);
 }
 EXPORT_SYMBOL(unmap_extent_buffer);
 
@@ -2278,10 +2275,9 @@ int memcmp_extent_buffer(struct extent_buffer *eb, const void *ptrv,
 
 		cur = min(len, (PAGE_CACHE_SIZE - offset));
 
-		// kaddr = kmap_atomic(page, KM_USER0);
-		kaddr = page_address(page);
+		kaddr = kmap_atomic(page, KM_USER0);
 		ret = memcmp(ptr, kaddr + offset, cur);
-		// kunmap_atomic(kaddr, KM_USER0);
+		kunmap_atomic(kaddr, KM_USER0);
 		if (ret)
 			break;
 
@@ -2317,10 +2313,9 @@ void write_extent_buffer(struct extent_buffer *eb, const void *srcv,
 		WARN_ON(!PageUptodate(page));
 
 		cur = min(len, PAGE_CACHE_SIZE - offset);
-		// kaddr = kmap_atomic(page, KM_USER0);
-		kaddr = page_address(page);
+		kaddr = kmap_atomic(page, KM_USER0);
 		memcpy(kaddr + offset, src, cur);
-		// kunmap_atomic(kaddr, KM_USER0);
+		kunmap_atomic(kaddr, KM_USER0);
 
 		src += cur;
 		len -= cur;
@@ -2352,10 +2347,9 @@ void memset_extent_buffer(struct extent_buffer *eb, char c,
 		WARN_ON(!PageUptodate(page));
 
 		cur = min(len, PAGE_CACHE_SIZE - offset);
-		// kaddr = kmap_atomic(page, KM_USER0);
-		kaddr = page_address(page);
+		kaddr = kmap_atomic(page, KM_USER0);
 		memset(kaddr + offset, c, cur);
-		// kunmap_atomic(kaddr, KM_USER0);
+		kunmap_atomic(kaddr, KM_USER0);
 
 		len -= cur;
 		offset = 0;
@@ -2388,10 +2382,9 @@ void copy_extent_buffer(struct extent_buffer *dst, struct extent_buffer *src,
 
 		cur = min(len, (unsigned long)(PAGE_CACHE_SIZE - offset));
 
-		// kaddr = kmap_atomic(page, KM_USER1);
-		kaddr = page_address(page);
+		kaddr = kmap_atomic(page, KM_USER1);
 		read_extent_buffer(src, kaddr + offset, src_offset, cur);
-		// kunmap_atomic(kaddr, KM_USER1);
+		kunmap_atomic(kaddr, KM_USER1);
 
 		src_offset += cur;
 		len -= cur;
@@ -2405,43 +2398,38 @@ static void move_pages(struct page *dst_page, struct page *src_page,
 		       unsigned long dst_off, unsigned long src_off,
 		       unsigned long len)
 {
-	// char *dst_kaddr = kmap_atomic(dst_page, KM_USER0);
-	char *dst_kaddr = page_address(dst_page);
+	char *dst_kaddr = kmap_atomic(dst_page, KM_USER0);
 	if (dst_page == src_page) {
 		memmove(dst_kaddr + dst_off, dst_kaddr + src_off, len);
 	} else {
-		// char *src_kaddr = kmap_atomic(src_page, KM_USER1);
-		char *src_kaddr = page_address(src_page);
+		char *src_kaddr = kmap_atomic(src_page, KM_USER1);
 		char *p = dst_kaddr + dst_off + len;
 		char *s = src_kaddr + src_off + len;
 
 		while (len--)
 			*--p = *--s;
 
-		// kunmap_atomic(src_kaddr, KM_USER1);
+		kunmap_atomic(src_kaddr, KM_USER1);
 	}
-	// kunmap_atomic(dst_kaddr, KM_USER0);
+	kunmap_atomic(dst_kaddr, KM_USER0);
 }
 
 static void copy_pages(struct page *dst_page, struct page *src_page,
 		       unsigned long dst_off, unsigned long src_off,
 		       unsigned long len)
 {
-	//kmap_atomic(dst_page, KM_USER0);
-	char *dst_kaddr = page_address(dst_page);
+	char *dst_kaddr = kmap_atomic(dst_page, KM_USER0);
 	char *src_kaddr;
 
 	if (dst_page != src_page)
-		src_kaddr = page_address(src_page); // kmap_atomic(src_page, KM_USER1);
+		src_kaddr = kmap_atomic(src_page, KM_USER1);
 	else
 		src_kaddr = dst_kaddr;
 
 	memcpy(dst_kaddr + dst_off, src_kaddr + src_off, len);
-	/*
 	kunmap_atomic(dst_kaddr, KM_USER0);
 	if (dst_page != src_page)
 		kunmap_atomic(src_kaddr, KM_USER1);
-	*/
 }
 
 void memcpy_extent_buffer(struct extent_buffer *dst, unsigned long dst_offset,
@@ -2537,7 +2525,7 @@ void memmove_extent_buffer(struct extent_buffer *dst, unsigned long dst_offset,
 
 		cur = min(len, src_off_in_page + 1);
 		cur = min(cur, dst_off_in_page + 1);
-// printk("move pages orig dst %lu src %lu len %lu, this %lu %lu %lu\n", dst_offset, src_offset, len, dst_off_in_page - cur + 1, src_off_in_page - cur + 1, cur);
+
 		move_pages(extent_buffer_page(dst, dst_i),
 			   extent_buffer_page(dst, src_i),
 			   dst_off_in_page - cur + 1,

commit 6d36dcd48f1e4e7446d603a3df9638bd314a182d
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Oct 15 16:14:37 2007 -0400

    Btrfs: Avoid memcpy where possible in extent_buffers
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 8bef309e1b37..d2c733c68b4c 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -18,6 +18,11 @@ struct kmem_cache *btrfs_cache_create(const char *name, size_t size,
 
 static struct kmem_cache *extent_map_cache;
 static struct kmem_cache *extent_state_cache;
+static struct kmem_cache *extent_buffer_cache;
+static LIST_HEAD(extent_buffers);
+static spinlock_t extent_buffers_lock;
+static int nr_extent_buffers;
+#define MAX_EXTENT_BUFFER_CACHE 128
 
 struct tree_entry {
 	u64 start;
@@ -29,21 +34,33 @@ struct tree_entry {
 void __init extent_map_init(void)
 {
 	extent_map_cache = btrfs_cache_create("extent_map",
-					    sizeof(struct extent_map),
-					    SLAB_DESTROY_BY_RCU,
+					    sizeof(struct extent_map), 0,
 					    NULL);
 	extent_state_cache = btrfs_cache_create("extent_state",
-					    sizeof(struct extent_state),
-					    SLAB_DESTROY_BY_RCU,
+					    sizeof(struct extent_state), 0,
 					    NULL);
+	extent_buffer_cache = btrfs_cache_create("extent_buffers",
+					    sizeof(struct extent_buffer), 0,
+					    NULL);
+	spin_lock_init(&extent_buffers_lock);
 }
 
 void __exit extent_map_exit(void)
 {
+	struct extent_buffer *eb;
+
+	while (!list_empty(&extent_buffers)) {
+		eb = list_entry(extent_buffers.next,
+				struct extent_buffer, list);
+		list_del(&eb->list);
+		kmem_cache_free(extent_buffer_cache, eb);
+	}
 	if (extent_map_cache)
 		kmem_cache_destroy(extent_map_cache);
 	if (extent_state_cache)
 		kmem_cache_destroy(extent_state_cache);
+	if (extent_buffer_cache)
+		kmem_cache_destroy(extent_buffer_cache);
 }
 
 void extent_map_tree_init(struct extent_map_tree *tree,
@@ -1858,6 +1875,48 @@ sector_t extent_bmap(struct address_space *mapping, sector_t iblock,
 	return (em->block_start + start - em->start) >> inode->i_blkbits;
 }
 
+static struct extent_buffer *__alloc_extent_buffer(gfp_t mask)
+{
+	struct extent_buffer *eb = NULL;
+	spin_lock(&extent_buffers_lock);
+	if (!list_empty(&extent_buffers)) {
+		eb = list_entry(extent_buffers.next, struct extent_buffer,
+				list);
+		list_del(&eb->list);
+		WARN_ON(nr_extent_buffers == 0);
+		nr_extent_buffers--;
+	}
+	spin_unlock(&extent_buffers_lock);
+	if (eb) {
+		memset(eb, 0, sizeof(*eb));
+		return eb;
+	}
+	return kmem_cache_zalloc(extent_buffer_cache, mask);
+}
+
+static void __free_extent_buffer(struct extent_buffer *eb)
+{
+	if (nr_extent_buffers >= MAX_EXTENT_BUFFER_CACHE) {
+		kmem_cache_free(extent_buffer_cache, eb);
+	} else {
+		spin_lock(&extent_buffers_lock);
+		list_add(&eb->list, &extent_buffers);
+		nr_extent_buffers++;
+		spin_unlock(&extent_buffers_lock);
+	}
+}
+
+static inline struct page *extent_buffer_page(struct extent_buffer *eb, int i)
+{
+	struct page *p;
+	if (i == 0)
+		return eb->first_page;
+	i += eb->start >> PAGE_CACHE_SHIFT;
+	p = find_get_page(eb->first_page->mapping, i);
+	page_cache_release(p);
+	return p;
+}
+
 struct extent_buffer *alloc_extent_buffer(struct extent_map_tree *tree,
 					  u64 start, unsigned long len,
 					  gfp_t mask)
@@ -1871,7 +1930,7 @@ struct extent_buffer *alloc_extent_buffer(struct extent_map_tree *tree,
 	struct address_space *mapping = tree->mapping;
 	int uptodate = 0;
 
-	eb = kzalloc(EXTENT_BUFFER_SIZE(num_pages), mask);
+	eb = __alloc_extent_buffer(mask);
 	if (!eb || IS_ERR(eb))
 		return NULL;
 
@@ -1881,9 +1940,16 @@ struct extent_buffer *alloc_extent_buffer(struct extent_map_tree *tree,
 
 	for (i = 0; i < num_pages; i++, index++) {
 		p = find_or_create_page(mapping, index, mask | __GFP_HIGHMEM);
-		if (!p)
+		if (!p) {
+			/* make sure the free only frees the pages we've
+			 * grabbed a reference on
+			 */
+			eb->len = i << PAGE_CACHE_SHIFT;
+			eb->start &= ~((u64)PAGE_CACHE_SIZE - 1);
 			goto fail;
-		eb->pages[i] = p;
+		}
+		if (i == 0)
+			eb->first_page = p;
 		if (!PageUptodate(p))
 			uptodate = 0;
 		unlock_page(p);
@@ -1909,7 +1975,7 @@ struct extent_buffer *find_extent_buffer(struct extent_map_tree *tree,
 	struct page *p;
 	struct address_space *mapping = tree->mapping;
 
-	eb = kzalloc(EXTENT_BUFFER_SIZE(num_pages), mask);
+	eb = __alloc_extent_buffer(mask);
 	if (!eb || IS_ERR(eb))
 		return NULL;
 
@@ -1919,9 +1985,16 @@ struct extent_buffer *find_extent_buffer(struct extent_map_tree *tree,
 
 	for (i = 0; i < num_pages; i++, index++) {
 		p = find_get_page(mapping, index);
-		if (!p)
+		if (!p) {
+			/* make sure the free only frees the pages we've
+			 * grabbed a reference on
+			 */
+			eb->len = i << PAGE_CACHE_SHIFT;
+			eb->start &= ~((u64)PAGE_CACHE_SIZE - 1);
 			goto fail;
-		eb->pages[i] = p;
+		}
+		if (i == 0)
+			eb->first_page = p;
 	}
 	return eb;
 fail:
@@ -1944,11 +2017,12 @@ void free_extent_buffer(struct extent_buffer *eb)
 	num_pages = ((eb->start + eb->len - 1) >> PAGE_CACHE_SHIFT) -
 		(eb->start >> PAGE_CACHE_SHIFT) + 1;
 
-	for (i = 0; i < num_pages; i++) {
-		if (eb->pages[i])
-			page_cache_release(eb->pages[i]);
+	if (eb->first_page)
+		page_cache_release(eb->first_page);
+	for (i = 1; i < num_pages; i++) {
+		page_cache_release(extent_buffer_page(eb, i));
 	}
-	kfree(eb);
+	__free_extent_buffer(eb);
 }
 EXPORT_SYMBOL(free_extent_buffer);
 
@@ -1968,7 +2042,7 @@ int clear_extent_buffer_dirty(struct extent_map_tree *tree,
 		(eb->start >> PAGE_CACHE_SHIFT) + 1;
 
 	for (i = 0; i < num_pages; i++) {
-		page = eb->pages[i];
+		page = extent_buffer_page(eb, i);
 		lock_page(page);
 		/*
 		 * if we're on the last page or the first page and the
@@ -2021,7 +2095,7 @@ int set_extent_buffer_uptodate(struct extent_map_tree *tree,
 	set_extent_uptodate(tree, eb->start, eb->start + eb->len - 1,
 			    GFP_NOFS);
 	for (i = 0; i < num_pages; i++) {
-		page = eb->pages[i];
+		page = extent_buffer_page(eb, i);
 		if ((i == 0 && (eb->start & (PAGE_CACHE_SIZE - 1))) ||
 		    ((i == num_pages - 1) &&
 		     ((eb->start + eb->len - 1) & (PAGE_CACHE_SIZE - 1)))) {
@@ -2064,7 +2138,7 @@ int read_extent_buffer_pages(struct extent_map_tree *tree,
 	num_pages = ((eb->start + eb->len - 1) >> PAGE_CACHE_SHIFT) -
 		(eb->start >> PAGE_CACHE_SHIFT) + 1;
 	for (i = 0; i < num_pages; i++) {
-		page = eb->pages[i];
+		page = extent_buffer_page(eb, i);
 		if (PageUptodate(page)) {
 			continue;
 		}
@@ -2090,7 +2164,7 @@ int read_extent_buffer_pages(struct extent_map_tree *tree,
 	}
 
 	for (i = 0; i < num_pages; i++) {
-		page = eb->pages[i];
+		page = extent_buffer_page(eb, i);
 		wait_on_page_locked(page);
 		if (!PageUptodate(page)) {
 			ret = -EIO;
@@ -2116,12 +2190,12 @@ void read_extent_buffer(struct extent_buffer *eb, void *dstv,
 	WARN_ON(start > eb->len);
 	WARN_ON(start + len > eb->start + eb->len);
 
-	page = eb->pages[i];
 	offset = start & ((unsigned long)PAGE_CACHE_SIZE - 1);
 	if (i == 0)
 		offset += start_offset;
 
 	while(len > 0) {
+		page = extent_buffer_page(eb, i);
 		WARN_ON(!PageUptodate(page));
 
 		cur = min(len, (PAGE_CACHE_SIZE - offset));
@@ -2134,7 +2208,6 @@ void read_extent_buffer(struct extent_buffer *eb, void *dstv,
 		len -= cur;
 		offset = 0;
 		i++;
-		page = eb->pages[i];
 	}
 }
 EXPORT_SYMBOL(read_extent_buffer);
@@ -2165,7 +2238,7 @@ int map_extent_buffer(struct extent_buffer *eb, unsigned long start,
 	}
 
 	// kaddr = kmap_atomic(eb->pages[i], km);
-	kaddr = page_address(eb->pages[i]);
+	kaddr = page_address(extent_buffer_page(eb, i));
 	*token = kaddr;
 	*map = kaddr + offset;
 	*map_len = PAGE_CACHE_SIZE - offset;
@@ -2195,12 +2268,12 @@ int memcmp_extent_buffer(struct extent_buffer *eb, const void *ptrv,
 	WARN_ON(start > eb->len);
 	WARN_ON(start + len > eb->start + eb->len);
 
-	page = eb->pages[i];
 	offset = start & ((unsigned long)PAGE_CACHE_SIZE - 1);
 	if (i == 0)
 		offset += start_offset;
 
 	while(len > 0) {
+		page = extent_buffer_page(eb, i);
 		WARN_ON(!PageUptodate(page));
 
 		cur = min(len, (PAGE_CACHE_SIZE - offset));
@@ -2216,7 +2289,6 @@ int memcmp_extent_buffer(struct extent_buffer *eb, const void *ptrv,
 		len -= cur;
 		offset = 0;
 		i++;
-		page = eb->pages[i];
 	}
 	return ret;
 }
@@ -2236,12 +2308,12 @@ void write_extent_buffer(struct extent_buffer *eb, const void *srcv,
 	WARN_ON(start > eb->len);
 	WARN_ON(start + len > eb->start + eb->len);
 
-	page = eb->pages[i];
 	offset = start & ((unsigned long)PAGE_CACHE_SIZE - 1);
 	if (i == 0)
 		offset += start_offset;
 
 	while(len > 0) {
+		page = extent_buffer_page(eb, i);
 		WARN_ON(!PageUptodate(page));
 
 		cur = min(len, PAGE_CACHE_SIZE - offset);
@@ -2254,7 +2326,6 @@ void write_extent_buffer(struct extent_buffer *eb, const void *srcv,
 		len -= cur;
 		offset = 0;
 		i++;
-		page = eb->pages[i];
 	}
 }
 EXPORT_SYMBOL(write_extent_buffer);
@@ -2272,12 +2343,12 @@ void memset_extent_buffer(struct extent_buffer *eb, char c,
 	WARN_ON(start > eb->len);
 	WARN_ON(start + len > eb->start + eb->len);
 
-	page = eb->pages[i];
 	offset = start & ((unsigned long)PAGE_CACHE_SIZE - 1);
 	if (i == 0)
 		offset += start_offset;
 
 	while(len > 0) {
+		page = extent_buffer_page(eb, i);
 		WARN_ON(!PageUptodate(page));
 
 		cur = min(len, PAGE_CACHE_SIZE - offset);
@@ -2289,7 +2360,6 @@ void memset_extent_buffer(struct extent_buffer *eb, char c,
 		len -= cur;
 		offset = 0;
 		i++;
-		page = eb->pages[i];
 	}
 }
 EXPORT_SYMBOL(memset_extent_buffer);
@@ -2313,7 +2383,7 @@ void copy_extent_buffer(struct extent_buffer *dst, struct extent_buffer *src,
 		offset += start_offset;
 
 	while(len > 0) {
-		page = dst->pages[i];
+		page = extent_buffer_page(dst, i);
 		WARN_ON(!PageUptodate(page));
 
 		cur = min(len, (unsigned long)(PAGE_CACHE_SIZE - offset));
@@ -2414,7 +2484,8 @@ void memcpy_extent_buffer(struct extent_buffer *dst, unsigned long dst_offset,
 		cur = min(cur, (unsigned long)(PAGE_CACHE_SIZE -
 					       dst_off_in_page));
 
-		copy_pages(dst->pages[dst_i], dst->pages[src_i],
+		copy_pages(extent_buffer_page(dst, dst_i),
+			   extent_buffer_page(dst, src_i),
 			   dst_off_in_page, src_off_in_page, cur);
 
 		src_offset += cur;
@@ -2467,7 +2538,8 @@ void memmove_extent_buffer(struct extent_buffer *dst, unsigned long dst_offset,
 		cur = min(len, src_off_in_page + 1);
 		cur = min(cur, dst_off_in_page + 1);
 // printk("move pages orig dst %lu src %lu len %lu, this %lu %lu %lu\n", dst_offset, src_offset, len, dst_off_in_page - cur + 1, src_off_in_page - cur + 1, cur);
-		move_pages(dst->pages[dst_i], dst->pages[src_i],
+		move_pages(extent_buffer_page(dst, dst_i),
+			   extent_buffer_page(dst, src_i),
 			   dst_off_in_page - cur + 1,
 			   src_off_in_page - cur + 1, cur);
 

commit 479965d66e320f1a095bb76027171daa675a9c72
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Oct 15 16:14:27 2007 -0400

    Btrfs: Optimizations for the extent_buffer code
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 55fefdfab84c..8bef309e1b37 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -2140,14 +2140,20 @@ void read_extent_buffer(struct extent_buffer *eb, void *dstv,
 EXPORT_SYMBOL(read_extent_buffer);
 
 int map_extent_buffer(struct extent_buffer *eb, unsigned long start,
+		      unsigned long min_len,
 		      char **token, char **map,
 		      unsigned long *map_start,
 		      unsigned long *map_len, int km)
 {
-	size_t offset;
+	size_t offset = start & (PAGE_CACHE_SIZE - 1);
 	char *kaddr;
 	size_t start_offset = eb->start & ((u64)PAGE_CACHE_SIZE - 1);
 	unsigned long i = (start_offset + start) >> PAGE_CACHE_SHIFT;
+	unsigned long end_i = (start_offset + start + min_len) >>
+				PAGE_CACHE_SHIFT;
+
+	if (i != end_i)
+		return -EINVAL;
 
 	WARN_ON(start > eb->len);
 
@@ -2155,8 +2161,7 @@ int map_extent_buffer(struct extent_buffer *eb, unsigned long start,
 		offset = start_offset;
 		*map_start = 0;
 	} else {
-		offset = 0;
-		*map_start = (i << PAGE_CACHE_SHIFT) - offset;
+		*map_start = (i << PAGE_CACHE_SHIFT) - start_offset;
 	}
 
 	// kaddr = kmap_atomic(eb->pages[i], km);

commit 5f39d397dfbe140a14edecd4e73c34ce23c4f9ee
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Oct 15 16:14:19 2007 -0400

    Btrfs: Create extent_buffer interface for large blocksizes
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 0ab368e091f9..55fefdfab84c 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -26,16 +26,6 @@ struct tree_entry {
 	struct rb_node rb_node;
 };
 
-/* bits for the extent state */
-#define EXTENT_DIRTY 1
-#define EXTENT_WRITEBACK (1 << 1)
-#define EXTENT_UPTODATE (1 << 2)
-#define EXTENT_LOCKED (1 << 3)
-#define EXTENT_NEW (1 << 4)
-#define EXTENT_DELALLOC (1 << 5)
-
-#define EXTENT_IOBITS (EXTENT_LOCKED | EXTENT_WRITEBACK)
-
 void __init extent_map_init(void)
 {
 	extent_map_cache = btrfs_cache_create("extent_map",
@@ -223,7 +213,8 @@ int add_extent_mapping(struct extent_map_tree *tree,
 		if (rb)
 			prev = rb_entry(rb, struct extent_map, rb_node);
 		if (prev && prev->end + 1 == em->start &&
-		    ((em->block_start == 0 && prev->block_start == 0) ||
+		    ((em->block_start == EXTENT_MAP_HOLE &&
+		      prev->block_start == EXTENT_MAP_HOLE) ||
 			     (em->block_start == prev->block_end + 1))) {
 			em->start = prev->start;
 			em->block_start = prev->block_start;
@@ -926,6 +917,40 @@ int set_range_writeback(struct extent_map_tree *tree, u64 start, u64 end)
 }
 EXPORT_SYMBOL(set_range_writeback);
 
+int find_first_extent_bit(struct extent_map_tree *tree, u64 start,
+			  u64 *start_ret, u64 *end_ret, int bits)
+{
+	struct rb_node *node;
+	struct extent_state *state;
+	int ret = 1;
+
+	write_lock_irq(&tree->lock);
+	/*
+	 * this search will find all the extents that end after
+	 * our range starts.
+	 */
+	node = tree_search(&tree->state, start);
+	if (!node || IS_ERR(node)) {
+		goto out;
+	}
+
+	while(1) {
+		state = rb_entry(node, struct extent_state, rb_node);
+		if (state->state & bits) {
+			*start_ret = state->start;
+			*end_ret = state->end;
+			ret = 0;
+		}
+		node = rb_next(node);
+		if (!node)
+			break;
+	}
+out:
+	write_unlock_irq(&tree->lock);
+	return ret;
+}
+EXPORT_SYMBOL(find_first_extent_bit);
+
 u64 find_lock_delalloc_range(struct extent_map_tree *tree,
 			     u64 start, u64 lock_start, u64 *end, u64 max_bytes)
 {
@@ -1450,7 +1475,7 @@ int extent_read_full_page(struct extent_map_tree *tree, struct page *page,
 		em = NULL;
 
 		/* we've found a hole, just zero and go on */
-		if (block_start == 0) {
+		if (block_start == EXTENT_MAP_HOLE) {
 			zero_user_page(page, page_offset, iosize, KM_USER0);
 			set_extent_uptodate(tree, cur, cur + iosize - 1,
 					    GFP_NOFS);
@@ -1593,7 +1618,8 @@ int extent_write_full_page(struct extent_map_tree *tree, struct page *page,
 		free_extent_map(em);
 		em = NULL;
 
-		if (block_start == 0 || block_start == EXTENT_MAP_INLINE) {
+		if (block_start == EXTENT_MAP_HOLE ||
+		    block_start == EXTENT_MAP_INLINE) {
 			clear_extent_dirty(tree, cur,
 					   cur + iosize - 1, GFP_NOFS);
 			cur = cur + iosize;
@@ -1630,7 +1656,6 @@ int extent_write_full_page(struct extent_map_tree *tree, struct page *page,
 		nr++;
 	}
 done:
-	WARN_ON(test_range_bit(tree, start, page_end, EXTENT_DIRTY, 0));
 	unlock_extent(tree, start, page_end, GFP_NOFS);
 	unlock_page(page);
 	return 0;
@@ -1827,8 +1852,623 @@ sector_t extent_bmap(struct address_space *mapping, sector_t iblock,
 
 	// XXX(hch): block 0 is valid in some cases, e.g. XFS RT device
 	if (em->block_start == EXTENT_MAP_INLINE ||
-	    em->block_start == 0)
-	    	return 0;
+	    em->block_start == EXTENT_MAP_HOLE)
+		return 0;
 
 	return (em->block_start + start - em->start) >> inode->i_blkbits;
 }
+
+struct extent_buffer *alloc_extent_buffer(struct extent_map_tree *tree,
+					  u64 start, unsigned long len,
+					  gfp_t mask)
+{
+	unsigned long num_pages = ((start + len - 1) >> PAGE_CACHE_SHIFT) -
+				  (start >> PAGE_CACHE_SHIFT) + 1;
+	unsigned long i;
+	unsigned long index = start >> PAGE_CACHE_SHIFT;
+	struct extent_buffer *eb;
+	struct page *p;
+	struct address_space *mapping = tree->mapping;
+	int uptodate = 0;
+
+	eb = kzalloc(EXTENT_BUFFER_SIZE(num_pages), mask);
+	if (!eb || IS_ERR(eb))
+		return NULL;
+
+	eb->start = start;
+	eb->len = len;
+	atomic_set(&eb->refs, 1);
+
+	for (i = 0; i < num_pages; i++, index++) {
+		p = find_or_create_page(mapping, index, mask | __GFP_HIGHMEM);
+		if (!p)
+			goto fail;
+		eb->pages[i] = p;
+		if (!PageUptodate(p))
+			uptodate = 0;
+		unlock_page(p);
+	}
+	if (uptodate)
+		eb->flags |= EXTENT_UPTODATE;
+	return eb;
+fail:
+	free_extent_buffer(eb);
+	return NULL;
+}
+EXPORT_SYMBOL(alloc_extent_buffer);
+
+struct extent_buffer *find_extent_buffer(struct extent_map_tree *tree,
+					 u64 start, unsigned long len,
+					  gfp_t mask)
+{
+	unsigned long num_pages = ((start + len - 1) >> PAGE_CACHE_SHIFT) -
+				  (start >> PAGE_CACHE_SHIFT) + 1;
+	unsigned long i;
+	unsigned long index = start >> PAGE_CACHE_SHIFT;
+	struct extent_buffer *eb;
+	struct page *p;
+	struct address_space *mapping = tree->mapping;
+
+	eb = kzalloc(EXTENT_BUFFER_SIZE(num_pages), mask);
+	if (!eb || IS_ERR(eb))
+		return NULL;
+
+	eb->start = start;
+	eb->len = len;
+	atomic_set(&eb->refs, 1);
+
+	for (i = 0; i < num_pages; i++, index++) {
+		p = find_get_page(mapping, index);
+		if (!p)
+			goto fail;
+		eb->pages[i] = p;
+	}
+	return eb;
+fail:
+	free_extent_buffer(eb);
+	return NULL;
+}
+EXPORT_SYMBOL(find_extent_buffer);
+
+void free_extent_buffer(struct extent_buffer *eb)
+{
+	unsigned long i;
+	unsigned long num_pages;
+
+	if (!eb)
+		return;
+
+	if (!atomic_dec_and_test(&eb->refs))
+		return;
+
+	num_pages = ((eb->start + eb->len - 1) >> PAGE_CACHE_SHIFT) -
+		(eb->start >> PAGE_CACHE_SHIFT) + 1;
+
+	for (i = 0; i < num_pages; i++) {
+		if (eb->pages[i])
+			page_cache_release(eb->pages[i]);
+	}
+	kfree(eb);
+}
+EXPORT_SYMBOL(free_extent_buffer);
+
+int clear_extent_buffer_dirty(struct extent_map_tree *tree,
+			      struct extent_buffer *eb)
+{
+	int set;
+	unsigned long i;
+	unsigned long num_pages;
+	struct page *page;
+
+	u64 start = eb->start;
+	u64 end = start + eb->len - 1;
+
+	set = clear_extent_dirty(tree, start, end, GFP_NOFS);
+	num_pages = ((eb->start + eb->len - 1) >> PAGE_CACHE_SHIFT) -
+		(eb->start >> PAGE_CACHE_SHIFT) + 1;
+
+	for (i = 0; i < num_pages; i++) {
+		page = eb->pages[i];
+		lock_page(page);
+		/*
+		 * if we're on the last page or the first page and the
+		 * block isn't aligned on a page boundary, do extra checks
+		 * to make sure we don't clean page that is partially dirty
+		 */
+		if ((i == 0 && (eb->start & (PAGE_CACHE_SIZE - 1))) ||
+		    ((i == num_pages - 1) &&
+		     ((eb->start + eb->len - 1) & (PAGE_CACHE_SIZE - 1)))) {
+			start = page->index << PAGE_CACHE_SHIFT;
+			end  = start + PAGE_CACHE_SIZE - 1;
+			if (test_range_bit(tree, start, end,
+					   EXTENT_DIRTY, 0)) {
+				unlock_page(page);
+				continue;
+			}
+		}
+		clear_page_dirty_for_io(page);
+		unlock_page(page);
+	}
+	return 0;
+}
+EXPORT_SYMBOL(clear_extent_buffer_dirty);
+
+int wait_on_extent_buffer_writeback(struct extent_map_tree *tree,
+				    struct extent_buffer *eb)
+{
+	return wait_on_extent_writeback(tree, eb->start,
+					eb->start + eb->len - 1);
+}
+EXPORT_SYMBOL(wait_on_extent_buffer_writeback);
+
+int set_extent_buffer_dirty(struct extent_map_tree *tree,
+			     struct extent_buffer *eb)
+{
+	return set_range_dirty(tree, eb->start, eb->start + eb->len - 1);
+}
+EXPORT_SYMBOL(set_extent_buffer_dirty);
+
+int set_extent_buffer_uptodate(struct extent_map_tree *tree,
+				struct extent_buffer *eb)
+{
+	unsigned long i;
+	struct page *page;
+	unsigned long num_pages;
+
+	num_pages = ((eb->start + eb->len - 1) >> PAGE_CACHE_SHIFT) -
+		(eb->start >> PAGE_CACHE_SHIFT) + 1;
+
+	set_extent_uptodate(tree, eb->start, eb->start + eb->len - 1,
+			    GFP_NOFS);
+	for (i = 0; i < num_pages; i++) {
+		page = eb->pages[i];
+		if ((i == 0 && (eb->start & (PAGE_CACHE_SIZE - 1))) ||
+		    ((i == num_pages - 1) &&
+		     ((eb->start + eb->len - 1) & (PAGE_CACHE_SIZE - 1)))) {
+			check_page_uptodate(tree, page);
+			continue;
+		}
+		SetPageUptodate(page);
+	}
+	return 0;
+}
+EXPORT_SYMBOL(set_extent_buffer_uptodate);
+
+int extent_buffer_uptodate(struct extent_map_tree *tree,
+			     struct extent_buffer *eb)
+{
+	if (eb->flags & EXTENT_UPTODATE)
+		return 1;
+	return test_range_bit(tree, eb->start, eb->start + eb->len - 1,
+			   EXTENT_UPTODATE, 1);
+}
+EXPORT_SYMBOL(extent_buffer_uptodate);
+
+int read_extent_buffer_pages(struct extent_map_tree *tree,
+			     struct extent_buffer *eb, int wait)
+{
+	unsigned long i;
+	struct page *page;
+	int err;
+	int ret = 0;
+	unsigned long num_pages;
+
+	if (eb->flags & EXTENT_UPTODATE)
+		return 0;
+
+	if (test_range_bit(tree, eb->start, eb->start + eb->len - 1,
+			   EXTENT_UPTODATE, 1)) {
+		return 0;
+	}
+
+	num_pages = ((eb->start + eb->len - 1) >> PAGE_CACHE_SHIFT) -
+		(eb->start >> PAGE_CACHE_SHIFT) + 1;
+	for (i = 0; i < num_pages; i++) {
+		page = eb->pages[i];
+		if (PageUptodate(page)) {
+			continue;
+		}
+		if (!wait) {
+			if (TestSetPageLocked(page)) {
+				continue;
+			}
+		} else {
+			lock_page(page);
+		}
+		if (!PageUptodate(page)) {
+			err = page->mapping->a_ops->readpage(NULL, page);
+			if (err) {
+				ret = err;
+			}
+		} else {
+			unlock_page(page);
+		}
+	}
+
+	if (ret || !wait) {
+		return ret;
+	}
+
+	for (i = 0; i < num_pages; i++) {
+		page = eb->pages[i];
+		wait_on_page_locked(page);
+		if (!PageUptodate(page)) {
+			ret = -EIO;
+		}
+	}
+	eb->flags |= EXTENT_UPTODATE;
+	return ret;
+}
+EXPORT_SYMBOL(read_extent_buffer_pages);
+
+void read_extent_buffer(struct extent_buffer *eb, void *dstv,
+			unsigned long start,
+			unsigned long len)
+{
+	size_t cur;
+	size_t offset;
+	struct page *page;
+	char *kaddr;
+	char *dst = (char *)dstv;
+	size_t start_offset = eb->start & ((u64)PAGE_CACHE_SIZE - 1);
+	unsigned long i = (start_offset + start) >> PAGE_CACHE_SHIFT;
+
+	WARN_ON(start > eb->len);
+	WARN_ON(start + len > eb->start + eb->len);
+
+	page = eb->pages[i];
+	offset = start & ((unsigned long)PAGE_CACHE_SIZE - 1);
+	if (i == 0)
+		offset += start_offset;
+
+	while(len > 0) {
+		WARN_ON(!PageUptodate(page));
+
+		cur = min(len, (PAGE_CACHE_SIZE - offset));
+		// kaddr = kmap_atomic(page, KM_USER0);
+		kaddr = page_address(page);
+		memcpy(dst, kaddr + offset, cur);
+		// kunmap_atomic(kaddr, KM_USER0);
+
+		dst += cur;
+		len -= cur;
+		offset = 0;
+		i++;
+		page = eb->pages[i];
+	}
+}
+EXPORT_SYMBOL(read_extent_buffer);
+
+int map_extent_buffer(struct extent_buffer *eb, unsigned long start,
+		      char **token, char **map,
+		      unsigned long *map_start,
+		      unsigned long *map_len, int km)
+{
+	size_t offset;
+	char *kaddr;
+	size_t start_offset = eb->start & ((u64)PAGE_CACHE_SIZE - 1);
+	unsigned long i = (start_offset + start) >> PAGE_CACHE_SHIFT;
+
+	WARN_ON(start > eb->len);
+
+	if (i == 0) {
+		offset = start_offset;
+		*map_start = 0;
+	} else {
+		offset = 0;
+		*map_start = (i << PAGE_CACHE_SHIFT) - offset;
+	}
+
+	// kaddr = kmap_atomic(eb->pages[i], km);
+	kaddr = page_address(eb->pages[i]);
+	*token = kaddr;
+	*map = kaddr + offset;
+	*map_len = PAGE_CACHE_SIZE - offset;
+	return 0;
+}
+EXPORT_SYMBOL(map_extent_buffer);
+
+void unmap_extent_buffer(struct extent_buffer *eb, char *token, int km)
+{
+	// kunmap_atomic(token, km);
+}
+EXPORT_SYMBOL(unmap_extent_buffer);
+
+int memcmp_extent_buffer(struct extent_buffer *eb, const void *ptrv,
+			  unsigned long start,
+			  unsigned long len)
+{
+	size_t cur;
+	size_t offset;
+	struct page *page;
+	char *kaddr;
+	char *ptr = (char *)ptrv;
+	size_t start_offset = eb->start & ((u64)PAGE_CACHE_SIZE - 1);
+	unsigned long i = (start_offset + start) >> PAGE_CACHE_SHIFT;
+	int ret = 0;
+
+	WARN_ON(start > eb->len);
+	WARN_ON(start + len > eb->start + eb->len);
+
+	page = eb->pages[i];
+	offset = start & ((unsigned long)PAGE_CACHE_SIZE - 1);
+	if (i == 0)
+		offset += start_offset;
+
+	while(len > 0) {
+		WARN_ON(!PageUptodate(page));
+
+		cur = min(len, (PAGE_CACHE_SIZE - offset));
+
+		// kaddr = kmap_atomic(page, KM_USER0);
+		kaddr = page_address(page);
+		ret = memcmp(ptr, kaddr + offset, cur);
+		// kunmap_atomic(kaddr, KM_USER0);
+		if (ret)
+			break;
+
+		ptr += cur;
+		len -= cur;
+		offset = 0;
+		i++;
+		page = eb->pages[i];
+	}
+	return ret;
+}
+EXPORT_SYMBOL(memcmp_extent_buffer);
+
+void write_extent_buffer(struct extent_buffer *eb, const void *srcv,
+			 unsigned long start, unsigned long len)
+{
+	size_t cur;
+	size_t offset;
+	struct page *page;
+	char *kaddr;
+	char *src = (char *)srcv;
+	size_t start_offset = eb->start & ((u64)PAGE_CACHE_SIZE - 1);
+	unsigned long i = (start_offset + start) >> PAGE_CACHE_SHIFT;
+
+	WARN_ON(start > eb->len);
+	WARN_ON(start + len > eb->start + eb->len);
+
+	page = eb->pages[i];
+	offset = start & ((unsigned long)PAGE_CACHE_SIZE - 1);
+	if (i == 0)
+		offset += start_offset;
+
+	while(len > 0) {
+		WARN_ON(!PageUptodate(page));
+
+		cur = min(len, PAGE_CACHE_SIZE - offset);
+		// kaddr = kmap_atomic(page, KM_USER0);
+		kaddr = page_address(page);
+		memcpy(kaddr + offset, src, cur);
+		// kunmap_atomic(kaddr, KM_USER0);
+
+		src += cur;
+		len -= cur;
+		offset = 0;
+		i++;
+		page = eb->pages[i];
+	}
+}
+EXPORT_SYMBOL(write_extent_buffer);
+
+void memset_extent_buffer(struct extent_buffer *eb, char c,
+			  unsigned long start, unsigned long len)
+{
+	size_t cur;
+	size_t offset;
+	struct page *page;
+	char *kaddr;
+	size_t start_offset = eb->start & ((u64)PAGE_CACHE_SIZE - 1);
+	unsigned long i = (start_offset + start) >> PAGE_CACHE_SHIFT;
+
+	WARN_ON(start > eb->len);
+	WARN_ON(start + len > eb->start + eb->len);
+
+	page = eb->pages[i];
+	offset = start & ((unsigned long)PAGE_CACHE_SIZE - 1);
+	if (i == 0)
+		offset += start_offset;
+
+	while(len > 0) {
+		WARN_ON(!PageUptodate(page));
+
+		cur = min(len, PAGE_CACHE_SIZE - offset);
+		// kaddr = kmap_atomic(page, KM_USER0);
+		kaddr = page_address(page);
+		memset(kaddr + offset, c, cur);
+		// kunmap_atomic(kaddr, KM_USER0);
+
+		len -= cur;
+		offset = 0;
+		i++;
+		page = eb->pages[i];
+	}
+}
+EXPORT_SYMBOL(memset_extent_buffer);
+
+void copy_extent_buffer(struct extent_buffer *dst, struct extent_buffer *src,
+			unsigned long dst_offset, unsigned long src_offset,
+			unsigned long len)
+{
+	u64 dst_len = dst->len;
+	size_t cur;
+	size_t offset;
+	struct page *page;
+	char *kaddr;
+	size_t start_offset = dst->start & ((u64)PAGE_CACHE_SIZE - 1);
+	unsigned long i = (start_offset + dst_offset) >> PAGE_CACHE_SHIFT;
+
+	WARN_ON(src->len != dst_len);
+
+	offset = dst_offset & ((unsigned long)PAGE_CACHE_SIZE - 1);
+	if (i == 0)
+		offset += start_offset;
+
+	while(len > 0) {
+		page = dst->pages[i];
+		WARN_ON(!PageUptodate(page));
+
+		cur = min(len, (unsigned long)(PAGE_CACHE_SIZE - offset));
+
+		// kaddr = kmap_atomic(page, KM_USER1);
+		kaddr = page_address(page);
+		read_extent_buffer(src, kaddr + offset, src_offset, cur);
+		// kunmap_atomic(kaddr, KM_USER1);
+
+		src_offset += cur;
+		len -= cur;
+		offset = 0;
+		i++;
+	}
+}
+EXPORT_SYMBOL(copy_extent_buffer);
+
+static void move_pages(struct page *dst_page, struct page *src_page,
+		       unsigned long dst_off, unsigned long src_off,
+		       unsigned long len)
+{
+	// char *dst_kaddr = kmap_atomic(dst_page, KM_USER0);
+	char *dst_kaddr = page_address(dst_page);
+	if (dst_page == src_page) {
+		memmove(dst_kaddr + dst_off, dst_kaddr + src_off, len);
+	} else {
+		// char *src_kaddr = kmap_atomic(src_page, KM_USER1);
+		char *src_kaddr = page_address(src_page);
+		char *p = dst_kaddr + dst_off + len;
+		char *s = src_kaddr + src_off + len;
+
+		while (len--)
+			*--p = *--s;
+
+		// kunmap_atomic(src_kaddr, KM_USER1);
+	}
+	// kunmap_atomic(dst_kaddr, KM_USER0);
+}
+
+static void copy_pages(struct page *dst_page, struct page *src_page,
+		       unsigned long dst_off, unsigned long src_off,
+		       unsigned long len)
+{
+	//kmap_atomic(dst_page, KM_USER0);
+	char *dst_kaddr = page_address(dst_page);
+	char *src_kaddr;
+
+	if (dst_page != src_page)
+		src_kaddr = page_address(src_page); // kmap_atomic(src_page, KM_USER1);
+	else
+		src_kaddr = dst_kaddr;
+
+	memcpy(dst_kaddr + dst_off, src_kaddr + src_off, len);
+	/*
+	kunmap_atomic(dst_kaddr, KM_USER0);
+	if (dst_page != src_page)
+		kunmap_atomic(src_kaddr, KM_USER1);
+	*/
+}
+
+void memcpy_extent_buffer(struct extent_buffer *dst, unsigned long dst_offset,
+			   unsigned long src_offset, unsigned long len)
+{
+	size_t cur;
+	size_t dst_off_in_page;
+	size_t src_off_in_page;
+	size_t start_offset = dst->start & ((u64)PAGE_CACHE_SIZE - 1);
+	unsigned long dst_i;
+	unsigned long src_i;
+
+	if (src_offset + len > dst->len) {
+		printk("memmove bogus src_offset %lu move len %lu len %lu\n",
+		       src_offset, len, dst->len);
+		BUG_ON(1);
+	}
+	if (dst_offset + len > dst->len) {
+		printk("memmove bogus dst_offset %lu move len %lu len %lu\n",
+		       dst_offset, len, dst->len);
+		BUG_ON(1);
+	}
+
+	while(len > 0) {
+		dst_off_in_page = dst_offset &
+			((unsigned long)PAGE_CACHE_SIZE - 1);
+		src_off_in_page = src_offset &
+			((unsigned long)PAGE_CACHE_SIZE - 1);
+
+		dst_i = (start_offset + dst_offset) >> PAGE_CACHE_SHIFT;
+		src_i = (start_offset + src_offset) >> PAGE_CACHE_SHIFT;
+
+		if (src_i == 0)
+			src_off_in_page += start_offset;
+		if (dst_i == 0)
+			dst_off_in_page += start_offset;
+
+		cur = min(len, (unsigned long)(PAGE_CACHE_SIZE -
+					       src_off_in_page));
+		cur = min(cur, (unsigned long)(PAGE_CACHE_SIZE -
+					       dst_off_in_page));
+
+		copy_pages(dst->pages[dst_i], dst->pages[src_i],
+			   dst_off_in_page, src_off_in_page, cur);
+
+		src_offset += cur;
+		dst_offset += cur;
+		len -= cur;
+	}
+}
+EXPORT_SYMBOL(memcpy_extent_buffer);
+
+void memmove_extent_buffer(struct extent_buffer *dst, unsigned long dst_offset,
+			   unsigned long src_offset, unsigned long len)
+{
+	size_t cur;
+	size_t dst_off_in_page;
+	size_t src_off_in_page;
+	unsigned long dst_end = dst_offset + len - 1;
+	unsigned long src_end = src_offset + len - 1;
+	size_t start_offset = dst->start & ((u64)PAGE_CACHE_SIZE - 1);
+	unsigned long dst_i;
+	unsigned long src_i;
+
+	if (src_offset + len > dst->len) {
+		printk("memmove bogus src_offset %lu move len %lu len %lu\n",
+		       src_offset, len, dst->len);
+		BUG_ON(1);
+	}
+	if (dst_offset + len > dst->len) {
+		printk("memmove bogus dst_offset %lu move len %lu len %lu\n",
+		       dst_offset, len, dst->len);
+		BUG_ON(1);
+	}
+	if (dst_offset < src_offset) {
+		memcpy_extent_buffer(dst, dst_offset, src_offset, len);
+		return;
+	}
+	while(len > 0) {
+		dst_i = (start_offset + dst_end) >> PAGE_CACHE_SHIFT;
+		src_i = (start_offset + src_end) >> PAGE_CACHE_SHIFT;
+
+		dst_off_in_page = dst_end &
+			((unsigned long)PAGE_CACHE_SIZE - 1);
+		src_off_in_page = src_end &
+			((unsigned long)PAGE_CACHE_SIZE - 1);
+
+		if (src_i == 0)
+			src_off_in_page += start_offset;
+		if (dst_i == 0)
+			dst_off_in_page += start_offset;
+
+		cur = min(len, src_off_in_page + 1);
+		cur = min(cur, dst_off_in_page + 1);
+// printk("move pages orig dst %lu src %lu len %lu, this %lu %lu %lu\n", dst_offset, src_offset, len, dst_off_in_page - cur + 1, src_off_in_page - cur + 1, cur);
+		move_pages(dst->pages[dst_i], dst->pages[src_i],
+			   dst_off_in_page - cur + 1,
+			   src_off_in_page - cur + 1, cur);
+
+		dst_end -= cur - 1;
+		src_end -= cur - 1;
+		len -= cur;
+	}
+}
+EXPORT_SYMBOL(memmove_extent_buffer);

commit b3cfa35a493866cffd00952438b6c77364d42d3c
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Sep 17 11:25:58 2007 -0400

    Btrfs: factor page private preparations into a helper
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index ab5bde31b687..0ab368e091f9 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -1379,6 +1379,16 @@ static int submit_extent_page(int rw, struct extent_map_tree *tree,
 	return ret;
 }
 
+void set_page_extent_mapped(struct page *page)
+{
+	if (!PagePrivate(page)) {
+		SetPagePrivate(page);
+		WARN_ON(!page->mapping->a_ops->invalidatepage);
+		set_page_private(page, 1);
+		page_cache_get(page);
+	}
+}
+
 /*
  * basic readpage implementation.  Locked extent state structs are inserted
  * into the tree that are removed when the IO is done (by the end_io
@@ -1405,12 +1415,7 @@ int extent_read_full_page(struct extent_map_tree *tree, struct page *page,
 	size_t iosize;
 	size_t blocksize = inode->i_sb->s_blocksize;
 
-	if (!PagePrivate(page)) {
-		SetPagePrivate(page);
-		WARN_ON(!page->mapping->a_ops->invalidatepage);
-		set_page_private(page, 1);
-		page_cache_get(page);
-	}
+	set_page_extent_mapped(page);
 
 	end = page_end;
 	lock_extent(tree, start, end, GFP_NOFS);
@@ -1531,12 +1536,7 @@ int extent_write_full_page(struct extent_map_tree *tree, struct page *page,
 			       PAGE_CACHE_SIZE - offset, KM_USER0);
 	}
 
-	if (!PagePrivate(page)) {
-		SetPagePrivate(page);
-		set_page_private(page, 1);
-		WARN_ON(!page->mapping->a_ops->invalidatepage);
-		page_cache_get(page);
-	}
+	set_page_extent_mapped(page);
 
 	lock_extent(tree, start, page_end, GFP_NOFS);
 	nr_delalloc = find_lock_delalloc_range(tree, start, page_end + 1,
@@ -1672,13 +1672,7 @@ int extent_commit_write(struct extent_map_tree *tree,
 {
 	loff_t pos = ((loff_t)page->index << PAGE_CACHE_SHIFT) + to;
 
-	if (!PagePrivate(page)) {
-		SetPagePrivate(page);
-		set_page_private(page, 1);
-		WARN_ON(!page->mapping->a_ops->invalidatepage);
-		page_cache_get(page);
-	}
-
+	set_page_extent_mapped(page);
 	set_page_dirty(page);
 
 	if (pos > inode->i_size) {
@@ -1709,12 +1703,8 @@ int extent_prepare_write(struct extent_map_tree *tree,
 	int ret = 0;
 	int isnew;
 
-	if (!PagePrivate(page)) {
-		SetPagePrivate(page);
-		set_page_private(page, 1);
-		WARN_ON(!page->mapping->a_ops->invalidatepage);
-		page_cache_get(page);
-	}
+	set_page_extent_mapped(page);
+
 	block_start = (page_start + from) & ~((u64)blocksize - 1);
 	block_end = (page_start + to - 1) | (blocksize - 1);
 	orig_block_start = block_start;

commit 0e2752a72cb37075b24899f01e9bc6a589de3b6c
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Sep 10 20:02:33 2007 -0400

    Btrfs: [PATCH] extent_map: add writepage_end_io hook
    
    XFS updates the ondisk inode size only after the data I/O has finished,
    so it needs a hook when the writepage end_bio handler has finished.
    
    Might not be worth applying as-is as the per-page callback is very
    ineffcient.  What XFS really wants is a callback when writeout of a
    whole extent has completed.  This delayed i_size updates scheme might
    be worthwile for btrfs aswell, btw.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 162766773a32..ab5bde31b687 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -1231,6 +1231,8 @@ static int end_bio_extent_writepage(struct bio *bio,
 			end_page_writeback(page);
 		else
 			check_page_writeback(tree, page);
+		if (tree->ops && tree->ops->writepage_end_io_hook)
+			tree->ops->writepage_end_io_hook(page, start, end);
 	} while (bvec >= bio->bi_io_vec);
 
 	bio_put(bio);

commit b06355f0fe04a62d08238438654ec453e05304ba
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Sep 10 20:02:32 2007 -0400

    Btrfs: [PATCH] extent_map: make the writepage_io hook optional
    
    The writepage_io is not mandatory, e.g. my port of xfs to the extent_map
    code does not have one for now.  So handle a NULL pointer gracefully
    here.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index ff2b1dd25bf9..162766773a32 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -1607,7 +1607,12 @@ int extent_write_full_page(struct extent_map_tree *tree, struct page *page,
 			continue;
 		}
 		clear_extent_dirty(tree, cur, cur + iosize - 1, GFP_NOFS);
-		ret = tree->ops->writepage_io_hook(page, cur, cur + iosize - 1);
+		if (tree->ops && tree->ops->writepage_io_hook) {
+			ret = tree->ops->writepage_io_hook(page, cur,
+						cur + iosize - 1);
+		} else {
+			ret = 0;
+		}
 		if (ret)
 			SetPageError(page);
 		else {

commit d396c6f554d182b7bce4e2bb2acb47f80430c05f
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Sep 10 20:02:30 2007 -0400

    Btrfs: [PATCH] extent_map: provide generic bmap
    
    generic_bmap is completely trivial, while the extent to bh mapping in
    btrfs is rather complex.  So provide a extent_bmap instead that takes
    a get_extent callback and can be used by filesystem using the extent_map
    code.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 3021564ac62c..ff2b1dd25bf9 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -1816,3 +1816,22 @@ int try_release_extent_mapping(struct extent_map_tree *tree, struct page *page)
 }
 EXPORT_SYMBOL(try_release_extent_mapping);
 
+sector_t extent_bmap(struct address_space *mapping, sector_t iblock,
+		get_extent_t *get_extent)
+{
+	struct inode *inode = mapping->host;
+	u64 start = iblock << inode->i_blkbits;
+	u64 end = start + (1 << inode->i_blkbits) - 1;
+	struct extent_map *em;
+
+	em = get_extent(inode, NULL, 0, start, end, 0);
+	if (!em || IS_ERR(em))
+		return 0;
+
+	// XXX(hch): block 0 is valid in some cases, e.g. XFS RT device
+	if (em->block_start == EXTENT_MAP_INLINE ||
+	    em->block_start == 0)
+	    	return 0;
+
+	return (em->block_start + start - em->start) >> inode->i_blkbits;
+}

commit 90f1c19a9fd2a943adc69d2b9b8c83bcc4bba6f9
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Sep 10 20:02:27 2007 -0400

    Btrfs: [PATCH] extent_map: fix locking for bio completion
    
    The bio completion handlers can be run in any context, e.g. when using
    the old ide driver they run in hardirq context with irqs disabled so
    lockdep rightfully warns about using write_lock_irq useage in these
    handlers.
    
    This patch switches clear_extent_bit and set_extent_bit to
    write_lock_irqsave to fix this problem.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 370ed97e52f5..3021564ac62c 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -456,6 +456,7 @@ int clear_extent_bit(struct extent_map_tree *tree, u64 start, u64 end,
 	struct extent_state *state;
 	struct extent_state *prealloc = NULL;
 	struct rb_node *node;
+	unsigned long flags;
 	int err;
 	int set = 0;
 
@@ -466,7 +467,7 @@ int clear_extent_bit(struct extent_map_tree *tree, u64 start, u64 end,
 			return -ENOMEM;
 	}
 
-	write_lock_irq(&tree->lock);
+	write_lock_irqsave(&tree->lock, flags);
 	/*
 	 * this search will find the extents that end after
 	 * our range starts
@@ -533,7 +534,7 @@ int clear_extent_bit(struct extent_map_tree *tree, u64 start, u64 end,
 	goto search_again;
 
 out:
-	write_unlock_irq(&tree->lock);
+	write_unlock_irqrestore(&tree->lock, flags);
 	if (prealloc)
 		free_extent_state(prealloc);
 
@@ -542,7 +543,7 @@ int clear_extent_bit(struct extent_map_tree *tree, u64 start, u64 end,
 search_again:
 	if (start >= end)
 		goto out;
-	write_unlock_irq(&tree->lock);
+	write_unlock_irqrestore(&tree->lock, flags);
 	if (mask & __GFP_WAIT)
 		cond_resched();
 	goto again;
@@ -628,6 +629,7 @@ int set_extent_bit(struct extent_map_tree *tree, u64 start, u64 end, int bits,
 	struct extent_state *state;
 	struct extent_state *prealloc = NULL;
 	struct rb_node *node;
+	unsigned long flags;
 	int err = 0;
 	int set;
 	u64 last_start;
@@ -639,7 +641,7 @@ int set_extent_bit(struct extent_map_tree *tree, u64 start, u64 end, int bits,
 			return -ENOMEM;
 	}
 
-	write_lock_irq(&tree->lock);
+	write_lock_irqsave(&tree->lock, flags);
 	/*
 	 * this search will find all the extents that end after
 	 * our range starts.
@@ -759,7 +761,7 @@ int set_extent_bit(struct extent_map_tree *tree, u64 start, u64 end, int bits,
 	goto search_again;
 
 out:
-	write_unlock_irq(&tree->lock);
+	write_unlock_irqrestore(&tree->lock, flags);
 	if (prealloc)
 		free_extent_state(prealloc);
 
@@ -768,7 +770,7 @@ int set_extent_bit(struct extent_map_tree *tree, u64 start, u64 end, int bits,
 search_again:
 	if (start > end)
 		goto out;
-	write_unlock_irq(&tree->lock);
+	write_unlock_irqrestore(&tree->lock, flags);
 	if (mask & __GFP_WAIT)
 		cond_resched();
 	goto again;

commit a8c450b211c010ac55190da23ceb6b39b393f411
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Sep 10 20:00:27 2007 -0400

    Btrfs: Reorder tests in set_extent_bit to properly find holes
    
    Yan Zheng noticed that set_extent_bit was exiting too early when there
    was a hole in the map.  The fix is to reorder the tests to check for the
    hole first.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 33f7a18dddf7..370ed97e52f5 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -712,28 +712,6 @@ int set_extent_bit(struct extent_map_tree *tree, u64 start, u64 end, int bits,
 		}
 		goto search_again;
 	}
-	/*
-	 * | ---- desired range ---- |
-	 *                        | state |
-	 * We need to split the extent, and set the bit
-	 * on the first half
-	 */
-	if (state->start <= end && state->end > end) {
-		set = state->state & bits;
-		if (exclusive && set) {
-			*failed_start = start;
-			err = -EEXIST;
-			goto out;
-		}
-		err = split_state(tree, state, prealloc, end + 1);
-		BUG_ON(err == -EEXIST);
-
-		prealloc->state |= bits;
-		merge_state(tree, prealloc);
-		prealloc = NULL;
-		goto out;
-	}
-
 	/*
 	 * | ---- desired range ---- |
 	 *     | state | or               | state |
@@ -756,6 +734,28 @@ int set_extent_bit(struct extent_map_tree *tree, u64 start, u64 end, int bits,
 		start = this_end + 1;
 		goto search_again;
 	}
+	/*
+	 * | ---- desired range ---- |
+	 *                        | state |
+	 * We need to split the extent, and set the bit
+	 * on the first half
+	 */
+	if (state->start <= end && state->end > end) {
+		set = state->state & bits;
+		if (exclusive && set) {
+			*failed_start = start;
+			err = -EEXIST;
+			goto out;
+		}
+		err = split_state(tree, state, prealloc, end + 1);
+		BUG_ON(err == -EEXIST);
+
+		prealloc->state |= bits;
+		merge_state(tree, prealloc);
+		prealloc = NULL;
+		goto out;
+	}
+
 	goto search_again;
 
 out:

commit 86479a04eef8f304a13aeb8b64bcc8e506a68268
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Sep 10 19:58:16 2007 -0400

    Add support for defragging files via btrfsctl -d.  Avoid OOM on extent tree
    defrag.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 8ad6f8efc5a0..33f7a18dddf7 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -10,6 +10,12 @@
 #include <linux/blkdev.h>
 #include "extent_map.h"
 
+/* temporary define until extent_map moves out of btrfs */
+struct kmem_cache *btrfs_cache_create(const char *name, size_t size,
+				       unsigned long extra_flags,
+				       void (*ctor)(void *, struct kmem_cache *,
+						    unsigned long));
+
 static struct kmem_cache *extent_map_cache;
 static struct kmem_cache *extent_state_cache;
 
@@ -32,14 +38,12 @@ struct tree_entry {
 
 void __init extent_map_init(void)
 {
-	extent_map_cache = kmem_cache_create("extent_map",
-					    sizeof(struct extent_map), 0,
-					    SLAB_RECLAIM_ACCOUNT |
+	extent_map_cache = btrfs_cache_create("extent_map",
+					    sizeof(struct extent_map),
 					    SLAB_DESTROY_BY_RCU,
 					    NULL);
-	extent_state_cache = kmem_cache_create("extent_state",
-					    sizeof(struct extent_state), 0,
-					    SLAB_RECLAIM_ACCOUNT |
+	extent_state_cache = btrfs_cache_create("extent_state",
+					    sizeof(struct extent_state),
 					    SLAB_DESTROY_BY_RCU,
 					    NULL);
 }

commit 2bf5a725a3b82efeaf7b292c085e69a9388a89ea
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Aug 30 11:54:02 2007 -0400

    Btrfs: fsx delalloc fixes
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index 6053f9db0739..8ad6f8efc5a0 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -77,6 +77,8 @@ EXPORT_SYMBOL(alloc_extent_map);
 
 void free_extent_map(struct extent_map *em)
 {
+	if (!em)
+		return;
 	if (atomic_dec_and_test(&em->refs)) {
 		WARN_ON(em->in_tree);
 		kmem_cache_free(extent_map_cache, em);
@@ -102,6 +104,8 @@ EXPORT_SYMBOL(alloc_extent_state);
 
 void free_extent_state(struct extent_state *state)
 {
+	if (!state)
+		return;
 	if (atomic_dec_and_test(&state->refs)) {
 		WARN_ON(state->in_tree);
 		kmem_cache_free(extent_state_cache, state);
@@ -1395,8 +1399,8 @@ int extent_read_full_page(struct extent_map_tree *tree, struct page *page,
 
 	if (!PagePrivate(page)) {
 		SetPagePrivate(page);
-		set_page_private(page, 1);
 		WARN_ON(!page->mapping->a_ops->invalidatepage);
+		set_page_private(page, 1);
 		page_cache_get(page);
 	}
 
@@ -1638,7 +1642,8 @@ int extent_invalidatepage(struct extent_map_tree *tree,
 
 	lock_extent(tree, start, end, GFP_NOFS);
 	wait_on_extent_writeback(tree, start, end);
-	clear_extent_bit(tree, start, end, EXTENT_LOCKED | EXTENT_DIRTY,
+	clear_extent_bit(tree, start, end,
+			 EXTENT_LOCKED | EXTENT_DIRTY | EXTENT_DELALLOC,
 			 1, 1, GFP_NOFS);
 	return 0;
 }

commit 07157aacb1ecd394a5494910b5f80d7e7d407d37
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Aug 30 08:50:51 2007 -0400

    Btrfs: Add file data csums back in via hooks in the extent map code
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index a9c7419615b9..6053f9db0739 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -57,7 +57,7 @@ void extent_map_tree_init(struct extent_map_tree *tree,
 {
 	tree->map.rb_node = NULL;
 	tree->state.rb_node = NULL;
-	tree->fill_delalloc = NULL;
+	tree->ops = NULL;
 	rwlock_init(&tree->lock);
 	tree->mapping = mapping;
 }
@@ -93,6 +93,7 @@ struct extent_state *alloc_extent_state(gfp_t mask)
 		return state;
 	state->state = 0;
 	state->in_tree = 0;
+	state->private = 0;
 	atomic_set(&state->refs, 1);
 	init_waitqueue_head(&state->wq);
 	return state;
@@ -1034,6 +1035,61 @@ int unlock_range(struct extent_map_tree *tree, u64 start, u64 end)
 }
 EXPORT_SYMBOL(unlock_range);
 
+int set_state_private(struct extent_map_tree *tree, u64 start, u64 private)
+{
+	struct rb_node *node;
+	struct extent_state *state;
+	int ret = 0;
+
+	write_lock_irq(&tree->lock);
+	/*
+	 * this search will find all the extents that end after
+	 * our range starts.
+	 */
+	node = tree_search(&tree->state, start);
+	if (!node || IS_ERR(node)) {
+		ret = -ENOENT;
+		goto out;
+	}
+	state = rb_entry(node, struct extent_state, rb_node);
+	if (state->start != start) {
+		ret = -ENOENT;
+		goto out;
+	}
+	state->private = private;
+out:
+	write_unlock_irq(&tree->lock);
+	return ret;
+
+}
+
+int get_state_private(struct extent_map_tree *tree, u64 start, u64 *private)
+{
+	struct rb_node *node;
+	struct extent_state *state;
+	int ret = 0;
+
+	read_lock_irq(&tree->lock);
+	/*
+	 * this search will find all the extents that end after
+	 * our range starts.
+	 */
+	node = tree_search(&tree->state, start);
+	if (!node || IS_ERR(node)) {
+		ret = -ENOENT;
+		goto out;
+	}
+	state = rb_entry(node, struct extent_state, rb_node);
+	if (state->start != start) {
+		ret = -ENOENT;
+		goto out;
+	}
+	*private = state->private;
+out:
+	read_unlock_irq(&tree->lock);
+	return ret;
+}
+
 /*
  * searches a range in the state tree for a given mask.
  * If 'filled' == 1, this returns 1 only if ever extent in the tree
@@ -1185,12 +1241,13 @@ static int end_bio_extent_writepage(struct bio *bio,
 static int end_bio_extent_readpage(struct bio *bio,
 				   unsigned int bytes_done, int err)
 {
-	const int uptodate = test_bit(BIO_UPTODATE, &bio->bi_flags);
+	int uptodate = test_bit(BIO_UPTODATE, &bio->bi_flags);
 	struct bio_vec *bvec = bio->bi_io_vec + bio->bi_vcnt - 1;
 	struct extent_map_tree *tree = bio->bi_private;
 	u64 start;
 	u64 end;
 	int whole_page;
+	int ret;
 
 	if (bio->bi_size)
 		return 1;
@@ -1208,6 +1265,11 @@ static int end_bio_extent_readpage(struct bio *bio,
 		if (--bvec >= bio->bi_io_vec)
 			prefetchw(&bvec->bv_page->flags);
 
+		if (uptodate && tree->ops && tree->ops->readpage_end_io_hook) {
+			ret = tree->ops->readpage_end_io_hook(page, start, end);
+			if (ret)
+				uptodate = 0;
+		}
 		if (uptodate) {
 			set_extent_uptodate(tree, start, end, GFP_ATOMIC);
 			if (whole_page)
@@ -1388,9 +1450,16 @@ int extent_read_full_page(struct extent_map_tree *tree, struct page *page,
 			continue;
 		}
 
-		ret = submit_extent_page(READ, tree, page,
-					 sector, iosize, page_offset, bdev,
-					 end_bio_extent_readpage);
+		ret = 0;
+		if (tree->ops && tree->ops->readpage_io_hook) {
+			ret = tree->ops->readpage_io_hook(page, cur,
+							  cur + iosize - 1);
+		}
+		if (!ret) {
+			ret = submit_extent_page(READ, tree, page,
+						 sector, iosize, page_offset,
+						 bdev, end_bio_extent_readpage);
+		}
 		if (ret)
 			SetPageError(page);
 		cur = cur + iosize;
@@ -1462,7 +1531,7 @@ int extent_write_full_page(struct extent_map_tree *tree, struct page *page,
 					       &delalloc_end,
 					       128 * 1024 * 1024);
 	if (nr_delalloc) {
-		tree->fill_delalloc(inode, start, delalloc_end);
+		tree->ops->fill_delalloc(inode, start, delalloc_end);
 		if (delalloc_end >= page_end + 1) {
 			clear_extent_bit(tree, page_end + 1, delalloc_end,
 					 EXTENT_LOCKED | EXTENT_DELALLOC,
@@ -1528,12 +1597,17 @@ int extent_write_full_page(struct extent_map_tree *tree, struct page *page,
 			continue;
 		}
 		clear_extent_dirty(tree, cur, cur + iosize - 1, GFP_NOFS);
-		set_range_writeback(tree, cur, cur + iosize - 1);
-		ret = submit_extent_page(WRITE, tree, page,
-					 sector, iosize, page_offset, bdev,
-					 end_bio_extent_writepage);
+		ret = tree->ops->writepage_io_hook(page, cur, cur + iosize - 1);
 		if (ret)
 			SetPageError(page);
+		else {
+			set_range_writeback(tree, cur, cur + iosize - 1);
+			ret = submit_extent_page(WRITE, tree, page, sector,
+						 iosize, page_offset, bdev,
+						 end_bio_extent_writepage);
+			if (ret)
+				SetPageError(page);
+		}
 		cur = cur + iosize;
 		page_offset += iosize;
 		nr++;

commit b888db2bd7b67f190b32934e6a86181f262ac3ec
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Aug 27 16:49:44 2007 -0400

    Btrfs: Add delayed allocation to the extent based page tree code
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
index d378edf0964e..a9c7419615b9 100644
--- a/fs/btrfs/extent_map.c
+++ b/fs/btrfs/extent_map.c
@@ -30,9 +30,6 @@ struct tree_entry {
 
 #define EXTENT_IOBITS (EXTENT_LOCKED | EXTENT_WRITEBACK)
 
-static LIST_HEAD(all_states);
-spinlock_t state_lock = SPIN_LOCK_UNLOCKED;
-
 void __init extent_map_init(void)
 {
 	extent_map_cache = kmem_cache_create("extent_map",
@@ -49,15 +46,6 @@ void __init extent_map_init(void)
 
 void __exit extent_map_exit(void)
 {
-	while(!list_empty(&all_states)) {
-		struct extent_state *state;
-		struct list_head *cur = all_states.next;
-		state = list_entry(cur, struct extent_state, list);
-		printk("found leaked state %Lu %Lu state %d in_tree %d\n",
-		       state->start, state->end, state->state, state->in_tree);
-		list_del(&state->list);
-		kfree(state);
-	}
 	if (extent_map_cache)
 		kmem_cache_destroy(extent_map_cache);
 	if (extent_state_cache)
@@ -69,6 +57,7 @@ void extent_map_tree_init(struct extent_map_tree *tree,
 {
 	tree->map.rb_node = NULL;
 	tree->state.rb_node = NULL;
+	tree->fill_delalloc = NULL;
 	rwlock_init(&tree->lock);
 	tree->mapping = mapping;
 }
@@ -106,9 +95,6 @@ struct extent_state *alloc_extent_state(gfp_t mask)
 	state->in_tree = 0;
 	atomic_set(&state->refs, 1);
 	init_waitqueue_head(&state->wq);
-	spin_lock_irq(&state_lock);
-	list_add(&state->list, &all_states);
-	spin_unlock_irq(&state_lock);
 	return state;
 }
 EXPORT_SYMBOL(alloc_extent_state);
@@ -117,9 +103,6 @@ void free_extent_state(struct extent_state *state)
 {
 	if (atomic_dec_and_test(&state->refs)) {
 		WARN_ON(state->in_tree);
-		spin_lock_irq(&state_lock);
-		list_del_init(&state->list);
-		spin_unlock_irq(&state_lock);
 		kmem_cache_free(extent_state_cache, state);
 	}
 }
@@ -369,7 +352,7 @@ static int insert_state(struct extent_map_tree *tree,
 	if (node) {
 		struct extent_state *found;
 		found = rb_entry(node, struct extent_state, rb_node);
-printk("found node %Lu %Lu on insert of %Lu %Lu\n", found->start, found->end, start, end);
+		printk("found node %Lu %Lu on insert of %Lu %Lu\n", found->start, found->end, start, end);
 		free_extent_state(state);
 		return -EEXIST;
 	}
@@ -408,7 +391,7 @@ static int split_state(struct extent_map_tree *tree, struct extent_state *orig,
 	if (node) {
 		struct extent_state *found;
 		found = rb_entry(node, struct extent_state, rb_node);
-printk("found node %Lu %Lu on insert of %Lu %Lu\n", found->start, found->end, prealloc->start, prealloc->end);
+		printk("found node %Lu %Lu on insert of %Lu %Lu\n", found->start, found->end, prealloc->start, prealloc->end);
 		free_extent_state(prealloc);
 		return -EEXIST;
 	}
@@ -792,10 +775,20 @@ int set_extent_dirty(struct extent_map_tree *tree, u64 start, u64 end,
 }
 EXPORT_SYMBOL(set_extent_dirty);
 
+int set_extent_delalloc(struct extent_map_tree *tree, u64 start, u64 end,
+		     gfp_t mask)
+{
+	return set_extent_bit(tree, start, end,
+			      EXTENT_DELALLOC | EXTENT_DIRTY, 0, NULL,
+			      mask);
+}
+EXPORT_SYMBOL(set_extent_delalloc);
+
 int clear_extent_dirty(struct extent_map_tree *tree, u64 start, u64 end,
 		       gfp_t mask)
 {
-	return clear_extent_bit(tree, start, end, EXTENT_DIRTY, 0, 0, mask);
+	return clear_extent_bit(tree, start, end,
+				EXTENT_DIRTY | EXTENT_DELALLOC, 0, 0, mask);
 }
 EXPORT_SYMBOL(clear_extent_dirty);
 
@@ -922,6 +915,62 @@ int set_range_writeback(struct extent_map_tree *tree, u64 start, u64 end)
 }
 EXPORT_SYMBOL(set_range_writeback);
 
+u64 find_lock_delalloc_range(struct extent_map_tree *tree,
+			     u64 start, u64 lock_start, u64 *end, u64 max_bytes)
+{
+	struct rb_node *node;
+	struct extent_state *state;
+	u64 cur_start = start;
+	u64 found = 0;
+	u64 total_bytes = 0;
+
+	write_lock_irq(&tree->lock);
+	/*
+	 * this search will find all the extents that end after
+	 * our range starts.
+	 */
+search_again:
+	node = tree_search(&tree->state, cur_start);
+	if (!node || IS_ERR(node)) {
+		goto out;
+	}
+
+	while(1) {
+		state = rb_entry(node, struct extent_state, rb_node);
+		if (state->start != cur_start) {
+			goto out;
+		}
+		if (!(state->state & EXTENT_DELALLOC)) {
+			goto out;
+		}
+		if (state->start >= lock_start) {
+			if (state->state & EXTENT_LOCKED) {
+				DEFINE_WAIT(wait);
+				atomic_inc(&state->refs);
+				write_unlock_irq(&tree->lock);
+				schedule();
+				write_lock_irq(&tree->lock);
+				finish_wait(&state->wq, &wait);
+				free_extent_state(state);
+				goto search_again;
+			}
+			state->state |= EXTENT_LOCKED;
+		}
+		found++;
+		*end = state->end;
+		cur_start = state->end + 1;
+		node = rb_next(node);
+		if (!node)
+			break;
+		total_bytes = state->end - state->start + 1;
+		if (total_bytes >= max_bytes)
+			break;
+	}
+out:
+	write_unlock_irq(&tree->lock);
+	return found;
+}
+
 /*
  * helper function to lock both pages and extents in the tree.
  * pages must be locked first.
@@ -1285,6 +1334,7 @@ int extent_read_full_page(struct extent_map_tree *tree, struct page *page,
 	if (!PagePrivate(page)) {
 		SetPagePrivate(page);
 		set_page_private(page, 1);
+		WARN_ON(!page->mapping->a_ops->invalidatepage);
 		page_cache_get(page);
 	}
 
@@ -1384,7 +1434,10 @@ int extent_write_full_page(struct extent_map_tree *tree, struct page *page,
 	size_t blocksize;
 	loff_t i_size = i_size_read(inode);
 	unsigned long end_index = i_size >> PAGE_CACHE_SHIFT;
+	u64 nr_delalloc;
+	u64 delalloc_end;
 
+	WARN_ON(!PageLocked(page));
 	if (page->index > end_index) {
 		clear_extent_dirty(tree, start, page_end, GFP_NOFS);
 		unlock_page(page);
@@ -1400,11 +1453,34 @@ int extent_write_full_page(struct extent_map_tree *tree, struct page *page,
 	if (!PagePrivate(page)) {
 		SetPagePrivate(page);
 		set_page_private(page, 1);
+		WARN_ON(!page->mapping->a_ops->invalidatepage);
 		page_cache_get(page);
 	}
 
-	end = page_end;
 	lock_extent(tree, start, page_end, GFP_NOFS);
+	nr_delalloc = find_lock_delalloc_range(tree, start, page_end + 1,
+					       &delalloc_end,
+					       128 * 1024 * 1024);
+	if (nr_delalloc) {
+		tree->fill_delalloc(inode, start, delalloc_end);
+		if (delalloc_end >= page_end + 1) {
+			clear_extent_bit(tree, page_end + 1, delalloc_end,
+					 EXTENT_LOCKED | EXTENT_DELALLOC,
+					 1, 0, GFP_NOFS);
+		}
+		clear_extent_bit(tree, start, page_end, EXTENT_DELALLOC,
+				 0, 0, GFP_NOFS);
+		if (test_range_bit(tree, start, page_end, EXTENT_DELALLOC, 0)) {
+			printk("found delalloc bits after clear extent_bit\n");
+		}
+	} else if (test_range_bit(tree, start, page_end, EXTENT_DELALLOC, 0)) {
+		printk("found delalloc bits after find_delalloc_range returns 0\n");
+	}
+
+	end = page_end;
+	if (test_range_bit(tree, start, page_end, EXTENT_DELALLOC, 0)) {
+		printk("found delalloc bits after lock_extent\n");
+	}
 
 	if (last_byte <= start) {
 		clear_extent_dirty(tree, start, page_end, GFP_NOFS);
@@ -1419,7 +1495,7 @@ int extent_write_full_page(struct extent_map_tree *tree, struct page *page,
 			clear_extent_dirty(tree, cur, page_end, GFP_NOFS);
 			break;
 		}
-		em = get_extent(inode, page, page_offset, cur, end, 1);
+		em = get_extent(inode, page, page_offset, cur, end, 0);
 		if (IS_ERR(em) || !em) {
 			SetPageError(page);
 			break;
@@ -1507,6 +1583,7 @@ int extent_commit_write(struct extent_map_tree *tree,
 	if (!PagePrivate(page)) {
 		SetPagePrivate(page);
 		set_page_private(page, 1);
+		WARN_ON(!page->mapping->a_ops->invalidatepage);
 		page_cache_get(page);
 	}
 
@@ -1543,6 +1620,7 @@ int extent_prepare_write(struct extent_map_tree *tree,
 	if (!PagePrivate(page)) {
 		SetPagePrivate(page);
 		set_page_private(page, 1);
+		WARN_ON(!page->mapping->a_ops->invalidatepage);
 		page_cache_get(page);
 	}
 	block_start = (page_start + from) & ~((u64)blocksize - 1);
@@ -1628,29 +1706,28 @@ int try_release_extent_mapping(struct extent_map_tree *tree, struct page *page)
 	u64 start = page->index << PAGE_CACHE_SHIFT;
 	u64 end = start + PAGE_CACHE_SIZE - 1;
 	u64 orig_start = start;
+	int ret = 1;
 
 	while (start <= end) {
 		em = lookup_extent_mapping(tree, start, end);
 		if (!em || IS_ERR(em))
 			break;
-		if (test_range_bit(tree, em->start, em->end,
-				   EXTENT_LOCKED, 0)) {
+		if (!test_range_bit(tree, em->start, em->end,
+				    EXTENT_LOCKED, 0)) {
+			remove_extent_mapping(tree, em);
+			/* once for the rb tree */
 			free_extent_map(em);
-			start = em->end + 1;
-printk("range still locked %Lu %Lu\n", em->start, em->end);
-			break;
 		}
-		remove_extent_mapping(tree, em);
 		start = em->end + 1;
-		/* once for the rb tree */
-		free_extent_map(em);
 		/* once for us */
 		free_extent_map(em);
 	}
-	WARN_ON(test_range_bit(tree, orig_start, end, EXTENT_WRITEBACK, 0));
-	clear_extent_bit(tree, orig_start, end, EXTENT_UPTODATE,
-			 1, 1, GFP_NOFS);
-	return 1;
+	if (test_range_bit(tree, orig_start, end, EXTENT_LOCKED, 0))
+		ret = 0;
+	else
+		clear_extent_bit(tree, orig_start, end, EXTENT_UPTODATE,
+				 1, 1, GFP_NOFS);
+	return ret;
 }
 EXPORT_SYMBOL(try_release_extent_mapping);
 

commit a52d9a8033c454cd9b4697cfafb467509fc1693f
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Aug 27 16:49:44 2007 -0400

    Btrfs: Extent based page cache code.  This uses an rbtree of extents and tests
    instead of buffer heads.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/extent_map.c b/fs/btrfs/extent_map.c
new file mode 100644
index 000000000000..d378edf0964e
--- /dev/null
+++ b/fs/btrfs/extent_map.c
@@ -0,0 +1,1656 @@
+#include <linux/bitops.h>
+#include <linux/slab.h>
+#include <linux/bio.h>
+#include <linux/mm.h>
+#include <linux/gfp.h>
+#include <linux/pagemap.h>
+#include <linux/page-flags.h>
+#include <linux/module.h>
+#include <linux/spinlock.h>
+#include <linux/blkdev.h>
+#include "extent_map.h"
+
+static struct kmem_cache *extent_map_cache;
+static struct kmem_cache *extent_state_cache;
+
+struct tree_entry {
+	u64 start;
+	u64 end;
+	int in_tree;
+	struct rb_node rb_node;
+};
+
+/* bits for the extent state */
+#define EXTENT_DIRTY 1
+#define EXTENT_WRITEBACK (1 << 1)
+#define EXTENT_UPTODATE (1 << 2)
+#define EXTENT_LOCKED (1 << 3)
+#define EXTENT_NEW (1 << 4)
+#define EXTENT_DELALLOC (1 << 5)
+
+#define EXTENT_IOBITS (EXTENT_LOCKED | EXTENT_WRITEBACK)
+
+static LIST_HEAD(all_states);
+spinlock_t state_lock = SPIN_LOCK_UNLOCKED;
+
+void __init extent_map_init(void)
+{
+	extent_map_cache = kmem_cache_create("extent_map",
+					    sizeof(struct extent_map), 0,
+					    SLAB_RECLAIM_ACCOUNT |
+					    SLAB_DESTROY_BY_RCU,
+					    NULL);
+	extent_state_cache = kmem_cache_create("extent_state",
+					    sizeof(struct extent_state), 0,
+					    SLAB_RECLAIM_ACCOUNT |
+					    SLAB_DESTROY_BY_RCU,
+					    NULL);
+}
+
+void __exit extent_map_exit(void)
+{
+	while(!list_empty(&all_states)) {
+		struct extent_state *state;
+		struct list_head *cur = all_states.next;
+		state = list_entry(cur, struct extent_state, list);
+		printk("found leaked state %Lu %Lu state %d in_tree %d\n",
+		       state->start, state->end, state->state, state->in_tree);
+		list_del(&state->list);
+		kfree(state);
+	}
+	if (extent_map_cache)
+		kmem_cache_destroy(extent_map_cache);
+	if (extent_state_cache)
+		kmem_cache_destroy(extent_state_cache);
+}
+
+void extent_map_tree_init(struct extent_map_tree *tree,
+			  struct address_space *mapping, gfp_t mask)
+{
+	tree->map.rb_node = NULL;
+	tree->state.rb_node = NULL;
+	rwlock_init(&tree->lock);
+	tree->mapping = mapping;
+}
+EXPORT_SYMBOL(extent_map_tree_init);
+
+struct extent_map *alloc_extent_map(gfp_t mask)
+{
+	struct extent_map *em;
+	em = kmem_cache_alloc(extent_map_cache, mask);
+	if (!em || IS_ERR(em))
+		return em;
+	em->in_tree = 0;
+	atomic_set(&em->refs, 1);
+	return em;
+}
+EXPORT_SYMBOL(alloc_extent_map);
+
+void free_extent_map(struct extent_map *em)
+{
+	if (atomic_dec_and_test(&em->refs)) {
+		WARN_ON(em->in_tree);
+		kmem_cache_free(extent_map_cache, em);
+	}
+}
+EXPORT_SYMBOL(free_extent_map);
+
+
+struct extent_state *alloc_extent_state(gfp_t mask)
+{
+	struct extent_state *state;
+	state = kmem_cache_alloc(extent_state_cache, mask);
+	if (!state || IS_ERR(state))
+		return state;
+	state->state = 0;
+	state->in_tree = 0;
+	atomic_set(&state->refs, 1);
+	init_waitqueue_head(&state->wq);
+	spin_lock_irq(&state_lock);
+	list_add(&state->list, &all_states);
+	spin_unlock_irq(&state_lock);
+	return state;
+}
+EXPORT_SYMBOL(alloc_extent_state);
+
+void free_extent_state(struct extent_state *state)
+{
+	if (atomic_dec_and_test(&state->refs)) {
+		WARN_ON(state->in_tree);
+		spin_lock_irq(&state_lock);
+		list_del_init(&state->list);
+		spin_unlock_irq(&state_lock);
+		kmem_cache_free(extent_state_cache, state);
+	}
+}
+EXPORT_SYMBOL(free_extent_state);
+
+static struct rb_node *tree_insert(struct rb_root *root, u64 offset,
+				   struct rb_node *node)
+{
+	struct rb_node ** p = &root->rb_node;
+	struct rb_node * parent = NULL;
+	struct tree_entry *entry;
+
+	while(*p) {
+		parent = *p;
+		entry = rb_entry(parent, struct tree_entry, rb_node);
+
+		if (offset < entry->start)
+			p = &(*p)->rb_left;
+		else if (offset > entry->end)
+			p = &(*p)->rb_right;
+		else
+			return parent;
+	}
+
+	entry = rb_entry(node, struct tree_entry, rb_node);
+	entry->in_tree = 1;
+	rb_link_node(node, parent, p);
+	rb_insert_color(node, root);
+	return NULL;
+}
+
+static struct rb_node *__tree_search(struct rb_root *root, u64 offset,
+				   struct rb_node **prev_ret)
+{
+	struct rb_node * n = root->rb_node;
+	struct rb_node *prev = NULL;
+	struct tree_entry *entry;
+	struct tree_entry *prev_entry = NULL;
+
+	while(n) {
+		entry = rb_entry(n, struct tree_entry, rb_node);
+		prev = n;
+		prev_entry = entry;
+
+		if (offset < entry->start)
+			n = n->rb_left;
+		else if (offset > entry->end)
+			n = n->rb_right;
+		else
+			return n;
+	}
+	if (!prev_ret)
+		return NULL;
+	while(prev && offset > prev_entry->end) {
+		prev = rb_next(prev);
+		prev_entry = rb_entry(prev, struct tree_entry, rb_node);
+	}
+	*prev_ret = prev;
+	return NULL;
+}
+
+static inline struct rb_node *tree_search(struct rb_root *root, u64 offset)
+{
+	struct rb_node *prev;
+	struct rb_node *ret;
+	ret = __tree_search(root, offset, &prev);
+	if (!ret)
+		return prev;
+	return ret;
+}
+
+static int tree_delete(struct rb_root *root, u64 offset)
+{
+	struct rb_node *node;
+	struct tree_entry *entry;
+
+	node = __tree_search(root, offset, NULL);
+	if (!node)
+		return -ENOENT;
+	entry = rb_entry(node, struct tree_entry, rb_node);
+	entry->in_tree = 0;
+	rb_erase(node, root);
+	return 0;
+}
+
+/*
+ * add_extent_mapping tries a simple backward merge with existing
+ * mappings.  The extent_map struct passed in will be inserted into
+ * the tree directly (no copies made, just a reference taken).
+ */
+int add_extent_mapping(struct extent_map_tree *tree,
+		       struct extent_map *em)
+{
+	int ret = 0;
+	struct extent_map *prev = NULL;
+	struct rb_node *rb;
+
+	write_lock_irq(&tree->lock);
+	rb = tree_insert(&tree->map, em->end, &em->rb_node);
+	if (rb) {
+		prev = rb_entry(rb, struct extent_map, rb_node);
+		printk("found extent map %Lu %Lu on insert of %Lu %Lu\n", prev->start, prev->end, em->start, em->end);
+		ret = -EEXIST;
+		goto out;
+	}
+	atomic_inc(&em->refs);
+	if (em->start != 0) {
+		rb = rb_prev(&em->rb_node);
+		if (rb)
+			prev = rb_entry(rb, struct extent_map, rb_node);
+		if (prev && prev->end + 1 == em->start &&
+		    ((em->block_start == 0 && prev->block_start == 0) ||
+			     (em->block_start == prev->block_end + 1))) {
+			em->start = prev->start;
+			em->block_start = prev->block_start;
+			rb_erase(&prev->rb_node, &tree->map);
+			prev->in_tree = 0;
+			free_extent_map(prev);
+		}
+	 }
+out:
+	write_unlock_irq(&tree->lock);
+	return ret;
+}
+EXPORT_SYMBOL(add_extent_mapping);
+
+/*
+ * lookup_extent_mapping returns the first extent_map struct in the
+ * tree that intersects the [start, end] (inclusive) range.  There may
+ * be additional objects in the tree that intersect, so check the object
+ * returned carefully to make sure you don't need additional lookups.
+ */
+struct extent_map *lookup_extent_mapping(struct extent_map_tree *tree,
+					 u64 start, u64 end)
+{
+	struct extent_map *em;
+	struct rb_node *rb_node;
+
+	read_lock_irq(&tree->lock);
+	rb_node = tree_search(&tree->map, start);
+	if (!rb_node) {
+		em = NULL;
+		goto out;
+	}
+	if (IS_ERR(rb_node)) {
+		em = ERR_PTR(PTR_ERR(rb_node));
+		goto out;
+	}
+	em = rb_entry(rb_node, struct extent_map, rb_node);
+	if (em->end < start || em->start > end) {
+		em = NULL;
+		goto out;
+	}
+	atomic_inc(&em->refs);
+out:
+	read_unlock_irq(&tree->lock);
+	return em;
+}
+EXPORT_SYMBOL(lookup_extent_mapping);
+
+/*
+ * removes an extent_map struct from the tree.  No reference counts are
+ * dropped, and no checks are done to  see if the range is in use
+ */
+int remove_extent_mapping(struct extent_map_tree *tree, struct extent_map *em)
+{
+	int ret;
+
+	write_lock_irq(&tree->lock);
+	ret = tree_delete(&tree->map, em->end);
+	write_unlock_irq(&tree->lock);
+	return ret;
+}
+EXPORT_SYMBOL(remove_extent_mapping);
+
+/*
+ * utility function to look for merge candidates inside a given range.
+ * Any extents with matching state are merged together into a single
+ * extent in the tree.  Extents with EXTENT_IO in their state field
+ * are not merged because the end_io handlers need to be able to do
+ * operations on them without sleeping (or doing allocations/splits).
+ *
+ * This should be called with the tree lock held.
+ */
+static int merge_state(struct extent_map_tree *tree,
+		       struct extent_state *state)
+{
+	struct extent_state *other;
+	struct rb_node *other_node;
+
+	if (state->state & EXTENT_IOBITS)
+		return 0;
+
+	other_node = rb_prev(&state->rb_node);
+	if (other_node) {
+		other = rb_entry(other_node, struct extent_state, rb_node);
+		if (other->end == state->start - 1 &&
+		    other->state == state->state) {
+			state->start = other->start;
+			other->in_tree = 0;
+			rb_erase(&other->rb_node, &tree->state);
+			free_extent_state(other);
+		}
+	}
+	other_node = rb_next(&state->rb_node);
+	if (other_node) {
+		other = rb_entry(other_node, struct extent_state, rb_node);
+		if (other->start == state->end + 1 &&
+		    other->state == state->state) {
+			other->start = state->start;
+			state->in_tree = 0;
+			rb_erase(&state->rb_node, &tree->state);
+			free_extent_state(state);
+		}
+	}
+	return 0;
+}
+
+/*
+ * insert an extent_state struct into the tree.  'bits' are set on the
+ * struct before it is inserted.
+ *
+ * This may return -EEXIST if the extent is already there, in which case the
+ * state struct is freed.
+ *
+ * The tree lock is not taken internally.  This is a utility function and
+ * probably isn't what you want to call (see set/clear_extent_bit).
+ */
+static int insert_state(struct extent_map_tree *tree,
+			struct extent_state *state, u64 start, u64 end,
+			int bits)
+{
+	struct rb_node *node;
+
+	if (end < start) {
+		printk("end < start %Lu %Lu\n", end, start);
+		WARN_ON(1);
+	}
+	state->state |= bits;
+	state->start = start;
+	state->end = end;
+	if ((end & 4095) == 0) {
+		printk("insert state %Lu %Lu strange end\n", start, end);
+		WARN_ON(1);
+	}
+	node = tree_insert(&tree->state, end, &state->rb_node);
+	if (node) {
+		struct extent_state *found;
+		found = rb_entry(node, struct extent_state, rb_node);
+printk("found node %Lu %Lu on insert of %Lu %Lu\n", found->start, found->end, start, end);
+		free_extent_state(state);
+		return -EEXIST;
+	}
+	merge_state(tree, state);
+	return 0;
+}
+
+/*
+ * split a given extent state struct in two, inserting the preallocated
+ * struct 'prealloc' as the newly created second half.  'split' indicates an
+ * offset inside 'orig' where it should be split.
+ *
+ * Before calling,
+ * the tree has 'orig' at [orig->start, orig->end].  After calling, there
+ * are two extent state structs in the tree:
+ * prealloc: [orig->start, split - 1]
+ * orig: [ split, orig->end ]
+ *
+ * The tree locks are not taken by this function. They need to be held
+ * by the caller.
+ */
+static int split_state(struct extent_map_tree *tree, struct extent_state *orig,
+		       struct extent_state *prealloc, u64 split)
+{
+	struct rb_node *node;
+	prealloc->start = orig->start;
+	prealloc->end = split - 1;
+	prealloc->state = orig->state;
+	orig->start = split;
+	if ((prealloc->end & 4095) == 0) {
+		printk("insert state %Lu %Lu strange end\n", prealloc->start,
+		       prealloc->end);
+		WARN_ON(1);
+	}
+	node = tree_insert(&tree->state, prealloc->end, &prealloc->rb_node);
+	if (node) {
+		struct extent_state *found;
+		found = rb_entry(node, struct extent_state, rb_node);
+printk("found node %Lu %Lu on insert of %Lu %Lu\n", found->start, found->end, prealloc->start, prealloc->end);
+		free_extent_state(prealloc);
+		return -EEXIST;
+	}
+	return 0;
+}
+
+/*
+ * utility function to clear some bits in an extent state struct.
+ * it will optionally wake up any one waiting on this state (wake == 1), or
+ * forcibly remove the state from the tree (delete == 1).
+ *
+ * If no bits are set on the state struct after clearing things, the
+ * struct is freed and removed from the tree
+ */
+static int clear_state_bit(struct extent_map_tree *tree,
+			    struct extent_state *state, int bits, int wake,
+			    int delete)
+{
+	int ret = state->state & bits;
+	state->state &= ~bits;
+	if (wake)
+		wake_up(&state->wq);
+	if (delete || state->state == 0) {
+		if (state->in_tree) {
+			rb_erase(&state->rb_node, &tree->state);
+			state->in_tree = 0;
+			free_extent_state(state);
+		} else {
+			WARN_ON(1);
+		}
+	} else {
+		merge_state(tree, state);
+	}
+	return ret;
+}
+
+/*
+ * clear some bits on a range in the tree.  This may require splitting
+ * or inserting elements in the tree, so the gfp mask is used to
+ * indicate which allocations or sleeping are allowed.
+ *
+ * pass 'wake' == 1 to kick any sleepers, and 'delete' == 1 to remove
+ * the given range from the tree regardless of state (ie for truncate).
+ *
+ * the range [start, end] is inclusive.
+ *
+ * This takes the tree lock, and returns < 0 on error, > 0 if any of the
+ * bits were already set, or zero if none of the bits were already set.
+ */
+int clear_extent_bit(struct extent_map_tree *tree, u64 start, u64 end,
+		     int bits, int wake, int delete, gfp_t mask)
+{
+	struct extent_state *state;
+	struct extent_state *prealloc = NULL;
+	struct rb_node *node;
+	int err;
+	int set = 0;
+
+again:
+	if (!prealloc && (mask & __GFP_WAIT)) {
+		prealloc = alloc_extent_state(mask);
+		if (!prealloc)
+			return -ENOMEM;
+	}
+
+	write_lock_irq(&tree->lock);
+	/*
+	 * this search will find the extents that end after
+	 * our range starts
+	 */
+	node = tree_search(&tree->state, start);
+	if (!node)
+		goto out;
+	state = rb_entry(node, struct extent_state, rb_node);
+	if (state->start > end)
+		goto out;
+	WARN_ON(state->end < start);
+
+	/*
+	 *     | ---- desired range ---- |
+	 *  | state | or
+	 *  | ------------- state -------------- |
+	 *
+	 * We need to split the extent we found, and may flip
+	 * bits on second half.
+	 *
+	 * If the extent we found extends past our range, we
+	 * just split and search again.  It'll get split again
+	 * the next time though.
+	 *
+	 * If the extent we found is inside our range, we clear
+	 * the desired bit on it.
+	 */
+
+	if (state->start < start) {
+		err = split_state(tree, state, prealloc, start);
+		BUG_ON(err == -EEXIST);
+		prealloc = NULL;
+		if (err)
+			goto out;
+		if (state->end <= end) {
+			start = state->end + 1;
+			set |= clear_state_bit(tree, state, bits,
+					wake, delete);
+		} else {
+			start = state->start;
+		}
+		goto search_again;
+	}
+	/*
+	 * | ---- desired range ---- |
+	 *                        | state |
+	 * We need to split the extent, and clear the bit
+	 * on the first half
+	 */
+	if (state->start <= end && state->end > end) {
+		err = split_state(tree, state, prealloc, end + 1);
+		BUG_ON(err == -EEXIST);
+
+		if (wake)
+			wake_up(&state->wq);
+		set |= clear_state_bit(tree, prealloc, bits,
+				       wake, delete);
+		prealloc = NULL;
+		goto out;
+	}
+
+	start = state->end + 1;
+	set |= clear_state_bit(tree, state, bits, wake, delete);
+	goto search_again;
+
+out:
+	write_unlock_irq(&tree->lock);
+	if (prealloc)
+		free_extent_state(prealloc);
+
+	return set;
+
+search_again:
+	if (start >= end)
+		goto out;
+	write_unlock_irq(&tree->lock);
+	if (mask & __GFP_WAIT)
+		cond_resched();
+	goto again;
+}
+EXPORT_SYMBOL(clear_extent_bit);
+
+static int wait_on_state(struct extent_map_tree *tree,
+			 struct extent_state *state)
+{
+	DEFINE_WAIT(wait);
+	prepare_to_wait(&state->wq, &wait, TASK_UNINTERRUPTIBLE);
+	read_unlock_irq(&tree->lock);
+	schedule();
+	read_lock_irq(&tree->lock);
+	finish_wait(&state->wq, &wait);
+	return 0;
+}
+
+/*
+ * waits for one or more bits to clear on a range in the state tree.
+ * The range [start, end] is inclusive.
+ * The tree lock is taken by this function
+ */
+int wait_extent_bit(struct extent_map_tree *tree, u64 start, u64 end, int bits)
+{
+	struct extent_state *state;
+	struct rb_node *node;
+
+	read_lock_irq(&tree->lock);
+again:
+	while (1) {
+		/*
+		 * this search will find all the extents that end after
+		 * our range starts
+		 */
+		node = tree_search(&tree->state, start);
+		if (!node)
+			break;
+
+		state = rb_entry(node, struct extent_state, rb_node);
+
+		if (state->start > end)
+			goto out;
+
+		if (state->state & bits) {
+			start = state->start;
+			atomic_inc(&state->refs);
+			wait_on_state(tree, state);
+			free_extent_state(state);
+			goto again;
+		}
+		start = state->end + 1;
+
+		if (start > end)
+			break;
+
+		if (need_resched()) {
+			read_unlock_irq(&tree->lock);
+			cond_resched();
+			read_lock_irq(&tree->lock);
+		}
+	}
+out:
+	read_unlock_irq(&tree->lock);
+	return 0;
+}
+EXPORT_SYMBOL(wait_extent_bit);
+
+/*
+ * set some bits on a range in the tree.  This may require allocations
+ * or sleeping, so the gfp mask is used to indicate what is allowed.
+ *
+ * If 'exclusive' == 1, this will fail with -EEXIST if some part of the
+ * range already has the desired bits set.  The start of the existing
+ * range is returned in failed_start in this case.
+ *
+ * [start, end] is inclusive
+ * This takes the tree lock.
+ */
+int set_extent_bit(struct extent_map_tree *tree, u64 start, u64 end, int bits,
+		   int exclusive, u64 *failed_start, gfp_t mask)
+{
+	struct extent_state *state;
+	struct extent_state *prealloc = NULL;
+	struct rb_node *node;
+	int err = 0;
+	int set;
+	u64 last_start;
+	u64 last_end;
+again:
+	if (!prealloc && (mask & __GFP_WAIT)) {
+		prealloc = alloc_extent_state(mask);
+		if (!prealloc)
+			return -ENOMEM;
+	}
+
+	write_lock_irq(&tree->lock);
+	/*
+	 * this search will find all the extents that end after
+	 * our range starts.
+	 */
+	node = tree_search(&tree->state, start);
+	if (!node) {
+		err = insert_state(tree, prealloc, start, end, bits);
+		prealloc = NULL;
+		BUG_ON(err == -EEXIST);
+		goto out;
+	}
+
+	state = rb_entry(node, struct extent_state, rb_node);
+	last_start = state->start;
+	last_end = state->end;
+
+	/*
+	 * | ---- desired range ---- |
+	 * | state |
+	 *
+	 * Just lock what we found and keep going
+	 */
+	if (state->start == start && state->end <= end) {
+		set = state->state & bits;
+		if (set && exclusive) {
+			*failed_start = state->start;
+			err = -EEXIST;
+			goto out;
+		}
+		state->state |= bits;
+		start = state->end + 1;
+		merge_state(tree, state);
+		goto search_again;
+	}
+
+	/*
+	 *     | ---- desired range ---- |
+	 * | state |
+	 *   or
+	 * | ------------- state -------------- |
+	 *
+	 * We need to split the extent we found, and may flip bits on
+	 * second half.
+	 *
+	 * If the extent we found extends past our
+	 * range, we just split and search again.  It'll get split
+	 * again the next time though.
+	 *
+	 * If the extent we found is inside our range, we set the
+	 * desired bit on it.
+	 */
+	if (state->start < start) {
+		set = state->state & bits;
+		if (exclusive && set) {
+			*failed_start = start;
+			err = -EEXIST;
+			goto out;
+		}
+		err = split_state(tree, state, prealloc, start);
+		BUG_ON(err == -EEXIST);
+		prealloc = NULL;
+		if (err)
+			goto out;
+		if (state->end <= end) {
+			state->state |= bits;
+			start = state->end + 1;
+			merge_state(tree, state);
+		} else {
+			start = state->start;
+		}
+		goto search_again;
+	}
+	/*
+	 * | ---- desired range ---- |
+	 *                        | state |
+	 * We need to split the extent, and set the bit
+	 * on the first half
+	 */
+	if (state->start <= end && state->end > end) {
+		set = state->state & bits;
+		if (exclusive && set) {
+			*failed_start = start;
+			err = -EEXIST;
+			goto out;
+		}
+		err = split_state(tree, state, prealloc, end + 1);
+		BUG_ON(err == -EEXIST);
+
+		prealloc->state |= bits;
+		merge_state(tree, prealloc);
+		prealloc = NULL;
+		goto out;
+	}
+
+	/*
+	 * | ---- desired range ---- |
+	 *     | state | or               | state |
+	 *
+	 * There's a hole, we need to insert something in it and
+	 * ignore the extent we found.
+	 */
+	if (state->start > start) {
+		u64 this_end;
+		if (end < last_start)
+			this_end = end;
+		else
+			this_end = last_start -1;
+		err = insert_state(tree, prealloc, start, this_end,
+				   bits);
+		prealloc = NULL;
+		BUG_ON(err == -EEXIST);
+		if (err)
+			goto out;
+		start = this_end + 1;
+		goto search_again;
+	}
+	goto search_again;
+
+out:
+	write_unlock_irq(&tree->lock);
+	if (prealloc)
+		free_extent_state(prealloc);
+
+	return err;
+
+search_again:
+	if (start > end)
+		goto out;
+	write_unlock_irq(&tree->lock);
+	if (mask & __GFP_WAIT)
+		cond_resched();
+	goto again;
+}
+EXPORT_SYMBOL(set_extent_bit);
+
+/* wrappers around set/clear extent bit */
+int set_extent_dirty(struct extent_map_tree *tree, u64 start, u64 end,
+		     gfp_t mask)
+{
+	return set_extent_bit(tree, start, end, EXTENT_DIRTY, 0, NULL,
+			      mask);
+}
+EXPORT_SYMBOL(set_extent_dirty);
+
+int clear_extent_dirty(struct extent_map_tree *tree, u64 start, u64 end,
+		       gfp_t mask)
+{
+	return clear_extent_bit(tree, start, end, EXTENT_DIRTY, 0, 0, mask);
+}
+EXPORT_SYMBOL(clear_extent_dirty);
+
+int set_extent_new(struct extent_map_tree *tree, u64 start, u64 end,
+		     gfp_t mask)
+{
+	return set_extent_bit(tree, start, end, EXTENT_NEW, 0, NULL,
+			      mask);
+}
+EXPORT_SYMBOL(set_extent_new);
+
+int clear_extent_new(struct extent_map_tree *tree, u64 start, u64 end,
+		       gfp_t mask)
+{
+	return clear_extent_bit(tree, start, end, EXTENT_NEW, 0, 0, mask);
+}
+EXPORT_SYMBOL(clear_extent_new);
+
+int set_extent_uptodate(struct extent_map_tree *tree, u64 start, u64 end,
+			gfp_t mask)
+{
+	return set_extent_bit(tree, start, end, EXTENT_UPTODATE, 0, NULL,
+			      mask);
+}
+EXPORT_SYMBOL(set_extent_uptodate);
+
+int clear_extent_uptodate(struct extent_map_tree *tree, u64 start, u64 end,
+			  gfp_t mask)
+{
+	return clear_extent_bit(tree, start, end, EXTENT_UPTODATE, 0, 0, mask);
+}
+EXPORT_SYMBOL(clear_extent_uptodate);
+
+int set_extent_writeback(struct extent_map_tree *tree, u64 start, u64 end,
+			 gfp_t mask)
+{
+	return set_extent_bit(tree, start, end, EXTENT_WRITEBACK,
+			      0, NULL, mask);
+}
+EXPORT_SYMBOL(set_extent_writeback);
+
+int clear_extent_writeback(struct extent_map_tree *tree, u64 start, u64 end,
+			   gfp_t mask)
+{
+	return clear_extent_bit(tree, start, end, EXTENT_WRITEBACK, 1, 0, mask);
+}
+EXPORT_SYMBOL(clear_extent_writeback);
+
+int wait_on_extent_writeback(struct extent_map_tree *tree, u64 start, u64 end)
+{
+	return wait_extent_bit(tree, start, end, EXTENT_WRITEBACK);
+}
+EXPORT_SYMBOL(wait_on_extent_writeback);
+
+/*
+ * locks a range in ascending order, waiting for any locked regions
+ * it hits on the way.  [start,end] are inclusive, and this will sleep.
+ */
+int lock_extent(struct extent_map_tree *tree, u64 start, u64 end, gfp_t mask)
+{
+	int err;
+	u64 failed_start;
+	while (1) {
+		err = set_extent_bit(tree, start, end, EXTENT_LOCKED, 1,
+				     &failed_start, mask);
+		if (err == -EEXIST && (mask & __GFP_WAIT)) {
+			wait_extent_bit(tree, failed_start, end, EXTENT_LOCKED);
+			start = failed_start;
+		} else {
+			break;
+		}
+		WARN_ON(start > end);
+	}
+	return err;
+}
+EXPORT_SYMBOL(lock_extent);
+
+int unlock_extent(struct extent_map_tree *tree, u64 start, u64 end,
+		  gfp_t mask)
+{
+	return clear_extent_bit(tree, start, end, EXTENT_LOCKED, 1, 0, mask);
+}
+EXPORT_SYMBOL(unlock_extent);
+
+/*
+ * helper function to set pages and extents in the tree dirty
+ */
+int set_range_dirty(struct extent_map_tree *tree, u64 start, u64 end)
+{
+	unsigned long index = start >> PAGE_CACHE_SHIFT;
+	unsigned long end_index = end >> PAGE_CACHE_SHIFT;
+	struct page *page;
+
+	while (index <= end_index) {
+		page = find_get_page(tree->mapping, index);
+		BUG_ON(!page);
+		__set_page_dirty_nobuffers(page);
+		page_cache_release(page);
+		index++;
+	}
+	set_extent_dirty(tree, start, end, GFP_NOFS);
+	return 0;
+}
+EXPORT_SYMBOL(set_range_dirty);
+
+/*
+ * helper function to set both pages and extents in the tree writeback
+ */
+int set_range_writeback(struct extent_map_tree *tree, u64 start, u64 end)
+{
+	unsigned long index = start >> PAGE_CACHE_SHIFT;
+	unsigned long end_index = end >> PAGE_CACHE_SHIFT;
+	struct page *page;
+
+	while (index <= end_index) {
+		page = find_get_page(tree->mapping, index);
+		BUG_ON(!page);
+		set_page_writeback(page);
+		page_cache_release(page);
+		index++;
+	}
+	set_extent_writeback(tree, start, end, GFP_NOFS);
+	return 0;
+}
+EXPORT_SYMBOL(set_range_writeback);
+
+/*
+ * helper function to lock both pages and extents in the tree.
+ * pages must be locked first.
+ */
+int lock_range(struct extent_map_tree *tree, u64 start, u64 end)
+{
+	unsigned long index = start >> PAGE_CACHE_SHIFT;
+	unsigned long end_index = end >> PAGE_CACHE_SHIFT;
+	struct page *page;
+	int err;
+
+	while (index <= end_index) {
+		page = grab_cache_page(tree->mapping, index);
+		if (!page) {
+			err = -ENOMEM;
+			goto failed;
+		}
+		if (IS_ERR(page)) {
+			err = PTR_ERR(page);
+			goto failed;
+		}
+		index++;
+	}
+	lock_extent(tree, start, end, GFP_NOFS);
+	return 0;
+
+failed:
+	/*
+	 * we failed above in getting the page at 'index', so we undo here
+	 * up to but not including the page at 'index'
+	 */
+	end_index = index;
+	index = start >> PAGE_CACHE_SHIFT;
+	while (index < end_index) {
+		page = find_get_page(tree->mapping, index);
+		unlock_page(page);
+		page_cache_release(page);
+		index++;
+	}
+	return err;
+}
+EXPORT_SYMBOL(lock_range);
+
+/*
+ * helper function to unlock both pages and extents in the tree.
+ */
+int unlock_range(struct extent_map_tree *tree, u64 start, u64 end)
+{
+	unsigned long index = start >> PAGE_CACHE_SHIFT;
+	unsigned long end_index = end >> PAGE_CACHE_SHIFT;
+	struct page *page;
+
+	while (index <= end_index) {
+		page = find_get_page(tree->mapping, index);
+		unlock_page(page);
+		page_cache_release(page);
+		index++;
+	}
+	unlock_extent(tree, start, end, GFP_NOFS);
+	return 0;
+}
+EXPORT_SYMBOL(unlock_range);
+
+/*
+ * searches a range in the state tree for a given mask.
+ * If 'filled' == 1, this returns 1 only if ever extent in the tree
+ * has the bits set.  Otherwise, 1 is returned if any bit in the
+ * range is found set.
+ */
+static int test_range_bit(struct extent_map_tree *tree, u64 start, u64 end,
+			  int bits, int filled)
+{
+	struct extent_state *state = NULL;
+	struct rb_node *node;
+	int bitset = 0;
+
+	read_lock_irq(&tree->lock);
+	node = tree_search(&tree->state, start);
+	while (node && start <= end) {
+		state = rb_entry(node, struct extent_state, rb_node);
+		if (state->start > end)
+			break;
+
+		if (filled && state->start > start) {
+			bitset = 0;
+			break;
+		}
+		if (state->state & bits) {
+			bitset = 1;
+			if (!filled)
+				break;
+		} else if (filled) {
+			bitset = 0;
+			break;
+		}
+		start = state->end + 1;
+		if (start > end)
+			break;
+		node = rb_next(node);
+	}
+	read_unlock_irq(&tree->lock);
+	return bitset;
+}
+
+/*
+ * helper function to set a given page up to date if all the
+ * extents in the tree for that page are up to date
+ */
+static int check_page_uptodate(struct extent_map_tree *tree,
+			       struct page *page)
+{
+	u64 start = page->index << PAGE_CACHE_SHIFT;
+	u64 end = start + PAGE_CACHE_SIZE - 1;
+	if (test_range_bit(tree, start, end, EXTENT_UPTODATE, 1))
+		SetPageUptodate(page);
+	return 0;
+}
+
+/*
+ * helper function to unlock a page if all the extents in the tree
+ * for that page are unlocked
+ */
+static int check_page_locked(struct extent_map_tree *tree,
+			     struct page *page)
+{
+	u64 start = page->index << PAGE_CACHE_SHIFT;
+	u64 end = start + PAGE_CACHE_SIZE - 1;
+	if (!test_range_bit(tree, start, end, EXTENT_LOCKED, 0))
+		unlock_page(page);
+	return 0;
+}
+
+/*
+ * helper function to end page writeback if all the extents
+ * in the tree for that page are done with writeback
+ */
+static int check_page_writeback(struct extent_map_tree *tree,
+			     struct page *page)
+{
+	u64 start = page->index << PAGE_CACHE_SHIFT;
+	u64 end = start + PAGE_CACHE_SIZE - 1;
+	if (!test_range_bit(tree, start, end, EXTENT_WRITEBACK, 0))
+		end_page_writeback(page);
+	return 0;
+}
+
+/* lots and lots of room for performance fixes in the end_bio funcs */
+
+/*
+ * after a writepage IO is done, we need to:
+ * clear the uptodate bits on error
+ * clear the writeback bits in the extent tree for this IO
+ * end_page_writeback if the page has no more pending IO
+ *
+ * Scheduling is not allowed, so the extent state tree is expected
+ * to have one and only one object corresponding to this IO.
+ */
+static int end_bio_extent_writepage(struct bio *bio,
+				   unsigned int bytes_done, int err)
+{
+	const int uptodate = test_bit(BIO_UPTODATE, &bio->bi_flags);
+	struct bio_vec *bvec = bio->bi_io_vec + bio->bi_vcnt - 1;
+	struct extent_map_tree *tree = bio->bi_private;
+	u64 start;
+	u64 end;
+	int whole_page;
+
+	if (bio->bi_size)
+		return 1;
+
+	do {
+		struct page *page = bvec->bv_page;
+		start = (page->index << PAGE_CACHE_SHIFT) + bvec->bv_offset;
+		end = start + bvec->bv_len - 1;
+
+		if (bvec->bv_offset == 0 && bvec->bv_len == PAGE_CACHE_SIZE)
+			whole_page = 1;
+		else
+			whole_page = 0;
+
+		if (--bvec >= bio->bi_io_vec)
+			prefetchw(&bvec->bv_page->flags);
+
+		if (!uptodate) {
+			clear_extent_uptodate(tree, start, end, GFP_ATOMIC);
+			ClearPageUptodate(page);
+			SetPageError(page);
+		}
+		clear_extent_writeback(tree, start, end, GFP_ATOMIC);
+
+		if (whole_page)
+			end_page_writeback(page);
+		else
+			check_page_writeback(tree, page);
+	} while (bvec >= bio->bi_io_vec);
+
+	bio_put(bio);
+	return 0;
+}
+
+/*
+ * after a readpage IO is done, we need to:
+ * clear the uptodate bits on error
+ * set the uptodate bits if things worked
+ * set the page up to date if all extents in the tree are uptodate
+ * clear the lock bit in the extent tree
+ * unlock the page if there are no other extents locked for it
+ *
+ * Scheduling is not allowed, so the extent state tree is expected
+ * to have one and only one object corresponding to this IO.
+ */
+static int end_bio_extent_readpage(struct bio *bio,
+				   unsigned int bytes_done, int err)
+{
+	const int uptodate = test_bit(BIO_UPTODATE, &bio->bi_flags);
+	struct bio_vec *bvec = bio->bi_io_vec + bio->bi_vcnt - 1;
+	struct extent_map_tree *tree = bio->bi_private;
+	u64 start;
+	u64 end;
+	int whole_page;
+
+	if (bio->bi_size)
+		return 1;
+
+	do {
+		struct page *page = bvec->bv_page;
+		start = (page->index << PAGE_CACHE_SHIFT) + bvec->bv_offset;
+		end = start + bvec->bv_len - 1;
+
+		if (bvec->bv_offset == 0 && bvec->bv_len == PAGE_CACHE_SIZE)
+			whole_page = 1;
+		else
+			whole_page = 0;
+
+		if (--bvec >= bio->bi_io_vec)
+			prefetchw(&bvec->bv_page->flags);
+
+		if (uptodate) {
+			set_extent_uptodate(tree, start, end, GFP_ATOMIC);
+			if (whole_page)
+				SetPageUptodate(page);
+			else
+				check_page_uptodate(tree, page);
+		} else {
+			ClearPageUptodate(page);
+			SetPageError(page);
+		}
+
+		unlock_extent(tree, start, end, GFP_ATOMIC);
+
+		if (whole_page)
+			unlock_page(page);
+		else
+			check_page_locked(tree, page);
+	} while (bvec >= bio->bi_io_vec);
+
+	bio_put(bio);
+	return 0;
+}
+
+/*
+ * IO done from prepare_write is pretty simple, we just unlock
+ * the structs in the extent tree when done, and set the uptodate bits
+ * as appropriate.
+ */
+static int end_bio_extent_preparewrite(struct bio *bio,
+				       unsigned int bytes_done, int err)
+{
+	const int uptodate = test_bit(BIO_UPTODATE, &bio->bi_flags);
+	struct bio_vec *bvec = bio->bi_io_vec + bio->bi_vcnt - 1;
+	struct extent_map_tree *tree = bio->bi_private;
+	u64 start;
+	u64 end;
+
+	if (bio->bi_size)
+		return 1;
+
+	do {
+		struct page *page = bvec->bv_page;
+		start = (page->index << PAGE_CACHE_SHIFT) + bvec->bv_offset;
+		end = start + bvec->bv_len - 1;
+
+		if (--bvec >= bio->bi_io_vec)
+			prefetchw(&bvec->bv_page->flags);
+
+		if (uptodate) {
+			set_extent_uptodate(tree, start, end, GFP_ATOMIC);
+		} else {
+			ClearPageUptodate(page);
+			SetPageError(page);
+		}
+
+		unlock_extent(tree, start, end, GFP_ATOMIC);
+
+	} while (bvec >= bio->bi_io_vec);
+
+	bio_put(bio);
+	return 0;
+}
+
+static int submit_extent_page(int rw, struct extent_map_tree *tree,
+			      struct page *page, sector_t sector,
+			      size_t size, unsigned long offset,
+			      struct block_device *bdev,
+			      bio_end_io_t end_io_func)
+{
+	struct bio *bio;
+	int ret = 0;
+
+	bio = bio_alloc(GFP_NOIO, 1);
+
+	bio->bi_sector = sector;
+	bio->bi_bdev = bdev;
+	bio->bi_io_vec[0].bv_page = page;
+	bio->bi_io_vec[0].bv_len = size;
+	bio->bi_io_vec[0].bv_offset = offset;
+
+	bio->bi_vcnt = 1;
+	bio->bi_idx = 0;
+	bio->bi_size = size;
+
+	bio->bi_end_io = end_io_func;
+	bio->bi_private = tree;
+
+	bio_get(bio);
+	submit_bio(rw, bio);
+
+	if (bio_flagged(bio, BIO_EOPNOTSUPP))
+		ret = -EOPNOTSUPP;
+
+	bio_put(bio);
+	return ret;
+}
+
+/*
+ * basic readpage implementation.  Locked extent state structs are inserted
+ * into the tree that are removed when the IO is done (by the end_io
+ * handlers)
+ */
+int extent_read_full_page(struct extent_map_tree *tree, struct page *page,
+			  get_extent_t *get_extent)
+{
+	struct inode *inode = page->mapping->host;
+	u64 start = page->index << PAGE_CACHE_SHIFT;
+	u64 page_end = start + PAGE_CACHE_SIZE - 1;
+	u64 end;
+	u64 cur = start;
+	u64 extent_offset;
+	u64 last_byte = i_size_read(inode);
+	u64 block_start;
+	u64 cur_end;
+	sector_t sector;
+	struct extent_map *em;
+	struct block_device *bdev;
+	int ret;
+	int nr = 0;
+	size_t page_offset = 0;
+	size_t iosize;
+	size_t blocksize = inode->i_sb->s_blocksize;
+
+	if (!PagePrivate(page)) {
+		SetPagePrivate(page);
+		set_page_private(page, 1);
+		page_cache_get(page);
+	}
+
+	end = page_end;
+	lock_extent(tree, start, end, GFP_NOFS);
+
+	while (cur <= end) {
+		if (cur >= last_byte) {
+			iosize = PAGE_CACHE_SIZE - page_offset;
+			zero_user_page(page, page_offset, iosize, KM_USER0);
+			set_extent_uptodate(tree, cur, cur + iosize - 1,
+					    GFP_NOFS);
+			unlock_extent(tree, cur, cur + iosize - 1, GFP_NOFS);
+			break;
+		}
+		em = get_extent(inode, page, page_offset, cur, end, 0);
+		if (IS_ERR(em) || !em) {
+			SetPageError(page);
+			unlock_extent(tree, cur, end, GFP_NOFS);
+			break;
+		}
+
+		extent_offset = cur - em->start;
+		BUG_ON(em->end < cur);
+		BUG_ON(end < cur);
+
+		iosize = min(em->end - cur, end - cur) + 1;
+		cur_end = min(em->end, end);
+		iosize = (iosize + blocksize - 1) & ~((u64)blocksize - 1);
+		sector = (em->block_start + extent_offset) >> 9;
+		bdev = em->bdev;
+		block_start = em->block_start;
+		free_extent_map(em);
+		em = NULL;
+
+		/* we've found a hole, just zero and go on */
+		if (block_start == 0) {
+			zero_user_page(page, page_offset, iosize, KM_USER0);
+			set_extent_uptodate(tree, cur, cur + iosize - 1,
+					    GFP_NOFS);
+			unlock_extent(tree, cur, cur + iosize - 1, GFP_NOFS);
+			cur = cur + iosize;
+			page_offset += iosize;
+			continue;
+		}
+		/* the get_extent function already copied into the page */
+		if (test_range_bit(tree, cur, cur_end, EXTENT_UPTODATE, 1)) {
+			unlock_extent(tree, cur, cur + iosize - 1, GFP_NOFS);
+			cur = cur + iosize;
+			page_offset += iosize;
+			continue;
+		}
+
+		ret = submit_extent_page(READ, tree, page,
+					 sector, iosize, page_offset, bdev,
+					 end_bio_extent_readpage);
+		if (ret)
+			SetPageError(page);
+		cur = cur + iosize;
+		page_offset += iosize;
+		nr++;
+	}
+	if (!nr) {
+		if (!PageError(page))
+			SetPageUptodate(page);
+		unlock_page(page);
+	}
+	return 0;
+}
+EXPORT_SYMBOL(extent_read_full_page);
+
+/*
+ * the writepage semantics are similar to regular writepage.  extent
+ * records are inserted to lock ranges in the tree, and as dirty areas
+ * are found, they are marked writeback.  Then the lock bits are removed
+ * and the end_io handler clears the writeback ranges
+ */
+int extent_write_full_page(struct extent_map_tree *tree, struct page *page,
+			  get_extent_t *get_extent,
+			  struct writeback_control *wbc)
+{
+	struct inode *inode = page->mapping->host;
+	u64 start = page->index << PAGE_CACHE_SHIFT;
+	u64 page_end = start + PAGE_CACHE_SIZE - 1;
+	u64 end;
+	u64 cur = start;
+	u64 extent_offset;
+	u64 last_byte = i_size_read(inode);
+	u64 block_start;
+	sector_t sector;
+	struct extent_map *em;
+	struct block_device *bdev;
+	int ret;
+	int nr = 0;
+	size_t page_offset = 0;
+	size_t iosize;
+	size_t blocksize;
+	loff_t i_size = i_size_read(inode);
+	unsigned long end_index = i_size >> PAGE_CACHE_SHIFT;
+
+	if (page->index > end_index) {
+		clear_extent_dirty(tree, start, page_end, GFP_NOFS);
+		unlock_page(page);
+		return 0;
+	}
+
+	if (page->index == end_index) {
+		size_t offset = i_size & (PAGE_CACHE_SIZE - 1);
+		zero_user_page(page, offset,
+			       PAGE_CACHE_SIZE - offset, KM_USER0);
+	}
+
+	if (!PagePrivate(page)) {
+		SetPagePrivate(page);
+		set_page_private(page, 1);
+		page_cache_get(page);
+	}
+
+	end = page_end;
+	lock_extent(tree, start, page_end, GFP_NOFS);
+
+	if (last_byte <= start) {
+		clear_extent_dirty(tree, start, page_end, GFP_NOFS);
+		goto done;
+	}
+
+	set_extent_uptodate(tree, start, page_end, GFP_NOFS);
+	blocksize = inode->i_sb->s_blocksize;
+
+	while (cur <= end) {
+		if (cur >= last_byte) {
+			clear_extent_dirty(tree, cur, page_end, GFP_NOFS);
+			break;
+		}
+		em = get_extent(inode, page, page_offset, cur, end, 1);
+		if (IS_ERR(em) || !em) {
+			SetPageError(page);
+			break;
+		}
+
+		extent_offset = cur - em->start;
+		BUG_ON(em->end < cur);
+		BUG_ON(end < cur);
+		iosize = min(em->end - cur, end - cur) + 1;
+		iosize = (iosize + blocksize - 1) & ~((u64)blocksize - 1);
+		sector = (em->block_start + extent_offset) >> 9;
+		bdev = em->bdev;
+		block_start = em->block_start;
+		free_extent_map(em);
+		em = NULL;
+
+		if (block_start == 0 || block_start == EXTENT_MAP_INLINE) {
+			clear_extent_dirty(tree, cur,
+					   cur + iosize - 1, GFP_NOFS);
+			cur = cur + iosize;
+			page_offset += iosize;
+			continue;
+		}
+
+		/* leave this out until we have a page_mkwrite call */
+		if (0 && !test_range_bit(tree, cur, cur + iosize - 1,
+				   EXTENT_DIRTY, 0)) {
+			cur = cur + iosize;
+			page_offset += iosize;
+			continue;
+		}
+		clear_extent_dirty(tree, cur, cur + iosize - 1, GFP_NOFS);
+		set_range_writeback(tree, cur, cur + iosize - 1);
+		ret = submit_extent_page(WRITE, tree, page,
+					 sector, iosize, page_offset, bdev,
+					 end_bio_extent_writepage);
+		if (ret)
+			SetPageError(page);
+		cur = cur + iosize;
+		page_offset += iosize;
+		nr++;
+	}
+done:
+	WARN_ON(test_range_bit(tree, start, page_end, EXTENT_DIRTY, 0));
+	unlock_extent(tree, start, page_end, GFP_NOFS);
+	unlock_page(page);
+	return 0;
+}
+EXPORT_SYMBOL(extent_write_full_page);
+
+/*
+ * basic invalidatepage code, this waits on any locked or writeback
+ * ranges corresponding to the page, and then deletes any extent state
+ * records from the tree
+ */
+int extent_invalidatepage(struct extent_map_tree *tree,
+			  struct page *page, unsigned long offset)
+{
+	u64 start = (page->index << PAGE_CACHE_SHIFT);
+	u64 end = start + PAGE_CACHE_SIZE - 1;
+	size_t blocksize = page->mapping->host->i_sb->s_blocksize;
+
+	start += (offset + blocksize -1) & ~(blocksize - 1);
+	if (start > end)
+		return 0;
+
+	lock_extent(tree, start, end, GFP_NOFS);
+	wait_on_extent_writeback(tree, start, end);
+	clear_extent_bit(tree, start, end, EXTENT_LOCKED | EXTENT_DIRTY,
+			 1, 1, GFP_NOFS);
+	return 0;
+}
+EXPORT_SYMBOL(extent_invalidatepage);
+
+/*
+ * simple commit_write call, set_range_dirty is used to mark both
+ * the pages and the extent records as dirty
+ */
+int extent_commit_write(struct extent_map_tree *tree,
+			struct inode *inode, struct page *page,
+			unsigned from, unsigned to)
+{
+	loff_t pos = ((loff_t)page->index << PAGE_CACHE_SHIFT) + to;
+
+	if (!PagePrivate(page)) {
+		SetPagePrivate(page);
+		set_page_private(page, 1);
+		page_cache_get(page);
+	}
+
+	set_page_dirty(page);
+
+	if (pos > inode->i_size) {
+		i_size_write(inode, pos);
+		mark_inode_dirty(inode);
+	}
+	return 0;
+}
+EXPORT_SYMBOL(extent_commit_write);
+
+int extent_prepare_write(struct extent_map_tree *tree,
+			 struct inode *inode, struct page *page,
+			 unsigned from, unsigned to, get_extent_t *get_extent)
+{
+	u64 page_start = page->index << PAGE_CACHE_SHIFT;
+	u64 page_end = page_start + PAGE_CACHE_SIZE - 1;
+	u64 block_start;
+	u64 orig_block_start;
+	u64 block_end;
+	u64 cur_end;
+	struct extent_map *em;
+	unsigned blocksize = 1 << inode->i_blkbits;
+	size_t page_offset = 0;
+	size_t block_off_start;
+	size_t block_off_end;
+	int err = 0;
+	int iocount = 0;
+	int ret = 0;
+	int isnew;
+
+	if (!PagePrivate(page)) {
+		SetPagePrivate(page);
+		set_page_private(page, 1);
+		page_cache_get(page);
+	}
+	block_start = (page_start + from) & ~((u64)blocksize - 1);
+	block_end = (page_start + to - 1) | (blocksize - 1);
+	orig_block_start = block_start;
+
+	lock_extent(tree, page_start, page_end, GFP_NOFS);
+	while(block_start <= block_end) {
+		em = get_extent(inode, page, page_offset, block_start,
+				block_end, 1);
+		if (IS_ERR(em) || !em) {
+			goto err;
+		}
+		cur_end = min(block_end, em->end);
+		block_off_start = block_start & (PAGE_CACHE_SIZE - 1);
+		block_off_end = block_off_start + blocksize;
+		isnew = clear_extent_new(tree, block_start, cur_end, GFP_NOFS);
+
+		if (!PageUptodate(page) && isnew &&
+		    (block_off_end > to || block_off_start < from)) {
+			void *kaddr;
+
+			kaddr = kmap_atomic(page, KM_USER0);
+			if (block_off_end > to)
+				memset(kaddr + to, 0, block_off_end - to);
+			if (block_off_start < from)
+				memset(kaddr + block_off_start, 0,
+				       from - block_off_start);
+			flush_dcache_page(page);
+			kunmap_atomic(kaddr, KM_USER0);
+		}
+		if (!isnew && !PageUptodate(page) &&
+		    (block_off_end > to || block_off_start < from) &&
+		    !test_range_bit(tree, block_start, cur_end,
+				    EXTENT_UPTODATE, 1)) {
+			u64 sector;
+			u64 extent_offset = block_start - em->start;
+			size_t iosize;
+			sector = (em->block_start + extent_offset) >> 9;
+			iosize = (cur_end - block_start + blocksize - 1) &
+				~((u64)blocksize - 1);
+			/*
+			 * we've already got the extent locked, but we
+			 * need to split the state such that our end_bio
+			 * handler can clear the lock.
+			 */
+			set_extent_bit(tree, block_start,
+				       block_start + iosize - 1,
+				       EXTENT_LOCKED, 0, NULL, GFP_NOFS);
+			ret = submit_extent_page(READ, tree, page,
+					 sector, iosize, page_offset, em->bdev,
+					 end_bio_extent_preparewrite);
+			iocount++;
+			block_start = block_start + iosize;
+		} else {
+			set_extent_uptodate(tree, block_start, cur_end,
+					    GFP_NOFS);
+			unlock_extent(tree, block_start, cur_end, GFP_NOFS);
+			block_start = cur_end + 1;
+		}
+		page_offset = block_start & (PAGE_CACHE_SIZE - 1);
+		free_extent_map(em);
+	}
+	if (iocount) {
+		wait_extent_bit(tree, orig_block_start,
+				block_end, EXTENT_LOCKED);
+	}
+	check_page_uptodate(tree, page);
+err:
+	/* FIXME, zero out newly allocated blocks on error */
+	return err;
+}
+EXPORT_SYMBOL(extent_prepare_write);
+
+/*
+ * a helper for releasepage.  As long as there are no locked extents
+ * in the range corresponding to the page, both state records and extent
+ * map records are removed
+ */
+int try_release_extent_mapping(struct extent_map_tree *tree, struct page *page)
+{
+	struct extent_map *em;
+	u64 start = page->index << PAGE_CACHE_SHIFT;
+	u64 end = start + PAGE_CACHE_SIZE - 1;
+	u64 orig_start = start;
+
+	while (start <= end) {
+		em = lookup_extent_mapping(tree, start, end);
+		if (!em || IS_ERR(em))
+			break;
+		if (test_range_bit(tree, em->start, em->end,
+				   EXTENT_LOCKED, 0)) {
+			free_extent_map(em);
+			start = em->end + 1;
+printk("range still locked %Lu %Lu\n", em->start, em->end);
+			break;
+		}
+		remove_extent_mapping(tree, em);
+		start = em->end + 1;
+		/* once for the rb tree */
+		free_extent_map(em);
+		/* once for us */
+		free_extent_map(em);
+	}
+	WARN_ON(test_range_bit(tree, orig_start, end, EXTENT_WRITEBACK, 0));
+	clear_extent_bit(tree, orig_start, end, EXTENT_UPTODATE,
+			 1, 1, GFP_NOFS);
+	return 1;
+}
+EXPORT_SYMBOL(try_release_extent_mapping);
+
