commit c730ae0c6bb3125ccb776fb2ab6abbdff500c02c
Author: Marcos Paulo de Souza <mpdesouza@suse.com>
Date:   Tue Jun 16 15:54:29 2020 -0300

    btrfs: convert comments to fallthrough annotations
    
    Convert fall through comments to the pseudo-keyword which is now the
    preferred way.
    
    Signed-off-by: Marcos Paulo de Souza <mpdesouza@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 3a7648bff42c..82ab6e5a386d 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1196,7 +1196,7 @@ __tree_mod_log_rewind(struct btrfs_fs_info *fs_info, struct extent_buffer *eb,
 		switch (tm->op) {
 		case MOD_LOG_KEY_REMOVE_WHILE_FREEING:
 			BUG_ON(tm->slot < n);
-			/* Fallthrough */
+			fallthrough;
 		case MOD_LOG_KEY_REMOVE_WHILE_MOVING:
 		case MOD_LOG_KEY_REMOVE:
 			btrfs_set_node_key(eb, &tm->key, tm->slot);

commit 213ff4b72a9c7509dd85979db64c66774f4f26c1
Author: Nikolay Borisov <nborisov@suse.com>
Date:   Wed May 27 13:10:59 2020 +0300

    btrfs: remove redundant local variable in read_block_for_search
    
    The local 'b' variable is only used to directly read values from passed
    extent buffer. So eliminate  it and directly use the input parameter.
    Furthermore this shrinks the size of the following functions:
    
    ./scripts/bloat-o-meter ctree.orig fs/btrfs/ctree.o
    add/remove: 0/0 grow/shrink: 0/2 up/down: 0/-73 (-73)
    Function                                     old     new   delta
    read_block_for_search.isra                   876     871      -5
    push_node_left                              1112    1044     -68
    Total: Before=50348, After=50275, chg -0.14%
    
    Reviewed-by: Johannes Thumshirn <johannes.thumshirn@wdc.com>
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 5f94414f3336..3a7648bff42c 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2335,16 +2335,15 @@ read_block_for_search(struct btrfs_root *root, struct btrfs_path *p,
 	struct btrfs_fs_info *fs_info = root->fs_info;
 	u64 blocknr;
 	u64 gen;
-	struct extent_buffer *b = *eb_ret;
 	struct extent_buffer *tmp;
 	struct btrfs_key first_key;
 	int ret;
 	int parent_level;
 
-	blocknr = btrfs_node_blockptr(b, slot);
-	gen = btrfs_node_ptr_generation(b, slot);
-	parent_level = btrfs_header_level(b);
-	btrfs_node_key_to_cpu(b, &first_key, slot);
+	blocknr = btrfs_node_blockptr(*eb_ret, slot);
+	gen = btrfs_node_ptr_generation(*eb_ret, slot);
+	parent_level = btrfs_header_level(*eb_ret);
+	btrfs_node_key_to_cpu(*eb_ret, &first_key, slot);
 
 	tmp = find_extent_buffer(fs_info, blocknr);
 	if (tmp) {

commit 995e9a166b6909c9bb4af8f51b9502f8b8c18291
Author: Nikolay Borisov <nborisov@suse.com>
Date:   Wed May 27 13:10:53 2020 +0300

    btrfs: open code key_search
    
    This function wraps the optimisation implemented by d7396f07358a
    ("Btrfs: optimize key searches in btrfs_search_slot") however this
    optimisation is really used in only one place - btrfs_search_slot.
    
    Just open code the optimisation and also add a comment explaining how it
    works since it's not clear just by looking at the code - the key point
    here is it depends on an internal invariant that BTRFS' btree provides,
    namely intermediate pointers always contain the key at slot0 at the
    child node. So in the case of exact match we can safely assume that the
    given key will always be in slot 0 on lower levels.
    
    Furthermore this results in a reduction of btrfs_search_slot's size:
    
    ./scripts/bloat-o-meter ctree.orig fs/btrfs/ctree.o
    add/remove: 0/0 grow/shrink: 0/1 up/down: 0/-75 (-75)
    Function                                     old     new   delta
    btrfs_search_slot                           2783    2708     -75
    Total: Before=50423, After=50348, chg -0.15%
    
    Reviewed-by: Johannes Thumshirn <johannes.thumshirn@wdc.com>
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 92775554d1cc..5f94414f3336 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2488,19 +2488,6 @@ setup_nodes_for_search(struct btrfs_trans_handle *trans,
 	return ret;
 }
 
-static int key_search(struct extent_buffer *b, const struct btrfs_key *key,
-		      int *prev_cmp, int *slot)
-{
-	if (*prev_cmp != 0) {
-		*prev_cmp = btrfs_bin_search(b, key, slot);
-		return *prev_cmp;
-	}
-
-	*slot = 0;
-
-	return 0;
-}
-
 int btrfs_find_item(struct btrfs_root *fs_root, struct btrfs_path *path,
 		u64 iobjectid, u64 ioff, u8 key_type,
 		struct btrfs_key *found_key)
@@ -2770,9 +2757,23 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 			}
 		}
 
-		ret = key_search(b, key, &prev_cmp, &slot);
-		if (ret < 0)
-			goto done;
+		/*
+		 * If btrfs_bin_search returns an exact match (prev_cmp == 0)
+		 * we can safely assume the target key will always be in slot 0
+		 * on lower levels due to the invariants BTRFS' btree provides,
+		 * namely that a btrfs_key_ptr entry always points to the
+		 * lowest key in the child node, thus we can skip searching
+		 * lower levels
+		 */
+		if (prev_cmp == 0) {
+			slot = 0;
+			ret = 0;
+		} else {
+			ret = btrfs_bin_search(b, key, &slot);
+			prev_cmp = ret;
+			if (ret < 0)
+				goto done;
+		}
 
 		if (level == 0) {
 			p->slots[level] = slot;
@@ -2896,7 +2897,6 @@ int btrfs_search_old_slot(struct btrfs_root *root, const struct btrfs_key *key,
 	int level;
 	int lowest_unlock = 1;
 	u8 lowest_level = 0;
-	int prev_cmp = -1;
 
 	lowest_level = p->lowest_level;
 	WARN_ON(p->nodes[0] != NULL);
@@ -2929,12 +2929,7 @@ int btrfs_search_old_slot(struct btrfs_root *root, const struct btrfs_key *key,
 		 */
 		btrfs_unlock_up_safe(p, level + 1);
 
-		/*
-		 * Since we can unwind ebs we want to do a real search every
-		 * time.
-		 */
-		prev_cmp = -1;
-		ret = key_search(b, key, &prev_cmp, &slot);
+		ret = btrfs_bin_search(b, key, &slot);
 		if (ret < 0)
 			goto done;
 

commit 92a7cc4252231d1641b36c38cf845cfc50308ab0
Author: Qu Wenruo <wqu@suse.com>
Date:   Fri May 15 14:01:40 2020 +0800

    btrfs: rename BTRFS_ROOT_REF_COWS to BTRFS_ROOT_SHAREABLE
    
    The name BTRFS_ROOT_REF_COWS is not very clear about the meaning.
    
    In fact, that bit can only be set to those trees:
    
    - Subvolume roots
    - Data reloc root
    - Reloc roots for above roots
    
    All other trees won't get this bit set.  So just by the result, it is
    obvious that, roots with this bit set can have tree blocks shared with
    other trees.  Either shared by snapshots, or by reloc roots (an special
    snapshot created by relocation).
    
    This patch will rename BTRFS_ROOT_REF_COWS to BTRFS_ROOT_SHAREABLE to
    make it easier to understand, and update all comment mentioning
    "reference counted" to follow the rename.
    
    Signed-off-by: Qu Wenruo <wqu@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 746dec22f250..92775554d1cc 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -144,9 +144,10 @@ struct extent_buffer *btrfs_root_node(struct btrfs_root *root)
 	return eb;
 }
 
-/* cowonly root (everything not a reference counted cow subvolume), just get
- * put onto a simple dirty list.  transaction.c walks this to make sure they
- * get properly updated on disk.
+/*
+ * Cowonly root (not-shareable trees, everything not subvolume or reloc roots),
+ * just get put onto a simple dirty list.  Transaction walks this list to make
+ * sure they get properly updated on disk.
  */
 static void add_root_to_dirty_list(struct btrfs_root *root)
 {
@@ -185,9 +186,9 @@ int btrfs_copy_root(struct btrfs_trans_handle *trans,
 	int level;
 	struct btrfs_disk_key disk_key;
 
-	WARN_ON(test_bit(BTRFS_ROOT_REF_COWS, &root->state) &&
+	WARN_ON(test_bit(BTRFS_ROOT_SHAREABLE, &root->state) &&
 		trans->transid != fs_info->running_transaction->transid);
-	WARN_ON(test_bit(BTRFS_ROOT_REF_COWS, &root->state) &&
+	WARN_ON(test_bit(BTRFS_ROOT_SHAREABLE, &root->state) &&
 		trans->transid != root->last_trans);
 
 	level = btrfs_header_level(buf);
@@ -826,12 +827,11 @@ int btrfs_block_can_be_shared(struct btrfs_root *root,
 			      struct extent_buffer *buf)
 {
 	/*
-	 * Tree blocks not in reference counted trees and tree roots
-	 * are never shared. If a block was allocated after the last
-	 * snapshot and the block was not allocated by tree relocation,
-	 * we know the block is not shared.
+	 * Tree blocks not in shareable trees and tree roots are never shared.
+	 * If a block was allocated after the last snapshot and the block was
+	 * not allocated by tree relocation, we know the block is not shared.
 	 */
-	if (test_bit(BTRFS_ROOT_REF_COWS, &root->state) &&
+	if (test_bit(BTRFS_ROOT_SHAREABLE, &root->state) &&
 	    buf != root->node && buf != root->commit_root &&
 	    (btrfs_header_generation(buf) <=
 	     btrfs_root_last_snapshot(&root->root_item) ||
@@ -1024,9 +1024,9 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 
 	btrfs_assert_tree_locked(buf);
 
-	WARN_ON(test_bit(BTRFS_ROOT_REF_COWS, &root->state) &&
+	WARN_ON(test_bit(BTRFS_ROOT_SHAREABLE, &root->state) &&
 		trans->transid != fs_info->running_transaction->transid);
-	WARN_ON(test_bit(BTRFS_ROOT_REF_COWS, &root->state) &&
+	WARN_ON(test_bit(BTRFS_ROOT_SHAREABLE, &root->state) &&
 		trans->transid != root->last_trans);
 
 	level = btrfs_header_level(buf);
@@ -1065,7 +1065,7 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 		return ret;
 	}
 
-	if (test_bit(BTRFS_ROOT_REF_COWS, &root->state)) {
+	if (test_bit(BTRFS_ROOT_SHAREABLE, &root->state)) {
 		ret = btrfs_reloc_cow_block(trans, root, buf, cow);
 		if (ret) {
 			btrfs_abort_transaction(trans, ret);

commit 5cd17f343bd1c47dc673260fa2973abc14ecc549
Author: David Sterba <dsterba@suse.com>
Date:   Wed Apr 29 23:23:37 2020 +0200

    btrfs: speed up and simplify generic_bin_search
    
    The bin search jumps over the extent buffer item keys, comparing
    directly the bytes if the key is in one page, or storing it in a
    temporary buffer in case it spans two pages.
    
    The mapping start and length are obtained from map_private_extent_buffer,
    which is heavy weight compared to what we need. We know the key size and
    can find out the eb page in a simple way.  For keys spanning two pages
    the fallback read_extent_buffer is used.
    
    The temporary variables are reduced and moved to the scope of use.
    
    Reviewed-by: Johannes Thumshirn <johannes.thumshirn@wdc.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 6dbeb23c59ec..746dec22f250 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1668,15 +1668,8 @@ static noinline int generic_bin_search(struct extent_buffer *eb,
 {
 	int low = 0;
 	int high = max;
-	int mid;
 	int ret;
-	struct btrfs_disk_key *tmp = NULL;
-	struct btrfs_disk_key unaligned;
-	unsigned long offset;
-	char *kaddr = NULL;
-	unsigned long map_start = 0;
-	unsigned long map_len = 0;
-	int err;
+	const int key_size = sizeof(struct btrfs_disk_key);
 
 	if (low > high) {
 		btrfs_err(eb->fs_info,
@@ -1687,32 +1680,26 @@ static noinline int generic_bin_search(struct extent_buffer *eb,
 	}
 
 	while (low < high) {
+		unsigned long oip;
+		unsigned long offset;
+		struct btrfs_disk_key *tmp;
+		struct btrfs_disk_key unaligned;
+		int mid;
+
 		mid = (low + high) / 2;
 		offset = p + mid * item_size;
+		oip = offset_in_page(offset);
 
-		if (!kaddr || offset < map_start ||
-		    (offset + sizeof(struct btrfs_disk_key)) >
-		    map_start + map_len) {
-
-			err = map_private_extent_buffer(eb, offset,
-						sizeof(struct btrfs_disk_key),
-						&kaddr, &map_start, &map_len);
-
-			if (!err) {
-				tmp = (struct btrfs_disk_key *)(kaddr + offset -
-							map_start);
-			} else if (err == 1) {
-				read_extent_buffer(eb, &unaligned,
-						   offset, sizeof(unaligned));
-				tmp = &unaligned;
-			} else {
-				return err;
-			}
+		if (oip + key_size <= PAGE_SIZE) {
+			const unsigned long idx = offset >> PAGE_SHIFT;
+			char *kaddr = page_address(eb->pages[idx]);
 
+			tmp = (struct btrfs_disk_key *)(kaddr + oip);
 		} else {
-			tmp = (struct btrfs_disk_key *)(kaddr + offset -
-							map_start);
+			read_extent_buffer(eb, &unaligned, offset, key_size);
+			tmp = &unaligned;
 		}
+
 		ret = comp_keys(tmp, key);
 
 		if (ret < 0)

commit a31356b9e263b723d4991383efc87b71c6e87991
Author: David Sterba <dsterba@suse.com>
Date:   Wed Apr 29 22:56:01 2020 +0200

    btrfs: don't use set/get token in leaf_space_used
    
    The token is supposed to cache the last page used by the set/get
    helpers. In leaf_space_used the first and last items are accessed, it's
    not likely they'd be on the same page so there's some overhead caused
    updating the token address but not using it.
    
    Reviewed-by: Johannes Thumshirn <johannes.thumshirn@wdc.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 576111cdea1d..6dbeb23c59ec 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -3507,19 +3507,17 @@ static int leaf_space_used(struct extent_buffer *l, int start, int nr)
 {
 	struct btrfs_item *start_item;
 	struct btrfs_item *end_item;
-	struct btrfs_map_token token;
 	int data_len;
 	int nritems = btrfs_header_nritems(l);
 	int end = min(nritems, start + nr) - 1;
 
 	if (!nr)
 		return 0;
-	btrfs_init_map_token(&token, l);
 	start_item = btrfs_item_nr(start);
 	end_item = btrfs_item_nr(end);
-	data_len = btrfs_token_item_offset(&token, start_item) +
-		btrfs_token_item_size(&token, start_item);
-	data_len = data_len - btrfs_token_item_offset(&token, end_item);
+	data_len = btrfs_item_offset(l, start_item) +
+		   btrfs_item_size(l, start_item);
+	data_len = data_len - btrfs_item_offset(l, end_item);
 	data_len += sizeof(struct btrfs_item) * nr;
 	WARN_ON(data_len < 0);
 	return data_len;

commit cc4c13d55cba8a0b81bc18243eabc57be1aa44d2
Author: David Sterba <dsterba@suse.com>
Date:   Wed Apr 29 02:15:56 2020 +0200

    btrfs: drop eb parameter from set/get token helpers
    
    Now that all set/get helpers use the eb from the token, we don't need to
    pass it to many btrfs_token_*/btrfs_set_token_* helpers, saving some
    stack space.
    
    Reviewed-by: Johannes Thumshirn <johannes.thumshirn@wdc.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 6c28efe5b14a..576111cdea1d 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -3517,9 +3517,9 @@ static int leaf_space_used(struct extent_buffer *l, int start, int nr)
 	btrfs_init_map_token(&token, l);
 	start_item = btrfs_item_nr(start);
 	end_item = btrfs_item_nr(end);
-	data_len = btrfs_token_item_offset(l, start_item, &token) +
-		btrfs_token_item_size(l, start_item, &token);
-	data_len = data_len - btrfs_token_item_offset(l, end_item, &token);
+	data_len = btrfs_token_item_offset(&token, start_item) +
+		btrfs_token_item_size(&token, start_item);
+	data_len = data_len - btrfs_token_item_offset(&token, end_item);
 	data_len += sizeof(struct btrfs_item) * nr;
 	WARN_ON(data_len < 0);
 	return data_len;
@@ -3650,8 +3650,8 @@ static noinline int __push_leaf_right(struct btrfs_path *path,
 	push_space = BTRFS_LEAF_DATA_SIZE(fs_info);
 	for (i = 0; i < right_nritems; i++) {
 		item = btrfs_item_nr(i);
-		push_space -= btrfs_token_item_size(right, item, &token);
-		btrfs_set_token_item_offset(right, item, push_space, &token);
+		push_space -= btrfs_token_item_size(&token, item);
+		btrfs_set_token_item_offset(&token, item, push_space);
 	}
 
 	left_nritems -= push_items;
@@ -3859,10 +3859,9 @@ static noinline int __push_leaf_left(struct btrfs_path *path, int data_size,
 
 		item = btrfs_item_nr(i);
 
-		ioff = btrfs_token_item_offset(left, item, &token);
-		btrfs_set_token_item_offset(left, item,
-		      ioff - (BTRFS_LEAF_DATA_SIZE(fs_info) - old_left_item_size),
-		      &token);
+		ioff = btrfs_token_item_offset(&token, item);
+		btrfs_set_token_item_offset(&token, item,
+		      ioff - (BTRFS_LEAF_DATA_SIZE(fs_info) - old_left_item_size));
 	}
 	btrfs_set_header_nritems(left, old_left_nritems + push_items);
 
@@ -3892,9 +3891,8 @@ static noinline int __push_leaf_left(struct btrfs_path *path, int data_size,
 	for (i = 0; i < right_nritems; i++) {
 		item = btrfs_item_nr(i);
 
-		push_space = push_space - btrfs_token_item_size(right,
-								item, &token);
-		btrfs_set_token_item_offset(right, item, push_space, &token);
+		push_space = push_space - btrfs_token_item_size(&token, item);
+		btrfs_set_token_item_offset(&token, item, push_space);
 	}
 
 	btrfs_mark_buffer_dirty(left);
@@ -4036,9 +4034,8 @@ static noinline void copy_for_split(struct btrfs_trans_handle *trans,
 		struct btrfs_item *item = btrfs_item_nr(i);
 		u32 ioff;
 
-		ioff = btrfs_token_item_offset(right, item, &token);
-		btrfs_set_token_item_offset(right, item,
-					    ioff + rt_data_off, &token);
+		ioff = btrfs_token_item_offset(&token, item);
+		btrfs_set_token_item_offset(&token, item, ioff + rt_data_off);
 	}
 
 	btrfs_set_header_nritems(l, mid);
@@ -4541,9 +4538,8 @@ void btrfs_truncate_item(struct btrfs_path *path, u32 new_size, int from_end)
 		u32 ioff;
 		item = btrfs_item_nr(i);
 
-		ioff = btrfs_token_item_offset(leaf, item, &token);
-		btrfs_set_token_item_offset(leaf, item,
-					    ioff + size_diff, &token);
+		ioff = btrfs_token_item_offset(&token, item);
+		btrfs_set_token_item_offset(&token, item, ioff + size_diff);
 	}
 
 	/* shift the data */
@@ -4640,9 +4636,8 @@ void btrfs_extend_item(struct btrfs_path *path, u32 data_size)
 		u32 ioff;
 		item = btrfs_item_nr(i);
 
-		ioff = btrfs_token_item_offset(leaf, item, &token);
-		btrfs_set_token_item_offset(leaf, item,
-					    ioff - data_size, &token);
+		ioff = btrfs_token_item_offset(&token, item);
+		btrfs_set_token_item_offset(&token, item, ioff - data_size);
 	}
 
 	/* shift the data */
@@ -4718,9 +4713,9 @@ void setup_items_for_insert(struct btrfs_root *root, struct btrfs_path *path,
 			u32 ioff;
 
 			item = btrfs_item_nr(i);
-			ioff = btrfs_token_item_offset(leaf, item, &token);
-			btrfs_set_token_item_offset(leaf, item,
-						    ioff - total_data, &token);
+			ioff = btrfs_token_item_offset(&token, item);
+			btrfs_set_token_item_offset(&token, item,
+						    ioff - total_data);
 		}
 		/* shift the items */
 		memmove_extent_buffer(leaf, btrfs_item_nr_offset(slot + nr),
@@ -4739,10 +4734,9 @@ void setup_items_for_insert(struct btrfs_root *root, struct btrfs_path *path,
 		btrfs_cpu_key_to_disk(&disk_key, cpu_key + i);
 		btrfs_set_item_key(leaf, &disk_key, slot + i);
 		item = btrfs_item_nr(slot + i);
-		btrfs_set_token_item_offset(leaf, item,
-					    data_end - data_size[i], &token);
+		btrfs_set_token_item_offset(&token, item, data_end - data_size[i]);
 		data_end -= data_size[i];
-		btrfs_set_token_item_size(leaf, item, data_size[i], &token);
+		btrfs_set_token_item_size(&token, item, data_size[i]);
 	}
 
 	btrfs_set_header_nritems(leaf, nritems + nr);
@@ -4930,9 +4924,8 @@ int btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 			u32 ioff;
 
 			item = btrfs_item_nr(i);
-			ioff = btrfs_token_item_offset(leaf, item, &token);
-			btrfs_set_token_item_offset(leaf, item,
-						    ioff + dsize, &token);
+			ioff = btrfs_token_item_offset(&token, item);
+			btrfs_set_token_item_offset(&token, item, ioff + dsize);
 		}
 
 		memmove_extent_buffer(leaf, btrfs_item_nr_offset(slot),

commit e3b8336117e515a260da32fa10bb3354ba12c429
Author: Qu Wenruo <wqu@suse.com>
Date:   Fri Apr 17 15:08:21 2020 +0800

    btrfs: remove the redundant parameter level in btrfs_bin_search()
    
    All callers pass the eb::level so we can get read it directly inside the
    btrfs_bin_search and key_search.
    
    This is inspired by the work of Marek in U-boot.
    
    CC: Marek Behun <marek.behun@nic.cz>
    Reviewed-by: Johannes Thumshirn <johannes.thumshirn@wdc.com>
    Reviewed-by: Nikolay Borisov <nborisov@suse.com>
    Signed-off-by: Qu Wenruo <wqu@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index bfedbbe2311f..6c28efe5b14a 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1733,9 +1733,9 @@ static noinline int generic_bin_search(struct extent_buffer *eb,
  * leaves vs nodes
  */
 int btrfs_bin_search(struct extent_buffer *eb, const struct btrfs_key *key,
-		     int level, int *slot)
+		     int *slot)
 {
-	if (level == 0)
+	if (btrfs_header_level(eb) == 0)
 		return generic_bin_search(eb,
 					  offsetof(struct btrfs_leaf, items),
 					  sizeof(struct btrfs_item),
@@ -2502,10 +2502,10 @@ setup_nodes_for_search(struct btrfs_trans_handle *trans,
 }
 
 static int key_search(struct extent_buffer *b, const struct btrfs_key *key,
-		      int level, int *prev_cmp, int *slot)
+		      int *prev_cmp, int *slot)
 {
 	if (*prev_cmp != 0) {
-		*prev_cmp = btrfs_bin_search(b, key, level, slot);
+		*prev_cmp = btrfs_bin_search(b, key, slot);
 		return *prev_cmp;
 	}
 
@@ -2783,7 +2783,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 			}
 		}
 
-		ret = key_search(b, key, level, &prev_cmp, &slot);
+		ret = key_search(b, key, &prev_cmp, &slot);
 		if (ret < 0)
 			goto done;
 
@@ -2947,7 +2947,7 @@ int btrfs_search_old_slot(struct btrfs_root *root, const struct btrfs_key *key,
 		 * time.
 		 */
 		prev_cmp = -1;
-		ret = key_search(b, key, level, &prev_cmp, &slot);
+		ret = key_search(b, key, &prev_cmp, &slot);
 		if (ret < 0)
 			goto done;
 
@@ -5103,7 +5103,7 @@ int btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,
 	while (1) {
 		nritems = btrfs_header_nritems(cur);
 		level = btrfs_header_level(cur);
-		sret = btrfs_bin_search(cur, min_key, level, &slot);
+		sret = btrfs_bin_search(cur, min_key, &slot);
 		if (sret < 0) {
 			ret = sret;
 			goto out;

commit 59a0fcdb489dae25416e93b9fc67cc97a6847f01
Author: David Sterba <dsterba@suse.com>
Date:   Thu Feb 27 21:00:45 2020 +0100

    btrfs: inline checksum name and driver definitions
    
    There's an unnecessary indirection in the checksum definition table,
    pointer and the string itself. The strings are short and the overall
    size of one entry is now 24 bytes.
    
    Reviewed-by: Johannes Thumshirn <johannes.thumshirn@wdc.com>
    Reviewed-by: Qu Wenruo <wqu@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index f948435e87df..bfedbbe2311f 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -31,8 +31,8 @@ static void del_ptr(struct btrfs_root *root, struct btrfs_path *path,
 
 static const struct btrfs_csums {
 	u16		size;
-	const char	*name;
-	const char	*driver;
+	const char	name[10];
+	const char	driver[12];
 } btrfs_csums[] = {
 	[BTRFS_CSUM_TYPE_CRC32] = { .size = 4, .name = "crc32c" },
 	[BTRFS_CSUM_TYPE_XXHASH] = { .size = 8, .name = "xxhash64" },
@@ -63,7 +63,8 @@ const char *btrfs_super_csum_name(u16 csum_type)
 const char *btrfs_super_csum_driver(u16 csum_type)
 {
 	/* csum type is validated at mount time */
-	return btrfs_csums[csum_type].driver ?:
+	return btrfs_csums[csum_type].driver[0] ?
+		btrfs_csums[csum_type].driver :
 		btrfs_csums[csum_type].name;
 }
 

commit 42c9d0b524cf9af180dcb788a938cdc4c678e8cb
Author: David Sterba <dsterba@suse.com>
Date:   Wed Mar 20 11:54:13 2019 +0100

    btrfs: simplify parameters of btrfs_set_disk_extent_flags
    
    All callers pass extent buffer start and length so the extent buffer
    itself should work fine.
    
    Reviewed-by: Johannes Thumshirn <johannes.thumshirn@wdc.com>
    Reviewed-by: Anand Jain <anand.jain@oracle.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index b62721ac5ee8..f948435e87df 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -925,9 +925,7 @@ static noinline int update_ref_for_cow(struct btrfs_trans_handle *trans,
 		if (new_flags != 0) {
 			int level = btrfs_header_level(buf);
 
-			ret = btrfs_set_disk_extent_flags(trans,
-							  buf->start,
-							  buf->len,
+			ret = btrfs_set_disk_extent_flags(trans, buf,
 							  new_flags, level, 0);
 			if (ret)
 				return ret;

commit b908c334e7a419e5cd08a45d31284b4a93de3bd7
Author: David Sterba <dsterba@suse.com>
Date:   Wed Feb 5 17:26:51 2020 +0100

    btrfs: move root node locking helpers to locking.c
    
    The helpers are related to locking so move them there, update comments.
    
    Reviewed-by: Anand Jain <anand.jain@oracle.com>
    Reviewed-by: Josef Bacik <josef@toxicpanda.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 968faaec0e39..b62721ac5ee8 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -143,44 +143,6 @@ struct extent_buffer *btrfs_root_node(struct btrfs_root *root)
 	return eb;
 }
 
-/* loop around taking references on and locking the root node of the
- * tree until you end up with a lock on the root.  A locked buffer
- * is returned, with a reference held.
- */
-struct extent_buffer *btrfs_lock_root_node(struct btrfs_root *root)
-{
-	struct extent_buffer *eb;
-
-	while (1) {
-		eb = btrfs_root_node(root);
-		btrfs_tree_lock(eb);
-		if (eb == root->node)
-			break;
-		btrfs_tree_unlock(eb);
-		free_extent_buffer(eb);
-	}
-	return eb;
-}
-
-/* loop around taking references on and locking the root node of the
- * tree until you end up with a lock on the root.  A locked buffer
- * is returned, with a reference held.
- */
-struct extent_buffer *btrfs_read_lock_root_node(struct btrfs_root *root)
-{
-	struct extent_buffer *eb;
-
-	while (1) {
-		eb = btrfs_root_node(root);
-		btrfs_tree_read_lock(eb);
-		if (eb == root->node)
-			break;
-		btrfs_tree_read_unlock(eb);
-		free_extent_buffer(eb);
-	}
-	return eb;
-}
-
 /* cowonly root (everything not a reference counted cow subvolume), just get
  * put onto a simple dirty list.  transaction.c walks this to make sure they
  * get properly updated on disk.

commit 42836cf4ba9c9b1797d1f7fe3245d82cf6dea6c4
Author: Filipe Manana <fdmanana@suse.com>
Date:   Wed Jan 22 12:23:54 2020 +0000

    Btrfs: don't iterate mod seq list when putting a tree mod seq
    
    Each new element added to the mod seq list is always appended to the list,
    and each one gets a sequence number coming from a counter which gets
    incremented everytime a new element is added to the list (or a new node
    is added to the tree mod log rbtree). Therefore the element with the
    lowest sequence number is always the first element in the list.
    
    So just remove the list iteration at btrfs_put_tree_mod_seq() that
    computes the minimum sequence number in the list and replace it with
    a check for the first element's sequence number.
    
    Reviewed-by: Josef Bacik <josef@toxicpanda.com>
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index f2ec1a9bae28..968faaec0e39 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -341,7 +341,6 @@ void btrfs_put_tree_mod_seq(struct btrfs_fs_info *fs_info,
 	struct rb_root *tm_root;
 	struct rb_node *node;
 	struct rb_node *next;
-	struct seq_list *cur_elem;
 	struct tree_mod_elem *tm;
 	u64 min_seq = (u64)-1;
 	u64 seq_putting = elem->seq;
@@ -353,18 +352,20 @@ void btrfs_put_tree_mod_seq(struct btrfs_fs_info *fs_info,
 	list_del(&elem->list);
 	elem->seq = 0;
 
-	list_for_each_entry(cur_elem, &fs_info->tree_mod_seq_list, list) {
-		if (cur_elem->seq < min_seq) {
-			if (seq_putting > cur_elem->seq) {
-				/*
-				 * blocker with lower sequence number exists, we
-				 * cannot remove anything from the log
-				 */
-				write_unlock(&fs_info->tree_mod_log_lock);
-				return;
-			}
-			min_seq = cur_elem->seq;
+	if (!list_empty(&fs_info->tree_mod_seq_list)) {
+		struct seq_list *first;
+
+		first = list_first_entry(&fs_info->tree_mod_seq_list,
+					 struct seq_list, list);
+		if (seq_putting > first->seq) {
+			/*
+			 * Blocker with lower sequence number exists, we
+			 * cannot remove anything from the log.
+			 */
+			write_unlock(&fs_info->tree_mod_log_lock);
+			return;
 		}
+		min_seq = first->seq;
 	}
 
 	/*

commit 7227ff4de55d931bbdc156c8ef0ce4f100c78a5b
Author: Filipe Manana <fdmanana@suse.com>
Date:   Wed Jan 22 12:23:20 2020 +0000

    Btrfs: fix race between adding and putting tree mod seq elements and nodes
    
    There is a race between adding and removing elements to the tree mod log
    list and rbtree that can lead to use-after-free problems.
    
    Consider the following example that explains how/why the problems happens:
    
    1) Task A has mod log element with sequence number 200. It currently is
       the only element in the mod log list;
    
    2) Task A calls btrfs_put_tree_mod_seq() because it no longer needs to
       access the tree mod log. When it enters the function, it initializes
       'min_seq' to (u64)-1. Then it acquires the lock 'tree_mod_seq_lock'
       before checking if there are other elements in the mod seq list.
       Since the list it empty, 'min_seq' remains set to (u64)-1. Then it
       unlocks the lock 'tree_mod_seq_lock';
    
    3) Before task A acquires the lock 'tree_mod_log_lock', task B adds
       itself to the mod seq list through btrfs_get_tree_mod_seq() and gets a
       sequence number of 201;
    
    4) Some other task, name it task C, modifies a btree and because there
       elements in the mod seq list, it adds a tree mod elem to the tree
       mod log rbtree. That node added to the mod log rbtree is assigned
       a sequence number of 202;
    
    5) Task B, which is doing fiemap and resolving indirect back references,
       calls btrfs get_old_root(), with 'time_seq' == 201, which in turn
       calls tree_mod_log_search() - the search returns the mod log node
       from the rbtree with sequence number 202, created by task C;
    
    6) Task A now acquires the lock 'tree_mod_log_lock', starts iterating
       the mod log rbtree and finds the node with sequence number 202. Since
       202 is less than the previously computed 'min_seq', (u64)-1, it
       removes the node and frees it;
    
    7) Task B still has a pointer to the node with sequence number 202, and
       it dereferences the pointer itself and through the call to
       __tree_mod_log_rewind(), resulting in a use-after-free problem.
    
    This issue can be triggered sporadically with the test case generic/561
    from fstests, and it happens more frequently with a higher number of
    duperemove processes. When it happens to me, it either freezes the VM or
    it produces a trace like the following before crashing:
    
      [ 1245.321140] general protection fault: 0000 [#1] PREEMPT SMP DEBUG_PAGEALLOC PTI
      [ 1245.321200] CPU: 1 PID: 26997 Comm: pool Not tainted 5.5.0-rc6-btrfs-next-52 #1
      [ 1245.321235] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS rel-1.12.0-0-ga698c8995f-prebuilt.qemu.org 04/01/2014
      [ 1245.321287] RIP: 0010:rb_next+0x16/0x50
      [ 1245.321307] Code: ....
      [ 1245.321372] RSP: 0018:ffffa151c4d039b0 EFLAGS: 00010202
      [ 1245.321388] RAX: 6b6b6b6b6b6b6b6b RBX: ffff8ae221363c80 RCX: 6b6b6b6b6b6b6b6b
      [ 1245.321409] RDX: 0000000000000001 RSI: 0000000000000000 RDI: ffff8ae221363c80
      [ 1245.321439] RBP: ffff8ae20fcc4688 R08: 0000000000000002 R09: 0000000000000000
      [ 1245.321475] R10: ffff8ae20b120910 R11: 00000000243f8bb1 R12: 0000000000000038
      [ 1245.321506] R13: ffff8ae221363c80 R14: 000000000000075f R15: ffff8ae223f762b8
      [ 1245.321539] FS:  00007fdee1ec7700(0000) GS:ffff8ae236c80000(0000) knlGS:0000000000000000
      [ 1245.321591] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
      [ 1245.321614] CR2: 00007fded4030c48 CR3: 000000021da16003 CR4: 00000000003606e0
      [ 1245.321642] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
      [ 1245.321668] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
      [ 1245.321706] Call Trace:
      [ 1245.321798]  __tree_mod_log_rewind+0xbf/0x280 [btrfs]
      [ 1245.321841]  btrfs_search_old_slot+0x105/0xd00 [btrfs]
      [ 1245.321877]  resolve_indirect_refs+0x1eb/0xc60 [btrfs]
      [ 1245.321912]  find_parent_nodes+0x3dc/0x11b0 [btrfs]
      [ 1245.321947]  btrfs_check_shared+0x115/0x1c0 [btrfs]
      [ 1245.321980]  ? extent_fiemap+0x59d/0x6d0 [btrfs]
      [ 1245.322029]  extent_fiemap+0x59d/0x6d0 [btrfs]
      [ 1245.322066]  do_vfs_ioctl+0x45a/0x750
      [ 1245.322081]  ksys_ioctl+0x70/0x80
      [ 1245.322092]  ? trace_hardirqs_off_thunk+0x1a/0x1c
      [ 1245.322113]  __x64_sys_ioctl+0x16/0x20
      [ 1245.322126]  do_syscall_64+0x5c/0x280
      [ 1245.322139]  entry_SYSCALL_64_after_hwframe+0x49/0xbe
      [ 1245.322155] RIP: 0033:0x7fdee3942dd7
      [ 1245.322177] Code: ....
      [ 1245.322258] RSP: 002b:00007fdee1ec6c88 EFLAGS: 00000246 ORIG_RAX: 0000000000000010
      [ 1245.322294] RAX: ffffffffffffffda RBX: 00007fded40210d8 RCX: 00007fdee3942dd7
      [ 1245.322314] RDX: 00007fded40210d8 RSI: 00000000c020660b RDI: 0000000000000004
      [ 1245.322337] RBP: 0000562aa89e7510 R08: 0000000000000000 R09: 00007fdee1ec6d44
      [ 1245.322369] R10: 0000000000000073 R11: 0000000000000246 R12: 00007fdee1ec6d48
      [ 1245.322390] R13: 00007fdee1ec6d40 R14: 00007fded40210d0 R15: 00007fdee1ec6d50
      [ 1245.322423] Modules linked in: ....
      [ 1245.323443] ---[ end trace 01de1e9ec5dff3cd ]---
    
    Fix this by ensuring that btrfs_put_tree_mod_seq() computes the minimum
    sequence number and iterates the rbtree while holding the lock
    'tree_mod_log_lock' in write mode. Also get rid of the 'tree_mod_seq_lock'
    lock, since it is now redundant.
    
    Fixes: bd989ba359f2ac ("Btrfs: add tree modification log functions")
    Fixes: 097b8a7c9e48e2 ("Btrfs: join tree mod log code with the code holding back delayed refs")
    CC: stable@vger.kernel.org # 4.4+
    Reviewed-by: Josef Bacik <josef@toxicpanda.com>
    Reviewed-by: Nikolay Borisov <nborisov@suse.com>
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 24658b5a5787..f2ec1a9bae28 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -326,12 +326,10 @@ u64 btrfs_get_tree_mod_seq(struct btrfs_fs_info *fs_info,
 			   struct seq_list *elem)
 {
 	write_lock(&fs_info->tree_mod_log_lock);
-	spin_lock(&fs_info->tree_mod_seq_lock);
 	if (!elem->seq) {
 		elem->seq = btrfs_inc_tree_mod_seq(fs_info);
 		list_add_tail(&elem->list, &fs_info->tree_mod_seq_list);
 	}
-	spin_unlock(&fs_info->tree_mod_seq_lock);
 	write_unlock(&fs_info->tree_mod_log_lock);
 
 	return elem->seq;
@@ -351,7 +349,7 @@ void btrfs_put_tree_mod_seq(struct btrfs_fs_info *fs_info,
 	if (!seq_putting)
 		return;
 
-	spin_lock(&fs_info->tree_mod_seq_lock);
+	write_lock(&fs_info->tree_mod_log_lock);
 	list_del(&elem->list);
 	elem->seq = 0;
 
@@ -362,19 +360,17 @@ void btrfs_put_tree_mod_seq(struct btrfs_fs_info *fs_info,
 				 * blocker with lower sequence number exists, we
 				 * cannot remove anything from the log
 				 */
-				spin_unlock(&fs_info->tree_mod_seq_lock);
+				write_unlock(&fs_info->tree_mod_log_lock);
 				return;
 			}
 			min_seq = cur_elem->seq;
 		}
 	}
-	spin_unlock(&fs_info->tree_mod_seq_lock);
 
 	/*
 	 * anything that's lower than the lowest existing (read: blocked)
 	 * sequence number can be removed from the tree.
 	 */
-	write_lock(&fs_info->tree_mod_log_lock);
 	tm_root = &fs_info->tree_mod_log;
 	for (node = rb_first(tm_root); node; node = next) {
 		next = rb_next(node);

commit 6609fee8897ac475378388238456c84298bff802
Author: Filipe Manana <fdmanana@suse.com>
Date:   Fri Dec 6 12:27:39 2019 +0000

    Btrfs: fix removal logic of the tree mod log that leads to use-after-free issues
    
    When a tree mod log user no longer needs to use the tree it calls
    btrfs_put_tree_mod_seq() to remove itself from the list of users and
    delete all no longer used elements of the tree's red black tree, which
    should be all elements with a sequence number less then our equals to
    the caller's sequence number. However the logic is broken because it
    can delete and free elements from the red black tree that have a
    sequence number greater then the caller's sequence number:
    
    1) At a point in time we have sequence numbers 1, 2, 3 and 4 in the
       tree mod log;
    
    2) The task which got assigned the sequence number 1 calls
       btrfs_put_tree_mod_seq();
    
    3) Sequence number 1 is deleted from the list of sequence numbers;
    
    4) The current minimum sequence number is computed to be the sequence
       number 2;
    
    5) A task using sequence number 2 is at tree_mod_log_rewind() and gets
       a pointer to one of its elements from the red black tree through
       a call to tree_mod_log_search();
    
    6) The task with sequence number 1 iterates the red black tree of tree
       modification elements and deletes (and frees) all elements with a
       sequence number less then or equals to 2 (the computed minimum sequence
       number) - it ends up only leaving elements with sequence numbers of 3
       and 4;
    
    7) The task with sequence number 2 now uses the pointer to its element,
       already freed by the other task, at __tree_mod_log_rewind(), resulting
       in a use-after-free issue. When CONFIG_DEBUG_PAGEALLOC=y it produces
       a trace like the following:
    
      [16804.546854] general protection fault: 0000 [#1] PREEMPT SMP DEBUG_PAGEALLOC PTI
      [16804.547451] CPU: 0 PID: 28257 Comm: pool Tainted: G        W         5.4.0-rc8-btrfs-next-51 #1
      [16804.548059] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS rel-1.12.0-0-ga698c8995f-prebuilt.qemu.org 04/01/2014
      [16804.548666] RIP: 0010:rb_next+0x16/0x50
      (...)
      [16804.550581] RSP: 0018:ffffb948418ef9b0 EFLAGS: 00010202
      [16804.551227] RAX: 6b6b6b6b6b6b6b6b RBX: ffff90e0247f6600 RCX: 6b6b6b6b6b6b6b6b
      [16804.551873] RDX: 0000000000000000 RSI: 0000000000000000 RDI: ffff90e0247f6600
      [16804.552504] RBP: ffff90dffe0d4688 R08: 0000000000000001 R09: 0000000000000000
      [16804.553136] R10: ffff90dffa4a0040 R11: 0000000000000000 R12: 000000000000002e
      [16804.553768] R13: ffff90e0247f6600 R14: 0000000000001663 R15: ffff90dff77862b8
      [16804.554399] FS:  00007f4b197ae700(0000) GS:ffff90e036a00000(0000) knlGS:0000000000000000
      [16804.555039] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
      [16804.555683] CR2: 00007f4b10022000 CR3: 00000002060e2004 CR4: 00000000003606f0
      [16804.556336] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
      [16804.556968] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
      [16804.557583] Call Trace:
      [16804.558207]  __tree_mod_log_rewind+0xbf/0x280 [btrfs]
      [16804.558835]  btrfs_search_old_slot+0x105/0xd00 [btrfs]
      [16804.559468]  resolve_indirect_refs+0x1eb/0xc70 [btrfs]
      [16804.560087]  ? free_extent_buffer.part.19+0x5a/0xc0 [btrfs]
      [16804.560700]  find_parent_nodes+0x388/0x1120 [btrfs]
      [16804.561310]  btrfs_check_shared+0x115/0x1c0 [btrfs]
      [16804.561916]  ? extent_fiemap+0x59d/0x6d0 [btrfs]
      [16804.562518]  extent_fiemap+0x59d/0x6d0 [btrfs]
      [16804.563112]  ? __might_fault+0x11/0x90
      [16804.563706]  do_vfs_ioctl+0x45a/0x700
      [16804.564299]  ksys_ioctl+0x70/0x80
      [16804.564885]  ? trace_hardirqs_off_thunk+0x1a/0x20
      [16804.565461]  __x64_sys_ioctl+0x16/0x20
      [16804.566020]  do_syscall_64+0x5c/0x250
      [16804.566580]  entry_SYSCALL_64_after_hwframe+0x49/0xbe
      [16804.567153] RIP: 0033:0x7f4b1ba2add7
      (...)
      [16804.568907] RSP: 002b:00007f4b197adc88 EFLAGS: 00000246 ORIG_RAX: 0000000000000010
      [16804.569513] RAX: ffffffffffffffda RBX: 00007f4b100210d8 RCX: 00007f4b1ba2add7
      [16804.570133] RDX: 00007f4b100210d8 RSI: 00000000c020660b RDI: 0000000000000003
      [16804.570726] RBP: 000055de05a6cfe0 R08: 0000000000000000 R09: 00007f4b197add44
      [16804.571314] R10: 0000000000000000 R11: 0000000000000246 R12: 00007f4b197add48
      [16804.571905] R13: 00007f4b197add40 R14: 00007f4b100210d0 R15: 00007f4b197add50
      (...)
      [16804.575623] ---[ end trace 87317359aad4ba50 ]---
    
    Fix this by making btrfs_put_tree_mod_seq() skip deletion of elements that
    have a sequence number equals to the computed minimum sequence number, and
    not just elements with a sequence number greater then that minimum.
    
    Fixes: bd989ba359f2ac ("Btrfs: add tree modification log functions")
    CC: stable@vger.kernel.org # 4.4+
    Reviewed-by: Josef Bacik <josef@toxicpanda.com>
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 5b6e86aaf2e1..24658b5a5787 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -379,7 +379,7 @@ void btrfs_put_tree_mod_seq(struct btrfs_fs_info *fs_info,
 	for (node = rb_first(tm_root); node; node = next) {
 		next = rb_next(node);
 		tm = rb_entry(node, struct tree_mod_elem, node);
-		if (tm->seq > min_seq)
+		if (tm->seq >= min_seq)
 			continue;
 		rb_erase(node, tm_root);
 		kfree(tm);

commit 352ae07b599a03b08a2149d9c2ff9c3479591af2
Author: David Sterba <dsterba@suse.com>
Date:   Mon Oct 7 11:11:02 2019 +0200

    btrfs: add blake2b to checksumming algorithms
    
    Add blake2b (with 256 bit digest) to the list of possible checksumming
    algorithms used by BTRFS.
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index c211216b4524..5b6e86aaf2e1 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -37,6 +37,8 @@ static const struct btrfs_csums {
 	[BTRFS_CSUM_TYPE_CRC32] = { .size = 4, .name = "crc32c" },
 	[BTRFS_CSUM_TYPE_XXHASH] = { .size = 8, .name = "xxhash64" },
 	[BTRFS_CSUM_TYPE_SHA256] = { .size = 32, .name = "sha256" },
+	[BTRFS_CSUM_TYPE_BLAKE2] = { .size = 32, .name = "blake2b",
+				     .driver = "blake2b-256" },
 };
 
 int btrfs_super_csum_size(const struct btrfs_super_block *s)

commit b4e967be431cf37f56cd1993592943007d7ab03b
Author: David Sterba <dsterba@suse.com>
Date:   Tue Oct 8 18:41:33 2019 +0200

    btrfs: add member for a specific checksum driver
    
    Currently all the checksum algorithms generate a fixed size digest size
    and we use it.  The on-disk format can hold up to BTRFS_CSUM_SIZE bytes
    and BLAKE2b produces digest of 512 bits by default. We can't do that and
    will use the blake2b-256, this needs to be passed to the crypto API.
    
    Separate that from the base algorithm name and add a member to request
    specific driver, in this case with the digest size.
    
    The only place that uses the driver name is the crypto API setup.
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index c1561097e5a3..c211216b4524 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -32,6 +32,7 @@ static void del_ptr(struct btrfs_root *root, struct btrfs_path *path,
 static const struct btrfs_csums {
 	u16		size;
 	const char	*name;
+	const char	*driver;
 } btrfs_csums[] = {
 	[BTRFS_CSUM_TYPE_CRC32] = { .size = 4, .name = "crc32c" },
 	[BTRFS_CSUM_TYPE_XXHASH] = { .size = 8, .name = "xxhash64" },
@@ -53,6 +54,17 @@ const char *btrfs_super_csum_name(u16 csum_type)
 	return btrfs_csums[csum_type].name;
 }
 
+/*
+ * Return driver name if defined, otherwise the name that's also a valid driver
+ * name
+ */
+const char *btrfs_super_csum_driver(u16 csum_type)
+{
+	/* csum type is validated at mount time */
+	return btrfs_csums[csum_type].driver ?:
+		btrfs_csums[csum_type].name;
+}
+
 size_t __const btrfs_get_num_csums(void)
 {
 	return ARRAY_SIZE(btrfs_csums);

commit f7cea56c0fff95bd5a6cd21b9fa299f66193b604
Author: David Sterba <dsterba@suse.com>
Date:   Mon Oct 7 11:11:03 2019 +0200

    btrfs: sysfs: export supported checksums
    
    Export supported checksum algorithms via sysfs in the list of static
    features:
    
      /sys/fs/btrfs/features/supported_checksums
    
    Space spearated list of checksum algorithm names.
    
    Co-developed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index bd6ea433ed65..c1561097e5a3 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -53,6 +53,11 @@ const char *btrfs_super_csum_name(u16 csum_type)
 	return btrfs_csums[csum_type].name;
 }
 
+size_t __const btrfs_get_num_csums(void)
+{
+	return ARRAY_SIZE(btrfs_csums);
+}
+
 struct btrfs_path *btrfs_alloc_path(void)
 {
 	return kmem_cache_zalloc(btrfs_path_cachep, GFP_NOFS);

commit 3831bf0094abed51e71cbeca8b6edf8b88c2644b
Author: Johannes Thumshirn <jthumshirn@suse.de>
Date:   Mon Oct 7 11:11:02 2019 +0200

    btrfs: add sha256 to checksumming algorithm
    
    Add sha256 to the list of possible checksumming algorithms used by BTRFS.
    
    Signed-off-by: Johannes Thumshirn <jthumshirn@suse.de>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 7463c121791d..bd6ea433ed65 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -35,6 +35,7 @@ static const struct btrfs_csums {
 } btrfs_csums[] = {
 	[BTRFS_CSUM_TYPE_CRC32] = { .size = 4, .name = "crc32c" },
 	[BTRFS_CSUM_TYPE_XXHASH] = { .size = 8, .name = "xxhash64" },
+	[BTRFS_CSUM_TYPE_SHA256] = { .size = 32, .name = "sha256" },
 };
 
 int btrfs_super_csum_size(const struct btrfs_super_block *s)

commit 3951e7f050ac6a38bbc859fc3cd6093890c31d1c
Author: Johannes Thumshirn <jthumshirn@suse.de>
Date:   Mon Oct 7 11:11:01 2019 +0200

    btrfs: add xxhash64 to checksumming algorithms
    
    Add xxhash64 to the list of possible checksumming algorithms used by
    BTRFS.
    
    Signed-off-by: Johannes Thumshirn <jthumshirn@suse.de>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 739757978686..7463c121791d 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -34,6 +34,7 @@ static const struct btrfs_csums {
 	const char	*name;
 } btrfs_csums[] = {
 	[BTRFS_CSUM_TYPE_CRC32] = { .size = 4, .name = "crc32c" },
+	[BTRFS_CSUM_TYPE_XXHASH] = { .size = 8, .name = "xxhash64" },
 };
 
 int btrfs_super_csum_size(const struct btrfs_super_block *s)

commit 67439dadb03ad9da45bfccb4cdb6ef6b1a7f8da9
Author: David Sterba <dsterba@suse.com>
Date:   Tue Oct 8 13:28:47 2019 +0200

    btrfs: opencode extent_buffer_get
    
    The helper is trivial and we can understand what the atomic_inc on
    something named refs does.
    
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Reviewed-by: Anand Jain <anand.jain@oracle.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 3a4d8e27e565..739757978686 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1100,7 +1100,7 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 		    btrfs_header_backref_rev(buf) < BTRFS_MIXED_BACKREF_REV)
 			parent_start = buf->start;
 
-		extent_buffer_get(cow);
+		atomic_inc(&cow->refs);
 		ret = tree_mod_log_insert_root(root->node, cow, 1);
 		BUG_ON(ret < 0);
 		rcu_assign_pointer(root->node, cow);
@@ -2011,7 +2011,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 	/* update the path */
 	if (left) {
 		if (btrfs_header_nritems(left) > orig_slot) {
-			extent_buffer_get(left);
+			atomic_inc(&left->refs);
 			/* left was locked after cow */
 			path->nodes[level] = left;
 			path->slots[level + 1] -= 1;
@@ -2601,7 +2601,7 @@ static struct extent_buffer *btrfs_search_slot_get_root(struct btrfs_root *root,
 
 		} else {
 			b = root->commit_root;
-			extent_buffer_get(b);
+			atomic_inc(&b->refs);
 		}
 		level = btrfs_header_level(b);
 		/*
@@ -3375,7 +3375,7 @@ static noinline int insert_new_root(struct btrfs_trans_handle *trans,
 	free_extent_buffer(old);
 
 	add_root_to_dirty_list(root);
-	extent_buffer_get(c);
+	atomic_inc(&c->refs);
 	path->nodes[level] = c;
 	path->locks[level] = BTRFS_WRITE_LOCK_BLOCKING;
 	path->slots[level] = 0;
@@ -4908,7 +4908,7 @@ static noinline void btrfs_del_leaf(struct btrfs_trans_handle *trans,
 
 	root_sub_used(root, leaf->len);
 
-	extent_buffer_get(leaf);
+	atomic_inc(&leaf->refs);
 	btrfs_free_tree_block(trans, root, leaf, 0, 1);
 	free_extent_buffer_stale(leaf);
 }
@@ -4989,7 +4989,7 @@ int btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 			 * for possible call to del_ptr below
 			 */
 			slot = path->slots[1];
-			extent_buffer_get(leaf);
+			atomic_inc(&leaf->refs);
 
 			btrfs_set_path_blocking(path);
 			wret = push_leaf_left(trans, root, path, 1, 1,

commit e1f60a6580c04d0d2492bb6034e968b8c29c78cf
Author: David Sterba <dsterba@suse.com>
Date:   Tue Oct 1 19:57:39 2019 +0200

    btrfs: add __pure attribute to functions
    
    The attribute is more relaxed than const and the functions could
    dereference pointers, as long as the observable state is not changed. We
    do have such functions, based on -Wsuggest-attribute=pure .
    
    The visible effects of this patch are negligible, there are differences
    in the assembly but hard to summarize.
    
    Reviewed-by: Nikolay Borisov <nborisov@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index f2f9cf1149a4..3a4d8e27e565 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1538,7 +1538,7 @@ static int comp_keys(const struct btrfs_disk_key *disk,
 /*
  * same as comp_keys only with two btrfs_key's
  */
-int btrfs_comp_cpu_keys(const struct btrfs_key *k1, const struct btrfs_key *k2)
+int __pure btrfs_comp_cpu_keys(const struct btrfs_key *k1, const struct btrfs_key *k2)
 {
 	if (k1->objectid > k2->objectid)
 		return 1;

commit 1f95ec012cb4a3fabfef3efd9ba0b59e14ce48ce
Author: David Sterba <dsterba@suse.com>
Date:   Tue Sep 24 19:17:17 2019 +0200

    btrfs: move btrfs_unlock_up_safe to other locking functions
    
    The function belongs to the family of locking functions, so move it
    there. The 'noinline' keyword is dropped as it's now an exported
    function that does not need it.
    
    Reviewed-by: Josef Bacik <josef@toxicpanda.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index a55d55e5c913..f2f9cf1149a4 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2353,32 +2353,6 @@ static noinline void unlock_up(struct btrfs_path *path, int level,
 	}
 }
 
-/*
- * This releases any locks held in the path starting at level and
- * going all the way up to the root.
- *
- * btrfs_search_slot will keep the lock held on higher nodes in a few
- * corner cases, such as COW of the block at slot zero in the node.  This
- * ignores those rules, and it should only be called when there are no
- * more updates to be done higher up in the tree.
- */
-noinline void btrfs_unlock_up_safe(struct btrfs_path *path, int level)
-{
-	int i;
-
-	if (path->keep_locks)
-		return;
-
-	for (i = level; i < BTRFS_MAX_LEVEL; i++) {
-		if (!path->nodes[i])
-			continue;
-		if (!path->locks[i])
-			continue;
-		btrfs_tree_unlock_rw(path->nodes[i], path->locks[i]);
-		path->locks[i] = 0;
-	}
-}
-
 /*
  * helper function for btrfs_search_slot.  The goal is to find a block
  * in cache without setting the path to blocking.  If we find the block

commit ed2b1d36a9d027f9b841be5bfc9d61011462d447
Author: David Sterba <dsterba@suse.com>
Date:   Tue Sep 24 19:17:17 2019 +0200

    btrfs: move btrfs_set_path_blocking to other locking functions
    
    The function belongs to the family of locking functions, so move it
    there. The 'noinline' keyword is dropped as it's now an exported
    function that does not need it.
    
    Reviewed-by: Josef Bacik <josef@toxicpanda.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 0231141de289..a55d55e5c913 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -56,31 +56,6 @@ struct btrfs_path *btrfs_alloc_path(void)
 	return kmem_cache_zalloc(btrfs_path_cachep, GFP_NOFS);
 }
 
-/*
- * set all locked nodes in the path to blocking locks.  This should
- * be done before scheduling
- */
-noinline void btrfs_set_path_blocking(struct btrfs_path *p)
-{
-	int i;
-	for (i = 0; i < BTRFS_MAX_LEVEL; i++) {
-		if (!p->nodes[i] || !p->locks[i])
-			continue;
-		/*
-		 * If we currently have a spinning reader or writer lock this
-		 * will bump the count of blocking holders and drop the
-		 * spinlock.
-		 */
-		if (p->locks[i] == BTRFS_READ_LOCK) {
-			btrfs_set_lock_blocking_read(p->nodes[i]);
-			p->locks[i] = BTRFS_READ_LOCK_BLOCKING;
-		} else if (p->locks[i] == BTRFS_WRITE_LOCK) {
-			btrfs_set_lock_blocking_write(p->nodes[i]);
-			p->locks[i] = BTRFS_WRITE_LOCK_BLOCKING;
-		}
-	}
-}
-
 /* this also releases the path */
 void btrfs_free_path(struct btrfs_path *p)
 {

commit 34ffafdba12eee3e387694edd128e954fd355e8e
Author: Qu Wenruo <wqu@suse.com>
Date:   Tue Sep 10 15:40:19 2019 +0800

    btrfs: ctree: Remove stray comment of setting up path lock
    
    The following comment shows up in btrfs_search_slot() with out much
    sense:
    
            /*
             * setup the path here so we can release it under lock
             * contention with the cow code
             */
            if (cow) {
                    /* code touching path->lock[] is far away from here */
            }
    
    This comment hasn't been cleaned up after the relevant code has been
    removed.
    
    The original code is introduced in commit 65b51a009e29
    ("btrfs_search_slot: reduce lock contention by cowing in two stages"):
    
      +
      +               /*
      +                * setup the path here so we can release it under lock
      +                * contention with the cow code
      +                */
      +               p->nodes[level] = b;
      +               if (!p->skip_locking)
      +                       p->locks[level] = 1;
      +
    
    But in current code, we have different timing for modifying path lock,
    so just remove the comment.
    
    Reviewed-by: Nikolay Borisov <nborisov@suse.com>
    Signed-off-by: Qu Wenruo <wqu@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 42ca326a0e6b..0231141de289 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2789,10 +2789,6 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 
 		level = btrfs_header_level(b);
 
-		/*
-		 * setup the path here so we can release it under lock
-		 * contention with the cow code
-		 */
 		if (cow) {
 			bool last_level = (level == (BTRFS_MAX_LEVEL - 1));
 

commit abe9339d69bd096e2020ffc39fedee2ac0a9c03b
Author: Qu Wenruo <wqu@suse.com>
Date:   Tue Sep 10 15:40:18 2019 +0800

    btrfs: ctree: Reduce one indent level for btrfs_search_old_slot()
    
    Similar to btrfs_search_slot() done in previous patch, make a shortcut
    for the level 0 case and allow to reduce indentation for the remaining
    case.
    
    Reviewed-by: Anand Jain <anand.jain@oracle.com>
    Signed-off-by: Qu Wenruo <wqu@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    [ update changelog ]
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index dea29cd1f2e8..42ca326a0e6b 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -3005,6 +3005,8 @@ int btrfs_search_old_slot(struct btrfs_root *root, const struct btrfs_key *key,
 	p->locks[level] = BTRFS_READ_LOCK;
 
 	while (b) {
+		int dec = 0;
+
 		level = btrfs_header_level(b);
 		p->nodes[level] = b;
 
@@ -3025,47 +3027,45 @@ int btrfs_search_old_slot(struct btrfs_root *root, const struct btrfs_key *key,
 		if (ret < 0)
 			goto done;
 
-		if (level != 0) {
-			int dec = 0;
-			if (ret && slot > 0) {
-				dec = 1;
-				slot -= 1;
-			}
+		if (level == 0) {
 			p->slots[level] = slot;
 			unlock_up(p, level, lowest_unlock, 0, NULL);
+			goto done;
+		}
 
-			if (level == lowest_level) {
-				if (dec)
-					p->slots[level]++;
-				goto done;
-			}
+		if (ret && slot > 0) {
+			dec = 1;
+			slot--;
+		}
+		p->slots[level] = slot;
+		unlock_up(p, level, lowest_unlock, 0, NULL);
 
-			err = read_block_for_search(root, p, &b, level,
-						    slot, key);
-			if (err == -EAGAIN)
-				goto again;
-			if (err) {
-				ret = err;
-				goto done;
-			}
+		if (level == lowest_level) {
+			if (dec)
+				p->slots[level]++;
+			goto done;
+		}
 
-			level = btrfs_header_level(b);
-			if (!btrfs_tree_read_lock_atomic(b)) {
-				btrfs_set_path_blocking(p);
-				btrfs_tree_read_lock(b);
-			}
-			b = tree_mod_log_rewind(fs_info, p, b, time_seq);
-			if (!b) {
-				ret = -ENOMEM;
-				goto done;
-			}
-			p->locks[level] = BTRFS_READ_LOCK;
-			p->nodes[level] = b;
-		} else {
-			p->slots[level] = slot;
-			unlock_up(p, level, lowest_unlock, 0, NULL);
+		err = read_block_for_search(root, p, &b, level, slot, key);
+		if (err == -EAGAIN)
+			goto again;
+		if (err) {
+			ret = err;
 			goto done;
 		}
+
+		level = btrfs_header_level(b);
+		if (!btrfs_tree_read_lock_atomic(b)) {
+			btrfs_set_path_blocking(p);
+			btrfs_tree_read_lock(b);
+		}
+		b = tree_mod_log_rewind(fs_info, p, b, time_seq);
+		if (!b) {
+			ret = -ENOMEM;
+			goto done;
+		}
+		p->locks[level] = BTRFS_READ_LOCK;
+		p->nodes[level] = b;
 	}
 	ret = 1;
 done:

commit f624d976081d371b3dea65935501d8d9f142d5df
Author: Qu Wenruo <wqu@suse.com>
Date:   Tue Sep 10 15:40:17 2019 +0800

    btrfs: ctree: Reduce one indent level for btrfs_search_slot()
    
    In btrfs_search_slot(), we something like:
    
            if (level != 0) {
                    /* Do search inside tree nodes*/
            } else {
                    /* Do search inside tree leaves */
                    goto done;
            }
    
    This caused extra indent for tree node search code.  Change it to
    something like:
    
            if (level == 0) {
                    /* Do search inside tree leaves */
                    goto done'
            }
            /* Do search inside tree nodes */
    
    So we have more space to maneuver our code, this is especially useful as
    the tree nodes search code is more complex than the leaves search code.
    
    Reviewed-by: Anand Jain <anand.jain@oracle.com>
    Signed-off-by: Qu Wenruo <wqu@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index e59cde204b2f..dea29cd1f2e8 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2785,6 +2785,8 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 	}
 
 	while (b) {
+		int dec = 0;
+
 		level = btrfs_header_level(b);
 
 		/*
@@ -2861,73 +2863,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 		if (ret < 0)
 			goto done;
 
-		if (level != 0) {
-			int dec = 0;
-			if (ret && slot > 0) {
-				dec = 1;
-				slot -= 1;
-			}
-			p->slots[level] = slot;
-			err = setup_nodes_for_search(trans, root, p, b, level,
-					     ins_len, &write_lock_level);
-			if (err == -EAGAIN)
-				goto again;
-			if (err) {
-				ret = err;
-				goto done;
-			}
-			b = p->nodes[level];
-			slot = p->slots[level];
-
-			/*
-			 * slot 0 is special, if we change the key
-			 * we have to update the parent pointer
-			 * which means we must have a write lock
-			 * on the parent
-			 */
-			if (slot == 0 && ins_len &&
-			    write_lock_level < level + 1) {
-				write_lock_level = level + 1;
-				btrfs_release_path(p);
-				goto again;
-			}
-
-			unlock_up(p, level, lowest_unlock,
-				  min_write_lock_level, &write_lock_level);
-
-			if (level == lowest_level) {
-				if (dec)
-					p->slots[level]++;
-				goto done;
-			}
-
-			err = read_block_for_search(root, p, &b, level,
-						    slot, key);
-			if (err == -EAGAIN)
-				goto again;
-			if (err) {
-				ret = err;
-				goto done;
-			}
-
-			if (!p->skip_locking) {
-				level = btrfs_header_level(b);
-				if (level <= write_lock_level) {
-					if (!btrfs_try_tree_write_lock(b)) {
-						btrfs_set_path_blocking(p);
-						btrfs_tree_lock(b);
-					}
-					p->locks[level] = BTRFS_WRITE_LOCK;
-				} else {
-					if (!btrfs_tree_read_lock_atomic(b)) {
-						btrfs_set_path_blocking(p);
-						btrfs_tree_read_lock(b);
-					}
-					p->locks[level] = BTRFS_READ_LOCK;
-				}
-				p->nodes[level] = b;
-			}
-		} else {
+		if (level == 0) {
 			p->slots[level] = slot;
 			if (ins_len > 0 &&
 			    btrfs_leaf_free_space(b) < ins_len) {
@@ -2952,6 +2888,67 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 					  min_write_lock_level, NULL);
 			goto done;
 		}
+		if (ret && slot > 0) {
+			dec = 1;
+			slot--;
+		}
+		p->slots[level] = slot;
+		err = setup_nodes_for_search(trans, root, p, b, level, ins_len,
+					     &write_lock_level);
+		if (err == -EAGAIN)
+			goto again;
+		if (err) {
+			ret = err;
+			goto done;
+		}
+		b = p->nodes[level];
+		slot = p->slots[level];
+
+		/*
+		 * Slot 0 is special, if we change the key we have to update
+		 * the parent pointer which means we must have a write lock on
+		 * the parent
+		 */
+		if (slot == 0 && ins_len && write_lock_level < level + 1) {
+			write_lock_level = level + 1;
+			btrfs_release_path(p);
+			goto again;
+		}
+
+		unlock_up(p, level, lowest_unlock, min_write_lock_level,
+			  &write_lock_level);
+
+		if (level == lowest_level) {
+			if (dec)
+				p->slots[level]++;
+			goto done;
+		}
+
+		err = read_block_for_search(root, p, &b, level, slot, key);
+		if (err == -EAGAIN)
+			goto again;
+		if (err) {
+			ret = err;
+			goto done;
+		}
+
+		if (!p->skip_locking) {
+			level = btrfs_header_level(b);
+			if (level <= write_lock_level) {
+				if (!btrfs_try_tree_write_lock(b)) {
+					btrfs_set_path_blocking(p);
+					btrfs_tree_lock(b);
+				}
+				p->locks[level] = BTRFS_WRITE_LOCK;
+			} else {
+				if (!btrfs_tree_read_lock_atomic(b)) {
+					btrfs_set_path_blocking(p);
+					btrfs_tree_read_lock(b);
+				}
+				p->locks[level] = BTRFS_READ_LOCK;
+			}
+			p->nodes[level] = b;
+		}
 	}
 	ret = 1;
 done:

commit 65e99c43e9c2fee1a1f02c100154730fbeae9717
Author: Nikolay Borisov <nborisov@suse.com>
Date:   Wed Sep 4 20:22:39 2019 +0300

    btrfs: Don't assign retval of btrfs_try_tree_write_lock/btrfs_tree_read_lock_atomic
    
    Those function are simple boolean predicates there is no need to assign
    their return values to interim variables. Use them directly as
    predicates. No functional changes.
    
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 98f741c85905..e59cde204b2f 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2913,15 +2913,13 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 			if (!p->skip_locking) {
 				level = btrfs_header_level(b);
 				if (level <= write_lock_level) {
-					err = btrfs_try_tree_write_lock(b);
-					if (!err) {
+					if (!btrfs_try_tree_write_lock(b)) {
 						btrfs_set_path_blocking(p);
 						btrfs_tree_lock(b);
 					}
 					p->locks[level] = BTRFS_WRITE_LOCK;
 				} else {
-					err = btrfs_tree_read_lock_atomic(b);
-					if (!err) {
+					if (!btrfs_tree_read_lock_atomic(b)) {
 						btrfs_set_path_blocking(p);
 						btrfs_tree_read_lock(b);
 					}
@@ -3055,8 +3053,7 @@ int btrfs_search_old_slot(struct btrfs_root *root, const struct btrfs_key *key,
 			}
 
 			level = btrfs_header_level(b);
-			err = btrfs_tree_read_lock_atomic(b);
-			if (!err) {
+			if (!btrfs_tree_read_lock_atomic(b)) {
 				btrfs_set_path_blocking(p);
 				btrfs_tree_read_lock(b);
 			}

commit af024ed2e0e56f27279cdba4d27a23dbb7677e40
Author: Johannes Thumshirn <jthumshirn@suse.de>
Date:   Fri Aug 30 13:36:09 2019 +0200

    btrfs: create structure to encode checksum type and length
    
    Create a structure to encode the type and length for the known on-disk
    checksums.  This makes it easier to add new checksums later.
    
    The structure and helpers are moved from ctree.h so they don't occupy
    space in all headers including ctree.h. This save some space in the
    final object.
    
    Reviewed-by: Nikolay Borisov <nborisov@suse.com>
    Signed-off-by: Johannes Thumshirn <jthumshirn@suse.de>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 88c3b338508d..98f741c85905 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -29,6 +29,28 @@ static int balance_node_right(struct btrfs_trans_handle *trans,
 static void del_ptr(struct btrfs_root *root, struct btrfs_path *path,
 		    int level, int slot);
 
+static const struct btrfs_csums {
+	u16		size;
+	const char	*name;
+} btrfs_csums[] = {
+	[BTRFS_CSUM_TYPE_CRC32] = { .size = 4, .name = "crc32c" },
+};
+
+int btrfs_super_csum_size(const struct btrfs_super_block *s)
+{
+	u16 t = btrfs_super_csum_type(s);
+	/*
+	 * csum type is validated at mount time
+	 */
+	return btrfs_csums[t].size;
+}
+
+const char *btrfs_super_csum_name(u16 csum_type)
+{
+	/* csum type is validated at mount time */
+	return btrfs_csums[csum_type].name;
+}
+
 struct btrfs_path *btrfs_alloc_path(void)
 {
 	return kmem_cache_zalloc(btrfs_path_cachep, GFP_NOFS);

commit c82f823c9b006c31059341af41da9f8b2e3e64d9
Author: David Sterba <dsterba@suse.com>
Date:   Fri Aug 9 17:48:21 2019 +0200

    btrfs: tie extent buffer and it's token together
    
    Further simplifaction of the get/set helpers is possible when the token
    is uniquely tied to an extent buffer. A condition and an assignment can
    be avoided.
    
    The initializations are moved closer to the first use when the extent
    buffer is valid. There's one exception in __push_leaf_left where the
    token is reused.
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index fbf94e28fba8..88c3b338508d 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -3574,7 +3574,7 @@ static int leaf_space_used(struct extent_buffer *l, int start, int nr)
 
 	if (!nr)
 		return 0;
-	btrfs_init_map_token(&token);
+	btrfs_init_map_token(&token, l);
 	start_item = btrfs_item_nr(start);
 	end_item = btrfs_item_nr(end);
 	data_len = btrfs_token_item_offset(l, start_item, &token) +
@@ -3632,8 +3632,6 @@ static noinline int __push_leaf_right(struct btrfs_path *path,
 	u32 data_end;
 	u32 this_item_size;
 
-	btrfs_init_map_token(&token);
-
 	if (empty)
 		nr = 0;
 	else
@@ -3706,6 +3704,7 @@ static noinline int __push_leaf_right(struct btrfs_path *path,
 		   push_items * sizeof(struct btrfs_item));
 
 	/* update the item pointers */
+	btrfs_init_map_token(&token, right);
 	right_nritems += push_items;
 	btrfs_set_header_nritems(right, right_nritems);
 	push_space = BTRFS_LEAF_DATA_SIZE(fs_info);
@@ -3860,8 +3859,6 @@ static noinline int __push_leaf_left(struct btrfs_path *path, int data_size,
 	u32 old_left_item_size;
 	struct btrfs_map_token token;
 
-	btrfs_init_map_token(&token);
-
 	if (empty)
 		nr = min(right_nritems, max_slot);
 	else
@@ -3915,6 +3912,7 @@ static noinline int __push_leaf_left(struct btrfs_path *path, int data_size,
 	old_left_nritems = btrfs_header_nritems(left);
 	BUG_ON(old_left_nritems <= 0);
 
+	btrfs_init_map_token(&token, left);
 	old_left_item_size = btrfs_item_offset_nr(left, old_left_nritems - 1);
 	for (i = old_left_nritems; i < old_left_nritems + push_items; i++) {
 		u32 ioff;
@@ -3946,6 +3944,8 @@ static noinline int __push_leaf_left(struct btrfs_path *path, int data_size,
 			     (btrfs_header_nritems(right) - push_items) *
 			     sizeof(struct btrfs_item));
 	}
+
+	btrfs_init_map_token(&token, right);
 	right_nritems -= push_items;
 	btrfs_set_header_nritems(right, right_nritems);
 	push_space = BTRFS_LEAF_DATA_SIZE(fs_info);
@@ -4076,8 +4076,6 @@ static noinline void copy_for_split(struct btrfs_trans_handle *trans,
 	struct btrfs_disk_key disk_key;
 	struct btrfs_map_token token;
 
-	btrfs_init_map_token(&token);
-
 	nritems = nritems - mid;
 	btrfs_set_header_nritems(right, nritems);
 	data_copy_size = btrfs_item_end_nr(l, mid) - leaf_data_end(l);
@@ -4093,6 +4091,7 @@ static noinline void copy_for_split(struct btrfs_trans_handle *trans,
 
 	rt_data_off = BTRFS_LEAF_DATA_SIZE(fs_info) - btrfs_item_end_nr(l, mid);
 
+	btrfs_init_map_token(&token, right);
 	for (i = 0; i < nritems; i++) {
 		struct btrfs_item *item = btrfs_item_nr(i);
 		u32 ioff;
@@ -4576,8 +4575,6 @@ void btrfs_truncate_item(struct btrfs_path *path, u32 new_size, int from_end)
 	int i;
 	struct btrfs_map_token token;
 
-	btrfs_init_map_token(&token);
-
 	leaf = path->nodes[0];
 	slot = path->slots[0];
 
@@ -4599,6 +4596,7 @@ void btrfs_truncate_item(struct btrfs_path *path, u32 new_size, int from_end)
 	 * item0..itemN ... dataN.offset..dataN.size .. data0.size
 	 */
 	/* first correct the data pointers */
+	btrfs_init_map_token(&token, leaf);
 	for (i = slot; i < nritems; i++) {
 		u32 ioff;
 		item = btrfs_item_nr(i);
@@ -4673,8 +4671,6 @@ void btrfs_extend_item(struct btrfs_path *path, u32 data_size)
 	int i;
 	struct btrfs_map_token token;
 
-	btrfs_init_map_token(&token);
-
 	leaf = path->nodes[0];
 
 	nritems = btrfs_header_nritems(leaf);
@@ -4699,6 +4695,7 @@ void btrfs_extend_item(struct btrfs_path *path, u32 data_size)
 	 * item0..itemN ... dataN.offset..dataN.size .. data0.size
 	 */
 	/* first correct the data pointers */
+	btrfs_init_map_token(&token, leaf);
 	for (i = slot; i < nritems; i++) {
 		u32 ioff;
 		item = btrfs_item_nr(i);
@@ -4750,8 +4747,6 @@ void setup_items_for_insert(struct btrfs_root *root, struct btrfs_path *path,
 	}
 	btrfs_unlock_up_safe(path, 1);
 
-	btrfs_init_map_token(&token);
-
 	leaf = path->nodes[0];
 	slot = path->slots[0];
 
@@ -4765,6 +4760,7 @@ void setup_items_for_insert(struct btrfs_root *root, struct btrfs_path *path,
 		BUG();
 	}
 
+	btrfs_init_map_token(&token, leaf);
 	if (slot != nritems) {
 		unsigned int old_data = btrfs_item_end_nr(leaf, slot);
 
@@ -4971,9 +4967,6 @@ int btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 	int wret;
 	int i;
 	u32 nritems;
-	struct btrfs_map_token token;
-
-	btrfs_init_map_token(&token);
 
 	leaf = path->nodes[0];
 	last_off = btrfs_item_offset_nr(leaf, slot + nr - 1);
@@ -4985,12 +4978,14 @@ int btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 
 	if (slot + nr != nritems) {
 		int data_end = leaf_data_end(leaf);
+		struct btrfs_map_token token;
 
 		memmove_extent_buffer(leaf, BTRFS_LEAF_DATA_OFFSET +
 			      data_end + dsize,
 			      BTRFS_LEAF_DATA_OFFSET + data_end,
 			      last_off - data_end);
 
+		btrfs_init_map_token(&token, leaf);
 		for (i = slot + nr; i < nritems; i++) {
 			u32 ioff;
 

commit 18d0f5c6e16ce762f92ab7879c30ff2e37cd9cef
Author: David Sterba <dsterba@suse.com>
Date:   Wed Aug 21 19:12:59 2019 +0200

    btrfs: move functions for tree compare to send.c
    
    Send is the only user of tree_compare, we can move it there along with
    the other helpers and definitions.
    
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 3b585f3e4d11..fbf94e28fba8 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -5246,368 +5246,6 @@ int btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,
 	return ret;
 }
 
-static int tree_move_down(struct btrfs_path *path, int *level)
-{
-	struct extent_buffer *eb;
-
-	BUG_ON(*level == 0);
-	eb = btrfs_read_node_slot(path->nodes[*level], path->slots[*level]);
-	if (IS_ERR(eb))
-		return PTR_ERR(eb);
-
-	path->nodes[*level - 1] = eb;
-	path->slots[*level - 1] = 0;
-	(*level)--;
-	return 0;
-}
-
-static int tree_move_next_or_upnext(struct btrfs_path *path,
-				    int *level, int root_level)
-{
-	int ret = 0;
-	int nritems;
-	nritems = btrfs_header_nritems(path->nodes[*level]);
-
-	path->slots[*level]++;
-
-	while (path->slots[*level] >= nritems) {
-		if (*level == root_level)
-			return -1;
-
-		/* move upnext */
-		path->slots[*level] = 0;
-		free_extent_buffer(path->nodes[*level]);
-		path->nodes[*level] = NULL;
-		(*level)++;
-		path->slots[*level]++;
-
-		nritems = btrfs_header_nritems(path->nodes[*level]);
-		ret = 1;
-	}
-	return ret;
-}
-
-/*
- * Returns 1 if it had to move up and next. 0 is returned if it moved only next
- * or down.
- */
-static int tree_advance(struct btrfs_path *path,
-			int *level, int root_level,
-			int allow_down,
-			struct btrfs_key *key)
-{
-	int ret;
-
-	if (*level == 0 || !allow_down) {
-		ret = tree_move_next_or_upnext(path, level, root_level);
-	} else {
-		ret = tree_move_down(path, level);
-	}
-	if (ret >= 0) {
-		if (*level == 0)
-			btrfs_item_key_to_cpu(path->nodes[*level], key,
-					path->slots[*level]);
-		else
-			btrfs_node_key_to_cpu(path->nodes[*level], key,
-					path->slots[*level]);
-	}
-	return ret;
-}
-
-static int tree_compare_item(struct btrfs_path *left_path,
-			     struct btrfs_path *right_path,
-			     char *tmp_buf)
-{
-	int cmp;
-	int len1, len2;
-	unsigned long off1, off2;
-
-	len1 = btrfs_item_size_nr(left_path->nodes[0], left_path->slots[0]);
-	len2 = btrfs_item_size_nr(right_path->nodes[0], right_path->slots[0]);
-	if (len1 != len2)
-		return 1;
-
-	off1 = btrfs_item_ptr_offset(left_path->nodes[0], left_path->slots[0]);
-	off2 = btrfs_item_ptr_offset(right_path->nodes[0],
-				right_path->slots[0]);
-
-	read_extent_buffer(left_path->nodes[0], tmp_buf, off1, len1);
-
-	cmp = memcmp_extent_buffer(right_path->nodes[0], tmp_buf, off2, len1);
-	if (cmp)
-		return 1;
-	return 0;
-}
-
-#define ADVANCE 1
-#define ADVANCE_ONLY_NEXT -1
-
-/*
- * This function compares two trees and calls the provided callback for
- * every changed/new/deleted item it finds.
- * If shared tree blocks are encountered, whole subtrees are skipped, making
- * the compare pretty fast on snapshotted subvolumes.
- *
- * This currently works on commit roots only. As commit roots are read only,
- * we don't do any locking. The commit roots are protected with transactions.
- * Transactions are ended and rejoined when a commit is tried in between.
- *
- * This function checks for modifications done to the trees while comparing.
- * If it detects a change, it aborts immediately.
- */
-int btrfs_compare_trees(struct btrfs_root *left_root,
-			struct btrfs_root *right_root,
-			btrfs_changed_cb_t changed_cb, void *ctx)
-{
-	struct btrfs_fs_info *fs_info = left_root->fs_info;
-	int ret;
-	int cmp;
-	struct btrfs_path *left_path = NULL;
-	struct btrfs_path *right_path = NULL;
-	struct btrfs_key left_key;
-	struct btrfs_key right_key;
-	char *tmp_buf = NULL;
-	int left_root_level;
-	int right_root_level;
-	int left_level;
-	int right_level;
-	int left_end_reached;
-	int right_end_reached;
-	int advance_left;
-	int advance_right;
-	u64 left_blockptr;
-	u64 right_blockptr;
-	u64 left_gen;
-	u64 right_gen;
-
-	left_path = btrfs_alloc_path();
-	if (!left_path) {
-		ret = -ENOMEM;
-		goto out;
-	}
-	right_path = btrfs_alloc_path();
-	if (!right_path) {
-		ret = -ENOMEM;
-		goto out;
-	}
-
-	tmp_buf = kvmalloc(fs_info->nodesize, GFP_KERNEL);
-	if (!tmp_buf) {
-		ret = -ENOMEM;
-		goto out;
-	}
-
-	left_path->search_commit_root = 1;
-	left_path->skip_locking = 1;
-	right_path->search_commit_root = 1;
-	right_path->skip_locking = 1;
-
-	/*
-	 * Strategy: Go to the first items of both trees. Then do
-	 *
-	 * If both trees are at level 0
-	 *   Compare keys of current items
-	 *     If left < right treat left item as new, advance left tree
-	 *       and repeat
-	 *     If left > right treat right item as deleted, advance right tree
-	 *       and repeat
-	 *     If left == right do deep compare of items, treat as changed if
-	 *       needed, advance both trees and repeat
-	 * If both trees are at the same level but not at level 0
-	 *   Compare keys of current nodes/leafs
-	 *     If left < right advance left tree and repeat
-	 *     If left > right advance right tree and repeat
-	 *     If left == right compare blockptrs of the next nodes/leafs
-	 *       If they match advance both trees but stay at the same level
-	 *         and repeat
-	 *       If they don't match advance both trees while allowing to go
-	 *         deeper and repeat
-	 * If tree levels are different
-	 *   Advance the tree that needs it and repeat
-	 *
-	 * Advancing a tree means:
-	 *   If we are at level 0, try to go to the next slot. If that's not
-	 *   possible, go one level up and repeat. Stop when we found a level
-	 *   where we could go to the next slot. We may at this point be on a
-	 *   node or a leaf.
-	 *
-	 *   If we are not at level 0 and not on shared tree blocks, go one
-	 *   level deeper.
-	 *
-	 *   If we are not at level 0 and on shared tree blocks, go one slot to
-	 *   the right if possible or go up and right.
-	 */
-
-	down_read(&fs_info->commit_root_sem);
-	left_level = btrfs_header_level(left_root->commit_root);
-	left_root_level = left_level;
-	left_path->nodes[left_level] =
-			btrfs_clone_extent_buffer(left_root->commit_root);
-	if (!left_path->nodes[left_level]) {
-		up_read(&fs_info->commit_root_sem);
-		ret = -ENOMEM;
-		goto out;
-	}
-
-	right_level = btrfs_header_level(right_root->commit_root);
-	right_root_level = right_level;
-	right_path->nodes[right_level] =
-			btrfs_clone_extent_buffer(right_root->commit_root);
-	if (!right_path->nodes[right_level]) {
-		up_read(&fs_info->commit_root_sem);
-		ret = -ENOMEM;
-		goto out;
-	}
-	up_read(&fs_info->commit_root_sem);
-
-	if (left_level == 0)
-		btrfs_item_key_to_cpu(left_path->nodes[left_level],
-				&left_key, left_path->slots[left_level]);
-	else
-		btrfs_node_key_to_cpu(left_path->nodes[left_level],
-				&left_key, left_path->slots[left_level]);
-	if (right_level == 0)
-		btrfs_item_key_to_cpu(right_path->nodes[right_level],
-				&right_key, right_path->slots[right_level]);
-	else
-		btrfs_node_key_to_cpu(right_path->nodes[right_level],
-				&right_key, right_path->slots[right_level]);
-
-	left_end_reached = right_end_reached = 0;
-	advance_left = advance_right = 0;
-
-	while (1) {
-		if (advance_left && !left_end_reached) {
-			ret = tree_advance(left_path, &left_level,
-					left_root_level,
-					advance_left != ADVANCE_ONLY_NEXT,
-					&left_key);
-			if (ret == -1)
-				left_end_reached = ADVANCE;
-			else if (ret < 0)
-				goto out;
-			advance_left = 0;
-		}
-		if (advance_right && !right_end_reached) {
-			ret = tree_advance(right_path, &right_level,
-					right_root_level,
-					advance_right != ADVANCE_ONLY_NEXT,
-					&right_key);
-			if (ret == -1)
-				right_end_reached = ADVANCE;
-			else if (ret < 0)
-				goto out;
-			advance_right = 0;
-		}
-
-		if (left_end_reached && right_end_reached) {
-			ret = 0;
-			goto out;
-		} else if (left_end_reached) {
-			if (right_level == 0) {
-				ret = changed_cb(left_path, right_path,
-						&right_key,
-						BTRFS_COMPARE_TREE_DELETED,
-						ctx);
-				if (ret < 0)
-					goto out;
-			}
-			advance_right = ADVANCE;
-			continue;
-		} else if (right_end_reached) {
-			if (left_level == 0) {
-				ret = changed_cb(left_path, right_path,
-						&left_key,
-						BTRFS_COMPARE_TREE_NEW,
-						ctx);
-				if (ret < 0)
-					goto out;
-			}
-			advance_left = ADVANCE;
-			continue;
-		}
-
-		if (left_level == 0 && right_level == 0) {
-			cmp = btrfs_comp_cpu_keys(&left_key, &right_key);
-			if (cmp < 0) {
-				ret = changed_cb(left_path, right_path,
-						&left_key,
-						BTRFS_COMPARE_TREE_NEW,
-						ctx);
-				if (ret < 0)
-					goto out;
-				advance_left = ADVANCE;
-			} else if (cmp > 0) {
-				ret = changed_cb(left_path, right_path,
-						&right_key,
-						BTRFS_COMPARE_TREE_DELETED,
-						ctx);
-				if (ret < 0)
-					goto out;
-				advance_right = ADVANCE;
-			} else {
-				enum btrfs_compare_tree_result result;
-
-				WARN_ON(!extent_buffer_uptodate(left_path->nodes[0]));
-				ret = tree_compare_item(left_path, right_path,
-							tmp_buf);
-				if (ret)
-					result = BTRFS_COMPARE_TREE_CHANGED;
-				else
-					result = BTRFS_COMPARE_TREE_SAME;
-				ret = changed_cb(left_path, right_path,
-						 &left_key, result, ctx);
-				if (ret < 0)
-					goto out;
-				advance_left = ADVANCE;
-				advance_right = ADVANCE;
-			}
-		} else if (left_level == right_level) {
-			cmp = btrfs_comp_cpu_keys(&left_key, &right_key);
-			if (cmp < 0) {
-				advance_left = ADVANCE;
-			} else if (cmp > 0) {
-				advance_right = ADVANCE;
-			} else {
-				left_blockptr = btrfs_node_blockptr(
-						left_path->nodes[left_level],
-						left_path->slots[left_level]);
-				right_blockptr = btrfs_node_blockptr(
-						right_path->nodes[right_level],
-						right_path->slots[right_level]);
-				left_gen = btrfs_node_ptr_generation(
-						left_path->nodes[left_level],
-						left_path->slots[left_level]);
-				right_gen = btrfs_node_ptr_generation(
-						right_path->nodes[right_level],
-						right_path->slots[right_level]);
-				if (left_blockptr == right_blockptr &&
-				    left_gen == right_gen) {
-					/*
-					 * As we're on a shared block, don't
-					 * allow to go deeper.
-					 */
-					advance_left = ADVANCE_ONLY_NEXT;
-					advance_right = ADVANCE_ONLY_NEXT;
-				} else {
-					advance_left = ADVANCE;
-					advance_right = ADVANCE;
-				}
-			}
-		} else if (left_level < right_level) {
-			advance_right = ADVANCE;
-		} else {
-			advance_left = ADVANCE;
-		}
-	}
-
-out:
-	btrfs_free_path(left_path);
-	btrfs_free_path(right_path);
-	kvfree(tmp_buf);
-	return ret;
-}
-
 /*
  * this is similar to btrfs_next_leaf, but does not try to preserve
  * and fixup the path.  It looks for and returns the next key in the

commit 4b231ae47417d47a6bafab92b452ad629acdacb0
Author: David Sterba <dsterba@suse.com>
Date:   Wed Aug 21 19:16:27 2019 +0200

    btrfs: rename and export read_node_slot
    
    Preparatory work for code that will be moved out of ctree and uses this
    function.
    
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index a2f3cd7a619c..3b585f3e4d11 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1792,8 +1792,8 @@ static void root_sub_used(struct btrfs_root *root, u32 size)
 /* given a node and slot number, this reads the blocks it points to.  The
  * extent buffer is returned with a reference taken (but unlocked).
  */
-static noinline struct extent_buffer *read_node_slot(
-				struct extent_buffer *parent, int slot)
+struct extent_buffer *btrfs_read_node_slot(struct extent_buffer *parent,
+					   int slot)
 {
 	int level = btrfs_header_level(parent);
 	struct extent_buffer *eb;
@@ -1862,7 +1862,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 			return 0;
 
 		/* promote the child to a root */
-		child = read_node_slot(mid, 0);
+		child = btrfs_read_node_slot(mid, 0);
 		if (IS_ERR(child)) {
 			ret = PTR_ERR(child);
 			btrfs_handle_fs_error(fs_info, ret, NULL);
@@ -1902,7 +1902,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 	    BTRFS_NODEPTRS_PER_BLOCK(fs_info) / 4)
 		return 0;
 
-	left = read_node_slot(parent, pslot - 1);
+	left = btrfs_read_node_slot(parent, pslot - 1);
 	if (IS_ERR(left))
 		left = NULL;
 
@@ -1917,7 +1917,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		}
 	}
 
-	right = read_node_slot(parent, pslot + 1);
+	right = btrfs_read_node_slot(parent, pslot + 1);
 	if (IS_ERR(right))
 		right = NULL;
 
@@ -2077,7 +2077,7 @@ static noinline int push_nodes_for_insert(struct btrfs_trans_handle *trans,
 	if (!parent)
 		return 1;
 
-	left = read_node_slot(parent, pslot - 1);
+	left = btrfs_read_node_slot(parent, pslot - 1);
 	if (IS_ERR(left))
 		left = NULL;
 
@@ -2129,7 +2129,7 @@ static noinline int push_nodes_for_insert(struct btrfs_trans_handle *trans,
 		btrfs_tree_unlock(left);
 		free_extent_buffer(left);
 	}
-	right = read_node_slot(parent, pslot + 1);
+	right = btrfs_read_node_slot(parent, pslot + 1);
 	if (IS_ERR(right))
 		right = NULL;
 
@@ -3783,7 +3783,7 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 
 	btrfs_assert_tree_locked(path->nodes[1]);
 
-	right = read_node_slot(upper, slot + 1);
+	right = btrfs_read_node_slot(upper, slot + 1);
 	/*
 	 * slot + 1 is not valid or we fail to read the right node,
 	 * no big deal, just return.
@@ -4017,7 +4017,7 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 
 	btrfs_assert_tree_locked(path->nodes[1]);
 
-	left = read_node_slot(path->nodes[1], slot - 1);
+	left = btrfs_read_node_slot(path->nodes[1], slot - 1);
 	/*
 	 * slot - 1 is not valid or we fail to read the left node,
 	 * no big deal, just return.
@@ -5224,7 +5224,7 @@ int btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,
 			goto out;
 		}
 		btrfs_set_path_blocking(path);
-		cur = read_node_slot(cur, slot);
+		cur = btrfs_read_node_slot(cur, slot);
 		if (IS_ERR(cur)) {
 			ret = PTR_ERR(cur);
 			goto out;
@@ -5251,7 +5251,7 @@ static int tree_move_down(struct btrfs_path *path, int *level)
 	struct extent_buffer *eb;
 
 	BUG_ON(*level == 0);
-	eb = read_node_slot(path->nodes[*level], path->slots[*level]);
+	eb = btrfs_read_node_slot(path->nodes[*level], path->slots[*level]);
 	if (IS_ERR(eb))
 		return PTR_ERR(eb);
 

commit efad8a853ad2057f96664328a0d327a05ce39c76
Author: Filipe Manana <fdmanana@suse.com>
Date:   Mon Aug 12 19:14:29 2019 +0100

    Btrfs: fix use-after-free when using the tree modification log
    
    At ctree.c:get_old_root(), we are accessing a root's header owner field
    after we have freed the respective extent buffer. This results in an
    use-after-free that can lead to crashes, and when CONFIG_DEBUG_PAGEALLOC
    is set, results in a stack trace like the following:
    
      [ 3876.799331] stack segment: 0000 [#1] SMP DEBUG_PAGEALLOC PTI
      [ 3876.799363] CPU: 0 PID: 15436 Comm: pool Not tainted 5.3.0-rc3-btrfs-next-54 #1
      [ 3876.799385] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS rel-1.12.0-0-ga698c8995f-prebuilt.qemu.org 04/01/2014
      [ 3876.799433] RIP: 0010:btrfs_search_old_slot+0x652/0xd80 [btrfs]
      (...)
      [ 3876.799502] RSP: 0018:ffff9f08c1a2f9f0 EFLAGS: 00010286
      [ 3876.799518] RAX: ffff8dd300000000 RBX: ffff8dd85a7a9348 RCX: 000000038da26000
      [ 3876.799538] RDX: 0000000000000000 RSI: ffffe522ce368980 RDI: 0000000000000246
      [ 3876.799559] RBP: dae1922adadad000 R08: 0000000008020000 R09: ffffe522c0000000
      [ 3876.799579] R10: ffff8dd57fd788c8 R11: 000000007511b030 R12: ffff8dd781ddc000
      [ 3876.799599] R13: ffff8dd9e6240578 R14: ffff8dd6896f7a88 R15: ffff8dd688cf90b8
      [ 3876.799620] FS:  00007f23ddd97700(0000) GS:ffff8dda20200000(0000) knlGS:0000000000000000
      [ 3876.799643] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
      [ 3876.799660] CR2: 00007f23d4024000 CR3: 0000000710bb0005 CR4: 00000000003606f0
      [ 3876.799682] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
      [ 3876.799703] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
      [ 3876.799723] Call Trace:
      [ 3876.799735]  ? do_raw_spin_unlock+0x49/0xc0
      [ 3876.799749]  ? _raw_spin_unlock+0x24/0x30
      [ 3876.799779]  resolve_indirect_refs+0x1eb/0xc80 [btrfs]
      [ 3876.799810]  find_parent_nodes+0x38d/0x1180 [btrfs]
      [ 3876.799841]  btrfs_check_shared+0x11a/0x1d0 [btrfs]
      [ 3876.799870]  ? extent_fiemap+0x598/0x6e0 [btrfs]
      [ 3876.799895]  extent_fiemap+0x598/0x6e0 [btrfs]
      [ 3876.799913]  do_vfs_ioctl+0x45a/0x700
      [ 3876.799926]  ksys_ioctl+0x70/0x80
      [ 3876.799938]  ? trace_hardirqs_off_thunk+0x1a/0x20
      [ 3876.799953]  __x64_sys_ioctl+0x16/0x20
      [ 3876.799965]  do_syscall_64+0x62/0x220
      [ 3876.799977]  entry_SYSCALL_64_after_hwframe+0x49/0xbe
      [ 3876.799993] RIP: 0033:0x7f23e0013dd7
      (...)
      [ 3876.800056] RSP: 002b:00007f23ddd96ca8 EFLAGS: 00000246 ORIG_RAX: 0000000000000010
      [ 3876.800078] RAX: ffffffffffffffda RBX: 00007f23d80210f8 RCX: 00007f23e0013dd7
      [ 3876.800099] RDX: 00007f23d80210f8 RSI: 00000000c020660b RDI: 0000000000000003
      [ 3876.800626] RBP: 000055fa2a2a2440 R08: 0000000000000000 R09: 00007f23ddd96d7c
      [ 3876.801143] R10: 00007f23d8022000 R11: 0000000000000246 R12: 00007f23ddd96d80
      [ 3876.801662] R13: 00007f23ddd96d78 R14: 00007f23d80210f0 R15: 00007f23ddd96d80
      (...)
      [ 3876.805107] ---[ end trace e53161e179ef04f9 ]---
    
    Fix that by saving the root's header owner field into a local variable
    before freeing the root's extent buffer, and then use that local variable
    when needed.
    
    Fixes: 30b0463a9394d9 ("Btrfs: fix accessing the root pointer in tree mod log functions")
    CC: stable@vger.kernel.org # 3.10+
    Reviewed-by: Nikolay Borisov <nborisov@suse.com>
    Reviewed-by: Anand Jain <anand.jain@oracle.com>
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index f8dc582db690..a2f3cd7a619c 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1343,6 +1343,7 @@ get_old_root(struct btrfs_root *root, u64 time_seq)
 	struct tree_mod_elem *tm;
 	struct extent_buffer *eb = NULL;
 	struct extent_buffer *eb_root;
+	u64 eb_root_owner = 0;
 	struct extent_buffer *old;
 	struct tree_mod_root *old_root = NULL;
 	u64 old_generation = 0;
@@ -1380,6 +1381,7 @@ get_old_root(struct btrfs_root *root, u64 time_seq)
 			free_extent_buffer(old);
 		}
 	} else if (old_root) {
+		eb_root_owner = btrfs_header_owner(eb_root);
 		btrfs_tree_read_unlock(eb_root);
 		free_extent_buffer(eb_root);
 		eb = alloc_dummy_extent_buffer(fs_info, logical);
@@ -1396,7 +1398,7 @@ get_old_root(struct btrfs_root *root, u64 time_seq)
 	if (old_root) {
 		btrfs_set_header_bytenr(eb, eb->start);
 		btrfs_set_header_backref_rev(eb, BTRFS_MIXED_BACKREF_REV);
-		btrfs_set_header_owner(eb, btrfs_header_owner(eb_root));
+		btrfs_set_header_owner(eb, eb_root_owner);
 		btrfs_set_header_level(eb, old_root->level);
 		btrfs_set_header_generation(eb, old_generation);
 	}

commit 6a9fb468f1152d6254f49fee6ac28c3cfa3367e5
Author: Josef Bacik <josef@toxicpanda.com>
Date:   Thu Jun 20 15:37:52 2019 -0400

    btrfs: make caching_thread use btrfs_find_next_key
    
    extent-tree.c has a find_next_key that just walks up the path to find
    the next key, but it is used for both the caching stuff and the snapshot
    delete stuff.  The snapshot deletion stuff is special so it can't really
    use btrfs_find_next_key, but the caching thread stuff can.  We just need
    to fix btrfs_find_next_key to deal with ->skip_locking and then it works
    exactly the same as the private find_next_key helper.
    
    Signed-off-by: Josef Bacik <josef@toxicpanda.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 9d1d0a926cb0..f8dc582db690 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -5623,7 +5623,7 @@ int btrfs_find_next_key(struct btrfs_root *root, struct btrfs_path *path,
 	int slot;
 	struct extent_buffer *c;
 
-	WARN_ON(!path->keep_locks);
+	WARN_ON(!path->keep_locks && !path->skip_locking);
 	while (level < BTRFS_MAX_LEVEL) {
 		if (!path->nodes[level])
 			return 1;
@@ -5639,7 +5639,7 @@ int btrfs_find_next_key(struct btrfs_root *root, struct btrfs_path *path,
 			    !path->nodes[level + 1])
 				return 1;
 
-			if (path->locks[level + 1]) {
+			if (path->locks[level + 1] || path->skip_locking) {
 				level++;
 				continue;
 			}

commit 73e82fe4099bbf3e06a351430bd4ed9703212dda
Author: David Sterba <dsterba@suse.com>
Date:   Wed Mar 27 16:19:55 2019 +0100

    btrfs: assert tree mod log lock in __tree_mod_log_insert
    
    The tree is going to be modified so it must be the exclusive lock.
    
    Reviewed-by: Nikolay Borisov <nborisov@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 5df76c17775a..9d1d0a926cb0 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -376,8 +376,6 @@ void btrfs_put_tree_mod_seq(struct btrfs_fs_info *fs_info,
  * The 'start address' is the logical address of the *new* root node
  * for root replace operations, or the logical address of the affected
  * block for all other operations.
- *
- * Note: must be called with write lock for fs_info::tree_mod_log_lock.
  */
 static noinline int
 __tree_mod_log_insert(struct btrfs_fs_info *fs_info, struct tree_mod_elem *tm)
@@ -387,6 +385,8 @@ __tree_mod_log_insert(struct btrfs_fs_info *fs_info, struct tree_mod_elem *tm)
 	struct rb_node *parent = NULL;
 	struct tree_mod_elem *cur;
 
+	lockdep_assert_held_write(&fs_info->tree_mod_log_lock);
+
 	tm->seq = btrfs_inc_tree_mod_seq(fs_info);
 
 	tm_root = &fs_info->tree_mod_log;

commit 7c15d41016dc886cc011e3854d855e219759ae68
Author: Qu Wenruo <wqu@suse.com>
Date:   Thu Apr 25 08:55:53 2019 +0800

    btrfs: ctree: Dump the leaf before BUG_ON in btrfs_set_item_key_safe
    
    We have a long standing problem with reversed keys that's detected by
    btrfs_set_item_key_safe. This is hard to reproduce so we'd like to
    capture more information for later analysis.
    
    Let's dump the leaf content before triggering BUG_ON() so that we can
    have some clue on what's going wrong.  The output of tree locks should
    help us to debug such problem.
    
    Sample stacktrace:
    
     generic/522             [00:07:05]
     [26946.113381] run fstests generic/522 at 2019-04-16 00:07:05
     [27161.474720] kernel BUG at fs/btrfs/ctree.c:3192!
     [27161.475923] invalid opcode: 0000 [#1] PREEMPT SMP
     [27161.477167] CPU: 0 PID: 15676 Comm: fsx Tainted: G        W         5.1.0-rc5-default+ #562
     [27161.478932] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS rel-1.12.0-0-ga698c89-prebuilt.qemu.org 04/01/2014
     [27161.481099] RIP: 0010:btrfs_set_item_key_safe+0x146/0x1c0 [btrfs]
     [27161.485369] RSP: 0018:ffffb087499e39b0 EFLAGS: 00010286
     [27161.486464] RAX: 00000000ffffffff RBX: ffff941534d80e70 RCX: 0000000000024000
     [27161.487929] RDX: 0000000000013039 RSI: ffffb087499e3aa5 RDI: ffffb087499e39c7
     [27161.489289] RBP: 000000000000000e R08: ffff9414e0f49008 R09: 0000000000001000
     [27161.490807] R10: 0000000000000000 R11: 0000000000000003 R12: ffff9414e0f48e70
     [27161.492305] R13: ffffb087499e3aa5 R14: 0000000000000000 R15: 0000000000071000
     [27161.493845] FS:  00007f8ea58d0b80(0000) GS:ffff94153d400000(0000) knlGS:0000000000000000
     [27161.495608] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
     [27161.496717] CR2: 00007f8ea57a9000 CR3: 0000000016a33000 CR4: 00000000000006f0
     [27161.498100] Call Trace:
     [27161.498771]  __btrfs_drop_extents+0x6ec/0xdf0 [btrfs]
     [27161.499872]  btrfs_log_changed_extents.isra.26+0x3a2/0x9e0 [btrfs]
     [27161.501114]  btrfs_log_inode+0x7ff/0xdc0 [btrfs]
     [27161.502114]  ? __mutex_unlock_slowpath+0x4b/0x2b0
     [27161.503172]  btrfs_log_inode_parent+0x237/0x9c0 [btrfs]
     [27161.504348]  btrfs_log_dentry_safe+0x4a/0x70 [btrfs]
     [27161.505374]  btrfs_sync_file+0x1b7/0x480 [btrfs]
     [27161.506371]  __x64_sys_msync+0x180/0x210
     [27161.507208]  do_syscall_64+0x54/0x180
     [27161.507932]  entry_SYSCALL_64_after_hwframe+0x49/0xbe
     [27161.508839] RIP: 0033:0x7f8ea5aa9c61
     [27161.512616] RSP: 002b:00007ffea2a06498 EFLAGS: 00000246 ORIG_RAX: 000000000000001a
     [27161.514161] RAX: ffffffffffffffda RBX: 000000000002a938 RCX: 00007f8ea5aa9c61
     [27161.515376] RDX: 0000000000000004 RSI: 000000000001c9b2 RDI: 00007f8ea578d000
     [27161.516572] RBP: 000000000001c07a R08: fffffffffffffff8 R09: 000000000002a000
     [27161.517883] R10: 00007f8ea57a99b2 R11: 0000000000000246 R12: 0000000000000938
     [27161.519080] R13: 00007f8ea578d000 R14: 000000000001c9b2 R15: 0000000000000000
     [27161.520281] Modules linked in: btrfs libcrc32c xor zstd_decompress zstd_compress xxhash raid6_pq loop [last unloaded: scsi_debug]
     [27161.522272] ---[ end trace d5afec7ccac6a252 ]---
     [27161.523111] RIP: 0010:btrfs_set_item_key_safe+0x146/0x1c0 [btrfs]
     [27161.527253] RSP: 0018:ffffb087499e39b0 EFLAGS: 00010286
     [27161.528192] RAX: 00000000ffffffff RBX: ffff941534d80e70 RCX: 0000000000024000
     [27161.529392] RDX: 0000000000013039 RSI: ffffb087499e3aa5 RDI: ffffb087499e39c7
     [27161.530607] RBP: 000000000000000e R08: ffff9414e0f49008 R09: 0000000000001000
     [27161.531802] R10: 0000000000000000 R11: 0000000000000003 R12: ffff9414e0f48e70
     [27161.533018] R13: ffffb087499e3aa5 R14: 0000000000000000 R15: 0000000000071000
     [27161.534405] FS:  00007f8ea58d0b80(0000) GS:ffff94153d400000(0000) knlGS:0000000000000000
     [27161.536048] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
     [27161.537210] CR2: 00007f8ea57a9000 CR3: 0000000016a33000 CR4: 00000000000006f0
    
    Reviewed-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: Qu Wenruo <wqu@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 5116c2a1f0f9..5df76c17775a 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -3185,11 +3185,31 @@ void btrfs_set_item_key_safe(struct btrfs_fs_info *fs_info,
 	slot = path->slots[0];
 	if (slot > 0) {
 		btrfs_item_key(eb, &disk_key, slot - 1);
-		BUG_ON(comp_keys(&disk_key, new_key) >= 0);
+		if (unlikely(comp_keys(&disk_key, new_key) >= 0)) {
+			btrfs_crit(fs_info,
+		"slot %u key (%llu %u %llu) new key (%llu %u %llu)",
+				   slot, btrfs_disk_key_objectid(&disk_key),
+				   btrfs_disk_key_type(&disk_key),
+				   btrfs_disk_key_offset(&disk_key),
+				   new_key->objectid, new_key->type,
+				   new_key->offset);
+			btrfs_print_leaf(eb);
+			BUG();
+		}
 	}
 	if (slot < btrfs_header_nritems(eb) - 1) {
 		btrfs_item_key(eb, &disk_key, slot + 1);
-		BUG_ON(comp_keys(&disk_key, new_key) <= 0);
+		if (unlikely(comp_keys(&disk_key, new_key) <= 0)) {
+			btrfs_crit(fs_info,
+		"slot %u key (%llu %u %llu) new key (%llu %u %llu)",
+				   slot, btrfs_disk_key_objectid(&disk_key),
+				   btrfs_disk_key_type(&disk_key),
+				   btrfs_disk_key_offset(&disk_key),
+				   new_key->objectid, new_key->type,
+				   new_key->offset);
+			btrfs_print_leaf(eb);
+			BUG();
+		}
 	}
 
 	btrfs_cpu_key_to_disk(&disk_key, new_key);

commit f5c8daa5b2ae6de4baa18a95002271cd7f90be90
Author: David Sterba <dsterba@suse.com>
Date:   Wed Mar 20 11:43:36 2019 +0100

    btrfs: remove unused parameter fs_info from btrfs_set_disk_extent_flags
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index c4ae9cfea709..5116c2a1f0f9 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -948,7 +948,7 @@ static noinline int update_ref_for_cow(struct btrfs_trans_handle *trans,
 		if (new_flags != 0) {
 			int level = btrfs_header_level(buf);
 
-			ret = btrfs_set_disk_extent_flags(trans, fs_info,
+			ret = btrfs_set_disk_extent_flags(trans,
 							  buf->start,
 							  buf->len,
 							  new_flags, level, 0);

commit 179d1e6a3b6a0409e5d411d485dd4623632c42d8
Author: David Sterba <dsterba@suse.com>
Date:   Wed Mar 20 15:03:48 2019 +0100

    btrfs: remove unused parameter fs_info from from tree_advance
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index e8a24fcb9182..c4ae9cfea709 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -5269,8 +5269,7 @@ static int tree_move_next_or_upnext(struct btrfs_path *path,
  * Returns 1 if it had to move up and next. 0 is returned if it moved only next
  * or down.
  */
-static int tree_advance(struct btrfs_fs_info *fs_info,
-			struct btrfs_path *path,
+static int tree_advance(struct btrfs_path *path,
 			int *level, int root_level,
 			int allow_down,
 			struct btrfs_key *key)
@@ -5457,7 +5456,7 @@ int btrfs_compare_trees(struct btrfs_root *left_root,
 
 	while (1) {
 		if (advance_left && !left_end_reached) {
-			ret = tree_advance(fs_info, left_path, &left_level,
+			ret = tree_advance(left_path, &left_level,
 					left_root_level,
 					advance_left != ADVANCE_ONLY_NEXT,
 					&left_key);
@@ -5468,7 +5467,7 @@ int btrfs_compare_trees(struct btrfs_root *left_root,
 			advance_left = 0;
 		}
 		if (advance_right && !right_end_reached) {
-			ret = tree_advance(fs_info, right_path, &right_level,
+			ret = tree_advance(right_path, &right_level,
 					right_root_level,
 					advance_right != ADVANCE_ONLY_NEXT,
 					&right_key);

commit c7da9597fe8cadc846fa72f4ddf478bb435a913f
Author: David Sterba <dsterba@suse.com>
Date:   Wed Mar 20 15:02:46 2019 +0100

    btrfs: remove unused parameter fs_info from tree_move_down
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index eacd0b80e272..e8a24fcb9182 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -5224,9 +5224,7 @@ int btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,
 	return ret;
 }
 
-static int tree_move_down(struct btrfs_fs_info *fs_info,
-			   struct btrfs_path *path,
-			   int *level)
+static int tree_move_down(struct btrfs_path *path, int *level)
 {
 	struct extent_buffer *eb;
 
@@ -5282,7 +5280,7 @@ static int tree_advance(struct btrfs_fs_info *fs_info,
 	if (*level == 0 || !allow_down) {
 		ret = tree_move_next_or_upnext(path, level, root_level);
 	} else {
-		ret = tree_move_down(fs_info, path, level);
+		ret = tree_move_down(path, level);
 	}
 	if (ret >= 0) {
 		if (*level == 0)

commit c71dd88007bdc8ba62e99439d93050b0778f101a
Author: David Sterba <dsterba@suse.com>
Date:   Wed Mar 20 14:51:10 2019 +0100

    btrfs: remove unused parameter fs_info from btrfs_extend_item
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index d8252bae0537..eacd0b80e272 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -4639,8 +4639,7 @@ void btrfs_truncate_item(struct btrfs_path *path, u32 new_size, int from_end)
 /*
  * make the item pointed to by the path bigger, data_size is the added size.
  */
-void btrfs_extend_item(struct btrfs_fs_info *fs_info, struct btrfs_path *path,
-		       u32 data_size)
+void btrfs_extend_item(struct btrfs_path *path, u32 data_size)
 {
 	int slot;
 	struct extent_buffer *leaf;
@@ -4669,7 +4668,7 @@ void btrfs_extend_item(struct btrfs_fs_info *fs_info, struct btrfs_path *path,
 	BUG_ON(slot < 0);
 	if (slot >= nritems) {
 		btrfs_print_leaf(leaf);
-		btrfs_crit(fs_info, "slot %d too large, nritems %d",
+		btrfs_crit(leaf->fs_info, "slot %d too large, nritems %d",
 			   slot, nritems);
 		BUG();
 	}

commit 78ac4f9e5ae022bd183ca21da7b373d300b7be17
Author: David Sterba <dsterba@suse.com>
Date:   Wed Mar 20 14:49:12 2019 +0100

    btrfs: remove unused parameter fs_info from btrfs_truncate_item
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 928d4d8c542b..d8252bae0537 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -4541,8 +4541,7 @@ int btrfs_duplicate_item(struct btrfs_trans_handle *trans,
  * off the end of the item or if we shift the item to chop bytes off
  * the front.
  */
-void btrfs_truncate_item(struct btrfs_fs_info *fs_info,
-			 struct btrfs_path *path, u32 new_size, int from_end)
+void btrfs_truncate_item(struct btrfs_path *path, u32 new_size, int from_end)
 {
 	int slot;
 	struct extent_buffer *leaf;

commit 25263cd7cec1b41b55bf7991d09a019ca1ff1359
Author: David Sterba <dsterba@suse.com>
Date:   Wed Mar 20 14:44:57 2019 +0100

    btrfs: remove unused parameter fs_info from split_item
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index e26131d14cc4..928d4d8c542b 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -4399,8 +4399,7 @@ static noinline int setup_leaf_for_split(struct btrfs_trans_handle *trans,
 	return ret;
 }
 
-static noinline int split_item(struct btrfs_fs_info *fs_info,
-			       struct btrfs_path *path,
+static noinline int split_item(struct btrfs_path *path,
 			       const struct btrfs_key *new_key,
 			       unsigned long split_offset)
 {
@@ -4496,7 +4495,7 @@ int btrfs_split_item(struct btrfs_trans_handle *trans,
 	if (ret)
 		return ret;
 
-	ret = split_item(root->fs_info, path, new_key, split_offset);
+	ret = split_item(path, new_key, split_offset);
 	return ret;
 }
 

commit 8087c193450b29c80c8e8b62050de9d0999e20cf
Author: David Sterba <dsterba@suse.com>
Date:   Wed Mar 20 14:40:41 2019 +0100

    btrfs: get fs_info from eb in __push_leaf_left
    
    We can read fs_info from extent buffer and can drop it from the
    parameters.
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 727a7b656b44..e26131d14cc4 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -3819,12 +3819,12 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
  * item at 'max_slot' won't be touched.  Use (u32)-1 to make us do all the
  * items
  */
-static noinline int __push_leaf_left(struct btrfs_fs_info *fs_info,
-				     struct btrfs_path *path, int data_size,
+static noinline int __push_leaf_left(struct btrfs_path *path, int data_size,
 				     int empty, struct extent_buffer *left,
 				     int free_space, u32 right_nritems,
 				     u32 max_slot)
 {
+	struct btrfs_fs_info *fs_info = left->fs_info;
 	struct btrfs_disk_key disk_key;
 	struct extent_buffer *right = path->nodes[0];
 	int i;
@@ -3976,7 +3976,6 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 			  *root, struct btrfs_path *path, int min_data_size,
 			  int data_size, int empty, u32 max_slot)
 {
-	struct btrfs_fs_info *fs_info = root->fs_info;
 	struct extent_buffer *right = path->nodes[0];
 	struct extent_buffer *left;
 	int slot;
@@ -4029,7 +4028,7 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 		goto out;
 	}
 
-	return __push_leaf_left(fs_info, path, min_data_size,
+	return __push_leaf_left(path, min_data_size,
 			       empty, left, free_space, right_nritems,
 			       max_slot);
 out:

commit f72f0010b202f8ccf8fa671f9ef61e4c980fba3c
Author: David Sterba <dsterba@suse.com>
Date:   Wed Mar 20 14:39:45 2019 +0100

    btrfs: get fs_info from eb in __push_leaf_right
    
    We can read fs_info from extent buffer and can drop it from the
    parameters.
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index f52eb952597b..727a7b656b44 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -3589,13 +3589,13 @@ noinline int btrfs_leaf_free_space(struct extent_buffer *leaf)
  * min slot controls the lowest index we're willing to push to the
  * right.  We'll push up to and including min_slot, but no lower
  */
-static noinline int __push_leaf_right(struct btrfs_fs_info *fs_info,
-				      struct btrfs_path *path,
+static noinline int __push_leaf_right(struct btrfs_path *path,
 				      int data_size, int empty,
 				      struct extent_buffer *right,
 				      int free_space, u32 left_nritems,
 				      u32 min_slot)
 {
+	struct btrfs_fs_info *fs_info = right->fs_info;
 	struct extent_buffer *left = path->nodes[0];
 	struct extent_buffer *upper = path->nodes[1];
 	struct btrfs_map_token token;
@@ -3743,7 +3743,6 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 			   int min_data_size, int data_size,
 			   int empty, u32 min_slot)
 {
-	struct btrfs_fs_info *fs_info = root->fs_info;
 	struct extent_buffer *left = path->nodes[0];
 	struct extent_buffer *right;
 	struct extent_buffer *upper;
@@ -3804,7 +3803,7 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 		return 0;
 	}
 
-	return __push_leaf_right(fs_info, path, min_data_size, empty,
+	return __push_leaf_right(path, min_data_size, empty,
 				right, free_space, left_nritems, min_slot);
 out_unlock:
 	btrfs_tree_unlock(right);

commit 94f94ad9725550bc33e5561b55fc4506228b1d7b
Author: David Sterba <dsterba@suse.com>
Date:   Wed Mar 20 14:42:33 2019 +0100

    btrfs: get fs_info from trans in copy_for_split
    
    We can read fs_info from the transaction and can drop it from the
    parameters.
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index f779104ab199..f52eb952597b 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -4044,12 +4044,12 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
  * available for the resulting leaf level of the path.
  */
 static noinline void copy_for_split(struct btrfs_trans_handle *trans,
-				    struct btrfs_fs_info *fs_info,
 				    struct btrfs_path *path,
 				    struct extent_buffer *l,
 				    struct extent_buffer *right,
 				    int slot, int mid, int nritems)
 {
+	struct btrfs_fs_info *fs_info = trans->fs_info;
 	int data_copy_size;
 	int rt_data_off;
 	int i;
@@ -4316,7 +4316,7 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 		return ret;
 	}
 
-	copy_for_split(trans, fs_info, path, l, right, slot, mid, nritems);
+	copy_for_split(trans, path, l, right, slot, mid, nritems);
 
 	if (split == 2) {
 		BUG_ON(num_doubles != 0);

commit 6ad3cf6df0963845a681d39792822255598d68e2
Author: David Sterba <dsterba@suse.com>
Date:   Wed Mar 20 14:32:45 2019 +0100

    btrfs: get fs_info from trans in insert_ptr
    
    We can read fs_info from the transaction and can drop it from the
    parameters.
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index e76e69fdfa9e..f779104ab199 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -3407,7 +3407,7 @@ static noinline int insert_new_root(struct btrfs_trans_handle *trans,
  * blocknr is the block the key points to.
  */
 static void insert_ptr(struct btrfs_trans_handle *trans,
-		       struct btrfs_fs_info *fs_info, struct btrfs_path *path,
+		       struct btrfs_path *path,
 		       struct btrfs_disk_key *key, u64 bytenr,
 		       int slot, int level)
 {
@@ -3420,7 +3420,7 @@ static void insert_ptr(struct btrfs_trans_handle *trans,
 	lower = path->nodes[level];
 	nritems = btrfs_header_nritems(lower);
 	BUG_ON(slot > nritems);
-	BUG_ON(nritems == BTRFS_NODEPTRS_PER_BLOCK(fs_info));
+	BUG_ON(nritems == BTRFS_NODEPTRS_PER_BLOCK(trans->fs_info));
 	if (slot != nritems) {
 		if (level) {
 			ret = tree_mod_log_insert_move(lower, slot + 1, slot,
@@ -3520,7 +3520,7 @@ static noinline int split_node(struct btrfs_trans_handle *trans,
 	btrfs_mark_buffer_dirty(c);
 	btrfs_mark_buffer_dirty(split);
 
-	insert_ptr(trans, fs_info, path, &disk_key, split->start,
+	insert_ptr(trans, path, &disk_key, split->start,
 		   path->slots[level + 1] + 1, level + 1);
 
 	if (path->slots[level] >= mid) {
@@ -4084,8 +4084,7 @@ static noinline void copy_for_split(struct btrfs_trans_handle *trans,
 
 	btrfs_set_header_nritems(l, mid);
 	btrfs_item_key(right, &disk_key, 0);
-	insert_ptr(trans, fs_info, path, &disk_key, right->start,
-		   path->slots[1] + 1, 1);
+	insert_ptr(trans, path, &disk_key, right->start, path->slots[1] + 1, 1);
 
 	btrfs_mark_buffer_dirty(right);
 	btrfs_mark_buffer_dirty(l);
@@ -4291,7 +4290,7 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 	if (split == 0) {
 		if (mid <= slot) {
 			btrfs_set_header_nritems(right, 0);
-			insert_ptr(trans, fs_info, path, &disk_key,
+			insert_ptr(trans, path, &disk_key,
 				   right->start, path->slots[1] + 1, 1);
 			btrfs_tree_unlock(path->nodes[0]);
 			free_extent_buffer(path->nodes[0]);
@@ -4300,7 +4299,7 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 			path->slots[1] += 1;
 		} else {
 			btrfs_set_header_nritems(right, 0);
-			insert_ptr(trans, fs_info, path, &disk_key,
+			insert_ptr(trans, path, &disk_key,
 				   right->start, path->slots[1], 1);
 			btrfs_tree_unlock(path->nodes[0]);
 			free_extent_buffer(path->nodes[0]);

commit 55d32ed8d3e35e6d79d85aa6cbcb9e8dfb021c1a
Author: David Sterba <dsterba@suse.com>
Date:   Wed Mar 20 14:18:06 2019 +0100

    btrfs: get fs_info from trans in balance_node_right
    
    We can read fs_info from the transaction and can drop it from the
    parameters.
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 7d2223c56262..e76e69fdfa9e 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -24,7 +24,6 @@ static int push_node_left(struct btrfs_trans_handle *trans,
 			  struct extent_buffer *dst,
 			  struct extent_buffer *src, int empty);
 static int balance_node_right(struct btrfs_trans_handle *trans,
-			      struct btrfs_fs_info *fs_info,
 			      struct extent_buffer *dst_buf,
 			      struct extent_buffer *src_buf);
 static void del_ptr(struct btrfs_root *root, struct btrfs_path *path,
@@ -1979,7 +1978,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 			btrfs_handle_fs_error(fs_info, ret, NULL);
 			goto enospc;
 		}
-		wret = balance_node_right(trans, fs_info, mid, left);
+		wret = balance_node_right(trans, mid, left);
 		if (wret < 0) {
 			ret = wret;
 			goto enospc;
@@ -2151,8 +2150,7 @@ static noinline int push_nodes_for_insert(struct btrfs_trans_handle *trans,
 			if (ret)
 				wret = 1;
 			else {
-				wret = balance_node_right(trans, fs_info,
-							  right, mid);
+				wret = balance_node_right(trans, right, mid);
 			}
 		}
 		if (wret < 0)
@@ -3283,10 +3281,10 @@ static int push_node_left(struct btrfs_trans_handle *trans,
  * this will  only push up to 1/2 the contents of the left node over
  */
 static int balance_node_right(struct btrfs_trans_handle *trans,
-			      struct btrfs_fs_info *fs_info,
 			      struct extent_buffer *dst,
 			      struct extent_buffer *src)
 {
+	struct btrfs_fs_info *fs_info = trans->fs_info;
 	int push_items = 0;
 	int max_push;
 	int src_nritems;

commit d30a668f1bf13893cca35bf64ae85ba8b1bc69ec
Author: David Sterba <dsterba@suse.com>
Date:   Wed Mar 20 14:16:45 2019 +0100

    btrfs: get fs_info from trans in push_node_left
    
    We can read fs_info from the transaction and can drop it from the
    parameters.
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 0998e7fba98b..7d2223c56262 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -21,7 +21,6 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 		      const struct btrfs_key *ins_key, struct btrfs_path *path,
 		      int data_size, int extend);
 static int push_node_left(struct btrfs_trans_handle *trans,
-			  struct btrfs_fs_info *fs_info,
 			  struct extent_buffer *dst,
 			  struct extent_buffer *src, int empty);
 static int balance_node_right(struct btrfs_trans_handle *trans,
@@ -1935,7 +1934,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 	/* first, try to make some room in the middle buffer */
 	if (left) {
 		orig_slot += btrfs_header_nritems(left);
-		wret = push_node_left(trans, fs_info, left, mid, 1);
+		wret = push_node_left(trans, left, mid, 1);
 		if (wret < 0)
 			ret = wret;
 	}
@@ -1944,7 +1943,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 	 * then try to empty the right most buffer into the middle
 	 */
 	if (right) {
-		wret = push_node_left(trans, fs_info, mid, right, 1);
+		wret = push_node_left(trans, mid, right, 1);
 		if (wret < 0 && wret != -ENOSPC)
 			ret = wret;
 		if (btrfs_header_nritems(right) == 0) {
@@ -1986,7 +1985,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 			goto enospc;
 		}
 		if (wret == 1) {
-			wret = push_node_left(trans, fs_info, left, mid, 1);
+			wret = push_node_left(trans, left, mid, 1);
 			if (wret < 0)
 				ret = wret;
 		}
@@ -2097,8 +2096,7 @@ static noinline int push_nodes_for_insert(struct btrfs_trans_handle *trans,
 			if (ret)
 				wret = 1;
 			else {
-				wret = push_node_left(trans, fs_info,
-						      left, mid, 0);
+				wret = push_node_left(trans, left, mid, 0);
 			}
 		}
 		if (wret < 0)
@@ -3211,10 +3209,10 @@ void btrfs_set_item_key_safe(struct btrfs_fs_info *fs_info,
  * error, and > 0 if there was no room in the left hand block.
  */
 static int push_node_left(struct btrfs_trans_handle *trans,
-			  struct btrfs_fs_info *fs_info,
 			  struct extent_buffer *dst,
 			  struct extent_buffer *src, int empty)
 {
+	struct btrfs_fs_info *fs_info = trans->fs_info;
 	int push_items = 0;
 	int src_nritems;
 	int dst_nritems;

commit e064d5e9f0a00041f84d9eabd3d53546e4f8ab74
Author: David Sterba <dsterba@suse.com>
Date:   Wed Mar 20 14:58:13 2019 +0100

    btrfs: get fs_info from eb in btrfs_verify_level_key
    
    We can read fs_info from extent buffer and can drop it from the
    parameters.
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 3e26254fefe8..0998e7fba98b 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2420,7 +2420,7 @@ read_block_for_search(struct btrfs_root *root, struct btrfs_path *p,
 			 * being cached, read from scrub, or have multiple
 			 * parents (shared tree blocks).
 			 */
-			if (btrfs_verify_level_key(fs_info, tmp,
+			if (btrfs_verify_level_key(tmp,
 					parent_level - 1, &first_key, gen)) {
 				free_extent_buffer(tmp);
 				return -EUCLEAN;

commit d0d20b0f5c6d0ca109957160d40b67542c7e0b51
Author: David Sterba <dsterba@suse.com>
Date:   Wed Mar 20 14:54:01 2019 +0100

    btrfs: get fs_info from eb in read_node_slot
    
    We can read fs_info from extent buffer and can drop it from the
    parameters.
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 597337b07afa..3e26254fefe8 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1792,9 +1792,8 @@ static void root_sub_used(struct btrfs_root *root, u32 size)
 /* given a node and slot number, this reads the blocks it points to.  The
  * extent buffer is returned with a reference taken (but unlocked).
  */
-static noinline struct extent_buffer *
-read_node_slot(struct btrfs_fs_info *fs_info, struct extent_buffer *parent,
-	       int slot)
+static noinline struct extent_buffer *read_node_slot(
+				struct extent_buffer *parent, int slot)
 {
 	int level = btrfs_header_level(parent);
 	struct extent_buffer *eb;
@@ -1806,7 +1805,7 @@ read_node_slot(struct btrfs_fs_info *fs_info, struct extent_buffer *parent,
 	BUG_ON(level == 0);
 
 	btrfs_node_key_to_cpu(parent, &first_key, slot);
-	eb = read_tree_block(fs_info, btrfs_node_blockptr(parent, slot),
+	eb = read_tree_block(parent->fs_info, btrfs_node_blockptr(parent, slot),
 			     btrfs_node_ptr_generation(parent, slot),
 			     level - 1, &first_key);
 	if (!IS_ERR(eb) && !extent_buffer_uptodate(eb)) {
@@ -1863,7 +1862,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 			return 0;
 
 		/* promote the child to a root */
-		child = read_node_slot(fs_info, mid, 0);
+		child = read_node_slot(mid, 0);
 		if (IS_ERR(child)) {
 			ret = PTR_ERR(child);
 			btrfs_handle_fs_error(fs_info, ret, NULL);
@@ -1903,7 +1902,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 	    BTRFS_NODEPTRS_PER_BLOCK(fs_info) / 4)
 		return 0;
 
-	left = read_node_slot(fs_info, parent, pslot - 1);
+	left = read_node_slot(parent, pslot - 1);
 	if (IS_ERR(left))
 		left = NULL;
 
@@ -1918,7 +1917,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		}
 	}
 
-	right = read_node_slot(fs_info, parent, pslot + 1);
+	right = read_node_slot(parent, pslot + 1);
 	if (IS_ERR(right))
 		right = NULL;
 
@@ -2078,7 +2077,7 @@ static noinline int push_nodes_for_insert(struct btrfs_trans_handle *trans,
 	if (!parent)
 		return 1;
 
-	left = read_node_slot(fs_info, parent, pslot - 1);
+	left = read_node_slot(parent, pslot - 1);
 	if (IS_ERR(left))
 		left = NULL;
 
@@ -2131,7 +2130,7 @@ static noinline int push_nodes_for_insert(struct btrfs_trans_handle *trans,
 		btrfs_tree_unlock(left);
 		free_extent_buffer(left);
 	}
-	right = read_node_slot(fs_info, parent, pslot + 1);
+	right = read_node_slot(parent, pslot + 1);
 	if (IS_ERR(right))
 		right = NULL;
 
@@ -3767,7 +3766,7 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 
 	btrfs_assert_tree_locked(path->nodes[1]);
 
-	right = read_node_slot(fs_info, upper, slot + 1);
+	right = read_node_slot(upper, slot + 1);
 	/*
 	 * slot + 1 is not valid or we fail to read the right node,
 	 * no big deal, just return.
@@ -4002,7 +4001,7 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 
 	btrfs_assert_tree_locked(path->nodes[1]);
 
-	left = read_node_slot(fs_info, path->nodes[1], slot - 1);
+	left = read_node_slot(path->nodes[1], slot - 1);
 	/*
 	 * slot - 1 is not valid or we fail to read the left node,
 	 * no big deal, just return.
@@ -5133,7 +5132,6 @@ int btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,
 			 struct btrfs_path *path,
 			 u64 min_trans)
 {
-	struct btrfs_fs_info *fs_info = root->fs_info;
 	struct extent_buffer *cur;
 	struct btrfs_key found_key;
 	int slot;
@@ -5214,7 +5212,7 @@ int btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,
 			goto out;
 		}
 		btrfs_set_path_blocking(path);
-		cur = read_node_slot(fs_info, cur, slot);
+		cur = read_node_slot(cur, slot);
 		if (IS_ERR(cur)) {
 			ret = PTR_ERR(cur);
 			goto out;
@@ -5243,7 +5241,7 @@ static int tree_move_down(struct btrfs_fs_info *fs_info,
 	struct extent_buffer *eb;
 
 	BUG_ON(*level == 0);
-	eb = read_node_slot(fs_info, path->nodes[*level], path->slots[*level]);
+	eb = read_node_slot(path->nodes[*level], path->slots[*level]);
 	if (IS_ERR(eb))
 		return PTR_ERR(eb);
 

commit e902baac656479bdb956224ed693578424cf9e96
Author: David Sterba <dsterba@suse.com>
Date:   Wed Mar 20 14:36:46 2019 +0100

    btrfs: get fs_info from eb in btrfs_leaf_free_space
    
    We can read fs_info from extent buffer and can drop it from the
    parameters.
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 76e57a8c98df..597337b07afa 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2716,7 +2716,6 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 		      const struct btrfs_key *key, struct btrfs_path *p,
 		      int ins_len, int cow)
 {
-	struct btrfs_fs_info *fs_info = root->fs_info;
 	struct extent_buffer *b;
 	int slot;
 	int ret;
@@ -2914,7 +2913,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 		} else {
 			p->slots[level] = slot;
 			if (ins_len > 0 &&
-			    btrfs_leaf_free_space(fs_info, b) < ins_len) {
+			    btrfs_leaf_free_space(b) < ins_len) {
 				if (write_lock_level < 1) {
 					write_lock_level = 1;
 					btrfs_release_path(p);
@@ -3574,9 +3573,9 @@ static int leaf_space_used(struct extent_buffer *l, int start, int nr)
  * the start of the leaf data.  IOW, how much room
  * the leaf has left for both items and data
  */
-noinline int btrfs_leaf_free_space(struct btrfs_fs_info *fs_info,
-				   struct extent_buffer *leaf)
+noinline int btrfs_leaf_free_space(struct extent_buffer *leaf)
 {
+	struct btrfs_fs_info *fs_info = leaf->fs_info;
 	int nritems = btrfs_header_nritems(leaf);
 	int ret;
 
@@ -3635,7 +3634,8 @@ static noinline int __push_leaf_right(struct btrfs_fs_info *fs_info,
 			if (path->slots[0] > i)
 				break;
 			if (path->slots[0] == i) {
-				int space = btrfs_leaf_free_space(fs_info, left);
+				int space = btrfs_leaf_free_space(left);
+
 				if (space + push_space * 2 > free_space)
 					break;
 			}
@@ -3778,7 +3778,7 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 	btrfs_tree_lock(right);
 	btrfs_set_lock_blocking_write(right);
 
-	free_space = btrfs_leaf_free_space(fs_info, right);
+	free_space = btrfs_leaf_free_space(right);
 	if (free_space < data_size)
 		goto out_unlock;
 
@@ -3788,7 +3788,7 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 	if (ret)
 		goto out_unlock;
 
-	free_space = btrfs_leaf_free_space(fs_info, right);
+	free_space = btrfs_leaf_free_space(right);
 	if (free_space < data_size)
 		goto out_unlock;
 
@@ -3858,7 +3858,8 @@ static noinline int __push_leaf_left(struct btrfs_fs_info *fs_info,
 			if (path->slots[0] < i)
 				break;
 			if (path->slots[0] == i) {
-				int space = btrfs_leaf_free_space(fs_info, right);
+				int space = btrfs_leaf_free_space(right);
+
 				if (space + push_space * 2 > free_space)
 					break;
 			}
@@ -4012,7 +4013,7 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 	btrfs_tree_lock(left);
 	btrfs_set_lock_blocking_write(left);
 
-	free_space = btrfs_leaf_free_space(fs_info, left);
+	free_space = btrfs_leaf_free_space(left);
 	if (free_space < data_size) {
 		ret = 1;
 		goto out;
@@ -4028,7 +4029,7 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 		goto out;
 	}
 
-	free_space = btrfs_leaf_free_space(fs_info, left);
+	free_space = btrfs_leaf_free_space(left);
 	if (free_space < data_size) {
 		ret = 1;
 		goto out;
@@ -4124,7 +4125,6 @@ static noinline int push_for_double_split(struct btrfs_trans_handle *trans,
 					  struct btrfs_path *path,
 					  int data_size)
 {
-	struct btrfs_fs_info *fs_info = root->fs_info;
 	int ret;
 	int progress = 0;
 	int slot;
@@ -4133,7 +4133,7 @@ static noinline int push_for_double_split(struct btrfs_trans_handle *trans,
 
 	slot = path->slots[0];
 	if (slot < btrfs_header_nritems(path->nodes[0]))
-		space_needed -= btrfs_leaf_free_space(fs_info, path->nodes[0]);
+		space_needed -= btrfs_leaf_free_space(path->nodes[0]);
 
 	/*
 	 * try to push all the items after our slot into the
@@ -4154,14 +4154,14 @@ static noinline int push_for_double_split(struct btrfs_trans_handle *trans,
 	if (path->slots[0] == 0 || path->slots[0] == nritems)
 		return 0;
 
-	if (btrfs_leaf_free_space(fs_info, path->nodes[0]) >= data_size)
+	if (btrfs_leaf_free_space(path->nodes[0]) >= data_size)
 		return 0;
 
 	/* try to push all the items before our slot into the next leaf */
 	slot = path->slots[0];
 	space_needed = data_size;
 	if (slot > 0)
-		space_needed -= btrfs_leaf_free_space(fs_info, path->nodes[0]);
+		space_needed -= btrfs_leaf_free_space(path->nodes[0]);
 	ret = push_leaf_left(trans, root, path, 1, space_needed, 0, slot);
 	if (ret < 0)
 		return ret;
@@ -4210,7 +4210,7 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 		int space_needed = data_size;
 
 		if (slot < btrfs_header_nritems(l))
-			space_needed -= btrfs_leaf_free_space(fs_info, l);
+			space_needed -= btrfs_leaf_free_space(l);
 
 		wret = push_leaf_right(trans, root, path, space_needed,
 				       space_needed, 0, 0);
@@ -4219,8 +4219,7 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 		if (wret) {
 			space_needed = data_size;
 			if (slot > 0)
-				space_needed -= btrfs_leaf_free_space(fs_info,
-								      l);
+				space_needed -= btrfs_leaf_free_space(l);
 			wret = push_leaf_left(trans, root, path, space_needed,
 					      space_needed, 0, (u32)-1);
 			if (wret < 0)
@@ -4229,7 +4228,7 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 		l = path->nodes[0];
 
 		/* did the pushes work? */
-		if (btrfs_leaf_free_space(fs_info, l) >= data_size)
+		if (btrfs_leaf_free_space(l) >= data_size)
 			return 0;
 	}
 
@@ -4336,7 +4335,7 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 push_for_double:
 	push_for_double_split(trans, root, path, data_size);
 	tried_avoid_double = 1;
-	if (btrfs_leaf_free_space(fs_info, path->nodes[0]) >= data_size)
+	if (btrfs_leaf_free_space(path->nodes[0]) >= data_size)
 		return 0;
 	goto again;
 }
@@ -4345,7 +4344,6 @@ static noinline int setup_leaf_for_split(struct btrfs_trans_handle *trans,
 					 struct btrfs_root *root,
 					 struct btrfs_path *path, int ins_len)
 {
-	struct btrfs_fs_info *fs_info = root->fs_info;
 	struct btrfs_key key;
 	struct extent_buffer *leaf;
 	struct btrfs_file_extent_item *fi;
@@ -4359,7 +4357,7 @@ static noinline int setup_leaf_for_split(struct btrfs_trans_handle *trans,
 	BUG_ON(key.type != BTRFS_EXTENT_DATA_KEY &&
 	       key.type != BTRFS_EXTENT_CSUM_KEY);
 
-	if (btrfs_leaf_free_space(fs_info, leaf) >= ins_len)
+	if (btrfs_leaf_free_space(leaf) >= ins_len)
 		return 0;
 
 	item_size = btrfs_item_size_nr(leaf, path->slots[0]);
@@ -4386,7 +4384,7 @@ static noinline int setup_leaf_for_split(struct btrfs_trans_handle *trans,
 		goto err;
 
 	/* the leaf has  changed, it now has room.  return now */
-	if (btrfs_leaf_free_space(fs_info, path->nodes[0]) >= ins_len)
+	if (btrfs_leaf_free_space(path->nodes[0]) >= ins_len)
 		goto err;
 
 	if (key.type == BTRFS_EXTENT_DATA_KEY) {
@@ -4425,7 +4423,7 @@ static noinline int split_item(struct btrfs_fs_info *fs_info,
 	struct btrfs_disk_key disk_key;
 
 	leaf = path->nodes[0];
-	BUG_ON(btrfs_leaf_free_space(fs_info, leaf) < sizeof(struct btrfs_item));
+	BUG_ON(btrfs_leaf_free_space(leaf) < sizeof(struct btrfs_item));
 
 	btrfs_set_path_blocking(path);
 
@@ -4474,7 +4472,7 @@ static noinline int split_item(struct btrfs_fs_info *fs_info,
 			    item_size - split_offset);
 	btrfs_mark_buffer_dirty(leaf);
 
-	BUG_ON(btrfs_leaf_free_space(fs_info, leaf) < 0);
+	BUG_ON(btrfs_leaf_free_space(leaf) < 0);
 	kfree(buf);
 	return 0;
 }
@@ -4642,7 +4640,7 @@ void btrfs_truncate_item(struct btrfs_fs_info *fs_info,
 	btrfs_set_item_size(leaf, item, new_size);
 	btrfs_mark_buffer_dirty(leaf);
 
-	if (btrfs_leaf_free_space(fs_info, leaf) < 0) {
+	if (btrfs_leaf_free_space(leaf) < 0) {
 		btrfs_print_leaf(leaf);
 		BUG();
 	}
@@ -4671,7 +4669,7 @@ void btrfs_extend_item(struct btrfs_fs_info *fs_info, struct btrfs_path *path,
 	nritems = btrfs_header_nritems(leaf);
 	data_end = leaf_data_end(leaf);
 
-	if (btrfs_leaf_free_space(fs_info, leaf) < data_size) {
+	if (btrfs_leaf_free_space(leaf) < data_size) {
 		btrfs_print_leaf(leaf);
 		BUG();
 	}
@@ -4710,7 +4708,7 @@ void btrfs_extend_item(struct btrfs_fs_info *fs_info, struct btrfs_path *path,
 	btrfs_set_item_size(leaf, item, old_size + data_size);
 	btrfs_mark_buffer_dirty(leaf);
 
-	if (btrfs_leaf_free_space(fs_info, leaf) < 0) {
+	if (btrfs_leaf_free_space(leaf) < 0) {
 		btrfs_print_leaf(leaf);
 		BUG();
 	}
@@ -4749,10 +4747,10 @@ void setup_items_for_insert(struct btrfs_root *root, struct btrfs_path *path,
 	nritems = btrfs_header_nritems(leaf);
 	data_end = leaf_data_end(leaf);
 
-	if (btrfs_leaf_free_space(fs_info, leaf) < total_size) {
+	if (btrfs_leaf_free_space(leaf) < total_size) {
 		btrfs_print_leaf(leaf);
 		btrfs_crit(fs_info, "not enough freespace need %u have %d",
-			   total_size, btrfs_leaf_free_space(fs_info, leaf));
+			   total_size, btrfs_leaf_free_space(leaf));
 		BUG();
 	}
 
@@ -4803,7 +4801,7 @@ void setup_items_for_insert(struct btrfs_root *root, struct btrfs_path *path,
 	btrfs_set_header_nritems(leaf, nritems + nr);
 	btrfs_mark_buffer_dirty(leaf);
 
-	if (btrfs_leaf_free_space(fs_info, leaf) < 0) {
+	if (btrfs_leaf_free_space(leaf) < 0) {
 		btrfs_print_leaf(leaf);
 		BUG();
 	}

commit 6a884d7d527f32b5ea80dc472968a5430ffee9f5
Author: David Sterba <dsterba@suse.com>
Date:   Wed Mar 20 14:30:02 2019 +0100

    btrfs: get fs_info from eb in clean_tree_block
    
    We can read fs_info from extent buffer and can drop it from the
    parameters.
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index b83627a372b1..76e57a8c98df 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -970,7 +970,7 @@ static noinline int update_ref_for_cow(struct btrfs_trans_handle *trans,
 			if (ret)
 				return ret;
 		}
-		clean_tree_block(fs_info, buf);
+		btrfs_clean_tree_block(buf);
 		*last_ref = 1;
 	}
 	return 0;
@@ -1888,7 +1888,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 
 		path->locks[level] = 0;
 		path->nodes[level] = NULL;
-		clean_tree_block(fs_info, mid);
+		btrfs_clean_tree_block(mid);
 		btrfs_tree_unlock(mid);
 		/* once for the path */
 		free_extent_buffer(mid);
@@ -1949,7 +1949,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		if (wret < 0 && wret != -ENOSPC)
 			ret = wret;
 		if (btrfs_header_nritems(right) == 0) {
-			clean_tree_block(fs_info, right);
+			btrfs_clean_tree_block(right);
 			btrfs_tree_unlock(right);
 			del_ptr(root, path, level + 1, pslot + 1);
 			root_sub_used(root, right->len);
@@ -1994,7 +1994,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		BUG_ON(wret == 1);
 	}
 	if (btrfs_header_nritems(mid) == 0) {
-		clean_tree_block(fs_info, mid);
+		btrfs_clean_tree_block(mid);
 		btrfs_tree_unlock(mid);
 		del_ptr(root, path, level + 1, pslot);
 		root_sub_used(root, mid->len);
@@ -3704,7 +3704,7 @@ static noinline int __push_leaf_right(struct btrfs_fs_info *fs_info,
 	if (left_nritems)
 		btrfs_mark_buffer_dirty(left);
 	else
-		clean_tree_block(fs_info, left);
+		btrfs_clean_tree_block(left);
 
 	btrfs_mark_buffer_dirty(right);
 
@@ -3716,7 +3716,7 @@ static noinline int __push_leaf_right(struct btrfs_fs_info *fs_info,
 	if (path->slots[0] >= left_nritems) {
 		path->slots[0] -= left_nritems;
 		if (btrfs_header_nritems(path->nodes[0]) == 0)
-			clean_tree_block(fs_info, path->nodes[0]);
+			btrfs_clean_tree_block(path->nodes[0]);
 		btrfs_tree_unlock(path->nodes[0]);
 		free_extent_buffer(path->nodes[0]);
 		path->nodes[0] = right;
@@ -3944,7 +3944,7 @@ static noinline int __push_leaf_left(struct btrfs_fs_info *fs_info,
 	if (right_nritems)
 		btrfs_mark_buffer_dirty(right);
 	else
-		clean_tree_block(fs_info, right);
+		btrfs_clean_tree_block(right);
 
 	btrfs_item_key(right, &disk_key, 0);
 	fixup_low_keys(path, &disk_key, 1);
@@ -5005,7 +5005,7 @@ int btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 			btrfs_set_header_level(leaf, 0);
 		} else {
 			btrfs_set_path_blocking(path);
-			clean_tree_block(fs_info, leaf);
+			btrfs_clean_tree_block(leaf);
 			btrfs_del_leaf(trans, root, path, leaf);
 		}
 	} else {

commit ed874f0db89724d0af1b4793fb518f640f333b0b
Author: David Sterba <dsterba@suse.com>
Date:   Wed Mar 20 14:22:04 2019 +0100

    btrfs: get fs_info from eb in tree_mod_log_eb_copy
    
    We can read fs_info from extent buffer and can drop it from the
    parameters.
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 5be2beef18be..b83627a372b1 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -726,11 +726,11 @@ tree_mod_log_search(struct btrfs_fs_info *fs_info, u64 start, u64 min_seq)
 	return __tree_mod_log_search(fs_info, start, min_seq, 0);
 }
 
-static noinline int
-tree_mod_log_eb_copy(struct btrfs_fs_info *fs_info, struct extent_buffer *dst,
+static noinline int tree_mod_log_eb_copy(struct extent_buffer *dst,
 		     struct extent_buffer *src, unsigned long dst_offset,
 		     unsigned long src_offset, int nr_items)
 {
+	struct btrfs_fs_info *fs_info = dst->fs_info;
 	int ret = 0;
 	struct tree_mod_elem **tm_list = NULL;
 	struct tree_mod_elem **tm_list_add, **tm_list_rem;
@@ -3249,8 +3249,7 @@ static int push_node_left(struct btrfs_trans_handle *trans,
 	} else
 		push_items = min(src_nritems - 8, push_items);
 
-	ret = tree_mod_log_eb_copy(fs_info, dst, src, dst_nritems, 0,
-				   push_items);
+	ret = tree_mod_log_eb_copy(dst, src, dst_nritems, 0, push_items);
 	if (ret) {
 		btrfs_abort_transaction(trans, ret);
 		return ret;
@@ -3325,8 +3324,8 @@ static int balance_node_right(struct btrfs_trans_handle *trans,
 				      (dst_nritems) *
 				      sizeof(struct btrfs_key_ptr));
 
-	ret = tree_mod_log_eb_copy(fs_info, dst, src, 0,
-				   src_nritems - push_items, push_items);
+	ret = tree_mod_log_eb_copy(dst, src, 0, src_nritems - push_items,
+				   push_items);
 	if (ret) {
 		btrfs_abort_transaction(trans, ret);
 		return ret;
@@ -3511,7 +3510,7 @@ static noinline int split_node(struct btrfs_trans_handle *trans,
 	root_add_used(root, fs_info->nodesize);
 	ASSERT(btrfs_header_level(c) == level);
 
-	ret = tree_mod_log_eb_copy(fs_info, split, c, 0, mid, c_nritems - mid);
+	ret = tree_mod_log_eb_copy(split, c, 0, mid, c_nritems - mid);
 	if (ret) {
 		btrfs_abort_transaction(trans, ret);
 		return ret;

commit 8f881e8c1880fb7029e74ccdaa7891bd042b6c63
Author: David Sterba <dsterba@suse.com>
Date:   Wed Mar 20 11:33:10 2019 +0100

    btrfs: get fs_info from eb in leaf_data_end
    
    We can read fs_info from extent buffer and can drop it from the
    parameters.
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 7b1bc25f10cb..5be2beef18be 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -3665,10 +3665,10 @@ static noinline int __push_leaf_right(struct btrfs_fs_info *fs_info,
 	right_nritems = btrfs_header_nritems(right);
 
 	push_space = btrfs_item_end_nr(left, left_nritems - push_items);
-	push_space -= leaf_data_end(fs_info, left);
+	push_space -= leaf_data_end(left);
 
 	/* make room in the right data area */
-	data_end = leaf_data_end(fs_info, right);
+	data_end = leaf_data_end(right);
 	memmove_extent_buffer(right,
 			      BTRFS_LEAF_DATA_OFFSET + data_end - push_space,
 			      BTRFS_LEAF_DATA_OFFSET + data_end,
@@ -3677,7 +3677,7 @@ static noinline int __push_leaf_right(struct btrfs_fs_info *fs_info,
 	/* copy from the left data area */
 	copy_extent_buffer(right, left, BTRFS_LEAF_DATA_OFFSET +
 		     BTRFS_LEAF_DATA_SIZE(fs_info) - push_space,
-		     BTRFS_LEAF_DATA_OFFSET + leaf_data_end(fs_info, left),
+		     BTRFS_LEAF_DATA_OFFSET + leaf_data_end(left),
 		     push_space);
 
 	memmove_extent_buffer(right, btrfs_item_nr_offset(push_items),
@@ -3892,7 +3892,7 @@ static noinline int __push_leaf_left(struct btrfs_fs_info *fs_info,
 		     btrfs_item_offset_nr(right, push_items - 1);
 
 	copy_extent_buffer(left, right, BTRFS_LEAF_DATA_OFFSET +
-		     leaf_data_end(fs_info, left) - push_space,
+		     leaf_data_end(left) - push_space,
 		     BTRFS_LEAF_DATA_OFFSET +
 		     btrfs_item_offset_nr(right, push_items - 1),
 		     push_space);
@@ -3919,11 +3919,11 @@ static noinline int __push_leaf_left(struct btrfs_fs_info *fs_info,
 
 	if (push_items < right_nritems) {
 		push_space = btrfs_item_offset_nr(right, push_items - 1) -
-						  leaf_data_end(fs_info, right);
+						  leaf_data_end(right);
 		memmove_extent_buffer(right, BTRFS_LEAF_DATA_OFFSET +
 				      BTRFS_LEAF_DATA_SIZE(fs_info) - push_space,
 				      BTRFS_LEAF_DATA_OFFSET +
-				      leaf_data_end(fs_info, right), push_space);
+				      leaf_data_end(right), push_space);
 
 		memmove_extent_buffer(right, btrfs_item_nr_offset(0),
 			      btrfs_item_nr_offset(push_items),
@@ -4065,7 +4065,7 @@ static noinline void copy_for_split(struct btrfs_trans_handle *trans,
 
 	nritems = nritems - mid;
 	btrfs_set_header_nritems(right, nritems);
-	data_copy_size = btrfs_item_end_nr(l, mid) - leaf_data_end(fs_info, l);
+	data_copy_size = btrfs_item_end_nr(l, mid) - leaf_data_end(l);
 
 	copy_extent_buffer(right, l, btrfs_item_nr_offset(0),
 			   btrfs_item_nr_offset(mid),
@@ -4074,7 +4074,7 @@ static noinline void copy_for_split(struct btrfs_trans_handle *trans,
 	copy_extent_buffer(right, l,
 		     BTRFS_LEAF_DATA_OFFSET + BTRFS_LEAF_DATA_SIZE(fs_info) -
 		     data_copy_size, BTRFS_LEAF_DATA_OFFSET +
-		     leaf_data_end(fs_info, l), data_copy_size);
+		     leaf_data_end(l), data_copy_size);
 
 	rt_data_off = BTRFS_LEAF_DATA_SIZE(fs_info) - btrfs_item_end_nr(l, mid);
 
@@ -4577,7 +4577,7 @@ void btrfs_truncate_item(struct btrfs_fs_info *fs_info,
 		return;
 
 	nritems = btrfs_header_nritems(leaf);
-	data_end = leaf_data_end(fs_info, leaf);
+	data_end = leaf_data_end(leaf);
 
 	old_data_start = btrfs_item_offset_nr(leaf, slot);
 
@@ -4670,7 +4670,7 @@ void btrfs_extend_item(struct btrfs_fs_info *fs_info, struct btrfs_path *path,
 	leaf = path->nodes[0];
 
 	nritems = btrfs_header_nritems(leaf);
-	data_end = leaf_data_end(fs_info, leaf);
+	data_end = leaf_data_end(leaf);
 
 	if (btrfs_leaf_free_space(fs_info, leaf) < data_size) {
 		btrfs_print_leaf(leaf);
@@ -4748,7 +4748,7 @@ void setup_items_for_insert(struct btrfs_root *root, struct btrfs_path *path,
 	slot = path->slots[0];
 
 	nritems = btrfs_header_nritems(leaf);
-	data_end = leaf_data_end(fs_info, leaf);
+	data_end = leaf_data_end(leaf);
 
 	if (btrfs_leaf_free_space(fs_info, leaf) < total_size) {
 		btrfs_print_leaf(leaf);
@@ -4976,7 +4976,7 @@ int btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 	nritems = btrfs_header_nritems(leaf);
 
 	if (slot + nr != nritems) {
-		int data_end = leaf_data_end(fs_info, leaf);
+		int data_end = leaf_data_end(leaf);
 
 		memmove_extent_buffer(leaf, BTRFS_LEAF_DATA_OFFSET +
 			      data_end + dsize,

commit 290342f66108638048997b71393f0dd88e771352
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Mon Mar 25 14:02:25 2019 +0100

    btrfs: use BUG() instead of BUG_ON(1)
    
    BUG_ON(1) leads to bogus warnings from clang when
    CONFIG_PROFILE_ANNOTATED_BRANCHES is set:
    
    fs/btrfs/volumes.c:5041:3: error: variable 'max_chunk_size' is used uninitialized whenever 'if' condition is false
          [-Werror,-Wsometimes-uninitialized]
                    BUG_ON(1);
                    ^~~~~~~~~
    include/asm-generic/bug.h:61:36: note: expanded from macro 'BUG_ON'
     #define BUG_ON(condition) do { if (unlikely(condition)) BUG(); } while (0)
                                       ^~~~~~~~~~~~~~~~~~~
    include/linux/compiler.h:48:23: note: expanded from macro 'unlikely'
     #  define unlikely(x)   (__branch_check__(x, 0, __builtin_constant_p(x)))
                            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    fs/btrfs/volumes.c:5046:9: note: uninitialized use occurs here
                                 max_chunk_size);
                                 ^~~~~~~~~~~~~~
    include/linux/kernel.h:860:36: note: expanded from macro 'min'
     #define min(x, y)       __careful_cmp(x, y, <)
                                             ^
    include/linux/kernel.h:853:17: note: expanded from macro '__careful_cmp'
                    __cmp_once(x, y, __UNIQUE_ID(__x), __UNIQUE_ID(__y), op))
                                  ^
    include/linux/kernel.h:847:25: note: expanded from macro '__cmp_once'
                    typeof(y) unique_y = (y);               \
                                          ^
    fs/btrfs/volumes.c:5041:3: note: remove the 'if' if its condition is always true
                    BUG_ON(1);
                    ^
    include/asm-generic/bug.h:61:32: note: expanded from macro 'BUG_ON'
     #define BUG_ON(condition) do { if (unlikely(condition)) BUG(); } while (0)
                                   ^
    fs/btrfs/volumes.c:4993:20: note: initialize the variable 'max_chunk_size' to silence this warning
            u64 max_chunk_size;
                              ^
                               = 0
    
    Change it to BUG() so clang can see that this code path can never
    continue.
    
    Reviewed-by: Nikolay Borisov <nborisov@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 65b12963e72b..7b1bc25f10cb 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -4684,7 +4684,7 @@ void btrfs_extend_item(struct btrfs_fs_info *fs_info, struct btrfs_path *path,
 		btrfs_print_leaf(leaf);
 		btrfs_crit(fs_info, "slot %d too large, nritems %d",
 			   slot, nritems);
-		BUG_ON(1);
+		BUG();
 	}
 
 	/*
@@ -4764,7 +4764,7 @@ void setup_items_for_insert(struct btrfs_root *root, struct btrfs_path *path,
 			btrfs_print_leaf(leaf);
 			btrfs_crit(fs_info, "slot %d old_data %d data_end %d",
 				   slot, old_data, data_end);
-			BUG_ON(1);
+			BUG();
 		}
 		/*
 		 * item0..itemN ... dataN.offset..dataN.size .. data0.size

commit 448de471cd4cab0cedd15770082567a69a784a11
Author: Qu Wenruo <wqu@suse.com>
Date:   Tue Mar 12 17:10:40 2019 +0800

    btrfs: Check the first key and level for cached extent buffer
    
    [BUG]
    When reading a file from a fuzzed image, kernel can panic like:
    
      BTRFS warning (device loop0): csum failed root 5 ino 270 off 0 csum 0x98f94189 expected csum 0x00000000 mirror 1
      assertion failed: !memcmp_extent_buffer(b, &disk_key, offsetof(struct btrfs_leaf, items[0].key), sizeof(disk_key)), file: fs/btrfs/ctree.c, line: 2544
      ------------[ cut here ]------------
      kernel BUG at fs/btrfs/ctree.h:3500!
      invalid opcode: 0000 [#1] PREEMPT SMP NOPTI
      RIP: 0010:btrfs_search_slot.cold.24+0x61/0x63 [btrfs]
      Call Trace:
       btrfs_lookup_csum+0x52/0x150 [btrfs]
       __btrfs_lookup_bio_sums+0x209/0x640 [btrfs]
       btrfs_submit_bio_hook+0x103/0x170 [btrfs]
       submit_one_bio+0x59/0x80 [btrfs]
       extent_read_full_page+0x58/0x80 [btrfs]
       generic_file_read_iter+0x2f6/0x9d0
       __vfs_read+0x14d/0x1a0
       vfs_read+0x8d/0x140
       ksys_read+0x52/0xc0
       do_syscall_64+0x60/0x210
       entry_SYSCALL_64_after_hwframe+0x49/0xbe
    
    [CAUSE]
    The fuzzed image has a corrupted leaf whose first key doesn't match its
    parent:
    
      checksum tree key (CSUM_TREE ROOT_ITEM 0)
      node 29741056 level 1 items 14 free 107 generation 19 owner CSUM_TREE
      fs uuid 3381d111-94a3-4ac7-8f39-611bbbdab7e6
      chunk uuid 9af1c3c7-2af5-488b-8553-530bd515f14c
            ...
              key (EXTENT_CSUM EXTENT_CSUM 79691776) block 29761536 gen 19
    
      leaf 29761536 items 1 free space 1726 generation 19 owner CSUM_TREE
      leaf 29761536 flags 0x1(WRITTEN) backref revision 1
      fs uuid 3381d111-94a3-4ac7-8f39-611bbbdab7e6
      chunk uuid 9af1c3c7-2af5-488b-8553-530bd515f14c
              item 0 key (EXTENT_CSUM EXTENT_CSUM 8798638964736) itemoff 1751 itemsize 2244
                      range start 8798638964736 end 8798641262592 length 2297856
    
    When reading the above tree block, we have extent_buffer->refs = 2 in
    the context:
    
    - initial one from __alloc_extent_buffer()
      alloc_extent_buffer()
      |- __alloc_extent_buffer()
         |- atomic_set(&eb->refs, 1)
    
    - one being added to fs_info->buffer_radix
      alloc_extent_buffer()
      |- check_buffer_tree_ref()
         |- atomic_inc(&eb->refs)
    
    So if even we call free_extent_buffer() in read_tree_block or other
    similar situation, we only decrease the refs by 1, it doesn't reach 0
    and won't be freed right now.
    
    The staled eb and its corrupted content will still be kept cached.
    
    Furthermore, we have several extra cases where we either don't do first
    key check or the check is not proper for all callers:
    
    - scrub
      We just don't have first key in this context.
    
    - shared tree block
      One tree block can be shared by several snapshot/subvolume trees.
      In that case, the first key check for one subvolume doesn't apply to
      another.
    
    So for the above reasons, a corrupted extent buffer can sneak into the
    buffer cache.
    
    [FIX]
    Call verify_level_key in read_block_for_search to do another
    verification. For that purpose the function is exported.
    
    Due to above reasons, although we can free corrupted extent buffer from
    cache, we still need the check in read_block_for_search(), for scrub and
    shared tree blocks.
    
    Link: https://bugzilla.kernel.org/show_bug.cgi?id=202755
    Link: https://bugzilla.kernel.org/show_bug.cgi?id=202757
    Link: https://bugzilla.kernel.org/show_bug.cgi?id=202759
    Link: https://bugzilla.kernel.org/show_bug.cgi?id=202761
    Link: https://bugzilla.kernel.org/show_bug.cgi?id=202767
    Link: https://bugzilla.kernel.org/show_bug.cgi?id=202769
    Reported-by: Yoon Jungyeon <jungyeon@gatech.edu>
    CC: stable@vger.kernel.org # 4.19+
    Signed-off-by: Qu Wenruo <wqu@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 324df36d28bf..65b12963e72b 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2416,6 +2416,16 @@ read_block_for_search(struct btrfs_root *root, struct btrfs_path *p,
 	if (tmp) {
 		/* first we do an atomic uptodate check */
 		if (btrfs_buffer_uptodate(tmp, gen, 1) > 0) {
+			/*
+			 * Do extra check for first_key, eb can be stale due to
+			 * being cached, read from scrub, or have multiple
+			 * parents (shared tree blocks).
+			 */
+			if (btrfs_verify_level_key(fs_info, tmp,
+					parent_level - 1, &first_key, gen)) {
+				free_extent_buffer(tmp);
+				return -EUCLEAN;
+			}
 			*eb_ret = tmp;
 			return 0;
 		}

commit 253002f2e3f4e2bbb0cbdd9e9fe2f5b9ec88f694
Author: Filipe Manana <fdmanana@suse.com>
Date:   Wed Feb 20 11:11:43 2019 +0000

    Btrfs: remove assertion when searching for a key in a node/leaf
    
    At ctree.c:key_search(), the assertion that verifies the first key on a
    child extent buffer corresponds to the key at a specific slot in the
    parent has a disadvantage: we effectively hit a BUG_ON() which requires
    rebooting the machine later. It also does not tell any information about
    which extent buffer is affected, from which root, the expected and found
    keys, etc.
    
    However as of commit 581c1760415c48 ("btrfs: Validate child tree block's
    level and first key"), that assertion is not needed since at the time we
    read an extent buffer from disk we validate that its first key matches the
    key, at the respective slot, in the parent extent buffer. Therefore just
    remove the assertion at key_search().
    
    Reviewed-by: Qu Wenruo <wqu@suse.com>
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 399b9c5182d5..324df36d28bf 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2544,26 +2544,6 @@ setup_nodes_for_search(struct btrfs_trans_handle *trans,
 	return ret;
 }
 
-static void key_search_validate(struct extent_buffer *b,
-				const struct btrfs_key *key,
-				int level)
-{
-#ifdef CONFIG_BTRFS_ASSERT
-	struct btrfs_disk_key disk_key;
-
-	btrfs_cpu_key_to_disk(&disk_key, key);
-
-	if (level == 0)
-		ASSERT(!memcmp_extent_buffer(b, &disk_key,
-		    offsetof(struct btrfs_leaf, items[0].key),
-		    sizeof(disk_key)));
-	else
-		ASSERT(!memcmp_extent_buffer(b, &disk_key,
-		    offsetof(struct btrfs_node, ptrs[0].key),
-		    sizeof(disk_key)));
-#endif
-}
-
 static int key_search(struct extent_buffer *b, const struct btrfs_key *key,
 		      int level, int *prev_cmp, int *slot)
 {
@@ -2572,7 +2552,6 @@ static int key_search(struct extent_buffer *b, const struct btrfs_key *key,
 		return *prev_cmp;
 	}
 
-	key_search_validate(b, key, level);
 	*slot = 0;
 
 	return 0;

commit cbca7d59fea4e81ee3bf724cf20018f96d53ccea
Author: Filipe Manana <fdmanana@suse.com>
Date:   Mon Feb 18 16:57:26 2019 +0000

    Btrfs: add missing error handling after doing leaf/node binary search
    
    The function map_private_extent_buffer() can return an -EINVAL error, and
    it is called by generic_bin_search() which will return back the error. The
    btrfs_bin_search() function in turn calls generic_bin_search() and the
    key_search() function calls btrfs_bin_search(), so both can return the
    -EINVAL error coming from the map_private_extent_buffer() function. Some
    callers of these functions were ignoring that these functions can return
    an error, so fix them to deal with error return values.
    
    Reviewed-by: Nikolay Borisov <nborisov@suse.com>
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 4fac5cc2e648..399b9c5182d5 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -3020,6 +3020,8 @@ int btrfs_search_old_slot(struct btrfs_root *root, const struct btrfs_key *key,
 		 */
 		prev_cmp = -1;
 		ret = key_search(b, key, level, &prev_cmp, &slot);
+		if (ret < 0)
+			goto done;
 
 		if (level != 0) {
 			int dec = 0;
@@ -5171,6 +5173,10 @@ int btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,
 		nritems = btrfs_header_nritems(cur);
 		level = btrfs_header_level(cur);
 		sret = btrfs_bin_search(cur, min_key, level, &slot);
+		if (sret < 0) {
+			ret = sret;
+			goto out;
+		}
 
 		/* at the lowest level, we're done, setup the path and exit */
 		if (level == path->lowest_level) {

commit 766ece54f4c9c29b25eabd091a2ee939feb1669e
Author: David Sterba <dsterba@suse.com>
Date:   Wed Jan 23 18:07:14 2019 +0100

    btrfs: merge btrfs_set_lock_blocking_rw with it's caller
    
    The last caller that does not have a fixed value of lock is
    btrfs_set_path_blocking, that actually does the same conditional swtich
    by the lock type so we can merge the branches together and remove the
    helper.
    
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 3940d707fcf3..4fac5cc2e648 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -46,11 +46,18 @@ noinline void btrfs_set_path_blocking(struct btrfs_path *p)
 	for (i = 0; i < BTRFS_MAX_LEVEL; i++) {
 		if (!p->nodes[i] || !p->locks[i])
 			continue;
-		btrfs_set_lock_blocking_rw(p->nodes[i], p->locks[i]);
-		if (p->locks[i] == BTRFS_READ_LOCK)
+		/*
+		 * If we currently have a spinning reader or writer lock this
+		 * will bump the count of blocking holders and drop the
+		 * spinlock.
+		 */
+		if (p->locks[i] == BTRFS_READ_LOCK) {
+			btrfs_set_lock_blocking_read(p->nodes[i]);
 			p->locks[i] = BTRFS_READ_LOCK_BLOCKING;
-		else if (p->locks[i] == BTRFS_WRITE_LOCK)
+		} else if (p->locks[i] == BTRFS_WRITE_LOCK) {
+			btrfs_set_lock_blocking_write(p->nodes[i]);
 			p->locks[i] = BTRFS_WRITE_LOCK_BLOCKING;
+		}
 	}
 }
 

commit 8bead258206f4d4f485ad55bc1e39d23bbfe2fdd
Author: David Sterba <dsterba@suse.com>
Date:   Wed Apr 4 02:03:48 2018 +0200

    btrfs: open code now trivial btrfs_set_lock_blocking
    
    btrfs_set_lock_blocking is now only a simple wrapper around
    btrfs_set_lock_blocking_write. The name does not bring any semantic
    value that could not be inferred from the new function so there's no
    point keeping it.
    
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 75e0f737d7d2..3940d707fcf3 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1487,8 +1487,8 @@ noinline int btrfs_cow_block(struct btrfs_trans_handle *trans,
 	search_start = buf->start & ~((u64)SZ_1G - 1);
 
 	if (parent)
-		btrfs_set_lock_blocking(parent);
-	btrfs_set_lock_blocking(buf);
+		btrfs_set_lock_blocking_write(parent);
+	btrfs_set_lock_blocking_write(buf);
 
 	/*
 	 * Before CoWing this block for later modification, check if it's
@@ -1590,7 +1590,7 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 	if (parent_nritems <= 1)
 		return 0;
 
-	btrfs_set_lock_blocking(parent);
+	btrfs_set_lock_blocking_write(parent);
 
 	for (i = start_slot; i <= end_slot; i++) {
 		struct btrfs_key first_key;
@@ -1649,7 +1649,7 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 			search_start = last_block;
 
 		btrfs_tree_lock(cur);
-		btrfs_set_lock_blocking(cur);
+		btrfs_set_lock_blocking_write(cur);
 		err = __btrfs_cow_block(trans, root, cur, parent, i,
 					&cur, search_start,
 					min(16 * blocksize,
@@ -1864,7 +1864,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		}
 
 		btrfs_tree_lock(child);
-		btrfs_set_lock_blocking(child);
+		btrfs_set_lock_blocking_write(child);
 		ret = btrfs_cow_block(trans, root, child, mid, 0, &child);
 		if (ret) {
 			btrfs_tree_unlock(child);
@@ -1902,7 +1902,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 
 	if (left) {
 		btrfs_tree_lock(left);
-		btrfs_set_lock_blocking(left);
+		btrfs_set_lock_blocking_write(left);
 		wret = btrfs_cow_block(trans, root, left,
 				       parent, pslot - 1, &left);
 		if (wret) {
@@ -1917,7 +1917,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 
 	if (right) {
 		btrfs_tree_lock(right);
-		btrfs_set_lock_blocking(right);
+		btrfs_set_lock_blocking_write(right);
 		wret = btrfs_cow_block(trans, root, right,
 				       parent, pslot + 1, &right);
 		if (wret) {
@@ -2080,7 +2080,7 @@ static noinline int push_nodes_for_insert(struct btrfs_trans_handle *trans,
 		u32 left_nr;
 
 		btrfs_tree_lock(left);
-		btrfs_set_lock_blocking(left);
+		btrfs_set_lock_blocking_write(left);
 
 		left_nr = btrfs_header_nritems(left);
 		if (left_nr >= BTRFS_NODEPTRS_PER_BLOCK(fs_info) - 1) {
@@ -2135,7 +2135,7 @@ static noinline int push_nodes_for_insert(struct btrfs_trans_handle *trans,
 		u32 right_nr;
 
 		btrfs_tree_lock(right);
-		btrfs_set_lock_blocking(right);
+		btrfs_set_lock_blocking_write(right);
 
 		right_nr = btrfs_header_nritems(right);
 		if (right_nr >= BTRFS_NODEPTRS_PER_BLOCK(fs_info) - 1) {
@@ -3779,7 +3779,7 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 		return 1;
 
 	btrfs_tree_lock(right);
-	btrfs_set_lock_blocking(right);
+	btrfs_set_lock_blocking_write(right);
 
 	free_space = btrfs_leaf_free_space(fs_info, right);
 	if (free_space < data_size)
@@ -4013,7 +4013,7 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 		return 1;
 
 	btrfs_tree_lock(left);
-	btrfs_set_lock_blocking(left);
+	btrfs_set_lock_blocking_write(left);
 
 	free_space = btrfs_leaf_free_space(fs_info, left);
 	if (free_space < data_size) {

commit 300aa896e1199bcd0dfb61aae86356714e017355
Author: David Sterba <dsterba@suse.com>
Date:   Wed Apr 4 02:00:17 2018 +0200

    btrfs: replace btrfs_set_lock_blocking_rw with appropriate helpers
    
    We can use the right helper where the lock type is a fixed parameter.
    
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 0f2c20e0b108..75e0f737d7d2 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1289,7 +1289,7 @@ tree_mod_log_rewind(struct btrfs_fs_info *fs_info, struct btrfs_path *path,
 		return eb;
 
 	btrfs_set_path_blocking(path);
-	btrfs_set_lock_blocking_rw(eb, BTRFS_READ_LOCK);
+	btrfs_set_lock_blocking_read(eb);
 
 	if (tm->op == MOD_LOG_KEY_REMOVE_WHILE_FREEING) {
 		BUG_ON(tm->slot != 0);
@@ -1379,7 +1379,7 @@ get_old_root(struct btrfs_root *root, u64 time_seq)
 		free_extent_buffer(eb_root);
 		eb = alloc_dummy_extent_buffer(fs_info, logical);
 	} else {
-		btrfs_set_lock_blocking_rw(eb_root, BTRFS_READ_LOCK);
+		btrfs_set_lock_blocking_read(eb_root);
 		eb = btrfs_clone_extent_buffer(eb_root);
 		btrfs_tree_read_unlock_blocking(eb_root);
 		free_extent_buffer(eb_root);

commit f616f5cd9da7fceb7d884812da380b26040cd083
Author: Qu Wenruo <wqu@suse.com>
Date:   Wed Jan 23 15:15:17 2019 +0800

    btrfs: qgroup: Use delayed subtree rescan for balance
    
    Before this patch, qgroup code traces the whole subtree of subvolume and
    reloc trees unconditionally.
    
    This makes qgroup numbers consistent, but it could cause tons of
    unnecessary extent tracing, which causes a lot of overhead.
    
    However for subtree swap of balance, just swap both subtrees because
    they contain the same contents and tree structure, so qgroup numbers
    won't change.
    
    It's the race window between subtree swap and transaction commit could
    cause qgroup number change.
    
    This patch will delay the qgroup subtree scan until COW happens for the
    subtree root.
    
    So if there is no other operations for the fs, balance won't cause extra
    qgroup overhead. (best case scenario)
    Depending on the workload, most of the subtree scan can still be
    avoided.
    
    Only for worst case scenario, it will fall back to old subtree swap
    overhead. (scan all swapped subtrees)
    
    [[Benchmark]]
    Hardware:
            VM 4G vRAM, 8 vCPUs,
            disk is using 'unsafe' cache mode,
            backing device is SAMSUNG 850 evo SSD.
            Host has 16G ram.
    
    Mkfs parameter:
            --nodesize 4K (To bump up tree size)
    
    Initial subvolume contents:
            4G data copied from /usr and /lib.
            (With enough regular small files)
    
    Snapshots:
            16 snapshots of the original subvolume.
            each snapshot has 3 random files modified.
    
    balance parameter:
            -m
    
    So the content should be pretty similar to a real world root fs layout.
    
    And after file system population, there is no other activity, so it
    should be the best case scenario.
    
                         | v4.20-rc1            | w/ patchset    | diff
    -----------------------------------------------------------------------
    relocated extents    | 22615                | 22457          | -0.1%
    qgroup dirty extents | 163457               | 121606         | -25.6%
    time (sys)           | 22.884s              | 18.842s        | -17.6%
    time (real)          | 27.724s              | 22.884s        | -17.5%
    
    Signed-off-by: Qu Wenruo <wqu@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 5a6c39b44c84..0f2c20e0b108 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -13,6 +13,7 @@
 #include "print-tree.h"
 #include "locking.h"
 #include "volumes.h"
+#include "qgroup.h"
 
 static int split_node(struct btrfs_trans_handle *trans, struct btrfs_root
 		      *root, struct btrfs_path *path, int level);
@@ -1489,6 +1490,13 @@ noinline int btrfs_cow_block(struct btrfs_trans_handle *trans,
 		btrfs_set_lock_blocking(parent);
 	btrfs_set_lock_blocking(buf);
 
+	/*
+	 * Before CoWing this block for later modification, check if it's
+	 * the subtree root and do the delayed subtree trace if needed.
+	 *
+	 * Also We don't care about the error, as it's handled internally.
+	 */
+	btrfs_qgroup_trace_subtree_after_cow(trans, root, buf);
 	ret = __btrfs_cow_block(trans, root, buf, parent,
 				 parent_slot, cow_ret, search_start, 0);
 

commit a6279470762c19ba97e454f90798373dccdf6148
Author: Filipe Manana <fdmanana@suse.com>
Date:   Fri Jan 25 11:48:51 2019 +0000

    Btrfs: fix deadlock when allocating tree block during leaf/node split
    
    When splitting a leaf or node from one of the trees that are modified when
    flushing pending block groups (extent, chunk, device and free space trees),
    we need to allocate a new tree block, which in turn can result in the need
    to allocate a new block group. After allocating the new block group we may
    need to flush new block groups that were previously allocated during the
    course of the current transaction, which is what may cause a deadlock due
    to attempts to write lock twice the same leaf or node, as when splitting
    a leaf or node we are holding a write lock on it and its parent node.
    
    The same type of deadlock can also happen when increasing the tree's
    height, since we are holding a lock on the existing root while allocating
    the tree block to use as the new root node.
    
    An example trace when the deadlock happens during the leaf split path is:
    
      [27175.293054] CPU: 0 PID: 3005 Comm: kworker/u17:6 Tainted: G        W         4.19.16 #1
      [27175.293942] Hardware name: Penguin Computing Relion 1900/MD90-FS0-ZB-XX, BIOS R15 06/25/2018
      [27175.294846] Workqueue: btrfs-extent-refs btrfs_extent_refs_helper [btrfs]
      (...)
      [27175.298384] RSP: 0018:ffffab2087107758 EFLAGS: 00010246
      [27175.299269] RAX: 0000000000000bbd RBX: ffff9fadc7141c48 RCX: 0000000000000001
      [27175.300155] RDX: 0000000000000001 RSI: 0000000000000002 RDI: ffff9fadc7141c48
      [27175.301023] RBP: 0000000000000001 R08: ffff9faeb6ac1040 R09: ffff9fa9c0000000
      [27175.301887] R10: 0000000000000000 R11: 0000000000000040 R12: ffff9fb21aac8000
      [27175.302743] R13: ffff9fb1a64d6a20 R14: 0000000000000001 R15: ffff9fb1a64d6a18
      [27175.303601] FS:  0000000000000000(0000) GS:ffff9fb21fa00000(0000) knlGS:0000000000000000
      [27175.304468] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
      [27175.305339] CR2: 00007fdc8743ead8 CR3: 0000000763e0a006 CR4: 00000000003606f0
      [27175.306220] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
      [27175.307087] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
      [27175.307940] Call Trace:
      [27175.308802]  btrfs_search_slot+0x779/0x9a0 [btrfs]
      [27175.309669]  ? update_space_info+0xba/0xe0 [btrfs]
      [27175.310534]  btrfs_insert_empty_items+0x67/0xc0 [btrfs]
      [27175.311397]  btrfs_insert_item+0x60/0xd0 [btrfs]
      [27175.312253]  btrfs_create_pending_block_groups+0xee/0x210 [btrfs]
      [27175.313116]  do_chunk_alloc+0x25f/0x300 [btrfs]
      [27175.313984]  find_free_extent+0x706/0x10d0 [btrfs]
      [27175.314855]  btrfs_reserve_extent+0x9b/0x1d0 [btrfs]
      [27175.315707]  btrfs_alloc_tree_block+0x100/0x5b0 [btrfs]
      [27175.316548]  split_leaf+0x130/0x610 [btrfs]
      [27175.317390]  btrfs_search_slot+0x94d/0x9a0 [btrfs]
      [27175.318235]  btrfs_insert_empty_items+0x67/0xc0 [btrfs]
      [27175.319087]  alloc_reserved_file_extent+0x84/0x2c0 [btrfs]
      [27175.319938]  __btrfs_run_delayed_refs+0x596/0x1150 [btrfs]
      [27175.320792]  btrfs_run_delayed_refs+0xed/0x1b0 [btrfs]
      [27175.321643]  delayed_ref_async_start+0x81/0x90 [btrfs]
      [27175.322491]  normal_work_helper+0xd0/0x320 [btrfs]
      [27175.323328]  ? move_linked_works+0x6e/0xa0
      [27175.324160]  process_one_work+0x191/0x370
      [27175.324976]  worker_thread+0x4f/0x3b0
      [27175.325763]  kthread+0xf8/0x130
      [27175.326531]  ? rescuer_thread+0x320/0x320
      [27175.327284]  ? kthread_create_worker_on_cpu+0x50/0x50
      [27175.328027]  ret_from_fork+0x35/0x40
      [27175.328741] ---[ end trace 300a1b9f0ac30e26 ]---
    
    Fix this by preventing the flushing of new blocks groups when splitting a
    leaf/node and when inserting a new root node for one of the trees modified
    by the flushing operation, similar to what is done when COWing a node/leaf
    from on of these trees.
    
    Bugzilla: https://bugzilla.kernel.org/show_bug.cgi?id=202383
    Reported-by: Eli V <eliventer@gmail.com>
    CC: stable@vger.kernel.org # 4.4+
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index f64aad613727..5a6c39b44c84 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -968,6 +968,48 @@ static noinline int update_ref_for_cow(struct btrfs_trans_handle *trans,
 	return 0;
 }
 
+static struct extent_buffer *alloc_tree_block_no_bg_flush(
+					  struct btrfs_trans_handle *trans,
+					  struct btrfs_root *root,
+					  u64 parent_start,
+					  const struct btrfs_disk_key *disk_key,
+					  int level,
+					  u64 hint,
+					  u64 empty_size)
+{
+	struct btrfs_fs_info *fs_info = root->fs_info;
+	struct extent_buffer *ret;
+
+	/*
+	 * If we are COWing a node/leaf from the extent, chunk, device or free
+	 * space trees, make sure that we do not finish block group creation of
+	 * pending block groups. We do this to avoid a deadlock.
+	 * COWing can result in allocation of a new chunk, and flushing pending
+	 * block groups (btrfs_create_pending_block_groups()) can be triggered
+	 * when finishing allocation of a new chunk. Creation of a pending block
+	 * group modifies the extent, chunk, device and free space trees,
+	 * therefore we could deadlock with ourselves since we are holding a
+	 * lock on an extent buffer that btrfs_create_pending_block_groups() may
+	 * try to COW later.
+	 * For similar reasons, we also need to delay flushing pending block
+	 * groups when splitting a leaf or node, from one of those trees, since
+	 * we are holding a write lock on it and its parent or when inserting a
+	 * new root node for one of those trees.
+	 */
+	if (root == fs_info->extent_root ||
+	    root == fs_info->chunk_root ||
+	    root == fs_info->dev_root ||
+	    root == fs_info->free_space_root)
+		trans->can_flush_pending_bgs = false;
+
+	ret = btrfs_alloc_tree_block(trans, root, parent_start,
+				     root->root_key.objectid, disk_key, level,
+				     hint, empty_size);
+	trans->can_flush_pending_bgs = true;
+
+	return ret;
+}
+
 /*
  * does the dirty work in cow of a single block.  The parent block (if
  * supplied) is updated to point to the new cow copy.  The new buffer is marked
@@ -1015,28 +1057,8 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 	if ((root->root_key.objectid == BTRFS_TREE_RELOC_OBJECTID) && parent)
 		parent_start = parent->start;
 
-	/*
-	 * If we are COWing a node/leaf from the extent, chunk, device or free
-	 * space trees, make sure that we do not finish block group creation of
-	 * pending block groups. We do this to avoid a deadlock.
-	 * COWing can result in allocation of a new chunk, and flushing pending
-	 * block groups (btrfs_create_pending_block_groups()) can be triggered
-	 * when finishing allocation of a new chunk. Creation of a pending block
-	 * group modifies the extent, chunk, device and free space trees,
-	 * therefore we could deadlock with ourselves since we are holding a
-	 * lock on an extent buffer that btrfs_create_pending_block_groups() may
-	 * try to COW later.
-	 */
-	if (root == fs_info->extent_root ||
-	    root == fs_info->chunk_root ||
-	    root == fs_info->dev_root ||
-	    root == fs_info->free_space_root)
-		trans->can_flush_pending_bgs = false;
-
-	cow = btrfs_alloc_tree_block(trans, root, parent_start,
-			root->root_key.objectid, &disk_key, level,
-			search_start, empty_size);
-	trans->can_flush_pending_bgs = true;
+	cow = alloc_tree_block_no_bg_flush(trans, root, parent_start, &disk_key,
+					   level, search_start, empty_size);
 	if (IS_ERR(cow))
 		return PTR_ERR(cow);
 
@@ -3345,8 +3367,8 @@ static noinline int insert_new_root(struct btrfs_trans_handle *trans,
 	else
 		btrfs_node_key(lower, &lower_key, 0);
 
-	c = btrfs_alloc_tree_block(trans, root, 0, root->root_key.objectid,
-				   &lower_key, level, root->node->start, 0);
+	c = alloc_tree_block_no_bg_flush(trans, root, 0, &lower_key, level,
+					 root->node->start, 0);
 	if (IS_ERR(c))
 		return PTR_ERR(c);
 
@@ -3475,8 +3497,8 @@ static noinline int split_node(struct btrfs_trans_handle *trans,
 	mid = (c_nritems + 1) / 2;
 	btrfs_node_key(c, &disk_key, mid);
 
-	split = btrfs_alloc_tree_block(trans, root, 0, root->root_key.objectid,
-			&disk_key, level, c->start, 0);
+	split = alloc_tree_block_no_bg_flush(trans, root, 0, &disk_key, level,
+					     c->start, 0);
 	if (IS_ERR(split))
 		return PTR_ERR(split);
 
@@ -4260,8 +4282,8 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 	else
 		btrfs_item_key(l, &disk_key, mid);
 
-	right = btrfs_alloc_tree_block(trans, root, 0, root->root_key.objectid,
-			&disk_key, 0, l->start, 0);
+	right = alloc_tree_block_no_bg_flush(trans, root, 0, &disk_key, 0,
+					     l->start, 0);
 	if (IS_ERR(right))
 		return PTR_ERR(right);
 

commit a6d8654d885d7d79a3fb82da64eaa489ca332a82
Author: Filipe Manana <fdmanana@suse.com>
Date:   Tue Jan 8 11:44:41 2019 +0000

    Btrfs: fix deadlock when using free space tree due to block group creation
    
    When modifying the free space tree we can end up COWing one of its extent
    buffers which in turn might result in allocating a new chunk, which in
    turn can result in flushing (finish creation) of pending block groups. If
    that happens we can deadlock because creating a pending block group needs
    to update the free space tree, and if any of the updates tries to modify
    the same extent buffer that we are COWing, we end up in a deadlock since
    we try to write lock twice the same extent buffer.
    
    So fix this by skipping pending block group creation if we are COWing an
    extent buffer from the free space tree. This is a case missed by commit
    5ce555578e091 ("Btrfs: fix deadlock when writing out free space caches").
    
    Bugzilla: https://bugzilla.kernel.org/show_bug.cgi?id=202173
    Fixes: 5ce555578e091 ("Btrfs: fix deadlock when writing out free space caches")
    CC: stable@vger.kernel.org # 4.18+
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index d92462fe66c8..f64aad613727 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1016,19 +1016,21 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 		parent_start = parent->start;
 
 	/*
-	 * If we are COWing a node/leaf from the extent, chunk or device trees,
-	 * make sure that we do not finish block group creation of pending block
-	 * groups. We do this to avoid a deadlock.
+	 * If we are COWing a node/leaf from the extent, chunk, device or free
+	 * space trees, make sure that we do not finish block group creation of
+	 * pending block groups. We do this to avoid a deadlock.
 	 * COWing can result in allocation of a new chunk, and flushing pending
 	 * block groups (btrfs_create_pending_block_groups()) can be triggered
 	 * when finishing allocation of a new chunk. Creation of a pending block
-	 * group modifies the extent, chunk and device trees, therefore we could
-	 * deadlock with ourselves since we are holding a lock on an extent
-	 * buffer that btrfs_create_pending_block_groups() may try to COW later.
+	 * group modifies the extent, chunk, device and free space trees,
+	 * therefore we could deadlock with ourselves since we are holding a
+	 * lock on an extent buffer that btrfs_create_pending_block_groups() may
+	 * try to COW later.
 	 */
 	if (root == fs_info->extent_root ||
 	    root == fs_info->chunk_root ||
-	    root == fs_info->dev_root)
+	    root == fs_info->dev_root ||
+	    root == fs_info->free_space_root)
 		trans->can_flush_pending_bgs = false;
 
 	cow = btrfs_alloc_tree_block(trans, root, parent_start,

commit 52042d8e82ff50d40e76a275ac0b97aa663328b0
Author: Andrea Gelmini <andrea.gelmini@gelma.net>
Date:   Wed Nov 28 12:05:13 2018 +0100

    btrfs: Fix typos in comments and strings
    
    The typos accumulate over time so once in a while time they get fixed in
    a large patch.
    
    Signed-off-by: Andrea Gelmini <andrea.gelmini@gelma.net>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 4252e89df6ae..d92462fe66c8 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1414,7 +1414,7 @@ static inline int should_cow_block(struct btrfs_trans_handle *trans,
 	 *
 	 * What is forced COW:
 	 *    when we create snapshot during committing the transaction,
-	 *    after we've finished coping src root, we must COW the shared
+	 *    after we've finished copying src root, we must COW the shared
 	 *    block to ensure the metadata consistency.
 	 */
 	if (btrfs_header_generation(buf) == trans->transid &&
@@ -3771,7 +3771,7 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 		/* Key greater than all keys in the leaf, right neighbor has
 		 * enough room for it and we're not emptying our leaf to delete
 		 * it, therefore use right neighbor to insert the new item and
-		 * no need to touch/dirty our left leaft. */
+		 * no need to touch/dirty our left leaf. */
 		btrfs_tree_unlock(left);
 		free_extent_buffer(left);
 		path->nodes[0] = right;

commit be6821f82c3cc36e026f5afd10249988852b35ea
Author: Filipe Manana <fdmanana@suse.com>
Date:   Tue Dec 11 10:19:45 2018 +0000

    Btrfs: send, fix race with transaction commits that create snapshots
    
    If we create a snapshot of a snapshot currently being used by a send
    operation, we can end up with send failing unexpectedly (returning
    -ENOENT error to user space for example). The following diagram shows
    how this happens.
    
                CPU 1                                   CPU2                                CPU3
    
     btrfs_ioctl_send()
      (...)
                                         create_snapshot()
                                          -> creates snapshot of a
                                             root used by the send
                                             task
                                          btrfs_commit_transaction()
                                           create_pending_snapshot()
      __get_inode_info()
       btrfs_search_slot()
        btrfs_search_slot_get_root()
         down_read commit_root_sem
    
         get reference on eb of the
         commit root
          -> eb with bytenr == X
    
         up_read commit_root_sem
    
                                            btrfs_cow_block(root node)
                                             btrfs_free_tree_block()
                                              -> creates delayed ref to
                                                 free the extent
    
                                           btrfs_run_delayed_refs()
                                            -> runs the delayed ref,
                                               adds extent to
                                               fs_info->pinned_extents
    
                                           btrfs_finish_extent_commit()
                                            unpin_extent_range()
                                             -> marks extent as free
                                                in the free space cache
    
                                          transaction commit finishes
    
                                                                           btrfs_start_transaction()
                                                                            (...)
                                                                            btrfs_cow_block()
                                                                             btrfs_alloc_tree_block()
                                                                              btrfs_reserve_extent()
                                                                               -> allocates extent at
                                                                                  bytenr == X
                                                                              btrfs_init_new_buffer(bytenr X)
                                                                               btrfs_find_create_tree_block()
                                                                                alloc_extent_buffer(bytenr X)
                                                                                 find_extent_buffer(bytenr X)
                                                                                  -> returns existing eb,
                                                                                     which the send task got
    
                                                                            (...)
                                                                             -> modifies content of the
                                                                                eb with bytenr == X
    
        -> uses an eb that now
           belongs to some other
           tree and no more matches
           the commit root of the
           snapshot, resuts will be
           unpredictable
    
    The consequences of this race can be various, and can lead to searches in
    the commit root performed by the send task failing unexpectedly (unable to
    find inode items, returning -ENOENT to user space, for example) or not
    failing because an inode item with the same number was added to the tree
    that reused the metadata extent, in which case send can behave incorrectly
    in the worst case or just fail later for some reason.
    
    Fix this by performing a copy of the commit root's extent buffer when doing
    a search in the context of a send operation.
    
    CC: stable@vger.kernel.org # 4.4.x: 1fc28d8e2e9: Btrfs: move get root out of btrfs_search_slot to a helper
    CC: stable@vger.kernel.org # 4.4.x: f9ddfd0592a: Btrfs: remove unused check of skip_locking
    CC: stable@vger.kernel.org # 4.4.x
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 72745235896f..4252e89df6ae 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2587,14 +2587,27 @@ static struct extent_buffer *btrfs_search_slot_get_root(struct btrfs_root *root,
 	root_lock = BTRFS_READ_LOCK;
 
 	if (p->search_commit_root) {
-		/* The commit roots are read only so we always do read locks */
-		if (p->need_commit_sem)
+		/*
+		 * The commit roots are read only so we always do read locks,
+		 * and we always must hold the commit_root_sem when doing
+		 * searches on them, the only exception is send where we don't
+		 * want to block transaction commits for a long time, so
+		 * we need to clone the commit root in order to avoid races
+		 * with transaction commits that create a snapshot of one of
+		 * the roots used by a send operation.
+		 */
+		if (p->need_commit_sem) {
 			down_read(&fs_info->commit_root_sem);
-		b = root->commit_root;
-		extent_buffer_get(b);
-		level = btrfs_header_level(b);
-		if (p->need_commit_sem)
+			b = btrfs_clone_extent_buffer(root->commit_root);
 			up_read(&fs_info->commit_root_sem);
+			if (!b)
+				return ERR_PTR(-ENOMEM);
+
+		} else {
+			b = root->commit_root;
+			extent_buffer_get(b);
+		}
+		level = btrfs_header_level(b);
 		/*
 		 * Ensure that all callers have set skip_locking when
 		 * p->search_commit_root = 1.
@@ -2720,6 +2733,10 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 again:
 	prev_cmp = -1;
 	b = btrfs_search_slot_get_root(root, p, write_lock_level);
+	if (IS_ERR(b)) {
+		ret = PTR_ERR(b);
+		goto done;
+	}
 
 	while (b) {
 		level = btrfs_header_level(b);

commit 83354f0772cd07828b0fcdf1ef2872fd2032acb8
Author: Josef Bacik <jbacik@fb.com>
Date:   Fri Nov 30 11:52:13 2018 -0500

    btrfs: catch cow on deleting snapshots
    
    When debugging some weird extent reference bug I suspected that we were
    changing a snapshot while we were deleting it, which could explain my
    bug.  This was indeed what was happening, and this patch helped me
    verify my theory.  It is never correct to modify the snapshot once it's
    being deleted, so mark the root when we are deleting it and make sure we
    complain about it when it happens.
    
    Reviewed-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: Josef Bacik <josef@toxicpanda.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 5912a97b07a6..72745235896f 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1440,6 +1440,10 @@ noinline int btrfs_cow_block(struct btrfs_trans_handle *trans,
 	u64 search_start;
 	int ret;
 
+	if (test_bit(BTRFS_ROOT_DELETING, &root->state))
+		btrfs_err(fs_info,
+			"COW'ing blocks on a fs root that's being dropped");
+
 	if (trans->transaction != fs_info->running_transaction)
 		WARN(1, KERN_CRIT "trans %llu running %llu\n",
 		       trans->transid,

commit de37aa513105f864d3c21105bf5542d498f21ca2
Author: Nikolay Borisov <nborisov@suse.com>
Date:   Tue Oct 30 16:43:24 2018 +0200

    btrfs: Remove fsid/metadata_fsid fields from btrfs_info
    
    Currently btrfs_fs_info structure contains a copy of the
    fsid/metadata_uuid fields. Same values are also contained in the
    btrfs_fs_devices structure which fs_info has a reference to. Let's
    reduce duplication by removing the fields from fs_info and always refer
    to the ones in fs_devices. No functional changes.
    
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 3b7d80add475..5912a97b07a6 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -12,6 +12,7 @@
 #include "transaction.h"
 #include "print-tree.h"
 #include "locking.h"
+#include "volumes.h"
 
 static int split_node(struct btrfs_trans_handle *trans, struct btrfs_root
 		      *root, struct btrfs_path *path, int level);
@@ -224,7 +225,7 @@ int btrfs_copy_root(struct btrfs_trans_handle *trans,
 	else
 		btrfs_set_header_owner(cow, new_root_objectid);
 
-	write_extent_buffer_fsid(cow, fs_info->metadata_fsid);
+	write_extent_buffer_fsid(cow, fs_info->fs_devices->metadata_uuid);
 
 	WARN_ON(btrfs_header_generation(buf) > trans->transid);
 	if (new_root_objectid == BTRFS_TREE_RELOC_OBJECTID)
@@ -1050,7 +1051,7 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 	else
 		btrfs_set_header_owner(cow, root->root_key.objectid);
 
-	write_extent_buffer_fsid(cow, fs_info->metadata_fsid);
+	write_extent_buffer_fsid(cow, fs_info->fs_devices->metadata_uuid);
 
 	ret = update_ref_for_cow(trans, root, buf, cow, &last_ref);
 	if (ret) {

commit 7239ff4b2be8ec0c3160da7fdd1475785fdb4cb9
Author: Nikolay Borisov <nborisov@suse.com>
Date:   Tue Oct 30 16:43:23 2018 +0200

    btrfs: Introduce support for FSID change without metadata rewrite
    
    This field is going to be used when the user wants to change the UUID
    of the filesystem without having to rewrite all metadata blocks. This
    field adds another level of indirection such that when the FSID is
    changed what really happens is the current UUID (the one with which the
    fs was created) is copied to the 'metadata_uuid' field in the superblock
    as well as a new incompat flag is set METADATA_UUID. When the kernel
    detects this flag is set it knows that the superblock in fact has 2
    UUIDs:
    
    1. Is the UUID which is user-visible, currently known as FSID.
    2. Metadata UUID - this is the UUID which is stamped into all on-disk
       datastructures belonging to this file system.
    
    When the new incompat flag is present device scanning checks whether
    both fsid/metadata_uuid of the scanned device match any of the
    registered filesystems. When the flag is not set then both UUIDs are
    equal and only the FSID is retained on disk, metadata_uuid is set only
    in-memory during mount.
    
    Additionally a new metadata_uuid field is also added to the fs_info
    struct. It's initialised either with the FSID in case METADATA_UUID
    incompat flag is not set or with the metdata_uuid of the superblock
    otherwise.
    
    This commit introduces the new fields as well as the new incompat flag
    and switches all users of the fsid to the new logic.
    
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    [ minor updates in comments ]
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index a436259c71b1..3b7d80add475 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -224,7 +224,7 @@ int btrfs_copy_root(struct btrfs_trans_handle *trans,
 	else
 		btrfs_set_header_owner(cow, new_root_objectid);
 
-	write_extent_buffer_fsid(cow, fs_info->fsid);
+	write_extent_buffer_fsid(cow, fs_info->metadata_fsid);
 
 	WARN_ON(btrfs_header_generation(buf) > trans->transid);
 	if (new_root_objectid == BTRFS_TREE_RELOC_OBJECTID)
@@ -1050,7 +1050,7 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 	else
 		btrfs_set_header_owner(cow, root->root_key.objectid);
 
-	write_extent_buffer_fsid(cow, fs_info->fsid);
+	write_extent_buffer_fsid(cow, fs_info->metadata_fsid);
 
 	ret = update_ref_for_cow(trans, root, buf, cow, &last_ref);
 	if (ret) {

commit 8c7eeb6557877e3272f3105955ca103ee351f16d
Author: Nikolay Borisov <nborisov@suse.com>
Date:   Wed Aug 15 18:26:55 2018 +0300

    btrfs: Remove extra reference count bumps in btrfs_compare_trees
    
    When the 2 comparison trees roots are initialised they are private to
    the function and already have reference counts of 1 each. There is no
    need to further increment the reference count since the cloned buffers
    are already accessed via struct btrfs_path. Eventually the 2 paths used
    for comparison are going to be released, effectively disposing of the
    cloned buffers.
    
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index c1afcc269313..a436259c71b1 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -5388,7 +5388,6 @@ int btrfs_compare_trees(struct btrfs_root *left_root,
 		ret = -ENOMEM;
 		goto out;
 	}
-	extent_buffer_get(left_path->nodes[left_level]);
 
 	right_level = btrfs_header_level(right_root->commit_root);
 	right_root_level = right_level;
@@ -5399,7 +5398,6 @@ int btrfs_compare_trees(struct btrfs_root *left_root,
 		ret = -ENOMEM;
 		goto out;
 	}
-	extent_buffer_get(right_path->nodes[right_level]);
 	up_read(&fs_info->commit_root_sem);
 
 	if (left_level == 0)

commit 24cee18a1c1d7c731ea5987e0c99daea22ae7f4a
Author: Nikolay Borisov <nborisov@suse.com>
Date:   Wed Aug 15 18:26:54 2018 +0300

    btrfs: Remove extraneous extent_buffer_get from tree_mod_log_rewind
    
    When a rewound buffer is created it already has a ref count of 1 and the
    dummy flag set. Then another ref is taken bumping the count to 2.
    Finally when this buffer is released from btrfs_release_path the extra
    reference is decremented by the special handling code in
    free_extent_buffer.
    
    However, this special code is in fact redundant sinca ref count of 1 is
    still correct since the buffer is only accessed via btrfs_path struct.
    This paves the way forward of removing the special handling in
    free_extent_buffer.
    
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 0c5ece3fe741..c1afcc269313 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1290,7 +1290,6 @@ tree_mod_log_rewind(struct btrfs_fs_info *fs_info, struct btrfs_path *path,
 	btrfs_tree_read_unlock_blocking(eb);
 	free_extent_buffer(eb);
 
-	extent_buffer_get(eb_rewin);
 	btrfs_tree_read_lock(eb_rewin);
 	__tree_mod_log_rewind(fs_info, eb_rewin, time_seq, tm);
 	WARN_ON(btrfs_header_nritems(eb_rewin) >

commit 6c122e2a0c515cfb3f3a9cefb5dad4cb62109c78
Author: Nikolay Borisov <nborisov@suse.com>
Date:   Wed Aug 15 18:26:53 2018 +0300

    btrfs: Remove redundant extent_buffer_get in get_old_root
    
    get_old_root used used only by btrfs_search_old_slot to initialise the
    path structure. The old root is always a cloned buffer (either via alloc
    dummy or via btrfs_clone_extent_buffer) and its reference count is 2: 1
    from allocation, 1 from extent_buffer_get call in get_old_root.
    
    This latter explicit ref count acquire operation is in fact unnecessary
    since the semantic is such that the newly allocated buffer is handed
    over to the btrfs_path for lifetime management. Considering this just
    remove the extra extent_buffer_get in get_old_root.
    
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 539901fb5165..0c5ece3fe741 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1362,7 +1362,6 @@ get_old_root(struct btrfs_root *root, u64 time_seq)
 
 	if (!eb)
 		return NULL;
-	extent_buffer_get(eb);
 	btrfs_tree_read_lock(eb);
 	if (old_root) {
 		btrfs_set_header_bytenr(eb, eb->start);

commit 5ce555578e0919237fa4bda92b4670e2dd176f85
Author: Filipe Manana <fdmanana@suse.com>
Date:   Fri Oct 12 10:03:55 2018 +0100

    Btrfs: fix deadlock when writing out free space caches
    
    When writing out a block group free space cache we can end deadlocking
    with ourselves on an extent buffer lock resulting in a warning like the
    following:
    
      [245043.379979] WARNING: CPU: 4 PID: 2608 at fs/btrfs/locking.c:251 btrfs_tree_lock+0x1be/0x1d0 [btrfs]
      [245043.392792] CPU: 4 PID: 2608 Comm: btrfs-transacti Tainted: G
        W I      4.16.8 #1
      [245043.395489] RIP: 0010:btrfs_tree_lock+0x1be/0x1d0 [btrfs]
      [245043.396791] RSP: 0018:ffffc9000424b840 EFLAGS: 00010246
      [245043.398093] RAX: 0000000000000a30 RBX: ffff8807e20a3d20 RCX: 0000000000000001
      [245043.399414] RDX: 0000000000000001 RSI: 0000000000000002 RDI: ffff8807e20a3d20
      [245043.400732] RBP: 0000000000000001 R08: ffff88041f39a700 R09: ffff880000000000
      [245043.402021] R10: 0000000000000040 R11: ffff8807e20a3d20 R12: ffff8807cb220630
      [245043.403296] R13: 0000000000000001 R14: ffff8807cb220628 R15: ffff88041fbdf000
      [245043.404780] FS:  0000000000000000(0000) GS:ffff88082fc80000(0000) knlGS:0000000000000000
      [245043.406050] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
      [245043.407321] CR2: 00007fffdbdb9f10 CR3: 0000000001c09005 CR4: 00000000000206e0
      [245043.408670] Call Trace:
      [245043.409977]  btrfs_search_slot+0x761/0xa60 [btrfs]
      [245043.411278]  btrfs_insert_empty_items+0x62/0xb0 [btrfs]
      [245043.412572]  btrfs_insert_item+0x5b/0xc0 [btrfs]
      [245043.413922]  btrfs_create_pending_block_groups+0xfb/0x1e0 [btrfs]
      [245043.415216]  do_chunk_alloc+0x1e5/0x2a0 [btrfs]
      [245043.416487]  find_free_extent+0xcd0/0xf60 [btrfs]
      [245043.417813]  btrfs_reserve_extent+0x96/0x1e0 [btrfs]
      [245043.419105]  btrfs_alloc_tree_block+0xfb/0x4a0 [btrfs]
      [245043.420378]  __btrfs_cow_block+0x127/0x550 [btrfs]
      [245043.421652]  btrfs_cow_block+0xee/0x190 [btrfs]
      [245043.422979]  btrfs_search_slot+0x227/0xa60 [btrfs]
      [245043.424279]  ? btrfs_update_inode_item+0x59/0x100 [btrfs]
      [245043.425538]  ? iput+0x72/0x1e0
      [245043.426798]  write_one_cache_group.isra.49+0x20/0x90 [btrfs]
      [245043.428131]  btrfs_start_dirty_block_groups+0x102/0x420 [btrfs]
      [245043.429419]  btrfs_commit_transaction+0x11b/0x880 [btrfs]
      [245043.430712]  ? start_transaction+0x8e/0x410 [btrfs]
      [245043.432006]  transaction_kthread+0x184/0x1a0 [btrfs]
      [245043.433341]  kthread+0xf0/0x130
      [245043.434628]  ? btrfs_cleanup_transaction+0x4e0/0x4e0 [btrfs]
      [245043.435928]  ? kthread_create_worker_on_cpu+0x40/0x40
      [245043.437236]  ret_from_fork+0x1f/0x30
      [245043.441054] ---[ end trace 15abaa2aaf36827f ]---
    
    This is because at write_one_cache_group() when we are COWing a leaf from
    the extent tree we end up allocating a new block group (chunk) and,
    because we have hit a threshold on the number of bytes reserved for system
    chunks, we attempt to finalize the creation of new block groups from the
    current transaction, by calling btrfs_create_pending_block_groups().
    However here we also need to modify the extent tree in order to insert
    a block group item, and if the location for this new block group item
    happens to be in the same leaf that we were COWing earlier, we deadlock
    since btrfs_search_slot() tries to write lock the extent buffer that we
    locked before at write_one_cache_group().
    
    We have already hit similar cases in the past and commit d9a0540a79f8
    ("Btrfs: fix deadlock when finalizing block group creation") fixed some
    of those cases by delaying the creation of pending block groups at the
    known specific spots that could lead to a deadlock. This change reworks
    that commit to be more generic so that we don't have to add similar logic
    to every possible path that can lead to a deadlock. This is done by
    making __btrfs_cow_block() disallowing the creation of new block groups
    (setting the transaction's can_flush_pending_bgs to false) before it
    attempts to allocate a new extent buffer for either the extent, chunk or
    device trees, since those are the trees that pending block creation
    modifies. Once the new extent buffer is allocated, it allows creation of
    pending block groups to happen again.
    
    This change depends on a recent patch from Josef which is not yet in
    Linus' tree, named "btrfs: make sure we create all new block groups" in
    order to avoid occasional warnings at btrfs_trans_release_chunk_metadata().
    
    Fixes: d9a0540a79f8 ("Btrfs: fix deadlock when finalizing block group creation")
    CC: stable@vger.kernel.org # 4.4+
    Bugzilla: https://bugzilla.kernel.org/show_bug.cgi?id=199753
    Link: https://lore.kernel.org/linux-btrfs/CAJtFHUTHna09ST-_EEiyWmDH6gAqS6wa=zMNMBsifj8ABu99cw@mail.gmail.com/
    Reported-by: E V <eliventer@gmail.com>
    Reviewed-by: Josef Bacik <josef@toxicpanda.com>
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 2ee43b6a4f09..539901fb5165 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1014,9 +1014,26 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 	if ((root->root_key.objectid == BTRFS_TREE_RELOC_OBJECTID) && parent)
 		parent_start = parent->start;
 
+	/*
+	 * If we are COWing a node/leaf from the extent, chunk or device trees,
+	 * make sure that we do not finish block group creation of pending block
+	 * groups. We do this to avoid a deadlock.
+	 * COWing can result in allocation of a new chunk, and flushing pending
+	 * block groups (btrfs_create_pending_block_groups()) can be triggered
+	 * when finishing allocation of a new chunk. Creation of a pending block
+	 * group modifies the extent, chunk and device trees, therefore we could
+	 * deadlock with ourselves since we are holding a lock on an extent
+	 * buffer that btrfs_create_pending_block_groups() may try to COW later.
+	 */
+	if (root == fs_info->extent_root ||
+	    root == fs_info->chunk_root ||
+	    root == fs_info->dev_root)
+		trans->can_flush_pending_bgs = false;
+
 	cow = btrfs_alloc_tree_block(trans, root, parent_start,
 			root->root_key.objectid, &disk_key, level,
 			search_start, empty_size);
+	trans->can_flush_pending_bgs = true;
 	if (IS_ERR(cow))
 		return PTR_ERR(cow);
 

commit 523983401644ebeb331c923c28c9591c07430a7d
Author: Liu Bo <bo.liu@linux.alibaba.com>
Date:   Wed Aug 22 05:54:37 2018 +0800

    Btrfs: kill btrfs_clear_path_blocking
    
    Btrfs's btree locking has two modes, spinning mode and blocking mode,
    while searching btree, locking is always acquired in spinning mode and
    then converted to blocking mode if necessary, and in some hot paths we may
    switch the locking back to spinning mode by btrfs_clear_path_blocking().
    
    When acquiring locks, both of reader and writer need to wait for blocking
    readers and writers to complete before doing read_lock()/write_lock().
    
    The problem is that btrfs_clear_path_blocking() needs to switch nodes
    in the path to blocking mode at first (by btrfs_set_path_blocking) to
    make lockdep happy before doing its actual clearing blocking job.
    
    When switching to blocking mode from spinning mode, it consists of
    
    step 1) bumping up blocking readers counter and
    step 2) read_unlock()/write_unlock(),
    
    this has caused serious ping-pong effect if there're a great amount of
    concurrent readers/writers, as waiters will be woken up and go to
    sleep immediately.
    
    1) Killing this kind of ping-pong results in a big improvement in my 1600k
    files creation script,
    
    MNT=/mnt/btrfs
    mkfs.btrfs -f /dev/sdf
    mount /dev/def $MNT
    time fsmark  -D  10000  -S0  -n  100000  -s  0  -L  1 -l /tmp/fs_log.txt \
            -d  $MNT/0  -d  $MNT/1 \
            -d  $MNT/2  -d  $MNT/3 \
            -d  $MNT/4  -d  $MNT/5 \
            -d  $MNT/6  -d  $MNT/7 \
            -d  $MNT/8  -d  $MNT/9 \
            -d  $MNT/10  -d  $MNT/11 \
            -d  $MNT/12  -d  $MNT/13 \
            -d  $MNT/14  -d  $MNT/15
    
    w/o patch:
    real    2m27.307s
    user    0m12.839s
    sys     13m42.831s
    
    w/ patch:
    real    1m2.273s
    user    0m15.802s
    sys     8m16.495s
    
    1.1) latency histogram from funclatency[1]
    
    Overall with the patch, there're ~50% less write lock acquisition and
    the 95% max latency that write lock takes also reduces to ~100ms from
    >500ms.
    
    --------------------------------------------
    w/o patch:
    --------------------------------------------
    Function = btrfs_tree_lock
         msecs               : count     distribution
             0 -> 1          : 2385222  |****************************************|
             2 -> 3          : 37147    |                                        |
             4 -> 7          : 20452    |                                        |
             8 -> 15         : 13131    |                                        |
            16 -> 31         : 3877     |                                        |
            32 -> 63         : 3900     |                                        |
            64 -> 127        : 2612     |                                        |
           128 -> 255        : 974      |                                        |
           256 -> 511        : 165      |                                        |
           512 -> 1023       : 13       |                                        |
    
    Function = btrfs_tree_read_lock
         msecs               : count     distribution
             0 -> 1          : 6743860  |****************************************|
             2 -> 3          : 2146     |                                        |
             4 -> 7          : 190      |                                        |
             8 -> 15         : 38       |                                        |
            16 -> 31         : 4        |                                        |
    
    --------------------------------------------
    w/ patch:
    --------------------------------------------
    Function = btrfs_tree_lock
         msecs               : count     distribution
             0 -> 1          : 1318454  |****************************************|
             2 -> 3          : 6800     |                                        |
             4 -> 7          : 3664     |                                        |
             8 -> 15         : 2145     |                                        |
            16 -> 31         : 809      |                                        |
            32 -> 63         : 219      |                                        |
            64 -> 127        : 10       |                                        |
    
    Function = btrfs_tree_read_lock
         msecs               : count     distribution
             0 -> 1          : 6854317  |****************************************|
             2 -> 3          : 2383     |                                        |
             4 -> 7          : 601      |                                        |
             8 -> 15         : 92       |                                        |
    
    2) dbench also proves the improvement,
    dbench -t 120 -D /mnt/btrfs 16
    
    w/o patch:
    Throughput 158.363 MB/sec
    
    w/ patch:
    Throughput 449.52 MB/sec
    
    3) xfstests didn't show any additional failures.
    
    One thing to note is that callers may set path->leave_spinning to have
    all nodes in the path stay in spinning mode, which means callers are
    ready to not sleep before releasing the path, but it won't cause
    problems if they don't want to sleep in blocking mode.
    
    [1]: https://github.com/iovisor/bcc/blob/master/tools/funclatency.py
    
    Signed-off-by: Liu Bo <bo.liu@linux.alibaba.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 0a6c645fab0a..2ee43b6a4f09 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -52,42 +52,6 @@ noinline void btrfs_set_path_blocking(struct btrfs_path *p)
 	}
 }
 
-/*
- * reset all the locked nodes in the patch to spinning locks.
- *
- * held is used to keep lockdep happy, when lockdep is enabled
- * we set held to a blocking lock before we go around and
- * retake all the spinlocks in the path.  You can safely use NULL
- * for held
- */
-noinline void btrfs_clear_path_blocking(struct btrfs_path *p,
-					struct extent_buffer *held, int held_rw)
-{
-	int i;
-
-	if (held) {
-		btrfs_set_lock_blocking_rw(held, held_rw);
-		if (held_rw == BTRFS_WRITE_LOCK)
-			held_rw = BTRFS_WRITE_LOCK_BLOCKING;
-		else if (held_rw == BTRFS_READ_LOCK)
-			held_rw = BTRFS_READ_LOCK_BLOCKING;
-	}
-	btrfs_set_path_blocking(p);
-
-	for (i = BTRFS_MAX_LEVEL - 1; i >= 0; i--) {
-		if (p->nodes[i] && p->locks[i]) {
-			btrfs_clear_lock_blocking_rw(p->nodes[i], p->locks[i]);
-			if (p->locks[i] == BTRFS_WRITE_LOCK_BLOCKING)
-				p->locks[i] = BTRFS_WRITE_LOCK;
-			else if (p->locks[i] == BTRFS_READ_LOCK_BLOCKING)
-				p->locks[i] = BTRFS_READ_LOCK;
-		}
-	}
-
-	if (held)
-		btrfs_clear_lock_blocking_rw(held, held_rw);
-}
-
 /* this also releases the path */
 void btrfs_free_path(struct btrfs_path *p)
 {
@@ -1306,7 +1270,6 @@ tree_mod_log_rewind(struct btrfs_fs_info *fs_info, struct btrfs_path *path,
 		}
 	}
 
-	btrfs_clear_path_blocking(path, NULL, BTRFS_READ_LOCK);
 	btrfs_tree_read_unlock_blocking(eb);
 	free_extent_buffer(eb);
 
@@ -2482,7 +2445,6 @@ setup_nodes_for_search(struct btrfs_trans_handle *trans,
 		btrfs_set_path_blocking(p);
 		reada_for_balance(fs_info, p, level);
 		sret = split_node(trans, root, p, level);
-		btrfs_clear_path_blocking(p, NULL, 0);
 
 		BUG_ON(sret > 0);
 		if (sret) {
@@ -2503,7 +2465,6 @@ setup_nodes_for_search(struct btrfs_trans_handle *trans,
 		btrfs_set_path_blocking(p);
 		reada_for_balance(fs_info, p, level);
 		sret = balance_level(trans, root, p, level);
-		btrfs_clear_path_blocking(p, NULL, 0);
 
 		if (sret) {
 			ret = sret;
@@ -2788,7 +2749,10 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 		}
 cow_done:
 		p->nodes[level] = b;
-		btrfs_clear_path_blocking(p, NULL, 0);
+		/*
+		 * Leave path with blocking locks to avoid massive
+		 * lock context switch, this is made on purpose.
+		 */
 
 		/*
 		 * we have a lock on b and as long as we aren't changing
@@ -2870,8 +2834,6 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 					if (!err) {
 						btrfs_set_path_blocking(p);
 						btrfs_tree_lock(b);
-						btrfs_clear_path_blocking(p, b,
-								  BTRFS_WRITE_LOCK);
 					}
 					p->locks[level] = BTRFS_WRITE_LOCK;
 				} else {
@@ -2879,8 +2841,6 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 					if (!err) {
 						btrfs_set_path_blocking(p);
 						btrfs_tree_read_lock(b);
-						btrfs_clear_path_blocking(p, b,
-								  BTRFS_READ_LOCK);
 					}
 					p->locks[level] = BTRFS_READ_LOCK;
 				}
@@ -2899,7 +2859,6 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 				btrfs_set_path_blocking(p);
 				err = split_leaf(trans, root, key,
 						 p, ins_len, ret == 0);
-				btrfs_clear_path_blocking(p, NULL, 0);
 
 				BUG_ON(err > 0);
 				if (err) {
@@ -2970,7 +2929,6 @@ int btrfs_search_old_slot(struct btrfs_root *root, const struct btrfs_key *key,
 	while (b) {
 		level = btrfs_header_level(b);
 		p->nodes[level] = b;
-		btrfs_clear_path_blocking(p, NULL, 0);
 
 		/*
 		 * we have a lock on b and as long as we aren't changing
@@ -3016,8 +2974,6 @@ int btrfs_search_old_slot(struct btrfs_root *root, const struct btrfs_key *key,
 			if (!err) {
 				btrfs_set_path_blocking(p);
 				btrfs_tree_read_lock(b);
-				btrfs_clear_path_blocking(p, b,
-							  BTRFS_READ_LOCK);
 			}
 			b = tree_mod_log_rewind(fs_info, p, b, time_seq);
 			if (!b) {
@@ -5201,7 +5157,6 @@ int btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,
 		path->locks[level - 1] = BTRFS_READ_LOCK;
 		path->nodes[level - 1] = cur;
 		unlock_up(path, level, 1, 0, NULL);
-		btrfs_clear_path_blocking(path, NULL, 0);
 	}
 out:
 	path->keep_locks = keep_locks;
@@ -5786,8 +5741,6 @@ int btrfs_next_old_leaf(struct btrfs_root *root, struct btrfs_path *path,
 			if (!ret) {
 				btrfs_set_path_blocking(path);
 				btrfs_tree_read_lock(next);
-				btrfs_clear_path_blocking(path, next,
-							  BTRFS_READ_LOCK);
 			}
 			next_rw_lock = BTRFS_READ_LOCK;
 		}
@@ -5823,8 +5776,6 @@ int btrfs_next_old_leaf(struct btrfs_root *root, struct btrfs_path *path,
 			if (!ret) {
 				btrfs_set_path_blocking(path);
 				btrfs_tree_read_lock(next);
-				btrfs_clear_path_blocking(path, next,
-							  BTRFS_READ_LOCK);
 			}
 			next_rw_lock = BTRFS_READ_LOCK;
 		}

commit 315bed43fea532650933e7bba316a7601d439edf
Author: Nikolay Borisov <nborisov@suse.com>
Date:   Thu Sep 13 11:35:10 2018 +0300

    btrfs: handle error of get_old_root
    
    In btrfs_search_old_slot get_old_root is always used with the assumption
    it cannot fail. However, this is not true in rare circumstance it can
    fail and return null. This will lead to null point dereference when the
    header is read. Fix this by checking the return value and properly
    handling NULL by setting ret to -EIO and returning gracefully.
    
    Coverity-id: 1087503
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Reviewed-by: Lu Fengqi <lufq.fnst@cn.fujitsu.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 6178fadf80a1..0a6c645fab0a 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2960,6 +2960,10 @@ int btrfs_search_old_slot(struct btrfs_root *root, const struct btrfs_key *key,
 
 again:
 	b = get_old_root(root, time_seq);
+	if (!b) {
+		ret = -EIO;
+		goto done;
+	}
 	level = btrfs_header_level(b);
 	p->locks[level] = BTRFS_READ_LOCK;
 

commit 98e6b1eb4022f2eb9845f0da5f16c179e5f32b9f
Author: Liu Bo <bo.liu@linux.alibaba.com>
Date:   Wed Sep 12 06:06:23 2018 +0800

    Btrfs: remove unnecessary level check in balance_level
    
    In the callchain:
    
    btrfs_search_slot()
       if (level != 0)
          setup_nodes_for_search()
              balance_level()
    
    It is just impossible to have level=0 in balance_level, we can drop the
    check.
    
    Signed-off-by: Liu Bo <bo.liu@linux.alibaba.com>
    Reviewed-by: Nikolay Borisov <nborisov@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 1124d236291d..6178fadf80a1 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1815,8 +1815,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 	int orig_slot = path->slots[level];
 	u64 orig_ptr;
 
-	if (level == 0)
-		return 0;
+	ASSERT(level > 0);
 
 	mid = path->nodes[level];
 

commit 4b6f8e9695da65e29f9f8ee84b39e0ba5b45e8e9
Author: Liu Bo <bo.liu@linux.alibaba.com>
Date:   Tue Aug 14 10:46:53 2018 +0800

    Btrfs: do not unnecessarily pass write_lock_level when processing leaf
    
    As we're going to return right after the call, it's not necessary to get
    update the new write_lock_level from unlock_up.
    
    Signed-off-by: Liu Bo <bo.liu@linux.alibaba.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 1f71695cb0a8..1124d236291d 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2910,7 +2910,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 			}
 			if (!p->search_for_split)
 				unlock_up(p, level, lowest_unlock,
-					  min_write_lock_level, &write_lock_level);
+					  min_write_lock_level, NULL);
 			goto done;
 		}
 	}

commit 4fd786e6c3d67b1348e0ad4f450efe9fc9d7a306
Author: Misono Tomohiro <misono.tomohiro@jp.fujitsu.com>
Date:   Mon Aug 6 14:25:24 2018 +0900

    btrfs: Remove 'objectid' member from struct btrfs_root
    
    There are two members in struct btrfs_root which indicate root's
    objectid: objectid and root_key.objectid.
    
    They are both set to the same value in __setup_root():
    
      static void __setup_root(struct btrfs_root *root,
                               struct btrfs_fs_info *fs_info,
                               u64 objectid)
      {
        ...
        root->objectid = objectid;
        ...
        root->root_key.objectid = objecitd;
        ...
      }
    
    and not changed to other value after initialization.
    
    grep in btrfs directory shows both are used in many places:
      $ grep -rI "root->root_key.objectid" | wc -l
      133
      $ grep -rI "root->objectid" | wc -l
      55
     (4.17, inc. some noise)
    
    It is confusing to have two similar variable names and it seems
    that there is no rule about which should be used in a certain case.
    
    Since ->root_key itself is needed for tree reloc tree, let's remove
    'objecitd' member and unify code to use ->root_key.objectid in all places.
    
    Signed-off-by: Misono Tomohiro <misono.tomohiro@jp.fujitsu.com>
    Reviewed-by: Qu Wenruo <wqu@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index d436fb4c002e..1f71695cb0a8 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -207,7 +207,7 @@ static void add_root_to_dirty_list(struct btrfs_root *root)
 	spin_lock(&fs_info->trans_lock);
 	if (!test_and_set_bit(BTRFS_ROOT_DIRTY, &root->state)) {
 		/* Want the extent tree to be the last on the list */
-		if (root->objectid == BTRFS_EXTENT_TREE_OBJECTID)
+		if (root->root_key.objectid == BTRFS_EXTENT_TREE_OBJECTID)
 			list_move_tail(&root->dirty_list,
 				       &fs_info->dirty_cowonly_roots);
 		else

commit a79865c680d81220a1355cd13098e75227dc2994
Author: Nikolay Borisov <nborisov@suse.com>
Date:   Thu Jun 21 09:45:00 2018 +0300

    btrfs: Remove V0 extent support
    
    The v0 compat code was introduced in commit 5d4f98a28c7d
    ("Btrfs: Mixed back reference  (FORWARD ROLLING FORMAT CHANGE)") 9
    years ago, which was merged in 2.6.31. This means that the code is
    there to support filesystems which are _VERY_ old and if you are using
    btrfs on such an old kernel, you have much bigger problems. This coupled
    with the fact that no one is likely testing/maintining this code likely
    means it has bugs lurking. All things considered I think 43 kernel
    releases later it's high time this remnant of the past got removed.
    
    This patch removes all code wrapped in #ifdefs but leaves the BUG_ONs in case
    we have a v0 with no support intact as a sort of safety-net.
    
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 18f1ca1c5bd9..d436fb4c002e 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -888,11 +888,7 @@ int btrfs_block_can_be_shared(struct btrfs_root *root,
 	     btrfs_root_last_snapshot(&root->root_item) ||
 	     btrfs_header_flag(buf, BTRFS_HEADER_FLAG_RELOC)))
 		return 1;
-#ifdef BTRFS_COMPAT_EXTENT_TREE_V0
-	if (test_bit(BTRFS_ROOT_REF_COWS, &root->state) &&
-	    btrfs_header_backref_rev(buf) < BTRFS_MIXED_BACKREF_REV)
-		return 1;
-#endif
+
 	return 0;
 }
 

commit bc877d285ca3dba24c52406946a4a69847cc7422
Author: Nikolay Borisov <nborisov@suse.com>
Date:   Mon Jun 18 14:13:19 2018 +0300

    btrfs: Deduplicate extent_buffer init code
    
    When a new extent buffer is allocated there are a few mandatory fields
    which need to be set in order for the buffer to be sane: level,
    generation, bytenr, backref_rev, owner and FSID/UUID. Currently this
    is open coded in the callers of btrfs_alloc_tree_block, meaning it's
    fairly high in the abstraction hierarchy of operations. This patch
    solves this by simply moving this init code in btrfs_init_new_buffer,
    since this is the function which initializes a newly allocated
    extent buffer. No functional changes.
    
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 18fd80e2f278..18f1ca1c5bd9 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -3358,17 +3358,7 @@ static noinline int insert_new_root(struct btrfs_trans_handle *trans,
 
 	root_add_used(root, fs_info->nodesize);
 
-	memzero_extent_buffer(c, 0, sizeof(struct btrfs_header));
 	btrfs_set_header_nritems(c, 1);
-	btrfs_set_header_level(c, level);
-	btrfs_set_header_bytenr(c, c->start);
-	btrfs_set_header_generation(c, trans->transid);
-	btrfs_set_header_backref_rev(c, BTRFS_MIXED_BACKREF_REV);
-	btrfs_set_header_owner(c, root->root_key.objectid);
-
-	write_extent_buffer_fsid(c, fs_info->fsid);
-	write_extent_buffer_chunk_tree_uuid(c, fs_info->chunk_tree_uuid);
-
 	btrfs_set_node_key(c, &lower_key, 0);
 	btrfs_set_node_blockptr(c, 0, lower->start);
 	lower_gen = btrfs_header_generation(lower);
@@ -3497,15 +3487,7 @@ static noinline int split_node(struct btrfs_trans_handle *trans,
 		return PTR_ERR(split);
 
 	root_add_used(root, fs_info->nodesize);
-
-	memzero_extent_buffer(split, 0, sizeof(struct btrfs_header));
-	btrfs_set_header_level(split, btrfs_header_level(c));
-	btrfs_set_header_bytenr(split, split->start);
-	btrfs_set_header_generation(split, trans->transid);
-	btrfs_set_header_backref_rev(split, BTRFS_MIXED_BACKREF_REV);
-	btrfs_set_header_owner(split, root->root_key.objectid);
-	write_extent_buffer_fsid(split, fs_info->fsid);
-	write_extent_buffer_chunk_tree_uuid(split, fs_info->chunk_tree_uuid);
+	ASSERT(btrfs_header_level(c) == level);
 
 	ret = tree_mod_log_eb_copy(fs_info, split, c, 0, mid, c_nritems - mid);
 	if (ret) {
@@ -4291,15 +4273,6 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 
 	root_add_used(root, fs_info->nodesize);
 
-	memzero_extent_buffer(right, 0, sizeof(struct btrfs_header));
-	btrfs_set_header_bytenr(right, right->start);
-	btrfs_set_header_generation(right, trans->transid);
-	btrfs_set_header_backref_rev(right, BTRFS_MIXED_BACKREF_REV);
-	btrfs_set_header_owner(right, root->root_key.objectid);
-	btrfs_set_header_level(right, 0);
-	write_extent_buffer_fsid(right, fs_info->fsid);
-	write_extent_buffer_chunk_tree_uuid(right, fs_info->chunk_tree_uuid);
-
 	if (split == 0) {
 		if (mid <= slot) {
 			btrfs_set_header_nritems(right, 0);

commit b167fa9152015c91fef9489d059e9dd80d1032fd
Author: Nikolay Borisov <nborisov@suse.com>
Date:   Wed Jun 20 15:48:47 2018 +0300

    btrfs: Remove fs_info from fixup_low_keys
    
    This argument is unused. No functional changes.
    
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Reviewed-by: Qu Wenruo <wqu@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 4bc326df472e..18fd80e2f278 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -3128,8 +3128,7 @@ int btrfs_search_slot_for_read(struct btrfs_root *root,
  * higher levels
  *
  */
-static void fixup_low_keys(struct btrfs_fs_info *fs_info,
-			   struct btrfs_path *path,
+static void fixup_low_keys(struct btrfs_path *path,
 			   struct btrfs_disk_key *key, int level)
 {
 	int i;
@@ -3181,7 +3180,7 @@ void btrfs_set_item_key_safe(struct btrfs_fs_info *fs_info,
 	btrfs_set_item_key(eb, &disk_key, slot);
 	btrfs_mark_buffer_dirty(eb);
 	if (slot == 0)
-		fixup_low_keys(fs_info, path, &disk_key, 1);
+		fixup_low_keys(path, &disk_key, 1);
 }
 
 /*
@@ -3945,7 +3944,7 @@ static noinline int __push_leaf_left(struct btrfs_fs_info *fs_info,
 		clean_tree_block(fs_info, right);
 
 	btrfs_item_key(right, &disk_key, 0);
-	fixup_low_keys(fs_info, path, &disk_key, 1);
+	fixup_low_keys(path, &disk_key, 1);
 
 	/* then fixup the leaf pointer in the path */
 	if (path->slots[0] < push_items) {
@@ -4320,7 +4319,7 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 			path->nodes[0] = right;
 			path->slots[0] = 0;
 			if (path->slots[1] == 0)
-				fixup_low_keys(fs_info, path, &disk_key, 1);
+				fixup_low_keys(path, &disk_key, 1);
 		}
 		/*
 		 * We create a new leaf 'right' for the required ins_len and
@@ -4642,7 +4641,7 @@ void btrfs_truncate_item(struct btrfs_fs_info *fs_info,
 		btrfs_set_disk_key_offset(&disk_key, offset + size_diff);
 		btrfs_set_item_key(leaf, &disk_key, slot);
 		if (slot == 0)
-			fixup_low_keys(fs_info, path, &disk_key, 1);
+			fixup_low_keys(path, &disk_key, 1);
 	}
 
 	item = btrfs_item_nr(slot);
@@ -4744,7 +4743,7 @@ void setup_items_for_insert(struct btrfs_root *root, struct btrfs_path *path,
 
 	if (path->slots[0] == 0) {
 		btrfs_cpu_key_to_disk(&disk_key, cpu_key);
-		fixup_low_keys(fs_info, path, &disk_key, 1);
+		fixup_low_keys(path, &disk_key, 1);
 	}
 	btrfs_unlock_up_safe(path, 1);
 
@@ -4886,7 +4885,6 @@ int btrfs_insert_item(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 static void del_ptr(struct btrfs_root *root, struct btrfs_path *path,
 		    int level, int slot)
 {
-	struct btrfs_fs_info *fs_info = root->fs_info;
 	struct extent_buffer *parent = path->nodes[level];
 	u32 nritems;
 	int ret;
@@ -4919,7 +4917,7 @@ static void del_ptr(struct btrfs_root *root, struct btrfs_path *path,
 		struct btrfs_disk_key disk_key;
 
 		btrfs_node_key(parent, &disk_key, 0);
-		fixup_low_keys(fs_info, path, &disk_key, level + 1);
+		fixup_low_keys(path, &disk_key, level + 1);
 	}
 	btrfs_mark_buffer_dirty(parent);
 }
@@ -5022,7 +5020,7 @@ int btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 			struct btrfs_disk_key disk_key;
 
 			btrfs_item_key(leaf, &disk_key, 0);
-			fixup_low_keys(fs_info, path, &disk_key, 1);
+			fixup_low_keys(path, &disk_key, 1);
 		}
 
 		/* delete the leaf if it is mostly empty */

commit f9ddfd0592acf9bf01814e7d1d60134af7fd0a4d
Author: Liu Bo <bo.liu@linux.alibaba.com>
Date:   Tue May 29 21:27:06 2018 +0800

    Btrfs: remove unused check of skip_locking
    
    The check is superfluous since all callers who set search_for_commit
    also have skip_locking set.
    
    ASSERT() is put in place to ensure skip_locking is set by new callers.
    
    Reviewed-by: Qu Wenruo <wqu@suse.com>
    Signed-off-by: Liu Bo <bo.liu@linux.alibaba.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 973912ec8992..4bc326df472e 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2619,8 +2619,11 @@ static struct extent_buffer *btrfs_search_slot_get_root(struct btrfs_root *root,
 		level = btrfs_header_level(b);
 		if (p->need_commit_sem)
 			up_read(&fs_info->commit_root_sem);
-		if (!p->skip_locking)
-			btrfs_tree_read_lock(b);
+		/*
+		 * Ensure that all callers have set skip_locking when
+		 * p->search_commit_root = 1.
+		 */
+		ASSERT(p->skip_locking == 1);
 
 		goto out;
 	}

commit d80bb3f905ccf70b2c4dde541cff7286e6f936e8
Author: Liu Bo <bo.liu@linux.alibaba.com>
Date:   Fri May 18 11:00:24 2018 +0800

    Btrfs: remove always true check in unlock_up
    
    As unlock_up() is written as
    
    for () {
       if (!path->locks[i])
           break;
       ...
       if (... && path->locks[i]) {
       }
    }
    
    Apparently, @path->locks[i] is always true at this 'if'.
    
    Signed-off-by: Liu Bo <bo.liu@linux.alibaba.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Reviewed-by: Qu Wenruo <wqu@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 239682330929..973912ec8992 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2330,7 +2330,7 @@ static noinline void unlock_up(struct btrfs_path *path, int level,
 			no_skips = 1;
 
 		t = path->nodes[i];
-		if (i >= lowest_unlock && i > skip_level && path->locks[i]) {
+		if (i >= lowest_unlock && i > skip_level) {
 			btrfs_tree_unlock_rw(t, path->locks[i]);
 			path->locks[i] = 0;
 			if (write_lock_level &&

commit 662c653bfda58698cf48d7143a39bd3a063fd9c6
Author: Liu Bo <bo.liu@linux.alibaba.com>
Date:   Fri May 18 11:00:23 2018 +0800

    Btrfs: grab write lock directly if write_lock_level is the max level
    
    Typically, when acquiring root node's lock, btrfs tries its best to get
    read lock and trade for write lock if @write_lock_level implies to do so.
    
    In case of (cow && (p->keep_locks || p->lowest_level)), write_lock_level
    is set to BTRFS_MAX_LEVEL, which means we need to acquire root node's
    write lock directly.
    
    In this particular case, the dance of acquiring read lock and then trading
    for write lock can be saved.
    
    Signed-off-by: Liu Bo <bo.liu@linux.alibaba.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 44dd1950f88a..239682330929 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2632,19 +2632,24 @@ static struct extent_buffer *btrfs_search_slot_get_root(struct btrfs_root *root,
 	}
 
 	/*
-	 * We don't know the level of the root node until we actually have it
-	 * read locked
+	 * If the level is set to maximum, we can skip trying to get the read
+	 * lock.
 	 */
-	b = btrfs_read_lock_root_node(root);
-	level = btrfs_header_level(b);
-	if (level > write_lock_level)
-		goto out;
+	if (write_lock_level < BTRFS_MAX_LEVEL) {
+		/*
+		 * We don't know the level of the root node until we actually
+		 * have it read locked
+		 */
+		b = btrfs_read_lock_root_node(root);
+		level = btrfs_header_level(b);
+		if (level > write_lock_level)
+			goto out;
+
+		/* Whoops, must trade for write lock */
+		btrfs_tree_read_unlock(b);
+		free_extent_buffer(b);
+	}
 
-	/*
-	 * whoops, must trade for write lock
-	 */
-	btrfs_tree_read_unlock(b);
-	free_extent_buffer(b);
 	b = btrfs_lock_root_node(root);
 	root_lock = BTRFS_WRITE_LOCK;
 

commit 1fc28d8e2e9bf22044f1bacd17fe941cd0df5ba6
Author: Liu Bo <bo.liu@linux.alibaba.com>
Date:   Fri May 18 11:00:21 2018 +0800

    Btrfs: move get root out of btrfs_search_slot to a helper
    
    It's good to have a helper instead of having all get-root details
    open-coded.  The new helper locks (if necessary) and sets root node of
    the path.
    
    Also invert the checks to make the code flow easier to read.  There is
    no functional change in this commit.
    
    Signed-off-by: Liu Bo <bo.liu@linux.alibaba.com>
    Reviewed-by: Qu Wenruo <wqu@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 46afc9743ad0..44dd1950f88a 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2598,6 +2598,70 @@ int btrfs_find_item(struct btrfs_root *fs_root, struct btrfs_path *path,
 	return 0;
 }
 
+static struct extent_buffer *btrfs_search_slot_get_root(struct btrfs_root *root,
+							struct btrfs_path *p,
+							int write_lock_level)
+{
+	struct btrfs_fs_info *fs_info = root->fs_info;
+	struct extent_buffer *b;
+	int root_lock;
+	int level = 0;
+
+	/* We try very hard to do read locks on the root */
+	root_lock = BTRFS_READ_LOCK;
+
+	if (p->search_commit_root) {
+		/* The commit roots are read only so we always do read locks */
+		if (p->need_commit_sem)
+			down_read(&fs_info->commit_root_sem);
+		b = root->commit_root;
+		extent_buffer_get(b);
+		level = btrfs_header_level(b);
+		if (p->need_commit_sem)
+			up_read(&fs_info->commit_root_sem);
+		if (!p->skip_locking)
+			btrfs_tree_read_lock(b);
+
+		goto out;
+	}
+
+	if (p->skip_locking) {
+		b = btrfs_root_node(root);
+		level = btrfs_header_level(b);
+		goto out;
+	}
+
+	/*
+	 * We don't know the level of the root node until we actually have it
+	 * read locked
+	 */
+	b = btrfs_read_lock_root_node(root);
+	level = btrfs_header_level(b);
+	if (level > write_lock_level)
+		goto out;
+
+	/*
+	 * whoops, must trade for write lock
+	 */
+	btrfs_tree_read_unlock(b);
+	free_extent_buffer(b);
+	b = btrfs_lock_root_node(root);
+	root_lock = BTRFS_WRITE_LOCK;
+
+	/* The level might have changed, check again */
+	level = btrfs_header_level(b);
+
+out:
+	p->nodes[level] = b;
+	if (!p->skip_locking)
+		p->locks[level] = root_lock;
+	/*
+	 * Callers are responsible for dropping b's references.
+	 */
+	return b;
+}
+
+
 /*
  * btrfs_search_slot - look for a key in a tree and perform necessary
  * modifications to preserve tree invariants.
@@ -2634,7 +2698,6 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 	int err;
 	int level;
 	int lowest_unlock = 1;
-	int root_lock;
 	/* everything at write_lock_level or lower must be write locked */
 	int write_lock_level = 0;
 	u8 lowest_level = 0;
@@ -2672,50 +2735,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 
 again:
 	prev_cmp = -1;
-	/*
-	 * we try very hard to do read locks on the root
-	 */
-	root_lock = BTRFS_READ_LOCK;
-	level = 0;
-	if (p->search_commit_root) {
-		/*
-		 * the commit roots are read only
-		 * so we always do read locks
-		 */
-		if (p->need_commit_sem)
-			down_read(&fs_info->commit_root_sem);
-		b = root->commit_root;
-		extent_buffer_get(b);
-		level = btrfs_header_level(b);
-		if (p->need_commit_sem)
-			up_read(&fs_info->commit_root_sem);
-		if (!p->skip_locking)
-			btrfs_tree_read_lock(b);
-	} else {
-		if (p->skip_locking) {
-			b = btrfs_root_node(root);
-			level = btrfs_header_level(b);
-		} else {
-			/* we don't know the level of the root node
-			 * until we actually have it read locked
-			 */
-			b = btrfs_read_lock_root_node(root);
-			level = btrfs_header_level(b);
-			if (level <= write_lock_level) {
-				/* whoops, must trade for write lock */
-				btrfs_tree_read_unlock(b);
-				free_extent_buffer(b);
-				b = btrfs_lock_root_node(root);
-				root_lock = BTRFS_WRITE_LOCK;
-
-				/* the level might have changed, check again */
-				level = btrfs_header_level(b);
-			}
-		}
-	}
-	p->nodes[level] = b;
-	if (!p->skip_locking)
-		p->locks[level] = root_lock;
+	b = btrfs_search_slot_get_root(root, p, write_lock_level);
 
 	while (b) {
 		level = btrfs_header_level(b);

commit e6a1d6fd276965db0ca91e91dffc0a6fb7d89254
Author: Liu Bo <bo.liu@linux.alibaba.com>
Date:   Fri May 18 11:00:20 2018 +0800

    Btrfs: use more straightforward extent_buffer_uptodate check
    
    If parent_transid "0" is passed to btrfs_buffer_uptodate(),
    btrfs_buffer_uptodate() is equivalent to extent_buffer_uptodate(), but
    extent_buffer_uptodate() is preferred since we don't have to look into
    verify_parent_transid().
    
    Signed-off-by: Liu Bo <bo.liu@linux.alibaba.com>
    Reviewed-by: Qu Wenruo <wqu@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 820226d42d5d..46afc9743ad0 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2445,7 +2445,7 @@ read_block_for_search(struct btrfs_root *root, struct btrfs_path *p,
 		 * and give up so that our caller doesn't loop forever
 		 * on our EAGAINs.
 		 */
-		if (!btrfs_buffer_uptodate(tmp, 0, 0))
+		if (!extent_buffer_uptodate(tmp))
 			ret = -EIO;
 		free_extent_buffer(tmp);
 	} else {

commit ca19b4a69962fa850d5a22aa7a335106fcba0473
Author: Liu Bo <bo.liu@linux.alibaba.com>
Date:   Fri May 18 11:00:19 2018 +0800

    Btrfs: remove superfluous free_extent_buffer in read_block_for_search
    
    read_block_for_search() can be simplified as:
    
    tmp = find_extent_buffer();
    if (tmp)
       return;
    
    ...
    
    free_extent_buffer();
    read_tree_block();
    
    Apparently, @tmp must be NULL at this point, free_extent_buffer() is not
    needed.
    
    Signed-off-by: Liu Bo <bo.liu@linux.alibaba.com>
    Reviewed-by: Qu Wenruo <wqu@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 8c68961925b1..820226d42d5d 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2432,7 +2432,6 @@ read_block_for_search(struct btrfs_root *root, struct btrfs_path *p,
 	btrfs_unlock_up_safe(p, level + 1);
 	btrfs_set_path_blocking(p);
 
-	free_extent_buffer(tmp);
 	if (p->reada != READA_NONE)
 		reada_for_search(fs_info, p, level, slot, key->objectid);
 

commit 02a3307aa9c20b4f6626255b028f07f6cfa16feb
Author: Liu Bo <bo.liu@linux.alibaba.com>
Date:   Wed May 16 01:37:36 2018 +0800

    btrfs: fix reading stale metadata blocks after degraded raid1 mounts
    
    If a btree block, aka. extent buffer, is not available in the extent
    buffer cache, it'll be read out from the disk instead, i.e.
    
    btrfs_search_slot()
      read_block_for_search()  # hold parent and its lock, go to read child
        btrfs_release_path()
        read_tree_block()  # read child
    
    Unfortunately, the parent lock got released before reading child, so
    commit 5bdd3536cbbe ("Btrfs: Fix block generation verification race") had
    used 0 as parent transid to read the child block.  It forces
    read_tree_block() not to check if parent transid is different with the
    generation id of the child that it reads out from disk.
    
    A simple PoC is included in btrfs/124,
    
    0. A two-disk raid1 btrfs,
    
    1. Right after mkfs.btrfs, block A is allocated to be device tree's root.
    
    2. Mount this filesystem and put it in use, after a while, device tree's
       root got COW but block A hasn't been allocated/overwritten yet.
    
    3. Umount it and reload the btrfs module to remove both disks from the
       global @fs_devices list.
    
    4. mount -odegraded dev1 and write some data, so now block A is allocated
       to be a leaf in checksum tree.  Note that only dev1 has the latest
       metadata of this filesystem.
    
    5. Umount it and mount it again normally (with both disks), since raid1
       can pick up one disk by the writer task's pid, if btrfs_search_slot()
       needs to read block A, dev2 which does NOT have the latest metadata
       might be read for block A, then we got a stale block A.
    
    6. As parent transid is not checked, block A is marked as uptodate and
       put into the extent buffer cache, so the future search won't bother
       to read disk again, which means it'll make changes on this stale
       one and make it dirty and flush it onto disk.
    
    To avoid the problem, parent transid needs to be passed to
    read_tree_block().
    
    In order to get a valid parent transid, we need to hold the parent's
    lock until finishing reading child.
    
    This patch needs to be slightly adapted for stable kernels, the
    &first_key parameter added to read_tree_block() is from 4.16+
    (581c1760415c4). The fix is to replace 0 by 'gen'.
    
    Fixes: 5bdd3536cbbe ("Btrfs: Fix block generation verification race")
    CC: stable@vger.kernel.org # 4.4+
    Signed-off-by: Liu Bo <bo.liu@linux.alibaba.com>
    Reviewed-by: Filipe Manana <fdmanana@suse.com>
    Reviewed-by: Qu Wenruo <wqu@suse.com>
    [ update changelog ]
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 63488f0b850f..8c68961925b1 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2436,10 +2436,8 @@ read_block_for_search(struct btrfs_root *root, struct btrfs_path *p,
 	if (p->reada != READA_NONE)
 		reada_for_search(fs_info, p, level, slot, key->objectid);
 
-	btrfs_release_path(p);
-
 	ret = -EAGAIN;
-	tmp = read_tree_block(fs_info, blocknr, 0, parent_level - 1,
+	tmp = read_tree_block(fs_info, blocknr, gen, parent_level - 1,
 			      &first_key);
 	if (!IS_ERR(tmp)) {
 		/*
@@ -2454,6 +2452,8 @@ read_block_for_search(struct btrfs_root *root, struct btrfs_path *p,
 	} else {
 		ret = PTR_ERR(tmp);
 	}
+
+	btrfs_release_path(p);
 	return ret;
 }
 

commit 6f2f0b394b54e2b159ef969a0b5274e9bbf82ff2
Author: Robbie Ko <robbieko@synology.com>
Date:   Mon May 14 10:51:34 2018 +0800

    Btrfs: send, fix invalid access to commit roots due to concurrent snapshotting
    
    [BUG]
    btrfs incremental send BUG happens when creating a snapshot of snapshot
    that is being used by send.
    
    [REASON]
    The problem can happen if while we are doing a send one of the snapshots
    used (parent or send) is snapshotted, because snapshoting implies COWing
    the root of the source subvolume/snapshot.
    
    1. When doing an incremental send, the send process will get the commit
       roots from the parent and send snapshots, and add references to them
       through extent_buffer_get().
    
    2. When a snapshot/subvolume is snapshotted, its root node is COWed
       (transaction.c:create_pending_snapshot()).
    
    3. COWing releases the space used by the node immediately, through:
    
       __btrfs_cow_block()
       --btrfs_free_tree_block()
       ----btrfs_add_free_space(bytenr of node)
    
    4. Because send doesn't hold a transaction open, it's possible that
       the transaction used to create the snapshot commits, switches the
       commit root and the old space used by the previous root node gets
       assigned to some other node allocation. Allocation of a new node will
       use the existing extent buffer found in memory, which we previously
       got a reference through extent_buffer_get(), and allow the extent
       buffer's content (pages) to be modified:
    
       btrfs_alloc_tree_block
       --btrfs_reserve_extent
       ----find_free_extent (get bytenr of old node)
       --btrfs_init_new_buffer (use bytenr of old node)
       ----btrfs_find_create_tree_block
       ------alloc_extent_buffer
       --------find_extent_buffer (get old node)
    
    5. So send can access invalid memory content and have unpredictable
       behaviour.
    
    [FIX]
    So we fix the problem by copying the commit roots of the send and
    parent snapshots and use those copies.
    
    CallTrace looks like this:
     ------------[ cut here ]------------
     kernel BUG at fs/btrfs/ctree.c:1861!
     invalid opcode: 0000 [#1] SMP
     CPU: 6 PID: 24235 Comm: btrfs Tainted: P           O 3.10.105 #23721
     ffff88046652d680 ti: ffff88041b720000 task.ti: ffff88041b720000
     RIP: 0010:[<ffffffffa08dd0e8>] read_node_slot+0x108/0x110 [btrfs]
     RSP: 0018:ffff88041b723b68  EFLAGS: 00010246
     RAX: ffff88043ca6b000 RBX: ffff88041b723c50 RCX: ffff880000000000
     RDX: 000000000000004c RSI: ffff880314b133f8 RDI: ffff880458b24000
     RBP: 0000000000000000 R08: 0000000000000001 R09: ffff88041b723c66
     R10: 0000000000000001 R11: 0000000000001000 R12: ffff8803f3e48890
     R13: ffff8803f3e48880 R14: ffff880466351800 R15: 0000000000000001
     FS:  00007f8c321dc8c0(0000) GS:ffff88047fcc0000(0000)
     CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
     R2: 00007efd1006d000 CR3: 0000000213a24000 CR4: 00000000003407e0
     DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
     DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
     Stack:
     ffff88041b723c50 ffff8803f3e48880 ffff8803f3e48890 ffff8803f3e48880
     ffff880466351800 0000000000000001 ffffffffa08dd9d7 ffff88041b723c50
     ffff8803f3e48880 ffff88041b723c66 ffffffffa08dde85 a9ff88042d2c4400
     Call Trace:
     [<ffffffffa08dd9d7>] ? tree_move_down.isra.33+0x27/0x50 [btrfs]
     [<ffffffffa08dde85>] ? tree_advance+0xb5/0xc0 [btrfs]
     [<ffffffffa08e83d4>] ? btrfs_compare_trees+0x2d4/0x760 [btrfs]
     [<ffffffffa0982050>] ? finish_inode_if_needed+0x870/0x870 [btrfs]
     [<ffffffffa09841ea>] ? btrfs_ioctl_send+0xeda/0x1050 [btrfs]
     [<ffffffffa094bd3d>] ? btrfs_ioctl+0x1e3d/0x33f0 [btrfs]
     [<ffffffff81111133>] ? handle_pte_fault+0x373/0x990
     [<ffffffff8153a096>] ? atomic_notifier_call_chain+0x16/0x20
     [<ffffffff81063256>] ? set_task_cpu+0xb6/0x1d0
     [<ffffffff811122c3>] ? handle_mm_fault+0x143/0x2a0
     [<ffffffff81539cc0>] ? __do_page_fault+0x1d0/0x500
     [<ffffffff81062f07>] ? check_preempt_curr+0x57/0x90
     [<ffffffff8115075a>] ? do_vfs_ioctl+0x4aa/0x990
     [<ffffffff81034f83>] ? do_fork+0x113/0x3b0
     [<ffffffff812dd7d7>] ? trace_hardirqs_off_thunk+0x3a/0x6c
     [<ffffffff81150cc8>] ? SyS_ioctl+0x88/0xa0
     [<ffffffff8153e422>] ? system_call_fastpath+0x16/0x1b
     ---[ end trace 29576629ee80b2e1 ]---
    
    Fixes: 7069830a9e38 ("Btrfs: add btrfs_compare_trees function")
    CC: stable@vger.kernel.org # 3.6+
    Signed-off-by: Robbie Ko <robbieko@synology.com>
    Reviewed-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 3fd44835b386..63488f0b850f 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -5414,12 +5414,24 @@ int btrfs_compare_trees(struct btrfs_root *left_root,
 	down_read(&fs_info->commit_root_sem);
 	left_level = btrfs_header_level(left_root->commit_root);
 	left_root_level = left_level;
-	left_path->nodes[left_level] = left_root->commit_root;
+	left_path->nodes[left_level] =
+			btrfs_clone_extent_buffer(left_root->commit_root);
+	if (!left_path->nodes[left_level]) {
+		up_read(&fs_info->commit_root_sem);
+		ret = -ENOMEM;
+		goto out;
+	}
 	extent_buffer_get(left_path->nodes[left_level]);
 
 	right_level = btrfs_header_level(right_root->commit_root);
 	right_root_level = right_level;
-	right_path->nodes[right_level] = right_root->commit_root;
+	right_path->nodes[right_level] =
+			btrfs_clone_extent_buffer(right_root->commit_root);
+	if (!right_path->nodes[right_level]) {
+		up_read(&fs_info->commit_root_sem);
+		ret = -ENOMEM;
+		goto out;
+	}
 	extent_buffer_get(right_path->nodes[right_level]);
 	up_read(&fs_info->commit_root_sem);
 

commit c1d7c514f745628eb096c5cbb10737855879ae25
Author: David Sterba <dsterba@suse.com>
Date:   Tue Apr 3 19:23:33 2018 +0200

    btrfs: replace GPL boilerplate by SPDX -- sources
    
    Remove GPL boilerplate text (long, short, one-line) and keep the rest,
    ie. personal, company or original source copyright statements. Add the
    SPDX header.
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index a2c9d21176e2..3fd44835b386 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1,19 +1,6 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * Copyright (C) 2007,2008 Oracle.  All rights reserved.
- *
- * This program is free software; you can redistribute it and/or
- * modify it under the terms of the GNU General Public
- * License v2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * General Public License for more details.
- *
- * You should have received a copy of the GNU General Public
- * License along with this program; if not, write to the
- * Free Software Foundation, Inc., 59 Temple Place - Suite 330,
- * Boston, MA 021110-1307, USA.
  */
 
 #include <linux/sched.h>

commit d1980131ca7f0776542f776ceb777cd01eb983e2
Author: David Sterba <dsterba@suse.com>
Date:   Fri Mar 16 02:39:40 2018 +0100

    btrfs: update barrier in should_cow_block
    
    Once there was a simple int force_cow that was used with the plain
    barriers, and then converted to a bit, so we should use the appropriate
    barrier helper.
    
    Other variables in the complex if condition do not depend on a barrier,
    so we should be fine in case the atomic barrier becomes a no-op.
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 7c8faeb868f4..a2c9d21176e2 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1441,8 +1441,8 @@ static inline int should_cow_block(struct btrfs_trans_handle *trans,
 	if (btrfs_is_testing(root->fs_info))
 		return 0;
 
-	/* ensure we can see the force_cow */
-	smp_rmb();
+	/* Ensure we can see the FORCE_COW bit */
+	smp_mb__before_atomic();
 
 	/*
 	 * We do not need to cow a block if

commit 581c1760415c48cca9349b198bba52dd38750765
Author: Qu Wenruo <wqu@suse.com>
Date:   Thu Mar 29 09:08:11 2018 +0800

    btrfs: Validate child tree block's level and first key
    
    We have several reports about node pointer points to incorrect child
    tree blocks, which could have even wrong owner and level but still with
    valid generation and checksum.
    
    Although btrfs check could handle it and print error message like:
    leaf parent key incorrect 60670574592
    
    Kernel doesn't have enough check on this type of corruption correctly.
    At least add such check to read_tree_block() and btrfs_read_buffer(),
    where we need two new parameters @level and @first_key to verify the
    child tree block.
    
    The new @level check is mandatory and all call sites are already
    modified to extract expected level from its call chain.
    
    While @first_key is optional, the following call sites are skipping such
    check:
    1) Root node/leaf
       As ROOT_ITEM doesn't contain the first key, skip @first_key check.
    2) Direct backref
       Only parent bytenr and level is known and we need to resolve the key
       all by ourselves, skip @first_key check.
    
    Another note of this verification is, it needs extra info from nodeptr
    or ROOT_ITEM, so it can't fit into current tree-checker framework, which
    is limited to node/leaf boundary.
    
    Signed-off-by: Qu Wenruo <wqu@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 1ef6b67f893a..7c8faeb868f4 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1354,6 +1354,7 @@ get_old_root(struct btrfs_root *root, u64 time_seq)
 	struct tree_mod_root *old_root = NULL;
 	u64 old_generation = 0;
 	u64 logical;
+	int level;
 
 	eb_root = btrfs_read_lock_root_node(root);
 	tm = __tree_mod_log_oldest_root(eb_root, time_seq);
@@ -1364,15 +1365,17 @@ get_old_root(struct btrfs_root *root, u64 time_seq)
 		old_root = &tm->old_root;
 		old_generation = tm->generation;
 		logical = old_root->logical;
+		level = old_root->level;
 	} else {
 		logical = eb_root->start;
+		level = btrfs_header_level(eb_root);
 	}
 
 	tm = tree_mod_log_search(fs_info, logical, time_seq);
 	if (old_root && tm && tm->op != MOD_LOG_KEY_REMOVE_WHILE_FREEING) {
 		btrfs_tree_read_unlock(eb_root);
 		free_extent_buffer(eb_root);
-		old = read_tree_block(fs_info, logical, 0);
+		old = read_tree_block(fs_info, logical, 0, level, NULL);
 		if (WARN_ON(IS_ERR(old) || !extent_buffer_uptodate(old))) {
 			if (!IS_ERR(old))
 				free_extent_buffer(old);
@@ -1592,6 +1595,7 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 	btrfs_set_lock_blocking(parent);
 
 	for (i = start_slot; i <= end_slot; i++) {
+		struct btrfs_key first_key;
 		int close = 1;
 
 		btrfs_node_key(parent, &disk_key, i);
@@ -1601,6 +1605,7 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 		progress_passed = 1;
 		blocknr = btrfs_node_blockptr(parent, i);
 		gen = btrfs_node_ptr_generation(parent, i);
+		btrfs_node_key_to_cpu(parent, &first_key, i);
 		if (last_block == 0)
 			last_block = blocknr;
 
@@ -1624,7 +1629,9 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 			uptodate = 0;
 		if (!cur || !uptodate) {
 			if (!cur) {
-				cur = read_tree_block(fs_info, blocknr, gen);
+				cur = read_tree_block(fs_info, blocknr, gen,
+						      parent_level - 1,
+						      &first_key);
 				if (IS_ERR(cur)) {
 					return PTR_ERR(cur);
 				} else if (!extent_buffer_uptodate(cur)) {
@@ -1632,7 +1639,8 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 					return -EIO;
 				}
 			} else if (!uptodate) {
-				err = btrfs_read_buffer(cur, gen);
+				err = btrfs_read_buffer(cur, gen,
+						parent_level - 1,&first_key);
 				if (err) {
 					free_extent_buffer(cur);
 					return err;
@@ -1785,14 +1793,17 @@ read_node_slot(struct btrfs_fs_info *fs_info, struct extent_buffer *parent,
 {
 	int level = btrfs_header_level(parent);
 	struct extent_buffer *eb;
+	struct btrfs_key first_key;
 
 	if (slot < 0 || slot >= btrfs_header_nritems(parent))
 		return ERR_PTR(-ENOENT);
 
 	BUG_ON(level == 0);
 
+	btrfs_node_key_to_cpu(parent, &first_key, slot);
 	eb = read_tree_block(fs_info, btrfs_node_blockptr(parent, slot),
-			     btrfs_node_ptr_generation(parent, slot));
+			     btrfs_node_ptr_generation(parent, slot),
+			     level - 1, &first_key);
 	if (!IS_ERR(eb) && !extent_buffer_uptodate(eb)) {
 		free_extent_buffer(eb);
 		eb = ERR_PTR(-EIO);
@@ -2388,10 +2399,14 @@ read_block_for_search(struct btrfs_root *root, struct btrfs_path *p,
 	u64 gen;
 	struct extent_buffer *b = *eb_ret;
 	struct extent_buffer *tmp;
+	struct btrfs_key first_key;
 	int ret;
+	int parent_level;
 
 	blocknr = btrfs_node_blockptr(b, slot);
 	gen = btrfs_node_ptr_generation(b, slot);
+	parent_level = btrfs_header_level(b);
+	btrfs_node_key_to_cpu(b, &first_key, slot);
 
 	tmp = find_extent_buffer(fs_info, blocknr);
 	if (tmp) {
@@ -2410,7 +2425,7 @@ read_block_for_search(struct btrfs_root *root, struct btrfs_path *p,
 		btrfs_set_path_blocking(p);
 
 		/* now we're allowed to do a blocking uptodate check */
-		ret = btrfs_read_buffer(tmp, gen);
+		ret = btrfs_read_buffer(tmp, gen, parent_level - 1, &first_key);
 		if (!ret) {
 			*eb_ret = tmp;
 			return 0;
@@ -2437,7 +2452,8 @@ read_block_for_search(struct btrfs_root *root, struct btrfs_path *p,
 	btrfs_release_path(p);
 
 	ret = -EAGAIN;
-	tmp = read_tree_block(fs_info, blocknr, 0);
+	tmp = read_tree_block(fs_info, blocknr, 0, parent_level - 1,
+			      &first_key);
 	if (!IS_ERR(tmp)) {
 		/*
 		 * If the read above didn't mark this buffer up to date,

commit d9d19a010b8b186668ce1182bcf92ab53d11c084
Author: David Sterba <dsterba@suse.com>
Date:   Mon Mar 5 16:35:29 2018 +0100

    btrfs: kill tree_mod_log_set_root_pointer helper
    
    A useless wrapper around tree_mod_log_insert_root that hides missing
    error handling. Move it to the callers.
    
    Reviewed-by: Nikolay Borisov <nborisov@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 260ca86a5cf8..1ef6b67f893a 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -883,16 +883,6 @@ static noinline int tree_mod_log_free_eb(struct extent_buffer *eb)
 	return ret;
 }
 
-static noinline void
-tree_mod_log_set_root_pointer(struct btrfs_root *root,
-			      struct extent_buffer *new_root_node,
-			      int log_removal)
-{
-	int ret;
-	ret = tree_mod_log_insert_root(root->node, new_root_node, log_removal);
-	BUG_ON(ret < 0);
-}
-
 /*
  * check if the tree block can be shared by multiple trees
  */
@@ -1119,7 +1109,8 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 			parent_start = buf->start;
 
 		extent_buffer_get(cow);
-		tree_mod_log_set_root_pointer(root, cow, 1);
+		ret = tree_mod_log_insert_root(root->node, cow, 1);
+		BUG_ON(ret < 0);
 		rcu_assign_pointer(root->node, cow);
 
 		btrfs_free_tree_block(trans, root, buf, parent_start,
@@ -1873,7 +1864,8 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 			goto enospc;
 		}
 
-		tree_mod_log_set_root_pointer(root, child, 1);
+		ret = tree_mod_log_insert_root(root->node, child, 1);
+		BUG_ON(ret < 0);
 		rcu_assign_pointer(root->node, child);
 
 		add_root_to_dirty_list(root);
@@ -3319,6 +3311,7 @@ static noinline int insert_new_root(struct btrfs_trans_handle *trans,
 	struct extent_buffer *c;
 	struct extent_buffer *old;
 	struct btrfs_disk_key lower_key;
+	int ret;
 
 	BUG_ON(path->nodes[level]);
 	BUG_ON(path->nodes[level-1] != root->node);
@@ -3357,7 +3350,8 @@ static noinline int insert_new_root(struct btrfs_trans_handle *trans,
 	btrfs_mark_buffer_dirty(c);
 
 	old = root->node;
-	tree_mod_log_set_root_pointer(root, c, 0);
+	ret = tree_mod_log_insert_root(root->node, c, 0);
+	BUG_ON(ret < 0);
 	rcu_assign_pointer(root->node, c);
 
 	/* the super has an extra ref to root->node */

commit 0e82bcfe3c5e557ff9262eac59526e5208b84c60
Author: David Sterba <dsterba@suse.com>
Date:   Mon Mar 5 16:16:54 2018 +0100

    btrfs: kill tree_mod_log_set_node_key helper
    
    A trivial wrapper that can be simply opencoded and makes the GFP
    allocation request more visible. The error handling is now moved to the
    callers.
    
    Reviewed-by: Nikolay Borisov <nborisov@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 1e6a3281befe..260ca86a5cf8 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -837,16 +837,6 @@ tree_mod_log_eb_copy(struct btrfs_fs_info *fs_info, struct extent_buffer *dst,
 	return ret;
 }
 
-static noinline void tree_mod_log_set_node_key(struct extent_buffer *eb,
-		int slot, int atomic)
-{
-	int ret;
-
-	ret = tree_mod_log_insert_key(eb, slot, MOD_LOG_KEY_REPLACE,
-					atomic ? GFP_ATOMIC : GFP_NOFS);
-	BUG_ON(ret < 0);
-}
-
 static noinline int tree_mod_log_free_eb(struct extent_buffer *eb)
 {
 	struct tree_mod_elem **tm_list = NULL;
@@ -1962,7 +1952,9 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		} else {
 			struct btrfs_disk_key right_key;
 			btrfs_node_key(right, &right_key, 0);
-			tree_mod_log_set_node_key(parent, pslot + 1, 0);
+			ret = tree_mod_log_insert_key(parent, pslot + 1,
+					MOD_LOG_KEY_REPLACE, GFP_NOFS);
+			BUG_ON(ret < 0);
 			btrfs_set_node_key(parent, &right_key, pslot + 1);
 			btrfs_mark_buffer_dirty(parent);
 		}
@@ -2006,7 +1998,9 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		/* update the parent key to reflect our changes */
 		struct btrfs_disk_key mid_key;
 		btrfs_node_key(mid, &mid_key, 0);
-		tree_mod_log_set_node_key(parent, pslot, 0);
+		ret = tree_mod_log_insert_key(parent, pslot,
+				MOD_LOG_KEY_REPLACE, GFP_NOFS);
+		BUG_ON(ret < 0);
 		btrfs_set_node_key(parent, &mid_key, pslot);
 		btrfs_mark_buffer_dirty(parent);
 	}
@@ -2107,7 +2101,9 @@ static noinline int push_nodes_for_insert(struct btrfs_trans_handle *trans,
 			struct btrfs_disk_key disk_key;
 			orig_slot += left_nr;
 			btrfs_node_key(mid, &disk_key, 0);
-			tree_mod_log_set_node_key(parent, pslot, 0);
+			ret = tree_mod_log_insert_key(parent, pslot,
+					MOD_LOG_KEY_REPLACE, GFP_NOFS);
+			BUG_ON(ret < 0);
 			btrfs_set_node_key(parent, &disk_key, pslot);
 			btrfs_mark_buffer_dirty(parent);
 			if (btrfs_header_nritems(left) > orig_slot) {
@@ -2161,7 +2157,9 @@ static noinline int push_nodes_for_insert(struct btrfs_trans_handle *trans,
 			struct btrfs_disk_key disk_key;
 
 			btrfs_node_key(right, &disk_key, 0);
-			tree_mod_log_set_node_key(parent, pslot + 1, 0);
+			ret = tree_mod_log_insert_key(parent, pslot + 1,
+					MOD_LOG_KEY_REPLACE, GFP_NOFS);
+			BUG_ON(ret < 0);
 			btrfs_set_node_key(parent, &disk_key, pslot + 1);
 			btrfs_mark_buffer_dirty(parent);
 
@@ -3114,13 +3112,17 @@ static void fixup_low_keys(struct btrfs_fs_info *fs_info,
 {
 	int i;
 	struct extent_buffer *t;
+	int ret;
 
 	for (i = level; i < BTRFS_MAX_LEVEL; i++) {
 		int tslot = path->slots[i];
+
 		if (!path->nodes[i])
 			break;
 		t = path->nodes[i];
-		tree_mod_log_set_node_key(t, tslot, 1);
+		ret = tree_mod_log_insert_key(t, tslot, MOD_LOG_KEY_REPLACE,
+				GFP_ATOMIC);
+		BUG_ON(ret < 0);
 		btrfs_set_node_key(t, key, tslot);
 		btrfs_mark_buffer_dirty(path->nodes[i]);
 		if (tslot != 0)

commit bf1d342510bdba21b5f0c68057cd9fe6f68d0dc1
Author: David Sterba <dsterba@suse.com>
Date:   Mon Mar 5 15:47:39 2018 +0100

    btrfs: kill trivial wrapper tree_mod_log_eb_move
    
    The wrapper is effectively an alias for tree_mod_log_insert_move but
    also hides the missing error handling. To make that more visible, lift
    the BUG_ON to the callers.
    
    Reviewed-by: Nikolay Borisov <nborisov@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index a1c987a26827..1e6a3281befe 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -837,14 +837,6 @@ tree_mod_log_eb_copy(struct btrfs_fs_info *fs_info, struct extent_buffer *dst,
 	return ret;
 }
 
-static inline void tree_mod_log_eb_move(struct extent_buffer *dst,
-		     int dst_offset, int src_offset, int nr_items)
-{
-	int ret;
-	ret = tree_mod_log_insert_move(dst, dst_offset, src_offset, nr_items);
-	BUG_ON(ret < 0);
-}
-
 static noinline void tree_mod_log_set_node_key(struct extent_buffer *eb,
 		int slot, int atomic)
 {
@@ -3225,8 +3217,8 @@ static int push_node_left(struct btrfs_trans_handle *trans,
 
 	if (push_items < src_nritems) {
 		/*
-		 * don't call tree_mod_log_eb_move here, key removal was already
-		 * fully logged by tree_mod_log_eb_copy above.
+		 * Don't call tree_mod_log_insert_move here, key removal was
+		 * already fully logged by tree_mod_log_eb_copy above.
 		 */
 		memmove_extent_buffer(src, btrfs_node_key_ptr_offset(0),
 				      btrfs_node_key_ptr_offset(push_items),
@@ -3281,7 +3273,8 @@ static int balance_node_right(struct btrfs_trans_handle *trans,
 	if (max_push < push_items)
 		push_items = max_push;
 
-	tree_mod_log_eb_move(dst, push_items, 0, dst_nritems);
+	ret = tree_mod_log_insert_move(dst, push_items, 0, dst_nritems);
+	BUG_ON(ret < 0);
 	memmove_extent_buffer(dst, btrfs_node_key_ptr_offset(push_items),
 				      btrfs_node_key_ptr_offset(0),
 				      (dst_nritems) *
@@ -3399,9 +3392,11 @@ static void insert_ptr(struct btrfs_trans_handle *trans,
 	BUG_ON(slot > nritems);
 	BUG_ON(nritems == BTRFS_NODEPTRS_PER_BLOCK(fs_info));
 	if (slot != nritems) {
-		if (level)
-			tree_mod_log_eb_move(lower, slot + 1, slot,
+		if (level) {
+			ret = tree_mod_log_insert_move(lower, slot + 1, slot,
 					nritems - slot);
+			BUG_ON(ret < 0);
+		}
 		memmove_extent_buffer(lower,
 			      btrfs_node_key_ptr_offset(slot + 1),
 			      btrfs_node_key_ptr_offset(slot),
@@ -4872,9 +4867,11 @@ static void del_ptr(struct btrfs_root *root, struct btrfs_path *path,
 
 	nritems = btrfs_header_nritems(parent);
 	if (slot != nritems - 1) {
-		if (level)
-			tree_mod_log_eb_move(parent, slot, slot + 1,
+		if (level) {
+			ret = tree_mod_log_insert_move(parent, slot, slot + 1,
 					nritems - slot - 1);
+			BUG_ON(ret < 0);
+		}
 		memmove_extent_buffer(parent,
 			      btrfs_node_key_ptr_offset(slot),
 			      btrfs_node_key_ptr_offset(slot + 1),

commit b1a09f1ec540408abf3a50d15dff5d9506932693
Author: David Sterba <dsterba@suse.com>
Date:   Mon Mar 5 15:43:41 2018 +0100

    btrfs: remove trivial locking wrappers of tree mod log
    
    The wrappers are trivial and do not bring any extra value on top of the
    plain locking primitives.
    
    Reviewed-by: Nikolay Borisov <nborisov@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index dbd0f976e59d..a1c987a26827 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -330,26 +330,6 @@ struct tree_mod_elem {
 	struct tree_mod_root old_root;
 };
 
-static inline void tree_mod_log_read_lock(struct btrfs_fs_info *fs_info)
-{
-	read_lock(&fs_info->tree_mod_log_lock);
-}
-
-static inline void tree_mod_log_read_unlock(struct btrfs_fs_info *fs_info)
-{
-	read_unlock(&fs_info->tree_mod_log_lock);
-}
-
-static inline void tree_mod_log_write_lock(struct btrfs_fs_info *fs_info)
-{
-	write_lock(&fs_info->tree_mod_log_lock);
-}
-
-static inline void tree_mod_log_write_unlock(struct btrfs_fs_info *fs_info)
-{
-	write_unlock(&fs_info->tree_mod_log_lock);
-}
-
 /*
  * Pull a new tree mod seq number for our operation.
  */
@@ -369,14 +349,14 @@ static inline u64 btrfs_inc_tree_mod_seq(struct btrfs_fs_info *fs_info)
 u64 btrfs_get_tree_mod_seq(struct btrfs_fs_info *fs_info,
 			   struct seq_list *elem)
 {
-	tree_mod_log_write_lock(fs_info);
+	write_lock(&fs_info->tree_mod_log_lock);
 	spin_lock(&fs_info->tree_mod_seq_lock);
 	if (!elem->seq) {
 		elem->seq = btrfs_inc_tree_mod_seq(fs_info);
 		list_add_tail(&elem->list, &fs_info->tree_mod_seq_list);
 	}
 	spin_unlock(&fs_info->tree_mod_seq_lock);
-	tree_mod_log_write_unlock(fs_info);
+	write_unlock(&fs_info->tree_mod_log_lock);
 
 	return elem->seq;
 }
@@ -418,7 +398,7 @@ void btrfs_put_tree_mod_seq(struct btrfs_fs_info *fs_info,
 	 * anything that's lower than the lowest existing (read: blocked)
 	 * sequence number can be removed from the tree.
 	 */
-	tree_mod_log_write_lock(fs_info);
+	write_lock(&fs_info->tree_mod_log_lock);
 	tm_root = &fs_info->tree_mod_log;
 	for (node = rb_first(tm_root); node; node = next) {
 		next = rb_next(node);
@@ -428,7 +408,7 @@ void btrfs_put_tree_mod_seq(struct btrfs_fs_info *fs_info,
 		rb_erase(node, tm_root);
 		kfree(tm);
 	}
-	tree_mod_log_write_unlock(fs_info);
+	write_unlock(&fs_info->tree_mod_log_lock);
 }
 
 /*
@@ -439,7 +419,7 @@ void btrfs_put_tree_mod_seq(struct btrfs_fs_info *fs_info,
  * for root replace operations, or the logical address of the affected
  * block for all other operations.
  *
- * Note: must be called with write lock (tree_mod_log_write_lock).
+ * Note: must be called with write lock for fs_info::tree_mod_log_lock.
  */
 static noinline int
 __tree_mod_log_insert(struct btrfs_fs_info *fs_info, struct tree_mod_elem *tm)
@@ -477,7 +457,7 @@ __tree_mod_log_insert(struct btrfs_fs_info *fs_info, struct tree_mod_elem *tm)
  * Determines if logging can be omitted. Returns 1 if it can. Otherwise, it
  * returns zero with the tree_mod_log_lock acquired. The caller must hold
  * this until all tree mod log insertions are recorded in the rb tree and then
- * call tree_mod_log_write_unlock() to release.
+ * write unlock fs_info::tree_mod_log_lock.
  */
 static inline int tree_mod_dont_log(struct btrfs_fs_info *fs_info,
 				    struct extent_buffer *eb) {
@@ -487,9 +467,9 @@ static inline int tree_mod_dont_log(struct btrfs_fs_info *fs_info,
 	if (eb && btrfs_header_level(eb) == 0)
 		return 1;
 
-	tree_mod_log_write_lock(fs_info);
+	write_lock(&fs_info->tree_mod_log_lock);
 	if (list_empty(&(fs_info)->tree_mod_seq_list)) {
-		tree_mod_log_write_unlock(fs_info);
+		write_unlock(&fs_info->tree_mod_log_lock);
 		return 1;
 	}
 
@@ -551,7 +531,7 @@ static noinline int tree_mod_log_insert_key(struct extent_buffer *eb, int slot,
 	}
 
 	ret = __tree_mod_log_insert(eb->fs_info, tm);
-	tree_mod_log_write_unlock(eb->fs_info);
+	write_unlock(&eb->fs_info->tree_mod_log_lock);
 	if (ret)
 		kfree(tm);
 
@@ -613,7 +593,7 @@ static noinline int tree_mod_log_insert_move(struct extent_buffer *eb,
 	ret = __tree_mod_log_insert(eb->fs_info, tm);
 	if (ret)
 		goto free_tms;
-	tree_mod_log_write_unlock(eb->fs_info);
+	write_unlock(&eb->fs_info->tree_mod_log_lock);
 	kfree(tm_list);
 
 	return 0;
@@ -624,7 +604,7 @@ static noinline int tree_mod_log_insert_move(struct extent_buffer *eb,
 		kfree(tm_list[i]);
 	}
 	if (locked)
-		tree_mod_log_write_unlock(eb->fs_info);
+		write_unlock(&eb->fs_info->tree_mod_log_lock);
 	kfree(tm_list);
 	kfree(tm);
 
@@ -703,7 +683,7 @@ static noinline int tree_mod_log_insert_root(struct extent_buffer *old_root,
 	if (!ret)
 		ret = __tree_mod_log_insert(fs_info, tm);
 
-	tree_mod_log_write_unlock(fs_info);
+	write_unlock(&fs_info->tree_mod_log_lock);
 	if (ret)
 		goto free_tms;
 	kfree(tm_list);
@@ -730,7 +710,7 @@ __tree_mod_log_search(struct btrfs_fs_info *fs_info, u64 start, u64 min_seq,
 	struct tree_mod_elem *cur = NULL;
 	struct tree_mod_elem *found = NULL;
 
-	tree_mod_log_read_lock(fs_info);
+	read_lock(&fs_info->tree_mod_log_lock);
 	tm_root = &fs_info->tree_mod_log;
 	node = tm_root->rb_node;
 	while (node) {
@@ -758,7 +738,7 @@ __tree_mod_log_search(struct btrfs_fs_info *fs_info, u64 start, u64 min_seq,
 			break;
 		}
 	}
-	tree_mod_log_read_unlock(fs_info);
+	read_unlock(&fs_info->tree_mod_log_lock);
 
 	return found;
 }
@@ -839,7 +819,7 @@ tree_mod_log_eb_copy(struct btrfs_fs_info *fs_info, struct extent_buffer *dst,
 			goto free_tms;
 	}
 
-	tree_mod_log_write_unlock(fs_info);
+	write_unlock(&fs_info->tree_mod_log_lock);
 	kfree(tm_list);
 
 	return 0;
@@ -851,7 +831,7 @@ tree_mod_log_eb_copy(struct btrfs_fs_info *fs_info, struct extent_buffer *dst,
 		kfree(tm_list[i]);
 	}
 	if (locked)
-		tree_mod_log_write_unlock(fs_info);
+		write_unlock(&fs_info->tree_mod_log_lock);
 	kfree(tm_list);
 
 	return ret;
@@ -906,7 +886,7 @@ static noinline int tree_mod_log_free_eb(struct extent_buffer *eb)
 		goto free_tms;
 
 	ret = __tree_mod_log_free_eb(eb->fs_info, tm_list, nritems);
-	tree_mod_log_write_unlock(eb->fs_info);
+	write_unlock(&eb->fs_info->tree_mod_log_lock);
 	if (ret)
 		goto free_tms;
 	kfree(tm_list);
@@ -1262,7 +1242,7 @@ __tree_mod_log_rewind(struct btrfs_fs_info *fs_info, struct extent_buffer *eb,
 	unsigned long p_size = sizeof(struct btrfs_key_ptr);
 
 	n = btrfs_header_nritems(eb);
-	tree_mod_log_read_lock(fs_info);
+	read_lock(&fs_info->tree_mod_log_lock);
 	while (tm && tm->seq >= time_seq) {
 		/*
 		 * all the operations are recorded with the operator used for
@@ -1317,7 +1297,7 @@ __tree_mod_log_rewind(struct btrfs_fs_info *fs_info, struct extent_buffer *eb,
 		if (tm->logical != first_tm->logical)
 			break;
 	}
-	tree_mod_log_read_unlock(fs_info);
+	read_unlock(&fs_info->tree_mod_log_lock);
 	btrfs_set_header_nritems(eb, n);
 }
 

commit bcd24dabe080a627333fb058fdfe760dc579edf1
Author: David Sterba <dsterba@suse.com>
Date:   Mon Mar 5 15:33:18 2018 +0100

    btrfs: drop fs_info parameter from __tree_mod_log_oldest_root
    
    It's provided by the extent_buffer.
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 6b0192ebf01f..dbd0f976e59d 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1195,9 +1195,8 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
  * returns the logical address of the oldest predecessor of the given root.
  * entries older than time_seq are ignored.
  */
-static struct tree_mod_elem *
-__tree_mod_log_oldest_root(struct btrfs_fs_info *fs_info,
-			   struct extent_buffer *eb_root, u64 time_seq)
+static struct tree_mod_elem *__tree_mod_log_oldest_root(
+		struct extent_buffer *eb_root, u64 time_seq)
 {
 	struct tree_mod_elem *tm;
 	struct tree_mod_elem *found = NULL;
@@ -1214,7 +1213,7 @@ __tree_mod_log_oldest_root(struct btrfs_fs_info *fs_info,
 	 * first operation that's logged for this root.
 	 */
 	while (1) {
-		tm = tree_mod_log_search_oldest(fs_info, root_logical,
+		tm = tree_mod_log_search_oldest(eb_root->fs_info, root_logical,
 						time_seq);
 		if (!looped && !tm)
 			return NULL;
@@ -1404,7 +1403,7 @@ get_old_root(struct btrfs_root *root, u64 time_seq)
 	u64 logical;
 
 	eb_root = btrfs_read_lock_root_node(root);
-	tm = __tree_mod_log_oldest_root(fs_info, eb_root, time_seq);
+	tm = __tree_mod_log_oldest_root(eb_root, time_seq);
 	if (!tm)
 		return eb_root;
 
@@ -1468,7 +1467,7 @@ int btrfs_old_root_level(struct btrfs_root *root, u64 time_seq)
 	int level;
 	struct extent_buffer *eb_root = btrfs_root_node(root);
 
-	tm = __tree_mod_log_oldest_root(root->fs_info, eb_root, time_seq);
+	tm = __tree_mod_log_oldest_root(eb_root, time_seq);
 	if (tm && tm->op == MOD_LOG_ROOT_REPLACE) {
 		level = tm->old_root.level;
 	} else {

commit b6dfa35bd56762581bfc60b7eb9b1a4d7d10c289
Author: David Sterba <dsterba@suse.com>
Date:   Mon Mar 5 15:31:18 2018 +0100

    btrfs: embed tree_mod_move structure to tree_mod_elem
    
    The tree_mod_move is not used anywhere and can be embedded as anonymous
    structure.
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 53dfce12e3fb..6b0192ebf01f 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -299,11 +299,6 @@ enum mod_log_op {
 	MOD_LOG_ROOT_REPLACE,
 };
 
-struct tree_mod_move {
-	int dst_slot;
-	int nr_items;
-};
-
 struct tree_mod_root {
 	u64 logical;
 	u8 level;
@@ -326,7 +321,10 @@ struct tree_mod_elem {
 	u64 blockptr;
 
 	/* this is used for op == MOD_LOG_MOVE_KEYS */
-	struct tree_mod_move move;
+	struct {
+		int dst_slot;
+		int nr_items;
+	} move;
 
 	/* this is used for op == MOD_LOG_ROOT_REPLACE */
 	struct tree_mod_root old_root;

commit a446a979ff4ec6784b59583b929e4656fe2f8e32
Author: David Sterba <dsterba@suse.com>
Date:   Mon Mar 5 15:26:29 2018 +0100

    btrfs: drop unused fs_info parameter from tree_mod_log_eb_move
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 217e672bafb4..53dfce12e3fb 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -859,8 +859,7 @@ tree_mod_log_eb_copy(struct btrfs_fs_info *fs_info, struct extent_buffer *dst,
 	return ret;
 }
 
-static inline void
-tree_mod_log_eb_move(struct btrfs_fs_info *fs_info, struct extent_buffer *dst,
+static inline void tree_mod_log_eb_move(struct extent_buffer *dst,
 		     int dst_offset, int src_offset, int nr_items)
 {
 	int ret;
@@ -3305,7 +3304,7 @@ static int balance_node_right(struct btrfs_trans_handle *trans,
 	if (max_push < push_items)
 		push_items = max_push;
 
-	tree_mod_log_eb_move(fs_info, dst, push_items, 0, dst_nritems);
+	tree_mod_log_eb_move(dst, push_items, 0, dst_nritems);
 	memmove_extent_buffer(dst, btrfs_node_key_ptr_offset(push_items),
 				      btrfs_node_key_ptr_offset(0),
 				      (dst_nritems) *
@@ -3424,8 +3423,8 @@ static void insert_ptr(struct btrfs_trans_handle *trans,
 	BUG_ON(nritems == BTRFS_NODEPTRS_PER_BLOCK(fs_info));
 	if (slot != nritems) {
 		if (level)
-			tree_mod_log_eb_move(fs_info, lower, slot + 1,
-					     slot, nritems - slot);
+			tree_mod_log_eb_move(lower, slot + 1, slot,
+					nritems - slot);
 		memmove_extent_buffer(lower,
 			      btrfs_node_key_ptr_offset(slot + 1),
 			      btrfs_node_key_ptr_offset(slot),
@@ -4897,8 +4896,8 @@ static void del_ptr(struct btrfs_root *root, struct btrfs_path *path,
 	nritems = btrfs_header_nritems(parent);
 	if (slot != nritems - 1) {
 		if (level)
-			tree_mod_log_eb_move(fs_info, parent, slot,
-					     slot + 1, nritems - slot - 1);
+			tree_mod_log_eb_move(parent, slot, slot + 1,
+					nritems - slot - 1);
 		memmove_extent_buffer(parent,
 			      btrfs_node_key_ptr_offset(slot),
 			      btrfs_node_key_ptr_offset(slot + 1),

commit 95b757c164820d7c7262c1e546b47d79dd256c96
Author: David Sterba <dsterba@suse.com>
Date:   Mon Mar 5 15:22:30 2018 +0100

    btrfs: drop fs_info parameter from tree_mod_log_free_eb
    
    It's provided by the extent_buffer.
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index b3c4305b5ecd..217e672bafb4 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -654,12 +654,10 @@ __tree_mod_log_free_eb(struct btrfs_fs_info *fs_info,
 	return 0;
 }
 
-static noinline int
-tree_mod_log_insert_root(struct btrfs_fs_info *fs_info,
-			 struct extent_buffer *old_root,
-			 struct extent_buffer *new_root,
-			 int log_removal)
+static noinline int tree_mod_log_insert_root(struct extent_buffer *old_root,
+			 struct extent_buffer *new_root, int log_removal)
 {
+	struct btrfs_fs_info *fs_info = old_root->fs_info;
 	struct tree_mod_elem *tm = NULL;
 	struct tree_mod_elem **tm_list = NULL;
 	int nritems = 0;
@@ -932,8 +930,7 @@ tree_mod_log_set_root_pointer(struct btrfs_root *root,
 			      int log_removal)
 {
 	int ret;
-	ret = tree_mod_log_insert_root(root->fs_info, root->node,
-				       new_root_node, log_removal);
+	ret = tree_mod_log_insert_root(root->node, new_root_node, log_removal);
 	BUG_ON(ret < 0);
 }
 

commit db7279a20b0982587729cb2a23780e42d536721b
Author: David Sterba <dsterba@suse.com>
Date:   Mon Mar 5 15:14:25 2018 +0100

    btrfs: drop fs_info parameter from tree_mod_log_free_eb
    
    It's provided by the extent_buffer.
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 960d2c1008bf..b3c4305b5ecd 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -41,8 +41,6 @@ static int balance_node_right(struct btrfs_trans_handle *trans,
 			      struct extent_buffer *src_buf);
 static void del_ptr(struct btrfs_root *root, struct btrfs_path *path,
 		    int level, int slot);
-static int tree_mod_log_free_eb(struct btrfs_fs_info *fs_info,
-				 struct extent_buffer *eb);
 
 struct btrfs_path *btrfs_alloc_path(void)
 {
@@ -882,8 +880,7 @@ static noinline void tree_mod_log_set_node_key(struct extent_buffer *eb,
 	BUG_ON(ret < 0);
 }
 
-static noinline int
-tree_mod_log_free_eb(struct btrfs_fs_info *fs_info, struct extent_buffer *eb)
+static noinline int tree_mod_log_free_eb(struct extent_buffer *eb)
 {
 	struct tree_mod_elem **tm_list = NULL;
 	int nritems = 0;
@@ -893,7 +890,7 @@ tree_mod_log_free_eb(struct btrfs_fs_info *fs_info, struct extent_buffer *eb)
 	if (btrfs_header_level(eb) == 0)
 		return 0;
 
-	if (!tree_mod_need_log(fs_info, NULL))
+	if (!tree_mod_need_log(eb->fs_info, NULL))
 		return 0;
 
 	nritems = btrfs_header_nritems(eb);
@@ -910,11 +907,11 @@ tree_mod_log_free_eb(struct btrfs_fs_info *fs_info, struct extent_buffer *eb)
 		}
 	}
 
-	if (tree_mod_dont_log(fs_info, eb))
+	if (tree_mod_dont_log(eb->fs_info, eb))
 		goto free_tms;
 
-	ret = __tree_mod_log_free_eb(fs_info, tm_list, nritems);
-	tree_mod_log_write_unlock(fs_info);
+	ret = __tree_mod_log_free_eb(eb->fs_info, tm_list, nritems);
+	tree_mod_log_write_unlock(eb->fs_info);
 	if (ret)
 		goto free_tms;
 	kfree(tm_list);
@@ -1183,7 +1180,7 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 					      trans->transid);
 		btrfs_mark_buffer_dirty(parent);
 		if (last_ref) {
-			ret = tree_mod_log_free_eb(fs_info, buf);
+			ret = tree_mod_log_free_eb(buf);
 			if (ret) {
 				btrfs_abort_transaction(trans, ret);
 				return ret;

commit e09c2efe7eba1498bd3f9300b4aadef9b939a259
Author: David Sterba <dsterba@suse.com>
Date:   Mon Mar 5 15:09:03 2018 +0100

    btrfs: drop fs_info parameter from tree_mod_log_insert_key
    
    It's provided by the extent_buffer.
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 0358ed3e1562..960d2c1008bf 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -536,28 +536,26 @@ alloc_tree_mod_elem(struct extent_buffer *eb, int slot,
 	return tm;
 }
 
-static noinline int
-tree_mod_log_insert_key(struct btrfs_fs_info *fs_info,
-			struct extent_buffer *eb, int slot,
-			enum mod_log_op op, gfp_t flags)
+static noinline int tree_mod_log_insert_key(struct extent_buffer *eb, int slot,
+		enum mod_log_op op, gfp_t flags)
 {
 	struct tree_mod_elem *tm;
 	int ret;
 
-	if (!tree_mod_need_log(fs_info, eb))
+	if (!tree_mod_need_log(eb->fs_info, eb))
 		return 0;
 
 	tm = alloc_tree_mod_elem(eb, slot, op, flags);
 	if (!tm)
 		return -ENOMEM;
 
-	if (tree_mod_dont_log(fs_info, eb)) {
+	if (tree_mod_dont_log(eb->fs_info, eb)) {
 		kfree(tm);
 		return 0;
 	}
 
-	ret = __tree_mod_log_insert(fs_info, tm);
-	tree_mod_log_write_unlock(fs_info);
+	ret = __tree_mod_log_insert(eb->fs_info, tm);
+	tree_mod_log_write_unlock(eb->fs_info);
 	if (ret)
 		kfree(tm);
 
@@ -879,8 +877,7 @@ static noinline void tree_mod_log_set_node_key(struct extent_buffer *eb,
 {
 	int ret;
 
-	ret = tree_mod_log_insert_key(eb->fs_info, eb, slot,
-					MOD_LOG_KEY_REPLACE,
+	ret = tree_mod_log_insert_key(eb, slot, MOD_LOG_KEY_REPLACE,
 					atomic ? GFP_ATOMIC : GFP_NOFS);
 	BUG_ON(ret < 0);
 }
@@ -1178,7 +1175,7 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 		add_root_to_dirty_list(root);
 	} else {
 		WARN_ON(trans->transid != btrfs_header_generation(parent));
-		tree_mod_log_insert_key(fs_info, parent, parent_slot,
+		tree_mod_log_insert_key(parent, parent_slot,
 					MOD_LOG_KEY_REPLACE, GFP_NOFS);
 		btrfs_set_node_blockptr(parent, parent_slot,
 					cow->start);
@@ -3441,8 +3438,8 @@ static void insert_ptr(struct btrfs_trans_handle *trans,
 			      (nritems - slot) * sizeof(struct btrfs_key_ptr));
 	}
 	if (level) {
-		ret = tree_mod_log_insert_key(fs_info, lower, slot,
-					      MOD_LOG_KEY_ADD, GFP_NOFS);
+		ret = tree_mod_log_insert_key(lower, slot, MOD_LOG_KEY_ADD,
+				GFP_NOFS);
 		BUG_ON(ret < 0);
 	}
 	btrfs_set_node_key(lower, key, slot);
@@ -4914,8 +4911,8 @@ static void del_ptr(struct btrfs_root *root, struct btrfs_path *path,
 			      sizeof(struct btrfs_key_ptr) *
 			      (nritems - slot - 1));
 	} else if (level) {
-		ret = tree_mod_log_insert_key(fs_info, parent, slot,
-					      MOD_LOG_KEY_REMOVE, GFP_NOFS);
+		ret = tree_mod_log_insert_key(parent, slot, MOD_LOG_KEY_REMOVE,
+				GFP_NOFS);
 		BUG_ON(ret < 0);
 	}
 

commit 6074d45f60760f7a8aa34883f37cd4d54cd5855e
Author: David Sterba <dsterba@suse.com>
Date:   Mon Mar 5 15:03:52 2018 +0100

    btrfs: drop fs_info parameter from tree_mod_log_insert_move
    
    It's provided by the extent_buffer.
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 62aa776c77d1..0358ed3e1562 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -564,10 +564,8 @@ tree_mod_log_insert_key(struct btrfs_fs_info *fs_info,
 	return ret;
 }
 
-static noinline int
-tree_mod_log_insert_move(struct btrfs_fs_info *fs_info,
-			 struct extent_buffer *eb, int dst_slot, int src_slot,
-			 int nr_items)
+static noinline int tree_mod_log_insert_move(struct extent_buffer *eb,
+		int dst_slot, int src_slot, int nr_items)
 {
 	struct tree_mod_elem *tm = NULL;
 	struct tree_mod_elem **tm_list = NULL;
@@ -575,7 +573,7 @@ tree_mod_log_insert_move(struct btrfs_fs_info *fs_info,
 	int i;
 	int locked = 0;
 
-	if (!tree_mod_need_log(fs_info, eb))
+	if (!tree_mod_need_log(eb->fs_info, eb))
 		return 0;
 
 	tm_list = kcalloc(nr_items, sizeof(struct tree_mod_elem *), GFP_NOFS);
@@ -603,7 +601,7 @@ tree_mod_log_insert_move(struct btrfs_fs_info *fs_info,
 		}
 	}
 
-	if (tree_mod_dont_log(fs_info, eb))
+	if (tree_mod_dont_log(eb->fs_info, eb))
 		goto free_tms;
 	locked = 1;
 
@@ -613,26 +611,26 @@ tree_mod_log_insert_move(struct btrfs_fs_info *fs_info,
 	 * buffer, i.e. dst_slot < src_slot.
 	 */
 	for (i = 0; i + dst_slot < src_slot && i < nr_items; i++) {
-		ret = __tree_mod_log_insert(fs_info, tm_list[i]);
+		ret = __tree_mod_log_insert(eb->fs_info, tm_list[i]);
 		if (ret)
 			goto free_tms;
 	}
 
-	ret = __tree_mod_log_insert(fs_info, tm);
+	ret = __tree_mod_log_insert(eb->fs_info, tm);
 	if (ret)
 		goto free_tms;
-	tree_mod_log_write_unlock(fs_info);
+	tree_mod_log_write_unlock(eb->fs_info);
 	kfree(tm_list);
 
 	return 0;
 free_tms:
 	for (i = 0; i < nr_items; i++) {
 		if (tm_list[i] && !RB_EMPTY_NODE(&tm_list[i]->node))
-			rb_erase(&tm_list[i]->node, &fs_info->tree_mod_log);
+			rb_erase(&tm_list[i]->node, &eb->fs_info->tree_mod_log);
 		kfree(tm_list[i]);
 	}
 	if (locked)
-		tree_mod_log_write_unlock(fs_info);
+		tree_mod_log_write_unlock(eb->fs_info);
 	kfree(tm_list);
 	kfree(tm);
 
@@ -872,8 +870,7 @@ tree_mod_log_eb_move(struct btrfs_fs_info *fs_info, struct extent_buffer *dst,
 		     int dst_offset, int src_offset, int nr_items)
 {
 	int ret;
-	ret = tree_mod_log_insert_move(fs_info, dst, dst_offset, src_offset,
-				       nr_items);
+	ret = tree_mod_log_insert_move(dst, dst_offset, src_offset, nr_items);
 	BUG_ON(ret < 0);
 }
 

commit 3ac6de1abd7a485f48948af9ef18139500eef9ae
Author: David Sterba <dsterba@suse.com>
Date:   Mon Mar 5 15:00:37 2018 +0100

    btrfs: drop fs_info parameter from tree_mod_log_set_node_key
    
    It's provided by the extent_buffer.
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index a80fcd285b34..62aa776c77d1 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -877,13 +877,12 @@ tree_mod_log_eb_move(struct btrfs_fs_info *fs_info, struct extent_buffer *dst,
 	BUG_ON(ret < 0);
 }
 
-static noinline void
-tree_mod_log_set_node_key(struct btrfs_fs_info *fs_info,
-			  struct extent_buffer *eb, int slot, int atomic)
+static noinline void tree_mod_log_set_node_key(struct extent_buffer *eb,
+		int slot, int atomic)
 {
 	int ret;
 
-	ret = tree_mod_log_insert_key(fs_info, eb, slot,
+	ret = tree_mod_log_insert_key(eb->fs_info, eb, slot,
 					MOD_LOG_KEY_REPLACE,
 					atomic ? GFP_ATOMIC : GFP_NOFS);
 	BUG_ON(ret < 0);
@@ -2007,8 +2006,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		} else {
 			struct btrfs_disk_key right_key;
 			btrfs_node_key(right, &right_key, 0);
-			tree_mod_log_set_node_key(fs_info, parent,
-						  pslot + 1, 0);
+			tree_mod_log_set_node_key(parent, pslot + 1, 0);
 			btrfs_set_node_key(parent, &right_key, pslot + 1);
 			btrfs_mark_buffer_dirty(parent);
 		}
@@ -2052,7 +2050,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		/* update the parent key to reflect our changes */
 		struct btrfs_disk_key mid_key;
 		btrfs_node_key(mid, &mid_key, 0);
-		tree_mod_log_set_node_key(fs_info, parent, pslot, 0);
+		tree_mod_log_set_node_key(parent, pslot, 0);
 		btrfs_set_node_key(parent, &mid_key, pslot);
 		btrfs_mark_buffer_dirty(parent);
 	}
@@ -2153,7 +2151,7 @@ static noinline int push_nodes_for_insert(struct btrfs_trans_handle *trans,
 			struct btrfs_disk_key disk_key;
 			orig_slot += left_nr;
 			btrfs_node_key(mid, &disk_key, 0);
-			tree_mod_log_set_node_key(fs_info, parent, pslot, 0);
+			tree_mod_log_set_node_key(parent, pslot, 0);
 			btrfs_set_node_key(parent, &disk_key, pslot);
 			btrfs_mark_buffer_dirty(parent);
 			if (btrfs_header_nritems(left) > orig_slot) {
@@ -2207,8 +2205,7 @@ static noinline int push_nodes_for_insert(struct btrfs_trans_handle *trans,
 			struct btrfs_disk_key disk_key;
 
 			btrfs_node_key(right, &disk_key, 0);
-			tree_mod_log_set_node_key(fs_info, parent,
-						  pslot + 1, 0);
+			tree_mod_log_set_node_key(parent, pslot + 1, 0);
 			btrfs_set_node_key(parent, &disk_key, pslot + 1);
 			btrfs_mark_buffer_dirty(parent);
 
@@ -3167,7 +3164,7 @@ static void fixup_low_keys(struct btrfs_fs_info *fs_info,
 		if (!path->nodes[i])
 			break;
 		t = path->nodes[i];
-		tree_mod_log_set_node_key(fs_info, t, tslot, 1);
+		tree_mod_log_set_node_key(t, tslot, 1);
 		btrfs_set_node_key(t, key, tslot);
 		btrfs_mark_buffer_dirty(path->nodes[i]);
 		if (tslot != 0)

commit 448f3a17ac538f6e5b0fa94259cb41f3f019394f
Author: Nikolay Borisov <nborisov@suse.com>
Date:   Tue Feb 27 17:37:16 2018 +0200

    btrfs: Remove redundant comment from btrfs_search_forward
    
    This function always sets keep_locks to 1 and saves the old value of
    keep_locks which is restored at the end. So there is no way it can be
    called without keep_locks being set. Remove comment imposing redundant
    requirement on callers.
    
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index b88a79e69ddf..a80fcd285b34 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -5145,9 +5145,6 @@ int btrfs_prev_leaf(struct btrfs_root *root, struct btrfs_path *path)
  * into min_key, so you can call btrfs_search_slot with cow=1 on the
  * key and get a writable path.
  *
- * This does lock as it descends, and path->keep_locks should be set
- * to 1 by the caller.
- *
  * This honors path->lowest_level to prevent descent past a given level
  * of the tree.
  *

commit 4271ecea64f40683d28d83ad433ddc43e5ca2ee9
Author: Nikolay Borisov <nborisov@suse.com>
Date:   Wed Dec 13 09:38:14 2017 +0200

    btrfs: Improve btrfs_search_slot description
    
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 2a09577580b8..b88a79e69ddf 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2654,17 +2654,29 @@ int btrfs_find_item(struct btrfs_root *fs_root, struct btrfs_path *path,
 }
 
 /*
- * look for key in the tree.  path is filled in with nodes along the way
- * if key is found, we return zero and you can find the item in the leaf
- * level of the path (level 0)
+ * btrfs_search_slot - look for a key in a tree and perform necessary
+ * modifications to preserve tree invariants.
  *
- * If the key isn't found, the path points to the slot where it should
- * be inserted, and 1 is returned.  If there are other errors during the
- * search a negative error number is returned.
+ * @trans:	Handle of transaction, used when modifying the tree
+ * @p:		Holds all btree nodes along the search path
+ * @root:	The root node of the tree
+ * @key:	The key we are looking for
+ * @ins_len:	Indicates purpose of search, for inserts it is 1, for
+ *		deletions it's -1. 0 for plain searches
+ * @cow:	boolean should CoW operations be performed. Must always be 1
+ *		when modifying the tree.
  *
- * if ins_len > 0, nodes and leaves will be split as we walk down the
- * tree.  if ins_len < 0, nodes will be merged as we walk down the tree (if
- * possible)
+ * If @ins_len > 0, nodes and leaves will be split as we walk down the tree.
+ * If @ins_len < 0, nodes will be merged as we walk down the tree (if possible)
+ *
+ * If @key is found, 0 is returned and you can find the item in the leaf level
+ * of the path (level 0)
+ *
+ * If @key isn't found, 1 is returned and the leaf level of the path (level 0)
+ * points to the slot where it should be inserted
+ *
+ * If an error is encountered while searching the tree a negative error number
+ * is returned
  */
 int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 		      const struct btrfs_key *key, struct btrfs_path *p,

commit a74b35ec876df1c2d11b980bb5e4f697ea4fba94
Author: Nikolay Borisov <nborisov@suse.com>
Date:   Fri Dec 8 16:27:43 2017 +0200

    btrfs: Rename bin_search -> btrfs_bin_search
    
    Currently there are 2 function doing binary search on btrfs nodes:
    bin_search and btrfs_bin_search. The latter being a simple wrapper for
    the former. So eliminate the wrapper and just rename bin_search to
    btrfs_bin_search. No functional changes
    
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 5361f69433a3..2a09577580b8 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1807,8 +1807,8 @@ static noinline int generic_bin_search(struct extent_buffer *eb,
  * simple bin_search frontend that does the right thing for
  * leaves vs nodes
  */
-static int bin_search(struct extent_buffer *eb, const struct btrfs_key *key,
-		      int level, int *slot)
+int btrfs_bin_search(struct extent_buffer *eb, const struct btrfs_key *key,
+		     int level, int *slot)
 {
 	if (level == 0)
 		return generic_bin_search(eb,
@@ -1824,12 +1824,6 @@ static int bin_search(struct extent_buffer *eb, const struct btrfs_key *key,
 					  slot);
 }
 
-int btrfs_bin_search(struct extent_buffer *eb, const struct btrfs_key *key,
-		     int level, int *slot)
-{
-	return bin_search(eb, key, level, slot);
-}
-
 static void root_add_used(struct btrfs_root *root, u32 size)
 {
 	spin_lock(&root->accounting_lock);
@@ -2614,7 +2608,7 @@ static int key_search(struct extent_buffer *b, const struct btrfs_key *key,
 		      int level, int *prev_cmp, int *slot)
 {
 	if (*prev_cmp != 0) {
-		*prev_cmp = bin_search(b, key, level, slot);
+		*prev_cmp = btrfs_bin_search(b, key, level, slot);
 		return *prev_cmp;
 	}
 
@@ -5181,7 +5175,7 @@ int btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,
 	while (1) {
 		nritems = btrfs_header_nritems(cur);
 		level = btrfs_header_level(cur);
-		sret = bin_search(cur, min_key, level, &slot);
+		sret = btrfs_bin_search(cur, min_key, level, &slot);
 
 		/* at the lowest level, we're done, setup the path and exit */
 		if (level == path->lowest_level) {

commit 9ea2c7c9da13c9073e371c046cbbc45481ecb459
Author: Nikolay Borisov <nborisov@suse.com>
Date:   Tue Dec 12 11:14:49 2017 +0200

    btrfs: Fix out of bounds access in btrfs_search_slot
    
    When modifying a tree where the root is at BTRFS_MAX_LEVEL - 1 then
    the level variable is going to be 7 (this is the max height of the
    tree). On the other hand btrfs_cow_block is always called with
    "level + 1" as an index into the nodes and slots arrays. This leads to
    an out of bounds access. Admittdely this will be benign since an OOB
    access of the nodes array will likely read the 0th element from the
    slots array, which in this case is going to be 0 (since we start CoW at
    the top of the tree). The OOB access into the slots array in turn will
    read the 0th and 1st values of the locks array, which would both be 0
    at the time. However, this benign behavior relies on the fact that the
    path being passed hasn't been initialised, if it has already been used to
    query a btree then it could potentially have populated the nodes/slots arrays.
    
    Fix it by explicitly checking if we are at level 7 (the maximum allowed
    index in nodes/slots arrays) and explicitly call the CoW routine with
    NULL for parent's node/slot.
    
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Fixes-coverity-id: 711515
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 1e74cf826532..5361f69433a3 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2774,6 +2774,8 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 		 * contention with the cow code
 		 */
 		if (cow) {
+			bool last_level = (level == (BTRFS_MAX_LEVEL - 1));
+
 			/*
 			 * if we don't really need to cow this block
 			 * then we don't want to set the path blocking,
@@ -2798,9 +2800,13 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 			}
 
 			btrfs_set_path_blocking(p);
-			err = btrfs_cow_block(trans, root, b,
-					      p->nodes[level + 1],
-					      p->slots[level + 1], &b);
+			if (last_level)
+				err = btrfs_cow_block(trans, root, b, NULL, 0,
+						      &b);
+			else
+				err = btrfs_cow_block(trans, root, b,
+						      p->nodes[level + 1],
+						      p->slots[level + 1], &b);
 			if (err) {
 				ret = err;
 				goto done;

commit 692826b2738101549f032a761a9191636e83be4e
Author: Jeff Mahoney <jeffm@suse.com>
Date:   Tue Nov 21 13:58:49 2017 -0500

    btrfs: handle errors while updating refcounts in update_ref_for_cow
    
    Since commit fb235dc06fa (btrfs: qgroup: Move half of the qgroup
    accounting time out of commit trans) the assumption that
    btrfs_add_delayed_{data,tree}_ref can only return 0 or -ENOMEM has
    been false.  The qgroup operations call into btrfs_search_slot
    and friends and can now return the full spectrum of error codes.
    
    Fortunately, the fix here is easy since update_ref_for_cow failing
    is already handled so we just need to bail early with the error
    code.
    
    Fixes: fb235dc06fa (btrfs: qgroup: Move half of the qgroup accounting ...)
    Cc: <stable@vger.kernel.org> # v4.11+
    Signed-off-by: Jeff Mahoney <jeffm@suse.com>
    Reviewed-by: Edmund Nadolski <enadolski@suse.com>
    Reviewed-by: Qu Wenruo <wqu@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 531e0a8645b0..1e74cf826532 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1032,14 +1032,17 @@ static noinline int update_ref_for_cow(struct btrfs_trans_handle *trans,
 		     root->root_key.objectid == BTRFS_TREE_RELOC_OBJECTID) &&
 		    !(flags & BTRFS_BLOCK_FLAG_FULL_BACKREF)) {
 			ret = btrfs_inc_ref(trans, root, buf, 1);
-			BUG_ON(ret); /* -ENOMEM */
+			if (ret)
+				return ret;
 
 			if (root->root_key.objectid ==
 			    BTRFS_TREE_RELOC_OBJECTID) {
 				ret = btrfs_dec_ref(trans, root, buf, 0);
-				BUG_ON(ret); /* -ENOMEM */
+				if (ret)
+					return ret;
 				ret = btrfs_inc_ref(trans, root, cow, 1);
-				BUG_ON(ret); /* -ENOMEM */
+				if (ret)
+					return ret;
 			}
 			new_flags |= BTRFS_BLOCK_FLAG_FULL_BACKREF;
 		} else {
@@ -1049,7 +1052,8 @@ static noinline int update_ref_for_cow(struct btrfs_trans_handle *trans,
 				ret = btrfs_inc_ref(trans, root, cow, 1);
 			else
 				ret = btrfs_inc_ref(trans, root, cow, 0);
-			BUG_ON(ret); /* -ENOMEM */
+			if (ret)
+				return ret;
 		}
 		if (new_flags != 0) {
 			int level = btrfs_header_level(buf);
@@ -1068,9 +1072,11 @@ static noinline int update_ref_for_cow(struct btrfs_trans_handle *trans,
 				ret = btrfs_inc_ref(trans, root, cow, 1);
 			else
 				ret = btrfs_inc_ref(trans, root, cow, 0);
-			BUG_ON(ret); /* -ENOMEM */
+			if (ret)
+				return ret;
 			ret = btrfs_dec_ref(trans, root, buf, 1);
-			BUG_ON(ret); /* -ENOMEM */
+			if (ret)
+				return ret;
 		}
 		clean_tree_block(fs_info, buf);
 		*last_ref = 1;

commit 84f7d8e6242ceb377c7af10a7133c653cc7fea5f
Author: Josef Bacik <josef@toxicpanda.com>
Date:   Fri Sep 29 15:43:49 2017 -0400

    btrfs: pass root to various extent ref mod functions
    
    We need the actual root for the ref verifier tool to work, so change
    these functions to pass the root around instead.  This will be used in
    a subsequent patch.
    
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 19b9c5131745..531e0a8645b0 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -192,7 +192,7 @@ struct extent_buffer *btrfs_lock_root_node(struct btrfs_root *root)
  * tree until you end up with a lock on the root.  A locked buffer
  * is returned, with a reference held.
  */
-static struct extent_buffer *btrfs_read_lock_root_node(struct btrfs_root *root)
+struct extent_buffer *btrfs_read_lock_root_node(struct btrfs_root *root)
 {
 	struct extent_buffer *eb;
 

commit ee8c494f88736c1a1873fdd65559828f9a734bcf
Author: Nikolay Borisov <nborisov@suse.com>
Date:   Mon Aug 21 12:43:45 2017 +0300

    btrfs: Remove unused arguments from btrfs_changed_cb_t
    
    btrfs_changed_cb_t represents the signature of the callback being passed
    to btrfs_compare_trees. Currently there is only one such callback,
    namely changed_cb in send.c. This function doesn't really uses the first
    2 parameters, i.e. the roots. Since there are not other functions
    implementing the btrfs_changed_cb_t let's remove the unused parameters
    from the prototype and implementation.
    
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Reviewed-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 6d49db7d86be..19b9c5131745 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -5496,8 +5496,7 @@ int btrfs_compare_trees(struct btrfs_root *left_root,
 			goto out;
 		} else if (left_end_reached) {
 			if (right_level == 0) {
-				ret = changed_cb(left_root, right_root,
-						left_path, right_path,
+				ret = changed_cb(left_path, right_path,
 						&right_key,
 						BTRFS_COMPARE_TREE_DELETED,
 						ctx);
@@ -5508,8 +5507,7 @@ int btrfs_compare_trees(struct btrfs_root *left_root,
 			continue;
 		} else if (right_end_reached) {
 			if (left_level == 0) {
-				ret = changed_cb(left_root, right_root,
-						left_path, right_path,
+				ret = changed_cb(left_path, right_path,
 						&left_key,
 						BTRFS_COMPARE_TREE_NEW,
 						ctx);
@@ -5523,8 +5521,7 @@ int btrfs_compare_trees(struct btrfs_root *left_root,
 		if (left_level == 0 && right_level == 0) {
 			cmp = btrfs_comp_cpu_keys(&left_key, &right_key);
 			if (cmp < 0) {
-				ret = changed_cb(left_root, right_root,
-						left_path, right_path,
+				ret = changed_cb(left_path, right_path,
 						&left_key,
 						BTRFS_COMPARE_TREE_NEW,
 						ctx);
@@ -5532,8 +5529,7 @@ int btrfs_compare_trees(struct btrfs_root *left_root,
 					goto out;
 				advance_left = ADVANCE;
 			} else if (cmp > 0) {
-				ret = changed_cb(left_root, right_root,
-						left_path, right_path,
+				ret = changed_cb(left_path, right_path,
 						&right_key,
 						BTRFS_COMPARE_TREE_DELETED,
 						ctx);
@@ -5550,8 +5546,7 @@ int btrfs_compare_trees(struct btrfs_root *left_root,
 					result = BTRFS_COMPARE_TREE_CHANGED;
 				else
 					result = BTRFS_COMPARE_TREE_SAME;
-				ret = changed_cb(left_root, right_root,
-						 left_path, right_path,
+				ret = changed_cb(left_path, right_path,
 						 &left_key, result, ctx);
 				if (ret < 0)
 					goto out;

commit a4f78750ef1882e59bb4f947e216cf61ef2d67d2
Author: David Sterba <dsterba@suse.com>
Date:   Thu Jun 29 18:37:49 2017 +0200

    btrfs: get fs_info from eb in btrfs_print_leaf, remove argument
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 3f4daa9d6e2c..6d49db7d86be 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -4650,7 +4650,7 @@ void btrfs_truncate_item(struct btrfs_fs_info *fs_info,
 	btrfs_mark_buffer_dirty(leaf);
 
 	if (btrfs_leaf_free_space(fs_info, leaf) < 0) {
-		btrfs_print_leaf(fs_info, leaf);
+		btrfs_print_leaf(leaf);
 		BUG();
 	}
 }
@@ -4679,7 +4679,7 @@ void btrfs_extend_item(struct btrfs_fs_info *fs_info, struct btrfs_path *path,
 	data_end = leaf_data_end(fs_info, leaf);
 
 	if (btrfs_leaf_free_space(fs_info, leaf) < data_size) {
-		btrfs_print_leaf(fs_info, leaf);
+		btrfs_print_leaf(leaf);
 		BUG();
 	}
 	slot = path->slots[0];
@@ -4687,7 +4687,7 @@ void btrfs_extend_item(struct btrfs_fs_info *fs_info, struct btrfs_path *path,
 
 	BUG_ON(slot < 0);
 	if (slot >= nritems) {
-		btrfs_print_leaf(fs_info, leaf);
+		btrfs_print_leaf(leaf);
 		btrfs_crit(fs_info, "slot %d too large, nritems %d",
 			   slot, nritems);
 		BUG_ON(1);
@@ -4718,7 +4718,7 @@ void btrfs_extend_item(struct btrfs_fs_info *fs_info, struct btrfs_path *path,
 	btrfs_mark_buffer_dirty(leaf);
 
 	if (btrfs_leaf_free_space(fs_info, leaf) < 0) {
-		btrfs_print_leaf(fs_info, leaf);
+		btrfs_print_leaf(leaf);
 		BUG();
 	}
 }
@@ -4757,7 +4757,7 @@ void setup_items_for_insert(struct btrfs_root *root, struct btrfs_path *path,
 	data_end = leaf_data_end(fs_info, leaf);
 
 	if (btrfs_leaf_free_space(fs_info, leaf) < total_size) {
-		btrfs_print_leaf(fs_info, leaf);
+		btrfs_print_leaf(leaf);
 		btrfs_crit(fs_info, "not enough freespace need %u have %d",
 			   total_size, btrfs_leaf_free_space(fs_info, leaf));
 		BUG();
@@ -4767,7 +4767,7 @@ void setup_items_for_insert(struct btrfs_root *root, struct btrfs_path *path,
 		unsigned int old_data = btrfs_item_end_nr(leaf, slot);
 
 		if (old_data < data_end) {
-			btrfs_print_leaf(fs_info, leaf);
+			btrfs_print_leaf(leaf);
 			btrfs_crit(fs_info, "slot %d old_data %d data_end %d",
 				   slot, old_data, data_end);
 			BUG_ON(1);
@@ -4811,7 +4811,7 @@ void setup_items_for_insert(struct btrfs_root *root, struct btrfs_path *path,
 	btrfs_mark_buffer_dirty(leaf);
 
 	if (btrfs_leaf_free_space(fs_info, leaf) < 0) {
-		btrfs_print_leaf(fs_info, leaf);
+		btrfs_print_leaf(leaf);
 		BUG();
 	}
 }

commit adf0212396e3af238e25e7c54ecb2959f19def24
Author: David Sterba <dsterba@suse.com>
Date:   Wed May 31 19:44:31 2017 +0200

    btrfs: adjust includes after vmalloc removal
    
    As we don't use vmalloc/vzalloc/vfree directly in ctree.c, we can now
    use the proper header that defines kvmalloc.
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 6e1b02dd72d3..3f4daa9d6e2c 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -19,7 +19,7 @@
 #include <linux/sched.h>
 #include <linux/slab.h>
 #include <linux/rbtree.h>
-#include <linux/vmalloc.h>
+#include <linux/mm.h>
 #include "ctree.h"
 #include "disk-io.h"
 #include "transaction.h"

commit 3d9ec8c49ad16a5c113e8d23ba07abb96518a586
Author: Nikolay Borisov <nborisov@suse.com>
Date:   Mon May 29 09:43:43 2017 +0300

    btrfs: rename btrfs_leaf_data to BTRFS_LEAF_DATA_OFFSET
    
    Commit 5f39d397dfbe ("Btrfs: Create extent_buffer interface
    for large blocksizes") refactored btrfs_leaf_data function to take
    extent_buffer rather than struct btrfs_leaf. However, as it turns out the
    parameter being passed is never used. Furthermore this function no longer
    returns the leaf data but rather the offset to it. So rename the function
    to BTRFS_LEAF_DATA_OFFSET to make it consistent with other BTRFS_LEAF_*
    helpers and turn it into a macro.
    
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    [ removed () from the macro ]
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index a3a75f1de002..6e1b02dd72d3 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -3667,14 +3667,14 @@ static noinline int __push_leaf_right(struct btrfs_fs_info *fs_info,
 	/* make room in the right data area */
 	data_end = leaf_data_end(fs_info, right);
 	memmove_extent_buffer(right,
-			      btrfs_leaf_data(right) + data_end - push_space,
-			      btrfs_leaf_data(right) + data_end,
+			      BTRFS_LEAF_DATA_OFFSET + data_end - push_space,
+			      BTRFS_LEAF_DATA_OFFSET + data_end,
 			      BTRFS_LEAF_DATA_SIZE(fs_info) - data_end);
 
 	/* copy from the left data area */
-	copy_extent_buffer(right, left, btrfs_leaf_data(right) +
+	copy_extent_buffer(right, left, BTRFS_LEAF_DATA_OFFSET +
 		     BTRFS_LEAF_DATA_SIZE(fs_info) - push_space,
-		     btrfs_leaf_data(left) + leaf_data_end(fs_info, left),
+		     BTRFS_LEAF_DATA_OFFSET + leaf_data_end(fs_info, left),
 		     push_space);
 
 	memmove_extent_buffer(right, btrfs_item_nr_offset(push_items),
@@ -3888,9 +3888,9 @@ static noinline int __push_leaf_left(struct btrfs_fs_info *fs_info,
 	push_space = BTRFS_LEAF_DATA_SIZE(fs_info) -
 		     btrfs_item_offset_nr(right, push_items - 1);
 
-	copy_extent_buffer(left, right, btrfs_leaf_data(left) +
+	copy_extent_buffer(left, right, BTRFS_LEAF_DATA_OFFSET +
 		     leaf_data_end(fs_info, left) - push_space,
-		     btrfs_leaf_data(right) +
+		     BTRFS_LEAF_DATA_OFFSET +
 		     btrfs_item_offset_nr(right, push_items - 1),
 		     push_space);
 	old_left_nritems = btrfs_header_nritems(left);
@@ -3917,9 +3917,9 @@ static noinline int __push_leaf_left(struct btrfs_fs_info *fs_info,
 	if (push_items < right_nritems) {
 		push_space = btrfs_item_offset_nr(right, push_items - 1) -
 						  leaf_data_end(fs_info, right);
-		memmove_extent_buffer(right, btrfs_leaf_data(right) +
+		memmove_extent_buffer(right, BTRFS_LEAF_DATA_OFFSET +
 				      BTRFS_LEAF_DATA_SIZE(fs_info) - push_space,
-				      btrfs_leaf_data(right) +
+				      BTRFS_LEAF_DATA_OFFSET +
 				      leaf_data_end(fs_info, right), push_space);
 
 		memmove_extent_buffer(right, btrfs_item_nr_offset(0),
@@ -4069,8 +4069,8 @@ static noinline void copy_for_split(struct btrfs_trans_handle *trans,
 			   nritems * sizeof(struct btrfs_item));
 
 	copy_extent_buffer(right, l,
-		     btrfs_leaf_data(right) + BTRFS_LEAF_DATA_SIZE(fs_info) -
-		     data_copy_size, btrfs_leaf_data(l) +
+		     BTRFS_LEAF_DATA_OFFSET + BTRFS_LEAF_DATA_SIZE(fs_info) -
+		     data_copy_size, BTRFS_LEAF_DATA_OFFSET +
 		     leaf_data_end(fs_info, l), data_copy_size);
 
 	rt_data_off = BTRFS_LEAF_DATA_SIZE(fs_info) - btrfs_item_end_nr(l, mid);
@@ -4607,8 +4607,8 @@ void btrfs_truncate_item(struct btrfs_fs_info *fs_info,
 
 	/* shift the data */
 	if (from_end) {
-		memmove_extent_buffer(leaf, btrfs_leaf_data(leaf) +
-			      data_end + size_diff, btrfs_leaf_data(leaf) +
+		memmove_extent_buffer(leaf, BTRFS_LEAF_DATA_OFFSET +
+			      data_end + size_diff, BTRFS_LEAF_DATA_OFFSET +
 			      data_end, old_data_start + new_size - data_end);
 	} else {
 		struct btrfs_disk_key disk_key;
@@ -4634,8 +4634,8 @@ void btrfs_truncate_item(struct btrfs_fs_info *fs_info,
 			}
 		}
 
-		memmove_extent_buffer(leaf, btrfs_leaf_data(leaf) +
-			      data_end + size_diff, btrfs_leaf_data(leaf) +
+		memmove_extent_buffer(leaf, BTRFS_LEAF_DATA_OFFSET +
+			      data_end + size_diff, BTRFS_LEAF_DATA_OFFSET +
 			      data_end, old_data_start - data_end);
 
 		offset = btrfs_disk_key_offset(&disk_key);
@@ -4707,8 +4707,8 @@ void btrfs_extend_item(struct btrfs_fs_info *fs_info, struct btrfs_path *path,
 	}
 
 	/* shift the data */
-	memmove_extent_buffer(leaf, btrfs_leaf_data(leaf) +
-		      data_end - data_size, btrfs_leaf_data(leaf) +
+	memmove_extent_buffer(leaf, BTRFS_LEAF_DATA_OFFSET +
+		      data_end - data_size, BTRFS_LEAF_DATA_OFFSET +
 		      data_end, old_data - data_end);
 
 	data_end = old_data;
@@ -4790,8 +4790,8 @@ void setup_items_for_insert(struct btrfs_root *root, struct btrfs_path *path,
 			      (nritems - slot) * sizeof(struct btrfs_item));
 
 		/* shift the data */
-		memmove_extent_buffer(leaf, btrfs_leaf_data(leaf) +
-			      data_end - total_data, btrfs_leaf_data(leaf) +
+		memmove_extent_buffer(leaf, BTRFS_LEAF_DATA_OFFSET +
+			      data_end - total_data, BTRFS_LEAF_DATA_OFFSET +
 			      data_end, old_data - data_end);
 		data_end = old_data;
 	}
@@ -4983,9 +4983,9 @@ int btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 	if (slot + nr != nritems) {
 		int data_end = leaf_data_end(fs_info, leaf);
 
-		memmove_extent_buffer(leaf, btrfs_leaf_data(leaf) +
+		memmove_extent_buffer(leaf, BTRFS_LEAF_DATA_OFFSET +
 			      data_end + dsize,
-			      btrfs_leaf_data(leaf) + data_end,
+			      BTRFS_LEAF_DATA_OFFSET + data_end,
 			      last_off - data_end);
 
 		for (i = slot + nr; i < nritems; i++) {

commit 1176032cb12bb89ad558a3e57e82f2f25b817eff
Merge: 56868a460b83 9bcaaea7418d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed May 10 08:33:17 2017 -0700

    Merge branch 'for-linus-4.12' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs
    
    Pull btrfs updates from Chris Mason:
     "This has fixes and cleanups Dave Sterba collected for the merge
      window.
    
      The biggest functional fixes are between btrfs raid5/6 and scrub, and
      raid5/6 and device replacement. Some of our pending qgroup fixes are
      included as well while I bash on the rest in testing.
    
      We also have the usual set of cleanups, including one that makes
      __btrfs_map_block() much more maintainable, and conversions from
      atomic_t to refcount_t"
    
    * 'for-linus-4.12' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/linux-btrfs: (71 commits)
      btrfs: fix the gfp_mask for the reada_zones radix tree
      Btrfs: fix reported number of inode blocks
      Btrfs: send, fix file hole not being preserved due to inline extent
      Btrfs: fix extent map leak during fallocate error path
      Btrfs: fix incorrect space accounting after failure to insert inline extent
      Btrfs: fix invalid attempt to free reserved space on failure to cow range
      btrfs: Handle delalloc error correctly to avoid ordered extent hang
      btrfs: Fix metadata underflow caused by btrfs_reloc_clone_csum error
      btrfs: check if the device is flush capable
      btrfs: delete unused member nobarriers
      btrfs: scrub: Fix RAID56 recovery race condition
      btrfs: scrub: Introduce full stripe lock for RAID56
      btrfs: Use ktime_get_real_ts for root ctime
      Btrfs: handle only applicable errors returned by btrfs_get_extent
      btrfs: qgroup: Fix qgroup corruption caused by inode_cache mount option
      btrfs: use q which is already obtained from bdev_get_queue
      Btrfs: switch to div64_u64 if with a u64 divisor
      Btrfs: update scrub_parity to use u64 stripe_len
      Btrfs: enable repair during read for raid56 profile
      btrfs: use clear_page where appropriate
      ...

commit 752ade68cbd81d0321dfecc188f655a945551b25
Author: Michal Hocko <mhocko@suse.com>
Date:   Mon May 8 15:57:27 2017 -0700

    treewide: use kv[mz]alloc* rather than opencoded variants
    
    There are many code paths opencoding kvmalloc.  Let's use the helper
    instead.  The main difference to kvmalloc is that those users are
    usually not considering all the aspects of the memory allocator.  E.g.
    allocation requests <= 32kB (with 4kB pages) are basically never failing
    and invoke OOM killer to satisfy the allocation.  This sounds too
    disruptive for something that has a reasonable fallback - the vmalloc.
    On the other hand those requests might fallback to vmalloc even when the
    memory allocator would succeed after several more reclaim/compaction
    attempts previously.  There is no guarantee something like that happens
    though.
    
    This patch converts many of those places to kv[mz]alloc* helpers because
    they are more conservative.
    
    Link: http://lkml.kernel.org/r/20170306103327.2766-2-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Reviewed-by: Boris Ostrovsky <boris.ostrovsky@oracle.com> # Xen bits
    Acked-by: Kees Cook <keescook@chromium.org>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Acked-by: Andreas Dilger <andreas.dilger@intel.com> # Lustre
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com> # KVM/s390
    Acked-by: Dan Williams <dan.j.williams@intel.com> # nvdim
    Acked-by: David Sterba <dsterba@suse.com> # btrfs
    Acked-by: Ilya Dryomov <idryomov@gmail.com> # Ceph
    Acked-by: Tariq Toukan <tariqt@mellanox.com> # mlx4
    Acked-by: Leon Romanovsky <leonro@mellanox.com> # mlx5
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: Anton Vorontsov <anton@enomsg.org>
    Cc: Colin Cross <ccross@android.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: Ben Skeggs <bskeggs@redhat.com>
    Cc: Kent Overstreet <kent.overstreet@gmail.com>
    Cc: Santosh Raspatur <santosh@chelsio.com>
    Cc: Hariprasad S <hariprasad@chelsio.com>
    Cc: Yishai Hadas <yishaih@mellanox.com>
    Cc: Oleg Drokin <oleg.drokin@intel.com>
    Cc: "Yan, Zheng" <zyan@redhat.com>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: Alexei Starovoitov <ast@kernel.org>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: David Miller <davem@davemloft.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 7dc8844037e0..1c3b6c54d5ee 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -5392,13 +5392,10 @@ int btrfs_compare_trees(struct btrfs_root *left_root,
 		goto out;
 	}
 
-	tmp_buf = kmalloc(fs_info->nodesize, GFP_KERNEL | __GFP_NOWARN);
+	tmp_buf = kvmalloc(fs_info->nodesize, GFP_KERNEL);
 	if (!tmp_buf) {
-		tmp_buf = vmalloc(fs_info->nodesize);
-		if (!tmp_buf) {
-			ret = -ENOMEM;
-			goto out;
-		}
+		ret = -ENOMEM;
+		goto out;
 	}
 
 	left_path->search_commit_root = 1;

commit bcc8e07f9ef1cb3a302bed1d41d27ca72eaacc33
Author: David Sterba <dsterba@suse.com>
Date:   Tue Mar 28 14:35:42 2017 +0200

    btrfs: sink GFP flags parameter to tree_mod_log_insert_root
    
    All (1) callers pass the same value.
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index d034d47c5470..165e7ec12af7 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -663,7 +663,7 @@ __tree_mod_log_free_eb(struct btrfs_fs_info *fs_info,
 static noinline int
 tree_mod_log_insert_root(struct btrfs_fs_info *fs_info,
 			 struct extent_buffer *old_root,
-			 struct extent_buffer *new_root, gfp_t flags,
+			 struct extent_buffer *new_root,
 			 int log_removal)
 {
 	struct tree_mod_elem *tm = NULL;
@@ -678,14 +678,14 @@ tree_mod_log_insert_root(struct btrfs_fs_info *fs_info,
 	if (log_removal && btrfs_header_level(old_root) > 0) {
 		nritems = btrfs_header_nritems(old_root);
 		tm_list = kcalloc(nritems, sizeof(struct tree_mod_elem *),
-				  flags);
+				  GFP_NOFS);
 		if (!tm_list) {
 			ret = -ENOMEM;
 			goto free_tms;
 		}
 		for (i = 0; i < nritems; i++) {
 			tm_list[i] = alloc_tree_mod_elem(old_root, i,
-			    MOD_LOG_KEY_REMOVE_WHILE_FREEING, flags);
+			    MOD_LOG_KEY_REMOVE_WHILE_FREEING, GFP_NOFS);
 			if (!tm_list[i]) {
 				ret = -ENOMEM;
 				goto free_tms;
@@ -693,7 +693,7 @@ tree_mod_log_insert_root(struct btrfs_fs_info *fs_info,
 		}
 	}
 
-	tm = kzalloc(sizeof(*tm), flags);
+	tm = kzalloc(sizeof(*tm), GFP_NOFS);
 	if (!tm) {
 		ret = -ENOMEM;
 		goto free_tms;
@@ -943,7 +943,7 @@ tree_mod_log_set_root_pointer(struct btrfs_root *root,
 {
 	int ret;
 	ret = tree_mod_log_insert_root(root->fs_info, root->node,
-				       new_root_node, GFP_NOFS, log_removal);
+				       new_root_node, log_removal);
 	BUG_ON(ret < 0);
 }
 

commit 176ef8f5e662a79471f437839cd9a0c5b95b1e8d
Author: David Sterba <dsterba@suse.com>
Date:   Tue Mar 28 14:35:01 2017 +0200

    btrfs: sink GFP flags parameter to tree_mod_log_insert_move
    
    All (1) callers pass the same value.
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 7dc8844037e0..d034d47c5470 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -567,7 +567,7 @@ tree_mod_log_insert_key(struct btrfs_fs_info *fs_info,
 static noinline int
 tree_mod_log_insert_move(struct btrfs_fs_info *fs_info,
 			 struct extent_buffer *eb, int dst_slot, int src_slot,
-			 int nr_items, gfp_t flags)
+			 int nr_items)
 {
 	struct tree_mod_elem *tm = NULL;
 	struct tree_mod_elem **tm_list = NULL;
@@ -578,11 +578,11 @@ tree_mod_log_insert_move(struct btrfs_fs_info *fs_info,
 	if (!tree_mod_need_log(fs_info, eb))
 		return 0;
 
-	tm_list = kcalloc(nr_items, sizeof(struct tree_mod_elem *), flags);
+	tm_list = kcalloc(nr_items, sizeof(struct tree_mod_elem *), GFP_NOFS);
 	if (!tm_list)
 		return -ENOMEM;
 
-	tm = kzalloc(sizeof(*tm), flags);
+	tm = kzalloc(sizeof(*tm), GFP_NOFS);
 	if (!tm) {
 		ret = -ENOMEM;
 		goto free_tms;
@@ -596,7 +596,7 @@ tree_mod_log_insert_move(struct btrfs_fs_info *fs_info,
 
 	for (i = 0; i + dst_slot < src_slot && i < nr_items; i++) {
 		tm_list[i] = alloc_tree_mod_elem(eb, i + dst_slot,
-		    MOD_LOG_KEY_REMOVE_WHILE_MOVING, flags);
+		    MOD_LOG_KEY_REMOVE_WHILE_MOVING, GFP_NOFS);
 		if (!tm_list[i]) {
 			ret = -ENOMEM;
 			goto free_tms;
@@ -873,7 +873,7 @@ tree_mod_log_eb_move(struct btrfs_fs_info *fs_info, struct extent_buffer *dst,
 {
 	int ret;
 	ret = tree_mod_log_insert_move(fs_info, dst, dst_offset, src_offset,
-				       nr_items, GFP_NOFS);
+				       nr_items);
 	BUG_ON(ret < 0);
 }
 

commit e9f467d028cd7d8bee2a4d6c4fb806caf8cd580b
Merge: ef6ebf324216 20a7db8ab3f2
Author: Chris Mason <clm@fb.com>
Date:   Tue Feb 28 14:35:09 2017 -0800

    Merge branch 'for-chris-4.11-part2' of git://git.kernel.org/pub/scm/linux/kernel/git/kdave/linux into for-linus-4.11

commit 047e5e17c1e3551ef0fb34b629c66bec0591db0d
Author: David Sterba <dsterba@suse.com>
Date:   Tue Feb 14 17:33:27 2017 +0100

    btrfs: remove BUG_ON from __tree_mod_log_insert
    
    All callers dereference the 'tm' parameter before it gets to this
    function, the NULL check does not make much sense here.
    
    Reviewed-by: Liu Bo <bo.li.liu@oracle.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 1192bc7d2ee7..2c3c943bfcdc 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -453,8 +453,6 @@ __tree_mod_log_insert(struct btrfs_fs_info *fs_info, struct tree_mod_elem *tm)
 	struct rb_node *parent = NULL;
 	struct tree_mod_elem *cur;
 
-	BUG_ON(!tm);
-
 	tm->seq = btrfs_inc_tree_mod_seq(fs_info);
 
 	tm_root = &fs_info->tree_mod_log;

commit 263d3995c93c6020576f6c93506412a0b9d1e932
Author: Filipe Manana <fdmanana@suse.com>
Date:   Fri Feb 17 18:43:57 2017 +0000

    Btrfs: try harder to migrate items to left sibling before splitting a leaf
    
    Before attempting to split a leaf we try to migrate items from the leaf to
    its right and left siblings. We start by trying to move items into the
    rigth sibling and, if the new item is meant to be inserted at the end of
    our leaf, we try to free from our leaf an amount of bytes equal to the
    number of bytes used by the new item, by setting the variable space_needed
    to the byte size of that new item. However if we fail to move enough items
    to the right sibling due to lack of space in that sibling, we then try
    to move items into the left sibling, and in that case we try to free
    an amount equal to the size of the new item from our leaf, when we need
    only to free an amount corresponding to the size of the new item minus
    the current free space of our leaf. So make sure that before we try to
    move items to the left sibling we do set the variable space_needed with
    a value corresponding to the new item's size minus the leaf's current
    free space.
    
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Reviewed-by: Liu Bo <bo.li.liu@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 1192bc7d2ee7..1fb60ee77b4a 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -4159,6 +4159,9 @@ static noinline int push_for_double_split(struct btrfs_trans_handle *trans,
 
 	/* try to push all the items before our slot into the next leaf */
 	slot = path->slots[0];
+	space_needed = data_size;
+	if (slot > 0)
+		space_needed -= btrfs_leaf_free_space(fs_info, path->nodes[0]);
 	ret = push_leaf_left(trans, root, path, 1, space_needed, 0, slot);
 	if (ret < 0)
 		return ret;
@@ -4214,6 +4217,10 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 		if (wret < 0)
 			return wret;
 		if (wret) {
+			space_needed = data_size;
+			if (slot > 0)
+				space_needed -= btrfs_leaf_free_space(fs_info,
+								      l);
 			wret = push_leaf_left(trans, root, path, space_needed,
 					      space_needed, 0, (u32)-1);
 			if (wret < 0)

commit f1e30261925526990a880d1ef0e54c65a4607976
Author: David Sterba <dsterba@suse.com>
Date:   Fri Feb 10 19:25:51 2017 +0100

    btrfs: remove unused parameter from tree_move_next_or_upnext
    
    Not needed.
    
    Reviewed-by: Liu Bo <bo.li.liu@oracle.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index a981c2acd6fa..1192bc7d2ee7 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -5256,8 +5256,7 @@ static int tree_move_down(struct btrfs_fs_info *fs_info,
 	return 0;
 }
 
-static int tree_move_next_or_upnext(struct btrfs_fs_info *fs_info,
-				    struct btrfs_path *path,
+static int tree_move_next_or_upnext(struct btrfs_path *path,
 				    int *level, int root_level)
 {
 	int ret = 0;
@@ -5296,8 +5295,7 @@ static int tree_advance(struct btrfs_fs_info *fs_info,
 	int ret;
 
 	if (*level == 0 || !allow_down) {
-		ret = tree_move_next_or_upnext(fs_info, path, level,
-					       root_level);
+		ret = tree_move_next_or_upnext(path, level, root_level);
 	} else {
 		ret = tree_move_down(fs_info, path, level);
 	}

commit ab6a43e1221b55c1587a8e7a05deda61f4d25800
Author: David Sterba <dsterba@suse.com>
Date:   Fri Feb 10 19:24:53 2017 +0100

    btrfs: remove unused parameter from tree_move_down
    
    Never needed.
    
    Reviewed-by: Liu Bo <bo.li.liu@oracle.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index b98e90a3eee7..a981c2acd6fa 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -5241,7 +5241,7 @@ int btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,
 
 static int tree_move_down(struct btrfs_fs_info *fs_info,
 			   struct btrfs_path *path,
-			   int *level, int root_level)
+			   int *level)
 {
 	struct extent_buffer *eb;
 
@@ -5299,7 +5299,7 @@ static int tree_advance(struct btrfs_fs_info *fs_info,
 		ret = tree_move_next_or_upnext(fs_info, path, level,
 					       root_level);
 	} else {
-		ret = tree_move_down(fs_info, path, level, root_level);
+		ret = tree_move_down(fs_info, path, level);
 	}
 	if (ret >= 0) {
 		if (*level == 0)

commit 66cb7ddbf2a20517a6efb23960fadf698738e861
Author: David Sterba <dsterba@suse.com>
Date:   Fri Feb 10 19:14:36 2017 +0100

    btrfs: remove unused parameter from __push_leaf_left
    
    Unused since long ago.
    
    Reviewed-by: Liu Bo <bo.li.liu@oracle.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 2283f92b5484..b98e90a3eee7 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -3825,8 +3825,7 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
  * item at 'max_slot' won't be touched.  Use (u32)-1 to make us do all the
  * items
  */
-static noinline int __push_leaf_left(struct btrfs_trans_handle *trans,
-				     struct btrfs_fs_info *fs_info,
+static noinline int __push_leaf_left(struct btrfs_fs_info *fs_info,
 				     struct btrfs_path *path, int data_size,
 				     int empty, struct extent_buffer *left,
 				     int free_space, u32 right_nritems,
@@ -4035,7 +4034,7 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 		goto out;
 	}
 
-	return __push_leaf_left(trans, fs_info, path, min_data_size,
+	return __push_leaf_left(fs_info, path, min_data_size,
 			       empty, left, free_space, right_nritems,
 			       max_slot);
 out:

commit 1e47eef223bd53d54085832598afedf8bd6b5c77
Author: David Sterba <dsterba@suse.com>
Date:   Fri Feb 10 19:13:06 2017 +0100

    btrfs: remove unused parameter from __push_leaf_right
    
    Unused since long ago.
    
    Reviewed-by: Liu Bo <bo.li.liu@oracle.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 4c5d7c40c8bf..2283f92b5484 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -3595,8 +3595,7 @@ noinline int btrfs_leaf_free_space(struct btrfs_fs_info *fs_info,
  * min slot controls the lowest index we're willing to push to the
  * right.  We'll push up to and including min_slot, but no lower
  */
-static noinline int __push_leaf_right(struct btrfs_trans_handle *trans,
-				      struct btrfs_fs_info *fs_info,
+static noinline int __push_leaf_right(struct btrfs_fs_info *fs_info,
 				      struct btrfs_path *path,
 				      int data_size, int empty,
 				      struct extent_buffer *right,
@@ -3810,7 +3809,7 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 		return 0;
 	}
 
-	return __push_leaf_right(trans, fs_info, path, min_data_size, empty,
+	return __push_leaf_right(fs_info, path, min_data_size, empty,
 				right, free_space, left_nritems, min_slot);
 out_unlock:
 	btrfs_tree_unlock(right);

commit 4961e2930f0324dedd47f09108cd4c81c0ee4010
Author: David Sterba <dsterba@suse.com>
Date:   Fri Feb 10 18:49:53 2017 +0100

    btrfs: remove unused parameter from split_item
    
    Never used.
    
    Reviewed-by: Liu Bo <bo.li.liu@oracle.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index d509dafdb20c..4c5d7c40c8bf 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -4413,8 +4413,7 @@ static noinline int setup_leaf_for_split(struct btrfs_trans_handle *trans,
 	return ret;
 }
 
-static noinline int split_item(struct btrfs_trans_handle *trans,
-			       struct btrfs_fs_info *fs_info,
+static noinline int split_item(struct btrfs_fs_info *fs_info,
 			       struct btrfs_path *path,
 			       const struct btrfs_key *new_key,
 			       unsigned long split_offset)
@@ -4511,7 +4510,7 @@ int btrfs_split_item(struct btrfs_trans_handle *trans,
 	if (ret)
 		return ret;
 
-	ret = split_item(trans, root->fs_info, path, new_key, split_offset);
+	ret = split_item(root->fs_info, path, new_key, split_offset);
 	return ret;
 }
 

commit 7c302b49dd757b03d12bec772e1d1f462684c991
Author: David Sterba <dsterba@suse.com>
Date:   Fri Feb 10 18:47:57 2017 +0100

    btrfs: remove unused parameter from clean_tree_block
    
    Added but never needed.
    
    Reviewed-by: Liu Bo <bo.li.liu@oracle.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 35e22349c139..d509dafdb20c 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1074,7 +1074,7 @@ static noinline int update_ref_for_cow(struct btrfs_trans_handle *trans,
 			ret = btrfs_dec_ref(trans, root, buf, 1);
 			BUG_ON(ret); /* -ENOMEM */
 		}
-		clean_tree_block(trans, fs_info, buf);
+		clean_tree_block(fs_info, buf);
 		*last_ref = 1;
 	}
 	return 0;
@@ -1938,7 +1938,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 
 		path->locks[level] = 0;
 		path->nodes[level] = NULL;
-		clean_tree_block(trans, fs_info, mid);
+		clean_tree_block(fs_info, mid);
 		btrfs_tree_unlock(mid);
 		/* once for the path */
 		free_extent_buffer(mid);
@@ -1999,7 +1999,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		if (wret < 0 && wret != -ENOSPC)
 			ret = wret;
 		if (btrfs_header_nritems(right) == 0) {
-			clean_tree_block(trans, fs_info, right);
+			clean_tree_block(fs_info, right);
 			btrfs_tree_unlock(right);
 			del_ptr(root, path, level + 1, pslot + 1);
 			root_sub_used(root, right->len);
@@ -2043,7 +2043,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		BUG_ON(wret == 1);
 	}
 	if (btrfs_header_nritems(mid) == 0) {
-		clean_tree_block(trans, fs_info, mid);
+		clean_tree_block(fs_info, mid);
 		btrfs_tree_unlock(mid);
 		del_ptr(root, path, level + 1, pslot);
 		root_sub_used(root, mid->len);
@@ -3705,7 +3705,7 @@ static noinline int __push_leaf_right(struct btrfs_trans_handle *trans,
 	if (left_nritems)
 		btrfs_mark_buffer_dirty(left);
 	else
-		clean_tree_block(trans, fs_info, left);
+		clean_tree_block(fs_info, left);
 
 	btrfs_mark_buffer_dirty(right);
 
@@ -3717,7 +3717,7 @@ static noinline int __push_leaf_right(struct btrfs_trans_handle *trans,
 	if (path->slots[0] >= left_nritems) {
 		path->slots[0] -= left_nritems;
 		if (btrfs_header_nritems(path->nodes[0]) == 0)
-			clean_tree_block(trans, fs_info, path->nodes[0]);
+			clean_tree_block(fs_info, path->nodes[0]);
 		btrfs_tree_unlock(path->nodes[0]);
 		free_extent_buffer(path->nodes[0]);
 		path->nodes[0] = right;
@@ -3946,7 +3946,7 @@ static noinline int __push_leaf_left(struct btrfs_trans_handle *trans,
 	if (right_nritems)
 		btrfs_mark_buffer_dirty(right);
 	else
-		clean_tree_block(trans, fs_info, right);
+		clean_tree_block(fs_info, right);
 
 	btrfs_item_key(right, &disk_key, 0);
 	fixup_low_keys(fs_info, path, &disk_key, 1);
@@ -5009,7 +5009,7 @@ int btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 			btrfs_set_header_level(leaf, 0);
 		} else {
 			btrfs_set_path_blocking(path);
-			clean_tree_block(trans, fs_info, leaf);
+			clean_tree_block(fs_info, leaf);
 			btrfs_del_leaf(trans, root, path, leaf);
 		}
 	} else {

commit cda79c545ead7e00b1adaf82a13fcea892bf1f43
Author: David Sterba <dsterba@suse.com>
Date:   Fri Feb 10 18:44:32 2017 +0100

    btrfs: remove unused parameter from read_block_for_search
    
    Never used in that function.
    
    Reviewed-by: Liu Bo <bo.li.liu@oracle.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index fbeff20eb194..35e22349c139 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2440,7 +2440,7 @@ noinline void btrfs_unlock_up_safe(struct btrfs_path *path, int level)
 static int
 read_block_for_search(struct btrfs_root *root, struct btrfs_path *p,
 		      struct extent_buffer **eb_ret, int level, int slot,
-		      const struct btrfs_key *key, u64 time_seq)
+		      const struct btrfs_key *key)
 {
 	struct btrfs_fs_info *fs_info = root->fs_info;
 	u64 blocknr;
@@ -2871,7 +2871,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 			}
 
 			err = read_block_for_search(root, p, &b, level,
-						    slot, key, 0);
+						    slot, key);
 			if (err == -EAGAIN)
 				goto again;
 			if (err) {
@@ -3015,7 +3015,7 @@ int btrfs_search_old_slot(struct btrfs_root *root, const struct btrfs_key *key,
 			}
 
 			err = read_block_for_search(root, p, &b, level,
-						    slot, key, time_seq);
+						    slot, key);
 			if (err == -EAGAIN)
 				goto again;
 			if (err) {
@@ -5786,7 +5786,7 @@ int btrfs_next_old_leaf(struct btrfs_root *root, struct btrfs_path *path,
 		next = c;
 		next_rw_lock = path->locks[level];
 		ret = read_block_for_search(root, path, &next, level,
-					    slot, &key, 0);
+					    slot, &key);
 		if (ret == -EAGAIN)
 			goto again;
 
@@ -5836,7 +5836,7 @@ int btrfs_next_old_leaf(struct btrfs_root *root, struct btrfs_path *path,
 			break;
 
 		ret = read_block_for_search(root, path, &next, level,
-					    0, &key, 0);
+					    0, &key);
 		if (ret == -EAGAIN)
 			goto again;
 

commit d07b85284f83dee7955cb43dc66a37542a725c12
Author: Liu Bo <bo.li.liu@oracle.com>
Date:   Mon Jan 30 12:23:42 2017 -0800

    Btrfs: remove unused trans in read_block_for_search
    
    @trans is not used at all, this removes it.
    
    Signed-off-by: Liu Bo <bo.li.liu@oracle.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 72dd200f0478..fbeff20eb194 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2438,10 +2438,9 @@ noinline void btrfs_unlock_up_safe(struct btrfs_path *path, int level)
  * reada.  -EAGAIN is returned and the search must be repeated.
  */
 static int
-read_block_for_search(struct btrfs_trans_handle *trans,
-		       struct btrfs_root *root, struct btrfs_path *p,
-		       struct extent_buffer **eb_ret, int level, int slot,
-		       const struct btrfs_key *key, u64 time_seq)
+read_block_for_search(struct btrfs_root *root, struct btrfs_path *p,
+		      struct extent_buffer **eb_ret, int level, int slot,
+		      const struct btrfs_key *key, u64 time_seq)
 {
 	struct btrfs_fs_info *fs_info = root->fs_info;
 	u64 blocknr;
@@ -2871,8 +2870,8 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 				goto done;
 			}
 
-			err = read_block_for_search(trans, root, p,
-						    &b, level, slot, key, 0);
+			err = read_block_for_search(root, p, &b, level,
+						    slot, key, 0);
 			if (err == -EAGAIN)
 				goto again;
 			if (err) {
@@ -3015,7 +3014,7 @@ int btrfs_search_old_slot(struct btrfs_root *root, const struct btrfs_key *key,
 				goto done;
 			}
 
-			err = read_block_for_search(NULL, root, p, &b, level,
+			err = read_block_for_search(root, p, &b, level,
 						    slot, key, time_seq);
 			if (err == -EAGAIN)
 				goto again;
@@ -5786,7 +5785,7 @@ int btrfs_next_old_leaf(struct btrfs_root *root, struct btrfs_path *path,
 
 		next = c;
 		next_rw_lock = path->locks[level];
-		ret = read_block_for_search(NULL, root, path, &next, level,
+		ret = read_block_for_search(root, path, &next, level,
 					    slot, &key, 0);
 		if (ret == -EAGAIN)
 			goto again;
@@ -5836,7 +5835,7 @@ int btrfs_next_old_leaf(struct btrfs_root *root, struct btrfs_path *path,
 		if (!level)
 			break;
 
-		ret = read_block_for_search(NULL, root, path, &next, level,
+		ret = read_block_for_search(root, path, &next, level,
 					    0, &key, 0);
 		if (ret == -EAGAIN)
 			goto again;

commit 310712b2f73ac1da4c3a99fd9886e8b652727508
Author: Omar Sandoval <osandov@fb.com>
Date:   Tue Jan 17 23:24:37 2017 -0800

    Btrfs: constify struct btrfs_{,disk_}key wherever possible
    
    In a lot of places, it's unclear when it's safe to reuse a struct
    btrfs_key after it has been passed to a helper function. Constify these
    arguments wherever possible to make it obvious.
    
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 146b2dc0d2cf..72dd200f0478 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -28,9 +28,9 @@
 
 static int split_node(struct btrfs_trans_handle *trans, struct btrfs_root
 		      *root, struct btrfs_path *path, int level);
-static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
-		      *root, struct btrfs_key *ins_key,
-		      struct btrfs_path *path, int data_size, int extend);
+static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root *root,
+		      const struct btrfs_key *ins_key, struct btrfs_path *path,
+		      int data_size, int extend);
 static int push_node_left(struct btrfs_trans_handle *trans,
 			  struct btrfs_fs_info *fs_info,
 			  struct extent_buffer *dst,
@@ -1580,7 +1580,8 @@ static int close_blocks(u64 blocknr, u64 other, u32 blocksize)
 /*
  * compare two keys in a memcmp fashion
  */
-static int comp_keys(struct btrfs_disk_key *disk, struct btrfs_key *k2)
+static int comp_keys(const struct btrfs_disk_key *disk,
+		     const struct btrfs_key *k2)
 {
 	struct btrfs_key k1;
 
@@ -1592,7 +1593,7 @@ static int comp_keys(struct btrfs_disk_key *disk, struct btrfs_key *k2)
 /*
  * same as comp_keys only with two btrfs_key's
  */
-int btrfs_comp_cpu_keys(struct btrfs_key *k1, struct btrfs_key *k2)
+int btrfs_comp_cpu_keys(const struct btrfs_key *k1, const struct btrfs_key *k2)
 {
 	if (k1->objectid > k2->objectid)
 		return 1;
@@ -1732,8 +1733,8 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
  * slot may point to max if the key is bigger than all of the keys
  */
 static noinline int generic_bin_search(struct extent_buffer *eb,
-				       unsigned long p,
-				       int item_size, struct btrfs_key *key,
+				       unsigned long p, int item_size,
+				       const struct btrfs_key *key,
 				       int max, int *slot)
 {
 	int low = 0;
@@ -1802,7 +1803,7 @@ static noinline int generic_bin_search(struct extent_buffer *eb,
  * simple bin_search frontend that does the right thing for
  * leaves vs nodes
  */
-static int bin_search(struct extent_buffer *eb, struct btrfs_key *key,
+static int bin_search(struct extent_buffer *eb, const struct btrfs_key *key,
 		      int level, int *slot)
 {
 	if (level == 0)
@@ -1819,7 +1820,7 @@ static int bin_search(struct extent_buffer *eb, struct btrfs_key *key,
 					  slot);
 }
 
-int btrfs_bin_search(struct extent_buffer *eb, struct btrfs_key *key,
+int btrfs_bin_search(struct extent_buffer *eb, const struct btrfs_key *key,
 		     int level, int *slot)
 {
 	return bin_search(eb, key, level, slot);
@@ -2440,7 +2441,7 @@ static int
 read_block_for_search(struct btrfs_trans_handle *trans,
 		       struct btrfs_root *root, struct btrfs_path *p,
 		       struct extent_buffer **eb_ret, int level, int slot,
-		       struct btrfs_key *key, u64 time_seq)
+		       const struct btrfs_key *key, u64 time_seq)
 {
 	struct btrfs_fs_info *fs_info = root->fs_info;
 	u64 blocknr;
@@ -2587,7 +2588,7 @@ setup_nodes_for_search(struct btrfs_trans_handle *trans,
 }
 
 static void key_search_validate(struct extent_buffer *b,
-				struct btrfs_key *key,
+				const struct btrfs_key *key,
 				int level)
 {
 #ifdef CONFIG_BTRFS_ASSERT
@@ -2606,7 +2607,7 @@ static void key_search_validate(struct extent_buffer *b,
 #endif
 }
 
-static int key_search(struct extent_buffer *b, struct btrfs_key *key,
+static int key_search(struct extent_buffer *b, const struct btrfs_key *key,
 		      int level, int *prev_cmp, int *slot)
 {
 	if (*prev_cmp != 0) {
@@ -2668,9 +2669,9 @@ int btrfs_find_item(struct btrfs_root *fs_root, struct btrfs_path *path,
  * tree.  if ins_len < 0, nodes will be merged as we walk down the tree (if
  * possible)
  */
-int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
-		      *root, struct btrfs_key *key, struct btrfs_path *p, int
-		      ins_len, int cow)
+int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
+		      const struct btrfs_key *key, struct btrfs_path *p,
+		      int ins_len, int cow)
 {
 	struct btrfs_fs_info *fs_info = root->fs_info;
 	struct extent_buffer *b;
@@ -2953,7 +2954,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
  * The resulting path and return value will be set up as if we called
  * btrfs_search_slot at that point in time with ins_len and cow both set to 0.
  */
-int btrfs_search_old_slot(struct btrfs_root *root, struct btrfs_key *key,
+int btrfs_search_old_slot(struct btrfs_root *root, const struct btrfs_key *key,
 			  struct btrfs_path *p, u64 time_seq)
 {
 	struct btrfs_fs_info *fs_info = root->fs_info;
@@ -3067,8 +3068,9 @@ int btrfs_search_old_slot(struct btrfs_root *root, struct btrfs_key *key,
  * < 0 on error
  */
 int btrfs_search_slot_for_read(struct btrfs_root *root,
-			       struct btrfs_key *key, struct btrfs_path *p,
-			       int find_higher, int return_any)
+			       const struct btrfs_key *key,
+			       struct btrfs_path *p, int find_higher,
+			       int return_any)
 {
 	int ret;
 	struct extent_buffer *leaf;
@@ -3166,7 +3168,7 @@ static void fixup_low_keys(struct btrfs_fs_info *fs_info,
  */
 void btrfs_set_item_key_safe(struct btrfs_fs_info *fs_info,
 			     struct btrfs_path *path,
-			     struct btrfs_key *new_key)
+			     const struct btrfs_key *new_key)
 {
 	struct btrfs_disk_key disk_key;
 	struct extent_buffer *eb;
@@ -4180,7 +4182,7 @@ static noinline int push_for_double_split(struct btrfs_trans_handle *trans,
  */
 static noinline int split_leaf(struct btrfs_trans_handle *trans,
 			       struct btrfs_root *root,
-			       struct btrfs_key *ins_key,
+			       const struct btrfs_key *ins_key,
 			       struct btrfs_path *path, int data_size,
 			       int extend)
 {
@@ -4415,7 +4417,7 @@ static noinline int setup_leaf_for_split(struct btrfs_trans_handle *trans,
 static noinline int split_item(struct btrfs_trans_handle *trans,
 			       struct btrfs_fs_info *fs_info,
 			       struct btrfs_path *path,
-			       struct btrfs_key *new_key,
+			       const struct btrfs_key *new_key,
 			       unsigned long split_offset)
 {
 	struct extent_buffer *leaf;
@@ -4501,7 +4503,7 @@ static noinline int split_item(struct btrfs_trans_handle *trans,
 int btrfs_split_item(struct btrfs_trans_handle *trans,
 		     struct btrfs_root *root,
 		     struct btrfs_path *path,
-		     struct btrfs_key *new_key,
+		     const struct btrfs_key *new_key,
 		     unsigned long split_offset)
 {
 	int ret;
@@ -4525,7 +4527,7 @@ int btrfs_split_item(struct btrfs_trans_handle *trans,
 int btrfs_duplicate_item(struct btrfs_trans_handle *trans,
 			 struct btrfs_root *root,
 			 struct btrfs_path *path,
-			 struct btrfs_key *new_key)
+			 const struct btrfs_key *new_key)
 {
 	struct extent_buffer *leaf;
 	int ret;
@@ -4726,7 +4728,7 @@ void btrfs_extend_item(struct btrfs_fs_info *fs_info, struct btrfs_path *path,
  * that doesn't call btrfs_search_slot
  */
 void setup_items_for_insert(struct btrfs_root *root, struct btrfs_path *path,
-			    struct btrfs_key *cpu_key, u32 *data_size,
+			    const struct btrfs_key *cpu_key, u32 *data_size,
 			    u32 total_data, u32 total_size, int nr)
 {
 	struct btrfs_fs_info *fs_info = root->fs_info;
@@ -4820,7 +4822,7 @@ void setup_items_for_insert(struct btrfs_root *root, struct btrfs_path *path,
 int btrfs_insert_empty_items(struct btrfs_trans_handle *trans,
 			    struct btrfs_root *root,
 			    struct btrfs_path *path,
-			    struct btrfs_key *cpu_key, u32 *data_size,
+			    const struct btrfs_key *cpu_key, u32 *data_size,
 			    int nr)
 {
 	int ret = 0;
@@ -4851,9 +4853,9 @@ int btrfs_insert_empty_items(struct btrfs_trans_handle *trans,
  * Given a key and some data, insert an item into the tree.
  * This does all the path init required, making room in the tree if needed.
  */
-int btrfs_insert_item(struct btrfs_trans_handle *trans, struct btrfs_root
-		      *root, struct btrfs_key *cpu_key, void *data, u32
-		      data_size)
+int btrfs_insert_item(struct btrfs_trans_handle *trans, struct btrfs_root *root,
+		      const struct btrfs_key *cpu_key, void *data,
+		      u32 data_size)
 {
 	int ret = 0;
 	struct btrfs_path *path;

commit 6b4df8b6c59fc865deb2650b60e38aca4c67a972
Author: Geliang Tang <geliangtang@gmail.com>
Date:   Mon Dec 19 22:53:41 2016 +0800

    btrfs: use rb_entry() instead of container_of
    
    To make the code clearer, use rb_entry() instead of container_of() to
    deal with rbtree.
    
    Signed-off-by: Geliang Tang <geliangtang@gmail.com>
    Reviewed-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index a426dc822d4d..146b2dc0d2cf 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -426,7 +426,7 @@ void btrfs_put_tree_mod_seq(struct btrfs_fs_info *fs_info,
 	tm_root = &fs_info->tree_mod_log;
 	for (node = rb_first(tm_root); node; node = next) {
 		next = rb_next(node);
-		tm = container_of(node, struct tree_mod_elem, node);
+		tm = rb_entry(node, struct tree_mod_elem, node);
 		if (tm->seq > min_seq)
 			continue;
 		rb_erase(node, tm_root);
@@ -460,7 +460,7 @@ __tree_mod_log_insert(struct btrfs_fs_info *fs_info, struct tree_mod_elem *tm)
 	tm_root = &fs_info->tree_mod_log;
 	new = &tm_root->rb_node;
 	while (*new) {
-		cur = container_of(*new, struct tree_mod_elem, node);
+		cur = rb_entry(*new, struct tree_mod_elem, node);
 		parent = *new;
 		if (cur->logical < tm->logical)
 			new = &((*new)->rb_left);
@@ -746,7 +746,7 @@ __tree_mod_log_search(struct btrfs_fs_info *fs_info, u64 start, u64 min_seq,
 	tm_root = &fs_info->tree_mod_log;
 	node = tm_root->rb_node;
 	while (node) {
-		cur = container_of(node, struct tree_mod_elem, node);
+		cur = rb_entry(node, struct tree_mod_elem, node);
 		if (cur->logical < start) {
 			node = node->rb_left;
 		} else if (cur->logical > start) {
@@ -1326,7 +1326,7 @@ __tree_mod_log_rewind(struct btrfs_fs_info *fs_info, struct extent_buffer *eb,
 		next = rb_next(&tm->node);
 		if (!next)
 			break;
-		tm = container_of(next, struct tree_mod_elem, node);
+		tm = rb_entry(next, struct tree_mod_elem, node);
 		if (tm->logical != first_tm->logical)
 			break;
 	}

commit 2ff7e61e0d30ff166a2ae94575526bffe11fd1a8
Author: Jeff Mahoney <jeffm@suse.com>
Date:   Wed Jun 22 18:54:24 2016 -0400

    btrfs: take an fs_info directly when the root is not used otherwise
    
    There are loads of functions in btrfs that accept a root parameter
    but only use it to obtain an fs_info pointer.  Let's convert those to
    just accept an fs_info pointer directly.
    
    Signed-off-by: Jeff Mahoney <jeffm@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index b29c8d82e741..a426dc822d4d 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -32,10 +32,11 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 		      *root, struct btrfs_key *ins_key,
 		      struct btrfs_path *path, int data_size, int extend);
 static int push_node_left(struct btrfs_trans_handle *trans,
-			  struct btrfs_root *root, struct extent_buffer *dst,
+			  struct btrfs_fs_info *fs_info,
+			  struct extent_buffer *dst,
 			  struct extent_buffer *src, int empty);
 static int balance_node_right(struct btrfs_trans_handle *trans,
-			      struct btrfs_root *root,
+			      struct btrfs_fs_info *fs_info,
 			      struct extent_buffer *dst_buf,
 			      struct extent_buffer *src_buf);
 static void del_ptr(struct btrfs_root *root, struct btrfs_path *path,
@@ -1005,7 +1006,7 @@ static noinline int update_ref_for_cow(struct btrfs_trans_handle *trans,
 	 */
 
 	if (btrfs_block_can_be_shared(root, buf)) {
-		ret = btrfs_lookup_extent_info(trans, root, buf->start,
+		ret = btrfs_lookup_extent_info(trans, fs_info, buf->start,
 					       btrfs_header_level(buf), 1,
 					       &refs, &flags);
 		if (ret)
@@ -1055,7 +1056,7 @@ static noinline int update_ref_for_cow(struct btrfs_trans_handle *trans,
 		if (new_flags != 0) {
 			int level = btrfs_header_level(buf);
 
-			ret = btrfs_set_disk_extent_flags(trans, root,
+			ret = btrfs_set_disk_extent_flags(trans, fs_info,
 							  buf->start,
 							  buf->len,
 							  new_flags, level, 0);
@@ -1431,7 +1432,7 @@ get_old_root(struct btrfs_root *root, u64 time_seq)
 	if (old_root && tm && tm->op != MOD_LOG_KEY_REMOVE_WHILE_FREEING) {
 		btrfs_tree_read_unlock(eb_root);
 		free_extent_buffer(eb_root);
-		old = read_tree_block(root, logical, 0);
+		old = read_tree_block(fs_info, logical, 0);
 		if (WARN_ON(IS_ERR(old) || !extent_buffer_uptodate(old))) {
 			if (!IS_ERR(old))
 				free_extent_buffer(old);
@@ -1682,7 +1683,7 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 			uptodate = 0;
 		if (!cur || !uptodate) {
 			if (!cur) {
-				cur = read_tree_block(root, blocknr, gen);
+				cur = read_tree_block(fs_info, blocknr, gen);
 				if (IS_ERR(cur)) {
 					return PTR_ERR(cur);
 				} else if (!extent_buffer_uptodate(cur)) {
@@ -1843,8 +1844,9 @@ static void root_sub_used(struct btrfs_root *root, u32 size)
 /* given a node and slot number, this reads the blocks it points to.  The
  * extent buffer is returned with a reference taken (but unlocked).
  */
-static noinline struct extent_buffer *read_node_slot(struct btrfs_root *root,
-				   struct extent_buffer *parent, int slot)
+static noinline struct extent_buffer *
+read_node_slot(struct btrfs_fs_info *fs_info, struct extent_buffer *parent,
+	       int slot)
 {
 	int level = btrfs_header_level(parent);
 	struct extent_buffer *eb;
@@ -1854,7 +1856,7 @@ static noinline struct extent_buffer *read_node_slot(struct btrfs_root *root,
 
 	BUG_ON(level == 0);
 
-	eb = read_tree_block(root, btrfs_node_blockptr(parent, slot),
+	eb = read_tree_block(fs_info, btrfs_node_blockptr(parent, slot),
 			     btrfs_node_ptr_generation(parent, slot));
 	if (!IS_ERR(eb) && !extent_buffer_uptodate(eb)) {
 		free_extent_buffer(eb);
@@ -1911,7 +1913,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 			return 0;
 
 		/* promote the child to a root */
-		child = read_node_slot(root, mid, 0);
+		child = read_node_slot(fs_info, mid, 0);
 		if (IS_ERR(child)) {
 			ret = PTR_ERR(child);
 			btrfs_handle_fs_error(fs_info, ret, NULL);
@@ -1950,7 +1952,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 	    BTRFS_NODEPTRS_PER_BLOCK(fs_info) / 4)
 		return 0;
 
-	left = read_node_slot(root, parent, pslot - 1);
+	left = read_node_slot(fs_info, parent, pslot - 1);
 	if (IS_ERR(left))
 		left = NULL;
 
@@ -1965,7 +1967,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		}
 	}
 
-	right = read_node_slot(root, parent, pslot + 1);
+	right = read_node_slot(fs_info, parent, pslot + 1);
 	if (IS_ERR(right))
 		right = NULL;
 
@@ -1983,7 +1985,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 	/* first, try to make some room in the middle buffer */
 	if (left) {
 		orig_slot += btrfs_header_nritems(left);
-		wret = push_node_left(trans, root, left, mid, 1);
+		wret = push_node_left(trans, fs_info, left, mid, 1);
 		if (wret < 0)
 			ret = wret;
 	}
@@ -1992,7 +1994,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 	 * then try to empty the right most buffer into the middle
 	 */
 	if (right) {
-		wret = push_node_left(trans, root, mid, right, 1);
+		wret = push_node_left(trans, fs_info, mid, right, 1);
 		if (wret < 0 && wret != -ENOSPC)
 			ret = wret;
 		if (btrfs_header_nritems(right) == 0) {
@@ -2027,13 +2029,13 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 			btrfs_handle_fs_error(fs_info, ret, NULL);
 			goto enospc;
 		}
-		wret = balance_node_right(trans, root, mid, left);
+		wret = balance_node_right(trans, fs_info, mid, left);
 		if (wret < 0) {
 			ret = wret;
 			goto enospc;
 		}
 		if (wret == 1) {
-			wret = push_node_left(trans, root, left, mid, 1);
+			wret = push_node_left(trans, fs_info, left, mid, 1);
 			if (wret < 0)
 				ret = wret;
 		}
@@ -2122,7 +2124,7 @@ static noinline int push_nodes_for_insert(struct btrfs_trans_handle *trans,
 	if (!parent)
 		return 1;
 
-	left = read_node_slot(root, parent, pslot - 1);
+	left = read_node_slot(fs_info, parent, pslot - 1);
 	if (IS_ERR(left))
 		left = NULL;
 
@@ -2142,7 +2144,7 @@ static noinline int push_nodes_for_insert(struct btrfs_trans_handle *trans,
 			if (ret)
 				wret = 1;
 			else {
-				wret = push_node_left(trans, root,
+				wret = push_node_left(trans, fs_info,
 						      left, mid, 0);
 			}
 		}
@@ -2173,7 +2175,7 @@ static noinline int push_nodes_for_insert(struct btrfs_trans_handle *trans,
 		btrfs_tree_unlock(left);
 		free_extent_buffer(left);
 	}
-	right = read_node_slot(root, parent, pslot + 1);
+	right = read_node_slot(fs_info, parent, pslot + 1);
 	if (IS_ERR(right))
 		right = NULL;
 
@@ -2196,7 +2198,7 @@ static noinline int push_nodes_for_insert(struct btrfs_trans_handle *trans,
 			if (ret)
 				wret = 1;
 			else {
-				wret = balance_node_right(trans, root,
+				wret = balance_node_right(trans, fs_info,
 							  right, mid);
 			}
 		}
@@ -2234,11 +2236,10 @@ static noinline int push_nodes_for_insert(struct btrfs_trans_handle *trans,
  * readahead one full node of leaves, finding things that are close
  * to the block in 'slot', and triggering ra on them.
  */
-static void reada_for_search(struct btrfs_root *root,
+static void reada_for_search(struct btrfs_fs_info *fs_info,
 			     struct btrfs_path *path,
 			     int level, int slot, u64 objectid)
 {
-	struct btrfs_fs_info *fs_info = root->fs_info;
 	struct extent_buffer *node;
 	struct btrfs_disk_key disk_key;
 	u32 nritems;
@@ -2289,7 +2290,7 @@ static void reada_for_search(struct btrfs_root *root,
 		search = btrfs_node_blockptr(node, nr);
 		if ((search <= target && target - search <= 65536) ||
 		    (search > target && search - target <= 65536)) {
-			readahead_tree_block(root, search);
+			readahead_tree_block(fs_info, search);
 			nread += blocksize;
 		}
 		nscan++;
@@ -2298,10 +2299,9 @@ static void reada_for_search(struct btrfs_root *root,
 	}
 }
 
-static noinline void reada_for_balance(struct btrfs_root *root,
+static noinline void reada_for_balance(struct btrfs_fs_info *fs_info,
 				       struct btrfs_path *path, int level)
 {
-	struct btrfs_fs_info *fs_info = root->fs_info;
 	int slot;
 	int nritems;
 	struct extent_buffer *parent;
@@ -2340,9 +2340,9 @@ static noinline void reada_for_balance(struct btrfs_root *root,
 	}
 
 	if (block1)
-		readahead_tree_block(root, block1);
+		readahead_tree_block(fs_info, block1);
 	if (block2)
-		readahead_tree_block(root, block2);
+		readahead_tree_block(fs_info, block2);
 }
 
 
@@ -2491,12 +2491,12 @@ read_block_for_search(struct btrfs_trans_handle *trans,
 
 	free_extent_buffer(tmp);
 	if (p->reada != READA_NONE)
-		reada_for_search(root, p, level, slot, key->objectid);
+		reada_for_search(fs_info, p, level, slot, key->objectid);
 
 	btrfs_release_path(p);
 
 	ret = -EAGAIN;
-	tmp = read_tree_block(root, blocknr, 0);
+	tmp = read_tree_block(fs_info, blocknr, 0);
 	if (!IS_ERR(tmp)) {
 		/*
 		 * If the read above didn't mark this buffer up to date,
@@ -2542,7 +2542,7 @@ setup_nodes_for_search(struct btrfs_trans_handle *trans,
 		}
 
 		btrfs_set_path_blocking(p);
-		reada_for_balance(root, p, level);
+		reada_for_balance(fs_info, p, level);
 		sret = split_node(trans, root, p, level);
 		btrfs_clear_path_blocking(p, NULL, 0);
 
@@ -2563,7 +2563,7 @@ setup_nodes_for_search(struct btrfs_trans_handle *trans,
 		}
 
 		btrfs_set_path_blocking(p);
-		reada_for_balance(root, p, level);
+		reada_for_balance(fs_info, p, level);
 		sret = balance_level(trans, root, p, level);
 		btrfs_clear_path_blocking(p, NULL, 0);
 
@@ -2905,7 +2905,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 		} else {
 			p->slots[level] = slot;
 			if (ins_len > 0 &&
-			    btrfs_leaf_free_space(root, b) < ins_len) {
+			    btrfs_leaf_free_space(fs_info, b) < ins_len) {
 				if (write_lock_level < 1) {
 					write_lock_level = 1;
 					btrfs_release_path(p);
@@ -3198,10 +3198,10 @@ void btrfs_set_item_key_safe(struct btrfs_fs_info *fs_info,
  * error, and > 0 if there was no room in the left hand block.
  */
 static int push_node_left(struct btrfs_trans_handle *trans,
-			  struct btrfs_root *root, struct extent_buffer *dst,
+			  struct btrfs_fs_info *fs_info,
+			  struct extent_buffer *dst,
 			  struct extent_buffer *src, int empty)
 {
-	struct btrfs_fs_info *fs_info = root->fs_info;
 	int push_items = 0;
 	int src_nritems;
 	int dst_nritems;
@@ -3273,11 +3273,10 @@ static int push_node_left(struct btrfs_trans_handle *trans,
  * this will  only push up to 1/2 the contents of the left node over
  */
 static int balance_node_right(struct btrfs_trans_handle *trans,
-			      struct btrfs_root *root,
+			      struct btrfs_fs_info *fs_info,
 			      struct extent_buffer *dst,
 			      struct extent_buffer *src)
 {
-	struct btrfs_fs_info *fs_info = root->fs_info;
 	int push_items = 0;
 	int max_push;
 	int src_nritems;
@@ -3407,11 +3406,10 @@ static noinline int insert_new_root(struct btrfs_trans_handle *trans,
  * blocknr is the block the key points to.
  */
 static void insert_ptr(struct btrfs_trans_handle *trans,
-		       struct btrfs_root *root, struct btrfs_path *path,
+		       struct btrfs_fs_info *fs_info, struct btrfs_path *path,
 		       struct btrfs_disk_key *key, u64 bytenr,
 		       int slot, int level)
 {
-	struct btrfs_fs_info *fs_info = root->fs_info;
 	struct extent_buffer *lower;
 	int nritems;
 	int ret;
@@ -3527,7 +3525,7 @@ static noinline int split_node(struct btrfs_trans_handle *trans,
 	btrfs_mark_buffer_dirty(c);
 	btrfs_mark_buffer_dirty(split);
 
-	insert_ptr(trans, root, path, &disk_key, split->start,
+	insert_ptr(trans, fs_info, path, &disk_key, split->start,
 		   path->slots[level + 1] + 1, level + 1);
 
 	if (path->slots[level] >= mid) {
@@ -3575,10 +3573,9 @@ static int leaf_space_used(struct extent_buffer *l, int start, int nr)
  * the start of the leaf data.  IOW, how much room
  * the leaf has left for both items and data
  */
-noinline int btrfs_leaf_free_space(struct btrfs_root *root,
+noinline int btrfs_leaf_free_space(struct btrfs_fs_info *fs_info,
 				   struct extent_buffer *leaf)
 {
-	struct btrfs_fs_info *fs_info = root->fs_info;
 	int nritems = btrfs_header_nritems(leaf);
 	int ret;
 
@@ -3598,14 +3595,13 @@ noinline int btrfs_leaf_free_space(struct btrfs_root *root,
  * right.  We'll push up to and including min_slot, but no lower
  */
 static noinline int __push_leaf_right(struct btrfs_trans_handle *trans,
-				      struct btrfs_root *root,
+				      struct btrfs_fs_info *fs_info,
 				      struct btrfs_path *path,
 				      int data_size, int empty,
 				      struct extent_buffer *right,
 				      int free_space, u32 left_nritems,
 				      u32 min_slot)
 {
-	struct btrfs_fs_info *fs_info = root->fs_info;
 	struct extent_buffer *left = path->nodes[0];
 	struct extent_buffer *upper = path->nodes[1];
 	struct btrfs_map_token token;
@@ -3639,7 +3635,7 @@ static noinline int __push_leaf_right(struct btrfs_trans_handle *trans,
 			if (path->slots[0] > i)
 				break;
 			if (path->slots[0] == i) {
-				int space = btrfs_leaf_free_space(root, left);
+				int space = btrfs_leaf_free_space(fs_info, left);
 				if (space + push_space * 2 > free_space)
 					break;
 			}
@@ -3668,10 +3664,10 @@ static noinline int __push_leaf_right(struct btrfs_trans_handle *trans,
 	right_nritems = btrfs_header_nritems(right);
 
 	push_space = btrfs_item_end_nr(left, left_nritems - push_items);
-	push_space -= leaf_data_end(root, left);
+	push_space -= leaf_data_end(fs_info, left);
 
 	/* make room in the right data area */
-	data_end = leaf_data_end(root, right);
+	data_end = leaf_data_end(fs_info, right);
 	memmove_extent_buffer(right,
 			      btrfs_leaf_data(right) + data_end - push_space,
 			      btrfs_leaf_data(right) + data_end,
@@ -3680,7 +3676,7 @@ static noinline int __push_leaf_right(struct btrfs_trans_handle *trans,
 	/* copy from the left data area */
 	copy_extent_buffer(right, left, btrfs_leaf_data(right) +
 		     BTRFS_LEAF_DATA_SIZE(fs_info) - push_space,
-		     btrfs_leaf_data(left) + leaf_data_end(root, left),
+		     btrfs_leaf_data(left) + leaf_data_end(fs_info, left),
 		     push_space);
 
 	memmove_extent_buffer(right, btrfs_item_nr_offset(push_items),
@@ -3752,6 +3748,7 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 			   int min_data_size, int data_size,
 			   int empty, u32 min_slot)
 {
+	struct btrfs_fs_info *fs_info = root->fs_info;
 	struct extent_buffer *left = path->nodes[0];
 	struct extent_buffer *right;
 	struct extent_buffer *upper;
@@ -3770,7 +3767,7 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 
 	btrfs_assert_tree_locked(path->nodes[1]);
 
-	right = read_node_slot(root, upper, slot + 1);
+	right = read_node_slot(fs_info, upper, slot + 1);
 	/*
 	 * slot + 1 is not valid or we fail to read the right node,
 	 * no big deal, just return.
@@ -3781,7 +3778,7 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 	btrfs_tree_lock(right);
 	btrfs_set_lock_blocking(right);
 
-	free_space = btrfs_leaf_free_space(root, right);
+	free_space = btrfs_leaf_free_space(fs_info, right);
 	if (free_space < data_size)
 		goto out_unlock;
 
@@ -3791,7 +3788,7 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 	if (ret)
 		goto out_unlock;
 
-	free_space = btrfs_leaf_free_space(root, right);
+	free_space = btrfs_leaf_free_space(fs_info, right);
 	if (free_space < data_size)
 		goto out_unlock;
 
@@ -3812,7 +3809,7 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 		return 0;
 	}
 
-	return __push_leaf_right(trans, root, path, min_data_size, empty,
+	return __push_leaf_right(trans, fs_info, path, min_data_size, empty,
 				right, free_space, left_nritems, min_slot);
 out_unlock:
 	btrfs_tree_unlock(right);
@@ -3829,13 +3826,12 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
  * items
  */
 static noinline int __push_leaf_left(struct btrfs_trans_handle *trans,
-				     struct btrfs_root *root,
+				     struct btrfs_fs_info *fs_info,
 				     struct btrfs_path *path, int data_size,
 				     int empty, struct extent_buffer *left,
 				     int free_space, u32 right_nritems,
 				     u32 max_slot)
 {
-	struct btrfs_fs_info *fs_info = root->fs_info;
 	struct btrfs_disk_key disk_key;
 	struct extent_buffer *right = path->nodes[0];
 	int i;
@@ -3863,7 +3859,7 @@ static noinline int __push_leaf_left(struct btrfs_trans_handle *trans,
 			if (path->slots[0] < i)
 				break;
 			if (path->slots[0] == i) {
-				int space = btrfs_leaf_free_space(root, right);
+				int space = btrfs_leaf_free_space(fs_info, right);
 				if (space + push_space * 2 > free_space)
 					break;
 			}
@@ -3896,7 +3892,7 @@ static noinline int __push_leaf_left(struct btrfs_trans_handle *trans,
 		     btrfs_item_offset_nr(right, push_items - 1);
 
 	copy_extent_buffer(left, right, btrfs_leaf_data(left) +
-		     leaf_data_end(root, left) - push_space,
+		     leaf_data_end(fs_info, left) - push_space,
 		     btrfs_leaf_data(right) +
 		     btrfs_item_offset_nr(right, push_items - 1),
 		     push_space);
@@ -3923,11 +3919,11 @@ static noinline int __push_leaf_left(struct btrfs_trans_handle *trans,
 
 	if (push_items < right_nritems) {
 		push_space = btrfs_item_offset_nr(right, push_items - 1) -
-						  leaf_data_end(root, right);
+						  leaf_data_end(fs_info, right);
 		memmove_extent_buffer(right, btrfs_leaf_data(right) +
 				      BTRFS_LEAF_DATA_SIZE(fs_info) - push_space,
 				      btrfs_leaf_data(right) +
-				      leaf_data_end(root, right), push_space);
+				      leaf_data_end(fs_info, right), push_space);
 
 		memmove_extent_buffer(right, btrfs_item_nr_offset(0),
 			      btrfs_item_nr_offset(push_items),
@@ -3986,6 +3982,7 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 			  *root, struct btrfs_path *path, int min_data_size,
 			  int data_size, int empty, u32 max_slot)
 {
+	struct btrfs_fs_info *fs_info = root->fs_info;
 	struct extent_buffer *right = path->nodes[0];
 	struct extent_buffer *left;
 	int slot;
@@ -4005,7 +4002,7 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 
 	btrfs_assert_tree_locked(path->nodes[1]);
 
-	left = read_node_slot(root, path->nodes[1], slot - 1);
+	left = read_node_slot(fs_info, path->nodes[1], slot - 1);
 	/*
 	 * slot - 1 is not valid or we fail to read the left node,
 	 * no big deal, just return.
@@ -4016,7 +4013,7 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 	btrfs_tree_lock(left);
 	btrfs_set_lock_blocking(left);
 
-	free_space = btrfs_leaf_free_space(root, left);
+	free_space = btrfs_leaf_free_space(fs_info, left);
 	if (free_space < data_size) {
 		ret = 1;
 		goto out;
@@ -4032,13 +4029,13 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 		goto out;
 	}
 
-	free_space = btrfs_leaf_free_space(root, left);
+	free_space = btrfs_leaf_free_space(fs_info, left);
 	if (free_space < data_size) {
 		ret = 1;
 		goto out;
 	}
 
-	return __push_leaf_left(trans, root, path, min_data_size,
+	return __push_leaf_left(trans, fs_info, path, min_data_size,
 			       empty, left, free_space, right_nritems,
 			       max_slot);
 out:
@@ -4052,13 +4049,12 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
  * available for the resulting leaf level of the path.
  */
 static noinline void copy_for_split(struct btrfs_trans_handle *trans,
-				    struct btrfs_root *root,
+				    struct btrfs_fs_info *fs_info,
 				    struct btrfs_path *path,
 				    struct extent_buffer *l,
 				    struct extent_buffer *right,
 				    int slot, int mid, int nritems)
 {
-	struct btrfs_fs_info *fs_info = root->fs_info;
 	int data_copy_size;
 	int rt_data_off;
 	int i;
@@ -4069,7 +4065,7 @@ static noinline void copy_for_split(struct btrfs_trans_handle *trans,
 
 	nritems = nritems - mid;
 	btrfs_set_header_nritems(right, nritems);
-	data_copy_size = btrfs_item_end_nr(l, mid) - leaf_data_end(root, l);
+	data_copy_size = btrfs_item_end_nr(l, mid) - leaf_data_end(fs_info, l);
 
 	copy_extent_buffer(right, l, btrfs_item_nr_offset(0),
 			   btrfs_item_nr_offset(mid),
@@ -4078,7 +4074,7 @@ static noinline void copy_for_split(struct btrfs_trans_handle *trans,
 	copy_extent_buffer(right, l,
 		     btrfs_leaf_data(right) + BTRFS_LEAF_DATA_SIZE(fs_info) -
 		     data_copy_size, btrfs_leaf_data(l) +
-		     leaf_data_end(root, l), data_copy_size);
+		     leaf_data_end(fs_info, l), data_copy_size);
 
 	rt_data_off = BTRFS_LEAF_DATA_SIZE(fs_info) - btrfs_item_end_nr(l, mid);
 
@@ -4093,7 +4089,7 @@ static noinline void copy_for_split(struct btrfs_trans_handle *trans,
 
 	btrfs_set_header_nritems(l, mid);
 	btrfs_item_key(right, &disk_key, 0);
-	insert_ptr(trans, root, path, &disk_key, right->start,
+	insert_ptr(trans, fs_info, path, &disk_key, right->start,
 		   path->slots[1] + 1, 1);
 
 	btrfs_mark_buffer_dirty(right);
@@ -4129,6 +4125,7 @@ static noinline int push_for_double_split(struct btrfs_trans_handle *trans,
 					  struct btrfs_path *path,
 					  int data_size)
 {
+	struct btrfs_fs_info *fs_info = root->fs_info;
 	int ret;
 	int progress = 0;
 	int slot;
@@ -4137,7 +4134,7 @@ static noinline int push_for_double_split(struct btrfs_trans_handle *trans,
 
 	slot = path->slots[0];
 	if (slot < btrfs_header_nritems(path->nodes[0]))
-		space_needed -= btrfs_leaf_free_space(root, path->nodes[0]);
+		space_needed -= btrfs_leaf_free_space(fs_info, path->nodes[0]);
 
 	/*
 	 * try to push all the items after our slot into the
@@ -4158,7 +4155,7 @@ static noinline int push_for_double_split(struct btrfs_trans_handle *trans,
 	if (path->slots[0] == 0 || path->slots[0] == nritems)
 		return 0;
 
-	if (btrfs_leaf_free_space(root, path->nodes[0]) >= data_size)
+	if (btrfs_leaf_free_space(fs_info, path->nodes[0]) >= data_size)
 		return 0;
 
 	/* try to push all the items before our slot into the next leaf */
@@ -4211,7 +4208,7 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 		int space_needed = data_size;
 
 		if (slot < btrfs_header_nritems(l))
-			space_needed -= btrfs_leaf_free_space(root, l);
+			space_needed -= btrfs_leaf_free_space(fs_info, l);
 
 		wret = push_leaf_right(trans, root, path, space_needed,
 				       space_needed, 0, 0);
@@ -4226,7 +4223,7 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 		l = path->nodes[0];
 
 		/* did the pushes work? */
-		if (btrfs_leaf_free_space(root, l) >= data_size)
+		if (btrfs_leaf_free_space(fs_info, l) >= data_size)
 			return 0;
 	}
 
@@ -4303,8 +4300,8 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 	if (split == 0) {
 		if (mid <= slot) {
 			btrfs_set_header_nritems(right, 0);
-			insert_ptr(trans, root, path, &disk_key, right->start,
-				   path->slots[1] + 1, 1);
+			insert_ptr(trans, fs_info, path, &disk_key,
+				   right->start, path->slots[1] + 1, 1);
 			btrfs_tree_unlock(path->nodes[0]);
 			free_extent_buffer(path->nodes[0]);
 			path->nodes[0] = right;
@@ -4312,8 +4309,8 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 			path->slots[1] += 1;
 		} else {
 			btrfs_set_header_nritems(right, 0);
-			insert_ptr(trans, root, path, &disk_key, right->start,
-					  path->slots[1], 1);
+			insert_ptr(trans, fs_info, path, &disk_key,
+				   right->start, path->slots[1], 1);
 			btrfs_tree_unlock(path->nodes[0]);
 			free_extent_buffer(path->nodes[0]);
 			path->nodes[0] = right;
@@ -4329,7 +4326,7 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 		return ret;
 	}
 
-	copy_for_split(trans, root, path, l, right, slot, mid, nritems);
+	copy_for_split(trans, fs_info, path, l, right, slot, mid, nritems);
 
 	if (split == 2) {
 		BUG_ON(num_doubles != 0);
@@ -4342,7 +4339,7 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 push_for_double:
 	push_for_double_split(trans, root, path, data_size);
 	tried_avoid_double = 1;
-	if (btrfs_leaf_free_space(root, path->nodes[0]) >= data_size)
+	if (btrfs_leaf_free_space(fs_info, path->nodes[0]) >= data_size)
 		return 0;
 	goto again;
 }
@@ -4351,6 +4348,7 @@ static noinline int setup_leaf_for_split(struct btrfs_trans_handle *trans,
 					 struct btrfs_root *root,
 					 struct btrfs_path *path, int ins_len)
 {
+	struct btrfs_fs_info *fs_info = root->fs_info;
 	struct btrfs_key key;
 	struct extent_buffer *leaf;
 	struct btrfs_file_extent_item *fi;
@@ -4364,7 +4362,7 @@ static noinline int setup_leaf_for_split(struct btrfs_trans_handle *trans,
 	BUG_ON(key.type != BTRFS_EXTENT_DATA_KEY &&
 	       key.type != BTRFS_EXTENT_CSUM_KEY);
 
-	if (btrfs_leaf_free_space(root, leaf) >= ins_len)
+	if (btrfs_leaf_free_space(fs_info, leaf) >= ins_len)
 		return 0;
 
 	item_size = btrfs_item_size_nr(leaf, path->slots[0]);
@@ -4391,7 +4389,7 @@ static noinline int setup_leaf_for_split(struct btrfs_trans_handle *trans,
 		goto err;
 
 	/* the leaf has  changed, it now has room.  return now */
-	if (btrfs_leaf_free_space(root, path->nodes[0]) >= ins_len)
+	if (btrfs_leaf_free_space(fs_info, path->nodes[0]) >= ins_len)
 		goto err;
 
 	if (key.type == BTRFS_EXTENT_DATA_KEY) {
@@ -4415,7 +4413,7 @@ static noinline int setup_leaf_for_split(struct btrfs_trans_handle *trans,
 }
 
 static noinline int split_item(struct btrfs_trans_handle *trans,
-			       struct btrfs_root *root,
+			       struct btrfs_fs_info *fs_info,
 			       struct btrfs_path *path,
 			       struct btrfs_key *new_key,
 			       unsigned long split_offset)
@@ -4431,7 +4429,7 @@ static noinline int split_item(struct btrfs_trans_handle *trans,
 	struct btrfs_disk_key disk_key;
 
 	leaf = path->nodes[0];
-	BUG_ON(btrfs_leaf_free_space(root, leaf) < sizeof(struct btrfs_item));
+	BUG_ON(btrfs_leaf_free_space(fs_info, leaf) < sizeof(struct btrfs_item));
 
 	btrfs_set_path_blocking(path);
 
@@ -4480,7 +4478,7 @@ static noinline int split_item(struct btrfs_trans_handle *trans,
 			    item_size - split_offset);
 	btrfs_mark_buffer_dirty(leaf);
 
-	BUG_ON(btrfs_leaf_free_space(root, leaf) < 0);
+	BUG_ON(btrfs_leaf_free_space(fs_info, leaf) < 0);
 	kfree(buf);
 	return 0;
 }
@@ -4512,7 +4510,7 @@ int btrfs_split_item(struct btrfs_trans_handle *trans,
 	if (ret)
 		return ret;
 
-	ret = split_item(trans, root, path, new_key, split_offset);
+	ret = split_item(trans, root->fs_info, path, new_key, split_offset);
 	return ret;
 }
 
@@ -4558,10 +4556,9 @@ int btrfs_duplicate_item(struct btrfs_trans_handle *trans,
  * off the end of the item or if we shift the item to chop bytes off
  * the front.
  */
-void btrfs_truncate_item(struct btrfs_root *root, struct btrfs_path *path,
-			 u32 new_size, int from_end)
+void btrfs_truncate_item(struct btrfs_fs_info *fs_info,
+			 struct btrfs_path *path, u32 new_size, int from_end)
 {
-	struct btrfs_fs_info *fs_info = root->fs_info;
 	int slot;
 	struct extent_buffer *leaf;
 	struct btrfs_item *item;
@@ -4583,7 +4580,7 @@ void btrfs_truncate_item(struct btrfs_root *root, struct btrfs_path *path,
 		return;
 
 	nritems = btrfs_header_nritems(leaf);
-	data_end = leaf_data_end(root, leaf);
+	data_end = leaf_data_end(fs_info, leaf);
 
 	old_data_start = btrfs_item_offset_nr(leaf, slot);
 
@@ -4649,8 +4646,8 @@ void btrfs_truncate_item(struct btrfs_root *root, struct btrfs_path *path,
 	btrfs_set_item_size(leaf, item, new_size);
 	btrfs_mark_buffer_dirty(leaf);
 
-	if (btrfs_leaf_free_space(root, leaf) < 0) {
-		btrfs_print_leaf(root, leaf);
+	if (btrfs_leaf_free_space(fs_info, leaf) < 0) {
+		btrfs_print_leaf(fs_info, leaf);
 		BUG();
 	}
 }
@@ -4658,10 +4655,9 @@ void btrfs_truncate_item(struct btrfs_root *root, struct btrfs_path *path,
 /*
  * make the item pointed to by the path bigger, data_size is the added size.
  */
-void btrfs_extend_item(struct btrfs_root *root, struct btrfs_path *path,
+void btrfs_extend_item(struct btrfs_fs_info *fs_info, struct btrfs_path *path,
 		       u32 data_size)
 {
-	struct btrfs_fs_info *fs_info = root->fs_info;
 	int slot;
 	struct extent_buffer *leaf;
 	struct btrfs_item *item;
@@ -4677,10 +4673,10 @@ void btrfs_extend_item(struct btrfs_root *root, struct btrfs_path *path,
 	leaf = path->nodes[0];
 
 	nritems = btrfs_header_nritems(leaf);
-	data_end = leaf_data_end(root, leaf);
+	data_end = leaf_data_end(fs_info, leaf);
 
-	if (btrfs_leaf_free_space(root, leaf) < data_size) {
-		btrfs_print_leaf(root, leaf);
+	if (btrfs_leaf_free_space(fs_info, leaf) < data_size) {
+		btrfs_print_leaf(fs_info, leaf);
 		BUG();
 	}
 	slot = path->slots[0];
@@ -4688,7 +4684,7 @@ void btrfs_extend_item(struct btrfs_root *root, struct btrfs_path *path,
 
 	BUG_ON(slot < 0);
 	if (slot >= nritems) {
-		btrfs_print_leaf(root, leaf);
+		btrfs_print_leaf(fs_info, leaf);
 		btrfs_crit(fs_info, "slot %d too large, nritems %d",
 			   slot, nritems);
 		BUG_ON(1);
@@ -4718,8 +4714,8 @@ void btrfs_extend_item(struct btrfs_root *root, struct btrfs_path *path,
 	btrfs_set_item_size(leaf, item, old_size + data_size);
 	btrfs_mark_buffer_dirty(leaf);
 
-	if (btrfs_leaf_free_space(root, leaf) < 0) {
-		btrfs_print_leaf(root, leaf);
+	if (btrfs_leaf_free_space(fs_info, leaf) < 0) {
+		btrfs_print_leaf(fs_info, leaf);
 		BUG();
 	}
 }
@@ -4755,12 +4751,12 @@ void setup_items_for_insert(struct btrfs_root *root, struct btrfs_path *path,
 	slot = path->slots[0];
 
 	nritems = btrfs_header_nritems(leaf);
-	data_end = leaf_data_end(root, leaf);
+	data_end = leaf_data_end(fs_info, leaf);
 
-	if (btrfs_leaf_free_space(root, leaf) < total_size) {
-		btrfs_print_leaf(root, leaf);
+	if (btrfs_leaf_free_space(fs_info, leaf) < total_size) {
+		btrfs_print_leaf(fs_info, leaf);
 		btrfs_crit(fs_info, "not enough freespace need %u have %d",
-			   total_size, btrfs_leaf_free_space(root, leaf));
+			   total_size, btrfs_leaf_free_space(fs_info, leaf));
 		BUG();
 	}
 
@@ -4768,7 +4764,7 @@ void setup_items_for_insert(struct btrfs_root *root, struct btrfs_path *path,
 		unsigned int old_data = btrfs_item_end_nr(leaf, slot);
 
 		if (old_data < data_end) {
-			btrfs_print_leaf(root, leaf);
+			btrfs_print_leaf(fs_info, leaf);
 			btrfs_crit(fs_info, "slot %d old_data %d data_end %d",
 				   slot, old_data, data_end);
 			BUG_ON(1);
@@ -4811,8 +4807,8 @@ void setup_items_for_insert(struct btrfs_root *root, struct btrfs_path *path,
 	btrfs_set_header_nritems(leaf, nritems + nr);
 	btrfs_mark_buffer_dirty(leaf);
 
-	if (btrfs_leaf_free_space(root, leaf) < 0) {
-		btrfs_print_leaf(root, leaf);
+	if (btrfs_leaf_free_space(fs_info, leaf) < 0) {
+		btrfs_print_leaf(fs_info, leaf);
 		BUG();
 	}
 }
@@ -4982,7 +4978,7 @@ int btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 	nritems = btrfs_header_nritems(leaf);
 
 	if (slot + nr != nritems) {
-		int data_end = leaf_data_end(root, leaf);
+		int data_end = leaf_data_end(fs_info, leaf);
 
 		memmove_extent_buffer(leaf, btrfs_leaf_data(leaf) +
 			      data_end + dsize,
@@ -5145,6 +5141,7 @@ int btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,
 			 struct btrfs_path *path,
 			 u64 min_trans)
 {
+	struct btrfs_fs_info *fs_info = root->fs_info;
 	struct extent_buffer *cur;
 	struct btrfs_key found_key;
 	int slot;
@@ -5221,7 +5218,7 @@ int btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,
 			goto out;
 		}
 		btrfs_set_path_blocking(path);
-		cur = read_node_slot(root, cur, slot);
+		cur = read_node_slot(fs_info, cur, slot);
 		if (IS_ERR(cur)) {
 			ret = PTR_ERR(cur);
 			goto out;
@@ -5244,14 +5241,14 @@ int btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,
 	return ret;
 }
 
-static int tree_move_down(struct btrfs_root *root,
+static int tree_move_down(struct btrfs_fs_info *fs_info,
 			   struct btrfs_path *path,
 			   int *level, int root_level)
 {
 	struct extent_buffer *eb;
 
 	BUG_ON(*level == 0);
-	eb = read_node_slot(root, path->nodes[*level], path->slots[*level]);
+	eb = read_node_slot(fs_info, path->nodes[*level], path->slots[*level]);
 	if (IS_ERR(eb))
 		return PTR_ERR(eb);
 
@@ -5261,7 +5258,7 @@ static int tree_move_down(struct btrfs_root *root,
 	return 0;
 }
 
-static int tree_move_next_or_upnext(struct btrfs_root *root,
+static int tree_move_next_or_upnext(struct btrfs_fs_info *fs_info,
 				    struct btrfs_path *path,
 				    int *level, int root_level)
 {
@@ -5292,7 +5289,7 @@ static int tree_move_next_or_upnext(struct btrfs_root *root,
  * Returns 1 if it had to move up and next. 0 is returned if it moved only next
  * or down.
  */
-static int tree_advance(struct btrfs_root *root,
+static int tree_advance(struct btrfs_fs_info *fs_info,
 			struct btrfs_path *path,
 			int *level, int root_level,
 			int allow_down,
@@ -5301,9 +5298,10 @@ static int tree_advance(struct btrfs_root *root,
 	int ret;
 
 	if (*level == 0 || !allow_down) {
-		ret = tree_move_next_or_upnext(root, path, level, root_level);
+		ret = tree_move_next_or_upnext(fs_info, path, level,
+					       root_level);
 	} else {
-		ret = tree_move_down(root, path, level, root_level);
+		ret = tree_move_down(fs_info, path, level, root_level);
 	}
 	if (ret >= 0) {
 		if (*level == 0)
@@ -5316,8 +5314,7 @@ static int tree_advance(struct btrfs_root *root,
 	return ret;
 }
 
-static int tree_compare_item(struct btrfs_root *left_root,
-			     struct btrfs_path *left_path,
+static int tree_compare_item(struct btrfs_path *left_path,
 			     struct btrfs_path *right_path,
 			     char *tmp_buf)
 {
@@ -5474,7 +5471,7 @@ int btrfs_compare_trees(struct btrfs_root *left_root,
 
 	while (1) {
 		if (advance_left && !left_end_reached) {
-			ret = tree_advance(left_root, left_path, &left_level,
+			ret = tree_advance(fs_info, left_path, &left_level,
 					left_root_level,
 					advance_left != ADVANCE_ONLY_NEXT,
 					&left_key);
@@ -5485,7 +5482,7 @@ int btrfs_compare_trees(struct btrfs_root *left_root,
 			advance_left = 0;
 		}
 		if (advance_right && !right_end_reached) {
-			ret = tree_advance(right_root, right_path, &right_level,
+			ret = tree_advance(fs_info, right_path, &right_level,
 					right_root_level,
 					advance_right != ADVANCE_ONLY_NEXT,
 					&right_key);
@@ -5549,8 +5546,8 @@ int btrfs_compare_trees(struct btrfs_root *left_root,
 				enum btrfs_compare_tree_result result;
 
 				WARN_ON(!extent_buffer_uptodate(left_path->nodes[0]));
-				ret = tree_compare_item(left_root, left_path,
-						right_path, tmp_buf);
+				ret = tree_compare_item(left_path, right_path,
+							tmp_buf);
 				if (ret)
 					result = BTRFS_COMPARE_TREE_CHANGED;
 				else

commit 0b246afa62b0cf5b09d078121f543135f28492ad
Author: Jeff Mahoney <jeffm@suse.com>
Date:   Wed Jun 22 18:54:23 2016 -0400

    btrfs: root->fs_info cleanup, add fs_info convenience variables
    
    In routines where someptr->fs_info is referenced multiple times, we
    introduce a convenience variable.  This makes the code considerably
    more readable.
    
    Signed-off-by: Jeff Mahoney <jeffm@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 431b150a823a..b29c8d82e741 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -212,21 +212,23 @@ static struct extent_buffer *btrfs_read_lock_root_node(struct btrfs_root *root)
  */
 static void add_root_to_dirty_list(struct btrfs_root *root)
 {
+	struct btrfs_fs_info *fs_info = root->fs_info;
+
 	if (test_bit(BTRFS_ROOT_DIRTY, &root->state) ||
 	    !test_bit(BTRFS_ROOT_TRACK_DIRTY, &root->state))
 		return;
 
-	spin_lock(&root->fs_info->trans_lock);
+	spin_lock(&fs_info->trans_lock);
 	if (!test_and_set_bit(BTRFS_ROOT_DIRTY, &root->state)) {
 		/* Want the extent tree to be the last on the list */
 		if (root->objectid == BTRFS_EXTENT_TREE_OBJECTID)
 			list_move_tail(&root->dirty_list,
-				       &root->fs_info->dirty_cowonly_roots);
+				       &fs_info->dirty_cowonly_roots);
 		else
 			list_move(&root->dirty_list,
-				  &root->fs_info->dirty_cowonly_roots);
+				  &fs_info->dirty_cowonly_roots);
 	}
-	spin_unlock(&root->fs_info->trans_lock);
+	spin_unlock(&fs_info->trans_lock);
 }
 
 /*
@@ -239,13 +241,14 @@ int btrfs_copy_root(struct btrfs_trans_handle *trans,
 		      struct extent_buffer *buf,
 		      struct extent_buffer **cow_ret, u64 new_root_objectid)
 {
+	struct btrfs_fs_info *fs_info = root->fs_info;
 	struct extent_buffer *cow;
 	int ret = 0;
 	int level;
 	struct btrfs_disk_key disk_key;
 
 	WARN_ON(test_bit(BTRFS_ROOT_REF_COWS, &root->state) &&
-		trans->transid != root->fs_info->running_transaction->transid);
+		trans->transid != fs_info->running_transaction->transid);
 	WARN_ON(test_bit(BTRFS_ROOT_REF_COWS, &root->state) &&
 		trans->transid != root->last_trans);
 
@@ -271,7 +274,7 @@ int btrfs_copy_root(struct btrfs_trans_handle *trans,
 	else
 		btrfs_set_header_owner(cow, new_root_objectid);
 
-	write_extent_buffer_fsid(cow, root->fs_info->fsid);
+	write_extent_buffer_fsid(cow, fs_info->fsid);
 
 	WARN_ON(btrfs_header_generation(buf) > trans->transid);
 	if (new_root_objectid == BTRFS_TREE_RELOC_OBJECTID)
@@ -977,6 +980,7 @@ static noinline int update_ref_for_cow(struct btrfs_trans_handle *trans,
 				       struct extent_buffer *cow,
 				       int *last_ref)
 {
+	struct btrfs_fs_info *fs_info = root->fs_info;
 	u64 refs;
 	u64 owner;
 	u64 flags;
@@ -1008,7 +1012,7 @@ static noinline int update_ref_for_cow(struct btrfs_trans_handle *trans,
 			return ret;
 		if (refs == 0) {
 			ret = -EROFS;
-			btrfs_handle_fs_error(root->fs_info, ret, NULL);
+			btrfs_handle_fs_error(fs_info, ret, NULL);
 			return ret;
 		}
 	} else {
@@ -1069,7 +1073,7 @@ static noinline int update_ref_for_cow(struct btrfs_trans_handle *trans,
 			ret = btrfs_dec_ref(trans, root, buf, 1);
 			BUG_ON(ret); /* -ENOMEM */
 		}
-		clean_tree_block(trans, root->fs_info, buf);
+		clean_tree_block(trans, fs_info, buf);
 		*last_ref = 1;
 	}
 	return 0;
@@ -1094,6 +1098,7 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 			     struct extent_buffer **cow_ret,
 			     u64 search_start, u64 empty_size)
 {
+	struct btrfs_fs_info *fs_info = root->fs_info;
 	struct btrfs_disk_key disk_key;
 	struct extent_buffer *cow;
 	int level, ret;
@@ -1107,7 +1112,7 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 	btrfs_assert_tree_locked(buf);
 
 	WARN_ON(test_bit(BTRFS_ROOT_REF_COWS, &root->state) &&
-		trans->transid != root->fs_info->running_transaction->transid);
+		trans->transid != fs_info->running_transaction->transid);
 	WARN_ON(test_bit(BTRFS_ROOT_REF_COWS, &root->state) &&
 		trans->transid != root->last_trans);
 
@@ -1140,7 +1145,7 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 	else
 		btrfs_set_header_owner(cow, root->root_key.objectid);
 
-	write_extent_buffer_fsid(cow, root->fs_info->fsid);
+	write_extent_buffer_fsid(cow, fs_info->fsid);
 
 	ret = update_ref_for_cow(trans, root, buf, cow, &last_ref);
 	if (ret) {
@@ -1172,7 +1177,7 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 		add_root_to_dirty_list(root);
 	} else {
 		WARN_ON(trans->transid != btrfs_header_generation(parent));
-		tree_mod_log_insert_key(root->fs_info, parent, parent_slot,
+		tree_mod_log_insert_key(fs_info, parent, parent_slot,
 					MOD_LOG_KEY_REPLACE, GFP_NOFS);
 		btrfs_set_node_blockptr(parent, parent_slot,
 					cow->start);
@@ -1180,7 +1185,7 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 					      trans->transid);
 		btrfs_mark_buffer_dirty(parent);
 		if (last_ref) {
-			ret = tree_mod_log_free_eb(root->fs_info, buf);
+			ret = tree_mod_log_free_eb(fs_info, buf);
 			if (ret) {
 				btrfs_abort_transaction(trans, ret);
 				return ret;
@@ -1400,6 +1405,7 @@ tree_mod_log_rewind(struct btrfs_fs_info *fs_info, struct btrfs_path *path,
 static inline struct extent_buffer *
 get_old_root(struct btrfs_root *root, u64 time_seq)
 {
+	struct btrfs_fs_info *fs_info = root->fs_info;
 	struct tree_mod_elem *tm;
 	struct extent_buffer *eb = NULL;
 	struct extent_buffer *eb_root;
@@ -1409,7 +1415,7 @@ get_old_root(struct btrfs_root *root, u64 time_seq)
 	u64 logical;
 
 	eb_root = btrfs_read_lock_root_node(root);
-	tm = __tree_mod_log_oldest_root(root->fs_info, eb_root, time_seq);
+	tm = __tree_mod_log_oldest_root(fs_info, eb_root, time_seq);
 	if (!tm)
 		return eb_root;
 
@@ -1421,7 +1427,7 @@ get_old_root(struct btrfs_root *root, u64 time_seq)
 		logical = eb_root->start;
 	}
 
-	tm = tree_mod_log_search(root->fs_info, logical, time_seq);
+	tm = tree_mod_log_search(fs_info, logical, time_seq);
 	if (old_root && tm && tm->op != MOD_LOG_KEY_REMOVE_WHILE_FREEING) {
 		btrfs_tree_read_unlock(eb_root);
 		free_extent_buffer(eb_root);
@@ -1429,8 +1435,9 @@ get_old_root(struct btrfs_root *root, u64 time_seq)
 		if (WARN_ON(IS_ERR(old) || !extent_buffer_uptodate(old))) {
 			if (!IS_ERR(old))
 				free_extent_buffer(old);
-			btrfs_warn(root->fs_info,
-				"failed to read tree block %llu from get_old_root", logical);
+			btrfs_warn(fs_info,
+				   "failed to read tree block %llu from get_old_root",
+				   logical);
 		} else {
 			eb = btrfs_clone_extent_buffer(old);
 			free_extent_buffer(old);
@@ -1438,7 +1445,7 @@ get_old_root(struct btrfs_root *root, u64 time_seq)
 	} else if (old_root) {
 		btrfs_tree_read_unlock(eb_root);
 		free_extent_buffer(eb_root);
-		eb = alloc_dummy_extent_buffer(root->fs_info, logical);
+		eb = alloc_dummy_extent_buffer(fs_info, logical);
 	} else {
 		btrfs_set_lock_blocking_rw(eb_root, BTRFS_READ_LOCK);
 		eb = btrfs_clone_extent_buffer(eb_root);
@@ -1458,10 +1465,10 @@ get_old_root(struct btrfs_root *root, u64 time_seq)
 		btrfs_set_header_generation(eb, old_generation);
 	}
 	if (tm)
-		__tree_mod_log_rewind(root->fs_info, eb, time_seq, tm);
+		__tree_mod_log_rewind(fs_info, eb, time_seq, tm);
 	else
 		WARN_ON(btrfs_header_level(eb) != 0);
-	WARN_ON(btrfs_header_nritems(eb) > BTRFS_NODEPTRS_PER_BLOCK(root->fs_info));
+	WARN_ON(btrfs_header_nritems(eb) > BTRFS_NODEPTRS_PER_BLOCK(fs_info));
 
 	return eb;
 }
@@ -1523,17 +1530,18 @@ noinline int btrfs_cow_block(struct btrfs_trans_handle *trans,
 		    struct extent_buffer *parent, int parent_slot,
 		    struct extent_buffer **cow_ret)
 {
+	struct btrfs_fs_info *fs_info = root->fs_info;
 	u64 search_start;
 	int ret;
 
-	if (trans->transaction != root->fs_info->running_transaction)
+	if (trans->transaction != fs_info->running_transaction)
 		WARN(1, KERN_CRIT "trans %llu running %llu\n",
 		       trans->transid,
-		       root->fs_info->running_transaction->transid);
+		       fs_info->running_transaction->transid);
 
-	if (trans->transid != root->fs_info->generation)
+	if (trans->transid != fs_info->generation)
 		WARN(1, KERN_CRIT "trans %llu running %llu\n",
-		       trans->transid, root->fs_info->generation);
+		       trans->transid, fs_info->generation);
 
 	if (!should_cow_block(trans, root, buf)) {
 		trans->dirty = true;
@@ -1610,6 +1618,7 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 		       int start_slot, u64 *last_ret,
 		       struct btrfs_key *progress)
 {
+	struct btrfs_fs_info *fs_info = root->fs_info;
 	struct extent_buffer *cur;
 	u64 blocknr;
 	u64 gen;
@@ -1628,11 +1637,11 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 
 	parent_level = btrfs_header_level(parent);
 
-	WARN_ON(trans->transaction != root->fs_info->running_transaction);
-	WARN_ON(trans->transid != root->fs_info->generation);
+	WARN_ON(trans->transaction != fs_info->running_transaction);
+	WARN_ON(trans->transid != fs_info->generation);
 
 	parent_nritems = btrfs_header_nritems(parent);
-	blocksize = root->fs_info->nodesize;
+	blocksize = fs_info->nodesize;
 	end_slot = parent_nritems - 1;
 
 	if (parent_nritems <= 1)
@@ -1666,7 +1675,7 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 			continue;
 		}
 
-		cur = find_extent_buffer(root->fs_info, blocknr);
+		cur = find_extent_buffer(fs_info, blocknr);
 		if (cur)
 			uptodate = btrfs_buffer_uptodate(cur, gen, 0);
 		else
@@ -1711,7 +1720,6 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 	return err;
 }
 
-
 /*
  * search for key in the extent_buffer.  The items start at offset p,
  * and they are item_size apart.  There are 'max' items in p.
@@ -1865,6 +1873,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 			 struct btrfs_root *root,
 			 struct btrfs_path *path, int level)
 {
+	struct btrfs_fs_info *fs_info = root->fs_info;
 	struct extent_buffer *right = NULL;
 	struct extent_buffer *mid;
 	struct extent_buffer *left = NULL;
@@ -1905,7 +1914,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		child = read_node_slot(root, mid, 0);
 		if (IS_ERR(child)) {
 			ret = PTR_ERR(child);
-			btrfs_handle_fs_error(root->fs_info, ret, NULL);
+			btrfs_handle_fs_error(fs_info, ret, NULL);
 			goto enospc;
 		}
 
@@ -1926,7 +1935,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 
 		path->locks[level] = 0;
 		path->nodes[level] = NULL;
-		clean_tree_block(trans, root->fs_info, mid);
+		clean_tree_block(trans, fs_info, mid);
 		btrfs_tree_unlock(mid);
 		/* once for the path */
 		free_extent_buffer(mid);
@@ -1938,7 +1947,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		return 0;
 	}
 	if (btrfs_header_nritems(mid) >
-	    BTRFS_NODEPTRS_PER_BLOCK(root->fs_info) / 4)
+	    BTRFS_NODEPTRS_PER_BLOCK(fs_info) / 4)
 		return 0;
 
 	left = read_node_slot(root, parent, pslot - 1);
@@ -1987,7 +1996,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		if (wret < 0 && wret != -ENOSPC)
 			ret = wret;
 		if (btrfs_header_nritems(right) == 0) {
-			clean_tree_block(trans, root->fs_info, right);
+			clean_tree_block(trans, fs_info, right);
 			btrfs_tree_unlock(right);
 			del_ptr(root, path, level + 1, pslot + 1);
 			root_sub_used(root, right->len);
@@ -1997,7 +2006,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		} else {
 			struct btrfs_disk_key right_key;
 			btrfs_node_key(right, &right_key, 0);
-			tree_mod_log_set_node_key(root->fs_info, parent,
+			tree_mod_log_set_node_key(fs_info, parent,
 						  pslot + 1, 0);
 			btrfs_set_node_key(parent, &right_key, pslot + 1);
 			btrfs_mark_buffer_dirty(parent);
@@ -2015,7 +2024,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		 */
 		if (!left) {
 			ret = -EROFS;
-			btrfs_handle_fs_error(root->fs_info, ret, NULL);
+			btrfs_handle_fs_error(fs_info, ret, NULL);
 			goto enospc;
 		}
 		wret = balance_node_right(trans, root, mid, left);
@@ -2031,7 +2040,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		BUG_ON(wret == 1);
 	}
 	if (btrfs_header_nritems(mid) == 0) {
-		clean_tree_block(trans, root->fs_info, mid);
+		clean_tree_block(trans, fs_info, mid);
 		btrfs_tree_unlock(mid);
 		del_ptr(root, path, level + 1, pslot);
 		root_sub_used(root, mid->len);
@@ -2042,8 +2051,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		/* update the parent key to reflect our changes */
 		struct btrfs_disk_key mid_key;
 		btrfs_node_key(mid, &mid_key, 0);
-		tree_mod_log_set_node_key(root->fs_info, parent,
-					  pslot, 0);
+		tree_mod_log_set_node_key(fs_info, parent, pslot, 0);
 		btrfs_set_node_key(parent, &mid_key, pslot);
 		btrfs_mark_buffer_dirty(parent);
 	}
@@ -2090,6 +2098,7 @@ static noinline int push_nodes_for_insert(struct btrfs_trans_handle *trans,
 					  struct btrfs_root *root,
 					  struct btrfs_path *path, int level)
 {
+	struct btrfs_fs_info *fs_info = root->fs_info;
 	struct extent_buffer *right = NULL;
 	struct extent_buffer *mid;
 	struct extent_buffer *left = NULL;
@@ -2125,7 +2134,7 @@ static noinline int push_nodes_for_insert(struct btrfs_trans_handle *trans,
 		btrfs_set_lock_blocking(left);
 
 		left_nr = btrfs_header_nritems(left);
-		if (left_nr >= BTRFS_NODEPTRS_PER_BLOCK(root->fs_info) - 1) {
+		if (left_nr >= BTRFS_NODEPTRS_PER_BLOCK(fs_info) - 1) {
 			wret = 1;
 		} else {
 			ret = btrfs_cow_block(trans, root, left, parent,
@@ -2143,8 +2152,7 @@ static noinline int push_nodes_for_insert(struct btrfs_trans_handle *trans,
 			struct btrfs_disk_key disk_key;
 			orig_slot += left_nr;
 			btrfs_node_key(mid, &disk_key, 0);
-			tree_mod_log_set_node_key(root->fs_info, parent,
-						  pslot, 0);
+			tree_mod_log_set_node_key(fs_info, parent, pslot, 0);
 			btrfs_set_node_key(parent, &disk_key, pslot);
 			btrfs_mark_buffer_dirty(parent);
 			if (btrfs_header_nritems(left) > orig_slot) {
@@ -2179,7 +2187,7 @@ static noinline int push_nodes_for_insert(struct btrfs_trans_handle *trans,
 		btrfs_set_lock_blocking(right);
 
 		right_nr = btrfs_header_nritems(right);
-		if (right_nr >= BTRFS_NODEPTRS_PER_BLOCK(root->fs_info) - 1) {
+		if (right_nr >= BTRFS_NODEPTRS_PER_BLOCK(fs_info) - 1) {
 			wret = 1;
 		} else {
 			ret = btrfs_cow_block(trans, root, right,
@@ -2198,7 +2206,7 @@ static noinline int push_nodes_for_insert(struct btrfs_trans_handle *trans,
 			struct btrfs_disk_key disk_key;
 
 			btrfs_node_key(right, &disk_key, 0);
-			tree_mod_log_set_node_key(root->fs_info, parent,
+			tree_mod_log_set_node_key(fs_info, parent,
 						  pslot + 1, 0);
 			btrfs_set_node_key(parent, &disk_key, pslot + 1);
 			btrfs_mark_buffer_dirty(parent);
@@ -2230,6 +2238,7 @@ static void reada_for_search(struct btrfs_root *root,
 			     struct btrfs_path *path,
 			     int level, int slot, u64 objectid)
 {
+	struct btrfs_fs_info *fs_info = root->fs_info;
 	struct extent_buffer *node;
 	struct btrfs_disk_key disk_key;
 	u32 nritems;
@@ -2250,8 +2259,8 @@ static void reada_for_search(struct btrfs_root *root,
 	node = path->nodes[level];
 
 	search = btrfs_node_blockptr(node, slot);
-	blocksize = root->fs_info->nodesize;
-	eb = find_extent_buffer(root->fs_info, search);
+	blocksize = fs_info->nodesize;
+	eb = find_extent_buffer(fs_info, search);
 	if (eb) {
 		free_extent_buffer(eb);
 		return;
@@ -2292,6 +2301,7 @@ static void reada_for_search(struct btrfs_root *root,
 static noinline void reada_for_balance(struct btrfs_root *root,
 				       struct btrfs_path *path, int level)
 {
+	struct btrfs_fs_info *fs_info = root->fs_info;
 	int slot;
 	int nritems;
 	struct extent_buffer *parent;
@@ -2310,7 +2320,7 @@ static noinline void reada_for_balance(struct btrfs_root *root,
 	if (slot > 0) {
 		block1 = btrfs_node_blockptr(parent, slot - 1);
 		gen = btrfs_node_ptr_generation(parent, slot - 1);
-		eb = find_extent_buffer(root->fs_info, block1);
+		eb = find_extent_buffer(fs_info, block1);
 		/*
 		 * if we get -eagain from btrfs_buffer_uptodate, we
 		 * don't want to return eagain here.  That will loop
@@ -2323,7 +2333,7 @@ static noinline void reada_for_balance(struct btrfs_root *root,
 	if (slot + 1 < nritems) {
 		block2 = btrfs_node_blockptr(parent, slot + 1);
 		gen = btrfs_node_ptr_generation(parent, slot + 1);
-		eb = find_extent_buffer(root->fs_info, block2);
+		eb = find_extent_buffer(fs_info, block2);
 		if (eb && btrfs_buffer_uptodate(eb, gen, 1) != 0)
 			block2 = 0;
 		free_extent_buffer(eb);
@@ -2432,6 +2442,7 @@ read_block_for_search(struct btrfs_trans_handle *trans,
 		       struct extent_buffer **eb_ret, int level, int slot,
 		       struct btrfs_key *key, u64 time_seq)
 {
+	struct btrfs_fs_info *fs_info = root->fs_info;
 	u64 blocknr;
 	u64 gen;
 	struct extent_buffer *b = *eb_ret;
@@ -2441,7 +2452,7 @@ read_block_for_search(struct btrfs_trans_handle *trans,
 	blocknr = btrfs_node_blockptr(b, slot);
 	gen = btrfs_node_ptr_generation(b, slot);
 
-	tmp = find_extent_buffer(root->fs_info, blocknr);
+	tmp = find_extent_buffer(fs_info, blocknr);
 	if (tmp) {
 		/* first we do an atomic uptodate check */
 		if (btrfs_buffer_uptodate(tmp, gen, 1) > 0) {
@@ -2517,9 +2528,11 @@ setup_nodes_for_search(struct btrfs_trans_handle *trans,
 		       struct extent_buffer *b, int level, int ins_len,
 		       int *write_lock_level)
 {
+	struct btrfs_fs_info *fs_info = root->fs_info;
 	int ret;
+
 	if ((p->search_for_split || ins_len > 0) && btrfs_header_nritems(b) >=
-	    BTRFS_NODEPTRS_PER_BLOCK(root->fs_info) - 3) {
+	    BTRFS_NODEPTRS_PER_BLOCK(fs_info) - 3) {
 		int sret;
 
 		if (*write_lock_level < level + 1) {
@@ -2540,7 +2553,7 @@ setup_nodes_for_search(struct btrfs_trans_handle *trans,
 		}
 		b = p->nodes[level];
 	} else if (ins_len < 0 && btrfs_header_nritems(b) <
-		   BTRFS_NODEPTRS_PER_BLOCK(root->fs_info) / 2) {
+		   BTRFS_NODEPTRS_PER_BLOCK(fs_info) / 2) {
 		int sret;
 
 		if (*write_lock_level < level + 1) {
@@ -2659,6 +2672,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 		      *root, struct btrfs_key *key, struct btrfs_path *p, int
 		      ins_len, int cow)
 {
+	struct btrfs_fs_info *fs_info = root->fs_info;
 	struct extent_buffer *b;
 	int slot;
 	int ret;
@@ -2714,12 +2728,12 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 		 * so we always do read locks
 		 */
 		if (p->need_commit_sem)
-			down_read(&root->fs_info->commit_root_sem);
+			down_read(&fs_info->commit_root_sem);
 		b = root->commit_root;
 		extent_buffer_get(b);
 		level = btrfs_header_level(b);
 		if (p->need_commit_sem)
-			up_read(&root->fs_info->commit_root_sem);
+			up_read(&fs_info->commit_root_sem);
 		if (!p->skip_locking)
 			btrfs_tree_read_lock(b);
 	} else {
@@ -2942,6 +2956,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 int btrfs_search_old_slot(struct btrfs_root *root, struct btrfs_key *key,
 			  struct btrfs_path *p, u64 time_seq)
 {
+	struct btrfs_fs_info *fs_info = root->fs_info;
 	struct extent_buffer *b;
 	int slot;
 	int ret;
@@ -3016,7 +3031,7 @@ int btrfs_search_old_slot(struct btrfs_root *root, struct btrfs_key *key,
 				btrfs_clear_path_blocking(p, b,
 							  BTRFS_READ_LOCK);
 			}
-			b = tree_mod_log_rewind(root->fs_info, p, b, time_seq);
+			b = tree_mod_log_rewind(fs_info, p, b, time_seq);
 			if (!b) {
 				ret = -ENOMEM;
 				goto done;
@@ -3186,6 +3201,7 @@ static int push_node_left(struct btrfs_trans_handle *trans,
 			  struct btrfs_root *root, struct extent_buffer *dst,
 			  struct extent_buffer *src, int empty)
 {
+	struct btrfs_fs_info *fs_info = root->fs_info;
 	int push_items = 0;
 	int src_nritems;
 	int dst_nritems;
@@ -3193,7 +3209,7 @@ static int push_node_left(struct btrfs_trans_handle *trans,
 
 	src_nritems = btrfs_header_nritems(src);
 	dst_nritems = btrfs_header_nritems(dst);
-	push_items = BTRFS_NODEPTRS_PER_BLOCK(root->fs_info) - dst_nritems;
+	push_items = BTRFS_NODEPTRS_PER_BLOCK(fs_info) - dst_nritems;
 	WARN_ON(btrfs_header_generation(src) != trans->transid);
 	WARN_ON(btrfs_header_generation(dst) != trans->transid);
 
@@ -3218,7 +3234,7 @@ static int push_node_left(struct btrfs_trans_handle *trans,
 	} else
 		push_items = min(src_nritems - 8, push_items);
 
-	ret = tree_mod_log_eb_copy(root->fs_info, dst, src, dst_nritems, 0,
+	ret = tree_mod_log_eb_copy(fs_info, dst, src, dst_nritems, 0,
 				   push_items);
 	if (ret) {
 		btrfs_abort_transaction(trans, ret);
@@ -3261,6 +3277,7 @@ static int balance_node_right(struct btrfs_trans_handle *trans,
 			      struct extent_buffer *dst,
 			      struct extent_buffer *src)
 {
+	struct btrfs_fs_info *fs_info = root->fs_info;
 	int push_items = 0;
 	int max_push;
 	int src_nritems;
@@ -3272,7 +3289,7 @@ static int balance_node_right(struct btrfs_trans_handle *trans,
 
 	src_nritems = btrfs_header_nritems(src);
 	dst_nritems = btrfs_header_nritems(dst);
-	push_items = BTRFS_NODEPTRS_PER_BLOCK(root->fs_info) - dst_nritems;
+	push_items = BTRFS_NODEPTRS_PER_BLOCK(fs_info) - dst_nritems;
 	if (push_items <= 0)
 		return 1;
 
@@ -3287,13 +3304,13 @@ static int balance_node_right(struct btrfs_trans_handle *trans,
 	if (max_push < push_items)
 		push_items = max_push;
 
-	tree_mod_log_eb_move(root->fs_info, dst, push_items, 0, dst_nritems);
+	tree_mod_log_eb_move(fs_info, dst, push_items, 0, dst_nritems);
 	memmove_extent_buffer(dst, btrfs_node_key_ptr_offset(push_items),
 				      btrfs_node_key_ptr_offset(0),
 				      (dst_nritems) *
 				      sizeof(struct btrfs_key_ptr));
 
-	ret = tree_mod_log_eb_copy(root->fs_info, dst, src, 0,
+	ret = tree_mod_log_eb_copy(fs_info, dst, src, 0,
 				   src_nritems - push_items, push_items);
 	if (ret) {
 		btrfs_abort_transaction(trans, ret);
@@ -3324,6 +3341,7 @@ static noinline int insert_new_root(struct btrfs_trans_handle *trans,
 			   struct btrfs_root *root,
 			   struct btrfs_path *path, int level)
 {
+	struct btrfs_fs_info *fs_info = root->fs_info;
 	u64 lower_gen;
 	struct extent_buffer *lower;
 	struct extent_buffer *c;
@@ -3344,7 +3362,7 @@ static noinline int insert_new_root(struct btrfs_trans_handle *trans,
 	if (IS_ERR(c))
 		return PTR_ERR(c);
 
-	root_add_used(root, root->fs_info->nodesize);
+	root_add_used(root, fs_info->nodesize);
 
 	memzero_extent_buffer(c, 0, sizeof(struct btrfs_header));
 	btrfs_set_header_nritems(c, 1);
@@ -3354,8 +3372,8 @@ static noinline int insert_new_root(struct btrfs_trans_handle *trans,
 	btrfs_set_header_backref_rev(c, BTRFS_MIXED_BACKREF_REV);
 	btrfs_set_header_owner(c, root->root_key.objectid);
 
-	write_extent_buffer_fsid(c, root->fs_info->fsid);
-	write_extent_buffer_chunk_tree_uuid(c, root->fs_info->chunk_tree_uuid);
+	write_extent_buffer_fsid(c, fs_info->fsid);
+	write_extent_buffer_chunk_tree_uuid(c, fs_info->chunk_tree_uuid);
 
 	btrfs_set_node_key(c, &lower_key, 0);
 	btrfs_set_node_blockptr(c, 0, lower->start);
@@ -3393,6 +3411,7 @@ static void insert_ptr(struct btrfs_trans_handle *trans,
 		       struct btrfs_disk_key *key, u64 bytenr,
 		       int slot, int level)
 {
+	struct btrfs_fs_info *fs_info = root->fs_info;
 	struct extent_buffer *lower;
 	int nritems;
 	int ret;
@@ -3402,10 +3421,10 @@ static void insert_ptr(struct btrfs_trans_handle *trans,
 	lower = path->nodes[level];
 	nritems = btrfs_header_nritems(lower);
 	BUG_ON(slot > nritems);
-	BUG_ON(nritems == BTRFS_NODEPTRS_PER_BLOCK(root->fs_info));
+	BUG_ON(nritems == BTRFS_NODEPTRS_PER_BLOCK(fs_info));
 	if (slot != nritems) {
 		if (level)
-			tree_mod_log_eb_move(root->fs_info, lower, slot + 1,
+			tree_mod_log_eb_move(fs_info, lower, slot + 1,
 					     slot, nritems - slot);
 		memmove_extent_buffer(lower,
 			      btrfs_node_key_ptr_offset(slot + 1),
@@ -3413,7 +3432,7 @@ static void insert_ptr(struct btrfs_trans_handle *trans,
 			      (nritems - slot) * sizeof(struct btrfs_key_ptr));
 	}
 	if (level) {
-		ret = tree_mod_log_insert_key(root->fs_info, lower, slot,
+		ret = tree_mod_log_insert_key(fs_info, lower, slot,
 					      MOD_LOG_KEY_ADD, GFP_NOFS);
 		BUG_ON(ret < 0);
 	}
@@ -3438,6 +3457,7 @@ static noinline int split_node(struct btrfs_trans_handle *trans,
 			       struct btrfs_root *root,
 			       struct btrfs_path *path, int level)
 {
+	struct btrfs_fs_info *fs_info = root->fs_info;
 	struct extent_buffer *c;
 	struct extent_buffer *split;
 	struct btrfs_disk_key disk_key;
@@ -3465,7 +3485,7 @@ static noinline int split_node(struct btrfs_trans_handle *trans,
 		ret = push_nodes_for_insert(trans, root, path, level);
 		c = path->nodes[level];
 		if (!ret && btrfs_header_nritems(c) <
-		    BTRFS_NODEPTRS_PER_BLOCK(root->fs_info) - 3)
+		    BTRFS_NODEPTRS_PER_BLOCK(fs_info) - 3)
 			return 0;
 		if (ret < 0)
 			return ret;
@@ -3480,7 +3500,7 @@ static noinline int split_node(struct btrfs_trans_handle *trans,
 	if (IS_ERR(split))
 		return PTR_ERR(split);
 
-	root_add_used(root, root->fs_info->nodesize);
+	root_add_used(root, fs_info->nodesize);
 
 	memzero_extent_buffer(split, 0, sizeof(struct btrfs_header));
 	btrfs_set_header_level(split, btrfs_header_level(c));
@@ -3488,12 +3508,10 @@ static noinline int split_node(struct btrfs_trans_handle *trans,
 	btrfs_set_header_generation(split, trans->transid);
 	btrfs_set_header_backref_rev(split, BTRFS_MIXED_BACKREF_REV);
 	btrfs_set_header_owner(split, root->root_key.objectid);
-	write_extent_buffer_fsid(split, root->fs_info->fsid);
-	write_extent_buffer_chunk_tree_uuid(split,
-			root->fs_info->chunk_tree_uuid);
+	write_extent_buffer_fsid(split, fs_info->fsid);
+	write_extent_buffer_chunk_tree_uuid(split, fs_info->chunk_tree_uuid);
 
-	ret = tree_mod_log_eb_copy(root->fs_info, split, c, 0,
-				   mid, c_nritems - mid);
+	ret = tree_mod_log_eb_copy(fs_info, split, c, 0, mid, c_nritems - mid);
 	if (ret) {
 		btrfs_abort_transaction(trans, ret);
 		return ret;
@@ -3560,15 +3578,17 @@ static int leaf_space_used(struct extent_buffer *l, int start, int nr)
 noinline int btrfs_leaf_free_space(struct btrfs_root *root,
 				   struct extent_buffer *leaf)
 {
+	struct btrfs_fs_info *fs_info = root->fs_info;
 	int nritems = btrfs_header_nritems(leaf);
 	int ret;
-	ret = BTRFS_LEAF_DATA_SIZE(root->fs_info) - leaf_space_used(leaf, 0, nritems);
+
+	ret = BTRFS_LEAF_DATA_SIZE(fs_info) - leaf_space_used(leaf, 0, nritems);
 	if (ret < 0) {
-		btrfs_crit(root->fs_info,
-			"leaf free space ret %d, leaf data size %lu, used %d nritems %d",
-		       ret,
-		       (unsigned long) BTRFS_LEAF_DATA_SIZE(root->fs_info),
-		       leaf_space_used(leaf, 0, nritems), nritems);
+		btrfs_crit(fs_info,
+			   "leaf free space ret %d, leaf data size %lu, used %d nritems %d",
+			   ret,
+			   (unsigned long) BTRFS_LEAF_DATA_SIZE(fs_info),
+			   leaf_space_used(leaf, 0, nritems), nritems);
 	}
 	return ret;
 }
@@ -3585,6 +3605,7 @@ static noinline int __push_leaf_right(struct btrfs_trans_handle *trans,
 				      int free_space, u32 left_nritems,
 				      u32 min_slot)
 {
+	struct btrfs_fs_info *fs_info = root->fs_info;
 	struct extent_buffer *left = path->nodes[0];
 	struct extent_buffer *upper = path->nodes[1];
 	struct btrfs_map_token token;
@@ -3654,11 +3675,11 @@ static noinline int __push_leaf_right(struct btrfs_trans_handle *trans,
 	memmove_extent_buffer(right,
 			      btrfs_leaf_data(right) + data_end - push_space,
 			      btrfs_leaf_data(right) + data_end,
-			      BTRFS_LEAF_DATA_SIZE(root->fs_info) - data_end);
+			      BTRFS_LEAF_DATA_SIZE(fs_info) - data_end);
 
 	/* copy from the left data area */
 	copy_extent_buffer(right, left, btrfs_leaf_data(right) +
-		     BTRFS_LEAF_DATA_SIZE(root->fs_info) - push_space,
+		     BTRFS_LEAF_DATA_SIZE(fs_info) - push_space,
 		     btrfs_leaf_data(left) + leaf_data_end(root, left),
 		     push_space);
 
@@ -3674,7 +3695,7 @@ static noinline int __push_leaf_right(struct btrfs_trans_handle *trans,
 	/* update the item pointers */
 	right_nritems += push_items;
 	btrfs_set_header_nritems(right, right_nritems);
-	push_space = BTRFS_LEAF_DATA_SIZE(root->fs_info);
+	push_space = BTRFS_LEAF_DATA_SIZE(fs_info);
 	for (i = 0; i < right_nritems; i++) {
 		item = btrfs_item_nr(i);
 		push_space -= btrfs_token_item_size(right, item, &token);
@@ -3687,7 +3708,7 @@ static noinline int __push_leaf_right(struct btrfs_trans_handle *trans,
 	if (left_nritems)
 		btrfs_mark_buffer_dirty(left);
 	else
-		clean_tree_block(trans, root->fs_info, left);
+		clean_tree_block(trans, fs_info, left);
 
 	btrfs_mark_buffer_dirty(right);
 
@@ -3699,7 +3720,7 @@ static noinline int __push_leaf_right(struct btrfs_trans_handle *trans,
 	if (path->slots[0] >= left_nritems) {
 		path->slots[0] -= left_nritems;
 		if (btrfs_header_nritems(path->nodes[0]) == 0)
-			clean_tree_block(trans, root->fs_info, path->nodes[0]);
+			clean_tree_block(trans, fs_info, path->nodes[0]);
 		btrfs_tree_unlock(path->nodes[0]);
 		free_extent_buffer(path->nodes[0]);
 		path->nodes[0] = right;
@@ -3814,6 +3835,7 @@ static noinline int __push_leaf_left(struct btrfs_trans_handle *trans,
 				     int free_space, u32 right_nritems,
 				     u32 max_slot)
 {
+	struct btrfs_fs_info *fs_info = root->fs_info;
 	struct btrfs_disk_key disk_key;
 	struct extent_buffer *right = path->nodes[0];
 	int i;
@@ -3870,7 +3892,7 @@ static noinline int __push_leaf_left(struct btrfs_trans_handle *trans,
 			   btrfs_item_nr_offset(0),
 			   push_items * sizeof(struct btrfs_item));
 
-	push_space = BTRFS_LEAF_DATA_SIZE(root->fs_info) -
+	push_space = BTRFS_LEAF_DATA_SIZE(fs_info) -
 		     btrfs_item_offset_nr(right, push_items - 1);
 
 	copy_extent_buffer(left, right, btrfs_leaf_data(left) +
@@ -3889,7 +3911,7 @@ static noinline int __push_leaf_left(struct btrfs_trans_handle *trans,
 
 		ioff = btrfs_token_item_offset(left, item, &token);
 		btrfs_set_token_item_offset(left, item,
-		      ioff - (BTRFS_LEAF_DATA_SIZE(root->fs_info) - old_left_item_size),
+		      ioff - (BTRFS_LEAF_DATA_SIZE(fs_info) - old_left_item_size),
 		      &token);
 	}
 	btrfs_set_header_nritems(left, old_left_nritems + push_items);
@@ -3903,7 +3925,7 @@ static noinline int __push_leaf_left(struct btrfs_trans_handle *trans,
 		push_space = btrfs_item_offset_nr(right, push_items - 1) -
 						  leaf_data_end(root, right);
 		memmove_extent_buffer(right, btrfs_leaf_data(right) +
-				      BTRFS_LEAF_DATA_SIZE(root->fs_info) - push_space,
+				      BTRFS_LEAF_DATA_SIZE(fs_info) - push_space,
 				      btrfs_leaf_data(right) +
 				      leaf_data_end(root, right), push_space);
 
@@ -3914,7 +3936,7 @@ static noinline int __push_leaf_left(struct btrfs_trans_handle *trans,
 	}
 	right_nritems -= push_items;
 	btrfs_set_header_nritems(right, right_nritems);
-	push_space = BTRFS_LEAF_DATA_SIZE(root->fs_info);
+	push_space = BTRFS_LEAF_DATA_SIZE(fs_info);
 	for (i = 0; i < right_nritems; i++) {
 		item = btrfs_item_nr(i);
 
@@ -3927,10 +3949,10 @@ static noinline int __push_leaf_left(struct btrfs_trans_handle *trans,
 	if (right_nritems)
 		btrfs_mark_buffer_dirty(right);
 	else
-		clean_tree_block(trans, root->fs_info, right);
+		clean_tree_block(trans, fs_info, right);
 
 	btrfs_item_key(right, &disk_key, 0);
-	fixup_low_keys(root->fs_info, path, &disk_key, 1);
+	fixup_low_keys(fs_info, path, &disk_key, 1);
 
 	/* then fixup the leaf pointer in the path */
 	if (path->slots[0] < push_items) {
@@ -4036,6 +4058,7 @@ static noinline void copy_for_split(struct btrfs_trans_handle *trans,
 				    struct extent_buffer *right,
 				    int slot, int mid, int nritems)
 {
+	struct btrfs_fs_info *fs_info = root->fs_info;
 	int data_copy_size;
 	int rt_data_off;
 	int i;
@@ -4053,12 +4076,11 @@ static noinline void copy_for_split(struct btrfs_trans_handle *trans,
 			   nritems * sizeof(struct btrfs_item));
 
 	copy_extent_buffer(right, l,
-		     btrfs_leaf_data(right) + BTRFS_LEAF_DATA_SIZE(root->fs_info) -
+		     btrfs_leaf_data(right) + BTRFS_LEAF_DATA_SIZE(fs_info) -
 		     data_copy_size, btrfs_leaf_data(l) +
 		     leaf_data_end(root, l), data_copy_size);
 
-	rt_data_off = BTRFS_LEAF_DATA_SIZE(root->fs_info) -
-		      btrfs_item_end_nr(l, mid);
+	rt_data_off = BTRFS_LEAF_DATA_SIZE(fs_info) - btrfs_item_end_nr(l, mid);
 
 	for (i = 0; i < nritems; i++) {
 		struct btrfs_item *item = btrfs_item_nr(i);
@@ -4181,7 +4203,7 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 	l = path->nodes[0];
 	slot = path->slots[0];
 	if (extend && data_size + btrfs_item_size_nr(l, slot) +
-	    sizeof(struct btrfs_item) > BTRFS_LEAF_DATA_SIZE(root->fs_info))
+	    sizeof(struct btrfs_item) > BTRFS_LEAF_DATA_SIZE(fs_info))
 		return -EOVERFLOW;
 
 	/* first try to make some room by pushing left and right */
@@ -4223,14 +4245,14 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 	if (mid <= slot) {
 		if (nritems == 1 ||
 		    leaf_space_used(l, mid, nritems - mid) + data_size >
-			BTRFS_LEAF_DATA_SIZE(root->fs_info)) {
+			BTRFS_LEAF_DATA_SIZE(fs_info)) {
 			if (slot >= nritems) {
 				split = 0;
 			} else {
 				mid = slot;
 				if (mid != nritems &&
 				    leaf_space_used(l, mid, nritems - mid) +
-				    data_size > BTRFS_LEAF_DATA_SIZE(root->fs_info)) {
+				    data_size > BTRFS_LEAF_DATA_SIZE(fs_info)) {
 					if (data_size && !tried_avoid_double)
 						goto push_for_double;
 					split = 2;
@@ -4239,7 +4261,7 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 		}
 	} else {
 		if (leaf_space_used(l, 0, mid) + data_size >
-			BTRFS_LEAF_DATA_SIZE(root->fs_info)) {
+			BTRFS_LEAF_DATA_SIZE(fs_info)) {
 			if (!extend && data_size && slot == 0) {
 				split = 0;
 			} else if ((extend || !data_size) && slot == 0) {
@@ -4248,7 +4270,7 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 				mid = slot;
 				if (mid != nritems &&
 				    leaf_space_used(l, mid, nritems - mid) +
-				    data_size > BTRFS_LEAF_DATA_SIZE(root->fs_info)) {
+				    data_size > BTRFS_LEAF_DATA_SIZE(fs_info)) {
 					if (data_size && !tried_avoid_double)
 						goto push_for_double;
 					split = 2;
@@ -4267,7 +4289,7 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 	if (IS_ERR(right))
 		return PTR_ERR(right);
 
-	root_add_used(root, root->fs_info->nodesize);
+	root_add_used(root, fs_info->nodesize);
 
 	memzero_extent_buffer(right, 0, sizeof(struct btrfs_header));
 	btrfs_set_header_bytenr(right, right->start);
@@ -4539,6 +4561,7 @@ int btrfs_duplicate_item(struct btrfs_trans_handle *trans,
 void btrfs_truncate_item(struct btrfs_root *root, struct btrfs_path *path,
 			 u32 new_size, int from_end)
 {
+	struct btrfs_fs_info *fs_info = root->fs_info;
 	int slot;
 	struct extent_buffer *leaf;
 	struct btrfs_item *item;
@@ -4619,7 +4642,7 @@ void btrfs_truncate_item(struct btrfs_root *root, struct btrfs_path *path,
 		btrfs_set_disk_key_offset(&disk_key, offset + size_diff);
 		btrfs_set_item_key(leaf, &disk_key, slot);
 		if (slot == 0)
-			fixup_low_keys(root->fs_info, path, &disk_key, 1);
+			fixup_low_keys(fs_info, path, &disk_key, 1);
 	}
 
 	item = btrfs_item_nr(slot);
@@ -4638,6 +4661,7 @@ void btrfs_truncate_item(struct btrfs_root *root, struct btrfs_path *path,
 void btrfs_extend_item(struct btrfs_root *root, struct btrfs_path *path,
 		       u32 data_size)
 {
+	struct btrfs_fs_info *fs_info = root->fs_info;
 	int slot;
 	struct extent_buffer *leaf;
 	struct btrfs_item *item;
@@ -4665,8 +4689,8 @@ void btrfs_extend_item(struct btrfs_root *root, struct btrfs_path *path,
 	BUG_ON(slot < 0);
 	if (slot >= nritems) {
 		btrfs_print_leaf(root, leaf);
-		btrfs_crit(root->fs_info, "slot %d too large, nritems %d",
-		       slot, nritems);
+		btrfs_crit(fs_info, "slot %d too large, nritems %d",
+			   slot, nritems);
 		BUG_ON(1);
 	}
 
@@ -4709,6 +4733,7 @@ void setup_items_for_insert(struct btrfs_root *root, struct btrfs_path *path,
 			    struct btrfs_key *cpu_key, u32 *data_size,
 			    u32 total_data, u32 total_size, int nr)
 {
+	struct btrfs_fs_info *fs_info = root->fs_info;
 	struct btrfs_item *item;
 	int i;
 	u32 nritems;
@@ -4720,7 +4745,7 @@ void setup_items_for_insert(struct btrfs_root *root, struct btrfs_path *path,
 
 	if (path->slots[0] == 0) {
 		btrfs_cpu_key_to_disk(&disk_key, cpu_key);
-		fixup_low_keys(root->fs_info, path, &disk_key, 1);
+		fixup_low_keys(fs_info, path, &disk_key, 1);
 	}
 	btrfs_unlock_up_safe(path, 1);
 
@@ -4734,8 +4759,7 @@ void setup_items_for_insert(struct btrfs_root *root, struct btrfs_path *path,
 
 	if (btrfs_leaf_free_space(root, leaf) < total_size) {
 		btrfs_print_leaf(root, leaf);
-		btrfs_crit(root->fs_info,
-			   "not enough freespace need %u have %d",
+		btrfs_crit(fs_info, "not enough freespace need %u have %d",
 			   total_size, btrfs_leaf_free_space(root, leaf));
 		BUG();
 	}
@@ -4745,8 +4769,7 @@ void setup_items_for_insert(struct btrfs_root *root, struct btrfs_path *path,
 
 		if (old_data < data_end) {
 			btrfs_print_leaf(root, leaf);
-			btrfs_crit(root->fs_info,
-				   "slot %d old_data %d data_end %d",
+			btrfs_crit(fs_info, "slot %d old_data %d data_end %d",
 				   slot, old_data, data_end);
 			BUG_ON(1);
 		}
@@ -4864,6 +4887,7 @@ int btrfs_insert_item(struct btrfs_trans_handle *trans, struct btrfs_root
 static void del_ptr(struct btrfs_root *root, struct btrfs_path *path,
 		    int level, int slot)
 {
+	struct btrfs_fs_info *fs_info = root->fs_info;
 	struct extent_buffer *parent = path->nodes[level];
 	u32 nritems;
 	int ret;
@@ -4871,7 +4895,7 @@ static void del_ptr(struct btrfs_root *root, struct btrfs_path *path,
 	nritems = btrfs_header_nritems(parent);
 	if (slot != nritems - 1) {
 		if (level)
-			tree_mod_log_eb_move(root->fs_info, parent, slot,
+			tree_mod_log_eb_move(fs_info, parent, slot,
 					     slot + 1, nritems - slot - 1);
 		memmove_extent_buffer(parent,
 			      btrfs_node_key_ptr_offset(slot),
@@ -4879,7 +4903,7 @@ static void del_ptr(struct btrfs_root *root, struct btrfs_path *path,
 			      sizeof(struct btrfs_key_ptr) *
 			      (nritems - slot - 1));
 	} else if (level) {
-		ret = tree_mod_log_insert_key(root->fs_info, parent, slot,
+		ret = tree_mod_log_insert_key(fs_info, parent, slot,
 					      MOD_LOG_KEY_REMOVE, GFP_NOFS);
 		BUG_ON(ret < 0);
 	}
@@ -4894,7 +4918,7 @@ static void del_ptr(struct btrfs_root *root, struct btrfs_path *path,
 		struct btrfs_disk_key disk_key;
 
 		btrfs_node_key(parent, &disk_key, 0);
-		fixup_low_keys(root->fs_info, path, &disk_key, level + 1);
+		fixup_low_keys(fs_info, path, &disk_key, level + 1);
 	}
 	btrfs_mark_buffer_dirty(parent);
 }
@@ -4936,6 +4960,7 @@ static noinline void btrfs_del_leaf(struct btrfs_trans_handle *trans,
 int btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 		    struct btrfs_path *path, int slot, int nr)
 {
+	struct btrfs_fs_info *fs_info = root->fs_info;
 	struct extent_buffer *leaf;
 	struct btrfs_item *item;
 	u32 last_off;
@@ -4987,7 +5012,7 @@ int btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 			btrfs_set_header_level(leaf, 0);
 		} else {
 			btrfs_set_path_blocking(path);
-			clean_tree_block(trans, root->fs_info, leaf);
+			clean_tree_block(trans, fs_info, leaf);
 			btrfs_del_leaf(trans, root, path, leaf);
 		}
 	} else {
@@ -4996,11 +5021,11 @@ int btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 			struct btrfs_disk_key disk_key;
 
 			btrfs_item_key(leaf, &disk_key, 0);
-			fixup_low_keys(root->fs_info, path, &disk_key, 1);
+			fixup_low_keys(fs_info, path, &disk_key, 1);
 		}
 
 		/* delete the leaf if it is mostly empty */
-		if (used < BTRFS_LEAF_DATA_SIZE(root->fs_info) / 3) {
+		if (used < BTRFS_LEAF_DATA_SIZE(fs_info) / 3) {
 			/* push_leaf_left fixes the path.
 			 * make sure the path still points to our leaf
 			 * for possible call to del_ptr below
@@ -5337,6 +5362,7 @@ int btrfs_compare_trees(struct btrfs_root *left_root,
 			struct btrfs_root *right_root,
 			btrfs_changed_cb_t changed_cb, void *ctx)
 {
+	struct btrfs_fs_info *fs_info = left_root->fs_info;
 	int ret;
 	int cmp;
 	struct btrfs_path *left_path = NULL;
@@ -5368,10 +5394,9 @@ int btrfs_compare_trees(struct btrfs_root *left_root,
 		goto out;
 	}
 
-	tmp_buf = kmalloc(left_root->fs_info->nodesize,
-			  GFP_KERNEL | __GFP_NOWARN);
+	tmp_buf = kmalloc(fs_info->nodesize, GFP_KERNEL | __GFP_NOWARN);
 	if (!tmp_buf) {
-		tmp_buf = vmalloc(left_root->fs_info->nodesize);
+		tmp_buf = vmalloc(fs_info->nodesize);
 		if (!tmp_buf) {
 			ret = -ENOMEM;
 			goto out;
@@ -5419,7 +5444,7 @@ int btrfs_compare_trees(struct btrfs_root *left_root,
 	 *   the right if possible or go up and right.
 	 */
 
-	down_read(&left_root->fs_info->commit_root_sem);
+	down_read(&fs_info->commit_root_sem);
 	left_level = btrfs_header_level(left_root->commit_root);
 	left_root_level = left_level;
 	left_path->nodes[left_level] = left_root->commit_root;
@@ -5429,7 +5454,7 @@ int btrfs_compare_trees(struct btrfs_root *left_root,
 	right_root_level = right_level;
 	right_path->nodes[right_level] = right_root->commit_root;
 	extent_buffer_get(right_path->nodes[right_level]);
-	up_read(&left_root->fs_info->commit_root_sem);
+	up_read(&fs_info->commit_root_sem);
 
 	if (left_level == 0)
 		btrfs_item_key_to_cpu(left_path->nodes[left_level],

commit da17066c40472c2d6a1aab7bb0090c3d285531c9
Author: Jeff Mahoney <jeffm@suse.com>
Date:   Wed Jun 15 09:22:56 2016 -0400

    btrfs: pull node/sector/stripe sizes out of root and into fs_info
    
    We track the node sizes per-root, but they never vary from the values
    in the superblock.  This patch messes with the 80-column style a bit,
    but subsequent patches to factor out root->fs_info into a convenience
    variable fix it up again.
    
    Signed-off-by: Jeff Mahoney <jeffm@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 25286a5912fc..431b150a823a 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1357,8 +1357,7 @@ tree_mod_log_rewind(struct btrfs_fs_info *fs_info, struct btrfs_path *path,
 
 	if (tm->op == MOD_LOG_KEY_REMOVE_WHILE_FREEING) {
 		BUG_ON(tm->slot != 0);
-		eb_rewin = alloc_dummy_extent_buffer(fs_info, eb->start,
-						eb->len);
+		eb_rewin = alloc_dummy_extent_buffer(fs_info, eb->start);
 		if (!eb_rewin) {
 			btrfs_tree_read_unlock_blocking(eb);
 			free_extent_buffer(eb);
@@ -1386,7 +1385,7 @@ tree_mod_log_rewind(struct btrfs_fs_info *fs_info, struct btrfs_path *path,
 	btrfs_tree_read_lock(eb_rewin);
 	__tree_mod_log_rewind(fs_info, eb_rewin, time_seq, tm);
 	WARN_ON(btrfs_header_nritems(eb_rewin) >
-		BTRFS_NODEPTRS_PER_BLOCK(fs_info->tree_root));
+		BTRFS_NODEPTRS_PER_BLOCK(fs_info));
 
 	return eb_rewin;
 }
@@ -1439,8 +1438,7 @@ get_old_root(struct btrfs_root *root, u64 time_seq)
 	} else if (old_root) {
 		btrfs_tree_read_unlock(eb_root);
 		free_extent_buffer(eb_root);
-		eb = alloc_dummy_extent_buffer(root->fs_info, logical,
-					root->nodesize);
+		eb = alloc_dummy_extent_buffer(root->fs_info, logical);
 	} else {
 		btrfs_set_lock_blocking_rw(eb_root, BTRFS_READ_LOCK);
 		eb = btrfs_clone_extent_buffer(eb_root);
@@ -1463,7 +1461,7 @@ get_old_root(struct btrfs_root *root, u64 time_seq)
 		__tree_mod_log_rewind(root->fs_info, eb, time_seq, tm);
 	else
 		WARN_ON(btrfs_header_level(eb) != 0);
-	WARN_ON(btrfs_header_nritems(eb) > BTRFS_NODEPTRS_PER_BLOCK(root));
+	WARN_ON(btrfs_header_nritems(eb) > BTRFS_NODEPTRS_PER_BLOCK(root->fs_info));
 
 	return eb;
 }
@@ -1634,7 +1632,7 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 	WARN_ON(trans->transid != root->fs_info->generation);
 
 	parent_nritems = btrfs_header_nritems(parent);
-	blocksize = root->nodesize;
+	blocksize = root->fs_info->nodesize;
 	end_slot = parent_nritems - 1;
 
 	if (parent_nritems <= 1)
@@ -1940,7 +1938,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		return 0;
 	}
 	if (btrfs_header_nritems(mid) >
-	    BTRFS_NODEPTRS_PER_BLOCK(root) / 4)
+	    BTRFS_NODEPTRS_PER_BLOCK(root->fs_info) / 4)
 		return 0;
 
 	left = read_node_slot(root, parent, pslot - 1);
@@ -2127,7 +2125,7 @@ static noinline int push_nodes_for_insert(struct btrfs_trans_handle *trans,
 		btrfs_set_lock_blocking(left);
 
 		left_nr = btrfs_header_nritems(left);
-		if (left_nr >= BTRFS_NODEPTRS_PER_BLOCK(root) - 1) {
+		if (left_nr >= BTRFS_NODEPTRS_PER_BLOCK(root->fs_info) - 1) {
 			wret = 1;
 		} else {
 			ret = btrfs_cow_block(trans, root, left, parent,
@@ -2181,7 +2179,7 @@ static noinline int push_nodes_for_insert(struct btrfs_trans_handle *trans,
 		btrfs_set_lock_blocking(right);
 
 		right_nr = btrfs_header_nritems(right);
-		if (right_nr >= BTRFS_NODEPTRS_PER_BLOCK(root) - 1) {
+		if (right_nr >= BTRFS_NODEPTRS_PER_BLOCK(root->fs_info) - 1) {
 			wret = 1;
 		} else {
 			ret = btrfs_cow_block(trans, root, right,
@@ -2252,7 +2250,7 @@ static void reada_for_search(struct btrfs_root *root,
 	node = path->nodes[level];
 
 	search = btrfs_node_blockptr(node, slot);
-	blocksize = root->nodesize;
+	blocksize = root->fs_info->nodesize;
 	eb = find_extent_buffer(root->fs_info, search);
 	if (eb) {
 		free_extent_buffer(eb);
@@ -2521,7 +2519,7 @@ setup_nodes_for_search(struct btrfs_trans_handle *trans,
 {
 	int ret;
 	if ((p->search_for_split || ins_len > 0) && btrfs_header_nritems(b) >=
-	    BTRFS_NODEPTRS_PER_BLOCK(root) - 3) {
+	    BTRFS_NODEPTRS_PER_BLOCK(root->fs_info) - 3) {
 		int sret;
 
 		if (*write_lock_level < level + 1) {
@@ -2542,7 +2540,7 @@ setup_nodes_for_search(struct btrfs_trans_handle *trans,
 		}
 		b = p->nodes[level];
 	} else if (ins_len < 0 && btrfs_header_nritems(b) <
-		   BTRFS_NODEPTRS_PER_BLOCK(root) / 2) {
+		   BTRFS_NODEPTRS_PER_BLOCK(root->fs_info) / 2) {
 		int sret;
 
 		if (*write_lock_level < level + 1) {
@@ -3195,7 +3193,7 @@ static int push_node_left(struct btrfs_trans_handle *trans,
 
 	src_nritems = btrfs_header_nritems(src);
 	dst_nritems = btrfs_header_nritems(dst);
-	push_items = BTRFS_NODEPTRS_PER_BLOCK(root) - dst_nritems;
+	push_items = BTRFS_NODEPTRS_PER_BLOCK(root->fs_info) - dst_nritems;
 	WARN_ON(btrfs_header_generation(src) != trans->transid);
 	WARN_ON(btrfs_header_generation(dst) != trans->transid);
 
@@ -3274,7 +3272,7 @@ static int balance_node_right(struct btrfs_trans_handle *trans,
 
 	src_nritems = btrfs_header_nritems(src);
 	dst_nritems = btrfs_header_nritems(dst);
-	push_items = BTRFS_NODEPTRS_PER_BLOCK(root) - dst_nritems;
+	push_items = BTRFS_NODEPTRS_PER_BLOCK(root->fs_info) - dst_nritems;
 	if (push_items <= 0)
 		return 1;
 
@@ -3346,7 +3344,7 @@ static noinline int insert_new_root(struct btrfs_trans_handle *trans,
 	if (IS_ERR(c))
 		return PTR_ERR(c);
 
-	root_add_used(root, root->nodesize);
+	root_add_used(root, root->fs_info->nodesize);
 
 	memzero_extent_buffer(c, 0, sizeof(struct btrfs_header));
 	btrfs_set_header_nritems(c, 1);
@@ -3404,7 +3402,7 @@ static void insert_ptr(struct btrfs_trans_handle *trans,
 	lower = path->nodes[level];
 	nritems = btrfs_header_nritems(lower);
 	BUG_ON(slot > nritems);
-	BUG_ON(nritems == BTRFS_NODEPTRS_PER_BLOCK(root));
+	BUG_ON(nritems == BTRFS_NODEPTRS_PER_BLOCK(root->fs_info));
 	if (slot != nritems) {
 		if (level)
 			tree_mod_log_eb_move(root->fs_info, lower, slot + 1,
@@ -3467,7 +3465,7 @@ static noinline int split_node(struct btrfs_trans_handle *trans,
 		ret = push_nodes_for_insert(trans, root, path, level);
 		c = path->nodes[level];
 		if (!ret && btrfs_header_nritems(c) <
-		    BTRFS_NODEPTRS_PER_BLOCK(root) - 3)
+		    BTRFS_NODEPTRS_PER_BLOCK(root->fs_info) - 3)
 			return 0;
 		if (ret < 0)
 			return ret;
@@ -3482,7 +3480,7 @@ static noinline int split_node(struct btrfs_trans_handle *trans,
 	if (IS_ERR(split))
 		return PTR_ERR(split);
 
-	root_add_used(root, root->nodesize);
+	root_add_used(root, root->fs_info->nodesize);
 
 	memzero_extent_buffer(split, 0, sizeof(struct btrfs_header));
 	btrfs_set_header_level(split, btrfs_header_level(c));
@@ -3564,11 +3562,12 @@ noinline int btrfs_leaf_free_space(struct btrfs_root *root,
 {
 	int nritems = btrfs_header_nritems(leaf);
 	int ret;
-	ret = BTRFS_LEAF_DATA_SIZE(root) - leaf_space_used(leaf, 0, nritems);
+	ret = BTRFS_LEAF_DATA_SIZE(root->fs_info) - leaf_space_used(leaf, 0, nritems);
 	if (ret < 0) {
 		btrfs_crit(root->fs_info,
 			"leaf free space ret %d, leaf data size %lu, used %d nritems %d",
-		       ret, (unsigned long) BTRFS_LEAF_DATA_SIZE(root),
+		       ret,
+		       (unsigned long) BTRFS_LEAF_DATA_SIZE(root->fs_info),
 		       leaf_space_used(leaf, 0, nritems), nritems);
 	}
 	return ret;
@@ -3655,11 +3654,11 @@ static noinline int __push_leaf_right(struct btrfs_trans_handle *trans,
 	memmove_extent_buffer(right,
 			      btrfs_leaf_data(right) + data_end - push_space,
 			      btrfs_leaf_data(right) + data_end,
-			      BTRFS_LEAF_DATA_SIZE(root) - data_end);
+			      BTRFS_LEAF_DATA_SIZE(root->fs_info) - data_end);
 
 	/* copy from the left data area */
 	copy_extent_buffer(right, left, btrfs_leaf_data(right) +
-		     BTRFS_LEAF_DATA_SIZE(root) - push_space,
+		     BTRFS_LEAF_DATA_SIZE(root->fs_info) - push_space,
 		     btrfs_leaf_data(left) + leaf_data_end(root, left),
 		     push_space);
 
@@ -3675,7 +3674,7 @@ static noinline int __push_leaf_right(struct btrfs_trans_handle *trans,
 	/* update the item pointers */
 	right_nritems += push_items;
 	btrfs_set_header_nritems(right, right_nritems);
-	push_space = BTRFS_LEAF_DATA_SIZE(root);
+	push_space = BTRFS_LEAF_DATA_SIZE(root->fs_info);
 	for (i = 0; i < right_nritems; i++) {
 		item = btrfs_item_nr(i);
 		push_space -= btrfs_token_item_size(right, item, &token);
@@ -3871,7 +3870,7 @@ static noinline int __push_leaf_left(struct btrfs_trans_handle *trans,
 			   btrfs_item_nr_offset(0),
 			   push_items * sizeof(struct btrfs_item));
 
-	push_space = BTRFS_LEAF_DATA_SIZE(root) -
+	push_space = BTRFS_LEAF_DATA_SIZE(root->fs_info) -
 		     btrfs_item_offset_nr(right, push_items - 1);
 
 	copy_extent_buffer(left, right, btrfs_leaf_data(left) +
@@ -3890,7 +3889,7 @@ static noinline int __push_leaf_left(struct btrfs_trans_handle *trans,
 
 		ioff = btrfs_token_item_offset(left, item, &token);
 		btrfs_set_token_item_offset(left, item,
-		      ioff - (BTRFS_LEAF_DATA_SIZE(root) - old_left_item_size),
+		      ioff - (BTRFS_LEAF_DATA_SIZE(root->fs_info) - old_left_item_size),
 		      &token);
 	}
 	btrfs_set_header_nritems(left, old_left_nritems + push_items);
@@ -3904,7 +3903,7 @@ static noinline int __push_leaf_left(struct btrfs_trans_handle *trans,
 		push_space = btrfs_item_offset_nr(right, push_items - 1) -
 						  leaf_data_end(root, right);
 		memmove_extent_buffer(right, btrfs_leaf_data(right) +
-				      BTRFS_LEAF_DATA_SIZE(root) - push_space,
+				      BTRFS_LEAF_DATA_SIZE(root->fs_info) - push_space,
 				      btrfs_leaf_data(right) +
 				      leaf_data_end(root, right), push_space);
 
@@ -3915,7 +3914,7 @@ static noinline int __push_leaf_left(struct btrfs_trans_handle *trans,
 	}
 	right_nritems -= push_items;
 	btrfs_set_header_nritems(right, right_nritems);
-	push_space = BTRFS_LEAF_DATA_SIZE(root);
+	push_space = BTRFS_LEAF_DATA_SIZE(root->fs_info);
 	for (i = 0; i < right_nritems; i++) {
 		item = btrfs_item_nr(i);
 
@@ -4054,11 +4053,11 @@ static noinline void copy_for_split(struct btrfs_trans_handle *trans,
 			   nritems * sizeof(struct btrfs_item));
 
 	copy_extent_buffer(right, l,
-		     btrfs_leaf_data(right) + BTRFS_LEAF_DATA_SIZE(root) -
+		     btrfs_leaf_data(right) + BTRFS_LEAF_DATA_SIZE(root->fs_info) -
 		     data_copy_size, btrfs_leaf_data(l) +
 		     leaf_data_end(root, l), data_copy_size);
 
-	rt_data_off = BTRFS_LEAF_DATA_SIZE(root) -
+	rt_data_off = BTRFS_LEAF_DATA_SIZE(root->fs_info) -
 		      btrfs_item_end_nr(l, mid);
 
 	for (i = 0; i < nritems; i++) {
@@ -4182,7 +4181,7 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 	l = path->nodes[0];
 	slot = path->slots[0];
 	if (extend && data_size + btrfs_item_size_nr(l, slot) +
-	    sizeof(struct btrfs_item) > BTRFS_LEAF_DATA_SIZE(root))
+	    sizeof(struct btrfs_item) > BTRFS_LEAF_DATA_SIZE(root->fs_info))
 		return -EOVERFLOW;
 
 	/* first try to make some room by pushing left and right */
@@ -4224,14 +4223,14 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 	if (mid <= slot) {
 		if (nritems == 1 ||
 		    leaf_space_used(l, mid, nritems - mid) + data_size >
-			BTRFS_LEAF_DATA_SIZE(root)) {
+			BTRFS_LEAF_DATA_SIZE(root->fs_info)) {
 			if (slot >= nritems) {
 				split = 0;
 			} else {
 				mid = slot;
 				if (mid != nritems &&
 				    leaf_space_used(l, mid, nritems - mid) +
-				    data_size > BTRFS_LEAF_DATA_SIZE(root)) {
+				    data_size > BTRFS_LEAF_DATA_SIZE(root->fs_info)) {
 					if (data_size && !tried_avoid_double)
 						goto push_for_double;
 					split = 2;
@@ -4240,7 +4239,7 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 		}
 	} else {
 		if (leaf_space_used(l, 0, mid) + data_size >
-			BTRFS_LEAF_DATA_SIZE(root)) {
+			BTRFS_LEAF_DATA_SIZE(root->fs_info)) {
 			if (!extend && data_size && slot == 0) {
 				split = 0;
 			} else if ((extend || !data_size) && slot == 0) {
@@ -4249,7 +4248,7 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 				mid = slot;
 				if (mid != nritems &&
 				    leaf_space_used(l, mid, nritems - mid) +
-				    data_size > BTRFS_LEAF_DATA_SIZE(root)) {
+				    data_size > BTRFS_LEAF_DATA_SIZE(root->fs_info)) {
 					if (data_size && !tried_avoid_double)
 						goto push_for_double;
 					split = 2;
@@ -4268,7 +4267,7 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 	if (IS_ERR(right))
 		return PTR_ERR(right);
 
-	root_add_used(root, root->nodesize);
+	root_add_used(root, root->fs_info->nodesize);
 
 	memzero_extent_buffer(right, 0, sizeof(struct btrfs_header));
 	btrfs_set_header_bytenr(right, right->start);
@@ -5001,7 +5000,7 @@ int btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 		}
 
 		/* delete the leaf if it is mostly empty */
-		if (used < BTRFS_LEAF_DATA_SIZE(root) / 3) {
+		if (used < BTRFS_LEAF_DATA_SIZE(root->fs_info) / 3) {
 			/* push_leaf_left fixes the path.
 			 * make sure the path still points to our leaf
 			 * for possible call to del_ptr below
@@ -5369,9 +5368,10 @@ int btrfs_compare_trees(struct btrfs_root *left_root,
 		goto out;
 	}
 
-	tmp_buf = kmalloc(left_root->nodesize, GFP_KERNEL | __GFP_NOWARN);
+	tmp_buf = kmalloc(left_root->fs_info->nodesize,
+			  GFP_KERNEL | __GFP_NOWARN);
 	if (!tmp_buf) {
-		tmp_buf = vmalloc(left_root->nodesize);
+		tmp_buf = vmalloc(left_root->fs_info->nodesize);
 		if (!tmp_buf) {
 			ret = -ENOMEM;
 			goto out;

commit 58e8012cc12b3cdebea118981c4fd7136d52f2c7
Author: David Sterba <dsterba@suse.com>
Date:   Tue Nov 8 18:30:31 2016 +0100

    btrfs: add optimized version of eb to eb copy
    
    Using copy_extent_buffer is suitable for copying betwenn buffers from an
    arbitrary offset and deals with page boundaries. This is not necessary
    when doing a full extent_buffer-to-extent_buffer copy. We can utilize
    the copy_page helper as well.
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index be362b776138..25286a5912fc 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -260,7 +260,7 @@ int btrfs_copy_root(struct btrfs_trans_handle *trans,
 	if (IS_ERR(cow))
 		return PTR_ERR(cow);
 
-	copy_extent_buffer(cow, buf, 0, 0, cow->len);
+	copy_extent_buffer_full(cow, buf);
 	btrfs_set_header_bytenr(cow, cow->start);
 	btrfs_set_header_generation(cow, trans->transid);
 	btrfs_set_header_backref_rev(cow, BTRFS_MIXED_BACKREF_REV);
@@ -1129,7 +1129,7 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 
 	/* cow is set to blocking by btrfs_init_new_buffer */
 
-	copy_extent_buffer(cow, buf, 0, 0, cow->len);
+	copy_extent_buffer_full(cow, buf);
 	btrfs_set_header_bytenr(cow, cow->start);
 	btrfs_set_header_generation(cow, trans->transid);
 	btrfs_set_header_backref_rev(cow, BTRFS_MIXED_BACKREF_REV);

commit b159fa2808b1b53d784807a48ad95fa809be10b0
Author: David Sterba <dsterba@suse.com>
Date:   Tue Nov 8 18:09:03 2016 +0100

    btrfs: remove constant parameter to memset_extent_buffer and rename it
    
    The only memset we do is to 0, so sink the parameter to the function and
    simplify all calls. Rename the function to reflect the behaviour.
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 93bc38b98b3f..be362b776138 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -3348,7 +3348,7 @@ static noinline int insert_new_root(struct btrfs_trans_handle *trans,
 
 	root_add_used(root, root->nodesize);
 
-	memset_extent_buffer(c, 0, 0, sizeof(struct btrfs_header));
+	memzero_extent_buffer(c, 0, sizeof(struct btrfs_header));
 	btrfs_set_header_nritems(c, 1);
 	btrfs_set_header_level(c, level);
 	btrfs_set_header_bytenr(c, c->start);
@@ -3484,7 +3484,7 @@ static noinline int split_node(struct btrfs_trans_handle *trans,
 
 	root_add_used(root, root->nodesize);
 
-	memset_extent_buffer(split, 0, 0, sizeof(struct btrfs_header));
+	memzero_extent_buffer(split, 0, sizeof(struct btrfs_header));
 	btrfs_set_header_level(split, btrfs_header_level(c));
 	btrfs_set_header_bytenr(split, split->start);
 	btrfs_set_header_generation(split, trans->transid);
@@ -4270,7 +4270,7 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 
 	root_add_used(root, root->nodesize);
 
-	memset_extent_buffer(right, 0, 0, sizeof(struct btrfs_header));
+	memzero_extent_buffer(right, 0, sizeof(struct btrfs_header));
 	btrfs_set_header_bytenr(right, right->start);
 	btrfs_set_header_generation(right, trans->transid);
 	btrfs_set_header_backref_rev(right, BTRFS_MIXED_BACKREF_REV);

commit d24ee97b96db46123f766041d2ec0ca81491bd31
Author: David Sterba <dsterba@suse.com>
Date:   Wed Nov 9 17:44:25 2016 +0100

    btrfs: use new helpers to set uuids in eb
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 173768767d1b..93bc38b98b3f 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -271,8 +271,7 @@ int btrfs_copy_root(struct btrfs_trans_handle *trans,
 	else
 		btrfs_set_header_owner(cow, new_root_objectid);
 
-	write_extent_buffer(cow, root->fs_info->fsid, btrfs_header_fsid(),
-			    BTRFS_FSID_SIZE);
+	write_extent_buffer_fsid(cow, root->fs_info->fsid);
 
 	WARN_ON(btrfs_header_generation(buf) > trans->transid);
 	if (new_root_objectid == BTRFS_TREE_RELOC_OBJECTID)
@@ -1141,8 +1140,7 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 	else
 		btrfs_set_header_owner(cow, root->root_key.objectid);
 
-	write_extent_buffer(cow, root->fs_info->fsid, btrfs_header_fsid(),
-			    BTRFS_FSID_SIZE);
+	write_extent_buffer_fsid(cow, root->fs_info->fsid);
 
 	ret = update_ref_for_cow(trans, root, buf, cow, &last_ref);
 	if (ret) {
@@ -3358,11 +3356,8 @@ static noinline int insert_new_root(struct btrfs_trans_handle *trans,
 	btrfs_set_header_backref_rev(c, BTRFS_MIXED_BACKREF_REV);
 	btrfs_set_header_owner(c, root->root_key.objectid);
 
-	write_extent_buffer(c, root->fs_info->fsid, btrfs_header_fsid(),
-			    BTRFS_FSID_SIZE);
-
-	write_extent_buffer(c, root->fs_info->chunk_tree_uuid,
-			    btrfs_header_chunk_tree_uuid(c), BTRFS_UUID_SIZE);
+	write_extent_buffer_fsid(c, root->fs_info->fsid);
+	write_extent_buffer_chunk_tree_uuid(c, root->fs_info->chunk_tree_uuid);
 
 	btrfs_set_node_key(c, &lower_key, 0);
 	btrfs_set_node_blockptr(c, 0, lower->start);
@@ -3495,11 +3490,9 @@ static noinline int split_node(struct btrfs_trans_handle *trans,
 	btrfs_set_header_generation(split, trans->transid);
 	btrfs_set_header_backref_rev(split, BTRFS_MIXED_BACKREF_REV);
 	btrfs_set_header_owner(split, root->root_key.objectid);
-	write_extent_buffer(split, root->fs_info->fsid,
-			    btrfs_header_fsid(), BTRFS_FSID_SIZE);
-	write_extent_buffer(split, root->fs_info->chunk_tree_uuid,
-			    btrfs_header_chunk_tree_uuid(split),
-			    BTRFS_UUID_SIZE);
+	write_extent_buffer_fsid(split, root->fs_info->fsid);
+	write_extent_buffer_chunk_tree_uuid(split,
+			root->fs_info->chunk_tree_uuid);
 
 	ret = tree_mod_log_eb_copy(root->fs_info, split, c, 0,
 				   mid, c_nritems - mid);
@@ -4283,12 +4276,8 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 	btrfs_set_header_backref_rev(right, BTRFS_MIXED_BACKREF_REV);
 	btrfs_set_header_owner(right, root->root_key.objectid);
 	btrfs_set_header_level(right, 0);
-	write_extent_buffer(right, fs_info->fsid,
-			    btrfs_header_fsid(), BTRFS_FSID_SIZE);
-
-	write_extent_buffer(right, fs_info->chunk_tree_uuid,
-			    btrfs_header_chunk_tree_uuid(right),
-			    BTRFS_UUID_SIZE);
+	write_extent_buffer_fsid(right, fs_info->fsid);
+	write_extent_buffer_chunk_tree_uuid(right, fs_info->chunk_tree_uuid);
 
 	if (split == 0) {
 		if (mid <= slot) {

commit 62d1f9fe97dd25ca5e850bd7e140d4c9d4b9c7c7
Author: David Sterba <dsterba@suse.com>
Date:   Tue Nov 8 23:21:05 2016 +0100

    btrfs: remove trivial helper btrfs_find_tree_block
    
    During the time, the function has been shrunk to the point that it just
    calls find_extent_buffer, just passing the parameters.
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index f6ba165d3f81..173768767d1b 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1670,7 +1670,7 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 			continue;
 		}
 
-		cur = btrfs_find_tree_block(root->fs_info, blocknr);
+		cur = find_extent_buffer(root->fs_info, blocknr);
 		if (cur)
 			uptodate = btrfs_buffer_uptodate(cur, gen, 0);
 		else
@@ -2255,7 +2255,7 @@ static void reada_for_search(struct btrfs_root *root,
 
 	search = btrfs_node_blockptr(node, slot);
 	blocksize = root->nodesize;
-	eb = btrfs_find_tree_block(root->fs_info, search);
+	eb = find_extent_buffer(root->fs_info, search);
 	if (eb) {
 		free_extent_buffer(eb);
 		return;
@@ -2314,7 +2314,7 @@ static noinline void reada_for_balance(struct btrfs_root *root,
 	if (slot > 0) {
 		block1 = btrfs_node_blockptr(parent, slot - 1);
 		gen = btrfs_node_ptr_generation(parent, slot - 1);
-		eb = btrfs_find_tree_block(root->fs_info, block1);
+		eb = find_extent_buffer(root->fs_info, block1);
 		/*
 		 * if we get -eagain from btrfs_buffer_uptodate, we
 		 * don't want to return eagain here.  That will loop
@@ -2327,7 +2327,7 @@ static noinline void reada_for_balance(struct btrfs_root *root,
 	if (slot + 1 < nritems) {
 		block2 = btrfs_node_blockptr(parent, slot + 1);
 		gen = btrfs_node_ptr_generation(parent, slot + 1);
-		eb = btrfs_find_tree_block(root->fs_info, block2);
+		eb = find_extent_buffer(root->fs_info, block2);
 		if (eb && btrfs_buffer_uptodate(eb, gen, 1) != 0)
 			block2 = 0;
 		free_extent_buffer(eb);
@@ -2445,7 +2445,7 @@ read_block_for_search(struct btrfs_trans_handle *trans,
 	blocknr = btrfs_node_blockptr(b, slot);
 	gen = btrfs_node_ptr_generation(b, slot);
 
-	tmp = btrfs_find_tree_block(root->fs_info, blocknr);
+	tmp = find_extent_buffer(root->fs_info, blocknr);
 	if (tmp) {
 		/* first we do an atomic uptodate check */
 		if (btrfs_buffer_uptodate(tmp, gen, 1) > 0) {

commit 196e02490c934398f894e5cb0ee1ac8ad13ca576
Author: Liu Bo <bo.li.liu@oracle.com>
Date:   Wed Sep 7 14:48:28 2016 -0700

    Btrfs: remove unnecessary btrfs_mark_buffer_dirty in split_leaf
    
    When we're not able to get enough space through splitting leaf,
    we'd create a new sibling leaf instead, and it's possible that we return
     a zero-nritem sibling leaf and mark it dirty before it's in a consistent
    state.  With CONFIG_BTRFS_FS_CHECK_INTEGRITY=y, the integrity check of
    check_leaf will report panic due to this zero-nritem non-root leaf.
    
    This removes the unnecessary btrfs_mark_buffer_dirty.
    
    Reported-by: Filipe Manana <fdmanana@gmail.com>
    Signed-off-by: Liu Bo <bo.li.liu@oracle.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 8f67a14fa974..f6ba165d3f81 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -4311,7 +4311,11 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 			if (path->slots[1] == 0)
 				fixup_low_keys(fs_info, path, &disk_key, 1);
 		}
-		btrfs_mark_buffer_dirty(right);
+		/*
+		 * We create a new leaf 'right' for the required ins_len and
+		 * we'll do btrfs_mark_buffer_dirty() on this leaf after copying
+		 * the content of ins_len to 'right'.
+		 */
 		return ret;
 	}
 

commit 851cd173f06045816528176001cf82948282029c
Author: Liu Bo <bo.li.liu@oracle.com>
Date:   Fri Sep 23 13:44:44 2016 -0700

    Btrfs: memset to avoid stale content in btree leaf
    
    This is an additional patch to
    "Btrfs: memset to avoid stale content in btree node block".
    
    This uses memset to initialize the unused space in a leaf to avoid
    potential stale content, which may be incurred by pushing items
    between sibling leaves.
    
    Signed-off-by: Liu Bo <bo.li.liu@oracle.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 284cc9d6ecd7..8f67a14fa974 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1715,20 +1715,6 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 	return err;
 }
 
-/*
- * The leaf data grows from end-to-front in the node.
- * this returns the address of the start of the last item,
- * which is the stop of the leaf data stack
- */
-static inline unsigned int leaf_data_end(struct btrfs_root *root,
-					 struct extent_buffer *leaf)
-{
-	u32 nr = btrfs_header_nritems(leaf);
-	if (nr == 0)
-		return BTRFS_LEAF_DATA_SIZE(root);
-	return btrfs_item_offset_nr(leaf, nr - 1);
-}
-
 
 /*
  * search for key in the extent_buffer.  The items start at offset p,

commit 0f5053eb90f58c619885eac2757ccdc5eccd9046
Author: Goldwyn Rodrigues <rgoldwyn@suse.com>
Date:   Thu Sep 22 14:11:34 2016 -0500

    btrfs: parent_start initialization cleanup
    
    Code cleanup. parent_start is initialized multiple times when it is
    not necessary to do so.
    
    Signed-off-by: Goldwyn Rodrigues <rgoldwyn@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 0c910b1cb2f8..284cc9d6ecd7 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1100,7 +1100,7 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 	int level, ret;
 	int last_ref = 0;
 	int unlock_orig = 0;
-	u64 parent_start;
+	u64 parent_start = 0;
 
 	if (*cow_ret == buf)
 		unlock_orig = 1;
@@ -1119,13 +1119,8 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 	else
 		btrfs_node_key(buf, &disk_key, 0);
 
-	if (root->root_key.objectid == BTRFS_TREE_RELOC_OBJECTID) {
-		if (parent)
-			parent_start = parent->start;
-		else
-			parent_start = 0;
-	} else
-		parent_start = 0;
+	if ((root->root_key.objectid == BTRFS_TREE_RELOC_OBJECTID) && parent)
+		parent_start = parent->start;
 
 	cow = btrfs_alloc_tree_block(trans, root, parent_start,
 			root->root_key.objectid, &disk_key, level,
@@ -1168,8 +1163,6 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 		if (root->root_key.objectid == BTRFS_TREE_RELOC_OBJECTID ||
 		    btrfs_header_backref_rev(buf) < BTRFS_MIXED_BACKREF_REV)
 			parent_start = buf->start;
-		else
-			parent_start = 0;
 
 		extent_buffer_get(cow);
 		tree_mod_log_set_root_pointer(root, cow, 1);
@@ -1180,11 +1173,6 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 		free_extent_buffer(buf);
 		add_root_to_dirty_list(root);
 	} else {
-		if (root->root_key.objectid == BTRFS_TREE_RELOC_OBJECTID)
-			parent_start = parent->start;
-		else
-			parent_start = 0;
-
 		WARN_ON(trans->transid != btrfs_header_generation(parent));
 		tree_mod_log_insert_key(root->fs_info, parent, parent_slot,
 					MOD_LOG_KEY_REPLACE, GFP_NOFS);

commit 62e855771dacf7c4d6daf9741642a965e7066d31
Author: Jeff Mahoney <jeffm@suse.com>
Date:   Tue Sep 20 10:05:01 2016 -0400

    btrfs: convert printk(KERN_* to use pr_* calls
    
    This patch converts printk(KERN_* style messages to use the pr_* versions.
    
    One side effect is that anything that was KERN_DEBUG is now automatically
    a dynamic debug message.
    
    Signed-off-by: Jeff Mahoney <jeffm@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 0da812cd1a95..0c910b1cb2f8 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -4791,7 +4791,7 @@ void setup_items_for_insert(struct btrfs_root *root, struct btrfs_path *path,
 		for (i = slot; i < nritems; i++) {
 			u32 ioff;
 
-			item = btrfs_item_nr( i);
+			item = btrfs_item_nr(i);
 			ioff = btrfs_token_item_offset(leaf, item, &token);
 			btrfs_set_token_item_offset(leaf, item,
 						    ioff - total_data, &token);

commit 5d163e0e68ce743e1e919ddd3264c96ac02e9026
Author: Jeff Mahoney <jeffm@suse.com>
Date:   Tue Sep 20 10:05:00 2016 -0400

    btrfs: unsplit printed strings
    
    CodingStyle chapter 2:
    "[...] never break user-visible strings such as printk messages,
    because that breaks the ability to grep for them."
    
    This patch unsplits user-visible strings.
    
    Signed-off-by: Jeff Mahoney <jeffm@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 494e6b17bebd..0da812cd1a95 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -4768,8 +4768,9 @@ void setup_items_for_insert(struct btrfs_root *root, struct btrfs_path *path,
 
 	if (btrfs_leaf_free_space(root, leaf) < total_size) {
 		btrfs_print_leaf(root, leaf);
-		btrfs_crit(root->fs_info, "not enough freespace need %u have %d",
-		       total_size, btrfs_leaf_free_space(root, leaf));
+		btrfs_crit(root->fs_info,
+			   "not enough freespace need %u have %d",
+			   total_size, btrfs_leaf_free_space(root, leaf));
 		BUG();
 	}
 
@@ -4778,8 +4779,9 @@ void setup_items_for_insert(struct btrfs_root *root, struct btrfs_path *path,
 
 		if (old_data < data_end) {
 			btrfs_print_leaf(root, leaf);
-			btrfs_crit(root->fs_info, "slot %d old_data %d data_end %d",
-			       slot, old_data, data_end);
+			btrfs_crit(root->fs_info,
+				   "slot %d old_data %d data_end %d",
+				   slot, old_data, data_end);
 			BUG_ON(1);
 		}
 		/*

commit e2c8990734874061d144184dbf0d66e2827c216f
Author: Masahiro Yamada <yamada.masahiro@socionext.com>
Date:   Tue Sep 13 04:35:52 2016 +0900

    btrfs: squash lines for simple wrapper functions
    
    Remove unneeded variables and assignments.
    
    Signed-off-by: Masahiro Yamada <yamada.masahiro@socionext.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index dcd39cca7e43..494e6b17bebd 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -45,9 +45,7 @@ static int tree_mod_log_free_eb(struct btrfs_fs_info *fs_info,
 
 struct btrfs_path *btrfs_alloc_path(void)
 {
-	struct btrfs_path *path;
-	path = kmem_cache_zalloc(btrfs_path_cachep, GFP_NOFS);
-	return path;
+	return kmem_cache_zalloc(btrfs_path_cachep, GFP_NOFS);
 }
 
 /*

commit 2309e79650100073ca8347279690742777c6698b
Author: Luis Henriques <luis.henriques@canonical.com>
Date:   Tue Aug 23 23:23:23 2016 +0100

    btrfs: Fix warning "variable gen set but not used"
    
    Variable 'gen' in reada_for_search() is not used since commit 58dc4ce43251
    ("btrfs: remove unused parameter from readahead_tree_block").  This patch
    simply removes this variable.
    
    Signed-off-by: Luis Henriques <luis.henriques@canonical.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index d1c56c94dd5a..dcd39cca7e43 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2268,7 +2268,6 @@ static void reada_for_search(struct btrfs_root *root,
 	u64 search;
 	u64 target;
 	u64 nread = 0;
-	u64 gen;
 	struct extent_buffer *eb;
 	u32 nr;
 	u32 blocksize;
@@ -2313,7 +2312,6 @@ static void reada_for_search(struct btrfs_root *root,
 		search = btrfs_node_blockptr(node, nr);
 		if ((search <= target && target - search <= 65536) ||
 		    (search > target && search - target <= 65536)) {
-			gen = btrfs_node_ptr_generation(node, nr);
 			readahead_tree_block(root, search);
 			nread += blocksize;
 		}

commit 66642832f06a4351e23cea6cf254967c227f8224
Author: Jeff Mahoney <jeffm@suse.com>
Date:   Fri Jun 10 18:19:25 2016 -0400

    btrfs: btrfs_abort_transaction, drop root parameter
    
    __btrfs_abort_transaction doesn't use its root parameter except to
    obtain an fs_info pointer.  We can obtain that from trans->root->fs_info
    for now and from trans->fs_info in a later patch.
    
    Signed-off-by: Jeff Mahoney <jeffm@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index e926b9fdb64a..d1c56c94dd5a 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1153,14 +1153,14 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 
 	ret = update_ref_for_cow(trans, root, buf, cow, &last_ref);
 	if (ret) {
-		btrfs_abort_transaction(trans, root, ret);
+		btrfs_abort_transaction(trans, ret);
 		return ret;
 	}
 
 	if (test_bit(BTRFS_ROOT_REF_COWS, &root->state)) {
 		ret = btrfs_reloc_cow_block(trans, root, buf, cow);
 		if (ret) {
-			btrfs_abort_transaction(trans, root, ret);
+			btrfs_abort_transaction(trans, ret);
 			return ret;
 		}
 	}
@@ -1198,7 +1198,7 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 		if (last_ref) {
 			ret = tree_mod_log_free_eb(root->fs_info, buf);
 			if (ret) {
-				btrfs_abort_transaction(trans, root, ret);
+				btrfs_abort_transaction(trans, ret);
 				return ret;
 			}
 		}
@@ -3255,7 +3255,7 @@ static int push_node_left(struct btrfs_trans_handle *trans,
 	ret = tree_mod_log_eb_copy(root->fs_info, dst, src, dst_nritems, 0,
 				   push_items);
 	if (ret) {
-		btrfs_abort_transaction(trans, root, ret);
+		btrfs_abort_transaction(trans, ret);
 		return ret;
 	}
 	copy_extent_buffer(dst, src,
@@ -3330,7 +3330,7 @@ static int balance_node_right(struct btrfs_trans_handle *trans,
 	ret = tree_mod_log_eb_copy(root->fs_info, dst, src, 0,
 				   src_nritems - push_items, push_items);
 	if (ret) {
-		btrfs_abort_transaction(trans, root, ret);
+		btrfs_abort_transaction(trans, ret);
 		return ret;
 	}
 	copy_extent_buffer(dst, src,
@@ -3534,7 +3534,7 @@ static noinline int split_node(struct btrfs_trans_handle *trans,
 	ret = tree_mod_log_eb_copy(root->fs_info, split, c, 0,
 				   mid, c_nritems - mid);
 	if (ret) {
-		btrfs_abort_transaction(trans, root, ret);
+		btrfs_abort_transaction(trans, ret);
 		return ret;
 	}
 	copy_extent_buffer(split, c,

commit f5ee5c9ac56cd328fcc915582f81226affebd81c
Author: Jeff Mahoney <jeffm@suse.com>
Date:   Tue Jun 21 09:52:41 2016 -0400

    btrfs: tests, use BTRFS_FS_STATE_DUMMY_FS_INFO instead of dummy root
    
    Now that we have a dummy fs_info associated with each test that
    uses a root, we don't need the DUMMY_ROOT bit anymore.  This lets
    us make choices without needing an actual root like in e.g.
    btrfs_find_create_tree_block.
    
    Signed-off-by: Jeff Mahoney <jeffm@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 0abed1a0caa7..e926b9fdb64a 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1505,7 +1505,7 @@ static inline int should_cow_block(struct btrfs_trans_handle *trans,
 				   struct btrfs_root *root,
 				   struct extent_buffer *buf)
 {
-	if (btrfs_test_is_dummy_root(root))
+	if (btrfs_is_testing(root->fs_info))
 		return 0;
 
 	/* ensure we can see the force_cow */

commit fb770ae414d018255afa7a70b14ba1f8620762dd
Author: Liu Bo <bo.li.liu@oracle.com>
Date:   Tue Jul 5 12:10:14 2016 -0700

    Btrfs: fix read_node_slot to return errors
    
    We use read_node_slot() to read btree node and it has two cases,
    a) slot is out of range, which means 'no such entry'
    b) we fail to read the block, due to checksum fails or corrupted
       content or not with uptodate flag.
    But we're returning NULL in both cases, this makes it return -ENOENT
    in case a) and return -EIO in case b), and this fixes its callers
    as well as btrfs_search_forward() 's caller to catch the new errors.
    
    The problem is reported by Peter Becker, and I can manage to
    hit the same BUG_ON by mounting my fuzz image.
    
    Reported-by: Peter Becker <floyd.net@gmail.com>
    Signed-off-by: Liu Bo <bo.li.liu@oracle.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 362879da4f0d..0abed1a0caa7 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1866,7 +1866,6 @@ static void root_sub_used(struct btrfs_root *root, u32 size)
 
 /* given a node and slot number, this reads the blocks it points to.  The
  * extent buffer is returned with a reference taken (but unlocked).
- * NULL is returned on error.
  */
 static noinline struct extent_buffer *read_node_slot(struct btrfs_root *root,
 				   struct extent_buffer *parent, int slot)
@@ -1874,19 +1873,16 @@ static noinline struct extent_buffer *read_node_slot(struct btrfs_root *root,
 	int level = btrfs_header_level(parent);
 	struct extent_buffer *eb;
 
-	if (slot < 0)
-		return NULL;
-	if (slot >= btrfs_header_nritems(parent))
-		return NULL;
+	if (slot < 0 || slot >= btrfs_header_nritems(parent))
+		return ERR_PTR(-ENOENT);
 
 	BUG_ON(level == 0);
 
 	eb = read_tree_block(root, btrfs_node_blockptr(parent, slot),
 			     btrfs_node_ptr_generation(parent, slot));
-	if (IS_ERR(eb) || !extent_buffer_uptodate(eb)) {
-		if (!IS_ERR(eb))
-			free_extent_buffer(eb);
-		eb = NULL;
+	if (!IS_ERR(eb) && !extent_buffer_uptodate(eb)) {
+		free_extent_buffer(eb);
+		eb = ERR_PTR(-EIO);
 	}
 
 	return eb;
@@ -1939,8 +1935,8 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 
 		/* promote the child to a root */
 		child = read_node_slot(root, mid, 0);
-		if (!child) {
-			ret = -EROFS;
+		if (IS_ERR(child)) {
+			ret = PTR_ERR(child);
 			btrfs_handle_fs_error(root->fs_info, ret, NULL);
 			goto enospc;
 		}
@@ -1978,6 +1974,9 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		return 0;
 
 	left = read_node_slot(root, parent, pslot - 1);
+	if (IS_ERR(left))
+		left = NULL;
+
 	if (left) {
 		btrfs_tree_lock(left);
 		btrfs_set_lock_blocking(left);
@@ -1988,7 +1987,11 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 			goto enospc;
 		}
 	}
+
 	right = read_node_slot(root, parent, pslot + 1);
+	if (IS_ERR(right))
+		right = NULL;
+
 	if (right) {
 		btrfs_tree_lock(right);
 		btrfs_set_lock_blocking(right);
@@ -2143,6 +2146,8 @@ static noinline int push_nodes_for_insert(struct btrfs_trans_handle *trans,
 		return 1;
 
 	left = read_node_slot(root, parent, pslot - 1);
+	if (IS_ERR(left))
+		left = NULL;
 
 	/* first, try to make some room in the middle buffer */
 	if (left) {
@@ -2193,6 +2198,8 @@ static noinline int push_nodes_for_insert(struct btrfs_trans_handle *trans,
 		free_extent_buffer(left);
 	}
 	right = read_node_slot(root, parent, pslot + 1);
+	if (IS_ERR(right))
+		right = NULL;
 
 	/*
 	 * then try to empty the right most buffer into the middle
@@ -3781,7 +3788,11 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 	btrfs_assert_tree_locked(path->nodes[1]);
 
 	right = read_node_slot(root, upper, slot + 1);
-	if (right == NULL)
+	/*
+	 * slot + 1 is not valid or we fail to read the right node,
+	 * no big deal, just return.
+	 */
+	if (IS_ERR(right))
 		return 1;
 
 	btrfs_tree_lock(right);
@@ -4011,7 +4022,11 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 	btrfs_assert_tree_locked(path->nodes[1]);
 
 	left = read_node_slot(root, path->nodes[1], slot - 1);
-	if (left == NULL)
+	/*
+	 * slot - 1 is not valid or we fail to read the left node,
+	 * no big deal, just return.
+	 */
+	if (IS_ERR(left))
 		return 1;
 
 	btrfs_tree_lock(left);
@@ -5218,7 +5233,10 @@ int btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,
 		}
 		btrfs_set_path_blocking(path);
 		cur = read_node_slot(root, cur, slot);
-		BUG_ON(!cur); /* -ENOMEM */
+		if (IS_ERR(cur)) {
+			ret = PTR_ERR(cur);
+			goto out;
+		}
 
 		btrfs_tree_read_lock(cur);
 
@@ -5237,15 +5255,21 @@ int btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,
 	return ret;
 }
 
-static void tree_move_down(struct btrfs_root *root,
+static int tree_move_down(struct btrfs_root *root,
 			   struct btrfs_path *path,
 			   int *level, int root_level)
 {
+	struct extent_buffer *eb;
+
 	BUG_ON(*level == 0);
-	path->nodes[*level - 1] = read_node_slot(root, path->nodes[*level],
-					path->slots[*level]);
+	eb = read_node_slot(root, path->nodes[*level], path->slots[*level]);
+	if (IS_ERR(eb))
+		return PTR_ERR(eb);
+
+	path->nodes[*level - 1] = eb;
 	path->slots[*level - 1] = 0;
 	(*level)--;
+	return 0;
 }
 
 static int tree_move_next_or_upnext(struct btrfs_root *root,
@@ -5290,8 +5314,7 @@ static int tree_advance(struct btrfs_root *root,
 	if (*level == 0 || !allow_down) {
 		ret = tree_move_next_or_upnext(root, path, level, root_level);
 	} else {
-		tree_move_down(root, path, level, root_level);
-		ret = 0;
+		ret = tree_move_down(root, path, level, root_level);
 	}
 	if (ret >= 0) {
 		if (*level == 0)
@@ -5465,8 +5488,10 @@ int btrfs_compare_trees(struct btrfs_root *left_root,
 					left_root_level,
 					advance_left != ADVANCE_ONLY_NEXT,
 					&left_key);
-			if (ret < 0)
+			if (ret == -1)
 				left_end_reached = ADVANCE;
+			else if (ret < 0)
+				goto out;
 			advance_left = 0;
 		}
 		if (advance_right && !right_end_reached) {
@@ -5474,8 +5499,10 @@ int btrfs_compare_trees(struct btrfs_root *left_root,
 					right_root_level,
 					advance_right != ADVANCE_ONLY_NEXT,
 					&right_key);
-			if (ret < 0)
+			if (ret == -1)
 				right_end_reached = ADVANCE;
+			else if (ret < 0)
+				goto out;
 			advance_right = 0;
 		}
 

commit 5e24e9af01abcb151173bb133f1a72b94239c670
Author: Liu Bo <bo.li.liu@oracle.com>
Date:   Thu Jun 23 16:32:45 2016 -0700

    Btrfs: error out if generic_bin_search get invalid arguments
    
    With btrfs-corrupt-block, one can set btree node/leaf's field, if
    we assign a negative value to node/leaf, we can get various hangs,
    eg. if extent_root's nritems is -2ULL, then we get stuck in
     btrfs_read_block_groups() because it has a while loop and
    btrfs_search_slot() on extent_root will always return the first
     child.
    
    This lets us know what's happening and returns a EINVAL to callers
    instead of returning the first item.
    
    Signed-off-by: Liu Bo <bo.li.liu@oracle.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index a85cf7d23309..362879da4f0d 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1771,6 +1771,14 @@ static noinline int generic_bin_search(struct extent_buffer *eb,
 	unsigned long map_len = 0;
 	int err;
 
+	if (low > high) {
+		btrfs_err(eb->fs_info,
+		 "%s: low (%d) > high (%d) eb %llu owner %llu level %d",
+			  __func__, low, high, eb->start,
+			  btrfs_header_owner(eb), btrfs_header_level(eb));
+		return -EINVAL;
+	}
+
 	while (low < high) {
 		mid = (low + high) / 2;
 		offset = p + mid * item_size;

commit 415b35a55b57a701afe7391d32a6bb0193b7d3da
Author: Liu Bo <bo.li.liu@oracle.com>
Date:   Fri Jun 17 19:16:21 2016 -0700

    Btrfs: fix error handling in map_private_extent_buffer
    
    map_private_extent_buffer() can return -EINVAL in two different cases,
    1. when the requested contents span two pages if nodesize is larger
       than pagesize,
    2. when it detects something insane.
    
    The 2nd one used to be only a WARN_ON(1), and we decided to return a error
    to callers, but we didn't fix up all its callers, which will be
    addressed by this patch.
    
    Without this, btrfs may end up with 'general protection', ie.
    reading invalid memory.
    
    Reported-by: Vegard Nossum <vegard.nossum@oracle.com>
    Signed-off-by: Liu Bo <bo.li.liu@oracle.com>
    Signed-off-by: David Sterba <dsterba@suse.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 6276add8538a..a85cf7d23309 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1786,10 +1786,12 @@ static noinline int generic_bin_search(struct extent_buffer *eb,
 			if (!err) {
 				tmp = (struct btrfs_disk_key *)(kaddr + offset -
 							map_start);
-			} else {
+			} else if (err == 1) {
 				read_extent_buffer(eb, &unaligned,
 						   offset, sizeof(unaligned));
 				tmp = &unaligned;
+			} else {
+				return err;
 			}
 
 		} else {
@@ -2830,6 +2832,8 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 		}
 
 		ret = key_search(b, key, level, &prev_cmp, &slot);
+		if (ret < 0)
+			goto done;
 
 		if (level != 0) {
 			int dec = 0;

commit 64c12921e11b3a0c10d088606e328c58e29274d8
Author: Jeff Mahoney <jeffm@suse.com>
Date:   Wed Jun 8 00:36:38 2016 -0400

    btrfs: account for non-CoW'd blocks in btrfs_abort_transaction
    
    The test for !trans->blocks_used in btrfs_abort_transaction is
    insufficient to determine whether it's safe to drop the transaction
    handle on the floor.  btrfs_cow_block, informed by should_cow_block,
    can return blocks that have already been CoW'd in the current
    transaction.  trans->blocks_used is only incremented for new block
    allocations. If an operation overlaps the blocks in the current
    transaction entirely and must abort the transaction, we'll happily
    let it clean up the trans handle even though it may have modified
    the blocks and will commit an incomplete operation.
    
    In the long-term, I'd like to do closer tracking of when the fs
    is actually modified so we can still recover as gracefully as possible,
    but that approach will need some discussion.  In the short term,
    since this is the only code using trans->blocks_used, let's just
    switch it to a bool indicating whether any blocks were used and set
    it when should_cow_block returns false.
    
    Cc: stable@vger.kernel.org # 3.4+
    Signed-off-by: Jeff Mahoney <jeffm@suse.com>
    Reviewed-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 827c949fa4bc..6276add8538a 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1554,6 +1554,7 @@ noinline int btrfs_cow_block(struct btrfs_trans_handle *trans,
 		       trans->transid, root->fs_info->generation);
 
 	if (!should_cow_block(trans, root, buf)) {
+		trans->dirty = true;
 		*cow_ret = buf;
 		return 0;
 	}
@@ -2777,8 +2778,10 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 			 * then we don't want to set the path blocking,
 			 * so we test it here
 			 */
-			if (!should_cow_block(trans, root, b))
+			if (!should_cow_block(trans, root, b)) {
+				trans->dirty = true;
 				goto cow_done;
+			}
 
 			/*
 			 * must have write locks on this node and the

commit c871b0f2fd27e7f9097d507f47de5270f88003b9
Author: Liu Bo <bo.li.liu@oracle.com>
Date:   Mon Jun 6 12:01:23 2016 -0700

    Btrfs: check if extent buffer is aligned to sectorsize
    
    Thanks to fuzz testing, we can pass an invalid bytenr to extent buffer
    via alloc_extent_buffer().  An unaligned eb can have more pages than it
    should have, which ends up extent buffer's leak or some corrupted content
    in extent buffer.
    
    This adds a warning to let us quickly know what was happening.
    
    Now that alloc_extent_buffer() no more returns NULL, this changes its
    caller and callers of its caller to match with the new error
    handling.
    
    Signed-off-by: Liu Bo <bo.li.liu@oracle.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 46025688f1d0..827c949fa4bc 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2512,6 +2512,8 @@ read_block_for_search(struct btrfs_trans_handle *trans,
 		if (!btrfs_buffer_uptodate(tmp, 0, 0))
 			ret = -EIO;
 		free_extent_buffer(tmp);
+	} else {
+		ret = PTR_ERR(tmp);
 	}
 	return ret;
 }

commit b9ef22dedde08ab1b4ccd5f53344984c4dcb89f4
Author: Feifei Xu <xufeifei@linux.vnet.ibm.com>
Date:   Wed Jun 1 19:18:25 2016 +0800

    Btrfs: self-tests: Support non-4k page size
    
    self-tests code assumes 4k as the sectorsize and nodesize. This commit
    fix hardcoded 4K. Enables the self-tests code to be executed on non-4k
    page sized systems (e.g. ppc64).
    
    Reviewed-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: Feifei Xu <xufeifei@linux.vnet.ibm.com>
    Signed-off-by: Chandan Rajendra <chandan@linux.vnet.ibm.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 427c36b430a6..46025688f1d0 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1373,7 +1373,8 @@ tree_mod_log_rewind(struct btrfs_fs_info *fs_info, struct btrfs_path *path,
 
 	if (tm->op == MOD_LOG_KEY_REMOVE_WHILE_FREEING) {
 		BUG_ON(tm->slot != 0);
-		eb_rewin = alloc_dummy_extent_buffer(fs_info, eb->start);
+		eb_rewin = alloc_dummy_extent_buffer(fs_info, eb->start,
+						eb->len);
 		if (!eb_rewin) {
 			btrfs_tree_read_unlock_blocking(eb);
 			free_extent_buffer(eb);
@@ -1454,7 +1455,8 @@ get_old_root(struct btrfs_root *root, u64 time_seq)
 	} else if (old_root) {
 		btrfs_tree_read_unlock(eb_root);
 		free_extent_buffer(eb_root);
-		eb = alloc_dummy_extent_buffer(root->fs_info, logical);
+		eb = alloc_dummy_extent_buffer(root->fs_info, logical,
+					root->nodesize);
 	} else {
 		btrfs_set_lock_blocking_rw(eb_root, BTRFS_READ_LOCK);
 		eb = btrfs_clone_extent_buffer(eb_root);

commit 42f31734eb7658fd01fb186d56312be869450a42
Merge: e73440868fde 0132761017e0
Author: David Sterba <dsterba@suse.com>
Date:   Wed May 25 22:51:03 2016 +0200

    Merge branch 'cleanups-4.7' into for-chris-4.7-20160525

commit 0132761017e012ab4dc8584d679503f2ba26ca86
Author: Nicholas D Steeves <nsteeves@gmail.com>
Date:   Thu May 19 21:18:45 2016 -0400

    btrfs: fix string and comment grammatical issues and typos
    
    Signed-off-by: Nicholas D Steeves <nsteeves@gmail.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index ec7928a27aaa..4997f175b446 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -156,7 +156,7 @@ struct extent_buffer *btrfs_root_node(struct btrfs_root *root)
 
 		/*
 		 * RCU really hurts here, we could free up the root node because
-		 * it was cow'ed but we may not get the new root node yet so do
+		 * it was COWed but we may not get the new root node yet so do
 		 * the inc_not_zero dance and if it doesn't work then
 		 * synchronize_rcu and try again.
 		 */
@@ -955,7 +955,7 @@ int btrfs_block_can_be_shared(struct btrfs_root *root,
 			      struct extent_buffer *buf)
 {
 	/*
-	 * Tree blocks not in refernece counted trees and tree roots
+	 * Tree blocks not in reference counted trees and tree roots
 	 * are never shared. If a block was allocated after the last
 	 * snapshot and the block was not allocated by tree relocation,
 	 * we know the block is not shared.
@@ -1270,7 +1270,7 @@ __tree_mod_log_oldest_root(struct btrfs_fs_info *fs_info,
 
 /*
  * tm is a pointer to the first operation to rewind within eb. then, all
- * previous operations will be rewinded (until we reach something older than
+ * previous operations will be rewound (until we reach something older than
  * time_seq).
  */
 static void
@@ -1345,7 +1345,7 @@ __tree_mod_log_rewind(struct btrfs_fs_info *fs_info, struct extent_buffer *eb,
 }
 
 /*
- * Called with eb read locked. If the buffer cannot be rewinded, the same buffer
+ * Called with eb read locked. If the buffer cannot be rewound, the same buffer
  * is returned. If rewind operations happen, a fresh buffer is returned. The
  * returned buffer is always read-locked. If the returned buffer is not the
  * input buffer, the lock on the input buffer is released and the input buffer
@@ -1516,7 +1516,7 @@ static inline int should_cow_block(struct btrfs_trans_handle *trans,
 	 * 3) the root is not forced COW.
 	 *
 	 * What is forced COW:
-	 *    when we create snapshot during commiting the transaction,
+	 *    when we create snapshot during committing the transaction,
 	 *    after we've finished coping src root, we must COW the shared
 	 *    block to ensure the metadata consistency.
 	 */
@@ -1531,7 +1531,7 @@ static inline int should_cow_block(struct btrfs_trans_handle *trans,
 
 /*
  * cows a single block, see __btrfs_cow_block for the real work.
- * This version of it has extra checks so that a block isn't cow'd more than
+ * This version of it has extra checks so that a block isn't COWed more than
  * once per transaction, as long as it hasn't been written yet
  */
 noinline int btrfs_cow_block(struct btrfs_trans_handle *trans,
@@ -2986,7 +2986,7 @@ int btrfs_search_old_slot(struct btrfs_root *root, struct btrfs_key *key,
 		btrfs_unlock_up_safe(p, level + 1);
 
 		/*
-		 * Since we can unwind eb's we want to do a real search every
+		 * Since we can unwind ebs we want to do a real search every
 		 * time.
 		 */
 		prev_cmp = -1;

commit 34d9700702f4042ce10d68a092ab7f79575e7a3b
Author: Anand Jain <anand.jain@oracle.com>
Date:   Wed Mar 16 16:43:06 2016 +0800

    btrfs: rename btrfs_std_error to btrfs_handle_fs_error
    
    btrfs_std_error() handles errors, puts FS into readonly mode
    (as of now). So its good idea to rename it to btrfs_handle_fs_error().
    
    Signed-off-by: Anand Jain <anand.jain@oracle.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    [ edit changelog ]
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index ec7928a27aaa..decd0a3f5d61 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1011,7 +1011,7 @@ static noinline int update_ref_for_cow(struct btrfs_trans_handle *trans,
 			return ret;
 		if (refs == 0) {
 			ret = -EROFS;
-			btrfs_std_error(root->fs_info, ret, NULL);
+			btrfs_handle_fs_error(root->fs_info, ret, NULL);
 			return ret;
 		}
 	} else {
@@ -1928,7 +1928,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		child = read_node_slot(root, mid, 0);
 		if (!child) {
 			ret = -EROFS;
-			btrfs_std_error(root->fs_info, ret, NULL);
+			btrfs_handle_fs_error(root->fs_info, ret, NULL);
 			goto enospc;
 		}
 
@@ -2031,7 +2031,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		 */
 		if (!left) {
 			ret = -EROFS;
-			btrfs_std_error(root->fs_info, ret, NULL);
+			btrfs_handle_fs_error(root->fs_info, ret, NULL);
 			goto enospc;
 		}
 		wret = balance_node_right(trans, root, mid, left);

commit 8f282f71eaee7ac979cdbe525f76daa0722798a8
Author: David Sterba <dsterba@suse.com>
Date:   Wed Mar 30 16:01:12 2016 +0200

    btrfs: fallback to vmalloc in btrfs_compare_tree
    
    The allocation of node could fail if the memory is too fragmented for a
    given node size, practically observed with 64k.
    
    http://article.gmane.org/gmane.comp.file-systems.btrfs/54689
    
    Reported-and-tested-by: Jean-Denis Girard <jd.girard@sysnux.pf>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 77592931ab4f..ec7928a27aaa 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -19,6 +19,7 @@
 #include <linux/sched.h>
 #include <linux/slab.h>
 #include <linux/rbtree.h>
+#include <linux/vmalloc.h>
 #include "ctree.h"
 #include "disk-io.h"
 #include "transaction.h"
@@ -5361,10 +5362,13 @@ int btrfs_compare_trees(struct btrfs_root *left_root,
 		goto out;
 	}
 
-	tmp_buf = kmalloc(left_root->nodesize, GFP_KERNEL);
+	tmp_buf = kmalloc(left_root->nodesize, GFP_KERNEL | __GFP_NOWARN);
 	if (!tmp_buf) {
-		ret = -ENOMEM;
-		goto out;
+		tmp_buf = vmalloc(left_root->nodesize);
+		if (!tmp_buf) {
+			ret = -ENOMEM;
+			goto out;
+		}
 	}
 
 	left_path->search_commit_root = 1;
@@ -5565,7 +5569,7 @@ int btrfs_compare_trees(struct btrfs_root *left_root,
 out:
 	btrfs_free_path(left_path);
 	btrfs_free_path(right_path);
-	kfree(tmp_buf);
+	kvfree(tmp_buf);
 	return ret;
 }
 

commit e22b3d1fbe596c7feba6782dab2e11c7b99f1d90
Merge: 5f1b5664d978 66722f7c0590
Author: David Sterba <dsterba@suse.com>
Date:   Fri Feb 26 15:38:28 2016 +0100

    Merge branch 'dev/gfp-flags' into for-chris-4.6

commit e780b0d1c1523ec8cd489c6910fb8c5ee452bb6c
Author: David Sterba <dsterba@suse.com>
Date:   Mon Jan 18 18:42:13 2016 +0100

    btrfs: send: use GFP_KERNEL everywhere
    
    The send operation is not on the critical writeback path we don't need
    to use GFP_NOFS for allocations. All error paths are handled and the
    whole operation is restartable.
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 769e0ff1b4ce..00741eb94651 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -5361,7 +5361,7 @@ int btrfs_compare_trees(struct btrfs_root *left_root,
 		goto out;
 	}
 
-	tmp_buf = kmalloc(left_root->nodesize, GFP_NOFS);
+	tmp_buf = kmalloc(left_root->nodesize, GFP_KERNEL);
 	if (!tmp_buf) {
 		ret = -ENOMEM;
 		goto out;

commit 298cfd368364912076fd84fbceb0df864da42a67
Author: Chandan Rajendra <chandan@linux.vnet.ibm.com>
Date:   Thu Jan 21 15:55:59 2016 +0530

    Btrfs: Use (eb->start, seq) as search key for tree modification log
    
    In subpagesize-blocksize a page can map multiple extent buffers and hence
    using (page index, seq) as the search key is incorrect. For example, searching
    through tree modification log tree can return an entry associated with the
    first extent buffer mapped by the page (if such an entry exists), when we are
    actually searching for entries associated with extent buffers that are mapped
    at position 2 or more in the page.
    
    Reviewed-by: Liu Bo <bo.li.liu@oracle.com>
    Signed-off-by: Chandan Rajendra <chandan@linux.vnet.ibm.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 769e0ff1b4ce..f5ef7d171bb8 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -311,7 +311,7 @@ struct tree_mod_root {
 
 struct tree_mod_elem {
 	struct rb_node node;
-	u64 index;		/* shifted logical */
+	u64 logical;
 	u64 seq;
 	enum mod_log_op op;
 
@@ -435,11 +435,11 @@ void btrfs_put_tree_mod_seq(struct btrfs_fs_info *fs_info,
 
 /*
  * key order of the log:
- *       index -> sequence
+ *       node/leaf start address -> sequence
  *
- * the index is the shifted logical of the *new* root node for root replace
- * operations, or the shifted logical of the affected block for all other
- * operations.
+ * The 'start address' is the logical address of the *new* root node
+ * for root replace operations, or the logical address of the affected
+ * block for all other operations.
  *
  * Note: must be called with write lock (tree_mod_log_write_lock).
  */
@@ -460,9 +460,9 @@ __tree_mod_log_insert(struct btrfs_fs_info *fs_info, struct tree_mod_elem *tm)
 	while (*new) {
 		cur = container_of(*new, struct tree_mod_elem, node);
 		parent = *new;
-		if (cur->index < tm->index)
+		if (cur->logical < tm->logical)
 			new = &((*new)->rb_left);
-		else if (cur->index > tm->index)
+		else if (cur->logical > tm->logical)
 			new = &((*new)->rb_right);
 		else if (cur->seq < tm->seq)
 			new = &((*new)->rb_left);
@@ -523,7 +523,7 @@ alloc_tree_mod_elem(struct extent_buffer *eb, int slot,
 	if (!tm)
 		return NULL;
 
-	tm->index = eb->start >> PAGE_CACHE_SHIFT;
+	tm->logical = eb->start;
 	if (op != MOD_LOG_KEY_ADD) {
 		btrfs_node_key(eb, &tm->key, slot);
 		tm->blockptr = btrfs_node_blockptr(eb, slot);
@@ -588,7 +588,7 @@ tree_mod_log_insert_move(struct btrfs_fs_info *fs_info,
 		goto free_tms;
 	}
 
-	tm->index = eb->start >> PAGE_CACHE_SHIFT;
+	tm->logical = eb->start;
 	tm->slot = src_slot;
 	tm->move.dst_slot = dst_slot;
 	tm->move.nr_items = nr_items;
@@ -699,7 +699,7 @@ tree_mod_log_insert_root(struct btrfs_fs_info *fs_info,
 		goto free_tms;
 	}
 
-	tm->index = new_root->start >> PAGE_CACHE_SHIFT;
+	tm->logical = new_root->start;
 	tm->old_root.logical = old_root->start;
 	tm->old_root.level = btrfs_header_level(old_root);
 	tm->generation = btrfs_header_generation(old_root);
@@ -739,16 +739,15 @@ __tree_mod_log_search(struct btrfs_fs_info *fs_info, u64 start, u64 min_seq,
 	struct rb_node *node;
 	struct tree_mod_elem *cur = NULL;
 	struct tree_mod_elem *found = NULL;
-	u64 index = start >> PAGE_CACHE_SHIFT;
 
 	tree_mod_log_read_lock(fs_info);
 	tm_root = &fs_info->tree_mod_log;
 	node = tm_root->rb_node;
 	while (node) {
 		cur = container_of(node, struct tree_mod_elem, node);
-		if (cur->index < index) {
+		if (cur->logical < start) {
 			node = node->rb_left;
-		} else if (cur->index > index) {
+		} else if (cur->logical > start) {
 			node = node->rb_right;
 		} else if (cur->seq < min_seq) {
 			node = node->rb_left;
@@ -1230,9 +1229,10 @@ __tree_mod_log_oldest_root(struct btrfs_fs_info *fs_info,
 		return NULL;
 
 	/*
-	 * the very last operation that's logged for a root is the replacement
-	 * operation (if it is replaced at all). this has the index of the *new*
-	 * root, making it the very first operation that's logged for this root.
+	 * the very last operation that's logged for a root is the
+	 * replacement operation (if it is replaced at all). this has
+	 * the logical address of the *new* root, making it the very
+	 * first operation that's logged for this root.
 	 */
 	while (1) {
 		tm = tree_mod_log_search_oldest(fs_info, root_logical,
@@ -1336,7 +1336,7 @@ __tree_mod_log_rewind(struct btrfs_fs_info *fs_info, struct extent_buffer *eb,
 		if (!next)
 			break;
 		tm = container_of(next, struct tree_mod_elem, node);
-		if (tm->index != first_tm->index)
+		if (tm->logical != first_tm->logical)
 			break;
 	}
 	tree_mod_log_read_unlock(fs_info);

commit b28cf57246d5b797ba725bb033110c247f2c301f
Merge: a3058101c17d a7ca42256d9f
Author: Chris Mason <clm@fb.com>
Date:   Mon Jan 11 06:08:37 2016 -0800

    Merge branch 'misc-cleanups-4.5' of git://git.kernel.org/pub/scm/linux/kernel/git/kdave/linux into for-linus-4.5
    
    Signed-off-by: Chris Mason <clm@fb.com>

commit e4058b54d1e442b6b3eca949f0d63d49ba2b020d
Author: David Sterba <dsterba@suse.com>
Date:   Fri Nov 27 16:31:35 2015 +0100

    btrfs: cleanup, use enum values for btrfs_path reada
    
    Replace the integers by enums for better readability. The value 2 does
    not have any meaning since a717531942f488209dded30f6bc648167bcefa72
    "Btrfs: do less aggressive btree readahead" (2009-01-22).
    
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 5b8e235c4b6d..be1be0422ff4 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2248,7 +2248,6 @@ static void reada_for_search(struct btrfs_root *root,
 	u64 target;
 	u64 nread = 0;
 	u64 gen;
-	int direction = path->reada;
 	struct extent_buffer *eb;
 	u32 nr;
 	u32 blocksize;
@@ -2276,16 +2275,16 @@ static void reada_for_search(struct btrfs_root *root,
 	nr = slot;
 
 	while (1) {
-		if (direction < 0) {
+		if (path->reada == READA_BACK) {
 			if (nr == 0)
 				break;
 			nr--;
-		} else if (direction > 0) {
+		} else if (path->reada == READA_FORWARD) {
 			nr++;
 			if (nr >= nritems)
 				break;
 		}
-		if (path->reada < 0 && objectid) {
+		if (path->reada == READA_BACK && objectid) {
 			btrfs_node_key(node, &disk_key, nr);
 			if (btrfs_disk_key_objectid(&disk_key) != objectid)
 				break;
@@ -2493,7 +2492,7 @@ read_block_for_search(struct btrfs_trans_handle *trans,
 	btrfs_set_path_blocking(p);
 
 	free_extent_buffer(tmp);
-	if (p->reada)
+	if (p->reada != READA_NONE)
 		reada_for_search(root, p, level, slot, key->objectid);
 
 	btrfs_release_path(p);

commit ee22184b53c823f6956314c2815d4068e3820737
Author: Byongho Lee <bhlee.kernel@gmail.com>
Date:   Tue Dec 15 01:42:10 2015 +0900

    Btrfs: use linux/sizes.h to represent constants
    
    We use many constants to represent size and offset value.  And to make
    code readable we use '256 * 1024 * 1024' instead of '268435456' to
    represent '256MB'.  However we can make far more readable with 'SZ_256MB'
    which is defined in the 'linux/sizes.h'.
    
    So this patch replaces 'xxx * 1024 * 1024' kind of expression with
    single 'SZ_xxxMB' if 'xxx' is a power of 2 then 'xxx * SZ_1M' if 'xxx' is
    not a power of 2. And I haven't touched to '4096' & '8192' because it's
    more intuitive than 'SZ_4KB' & 'SZ_8KB'.
    
    Signed-off-by: Byongho Lee <bhlee.kernel@gmail.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 5b8e235c4b6d..cb7720f91a4a 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1555,7 +1555,7 @@ noinline int btrfs_cow_block(struct btrfs_trans_handle *trans,
 		return 0;
 	}
 
-	search_start = buf->start & ~((u64)(1024 * 1024 * 1024) - 1);
+	search_start = buf->start & ~((u64)SZ_1G - 1);
 
 	if (parent)
 		btrfs_set_lock_blocking(parent);

commit a0d58e48db58801a0e764e9b9c87e1782d390fcb
Merge: 6db4a7335dd7 ddd664f4478a
Author: Chris Mason <clm@fb.com>
Date:   Wed Oct 21 18:21:40 2015 -0700

    Merge branch 'cleanups/for-4.4' of git://git.kernel.org/pub/scm/linux/kernel/git/kdave/linux into for-linus-4.4

commit ce0eac2a1d82eb841eae357bcef7ac39df131b21
Author: Alexandru Moise <00moses.alexander00@gmail.com>
Date:   Sun Aug 23 16:01:42 2015 +0000

    btrfs: Fixed dsize and last_off declarations
    
    The return values of btrfs_item_offset_nr and btrfs_item_size_nr are of
    type u32. To avoid mixing signed and unsigned integers we should also
    declare dsize and last_off to be of type u32.
    
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: Alexandru Moise <00moses.alexander00@gmail.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 5f745eadf77d..f55391301256 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -4940,8 +4940,8 @@ int btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 {
 	struct extent_buffer *leaf;
 	struct btrfs_item *item;
-	int last_off;
-	int dsize = 0;
+	u32 last_off;
+	u32 dsize = 0;
 	int ret = 0;
 	int wret;
 	int i;

commit a4553fefb59cb0336f543fa567170b47e90142a9
Author: Anand Jain <anand.jain@oracle.com>
Date:   Fri Sep 25 14:43:01 2015 +0800

    Btrfs: consolidate btrfs_error() to btrfs_std_error()
    
    btrfs_error() and btrfs_std_error() does the same thing
    and calls _btrfs_std_error(), so consolidate them together.
    And the main motivation is that btrfs_error() is closely
    named with btrfs_err(), one handles error action the other
    is to log the error, so don't closely name them.
    
    Signed-off-by: Anand Jain <anand.jain@oracle.com>
    Suggested-by: David Sterba <dsterba@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 5f745eadf77d..1063315fb387 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1011,7 +1011,7 @@ static noinline int update_ref_for_cow(struct btrfs_trans_handle *trans,
 			return ret;
 		if (refs == 0) {
 			ret = -EROFS;
-			btrfs_std_error(root->fs_info, ret);
+			btrfs_std_error(root->fs_info, ret, NULL);
 			return ret;
 		}
 	} else {
@@ -1927,7 +1927,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		child = read_node_slot(root, mid, 0);
 		if (!child) {
 			ret = -EROFS;
-			btrfs_std_error(root->fs_info, ret);
+			btrfs_std_error(root->fs_info, ret, NULL);
 			goto enospc;
 		}
 
@@ -2030,7 +2030,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		 */
 		if (!left) {
 			ret = -EROFS;
-			btrfs_std_error(root->fs_info, ret);
+			btrfs_std_error(root->fs_info, ret, NULL);
 			goto enospc;
 		}
 		wret = balance_node_right(trans, root, mid, left);

commit 93314e3b64fd2e77237fdba7cfcc0d38dcd05579
Author: Zhaolei <zhaolei@cn.fujitsu.com>
Date:   Thu Aug 6 21:56:58 2015 +0800

    btrfs: abort transaction on btrfs_reloc_cow_block()
    
    When btrfs_reloc_cow_block() failed in __btrfs_cow_block(), current
    code just return a err-value to caller, but leave new_created extent
    buffer exist and locked.
    
    Then subsequent code (in relocate) try to lock above eb again,
    and caused deadlock without any dmesg.
    (eb lock use wait_event(), so no lockdep message)
    
    It is hard to do recover work in __btrfs_cow_block() at this error
    point, but we can abort transaction to avoid deadlock and operate on
    unstable state.a
    
    It also helps developer to find wrong place quickly.
    (better than a frozen fs without any dmesg before patch)
    
    Signed-off-by: Zhao Lei <zhaolei@cn.fujitsu.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 54114b4887dd..5f745eadf77d 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1159,8 +1159,10 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 
 	if (test_bit(BTRFS_ROOT_REF_COWS, &root->state)) {
 		ret = btrfs_reloc_cow_block(trans, root, buf, cow);
-		if (ret)
+		if (ret) {
+			btrfs_abort_transaction(trans, root, ret);
 			return ret;
+		}
 	}
 
 	if (buf == root->node) {

commit 64c043de466d5746e7ca306dab9d418cd871cefc
Author: Liu Bo <bo.li.liu@oracle.com>
Date:   Mon May 25 17:30:15 2015 +0800

    Btrfs: fix up read_tree_block to return proper error
    
    The return value of read_tree_block() can confuse callers as it always
    returns NULL for either -ENOMEM or -EIO, so it's likely that callers
    parse it to a wrong error, for instance, in btrfs_read_tree_root().
    
    This fixes the above issue.
    
    Signed-off-by: Liu Bo <bo.li.liu@oracle.com>
    Reviewed-by: David Sterba <dsterba@suse.cz>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 0f11ebc92f02..54114b4887dd 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1439,8 +1439,9 @@ get_old_root(struct btrfs_root *root, u64 time_seq)
 		btrfs_tree_read_unlock(eb_root);
 		free_extent_buffer(eb_root);
 		old = read_tree_block(root, logical, 0);
-		if (WARN_ON(!old || !extent_buffer_uptodate(old))) {
-			free_extent_buffer(old);
+		if (WARN_ON(IS_ERR(old) || !extent_buffer_uptodate(old))) {
+			if (!IS_ERR(old))
+				free_extent_buffer(old);
 			btrfs_warn(root->fs_info,
 				"failed to read tree block %llu from get_old_root", logical);
 		} else {
@@ -1685,7 +1686,9 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 		if (!cur || !uptodate) {
 			if (!cur) {
 				cur = read_tree_block(root, blocknr, gen);
-				if (!cur || !extent_buffer_uptodate(cur)) {
+				if (IS_ERR(cur)) {
+					return PTR_ERR(cur);
+				} else if (!extent_buffer_uptodate(cur)) {
 					free_extent_buffer(cur);
 					return -EIO;
 				}
@@ -1864,8 +1867,9 @@ static noinline struct extent_buffer *read_node_slot(struct btrfs_root *root,
 
 	eb = read_tree_block(root, btrfs_node_blockptr(parent, slot),
 			     btrfs_node_ptr_generation(parent, slot));
-	if (eb && !extent_buffer_uptodate(eb)) {
-		free_extent_buffer(eb);
+	if (IS_ERR(eb) || !extent_buffer_uptodate(eb)) {
+		if (!IS_ERR(eb))
+			free_extent_buffer(eb);
 		eb = NULL;
 	}
 
@@ -2494,7 +2498,7 @@ read_block_for_search(struct btrfs_trans_handle *trans,
 
 	ret = -EAGAIN;
 	tmp = read_tree_block(root, blocknr, 0);
-	if (tmp) {
+	if (!IS_ERR(tmp)) {
 		/*
 		 * If the read above didn't mark this buffer up to date,
 		 * it will never end up being up to date.  Set ret to EIO now

commit fc4c3c872f44bf425963feba57eb9c3f8ac2d7eb
Merge: 9deed229fa8a a4f3d2c4efe2
Author: Chris Mason <clm@fb.com>
Date:   Wed Mar 25 10:52:48 2015 -0700

    Merge branch 'cleanups-post-3.19' of git://git.kernel.org/pub/scm/linux/kernel/git/kdave/linux into for-linus-4.1
    
    Signed-off-by: Chris Mason <clm@fb.com>
    
    Conflicts:
            fs/btrfs/disk-io.c

commit 9deed229fa8a83bb5cd713b2d2a8e5c022a4b45b
Merge: bc465aa9d045 258ece02126a
Author: Chris Mason <clm@fb.com>
Date:   Wed Mar 25 10:43:16 2015 -0700

    Merge branch 'cleanups-for-4.1-v2' of git://git.kernel.org/pub/scm/linux/kernel/git/kdave/linux into for-linus-4.1

commit 31e818fe7375d60de9953051f7bd1615cebc3681
Author: David Sterba <dsterba@suse.cz>
Date:   Fri Feb 20 18:00:26 2015 +0100

    btrfs: cleanup, use kmalloc_array/kcalloc array helpers
    
    Convert kmalloc(nr * size, ..) to kmalloc_array that does additional
    overflow checks, the zeroing variant is kcalloc.
    
    Signed-off-by: David Sterba <dsterba@suse.cz>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 993642199326..8595fdd50a22 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -578,7 +578,7 @@ tree_mod_log_insert_move(struct btrfs_fs_info *fs_info,
 	if (!tree_mod_need_log(fs_info, eb))
 		return 0;
 
-	tm_list = kzalloc(nr_items * sizeof(struct tree_mod_elem *), flags);
+	tm_list = kcalloc(nr_items, sizeof(struct tree_mod_elem *), flags);
 	if (!tm_list)
 		return -ENOMEM;
 
@@ -677,7 +677,7 @@ tree_mod_log_insert_root(struct btrfs_fs_info *fs_info,
 
 	if (log_removal && btrfs_header_level(old_root) > 0) {
 		nritems = btrfs_header_nritems(old_root);
-		tm_list = kzalloc(nritems * sizeof(struct tree_mod_elem *),
+		tm_list = kcalloc(nritems, sizeof(struct tree_mod_elem *),
 				  flags);
 		if (!tm_list) {
 			ret = -ENOMEM;
@@ -814,7 +814,7 @@ tree_mod_log_eb_copy(struct btrfs_fs_info *fs_info, struct extent_buffer *dst,
 	if (btrfs_header_level(dst) == 0 && btrfs_header_level(src) == 0)
 		return 0;
 
-	tm_list = kzalloc(nr_items * 2 * sizeof(struct tree_mod_elem *),
+	tm_list = kcalloc(nr_items * 2, sizeof(struct tree_mod_elem *),
 			  GFP_NOFS);
 	if (!tm_list)
 		return -ENOMEM;
@@ -905,8 +905,7 @@ tree_mod_log_free_eb(struct btrfs_fs_info *fs_info, struct extent_buffer *eb)
 		return 0;
 
 	nritems = btrfs_header_nritems(eb);
-	tm_list = kzalloc(nritems * sizeof(struct tree_mod_elem *),
-			  GFP_NOFS);
+	tm_list = kcalloc(nritems, sizeof(struct tree_mod_elem *), GFP_NOFS);
 	if (!tm_list)
 		return -ENOMEM;
 

commit 5dfe2be7ead15863fd7b3fcc8bd69e470fae2bec
Author: Filipe Manana <fdmanana@suse.com>
Date:   Mon Feb 23 19:48:52 2015 +0000

    Btrfs: fix off-by-one logic error in btrfs_realloc_node
    
    The end_slot variable actually matches the number of pointers in the
    node and not the last slot (which is 'nritems - 1'). Therefore in order
    to check that the current slot in the for loop doesn't match the last
    one, the correct logic is to check if 'i' is less than 'end_slot - 1'
    and not 'end_slot - 2'.
    
    Fix this and set end_slot to be 'nritems - 1', as it's less confusing
    since the variable name implies it's inclusive rather then exclusive.
    
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 993642199326..6d67f32e648d 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1645,14 +1645,14 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 
 	parent_nritems = btrfs_header_nritems(parent);
 	blocksize = root->nodesize;
-	end_slot = parent_nritems;
+	end_slot = parent_nritems - 1;
 
-	if (parent_nritems == 1)
+	if (parent_nritems <= 1)
 		return 0;
 
 	btrfs_set_lock_blocking(parent);
 
-	for (i = start_slot; i < end_slot; i++) {
+	for (i = start_slot; i <= end_slot; i++) {
 		int close = 1;
 
 		btrfs_node_key(parent, &disk_key, i);
@@ -1669,7 +1669,7 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 			other = btrfs_node_blockptr(parent, i - 1);
 			close = close_blocks(blocknr, other, blocksize);
 		}
-		if (!close && i < end_slot - 2) {
+		if (!close && i < end_slot) {
 			other = btrfs_node_blockptr(parent, i + 1);
 			close = close_blocks(blocknr, other, blocksize);
 		}

commit 01d58472a887cba61da7b4e6dc251b5170b57e2e
Author: Daniel Dressler <danieru.dressler@gmail.com>
Date:   Fri Nov 21 17:15:07 2014 +0900

    Btrfs: disk-io: replace root args iff only fs_info used
    
    This is the 3rd independent patch of a larger project to cleanup btrfs's
    internal usage of btrfs_root. Many functions take btrfs_root only to
    grab the fs_info struct.
    
    By requiring a root these functions cause programmer overhead. That
    these functions can accept any valid root is not obvious until
    inspection.
    
    This patch reduces the specificity of such functions to accept the
    fs_info directly.
    
    These patches can be applied independently and thus are not being
    submitted as a patch series. There should be about 26 patches by the
    project's completion. Each patch will cleanup between 1 and 34 functions
    apiece.  Each patch covers a single file's functions.
    
    This patch affects the following function(s):
      1) csum_tree_block
      2) csum_dirty_buffer
      3) check_tree_block_fsid
      4) btrfs_find_tree_block
      5) clean_tree_block
    
    Signed-off-by: Daniel Dressler <danieru.dressler@gmail.com>
    Signed-off-by: David Sterba <dsterba@suse.cz>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 8c03c9222154..e1a0981159ab 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1073,7 +1073,7 @@ static noinline int update_ref_for_cow(struct btrfs_trans_handle *trans,
 			ret = btrfs_dec_ref(trans, root, buf, 1);
 			BUG_ON(ret); /* -ENOMEM */
 		}
-		clean_tree_block(trans, root, buf);
+		clean_tree_block(trans, root->fs_info, buf);
 		*last_ref = 1;
 	}
 	return 0;
@@ -1678,7 +1678,7 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 			continue;
 		}
 
-		cur = btrfs_find_tree_block(root, blocknr);
+		cur = btrfs_find_tree_block(root->fs_info, blocknr);
 		if (cur)
 			uptodate = btrfs_buffer_uptodate(cur, gen, 0);
 		else
@@ -1943,7 +1943,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 
 		path->locks[level] = 0;
 		path->nodes[level] = NULL;
-		clean_tree_block(trans, root, mid);
+		clean_tree_block(trans, root->fs_info, mid);
 		btrfs_tree_unlock(mid);
 		/* once for the path */
 		free_extent_buffer(mid);
@@ -1997,7 +1997,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		if (wret < 0 && wret != -ENOSPC)
 			ret = wret;
 		if (btrfs_header_nritems(right) == 0) {
-			clean_tree_block(trans, root, right);
+			clean_tree_block(trans, root->fs_info, right);
 			btrfs_tree_unlock(right);
 			del_ptr(root, path, level + 1, pslot + 1);
 			root_sub_used(root, right->len);
@@ -2041,7 +2041,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		BUG_ON(wret == 1);
 	}
 	if (btrfs_header_nritems(mid) == 0) {
-		clean_tree_block(trans, root, mid);
+		clean_tree_block(trans, root->fs_info, mid);
 		btrfs_tree_unlock(mid);
 		del_ptr(root, path, level + 1, pslot);
 		root_sub_used(root, mid->len);
@@ -2259,7 +2259,7 @@ static void reada_for_search(struct btrfs_root *root,
 
 	search = btrfs_node_blockptr(node, slot);
 	blocksize = root->nodesize;
-	eb = btrfs_find_tree_block(root, search);
+	eb = btrfs_find_tree_block(root->fs_info, search);
 	if (eb) {
 		free_extent_buffer(eb);
 		return;
@@ -2319,7 +2319,7 @@ static noinline void reada_for_balance(struct btrfs_root *root,
 	if (slot > 0) {
 		block1 = btrfs_node_blockptr(parent, slot - 1);
 		gen = btrfs_node_ptr_generation(parent, slot - 1);
-		eb = btrfs_find_tree_block(root, block1);
+		eb = btrfs_find_tree_block(root->fs_info, block1);
 		/*
 		 * if we get -eagain from btrfs_buffer_uptodate, we
 		 * don't want to return eagain here.  That will loop
@@ -2332,7 +2332,7 @@ static noinline void reada_for_balance(struct btrfs_root *root,
 	if (slot + 1 < nritems) {
 		block2 = btrfs_node_blockptr(parent, slot + 1);
 		gen = btrfs_node_ptr_generation(parent, slot + 1);
-		eb = btrfs_find_tree_block(root, block2);
+		eb = btrfs_find_tree_block(root->fs_info, block2);
 		if (eb && btrfs_buffer_uptodate(eb, gen, 1) != 0)
 			block2 = 0;
 		free_extent_buffer(eb);
@@ -2450,7 +2450,7 @@ read_block_for_search(struct btrfs_trans_handle *trans,
 	blocknr = btrfs_node_blockptr(b, slot);
 	gen = btrfs_node_ptr_generation(b, slot);
 
-	tmp = btrfs_find_tree_block(root, blocknr);
+	tmp = btrfs_find_tree_block(root->fs_info, blocknr);
 	if (tmp) {
 		/* first we do an atomic uptodate check */
 		if (btrfs_buffer_uptodate(tmp, gen, 1) > 0) {
@@ -3694,7 +3694,7 @@ static noinline int __push_leaf_right(struct btrfs_trans_handle *trans,
 	if (left_nritems)
 		btrfs_mark_buffer_dirty(left);
 	else
-		clean_tree_block(trans, root, left);
+		clean_tree_block(trans, root->fs_info, left);
 
 	btrfs_mark_buffer_dirty(right);
 
@@ -3706,7 +3706,7 @@ static noinline int __push_leaf_right(struct btrfs_trans_handle *trans,
 	if (path->slots[0] >= left_nritems) {
 		path->slots[0] -= left_nritems;
 		if (btrfs_header_nritems(path->nodes[0]) == 0)
-			clean_tree_block(trans, root, path->nodes[0]);
+			clean_tree_block(trans, root->fs_info, path->nodes[0]);
 		btrfs_tree_unlock(path->nodes[0]);
 		free_extent_buffer(path->nodes[0]);
 		path->nodes[0] = right;
@@ -3930,7 +3930,7 @@ static noinline int __push_leaf_left(struct btrfs_trans_handle *trans,
 	if (right_nritems)
 		btrfs_mark_buffer_dirty(right);
 	else
-		clean_tree_block(trans, root, right);
+		clean_tree_block(trans, root->fs_info, right);
 
 	btrfs_item_key(right, &disk_key, 0);
 	fixup_low_keys(root->fs_info, path, &disk_key, 1);
@@ -4984,7 +4984,7 @@ int btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 			btrfs_set_header_level(leaf, 0);
 		} else {
 			btrfs_set_path_blocking(path);
-			clean_tree_block(trans, root, leaf);
+			clean_tree_block(trans, root->fs_info, leaf);
 			btrfs_del_leaf(trans, root, path, leaf);
 		}
 	} else {

commit b7a0365ec7a0fb1d39113846fd34038af68ebd01
Author: Daniel Dressler <danieru.dressler@gmail.com>
Date:   Wed Nov 12 13:43:09 2014 +0900

    Btrfs: ctree: reduce args where only fs_info used
    
    This patch is part of a larger project to cleanup btrfs's internal usage
    of struct btrfs_root. Many functions take btrfs_root only to grab a
    pointer to fs_info.
    
    This causes programmers to ponder which root can be passed. Since only
    the fs_info is read affected functions can accept any root, except this
    is only obvious upon inspection.
    
    This patch reduces the specificty of such functions to accept the
    fs_info directly.
    
    This patch does not address the two functions in ctree.c (insert_ptr,
    and split_item) which only use root for BUG_ONs in ctree.c
    
    This patch affects the following functions:
      1) fixup_low_keys
      2) btrfs_set_item_key_safe
    
    Signed-off-by: Daniel Dressler <danieru.dressler@gmail.com>
    Signed-off-by: David Sterba <dsterba@suse.cz>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 993642199326..8c03c9222154 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -3126,7 +3126,8 @@ int btrfs_search_slot_for_read(struct btrfs_root *root,
  * higher levels
  *
  */
-static void fixup_low_keys(struct btrfs_root *root, struct btrfs_path *path,
+static void fixup_low_keys(struct btrfs_fs_info *fs_info,
+			   struct btrfs_path *path,
 			   struct btrfs_disk_key *key, int level)
 {
 	int i;
@@ -3137,7 +3138,7 @@ static void fixup_low_keys(struct btrfs_root *root, struct btrfs_path *path,
 		if (!path->nodes[i])
 			break;
 		t = path->nodes[i];
-		tree_mod_log_set_node_key(root->fs_info, t, tslot, 1);
+		tree_mod_log_set_node_key(fs_info, t, tslot, 1);
 		btrfs_set_node_key(t, key, tslot);
 		btrfs_mark_buffer_dirty(path->nodes[i]);
 		if (tslot != 0)
@@ -3151,7 +3152,8 @@ static void fixup_low_keys(struct btrfs_root *root, struct btrfs_path *path,
  * This function isn't completely safe. It's the caller's responsibility
  * that the new key won't break the order
  */
-void btrfs_set_item_key_safe(struct btrfs_root *root, struct btrfs_path *path,
+void btrfs_set_item_key_safe(struct btrfs_fs_info *fs_info,
+			     struct btrfs_path *path,
 			     struct btrfs_key *new_key)
 {
 	struct btrfs_disk_key disk_key;
@@ -3173,7 +3175,7 @@ void btrfs_set_item_key_safe(struct btrfs_root *root, struct btrfs_path *path,
 	btrfs_set_item_key(eb, &disk_key, slot);
 	btrfs_mark_buffer_dirty(eb);
 	if (slot == 0)
-		fixup_low_keys(root, path, &disk_key, 1);
+		fixup_low_keys(fs_info, path, &disk_key, 1);
 }
 
 /*
@@ -3931,7 +3933,7 @@ static noinline int __push_leaf_left(struct btrfs_trans_handle *trans,
 		clean_tree_block(trans, root, right);
 
 	btrfs_item_key(right, &disk_key, 0);
-	fixup_low_keys(root, path, &disk_key, 1);
+	fixup_low_keys(root->fs_info, path, &disk_key, 1);
 
 	/* then fixup the leaf pointer in the path */
 	if (path->slots[0] < push_items) {
@@ -4168,6 +4170,7 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 	int mid;
 	int slot;
 	struct extent_buffer *right;
+	struct btrfs_fs_info *fs_info = root->fs_info;
 	int ret = 0;
 	int wret;
 	int split;
@@ -4271,10 +4274,10 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 	btrfs_set_header_backref_rev(right, BTRFS_MIXED_BACKREF_REV);
 	btrfs_set_header_owner(right, root->root_key.objectid);
 	btrfs_set_header_level(right, 0);
-	write_extent_buffer(right, root->fs_info->fsid,
+	write_extent_buffer(right, fs_info->fsid,
 			    btrfs_header_fsid(), BTRFS_FSID_SIZE);
 
-	write_extent_buffer(right, root->fs_info->chunk_tree_uuid,
+	write_extent_buffer(right, fs_info->chunk_tree_uuid,
 			    btrfs_header_chunk_tree_uuid(right),
 			    BTRFS_UUID_SIZE);
 
@@ -4297,7 +4300,7 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 			path->nodes[0] = right;
 			path->slots[0] = 0;
 			if (path->slots[1] == 0)
-				fixup_low_keys(root, path, &disk_key, 1);
+				fixup_low_keys(fs_info, path, &disk_key, 1);
 		}
 		btrfs_mark_buffer_dirty(right);
 		return ret;
@@ -4615,7 +4618,7 @@ void btrfs_truncate_item(struct btrfs_root *root, struct btrfs_path *path,
 		btrfs_set_disk_key_offset(&disk_key, offset + size_diff);
 		btrfs_set_item_key(leaf, &disk_key, slot);
 		if (slot == 0)
-			fixup_low_keys(root, path, &disk_key, 1);
+			fixup_low_keys(root->fs_info, path, &disk_key, 1);
 	}
 
 	item = btrfs_item_nr(slot);
@@ -4716,7 +4719,7 @@ void setup_items_for_insert(struct btrfs_root *root, struct btrfs_path *path,
 
 	if (path->slots[0] == 0) {
 		btrfs_cpu_key_to_disk(&disk_key, cpu_key);
-		fixup_low_keys(root, path, &disk_key, 1);
+		fixup_low_keys(root->fs_info, path, &disk_key, 1);
 	}
 	btrfs_unlock_up_safe(path, 1);
 
@@ -4888,7 +4891,7 @@ static void del_ptr(struct btrfs_root *root, struct btrfs_path *path,
 		struct btrfs_disk_key disk_key;
 
 		btrfs_node_key(parent, &disk_key, 0);
-		fixup_low_keys(root, path, &disk_key, level + 1);
+		fixup_low_keys(root->fs_info, path, &disk_key, level + 1);
 	}
 	btrfs_mark_buffer_dirty(parent);
 }
@@ -4990,7 +4993,7 @@ int btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 			struct btrfs_disk_key disk_key;
 
 			btrfs_item_key(leaf, &disk_key, 0);
-			fixup_low_keys(root, path, &disk_key, 1);
+			fixup_low_keys(root->fs_info, path, &disk_key, 1);
 		}
 
 		/* delete the leaf if it is mostly empty */

commit 95449a1626fb6b643b576b9fbafed02793627577
Author: chandan <chandan@linux.vnet.ibm.com>
Date:   Thu Jan 15 12:22:03 2015 +0530

    Btrfs: insert_new_root: Fix lock type of the extent buffer.
    
    btrfs_alloc_tree_block() returns an extent buffer on which a blocked lock has
    been taken. Hence assign the appropriate value to path->locks[level].
    
    Signed-off-by: Chandan Rajendra <chandan@linux.vnet.ibm.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index fdd8fb4c6075..993642199326 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -3380,7 +3380,7 @@ static noinline int insert_new_root(struct btrfs_trans_handle *trans,
 	add_root_to_dirty_list(root);
 	extent_buffer_get(c);
 	path->nodes[level] = c;
-	path->locks[level] = BTRFS_WRITE_LOCK;
+	path->locks[level] = BTRFS_WRITE_LOCK_BLOCKING;
 	path->slots[level] = 0;
 	return 0;
 }

commit a8df6fe666f9a3a7c08df70f94351f967462dd95
Author: Filipe Manana <fdmanana@suse.com>
Date:   Tue Jan 20 12:40:53 2015 +0000

    Btrfs: fix setup_leaf_for_split() to avoid leaf corruption
    
    We were incorrectly detecting when the target key didn't exist anymore
    after releasing the path and re-searching the tree. This could make
    us split or duplicate (btrfs_split_item() and btrfs_duplicate_item()
    are its only callers at the moment) an item when we should not.
    
    For the case of duplicating an item, we currently only duplicate
    checksum items (csum tree) and file extent items (fs/subvol trees).
    For the checksum items we end up overriding the item completely,
    but for file extent items we update only some of their fields in
    the copy (done in __btrfs_drop_extents), which means we can end up
    having a logical corruption for some values.
    
    Also for the case where we duplicate a file extent item it will make
    us produce a leaf with a wrong key order, as btrfs_duplicate_item()
    advances us to the next slot and then its caller sets a smaller key
    on the new item at that slot (like in __btrfs_drop_extents() e.g.).
    Alternatively if the tree search in setup_leaf_for_split() leaves
    with path->slots[0] == btrfs_header_nritems(path->nodes[0]), we end
    up accessing beyond the leaf's end (when we check if the item's size
    has changed) and make our caller insert an item at the invalid slot
    btrfs_header_nritems(path->nodes[0]) + 1, causing an invalid memory
    access if the leaf is full or nearly full.
    
    This issue has been present since the introduction of this function
    in 2009:
    
        Btrfs: Add btrfs_duplicate_item
        commit ad48fd754676bfae4139be1a897b1ea58f9aaf21
    
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 6b2ec9089229..fdd8fb4c6075 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -4353,13 +4353,15 @@ static noinline int setup_leaf_for_split(struct btrfs_trans_handle *trans,
 	path->search_for_split = 1;
 	ret = btrfs_search_slot(trans, root, &key, path, 0, 1);
 	path->search_for_split = 0;
+	if (ret > 0)
+		ret = -EAGAIN;
 	if (ret < 0)
 		goto err;
 
 	ret = -EAGAIN;
 	leaf = path->nodes[0];
-	/* if our item isn't there or got smaller, return now */
-	if (ret > 0 || item_size != btrfs_item_size_nr(leaf, path->slots[0]))
+	/* if our item isn't there, return now */
+	if (item_size != btrfs_item_size_nr(leaf, path->slots[0]))
 		goto err;
 
 	/* the leaf has  changed, it now has room.  return now */

commit 57bbddd7fbf468336940d18a4c68a8678163acb0
Merge: d3541834884f ce3e69847e3e
Author: Chris Mason <clm@fb.com>
Date:   Wed Jan 21 17:49:35 2015 -0800

    Merge branch 'cleanup/blocksize-diet-part2' of git://git.kernel.org/pub/scm/linux/kernel/git/kdave/linux into for-linus

commit d3541834884f042aaaab1d6c0610cdc3488028e4
Merge: ce93ec548cfa 1d4c08e0a60b
Author: Chris Mason <clm@fb.com>
Date:   Wed Jan 21 17:45:25 2015 -0800

    Merge branch 'fix/find-item-path-leak' of git://git.kernel.org/pub/scm/linux/kernel/git/kdave/linux into for-linus

commit e7070be198b34c26f39bd9010a29ce6462dc4f3e
Author: Josef Bacik <jbacik@fb.com>
Date:   Tue Dec 16 08:54:43 2014 -0800

    Btrfs: change how we track dirty roots
    
    I've been overloading root->dirty_list to keep track of dirty roots and which
    roots need to have their commit roots switched at transaction commit time.  This
    could cause us to lose an update to the root which could corrupt the file
    system.  To fix this use a state bit to know if the root is dirty, and if it
    isn't set we go ahead and move the root to the dirty list.  This way if we
    re-dirty the root after adding it to the switch_commit list we make sure to
    update it.  This also makes it so that the extent root is always the last root
    on the dirty list to try and keep the amount of churn down at this point in the
    commit.  Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 14a72ed14ef7..97a98fc07cfc 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -213,11 +213,19 @@ static struct extent_buffer *btrfs_read_lock_root_node(struct btrfs_root *root)
  */
 static void add_root_to_dirty_list(struct btrfs_root *root)
 {
+	if (test_bit(BTRFS_ROOT_DIRTY, &root->state) ||
+	    !test_bit(BTRFS_ROOT_TRACK_DIRTY, &root->state))
+		return;
+
 	spin_lock(&root->fs_info->trans_lock);
-	if (test_bit(BTRFS_ROOT_TRACK_DIRTY, &root->state) &&
-	    list_empty(&root->dirty_list)) {
-		list_add(&root->dirty_list,
-			 &root->fs_info->dirty_cowonly_roots);
+	if (!test_and_set_bit(BTRFS_ROOT_DIRTY, &root->state)) {
+		/* Want the extent tree to be the last on the list */
+		if (root->objectid == BTRFS_EXTENT_TREE_OBJECTID)
+			list_move_tail(&root->dirty_list,
+				       &root->fs_info->dirty_cowonly_roots);
+		else
+			list_move(&root->dirty_list,
+				  &root->fs_info->dirty_cowonly_roots);
 	}
 	spin_unlock(&root->fs_info->trans_lock);
 }

commit 1d4c08e0a60be356134d0c466744d0d4e16ebab0
Author: David Sterba <dsterba@suse.cz>
Date:   Fri Jan 2 19:36:14 2015 +0100

    btrfs: expand btrfs_find_item if found_key is NULL
    
    If the found_key is NULL, then btrfs_find_item becomes a verbose wrapper
    for simple btrfs_search_slot.
    
    After we've removed all such callers, passing a NULL key is not valid
    anymore.
    
    Signed-off-by: David Sterba <dsterba@suse.cz>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index f54511dd287e..20d1f2b0403d 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2618,13 +2618,14 @@ int btrfs_find_item(struct btrfs_root *fs_root, struct btrfs_path *path,
 	struct extent_buffer *eb;
 
 	ASSERT(path);
+	ASSERT(found_key);
 
 	key.type = key_type;
 	key.objectid = iobjectid;
 	key.offset = ioff;
 
 	ret = btrfs_search_slot(NULL, fs_root, &key, path, 0, 0);
-	if ((ret < 0) || (found_key == NULL))
+	if (ret < 0)
 		return ret;
 
 	eb = path->nodes[0];

commit 381cf6587f8a8a8e981bc0c1aaaa8859b51dc756
Author: David Sterba <dsterba@suse.cz>
Date:   Fri Jan 2 18:45:16 2015 +0100

    btrfs: fix leak of path in btrfs_find_item
    
    If btrfs_find_item is called with NULL path it allocates one locally but
    does not free it. Affected paths are inserting an orphan item for a file
    and for a subvol root.
    
    Move the path allocation to the callers.
    
    CC: <stable@vger.kernel.org> # 3.14+
    Fixes: 3f870c289900 ("btrfs: expand btrfs_find_item() to include find_orphan_item functionality")
    Signed-off-by: David Sterba <dsterba@suse.cz>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 14a72ed14ef7..f54511dd287e 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2609,32 +2609,23 @@ static int key_search(struct extent_buffer *b, struct btrfs_key *key,
 	return 0;
 }
 
-int btrfs_find_item(struct btrfs_root *fs_root, struct btrfs_path *found_path,
+int btrfs_find_item(struct btrfs_root *fs_root, struct btrfs_path *path,
 		u64 iobjectid, u64 ioff, u8 key_type,
 		struct btrfs_key *found_key)
 {
 	int ret;
 	struct btrfs_key key;
 	struct extent_buffer *eb;
-	struct btrfs_path *path;
+
+	ASSERT(path);
 
 	key.type = key_type;
 	key.objectid = iobjectid;
 	key.offset = ioff;
 
-	if (found_path == NULL) {
-		path = btrfs_alloc_path();
-		if (!path)
-			return -ENOMEM;
-	} else
-		path = found_path;
-
 	ret = btrfs_search_slot(NULL, fs_root, &key, path, 0, 0);
-	if ((ret < 0) || (found_key == NULL)) {
-		if (path != found_path)
-			btrfs_free_path(path);
+	if ((ret < 0) || (found_key == NULL))
 		return ret;
-	}
 
 	eb = path->nodes[0];
 	if (ret && path->slots[0] >= btrfs_header_nritems(eb)) {

commit 3f556f7853ec4845a7c219d026cbcdf4cfa8cea7
Author: David Sterba <dsterba@suse.cz>
Date:   Sun Jun 15 03:20:26 2014 +0200

    btrfs: unify extent buffer allocation api
    
    Make the extent buffer allocation interface consistent.  Cloned eb will
    set a valid fs_info.  For dummy eb, we can drop the length parameter and
    set it from fs_info.
    
    The built-in sanity checks may pass a NULL fs_info that's queried for
    nodesize, but we know it's 4096.
    
    Signed-off-by: David Sterba <dsterba@suse.cz>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 50eca331812c..276d4187cbf0 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1363,8 +1363,7 @@ tree_mod_log_rewind(struct btrfs_fs_info *fs_info, struct btrfs_path *path,
 
 	if (tm->op == MOD_LOG_KEY_REMOVE_WHILE_FREEING) {
 		BUG_ON(tm->slot != 0);
-		eb_rewin = alloc_dummy_extent_buffer(eb->start,
-						fs_info->tree_root->nodesize);
+		eb_rewin = alloc_dummy_extent_buffer(fs_info, eb->start);
 		if (!eb_rewin) {
 			btrfs_tree_read_unlock_blocking(eb);
 			free_extent_buffer(eb);
@@ -1444,7 +1443,7 @@ get_old_root(struct btrfs_root *root, u64 time_seq)
 	} else if (old_root) {
 		btrfs_tree_read_unlock(eb_root);
 		free_extent_buffer(eb_root);
-		eb = alloc_dummy_extent_buffer(logical, root->nodesize);
+		eb = alloc_dummy_extent_buffer(root->fs_info, logical);
 	} else {
 		btrfs_set_lock_blocking_rw(eb_root, BTRFS_READ_LOCK);
 		eb = btrfs_clone_extent_buffer(eb_root);

commit d3e46fea1b1e8ba97a8c9dd8f54b97d086cd25aa
Author: David Sterba <dsterba@suse.cz>
Date:   Sun Jun 15 02:04:19 2014 +0200

    btrfs: sink blocksize parameter to readahead_tree_block
    
    All callers pass nodesize.
    
    Signed-off-by: David Sterba <dsterba@suse.cz>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 14a72ed14ef7..50eca331812c 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2282,7 +2282,7 @@ static void reada_for_search(struct btrfs_root *root,
 		if ((search <= target && target - search <= 65536) ||
 		    (search > target && search - target <= 65536)) {
 			gen = btrfs_node_ptr_generation(node, nr);
-			readahead_tree_block(root, search, blocksize);
+			readahead_tree_block(root, search);
 			nread += blocksize;
 		}
 		nscan++;
@@ -2301,7 +2301,6 @@ static noinline void reada_for_balance(struct btrfs_root *root,
 	u64 gen;
 	u64 block1 = 0;
 	u64 block2 = 0;
-	int blocksize;
 
 	parent = path->nodes[level + 1];
 	if (!parent)
@@ -2309,7 +2308,6 @@ static noinline void reada_for_balance(struct btrfs_root *root,
 
 	nritems = btrfs_header_nritems(parent);
 	slot = path->slots[level + 1];
-	blocksize = root->nodesize;
 
 	if (slot > 0) {
 		block1 = btrfs_node_blockptr(parent, slot - 1);
@@ -2334,9 +2332,9 @@ static noinline void reada_for_balance(struct btrfs_root *root,
 	}
 
 	if (block1)
-		readahead_tree_block(root, block1, blocksize);
+		readahead_tree_block(root, block1);
 	if (block2)
-		readahead_tree_block(root, block2, blocksize);
+		readahead_tree_block(root, block2);
 }
 
 

commit 9627aeee3e203e30679549e4962633698a6bf87f
Merge: cb83b7b81698 5d3edd8f44aa
Author: Chris Mason <clm@fb.com>
Date:   Tue Dec 2 18:42:03 2014 -0800

    Merge branch 'raid56-scrub-replace' of git://github.com/miaoxie/linux-btrfs into for-linus

commit 5f5bc6b1e2d5a6f827bc860ef2dc5b6f365d1339
Author: Filipe Manana <fdmanana@suse.com>
Date:   Sun Nov 9 08:38:39 2014 +0000

    Btrfs: make xattr replace operations atomic
    
    Replacing a xattr consists of doing a lookup for its existing value, delete
    the current value from the respective leaf, release the search path and then
    finally insert the new value. This leaves a time window where readers (getxattr,
    listxattrs) won't see any value for the xattr. Xattrs are used to store ACLs,
    so this has security implications.
    
    This change also fixes 2 other existing issues which were:
    
    *) Deleting the old xattr value without verifying first if the new xattr will
       fit in the existing leaf item (in case multiple xattrs are packed in the
       same item due to name hash collision);
    
    *) Returning -EEXIST when the flag XATTR_CREATE is given and the xattr doesn't
       exist but we have have an existing item that packs muliple xattrs with
       the same name hash as the input xattr. In this case we should return ENOSPC.
    
    A test case for xfstests follows soon.
    
    Thanks to Alexandre Oliva for reporting the non-atomicity of the xattr replace
    implementation.
    
    Reported-by: Alexandre Oliva <oliva@gnu.org>
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 19bc6162fb8e..817234168a7f 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2939,7 +2939,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 	 */
 	if (!p->leave_spinning)
 		btrfs_set_path_blocking(p);
-	if (ret < 0)
+	if (ret < 0 && !p->skip_release_on_error)
 		btrfs_release_path(p);
 	return ret;
 }

commit f82c458a2c3ffb94b431fc6ad791a79df1b3713e
Author: Chris Mason <clm@fb.com>
Date:   Wed Nov 19 10:25:09 2014 -0800

    btrfs: fix lockups from btrfs_clear_path_blocking
    
    The fair reader/writer locks mean that btrfs_clear_path_blocking needs
    to strictly follow lock ordering rules even when we already have
    blocking locks on a given path.
    
    Before we can clear a blocking lock on the path, we need to make sure
    all of the locks have been converted to blocking.  This will remove lock
    inversions against anyone spinning in write_lock() against the buffers
    we're trying to get read locks on.  These inversions didn't exist before
    the fair read/writer locks, but now we need to be more careful.
    
    We papered over this deadlock in the past by changing
    btrfs_try_read_lock() to be a true trylock against both the spinlock and
    the blocking lock.  This was slower, and not sufficient to fix all the
    deadlocks.  This patch adds a btrfs_tree_read_lock_atomic(), which
    basically means get the spinlock but trylock on the blocking lock.
    
    Signed-off-by: Chris Mason <clm@fb.com>
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Reported-by: Patrick Schmid <schmid@phys.ethz.ch>
    cc: stable@vger.kernel.org #v3.15+

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 19bc6162fb8e..150822ee0a0b 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -80,13 +80,6 @@ noinline void btrfs_clear_path_blocking(struct btrfs_path *p,
 {
 	int i;
 
-#ifdef CONFIG_DEBUG_LOCK_ALLOC
-	/* lockdep really cares that we take all of these spinlocks
-	 * in the right order.  If any of the locks in the path are not
-	 * currently blocking, it is going to complain.  So, make really
-	 * really sure by forcing the path to blocking before we clear
-	 * the path blocking.
-	 */
 	if (held) {
 		btrfs_set_lock_blocking_rw(held, held_rw);
 		if (held_rw == BTRFS_WRITE_LOCK)
@@ -95,7 +88,6 @@ noinline void btrfs_clear_path_blocking(struct btrfs_path *p,
 			held_rw = BTRFS_READ_LOCK_BLOCKING;
 	}
 	btrfs_set_path_blocking(p);
-#endif
 
 	for (i = BTRFS_MAX_LEVEL - 1; i >= 0; i--) {
 		if (p->nodes[i] && p->locks[i]) {
@@ -107,10 +99,8 @@ noinline void btrfs_clear_path_blocking(struct btrfs_path *p,
 		}
 	}
 
-#ifdef CONFIG_DEBUG_LOCK_ALLOC
 	if (held)
 		btrfs_clear_lock_blocking_rw(held, held_rw);
-#endif
 }
 
 /* this also releases the path */
@@ -2893,7 +2883,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 					}
 					p->locks[level] = BTRFS_WRITE_LOCK;
 				} else {
-					err = btrfs_try_tree_read_lock(b);
+					err = btrfs_tree_read_lock_atomic(b);
 					if (!err) {
 						btrfs_set_path_blocking(p);
 						btrfs_tree_read_lock(b);
@@ -3025,7 +3015,7 @@ int btrfs_search_old_slot(struct btrfs_root *root, struct btrfs_key *key,
 			}
 
 			level = btrfs_header_level(b);
-			err = btrfs_try_tree_read_lock(b);
+			err = btrfs_tree_read_lock_atomic(b);
 			if (!err) {
 				btrfs_set_path_blocking(p);
 				btrfs_tree_read_lock(b);

commit 27b19cc8864e206c4203041892b0f706f044a0f1
Merge: bbf65cf0b5b6 4d75f8a9c87b
Author: Chris Mason <clm@fb.com>
Date:   Sat Oct 4 09:57:14 2014 -0700

    Merge branch 'cleanup/blocksize-diet-part1' of git://git.kernel.org/pub/scm/linux/kernel/git/kdave/linux into for-linus

commit bbf65cf0b5b67843ca094df01019222b85af2183
Merge: bf8e8ca6fd4a fccb84c94a97
Author: Chris Mason <clm@fb.com>
Date:   Sat Oct 4 09:56:45 2014 -0700

    Merge branch 'cleanup/misc-for-3.18' of git://git.kernel.org/pub/scm/linux/kernel/git/kdave/linux into for-linus
    
    Signed-off-by: Chris Mason <clm@fb.com>
    
    Conflicts:
            fs/btrfs/extent_io.c

commit b99d9a6a4a41712c609a0b468512b2043a1b5f1d
Author: Fabian Frederick <fabf@skynet.be>
Date:   Thu Sep 25 19:35:02 2014 +0200

    btrfs: fix shadow warning on cmp
    
    cmp was declared twice in btrfs_compare_trees resulting in a shadow
    warning. This patch renames second internal variable.
    
    Signed-off-by: Fabian Frederick <fabf@skynet.be>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 39021bf2df9a..8d0703cc7402 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -5522,18 +5522,18 @@ int btrfs_compare_trees(struct btrfs_root *left_root,
 					goto out;
 				advance_right = ADVANCE;
 			} else {
-				enum btrfs_compare_tree_result cmp;
+				enum btrfs_compare_tree_result result;
 
 				WARN_ON(!extent_buffer_uptodate(left_path->nodes[0]));
 				ret = tree_compare_item(left_root, left_path,
 						right_path, tmp_buf);
 				if (ret)
-					cmp = BTRFS_COMPARE_TREE_CHANGED;
+					result = BTRFS_COMPARE_TREE_CHANGED;
 				else
-					cmp = BTRFS_COMPARE_TREE_SAME;
+					result = BTRFS_COMPARE_TREE_SAME;
 				ret = changed_cb(left_root, right_root,
 						 left_path, right_path,
-						 &left_key, cmp, ctx);
+						 &left_key, result, ctx);
 				if (ret < 0)
 					goto out;
 				advance_left = ADVANCE;

commit fccb84c94a9755f48668e43d0a44d6ecc750900f
Author: David Sterba <dsterba@suse.cz>
Date:   Mon Sep 29 23:53:21 2014 +0200

    btrfs: move checks for DUMMY_ROOT into a helper
    
    Signed-off-by: David Sterba <dsterba@suse.cz>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 533657c508e2..ce1d71d171bb 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1506,10 +1506,9 @@ static inline int should_cow_block(struct btrfs_trans_handle *trans,
 				   struct btrfs_root *root,
 				   struct extent_buffer *buf)
 {
-#ifdef CONFIG_BTRFS_FS_RUN_SANITY_TESTS
-	if (unlikely(test_bit(BTRFS_ROOT_DUMMY_ROOT, &root->state)))
+	if (btrfs_test_is_dummy_root(root))
 		return 0;
-#endif
+
 	/* ensure we can see the force_cow */
 	smp_rmb();
 

commit 7ec20afbcb7b257aec82ea5d66e6b0b7499abaca
Author: David Sterba <dsterba@suse.cz>
Date:   Thu Jul 24 17:34:58 2014 +0200

    btrfs: new define for the inline extent data start
    
    Use a common definition for the inline data start so we don't have to
    open-code it and introduce bugs like "Btrfs: fix wrong max inline data
    size limit" fixed.
    
    Signed-off-by: David Sterba <dsterba@suse.cz>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 39021bf2df9a..533657c508e2 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -4625,8 +4625,7 @@ void btrfs_truncate_item(struct btrfs_root *root, struct btrfs_path *path,
 				ptr = btrfs_item_ptr_offset(leaf, slot);
 				memmove_extent_buffer(leaf, ptr,
 				      (unsigned long)fi,
-				      offsetof(struct btrfs_file_extent_item,
-						 disk_bytenr));
+				      BTRFS_FILE_EXTENT_INLINE_DATA_START);
 			}
 		}
 

commit 4d75f8a9c87b843c8ded15b82b8d137b9724cccc
Author: David Sterba <dsterba@suse.cz>
Date:   Sun Jun 15 01:54:12 2014 +0200

    btrfs: remove blocksize from btrfs_alloc_free_block and rename
    
    Rename to btrfs_alloc_tree_block as it fits to the alloc/find/free +
    _tree_block family. The parameter blocksize was set to the metadata
    block size, directly or indirectly.
    
    Signed-off-by: David Sterba <dsterba@suse.cz>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 2fb4ab659a0f..d498982bd202 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -258,9 +258,8 @@ int btrfs_copy_root(struct btrfs_trans_handle *trans,
 	else
 		btrfs_node_key(buf, &disk_key, 0);
 
-	cow = btrfs_alloc_free_block(trans, root, buf->len, 0,
-				     new_root_objectid, &disk_key, level,
-				     buf->start, 0);
+	cow = btrfs_alloc_tree_block(trans, root, 0, new_root_objectid,
+			&disk_key, level, buf->start, 0);
 	if (IS_ERR(cow))
 		return PTR_ERR(cow);
 
@@ -1133,9 +1132,9 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 	} else
 		parent_start = 0;
 
-	cow = btrfs_alloc_free_block(trans, root, buf->len, parent_start,
-				     root->root_key.objectid, &disk_key,
-				     level, search_start, empty_size);
+	cow = btrfs_alloc_tree_block(trans, root, parent_start,
+			root->root_key.objectid, &disk_key, level,
+			search_start, empty_size);
 	if (IS_ERR(cow))
 		return PTR_ERR(cow);
 
@@ -3355,9 +3354,8 @@ static noinline int insert_new_root(struct btrfs_trans_handle *trans,
 	else
 		btrfs_node_key(lower, &lower_key, 0);
 
-	c = btrfs_alloc_free_block(trans, root, root->nodesize, 0,
-				   root->root_key.objectid, &lower_key,
-				   level, root->node->start, 0);
+	c = btrfs_alloc_tree_block(trans, root, 0, root->root_key.objectid,
+				   &lower_key, level, root->node->start, 0);
 	if (IS_ERR(c))
 		return PTR_ERR(c);
 
@@ -3495,9 +3493,8 @@ static noinline int split_node(struct btrfs_trans_handle *trans,
 	mid = (c_nritems + 1) / 2;
 	btrfs_node_key(c, &disk_key, mid);
 
-	split = btrfs_alloc_free_block(trans, root, root->nodesize, 0,
-					root->root_key.objectid,
-					&disk_key, level, c->start, 0);
+	split = btrfs_alloc_tree_block(trans, root, 0, root->root_key.objectid,
+			&disk_key, level, c->start, 0);
 	if (IS_ERR(split))
 		return PTR_ERR(split);
 
@@ -4275,9 +4272,8 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 	else
 		btrfs_item_key(l, &disk_key, mid);
 
-	right = btrfs_alloc_free_block(trans, root, root->nodesize, 0,
-					root->root_key.objectid,
-					&disk_key, 0, l->start, 0);
+	right = btrfs_alloc_tree_block(trans, root, 0, root->root_key.objectid,
+			&disk_key, 0, l->start, 0);
 	if (IS_ERR(right))
 		return PTR_ERR(right);
 

commit 0308af4465897c889e32754ef37bb465a1b2b872
Author: David Sterba <dsterba@suse.cz>
Date:   Sun Jun 15 01:43:40 2014 +0200

    btrfs: remove unused parameter blocksize from btrfs_find_tree_block
    
    Signed-off-by: David Sterba <dsterba@suse.cz>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 302c3f955706..2fb4ab659a0f 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1683,7 +1683,7 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 			continue;
 		}
 
-		cur = btrfs_find_tree_block(root, blocknr, blocksize);
+		cur = btrfs_find_tree_block(root, blocknr);
 		if (cur)
 			uptodate = btrfs_buffer_uptodate(cur, gen, 0);
 		else
@@ -2264,7 +2264,7 @@ static void reada_for_search(struct btrfs_root *root,
 
 	search = btrfs_node_blockptr(node, slot);
 	blocksize = root->nodesize;
-	eb = btrfs_find_tree_block(root, search, blocksize);
+	eb = btrfs_find_tree_block(root, search);
 	if (eb) {
 		free_extent_buffer(eb);
 		return;
@@ -2326,7 +2326,7 @@ static noinline void reada_for_balance(struct btrfs_root *root,
 	if (slot > 0) {
 		block1 = btrfs_node_blockptr(parent, slot - 1);
 		gen = btrfs_node_ptr_generation(parent, slot - 1);
-		eb = btrfs_find_tree_block(root, block1, blocksize);
+		eb = btrfs_find_tree_block(root, block1);
 		/*
 		 * if we get -eagain from btrfs_buffer_uptodate, we
 		 * don't want to return eagain here.  That will loop
@@ -2339,7 +2339,7 @@ static noinline void reada_for_balance(struct btrfs_root *root,
 	if (slot + 1 < nritems) {
 		block2 = btrfs_node_blockptr(parent, slot + 1);
 		gen = btrfs_node_ptr_generation(parent, slot + 1);
-		eb = btrfs_find_tree_block(root, block2, blocksize);
+		eb = btrfs_find_tree_block(root, block2);
 		if (eb && btrfs_buffer_uptodate(eb, gen, 1) != 0)
 			block2 = 0;
 		free_extent_buffer(eb);
@@ -2450,16 +2450,14 @@ read_block_for_search(struct btrfs_trans_handle *trans,
 {
 	u64 blocknr;
 	u64 gen;
-	u32 blocksize;
 	struct extent_buffer *b = *eb_ret;
 	struct extent_buffer *tmp;
 	int ret;
 
 	blocknr = btrfs_node_blockptr(b, slot);
 	gen = btrfs_node_ptr_generation(b, slot);
-	blocksize = root->nodesize;
 
-	tmp = btrfs_find_tree_block(root, blocknr, blocksize);
+	tmp = btrfs_find_tree_block(root, blocknr);
 	if (tmp) {
 		/* first we do an atomic uptodate check */
 		if (btrfs_buffer_uptodate(tmp, gen, 1) > 0) {

commit ce86cd59179279a6fe673d2a105d24fb7e70aef3
Author: David Sterba <dsterba@suse.cz>
Date:   Sun Jun 15 01:07:32 2014 +0200

    btrfs: remove parameter blocksize from read_tree_block
    
    We know the tree block size, no need to pass it around.
    
    Signed-off-by: David Sterba <dsterba@suse.cz>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 1b7e3545a596..302c3f955706 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1425,7 +1425,6 @@ get_old_root(struct btrfs_root *root, u64 time_seq)
 	struct tree_mod_root *old_root = NULL;
 	u64 old_generation = 0;
 	u64 logical;
-	u32 blocksize;
 
 	eb_root = btrfs_read_lock_root_node(root);
 	tm = __tree_mod_log_oldest_root(root->fs_info, eb_root, time_seq);
@@ -1444,8 +1443,7 @@ get_old_root(struct btrfs_root *root, u64 time_seq)
 	if (old_root && tm && tm->op != MOD_LOG_KEY_REMOVE_WHILE_FREEING) {
 		btrfs_tree_read_unlock(eb_root);
 		free_extent_buffer(eb_root);
-		blocksize = root->nodesize;
-		old = read_tree_block(root, logical, blocksize, 0);
+		old = read_tree_block(root, logical, 0);
 		if (WARN_ON(!old || !extent_buffer_uptodate(old))) {
 			free_extent_buffer(old);
 			btrfs_warn(root->fs_info,
@@ -1692,8 +1690,7 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 			uptodate = 0;
 		if (!cur || !uptodate) {
 			if (!cur) {
-				cur = read_tree_block(root, blocknr,
-							 blocksize, gen);
+				cur = read_tree_block(root, blocknr, gen);
 				if (!cur || !extent_buffer_uptodate(cur)) {
 					free_extent_buffer(cur);
 					return -EIO;
@@ -1872,7 +1869,6 @@ static noinline struct extent_buffer *read_node_slot(struct btrfs_root *root,
 	BUG_ON(level == 0);
 
 	eb = read_tree_block(root, btrfs_node_blockptr(parent, slot),
-			     root->nodesize,
 			     btrfs_node_ptr_generation(parent, slot));
 	if (eb && !extent_buffer_uptodate(eb)) {
 		free_extent_buffer(eb);
@@ -2507,7 +2503,7 @@ read_block_for_search(struct btrfs_trans_handle *trans,
 	btrfs_release_path(p);
 
 	ret = -EAGAIN;
-	tmp = read_tree_block(root, blocknr, blocksize, 0);
+	tmp = read_tree_block(root, blocknr, 0);
 	if (tmp) {
 		/*
 		 * If the read above didn't mark this buffer up to date,

commit 58dc4ce4325108b35425ffd30e6acfad9644d49d
Author: David Sterba <dsterba@suse.cz>
Date:   Sun Jun 15 00:29:04 2014 +0200

    btrfs: remove unused parameter from readahead_tree_block
    
    The parent_transid parameter has been unused since its introduction in
    ca7a79ad8dbe2466 ("Pass down the expected generation number when reading
    tree blocks").  In reada_tree_block, it was even wrongly set to leafsize.
    Transid check is done in the proper read and readahead ignores errors.
    
    Signed-off-by: David Sterba <dsterba@suse.cz>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 39021bf2df9a..1b7e3545a596 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2298,7 +2298,7 @@ static void reada_for_search(struct btrfs_root *root,
 		if ((search <= target && target - search <= 65536) ||
 		    (search > target && search - target <= 65536)) {
 			gen = btrfs_node_ptr_generation(node, nr);
-			readahead_tree_block(root, search, blocksize, gen);
+			readahead_tree_block(root, search, blocksize);
 			nread += blocksize;
 		}
 		nscan++;
@@ -2350,9 +2350,9 @@ static noinline void reada_for_balance(struct btrfs_root *root,
 	}
 
 	if (block1)
-		readahead_tree_block(root, block1, blocksize, 0);
+		readahead_tree_block(root, block1, blocksize);
 	if (block2)
-		readahead_tree_block(root, block2, blocksize, 0);
+		readahead_tree_block(root, block2, blocksize);
 }
 
 

commit f98de9b9c07485f7e21edfd5b2b20c89d662af3c
Author: Filipe Manana <fdmanana@suse.com>
Date:   Mon Aug 4 19:37:21 2014 +0100

    Btrfs: make btrfs_search_forward return with nodes unlocked
    
    None of the uses of btrfs_search_forward() need to have the path
    nodes (level >= 1) read locked, only the leaf needs to be locked
    while the caller processes it. Therefore make it return a path
    with all nodes unlocked, except for the leaf.
    
    This change is motivated by the observation that during a file
    fsync we repeatdly call btrfs_search_forward() and process the
    returned leaf while upper nodes of the returned path (level >= 1)
    are read locked, which unnecessarily blocks other tasks that want
    to write to the same fs/subvol btree.
    Therefore instead of modifying the fsync code to unlock all nodes
    with level >= 1 immediately after calling btrfs_search_forward(),
    change btrfs_search_forward() to do it, so that it benefits all
    callers.
    
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 783ea3bac7dc..39021bf2df9a 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -5144,8 +5144,9 @@ int btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,
 	u32 nritems;
 	int level;
 	int ret = 1;
+	int keep_locks = path->keep_locks;
 
-	WARN_ON(!path->keep_locks);
+	path->keep_locks = 1;
 again:
 	cur = btrfs_read_lock_root_node(root);
 	level = btrfs_header_level(cur);
@@ -5209,7 +5210,6 @@ int btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,
 		path->slots[level] = slot;
 		if (level == path->lowest_level) {
 			ret = 0;
-			unlock_up(path, level, 1, 0, NULL);
 			goto out;
 		}
 		btrfs_set_path_blocking(path);
@@ -5224,9 +5224,12 @@ int btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,
 		btrfs_clear_path_blocking(path, NULL, 0);
 	}
 out:
-	if (ret == 0)
+	path->keep_locks = keep_locks;
+	if (ret == 0) {
+		btrfs_unlock_up_safe(path, path->lowest_level + 1);
+		btrfs_set_path_blocking(path);
 		memcpy(min_key, &found_key, sizeof(found_key));
-	btrfs_set_path_blocking(path);
+	}
 	return ret;
 }
 

commit 160f4089c8580b32b5805e7fd8ec7b3810f442c1
Author: Filipe Manana <fdmanana@suse.com>
Date:   Mon Jul 28 19:37:17 2014 +0100

    Btrfs: avoid unnecessary switch of path locks to blocking mode
    
    If we need to cow a node, increase the write lock level and retry the
    tree search, there's no point of changing the node locks in our path
    to blocking mode, as we only waste time and unnecessarily wake up other
    tasks waiting on the spinning locks (just to block them again shortly
    after) because we release our path before repeating the tree search.
    
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index bd0ae3ec76ff..783ea3bac7dc 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2792,8 +2792,6 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 			if (!should_cow_block(trans, root, b))
 				goto cow_done;
 
-			btrfs_set_path_blocking(p);
-
 			/*
 			 * must have write locks on this node and the
 			 * parent
@@ -2807,6 +2805,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 				goto again;
 			}
 
+			btrfs_set_path_blocking(p);
 			err = btrfs_cow_block(trans, root, b,
 					      p->nodes[level + 1],
 					      p->slots[level + 1], &b);

commit 24cdc847d9842bdfd85a005ebc39050bcbc7acba
Author: Filipe Manana <fdmanana@suse.com>
Date:   Mon Jul 28 19:34:35 2014 +0100

    Btrfs: unlock nodes earlier when inserting items in a btree
    
    In ctree.c:setup_items_for_insert(), we can unlock all nodes in our
    path before we process the leaf (shift items and data, adjust data
    offsets, etc). This allows for better btree concurrency, as we're
    often holding a write lock on at least the node at level 1.
    
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 263145b27155..bd0ae3ec76ff 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -4738,6 +4738,12 @@ void setup_items_for_insert(struct btrfs_root *root, struct btrfs_path *path,
 	int slot;
 	struct btrfs_map_token token;
 
+	if (path->slots[0] == 0) {
+		btrfs_cpu_key_to_disk(&disk_key, cpu_key);
+		fixup_low_keys(root, path, &disk_key, 1);
+	}
+	btrfs_unlock_up_safe(path, 1);
+
 	btrfs_init_map_token(&token);
 
 	leaf = path->nodes[0];
@@ -4798,12 +4804,6 @@ void setup_items_for_insert(struct btrfs_root *root, struct btrfs_path *path,
 	}
 
 	btrfs_set_header_nritems(leaf, nritems + nr);
-
-	if (slot == 0) {
-		btrfs_cpu_key_to_disk(&disk_key, cpu_key);
-		fixup_low_keys(root, path, &disk_key, 1);
-	}
-	btrfs_unlock_up_safe(path, 1);
 	btrfs_mark_buffer_dirty(leaf);
 
 	if (btrfs_leaf_free_space(root, leaf) < 0) {

commit 707e8a071528385a87b63a72a37c2322e463c7b8
Author: David Sterba <dsterba@suse.cz>
Date:   Wed Jun 4 19:22:26 2014 +0200

    btrfs: use nodesize everywhere, kill leafsize
    
    The nodesize and leafsize were never of different values. Unify the
    usage and make nodesize the one. Cleanup the redundant checks and
    helpers.
    
    Shaves a few bytes from .text:
    
      text    data     bss     dec     hex filename
    852418   24560   23112  900090   dbbfa btrfs.ko.before
    851074   24584   23112  898770   db6d2 btrfs.ko.after
    
    Signed-off-by: David Sterba <dsterba@suse.cz>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 44ee5d2e52a4..263145b27155 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1444,7 +1444,7 @@ get_old_root(struct btrfs_root *root, u64 time_seq)
 	if (old_root && tm && tm->op != MOD_LOG_KEY_REMOVE_WHILE_FREEING) {
 		btrfs_tree_read_unlock(eb_root);
 		free_extent_buffer(eb_root);
-		blocksize = btrfs_level_size(root, old_root->level);
+		blocksize = root->nodesize;
 		old = read_tree_block(root, logical, blocksize, 0);
 		if (WARN_ON(!old || !extent_buffer_uptodate(old))) {
 			free_extent_buffer(old);
@@ -1651,7 +1651,7 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 	WARN_ON(trans->transid != root->fs_info->generation);
 
 	parent_nritems = btrfs_header_nritems(parent);
-	blocksize = btrfs_level_size(root, parent_level - 1);
+	blocksize = root->nodesize;
 	end_slot = parent_nritems;
 
 	if (parent_nritems == 1)
@@ -1872,7 +1872,7 @@ static noinline struct extent_buffer *read_node_slot(struct btrfs_root *root,
 	BUG_ON(level == 0);
 
 	eb = read_tree_block(root, btrfs_node_blockptr(parent, slot),
-			     btrfs_level_size(root, level - 1),
+			     root->nodesize,
 			     btrfs_node_ptr_generation(parent, slot));
 	if (eb && !extent_buffer_uptodate(eb)) {
 		free_extent_buffer(eb);
@@ -2267,7 +2267,7 @@ static void reada_for_search(struct btrfs_root *root,
 	node = path->nodes[level];
 
 	search = btrfs_node_blockptr(node, slot);
-	blocksize = btrfs_level_size(root, level - 1);
+	blocksize = root->nodesize;
 	eb = btrfs_find_tree_block(root, search, blocksize);
 	if (eb) {
 		free_extent_buffer(eb);
@@ -2325,7 +2325,7 @@ static noinline void reada_for_balance(struct btrfs_root *root,
 
 	nritems = btrfs_header_nritems(parent);
 	slot = path->slots[level + 1];
-	blocksize = btrfs_level_size(root, level);
+	blocksize = root->nodesize;
 
 	if (slot > 0) {
 		block1 = btrfs_node_blockptr(parent, slot - 1);
@@ -2461,7 +2461,7 @@ read_block_for_search(struct btrfs_trans_handle *trans,
 
 	blocknr = btrfs_node_blockptr(b, slot);
 	gen = btrfs_node_ptr_generation(b, slot);
-	blocksize = btrfs_level_size(root, level - 1);
+	blocksize = root->nodesize;
 
 	tmp = btrfs_find_tree_block(root, blocknr, blocksize);
 	if (tmp) {
@@ -4282,13 +4282,13 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 	else
 		btrfs_item_key(l, &disk_key, mid);
 
-	right = btrfs_alloc_free_block(trans, root, root->leafsize, 0,
+	right = btrfs_alloc_free_block(trans, root, root->nodesize, 0,
 					root->root_key.objectid,
 					&disk_key, 0, l->start, 0);
 	if (IS_ERR(right))
 		return PTR_ERR(right);
 
-	root_add_used(root, root->leafsize);
+	root_add_used(root, root->nodesize);
 
 	memset_extent_buffer(right, 0, 0, sizeof(struct btrfs_header));
 	btrfs_set_header_bytenr(right, right->start);
@@ -5375,7 +5375,7 @@ int btrfs_compare_trees(struct btrfs_root *left_root,
 		goto out;
 	}
 
-	tmp_buf = kmalloc(left_root->leafsize, GFP_NOFS);
+	tmp_buf = kmalloc(left_root->nodesize, GFP_NOFS);
 	if (!tmp_buf) {
 		ret = -ENOMEM;
 		goto out;

commit e339a6b097c515a31ce230d498c44ff2e89f1cf4
Author: Josef Bacik <jbacik@fb.com>
Date:   Wed Jul 2 10:54:25 2014 -0700

    Btrfs: __btrfs_mod_ref should always use no_quota
    
    Before I extended the no_quota arg to btrfs_dec/inc_ref because I didn't
    understand how snapshot delete was using it and assumed that we needed the
    quota operations there.  With Mark's work this has turned out to be not the
    case, we _always_ need to use no_quota for btrfs_dec/inc_ref, so just drop the
    argument and make __btrfs_mod_ref call it's process function with no_quota set
    always.  Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index aeab453b8e24..44ee5d2e52a4 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -280,9 +280,9 @@ int btrfs_copy_root(struct btrfs_trans_handle *trans,
 
 	WARN_ON(btrfs_header_generation(buf) > trans->transid);
 	if (new_root_objectid == BTRFS_TREE_RELOC_OBJECTID)
-		ret = btrfs_inc_ref(trans, root, cow, 1, 1);
+		ret = btrfs_inc_ref(trans, root, cow, 1);
 	else
-		ret = btrfs_inc_ref(trans, root, cow, 0, 1);
+		ret = btrfs_inc_ref(trans, root, cow, 0);
 
 	if (ret)
 		return ret;
@@ -1035,14 +1035,14 @@ static noinline int update_ref_for_cow(struct btrfs_trans_handle *trans,
 		if ((owner == root->root_key.objectid ||
 		     root->root_key.objectid == BTRFS_TREE_RELOC_OBJECTID) &&
 		    !(flags & BTRFS_BLOCK_FLAG_FULL_BACKREF)) {
-			ret = btrfs_inc_ref(trans, root, buf, 1, 1);
+			ret = btrfs_inc_ref(trans, root, buf, 1);
 			BUG_ON(ret); /* -ENOMEM */
 
 			if (root->root_key.objectid ==
 			    BTRFS_TREE_RELOC_OBJECTID) {
-				ret = btrfs_dec_ref(trans, root, buf, 0, 1);
+				ret = btrfs_dec_ref(trans, root, buf, 0);
 				BUG_ON(ret); /* -ENOMEM */
-				ret = btrfs_inc_ref(trans, root, cow, 1, 1);
+				ret = btrfs_inc_ref(trans, root, cow, 1);
 				BUG_ON(ret); /* -ENOMEM */
 			}
 			new_flags |= BTRFS_BLOCK_FLAG_FULL_BACKREF;
@@ -1050,9 +1050,9 @@ static noinline int update_ref_for_cow(struct btrfs_trans_handle *trans,
 
 			if (root->root_key.objectid ==
 			    BTRFS_TREE_RELOC_OBJECTID)
-				ret = btrfs_inc_ref(trans, root, cow, 1, 1);
+				ret = btrfs_inc_ref(trans, root, cow, 1);
 			else
-				ret = btrfs_inc_ref(trans, root, cow, 0, 1);
+				ret = btrfs_inc_ref(trans, root, cow, 0);
 			BUG_ON(ret); /* -ENOMEM */
 		}
 		if (new_flags != 0) {
@@ -1069,11 +1069,11 @@ static noinline int update_ref_for_cow(struct btrfs_trans_handle *trans,
 		if (flags & BTRFS_BLOCK_FLAG_FULL_BACKREF) {
 			if (root->root_key.objectid ==
 			    BTRFS_TREE_RELOC_OBJECTID)
-				ret = btrfs_inc_ref(trans, root, cow, 1, 1);
+				ret = btrfs_inc_ref(trans, root, cow, 1);
 			else
-				ret = btrfs_inc_ref(trans, root, cow, 0, 1);
+				ret = btrfs_inc_ref(trans, root, cow, 0);
 			BUG_ON(ret); /* -ENOMEM */
-			ret = btrfs_dec_ref(trans, root, buf, 1, 1);
+			ret = btrfs_dec_ref(trans, root, buf, 1);
 			BUG_ON(ret); /* -ENOMEM */
 		}
 		clean_tree_block(trans, root, buf);

commit 0b43e04f700bce8eecf3581612e9d3e15bf258bc
Author: Liu Bo <bo.li.liu@oracle.com>
Date:   Mon Jun 9 11:04:49 2014 +0800

    Btrfs: fix leaf corruption after __btrfs_drop_extents
    
    Several reports about leaf corruption has been floating on the list, one of them
    points to __btrfs_drop_extents(), and we find that the leaf becomes corrupted
    after __btrfs_drop_extents(), it's really a rare case but it does exist.
    
    The problem turns out to be btrfs_next_leaf() called in __btrfs_drop_extents().
    
    So in btrfs_next_leaf(), we release the current path to re-search the last key of
    the leaf for locating next leaf, and we've taken it into account that there might
    be balance operations between leafs during this 'unlock and re-lock' dance, so
    we check the path again and advance it if there are now more items available.
    But things are a bit different if that last key happens to be removed and balance
    gets a bigger key as the last one, and btrfs_search_slot will return it with
    ret > 0, IOW, nothing change in this leaf except the new last key, then we think
    we're okay because there is no more item balanced in, fine, we thinks we can
    go to the next leaf.
    
    However, we should return that bigger key, otherwise we deserve leaf corruption,
    for example, in endio, skipping that key means that __btrfs_drop_extents() thinks
    it has dropped all extent matched the required range and finish_ordered_io can
    safely insert a new extent, but it actually doesn't and ends up a leaf
    corruption.
    
    One may be asking that why our locking on extent io tree doesn't work as
    expected, ie. it should avoid this kind of race situation.  But in
    __btrfs_drop_extents(), we don't always find extents which are included within
    our locking range, IOW, extents can start before our searching start, in this
    case locking on extent io tree doesn't protect us from the race.
    
    This takes the special case into account.
    
    Reviewed-by: Filipe Manana <fdmanana@gmail.com>
    Signed-off-by: Liu Bo <bo.li.liu@oracle.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 4eada52f3969..aeab453b8e24 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -5718,6 +5718,24 @@ int btrfs_next_old_leaf(struct btrfs_root *root, struct btrfs_path *path,
 		ret = 0;
 		goto done;
 	}
+	/*
+	 * So the above check misses one case:
+	 * - after releasing the path above, someone has removed the item that
+	 *   used to be at the very end of the block, and balance between leafs
+	 *   gets another one with bigger key.offset to replace it.
+	 *
+	 * This one should be returned as well, or we can get leaf corruption
+	 * later(esp. in __btrfs_drop_extents()).
+	 *
+	 * And a bit more explanation about this check,
+	 * with ret > 0, the key isn't found, the path points to the slot
+	 * where it should be inserted, so the path->slots[0] item must be the
+	 * bigger one.
+	 */
+	if (nritems > 0 && ret > 0 && path->slots[0] == nritems - 1) {
+		ret = 0;
+		goto done;
+	}
 
 	while (level < BTRFS_MAX_LEVEL) {
 		if (!path->nodes[level]) {

commit 337c6f6830a5ceb650eaf10d38ed8e8f496c8f61
Author: Filipe Manana <fdmanana@gmail.com>
Date:   Mon Jun 9 13:22:13 2014 +0100

    Btrfs: ensure btrfs_prev_leaf doesn't miss 1 item
    
    We might have had an item with the previous key in the tree right
    before we released our path. And after we released our path, that
    item might have been pushed to the first slot (0) of the leaf we
    were holding due to a tree balance. Alternatively, an item with the
    previous key can exist as the only element of a leaf (big fat item).
    Therefore account for these 2 cases, so that our callers (like
    btrfs_previous_item) don't miss an existing item with a key matching
    the previous key we computed above.
    
    Signed-off-by: Filipe David Borba Manana <fdmanana@gmail.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index d99d9651dd58..4eada52f3969 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -5097,7 +5097,17 @@ int btrfs_prev_leaf(struct btrfs_root *root, struct btrfs_path *path)
 		return ret;
 	btrfs_item_key(path->nodes[0], &found_key, 0);
 	ret = comp_keys(&found_key, &key);
-	if (ret < 0)
+	/*
+	 * We might have had an item with the previous key in the tree right
+	 * before we released our path. And after we released our path, that
+	 * item might have been pushed to the first slot (0) of the leaf we
+	 * were holding due to a tree balance. Alternatively, an item with the
+	 * previous key can exist as the only element of a leaf (big fat item).
+	 * Therefore account for these 2 cases, so that our callers (like
+	 * btrfs_previous_item) don't miss an existing item with a key matching
+	 * the previous key we computed above.
+	 */
+	if (ret <= 0)
 		return 0;
 	return 1;
 }

commit faa2dbf004e89e8f7ccd28fbe6f07c308417b8ae
Author: Josef Bacik <jbacik@fb.com>
Date:   Wed May 7 17:06:09 2014 -0400

    Btrfs: add sanity tests for new qgroup accounting code
    
    This exercises the various parts of the new qgroup accounting code.  We do some
    basic stuff and do some things with the shared refs to make sure all that code
    works.  I had to add a bunch of infrastructure because I needed to be able to
    insert items into a fake tree without having to do all the hard work myself,
    hopefully this will be usefull in the future.  Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index bbbe4f1c5086..d99d9651dd58 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1506,6 +1506,10 @@ static inline int should_cow_block(struct btrfs_trans_handle *trans,
 				   struct btrfs_root *root,
 				   struct extent_buffer *buf)
 {
+#ifdef CONFIG_BTRFS_FS_RUN_SANITY_TESTS
+	if (unlikely(test_bit(BTRFS_ROOT_DUMMY_ROOT, &root->state)))
+		return 0;
+#endif
 	/* ensure we can see the force_cow */
 	smp_rmb();
 

commit fcebe4562dec83b3f8d3088d77584727b09130b2
Author: Josef Bacik <jbacik@fb.com>
Date:   Tue May 13 17:30:47 2014 -0700

    Btrfs: rework qgroup accounting
    
    Currently qgroups account for space by intercepting delayed ref updates to fs
    trees.  It does this by adding sequence numbers to delayed ref updates so that
    it can figure out how the tree looked before the update so we can adjust the
    counters properly.  The problem with this is that it does not allow delayed refs
    to be merged, so if you say are defragging an extent with 5k snapshots pointing
    to it we will thrash the delayed ref lock because we need to go back and
    manually merge these things together.  Instead we want to process quota changes
    when we know they are going to happen, like when we first allocate an extent, we
    free a reference for an extent, we add new references etc.  This patch
    accomplishes this by only adding qgroup operations for real ref changes.  We
    only modify the sequence number when we need to lookup roots for bytenrs, this
    reduces the amount of churn on the sequence number and allows us to merge
    delayed refs as we add them most of the time.  This patch encompasses a bunch of
    architectural changes
    
    1) qgroup ref operations: instead of tracking qgroup operations through the
    delayed refs we simply add new ref operations whenever we notice that we need to
    when we've modified the refs themselves.
    
    2) tree mod seq:  we no longer have this separation of major/minor counters.
    this makes the sequence number stuff much more sane and we can remove some
    locking that was needed to protect the counter.
    
    3) delayed ref seq: we now read the tree mod seq number and use that as our
    sequence.  This means each new delayed ref doesn't have it's own unique sequence
    number, rather whenever we go to lookup backrefs we inc the sequence number so
    we can make sure to keep any new operations from screwing up our world view at
    that given point.  This allows us to merge delayed refs during runtime.
    
    With all of these changes the delayed ref stuff is a little saner and the qgroup
    accounting stuff no longer goes negative in some cases like it was before.
    Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 2f10e12ae94c..bbbe4f1c5086 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -356,43 +356,13 @@ static inline void tree_mod_log_write_unlock(struct btrfs_fs_info *fs_info)
 }
 
 /*
- * Increment the upper half of tree_mod_seq, set lower half zero.
- *
- * Must be called with fs_info->tree_mod_seq_lock held.
- */
-static inline u64 btrfs_inc_tree_mod_seq_major(struct btrfs_fs_info *fs_info)
-{
-	u64 seq = atomic64_read(&fs_info->tree_mod_seq);
-	seq &= 0xffffffff00000000ull;
-	seq += 1ull << 32;
-	atomic64_set(&fs_info->tree_mod_seq, seq);
-	return seq;
-}
-
-/*
- * Increment the lower half of tree_mod_seq.
- *
- * Must be called with fs_info->tree_mod_seq_lock held. The way major numbers
- * are generated should not technically require a spin lock here. (Rationale:
- * incrementing the minor while incrementing the major seq number is between its
- * atomic64_read and atomic64_set calls doesn't duplicate sequence numbers, it
- * just returns a unique sequence number as usual.) We have decided to leave
- * that requirement in here and rethink it once we notice it really imposes a
- * problem on some workload.
+ * Pull a new tree mod seq number for our operation.
  */
-static inline u64 btrfs_inc_tree_mod_seq_minor(struct btrfs_fs_info *fs_info)
+static inline u64 btrfs_inc_tree_mod_seq(struct btrfs_fs_info *fs_info)
 {
 	return atomic64_inc_return(&fs_info->tree_mod_seq);
 }
 
-/*
- * return the last minor in the previous major tree_mod_seq number
- */
-u64 btrfs_tree_mod_seq_prev(u64 seq)
-{
-	return (seq & 0xffffffff00000000ull) - 1ull;
-}
-
 /*
  * This adds a new blocker to the tree mod log's blocker list if the @elem
  * passed does not already have a sequence number set. So when a caller expects
@@ -404,19 +374,16 @@ u64 btrfs_tree_mod_seq_prev(u64 seq)
 u64 btrfs_get_tree_mod_seq(struct btrfs_fs_info *fs_info,
 			   struct seq_list *elem)
 {
-	u64 seq;
-
 	tree_mod_log_write_lock(fs_info);
 	spin_lock(&fs_info->tree_mod_seq_lock);
 	if (!elem->seq) {
-		elem->seq = btrfs_inc_tree_mod_seq_major(fs_info);
+		elem->seq = btrfs_inc_tree_mod_seq(fs_info);
 		list_add_tail(&elem->list, &fs_info->tree_mod_seq_list);
 	}
-	seq = btrfs_inc_tree_mod_seq_minor(fs_info);
 	spin_unlock(&fs_info->tree_mod_seq_lock);
 	tree_mod_log_write_unlock(fs_info);
 
-	return seq;
+	return elem->seq;
 }
 
 void btrfs_put_tree_mod_seq(struct btrfs_fs_info *fs_info,
@@ -489,9 +456,7 @@ __tree_mod_log_insert(struct btrfs_fs_info *fs_info, struct tree_mod_elem *tm)
 
 	BUG_ON(!tm);
 
-	spin_lock(&fs_info->tree_mod_seq_lock);
-	tm->seq = btrfs_inc_tree_mod_seq_minor(fs_info);
-	spin_unlock(&fs_info->tree_mod_seq_lock);
+	tm->seq = btrfs_inc_tree_mod_seq(fs_info);
 
 	tm_root = &fs_info->tree_mod_log;
 	new = &tm_root->rb_node;

commit 27cdeb7096b86f05ad018a24cdb63acdf0850a5d
Author: Miao Xie <miaox@cn.fujitsu.com>
Date:   Wed Apr 2 19:51:05 2014 +0800

    Btrfs: use bitfield instead of integer data type for the some variants in btrfs_root
    
    Signed-off-by: Miao Xie <miaox@cn.fujitsu.com>
    Signed-off-by: Wang Shilong <wangsl.fnst@cn.fujitsu.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 1bcfcdb23cf4..2f10e12ae94c 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -224,7 +224,8 @@ static struct extent_buffer *btrfs_read_lock_root_node(struct btrfs_root *root)
 static void add_root_to_dirty_list(struct btrfs_root *root)
 {
 	spin_lock(&root->fs_info->trans_lock);
-	if (root->track_dirty && list_empty(&root->dirty_list)) {
+	if (test_bit(BTRFS_ROOT_TRACK_DIRTY, &root->state) &&
+	    list_empty(&root->dirty_list)) {
 		list_add(&root->dirty_list,
 			 &root->fs_info->dirty_cowonly_roots);
 	}
@@ -246,9 +247,10 @@ int btrfs_copy_root(struct btrfs_trans_handle *trans,
 	int level;
 	struct btrfs_disk_key disk_key;
 
-	WARN_ON(root->ref_cows && trans->transid !=
-		root->fs_info->running_transaction->transid);
-	WARN_ON(root->ref_cows && trans->transid != root->last_trans);
+	WARN_ON(test_bit(BTRFS_ROOT_REF_COWS, &root->state) &&
+		trans->transid != root->fs_info->running_transaction->transid);
+	WARN_ON(test_bit(BTRFS_ROOT_REF_COWS, &root->state) &&
+		trans->transid != root->last_trans);
 
 	level = btrfs_header_level(buf);
 	if (level == 0)
@@ -997,14 +999,14 @@ int btrfs_block_can_be_shared(struct btrfs_root *root,
 	 * snapshot and the block was not allocated by tree relocation,
 	 * we know the block is not shared.
 	 */
-	if (root->ref_cows &&
+	if (test_bit(BTRFS_ROOT_REF_COWS, &root->state) &&
 	    buf != root->node && buf != root->commit_root &&
 	    (btrfs_header_generation(buf) <=
 	     btrfs_root_last_snapshot(&root->root_item) ||
 	     btrfs_header_flag(buf, BTRFS_HEADER_FLAG_RELOC)))
 		return 1;
 #ifdef BTRFS_COMPAT_EXTENT_TREE_V0
-	if (root->ref_cows &&
+	if (test_bit(BTRFS_ROOT_REF_COWS, &root->state) &&
 	    btrfs_header_backref_rev(buf) < BTRFS_MIXED_BACKREF_REV)
 		return 1;
 #endif
@@ -1146,9 +1148,10 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 
 	btrfs_assert_tree_locked(buf);
 
-	WARN_ON(root->ref_cows && trans->transid !=
-		root->fs_info->running_transaction->transid);
-	WARN_ON(root->ref_cows && trans->transid != root->last_trans);
+	WARN_ON(test_bit(BTRFS_ROOT_REF_COWS, &root->state) &&
+		trans->transid != root->fs_info->running_transaction->transid);
+	WARN_ON(test_bit(BTRFS_ROOT_REF_COWS, &root->state) &&
+		trans->transid != root->last_trans);
 
 	level = btrfs_header_level(buf);
 
@@ -1193,7 +1196,7 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 		return ret;
 	}
 
-	if (root->ref_cows) {
+	if (test_bit(BTRFS_ROOT_REF_COWS, &root->state)) {
 		ret = btrfs_reloc_cow_block(trans, root, buf, cow);
 		if (ret)
 			return ret;
@@ -1556,7 +1559,7 @@ static inline int should_cow_block(struct btrfs_trans_handle *trans,
 	    !btrfs_header_flag(buf, BTRFS_HEADER_FLAG_WRITTEN) &&
 	    !(root->root_key.objectid != BTRFS_TREE_RELOC_OBJECTID &&
 	      btrfs_header_flag(buf, BTRFS_HEADER_FLAG_RELOC)) &&
-	    !root->force_cow)
+	    !test_bit(BTRFS_ROOT_FORCE_COW, &root->state))
 		return 0;
 	return 1;
 }

commit 3f8a18cc53bd1be26eb5b5247e1386ad0e21b623
Author: Josef Bacik <jbacik@fb.com>
Date:   Fri Mar 28 17:16:01 2014 -0400

    Btrfs: hold the commit_root_sem when getting the commit root during send
    
    We currently rely too heavily on roots being read-only to save us from just
    accessing root->commit_root.  We can easily balance blocks out from underneath a
    read only root, so to save us from getting screwed make sure we only access
    root->commit_root under the commit root sem.  Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 9d89c1648e8e..1bcfcdb23cf4 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2769,9 +2769,13 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 		 * the commit roots are read only
 		 * so we always do read locks
 		 */
+		if (p->need_commit_sem)
+			down_read(&root->fs_info->commit_root_sem);
 		b = root->commit_root;
 		extent_buffer_get(b);
 		level = btrfs_header_level(b);
+		if (p->need_commit_sem)
+			up_read(&root->fs_info->commit_root_sem);
 		if (!p->skip_locking)
 			btrfs_tree_read_lock(b);
 	} else {
@@ -5436,6 +5440,7 @@ int btrfs_compare_trees(struct btrfs_root *left_root,
 	 *   the right if possible or go up and right.
 	 */
 
+	down_read(&left_root->fs_info->commit_root_sem);
 	left_level = btrfs_header_level(left_root->commit_root);
 	left_root_level = left_level;
 	left_path->nodes[left_level] = left_root->commit_root;
@@ -5445,6 +5450,7 @@ int btrfs_compare_trees(struct btrfs_root *left_root,
 	right_root_level = right_level;
 	right_path->nodes[right_level] = right_root->commit_root;
 	extent_buffer_get(right_path->nodes[right_level]);
+	up_read(&left_root->fs_info->commit_root_sem);
 
 	if (left_level == 0)
 		btrfs_item_key_to_cpu(left_path->nodes[left_level],

commit 9e351cc862b098d8ec8f8022d110932490794925
Author: Josef Bacik <jbacik@fb.com>
Date:   Thu Mar 13 15:42:13 2014 -0400

    Btrfs: remove transaction from send
    
    Lets try this again.  We can deadlock the box if we send on a box and try to
    write onto the same fs with the app that is trying to listen to the send pipe.
    This is because the writer could get stuck waiting for a transaction commit
    which is being blocked by the send.  So fix this by making sure looking at the
    commit roots is always going to be consistent.  We do this by keeping track of
    which roots need to have their commit roots swapped during commit, and then
    taking the commit_root_sem and swapping them all at once.  Then make sure we
    take a read lock on the commit_root_sem in cases where we search the commit root
    to make sure we're always looking at a consistent view of the commit roots.
    Previously we had problems with this because we would swap a fs tree commit root
    and then swap the extent tree commit root independently which would cause the
    backref walking code to screw up sometimes.  With this patch we no longer
    deadlock and pass all the weird send/receive corner cases.  Thanks,
    
    Reportedy-by: Hugo Mills <hugo@carfax.org.uk>
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 88d1b1eedc9c..9d89c1648e8e 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -5360,7 +5360,6 @@ int btrfs_compare_trees(struct btrfs_root *left_root,
 {
 	int ret;
 	int cmp;
-	struct btrfs_trans_handle *trans = NULL;
 	struct btrfs_path *left_path = NULL;
 	struct btrfs_path *right_path = NULL;
 	struct btrfs_key left_key;
@@ -5378,9 +5377,6 @@ int btrfs_compare_trees(struct btrfs_root *left_root,
 	u64 right_blockptr;
 	u64 left_gen;
 	u64 right_gen;
-	u64 left_start_ctransid;
-	u64 right_start_ctransid;
-	u64 ctransid;
 
 	left_path = btrfs_alloc_path();
 	if (!left_path) {
@@ -5404,21 +5400,6 @@ int btrfs_compare_trees(struct btrfs_root *left_root,
 	right_path->search_commit_root = 1;
 	right_path->skip_locking = 1;
 
-	spin_lock(&left_root->root_item_lock);
-	left_start_ctransid = btrfs_root_ctransid(&left_root->root_item);
-	spin_unlock(&left_root->root_item_lock);
-
-	spin_lock(&right_root->root_item_lock);
-	right_start_ctransid = btrfs_root_ctransid(&right_root->root_item);
-	spin_unlock(&right_root->root_item_lock);
-
-	trans = btrfs_join_transaction(left_root);
-	if (IS_ERR(trans)) {
-		ret = PTR_ERR(trans);
-		trans = NULL;
-		goto out;
-	}
-
 	/*
 	 * Strategy: Go to the first items of both trees. Then do
 	 *
@@ -5482,67 +5463,6 @@ int btrfs_compare_trees(struct btrfs_root *left_root,
 	advance_left = advance_right = 0;
 
 	while (1) {
-		/*
-		 * We need to make sure the transaction does not get committed
-		 * while we do anything on commit roots. This means, we need to
-		 * join and leave transactions for every item that we process.
-		 */
-		if (trans && btrfs_should_end_transaction(trans, left_root)) {
-			btrfs_release_path(left_path);
-			btrfs_release_path(right_path);
-
-			ret = btrfs_end_transaction(trans, left_root);
-			trans = NULL;
-			if (ret < 0)
-				goto out;
-		}
-		/* now rejoin the transaction */
-		if (!trans) {
-			trans = btrfs_join_transaction(left_root);
-			if (IS_ERR(trans)) {
-				ret = PTR_ERR(trans);
-				trans = NULL;
-				goto out;
-			}
-
-			spin_lock(&left_root->root_item_lock);
-			ctransid = btrfs_root_ctransid(&left_root->root_item);
-			spin_unlock(&left_root->root_item_lock);
-			if (ctransid != left_start_ctransid)
-				left_start_ctransid = 0;
-
-			spin_lock(&right_root->root_item_lock);
-			ctransid = btrfs_root_ctransid(&right_root->root_item);
-			spin_unlock(&right_root->root_item_lock);
-			if (ctransid != right_start_ctransid)
-				right_start_ctransid = 0;
-
-			if (!left_start_ctransid || !right_start_ctransid) {
-				WARN(1, KERN_WARNING
-					"BTRFS: btrfs_compare_tree detected "
-					"a change in one of the trees while "
-					"iterating. This is probably a "
-					"bug.\n");
-				ret = -EIO;
-				goto out;
-			}
-
-			/*
-			 * the commit root may have changed, so start again
-			 * where we stopped
-			 */
-			left_path->lowest_level = left_level;
-			right_path->lowest_level = right_level;
-			ret = btrfs_search_slot(NULL, left_root,
-					&left_key, left_path, 0, 0);
-			if (ret < 0)
-				goto out;
-			ret = btrfs_search_slot(NULL, right_root,
-					&right_key, right_path, 0, 0);
-			if (ret < 0)
-				goto out;
-		}
-
 		if (advance_left && !left_end_reached) {
 			ret = tree_advance(left_root, left_path, &left_level,
 					left_root_level,
@@ -5672,14 +5592,6 @@ int btrfs_compare_trees(struct btrfs_root *left_root,
 	btrfs_free_path(left_path);
 	btrfs_free_path(right_path);
 	kfree(tmp_buf);
-
-	if (trans) {
-		if (!ret)
-			ret = btrfs_end_transaction(trans, left_root);
-		else
-			btrfs_end_transaction(trans, left_root);
-	}
-
 	return ret;
 }
 

commit 6baa4293af8abe95018e911c3df60ed5bfacc76f
Author: Filipe Manana <fdmanana@gmail.com>
Date:   Thu Feb 20 21:15:25 2014 +0000

    Btrfs: correctly determine if blocks are shared in btrfs_compare_trees
    
    Just comparing the pointers (logical disk addresses) of the btree nodes is
    not completely bullet proof, we have to check if their generation numbers
    match too.
    
    It is guaranteed that a COW operation will result in a block with a different
    logical disk address than the original block's address, but over time we can
    reuse that former logical disk address.
    
    For example, creating a 2Gb filesystem on a loop device, and having a script
    running in a loop always updating the access timestamp of a file, resulted in
    the same logical disk address being reused for the same fs btree block in about
    only 4 minutes.
    
    This could make us skip entire subtrees when doing an incremental send (which
    is currently the only user of btrfs_compare_trees). However the odds of getting
    2 blocks at the same tree level, with the same logical disk address, equal first
    slot keys and different generations, should hopefully be very low.
    
    Signed-off-by: Filipe David Borba Manana <fdmanana@gmail.com>
    Signed-off-by: Josef Bacik <jbacik@fb.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index cbd3a7d6fa68..88d1b1eedc9c 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -5376,6 +5376,8 @@ int btrfs_compare_trees(struct btrfs_root *left_root,
 	int advance_right;
 	u64 left_blockptr;
 	u64 right_blockptr;
+	u64 left_gen;
+	u64 right_gen;
 	u64 left_start_ctransid;
 	u64 right_start_ctransid;
 	u64 ctransid;
@@ -5640,7 +5642,14 @@ int btrfs_compare_trees(struct btrfs_root *left_root,
 				right_blockptr = btrfs_node_blockptr(
 						right_path->nodes[right_level],
 						right_path->slots[right_level]);
-				if (left_blockptr == right_blockptr) {
+				left_gen = btrfs_node_ptr_generation(
+						left_path->nodes[left_level],
+						left_path->slots[left_level]);
+				right_gen = btrfs_node_ptr_generation(
+						right_path->nodes[right_level],
+						right_path->slots[right_level]);
+				if (left_blockptr == right_blockptr &&
+				    left_gen == right_gen) {
 					/*
 					 * As we're on a shared block, don't
 					 * allow to go deeper.

commit 23c6bf6a91e96c17a452e07b12b38ed66504e799
Author: Filipe David Borba Manana <fdmanana@gmail.com>
Date:   Sat Jan 11 21:28:54 2014 +0000

    Btrfs: fix btrfs_search_slot_for_read backwards iteration
    
    If the current path's leaf slot is 0, we do search for the previous
    leaf (via btrfs_prev_leaf) and set the new path's leaf slot to a
    value corresponding to the number of items - 1 of the former leaf.
    Fix this by using the slot set by btrfs_prev_leaf, decrementing it
    by 1 if it's equal to the leaf's number of items.
    
    Use of btrfs_search_slot_for_read() for backward iteration is used in
    particular by the send feature, which could miss items when the input
    leaf has less items than its previous leaf.
    
    This could be reproduced by running btrfs/007 from xfstests in a loop.
    
    Signed-off-by: Filipe David Borba Manana <fdmanana@gmail.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 30f5b11d7dd3..cbd3a7d6fa68 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -3142,7 +3142,9 @@ int btrfs_search_slot_for_read(struct btrfs_root *root,
 			if (ret < 0)
 				return ret;
 			if (!ret) {
-				p->slots[0] = btrfs_header_nritems(leaf) - 1;
+				leaf = p->nodes[0];
+				if (p->slots[0] == btrfs_header_nritems(leaf))
+					p->slots[0]--;
 				return 0;
 			}
 			if (!return_any)

commit ade2e0b3eeca941a5cd486bac21599ff87f288c8
Author: Wang Shilong <wangsl.fnst@cn.fujitsu.com>
Date:   Sun Jan 12 21:38:33 2014 +0800

    Btrfs: fix to search previous metadata extent item since skinny metadata
    
    There is a bug that using btrfs_previous_item() to search metadata extent item.
    This is because in btrfs_previous_item(), we need type match, however, since
    skinny metada was introduced by josef, we may mix this two types. So just
    use btrfs_previous_item() is not working right.
    
    To keep btrfs_previous_item() like normal tree search, i introduce another
    function btrfs_previous_extent_item().
    
    Signed-off-by: Wang Shilong <wangsl.fnst@cn.fujitsu.com>
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 9e9de68eb813..30f5b11d7dd3 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -5955,3 +5955,46 @@ int btrfs_previous_item(struct btrfs_root *root,
 	}
 	return 1;
 }
+
+/*
+ * search in extent tree to find a previous Metadata/Data extent item with
+ * min objecitd.
+ *
+ * returns 0 if something is found, 1 if nothing was found and < 0 on error
+ */
+int btrfs_previous_extent_item(struct btrfs_root *root,
+			struct btrfs_path *path, u64 min_objectid)
+{
+	struct btrfs_key found_key;
+	struct extent_buffer *leaf;
+	u32 nritems;
+	int ret;
+
+	while (1) {
+		if (path->slots[0] == 0) {
+			btrfs_set_path_blocking(path);
+			ret = btrfs_prev_leaf(root, path);
+			if (ret != 0)
+				return ret;
+		} else {
+			path->slots[0]--;
+		}
+		leaf = path->nodes[0];
+		nritems = btrfs_header_nritems(leaf);
+		if (nritems == 0)
+			return 1;
+		if (path->slots[0] == nritems)
+			path->slots[0]--;
+
+		btrfs_item_key_to_cpu(leaf, &found_key, path->slots[0]);
+		if (found_key.objectid < min_objectid)
+			break;
+		if (found_key.type == BTRFS_EXTENT_ITEM_KEY ||
+		    found_key.type == BTRFS_METADATA_ITEM_KEY)
+			return 0;
+		if (found_key.objectid == min_objectid &&
+		    found_key.type < BTRFS_EXTENT_ITEM_KEY)
+			break;
+	}
+	return 1;
+}

commit eb653de15987612444b6cde3b0e67b1edd94625f
Author: Filipe David Borba Manana <fdmanana@gmail.com>
Date:   Mon Dec 23 11:53:02 2013 +0000

    Btrfs: reduce btree node locking duration on item update
    
    If we do a btree search with the goal of updating an existing item
    without changing its size (ins_len == 0 and cow == 1), then we never
    need to hold locks on upper level nodes (even when slot == 0) after we
    COW their child nodes/leaves, as we won't have node splits or merges
    in this scenario (that is, no key additions, removals or shifts on any
    nodes or leaves).
    
    Therefore release the locks immediately after COWing the child nodes/leaves
    while navigating the btree, even if their parent slot is 0, instead of
    returning a path to the caller with those nodes locked, which would get
    released only when the caller releases or frees the path (or if it calls
    btrfs_unlock_up_safe).
    
    This is a common scenario, for example when updating inode items in fs
    trees and block group items in the extent tree.
    
    The following benchmarks were performed on a quad core machine with 32Gb
    of ram, using a leaf/node size of 4Kb (to generate deeper fs trees more
    quickly).
    
      sysbench --test=fileio --file-num=131072 --file-total-size=8G \
        --file-test-mode=seqwr --num-threads=512 --file-block-size=8192 \
        --max-requests=100000 --file-io-mode=sync [prepare|run]
    
    Before this change:  49.85Mb/s (average of 5 runs)
    After this change:   50.38Mb/s (average of 5 runs)
    
    Signed-off-by: Filipe David Borba Manana <fdmanana@gmail.com>
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 062438d38985..9e9de68eb813 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2731,6 +2731,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 	lowest_level = p->lowest_level;
 	WARN_ON(lowest_level && ins_len > 0);
 	WARN_ON(p->nodes[0] != NULL);
+	BUG_ON(!cow && ins_len);
 
 	if (ins_len < 0) {
 		lowest_unlock = 2;
@@ -2839,8 +2840,6 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 			}
 		}
 cow_done:
-		BUG_ON(!cow && ins_len);
-
 		p->nodes[level] = b;
 		btrfs_clear_path_blocking(p, NULL, 0);
 
@@ -2850,13 +2849,19 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 		 * It is safe to drop the lock on our parent before we
 		 * go through the expensive btree search on b.
 		 *
-		 * If cow is true, then we might be changing slot zero,
-		 * which may require changing the parent.  So, we can't
-		 * drop the lock until after we know which slot we're
-		 * operating on.
+		 * If we're inserting or deleting (ins_len != 0), then we might
+		 * be changing slot zero, which may require changing the parent.
+		 * So, we can't drop the lock until after we know which slot
+		 * we're operating on.
 		 */
-		if (!cow)
-			btrfs_unlock_up_safe(p, level + 1);
+		if (!ins_len && !p->keep_locks) {
+			int u = level + 1;
+
+			if (u < BTRFS_MAX_LEVEL && p->locks[u]) {
+				btrfs_tree_unlock_rw(p->nodes[u], p->locks[u]);
+				p->locks[u] = 0;
+			}
+		}
 
 		ret = key_search(b, key, level, &prev_cmp, &slot);
 
@@ -2884,7 +2889,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 			 * which means we must have a write lock
 			 * on the parent
 			 */
-			if (slot == 0 && cow &&
+			if (slot == 0 && ins_len &&
 			    write_lock_level < level + 1) {
 				write_lock_level = level + 1;
 				btrfs_release_path(p);

commit efe120a067c8674a8ae21b194f0e68f098b61ee2
Author: Frank Holton <fholton@gmail.com>
Date:   Fri Dec 20 11:37:06 2013 -0500

    Btrfs: convert printk to btrfs_ and fix BTRFS prefix
    
    Convert all applicable cases of printk and pr_* to the btrfs_* macros.
    
    Fix all uses of the BTRFS prefix.
    
    Signed-off-by: Frank Holton <fholton@gmail.com>
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 7d88d8543aa1..062438d38985 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1480,8 +1480,8 @@ get_old_root(struct btrfs_root *root, u64 time_seq)
 		old = read_tree_block(root, logical, blocksize, 0);
 		if (WARN_ON(!old || !extent_buffer_uptodate(old))) {
 			free_extent_buffer(old);
-			pr_warn("btrfs: failed to read tree block %llu from get_old_root\n",
-				logical);
+			btrfs_warn(root->fs_info,
+				"failed to read tree block %llu from get_old_root", logical);
 		} else {
 			eb = btrfs_clone_extent_buffer(old);
 			free_extent_buffer(old);
@@ -3611,8 +3611,8 @@ noinline int btrfs_leaf_free_space(struct btrfs_root *root,
 	int ret;
 	ret = BTRFS_LEAF_DATA_SIZE(root) - leaf_space_used(leaf, 0, nritems);
 	if (ret < 0) {
-		printk(KERN_CRIT "leaf free space ret %d, leaf data size %lu, "
-		       "used %d nritems %d\n",
+		btrfs_crit(root->fs_info,
+			"leaf free space ret %d, leaf data size %lu, used %d nritems %d",
 		       ret, (unsigned long) BTRFS_LEAF_DATA_SIZE(root),
 		       leaf_space_used(leaf, 0, nritems), nritems);
 	}
@@ -4702,7 +4702,7 @@ void btrfs_extend_item(struct btrfs_root *root, struct btrfs_path *path,
 	BUG_ON(slot < 0);
 	if (slot >= nritems) {
 		btrfs_print_leaf(root, leaf);
-		printk(KERN_CRIT "slot %d too large, nritems %d\n",
+		btrfs_crit(root->fs_info, "slot %d too large, nritems %d",
 		       slot, nritems);
 		BUG_ON(1);
 	}
@@ -4765,7 +4765,7 @@ void setup_items_for_insert(struct btrfs_root *root, struct btrfs_path *path,
 
 	if (btrfs_leaf_free_space(root, leaf) < total_size) {
 		btrfs_print_leaf(root, leaf);
-		printk(KERN_CRIT "not enough freespace need %u have %d\n",
+		btrfs_crit(root->fs_info, "not enough freespace need %u have %d",
 		       total_size, btrfs_leaf_free_space(root, leaf));
 		BUG();
 	}
@@ -4775,7 +4775,7 @@ void setup_items_for_insert(struct btrfs_root *root, struct btrfs_path *path,
 
 		if (old_data < data_end) {
 			btrfs_print_leaf(root, leaf);
-			printk(KERN_CRIT "slot %d old_data %d data_end %d\n",
+			btrfs_crit(root->fs_info, "slot %d old_data %d data_end %d",
 			       slot, old_data, data_end);
 			BUG_ON(1);
 		}
@@ -5510,7 +5510,7 @@ int btrfs_compare_trees(struct btrfs_root *left_root,
 
 			if (!left_start_ctransid || !right_start_ctransid) {
 				WARN(1, KERN_WARNING
-					"btrfs: btrfs_compare_tree detected "
+					"BTRFS: btrfs_compare_tree detected "
 					"a change in one of the trees while "
 					"iterating. This is probably a "
 					"bug.\n");

commit 5de865eebb8330eee19c37b31fb6f315a09d4273
Author: Filipe David Borba Manana <fdmanana@gmail.com>
Date:   Fri Dec 20 15:17:46 2013 +0000

    Btrfs: fix tree mod logging
    
    While running the test btrfs/004 from xfstests in a loop, it failed
    about 1 time out of 20 runs in my desktop. The failure happened in
    the backref walking part of the test, and the test's error message was
    like this:
    
      btrfs/004 93s ... [failed, exit status 1] - output mismatch (see /home/fdmanana/git/hub/xfstests_2/results//btrfs/004.out.bad)
          --- tests/btrfs/004.out   2013-11-26 18:25:29.263333714 +0000
          +++ /home/fdmanana/git/hub/xfstests_2/results//btrfs/004.out.bad  2013-12-10 15:25:10.327518516 +0000
          @@ -1,3 +1,8 @@
           QA output created by 004
           *** test backref walking
          -*** done
          +unexpected output from
          + /home/fdmanana/git/hub/btrfs-progs/btrfs inspect-internal logical-resolve -P 141512704 /home/fdmanana/btrfs-tests/scratch_1
          +expected inum: 405, expected address: 454656, file: /home/fdmanana/btrfs-tests/scratch_1/snap1/p0/d6/d3d/d156/fce, got:
          +
           ...
           (Run 'diff -u tests/btrfs/004.out /home/fdmanana/git/hub/xfstests_2/results//btrfs/004.out.bad' to see the entire diff)
      Ran: btrfs/004
      Failures: btrfs/004
      Failed 1 of 1 tests
    
    But immediately after the test finished, the btrfs inspect-internal command
    returned the expected output:
    
      $ btrfs inspect-internal logical-resolve -P 141512704 /home/fdmanana/btrfs-tests/scratch_1
      inode 405 offset 454656 root 258
      inode 405 offset 454656 root 5
    
    It turned out this was because the btrfs_search_old_slot() calls performed
    during backref walking (backref.c:__resolve_indirect_ref) were not finding
    anything. The reason for this turned out to be that the tree mod logging
    code was not logging some node multi-step operations atomically, therefore
    btrfs_search_old_slot() callers iterated often over an incomplete tree that
    wasn't fully consistent with any tree state from the past. Besides missing
    items, this often (but not always) resulted in -EIO errors during old slot
    searches, reported in dmesg like this:
    
    [ 4299.933936] ------------[ cut here ]------------
    [ 4299.933949] WARNING: CPU: 0 PID: 23190 at fs/btrfs/ctree.c:1343 btrfs_search_old_slot+0x57b/0xab0 [btrfs]()
    [ 4299.933950] Modules linked in: btrfs raid6_pq xor pci_stub vboxpci(O) vboxnetadp(O) vboxnetflt(O) vboxdrv(O) bnep rfcomm bluetooth parport_pc ppdev binfmt_misc joydev snd_hda_codec_h
    [ 4299.933977] CPU: 0 PID: 23190 Comm: btrfs Tainted: G        W  O 3.12.0-fdm-btrfs-next-16+ #70
    [ 4299.933978] Hardware name: To Be Filled By O.E.M. To Be Filled By O.E.M./Z77 Pro4, BIOS P1.50 09/04/2012
    [ 4299.933979]  000000000000053f ffff8806f3fd98f8 ffffffff8176d284 0000000000000007
    [ 4299.933982]  0000000000000000 ffff8806f3fd9938 ffffffff8104a81c ffff880659c64b70
    [ 4299.933984]  ffff880659c643d0 ffff8806599233d8 ffff880701e2e938 0000160000000000
    [ 4299.933987] Call Trace:
    [ 4299.933991]  [<ffffffff8176d284>] dump_stack+0x55/0x76
    [ 4299.933994]  [<ffffffff8104a81c>] warn_slowpath_common+0x8c/0xc0
    [ 4299.933997]  [<ffffffff8104a86a>] warn_slowpath_null+0x1a/0x20
    [ 4299.934003]  [<ffffffffa065d3bb>] btrfs_search_old_slot+0x57b/0xab0 [btrfs]
    [ 4299.934005]  [<ffffffff81775f3b>] ? _raw_read_unlock+0x2b/0x50
    [ 4299.934010]  [<ffffffffa0655001>] ? __tree_mod_log_search+0x81/0xc0 [btrfs]
    [ 4299.934019]  [<ffffffffa06dd9b0>] __resolve_indirect_refs+0x130/0x5f0 [btrfs]
    [ 4299.934027]  [<ffffffffa06a21f1>] ? free_extent_buffer+0x61/0xc0 [btrfs]
    [ 4299.934034]  [<ffffffffa06de39c>] find_parent_nodes+0x1fc/0xe40 [btrfs]
    [ 4299.934042]  [<ffffffffa06b13e0>] ? defrag_lookup_extent+0xe0/0xe0 [btrfs]
    [ 4299.934048]  [<ffffffffa06b13e0>] ? defrag_lookup_extent+0xe0/0xe0 [btrfs]
    [ 4299.934056]  [<ffffffffa06df980>] iterate_extent_inodes+0xe0/0x250 [btrfs]
    [ 4299.934058]  [<ffffffff817762db>] ? _raw_spin_unlock+0x2b/0x50
    [ 4299.934065]  [<ffffffffa06dfb82>] iterate_inodes_from_logical+0x92/0xb0 [btrfs]
    [ 4299.934071]  [<ffffffffa06b13e0>] ? defrag_lookup_extent+0xe0/0xe0 [btrfs]
    [ 4299.934078]  [<ffffffffa06b7015>] btrfs_ioctl+0xf65/0x1f60 [btrfs]
    [ 4299.934080]  [<ffffffff811658b8>] ? handle_mm_fault+0x278/0xb00
    [ 4299.934083]  [<ffffffff81075563>] ? up_read+0x23/0x40
    [ 4299.934085]  [<ffffffff8177a41c>] ? __do_page_fault+0x20c/0x5a0
    [ 4299.934088]  [<ffffffff811b2946>] do_vfs_ioctl+0x96/0x570
    [ 4299.934090]  [<ffffffff81776e23>] ? error_sti+0x5/0x6
    [ 4299.934093]  [<ffffffff810b71e8>] ? trace_hardirqs_off_caller+0x28/0xd0
    [ 4299.934096]  [<ffffffff81776a09>] ? retint_swapgs+0xe/0x13
    [ 4299.934098]  [<ffffffff811b2eb1>] SyS_ioctl+0x91/0xb0
    [ 4299.934100]  [<ffffffff813eecde>] ? trace_hardirqs_on_thunk+0x3a/0x3f
    [ 4299.934102]  [<ffffffff8177ef12>] system_call_fastpath+0x16/0x1b
    [ 4299.934102]  [<ffffffff8177ef12>] system_call_fastpath+0x16/0x1b
    [ 4299.934104] ---[ end trace 48f0cfc902491414 ]---
    [ 4299.934378] btrfs bad fsid on block 0
    
    These tree mod log operations that must be performed atomically, tree_mod_log_free_eb,
    tree_mod_log_eb_copy, tree_mod_log_insert_root and tree_mod_log_insert_move, used to
    be performed atomically before the following commit:
    
      c8cc6341653721b54760480b0d0d9b5f09b46741
      (Btrfs: stop using GFP_ATOMIC for the tree mod log allocations)
    
    That change removed the atomicity of such operations. This patch restores the
    atomicity while still not doing the GFP_ATOMIC allocations of tree_mod_elem
    structures, so it has to do the allocations using GFP_NOFS before acquiring
    the mod log lock.
    
    This issue has been experienced by several users recently, such as for example:
    
      http://www.spinics.net/lists/linux-btrfs/msg28574.html
    
    After running the btrfs/004 test for 679 consecutive iterations with this
    patch applied, I didn't ran into the issue anymore.
    
    Cc: stable@vger.kernel.org
    Signed-off-by: Filipe David Borba Manana <fdmanana@gmail.com>
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 59664f6cbc4e..7d88d8543aa1 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -39,7 +39,7 @@ static int balance_node_right(struct btrfs_trans_handle *trans,
 			      struct extent_buffer *src_buf);
 static void del_ptr(struct btrfs_root *root, struct btrfs_path *path,
 		    int level, int slot);
-static void tree_mod_log_free_eb(struct btrfs_fs_info *fs_info,
+static int tree_mod_log_free_eb(struct btrfs_fs_info *fs_info,
 				 struct extent_buffer *eb);
 
 struct btrfs_path *btrfs_alloc_path(void)
@@ -474,6 +474,8 @@ void btrfs_put_tree_mod_seq(struct btrfs_fs_info *fs_info,
  * the index is the shifted logical of the *new* root node for root replace
  * operations, or the shifted logical of the affected block for all other
  * operations.
+ *
+ * Note: must be called with write lock (tree_mod_log_write_lock).
  */
 static noinline int
 __tree_mod_log_insert(struct btrfs_fs_info *fs_info, struct tree_mod_elem *tm)
@@ -482,24 +484,9 @@ __tree_mod_log_insert(struct btrfs_fs_info *fs_info, struct tree_mod_elem *tm)
 	struct rb_node **new;
 	struct rb_node *parent = NULL;
 	struct tree_mod_elem *cur;
-	int ret = 0;
 
 	BUG_ON(!tm);
 
-	tree_mod_log_write_lock(fs_info);
-	if (list_empty(&fs_info->tree_mod_seq_list)) {
-		tree_mod_log_write_unlock(fs_info);
-		/*
-		 * Ok we no longer care about logging modifications, free up tm
-		 * and return 0.  Any callers shouldn't be using tm after
-		 * calling tree_mod_log_insert, but if they do we can just
-		 * change this to return a special error code to let the callers
-		 * do their own thing.
-		 */
-		kfree(tm);
-		return 0;
-	}
-
 	spin_lock(&fs_info->tree_mod_seq_lock);
 	tm->seq = btrfs_inc_tree_mod_seq_minor(fs_info);
 	spin_unlock(&fs_info->tree_mod_seq_lock);
@@ -517,18 +504,13 @@ __tree_mod_log_insert(struct btrfs_fs_info *fs_info, struct tree_mod_elem *tm)
 			new = &((*new)->rb_left);
 		else if (cur->seq > tm->seq)
 			new = &((*new)->rb_right);
-		else {
-			ret = -EEXIST;
-			kfree(tm);
-			goto out;
-		}
+		else
+			return -EEXIST;
 	}
 
 	rb_link_node(&tm->node, parent, new);
 	rb_insert_color(&tm->node, tm_root);
-out:
-	tree_mod_log_write_unlock(fs_info);
-	return ret;
+	return 0;
 }
 
 /*
@@ -544,19 +526,38 @@ static inline int tree_mod_dont_log(struct btrfs_fs_info *fs_info,
 		return 1;
 	if (eb && btrfs_header_level(eb) == 0)
 		return 1;
+
+	tree_mod_log_write_lock(fs_info);
+	if (list_empty(&(fs_info)->tree_mod_seq_list)) {
+		tree_mod_log_write_unlock(fs_info);
+		return 1;
+	}
+
 	return 0;
 }
 
-static inline int
-__tree_mod_log_insert_key(struct btrfs_fs_info *fs_info,
-			  struct extent_buffer *eb, int slot,
-			  enum mod_log_op op, gfp_t flags)
+/* Similar to tree_mod_dont_log, but doesn't acquire any locks. */
+static inline int tree_mod_need_log(const struct btrfs_fs_info *fs_info,
+				    struct extent_buffer *eb)
+{
+	smp_mb();
+	if (list_empty(&(fs_info)->tree_mod_seq_list))
+		return 0;
+	if (eb && btrfs_header_level(eb) == 0)
+		return 0;
+
+	return 1;
+}
+
+static struct tree_mod_elem *
+alloc_tree_mod_elem(struct extent_buffer *eb, int slot,
+		    enum mod_log_op op, gfp_t flags)
 {
 	struct tree_mod_elem *tm;
 
 	tm = kzalloc(sizeof(*tm), flags);
 	if (!tm)
-		return -ENOMEM;
+		return NULL;
 
 	tm->index = eb->start >> PAGE_CACHE_SHIFT;
 	if (op != MOD_LOG_KEY_ADD) {
@@ -566,8 +567,9 @@ __tree_mod_log_insert_key(struct btrfs_fs_info *fs_info,
 	tm->op = op;
 	tm->slot = slot;
 	tm->generation = btrfs_node_ptr_generation(eb, slot);
+	RB_CLEAR_NODE(&tm->node);
 
-	return __tree_mod_log_insert(fs_info, tm);
+	return tm;
 }
 
 static noinline int
@@ -575,10 +577,27 @@ tree_mod_log_insert_key(struct btrfs_fs_info *fs_info,
 			struct extent_buffer *eb, int slot,
 			enum mod_log_op op, gfp_t flags)
 {
-	if (tree_mod_dont_log(fs_info, eb))
+	struct tree_mod_elem *tm;
+	int ret;
+
+	if (!tree_mod_need_log(fs_info, eb))
+		return 0;
+
+	tm = alloc_tree_mod_elem(eb, slot, op, flags);
+	if (!tm)
+		return -ENOMEM;
+
+	if (tree_mod_dont_log(fs_info, eb)) {
+		kfree(tm);
 		return 0;
+	}
+
+	ret = __tree_mod_log_insert(fs_info, tm);
+	tree_mod_log_write_unlock(fs_info);
+	if (ret)
+		kfree(tm);
 
-	return __tree_mod_log_insert_key(fs_info, eb, slot, op, flags);
+	return ret;
 }
 
 static noinline int
@@ -586,53 +605,95 @@ tree_mod_log_insert_move(struct btrfs_fs_info *fs_info,
 			 struct extent_buffer *eb, int dst_slot, int src_slot,
 			 int nr_items, gfp_t flags)
 {
-	struct tree_mod_elem *tm;
-	int ret;
+	struct tree_mod_elem *tm = NULL;
+	struct tree_mod_elem **tm_list = NULL;
+	int ret = 0;
 	int i;
+	int locked = 0;
 
-	if (tree_mod_dont_log(fs_info, eb))
+	if (!tree_mod_need_log(fs_info, eb))
 		return 0;
 
+	tm_list = kzalloc(nr_items * sizeof(struct tree_mod_elem *), flags);
+	if (!tm_list)
+		return -ENOMEM;
+
+	tm = kzalloc(sizeof(*tm), flags);
+	if (!tm) {
+		ret = -ENOMEM;
+		goto free_tms;
+	}
+
+	tm->index = eb->start >> PAGE_CACHE_SHIFT;
+	tm->slot = src_slot;
+	tm->move.dst_slot = dst_slot;
+	tm->move.nr_items = nr_items;
+	tm->op = MOD_LOG_MOVE_KEYS;
+
+	for (i = 0; i + dst_slot < src_slot && i < nr_items; i++) {
+		tm_list[i] = alloc_tree_mod_elem(eb, i + dst_slot,
+		    MOD_LOG_KEY_REMOVE_WHILE_MOVING, flags);
+		if (!tm_list[i]) {
+			ret = -ENOMEM;
+			goto free_tms;
+		}
+	}
+
+	if (tree_mod_dont_log(fs_info, eb))
+		goto free_tms;
+	locked = 1;
+
 	/*
 	 * When we override something during the move, we log these removals.
 	 * This can only happen when we move towards the beginning of the
 	 * buffer, i.e. dst_slot < src_slot.
 	 */
 	for (i = 0; i + dst_slot < src_slot && i < nr_items; i++) {
-		ret = __tree_mod_log_insert_key(fs_info, eb, i + dst_slot,
-				MOD_LOG_KEY_REMOVE_WHILE_MOVING, GFP_NOFS);
-		BUG_ON(ret < 0);
+		ret = __tree_mod_log_insert(fs_info, tm_list[i]);
+		if (ret)
+			goto free_tms;
 	}
 
-	tm = kzalloc(sizeof(*tm), flags);
-	if (!tm)
-		return -ENOMEM;
+	ret = __tree_mod_log_insert(fs_info, tm);
+	if (ret)
+		goto free_tms;
+	tree_mod_log_write_unlock(fs_info);
+	kfree(tm_list);
 
-	tm->index = eb->start >> PAGE_CACHE_SHIFT;
-	tm->slot = src_slot;
-	tm->move.dst_slot = dst_slot;
-	tm->move.nr_items = nr_items;
-	tm->op = MOD_LOG_MOVE_KEYS;
+	return 0;
+free_tms:
+	for (i = 0; i < nr_items; i++) {
+		if (tm_list[i] && !RB_EMPTY_NODE(&tm_list[i]->node))
+			rb_erase(&tm_list[i]->node, &fs_info->tree_mod_log);
+		kfree(tm_list[i]);
+	}
+	if (locked)
+		tree_mod_log_write_unlock(fs_info);
+	kfree(tm_list);
+	kfree(tm);
 
-	return __tree_mod_log_insert(fs_info, tm);
+	return ret;
 }
 
-static inline void
-__tree_mod_log_free_eb(struct btrfs_fs_info *fs_info, struct extent_buffer *eb)
+static inline int
+__tree_mod_log_free_eb(struct btrfs_fs_info *fs_info,
+		       struct tree_mod_elem **tm_list,
+		       int nritems)
 {
-	int i;
-	u32 nritems;
+	int i, j;
 	int ret;
 
-	if (btrfs_header_level(eb) == 0)
-		return;
-
-	nritems = btrfs_header_nritems(eb);
 	for (i = nritems - 1; i >= 0; i--) {
-		ret = __tree_mod_log_insert_key(fs_info, eb, i,
-				MOD_LOG_KEY_REMOVE_WHILE_FREEING, GFP_NOFS);
-		BUG_ON(ret < 0);
+		ret = __tree_mod_log_insert(fs_info, tm_list[i]);
+		if (ret) {
+			for (j = nritems - 1; j > i; j--)
+				rb_erase(&tm_list[j]->node,
+					 &fs_info->tree_mod_log);
+			return ret;
+		}
 	}
+
+	return 0;
 }
 
 static noinline int
@@ -641,17 +702,38 @@ tree_mod_log_insert_root(struct btrfs_fs_info *fs_info,
 			 struct extent_buffer *new_root, gfp_t flags,
 			 int log_removal)
 {
-	struct tree_mod_elem *tm;
+	struct tree_mod_elem *tm = NULL;
+	struct tree_mod_elem **tm_list = NULL;
+	int nritems = 0;
+	int ret = 0;
+	int i;
 
-	if (tree_mod_dont_log(fs_info, NULL))
+	if (!tree_mod_need_log(fs_info, NULL))
 		return 0;
 
-	if (log_removal)
-		__tree_mod_log_free_eb(fs_info, old_root);
+	if (log_removal && btrfs_header_level(old_root) > 0) {
+		nritems = btrfs_header_nritems(old_root);
+		tm_list = kzalloc(nritems * sizeof(struct tree_mod_elem *),
+				  flags);
+		if (!tm_list) {
+			ret = -ENOMEM;
+			goto free_tms;
+		}
+		for (i = 0; i < nritems; i++) {
+			tm_list[i] = alloc_tree_mod_elem(old_root, i,
+			    MOD_LOG_KEY_REMOVE_WHILE_FREEING, flags);
+			if (!tm_list[i]) {
+				ret = -ENOMEM;
+				goto free_tms;
+			}
+		}
+	}
 
 	tm = kzalloc(sizeof(*tm), flags);
-	if (!tm)
-		return -ENOMEM;
+	if (!tm) {
+		ret = -ENOMEM;
+		goto free_tms;
+	}
 
 	tm->index = new_root->start >> PAGE_CACHE_SHIFT;
 	tm->old_root.logical = old_root->start;
@@ -659,7 +741,30 @@ tree_mod_log_insert_root(struct btrfs_fs_info *fs_info,
 	tm->generation = btrfs_header_generation(old_root);
 	tm->op = MOD_LOG_ROOT_REPLACE;
 
-	return __tree_mod_log_insert(fs_info, tm);
+	if (tree_mod_dont_log(fs_info, NULL))
+		goto free_tms;
+
+	if (tm_list)
+		ret = __tree_mod_log_free_eb(fs_info, tm_list, nritems);
+	if (!ret)
+		ret = __tree_mod_log_insert(fs_info, tm);
+
+	tree_mod_log_write_unlock(fs_info);
+	if (ret)
+		goto free_tms;
+	kfree(tm_list);
+
+	return ret;
+
+free_tms:
+	if (tm_list) {
+		for (i = 0; i < nritems; i++)
+			kfree(tm_list[i]);
+		kfree(tm_list);
+	}
+	kfree(tm);
+
+	return ret;
 }
 
 static struct tree_mod_elem *
@@ -728,31 +833,75 @@ tree_mod_log_search(struct btrfs_fs_info *fs_info, u64 start, u64 min_seq)
 	return __tree_mod_log_search(fs_info, start, min_seq, 0);
 }
 
-static noinline void
+static noinline int
 tree_mod_log_eb_copy(struct btrfs_fs_info *fs_info, struct extent_buffer *dst,
 		     struct extent_buffer *src, unsigned long dst_offset,
 		     unsigned long src_offset, int nr_items)
 {
-	int ret;
+	int ret = 0;
+	struct tree_mod_elem **tm_list = NULL;
+	struct tree_mod_elem **tm_list_add, **tm_list_rem;
 	int i;
+	int locked = 0;
 
-	if (tree_mod_dont_log(fs_info, NULL))
-		return;
+	if (!tree_mod_need_log(fs_info, NULL))
+		return 0;
 
 	if (btrfs_header_level(dst) == 0 && btrfs_header_level(src) == 0)
-		return;
+		return 0;
 
+	tm_list = kzalloc(nr_items * 2 * sizeof(struct tree_mod_elem *),
+			  GFP_NOFS);
+	if (!tm_list)
+		return -ENOMEM;
+
+	tm_list_add = tm_list;
+	tm_list_rem = tm_list + nr_items;
 	for (i = 0; i < nr_items; i++) {
-		ret = __tree_mod_log_insert_key(fs_info, src,
-						i + src_offset,
-						MOD_LOG_KEY_REMOVE, GFP_NOFS);
-		BUG_ON(ret < 0);
-		ret = __tree_mod_log_insert_key(fs_info, dst,
-						     i + dst_offset,
-						     MOD_LOG_KEY_ADD,
-						     GFP_NOFS);
-		BUG_ON(ret < 0);
+		tm_list_rem[i] = alloc_tree_mod_elem(src, i + src_offset,
+		    MOD_LOG_KEY_REMOVE, GFP_NOFS);
+		if (!tm_list_rem[i]) {
+			ret = -ENOMEM;
+			goto free_tms;
+		}
+
+		tm_list_add[i] = alloc_tree_mod_elem(dst, i + dst_offset,
+		    MOD_LOG_KEY_ADD, GFP_NOFS);
+		if (!tm_list_add[i]) {
+			ret = -ENOMEM;
+			goto free_tms;
+		}
+	}
+
+	if (tree_mod_dont_log(fs_info, NULL))
+		goto free_tms;
+	locked = 1;
+
+	for (i = 0; i < nr_items; i++) {
+		ret = __tree_mod_log_insert(fs_info, tm_list_rem[i]);
+		if (ret)
+			goto free_tms;
+		ret = __tree_mod_log_insert(fs_info, tm_list_add[i]);
+		if (ret)
+			goto free_tms;
+	}
+
+	tree_mod_log_write_unlock(fs_info);
+	kfree(tm_list);
+
+	return 0;
+
+free_tms:
+	for (i = 0; i < nr_items * 2; i++) {
+		if (tm_list[i] && !RB_EMPTY_NODE(&tm_list[i]->node))
+			rb_erase(&tm_list[i]->node, &fs_info->tree_mod_log);
+		kfree(tm_list[i]);
 	}
+	if (locked)
+		tree_mod_log_write_unlock(fs_info);
+	kfree(tm_list);
+
+	return ret;
 }
 
 static inline void
@@ -777,12 +926,52 @@ tree_mod_log_set_node_key(struct btrfs_fs_info *fs_info,
 	BUG_ON(ret < 0);
 }
 
-static noinline void
+static noinline int
 tree_mod_log_free_eb(struct btrfs_fs_info *fs_info, struct extent_buffer *eb)
 {
+	struct tree_mod_elem **tm_list = NULL;
+	int nritems = 0;
+	int i;
+	int ret = 0;
+
+	if (btrfs_header_level(eb) == 0)
+		return 0;
+
+	if (!tree_mod_need_log(fs_info, NULL))
+		return 0;
+
+	nritems = btrfs_header_nritems(eb);
+	tm_list = kzalloc(nritems * sizeof(struct tree_mod_elem *),
+			  GFP_NOFS);
+	if (!tm_list)
+		return -ENOMEM;
+
+	for (i = 0; i < nritems; i++) {
+		tm_list[i] = alloc_tree_mod_elem(eb, i,
+		    MOD_LOG_KEY_REMOVE_WHILE_FREEING, GFP_NOFS);
+		if (!tm_list[i]) {
+			ret = -ENOMEM;
+			goto free_tms;
+		}
+	}
+
 	if (tree_mod_dont_log(fs_info, eb))
-		return;
-	__tree_mod_log_free_eb(fs_info, eb);
+		goto free_tms;
+
+	ret = __tree_mod_log_free_eb(fs_info, tm_list, nritems);
+	tree_mod_log_write_unlock(fs_info);
+	if (ret)
+		goto free_tms;
+	kfree(tm_list);
+
+	return 0;
+
+free_tms:
+	for (i = 0; i < nritems; i++)
+		kfree(tm_list[i]);
+	kfree(tm_list);
+
+	return ret;
 }
 
 static noinline void
@@ -1040,8 +1229,13 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 		btrfs_set_node_ptr_generation(parent, parent_slot,
 					      trans->transid);
 		btrfs_mark_buffer_dirty(parent);
-		if (last_ref)
-			tree_mod_log_free_eb(root->fs_info, buf);
+		if (last_ref) {
+			ret = tree_mod_log_free_eb(root->fs_info, buf);
+			if (ret) {
+				btrfs_abort_transaction(trans, root, ret);
+				return ret;
+			}
+		}
 		btrfs_free_tree_block(trans, root, buf, parent_start,
 				      last_ref);
 	}
@@ -3064,8 +3258,12 @@ static int push_node_left(struct btrfs_trans_handle *trans,
 	} else
 		push_items = min(src_nritems - 8, push_items);
 
-	tree_mod_log_eb_copy(root->fs_info, dst, src, dst_nritems, 0,
-			     push_items);
+	ret = tree_mod_log_eb_copy(root->fs_info, dst, src, dst_nritems, 0,
+				   push_items);
+	if (ret) {
+		btrfs_abort_transaction(trans, root, ret);
+		return ret;
+	}
 	copy_extent_buffer(dst, src,
 			   btrfs_node_key_ptr_offset(dst_nritems),
 			   btrfs_node_key_ptr_offset(0),
@@ -3135,8 +3333,12 @@ static int balance_node_right(struct btrfs_trans_handle *trans,
 				      (dst_nritems) *
 				      sizeof(struct btrfs_key_ptr));
 
-	tree_mod_log_eb_copy(root->fs_info, dst, src, 0,
-			     src_nritems - push_items, push_items);
+	ret = tree_mod_log_eb_copy(root->fs_info, dst, src, 0,
+				   src_nritems - push_items, push_items);
+	if (ret) {
+		btrfs_abort_transaction(trans, root, ret);
+		return ret;
+	}
 	copy_extent_buffer(dst, src,
 			   btrfs_node_key_ptr_offset(0),
 			   btrfs_node_key_ptr_offset(src_nritems - push_items),
@@ -3337,7 +3539,12 @@ static noinline int split_node(struct btrfs_trans_handle *trans,
 			    btrfs_header_chunk_tree_uuid(split),
 			    BTRFS_UUID_SIZE);
 
-	tree_mod_log_eb_copy(root->fs_info, split, c, 0, mid, c_nritems - mid);
+	ret = tree_mod_log_eb_copy(root->fs_info, split, c, 0,
+				   mid, c_nritems - mid);
+	if (ret) {
+		btrfs_abort_transaction(trans, root, ret);
+		return ret;
+	}
 	copy_extent_buffer(split, c,
 			   btrfs_node_key_ptr_offset(0),
 			   btrfs_node_key_ptr_offset(mid),

commit 783577663507411e36e459390ef056556e93ef29
Author: Filipe David Borba Manana <fdmanana@gmail.com>
Date:   Thu Dec 12 19:19:52 2013 +0000

    Btrfs: return immediately if tree log mod is not necessary
    
    In ctree.c:tree_mod_log_set_node_key() we were calling
    __tree_mod_log_insert_key() even when the modification doesn't need
    to be logged. This would allocate a tree_mod_elem structure, fill it
    and pass it to  __tree_mod_log_insert(), which would just acquire
    the tree mod log write lock and then free the tree_mod_elem structure
    and return (that is, a no-op).
    
    Therefore call tree_mod_log_insert() instead of __tree_mod_log_insert()
    which just returns immediately if the modification doesn't need to be
    logged (without allocating the structure, fill it, acquire write lock,
    free structure).
    
    Signed-off-by: Filipe David Borba Manana <fdmanana@gmail.com>
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index a57507aa6702..59664f6cbc4e 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -771,7 +771,7 @@ tree_mod_log_set_node_key(struct btrfs_fs_info *fs_info,
 {
 	int ret;
 
-	ret = __tree_mod_log_insert_key(fs_info, eb, slot,
+	ret = tree_mod_log_insert_key(fs_info, eb, slot,
 					MOD_LOG_KEY_REPLACE,
 					atomic ? GFP_ATOMIC : GFP_NOFS);
 	BUG_ON(ret < 0);

commit 2ef1fed285d58e77ce777d9a525fed4788b5a6d0
Author: Filipe David Borba Manana <fdmanana@gmail.com>
Date:   Wed Dec 4 22:17:39 2013 +0000

    Btrfs: more efficient push_leaf_right
    
    Currently when finding the leaf to insert a key into a btree, if the
    leaf doesn't have enough space to store the item we attempt to move
    off some items from our leaf to its right neighbor leaf, and if this
    fails to create enough free space in our leaf, we try to move off more
    items to the left neighbor leaf as well.
    
    When trying to move off items to the right neighbor leaf, if it has
    enough room to store the new key but not not enough room to move off
    at least one item from our target leaf, __push_leaf_right returns 1 and
    we have to attempt to move items to the left neighbor (push_leaf_left
    function) without touching the right neighbor leaf.
    For the case where the right leaf has enough room to store at least 1
    item from our leaf, we end up modifying (and dirtying) both our leaf
    and the right leaf. This is non-optimal for the case where the new key
    is greater than any key in our target leaf because it can be inserted at
    slot 0 of the right neighbor leaf and we don't need to touch our leaf
    at all nor to attempt to move off items to the left neighbor leaf.
    
    Therefore this change just selects the right neighbor leaf as our new
    target leaf if it has enough room for the new key without modifying our
    initial target leaf - we do this only if the new key is higher than any
    key in the initial target leaf.
    
    While running the following test, push_leaf_right was called by split_leaf
    4802 times. Out of those 4802 calls, for 2571 calls (53.5%) we hit this
    special case (right leaf has enough room and new key is higher than any key
    in the initial target leaf).
    
    Test:
    
      sysbench --test=fileio --file-num=512 --file-total-size=5G \
        --file-test-mode=[seqwr|rndwr] --num-threads=512 --file-block-size=8192 \
        --max-requests=100000 --file-io-mode=sync [prepare|run]
    
    Results:
    
    sequential writes
    
    Throughput before this change: 65.71Mb/sec (average of 10 runs)
    Throughput after this change:  66.58Mb/sec (average of 10 runs)
    
    random writes
    
    Throughput before this change: 10.75Mb/sec (average of 10 runs)
    Throughput after this change:  11.56Mb/sec (average of 10 runs)
    
    Signed-off-by: Filipe David Borba Manana <fdmanana@gmail.com>
    Reviewed-by: Liu Bo <bo.li.liu@oracle.com>
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 11f9a1848c7b..a57507aa6702 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -3613,6 +3613,19 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 	if (left_nritems == 0)
 		goto out_unlock;
 
+	if (path->slots[0] == left_nritems && !empty) {
+		/* Key greater than all keys in the leaf, right neighbor has
+		 * enough room for it and we're not emptying our leaf to delete
+		 * it, therefore use right neighbor to insert the new item and
+		 * no need to touch/dirty our left leaft. */
+		btrfs_tree_unlock(left);
+		free_extent_buffer(left);
+		path->nodes[0] = right;
+		path->slots[0] = 0;
+		path->slots[1]++;
+		return 0;
+	}
+
 	return __push_leaf_right(trans, root, path, min_data_size, empty,
 				right, free_space, left_nritems, min_slot);
 out_unlock:

commit 5a4267ca20d4c452a97dace4612f1dfc04147fbd
Author: Filipe David Borba Manana <fdmanana@gmail.com>
Date:   Mon Nov 25 03:20:46 2013 +0000

    Btrfs: try harder to avoid btree node splits
    
    When attempting to move items from our target leaf to its neighbor
    leaves (right and left), we only need to free data_size - free_space
    bytes from our leaf in order to add the new item (which has size of
    data_size bytes). Therefore attempt to move items to the right and
    left leaves if they have at least data_size - free_space bytes free,
    instead of data_size bytes free.
    
    After 5 runs of the following test, I got a smaller number of btree
    node splits overall:
    
    sysbench --test=fileio --file-num=512 --file-total-size=5G \
      --file-test-mode=seqwr --num-threads=512 \
       --file-block-size=8192 --max-requests=100000 --file-io-mode=sync
    
    Before this change:
    * 6171 splits (average of 5 test runs)
    * 61.508Mb/sec of throughput (average of 5 test runs)
    
    After this change:
    * 6036 splits (average of 5 test runs)
    * 63.533Mb/sec of throughput (average of 5 test runs)
    
    An ideal test would not just have multiple threads/processes writing
    to a file (insertion of file extent items) but also do other operations
    that result in insertion of items with varied sizes, like file/directory
    creations, creation of links, symlinks, xattrs, etc.
    
    Signed-off-by: Filipe David Borba Manana <fdmanana@gmail.com>
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 22629809c9c4..11f9a1848c7b 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -3929,14 +3929,17 @@ static noinline int push_for_double_split(struct btrfs_trans_handle *trans,
 	int progress = 0;
 	int slot;
 	u32 nritems;
+	int space_needed = data_size;
 
 	slot = path->slots[0];
+	if (slot < btrfs_header_nritems(path->nodes[0]))
+		space_needed -= btrfs_leaf_free_space(root, path->nodes[0]);
 
 	/*
 	 * try to push all the items after our slot into the
 	 * right leaf
 	 */
-	ret = push_leaf_right(trans, root, path, 1, data_size, 0, slot);
+	ret = push_leaf_right(trans, root, path, 1, space_needed, 0, slot);
 	if (ret < 0)
 		return ret;
 
@@ -3956,7 +3959,7 @@ static noinline int push_for_double_split(struct btrfs_trans_handle *trans,
 
 	/* try to push all the items before our slot into the next leaf */
 	slot = path->slots[0];
-	ret = push_leaf_left(trans, root, path, 1, data_size, 0, slot);
+	ret = push_leaf_left(trans, root, path, 1, space_needed, 0, slot);
 	if (ret < 0)
 		return ret;
 
@@ -4000,13 +4003,18 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 
 	/* first try to make some room by pushing left and right */
 	if (data_size && path->nodes[1]) {
-		wret = push_leaf_right(trans, root, path, data_size,
-				       data_size, 0, 0);
+		int space_needed = data_size;
+
+		if (slot < btrfs_header_nritems(l))
+			space_needed -= btrfs_leaf_free_space(root, l);
+
+		wret = push_leaf_right(trans, root, path, space_needed,
+				       space_needed, 0, 0);
 		if (wret < 0)
 			return wret;
 		if (wret) {
-			wret = push_leaf_left(trans, root, path, data_size,
-					      data_size, 0, (u32)-1);
+			wret = push_leaf_left(trans, root, path, space_needed,
+					      space_needed, 0, (u32)-1);
 			if (wret < 0)
 				return wret;
 		}

commit 3f870c28990015a1fd6c67807efcdb02a75b35e1
Author: Kelley Nielsen <kelleynnn@gmail.com>
Date:   Mon Nov 4 19:37:39 2013 -0800

    btrfs: expand btrfs_find_item() to include find_orphan_item functionality
    
    This is the third step in bootstrapping the btrfs_find_item interface.
    The function find_orphan_item(), in orphan.c, is similar to the two
    functions already replaced by the new interface. It uses two parameters,
    which are already present in the interface, and is nearly identical to
    the function brought in in the previous patch.
    
    Replace the two calls to find_orphan_item() with calls to
    btrfs_find_item(), with the defined objectid and type that was used
    internally by find_orphan_item(), a null path, and a null key. Add a
    test for a null path to btrfs_find_item, and if it passes, allocate and
    free the path. Finally, remove find_orphan_item().
    
    Signed-off-by: Kelley Nielsen <kelleynnn@gmail.com>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 64b87894460b..22629809c9c4 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2461,32 +2461,32 @@ static int key_search(struct extent_buffer *b, struct btrfs_key *key,
 	return 0;
 }
 
-/* Proposed generic search function, meant to take the place of the
-* various small search helper functions throughout the code and standardize
-* the search interface. Right now, it only replaces the former __inode_info
-* in backref.c, and the former btrfs_find_root_ref in root-tree.c.
-*
-* If a null key is passed, it returns immediately after running
-* btrfs_search_slot, leaving the path filled as it is and passing its
-* return value upward. If a real key is passed, it will set the caller's
-* path to point to the first item in the tree after its specified
-* objectid, type, and offset for which objectid and type match the input.
-*/
-int btrfs_find_item(struct btrfs_root *fs_root, struct btrfs_path *path,
+int btrfs_find_item(struct btrfs_root *fs_root, struct btrfs_path *found_path,
 		u64 iobjectid, u64 ioff, u8 key_type,
 		struct btrfs_key *found_key)
 {
 	int ret;
 	struct btrfs_key key;
 	struct extent_buffer *eb;
+	struct btrfs_path *path;
 
 	key.type = key_type;
 	key.objectid = iobjectid;
 	key.offset = ioff;
 
+	if (found_path == NULL) {
+		path = btrfs_alloc_path();
+		if (!path)
+			return -ENOMEM;
+	} else
+		path = found_path;
+
 	ret = btrfs_search_slot(NULL, fs_root, &key, path, 0, 0);
-	if ((ret < 0) || (found_key == NULL))
+	if ((ret < 0) || (found_key == NULL)) {
+		if (path != found_path)
+			btrfs_free_path(path);
 		return ret;
+	}
 
 	eb = path->nodes[0];
 	if (ret && path->slots[0] >= btrfs_header_nritems(eb)) {

commit 75ac2dd907013b44edbdec16f8969d14811149c9
Author: Kelley Nielsen <kelleynnn@gmail.com>
Date:   Mon Nov 4 19:35:58 2013 -0800

    btrfs: expand btrfs_find_item() to include find_root_ref functionality
    
    This patch is the second step in bootstrapping the btrfs_find_item
    interface. The btrfs_find_root_ref() is similar to the former
    __inode_info(); it accepts four of its parameters, and duplicates the
    first half of its functionality.
    
    Replace the one former call to btrfs_find_root_ref() with a call to
    btrfs_find_item(), along with the defined key type that was used
    internally by btrfs_find_root ref, and a null found key. In
    btrfs_find_item(), add a test for the null key at the place where
    the functionality of btrfs_find_root_ref() ends; btrfs_find_item()
    then returns if the test passes. Finally, remove btrfs_find_root_ref().
    
    Signed-off-by: Kelley Nielsen <kelleynnn@gmail.com>
    Suggested-by: Zach Brown <zab@redhat.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 141c15ca294e..64b87894460b 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2464,7 +2464,13 @@ static int key_search(struct extent_buffer *b, struct btrfs_key *key,
 /* Proposed generic search function, meant to take the place of the
 * various small search helper functions throughout the code and standardize
 * the search interface. Right now, it only replaces the former __inode_info
-* in backref.c.
+* in backref.c, and the former btrfs_find_root_ref in root-tree.c.
+*
+* If a null key is passed, it returns immediately after running
+* btrfs_search_slot, leaving the path filled as it is and passing its
+* return value upward. If a real key is passed, it will set the caller's
+* path to point to the first item in the tree after its specified
+* objectid, type, and offset for which objectid and type match the input.
 */
 int btrfs_find_item(struct btrfs_root *fs_root, struct btrfs_path *path,
 		u64 iobjectid, u64 ioff, u8 key_type,
@@ -2479,7 +2485,7 @@ int btrfs_find_item(struct btrfs_root *fs_root, struct btrfs_path *path,
 	key.offset = ioff;
 
 	ret = btrfs_search_slot(NULL, fs_root, &key, path, 0, 0);
-	if (ret < 0)
+	if ((ret < 0) || (found_key == NULL))
 		return ret;
 
 	eb = path->nodes[0];

commit e33d5c3d6d61518c7f115af6d11d3dffa230d31f
Author: Kelley Nielsen <kelleynnn@gmail.com>
Date:   Mon Nov 4 19:33:33 2013 -0800

    btrfs: bootstrap generic btrfs_find_item interface
    
    There are many btrfs functions that manually search the tree for an
    item. They all reimplement the same mechanism and differ in the
    conditions that they use to find the item. __inode_info() is one such
    example. Zach Brown proposed creating a new interface to take the place
    of these functions.
    
    This patch is the first step to creating the interface. A new function,
    btrfs_find_item, has been added to ctree.c and prototyped in ctree.h.
    It is identical to __inode_info, except that the order of the parameters
    has been rearranged to more closely those of similar functions elsewhere
    in the code (now, root and path come first, then the objectid, offset
    and type, and the key to be filled in last). __inode_info's callers have
    been set to call this new function instead, and __inode_info itself has
    been removed.
    
    Signed-off-by: Kelley Nielsen <kelleynnn@gmail.com>
    Suggested-by: Zach Brown <zab@redhat.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index bcd0bd85e3ed..141c15ca294e 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2461,6 +2461,43 @@ static int key_search(struct extent_buffer *b, struct btrfs_key *key,
 	return 0;
 }
 
+/* Proposed generic search function, meant to take the place of the
+* various small search helper functions throughout the code and standardize
+* the search interface. Right now, it only replaces the former __inode_info
+* in backref.c.
+*/
+int btrfs_find_item(struct btrfs_root *fs_root, struct btrfs_path *path,
+		u64 iobjectid, u64 ioff, u8 key_type,
+		struct btrfs_key *found_key)
+{
+	int ret;
+	struct btrfs_key key;
+	struct extent_buffer *eb;
+
+	key.type = key_type;
+	key.objectid = iobjectid;
+	key.offset = ioff;
+
+	ret = btrfs_search_slot(NULL, fs_root, &key, path, 0, 0);
+	if (ret < 0)
+		return ret;
+
+	eb = path->nodes[0];
+	if (ret && path->slots[0] >= btrfs_header_nritems(eb)) {
+		ret = btrfs_next_leaf(fs_root, path);
+		if (ret)
+			return ret;
+		eb = path->nodes[0];
+	}
+
+	btrfs_item_key_to_cpu(eb, found_key, path->slots[0]);
+	if (found_key->type != key.type ||
+			found_key->objectid != key.objectid)
+		return 1;
+
+	return 0;
+}
+
 /*
  * look for key in the tree.  path is filled in with nodes along the way
  * if key is found, we return zero and you can find the item in the leaf

commit 16e7549f045d33b0c5b0ebf19d08439e9221d40c
Author: Josef Bacik <jbacik@fusionio.com>
Date:   Tue Oct 22 12:18:51 2013 -0400

    Btrfs: incompatible format change to remove hole extents
    
    Btrfs has always had these filler extent data items for holes in inodes.  This
    has made somethings very easy, like logging hole punches and sending hole
    punches.  However for large holey files these extent data items are pure
    overhead.  So add an incompatible feature to no longer add hole extents to
    reduce the amount of metadata used by these sort of files.  This has a few
    changes for logging and send obviously since they will need to detect holes and
    log/send the holes if there are any.  I've tested this thoroughly with xfstests
    and it doesn't cause any issues with and without the incompat format set.
    Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>
    Signed-off-by: Chris Mason <clm@fb.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 316136bd6dd7..bcd0bd85e3ed 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -41,7 +41,6 @@ static void del_ptr(struct btrfs_root *root, struct btrfs_path *path,
 		    int level, int slot);
 static void tree_mod_log_free_eb(struct btrfs_fs_info *fs_info,
 				 struct extent_buffer *eb);
-static int btrfs_prev_leaf(struct btrfs_root *root, struct btrfs_path *path);
 
 struct btrfs_path *btrfs_alloc_path(void)
 {
@@ -4817,7 +4816,7 @@ int btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,
  * This may release the path, and so you may lose any locks held at the
  * time you call it.
  */
-static int btrfs_prev_leaf(struct btrfs_root *root, struct btrfs_path *path)
+int btrfs_prev_leaf(struct btrfs_root *root, struct btrfs_path *path)
 {
 	struct btrfs_key key;
 	struct btrfs_disk_key found_key;

commit 678712545b62715a6c867471320ff5f60a521f3a
Author: Dulshani Gunawardhana <dulshani.gunawardhana89@gmail.com>
Date:   Thu Oct 31 10:33:04 2013 +0530

    btrfs: Fix checkpatch.pl warning of spacing issues
    
    Fix spacing issues detected via checkpatch.pl in accordance with the
    kernel style guidelines.
    
    Signed-off-by: Dulshani Gunawardhana <dulshani.gunawardhana89@gmail.com>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 03c606cefd50..316136bd6dd7 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -4018,7 +4018,7 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 				    data_size > BTRFS_LEAF_DATA_SIZE(root)) {
 					if (data_size && !tried_avoid_double)
 						goto push_for_double;
-					split = 2 ;
+					split = 2;
 				}
 			}
 		}

commit fae7f21cece9a4c181a8d8131870c7247e153f65
Author: Dulshani Gunawardhana <dulshani.gunawardhana89@gmail.com>
Date:   Thu Oct 31 10:30:08 2013 +0530

    btrfs: Use WARN_ON()'s return value in place of WARN_ON(1)
    
    Use WARN_ON()'s return value in place of WARN_ON(1) for cleaner source
    code that outputs a more descriptive warnings. Also fix the styling
    warning of redundant braces that came up as a result of this fix.
    
    Signed-off-by: Dulshani Gunawardhana <dulshani.gunawardhana89@gmail.com>
    Reviewed-by: Zach Brown <zab@redhat.com>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index a749121203df..03c606cefd50 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1285,11 +1285,10 @@ get_old_root(struct btrfs_root *root, u64 time_seq)
 		free_extent_buffer(eb_root);
 		blocksize = btrfs_level_size(root, old_root->level);
 		old = read_tree_block(root, logical, blocksize, 0);
-		if (!old || !extent_buffer_uptodate(old)) {
+		if (WARN_ON(!old || !extent_buffer_uptodate(old))) {
 			free_extent_buffer(old);
 			pr_warn("btrfs: failed to read tree block %llu from get_old_root\n",
 				logical);
-			WARN_ON(1);
 		} else {
 			eb = btrfs_clone_extent_buffer(old);
 			free_extent_buffer(old);
@@ -3643,8 +3642,7 @@ static noinline int __push_leaf_left(struct btrfs_trans_handle *trans,
 		ret = 1;
 		goto out;
 	}
-	if (!empty && push_items == btrfs_header_nritems(right))
-		WARN_ON(1);
+	WARN_ON(!empty && push_items == btrfs_header_nritems(right));
 
 	/* push data from right to left */
 	copy_extent_buffer(left, right,

commit e8b0d724d596f2ac1264ad830a04ef8e415be956
Author: Filipe David Borba Manana <fdmanana@gmail.com>
Date:   Tue Oct 15 00:12:27 2013 +0100

    Btrfs: fix btrfs_prev_leaf() previous key computation
    
    If we decrement the key type, we must reset its offset to the largest
    possible offset (u64)-1. If we decrement the key's objectid, then we
    must reset the key's type and offset to their largest possible values,
    (u8)-1 and (u64)-1 respectively. Not doing so can make us miss an
    items in the tree.
    
    Signed-off-by: Filipe David Borba Manana <fdmanana@gmail.com>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 8f3d6f893585..a749121203df 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -4827,14 +4827,18 @@ static int btrfs_prev_leaf(struct btrfs_root *root, struct btrfs_path *path)
 
 	btrfs_item_key_to_cpu(path->nodes[0], &key, 0);
 
-	if (key.offset > 0)
+	if (key.offset > 0) {
 		key.offset--;
-	else if (key.type > 0)
+	} else if (key.type > 0) {
 		key.type--;
-	else if (key.objectid > 0)
+		key.offset = (u64)-1;
+	} else if (key.objectid > 0) {
 		key.objectid--;
-	else
+		key.type = (u8)-1;
+		key.offset = (u64)-1;
+	} else {
 		return 1;
+	}
 
 	btrfs_release_path(path);
 	ret = btrfs_search_slot(NULL, root, &key, path, 0, 0);

commit 498456d33e2ee5150f045e604e4531b088083e7a
Author: Liu Bo <bo.li.liu@oracle.com>
Date:   Tue Oct 8 18:19:43 2013 +0800

    Btrfs: kill unused code in btrfs_search_forward
    
    After commit de78b51a2852bddccd6535e9e12de65f92787a1e
    (btrfs: remove cache only arguments from defrag path), @blockptr is no more
    used.
    
    Signed-off-by: Liu Bo <bo.li.liu@oracle.com>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 33e9dbdd043d..8f3d6f893585 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -4914,10 +4914,8 @@ int btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,
 		 * If it is too old, old, skip to the next one.
 		 */
 		while (slot < nritems) {
-			u64 blockptr;
 			u64 gen;
 
-			blockptr = btrfs_node_blockptr(cur, slot);
 			gen = btrfs_node_ptr_generation(cur, slot);
 			if (gen < min_trans) {
 				slot++;

commit 6174d3cb43aa974d0c8590a3e628ac35ab0bbc13
Author: Filipe David Borba Manana <fdmanana@gmail.com>
Date:   Tue Oct 1 16:13:42 2013 +0100

    Btrfs: remove unused max_key arg from btrfs_search_forward
    
    It is not used for anything.
    
    Signed-off-by: Filipe David Borba Manana <fdmanana@gmail.com>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 6c5239b55495..33e9dbdd043d 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -4870,7 +4870,6 @@ static int btrfs_prev_leaf(struct btrfs_root *root, struct btrfs_path *path)
  * was nothing in the tree that matched the search criteria.
  */
 int btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,
-			 struct btrfs_key *max_key,
 			 struct btrfs_path *path,
 			 u64 min_trans)
 {

commit 0a4e558609dd4df30a58a07d9eb14c5ddc2c1241
Author: Ross Kirk <ross.kirk@gmail.com>
Date:   Tue Sep 24 10:12:38 2013 +0100

    btrfs: remove unused parameter from btrfs_header_fsid
    
    Remove unused parameter, 'eb'. Unused since introduction in
    5f39d397dfbe140a14edecd4e73c34ce23c4f9ee
    
    Updated to be rebased against current upstream and correct diff supplied this time!
    
    Signed-off-by: Ross Kirk <ross.kirk@gmail.com>
    Reviewed-by: Eric Sandeen <sandeen@redhat.com>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 6932686b61cf..6c5239b55495 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -274,7 +274,7 @@ int btrfs_copy_root(struct btrfs_trans_handle *trans,
 	else
 		btrfs_set_header_owner(cow, new_root_objectid);
 
-	write_extent_buffer(cow, root->fs_info->fsid, btrfs_header_fsid(cow),
+	write_extent_buffer(cow, root->fs_info->fsid, btrfs_header_fsid(),
 			    BTRFS_FSID_SIZE);
 
 	WARN_ON(btrfs_header_generation(buf) > trans->transid);
@@ -996,7 +996,7 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 	else
 		btrfs_set_header_owner(cow, root->root_key.objectid);
 
-	write_extent_buffer(cow, root->fs_info->fsid, btrfs_header_fsid(cow),
+	write_extent_buffer(cow, root->fs_info->fsid, btrfs_header_fsid(),
 			    BTRFS_FSID_SIZE);
 
 	ret = update_ref_for_cow(trans, root, buf, cow, &last_ref);
@@ -3152,7 +3152,7 @@ static noinline int insert_new_root(struct btrfs_trans_handle *trans,
 	btrfs_set_header_backref_rev(c, BTRFS_MIXED_BACKREF_REV);
 	btrfs_set_header_owner(c, root->root_key.objectid);
 
-	write_extent_buffer(c, root->fs_info->fsid, btrfs_header_fsid(c),
+	write_extent_buffer(c, root->fs_info->fsid, btrfs_header_fsid(),
 			    BTRFS_FSID_SIZE);
 
 	write_extent_buffer(c, root->fs_info->chunk_tree_uuid,
@@ -3291,7 +3291,7 @@ static noinline int split_node(struct btrfs_trans_handle *trans,
 	btrfs_set_header_backref_rev(split, BTRFS_MIXED_BACKREF_REV);
 	btrfs_set_header_owner(split, root->root_key.objectid);
 	write_extent_buffer(split, root->fs_info->fsid,
-			    btrfs_header_fsid(split), BTRFS_FSID_SIZE);
+			    btrfs_header_fsid(), BTRFS_FSID_SIZE);
 	write_extent_buffer(split, root->fs_info->chunk_tree_uuid,
 			    btrfs_header_chunk_tree_uuid(split),
 			    BTRFS_UUID_SIZE);
@@ -4046,7 +4046,7 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 	btrfs_set_header_owner(right, root->root_key.objectid);
 	btrfs_set_header_level(right, 0);
 	write_extent_buffer(right, root->fs_info->fsid,
-			    btrfs_header_fsid(right), BTRFS_FSID_SIZE);
+			    btrfs_header_fsid(), BTRFS_FSID_SIZE);
 
 	write_extent_buffer(right, root->fs_info->chunk_tree_uuid,
 			    btrfs_header_chunk_tree_uuid(right),

commit d4b4087c43cc00a196c5be57fac41f41309f1d56
Author: Josef Bacik <jbacik@fusionio.com>
Date:   Tue Sep 24 14:09:34 2013 -0400

    Btrfs: do a full search everytime in btrfs_search_old_slot
    
    While running some snashot aware defrag tests I noticed I was panicing every
    once and a while in key_search.  This is because of the optimization that says
    if we find a key at slot 0 it will be at slot 0 all the way down the rest of the
    tree.  This isn't the case for btrfs_search_old_slot since it will likely replay
    changes to a buffer if something has changed since we took our sequence number.
    So short circuit this optimization by setting prev_cmp to -1 every time we call
    key_search so we will do our normal binary search.  With this patch I am no
    longer seeing the panics I was seeing before.  Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index c274a752c93d..6932686b61cf 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2758,7 +2758,7 @@ int btrfs_search_old_slot(struct btrfs_root *root, struct btrfs_key *key,
 	int level;
 	int lowest_unlock = 1;
 	u8 lowest_level = 0;
-	int prev_cmp;
+	int prev_cmp = -1;
 
 	lowest_level = p->lowest_level;
 	WARN_ON(p->nodes[0] != NULL);
@@ -2769,7 +2769,6 @@ int btrfs_search_old_slot(struct btrfs_root *root, struct btrfs_key *key,
 	}
 
 again:
-	prev_cmp = -1;
 	b = get_old_root(root, time_seq);
 	level = btrfs_header_level(b);
 	p->locks[level] = BTRFS_READ_LOCK;
@@ -2787,6 +2786,11 @@ int btrfs_search_old_slot(struct btrfs_root *root, struct btrfs_key *key,
 		 */
 		btrfs_unlock_up_safe(p, level + 1);
 
+		/*
+		 * Since we can unwind eb's we want to do a real search every
+		 * time.
+		 */
+		prev_cmp = -1;
 		ret = key_search(b, key, level, &prev_cmp, &slot);
 
 		if (level != 0) {

commit dd3cc16b8750251ea9b1a843ce7806e82b015d5e
Author: Ross Kirk <ross.kirk@gmail.com>
Date:   Mon Sep 16 15:58:09 2013 +0100

    btrfs: drop unused parameter from btrfs_item_nr
    
    Remove unused eb parameter from btrfs_item_nr
    
    Signed-off-by: Ross Kirk <ross.kirk@gmail.com>
    Reviewed-by: David Sterba <dsterba@suse.cz>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 61b5bcd57b7e..c274a752c93d 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -3337,8 +3337,8 @@ static int leaf_space_used(struct extent_buffer *l, int start, int nr)
 	if (!nr)
 		return 0;
 	btrfs_init_map_token(&token);
-	start_item = btrfs_item_nr(l, start);
-	end_item = btrfs_item_nr(l, end);
+	start_item = btrfs_item_nr(start);
+	end_item = btrfs_item_nr(end);
 	data_len = btrfs_token_item_offset(l, start_item, &token) +
 		btrfs_token_item_size(l, start_item, &token);
 	data_len = data_len - btrfs_token_item_offset(l, end_item, &token);
@@ -3406,7 +3406,7 @@ static noinline int __push_leaf_right(struct btrfs_trans_handle *trans,
 	slot = path->slots[1];
 	i = left_nritems - 1;
 	while (i >= nr) {
-		item = btrfs_item_nr(left, i);
+		item = btrfs_item_nr(i);
 
 		if (!empty && push_items > 0) {
 			if (path->slots[0] > i)
@@ -3470,7 +3470,7 @@ static noinline int __push_leaf_right(struct btrfs_trans_handle *trans,
 	btrfs_set_header_nritems(right, right_nritems);
 	push_space = BTRFS_LEAF_DATA_SIZE(root);
 	for (i = 0; i < right_nritems; i++) {
-		item = btrfs_item_nr(right, i);
+		item = btrfs_item_nr(i);
 		push_space -= btrfs_token_item_size(right, item, &token);
 		btrfs_set_token_item_offset(right, item, push_space, &token);
 	}
@@ -3612,7 +3612,7 @@ static noinline int __push_leaf_left(struct btrfs_trans_handle *trans,
 		nr = min(right_nritems - 1, max_slot);
 
 	for (i = 0; i < nr; i++) {
-		item = btrfs_item_nr(right, i);
+		item = btrfs_item_nr(i);
 
 		if (!empty && push_items > 0) {
 			if (path->slots[0] < i)
@@ -3663,7 +3663,7 @@ static noinline int __push_leaf_left(struct btrfs_trans_handle *trans,
 	for (i = old_left_nritems; i < old_left_nritems + push_items; i++) {
 		u32 ioff;
 
-		item = btrfs_item_nr(left, i);
+		item = btrfs_item_nr(i);
 
 		ioff = btrfs_token_item_offset(left, item, &token);
 		btrfs_set_token_item_offset(left, item,
@@ -3694,7 +3694,7 @@ static noinline int __push_leaf_left(struct btrfs_trans_handle *trans,
 	btrfs_set_header_nritems(right, right_nritems);
 	push_space = BTRFS_LEAF_DATA_SIZE(root);
 	for (i = 0; i < right_nritems; i++) {
-		item = btrfs_item_nr(right, i);
+		item = btrfs_item_nr(i);
 
 		push_space = push_space - btrfs_token_item_size(right,
 								item, &token);
@@ -3835,7 +3835,7 @@ static noinline void copy_for_split(struct btrfs_trans_handle *trans,
 		      btrfs_item_end_nr(l, mid);
 
 	for (i = 0; i < nritems; i++) {
-		struct btrfs_item *item = btrfs_item_nr(right, i);
+		struct btrfs_item *item = btrfs_item_nr(i);
 		u32 ioff;
 
 		ioff = btrfs_token_item_offset(right, item, &token);
@@ -4177,7 +4177,7 @@ static noinline int split_item(struct btrfs_trans_handle *trans,
 
 	btrfs_set_path_blocking(path);
 
-	item = btrfs_item_nr(leaf, path->slots[0]);
+	item = btrfs_item_nr(path->slots[0]);
 	orig_offset = btrfs_item_offset(leaf, item);
 	item_size = btrfs_item_size(leaf, item);
 
@@ -4200,7 +4200,7 @@ static noinline int split_item(struct btrfs_trans_handle *trans,
 	btrfs_cpu_key_to_disk(&disk_key, new_key);
 	btrfs_set_item_key(leaf, &disk_key, slot);
 
-	new_item = btrfs_item_nr(leaf, slot);
+	new_item = btrfs_item_nr(slot);
 
 	btrfs_set_item_offset(leaf, new_item, orig_offset);
 	btrfs_set_item_size(leaf, new_item, item_size - split_offset);
@@ -4339,7 +4339,7 @@ void btrfs_truncate_item(struct btrfs_root *root, struct btrfs_path *path,
 	/* first correct the data pointers */
 	for (i = slot; i < nritems; i++) {
 		u32 ioff;
-		item = btrfs_item_nr(leaf, i);
+		item = btrfs_item_nr(i);
 
 		ioff = btrfs_token_item_offset(leaf, item, &token);
 		btrfs_set_token_item_offset(leaf, item,
@@ -4387,7 +4387,7 @@ void btrfs_truncate_item(struct btrfs_root *root, struct btrfs_path *path,
 			fixup_low_keys(root, path, &disk_key, 1);
 	}
 
-	item = btrfs_item_nr(leaf, slot);
+	item = btrfs_item_nr(slot);
 	btrfs_set_item_size(leaf, item, new_size);
 	btrfs_mark_buffer_dirty(leaf);
 
@@ -4441,7 +4441,7 @@ void btrfs_extend_item(struct btrfs_root *root, struct btrfs_path *path,
 	/* first correct the data pointers */
 	for (i = slot; i < nritems; i++) {
 		u32 ioff;
-		item = btrfs_item_nr(leaf, i);
+		item = btrfs_item_nr(i);
 
 		ioff = btrfs_token_item_offset(leaf, item, &token);
 		btrfs_set_token_item_offset(leaf, item,
@@ -4455,7 +4455,7 @@ void btrfs_extend_item(struct btrfs_root *root, struct btrfs_path *path,
 
 	data_end = old_data;
 	old_size = btrfs_item_size_nr(leaf, slot);
-	item = btrfs_item_nr(leaf, slot);
+	item = btrfs_item_nr(slot);
 	btrfs_set_item_size(leaf, item, old_size + data_size);
 	btrfs_mark_buffer_dirty(leaf);
 
@@ -4514,7 +4514,7 @@ void setup_items_for_insert(struct btrfs_root *root, struct btrfs_path *path,
 		for (i = slot; i < nritems; i++) {
 			u32 ioff;
 
-			item = btrfs_item_nr(leaf, i);
+			item = btrfs_item_nr( i);
 			ioff = btrfs_token_item_offset(leaf, item, &token);
 			btrfs_set_token_item_offset(leaf, item,
 						    ioff - total_data, &token);
@@ -4535,7 +4535,7 @@ void setup_items_for_insert(struct btrfs_root *root, struct btrfs_path *path,
 	for (i = 0; i < nr; i++) {
 		btrfs_cpu_key_to_disk(&disk_key, cpu_key + i);
 		btrfs_set_item_key(leaf, &disk_key, slot + i);
-		item = btrfs_item_nr(leaf, slot + i);
+		item = btrfs_item_nr(slot + i);
 		btrfs_set_token_item_offset(leaf, item,
 					    data_end - data_size[i], &token);
 		data_end -= data_size[i];
@@ -4730,7 +4730,7 @@ int btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 		for (i = slot + nr; i < nritems; i++) {
 			u32 ioff;
 
-			item = btrfs_item_nr(leaf, i);
+			item = btrfs_item_nr(i);
 			ioff = btrfs_token_item_offset(leaf, item, &token);
 			btrfs_set_token_item_offset(leaf, item,
 						    ioff + dsize, &token);

commit 83d4cfd4da57b6ff16296875a962de2158799de6
Author: Josef Bacik <jbacik@fusionio.com>
Date:   Fri Aug 30 15:09:51 2013 -0400

    Btrfs: fixup error handling in btrfs_reloc_cow
    
    If we failed to actually allocate the correct size of the extent to relocate we
    will end up in an infinite loop because we won't return an error, we'll just
    move on to the next extent.  So fix this up by returning an error, and then fix
    all the callers to return an error up the stack rather than BUG_ON()'ing.
    Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 64346721173f..61b5bcd57b7e 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1005,8 +1005,11 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 		return ret;
 	}
 
-	if (root->ref_cows)
-		btrfs_reloc_cow_block(trans, root, buf, cow);
+	if (root->ref_cows) {
+		ret = btrfs_reloc_cow_block(trans, root, buf, cow);
+		if (ret)
+			return ret;
+	}
 
 	if (buf == root->node) {
 		WARN_ON(parent && parent != buf);

commit d7396f07358a7c6e22c238d36d1d85f9d652a414
Author: Filipe David Borba Manana <fdmanana@gmail.com>
Date:   Fri Aug 30 15:46:43 2013 +0100

    Btrfs: optimize key searches in btrfs_search_slot
    
    When the binary search returns 0 (exact match), the target key
    will necessarily be at slot 0 of all nodes below the current one,
    so in this case the binary search is not needed because it will
    always return 0, and we waste time doing it, holding node locks
    for longer than necessary, etc.
    
    Below follow histograms with the times spent on the current approach of
    doing a binary search when the previous binary search returned 0, and
    times for the new approach, which directly picks the first item/child
    node in the leaf/node.
    
    Current approach:
    
    Count: 6682
    Range: 35.000 - 8370.000; Mean: 85.837; Median: 75.000; Stddev: 106.429
    Percentiles:  90th: 124.000; 95th: 145.000; 99th: 206.000
      35.000 -   61.080:  1235 ################
      61.080 -  106.053:  4207 #####################################################
     106.053 -  183.606:  1122 ##############
     183.606 -  317.341:   111 #
     317.341 -  547.959:     6 |
     547.959 - 8370.000:     1 |
    
    Approach proposed by this patch:
    
    Count: 6682
    Range:  6.000 - 135.000; Mean: 16.690; Median: 16.000; Stddev:  7.160
    Percentiles:  90th: 23.000; 95th: 27.000; 99th: 40.000
       6.000 -    8.418:    58 #
       8.418 -   11.670:  1149 #########################
      11.670 -   16.046:  2418 #####################################################
      16.046 -   21.934:  2098 ##############################################
      21.934 -   29.854:   744 ################
      29.854 -   40.511:   154 ###
      40.511 -   54.848:    41 #
      54.848 -   74.136:     5 |
      74.136 -  100.087:     9 |
     100.087 -  135.000:     6 |
    
    These samples were captured during a run of the btrfs tests 001, 002 and
    004 in the xfstests, with a leaf/node size of 4Kb.
    
    Signed-off-by: Filipe David Borba Manana <fdmanana@gmail.com>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 5fa521bec07b..64346721173f 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2426,6 +2426,40 @@ setup_nodes_for_search(struct btrfs_trans_handle *trans,
 	return ret;
 }
 
+static void key_search_validate(struct extent_buffer *b,
+				struct btrfs_key *key,
+				int level)
+{
+#ifdef CONFIG_BTRFS_ASSERT
+	struct btrfs_disk_key disk_key;
+
+	btrfs_cpu_key_to_disk(&disk_key, key);
+
+	if (level == 0)
+		ASSERT(!memcmp_extent_buffer(b, &disk_key,
+		    offsetof(struct btrfs_leaf, items[0].key),
+		    sizeof(disk_key)));
+	else
+		ASSERT(!memcmp_extent_buffer(b, &disk_key,
+		    offsetof(struct btrfs_node, ptrs[0].key),
+		    sizeof(disk_key)));
+#endif
+}
+
+static int key_search(struct extent_buffer *b, struct btrfs_key *key,
+		      int level, int *prev_cmp, int *slot)
+{
+	if (*prev_cmp != 0) {
+		*prev_cmp = bin_search(b, key, level, slot);
+		return *prev_cmp;
+	}
+
+	key_search_validate(b, key, level);
+	*slot = 0;
+
+	return 0;
+}
+
 /*
  * look for key in the tree.  path is filled in with nodes along the way
  * if key is found, we return zero and you can find the item in the leaf
@@ -2454,6 +2488,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 	int write_lock_level = 0;
 	u8 lowest_level = 0;
 	int min_write_lock_level;
+	int prev_cmp;
 
 	lowest_level = p->lowest_level;
 	WARN_ON(lowest_level && ins_len > 0);
@@ -2484,6 +2519,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 	min_write_lock_level = write_lock_level;
 
 again:
+	prev_cmp = -1;
 	/*
 	 * we try very hard to do read locks on the root
 	 */
@@ -2584,7 +2620,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 		if (!cow)
 			btrfs_unlock_up_safe(p, level + 1);
 
-		ret = bin_search(b, key, level, &slot);
+		ret = key_search(b, key, level, &prev_cmp, &slot);
 
 		if (level != 0) {
 			int dec = 0;
@@ -2719,6 +2755,7 @@ int btrfs_search_old_slot(struct btrfs_root *root, struct btrfs_key *key,
 	int level;
 	int lowest_unlock = 1;
 	u8 lowest_level = 0;
+	int prev_cmp;
 
 	lowest_level = p->lowest_level;
 	WARN_ON(p->nodes[0] != NULL);
@@ -2729,6 +2766,7 @@ int btrfs_search_old_slot(struct btrfs_root *root, struct btrfs_key *key,
 	}
 
 again:
+	prev_cmp = -1;
 	b = get_old_root(root, time_seq);
 	level = btrfs_header_level(b);
 	p->locks[level] = BTRFS_READ_LOCK;
@@ -2746,7 +2784,7 @@ int btrfs_search_old_slot(struct btrfs_root *root, struct btrfs_key *key,
 		 */
 		btrfs_unlock_up_safe(p, level + 1);
 
-		ret = bin_search(b, key, level, &slot);
+		ret = key_search(b, key, level, &prev_cmp, &slot);
 
 		if (level != 0) {
 			int dec = 0;

commit b308bc2f05a86e728bd035e21a4974acd05f4d1e
Author: Geert Uytterhoeven <geert@linux-m68k.org>
Date:   Tue Aug 20 13:20:15 2013 +0200

    Btrfs: Make btrfs_header_chunk_tree_uuid() return unsigned long
    
    Internally, btrfs_header_chunk_tree_uuid() calculates an unsigned long, but
    casts it to a pointer, while all callers cast it to unsigned long again.
    
    Signed-off-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 8e3efe3bad24..5fa521bec07b 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -3111,8 +3111,7 @@ static noinline int insert_new_root(struct btrfs_trans_handle *trans,
 			    BTRFS_FSID_SIZE);
 
 	write_extent_buffer(c, root->fs_info->chunk_tree_uuid,
-			    (unsigned long)btrfs_header_chunk_tree_uuid(c),
-			    BTRFS_UUID_SIZE);
+			    btrfs_header_chunk_tree_uuid(c), BTRFS_UUID_SIZE);
 
 	btrfs_set_node_key(c, &lower_key, 0);
 	btrfs_set_node_blockptr(c, 0, lower->start);
@@ -3249,7 +3248,7 @@ static noinline int split_node(struct btrfs_trans_handle *trans,
 	write_extent_buffer(split, root->fs_info->fsid,
 			    btrfs_header_fsid(split), BTRFS_FSID_SIZE);
 	write_extent_buffer(split, root->fs_info->chunk_tree_uuid,
-			    (unsigned long)btrfs_header_chunk_tree_uuid(split),
+			    btrfs_header_chunk_tree_uuid(split),
 			    BTRFS_UUID_SIZE);
 
 	tree_mod_log_eb_copy(root->fs_info, split, c, 0, mid, c_nritems - mid);
@@ -4005,7 +4004,7 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 			    btrfs_header_fsid(right), BTRFS_FSID_SIZE);
 
 	write_extent_buffer(right, root->fs_info->chunk_tree_uuid,
-			    (unsigned long)btrfs_header_chunk_tree_uuid(right),
+			    btrfs_header_chunk_tree_uuid(right),
 			    BTRFS_UUID_SIZE);
 
 	if (split == 0) {

commit fba6aa75654394fccf2530041e9451414c28084f
Author: Geert Uytterhoeven <geert@linux-m68k.org>
Date:   Tue Aug 20 13:20:14 2013 +0200

    Btrfs: Make btrfs_header_fsid() return unsigned long
    
    Internally, btrfs_header_fsid() calculates an unsigned long, but casts
    it to a pointer, while all callers cast it to unsigned long again.
    
    Signed-off-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 1d94242ec002..8e3efe3bad24 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -274,8 +274,7 @@ int btrfs_copy_root(struct btrfs_trans_handle *trans,
 	else
 		btrfs_set_header_owner(cow, new_root_objectid);
 
-	write_extent_buffer(cow, root->fs_info->fsid,
-			    (unsigned long)btrfs_header_fsid(cow),
+	write_extent_buffer(cow, root->fs_info->fsid, btrfs_header_fsid(cow),
 			    BTRFS_FSID_SIZE);
 
 	WARN_ON(btrfs_header_generation(buf) > trans->transid);
@@ -997,8 +996,7 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 	else
 		btrfs_set_header_owner(cow, root->root_key.objectid);
 
-	write_extent_buffer(cow, root->fs_info->fsid,
-			    (unsigned long)btrfs_header_fsid(cow),
+	write_extent_buffer(cow, root->fs_info->fsid, btrfs_header_fsid(cow),
 			    BTRFS_FSID_SIZE);
 
 	ret = update_ref_for_cow(trans, root, buf, cow, &last_ref);
@@ -3109,8 +3107,7 @@ static noinline int insert_new_root(struct btrfs_trans_handle *trans,
 	btrfs_set_header_backref_rev(c, BTRFS_MIXED_BACKREF_REV);
 	btrfs_set_header_owner(c, root->root_key.objectid);
 
-	write_extent_buffer(c, root->fs_info->fsid,
-			    (unsigned long)btrfs_header_fsid(c),
+	write_extent_buffer(c, root->fs_info->fsid, btrfs_header_fsid(c),
 			    BTRFS_FSID_SIZE);
 
 	write_extent_buffer(c, root->fs_info->chunk_tree_uuid,
@@ -3250,8 +3247,7 @@ static noinline int split_node(struct btrfs_trans_handle *trans,
 	btrfs_set_header_backref_rev(split, BTRFS_MIXED_BACKREF_REV);
 	btrfs_set_header_owner(split, root->root_key.objectid);
 	write_extent_buffer(split, root->fs_info->fsid,
-			    (unsigned long)btrfs_header_fsid(split),
-			    BTRFS_FSID_SIZE);
+			    btrfs_header_fsid(split), BTRFS_FSID_SIZE);
 	write_extent_buffer(split, root->fs_info->chunk_tree_uuid,
 			    (unsigned long)btrfs_header_chunk_tree_uuid(split),
 			    BTRFS_UUID_SIZE);
@@ -4006,8 +4002,7 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 	btrfs_set_header_owner(right, root->root_key.objectid);
 	btrfs_set_header_level(right, 0);
 	write_extent_buffer(right, root->fs_info->fsid,
-			    (unsigned long)btrfs_header_fsid(right),
-			    BTRFS_FSID_SIZE);
+			    btrfs_header_fsid(right), BTRFS_FSID_SIZE);
 
 	write_extent_buffer(right, root->fs_info->chunk_tree_uuid,
 			    (unsigned long)btrfs_header_chunk_tree_uuid(right),

commit c1c9ff7c94e83fae89a742df74db51156869bad5
Author: Geert Uytterhoeven <geert@linux-m68k.org>
Date:   Tue Aug 20 13:20:07 2013 +0200

    Btrfs: Remove superfluous casts from u64 to unsigned long long
    
    u64 is "unsigned long long" on all architectures now, so there's no need to
    cast it when formatting it using the "ll" length modifier.
    
    Signed-off-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 09b3870c2729..1d94242ec002 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1383,14 +1383,12 @@ noinline int btrfs_cow_block(struct btrfs_trans_handle *trans,
 
 	if (trans->transaction != root->fs_info->running_transaction)
 		WARN(1, KERN_CRIT "trans %llu running %llu\n",
-		       (unsigned long long)trans->transid,
-		       (unsigned long long)
+		       trans->transid,
 		       root->fs_info->running_transaction->transid);
 
 	if (trans->transid != root->fs_info->generation)
 		WARN(1, KERN_CRIT "trans %llu running %llu\n",
-		       (unsigned long long)trans->transid,
-		       (unsigned long long)root->fs_info->generation);
+		       trans->transid, root->fs_info->generation);
 
 	if (!should_cow_block(trans, root, buf)) {
 		*cow_ret = buf;

commit 35a3621beb3e2face3e7954eaee20a8fa0043fac
Author: Stefan Behrens <sbehrens@giantdisaster.de>
Date:   Wed Aug 14 18:12:25 2013 +0200

    Btrfs: get rid of sparse warnings
    
    make C=2 fs/btrfs/ CF=-D__CHECK_ENDIAN__
    
    I tried to filter out the warnings for which patches have already
    been sent to the mailing list, pending for inclusion in btrfs-next.
    
    All these changes should be obviously safe.
    
    Signed-off-by: Stefan Behrens <sbehrens@giantdisaster.de>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 0708ebed2df7..09b3870c2729 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1067,7 +1067,7 @@ __tree_mod_log_oldest_root(struct btrfs_fs_info *fs_info,
 	int looped = 0;
 
 	if (!time_seq)
-		return 0;
+		return NULL;
 
 	/*
 	 * the very last operation that's logged for a root is the replacement
@@ -1078,7 +1078,7 @@ __tree_mod_log_oldest_root(struct btrfs_fs_info *fs_info,
 		tm = tree_mod_log_search_oldest(fs_info, root_logical,
 						time_seq);
 		if (!looped && !tm)
-			return 0;
+			return NULL;
 		/*
 		 * if there are no tree operation for the oldest root, we simply
 		 * return it. this should only happen if that (old) root is at
@@ -4782,7 +4782,7 @@ int btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,
  * This may release the path, and so you may lose any locks held at the
  * time you call it.
  */
-int btrfs_prev_leaf(struct btrfs_root *root, struct btrfs_path *path)
+static int btrfs_prev_leaf(struct btrfs_root *root, struct btrfs_path *path)
 {
 	struct btrfs_key key;
 	struct btrfs_disk_key found_key;

commit ba5e8f2e2d3074bf151dd222dae9bb400e621b82
Author: Josef Bacik <jbacik@fusionio.com>
Date:   Fri Aug 16 16:52:55 2013 -0400

    Btrfs: fix send issues related to inode number reuse
    
    If you are sending a snapshot and specifying a parent snapshot we will walk the
    trees and figure out where they differ and send the differences only.  The way
    we check for differences are if the leaves aren't the same and if the keys are
    not the same within the leaves.  So if neither leaf is the same (ie the leaf has
    been cow'ed from the parent snapshot) we walk each item in the send root and
    check it against the parent root.  If the items match exactly then we don't do
    anything.  This doesn't quite work for inode refs, since they will just have the
    name and the parent objectid.  If you move the file from a directory and then
    remove that directory and re-create a directory with the same inode number as
    the old directory and then move that file back into that directory we will
    assume that nothing changed and you will get errors when you try to receive.
    
    In order to fix this we need to do extra checking to see if the inode ref really
    is the same or not.  So do this by passing down BTRFS_COMPARE_TREE_SAME if the
    items match.  Then if the key type is an inode ref we can do some extra
    checking, otherwise we just keep processing.  The extra checking is to look up
    the generation of the directory in the parent volume and compare it to the
    generation of the send volume.  If they match then they are the same directory
    and we are good to go.  If they don't we have to add them to the changed refs
    list.
    
    This means we have to track the generation of the ref we're trying to lookup
    when we iterate all the refs for a particular inode.  So in the case of looking
    for new refs we have to get the generation from the parent volume, and in the
    case of looking for deleted refs we have to get the generation from the send
    volume to compare with.
    
    There was also the issue of using a ulist to keep track of the directories we
    needed to check.  Because we can get a deleted ref and a new ref for the same
    inode number the ulist won't work since it indexes based on the value.  So
    instead just dup any directory ref we find and add it to a local list, and then
    process that list as normal and do away with using a ulist for this altogether.
    
    Before we would fail all of the tests in the far-progs that related to moving
    directories (test group 32).  With this patch we now pass these tests, and all
    of the tests in the far-progs send testing suite.  Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 5f7a97556583..0708ebed2df7 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -5297,19 +5297,20 @@ int btrfs_compare_trees(struct btrfs_root *left_root,
 					goto out;
 				advance_right = ADVANCE;
 			} else {
+				enum btrfs_compare_tree_result cmp;
+
 				WARN_ON(!extent_buffer_uptodate(left_path->nodes[0]));
 				ret = tree_compare_item(left_root, left_path,
 						right_path, tmp_buf);
-				if (ret) {
-					WARN_ON(!extent_buffer_uptodate(left_path->nodes[0]));
-					ret = changed_cb(left_root, right_root,
-						left_path, right_path,
-						&left_key,
-						BTRFS_COMPARE_TREE_CHANGED,
-						ctx);
-					if (ret < 0)
-						goto out;
-				}
+				if (ret)
+					cmp = BTRFS_COMPARE_TREE_CHANGED;
+				else
+					cmp = BTRFS_COMPARE_TREE_SAME;
+				ret = changed_cb(left_root, right_root,
+						 left_path, right_path,
+						 &left_key, cmp, ctx);
+				if (ret < 0)
+					goto out;
 				advance_left = ADVANCE;
 				advance_right = ADVANCE;
 			}

commit 9ec726775188906192f78ab9187640afd81ab996
Author: Josef Bacik <jbacik@fusionio.com>
Date:   Wed Aug 7 16:57:23 2013 -0400

    Btrfs: stop using GFP_ATOMIC when allocating rewind ebs
    
    There is no reason we can't just set the path to blocking and then do normal
    GFP_NOFS allocations for these extent buffers.  Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 1dd8a71f567d..5f7a97556583 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1191,8 +1191,8 @@ __tree_mod_log_rewind(struct btrfs_fs_info *fs_info, struct extent_buffer *eb,
  * is freed (its refcount is decremented).
  */
 static struct extent_buffer *
-tree_mod_log_rewind(struct btrfs_fs_info *fs_info, struct extent_buffer *eb,
-		    u64 time_seq)
+tree_mod_log_rewind(struct btrfs_fs_info *fs_info, struct btrfs_path *path,
+		    struct extent_buffer *eb, u64 time_seq)
 {
 	struct extent_buffer *eb_rewin;
 	struct tree_mod_elem *tm;
@@ -1207,12 +1207,15 @@ tree_mod_log_rewind(struct btrfs_fs_info *fs_info, struct extent_buffer *eb,
 	if (!tm)
 		return eb;
 
+	btrfs_set_path_blocking(path);
+	btrfs_set_lock_blocking_rw(eb, BTRFS_READ_LOCK);
+
 	if (tm->op == MOD_LOG_KEY_REMOVE_WHILE_FREEING) {
 		BUG_ON(tm->slot != 0);
 		eb_rewin = alloc_dummy_extent_buffer(eb->start,
 						fs_info->tree_root->nodesize);
 		if (!eb_rewin) {
-			btrfs_tree_read_unlock(eb);
+			btrfs_tree_read_unlock_blocking(eb);
 			free_extent_buffer(eb);
 			return NULL;
 		}
@@ -1224,13 +1227,14 @@ tree_mod_log_rewind(struct btrfs_fs_info *fs_info, struct extent_buffer *eb,
 	} else {
 		eb_rewin = btrfs_clone_extent_buffer(eb);
 		if (!eb_rewin) {
-			btrfs_tree_read_unlock(eb);
+			btrfs_tree_read_unlock_blocking(eb);
 			free_extent_buffer(eb);
 			return NULL;
 		}
 	}
 
-	btrfs_tree_read_unlock(eb);
+	btrfs_clear_path_blocking(path, NULL, BTRFS_READ_LOCK);
+	btrfs_tree_read_unlock_blocking(eb);
 	free_extent_buffer(eb);
 
 	extent_buffer_get(eb_rewin);
@@ -1294,8 +1298,9 @@ get_old_root(struct btrfs_root *root, u64 time_seq)
 		free_extent_buffer(eb_root);
 		eb = alloc_dummy_extent_buffer(logical, root->nodesize);
 	} else {
+		btrfs_set_lock_blocking_rw(eb_root, BTRFS_READ_LOCK);
 		eb = btrfs_clone_extent_buffer(eb_root);
-		btrfs_tree_read_unlock(eb_root);
+		btrfs_tree_read_unlock_blocking(eb_root);
 		free_extent_buffer(eb_root);
 	}
 
@@ -2779,7 +2784,7 @@ int btrfs_search_old_slot(struct btrfs_root *root, struct btrfs_key *key,
 				btrfs_clear_path_blocking(p, b,
 							  BTRFS_READ_LOCK);
 			}
-			b = tree_mod_log_rewind(root->fs_info, b, time_seq);
+			b = tree_mod_log_rewind(root->fs_info, p, b, time_seq);
 			if (!b) {
 				ret = -ENOMEM;
 				goto done;

commit db7f3436c1c186f8271018751fcb338cf3706e8d
Author: Josef Bacik <jbacik@fusionio.com>
Date:   Wed Aug 7 14:54:37 2013 -0400

    Btrfs: deal with enomem in the rewind path
    
    We can get ENOMEM trying to allocate dummy bufs for the rewind operation of the
    tree mod log.  Instead of BUG_ON()'ing in this case pass up ENOMEM.  I looked
    back through the callers and I'm pretty sure I got everybody who did BUG_ON(ret)
    in this path.  Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 0d5c686f2b98..1dd8a71f567d 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1211,7 +1211,11 @@ tree_mod_log_rewind(struct btrfs_fs_info *fs_info, struct extent_buffer *eb,
 		BUG_ON(tm->slot != 0);
 		eb_rewin = alloc_dummy_extent_buffer(eb->start,
 						fs_info->tree_root->nodesize);
-		BUG_ON(!eb_rewin);
+		if (!eb_rewin) {
+			btrfs_tree_read_unlock(eb);
+			free_extent_buffer(eb);
+			return NULL;
+		}
 		btrfs_set_header_bytenr(eb_rewin, eb->start);
 		btrfs_set_header_backref_rev(eb_rewin,
 					     btrfs_header_backref_rev(eb));
@@ -1219,7 +1223,11 @@ tree_mod_log_rewind(struct btrfs_fs_info *fs_info, struct extent_buffer *eb,
 		btrfs_set_header_level(eb_rewin, btrfs_header_level(eb));
 	} else {
 		eb_rewin = btrfs_clone_extent_buffer(eb);
-		BUG_ON(!eb_rewin);
+		if (!eb_rewin) {
+			btrfs_tree_read_unlock(eb);
+			free_extent_buffer(eb);
+			return NULL;
+		}
 	}
 
 	btrfs_tree_read_unlock(eb);
@@ -2772,6 +2780,10 @@ int btrfs_search_old_slot(struct btrfs_root *root, struct btrfs_key *key,
 							  BTRFS_READ_LOCK);
 			}
 			b = tree_mod_log_rewind(root->fs_info, b, time_seq);
+			if (!b) {
+				ret = -ENOMEM;
+				goto done;
+			}
 			p->locks[level] = BTRFS_READ_LOCK;
 			p->nodes[level] = b;
 		} else {

commit c8cc6341653721b54760480b0d0d9b5f09b46741
Author: Josef Bacik <jbacik@fusionio.com>
Date:   Mon Jul 1 16:18:19 2013 -0400

    Btrfs: stop using GFP_ATOMIC for the tree mod log allocations
    
    Previously we held the tree mod lock when adding stuff because we use it to
    check and see if we truly do want to track tree modifications.  This is
    admirable, but GFP_ATOMIC in a critical area that is going to get hit pretty
    hard and often is not nice.  So instead do our basic checks to see if we don't
    need to track modifications, and if those pass then do our allocation, and then
    when we go to insert the new modification check if we still care, and if we
    don't just free up our mod and return.  Otherwise we're good to go and we can
    carry on.  Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index ed504607d8ec..0d5c686f2b98 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -484,8 +484,27 @@ __tree_mod_log_insert(struct btrfs_fs_info *fs_info, struct tree_mod_elem *tm)
 	struct rb_node **new;
 	struct rb_node *parent = NULL;
 	struct tree_mod_elem *cur;
+	int ret = 0;
+
+	BUG_ON(!tm);
+
+	tree_mod_log_write_lock(fs_info);
+	if (list_empty(&fs_info->tree_mod_seq_list)) {
+		tree_mod_log_write_unlock(fs_info);
+		/*
+		 * Ok we no longer care about logging modifications, free up tm
+		 * and return 0.  Any callers shouldn't be using tm after
+		 * calling tree_mod_log_insert, but if they do we can just
+		 * change this to return a special error code to let the callers
+		 * do their own thing.
+		 */
+		kfree(tm);
+		return 0;
+	}
 
-	BUG_ON(!tm || !tm->seq);
+	spin_lock(&fs_info->tree_mod_seq_lock);
+	tm->seq = btrfs_inc_tree_mod_seq_minor(fs_info);
+	spin_unlock(&fs_info->tree_mod_seq_lock);
 
 	tm_root = &fs_info->tree_mod_log;
 	new = &tm_root->rb_node;
@@ -501,14 +520,17 @@ __tree_mod_log_insert(struct btrfs_fs_info *fs_info, struct tree_mod_elem *tm)
 		else if (cur->seq > tm->seq)
 			new = &((*new)->rb_right);
 		else {
+			ret = -EEXIST;
 			kfree(tm);
-			return -EEXIST;
+			goto out;
 		}
 	}
 
 	rb_link_node(&tm->node, parent, new);
 	rb_insert_color(&tm->node, tm_root);
-	return 0;
+out:
+	tree_mod_log_write_unlock(fs_info);
+	return ret;
 }
 
 /*
@@ -524,57 +546,19 @@ static inline int tree_mod_dont_log(struct btrfs_fs_info *fs_info,
 		return 1;
 	if (eb && btrfs_header_level(eb) == 0)
 		return 1;
-
-	tree_mod_log_write_lock(fs_info);
-	if (list_empty(&fs_info->tree_mod_seq_list)) {
-		/*
-		 * someone emptied the list while we were waiting for the lock.
-		 * we must not add to the list when no blocker exists.
-		 */
-		tree_mod_log_write_unlock(fs_info);
-		return 1;
-	}
-
 	return 0;
 }
 
-/*
- * This allocates memory and gets a tree modification sequence number.
- *
- * Returns <0 on error.
- * Returns >0 (the added sequence number) on success.
- */
-static inline int tree_mod_alloc(struct btrfs_fs_info *fs_info, gfp_t flags,
-				 struct tree_mod_elem **tm_ret)
-{
-	struct tree_mod_elem *tm;
-
-	/*
-	 * once we switch from spin locks to something different, we should
-	 * honor the flags parameter here.
-	 */
-	tm = *tm_ret = kzalloc(sizeof(*tm), GFP_ATOMIC);
-	if (!tm)
-		return -ENOMEM;
-
-	spin_lock(&fs_info->tree_mod_seq_lock);
-	tm->seq = btrfs_inc_tree_mod_seq_minor(fs_info);
-	spin_unlock(&fs_info->tree_mod_seq_lock);
-
-	return tm->seq;
-}
-
 static inline int
 __tree_mod_log_insert_key(struct btrfs_fs_info *fs_info,
 			  struct extent_buffer *eb, int slot,
 			  enum mod_log_op op, gfp_t flags)
 {
-	int ret;
 	struct tree_mod_elem *tm;
 
-	ret = tree_mod_alloc(fs_info, flags, &tm);
-	if (ret < 0)
-		return ret;
+	tm = kzalloc(sizeof(*tm), flags);
+	if (!tm)
+		return -ENOMEM;
 
 	tm->index = eb->start >> PAGE_CACHE_SHIFT;
 	if (op != MOD_LOG_KEY_ADD) {
@@ -589,34 +573,14 @@ __tree_mod_log_insert_key(struct btrfs_fs_info *fs_info,
 }
 
 static noinline int
-tree_mod_log_insert_key_mask(struct btrfs_fs_info *fs_info,
-			     struct extent_buffer *eb, int slot,
-			     enum mod_log_op op, gfp_t flags)
+tree_mod_log_insert_key(struct btrfs_fs_info *fs_info,
+			struct extent_buffer *eb, int slot,
+			enum mod_log_op op, gfp_t flags)
 {
-	int ret;
-
 	if (tree_mod_dont_log(fs_info, eb))
 		return 0;
 
-	ret = __tree_mod_log_insert_key(fs_info, eb, slot, op, flags);
-
-	tree_mod_log_write_unlock(fs_info);
-	return ret;
-}
-
-static noinline int
-tree_mod_log_insert_key(struct btrfs_fs_info *fs_info, struct extent_buffer *eb,
-			int slot, enum mod_log_op op)
-{
-	return tree_mod_log_insert_key_mask(fs_info, eb, slot, op, GFP_NOFS);
-}
-
-static noinline int
-tree_mod_log_insert_key_locked(struct btrfs_fs_info *fs_info,
-			     struct extent_buffer *eb, int slot,
-			     enum mod_log_op op)
-{
-	return __tree_mod_log_insert_key(fs_info, eb, slot, op, GFP_NOFS);
+	return __tree_mod_log_insert_key(fs_info, eb, slot, op, flags);
 }
 
 static noinline int
@@ -637,14 +601,14 @@ tree_mod_log_insert_move(struct btrfs_fs_info *fs_info,
 	 * buffer, i.e. dst_slot < src_slot.
 	 */
 	for (i = 0; i + dst_slot < src_slot && i < nr_items; i++) {
-		ret = tree_mod_log_insert_key_locked(fs_info, eb, i + dst_slot,
-					      MOD_LOG_KEY_REMOVE_WHILE_MOVING);
+		ret = __tree_mod_log_insert_key(fs_info, eb, i + dst_slot,
+				MOD_LOG_KEY_REMOVE_WHILE_MOVING, GFP_NOFS);
 		BUG_ON(ret < 0);
 	}
 
-	ret = tree_mod_alloc(fs_info, flags, &tm);
-	if (ret < 0)
-		goto out;
+	tm = kzalloc(sizeof(*tm), flags);
+	if (!tm)
+		return -ENOMEM;
 
 	tm->index = eb->start >> PAGE_CACHE_SHIFT;
 	tm->slot = src_slot;
@@ -652,10 +616,7 @@ tree_mod_log_insert_move(struct btrfs_fs_info *fs_info,
 	tm->move.nr_items = nr_items;
 	tm->op = MOD_LOG_MOVE_KEYS;
 
-	ret = __tree_mod_log_insert(fs_info, tm);
-out:
-	tree_mod_log_write_unlock(fs_info);
-	return ret;
+	return __tree_mod_log_insert(fs_info, tm);
 }
 
 static inline void
@@ -670,8 +631,8 @@ __tree_mod_log_free_eb(struct btrfs_fs_info *fs_info, struct extent_buffer *eb)
 
 	nritems = btrfs_header_nritems(eb);
 	for (i = nritems - 1; i >= 0; i--) {
-		ret = tree_mod_log_insert_key_locked(fs_info, eb, i,
-					      MOD_LOG_KEY_REMOVE_WHILE_FREEING);
+		ret = __tree_mod_log_insert_key(fs_info, eb, i,
+				MOD_LOG_KEY_REMOVE_WHILE_FREEING, GFP_NOFS);
 		BUG_ON(ret < 0);
 	}
 }
@@ -683,7 +644,6 @@ tree_mod_log_insert_root(struct btrfs_fs_info *fs_info,
 			 int log_removal)
 {
 	struct tree_mod_elem *tm;
-	int ret;
 
 	if (tree_mod_dont_log(fs_info, NULL))
 		return 0;
@@ -691,9 +651,9 @@ tree_mod_log_insert_root(struct btrfs_fs_info *fs_info,
 	if (log_removal)
 		__tree_mod_log_free_eb(fs_info, old_root);
 
-	ret = tree_mod_alloc(fs_info, flags, &tm);
-	if (ret < 0)
-		goto out;
+	tm = kzalloc(sizeof(*tm), flags);
+	if (!tm)
+		return -ENOMEM;
 
 	tm->index = new_root->start >> PAGE_CACHE_SHIFT;
 	tm->old_root.logical = old_root->start;
@@ -701,10 +661,7 @@ tree_mod_log_insert_root(struct btrfs_fs_info *fs_info,
 	tm->generation = btrfs_header_generation(old_root);
 	tm->op = MOD_LOG_ROOT_REPLACE;
 
-	ret = __tree_mod_log_insert(fs_info, tm);
-out:
-	tree_mod_log_write_unlock(fs_info);
-	return ret;
+	return __tree_mod_log_insert(fs_info, tm);
 }
 
 static struct tree_mod_elem *
@@ -784,23 +741,20 @@ tree_mod_log_eb_copy(struct btrfs_fs_info *fs_info, struct extent_buffer *dst,
 	if (tree_mod_dont_log(fs_info, NULL))
 		return;
 
-	if (btrfs_header_level(dst) == 0 && btrfs_header_level(src) == 0) {
-		tree_mod_log_write_unlock(fs_info);
+	if (btrfs_header_level(dst) == 0 && btrfs_header_level(src) == 0)
 		return;
-	}
 
 	for (i = 0; i < nr_items; i++) {
-		ret = tree_mod_log_insert_key_locked(fs_info, src,
+		ret = __tree_mod_log_insert_key(fs_info, src,
 						i + src_offset,
-						MOD_LOG_KEY_REMOVE);
+						MOD_LOG_KEY_REMOVE, GFP_NOFS);
 		BUG_ON(ret < 0);
-		ret = tree_mod_log_insert_key_locked(fs_info, dst,
+		ret = __tree_mod_log_insert_key(fs_info, dst,
 						     i + dst_offset,
-						     MOD_LOG_KEY_ADD);
+						     MOD_LOG_KEY_ADD,
+						     GFP_NOFS);
 		BUG_ON(ret < 0);
 	}
-
-	tree_mod_log_write_unlock(fs_info);
 }
 
 static inline void
@@ -819,9 +773,9 @@ tree_mod_log_set_node_key(struct btrfs_fs_info *fs_info,
 {
 	int ret;
 
-	ret = tree_mod_log_insert_key_mask(fs_info, eb, slot,
-					   MOD_LOG_KEY_REPLACE,
-					   atomic ? GFP_ATOMIC : GFP_NOFS);
+	ret = __tree_mod_log_insert_key(fs_info, eb, slot,
+					MOD_LOG_KEY_REPLACE,
+					atomic ? GFP_ATOMIC : GFP_NOFS);
 	BUG_ON(ret < 0);
 }
 
@@ -830,10 +784,7 @@ tree_mod_log_free_eb(struct btrfs_fs_info *fs_info, struct extent_buffer *eb)
 {
 	if (tree_mod_dont_log(fs_info, eb))
 		return;
-
 	__tree_mod_log_free_eb(fs_info, eb);
-
-	tree_mod_log_write_unlock(fs_info);
 }
 
 static noinline void
@@ -1083,7 +1034,7 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 
 		WARN_ON(trans->transid != btrfs_header_generation(parent));
 		tree_mod_log_insert_key(root->fs_info, parent, parent_slot,
-					MOD_LOG_KEY_REPLACE);
+					MOD_LOG_KEY_REPLACE, GFP_NOFS);
 		btrfs_set_node_blockptr(parent, parent_slot,
 					cow->start);
 		btrfs_set_node_ptr_generation(parent, parent_slot,
@@ -3208,7 +3159,7 @@ static void insert_ptr(struct btrfs_trans_handle *trans,
 	}
 	if (level) {
 		ret = tree_mod_log_insert_key(root->fs_info, lower, slot,
-					      MOD_LOG_KEY_ADD);
+					      MOD_LOG_KEY_ADD, GFP_NOFS);
 		BUG_ON(ret < 0);
 	}
 	btrfs_set_node_key(lower, key, slot);
@@ -4642,7 +4593,7 @@ static void del_ptr(struct btrfs_root *root, struct btrfs_path *path,
 			      (nritems - slot - 1));
 	} else if (level) {
 		ret = tree_mod_log_insert_key(root->fs_info, parent, slot,
-					      MOD_LOG_KEY_REMOVE);
+					      MOD_LOG_KEY_REMOVE, GFP_NOFS);
 		BUG_ON(ret < 0);
 	}
 

commit b5b9b5b318f9b6fef1bd3e2c8c63d0bff47703a1
Author: Liu Bo <bo.li.liu@oracle.com>
Date:   Wed Jul 3 14:40:44 2013 +0800

    Btrfs: fix extent buffer leak after backref walking
    
    commit 47fb091fb787420cd195e66f162737401cce023f(Btrfs: fix unlock after free on rewinded tree blocks)
    takes an extra increment on the reference of allocated dummy extent buffer, so now we
    cannot free this dummy one, and end up with extent buffer leak.
    
    Signed-off-by: Liu Bo <bo.li.liu@oracle.com>
    Reviewed-by: Jan Schmidt <list.btrfs@jan-o-sch.net>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 5bf4c39e2ad6..ed504607d8ec 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1271,7 +1271,6 @@ tree_mod_log_rewind(struct btrfs_fs_info *fs_info, struct extent_buffer *eb,
 		BUG_ON(!eb_rewin);
 	}
 
-	extent_buffer_get(eb_rewin);
 	btrfs_tree_read_unlock(eb);
 	free_extent_buffer(eb);
 

commit 7fb7d76f96bfcbea25007d190ba828b18e13d29d
Author: Josef Bacik <jbacik@fusionio.com>
Date:   Mon Jul 1 16:10:16 2013 -0400

    Btrfs: only do the tree_mod_log_free_eb if this is our last ref
    
    There is another bug in the tree mod log stuff in that we're calling
    tree_mod_log_free_eb every single time a block is cow'ed.  The problem with this
    is that if this block is shared by multiple snapshots we will call this multiple
    times per block, so if we go to rewind the mod log for this block we'll BUG_ON()
    in __tree_mod_log_rewind because we try to rewind a free twice.  We only want to
    call tree_mod_log_free_eb if we are actually freeing the block.  With this patch
    I no longer hit the panic in __tree_mod_log_rewind.  Thanks,
    
    Cc: stable@vger.kernel.org
    Reviewed-by: Jan Schmidt <list.btrfs@jan-o-sch.net>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 7921e1d9d59c..5bf4c39e2ad6 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1089,7 +1089,8 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 		btrfs_set_node_ptr_generation(parent, parent_slot,
 					      trans->transid);
 		btrfs_mark_buffer_dirty(parent);
-		tree_mod_log_free_eb(root->fs_info, buf);
+		if (last_ref)
+			tree_mod_log_free_eb(root->fs_info, buf);
 		btrfs_free_tree_block(trans, root, buf, parent_start,
 				      last_ref);
 	}

commit f1ca7e98a67da618d8595866e0860308525154da
Author: Josef Bacik <jbacik@fusionio.com>
Date:   Sat Jun 29 23:15:19 2013 -0400

    Btrfs: hold the tree mod lock in __tree_mod_log_rewind
    
    We need to hold the tree mod log lock in __tree_mod_log_rewind since we walk
    forward in the tree mod entries, otherwise we'll end up with random entries and
    trip the BUG_ON() at the front of __tree_mod_log_rewind.  This fixes the panics
    people were seeing when running
    
    find /whatever -type f -exec btrfs fi defrag {} \;
    
    Thansk,
    
    Cc: stable@vger.kernel.org
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index c32d03dff4fc..7921e1d9d59c 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1161,8 +1161,8 @@ __tree_mod_log_oldest_root(struct btrfs_fs_info *fs_info,
  * time_seq).
  */
 static void
-__tree_mod_log_rewind(struct extent_buffer *eb, u64 time_seq,
-		      struct tree_mod_elem *first_tm)
+__tree_mod_log_rewind(struct btrfs_fs_info *fs_info, struct extent_buffer *eb,
+		      u64 time_seq, struct tree_mod_elem *first_tm)
 {
 	u32 n;
 	struct rb_node *next;
@@ -1172,6 +1172,7 @@ __tree_mod_log_rewind(struct extent_buffer *eb, u64 time_seq,
 	unsigned long p_size = sizeof(struct btrfs_key_ptr);
 
 	n = btrfs_header_nritems(eb);
+	tree_mod_log_read_lock(fs_info);
 	while (tm && tm->seq >= time_seq) {
 		/*
 		 * all the operations are recorded with the operator used for
@@ -1226,6 +1227,7 @@ __tree_mod_log_rewind(struct extent_buffer *eb, u64 time_seq,
 		if (tm->index != first_tm->index)
 			break;
 	}
+	tree_mod_log_read_unlock(fs_info);
 	btrfs_set_header_nritems(eb, n);
 }
 
@@ -1274,7 +1276,7 @@ tree_mod_log_rewind(struct btrfs_fs_info *fs_info, struct extent_buffer *eb,
 
 	extent_buffer_get(eb_rewin);
 	btrfs_tree_read_lock(eb_rewin);
-	__tree_mod_log_rewind(eb_rewin, time_seq, tm);
+	__tree_mod_log_rewind(fs_info, eb_rewin, time_seq, tm);
 	WARN_ON(btrfs_header_nritems(eb_rewin) >
 		BTRFS_NODEPTRS_PER_BLOCK(fs_info->tree_root));
 
@@ -1350,7 +1352,7 @@ get_old_root(struct btrfs_root *root, u64 time_seq)
 		btrfs_set_header_generation(eb, old_generation);
 	}
 	if (tm)
-		__tree_mod_log_rewind(eb, time_seq, tm);
+		__tree_mod_log_rewind(root->fs_info, eb, time_seq, tm);
 	else
 		WARN_ON(btrfs_header_level(eb) != 0);
 	WARN_ON(btrfs_header_nritems(eb) > BTRFS_NODEPTRS_PER_BLOCK(root));

commit 0b08851fdaa5f9f74357345d7be44ea584665d5f
Author: Josef Bacik <jbacik@fusionio.com>
Date:   Mon Jun 17 14:23:02 2013 -0400

    Btrfs: optimize reada_for_balance
    
    This patch does two things.  First we no longer explicitly read in the blocks
    we're trying to readahead.  For things like balance_level we may never actually
    use the blocks so this just adds uneeded latency, and balance_level and
    split_node will both read in the blocks they care about explicitly so if the
    blocks need to be waited on it will be done there.  Secondly we no longer drop
    the path if we do readahead, we just set the path blocking before we call
    reada_for_balance() and then we're good to go.  Hopefully this will cut down on
    the number of re-searches.  Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index c85cde761248..c32d03dff4fc 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2178,12 +2178,8 @@ static void reada_for_search(struct btrfs_root *root,
 	}
 }
 
-/*
- * returns -EAGAIN if it had to drop the path, or zero if everything was in
- * cache
- */
-static noinline int reada_for_balance(struct btrfs_root *root,
-				      struct btrfs_path *path, int level)
+static noinline void reada_for_balance(struct btrfs_root *root,
+				       struct btrfs_path *path, int level)
 {
 	int slot;
 	int nritems;
@@ -2192,12 +2188,11 @@ static noinline int reada_for_balance(struct btrfs_root *root,
 	u64 gen;
 	u64 block1 = 0;
 	u64 block2 = 0;
-	int ret = 0;
 	int blocksize;
 
 	parent = path->nodes[level + 1];
 	if (!parent)
-		return 0;
+		return;
 
 	nritems = btrfs_header_nritems(parent);
 	slot = path->slots[level + 1];
@@ -2224,28 +2219,11 @@ static noinline int reada_for_balance(struct btrfs_root *root,
 			block2 = 0;
 		free_extent_buffer(eb);
 	}
-	if (block1 || block2) {
-		ret = -EAGAIN;
-
-		/* release the whole path */
-		btrfs_release_path(path);
 
-		/* read the blocks */
-		if (block1)
-			readahead_tree_block(root, block1, blocksize, 0);
-		if (block2)
-			readahead_tree_block(root, block2, blocksize, 0);
-
-		if (block1) {
-			eb = read_tree_block(root, block1, blocksize, 0);
-			free_extent_buffer(eb);
-		}
-		if (block2) {
-			eb = read_tree_block(root, block2, blocksize, 0);
-			free_extent_buffer(eb);
-		}
-	}
-	return ret;
+	if (block1)
+		readahead_tree_block(root, block1, blocksize, 0);
+	if (block2)
+		readahead_tree_block(root, block2, blocksize, 0);
 }
 
 
@@ -2441,11 +2419,8 @@ setup_nodes_for_search(struct btrfs_trans_handle *trans,
 			goto again;
 		}
 
-		sret = reada_for_balance(root, p, level);
-		if (sret)
-			goto again;
-
 		btrfs_set_path_blocking(p);
+		reada_for_balance(root, p, level);
 		sret = split_node(trans, root, p, level);
 		btrfs_clear_path_blocking(p, NULL, 0);
 
@@ -2465,11 +2440,8 @@ setup_nodes_for_search(struct btrfs_trans_handle *trans,
 			goto again;
 		}
 
-		sret = reada_for_balance(root, p, level);
-		if (sret)
-			goto again;
-
 		btrfs_set_path_blocking(p);
+		reada_for_balance(root, p, level);
 		sret = balance_level(trans, root, p, level);
 		btrfs_clear_path_blocking(p, NULL, 0);
 

commit bdf7c00e8f56386dd1df545b37cf59d55ce4216d
Author: Josef Bacik <jbacik@fusionio.com>
Date:   Mon Jun 17 13:44:48 2013 -0400

    Btrfs: optimize read_block_for_search
    
    This patch does two things, first it only does one call to
    btrfs_buffer_uptodate() with the gen specified instead of once with 0 and then
    again with gen specified.  The other thing is to call btrfs_read_buffer() on the
    buffer we've found instead of dropping it and then calling read_tree_block().
    This will keep us from doing yet another radix tree lookup for a buffer we've
    already found.  Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 1c9dc71089ce..c85cde761248 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2359,35 +2359,28 @@ read_block_for_search(struct btrfs_trans_handle *trans,
 	tmp = btrfs_find_tree_block(root, blocknr, blocksize);
 	if (tmp) {
 		/* first we do an atomic uptodate check */
-		if (btrfs_buffer_uptodate(tmp, 0, 1) > 0) {
-			if (btrfs_buffer_uptodate(tmp, gen, 1) > 0) {
-				/*
-				 * we found an up to date block without
-				 * sleeping, return
-				 * right away
-				 */
-				*eb_ret = tmp;
-				return 0;
-			}
-			/* the pages were up to date, but we failed
-			 * the generation number check.  Do a full
-			 * read for the generation number that is correct.
-			 * We must do this without dropping locks so
-			 * we can trust our generation number
-			 */
-			free_extent_buffer(tmp);
-			btrfs_set_path_blocking(p);
+		if (btrfs_buffer_uptodate(tmp, gen, 1) > 0) {
+			*eb_ret = tmp;
+			return 0;
+		}
 
-			/* now we're allowed to do a blocking uptodate check */
-			tmp = read_tree_block(root, blocknr, blocksize, gen);
-			if (tmp && btrfs_buffer_uptodate(tmp, gen, 0) > 0) {
-				*eb_ret = tmp;
-				return 0;
-			}
-			free_extent_buffer(tmp);
-			btrfs_release_path(p);
-			return -EIO;
+		/* the pages were up to date, but we failed
+		 * the generation number check.  Do a full
+		 * read for the generation number that is correct.
+		 * We must do this without dropping locks so
+		 * we can trust our generation number
+		 */
+		btrfs_set_path_blocking(p);
+
+		/* now we're allowed to do a blocking uptodate check */
+		ret = btrfs_read_buffer(tmp, gen);
+		if (!ret) {
+			*eb_ret = tmp;
+			return 0;
 		}
+		free_extent_buffer(tmp);
+		btrfs_release_path(p);
+		return -EIO;
 	}
 
 	/*

commit 33157e05dbd60c3edd30b56d4dfc7755cae787c5
Author: Liu Bo <bo.li.liu@oracle.com>
Date:   Wed May 22 12:07:06 2013 +0000

    Btrfs: check if leaf's parent exists before pushing items around
    
    During splitting a leaf, pushing items around to hopefully get some space only
    works when we have a parent, ie. we have at least one sibling leaf.
    
    Signed-off-by: Liu Bo <bo.li.liu@oracle.com>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 0a430f7c5afd..1c9dc71089ce 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -3986,7 +3986,7 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 		return -EOVERFLOW;
 
 	/* first try to make some room by pushing left and right */
-	if (data_size) {
+	if (data_size && path->nodes[1]) {
 		wret = push_leaf_right(trans, root, path, data_size,
 				       data_size, 0, 0);
 		if (wret < 0)

commit fdd99c729433ed6ab8d02aa4760c6708a6c5de2a
Author: Liu Bo <bo.li.liu@oracle.com>
Date:   Wed May 22 12:06:51 2013 +0000

    Btrfs: dont do log_removal in insert_new_root
    
    As for splitting a leaf, root is just the leaf, and tree mod log does not apply
    on leaf, so in this case, we don't do log_removal.
    
    As for splitting a node, the old root is kept as a normal node and we have nicely
    put records in tree mod log for moving keys and items, so in this case we don't do
    that either.
    
    As above, insert_new_root can get rid of log_removal.
    
    Signed-off-by: Liu Bo <bo.li.liu@oracle.com>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 17dffe33e8d0..0a430f7c5afd 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -3143,7 +3143,7 @@ static int balance_node_right(struct btrfs_trans_handle *trans,
  */
 static noinline int insert_new_root(struct btrfs_trans_handle *trans,
 			   struct btrfs_root *root,
-			   struct btrfs_path *path, int level, int log_removal)
+			   struct btrfs_path *path, int level)
 {
 	u64 lower_gen;
 	struct extent_buffer *lower;
@@ -3194,7 +3194,7 @@ static noinline int insert_new_root(struct btrfs_trans_handle *trans,
 	btrfs_mark_buffer_dirty(c);
 
 	old = root->node;
-	tree_mod_log_set_root_pointer(root, c, log_removal);
+	tree_mod_log_set_root_pointer(root, c, 0);
 	rcu_assign_pointer(root->node, c);
 
 	/* the super has an extra ref to root->node */
@@ -3278,14 +3278,14 @@ static noinline int split_node(struct btrfs_trans_handle *trans,
 		/*
 		 * trying to split the root, lets make a new one
 		 *
-		 * tree mod log: We pass 0 as log_removal parameter to
+		 * tree mod log: We don't log_removal old root in
 		 * insert_new_root, because that root buffer will be kept as a
 		 * normal node. We are going to log removal of half of the
 		 * elements below with tree_mod_log_eb_copy. We're holding a
 		 * tree lock on the buffer, which is why we cannot race with
 		 * other tree_mod_log users.
 		 */
-		ret = insert_new_root(trans, root, path, level + 1, 0);
+		ret = insert_new_root(trans, root, path, level + 1);
 		if (ret)
 			return ret;
 	} else {
@@ -4005,7 +4005,7 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 	}
 
 	if (!path->nodes[1]) {
-		ret = insert_new_root(trans, root, path, 1, 1);
+		ret = insert_new_root(trans, root, path, 1);
 		if (ret)
 			return ret;
 	}

commit 8f69dbd236c1dc265c0fcd66e85a02712d5b9035
Author: Stefan Behrens <sbehrens@giantdisaster.de>
Date:   Tue May 7 10:23:30 2013 +0000

    Btrfs: fix a comment
    
    The size parameter to btrfs_extend_item() is the number of bytes
    to add to the item, not the size of the item after the operation
    (like it is for btrfs_truncate_item(), there the size parameter
    is not the number of bytes to take away, but the total size of
    the item after truncation).
    Fix it in the comment.
    
    Signed-off-by: Stefan Behrens <sbehrens@giantdisaster.de>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 02fae7f7e42c..17dffe33e8d0 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -4430,7 +4430,7 @@ void btrfs_truncate_item(struct btrfs_root *root, struct btrfs_path *path,
 }
 
 /*
- * make the item pointed to by the path bigger, data_size is the new size.
+ * make the item pointed to by the path bigger, data_size is the added size.
  */
 void btrfs_extend_item(struct btrfs_root *root, struct btrfs_path *path,
 		       u32 data_size)

commit b1c79e0947e0c190f865e2eb7b84a0fea0021cec
Author: Josef Bacik <jbacik@fusionio.com>
Date:   Thu May 9 13:49:30 2013 -0400

    Btrfs: handle running extent ops with skinny metadata
    
    Chris hit a bug where we weren't finding extent records when running extent ops.
    This is because we use the delayed_ref_head when running the extent op, which
    means we can't use the ->type checks to see if we are metadata.  We also lose
    the level of the metadata we are working on.  So to fix this we can just check
    the ->is_data section of the extent_op, and we can store the level of the buffer
    we were modifying in the extent_op.  Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index de6de8e60b46..02fae7f7e42c 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -951,10 +951,12 @@ static noinline int update_ref_for_cow(struct btrfs_trans_handle *trans,
 			BUG_ON(ret); /* -ENOMEM */
 		}
 		if (new_flags != 0) {
+			int level = btrfs_header_level(buf);
+
 			ret = btrfs_set_disk_extent_flags(trans, root,
 							  buf->start,
 							  buf->len,
-							  new_flags, 0);
+							  new_flags, level, 0);
 			if (ret)
 				return ret;
 		}

commit 48a3b6366f6913683563d934eb16fea67dead9c1
Author: Eric Sandeen <sandeen@redhat.com>
Date:   Thu Apr 25 20:41:01 2013 +0000

    btrfs: make static code static & remove dead code
    
    Big patch, but all it does is add statics to functions which
    are in fact static, then remove the associated dead-code fallout.
    
    removed functions:
    
    btrfs_iref_to_path()
    __btrfs_lookup_delayed_deletion_item()
    __btrfs_search_delayed_insertion_item()
    __btrfs_search_delayed_deletion_item()
    find_eb_for_page()
    btrfs_find_block_group()
    range_straddles_pages()
    extent_range_uptodate()
    btrfs_file_extent_length()
    btrfs_scrub_cancel_devid()
    btrfs_start_transaction_lflush()
    
    btrfs_print_tree() is left because it is used for debugging.
    btrfs_start_transaction_lflush() and btrfs_reada_detach() are
    left for symmetry.
    
    ulist.c functions are left, another patch will take care of those.
    
    Signed-off-by: Eric Sandeen <sandeen@redhat.com>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index a17d9991c333..de6de8e60b46 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -41,12 +41,7 @@ static void del_ptr(struct btrfs_root *root, struct btrfs_path *path,
 		    int level, int slot);
 static void tree_mod_log_free_eb(struct btrfs_fs_info *fs_info,
 				 struct extent_buffer *eb);
-struct extent_buffer *read_old_tree_block(struct btrfs_root *root, u64 bytenr,
-					  u32 blocksize, u64 parent_transid,
-					  u64 time_seq);
-struct extent_buffer *btrfs_find_old_tree_block(struct btrfs_root *root,
-						u64 bytenr, u32 blocksize,
-						u64 time_seq);
+static int btrfs_prev_leaf(struct btrfs_root *root, struct btrfs_path *path);
 
 struct btrfs_path *btrfs_alloc_path(void)
 {
@@ -208,7 +203,7 @@ struct extent_buffer *btrfs_lock_root_node(struct btrfs_root *root)
  * tree until you end up with a lock on the root.  A locked buffer
  * is returned, with a reference held.
  */
-struct extent_buffer *btrfs_read_lock_root_node(struct btrfs_root *root)
+static struct extent_buffer *btrfs_read_lock_root_node(struct btrfs_root *root)
 {
 	struct extent_buffer *eb;
 

commit fc36ed7e0b13955ba66fc56dc5067e67ac105150
Author: Jan Schmidt <list.btrfs@jan-o-sch.net>
Date:   Wed Apr 24 16:57:33 2013 +0000

    Btrfs: separate sequence numbers for delayed ref tracking and tree mod log
    
    Sequence numbers for delayed refs have been introduced in the first version
    of the qgroup patch set. To solve the problem of find_all_roots on a busy
    file system, the tree mod log was introduced. The sequence numbers for that
    were simply shared between those two users.
    
    However, at one point in qgroup's quota accounting, there's a statement
    accessing the previous sequence number, that's still just doing (seq - 1)
    just as it would have to in the very first version.
    
    To satisfy that requirement, this patch makes the sequence number counter 64
    bit and splits it into a major part (used for qgroup sequence number
    counting) and a minor part (incremented for each tree modification in the
    log). This enables us to go exactly one major step backwards, as required
    for qgroups, while still incrementing the sequence counter for tree mod log
    insertions to keep track of their order. Keeping them in a single variable
    means there's no need to change all the code dealing with comparisons of two
    sequence numbers.
    
    The sequence number is reset to 0 on commit (not new in this patch), which
    ensures we won't overflow the two 32 bit counters.
    
    Without this fix, the qgroup tracking can occasionally go wrong and WARN_ONs
    from the tree mod log code may happen.
    
    Signed-off-by: Jan Schmidt <list.btrfs@jan-o-sch.net>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 2bc34408872d..a17d9991c333 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -360,6 +360,44 @@ static inline void tree_mod_log_write_unlock(struct btrfs_fs_info *fs_info)
 	write_unlock(&fs_info->tree_mod_log_lock);
 }
 
+/*
+ * Increment the upper half of tree_mod_seq, set lower half zero.
+ *
+ * Must be called with fs_info->tree_mod_seq_lock held.
+ */
+static inline u64 btrfs_inc_tree_mod_seq_major(struct btrfs_fs_info *fs_info)
+{
+	u64 seq = atomic64_read(&fs_info->tree_mod_seq);
+	seq &= 0xffffffff00000000ull;
+	seq += 1ull << 32;
+	atomic64_set(&fs_info->tree_mod_seq, seq);
+	return seq;
+}
+
+/*
+ * Increment the lower half of tree_mod_seq.
+ *
+ * Must be called with fs_info->tree_mod_seq_lock held. The way major numbers
+ * are generated should not technically require a spin lock here. (Rationale:
+ * incrementing the minor while incrementing the major seq number is between its
+ * atomic64_read and atomic64_set calls doesn't duplicate sequence numbers, it
+ * just returns a unique sequence number as usual.) We have decided to leave
+ * that requirement in here and rethink it once we notice it really imposes a
+ * problem on some workload.
+ */
+static inline u64 btrfs_inc_tree_mod_seq_minor(struct btrfs_fs_info *fs_info)
+{
+	return atomic64_inc_return(&fs_info->tree_mod_seq);
+}
+
+/*
+ * return the last minor in the previous major tree_mod_seq number
+ */
+u64 btrfs_tree_mod_seq_prev(u64 seq)
+{
+	return (seq & 0xffffffff00000000ull) - 1ull;
+}
+
 /*
  * This adds a new blocker to the tree mod log's blocker list if the @elem
  * passed does not already have a sequence number set. So when a caller expects
@@ -376,10 +414,10 @@ u64 btrfs_get_tree_mod_seq(struct btrfs_fs_info *fs_info,
 	tree_mod_log_write_lock(fs_info);
 	spin_lock(&fs_info->tree_mod_seq_lock);
 	if (!elem->seq) {
-		elem->seq = btrfs_inc_tree_mod_seq(fs_info);
+		elem->seq = btrfs_inc_tree_mod_seq_major(fs_info);
 		list_add_tail(&elem->list, &fs_info->tree_mod_seq_list);
 	}
-	seq = btrfs_inc_tree_mod_seq(fs_info);
+	seq = btrfs_inc_tree_mod_seq_minor(fs_info);
 	spin_unlock(&fs_info->tree_mod_seq_lock);
 	tree_mod_log_write_unlock(fs_info);
 
@@ -524,7 +562,10 @@ static inline int tree_mod_alloc(struct btrfs_fs_info *fs_info, gfp_t flags,
 	if (!tm)
 		return -ENOMEM;
 
-	tm->seq = btrfs_inc_tree_mod_seq(fs_info);
+	spin_lock(&fs_info->tree_mod_seq_lock);
+	tm->seq = btrfs_inc_tree_mod_seq_minor(fs_info);
+	spin_unlock(&fs_info->tree_mod_seq_lock);
+
 	return tm->seq;
 }
 

commit 416bc6580bb01ddf67befaaeb94f087b392e7f47
Author: Josef Bacik <jbacik@fusionio.com>
Date:   Tue Apr 23 14:17:42 2013 -0400

    Btrfs: fix all callers of read_tree_block
    
    We kept leaking extent buffers when mounting a broken file system and it turns
    out it's because not everybody uses read_tree_block properly.  You need to check
    and make sure the extent_buffer is uptodate before you use it.  This patch fixes
    everybody who calls read_tree_block directly to make sure they check that it is
    uptodate and free it and return an error if it is not.  With this we no longer
    leak EB's when things go horribly wrong.  Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 566d99b51bef..2bc34408872d 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1281,7 +1281,8 @@ get_old_root(struct btrfs_root *root, u64 time_seq)
 		free_extent_buffer(eb_root);
 		blocksize = btrfs_level_size(root, old_root->level);
 		old = read_tree_block(root, logical, blocksize, 0);
-		if (!old) {
+		if (!old || !extent_buffer_uptodate(old)) {
+			free_extent_buffer(old);
 			pr_warn("btrfs: failed to read tree block %llu from get_old_root\n",
 				logical);
 			WARN_ON(1);
@@ -1526,8 +1527,10 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 			if (!cur) {
 				cur = read_tree_block(root, blocknr,
 							 blocksize, gen);
-				if (!cur)
+				if (!cur || !extent_buffer_uptodate(cur)) {
+					free_extent_buffer(cur);
 					return -EIO;
+				}
 			} else if (!uptodate) {
 				err = btrfs_read_buffer(cur, gen);
 				if (err) {
@@ -1692,6 +1695,8 @@ static noinline struct extent_buffer *read_node_slot(struct btrfs_root *root,
 				   struct extent_buffer *parent, int slot)
 {
 	int level = btrfs_header_level(parent);
+	struct extent_buffer *eb;
+
 	if (slot < 0)
 		return NULL;
 	if (slot >= btrfs_header_nritems(parent))
@@ -1699,9 +1704,15 @@ static noinline struct extent_buffer *read_node_slot(struct btrfs_root *root,
 
 	BUG_ON(level == 0);
 
-	return read_tree_block(root, btrfs_node_blockptr(parent, slot),
-		       btrfs_level_size(root, level - 1),
-		       btrfs_node_ptr_generation(parent, slot));
+	eb = read_tree_block(root, btrfs_node_blockptr(parent, slot),
+			     btrfs_level_size(root, level - 1),
+			     btrfs_node_ptr_generation(parent, slot));
+	if (eb && !extent_buffer_uptodate(eb)) {
+		free_extent_buffer(eb);
+		eb = NULL;
+	}
+
+	return eb;
 }
 
 /*

commit 4b90c68015a7c0863292d6306501552d4ffa33ff
Author: Tsutomu Itoh <t-itoh@jp.fujitsu.com>
Date:   Tue Apr 16 05:18:49 2013 +0000

    Btrfs: remove unused argument of btrfs_extend_item()
    
    Argument 'trans' is not used in btrfs_extend_item().
    
    Signed-off-by: Tsutomu Itoh <t-itoh@jp.fujitsu.com>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 01a8a3bd4492..566d99b51bef 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -4383,8 +4383,7 @@ void btrfs_truncate_item(struct btrfs_root *root, struct btrfs_path *path,
 /*
  * make the item pointed to by the path bigger, data_size is the new size.
  */
-void btrfs_extend_item(struct btrfs_trans_handle *trans,
-		       struct btrfs_root *root, struct btrfs_path *path,
+void btrfs_extend_item(struct btrfs_root *root, struct btrfs_path *path,
 		       u32 data_size)
 {
 	int slot;

commit afe5fea72bd50b1df2e6a721ef50559427d42f2b
Author: Tsutomu Itoh <t-itoh@jp.fujitsu.com>
Date:   Tue Apr 16 05:18:22 2013 +0000

    Btrfs: cleanup of function where fixup_low_keys() is called
    
    If argument 'trans' is unnecessary in the function where
    fixup_low_keys() is called, 'trans' is deleted.
    
    Signed-off-by: Tsutomu Itoh <t-itoh@jp.fujitsu.com>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 71442d0d3140..01a8a3bd4492 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -37,8 +37,8 @@ static int balance_node_right(struct btrfs_trans_handle *trans,
 			      struct btrfs_root *root,
 			      struct extent_buffer *dst_buf,
 			      struct extent_buffer *src_buf);
-static void del_ptr(struct btrfs_trans_handle *trans, struct btrfs_root *root,
-		    struct btrfs_path *path, int level, int slot);
+static void del_ptr(struct btrfs_root *root, struct btrfs_path *path,
+		    int level, int slot);
 static void tree_mod_log_free_eb(struct btrfs_fs_info *fs_info,
 				 struct extent_buffer *eb);
 struct extent_buffer *read_old_tree_block(struct btrfs_root *root, u64 bytenr,
@@ -1830,7 +1830,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		if (btrfs_header_nritems(right) == 0) {
 			clean_tree_block(trans, root, right);
 			btrfs_tree_unlock(right);
-			del_ptr(trans, root, path, level + 1, pslot + 1);
+			del_ptr(root, path, level + 1, pslot + 1);
 			root_sub_used(root, right->len);
 			btrfs_free_tree_block(trans, root, right, 0, 1);
 			free_extent_buffer_stale(right);
@@ -1874,7 +1874,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 	if (btrfs_header_nritems(mid) == 0) {
 		clean_tree_block(trans, root, mid);
 		btrfs_tree_unlock(mid);
-		del_ptr(trans, root, path, level + 1, pslot);
+		del_ptr(root, path, level + 1, pslot);
 		root_sub_used(root, mid->len);
 		btrfs_free_tree_block(trans, root, mid, 0, 1);
 		free_extent_buffer_stale(mid);
@@ -2930,8 +2930,7 @@ static void fixup_low_keys(struct btrfs_root *root, struct btrfs_path *path,
  * This function isn't completely safe. It's the caller's responsibility
  * that the new key won't break the order
  */
-void btrfs_set_item_key_safe(struct btrfs_trans_handle *trans,
-			     struct btrfs_root *root, struct btrfs_path *path,
+void btrfs_set_item_key_safe(struct btrfs_root *root, struct btrfs_path *path,
 			     struct btrfs_key *new_key)
 {
 	struct btrfs_disk_key disk_key;
@@ -4267,7 +4266,7 @@ int btrfs_duplicate_item(struct btrfs_trans_handle *trans,
 		return ret;
 
 	path->slots[0]++;
-	setup_items_for_insert(trans, root, path, new_key, &item_size,
+	setup_items_for_insert(root, path, new_key, &item_size,
 			       item_size, item_size +
 			       sizeof(struct btrfs_item), 1);
 	leaf = path->nodes[0];
@@ -4284,9 +4283,7 @@ int btrfs_duplicate_item(struct btrfs_trans_handle *trans,
  * off the end of the item or if we shift the item to chop bytes off
  * the front.
  */
-void btrfs_truncate_item(struct btrfs_trans_handle *trans,
-			 struct btrfs_root *root,
-			 struct btrfs_path *path,
+void btrfs_truncate_item(struct btrfs_root *root, struct btrfs_path *path,
 			 u32 new_size, int from_end)
 {
 	int slot;
@@ -4457,8 +4454,7 @@ void btrfs_extend_item(struct btrfs_trans_handle *trans,
  * to save stack depth by doing the bulk of the work in a function
  * that doesn't call btrfs_search_slot
  */
-void setup_items_for_insert(struct btrfs_trans_handle *trans,
-			    struct btrfs_root *root, struct btrfs_path *path,
+void setup_items_for_insert(struct btrfs_root *root, struct btrfs_path *path,
 			    struct btrfs_key *cpu_key, u32 *data_size,
 			    u32 total_data, u32 total_size, int nr)
 {
@@ -4574,7 +4570,7 @@ int btrfs_insert_empty_items(struct btrfs_trans_handle *trans,
 	slot = path->slots[0];
 	BUG_ON(slot < 0);
 
-	setup_items_for_insert(trans, root, path, cpu_key, data_size,
+	setup_items_for_insert(root, path, cpu_key, data_size,
 			       total_data, total_size, nr);
 	return 0;
 }
@@ -4612,8 +4608,8 @@ int btrfs_insert_item(struct btrfs_trans_handle *trans, struct btrfs_root
  * the tree should have been previously balanced so the deletion does not
  * empty a node.
  */
-static void del_ptr(struct btrfs_trans_handle *trans, struct btrfs_root *root,
-		    struct btrfs_path *path, int level, int slot)
+static void del_ptr(struct btrfs_root *root, struct btrfs_path *path,
+		    int level, int slot)
 {
 	struct extent_buffer *parent = path->nodes[level];
 	u32 nritems;
@@ -4666,7 +4662,7 @@ static noinline void btrfs_del_leaf(struct btrfs_trans_handle *trans,
 				    struct extent_buffer *leaf)
 {
 	WARN_ON(btrfs_header_generation(leaf) != trans->transid);
-	del_ptr(trans, root, path, 1, path->slots[1]);
+	del_ptr(root, path, 1, path->slots[1]);
 
 	/*
 	 * btrfs_free_extent is expensive, we want to make sure we

commit d6a0a12684523d635e0530a9d70a1eba4b8c4fb9
Author: Tsutomu Itoh <t-itoh@jp.fujitsu.com>
Date:   Tue Apr 16 05:18:02 2013 +0000

    Btrfs: remove unused argument of fixup_low_keys()
    
    Argument 'trans' is not used in fixup_low_keys(). So, remove it.
    
    Signed-off-by: Tsutomu Itoh <t-itoh@jp.fujitsu.com>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index c3c3d37f9c58..71442d0d3140 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2905,8 +2905,7 @@ int btrfs_search_slot_for_read(struct btrfs_root *root,
  * higher levels
  *
  */
-static void fixup_low_keys(struct btrfs_trans_handle *trans,
-			   struct btrfs_root *root, struct btrfs_path *path,
+static void fixup_low_keys(struct btrfs_root *root, struct btrfs_path *path,
 			   struct btrfs_disk_key *key, int level)
 {
 	int i;
@@ -2954,7 +2953,7 @@ void btrfs_set_item_key_safe(struct btrfs_trans_handle *trans,
 	btrfs_set_item_key(eb, &disk_key, slot);
 	btrfs_mark_buffer_dirty(eb);
 	if (slot == 0)
-		fixup_low_keys(trans, root, path, &disk_key, 1);
+		fixup_low_keys(root, path, &disk_key, 1);
 }
 
 /*
@@ -3692,7 +3691,7 @@ static noinline int __push_leaf_left(struct btrfs_trans_handle *trans,
 		clean_tree_block(trans, root, right);
 
 	btrfs_item_key(right, &disk_key, 0);
-	fixup_low_keys(trans, root, path, &disk_key, 1);
+	fixup_low_keys(root, path, &disk_key, 1);
 
 	/* then fixup the leaf pointer in the path */
 	if (path->slots[0] < push_items) {
@@ -4052,8 +4051,7 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 			path->nodes[0] = right;
 			path->slots[0] = 0;
 			if (path->slots[1] == 0)
-				fixup_low_keys(trans, root, path,
-					       &disk_key, 1);
+				fixup_low_keys(root, path, &disk_key, 1);
 		}
 		btrfs_mark_buffer_dirty(right);
 		return ret;
@@ -4372,7 +4370,7 @@ void btrfs_truncate_item(struct btrfs_trans_handle *trans,
 		btrfs_set_disk_key_offset(&disk_key, offset + size_diff);
 		btrfs_set_item_key(leaf, &disk_key, slot);
 		if (slot == 0)
-			fixup_low_keys(trans, root, path, &disk_key, 1);
+			fixup_low_keys(root, path, &disk_key, 1);
 	}
 
 	item = btrfs_item_nr(leaf, slot);
@@ -4536,7 +4534,7 @@ void setup_items_for_insert(struct btrfs_trans_handle *trans,
 
 	if (slot == 0) {
 		btrfs_cpu_key_to_disk(&disk_key, cpu_key);
-		fixup_low_keys(trans, root, path, &disk_key, 1);
+		fixup_low_keys(root, path, &disk_key, 1);
 	}
 	btrfs_unlock_up_safe(path, 1);
 	btrfs_mark_buffer_dirty(leaf);
@@ -4647,7 +4645,7 @@ static void del_ptr(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 		struct btrfs_disk_key disk_key;
 
 		btrfs_node_key(parent, &disk_key, 0);
-		fixup_low_keys(trans, root, path, &disk_key, level + 1);
+		fixup_low_keys(root, path, &disk_key, level + 1);
 	}
 	btrfs_mark_buffer_dirty(parent);
 }
@@ -4749,7 +4747,7 @@ int btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 			struct btrfs_disk_key disk_key;
 
 			btrfs_item_key(leaf, &disk_key, 0);
-			fixup_low_keys(trans, root, path, &disk_key, 1);
+			fixup_low_keys(root, path, &disk_key, 1);
 		}
 
 		/* delete the leaf if it is mostly empty */

commit 47fb091fb787420cd195e66f162737401cce023f
Author: Jan Schmidt <list.btrfs@jan-o-sch.net>
Date:   Sat Apr 13 13:19:55 2013 +0000

    Btrfs: fix unlock after free on rewinded tree blocks
    
    When tree_mod_log_rewind decides to make a copy of the current tree buffer
    for its modifications, it subsequently freed the buffer before unlocking it.
    Obviously, those operations are required in reverse order.
    
    Signed-off-by: Jan Schmidt <list.btrfs@jan-o-sch.net>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index d7e0576038fb..c3c3d37f9c58 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1191,6 +1191,13 @@ __tree_mod_log_rewind(struct extent_buffer *eb, u64 time_seq,
 	btrfs_set_header_nritems(eb, n);
 }
 
+/*
+ * Called with eb read locked. If the buffer cannot be rewinded, the same buffer
+ * is returned. If rewind operations happen, a fresh buffer is returned. The
+ * returned buffer is always read-locked. If the returned buffer is not the
+ * input buffer, the lock on the input buffer is released and the input buffer
+ * is freed (its refcount is decremented).
+ */
 static struct extent_buffer *
 tree_mod_log_rewind(struct btrfs_fs_info *fs_info, struct extent_buffer *eb,
 		    u64 time_seq)
@@ -1224,8 +1231,11 @@ tree_mod_log_rewind(struct btrfs_fs_info *fs_info, struct extent_buffer *eb,
 	}
 
 	extent_buffer_get(eb_rewin);
+	btrfs_tree_read_unlock(eb);
 	free_extent_buffer(eb);
 
+	extent_buffer_get(eb_rewin);
+	btrfs_tree_read_lock(eb_rewin);
 	__tree_mod_log_rewind(eb_rewin, time_seq, tm);
 	WARN_ON(btrfs_header_nritems(eb_rewin) >
 		BTRFS_NODEPTRS_PER_BLOCK(fs_info->tree_root));
@@ -2794,15 +2804,9 @@ int btrfs_search_old_slot(struct btrfs_root *root, struct btrfs_key *key,
 				btrfs_clear_path_blocking(p, b,
 							  BTRFS_READ_LOCK);
 			}
+			b = tree_mod_log_rewind(root->fs_info, b, time_seq);
 			p->locks[level] = BTRFS_READ_LOCK;
 			p->nodes[level] = b;
-			b = tree_mod_log_rewind(root->fs_info, b, time_seq);
-			if (b != p->nodes[level]) {
-				btrfs_tree_unlock_rw(p->nodes[level],
-						     p->locks[level]);
-				p->locks[level] = 0;
-				p->nodes[level] = b;
-			}
 		} else {
 			p->slots[level] = slot;
 			unlock_up(p, level, lowest_unlock, 0, NULL);

commit 30b0463a9394d9e41596e96def5461fe33222f13
Author: Jan Schmidt <list.btrfs@jan-o-sch.net>
Date:   Sat Apr 13 13:19:54 2013 +0000

    Btrfs: fix accessing the root pointer in tree mod log functions
    
    The tree mod log functions were accessing root->node->... directly, without
    use of btrfs_root_node() or explicit rcu locking. This could lead to an
    extent buffer reference being leaked and another reference being freed too
    early when preemtion was enabled.
    
    Signed-off-by: Jan Schmidt <list.btrfs@jan-o-sch.net>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 1180209965db..d7e0576038fb 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1069,11 +1069,11 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
  */
 static struct tree_mod_elem *
 __tree_mod_log_oldest_root(struct btrfs_fs_info *fs_info,
-			   struct btrfs_root *root, u64 time_seq)
+			   struct extent_buffer *eb_root, u64 time_seq)
 {
 	struct tree_mod_elem *tm;
 	struct tree_mod_elem *found = NULL;
-	u64 root_logical = root->node->start;
+	u64 root_logical = eb_root->start;
 	int looped = 0;
 
 	if (!time_seq)
@@ -1107,7 +1107,6 @@ __tree_mod_log_oldest_root(struct btrfs_fs_info *fs_info,
 
 		found = tm;
 		root_logical = tm->old_root.logical;
-		BUG_ON(root_logical == root->node->start);
 		looped = 1;
 	}
 
@@ -1245,30 +1244,31 @@ static inline struct extent_buffer *
 get_old_root(struct btrfs_root *root, u64 time_seq)
 {
 	struct tree_mod_elem *tm;
-	struct extent_buffer *eb;
+	struct extent_buffer *eb = NULL;
+	struct extent_buffer *eb_root;
 	struct extent_buffer *old;
 	struct tree_mod_root *old_root = NULL;
 	u64 old_generation = 0;
 	u64 logical;
 	u32 blocksize;
 
-	eb = btrfs_read_lock_root_node(root);
-	tm = __tree_mod_log_oldest_root(root->fs_info, root, time_seq);
+	eb_root = btrfs_read_lock_root_node(root);
+	tm = __tree_mod_log_oldest_root(root->fs_info, eb_root, time_seq);
 	if (!tm)
-		return root->node;
+		return eb_root;
 
 	if (tm->op == MOD_LOG_ROOT_REPLACE) {
 		old_root = &tm->old_root;
 		old_generation = tm->generation;
 		logical = old_root->logical;
 	} else {
-		logical = root->node->start;
+		logical = eb_root->start;
 	}
 
 	tm = tree_mod_log_search(root->fs_info, logical, time_seq);
 	if (old_root && tm && tm->op != MOD_LOG_KEY_REMOVE_WHILE_FREEING) {
-		btrfs_tree_read_unlock(root->node);
-		free_extent_buffer(root->node);
+		btrfs_tree_read_unlock(eb_root);
+		free_extent_buffer(eb_root);
 		blocksize = btrfs_level_size(root, old_root->level);
 		old = read_tree_block(root, logical, blocksize, 0);
 		if (!old) {
@@ -1280,13 +1280,13 @@ get_old_root(struct btrfs_root *root, u64 time_seq)
 			free_extent_buffer(old);
 		}
 	} else if (old_root) {
-		btrfs_tree_read_unlock(root->node);
-		free_extent_buffer(root->node);
+		btrfs_tree_read_unlock(eb_root);
+		free_extent_buffer(eb_root);
 		eb = alloc_dummy_extent_buffer(logical, root->nodesize);
 	} else {
-		eb = btrfs_clone_extent_buffer(root->node);
-		btrfs_tree_read_unlock(root->node);
-		free_extent_buffer(root->node);
+		eb = btrfs_clone_extent_buffer(eb_root);
+		btrfs_tree_read_unlock(eb_root);
+		free_extent_buffer(eb_root);
 	}
 
 	if (!eb)
@@ -1296,7 +1296,7 @@ get_old_root(struct btrfs_root *root, u64 time_seq)
 	if (old_root) {
 		btrfs_set_header_bytenr(eb, eb->start);
 		btrfs_set_header_backref_rev(eb, BTRFS_MIXED_BACKREF_REV);
-		btrfs_set_header_owner(eb, root->root_key.objectid);
+		btrfs_set_header_owner(eb, btrfs_header_owner(eb_root));
 		btrfs_set_header_level(eb, old_root->level);
 		btrfs_set_header_generation(eb, old_generation);
 	}
@@ -1313,15 +1313,15 @@ int btrfs_old_root_level(struct btrfs_root *root, u64 time_seq)
 {
 	struct tree_mod_elem *tm;
 	int level;
+	struct extent_buffer *eb_root = btrfs_root_node(root);
 
-	tm = __tree_mod_log_oldest_root(root->fs_info, root, time_seq);
+	tm = __tree_mod_log_oldest_root(root->fs_info, eb_root, time_seq);
 	if (tm && tm->op == MOD_LOG_ROOT_REPLACE) {
 		level = tm->old_root.level;
 	} else {
-		rcu_read_lock();
-		level = btrfs_header_level(root->node);
-		rcu_read_unlock();
+		level = btrfs_header_level(eb_root);
 	}
+	free_extent_buffer(eb_root);
 
 	return level;
 }

commit 90f8d62ebb55e2188c1618b650378f9857f9e9a4
Author: Jan Schmidt <list.btrfs@jan-o-sch.net>
Date:   Sat Apr 13 13:19:53 2013 +0000

    Btrfs: fix tree mod log regression on root split operations
    
    Commit d9abbf1c changed tree mod log locking around ROOT_REPLACE operations.
    When a tree root is split, however, we were logging removal of all elements
    from the root node before logging removal of half of the elements for the
    split operation. This leads to a BUG_ON when rewinding.
    
    This commit removes the erroneous logging of removal of all elements.
    
    Signed-off-by: Jan Schmidt <list.btrfs@jan-o-sch.net>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 9ca0f6aefa22..1180209965db 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -643,7 +643,8 @@ __tree_mod_log_free_eb(struct btrfs_fs_info *fs_info, struct extent_buffer *eb)
 static noinline int
 tree_mod_log_insert_root(struct btrfs_fs_info *fs_info,
 			 struct extent_buffer *old_root,
-			 struct extent_buffer *new_root, gfp_t flags)
+			 struct extent_buffer *new_root, gfp_t flags,
+			 int log_removal)
 {
 	struct tree_mod_elem *tm;
 	int ret;
@@ -651,7 +652,8 @@ tree_mod_log_insert_root(struct btrfs_fs_info *fs_info,
 	if (tree_mod_dont_log(fs_info, NULL))
 		return 0;
 
-	__tree_mod_log_free_eb(fs_info, old_root);
+	if (log_removal)
+		__tree_mod_log_free_eb(fs_info, old_root);
 
 	ret = tree_mod_alloc(fs_info, flags, &tm);
 	if (ret < 0)
@@ -738,7 +740,7 @@ tree_mod_log_search(struct btrfs_fs_info *fs_info, u64 start, u64 min_seq)
 static noinline void
 tree_mod_log_eb_copy(struct btrfs_fs_info *fs_info, struct extent_buffer *dst,
 		     struct extent_buffer *src, unsigned long dst_offset,
-		     unsigned long src_offset, int nr_items, int log_removal)
+		     unsigned long src_offset, int nr_items)
 {
 	int ret;
 	int i;
@@ -752,12 +754,10 @@ tree_mod_log_eb_copy(struct btrfs_fs_info *fs_info, struct extent_buffer *dst,
 	}
 
 	for (i = 0; i < nr_items; i++) {
-		if (log_removal) {
-			ret = tree_mod_log_insert_key_locked(fs_info, src,
-							i + src_offset,
-							MOD_LOG_KEY_REMOVE);
-			BUG_ON(ret < 0);
-		}
+		ret = tree_mod_log_insert_key_locked(fs_info, src,
+						i + src_offset,
+						MOD_LOG_KEY_REMOVE);
+		BUG_ON(ret < 0);
 		ret = tree_mod_log_insert_key_locked(fs_info, dst,
 						     i + dst_offset,
 						     MOD_LOG_KEY_ADD);
@@ -802,11 +802,12 @@ tree_mod_log_free_eb(struct btrfs_fs_info *fs_info, struct extent_buffer *eb)
 
 static noinline void
 tree_mod_log_set_root_pointer(struct btrfs_root *root,
-			      struct extent_buffer *new_root_node)
+			      struct extent_buffer *new_root_node,
+			      int log_removal)
 {
 	int ret;
 	ret = tree_mod_log_insert_root(root->fs_info, root->node,
-				       new_root_node, GFP_NOFS);
+				       new_root_node, GFP_NOFS, log_removal);
 	BUG_ON(ret < 0);
 }
 
@@ -1029,7 +1030,7 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 			parent_start = 0;
 
 		extent_buffer_get(cow);
-		tree_mod_log_set_root_pointer(root, cow);
+		tree_mod_log_set_root_pointer(root, cow, 1);
 		rcu_assign_pointer(root->node, cow);
 
 		btrfs_free_tree_block(trans, root, buf, parent_start,
@@ -1755,7 +1756,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 			goto enospc;
 		}
 
-		tree_mod_log_set_root_pointer(root, child);
+		tree_mod_log_set_root_pointer(root, child, 1);
 		rcu_assign_pointer(root->node, child);
 
 		add_root_to_dirty_list(root);
@@ -2996,7 +2997,7 @@ static int push_node_left(struct btrfs_trans_handle *trans,
 		push_items = min(src_nritems - 8, push_items);
 
 	tree_mod_log_eb_copy(root->fs_info, dst, src, dst_nritems, 0,
-			     push_items, 1);
+			     push_items);
 	copy_extent_buffer(dst, src,
 			   btrfs_node_key_ptr_offset(dst_nritems),
 			   btrfs_node_key_ptr_offset(0),
@@ -3067,7 +3068,7 @@ static int balance_node_right(struct btrfs_trans_handle *trans,
 				      sizeof(struct btrfs_key_ptr));
 
 	tree_mod_log_eb_copy(root->fs_info, dst, src, 0,
-			     src_nritems - push_items, push_items, 1);
+			     src_nritems - push_items, push_items);
 	copy_extent_buffer(dst, src,
 			   btrfs_node_key_ptr_offset(0),
 			   btrfs_node_key_ptr_offset(src_nritems - push_items),
@@ -3091,7 +3092,7 @@ static int balance_node_right(struct btrfs_trans_handle *trans,
  */
 static noinline int insert_new_root(struct btrfs_trans_handle *trans,
 			   struct btrfs_root *root,
-			   struct btrfs_path *path, int level)
+			   struct btrfs_path *path, int level, int log_removal)
 {
 	u64 lower_gen;
 	struct extent_buffer *lower;
@@ -3142,7 +3143,7 @@ static noinline int insert_new_root(struct btrfs_trans_handle *trans,
 	btrfs_mark_buffer_dirty(c);
 
 	old = root->node;
-	tree_mod_log_set_root_pointer(root, c);
+	tree_mod_log_set_root_pointer(root, c, log_removal);
 	rcu_assign_pointer(root->node, c);
 
 	/* the super has an extra ref to root->node */
@@ -3219,18 +3220,21 @@ static noinline int split_node(struct btrfs_trans_handle *trans,
 	int mid;
 	int ret;
 	u32 c_nritems;
-	int tree_mod_log_removal = 1;
 
 	c = path->nodes[level];
 	WARN_ON(btrfs_header_generation(c) != trans->transid);
 	if (c == root->node) {
-		/* trying to split the root, lets make a new one */
-		ret = insert_new_root(trans, root, path, level + 1);
 		/*
-		 * removal of root nodes has been logged by
-		 * tree_mod_log_set_root_pointer due to locking
+		 * trying to split the root, lets make a new one
+		 *
+		 * tree mod log: We pass 0 as log_removal parameter to
+		 * insert_new_root, because that root buffer will be kept as a
+		 * normal node. We are going to log removal of half of the
+		 * elements below with tree_mod_log_eb_copy. We're holding a
+		 * tree lock on the buffer, which is why we cannot race with
+		 * other tree_mod_log users.
 		 */
-		tree_mod_log_removal = 0;
+		ret = insert_new_root(trans, root, path, level + 1, 0);
 		if (ret)
 			return ret;
 	} else {
@@ -3268,8 +3272,7 @@ static noinline int split_node(struct btrfs_trans_handle *trans,
 			    (unsigned long)btrfs_header_chunk_tree_uuid(split),
 			    BTRFS_UUID_SIZE);
 
-	tree_mod_log_eb_copy(root->fs_info, split, c, 0, mid, c_nritems - mid,
-			     tree_mod_log_removal);
+	tree_mod_log_eb_copy(root->fs_info, split, c, 0, mid, c_nritems - mid);
 	copy_extent_buffer(split, c,
 			   btrfs_node_key_ptr_offset(0),
 			   btrfs_node_key_ptr_offset(mid),
@@ -3951,7 +3954,7 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 	}
 
 	if (!path->nodes[1]) {
-		ret = insert_new_root(trans, root, path, 1);
+		ret = insert_new_root(trans, root, path, 1, 1);
 		if (ret)
 			return ret;
 	}

commit 09a2a8f96e3009273bed1833b3f210e2c68728a5
Author: Josef Bacik <jbacik@fusionio.com>
Date:   Fri Apr 5 16:51:15 2013 -0400

    Btrfs: fix bad extent logging
    
    A user sent me a btrfs-image of a file system that was panicing on mount during
    the log recovery.  I had originally thought these problems were from a bug in
    the free space cache code, but that was just a symptom of the problem.  The
    problem is if your application does something like this
    
    [prealloc][prealloc][prealloc]
    
    the internal extent maps will merge those all together into one extent map, even
    though on disk they are 3 separate extents.  So if you go to write into one of
    these ranges the extent map will be right since we use the physical extent when
    doing the write, but when we log the extents they will use the wrong sizes for
    the remainder prealloc space.  If this doesn't happen to trip up the free space
    cache (which it won't in a lot of cases) then you will get bogus entries in your
    extent tree which will screw stuff up later.  The data and such will still work,
    but everything else is broken.  This patch fixes this by not allowing extents
    that are on the modified list to be merged.  This has the side effect that we
    are no longer adding everything to the modified list all the time, which means
    we now have to call btrfs_drop_extents every time we log an extent into the
    tree.  So this allows me to drop all this speciality code I was using to get
    around calling btrfs_drop_extents.  With this patch the testcase I've created no
    longer creates a bogus file system after replaying the log.  Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index fe032ab6bd8a..9ca0f6aefa22 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2211,9 +2211,6 @@ static noinline void unlock_up(struct btrfs_path *path, int level,
 	int no_skips = 0;
 	struct extent_buffer *t;
 
-	if (path->really_keep_locks)
-		return;
-
 	for (i = level; i < BTRFS_MAX_LEVEL; i++) {
 		if (!path->nodes[i])
 			break;
@@ -2261,7 +2258,7 @@ noinline void btrfs_unlock_up_safe(struct btrfs_path *path, int level)
 {
 	int i;
 
-	if (path->keep_locks || path->really_keep_locks)
+	if (path->keep_locks)
 		return;
 
 	for (i = level; i < BTRFS_MAX_LEVEL; i++) {
@@ -2494,7 +2491,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 	if (!cow)
 		write_lock_level = -1;
 
-	if (cow && (p->really_keep_locks || p->keep_locks || p->lowest_level))
+	if (cow && (p->keep_locks || p->lowest_level))
 		write_lock_level = BTRFS_MAX_LEVEL;
 
 	min_write_lock_level = write_lock_level;
@@ -5465,139 +5462,6 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 	return btrfs_next_old_leaf(root, path, 0);
 }
 
-/* Release the path up to but not including the given level */
-static void btrfs_release_level(struct btrfs_path *path, int level)
-{
-	int i;
-
-	for (i = 0; i < level; i++) {
-		path->slots[i] = 0;
-		if (!path->nodes[i])
-			continue;
-		if (path->locks[i]) {
-			btrfs_tree_unlock_rw(path->nodes[i], path->locks[i]);
-			path->locks[i] = 0;
-		}
-		free_extent_buffer(path->nodes[i]);
-		path->nodes[i] = NULL;
-	}
-}
-
-/*
- * This function assumes 2 things
- *
- * 1) You are using path->keep_locks
- * 2) You are not inserting items.
- *
- * If either of these are not true do not use this function. If you need a next
- * leaf with either of these not being true then this function can be easily
- * adapted to do that, but at the moment these are the limitations.
- */
-int btrfs_next_leaf_write(struct btrfs_trans_handle *trans,
-			  struct btrfs_root *root, struct btrfs_path *path,
-			  int del)
-{
-	struct extent_buffer *b;
-	struct btrfs_key key;
-	u32 nritems;
-	int level = 1;
-	int slot;
-	int ret = 1;
-	int write_lock_level = BTRFS_MAX_LEVEL;
-	int ins_len = del ? -1 : 0;
-
-	WARN_ON(!(path->keep_locks || path->really_keep_locks));
-
-	nritems = btrfs_header_nritems(path->nodes[0]);
-	btrfs_item_key_to_cpu(path->nodes[0], &key, nritems - 1);
-
-	while (path->nodes[level]) {
-		nritems = btrfs_header_nritems(path->nodes[level]);
-		if (!(path->locks[level] & BTRFS_WRITE_LOCK)) {
-search:
-			btrfs_release_path(path);
-			ret = btrfs_search_slot(trans, root, &key, path,
-						ins_len, 1);
-			if (ret < 0)
-				goto out;
-			level = 1;
-			continue;
-		}
-
-		if (path->slots[level] >= nritems - 1) {
-			level++;
-			continue;
-		}
-
-		btrfs_release_level(path, level);
-		break;
-	}
-
-	if (!path->nodes[level]) {
-		ret = 1;
-		goto out;
-	}
-
-	path->slots[level]++;
-	b = path->nodes[level];
-
-	while (b) {
-		level = btrfs_header_level(b);
-
-		if (!should_cow_block(trans, root, b))
-			goto cow_done;
-
-		btrfs_set_path_blocking(path);
-		ret = btrfs_cow_block(trans, root, b,
-				      path->nodes[level + 1],
-				      path->slots[level + 1], &b);
-		if (ret)
-			goto out;
-cow_done:
-		path->nodes[level] = b;
-		btrfs_clear_path_blocking(path, NULL, 0);
-		if (level != 0) {
-			ret = setup_nodes_for_search(trans, root, path, b,
-						     level, ins_len,
-						     &write_lock_level);
-			if (ret == -EAGAIN)
-				goto search;
-			if (ret)
-				goto out;
-
-			b = path->nodes[level];
-			slot = path->slots[level];
-
-			ret = read_block_for_search(trans, root, path,
-						    &b, level, slot, &key, 0);
-			if (ret == -EAGAIN)
-				goto search;
-			if (ret)
-				goto out;
-			level = btrfs_header_level(b);
-			if (!btrfs_try_tree_write_lock(b)) {
-				btrfs_set_path_blocking(path);
-				btrfs_tree_lock(b);
-				btrfs_clear_path_blocking(path, b,
-							  BTRFS_WRITE_LOCK);
-			}
-			path->locks[level] = BTRFS_WRITE_LOCK;
-			path->nodes[level] = b;
-			path->slots[level] = 0;
-		} else {
-			path->slots[level] = 0;
-			ret = 0;
-			break;
-		}
-	}
-
-out:
-	if (ret)
-		btrfs_release_path(path);
-
-	return ret;
-}
-
 int btrfs_next_old_leaf(struct btrfs_root *root, struct btrfs_path *path,
 			u64 time_seq)
 {

commit 3173a18f70554fe7880bb2d85c7da566e364eb3c
Author: Josef Bacik <jbacik@fusionio.com>
Date:   Thu Mar 7 14:22:04 2013 -0500

    Btrfs: add a incompatible format change for smaller metadata extent refs
    
    We currently store the first key of the tree block inside the reference for the
    tree block in the extent tree.  This takes up quite a bit of space.  Make a new
    key type for metadata which holds the level as the offset and completely removes
    storing the btrfs_tree_block_info inside the extent ref.  This reduces the size
    from 51 bytes to 33 bytes per extent reference for each tree block.  In practice
    this results in a 30-35% decrease in the size of our extent tree, which means we
    COW less and can keep more of the extent tree in memory which makes our heavy
    metadata operations go much faster.  This is not an automatic format change, you
    must enable it at mkfs time or with btrfstune.  This patch deals with having
    metadata stored as either the old format or the new format so it is easy to
    convert.  Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index ca9d8f1a3bb6..fe032ab6bd8a 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -867,7 +867,8 @@ static noinline int update_ref_for_cow(struct btrfs_trans_handle *trans,
 
 	if (btrfs_block_can_be_shared(root, buf)) {
 		ret = btrfs_lookup_extent_info(trans, root, buf->start,
-					       buf->len, &refs, &flags);
+					       btrfs_header_level(buf), 1,
+					       &refs, &flags);
 		if (ret)
 			return ret;
 		if (refs == 0) {

commit d9abbf1c3131b679379762700201ae69367f3f62
Author: Jan Schmidt <list.btrfs@jan-o-sch.net>
Date:   Wed Mar 20 13:49:48 2013 +0000

    Btrfs: fix locking on ROOT_REPLACE operations in tree mod log
    
    To resolve backrefs, ROOT_REPLACE operations in the tree mod log are
    required to be tied to at least one KEY_REMOVE_WHILE_FREEING operation.
    Therefore, those operations must be enclosed by tree_mod_log_write_lock()
    and tree_mod_log_write_unlock() calls.
    
    Those calls are private to the tree_mod_log_* functions, which means that
    removal of the elements of an old root node must be logged from
    tree_mod_log_insert_root. This partly reverts and corrects commit ba1bfbd5
    (Btrfs: fix a tree mod logging issue for root replacement operations).
    
    This fixes the brand-new version of xfstest 276 as of commit cfe73f71.
    
    Cc: stable@vger.kernel.org
    Signed-off-by: Jan Schmidt <list.btrfs@jan-o-sch.net>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index ecd25a1b4e51..ca9d8f1a3bb6 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -651,6 +651,8 @@ tree_mod_log_insert_root(struct btrfs_fs_info *fs_info,
 	if (tree_mod_dont_log(fs_info, NULL))
 		return 0;
 
+	__tree_mod_log_free_eb(fs_info, old_root);
+
 	ret = tree_mod_alloc(fs_info, flags, &tm);
 	if (ret < 0)
 		goto out;
@@ -736,7 +738,7 @@ tree_mod_log_search(struct btrfs_fs_info *fs_info, u64 start, u64 min_seq)
 static noinline void
 tree_mod_log_eb_copy(struct btrfs_fs_info *fs_info, struct extent_buffer *dst,
 		     struct extent_buffer *src, unsigned long dst_offset,
-		     unsigned long src_offset, int nr_items)
+		     unsigned long src_offset, int nr_items, int log_removal)
 {
 	int ret;
 	int i;
@@ -750,10 +752,12 @@ tree_mod_log_eb_copy(struct btrfs_fs_info *fs_info, struct extent_buffer *dst,
 	}
 
 	for (i = 0; i < nr_items; i++) {
-		ret = tree_mod_log_insert_key_locked(fs_info, src,
-						     i + src_offset,
-						     MOD_LOG_KEY_REMOVE);
-		BUG_ON(ret < 0);
+		if (log_removal) {
+			ret = tree_mod_log_insert_key_locked(fs_info, src,
+							i + src_offset,
+							MOD_LOG_KEY_REMOVE);
+			BUG_ON(ret < 0);
+		}
 		ret = tree_mod_log_insert_key_locked(fs_info, dst,
 						     i + dst_offset,
 						     MOD_LOG_KEY_ADD);
@@ -927,7 +931,6 @@ static noinline int update_ref_for_cow(struct btrfs_trans_handle *trans,
 			ret = btrfs_dec_ref(trans, root, buf, 1, 1);
 			BUG_ON(ret); /* -ENOMEM */
 		}
-		tree_mod_log_free_eb(root->fs_info, buf);
 		clean_tree_block(trans, root, buf);
 		*last_ref = 1;
 	}
@@ -1046,6 +1049,7 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 		btrfs_set_node_ptr_generation(parent, parent_slot,
 					      trans->transid);
 		btrfs_mark_buffer_dirty(parent);
+		tree_mod_log_free_eb(root->fs_info, buf);
 		btrfs_free_tree_block(trans, root, buf, parent_start,
 				      last_ref);
 	}
@@ -1750,7 +1754,6 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 			goto enospc;
 		}
 
-		tree_mod_log_free_eb(root->fs_info, root->node);
 		tree_mod_log_set_root_pointer(root, child);
 		rcu_assign_pointer(root->node, child);
 
@@ -2995,7 +2998,7 @@ static int push_node_left(struct btrfs_trans_handle *trans,
 		push_items = min(src_nritems - 8, push_items);
 
 	tree_mod_log_eb_copy(root->fs_info, dst, src, dst_nritems, 0,
-			     push_items);
+			     push_items, 1);
 	copy_extent_buffer(dst, src,
 			   btrfs_node_key_ptr_offset(dst_nritems),
 			   btrfs_node_key_ptr_offset(0),
@@ -3066,7 +3069,7 @@ static int balance_node_right(struct btrfs_trans_handle *trans,
 				      sizeof(struct btrfs_key_ptr));
 
 	tree_mod_log_eb_copy(root->fs_info, dst, src, 0,
-			     src_nritems - push_items, push_items);
+			     src_nritems - push_items, push_items, 1);
 	copy_extent_buffer(dst, src,
 			   btrfs_node_key_ptr_offset(0),
 			   btrfs_node_key_ptr_offset(src_nritems - push_items),
@@ -3218,12 +3221,18 @@ static noinline int split_node(struct btrfs_trans_handle *trans,
 	int mid;
 	int ret;
 	u32 c_nritems;
+	int tree_mod_log_removal = 1;
 
 	c = path->nodes[level];
 	WARN_ON(btrfs_header_generation(c) != trans->transid);
 	if (c == root->node) {
 		/* trying to split the root, lets make a new one */
 		ret = insert_new_root(trans, root, path, level + 1);
+		/*
+		 * removal of root nodes has been logged by
+		 * tree_mod_log_set_root_pointer due to locking
+		 */
+		tree_mod_log_removal = 0;
 		if (ret)
 			return ret;
 	} else {
@@ -3261,7 +3270,8 @@ static noinline int split_node(struct btrfs_trans_handle *trans,
 			    (unsigned long)btrfs_header_chunk_tree_uuid(split),
 			    BTRFS_UUID_SIZE);
 
-	tree_mod_log_eb_copy(root->fs_info, split, c, 0, mid, c_nritems - mid);
+	tree_mod_log_eb_copy(root->fs_info, split, c, 0, mid, c_nritems - mid,
+			     tree_mod_log_removal);
 	copy_extent_buffer(split, c,
 			   btrfs_node_key_ptr_offset(0),
 			   btrfs_node_key_ptr_offset(mid),

commit de78b51a2852bddccd6535e9e12de65f92787a1e
Author: Eric Sandeen <sandeen@redhat.com>
Date:   Thu Jan 31 18:21:12 2013 +0000

    btrfs: remove cache only arguments from defrag path
    
    The entry point at the defrag ioctl always sets "cache only" to 0;
    the codepaths haven't run for a long time as far as I can
    tell.  Chris says they're dead code, so remove them.
    
    Signed-off-by: Eric Sandeen <sandeen@redhat.com>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 35444013f0cf..ecd25a1b4e51 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1442,7 +1442,7 @@ int btrfs_comp_cpu_keys(struct btrfs_key *k1, struct btrfs_key *k2)
  */
 int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 		       struct btrfs_root *root, struct extent_buffer *parent,
-		       int start_slot, int cache_only, u64 *last_ret,
+		       int start_slot, u64 *last_ret,
 		       struct btrfs_key *progress)
 {
 	struct extent_buffer *cur;
@@ -1462,8 +1462,6 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 	struct btrfs_disk_key disk_key;
 
 	parent_level = btrfs_header_level(parent);
-	if (cache_only && parent_level != 1)
-		return 0;
 
 	WARN_ON(trans->transaction != root->fs_info->running_transaction);
 	WARN_ON(trans->transid != root->fs_info->generation);
@@ -1509,10 +1507,6 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 		else
 			uptodate = 0;
 		if (!cur || !uptodate) {
-			if (cache_only) {
-				free_extent_buffer(cur);
-				continue;
-			}
 			if (!cur) {
 				cur = read_tree_block(root, blocknr,
 							 blocksize, gen);
@@ -4826,8 +4820,8 @@ int btrfs_prev_leaf(struct btrfs_root *root, struct btrfs_path *path)
 
 /*
  * A helper function to walk down the tree starting at min_key, and looking
- * for nodes or leaves that are either in cache or have a minimum
- * transaction id.  This is used by the btree defrag code, and tree logging
+ * for nodes or leaves that are have a minimum transaction id.
+ * This is used by the btree defrag code, and tree logging
  *
  * This does not cow, but it does stuff the starting key it finds back
  * into min_key, so you can call btrfs_search_slot with cow=1 on the
@@ -4848,7 +4842,7 @@ int btrfs_prev_leaf(struct btrfs_root *root, struct btrfs_path *path)
  */
 int btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,
 			 struct btrfs_key *max_key,
-			 struct btrfs_path *path, int cache_only,
+			 struct btrfs_path *path,
 			 u64 min_trans)
 {
 	struct extent_buffer *cur;
@@ -4888,15 +4882,12 @@ int btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,
 		if (sret && slot > 0)
 			slot--;
 		/*
-		 * check this node pointer against the cache_only and
-		 * min_trans parameters.  If it isn't in cache or is too
-		 * old, skip to the next one.
+		 * check this node pointer against the min_trans parameters.
+		 * If it is too old, old, skip to the next one.
 		 */
 		while (slot < nritems) {
 			u64 blockptr;
 			u64 gen;
-			struct extent_buffer *tmp;
-			struct btrfs_disk_key disk_key;
 
 			blockptr = btrfs_node_blockptr(cur, slot);
 			gen = btrfs_node_ptr_generation(cur, slot);
@@ -4904,27 +4895,7 @@ int btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,
 				slot++;
 				continue;
 			}
-			if (!cache_only)
-				break;
-
-			if (max_key) {
-				btrfs_node_key(cur, &disk_key, slot);
-				if (comp_keys(&disk_key, max_key) >= 0) {
-					ret = 1;
-					goto out;
-				}
-			}
-
-			tmp = btrfs_find_tree_block(root, blockptr,
-					    btrfs_level_size(root, level - 1));
-
-			if (tmp && btrfs_buffer_uptodate(tmp, gen, 1) > 0) {
-				free_extent_buffer(tmp);
-				break;
-			}
-			if (tmp)
-				free_extent_buffer(tmp);
-			slot++;
+			break;
 		}
 find_next_key:
 		/*
@@ -4935,7 +4906,7 @@ int btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,
 			path->slots[level] = slot;
 			btrfs_set_path_blocking(path);
 			sret = btrfs_find_next_key(root, path, min_key, level,
-						  cache_only, min_trans);
+						  min_trans);
 			if (sret == 0) {
 				btrfs_release_path(path);
 				goto again;
@@ -5400,8 +5371,7 @@ int btrfs_compare_trees(struct btrfs_root *left_root,
 /*
  * this is similar to btrfs_next_leaf, but does not try to preserve
  * and fixup the path.  It looks for and returns the next key in the
- * tree based on the current path and the cache_only and min_trans
- * parameters.
+ * tree based on the current path and the min_trans parameters.
  *
  * 0 is returned if another key is found, < 0 if there are any errors
  * and 1 is returned if there are no higher keys in the tree
@@ -5410,8 +5380,7 @@ int btrfs_compare_trees(struct btrfs_root *left_root,
  * calling this function.
  */
 int btrfs_find_next_key(struct btrfs_root *root, struct btrfs_path *path,
-			struct btrfs_key *key, int level,
-			int cache_only, u64 min_trans)
+			struct btrfs_key *key, int level, u64 min_trans)
 {
 	int slot;
 	struct extent_buffer *c;
@@ -5462,22 +5431,8 @@ int btrfs_find_next_key(struct btrfs_root *root, struct btrfs_path *path,
 		if (level == 0)
 			btrfs_item_key_to_cpu(c, key, slot);
 		else {
-			u64 blockptr = btrfs_node_blockptr(c, slot);
 			u64 gen = btrfs_node_ptr_generation(c, slot);
 
-			if (cache_only) {
-				struct extent_buffer *cur;
-				cur = btrfs_find_tree_block(root, blockptr,
-					    btrfs_level_size(root, level - 1));
-				if (!cur ||
-				    btrfs_buffer_uptodate(cur, gen, 1) <= 0) {
-					slot++;
-					if (cur)
-						free_extent_buffer(cur);
-					goto next;
-				}
-				free_extent_buffer(cur);
-			}
 			if (gen < min_trans) {
 				slot++;
 				goto next;

commit 1c697d4acc2e10b2a65b22abba2687a2897ecd74
Author: Eric Sandeen <sandeen@redhat.com>
Date:   Thu Jan 31 00:54:56 2013 +0000

    btrfs: annotate intentional switch case fallthroughs
    
    This keeps static checkers happy.
    
    Signed-off-by: Eric Sandeen <sandeen@redhat.com>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 6eff0fa9ecaa..35444013f0cf 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1138,6 +1138,7 @@ __tree_mod_log_rewind(struct extent_buffer *eb, u64 time_seq,
 		switch (tm->op) {
 		case MOD_LOG_KEY_REMOVE_WHILE_FREEING:
 			BUG_ON(tm->slot < n);
+			/* Fallthrough */
 		case MOD_LOG_KEY_REMOVE_WHILE_MOVING:
 		case MOD_LOG_KEY_REMOVE:
 			btrfs_set_node_key(eb, &tm->key, tm->slot);

commit 2a745b14bc99d52c29d0c886a110321f651cf183
Author: Arne Jansen <sensille@gmx.net>
Date:   Wed Feb 13 04:20:01 2013 -0700

    Btrfs: fix crash in log replay with qgroups enabled
    
    When replaying a log tree with qgroups enabled, tree_mod_log_rewind does a
    sanity-check of the number of items against the maximum possible number.
    It calculates that number with the nodesize of fs_root. Unfortunately
    fs_root is not yet set at this stage. So instead use the nodesize from
    tree_root, which is already initialized.
    
    Signed-off-by: Arne Jansen <sensille@gmx.net>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index eea5da7a2b9a..6eff0fa9ecaa 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1222,7 +1222,7 @@ tree_mod_log_rewind(struct btrfs_fs_info *fs_info, struct extent_buffer *eb,
 
 	__tree_mod_log_rewind(eb_rewin, time_seq, tm);
 	WARN_ON(btrfs_header_nritems(eb_rewin) >
-		BTRFS_NODEPTRS_PER_BLOCK(fs_info->fs_root));
+		BTRFS_NODEPTRS_PER_BLOCK(fs_info->tree_root));
 
 	return eb_rewin;
 }

commit 57ba86c00f9573b63b8c06810d4f6915efed2442
Author: Chris Mason <chris.mason@fusionio.com>
Date:   Tue Dec 18 19:35:32 2012 -0500

    Revert "Btrfs: reorder tree mod log operations in deleting a pointer"
    
    This reverts commit 6a7a665d78c5dd8bc76a010648c4e7d84517ab5a.
    
    This was bug was fixed differently in 3.6, so this commit
    isn't needed.
    
    Conflicts:
            fs/btrfs/ctree.c
    
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 569c0dfb526c..eea5da7a2b9a 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -4611,12 +4611,6 @@ static void del_ptr(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 	u32 nritems;
 	int ret;
 
-	if (level) {
-		ret = tree_mod_log_insert_key(root->fs_info, parent, slot,
-					      MOD_LOG_KEY_REMOVE);
-		BUG_ON(ret < 0);
-	}
-
 	nritems = btrfs_header_nritems(parent);
 	if (slot != nritems - 1) {
 		if (level)
@@ -4627,6 +4621,10 @@ static void del_ptr(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 			      btrfs_node_key_ptr_offset(slot + 1),
 			      sizeof(struct btrfs_key_ptr) *
 			      (nritems - slot - 1));
+	} else if (level) {
+		ret = tree_mod_log_insert_key(root->fs_info, parent, slot,
+					      MOD_LOG_KEY_REMOVE);
+		BUG_ON(ret < 0);
 	}
 
 	nritems--;

commit 4c3e696981a565aace08678e70c40709a85f9b2b
Author: Chris Mason <chris.mason@fusionio.com>
Date:   Tue Dec 18 15:43:18 2012 -0500

    Revert "Btrfs: MOD_LOG_KEY_REMOVE_WHILE_MOVING never change node's nritems"
    
    This reverts commit 95c80bb1f6b24b57058d971ed252b2c1c5121b51.
    
    The bug addressed by this commit was fixed differently back in 3.6
    
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index c7b67cf24bba..569c0dfb526c 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1138,13 +1138,13 @@ __tree_mod_log_rewind(struct extent_buffer *eb, u64 time_seq,
 		switch (tm->op) {
 		case MOD_LOG_KEY_REMOVE_WHILE_FREEING:
 			BUG_ON(tm->slot < n);
-		case MOD_LOG_KEY_REMOVE:
-			n++;
 		case MOD_LOG_KEY_REMOVE_WHILE_MOVING:
+		case MOD_LOG_KEY_REMOVE:
 			btrfs_set_node_key(eb, &tm->key, tm->slot);
 			btrfs_set_node_blockptr(eb, tm->slot, tm->blockptr);
 			btrfs_set_node_ptr_generation(eb, tm->slot,
 						      tm->generation);
+			n++;
 			break;
 		case MOD_LOG_KEY_REPLACE:
 			BUG_ON(tm->slot >= n);

commit 5124e00ec5b0be56155a11aec416fcc5125339f1
Author: Josef Bacik <jbacik@fusionio.com>
Date:   Wed Nov 7 13:44:13 2012 -0500

    Btrfs: only unlock and relock if we have to
    
    I noticed while doing fsync tests that we were always dropping the path and
    re-searching when we first cow the log root even though we've already gotten
    the write lock on the root.  That's because we don't take into account that
    there might not be a parent node, so fix the check to make sure there is
    actually a parent node before we undo all of this work for nothing.  Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index e7bea1d5f75f..c7b67cf24bba 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2564,7 +2564,10 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 			 * must have write locks on this node and the
 			 * parent
 			 */
-			if (level + 1 > write_lock_level) {
+			if (level > write_lock_level ||
+			    (level + 1 > write_lock_level &&
+			    level + 1 < BTRFS_MAX_LEVEL &&
+			    p->nodes[level + 1])) {
 				write_lock_level = level + 1;
 				btrfs_release_path(p);
 				goto again;

commit 41be1f3b40b87de33cd2e7463dce88596dbdccc4
Author: Josef Bacik <jbacik@fusionio.com>
Date:   Mon Oct 15 13:43:18 2012 -0400

    Btrfs: optimize leaf_space_used
    
    This gets called at least 4 times for every level while adding an object,
    and it involves 3 kmapping calls, which on my box take about 5us a piece.
    So instead use a token, which brings us down to 1 kmap call and makes this
    function take 1/3 of the time per call.  Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index e8b32641ea90..e7bea1d5f75f 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -3298,14 +3298,21 @@ static noinline int split_node(struct btrfs_trans_handle *trans,
  */
 static int leaf_space_used(struct extent_buffer *l, int start, int nr)
 {
+	struct btrfs_item *start_item;
+	struct btrfs_item *end_item;
+	struct btrfs_map_token token;
 	int data_len;
 	int nritems = btrfs_header_nritems(l);
 	int end = min(nritems, start + nr) - 1;
 
 	if (!nr)
 		return 0;
-	data_len = btrfs_item_end_nr(l, start);
-	data_len = data_len - btrfs_item_offset_nr(l, end);
+	btrfs_init_map_token(&token);
+	start_item = btrfs_item_nr(l, start);
+	end_item = btrfs_item_nr(l, end);
+	data_len = btrfs_token_item_offset(l, start_item, &token) +
+		btrfs_token_item_size(l, start_item, &token);
+	data_len = data_len - btrfs_token_item_offset(l, end_item, &token);
 	data_len += sizeof(struct btrfs_item) * nr;
 	WARN_ON(data_len < 0);
 	return data_len;

commit 70c8a91ce21b83ccd2d9e7c968775430ead4353d
Author: Josef Bacik <jbacik@fusionio.com>
Date:   Thu Oct 11 16:54:30 2012 -0400

    Btrfs: log changed inodes based on the extent map tree
    
    We don't really need to copy extents from the source tree since we have all
    of the information already available to us in the extent_map tree.  So
    instead just write the extents straight to the log tree and don't bother to
    copy the extent items from the source tree.
    
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 0c5c28ff794f..e8b32641ea90 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -5490,6 +5490,139 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 	return btrfs_next_old_leaf(root, path, 0);
 }
 
+/* Release the path up to but not including the given level */
+static void btrfs_release_level(struct btrfs_path *path, int level)
+{
+	int i;
+
+	for (i = 0; i < level; i++) {
+		path->slots[i] = 0;
+		if (!path->nodes[i])
+			continue;
+		if (path->locks[i]) {
+			btrfs_tree_unlock_rw(path->nodes[i], path->locks[i]);
+			path->locks[i] = 0;
+		}
+		free_extent_buffer(path->nodes[i]);
+		path->nodes[i] = NULL;
+	}
+}
+
+/*
+ * This function assumes 2 things
+ *
+ * 1) You are using path->keep_locks
+ * 2) You are not inserting items.
+ *
+ * If either of these are not true do not use this function. If you need a next
+ * leaf with either of these not being true then this function can be easily
+ * adapted to do that, but at the moment these are the limitations.
+ */
+int btrfs_next_leaf_write(struct btrfs_trans_handle *trans,
+			  struct btrfs_root *root, struct btrfs_path *path,
+			  int del)
+{
+	struct extent_buffer *b;
+	struct btrfs_key key;
+	u32 nritems;
+	int level = 1;
+	int slot;
+	int ret = 1;
+	int write_lock_level = BTRFS_MAX_LEVEL;
+	int ins_len = del ? -1 : 0;
+
+	WARN_ON(!(path->keep_locks || path->really_keep_locks));
+
+	nritems = btrfs_header_nritems(path->nodes[0]);
+	btrfs_item_key_to_cpu(path->nodes[0], &key, nritems - 1);
+
+	while (path->nodes[level]) {
+		nritems = btrfs_header_nritems(path->nodes[level]);
+		if (!(path->locks[level] & BTRFS_WRITE_LOCK)) {
+search:
+			btrfs_release_path(path);
+			ret = btrfs_search_slot(trans, root, &key, path,
+						ins_len, 1);
+			if (ret < 0)
+				goto out;
+			level = 1;
+			continue;
+		}
+
+		if (path->slots[level] >= nritems - 1) {
+			level++;
+			continue;
+		}
+
+		btrfs_release_level(path, level);
+		break;
+	}
+
+	if (!path->nodes[level]) {
+		ret = 1;
+		goto out;
+	}
+
+	path->slots[level]++;
+	b = path->nodes[level];
+
+	while (b) {
+		level = btrfs_header_level(b);
+
+		if (!should_cow_block(trans, root, b))
+			goto cow_done;
+
+		btrfs_set_path_blocking(path);
+		ret = btrfs_cow_block(trans, root, b,
+				      path->nodes[level + 1],
+				      path->slots[level + 1], &b);
+		if (ret)
+			goto out;
+cow_done:
+		path->nodes[level] = b;
+		btrfs_clear_path_blocking(path, NULL, 0);
+		if (level != 0) {
+			ret = setup_nodes_for_search(trans, root, path, b,
+						     level, ins_len,
+						     &write_lock_level);
+			if (ret == -EAGAIN)
+				goto search;
+			if (ret)
+				goto out;
+
+			b = path->nodes[level];
+			slot = path->slots[level];
+
+			ret = read_block_for_search(trans, root, path,
+						    &b, level, slot, &key, 0);
+			if (ret == -EAGAIN)
+				goto search;
+			if (ret)
+				goto out;
+			level = btrfs_header_level(b);
+			if (!btrfs_try_tree_write_lock(b)) {
+				btrfs_set_path_blocking(path);
+				btrfs_tree_lock(b);
+				btrfs_clear_path_blocking(path, b,
+							  BTRFS_WRITE_LOCK);
+			}
+			path->locks[level] = BTRFS_WRITE_LOCK;
+			path->nodes[level] = b;
+			path->slots[level] = 0;
+		} else {
+			path->slots[level] = 0;
+			ret = 0;
+			break;
+		}
+	}
+
+out:
+	if (ret)
+		btrfs_release_path(path);
+
+	return ret;
+}
+
 int btrfs_next_old_leaf(struct btrfs_root *root, struct btrfs_path *path,
 			u64 time_seq)
 {

commit d6393786cd40f67709324bc4f08d7e4b911153fe
Author: Josef Bacik <jbacik@fusionio.com>
Date:   Wed Dec 12 17:00:01 2012 -0500

    Btrfs: add path->really_keep_locks
    
    You'd think path->keep_locks would keep all the locks wouldn't you?  You'd
    be wrong.  It only keeps them if the slot is pointing to the last item in
    the node.  This is for use with btrfs_next_leaf, which needs this sort of
    thing.  But the horrible horrible things I'm going to do to the tree log
    means I really need everything held from root to leaf so I can add and
    delete items in the same search.  Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 01efcbc80dfb..0c5c28ff794f 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2212,6 +2212,9 @@ static noinline void unlock_up(struct btrfs_path *path, int level,
 	int no_skips = 0;
 	struct extent_buffer *t;
 
+	if (path->really_keep_locks)
+		return;
+
 	for (i = level; i < BTRFS_MAX_LEVEL; i++) {
 		if (!path->nodes[i])
 			break;
@@ -2259,7 +2262,7 @@ noinline void btrfs_unlock_up_safe(struct btrfs_path *path, int level)
 {
 	int i;
 
-	if (path->keep_locks)
+	if (path->keep_locks || path->really_keep_locks)
 		return;
 
 	for (i = level; i < BTRFS_MAX_LEVEL; i++) {
@@ -2492,7 +2495,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 	if (!cow)
 		write_lock_level = -1;
 
-	if (cow && (p->keep_locks || p->lowest_level))
+	if (cow && (p->really_keep_locks || p->keep_locks || p->lowest_level))
 		write_lock_level = BTRFS_MAX_LEVEL;
 
 	min_write_lock_level = write_lock_level;

commit 5f3ab90a72f98adbf00c50ac2d4d2b47cf4a9685
Author: Anand Jain <anand.jain@oracle.com>
Date:   Fri Dec 7 09:28:54 2012 +0000

    Btrfs: rename root_times_lock to root_item_lock
    
    Originally root_times_lock was introduced as part of send/receive
    code however newly developed patch to label the subvol reused
    the same lock, so renaming it for a meaningful name.
    
    Signed-off-by: Anand Jain <anand.jain@oracle.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 5c2cf992e717..01efcbc80dfb 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -5114,13 +5114,13 @@ int btrfs_compare_trees(struct btrfs_root *left_root,
 	right_path->search_commit_root = 1;
 	right_path->skip_locking = 1;
 
-	spin_lock(&left_root->root_times_lock);
+	spin_lock(&left_root->root_item_lock);
 	left_start_ctransid = btrfs_root_ctransid(&left_root->root_item);
-	spin_unlock(&left_root->root_times_lock);
+	spin_unlock(&left_root->root_item_lock);
 
-	spin_lock(&right_root->root_times_lock);
+	spin_lock(&right_root->root_item_lock);
 	right_start_ctransid = btrfs_root_ctransid(&right_root->root_item);
-	spin_unlock(&right_root->root_times_lock);
+	spin_unlock(&right_root->root_item_lock);
 
 	trans = btrfs_join_transaction(left_root);
 	if (IS_ERR(trans)) {
@@ -5215,15 +5215,15 @@ int btrfs_compare_trees(struct btrfs_root *left_root,
 				goto out;
 			}
 
-			spin_lock(&left_root->root_times_lock);
+			spin_lock(&left_root->root_item_lock);
 			ctransid = btrfs_root_ctransid(&left_root->root_item);
-			spin_unlock(&left_root->root_times_lock);
+			spin_unlock(&left_root->root_item_lock);
 			if (ctransid != left_start_ctransid)
 				left_start_ctransid = 0;
 
-			spin_lock(&right_root->root_times_lock);
+			spin_lock(&right_root->root_item_lock);
 			ctransid = btrfs_root_ctransid(&right_root->root_item);
-			spin_unlock(&right_root->root_times_lock);
+			spin_unlock(&right_root->root_item_lock);
 			if (ctransid != right_start_ctransid)
 				right_start_ctransid = 0;
 

commit 6c1500f22a7be3a24ad3dffcdbf04be3f676521b
Author: Julia Lawall <Julia.Lawall@lip6.fr>
Date:   Sat Nov 3 20:30:18 2012 +0000

    fs/btrfs: drop if around WARN_ON
    
    Just use WARN_ON rather than an if containing only WARN_ON(1).
    
    A simplified version of the semantic patch that makes this transformation
    is as follows: (http://coccinelle.lip6.fr/)
    
    // <smpl>
    @@
    expression e;
    @@
    - if (e) WARN_ON(1);
    + WARN_ON(e);
    // </smpl>
    
    Signed-off-by: Julia Lawall <Julia.Lawall@lip6.fr>
    Reviewed-by: David Sterba <dsterba@suse.cz>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 0e4adb00e9d9..5c2cf992e717 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1464,10 +1464,8 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 	if (cache_only && parent_level != 1)
 		return 0;
 
-	if (trans->transaction != root->fs_info->running_transaction)
-		WARN_ON(1);
-	if (trans->transid != root->fs_info->generation)
-		WARN_ON(1);
+	WARN_ON(trans->transaction != root->fs_info->running_transaction);
+	WARN_ON(trans->transid != root->fs_info->generation);
 
 	parent_nritems = btrfs_header_nritems(parent);
 	blocksize = btrfs_level_size(root, parent_level - 1);
@@ -3398,8 +3396,7 @@ static noinline int __push_leaf_right(struct btrfs_trans_handle *trans,
 	if (push_items == 0)
 		goto out_unlock;
 
-	if (!empty && push_items == left_nritems)
-		WARN_ON(1);
+	WARN_ON(!empty && push_items == left_nritems);
 
 	/* push left to right */
 	right_nritems = btrfs_header_nritems(right);

commit 31b1a2bd758f439fc945b3ac5899d890cb7e2dc6
Author: Julia Lawall <Julia.Lawall@lip6.fr>
Date:   Sat Nov 3 10:58:34 2012 +0000

    fs/btrfs: use WARN
    
    Use WARN rather than printk followed by WARN_ON(1), for conciseness.
    
    A simplified version of the semantic patch that makes this transformation
    is as follows: (http://coccinelle.lip6.fr/)
    
    // <smpl>
    @@
    expression list es;
    @@
    
    -printk(
    +WARN(1,
      es);
    -WARN_ON(1);
    // </smpl>
    
    Signed-off-by: Julia Lawall <Julia.Lawall@lip6.fr>
    Reviewed-by: David Sterba <dsterba@suse.cz>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 100c274a1cfe..0e4adb00e9d9 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1359,19 +1359,16 @@ noinline int btrfs_cow_block(struct btrfs_trans_handle *trans,
 	u64 search_start;
 	int ret;
 
-	if (trans->transaction != root->fs_info->running_transaction) {
-		printk(KERN_CRIT "trans %llu running %llu\n",
+	if (trans->transaction != root->fs_info->running_transaction)
+		WARN(1, KERN_CRIT "trans %llu running %llu\n",
 		       (unsigned long long)trans->transid,
 		       (unsigned long long)
 		       root->fs_info->running_transaction->transid);
-		WARN_ON(1);
-	}
-	if (trans->transid != root->fs_info->generation) {
-		printk(KERN_CRIT "trans %llu running %llu\n",
+
+	if (trans->transid != root->fs_info->generation)
+		WARN(1, KERN_CRIT "trans %llu running %llu\n",
 		       (unsigned long long)trans->transid,
 		       (unsigned long long)root->fs_info->generation);
-		WARN_ON(1);
-	}
 
 	if (!should_cow_block(trans, root, buf)) {
 		*cow_ret = buf;
@@ -3640,11 +3637,9 @@ static noinline int __push_leaf_left(struct btrfs_trans_handle *trans,
 	btrfs_set_header_nritems(left, old_left_nritems + push_items);
 
 	/* fixup right node */
-	if (push_items > right_nritems) {
-		printk(KERN_CRIT "push items %d nr %u\n", push_items,
+	if (push_items > right_nritems)
+		WARN(1, KERN_CRIT "push items %d nr %u\n", push_items,
 		       right_nritems);
-		WARN_ON(1);
-	}
 
 	if (push_items < right_nritems) {
 		push_space = btrfs_item_offset_nr(right, push_items - 1) -

commit 32adf0901371c8b9d258dba7811f3067d1d2ea5c
Author: Liu Bo <bo.li.liu@oracle.com>
Date:   Fri Oct 19 12:52:15 2012 +0000

    Btrfs: cleanup unused arguments
    
    'disk_key' is not used at all.
    
    Signed-off-by: Liu Bo <bo.li.liu@oracle.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 615b74968fab..100c274a1cfe 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -775,8 +775,7 @@ tree_mod_log_eb_move(struct btrfs_fs_info *fs_info, struct extent_buffer *dst,
 
 static noinline void
 tree_mod_log_set_node_key(struct btrfs_fs_info *fs_info,
-			  struct extent_buffer *eb,
-			  struct btrfs_disk_key *disk_key, int slot, int atomic)
+			  struct extent_buffer *eb, int slot, int atomic)
 {
 	int ret;
 
@@ -1835,7 +1834,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 			struct btrfs_disk_key right_key;
 			btrfs_node_key(right, &right_key, 0);
 			tree_mod_log_set_node_key(root->fs_info, parent,
-						  &right_key, pslot + 1, 0);
+						  pslot + 1, 0);
 			btrfs_set_node_key(parent, &right_key, pslot + 1);
 			btrfs_mark_buffer_dirty(parent);
 		}
@@ -1879,7 +1878,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		/* update the parent key to reflect our changes */
 		struct btrfs_disk_key mid_key;
 		btrfs_node_key(mid, &mid_key, 0);
-		tree_mod_log_set_node_key(root->fs_info, parent, &mid_key,
+		tree_mod_log_set_node_key(root->fs_info, parent,
 					  pslot, 0);
 		btrfs_set_node_key(parent, &mid_key, pslot);
 		btrfs_mark_buffer_dirty(parent);
@@ -1979,7 +1978,7 @@ static noinline int push_nodes_for_insert(struct btrfs_trans_handle *trans,
 			orig_slot += left_nr;
 			btrfs_node_key(mid, &disk_key, 0);
 			tree_mod_log_set_node_key(root->fs_info, parent,
-						  &disk_key, pslot, 0);
+						  pslot, 0);
 			btrfs_set_node_key(parent, &disk_key, pslot);
 			btrfs_mark_buffer_dirty(parent);
 			if (btrfs_header_nritems(left) > orig_slot) {
@@ -2032,7 +2031,7 @@ static noinline int push_nodes_for_insert(struct btrfs_trans_handle *trans,
 
 			btrfs_node_key(right, &disk_key, 0);
 			tree_mod_log_set_node_key(root->fs_info, parent,
-						  &disk_key, pslot + 1, 0);
+						  pslot + 1, 0);
 			btrfs_set_node_key(parent, &disk_key, pslot + 1);
 			btrfs_mark_buffer_dirty(parent);
 
@@ -2916,7 +2915,7 @@ static void fixup_low_keys(struct btrfs_trans_handle *trans,
 		if (!path->nodes[i])
 			break;
 		t = path->nodes[i];
-		tree_mod_log_set_node_key(root->fs_info, t, key, tslot, 1);
+		tree_mod_log_set_node_key(root->fs_info, t, tslot, 1);
 		btrfs_set_node_key(t, key, tslot);
 		btrfs_mark_buffer_dirty(path->nodes[i]);
 		if (tslot != 0)

commit 0e411ecec60138f22442728f036d38cfea007817
Author: Liu Bo <bo.li.liu@oracle.com>
Date:   Fri Oct 19 09:50:54 2012 +0000

    Btrfs: kill unnecessary arguments in del_ptr
    
    The argument 'tree_mod_log' is not necessary since all of callers enable it.
    
    Signed-off-by: Liu Bo <bo.li.liu@oracle.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 4d518bd7751d..615b74968fab 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -38,8 +38,7 @@ static int balance_node_right(struct btrfs_trans_handle *trans,
 			      struct extent_buffer *dst_buf,
 			      struct extent_buffer *src_buf);
 static void del_ptr(struct btrfs_trans_handle *trans, struct btrfs_root *root,
-		    struct btrfs_path *path, int level, int slot,
-		    int tree_mod_log);
+		    struct btrfs_path *path, int level, int slot);
 static void tree_mod_log_free_eb(struct btrfs_fs_info *fs_info,
 				 struct extent_buffer *eb);
 struct extent_buffer *read_old_tree_block(struct btrfs_root *root, u64 bytenr,
@@ -1827,7 +1826,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		if (btrfs_header_nritems(right) == 0) {
 			clean_tree_block(trans, root, right);
 			btrfs_tree_unlock(right);
-			del_ptr(trans, root, path, level + 1, pslot + 1, 1);
+			del_ptr(trans, root, path, level + 1, pslot + 1);
 			root_sub_used(root, right->len);
 			btrfs_free_tree_block(trans, root, right, 0, 1);
 			free_extent_buffer_stale(right);
@@ -1871,7 +1870,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 	if (btrfs_header_nritems(mid) == 0) {
 		clean_tree_block(trans, root, mid);
 		btrfs_tree_unlock(mid);
-		del_ptr(trans, root, path, level + 1, pslot, 1);
+		del_ptr(trans, root, path, level + 1, pslot);
 		root_sub_used(root, mid->len);
 		btrfs_free_tree_block(trans, root, mid, 0, 1);
 		free_extent_buffer_stale(mid);
@@ -4602,14 +4601,13 @@ int btrfs_insert_item(struct btrfs_trans_handle *trans, struct btrfs_root
  * empty a node.
  */
 static void del_ptr(struct btrfs_trans_handle *trans, struct btrfs_root *root,
-		    struct btrfs_path *path, int level, int slot,
-		    int tree_mod_log)
+		    struct btrfs_path *path, int level, int slot)
 {
 	struct extent_buffer *parent = path->nodes[level];
 	u32 nritems;
 	int ret;
 
-	if (tree_mod_log && level) {
+	if (level) {
 		ret = tree_mod_log_insert_key(root->fs_info, parent, slot,
 					      MOD_LOG_KEY_REMOVE);
 		BUG_ON(ret < 0);
@@ -4617,7 +4615,7 @@ static void del_ptr(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 
 	nritems = btrfs_header_nritems(parent);
 	if (slot != nritems - 1) {
-		if (tree_mod_log && level)
+		if (level)
 			tree_mod_log_eb_move(root->fs_info, parent, slot,
 					     slot + 1, nritems - slot - 1);
 		memmove_extent_buffer(parent,
@@ -4658,7 +4656,7 @@ static noinline void btrfs_del_leaf(struct btrfs_trans_handle *trans,
 				    struct extent_buffer *leaf)
 {
 	WARN_ON(btrfs_header_generation(leaf) != trans->transid);
-	del_ptr(trans, root, path, 1, path->slots[1], 1);
+	del_ptr(trans, root, path, 1, path->slots[1]);
 
 	/*
 	 * btrfs_free_extent is expensive, we want to make sure we

commit 6a7a665d78c5dd8bc76a010648c4e7d84517ab5a
Author: Liu Bo <bo.li.liu@oracle.com>
Date:   Fri Oct 19 09:50:53 2012 +0000

    Btrfs: reorder tree mod log operations in deleting a pointer
    
    Since we don't use MOD_LOG_KEY_REMOVE_WHILE_MOVING to add nritems
    during rewinding, we should insert a MOD_LOG_KEY_REMOVE operation first.
    
    Signed-off-by: Liu Bo <bo.li.liu@oracle.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index b12c03959162..4d518bd7751d 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -4609,6 +4609,12 @@ static void del_ptr(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 	u32 nritems;
 	int ret;
 
+	if (tree_mod_log && level) {
+		ret = tree_mod_log_insert_key(root->fs_info, parent, slot,
+					      MOD_LOG_KEY_REMOVE);
+		BUG_ON(ret < 0);
+	}
+
 	nritems = btrfs_header_nritems(parent);
 	if (slot != nritems - 1) {
 		if (tree_mod_log && level)
@@ -4619,10 +4625,6 @@ static void del_ptr(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 			      btrfs_node_key_ptr_offset(slot + 1),
 			      sizeof(struct btrfs_key_ptr) *
 			      (nritems - slot - 1));
-	} else if (tree_mod_log && level) {
-		ret = tree_mod_log_insert_key(root->fs_info, parent, slot,
-					      MOD_LOG_KEY_REMOVE);
-		BUG_ON(ret < 0);
 	}
 
 	nritems--;

commit 95c80bb1f6b24b57058d971ed252b2c1c5121b51
Author: Liu Bo <bo.li.liu@oracle.com>
Date:   Fri Oct 19 09:50:52 2012 +0000

    Btrfs: MOD_LOG_KEY_REMOVE_WHILE_MOVING never change node's nritems
    
    Key MOD_LOG_KEY_REMOVE_WHILE_MOVING means that we're doing memmove inside
    an extent buffer node, and the node's number of items remains unchanged
    (unless we are inserting a single pointer, but we have MOD_LOG_KEY_ADD for that).
    
    So we don't need to increase node's number of items during rewinding,
    otherwise we may get an node larger than leafsize and cause general protection
    errors later.
    
    Here is the details,
    - If we do memory move for inserting a single pointer, we need to
      add node's nritems by one, and we honor MOD_LOG_KEY_ADD for adding.
    
    - If we do memory move for deleting a single pointer, we need to
      decrease node's nritems by one, and we honor MOD_LOG_KEY_REMOVE for
      deleting.
    
    - If we do memory move for balance left/right, we need to decrease
      node's nritems, and we honor MOD_LOG_KEY_REMOVE for balaning.
    
    Signed-off-by: Liu Bo <bo.li.liu@oracle.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index cdfb4c49a806..b12c03959162 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1140,13 +1140,13 @@ __tree_mod_log_rewind(struct extent_buffer *eb, u64 time_seq,
 		switch (tm->op) {
 		case MOD_LOG_KEY_REMOVE_WHILE_FREEING:
 			BUG_ON(tm->slot < n);
-		case MOD_LOG_KEY_REMOVE_WHILE_MOVING:
 		case MOD_LOG_KEY_REMOVE:
+			n++;
+		case MOD_LOG_KEY_REMOVE_WHILE_MOVING:
 			btrfs_set_node_key(eb, &tm->key, tm->slot);
 			btrfs_set_node_blockptr(eb, tm->slot, tm->blockptr);
 			btrfs_set_node_ptr_generation(eb, tm->slot,
 						      tm->generation);
-			n++;
 			break;
 		case MOD_LOG_KEY_REPLACE:
 			BUG_ON(tm->slot >= n);

commit 7bfdcf7fbad56c0f1fc3e2d26431bed72bdcce2d
Author: Liu Bo <bo.li.liu@oracle.com>
Date:   Thu Oct 25 07:30:19 2012 -0600

    Btrfs: fix memory leak when cloning root's node
    
    After cloning root's node, we forgot to dec the src's ref
    which can lead to a memory leak.
    
    Signed-off-by: Liu Bo <bo.li.liu@oracle.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index eba44b076829..cdfb4c49a806 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1241,6 +1241,7 @@ get_old_root(struct btrfs_root *root, u64 time_seq)
 {
 	struct tree_mod_elem *tm;
 	struct extent_buffer *eb;
+	struct extent_buffer *old;
 	struct tree_mod_root *old_root = NULL;
 	u64 old_generation = 0;
 	u64 logical;
@@ -1264,13 +1265,14 @@ get_old_root(struct btrfs_root *root, u64 time_seq)
 		btrfs_tree_read_unlock(root->node);
 		free_extent_buffer(root->node);
 		blocksize = btrfs_level_size(root, old_root->level);
-		eb = read_tree_block(root, logical, blocksize, 0);
-		if (!eb) {
+		old = read_tree_block(root, logical, blocksize, 0);
+		if (!old) {
 			pr_warn("btrfs: failed to read tree block %llu from get_old_root\n",
 				logical);
 			WARN_ON(1);
 		} else {
-			eb = btrfs_clone_extent_buffer(eb);
+			eb = btrfs_clone_extent_buffer(old);
+			free_extent_buffer(old);
 		}
 	} else if (old_root) {
 		btrfs_tree_read_unlock(root->node);

commit 01763a2e37425ae3f37a3dc051e0703fdade5956
Author: Jan Schmidt <list.btrfs@jan-o-sch.net>
Date:   Tue Oct 23 15:02:12 2012 +0200

    Btrfs: comment for loop in tree_mod_log_insert_move
    
    Emphasis the way tree_mod_log_insert_move avoids adding
    MOD_LOG_KEY_REMOVE_WHILE_MOVING operations, depending on the direction of
    the move operation.
    
    Signed-off-by: Jan Schmidt <list.btrfs@jan-o-sch.net>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index f6739903d072..eba44b076829 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -596,6 +596,11 @@ tree_mod_log_insert_move(struct btrfs_fs_info *fs_info,
 	if (tree_mod_dont_log(fs_info, eb))
 		return 0;
 
+	/*
+	 * When we override something during the move, we log these removals.
+	 * This can only happen when we move towards the beginning of the
+	 * buffer, i.e. dst_slot < src_slot.
+	 */
 	for (i = 0; i + dst_slot < src_slot && i < nr_items; i++) {
 		ret = tree_mod_log_insert_key_locked(fs_info, eb, i + dst_slot,
 					      MOD_LOG_KEY_REMOVE_WHILE_MOVING);

commit d638108484d186bf97a838d037f165d9b404e6ee
Author: Jan Schmidt <list.btrfs@jan-o-sch.net>
Date:   Tue Oct 23 14:21:05 2012 +0200

    Btrfs: fix extent buffer reference for tree mod log roots
    
    In get_old_root we grab a lock on the extent buffer before we obtain a
    reference on that buffer. That order is changed now.
    
    Signed-off-by: Jan Schmidt <list.btrfs@jan-o-sch.net>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index ef68c33eefd9..f6739903d072 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1279,6 +1279,7 @@ get_old_root(struct btrfs_root *root, u64 time_seq)
 
 	if (!eb)
 		return NULL;
+	extent_buffer_get(eb);
 	btrfs_tree_read_lock(eb);
 	if (old_root) {
 		btrfs_set_header_bytenr(eb, eb->start);
@@ -1291,7 +1292,6 @@ get_old_root(struct btrfs_root *root, u64 time_seq)
 		__tree_mod_log_rewind(eb, time_seq, tm);
 	else
 		WARN_ON(btrfs_header_level(eb) != 0);
-	extent_buffer_get(eb);
 	WARN_ON(btrfs_header_nritems(eb) > BTRFS_NODEPTRS_PER_BLOCK(root));
 
 	return eb;

commit 5b6602e762cae17c8891d19698afea451e9c1d95
Author: Jan Schmidt <list.btrfs@jan-o-sch.net>
Date:   Tue Oct 23 11:28:27 2012 +0200

    Btrfs: determine level of old roots
    
    In btrfs_find_all_roots' termination condition, we compare the level of the
    old buffer we got from btrfs_search_old_slot to the level of the current
    root node. We'd better compare it to the level of the rewinded root node.
    
    Signed-off-by: Jan Schmidt <list.btrfs@jan-o-sch.net>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index d9308c38a8f2..ef68c33eefd9 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1297,6 +1297,23 @@ get_old_root(struct btrfs_root *root, u64 time_seq)
 	return eb;
 }
 
+int btrfs_old_root_level(struct btrfs_root *root, u64 time_seq)
+{
+	struct tree_mod_elem *tm;
+	int level;
+
+	tm = __tree_mod_log_oldest_root(root->fs_info, root, time_seq);
+	if (tm && tm->op == MOD_LOG_ROOT_REPLACE) {
+		level = tm->old_root.level;
+	} else {
+		rcu_read_lock();
+		level = btrfs_header_level(root->node);
+		rcu_read_unlock();
+	}
+
+	return level;
+}
+
 static inline int should_cow_block(struct btrfs_trans_handle *trans,
 				   struct btrfs_root *root,
 				   struct extent_buffer *buf)

commit 834328a8493079d15f30866ace42489463f52571
Author: Jan Schmidt <list.btrfs@jan-o-sch.net>
Date:   Tue Oct 23 11:27:33 2012 +0200

    Btrfs: tree mod log's old roots could still be part of the tree
    
    Tree mod log treated old root buffers as always empty buffers when starting
    the rewind operations. However, the old root may still be part of the
    current tree at a lower level, with still some valid entries.
    
    Signed-off-by: Jan Schmidt <list.btrfs@jan-o-sch.net>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index e6b75ccd2850..d9308c38a8f2 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1239,6 +1239,7 @@ get_old_root(struct btrfs_root *root, u64 time_seq)
 	struct tree_mod_root *old_root = NULL;
 	u64 old_generation = 0;
 	u64 logical;
+	u32 blocksize;
 
 	eb = btrfs_read_lock_root_node(root);
 	tm = __tree_mod_log_oldest_root(root->fs_info, root, time_seq);
@@ -1254,12 +1255,28 @@ get_old_root(struct btrfs_root *root, u64 time_seq)
 	}
 
 	tm = tree_mod_log_search(root->fs_info, logical, time_seq);
-	if (old_root)
+	if (old_root && tm && tm->op != MOD_LOG_KEY_REMOVE_WHILE_FREEING) {
+		btrfs_tree_read_unlock(root->node);
+		free_extent_buffer(root->node);
+		blocksize = btrfs_level_size(root, old_root->level);
+		eb = read_tree_block(root, logical, blocksize, 0);
+		if (!eb) {
+			pr_warn("btrfs: failed to read tree block %llu from get_old_root\n",
+				logical);
+			WARN_ON(1);
+		} else {
+			eb = btrfs_clone_extent_buffer(eb);
+		}
+	} else if (old_root) {
+		btrfs_tree_read_unlock(root->node);
+		free_extent_buffer(root->node);
 		eb = alloc_dummy_extent_buffer(logical, root->nodesize);
-	else
+	} else {
 		eb = btrfs_clone_extent_buffer(root->node);
-	btrfs_tree_read_unlock(root->node);
-	free_extent_buffer(root->node);
+		btrfs_tree_read_unlock(root->node);
+		free_extent_buffer(root->node);
+	}
+
 	if (!eb)
 		return NULL;
 	btrfs_tree_read_lock(eb);

commit ba1bfbd592c1adddd5d0005f587a6e308e25c949
Author: Jan Schmidt <list.btrfs@jan-o-sch.net>
Date:   Mon Oct 22 20:02:56 2012 +0200

    Btrfs: fix a tree mod logging issue for root replacement operations
    
    Avoid the implicit free by tree_mod_log_set_root_pointer, which is wrong in
    two places. Where needed, we call tree_mod_log_free_eb explicitly now.
    
    Signed-off-by: Jan Schmidt <list.btrfs@jan-o-sch.net>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 44a7e25353a6..e6b75ccd2850 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -647,8 +647,6 @@ tree_mod_log_insert_root(struct btrfs_fs_info *fs_info,
 	if (tree_mod_dont_log(fs_info, NULL))
 		return 0;
 
-	__tree_mod_log_free_eb(fs_info, old_root);
-
 	ret = tree_mod_alloc(fs_info, flags, &tm);
 	if (ret < 0)
 		goto out;
@@ -926,12 +924,7 @@ static noinline int update_ref_for_cow(struct btrfs_trans_handle *trans,
 			ret = btrfs_dec_ref(trans, root, buf, 1, 1);
 			BUG_ON(ret); /* -ENOMEM */
 		}
-		/*
-		 * don't log freeing in case we're freeing the root node, this
-		 * is done by tree_mod_log_set_root_pointer later
-		 */
-		if (buf != root->node && btrfs_header_level(buf) != 0)
-			tree_mod_log_free_eb(root->fs_info, buf);
+		tree_mod_log_free_eb(root->fs_info, buf);
 		clean_tree_block(trans, root, buf);
 		*last_ref = 1;
 	}
@@ -1728,6 +1721,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 			goto enospc;
 		}
 
+		tree_mod_log_free_eb(root->fs_info, root->node);
 		tree_mod_log_set_root_pointer(root, child);
 		rcu_assign_pointer(root->node, child);
 

commit 57911b8ba814fae01306376a0d02bc7cdc88dc94
Author: Jan Schmidt <list.btrfs@jan-o-sch.net>
Date:   Fri Oct 19 09:22:03 2012 +0200

    Btrfs: don't put removals from push_node_left into tree mod log twice
    
    Independant of the check (push_items < src_items) tree_mod_log_eb_copy did
    log the removal of the old data entries from the source buffer. Therefore,
    we must not call tree_mod_log_eb_move if the check evaluates to true, as
    that would log the removal twice, finally resulting in (rewinded) buffers
    with wrong values for header_nritems.
    
    Signed-off-by: Jan Schmidt <list.btrfs@jan-o-sch.net>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index b33436211000..44a7e25353a6 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1225,6 +1225,8 @@ tree_mod_log_rewind(struct btrfs_fs_info *fs_info, struct extent_buffer *eb,
 	free_extent_buffer(eb);
 
 	__tree_mod_log_rewind(eb_rewin, time_seq, tm);
+	WARN_ON(btrfs_header_nritems(eb_rewin) >
+		BTRFS_NODEPTRS_PER_BLOCK(fs_info->fs_root));
 
 	return eb_rewin;
 }
@@ -1280,6 +1282,7 @@ get_old_root(struct btrfs_root *root, u64 time_seq)
 	else
 		WARN_ON(btrfs_header_level(eb) != 0);
 	extent_buffer_get(eb);
+	WARN_ON(btrfs_header_nritems(eb) > BTRFS_NODEPTRS_PER_BLOCK(root));
 
 	return eb;
 }
@@ -2970,8 +2973,10 @@ static int push_node_left(struct btrfs_trans_handle *trans,
 			   push_items * sizeof(struct btrfs_key_ptr));
 
 	if (push_items < src_nritems) {
-		tree_mod_log_eb_move(root->fs_info, src, 0, push_items,
-				     src_nritems - push_items);
+		/*
+		 * don't call tree_mod_log_eb_move here, key removal was already
+		 * fully logged by tree_mod_log_eb_copy above.
+		 */
 		memmove_extent_buffer(src, btrfs_node_key_ptr_offset(0),
 				      btrfs_node_key_ptr_offset(push_items),
 				      (src_nritems - push_items) *

commit 8d1a1317af439cc7f349907b79198d35feffe179
Author: Robin Dong <sanbai@taobao.com>
Date:   Sat Sep 29 02:07:46 2012 -0600

    btrfs: remove unused function btrfs_insert_some_items()
    
    The function btrfs_insert_some_items() would not be called by any other functions,
    so remove it.
    
    Signed-off-by: Robin Dong <sanbai@taobao.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index fcc8c21a6233..b33436211000 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -4401,149 +4401,6 @@ void btrfs_extend_item(struct btrfs_trans_handle *trans,
 	}
 }
 
-/*
- * Given a key and some data, insert items into the tree.
- * This does all the path init required, making room in the tree if needed.
- * Returns the number of keys that were inserted.
- */
-int btrfs_insert_some_items(struct btrfs_trans_handle *trans,
-			    struct btrfs_root *root,
-			    struct btrfs_path *path,
-			    struct btrfs_key *cpu_key, u32 *data_size,
-			    int nr)
-{
-	struct extent_buffer *leaf;
-	struct btrfs_item *item;
-	int ret = 0;
-	int slot;
-	int i;
-	u32 nritems;
-	u32 total_data = 0;
-	u32 total_size = 0;
-	unsigned int data_end;
-	struct btrfs_disk_key disk_key;
-	struct btrfs_key found_key;
-	struct btrfs_map_token token;
-
-	btrfs_init_map_token(&token);
-
-	for (i = 0; i < nr; i++) {
-		if (total_size + data_size[i] + sizeof(struct btrfs_item) >
-		    BTRFS_LEAF_DATA_SIZE(root)) {
-			break;
-			nr = i;
-		}
-		total_data += data_size[i];
-		total_size += data_size[i] + sizeof(struct btrfs_item);
-	}
-	BUG_ON(nr == 0);
-
-	ret = btrfs_search_slot(trans, root, cpu_key, path, total_size, 1);
-	if (ret == 0)
-		return -EEXIST;
-	if (ret < 0)
-		goto out;
-
-	leaf = path->nodes[0];
-
-	nritems = btrfs_header_nritems(leaf);
-	data_end = leaf_data_end(root, leaf);
-
-	if (btrfs_leaf_free_space(root, leaf) < total_size) {
-		for (i = nr; i >= 0; i--) {
-			total_data -= data_size[i];
-			total_size -= data_size[i] + sizeof(struct btrfs_item);
-			if (total_size < btrfs_leaf_free_space(root, leaf))
-				break;
-		}
-		nr = i;
-	}
-
-	slot = path->slots[0];
-	BUG_ON(slot < 0);
-
-	if (slot != nritems) {
-		unsigned int old_data = btrfs_item_end_nr(leaf, slot);
-
-		item = btrfs_item_nr(leaf, slot);
-		btrfs_item_key_to_cpu(leaf, &found_key, slot);
-
-		/* figure out how many keys we can insert in here */
-		total_data = data_size[0];
-		for (i = 1; i < nr; i++) {
-			if (btrfs_comp_cpu_keys(&found_key, cpu_key + i) <= 0)
-				break;
-			total_data += data_size[i];
-		}
-		nr = i;
-
-		if (old_data < data_end) {
-			btrfs_print_leaf(root, leaf);
-			printk(KERN_CRIT "slot %d old_data %d data_end %d\n",
-			       slot, old_data, data_end);
-			BUG_ON(1);
-		}
-		/*
-		 * item0..itemN ... dataN.offset..dataN.size .. data0.size
-		 */
-		/* first correct the data pointers */
-		for (i = slot; i < nritems; i++) {
-			u32 ioff;
-
-			item = btrfs_item_nr(leaf, i);
-			ioff = btrfs_token_item_offset(leaf, item, &token);
-			btrfs_set_token_item_offset(leaf, item,
-						    ioff - total_data, &token);
-		}
-		/* shift the items */
-		memmove_extent_buffer(leaf, btrfs_item_nr_offset(slot + nr),
-			      btrfs_item_nr_offset(slot),
-			      (nritems - slot) * sizeof(struct btrfs_item));
-
-		/* shift the data */
-		memmove_extent_buffer(leaf, btrfs_leaf_data(leaf) +
-			      data_end - total_data, btrfs_leaf_data(leaf) +
-			      data_end, old_data - data_end);
-		data_end = old_data;
-	} else {
-		/*
-		 * this sucks but it has to be done, if we are inserting at
-		 * the end of the leaf only insert 1 of the items, since we
-		 * have no way of knowing whats on the next leaf and we'd have
-		 * to drop our current locks to figure it out
-		 */
-		nr = 1;
-	}
-
-	/* setup the item for the new data */
-	for (i = 0; i < nr; i++) {
-		btrfs_cpu_key_to_disk(&disk_key, cpu_key + i);
-		btrfs_set_item_key(leaf, &disk_key, slot + i);
-		item = btrfs_item_nr(leaf, slot + i);
-		btrfs_set_token_item_offset(leaf, item,
-					    data_end - data_size[i], &token);
-		data_end -= data_size[i];
-		btrfs_set_token_item_size(leaf, item, data_size[i], &token);
-	}
-	btrfs_set_header_nritems(leaf, nritems + nr);
-	btrfs_mark_buffer_dirty(leaf);
-
-	ret = 0;
-	if (slot == 0) {
-		btrfs_cpu_key_to_disk(&disk_key, cpu_key);
-		fixup_low_keys(trans, root, path, &disk_key, 1);
-	}
-
-	if (btrfs_leaf_free_space(root, leaf) < 0) {
-		btrfs_print_leaf(root, leaf);
-		BUG();
-	}
-out:
-	if (!ret)
-		ret = nr;
-	return ret;
-}
-
 /*
  * this is a helper for btrfs_insert_empty_items, the main goal here is
  * to save stack depth by doing the bulk of the work in a function

commit 74dd17fbe3d65829e75d84f00a9525b2ace93998
Author: Chris Mason <chris.mason@fusionio.com>
Date:   Tue Aug 7 16:25:13 2012 -0400

    Btrfs: fix btrfs send for inline items and compression
    
    The btrfs send code was assuming the offset of the file item into the
    extent translated to bytes on disk.  If we're compressed, this isn't
    true, and so it was off into extents owned by other files.
    
    It was also improperly handling inline extents.  This solves a crash
    where we may have gone past the end of the file extent item by not
    testing early enough for an inline extent.  It also solves problems
    where we have a whole between the end of the inline item and the start
    of the full extent.
    
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 6d183f60d63a..fcc8c21a6233 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -5073,6 +5073,7 @@ static void tree_move_down(struct btrfs_root *root,
 			   struct btrfs_path *path,
 			   int *level, int root_level)
 {
+	BUG_ON(*level == 0);
 	path->nodes[*level - 1] = read_node_slot(root, path->nodes[*level],
 					path->slots[*level]);
 	path->slots[*level - 1] = 0;
@@ -5089,7 +5090,7 @@ static int tree_move_next_or_upnext(struct btrfs_root *root,
 
 	path->slots[*level]++;
 
-	while (path->slots[*level] == nritems) {
+	while (path->slots[*level] >= nritems) {
 		if (*level == root_level)
 			return -1;
 
@@ -5433,9 +5434,11 @@ int btrfs_compare_trees(struct btrfs_root *left_root,
 					goto out;
 				advance_right = ADVANCE;
 			} else {
+				WARN_ON(!extent_buffer_uptodate(left_path->nodes[0]));
 				ret = tree_compare_item(left_root, left_path,
 						right_path, tmp_buf);
 				if (ret) {
+					WARN_ON(!extent_buffer_uptodate(left_path->nodes[0]));
 					ret = changed_cb(left_root, right_root,
 						left_path, right_path,
 						&left_key,

commit b12a3b1ea209d9dec02731fba58c3dbe7d31cfd8
Author: Chris Mason <chris.mason@fusionio.com>
Date:   Tue Aug 7 15:34:49 2012 -0400

    Btrfs: don't run __tree_mod_log_free_eb on leaves
    
    When we split a leaf, we may end up inserting a new root on top of that
    leaf.  The reflog code was incorrectly assuming the old root was always
    a node.  This makes sure we skip over leaves.
    
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 08e0b11ba0a1..6d183f60d63a 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -625,6 +625,9 @@ __tree_mod_log_free_eb(struct btrfs_fs_info *fs_info, struct extent_buffer *eb)
 	u32 nritems;
 	int ret;
 
+	if (btrfs_header_level(eb) == 0)
+		return;
+
 	nritems = btrfs_header_nritems(eb);
 	for (i = nritems - 1; i >= 0; i--) {
 		ret = tree_mod_log_insert_key_locked(fs_info, eb, i,

commit 1fa11e265fa2562fb713171b6a58e72bb7afd276
Author: Arne Jansen <sensille@gmx.net>
Date:   Mon Aug 6 14:18:51 2012 -0600

    Btrfs: fix deadlock in wait_for_more_refs
    
    Commit a168650c introduced a waiting mechanism to prevent busy waiting in
    btrfs_run_delayed_refs. This can deadlock with btrfs_run_ordered_operations,
    where a tree_mod_seq is held while waiting for the io to complete, while
    the end_io calls btrfs_run_delayed_refs.
    This whole mechanism is unnecessary. If not enough runnable refs are
    available to satisfy count, just return as count is more like a guideline
    than a strict requirement.
    In case we have to run all refs, commit transaction makes sure that no
    other threads are working in the transaction anymore, so we just assert
    here that no refs are blocked.
    
    Signed-off-by: Arne Jansen <sensille@gmx.net>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 9d7621f271ff..08e0b11ba0a1 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -420,12 +420,6 @@ void btrfs_put_tree_mod_seq(struct btrfs_fs_info *fs_info,
 	}
 	spin_unlock(&fs_info->tree_mod_seq_lock);
 
-	/*
-	 * we removed the lowest blocker from the blocker list, so there may be
-	 * more processible delayed refs.
-	 */
-	wake_up(&fs_info->tree_mod_seq_wait);
-
 	/*
 	 * anything that's lower than the lowest existing (read: blocked)
 	 * sequence number can be removed from the tree.

commit 113c1cb530e10bcada93d88ffaa6b521aae2d251
Merge: cd1cfc49153b 31db9f7c23fb
Author: Chris Mason <chris.mason@fusionio.com>
Date:   Wed Jul 25 19:17:39 2012 -0400

    Merge branch 'send-v2' of git://github.com/ablock84/linux-btrfs into for-linus
    
    This is the kernel portion of btrfs send/receive
    
    Conflicts:
            fs/btrfs/Makefile
            fs/btrfs/backref.h
            fs/btrfs/ctree.c
            fs/btrfs/ioctl.c
            fs/btrfs/ioctl.h
    
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

commit 7069830a9e381e33d44ded45095f764844c71d24
Author: Alexander Block <ablock84@googlemail.com>
Date:   Tue Jun 5 21:07:48 2012 +0200

    Btrfs: add btrfs_compare_trees function
    
    This function is used to find the differences between
    two trees. The tree compare skips whole subtrees if it
    detects shared tree blocks and thus is pretty fast.
    
    Signed-off-by: Alexander Block <ablock84@googlemail.com>
    Reviewed-by: David Sterba <dave@jikos.cz>
    Reviewed-by: Arne Jansen <sensille@gmx.net>
    Reviewed-by: Jan Schmidt <list.btrfs@jan-o-sch.net>
    Reviewed-by: Alex Lyakas <alex.bolshoy.btrfs@gmail.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index c82a9e4a953e..4c10fd19d481 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -5005,6 +5005,431 @@ int btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,
 	return ret;
 }
 
+static void tree_move_down(struct btrfs_root *root,
+			   struct btrfs_path *path,
+			   int *level, int root_level)
+{
+	path->nodes[*level - 1] = read_node_slot(root, path->nodes[*level],
+					path->slots[*level]);
+	path->slots[*level - 1] = 0;
+	(*level)--;
+}
+
+static int tree_move_next_or_upnext(struct btrfs_root *root,
+				    struct btrfs_path *path,
+				    int *level, int root_level)
+{
+	int ret = 0;
+	int nritems;
+	nritems = btrfs_header_nritems(path->nodes[*level]);
+
+	path->slots[*level]++;
+
+	while (path->slots[*level] == nritems) {
+		if (*level == root_level)
+			return -1;
+
+		/* move upnext */
+		path->slots[*level] = 0;
+		free_extent_buffer(path->nodes[*level]);
+		path->nodes[*level] = NULL;
+		(*level)++;
+		path->slots[*level]++;
+
+		nritems = btrfs_header_nritems(path->nodes[*level]);
+		ret = 1;
+	}
+	return ret;
+}
+
+/*
+ * Returns 1 if it had to move up and next. 0 is returned if it moved only next
+ * or down.
+ */
+static int tree_advance(struct btrfs_root *root,
+			struct btrfs_path *path,
+			int *level, int root_level,
+			int allow_down,
+			struct btrfs_key *key)
+{
+	int ret;
+
+	if (*level == 0 || !allow_down) {
+		ret = tree_move_next_or_upnext(root, path, level, root_level);
+	} else {
+		tree_move_down(root, path, level, root_level);
+		ret = 0;
+	}
+	if (ret >= 0) {
+		if (*level == 0)
+			btrfs_item_key_to_cpu(path->nodes[*level], key,
+					path->slots[*level]);
+		else
+			btrfs_node_key_to_cpu(path->nodes[*level], key,
+					path->slots[*level]);
+	}
+	return ret;
+}
+
+static int tree_compare_item(struct btrfs_root *left_root,
+			     struct btrfs_path *left_path,
+			     struct btrfs_path *right_path,
+			     char *tmp_buf)
+{
+	int cmp;
+	int len1, len2;
+	unsigned long off1, off2;
+
+	len1 = btrfs_item_size_nr(left_path->nodes[0], left_path->slots[0]);
+	len2 = btrfs_item_size_nr(right_path->nodes[0], right_path->slots[0]);
+	if (len1 != len2)
+		return 1;
+
+	off1 = btrfs_item_ptr_offset(left_path->nodes[0], left_path->slots[0]);
+	off2 = btrfs_item_ptr_offset(right_path->nodes[0],
+				right_path->slots[0]);
+
+	read_extent_buffer(left_path->nodes[0], tmp_buf, off1, len1);
+
+	cmp = memcmp_extent_buffer(right_path->nodes[0], tmp_buf, off2, len1);
+	if (cmp)
+		return 1;
+	return 0;
+}
+
+#define ADVANCE 1
+#define ADVANCE_ONLY_NEXT -1
+
+/*
+ * This function compares two trees and calls the provided callback for
+ * every changed/new/deleted item it finds.
+ * If shared tree blocks are encountered, whole subtrees are skipped, making
+ * the compare pretty fast on snapshotted subvolumes.
+ *
+ * This currently works on commit roots only. As commit roots are read only,
+ * we don't do any locking. The commit roots are protected with transactions.
+ * Transactions are ended and rejoined when a commit is tried in between.
+ *
+ * This function checks for modifications done to the trees while comparing.
+ * If it detects a change, it aborts immediately.
+ */
+int btrfs_compare_trees(struct btrfs_root *left_root,
+			struct btrfs_root *right_root,
+			btrfs_changed_cb_t changed_cb, void *ctx)
+{
+	int ret;
+	int cmp;
+	struct btrfs_trans_handle *trans = NULL;
+	struct btrfs_path *left_path = NULL;
+	struct btrfs_path *right_path = NULL;
+	struct btrfs_key left_key;
+	struct btrfs_key right_key;
+	char *tmp_buf = NULL;
+	int left_root_level;
+	int right_root_level;
+	int left_level;
+	int right_level;
+	int left_end_reached;
+	int right_end_reached;
+	int advance_left;
+	int advance_right;
+	u64 left_blockptr;
+	u64 right_blockptr;
+	u64 left_start_ctransid;
+	u64 right_start_ctransid;
+	u64 ctransid;
+
+	left_path = btrfs_alloc_path();
+	if (!left_path) {
+		ret = -ENOMEM;
+		goto out;
+	}
+	right_path = btrfs_alloc_path();
+	if (!right_path) {
+		ret = -ENOMEM;
+		goto out;
+	}
+
+	tmp_buf = kmalloc(left_root->leafsize, GFP_NOFS);
+	if (!tmp_buf) {
+		ret = -ENOMEM;
+		goto out;
+	}
+
+	left_path->search_commit_root = 1;
+	left_path->skip_locking = 1;
+	right_path->search_commit_root = 1;
+	right_path->skip_locking = 1;
+
+	spin_lock(&left_root->root_times_lock);
+	left_start_ctransid = btrfs_root_ctransid(&left_root->root_item);
+	spin_unlock(&left_root->root_times_lock);
+
+	spin_lock(&right_root->root_times_lock);
+	right_start_ctransid = btrfs_root_ctransid(&right_root->root_item);
+	spin_unlock(&right_root->root_times_lock);
+
+	trans = btrfs_join_transaction(left_root);
+	if (IS_ERR(trans)) {
+		ret = PTR_ERR(trans);
+		trans = NULL;
+		goto out;
+	}
+
+	/*
+	 * Strategy: Go to the first items of both trees. Then do
+	 *
+	 * If both trees are at level 0
+	 *   Compare keys of current items
+	 *     If left < right treat left item as new, advance left tree
+	 *       and repeat
+	 *     If left > right treat right item as deleted, advance right tree
+	 *       and repeat
+	 *     If left == right do deep compare of items, treat as changed if
+	 *       needed, advance both trees and repeat
+	 * If both trees are at the same level but not at level 0
+	 *   Compare keys of current nodes/leafs
+	 *     If left < right advance left tree and repeat
+	 *     If left > right advance right tree and repeat
+	 *     If left == right compare blockptrs of the next nodes/leafs
+	 *       If they match advance both trees but stay at the same level
+	 *         and repeat
+	 *       If they don't match advance both trees while allowing to go
+	 *         deeper and repeat
+	 * If tree levels are different
+	 *   Advance the tree that needs it and repeat
+	 *
+	 * Advancing a tree means:
+	 *   If we are at level 0, try to go to the next slot. If that's not
+	 *   possible, go one level up and repeat. Stop when we found a level
+	 *   where we could go to the next slot. We may at this point be on a
+	 *   node or a leaf.
+	 *
+	 *   If we are not at level 0 and not on shared tree blocks, go one
+	 *   level deeper.
+	 *
+	 *   If we are not at level 0 and on shared tree blocks, go one slot to
+	 *   the right if possible or go up and right.
+	 */
+
+	left_level = btrfs_header_level(left_root->commit_root);
+	left_root_level = left_level;
+	left_path->nodes[left_level] = left_root->commit_root;
+	extent_buffer_get(left_path->nodes[left_level]);
+
+	right_level = btrfs_header_level(right_root->commit_root);
+	right_root_level = right_level;
+	right_path->nodes[right_level] = right_root->commit_root;
+	extent_buffer_get(right_path->nodes[right_level]);
+
+	if (left_level == 0)
+		btrfs_item_key_to_cpu(left_path->nodes[left_level],
+				&left_key, left_path->slots[left_level]);
+	else
+		btrfs_node_key_to_cpu(left_path->nodes[left_level],
+				&left_key, left_path->slots[left_level]);
+	if (right_level == 0)
+		btrfs_item_key_to_cpu(right_path->nodes[right_level],
+				&right_key, right_path->slots[right_level]);
+	else
+		btrfs_node_key_to_cpu(right_path->nodes[right_level],
+				&right_key, right_path->slots[right_level]);
+
+	left_end_reached = right_end_reached = 0;
+	advance_left = advance_right = 0;
+
+	while (1) {
+		/*
+		 * We need to make sure the transaction does not get committed
+		 * while we do anything on commit roots. This means, we need to
+		 * join and leave transactions for every item that we process.
+		 */
+		if (trans && btrfs_should_end_transaction(trans, left_root)) {
+			btrfs_release_path(left_path);
+			btrfs_release_path(right_path);
+
+			ret = btrfs_end_transaction(trans, left_root);
+			trans = NULL;
+			if (ret < 0)
+				goto out;
+		}
+		/* now rejoin the transaction */
+		if (!trans) {
+			trans = btrfs_join_transaction(left_root);
+			if (IS_ERR(trans)) {
+				ret = PTR_ERR(trans);
+				trans = NULL;
+				goto out;
+			}
+
+			spin_lock(&left_root->root_times_lock);
+			ctransid = btrfs_root_ctransid(&left_root->root_item);
+			spin_unlock(&left_root->root_times_lock);
+			if (ctransid != left_start_ctransid)
+				left_start_ctransid = 0;
+
+			spin_lock(&right_root->root_times_lock);
+			ctransid = btrfs_root_ctransid(&right_root->root_item);
+			spin_unlock(&right_root->root_times_lock);
+			if (ctransid != right_start_ctransid)
+				right_start_ctransid = 0;
+
+			if (!left_start_ctransid || !right_start_ctransid) {
+				WARN(1, KERN_WARNING
+					"btrfs: btrfs_compare_tree detected "
+					"a change in one of the trees while "
+					"iterating. This is probably a "
+					"bug.\n");
+				ret = -EIO;
+				goto out;
+			}
+
+			/*
+			 * the commit root may have changed, so start again
+			 * where we stopped
+			 */
+			left_path->lowest_level = left_level;
+			right_path->lowest_level = right_level;
+			ret = btrfs_search_slot(NULL, left_root,
+					&left_key, left_path, 0, 0);
+			if (ret < 0)
+				goto out;
+			ret = btrfs_search_slot(NULL, right_root,
+					&right_key, right_path, 0, 0);
+			if (ret < 0)
+				goto out;
+		}
+
+		if (advance_left && !left_end_reached) {
+			ret = tree_advance(left_root, left_path, &left_level,
+					left_root_level,
+					advance_left != ADVANCE_ONLY_NEXT,
+					&left_key);
+			if (ret < 0)
+				left_end_reached = ADVANCE;
+			advance_left = 0;
+		}
+		if (advance_right && !right_end_reached) {
+			ret = tree_advance(right_root, right_path, &right_level,
+					right_root_level,
+					advance_right != ADVANCE_ONLY_NEXT,
+					&right_key);
+			if (ret < 0)
+				right_end_reached = ADVANCE;
+			advance_right = 0;
+		}
+
+		if (left_end_reached && right_end_reached) {
+			ret = 0;
+			goto out;
+		} else if (left_end_reached) {
+			if (right_level == 0) {
+				ret = changed_cb(left_root, right_root,
+						left_path, right_path,
+						&right_key,
+						BTRFS_COMPARE_TREE_DELETED,
+						ctx);
+				if (ret < 0)
+					goto out;
+			}
+			advance_right = ADVANCE;
+			continue;
+		} else if (right_end_reached) {
+			if (left_level == 0) {
+				ret = changed_cb(left_root, right_root,
+						left_path, right_path,
+						&left_key,
+						BTRFS_COMPARE_TREE_NEW,
+						ctx);
+				if (ret < 0)
+					goto out;
+			}
+			advance_left = ADVANCE;
+			continue;
+		}
+
+		if (left_level == 0 && right_level == 0) {
+			cmp = btrfs_comp_cpu_keys(&left_key, &right_key);
+			if (cmp < 0) {
+				ret = changed_cb(left_root, right_root,
+						left_path, right_path,
+						&left_key,
+						BTRFS_COMPARE_TREE_NEW,
+						ctx);
+				if (ret < 0)
+					goto out;
+				advance_left = ADVANCE;
+			} else if (cmp > 0) {
+				ret = changed_cb(left_root, right_root,
+						left_path, right_path,
+						&right_key,
+						BTRFS_COMPARE_TREE_DELETED,
+						ctx);
+				if (ret < 0)
+					goto out;
+				advance_right = ADVANCE;
+			} else {
+				ret = tree_compare_item(left_root, left_path,
+						right_path, tmp_buf);
+				if (ret) {
+					ret = changed_cb(left_root, right_root,
+						left_path, right_path,
+						&left_key,
+						BTRFS_COMPARE_TREE_CHANGED,
+						ctx);
+					if (ret < 0)
+						goto out;
+				}
+				advance_left = ADVANCE;
+				advance_right = ADVANCE;
+			}
+		} else if (left_level == right_level) {
+			cmp = btrfs_comp_cpu_keys(&left_key, &right_key);
+			if (cmp < 0) {
+				advance_left = ADVANCE;
+			} else if (cmp > 0) {
+				advance_right = ADVANCE;
+			} else {
+				left_blockptr = btrfs_node_blockptr(
+						left_path->nodes[left_level],
+						left_path->slots[left_level]);
+				right_blockptr = btrfs_node_blockptr(
+						right_path->nodes[right_level],
+						right_path->slots[right_level]);
+				if (left_blockptr == right_blockptr) {
+					/*
+					 * As we're on a shared block, don't
+					 * allow to go deeper.
+					 */
+					advance_left = ADVANCE_ONLY_NEXT;
+					advance_right = ADVANCE_ONLY_NEXT;
+				} else {
+					advance_left = ADVANCE;
+					advance_right = ADVANCE;
+				}
+			}
+		} else if (left_level < right_level) {
+			advance_right = ADVANCE;
+		} else {
+			advance_left = ADVANCE;
+		}
+	}
+
+out:
+	btrfs_free_path(left_path);
+	btrfs_free_path(right_path);
+	kfree(tmp_buf);
+
+	if (trans) {
+		if (!ret)
+			ret = btrfs_end_transaction(trans, left_root);
+		else
+			btrfs_end_transaction(trans, left_root);
+	}
+
+	return ret;
+}
+
 /*
  * this is similar to btrfs_next_leaf, but does not try to preserve
  * and fixup the path.  It looks for and returns the next key in the

commit e679376911d016b670c8cfc1645c178f77e8d1d3
Author: Arne Jansen <sensille@gmx.net>
Date:   Tue Sep 13 11:18:10 2011 +0200

    Btrfs: add helper for tree enumeration
    
    Often no exact match is wanted but just the next lower or
    higher item. There's a lot of duplicated code throughout
    btrfs to deal with the corner cases. This patch adds a
    helper function that can facilitate searching.
    
    Signed-off-by: Arne Jansen <sensille@gmx.net>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 8206b3900587..c82a9e4a953e 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2721,6 +2721,80 @@ int btrfs_search_old_slot(struct btrfs_root *root, struct btrfs_key *key,
 	return ret;
 }
 
+/*
+ * helper to use instead of search slot if no exact match is needed but
+ * instead the next or previous item should be returned.
+ * When find_higher is true, the next higher item is returned, the next lower
+ * otherwise.
+ * When return_any and find_higher are both true, and no higher item is found,
+ * return the next lower instead.
+ * When return_any is true and find_higher is false, and no lower item is found,
+ * return the next higher instead.
+ * It returns 0 if any item is found, 1 if none is found (tree empty), and
+ * < 0 on error
+ */
+int btrfs_search_slot_for_read(struct btrfs_root *root,
+			       struct btrfs_key *key, struct btrfs_path *p,
+			       int find_higher, int return_any)
+{
+	int ret;
+	struct extent_buffer *leaf;
+
+again:
+	ret = btrfs_search_slot(NULL, root, key, p, 0, 0);
+	if (ret <= 0)
+		return ret;
+	/*
+	 * a return value of 1 means the path is at the position where the
+	 * item should be inserted. Normally this is the next bigger item,
+	 * but in case the previous item is the last in a leaf, path points
+	 * to the first free slot in the previous leaf, i.e. at an invalid
+	 * item.
+	 */
+	leaf = p->nodes[0];
+
+	if (find_higher) {
+		if (p->slots[0] >= btrfs_header_nritems(leaf)) {
+			ret = btrfs_next_leaf(root, p);
+			if (ret <= 0)
+				return ret;
+			if (!return_any)
+				return 1;
+			/*
+			 * no higher item found, return the next
+			 * lower instead
+			 */
+			return_any = 0;
+			find_higher = 0;
+			btrfs_release_path(p);
+			goto again;
+		}
+	} else {
+		if (p->slots[0] == 0) {
+			ret = btrfs_prev_leaf(root, p);
+			if (ret < 0)
+				return ret;
+			if (!ret) {
+				p->slots[0] = btrfs_header_nritems(leaf) - 1;
+				return 0;
+			}
+			if (!return_any)
+				return 1;
+			/*
+			 * no lower item found, return the next
+			 * higher instead
+			 */
+			return_any = 0;
+			find_higher = 1;
+			btrfs_release_path(p);
+			goto again;
+		} else {
+			--p->slots[0];
+		}
+	}
+	return 0;
+}
+
 /*
  * adjust the pointers going up the tree, starting at level
  * making sure the right key of each node is points to 'key'.

commit 2f38b3e1900634e64a186873b3388b1bf85dabc0
Author: Arne Jansen <sensille@gmx.net>
Date:   Tue Sep 13 11:18:10 2011 +0200

    Btrfs: add helper for tree enumeration
    
    Often no exact match is wanted but just the next lower or
    higher item. There's a lot of duplicated code throughout
    btrfs to deal with the corner cases. This patch adds a
    helper function that can facilitate searching.
    
    Signed-off-by: Arne Jansen <sensille@gmx.net>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index bef68ab32204..fb21431fe4e0 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2788,6 +2788,78 @@ int btrfs_search_old_slot(struct btrfs_root *root, struct btrfs_key *key,
 	return ret;
 }
 
+/*
+ * helper to use instead of search slot if no exact match is needed but
+ * instead the next or previous item should be returned.
+ * When find_higher is true, the next higher item is returned, the next lower
+ * otherwise.
+ * When return_any and find_higher are both true, and no higher item is found,
+ * return the next lower instead.
+ * When return_any is true and find_higher is false, and no lower item is found,
+ * return the next higher instead.
+ * It returns 0 if any item is found, 1 if none is found (tree empty), and
+ * < 0 on error
+ */
+int btrfs_search_slot_for_read(struct btrfs_root *root,
+			       struct btrfs_key *key, struct btrfs_path *p,
+			       int find_higher, int return_any)
+{
+	int ret;
+	struct extent_buffer *leaf;
+
+again:
+	ret = btrfs_search_slot(NULL, root, key, p, 0, 0);
+	if (ret <= 0)
+		return ret;
+	/*
+	 * a return value of 1 means the path is at the position where the
+	 * item should be inserted. Normally this is the next bigger item,
+	 * but in case the previous item is the last in a leaf, path points
+	 * to the first free slot in the previous leaf, i.e. at an invalid
+	 * item.
+	 */
+	leaf = p->nodes[0];
+
+	if (find_higher) {
+		if (p->slots[0] >= btrfs_header_nritems(leaf)) {
+			ret = btrfs_next_leaf(root, p);
+			if (ret <= 0)
+				return ret;
+			if (!return_any)
+				return 1;
+			/*
+			 * no higher item found, return the next
+			 * lower instead
+			 */
+			return_any = 0;
+			find_higher = 0;
+			btrfs_release_path(p);
+			goto again;
+		}
+	} else {
+		if (p->slots[0] >= btrfs_header_nritems(leaf)) {
+			/* we're sitting on an invalid slot */
+			if (p->slots[0] == 0) {
+				ret = btrfs_prev_leaf(root, p);
+				if (ret <= 0)
+					return ret;
+				if (!return_any)
+					return 1;
+				/*
+				 * no lower item found, return the next
+				 * higher instead
+				 */
+				return_any = 0;
+				find_higher = 1;
+				btrfs_release_path(p);
+				goto again;
+			}
+			--p->slots[0];
+		}
+	}
+	return 0;
+}
+
 /*
  * adjust the pointers going up the tree, starting at level
  * making sure the right key of each node is points to 'key'.

commit 097b8a7c9e48e2cb50fd0eb9315791921beaf484
Author: Jan Schmidt <list.btrfs@jan-o-sch.net>
Date:   Thu Jun 21 11:08:04 2012 +0200

    Btrfs: join tree mod log code with the code holding back delayed refs
    
    We've got two mechanisms both required for reliable backref resolving (tree
    mod log and holding back delayed refs). You cannot make use of one without
    the other. So instead of requiring the user of this mechanism to setup both
    correctly, we join them into a single interface.
    
    Additionally, we stop inserting non-blockers into fs_info->tree_mod_seq_list
    as we did before, which was of no value.
    
    Signed-off-by: Jan Schmidt <list.btrfs@jan-o-sch.net>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 67fe46fdee6f..bef68ab32204 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -321,7 +321,7 @@ struct tree_mod_root {
 struct tree_mod_elem {
 	struct rb_node node;
 	u64 index;		/* shifted logical */
-	struct seq_list elem;
+	u64 seq;
 	enum mod_log_op op;
 
 	/* this is used for MOD_LOG_KEY_* and MOD_LOG_MOVE_KEYS operations */
@@ -341,20 +341,50 @@ struct tree_mod_elem {
 	struct tree_mod_root old_root;
 };
 
-static inline void
-__get_tree_mod_seq(struct btrfs_fs_info *fs_info, struct seq_list *elem)
+static inline void tree_mod_log_read_lock(struct btrfs_fs_info *fs_info)
 {
-	elem->seq = atomic_inc_return(&fs_info->tree_mod_seq);
-	list_add_tail(&elem->list, &fs_info->tree_mod_seq_list);
+	read_lock(&fs_info->tree_mod_log_lock);
 }
 
-void btrfs_get_tree_mod_seq(struct btrfs_fs_info *fs_info,
-			    struct seq_list *elem)
+static inline void tree_mod_log_read_unlock(struct btrfs_fs_info *fs_info)
+{
+	read_unlock(&fs_info->tree_mod_log_lock);
+}
+
+static inline void tree_mod_log_write_lock(struct btrfs_fs_info *fs_info)
+{
+	write_lock(&fs_info->tree_mod_log_lock);
+}
+
+static inline void tree_mod_log_write_unlock(struct btrfs_fs_info *fs_info)
 {
-	elem->flags = 1;
+	write_unlock(&fs_info->tree_mod_log_lock);
+}
+
+/*
+ * This adds a new blocker to the tree mod log's blocker list if the @elem
+ * passed does not already have a sequence number set. So when a caller expects
+ * to record tree modifications, it should ensure to set elem->seq to zero
+ * before calling btrfs_get_tree_mod_seq.
+ * Returns a fresh, unused tree log modification sequence number, even if no new
+ * blocker was added.
+ */
+u64 btrfs_get_tree_mod_seq(struct btrfs_fs_info *fs_info,
+			   struct seq_list *elem)
+{
+	u64 seq;
+
+	tree_mod_log_write_lock(fs_info);
 	spin_lock(&fs_info->tree_mod_seq_lock);
-	__get_tree_mod_seq(fs_info, elem);
+	if (!elem->seq) {
+		elem->seq = btrfs_inc_tree_mod_seq(fs_info);
+		list_add_tail(&elem->list, &fs_info->tree_mod_seq_list);
+	}
+	seq = btrfs_inc_tree_mod_seq(fs_info);
 	spin_unlock(&fs_info->tree_mod_seq_lock);
+	tree_mod_log_write_unlock(fs_info);
+
+	return seq;
 }
 
 void btrfs_put_tree_mod_seq(struct btrfs_fs_info *fs_info,
@@ -371,41 +401,46 @@ void btrfs_put_tree_mod_seq(struct btrfs_fs_info *fs_info,
 	if (!seq_putting)
 		return;
 
-	BUG_ON(!(elem->flags & 1));
 	spin_lock(&fs_info->tree_mod_seq_lock);
 	list_del(&elem->list);
+	elem->seq = 0;
 
 	list_for_each_entry(cur_elem, &fs_info->tree_mod_seq_list, list) {
-		if ((cur_elem->flags & 1) && cur_elem->seq < min_seq) {
+		if (cur_elem->seq < min_seq) {
 			if (seq_putting > cur_elem->seq) {
 				/*
 				 * blocker with lower sequence number exists, we
 				 * cannot remove anything from the log
 				 */
-				goto out;
+				spin_unlock(&fs_info->tree_mod_seq_lock);
+				return;
 			}
 			min_seq = cur_elem->seq;
 		}
 	}
+	spin_unlock(&fs_info->tree_mod_seq_lock);
+
+	/*
+	 * we removed the lowest blocker from the blocker list, so there may be
+	 * more processible delayed refs.
+	 */
+	wake_up(&fs_info->tree_mod_seq_wait);
 
 	/*
 	 * anything that's lower than the lowest existing (read: blocked)
 	 * sequence number can be removed from the tree.
 	 */
-	write_lock(&fs_info->tree_mod_log_lock);
+	tree_mod_log_write_lock(fs_info);
 	tm_root = &fs_info->tree_mod_log;
 	for (node = rb_first(tm_root); node; node = next) {
 		next = rb_next(node);
 		tm = container_of(node, struct tree_mod_elem, node);
-		if (tm->elem.seq > min_seq)
+		if (tm->seq > min_seq)
 			continue;
 		rb_erase(node, tm_root);
-		list_del(&tm->elem.list);
 		kfree(tm);
 	}
-	write_unlock(&fs_info->tree_mod_log_lock);
-out:
-	spin_unlock(&fs_info->tree_mod_seq_lock);
+	tree_mod_log_write_unlock(fs_info);
 }
 
 /*
@@ -423,11 +458,9 @@ __tree_mod_log_insert(struct btrfs_fs_info *fs_info, struct tree_mod_elem *tm)
 	struct rb_node **new;
 	struct rb_node *parent = NULL;
 	struct tree_mod_elem *cur;
-	int ret = 0;
 
-	BUG_ON(!tm || !tm->elem.seq);
+	BUG_ON(!tm || !tm->seq);
 
-	write_lock(&fs_info->tree_mod_log_lock);
 	tm_root = &fs_info->tree_mod_log;
 	new = &tm_root->rb_node;
 	while (*new) {
@@ -437,88 +470,81 @@ __tree_mod_log_insert(struct btrfs_fs_info *fs_info, struct tree_mod_elem *tm)
 			new = &((*new)->rb_left);
 		else if (cur->index > tm->index)
 			new = &((*new)->rb_right);
-		else if (cur->elem.seq < tm->elem.seq)
+		else if (cur->seq < tm->seq)
 			new = &((*new)->rb_left);
-		else if (cur->elem.seq > tm->elem.seq)
+		else if (cur->seq > tm->seq)
 			new = &((*new)->rb_right);
 		else {
 			kfree(tm);
-			ret = -EEXIST;
-			goto unlock;
+			return -EEXIST;
 		}
 	}
 
 	rb_link_node(&tm->node, parent, new);
 	rb_insert_color(&tm->node, tm_root);
-unlock:
-	write_unlock(&fs_info->tree_mod_log_lock);
-	return ret;
+	return 0;
 }
 
+/*
+ * Determines if logging can be omitted. Returns 1 if it can. Otherwise, it
+ * returns zero with the tree_mod_log_lock acquired. The caller must hold
+ * this until all tree mod log insertions are recorded in the rb tree and then
+ * call tree_mod_log_write_unlock() to release.
+ */
 static inline int tree_mod_dont_log(struct btrfs_fs_info *fs_info,
 				    struct extent_buffer *eb) {
 	smp_mb();
 	if (list_empty(&(fs_info)->tree_mod_seq_list))
 		return 1;
-	if (!eb)
-		return 0;
-	if (btrfs_header_level(eb) == 0)
+	if (eb && btrfs_header_level(eb) == 0)
 		return 1;
+
+	tree_mod_log_write_lock(fs_info);
+	if (list_empty(&fs_info->tree_mod_seq_list)) {
+		/*
+		 * someone emptied the list while we were waiting for the lock.
+		 * we must not add to the list when no blocker exists.
+		 */
+		tree_mod_log_write_unlock(fs_info);
+		return 1;
+	}
+
 	return 0;
 }
 
 /*
- * This allocates memory and gets a tree modification sequence number when
- * needed.
+ * This allocates memory and gets a tree modification sequence number.
  *
- * Returns 0 when no sequence number is needed, < 0 on error.
- * Returns 1 when a sequence number was added. In this case,
- * fs_info->tree_mod_seq_lock was acquired and must be released by the caller
- * after inserting into the rb tree.
+ * Returns <0 on error.
+ * Returns >0 (the added sequence number) on success.
  */
 static inline int tree_mod_alloc(struct btrfs_fs_info *fs_info, gfp_t flags,
 				 struct tree_mod_elem **tm_ret)
 {
 	struct tree_mod_elem *tm;
-	int seq;
 
-	if (tree_mod_dont_log(fs_info, NULL))
-		return 0;
-
-	tm = *tm_ret = kzalloc(sizeof(*tm), flags);
+	/*
+	 * once we switch from spin locks to something different, we should
+	 * honor the flags parameter here.
+	 */
+	tm = *tm_ret = kzalloc(sizeof(*tm), GFP_ATOMIC);
 	if (!tm)
 		return -ENOMEM;
 
-	tm->elem.flags = 0;
-	spin_lock(&fs_info->tree_mod_seq_lock);
-	if (list_empty(&fs_info->tree_mod_seq_list)) {
-		/*
-		 * someone emptied the list while we were waiting for the lock.
-		 * we must not add to the list, because no blocker exists. items
-		 * are removed from the list only when the existing blocker is
-		 * removed from the list.
-		 */
-		kfree(tm);
-		seq = 0;
-		spin_unlock(&fs_info->tree_mod_seq_lock);
-	} else {
-		__get_tree_mod_seq(fs_info, &tm->elem);
-		seq = tm->elem.seq;
-	}
-
-	return seq;
+	tm->seq = btrfs_inc_tree_mod_seq(fs_info);
+	return tm->seq;
 }
 
-static noinline int
-tree_mod_log_insert_key_mask(struct btrfs_fs_info *fs_info,
-			     struct extent_buffer *eb, int slot,
-			     enum mod_log_op op, gfp_t flags)
+static inline int
+__tree_mod_log_insert_key(struct btrfs_fs_info *fs_info,
+			  struct extent_buffer *eb, int slot,
+			  enum mod_log_op op, gfp_t flags)
 {
-	struct tree_mod_elem *tm;
 	int ret;
+	struct tree_mod_elem *tm;
 
 	ret = tree_mod_alloc(fs_info, flags, &tm);
-	if (ret <= 0)
+	if (ret < 0)
 		return ret;
 
 	tm->index = eb->start >> PAGE_CACHE_SHIFT;
@@ -530,8 +556,22 @@ tree_mod_log_insert_key_mask(struct btrfs_fs_info *fs_info,
 	tm->slot = slot;
 	tm->generation = btrfs_node_ptr_generation(eb, slot);
 
-	ret = __tree_mod_log_insert(fs_info, tm);
-	spin_unlock(&fs_info->tree_mod_seq_lock);
+	return __tree_mod_log_insert(fs_info, tm);
+}
+
+static noinline int
+tree_mod_log_insert_key_mask(struct btrfs_fs_info *fs_info,
+			     struct extent_buffer *eb, int slot,
+			     enum mod_log_op op, gfp_t flags)
+{
+	int ret;
+
+	if (tree_mod_dont_log(fs_info, eb))
+		return 0;
+
+	ret = __tree_mod_log_insert_key(fs_info, eb, slot, op, flags);
+
+	tree_mod_log_write_unlock(fs_info);
 	return ret;
 }
 
@@ -542,6 +582,14 @@ tree_mod_log_insert_key(struct btrfs_fs_info *fs_info, struct extent_buffer *eb,
 	return tree_mod_log_insert_key_mask(fs_info, eb, slot, op, GFP_NOFS);
 }
 
+static noinline int
+tree_mod_log_insert_key_locked(struct btrfs_fs_info *fs_info,
+			     struct extent_buffer *eb, int slot,
+			     enum mod_log_op op)
+{
+	return __tree_mod_log_insert_key(fs_info, eb, slot, op, GFP_NOFS);
+}
+
 static noinline int
 tree_mod_log_insert_move(struct btrfs_fs_info *fs_info,
 			 struct extent_buffer *eb, int dst_slot, int src_slot,
@@ -555,14 +603,14 @@ tree_mod_log_insert_move(struct btrfs_fs_info *fs_info,
 		return 0;
 
 	for (i = 0; i + dst_slot < src_slot && i < nr_items; i++) {
-		ret = tree_mod_log_insert_key(fs_info, eb, i + dst_slot,
+		ret = tree_mod_log_insert_key_locked(fs_info, eb, i + dst_slot,
 					      MOD_LOG_KEY_REMOVE_WHILE_MOVING);
 		BUG_ON(ret < 0);
 	}
 
 	ret = tree_mod_alloc(fs_info, flags, &tm);
-	if (ret <= 0)
-		return ret;
+	if (ret < 0)
+		goto out;
 
 	tm->index = eb->start >> PAGE_CACHE_SHIFT;
 	tm->slot = src_slot;
@@ -571,10 +619,26 @@ tree_mod_log_insert_move(struct btrfs_fs_info *fs_info,
 	tm->op = MOD_LOG_MOVE_KEYS;
 
 	ret = __tree_mod_log_insert(fs_info, tm);
-	spin_unlock(&fs_info->tree_mod_seq_lock);
+out:
+	tree_mod_log_write_unlock(fs_info);
 	return ret;
 }
 
+static inline void
+__tree_mod_log_free_eb(struct btrfs_fs_info *fs_info, struct extent_buffer *eb)
+{
+	int i;
+	u32 nritems;
+	int ret;
+
+	nritems = btrfs_header_nritems(eb);
+	for (i = nritems - 1; i >= 0; i--) {
+		ret = tree_mod_log_insert_key_locked(fs_info, eb, i,
+					      MOD_LOG_KEY_REMOVE_WHILE_FREEING);
+		BUG_ON(ret < 0);
+	}
+}
+
 static noinline int
 tree_mod_log_insert_root(struct btrfs_fs_info *fs_info,
 			 struct extent_buffer *old_root,
@@ -583,9 +647,14 @@ tree_mod_log_insert_root(struct btrfs_fs_info *fs_info,
 	struct tree_mod_elem *tm;
 	int ret;
 
+	if (tree_mod_dont_log(fs_info, NULL))
+		return 0;
+
+	__tree_mod_log_free_eb(fs_info, old_root);
+
 	ret = tree_mod_alloc(fs_info, flags, &tm);
-	if (ret <= 0)
-		return ret;
+	if (ret < 0)
+		goto out;
 
 	tm->index = new_root->start >> PAGE_CACHE_SHIFT;
 	tm->old_root.logical = old_root->start;
@@ -594,7 +663,8 @@ tree_mod_log_insert_root(struct btrfs_fs_info *fs_info,
 	tm->op = MOD_LOG_ROOT_REPLACE;
 
 	ret = __tree_mod_log_insert(fs_info, tm);
-	spin_unlock(&fs_info->tree_mod_seq_lock);
+out:
+	tree_mod_log_write_unlock(fs_info);
 	return ret;
 }
 
@@ -608,7 +678,7 @@ __tree_mod_log_search(struct btrfs_fs_info *fs_info, u64 start, u64 min_seq,
 	struct tree_mod_elem *found = NULL;
 	u64 index = start >> PAGE_CACHE_SHIFT;
 
-	read_lock(&fs_info->tree_mod_log_lock);
+	tree_mod_log_read_lock(fs_info);
 	tm_root = &fs_info->tree_mod_log;
 	node = tm_root->rb_node;
 	while (node) {
@@ -617,18 +687,18 @@ __tree_mod_log_search(struct btrfs_fs_info *fs_info, u64 start, u64 min_seq,
 			node = node->rb_left;
 		} else if (cur->index > index) {
 			node = node->rb_right;
-		} else if (cur->elem.seq < min_seq) {
+		} else if (cur->seq < min_seq) {
 			node = node->rb_left;
 		} else if (!smallest) {
 			/* we want the node with the highest seq */
 			if (found)
-				BUG_ON(found->elem.seq > cur->elem.seq);
+				BUG_ON(found->seq > cur->seq);
 			found = cur;
 			node = node->rb_left;
-		} else if (cur->elem.seq > min_seq) {
+		} else if (cur->seq > min_seq) {
 			/* we want the node with the smallest seq */
 			if (found)
-				BUG_ON(found->elem.seq < cur->elem.seq);
+				BUG_ON(found->seq < cur->seq);
 			found = cur;
 			node = node->rb_right;
 		} else {
@@ -636,7 +706,7 @@ __tree_mod_log_search(struct btrfs_fs_info *fs_info, u64 start, u64 min_seq,
 			break;
 		}
 	}
-	read_unlock(&fs_info->tree_mod_log_lock);
+	tree_mod_log_read_unlock(fs_info);
 
 	return found;
 }
@@ -664,7 +734,7 @@ tree_mod_log_search(struct btrfs_fs_info *fs_info, u64 start, u64 min_seq)
 	return __tree_mod_log_search(fs_info, start, min_seq, 0);
 }
 
-static inline void
+static noinline void
 tree_mod_log_eb_copy(struct btrfs_fs_info *fs_info, struct extent_buffer *dst,
 		     struct extent_buffer *src, unsigned long dst_offset,
 		     unsigned long src_offset, int nr_items)
@@ -675,18 +745,23 @@ tree_mod_log_eb_copy(struct btrfs_fs_info *fs_info, struct extent_buffer *dst,
 	if (tree_mod_dont_log(fs_info, NULL))
 		return;
 
-	if (btrfs_header_level(dst) == 0 && btrfs_header_level(src) == 0)
+	if (btrfs_header_level(dst) == 0 && btrfs_header_level(src) == 0) {
+		tree_mod_log_write_unlock(fs_info);
 		return;
+	}
 
-	/* speed this up by single seq for all operations? */
 	for (i = 0; i < nr_items; i++) {
-		ret = tree_mod_log_insert_key(fs_info, src, i + src_offset,
-					      MOD_LOG_KEY_REMOVE);
+		ret = tree_mod_log_insert_key_locked(fs_info, src,
+						     i + src_offset,
+						     MOD_LOG_KEY_REMOVE);
 		BUG_ON(ret < 0);
-		ret = tree_mod_log_insert_key(fs_info, dst, i + dst_offset,
-					      MOD_LOG_KEY_ADD);
+		ret = tree_mod_log_insert_key_locked(fs_info, dst,
+						     i + dst_offset,
+						     MOD_LOG_KEY_ADD);
 		BUG_ON(ret < 0);
 	}
+
+	tree_mod_log_write_unlock(fs_info);
 }
 
 static inline void
@@ -699,7 +774,7 @@ tree_mod_log_eb_move(struct btrfs_fs_info *fs_info, struct extent_buffer *dst,
 	BUG_ON(ret < 0);
 }
 
-static inline void
+static noinline void
 tree_mod_log_set_node_key(struct btrfs_fs_info *fs_info,
 			  struct extent_buffer *eb,
 			  struct btrfs_disk_key *disk_key, int slot, int atomic)
@@ -712,30 +787,22 @@ tree_mod_log_set_node_key(struct btrfs_fs_info *fs_info,
 	BUG_ON(ret < 0);
 }
 
-static void tree_mod_log_free_eb(struct btrfs_fs_info *fs_info,
-				 struct extent_buffer *eb)
+static noinline void
+tree_mod_log_free_eb(struct btrfs_fs_info *fs_info, struct extent_buffer *eb)
 {
-	int i;
-	int ret;
-	u32 nritems;
-
 	if (tree_mod_dont_log(fs_info, eb))
 		return;
 
-	nritems = btrfs_header_nritems(eb);
-	for (i = nritems - 1; i >= 0; i--) {
-		ret = tree_mod_log_insert_key(fs_info, eb, i,
-					      MOD_LOG_KEY_REMOVE_WHILE_FREEING);
-		BUG_ON(ret < 0);
-	}
+	__tree_mod_log_free_eb(fs_info, eb);
+
+	tree_mod_log_write_unlock(fs_info);
 }
 
-static inline void
+static noinline void
 tree_mod_log_set_root_pointer(struct btrfs_root *root,
 			      struct extent_buffer *new_root_node)
 {
 	int ret;
-	tree_mod_log_free_eb(root->fs_info, root->node);
 	ret = tree_mod_log_insert_root(root->fs_info, root->node,
 				       new_root_node, GFP_NOFS);
 	BUG_ON(ret < 0);
@@ -1069,7 +1136,7 @@ __tree_mod_log_rewind(struct extent_buffer *eb, u64 time_seq,
 	unsigned long p_size = sizeof(struct btrfs_key_ptr);
 
 	n = btrfs_header_nritems(eb);
-	while (tm && tm->elem.seq >= time_seq) {
+	while (tm && tm->seq >= time_seq) {
 		/*
 		 * all the operations are recorded with the operator used for
 		 * the modification. as we're going backwards, we do the

commit cf5388307a2b4faab4b11d732b61c85741be6169
Author: Jan Schmidt <list.btrfs@jan-o-sch.net>
Date:   Wed Jul 4 15:42:48 2012 +0200

    Btrfs: fix buffer leak in btrfs_next_old_leaf
    
    When calling btrfs_next_old_leaf, we were leaking an extent buffer in the
    rare case of using the deadlock avoidance code needed for the tree mod log.
    
    Signed-off-by: Jan Schmidt <list.btrfs@jan-o-sch.net>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 8206b3900587..67fe46fdee6f 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -5127,6 +5127,7 @@ int btrfs_next_old_leaf(struct btrfs_root *root, struct btrfs_path *path,
 				 * locked. To solve this situation, we give up
 				 * on our lock and cycle.
 				 */
+				free_extent_buffer(next);
 				btrfs_release_path(path);
 				cond_resched();
 				goto again;

commit d42244a0d36ad0939c5f173ebf15841a0678899c
Author: Jan Schmidt <list.btrfs@jan-o-sch.net>
Date:   Fri Jun 22 14:51:15 2012 +0200

    Btrfs: resolve tree mod log locking issue in btrfs_next_leaf
    
    With the tree mod log, we may end up with two roots (the current root and a
    rewinded version of it) both pointing to two leaves, l1 and l2, of which l2
    had already been cow-ed in the current transaction. If we don't rewind any
    tree blocks, we cannot have two roots both pointing to an already cowed tree
    block.
    
    Now there is btrfs_next_leaf, which has a leaf locked and wants a lock on
    the next (right) leaf. And there is push_leaf_left, which has a (cowed!)
    leaf locked and wants a lock on the previous (left) leaf.
    
    In order to solve this dead lock situation, we use try_lock in
    btrfs_next_leaf (only in case it's called with a tree mod log time_seq
    paramter) and if we fail to get a lock on the next leaf, we give up our lock
    on the current leaf and retry from the very beginning.
    
    Signed-off-by: Jan Schmidt <list.btrfs@jan-o-sch.net>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index b98f8604f4f6..8206b3900587 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -5119,6 +5119,18 @@ int btrfs_next_old_leaf(struct btrfs_root *root, struct btrfs_path *path,
 
 		if (!path->skip_locking) {
 			ret = btrfs_try_tree_read_lock(next);
+			if (!ret && time_seq) {
+				/*
+				 * If we don't get the lock, we may be racing
+				 * with push_leaf_left, holding that lock while
+				 * itself waiting for the leaf we've currently
+				 * locked. To solve this situation, we give up
+				 * on our lock and cycle.
+				 */
+				btrfs_release_path(path);
+				cond_resched();
+				goto again;
+			}
 			if (!ret) {
 				btrfs_set_path_blocking(path);
 				btrfs_tree_read_lock(next);

commit 19956c7e94a7a58d6df8c4db5ae62f9109a7c663
Author: Jan Schmidt <list.btrfs@jan-o-sch.net>
Date:   Fri Jun 22 14:52:13 2012 +0200

    Btrfs: fix tree mod log rewind of ADD operations
    
    When a MOD_LOG_KEY_ADD operation is rewinded, we remove the key from the
    tree block. If its not the last key, removal involves a move operation.
    This move operation was explicitly done before this commit.
    
    However, at insertion time, there's a move operation before the actual
    addition to make room for the new key, which is recorded in the tree mod
    log as well. This means, we must drop the move operation when rewinding the
    add operation, because the next operation we'll be rewinding will be the
    corresponding MOD_LOG_MOVE_KEYS operation.
    
    Signed-off-by: Jan Schmidt <list.btrfs@jan-o-sch.net>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index e005d9b04616..b98f8604f4f6 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1094,11 +1094,7 @@ __tree_mod_log_rewind(struct extent_buffer *eb, u64 time_seq,
 						      tm->generation);
 			break;
 		case MOD_LOG_KEY_ADD:
-			if (tm->slot != n - 1) {
-				o_dst = btrfs_node_key_ptr_offset(tm->slot);
-				o_src = btrfs_node_key_ptr_offset(tm->slot + 1);
-				memmove_extent_buffer(eb, o_dst, o_src, p_size);
-			}
+			/* if a move operation is needed it's in the log */
 			n--;
 			break;
 		case MOD_LOG_MOVE_KEYS:

commit c3e0696523862c48b4d8c73ffb2867e9db478338
Author: Jan Schmidt <list.btrfs@jan-o-sch.net>
Date:   Thu Jun 21 11:01:06 2012 +0200

    Btrfs: always put insert_ptr modifications into the tree mod log
    
    Several callers of insert_ptr set the tree_mod_log parameter to 0 to avoid
    addition to the tree mod log. In fact, we need all of those operations. This
    commit simply removes the additional parameter and makes addition to the
    tree mod log unconditional.
    
    Signed-off-by: Jan Schmidt <list.btrfs@jan-o-sch.net>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 7d1e4fc5fb6a..e005d9b04616 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2997,7 +2997,7 @@ static noinline int insert_new_root(struct btrfs_trans_handle *trans,
 static void insert_ptr(struct btrfs_trans_handle *trans,
 		       struct btrfs_root *root, struct btrfs_path *path,
 		       struct btrfs_disk_key *key, u64 bytenr,
-		       int slot, int level, int tree_mod_log)
+		       int slot, int level)
 {
 	struct extent_buffer *lower;
 	int nritems;
@@ -3010,7 +3010,7 @@ static void insert_ptr(struct btrfs_trans_handle *trans,
 	BUG_ON(slot > nritems);
 	BUG_ON(nritems == BTRFS_NODEPTRS_PER_BLOCK(root));
 	if (slot != nritems) {
-		if (tree_mod_log && level)
+		if (level)
 			tree_mod_log_eb_move(root->fs_info, lower, slot + 1,
 					     slot, nritems - slot);
 		memmove_extent_buffer(lower,
@@ -3018,7 +3018,7 @@ static void insert_ptr(struct btrfs_trans_handle *trans,
 			      btrfs_node_key_ptr_offset(slot),
 			      (nritems - slot) * sizeof(struct btrfs_key_ptr));
 	}
-	if (tree_mod_log && level) {
+	if (level) {
 		ret = tree_mod_log_insert_key(root->fs_info, lower, slot,
 					      MOD_LOG_KEY_ADD);
 		BUG_ON(ret < 0);
@@ -3106,7 +3106,7 @@ static noinline int split_node(struct btrfs_trans_handle *trans,
 	btrfs_mark_buffer_dirty(split);
 
 	insert_ptr(trans, root, path, &disk_key, split->start,
-		   path->slots[level + 1] + 1, level + 1, 1);
+		   path->slots[level + 1] + 1, level + 1);
 
 	if (path->slots[level] >= mid) {
 		path->slots[level] -= mid;
@@ -3643,7 +3643,7 @@ static noinline void copy_for_split(struct btrfs_trans_handle *trans,
 	btrfs_set_header_nritems(l, mid);
 	btrfs_item_key(right, &disk_key, 0);
 	insert_ptr(trans, root, path, &disk_key, right->start,
-		   path->slots[1] + 1, 1, 0);
+		   path->slots[1] + 1, 1);
 
 	btrfs_mark_buffer_dirty(right);
 	btrfs_mark_buffer_dirty(l);
@@ -3850,7 +3850,7 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 		if (mid <= slot) {
 			btrfs_set_header_nritems(right, 0);
 			insert_ptr(trans, root, path, &disk_key, right->start,
-				   path->slots[1] + 1, 1, 0);
+				   path->slots[1] + 1, 1);
 			btrfs_tree_unlock(path->nodes[0]);
 			free_extent_buffer(path->nodes[0]);
 			path->nodes[0] = right;
@@ -3859,7 +3859,7 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 		} else {
 			btrfs_set_header_nritems(right, 0);
 			insert_ptr(trans, root, path, &disk_key, right->start,
-					  path->slots[1], 1, 0);
+					  path->slots[1], 1);
 			btrfs_tree_unlock(path->nodes[0]);
 			free_extent_buffer(path->nodes[0]);
 			path->nodes[0] = right;

commit 28da9fb4467f7a650cd31af6dfad3a4e4a3abf6e
Author: Jan Schmidt <list.btrfs@jan-o-sch.net>
Date:   Thu Jun 21 10:59:13 2012 +0200

    Btrfs: fix tree mod log for root replacements at leaf level
    
    For the tree mod log, we don't log any operations at leaf level. If the root
    is at the leaf level (i.e. the tree consists only of the root), then
    __tree_mod_log_oldest_root will find a ROOT_REPLACE operation in the log
    (because we always log that one no matter which level), but no other
    operations.
    
    With this patch __tree_mod_log_oldest_root exits cleanly instead of
    BUGging in this situation. get_old_root checks if its really a root at leaf
    level in case we don't have any operations and WARNs if this assumption
    breaks.
    
    Signed-off-by: Jan Schmidt <list.btrfs@jan-o-sch.net>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 15cbc2bf4ff0..7d1e4fc5fb6a 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1024,11 +1024,18 @@ __tree_mod_log_oldest_root(struct btrfs_fs_info *fs_info,
 		if (!looped && !tm)
 			return 0;
 		/*
-		 * we must have key remove operations in the log before the
-		 * replace operation.
+		 * if there are no tree operation for the oldest root, we simply
+		 * return it. this should only happen if that (old) root is at
+		 * level 0.
 		 */
-		BUG_ON(!tm);
+		if (!tm)
+			break;
 
+		/*
+		 * if there's an operation that's not a root replacement, we
+		 * found the oldest version of our root. normally, we'll find a
+		 * MOD_LOG_KEY_REMOVE_WHILE_FREEING operation here.
+		 */
 		if (tm->op != MOD_LOG_ROOT_REPLACE)
 			break;
 
@@ -1192,16 +1199,8 @@ get_old_root(struct btrfs_root *root, u64 time_seq)
 	}
 
 	tm = tree_mod_log_search(root->fs_info, logical, time_seq);
-	/*
-	 * there was an item in the log when __tree_mod_log_oldest_root
-	 * returned. this one must not go away, because the time_seq passed to
-	 * us must be blocking its removal.
-	 */
-	BUG_ON(!tm);
-
 	if (old_root)
-		eb = alloc_dummy_extent_buffer(tm->index << PAGE_CACHE_SHIFT,
-					       root->nodesize);
+		eb = alloc_dummy_extent_buffer(logical, root->nodesize);
 	else
 		eb = btrfs_clone_extent_buffer(root->node);
 	btrfs_tree_read_unlock(root->node);
@@ -1216,7 +1215,10 @@ get_old_root(struct btrfs_root *root, u64 time_seq)
 		btrfs_set_header_level(eb, old_root->level);
 		btrfs_set_header_generation(eb, old_generation);
 	}
-	__tree_mod_log_rewind(eb, time_seq, tm);
+	if (tm)
+		__tree_mod_log_rewind(eb, time_seq, tm);
+	else
+		WARN_ON(btrfs_header_level(eb) != 0);
 	extent_buffer_get(eb);
 
 	return eb;

commit 4325edd0786fdd3909f9550e52a963b2fe54f78b
Author: Chris Mason <chris.mason@fusionio.com>
Date:   Fri Jun 15 20:02:02 2012 -0400

    Btrfs: init old_generation in get_old_root
    
    gcc was giving an uninit variable warning here.  Strictly
    speaking we don't need to init it, but this will make things
    much less error prone.
    
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 04b06bcf2ca9..15cbc2bf4ff0 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1175,7 +1175,7 @@ get_old_root(struct btrfs_root *root, u64 time_seq)
 	struct tree_mod_elem *tm;
 	struct extent_buffer *eb;
 	struct tree_mod_root *old_root = NULL;
-	u64 old_generation;
+	u64 old_generation = 0;
 	u64 logical;
 
 	eb = btrfs_read_lock_root_node(root);

commit 3310c36eef330766fd23d50796287ad764929e24
Author: Jan Schmidt <list.btrfs@jan-o-sch.net>
Date:   Mon Jun 11 10:52:38 2012 +0200

    Btrfs: fix race in tree mod log addition
    
    When adding to the tree modification log, we grab two locks at different
    stages. We must not drop the outer lock until we're done with section
    protected by the inner lock. This moves the unlock call for the outer lock
    to the appropriate position.
    
    Signed-off-by: Jan Schmidt <list.btrfs@jan-o-sch.net>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index cb76b2a1b908..04b06bcf2ca9 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -467,6 +467,15 @@ static inline int tree_mod_dont_log(struct btrfs_fs_info *fs_info,
 	return 0;
 }
 
+/*
+ * This allocates memory and gets a tree modification sequence number when
+ * needed.
+ *
+ * Returns 0 when no sequence number is needed, < 0 on error.
+ * Returns 1 when a sequence number was added. In this case,
+ * fs_info->tree_mod_seq_lock was acquired and must be released by the caller
+ * after inserting into the rb tree.
+ */
 static inline int tree_mod_alloc(struct btrfs_fs_info *fs_info, gfp_t flags,
 				 struct tree_mod_elem **tm_ret)
 {
@@ -491,11 +500,11 @@ static inline int tree_mod_alloc(struct btrfs_fs_info *fs_info, gfp_t flags,
 		 */
 		kfree(tm);
 		seq = 0;
+		spin_unlock(&fs_info->tree_mod_seq_lock);
 	} else {
 		__get_tree_mod_seq(fs_info, &tm->elem);
 		seq = tm->elem.seq;
 	}
-	spin_unlock(&fs_info->tree_mod_seq_lock);
 
 	return seq;
 }
@@ -521,7 +530,9 @@ tree_mod_log_insert_key_mask(struct btrfs_fs_info *fs_info,
 	tm->slot = slot;
 	tm->generation = btrfs_node_ptr_generation(eb, slot);
 
-	return __tree_mod_log_insert(fs_info, tm);
+	ret = __tree_mod_log_insert(fs_info, tm);
+	spin_unlock(&fs_info->tree_mod_seq_lock);
+	return ret;
 }
 
 static noinline int
@@ -559,7 +570,9 @@ tree_mod_log_insert_move(struct btrfs_fs_info *fs_info,
 	tm->move.nr_items = nr_items;
 	tm->op = MOD_LOG_MOVE_KEYS;
 
-	return __tree_mod_log_insert(fs_info, tm);
+	ret = __tree_mod_log_insert(fs_info, tm);
+	spin_unlock(&fs_info->tree_mod_seq_lock);
+	return ret;
 }
 
 static noinline int
@@ -580,7 +593,9 @@ tree_mod_log_insert_root(struct btrfs_fs_info *fs_info,
 	tm->generation = btrfs_header_generation(old_root);
 	tm->op = MOD_LOG_ROOT_REPLACE;
 
-	return __tree_mod_log_insert(fs_info, tm);
+	ret = __tree_mod_log_insert(fs_info, tm);
+	spin_unlock(&fs_info->tree_mod_seq_lock);
+	return ret;
 }
 
 static struct tree_mod_elem *

commit 3d7806eca43e73a9721d2e09369200ed93036bd0
Author: Jan Schmidt <list.btrfs@jan-o-sch.net>
Date:   Mon Jun 11 08:29:29 2012 +0200

    Btrfs: add btrfs_next_old_leaf
    
    To make sense of the tree mod log, the backref walker not only needs
    btrfs_search_old_slot, but it also called btrfs_next_leaf, which in turn was
    calling btrfs_search_slot. This obviously didn't give the correct result.
    
    This commit adds btrfs_next_old_leaf, a drop-in replacement for
    btrfs_next_leaf with a time_seq parameter. If it is zero, it behaves exactly
    like btrfs_next_leaf. If it is non-zero, it will use btrfs_search_old_slot
    with this time_seq parameter.
    
    Signed-off-by: Jan Schmidt <list.btrfs@jan-o-sch.net>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 50d7c99ddce7..cb76b2a1b908 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -5016,6 +5016,12 @@ int btrfs_find_next_key(struct btrfs_root *root, struct btrfs_path *path,
  * returns < 0 on io errors.
  */
 int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
+{
+	return btrfs_next_old_leaf(root, path, 0);
+}
+
+int btrfs_next_old_leaf(struct btrfs_root *root, struct btrfs_path *path,
+			u64 time_seq)
 {
 	int slot;
 	int level;
@@ -5041,7 +5047,10 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 	path->keep_locks = 1;
 	path->leave_spinning = 1;
 
-	ret = btrfs_search_slot(NULL, root, &key, path, 0, 0);
+	if (time_seq)
+		ret = btrfs_search_old_slot(root, &key, path, time_seq);
+	else
+		ret = btrfs_search_slot(NULL, root, &key, path, 0, 0);
 	path->keep_locks = 0;
 
 	if (ret < 0)

commit a95236d99fa56766f11056903439f55fe5038bcf
Author: Jan Schmidt <list.btrfs@jan-o-sch.net>
Date:   Tue Jun 5 16:41:24 2012 +0200

    Btrfs: fix return value for __tree_mod_log_oldest_root
    
    In __tree_mod_log_oldest_root() we must return the found operation even if
    it's not a ROOT_REPLACE operation. Otherwise, the caller assumes that there
    are no operations to be rewinded and returns immediately.
    
    The code in the caller is modified to improve readability.
    
    Signed-off-by: Jan Schmidt <list.btrfs@jan-o-sch.net>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 2cde7b0a0106..50d7c99ddce7 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1023,6 +1023,10 @@ __tree_mod_log_oldest_root(struct btrfs_fs_info *fs_info,
 		looped = 1;
 	}
 
+	/* if there's no old root to return, return what we found instead */
+	if (!found)
+		found = tm;
+
 	return found;
 }
 
@@ -1155,18 +1159,24 @@ get_old_root(struct btrfs_root *root, u64 time_seq)
 {
 	struct tree_mod_elem *tm;
 	struct extent_buffer *eb;
-	struct tree_mod_root *old_root;
+	struct tree_mod_root *old_root = NULL;
 	u64 old_generation;
+	u64 logical;
 
 	eb = btrfs_read_lock_root_node(root);
 	tm = __tree_mod_log_oldest_root(root->fs_info, root, time_seq);
 	if (!tm)
 		return root->node;
 
-	old_root = &tm->old_root;
-	old_generation = tm->generation;
+	if (tm->op == MOD_LOG_ROOT_REPLACE) {
+		old_root = &tm->old_root;
+		old_generation = tm->generation;
+		logical = old_root->logical;
+	} else {
+		logical = root->node->start;
+	}
 
-	tm = tree_mod_log_search(root->fs_info, old_root->logical, time_seq);
+	tm = tree_mod_log_search(root->fs_info, logical, time_seq);
 	/*
 	 * there was an item in the log when __tree_mod_log_oldest_root
 	 * returned. this one must not go away, because the time_seq passed to
@@ -1174,26 +1184,23 @@ get_old_root(struct btrfs_root *root, u64 time_seq)
 	 */
 	BUG_ON(!tm);
 
-	if (old_root->logical == root->node->start) {
-		/* there are logged operations for the current root */
-		eb = btrfs_clone_extent_buffer(root->node);
-	} else {
-		/* there's a root replace operation for the current root */
+	if (old_root)
 		eb = alloc_dummy_extent_buffer(tm->index << PAGE_CACHE_SHIFT,
 					       root->nodesize);
-	}
+	else
+		eb = btrfs_clone_extent_buffer(root->node);
 	btrfs_tree_read_unlock(root->node);
 	free_extent_buffer(root->node);
 	if (!eb)
 		return NULL;
 	btrfs_tree_read_lock(eb);
-	if (old_root->logical != root->node->start) {
+	if (old_root) {
 		btrfs_set_header_bytenr(eb, eb->start);
 		btrfs_set_header_backref_rev(eb, BTRFS_MIXED_BACKREF_REV);
 		btrfs_set_header_owner(eb, root->root_key.objectid);
+		btrfs_set_header_level(eb, old_root->level);
+		btrfs_set_header_generation(eb, old_generation);
 	}
-	btrfs_set_header_level(eb, old_root->level);
-	btrfs_set_header_generation(eb, old_generation);
 	__tree_mod_log_rewind(eb, time_seq, tm);
 	extent_buffer_get(eb);
 

commit 8ba97a15e7d4f70b9af71fa1db86a28dd17ad1b2
Author: Jan Schmidt <list.btrfs@jan-o-sch.net>
Date:   Mon Jun 4 16:54:57 2012 +0200

    Btrfs: use btrfs_read_lock_root_node in get_old_root
    
    get_old_root could race with root node updates because we weren't locking
    the node early enough. Use btrfs_read_lock_root_node to grab the root locked
    in the very beginning and release the lock as soon as possible (just like
    btrfs_search_slot does).
    
    Signed-off-by: Jan Schmidt <list.btrfs@jan-o-sch.net>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 836e4e03edca..2cde7b0a0106 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1143,6 +1143,13 @@ tree_mod_log_rewind(struct btrfs_fs_info *fs_info, struct extent_buffer *eb,
 	return eb_rewin;
 }
 
+/*
+ * get_old_root() rewinds the state of @root's root node to the given @time_seq
+ * value. If there are no changes, the current root->root_node is returned. If
+ * anything changed in between, there's a fresh buffer allocated on which the
+ * rewind operations are done. In any case, the returned buffer is read locked.
+ * Returns NULL on error (with no locks held).
+ */
 static inline struct extent_buffer *
 get_old_root(struct btrfs_root *root, u64 time_seq)
 {
@@ -1151,6 +1158,7 @@ get_old_root(struct btrfs_root *root, u64 time_seq)
 	struct tree_mod_root *old_root;
 	u64 old_generation;
 
+	eb = btrfs_read_lock_root_node(root);
 	tm = __tree_mod_log_oldest_root(root->fs_info, root, time_seq);
 	if (!tm)
 		return root->node;
@@ -1173,15 +1181,21 @@ get_old_root(struct btrfs_root *root, u64 time_seq)
 		/* there's a root replace operation for the current root */
 		eb = alloc_dummy_extent_buffer(tm->index << PAGE_CACHE_SHIFT,
 					       root->nodesize);
+	}
+	btrfs_tree_read_unlock(root->node);
+	free_extent_buffer(root->node);
+	if (!eb)
+		return NULL;
+	btrfs_tree_read_lock(eb);
+	if (old_root->logical != root->node->start) {
 		btrfs_set_header_bytenr(eb, eb->start);
 		btrfs_set_header_backref_rev(eb, BTRFS_MIXED_BACKREF_REV);
 		btrfs_set_header_owner(eb, root->root_key.objectid);
 	}
-	if (!eb)
-		return NULL;
 	btrfs_set_header_level(eb, old_root->level);
 	btrfs_set_header_generation(eb, old_generation);
 	__tree_mod_log_rewind(eb, time_seq, tm);
+	extent_buffer_get(eb);
 
 	return eb;
 }
@@ -2612,9 +2626,7 @@ int btrfs_search_old_slot(struct btrfs_root *root, struct btrfs_key *key,
 
 again:
 	b = get_old_root(root, time_seq);
-	extent_buffer_get(b);
 	level = btrfs_header_level(b);
-	btrfs_tree_read_lock(b);
 	p->locks[level] = BTRFS_READ_LOCK;
 
 	while (b) {

commit 4d5a0565cebf12c2ef854e4ac1961f13a710a950
Author: Jan Schmidt <list.btrfs@jan-o-sch.net>
Date:   Mon Apr 30 11:17:25 2012 +0200

    Btrfs: remove call to btrfs_header_nritems with no effect
    
    This is a leftover from cleanup patch 559af821. Before the cleanup,
    btrfs_header_nritems was called inside an if condition. As it has no side
    effects we need to preserve here, it should simply be dropped.
    
    Signed-off-by: Jan Schmidt <list.btrfs@jan-o-sch.net>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index d7a96cfdc50a..836e4e03edca 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1650,8 +1650,6 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 	    BTRFS_NODEPTRS_PER_BLOCK(root) / 4)
 		return 0;
 
-	btrfs_header_nritems(mid);
-
 	left = read_node_slot(root, parent, pslot - 1);
 	if (left) {
 		btrfs_tree_lock(left);
@@ -1681,7 +1679,6 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		wret = push_node_left(trans, root, left, mid, 1);
 		if (wret < 0)
 			ret = wret;
-		btrfs_header_nritems(mid);
 	}
 
 	/*

commit 1e20932a23578bb1ec59107843574e259b96193f
Merge: cfc442b69696 c31931088fd6
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu May 31 16:50:28 2012 -0400

    Merge branch 'for-chris' of git://git.jan-o-sch.net/btrfs-unstable into for-linus
    
    Conflicts:
            fs/btrfs/ulist.h
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

commit c31931088fd6cf953bd0868a2647b6c3928e6c96
Author: Jan Schmidt <list.btrfs@jan-o-sch.net>
Date:   Thu May 31 19:24:36 2012 +0200

    Btrfs: fix tree mod log rewinded level and rewinding of moved keys
    
    When we rewind REMOVE_WHILE_FREEING operations, there's code that allocates
    a fresh buffer instead of cloning the old one. Setting that buffer's level
    correctly was missing in this case.
    
    When rewinding a MOVE_KEYS operation, btrfs_node_key_ptr_offset(slot) was
    missing for memmove_extent_buffer()'s arguments.
    
    Signed-off-by: Jan Schmidt <list.btrfs@jan-o-sch.net>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 63147c1315a7..b4534d918e42 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1076,7 +1076,9 @@ __tree_mod_log_rewind(struct extent_buffer *eb, u64 time_seq,
 			n--;
 			break;
 		case MOD_LOG_MOVE_KEYS:
-			memmove_extent_buffer(eb, tm->slot, tm->move.dst_slot,
+			o_dst = btrfs_node_key_ptr_offset(tm->slot);
+			o_src = btrfs_node_key_ptr_offset(tm->move.dst_slot);
+			memmove_extent_buffer(eb, o_dst, o_src,
 					      tm->move.nr_items * p_size);
 			break;
 		case MOD_LOG_ROOT_REPLACE:
@@ -1127,6 +1129,7 @@ tree_mod_log_rewind(struct btrfs_fs_info *fs_info, struct extent_buffer *eb,
 		btrfs_set_header_backref_rev(eb_rewin,
 					     btrfs_header_backref_rev(eb));
 		btrfs_set_header_owner(eb_rewin, btrfs_header_owner(eb));
+		btrfs_set_header_level(eb_rewin, btrfs_header_level(eb));
 	} else {
 		eb_rewin = btrfs_clone_extent_buffer(eb);
 		BUG_ON(!eb_rewin);
@@ -2609,7 +2612,6 @@ int btrfs_search_old_slot(struct btrfs_root *root, struct btrfs_key *key,
 	}
 
 again:
-	level = 0;
 	b = get_old_root(root, time_seq);
 	extent_buffer_get(b);
 	level = btrfs_header_level(b);

commit f395694c2cd76cb1882fa82dd37e761598367fe9
Author: Jan Schmidt <list.btrfs@jan-o-sch.net>
Date:   Thu May 31 15:02:32 2012 +0200

    Btrfs: fix tree mod log del_ptr
    
    Logging for del_ptr when we're not deleting the last pointer was wrong. This
    fixes both, duplicate log entries and log sequence.
    
    Signed-off-by: Jan Schmidt <list.btrfs@jan-o-sch.net>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index c7c48489b963..63147c1315a7 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -540,9 +540,8 @@ tree_mod_log_insert_move(struct btrfs_fs_info *fs_info,
 	int ret;
 	int i;
 
-	ret = tree_mod_alloc(fs_info, flags, &tm);
-	if (ret <= 0)
-		return ret;
+	if (tree_mod_dont_log(fs_info, eb))
+		return 0;
 
 	for (i = 0; i + dst_slot < src_slot && i < nr_items; i++) {
 		ret = tree_mod_log_insert_key(fs_info, eb, i + dst_slot,
@@ -550,6 +549,10 @@ tree_mod_log_insert_move(struct btrfs_fs_info *fs_info,
 		BUG_ON(ret < 0);
 	}
 
+	ret = tree_mod_alloc(fs_info, flags, &tm);
+	if (ret <= 0)
+		return ret;
+
 	tm->index = eb->start >> PAGE_CACHE_SHIFT;
 	tm->slot = src_slot;
 	tm->move.dst_slot = dst_slot;
@@ -4548,9 +4551,7 @@ static void del_ptr(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 			      btrfs_node_key_ptr_offset(slot + 1),
 			      sizeof(struct btrfs_key_ptr) *
 			      (nritems - slot - 1));
-	}
-
-	if (tree_mod_log && level) {
+	} else if (tree_mod_log && level) {
 		ret = tree_mod_log_insert_key(root->fs_info, parent, slot,
 					      MOD_LOG_KEY_REMOVE);
 		BUG_ON(ret < 0);

commit e9b7fd4d8b7c915cff353ca085b83bd19737396b
Author: Jan Schmidt <list.btrfs@jan-o-sch.net>
Date:   Thu May 31 14:59:09 2012 +0200

    Btrfs: add tree_mod_dont_log helper
    
    Replace duplicate code by small inline helper function.
    
    Signed-off-by: Jan Schmidt <list.btrfs@jan-o-sch.net>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 26e8dc1681b0..c7c48489b963 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -455,14 +455,25 @@ __tree_mod_log_insert(struct btrfs_fs_info *fs_info, struct tree_mod_elem *tm)
 	return ret;
 }
 
+static inline int tree_mod_dont_log(struct btrfs_fs_info *fs_info,
+				    struct extent_buffer *eb) {
+	smp_mb();
+	if (list_empty(&(fs_info)->tree_mod_seq_list))
+		return 1;
+	if (!eb)
+		return 0;
+	if (btrfs_header_level(eb) == 0)
+		return 1;
+	return 0;
+}
+
 static inline int tree_mod_alloc(struct btrfs_fs_info *fs_info, gfp_t flags,
 				 struct tree_mod_elem **tm_ret)
 {
 	struct tree_mod_elem *tm;
 	int seq;
 
-	smp_mb();
-	if (list_empty(&fs_info->tree_mod_seq_list))
+	if (tree_mod_dont_log(fs_info, NULL))
 		return 0;
 
 	tm = *tm_ret = kzalloc(sizeof(*tm), flags);
@@ -643,8 +654,7 @@ tree_mod_log_eb_copy(struct btrfs_fs_info *fs_info, struct extent_buffer *dst,
 	int ret;
 	int i;
 
-	smp_mb();
-	if (list_empty(&fs_info->tree_mod_seq_list))
+	if (tree_mod_dont_log(fs_info, NULL))
 		return;
 
 	if (btrfs_header_level(dst) == 0 && btrfs_header_level(src) == 0)
@@ -691,11 +701,7 @@ static void tree_mod_log_free_eb(struct btrfs_fs_info *fs_info,
 	int ret;
 	u32 nritems;
 
-	smp_mb();
-	if (list_empty(&fs_info->tree_mod_seq_list))
-		return;
-
-	if (btrfs_header_level(eb) == 0)
+	if (tree_mod_dont_log(fs_info, eb))
 		return;
 
 	nritems = btrfs_header_nritems(eb);

commit 926dd8a640da1bbf7478eebea1c23a842fc9c890
Author: Jan Schmidt <list.btrfs@jan-o-sch.net>
Date:   Thu May 31 14:00:15 2012 +0200

    Btrfs: add missing spin_lock for insertion into tree mod log
    
    tree_mod_alloc calls __get_tree_mod_seq and must acquire a spinlock before
    doing so.
    
    Signed-off-by: Jan Schmidt <list.btrfs@jan-o-sch.net>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 0954f1770fd0..26e8dc1681b0 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -455,11 +455,11 @@ __tree_mod_log_insert(struct btrfs_fs_info *fs_info, struct tree_mod_elem *tm)
 	return ret;
 }
 
-int tree_mod_alloc(struct btrfs_fs_info *fs_info, gfp_t flags,
-		   struct tree_mod_elem **tm_ret)
+static inline int tree_mod_alloc(struct btrfs_fs_info *fs_info, gfp_t flags,
+				 struct tree_mod_elem **tm_ret)
 {
 	struct tree_mod_elem *tm;
-	u64 seq = 0;
+	int seq;
 
 	smp_mb();
 	if (list_empty(&fs_info->tree_mod_seq_list))
@@ -469,9 +469,22 @@ int tree_mod_alloc(struct btrfs_fs_info *fs_info, gfp_t flags,
 	if (!tm)
 		return -ENOMEM;
 
-	__get_tree_mod_seq(fs_info, &tm->elem);
-	seq = tm->elem.seq;
 	tm->elem.flags = 0;
+	spin_lock(&fs_info->tree_mod_seq_lock);
+	if (list_empty(&fs_info->tree_mod_seq_list)) {
+		/*
+		 * someone emptied the list while we were waiting for the lock.
+		 * we must not add to the list, because no blocker exists. items
+		 * are removed from the list only when the existing blocker is
+		 * removed from the list.
+		 */
+		kfree(tm);
+		seq = 0;
+	} else {
+		__get_tree_mod_seq(fs_info, &tm->elem);
+		seq = tm->elem.seq;
+	}
+	spin_unlock(&fs_info->tree_mod_seq_lock);
 
 	return seq;
 }

commit 018642a1f197887058e97291460b890d296e8953
Author: Tsutomu Itoh <t-itoh@jp.fujitsu.com>
Date:   Tue May 29 18:10:13 2012 +0900

    Btrfs: return value of btrfs_read_buffer is checked correctly
    
    btrfs_read_buffer() has the possibility of returning the error.
    Therefore, I add the code in which the return value of btrfs_read_buffer()
    is checked.
    
    Signed-off-by: Tsutomu Itoh <t-itoh@jp.fujitsu.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 26847999c649..99fcad631a21 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -739,7 +739,11 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 				if (!cur)
 					return -EIO;
 			} else if (!uptodate) {
-				btrfs_read_buffer(cur, gen);
+				err = btrfs_read_buffer(cur, gen);
+				if (err) {
+					free_extent_buffer(cur);
+					return err;
+				}
 			}
 		}
 		if (search_start == 0)

commit 5d9e75c41d11ca79b1d1ff6ed17c88c9047d1fea
Author: Jan Schmidt <list.btrfs@jan-o-sch.net>
Date:   Wed May 16 18:25:47 2012 +0200

    Btrfs: add btrfs_search_old_slot
    
    The tree modification log together with the current state of the tree gives
    a consistent, old version of the tree. btrfs_search_old_slot is used to
    search through this old version and return old (dummy!) extent buffers.
    Naturally, this function cannot do any tree modifications.
    
    Signed-off-by: Jan Schmidt <list.btrfs@jan-o-sch.net>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 69ce3f7deeef..0954f1770fd0 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -960,6 +960,207 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 	return 0;
 }
 
+/*
+ * returns the logical address of the oldest predecessor of the given root.
+ * entries older than time_seq are ignored.
+ */
+static struct tree_mod_elem *
+__tree_mod_log_oldest_root(struct btrfs_fs_info *fs_info,
+			   struct btrfs_root *root, u64 time_seq)
+{
+	struct tree_mod_elem *tm;
+	struct tree_mod_elem *found = NULL;
+	u64 root_logical = root->node->start;
+	int looped = 0;
+
+	if (!time_seq)
+		return 0;
+
+	/*
+	 * the very last operation that's logged for a root is the replacement
+	 * operation (if it is replaced at all). this has the index of the *new*
+	 * root, making it the very first operation that's logged for this root.
+	 */
+	while (1) {
+		tm = tree_mod_log_search_oldest(fs_info, root_logical,
+						time_seq);
+		if (!looped && !tm)
+			return 0;
+		/*
+		 * we must have key remove operations in the log before the
+		 * replace operation.
+		 */
+		BUG_ON(!tm);
+
+		if (tm->op != MOD_LOG_ROOT_REPLACE)
+			break;
+
+		found = tm;
+		root_logical = tm->old_root.logical;
+		BUG_ON(root_logical == root->node->start);
+		looped = 1;
+	}
+
+	return found;
+}
+
+/*
+ * tm is a pointer to the first operation to rewind within eb. then, all
+ * previous operations will be rewinded (until we reach something older than
+ * time_seq).
+ */
+static void
+__tree_mod_log_rewind(struct extent_buffer *eb, u64 time_seq,
+		      struct tree_mod_elem *first_tm)
+{
+	u32 n;
+	struct rb_node *next;
+	struct tree_mod_elem *tm = first_tm;
+	unsigned long o_dst;
+	unsigned long o_src;
+	unsigned long p_size = sizeof(struct btrfs_key_ptr);
+
+	n = btrfs_header_nritems(eb);
+	while (tm && tm->elem.seq >= time_seq) {
+		/*
+		 * all the operations are recorded with the operator used for
+		 * the modification. as we're going backwards, we do the
+		 * opposite of each operation here.
+		 */
+		switch (tm->op) {
+		case MOD_LOG_KEY_REMOVE_WHILE_FREEING:
+			BUG_ON(tm->slot < n);
+		case MOD_LOG_KEY_REMOVE_WHILE_MOVING:
+		case MOD_LOG_KEY_REMOVE:
+			btrfs_set_node_key(eb, &tm->key, tm->slot);
+			btrfs_set_node_blockptr(eb, tm->slot, tm->blockptr);
+			btrfs_set_node_ptr_generation(eb, tm->slot,
+						      tm->generation);
+			n++;
+			break;
+		case MOD_LOG_KEY_REPLACE:
+			BUG_ON(tm->slot >= n);
+			btrfs_set_node_key(eb, &tm->key, tm->slot);
+			btrfs_set_node_blockptr(eb, tm->slot, tm->blockptr);
+			btrfs_set_node_ptr_generation(eb, tm->slot,
+						      tm->generation);
+			break;
+		case MOD_LOG_KEY_ADD:
+			if (tm->slot != n - 1) {
+				o_dst = btrfs_node_key_ptr_offset(tm->slot);
+				o_src = btrfs_node_key_ptr_offset(tm->slot + 1);
+				memmove_extent_buffer(eb, o_dst, o_src, p_size);
+			}
+			n--;
+			break;
+		case MOD_LOG_MOVE_KEYS:
+			memmove_extent_buffer(eb, tm->slot, tm->move.dst_slot,
+					      tm->move.nr_items * p_size);
+			break;
+		case MOD_LOG_ROOT_REPLACE:
+			/*
+			 * this operation is special. for roots, this must be
+			 * handled explicitly before rewinding.
+			 * for non-roots, this operation may exist if the node
+			 * was a root: root A -> child B; then A gets empty and
+			 * B is promoted to the new root. in the mod log, we'll
+			 * have a root-replace operation for B, a tree block
+			 * that is no root. we simply ignore that operation.
+			 */
+			break;
+		}
+		next = rb_next(&tm->node);
+		if (!next)
+			break;
+		tm = container_of(next, struct tree_mod_elem, node);
+		if (tm->index != first_tm->index)
+			break;
+	}
+	btrfs_set_header_nritems(eb, n);
+}
+
+static struct extent_buffer *
+tree_mod_log_rewind(struct btrfs_fs_info *fs_info, struct extent_buffer *eb,
+		    u64 time_seq)
+{
+	struct extent_buffer *eb_rewin;
+	struct tree_mod_elem *tm;
+
+	if (!time_seq)
+		return eb;
+
+	if (btrfs_header_level(eb) == 0)
+		return eb;
+
+	tm = tree_mod_log_search(fs_info, eb->start, time_seq);
+	if (!tm)
+		return eb;
+
+	if (tm->op == MOD_LOG_KEY_REMOVE_WHILE_FREEING) {
+		BUG_ON(tm->slot != 0);
+		eb_rewin = alloc_dummy_extent_buffer(eb->start,
+						fs_info->tree_root->nodesize);
+		BUG_ON(!eb_rewin);
+		btrfs_set_header_bytenr(eb_rewin, eb->start);
+		btrfs_set_header_backref_rev(eb_rewin,
+					     btrfs_header_backref_rev(eb));
+		btrfs_set_header_owner(eb_rewin, btrfs_header_owner(eb));
+	} else {
+		eb_rewin = btrfs_clone_extent_buffer(eb);
+		BUG_ON(!eb_rewin);
+	}
+
+	extent_buffer_get(eb_rewin);
+	free_extent_buffer(eb);
+
+	__tree_mod_log_rewind(eb_rewin, time_seq, tm);
+
+	return eb_rewin;
+}
+
+static inline struct extent_buffer *
+get_old_root(struct btrfs_root *root, u64 time_seq)
+{
+	struct tree_mod_elem *tm;
+	struct extent_buffer *eb;
+	struct tree_mod_root *old_root;
+	u64 old_generation;
+
+	tm = __tree_mod_log_oldest_root(root->fs_info, root, time_seq);
+	if (!tm)
+		return root->node;
+
+	old_root = &tm->old_root;
+	old_generation = tm->generation;
+
+	tm = tree_mod_log_search(root->fs_info, old_root->logical, time_seq);
+	/*
+	 * there was an item in the log when __tree_mod_log_oldest_root
+	 * returned. this one must not go away, because the time_seq passed to
+	 * us must be blocking its removal.
+	 */
+	BUG_ON(!tm);
+
+	if (old_root->logical == root->node->start) {
+		/* there are logged operations for the current root */
+		eb = btrfs_clone_extent_buffer(root->node);
+	} else {
+		/* there's a root replace operation for the current root */
+		eb = alloc_dummy_extent_buffer(tm->index << PAGE_CACHE_SHIFT,
+					       root->nodesize);
+		btrfs_set_header_bytenr(eb, eb->start);
+		btrfs_set_header_backref_rev(eb, BTRFS_MIXED_BACKREF_REV);
+		btrfs_set_header_owner(eb, root->root_key.objectid);
+	}
+	if (!eb)
+		return NULL;
+	btrfs_set_header_level(eb, old_root->level);
+	btrfs_set_header_generation(eb, old_generation);
+	__tree_mod_log_rewind(eb, time_seq, tm);
+
+	return eb;
+}
+
 static inline int should_cow_block(struct btrfs_trans_handle *trans,
 				   struct btrfs_root *root,
 				   struct extent_buffer *buf)
@@ -1930,7 +2131,7 @@ static int
 read_block_for_search(struct btrfs_trans_handle *trans,
 		       struct btrfs_root *root, struct btrfs_path *p,
 		       struct extent_buffer **eb_ret, int level, int slot,
-		       struct btrfs_key *key)
+		       struct btrfs_key *key, u64 time_seq)
 {
 	u64 blocknr;
 	u64 gen;
@@ -2284,7 +2485,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 			}
 
 			err = read_block_for_search(trans, root, p,
-						    &b, level, slot, key);
+						    &b, level, slot, key, 0);
 			if (err == -EAGAIN)
 				goto again;
 			if (err) {
@@ -2355,6 +2556,116 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 	return ret;
 }
 
+/*
+ * Like btrfs_search_slot, this looks for a key in the given tree. It uses the
+ * current state of the tree together with the operations recorded in the tree
+ * modification log to search for the key in a previous version of this tree, as
+ * denoted by the time_seq parameter.
+ *
+ * Naturally, there is no support for insert, delete or cow operations.
+ *
+ * The resulting path and return value will be set up as if we called
+ * btrfs_search_slot at that point in time with ins_len and cow both set to 0.
+ */
+int btrfs_search_old_slot(struct btrfs_root *root, struct btrfs_key *key,
+			  struct btrfs_path *p, u64 time_seq)
+{
+	struct extent_buffer *b;
+	int slot;
+	int ret;
+	int err;
+	int level;
+	int lowest_unlock = 1;
+	u8 lowest_level = 0;
+
+	lowest_level = p->lowest_level;
+	WARN_ON(p->nodes[0] != NULL);
+
+	if (p->search_commit_root) {
+		BUG_ON(time_seq);
+		return btrfs_search_slot(NULL, root, key, p, 0, 0);
+	}
+
+again:
+	level = 0;
+	b = get_old_root(root, time_seq);
+	extent_buffer_get(b);
+	level = btrfs_header_level(b);
+	btrfs_tree_read_lock(b);
+	p->locks[level] = BTRFS_READ_LOCK;
+
+	while (b) {
+		level = btrfs_header_level(b);
+		p->nodes[level] = b;
+		btrfs_clear_path_blocking(p, NULL, 0);
+
+		/*
+		 * we have a lock on b and as long as we aren't changing
+		 * the tree, there is no way to for the items in b to change.
+		 * It is safe to drop the lock on our parent before we
+		 * go through the expensive btree search on b.
+		 */
+		btrfs_unlock_up_safe(p, level + 1);
+
+		ret = bin_search(b, key, level, &slot);
+
+		if (level != 0) {
+			int dec = 0;
+			if (ret && slot > 0) {
+				dec = 1;
+				slot -= 1;
+			}
+			p->slots[level] = slot;
+			unlock_up(p, level, lowest_unlock, 0, NULL);
+
+			if (level == lowest_level) {
+				if (dec)
+					p->slots[level]++;
+				goto done;
+			}
+
+			err = read_block_for_search(NULL, root, p, &b, level,
+						    slot, key, time_seq);
+			if (err == -EAGAIN)
+				goto again;
+			if (err) {
+				ret = err;
+				goto done;
+			}
+
+			level = btrfs_header_level(b);
+			err = btrfs_try_tree_read_lock(b);
+			if (!err) {
+				btrfs_set_path_blocking(p);
+				btrfs_tree_read_lock(b);
+				btrfs_clear_path_blocking(p, b,
+							  BTRFS_READ_LOCK);
+			}
+			p->locks[level] = BTRFS_READ_LOCK;
+			p->nodes[level] = b;
+			b = tree_mod_log_rewind(root->fs_info, b, time_seq);
+			if (b != p->nodes[level]) {
+				btrfs_tree_unlock_rw(p->nodes[level],
+						     p->locks[level]);
+				p->locks[level] = 0;
+				p->nodes[level] = b;
+			}
+		} else {
+			p->slots[level] = slot;
+			unlock_up(p, level, lowest_unlock, 0, NULL);
+			goto done;
+		}
+	}
+	ret = 1;
+done:
+	if (!p->leave_spinning)
+		btrfs_set_path_blocking(p);
+	if (ret < 0)
+		btrfs_release_path(p);
+
+	return ret;
+}
+
 /*
  * adjust the pointers going up the tree, starting at level
  * making sure the right key of each node is points to 'key'.
@@ -4735,7 +5046,7 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 		next = c;
 		next_rw_lock = path->locks[level];
 		ret = read_block_for_search(NULL, root, path, &next, level,
-					    slot, &key);
+					    slot, &key, 0);
 		if (ret == -EAGAIN)
 			goto again;
 
@@ -4772,7 +5083,7 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 			break;
 
 		ret = read_block_for_search(NULL, root, path, &next, level,
-					    0, &key);
+					    0, &key, 0);
 		if (ret == -EAGAIN)
 			goto again;
 

commit f3ea38da3e76455fbd6d405cdca4d050ed085458
Author: Jan Schmidt <list.btrfs@jan-o-sch.net>
Date:   Sat May 26 11:45:21 2012 +0200

    Btrfs: add del_ptr and insert_ptr modifications to the tree mod log
    
    Record all relevant modifications to block pointers in the tree mod log so
    that we can rewind them later on for backref walking.
    
    Signed-off-by: Jan Schmidt <list.btrfs@jan-o-sch.net>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 07c1a96aa4a8..69ce3f7deeef 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -38,7 +38,8 @@ static int balance_node_right(struct btrfs_trans_handle *trans,
 			      struct extent_buffer *dst_buf,
 			      struct extent_buffer *src_buf);
 static void del_ptr(struct btrfs_trans_handle *trans, struct btrfs_root *root,
-		   struct btrfs_path *path, int level, int slot);
+		    struct btrfs_path *path, int level, int slot,
+		    int tree_mod_log);
 static void tree_mod_log_free_eb(struct btrfs_fs_info *fs_info,
 				 struct extent_buffer *eb);
 struct extent_buffer *read_old_tree_block(struct btrfs_root *root, u64 bytenr,
@@ -1465,7 +1466,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		if (btrfs_header_nritems(right) == 0) {
 			clean_tree_block(trans, root, right);
 			btrfs_tree_unlock(right);
-			del_ptr(trans, root, path, level + 1, pslot + 1);
+			del_ptr(trans, root, path, level + 1, pslot + 1, 1);
 			root_sub_used(root, right->len);
 			btrfs_free_tree_block(trans, root, right, 0, 1);
 			free_extent_buffer_stale(right);
@@ -1509,7 +1510,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 	if (btrfs_header_nritems(mid) == 0) {
 		clean_tree_block(trans, root, mid);
 		btrfs_tree_unlock(mid);
-		del_ptr(trans, root, path, level + 1, pslot);
+		del_ptr(trans, root, path, level + 1, pslot, 1);
 		root_sub_used(root, mid->len);
 		btrfs_free_tree_block(trans, root, mid, 0, 1);
 		free_extent_buffer_stale(mid);
@@ -2626,10 +2627,11 @@ static noinline int insert_new_root(struct btrfs_trans_handle *trans,
 static void insert_ptr(struct btrfs_trans_handle *trans,
 		       struct btrfs_root *root, struct btrfs_path *path,
 		       struct btrfs_disk_key *key, u64 bytenr,
-		       int slot, int level)
+		       int slot, int level, int tree_mod_log)
 {
 	struct extent_buffer *lower;
 	int nritems;
+	int ret;
 
 	BUG_ON(!path->nodes[level]);
 	btrfs_assert_tree_locked(path->nodes[level]);
@@ -2638,11 +2640,19 @@ static void insert_ptr(struct btrfs_trans_handle *trans,
 	BUG_ON(slot > nritems);
 	BUG_ON(nritems == BTRFS_NODEPTRS_PER_BLOCK(root));
 	if (slot != nritems) {
+		if (tree_mod_log && level)
+			tree_mod_log_eb_move(root->fs_info, lower, slot + 1,
+					     slot, nritems - slot);
 		memmove_extent_buffer(lower,
 			      btrfs_node_key_ptr_offset(slot + 1),
 			      btrfs_node_key_ptr_offset(slot),
 			      (nritems - slot) * sizeof(struct btrfs_key_ptr));
 	}
+	if (tree_mod_log && level) {
+		ret = tree_mod_log_insert_key(root->fs_info, lower, slot,
+					      MOD_LOG_KEY_ADD);
+		BUG_ON(ret < 0);
+	}
 	btrfs_set_node_key(lower, key, slot);
 	btrfs_set_node_blockptr(lower, slot, bytenr);
 	WARN_ON(trans->transid == 0);
@@ -2726,7 +2736,7 @@ static noinline int split_node(struct btrfs_trans_handle *trans,
 	btrfs_mark_buffer_dirty(split);
 
 	insert_ptr(trans, root, path, &disk_key, split->start,
-		   path->slots[level + 1] + 1, level + 1);
+		   path->slots[level + 1] + 1, level + 1, 1);
 
 	if (path->slots[level] >= mid) {
 		path->slots[level] -= mid;
@@ -3263,7 +3273,7 @@ static noinline void copy_for_split(struct btrfs_trans_handle *trans,
 	btrfs_set_header_nritems(l, mid);
 	btrfs_item_key(right, &disk_key, 0);
 	insert_ptr(trans, root, path, &disk_key, right->start,
-		   path->slots[1] + 1, 1);
+		   path->slots[1] + 1, 1, 0);
 
 	btrfs_mark_buffer_dirty(right);
 	btrfs_mark_buffer_dirty(l);
@@ -3470,7 +3480,7 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 		if (mid <= slot) {
 			btrfs_set_header_nritems(right, 0);
 			insert_ptr(trans, root, path, &disk_key, right->start,
-				   path->slots[1] + 1, 1);
+				   path->slots[1] + 1, 1, 0);
 			btrfs_tree_unlock(path->nodes[0]);
 			free_extent_buffer(path->nodes[0]);
 			path->nodes[0] = right;
@@ -3479,7 +3489,7 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 		} else {
 			btrfs_set_header_nritems(right, 0);
 			insert_ptr(trans, root, path, &disk_key, right->start,
-					  path->slots[1], 1);
+					  path->slots[1], 1, 0);
 			btrfs_tree_unlock(path->nodes[0]);
 			free_extent_buffer(path->nodes[0]);
 			path->nodes[0] = right;
@@ -4191,19 +4201,31 @@ int btrfs_insert_item(struct btrfs_trans_handle *trans, struct btrfs_root
  * empty a node.
  */
 static void del_ptr(struct btrfs_trans_handle *trans, struct btrfs_root *root,
-		    struct btrfs_path *path, int level, int slot)
+		    struct btrfs_path *path, int level, int slot,
+		    int tree_mod_log)
 {
 	struct extent_buffer *parent = path->nodes[level];
 	u32 nritems;
+	int ret;
 
 	nritems = btrfs_header_nritems(parent);
 	if (slot != nritems - 1) {
+		if (tree_mod_log && level)
+			tree_mod_log_eb_move(root->fs_info, parent, slot,
+					     slot + 1, nritems - slot - 1);
 		memmove_extent_buffer(parent,
 			      btrfs_node_key_ptr_offset(slot),
 			      btrfs_node_key_ptr_offset(slot + 1),
 			      sizeof(struct btrfs_key_ptr) *
 			      (nritems - slot - 1));
 	}
+
+	if (tree_mod_log && level) {
+		ret = tree_mod_log_insert_key(root->fs_info, parent, slot,
+					      MOD_LOG_KEY_REMOVE);
+		BUG_ON(ret < 0);
+	}
+
 	nritems--;
 	btrfs_set_header_nritems(parent, nritems);
 	if (nritems == 0 && parent == root->node) {
@@ -4235,7 +4257,7 @@ static noinline void btrfs_del_leaf(struct btrfs_trans_handle *trans,
 				    struct extent_buffer *leaf)
 {
 	WARN_ON(btrfs_header_generation(leaf) != trans->transid);
-	del_ptr(trans, root, path, 1, path->slots[1]);
+	del_ptr(trans, root, path, 1, path->slots[1], 1);
 
 	/*
 	 * btrfs_free_extent is expensive, we want to make sure we

commit f230475e62f77930e776881deb6e95cfd2226bd4
Author: Jan Schmidt <list.btrfs@jan-o-sch.net>
Date:   Sat May 26 11:43:17 2012 +0200

    Btrfs: put all block modifications into the tree mod log
    
    When running functions that can make changes to the internal trees
    (e.g. btrfs_search_slot), we check if somebody may be interested in the
    block we're currently modifying. If so, we record our modification to be
    able to rewind it later on.
    
    Signed-off-by: Jan Schmidt <list.btrfs@jan-o-sch.net>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 72b9f97e2fdc..07c1a96aa4a8 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -39,6 +39,14 @@ static int balance_node_right(struct btrfs_trans_handle *trans,
 			      struct extent_buffer *src_buf);
 static void del_ptr(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 		   struct btrfs_path *path, int level, int slot);
+static void tree_mod_log_free_eb(struct btrfs_fs_info *fs_info,
+				 struct extent_buffer *eb);
+struct extent_buffer *read_old_tree_block(struct btrfs_root *root, u64 bytenr,
+					  u32 blocksize, u64 parent_transid,
+					  u64 time_seq);
+struct extent_buffer *btrfs_find_old_tree_block(struct btrfs_root *root,
+						u64 bytenr, u32 blocksize,
+						u64 time_seq);
 
 struct btrfs_path *btrfs_alloc_path(void)
 {
@@ -816,6 +824,12 @@ static noinline int update_ref_for_cow(struct btrfs_trans_handle *trans,
 			ret = btrfs_dec_ref(trans, root, buf, 1, 1);
 			BUG_ON(ret); /* -ENOMEM */
 		}
+		/*
+		 * don't log freeing in case we're freeing the root node, this
+		 * is done by tree_mod_log_set_root_pointer later
+		 */
+		if (buf != root->node && btrfs_header_level(buf) != 0)
+			tree_mod_log_free_eb(root->fs_info, buf);
 		clean_tree_block(trans, root, buf);
 		*last_ref = 1;
 	}
@@ -913,6 +927,7 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 			parent_start = 0;
 
 		extent_buffer_get(cow);
+		tree_mod_log_set_root_pointer(root, cow);
 		rcu_assign_pointer(root->node, cow);
 
 		btrfs_free_tree_block(trans, root, buf, parent_start,
@@ -926,6 +941,8 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 			parent_start = 0;
 
 		WARN_ON(trans->transid != btrfs_header_generation(parent));
+		tree_mod_log_insert_key(root->fs_info, parent, parent_slot,
+					MOD_LOG_KEY_REPLACE);
 		btrfs_set_node_blockptr(parent, parent_slot,
 					cow->start);
 		btrfs_set_node_ptr_generation(parent, parent_slot,
@@ -1381,6 +1398,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 			goto enospc;
 		}
 
+		tree_mod_log_set_root_pointer(root, child);
 		rcu_assign_pointer(root->node, child);
 
 		add_root_to_dirty_list(root);
@@ -1455,6 +1473,8 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		} else {
 			struct btrfs_disk_key right_key;
 			btrfs_node_key(right, &right_key, 0);
+			tree_mod_log_set_node_key(root->fs_info, parent,
+						  &right_key, pslot + 1, 0);
 			btrfs_set_node_key(parent, &right_key, pslot + 1);
 			btrfs_mark_buffer_dirty(parent);
 		}
@@ -1498,6 +1518,8 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		/* update the parent key to reflect our changes */
 		struct btrfs_disk_key mid_key;
 		btrfs_node_key(mid, &mid_key, 0);
+		tree_mod_log_set_node_key(root->fs_info, parent, &mid_key,
+					  pslot, 0);
 		btrfs_set_node_key(parent, &mid_key, pslot);
 		btrfs_mark_buffer_dirty(parent);
 	}
@@ -1595,6 +1617,8 @@ static noinline int push_nodes_for_insert(struct btrfs_trans_handle *trans,
 			struct btrfs_disk_key disk_key;
 			orig_slot += left_nr;
 			btrfs_node_key(mid, &disk_key, 0);
+			tree_mod_log_set_node_key(root->fs_info, parent,
+						  &disk_key, pslot, 0);
 			btrfs_set_node_key(parent, &disk_key, pslot);
 			btrfs_mark_buffer_dirty(parent);
 			if (btrfs_header_nritems(left) > orig_slot) {
@@ -1646,6 +1670,8 @@ static noinline int push_nodes_for_insert(struct btrfs_trans_handle *trans,
 			struct btrfs_disk_key disk_key;
 
 			btrfs_node_key(right, &disk_key, 0);
+			tree_mod_log_set_node_key(root->fs_info, parent,
+						  &disk_key, pslot + 1, 0);
 			btrfs_set_node_key(parent, &disk_key, pslot + 1);
 			btrfs_mark_buffer_dirty(parent);
 
@@ -2348,6 +2374,7 @@ static void fixup_low_keys(struct btrfs_trans_handle *trans,
 		if (!path->nodes[i])
 			break;
 		t = path->nodes[i];
+		tree_mod_log_set_node_key(root->fs_info, t, key, tslot, 1);
 		btrfs_set_node_key(t, key, tslot);
 		btrfs_mark_buffer_dirty(path->nodes[i]);
 		if (tslot != 0)
@@ -2430,12 +2457,16 @@ static int push_node_left(struct btrfs_trans_handle *trans,
 	} else
 		push_items = min(src_nritems - 8, push_items);
 
+	tree_mod_log_eb_copy(root->fs_info, dst, src, dst_nritems, 0,
+			     push_items);
 	copy_extent_buffer(dst, src,
 			   btrfs_node_key_ptr_offset(dst_nritems),
 			   btrfs_node_key_ptr_offset(0),
 			   push_items * sizeof(struct btrfs_key_ptr));
 
 	if (push_items < src_nritems) {
+		tree_mod_log_eb_move(root->fs_info, src, 0, push_items,
+				     src_nritems - push_items);
 		memmove_extent_buffer(src, btrfs_node_key_ptr_offset(0),
 				      btrfs_node_key_ptr_offset(push_items),
 				      (src_nritems - push_items) *
@@ -2489,11 +2520,14 @@ static int balance_node_right(struct btrfs_trans_handle *trans,
 	if (max_push < push_items)
 		push_items = max_push;
 
+	tree_mod_log_eb_move(root->fs_info, dst, push_items, 0, dst_nritems);
 	memmove_extent_buffer(dst, btrfs_node_key_ptr_offset(push_items),
 				      btrfs_node_key_ptr_offset(0),
 				      (dst_nritems) *
 				      sizeof(struct btrfs_key_ptr));
 
+	tree_mod_log_eb_copy(root->fs_info, dst, src, 0,
+			     src_nritems - push_items, push_items);
 	copy_extent_buffer(dst, src,
 			   btrfs_node_key_ptr_offset(0),
 			   btrfs_node_key_ptr_offset(src_nritems - push_items),
@@ -2568,6 +2602,7 @@ static noinline int insert_new_root(struct btrfs_trans_handle *trans,
 	btrfs_mark_buffer_dirty(c);
 
 	old = root->node;
+	tree_mod_log_set_root_pointer(root, c);
 	rcu_assign_pointer(root->node, c);
 
 	/* the super has an extra ref to root->node */
@@ -2678,6 +2713,7 @@ static noinline int split_node(struct btrfs_trans_handle *trans,
 			    (unsigned long)btrfs_header_chunk_tree_uuid(split),
 			    BTRFS_UUID_SIZE);
 
+	tree_mod_log_eb_copy(root->fs_info, split, c, 0, mid, c_nritems - mid);
 	copy_extent_buffer(split, c,
 			   btrfs_node_key_ptr_offset(0),
 			   btrfs_node_key_ptr_offset(mid),

commit bd989ba359f2acb8bc5f5490e19010fc0a6f8356
Author: Jan Schmidt <list.btrfs@jan-o-sch.net>
Date:   Wed May 16 17:18:50 2012 +0200

    Btrfs: add tree modification log functions
    
    The tree mod log will log modifications made fs-tree nodes. Most
    modifications are done by autobalance of the tree. Such changes are recorded
    as long as a block entry exists. When released, the log is cleaned.
    
    With the tree modification log, it's possible to reconstruct a consistent
    old state of the tree. This is required to do backref walking on a busy
    file system.
    
    Signed-off-by: Jan Schmidt <list.btrfs@jan-o-sch.net>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 56485b3b7c31..72b9f97e2fdc 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -18,6 +18,7 @@
 
 #include <linux/sched.h>
 #include <linux/slab.h>
+#include <linux/rbtree.h>
 #include "ctree.h"
 #include "disk-io.h"
 #include "transaction.h"
@@ -288,6 +289,412 @@ int btrfs_copy_root(struct btrfs_trans_handle *trans,
 	return 0;
 }
 
+enum mod_log_op {
+	MOD_LOG_KEY_REPLACE,
+	MOD_LOG_KEY_ADD,
+	MOD_LOG_KEY_REMOVE,
+	MOD_LOG_KEY_REMOVE_WHILE_FREEING,
+	MOD_LOG_KEY_REMOVE_WHILE_MOVING,
+	MOD_LOG_MOVE_KEYS,
+	MOD_LOG_ROOT_REPLACE,
+};
+
+struct tree_mod_move {
+	int dst_slot;
+	int nr_items;
+};
+
+struct tree_mod_root {
+	u64 logical;
+	u8 level;
+};
+
+struct tree_mod_elem {
+	struct rb_node node;
+	u64 index;		/* shifted logical */
+	struct seq_list elem;
+	enum mod_log_op op;
+
+	/* this is used for MOD_LOG_KEY_* and MOD_LOG_MOVE_KEYS operations */
+	int slot;
+
+	/* this is used for MOD_LOG_KEY* and MOD_LOG_ROOT_REPLACE */
+	u64 generation;
+
+	/* those are used for op == MOD_LOG_KEY_{REPLACE,REMOVE} */
+	struct btrfs_disk_key key;
+	u64 blockptr;
+
+	/* this is used for op == MOD_LOG_MOVE_KEYS */
+	struct tree_mod_move move;
+
+	/* this is used for op == MOD_LOG_ROOT_REPLACE */
+	struct tree_mod_root old_root;
+};
+
+static inline void
+__get_tree_mod_seq(struct btrfs_fs_info *fs_info, struct seq_list *elem)
+{
+	elem->seq = atomic_inc_return(&fs_info->tree_mod_seq);
+	list_add_tail(&elem->list, &fs_info->tree_mod_seq_list);
+}
+
+void btrfs_get_tree_mod_seq(struct btrfs_fs_info *fs_info,
+			    struct seq_list *elem)
+{
+	elem->flags = 1;
+	spin_lock(&fs_info->tree_mod_seq_lock);
+	__get_tree_mod_seq(fs_info, elem);
+	spin_unlock(&fs_info->tree_mod_seq_lock);
+}
+
+void btrfs_put_tree_mod_seq(struct btrfs_fs_info *fs_info,
+			    struct seq_list *elem)
+{
+	struct rb_root *tm_root;
+	struct rb_node *node;
+	struct rb_node *next;
+	struct seq_list *cur_elem;
+	struct tree_mod_elem *tm;
+	u64 min_seq = (u64)-1;
+	u64 seq_putting = elem->seq;
+
+	if (!seq_putting)
+		return;
+
+	BUG_ON(!(elem->flags & 1));
+	spin_lock(&fs_info->tree_mod_seq_lock);
+	list_del(&elem->list);
+
+	list_for_each_entry(cur_elem, &fs_info->tree_mod_seq_list, list) {
+		if ((cur_elem->flags & 1) && cur_elem->seq < min_seq) {
+			if (seq_putting > cur_elem->seq) {
+				/*
+				 * blocker with lower sequence number exists, we
+				 * cannot remove anything from the log
+				 */
+				goto out;
+			}
+			min_seq = cur_elem->seq;
+		}
+	}
+
+	/*
+	 * anything that's lower than the lowest existing (read: blocked)
+	 * sequence number can be removed from the tree.
+	 */
+	write_lock(&fs_info->tree_mod_log_lock);
+	tm_root = &fs_info->tree_mod_log;
+	for (node = rb_first(tm_root); node; node = next) {
+		next = rb_next(node);
+		tm = container_of(node, struct tree_mod_elem, node);
+		if (tm->elem.seq > min_seq)
+			continue;
+		rb_erase(node, tm_root);
+		list_del(&tm->elem.list);
+		kfree(tm);
+	}
+	write_unlock(&fs_info->tree_mod_log_lock);
+out:
+	spin_unlock(&fs_info->tree_mod_seq_lock);
+}
+
+/*
+ * key order of the log:
+ *       index -> sequence
+ *
+ * the index is the shifted logical of the *new* root node for root replace
+ * operations, or the shifted logical of the affected block for all other
+ * operations.
+ */
+static noinline int
+__tree_mod_log_insert(struct btrfs_fs_info *fs_info, struct tree_mod_elem *tm)
+{
+	struct rb_root *tm_root;
+	struct rb_node **new;
+	struct rb_node *parent = NULL;
+	struct tree_mod_elem *cur;
+	int ret = 0;
+
+	BUG_ON(!tm || !tm->elem.seq);
+
+	write_lock(&fs_info->tree_mod_log_lock);
+	tm_root = &fs_info->tree_mod_log;
+	new = &tm_root->rb_node;
+	while (*new) {
+		cur = container_of(*new, struct tree_mod_elem, node);
+		parent = *new;
+		if (cur->index < tm->index)
+			new = &((*new)->rb_left);
+		else if (cur->index > tm->index)
+			new = &((*new)->rb_right);
+		else if (cur->elem.seq < tm->elem.seq)
+			new = &((*new)->rb_left);
+		else if (cur->elem.seq > tm->elem.seq)
+			new = &((*new)->rb_right);
+		else {
+			kfree(tm);
+			ret = -EEXIST;
+			goto unlock;
+		}
+	}
+
+	rb_link_node(&tm->node, parent, new);
+	rb_insert_color(&tm->node, tm_root);
+unlock:
+	write_unlock(&fs_info->tree_mod_log_lock);
+	return ret;
+}
+
+int tree_mod_alloc(struct btrfs_fs_info *fs_info, gfp_t flags,
+		   struct tree_mod_elem **tm_ret)
+{
+	struct tree_mod_elem *tm;
+	u64 seq = 0;
+
+	smp_mb();
+	if (list_empty(&fs_info->tree_mod_seq_list))
+		return 0;
+
+	tm = *tm_ret = kzalloc(sizeof(*tm), flags);
+	if (!tm)
+		return -ENOMEM;
+
+	__get_tree_mod_seq(fs_info, &tm->elem);
+	seq = tm->elem.seq;
+	tm->elem.flags = 0;
+
+	return seq;
+}
+
+static noinline int
+tree_mod_log_insert_key_mask(struct btrfs_fs_info *fs_info,
+			     struct extent_buffer *eb, int slot,
+			     enum mod_log_op op, gfp_t flags)
+{
+	struct tree_mod_elem *tm;
+	int ret;
+
+	ret = tree_mod_alloc(fs_info, flags, &tm);
+	if (ret <= 0)
+		return ret;
+
+	tm->index = eb->start >> PAGE_CACHE_SHIFT;
+	if (op != MOD_LOG_KEY_ADD) {
+		btrfs_node_key(eb, &tm->key, slot);
+		tm->blockptr = btrfs_node_blockptr(eb, slot);
+	}
+	tm->op = op;
+	tm->slot = slot;
+	tm->generation = btrfs_node_ptr_generation(eb, slot);
+
+	return __tree_mod_log_insert(fs_info, tm);
+}
+
+static noinline int
+tree_mod_log_insert_key(struct btrfs_fs_info *fs_info, struct extent_buffer *eb,
+			int slot, enum mod_log_op op)
+{
+	return tree_mod_log_insert_key_mask(fs_info, eb, slot, op, GFP_NOFS);
+}
+
+static noinline int
+tree_mod_log_insert_move(struct btrfs_fs_info *fs_info,
+			 struct extent_buffer *eb, int dst_slot, int src_slot,
+			 int nr_items, gfp_t flags)
+{
+	struct tree_mod_elem *tm;
+	int ret;
+	int i;
+
+	ret = tree_mod_alloc(fs_info, flags, &tm);
+	if (ret <= 0)
+		return ret;
+
+	for (i = 0; i + dst_slot < src_slot && i < nr_items; i++) {
+		ret = tree_mod_log_insert_key(fs_info, eb, i + dst_slot,
+					      MOD_LOG_KEY_REMOVE_WHILE_MOVING);
+		BUG_ON(ret < 0);
+	}
+
+	tm->index = eb->start >> PAGE_CACHE_SHIFT;
+	tm->slot = src_slot;
+	tm->move.dst_slot = dst_slot;
+	tm->move.nr_items = nr_items;
+	tm->op = MOD_LOG_MOVE_KEYS;
+
+	return __tree_mod_log_insert(fs_info, tm);
+}
+
+static noinline int
+tree_mod_log_insert_root(struct btrfs_fs_info *fs_info,
+			 struct extent_buffer *old_root,
+			 struct extent_buffer *new_root, gfp_t flags)
+{
+	struct tree_mod_elem *tm;
+	int ret;
+
+	ret = tree_mod_alloc(fs_info, flags, &tm);
+	if (ret <= 0)
+		return ret;
+
+	tm->index = new_root->start >> PAGE_CACHE_SHIFT;
+	tm->old_root.logical = old_root->start;
+	tm->old_root.level = btrfs_header_level(old_root);
+	tm->generation = btrfs_header_generation(old_root);
+	tm->op = MOD_LOG_ROOT_REPLACE;
+
+	return __tree_mod_log_insert(fs_info, tm);
+}
+
+static struct tree_mod_elem *
+__tree_mod_log_search(struct btrfs_fs_info *fs_info, u64 start, u64 min_seq,
+		      int smallest)
+{
+	struct rb_root *tm_root;
+	struct rb_node *node;
+	struct tree_mod_elem *cur = NULL;
+	struct tree_mod_elem *found = NULL;
+	u64 index = start >> PAGE_CACHE_SHIFT;
+
+	read_lock(&fs_info->tree_mod_log_lock);
+	tm_root = &fs_info->tree_mod_log;
+	node = tm_root->rb_node;
+	while (node) {
+		cur = container_of(node, struct tree_mod_elem, node);
+		if (cur->index < index) {
+			node = node->rb_left;
+		} else if (cur->index > index) {
+			node = node->rb_right;
+		} else if (cur->elem.seq < min_seq) {
+			node = node->rb_left;
+		} else if (!smallest) {
+			/* we want the node with the highest seq */
+			if (found)
+				BUG_ON(found->elem.seq > cur->elem.seq);
+			found = cur;
+			node = node->rb_left;
+		} else if (cur->elem.seq > min_seq) {
+			/* we want the node with the smallest seq */
+			if (found)
+				BUG_ON(found->elem.seq < cur->elem.seq);
+			found = cur;
+			node = node->rb_right;
+		} else {
+			found = cur;
+			break;
+		}
+	}
+	read_unlock(&fs_info->tree_mod_log_lock);
+
+	return found;
+}
+
+/*
+ * this returns the element from the log with the smallest time sequence
+ * value that's in the log (the oldest log item). any element with a time
+ * sequence lower than min_seq will be ignored.
+ */
+static struct tree_mod_elem *
+tree_mod_log_search_oldest(struct btrfs_fs_info *fs_info, u64 start,
+			   u64 min_seq)
+{
+	return __tree_mod_log_search(fs_info, start, min_seq, 1);
+}
+
+/*
+ * this returns the element from the log with the largest time sequence
+ * value that's in the log (the most recent log item). any element with
+ * a time sequence lower than min_seq will be ignored.
+ */
+static struct tree_mod_elem *
+tree_mod_log_search(struct btrfs_fs_info *fs_info, u64 start, u64 min_seq)
+{
+	return __tree_mod_log_search(fs_info, start, min_seq, 0);
+}
+
+static inline void
+tree_mod_log_eb_copy(struct btrfs_fs_info *fs_info, struct extent_buffer *dst,
+		     struct extent_buffer *src, unsigned long dst_offset,
+		     unsigned long src_offset, int nr_items)
+{
+	int ret;
+	int i;
+
+	smp_mb();
+	if (list_empty(&fs_info->tree_mod_seq_list))
+		return;
+
+	if (btrfs_header_level(dst) == 0 && btrfs_header_level(src) == 0)
+		return;
+
+	/* speed this up by single seq for all operations? */
+	for (i = 0; i < nr_items; i++) {
+		ret = tree_mod_log_insert_key(fs_info, src, i + src_offset,
+					      MOD_LOG_KEY_REMOVE);
+		BUG_ON(ret < 0);
+		ret = tree_mod_log_insert_key(fs_info, dst, i + dst_offset,
+					      MOD_LOG_KEY_ADD);
+		BUG_ON(ret < 0);
+	}
+}
+
+static inline void
+tree_mod_log_eb_move(struct btrfs_fs_info *fs_info, struct extent_buffer *dst,
+		     int dst_offset, int src_offset, int nr_items)
+{
+	int ret;
+	ret = tree_mod_log_insert_move(fs_info, dst, dst_offset, src_offset,
+				       nr_items, GFP_NOFS);
+	BUG_ON(ret < 0);
+}
+
+static inline void
+tree_mod_log_set_node_key(struct btrfs_fs_info *fs_info,
+			  struct extent_buffer *eb,
+			  struct btrfs_disk_key *disk_key, int slot, int atomic)
+{
+	int ret;
+
+	ret = tree_mod_log_insert_key_mask(fs_info, eb, slot,
+					   MOD_LOG_KEY_REPLACE,
+					   atomic ? GFP_ATOMIC : GFP_NOFS);
+	BUG_ON(ret < 0);
+}
+
+static void tree_mod_log_free_eb(struct btrfs_fs_info *fs_info,
+				 struct extent_buffer *eb)
+{
+	int i;
+	int ret;
+	u32 nritems;
+
+	smp_mb();
+	if (list_empty(&fs_info->tree_mod_seq_list))
+		return;
+
+	if (btrfs_header_level(eb) == 0)
+		return;
+
+	nritems = btrfs_header_nritems(eb);
+	for (i = nritems - 1; i >= 0; i--) {
+		ret = tree_mod_log_insert_key(fs_info, eb, i,
+					      MOD_LOG_KEY_REMOVE_WHILE_FREEING);
+		BUG_ON(ret < 0);
+	}
+}
+
+static inline void
+tree_mod_log_set_root_pointer(struct btrfs_root *root,
+			      struct extent_buffer *new_root_node)
+{
+	int ret;
+	tree_mod_log_free_eb(root->fs_info, root->node);
+	ret = tree_mod_log_insert_root(root->fs_info, root->node,
+				       new_root_node, GFP_NOFS);
+	BUG_ON(ret < 0);
+}
+
 /*
  * check if the tree block can be shared by multiple trees
  */
@@ -2271,7 +2678,6 @@ static noinline int split_node(struct btrfs_trans_handle *trans,
 			    (unsigned long)btrfs_header_chunk_tree_uuid(split),
 			    BTRFS_UUID_SIZE);
 
-
 	copy_extent_buffer(split, c,
 			   btrfs_node_key_ptr_offset(0),
 			   btrfs_node_key_ptr_offset(mid),

commit 5581a51a59a1f5f51ac3d4bacafb738d35e0350b
Author: Jan Schmidt <list.btrfs@jan-o-sch.net>
Date:   Wed May 16 17:04:52 2012 +0200

    Btrfs: don't set for_cow parameter for tree block functions
    
    Three callers of btrfs_free_tree_block or btrfs_alloc_tree_block passed
    parameter for_cow = 1. In fact, these two functions should never mark
    their tree modification operations as for_cow, because they can change
    the number of blocks referenced by a tree.
    
    Hence, we remove the extra for_cow parameter from these functions and
    make them pass a zero down.
    
    Signed-off-by: Jan Schmidt <list.btrfs@jan-o-sch.net>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 4106264fbc65..56485b3b7c31 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -255,7 +255,7 @@ int btrfs_copy_root(struct btrfs_trans_handle *trans,
 
 	cow = btrfs_alloc_free_block(trans, root, buf->len, 0,
 				     new_root_objectid, &disk_key, level,
-				     buf->start, 0, 1);
+				     buf->start, 0);
 	if (IS_ERR(cow))
 		return PTR_ERR(cow);
 
@@ -467,7 +467,7 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 
 	cow = btrfs_alloc_free_block(trans, root, buf->len, parent_start,
 				     root->root_key.objectid, &disk_key,
-				     level, search_start, empty_size, 1);
+				     level, search_start, empty_size);
 	if (IS_ERR(cow))
 		return PTR_ERR(cow);
 
@@ -509,7 +509,7 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 		rcu_assign_pointer(root->node, cow);
 
 		btrfs_free_tree_block(trans, root, buf, parent_start,
-				      last_ref, 1);
+				      last_ref);
 		free_extent_buffer(buf);
 		add_root_to_dirty_list(root);
 	} else {
@@ -525,7 +525,7 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 					      trans->transid);
 		btrfs_mark_buffer_dirty(parent);
 		btrfs_free_tree_block(trans, root, buf, parent_start,
-				      last_ref, 1);
+				      last_ref);
 	}
 	if (unlock_orig)
 		btrfs_tree_unlock(buf);
@@ -987,7 +987,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		free_extent_buffer(mid);
 
 		root_sub_used(root, mid->len);
-		btrfs_free_tree_block(trans, root, mid, 0, 1, 0);
+		btrfs_free_tree_block(trans, root, mid, 0, 1);
 		/* once for the root ptr */
 		free_extent_buffer_stale(mid);
 		return 0;
@@ -1042,7 +1042,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 			btrfs_tree_unlock(right);
 			del_ptr(trans, root, path, level + 1, pslot + 1);
 			root_sub_used(root, right->len);
-			btrfs_free_tree_block(trans, root, right, 0, 1, 0);
+			btrfs_free_tree_block(trans, root, right, 0, 1);
 			free_extent_buffer_stale(right);
 			right = NULL;
 		} else {
@@ -1084,7 +1084,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		btrfs_tree_unlock(mid);
 		del_ptr(trans, root, path, level + 1, pslot);
 		root_sub_used(root, mid->len);
-		btrfs_free_tree_block(trans, root, mid, 0, 1, 0);
+		btrfs_free_tree_block(trans, root, mid, 0, 1);
 		free_extent_buffer_stale(mid);
 		mid = NULL;
 	} else {
@@ -2129,7 +2129,7 @@ static noinline int insert_new_root(struct btrfs_trans_handle *trans,
 
 	c = btrfs_alloc_free_block(trans, root, root->nodesize, 0,
 				   root->root_key.objectid, &lower_key,
-				   level, root->node->start, 0, 0);
+				   level, root->node->start, 0);
 	if (IS_ERR(c))
 		return PTR_ERR(c);
 
@@ -2252,7 +2252,7 @@ static noinline int split_node(struct btrfs_trans_handle *trans,
 
 	split = btrfs_alloc_free_block(trans, root, root->nodesize, 0,
 					root->root_key.objectid,
-					&disk_key, level, c->start, 0, 0);
+					&disk_key, level, c->start, 0);
 	if (IS_ERR(split))
 		return PTR_ERR(split);
 
@@ -3004,7 +3004,7 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 
 	right = btrfs_alloc_free_block(trans, root, root->leafsize, 0,
 					root->root_key.objectid,
-					&disk_key, 0, l->start, 0, 0);
+					&disk_key, 0, l->start, 0);
 	if (IS_ERR(right))
 		return PTR_ERR(right);
 
@@ -3804,7 +3804,7 @@ static noinline void btrfs_del_leaf(struct btrfs_trans_handle *trans,
 	root_sub_used(root, leaf->len);
 
 	extent_buffer_get(leaf);
-	btrfs_free_tree_block(trans, root, leaf, 0, 1, 0);
+	btrfs_free_tree_block(trans, root, leaf, 0, 1);
 	free_extent_buffer_stale(leaf);
 }
 /*

commit f775738f6fba9c7f6deaa540860d6fb7e2d28445
Author: Wang Sheng-Hui <shhuiw@gmail.com>
Date:   Fri Mar 30 15:14:27 2012 +0800

    btrfs/ctree.c: remove the unnecessary 'return -1;' at the end of bin_search
    
    The code path should not reach there. Remove it.
    
    Signed-off-by: Wang Sheng-Hui <shhuiw@gmail.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 4106264fbc65..26847999c649 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -854,20 +854,18 @@ static noinline int generic_bin_search(struct extent_buffer *eb,
 static int bin_search(struct extent_buffer *eb, struct btrfs_key *key,
 		      int level, int *slot)
 {
-	if (level == 0) {
+	if (level == 0)
 		return generic_bin_search(eb,
 					  offsetof(struct btrfs_leaf, items),
 					  sizeof(struct btrfs_item),
 					  key, btrfs_header_nritems(eb),
 					  slot);
-	} else {
+	else
 		return generic_bin_search(eb,
 					  offsetof(struct btrfs_node, ptrs),
 					  sizeof(struct btrfs_key_ptr),
 					  key, btrfs_header_nritems(eb),
 					  slot);
-	}
-	return -1;
 }
 
 int btrfs_bin_search(struct extent_buffer *eb, struct btrfs_key *key,

commit b9fab919b748c7b39c19ff236ed6c5682c266dde
Author: Chris Mason <chris.mason@oracle.com>
Date:   Sun May 6 07:23:47 2012 -0400

    Btrfs: avoid sleeping in verify_parent_transid while atomic
    
    verify_parent_transid needs to lock the extent range to make
    sure no IO is underway, and so it can safely clear the
    uptodate bits if our checks fail.
    
    But, a few callers are using it with spinlocks held.  Most
    of the time, the generation numbers are going to match, and
    we don't want to switch to a blocking lock just for the error
    case.  This adds an atomic flag to verify_parent_transid,
    and changes it to return EAGAIN if it needs to block to
    properly verifiy things.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 086303b9be64..4106264fbc65 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -725,7 +725,7 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 
 		cur = btrfs_find_tree_block(root, blocknr, blocksize);
 		if (cur)
-			uptodate = btrfs_buffer_uptodate(cur, gen);
+			uptodate = btrfs_buffer_uptodate(cur, gen, 0);
 		else
 			uptodate = 0;
 		if (!cur || !uptodate) {
@@ -1360,7 +1360,12 @@ static noinline int reada_for_balance(struct btrfs_root *root,
 		block1 = btrfs_node_blockptr(parent, slot - 1);
 		gen = btrfs_node_ptr_generation(parent, slot - 1);
 		eb = btrfs_find_tree_block(root, block1, blocksize);
-		if (eb && btrfs_buffer_uptodate(eb, gen))
+		/*
+		 * if we get -eagain from btrfs_buffer_uptodate, we
+		 * don't want to return eagain here.  That will loop
+		 * forever
+		 */
+		if (eb && btrfs_buffer_uptodate(eb, gen, 1) != 0)
 			block1 = 0;
 		free_extent_buffer(eb);
 	}
@@ -1368,7 +1373,7 @@ static noinline int reada_for_balance(struct btrfs_root *root,
 		block2 = btrfs_node_blockptr(parent, slot + 1);
 		gen = btrfs_node_ptr_generation(parent, slot + 1);
 		eb = btrfs_find_tree_block(root, block2, blocksize);
-		if (eb && btrfs_buffer_uptodate(eb, gen))
+		if (eb && btrfs_buffer_uptodate(eb, gen, 1) != 0)
 			block2 = 0;
 		free_extent_buffer(eb);
 	}
@@ -1506,8 +1511,9 @@ read_block_for_search(struct btrfs_trans_handle *trans,
 
 	tmp = btrfs_find_tree_block(root, blocknr, blocksize);
 	if (tmp) {
-		if (btrfs_buffer_uptodate(tmp, 0)) {
-			if (btrfs_buffer_uptodate(tmp, gen)) {
+		/* first we do an atomic uptodate check */
+		if (btrfs_buffer_uptodate(tmp, 0, 1) > 0) {
+			if (btrfs_buffer_uptodate(tmp, gen, 1) > 0) {
 				/*
 				 * we found an up to date block without
 				 * sleeping, return
@@ -1525,8 +1531,9 @@ read_block_for_search(struct btrfs_trans_handle *trans,
 			free_extent_buffer(tmp);
 			btrfs_set_path_blocking(p);
 
+			/* now we're allowed to do a blocking uptodate check */
 			tmp = read_tree_block(root, blocknr, blocksize, gen);
-			if (tmp && btrfs_buffer_uptodate(tmp, gen)) {
+			if (tmp && btrfs_buffer_uptodate(tmp, gen, 0) > 0) {
 				*eb_ret = tmp;
 				return 0;
 			}
@@ -1561,7 +1568,7 @@ read_block_for_search(struct btrfs_trans_handle *trans,
 		 * and give up so that our caller doesn't loop forever
 		 * on our EAGAINs.
 		 */
-		if (!btrfs_buffer_uptodate(tmp, 0))
+		if (!btrfs_buffer_uptodate(tmp, 0, 0))
 			ret = -EIO;
 		free_extent_buffer(tmp);
 	}
@@ -4045,7 +4052,7 @@ int btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,
 			tmp = btrfs_find_tree_block(root, blockptr,
 					    btrfs_level_size(root, level - 1));
 
-			if (tmp && btrfs_buffer_uptodate(tmp, gen)) {
+			if (tmp && btrfs_buffer_uptodate(tmp, gen, 1) > 0) {
 				free_extent_buffer(tmp);
 				break;
 			}
@@ -4168,7 +4175,8 @@ int btrfs_find_next_key(struct btrfs_root *root, struct btrfs_path *path,
 				struct extent_buffer *cur;
 				cur = btrfs_find_tree_block(root, blockptr,
 					    btrfs_level_size(root, level - 1));
-				if (!cur || !btrfs_buffer_uptodate(cur, gen)) {
+				if (!cur ||
+				    btrfs_buffer_uptodate(cur, gen, 1) <= 0) {
 					slot++;
 					if (cur)
 						free_extent_buffer(cur);

commit e5846fc665d1c3dd32d877febe7402ccd583b8a1
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu May 3 12:08:48 2012 -0400

    Btrfs: Add properly locking around add_root_to_dirty_list
    
    add_root_to_dirty_list happens once at the very beginning of the
    transaction, but it is still racey.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index e801f226d7e0..086303b9be64 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -220,10 +220,12 @@ struct extent_buffer *btrfs_read_lock_root_node(struct btrfs_root *root)
  */
 static void add_root_to_dirty_list(struct btrfs_root *root)
 {
+	spin_lock(&root->fs_info->trans_lock);
 	if (root->track_dirty && list_empty(&root->dirty_list)) {
 		list_add(&root->dirty_list,
 			 &root->fs_info->dirty_cowonly_roots);
 	}
+	spin_unlock(&root->fs_info->trans_lock);
 }
 
 /*

commit 1d4284bd6e8d7dd1d5521a6747bdb6dc1caf0225
Merge: b5d67f64f9bc 65139ed99234
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Mar 28 20:31:37 2012 -0400

    Merge branch 'error-handling' into for-linus
    
    Conflicts:
            fs/btrfs/ctree.c
            fs/btrfs/disk-io.c
            fs/btrfs/extent-tree.c
            fs/btrfs/extent_io.c
            fs/btrfs/extent_io.h
            fs/btrfs/inode.c
            fs/btrfs/scrub.c
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

commit f7c79f30cb2d3883488e70cafc9e3a7edd4b9fdb
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Mar 19 15:54:38 2012 -0400

    Btrfs: adjust the write_lock_level as we unlock
    
    btrfs_search_slot sometimes needs write locks on high levels of
    the tree.  It remembers the highest level that needs a write lock
    and will use that for all future searches through the tree in a given
    call.
    
    But, very often we'll just cow the top level or the level below and we
    won't really need write locks on the root again after that.  This patch
    changes things to adjust the write lock requirement as it unlocks
    levels.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 8af24da983b1..270655da11d1 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1395,7 +1395,8 @@ static noinline int reada_for_balance(struct btrfs_root *root,
  * if lowest_unlock is 1, level 0 won't be unlocked
  */
 static noinline void unlock_up(struct btrfs_path *path, int level,
-			       int lowest_unlock)
+			       int lowest_unlock, int min_write_lock_level,
+			       int *write_lock_level)
 {
 	int i;
 	int skip_level = level;
@@ -1427,6 +1428,11 @@ static noinline void unlock_up(struct btrfs_path *path, int level,
 		if (i >= lowest_unlock && i > skip_level && path->locks[i]) {
 			btrfs_tree_unlock_rw(t, path->locks[i]);
 			path->locks[i] = 0;
+			if (write_lock_level &&
+			    i > min_write_lock_level &&
+			    i <= *write_lock_level) {
+				*write_lock_level = i - 1;
+			}
 		}
 	}
 }
@@ -1650,6 +1656,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 	/* everything at write_lock_level or lower must be write locked */
 	int write_lock_level = 0;
 	u8 lowest_level = 0;
+	int min_write_lock_level;
 
 	lowest_level = p->lowest_level;
 	WARN_ON(lowest_level && ins_len > 0);
@@ -1677,6 +1684,8 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 	if (cow && (p->keep_locks || p->lowest_level))
 		write_lock_level = BTRFS_MAX_LEVEL;
 
+	min_write_lock_level = write_lock_level;
+
 again:
 	/*
 	 * we try very hard to do read locks on the root
@@ -1808,7 +1817,8 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 				goto again;
 			}
 
-			unlock_up(p, level, lowest_unlock);
+			unlock_up(p, level, lowest_unlock,
+				  min_write_lock_level, &write_lock_level);
 
 			if (level == lowest_level) {
 				if (dec)
@@ -1870,7 +1880,8 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 				}
 			}
 			if (!p->search_for_split)
-				unlock_up(p, level, lowest_unlock);
+				unlock_up(p, level, lowest_unlock,
+					  min_write_lock_level, &write_lock_level);
 			goto done;
 		}
 	}
@@ -4108,7 +4119,7 @@ int btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,
 		path->slots[level] = slot;
 		if (level == path->lowest_level) {
 			ret = 0;
-			unlock_up(path, level, 1);
+			unlock_up(path, level, 1, 0, NULL);
 			goto out;
 		}
 		btrfs_set_path_blocking(path);
@@ -4119,7 +4130,7 @@ int btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,
 
 		path->locks[level - 1] = BTRFS_READ_LOCK;
 		path->nodes[level - 1] = cur;
-		unlock_up(path, level, 1);
+		unlock_up(path, level, 1, 0, NULL);
 		btrfs_clear_path_blocking(path, NULL, 0);
 	}
 out:
@@ -4355,7 +4366,7 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 	}
 	ret = 0;
 done:
-	unlock_up(path, 0, 1);
+	unlock_up(path, 0, 1, 0, NULL);
 	path->leave_spinning = old_spinning;
 	if (!old_spinning)
 		btrfs_set_path_blocking(path);

commit cfed81a04eb555f5606d1b6a54bdbabab0ee1ac3
Author: Chris Mason <chris.mason@oracle.com>
Date:   Sat Mar 3 07:40:03 2012 -0500

    Btrfs: add the ability to cache a pointer into the eb
    
    This cuts down on the CPU time used by map_private_extent_buffer
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 74c03fb0ca1d..8af24da983b1 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2333,6 +2333,7 @@ static noinline int __push_leaf_right(struct btrfs_trans_handle *trans,
 {
 	struct extent_buffer *left = path->nodes[0];
 	struct extent_buffer *upper = path->nodes[1];
+	struct btrfs_map_token token;
 	struct btrfs_disk_key disk_key;
 	int slot;
 	u32 i;
@@ -2344,6 +2345,8 @@ static noinline int __push_leaf_right(struct btrfs_trans_handle *trans,
 	u32 data_end;
 	u32 this_item_size;
 
+	btrfs_init_map_token(&token);
+
 	if (empty)
 		nr = 0;
 	else
@@ -2421,8 +2424,8 @@ static noinline int __push_leaf_right(struct btrfs_trans_handle *trans,
 	push_space = BTRFS_LEAF_DATA_SIZE(root);
 	for (i = 0; i < right_nritems; i++) {
 		item = btrfs_item_nr(right, i);
-		push_space -= btrfs_item_size(right, item);
-		btrfs_set_item_offset(right, item, push_space);
+		push_space -= btrfs_token_item_size(right, item, &token);
+		btrfs_set_token_item_offset(right, item, push_space, &token);
 	}
 
 	left_nritems -= push_items;
@@ -2553,6 +2556,9 @@ static noinline int __push_leaf_left(struct btrfs_trans_handle *trans,
 	int wret;
 	u32 this_item_size;
 	u32 old_left_item_size;
+	struct btrfs_map_token token;
+
+	btrfs_init_map_token(&token);
 
 	if (empty)
 		nr = min(right_nritems, max_slot);
@@ -2613,9 +2619,10 @@ static noinline int __push_leaf_left(struct btrfs_trans_handle *trans,
 
 		item = btrfs_item_nr(left, i);
 
-		ioff = btrfs_item_offset(left, item);
-		btrfs_set_item_offset(left, item,
-		      ioff - (BTRFS_LEAF_DATA_SIZE(root) - old_left_item_size));
+		ioff = btrfs_token_item_offset(left, item, &token);
+		btrfs_set_token_item_offset(left, item,
+		      ioff - (BTRFS_LEAF_DATA_SIZE(root) - old_left_item_size),
+		      &token);
 	}
 	btrfs_set_header_nritems(left, old_left_nritems + push_items);
 
@@ -2645,8 +2652,9 @@ static noinline int __push_leaf_left(struct btrfs_trans_handle *trans,
 	for (i = 0; i < right_nritems; i++) {
 		item = btrfs_item_nr(right, i);
 
-		push_space = push_space - btrfs_item_size(right, item);
-		btrfs_set_item_offset(right, item, push_space);
+		push_space = push_space - btrfs_token_item_size(right,
+								item, &token);
+		btrfs_set_token_item_offset(right, item, push_space, &token);
 	}
 
 	btrfs_mark_buffer_dirty(left);
@@ -2767,6 +2775,9 @@ static noinline int copy_for_split(struct btrfs_trans_handle *trans,
 	int ret = 0;
 	int wret;
 	struct btrfs_disk_key disk_key;
+	struct btrfs_map_token token;
+
+	btrfs_init_map_token(&token);
 
 	nritems = nritems - mid;
 	btrfs_set_header_nritems(right, nritems);
@@ -2788,8 +2799,9 @@ static noinline int copy_for_split(struct btrfs_trans_handle *trans,
 		struct btrfs_item *item = btrfs_item_nr(right, i);
 		u32 ioff;
 
-		ioff = btrfs_item_offset(right, item);
-		btrfs_set_item_offset(right, item, ioff + rt_data_off);
+		ioff = btrfs_token_item_offset(right, item, &token);
+		btrfs_set_token_item_offset(right, item,
+					    ioff + rt_data_off, &token);
 	}
 
 	btrfs_set_header_nritems(l, mid);
@@ -3284,6 +3296,9 @@ int btrfs_truncate_item(struct btrfs_trans_handle *trans,
 	unsigned int old_size;
 	unsigned int size_diff;
 	int i;
+	struct btrfs_map_token token;
+
+	btrfs_init_map_token(&token);
 
 	leaf = path->nodes[0];
 	slot = path->slots[0];
@@ -3310,8 +3325,9 @@ int btrfs_truncate_item(struct btrfs_trans_handle *trans,
 		u32 ioff;
 		item = btrfs_item_nr(leaf, i);
 
-		ioff = btrfs_item_offset(leaf, item);
-		btrfs_set_item_offset(leaf, item, ioff + size_diff);
+		ioff = btrfs_token_item_offset(leaf, item, &token);
+		btrfs_set_token_item_offset(leaf, item,
+					    ioff + size_diff, &token);
 	}
 
 	/* shift the data */
@@ -3381,6 +3397,9 @@ int btrfs_extend_item(struct btrfs_trans_handle *trans,
 	unsigned int old_data;
 	unsigned int old_size;
 	int i;
+	struct btrfs_map_token token;
+
+	btrfs_init_map_token(&token);
 
 	leaf = path->nodes[0];
 
@@ -3410,8 +3429,9 @@ int btrfs_extend_item(struct btrfs_trans_handle *trans,
 		u32 ioff;
 		item = btrfs_item_nr(leaf, i);
 
-		ioff = btrfs_item_offset(leaf, item);
-		btrfs_set_item_offset(leaf, item, ioff - data_size);
+		ioff = btrfs_token_item_offset(leaf, item, &token);
+		btrfs_set_token_item_offset(leaf, item,
+					    ioff - data_size, &token);
 	}
 
 	/* shift the data */
@@ -3454,6 +3474,9 @@ int btrfs_insert_some_items(struct btrfs_trans_handle *trans,
 	unsigned int data_end;
 	struct btrfs_disk_key disk_key;
 	struct btrfs_key found_key;
+	struct btrfs_map_token token;
+
+	btrfs_init_map_token(&token);
 
 	for (i = 0; i < nr; i++) {
 		if (total_size + data_size[i] + sizeof(struct btrfs_item) >
@@ -3519,8 +3542,9 @@ int btrfs_insert_some_items(struct btrfs_trans_handle *trans,
 			u32 ioff;
 
 			item = btrfs_item_nr(leaf, i);
-			ioff = btrfs_item_offset(leaf, item);
-			btrfs_set_item_offset(leaf, item, ioff - total_data);
+			ioff = btrfs_token_item_offset(leaf, item, &token);
+			btrfs_set_token_item_offset(leaf, item,
+						    ioff - total_data, &token);
 		}
 		/* shift the items */
 		memmove_extent_buffer(leaf, btrfs_item_nr_offset(slot + nr),
@@ -3547,9 +3571,10 @@ int btrfs_insert_some_items(struct btrfs_trans_handle *trans,
 		btrfs_cpu_key_to_disk(&disk_key, cpu_key + i);
 		btrfs_set_item_key(leaf, &disk_key, slot + i);
 		item = btrfs_item_nr(leaf, slot + i);
-		btrfs_set_item_offset(leaf, item, data_end - data_size[i]);
+		btrfs_set_token_item_offset(leaf, item,
+					    data_end - data_size[i], &token);
 		data_end -= data_size[i];
-		btrfs_set_item_size(leaf, item, data_size[i]);
+		btrfs_set_token_item_size(leaf, item, data_size[i], &token);
 	}
 	btrfs_set_header_nritems(leaf, nritems + nr);
 	btrfs_mark_buffer_dirty(leaf);
@@ -3588,6 +3613,9 @@ int setup_items_for_insert(struct btrfs_trans_handle *trans,
 	int ret;
 	struct extent_buffer *leaf;
 	int slot;
+	struct btrfs_map_token token;
+
+	btrfs_init_map_token(&token);
 
 	leaf = path->nodes[0];
 	slot = path->slots[0];
@@ -3619,8 +3647,9 @@ int setup_items_for_insert(struct btrfs_trans_handle *trans,
 			u32 ioff;
 
 			item = btrfs_item_nr(leaf, i);
-			ioff = btrfs_item_offset(leaf, item);
-			btrfs_set_item_offset(leaf, item, ioff - total_data);
+			ioff = btrfs_token_item_offset(leaf, item, &token);
+			btrfs_set_token_item_offset(leaf, item,
+						    ioff - total_data, &token);
 		}
 		/* shift the items */
 		memmove_extent_buffer(leaf, btrfs_item_nr_offset(slot + nr),
@@ -3639,9 +3668,10 @@ int setup_items_for_insert(struct btrfs_trans_handle *trans,
 		btrfs_cpu_key_to_disk(&disk_key, cpu_key + i);
 		btrfs_set_item_key(leaf, &disk_key, slot + i);
 		item = btrfs_item_nr(leaf, slot + i);
-		btrfs_set_item_offset(leaf, item, data_end - data_size[i]);
+		btrfs_set_token_item_offset(leaf, item,
+					    data_end - data_size[i], &token);
 		data_end -= data_size[i];
-		btrfs_set_item_size(leaf, item, data_size[i]);
+		btrfs_set_token_item_size(leaf, item, data_size[i], &token);
 	}
 
 	btrfs_set_header_nritems(leaf, nritems + nr);
@@ -3814,6 +3844,9 @@ int btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 	int wret;
 	int i;
 	u32 nritems;
+	struct btrfs_map_token token;
+
+	btrfs_init_map_token(&token);
 
 	leaf = path->nodes[0];
 	last_off = btrfs_item_offset_nr(leaf, slot + nr - 1);
@@ -3835,8 +3868,9 @@ int btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 			u32 ioff;
 
 			item = btrfs_item_nr(leaf, i);
-			ioff = btrfs_item_offset(leaf, item);
-			btrfs_set_item_offset(leaf, item, ioff + dsize);
+			ioff = btrfs_token_item_offset(leaf, item, &token);
+			btrfs_set_token_item_offset(leaf, item,
+						    ioff + dsize, &token);
 		}
 
 		memmove_extent_buffer(leaf, btrfs_item_nr_offset(slot),

commit 3083ee2e18b701122a3b841db83448543a87a583
Author: Josef Bacik <josef@redhat.com>
Date:   Fri Mar 9 16:01:49 2012 -0500

    Btrfs: introduce free_extent_buffer_stale
    
    Because btrfs cow's we can end up with extent buffers that are no longer
    necessary just sitting around in memory.  So instead of evicting these pages, we
    could end up evicting things we actually care about.  Thus we have
    free_extent_buffer_stale for use when we are freeing tree blocks.  This will
    make it so that the ref for the eb being in the radix tree is dropped as soon as
    possible and then is freed when the refcount hits 0 instead of waiting to be
    released by releasepage.  Thanks,
    
    Signed-off-by: Josef Bacik <josef@redhat.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 0639a555e16e..74c03fb0ca1d 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -156,10 +156,23 @@ struct extent_buffer *btrfs_root_node(struct btrfs_root *root)
 {
 	struct extent_buffer *eb;
 
-	rcu_read_lock();
-	eb = rcu_dereference(root->node);
-	extent_buffer_get(eb);
-	rcu_read_unlock();
+	while (1) {
+		rcu_read_lock();
+		eb = rcu_dereference(root->node);
+
+		/*
+		 * RCU really hurts here, we could free up the root node because
+		 * it was cow'ed but we may not get the new root node yet so do
+		 * the inc_not_zero dance and if it doesn't work then
+		 * synchronize_rcu and try again.
+		 */
+		if (atomic_inc_not_zero(&eb->refs)) {
+			rcu_read_unlock();
+			break;
+		}
+		rcu_read_unlock();
+		synchronize_rcu();
+	}
 	return eb;
 }
 
@@ -504,7 +517,7 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 	}
 	if (unlock_orig)
 		btrfs_tree_unlock(buf);
-	free_extent_buffer(buf);
+	free_extent_buffer_stale(buf);
 	btrfs_mark_buffer_dirty(cow);
 	*cow_ret = cow;
 	return 0;
@@ -959,7 +972,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		root_sub_used(root, mid->len);
 		btrfs_free_tree_block(trans, root, mid, 0, 1, 0);
 		/* once for the root ptr */
-		free_extent_buffer(mid);
+		free_extent_buffer_stale(mid);
 		return 0;
 	}
 	if (btrfs_header_nritems(mid) >
@@ -1016,7 +1029,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 				ret = wret;
 			root_sub_used(root, right->len);
 			btrfs_free_tree_block(trans, root, right, 0, 1, 0);
-			free_extent_buffer(right);
+			free_extent_buffer_stale(right);
 			right = NULL;
 		} else {
 			struct btrfs_disk_key right_key;
@@ -1056,7 +1069,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 			ret = wret;
 		root_sub_used(root, mid->len);
 		btrfs_free_tree_block(trans, root, mid, 0, 1, 0);
-		free_extent_buffer(mid);
+		free_extent_buffer_stale(mid);
 		mid = NULL;
 	} else {
 		/* update the parent key to reflect our changes */
@@ -3781,7 +3794,9 @@ static noinline int btrfs_del_leaf(struct btrfs_trans_handle *trans,
 
 	root_sub_used(root, leaf->len);
 
+	extent_buffer_get(leaf);
 	btrfs_free_tree_block(trans, root, leaf, 0, 1, 0);
+	free_extent_buffer_stale(leaf);
 	return 0;
 }
 /*

commit 79787eaab46121d4713ed03c8fc63b9ec3eaec76
Author: Jeff Mahoney <jeffm@suse.com>
Date:   Mon Mar 12 16:03:00 2012 +0100

    btrfs: replace many BUG_ONs with proper error handling
    
     btrfs currently handles most errors with BUG_ON. This patch is a work-in-
     progress but aims to handle most errors other than internal logic
     errors and ENOMEM more gracefully.
    
     This iteration prevents most crashes but can run into lockups with
     the page lock on occasion when the timing "works out."
    
    Signed-off-by: Jeff Mahoney <jeffm@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 651a26a6c651..e697afd18159 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -356,14 +356,14 @@ static noinline int update_ref_for_cow(struct btrfs_trans_handle *trans,
 		     root->root_key.objectid == BTRFS_TREE_RELOC_OBJECTID) &&
 		    !(flags & BTRFS_BLOCK_FLAG_FULL_BACKREF)) {
 			ret = btrfs_inc_ref(trans, root, buf, 1, 1);
-			BUG_ON(ret);
+			BUG_ON(ret); /* -ENOMEM */
 
 			if (root->root_key.objectid ==
 			    BTRFS_TREE_RELOC_OBJECTID) {
 				ret = btrfs_dec_ref(trans, root, buf, 0, 1);
-				BUG_ON(ret);
+				BUG_ON(ret); /* -ENOMEM */
 				ret = btrfs_inc_ref(trans, root, cow, 1, 1);
-				BUG_ON(ret);
+				BUG_ON(ret); /* -ENOMEM */
 			}
 			new_flags |= BTRFS_BLOCK_FLAG_FULL_BACKREF;
 		} else {
@@ -373,7 +373,7 @@ static noinline int update_ref_for_cow(struct btrfs_trans_handle *trans,
 				ret = btrfs_inc_ref(trans, root, cow, 1, 1);
 			else
 				ret = btrfs_inc_ref(trans, root, cow, 0, 1);
-			BUG_ON(ret);
+			BUG_ON(ret); /* -ENOMEM */
 		}
 		if (new_flags != 0) {
 			ret = btrfs_set_disk_extent_flags(trans, root,
@@ -390,9 +390,9 @@ static noinline int update_ref_for_cow(struct btrfs_trans_handle *trans,
 				ret = btrfs_inc_ref(trans, root, cow, 1, 1);
 			else
 				ret = btrfs_inc_ref(trans, root, cow, 0, 1);
-			BUG_ON(ret);
+			BUG_ON(ret); /* -ENOMEM */
 			ret = btrfs_dec_ref(trans, root, buf, 1, 1);
-			BUG_ON(ret);
+			BUG_ON(ret); /* -ENOMEM */
 		}
 		clean_tree_block(trans, root, buf);
 		*last_ref = 1;
@@ -475,7 +475,7 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 
 	ret = update_ref_for_cow(trans, root, buf, cow, &last_ref);
 	if (ret) {
-		btrfs_std_error(root->fs_info, ret);
+		btrfs_abort_transaction(trans, root, ret);
 		return ret;
 	}
 
@@ -2713,7 +2713,8 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 			      path->nodes[1], slot - 1, &left);
 	if (ret) {
 		/* we hit -ENOSPC, but it isn't fatal here */
-		ret = 1;
+		if (ret == -ENOSPC)
+			ret = 1;
 		goto out;
 	}
 
@@ -4017,7 +4018,7 @@ int btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,
 		}
 		btrfs_set_path_blocking(path);
 		cur = read_node_slot(root, cur, slot);
-		BUG_ON(!cur);
+		BUG_ON(!cur); /* -ENOMEM */
 
 		btrfs_tree_read_lock(cur);
 

commit 305a26af5b2561a66859ef05ed7eb73d3c9f0913
Author: Mark Fasheh <mfasheh@suse.com>
Date:   Thu Sep 1 11:27:57 2011 -0700

    btrfs: Go readonly on tree errors in balance_level
    
    balace_level() seems to deal with missing tree nodes by BUG_ON(). Instead,
    we can easily just set the file system readonly and bubble -EROFS back up
    the stack.
    
    Signed-off-by: Mark Fasheh <mfasheh@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 36e16bd50798..651a26a6c651 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -944,7 +944,12 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 
 		/* promote the child to a root */
 		child = read_node_slot(root, mid, 0);
-		BUG_ON(!child);
+		if (!child) {
+			ret = -EROFS;
+			btrfs_std_error(root->fs_info, ret);
+			goto enospc;
+		}
+
 		btrfs_tree_lock(child);
 		btrfs_set_lock_blocking(child);
 		ret = btrfs_cow_block(trans, root, child, mid, 0, &child);
@@ -1042,7 +1047,11 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		 * otherwise we would have pulled some pointers from the
 		 * right
 		 */
-		BUG_ON(!left);
+		if (!left) {
+			ret = -EROFS;
+			btrfs_std_error(root->fs_info, ret);
+			goto enospc;
+		}
 		wret = balance_node_right(trans, root, mid, left);
 		if (wret < 0) {
 			ret = wret;

commit b68dc2a93e794c8507338c91577a277efa4555d5
Author: Mark Fasheh <mfasheh@suse.com>
Date:   Mon Aug 29 14:30:39 2011 -0700

    btrfs: Don't BUG_ON errors from update_ref_for_cow()
    
    __btrfs_cow_block(), the only caller of update_ref_for_cow() will BUG_ON()
    any error return.  Instead, we can go read-only fs as update_ref_for_cow()
    manipulates disk data in a way which doesn't look like it's easily rolled
    back.
    
    Signed-off-by: Mark Fasheh <mfasheh@suse.de>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 3b767d2b68e8..36e16bd50798 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -474,7 +474,10 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 			    BTRFS_FSID_SIZE);
 
 	ret = update_ref_for_cow(trans, root, buf, cow, &last_ref);
-	BUG_ON(ret);
+	if (ret) {
+		btrfs_std_error(root->fs_info, ret);
+		return ret;
+	}
 
 	if (root->ref_cows)
 		btrfs_reloc_cow_block(trans, root, buf, cow);

commit e5df957328b18baa731307c66cfe8e7a4981df65
Author: Mark Fasheh <mfasheh@suse.com>
Date:   Mon Aug 29 14:17:04 2011 -0700

    btrfs: Go readonly on bad extent refs in update_ref_for_cow()
    
    update_ref_for_cow() will BUG_ON() after it's call to
    btrfs_lookup_extent_info() if no existing references are found.  Since refs
    are computed directly from disk, this should be treated as a corruption
    instead of a logic error.
    
    Signed-off-by: Mark Fasheh <mfasheh@suse.de>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index e2e43c07f6b1..3b767d2b68e8 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -333,7 +333,11 @@ static noinline int update_ref_for_cow(struct btrfs_trans_handle *trans,
 					       buf->len, &refs, &flags);
 		if (ret)
 			return ret;
-		BUG_ON(refs == 0);
+		if (refs == 0) {
+			ret = -EROFS;
+			btrfs_std_error(root->fs_info, ret);
+			return ret;
+		}
 	} else {
 		refs = 1;
 		if (root->root_key.objectid == BTRFS_TREE_RELOC_OBJECTID ||

commit be1a5564fd39fa2ca6adbb41c75fb08f96a1ffcb
Author: Mark Fasheh <mfasheh@suse.com>
Date:   Mon Aug 8 13:20:18 2011 -0700

    btrfs: Don't BUG_ON() errors in update_ref_for_cow()
    
    The only caller of update_ref_for_cow() is __btrfs_cow_block() which was
    originally ignoring any return values. update_ref_for_cow() however doesn't
    look like a candidate to become a void function - there are a few places
    where errors can occur.
    
    So instead I changed update_ref_for_cow() to bubble all errors up (instead
    of BUG_ON). __btrfs_cow_block() was then updated to catch and BUG_ON() any
    errors from update_ref_for_cow(). The end effect is that we have no change
    in behavior, but about 8 different places where a BUG_ON(ret) was removed.
    
    Obviously a future patch will have to address the BUG_ON() in
    __btrfs_cow_block().
    
    Signed-off-by: Mark Fasheh <mfasheh@suse.de>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 9d472c2a8de8..e2e43c07f6b1 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -331,7 +331,8 @@ static noinline int update_ref_for_cow(struct btrfs_trans_handle *trans,
 	if (btrfs_block_can_be_shared(root, buf)) {
 		ret = btrfs_lookup_extent_info(trans, root, buf->start,
 					       buf->len, &refs, &flags);
-		BUG_ON(ret);
+		if (ret)
+			return ret;
 		BUG_ON(refs == 0);
 	} else {
 		refs = 1;
@@ -375,7 +376,8 @@ static noinline int update_ref_for_cow(struct btrfs_trans_handle *trans,
 							  buf->start,
 							  buf->len,
 							  new_flags, 0);
-			BUG_ON(ret);
+			if (ret)
+				return ret;
 		}
 	} else {
 		if (flags & BTRFS_BLOCK_FLAG_FULL_BACKREF) {
@@ -415,7 +417,7 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 {
 	struct btrfs_disk_key disk_key;
 	struct extent_buffer *cow;
-	int level;
+	int level, ret;
 	int last_ref = 0;
 	int unlock_orig = 0;
 	u64 parent_start;
@@ -467,7 +469,8 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 			    (unsigned long)btrfs_header_fsid(cow),
 			    BTRFS_FSID_SIZE);
 
-	update_ref_for_cow(trans, root, buf, cow, &last_ref);
+	ret = update_ref_for_cow(trans, root, buf, cow, &last_ref);
+	BUG_ON(ret);
 
 	if (root->ref_cows)
 		btrfs_reloc_cow_block(trans, root, buf, cow);

commit 143bede527b054a271053f41bfaca2b57baa9408
Author: Jeff Mahoney <jeffm@suse.com>
Date:   Thu Mar 1 14:56:26 2012 +0100

    btrfs: return void in functions without error conditions
    
    Signed-off-by: Jeff Mahoney <jeffm@suse.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 0639a555e16e..9d472c2a8de8 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -36,7 +36,7 @@ static int balance_node_right(struct btrfs_trans_handle *trans,
 			      struct btrfs_root *root,
 			      struct extent_buffer *dst_buf,
 			      struct extent_buffer *src_buf);
-static int del_ptr(struct btrfs_trans_handle *trans, struct btrfs_root *root,
+static void del_ptr(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 		   struct btrfs_path *path, int level, int slot);
 
 struct btrfs_path *btrfs_alloc_path(void)
@@ -1010,10 +1010,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		if (btrfs_header_nritems(right) == 0) {
 			clean_tree_block(trans, root, right);
 			btrfs_tree_unlock(right);
-			wret = del_ptr(trans, root, path, level + 1, pslot +
-				       1);
-			if (wret)
-				ret = wret;
+			del_ptr(trans, root, path, level + 1, pslot + 1);
 			root_sub_used(root, right->len);
 			btrfs_free_tree_block(trans, root, right, 0, 1, 0);
 			free_extent_buffer(right);
@@ -1051,9 +1048,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 	if (btrfs_header_nritems(mid) == 0) {
 		clean_tree_block(trans, root, mid);
 		btrfs_tree_unlock(mid);
-		wret = del_ptr(trans, root, path, level + 1, pslot);
-		if (wret)
-			ret = wret;
+		del_ptr(trans, root, path, level + 1, pslot);
 		root_sub_used(root, mid->len);
 		btrfs_free_tree_block(trans, root, mid, 0, 1, 0);
 		free_extent_buffer(mid);
@@ -1881,15 +1876,12 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
  * fixing up pointers when a given leaf/node is not in slot 0 of the
  * higher levels
  *
- * If this fails to write a tree block, it returns -1, but continues
- * fixing up the blocks in ram so the tree is consistent.
  */
-static int fixup_low_keys(struct btrfs_trans_handle *trans,
-			  struct btrfs_root *root, struct btrfs_path *path,
-			  struct btrfs_disk_key *key, int level)
+static void fixup_low_keys(struct btrfs_trans_handle *trans,
+			   struct btrfs_root *root, struct btrfs_path *path,
+			   struct btrfs_disk_key *key, int level)
 {
 	int i;
-	int ret = 0;
 	struct extent_buffer *t;
 
 	for (i = level; i < BTRFS_MAX_LEVEL; i++) {
@@ -1902,7 +1894,6 @@ static int fixup_low_keys(struct btrfs_trans_handle *trans,
 		if (tslot != 0)
 			break;
 	}
-	return ret;
 }
 
 /*
@@ -1911,9 +1902,9 @@ static int fixup_low_keys(struct btrfs_trans_handle *trans,
  * This function isn't completely safe. It's the caller's responsibility
  * that the new key won't break the order
  */
-int btrfs_set_item_key_safe(struct btrfs_trans_handle *trans,
-			    struct btrfs_root *root, struct btrfs_path *path,
-			    struct btrfs_key *new_key)
+void btrfs_set_item_key_safe(struct btrfs_trans_handle *trans,
+			     struct btrfs_root *root, struct btrfs_path *path,
+			     struct btrfs_key *new_key)
 {
 	struct btrfs_disk_key disk_key;
 	struct extent_buffer *eb;
@@ -1923,13 +1914,11 @@ int btrfs_set_item_key_safe(struct btrfs_trans_handle *trans,
 	slot = path->slots[0];
 	if (slot > 0) {
 		btrfs_item_key(eb, &disk_key, slot - 1);
-		if (comp_keys(&disk_key, new_key) >= 0)
-			return -1;
+		BUG_ON(comp_keys(&disk_key, new_key) >= 0);
 	}
 	if (slot < btrfs_header_nritems(eb) - 1) {
 		btrfs_item_key(eb, &disk_key, slot + 1);
-		if (comp_keys(&disk_key, new_key) <= 0)
-			return -1;
+		BUG_ON(comp_keys(&disk_key, new_key) <= 0);
 	}
 
 	btrfs_cpu_key_to_disk(&disk_key, new_key);
@@ -1937,7 +1926,6 @@ int btrfs_set_item_key_safe(struct btrfs_trans_handle *trans,
 	btrfs_mark_buffer_dirty(eb);
 	if (slot == 0)
 		fixup_low_keys(trans, root, path, &disk_key, 1);
-	return 0;
 }
 
 /*
@@ -2140,12 +2128,11 @@ static noinline int insert_new_root(struct btrfs_trans_handle *trans,
  *
  * slot and level indicate where you want the key to go, and
  * blocknr is the block the key points to.
- *
- * returns zero on success and < 0 on any error
  */
-static int insert_ptr(struct btrfs_trans_handle *trans, struct btrfs_root
-		      *root, struct btrfs_path *path, struct btrfs_disk_key
-		      *key, u64 bytenr, int slot, int level)
+static void insert_ptr(struct btrfs_trans_handle *trans,
+		       struct btrfs_root *root, struct btrfs_path *path,
+		       struct btrfs_disk_key *key, u64 bytenr,
+		       int slot, int level)
 {
 	struct extent_buffer *lower;
 	int nritems;
@@ -2155,8 +2142,7 @@ static int insert_ptr(struct btrfs_trans_handle *trans, struct btrfs_root
 	lower = path->nodes[level];
 	nritems = btrfs_header_nritems(lower);
 	BUG_ON(slot > nritems);
-	if (nritems == BTRFS_NODEPTRS_PER_BLOCK(root))
-		BUG();
+	BUG_ON(nritems == BTRFS_NODEPTRS_PER_BLOCK(root));
 	if (slot != nritems) {
 		memmove_extent_buffer(lower,
 			      btrfs_node_key_ptr_offset(slot + 1),
@@ -2169,7 +2155,6 @@ static int insert_ptr(struct btrfs_trans_handle *trans, struct btrfs_root
 	btrfs_set_node_ptr_generation(lower, slot, trans->transid);
 	btrfs_set_header_nritems(lower, nritems + 1);
 	btrfs_mark_buffer_dirty(lower);
-	return 0;
 }
 
 /*
@@ -2190,7 +2175,6 @@ static noinline int split_node(struct btrfs_trans_handle *trans,
 	struct btrfs_disk_key disk_key;
 	int mid;
 	int ret;
-	int wret;
 	u32 c_nritems;
 
 	c = path->nodes[level];
@@ -2247,11 +2231,8 @@ static noinline int split_node(struct btrfs_trans_handle *trans,
 	btrfs_mark_buffer_dirty(c);
 	btrfs_mark_buffer_dirty(split);
 
-	wret = insert_ptr(trans, root, path, &disk_key, split->start,
-			  path->slots[level + 1] + 1,
-			  level + 1);
-	if (wret)
-		ret = wret;
+	insert_ptr(trans, root, path, &disk_key, split->start,
+		   path->slots[level + 1] + 1, level + 1);
 
 	if (path->slots[level] >= mid) {
 		path->slots[level] -= mid;
@@ -2537,7 +2518,6 @@ static noinline int __push_leaf_left(struct btrfs_trans_handle *trans,
 	u32 old_left_nritems;
 	u32 nr;
 	int ret = 0;
-	int wret;
 	u32 this_item_size;
 	u32 old_left_item_size;
 
@@ -2643,9 +2623,7 @@ static noinline int __push_leaf_left(struct btrfs_trans_handle *trans,
 		clean_tree_block(trans, root, right);
 
 	btrfs_item_key(right, &disk_key, 0);
-	wret = fixup_low_keys(trans, root, path, &disk_key, 1);
-	if (wret)
-		ret = wret;
+	fixup_low_keys(trans, root, path, &disk_key, 1);
 
 	/* then fixup the leaf pointer in the path */
 	if (path->slots[0] < push_items) {
@@ -2738,21 +2716,17 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 /*
  * split the path's leaf in two, making sure there is at least data_size
  * available for the resulting leaf level of the path.
- *
- * returns 0 if all went well and < 0 on failure.
  */
-static noinline int copy_for_split(struct btrfs_trans_handle *trans,
-			       struct btrfs_root *root,
-			       struct btrfs_path *path,
-			       struct extent_buffer *l,
-			       struct extent_buffer *right,
-			       int slot, int mid, int nritems)
+static noinline void copy_for_split(struct btrfs_trans_handle *trans,
+				    struct btrfs_root *root,
+				    struct btrfs_path *path,
+				    struct extent_buffer *l,
+				    struct extent_buffer *right,
+				    int slot, int mid, int nritems)
 {
 	int data_copy_size;
 	int rt_data_off;
 	int i;
-	int ret = 0;
-	int wret;
 	struct btrfs_disk_key disk_key;
 
 	nritems = nritems - mid;
@@ -2780,12 +2754,9 @@ static noinline int copy_for_split(struct btrfs_trans_handle *trans,
 	}
 
 	btrfs_set_header_nritems(l, mid);
-	ret = 0;
 	btrfs_item_key(right, &disk_key, 0);
-	wret = insert_ptr(trans, root, path, &disk_key, right->start,
-			  path->slots[1] + 1, 1);
-	if (wret)
-		ret = wret;
+	insert_ptr(trans, root, path, &disk_key, right->start,
+		   path->slots[1] + 1, 1);
 
 	btrfs_mark_buffer_dirty(right);
 	btrfs_mark_buffer_dirty(l);
@@ -2803,8 +2774,6 @@ static noinline int copy_for_split(struct btrfs_trans_handle *trans,
 	}
 
 	BUG_ON(path->slots[0] < 0);
-
-	return ret;
 }
 
 /*
@@ -2993,12 +2962,8 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 	if (split == 0) {
 		if (mid <= slot) {
 			btrfs_set_header_nritems(right, 0);
-			wret = insert_ptr(trans, root, path,
-					  &disk_key, right->start,
-					  path->slots[1] + 1, 1);
-			if (wret)
-				ret = wret;
-
+			insert_ptr(trans, root, path, &disk_key, right->start,
+				   path->slots[1] + 1, 1);
 			btrfs_tree_unlock(path->nodes[0]);
 			free_extent_buffer(path->nodes[0]);
 			path->nodes[0] = right;
@@ -3006,29 +2971,21 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 			path->slots[1] += 1;
 		} else {
 			btrfs_set_header_nritems(right, 0);
-			wret = insert_ptr(trans, root, path,
-					  &disk_key,
-					  right->start,
+			insert_ptr(trans, root, path, &disk_key, right->start,
 					  path->slots[1], 1);
-			if (wret)
-				ret = wret;
 			btrfs_tree_unlock(path->nodes[0]);
 			free_extent_buffer(path->nodes[0]);
 			path->nodes[0] = right;
 			path->slots[0] = 0;
-			if (path->slots[1] == 0) {
-				wret = fixup_low_keys(trans, root,
-						path, &disk_key, 1);
-				if (wret)
-					ret = wret;
-			}
+			if (path->slots[1] == 0)
+				fixup_low_keys(trans, root, path,
+					       &disk_key, 1);
 		}
 		btrfs_mark_buffer_dirty(right);
 		return ret;
 	}
 
-	ret = copy_for_split(trans, root, path, l, right, slot, mid, nritems);
-	BUG_ON(ret);
+	copy_for_split(trans, root, path, l, right, slot, mid, nritems);
 
 	if (split == 2) {
 		BUG_ON(num_doubles != 0);
@@ -3036,7 +2993,7 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 		goto again;
 	}
 
-	return ret;
+	return 0;
 
 push_for_double:
 	push_for_double_split(trans, root, path, data_size);
@@ -3238,11 +3195,9 @@ int btrfs_duplicate_item(struct btrfs_trans_handle *trans,
 		return ret;
 
 	path->slots[0]++;
-	ret = setup_items_for_insert(trans, root, path, new_key, &item_size,
-				     item_size, item_size +
-				     sizeof(struct btrfs_item), 1);
-	BUG_ON(ret);
-
+	setup_items_for_insert(trans, root, path, new_key, &item_size,
+			       item_size, item_size +
+			       sizeof(struct btrfs_item), 1);
 	leaf = path->nodes[0];
 	memcpy_extent_buffer(leaf,
 			     btrfs_item_ptr_offset(leaf, path->slots[0]),
@@ -3257,10 +3212,10 @@ int btrfs_duplicate_item(struct btrfs_trans_handle *trans,
  * off the end of the item or if we shift the item to chop bytes off
  * the front.
  */
-int btrfs_truncate_item(struct btrfs_trans_handle *trans,
-			struct btrfs_root *root,
-			struct btrfs_path *path,
-			u32 new_size, int from_end)
+void btrfs_truncate_item(struct btrfs_trans_handle *trans,
+			 struct btrfs_root *root,
+			 struct btrfs_path *path,
+			 u32 new_size, int from_end)
 {
 	int slot;
 	struct extent_buffer *leaf;
@@ -3277,7 +3232,7 @@ int btrfs_truncate_item(struct btrfs_trans_handle *trans,
 
 	old_size = btrfs_item_size_nr(leaf, slot);
 	if (old_size == new_size)
-		return 0;
+		return;
 
 	nritems = btrfs_header_nritems(leaf);
 	data_end = leaf_data_end(root, leaf);
@@ -3350,15 +3305,14 @@ int btrfs_truncate_item(struct btrfs_trans_handle *trans,
 		btrfs_print_leaf(root, leaf);
 		BUG();
 	}
-	return 0;
 }
 
 /*
  * make the item pointed to by the path bigger, data_size is the new size.
  */
-int btrfs_extend_item(struct btrfs_trans_handle *trans,
-		      struct btrfs_root *root, struct btrfs_path *path,
-		      u32 data_size)
+void btrfs_extend_item(struct btrfs_trans_handle *trans,
+		       struct btrfs_root *root, struct btrfs_path *path,
+		       u32 data_size)
 {
 	int slot;
 	struct extent_buffer *leaf;
@@ -3416,7 +3370,6 @@ int btrfs_extend_item(struct btrfs_trans_handle *trans,
 		btrfs_print_leaf(root, leaf);
 		BUG();
 	}
-	return 0;
 }
 
 /*
@@ -3544,7 +3497,7 @@ int btrfs_insert_some_items(struct btrfs_trans_handle *trans,
 	ret = 0;
 	if (slot == 0) {
 		btrfs_cpu_key_to_disk(&disk_key, cpu_key);
-		ret = fixup_low_keys(trans, root, path, &disk_key, 1);
+		fixup_low_keys(trans, root, path, &disk_key, 1);
 	}
 
 	if (btrfs_leaf_free_space(root, leaf) < 0) {
@@ -3562,17 +3515,16 @@ int btrfs_insert_some_items(struct btrfs_trans_handle *trans,
  * to save stack depth by doing the bulk of the work in a function
  * that doesn't call btrfs_search_slot
  */
-int setup_items_for_insert(struct btrfs_trans_handle *trans,
-			   struct btrfs_root *root, struct btrfs_path *path,
-			   struct btrfs_key *cpu_key, u32 *data_size,
-			   u32 total_data, u32 total_size, int nr)
+void setup_items_for_insert(struct btrfs_trans_handle *trans,
+			    struct btrfs_root *root, struct btrfs_path *path,
+			    struct btrfs_key *cpu_key, u32 *data_size,
+			    u32 total_data, u32 total_size, int nr)
 {
 	struct btrfs_item *item;
 	int i;
 	u32 nritems;
 	unsigned int data_end;
 	struct btrfs_disk_key disk_key;
-	int ret;
 	struct extent_buffer *leaf;
 	int slot;
 
@@ -3633,10 +3585,9 @@ int setup_items_for_insert(struct btrfs_trans_handle *trans,
 
 	btrfs_set_header_nritems(leaf, nritems + nr);
 
-	ret = 0;
 	if (slot == 0) {
 		btrfs_cpu_key_to_disk(&disk_key, cpu_key);
-		ret = fixup_low_keys(trans, root, path, &disk_key, 1);
+		fixup_low_keys(trans, root, path, &disk_key, 1);
 	}
 	btrfs_unlock_up_safe(path, 1);
 	btrfs_mark_buffer_dirty(leaf);
@@ -3645,7 +3596,6 @@ int setup_items_for_insert(struct btrfs_trans_handle *trans,
 		btrfs_print_leaf(root, leaf);
 		BUG();
 	}
-	return ret;
 }
 
 /*
@@ -3672,16 +3622,14 @@ int btrfs_insert_empty_items(struct btrfs_trans_handle *trans,
 	if (ret == 0)
 		return -EEXIST;
 	if (ret < 0)
-		goto out;
+		return ret;
 
 	slot = path->slots[0];
 	BUG_ON(slot < 0);
 
-	ret = setup_items_for_insert(trans, root, path, cpu_key, data_size,
+	setup_items_for_insert(trans, root, path, cpu_key, data_size,
 			       total_data, total_size, nr);
-
-out:
-	return ret;
+	return 0;
 }
 
 /*
@@ -3717,13 +3665,11 @@ int btrfs_insert_item(struct btrfs_trans_handle *trans, struct btrfs_root
  * the tree should have been previously balanced so the deletion does not
  * empty a node.
  */
-static int del_ptr(struct btrfs_trans_handle *trans, struct btrfs_root *root,
-		   struct btrfs_path *path, int level, int slot)
+static void del_ptr(struct btrfs_trans_handle *trans, struct btrfs_root *root,
+		    struct btrfs_path *path, int level, int slot)
 {
 	struct extent_buffer *parent = path->nodes[level];
 	u32 nritems;
-	int ret = 0;
-	int wret;
 
 	nritems = btrfs_header_nritems(parent);
 	if (slot != nritems - 1) {
@@ -3743,12 +3689,9 @@ static int del_ptr(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 		struct btrfs_disk_key disk_key;
 
 		btrfs_node_key(parent, &disk_key, 0);
-		wret = fixup_low_keys(trans, root, path, &disk_key, level + 1);
-		if (wret)
-			ret = wret;
+		fixup_low_keys(trans, root, path, &disk_key, level + 1);
 	}
 	btrfs_mark_buffer_dirty(parent);
-	return ret;
 }
 
 /*
@@ -3761,17 +3704,13 @@ static int del_ptr(struct btrfs_trans_handle *trans, struct btrfs_root *root,
  * The path must have already been setup for deleting the leaf, including
  * all the proper balancing.  path->nodes[1] must be locked.
  */
-static noinline int btrfs_del_leaf(struct btrfs_trans_handle *trans,
-				   struct btrfs_root *root,
-				   struct btrfs_path *path,
-				   struct extent_buffer *leaf)
+static noinline void btrfs_del_leaf(struct btrfs_trans_handle *trans,
+				    struct btrfs_root *root,
+				    struct btrfs_path *path,
+				    struct extent_buffer *leaf)
 {
-	int ret;
-
 	WARN_ON(btrfs_header_generation(leaf) != trans->transid);
-	ret = del_ptr(trans, root, path, 1, path->slots[1]);
-	if (ret)
-		return ret;
+	del_ptr(trans, root, path, 1, path->slots[1]);
 
 	/*
 	 * btrfs_free_extent is expensive, we want to make sure we
@@ -3782,7 +3721,6 @@ static noinline int btrfs_del_leaf(struct btrfs_trans_handle *trans,
 	root_sub_used(root, leaf->len);
 
 	btrfs_free_tree_block(trans, root, leaf, 0, 1, 0);
-	return 0;
 }
 /*
  * delete the item at the leaf level in path.  If that empties
@@ -3839,8 +3777,7 @@ int btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 		} else {
 			btrfs_set_path_blocking(path);
 			clean_tree_block(trans, root, leaf);
-			ret = btrfs_del_leaf(trans, root, path, leaf);
-			BUG_ON(ret);
+			btrfs_del_leaf(trans, root, path, leaf);
 		}
 	} else {
 		int used = leaf_space_used(leaf, 0, nritems);
@@ -3848,10 +3785,7 @@ int btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 			struct btrfs_disk_key disk_key;
 
 			btrfs_item_key(leaf, &disk_key, 0);
-			wret = fixup_low_keys(trans, root, path,
-					      &disk_key, 1);
-			if (wret)
-				ret = wret;
+			fixup_low_keys(trans, root, path, &disk_key, 1);
 		}
 
 		/* delete the leaf if it is mostly empty */
@@ -3879,9 +3813,9 @@ int btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 
 			if (btrfs_header_nritems(leaf) == 0) {
 				path->slots[1] = slot;
-				ret = btrfs_del_leaf(trans, root, path, leaf);
-				BUG_ON(ret);
+				btrfs_del_leaf(trans, root, path, leaf);
 				free_extent_buffer(leaf);
+				ret = 0;
 			} else {
 				/* if we're still in the path, make sure
 				 * we're dirty.  Otherwise, one of the

commit 66d7e7f09f77456fe68683247d77721032a00ee5
Author: Arne Jansen <sensille@gmx.net>
Date:   Mon Sep 12 15:26:38 2011 +0200

    Btrfs: mark delayed refs as for cow
    
    Add a for_cow parameter to add_delayed_*_ref and pass the appropriate value
    from every call site. The for_cow parameter will later on be used to
    determine if a ref will change anything with respect to qgroups.
    
    Delayed refs coming from relocation are always counted as for_cow, as they
    don't change subvol quota.
    
    Also pass in the fs_info for later use.
    
    btrfs_find_all_roots() will use this as an optimization, as changes that are
    for_cow will not change anything with respect to which root points to a
    certain leaf. Thus, we don't need to add the current sequence number to
    those delayed refs.
    
    Signed-off-by: Arne Jansen <sensille@gmx.net>
    Signed-off-by: Jan Schmidt <list.btrfs@jan-o-sch.net>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index dede441bdeee..0639a555e16e 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -240,7 +240,7 @@ int btrfs_copy_root(struct btrfs_trans_handle *trans,
 
 	cow = btrfs_alloc_free_block(trans, root, buf->len, 0,
 				     new_root_objectid, &disk_key, level,
-				     buf->start, 0);
+				     buf->start, 0, 1);
 	if (IS_ERR(cow))
 		return PTR_ERR(cow);
 
@@ -261,9 +261,9 @@ int btrfs_copy_root(struct btrfs_trans_handle *trans,
 
 	WARN_ON(btrfs_header_generation(buf) > trans->transid);
 	if (new_root_objectid == BTRFS_TREE_RELOC_OBJECTID)
-		ret = btrfs_inc_ref(trans, root, cow, 1);
+		ret = btrfs_inc_ref(trans, root, cow, 1, 1);
 	else
-		ret = btrfs_inc_ref(trans, root, cow, 0);
+		ret = btrfs_inc_ref(trans, root, cow, 0, 1);
 
 	if (ret)
 		return ret;
@@ -350,14 +350,14 @@ static noinline int update_ref_for_cow(struct btrfs_trans_handle *trans,
 		if ((owner == root->root_key.objectid ||
 		     root->root_key.objectid == BTRFS_TREE_RELOC_OBJECTID) &&
 		    !(flags & BTRFS_BLOCK_FLAG_FULL_BACKREF)) {
-			ret = btrfs_inc_ref(trans, root, buf, 1);
+			ret = btrfs_inc_ref(trans, root, buf, 1, 1);
 			BUG_ON(ret);
 
 			if (root->root_key.objectid ==
 			    BTRFS_TREE_RELOC_OBJECTID) {
-				ret = btrfs_dec_ref(trans, root, buf, 0);
+				ret = btrfs_dec_ref(trans, root, buf, 0, 1);
 				BUG_ON(ret);
-				ret = btrfs_inc_ref(trans, root, cow, 1);
+				ret = btrfs_inc_ref(trans, root, cow, 1, 1);
 				BUG_ON(ret);
 			}
 			new_flags |= BTRFS_BLOCK_FLAG_FULL_BACKREF;
@@ -365,9 +365,9 @@ static noinline int update_ref_for_cow(struct btrfs_trans_handle *trans,
 
 			if (root->root_key.objectid ==
 			    BTRFS_TREE_RELOC_OBJECTID)
-				ret = btrfs_inc_ref(trans, root, cow, 1);
+				ret = btrfs_inc_ref(trans, root, cow, 1, 1);
 			else
-				ret = btrfs_inc_ref(trans, root, cow, 0);
+				ret = btrfs_inc_ref(trans, root, cow, 0, 1);
 			BUG_ON(ret);
 		}
 		if (new_flags != 0) {
@@ -381,11 +381,11 @@ static noinline int update_ref_for_cow(struct btrfs_trans_handle *trans,
 		if (flags & BTRFS_BLOCK_FLAG_FULL_BACKREF) {
 			if (root->root_key.objectid ==
 			    BTRFS_TREE_RELOC_OBJECTID)
-				ret = btrfs_inc_ref(trans, root, cow, 1);
+				ret = btrfs_inc_ref(trans, root, cow, 1, 1);
 			else
-				ret = btrfs_inc_ref(trans, root, cow, 0);
+				ret = btrfs_inc_ref(trans, root, cow, 0, 1);
 			BUG_ON(ret);
-			ret = btrfs_dec_ref(trans, root, buf, 1);
+			ret = btrfs_dec_ref(trans, root, buf, 1, 1);
 			BUG_ON(ret);
 		}
 		clean_tree_block(trans, root, buf);
@@ -446,7 +446,7 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 
 	cow = btrfs_alloc_free_block(trans, root, buf->len, parent_start,
 				     root->root_key.objectid, &disk_key,
-				     level, search_start, empty_size);
+				     level, search_start, empty_size, 1);
 	if (IS_ERR(cow))
 		return PTR_ERR(cow);
 
@@ -484,7 +484,7 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 		rcu_assign_pointer(root->node, cow);
 
 		btrfs_free_tree_block(trans, root, buf, parent_start,
-				      last_ref);
+				      last_ref, 1);
 		free_extent_buffer(buf);
 		add_root_to_dirty_list(root);
 	} else {
@@ -500,7 +500,7 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 					      trans->transid);
 		btrfs_mark_buffer_dirty(parent);
 		btrfs_free_tree_block(trans, root, buf, parent_start,
-				      last_ref);
+				      last_ref, 1);
 	}
 	if (unlock_orig)
 		btrfs_tree_unlock(buf);
@@ -957,7 +957,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		free_extent_buffer(mid);
 
 		root_sub_used(root, mid->len);
-		btrfs_free_tree_block(trans, root, mid, 0, 1);
+		btrfs_free_tree_block(trans, root, mid, 0, 1, 0);
 		/* once for the root ptr */
 		free_extent_buffer(mid);
 		return 0;
@@ -1015,7 +1015,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 			if (wret)
 				ret = wret;
 			root_sub_used(root, right->len);
-			btrfs_free_tree_block(trans, root, right, 0, 1);
+			btrfs_free_tree_block(trans, root, right, 0, 1, 0);
 			free_extent_buffer(right);
 			right = NULL;
 		} else {
@@ -1055,7 +1055,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		if (wret)
 			ret = wret;
 		root_sub_used(root, mid->len);
-		btrfs_free_tree_block(trans, root, mid, 0, 1);
+		btrfs_free_tree_block(trans, root, mid, 0, 1, 0);
 		free_extent_buffer(mid);
 		mid = NULL;
 	} else {
@@ -2089,7 +2089,7 @@ static noinline int insert_new_root(struct btrfs_trans_handle *trans,
 
 	c = btrfs_alloc_free_block(trans, root, root->nodesize, 0,
 				   root->root_key.objectid, &lower_key,
-				   level, root->node->start, 0);
+				   level, root->node->start, 0, 0);
 	if (IS_ERR(c))
 		return PTR_ERR(c);
 
@@ -2216,7 +2216,7 @@ static noinline int split_node(struct btrfs_trans_handle *trans,
 
 	split = btrfs_alloc_free_block(trans, root, root->nodesize, 0,
 					root->root_key.objectid,
-					&disk_key, level, c->start, 0);
+					&disk_key, level, c->start, 0, 0);
 	if (IS_ERR(split))
 		return PTR_ERR(split);
 
@@ -2970,7 +2970,7 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 
 	right = btrfs_alloc_free_block(trans, root, root->leafsize, 0,
 					root->root_key.objectid,
-					&disk_key, 0, l->start, 0);
+					&disk_key, 0, l->start, 0, 0);
 	if (IS_ERR(right))
 		return PTR_ERR(right);
 
@@ -3781,7 +3781,7 @@ static noinline int btrfs_del_leaf(struct btrfs_trans_handle *trans,
 
 	root_sub_used(root, leaf->len);
 
-	btrfs_free_tree_block(trans, root, leaf, 0, 1);
+	btrfs_free_tree_block(trans, root, leaf, 0, 1, 0);
 	return 0;
 }
 /*

commit f1ebcc74d5b2159f44c96b479b6eb8afc7829095
Author: Liu Bo <liubo2009@cn.fujitsu.com>
Date:   Mon Nov 14 20:48:06 2011 -0500

    Btrfs: fix tree corruption after multi-thread snapshots and inode_cache flush
    
    The btrfs snapshotting code requires that once a root has been
    snapshotted, we don't change it during a commit.
    
    But there are two cases to lead to tree corruptions:
    
    1) multi-thread snapshots can commit serveral snapshots in a transaction,
       and this may change the src root when processing the following pending
       snapshots, which lead to the former snapshots corruptions;
    
    2) the free inode cache was changing the roots when it root the cache,
       which lead to corruptions.
    
    This fixes things by making sure we force COW the block after we create a
    snapshot during commiting a transaction, then any changes to the roots
    will result in COW, and we get all the fs roots and snapshot roots to be
    consistent.
    
    Signed-off-by: Liu Bo <liubo2009@cn.fujitsu.com>
    Signed-off-by: Miao Xie <miaox@cn.fujitsu.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 0fe615e4ea38..dede441bdeee 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -514,10 +514,25 @@ static inline int should_cow_block(struct btrfs_trans_handle *trans,
 				   struct btrfs_root *root,
 				   struct extent_buffer *buf)
 {
+	/* ensure we can see the force_cow */
+	smp_rmb();
+
+	/*
+	 * We do not need to cow a block if
+	 * 1) this block is not created or changed in this transaction;
+	 * 2) this block does not belong to TREE_RELOC tree;
+	 * 3) the root is not forced COW.
+	 *
+	 * What is forced COW:
+	 *    when we create snapshot during commiting the transaction,
+	 *    after we've finished coping src root, we must COW the shared
+	 *    block to ensure the metadata consistency.
+	 */
 	if (btrfs_header_generation(buf) == trans->transid &&
 	    !btrfs_header_flag(buf, BTRFS_HEADER_FLAG_WRITTEN) &&
 	    !(root->root_key.objectid != BTRFS_TREE_RELOC_OBJECTID &&
-	      btrfs_header_flag(buf, BTRFS_HEADER_FLAG_RELOC)))
+	      btrfs_header_flag(buf, BTRFS_HEADER_FLAG_RELOC)) &&
+	    !root->force_cow)
 		return 0;
 	return 1;
 }

commit a05a9bb18ae0abec0b513b5fde876c47905fa13e
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Tue Sep 6 16:55:34 2011 +0800

    Btrfs: fix array bound checking
    
    Otherwise we can execced the array bound of path->slots[].
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 011cab3aca8d..0fe615e4ea38 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -902,9 +902,10 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 
 	orig_ptr = btrfs_node_blockptr(mid, orig_slot);
 
-	if (level < BTRFS_MAX_LEVEL - 1)
+	if (level < BTRFS_MAX_LEVEL - 1) {
 		parent = path->nodes[level + 1];
-	pslot = path->slots[level + 1];
+		pslot = path->slots[level + 1];
+	}
 
 	/*
 	 * deal with the case where there is only one pointer in the root
@@ -1107,9 +1108,10 @@ static noinline int push_nodes_for_insert(struct btrfs_trans_handle *trans,
 	mid = path->nodes[level];
 	WARN_ON(btrfs_header_generation(mid) != trans->transid);
 
-	if (level < BTRFS_MAX_LEVEL - 1)
+	if (level < BTRFS_MAX_LEVEL - 1) {
 		parent = path->nodes[level + 1];
-	pslot = path->slots[level + 1];
+		pslot = path->slots[level + 1];
+	}
 
 	if (!parent)
 		return 1;

commit 31533fb263928c93a34cda41b66a6e83ade5c766
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Jul 26 16:01:59 2011 -0400

    Btrfs: remove lockdep magic from btrfs_next_leaf
    
    Before the reader/writer locks, btrfs_next_leaf needed to keep
    the path blocking to avoid making lockdep upset.
    
    Now that btrfs_next_leaf only takes read locks, this isn't required.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 0ad48e782d37..011cab3aca8d 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -4169,21 +4169,12 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 	u32 nritems;
 	int ret;
 	int old_spinning = path->leave_spinning;
-	int force_blocking = 0;
 	int next_rw_lock = 0;
 
 	nritems = btrfs_header_nritems(path->nodes[0]);
 	if (nritems == 0)
 		return 1;
 
-	/*
-	 * we take the blocks in an order that upsets lockdep.  Using
-	 * blocking mode is the only way around it.
-	 */
-#ifdef CONFIG_DEBUG_LOCK_ALLOC
-	force_blocking = 1;
-#endif
-
 	btrfs_item_key_to_cpu(path->nodes[0], &key, nritems - 1);
 again:
 	level = 1;
@@ -4192,9 +4183,7 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 	btrfs_release_path(path);
 
 	path->keep_locks = 1;
-
-	if (!force_blocking)
-		path->leave_spinning = 1;
+	path->leave_spinning = 1;
 
 	ret = btrfs_search_slot(NULL, root, &key, path, 0, 0);
 	path->keep_locks = 0;
@@ -4255,18 +4244,10 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 			if (!ret) {
 				btrfs_set_path_blocking(path);
 				btrfs_tree_read_lock(next);
-				if (!force_blocking) {
-					btrfs_clear_path_blocking(path, next,
+				btrfs_clear_path_blocking(path, next,
 							  BTRFS_READ_LOCK);
-				}
-			}
-			if (force_blocking) {
-				btrfs_set_lock_blocking_rw(next,
-							   BTRFS_READ_LOCK);
-				next_rw_lock = BTRFS_READ_LOCK_BLOCKING;
-			} else {
-				next_rw_lock = BTRFS_READ_LOCK;
 			}
+			next_rw_lock = BTRFS_READ_LOCK;
 		}
 		break;
 	}
@@ -4300,17 +4281,10 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 			if (!ret) {
 				btrfs_set_path_blocking(path);
 				btrfs_tree_read_lock(next);
-				if (!force_blocking)
-					btrfs_clear_path_blocking(path, next,
+				btrfs_clear_path_blocking(path, next,
 							  BTRFS_READ_LOCK);
 			}
-			if (force_blocking) {
-				btrfs_set_lock_blocking_rw(next,
-						   BTRFS_READ_LOCK);
-				next_rw_lock = BTRFS_READ_LOCK_BLOCKING;
-			} else {
-				next_rw_lock = BTRFS_READ_LOCK;
-			}
+			next_rw_lock = BTRFS_READ_LOCK;
 		}
 	}
 	ret = 0;

commit bd681513fa6f2ff29aa391f01e413a2d1c59fd77
Author: Chris Mason <chris.mason@oracle.com>
Date:   Sat Jul 16 15:23:14 2011 -0400

    Btrfs: switch the btrfs tree locks to reader/writer
    
    The btrfs metadata btree is the source of significant
    lock contention, especially in the root node.   This
    commit changes our locking to use a reader/writer
    lock.
    
    The lock is built on top of rw spinlocks, and it
    extends the lock tracking to remember if we have a
    read lock or a write lock when we go to blocking.  Atomics
    count the number of blocking readers or writers at any
    given time.
    
    It removes all of the adaptive spinning from the old code
    and uses only the spinning/blocking hints inside of btrfs
    to decide when it should continue spinning.
    
    In read heavy workloads this is dramatically faster.  In write
    heavy workloads we're still faster because of less contention
    on the root node lock.
    
    We suffer slightly in dbench because we schedule more often
    during write locks, but all other benchmarks so far are improved.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index d2431284b913..0ad48e782d37 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -54,8 +54,13 @@ noinline void btrfs_set_path_blocking(struct btrfs_path *p)
 {
 	int i;
 	for (i = 0; i < BTRFS_MAX_LEVEL; i++) {
-		if (p->nodes[i] && p->locks[i])
-			btrfs_set_lock_blocking(p->nodes[i]);
+		if (!p->nodes[i] || !p->locks[i])
+			continue;
+		btrfs_set_lock_blocking_rw(p->nodes[i], p->locks[i]);
+		if (p->locks[i] == BTRFS_READ_LOCK)
+			p->locks[i] = BTRFS_READ_LOCK_BLOCKING;
+		else if (p->locks[i] == BTRFS_WRITE_LOCK)
+			p->locks[i] = BTRFS_WRITE_LOCK_BLOCKING;
 	}
 }
 
@@ -68,7 +73,7 @@ noinline void btrfs_set_path_blocking(struct btrfs_path *p)
  * for held
  */
 noinline void btrfs_clear_path_blocking(struct btrfs_path *p,
-					struct extent_buffer *held)
+					struct extent_buffer *held, int held_rw)
 {
 	int i;
 
@@ -79,19 +84,29 @@ noinline void btrfs_clear_path_blocking(struct btrfs_path *p,
 	 * really sure by forcing the path to blocking before we clear
 	 * the path blocking.
 	 */
-	if (held)
-		btrfs_set_lock_blocking(held);
+	if (held) {
+		btrfs_set_lock_blocking_rw(held, held_rw);
+		if (held_rw == BTRFS_WRITE_LOCK)
+			held_rw = BTRFS_WRITE_LOCK_BLOCKING;
+		else if (held_rw == BTRFS_READ_LOCK)
+			held_rw = BTRFS_READ_LOCK_BLOCKING;
+	}
 	btrfs_set_path_blocking(p);
 #endif
 
 	for (i = BTRFS_MAX_LEVEL - 1; i >= 0; i--) {
-		if (p->nodes[i] && p->locks[i])
-			btrfs_clear_lock_blocking(p->nodes[i]);
+		if (p->nodes[i] && p->locks[i]) {
+			btrfs_clear_lock_blocking_rw(p->nodes[i], p->locks[i]);
+			if (p->locks[i] == BTRFS_WRITE_LOCK_BLOCKING)
+				p->locks[i] = BTRFS_WRITE_LOCK;
+			else if (p->locks[i] == BTRFS_READ_LOCK_BLOCKING)
+				p->locks[i] = BTRFS_READ_LOCK;
+		}
 	}
 
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 	if (held)
-		btrfs_clear_lock_blocking(held);
+		btrfs_clear_lock_blocking_rw(held, held_rw);
 #endif
 }
 
@@ -119,7 +134,7 @@ noinline void btrfs_release_path(struct btrfs_path *p)
 		if (!p->nodes[i])
 			continue;
 		if (p->locks[i]) {
-			btrfs_tree_unlock(p->nodes[i]);
+			btrfs_tree_unlock_rw(p->nodes[i], p->locks[i]);
 			p->locks[i] = 0;
 		}
 		free_extent_buffer(p->nodes[i]);
@@ -167,6 +182,25 @@ struct extent_buffer *btrfs_lock_root_node(struct btrfs_root *root)
 	return eb;
 }
 
+/* loop around taking references on and locking the root node of the
+ * tree until you end up with a lock on the root.  A locked buffer
+ * is returned, with a reference held.
+ */
+struct extent_buffer *btrfs_read_lock_root_node(struct btrfs_root *root)
+{
+	struct extent_buffer *eb;
+
+	while (1) {
+		eb = btrfs_root_node(root);
+		btrfs_tree_read_lock(eb);
+		if (eb == root->node)
+			break;
+		btrfs_tree_read_unlock(eb);
+		free_extent_buffer(eb);
+	}
+	return eb;
+}
+
 /* cowonly root (everything not a reference counted cow subvolume), just get
  * put onto a simple dirty list.  transaction.c walks this to make sure they
  * get properly updated on disk.
@@ -862,7 +896,8 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 
 	mid = path->nodes[level];
 
-	WARN_ON(!path->locks[level]);
+	WARN_ON(path->locks[level] != BTRFS_WRITE_LOCK &&
+		path->locks[level] != BTRFS_WRITE_LOCK_BLOCKING);
 	WARN_ON(btrfs_header_generation(mid) != trans->transid);
 
 	orig_ptr = btrfs_node_blockptr(mid, orig_slot);
@@ -1360,7 +1395,7 @@ static noinline void unlock_up(struct btrfs_path *path, int level,
 
 		t = path->nodes[i];
 		if (i >= lowest_unlock && i > skip_level && path->locks[i]) {
-			btrfs_tree_unlock(t);
+			btrfs_tree_unlock_rw(t, path->locks[i]);
 			path->locks[i] = 0;
 		}
 	}
@@ -1387,7 +1422,7 @@ noinline void btrfs_unlock_up_safe(struct btrfs_path *path, int level)
 			continue;
 		if (!path->locks[i])
 			continue;
-		btrfs_tree_unlock(path->nodes[i]);
+		btrfs_tree_unlock_rw(path->nodes[i], path->locks[i]);
 		path->locks[i] = 0;
 	}
 }
@@ -1436,6 +1471,8 @@ read_block_for_search(struct btrfs_trans_handle *trans,
 			 * we can trust our generation number
 			 */
 			free_extent_buffer(tmp);
+			btrfs_set_path_blocking(p);
+
 			tmp = read_tree_block(root, blocknr, blocksize, gen);
 			if (tmp && btrfs_buffer_uptodate(tmp, gen)) {
 				*eb_ret = tmp;
@@ -1491,20 +1528,27 @@ read_block_for_search(struct btrfs_trans_handle *trans,
 static int
 setup_nodes_for_search(struct btrfs_trans_handle *trans,
 		       struct btrfs_root *root, struct btrfs_path *p,
-		       struct extent_buffer *b, int level, int ins_len)
+		       struct extent_buffer *b, int level, int ins_len,
+		       int *write_lock_level)
 {
 	int ret;
 	if ((p->search_for_split || ins_len > 0) && btrfs_header_nritems(b) >=
 	    BTRFS_NODEPTRS_PER_BLOCK(root) - 3) {
 		int sret;
 
+		if (*write_lock_level < level + 1) {
+			*write_lock_level = level + 1;
+			btrfs_release_path(p);
+			goto again;
+		}
+
 		sret = reada_for_balance(root, p, level);
 		if (sret)
 			goto again;
 
 		btrfs_set_path_blocking(p);
 		sret = split_node(trans, root, p, level);
-		btrfs_clear_path_blocking(p, NULL);
+		btrfs_clear_path_blocking(p, NULL, 0);
 
 		BUG_ON(sret > 0);
 		if (sret) {
@@ -1516,13 +1560,19 @@ setup_nodes_for_search(struct btrfs_trans_handle *trans,
 		   BTRFS_NODEPTRS_PER_BLOCK(root) / 2) {
 		int sret;
 
+		if (*write_lock_level < level + 1) {
+			*write_lock_level = level + 1;
+			btrfs_release_path(p);
+			goto again;
+		}
+
 		sret = reada_for_balance(root, p, level);
 		if (sret)
 			goto again;
 
 		btrfs_set_path_blocking(p);
 		sret = balance_level(trans, root, p, level);
-		btrfs_clear_path_blocking(p, NULL);
+		btrfs_clear_path_blocking(p, NULL, 0);
 
 		if (sret) {
 			ret = sret;
@@ -1566,27 +1616,78 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 	int err;
 	int level;
 	int lowest_unlock = 1;
+	int root_lock;
+	/* everything at write_lock_level or lower must be write locked */
+	int write_lock_level = 0;
 	u8 lowest_level = 0;
 
 	lowest_level = p->lowest_level;
 	WARN_ON(lowest_level && ins_len > 0);
 	WARN_ON(p->nodes[0] != NULL);
 
-	if (ins_len < 0)
+	if (ins_len < 0) {
 		lowest_unlock = 2;
 
+		/* when we are removing items, we might have to go up to level
+		 * two as we update tree pointers  Make sure we keep write
+		 * for those levels as well
+		 */
+		write_lock_level = 2;
+	} else if (ins_len > 0) {
+		/*
+		 * for inserting items, make sure we have a write lock on
+		 * level 1 so we can update keys
+		 */
+		write_lock_level = 1;
+	}
+
+	if (!cow)
+		write_lock_level = -1;
+
+	if (cow && (p->keep_locks || p->lowest_level))
+		write_lock_level = BTRFS_MAX_LEVEL;
+
 again:
+	/*
+	 * we try very hard to do read locks on the root
+	 */
+	root_lock = BTRFS_READ_LOCK;
+	level = 0;
 	if (p->search_commit_root) {
+		/*
+		 * the commit roots are read only
+		 * so we always do read locks
+		 */
 		b = root->commit_root;
 		extent_buffer_get(b);
+		level = btrfs_header_level(b);
 		if (!p->skip_locking)
-			btrfs_tree_lock(b);
+			btrfs_tree_read_lock(b);
 	} else {
-		if (p->skip_locking)
+		if (p->skip_locking) {
 			b = btrfs_root_node(root);
-		else
-			b = btrfs_lock_root_node(root);
+			level = btrfs_header_level(b);
+		} else {
+			/* we don't know the level of the root node
+			 * until we actually have it read locked
+			 */
+			b = btrfs_read_lock_root_node(root);
+			level = btrfs_header_level(b);
+			if (level <= write_lock_level) {
+				/* whoops, must trade for write lock */
+				btrfs_tree_read_unlock(b);
+				free_extent_buffer(b);
+				b = btrfs_lock_root_node(root);
+				root_lock = BTRFS_WRITE_LOCK;
+
+				/* the level might have changed, check again */
+				level = btrfs_header_level(b);
+			}
+		}
 	}
+	p->nodes[level] = b;
+	if (!p->skip_locking)
+		p->locks[level] = root_lock;
 
 	while (b) {
 		level = btrfs_header_level(b);
@@ -1595,10 +1696,6 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 		 * setup the path here so we can release it under lock
 		 * contention with the cow code
 		 */
-		p->nodes[level] = b;
-		if (!p->skip_locking)
-			p->locks[level] = 1;
-
 		if (cow) {
 			/*
 			 * if we don't really need to cow this block
@@ -1610,6 +1707,16 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 
 			btrfs_set_path_blocking(p);
 
+			/*
+			 * must have write locks on this node and the
+			 * parent
+			 */
+			if (level + 1 > write_lock_level) {
+				write_lock_level = level + 1;
+				btrfs_release_path(p);
+				goto again;
+			}
+
 			err = btrfs_cow_block(trans, root, b,
 					      p->nodes[level + 1],
 					      p->slots[level + 1], &b);
@@ -1622,10 +1729,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 		BUG_ON(!cow && ins_len);
 
 		p->nodes[level] = b;
-		if (!p->skip_locking)
-			p->locks[level] = 1;
-
-		btrfs_clear_path_blocking(p, NULL);
+		btrfs_clear_path_blocking(p, NULL, 0);
 
 		/*
 		 * we have a lock on b and as long as we aren't changing
@@ -1651,7 +1755,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 			}
 			p->slots[level] = slot;
 			err = setup_nodes_for_search(trans, root, p, b, level,
-						     ins_len);
+					     ins_len, &write_lock_level);
 			if (err == -EAGAIN)
 				goto again;
 			if (err) {
@@ -1661,6 +1765,19 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 			b = p->nodes[level];
 			slot = p->slots[level];
 
+			/*
+			 * slot 0 is special, if we change the key
+			 * we have to update the parent pointer
+			 * which means we must have a write lock
+			 * on the parent
+			 */
+			if (slot == 0 && cow &&
+			    write_lock_level < level + 1) {
+				write_lock_level = level + 1;
+				btrfs_release_path(p);
+				goto again;
+			}
+
 			unlock_up(p, level, lowest_unlock);
 
 			if (level == lowest_level) {
@@ -1679,23 +1796,42 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 			}
 
 			if (!p->skip_locking) {
-				btrfs_clear_path_blocking(p, NULL);
-				err = btrfs_try_spin_lock(b);
-
-				if (!err) {
-					btrfs_set_path_blocking(p);
-					btrfs_tree_lock(b);
-					btrfs_clear_path_blocking(p, b);
+				level = btrfs_header_level(b);
+				if (level <= write_lock_level) {
+					err = btrfs_try_tree_write_lock(b);
+					if (!err) {
+						btrfs_set_path_blocking(p);
+						btrfs_tree_lock(b);
+						btrfs_clear_path_blocking(p, b,
+								  BTRFS_WRITE_LOCK);
+					}
+					p->locks[level] = BTRFS_WRITE_LOCK;
+				} else {
+					err = btrfs_try_tree_read_lock(b);
+					if (!err) {
+						btrfs_set_path_blocking(p);
+						btrfs_tree_read_lock(b);
+						btrfs_clear_path_blocking(p, b,
+								  BTRFS_READ_LOCK);
+					}
+					p->locks[level] = BTRFS_READ_LOCK;
 				}
+				p->nodes[level] = b;
 			}
 		} else {
 			p->slots[level] = slot;
 			if (ins_len > 0 &&
 			    btrfs_leaf_free_space(root, b) < ins_len) {
+				if (write_lock_level < 1) {
+					write_lock_level = 1;
+					btrfs_release_path(p);
+					goto again;
+				}
+
 				btrfs_set_path_blocking(p);
 				err = split_leaf(trans, root, key,
 						 p, ins_len, ret == 0);
-				btrfs_clear_path_blocking(p, NULL);
+				btrfs_clear_path_blocking(p, NULL, 0);
 
 				BUG_ON(err > 0);
 				if (err) {
@@ -1976,7 +2112,7 @@ static noinline int insert_new_root(struct btrfs_trans_handle *trans,
 	add_root_to_dirty_list(root);
 	extent_buffer_get(c);
 	path->nodes[level] = c;
-	path->locks[level] = 1;
+	path->locks[level] = BTRFS_WRITE_LOCK;
 	path->slots[level] = 0;
 	return 0;
 }
@@ -3819,11 +3955,11 @@ int btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,
 
 	WARN_ON(!path->keep_locks);
 again:
-	cur = btrfs_lock_root_node(root);
+	cur = btrfs_read_lock_root_node(root);
 	level = btrfs_header_level(cur);
 	WARN_ON(path->nodes[level]);
 	path->nodes[level] = cur;
-	path->locks[level] = 1;
+	path->locks[level] = BTRFS_READ_LOCK;
 
 	if (btrfs_header_generation(cur) < min_trans) {
 		ret = 1;
@@ -3913,12 +4049,12 @@ int btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,
 		cur = read_node_slot(root, cur, slot);
 		BUG_ON(!cur);
 
-		btrfs_tree_lock(cur);
+		btrfs_tree_read_lock(cur);
 
-		path->locks[level - 1] = 1;
+		path->locks[level - 1] = BTRFS_READ_LOCK;
 		path->nodes[level - 1] = cur;
 		unlock_up(path, level, 1);
-		btrfs_clear_path_blocking(path, NULL);
+		btrfs_clear_path_blocking(path, NULL, 0);
 	}
 out:
 	if (ret == 0)
@@ -4034,6 +4170,7 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 	int ret;
 	int old_spinning = path->leave_spinning;
 	int force_blocking = 0;
+	int next_rw_lock = 0;
 
 	nritems = btrfs_header_nritems(path->nodes[0]);
 	if (nritems == 0)
@@ -4051,6 +4188,7 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 again:
 	level = 1;
 	next = NULL;
+	next_rw_lock = 0;
 	btrfs_release_path(path);
 
 	path->keep_locks = 1;
@@ -4096,11 +4234,12 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 		}
 
 		if (next) {
-			btrfs_tree_unlock(next);
+			btrfs_tree_unlock_rw(next, next_rw_lock);
 			free_extent_buffer(next);
 		}
 
 		next = c;
+		next_rw_lock = path->locks[level];
 		ret = read_block_for_search(NULL, root, path, &next, level,
 					    slot, &key);
 		if (ret == -EAGAIN)
@@ -4112,15 +4251,22 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 		}
 
 		if (!path->skip_locking) {
-			ret = btrfs_try_spin_lock(next);
+			ret = btrfs_try_tree_read_lock(next);
 			if (!ret) {
 				btrfs_set_path_blocking(path);
-				btrfs_tree_lock(next);
-				if (!force_blocking)
-					btrfs_clear_path_blocking(path, next);
+				btrfs_tree_read_lock(next);
+				if (!force_blocking) {
+					btrfs_clear_path_blocking(path, next,
+							  BTRFS_READ_LOCK);
+				}
+			}
+			if (force_blocking) {
+				btrfs_set_lock_blocking_rw(next,
+							   BTRFS_READ_LOCK);
+				next_rw_lock = BTRFS_READ_LOCK_BLOCKING;
+			} else {
+				next_rw_lock = BTRFS_READ_LOCK;
 			}
-			if (force_blocking)
-				btrfs_set_lock_blocking(next);
 		}
 		break;
 	}
@@ -4129,14 +4275,13 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 		level--;
 		c = path->nodes[level];
 		if (path->locks[level])
-			btrfs_tree_unlock(c);
+			btrfs_tree_unlock_rw(c, path->locks[level]);
 
 		free_extent_buffer(c);
 		path->nodes[level] = next;
 		path->slots[level] = 0;
 		if (!path->skip_locking)
-			path->locks[level] = 1;
-
+			path->locks[level] = next_rw_lock;
 		if (!level)
 			break;
 
@@ -4151,16 +4296,21 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 		}
 
 		if (!path->skip_locking) {
-			btrfs_assert_tree_locked(path->nodes[level]);
-			ret = btrfs_try_spin_lock(next);
+			ret = btrfs_try_tree_read_lock(next);
 			if (!ret) {
 				btrfs_set_path_blocking(path);
-				btrfs_tree_lock(next);
+				btrfs_tree_read_lock(next);
 				if (!force_blocking)
-					btrfs_clear_path_blocking(path, next);
+					btrfs_clear_path_blocking(path, next,
+							  BTRFS_READ_LOCK);
+			}
+			if (force_blocking) {
+				btrfs_set_lock_blocking_rw(next,
+						   BTRFS_READ_LOCK);
+				next_rw_lock = BTRFS_READ_LOCK_BLOCKING;
+			} else {
+				next_rw_lock = BTRFS_READ_LOCK;
 			}
-			if (force_blocking)
-				btrfs_set_lock_blocking(next);
 		}
 	}
 	ret = 0;

commit a65917156e345946dbde3d7effd28124c6d6a8c2
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Jul 19 12:04:14 2011 -0400

    Btrfs: stop using highmem for extent_buffers
    
    The extent_buffers have a very complex interface where
    we use HIGHMEM for metadata and try to cache a kmap mapping
    to access the memory.
    
    The next commit adds reader/writer locks, and concurrent use
    of this kmap cache would make it even more complex.
    
    This commit drops the ability to use HIGHMEM with extent buffers,
    and rips out all of the related code.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 2e667868e0d2..d2431284b913 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -626,14 +626,6 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 	for (i = start_slot; i < end_slot; i++) {
 		int close = 1;
 
-		if (!parent->map_token) {
-			map_extent_buffer(parent,
-					btrfs_node_key_ptr_offset(i),
-					sizeof(struct btrfs_key_ptr),
-					&parent->map_token, &parent->kaddr,
-					&parent->map_start, &parent->map_len,
-					KM_USER1);
-		}
 		btrfs_node_key(parent, &disk_key, i);
 		if (!progress_passed && comp_keys(&disk_key, progress) < 0)
 			continue;
@@ -656,11 +648,6 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 			last_block = blocknr;
 			continue;
 		}
-		if (parent->map_token) {
-			unmap_extent_buffer(parent, parent->map_token,
-					    KM_USER1);
-			parent->map_token = NULL;
-		}
 
 		cur = btrfs_find_tree_block(root, blocknr, blocksize);
 		if (cur)
@@ -701,11 +688,6 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 		btrfs_tree_unlock(cur);
 		free_extent_buffer(cur);
 	}
-	if (parent->map_token) {
-		unmap_extent_buffer(parent, parent->map_token,
-				    KM_USER1);
-		parent->map_token = NULL;
-	}
 	return err;
 }
 
@@ -746,7 +728,6 @@ static noinline int generic_bin_search(struct extent_buffer *eb,
 	struct btrfs_disk_key *tmp = NULL;
 	struct btrfs_disk_key unaligned;
 	unsigned long offset;
-	char *map_token = NULL;
 	char *kaddr = NULL;
 	unsigned long map_start = 0;
 	unsigned long map_len = 0;
@@ -756,18 +737,13 @@ static noinline int generic_bin_search(struct extent_buffer *eb,
 		mid = (low + high) / 2;
 		offset = p + mid * item_size;
 
-		if (!map_token || offset < map_start ||
+		if (!kaddr || offset < map_start ||
 		    (offset + sizeof(struct btrfs_disk_key)) >
 		    map_start + map_len) {
-			if (map_token) {
-				unmap_extent_buffer(eb, map_token, KM_USER0);
-				map_token = NULL;
-			}
 
 			err = map_private_extent_buffer(eb, offset,
 						sizeof(struct btrfs_disk_key),
-						&map_token, &kaddr,
-						&map_start, &map_len, KM_USER0);
+						&kaddr, &map_start, &map_len);
 
 			if (!err) {
 				tmp = (struct btrfs_disk_key *)(kaddr + offset -
@@ -790,14 +766,10 @@ static noinline int generic_bin_search(struct extent_buffer *eb,
 			high = mid;
 		else {
 			*slot = mid;
-			if (map_token)
-				unmap_extent_buffer(eb, map_token, KM_USER0);
 			return 0;
 		}
 	}
 	*slot = low;
-	if (map_token)
-		unmap_extent_buffer(eb, map_token, KM_USER0);
 	return 1;
 }
 
@@ -1228,7 +1200,6 @@ static void reada_for_search(struct btrfs_root *root,
 	u32 nr;
 	u32 blocksize;
 	u32 nscan = 0;
-	bool map = true;
 
 	if (level != 1)
 		return;
@@ -1250,19 +1221,8 @@ static void reada_for_search(struct btrfs_root *root,
 
 	nritems = btrfs_header_nritems(node);
 	nr = slot;
-	if (node->map_token || path->skip_locking)
-		map = false;
 
 	while (1) {
-		if (map && !node->map_token) {
-			unsigned long offset = btrfs_node_key_ptr_offset(nr);
-			map_private_extent_buffer(node, offset,
-						  sizeof(struct btrfs_key_ptr),
-						  &node->map_token,
-						  &node->kaddr,
-						  &node->map_start,
-						  &node->map_len, KM_USER1);
-		}
 		if (direction < 0) {
 			if (nr == 0)
 				break;
@@ -1281,11 +1241,6 @@ static void reada_for_search(struct btrfs_root *root,
 		if ((search <= target && target - search <= 65536) ||
 		    (search > target && search - target <= 65536)) {
 			gen = btrfs_node_ptr_generation(node, nr);
-			if (map && node->map_token) {
-				unmap_extent_buffer(node, node->map_token,
-						    KM_USER1);
-				node->map_token = NULL;
-			}
 			readahead_tree_block(root, search, blocksize, gen);
 			nread += blocksize;
 		}
@@ -1293,10 +1248,6 @@ static void reada_for_search(struct btrfs_root *root,
 		if ((nread > 65536 || nscan > 32))
 			break;
 	}
-	if (map && node->map_token) {
-		unmap_extent_buffer(node, node->map_token, KM_USER1);
-		node->map_token = NULL;
-	}
 }
 
 /*
@@ -2253,14 +2204,6 @@ static noinline int __push_leaf_right(struct btrfs_trans_handle *trans,
 		if (path->slots[0] == i)
 			push_space += data_size;
 
-		if (!left->map_token) {
-			map_extent_buffer(left, (unsigned long)item,
-					sizeof(struct btrfs_item),
-					&left->map_token, &left->kaddr,
-					&left->map_start, &left->map_len,
-					KM_USER1);
-		}
-
 		this_item_size = btrfs_item_size(left, item);
 		if (this_item_size + sizeof(*item) + push_space > free_space)
 			break;
@@ -2271,10 +2214,6 @@ static noinline int __push_leaf_right(struct btrfs_trans_handle *trans,
 			break;
 		i--;
 	}
-	if (left->map_token) {
-		unmap_extent_buffer(left, left->map_token, KM_USER1);
-		left->map_token = NULL;
-	}
 
 	if (push_items == 0)
 		goto out_unlock;
@@ -2316,21 +2255,10 @@ static noinline int __push_leaf_right(struct btrfs_trans_handle *trans,
 	push_space = BTRFS_LEAF_DATA_SIZE(root);
 	for (i = 0; i < right_nritems; i++) {
 		item = btrfs_item_nr(right, i);
-		if (!right->map_token) {
-			map_extent_buffer(right, (unsigned long)item,
-					sizeof(struct btrfs_item),
-					&right->map_token, &right->kaddr,
-					&right->map_start, &right->map_len,
-					KM_USER1);
-		}
 		push_space -= btrfs_item_size(right, item);
 		btrfs_set_item_offset(right, item, push_space);
 	}
 
-	if (right->map_token) {
-		unmap_extent_buffer(right, right->map_token, KM_USER1);
-		right->map_token = NULL;
-	}
 	left_nritems -= push_items;
 	btrfs_set_header_nritems(left, left_nritems);
 
@@ -2467,13 +2395,6 @@ static noinline int __push_leaf_left(struct btrfs_trans_handle *trans,
 
 	for (i = 0; i < nr; i++) {
 		item = btrfs_item_nr(right, i);
-		if (!right->map_token) {
-			map_extent_buffer(right, (unsigned long)item,
-					sizeof(struct btrfs_item),
-					&right->map_token, &right->kaddr,
-					&right->map_start, &right->map_len,
-					KM_USER1);
-		}
 
 		if (!empty && push_items > 0) {
 			if (path->slots[0] < i)
@@ -2496,11 +2417,6 @@ static noinline int __push_leaf_left(struct btrfs_trans_handle *trans,
 		push_space += this_item_size + sizeof(*item);
 	}
 
-	if (right->map_token) {
-		unmap_extent_buffer(right, right->map_token, KM_USER1);
-		right->map_token = NULL;
-	}
-
 	if (push_items == 0) {
 		ret = 1;
 		goto out;
@@ -2530,23 +2446,12 @@ static noinline int __push_leaf_left(struct btrfs_trans_handle *trans,
 		u32 ioff;
 
 		item = btrfs_item_nr(left, i);
-		if (!left->map_token) {
-			map_extent_buffer(left, (unsigned long)item,
-					sizeof(struct btrfs_item),
-					&left->map_token, &left->kaddr,
-					&left->map_start, &left->map_len,
-					KM_USER1);
-		}
 
 		ioff = btrfs_item_offset(left, item);
 		btrfs_set_item_offset(left, item,
 		      ioff - (BTRFS_LEAF_DATA_SIZE(root) - old_left_item_size));
 	}
 	btrfs_set_header_nritems(left, old_left_nritems + push_items);
-	if (left->map_token) {
-		unmap_extent_buffer(left, left->map_token, KM_USER1);
-		left->map_token = NULL;
-	}
 
 	/* fixup right node */
 	if (push_items > right_nritems) {
@@ -2574,21 +2479,9 @@ static noinline int __push_leaf_left(struct btrfs_trans_handle *trans,
 	for (i = 0; i < right_nritems; i++) {
 		item = btrfs_item_nr(right, i);
 
-		if (!right->map_token) {
-			map_extent_buffer(right, (unsigned long)item,
-					sizeof(struct btrfs_item),
-					&right->map_token, &right->kaddr,
-					&right->map_start, &right->map_len,
-					KM_USER1);
-		}
-
 		push_space = push_space - btrfs_item_size(right, item);
 		btrfs_set_item_offset(right, item, push_space);
 	}
-	if (right->map_token) {
-		unmap_extent_buffer(right, right->map_token, KM_USER1);
-		right->map_token = NULL;
-	}
 
 	btrfs_mark_buffer_dirty(left);
 	if (right_nritems)
@@ -2729,23 +2622,10 @@ static noinline int copy_for_split(struct btrfs_trans_handle *trans,
 		struct btrfs_item *item = btrfs_item_nr(right, i);
 		u32 ioff;
 
-		if (!right->map_token) {
-			map_extent_buffer(right, (unsigned long)item,
-					sizeof(struct btrfs_item),
-					&right->map_token, &right->kaddr,
-					&right->map_start, &right->map_len,
-					KM_USER1);
-		}
-
 		ioff = btrfs_item_offset(right, item);
 		btrfs_set_item_offset(right, item, ioff + rt_data_off);
 	}
 
-	if (right->map_token) {
-		unmap_extent_buffer(right, right->map_token, KM_USER1);
-		right->map_token = NULL;
-	}
-
 	btrfs_set_header_nritems(l, mid);
 	ret = 0;
 	btrfs_item_key(right, &disk_key, 0);
@@ -3264,23 +3144,10 @@ int btrfs_truncate_item(struct btrfs_trans_handle *trans,
 		u32 ioff;
 		item = btrfs_item_nr(leaf, i);
 
-		if (!leaf->map_token) {
-			map_extent_buffer(leaf, (unsigned long)item,
-					sizeof(struct btrfs_item),
-					&leaf->map_token, &leaf->kaddr,
-					&leaf->map_start, &leaf->map_len,
-					KM_USER1);
-		}
-
 		ioff = btrfs_item_offset(leaf, item);
 		btrfs_set_item_offset(leaf, item, ioff + size_diff);
 	}
 
-	if (leaf->map_token) {
-		unmap_extent_buffer(leaf, leaf->map_token, KM_USER1);
-		leaf->map_token = NULL;
-	}
-
 	/* shift the data */
 	if (from_end) {
 		memmove_extent_buffer(leaf, btrfs_leaf_data(leaf) +
@@ -3377,22 +3244,10 @@ int btrfs_extend_item(struct btrfs_trans_handle *trans,
 		u32 ioff;
 		item = btrfs_item_nr(leaf, i);
 
-		if (!leaf->map_token) {
-			map_extent_buffer(leaf, (unsigned long)item,
-					sizeof(struct btrfs_item),
-					&leaf->map_token, &leaf->kaddr,
-					&leaf->map_start, &leaf->map_len,
-					KM_USER1);
-		}
 		ioff = btrfs_item_offset(leaf, item);
 		btrfs_set_item_offset(leaf, item, ioff - data_size);
 	}
 
-	if (leaf->map_token) {
-		unmap_extent_buffer(leaf, leaf->map_token, KM_USER1);
-		leaf->map_token = NULL;
-	}
-
 	/* shift the data */
 	memmove_extent_buffer(leaf, btrfs_leaf_data(leaf) +
 		      data_end - data_size, btrfs_leaf_data(leaf) +
@@ -3494,27 +3349,13 @@ int btrfs_insert_some_items(struct btrfs_trans_handle *trans,
 		 * item0..itemN ... dataN.offset..dataN.size .. data0.size
 		 */
 		/* first correct the data pointers */
-		WARN_ON(leaf->map_token);
 		for (i = slot; i < nritems; i++) {
 			u32 ioff;
 
 			item = btrfs_item_nr(leaf, i);
-			if (!leaf->map_token) {
-				map_extent_buffer(leaf, (unsigned long)item,
-					sizeof(struct btrfs_item),
-					&leaf->map_token, &leaf->kaddr,
-					&leaf->map_start, &leaf->map_len,
-					KM_USER1);
-			}
-
 			ioff = btrfs_item_offset(leaf, item);
 			btrfs_set_item_offset(leaf, item, ioff - total_data);
 		}
-		if (leaf->map_token) {
-			unmap_extent_buffer(leaf, leaf->map_token, KM_USER1);
-			leaf->map_token = NULL;
-		}
-
 		/* shift the items */
 		memmove_extent_buffer(leaf, btrfs_item_nr_offset(slot + nr),
 			      btrfs_item_nr_offset(slot),
@@ -3608,27 +3449,13 @@ int setup_items_for_insert(struct btrfs_trans_handle *trans,
 		 * item0..itemN ... dataN.offset..dataN.size .. data0.size
 		 */
 		/* first correct the data pointers */
-		WARN_ON(leaf->map_token);
 		for (i = slot; i < nritems; i++) {
 			u32 ioff;
 
 			item = btrfs_item_nr(leaf, i);
-			if (!leaf->map_token) {
-				map_extent_buffer(leaf, (unsigned long)item,
-					sizeof(struct btrfs_item),
-					&leaf->map_token, &leaf->kaddr,
-					&leaf->map_start, &leaf->map_len,
-					KM_USER1);
-			}
-
 			ioff = btrfs_item_offset(leaf, item);
 			btrfs_set_item_offset(leaf, item, ioff - total_data);
 		}
-		if (leaf->map_token) {
-			unmap_extent_buffer(leaf, leaf->map_token, KM_USER1);
-			leaf->map_token = NULL;
-		}
-
 		/* shift the items */
 		memmove_extent_buffer(leaf, btrfs_item_nr_offset(slot + nr),
 			      btrfs_item_nr_offset(slot),
@@ -3840,22 +3667,10 @@ int btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 			u32 ioff;
 
 			item = btrfs_item_nr(leaf, i);
-			if (!leaf->map_token) {
-				map_extent_buffer(leaf, (unsigned long)item,
-					sizeof(struct btrfs_item),
-					&leaf->map_token, &leaf->kaddr,
-					&leaf->map_start, &leaf->map_len,
-					KM_USER1);
-			}
 			ioff = btrfs_item_offset(leaf, item);
 			btrfs_set_item_offset(leaf, item, ioff + dsize);
 		}
 
-		if (leaf->map_token) {
-			unmap_extent_buffer(leaf, leaf->map_token, KM_USER1);
-			leaf->map_token = NULL;
-		}
-
 		memmove_extent_buffer(leaf, btrfs_item_nr_offset(slot),
 			      btrfs_item_nr_offset(slot + nr),
 			      sizeof(struct btrfs_item) *

commit 25b8b936ed44814a5ce6fc3b2a21401f33cd56f6
Author: Josef Bacik <josef@redhat.com>
Date:   Wed Jun 8 14:36:54 2011 -0400

    Btrfs: don't map extent buffer if path->skip_locking is set
    
    Arne's scrub stuff exposed a problem with mapping the extent buffer in
    reada_for_search.  He searches the commit root with multiple threads and with
    skip_locking set, so we can race and overwrite node->map_token since node isn't
    locked.  So fix this so that we only map the extent buffer if we don't already
    have a map_token and skip_locking isn't set.  Without this patch scrub would
    panic almost immediately, with the patch it doesn't panic anymore.  Thanks,
    
    Reported-by: Arne Jansen <sensille@gmx.net>
    Signed-off-by: Josef Bacik <josef@redhat.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index d84089349c82..2e667868e0d2 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1228,6 +1228,7 @@ static void reada_for_search(struct btrfs_root *root,
 	u32 nr;
 	u32 blocksize;
 	u32 nscan = 0;
+	bool map = true;
 
 	if (level != 1)
 		return;
@@ -1249,8 +1250,11 @@ static void reada_for_search(struct btrfs_root *root,
 
 	nritems = btrfs_header_nritems(node);
 	nr = slot;
+	if (node->map_token || path->skip_locking)
+		map = false;
+
 	while (1) {
-		if (!node->map_token) {
+		if (map && !node->map_token) {
 			unsigned long offset = btrfs_node_key_ptr_offset(nr);
 			map_private_extent_buffer(node, offset,
 						  sizeof(struct btrfs_key_ptr),
@@ -1277,7 +1281,7 @@ static void reada_for_search(struct btrfs_root *root,
 		if ((search <= target && target - search <= 65536) ||
 		    (search > target && search - target <= 65536)) {
 			gen = btrfs_node_ptr_generation(node, nr);
-			if (node->map_token) {
+			if (map && node->map_token) {
 				unmap_extent_buffer(node, node->map_token,
 						    KM_USER1);
 				node->map_token = NULL;
@@ -1289,7 +1293,7 @@ static void reada_for_search(struct btrfs_root *root,
 		if ((nread > 65536 || nscan > 32))
 			break;
 	}
-	if (node->map_token) {
+	if (map && node->map_token) {
 		unmap_extent_buffer(node, node->map_token, KM_USER1);
 		node->map_token = NULL;
 	}

commit ff5714cca971848963b87d6b477c16ca8abbaa54
Merge: 174ba50915b0 d90c732122a1
Author: Chris Mason <chris.mason@oracle.com>
Date:   Sat May 28 07:00:39 2011 -0400

    Merge branch 'for-chris' of
    git://git.kernel.org/pub/scm/linux/kernel/git/josef/btrfs-work into for-linus
    
    Conflicts:
            fs/btrfs/disk-io.c
            fs/btrfs/extent-tree.c
            fs/btrfs/free-space-cache.c
            fs/btrfs/inode.c
            fs/btrfs/transaction.c
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

commit d6c0cb379c5198487e4ac124728cbb2346d63b1f
Merge: 8e531cdfeb75 1f78160ce1b1
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon May 23 14:37:47 2011 -0400

    Merge branch 'cleanups_and_fixes' into inode_numbers
    
    Conflicts:
            fs/btrfs/tree-log.c
            fs/btrfs/volumes.c
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

commit 1cd307990d6e2b4965620e339a92e0d7ae853e13
Author: Tsutomu Itoh <t-itoh@jp.fujitsu.com>
Date:   Thu May 19 05:19:08 2011 +0000

    Btrfs: BUG_ON is deleted from the caller of btrfs_truncate_item & btrfs_extend_item
    
    Currently, btrfs_truncate_item and btrfs_extend_item returns only 0.
    So, the check by BUG_ON in the caller is unnecessary.
    
    Signed-off-by: Tsutomu Itoh <t-itoh@jp.fujitsu.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 84d7ca1fe0ba..6f1a59cc41ff 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -3216,7 +3216,6 @@ int btrfs_truncate_item(struct btrfs_trans_handle *trans,
 			struct btrfs_path *path,
 			u32 new_size, int from_end)
 {
-	int ret = 0;
 	int slot;
 	struct extent_buffer *leaf;
 	struct btrfs_item *item;
@@ -3314,12 +3313,11 @@ int btrfs_truncate_item(struct btrfs_trans_handle *trans,
 	btrfs_set_item_size(leaf, item, new_size);
 	btrfs_mark_buffer_dirty(leaf);
 
-	ret = 0;
 	if (btrfs_leaf_free_space(root, leaf) < 0) {
 		btrfs_print_leaf(root, leaf);
 		BUG();
 	}
-	return ret;
+	return 0;
 }
 
 /*
@@ -3329,7 +3327,6 @@ int btrfs_extend_item(struct btrfs_trans_handle *trans,
 		      struct btrfs_root *root, struct btrfs_path *path,
 		      u32 data_size)
 {
-	int ret = 0;
 	int slot;
 	struct extent_buffer *leaf;
 	struct btrfs_item *item;
@@ -3394,12 +3391,11 @@ int btrfs_extend_item(struct btrfs_trans_handle *trans,
 	btrfs_set_item_size(leaf, item, old_size + data_size);
 	btrfs_mark_buffer_dirty(leaf);
 
-	ret = 0;
 	if (btrfs_leaf_free_space(root, leaf) < 0) {
 		btrfs_print_leaf(root, leaf);
 		BUG();
 	}
-	return ret;
+	return 0;
 }
 
 /*

commit 026fd317828500524cdc7e5ff9e8e7923abb2868
Author: Josef Bacik <josef@redhat.com>
Date:   Fri May 13 10:32:11 2011 -0400

    Btrfs: don't always do readahead
    
    Our readahead is sort of sloppy, and really isn't always needed.  For example if
    ls is doing a stating ls (which is the default) it's going to stat in non-disk
    order, so if say you have a directory with a stupid amount of files, readahead
    is going to do nothing but waste time in the case of doing the stat.  Taking the
    unconditional readahead out made my test go from 57 minutes to 36 minutes.  This
    means that everywhere we do loop through the tree we want to make sure we do set
    path->reada properly, so I went through and found all of the places where we
    loop through the path and set reada to 1.  Thanks,
    
    Signed-off-by: Josef Bacik <josef@redhat.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index f7a0a64b868f..f61c16c1481a 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -48,8 +48,6 @@ struct btrfs_path *btrfs_alloc_path(void)
 {
 	struct btrfs_path *path;
 	path = kmem_cache_zalloc(btrfs_path_cachep, GFP_NOFS);
-	if (path)
-		path->reada = 1;
 	return path;
 }
 

commit 7e2355ba1a11649f0b212a29fdb9f47476f1248e
Author: Josef Bacik <josef@redhat.com>
Date:   Wed May 11 12:25:37 2011 -0400

    Btrfs: don't look at the extent buffer level 3 times in a row
    
    We have a bit of debugging in btrfs_search_slot to make sure the level of the
    cow block is the same as the original block we were cow'ing.  I don't think I've
    ever seen this tripped, so kill it.  This saves us 2 kmap's per level in our
    search.  Thanks,
    
    Signed-off-by: Josef Bacik <josef@redhat.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 009bcf7f1e4b..f7a0a64b868f 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1672,9 +1672,6 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 		}
 cow_done:
 		BUG_ON(!cow && ins_len);
-		if (level != btrfs_header_level(b))
-			WARN_ON(1);
-		level = btrfs_header_level(b);
 
 		p->nodes[level] = b;
 		if (!p->skip_locking)

commit cb25c2ea6a79702ab7895b873c6c43e0d3bc3c72
Author: Josef Bacik <josef@redhat.com>
Date:   Wed May 11 12:17:34 2011 -0400

    Btrfs: map the node block when looking for readahead targets
    
    If we have particularly full nodes, we could call btrfs_node_blockptr up to 32
    times, which is 32 pairs of kmap/kunmap, which _sucks_.  So go ahead and map the
    extent buffer while we look for readahead targets.  Thanks,
    
    Signed-off-by: Josef Bacik <josef@redhat.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 84d7ca1fe0ba..009bcf7f1e4b 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1229,6 +1229,7 @@ static void reada_for_search(struct btrfs_root *root,
 	u64 search;
 	u64 target;
 	u64 nread = 0;
+	u64 gen;
 	int direction = path->reada;
 	struct extent_buffer *eb;
 	u32 nr;
@@ -1256,6 +1257,15 @@ static void reada_for_search(struct btrfs_root *root,
 	nritems = btrfs_header_nritems(node);
 	nr = slot;
 	while (1) {
+		if (!node->map_token) {
+			unsigned long offset = btrfs_node_key_ptr_offset(nr);
+			map_private_extent_buffer(node, offset,
+						  sizeof(struct btrfs_key_ptr),
+						  &node->map_token,
+						  &node->kaddr,
+						  &node->map_start,
+						  &node->map_len, KM_USER1);
+		}
 		if (direction < 0) {
 			if (nr == 0)
 				break;
@@ -1273,14 +1283,23 @@ static void reada_for_search(struct btrfs_root *root,
 		search = btrfs_node_blockptr(node, nr);
 		if ((search <= target && target - search <= 65536) ||
 		    (search > target && search - target <= 65536)) {
-			readahead_tree_block(root, search, blocksize,
-				     btrfs_node_ptr_generation(node, nr));
+			gen = btrfs_node_ptr_generation(node, nr);
+			if (node->map_token) {
+				unmap_extent_buffer(node, node->map_token,
+						    KM_USER1);
+				node->map_token = NULL;
+			}
+			readahead_tree_block(root, search, blocksize, gen);
 			nread += blocksize;
 		}
 		nscan++;
 		if ((nread > 65536 || nscan > 32))
 			break;
 	}
+	if (node->map_token) {
+		unmap_extent_buffer(node, node->map_token, KM_USER1);
+		node->map_token = NULL;
+	}
 }
 
 /*

commit 945d8962ceee6bb273365d0bdf42f763225b290f
Merge: 0d0ca30f1809 4ea028859bbd
Author: Chris Mason <chris.mason@oracle.com>
Date:   Sun May 22 12:33:42 2011 -0400

    Merge branch 'cleanups' of git://repo.or.cz/linux-2.6/btrfs-unstable into inode_numbers
    
    Conflicts:
            fs/btrfs/extent-tree.c
            fs/btrfs/free-space-cache.c
            fs/btrfs/inode.c
            fs/btrfs/tree-log.c
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

commit 16cdcec736cd214350cdb591bf1091f8beedefa0
Author: Miao Xie <miaox@cn.fujitsu.com>
Date:   Fri Apr 22 18:12:22 2011 +0800

    btrfs: implement delayed inode items operation
    
    Changelog V5 -> V6:
    - Fix oom when the memory load is high, by storing the delayed nodes into the
      root's radix tree, and letting btrfs inodes go.
    
    Changelog V4 -> V5:
    - Fix the race on adding the delayed node to the inode, which is spotted by
      Chris Mason.
    - Merge Chris Mason's incremental patch into this patch.
    - Fix deadlock between readdir() and memory fault, which is reported by
      Itaru Kitayama.
    
    Changelog V3 -> V4:
    - Fix nested lock, which is reported by Itaru Kitayama, by updating space cache
      inode in time.
    
    Changelog V2 -> V3:
    - Fix the race between the delayed worker and the task which does delayed items
      balance, which is reported by Tsutomu Itoh.
    - Modify the patch address David Sterba's comment.
    - Fix the bug of the cpu recursion spinlock, reported by Chris Mason
    
    Changelog V1 -> V2:
    - break up the global rb-tree, use a list to manage the delayed nodes,
      which is created for every directory and file, and used to manage the
      delayed directory name index items and the delayed inode item.
    - introduce a worker to deal with the delayed nodes.
    
    Compare with Ext3/4, the performance of file creation and deletion on btrfs
    is very poor. the reason is that btrfs must do a lot of b+ tree insertions,
    such as inode item, directory name item, directory name index and so on.
    
    If we can do some delayed b+ tree insertion or deletion, we can improve the
    performance, so we made this patch which implemented delayed directory name
    index insertion/deletion and delayed inode update.
    
    Implementation:
    - introduce a delayed root object into the filesystem, that use two lists to
      manage the delayed nodes which are created for every file/directory.
      One is used to manage all the delayed nodes that have delayed items. And the
      other is used to manage the delayed nodes which is waiting to be dealt with
      by the work thread.
    - Every delayed node has two rb-tree, one is used to manage the directory name
      index which is going to be inserted into b+ tree, and the other is used to
      manage the directory name index which is going to be deleted from b+ tree.
    - introduce a worker to deal with the delayed operation. This worker is used
      to deal with the works of the delayed directory name index items insertion
      and deletion and the delayed inode update.
      When the delayed items is beyond the lower limit, we create works for some
      delayed nodes and insert them into the work queue of the worker, and then
      go back.
      When the delayed items is beyond the upper bound, we create works for all
      the delayed nodes that haven't been dealt with, and insert them into the work
      queue of the worker, and then wait for that the untreated items is below some
      threshold value.
    - When we want to insert a directory name index into b+ tree, we just add the
      information into the delayed inserting rb-tree.
      And then we check the number of the delayed items and do delayed items
      balance. (The balance policy is above.)
    - When we want to delete a directory name index from the b+ tree, we search it
      in the inserting rb-tree at first. If we look it up, just drop it. If not,
      add the key of it into the delayed deleting rb-tree.
      Similar to the delayed inserting rb-tree, we also check the number of the
      delayed items and do delayed items balance.
      (The same to inserting manipulation)
    - When we want to update the metadata of some inode, we cached the data of the
      inode into the delayed node. the worker will flush it into the b+ tree after
      dealing with the delayed insertion and deletion.
    - We will move the delayed node to the tail of the list after we access the
      delayed node, By this way, we can cache more delayed items and merge more
      inode updates.
    - If we want to commit transaction, we will deal with all the delayed node.
    - the delayed node will be freed when we free the btrfs inode.
    - Before we log the inode items, we commit all the directory name index items
      and the delayed inode update.
    
    I did a quick test by the benchmark tool[1] and found we can improve the
    performance of file creation by ~15%, and file deletion by ~20%.
    
    Before applying this patch:
    Create files:
            Total files: 50000
            Total time: 1.096108
            Average time: 0.000022
    Delete files:
            Total files: 50000
            Total time: 1.510403
            Average time: 0.000030
    
    After applying this patch:
    Create files:
            Total files: 50000
            Total time: 0.932899
            Average time: 0.000019
    Delete files:
            Total files: 50000
            Total time: 1.215732
            Average time: 0.000024
    
    [1] http://marc.info/?l=linux-btrfs&m=128212635122920&q=p3
    
    Many thanks for Kitayama-san's help!
    
    Signed-off-by: Miao Xie <miaox@cn.fujitsu.com>
    Reviewed-by: David Sterba <dave@jikos.cz>
    Tested-by: Tsutomu Itoh <t-itoh@jp.fujitsu.com>
    Tested-by: Itaru Kitayama <kitayama@cl.bb4u.ne.jp>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 84d7ca1fe0ba..2736b6b2ff5f 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -38,11 +38,6 @@ static int balance_node_right(struct btrfs_trans_handle *trans,
 			      struct extent_buffer *src_buf);
 static int del_ptr(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 		   struct btrfs_path *path, int level, int slot);
-static int setup_items_for_insert(struct btrfs_trans_handle *trans,
-			struct btrfs_root *root, struct btrfs_path *path,
-			struct btrfs_key *cpu_key, u32 *data_size,
-			u32 total_data, u32 total_size, int nr);
-
 
 struct btrfs_path *btrfs_alloc_path(void)
 {
@@ -3559,11 +3554,10 @@ int btrfs_insert_some_items(struct btrfs_trans_handle *trans,
  * to save stack depth by doing the bulk of the work in a function
  * that doesn't call btrfs_search_slot
  */
-static noinline_for_stack int
-setup_items_for_insert(struct btrfs_trans_handle *trans,
-		      struct btrfs_root *root, struct btrfs_path *path,
-		      struct btrfs_key *cpu_key, u32 *data_size,
-		      u32 total_data, u32 total_size, int nr)
+int setup_items_for_insert(struct btrfs_trans_handle *trans,
+			   struct btrfs_root *root, struct btrfs_path *path,
+			   struct btrfs_key *cpu_key, u32 *data_size,
+			   u32 total_data, u32 total_size, int nr)
 {
 	struct btrfs_item *item;
 	int i;

commit b3b4aa74b58bded927f579fff787fb6fa1c0393c
Author: David Sterba <dsterba@suse.cz>
Date:   Thu Apr 21 01:20:15 2011 +0200

    btrfs: drop unused parameter from btrfs_release_path
    
    parameter tree root it's not used since commit
    5f39d397dfbe140a14edecd4e73c34ce23c4f9ee ("Btrfs: Create extent_buffer
    interface for large blocksizes")
    
    Signed-off-by: David Sterba <dsterba@suse.cz>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index a36c87db4dc4..fad8f23d70f0 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -107,7 +107,7 @@ void btrfs_free_path(struct btrfs_path *p)
 {
 	if (!p)
 		return;
-	btrfs_release_path(NULL, p);
+	btrfs_release_path(p);
 	kmem_cache_free(btrfs_path_cachep, p);
 }
 
@@ -117,7 +117,7 @@ void btrfs_free_path(struct btrfs_path *p)
  *
  * It is safe to call this on paths that no locks or extent buffers held.
  */
-noinline void btrfs_release_path(struct btrfs_root *root, struct btrfs_path *p)
+noinline void btrfs_release_path(struct btrfs_path *p)
 {
 	int i;
 
@@ -1328,7 +1328,7 @@ static noinline int reada_for_balance(struct btrfs_root *root,
 		ret = -EAGAIN;
 
 		/* release the whole path */
-		btrfs_release_path(root, path);
+		btrfs_release_path(path);
 
 		/* read the blocks */
 		if (block1)
@@ -1475,7 +1475,7 @@ read_block_for_search(struct btrfs_trans_handle *trans,
 				return 0;
 			}
 			free_extent_buffer(tmp);
-			btrfs_release_path(NULL, p);
+			btrfs_release_path(p);
 			return -EIO;
 		}
 	}
@@ -1494,7 +1494,7 @@ read_block_for_search(struct btrfs_trans_handle *trans,
 	if (p->reada)
 		reada_for_search(root, p, level, slot, key->objectid);
 
-	btrfs_release_path(NULL, p);
+	btrfs_release_path(p);
 
 	ret = -EAGAIN;
 	tmp = read_tree_block(root, blocknr, blocksize, 0);
@@ -1563,7 +1563,7 @@ setup_nodes_for_search(struct btrfs_trans_handle *trans,
 		}
 		b = p->nodes[level];
 		if (!b) {
-			btrfs_release_path(NULL, p);
+			btrfs_release_path(p);
 			goto again;
 		}
 		BUG_ON(btrfs_header_nritems(b) == 1);
@@ -1753,7 +1753,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 	if (!p->leave_spinning)
 		btrfs_set_path_blocking(p);
 	if (ret < 0)
-		btrfs_release_path(root, p);
+		btrfs_release_path(p);
 	return ret;
 }
 
@@ -3026,7 +3026,7 @@ static noinline int setup_leaf_for_split(struct btrfs_trans_handle *trans,
 				    struct btrfs_file_extent_item);
 		extent_len = btrfs_file_extent_num_bytes(leaf, fi);
 	}
-	btrfs_release_path(root, path);
+	btrfs_release_path(path);
 
 	path->keep_locks = 1;
 	path->search_for_split = 1;
@@ -3948,7 +3948,7 @@ int btrfs_prev_leaf(struct btrfs_root *root, struct btrfs_path *path)
 	else
 		return 1;
 
-	btrfs_release_path(root, path);
+	btrfs_release_path(path);
 	ret = btrfs_search_slot(NULL, root, &key, path, 0, 0);
 	if (ret < 0)
 		return ret;
@@ -4072,7 +4072,7 @@ int btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,
 			sret = btrfs_find_next_key(root, path, min_key, level,
 						  cache_only, min_trans);
 			if (sret == 0) {
-				btrfs_release_path(root, path);
+				btrfs_release_path(path);
 				goto again;
 			} else {
 				goto out;
@@ -4151,7 +4151,7 @@ int btrfs_find_next_key(struct btrfs_root *root, struct btrfs_path *path,
 				btrfs_node_key_to_cpu(c, &cur_key, slot);
 
 			orig_lowest = path->lowest_level;
-			btrfs_release_path(root, path);
+			btrfs_release_path(path);
 			path->lowest_level = level;
 			ret = btrfs_search_slot(NULL, root, &cur_key, path,
 						0, 0);
@@ -4228,7 +4228,7 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 again:
 	level = 1;
 	next = NULL;
-	btrfs_release_path(root, path);
+	btrfs_release_path(path);
 
 	path->keep_locks = 1;
 
@@ -4284,7 +4284,7 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 			goto again;
 
 		if (ret < 0) {
-			btrfs_release_path(root, path);
+			btrfs_release_path(path);
 			goto done;
 		}
 
@@ -4323,7 +4323,7 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 			goto again;
 
 		if (ret < 0) {
-			btrfs_release_path(root, path);
+			btrfs_release_path(path);
 			goto done;
 		}
 

commit 62a45b60923a576170a1a0c309c240d9f40d193d
Author: David Sterba <dsterba@suse.cz>
Date:   Wed Apr 20 15:52:26 2011 +0200

    btrfs: make functions static when possible
    
    Signed-off-by: David Sterba <dsterba@suse.cz>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index c60197b36bc8..a36c87db4dc4 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -74,8 +74,8 @@ noinline void btrfs_set_path_blocking(struct btrfs_path *p)
  * retake all the spinlocks in the path.  You can safely use NULL
  * for held
  */
-noinline void btrfs_clear_path_blocking(struct btrfs_path *p,
-					struct extent_buffer *held)
+static noinline void btrfs_clear_path_blocking(struct btrfs_path *p,
+					       struct extent_buffer *held)
 {
 	int i;
 

commit edc95aec57661c8e568e18f6c3f002aefa07ebc8
Author: David Sterba <dsterba@suse.cz>
Date:   Tue Apr 19 14:31:08 2011 +0200

    btrfs: remove nested duplicate variable declarations
    
    Signed-off-by: David Sterba <dsterba@suse.cz>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 84d7ca1fe0ba..c60197b36bc8 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -3647,7 +3647,6 @@ setup_items_for_insert(struct btrfs_trans_handle *trans,
 
 	ret = 0;
 	if (slot == 0) {
-		struct btrfs_disk_key disk_key;
 		btrfs_cpu_key_to_disk(&disk_key, cpu_key);
 		ret = fixup_low_keys(trans, root, path, &disk_key, 1);
 	}

commit 97d9a8a420444eb5b5c071d4b3b9c4100a7ae015
Author: Tsutomu Itoh <t-itoh@jp.fujitsu.com>
Date:   Thu Mar 24 06:33:21 2011 +0000

    Btrfs: check return value of read_tree_block()
    
    This patch is checking return value of read_tree_block(),
    and if it is NULL, error processing.
    
    Signed-off-by: Tsutomu Itoh <t-itoh@jp.fujitsu.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 4edcbe915736..84d7ca1fe0ba 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -682,6 +682,8 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 			if (!cur) {
 				cur = read_tree_block(root, blocknr,
 							 blocksize, gen);
+				if (!cur)
+					return -EIO;
 			} else if (!uptodate) {
 				btrfs_read_buffer(cur, gen);
 			}
@@ -4087,6 +4089,7 @@ int btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,
 		}
 		btrfs_set_path_blocking(path);
 		cur = read_node_slot(root, cur, slot);
+		BUG_ON(!cur);
 
 		btrfs_tree_lock(cur);
 

commit db5b493ac78e46c7b6bad22cd25d8041564cd8ea
Author: Tsutomu Itoh <t-itoh@jp.fujitsu.com>
Date:   Wed Mar 23 08:14:16 2011 +0000

    Btrfs: cleanup some BUG_ON()
    
    This patch changes some BUG_ON() to the error return.
    (but, most callers still use BUG_ON())
    
    Signed-off-by: Tsutomu Itoh <t-itoh@jp.fujitsu.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 465b5d7d6b48..4edcbe915736 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -3709,7 +3709,8 @@ int btrfs_insert_item(struct btrfs_trans_handle *trans, struct btrfs_root
 	unsigned long ptr;
 
 	path = btrfs_alloc_path();
-	BUG_ON(!path);
+	if (!path)
+		return -ENOMEM;
 	ret = btrfs_insert_empty_item(trans, root, path, cpu_key, data_size);
 	if (!ret) {
 		leaf = path->nodes[0];

commit 1abe9b8a138c9988ba8f7bfded6453649a31541f
Author: liubo <liubo2009@cn.fujitsu.com>
Date:   Thu Mar 24 11:18:59 2011 +0000

    Btrfs: add initial tracepoint support for btrfs
    
    Tracepoints can provide insight into why btrfs hits bugs and be greatly
    helpful for debugging, e.g
                  dd-7822  [000]  2121.641088: btrfs_inode_request: root = 5(FS_TREE), gen = 4, ino = 256, blocks = 8, disk_i_size = 0, last_trans = 8, logged_trans = 0
                  dd-7822  [000]  2121.641100: btrfs_inode_new: root = 5(FS_TREE), gen = 8, ino = 257, blocks = 0, disk_i_size = 0, last_trans = 0, logged_trans = 0
     btrfs-transacti-7804  [001]  2146.935420: btrfs_cow_block: root = 2(EXTENT_TREE), refs = 2, orig_buf = 29368320 (orig_level = 0), cow_buf = 29388800 (cow_level = 0)
     btrfs-transacti-7804  [001]  2146.935473: btrfs_cow_block: root = 1(ROOT_TREE), refs = 2, orig_buf = 29364224 (orig_level = 0), cow_buf = 29392896 (cow_level = 0)
     btrfs-transacti-7804  [001]  2146.972221: btrfs_transaction_commit: root = 1(ROOT_TREE), gen = 8
       flush-btrfs-2-7821  [001]  2155.824210: btrfs_chunk_alloc: root = 3(CHUNK_TREE), offset = 1103101952, size = 1073741824, num_stripes = 1, sub_stripes = 0, type = DATA
       flush-btrfs-2-7821  [001]  2155.824241: btrfs_cow_block: root = 2(EXTENT_TREE), refs = 2, orig_buf = 29388800 (orig_level = 0), cow_buf = 29396992 (cow_level = 0)
       flush-btrfs-2-7821  [001]  2155.824255: btrfs_cow_block: root = 4(DEV_TREE), refs = 2, orig_buf = 29372416 (orig_level = 0), cow_buf = 29401088 (cow_level = 0)
       flush-btrfs-2-7821  [000]  2155.824329: btrfs_cow_block: root = 3(CHUNK_TREE), refs = 2, orig_buf = 20971520 (orig_level = 0), cow_buf = 20975616 (cow_level = 0)
     btrfs-endio-wri-7800  [001]  2155.898019: btrfs_cow_block: root = 5(FS_TREE), refs = 2, orig_buf = 29384704 (orig_level = 0), cow_buf = 29405184 (cow_level = 0)
     btrfs-endio-wri-7800  [001]  2155.898043: btrfs_cow_block: root = 7(CSUM_TREE), refs = 2, orig_buf = 29376512 (orig_level = 0), cow_buf = 29409280 (cow_level = 0)
    
    Here is what I have added:
    
    1) ordere_extent:
            btrfs_ordered_extent_add
            btrfs_ordered_extent_remove
            btrfs_ordered_extent_start
            btrfs_ordered_extent_put
    
    These provide critical information to understand how ordered_extents are
    updated.
    
    2) extent_map:
            btrfs_get_extent
    
    extent_map is used in both read and write cases, and it is useful for tracking
    how btrfs specific IO is running.
    
    3) writepage:
            __extent_writepage
            btrfs_writepage_end_io_hook
    
    Pages are cirtical resourses and produce a lot of corner cases during writeback,
    so it is valuable to know how page is written to disk.
    
    4) inode:
            btrfs_inode_new
            btrfs_inode_request
            btrfs_inode_evict
    
    These can show where and when a inode is created, when a inode is evicted.
    
    5) sync:
            btrfs_sync_file
            btrfs_sync_fs
    
    These show sync arguments.
    
    6) transaction:
            btrfs_transaction_commit
    
    In transaction based filesystem, it will be useful to know the generation and
    who does commit.
    
    7) back reference and cow:
            btrfs_delayed_tree_ref
            btrfs_delayed_data_ref
            btrfs_delayed_ref_head
            btrfs_cow_block
    
    Btrfs natively supports back references, these tracepoints are helpful on
    understanding btrfs's COW mechanism.
    
    8) chunk:
            btrfs_chunk_alloc
            btrfs_chunk_free
    
    Chunk is a link between physical offset and logical offset, and stands for space
    infomation in btrfs, and these are helpful on tracing space things.
    
    9) reserved_extent:
            btrfs_reserved_extent_alloc
            btrfs_reserved_extent_free
    
    These can show how btrfs uses its space.
    
    Signed-off-by: Liu Bo <liubo2009@cn.fujitsu.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 8680110f0a5a..465b5d7d6b48 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -535,6 +535,9 @@ noinline int btrfs_cow_block(struct btrfs_trans_handle *trans,
 
 	ret = __btrfs_cow_block(trans, root, buf, parent,
 				 parent_slot, cow_ret, search_start, 0);
+
+	trace_btrfs_cow_block(root, buf, *cow_ret);
+
 	return ret;
 }
 

commit 240f62c8756df285da11469259b3900f32883168
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Mar 23 14:54:42 2011 -0400

    Btrfs: use RCU instead of a spinlock to protect the root node
    
    The pointer to the extent buffer for the root of each tree
    is protected by a spinlock so that we can safely read the pointer
    and take a reference on the extent buffer.
    
    But now that the extent buffers are freed via RCU, we can safely
    use rcu_read_lock instead.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 73e53009e126..8680110f0a5a 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -147,10 +147,11 @@ noinline void btrfs_release_path(struct btrfs_root *root, struct btrfs_path *p)
 struct extent_buffer *btrfs_root_node(struct btrfs_root *root)
 {
 	struct extent_buffer *eb;
-	spin_lock(&root->node_lock);
-	eb = root->node;
+
+	rcu_read_lock();
+	eb = rcu_dereference(root->node);
 	extent_buffer_get(eb);
-	spin_unlock(&root->node_lock);
+	rcu_read_unlock();
 	return eb;
 }
 
@@ -165,14 +166,8 @@ struct extent_buffer *btrfs_lock_root_node(struct btrfs_root *root)
 	while (1) {
 		eb = btrfs_root_node(root);
 		btrfs_tree_lock(eb);
-
-		spin_lock(&root->node_lock);
-		if (eb == root->node) {
-			spin_unlock(&root->node_lock);
+		if (eb == root->node)
 			break;
-		}
-		spin_unlock(&root->node_lock);
-
 		btrfs_tree_unlock(eb);
 		free_extent_buffer(eb);
 	}
@@ -458,10 +453,8 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 		else
 			parent_start = 0;
 
-		spin_lock(&root->node_lock);
-		root->node = cow;
 		extent_buffer_get(cow);
-		spin_unlock(&root->node_lock);
+		rcu_assign_pointer(root->node, cow);
 
 		btrfs_free_tree_block(trans, root, buf, parent_start,
 				      last_ref);
@@ -930,9 +923,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 			goto enospc;
 		}
 
-		spin_lock(&root->node_lock);
-		root->node = child;
-		spin_unlock(&root->node_lock);
+		rcu_assign_pointer(root->node, child);
 
 		add_root_to_dirty_list(root);
 		btrfs_tree_unlock(child);
@@ -2007,10 +1998,8 @@ static noinline int insert_new_root(struct btrfs_trans_handle *trans,
 
 	btrfs_mark_buffer_dirty(c);
 
-	spin_lock(&root->node_lock);
 	old = root->node;
-	root->node = c;
-	spin_unlock(&root->node_lock);
+	rcu_assign_pointer(root->node, c);
 
 	/* the super has an extra ref to root->node */
 	free_extent_buffer(old);

commit a826d6dcb32d811b4c81df57a5ef1367516586b0
Author: Josef Bacik <josef@redhat.com>
Date:   Wed Mar 16 13:42:43 2011 -0400

    Btrfs: check items for correctness as we search
    
    Currently if we have corrupted items things will blow up in spectacular ways.
    So as we read in blocks and they are leaves, check the entire leaf to make sure
    all of the items are correct and point to valid parts in the leaf for the item
    data the are responsible for.  If the item is corrupt we will kick back EIO and
    not read any of the copies since they are likely to not be correct either.  This
    will catch generic corruptions, it will be up to the individual callers of
    btrfs_search_slot to make sure their items are right.  Thanks,
    
    Signed-off-by: Josef Bacik <josef@redhat.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index b5baff0dccfe..73e53009e126 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -732,122 +732,6 @@ static inline unsigned int leaf_data_end(struct btrfs_root *root,
 	return btrfs_item_offset_nr(leaf, nr - 1);
 }
 
-/*
- * extra debugging checks to make sure all the items in a key are
- * well formed and in the proper order
- */
-static int check_node(struct btrfs_root *root, struct btrfs_path *path,
-		      int level)
-{
-	struct extent_buffer *parent = NULL;
-	struct extent_buffer *node = path->nodes[level];
-	struct btrfs_disk_key parent_key;
-	struct btrfs_disk_key node_key;
-	int parent_slot;
-	int slot;
-	struct btrfs_key cpukey;
-	u32 nritems = btrfs_header_nritems(node);
-
-	if (path->nodes[level + 1])
-		parent = path->nodes[level + 1];
-
-	slot = path->slots[level];
-	BUG_ON(nritems == 0);
-	if (parent) {
-		parent_slot = path->slots[level + 1];
-		btrfs_node_key(parent, &parent_key, parent_slot);
-		btrfs_node_key(node, &node_key, 0);
-		BUG_ON(memcmp(&parent_key, &node_key,
-			      sizeof(struct btrfs_disk_key)));
-		BUG_ON(btrfs_node_blockptr(parent, parent_slot) !=
-		       btrfs_header_bytenr(node));
-	}
-	BUG_ON(nritems > BTRFS_NODEPTRS_PER_BLOCK(root));
-	if (slot != 0) {
-		btrfs_node_key_to_cpu(node, &cpukey, slot - 1);
-		btrfs_node_key(node, &node_key, slot);
-		BUG_ON(comp_keys(&node_key, &cpukey) <= 0);
-	}
-	if (slot < nritems - 1) {
-		btrfs_node_key_to_cpu(node, &cpukey, slot + 1);
-		btrfs_node_key(node, &node_key, slot);
-		BUG_ON(comp_keys(&node_key, &cpukey) >= 0);
-	}
-	return 0;
-}
-
-/*
- * extra checking to make sure all the items in a leaf are
- * well formed and in the proper order
- */
-static int check_leaf(struct btrfs_root *root, struct btrfs_path *path,
-		      int level)
-{
-	struct extent_buffer *leaf = path->nodes[level];
-	struct extent_buffer *parent = NULL;
-	int parent_slot;
-	struct btrfs_key cpukey;
-	struct btrfs_disk_key parent_key;
-	struct btrfs_disk_key leaf_key;
-	int slot = path->slots[0];
-
-	u32 nritems = btrfs_header_nritems(leaf);
-
-	if (path->nodes[level + 1])
-		parent = path->nodes[level + 1];
-
-	if (nritems == 0)
-		return 0;
-
-	if (parent) {
-		parent_slot = path->slots[level + 1];
-		btrfs_node_key(parent, &parent_key, parent_slot);
-		btrfs_item_key(leaf, &leaf_key, 0);
-
-		BUG_ON(memcmp(&parent_key, &leaf_key,
-		       sizeof(struct btrfs_disk_key)));
-		BUG_ON(btrfs_node_blockptr(parent, parent_slot) !=
-		       btrfs_header_bytenr(leaf));
-	}
-	if (slot != 0 && slot < nritems - 1) {
-		btrfs_item_key(leaf, &leaf_key, slot);
-		btrfs_item_key_to_cpu(leaf, &cpukey, slot - 1);
-		if (comp_keys(&leaf_key, &cpukey) <= 0) {
-			btrfs_print_leaf(root, leaf);
-			printk(KERN_CRIT "slot %d offset bad key\n", slot);
-			BUG_ON(1);
-		}
-		if (btrfs_item_offset_nr(leaf, slot - 1) !=
-		       btrfs_item_end_nr(leaf, slot)) {
-			btrfs_print_leaf(root, leaf);
-			printk(KERN_CRIT "slot %d offset bad\n", slot);
-			BUG_ON(1);
-		}
-	}
-	if (slot < nritems - 1) {
-		btrfs_item_key(leaf, &leaf_key, slot);
-		btrfs_item_key_to_cpu(leaf, &cpukey, slot + 1);
-		BUG_ON(comp_keys(&leaf_key, &cpukey) >= 0);
-		if (btrfs_item_offset_nr(leaf, slot) !=
-			btrfs_item_end_nr(leaf, slot + 1)) {
-			btrfs_print_leaf(root, leaf);
-			printk(KERN_CRIT "slot %d offset bad\n", slot);
-			BUG_ON(1);
-		}
-	}
-	BUG_ON(btrfs_item_offset_nr(leaf, 0) +
-	       btrfs_item_size_nr(leaf, 0) != BTRFS_LEAF_DATA_SIZE(root));
-	return 0;
-}
-
-static noinline int check_block(struct btrfs_root *root,
-				struct btrfs_path *path, int level)
-{
-	return 0;
-	if (level == 0)
-		return check_leaf(root, path, level);
-	return check_node(root, path, level);
-}
 
 /*
  * search for key in the extent_buffer.  The items start at offset p,
@@ -1188,7 +1072,6 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		}
 	}
 	/* double check we haven't messed things up */
-	check_block(root, path, level);
 	if (orig_ptr !=
 	    btrfs_node_blockptr(path->nodes[level], path->slots[level]))
 		BUG();
@@ -1798,12 +1681,6 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 		if (!cow)
 			btrfs_unlock_up_safe(p, level + 1);
 
-		ret = check_block(root, p, level);
-		if (ret) {
-			ret = -1;
-			goto done;
-		}
-
 		ret = bin_search(b, key, level, &slot);
 
 		if (level != 0) {

commit 91ca338d776e0cefb255bf2979b6448febd880f5
Author: Tsutomu Itoh <t-itoh@jp.fujitsu.com>
Date:   Wed Jan 5 02:32:22 2011 +0000

    btrfs: check NULL or not
    
    Should check if functions returns NULL or not.
    
    Signed-off-by: Tsutomu Itoh <t-itoh@jp.fujitsu.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 99599f1c1554..b5baff0dccfe 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2516,6 +2516,9 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 	btrfs_assert_tree_locked(path->nodes[1]);
 
 	right = read_node_slot(root, upper, slot + 1);
+	if (right == NULL)
+		return 1;
+
 	btrfs_tree_lock(right);
 	btrfs_set_lock_blocking(right);
 
@@ -2766,6 +2769,9 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 	btrfs_assert_tree_locked(path->nodes[1]);
 
 	left = read_node_slot(root, path->nodes[1], slot - 1);
+	if (left == NULL)
+		return 1;
+
 	btrfs_tree_lock(left);
 	btrfs_set_lock_blocking(left);
 

commit ff175d57f057f77d2d3031d674c2af9167a4af02
Author: Jesper Juhl <jj@chaosbits.net>
Date:   Sat Dec 25 21:22:30 2010 +0000

    btrfs: Don't pass NULL ptr to func that may deref it.
    
    Hi,
    
    In fs/btrfs/inode.c::fixup_tree_root_location() we have this code:
    
    ...
                    if (!path) {
                            err = -ENOMEM;
                            goto out;
                    }
    ...
            out:
                    btrfs_free_path(path);
                    return err;
    
    btrfs_free_path() passes its argument on to other functions and some of
    them end up dereferencing the pointer.
    In the code above that pointer is clearly NULL, so btrfs_free_path() will
    eventually cause a NULL dereference.
    
    There are many ways to cut this cake (fix the bug). The one I chose was to
    make btrfs_free_path() deal gracefully with NULL pointers. If you
    disagree, feel free to come up with an alternative patch.
    
    Signed-off-by: Jesper Juhl <jj@chaosbits.net>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 9ac171599258..99599f1c1554 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -105,6 +105,8 @@ noinline void btrfs_clear_path_blocking(struct btrfs_path *p,
 /* this also releases the path */
 void btrfs_free_path(struct btrfs_path *p)
 {
+	if (!p)
+		return;
 	btrfs_release_path(NULL, p);
 	kmem_cache_free(btrfs_path_cachep, p);
 }

commit 559af8211433b8c0b20e6c43c61409cb9c9c2996
Author: Andi Kleen <andi@firstfloor.org>
Date:   Fri Oct 29 15:14:37 2010 -0400

    Btrfs: cleanup warnings from gcc 4.6 (nonbugs)
    
    These are all the cases where a variable is set, but not read which are
    not bugs as far as I can see, but simply leftovers.
    
    Still needs more review.
    
    Found by gcc 4.6's new warnings
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Cc: Chris Mason <chris.mason@oracle.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 6921231e0efb..9ac171599258 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -200,7 +200,6 @@ int btrfs_copy_root(struct btrfs_trans_handle *trans,
 		      struct extent_buffer **cow_ret, u64 new_root_objectid)
 {
 	struct extent_buffer *cow;
-	u32 nritems;
 	int ret = 0;
 	int level;
 	struct btrfs_disk_key disk_key;
@@ -210,7 +209,6 @@ int btrfs_copy_root(struct btrfs_trans_handle *trans,
 	WARN_ON(root->ref_cows && trans->transid != root->last_trans);
 
 	level = btrfs_header_level(buf);
-	nritems = btrfs_header_nritems(buf);
 	if (level == 0)
 		btrfs_item_key(buf, &disk_key, 0);
 	else
@@ -1008,7 +1006,6 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 	int wret;
 	int pslot;
 	int orig_slot = path->slots[level];
-	int err_on_enospc = 0;
 	u64 orig_ptr;
 
 	if (level == 0)
@@ -1071,8 +1068,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 	    BTRFS_NODEPTRS_PER_BLOCK(root) / 4)
 		return 0;
 
-	if (btrfs_header_nritems(mid) < 2)
-		err_on_enospc = 1;
+	btrfs_header_nritems(mid);
 
 	left = read_node_slot(root, parent, pslot - 1);
 	if (left) {
@@ -1103,8 +1099,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		wret = push_node_left(trans, root, left, mid, 1);
 		if (wret < 0)
 			ret = wret;
-		if (btrfs_header_nritems(mid) < 2)
-			err_on_enospc = 1;
+		btrfs_header_nritems(mid);
 	}
 
 	/*
@@ -1224,14 +1219,12 @@ static noinline int push_nodes_for_insert(struct btrfs_trans_handle *trans,
 	int wret;
 	int pslot;
 	int orig_slot = path->slots[level];
-	u64 orig_ptr;
 
 	if (level == 0)
 		return 1;
 
 	mid = path->nodes[level];
 	WARN_ON(btrfs_header_generation(mid) != trans->transid);
-	orig_ptr = btrfs_node_blockptr(mid, orig_slot);
 
 	if (level < BTRFS_MAX_LEVEL - 1)
 		parent = path->nodes[level + 1];
@@ -2567,7 +2560,6 @@ static noinline int __push_leaf_left(struct btrfs_trans_handle *trans,
 {
 	struct btrfs_disk_key disk_key;
 	struct extent_buffer *right = path->nodes[0];
-	int slot;
 	int i;
 	int push_space = 0;
 	int push_items = 0;
@@ -2579,8 +2571,6 @@ static noinline int __push_leaf_left(struct btrfs_trans_handle *trans,
 	u32 this_item_size;
 	u32 old_left_item_size;
 
-	slot = path->slots[1];
-
 	if (empty)
 		nr = min(right_nritems, max_slot);
 	else
@@ -3349,7 +3339,6 @@ int btrfs_truncate_item(struct btrfs_trans_handle *trans,
 {
 	int ret = 0;
 	int slot;
-	int slot_orig;
 	struct extent_buffer *leaf;
 	struct btrfs_item *item;
 	u32 nritems;
@@ -3359,7 +3348,6 @@ int btrfs_truncate_item(struct btrfs_trans_handle *trans,
 	unsigned int size_diff;
 	int i;
 
-	slot_orig = path->slots[0];
 	leaf = path->nodes[0];
 	slot = path->slots[0];
 
@@ -3464,7 +3452,6 @@ int btrfs_extend_item(struct btrfs_trans_handle *trans,
 {
 	int ret = 0;
 	int slot;
-	int slot_orig;
 	struct extent_buffer *leaf;
 	struct btrfs_item *item;
 	u32 nritems;
@@ -3473,7 +3460,6 @@ int btrfs_extend_item(struct btrfs_trans_handle *trans,
 	unsigned int old_size;
 	int i;
 
-	slot_orig = path->slots[0];
 	leaf = path->nodes[0];
 
 	nritems = btrfs_header_nritems(leaf);
@@ -3806,7 +3792,6 @@ int btrfs_insert_empty_items(struct btrfs_trans_handle *trans,
 			    struct btrfs_key *cpu_key, u32 *data_size,
 			    int nr)
 {
-	struct extent_buffer *leaf;
 	int ret = 0;
 	int slot;
 	int i;
@@ -3823,7 +3808,6 @@ int btrfs_insert_empty_items(struct btrfs_trans_handle *trans,
 	if (ret < 0)
 		goto out;
 
-	leaf = path->nodes[0];
 	slot = path->slots[0];
 	BUG_ON(slot < 0);
 

commit cb44921a09221f0a90217b44044448f63190f3e5
Author: Chris Mason <chris.mason@oracle.com>
Date:   Sun Oct 24 11:01:27 2010 -0400

    Btrfs: don't loop forever on bad btree blocks
    
    When btrfs discovers the generation number in a btree block is
    incorrect, it can loop forever without forcing the RAID
    code to try a valid mirror, and without returning EIO.
    
    This changes things to properly kick out the EIO.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index c3df14ce2cc2..6921231e0efb 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1577,13 +1577,33 @@ read_block_for_search(struct btrfs_trans_handle *trans,
 	blocksize = btrfs_level_size(root, level - 1);
 
 	tmp = btrfs_find_tree_block(root, blocknr, blocksize);
-	if (tmp && btrfs_buffer_uptodate(tmp, gen)) {
-		/*
-		 * we found an up to date block without sleeping, return
-		 * right away
-		 */
-		*eb_ret = tmp;
-		return 0;
+	if (tmp) {
+		if (btrfs_buffer_uptodate(tmp, 0)) {
+			if (btrfs_buffer_uptodate(tmp, gen)) {
+				/*
+				 * we found an up to date block without
+				 * sleeping, return
+				 * right away
+				 */
+				*eb_ret = tmp;
+				return 0;
+			}
+			/* the pages were up to date, but we failed
+			 * the generation number check.  Do a full
+			 * read for the generation number that is correct.
+			 * We must do this without dropping locks so
+			 * we can trust our generation number
+			 */
+			free_extent_buffer(tmp);
+			tmp = read_tree_block(root, blocknr, blocksize, gen);
+			if (tmp && btrfs_buffer_uptodate(tmp, gen)) {
+				*eb_ret = tmp;
+				return 0;
+			}
+			free_extent_buffer(tmp);
+			btrfs_release_path(NULL, p);
+			return -EIO;
+		}
 	}
 
 	/*
@@ -1596,8 +1616,7 @@ read_block_for_search(struct btrfs_trans_handle *trans,
 	btrfs_unlock_up_safe(p, level + 1);
 	btrfs_set_path_blocking(p);
 
-	if (tmp)
-		free_extent_buffer(tmp);
+	free_extent_buffer(tmp);
 	if (p->reada)
 		reada_for_search(root, p, level, slot, key->objectid);
 

commit 99d8f83c98930100cd70437b0c81a935e7a14b0b
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Jul 7 10:51:48 2010 -0400

    Btrfs: fix split_leaf double split corner case
    
    split_leaf was not properly balancing leaves when it was forced to
    split a leaf twice.  This commit adds an extra push left and right
    before forcing the double split in hopes of getting the slot where
    we want to insert at either the start or end of the leaf.
    
    If the extra pushes do work, then we are able to avoid splitting twice
    and we keep the tree properly balanced.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 0d1d966b0fe4..c3df14ce2cc2 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2304,12 +2304,17 @@ noinline int btrfs_leaf_free_space(struct btrfs_root *root,
 	return ret;
 }
 
+/*
+ * min slot controls the lowest index we're willing to push to the
+ * right.  We'll push up to and including min_slot, but no lower
+ */
 static noinline int __push_leaf_right(struct btrfs_trans_handle *trans,
 				      struct btrfs_root *root,
 				      struct btrfs_path *path,
 				      int data_size, int empty,
 				      struct extent_buffer *right,
-				      int free_space, u32 left_nritems)
+				      int free_space, u32 left_nritems,
+				      u32 min_slot)
 {
 	struct extent_buffer *left = path->nodes[0];
 	struct extent_buffer *upper = path->nodes[1];
@@ -2327,7 +2332,7 @@ static noinline int __push_leaf_right(struct btrfs_trans_handle *trans,
 	if (empty)
 		nr = 0;
 	else
-		nr = 1;
+		nr = max_t(u32, 1, min_slot);
 
 	if (path->slots[0] >= left_nritems)
 		push_space += data_size;
@@ -2469,10 +2474,14 @@ static noinline int __push_leaf_right(struct btrfs_trans_handle *trans,
  *
  * returns 1 if the push failed because the other node didn't have enough
  * room, 0 if everything worked out and < 0 if there were major errors.
+ *
+ * this will push starting from min_slot to the end of the leaf.  It won't
+ * push any slot lower than min_slot
  */
 static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
-			   *root, struct btrfs_path *path, int data_size,
-			   int empty)
+			   *root, struct btrfs_path *path,
+			   int min_data_size, int data_size,
+			   int empty, u32 min_slot)
 {
 	struct extent_buffer *left = path->nodes[0];
 	struct extent_buffer *right;
@@ -2514,8 +2523,8 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 	if (left_nritems == 0)
 		goto out_unlock;
 
-	return __push_leaf_right(trans, root, path, data_size, empty,
-				right, free_space, left_nritems);
+	return __push_leaf_right(trans, root, path, min_data_size, empty,
+				right, free_space, left_nritems, min_slot);
 out_unlock:
 	btrfs_tree_unlock(right);
 	free_extent_buffer(right);
@@ -2525,12 +2534,17 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 /*
  * push some data in the path leaf to the left, trying to free up at
  * least data_size bytes.  returns zero if the push worked, nonzero otherwise
+ *
+ * max_slot can put a limit on how far into the leaf we'll push items.  The
+ * item at 'max_slot' won't be touched.  Use (u32)-1 to make us do all the
+ * items
  */
 static noinline int __push_leaf_left(struct btrfs_trans_handle *trans,
 				     struct btrfs_root *root,
 				     struct btrfs_path *path, int data_size,
 				     int empty, struct extent_buffer *left,
-				     int free_space, int right_nritems)
+				     int free_space, u32 right_nritems,
+				     u32 max_slot)
 {
 	struct btrfs_disk_key disk_key;
 	struct extent_buffer *right = path->nodes[0];
@@ -2549,9 +2563,9 @@ static noinline int __push_leaf_left(struct btrfs_trans_handle *trans,
 	slot = path->slots[1];
 
 	if (empty)
-		nr = right_nritems;
+		nr = min(right_nritems, max_slot);
 	else
-		nr = right_nritems - 1;
+		nr = min(right_nritems - 1, max_slot);
 
 	for (i = 0; i < nr; i++) {
 		item = btrfs_item_nr(right, i);
@@ -2712,10 +2726,14 @@ static noinline int __push_leaf_left(struct btrfs_trans_handle *trans,
 /*
  * push some data in the path leaf to the left, trying to free up at
  * least data_size bytes.  returns zero if the push worked, nonzero otherwise
+ *
+ * max_slot can put a limit on how far into the leaf we'll push items.  The
+ * item at 'max_slot' won't be touched.  Use (u32)-1 to make us push all the
+ * items
  */
 static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
-			  *root, struct btrfs_path *path, int data_size,
-			  int empty)
+			  *root, struct btrfs_path *path, int min_data_size,
+			  int data_size, int empty, u32 max_slot)
 {
 	struct extent_buffer *right = path->nodes[0];
 	struct extent_buffer *left;
@@ -2761,8 +2779,9 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 		goto out;
 	}
 
-	return __push_leaf_left(trans, root, path, data_size,
-			       empty, left, free_space, right_nritems);
+	return __push_leaf_left(trans, root, path, min_data_size,
+			       empty, left, free_space, right_nritems,
+			       max_slot);
 out:
 	btrfs_tree_unlock(left);
 	free_extent_buffer(left);
@@ -2854,6 +2873,64 @@ static noinline int copy_for_split(struct btrfs_trans_handle *trans,
 	return ret;
 }
 
+/*
+ * double splits happen when we need to insert a big item in the middle
+ * of a leaf.  A double split can leave us with 3 mostly empty leaves:
+ * leaf: [ slots 0 - N] [ our target ] [ N + 1 - total in leaf ]
+ *          A                 B                 C
+ *
+ * We avoid this by trying to push the items on either side of our target
+ * into the adjacent leaves.  If all goes well we can avoid the double split
+ * completely.
+ */
+static noinline int push_for_double_split(struct btrfs_trans_handle *trans,
+					  struct btrfs_root *root,
+					  struct btrfs_path *path,
+					  int data_size)
+{
+	int ret;
+	int progress = 0;
+	int slot;
+	u32 nritems;
+
+	slot = path->slots[0];
+
+	/*
+	 * try to push all the items after our slot into the
+	 * right leaf
+	 */
+	ret = push_leaf_right(trans, root, path, 1, data_size, 0, slot);
+	if (ret < 0)
+		return ret;
+
+	if (ret == 0)
+		progress++;
+
+	nritems = btrfs_header_nritems(path->nodes[0]);
+	/*
+	 * our goal is to get our slot at the start or end of a leaf.  If
+	 * we've done so we're done
+	 */
+	if (path->slots[0] == 0 || path->slots[0] == nritems)
+		return 0;
+
+	if (btrfs_leaf_free_space(root, path->nodes[0]) >= data_size)
+		return 0;
+
+	/* try to push all the items before our slot into the next leaf */
+	slot = path->slots[0];
+	ret = push_leaf_left(trans, root, path, 1, data_size, 0, slot);
+	if (ret < 0)
+		return ret;
+
+	if (ret == 0)
+		progress++;
+
+	if (progress)
+		return 0;
+	return 1;
+}
+
 /*
  * split the path's leaf in two, making sure there is at least data_size
  * available for the resulting leaf level of the path.
@@ -2876,6 +2953,7 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 	int wret;
 	int split;
 	int num_doubles = 0;
+	int tried_avoid_double = 0;
 
 	l = path->nodes[0];
 	slot = path->slots[0];
@@ -2884,12 +2962,14 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 		return -EOVERFLOW;
 
 	/* first try to make some room by pushing left and right */
-	if (data_size && ins_key->type != BTRFS_DIR_ITEM_KEY) {
-		wret = push_leaf_right(trans, root, path, data_size, 0);
+	if (data_size) {
+		wret = push_leaf_right(trans, root, path, data_size,
+				       data_size, 0, 0);
 		if (wret < 0)
 			return wret;
 		if (wret) {
-			wret = push_leaf_left(trans, root, path, data_size, 0);
+			wret = push_leaf_left(trans, root, path, data_size,
+					      data_size, 0, (u32)-1);
 			if (wret < 0)
 				return wret;
 		}
@@ -2923,6 +3003,8 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 				if (mid != nritems &&
 				    leaf_space_used(l, mid, nritems - mid) +
 				    data_size > BTRFS_LEAF_DATA_SIZE(root)) {
+					if (data_size && !tried_avoid_double)
+						goto push_for_double;
 					split = 2;
 				}
 			}
@@ -2939,6 +3021,8 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 				if (mid != nritems &&
 				    leaf_space_used(l, mid, nritems - mid) +
 				    data_size > BTRFS_LEAF_DATA_SIZE(root)) {
+					if (data_size && !tried_avoid_double)
+						goto push_for_double;
 					split = 2 ;
 				}
 			}
@@ -3019,6 +3103,13 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 	}
 
 	return ret;
+
+push_for_double:
+	push_for_double_split(trans, root, path, data_size);
+	tried_avoid_double = 1;
+	if (btrfs_leaf_free_space(root, path->nodes[0]) >= data_size)
+		return 0;
+	goto again;
 }
 
 static noinline int setup_leaf_for_split(struct btrfs_trans_handle *trans,
@@ -3915,13 +4006,15 @@ int btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 			extent_buffer_get(leaf);
 
 			btrfs_set_path_blocking(path);
-			wret = push_leaf_left(trans, root, path, 1, 1);
+			wret = push_leaf_left(trans, root, path, 1, 1,
+					      1, (u32)-1);
 			if (wret < 0 && wret != -ENOSPC)
 				ret = wret;
 
 			if (path->nodes[0] == leaf &&
 			    btrfs_header_nritems(leaf)) {
-				wret = push_leaf_right(trans, root, path, 1, 1);
+				wret = push_leaf_right(trans, root, path, 1,
+						       1, 1, 0);
 				if (wret < 0 && wret != -ENOSPC)
 					ret = wret;
 			}

commit 5bdd3536cbbe2ecd94ecc14410c6b1b31da16381
Author: Yan, Zheng <zheng.yan@oracle.com>
Date:   Wed May 26 11:20:30 2010 -0400

    Btrfs: Fix block generation verification race
    
    After the path is released, the generation number got from block
    pointer is no long valid. The race may cause disk corruption, because
    verify_parent_transid() calls clear_extent_buffer_uptodate() when
    generation numbers mismatch.
    
    Signed-off-by: Yan Zheng <zheng.yan@oracle.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index acd532af8e55..0d1d966b0fe4 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1604,7 +1604,7 @@ read_block_for_search(struct btrfs_trans_handle *trans,
 	btrfs_release_path(NULL, p);
 
 	ret = -EAGAIN;
-	tmp = read_tree_block(root, blocknr, blocksize, gen);
+	tmp = read_tree_block(root, blocknr, blocksize, 0);
 	if (tmp) {
 		/*
 		 * If the read above didn't mark this buffer up to date,

commit 3fd0a5585eb98e074fb9934549c8d85c49756c0d
Author: Yan, Zheng <zheng.yan@oracle.com>
Date:   Sun May 16 10:49:59 2010 -0400

    Btrfs: Metadata ENOSPC handling for balance
    
    This patch adds metadata ENOSPC handling for the balance code.
    It is consisted by following major changes:
    
    1. Avoid COW tree leave in the phrase of merging tree.
    
    2. Handle interaction with snapshot creation.
    
    3. make the backref cache can live across transactions.
    
    Signed-off-by: Yan Zheng <zheng.yan@oracle.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 6bee8e5204fb..acd532af8e55 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -447,6 +447,9 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 
 	update_ref_for_cow(trans, root, buf, cow, &last_ref);
 
+	if (root->ref_cows)
+		btrfs_reloc_cow_block(trans, root, buf, cow);
+
 	if (buf == root->node) {
 		WARN_ON(parent && parent != buf);
 		if (root->root_key.objectid == BTRFS_TREE_RELOC_OBJECTID ||

commit f0486c68e4bd9a06a5904d3eeb3a0d73a83befb8
Author: Yan, Zheng <zheng.yan@oracle.com>
Date:   Sun May 16 10:46:25 2010 -0400

    Btrfs: Introduce contexts for metadata reservation
    
    Introducing metadata reseravtion contexts has two major advantages.
    First, it makes metadata reseravtion more traceable. Second, it can
    reclaim freed space and re-add them to the itself after transaction
    committed.
    
    Besides add btrfs_block_rsv structure and related helper functions,
    This patch contains following changes:
    
    Move code that decides if freed tree block should be pinned into
    btrfs_free_tree_block().
    
    Make space accounting more accurate, mainly for handling read only
    block groups.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 6795a713b205..6bee8e5204fb 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -280,7 +280,8 @@ int btrfs_block_can_be_shared(struct btrfs_root *root,
 static noinline int update_ref_for_cow(struct btrfs_trans_handle *trans,
 				       struct btrfs_root *root,
 				       struct extent_buffer *buf,
-				       struct extent_buffer *cow)
+				       struct extent_buffer *cow,
+				       int *last_ref)
 {
 	u64 refs;
 	u64 owner;
@@ -366,6 +367,7 @@ static noinline int update_ref_for_cow(struct btrfs_trans_handle *trans,
 			BUG_ON(ret);
 		}
 		clean_tree_block(trans, root, buf);
+		*last_ref = 1;
 	}
 	return 0;
 }
@@ -392,6 +394,7 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 	struct btrfs_disk_key disk_key;
 	struct extent_buffer *cow;
 	int level;
+	int last_ref = 0;
 	int unlock_orig = 0;
 	u64 parent_start;
 
@@ -442,7 +445,7 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 			    (unsigned long)btrfs_header_fsid(cow),
 			    BTRFS_FSID_SIZE);
 
-	update_ref_for_cow(trans, root, buf, cow);
+	update_ref_for_cow(trans, root, buf, cow, &last_ref);
 
 	if (buf == root->node) {
 		WARN_ON(parent && parent != buf);
@@ -457,8 +460,8 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 		extent_buffer_get(cow);
 		spin_unlock(&root->node_lock);
 
-		btrfs_free_tree_block(trans, root, buf->start, buf->len,
-				parent_start, root->root_key.objectid, level);
+		btrfs_free_tree_block(trans, root, buf, parent_start,
+				      last_ref);
 		free_extent_buffer(buf);
 		add_root_to_dirty_list(root);
 	} else {
@@ -473,8 +476,8 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 		btrfs_set_node_ptr_generation(parent, parent_slot,
 					      trans->transid);
 		btrfs_mark_buffer_dirty(parent);
-		btrfs_free_tree_block(trans, root, buf->start, buf->len,
-				parent_start, root->root_key.objectid, level);
+		btrfs_free_tree_block(trans, root, buf, parent_start,
+				      last_ref);
 	}
 	if (unlock_orig)
 		btrfs_tree_unlock(buf);
@@ -949,6 +952,22 @@ int btrfs_bin_search(struct extent_buffer *eb, struct btrfs_key *key,
 	return bin_search(eb, key, level, slot);
 }
 
+static void root_add_used(struct btrfs_root *root, u32 size)
+{
+	spin_lock(&root->accounting_lock);
+	btrfs_set_root_used(&root->root_item,
+			    btrfs_root_used(&root->root_item) + size);
+	spin_unlock(&root->accounting_lock);
+}
+
+static void root_sub_used(struct btrfs_root *root, u32 size)
+{
+	spin_lock(&root->accounting_lock);
+	btrfs_set_root_used(&root->root_item,
+			    btrfs_root_used(&root->root_item) - size);
+	spin_unlock(&root->accounting_lock);
+}
+
 /* given a node and slot number, this reads the blocks it points to.  The
  * extent buffer is returned with a reference taken (but unlocked).
  * NULL is returned on error.
@@ -1019,7 +1038,11 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		btrfs_tree_lock(child);
 		btrfs_set_lock_blocking(child);
 		ret = btrfs_cow_block(trans, root, child, mid, 0, &child);
-		BUG_ON(ret);
+		if (ret) {
+			btrfs_tree_unlock(child);
+			free_extent_buffer(child);
+			goto enospc;
+		}
 
 		spin_lock(&root->node_lock);
 		root->node = child;
@@ -1034,11 +1057,12 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		btrfs_tree_unlock(mid);
 		/* once for the path */
 		free_extent_buffer(mid);
-		ret = btrfs_free_tree_block(trans, root, mid->start, mid->len,
-					    0, root->root_key.objectid, level);
+
+		root_sub_used(root, mid->len);
+		btrfs_free_tree_block(trans, root, mid, 0, 1);
 		/* once for the root ptr */
 		free_extent_buffer(mid);
-		return ret;
+		return 0;
 	}
 	if (btrfs_header_nritems(mid) >
 	    BTRFS_NODEPTRS_PER_BLOCK(root) / 4)
@@ -1088,23 +1112,16 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		if (wret < 0 && wret != -ENOSPC)
 			ret = wret;
 		if (btrfs_header_nritems(right) == 0) {
-			u64 bytenr = right->start;
-			u32 blocksize = right->len;
-
 			clean_tree_block(trans, root, right);
 			btrfs_tree_unlock(right);
-			free_extent_buffer(right);
-			right = NULL;
 			wret = del_ptr(trans, root, path, level + 1, pslot +
 				       1);
 			if (wret)
 				ret = wret;
-			wret = btrfs_free_tree_block(trans, root,
-						     bytenr, blocksize, 0,
-						     root->root_key.objectid,
-						     level);
-			if (wret)
-				ret = wret;
+			root_sub_used(root, right->len);
+			btrfs_free_tree_block(trans, root, right, 0, 1);
+			free_extent_buffer(right);
+			right = NULL;
 		} else {
 			struct btrfs_disk_key right_key;
 			btrfs_node_key(right, &right_key, 0);
@@ -1136,21 +1153,15 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		BUG_ON(wret == 1);
 	}
 	if (btrfs_header_nritems(mid) == 0) {
-		/* we've managed to empty the middle node, drop it */
-		u64 bytenr = mid->start;
-		u32 blocksize = mid->len;
-
 		clean_tree_block(trans, root, mid);
 		btrfs_tree_unlock(mid);
-		free_extent_buffer(mid);
-		mid = NULL;
 		wret = del_ptr(trans, root, path, level + 1, pslot);
 		if (wret)
 			ret = wret;
-		wret = btrfs_free_tree_block(trans, root, bytenr, blocksize,
-					 0, root->root_key.objectid, level);
-		if (wret)
-			ret = wret;
+		root_sub_used(root, mid->len);
+		btrfs_free_tree_block(trans, root, mid, 0, 1);
+		free_extent_buffer(mid);
+		mid = NULL;
 	} else {
 		/* update the parent key to reflect our changes */
 		struct btrfs_disk_key mid_key;
@@ -1740,7 +1751,6 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 					      p->nodes[level + 1],
 					      p->slots[level + 1], &b);
 			if (err) {
-				free_extent_buffer(b);
 				ret = err;
 				goto done;
 			}
@@ -2076,6 +2086,8 @@ static noinline int insert_new_root(struct btrfs_trans_handle *trans,
 	if (IS_ERR(c))
 		return PTR_ERR(c);
 
+	root_add_used(root, root->nodesize);
+
 	memset_extent_buffer(c, 0, 0, sizeof(struct btrfs_header));
 	btrfs_set_header_nritems(c, 1);
 	btrfs_set_header_level(c, level);
@@ -2134,6 +2146,7 @@ static int insert_ptr(struct btrfs_trans_handle *trans, struct btrfs_root
 	int nritems;
 
 	BUG_ON(!path->nodes[level]);
+	btrfs_assert_tree_locked(path->nodes[level]);
 	lower = path->nodes[level];
 	nritems = btrfs_header_nritems(lower);
 	BUG_ON(slot > nritems);
@@ -2202,6 +2215,8 @@ static noinline int split_node(struct btrfs_trans_handle *trans,
 	if (IS_ERR(split))
 		return PTR_ERR(split);
 
+	root_add_used(root, root->nodesize);
+
 	memset_extent_buffer(split, 0, 0, sizeof(struct btrfs_header));
 	btrfs_set_header_level(split, btrfs_header_level(c));
 	btrfs_set_header_bytenr(split, split->start);
@@ -2415,6 +2430,9 @@ static noinline int __push_leaf_right(struct btrfs_trans_handle *trans,
 
 	if (left_nritems)
 		btrfs_mark_buffer_dirty(left);
+	else
+		clean_tree_block(trans, root, left);
+
 	btrfs_mark_buffer_dirty(right);
 
 	btrfs_item_key(right, &disk_key, 0);
@@ -2660,6 +2678,8 @@ static noinline int __push_leaf_left(struct btrfs_trans_handle *trans,
 	btrfs_mark_buffer_dirty(left);
 	if (right_nritems)
 		btrfs_mark_buffer_dirty(right);
+	else
+		clean_tree_block(trans, root, right);
 
 	btrfs_item_key(right, &disk_key, 0);
 	wret = fixup_low_keys(trans, root, path, &disk_key, 1);
@@ -2669,8 +2689,6 @@ static noinline int __push_leaf_left(struct btrfs_trans_handle *trans,
 	/* then fixup the leaf pointer in the path */
 	if (path->slots[0] < push_items) {
 		path->slots[0] += old_left_nritems;
-		if (btrfs_header_nritems(path->nodes[0]) == 0)
-			clean_tree_block(trans, root, path->nodes[0]);
 		btrfs_tree_unlock(path->nodes[0]);
 		free_extent_buffer(path->nodes[0]);
 		path->nodes[0] = left;
@@ -2932,10 +2950,10 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 	right = btrfs_alloc_free_block(trans, root, root->leafsize, 0,
 					root->root_key.objectid,
 					&disk_key, 0, l->start, 0);
-	if (IS_ERR(right)) {
-		BUG_ON(1);
+	if (IS_ERR(right))
 		return PTR_ERR(right);
-	}
+
+	root_add_used(root, root->leafsize);
 
 	memset_extent_buffer(right, 0, 0, sizeof(struct btrfs_header));
 	btrfs_set_header_bytenr(right, right->start);
@@ -3054,7 +3072,8 @@ static noinline int setup_leaf_for_split(struct btrfs_trans_handle *trans,
 
 	btrfs_set_path_blocking(path);
 	ret = split_leaf(trans, root, &key, path, ins_len, 1);
-	BUG_ON(ret);
+	if (ret)
+		goto err;
 
 	path->keep_locks = 0;
 	btrfs_unlock_up_safe(path, 1);
@@ -3796,9 +3815,10 @@ static noinline int btrfs_del_leaf(struct btrfs_trans_handle *trans,
 	 */
 	btrfs_unlock_up_safe(path, 0);
 
-	ret = btrfs_free_tree_block(trans, root, leaf->start, leaf->len,
-				    0, root->root_key.objectid, 0);
-	return ret;
+	root_sub_used(root, leaf->len);
+
+	btrfs_free_tree_block(trans, root, leaf, 0, 1);
+	return 0;
 }
 /*
  * delete the item at the leaf level in path.  If that empties
@@ -3865,6 +3885,8 @@ int btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 		if (leaf == root->node) {
 			btrfs_set_header_level(leaf, 0);
 		} else {
+			btrfs_set_path_blocking(path);
+			clean_tree_block(trans, root, leaf);
 			ret = btrfs_del_leaf(trans, root, path, leaf);
 			BUG_ON(ret);
 		}

commit 795d580baec0d5386b83a8b557df47c20810e86b
Merge: 449cedf099b2 109f6aef5fc4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Apr 5 13:21:15 2010 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/mason/btrfs-unstable
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/mason/btrfs-unstable:
      Btrfs: add check for changed leaves in setup_leaf_for_split
      Btrfs: create snapshot references in same commit as snapshot
      Btrfs: fix small race with delalloc flushing waitqueue's
      Btrfs: use add_to_page_cache_lru, use __page_cache_alloc
      Btrfs: fix chunk allocate size calculation
      Btrfs: kill max_extent mount option
      Btrfs: fail to mount if we have problems reading the block groups
      Btrfs: check btrfs_get_extent return for IS_ERR()
      Btrfs: handle kmalloc() failure in inode lookup ioctl
      Btrfs: dereferencing freed memory
      Btrfs: Simplify num_stripes's calculation logical for __btrfs_alloc_chunk()
      Btrfs: Add error handle for btrfs_search_slot() in btrfs_read_chunk_tree()
      Btrfs: Remove unnecessary finish_wait() in wait_current_trans()
      Btrfs: add NULL check for do_walk_down()
      Btrfs: remove duplicate include in ioctl.c
    
    Fix trivial conflict in fs/btrfs/compression.c due to slab.h include
    cleanups.

commit 109f6aef5fc436f355ad027f4d97bd696df2049a
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Apr 2 09:20:18 2010 -0400

    Btrfs: add check for changed leaves in setup_leaf_for_split
    
    setup_leaf_for_split needs to drop the path and search again, and has
    checks to see if the item we want to split changed size.  But, it misses
    the case where the leaf changed and now has enough room for the item
    we want to insert.
    
    This adds an extra check to make sure the leaf really needs splitting
    before we call btrfs_split_leaf(), which keeps us from trying to split
    a leaf with a single item.
    
    btrfs_split_leaf() will blindly split the single item leaf, leaving us
    with one good leaf and one empty leaf and then a crash.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index c4bc570a396e..babf7fbaec84 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -3040,6 +3040,10 @@ static noinline int setup_leaf_for_split(struct btrfs_trans_handle *trans,
 	if (ret > 0 || item_size != btrfs_item_size_nr(leaf, path->slots[0]))
 		goto err;
 
+	/* the leaf has  changed, it now has room.  return now */
+	if (btrfs_leaf_free_space(root, path->nodes[0]) >= ins_len)
+		goto err;
+
 	if (key.type == BTRFS_EXTENT_DATA_KEY) {
 		fi = btrfs_item_ptr(leaf, path->slots[0],
 				    struct btrfs_file_extent_item);

commit 5a0e3ad6af8660be21ca98a971cd00f331318c05
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Mar 24 17:04:11 2010 +0900

    include cleanup: Update gfp.h and slab.h includes to prepare for breaking implicit slab.h inclusion from percpu.h
    
    percpu.h is included by sched.h and module.h and thus ends up being
    included when building most .c files.  percpu.h includes slab.h which
    in turn includes gfp.h making everything defined by the two files
    universally available and complicating inclusion dependencies.
    
    percpu.h -> slab.h dependency is about to be removed.  Prepare for
    this change by updating users of gfp and slab facilities include those
    headers directly instead of assuming availability.  As this conversion
    needs to touch large number of source files, the following script is
    used as the basis of conversion.
    
      http://userweb.kernel.org/~tj/misc/slabh-sweep.py
    
    The script does the followings.
    
    * Scan files for gfp and slab usages and update includes such that
      only the necessary includes are there.  ie. if only gfp is used,
      gfp.h, if slab is used, slab.h.
    
    * When the script inserts a new include, it looks at the include
      blocks and try to put the new include such that its order conforms
      to its surrounding.  It's put in the include block which contains
      core kernel includes, in the same order that the rest are ordered -
      alphabetical, Christmas tree, rev-Xmas-tree or at the end if there
      doesn't seem to be any matching order.
    
    * If the script can't find a place to put a new include (mostly
      because the file doesn't have fitting include block), it prints out
      an error message indicating which .h file needs to be added to the
      file.
    
    The conversion was done in the following steps.
    
    1. The initial automatic conversion of all .c files updated slightly
       over 4000 files, deleting around 700 includes and adding ~480 gfp.h
       and ~3000 slab.h inclusions.  The script emitted errors for ~400
       files.
    
    2. Each error was manually checked.  Some didn't need the inclusion,
       some needed manual addition while adding it to implementation .h or
       embedding .c file was more appropriate for others.  This step added
       inclusions to around 150 files.
    
    3. The script was run again and the output was compared to the edits
       from #2 to make sure no file was left behind.
    
    4. Several build tests were done and a couple of problems were fixed.
       e.g. lib/decompress_*.c used malloc/free() wrappers around slab
       APIs requiring slab.h to be added manually.
    
    5. The script was run on all .h files but without automatically
       editing them as sprinkling gfp.h and slab.h inclusions around .h
       files could easily lead to inclusion dependency hell.  Most gfp.h
       inclusion directives were ignored as stuff from gfp.h was usually
       wildly available and often used in preprocessor macros.  Each
       slab.h inclusion directive was examined and added manually as
       necessary.
    
    6. percpu.h was updated not to include slab.h.
    
    7. Build test were done on the following configurations and failures
       were fixed.  CONFIG_GCOV_KERNEL was turned off for all tests (as my
       distributed build env didn't work with gcov compiles) and a few
       more options had to be turned off depending on archs to make things
       build (like ipr on powerpc/64 which failed due to missing writeq).
    
       * x86 and x86_64 UP and SMP allmodconfig and a custom test config.
       * powerpc and powerpc64 SMP allmodconfig
       * sparc and sparc64 SMP allmodconfig
       * ia64 SMP allmodconfig
       * s390 SMP allmodconfig
       * alpha SMP allmodconfig
       * um on x86_64 SMP allmodconfig
    
    8. percpu.h modifications were reverted so that it could be applied as
       a separate patch and serve as bisection point.
    
    Given the fact that I had only a couple of failures from tests on step
    6, I'm fairly confident about the coverage of this conversion patch.
    If there is a breakage, it's likely to be something in one of the arch
    headers which should be easily discoverable easily on most builds of
    the specific arch.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Guess-its-ok-by: Christoph Lameter <cl@linux-foundation.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Lee Schermerhorn <Lee.Schermerhorn@hp.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index c4bc570a396e..8e29260cd0bb 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -17,6 +17,7 @@
  */
 
 #include <linux/sched.h>
+#include <linux/slab.h>
 #include "ctree.h"
 #include "disk-io.h"
 #include "transaction.h"

commit 86b9f2eca5e0984145e3c7698a7cd6dd65c2a93f
Author: Yan, Zheng <zheng.yan@oracle.com>
Date:   Thu Nov 12 09:36:50 2009 +0000

    Btrfs: Fix per root used space accounting
    
    The bytes_used field in root item was originally planned to
    trace the amount of used data and tree blocks. But it never
    worked right since we can't trace freeing of data accurately.
    This patch changes it to only trace the amount of tree blocks.
    
    Signed-off-by: Yan Zheng <zheng.yan@oracle.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 9d4ba3470c17..c4bc570a396e 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -456,9 +456,8 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 		extent_buffer_get(cow);
 		spin_unlock(&root->node_lock);
 
-		btrfs_free_extent(trans, root, buf->start, buf->len,
-				  parent_start, root->root_key.objectid,
-				  level, 0);
+		btrfs_free_tree_block(trans, root, buf->start, buf->len,
+				parent_start, root->root_key.objectid, level);
 		free_extent_buffer(buf);
 		add_root_to_dirty_list(root);
 	} else {
@@ -473,9 +472,8 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 		btrfs_set_node_ptr_generation(parent, parent_slot,
 					      trans->transid);
 		btrfs_mark_buffer_dirty(parent);
-		btrfs_free_extent(trans, root, buf->start, buf->len,
-				  parent_start, root->root_key.objectid,
-				  level, 0);
+		btrfs_free_tree_block(trans, root, buf->start, buf->len,
+				parent_start, root->root_key.objectid, level);
 	}
 	if (unlock_orig)
 		btrfs_tree_unlock(buf);
@@ -1035,8 +1033,8 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		btrfs_tree_unlock(mid);
 		/* once for the path */
 		free_extent_buffer(mid);
-		ret = btrfs_free_extent(trans, root, mid->start, mid->len,
-					0, root->root_key.objectid, level, 1);
+		ret = btrfs_free_tree_block(trans, root, mid->start, mid->len,
+					    0, root->root_key.objectid, level);
 		/* once for the root ptr */
 		free_extent_buffer(mid);
 		return ret;
@@ -1100,10 +1098,10 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 				       1);
 			if (wret)
 				ret = wret;
-			wret = btrfs_free_extent(trans, root, bytenr,
-						 blocksize, 0,
-						 root->root_key.objectid,
-						 level, 0);
+			wret = btrfs_free_tree_block(trans, root,
+						     bytenr, blocksize, 0,
+						     root->root_key.objectid,
+						     level);
 			if (wret)
 				ret = wret;
 		} else {
@@ -1148,9 +1146,8 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		wret = del_ptr(trans, root, path, level + 1, pslot);
 		if (wret)
 			ret = wret;
-		wret = btrfs_free_extent(trans, root, bytenr, blocksize,
-					 0, root->root_key.objectid,
-					 level, 0);
+		wret = btrfs_free_tree_block(trans, root, bytenr, blocksize,
+					 0, root->root_key.objectid, level);
 		if (wret)
 			ret = wret;
 	} else {
@@ -3794,8 +3791,8 @@ static noinline int btrfs_del_leaf(struct btrfs_trans_handle *trans,
 	 */
 	btrfs_unlock_up_safe(path, 0);
 
-	ret = btrfs_free_extent(trans, root, leaf->start, leaf->len,
-				0, root->root_key.objectid, 0, 0);
+	ret = btrfs_free_tree_block(trans, root, leaf->start, leaf->len,
+				    0, root->root_key.objectid, 0);
 	return ret;
 }
 /*

commit ad48fd754676bfae4139be1a897b1ea58f9aaf21
Author: Yan, Zheng <zheng.yan@oracle.com>
Date:   Thu Nov 12 09:33:58 2009 +0000

    Btrfs: Add btrfs_duplicate_item
    
    btrfs_duplicate_item duplicates item with new key, guaranteeing
    the source item and the new items are in the same tree leaf and
    contiguous. It allows us to split file extent in place, without
    using lock_extent to prevent bookend extent race.
    
    Signed-off-by: Yan Zheng <zheng.yan@oracle.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index ec96f3a6d536..9d4ba3470c17 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -37,6 +37,11 @@ static int balance_node_right(struct btrfs_trans_handle *trans,
 			      struct extent_buffer *src_buf);
 static int del_ptr(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 		   struct btrfs_path *path, int level, int slot);
+static int setup_items_for_insert(struct btrfs_trans_handle *trans,
+			struct btrfs_root *root, struct btrfs_path *path,
+			struct btrfs_key *cpu_key, u32 *data_size,
+			u32 total_data, u32 total_size, int nr);
+
 
 struct btrfs_path *btrfs_alloc_path(void)
 {
@@ -2997,75 +3002,85 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 	return ret;
 }
 
-/*
- * This function splits a single item into two items,
- * giving 'new_key' to the new item and splitting the
- * old one at split_offset (from the start of the item).
- *
- * The path may be released by this operation.  After
- * the split, the path is pointing to the old item.  The
- * new item is going to be in the same node as the old one.
- *
- * Note, the item being split must be smaller enough to live alone on
- * a tree block with room for one extra struct btrfs_item
- *
- * This allows us to split the item in place, keeping a lock on the
- * leaf the entire time.
- */
-int btrfs_split_item(struct btrfs_trans_handle *trans,
-		     struct btrfs_root *root,
-		     struct btrfs_path *path,
-		     struct btrfs_key *new_key,
-		     unsigned long split_offset)
+static noinline int setup_leaf_for_split(struct btrfs_trans_handle *trans,
+					 struct btrfs_root *root,
+					 struct btrfs_path *path, int ins_len)
 {
-	u32 item_size;
+	struct btrfs_key key;
 	struct extent_buffer *leaf;
-	struct btrfs_key orig_key;
-	struct btrfs_item *item;
-	struct btrfs_item *new_item;
-	int ret = 0;
-	int slot;
-	u32 nritems;
-	u32 orig_offset;
-	struct btrfs_disk_key disk_key;
-	char *buf;
+	struct btrfs_file_extent_item *fi;
+	u64 extent_len = 0;
+	u32 item_size;
+	int ret;
 
 	leaf = path->nodes[0];
-	btrfs_item_key_to_cpu(leaf, &orig_key, path->slots[0]);
-	if (btrfs_leaf_free_space(root, leaf) >= sizeof(struct btrfs_item))
-		goto split;
+	btrfs_item_key_to_cpu(leaf, &key, path->slots[0]);
+
+	BUG_ON(key.type != BTRFS_EXTENT_DATA_KEY &&
+	       key.type != BTRFS_EXTENT_CSUM_KEY);
+
+	if (btrfs_leaf_free_space(root, leaf) >= ins_len)
+		return 0;
 
 	item_size = btrfs_item_size_nr(leaf, path->slots[0]);
+	if (key.type == BTRFS_EXTENT_DATA_KEY) {
+		fi = btrfs_item_ptr(leaf, path->slots[0],
+				    struct btrfs_file_extent_item);
+		extent_len = btrfs_file_extent_num_bytes(leaf, fi);
+	}
 	btrfs_release_path(root, path);
 
-	path->search_for_split = 1;
 	path->keep_locks = 1;
-
-	ret = btrfs_search_slot(trans, root, &orig_key, path, 0, 1);
+	path->search_for_split = 1;
+	ret = btrfs_search_slot(trans, root, &key, path, 0, 1);
 	path->search_for_split = 0;
+	if (ret < 0)
+		goto err;
 
+	ret = -EAGAIN;
+	leaf = path->nodes[0];
 	/* if our item isn't there or got smaller, return now */
-	if (ret != 0 || item_size != btrfs_item_size_nr(path->nodes[0],
-							path->slots[0])) {
-		path->keep_locks = 0;
-		return -EAGAIN;
+	if (ret > 0 || item_size != btrfs_item_size_nr(leaf, path->slots[0]))
+		goto err;
+
+	if (key.type == BTRFS_EXTENT_DATA_KEY) {
+		fi = btrfs_item_ptr(leaf, path->slots[0],
+				    struct btrfs_file_extent_item);
+		if (extent_len != btrfs_file_extent_num_bytes(leaf, fi))
+			goto err;
 	}
 
 	btrfs_set_path_blocking(path);
-	ret = split_leaf(trans, root, &orig_key, path,
-			 sizeof(struct btrfs_item), 1);
-	path->keep_locks = 0;
+	ret = split_leaf(trans, root, &key, path, ins_len, 1);
 	BUG_ON(ret);
 
+	path->keep_locks = 0;
 	btrfs_unlock_up_safe(path, 1);
+	return 0;
+err:
+	path->keep_locks = 0;
+	return ret;
+}
+
+static noinline int split_item(struct btrfs_trans_handle *trans,
+			       struct btrfs_root *root,
+			       struct btrfs_path *path,
+			       struct btrfs_key *new_key,
+			       unsigned long split_offset)
+{
+	struct extent_buffer *leaf;
+	struct btrfs_item *item;
+	struct btrfs_item *new_item;
+	int slot;
+	char *buf;
+	u32 nritems;
+	u32 item_size;
+	u32 orig_offset;
+	struct btrfs_disk_key disk_key;
+
 	leaf = path->nodes[0];
 	BUG_ON(btrfs_leaf_free_space(root, leaf) < sizeof(struct btrfs_item));
 
-split:
-	/*
-	 * make sure any changes to the path from split_leaf leave it
-	 * in a blocking state
-	 */
 	btrfs_set_path_blocking(path);
 
 	item = btrfs_item_nr(leaf, path->slots[0]);
@@ -3073,19 +3088,19 @@ int btrfs_split_item(struct btrfs_trans_handle *trans,
 	item_size = btrfs_item_size(leaf, item);
 
 	buf = kmalloc(item_size, GFP_NOFS);
+	if (!buf)
+		return -ENOMEM;
+
 	read_extent_buffer(leaf, buf, btrfs_item_ptr_offset(leaf,
 			    path->slots[0]), item_size);
-	slot = path->slots[0] + 1;
-	leaf = path->nodes[0];
 
+	slot = path->slots[0] + 1;
 	nritems = btrfs_header_nritems(leaf);
-
 	if (slot != nritems) {
 		/* shift the items */
 		memmove_extent_buffer(leaf, btrfs_item_nr_offset(slot + 1),
-			      btrfs_item_nr_offset(slot),
-			      (nritems - slot) * sizeof(struct btrfs_item));
-
+				btrfs_item_nr_offset(slot),
+				(nritems - slot) * sizeof(struct btrfs_item));
 	}
 
 	btrfs_cpu_key_to_disk(&disk_key, new_key);
@@ -3113,15 +3128,80 @@ int btrfs_split_item(struct btrfs_trans_handle *trans,
 			    item_size - split_offset);
 	btrfs_mark_buffer_dirty(leaf);
 
-	ret = 0;
-	if (btrfs_leaf_free_space(root, leaf) < 0) {
-		btrfs_print_leaf(root, leaf);
-		BUG();
-	}
+	BUG_ON(btrfs_leaf_free_space(root, leaf) < 0);
 	kfree(buf);
+	return 0;
+}
+
+/*
+ * This function splits a single item into two items,
+ * giving 'new_key' to the new item and splitting the
+ * old one at split_offset (from the start of the item).
+ *
+ * The path may be released by this operation.  After
+ * the split, the path is pointing to the old item.  The
+ * new item is going to be in the same node as the old one.
+ *
+ * Note, the item being split must be smaller enough to live alone on
+ * a tree block with room for one extra struct btrfs_item
+ *
+ * This allows us to split the item in place, keeping a lock on the
+ * leaf the entire time.
+ */
+int btrfs_split_item(struct btrfs_trans_handle *trans,
+		     struct btrfs_root *root,
+		     struct btrfs_path *path,
+		     struct btrfs_key *new_key,
+		     unsigned long split_offset)
+{
+	int ret;
+	ret = setup_leaf_for_split(trans, root, path,
+				   sizeof(struct btrfs_item));
+	if (ret)
+		return ret;
+
+	ret = split_item(trans, root, path, new_key, split_offset);
 	return ret;
 }
 
+/*
+ * This function duplicate a item, giving 'new_key' to the new item.
+ * It guarantees both items live in the same tree leaf and the new item
+ * is contiguous with the original item.
+ *
+ * This allows us to split file extent in place, keeping a lock on the
+ * leaf the entire time.
+ */
+int btrfs_duplicate_item(struct btrfs_trans_handle *trans,
+			 struct btrfs_root *root,
+			 struct btrfs_path *path,
+			 struct btrfs_key *new_key)
+{
+	struct extent_buffer *leaf;
+	int ret;
+	u32 item_size;
+
+	leaf = path->nodes[0];
+	item_size = btrfs_item_size_nr(leaf, path->slots[0]);
+	ret = setup_leaf_for_split(trans, root, path,
+				   item_size + sizeof(struct btrfs_item));
+	if (ret)
+		return ret;
+
+	path->slots[0]++;
+	ret = setup_items_for_insert(trans, root, path, new_key, &item_size,
+				     item_size, item_size +
+				     sizeof(struct btrfs_item), 1);
+	BUG_ON(ret);
+
+	leaf = path->nodes[0];
+	memcpy_extent_buffer(leaf,
+			     btrfs_item_ptr_offset(leaf, path->slots[0]),
+			     btrfs_item_ptr_offset(leaf, path->slots[0] - 1),
+			     item_size);
+	return 0;
+}
+
 /*
  * make the item pointed to by the path smaller.  new_size indicates
  * how small to make it, and from_end tells us if we just chop bytes

commit a57195214358b75807a74bad96a8601a36262af7
Author: Yan, Zheng <zheng.yan@oracle.com>
Date:   Thu Sep 24 09:17:31 2009 -0400

    Btrfs: check size of inode backref before adding hardlink
    
    For every hardlink in btrfs, there is a corresponding inode back
    reference. All inode back references for hardlinks in a given
    directory are stored in single b-tree item. The size of b-tree item
    is limited by the size of b-tree leaf, so we can only create limited
    number of hardlinks to a given file in a directory.
    
    The original code lacks of the check, it oops if the number of
    hardlinks goes over the limit. This patch fixes the issue by adding
    check to btrfs_link and btrfs_rename.
    
    Signed-off-by: Yan Zheng <zheng.yan@oracle.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 3fdcc0512d3a..ec96f3a6d536 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2853,6 +2853,12 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 	int split;
 	int num_doubles = 0;
 
+	l = path->nodes[0];
+	slot = path->slots[0];
+	if (extend && data_size + btrfs_item_size_nr(l, slot) +
+	    sizeof(struct btrfs_item) > BTRFS_LEAF_DATA_SIZE(root))
+		return -EOVERFLOW;
+
 	/* first try to make some room by pushing left and right */
 	if (data_size && ins_key->type != BTRFS_DIR_ITEM_KEY) {
 		wret = push_leaf_right(trans, root, path, data_size, 0);

commit d717aa1d31c36cb56059e97966cb76f0be021969
Author: Yan Zheng <zheng.yan@oracle.com>
Date:   Fri Jul 24 12:42:46 2009 -0400

    Btrfs: Avoid delayed reference update looping
    
    btrfs_split_leaf and btrfs_del_items can end up in a loop
    where one is constantly spliting a given leaf and the other
    is constantly merging it back with the adjacent nodes.
    
    There is a better fix for this, but in the interest of something
    small, this patch just changes btrfs_del_items back to balancing less
    often.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 978449af4ccd..3fdcc0512d3a 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1040,9 +1040,6 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 	    BTRFS_NODEPTRS_PER_BLOCK(root) / 4)
 		return 0;
 
-	if (btrfs_header_nritems(mid) > 2)
-		return 0;
-
 	if (btrfs_header_nritems(mid) < 2)
 		err_on_enospc = 1;
 
@@ -3796,7 +3793,7 @@ int btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 		}
 
 		/* delete the leaf if it is mostly empty */
-		if (used < BTRFS_LEAF_DATA_SIZE(root) / 2) {
+		if (used < BTRFS_LEAF_DATA_SIZE(root) / 3) {
 			/* push_leaf_left fixes the path.
 			 * make sure the path still points to our leaf
 			 * for possible call to del_ptr below

commit 0a4eefbb745ec0e8a5b694ae3f40cc34082d8f61
Author: Yan Zheng <zheng.yan@oracle.com>
Date:   Fri Jul 24 11:06:53 2009 -0400

    Btrfs: Fix ordering of key field checks in btrfs_previous_item
    
    Check objectid of item before checking the item type, otherwise we may return
    zero for a key that is actually too low.
    
    Signed-off-by: Yan Zheng <zheng.yan@oracle.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 91572091c0a4..978449af4ccd 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -4296,10 +4296,10 @@ int btrfs_previous_item(struct btrfs_root *root,
 			path->slots[0]--;
 
 		btrfs_item_key_to_cpu(leaf, &found_key, path->slots[0]);
-		if (found_key.type == type)
-			return 0;
 		if (found_key.objectid < min_objectid)
 			break;
+		if (found_key.type == type)
+			return 0;
 		if (found_key.objectid == min_objectid &&
 		    found_key.type < type)
 			break;

commit 20736abaa361bea488df6a1f66f6b37fb01107b9
Author: Diego Calleja <diegocg@gmail.com>
Date:   Fri Jul 24 11:06:52 2009 -0400

    Btrfs: Remove code duplication in comp_keys
    
    comp_keys is duplicating what is done in btrfs_comp_cpu_keys, so just
    call it.
    
    Signed-off-by: Diego Calleja <diegocg@gmail.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index fdd423a550d6..91572091c0a4 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -557,19 +557,7 @@ static int comp_keys(struct btrfs_disk_key *disk, struct btrfs_key *k2)
 
 	btrfs_disk_key_to_cpu(&k1, disk);
 
-	if (k1.objectid > k2->objectid)
-		return 1;
-	if (k1.objectid < k2->objectid)
-		return -1;
-	if (k1.type > k2->type)
-		return 1;
-	if (k1.type < k2->type)
-		return -1;
-	if (k1.offset > k2->offset)
-		return 1;
-	if (k1.offset < k2->offset)
-		return -1;
-	return 0;
+	return btrfs_comp_cpu_keys(&k1, k2);
 }
 
 /*

commit 33c66f430bfa3a033e70470e4c93f967156b696d
Author: Yan Zheng <zheng.yan@oracle.com>
Date:   Wed Jul 22 09:59:00 2009 -0400

    Btrfs: fix locking issue in btrfs_find_next_key
    
    When walking up the tree, btrfs_find_next_key assumes the upper level tree
    block is properly locked. This isn't always true even path->keep_locks is 1.
    This is because btrfs_find_next_key may advance path->slots[] several times
    instead of only once.
    
    When 'path->slots[level] >= btrfs_header_nritems(path->nodes[level])' is found,
    we can't guarantee the original value of 'path->slots[level]' is
    'btrfs_header_nritems(path->nodes[level]) - 1'. If it's not, the tree block at
    'level + 1' isn't locked.
    
    This patch fixes the issue by explicitly checking the locking state,
    re-searching the tree if it's not locked.
    
    Signed-off-by: Yan Zheng <zheng.yan@oracle.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 7bb66c65ddfd..fdd423a550d6 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1701,6 +1701,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 	struct extent_buffer *b;
 	int slot;
 	int ret;
+	int err;
 	int level;
 	int lowest_unlock = 1;
 	u8 lowest_level = 0;
@@ -1737,8 +1738,6 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 			p->locks[level] = 1;
 
 		if (cow) {
-			int wret;
-
 			/*
 			 * if we don't really need to cow this block
 			 * then we don't want to set the path blocking,
@@ -1749,12 +1748,12 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 
 			btrfs_set_path_blocking(p);
 
-			wret = btrfs_cow_block(trans, root, b,
-					       p->nodes[level + 1],
-					       p->slots[level + 1], &b);
-			if (wret) {
+			err = btrfs_cow_block(trans, root, b,
+					      p->nodes[level + 1],
+					      p->slots[level + 1], &b);
+			if (err) {
 				free_extent_buffer(b);
-				ret = wret;
+				ret = err;
 				goto done;
 			}
 		}
@@ -1793,41 +1792,45 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 		ret = bin_search(b, key, level, &slot);
 
 		if (level != 0) {
-			if (ret && slot > 0)
+			int dec = 0;
+			if (ret && slot > 0) {
+				dec = 1;
 				slot -= 1;
+			}
 			p->slots[level] = slot;
-			ret = setup_nodes_for_search(trans, root, p, b, level,
+			err = setup_nodes_for_search(trans, root, p, b, level,
 						     ins_len);
-			if (ret == -EAGAIN)
+			if (err == -EAGAIN)
 				goto again;
-			else if (ret)
+			if (err) {
+				ret = err;
 				goto done;
+			}
 			b = p->nodes[level];
 			slot = p->slots[level];
 
 			unlock_up(p, level, lowest_unlock);
 
-			/* this is only true while dropping a snapshot */
 			if (level == lowest_level) {
-				ret = 0;
+				if (dec)
+					p->slots[level]++;
 				goto done;
 			}
 
-			ret = read_block_for_search(trans, root, p,
+			err = read_block_for_search(trans, root, p,
 						    &b, level, slot, key);
-			if (ret == -EAGAIN)
+			if (err == -EAGAIN)
 				goto again;
-
-			if (ret == -EIO)
+			if (err) {
+				ret = err;
 				goto done;
+			}
 
 			if (!p->skip_locking) {
-				int lret;
-
 				btrfs_clear_path_blocking(p, NULL);
-				lret = btrfs_try_spin_lock(b);
+				err = btrfs_try_spin_lock(b);
 
-				if (!lret) {
+				if (!err) {
 					btrfs_set_path_blocking(p);
 					btrfs_tree_lock(b);
 					btrfs_clear_path_blocking(p, b);
@@ -1837,16 +1840,14 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 			p->slots[level] = slot;
 			if (ins_len > 0 &&
 			    btrfs_leaf_free_space(root, b) < ins_len) {
-				int sret;
-
 				btrfs_set_path_blocking(p);
-				sret = split_leaf(trans, root, key,
-						      p, ins_len, ret == 0);
+				err = split_leaf(trans, root, key,
+						 p, ins_len, ret == 0);
 				btrfs_clear_path_blocking(p, NULL);
 
-				BUG_ON(sret > 0);
-				if (sret) {
-					ret = sret;
+				BUG_ON(err > 0);
+				if (err) {
+					ret = err;
 					goto done;
 				}
 			}
@@ -4042,10 +4043,9 @@ int btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,
  * calling this function.
  */
 int btrfs_find_next_key(struct btrfs_root *root, struct btrfs_path *path,
-			struct btrfs_key *key, int lowest_level,
+			struct btrfs_key *key, int level,
 			int cache_only, u64 min_trans)
 {
-	int level = lowest_level;
 	int slot;
 	struct extent_buffer *c;
 
@@ -4058,11 +4058,40 @@ int btrfs_find_next_key(struct btrfs_root *root, struct btrfs_path *path,
 		c = path->nodes[level];
 next:
 		if (slot >= btrfs_header_nritems(c)) {
-			level++;
-			if (level == BTRFS_MAX_LEVEL)
+			int ret;
+			int orig_lowest;
+			struct btrfs_key cur_key;
+			if (level + 1 >= BTRFS_MAX_LEVEL ||
+			    !path->nodes[level + 1])
 				return 1;
-			continue;
+
+			if (path->locks[level + 1]) {
+				level++;
+				continue;
+			}
+
+			slot = btrfs_header_nritems(c) - 1;
+			if (level == 0)
+				btrfs_item_key_to_cpu(c, &cur_key, slot);
+			else
+				btrfs_node_key_to_cpu(c, &cur_key, slot);
+
+			orig_lowest = path->lowest_level;
+			btrfs_release_path(root, path);
+			path->lowest_level = level;
+			ret = btrfs_search_slot(NULL, root, &cur_key, path,
+						0, 0);
+			path->lowest_level = orig_lowest;
+			if (ret < 0)
+				return ret;
+
+			c = path->nodes[level];
+			slot = path->slots[level];
+			if (ret == 0)
+				slot++;
+			goto next;
 		}
+
 		if (level == 0)
 			btrfs_item_key_to_cpu(c, key, slot);
 		else {

commit e457afec60fdbd86b963d36f4a8a9285088c6043
Author: Yan Zheng <zheng.yan@oracle.com>
Date:   Wed Jul 22 09:59:00 2009 -0400

    Btrfs: fix double increment of path->slots[0] in btrfs_next_leaf
    
    if 1 is returned by btrfs_search_slot, the path already points to the
    first item with 'key > searching key'. So increasing path->slots[0] by
    one is superfluous in that case.
    
    Signed-off-by: Yan Zheng <zheng.yan@oracle.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 60a45f3a4e91..7bb66c65ddfd 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -4146,7 +4146,8 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 	 * advance the path if there are now more items available.
 	 */
 	if (nritems > 0 && path->slots[0] < nritems - 1) {
-		path->slots[0]++;
+		if (ret == 0)
+			path->slots[0]++;
 		ret = 0;
 		goto done;
 	}

commit cfbb9308463f6dad1334884db046ccf0f1a77918
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon May 18 10:41:58 2009 -0400

    Btrfs: balance btree more often
    
    With the new back reference code, the cost of a balance has gone down
    in terms of the number of back reference updates done.  This commit
    makes us more aggressively balance leaves and nodes as they become
    less full.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 2f633e751198..60a45f3a4e91 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1651,7 +1651,7 @@ setup_nodes_for_search(struct btrfs_trans_handle *trans,
 		}
 		b = p->nodes[level];
 	} else if (ins_len < 0 && btrfs_header_nritems(b) <
-		   BTRFS_NODEPTRS_PER_BLOCK(root) / 4) {
+		   BTRFS_NODEPTRS_PER_BLOCK(root) / 2) {
 		int sret;
 
 		sret = reada_for_balance(root, p, level);
@@ -3807,7 +3807,7 @@ int btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 		}
 
 		/* delete the leaf if it is mostly empty */
-		if (used < BTRFS_LEAF_DATA_SIZE(root) / 4) {
+		if (used < BTRFS_LEAF_DATA_SIZE(root) / 2) {
 			/* push_leaf_left fixes the path.
 			 * make sure the path still points to our leaf
 			 * for possible call to del_ptr below

commit b36124210248706186a02093427bdff4b3f548e8
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed May 13 19:12:15 2009 -0400

    Btrfs: stop avoiding balancing at the end of the transaction.
    
    When the delayed reference code was added, some checks were added
    to avoid extra balancing while the delayed references were being flushed.
    This made for less efficient btrees, but it reduced the chances of
    loops where no forward progress was made because the balances made
    more delayed ref updates.
    
    With the new dead root removal code and the mixed back references,
    the extent allocation tree is no longer using precise back refs, and
    the delayed reference updates don't carry the risk of looping forever
    anymore.  So, the balance avoidance is no longer required.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 2b960278a2f9..2f633e751198 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1052,8 +1052,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 	    BTRFS_NODEPTRS_PER_BLOCK(root) / 4)
 		return 0;
 
-	if (trans->transaction->delayed_refs.flushing &&
-	    btrfs_header_nritems(mid) > 2)
+	if (btrfs_header_nritems(mid) > 2)
 		return 0;
 
 	if (btrfs_header_nritems(mid) < 2)
@@ -2194,7 +2193,7 @@ static noinline int split_node(struct btrfs_trans_handle *trans,
 		ret = insert_new_root(trans, root, path, level + 1);
 		if (ret)
 			return ret;
-	} else if (!trans->transaction->delayed_refs.flushing) {
+	} else {
 		ret = push_nodes_for_insert(trans, root, path, level);
 		c = path->nodes[level];
 		if (!ret && btrfs_header_nritems(c) <
@@ -2869,8 +2868,7 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 	int num_doubles = 0;
 
 	/* first try to make some room by pushing left and right */
-	if (data_size && ins_key->type != BTRFS_DIR_ITEM_KEY &&
-	    !trans->transaction->delayed_refs.flushing) {
+	if (data_size && ins_key->type != BTRFS_DIR_ITEM_KEY) {
 		wret = push_leaf_right(trans, root, path, data_size, 0);
 		if (wret < 0)
 			return wret;
@@ -3809,8 +3807,7 @@ int btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 		}
 
 		/* delete the leaf if it is mostly empty */
-		if (used < BTRFS_LEAF_DATA_SIZE(root) / 4 &&
-		    !trans->transaction->delayed_refs.flushing) {
+		if (used < BTRFS_LEAF_DATA_SIZE(root) / 4) {
 			/* push_leaf_left fixes the path.
 			 * make sure the path still points to our leaf
 			 * for possible call to del_ptr below

commit 5d4f98a28c7d334091c1b7744f48a1acdd2a4ae0
Author: Yan Zheng <zheng.yan@oracle.com>
Date:   Wed Jun 10 10:45:14 2009 -0400

    Btrfs: Mixed back reference  (FORWARD ROLLING FORMAT CHANGE)
    
    This commit introduces a new kind of back reference for btrfs metadata.
    Once a filesystem has been mounted with this commit, IT WILL NO LONGER
    BE MOUNTABLE BY OLDER KERNELS.
    
    When a tree block in subvolume tree is cow'd, the reference counts of all
    extents it points to are increased by one.  At transaction commit time,
    the old root of the subvolume is recorded in a "dead root" data structure,
    and the btree it points to is later walked, dropping reference counts
    and freeing any blocks where the reference count goes to 0.
    
    The increments done during cow and decrements done after commit cancel out,
    and the walk is a very expensive way to go about freeing the blocks that
    are no longer referenced by the new btree root.  This commit reduces the
    transaction overhead by avoiding the need for dead root records.
    
    When a non-shared tree block is cow'd, we free the old block at once, and the
    new block inherits old block's references. When a tree block with reference
    count > 1 is cow'd, we increase the reference counts of all extents
    the new block points to by one, and decrease the old block's reference count by
    one.
    
    This dead tree avoidance code removes the need to modify the reference
    counts of lower level extents when a non-shared tree block is cow'd.
    But we still need to update back ref for all pointers in the block.
    This is because the location of the block is recorded in the back ref
    item.
    
    We can solve this by introducing a new type of back ref. The new
    back ref provides information about pointer's key, level and in which
    tree the pointer lives. This information allow us to find the pointer
    by searching the tree. The shortcoming of the new back ref is that it
    only works for pointers in tree blocks referenced by their owner trees.
    
    This is mostly a problem for snapshots, where resolving one of these
    fuzzy back references would be O(number_of_snapshots) and quite slow.
    The solution used here is to use the fuzzy back references in the common
    case where a given tree block is only referenced by one root,
    and use the full back references when multiple roots have a reference
    on a given block.
    
    This commit adds per subvolume red-black tree to keep trace of cached
    inodes. The red-black tree helps the balancing code to find cached
    inodes whose inode numbers within a given range.
    
    This commit improves the balancing code by introducing several data
    structures to keep the state of balancing. The most important one
    is the back ref cache. It caches how the upper level tree blocks are
    referenced. This greatly reduce the overhead of checking back ref.
    
    The improved balancing code scales significantly better with a large
    number of snapshots.
    
    This is a very large commit and was written in a number of
    pieces.  But, they depend heavily on the disk format change and were
    squashed together to make sure git bisect didn't end up in a
    bad state wrt space balancing or the format change.
    
    Signed-off-by: Yan Zheng <zheng.yan@oracle.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index fedf8b9f03a2..2b960278a2f9 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -197,14 +197,7 @@ int btrfs_copy_root(struct btrfs_trans_handle *trans,
 	u32 nritems;
 	int ret = 0;
 	int level;
-	struct btrfs_root *new_root;
-
-	new_root = kmalloc(sizeof(*new_root), GFP_NOFS);
-	if (!new_root)
-		return -ENOMEM;
-
-	memcpy(new_root, root, sizeof(*new_root));
-	new_root->root_key.objectid = new_root_objectid;
+	struct btrfs_disk_key disk_key;
 
 	WARN_ON(root->ref_cows && trans->transid !=
 		root->fs_info->running_transaction->transid);
@@ -212,28 +205,37 @@ int btrfs_copy_root(struct btrfs_trans_handle *trans,
 
 	level = btrfs_header_level(buf);
 	nritems = btrfs_header_nritems(buf);
+	if (level == 0)
+		btrfs_item_key(buf, &disk_key, 0);
+	else
+		btrfs_node_key(buf, &disk_key, 0);
 
-	cow = btrfs_alloc_free_block(trans, new_root, buf->len, 0,
-				     new_root_objectid, trans->transid,
-				     level, buf->start, 0);
-	if (IS_ERR(cow)) {
-		kfree(new_root);
+	cow = btrfs_alloc_free_block(trans, root, buf->len, 0,
+				     new_root_objectid, &disk_key, level,
+				     buf->start, 0);
+	if (IS_ERR(cow))
 		return PTR_ERR(cow);
-	}
 
 	copy_extent_buffer(cow, buf, 0, 0, cow->len);
 	btrfs_set_header_bytenr(cow, cow->start);
 	btrfs_set_header_generation(cow, trans->transid);
-	btrfs_set_header_owner(cow, new_root_objectid);
-	btrfs_clear_header_flag(cow, BTRFS_HEADER_FLAG_WRITTEN);
+	btrfs_set_header_backref_rev(cow, BTRFS_MIXED_BACKREF_REV);
+	btrfs_clear_header_flag(cow, BTRFS_HEADER_FLAG_WRITTEN |
+				     BTRFS_HEADER_FLAG_RELOC);
+	if (new_root_objectid == BTRFS_TREE_RELOC_OBJECTID)
+		btrfs_set_header_flag(cow, BTRFS_HEADER_FLAG_RELOC);
+	else
+		btrfs_set_header_owner(cow, new_root_objectid);
 
 	write_extent_buffer(cow, root->fs_info->fsid,
 			    (unsigned long)btrfs_header_fsid(cow),
 			    BTRFS_FSID_SIZE);
 
 	WARN_ON(btrfs_header_generation(buf) > trans->transid);
-	ret = btrfs_inc_ref(trans, new_root, buf, cow, NULL);
-	kfree(new_root);
+	if (new_root_objectid == BTRFS_TREE_RELOC_OBJECTID)
+		ret = btrfs_inc_ref(trans, root, cow, 1);
+	else
+		ret = btrfs_inc_ref(trans, root, cow, 0);
 
 	if (ret)
 		return ret;
@@ -243,6 +245,125 @@ int btrfs_copy_root(struct btrfs_trans_handle *trans,
 	return 0;
 }
 
+/*
+ * check if the tree block can be shared by multiple trees
+ */
+int btrfs_block_can_be_shared(struct btrfs_root *root,
+			      struct extent_buffer *buf)
+{
+	/*
+	 * Tree blocks not in refernece counted trees and tree roots
+	 * are never shared. If a block was allocated after the last
+	 * snapshot and the block was not allocated by tree relocation,
+	 * we know the block is not shared.
+	 */
+	if (root->ref_cows &&
+	    buf != root->node && buf != root->commit_root &&
+	    (btrfs_header_generation(buf) <=
+	     btrfs_root_last_snapshot(&root->root_item) ||
+	     btrfs_header_flag(buf, BTRFS_HEADER_FLAG_RELOC)))
+		return 1;
+#ifdef BTRFS_COMPAT_EXTENT_TREE_V0
+	if (root->ref_cows &&
+	    btrfs_header_backref_rev(buf) < BTRFS_MIXED_BACKREF_REV)
+		return 1;
+#endif
+	return 0;
+}
+
+static noinline int update_ref_for_cow(struct btrfs_trans_handle *trans,
+				       struct btrfs_root *root,
+				       struct extent_buffer *buf,
+				       struct extent_buffer *cow)
+{
+	u64 refs;
+	u64 owner;
+	u64 flags;
+	u64 new_flags = 0;
+	int ret;
+
+	/*
+	 * Backrefs update rules:
+	 *
+	 * Always use full backrefs for extent pointers in tree block
+	 * allocated by tree relocation.
+	 *
+	 * If a shared tree block is no longer referenced by its owner
+	 * tree (btrfs_header_owner(buf) == root->root_key.objectid),
+	 * use full backrefs for extent pointers in tree block.
+	 *
+	 * If a tree block is been relocating
+	 * (root->root_key.objectid == BTRFS_TREE_RELOC_OBJECTID),
+	 * use full backrefs for extent pointers in tree block.
+	 * The reason for this is some operations (such as drop tree)
+	 * are only allowed for blocks use full backrefs.
+	 */
+
+	if (btrfs_block_can_be_shared(root, buf)) {
+		ret = btrfs_lookup_extent_info(trans, root, buf->start,
+					       buf->len, &refs, &flags);
+		BUG_ON(ret);
+		BUG_ON(refs == 0);
+	} else {
+		refs = 1;
+		if (root->root_key.objectid == BTRFS_TREE_RELOC_OBJECTID ||
+		    btrfs_header_backref_rev(buf) < BTRFS_MIXED_BACKREF_REV)
+			flags = BTRFS_BLOCK_FLAG_FULL_BACKREF;
+		else
+			flags = 0;
+	}
+
+	owner = btrfs_header_owner(buf);
+	BUG_ON(owner == BTRFS_TREE_RELOC_OBJECTID &&
+	       !(flags & BTRFS_BLOCK_FLAG_FULL_BACKREF));
+
+	if (refs > 1) {
+		if ((owner == root->root_key.objectid ||
+		     root->root_key.objectid == BTRFS_TREE_RELOC_OBJECTID) &&
+		    !(flags & BTRFS_BLOCK_FLAG_FULL_BACKREF)) {
+			ret = btrfs_inc_ref(trans, root, buf, 1);
+			BUG_ON(ret);
+
+			if (root->root_key.objectid ==
+			    BTRFS_TREE_RELOC_OBJECTID) {
+				ret = btrfs_dec_ref(trans, root, buf, 0);
+				BUG_ON(ret);
+				ret = btrfs_inc_ref(trans, root, cow, 1);
+				BUG_ON(ret);
+			}
+			new_flags |= BTRFS_BLOCK_FLAG_FULL_BACKREF;
+		} else {
+
+			if (root->root_key.objectid ==
+			    BTRFS_TREE_RELOC_OBJECTID)
+				ret = btrfs_inc_ref(trans, root, cow, 1);
+			else
+				ret = btrfs_inc_ref(trans, root, cow, 0);
+			BUG_ON(ret);
+		}
+		if (new_flags != 0) {
+			ret = btrfs_set_disk_extent_flags(trans, root,
+							  buf->start,
+							  buf->len,
+							  new_flags, 0);
+			BUG_ON(ret);
+		}
+	} else {
+		if (flags & BTRFS_BLOCK_FLAG_FULL_BACKREF) {
+			if (root->root_key.objectid ==
+			    BTRFS_TREE_RELOC_OBJECTID)
+				ret = btrfs_inc_ref(trans, root, cow, 1);
+			else
+				ret = btrfs_inc_ref(trans, root, cow, 0);
+			BUG_ON(ret);
+			ret = btrfs_dec_ref(trans, root, buf, 1);
+			BUG_ON(ret);
+		}
+		clean_tree_block(trans, root, buf);
+	}
+	return 0;
+}
+
 /*
  * does the dirty work in cow of a single block.  The parent block (if
  * supplied) is updated to point to the new cow copy.  The new buffer is marked
@@ -262,34 +383,39 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 			     struct extent_buffer **cow_ret,
 			     u64 search_start, u64 empty_size)
 {
-	u64 parent_start;
+	struct btrfs_disk_key disk_key;
 	struct extent_buffer *cow;
-	u32 nritems;
-	int ret = 0;
 	int level;
 	int unlock_orig = 0;
+	u64 parent_start;
 
 	if (*cow_ret == buf)
 		unlock_orig = 1;
 
 	btrfs_assert_tree_locked(buf);
 
-	if (parent)
-		parent_start = parent->start;
-	else
-		parent_start = 0;
-
 	WARN_ON(root->ref_cows && trans->transid !=
 		root->fs_info->running_transaction->transid);
 	WARN_ON(root->ref_cows && trans->transid != root->last_trans);
 
 	level = btrfs_header_level(buf);
-	nritems = btrfs_header_nritems(buf);
 
-	cow = btrfs_alloc_free_block(trans, root, buf->len,
-				     parent_start, root->root_key.objectid,
-				     trans->transid, level,
-				     search_start, empty_size);
+	if (level == 0)
+		btrfs_item_key(buf, &disk_key, 0);
+	else
+		btrfs_node_key(buf, &disk_key, 0);
+
+	if (root->root_key.objectid == BTRFS_TREE_RELOC_OBJECTID) {
+		if (parent)
+			parent_start = parent->start;
+		else
+			parent_start = 0;
+	} else
+		parent_start = 0;
+
+	cow = btrfs_alloc_free_block(trans, root, buf->len, parent_start,
+				     root->root_key.objectid, &disk_key,
+				     level, search_start, empty_size);
 	if (IS_ERR(cow))
 		return PTR_ERR(cow);
 
@@ -298,83 +424,53 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 	copy_extent_buffer(cow, buf, 0, 0, cow->len);
 	btrfs_set_header_bytenr(cow, cow->start);
 	btrfs_set_header_generation(cow, trans->transid);
-	btrfs_set_header_owner(cow, root->root_key.objectid);
-	btrfs_clear_header_flag(cow, BTRFS_HEADER_FLAG_WRITTEN);
+	btrfs_set_header_backref_rev(cow, BTRFS_MIXED_BACKREF_REV);
+	btrfs_clear_header_flag(cow, BTRFS_HEADER_FLAG_WRITTEN |
+				     BTRFS_HEADER_FLAG_RELOC);
+	if (root->root_key.objectid == BTRFS_TREE_RELOC_OBJECTID)
+		btrfs_set_header_flag(cow, BTRFS_HEADER_FLAG_RELOC);
+	else
+		btrfs_set_header_owner(cow, root->root_key.objectid);
 
 	write_extent_buffer(cow, root->fs_info->fsid,
 			    (unsigned long)btrfs_header_fsid(cow),
 			    BTRFS_FSID_SIZE);
 
-	WARN_ON(btrfs_header_generation(buf) > trans->transid);
-	if (btrfs_header_generation(buf) != trans->transid) {
-		u32 nr_extents;
-		ret = btrfs_inc_ref(trans, root, buf, cow, &nr_extents);
-		if (ret)
-			return ret;
-
-		ret = btrfs_cache_ref(trans, root, buf, nr_extents);
-		WARN_ON(ret);
-	} else if (btrfs_header_owner(buf) == BTRFS_TREE_RELOC_OBJECTID) {
-		/*
-		 * There are only two places that can drop reference to
-		 * tree blocks owned by living reloc trees, one is here,
-		 * the other place is btrfs_drop_subtree. In both places,
-		 * we check reference count while tree block is locked.
-		 * Furthermore, if reference count is one, it won't get
-		 * increased by someone else.
-		 */
-		u32 refs;
-		ret = btrfs_lookup_extent_ref(trans, root, buf->start,
-					      buf->len, &refs);
-		BUG_ON(ret);
-		if (refs == 1) {
-			ret = btrfs_update_ref(trans, root, buf, cow,
-					       0, nritems);
-			clean_tree_block(trans, root, buf);
-		} else {
-			ret = btrfs_inc_ref(trans, root, buf, cow, NULL);
-		}
-		BUG_ON(ret);
-	} else {
-		ret = btrfs_update_ref(trans, root, buf, cow, 0, nritems);
-		if (ret)
-			return ret;
-		clean_tree_block(trans, root, buf);
-	}
-
-	if (root->root_key.objectid == BTRFS_TREE_RELOC_OBJECTID) {
-		ret = btrfs_reloc_tree_cache_ref(trans, root, cow, buf->start);
-		WARN_ON(ret);
-	}
+	update_ref_for_cow(trans, root, buf, cow);
 
 	if (buf == root->node) {
 		WARN_ON(parent && parent != buf);
+		if (root->root_key.objectid == BTRFS_TREE_RELOC_OBJECTID ||
+		    btrfs_header_backref_rev(buf) < BTRFS_MIXED_BACKREF_REV)
+			parent_start = buf->start;
+		else
+			parent_start = 0;
 
 		spin_lock(&root->node_lock);
 		root->node = cow;
 		extent_buffer_get(cow);
 		spin_unlock(&root->node_lock);
 
-		if (buf != root->commit_root) {
-			btrfs_free_extent(trans, root, buf->start,
-					  buf->len, buf->start,
-					  root->root_key.objectid,
-					  btrfs_header_generation(buf),
-					  level, 1);
-		}
+		btrfs_free_extent(trans, root, buf->start, buf->len,
+				  parent_start, root->root_key.objectid,
+				  level, 0);
 		free_extent_buffer(buf);
 		add_root_to_dirty_list(root);
 	} else {
+		if (root->root_key.objectid == BTRFS_TREE_RELOC_OBJECTID)
+			parent_start = parent->start;
+		else
+			parent_start = 0;
+
+		WARN_ON(trans->transid != btrfs_header_generation(parent));
 		btrfs_set_node_blockptr(parent, parent_slot,
 					cow->start);
-		WARN_ON(trans->transid == 0);
 		btrfs_set_node_ptr_generation(parent, parent_slot,
 					      trans->transid);
 		btrfs_mark_buffer_dirty(parent);
-		WARN_ON(btrfs_header_generation(parent) != trans->transid);
 		btrfs_free_extent(trans, root, buf->start, buf->len,
-				  parent_start, btrfs_header_owner(parent),
-				  btrfs_header_generation(parent), level, 1);
+				  parent_start, root->root_key.objectid,
+				  level, 0);
 	}
 	if (unlock_orig)
 		btrfs_tree_unlock(buf);
@@ -384,6 +480,18 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 	return 0;
 }
 
+static inline int should_cow_block(struct btrfs_trans_handle *trans,
+				   struct btrfs_root *root,
+				   struct extent_buffer *buf)
+{
+	if (btrfs_header_generation(buf) == trans->transid &&
+	    !btrfs_header_flag(buf, BTRFS_HEADER_FLAG_WRITTEN) &&
+	    !(root->root_key.objectid != BTRFS_TREE_RELOC_OBJECTID &&
+	      btrfs_header_flag(buf, BTRFS_HEADER_FLAG_RELOC)))
+		return 0;
+	return 1;
+}
+
 /*
  * cows a single block, see __btrfs_cow_block for the real work.
  * This version of it has extra checks so that a block isn't cow'd more than
@@ -411,9 +519,7 @@ noinline int btrfs_cow_block(struct btrfs_trans_handle *trans,
 		WARN_ON(1);
 	}
 
-	if (btrfs_header_generation(buf) == trans->transid &&
-	    btrfs_header_owner(buf) == root->root_key.objectid &&
-	    !btrfs_header_flag(buf, BTRFS_HEADER_FLAG_WRITTEN)) {
+	if (!should_cow_block(trans, root, buf)) {
 		*cow_ret = buf;
 		return 0;
 	}
@@ -469,7 +575,7 @@ static int comp_keys(struct btrfs_disk_key *disk, struct btrfs_key *k2)
 /*
  * same as comp_keys only with two btrfs_key's
  */
-static int comp_cpu_keys(struct btrfs_key *k1, struct btrfs_key *k2)
+int btrfs_comp_cpu_keys(struct btrfs_key *k1, struct btrfs_key *k2)
 {
 	if (k1->objectid > k2->objectid)
 		return 1;
@@ -845,6 +951,12 @@ static int bin_search(struct extent_buffer *eb, struct btrfs_key *key,
 	return -1;
 }
 
+int btrfs_bin_search(struct extent_buffer *eb, struct btrfs_key *key,
+		     int level, int *slot)
+{
+	return bin_search(eb, key, level, slot);
+}
+
 /* given a node and slot number, this reads the blocks it points to.  The
  * extent buffer is returned with a reference taken (but unlocked).
  * NULL is returned on error.
@@ -921,13 +1033,6 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		root->node = child;
 		spin_unlock(&root->node_lock);
 
-		ret = btrfs_update_extent_ref(trans, root, child->start,
-					      child->len,
-					      mid->start, child->start,
-					      root->root_key.objectid,
-					      trans->transid, level - 1);
-		BUG_ON(ret);
-
 		add_root_to_dirty_list(root);
 		btrfs_tree_unlock(child);
 
@@ -938,9 +1043,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		/* once for the path */
 		free_extent_buffer(mid);
 		ret = btrfs_free_extent(trans, root, mid->start, mid->len,
-					mid->start, root->root_key.objectid,
-					btrfs_header_generation(mid),
-					level, 1);
+					0, root->root_key.objectid, level, 1);
 		/* once for the root ptr */
 		free_extent_buffer(mid);
 		return ret;
@@ -998,7 +1101,6 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 			ret = wret;
 		if (btrfs_header_nritems(right) == 0) {
 			u64 bytenr = right->start;
-			u64 generation = btrfs_header_generation(parent);
 			u32 blocksize = right->len;
 
 			clean_tree_block(trans, root, right);
@@ -1010,9 +1112,9 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 			if (wret)
 				ret = wret;
 			wret = btrfs_free_extent(trans, root, bytenr,
-						 blocksize, parent->start,
-						 btrfs_header_owner(parent),
-						 generation, level, 1);
+						 blocksize, 0,
+						 root->root_key.objectid,
+						 level, 0);
 			if (wret)
 				ret = wret;
 		} else {
@@ -1047,7 +1149,6 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 	}
 	if (btrfs_header_nritems(mid) == 0) {
 		/* we've managed to empty the middle node, drop it */
-		u64 root_gen = btrfs_header_generation(parent);
 		u64 bytenr = mid->start;
 		u32 blocksize = mid->len;
 
@@ -1059,9 +1160,8 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		if (wret)
 			ret = wret;
 		wret = btrfs_free_extent(trans, root, bytenr, blocksize,
-					 parent->start,
-					 btrfs_header_owner(parent),
-					 root_gen, level, 1);
+					 0, root->root_key.objectid,
+					 level, 0);
 		if (wret)
 			ret = wret;
 	} else {
@@ -1437,7 +1537,7 @@ noinline void btrfs_unlock_up_safe(struct btrfs_path *path, int level)
 {
 	int i;
 
-	if (path->keep_locks || path->lowest_level)
+	if (path->keep_locks)
 		return;
 
 	for (i = level; i < BTRFS_MAX_LEVEL; i++) {
@@ -1614,10 +1714,17 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 		lowest_unlock = 2;
 
 again:
-	if (p->skip_locking)
-		b = btrfs_root_node(root);
-	else
-		b = btrfs_lock_root_node(root);
+	if (p->search_commit_root) {
+		b = root->commit_root;
+		extent_buffer_get(b);
+		if (!p->skip_locking)
+			btrfs_tree_lock(b);
+	} else {
+		if (p->skip_locking)
+			b = btrfs_root_node(root);
+		else
+			b = btrfs_lock_root_node(root);
+	}
 
 	while (b) {
 		level = btrfs_header_level(b);
@@ -1638,11 +1745,9 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 			 * then we don't want to set the path blocking,
 			 * so we test it here
 			 */
-			if (btrfs_header_generation(b) == trans->transid &&
-			    btrfs_header_owner(b) == root->root_key.objectid &&
-			    !btrfs_header_flag(b, BTRFS_HEADER_FLAG_WRITTEN)) {
+			if (!should_cow_block(trans, root, b))
 				goto cow_done;
-			}
+
 			btrfs_set_path_blocking(p);
 
 			wret = btrfs_cow_block(trans, root, b,
@@ -1764,138 +1869,6 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 	return ret;
 }
 
-int btrfs_merge_path(struct btrfs_trans_handle *trans,
-		     struct btrfs_root *root,
-		     struct btrfs_key *node_keys,
-		     u64 *nodes, int lowest_level)
-{
-	struct extent_buffer *eb;
-	struct extent_buffer *parent;
-	struct btrfs_key key;
-	u64 bytenr;
-	u64 generation;
-	u32 blocksize;
-	int level;
-	int slot;
-	int key_match;
-	int ret;
-
-	eb = btrfs_lock_root_node(root);
-	ret = btrfs_cow_block(trans, root, eb, NULL, 0, &eb);
-	BUG_ON(ret);
-
-	btrfs_set_lock_blocking(eb);
-
-	parent = eb;
-	while (1) {
-		level = btrfs_header_level(parent);
-		if (level == 0 || level <= lowest_level)
-			break;
-
-		ret = bin_search(parent, &node_keys[lowest_level], level,
-				 &slot);
-		if (ret && slot > 0)
-			slot--;
-
-		bytenr = btrfs_node_blockptr(parent, slot);
-		if (nodes[level - 1] == bytenr)
-			break;
-
-		blocksize = btrfs_level_size(root, level - 1);
-		generation = btrfs_node_ptr_generation(parent, slot);
-		btrfs_node_key_to_cpu(eb, &key, slot);
-		key_match = !memcmp(&key, &node_keys[level - 1], sizeof(key));
-
-		if (generation == trans->transid) {
-			eb = read_tree_block(root, bytenr, blocksize,
-					     generation);
-			btrfs_tree_lock(eb);
-			btrfs_set_lock_blocking(eb);
-		}
-
-		/*
-		 * if node keys match and node pointer hasn't been modified
-		 * in the running transaction, we can merge the path. for
-		 * blocks owened by reloc trees, the node pointer check is
-		 * skipped, this is because these blocks are fully controlled
-		 * by the space balance code, no one else can modify them.
-		 */
-		if (!nodes[level - 1] || !key_match ||
-		    (generation == trans->transid &&
-		     btrfs_header_owner(eb) != BTRFS_TREE_RELOC_OBJECTID)) {
-			if (level == 1 || level == lowest_level + 1) {
-				if (generation == trans->transid) {
-					btrfs_tree_unlock(eb);
-					free_extent_buffer(eb);
-				}
-				break;
-			}
-
-			if (generation != trans->transid) {
-				eb = read_tree_block(root, bytenr, blocksize,
-						generation);
-				btrfs_tree_lock(eb);
-				btrfs_set_lock_blocking(eb);
-			}
-
-			ret = btrfs_cow_block(trans, root, eb, parent, slot,
-					      &eb);
-			BUG_ON(ret);
-
-			if (root->root_key.objectid ==
-			    BTRFS_TREE_RELOC_OBJECTID) {
-				if (!nodes[level - 1]) {
-					nodes[level - 1] = eb->start;
-					memcpy(&node_keys[level - 1], &key,
-					       sizeof(node_keys[0]));
-				} else {
-					WARN_ON(1);
-				}
-			}
-
-			btrfs_tree_unlock(parent);
-			free_extent_buffer(parent);
-			parent = eb;
-			continue;
-		}
-
-		btrfs_set_node_blockptr(parent, slot, nodes[level - 1]);
-		btrfs_set_node_ptr_generation(parent, slot, trans->transid);
-		btrfs_mark_buffer_dirty(parent);
-
-		ret = btrfs_inc_extent_ref(trans, root,
-					nodes[level - 1],
-					blocksize, parent->start,
-					btrfs_header_owner(parent),
-					btrfs_header_generation(parent),
-					level - 1);
-		BUG_ON(ret);
-
-		/*
-		 * If the block was created in the running transaction,
-		 * it's possible this is the last reference to it, so we
-		 * should drop the subtree.
-		 */
-		if (generation == trans->transid) {
-			ret = btrfs_drop_subtree(trans, root, eb, parent);
-			BUG_ON(ret);
-			btrfs_tree_unlock(eb);
-			free_extent_buffer(eb);
-		} else {
-			ret = btrfs_free_extent(trans, root, bytenr,
-					blocksize, parent->start,
-					btrfs_header_owner(parent),
-					btrfs_header_generation(parent),
-					level - 1, 1);
-			BUG_ON(ret);
-		}
-		break;
-	}
-	btrfs_tree_unlock(parent);
-	free_extent_buffer(parent);
-	return 0;
-}
-
 /*
  * adjust the pointers going up the tree, starting at level
  * making sure the right key of each node is points to 'key'.
@@ -2021,9 +1994,6 @@ static int push_node_left(struct btrfs_trans_handle *trans,
 	btrfs_mark_buffer_dirty(src);
 	btrfs_mark_buffer_dirty(dst);
 
-	ret = btrfs_update_ref(trans, root, src, dst, dst_nritems, push_items);
-	BUG_ON(ret);
-
 	return ret;
 }
 
@@ -2083,9 +2053,6 @@ static int balance_node_right(struct btrfs_trans_handle *trans,
 	btrfs_mark_buffer_dirty(src);
 	btrfs_mark_buffer_dirty(dst);
 
-	ret = btrfs_update_ref(trans, root, src, dst, 0, push_items);
-	BUG_ON(ret);
-
 	return ret;
 }
 
@@ -2105,7 +2072,6 @@ static noinline int insert_new_root(struct btrfs_trans_handle *trans,
 	struct extent_buffer *c;
 	struct extent_buffer *old;
 	struct btrfs_disk_key lower_key;
-	int ret;
 
 	BUG_ON(path->nodes[level]);
 	BUG_ON(path->nodes[level-1] != root->node);
@@ -2117,16 +2083,17 @@ static noinline int insert_new_root(struct btrfs_trans_handle *trans,
 		btrfs_node_key(lower, &lower_key, 0);
 
 	c = btrfs_alloc_free_block(trans, root, root->nodesize, 0,
-				   root->root_key.objectid, trans->transid,
+				   root->root_key.objectid, &lower_key,
 				   level, root->node->start, 0);
 	if (IS_ERR(c))
 		return PTR_ERR(c);
 
-	memset_extent_buffer(c, 0, 0, root->nodesize);
+	memset_extent_buffer(c, 0, 0, sizeof(struct btrfs_header));
 	btrfs_set_header_nritems(c, 1);
 	btrfs_set_header_level(c, level);
 	btrfs_set_header_bytenr(c, c->start);
 	btrfs_set_header_generation(c, trans->transid);
+	btrfs_set_header_backref_rev(c, BTRFS_MIXED_BACKREF_REV);
 	btrfs_set_header_owner(c, root->root_key.objectid);
 
 	write_extent_buffer(c, root->fs_info->fsid,
@@ -2151,12 +2118,6 @@ static noinline int insert_new_root(struct btrfs_trans_handle *trans,
 	root->node = c;
 	spin_unlock(&root->node_lock);
 
-	ret = btrfs_update_extent_ref(trans, root, lower->start,
-				      lower->len, lower->start, c->start,
-				      root->root_key.objectid,
-				      trans->transid, level - 1);
-	BUG_ON(ret);
-
 	/* the super has an extra ref to root->node */
 	free_extent_buffer(old);
 
@@ -2244,20 +2205,21 @@ static noinline int split_node(struct btrfs_trans_handle *trans,
 	}
 
 	c_nritems = btrfs_header_nritems(c);
+	mid = (c_nritems + 1) / 2;
+	btrfs_node_key(c, &disk_key, mid);
 
-	split = btrfs_alloc_free_block(trans, root, root->nodesize,
-					path->nodes[level + 1]->start,
+	split = btrfs_alloc_free_block(trans, root, root->nodesize, 0,
 					root->root_key.objectid,
-					trans->transid, level, c->start, 0);
+					&disk_key, level, c->start, 0);
 	if (IS_ERR(split))
 		return PTR_ERR(split);
 
-	btrfs_set_header_flags(split, btrfs_header_flags(c));
+	memset_extent_buffer(split, 0, 0, sizeof(struct btrfs_header));
 	btrfs_set_header_level(split, btrfs_header_level(c));
 	btrfs_set_header_bytenr(split, split->start);
 	btrfs_set_header_generation(split, trans->transid);
+	btrfs_set_header_backref_rev(split, BTRFS_MIXED_BACKREF_REV);
 	btrfs_set_header_owner(split, root->root_key.objectid);
-	btrfs_set_header_flags(split, 0);
 	write_extent_buffer(split, root->fs_info->fsid,
 			    (unsigned long)btrfs_header_fsid(split),
 			    BTRFS_FSID_SIZE);
@@ -2265,7 +2227,6 @@ static noinline int split_node(struct btrfs_trans_handle *trans,
 			    (unsigned long)btrfs_header_chunk_tree_uuid(split),
 			    BTRFS_UUID_SIZE);
 
-	mid = (c_nritems + 1) / 2;
 
 	copy_extent_buffer(split, c,
 			   btrfs_node_key_ptr_offset(0),
@@ -2278,16 +2239,12 @@ static noinline int split_node(struct btrfs_trans_handle *trans,
 	btrfs_mark_buffer_dirty(c);
 	btrfs_mark_buffer_dirty(split);
 
-	btrfs_node_key(split, &disk_key, 0);
 	wret = insert_ptr(trans, root, path, &disk_key, split->start,
 			  path->slots[level + 1] + 1,
 			  level + 1);
 	if (wret)
 		ret = wret;
 
-	ret = btrfs_update_ref(trans, root, c, split, 0, c_nritems - mid);
-	BUG_ON(ret);
-
 	if (path->slots[level] >= mid) {
 		path->slots[level] -= mid;
 		btrfs_tree_unlock(c);
@@ -2360,7 +2317,6 @@ static noinline int __push_leaf_right(struct btrfs_trans_handle *trans,
 	u32 right_nritems;
 	u32 data_end;
 	u32 this_item_size;
-	int ret;
 
 	if (empty)
 		nr = 0;
@@ -2473,9 +2429,6 @@ static noinline int __push_leaf_right(struct btrfs_trans_handle *trans,
 		btrfs_mark_buffer_dirty(left);
 	btrfs_mark_buffer_dirty(right);
 
-	ret = btrfs_update_ref(trans, root, left, right, 0, push_items);
-	BUG_ON(ret);
-
 	btrfs_item_key(right, &disk_key, 0);
 	btrfs_set_node_key(upper, &disk_key, slot + 1);
 	btrfs_mark_buffer_dirty(upper);
@@ -2720,10 +2673,6 @@ static noinline int __push_leaf_left(struct btrfs_trans_handle *trans,
 	if (right_nritems)
 		btrfs_mark_buffer_dirty(right);
 
-	ret = btrfs_update_ref(trans, root, right, left,
-			       old_left_nritems, push_items);
-	BUG_ON(ret);
-
 	btrfs_item_key(right, &disk_key, 0);
 	wret = fixup_low_keys(trans, root, path, &disk_key, 1);
 	if (wret)
@@ -2880,9 +2829,6 @@ static noinline int copy_for_split(struct btrfs_trans_handle *trans,
 	btrfs_mark_buffer_dirty(l);
 	BUG_ON(path->slots[0] != slot);
 
-	ret = btrfs_update_ref(trans, root, l, right, 0, nritems);
-	BUG_ON(ret);
-
 	if (mid <= slot) {
 		btrfs_tree_unlock(path->nodes[0]);
 		free_extent_buffer(path->nodes[0]);
@@ -2911,6 +2857,7 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 			       struct btrfs_path *path, int data_size,
 			       int extend)
 {
+	struct btrfs_disk_key disk_key;
 	struct extent_buffer *l;
 	u32 nritems;
 	int mid;
@@ -2918,7 +2865,7 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 	struct extent_buffer *right;
 	int ret = 0;
 	int wret;
-	int double_split;
+	int split;
 	int num_doubles = 0;
 
 	/* first try to make some room by pushing left and right */
@@ -2945,16 +2892,53 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 			return ret;
 	}
 again:
-	double_split = 0;
+	split = 1;
 	l = path->nodes[0];
 	slot = path->slots[0];
 	nritems = btrfs_header_nritems(l);
 	mid = (nritems + 1) / 2;
 
-	right = btrfs_alloc_free_block(trans, root, root->leafsize,
-					path->nodes[1]->start,
+	if (mid <= slot) {
+		if (nritems == 1 ||
+		    leaf_space_used(l, mid, nritems - mid) + data_size >
+			BTRFS_LEAF_DATA_SIZE(root)) {
+			if (slot >= nritems) {
+				split = 0;
+			} else {
+				mid = slot;
+				if (mid != nritems &&
+				    leaf_space_used(l, mid, nritems - mid) +
+				    data_size > BTRFS_LEAF_DATA_SIZE(root)) {
+					split = 2;
+				}
+			}
+		}
+	} else {
+		if (leaf_space_used(l, 0, mid) + data_size >
+			BTRFS_LEAF_DATA_SIZE(root)) {
+			if (!extend && data_size && slot == 0) {
+				split = 0;
+			} else if ((extend || !data_size) && slot == 0) {
+				mid = 1;
+			} else {
+				mid = slot;
+				if (mid != nritems &&
+				    leaf_space_used(l, mid, nritems - mid) +
+				    data_size > BTRFS_LEAF_DATA_SIZE(root)) {
+					split = 2 ;
+				}
+			}
+		}
+	}
+
+	if (split == 0)
+		btrfs_cpu_key_to_disk(&disk_key, ins_key);
+	else
+		btrfs_item_key(l, &disk_key, mid);
+
+	right = btrfs_alloc_free_block(trans, root, root->leafsize, 0,
 					root->root_key.objectid,
-					trans->transid, 0, l->start, 0);
+					&disk_key, 0, l->start, 0);
 	if (IS_ERR(right)) {
 		BUG_ON(1);
 		return PTR_ERR(right);
@@ -2963,6 +2947,7 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 	memset_extent_buffer(right, 0, 0, sizeof(struct btrfs_header));
 	btrfs_set_header_bytenr(right, right->start);
 	btrfs_set_header_generation(right, trans->transid);
+	btrfs_set_header_backref_rev(right, BTRFS_MIXED_BACKREF_REV);
 	btrfs_set_header_owner(right, root->root_key.objectid);
 	btrfs_set_header_level(right, 0);
 	write_extent_buffer(right, root->fs_info->fsid,
@@ -2973,79 +2958,47 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 			    (unsigned long)btrfs_header_chunk_tree_uuid(right),
 			    BTRFS_UUID_SIZE);
 
-	if (mid <= slot) {
-		if (nritems == 1 ||
-		    leaf_space_used(l, mid, nritems - mid) + data_size >
-			BTRFS_LEAF_DATA_SIZE(root)) {
-			if (slot >= nritems) {
-				struct btrfs_disk_key disk_key;
-
-				btrfs_cpu_key_to_disk(&disk_key, ins_key);
-				btrfs_set_header_nritems(right, 0);
-				wret = insert_ptr(trans, root, path,
-						  &disk_key, right->start,
-						  path->slots[1] + 1, 1);
-				if (wret)
-					ret = wret;
+	if (split == 0) {
+		if (mid <= slot) {
+			btrfs_set_header_nritems(right, 0);
+			wret = insert_ptr(trans, root, path,
+					  &disk_key, right->start,
+					  path->slots[1] + 1, 1);
+			if (wret)
+				ret = wret;
 
-				btrfs_tree_unlock(path->nodes[0]);
-				free_extent_buffer(path->nodes[0]);
-				path->nodes[0] = right;
-				path->slots[0] = 0;
-				path->slots[1] += 1;
-				btrfs_mark_buffer_dirty(right);
-				return ret;
-			}
-			mid = slot;
-			if (mid != nritems &&
-			    leaf_space_used(l, mid, nritems - mid) +
-			    data_size > BTRFS_LEAF_DATA_SIZE(root)) {
-				double_split = 1;
-			}
-		}
-	} else {
-		if (leaf_space_used(l, 0, mid) + data_size >
-			BTRFS_LEAF_DATA_SIZE(root)) {
-			if (!extend && data_size && slot == 0) {
-				struct btrfs_disk_key disk_key;
-
-				btrfs_cpu_key_to_disk(&disk_key, ins_key);
-				btrfs_set_header_nritems(right, 0);
-				wret = insert_ptr(trans, root, path,
-						  &disk_key,
-						  right->start,
-						  path->slots[1], 1);
+			btrfs_tree_unlock(path->nodes[0]);
+			free_extent_buffer(path->nodes[0]);
+			path->nodes[0] = right;
+			path->slots[0] = 0;
+			path->slots[1] += 1;
+		} else {
+			btrfs_set_header_nritems(right, 0);
+			wret = insert_ptr(trans, root, path,
+					  &disk_key,
+					  right->start,
+					  path->slots[1], 1);
+			if (wret)
+				ret = wret;
+			btrfs_tree_unlock(path->nodes[0]);
+			free_extent_buffer(path->nodes[0]);
+			path->nodes[0] = right;
+			path->slots[0] = 0;
+			if (path->slots[1] == 0) {
+				wret = fixup_low_keys(trans, root,
+						path, &disk_key, 1);
 				if (wret)
 					ret = wret;
-				btrfs_tree_unlock(path->nodes[0]);
-				free_extent_buffer(path->nodes[0]);
-				path->nodes[0] = right;
-				path->slots[0] = 0;
-				if (path->slots[1] == 0) {
-					wret = fixup_low_keys(trans, root,
-						      path, &disk_key, 1);
-					if (wret)
-						ret = wret;
-				}
-				btrfs_mark_buffer_dirty(right);
-				return ret;
-			} else if ((extend || !data_size) && slot == 0) {
-				mid = 1;
-			} else {
-				mid = slot;
-				if (mid != nritems &&
-				    leaf_space_used(l, mid, nritems - mid) +
-				    data_size > BTRFS_LEAF_DATA_SIZE(root)) {
-					double_split = 1;
-				}
 			}
 		}
+		btrfs_mark_buffer_dirty(right);
+		return ret;
 	}
 
 	ret = copy_for_split(trans, root, path, l, right, slot, mid, nritems);
 	BUG_ON(ret);
 
-	if (double_split) {
+	if (split == 2) {
 		BUG_ON(num_doubles != 0);
 		num_doubles++;
 		goto again;
@@ -3447,7 +3400,7 @@ int btrfs_insert_some_items(struct btrfs_trans_handle *trans,
 		/* figure out how many keys we can insert in here */
 		total_data = data_size[0];
 		for (i = 1; i < nr; i++) {
-			if (comp_cpu_keys(&found_key, cpu_key + i) <= 0)
+			if (btrfs_comp_cpu_keys(&found_key, cpu_key + i) <= 0)
 				break;
 			total_data += data_size[i];
 		}
@@ -3745,9 +3698,7 @@ static int del_ptr(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 
 /*
  * a helper function to delete the leaf pointed to by path->slots[1] and
- * path->nodes[1].  bytenr is the node block pointer, but since the callers
- * already know it, it is faster to have them pass it down than to
- * read it out of the node again.
+ * path->nodes[1].
  *
  * This deletes the pointer in path->nodes[1] and frees the leaf
  * block extent.  zero is returned if it all worked out, < 0 otherwise.
@@ -3755,15 +3706,14 @@ static int del_ptr(struct btrfs_trans_handle *trans, struct btrfs_root *root,
  * The path must have already been setup for deleting the leaf, including
  * all the proper balancing.  path->nodes[1] must be locked.
  */
-noinline int btrfs_del_leaf(struct btrfs_trans_handle *trans,
-			    struct btrfs_root *root,
-			    struct btrfs_path *path, u64 bytenr)
+static noinline int btrfs_del_leaf(struct btrfs_trans_handle *trans,
+				   struct btrfs_root *root,
+				   struct btrfs_path *path,
+				   struct extent_buffer *leaf)
 {
 	int ret;
-	u64 root_gen = btrfs_header_generation(path->nodes[1]);
-	u64 parent_start = path->nodes[1]->start;
-	u64 parent_owner = btrfs_header_owner(path->nodes[1]);
 
+	WARN_ON(btrfs_header_generation(leaf) != trans->transid);
 	ret = del_ptr(trans, root, path, 1, path->slots[1]);
 	if (ret)
 		return ret;
@@ -3774,10 +3724,8 @@ noinline int btrfs_del_leaf(struct btrfs_trans_handle *trans,
 	 */
 	btrfs_unlock_up_safe(path, 0);
 
-	ret = btrfs_free_extent(trans, root, bytenr,
-				btrfs_level_size(root, 0),
-				parent_start, parent_owner,
-				root_gen, 0, 1);
+	ret = btrfs_free_extent(trans, root, leaf->start, leaf->len,
+				0, root->root_key.objectid, 0, 0);
 	return ret;
 }
 /*
@@ -3845,7 +3793,7 @@ int btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 		if (leaf == root->node) {
 			btrfs_set_header_level(leaf, 0);
 		} else {
-			ret = btrfs_del_leaf(trans, root, path, leaf->start);
+			ret = btrfs_del_leaf(trans, root, path, leaf);
 			BUG_ON(ret);
 		}
 	} else {
@@ -3884,8 +3832,7 @@ int btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 
 			if (btrfs_header_nritems(leaf) == 0) {
 				path->slots[1] = slot;
-				ret = btrfs_del_leaf(trans, root, path,
-						     leaf->start);
+				ret = btrfs_del_leaf(trans, root, path, leaf);
 				BUG_ON(ret);
 				free_extent_buffer(leaf);
 			} else {

commit 76a05b35a320e8c968d0fec8f512a1acae227309
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu May 14 13:24:30 2009 -0400

    Btrfs: Don't loop forever on metadata IO failures
    
    When a btrfs metadata read fails, the first thing we try to do is find
    a good copy on another mirror of the block.  If this fails, read_tree_block()
    ends up returning a buffer that isn't up to date.
    
    The btrfs btree reading code was reworked to drop locks and repeat
    the search when IO was done, but the changes didn't add a check for failed
    reads.  The end result was looping forever on buffers that were never
    going to become up to date.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index a99f1c2a710d..fedf8b9f03a2 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1469,6 +1469,7 @@ read_block_for_search(struct btrfs_trans_handle *trans,
 	u32 blocksize;
 	struct extent_buffer *b = *eb_ret;
 	struct extent_buffer *tmp;
+	int ret;
 
 	blocknr = btrfs_node_blockptr(b, slot);
 	gen = btrfs_node_ptr_generation(b, slot);
@@ -1476,6 +1477,10 @@ read_block_for_search(struct btrfs_trans_handle *trans,
 
 	tmp = btrfs_find_tree_block(root, blocknr, blocksize);
 	if (tmp && btrfs_buffer_uptodate(tmp, gen)) {
+		/*
+		 * we found an up to date block without sleeping, return
+		 * right away
+		 */
 		*eb_ret = tmp;
 		return 0;
 	}
@@ -1483,7 +1488,9 @@ read_block_for_search(struct btrfs_trans_handle *trans,
 	/*
 	 * reduce lock contention at high levels
 	 * of the btree by dropping locks before
-	 * we read.
+	 * we read.  Don't release the lock on the current
+	 * level because we need to walk this node to figure
+	 * out which blocks to read.
 	 */
 	btrfs_unlock_up_safe(p, level + 1);
 	btrfs_set_path_blocking(p);
@@ -1494,10 +1501,21 @@ read_block_for_search(struct btrfs_trans_handle *trans,
 		reada_for_search(root, p, level, slot, key->objectid);
 
 	btrfs_release_path(NULL, p);
+
+	ret = -EAGAIN;
 	tmp = read_tree_block(root, blocknr, blocksize, gen);
-	if (tmp)
+	if (tmp) {
+		/*
+		 * If the read above didn't mark this buffer up to date,
+		 * it will never end up being up to date.  Set ret to EIO now
+		 * and give up so that our caller doesn't loop forever
+		 * on our EAGAINs.
+		 */
+		if (!btrfs_buffer_uptodate(tmp, 0))
+			ret = -EIO;
 		free_extent_buffer(tmp);
-	return -EAGAIN;
+	}
+	return ret;
 }
 
 /*
@@ -1696,6 +1714,9 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 			if (ret == -EAGAIN)
 				goto again;
 
+			if (ret == -EIO)
+				goto done;
+
 			if (!p->skip_locking) {
 				int lret;
 
@@ -1738,6 +1759,8 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 	 */
 	if (!p->leave_spinning)
 		btrfs_set_path_blocking(p);
+	if (ret < 0)
+		btrfs_release_path(root, p);
 	return ret;
 }
 
@@ -4212,6 +4235,11 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 		if (ret == -EAGAIN)
 			goto again;
 
+		if (ret < 0) {
+			btrfs_release_path(root, path);
+			goto done;
+		}
+
 		if (!path->skip_locking) {
 			ret = btrfs_try_spin_lock(next);
 			if (!ret) {
@@ -4246,6 +4274,11 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 		if (ret == -EAGAIN)
 			goto again;
 
+		if (ret < 0) {
+			btrfs_release_path(root, path);
+			goto done;
+		}
+
 		if (!path->skip_locking) {
 			btrfs_assert_tree_locked(path->nodes[level]);
 			ret = btrfs_try_spin_lock(next);

commit 8c594ea81d7abbbffdda447b127f8ba8d76f319d
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Apr 20 15:50:10 2009 -0400

    Btrfs: use the right node in reada_for_balance
    
    reada_for_balance was using the wrong index into the path node array,
    so it wasn't reading the right blocks.  We never directly used the
    results of the read done by this function because the btree search is
    started over at the end.
    
    This fixes reada_for_balance to reada in the correct node and to
    avoid searching past the last slot in the node.  It also makes sure to
    hold the parent lock while we are finding the nodes to read.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index e5b2533b691a..a99f1c2a710d 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1325,12 +1325,12 @@ static noinline int reada_for_balance(struct btrfs_root *root,
 	int ret = 0;
 	int blocksize;
 
-	parent = path->nodes[level - 1];
+	parent = path->nodes[level + 1];
 	if (!parent)
 		return 0;
 
 	nritems = btrfs_header_nritems(parent);
-	slot = path->slots[level];
+	slot = path->slots[level + 1];
 	blocksize = btrfs_level_size(root, level);
 
 	if (slot > 0) {
@@ -1341,7 +1341,7 @@ static noinline int reada_for_balance(struct btrfs_root *root,
 			block1 = 0;
 		free_extent_buffer(eb);
 	}
-	if (slot < nritems) {
+	if (slot + 1 < nritems) {
 		block2 = btrfs_node_blockptr(parent, slot + 1);
 		gen = btrfs_node_ptr_generation(parent, slot + 1);
 		eb = btrfs_find_tree_block(root, block2, blocksize);
@@ -1351,7 +1351,11 @@ static noinline int reada_for_balance(struct btrfs_root *root,
 	}
 	if (block1 || block2) {
 		ret = -EAGAIN;
+
+		/* release the whole path */
 		btrfs_release_path(root, path);
+
+		/* read the blocks */
 		if (block1)
 			readahead_tree_block(root, block1, blocksize, 0);
 		if (block2)
@@ -1361,7 +1365,7 @@ static noinline int reada_for_balance(struct btrfs_root *root,
 			eb = read_tree_block(root, block1, blocksize, 0);
 			free_extent_buffer(eb);
 		}
-		if (block1) {
+		if (block2) {
 			eb = read_tree_block(root, block2, blocksize, 0);
 			free_extent_buffer(eb);
 		}
@@ -1481,12 +1485,15 @@ read_block_for_search(struct btrfs_trans_handle *trans,
 	 * of the btree by dropping locks before
 	 * we read.
 	 */
-	btrfs_release_path(NULL, p);
+	btrfs_unlock_up_safe(p, level + 1);
+	btrfs_set_path_blocking(p);
+
 	if (tmp)
 		free_extent_buffer(tmp);
 	if (p->reada)
 		reada_for_search(root, p, level, slot, key->objectid);
 
+	btrfs_release_path(NULL, p);
 	tmp = read_tree_block(root, blocknr, blocksize, gen);
 	if (tmp)
 		free_extent_buffer(tmp);

commit c293498be69816087746161338de4b81efdf69fc
Author: Stoyan Gaydarov <stoyboyker@gmail.com>
Date:   Thu Apr 2 17:05:11 2009 -0400

    Btrfs: BUG to BUG_ON changes
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index b8082762ca78..e5b2533b691a 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2157,8 +2157,7 @@ static int insert_ptr(struct btrfs_trans_handle *trans, struct btrfs_root
 	BUG_ON(!path->nodes[level]);
 	lower = path->nodes[level];
 	nritems = btrfs_header_nritems(lower);
-	if (slot > nritems)
-		BUG();
+	BUG_ON(slot > nritems);
 	if (nritems == BTRFS_NODEPTRS_PER_BLOCK(root))
 		BUG();
 	if (slot != nritems) {

commit 8e73f275011b3264a87339fd9f1690e944e381c9
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Apr 3 10:14:18 2009 -0400

    Btrfs: Optimize locking in btrfs_next_leaf()
    
    btrfs_next_leaf was using blocking locks when it could have been using
    faster spinning ones instead.  This adds a few extra checks around
    the pieces that block and switches over to spinning locks.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 271b05e507d7..b8082762ca78 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -4127,28 +4127,44 @@ int btrfs_find_next_key(struct btrfs_root *root, struct btrfs_path *path,
 int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 {
 	int slot;
-	int level = 1;
+	int level;
 	struct extent_buffer *c;
-	struct extent_buffer *next = NULL;
+	struct extent_buffer *next;
 	struct btrfs_key key;
 	u32 nritems;
 	int ret;
+	int old_spinning = path->leave_spinning;
+	int force_blocking = 0;
 
 	nritems = btrfs_header_nritems(path->nodes[0]);
 	if (nritems == 0)
 		return 1;
 
-	btrfs_item_key_to_cpu(path->nodes[0], &key, nritems - 1);
+	/*
+	 * we take the blocks in an order that upsets lockdep.  Using
+	 * blocking mode is the only way around it.
+	 */
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+	force_blocking = 1;
+#endif
 
+	btrfs_item_key_to_cpu(path->nodes[0], &key, nritems - 1);
+again:
+	level = 1;
+	next = NULL;
 	btrfs_release_path(root, path);
+
 	path->keep_locks = 1;
+
+	if (!force_blocking)
+		path->leave_spinning = 1;
+
 	ret = btrfs_search_slot(NULL, root, &key, path, 0, 0);
 	path->keep_locks = 0;
 
 	if (ret < 0)
 		return ret;
 
-	btrfs_set_path_blocking(path);
 	nritems = btrfs_header_nritems(path->nodes[0]);
 	/*
 	 * by releasing the path above we dropped all our locks.  A balance
@@ -4158,19 +4174,24 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 	 */
 	if (nritems > 0 && path->slots[0] < nritems - 1) {
 		path->slots[0]++;
+		ret = 0;
 		goto done;
 	}
 
 	while (level < BTRFS_MAX_LEVEL) {
-		if (!path->nodes[level])
-			return 1;
+		if (!path->nodes[level]) {
+			ret = 1;
+			goto done;
+		}
 
 		slot = path->slots[level] + 1;
 		c = path->nodes[level];
 		if (slot >= btrfs_header_nritems(c)) {
 			level++;
-			if (level == BTRFS_MAX_LEVEL)
-				return 1;
+			if (level == BTRFS_MAX_LEVEL) {
+				ret = 1;
+				goto done;
+			}
 			continue;
 		}
 
@@ -4179,16 +4200,22 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 			free_extent_buffer(next);
 		}
 
-		/* the path was set to blocking above */
-		if (level == 1 && (path->locks[1] || path->skip_locking) &&
-		    path->reada)
-			reada_for_search(root, path, level, slot, 0);
+		next = c;
+		ret = read_block_for_search(NULL, root, path, &next, level,
+					    slot, &key);
+		if (ret == -EAGAIN)
+			goto again;
 
-		next = read_node_slot(root, c, slot);
 		if (!path->skip_locking) {
-			btrfs_assert_tree_locked(c);
-			btrfs_tree_lock(next);
-			btrfs_set_lock_blocking(next);
+			ret = btrfs_try_spin_lock(next);
+			if (!ret) {
+				btrfs_set_path_blocking(path);
+				btrfs_tree_lock(next);
+				if (!force_blocking)
+					btrfs_clear_path_blocking(path, next);
+			}
+			if (force_blocking)
+				btrfs_set_lock_blocking(next);
 		}
 		break;
 	}
@@ -4198,27 +4225,42 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 		c = path->nodes[level];
 		if (path->locks[level])
 			btrfs_tree_unlock(c);
+
 		free_extent_buffer(c);
 		path->nodes[level] = next;
 		path->slots[level] = 0;
 		if (!path->skip_locking)
 			path->locks[level] = 1;
+
 		if (!level)
 			break;
 
-		btrfs_set_path_blocking(path);
-		if (level == 1 && path->locks[1] && path->reada)
-			reada_for_search(root, path, level, slot, 0);
-		next = read_node_slot(root, next, 0);
+		ret = read_block_for_search(NULL, root, path, &next, level,
+					    0, &key);
+		if (ret == -EAGAIN)
+			goto again;
+
 		if (!path->skip_locking) {
 			btrfs_assert_tree_locked(path->nodes[level]);
-			btrfs_tree_lock(next);
-			btrfs_set_lock_blocking(next);
+			ret = btrfs_try_spin_lock(next);
+			if (!ret) {
+				btrfs_set_path_blocking(path);
+				btrfs_tree_lock(next);
+				if (!force_blocking)
+					btrfs_clear_path_blocking(path, next);
+			}
+			if (force_blocking)
+				btrfs_set_lock_blocking(next);
 		}
 	}
+	ret = 0;
 done:
 	unlock_up(path, 0, 1);
-	return 0;
+	path->leave_spinning = old_spinning;
+	if (!old_spinning)
+		btrfs_set_path_blocking(path);
+
+	return ret;
 }
 
 /*

commit c8c42864f6193638eed136e0243f426a0b7f4bc2
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Apr 3 10:14:18 2009 -0400

    Btrfs: break up btrfs_search_slot into smaller pieces
    
    btrfs_search_slot was doing too many things at once.  This breaks
    it up into more reasonable units.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index dbb724124633..271b05e507d7 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1244,9 +1244,9 @@ static noinline int push_nodes_for_insert(struct btrfs_trans_handle *trans,
  * readahead one full node of leaves, finding things that are close
  * to the block in 'slot', and triggering ra on them.
  */
-static noinline void reada_for_search(struct btrfs_root *root,
-				      struct btrfs_path *path,
-				      int level, int slot, u64 objectid)
+static void reada_for_search(struct btrfs_root *root,
+			     struct btrfs_path *path,
+			     int level, int slot, u64 objectid)
 {
 	struct extent_buffer *node;
 	struct btrfs_disk_key disk_key;
@@ -1446,6 +1446,117 @@ noinline void btrfs_unlock_up_safe(struct btrfs_path *path, int level)
 	}
 }
 
+/*
+ * helper function for btrfs_search_slot.  The goal is to find a block
+ * in cache without setting the path to blocking.  If we find the block
+ * we return zero and the path is unchanged.
+ *
+ * If we can't find the block, we set the path blocking and do some
+ * reada.  -EAGAIN is returned and the search must be repeated.
+ */
+static int
+read_block_for_search(struct btrfs_trans_handle *trans,
+		       struct btrfs_root *root, struct btrfs_path *p,
+		       struct extent_buffer **eb_ret, int level, int slot,
+		       struct btrfs_key *key)
+{
+	u64 blocknr;
+	u64 gen;
+	u32 blocksize;
+	struct extent_buffer *b = *eb_ret;
+	struct extent_buffer *tmp;
+
+	blocknr = btrfs_node_blockptr(b, slot);
+	gen = btrfs_node_ptr_generation(b, slot);
+	blocksize = btrfs_level_size(root, level - 1);
+
+	tmp = btrfs_find_tree_block(root, blocknr, blocksize);
+	if (tmp && btrfs_buffer_uptodate(tmp, gen)) {
+		*eb_ret = tmp;
+		return 0;
+	}
+
+	/*
+	 * reduce lock contention at high levels
+	 * of the btree by dropping locks before
+	 * we read.
+	 */
+	btrfs_release_path(NULL, p);
+	if (tmp)
+		free_extent_buffer(tmp);
+	if (p->reada)
+		reada_for_search(root, p, level, slot, key->objectid);
+
+	tmp = read_tree_block(root, blocknr, blocksize, gen);
+	if (tmp)
+		free_extent_buffer(tmp);
+	return -EAGAIN;
+}
+
+/*
+ * helper function for btrfs_search_slot.  This does all of the checks
+ * for node-level blocks and does any balancing required based on
+ * the ins_len.
+ *
+ * If no extra work was required, zero is returned.  If we had to
+ * drop the path, -EAGAIN is returned and btrfs_search_slot must
+ * start over
+ */
+static int
+setup_nodes_for_search(struct btrfs_trans_handle *trans,
+		       struct btrfs_root *root, struct btrfs_path *p,
+		       struct extent_buffer *b, int level, int ins_len)
+{
+	int ret;
+	if ((p->search_for_split || ins_len > 0) && btrfs_header_nritems(b) >=
+	    BTRFS_NODEPTRS_PER_BLOCK(root) - 3) {
+		int sret;
+
+		sret = reada_for_balance(root, p, level);
+		if (sret)
+			goto again;
+
+		btrfs_set_path_blocking(p);
+		sret = split_node(trans, root, p, level);
+		btrfs_clear_path_blocking(p, NULL);
+
+		BUG_ON(sret > 0);
+		if (sret) {
+			ret = sret;
+			goto done;
+		}
+		b = p->nodes[level];
+	} else if (ins_len < 0 && btrfs_header_nritems(b) <
+		   BTRFS_NODEPTRS_PER_BLOCK(root) / 4) {
+		int sret;
+
+		sret = reada_for_balance(root, p, level);
+		if (sret)
+			goto again;
+
+		btrfs_set_path_blocking(p);
+		sret = balance_level(trans, root, p, level);
+		btrfs_clear_path_blocking(p, NULL);
+
+		if (sret) {
+			ret = sret;
+			goto done;
+		}
+		b = p->nodes[level];
+		if (!b) {
+			btrfs_release_path(NULL, p);
+			goto again;
+		}
+		BUG_ON(btrfs_header_nritems(b) == 1);
+	}
+	return 0;
+
+again:
+	ret = -EAGAIN;
+done:
+	return ret;
+}
+
 /*
  * look for key in the tree.  path is filled in with nodes along the way
  * if key is found, we return zero and you can find the item in the leaf
@@ -1464,16 +1575,11 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 		      ins_len, int cow)
 {
 	struct extent_buffer *b;
-	struct extent_buffer *tmp;
 	int slot;
 	int ret;
 	int level;
-	int should_reada = p->reada;
 	int lowest_unlock = 1;
-	int blocksize;
 	u8 lowest_level = 0;
-	u64 blocknr;
-	u64 gen;
 
 	lowest_level = p->lowest_level;
 	WARN_ON(lowest_level && ins_len > 0);
@@ -1502,7 +1608,11 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 		if (cow) {
 			int wret;
 
-			/* is a cow on this block not required */
+			/*
+			 * if we don't really need to cow this block
+			 * then we don't want to set the path blocking,
+			 * so we test it here
+			 */
 			if (btrfs_header_generation(b) == trans->transid &&
 			    btrfs_header_owner(b) == root->root_key.objectid &&
 			    !btrfs_header_flag(b, BTRFS_HEADER_FLAG_WRITTEN)) {
@@ -1557,51 +1667,15 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 			if (ret && slot > 0)
 				slot -= 1;
 			p->slots[level] = slot;
-			if ((p->search_for_split || ins_len > 0) &&
-			    btrfs_header_nritems(b) >=
-			    BTRFS_NODEPTRS_PER_BLOCK(root) - 3) {
-				int sret;
-
-				sret = reada_for_balance(root, p, level);
-				if (sret)
-					goto again;
-
-				btrfs_set_path_blocking(p);
-				sret = split_node(trans, root, p, level);
-				btrfs_clear_path_blocking(p, NULL);
-
-				BUG_ON(sret > 0);
-				if (sret) {
-					ret = sret;
-					goto done;
-				}
-				b = p->nodes[level];
-				slot = p->slots[level];
-			} else if (ins_len < 0 &&
-				   btrfs_header_nritems(b) <
-				   BTRFS_NODEPTRS_PER_BLOCK(root) / 4) {
-				int sret;
-
-				sret = reada_for_balance(root, p, level);
-				if (sret)
-					goto again;
-
-				btrfs_set_path_blocking(p);
-				sret = balance_level(trans, root, p, level);
-				btrfs_clear_path_blocking(p, NULL);
+			ret = setup_nodes_for_search(trans, root, p, b, level,
+						     ins_len);
+			if (ret == -EAGAIN)
+				goto again;
+			else if (ret)
+				goto done;
+			b = p->nodes[level];
+			slot = p->slots[level];
 
-				if (sret) {
-					ret = sret;
-					goto done;
-				}
-				b = p->nodes[level];
-				if (!b) {
-					btrfs_release_path(NULL, p);
-					goto again;
-				}
-				slot = p->slots[level];
-				BUG_ON(btrfs_header_nritems(b) == 1);
-			}
 			unlock_up(p, level, lowest_unlock);
 
 			/* this is only true while dropping a snapshot */
@@ -1610,44 +1684,11 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 				goto done;
 			}
 
-			blocknr = btrfs_node_blockptr(b, slot);
-			gen = btrfs_node_ptr_generation(b, slot);
-			blocksize = btrfs_level_size(root, level - 1);
+			ret = read_block_for_search(trans, root, p,
+						    &b, level, slot, key);
+			if (ret == -EAGAIN)
+				goto again;
 
-			tmp = btrfs_find_tree_block(root, blocknr, blocksize);
-			if (tmp && btrfs_buffer_uptodate(tmp, gen)) {
-				b = tmp;
-			} else {
-				/*
-				 * reduce lock contention at high levels
-				 * of the btree by dropping locks before
-				 * we read.
-				 */
-				if (level > 0) {
-					btrfs_release_path(NULL, p);
-					if (tmp)
-						free_extent_buffer(tmp);
-					if (should_reada)
-						reada_for_search(root, p,
-								 level, slot,
-								 key->objectid);
-
-					tmp = read_tree_block(root, blocknr,
-							 blocksize, gen);
-					if (tmp)
-						free_extent_buffer(tmp);
-					goto again;
-				} else {
-					btrfs_set_path_blocking(p);
-					if (tmp)
-						free_extent_buffer(tmp);
-					if (should_reada)
-						reada_for_search(root, p,
-								 level, slot,
-								 key->objectid);
-					b = read_node_slot(root, b, slot);
-				}
-			}
 			if (!p->skip_locking) {
 				int lret;
 

commit a4b6e07d1a8a9b907e82b9acbf51a026fbb9301c
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Mar 16 10:59:57 2009 -0400

    Btrfs: limit balancing work while flushing delayed refs
    
    The delayed reference mechanism is responsible for all updates to the
    extent allocation trees, including those updates created while processing
    the delayed references.
    
    This commit tries to limit the amount of work that gets created during
    the final run of delayed refs before a commit.  It avoids cowing new blocks
    unless it is required to finish the commit, and so it avoids new allocations
    that were not really required.
    
    The goal is to avoid infinite loops where we are always making more work
    on the final run of delayed refs.  Over the long term we'll make a
    special log for the last delayed ref updates as well.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 8686a3d2ab3a..dbb724124633 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -949,6 +949,10 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 	    BTRFS_NODEPTRS_PER_BLOCK(root) / 4)
 		return 0;
 
+	if (trans->transaction->delayed_refs.flushing &&
+	    btrfs_header_nritems(mid) > 2)
+		return 0;
+
 	if (btrfs_header_nritems(mid) < 2)
 		err_on_enospc = 1;
 
@@ -2159,7 +2163,7 @@ static noinline int split_node(struct btrfs_trans_handle *trans,
 		ret = insert_new_root(trans, root, path, level + 1);
 		if (ret)
 			return ret;
-	} else {
+	} else if (!trans->transaction->delayed_refs.flushing) {
 		ret = push_nodes_for_insert(trans, root, path, level);
 		c = path->nodes[level];
 		if (!ret && btrfs_header_nritems(c) <
@@ -2848,7 +2852,8 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 	int num_doubles = 0;
 
 	/* first try to make some room by pushing left and right */
-	if (data_size && ins_key->type != BTRFS_DIR_ITEM_KEY) {
+	if (data_size && ins_key->type != BTRFS_DIR_ITEM_KEY &&
+	    !trans->transaction->delayed_refs.flushing) {
 		wret = push_leaf_right(trans, root, path, data_size, 0);
 		if (wret < 0)
 			return wret;
@@ -3786,7 +3791,8 @@ int btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 		}
 
 		/* delete the leaf if it is mostly empty */
-		if (used < BTRFS_LEAF_DATA_SIZE(root) / 4) {
+		if (used < BTRFS_LEAF_DATA_SIZE(root) / 4 &&
+		    !trans->transaction->delayed_refs.flushing) {
 			/* push_leaf_left fixes the path.
 			 * make sure the path still points to our leaf
 			 * for possible call to del_ptr below

commit b9473439d3e84d9fc1a0a83faca69cc1b7566341
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Mar 13 11:00:37 2009 -0400

    Btrfs: leave btree locks spinning more often
    
    btrfs_mark_buffer dirty would set dirty bits in the extent_io tree
    for the buffers it was dirtying.  This may require a kmalloc and it
    was not atomic.  So, anyone who called btrfs_mark_buffer_dirty had to
    set any btree locks they were holding to blocking first.
    
    This commit changes dirty tracking for extent buffers to just use a flag
    in the extent buffer.  Now that we have one and only one extent buffer
    per page, this can be safely done without losing dirty bits along the way.
    
    This also introduces a path->leave_spinning flag that callers of
    btrfs_search_slot can use to indicate they will properly deal with a
    path returned where all the locks are spinning instead of blocking.
    
    Many of the btree search callers now expect spinning paths,
    resulting in better btree concurrency overall.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 3764248bdc05..8686a3d2ab3a 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1684,7 +1684,8 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 	 * we don't really know what they plan on doing with the path
 	 * from here on, so for now just mark it as blocking
 	 */
-	btrfs_set_path_blocking(p);
+	if (!p->leave_spinning)
+		btrfs_set_path_blocking(p);
 	return ret;
 }
 
@@ -3032,26 +3033,27 @@ int btrfs_split_item(struct btrfs_trans_handle *trans,
 		return -EAGAIN;
 	}
 
+	btrfs_set_path_blocking(path);
 	ret = split_leaf(trans, root, &orig_key, path,
 			 sizeof(struct btrfs_item), 1);
 	path->keep_locks = 0;
 	BUG_ON(ret);
 
+	btrfs_unlock_up_safe(path, 1);
+	leaf = path->nodes[0];
+	BUG_ON(btrfs_leaf_free_space(root, leaf) < sizeof(struct btrfs_item));
+
+split:
 	/*
 	 * make sure any changes to the path from split_leaf leave it
 	 * in a blocking state
 	 */
 	btrfs_set_path_blocking(path);
 
-	leaf = path->nodes[0];
-	BUG_ON(btrfs_leaf_free_space(root, leaf) < sizeof(struct btrfs_item));
-
-split:
 	item = btrfs_item_nr(leaf, path->slots[0]);
 	orig_offset = btrfs_item_offset(leaf, item);
 	item_size = btrfs_item_size(leaf, item);
 
-
 	buf = kmalloc(item_size, GFP_NOFS);
 	read_extent_buffer(leaf, buf, btrfs_item_ptr_offset(leaf,
 			    path->slots[0]), item_size);
@@ -3545,7 +3547,6 @@ setup_items_for_insert(struct btrfs_trans_handle *trans,
 	}
 
 	btrfs_set_header_nritems(leaf, nritems + nr);
-	btrfs_mark_buffer_dirty(leaf);
 
 	ret = 0;
 	if (slot == 0) {
@@ -3553,6 +3554,8 @@ setup_items_for_insert(struct btrfs_trans_handle *trans,
 		btrfs_cpu_key_to_disk(&disk_key, cpu_key);
 		ret = fixup_low_keys(trans, root, path, &disk_key, 1);
 	}
+	btrfs_unlock_up_safe(path, 1);
+	btrfs_mark_buffer_dirty(leaf);
 
 	if (btrfs_leaf_free_space(root, leaf) < 0) {
 		btrfs_print_leaf(root, leaf);
@@ -3596,7 +3599,6 @@ int btrfs_insert_empty_items(struct btrfs_trans_handle *trans,
 			       total_data, total_size, nr);
 
 out:
-	btrfs_unlock_up_safe(path, 1);
 	return ret;
 }
 
@@ -3792,6 +3794,7 @@ int btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 			slot = path->slots[1];
 			extent_buffer_get(leaf);
 
+			btrfs_set_path_blocking(path);
 			wret = push_leaf_left(trans, root, path, 1, 1);
 			if (wret < 0 && wret != -ENOSPC)
 				ret = wret;

commit 44871b1b24b593996db43495cf4484cc580bdc10
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Mar 13 10:04:31 2009 -0400

    Btrfs: reduce stack usage in some crucial tree balancing functions
    
    Many of the tree balancing functions follow the same pattern.
    
    1) cow a block
    2) do something to the result
    
    This commit breaks them up into two functions so the variables and
    code required for part two don't suck down stack during part one.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index bebc9fd16665..3764248bdc05 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2266,66 +2266,27 @@ noinline int btrfs_leaf_free_space(struct btrfs_root *root,
 	return ret;
 }
 
-/*
- * push some data in the path leaf to the right, trying to free up at
- * least data_size bytes.  returns zero if the push worked, nonzero otherwise
- *
- * returns 1 if the push failed because the other node didn't have enough
- * room, 0 if everything worked out and < 0 if there were major errors.
- */
-static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
-			   *root, struct btrfs_path *path, int data_size,
-			   int empty)
+static noinline int __push_leaf_right(struct btrfs_trans_handle *trans,
+				      struct btrfs_root *root,
+				      struct btrfs_path *path,
+				      int data_size, int empty,
+				      struct extent_buffer *right,
+				      int free_space, u32 left_nritems)
 {
 	struct extent_buffer *left = path->nodes[0];
-	struct extent_buffer *right;
-	struct extent_buffer *upper;
+	struct extent_buffer *upper = path->nodes[1];
 	struct btrfs_disk_key disk_key;
 	int slot;
 	u32 i;
-	int free_space;
 	int push_space = 0;
 	int push_items = 0;
 	struct btrfs_item *item;
-	u32 left_nritems;
 	u32 nr;
 	u32 right_nritems;
 	u32 data_end;
 	u32 this_item_size;
 	int ret;
 
-	slot = path->slots[1];
-	if (!path->nodes[1])
-		return 1;
-
-	upper = path->nodes[1];
-	if (slot >= btrfs_header_nritems(upper) - 1)
-		return 1;
-
-	btrfs_assert_tree_locked(path->nodes[1]);
-
-	right = read_node_slot(root, upper, slot + 1);
-	btrfs_tree_lock(right);
-	btrfs_set_lock_blocking(right);
-
-	free_space = btrfs_leaf_free_space(root, right);
-	if (free_space < data_size)
-		goto out_unlock;
-
-	/* cow and double check */
-	ret = btrfs_cow_block(trans, root, right, upper,
-			      slot + 1, &right);
-	if (ret)
-		goto out_unlock;
-
-	free_space = btrfs_leaf_free_space(root, right);
-	if (free_space < data_size)
-		goto out_unlock;
-
-	left_nritems = btrfs_header_nritems(left);
-	if (left_nritems == 0)
-		goto out_unlock;
-
 	if (empty)
 		nr = 0;
 	else
@@ -2334,6 +2295,7 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 	if (path->slots[0] >= left_nritems)
 		push_space += data_size;
 
+	slot = path->slots[1];
 	i = left_nritems - 1;
 	while (i >= nr) {
 		item = btrfs_item_nr(left, i);
@@ -2464,25 +2426,83 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 	return 1;
 }
 
+/*
+ * push some data in the path leaf to the right, trying to free up at
+ * least data_size bytes.  returns zero if the push worked, nonzero otherwise
+ *
+ * returns 1 if the push failed because the other node didn't have enough
+ * room, 0 if everything worked out and < 0 if there were major errors.
+ */
+static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
+			   *root, struct btrfs_path *path, int data_size,
+			   int empty)
+{
+	struct extent_buffer *left = path->nodes[0];
+	struct extent_buffer *right;
+	struct extent_buffer *upper;
+	int slot;
+	int free_space;
+	u32 left_nritems;
+	int ret;
+
+	if (!path->nodes[1])
+		return 1;
+
+	slot = path->slots[1];
+	upper = path->nodes[1];
+	if (slot >= btrfs_header_nritems(upper) - 1)
+		return 1;
+
+	btrfs_assert_tree_locked(path->nodes[1]);
+
+	right = read_node_slot(root, upper, slot + 1);
+	btrfs_tree_lock(right);
+	btrfs_set_lock_blocking(right);
+
+	free_space = btrfs_leaf_free_space(root, right);
+	if (free_space < data_size)
+		goto out_unlock;
+
+	/* cow and double check */
+	ret = btrfs_cow_block(trans, root, right, upper,
+			      slot + 1, &right);
+	if (ret)
+		goto out_unlock;
+
+	free_space = btrfs_leaf_free_space(root, right);
+	if (free_space < data_size)
+		goto out_unlock;
+
+	left_nritems = btrfs_header_nritems(left);
+	if (left_nritems == 0)
+		goto out_unlock;
+
+	return __push_leaf_right(trans, root, path, data_size, empty,
+				right, free_space, left_nritems);
+out_unlock:
+	btrfs_tree_unlock(right);
+	free_extent_buffer(right);
+	return 1;
+}
+
 /*
  * push some data in the path leaf to the left, trying to free up at
  * least data_size bytes.  returns zero if the push worked, nonzero otherwise
  */
-static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
-			  *root, struct btrfs_path *path, int data_size,
-			  int empty)
+static noinline int __push_leaf_left(struct btrfs_trans_handle *trans,
+				     struct btrfs_root *root,
+				     struct btrfs_path *path, int data_size,
+				     int empty, struct extent_buffer *left,
+				     int free_space, int right_nritems)
 {
 	struct btrfs_disk_key disk_key;
 	struct extent_buffer *right = path->nodes[0];
-	struct extent_buffer *left;
 	int slot;
 	int i;
-	int free_space;
 	int push_space = 0;
 	int push_items = 0;
 	struct btrfs_item *item;
 	u32 old_left_nritems;
-	u32 right_nritems;
 	u32 nr;
 	int ret = 0;
 	int wret;
@@ -2490,41 +2510,6 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 	u32 old_left_item_size;
 
 	slot = path->slots[1];
-	if (slot == 0)
-		return 1;
-	if (!path->nodes[1])
-		return 1;
-
-	right_nritems = btrfs_header_nritems(right);
-	if (right_nritems == 0)
-		return 1;
-
-	btrfs_assert_tree_locked(path->nodes[1]);
-
-	left = read_node_slot(root, path->nodes[1], slot - 1);
-	btrfs_tree_lock(left);
-	btrfs_set_lock_blocking(left);
-
-	free_space = btrfs_leaf_free_space(root, left);
-	if (free_space < data_size) {
-		ret = 1;
-		goto out;
-	}
-
-	/* cow and double check */
-	ret = btrfs_cow_block(trans, root, left,
-			      path->nodes[1], slot - 1, &left);
-	if (ret) {
-		/* we hit -ENOSPC, but it isn't fatal here */
-		ret = 1;
-		goto out;
-	}
-
-	free_space = btrfs_leaf_free_space(root, left);
-	if (free_space < data_size) {
-		ret = 1;
-		goto out;
-	}
 
 	if (empty)
 		nr = right_nritems;
@@ -2691,6 +2676,154 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 	return ret;
 }
 
+/*
+ * push some data in the path leaf to the left, trying to free up at
+ * least data_size bytes.  returns zero if the push worked, nonzero otherwise
+ */
+static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
+			  *root, struct btrfs_path *path, int data_size,
+			  int empty)
+{
+	struct extent_buffer *right = path->nodes[0];
+	struct extent_buffer *left;
+	int slot;
+	int free_space;
+	u32 right_nritems;
+	int ret = 0;
+
+	slot = path->slots[1];
+	if (slot == 0)
+		return 1;
+	if (!path->nodes[1])
+		return 1;
+
+	right_nritems = btrfs_header_nritems(right);
+	if (right_nritems == 0)
+		return 1;
+
+	btrfs_assert_tree_locked(path->nodes[1]);
+
+	left = read_node_slot(root, path->nodes[1], slot - 1);
+	btrfs_tree_lock(left);
+	btrfs_set_lock_blocking(left);
+
+	free_space = btrfs_leaf_free_space(root, left);
+	if (free_space < data_size) {
+		ret = 1;
+		goto out;
+	}
+
+	/* cow and double check */
+	ret = btrfs_cow_block(trans, root, left,
+			      path->nodes[1], slot - 1, &left);
+	if (ret) {
+		/* we hit -ENOSPC, but it isn't fatal here */
+		ret = 1;
+		goto out;
+	}
+
+	free_space = btrfs_leaf_free_space(root, left);
+	if (free_space < data_size) {
+		ret = 1;
+		goto out;
+	}
+
+	return __push_leaf_left(trans, root, path, data_size,
+			       empty, left, free_space, right_nritems);
+out:
+	btrfs_tree_unlock(left);
+	free_extent_buffer(left);
+	return ret;
+}
+
+/*
+ * split the path's leaf in two, making sure there is at least data_size
+ * available for the resulting leaf level of the path.
+ *
+ * returns 0 if all went well and < 0 on failure.
+ */
+static noinline int copy_for_split(struct btrfs_trans_handle *trans,
+			       struct btrfs_root *root,
+			       struct btrfs_path *path,
+			       struct extent_buffer *l,
+			       struct extent_buffer *right,
+			       int slot, int mid, int nritems)
+{
+	int data_copy_size;
+	int rt_data_off;
+	int i;
+	int ret = 0;
+	int wret;
+	struct btrfs_disk_key disk_key;
+
+	nritems = nritems - mid;
+	btrfs_set_header_nritems(right, nritems);
+	data_copy_size = btrfs_item_end_nr(l, mid) - leaf_data_end(root, l);
+
+	copy_extent_buffer(right, l, btrfs_item_nr_offset(0),
+			   btrfs_item_nr_offset(mid),
+			   nritems * sizeof(struct btrfs_item));
+
+	copy_extent_buffer(right, l,
+		     btrfs_leaf_data(right) + BTRFS_LEAF_DATA_SIZE(root) -
+		     data_copy_size, btrfs_leaf_data(l) +
+		     leaf_data_end(root, l), data_copy_size);
+
+	rt_data_off = BTRFS_LEAF_DATA_SIZE(root) -
+		      btrfs_item_end_nr(l, mid);
+
+	for (i = 0; i < nritems; i++) {
+		struct btrfs_item *item = btrfs_item_nr(right, i);
+		u32 ioff;
+
+		if (!right->map_token) {
+			map_extent_buffer(right, (unsigned long)item,
+					sizeof(struct btrfs_item),
+					&right->map_token, &right->kaddr,
+					&right->map_start, &right->map_len,
+					KM_USER1);
+		}
+
+		ioff = btrfs_item_offset(right, item);
+		btrfs_set_item_offset(right, item, ioff + rt_data_off);
+	}
+
+	if (right->map_token) {
+		unmap_extent_buffer(right, right->map_token, KM_USER1);
+		right->map_token = NULL;
+	}
+
+	btrfs_set_header_nritems(l, mid);
+	ret = 0;
+	btrfs_item_key(right, &disk_key, 0);
+	wret = insert_ptr(trans, root, path, &disk_key, right->start,
+			  path->slots[1] + 1, 1);
+	if (wret)
+		ret = wret;
+
+	btrfs_mark_buffer_dirty(right);
+	btrfs_mark_buffer_dirty(l);
+	BUG_ON(path->slots[0] != slot);
+
+	ret = btrfs_update_ref(trans, root, l, right, 0, nritems);
+	BUG_ON(ret);
+
+	if (mid <= slot) {
+		btrfs_tree_unlock(path->nodes[0]);
+		free_extent_buffer(path->nodes[0]);
+		path->nodes[0] = right;
+		path->slots[0] -= mid;
+		path->slots[1] += 1;
+	} else {
+		btrfs_tree_unlock(right);
+		free_extent_buffer(right);
+	}
+
+	BUG_ON(path->slots[0] < 0);
+
+	return ret;
+}
+
 /*
  * split the path's leaf in two, making sure there is at least data_size
  * available for the resulting leaf level of the path.
@@ -2708,14 +2841,10 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 	int mid;
 	int slot;
 	struct extent_buffer *right;
-	int data_copy_size;
-	int rt_data_off;
-	int i;
 	int ret = 0;
 	int wret;
 	int double_split;
 	int num_doubles = 0;
-	struct btrfs_disk_key disk_key;
 
 	/* first try to make some room by pushing left and right */
 	if (data_size && ins_key->type != BTRFS_DIR_ITEM_KEY) {
@@ -2767,11 +2896,14 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 	write_extent_buffer(right, root->fs_info->chunk_tree_uuid,
 			    (unsigned long)btrfs_header_chunk_tree_uuid(right),
 			    BTRFS_UUID_SIZE);
+
 	if (mid <= slot) {
 		if (nritems == 1 ||
 		    leaf_space_used(l, mid, nritems - mid) + data_size >
 			BTRFS_LEAF_DATA_SIZE(root)) {
 			if (slot >= nritems) {
+				struct btrfs_disk_key disk_key;
+
 				btrfs_cpu_key_to_disk(&disk_key, ins_key);
 				btrfs_set_header_nritems(right, 0);
 				wret = insert_ptr(trans, root, path,
@@ -2799,6 +2931,8 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 		if (leaf_space_used(l, 0, mid) + data_size >
 			BTRFS_LEAF_DATA_SIZE(root)) {
 			if (!extend && data_size && slot == 0) {
+				struct btrfs_disk_key disk_key;
+
 				btrfs_cpu_key_to_disk(&disk_key, ins_key);
 				btrfs_set_header_nritems(right, 0);
 				wret = insert_ptr(trans, root, path,
@@ -2831,76 +2965,16 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 			}
 		}
 	}
-	nritems = nritems - mid;
-	btrfs_set_header_nritems(right, nritems);
-	data_copy_size = btrfs_item_end_nr(l, mid) - leaf_data_end(root, l);
-
-	copy_extent_buffer(right, l, btrfs_item_nr_offset(0),
-			   btrfs_item_nr_offset(mid),
-			   nritems * sizeof(struct btrfs_item));
-
-	copy_extent_buffer(right, l,
-		     btrfs_leaf_data(right) + BTRFS_LEAF_DATA_SIZE(root) -
-		     data_copy_size, btrfs_leaf_data(l) +
-		     leaf_data_end(root, l), data_copy_size);
-
-	rt_data_off = BTRFS_LEAF_DATA_SIZE(root) -
-		      btrfs_item_end_nr(l, mid);
 
-	for (i = 0; i < nritems; i++) {
-		struct btrfs_item *item = btrfs_item_nr(right, i);
-		u32 ioff;
-
-		if (!right->map_token) {
-			map_extent_buffer(right, (unsigned long)item,
-					sizeof(struct btrfs_item),
-					&right->map_token, &right->kaddr,
-					&right->map_start, &right->map_len,
-					KM_USER1);
-		}
-
-		ioff = btrfs_item_offset(right, item);
-		btrfs_set_item_offset(right, item, ioff + rt_data_off);
-	}
-
-	if (right->map_token) {
-		unmap_extent_buffer(right, right->map_token, KM_USER1);
-		right->map_token = NULL;
-	}
-
-	btrfs_set_header_nritems(l, mid);
-	ret = 0;
-	btrfs_item_key(right, &disk_key, 0);
-	wret = insert_ptr(trans, root, path, &disk_key, right->start,
-			  path->slots[1] + 1, 1);
-	if (wret)
-		ret = wret;
-
-	btrfs_mark_buffer_dirty(right);
-	btrfs_mark_buffer_dirty(l);
-	BUG_ON(path->slots[0] != slot);
-
-	ret = btrfs_update_ref(trans, root, l, right, 0, nritems);
+	ret = copy_for_split(trans, root, path, l, right, slot, mid, nritems);
 	BUG_ON(ret);
 
-	if (mid <= slot) {
-		btrfs_tree_unlock(path->nodes[0]);
-		free_extent_buffer(path->nodes[0]);
-		path->nodes[0] = right;
-		path->slots[0] -= mid;
-		path->slots[1] += 1;
-	} else {
-		btrfs_tree_unlock(right);
-		free_extent_buffer(right);
-	}
-
-	BUG_ON(path->slots[0] < 0);
-
 	if (double_split) {
 		BUG_ON(num_doubles != 0);
 		num_doubles++;
 		goto again;
 	}
+
 	return ret;
 }
 
@@ -3382,39 +3456,27 @@ int btrfs_insert_some_items(struct btrfs_trans_handle *trans,
 }
 
 /*
- * Given a key and some data, insert items into the tree.
- * This does all the path init required, making room in the tree if needed.
+ * this is a helper for btrfs_insert_empty_items, the main goal here is
+ * to save stack depth by doing the bulk of the work in a function
+ * that doesn't call btrfs_search_slot
  */
-int btrfs_insert_empty_items(struct btrfs_trans_handle *trans,
-			    struct btrfs_root *root,
-			    struct btrfs_path *path,
-			    struct btrfs_key *cpu_key, u32 *data_size,
-			    int nr)
+static noinline_for_stack int
+setup_items_for_insert(struct btrfs_trans_handle *trans,
+		      struct btrfs_root *root, struct btrfs_path *path,
+		      struct btrfs_key *cpu_key, u32 *data_size,
+		      u32 total_data, u32 total_size, int nr)
 {
-	struct extent_buffer *leaf;
 	struct btrfs_item *item;
-	int ret = 0;
-	int slot;
-	int slot_orig;
 	int i;
 	u32 nritems;
-	u32 total_size = 0;
-	u32 total_data = 0;
 	unsigned int data_end;
 	struct btrfs_disk_key disk_key;
+	int ret;
+	struct extent_buffer *leaf;
+	int slot;
 
-	for (i = 0; i < nr; i++)
-		total_data += data_size[i];
-
-	total_size = total_data + (nr * sizeof(struct btrfs_item));
-	ret = btrfs_search_slot(trans, root, cpu_key, path, total_size, 1);
-	if (ret == 0)
-		return -EEXIST;
-	if (ret < 0)
-		goto out;
-
-	slot_orig = path->slots[0];
 	leaf = path->nodes[0];
+	slot = path->slots[0];
 
 	nritems = btrfs_header_nritems(leaf);
 	data_end = leaf_data_end(root, leaf);
@@ -3426,9 +3488,6 @@ int btrfs_insert_empty_items(struct btrfs_trans_handle *trans,
 		BUG();
 	}
 
-	slot = path->slots[0];
-	BUG_ON(slot < 0);
-
 	if (slot != nritems) {
 		unsigned int old_data = btrfs_item_end_nr(leaf, slot);
 
@@ -3484,11 +3543,13 @@ int btrfs_insert_empty_items(struct btrfs_trans_handle *trans,
 		data_end -= data_size[i];
 		btrfs_set_item_size(leaf, item, data_size[i]);
 	}
+
 	btrfs_set_header_nritems(leaf, nritems + nr);
 	btrfs_mark_buffer_dirty(leaf);
 
 	ret = 0;
 	if (slot == 0) {
+		struct btrfs_disk_key disk_key;
 		btrfs_cpu_key_to_disk(&disk_key, cpu_key);
 		ret = fixup_low_keys(trans, root, path, &disk_key, 1);
 	}
@@ -3497,6 +3558,43 @@ int btrfs_insert_empty_items(struct btrfs_trans_handle *trans,
 		btrfs_print_leaf(root, leaf);
 		BUG();
 	}
+	return ret;
+}
+
+/*
+ * Given a key and some data, insert items into the tree.
+ * This does all the path init required, making room in the tree if needed.
+ */
+int btrfs_insert_empty_items(struct btrfs_trans_handle *trans,
+			    struct btrfs_root *root,
+			    struct btrfs_path *path,
+			    struct btrfs_key *cpu_key, u32 *data_size,
+			    int nr)
+{
+	struct extent_buffer *leaf;
+	int ret = 0;
+	int slot;
+	int i;
+	u32 total_size = 0;
+	u32 total_data = 0;
+
+	for (i = 0; i < nr; i++)
+		total_data += data_size[i];
+
+	total_size = total_data + (nr * sizeof(struct btrfs_item));
+	ret = btrfs_search_slot(trans, root, cpu_key, path, total_size, 1);
+	if (ret == 0)
+		return -EEXIST;
+	if (ret < 0)
+		goto out;
+
+	leaf = path->nodes[0];
+	slot = path->slots[0];
+	BUG_ON(slot < 0);
+
+	ret = setup_items_for_insert(trans, root, path, cpu_key, data_size,
+			       total_data, total_size, nr);
+
 out:
 	btrfs_unlock_up_safe(path, 1);
 	return ret;

commit 56bec294dea971335d4466b30f2d959f28f6e36d
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Mar 13 10:10:06 2009 -0400

    Btrfs: do extent allocation and reference count updates in the background
    
    The extent allocation tree maintains a reference count and full
    back reference information for every extent allocated in the
    filesystem.  For subvolume and snapshot trees, every time
    a block goes through COW, the new copy of the block adds a reference
    on every block it points to.
    
    If a btree node points to 150 leaves, then the COW code needs to go
    and add backrefs on 150 different extents, which might be spread all
    over the extent allocation tree.
    
    These updates currently happen during btrfs_cow_block, and most COWs
    happen during btrfs_search_slot.  btrfs_search_slot has locks held
    on both the parent and the node we are COWing, and so we really want
    to avoid IO during the COW if we can.
    
    This commit adds an rbtree of pending reference count updates and extent
    allocations.  The tree is ordered by byte number of the extent and byte number
    of the parent for the back reference.  The tree allows us to:
    
    1) Modify back references in something close to disk order, reducing seeks
    2) Significantly reduce the number of modifications made as block pointers
    are balanced around
    3) Do all of the extent insertion and back reference modifications outside
    of the performance critical btrfs_search_slot code.
    
    #3 has the added benefit of greatly reducing the btrfs stack footprint.
    The extent allocation tree modifications are done without the deep
    (and somewhat recursive) call chains used in the past.
    
    These delayed back reference updates must be done before the transaction
    commits, and so the rbtree is tied to the transaction.  Throttling is
    implemented to help keep the queue of backrefs at a reasonable size.
    
    Since there was a similar mechanism in place for the extent tree
    extents, that is removed and replaced by the delayed reference tree.
    
    Yan Zheng <yan.zheng@oracle.com> helped review and fixup this code.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 87c90387283b..bebc9fd16665 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -922,6 +922,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		spin_unlock(&root->node_lock);
 
 		ret = btrfs_update_extent_ref(trans, root, child->start,
+					      child->len,
 					      mid->start, child->start,
 					      root->root_key.objectid,
 					      trans->transid, level - 1);
@@ -2075,7 +2076,7 @@ static noinline int insert_new_root(struct btrfs_trans_handle *trans,
 	spin_unlock(&root->node_lock);
 
 	ret = btrfs_update_extent_ref(trans, root, lower->start,
-				      lower->start, c->start,
+				      lower->len, lower->start, c->start,
 				      root->root_key.objectid,
 				      trans->transid, level - 1);
 	BUG_ON(ret);

commit 9fa8cfe706f9c20067c042a064999d5825a35330
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Mar 13 10:24:59 2009 -0400

    Btrfs: don't preallocate metadata blocks during btrfs_search_slot
    
    In order to avoid doing expensive extent management with tree locks held,
    btrfs_search_slot will preallocate tree blocks for use by COW without
    any tree locks held.
    
    A later commit moves all of the extent allocation work for COW into
    a delayed update mechanism, and this preallocation will no longer be
    required.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 37f31b5529aa..87c90387283b 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -254,18 +254,13 @@ int btrfs_copy_root(struct btrfs_trans_handle *trans,
  * empty_size -- a hint that you plan on doing more cow.  This is the size in
  * bytes the allocator should try to find free next to the block it returns.
  * This is just a hint and may be ignored by the allocator.
- *
- * prealloc_dest -- if you have already reserved a destination for the cow,
- * this uses that block instead of allocating a new one.
- * btrfs_alloc_reserved_extent is used to finish the allocation.
  */
 static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 			     struct btrfs_root *root,
 			     struct extent_buffer *buf,
 			     struct extent_buffer *parent, int parent_slot,
 			     struct extent_buffer **cow_ret,
-			     u64 search_start, u64 empty_size,
-			     u64 prealloc_dest)
+			     u64 search_start, u64 empty_size)
 {
 	u64 parent_start;
 	struct extent_buffer *cow;
@@ -291,26 +286,10 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 	level = btrfs_header_level(buf);
 	nritems = btrfs_header_nritems(buf);
 
-	if (prealloc_dest) {
-		struct btrfs_key ins;
-
-		ins.objectid = prealloc_dest;
-		ins.offset = buf->len;
-		ins.type = BTRFS_EXTENT_ITEM_KEY;
-
-		ret = btrfs_alloc_reserved_extent(trans, root, parent_start,
-						  root->root_key.objectid,
-						  trans->transid, level, &ins);
-		BUG_ON(ret);
-		cow = btrfs_init_new_buffer(trans, root, prealloc_dest,
-					    buf->len, level);
-	} else {
-		cow = btrfs_alloc_free_block(trans, root, buf->len,
-					     parent_start,
-					     root->root_key.objectid,
-					     trans->transid, level,
-					     search_start, empty_size);
-	}
+	cow = btrfs_alloc_free_block(trans, root, buf->len,
+				     parent_start, root->root_key.objectid,
+				     trans->transid, level,
+				     search_start, empty_size);
 	if (IS_ERR(cow))
 		return PTR_ERR(cow);
 
@@ -413,7 +392,7 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 noinline int btrfs_cow_block(struct btrfs_trans_handle *trans,
 		    struct btrfs_root *root, struct extent_buffer *buf,
 		    struct extent_buffer *parent, int parent_slot,
-		    struct extent_buffer **cow_ret, u64 prealloc_dest)
+		    struct extent_buffer **cow_ret)
 {
 	u64 search_start;
 	int ret;
@@ -436,7 +415,6 @@ noinline int btrfs_cow_block(struct btrfs_trans_handle *trans,
 	    btrfs_header_owner(buf) == root->root_key.objectid &&
 	    !btrfs_header_flag(buf, BTRFS_HEADER_FLAG_WRITTEN)) {
 		*cow_ret = buf;
-		WARN_ON(prealloc_dest);
 		return 0;
 	}
 
@@ -447,8 +425,7 @@ noinline int btrfs_cow_block(struct btrfs_trans_handle *trans,
 	btrfs_set_lock_blocking(buf);
 
 	ret = __btrfs_cow_block(trans, root, buf, parent,
-				 parent_slot, cow_ret, search_start, 0,
-				 prealloc_dest);
+				 parent_slot, cow_ret, search_start, 0);
 	return ret;
 }
 
@@ -617,7 +594,7 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 		err = __btrfs_cow_block(trans, root, cur, parent, i,
 					&cur, search_start,
 					min(16 * blocksize,
-					    (end_slot - i) * blocksize), 0);
+					    (end_slot - i) * blocksize));
 		if (err) {
 			btrfs_tree_unlock(cur);
 			free_extent_buffer(cur);
@@ -937,7 +914,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		BUG_ON(!child);
 		btrfs_tree_lock(child);
 		btrfs_set_lock_blocking(child);
-		ret = btrfs_cow_block(trans, root, child, mid, 0, &child, 0);
+		ret = btrfs_cow_block(trans, root, child, mid, 0, &child);
 		BUG_ON(ret);
 
 		spin_lock(&root->node_lock);
@@ -979,7 +956,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		btrfs_tree_lock(left);
 		btrfs_set_lock_blocking(left);
 		wret = btrfs_cow_block(trans, root, left,
-				       parent, pslot - 1, &left, 0);
+				       parent, pslot - 1, &left);
 		if (wret) {
 			ret = wret;
 			goto enospc;
@@ -990,7 +967,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		btrfs_tree_lock(right);
 		btrfs_set_lock_blocking(right);
 		wret = btrfs_cow_block(trans, root, right,
-				       parent, pslot + 1, &right, 0);
+				       parent, pslot + 1, &right);
 		if (wret) {
 			ret = wret;
 			goto enospc;
@@ -1171,7 +1148,7 @@ static noinline int push_nodes_for_insert(struct btrfs_trans_handle *trans,
 			wret = 1;
 		} else {
 			ret = btrfs_cow_block(trans, root, left, parent,
-					      pslot - 1, &left, 0);
+					      pslot - 1, &left);
 			if (ret)
 				wret = 1;
 			else {
@@ -1222,7 +1199,7 @@ static noinline int push_nodes_for_insert(struct btrfs_trans_handle *trans,
 		} else {
 			ret = btrfs_cow_block(trans, root, right,
 					      parent, pslot + 1,
-					      &right, 0);
+					      &right);
 			if (ret)
 				wret = 1;
 			else {
@@ -1492,7 +1469,6 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 	u8 lowest_level = 0;
 	u64 blocknr;
 	u64 gen;
-	struct btrfs_key prealloc_block;
 
 	lowest_level = p->lowest_level;
 	WARN_ON(lowest_level && ins_len > 0);
@@ -1501,8 +1477,6 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 	if (ins_len < 0)
 		lowest_unlock = 2;
 
-	prealloc_block.objectid = 0;
-
 again:
 	if (p->skip_locking)
 		b = btrfs_root_node(root);
@@ -1529,44 +1503,11 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 			    !btrfs_header_flag(b, BTRFS_HEADER_FLAG_WRITTEN)) {
 				goto cow_done;
 			}
-
-			/* ok, we have to cow, is our old prealloc the right
-			 * size?
-			 */
-			if (prealloc_block.objectid &&
-			    prealloc_block.offset != b->len) {
-				btrfs_release_path(root, p);
-				btrfs_free_reserved_extent(root,
-					   prealloc_block.objectid,
-					   prealloc_block.offset);
-				prealloc_block.objectid = 0;
-				goto again;
-			}
-
-			/*
-			 * for higher level blocks, try not to allocate blocks
-			 * with the block and the parent locks held.
-			 */
-			if (level > 0 && !prealloc_block.objectid) {
-				u32 size = b->len;
-				u64 hint = b->start;
-
-				btrfs_release_path(root, p);
-				ret = btrfs_reserve_extent(trans, root,
-							   size, size, 0,
-							   hint, (u64)-1,
-							   &prealloc_block, 0);
-				BUG_ON(ret);
-				goto again;
-			}
-
 			btrfs_set_path_blocking(p);
 
 			wret = btrfs_cow_block(trans, root, b,
 					       p->nodes[level + 1],
-					       p->slots[level + 1],
-					       &b, prealloc_block.objectid);
-			prealloc_block.objectid = 0;
+					       p->slots[level + 1], &b);
 			if (wret) {
 				free_extent_buffer(b);
 				ret = wret;
@@ -1743,11 +1684,6 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 	 * from here on, so for now just mark it as blocking
 	 */
 	btrfs_set_path_blocking(p);
-	if (prealloc_block.objectid) {
-		btrfs_free_reserved_extent(root,
-			   prealloc_block.objectid,
-			   prealloc_block.offset);
-	}
 	return ret;
 }
 
@@ -1768,7 +1704,7 @@ int btrfs_merge_path(struct btrfs_trans_handle *trans,
 	int ret;
 
 	eb = btrfs_lock_root_node(root);
-	ret = btrfs_cow_block(trans, root, eb, NULL, 0, &eb, 0);
+	ret = btrfs_cow_block(trans, root, eb, NULL, 0, &eb);
 	BUG_ON(ret);
 
 	btrfs_set_lock_blocking(eb);
@@ -1826,7 +1762,7 @@ int btrfs_merge_path(struct btrfs_trans_handle *trans,
 			}
 
 			ret = btrfs_cow_block(trans, root, eb, parent, slot,
-					      &eb, 0);
+					      &eb);
 			BUG_ON(ret);
 
 			if (root->root_key.objectid ==
@@ -2377,7 +2313,7 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 
 	/* cow and double check */
 	ret = btrfs_cow_block(trans, root, right, upper,
-			      slot + 1, &right, 0);
+			      slot + 1, &right);
 	if (ret)
 		goto out_unlock;
 
@@ -2576,7 +2512,7 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 
 	/* cow and double check */
 	ret = btrfs_cow_block(trans, root, left,
-			      path->nodes[1], slot - 1, &left, 0);
+			      path->nodes[1], slot - 1, &left);
 	if (ret) {
 		/* we hit -ENOSPC, but it isn't fatal here */
 		ret = 1;

commit b9447ef80bd301b932ac4d85c9622e929de5fd62
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Mar 9 11:45:38 2009 -0400

    Btrfs: fix spinlock assertions on UP systems
    
    btrfs_tree_locked was being used to make sure a given extent_buffer was
    properly locked in a few places.  But, it wasn't correct for UP compiled
    kernels.
    
    This switches it to using assert_spin_locked instead, and renames it to
    btrfs_assert_tree_locked to better reflect how it was really being used.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 42491d728e99..37f31b5529aa 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -277,7 +277,7 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 	if (*cow_ret == buf)
 		unlock_orig = 1;
 
-	WARN_ON(!btrfs_tree_locked(buf));
+	btrfs_assert_tree_locked(buf);
 
 	if (parent)
 		parent_start = parent->start;
@@ -2365,7 +2365,7 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 	if (slot >= btrfs_header_nritems(upper) - 1)
 		return 1;
 
-	WARN_ON(!btrfs_tree_locked(path->nodes[1]));
+	btrfs_assert_tree_locked(path->nodes[1]);
 
 	right = read_node_slot(root, upper, slot + 1);
 	btrfs_tree_lock(right);
@@ -2562,7 +2562,7 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 	if (right_nritems == 0)
 		return 1;
 
-	WARN_ON(!btrfs_tree_locked(path->nodes[1]));
+	btrfs_assert_tree_locked(path->nodes[1]);
 
 	left = read_node_slot(root, path->nodes[1], slot - 1);
 	btrfs_tree_lock(left);
@@ -4101,7 +4101,7 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 
 		next = read_node_slot(root, c, slot);
 		if (!path->skip_locking) {
-			WARN_ON(!btrfs_tree_locked(c));
+			btrfs_assert_tree_locked(c);
 			btrfs_tree_lock(next);
 			btrfs_set_lock_blocking(next);
 		}
@@ -4126,7 +4126,7 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 			reada_for_search(root, path, level, slot, 0);
 		next = read_node_slot(root, next, 0);
 		if (!path->skip_locking) {
-			WARN_ON(!btrfs_tree_locked(path->nodes[level]));
+			btrfs_assert_tree_locked(path->nodes[level]);
 			btrfs_tree_lock(next);
 			btrfs_set_lock_blocking(next);
 		}

commit 4008c04a07c73ec3cb1be4c1391d2159a8f75d6d
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Feb 12 14:09:45 2009 -0500

    Btrfs: make a lockdep class for the extent buffer locks
    
    Btrfs is currently using spin_lock_nested with a nested value based
    on the tree depth of the block.  But, this doesn't quite work because
    the max tree depth is bigger than what spin_lock_nested can deal with,
    and because locks are sometimes taken before the level field is filled in.
    
    The solution here is to use lockdep_set_class_and_name instead, and to
    set the class before unlocking the pages when the block is read from the
    disk and just after init of a freshly allocated tree block.
    
    btrfs_clear_path_blocking is also changed to take the locks in the proper
    order, and it also makes sure all the locks currently held are properly
    set to blocking before it tries to retake the spinlocks.  Otherwise, lockdep
    gets upset about bad lock orderin.
    
    The lockdep magic cam from Peter Zijlstra <peterz@infradead.org>
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index c8f4c540cc2c..42491d728e99 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -62,14 +62,38 @@ noinline void btrfs_set_path_blocking(struct btrfs_path *p)
 
 /*
  * reset all the locked nodes in the patch to spinning locks.
+ *
+ * held is used to keep lockdep happy, when lockdep is enabled
+ * we set held to a blocking lock before we go around and
+ * retake all the spinlocks in the path.  You can safely use NULL
+ * for held
  */
-noinline void btrfs_clear_path_blocking(struct btrfs_path *p)
+noinline void btrfs_clear_path_blocking(struct btrfs_path *p,
+					struct extent_buffer *held)
 {
 	int i;
-	for (i = 0; i < BTRFS_MAX_LEVEL; i++) {
+
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+	/* lockdep really cares that we take all of these spinlocks
+	 * in the right order.  If any of the locks in the path are not
+	 * currently blocking, it is going to complain.  So, make really
+	 * really sure by forcing the path to blocking before we clear
+	 * the path blocking.
+	 */
+	if (held)
+		btrfs_set_lock_blocking(held);
+	btrfs_set_path_blocking(p);
+#endif
+
+	for (i = BTRFS_MAX_LEVEL - 1; i >= 0; i--) {
 		if (p->nodes[i] && p->locks[i])
 			btrfs_clear_lock_blocking(p->nodes[i]);
 	}
+
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+	if (held)
+		btrfs_clear_lock_blocking(held);
+#endif
 }
 
 /* this also releases the path */
@@ -279,7 +303,7 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 						  trans->transid, level, &ins);
 		BUG_ON(ret);
 		cow = btrfs_init_new_buffer(trans, root, prealloc_dest,
-					    buf->len);
+					    buf->len, level);
 	} else {
 		cow = btrfs_alloc_free_block(trans, root, buf->len,
 					     parent_start,
@@ -1559,7 +1583,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 		if (!p->skip_locking)
 			p->locks[level] = 1;
 
-		btrfs_clear_path_blocking(p);
+		btrfs_clear_path_blocking(p, NULL);
 
 		/*
 		 * we have a lock on b and as long as we aren't changing
@@ -1598,7 +1622,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 
 				btrfs_set_path_blocking(p);
 				sret = split_node(trans, root, p, level);
-				btrfs_clear_path_blocking(p);
+				btrfs_clear_path_blocking(p, NULL);
 
 				BUG_ON(sret > 0);
 				if (sret) {
@@ -1618,7 +1642,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 
 				btrfs_set_path_blocking(p);
 				sret = balance_level(trans, root, p, level);
-				btrfs_clear_path_blocking(p);
+				btrfs_clear_path_blocking(p, NULL);
 
 				if (sret) {
 					ret = sret;
@@ -1681,13 +1705,13 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 			if (!p->skip_locking) {
 				int lret;
 
-				btrfs_clear_path_blocking(p);
+				btrfs_clear_path_blocking(p, NULL);
 				lret = btrfs_try_spin_lock(b);
 
 				if (!lret) {
 					btrfs_set_path_blocking(p);
 					btrfs_tree_lock(b);
-					btrfs_clear_path_blocking(p);
+					btrfs_clear_path_blocking(p, b);
 				}
 			}
 		} else {
@@ -1699,7 +1723,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 				btrfs_set_path_blocking(p);
 				sret = split_leaf(trans, root, key,
 						      p, ins_len, ret == 0);
-				btrfs_clear_path_blocking(p);
+				btrfs_clear_path_blocking(p, NULL);
 
 				BUG_ON(sret > 0);
 				if (sret) {
@@ -3919,7 +3943,6 @@ int btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,
 				btrfs_release_path(root, path);
 				goto again;
 			} else {
-				btrfs_clear_path_blocking(path);
 				goto out;
 			}
 		}
@@ -3939,7 +3962,7 @@ int btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,
 		path->locks[level - 1] = 1;
 		path->nodes[level - 1] = cur;
 		unlock_up(path, level, 1);
-		btrfs_clear_path_blocking(path);
+		btrfs_clear_path_blocking(path, NULL);
 	}
 out:
 	if (ret == 0)

commit e00f7308658622fbd483cb0d9fe41165bf9050d0
Author: Jeff Mahoney <jeffm@suse.com>
Date:   Thu Feb 12 14:11:25 2009 -0500

    Btrfs: remove btrfs_init_path
    
    btrfs_init_path was initially used when the path objects were on the
    stack.  Now all the work is done by btrfs_alloc_path and btrfs_init_path
    isn't required.
    
    This patch removes it, and just uses kmem_cache_zalloc to zero out the object.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 6674692f7023..c8f4c540cc2c 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -38,19 +38,12 @@ static int balance_node_right(struct btrfs_trans_handle *trans,
 static int del_ptr(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 		   struct btrfs_path *path, int level, int slot);
 
-inline void btrfs_init_path(struct btrfs_path *p)
-{
-	memset(p, 0, sizeof(*p));
-}
-
 struct btrfs_path *btrfs_alloc_path(void)
 {
 	struct btrfs_path *path;
-	path = kmem_cache_alloc(btrfs_path_cachep, GFP_NOFS);
-	if (path) {
-		btrfs_init_path(path);
+	path = kmem_cache_zalloc(btrfs_path_cachep, GFP_NOFS);
+	if (path)
 		path->reada = 1;
-	}
 	return path;
 }
 

commit 7951f3cefbd711f4429a0cd014aa83a844c399a0
Author: Jeff Mahoney <jeffm@suse.com>
Date:   Thu Feb 12 10:06:15 2009 -0500

    Btrfs: balance_level checks !child after access
    
    The BUG_ON() is in the wrong spot.
    
    Signed-off-by: Jeff Mahoney <jeffm@suse.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 35443cc4b9a9..6674692f7023 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -917,9 +917,9 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 
 		/* promote the child to a root */
 		child = read_node_slot(root, mid, 0);
+		BUG_ON(!child);
 		btrfs_tree_lock(child);
 		btrfs_set_lock_blocking(child);
-		BUG_ON(!child);
 		ret = btrfs_cow_block(trans, root, child, mid, 0, &child, 0);
 		BUG_ON(ret);
 

commit 284b066af41579f62649048fdec5c5e7091703e6
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Feb 9 16:22:03 2009 -0500

    Btrfs: don't use spin_is_contended
    
    Btrfs was using spin_is_contended to see if it should drop locks before
    doing extent allocations during btrfs_search_slot.  The idea was to avoid
    expensive searches in the tree unless the lock was actually contended.
    
    But, spin_is_contended is specific to the ticket spinlocks on x86, so this
    is causing compile errors everywhere else.
    
    In practice, the contention could easily appear some time after we started
    doing the extent allocation, and it makes more sense to always drop the lock
    instead.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 551177c0011a..35443cc4b9a9 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1530,8 +1530,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 			 * for higher level blocks, try not to allocate blocks
 			 * with the block and the parent locks held.
 			 */
-			if (level > 0 && !prealloc_block.objectid &&
-			    btrfs_path_lock_waiting(p, level)) {
+			if (level > 0 && !prealloc_block.objectid) {
 				u32 size = b->len;
 				u64 hint = b->start;
 

commit 7b78c170dc4f538cc7ee66f47b3aac3f3974a36c
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Feb 4 09:12:46 2009 -0500

    Btrfs: Only prep for btree deletion balances when nodes are mostly empty
    
    Whenever an item deletion is done, we need to balance all the nodes
    in the tree to make sure we don't end up with an empty node if a pointer
    is deleted.  This balance prep happens from the root of the tree down
    so we can drop our locks as we go.
    
    reada_for_balance was triggering read-ahead on neighboring nodes even
    when no balancing was required.  This adds an extra check to avoid
    calling balance_level() and avoid reada_for_balance() when a balance
    won't be required.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 0d1e3b91e7bd..551177c0011a 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1518,18 +1518,19 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 			 */
 			if (prealloc_block.objectid &&
 			    prealloc_block.offset != b->len) {
-				btrfs_set_path_blocking(p);
+				btrfs_release_path(root, p);
 				btrfs_free_reserved_extent(root,
 					   prealloc_block.objectid,
 					   prealloc_block.offset);
 				prealloc_block.objectid = 0;
+				goto again;
 			}
 
 			/*
 			 * for higher level blocks, try not to allocate blocks
 			 * with the block and the parent locks held.
 			 */
-			if (level > 1 && !prealloc_block.objectid &&
+			if (level > 0 && !prealloc_block.objectid &&
 			    btrfs_path_lock_waiting(p, level)) {
 				u32 size = b->len;
 				u64 hint = b->start;
@@ -1614,7 +1615,9 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 				}
 				b = p->nodes[level];
 				slot = p->slots[level];
-			} else if (ins_len < 0) {
+			} else if (ins_len < 0 &&
+				   btrfs_header_nritems(b) <
+				   BTRFS_NODEPTRS_PER_BLOCK(root) / 4) {
 				int sret;
 
 				sret = reada_for_balance(root, p, level);

commit 12f4daccfc3732280debba8f9ba49720372de831
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Feb 4 09:31:42 2009 -0500

    Btrfs: fix btrfs_unlock_up_safe to walk the entire path
    
    btrfs_unlock_up_safe would break out at the first NULL node entry or
    unlocked node it found in the path.
    
    Some of the callers have missing nodes at the lower levels of the path, so this
    commit fixes things to check all the nodes in the path before returning.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index f6916ceb3920..0d1e3b91e7bd 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1439,9 +1439,9 @@ noinline void btrfs_unlock_up_safe(struct btrfs_path *path, int level)
 
 	for (i = level; i < BTRFS_MAX_LEVEL; i++) {
 		if (!path->nodes[i])
-			break;
+			continue;
 		if (!path->locks[i])
-			break;
+			continue;
 		btrfs_tree_unlock(path->nodes[i]);
 		path->locks[i] = 0;
 	}

commit 4d081c41a4f98aecb5e86ef7d3e644cc7b52131f
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Feb 4 09:31:28 2009 -0500

    Btrfs: change btrfs_del_leaf to drop locks earlier
    
    btrfs_del_leaf does two things.  First it removes the pointer in the
    parent, and then it frees the block that has the leaf.  It has the
    parent node locked for both operations.
    
    But, it only needs the parent locked while it is deleting the pointer.
    After that it can safely free the block without the parent locked.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 3af777357acb..f6916ceb3920 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -3630,15 +3630,22 @@ noinline int btrfs_del_leaf(struct btrfs_trans_handle *trans,
 {
 	int ret;
 	u64 root_gen = btrfs_header_generation(path->nodes[1]);
+	u64 parent_start = path->nodes[1]->start;
+	u64 parent_owner = btrfs_header_owner(path->nodes[1]);
 
 	ret = del_ptr(trans, root, path, 1, path->slots[1]);
 	if (ret)
 		return ret;
 
+	/*
+	 * btrfs_free_extent is expensive, we want to make sure we
+	 * aren't holding any locks when we call it
+	 */
+	btrfs_unlock_up_safe(path, 0);
+
 	ret = btrfs_free_extent(trans, root, bytenr,
 				btrfs_level_size(root, 0),
-				path->nodes[1]->start,
-				btrfs_header_owner(path->nodes[1]),
+				parent_start, parent_owner,
 				root_gen, 0, 1);
 	return ret;
 }

commit b4ce94de9b4d64e8ab3cf155d13653c666e22b9b
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Feb 4 09:25:08 2009 -0500

    Btrfs: Change btree locking to use explicit blocking points
    
    Most of the btrfs metadata operations can be protected by a spinlock,
    but some operations still need to schedule.
    
    So far, btrfs has been using a mutex along with a trylock loop,
    most of the time it is able to avoid going for the full mutex, so
    the trylock loop is a big performance gain.
    
    This commit is step one for getting rid of the blocking locks entirely.
    btrfs_tree_lock takes a spinlock, and the code explicitly switches
    to a blocking lock when it starts an operation that can schedule.
    
    We'll be able get rid of the blocking locks in smaller pieces over time.
    Tracing allows us to find the most common cause of blocking, so we
    can start with the hot spots first.
    
    The basic idea is:
    
    btrfs_tree_lock() returns with the spin lock held
    
    btrfs_set_lock_blocking() sets the EXTENT_BUFFER_BLOCKING bit in
    the extent buffer flags, and then drops the spin lock.  The buffer is
    still considered locked by all of the btrfs code.
    
    If btrfs_tree_lock gets the spinlock but finds the blocking bit set, it drops
    the spin lock and waits on a wait queue for the blocking bit to go away.
    
    Much of the code that needs to set the blocking bit finishes without actually
    blocking a good percentage of the time.  So, an adaptive spin is still
    used against the blocking bit to avoid very high context switch rates.
    
    btrfs_clear_lock_blocking() clears the blocking bit and returns
    with the spinlock held again.
    
    btrfs_tree_unlock() can be called on either blocking or spinning locks,
    it does the right thing based on the blocking bit.
    
    ctree.c has a helper function to set/clear all the locked buffers in a
    path as blocking.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 3b6e35aafc9e..3af777357acb 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -54,6 +54,31 @@ struct btrfs_path *btrfs_alloc_path(void)
 	return path;
 }
 
+/*
+ * set all locked nodes in the path to blocking locks.  This should
+ * be done before scheduling
+ */
+noinline void btrfs_set_path_blocking(struct btrfs_path *p)
+{
+	int i;
+	for (i = 0; i < BTRFS_MAX_LEVEL; i++) {
+		if (p->nodes[i] && p->locks[i])
+			btrfs_set_lock_blocking(p->nodes[i]);
+	}
+}
+
+/*
+ * reset all the locked nodes in the patch to spinning locks.
+ */
+noinline void btrfs_clear_path_blocking(struct btrfs_path *p)
+{
+	int i;
+	for (i = 0; i < BTRFS_MAX_LEVEL; i++) {
+		if (p->nodes[i] && p->locks[i])
+			btrfs_clear_lock_blocking(p->nodes[i]);
+	}
+}
+
 /* this also releases the path */
 void btrfs_free_path(struct btrfs_path *p)
 {
@@ -272,6 +297,8 @@ static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 	if (IS_ERR(cow))
 		return PTR_ERR(cow);
 
+	/* cow is set to blocking by btrfs_init_new_buffer */
+
 	copy_extent_buffer(cow, buf, 0, 0, cow->len);
 	btrfs_set_header_bytenr(cow, cow->start);
 	btrfs_set_header_generation(cow, trans->transid);
@@ -397,6 +424,11 @@ noinline int btrfs_cow_block(struct btrfs_trans_handle *trans,
 	}
 
 	search_start = buf->start & ~((u64)(1024 * 1024 * 1024) - 1);
+
+	if (parent)
+		btrfs_set_lock_blocking(parent);
+	btrfs_set_lock_blocking(buf);
+
 	ret = __btrfs_cow_block(trans, root, buf, parent,
 				 parent_slot, cow_ret, search_start, 0,
 				 prealloc_dest);
@@ -502,6 +534,8 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 	if (parent_nritems == 1)
 		return 0;
 
+	btrfs_set_lock_blocking(parent);
+
 	for (i = start_slot; i < end_slot; i++) {
 		int close = 1;
 
@@ -562,6 +596,7 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 			search_start = last_block;
 
 		btrfs_tree_lock(cur);
+		btrfs_set_lock_blocking(cur);
 		err = __btrfs_cow_block(trans, root, cur, parent, i,
 					&cur, search_start,
 					min(16 * blocksize,
@@ -860,6 +895,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		return 0;
 
 	mid = path->nodes[level];
+
 	WARN_ON(!path->locks[level]);
 	WARN_ON(btrfs_header_generation(mid) != trans->transid);
 
@@ -882,6 +918,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		/* promote the child to a root */
 		child = read_node_slot(root, mid, 0);
 		btrfs_tree_lock(child);
+		btrfs_set_lock_blocking(child);
 		BUG_ON(!child);
 		ret = btrfs_cow_block(trans, root, child, mid, 0, &child, 0);
 		BUG_ON(ret);
@@ -898,6 +935,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 
 		add_root_to_dirty_list(root);
 		btrfs_tree_unlock(child);
+
 		path->locks[level] = 0;
 		path->nodes[level] = NULL;
 		clean_tree_block(trans, root, mid);
@@ -922,6 +960,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 	left = read_node_slot(root, parent, pslot - 1);
 	if (left) {
 		btrfs_tree_lock(left);
+		btrfs_set_lock_blocking(left);
 		wret = btrfs_cow_block(trans, root, left,
 				       parent, pslot - 1, &left, 0);
 		if (wret) {
@@ -932,6 +971,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 	right = read_node_slot(root, parent, pslot + 1);
 	if (right) {
 		btrfs_tree_lock(right);
+		btrfs_set_lock_blocking(right);
 		wret = btrfs_cow_block(trans, root, right,
 				       parent, pslot + 1, &right, 0);
 		if (wret) {
@@ -1107,6 +1147,8 @@ static noinline int push_nodes_for_insert(struct btrfs_trans_handle *trans,
 		u32 left_nr;
 
 		btrfs_tree_lock(left);
+		btrfs_set_lock_blocking(left);
+
 		left_nr = btrfs_header_nritems(left);
 		if (left_nr >= BTRFS_NODEPTRS_PER_BLOCK(root) - 1) {
 			wret = 1;
@@ -1153,7 +1195,10 @@ static noinline int push_nodes_for_insert(struct btrfs_trans_handle *trans,
 	 */
 	if (right) {
 		u32 right_nr;
+
 		btrfs_tree_lock(right);
+		btrfs_set_lock_blocking(right);
+
 		right_nr = btrfs_header_nritems(right);
 		if (right_nr >= BTRFS_NODEPTRS_PER_BLOCK(root) - 1) {
 			wret = 1;
@@ -1264,6 +1309,68 @@ static noinline void reada_for_search(struct btrfs_root *root,
 	}
 }
 
+/*
+ * returns -EAGAIN if it had to drop the path, or zero if everything was in
+ * cache
+ */
+static noinline int reada_for_balance(struct btrfs_root *root,
+				      struct btrfs_path *path, int level)
+{
+	int slot;
+	int nritems;
+	struct extent_buffer *parent;
+	struct extent_buffer *eb;
+	u64 gen;
+	u64 block1 = 0;
+	u64 block2 = 0;
+	int ret = 0;
+	int blocksize;
+
+	parent = path->nodes[level - 1];
+	if (!parent)
+		return 0;
+
+	nritems = btrfs_header_nritems(parent);
+	slot = path->slots[level];
+	blocksize = btrfs_level_size(root, level);
+
+	if (slot > 0) {
+		block1 = btrfs_node_blockptr(parent, slot - 1);
+		gen = btrfs_node_ptr_generation(parent, slot - 1);
+		eb = btrfs_find_tree_block(root, block1, blocksize);
+		if (eb && btrfs_buffer_uptodate(eb, gen))
+			block1 = 0;
+		free_extent_buffer(eb);
+	}
+	if (slot < nritems) {
+		block2 = btrfs_node_blockptr(parent, slot + 1);
+		gen = btrfs_node_ptr_generation(parent, slot + 1);
+		eb = btrfs_find_tree_block(root, block2, blocksize);
+		if (eb && btrfs_buffer_uptodate(eb, gen))
+			block2 = 0;
+		free_extent_buffer(eb);
+	}
+	if (block1 || block2) {
+		ret = -EAGAIN;
+		btrfs_release_path(root, path);
+		if (block1)
+			readahead_tree_block(root, block1, blocksize, 0);
+		if (block2)
+			readahead_tree_block(root, block2, blocksize, 0);
+
+		if (block1) {
+			eb = read_tree_block(root, block1, blocksize, 0);
+			free_extent_buffer(eb);
+		}
+		if (block1) {
+			eb = read_tree_block(root, block2, blocksize, 0);
+			free_extent_buffer(eb);
+		}
+	}
+	return ret;
+}
+
+
 /*
  * when we walk down the tree, it is usually safe to unlock the higher layers
  * in the tree.  The exceptions are when our path goes through slot 0, because
@@ -1314,6 +1421,32 @@ static noinline void unlock_up(struct btrfs_path *path, int level,
 	}
 }
 
+/*
+ * This releases any locks held in the path starting at level and
+ * going all the way up to the root.
+ *
+ * btrfs_search_slot will keep the lock held on higher nodes in a few
+ * corner cases, such as COW of the block at slot zero in the node.  This
+ * ignores those rules, and it should only be called when there are no
+ * more updates to be done higher up in the tree.
+ */
+noinline void btrfs_unlock_up_safe(struct btrfs_path *path, int level)
+{
+	int i;
+
+	if (path->keep_locks || path->lowest_level)
+		return;
+
+	for (i = level; i < BTRFS_MAX_LEVEL; i++) {
+		if (!path->nodes[i])
+			break;
+		if (!path->locks[i])
+			break;
+		btrfs_tree_unlock(path->nodes[i]);
+		path->locks[i] = 0;
+	}
+}
+
 /*
  * look for key in the tree.  path is filled in with nodes along the way
  * if key is found, we return zero and you can find the item in the leaf
@@ -1385,6 +1518,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 			 */
 			if (prealloc_block.objectid &&
 			    prealloc_block.offset != b->len) {
+				btrfs_set_path_blocking(p);
 				btrfs_free_reserved_extent(root,
 					   prealloc_block.objectid,
 					   prealloc_block.offset);
@@ -1409,6 +1543,8 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 				goto again;
 			}
 
+			btrfs_set_path_blocking(p);
+
 			wret = btrfs_cow_block(trans, root, b,
 					       p->nodes[level + 1],
 					       p->slots[level + 1],
@@ -1430,6 +1566,22 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 		if (!p->skip_locking)
 			p->locks[level] = 1;
 
+		btrfs_clear_path_blocking(p);
+
+		/*
+		 * we have a lock on b and as long as we aren't changing
+		 * the tree, there is no way to for the items in b to change.
+		 * It is safe to drop the lock on our parent before we
+		 * go through the expensive btree search on b.
+		 *
+		 * If cow is true, then we might be changing slot zero,
+		 * which may require changing the parent.  So, we can't
+		 * drop the lock until after we know which slot we're
+		 * operating on.
+		 */
+		if (!cow)
+			btrfs_unlock_up_safe(p, level + 1);
+
 		ret = check_block(root, p, level);
 		if (ret) {
 			ret = -1;
@@ -1437,6 +1589,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 		}
 
 		ret = bin_search(b, key, level, &slot);
+
 		if (level != 0) {
 			if (ret && slot > 0)
 				slot -= 1;
@@ -1444,7 +1597,16 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 			if ((p->search_for_split || ins_len > 0) &&
 			    btrfs_header_nritems(b) >=
 			    BTRFS_NODEPTRS_PER_BLOCK(root) - 3) {
-				int sret = split_node(trans, root, p, level);
+				int sret;
+
+				sret = reada_for_balance(root, p, level);
+				if (sret)
+					goto again;
+
+				btrfs_set_path_blocking(p);
+				sret = split_node(trans, root, p, level);
+				btrfs_clear_path_blocking(p);
+
 				BUG_ON(sret > 0);
 				if (sret) {
 					ret = sret;
@@ -1453,8 +1615,16 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 				b = p->nodes[level];
 				slot = p->slots[level];
 			} else if (ins_len < 0) {
-				int sret = balance_level(trans, root, p,
-							 level);
+				int sret;
+
+				sret = reada_for_balance(root, p, level);
+				if (sret)
+					goto again;
+
+				btrfs_set_path_blocking(p);
+				sret = balance_level(trans, root, p, level);
+				btrfs_clear_path_blocking(p);
+
 				if (sret) {
 					ret = sret;
 					goto done;
@@ -1488,7 +1658,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 				 * of the btree by dropping locks before
 				 * we read.
 				 */
-				if (level > 1) {
+				if (level > 0) {
 					btrfs_release_path(NULL, p);
 					if (tmp)
 						free_extent_buffer(tmp);
@@ -1503,6 +1673,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 						free_extent_buffer(tmp);
 					goto again;
 				} else {
+					btrfs_set_path_blocking(p);
 					if (tmp)
 						free_extent_buffer(tmp);
 					if (should_reada)
@@ -1512,14 +1683,29 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 					b = read_node_slot(root, b, slot);
 				}
 			}
-			if (!p->skip_locking)
-				btrfs_tree_lock(b);
+			if (!p->skip_locking) {
+				int lret;
+
+				btrfs_clear_path_blocking(p);
+				lret = btrfs_try_spin_lock(b);
+
+				if (!lret) {
+					btrfs_set_path_blocking(p);
+					btrfs_tree_lock(b);
+					btrfs_clear_path_blocking(p);
+				}
+			}
 		} else {
 			p->slots[level] = slot;
 			if (ins_len > 0 &&
 			    btrfs_leaf_free_space(root, b) < ins_len) {
-				int sret = split_leaf(trans, root, key,
+				int sret;
+
+				btrfs_set_path_blocking(p);
+				sret = split_leaf(trans, root, key,
 						      p, ins_len, ret == 0);
+				btrfs_clear_path_blocking(p);
+
 				BUG_ON(sret > 0);
 				if (sret) {
 					ret = sret;
@@ -1533,12 +1719,16 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 	}
 	ret = 1;
 done:
+	/*
+	 * we don't really know what they plan on doing with the path
+	 * from here on, so for now just mark it as blocking
+	 */
+	btrfs_set_path_blocking(p);
 	if (prealloc_block.objectid) {
 		btrfs_free_reserved_extent(root,
 			   prealloc_block.objectid,
 			   prealloc_block.offset);
 	}
-
 	return ret;
 }
 
@@ -1562,6 +1752,8 @@ int btrfs_merge_path(struct btrfs_trans_handle *trans,
 	ret = btrfs_cow_block(trans, root, eb, NULL, 0, &eb, 0);
 	BUG_ON(ret);
 
+	btrfs_set_lock_blocking(eb);
+
 	parent = eb;
 	while (1) {
 		level = btrfs_header_level(parent);
@@ -1586,6 +1778,7 @@ int btrfs_merge_path(struct btrfs_trans_handle *trans,
 			eb = read_tree_block(root, bytenr, blocksize,
 					     generation);
 			btrfs_tree_lock(eb);
+			btrfs_set_lock_blocking(eb);
 		}
 
 		/*
@@ -1610,6 +1803,7 @@ int btrfs_merge_path(struct btrfs_trans_handle *trans,
 				eb = read_tree_block(root, bytenr, blocksize,
 						generation);
 				btrfs_tree_lock(eb);
+				btrfs_set_lock_blocking(eb);
 			}
 
 			ret = btrfs_cow_block(trans, root, eb, parent, slot,
@@ -2156,6 +2350,8 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 
 	right = read_node_slot(root, upper, slot + 1);
 	btrfs_tree_lock(right);
+	btrfs_set_lock_blocking(right);
+
 	free_space = btrfs_leaf_free_space(root, right);
 	if (free_space < data_size)
 		goto out_unlock;
@@ -2351,6 +2547,8 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 
 	left = read_node_slot(root, path->nodes[1], slot - 1);
 	btrfs_tree_lock(left);
+	btrfs_set_lock_blocking(left);
+
 	free_space = btrfs_leaf_free_space(root, left);
 	if (free_space < data_size) {
 		ret = 1;
@@ -2809,6 +3007,12 @@ int btrfs_split_item(struct btrfs_trans_handle *trans,
 	path->keep_locks = 0;
 	BUG_ON(ret);
 
+	/*
+	 * make sure any changes to the path from split_leaf leave it
+	 * in a blocking state
+	 */
+	btrfs_set_path_blocking(path);
+
 	leaf = path->nodes[0];
 	BUG_ON(btrfs_leaf_free_space(root, leaf) < sizeof(struct btrfs_item));
 
@@ -3338,6 +3542,7 @@ int btrfs_insert_empty_items(struct btrfs_trans_handle *trans,
 		BUG();
 	}
 out:
+	btrfs_unlock_up_safe(path, 1);
 	return ret;
 }
 
@@ -3705,12 +3910,14 @@ int btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,
 		 */
 		if (slot >= nritems) {
 			path->slots[level] = slot;
+			btrfs_set_path_blocking(path);
 			sret = btrfs_find_next_key(root, path, min_key, level,
 						  cache_only, min_trans);
 			if (sret == 0) {
 				btrfs_release_path(root, path);
 				goto again;
 			} else {
+				btrfs_clear_path_blocking(path);
 				goto out;
 			}
 		}
@@ -3722,16 +3929,20 @@ int btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,
 			unlock_up(path, level, 1);
 			goto out;
 		}
+		btrfs_set_path_blocking(path);
 		cur = read_node_slot(root, cur, slot);
 
 		btrfs_tree_lock(cur);
+
 		path->locks[level - 1] = 1;
 		path->nodes[level - 1] = cur;
 		unlock_up(path, level, 1);
+		btrfs_clear_path_blocking(path);
 	}
 out:
 	if (ret == 0)
 		memcpy(min_key, &found_key, sizeof(found_key));
+	btrfs_set_path_blocking(path);
 	return ret;
 }
 
@@ -3827,6 +4038,7 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 	if (ret < 0)
 		return ret;
 
+	btrfs_set_path_blocking(path);
 	nritems = btrfs_header_nritems(path->nodes[0]);
 	/*
 	 * by releasing the path above we dropped all our locks.  A balance
@@ -3857,6 +4069,7 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 			free_extent_buffer(next);
 		}
 
+		/* the path was set to blocking above */
 		if (level == 1 && (path->locks[1] || path->skip_locking) &&
 		    path->reada)
 			reada_for_search(root, path, level, slot, 0);
@@ -3865,6 +4078,7 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 		if (!path->skip_locking) {
 			WARN_ON(!btrfs_tree_locked(c));
 			btrfs_tree_lock(next);
+			btrfs_set_lock_blocking(next);
 		}
 		break;
 	}
@@ -3881,12 +4095,15 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 			path->locks[level] = 1;
 		if (!level)
 			break;
+
+		btrfs_set_path_blocking(path);
 		if (level == 1 && path->locks[1] && path->reada)
 			reada_for_search(root, path, level, slot, 0);
 		next = read_node_slot(root, next, 0);
 		if (!path->skip_locking) {
 			WARN_ON(!btrfs_tree_locked(path->nodes[level]));
 			btrfs_tree_lock(next);
+			btrfs_set_lock_blocking(next);
 		}
 	}
 done:
@@ -3911,6 +4128,7 @@ int btrfs_previous_item(struct btrfs_root *root,
 
 	while (1) {
 		if (path->slots[0] == 0) {
+			btrfs_set_path_blocking(path);
 			ret = btrfs_prev_leaf(root, path);
 			if (ret != 0)
 				return ret;

commit c487685d7c18a8481900755aa5c56a7a74193101
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Feb 4 09:24:25 2009 -0500

    Btrfs: hash_lock is no longer needed
    
    Before metadata is written to disk, it is updated to reflect that writeout
    has begun.  Once this update is done, the block must be cow'd before it
    can be modified again.
    
    This update was originally synchronized by using a per-fs spinlock.  Today
    the buffers for the metadata blocks are locked before writeout begins,
    and everyone that tests the flag has the buffer locked as well.
    
    So, the per-fs spinlock (called hash_lock for no good reason) is no
    longer required.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 2603ee539b7a..3b6e35aafc9e 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -388,16 +388,14 @@ noinline int btrfs_cow_block(struct btrfs_trans_handle *trans,
 		WARN_ON(1);
 	}
 
-	spin_lock(&root->fs_info->hash_lock);
 	if (btrfs_header_generation(buf) == trans->transid &&
 	    btrfs_header_owner(buf) == root->root_key.objectid &&
 	    !btrfs_header_flag(buf, BTRFS_HEADER_FLAG_WRITTEN)) {
 		*cow_ret = buf;
-		spin_unlock(&root->fs_info->hash_lock);
 		WARN_ON(prealloc_dest);
 		return 0;
 	}
-	spin_unlock(&root->fs_info->hash_lock);
+
 	search_start = buf->start & ~((u64)(1024 * 1024 * 1024) - 1);
 	ret = __btrfs_cow_block(trans, root, buf, parent,
 				 parent_slot, cow_ret, search_start, 0,
@@ -1376,14 +1374,11 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 			int wret;
 
 			/* is a cow on this block not required */
-			spin_lock(&root->fs_info->hash_lock);
 			if (btrfs_header_generation(b) == trans->transid &&
 			    btrfs_header_owner(b) == root->root_key.objectid &&
 			    !btrfs_header_flag(b, BTRFS_HEADER_FLAG_WRITTEN)) {
-				spin_unlock(&root->fs_info->hash_lock);
 				goto cow_done;
 			}
-			spin_unlock(&root->fs_info->hash_lock);
 
 			/* ok, we have to cow, is our old prealloc the right
 			 * size?

commit a717531942f488209dded30f6bc648167bcefa72
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Jan 22 09:23:10 2009 -0500

    Btrfs: do less aggressive btree readahead
    
    Just before reading a leaf, btrfs scans the node for blocks that are
    close by and reads them too.  It tries to build up a large window
    of IO looking for blocks that are within a max distance from the top
    and bottom of the IO window.
    
    This patch changes things to just look for blocks within 64k of the
    target block.  It will trigger less IO and make for lower latencies on
    the read size.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 9e46c0776816..2603ee539b7a 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1210,8 +1210,7 @@ static noinline void reada_for_search(struct btrfs_root *root,
 	struct btrfs_disk_key disk_key;
 	u32 nritems;
 	u64 search;
-	u64 lowest_read;
-	u64 highest_read;
+	u64 target;
 	u64 nread = 0;
 	int direction = path->reada;
 	struct extent_buffer *eb;
@@ -1235,8 +1234,7 @@ static noinline void reada_for_search(struct btrfs_root *root,
 		return;
 	}
 
-	highest_read = search;
-	lowest_read = search;
+	target = search;
 
 	nritems = btrfs_header_nritems(node);
 	nr = slot;
@@ -1256,24 +1254,15 @@ static noinline void reada_for_search(struct btrfs_root *root,
 				break;
 		}
 		search = btrfs_node_blockptr(node, nr);
-		if ((search >= lowest_read && search <= highest_read) ||
-		    (search < lowest_read && lowest_read - search <= 16384) ||
-		    (search > highest_read && search - highest_read <= 16384)) {
+		if ((search <= target && target - search <= 65536) ||
+		    (search > target && search - target <= 65536)) {
 			readahead_tree_block(root, search, blocksize,
 				     btrfs_node_ptr_generation(node, nr));
 			nread += blocksize;
 		}
 		nscan++;
-		if (path->reada < 2 && (nread > (64 * 1024) || nscan > 32))
+		if ((nread > 65536 || nscan > 32))
 			break;
-
-		if (nread > (256 * 1024) || nscan > 128)
-			break;
-
-		if (search < lowest_read)
-			lowest_read = search;
-		if (search > highest_read)
-			highest_read = search;
 	}
 }
 

commit d397712bcc6a759a560fd247e6053ecae091f958
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Jan 5 21:25:51 2009 -0500

    Btrfs: Fix checkpatch.pl warnings
    
    There were many, most are fixed now.  struct-funcs.c generates some warnings
    but these are bogus.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 7fad2e3ad6ff..9e46c0776816 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -67,7 +67,7 @@ void btrfs_free_path(struct btrfs_path *p)
  *
  * It is safe to call this on paths that no locks or extent buffers held.
  */
-void noinline btrfs_release_path(struct btrfs_root *root, struct btrfs_path *p)
+noinline void btrfs_release_path(struct btrfs_root *root, struct btrfs_path *p)
 {
 	int i;
 
@@ -112,7 +112,7 @@ struct extent_buffer *btrfs_lock_root_node(struct btrfs_root *root)
 {
 	struct extent_buffer *eb;
 
-	while(1) {
+	while (1) {
 		eb = btrfs_root_node(root);
 		btrfs_tree_lock(eb);
 
@@ -202,22 +202,22 @@ int btrfs_copy_root(struct btrfs_trans_handle *trans,
 }
 
 /*
- * does the dirty work in cow of a single block.  The parent block
- * (if supplied) is updated to point to the new cow copy.  The new
- * buffer is marked dirty and returned locked.  If you modify the block
- * it needs to be marked dirty again.
+ * does the dirty work in cow of a single block.  The parent block (if
+ * supplied) is updated to point to the new cow copy.  The new buffer is marked
+ * dirty and returned locked.  If you modify the block it needs to be marked
+ * dirty again.
  *
  * search_start -- an allocation hint for the new block
  *
- * empty_size -- a hint that you plan on doing more cow.  This is the size in bytes
- * the allocator should try to find free next to the block it returns.  This is
- * just a hint and may be ignored by the allocator.
+ * empty_size -- a hint that you plan on doing more cow.  This is the size in
+ * bytes the allocator should try to find free next to the block it returns.
+ * This is just a hint and may be ignored by the allocator.
  *
  * prealloc_dest -- if you have already reserved a destination for the cow,
- * this uses that block instead of allocating a new one.  btrfs_alloc_reserved_extent
- * is used to finish the allocation.
+ * this uses that block instead of allocating a new one.
+ * btrfs_alloc_reserved_extent is used to finish the allocation.
  */
-static int noinline __btrfs_cow_block(struct btrfs_trans_handle *trans,
+static noinline int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 			     struct btrfs_root *root,
 			     struct extent_buffer *buf,
 			     struct extent_buffer *parent, int parent_slot,
@@ -366,7 +366,7 @@ static int noinline __btrfs_cow_block(struct btrfs_trans_handle *trans,
  * This version of it has extra checks so that a block isn't cow'd more than
  * once per transaction, as long as it hasn't been written yet
  */
-int noinline btrfs_cow_block(struct btrfs_trans_handle *trans,
+noinline int btrfs_cow_block(struct btrfs_trans_handle *trans,
 		    struct btrfs_root *root, struct extent_buffer *buf,
 		    struct extent_buffer *parent, int parent_slot,
 		    struct extent_buffer **cow_ret, u64 prealloc_dest)
@@ -375,13 +375,16 @@ int noinline btrfs_cow_block(struct btrfs_trans_handle *trans,
 	int ret;
 
 	if (trans->transaction != root->fs_info->running_transaction) {
-		printk(KERN_CRIT "trans %Lu running %Lu\n", trans->transid,
+		printk(KERN_CRIT "trans %llu running %llu\n",
+		       (unsigned long long)trans->transid,
+		       (unsigned long long)
 		       root->fs_info->running_transaction->transid);
 		WARN_ON(1);
 	}
 	if (trans->transid != root->fs_info->generation) {
-		printk(KERN_CRIT "trans %Lu running %Lu\n", trans->transid,
-		       root->fs_info->generation);
+		printk(KERN_CRIT "trans %llu running %llu\n",
+		       (unsigned long long)trans->transid,
+		       (unsigned long long)root->fs_info->generation);
 		WARN_ON(1);
 	}
 
@@ -489,16 +492,10 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 	if (cache_only && parent_level != 1)
 		return 0;
 
-	if (trans->transaction != root->fs_info->running_transaction) {
-		printk(KERN_CRIT "trans %Lu running %Lu\n", trans->transid,
-		       root->fs_info->running_transaction->transid);
+	if (trans->transaction != root->fs_info->running_transaction)
 		WARN_ON(1);
-	}
-	if (trans->transid != root->fs_info->generation) {
-		printk(KERN_CRIT "trans %Lu running %Lu\n", trans->transid,
-		       root->fs_info->generation);
+	if (trans->transid != root->fs_info->generation)
 		WARN_ON(1);
-	}
 
 	parent_nritems = btrfs_header_nritems(parent);
 	blocksize = btrfs_level_size(root, parent_level - 1);
@@ -681,51 +678,18 @@ static int check_leaf(struct btrfs_root *root, struct btrfs_path *path,
 		BUG_ON(btrfs_node_blockptr(parent, parent_slot) !=
 		       btrfs_header_bytenr(leaf));
 	}
-#if 0
-	for (i = 0; nritems > 1 && i < nritems - 2; i++) {
-		btrfs_item_key_to_cpu(leaf, &cpukey, i + 1);
-		btrfs_item_key(leaf, &leaf_key, i);
-		if (comp_keys(&leaf_key, &cpukey) >= 0) {
-			btrfs_print_leaf(root, leaf);
-			printk("slot %d offset bad key\n", i);
-			BUG_ON(1);
-		}
-		if (btrfs_item_offset_nr(leaf, i) !=
-			btrfs_item_end_nr(leaf, i + 1)) {
-			btrfs_print_leaf(root, leaf);
-			printk("slot %d offset bad\n", i);
-			BUG_ON(1);
-		}
-		if (i == 0) {
-			if (btrfs_item_offset_nr(leaf, i) +
-			       btrfs_item_size_nr(leaf, i) !=
-			       BTRFS_LEAF_DATA_SIZE(root)) {
-				btrfs_print_leaf(root, leaf);
-				printk("slot %d first offset bad\n", i);
-				BUG_ON(1);
-			}
-		}
-	}
-	if (nritems > 0) {
-		if (btrfs_item_size_nr(leaf, nritems - 1) > 4096) {
-				btrfs_print_leaf(root, leaf);
-				printk("slot %d bad size \n", nritems - 1);
-				BUG_ON(1);
-		}
-	}
-#endif
 	if (slot != 0 && slot < nritems - 1) {
 		btrfs_item_key(leaf, &leaf_key, slot);
 		btrfs_item_key_to_cpu(leaf, &cpukey, slot - 1);
 		if (comp_keys(&leaf_key, &cpukey) <= 0) {
 			btrfs_print_leaf(root, leaf);
-			printk("slot %d offset bad key\n", slot);
+			printk(KERN_CRIT "slot %d offset bad key\n", slot);
 			BUG_ON(1);
 		}
 		if (btrfs_item_offset_nr(leaf, slot - 1) !=
 		       btrfs_item_end_nr(leaf, slot)) {
 			btrfs_print_leaf(root, leaf);
-			printk("slot %d offset bad\n", slot);
+			printk(KERN_CRIT "slot %d offset bad\n", slot);
 			BUG_ON(1);
 		}
 	}
@@ -736,7 +700,7 @@ static int check_leaf(struct btrfs_root *root, struct btrfs_path *path,
 		if (btrfs_item_offset_nr(leaf, slot) !=
 			btrfs_item_end_nr(leaf, slot + 1)) {
 			btrfs_print_leaf(root, leaf);
-			printk("slot %d offset bad\n", slot);
+			printk(KERN_CRIT "slot %d offset bad\n", slot);
 			BUG_ON(1);
 		}
 	}
@@ -745,30 +709,10 @@ static int check_leaf(struct btrfs_root *root, struct btrfs_path *path,
 	return 0;
 }
 
-static int noinline check_block(struct btrfs_root *root,
+static noinline int check_block(struct btrfs_root *root,
 				struct btrfs_path *path, int level)
 {
-	u64 found_start;
 	return 0;
-	if (btrfs_header_level(path->nodes[level]) != level)
-	    printk("warning: bad level %Lu wanted %d found %d\n",
-		   path->nodes[level]->start, level,
-		   btrfs_header_level(path->nodes[level]));
-	found_start = btrfs_header_bytenr(path->nodes[level]);
-	if (found_start != path->nodes[level]->start) {
-	    printk("warning: bad bytentr %Lu found %Lu\n",
-		   path->nodes[level]->start, found_start);
-	}
-#if 0
-	struct extent_buffer *buf = path->nodes[level];
-
-	if (memcmp_extent_buffer(buf, root->fs_info->fsid,
-				 (unsigned long)btrfs_header_fsid(buf),
-				 BTRFS_FSID_SIZE)) {
-		printk("warning bad block %Lu\n", buf->start);
-		return 1;
-	}
-#endif
 	if (level == 0)
 		return check_leaf(root, path, level);
 	return check_node(root, path, level);
@@ -802,7 +746,7 @@ static noinline int generic_bin_search(struct extent_buffer *eb,
 	unsigned long map_len = 0;
 	int err;
 
-	while(low < high) {
+	while (low < high) {
 		mid = (low + high) / 2;
 		offset = p + mid * item_size;
 
@@ -1130,7 +1074,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
  * when they are completely full.  This is also done top down, so we
  * have to be pessimistic.
  */
-static int noinline push_nodes_for_insert(struct btrfs_trans_handle *trans,
+static noinline int push_nodes_for_insert(struct btrfs_trans_handle *trans,
 					  struct btrfs_root *root,
 					  struct btrfs_path *path, int level)
 {
@@ -1296,7 +1240,7 @@ static noinline void reada_for_search(struct btrfs_root *root,
 
 	nritems = btrfs_header_nritems(node);
 	nr = slot;
-	while(1) {
+	while (1) {
 		if (direction < 0) {
 			if (nr == 0)
 				break;
@@ -1322,7 +1266,8 @@ static noinline void reada_for_search(struct btrfs_root *root,
 		nscan++;
 		if (path->reada < 2 && (nread > (64 * 1024) || nscan > 32))
 			break;
-		if(nread > (256 * 1024) || nscan > 128)
+
+		if (nread > (256 * 1024) || nscan > 128)
 			break;
 
 		if (search < lowest_read)
@@ -1333,17 +1278,17 @@ static noinline void reada_for_search(struct btrfs_root *root,
 }
 
 /*
- * when we walk down the tree, it is usually safe to unlock the higher layers in
- * the tree.  The exceptions are when our path goes through slot 0, because operations
- * on the tree might require changing key pointers higher up in the tree.
+ * when we walk down the tree, it is usually safe to unlock the higher layers
+ * in the tree.  The exceptions are when our path goes through slot 0, because
+ * operations on the tree might require changing key pointers higher up in the
+ * tree.
  *
- * callers might also have set path->keep_locks, which tells this code to
- * keep the lock if the path points to the last slot in the block.  This is
- * part of walking through the tree, and selecting the next slot in the higher
- * block.
+ * callers might also have set path->keep_locks, which tells this code to keep
+ * the lock if the path points to the last slot in the block.  This is part of
+ * walking through the tree, and selecting the next slot in the higher block.
  *
- * lowest_unlock sets the lowest level in the tree we're allowed to unlock.
- * so if lowest_unlock is 1, level 0 won't be unlocked
+ * lowest_unlock sets the lowest level in the tree we're allowed to unlock.  so
+ * if lowest_unlock is 1, level 0 won't be unlocked
  */
 static noinline void unlock_up(struct btrfs_path *path, int level,
 			       int lowest_unlock)
@@ -1832,9 +1777,8 @@ static int push_node_left(struct btrfs_trans_handle *trans,
 	if (!empty && src_nritems <= 8)
 		return 1;
 
-	if (push_items <= 0) {
+	if (push_items <= 0)
 		return 1;
-	}
 
 	if (empty) {
 		push_items = min(src_nritems, push_items);
@@ -1854,7 +1798,7 @@ static int push_node_left(struct btrfs_trans_handle *trans,
 	copy_extent_buffer(dst, src,
 			   btrfs_node_key_ptr_offset(dst_nritems),
 			   btrfs_node_key_ptr_offset(0),
-		           push_items * sizeof(struct btrfs_key_ptr));
+			   push_items * sizeof(struct btrfs_key_ptr));
 
 	if (push_items < src_nritems) {
 		memmove_extent_buffer(src, btrfs_node_key_ptr_offset(0),
@@ -1899,19 +1843,16 @@ static int balance_node_right(struct btrfs_trans_handle *trans,
 	src_nritems = btrfs_header_nritems(src);
 	dst_nritems = btrfs_header_nritems(dst);
 	push_items = BTRFS_NODEPTRS_PER_BLOCK(root) - dst_nritems;
-	if (push_items <= 0) {
+	if (push_items <= 0)
 		return 1;
-	}
 
-	if (src_nritems < 4) {
+	if (src_nritems < 4)
 		return 1;
-	}
 
 	max_push = src_nritems / 2 + 1;
 	/* don't try to empty the node */
-	if (max_push >= src_nritems) {
+	if (max_push >= src_nritems)
 		return 1;
-	}
 
 	if (max_push < push_items)
 		push_items = max_push;
@@ -1924,7 +1865,7 @@ static int balance_node_right(struct btrfs_trans_handle *trans,
 	copy_extent_buffer(dst, src,
 			   btrfs_node_key_ptr_offset(0),
 			   btrfs_node_key_ptr_offset(src_nritems - push_items),
-		           push_items * sizeof(struct btrfs_key_ptr));
+			   push_items * sizeof(struct btrfs_key_ptr));
 
 	btrfs_set_header_nritems(src, src_nritems - push_items);
 	btrfs_set_header_nritems(dst, dst_nritems + push_items);
@@ -1945,7 +1886,7 @@ static int balance_node_right(struct btrfs_trans_handle *trans,
  *
  * returns zero on success or < 0 on failure.
  */
-static int noinline insert_new_root(struct btrfs_trans_handle *trans,
+static noinline int insert_new_root(struct btrfs_trans_handle *trans,
 			   struct btrfs_root *root,
 			   struct btrfs_path *path, int level)
 {
@@ -2176,14 +2117,15 @@ static int leaf_space_used(struct extent_buffer *l, int start, int nr)
  * the start of the leaf data.  IOW, how much room
  * the leaf has left for both items and data
  */
-int noinline btrfs_leaf_free_space(struct btrfs_root *root,
+noinline int btrfs_leaf_free_space(struct btrfs_root *root,
 				   struct extent_buffer *leaf)
 {
 	int nritems = btrfs_header_nritems(leaf);
 	int ret;
 	ret = BTRFS_LEAF_DATA_SIZE(root) - leaf_space_used(leaf, 0, nritems);
 	if (ret < 0) {
-		printk("leaf free space ret %d, leaf data size %lu, used %d nritems %d\n",
+		printk(KERN_CRIT "leaf free space ret %d, leaf data size %lu, "
+		       "used %d nritems %d\n",
 		       ret, (unsigned long) BTRFS_LEAF_DATA_SIZE(root),
 		       leaf_space_used(leaf, 0, nritems), nritems);
 	}
@@ -2219,9 +2161,9 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 	int ret;
 
 	slot = path->slots[1];
-	if (!path->nodes[1]) {
+	if (!path->nodes[1])
 		return 1;
-	}
+
 	upper = path->nodes[1];
 	if (slot >= btrfs_header_nritems(upper) - 1)
 		return 1;
@@ -2418,9 +2360,8 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 		return 1;
 
 	right_nritems = btrfs_header_nritems(right);
-	if (right_nritems == 0) {
+	if (right_nritems == 0)
 		return 1;
-	}
 
 	WARN_ON(!btrfs_tree_locked(path->nodes[1]));
 
@@ -2502,7 +2443,7 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 			   push_items * sizeof(struct btrfs_item));
 
 	push_space = BTRFS_LEAF_DATA_SIZE(root) -
-		     btrfs_item_offset_nr(right, push_items -1);
+		     btrfs_item_offset_nr(right, push_items - 1);
 
 	copy_extent_buffer(left, right, btrfs_leaf_data(left) +
 		     leaf_data_end(root, left) - push_space,
@@ -2537,7 +2478,8 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 
 	/* fixup right node */
 	if (push_items > right_nritems) {
-		printk("push items %d nr %u\n", push_items, right_nritems);
+		printk(KERN_CRIT "push items %d nr %u\n", push_items,
+		       right_nritems);
 		WARN_ON(1);
 	}
 
@@ -2640,9 +2582,8 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 	/* first try to make some room by pushing left and right */
 	if (data_size && ins_key->type != BTRFS_DIR_ITEM_KEY) {
 		wret = push_leaf_right(trans, root, path, data_size, 0);
-		if (wret < 0) {
+		if (wret < 0)
 			return wret;
-		}
 		if (wret) {
 			wret = push_leaf_left(trans, root, path, data_size, 0);
 			if (wret < 0)
@@ -2665,7 +2606,7 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 	l = path->nodes[0];
 	slot = path->slots[0];
 	nritems = btrfs_header_nritems(l);
-	mid = (nritems + 1)/ 2;
+	mid = (nritems + 1) / 2;
 
 	right = btrfs_alloc_free_block(trans, root, root->leafsize,
 					path->nodes[1]->start,
@@ -2734,7 +2675,7 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 				path->slots[0] = 0;
 				if (path->slots[1] == 0) {
 					wret = fixup_low_keys(trans, root,
-					           path, &disk_key, 1);
+						      path, &disk_key, 1);
 					if (wret)
 						ret = wret;
 				}
@@ -3033,8 +2974,8 @@ int btrfs_truncate_item(struct btrfs_trans_handle *trans,
 			    BTRFS_FILE_EXTENT_INLINE) {
 				ptr = btrfs_item_ptr_offset(leaf, slot);
 				memmove_extent_buffer(leaf, ptr,
-				        (unsigned long)fi,
-				        offsetof(struct btrfs_file_extent_item,
+				      (unsigned long)fi,
+				      offsetof(struct btrfs_file_extent_item,
 						 disk_bytenr));
 			}
 		}
@@ -3096,7 +3037,8 @@ int btrfs_extend_item(struct btrfs_trans_handle *trans,
 	BUG_ON(slot < 0);
 	if (slot >= nritems) {
 		btrfs_print_leaf(root, leaf);
-		printk("slot %d too large, nritems %d\n", slot, nritems);
+		printk(KERN_CRIT "slot %d too large, nritems %d\n",
+		       slot, nritems);
 		BUG_ON(1);
 	}
 
@@ -3218,7 +3160,7 @@ int btrfs_insert_some_items(struct btrfs_trans_handle *trans,
 
 		if (old_data < data_end) {
 			btrfs_print_leaf(root, leaf);
-			printk("slot %d old_data %d data_end %d\n",
+			printk(KERN_CRIT "slot %d old_data %d data_end %d\n",
 			       slot, old_data, data_end);
 			BUG_ON(1);
 		}
@@ -3317,9 +3259,8 @@ int btrfs_insert_empty_items(struct btrfs_trans_handle *trans,
 	unsigned int data_end;
 	struct btrfs_disk_key disk_key;
 
-	for (i = 0; i < nr; i++) {
+	for (i = 0; i < nr; i++)
 		total_data += data_size[i];
-	}
 
 	total_size = total_data + (nr * sizeof(struct btrfs_item));
 	ret = btrfs_search_slot(trans, root, cpu_key, path, total_size, 1);
@@ -3336,7 +3277,7 @@ int btrfs_insert_empty_items(struct btrfs_trans_handle *trans,
 
 	if (btrfs_leaf_free_space(root, leaf) < total_size) {
 		btrfs_print_leaf(root, leaf);
-		printk("not enough freespace need %u have %d\n",
+		printk(KERN_CRIT "not enough freespace need %u have %d\n",
 		       total_size, btrfs_leaf_free_space(root, leaf));
 		BUG();
 	}
@@ -3349,7 +3290,7 @@ int btrfs_insert_empty_items(struct btrfs_trans_handle *trans,
 
 		if (old_data < data_end) {
 			btrfs_print_leaf(root, leaf);
-			printk("slot %d old_data %d data_end %d\n",
+			printk(KERN_CRIT "slot %d old_data %d data_end %d\n",
 			       slot, old_data, data_end);
 			BUG_ON(1);
 		}
@@ -3457,7 +3398,7 @@ static int del_ptr(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 	int wret;
 
 	nritems = btrfs_header_nritems(parent);
-	if (slot != nritems -1) {
+	if (slot != nritems - 1) {
 		memmove_extent_buffer(parent,
 			      btrfs_node_key_ptr_offset(slot),
 			      btrfs_node_key_ptr_offset(slot + 1),
@@ -3614,7 +3555,8 @@ int btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 
 			if (btrfs_header_nritems(leaf) == 0) {
 				path->slots[1] = slot;
-				ret = btrfs_del_leaf(trans, root, path, leaf->start);
+				ret = btrfs_del_leaf(trans, root, path,
+						     leaf->start);
 				BUG_ON(ret);
 				free_extent_buffer(leaf);
 			} else {
@@ -3717,7 +3659,7 @@ int btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,
 		ret = 1;
 		goto out;
 	}
-	while(1) {
+	while (1) {
 		nritems = btrfs_header_nritems(cur);
 		level = btrfs_header_level(cur);
 		sret = bin_search(cur, min_key, level, &slot);
@@ -3738,7 +3680,7 @@ int btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,
 		 * min_trans parameters.  If it isn't in cache or is too
 		 * old, skip to the next one.
 		 */
-		while(slot < nritems) {
+		while (slot < nritems) {
 			u64 blockptr;
 			u64 gen;
 			struct extent_buffer *tmp;
@@ -3830,7 +3772,7 @@ int btrfs_find_next_key(struct btrfs_root *root, struct btrfs_path *path,
 	struct extent_buffer *c;
 
 	WARN_ON(!path->keep_locks);
-	while(level < BTRFS_MAX_LEVEL) {
+	while (level < BTRFS_MAX_LEVEL) {
 		if (!path->nodes[level])
 			return 1;
 
@@ -3839,9 +3781,8 @@ int btrfs_find_next_key(struct btrfs_root *root, struct btrfs_path *path,
 next:
 		if (slot >= btrfs_header_nritems(c)) {
 			level++;
-			if (level == BTRFS_MAX_LEVEL) {
+			if (level == BTRFS_MAX_LEVEL)
 				return 1;
-			}
 			continue;
 		}
 		if (level == 0)
@@ -3889,9 +3830,8 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 	int ret;
 
 	nritems = btrfs_header_nritems(path->nodes[0]);
-	if (nritems == 0) {
+	if (nritems == 0)
 		return 1;
-	}
 
 	btrfs_item_key_to_cpu(path->nodes[0], &key, nritems - 1);
 
@@ -3915,7 +3855,7 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 		goto done;
 	}
 
-	while(level < BTRFS_MAX_LEVEL) {
+	while (level < BTRFS_MAX_LEVEL) {
 		if (!path->nodes[level])
 			return 1;
 
@@ -3923,9 +3863,8 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 		c = path->nodes[level];
 		if (slot >= btrfs_header_nritems(c)) {
 			level++;
-			if (level == BTRFS_MAX_LEVEL) {
+			if (level == BTRFS_MAX_LEVEL)
 				return 1;
-			}
 			continue;
 		}
 
@@ -3946,7 +3885,7 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 		break;
 	}
 	path->slots[level] = slot;
-	while(1) {
+	while (1) {
 		level--;
 		c = path->nodes[level];
 		if (path->locks[level])
@@ -3986,7 +3925,7 @@ int btrfs_previous_item(struct btrfs_root *root,
 	u32 nritems;
 	int ret;
 
-	while(1) {
+	while (1) {
 		if (path->slots[0] == 0) {
 			ret = btrfs_prev_leaf(root, path);
 			if (ret != 0)

commit 87b29b208c6c38f3446d2de6ece946e2459052cf
Author: Yan Zheng <zheng.yan@oracle.com>
Date:   Wed Dec 17 10:21:48 2008 -0500

    Btrfs: properly check free space for tree balancing
    
    btrfs_insert_empty_items takes the space needed by the btrfs_item
    structure into account when calculating the required free space.
    
    So the tree balancing code shouldn't add sizeof(struct btrfs_item)
    to the size when checking the free space. This patch removes these
    superfluous additions.
    
    Signed-off-by: Yan Zheng <zheng.yan@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index f6f7a6af0357..7fad2e3ad6ff 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1587,8 +1587,8 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 				btrfs_tree_lock(b);
 		} else {
 			p->slots[level] = slot;
-			if (ins_len > 0 && btrfs_leaf_free_space(root, b) <
-			    sizeof(struct btrfs_item) + ins_len) {
+			if (ins_len > 0 &&
+			    btrfs_leaf_free_space(root, b) < ins_len) {
 				int sret = split_leaf(trans, root, key,
 						      p, ins_len, ret == 0);
 				BUG_ON(sret > 0);
@@ -2231,7 +2231,7 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 	right = read_node_slot(root, upper, slot + 1);
 	btrfs_tree_lock(right);
 	free_space = btrfs_leaf_free_space(root, right);
-	if (free_space < data_size + sizeof(struct btrfs_item))
+	if (free_space < data_size)
 		goto out_unlock;
 
 	/* cow and double check */
@@ -2241,7 +2241,7 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 		goto out_unlock;
 
 	free_space = btrfs_leaf_free_space(root, right);
-	if (free_space < data_size + sizeof(struct btrfs_item))
+	if (free_space < data_size)
 		goto out_unlock;
 
 	left_nritems = btrfs_header_nritems(left);
@@ -2254,7 +2254,7 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 		nr = 1;
 
 	if (path->slots[0] >= left_nritems)
-		push_space += data_size + sizeof(*item);
+		push_space += data_size;
 
 	i = left_nritems - 1;
 	while (i >= nr) {
@@ -2271,7 +2271,7 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 		}
 
 		if (path->slots[0] == i)
-			push_space += data_size + sizeof(*item);
+			push_space += data_size;
 
 		if (!left->map_token) {
 			map_extent_buffer(left, (unsigned long)item,
@@ -2427,7 +2427,7 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 	left = read_node_slot(root, path->nodes[1], slot - 1);
 	btrfs_tree_lock(left);
 	free_space = btrfs_leaf_free_space(root, left);
-	if (free_space < data_size + sizeof(struct btrfs_item)) {
+	if (free_space < data_size) {
 		ret = 1;
 		goto out;
 	}
@@ -2442,7 +2442,7 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 	}
 
 	free_space = btrfs_leaf_free_space(root, left);
-	if (free_space < data_size + sizeof(struct btrfs_item)) {
+	if (free_space < data_size) {
 		ret = 1;
 		goto out;
 	}
@@ -2473,7 +2473,7 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 		}
 
 		if (path->slots[0] == i)
-			push_space += data_size + sizeof(*item);
+			push_space += data_size;
 
 		this_item_size = btrfs_item_size(right, item);
 		if (this_item_size + sizeof(*item) + push_space > free_space)
@@ -2510,7 +2510,7 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 		     btrfs_item_offset_nr(right, push_items - 1),
 		     push_space);
 	old_left_nritems = btrfs_header_nritems(left);
-	BUG_ON(old_left_nritems < 0);
+	BUG_ON(old_left_nritems <= 0);
 
 	old_left_item_size = btrfs_item_offset_nr(left, old_left_nritems - 1);
 	for (i = old_left_nritems; i < old_left_nritems + push_items; i++) {
@@ -2628,7 +2628,6 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 	int mid;
 	int slot;
 	struct extent_buffer *right;
-	int space_needed = data_size + sizeof(struct btrfs_item);
 	int data_copy_size;
 	int rt_data_off;
 	int i;
@@ -2638,9 +2637,6 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 	int num_doubles = 0;
 	struct btrfs_disk_key disk_key;
 
-	if (extend && data_size)
-		space_needed = data_size;
-
 	/* first try to make some room by pushing left and right */
 	if (data_size && ins_key->type != BTRFS_DIR_ITEM_KEY) {
 		wret = push_leaf_right(trans, root, path, data_size, 0);
@@ -2655,7 +2651,7 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 		l = path->nodes[0];
 
 		/* did the pushes work? */
-		if (btrfs_leaf_free_space(root, l) >= space_needed)
+		if (btrfs_leaf_free_space(root, l) >= data_size)
 			return 0;
 	}
 
@@ -2694,7 +2690,7 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 			    BTRFS_UUID_SIZE);
 	if (mid <= slot) {
 		if (nritems == 1 ||
-		    leaf_space_used(l, mid, nritems - mid) + space_needed >
+		    leaf_space_used(l, mid, nritems - mid) + data_size >
 			BTRFS_LEAF_DATA_SIZE(root)) {
 			if (slot >= nritems) {
 				btrfs_cpu_key_to_disk(&disk_key, ins_key);
@@ -2716,12 +2712,12 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 			mid = slot;
 			if (mid != nritems &&
 			    leaf_space_used(l, mid, nritems - mid) +
-			    space_needed > BTRFS_LEAF_DATA_SIZE(root)) {
+			    data_size > BTRFS_LEAF_DATA_SIZE(root)) {
 				double_split = 1;
 			}
 		}
 	} else {
-		if (leaf_space_used(l, 0, mid + 1) + space_needed >
+		if (leaf_space_used(l, 0, mid) + data_size >
 			BTRFS_LEAF_DATA_SIZE(root)) {
 			if (!extend && data_size && slot == 0) {
 				btrfs_cpu_key_to_disk(&disk_key, ins_key);
@@ -2750,7 +2746,7 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 				mid = slot;
 				if (mid != nritems &&
 				    leaf_space_used(l, mid, nritems - mid) +
-				    space_needed > BTRFS_LEAF_DATA_SIZE(root)) {
+				    data_size > BTRFS_LEAF_DATA_SIZE(root)) {
 					double_split = 1;
 				}
 			}
@@ -2883,7 +2879,8 @@ int btrfs_split_item(struct btrfs_trans_handle *trans,
 		return -EAGAIN;
 	}
 
-	ret = split_leaf(trans, root, &orig_key, path, 0, 0);
+	ret = split_leaf(trans, root, &orig_key, path,
+			 sizeof(struct btrfs_item), 1);
 	path->keep_locks = 0;
 	BUG_ON(ret);
 
@@ -3169,14 +3166,17 @@ int btrfs_insert_some_items(struct btrfs_trans_handle *trans,
 	struct btrfs_disk_key disk_key;
 	struct btrfs_key found_key;
 
-	found_key.objectid = 0;
-	nr = min_t(int, nr, BTRFS_NODEPTRS_PER_BLOCK(root));
-
-	for (i = 0; i < nr; i++)
+	for (i = 0; i < nr; i++) {
+		if (total_size + data_size[i] + sizeof(struct btrfs_item) >
+		    BTRFS_LEAF_DATA_SIZE(root)) {
+			break;
+			nr = i;
+		}
 		total_data += data_size[i];
+		total_size += data_size[i] + sizeof(struct btrfs_item);
+	}
+	BUG_ON(nr == 0);
 
-	total_data = min_t(u32, total_data, BTRFS_LEAF_DATA_SIZE(root));
-	total_size = total_data + (nr * sizeof(struct btrfs_item));
 	ret = btrfs_search_slot(trans, root, cpu_key, path, total_size, 1);
 	if (ret == 0)
 		return -EEXIST;

commit 42dc7babdcc99feadb04d461592ce5898a362550
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Dec 15 11:44:56 2008 -0500

    Btrfs: Fix compressed writes on truncated pages
    
    The compression code was using isize to limit the amount of data it
    sent through zlib.  But, it wasn't properly limiting the looping to
    just the pages inside i_size.  The end result was trying to compress
    too many pages, including those that had not been setup and properly locked
    down.  This made the compression code oops while trying find_get_page on a
    page that didn't exist.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index c0c95cccbb5b..f6f7a6af0357 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2887,8 +2887,8 @@ int btrfs_split_item(struct btrfs_trans_handle *trans,
 	path->keep_locks = 0;
 	BUG_ON(ret);
 
-	BUG_ON(btrfs_leaf_free_space(root, leaf) < sizeof(struct btrfs_item));
 	leaf = path->nodes[0];
+	BUG_ON(btrfs_leaf_free_space(root, leaf) < sizeof(struct btrfs_item));
 
 split:
 	item = btrfs_item_nr(leaf, path->slots[0]);

commit 459931eca5f4b8c9ad259d07cc1ca49afed54804
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Dec 10 09:10:46 2008 -0500

    Btrfs: Delete csum items when freeing extents
    
    This finishes off the new checksumming code by removing csum items
    for extents that are no longer in use.
    
    The trick is doing it without racing because a single csum item may
    hold csums for more than one extent.  Extra checks are added to
    btrfs_csum_file_blocks to make sure that we are using the correct
    csum item after dropping locks.
    
    A new btrfs_split_item is added to split a single csum item so it
    can be split without dropping the leaf lock.  This is used to
    remove csum bytes from the middle of an item.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 19c0dd33b1e8..c0c95cccbb5b 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1512,7 +1512,8 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 			if (ret && slot > 0)
 				slot -= 1;
 			p->slots[level] = slot;
-			if (ins_len > 0 && btrfs_header_nritems(b) >=
+			if ((p->search_for_split || ins_len > 0) &&
+			    btrfs_header_nritems(b) >=
 			    BTRFS_NODEPTRS_PER_BLOCK(root) - 3) {
 				int sret = split_node(trans, root, p, level);
 				BUG_ON(sret > 0);
@@ -1596,7 +1597,8 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 					goto done;
 				}
 			}
-			unlock_up(p, level, lowest_unlock);
+			if (!p->search_for_split)
+				unlock_up(p, level, lowest_unlock);
 			goto done;
 		}
 	}
@@ -2636,11 +2638,11 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 	int num_doubles = 0;
 	struct btrfs_disk_key disk_key;
 
-	if (extend)
+	if (extend && data_size)
 		space_needed = data_size;
 
 	/* first try to make some room by pushing left and right */
-	if (ins_key->type != BTRFS_DIR_ITEM_KEY) {
+	if (data_size && ins_key->type != BTRFS_DIR_ITEM_KEY) {
 		wret = push_leaf_right(trans, root, path, data_size, 0);
 		if (wret < 0) {
 			return wret;
@@ -2721,7 +2723,7 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 	} else {
 		if (leaf_space_used(l, 0, mid + 1) + space_needed >
 			BTRFS_LEAF_DATA_SIZE(root)) {
-			if (!extend && slot == 0) {
+			if (!extend && data_size && slot == 0) {
 				btrfs_cpu_key_to_disk(&disk_key, ins_key);
 				btrfs_set_header_nritems(right, 0);
 				wret = insert_ptr(trans, root, path,
@@ -2742,7 +2744,7 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 				}
 				btrfs_mark_buffer_dirty(right);
 				return ret;
-			} else if (extend && slot == 0) {
+			} else if ((extend || !data_size) && slot == 0) {
 				mid = 1;
 			} else {
 				mid = slot;
@@ -2827,6 +2829,123 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 	return ret;
 }
 
+/*
+ * This function splits a single item into two items,
+ * giving 'new_key' to the new item and splitting the
+ * old one at split_offset (from the start of the item).
+ *
+ * The path may be released by this operation.  After
+ * the split, the path is pointing to the old item.  The
+ * new item is going to be in the same node as the old one.
+ *
+ * Note, the item being split must be smaller enough to live alone on
+ * a tree block with room for one extra struct btrfs_item
+ *
+ * This allows us to split the item in place, keeping a lock on the
+ * leaf the entire time.
+ */
+int btrfs_split_item(struct btrfs_trans_handle *trans,
+		     struct btrfs_root *root,
+		     struct btrfs_path *path,
+		     struct btrfs_key *new_key,
+		     unsigned long split_offset)
+{
+	u32 item_size;
+	struct extent_buffer *leaf;
+	struct btrfs_key orig_key;
+	struct btrfs_item *item;
+	struct btrfs_item *new_item;
+	int ret = 0;
+	int slot;
+	u32 nritems;
+	u32 orig_offset;
+	struct btrfs_disk_key disk_key;
+	char *buf;
+
+	leaf = path->nodes[0];
+	btrfs_item_key_to_cpu(leaf, &orig_key, path->slots[0]);
+	if (btrfs_leaf_free_space(root, leaf) >= sizeof(struct btrfs_item))
+		goto split;
+
+	item_size = btrfs_item_size_nr(leaf, path->slots[0]);
+	btrfs_release_path(root, path);
+
+	path->search_for_split = 1;
+	path->keep_locks = 1;
+
+	ret = btrfs_search_slot(trans, root, &orig_key, path, 0, 1);
+	path->search_for_split = 0;
+
+	/* if our item isn't there or got smaller, return now */
+	if (ret != 0 || item_size != btrfs_item_size_nr(path->nodes[0],
+							path->slots[0])) {
+		path->keep_locks = 0;
+		return -EAGAIN;
+	}
+
+	ret = split_leaf(trans, root, &orig_key, path, 0, 0);
+	path->keep_locks = 0;
+	BUG_ON(ret);
+
+	BUG_ON(btrfs_leaf_free_space(root, leaf) < sizeof(struct btrfs_item));
+	leaf = path->nodes[0];
+
+split:
+	item = btrfs_item_nr(leaf, path->slots[0]);
+	orig_offset = btrfs_item_offset(leaf, item);
+	item_size = btrfs_item_size(leaf, item);
+
+
+	buf = kmalloc(item_size, GFP_NOFS);
+	read_extent_buffer(leaf, buf, btrfs_item_ptr_offset(leaf,
+			    path->slots[0]), item_size);
+	slot = path->slots[0] + 1;
+	leaf = path->nodes[0];
+
+	nritems = btrfs_header_nritems(leaf);
+
+	if (slot != nritems) {
+		/* shift the items */
+		memmove_extent_buffer(leaf, btrfs_item_nr_offset(slot + 1),
+			      btrfs_item_nr_offset(slot),
+			      (nritems - slot) * sizeof(struct btrfs_item));
+
+	}
+
+	btrfs_cpu_key_to_disk(&disk_key, new_key);
+	btrfs_set_item_key(leaf, &disk_key, slot);
+
+	new_item = btrfs_item_nr(leaf, slot);
+
+	btrfs_set_item_offset(leaf, new_item, orig_offset);
+	btrfs_set_item_size(leaf, new_item, item_size - split_offset);
+
+	btrfs_set_item_offset(leaf, item,
+			      orig_offset + item_size - split_offset);
+	btrfs_set_item_size(leaf, item, split_offset);
+
+	btrfs_set_header_nritems(leaf, nritems + 1);
+
+	/* write the data for the start of the original item */
+	write_extent_buffer(leaf, buf,
+			    btrfs_item_ptr_offset(leaf, path->slots[0]),
+			    split_offset);
+
+	/* write the data for the new item */
+	write_extent_buffer(leaf, buf + split_offset,
+			    btrfs_item_ptr_offset(leaf, slot),
+			    item_size - split_offset);
+	btrfs_mark_buffer_dirty(leaf);
+
+	ret = 0;
+	if (btrfs_leaf_free_space(root, leaf) < 0) {
+		btrfs_print_leaf(root, leaf);
+		BUG();
+	}
+	kfree(buf);
+	return ret;
+}
+
 /*
  * make the item pointed to by the path smaller.  new_size indicates
  * how small to make it, and from_end tells us if we just chop bytes

commit 934d375bacf9ea8a37fbfff5f3cf1c093f324095
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Dec 8 16:43:10 2008 -0500

    Btrfs: Use map_private_extent_buffer during generic_bin_search
    
    It is possible that generic_bin_search will be called on a tree block
    that has not been locked.  This happens because cache_block_block skips
    locking on the tree blocks.
    
    Since the tree block isn't locked, we aren't allowed to change
    the extent_buffer->map_token field.  Using map_private_extent_buffer
    avoids any changes to the internal extent buffer fields.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index a83cbdf1d8c4..19c0dd33b1e8 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -813,7 +813,8 @@ static noinline int generic_bin_search(struct extent_buffer *eb,
 				unmap_extent_buffer(eb, map_token, KM_USER0);
 				map_token = NULL;
 			}
-			err = map_extent_buffer(eb, offset,
+
+			err = map_private_extent_buffer(eb, offset,
 						sizeof(struct btrfs_disk_key),
 						&map_token, &kaddr,
 						&map_start, &map_len, KM_USER0);
@@ -3585,6 +3586,7 @@ int btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,
 	int level;
 	int ret = 1;
 
+	WARN_ON(!path->keep_locks);
 again:
 	cur = btrfs_lock_root_node(root);
 	level = btrfs_header_level(cur);
@@ -3708,6 +3710,7 @@ int btrfs_find_next_key(struct btrfs_root *root, struct btrfs_path *path,
 	int slot;
 	struct extent_buffer *c;
 
+	WARN_ON(!path->keep_locks);
 	while(level < BTRFS_MAX_LEVEL) {
 		if (!path->nodes[level])
 			return 1;

commit b2950863c61bc24cf0f63bc05947d9d50663c4c0
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Dec 2 09:54:17 2008 -0500

    Btrfs: make things static and include the right headers
    
    Shut up various sparse warnings about symbols that should be either
    static or have their declarations in scope.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 71ef0a2e2daa..a83cbdf1d8c4 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -217,7 +217,7 @@ int btrfs_copy_root(struct btrfs_trans_handle *trans,
  * this uses that block instead of allocating a new one.  btrfs_alloc_reserved_extent
  * is used to finish the allocation.
  */
-int noinline __btrfs_cow_block(struct btrfs_trans_handle *trans,
+static int noinline __btrfs_cow_block(struct btrfs_trans_handle *trans,
 			     struct btrfs_root *root,
 			     struct extent_buffer *buf,
 			     struct extent_buffer *parent, int parent_slot,

commit b4eec2ca1167bae46295aeb66abd15fd52387845
Author: Liu Hui <onlyflyer@gmail.com>
Date:   Tue Nov 18 11:30:10 2008 -0500

    Btrfs: Some fixes for batching extent insert.
    
    In insert_extents(), when ret==1 and last is not zero, it should
    check if the current inserted item is the last item in this batching
    inserts. If so, it should just break from loop. If not, 'cur =
    insert_list->next' will make no sense because the list is empty now,
    and 'op' will point to an unexpectable place.
    
    There are also some trivial fixs in this patch including one comment
    typo error and deleting two redundant lines.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index dd1c03aea2df..71ef0a2e2daa 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -3041,7 +3041,6 @@ int btrfs_insert_some_items(struct btrfs_trans_handle *trans,
 	struct btrfs_item *item;
 	int ret = 0;
 	int slot;
-	int slot_orig;
 	int i;
 	u32 nritems;
 	u32 total_data = 0;
@@ -3064,7 +3063,6 @@ int btrfs_insert_some_items(struct btrfs_trans_handle *trans,
 	if (ret < 0)
 		goto out;
 
-	slot_orig = path->slots[0];
 	leaf = path->nodes[0];
 
 	nritems = btrfs_header_nritems(leaf);

commit 2b82032c34ec40515d3c45c36cd1961f37977de8
Author: Yan Zheng <zheng.yan@oracle.com>
Date:   Mon Nov 17 21:11:30 2008 -0500

    Btrfs: Seed device support
    
    Seed device is a special btrfs with SEEDING super flag
    set and can only be mounted in read-only mode. Seed
    devices allow people to create new btrfs on top of it.
    
    The new FS contains the same contents as the seed device,
    but it can be mounted in read-write mode.
    
    This patch does the following:
    
    1) split code in btrfs_alloc_chunk into two parts. The first part does makes
    the newly allocated chunk usable, but does not do any operation that modifies
    the chunk tree. The second part does the the chunk tree modifications. This
    division is for the bootstrap step of adding storage to the seed device.
    
    2) Update device management code to handle seed device.
    The basic idea is: For an FS grown from seed devices, its
    seed devices are put into a list. Seed devices are
    opened on demand at mounting time. If any seed device is
    missing or has been changed, btrfs kernel module will
    refuse to mount the FS.
    
    3) make btrfs_find_block_group not return NULL when all
    block groups are read-only.
    
    Signed-off-by: Yan Zheng <zheng.yan@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 8bb452456d90..dd1c03aea2df 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -185,6 +185,10 @@ int btrfs_copy_root(struct btrfs_trans_handle *trans,
 	btrfs_set_header_owner(cow, new_root_objectid);
 	btrfs_clear_header_flag(cow, BTRFS_HEADER_FLAG_WRITTEN);
 
+	write_extent_buffer(cow, root->fs_info->fsid,
+			    (unsigned long)btrfs_header_fsid(cow),
+			    BTRFS_FSID_SIZE);
+
 	WARN_ON(btrfs_header_generation(buf) > trans->transid);
 	ret = btrfs_inc_ref(trans, new_root, buf, cow, NULL);
 	kfree(new_root);
@@ -274,6 +278,10 @@ int noinline __btrfs_cow_block(struct btrfs_trans_handle *trans,
 	btrfs_set_header_owner(cow, root->root_key.objectid);
 	btrfs_clear_header_flag(cow, BTRFS_HEADER_FLAG_WRITTEN);
 
+	write_extent_buffer(cow, root->fs_info->fsid,
+			    (unsigned long)btrfs_header_fsid(cow),
+			    BTRFS_FSID_SIZE);
+
 	WARN_ON(btrfs_header_generation(buf) > trans->transid);
 	if (btrfs_header_generation(buf) != trans->transid) {
 		u32 nr_extents;

commit f3465ca44e2a51fd647c167045768a8ab5a96603
Author: Josef Bacik <jbacik@redhat.com>
Date:   Wed Nov 12 14:19:50 2008 -0500

    Btrfs: batch extent inserts/updates/deletions on the extent root
    
    While profiling the allocator I noticed a good amount of time was being spent in
    finish_current_insert and del_pending_extents, and as the filesystem filled up
    more and more time was being spent in those functions.  This patch aims to try
    and reduce that problem.  This happens two ways
    
    1) track if we tried to delete an extent that we are going to update or insert.
    Once we get into finish_current_insert we discard any of the extents that were
    marked for deletion.  This saves us from doing unnecessary work almost every
    time finish_current_insert runs.
    
    2) Batch insertion/updates/deletions.  Instead of doing a btrfs_search_slot for
    each individual extent and doing the needed operation, we instead keep the leaf
    around and see if there is anything else we can do on that leaf.  On the insert
    case I introduced a btrfs_insert_some_items, which will take an array of keys
    with an array of data_sizes and try and squeeze in as many of those keys as
    possible, and then return how many keys it was able to insert.  In the update
    case we search for an extent ref, update the ref and then loop through the leaf
    to see if any of the other refs we are looking to update are on that leaf, and
    then once we are done we release the path and search for the next ref we need to
    update.  And finally for the deletion we try and delete the extent+ref in pairs,
    so we will try to find extent+ref pairs next to the extent we are trying to free
    and free them in bulk if possible.
    
    This along with the other cluster fix that Chris pushed out a bit ago helps make
    the allocator preform more uniformly as it fills up the disk.  There is still a
    slight drop as we fill up the disk since we start having to stick new blocks in
    odd places which results in more COW's than on a empty fs, but the drop is not
    nearly as severe as it was before.
    
    Signed-off-by: Josef Bacik <jbacik@redhat.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index ac61c50a3311..8bb452456d90 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -431,6 +431,25 @@ static int comp_keys(struct btrfs_disk_key *disk, struct btrfs_key *k2)
 	return 0;
 }
 
+/*
+ * same as comp_keys only with two btrfs_key's
+ */
+static int comp_cpu_keys(struct btrfs_key *k1, struct btrfs_key *k2)
+{
+	if (k1->objectid > k2->objectid)
+		return 1;
+	if (k1->objectid < k2->objectid)
+		return -1;
+	if (k1->type > k2->type)
+		return 1;
+	if (k1->type < k2->type)
+		return -1;
+	if (k1->offset > k2->offset)
+		return 1;
+	if (k1->offset < k2->offset)
+		return -1;
+	return 0;
+}
 
 /*
  * this is used by the defrag code to go through all the
@@ -2999,6 +3018,157 @@ int btrfs_extend_item(struct btrfs_trans_handle *trans,
 	return ret;
 }
 
+/*
+ * Given a key and some data, insert items into the tree.
+ * This does all the path init required, making room in the tree if needed.
+ * Returns the number of keys that were inserted.
+ */
+int btrfs_insert_some_items(struct btrfs_trans_handle *trans,
+			    struct btrfs_root *root,
+			    struct btrfs_path *path,
+			    struct btrfs_key *cpu_key, u32 *data_size,
+			    int nr)
+{
+	struct extent_buffer *leaf;
+	struct btrfs_item *item;
+	int ret = 0;
+	int slot;
+	int slot_orig;
+	int i;
+	u32 nritems;
+	u32 total_data = 0;
+	u32 total_size = 0;
+	unsigned int data_end;
+	struct btrfs_disk_key disk_key;
+	struct btrfs_key found_key;
+
+	found_key.objectid = 0;
+	nr = min_t(int, nr, BTRFS_NODEPTRS_PER_BLOCK(root));
+
+	for (i = 0; i < nr; i++)
+		total_data += data_size[i];
+
+	total_data = min_t(u32, total_data, BTRFS_LEAF_DATA_SIZE(root));
+	total_size = total_data + (nr * sizeof(struct btrfs_item));
+	ret = btrfs_search_slot(trans, root, cpu_key, path, total_size, 1);
+	if (ret == 0)
+		return -EEXIST;
+	if (ret < 0)
+		goto out;
+
+	slot_orig = path->slots[0];
+	leaf = path->nodes[0];
+
+	nritems = btrfs_header_nritems(leaf);
+	data_end = leaf_data_end(root, leaf);
+
+	if (btrfs_leaf_free_space(root, leaf) < total_size) {
+		for (i = nr; i >= 0; i--) {
+			total_data -= data_size[i];
+			total_size -= data_size[i] + sizeof(struct btrfs_item);
+			if (total_size < btrfs_leaf_free_space(root, leaf))
+				break;
+		}
+		nr = i;
+	}
+
+	slot = path->slots[0];
+	BUG_ON(slot < 0);
+
+	if (slot != nritems) {
+		unsigned int old_data = btrfs_item_end_nr(leaf, slot);
+
+		item = btrfs_item_nr(leaf, slot);
+		btrfs_item_key_to_cpu(leaf, &found_key, slot);
+
+		/* figure out how many keys we can insert in here */
+		total_data = data_size[0];
+		for (i = 1; i < nr; i++) {
+			if (comp_cpu_keys(&found_key, cpu_key + i) <= 0)
+				break;
+			total_data += data_size[i];
+		}
+		nr = i;
+
+		if (old_data < data_end) {
+			btrfs_print_leaf(root, leaf);
+			printk("slot %d old_data %d data_end %d\n",
+			       slot, old_data, data_end);
+			BUG_ON(1);
+		}
+		/*
+		 * item0..itemN ... dataN.offset..dataN.size .. data0.size
+		 */
+		/* first correct the data pointers */
+		WARN_ON(leaf->map_token);
+		for (i = slot; i < nritems; i++) {
+			u32 ioff;
+
+			item = btrfs_item_nr(leaf, i);
+			if (!leaf->map_token) {
+				map_extent_buffer(leaf, (unsigned long)item,
+					sizeof(struct btrfs_item),
+					&leaf->map_token, &leaf->kaddr,
+					&leaf->map_start, &leaf->map_len,
+					KM_USER1);
+			}
+
+			ioff = btrfs_item_offset(leaf, item);
+			btrfs_set_item_offset(leaf, item, ioff - total_data);
+		}
+		if (leaf->map_token) {
+			unmap_extent_buffer(leaf, leaf->map_token, KM_USER1);
+			leaf->map_token = NULL;
+		}
+
+		/* shift the items */
+		memmove_extent_buffer(leaf, btrfs_item_nr_offset(slot + nr),
+			      btrfs_item_nr_offset(slot),
+			      (nritems - slot) * sizeof(struct btrfs_item));
+
+		/* shift the data */
+		memmove_extent_buffer(leaf, btrfs_leaf_data(leaf) +
+			      data_end - total_data, btrfs_leaf_data(leaf) +
+			      data_end, old_data - data_end);
+		data_end = old_data;
+	} else {
+		/*
+		 * this sucks but it has to be done, if we are inserting at
+		 * the end of the leaf only insert 1 of the items, since we
+		 * have no way of knowing whats on the next leaf and we'd have
+		 * to drop our current locks to figure it out
+		 */
+		nr = 1;
+	}
+
+	/* setup the item for the new data */
+	for (i = 0; i < nr; i++) {
+		btrfs_cpu_key_to_disk(&disk_key, cpu_key + i);
+		btrfs_set_item_key(leaf, &disk_key, slot + i);
+		item = btrfs_item_nr(leaf, slot + i);
+		btrfs_set_item_offset(leaf, item, data_end - data_size[i]);
+		data_end -= data_size[i];
+		btrfs_set_item_size(leaf, item, data_size[i]);
+	}
+	btrfs_set_header_nritems(leaf, nritems + nr);
+	btrfs_mark_buffer_dirty(leaf);
+
+	ret = 0;
+	if (slot == 0) {
+		btrfs_cpu_key_to_disk(&disk_key, cpu_key);
+		ret = fixup_low_keys(trans, root, path, &disk_key, 1);
+	}
+
+	if (btrfs_leaf_free_space(root, leaf) < 0) {
+		btrfs_print_leaf(root, leaf);
+		BUG();
+	}
+out:
+	if (!ret)
+		ret = nr;
+	return ret;
+}
+
 /*
  * Given a key and some data, insert items into the tree.
  * This does all the path init required, making room in the tree if needed.

commit 6f3577bdc768e6dae3c4d419e89b5a904f470728
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Nov 13 09:59:36 2008 -0500

    Btrfs: Improve metadata read latencies
    
    This fixes latency problems on metadata reads by making sure they
    don't go through the async submit queue, and by tuning down the amount
    of readahead done during btree searches.
    
    Also, the btrfs bdi congestion function is tuned to ignore the
    number of pending async bios and checksums pending.  There is additional
    code that throttles new async bios now and the congestion function
    doesn't need to worry about it anymore.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index f82f8db02275..ac61c50a3311 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1285,16 +1285,16 @@ static noinline void reada_for_search(struct btrfs_root *root,
 		}
 		search = btrfs_node_blockptr(node, nr);
 		if ((search >= lowest_read && search <= highest_read) ||
-		    (search < lowest_read && lowest_read - search <= 32768) ||
-		    (search > highest_read && search - highest_read <= 32768)) {
+		    (search < lowest_read && lowest_read - search <= 16384) ||
+		    (search > highest_read && search - highest_read <= 16384)) {
 			readahead_tree_block(root, search, blocksize,
 				     btrfs_node_ptr_generation(node, nr));
 			nread += blocksize;
 		}
 		nscan++;
-		if (path->reada < 2 && (nread > (256 * 1024) || nscan > 32))
+		if (path->reada < 2 && (nread > (64 * 1024) || nscan > 32))
 			break;
-		if(nread > (1024 * 1024) || nscan > 128)
+		if(nread > (256 * 1024) || nscan > 128)
 			break;
 
 		if (search < lowest_read)

commit 2517920135b0d29e70453e5b03d70d7b94207df3
Author: Josef Bacik <jbacik@redhat.com>
Date:   Wed Oct 29 14:49:05 2008 -0400

    Btrfs: nuke fs wide allocation mutex V2
    
    This patch removes the giant fs_info->alloc_mutex and replaces it with a bunch
    of little locks.
    
    There is now a pinned_mutex, which is used when messing with the pinned_extents
    extent io tree, and the extent_ins_mutex which is used with the pending_del and
    extent_ins extent io trees.
    
    The locking for the extent tree stuff was inspired by a patch that Yan Zheng
    wrote to fix a race condition, I cleaned it up some and changed the locking
    around a little bit, but the idea remains the same.  Basically instead of
    holding the extent_ins_mutex throughout the processing of an extent on the
    extent_ins or pending_del trees, we just hold it while we're searching and when
    we clear the bits on those trees, and lock the extent for the duration of the
    operations on the extent.
    
    Also to keep from getting hung up waiting to lock an extent, I've added a
    try_lock_extent so if we cannot lock the extent, move on to the next one in the
    tree and we'll come back to that one.  I have tested this heavily and it does
    not appear to break anything.  This has to be applied on top of my
    find_free_extent redo patch.
    
    I tested this patch on top of Yan's space reblancing code and it worked fine.
    The only thing that has changed since the last version is I pulled out all my
    debugging stuff, apparently I forgot to run guilt refresh before I sent the
    last patch out.  Thank you,
    
    Signed-off-by: Josef Bacik <jbacik@redhat.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 73899d0f9d8f..f82f8db02275 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1387,8 +1387,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 	lowest_level = p->lowest_level;
 	WARN_ON(lowest_level && ins_len > 0);
 	WARN_ON(p->nodes[0] != NULL);
-	WARN_ON(cow && root == root->fs_info->extent_root &&
-		!mutex_is_locked(&root->fs_info->alloc_mutex));
+
 	if (ins_len < 0)
 		lowest_unlock = 2;
 

commit f82d02d9d8222183b7945e893111a6d1bf67ae4a
Author: Yan Zheng <zheng.yan@oracle.com>
Date:   Wed Oct 29 14:49:05 2008 -0400

    Btrfs: Improve space balancing code
    
    This patch improves the space balancing code to keep more sharing
    of tree blocks. The only case that breaks sharing of tree blocks is
    data extents get fragmented during balancing. The main changes in
    this patch are:
    
    Add a 'drop sub-tree' function. This solves the problem in old code
    that BTRFS_HEADER_FLAG_WRITTEN check breaks sharing of tree block.
    
    Remove relocation mapping tree. Relocation mappings are stored in
    struct btrfs_ref_path and updated dynamically during walking up/down
    the reference path. This reduces CPU usage and simplifies code.
    
    This patch also fixes a bug. Root items for reloc trees should be
    updated in btrfs_free_reloc_root.
    
    Signed-off-by: Yan Zheng <zheng.yan@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 9caeb377de63..73899d0f9d8f 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -287,7 +287,7 @@ int noinline __btrfs_cow_block(struct btrfs_trans_handle *trans,
 		/*
 		 * There are only two places that can drop reference to
 		 * tree blocks owned by living reloc trees, one is here,
-		 * the other place is btrfs_merge_path. In both places,
+		 * the other place is btrfs_drop_subtree. In both places,
 		 * we check reference count while tree block is locked.
 		 * Furthermore, if reference count is one, it won't get
 		 * increased by someone else.
@@ -312,9 +312,6 @@ int noinline __btrfs_cow_block(struct btrfs_trans_handle *trans,
 	}
 
 	if (root->root_key.objectid == BTRFS_TREE_RELOC_OBJECTID) {
-		ret = btrfs_add_reloc_mapping(root, buf->start,
-					      buf->len, cow->start);
-		BUG_ON(ret);
 		ret = btrfs_reloc_tree_cache_ref(trans, root, cow, buf->start);
 		WARN_ON(ret);
 	}
@@ -1627,61 +1624,57 @@ int btrfs_merge_path(struct btrfs_trans_handle *trans,
 		btrfs_node_key_to_cpu(eb, &key, slot);
 		key_match = !memcmp(&key, &node_keys[level - 1], sizeof(key));
 
+		if (generation == trans->transid) {
+			eb = read_tree_block(root, bytenr, blocksize,
+					     generation);
+			btrfs_tree_lock(eb);
+		}
+
 		/*
 		 * if node keys match and node pointer hasn't been modified
 		 * in the running transaction, we can merge the path. for
-		 * reloc trees, the node pointer check is skipped, this is
-		 * because the reloc trees are fully controlled by the space
-		 * balance code, no one else can modify them.
+		 * blocks owened by reloc trees, the node pointer check is
+		 * skipped, this is because these blocks are fully controlled
+		 * by the space balance code, no one else can modify them.
 		 */
 		if (!nodes[level - 1] || !key_match ||
 		    (generation == trans->transid &&
-		     root->root_key.objectid != BTRFS_TREE_RELOC_OBJECTID)) {
-next_level:
-			if (level == 1 || level == lowest_level + 1)
+		     btrfs_header_owner(eb) != BTRFS_TREE_RELOC_OBJECTID)) {
+			if (level == 1 || level == lowest_level + 1) {
+				if (generation == trans->transid) {
+					btrfs_tree_unlock(eb);
+					free_extent_buffer(eb);
+				}
 				break;
+			}
 
-			eb = read_tree_block(root, bytenr, blocksize,
-					     generation);
-			btrfs_tree_lock(eb);
+			if (generation != trans->transid) {
+				eb = read_tree_block(root, bytenr, blocksize,
+						generation);
+				btrfs_tree_lock(eb);
+			}
 
 			ret = btrfs_cow_block(trans, root, eb, parent, slot,
 					      &eb, 0);
 			BUG_ON(ret);
 
+			if (root->root_key.objectid ==
+			    BTRFS_TREE_RELOC_OBJECTID) {
+				if (!nodes[level - 1]) {
+					nodes[level - 1] = eb->start;
+					memcpy(&node_keys[level - 1], &key,
+					       sizeof(node_keys[0]));
+				} else {
+					WARN_ON(1);
+				}
+			}
+
 			btrfs_tree_unlock(parent);
 			free_extent_buffer(parent);
 			parent = eb;
 			continue;
 		}
 
-		if (generation == trans->transid) {
-			u32 refs;
-			BUG_ON(btrfs_header_owner(eb) !=
-			       BTRFS_TREE_RELOC_OBJECTID);
-			/*
-			 * lock the block to keep __btrfs_cow_block from
-			 * changing the reference count.
-			 */
-			eb = read_tree_block(root, bytenr, blocksize,
-					     generation);
-			btrfs_tree_lock(eb);
-
-			ret = btrfs_lookup_extent_ref(trans, root, bytenr,
-						      blocksize, &refs);
-			BUG_ON(ret);
-			/*
-			 * if replace block whose reference count is one,
-			 * we have to "drop the subtree". so skip it for
-			 * simplicity
-			 */
-			if (refs == 1) {
-				btrfs_tree_unlock(eb);
-				free_extent_buffer(eb);
-				goto next_level;
-			}
-		}
-
 		btrfs_set_node_blockptr(parent, slot, nodes[level - 1]);
 		btrfs_set_node_ptr_generation(parent, slot, trans->transid);
 		btrfs_mark_buffer_dirty(parent);
@@ -1693,16 +1686,24 @@ int btrfs_merge_path(struct btrfs_trans_handle *trans,
 					btrfs_header_generation(parent),
 					level - 1);
 		BUG_ON(ret);
-		ret = btrfs_free_extent(trans, root, bytenr,
-					blocksize, parent->start,
-					btrfs_header_owner(parent),
-					btrfs_header_generation(parent),
-					level - 1, 1);
-		BUG_ON(ret);
 
+		/*
+		 * If the block was created in the running transaction,
+		 * it's possible this is the last reference to it, so we
+		 * should drop the subtree.
+		 */
 		if (generation == trans->transid) {
+			ret = btrfs_drop_subtree(trans, root, eb, parent);
+			BUG_ON(ret);
 			btrfs_tree_unlock(eb);
 			free_extent_buffer(eb);
+		} else {
+			ret = btrfs_free_extent(trans, root, bytenr,
+					blocksize, parent->start,
+					btrfs_header_owner(parent),
+					btrfs_header_generation(parent),
+					level - 1, 1);
+			BUG_ON(ret);
 		}
 		break;
 	}

commit 3bb1a1bc42f2ae9582c28adf620484efcd4da38d
Author: Yan Zheng <zheng.yan@oracle.com>
Date:   Thu Oct 9 11:46:24 2008 -0400

    Btrfs: Remove offset field from struct btrfs_extent_ref
    
    The offset field in struct btrfs_extent_ref records the position
    inside file that file extent is referenced by. In the new back
    reference system, tree leaves holding references to file extent
    are recorded explicitly. We can scan these tree leaves very quickly, so the
    offset field is not required.
    
    This patch also makes the back reference system check the objectid
    when extents are in deleting.
    
    Signed-off-by: Yan Zheng <zheng.yan@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 2eab4643dcbc..9caeb377de63 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -254,8 +254,7 @@ int noinline __btrfs_cow_block(struct btrfs_trans_handle *trans,
 
 		ret = btrfs_alloc_reserved_extent(trans, root, parent_start,
 						  root->root_key.objectid,
-						  trans->transid, level, 0,
-						  &ins);
+						  trans->transid, level, &ins);
 		BUG_ON(ret);
 		cow = btrfs_init_new_buffer(trans, root, prealloc_dest,
 					    buf->len);
@@ -333,7 +332,7 @@ int noinline __btrfs_cow_block(struct btrfs_trans_handle *trans,
 					  buf->len, buf->start,
 					  root->root_key.objectid,
 					  btrfs_header_generation(buf),
-					  0, 0, 1);
+					  level, 1);
 		}
 		free_extent_buffer(buf);
 		add_root_to_dirty_list(root);
@@ -347,7 +346,7 @@ int noinline __btrfs_cow_block(struct btrfs_trans_handle *trans,
 		WARN_ON(btrfs_header_generation(parent) != trans->transid);
 		btrfs_free_extent(trans, root, buf->start, buf->len,
 				  parent_start, btrfs_header_owner(parent),
-				  btrfs_header_generation(parent), 0, 0, 1);
+				  btrfs_header_generation(parent), level, 1);
 	}
 	if (unlock_orig)
 		btrfs_tree_unlock(buf);
@@ -927,7 +926,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		ret = btrfs_update_extent_ref(trans, root, child->start,
 					      mid->start, child->start,
 					      root->root_key.objectid,
-					      trans->transid, level - 1, 0);
+					      trans->transid, level - 1);
 		BUG_ON(ret);
 
 		add_root_to_dirty_list(root);
@@ -940,7 +939,8 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		free_extent_buffer(mid);
 		ret = btrfs_free_extent(trans, root, mid->start, mid->len,
 					mid->start, root->root_key.objectid,
-					btrfs_header_generation(mid), 0, 0, 1);
+					btrfs_header_generation(mid),
+					level, 1);
 		/* once for the root ptr */
 		free_extent_buffer(mid);
 		return ret;
@@ -1006,7 +1006,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 			wret = btrfs_free_extent(trans, root, bytenr,
 						 blocksize, parent->start,
 						 btrfs_header_owner(parent),
-						 generation, 0, 0, 1);
+						 generation, level, 1);
 			if (wret)
 				ret = wret;
 		} else {
@@ -1055,7 +1055,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		wret = btrfs_free_extent(trans, root, bytenr, blocksize,
 					 parent->start,
 					 btrfs_header_owner(parent),
-					 root_gen, 0, 0, 1);
+					 root_gen, level, 1);
 		if (wret)
 			ret = wret;
 	} else {
@@ -1691,13 +1691,13 @@ int btrfs_merge_path(struct btrfs_trans_handle *trans,
 					blocksize, parent->start,
 					btrfs_header_owner(parent),
 					btrfs_header_generation(parent),
-					level - 1, 0);
+					level - 1);
 		BUG_ON(ret);
 		ret = btrfs_free_extent(trans, root, bytenr,
 					blocksize, parent->start,
 					btrfs_header_owner(parent),
 					btrfs_header_generation(parent),
-					level - 1, 0, 1);
+					level - 1, 1);
 		BUG_ON(ret);
 
 		if (generation == trans->transid) {
@@ -1973,7 +1973,7 @@ static int noinline insert_new_root(struct btrfs_trans_handle *trans,
 	ret = btrfs_update_extent_ref(trans, root, lower->start,
 				      lower->start, c->start,
 				      root->root_key.objectid,
-				      trans->transid, level - 1, 0);
+				      trans->transid, level - 1);
 	BUG_ON(ret);
 
 	/* the super has an extra ref to root->node */
@@ -3213,7 +3213,7 @@ noinline int btrfs_del_leaf(struct btrfs_trans_handle *trans,
 				btrfs_level_size(root, 0),
 				path->nodes[1]->start,
 				btrfs_header_owner(path->nodes[1]),
-				root_gen, 0, 0, 1);
+				root_gen, 0, 1);
 	return ret;
 }
 /*

commit 323ac95bce442bbde514e3ce57e840402f80d909
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Oct 1 19:05:46 2008 -0400

    Btrfs: don't read leaf blocks containing only checksums during truncate
    
    Checksum items take up a significant portion of the metadata for large files.
    It is possible to avoid reading them during truncates by checking the keys in
    the higher level nodes.
    
    If a given leaf is followed by another leaf where the lowest key is a checksum
    item from the same file, we know we can safely delete the leaf without
    reading it.
    
    For a 32GB file on a 6 drive raid0 array, Btrfs needs 8s to delete
    the file with a cold cache.  It is read bound during the run.
    
    With this change, Btrfs is able to delete the file in 0.5s
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index ff3261ff2e19..2eab4643dcbc 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1388,7 +1388,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 	struct btrfs_key prealloc_block;
 
 	lowest_level = p->lowest_level;
-	WARN_ON(lowest_level && ins_len);
+	WARN_ON(lowest_level && ins_len > 0);
 	WARN_ON(p->nodes[0] != NULL);
 	WARN_ON(cow && root == root->fs_info->extent_root &&
 		!mutex_is_locked(&root->fs_info->alloc_mutex));
@@ -3186,6 +3186,36 @@ static int del_ptr(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 	return ret;
 }
 
+/*
+ * a helper function to delete the leaf pointed to by path->slots[1] and
+ * path->nodes[1].  bytenr is the node block pointer, but since the callers
+ * already know it, it is faster to have them pass it down than to
+ * read it out of the node again.
+ *
+ * This deletes the pointer in path->nodes[1] and frees the leaf
+ * block extent.  zero is returned if it all worked out, < 0 otherwise.
+ *
+ * The path must have already been setup for deleting the leaf, including
+ * all the proper balancing.  path->nodes[1] must be locked.
+ */
+noinline int btrfs_del_leaf(struct btrfs_trans_handle *trans,
+			    struct btrfs_root *root,
+			    struct btrfs_path *path, u64 bytenr)
+{
+	int ret;
+	u64 root_gen = btrfs_header_generation(path->nodes[1]);
+
+	ret = del_ptr(trans, root, path, 1, path->slots[1]);
+	if (ret)
+		return ret;
+
+	ret = btrfs_free_extent(trans, root, bytenr,
+				btrfs_level_size(root, 0),
+				path->nodes[1]->start,
+				btrfs_header_owner(path->nodes[1]),
+				root_gen, 0, 0, 1);
+	return ret;
+}
 /*
  * delete the item at the leaf level in path.  If that empties
  * the leaf, remove it from the tree
@@ -3251,17 +3281,8 @@ int btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 		if (leaf == root->node) {
 			btrfs_set_header_level(leaf, 0);
 		} else {
-			u64 root_gen = btrfs_header_generation(path->nodes[1]);
-			wret = del_ptr(trans, root, path, 1, path->slots[1]);
-			if (wret)
-				ret = wret;
-			wret = btrfs_free_extent(trans, root,
-					 leaf->start, leaf->len,
-					 path->nodes[1]->start,
-					 btrfs_header_owner(path->nodes[1]),
-					 root_gen, 0, 0, 1);
-			if (wret)
-				ret = wret;
+			ret = btrfs_del_leaf(trans, root, path, leaf->start);
+			BUG_ON(ret);
 		}
 	} else {
 		int used = leaf_space_used(leaf, 0, nritems);
@@ -3296,24 +3317,10 @@ int btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 			}
 
 			if (btrfs_header_nritems(leaf) == 0) {
-				u64 root_gen;
-				u64 bytenr = leaf->start;
-				u32 blocksize = leaf->len;
-
-				root_gen = btrfs_header_generation(
-							   path->nodes[1]);
-
-				wret = del_ptr(trans, root, path, 1, slot);
-				if (wret)
-					ret = wret;
-
+				path->slots[1] = slot;
+				ret = btrfs_del_leaf(trans, root, path, leaf->start);
+				BUG_ON(ret);
 				free_extent_buffer(leaf);
-				wret = btrfs_free_extent(trans, root, bytenr,
-					     blocksize, path->nodes[1]->start,
-					     btrfs_header_owner(path->nodes[1]),
-					     root_gen, 0, 0, 1);
-				if (wret)
-					ret = wret;
 			} else {
 				/* if we're still in the path, make sure
 				 * we're dirty.  Otherwise, one of the
@@ -3418,8 +3425,8 @@ int btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,
 		level = btrfs_header_level(cur);
 		sret = bin_search(cur, min_key, level, &slot);
 
-		/* at level = 0, we're done, setup the path and exit */
-		if (level == 0) {
+		/* at the lowest level, we're done, setup the path and exit */
+		if (level == path->lowest_level) {
 			if (slot >= nritems)
 				goto find_next_key;
 			ret = 0;

commit d352ac68148b69937d39ca5d48bcc4478e118dbf
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Sep 29 15:18:18 2008 -0400

    Btrfs: add and improve comments
    
    This improves the comments at the top of many functions.  It didn't
    dive into the guts of functions because I was trying to
    avoid merging problems with the new allocator and back reference work.
    
    extent-tree.c and volumes.c were both skipped, and there is definitely
    more work todo in cleaning and commenting the code.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 50e81f43e6d4..ff3261ff2e19 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1,5 +1,5 @@
 /*
- * Copyright (C) 2007 Oracle.  All rights reserved.
+ * Copyright (C) 2007,2008 Oracle.  All rights reserved.
  *
  * This program is free software; you can redistribute it and/or
  * modify it under the terms of the GNU General Public
@@ -54,12 +54,19 @@ struct btrfs_path *btrfs_alloc_path(void)
 	return path;
 }
 
+/* this also releases the path */
 void btrfs_free_path(struct btrfs_path *p)
 {
 	btrfs_release_path(NULL, p);
 	kmem_cache_free(btrfs_path_cachep, p);
 }
 
+/*
+ * path release drops references on the extent buffers in the path
+ * and it drops any locks held by this path
+ *
+ * It is safe to call this on paths that no locks or extent buffers held.
+ */
 void noinline btrfs_release_path(struct btrfs_root *root, struct btrfs_path *p)
 {
 	int i;
@@ -77,6 +84,16 @@ void noinline btrfs_release_path(struct btrfs_root *root, struct btrfs_path *p)
 	}
 }
 
+/*
+ * safely gets a reference on the root node of a tree.  A lock
+ * is not taken, so a concurrent writer may put a different node
+ * at the root of the tree.  See btrfs_lock_root_node for the
+ * looping required.
+ *
+ * The extent buffer returned by this has a reference taken, so
+ * it won't disappear.  It may stop being the root of the tree
+ * at any time because there are no locks held.
+ */
 struct extent_buffer *btrfs_root_node(struct btrfs_root *root)
 {
 	struct extent_buffer *eb;
@@ -87,6 +104,10 @@ struct extent_buffer *btrfs_root_node(struct btrfs_root *root)
 	return eb;
 }
 
+/* loop around taking references on and locking the root node of the
+ * tree until you end up with a lock on the root.  A locked buffer
+ * is returned, with a reference held.
+ */
 struct extent_buffer *btrfs_lock_root_node(struct btrfs_root *root)
 {
 	struct extent_buffer *eb;
@@ -108,6 +129,10 @@ struct extent_buffer *btrfs_lock_root_node(struct btrfs_root *root)
 	return eb;
 }
 
+/* cowonly root (everything not a reference counted cow subvolume), just get
+ * put onto a simple dirty list.  transaction.c walks this to make sure they
+ * get properly updated on disk.
+ */
 static void add_root_to_dirty_list(struct btrfs_root *root)
 {
 	if (root->track_dirty && list_empty(&root->dirty_list)) {
@@ -116,6 +141,11 @@ static void add_root_to_dirty_list(struct btrfs_root *root)
 	}
 }
 
+/*
+ * used by snapshot creation to make a copy of a root for a tree with
+ * a given objectid.  The buffer with the new root node is returned in
+ * cow_ret, and this func returns zero on success or a negative error code.
+ */
 int btrfs_copy_root(struct btrfs_trans_handle *trans,
 		      struct btrfs_root *root,
 		      struct extent_buffer *buf,
@@ -167,6 +197,22 @@ int btrfs_copy_root(struct btrfs_trans_handle *trans,
 	return 0;
 }
 
+/*
+ * does the dirty work in cow of a single block.  The parent block
+ * (if supplied) is updated to point to the new cow copy.  The new
+ * buffer is marked dirty and returned locked.  If you modify the block
+ * it needs to be marked dirty again.
+ *
+ * search_start -- an allocation hint for the new block
+ *
+ * empty_size -- a hint that you plan on doing more cow.  This is the size in bytes
+ * the allocator should try to find free next to the block it returns.  This is
+ * just a hint and may be ignored by the allocator.
+ *
+ * prealloc_dest -- if you have already reserved a destination for the cow,
+ * this uses that block instead of allocating a new one.  btrfs_alloc_reserved_extent
+ * is used to finish the allocation.
+ */
 int noinline __btrfs_cow_block(struct btrfs_trans_handle *trans,
 			     struct btrfs_root *root,
 			     struct extent_buffer *buf,
@@ -311,6 +357,11 @@ int noinline __btrfs_cow_block(struct btrfs_trans_handle *trans,
 	return 0;
 }
 
+/*
+ * cows a single block, see __btrfs_cow_block for the real work.
+ * This version of it has extra checks so that a block isn't cow'd more than
+ * once per transaction, as long as it hasn't been written yet
+ */
 int noinline btrfs_cow_block(struct btrfs_trans_handle *trans,
 		    struct btrfs_root *root, struct extent_buffer *buf,
 		    struct extent_buffer *parent, int parent_slot,
@@ -347,6 +398,10 @@ int noinline btrfs_cow_block(struct btrfs_trans_handle *trans,
 	return ret;
 }
 
+/*
+ * helper function for defrag to decide if two blocks pointed to by a
+ * node are actually close by
+ */
 static int close_blocks(u64 blocknr, u64 other, u32 blocksize)
 {
 	if (blocknr < other && other - (blocknr + blocksize) < 32768)
@@ -381,6 +436,11 @@ static int comp_keys(struct btrfs_disk_key *disk, struct btrfs_key *k2)
 }
 
 
+/*
+ * this is used by the defrag code to go through all the
+ * leaves pointed to by a node and reallocate them so that
+ * disk order is close to key order
+ */
 int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 		       struct btrfs_root *root, struct extent_buffer *parent,
 		       int start_slot, int cache_only, u64 *last_ret,
@@ -521,6 +581,10 @@ static inline unsigned int leaf_data_end(struct btrfs_root *root,
 	return btrfs_item_offset_nr(leaf, nr - 1);
 }
 
+/*
+ * extra debugging checks to make sure all the items in a key are
+ * well formed and in the proper order
+ */
 static int check_node(struct btrfs_root *root, struct btrfs_path *path,
 		      int level)
 {
@@ -561,6 +625,10 @@ static int check_node(struct btrfs_root *root, struct btrfs_path *path,
 	return 0;
 }
 
+/*
+ * extra checking to make sure all the items in a leaf are
+ * well formed and in the proper order
+ */
 static int check_leaf(struct btrfs_root *root, struct btrfs_path *path,
 		      int level)
 {
@@ -782,6 +850,10 @@ static int bin_search(struct extent_buffer *eb, struct btrfs_key *key,
 	return -1;
 }
 
+/* given a node and slot number, this reads the blocks it points to.  The
+ * extent buffer is returned with a reference taken (but unlocked).
+ * NULL is returned on error.
+ */
 static noinline struct extent_buffer *read_node_slot(struct btrfs_root *root,
 				   struct extent_buffer *parent, int slot)
 {
@@ -798,6 +870,11 @@ static noinline struct extent_buffer *read_node_slot(struct btrfs_root *root,
 		       btrfs_node_ptr_generation(parent, slot));
 }
 
+/*
+ * node level balancing, used to make sure nodes are in proper order for
+ * item deletion.  We balance from the top down, so we have to make sure
+ * that a deletion won't leave an node completely empty later on.
+ */
 static noinline int balance_level(struct btrfs_trans_handle *trans,
 			 struct btrfs_root *root,
 			 struct btrfs_path *path, int level)
@@ -1024,7 +1101,10 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 	return ret;
 }
 
-/* returns zero if the push worked, non-zero otherwise */
+/* Node balancing for insertion.  Here we only split or push nodes around
+ * when they are completely full.  This is also done top down, so we
+ * have to be pessimistic.
+ */
 static int noinline push_nodes_for_insert(struct btrfs_trans_handle *trans,
 					  struct btrfs_root *root,
 					  struct btrfs_path *path, int level)
@@ -1150,7 +1230,8 @@ static int noinline push_nodes_for_insert(struct btrfs_trans_handle *trans,
 }
 
 /*
- * readahead one full node of leaves
+ * readahead one full node of leaves, finding things that are close
+ * to the block in 'slot', and triggering ra on them.
  */
 static noinline void reada_for_search(struct btrfs_root *root,
 				      struct btrfs_path *path,
@@ -1226,6 +1307,19 @@ static noinline void reada_for_search(struct btrfs_root *root,
 	}
 }
 
+/*
+ * when we walk down the tree, it is usually safe to unlock the higher layers in
+ * the tree.  The exceptions are when our path goes through slot 0, because operations
+ * on the tree might require changing key pointers higher up in the tree.
+ *
+ * callers might also have set path->keep_locks, which tells this code to
+ * keep the lock if the path points to the last slot in the block.  This is
+ * part of walking through the tree, and selecting the next slot in the higher
+ * block.
+ *
+ * lowest_unlock sets the lowest level in the tree we're allowed to unlock.
+ * so if lowest_unlock is 1, level 0 won't be unlocked
+ */
 static noinline void unlock_up(struct btrfs_path *path, int level,
 			       int lowest_unlock)
 {
@@ -2705,6 +2799,12 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 	return ret;
 }
 
+/*
+ * make the item pointed to by the path smaller.  new_size indicates
+ * how small to make it, and from_end tells us if we just chop bytes
+ * off the end of the item or if we shift the item to chop bytes off
+ * the front.
+ */
 int btrfs_truncate_item(struct btrfs_trans_handle *trans,
 			struct btrfs_root *root,
 			struct btrfs_path *path,
@@ -2818,6 +2918,9 @@ int btrfs_truncate_item(struct btrfs_trans_handle *trans,
 	return ret;
 }
 
+/*
+ * make the item pointed to by the path bigger, data_size is the new size.
+ */
 int btrfs_extend_item(struct btrfs_trans_handle *trans,
 		      struct btrfs_root *root, struct btrfs_path *path,
 		      u32 data_size)
@@ -2897,7 +3000,7 @@ int btrfs_extend_item(struct btrfs_trans_handle *trans,
 }
 
 /*
- * Given a key and some data, insert an item into the tree.
+ * Given a key and some data, insert items into the tree.
  * This does all the path init required, making room in the tree if needed.
  */
 int btrfs_insert_empty_items(struct btrfs_trans_handle *trans,
@@ -3046,9 +3149,8 @@ int btrfs_insert_item(struct btrfs_trans_handle *trans, struct btrfs_root
 /*
  * delete the pointer from a given node.
  *
- * If the delete empties a node, the node is removed from the tree,
- * continuing all the way the root if required.  The root is converted into
- * a leaf if all the nodes are emptied.
+ * the tree should have been previously balanced so the deletion does not
+ * empty a node.
  */
 static int del_ptr(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 		   struct btrfs_path *path, int level, int slot)
@@ -3233,6 +3335,9 @@ int btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,
  * search the tree again to find a leaf with lesser keys
  * returns 0 if it found something or 1 if there are no lesser leaves.
  * returns < 0 on io errors.
+ *
+ * This may release the path, and so you may lose any locks held at the
+ * time you call it.
  */
 int btrfs_prev_leaf(struct btrfs_root *root, struct btrfs_path *path)
 {
@@ -3265,9 +3370,7 @@ int btrfs_prev_leaf(struct btrfs_root *root, struct btrfs_path *path)
 /*
  * A helper function to walk down the tree starting at min_key, and looking
  * for nodes or leaves that are either in cache or have a minimum
- * transaction id.  This is used by the btree defrag code, but could
- * also be used to search for blocks that have changed since a given
- * transaction id.
+ * transaction id.  This is used by the btree defrag code, and tree logging
  *
  * This does not cow, but it does stuff the starting key it finds back
  * into min_key, so you can call btrfs_search_slot with cow=1 on the
@@ -3279,6 +3382,10 @@ int btrfs_prev_leaf(struct btrfs_root *root, struct btrfs_path *path)
  * This honors path->lowest_level to prevent descent past a given level
  * of the tree.
  *
+ * min_trans indicates the oldest transaction that you are interested
+ * in walking through.  Any nodes or leaves older than min_trans are
+ * skipped over (without reading them).
+ *
  * returns zero if something useful was found, < 0 on error and 1 if there
  * was nothing in the tree that matched the search criteria.
  */

commit 1a40e23b95da45051ee4d74374c58ae87a14051c
Author: Zheng Yan <zheng.yan@oracle.com>
Date:   Fri Sep 26 10:09:34 2008 -0400

    Btrfs: update space balancing code
    
    This patch updates the space balancing code to utilize the new
    backref format.  Before, btrfs-vol -b would break any COW links
    on data blocks or metadata.  This was slow and caused the amount
    of space used to explode if a large number of snapshots were present.
    
    The new code can keeps the sharing of all data extents and
    most of the tree blocks.
    
    To maintain the sharing of data extents, the space balance code uses
    a seperate inode hold data extent pointers, then updates the references
    to point to the new location.
    
    To maintain the sharing of tree blocks, the space balance code uses
    reloc trees to relocate tree blocks in reference counted roots.
    There is one reloc tree for each subvol, and all reloc trees share
    same root key objectid. Reloc trees are snapshots of the latest
    committed roots of subvols (root->commit_root).
    
    To relocate a tree block referenced by a subvol, there are two steps.
    COW the block through subvol's reloc tree, then update block pointer in
    the subvol to point to the new block. Since all reloc trees share
    same root key objectid, doing special handing for tree blocks
    owned by them is easy. Once a tree block has been COWed in one
    reloc tree, we can use the resulting new block directly when the
    same block is required to COW again through other reloc trees.
    In this way, relocated tree blocks are shared between reloc trees,
    so they are also shared between subvols.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index f9cd40967d04..50e81f43e6d4 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -179,7 +179,6 @@ int noinline __btrfs_cow_block(struct btrfs_trans_handle *trans,
 	struct extent_buffer *cow;
 	u32 nritems;
 	int ret = 0;
-	int different_trans = 0;
 	int level;
 	int unlock_orig = 0;
 
@@ -233,13 +232,33 @@ int noinline __btrfs_cow_block(struct btrfs_trans_handle *trans,
 	WARN_ON(btrfs_header_generation(buf) > trans->transid);
 	if (btrfs_header_generation(buf) != trans->transid) {
 		u32 nr_extents;
-		different_trans = 1;
 		ret = btrfs_inc_ref(trans, root, buf, cow, &nr_extents);
 		if (ret)
 			return ret;
 
 		ret = btrfs_cache_ref(trans, root, buf, nr_extents);
 		WARN_ON(ret);
+	} else if (btrfs_header_owner(buf) == BTRFS_TREE_RELOC_OBJECTID) {
+		/*
+		 * There are only two places that can drop reference to
+		 * tree blocks owned by living reloc trees, one is here,
+		 * the other place is btrfs_merge_path. In both places,
+		 * we check reference count while tree block is locked.
+		 * Furthermore, if reference count is one, it won't get
+		 * increased by someone else.
+		 */
+		u32 refs;
+		ret = btrfs_lookup_extent_ref(trans, root, buf->start,
+					      buf->len, &refs);
+		BUG_ON(ret);
+		if (refs == 1) {
+			ret = btrfs_update_ref(trans, root, buf, cow,
+					       0, nritems);
+			clean_tree_block(trans, root, buf);
+		} else {
+			ret = btrfs_inc_ref(trans, root, buf, cow, NULL);
+		}
+		BUG_ON(ret);
 	} else {
 		ret = btrfs_update_ref(trans, root, buf, cow, 0, nritems);
 		if (ret)
@@ -247,6 +266,14 @@ int noinline __btrfs_cow_block(struct btrfs_trans_handle *trans,
 		clean_tree_block(trans, root, buf);
 	}
 
+	if (root->root_key.objectid == BTRFS_TREE_RELOC_OBJECTID) {
+		ret = btrfs_add_reloc_mapping(root, buf->start,
+					      buf->len, cow->start);
+		BUG_ON(ret);
+		ret = btrfs_reloc_tree_cache_ref(trans, root, cow, buf->start);
+		WARN_ON(ret);
+	}
+
 	if (buf == root->node) {
 		WARN_ON(parent && parent != buf);
 
@@ -1466,6 +1493,130 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 	return ret;
 }
 
+int btrfs_merge_path(struct btrfs_trans_handle *trans,
+		     struct btrfs_root *root,
+		     struct btrfs_key *node_keys,
+		     u64 *nodes, int lowest_level)
+{
+	struct extent_buffer *eb;
+	struct extent_buffer *parent;
+	struct btrfs_key key;
+	u64 bytenr;
+	u64 generation;
+	u32 blocksize;
+	int level;
+	int slot;
+	int key_match;
+	int ret;
+
+	eb = btrfs_lock_root_node(root);
+	ret = btrfs_cow_block(trans, root, eb, NULL, 0, &eb, 0);
+	BUG_ON(ret);
+
+	parent = eb;
+	while (1) {
+		level = btrfs_header_level(parent);
+		if (level == 0 || level <= lowest_level)
+			break;
+
+		ret = bin_search(parent, &node_keys[lowest_level], level,
+				 &slot);
+		if (ret && slot > 0)
+			slot--;
+
+		bytenr = btrfs_node_blockptr(parent, slot);
+		if (nodes[level - 1] == bytenr)
+			break;
+
+		blocksize = btrfs_level_size(root, level - 1);
+		generation = btrfs_node_ptr_generation(parent, slot);
+		btrfs_node_key_to_cpu(eb, &key, slot);
+		key_match = !memcmp(&key, &node_keys[level - 1], sizeof(key));
+
+		/*
+		 * if node keys match and node pointer hasn't been modified
+		 * in the running transaction, we can merge the path. for
+		 * reloc trees, the node pointer check is skipped, this is
+		 * because the reloc trees are fully controlled by the space
+		 * balance code, no one else can modify them.
+		 */
+		if (!nodes[level - 1] || !key_match ||
+		    (generation == trans->transid &&
+		     root->root_key.objectid != BTRFS_TREE_RELOC_OBJECTID)) {
+next_level:
+			if (level == 1 || level == lowest_level + 1)
+				break;
+
+			eb = read_tree_block(root, bytenr, blocksize,
+					     generation);
+			btrfs_tree_lock(eb);
+
+			ret = btrfs_cow_block(trans, root, eb, parent, slot,
+					      &eb, 0);
+			BUG_ON(ret);
+
+			btrfs_tree_unlock(parent);
+			free_extent_buffer(parent);
+			parent = eb;
+			continue;
+		}
+
+		if (generation == trans->transid) {
+			u32 refs;
+			BUG_ON(btrfs_header_owner(eb) !=
+			       BTRFS_TREE_RELOC_OBJECTID);
+			/*
+			 * lock the block to keep __btrfs_cow_block from
+			 * changing the reference count.
+			 */
+			eb = read_tree_block(root, bytenr, blocksize,
+					     generation);
+			btrfs_tree_lock(eb);
+
+			ret = btrfs_lookup_extent_ref(trans, root, bytenr,
+						      blocksize, &refs);
+			BUG_ON(ret);
+			/*
+			 * if replace block whose reference count is one,
+			 * we have to "drop the subtree". so skip it for
+			 * simplicity
+			 */
+			if (refs == 1) {
+				btrfs_tree_unlock(eb);
+				free_extent_buffer(eb);
+				goto next_level;
+			}
+		}
+
+		btrfs_set_node_blockptr(parent, slot, nodes[level - 1]);
+		btrfs_set_node_ptr_generation(parent, slot, trans->transid);
+		btrfs_mark_buffer_dirty(parent);
+
+		ret = btrfs_inc_extent_ref(trans, root,
+					nodes[level - 1],
+					blocksize, parent->start,
+					btrfs_header_owner(parent),
+					btrfs_header_generation(parent),
+					level - 1, 0);
+		BUG_ON(ret);
+		ret = btrfs_free_extent(trans, root, bytenr,
+					blocksize, parent->start,
+					btrfs_header_owner(parent),
+					btrfs_header_generation(parent),
+					level - 1, 0, 1);
+		BUG_ON(ret);
+
+		if (generation == trans->transid) {
+			btrfs_tree_unlock(eb);
+			free_extent_buffer(eb);
+		}
+		break;
+	}
+	btrfs_tree_unlock(parent);
+	free_extent_buffer(parent);
+	return 0;
+}
+
 /*
  * adjust the pointers going up the tree, starting at level
  * making sure the right key of each node is points to 'key'.

commit 5b21f2ed3f2947b5195b65c9fdbdd9e52904cc03
Author: Zheng Yan <zheng.yan@oracle.com>
Date:   Fri Sep 26 10:05:38 2008 -0400

    Btrfs: extent_map and data=ordered fixes for space balancing
    
    * Add an EXTENT_BOUNDARY state bit to keep the writepage code
    from merging data extents that are in the process of being
    relocated.  This allows us to do accounting for them properly.
    
    * The balancing code relocates data extents indepdent of the underlying
    inode.  The extent_map code was modified to properly account for
    things moving around (invalidating extent_map caches in the inode).
    
    * Don't take the drop_mutex in the create_subvol ioctl.  It isn't
    required.
    
    * Fix walking of the ordered extent list to avoid races with sys_unlink
    
    * Change the lock ordering rules.  Transaction start goes outside
    the drop_mutex.  This allows btrfs_commit_transaction to directly
    drop the relocation trees.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 50aea8cb653a..f9cd40967d04 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -290,7 +290,6 @@ int noinline btrfs_cow_block(struct btrfs_trans_handle *trans,
 		    struct extent_buffer **cow_ret, u64 prealloc_dest)
 {
 	u64 search_start;
-	u64 header_trans;
 	int ret;
 
 	if (trans->transaction != root->fs_info->running_transaction) {
@@ -304,9 +303,9 @@ int noinline btrfs_cow_block(struct btrfs_trans_handle *trans,
 		WARN_ON(1);
 	}
 
-	header_trans = btrfs_header_generation(buf);
 	spin_lock(&root->fs_info->hash_lock);
-	if (header_trans == trans->transid &&
+	if (btrfs_header_generation(buf) == trans->transid &&
+	    btrfs_header_owner(buf) == root->root_key.objectid &&
 	    !btrfs_header_flag(buf, BTRFS_HEADER_FLAG_WRITTEN)) {
 		*cow_ret = buf;
 		spin_unlock(&root->fs_info->hash_lock);
@@ -1300,6 +1299,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 			/* is a cow on this block not required */
 			spin_lock(&root->fs_info->hash_lock);
 			if (btrfs_header_generation(b) == trans->transid &&
+			    btrfs_header_owner(b) == root->root_key.objectid &&
 			    !btrfs_header_flag(b, BTRFS_HEADER_FLAG_WRITTEN)) {
 				spin_unlock(&root->fs_info->hash_lock);
 				goto cow_done;
@@ -1396,7 +1396,8 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 
 			/* this is only true while dropping a snapshot */
 			if (level == lowest_level) {
-				break;
+				ret = 0;
+				goto done;
 			}
 
 			blocknr = btrfs_node_blockptr(b, slot);

commit 31840ae1a6b433ca0e6a8d341756ff478bbf959e
Author: Zheng Yan <zheng.yan@oracle.com>
Date:   Tue Sep 23 13:14:14 2008 -0400

    Btrfs: Full back reference support
    
    This patch makes the back reference system to explicit record the
    location of parent node for all types of extents. The location of
    parent node is placed into the offset field of backref key. Every
    time a tree block is balanced, the back references for the affected
    lower level extents are updated.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 6f467901246f..50aea8cb653a 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -125,7 +125,6 @@ int btrfs_copy_root(struct btrfs_trans_handle *trans,
 	u32 nritems;
 	int ret = 0;
 	int level;
-	struct btrfs_key first_key;
 	struct btrfs_root *new_root;
 
 	new_root = kmalloc(sizeof(*new_root), GFP_NOFS);
@@ -141,18 +140,10 @@ int btrfs_copy_root(struct btrfs_trans_handle *trans,
 
 	level = btrfs_header_level(buf);
 	nritems = btrfs_header_nritems(buf);
-	if (nritems) {
-		if (level == 0)
-			btrfs_item_key_to_cpu(buf, &first_key, 0);
-		else
-			btrfs_node_key_to_cpu(buf, &first_key, 0);
-	} else {
-		first_key.objectid = 0;
-	}
-	cow = btrfs_alloc_free_block(trans, new_root, buf->len,
-				       new_root_objectid,
-				       trans->transid, first_key.objectid,
-				       level, buf->start, 0);
+
+	cow = btrfs_alloc_free_block(trans, new_root, buf->len, 0,
+				     new_root_objectid, trans->transid,
+				     level, buf->start, 0);
 	if (IS_ERR(cow)) {
 		kfree(new_root);
 		return PTR_ERR(cow);
@@ -165,7 +156,7 @@ int btrfs_copy_root(struct btrfs_trans_handle *trans,
 	btrfs_clear_header_flag(cow, BTRFS_HEADER_FLAG_WRITTEN);
 
 	WARN_ON(btrfs_header_generation(buf) > trans->transid);
-	ret = btrfs_inc_ref(trans, new_root, buf, 0);
+	ret = btrfs_inc_ref(trans, new_root, buf, cow, NULL);
 	kfree(new_root);
 
 	if (ret)
@@ -184,39 +175,31 @@ int noinline __btrfs_cow_block(struct btrfs_trans_handle *trans,
 			     u64 search_start, u64 empty_size,
 			     u64 prealloc_dest)
 {
-	u64 root_gen;
+	u64 parent_start;
 	struct extent_buffer *cow;
 	u32 nritems;
 	int ret = 0;
 	int different_trans = 0;
 	int level;
 	int unlock_orig = 0;
-	struct btrfs_key first_key;
 
 	if (*cow_ret == buf)
 		unlock_orig = 1;
 
 	WARN_ON(!btrfs_tree_locked(buf));
 
-	if (root->ref_cows) {
-		root_gen = trans->transid;
-	} else {
-		root_gen = 0;
-	}
+	if (parent)
+		parent_start = parent->start;
+	else
+		parent_start = 0;
+
 	WARN_ON(root->ref_cows && trans->transid !=
 		root->fs_info->running_transaction->transid);
 	WARN_ON(root->ref_cows && trans->transid != root->last_trans);
 
 	level = btrfs_header_level(buf);
 	nritems = btrfs_header_nritems(buf);
-	if (nritems) {
-		if (level == 0)
-			btrfs_item_key_to_cpu(buf, &first_key, 0);
-		else
-			btrfs_node_key_to_cpu(buf, &first_key, 0);
-	} else {
-		first_key.objectid = 0;
-	}
+
 	if (prealloc_dest) {
 		struct btrfs_key ins;
 
@@ -224,19 +207,19 @@ int noinline __btrfs_cow_block(struct btrfs_trans_handle *trans,
 		ins.offset = buf->len;
 		ins.type = BTRFS_EXTENT_ITEM_KEY;
 
-		ret = btrfs_alloc_reserved_extent(trans, root,
+		ret = btrfs_alloc_reserved_extent(trans, root, parent_start,
 						  root->root_key.objectid,
-						  root_gen, level,
-						  first_key.objectid,
+						  trans->transid, level, 0,
 						  &ins);
 		BUG_ON(ret);
 		cow = btrfs_init_new_buffer(trans, root, prealloc_dest,
 					    buf->len);
 	} else {
 		cow = btrfs_alloc_free_block(trans, root, buf->len,
+					     parent_start,
 					     root->root_key.objectid,
-					     root_gen, first_key.objectid,
-					     level, search_start, empty_size);
+					     trans->transid, level,
+					     search_start, empty_size);
 	}
 	if (IS_ERR(cow))
 		return PTR_ERR(cow);
@@ -249,17 +232,23 @@ int noinline __btrfs_cow_block(struct btrfs_trans_handle *trans,
 
 	WARN_ON(btrfs_header_generation(buf) > trans->transid);
 	if (btrfs_header_generation(buf) != trans->transid) {
+		u32 nr_extents;
 		different_trans = 1;
-		ret = btrfs_inc_ref(trans, root, buf, 1);
+		ret = btrfs_inc_ref(trans, root, buf, cow, &nr_extents);
 		if (ret)
 			return ret;
+
+		ret = btrfs_cache_ref(trans, root, buf, nr_extents);
+		WARN_ON(ret);
 	} else {
+		ret = btrfs_update_ref(trans, root, buf, cow, 0, nritems);
+		if (ret)
+			return ret;
 		clean_tree_block(trans, root, buf);
 	}
 
 	if (buf == root->node) {
 		WARN_ON(parent && parent != buf);
-		root_gen = btrfs_header_generation(buf);
 
 		spin_lock(&root->node_lock);
 		root->node = cow;
@@ -268,13 +257,14 @@ int noinline __btrfs_cow_block(struct btrfs_trans_handle *trans,
 
 		if (buf != root->commit_root) {
 			btrfs_free_extent(trans, root, buf->start,
-					  buf->len, root->root_key.objectid,
-					  root_gen, 0, 0, 1);
+					  buf->len, buf->start,
+					  root->root_key.objectid,
+					  btrfs_header_generation(buf),
+					  0, 0, 1);
 		}
 		free_extent_buffer(buf);
 		add_root_to_dirty_list(root);
 	} else {
-		root_gen = btrfs_header_generation(parent);
 		btrfs_set_node_blockptr(parent, parent_slot,
 					cow->start);
 		WARN_ON(trans->transid == 0);
@@ -283,8 +273,8 @@ int noinline __btrfs_cow_block(struct btrfs_trans_handle *trans,
 		btrfs_mark_buffer_dirty(parent);
 		WARN_ON(btrfs_header_generation(parent) != trans->transid);
 		btrfs_free_extent(trans, root, buf->start, buf->len,
-				  btrfs_header_owner(parent), root_gen,
-				  0, 0, 1);
+				  parent_start, btrfs_header_owner(parent),
+				  btrfs_header_generation(parent), 0, 0, 1);
 	}
 	if (unlock_orig)
 		btrfs_tree_unlock(buf);
@@ -831,6 +821,12 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		root->node = child;
 		spin_unlock(&root->node_lock);
 
+		ret = btrfs_update_extent_ref(trans, root, child->start,
+					      mid->start, child->start,
+					      root->root_key.objectid,
+					      trans->transid, level - 1, 0);
+		BUG_ON(ret);
+
 		add_root_to_dirty_list(root);
 		btrfs_tree_unlock(child);
 		path->locks[level] = 0;
@@ -840,7 +836,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		/* once for the path */
 		free_extent_buffer(mid);
 		ret = btrfs_free_extent(trans, root, mid->start, mid->len,
-					root->root_key.objectid,
+					mid->start, root->root_key.objectid,
 					btrfs_header_generation(mid), 0, 0, 1);
 		/* once for the root ptr */
 		free_extent_buffer(mid);
@@ -905,7 +901,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 			if (wret)
 				ret = wret;
 			wret = btrfs_free_extent(trans, root, bytenr,
-						 blocksize,
+						 blocksize, parent->start,
 						 btrfs_header_owner(parent),
 						 generation, 0, 0, 1);
 			if (wret)
@@ -954,6 +950,7 @@ static noinline int balance_level(struct btrfs_trans_handle *trans,
 		if (wret)
 			ret = wret;
 		wret = btrfs_free_extent(trans, root, bytenr, blocksize,
+					 parent->start,
 					 btrfs_header_owner(parent),
 					 root_gen, 0, 0, 1);
 		if (wret)
@@ -1499,6 +1496,41 @@ static int fixup_low_keys(struct btrfs_trans_handle *trans,
 	return ret;
 }
 
+/*
+ * update item key.
+ *
+ * This function isn't completely safe. It's the caller's responsibility
+ * that the new key won't break the order
+ */
+int btrfs_set_item_key_safe(struct btrfs_trans_handle *trans,
+			    struct btrfs_root *root, struct btrfs_path *path,
+			    struct btrfs_key *new_key)
+{
+	struct btrfs_disk_key disk_key;
+	struct extent_buffer *eb;
+	int slot;
+
+	eb = path->nodes[0];
+	slot = path->slots[0];
+	if (slot > 0) {
+		btrfs_item_key(eb, &disk_key, slot - 1);
+		if (comp_keys(&disk_key, new_key) >= 0)
+			return -1;
+	}
+	if (slot < btrfs_header_nritems(eb) - 1) {
+		btrfs_item_key(eb, &disk_key, slot + 1);
+		if (comp_keys(&disk_key, new_key) <= 0)
+			return -1;
+	}
+
+	btrfs_cpu_key_to_disk(&disk_key, new_key);
+	btrfs_set_item_key(eb, &disk_key, slot);
+	btrfs_mark_buffer_dirty(eb);
+	if (slot == 0)
+		fixup_low_keys(trans, root, path, &disk_key, 1);
+	return 0;
+}
+
 /*
  * try to push data from one node into the next node left in the
  * tree.
@@ -1558,6 +1590,10 @@ static int push_node_left(struct btrfs_trans_handle *trans,
 	btrfs_set_header_nritems(dst, dst_nritems + push_items);
 	btrfs_mark_buffer_dirty(src);
 	btrfs_mark_buffer_dirty(dst);
+
+	ret = btrfs_update_ref(trans, root, src, dst, dst_nritems, push_items);
+	BUG_ON(ret);
+
 	return ret;
 }
 
@@ -1619,6 +1655,10 @@ static int balance_node_right(struct btrfs_trans_handle *trans,
 
 	btrfs_mark_buffer_dirty(src);
 	btrfs_mark_buffer_dirty(dst);
+
+	ret = btrfs_update_ref(trans, root, src, dst, 0, push_items);
+	BUG_ON(ret);
+
 	return ret;
 }
 
@@ -1633,30 +1673,24 @@ static int noinline insert_new_root(struct btrfs_trans_handle *trans,
 			   struct btrfs_root *root,
 			   struct btrfs_path *path, int level)
 {
-	u64 root_gen;
 	u64 lower_gen;
 	struct extent_buffer *lower;
 	struct extent_buffer *c;
 	struct extent_buffer *old;
 	struct btrfs_disk_key lower_key;
+	int ret;
 
 	BUG_ON(path->nodes[level]);
 	BUG_ON(path->nodes[level-1] != root->node);
 
-	if (root->ref_cows)
-		root_gen = trans->transid;
-	else
-		root_gen = 0;
-
 	lower = path->nodes[level-1];
 	if (level == 1)
 		btrfs_item_key(lower, &lower_key, 0);
 	else
 		btrfs_node_key(lower, &lower_key, 0);
 
-	c = btrfs_alloc_free_block(trans, root, root->nodesize,
-				   root->root_key.objectid,
-				   root_gen, le64_to_cpu(lower_key.objectid),
+	c = btrfs_alloc_free_block(trans, root, root->nodesize, 0,
+				   root->root_key.objectid, trans->transid,
 				   level, root->node->start, 0);
 	if (IS_ERR(c))
 		return PTR_ERR(c);
@@ -1679,7 +1713,7 @@ static int noinline insert_new_root(struct btrfs_trans_handle *trans,
 	btrfs_set_node_key(c, &lower_key, 0);
 	btrfs_set_node_blockptr(c, 0, lower->start);
 	lower_gen = btrfs_header_generation(lower);
-	WARN_ON(lower_gen == 0);
+	WARN_ON(lower_gen != trans->transid);
 
 	btrfs_set_node_ptr_generation(c, 0, lower_gen);
 
@@ -1690,6 +1724,12 @@ static int noinline insert_new_root(struct btrfs_trans_handle *trans,
 	root->node = c;
 	spin_unlock(&root->node_lock);
 
+	ret = btrfs_update_extent_ref(trans, root, lower->start,
+				      lower->start, c->start,
+				      root->root_key.objectid,
+				      trans->transid, level - 1, 0);
+	BUG_ON(ret);
+
 	/* the super has an extra ref to root->node */
 	free_extent_buffer(old);
 
@@ -1698,20 +1738,6 @@ static int noinline insert_new_root(struct btrfs_trans_handle *trans,
 	path->nodes[level] = c;
 	path->locks[level] = 1;
 	path->slots[level] = 0;
-
-	if (root->ref_cows && lower_gen != trans->transid) {
-		struct btrfs_path *back_path = btrfs_alloc_path();
-		int ret;
-		mutex_lock(&root->fs_info->alloc_mutex);
-		ret = btrfs_insert_extent_backref(trans,
-						  root->fs_info->extent_root,
-						  path, lower->start,
-						  root->root_key.objectid,
-						  trans->transid, 0, 0);
-		BUG_ON(ret);
-		mutex_unlock(&root->fs_info->alloc_mutex);
-		btrfs_free_path(back_path);
-	}
 	return 0;
 }
 
@@ -1766,7 +1792,6 @@ static noinline int split_node(struct btrfs_trans_handle *trans,
 			       struct btrfs_root *root,
 			       struct btrfs_path *path, int level)
 {
-	u64 root_gen;
 	struct extent_buffer *c;
 	struct extent_buffer *split;
 	struct btrfs_disk_key disk_key;
@@ -1793,17 +1818,11 @@ static noinline int split_node(struct btrfs_trans_handle *trans,
 	}
 
 	c_nritems = btrfs_header_nritems(c);
-	if (root->ref_cows)
-		root_gen = trans->transid;
-	else
-		root_gen = 0;
 
-	btrfs_node_key(c, &disk_key, 0);
 	split = btrfs_alloc_free_block(trans, root, root->nodesize,
-					 root->root_key.objectid,
-					 root_gen,
-					 btrfs_disk_key_objectid(&disk_key),
-					 level, c->start, 0);
+					path->nodes[level + 1]->start,
+					root->root_key.objectid,
+					trans->transid, level, c->start, 0);
 	if (IS_ERR(split))
 		return PTR_ERR(split);
 
@@ -1840,6 +1859,9 @@ static noinline int split_node(struct btrfs_trans_handle *trans,
 	if (wret)
 		ret = wret;
 
+	ret = btrfs_update_ref(trans, root, c, split, 0, c_nritems - mid);
+	BUG_ON(ret);
+
 	if (path->slots[level] >= mid) {
 		path->slots[level] -= mid;
 		btrfs_tree_unlock(c);
@@ -1955,10 +1977,23 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 	else
 		nr = 1;
 
+	if (path->slots[0] >= left_nritems)
+		push_space += data_size + sizeof(*item);
+
 	i = left_nritems - 1;
 	while (i >= nr) {
 		item = btrfs_item_nr(left, i);
 
+		if (!empty && push_items > 0) {
+			if (path->slots[0] > i)
+				break;
+			if (path->slots[0] == i) {
+				int space = btrfs_leaf_free_space(root, left);
+				if (space + push_space * 2 > free_space)
+					break;
+			}
+		}
+
 		if (path->slots[0] == i)
 			push_space += data_size + sizeof(*item);
 
@@ -1973,6 +2008,7 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 		this_item_size = btrfs_item_size(left, item);
 		if (this_item_size + sizeof(*item) + push_space > free_space)
 			break;
+
 		push_items++;
 		push_space += this_item_size + sizeof(*item);
 		if (i == 0)
@@ -2046,6 +2082,9 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 		btrfs_mark_buffer_dirty(left);
 	btrfs_mark_buffer_dirty(right);
 
+	ret = btrfs_update_ref(trans, root, left, right, 0, push_items);
+	BUG_ON(ret);
+
 	btrfs_item_key(right, &disk_key, 0);
 	btrfs_set_node_key(upper, &disk_key, slot + 1);
 	btrfs_mark_buffer_dirty(upper);
@@ -2147,6 +2186,16 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 					KM_USER1);
 		}
 
+		if (!empty && push_items > 0) {
+			if (path->slots[0] < i)
+				break;
+			if (path->slots[0] == i) {
+				int space = btrfs_leaf_free_space(root, right);
+				if (space + push_space * 2 > free_space)
+					break;
+			}
+		}
+
 		if (path->slots[0] == i)
 			push_space += data_size + sizeof(*item);
 
@@ -2255,6 +2304,10 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 	if (right_nritems)
 		btrfs_mark_buffer_dirty(right);
 
+	ret = btrfs_update_ref(trans, root, right, left,
+			       old_left_nritems, push_items);
+	BUG_ON(ret);
+
 	btrfs_item_key(right, &disk_key, 0);
 	wret = fixup_low_keys(trans, root, path, &disk_key, 1);
 	if (wret)
@@ -2294,7 +2347,6 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 			       struct btrfs_path *path, int data_size,
 			       int extend)
 {
-	u64 root_gen;
 	struct extent_buffer *l;
 	u32 nritems;
 	int mid;
@@ -2313,11 +2365,6 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 	if (extend)
 		space_needed = data_size;
 
-	if (root->ref_cows)
-		root_gen = trans->transid;
-	else
-		root_gen = 0;
-
 	/* first try to make some room by pushing left and right */
 	if (ins_key->type != BTRFS_DIR_ITEM_KEY) {
 		wret = push_leaf_right(trans, root, path, data_size, 0);
@@ -2348,13 +2395,10 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 	nritems = btrfs_header_nritems(l);
 	mid = (nritems + 1)/ 2;
 
-	btrfs_item_key(l, &disk_key, 0);
-
 	right = btrfs_alloc_free_block(trans, root, root->leafsize,
-					 root->root_key.objectid,
-					 root_gen,
-					 le64_to_cpu(disk_key.objectid),
-					 0, l->start, 0);
+					path->nodes[1]->start,
+					root->root_key.objectid,
+					trans->transid, 0, l->start, 0);
 	if (IS_ERR(right)) {
 		BUG_ON(1);
 		return PTR_ERR(right);
@@ -2485,6 +2529,9 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 	btrfs_mark_buffer_dirty(l);
 	BUG_ON(path->slots[0] != slot);
 
+	ret = btrfs_update_ref(trans, root, l, right, 0, nritems);
+	BUG_ON(ret);
+
 	if (mid <= slot) {
 		btrfs_tree_unlock(path->nodes[0]);
 		free_extent_buffer(path->nodes[0]);
@@ -2956,6 +3003,7 @@ int btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 				ret = wret;
 			wret = btrfs_free_extent(trans, root,
 					 leaf->start, leaf->len,
+					 path->nodes[1]->start,
 					 btrfs_header_owner(path->nodes[1]),
 					 root_gen, 0, 0, 1);
 			if (wret)
@@ -3007,7 +3055,7 @@ int btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 
 				free_extent_buffer(leaf);
 				wret = btrfs_free_extent(trans, root, bytenr,
-					     blocksize,
+					     blocksize, path->nodes[1]->start,
 					     btrfs_header_owner(path->nodes[1]),
 					     root_gen, 0, 0, 1);
 				if (wret)

commit 0f9dd46cda36b8de3b9f48bc42bd09d20b9c3b52
Author: Josef Bacik <jbacik@redhat.com>
Date:   Tue Sep 23 13:14:11 2008 -0400

    Btrfs: free space accounting redo
    
    1) replace the per fs_info extent_io_tree that tracked free space with two
    rb-trees per block group to track free space areas via offset and size.  The
    reason to do this is because most allocations come with a hint byte where to
    start, so we can usually find a chunk of free space at that hint byte to satisfy
    the allocation and get good space packing.  If we cannot find free space at or
    after the given offset we fall back on looking for a chunk of the given size as
    close to that given offset as possible.  When we fall back on the size search we
    also try to find a slot as close to the size we want as possible, to avoid
    breaking small chunks off of huge areas if possible.
    
    2) remove the extent_io_tree that tracked the block group cache from fs_info and
    replaced it with an rb-tree thats tracks block group cache via offset.  also
    added a per space_info list that tracks the block group cache for the particular
    space so we can lookup related block groups easily.
    
    3) cleaned up the allocation code to make it a little easier to read and a
    little less complicated.  Basically there are 3 steps, first look from our
    provided hint.  If we couldn't find from that given hint, start back at our
    original search start and look for space from there.  If that fails try to
    allocate space if we can and start looking again.  If not we're screwed and need
    to start over again.
    
    4) small fixes.  there were some issues in volumes.c where we wouldn't allocate
    the rest of the disk.  fixed cow_file_range to actually pass the alloc_hint,
    which has helped a good bit in making the fs_mark test I run have semi-normal
    results as we run out of space.  Generally with data allocations we don't track
    where we last allocated from, so everytime we did a data allocation we'd search
    through every block group that we have looking for free space.  Now searching a
    block group with no free space isn't terribly time consuming, it was causing a
    slight degradation as we got more data block groups.  The alloc_hint has fixed
    this slight degredation and made things semi-normal.
    
    There is still one nagging problem I'm working on where we will get ENOSPC when
    there is definitely plenty of space.  This only happens with metadata
    allocations, and only when we are almost full.  So you generally hit the 85%
    mark first, but sometimes you'll hit the BUG before you hit the 85% wall.  I'm
    still tracking it down, but until then this seems to be pretty stable and make a
    significant performance gain.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 18e84472abb5..6f467901246f 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2725,9 +2725,8 @@ int btrfs_insert_empty_items(struct btrfs_trans_handle *trans,
 
 	total_size = total_data + (nr * sizeof(struct btrfs_item));
 	ret = btrfs_search_slot(trans, root, cpu_key, path, total_size, 1);
-	if (ret == 0) {
+	if (ret == 0)
 		return -EEXIST;
-	}
 	if (ret < 0)
 		goto out;
 

commit f25956cc582f5954033cca3f9488c0a5d5193541
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Sep 12 15:32:53 2008 -0400

    Fix leaf overflow check in btrfs_insert_empty_items
    
    It was incorrectly adding an extra sizeof(struct btrfs_item) and causing
    false positives (oops)
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index ab07557bac07..18e84472abb5 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2737,8 +2737,7 @@ int btrfs_insert_empty_items(struct btrfs_trans_handle *trans,
 	nritems = btrfs_header_nritems(leaf);
 	data_end = leaf_data_end(root, leaf);
 
-	if (btrfs_leaf_free_space(root, leaf) <
-	    sizeof(struct btrfs_item) + total_size) {
+	if (btrfs_leaf_free_space(root, leaf) < total_size) {
 		btrfs_print_leaf(root, leaf);
 		printk("not enough freespace need %u have %d\n",
 		       total_size, btrfs_leaf_free_space(root, leaf));

commit b214107eda845f9a5851ae198f5b972e0dc30c45
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Sep 5 16:43:31 2008 -0400

    Btrfs: trivial sparse fixes
    
    Fix a bunch of trivial sparse complaints.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 6cd4e929bd4d..ab07557bac07 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2749,7 +2749,6 @@ int btrfs_insert_empty_items(struct btrfs_trans_handle *trans,
 	BUG_ON(slot < 0);
 
 	if (slot != nritems) {
-		int i;
 		unsigned int old_data = btrfs_item_end_nr(leaf, slot);
 
 		if (old_data < data_end) {
@@ -2913,7 +2912,6 @@ int btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 	nritems = btrfs_header_nritems(leaf);
 
 	if (slot + nr != nritems) {
-		int i;
 		int data_end = leaf_data_end(root, leaf);
 
 		memmove_extent_buffer(leaf, btrfs_leaf_data(leaf) +

commit ad3d81ba8f2880dfa3ee518c779a7ea09a024520
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Sep 5 16:43:28 2008 -0400

    Btrfs: missing endianess conversion in insert_new_root
    
    Add two missing endianess conversions in this function, found by sparse.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 579124043d9b..6cd4e929bd4d 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1656,8 +1656,8 @@ static int noinline insert_new_root(struct btrfs_trans_handle *trans,
 
 	c = btrfs_alloc_free_block(trans, root, root->nodesize,
 				   root->root_key.objectid,
-				   root_gen, lower_key.objectid, level,
-				   root->node->start, 0);
+				   root_gen, le64_to_cpu(lower_key.objectid),
+				   level, root->node->start, 0);
 	if (IS_ERR(c))
 		return PTR_ERR(c);
 
@@ -2352,8 +2352,9 @@ static noinline int split_leaf(struct btrfs_trans_handle *trans,
 
 	right = btrfs_alloc_free_block(trans, root, root->leafsize,
 					 root->root_key.objectid,
-					 root_gen, disk_key.objectid, 0,
-					 l->start, 0);
+					 root_gen,
+					 le64_to_cpu(disk_key.objectid),
+					 0, l->start, 0);
 	if (IS_ERR(right)) {
 		BUG_ON(1);
 		return PTR_ERR(right);

commit e02119d5a7b4396c5a872582fddc8bd6d305a70a
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Sep 5 16:13:11 2008 -0400

    Btrfs: Add a write ahead tree log to optimize synchronous operations
    
    File syncs and directory syncs are optimized by copying their
    items into a special (copy-on-write) log tree.  There is one log tree per
    subvolume and the btrfs super block points to a tree of log tree roots.
    
    After a crash, items are copied out of the log tree and back into the
    subvolume.  See tree-log.c for all the details.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 7114faafa9d4..579124043d9b 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -60,7 +60,7 @@ void btrfs_free_path(struct btrfs_path *p)
 	kmem_cache_free(btrfs_path_cachep, p);
 }
 
-void btrfs_release_path(struct btrfs_root *root, struct btrfs_path *p)
+void noinline btrfs_release_path(struct btrfs_root *root, struct btrfs_path *p)
 {
 	int i;
 
@@ -176,7 +176,7 @@ int btrfs_copy_root(struct btrfs_trans_handle *trans,
 	return 0;
 }
 
-int __btrfs_cow_block(struct btrfs_trans_handle *trans,
+int noinline __btrfs_cow_block(struct btrfs_trans_handle *trans,
 			     struct btrfs_root *root,
 			     struct extent_buffer *buf,
 			     struct extent_buffer *parent, int parent_slot,
@@ -294,7 +294,7 @@ int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 	return 0;
 }
 
-int btrfs_cow_block(struct btrfs_trans_handle *trans,
+int noinline btrfs_cow_block(struct btrfs_trans_handle *trans,
 		    struct btrfs_root *root, struct extent_buffer *buf,
 		    struct extent_buffer *parent, int parent_slot,
 		    struct extent_buffer **cow_ret, u64 prealloc_dest)
@@ -677,9 +677,10 @@ static int noinline check_block(struct btrfs_root *root,
  *
  * slot may point to max if the key is bigger than all of the keys
  */
-static int generic_bin_search(struct extent_buffer *eb, unsigned long p,
-			      int item_size, struct btrfs_key *key,
-			      int max, int *slot)
+static noinline int generic_bin_search(struct extent_buffer *eb,
+				       unsigned long p,
+				       int item_size, struct btrfs_key *key,
+				       int max, int *slot)
 {
 	int low = 0;
 	int high = max;
@@ -765,7 +766,7 @@ static int bin_search(struct extent_buffer *eb, struct btrfs_key *key,
 	return -1;
 }
 
-static struct extent_buffer *read_node_slot(struct btrfs_root *root,
+static noinline struct extent_buffer *read_node_slot(struct btrfs_root *root,
 				   struct extent_buffer *parent, int slot)
 {
 	int level = btrfs_header_level(parent);
@@ -781,7 +782,7 @@ static struct extent_buffer *read_node_slot(struct btrfs_root *root,
 		       btrfs_node_ptr_generation(parent, slot));
 }
 
-static int balance_level(struct btrfs_trans_handle *trans,
+static noinline int balance_level(struct btrfs_trans_handle *trans,
 			 struct btrfs_root *root,
 			 struct btrfs_path *path, int level)
 {
@@ -1128,8 +1129,9 @@ static int noinline push_nodes_for_insert(struct btrfs_trans_handle *trans,
 /*
  * readahead one full node of leaves
  */
-static void reada_for_search(struct btrfs_root *root, struct btrfs_path *path,
-			     int level, int slot, u64 objectid)
+static noinline void reada_for_search(struct btrfs_root *root,
+				      struct btrfs_path *path,
+				      int level, int slot, u64 objectid)
 {
 	struct extent_buffer *node;
 	struct btrfs_disk_key disk_key;
@@ -1201,7 +1203,8 @@ static void reada_for_search(struct btrfs_root *root, struct btrfs_path *path,
 	}
 }
 
-static void unlock_up(struct btrfs_path *path, int level, int lowest_unlock)
+static noinline void unlock_up(struct btrfs_path *path, int level,
+			       int lowest_unlock)
 {
 	int i;
 	int skip_level = level;
@@ -1759,8 +1762,9 @@ static int insert_ptr(struct btrfs_trans_handle *trans, struct btrfs_root
  *
  * returns 0 on success and < 0 on failure
  */
-static int split_node(struct btrfs_trans_handle *trans, struct btrfs_root
-		      *root, struct btrfs_path *path, int level)
+static noinline int split_node(struct btrfs_trans_handle *trans,
+			       struct btrfs_root *root,
+			       struct btrfs_path *path, int level)
 {
 	u64 root_gen;
 	struct extent_buffer *c;
@@ -1874,7 +1878,8 @@ static int leaf_space_used(struct extent_buffer *l, int start, int nr)
  * the start of the leaf data.  IOW, how much room
  * the leaf has left for both items and data
  */
-int btrfs_leaf_free_space(struct btrfs_root *root, struct extent_buffer *leaf)
+int noinline btrfs_leaf_free_space(struct btrfs_root *root,
+				   struct extent_buffer *leaf)
 {
 	int nritems = btrfs_header_nritems(leaf);
 	int ret;
@@ -2283,9 +2288,11 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
  *
  * returns 0 if all went well and < 0 on failure.
  */
-static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
-		      *root, struct btrfs_key *ins_key,
-		      struct btrfs_path *path, int data_size, int extend)
+static noinline int split_leaf(struct btrfs_trans_handle *trans,
+			       struct btrfs_root *root,
+			       struct btrfs_key *ins_key,
+			       struct btrfs_path *path, int data_size,
+			       int extend)
 {
 	u64 root_gen;
 	struct extent_buffer *l;
@@ -3079,6 +3086,7 @@ int btrfs_prev_leaf(struct btrfs_root *root, struct btrfs_path *path)
  * was nothing in the tree that matched the search criteria.
  */
 int btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,
+			 struct btrfs_key *max_key,
 			 struct btrfs_path *path, int cache_only,
 			 u64 min_trans)
 {
@@ -3093,6 +3101,7 @@ int btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,
 again:
 	cur = btrfs_lock_root_node(root);
 	level = btrfs_header_level(cur);
+	WARN_ON(path->nodes[level]);
 	path->nodes[level] = cur;
 	path->locks[level] = 1;
 
@@ -3107,6 +3116,8 @@ int btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,
 
 		/* at level = 0, we're done, setup the path and exit */
 		if (level == 0) {
+			if (slot >= nritems)
+				goto find_next_key;
 			ret = 0;
 			path->slots[level] = slot;
 			btrfs_item_key_to_cpu(cur, &found_key, slot);
@@ -3123,6 +3134,8 @@ int btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,
 			u64 blockptr;
 			u64 gen;
 			struct extent_buffer *tmp;
+			struct btrfs_disk_key disk_key;
+
 			blockptr = btrfs_node_blockptr(cur, slot);
 			gen = btrfs_node_ptr_generation(cur, slot);
 			if (gen < min_trans) {
@@ -3132,6 +3145,14 @@ int btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,
 			if (!cache_only)
 				break;
 
+			if (max_key) {
+				btrfs_node_key(cur, &disk_key, slot);
+				if (comp_keys(&disk_key, max_key) >= 0) {
+					ret = 1;
+					goto out;
+				}
+			}
+
 			tmp = btrfs_find_tree_block(root, blockptr,
 					    btrfs_level_size(root, level - 1));
 
@@ -3143,14 +3164,16 @@ int btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,
 				free_extent_buffer(tmp);
 			slot++;
 		}
+find_next_key:
 		/*
 		 * we didn't find a candidate key in this node, walk forward
 		 * and find another one
 		 */
 		if (slot >= nritems) {
-			ret = btrfs_find_next_key(root, path, min_key, level,
+			path->slots[level] = slot;
+			sret = btrfs_find_next_key(root, path, min_key, level,
 						  cache_only, min_trans);
-			if (ret == 0) {
+			if (sret == 0) {
 				btrfs_release_path(root, path);
 				goto again;
 			} else {
@@ -3351,6 +3374,7 @@ int btrfs_previous_item(struct btrfs_root *root,
 {
 	struct btrfs_key found_key;
 	struct extent_buffer *leaf;
+	u32 nritems;
 	int ret;
 
 	while(1) {
@@ -3362,9 +3386,20 @@ int btrfs_previous_item(struct btrfs_root *root,
 			path->slots[0]--;
 		}
 		leaf = path->nodes[0];
+		nritems = btrfs_header_nritems(leaf);
+		if (nritems == 0)
+			return 1;
+		if (path->slots[0] == nritems)
+			path->slots[0]--;
+
 		btrfs_item_key_to_cpu(leaf, &found_key, path->slots[0]);
 		if (found_key.type == type)
 			return 0;
+		if (found_key.objectid < min_objectid)
+			break;
+		if (found_key.objectid == min_objectid &&
+		    found_key.type < type)
+			break;
 	}
 	return 1;
 }

commit 65b51a009e29e64c0951f21ea17fdc66bbb0fbd7
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Aug 1 15:11:20 2008 -0400

    btrfs_search_slot: reduce lock contention by cowing in two stages
    
    A btree block cow has two parts, the first is to allocate a destination
    block and the second is to copy the old bock over.
    
    The first part needs locks in the extent allocation tree, and may need to
    do IO.  This changeset splits that into a separate function that can be
    called without any tree locks held.
    
    btrfs_search_slot is changed to drop its path and start over if it has
    to COW a contended block.  This often means that many writers will
    pre-alloc a new destination for a the same contended block, but they
    cache their prealloc for later use on lower levels in the tree.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index c4792062dd53..7114faafa9d4 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -181,7 +181,8 @@ int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 			     struct extent_buffer *buf,
 			     struct extent_buffer *parent, int parent_slot,
 			     struct extent_buffer **cow_ret,
-			     u64 search_start, u64 empty_size)
+			     u64 search_start, u64 empty_size,
+			     u64 prealloc_dest)
 {
 	u64 root_gen;
 	struct extent_buffer *cow;
@@ -216,10 +217,27 @@ int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 	} else {
 		first_key.objectid = 0;
 	}
-	cow = btrfs_alloc_free_block(trans, root, buf->len,
-				     root->root_key.objectid,
-				     root_gen, first_key.objectid, level,
-				     search_start, empty_size);
+	if (prealloc_dest) {
+		struct btrfs_key ins;
+
+		ins.objectid = prealloc_dest;
+		ins.offset = buf->len;
+		ins.type = BTRFS_EXTENT_ITEM_KEY;
+
+		ret = btrfs_alloc_reserved_extent(trans, root,
+						  root->root_key.objectid,
+						  root_gen, level,
+						  first_key.objectid,
+						  &ins);
+		BUG_ON(ret);
+		cow = btrfs_init_new_buffer(trans, root, prealloc_dest,
+					    buf->len);
+	} else {
+		cow = btrfs_alloc_free_block(trans, root, buf->len,
+					     root->root_key.objectid,
+					     root_gen, first_key.objectid,
+					     level, search_start, empty_size);
+	}
 	if (IS_ERR(cow))
 		return PTR_ERR(cow);
 
@@ -279,7 +297,7 @@ int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 int btrfs_cow_block(struct btrfs_trans_handle *trans,
 		    struct btrfs_root *root, struct extent_buffer *buf,
 		    struct extent_buffer *parent, int parent_slot,
-		    struct extent_buffer **cow_ret)
+		    struct extent_buffer **cow_ret, u64 prealloc_dest)
 {
 	u64 search_start;
 	u64 header_trans;
@@ -302,12 +320,14 @@ int btrfs_cow_block(struct btrfs_trans_handle *trans,
 	    !btrfs_header_flag(buf, BTRFS_HEADER_FLAG_WRITTEN)) {
 		*cow_ret = buf;
 		spin_unlock(&root->fs_info->hash_lock);
+		WARN_ON(prealloc_dest);
 		return 0;
 	}
 	spin_unlock(&root->fs_info->hash_lock);
 	search_start = buf->start & ~((u64)(1024 * 1024 * 1024) - 1);
 	ret = __btrfs_cow_block(trans, root, buf, parent,
-				 parent_slot, cow_ret, search_start, 0);
+				 parent_slot, cow_ret, search_start, 0,
+				 prealloc_dest);
 	return ret;
 }
 
@@ -451,7 +471,7 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 		err = __btrfs_cow_block(trans, root, cur, parent, i,
 					&cur, search_start,
 					min(16 * blocksize,
-					    (end_slot - i) * blocksize));
+					    (end_slot - i) * blocksize), 0);
 		if (err) {
 			btrfs_tree_unlock(cur);
 			free_extent_buffer(cur);
@@ -803,7 +823,7 @@ static int balance_level(struct btrfs_trans_handle *trans,
 		child = read_node_slot(root, mid, 0);
 		btrfs_tree_lock(child);
 		BUG_ON(!child);
-		ret = btrfs_cow_block(trans, root, child, mid, 0, &child);
+		ret = btrfs_cow_block(trans, root, child, mid, 0, &child, 0);
 		BUG_ON(ret);
 
 		spin_lock(&root->node_lock);
@@ -836,7 +856,7 @@ static int balance_level(struct btrfs_trans_handle *trans,
 	if (left) {
 		btrfs_tree_lock(left);
 		wret = btrfs_cow_block(trans, root, left,
-				       parent, pslot - 1, &left);
+				       parent, pslot - 1, &left, 0);
 		if (wret) {
 			ret = wret;
 			goto enospc;
@@ -846,7 +866,7 @@ static int balance_level(struct btrfs_trans_handle *trans,
 	if (right) {
 		btrfs_tree_lock(right);
 		wret = btrfs_cow_block(trans, root, right,
-				       parent, pslot + 1, &right);
+				       parent, pslot + 1, &right, 0);
 		if (wret) {
 			ret = wret;
 			goto enospc;
@@ -1021,7 +1041,7 @@ static int noinline push_nodes_for_insert(struct btrfs_trans_handle *trans,
 			wret = 1;
 		} else {
 			ret = btrfs_cow_block(trans, root, left, parent,
-					      pslot - 1, &left);
+					      pslot - 1, &left, 0);
 			if (ret)
 				wret = 1;
 			else {
@@ -1069,7 +1089,7 @@ static int noinline push_nodes_for_insert(struct btrfs_trans_handle *trans,
 		} else {
 			ret = btrfs_cow_block(trans, root, right,
 					      parent, pslot + 1,
-					      &right);
+					      &right, 0);
 			if (ret)
 				wret = 1;
 			else {
@@ -1245,6 +1265,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 	u8 lowest_level = 0;
 	u64 blocknr;
 	u64 gen;
+	struct btrfs_key prealloc_block;
 
 	lowest_level = p->lowest_level;
 	WARN_ON(lowest_level && ins_len);
@@ -1253,6 +1274,9 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 		!mutex_is_locked(&root->fs_info->alloc_mutex));
 	if (ins_len < 0)
 		lowest_unlock = 2;
+
+	prealloc_block.objectid = 0;
+
 again:
 	if (p->skip_locking)
 		b = btrfs_root_node(root);
@@ -1261,27 +1285,82 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 
 	while (b) {
 		level = btrfs_header_level(b);
+
+		/*
+		 * setup the path here so we can release it under lock
+		 * contention with the cow code
+		 */
+		p->nodes[level] = b;
+		if (!p->skip_locking)
+			p->locks[level] = 1;
+
 		if (cow) {
 			int wret;
+
+			/* is a cow on this block not required */
+			spin_lock(&root->fs_info->hash_lock);
+			if (btrfs_header_generation(b) == trans->transid &&
+			    !btrfs_header_flag(b, BTRFS_HEADER_FLAG_WRITTEN)) {
+				spin_unlock(&root->fs_info->hash_lock);
+				goto cow_done;
+			}
+			spin_unlock(&root->fs_info->hash_lock);
+
+			/* ok, we have to cow, is our old prealloc the right
+			 * size?
+			 */
+			if (prealloc_block.objectid &&
+			    prealloc_block.offset != b->len) {
+				btrfs_free_reserved_extent(root,
+					   prealloc_block.objectid,
+					   prealloc_block.offset);
+				prealloc_block.objectid = 0;
+			}
+
+			/*
+			 * for higher level blocks, try not to allocate blocks
+			 * with the block and the parent locks held.
+			 */
+			if (level > 1 && !prealloc_block.objectid &&
+			    btrfs_path_lock_waiting(p, level)) {
+				u32 size = b->len;
+				u64 hint = b->start;
+
+				btrfs_release_path(root, p);
+				ret = btrfs_reserve_extent(trans, root,
+							   size, size, 0,
+							   hint, (u64)-1,
+							   &prealloc_block, 0);
+				BUG_ON(ret);
+				goto again;
+			}
+
 			wret = btrfs_cow_block(trans, root, b,
 					       p->nodes[level + 1],
 					       p->slots[level + 1],
-					       &b);
+					       &b, prealloc_block.objectid);
+			prealloc_block.objectid = 0;
 			if (wret) {
 				free_extent_buffer(b);
-				return wret;
+				ret = wret;
+				goto done;
 			}
 		}
+cow_done:
 		BUG_ON(!cow && ins_len);
 		if (level != btrfs_header_level(b))
 			WARN_ON(1);
 		level = btrfs_header_level(b);
+
 		p->nodes[level] = b;
 		if (!p->skip_locking)
 			p->locks[level] = 1;
+
 		ret = check_block(root, p, level);
-		if (ret)
-			return -1;
+		if (ret) {
+			ret = -1;
+			goto done;
+		}
 
 		ret = bin_search(b, key, level, &slot);
 		if (level != 0) {
@@ -1292,15 +1371,19 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 			    BTRFS_NODEPTRS_PER_BLOCK(root) - 3) {
 				int sret = split_node(trans, root, p, level);
 				BUG_ON(sret > 0);
-				if (sret)
-					return sret;
+				if (sret) {
+					ret = sret;
+					goto done;
+				}
 				b = p->nodes[level];
 				slot = p->slots[level];
 			} else if (ins_len < 0) {
 				int sret = balance_level(trans, root, p,
 							 level);
-				if (sret)
-					return sret;
+				if (sret) {
+					ret = sret;
+					goto done;
+				}
 				b = p->nodes[level];
 				if (!b) {
 					btrfs_release_path(NULL, p);
@@ -1362,14 +1445,24 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 				int sret = split_leaf(trans, root, key,
 						      p, ins_len, ret == 0);
 				BUG_ON(sret > 0);
-				if (sret)
-					return sret;
+				if (sret) {
+					ret = sret;
+					goto done;
+				}
 			}
 			unlock_up(p, level, lowest_unlock);
-			return ret;
+			goto done;
 		}
 	}
-	return 1;
+	ret = 1;
+done:
+	if (prealloc_block.objectid) {
+		btrfs_free_reserved_extent(root,
+			   prealloc_block.objectid,
+			   prealloc_block.offset);
+	}
+
+	return ret;
 }
 
 /*
@@ -1840,7 +1933,7 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 
 	/* cow and double check */
 	ret = btrfs_cow_block(trans, root, right, upper,
-			      slot + 1, &right);
+			      slot + 1, &right, 0);
 	if (ret)
 		goto out_unlock;
 
@@ -2021,7 +2114,7 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 
 	/* cow and double check */
 	ret = btrfs_cow_block(trans, root, left,
-			      path->nodes[1], slot - 1, &left);
+			      path->nodes[1], slot - 1, &left, 0);
 	if (ret) {
 		/* we hit -ENOSPC, but it isn't fatal here */
 		ret = 1;

commit bcc63abbf3e9bf948a1b0129b3e6120ec7d7f698
Author: Yan <zheng.yan@oracle.com>
Date:   Wed Jul 30 16:29:20 2008 -0400

    Btrfs: implement memory reclaim for leaf reference cache
    
    The memory reclaiming issue happens when snapshot exists. In that
    case, some cache entries may not be used during old snapshot dropping,
    so they will remain in the cache until umount.
    
    The patch adds a field to struct btrfs_leaf_ref to record create time. Besides,
    the patch makes all dead roots of a given snapshot linked together in order of
    create time. After a old snapshot was completely dropped, we check the dead
    root list and remove all cache entries created before the oldest dead root in
    the list.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 245eb00435dd..c4792062dd53 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -3275,4 +3275,3 @@ int btrfs_previous_item(struct btrfs_root *root,
 	}
 	return 1;
 }
-

commit 31153d81284934601d08110ac7698fd9a535e4c0
Author: Yan Zheng <zheng.yan@oracle.com>
Date:   Mon Jul 28 15:32:19 2008 -0400

    Btrfs: Add a leaf reference cache
    
    Much of the IO done while dropping snapshots is done looking up
    leaves in the filesystem trees to see if they point to any extents and
    to drop the references on any extents found.
    
    This creates a cache so that IO isn't required.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index ab4ac0365c7f..245eb00435dd 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -165,7 +165,7 @@ int btrfs_copy_root(struct btrfs_trans_handle *trans,
 	btrfs_clear_header_flag(cow, BTRFS_HEADER_FLAG_WRITTEN);
 
 	WARN_ON(btrfs_header_generation(buf) > trans->transid);
-	ret = btrfs_inc_ref(trans, new_root, buf);
+	ret = btrfs_inc_ref(trans, new_root, buf, 0);
 	kfree(new_root);
 
 	if (ret)
@@ -232,7 +232,7 @@ int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 	WARN_ON(btrfs_header_generation(buf) > trans->transid);
 	if (btrfs_header_generation(buf) != trans->transid) {
 		different_trans = 1;
-		ret = btrfs_inc_ref(trans, root, buf);
+		ret = btrfs_inc_ref(trans, root, buf, 1);
 		if (ret)
 			return ret;
 	} else {

commit 9652480bf48500885a30754b4a5c436b5b34456d
Author: Yan <yanzheng@21cn.com>
Date:   Thu Jul 24 12:19:49 2008 -0400

    Fix path slots selection in btrfs_search_forward
    
    We should decrease the found slot by one as btrfs_search_slot does
    when bin_search return 1 and node level > 0.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index f2a94999c371..ab4ac0365c7f 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2992,6 +2992,7 @@ int btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,
 	struct extent_buffer *cur;
 	struct btrfs_key found_key;
 	int slot;
+	int sret;
 	u32 nritems;
 	int level;
 	int ret = 1;
@@ -3009,7 +3010,7 @@ int btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,
 	while(1) {
 		nritems = btrfs_header_nritems(cur);
 		level = btrfs_header_level(cur);
-		bin_search(cur, min_key, level, &slot);
+		sret = bin_search(cur, min_key, level, &slot);
 
 		/* at level = 0, we're done, setup the path and exit */
 		if (level == 0) {
@@ -3018,6 +3019,8 @@ int btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,
 			btrfs_item_key_to_cpu(cur, &found_key, slot);
 			goto out;
 		}
+		if (sret && slot > 0)
+			slot--;
 		/*
 		 * check this node pointer against the cache_only and
 		 * min_trans parameters.  If it isn't in cache or is too

commit 7b1287662304c3cb05cb38f5e3e2d69f386e8f10
Author: Josef Bacik <jbacik@redhat.com>
Date:   Thu Jul 24 12:17:14 2008 -0400

    Btrfs: Create orphan inode records to prevent lost files after a crash
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index cdc713062b03..f2a94999c371 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2622,7 +2622,7 @@ int btrfs_insert_empty_items(struct btrfs_trans_handle *trans,
 		total_data += data_size[i];
 	}
 
-	total_size = total_data + (nr - 1) * sizeof(struct btrfs_item);
+	total_size = total_data + (nr * sizeof(struct btrfs_item));
 	ret = btrfs_search_slot(trans, root, cpu_key, path, total_size, 1);
 	if (ret == 0) {
 		return -EEXIST;

commit 0bd40a718444b28793283b70286f9e29d464a189
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Jul 17 12:54:43 2008 -0400

    btrfs_next_leaf: do readahead when skip_locking is turned on
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index bbf9bf374066..cdc713062b03 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -3206,7 +3206,8 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 			free_extent_buffer(next);
 		}
 
-		if (level == 1 && path->locks[1] && path->reada)
+		if (level == 1 && (path->locks[1] || path->skip_locking) &&
+		    path->reada)
 			reada_for_search(root, path, level, slot, 0);
 
 		next = read_node_slot(root, c, slot);

commit 7d9eb12c8739e7dc80c78c6b3596f912ecd8f941
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Jul 8 14:19:17 2008 -0400

    Btrfs: Add locking around volume management (device add/remove/balance)
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index c6759fc1004a..bbf9bf374066 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1251,10 +1251,6 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 	WARN_ON(p->nodes[0] != NULL);
 	WARN_ON(cow && root == root->fs_info->extent_root &&
 		!mutex_is_locked(&root->fs_info->alloc_mutex));
-	WARN_ON(root == root->fs_info->chunk_root &&
-		!mutex_is_locked(&root->fs_info->chunk_mutex));
-	WARN_ON(root == root->fs_info->dev_root &&
-		!mutex_is_locked(&root->fs_info->chunk_mutex));
 	if (ins_len < 0)
 		lowest_unlock = 2;
 again:

commit f9efa9c784aa3b801feb367f72c6867d26fb348e
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Jun 25 16:14:04 2008 -0400

    Btrfs: Reduce contention on the root node
    
    This calls unlock_up sooner in btrfs_search_slot in order to decrease the
    amount of work done with the higher level tree locks held.
    
    Also, it changes btrfs_tree_lock to spin for a big against the page lock
    before scheduling.  This makes a big difference in context switch rate under
    highly contended workloads.
    
    Longer term, a better locking structure is needed than the page lock.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 0cb80f32a9c7..c6759fc1004a 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1313,16 +1313,13 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 				slot = p->slots[level];
 				BUG_ON(btrfs_header_nritems(b) == 1);
 			}
+			unlock_up(p, level, lowest_unlock);
+
 			/* this is only true while dropping a snapshot */
 			if (level == lowest_level) {
-				unlock_up(p, level, lowest_unlock);
 				break;
 			}
 
-			if (should_reada)
-				reada_for_search(root, p, level, slot,
-						 key->objectid);
-
 			blocknr = btrfs_node_blockptr(b, slot);
 			gen = btrfs_node_ptr_generation(b, slot);
 			blocksize = btrfs_level_size(root, level - 1);
@@ -1340,6 +1337,11 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 					btrfs_release_path(NULL, p);
 					if (tmp)
 						free_extent_buffer(tmp);
+					if (should_reada)
+						reada_for_search(root, p,
+								 level, slot,
+								 key->objectid);
+
 					tmp = read_tree_block(root, blocknr,
 							 blocksize, gen);
 					if (tmp)
@@ -1348,12 +1350,15 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 				} else {
 					if (tmp)
 						free_extent_buffer(tmp);
+					if (should_reada)
+						reada_for_search(root, p,
+								 level, slot,
+								 key->objectid);
 					b = read_node_slot(root, b, slot);
 				}
 			}
 			if (!p->skip_locking)
 				btrfs_tree_lock(b);
-			unlock_up(p, level, lowest_unlock);
 		} else {
 			p->slots[level] = slot;
 			if (ins_len > 0 && btrfs_leaf_free_space(root, b) <

commit 3f157a2fd2ad731e1ed9964fecdc5f459f04a4a4
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Jun 25 16:01:31 2008 -0400

    Btrfs: Online btree defragmentation fixes
    
    The btree defragger wasn't making forward progress because the new key wasn't
    being saved by the btrfs_search_forward function.
    
    This also disables the automatic btree defrag, it wasn't scaling well to
    huge filesystems.  The auto-defrag needs to be done differently.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 7f4cc2b88d09..0cb80f32a9c7 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -63,10 +63,9 @@ void btrfs_free_path(struct btrfs_path *p)
 void btrfs_release_path(struct btrfs_root *root, struct btrfs_path *p)
 {
 	int i;
-	int keep = p->keep_locks;
-	int skip = p->skip_locking;
 
 	for (i = 0; i < BTRFS_MAX_LEVEL; i++) {
+		p->slots[i] = 0;
 		if (!p->nodes[i])
 			continue;
 		if (p->locks[i]) {
@@ -74,10 +73,8 @@ void btrfs_release_path(struct btrfs_root *root, struct btrfs_path *p)
 			p->locks[i] = 0;
 		}
 		free_extent_buffer(p->nodes[i]);
+		p->nodes[i] = NULL;
 	}
-	memset(p, 0, sizeof(*p));
-	p->keep_locks = keep;
-	p->skip_locking = skip;
 }
 
 struct extent_buffer *btrfs_root_node(struct btrfs_root *root)
@@ -463,8 +460,6 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 		search_start = cur->start;
 		last_block = cur->start;
 		*last_ret = search_start;
-		if (parent_level == 1)
-			btrfs_clear_buffer_defrag(cur);
 		btrfs_tree_unlock(cur);
 		free_extent_buffer(cur);
 	}
@@ -2969,8 +2964,138 @@ int btrfs_prev_leaf(struct btrfs_root *root, struct btrfs_path *path)
 	return 1;
 }
 
+/*
+ * A helper function to walk down the tree starting at min_key, and looking
+ * for nodes or leaves that are either in cache or have a minimum
+ * transaction id.  This is used by the btree defrag code, but could
+ * also be used to search for blocks that have changed since a given
+ * transaction id.
+ *
+ * This does not cow, but it does stuff the starting key it finds back
+ * into min_key, so you can call btrfs_search_slot with cow=1 on the
+ * key and get a writable path.
+ *
+ * This does lock as it descends, and path->keep_locks should be set
+ * to 1 by the caller.
+ *
+ * This honors path->lowest_level to prevent descent past a given level
+ * of the tree.
+ *
+ * returns zero if something useful was found, < 0 on error and 1 if there
+ * was nothing in the tree that matched the search criteria.
+ */
+int btrfs_search_forward(struct btrfs_root *root, struct btrfs_key *min_key,
+			 struct btrfs_path *path, int cache_only,
+			 u64 min_trans)
+{
+	struct extent_buffer *cur;
+	struct btrfs_key found_key;
+	int slot;
+	u32 nritems;
+	int level;
+	int ret = 1;
+
+again:
+	cur = btrfs_lock_root_node(root);
+	level = btrfs_header_level(cur);
+	path->nodes[level] = cur;
+	path->locks[level] = 1;
+
+	if (btrfs_header_generation(cur) < min_trans) {
+		ret = 1;
+		goto out;
+	}
+	while(1) {
+		nritems = btrfs_header_nritems(cur);
+		level = btrfs_header_level(cur);
+		bin_search(cur, min_key, level, &slot);
+
+		/* at level = 0, we're done, setup the path and exit */
+		if (level == 0) {
+			ret = 0;
+			path->slots[level] = slot;
+			btrfs_item_key_to_cpu(cur, &found_key, slot);
+			goto out;
+		}
+		/*
+		 * check this node pointer against the cache_only and
+		 * min_trans parameters.  If it isn't in cache or is too
+		 * old, skip to the next one.
+		 */
+		while(slot < nritems) {
+			u64 blockptr;
+			u64 gen;
+			struct extent_buffer *tmp;
+			blockptr = btrfs_node_blockptr(cur, slot);
+			gen = btrfs_node_ptr_generation(cur, slot);
+			if (gen < min_trans) {
+				slot++;
+				continue;
+			}
+			if (!cache_only)
+				break;
+
+			tmp = btrfs_find_tree_block(root, blockptr,
+					    btrfs_level_size(root, level - 1));
+
+			if (tmp && btrfs_buffer_uptodate(tmp, gen)) {
+				free_extent_buffer(tmp);
+				break;
+			}
+			if (tmp)
+				free_extent_buffer(tmp);
+			slot++;
+		}
+		/*
+		 * we didn't find a candidate key in this node, walk forward
+		 * and find another one
+		 */
+		if (slot >= nritems) {
+			ret = btrfs_find_next_key(root, path, min_key, level,
+						  cache_only, min_trans);
+			if (ret == 0) {
+				btrfs_release_path(root, path);
+				goto again;
+			} else {
+				goto out;
+			}
+		}
+		/* save our key for returning back */
+		btrfs_node_key_to_cpu(cur, &found_key, slot);
+		path->slots[level] = slot;
+		if (level == path->lowest_level) {
+			ret = 0;
+			unlock_up(path, level, 1);
+			goto out;
+		}
+		cur = read_node_slot(root, cur, slot);
+
+		btrfs_tree_lock(cur);
+		path->locks[level - 1] = 1;
+		path->nodes[level - 1] = cur;
+		unlock_up(path, level, 1);
+	}
+out:
+	if (ret == 0)
+		memcpy(min_key, &found_key, sizeof(found_key));
+	return ret;
+}
+
+/*
+ * this is similar to btrfs_next_leaf, but does not try to preserve
+ * and fixup the path.  It looks for and returns the next key in the
+ * tree based on the current path and the cache_only and min_trans
+ * parameters.
+ *
+ * 0 is returned if another key is found, < 0 if there are any errors
+ * and 1 is returned if there are no higher keys in the tree
+ *
+ * path->keep_locks should be set to 1 on the search made before
+ * calling this function.
+ */
 int btrfs_find_next_key(struct btrfs_root *root, struct btrfs_path *path,
-			struct btrfs_key *key, int lowest_level)
+			struct btrfs_key *key, int lowest_level,
+			int cache_only, u64 min_trans)
 {
 	int level = lowest_level;
 	int slot;
@@ -2982,6 +3107,7 @@ int btrfs_find_next_key(struct btrfs_root *root, struct btrfs_path *path,
 
 		slot = path->slots[level] + 1;
 		c = path->nodes[level];
+next:
 		if (slot >= btrfs_header_nritems(c)) {
 			level++;
 			if (level == BTRFS_MAX_LEVEL) {
@@ -2991,8 +3117,28 @@ int btrfs_find_next_key(struct btrfs_root *root, struct btrfs_path *path,
 		}
 		if (level == 0)
 			btrfs_item_key_to_cpu(c, key, slot);
-		else
+		else {
+			u64 blockptr = btrfs_node_blockptr(c, slot);
+			u64 gen = btrfs_node_ptr_generation(c, slot);
+
+			if (cache_only) {
+				struct extent_buffer *cur;
+				cur = btrfs_find_tree_block(root, blockptr,
+					    btrfs_level_size(root, level - 1));
+				if (!cur || !btrfs_buffer_uptodate(cur, gen)) {
+					slot++;
+					if (cur)
+						free_extent_buffer(cur);
+					goto next;
+				}
+				free_extent_buffer(cur);
+			}
+			if (gen < min_trans) {
+				slot++;
+				goto next;
+			}
 			btrfs_node_key_to_cpu(c, key, slot);
+		}
 		return 0;
 	}
 	return 1;
@@ -3095,6 +3241,12 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 	return 0;
 }
 
+/*
+ * this uses btrfs_prev_leaf to walk backwards in the tree, and keeps
+ * searching until it gets past min_objectid or finds an item of 'type'
+ *
+ * returns 0 if something is found, 1 if nothing was found and < 0 on error
+ */
 int btrfs_previous_item(struct btrfs_root *root,
 			struct btrfs_path *path, u64 min_objectid,
 			int type)

commit e7a84565bcdb239caad29ccbe559ef978090ac7e
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Jun 25 16:01:31 2008 -0400

    Btrfs: Add btree locking to the tree defragmentation code
    
    The online btree defragger is simplified and rewritten to use
    standard btree searches instead of a walk up / down mechanism.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 40f0e0cb804b..7f4cc2b88d09 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -354,7 +354,6 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 		       struct btrfs_key *progress)
 {
 	struct extent_buffer *cur;
-	struct extent_buffer *tmp;
 	u64 blocknr;
 	u64 gen;
 	u64 search_start = *last_ret;
@@ -370,9 +369,6 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 	int progress_passed = 0;
 	struct btrfs_disk_key disk_key;
 
-	/* FIXME this code needs locking */
-	return 0;
-
 	parent_level = btrfs_header_level(parent);
 	if (cache_only && parent_level != 1)
 		return 0;
@@ -454,20 +450,23 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 		if (search_start == 0)
 			search_start = last_block;
 
+		btrfs_tree_lock(cur);
 		err = __btrfs_cow_block(trans, root, cur, parent, i,
-					&tmp, search_start,
+					&cur, search_start,
 					min(16 * blocksize,
 					    (end_slot - i) * blocksize));
 		if (err) {
+			btrfs_tree_unlock(cur);
 			free_extent_buffer(cur);
 			break;
 		}
-		search_start = tmp->start;
-		last_block = tmp->start;
+		search_start = cur->start;
+		last_block = cur->start;
 		*last_ret = search_start;
 		if (parent_level == 1)
-			btrfs_clear_buffer_defrag(tmp);
-		free_extent_buffer(tmp);
+			btrfs_clear_buffer_defrag(cur);
+		btrfs_tree_unlock(cur);
+		free_extent_buffer(cur);
 	}
 	if (parent->map_token) {
 		unmap_extent_buffer(parent, parent->map_token,
@@ -2970,6 +2969,35 @@ int btrfs_prev_leaf(struct btrfs_root *root, struct btrfs_path *path)
 	return 1;
 }
 
+int btrfs_find_next_key(struct btrfs_root *root, struct btrfs_path *path,
+			struct btrfs_key *key, int lowest_level)
+{
+	int level = lowest_level;
+	int slot;
+	struct extent_buffer *c;
+
+	while(level < BTRFS_MAX_LEVEL) {
+		if (!path->nodes[level])
+			return 1;
+
+		slot = path->slots[level] + 1;
+		c = path->nodes[level];
+		if (slot >= btrfs_header_nritems(c)) {
+			level++;
+			if (level == BTRFS_MAX_LEVEL) {
+				return 1;
+			}
+			continue;
+		}
+		if (level == 0)
+			btrfs_item_key_to_cpu(c, key, slot);
+		else
+			btrfs_node_key_to_cpu(c, key, slot);
+		return 0;
+	}
+	return 1;
+}
+
 /*
  * search the tree again to find a leaf with greater keys
  * returns 0 if it found something or 1 if there are no greater leaves.

commit a74a4b97b61beede185b4b3ad359d7d378b0d312
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Jun 25 16:01:31 2008 -0400

    Btrfs: Replace the transaction work queue with kthreads
    
    This creates one kthread for commits and one kthread for
    deleting old snapshots.  All the work queues are removed.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 5edbcc09b3cc..40f0e0cb804b 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1352,6 +1352,8 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 						free_extent_buffer(tmp);
 					goto again;
 				} else {
+					if (tmp)
+						free_extent_buffer(tmp);
 					b = read_node_slot(root, b, slot);
 				}
 			}
@@ -3048,7 +3050,8 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 		free_extent_buffer(c);
 		path->nodes[level] = next;
 		path->slots[level] = 0;
-		path->locks[level] = 1;
+		if (!path->skip_locking)
+			path->locks[level] = 1;
 		if (!level)
 			break;
 		if (level == 1 && path->locks[1] && path->reada)

commit 333db94cdde9e6dfdedab9290d04d812f83e0922
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Jun 25 16:01:30 2008 -0400

    Btrfs: Fix snapshot deletion to release the alloc_mutex much more often.
    
    This lowers the impact of snapshot deletion on the rest of the FS.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index fd68601b60b1..5edbcc09b3cc 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1255,7 +1255,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 	lowest_level = p->lowest_level;
 	WARN_ON(lowest_level && ins_len);
 	WARN_ON(p->nodes[0] != NULL);
-	WARN_ON(root == root->fs_info->extent_root &&
+	WARN_ON(cow && root == root->fs_info->extent_root &&
 		!mutex_is_locked(&root->fs_info->alloc_mutex));
 	WARN_ON(root == root->fs_info->chunk_root &&
 		!mutex_is_locked(&root->fs_info->chunk_mutex));

commit 5cd57b2cbbb06a350df2698314e4e6a80805fc2f
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Jun 25 16:01:30 2008 -0400

    Btrfs: Add a skip_locking parameter to struct path, and make various funcs honor it
    
    Allocations may need to read in block groups from the extent allocation tree,
    which will require a tree search and take locks on the extent allocation
    tree.  But, those locks might already be held in other places, leading
    to deadlocks.
    
    Since the alloc_mutex serializes everything right now, it is safe to
    skip the btree locking while caching block groups.  A better fix will be
    to either create a recursive lock or find a way to back off existing
    locks while caching block groups.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index b8f7aecf68d7..fd68601b60b1 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -64,6 +64,7 @@ void btrfs_release_path(struct btrfs_root *root, struct btrfs_path *p)
 {
 	int i;
 	int keep = p->keep_locks;
+	int skip = p->skip_locking;
 
 	for (i = 0; i < BTRFS_MAX_LEVEL; i++) {
 		if (!p->nodes[i])
@@ -76,6 +77,7 @@ void btrfs_release_path(struct btrfs_root *root, struct btrfs_path *p)
 	}
 	memset(p, 0, sizeof(*p));
 	p->keep_locks = keep;
+	p->skip_locking = skip;
 }
 
 struct extent_buffer *btrfs_root_node(struct btrfs_root *root)
@@ -1262,7 +1264,10 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 	if (ins_len < 0)
 		lowest_unlock = 2;
 again:
-	b = btrfs_lock_root_node(root);
+	if (p->skip_locking)
+		b = btrfs_root_node(root);
+	else
+		b = btrfs_lock_root_node(root);
 
 	while (b) {
 		level = btrfs_header_level(b);
@@ -1282,7 +1287,8 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 			WARN_ON(1);
 		level = btrfs_header_level(b);
 		p->nodes[level] = b;
-		p->locks[level] = 1;
+		if (!p->skip_locking)
+			p->locks[level] = 1;
 		ret = check_block(root, p, level);
 		if (ret)
 			return -1;
@@ -1349,7 +1355,8 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 					b = read_node_slot(root, b, slot);
 				}
 			}
-			btrfs_tree_lock(b);
+			if (!p->skip_locking)
+				btrfs_tree_lock(b);
 			unlock_up(p, level, lowest_unlock);
 		} else {
 			p->slots[level] = slot;
@@ -1392,13 +1399,6 @@ static int fixup_low_keys(struct btrfs_trans_handle *trans,
 			break;
 		t = path->nodes[i];
 		btrfs_set_node_key(t, key, tslot);
-		if (!btrfs_tree_locked(path->nodes[i])) {
-			int ii;
-printk("fixup without lock on level %d\n", btrfs_header_level(path->nodes[i]));
-			for (ii = 0; ii < BTRFS_MAX_LEVEL; ii++) {
-printk("level %d slot %d\n", ii, path->slots[ii]);
-			}
-		}
 		btrfs_mark_buffer_dirty(path->nodes[i]);
 		if (tslot != 0)
 			break;
@@ -3033,8 +3033,10 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 			reada_for_search(root, path, level, slot, 0);
 
 		next = read_node_slot(root, c, slot);
-		WARN_ON(!btrfs_tree_locked(c));
-		btrfs_tree_lock(next);
+		if (!path->skip_locking) {
+			WARN_ON(!btrfs_tree_locked(c));
+			btrfs_tree_lock(next);
+		}
 		break;
 	}
 	path->slots[level] = slot;
@@ -3052,8 +3054,10 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 		if (level == 1 && path->locks[1] && path->reada)
 			reada_for_search(root, path, level, slot, 0);
 		next = read_node_slot(root, next, 0);
-		WARN_ON(!btrfs_tree_locked(path->nodes[level]));
-		btrfs_tree_lock(next);
+		if (!path->skip_locking) {
+			WARN_ON(!btrfs_tree_locked(path->nodes[level]));
+			btrfs_tree_lock(next);
+		}
 	}
 done:
 	unlock_up(path, 0, 1);

commit 168fd7d271d9d8e81ff0b03eb08c36d82670c8a9
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Jun 25 16:01:30 2008 -0400

    Fix btrfs_next_leaf to check for new items after dropping locks
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 9601241e552b..b8f7aecf68d7 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2999,7 +2999,14 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 		return ret;
 
 	nritems = btrfs_header_nritems(path->nodes[0]);
+	/*
+	 * by releasing the path above we dropped all our locks.  A balance
+	 * could have added more items next to the key that used to be
+	 * at the very end of the block.  So, check again here and
+	 * advance the path if there are now more items available.
+	 */
 	if (nritems > 0 && path->slots[0] < nritems - 1) {
+		path->slots[0]++;
 		goto done;
 	}
 

commit 594a24eb0e7fa8413f8b443863be4b7c72bfde9f
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Jun 25 16:01:30 2008 -0400

    Fix btrfs_del_ordered_inode to allow forcing the drop during unlinks
    
    This allows us to delete an unlinked inode with dirty pages from the list
    instead of forcing commit to write these out before deleting the inode.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 1b756fae2799..9601241e552b 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1245,7 +1245,10 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 	int level;
 	int should_reada = p->reada;
 	int lowest_unlock = 1;
+	int blocksize;
 	u8 lowest_level = 0;
+	u64 blocknr;
+	u64 gen;
 
 	lowest_level = p->lowest_level;
 	WARN_ON(lowest_level && ins_len);
@@ -1320,11 +1323,12 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 				reada_for_search(root, p, level, slot,
 						 key->objectid);
 
-			tmp = btrfs_find_tree_block(root,
-					  btrfs_node_blockptr(b, slot),
-					  btrfs_level_size(root, level - 1));
-			if (tmp && btrfs_buffer_uptodate(tmp,
-				   btrfs_node_ptr_generation(b, slot))) {
+			blocknr = btrfs_node_blockptr(b, slot);
+			gen = btrfs_node_ptr_generation(b, slot);
+			blocksize = btrfs_level_size(root, level - 1);
+
+			tmp = btrfs_find_tree_block(root, blocknr, blocksize);
+			if (tmp && btrfs_buffer_uptodate(tmp, gen)) {
 				b = tmp;
 			} else {
 				/*
@@ -1334,6 +1338,10 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 				 */
 				if (level > 1) {
 					btrfs_release_path(NULL, p);
+					if (tmp)
+						free_extent_buffer(tmp);
+					tmp = read_tree_block(root, blocknr,
+							 blocksize, gen);
 					if (tmp)
 						free_extent_buffer(tmp);
 					goto again;

commit 051e1b9f748ae673b7325d3fc049bb838606cffa
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Jun 25 16:01:30 2008 -0400

    Drop locks in btrfs_search_slot when reading a tree block.
    
    One lock per btree block can make for significant congestion if everyone
    has to wait for IO at the high levels of the btree.  This drops
    locks held by a path when doing reads during a tree search.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index dff4da082d06..1b756fae2799 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -63,7 +63,6 @@ void btrfs_free_path(struct btrfs_path *p)
 void btrfs_release_path(struct btrfs_root *root, struct btrfs_path *p)
 {
 	int i;
-	int skip = p->skip_locking;
 	int keep = p->keep_locks;
 
 	for (i = 0; i < BTRFS_MAX_LEVEL; i++) {
@@ -76,7 +75,6 @@ void btrfs_release_path(struct btrfs_root *root, struct btrfs_path *p)
 		free_extent_buffer(p->nodes[i]);
 	}
 	memset(p, 0, sizeof(*p));
-	p->skip_locking = skip;
 	p->keep_locks = keep;
 }
 
@@ -1137,7 +1135,6 @@ static void reada_for_search(struct btrfs_root *root, struct btrfs_path *path,
 		return;
 
 	node = path->nodes[level];
-	WARN_ON(!path->skip_locking && !btrfs_tree_locked(node));
 
 	search = btrfs_node_blockptr(node, slot);
 	blocksize = btrfs_level_size(root, level - 1);
@@ -1192,6 +1189,7 @@ static void unlock_up(struct btrfs_path *path, int level, int lowest_unlock)
 {
 	int i;
 	int skip_level = level;
+	int no_skips = 0;
 	struct extent_buffer *t;
 
 	for (i = level; i < BTRFS_MAX_LEVEL; i++) {
@@ -1199,27 +1197,24 @@ static void unlock_up(struct btrfs_path *path, int level, int lowest_unlock)
 			break;
 		if (!path->locks[i])
 			break;
-		if (path->slots[i] == 0) {
+		if (!no_skips && path->slots[i] == 0) {
 			skip_level = i + 1;
 			continue;
 		}
-		if (path->keep_locks) {
+		if (!no_skips && path->keep_locks) {
 			u32 nritems;
 			t = path->nodes[i];
 			nritems = btrfs_header_nritems(t);
-			if (nritems < 2 || path->slots[i] >= nritems - 2) {
-if (path->keep_locks) {
-//printk("path %p skip level now %d\n", path, skip_level);
-}
+			if (nritems < 1 || path->slots[i] >= nritems - 1) {
 				skip_level = i + 1;
 				continue;
 			}
 		}
+		if (skip_level < i && i >= lowest_unlock)
+			no_skips = 1;
+
 		t = path->nodes[i];
 		if (i >= lowest_unlock && i > skip_level && path->locks[i]) {
-if (path->keep_locks) {
-//printk("path %p unlocking level %d slot %d nritems %d skip_level %d\n", path, i, path->slots[i], btrfs_header_nritems(t), skip_level);
-}
 			btrfs_tree_unlock(t);
 			path->locks[i] = 0;
 		}
@@ -1244,6 +1239,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 		      ins_len, int cow)
 {
 	struct extent_buffer *b;
+	struct extent_buffer *tmp;
 	int slot;
 	int ret;
 	int level;
@@ -1263,10 +1259,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 	if (ins_len < 0)
 		lowest_unlock = 2;
 again:
-	if (!p->skip_locking)
-		b = btrfs_lock_root_node(root);
-	else
-		b = btrfs_root_node(root);
+	b = btrfs_lock_root_node(root);
 
 	while (b) {
 		level = btrfs_header_level(b);
@@ -1286,8 +1279,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 			WARN_ON(1);
 		level = btrfs_header_level(b);
 		p->nodes[level] = b;
-		if (!p->skip_locking)
-			p->locks[level] = 1;
+		p->locks[level] = 1;
 		ret = check_block(root, p, level);
 		if (ret)
 			return -1;
@@ -1328,10 +1320,29 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 				reada_for_search(root, p, level, slot,
 						 key->objectid);
 
-			b = read_node_slot(root, b, slot);
-			if (!p->skip_locking)
-				btrfs_tree_lock(b);
-			unlock_up(p, level + 1, lowest_unlock);
+			tmp = btrfs_find_tree_block(root,
+					  btrfs_node_blockptr(b, slot),
+					  btrfs_level_size(root, level - 1));
+			if (tmp && btrfs_buffer_uptodate(tmp,
+				   btrfs_node_ptr_generation(b, slot))) {
+				b = tmp;
+			} else {
+				/*
+				 * reduce lock contention at high levels
+				 * of the btree by dropping locks before
+				 * we read.
+				 */
+				if (level > 1) {
+					btrfs_release_path(NULL, p);
+					if (tmp)
+						free_extent_buffer(tmp);
+					goto again;
+				} else {
+					b = read_node_slot(root, b, slot);
+				}
+			}
+			btrfs_tree_lock(b);
+			unlock_up(p, level, lowest_unlock);
 		} else {
 			p->slots[level] = slot;
 			if (ins_len > 0 && btrfs_leaf_free_space(root, b) <
@@ -3007,17 +3018,8 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 			reada_for_search(root, path, level, slot, 0);
 
 		next = read_node_slot(root, c, slot);
-		if (!path->skip_locking) {
-			if (!btrfs_tree_locked(c)) {
-				int i;
-				WARN_ON(1);
-printk("path %p no lock on level %d\n", path, level);
-for (i = 0; i < BTRFS_MAX_LEVEL; i++) {
-printk("path %p level %d slot %d nritems %d\n", path, i, path->slots[i], btrfs_header_nritems(path->nodes[i]));
-}
-			}
-			btrfs_tree_lock(next);
-		}
+		WARN_ON(!btrfs_tree_locked(c));
+		btrfs_tree_lock(next);
 		break;
 	}
 	path->slots[level] = slot;
@@ -3035,10 +3037,8 @@ printk("path %p level %d slot %d nritems %d\n", path, i, path->slots[i], btrfs_h
 		if (level == 1 && path->locks[1] && path->reada)
 			reada_for_search(root, path, level, slot, 0);
 		next = read_node_slot(root, next, 0);
-		if (!path->skip_locking) {
-			WARN_ON(!btrfs_tree_locked(path->nodes[level]));
-			btrfs_tree_lock(next);
-		}
+		WARN_ON(!btrfs_tree_locked(path->nodes[level]));
+		btrfs_tree_lock(next);
 	}
 done:
 	unlock_up(path, 0, 1);

commit a213501153fd66e2359e091b1612841305ba6551
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Jun 25 16:01:30 2008 -0400

    Btrfs: Replace the big fs_mutex with a collection of other locks
    
    Extent alloctions are still protected by a large alloc_mutex.
    Objectid allocations are covered by a objectid mutex
    Other btree operations are protected by a lock on individual btree nodes
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 75625c68fdd8..dff4da082d06 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -63,6 +63,9 @@ void btrfs_free_path(struct btrfs_path *p)
 void btrfs_release_path(struct btrfs_root *root, struct btrfs_path *p)
 {
 	int i;
+	int skip = p->skip_locking;
+	int keep = p->keep_locks;
+
 	for (i = 0; i < BTRFS_MAX_LEVEL; i++) {
 		if (!p->nodes[i])
 			continue;
@@ -73,6 +76,8 @@ void btrfs_release_path(struct btrfs_root *root, struct btrfs_path *p)
 		free_extent_buffer(p->nodes[i]);
 	}
 	memset(p, 0, sizeof(*p));
+	p->skip_locking = skip;
+	p->keep_locks = keep;
 }
 
 struct extent_buffer *btrfs_root_node(struct btrfs_root *root)
@@ -1202,13 +1207,19 @@ static void unlock_up(struct btrfs_path *path, int level, int lowest_unlock)
 			u32 nritems;
 			t = path->nodes[i];
 			nritems = btrfs_header_nritems(t);
-			if (path->slots[i] >= nritems - 1) {
+			if (nritems < 2 || path->slots[i] >= nritems - 2) {
+if (path->keep_locks) {
+//printk("path %p skip level now %d\n", path, skip_level);
+}
 				skip_level = i + 1;
 				continue;
 			}
 		}
 		t = path->nodes[i];
 		if (i >= lowest_unlock && i > skip_level && path->locks[i]) {
+if (path->keep_locks) {
+//printk("path %p unlocking level %d slot %d nritems %d skip_level %d\n", path, i, path->slots[i], btrfs_header_nritems(t), skip_level);
+}
 			btrfs_tree_unlock(t);
 			path->locks[i] = 0;
 		}
@@ -1243,7 +1254,6 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 	lowest_level = p->lowest_level;
 	WARN_ON(lowest_level && ins_len);
 	WARN_ON(p->nodes[0] != NULL);
-	// WARN_ON(!mutex_is_locked(&root->fs_info->fs_mutex));
 	WARN_ON(root == root->fs_info->extent_root &&
 		!mutex_is_locked(&root->fs_info->alloc_mutex));
 	WARN_ON(root == root->fs_info->chunk_root &&
@@ -1321,7 +1331,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 			b = read_node_slot(root, b, slot);
 			if (!p->skip_locking)
 				btrfs_tree_lock(b);
-			unlock_up(p, level, lowest_unlock);
+			unlock_up(p, level + 1, lowest_unlock);
 		} else {
 			p->slots[level] = slot;
 			if (ins_len > 0 && btrfs_leaf_free_space(root, b) <
@@ -1804,6 +1814,8 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 	if (slot >= btrfs_header_nritems(upper) - 1)
 		return 1;
 
+	WARN_ON(!btrfs_tree_locked(path->nodes[1]));
+
 	right = read_node_slot(root, upper, slot + 1);
 	btrfs_tree_lock(right);
 	free_space = btrfs_leaf_free_space(root, right);
@@ -1981,6 +1993,8 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 		return 1;
 	}
 
+	WARN_ON(!btrfs_tree_locked(path->nodes[1]));
+
 	left = read_node_slot(root, path->nodes[1], slot - 1);
 	btrfs_tree_lock(left);
 	free_space = btrfs_leaf_free_space(root, left);
@@ -2957,15 +2971,16 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 
 	btrfs_item_key_to_cpu(path->nodes[0], &key, nritems - 1);
 
-	path->keep_locks = 1;
 	btrfs_release_path(root, path);
+	path->keep_locks = 1;
 	ret = btrfs_search_slot(NULL, root, &key, path, 0, 0);
 	path->keep_locks = 0;
 
 	if (ret < 0)
 		return ret;
 
-	if (path->slots[0] < nritems - 1) {
+	nritems = btrfs_header_nritems(path->nodes[0]);
+	if (nritems > 0 && path->slots[0] < nritems - 1) {
 		goto done;
 	}
 
@@ -2992,8 +3007,17 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 			reada_for_search(root, path, level, slot, 0);
 
 		next = read_node_slot(root, c, slot);
-		if (!path->skip_locking)
+		if (!path->skip_locking) {
+			if (!btrfs_tree_locked(c)) {
+				int i;
+				WARN_ON(1);
+printk("path %p no lock on level %d\n", path, level);
+for (i = 0; i < BTRFS_MAX_LEVEL; i++) {
+printk("path %p level %d slot %d nritems %d\n", path, i, path->slots[i], btrfs_header_nritems(path->nodes[i]));
+}
+			}
 			btrfs_tree_lock(next);
+		}
 		break;
 	}
 	path->slots[level] = slot;
@@ -3011,8 +3035,10 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 		if (level == 1 && path->locks[1] && path->reada)
 			reada_for_search(root, path, level, slot, 0);
 		next = read_node_slot(root, next, 0);
-		if (!path->skip_locking)
+		if (!path->skip_locking) {
+			WARN_ON(!btrfs_tree_locked(path->nodes[level]));
 			btrfs_tree_lock(next);
+		}
 	}
 done:
 	unlock_up(path, 0, 1);

commit 925baeddc5b0764a53f2214a1253251bab0e0324
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Jun 25 16:01:30 2008 -0400

    Btrfs: Start btree concurrency work.
    
    The allocation trees and the chunk trees are serialized via their own
    dedicated mutexes.  This means allocation location is still not very
    fine grained.
    
    The main FS btree is protected by locks on each block in the btree.  Locks
    are taken top / down, and as processing finishes on a given level of the
    tree, the lock is released after locking the lower level.
    
    The end result of a search is now a path where only the lowest level
    is locked.  Releasing or freeing the path drops any locks held.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 1d404bde3cfa..75625c68fdd8 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -21,6 +21,7 @@
 #include "disk-io.h"
 #include "transaction.h"
 #include "print-tree.h"
+#include "locking.h"
 
 static int split_node(struct btrfs_trans_handle *trans, struct btrfs_root
 		      *root, struct btrfs_path *path, int level);
@@ -64,12 +65,47 @@ void btrfs_release_path(struct btrfs_root *root, struct btrfs_path *p)
 	int i;
 	for (i = 0; i < BTRFS_MAX_LEVEL; i++) {
 		if (!p->nodes[i])
-			break;
+			continue;
+		if (p->locks[i]) {
+			btrfs_tree_unlock(p->nodes[i]);
+			p->locks[i] = 0;
+		}
 		free_extent_buffer(p->nodes[i]);
 	}
 	memset(p, 0, sizeof(*p));
 }
 
+struct extent_buffer *btrfs_root_node(struct btrfs_root *root)
+{
+	struct extent_buffer *eb;
+	spin_lock(&root->node_lock);
+	eb = root->node;
+	extent_buffer_get(eb);
+	spin_unlock(&root->node_lock);
+	return eb;
+}
+
+struct extent_buffer *btrfs_lock_root_node(struct btrfs_root *root)
+{
+	struct extent_buffer *eb;
+
+	while(1) {
+		eb = btrfs_root_node(root);
+		btrfs_tree_lock(eb);
+
+		spin_lock(&root->node_lock);
+		if (eb == root->node) {
+			spin_unlock(&root->node_lock);
+			break;
+		}
+		spin_unlock(&root->node_lock);
+
+		btrfs_tree_unlock(eb);
+		free_extent_buffer(eb);
+	}
+	return eb;
+}
+
 static void add_root_to_dirty_list(struct btrfs_root *root)
 {
 	if (root->track_dirty && list_empty(&root->dirty_list)) {
@@ -111,7 +147,7 @@ int btrfs_copy_root(struct btrfs_trans_handle *trans,
 	} else {
 		first_key.objectid = 0;
 	}
-	cow = __btrfs_alloc_free_block(trans, new_root, buf->len,
+	cow = btrfs_alloc_free_block(trans, new_root, buf->len,
 				       new_root_objectid,
 				       trans->transid, first_key.objectid,
 				       level, buf->start, 0);
@@ -151,8 +187,14 @@ int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 	int ret = 0;
 	int different_trans = 0;
 	int level;
+	int unlock_orig = 0;
 	struct btrfs_key first_key;
 
+	if (*cow_ret == buf)
+		unlock_orig = 1;
+
+	WARN_ON(!btrfs_tree_locked(buf));
+
 	if (root->ref_cows) {
 		root_gen = trans->transid;
 	} else {
@@ -172,7 +214,7 @@ int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 	} else {
 		first_key.objectid = 0;
 	}
-	cow = __btrfs_alloc_free_block(trans, root, buf->len,
+	cow = btrfs_alloc_free_block(trans, root, buf->len,
 				     root->root_key.objectid,
 				     root_gen, first_key.objectid, level,
 				     search_start, empty_size);
@@ -196,9 +238,14 @@ int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 	}
 
 	if (buf == root->node) {
+		WARN_ON(parent && parent != buf);
 		root_gen = btrfs_header_generation(buf);
+
+		spin_lock(&root->node_lock);
 		root->node = cow;
 		extent_buffer_get(cow);
+		spin_unlock(&root->node_lock);
+
 		if (buf != root->commit_root) {
 			btrfs_free_extent(trans, root, buf->start,
 					  buf->len, root->root_key.objectid,
@@ -219,6 +266,8 @@ int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 				  btrfs_header_owner(parent), root_gen,
 				  0, 0, 1);
 	}
+	if (unlock_orig)
+		btrfs_tree_unlock(buf);
 	free_extent_buffer(buf);
 	btrfs_mark_buffer_dirty(cow);
 	*cow_ret = cow;
@@ -316,6 +365,9 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 	int progress_passed = 0;
 	struct btrfs_disk_key disk_key;
 
+	/* FIXME this code needs locking */
+	return 0;
+
 	parent_level = btrfs_header_level(parent);
 	if (cache_only && parent_level != 1)
 		return 0;
@@ -729,6 +781,7 @@ static int balance_level(struct btrfs_trans_handle *trans,
 		return 0;
 
 	mid = path->nodes[level];
+	WARN_ON(!path->locks[level]);
 	WARN_ON(btrfs_header_generation(mid) != trans->transid);
 
 	orig_ptr = btrfs_node_blockptr(mid, orig_slot);
@@ -749,14 +802,21 @@ static int balance_level(struct btrfs_trans_handle *trans,
 
 		/* promote the child to a root */
 		child = read_node_slot(root, mid, 0);
+		btrfs_tree_lock(child);
 		BUG_ON(!child);
 		ret = btrfs_cow_block(trans, root, child, mid, 0, &child);
 		BUG_ON(ret);
 
+		spin_lock(&root->node_lock);
 		root->node = child;
+		spin_unlock(&root->node_lock);
+
 		add_root_to_dirty_list(root);
+		btrfs_tree_unlock(child);
+		path->locks[level] = 0;
 		path->nodes[level] = NULL;
 		clean_tree_block(trans, root, mid);
+		btrfs_tree_unlock(mid);
 		/* once for the path */
 		free_extent_buffer(mid);
 		ret = btrfs_free_extent(trans, root, mid->start, mid->len,
@@ -775,6 +835,7 @@ static int balance_level(struct btrfs_trans_handle *trans,
 
 	left = read_node_slot(root, parent, pslot - 1);
 	if (left) {
+		btrfs_tree_lock(left);
 		wret = btrfs_cow_block(trans, root, left,
 				       parent, pslot - 1, &left);
 		if (wret) {
@@ -784,6 +845,7 @@ static int balance_level(struct btrfs_trans_handle *trans,
 	}
 	right = read_node_slot(root, parent, pslot + 1);
 	if (right) {
+		btrfs_tree_lock(right);
 		wret = btrfs_cow_block(trans, root, right,
 				       parent, pslot + 1, &right);
 		if (wret) {
@@ -815,6 +877,7 @@ static int balance_level(struct btrfs_trans_handle *trans,
 			u32 blocksize = right->len;
 
 			clean_tree_block(trans, root, right);
+			btrfs_tree_unlock(right);
 			free_extent_buffer(right);
 			right = NULL;
 			wret = del_ptr(trans, root, path, level + 1, pslot +
@@ -862,7 +925,9 @@ static int balance_level(struct btrfs_trans_handle *trans,
 		u64 root_gen = btrfs_header_generation(parent);
 		u64 bytenr = mid->start;
 		u32 blocksize = mid->len;
+
 		clean_tree_block(trans, root, mid);
+		btrfs_tree_unlock(mid);
 		free_extent_buffer(mid);
 		mid = NULL;
 		wret = del_ptr(trans, root, path, level + 1, pslot);
@@ -885,11 +950,14 @@ static int balance_level(struct btrfs_trans_handle *trans,
 	if (left) {
 		if (btrfs_header_nritems(left) > orig_slot) {
 			extent_buffer_get(left);
+			/* left was locked after cow */
 			path->nodes[level] = left;
 			path->slots[level + 1] -= 1;
 			path->slots[level] = orig_slot;
-			if (mid)
+			if (mid) {
+				btrfs_tree_unlock(mid);
 				free_extent_buffer(mid);
+			}
 		} else {
 			orig_slot -= btrfs_header_nritems(left);
 			path->slots[level] = orig_slot;
@@ -901,10 +969,15 @@ static int balance_level(struct btrfs_trans_handle *trans,
 	    btrfs_node_blockptr(path->nodes[level], path->slots[level]))
 		BUG();
 enospc:
-	if (right)
+	if (right) {
+		btrfs_tree_unlock(right);
 		free_extent_buffer(right);
-	if (left)
+	}
+	if (left) {
+		if (path->nodes[level] != left)
+			btrfs_tree_unlock(left);
 		free_extent_buffer(left);
+	}
 	return ret;
 }
 
@@ -942,6 +1015,8 @@ static int noinline push_nodes_for_insert(struct btrfs_trans_handle *trans,
 	/* first, try to make some room in the middle buffer */
 	if (left) {
 		u32 left_nr;
+
+		btrfs_tree_lock(left);
 		left_nr = btrfs_header_nritems(left);
 		if (left_nr >= BTRFS_NODEPTRS_PER_BLOCK(root) - 1) {
 			wret = 1;
@@ -967,24 +1042,28 @@ static int noinline push_nodes_for_insert(struct btrfs_trans_handle *trans,
 				path->nodes[level] = left;
 				path->slots[level + 1] -= 1;
 				path->slots[level] = orig_slot;
+				btrfs_tree_unlock(mid);
 				free_extent_buffer(mid);
 			} else {
 				orig_slot -=
 					btrfs_header_nritems(left);
 				path->slots[level] = orig_slot;
+				btrfs_tree_unlock(left);
 				free_extent_buffer(left);
 			}
 			return 0;
 		}
+		btrfs_tree_unlock(left);
 		free_extent_buffer(left);
 	}
-	right= read_node_slot(root, parent, pslot + 1);
+	right = read_node_slot(root, parent, pslot + 1);
 
 	/*
 	 * then try to empty the right most buffer into the middle
 	 */
 	if (right) {
 		u32 right_nr;
+		btrfs_tree_lock(right);
 		right_nr = btrfs_header_nritems(right);
 		if (right_nr >= BTRFS_NODEPTRS_PER_BLOCK(root) - 1) {
 			wret = 1;
@@ -1013,12 +1092,15 @@ static int noinline push_nodes_for_insert(struct btrfs_trans_handle *trans,
 				path->slots[level + 1] += 1;
 				path->slots[level] = orig_slot -
 					btrfs_header_nritems(mid);
+				btrfs_tree_unlock(mid);
 				free_extent_buffer(mid);
 			} else {
+				btrfs_tree_unlock(right);
 				free_extent_buffer(right);
 			}
 			return 0;
 		}
+		btrfs_tree_unlock(right);
 		free_extent_buffer(right);
 	}
 	return 1;
@@ -1050,6 +1132,8 @@ static void reada_for_search(struct btrfs_root *root, struct btrfs_path *path,
 		return;
 
 	node = path->nodes[level];
+	WARN_ON(!path->skip_locking && !btrfs_tree_locked(node));
+
 	search = btrfs_node_blockptr(node, slot);
 	blocksize = btrfs_level_size(root, level - 1);
 	eb = btrfs_find_tree_block(root, search, blocksize);
@@ -1098,6 +1182,39 @@ static void reada_for_search(struct btrfs_root *root, struct btrfs_path *path,
 			highest_read = search;
 	}
 }
+
+static void unlock_up(struct btrfs_path *path, int level, int lowest_unlock)
+{
+	int i;
+	int skip_level = level;
+	struct extent_buffer *t;
+
+	for (i = level; i < BTRFS_MAX_LEVEL; i++) {
+		if (!path->nodes[i])
+			break;
+		if (!path->locks[i])
+			break;
+		if (path->slots[i] == 0) {
+			skip_level = i + 1;
+			continue;
+		}
+		if (path->keep_locks) {
+			u32 nritems;
+			t = path->nodes[i];
+			nritems = btrfs_header_nritems(t);
+			if (path->slots[i] >= nritems - 1) {
+				skip_level = i + 1;
+				continue;
+			}
+		}
+		t = path->nodes[i];
+		if (i >= lowest_unlock && i > skip_level && path->locks[i]) {
+			btrfs_tree_unlock(t);
+			path->locks[i] = 0;
+		}
+	}
+}
+
 /*
  * look for key in the tree.  path is filled in with nodes along the way
  * if key is found, we return zero and you can find the item in the leaf
@@ -1120,15 +1237,27 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 	int ret;
 	int level;
 	int should_reada = p->reada;
+	int lowest_unlock = 1;
 	u8 lowest_level = 0;
 
 	lowest_level = p->lowest_level;
 	WARN_ON(lowest_level && ins_len);
 	WARN_ON(p->nodes[0] != NULL);
-	WARN_ON(!mutex_is_locked(&root->fs_info->fs_mutex));
+	// WARN_ON(!mutex_is_locked(&root->fs_info->fs_mutex));
+	WARN_ON(root == root->fs_info->extent_root &&
+		!mutex_is_locked(&root->fs_info->alloc_mutex));
+	WARN_ON(root == root->fs_info->chunk_root &&
+		!mutex_is_locked(&root->fs_info->chunk_mutex));
+	WARN_ON(root == root->fs_info->dev_root &&
+		!mutex_is_locked(&root->fs_info->chunk_mutex));
+	if (ins_len < 0)
+		lowest_unlock = 2;
 again:
-	b = root->node;
-	extent_buffer_get(b);
+	if (!p->skip_locking)
+		b = btrfs_lock_root_node(root);
+	else
+		b = btrfs_root_node(root);
+
 	while (b) {
 		level = btrfs_header_level(b);
 		if (cow) {
@@ -1147,9 +1276,12 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 			WARN_ON(1);
 		level = btrfs_header_level(b);
 		p->nodes[level] = b;
+		if (!p->skip_locking)
+			p->locks[level] = 1;
 		ret = check_block(root, p, level);
 		if (ret)
 			return -1;
+
 		ret = bin_search(b, key, level, &slot);
 		if (level != 0) {
 			if (ret && slot > 0)
@@ -1177,14 +1309,19 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 				BUG_ON(btrfs_header_nritems(b) == 1);
 			}
 			/* this is only true while dropping a snapshot */
-			if (level == lowest_level)
+			if (level == lowest_level) {
+				unlock_up(p, level, lowest_unlock);
 				break;
+			}
 
 			if (should_reada)
 				reada_for_search(root, p, level, slot,
 						 key->objectid);
 
 			b = read_node_slot(root, b, slot);
+			if (!p->skip_locking)
+				btrfs_tree_lock(b);
+			unlock_up(p, level, lowest_unlock);
 		} else {
 			p->slots[level] = slot;
 			if (ins_len > 0 && btrfs_leaf_free_space(root, b) <
@@ -1195,6 +1332,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 				if (sret)
 					return sret;
 			}
+			unlock_up(p, level, lowest_unlock);
 			return ret;
 		}
 	}
@@ -1225,6 +1363,13 @@ static int fixup_low_keys(struct btrfs_trans_handle *trans,
 			break;
 		t = path->nodes[i];
 		btrfs_set_node_key(t, key, tslot);
+		if (!btrfs_tree_locked(path->nodes[i])) {
+			int ii;
+printk("fixup without lock on level %d\n", btrfs_header_level(path->nodes[i]));
+			for (ii = 0; ii < BTRFS_MAX_LEVEL; ii++) {
+printk("level %d slot %d\n", ii, path->slots[ii]);
+			}
+		}
 		btrfs_mark_buffer_dirty(path->nodes[i]);
 		if (tslot != 0)
 			break;
@@ -1370,6 +1515,7 @@ static int noinline insert_new_root(struct btrfs_trans_handle *trans,
 	u64 lower_gen;
 	struct extent_buffer *lower;
 	struct extent_buffer *c;
+	struct extent_buffer *old;
 	struct btrfs_disk_key lower_key;
 
 	BUG_ON(path->nodes[level]);
@@ -1386,12 +1532,13 @@ static int noinline insert_new_root(struct btrfs_trans_handle *trans,
 	else
 		btrfs_node_key(lower, &lower_key, 0);
 
-	c = __btrfs_alloc_free_block(trans, root, root->nodesize,
+	c = btrfs_alloc_free_block(trans, root, root->nodesize,
 				   root->root_key.objectid,
 				   root_gen, lower_key.objectid, level,
 				   root->node->start, 0);
 	if (IS_ERR(c))
 		return PTR_ERR(c);
+
 	memset_extent_buffer(c, 0, 0, root->nodesize);
 	btrfs_set_header_nritems(c, 1);
 	btrfs_set_header_level(c, level);
@@ -1416,23 +1563,31 @@ static int noinline insert_new_root(struct btrfs_trans_handle *trans,
 
 	btrfs_mark_buffer_dirty(c);
 
-	/* the super has an extra ref to root->node */
-	free_extent_buffer(root->node);
+	spin_lock(&root->node_lock);
+	old = root->node;
 	root->node = c;
+	spin_unlock(&root->node_lock);
+
+	/* the super has an extra ref to root->node */
+	free_extent_buffer(old);
+
 	add_root_to_dirty_list(root);
 	extent_buffer_get(c);
 	path->nodes[level] = c;
+	path->locks[level] = 1;
 	path->slots[level] = 0;
 
 	if (root->ref_cows && lower_gen != trans->transid) {
 		struct btrfs_path *back_path = btrfs_alloc_path();
 		int ret;
+		mutex_lock(&root->fs_info->alloc_mutex);
 		ret = btrfs_insert_extent_backref(trans,
 						  root->fs_info->extent_root,
 						  path, lower->start,
 						  root->root_key.objectid,
 						  trans->transid, 0, 0);
 		BUG_ON(ret);
+		mutex_unlock(&root->fs_info->alloc_mutex);
 		btrfs_free_path(back_path);
 	}
 	return 0;
@@ -1521,7 +1676,7 @@ static int split_node(struct btrfs_trans_handle *trans, struct btrfs_root
 		root_gen = 0;
 
 	btrfs_node_key(c, &disk_key, 0);
-	split = __btrfs_alloc_free_block(trans, root, root->nodesize,
+	split = btrfs_alloc_free_block(trans, root, root->nodesize,
 					 root->root_key.objectid,
 					 root_gen,
 					 btrfs_disk_key_objectid(&disk_key),
@@ -1564,10 +1719,12 @@ static int split_node(struct btrfs_trans_handle *trans, struct btrfs_root
 
 	if (path->slots[level] >= mid) {
 		path->slots[level] -= mid;
+		btrfs_tree_unlock(c);
 		free_extent_buffer(c);
 		path->nodes[level] = split;
 		path->slots[level + 1] += 1;
 	} else {
+		btrfs_tree_unlock(split);
 		free_extent_buffer(split);
 	}
 	return ret;
@@ -1648,30 +1805,24 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 		return 1;
 
 	right = read_node_slot(root, upper, slot + 1);
+	btrfs_tree_lock(right);
 	free_space = btrfs_leaf_free_space(root, right);
-	if (free_space < data_size + sizeof(struct btrfs_item)) {
-		free_extent_buffer(right);
-		return 1;
-	}
+	if (free_space < data_size + sizeof(struct btrfs_item))
+		goto out_unlock;
 
 	/* cow and double check */
 	ret = btrfs_cow_block(trans, root, right, upper,
 			      slot + 1, &right);
-	if (ret) {
-		free_extent_buffer(right);
-		return 1;
-	}
+	if (ret)
+		goto out_unlock;
+
 	free_space = btrfs_leaf_free_space(root, right);
-	if (free_space < data_size + sizeof(struct btrfs_item)) {
-		free_extent_buffer(right);
-		return 1;
-	}
+	if (free_space < data_size + sizeof(struct btrfs_item))
+		goto out_unlock;
 
 	left_nritems = btrfs_header_nritems(left);
-	if (left_nritems == 0) {
-		free_extent_buffer(right);
-		return 1;
-	}
+	if (left_nritems == 0)
+		goto out_unlock;
 
 	if (empty)
 		nr = 0;
@@ -1707,10 +1858,8 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 		left->map_token = NULL;
 	}
 
-	if (push_items == 0) {
-		free_extent_buffer(right);
-		return 1;
-	}
+	if (push_items == 0)
+		goto out_unlock;
 
 	if (!empty && push_items == left_nritems)
 		WARN_ON(1);
@@ -1778,14 +1927,24 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 	/* then fixup the leaf pointer in the path */
 	if (path->slots[0] >= left_nritems) {
 		path->slots[0] -= left_nritems;
+		if (btrfs_header_nritems(path->nodes[0]) == 0)
+			clean_tree_block(trans, root, path->nodes[0]);
+		btrfs_tree_unlock(path->nodes[0]);
 		free_extent_buffer(path->nodes[0]);
 		path->nodes[0] = right;
 		path->slots[1] += 1;
 	} else {
+		btrfs_tree_unlock(right);
 		free_extent_buffer(right);
 	}
 	return 0;
+
+out_unlock:
+	btrfs_tree_unlock(right);
+	free_extent_buffer(right);
+	return 1;
 }
+
 /*
  * push some data in the path leaf to the left, trying to free up at
  * least data_size bytes.  returns zero if the push worked, nonzero otherwise
@@ -1823,10 +1982,11 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 	}
 
 	left = read_node_slot(root, path->nodes[1], slot - 1);
+	btrfs_tree_lock(left);
 	free_space = btrfs_leaf_free_space(root, left);
 	if (free_space < data_size + sizeof(struct btrfs_item)) {
-		free_extent_buffer(left);
-		return 1;
+		ret = 1;
+		goto out;
 	}
 
 	/* cow and double check */
@@ -1834,14 +1994,14 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 			      path->nodes[1], slot - 1, &left);
 	if (ret) {
 		/* we hit -ENOSPC, but it isn't fatal here */
-		free_extent_buffer(left);
-		return 1;
+		ret = 1;
+		goto out;
 	}
 
 	free_space = btrfs_leaf_free_space(root, left);
 	if (free_space < data_size + sizeof(struct btrfs_item)) {
-		free_extent_buffer(left);
-		return 1;
+		ret = 1;
+		goto out;
 	}
 
 	if (empty)
@@ -1876,8 +2036,8 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 	}
 
 	if (push_items == 0) {
-		free_extent_buffer(left);
-		return 1;
+		ret = 1;
+		goto out;
 	}
 	if (!empty && push_items == btrfs_header_nritems(right))
 		WARN_ON(1);
@@ -1975,15 +2135,23 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 	/* then fixup the leaf pointer in the path */
 	if (path->slots[0] < push_items) {
 		path->slots[0] += old_left_nritems;
+		if (btrfs_header_nritems(path->nodes[0]) == 0)
+			clean_tree_block(trans, root, path->nodes[0]);
+		btrfs_tree_unlock(path->nodes[0]);
 		free_extent_buffer(path->nodes[0]);
 		path->nodes[0] = left;
 		path->slots[1] -= 1;
 	} else {
+		btrfs_tree_unlock(left);
 		free_extent_buffer(left);
 		path->slots[0] -= push_items;
 	}
 	BUG_ON(path->slots[0] < 0);
 	return ret;
+out:
+	btrfs_tree_unlock(left);
+	free_extent_buffer(left);
+	return ret;
 }
 
 /*
@@ -2052,7 +2220,7 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 
 	btrfs_item_key(l, &disk_key, 0);
 
-	right = __btrfs_alloc_free_block(trans, root, root->leafsize,
+	right = btrfs_alloc_free_block(trans, root, root->leafsize,
 					 root->root_key.objectid,
 					 root_gen, disk_key.objectid, 0,
 					 l->start, 0);
@@ -2085,6 +2253,8 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 						  path->slots[1] + 1, 1);
 				if (wret)
 					ret = wret;
+
+				btrfs_tree_unlock(path->nodes[0]);
 				free_extent_buffer(path->nodes[0]);
 				path->nodes[0] = right;
 				path->slots[0] = 0;
@@ -2111,6 +2281,7 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 						  path->slots[1], 1);
 				if (wret)
 					ret = wret;
+				btrfs_tree_unlock(path->nodes[0]);
 				free_extent_buffer(path->nodes[0]);
 				path->nodes[0] = right;
 				path->slots[0] = 0;
@@ -2184,12 +2355,15 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 	BUG_ON(path->slots[0] != slot);
 
 	if (mid <= slot) {
+		btrfs_tree_unlock(path->nodes[0]);
 		free_extent_buffer(path->nodes[0]);
 		path->nodes[0] = right;
 		path->slots[0] -= mid;
 		path->slots[1] += 1;
-	} else
+	} else {
+		btrfs_tree_unlock(right);
 		free_extent_buffer(right);
+	}
 
 	BUG_ON(path->slots[0] < 0);
 
@@ -2418,10 +2592,6 @@ int btrfs_insert_empty_items(struct btrfs_trans_handle *trans,
 		total_data += data_size[i];
 	}
 
-	/* create a root if there isn't one */
-	if (!root->node)
-		BUG();
-
 	total_size = total_data + (nr - 1) * sizeof(struct btrfs_item);
 	ret = btrfs_search_slot(trans, root, cpu_key, path, total_size, 1);
 	if (ret == 0) {
@@ -2516,7 +2686,6 @@ int btrfs_insert_empty_items(struct btrfs_trans_handle *trans,
 		btrfs_print_leaf(root, leaf);
 		BUG();
 	}
-
 out:
 	return ret;
 }
@@ -2655,7 +2824,6 @@ int btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 			btrfs_set_header_level(leaf, 0);
 		} else {
 			u64 root_gen = btrfs_header_generation(path->nodes[1]);
-			clean_tree_block(trans, root, leaf);
 			wret = del_ptr(trans, root, path, 1, path->slots[1]);
 			if (wret)
 				ret = wret;
@@ -2706,8 +2874,6 @@ int btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 				root_gen = btrfs_header_generation(
 							   path->nodes[1]);
 
-				clean_tree_block(trans, root, leaf);
-
 				wret = del_ptr(trans, root, path, 1, slot);
 				if (wret)
 					ret = wret;
@@ -2720,7 +2886,13 @@ int btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 				if (wret)
 					ret = wret;
 			} else {
-				btrfs_mark_buffer_dirty(leaf);
+				/* if we're still in the path, make sure
+				 * we're dirty.  Otherwise, one of the
+				 * push_leaf functions must have already
+				 * dirtied this buffer
+				 */
+				if (path->nodes[0] == leaf)
+					btrfs_mark_buffer_dirty(leaf);
 				free_extent_buffer(leaf);
 			}
 		} else {
@@ -2731,56 +2903,40 @@ int btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 }
 
 /*
- * walk up the tree as far as required to find the previous leaf.
+ * search the tree again to find a leaf with lesser keys
  * returns 0 if it found something or 1 if there are no lesser leaves.
  * returns < 0 on io errors.
  */
 int btrfs_prev_leaf(struct btrfs_root *root, struct btrfs_path *path)
 {
-	int slot;
-	int level = 1;
-	struct extent_buffer *c;
-	struct extent_buffer *next = NULL;
+	struct btrfs_key key;
+	struct btrfs_disk_key found_key;
+	int ret;
 
-	while(level < BTRFS_MAX_LEVEL) {
-		if (!path->nodes[level])
-			return 1;
+	btrfs_item_key_to_cpu(path->nodes[0], &key, 0);
 
-		slot = path->slots[level];
-		c = path->nodes[level];
-		if (slot == 0) {
-			level++;
-			if (level == BTRFS_MAX_LEVEL)
-				return 1;
-			continue;
-		}
-		slot--;
-
-		if (next)
-			free_extent_buffer(next);
+	if (key.offset > 0)
+		key.offset--;
+	else if (key.type > 0)
+		key.type--;
+	else if (key.objectid > 0)
+		key.objectid--;
+	else
+		return 1;
 
-		next = read_node_slot(root, c, slot);
-		break;
-	}
-	path->slots[level] = slot;
-	while(1) {
-		level--;
-		c = path->nodes[level];
-		free_extent_buffer(c);
-		slot = btrfs_header_nritems(next);
-		if (slot != 0)
-			slot--;
-		path->nodes[level] = next;
-		path->slots[level] = slot;
-		if (!level)
-			break;
-		next = read_node_slot(root, next, slot);
-	}
-	return 0;
+	btrfs_release_path(root, path);
+	ret = btrfs_search_slot(NULL, root, &key, path, 0, 0);
+	if (ret < 0)
+		return ret;
+	btrfs_item_key(path->nodes[0], &found_key, 0);
+	ret = comp_keys(&found_key, &key);
+	if (ret < 0)
+		return 0;
+	return 1;
 }
 
 /*
- * walk up the tree as far as required to find the next leaf.
+ * search the tree again to find a leaf with greater keys
  * returns 0 if it found something or 1 if there are no greater leaves.
  * returns < 0 on io errors.
  */
@@ -2790,6 +2946,28 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 	int level = 1;
 	struct extent_buffer *c;
 	struct extent_buffer *next = NULL;
+	struct btrfs_key key;
+	u32 nritems;
+	int ret;
+
+	nritems = btrfs_header_nritems(path->nodes[0]);
+	if (nritems == 0) {
+		return 1;
+	}
+
+	btrfs_item_key_to_cpu(path->nodes[0], &key, nritems - 1);
+
+	path->keep_locks = 1;
+	btrfs_release_path(root, path);
+	ret = btrfs_search_slot(NULL, root, &key, path, 0, 0);
+	path->keep_locks = 0;
+
+	if (ret < 0)
+		return ret;
+
+	if (path->slots[0] < nritems - 1) {
+		goto done;
+	}
 
 	while(level < BTRFS_MAX_LEVEL) {
 		if (!path->nodes[level])
@@ -2799,33 +2977,45 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 		c = path->nodes[level];
 		if (slot >= btrfs_header_nritems(c)) {
 			level++;
-			if (level == BTRFS_MAX_LEVEL)
+			if (level == BTRFS_MAX_LEVEL) {
 				return 1;
+			}
 			continue;
 		}
 
-		if (next)
+		if (next) {
+			btrfs_tree_unlock(next);
 			free_extent_buffer(next);
+		}
 
-		if (path->reada)
+		if (level == 1 && path->locks[1] && path->reada)
 			reada_for_search(root, path, level, slot, 0);
 
 		next = read_node_slot(root, c, slot);
+		if (!path->skip_locking)
+			btrfs_tree_lock(next);
 		break;
 	}
 	path->slots[level] = slot;
 	while(1) {
 		level--;
 		c = path->nodes[level];
+		if (path->locks[level])
+			btrfs_tree_unlock(c);
 		free_extent_buffer(c);
 		path->nodes[level] = next;
 		path->slots[level] = 0;
+		path->locks[level] = 1;
 		if (!level)
 			break;
-		if (path->reada)
-			reada_for_search(root, path, level, 0, 0);
+		if (level == 1 && path->locks[1] && path->reada)
+			reada_for_search(root, path, level, slot, 0);
 		next = read_node_slot(root, next, 0);
+		if (!path->skip_locking)
+			btrfs_tree_lock(next);
 	}
+done:
+	unlock_up(path, 0, 1);
 	return 0;
 }
 

commit 0ef3e66b6700eb8f052daa8b89443ff872fbbdfc
Author: Chris Mason <chris.mason@oracle.com>
Date:   Sat May 24 14:04:53 2008 -0400

    Btrfs: Allocator fix variety pack
    
    * Force chunk allocation when find_free_extent has to do a full scan
    * Record the max key at the start of defrag so it doesn't run forever
    * Block groups might not be contiguous, make a forward search for the
      next block group in extent-tree.c
    * Get rid of extra checks for total fs size
    * Fix relocate_one_reference to avoid relocating the same file data block
      twice when referenced by an older transaction
    * Use the open device count when allocating chunks so that we don't
      try to allocate from devices that don't exist
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 02e571e6ee62..1d404bde3cfa 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -363,7 +363,7 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 			other = btrfs_node_blockptr(parent, i - 1);
 			close = close_blocks(blocknr, other, blocksize);
 		}
-		if (close && i < end_slot - 2) {
+		if (!close && i < end_slot - 2) {
 			other = btrfs_node_blockptr(parent, i + 1);
 			close = close_blocks(blocknr, other, blocksize);
 		}

commit 1259ab75c62462b8ffad90067b5e1f6312786a18
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon May 12 13:39:03 2008 -0400

    Btrfs: Handle write errors on raid1 and raid10
    
    When duplicate copies exist, writes are allowed to fail to one of those
    copies.  This changeset includes a few changes that allow the FS to
    continue even when some IOs fail.
    
    It also adds verification of the parent generation number for btree blocks.
    This generation is stored in the pointer to a block, and it ensures
    that missed writes to are detected.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 289d71d8653a..02e571e6ee62 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -379,7 +379,7 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 
 		cur = btrfs_find_tree_block(root, blocknr, blocksize);
 		if (cur)
-			uptodate = btrfs_buffer_uptodate(cur);
+			uptodate = btrfs_buffer_uptodate(cur, gen);
 		else
 			uptodate = 0;
 		if (!cur || !uptodate) {

commit ca7a79ad8dbe24669a8ced01f9fc0126b872745d
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon May 12 12:59:19 2008 -0400

    Btrfs: Pass down the expected generation number when reading tree blocks
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 29e5674b043d..289d71d8653a 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -302,6 +302,7 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 	struct extent_buffer *cur;
 	struct extent_buffer *tmp;
 	u64 blocknr;
+	u64 gen;
 	u64 search_start = *last_ret;
 	u64 last_block = 0;
 	u64 other;
@@ -354,6 +355,7 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 
 		progress_passed = 1;
 		blocknr = btrfs_node_blockptr(parent, i);
+		gen = btrfs_node_ptr_generation(parent, i);
 		if (last_block == 0)
 			last_block = blocknr;
 
@@ -387,15 +389,14 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 			}
 			if (!cur) {
 				cur = read_tree_block(root, blocknr,
-							 blocksize);
+							 blocksize, gen);
 			} else if (!uptodate) {
-				btrfs_read_buffer(cur);
+				btrfs_read_buffer(cur, gen);
 			}
 		}
 		if (search_start == 0)
 			search_start = last_block;
 
-		btrfs_verify_block_csum(root, cur);
 		err = __btrfs_cow_block(trans, root, cur, parent, i,
 					&tmp, search_start,
 					min(16 * blocksize,
@@ -696,12 +697,17 @@ static int bin_search(struct extent_buffer *eb, struct btrfs_key *key,
 static struct extent_buffer *read_node_slot(struct btrfs_root *root,
 				   struct extent_buffer *parent, int slot)
 {
+	int level = btrfs_header_level(parent);
 	if (slot < 0)
 		return NULL;
 	if (slot >= btrfs_header_nritems(parent))
 		return NULL;
+
+	BUG_ON(level == 0);
+
 	return read_tree_block(root, btrfs_node_blockptr(parent, slot),
-		       btrfs_level_size(root, btrfs_header_level(parent) - 1));
+		       btrfs_level_size(root, level - 1),
+		       btrfs_node_ptr_generation(parent, slot));
 }
 
 static int balance_level(struct btrfs_trans_handle *trans,
@@ -1076,7 +1082,8 @@ static void reada_for_search(struct btrfs_root *root, struct btrfs_path *path,
 		if ((search >= lowest_read && search <= highest_read) ||
 		    (search < lowest_read && lowest_read - search <= 32768) ||
 		    (search > highest_read && search - highest_read <= 32768)) {
-			readahead_tree_block(root, search, blocksize);
+			readahead_tree_block(root, search, blocksize,
+				     btrfs_node_ptr_generation(node, nr));
 			nread += blocksize;
 		}
 		nscan++;
@@ -1109,8 +1116,6 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 		      ins_len, int cow)
 {
 	struct extent_buffer *b;
-	u64 bytenr;
-	u64 ptr_gen;
 	int slot;
 	int ret;
 	int level;
@@ -1174,20 +1179,12 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 			/* this is only true while dropping a snapshot */
 			if (level == lowest_level)
 				break;
-			bytenr = btrfs_node_blockptr(b, slot);
-			ptr_gen = btrfs_node_ptr_generation(b, slot);
+
 			if (should_reada)
 				reada_for_search(root, p, level, slot,
 						 key->objectid);
-			b = read_tree_block(root, bytenr,
-					    btrfs_level_size(root, level - 1));
-			if (ptr_gen != btrfs_header_generation(b)) {
-				printk("block %llu bad gen wanted %llu "
-				       "found %llu\n",
-			        (unsigned long long)b->start,
-				(unsigned long long)ptr_gen,
-			        (unsigned long long)btrfs_header_generation(b));
-			}
+
+			b = read_node_slot(root, b, slot);
 		} else {
 			p->slots[level] = slot;
 			if (ins_len > 0 && btrfs_leaf_free_space(root, b) <
@@ -1650,8 +1647,7 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 	if (slot >= btrfs_header_nritems(upper) - 1)
 		return 1;
 
-	right = read_tree_block(root, btrfs_node_blockptr(upper, slot + 1),
-				root->leafsize);
+	right = read_node_slot(root, upper, slot + 1);
 	free_space = btrfs_leaf_free_space(root, right);
 	if (free_space < data_size + sizeof(struct btrfs_item)) {
 		free_extent_buffer(right);
@@ -1826,8 +1822,7 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 		return 1;
 	}
 
-	left = read_tree_block(root, btrfs_node_blockptr(path->nodes[1],
-			       slot - 1), root->leafsize);
+	left = read_node_slot(root, path->nodes[1], slot - 1);
 	free_space = btrfs_leaf_free_space(root, left);
 	if (free_space < data_size + sizeof(struct btrfs_item)) {
 		free_extent_buffer(left);
@@ -2742,7 +2737,6 @@ int btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,
  */
 int btrfs_prev_leaf(struct btrfs_root *root, struct btrfs_path *path)
 {
-	u64 bytenr;
 	int slot;
 	int level = 1;
 	struct extent_buffer *c;
@@ -2762,12 +2756,10 @@ int btrfs_prev_leaf(struct btrfs_root *root, struct btrfs_path *path)
 		}
 		slot--;
 
-		bytenr = btrfs_node_blockptr(c, slot);
 		if (next)
 			free_extent_buffer(next);
 
-		next = read_tree_block(root, bytenr,
-				       btrfs_level_size(root, level - 1));
+		next = read_node_slot(root, c, slot);
 		break;
 	}
 	path->slots[level] = slot;
@@ -2782,8 +2774,7 @@ int btrfs_prev_leaf(struct btrfs_root *root, struct btrfs_path *path)
 		path->slots[level] = slot;
 		if (!level)
 			break;
-		next = read_tree_block(root, btrfs_node_blockptr(next, slot),
-				       btrfs_level_size(root, level - 1));
+		next = read_node_slot(root, next, slot);
 	}
 	return 0;
 }
@@ -2797,7 +2788,6 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 {
 	int slot;
 	int level = 1;
-	u64 bytenr;
 	struct extent_buffer *c;
 	struct extent_buffer *next = NULL;
 
@@ -2814,15 +2804,13 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 			continue;
 		}
 
-		bytenr = btrfs_node_blockptr(c, slot);
 		if (next)
 			free_extent_buffer(next);
 
 		if (path->reada)
 			reada_for_search(root, path, level, slot, 0);
 
-		next = read_tree_block(root, bytenr,
-				       btrfs_level_size(root, level -1));
+		next = read_node_slot(root, c, slot);
 		break;
 	}
 	path->slots[level] = slot;
@@ -2836,8 +2824,7 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 			break;
 		if (path->reada)
 			reada_for_search(root, path, level, 0, 0);
-		next = read_tree_block(root, btrfs_node_blockptr(next, 0),
-				       btrfs_level_size(root, level - 1));
+		next = read_node_slot(root, next, 0);
 	}
 	return 0;
 }

commit bce4eae9864e6b6ebde5d7f05576ff0a6c3724d0
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Apr 24 14:42:46 2008 -0400

    Btrfs: Fix balance_level to free the middle block if there is room in the left one
    
    balance level starts by trying to empty the middle block, and then
    pushes from the right to the middle.  This might empty the right block
    and leave a small number of pointers in the middle.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 319f9f9bf8d4..29e5674b043d 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -789,7 +789,7 @@ static int balance_level(struct btrfs_trans_handle *trans,
 	/* first, try to make some room in the middle buffer */
 	if (left) {
 		orig_slot += btrfs_header_nritems(left);
-		wret = push_node_left(trans, root, left, mid, 0);
+		wret = push_node_left(trans, root, left, mid, 1);
 		if (wret < 0)
 			ret = wret;
 		if (btrfs_header_nritems(mid) < 2)
@@ -844,6 +844,11 @@ static int balance_level(struct btrfs_trans_handle *trans,
 			ret = wret;
 			goto enospc;
 		}
+		if (wret == 1) {
+			wret = push_node_left(trans, root, left, mid, 1);
+			if (wret < 0)
+				ret = wret;
+		}
 		BUG_ON(wret == 1);
 	}
 	if (btrfs_header_nritems(mid) == 0) {
@@ -1252,17 +1257,27 @@ static int push_node_left(struct btrfs_trans_handle *trans,
 	WARN_ON(btrfs_header_generation(src) != trans->transid);
 	WARN_ON(btrfs_header_generation(dst) != trans->transid);
 
-	if (!empty && src_nritems <= 2)
+	if (!empty && src_nritems <= 8)
 		return 1;
 
 	if (push_items <= 0) {
 		return 1;
 	}
 
-	if (empty)
+	if (empty) {
 		push_items = min(src_nritems, push_items);
-	else
-		push_items = min(src_nritems - 2, push_items);
+		if (push_items < src_nritems) {
+			/* leave at least 8 pointers in the node if
+			 * we aren't going to empty it
+			 */
+			if (src_nritems - push_items < 8) {
+				if (push_items <= 8)
+					return 1;
+				push_items -= 8;
+			}
+		}
+	} else
+		push_items = min(src_nritems - 8, push_items);
 
 	copy_extent_buffer(dst, src,
 			   btrfs_node_key_ptr_offset(dst_nritems),
@@ -1308,13 +1323,19 @@ static int balance_node_right(struct btrfs_trans_handle *trans,
 	src_nritems = btrfs_header_nritems(src);
 	dst_nritems = btrfs_header_nritems(dst);
 	push_items = BTRFS_NODEPTRS_PER_BLOCK(root) - dst_nritems;
-	if (push_items <= 0)
+	if (push_items <= 0) {
 		return 1;
+	}
+
+	if (src_nritems < 4) {
+		return 1;
+	}
 
 	max_push = src_nritems / 2 + 1;
 	/* don't try to empty the node */
-	if (max_push >= src_nritems)
+	if (max_push >= src_nritems) {
 		return 1;
+	}
 
 	if (max_push < push_items)
 		push_items = max_push;

commit 971a1f6648bfafd4239234f1ffb9c26e907c2744
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Apr 24 10:54:32 2008 -0400

    Btrfs: Don't empty the middle buffer in push_nodes_for_insert
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index ac999f0060b6..319f9f9bf8d4 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -29,7 +29,7 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 		      struct btrfs_path *path, int data_size, int extend);
 static int push_node_left(struct btrfs_trans_handle *trans,
 			  struct btrfs_root *root, struct extent_buffer *dst,
-			  struct extent_buffer *src);
+			  struct extent_buffer *src, int empty);
 static int balance_node_right(struct btrfs_trans_handle *trans,
 			      struct btrfs_root *root,
 			      struct extent_buffer *dst_buf,
@@ -789,7 +789,7 @@ static int balance_level(struct btrfs_trans_handle *trans,
 	/* first, try to make some room in the middle buffer */
 	if (left) {
 		orig_slot += btrfs_header_nritems(left);
-		wret = push_node_left(trans, root, left, mid);
+		wret = push_node_left(trans, root, left, mid, 0);
 		if (wret < 0)
 			ret = wret;
 		if (btrfs_header_nritems(mid) < 2)
@@ -800,7 +800,7 @@ static int balance_level(struct btrfs_trans_handle *trans,
 	 * then try to empty the right most buffer into the middle
 	 */
 	if (right) {
-		wret = push_node_left(trans, root, mid, right);
+		wret = push_node_left(trans, root, mid, right, 1);
 		if (wret < 0 && wret != -ENOSPC)
 			ret = wret;
 		if (btrfs_header_nritems(right) == 0) {
@@ -941,7 +941,7 @@ static int noinline push_nodes_for_insert(struct btrfs_trans_handle *trans,
 				wret = 1;
 			else {
 				wret = push_node_left(trans, root,
-						      left, mid);
+						      left, mid, 0);
 			}
 		}
 		if (wret < 0)
@@ -1239,7 +1239,7 @@ static int fixup_low_keys(struct btrfs_trans_handle *trans,
  */
 static int push_node_left(struct btrfs_trans_handle *trans,
 			  struct btrfs_root *root, struct extent_buffer *dst,
-			  struct extent_buffer *src)
+			  struct extent_buffer *src, int empty)
 {
 	int push_items = 0;
 	int src_nritems;
@@ -1252,12 +1252,17 @@ static int push_node_left(struct btrfs_trans_handle *trans,
 	WARN_ON(btrfs_header_generation(src) != trans->transid);
 	WARN_ON(btrfs_header_generation(dst) != trans->transid);
 
+	if (!empty && src_nritems <= 2)
+		return 1;
+
 	if (push_items <= 0) {
 		return 1;
 	}
 
-	if (src_nritems < push_items)
-		push_items = src_nritems;
+	if (empty)
+		push_items = min(src_nritems, push_items);
+	else
+		push_items = min(src_nritems - 2, push_items);
 
 	copy_extent_buffer(dst, src,
 			   btrfs_node_key_ptr_offset(dst_nritems),

commit c448acf0a057debc34507277add0fc09b1b37312
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Apr 24 09:34:34 2008 -0400

    Btrfs: Fix split_node to require more empty slots in the node as well
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 8ad8b5cc295a..ac999f0060b6 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1485,7 +1485,7 @@ static int split_node(struct btrfs_trans_handle *trans, struct btrfs_root
 		ret = push_nodes_for_insert(trans, root, path, level);
 		c = path->nodes[level];
 		if (!ret && btrfs_header_nritems(c) <
-		    BTRFS_NODEPTRS_PER_BLOCK(root) - 1)
+		    BTRFS_NODEPTRS_PER_BLOCK(root) - 3)
 			return 0;
 		if (ret < 0)
 			return ret;

commit 1514794e4253cf4a32a2acc6de52f2527ca1bdce
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Apr 24 09:22:51 2008 -0400

    Btrfs: Make sure nodes have enough room for a double split
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 1c3d9d6fbdad..8ad8b5cc295a 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1146,7 +1146,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 				slot -= 1;
 			p->slots[level] = slot;
 			if (ins_len > 0 && btrfs_header_nritems(b) >=
-			    BTRFS_NODEPTRS_PER_BLOCK(root) - 1) {
+			    BTRFS_NODEPTRS_PER_BLOCK(root) - 3) {
 				int sret = split_node(trans, root, p, level);
 				BUG_ON(sret > 0);
 				if (sret)

commit 699122f55904bec252ea4f8a64228ce98e3d9fda
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Apr 16 12:59:22 2008 -0400

    Btrfs: Don't wait on tree block writeback before freeing them anymore
    
    This isn't required anymore because we don't reallocate blocks that
    have already been written in this transaction.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 618e526c9046..1c3d9d6fbdad 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -751,7 +751,6 @@ static int balance_level(struct btrfs_trans_handle *trans,
 		add_root_to_dirty_list(root);
 		path->nodes[level] = NULL;
 		clean_tree_block(trans, root, mid);
-		wait_on_tree_block_writeback(root, mid);
 		/* once for the path */
 		free_extent_buffer(mid);
 		ret = btrfs_free_extent(trans, root, mid->start, mid->len,
@@ -810,7 +809,6 @@ static int balance_level(struct btrfs_trans_handle *trans,
 			u32 blocksize = right->len;
 
 			clean_tree_block(trans, root, right);
-			wait_on_tree_block_writeback(root, right);
 			free_extent_buffer(right);
 			right = NULL;
 			wret = del_ptr(trans, root, path, level + 1, pslot +
@@ -854,7 +852,6 @@ static int balance_level(struct btrfs_trans_handle *trans,
 		u64 bytenr = mid->start;
 		u32 blocksize = mid->len;
 		clean_tree_block(trans, root, mid);
-		wait_on_tree_block_writeback(root, mid);
 		free_extent_buffer(mid);
 		mid = NULL;
 		wret = del_ptr(trans, root, path, level + 1, pslot);
@@ -2638,7 +2635,6 @@ int btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 		} else {
 			u64 root_gen = btrfs_header_generation(path->nodes[1]);
 			clean_tree_block(trans, root, leaf);
-			wait_on_tree_block_writeback(root, leaf);
 			wret = del_ptr(trans, root, path, 1, path->slots[1]);
 			if (wret)
 				ret = wret;
@@ -2690,7 +2686,6 @@ int btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 							   path->nodes[1]);
 
 				clean_tree_block(trans, root, leaf);
-				wait_on_tree_block_writeback(root, leaf);
 
 				wret = del_ptr(trans, root, path, 1, slot);
 				if (wret)

commit e17cade25ff8074101d653557a78df09c16ca276
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Apr 15 15:41:47 2008 -0400

    Btrfs: Add chunk uuids and update multi-device back references
    
    Block headers now store the chunk tree uuid
    
    Chunk items records the device uuid for each stripes
    
    Device extent items record better back refs to the chunk tree
    
    Block groups record better back refs to the chunk tree
    
    The chunk tree format has also changed.  The objectid of BTRFS_CHUNK_ITEM_KEY
    used to be the logical offset of the chunk.  Now it is a chunk tree id,
    with the logical offset being stored in the offset field of the key.
    
    This allows a single chunk tree to record multiple logical address spaces,
    upping the number of bytes indexed by a chunk tree from 2^64 to
    2^128.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index e8bf6c221e4e..618e526c9046 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1382,6 +1382,11 @@ static int noinline insert_new_root(struct btrfs_trans_handle *trans,
 	write_extent_buffer(c, root->fs_info->fsid,
 			    (unsigned long)btrfs_header_fsid(c),
 			    BTRFS_FSID_SIZE);
+
+	write_extent_buffer(c, root->fs_info->chunk_tree_uuid,
+			    (unsigned long)btrfs_header_chunk_tree_uuid(c),
+			    BTRFS_UUID_SIZE);
+
 	btrfs_set_node_key(c, &lower_key, 0);
 	btrfs_set_node_blockptr(c, 0, lower->start);
 	lower_gen = btrfs_header_generation(lower);
@@ -1513,6 +1518,9 @@ static int split_node(struct btrfs_trans_handle *trans, struct btrfs_root
 	write_extent_buffer(split, root->fs_info->fsid,
 			    (unsigned long)btrfs_header_fsid(split),
 			    BTRFS_FSID_SIZE);
+	write_extent_buffer(split, root->fs_info->chunk_tree_uuid,
+			    (unsigned long)btrfs_header_chunk_tree_uuid(split),
+			    BTRFS_UUID_SIZE);
 
 	mid = (c_nritems + 1) / 2;
 
@@ -2043,6 +2051,10 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 	write_extent_buffer(right, root->fs_info->fsid,
 			    (unsigned long)btrfs_header_fsid(right),
 			    BTRFS_FSID_SIZE);
+
+	write_extent_buffer(right, root->fs_info->chunk_tree_uuid,
+			    (unsigned long)btrfs_header_chunk_tree_uuid(right),
+			    BTRFS_UUID_SIZE);
 	if (mid <= slot) {
 		if (nritems == 1 ||
 		    leaf_space_used(l, mid, nritems - mid) + space_needed >

commit 85d824c4a407ba8dbd897f4e05bbe21bf24cc071
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Apr 10 10:23:19 2008 -0400

    Btrfs: Disable extra debugging checks on tree blocks
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index ff4e9c6859d1..e8bf6c221e4e 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -570,6 +570,7 @@ static int noinline check_block(struct btrfs_root *root,
 				struct btrfs_path *path, int level)
 {
 	u64 found_start;
+	return 0;
 	if (btrfs_header_level(path->nodes[level]) != level)
 	    printk("warning: bad level %Lu wanted %d found %d\n",
 		   path->nodes[level]->start, level,

commit f188591e987e21b6f7f8864c66a02858b95b530e
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Apr 9 16:28:12 2008 -0400

    Btrfs: Retry metadata reads in the face of checksum failures
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index efce173a935c..ff4e9c6859d1 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -569,7 +569,16 @@ static int check_leaf(struct btrfs_root *root, struct btrfs_path *path,
 static int noinline check_block(struct btrfs_root *root,
 				struct btrfs_path *path, int level)
 {
-	return 0;
+	u64 found_start;
+	if (btrfs_header_level(path->nodes[level]) != level)
+	    printk("warning: bad level %Lu wanted %d found %d\n",
+		   path->nodes[level]->start, level,
+		   btrfs_header_level(path->nodes[level]));
+	found_start = btrfs_header_bytenr(path->nodes[level]);
+	if (found_start != path->nodes[level]->start) {
+	    printk("warning: bad bytentr %Lu found %Lu\n",
+		   path->nodes[level]->start, found_start);
+	}
 #if 0
 	struct extent_buffer *buf = path->nodes[level];
 

commit ce9adaa5a792c2099a83246265eb4055bc38b6b8
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Apr 9 16:28:12 2008 -0400

    Btrfs: Do metadata checksums for reads via a workqueue
    
    Before, metadata checksumming was done by the callers of read_tree_block,
    which would set EXTENT_CSUM bits in the extent tree to show that a given
    range of pages was already checksummed and didn't need to be verified
    again.
    
    But, those bits could go away via try_to_releasepage, and the end
    result was bogus checksum failures on pages that never left the cache.
    
    The new code validates checksums when the page is read.  It is a little
    tricky because metadata blocks can span pages and a single read may
    end up going via multiple bios.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index c92c6b0ee582..efce173a935c 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -158,9 +158,6 @@ int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 	} else {
 		root_gen = 0;
 	}
-	if (!(buf->flags & EXTENT_CSUM))
-		WARN_ON(1);
-
 	WARN_ON(root->ref_cows && trans->transid !=
 		root->fs_info->running_transaction->transid);
 	WARN_ON(root->ref_cows && trans->transid != root->last_trans);
@@ -247,8 +244,6 @@ int btrfs_cow_block(struct btrfs_trans_handle *trans,
 		       root->fs_info->generation);
 		WARN_ON(1);
 	}
-	if (!(buf->flags & EXTENT_CSUM))
-		WARN_ON(1);
 
 	header_trans = btrfs_header_generation(buf);
 	spin_lock(&root->fs_info->hash_lock);

commit cea9e4452ebaf18dd0951e90dc84d82a5dee40b2
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Apr 9 16:28:12 2008 -0400

    Change btrfs_map_block to return a structure with mappings for all stripes
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 3c7891cf0d72..c92c6b0ee582 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2025,8 +2025,10 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 					 root->root_key.objectid,
 					 root_gen, disk_key.objectid, 0,
 					 l->start, 0);
-	if (IS_ERR(right))
+	if (IS_ERR(right)) {
+		BUG_ON(1);
 		return PTR_ERR(right);
+	}
 
 	memset_extent_buffer(right, 0, 0, sizeof(struct btrfs_header));
 	btrfs_set_header_bytenr(right, right->start);

commit 0ef8b2428a339e83fe225e82ef8bd0ea3f62b180
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Apr 3 16:29:02 2008 -0400

    Btrfs: Properly dirty buffers in the split corner cases
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index df090bf2eec0..3c7891cf0d72 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2052,6 +2052,7 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 				path->nodes[0] = right;
 				path->slots[0] = 0;
 				path->slots[1] += 1;
+				btrfs_mark_buffer_dirty(right);
 				return ret;
 			}
 			mid = slot;
@@ -2082,6 +2083,7 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 					if (wret)
 						ret = wret;
 				}
+				btrfs_mark_buffer_dirty(right);
 				return ret;
 			} else if (extend && slot == 0) {
 				mid = 1;

commit 0999df54f850fe1aba29b10d5c869493af107478
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Apr 1 13:48:14 2008 -0400

    Btrfs: Verify checksums on tree blocks found without read_tree_block
    
    Checksums were only verified by btrfs_read_tree_block, which meant the
    functions to probe the page cache for blocks were not validating checksums.
    Normally this is fine because the buffers will only be in cache if they
    have already been validated.
    
    But, there is a window while the buffer is being read from disk where
    it could be up to date in the cache but not yet verified.  This patch
    makes sure all buffers go through checksum verification before they
    are used.
    
    This is safer, and it prevents modification of buffers before they go
    through the csum code.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 6ba5394834be..df090bf2eec0 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -158,6 +158,8 @@ int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 	} else {
 		root_gen = 0;
 	}
+	if (!(buf->flags & EXTENT_CSUM))
+		WARN_ON(1);
 
 	WARN_ON(root->ref_cows && trans->transid !=
 		root->fs_info->running_transaction->transid);
@@ -245,6 +247,8 @@ int btrfs_cow_block(struct btrfs_trans_handle *trans,
 		       root->fs_info->generation);
 		WARN_ON(1);
 	}
+	if (!(buf->flags & EXTENT_CSUM))
+		WARN_ON(1);
 
 	header_trans = btrfs_header_generation(buf);
 	spin_lock(&root->fs_info->hash_lock);
@@ -396,6 +400,7 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 		if (search_start == 0)
 			search_start = last_block;
 
+		btrfs_verify_block_csum(root, cur);
 		err = __btrfs_cow_block(trans, root, cur, parent, i,
 					&tmp, search_start,
 					min(16 * blocksize,

commit 63b10fc4874a014e22bc4c64e3d92b71180661fe
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Apr 1 11:21:32 2008 -0400

    Reorder the flags field in struct btrfs_header and record a flag on writeout
    
    This allows detection of blocks that have already been written in the
    running transaction so they can be recowed instead of modified again.
    It is step one in trusting the transid field of the block pointers.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 70b6ddfe15a1..6ba5394834be 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -124,6 +124,7 @@ int btrfs_copy_root(struct btrfs_trans_handle *trans,
 	btrfs_set_header_bytenr(cow, cow->start);
 	btrfs_set_header_generation(cow, trans->transid);
 	btrfs_set_header_owner(cow, new_root_objectid);
+	btrfs_clear_header_flag(cow, BTRFS_HEADER_FLAG_WRITTEN);
 
 	WARN_ON(btrfs_header_generation(buf) > trans->transid);
 	ret = btrfs_inc_ref(trans, new_root, buf);
@@ -183,6 +184,7 @@ int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 	btrfs_set_header_bytenr(cow, cow->start);
 	btrfs_set_header_generation(cow, trans->transid);
 	btrfs_set_header_owner(cow, root->root_key.objectid);
+	btrfs_clear_header_flag(cow, BTRFS_HEADER_FLAG_WRITTEN);
 
 	WARN_ON(btrfs_header_generation(buf) > trans->transid);
 	if (btrfs_header_generation(buf) != trans->transid) {
@@ -245,11 +247,14 @@ int btrfs_cow_block(struct btrfs_trans_handle *trans,
 	}
 
 	header_trans = btrfs_header_generation(buf);
-	if (header_trans == trans->transid) {
+	spin_lock(&root->fs_info->hash_lock);
+	if (header_trans == trans->transid &&
+	    !btrfs_header_flag(buf, BTRFS_HEADER_FLAG_WRITTEN)) {
 		*cow_ret = buf;
+		spin_unlock(&root->fs_info->hash_lock);
 		return 0;
 	}
-
+	spin_unlock(&root->fs_info->hash_lock);
 	search_start = buf->start & ~((u64)(1024 * 1024 * 1024) - 1);
 	ret = __btrfs_cow_block(trans, root, buf, parent,
 				 parent_slot, cow_ret, search_start, 0);
@@ -1494,6 +1499,7 @@ static int split_node(struct btrfs_trans_handle *trans, struct btrfs_root
 	btrfs_set_header_bytenr(split, split->start);
 	btrfs_set_header_generation(split, trans->transid);
 	btrfs_set_header_owner(split, root->root_key.objectid);
+	btrfs_set_header_flags(split, 0);
 	write_extent_buffer(split, root->fs_info->fsid,
 			    (unsigned long)btrfs_header_fsid(split),
 			    BTRFS_FSID_SIZE);

commit 0b86a832a1f38abec695864ec2eaedc9d2383f1b
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Mar 24 15:01:56 2008 -0400

    Btrfs: Add support for multiple devices per filesystem
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index ef8809c1e69c..70b6ddfe15a1 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -70,6 +70,14 @@ void btrfs_release_path(struct btrfs_root *root, struct btrfs_path *p)
 	memset(p, 0, sizeof(*p));
 }
 
+static void add_root_to_dirty_list(struct btrfs_root *root)
+{
+	if (root->track_dirty && list_empty(&root->dirty_list)) {
+		list_add(&root->dirty_list,
+			 &root->fs_info->dirty_cowonly_roots);
+	}
+}
+
 int btrfs_copy_root(struct btrfs_trans_handle *trans,
 		      struct btrfs_root *root,
 		      struct extent_buffer *buf,
@@ -196,6 +204,7 @@ int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 					  root_gen, 0, 0, 1);
 		}
 		free_extent_buffer(buf);
+		add_root_to_dirty_list(root);
 	} else {
 		root_gen = btrfs_header_generation(parent);
 		btrfs_set_node_blockptr(parent, parent_slot,
@@ -241,7 +250,7 @@ int btrfs_cow_block(struct btrfs_trans_handle *trans,
 		return 0;
 	}
 
-	search_start = buf->start & ~((u64)BTRFS_BLOCK_GROUP_SIZE - 1);
+	search_start = buf->start & ~((u64)(1024 * 1024 * 1024) - 1);
 	ret = __btrfs_cow_block(trans, root, buf, parent,
 				 parent_slot, cow_ret, search_start, 0);
 	return ret;
@@ -724,6 +733,7 @@ static int balance_level(struct btrfs_trans_handle *trans,
 		BUG_ON(ret);
 
 		root->node = child;
+		add_root_to_dirty_list(root);
 		path->nodes[level] = NULL;
 		clean_tree_block(trans, root, mid);
 		wait_on_tree_block_writeback(root, mid);
@@ -1369,6 +1379,7 @@ static int noinline insert_new_root(struct btrfs_trans_handle *trans,
 	/* the super has an extra ref to root->node */
 	free_extent_buffer(root->node);
 	root->node = c;
+	add_root_to_dirty_list(root);
 	extent_buffer_get(c);
 	path->nodes[level] = c;
 	path->slots[level] = 0;
@@ -2777,3 +2788,28 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 	}
 	return 0;
 }
+
+int btrfs_previous_item(struct btrfs_root *root,
+			struct btrfs_path *path, u64 min_objectid,
+			int type)
+{
+	struct btrfs_key found_key;
+	struct extent_buffer *leaf;
+	int ret;
+
+	while(1) {
+		if (path->slots[0] == 0) {
+			ret = btrfs_prev_leaf(root, path);
+			if (ret != 0)
+				return ret;
+		} else {
+			path->slots[0]--;
+		}
+		leaf = path->nodes[0];
+		btrfs_item_key_to_cpu(leaf, &found_key, path->slots[0]);
+		if (found_key.type == type)
+			return 0;
+	}
+	return 1;
+}
+

commit 2f375ab9c55bf22bf77ed9e3082b93e77ea10f55
Author: Yan <yanzheng@21cn.com>
Date:   Fri Feb 1 14:58:07 2008 -0500

    Call btrfs_cow_block while lowering tree level.
    
    When freeing root block of a tree,  btrfs_free_extent' parameter
    'ref_generation' is from root block itseft.  When freeing non-root
    block,  'ref_generation' is from its parent. so when converting a
    non-root block to root block, we must guarantee its generation is
    equal to its parent's generation.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index cf32651876bf..ef8809c1e69c 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -720,6 +720,9 @@ static int balance_level(struct btrfs_trans_handle *trans,
 		/* promote the child to a root */
 		child = read_node_slot(root, mid, 0);
 		BUG_ON(!child);
+		ret = btrfs_cow_block(trans, root, child, mid, 0, &child);
+		BUG_ON(ret);
+
 		root->node = child;
 		path->nodes[level] = NULL;
 		clean_tree_block(trans, root, mid);

commit 5a01a2e3a9d4dc9cb4871dde4d832a3b8de9f748
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Jan 30 11:43:54 2008 -0500

    Btrfs: Copy correct tree when inserting into slot 0
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index fb2e2bd506c8..cf32651876bf 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2443,13 +2443,16 @@ int btrfs_insert_empty_items(struct btrfs_trans_handle *trans,
 	btrfs_mark_buffer_dirty(leaf);
 
 	ret = 0;
-	if (slot == 0)
+	if (slot == 0) {
+		btrfs_cpu_key_to_disk(&disk_key, cpu_key);
 		ret = fixup_low_keys(trans, root, path, &disk_key, 1);
+	}
 
 	if (btrfs_leaf_free_space(root, leaf) < 0) {
 		btrfs_print_leaf(root, leaf);
 		BUG();
 	}
+
 out:
 	return ret;
 }

commit 9c58309d6cf22471dacbcb6de54d00cef9ca20d4
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Jan 29 15:15:18 2008 -0500

    Btrfs: Add inode item and backref in one insert, reducing cpu usage
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 84ad53e06b3a..fb2e2bd506c8 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2332,27 +2332,34 @@ int btrfs_extend_item(struct btrfs_trans_handle *trans,
  * Given a key and some data, insert an item into the tree.
  * This does all the path init required, making room in the tree if needed.
  */
-int btrfs_insert_empty_item(struct btrfs_trans_handle *trans,
+int btrfs_insert_empty_items(struct btrfs_trans_handle *trans,
 			    struct btrfs_root *root,
 			    struct btrfs_path *path,
-			    struct btrfs_key *cpu_key, u32 data_size)
+			    struct btrfs_key *cpu_key, u32 *data_size,
+			    int nr)
 {
 	struct extent_buffer *leaf;
 	struct btrfs_item *item;
 	int ret = 0;
 	int slot;
 	int slot_orig;
+	int i;
 	u32 nritems;
+	u32 total_size = 0;
+	u32 total_data = 0;
 	unsigned int data_end;
 	struct btrfs_disk_key disk_key;
 
-	btrfs_cpu_key_to_disk(&disk_key, cpu_key);
+	for (i = 0; i < nr; i++) {
+		total_data += data_size[i];
+	}
 
 	/* create a root if there isn't one */
 	if (!root->node)
 		BUG();
 
-	ret = btrfs_search_slot(trans, root, cpu_key, path, data_size, 1);
+	total_size = total_data + (nr - 1) * sizeof(struct btrfs_item);
+	ret = btrfs_search_slot(trans, root, cpu_key, path, total_size, 1);
 	if (ret == 0) {
 		return -EEXIST;
 	}
@@ -2366,10 +2373,10 @@ int btrfs_insert_empty_item(struct btrfs_trans_handle *trans,
 	data_end = leaf_data_end(root, leaf);
 
 	if (btrfs_leaf_free_space(root, leaf) <
-	    sizeof(struct btrfs_item) + data_size) {
+	    sizeof(struct btrfs_item) + total_size) {
 		btrfs_print_leaf(root, leaf);
 		printk("not enough freespace need %u have %d\n",
-		       data_size, btrfs_leaf_free_space(root, leaf));
+		       total_size, btrfs_leaf_free_space(root, leaf));
 		BUG();
 	}
 
@@ -2404,7 +2411,7 @@ int btrfs_insert_empty_item(struct btrfs_trans_handle *trans,
 			}
 
 			ioff = btrfs_item_offset(leaf, item);
-			btrfs_set_item_offset(leaf, item, ioff - data_size);
+			btrfs_set_item_offset(leaf, item, ioff - total_data);
 		}
 		if (leaf->map_token) {
 			unmap_extent_buffer(leaf, leaf->map_token, KM_USER1);
@@ -2412,23 +2419,27 @@ int btrfs_insert_empty_item(struct btrfs_trans_handle *trans,
 		}
 
 		/* shift the items */
-		memmove_extent_buffer(leaf, btrfs_item_nr_offset(slot + 1),
+		memmove_extent_buffer(leaf, btrfs_item_nr_offset(slot + nr),
 			      btrfs_item_nr_offset(slot),
 			      (nritems - slot) * sizeof(struct btrfs_item));
 
 		/* shift the data */
 		memmove_extent_buffer(leaf, btrfs_leaf_data(leaf) +
-			      data_end - data_size, btrfs_leaf_data(leaf) +
+			      data_end - total_data, btrfs_leaf_data(leaf) +
 			      data_end, old_data - data_end);
 		data_end = old_data;
 	}
 
 	/* setup the item for the new data */
-	btrfs_set_item_key(leaf, &disk_key, slot);
-	item = btrfs_item_nr(leaf, slot);
-	btrfs_set_item_offset(leaf, item, data_end - data_size);
-	btrfs_set_item_size(leaf, item, data_size);
-	btrfs_set_header_nritems(leaf, nritems + 1);
+	for (i = 0; i < nr; i++) {
+		btrfs_cpu_key_to_disk(&disk_key, cpu_key + i);
+		btrfs_set_item_key(leaf, &disk_key, slot + i);
+		item = btrfs_item_nr(leaf, slot + i);
+		btrfs_set_item_offset(leaf, item, data_end - data_size[i]);
+		data_end -= data_size[i];
+		btrfs_set_item_size(leaf, item, data_size[i]);
+	}
+	btrfs_set_header_nritems(leaf, nritems + nr);
 	btrfs_mark_buffer_dirty(leaf);
 
 	ret = 0;

commit 85e21bac165b4ba1f6f90431ad6fc658ffcbaf3a
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Jan 29 15:11:36 2008 -0500

    Btrfs: During deletes and truncate, remove many items at once from the tree
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 43d23148a4fe..84ad53e06b3a 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2514,34 +2514,36 @@ static int del_ptr(struct btrfs_trans_handle *trans, struct btrfs_root *root,
  * delete the item at the leaf level in path.  If that empties
  * the leaf, remove it from the tree
  */
-int btrfs_del_item(struct btrfs_trans_handle *trans, struct btrfs_root *root,
-		   struct btrfs_path *path)
+int btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,
+		    struct btrfs_path *path, int slot, int nr)
 {
-	int slot;
 	struct extent_buffer *leaf;
 	struct btrfs_item *item;
-	int doff;
-	int dsize;
+	int last_off;
+	int dsize = 0;
 	int ret = 0;
 	int wret;
+	int i;
 	u32 nritems;
 
 	leaf = path->nodes[0];
-	slot = path->slots[0];
-	doff = btrfs_item_offset_nr(leaf, slot);
-	dsize = btrfs_item_size_nr(leaf, slot);
+	last_off = btrfs_item_offset_nr(leaf, slot + nr - 1);
+
+	for (i = 0; i < nr; i++)
+		dsize += btrfs_item_size_nr(leaf, slot + i);
+
 	nritems = btrfs_header_nritems(leaf);
 
-	if (slot != nritems - 1) {
+	if (slot + nr != nritems) {
 		int i;
 		int data_end = leaf_data_end(root, leaf);
 
 		memmove_extent_buffer(leaf, btrfs_leaf_data(leaf) +
 			      data_end + dsize,
 			      btrfs_leaf_data(leaf) + data_end,
-			      doff - data_end);
+			      last_off - data_end);
 
-		for (i = slot + 1; i < nritems; i++) {
+		for (i = slot + nr; i < nritems; i++) {
 			u32 ioff;
 
 			item = btrfs_item_nr(leaf, i);
@@ -2562,12 +2564,12 @@ int btrfs_del_item(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 		}
 
 		memmove_extent_buffer(leaf, btrfs_item_nr_offset(slot),
-			      btrfs_item_nr_offset(slot + 1),
+			      btrfs_item_nr_offset(slot + nr),
 			      sizeof(struct btrfs_item) *
-			      (nritems - slot - 1));
+			      (nritems - slot - nr));
 	}
-	btrfs_set_header_nritems(leaf, nritems - 1);
-	nritems--;
+	btrfs_set_header_nritems(leaf, nritems - nr);
+	nritems -= nr;
 
 	/* delete the leaf if we've emptied it */
 	if (nritems == 0) {
@@ -2600,7 +2602,7 @@ int btrfs_del_item(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 		}
 
 		/* delete the leaf if it is mostly empty */
-		if (used < BTRFS_LEAF_DATA_SIZE(root) / 3) {
+		if (used < BTRFS_LEAF_DATA_SIZE(root) / 4) {
 			/* push_leaf_left fixes the path.
 			 * make sure the path still points to our leaf
 			 * for possible call to del_ptr below
@@ -2608,13 +2610,13 @@ int btrfs_del_item(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 			slot = path->slots[1];
 			extent_buffer_get(leaf);
 
-			wret = push_leaf_right(trans, root, path, 1, 1);
+			wret = push_leaf_left(trans, root, path, 1, 1);
 			if (wret < 0 && wret != -ENOSPC)
 				ret = wret;
 
 			if (path->nodes[0] == leaf &&
 			    btrfs_header_nritems(leaf)) {
-				wret = push_leaf_left(trans, root, path, 1, 1);
+				wret = push_leaf_right(trans, root, path, 1, 1);
 				if (wret < 0 && wret != -ENOSPC)
 					ret = wret;
 			}

commit dc17ff8f11d129db9e83ab7244769e4eae05e14d
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Jan 8 15:46:30 2008 -0500

    Btrfs: Add data=ordered support
    
    This forces file data extents down the disk along with the metadata that
    references them.  The current implementation is fairly simple, and just
    writes out all of the dirty pages in an inode before the commit.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 35c57074a376..43d23148a4fe 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -221,7 +221,9 @@ int btrfs_cow_block(struct btrfs_trans_handle *trans,
 		    struct extent_buffer **cow_ret)
 {
 	u64 search_start;
+	u64 header_trans;
 	int ret;
+
 	if (trans->transaction != root->fs_info->running_transaction) {
 		printk(KERN_CRIT "trans %Lu running %Lu\n", trans->transid,
 		       root->fs_info->running_transaction->transid);
@@ -232,7 +234,9 @@ int btrfs_cow_block(struct btrfs_trans_handle *trans,
 		       root->fs_info->generation);
 		WARN_ON(1);
 	}
-	if (btrfs_header_generation(buf) == trans->transid) {
+
+	header_trans = btrfs_header_generation(buf);
+	if (header_trans == trans->transid) {
 		*cow_ret = buf;
 		return 0;
 	}

commit 98ed51747b63435b9987ef12692a75c223818bbe
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Jan 3 10:01:48 2008 -0500

    Btrfs: Force inlining off in a few places to save stack usage
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 8fa92a2d9819..35c57074a376 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -548,8 +548,8 @@ static int check_leaf(struct btrfs_root *root, struct btrfs_path *path,
 	return 0;
 }
 
-static int check_block(struct btrfs_root *root, struct btrfs_path *path,
-			int level)
+static int noinline check_block(struct btrfs_root *root,
+				struct btrfs_path *path, int level)
 {
 	return 0;
 #if 0
@@ -676,8 +676,9 @@ static struct extent_buffer *read_node_slot(struct btrfs_root *root,
 		       btrfs_level_size(root, btrfs_header_level(parent) - 1));
 }
 
-static int balance_level(struct btrfs_trans_handle *trans, struct btrfs_root
-			 *root, struct btrfs_path *path, int level)
+static int balance_level(struct btrfs_trans_handle *trans,
+			 struct btrfs_root *root,
+			 struct btrfs_path *path, int level)
 {
 	struct extent_buffer *right = NULL;
 	struct extent_buffer *mid;
@@ -868,9 +869,9 @@ static int balance_level(struct btrfs_trans_handle *trans, struct btrfs_root
 }
 
 /* returns zero if the push worked, non-zero otherwise */
-static int push_nodes_for_insert(struct btrfs_trans_handle *trans,
-				struct btrfs_root *root,
-				struct btrfs_path *path, int level)
+static int noinline push_nodes_for_insert(struct btrfs_trans_handle *trans,
+					  struct btrfs_root *root,
+					  struct btrfs_path *path, int level)
 {
 	struct extent_buffer *right = NULL;
 	struct extent_buffer *mid;
@@ -1207,8 +1208,8 @@ static int fixup_low_keys(struct btrfs_trans_handle *trans,
  * returns 0 if some ptrs were pushed left, < 0 if there was some horrible
  * error, and > 0 if there was no room in the left hand block.
  */
-static int push_node_left(struct btrfs_trans_handle *trans, struct btrfs_root
-			  *root, struct extent_buffer *dst,
+static int push_node_left(struct btrfs_trans_handle *trans,
+			  struct btrfs_root *root, struct extent_buffer *dst,
 			  struct extent_buffer *src)
 {
 	int push_items = 0;
@@ -1309,7 +1310,7 @@ static int balance_node_right(struct btrfs_trans_handle *trans,
  *
  * returns zero on success or < 0 on failure.
  */
-static int insert_new_root(struct btrfs_trans_handle *trans,
+static int noinline insert_new_root(struct btrfs_trans_handle *trans,
 			   struct btrfs_root *root,
 			   struct btrfs_path *path, int level)
 {

commit 8f662a76c6af8eb367fa519e9bb9766040d9cea8
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Jan 2 10:01:11 2008 -0500

    Btrfs: Add readahead to the online shrinker, and a mount -o alloc_start= for testing
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 2b3ffa707eae..8fa92a2d9819 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2654,9 +2654,9 @@ int btrfs_del_item(struct btrfs_trans_handle *trans, struct btrfs_root *root,
  */
 int btrfs_prev_leaf(struct btrfs_root *root, struct btrfs_path *path)
 {
+	u64 bytenr;
 	int slot;
 	int level = 1;
-	u64 bytenr;
 	struct extent_buffer *c;
 	struct extent_buffer *next = NULL;
 
@@ -2687,11 +2687,14 @@ int btrfs_prev_leaf(struct btrfs_root *root, struct btrfs_path *path)
 		level--;
 		c = path->nodes[level];
 		free_extent_buffer(c);
+		slot = btrfs_header_nritems(next);
+		if (slot != 0)
+			slot--;
 		path->nodes[level] = next;
-		path->slots[level] = 0;
+		path->slots[level] = slot;
 		if (!level)
 			break;
-		next = read_tree_block(root, btrfs_node_blockptr(next, 0),
+		next = read_tree_block(root, btrfs_node_blockptr(next, slot),
 				       btrfs_level_size(root, level - 1));
 	}
 	return 0;

commit 01f466580502c57001bf80fff709479fdb9e87a5
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Dec 21 16:24:26 2007 -0500

    Btrfs: Less aggressive readahead on deletes
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 7f764455f26e..2b3ffa707eae 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -987,9 +987,10 @@ static int push_nodes_for_insert(struct btrfs_trans_handle *trans,
  * readahead one full node of leaves
  */
 static void reada_for_search(struct btrfs_root *root, struct btrfs_path *path,
-			     int level, int slot)
+			     int level, int slot, u64 objectid)
 {
 	struct extent_buffer *node;
+	struct btrfs_disk_key disk_key;
 	u32 nritems;
 	u64 search;
 	u64 lowest_read;
@@ -1031,6 +1032,11 @@ static void reada_for_search(struct btrfs_root *root, struct btrfs_path *path,
 			if (nr >= nritems)
 				break;
 		}
+		if (path->reada < 0 && objectid) {
+			btrfs_node_key(node, &disk_key, nr);
+			if (btrfs_disk_key_objectid(&disk_key) != objectid)
+				break;
+		}
 		search = btrfs_node_blockptr(node, nr);
 		if ((search >= lowest_read && search <= highest_read) ||
 		    (search < lowest_read && lowest_read - search <= 32768) ||
@@ -1136,7 +1142,8 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 			bytenr = btrfs_node_blockptr(b, slot);
 			ptr_gen = btrfs_node_ptr_generation(b, slot);
 			if (should_reada)
-				reada_for_search(root, p, level, slot);
+				reada_for_search(root, p, level, slot,
+						 key->objectid);
 			b = read_tree_block(root, bytenr,
 					    btrfs_level_size(root, level - 1));
 			if (ptr_gen != btrfs_header_generation(b)) {
@@ -2671,9 +2678,6 @@ int btrfs_prev_leaf(struct btrfs_root *root, struct btrfs_path *path)
 		if (next)
 			free_extent_buffer(next);
 
-		if (path->reada < 0)
-			reada_for_search(root, path, level, slot);
-
 		next = read_tree_block(root, bytenr,
 				       btrfs_level_size(root, level - 1));
 		break;
@@ -2687,8 +2691,6 @@ int btrfs_prev_leaf(struct btrfs_root *root, struct btrfs_path *path)
 		path->slots[level] = 0;
 		if (!level)
 			break;
-		if (path->reada)
-			reada_for_search(root, path, level, 0);
 		next = read_tree_block(root, btrfs_node_blockptr(next, 0),
 				       btrfs_level_size(root, level - 1));
 	}
@@ -2726,7 +2728,7 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 			free_extent_buffer(next);
 
 		if (path->reada)
-			reada_for_search(root, path, level, slot);
+			reada_for_search(root, path, level, slot, 0);
 
 		next = read_tree_block(root, bytenr,
 				       btrfs_level_size(root, level -1));
@@ -2742,7 +2744,7 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 		if (!level)
 			break;
 		if (path->reada)
-			reada_for_search(root, path, level, 0);
+			reada_for_search(root, path, level, 0, 0);
 		next = read_tree_block(root, btrfs_node_blockptr(next, 0),
 				       btrfs_level_size(root, level - 1));
 	}

commit 4aec2b52327b3a3a20faa3f1af3102c0d97c7c92
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Dec 18 16:25:45 2007 -0500

    kmalloc a few large stack objects in the btrfs_ioctl path
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 585f279d1112..7f764455f26e 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -80,10 +80,14 @@ int btrfs_copy_root(struct btrfs_trans_handle *trans,
 	int ret = 0;
 	int level;
 	struct btrfs_key first_key;
-	struct btrfs_root new_root;
+	struct btrfs_root *new_root;
 
-	memcpy(&new_root, root, sizeof(new_root));
-	new_root.root_key.objectid = new_root_objectid;
+	new_root = kmalloc(sizeof(*new_root), GFP_NOFS);
+	if (!new_root)
+		return -ENOMEM;
+
+	memcpy(new_root, root, sizeof(*new_root));
+	new_root->root_key.objectid = new_root_objectid;
 
 	WARN_ON(root->ref_cows && trans->transid !=
 		root->fs_info->running_transaction->transid);
@@ -99,12 +103,14 @@ int btrfs_copy_root(struct btrfs_trans_handle *trans,
 	} else {
 		first_key.objectid = 0;
 	}
-	cow = __btrfs_alloc_free_block(trans, &new_root, buf->len,
+	cow = __btrfs_alloc_free_block(trans, new_root, buf->len,
 				       new_root_objectid,
 				       trans->transid, first_key.objectid,
 				       level, buf->start, 0);
-	if (IS_ERR(cow))
+	if (IS_ERR(cow)) {
+		kfree(new_root);
 		return PTR_ERR(cow);
+	}
 
 	copy_extent_buffer(cow, buf, 0, 0, cow->len);
 	btrfs_set_header_bytenr(cow, cow->start);
@@ -112,7 +118,9 @@ int btrfs_copy_root(struct btrfs_trans_handle *trans,
 	btrfs_set_header_owner(cow, new_root_objectid);
 
 	WARN_ON(btrfs_header_generation(buf) > trans->transid);
-	ret = btrfs_inc_ref(trans, &new_root, buf);
+	ret = btrfs_inc_ref(trans, new_root, buf);
+	kfree(new_root);
+
 	if (ret)
 		return ret;
 

commit be20aa9dbadc8c06283784ee12bbc0d97dea3418
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Dec 17 20:14:01 2007 -0500

    Btrfs: Add mount option to turn off data cow
    
    A number of workloads do not require copy on write data or checksumming.
    mount -o nodatasum to disable checksums and -o nodatacow to disable
    both copy on write and checksumming.
    
    In nodatacow mode, copy on write is still performed when a given extent
    is under snapshot.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index fd8233e05cf4..585f279d1112 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -70,7 +70,58 @@ void btrfs_release_path(struct btrfs_root *root, struct btrfs_path *p)
 	memset(p, 0, sizeof(*p));
 }
 
-static int __btrfs_cow_block(struct btrfs_trans_handle *trans,
+int btrfs_copy_root(struct btrfs_trans_handle *trans,
+		      struct btrfs_root *root,
+		      struct extent_buffer *buf,
+		      struct extent_buffer **cow_ret, u64 new_root_objectid)
+{
+	struct extent_buffer *cow;
+	u32 nritems;
+	int ret = 0;
+	int level;
+	struct btrfs_key first_key;
+	struct btrfs_root new_root;
+
+	memcpy(&new_root, root, sizeof(new_root));
+	new_root.root_key.objectid = new_root_objectid;
+
+	WARN_ON(root->ref_cows && trans->transid !=
+		root->fs_info->running_transaction->transid);
+	WARN_ON(root->ref_cows && trans->transid != root->last_trans);
+
+	level = btrfs_header_level(buf);
+	nritems = btrfs_header_nritems(buf);
+	if (nritems) {
+		if (level == 0)
+			btrfs_item_key_to_cpu(buf, &first_key, 0);
+		else
+			btrfs_node_key_to_cpu(buf, &first_key, 0);
+	} else {
+		first_key.objectid = 0;
+	}
+	cow = __btrfs_alloc_free_block(trans, &new_root, buf->len,
+				       new_root_objectid,
+				       trans->transid, first_key.objectid,
+				       level, buf->start, 0);
+	if (IS_ERR(cow))
+		return PTR_ERR(cow);
+
+	copy_extent_buffer(cow, buf, 0, 0, cow->len);
+	btrfs_set_header_bytenr(cow, cow->start);
+	btrfs_set_header_generation(cow, trans->transid);
+	btrfs_set_header_owner(cow, new_root_objectid);
+
+	WARN_ON(btrfs_header_generation(buf) > trans->transid);
+	ret = btrfs_inc_ref(trans, &new_root, buf);
+	if (ret)
+		return ret;
+
+	btrfs_mark_buffer_dirty(cow);
+	*cow_ret = cow;
+	return 0;
+}
+
+int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 			     struct btrfs_root *root,
 			     struct extent_buffer *buf,
 			     struct extent_buffer *parent, int parent_slot,

commit 7bb86316c3961d1bc401ef184fd996f999556c7f
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Dec 11 09:25:06 2007 -0500

    Btrfs: Add back pointers from extents to the btree or file referencing them
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 5697705f7530..fd8233e05cf4 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -77,13 +77,37 @@ static int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 			     struct extent_buffer **cow_ret,
 			     u64 search_start, u64 empty_size)
 {
+	u64 root_gen;
 	struct extent_buffer *cow;
+	u32 nritems;
 	int ret = 0;
 	int different_trans = 0;
+	int level;
+	struct btrfs_key first_key;
+
+	if (root->ref_cows) {
+		root_gen = trans->transid;
+	} else {
+		root_gen = 0;
+	}
 
+	WARN_ON(root->ref_cows && trans->transid !=
+		root->fs_info->running_transaction->transid);
 	WARN_ON(root->ref_cows && trans->transid != root->last_trans);
 
-	cow = btrfs_alloc_free_block(trans, root, buf->len,
+	level = btrfs_header_level(buf);
+	nritems = btrfs_header_nritems(buf);
+	if (nritems) {
+		if (level == 0)
+			btrfs_item_key_to_cpu(buf, &first_key, 0);
+		else
+			btrfs_node_key_to_cpu(buf, &first_key, 0);
+	} else {
+		first_key.objectid = 0;
+	}
+	cow = __btrfs_alloc_free_block(trans, root, buf->len,
+				     root->root_key.objectid,
+				     root_gen, first_key.objectid, level,
 				     search_start, empty_size);
 	if (IS_ERR(cow))
 		return PTR_ERR(cow);
@@ -104,14 +128,17 @@ static int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 	}
 
 	if (buf == root->node) {
+		root_gen = btrfs_header_generation(buf);
 		root->node = cow;
 		extent_buffer_get(cow);
 		if (buf != root->commit_root) {
 			btrfs_free_extent(trans, root, buf->start,
-					  buf->len, 1);
+					  buf->len, root->root_key.objectid,
+					  root_gen, 0, 0, 1);
 		}
 		free_extent_buffer(buf);
 	} else {
+		root_gen = btrfs_header_generation(parent);
 		btrfs_set_node_blockptr(parent, parent_slot,
 					cow->start);
 		WARN_ON(trans->transid == 0);
@@ -119,7 +146,9 @@ static int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 					      trans->transid);
 		btrfs_mark_buffer_dirty(parent);
 		WARN_ON(btrfs_header_generation(parent) != trans->transid);
-		btrfs_free_extent(trans, root, buf->start, buf->len, 1);
+		btrfs_free_extent(trans, root, buf->start, buf->len,
+				  btrfs_header_owner(parent), root_gen,
+				  0, 0, 1);
 	}
 	free_extent_buffer(buf);
 	btrfs_mark_buffer_dirty(cow);
@@ -606,6 +635,8 @@ static int balance_level(struct btrfs_trans_handle *trans, struct btrfs_root
 		return 0;
 
 	mid = path->nodes[level];
+	WARN_ON(btrfs_header_generation(mid) != trans->transid);
+
 	orig_ptr = btrfs_node_blockptr(mid, orig_slot);
 
 	if (level < BTRFS_MAX_LEVEL - 1)
@@ -631,7 +662,9 @@ static int balance_level(struct btrfs_trans_handle *trans, struct btrfs_root
 		wait_on_tree_block_writeback(root, mid);
 		/* once for the path */
 		free_extent_buffer(mid);
-		ret = btrfs_free_extent(trans, root, mid->start, mid->len, 1);
+		ret = btrfs_free_extent(trans, root, mid->start, mid->len,
+					root->root_key.objectid,
+					btrfs_header_generation(mid), 0, 0, 1);
 		/* once for the root ptr */
 		free_extent_buffer(mid);
 		return ret;
@@ -681,6 +714,7 @@ static int balance_level(struct btrfs_trans_handle *trans, struct btrfs_root
 			ret = wret;
 		if (btrfs_header_nritems(right) == 0) {
 			u64 bytenr = right->start;
+			u64 generation = btrfs_header_generation(parent);
 			u32 blocksize = right->len;
 
 			clean_tree_block(trans, root, right);
@@ -692,7 +726,9 @@ static int balance_level(struct btrfs_trans_handle *trans, struct btrfs_root
 			if (wret)
 				ret = wret;
 			wret = btrfs_free_extent(trans, root, bytenr,
-						 blocksize, 1);
+						 blocksize,
+						 btrfs_header_owner(parent),
+						 generation, 0, 0, 1);
 			if (wret)
 				ret = wret;
 		} else {
@@ -722,6 +758,7 @@ static int balance_level(struct btrfs_trans_handle *trans, struct btrfs_root
 	}
 	if (btrfs_header_nritems(mid) == 0) {
 		/* we've managed to empty the middle node, drop it */
+		u64 root_gen = btrfs_header_generation(parent);
 		u64 bytenr = mid->start;
 		u32 blocksize = mid->len;
 		clean_tree_block(trans, root, mid);
@@ -731,7 +768,9 @@ static int balance_level(struct btrfs_trans_handle *trans, struct btrfs_root
 		wret = del_ptr(trans, root, path, level + 1, pslot);
 		if (wret)
 			ret = wret;
-		wret = btrfs_free_extent(trans, root, bytenr, blocksize, 1);
+		wret = btrfs_free_extent(trans, root, bytenr, blocksize,
+					 btrfs_header_owner(parent),
+					 root_gen, 0, 0, 1);
 		if (wret)
 			ret = wret;
 	} else {
@@ -788,6 +827,7 @@ static int push_nodes_for_insert(struct btrfs_trans_handle *trans,
 		return 1;
 
 	mid = path->nodes[level];
+	WARN_ON(btrfs_header_generation(mid) != trans->transid);
 	orig_ptr = btrfs_node_blockptr(mid, orig_slot);
 
 	if (level < BTRFS_MAX_LEVEL - 1)
@@ -1113,6 +1153,8 @@ static int push_node_left(struct btrfs_trans_handle *trans, struct btrfs_root
 	src_nritems = btrfs_header_nritems(src);
 	dst_nritems = btrfs_header_nritems(dst);
 	push_items = BTRFS_NODEPTRS_PER_BLOCK(root) - dst_nritems;
+	WARN_ON(btrfs_header_generation(src) != trans->transid);
+	WARN_ON(btrfs_header_generation(dst) != trans->transid);
 
 	if (push_items <= 0) {
 		return 1;
@@ -1159,6 +1201,9 @@ static int balance_node_right(struct btrfs_trans_handle *trans,
 	int dst_nritems;
 	int ret = 0;
 
+	WARN_ON(btrfs_header_generation(src) != trans->transid);
+	WARN_ON(btrfs_header_generation(dst) != trans->transid);
+
 	src_nritems = btrfs_header_nritems(src);
 	dst_nritems = btrfs_header_nritems(dst);
 	push_items = BTRFS_NODEPTRS_PER_BLOCK(root) - dst_nritems;
@@ -1202,6 +1247,8 @@ static int insert_new_root(struct btrfs_trans_handle *trans,
 			   struct btrfs_root *root,
 			   struct btrfs_path *path, int level)
 {
+	u64 root_gen;
+	u64 lower_gen;
 	struct extent_buffer *lower;
 	struct extent_buffer *c;
 	struct btrfs_disk_key lower_key;
@@ -1209,7 +1256,20 @@ static int insert_new_root(struct btrfs_trans_handle *trans,
 	BUG_ON(path->nodes[level]);
 	BUG_ON(path->nodes[level-1] != root->node);
 
-	c = btrfs_alloc_free_block(trans, root, root->nodesize,
+	if (root->ref_cows)
+		root_gen = trans->transid;
+	else
+		root_gen = 0;
+
+	lower = path->nodes[level-1];
+	if (level == 1)
+		btrfs_item_key(lower, &lower_key, 0);
+	else
+		btrfs_node_key(lower, &lower_key, 0);
+
+	c = __btrfs_alloc_free_block(trans, root, root->nodesize,
+				   root->root_key.objectid,
+				   root_gen, lower_key.objectid, level,
 				   root->node->start, 0);
 	if (IS_ERR(c))
 		return PTR_ERR(c);
@@ -1219,19 +1279,16 @@ static int insert_new_root(struct btrfs_trans_handle *trans,
 	btrfs_set_header_bytenr(c, c->start);
 	btrfs_set_header_generation(c, trans->transid);
 	btrfs_set_header_owner(c, root->root_key.objectid);
-	lower = path->nodes[level-1];
 
 	write_extent_buffer(c, root->fs_info->fsid,
 			    (unsigned long)btrfs_header_fsid(c),
 			    BTRFS_FSID_SIZE);
-	if (level == 1)
-		btrfs_item_key(lower, &lower_key, 0);
-	else
-		btrfs_node_key(lower, &lower_key, 0);
 	btrfs_set_node_key(c, &lower_key, 0);
 	btrfs_set_node_blockptr(c, 0, lower->start);
-	WARN_ON(btrfs_header_generation(lower) == 0);
-	btrfs_set_node_ptr_generation(c, 0, btrfs_header_generation(lower));
+	lower_gen = btrfs_header_generation(lower);
+	WARN_ON(lower_gen == 0);
+
+	btrfs_set_node_ptr_generation(c, 0, lower_gen);
 
 	btrfs_mark_buffer_dirty(c);
 
@@ -1241,6 +1298,18 @@ static int insert_new_root(struct btrfs_trans_handle *trans,
 	extent_buffer_get(c);
 	path->nodes[level] = c;
 	path->slots[level] = 0;
+
+	if (root->ref_cows && lower_gen != trans->transid) {
+		struct btrfs_path *back_path = btrfs_alloc_path();
+		int ret;
+		ret = btrfs_insert_extent_backref(trans,
+						  root->fs_info->extent_root,
+						  path, lower->start,
+						  root->root_key.objectid,
+						  trans->transid, 0, 0);
+		BUG_ON(ret);
+		btrfs_free_path(back_path);
+	}
 	return 0;
 }
 
@@ -1294,6 +1363,7 @@ static int insert_ptr(struct btrfs_trans_handle *trans, struct btrfs_root
 static int split_node(struct btrfs_trans_handle *trans, struct btrfs_root
 		      *root, struct btrfs_path *path, int level)
 {
+	u64 root_gen;
 	struct extent_buffer *c;
 	struct extent_buffer *split;
 	struct btrfs_disk_key disk_key;
@@ -1303,6 +1373,7 @@ static int split_node(struct btrfs_trans_handle *trans, struct btrfs_root
 	u32 c_nritems;
 
 	c = path->nodes[level];
+	WARN_ON(btrfs_header_generation(c) != trans->transid);
 	if (c == root->node) {
 		/* trying to split the root, lets make a new one */
 		ret = insert_new_root(trans, root, path, level + 1);
@@ -1319,8 +1390,17 @@ static int split_node(struct btrfs_trans_handle *trans, struct btrfs_root
 	}
 
 	c_nritems = btrfs_header_nritems(c);
-	split = btrfs_alloc_free_block(trans, root, root->nodesize,
-				       c->start, 0);
+	if (root->ref_cows)
+		root_gen = trans->transid;
+	else
+		root_gen = 0;
+
+	btrfs_node_key(c, &disk_key, 0);
+	split = __btrfs_alloc_free_block(trans, root, root->nodesize,
+					 root->root_key.objectid,
+					 root_gen,
+					 btrfs_disk_key_objectid(&disk_key),
+					 level, c->start, 0);
 	if (IS_ERR(split))
 		return PTR_ERR(split);
 
@@ -1789,6 +1869,7 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 		      *root, struct btrfs_key *ins_key,
 		      struct btrfs_path *path, int data_size, int extend)
 {
+	u64 root_gen;
 	struct extent_buffer *l;
 	u32 nritems;
 	int mid;
@@ -1807,6 +1888,11 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 	if (extend)
 		space_needed = data_size;
 
+	if (root->ref_cows)
+		root_gen = trans->transid;
+	else
+		root_gen = 0;
+
 	/* first try to make some room by pushing left and right */
 	if (ins_key->type != BTRFS_DIR_ITEM_KEY) {
 		wret = push_leaf_right(trans, root, path, data_size, 0);
@@ -1837,8 +1923,12 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 	nritems = btrfs_header_nritems(l);
 	mid = (nritems + 1)/ 2;
 
-	right = btrfs_alloc_free_block(trans, root, root->leafsize,
-				       l->start, 0);
+	btrfs_item_key(l, &disk_key, 0);
+
+	right = __btrfs_alloc_free_block(trans, root, root->leafsize,
+					 root->root_key.objectid,
+					 root_gen, disk_key.objectid, 0,
+					 l->start, 0);
 	if (IS_ERR(right))
 		return PTR_ERR(right);
 
@@ -2413,13 +2503,16 @@ int btrfs_del_item(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 		if (leaf == root->node) {
 			btrfs_set_header_level(leaf, 0);
 		} else {
+			u64 root_gen = btrfs_header_generation(path->nodes[1]);
 			clean_tree_block(trans, root, leaf);
 			wait_on_tree_block_writeback(root, leaf);
 			wret = del_ptr(trans, root, path, 1, path->slots[1]);
 			if (wret)
 				ret = wret;
 			wret = btrfs_free_extent(trans, root,
-						 leaf->start, leaf->len, 1);
+					 leaf->start, leaf->len,
+					 btrfs_header_owner(path->nodes[1]),
+					 root_gen, 0, 0, 1);
 			if (wret)
 				ret = wret;
 		}
@@ -2456,9 +2549,13 @@ int btrfs_del_item(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 			}
 
 			if (btrfs_header_nritems(leaf) == 0) {
+				u64 root_gen;
 				u64 bytenr = leaf->start;
 				u32 blocksize = leaf->len;
 
+				root_gen = btrfs_header_generation(
+							   path->nodes[1]);
+
 				clean_tree_block(trans, root, leaf);
 				wait_on_tree_block_writeback(root, leaf);
 
@@ -2468,7 +2565,9 @@ int btrfs_del_item(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 
 				free_extent_buffer(leaf);
 				wret = btrfs_free_extent(trans, root, bytenr,
-							 blocksize, 1);
+					     blocksize,
+					     btrfs_header_owner(path->nodes[1]),
+					     root_gen, 0, 0, 1);
 				if (wret)
 					ret = wret;
 			} else {
@@ -2482,6 +2581,61 @@ int btrfs_del_item(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 	return ret;
 }
 
+/*
+ * walk up the tree as far as required to find the previous leaf.
+ * returns 0 if it found something or 1 if there are no lesser leaves.
+ * returns < 0 on io errors.
+ */
+int btrfs_prev_leaf(struct btrfs_root *root, struct btrfs_path *path)
+{
+	int slot;
+	int level = 1;
+	u64 bytenr;
+	struct extent_buffer *c;
+	struct extent_buffer *next = NULL;
+
+	while(level < BTRFS_MAX_LEVEL) {
+		if (!path->nodes[level])
+			return 1;
+
+		slot = path->slots[level];
+		c = path->nodes[level];
+		if (slot == 0) {
+			level++;
+			if (level == BTRFS_MAX_LEVEL)
+				return 1;
+			continue;
+		}
+		slot--;
+
+		bytenr = btrfs_node_blockptr(c, slot);
+		if (next)
+			free_extent_buffer(next);
+
+		if (path->reada < 0)
+			reada_for_search(root, path, level, slot);
+
+		next = read_tree_block(root, bytenr,
+				       btrfs_level_size(root, level - 1));
+		break;
+	}
+	path->slots[level] = slot;
+	while(1) {
+		level--;
+		c = path->nodes[level];
+		free_extent_buffer(c);
+		path->nodes[level] = next;
+		path->slots[level] = 0;
+		if (!level)
+			break;
+		if (path->reada)
+			reada_for_search(root, path, level, 0);
+		next = read_tree_block(root, btrfs_node_blockptr(next, 0),
+				       btrfs_level_size(root, level - 1));
+	}
+	return 0;
+}
+
 /*
  * walk up the tree as far as required to find the next leaf.
  * returns 0 if it found something or 1 if there are no greater leaves.
@@ -2503,6 +2657,8 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 		c = path->nodes[level];
 		if (slot >= btrfs_header_nritems(c)) {
 			level++;
+			if (level == BTRFS_MAX_LEVEL)
+				return 1;
 			continue;
 		}
 

commit 74493f7a59bfd4d1c7029c74ab2cd0e400612c6b
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Dec 11 09:25:06 2007 -0500

    Btrfs: Implement generation numbers in block pointers
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 3b16051b121b..5697705f7530 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -114,6 +114,9 @@ static int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 	} else {
 		btrfs_set_node_blockptr(parent, parent_slot,
 					cow->start);
+		WARN_ON(trans->transid == 0);
+		btrfs_set_node_ptr_generation(parent, parent_slot,
+					      trans->transid);
 		btrfs_mark_buffer_dirty(parent);
 		WARN_ON(btrfs_header_generation(parent) != trans->transid);
 		btrfs_free_extent(trans, root, buf->start, buf->len, 1);
@@ -967,6 +970,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 {
 	struct extent_buffer *b;
 	u64 bytenr;
+	u64 ptr_gen;
 	int slot;
 	int ret;
 	int level;
@@ -1031,10 +1035,18 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 			if (level == lowest_level)
 				break;
 			bytenr = btrfs_node_blockptr(b, slot);
+			ptr_gen = btrfs_node_ptr_generation(b, slot);
 			if (should_reada)
 				reada_for_search(root, p, level, slot);
 			b = read_tree_block(root, bytenr,
 					    btrfs_level_size(root, level - 1));
+			if (ptr_gen != btrfs_header_generation(b)) {
+				printk("block %llu bad gen wanted %llu "
+				       "found %llu\n",
+			        (unsigned long long)b->start,
+				(unsigned long long)ptr_gen,
+			        (unsigned long long)btrfs_header_generation(b));
+			}
 		} else {
 			p->slots[level] = slot;
 			if (ins_len > 0 && btrfs_leaf_free_space(root, b) <
@@ -1218,6 +1230,8 @@ static int insert_new_root(struct btrfs_trans_handle *trans,
 		btrfs_node_key(lower, &lower_key, 0);
 	btrfs_set_node_key(c, &lower_key, 0);
 	btrfs_set_node_blockptr(c, 0, lower->start);
+	WARN_ON(btrfs_header_generation(lower) == 0);
+	btrfs_set_node_ptr_generation(c, 0, btrfs_header_generation(lower));
 
 	btrfs_mark_buffer_dirty(c);
 
@@ -1261,6 +1275,8 @@ static int insert_ptr(struct btrfs_trans_handle *trans, struct btrfs_root
 	}
 	btrfs_set_node_key(lower, key, slot);
 	btrfs_set_node_blockptr(lower, slot, bytenr);
+	WARN_ON(trans->transid == 0);
+	btrfs_set_node_ptr_generation(lower, slot, trans->transid);
 	btrfs_set_header_nritems(lower, nritems + 1);
 	btrfs_mark_buffer_dirty(lower);
 	return 0;

commit eef1c494a2d4212e13c67d05e9cc3cd1e6dfed5d
Author: Yan <yanzheng@21cn.com>
Date:   Mon Nov 26 10:58:13 2007 -0500

    Btrfs: Properly update right_nritems in push_leaf_left
    
    The codes that fixup the right leaf and the codes that dirty the
    extnet buffer use the variable 'right_nritems' ,  both of them expect
    'right_nritems' is the number of items in right leaf after the push.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 1b47fe71e0b4..3b16051b121b 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1717,11 +1717,10 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 			      btrfs_item_nr_offset(push_items),
 			     (btrfs_header_nritems(right) - push_items) *
 			     sizeof(struct btrfs_item));
-
 	}
-	btrfs_set_header_nritems(right, right_nritems - push_items);
+	right_nritems -= push_items;
+	btrfs_set_header_nritems(right, right_nritems);
 	push_space = BTRFS_LEAF_DATA_SIZE(root);
-
 	for (i = 0; i < right_nritems; i++) {
 		item = btrfs_item_nr(right, i);
 

commit 34a3821873aeabff2607c8093bce82cd1fbcfd60
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Nov 7 13:31:03 2007 -0500

    Btrfs: Change push_leaf_{leaf,right} to empty the src leave during item deletion
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index ea9b46699349..1b47fe71e0b4 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1394,19 +1394,21 @@ int btrfs_leaf_free_space(struct btrfs_root *root, struct extent_buffer *leaf)
  * room, 0 if everything worked out and < 0 if there were major errors.
  */
 static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
-			   *root, struct btrfs_path *path, int data_size)
+			   *root, struct btrfs_path *path, int data_size,
+			   int empty)
 {
 	struct extent_buffer *left = path->nodes[0];
 	struct extent_buffer *right;
 	struct extent_buffer *upper;
 	struct btrfs_disk_key disk_key;
 	int slot;
-	int i;
+	u32 i;
 	int free_space;
 	int push_space = 0;
 	int push_items = 0;
 	struct btrfs_item *item;
 	u32 left_nritems;
+	u32 nr;
 	u32 right_nritems;
 	u32 data_end;
 	u32 this_item_size;
@@ -1447,7 +1449,13 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 		return 1;
 	}
 
-	for (i = left_nritems - 1; i >= 1; i--) {
+	if (empty)
+		nr = 0;
+	else
+		nr = 1;
+
+	i = left_nritems - 1;
+	while (i >= nr) {
 		item = btrfs_item_nr(left, i);
 
 		if (path->slots[0] == i)
@@ -1466,6 +1474,9 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 			break;
 		push_items++;
 		push_space += this_item_size + sizeof(*item);
+		if (i == 0)
+			break;
+		i--;
 	}
 	if (left->map_token) {
 		unmap_extent_buffer(left, left->map_token, KM_USER1);
@@ -1477,11 +1488,12 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 		return 1;
 	}
 
-	if (push_items == left_nritems)
+	if (!empty && push_items == left_nritems)
 		WARN_ON(1);
 
 	/* push left to right */
 	right_nritems = btrfs_header_nritems(right);
+
 	push_space = btrfs_item_end_nr(left, left_nritems - push_items);
 	push_space -= leaf_data_end(root, left);
 
@@ -1511,7 +1523,6 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 	right_nritems += push_items;
 	btrfs_set_header_nritems(right, right_nritems);
 	push_space = BTRFS_LEAF_DATA_SIZE(root);
-
 	for (i = 0; i < right_nritems; i++) {
 		item = btrfs_item_nr(right, i);
 		if (!right->map_token) {
@@ -1532,7 +1543,8 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 	left_nritems -= push_items;
 	btrfs_set_header_nritems(left, left_nritems);
 
-	btrfs_mark_buffer_dirty(left);
+	if (left_nritems)
+		btrfs_mark_buffer_dirty(left);
 	btrfs_mark_buffer_dirty(right);
 
 	btrfs_item_key(right, &disk_key, 0);
@@ -1555,7 +1567,8 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
  * least data_size bytes.  returns zero if the push worked, nonzero otherwise
  */
 static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
-			  *root, struct btrfs_path *path, int data_size)
+			  *root, struct btrfs_path *path, int data_size,
+			  int empty)
 {
 	struct btrfs_disk_key disk_key;
 	struct extent_buffer *right = path->nodes[0];
@@ -1568,6 +1581,7 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 	struct btrfs_item *item;
 	u32 old_left_nritems;
 	u32 right_nritems;
+	u32 nr;
 	int ret = 0;
 	int wret;
 	u32 this_item_size;
@@ -1607,7 +1621,12 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 		return 1;
 	}
 
-	for (i = 0; i < right_nritems - 1; i++) {
+	if (empty)
+		nr = right_nritems;
+	else
+		nr = right_nritems - 1;
+
+	for (i = 0; i < nr; i++) {
 		item = btrfs_item_nr(right, i);
 		if (!right->map_token) {
 			map_extent_buffer(right, (unsigned long)item,
@@ -1637,7 +1656,7 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 		free_extent_buffer(left);
 		return 1;
 	}
-	if (push_items == btrfs_header_nritems(right))
+	if (!empty && push_items == btrfs_header_nritems(right))
 		WARN_ON(1);
 
 	/* push data from right to left */
@@ -1681,20 +1700,26 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 	}
 
 	/* fixup right node */
-	push_space = btrfs_item_offset_nr(right, push_items - 1) -
-					  leaf_data_end(root, right);
-	memmove_extent_buffer(right, btrfs_leaf_data(right) +
-			      BTRFS_LEAF_DATA_SIZE(root) - push_space,
-			      btrfs_leaf_data(right) +
-			      leaf_data_end(root, right), push_space);
-
-	memmove_extent_buffer(right, btrfs_item_nr_offset(0),
+	if (push_items > right_nritems) {
+		printk("push items %d nr %u\n", push_items, right_nritems);
+		WARN_ON(1);
+	}
+
+	if (push_items < right_nritems) {
+		push_space = btrfs_item_offset_nr(right, push_items - 1) -
+						  leaf_data_end(root, right);
+		memmove_extent_buffer(right, btrfs_leaf_data(right) +
+				      BTRFS_LEAF_DATA_SIZE(root) - push_space,
+				      btrfs_leaf_data(right) +
+				      leaf_data_end(root, right), push_space);
+
+		memmove_extent_buffer(right, btrfs_item_nr_offset(0),
 			      btrfs_item_nr_offset(push_items),
 			     (btrfs_header_nritems(right) - push_items) *
 			     sizeof(struct btrfs_item));
 
-	right_nritems = btrfs_header_nritems(right) - push_items;
-	btrfs_set_header_nritems(right, right_nritems);
+	}
+	btrfs_set_header_nritems(right, right_nritems - push_items);
 	push_space = BTRFS_LEAF_DATA_SIZE(root);
 
 	for (i = 0; i < right_nritems; i++) {
@@ -1717,7 +1742,8 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 	}
 
 	btrfs_mark_buffer_dirty(left);
-	btrfs_mark_buffer_dirty(right);
+	if (right_nritems)
+		btrfs_mark_buffer_dirty(right);
 
 	btrfs_item_key(right, &disk_key, 0);
 	wret = fixup_low_keys(trans, root, path, &disk_key, 1);
@@ -1768,12 +1794,12 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 
 	/* first try to make some room by pushing left and right */
 	if (ins_key->type != BTRFS_DIR_ITEM_KEY) {
-		wret = push_leaf_right(trans, root, path, data_size);
+		wret = push_leaf_right(trans, root, path, data_size, 0);
 		if (wret < 0) {
 			return wret;
 		}
 		if (wret) {
-			wret = push_leaf_left(trans, root, path, data_size);
+			wret = push_leaf_left(trans, root, path, data_size, 0);
 			if (wret < 0)
 				return wret;
 		}
@@ -2403,13 +2429,13 @@ int btrfs_del_item(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 			slot = path->slots[1];
 			extent_buffer_get(leaf);
 
-			wret = push_leaf_right(trans, root, path, 1);
+			wret = push_leaf_right(trans, root, path, 1, 1);
 			if (wret < 0 && wret != -ENOSPC)
 				ret = wret;
 
 			if (path->nodes[0] == leaf &&
 			    btrfs_header_nritems(leaf)) {
-				wret = push_leaf_left(trans, root, path, 1);
+				wret = push_leaf_left(trans, root, path, 1, 1);
 				if (wret < 0 && wret != -ENOSPC)
 					ret = wret;
 			}

commit 081e95736d9118a96e9f6dcc5cec02dc75c9e1cb
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Nov 6 10:26:24 2007 -0500

    Btrfs: Make defrag check nodes against the progress key to prevent repeating work
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index eef9c92f86d5..ea9b46699349 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -161,6 +161,31 @@ static int close_blocks(u64 blocknr, u64 other, u32 blocksize)
 	return 0;
 }
 
+/*
+ * compare two keys in a memcmp fashion
+ */
+static int comp_keys(struct btrfs_disk_key *disk, struct btrfs_key *k2)
+{
+	struct btrfs_key k1;
+
+	btrfs_disk_key_to_cpu(&k1, disk);
+
+	if (k1.objectid > k2->objectid)
+		return 1;
+	if (k1.objectid < k2->objectid)
+		return -1;
+	if (k1.type > k2->type)
+		return 1;
+	if (k1.type < k2->type)
+		return -1;
+	if (k1.offset > k2->offset)
+		return 1;
+	if (k1.offset < k2->offset)
+		return -1;
+	return 0;
+}
+
+
 int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 		       struct btrfs_root *root, struct extent_buffer *parent,
 		       int start_slot, int cache_only, u64 *last_ret,
@@ -179,6 +204,8 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 	int parent_level;
 	int uptodate;
 	u32 blocksize;
+	int progress_passed = 0;
+	struct btrfs_disk_key disk_key;
 
 	parent_level = btrfs_header_level(parent);
 	if (cache_only && parent_level != 1)
@@ -213,6 +240,11 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 					&parent->map_start, &parent->map_len,
 					KM_USER1);
 		}
+		btrfs_node_key(parent, &disk_key, i);
+		if (!progress_passed && comp_keys(&disk_key, progress) < 0)
+			continue;
+
+		progress_passed = 1;
 		blocknr = btrfs_node_blockptr(parent, i);
 		if (last_block == 0)
 			last_block = blocknr;
@@ -292,30 +324,6 @@ static inline unsigned int leaf_data_end(struct btrfs_root *root,
 	return btrfs_item_offset_nr(leaf, nr - 1);
 }
 
-/*
- * compare two keys in a memcmp fashion
- */
-static int comp_keys(struct btrfs_disk_key *disk, struct btrfs_key *k2)
-{
-	struct btrfs_key k1;
-
-	btrfs_disk_key_to_cpu(&k1, disk);
-
-	if (k1.objectid > k2->objectid)
-		return 1;
-	if (k1.objectid < k2->objectid)
-		return -1;
-	if (k1.type > k2->type)
-		return 1;
-	if (k1.type < k2->type)
-		return -1;
-	if (k1.offset > k2->offset)
-		return 1;
-	if (k1.offset < k2->offset)
-		return -1;
-	return 0;
-}
-
 static int check_node(struct btrfs_root *root, struct btrfs_path *path,
 		      int level)
 {

commit 179e29e488cc74f1e9bd67bc45f70b832740e9ec
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Nov 1 11:28:41 2007 -0400

    Btrfs: Fix a number of inline extent problems that Yan Zheng reported.
    
    The fixes do a number of things:
    
    1) Most btrfs_drop_extent callers will try to leave the inline extents in
    place.  It can truncate bytes off the beginning of the inline extent if
    required.
    
    2) writepage can now update the inline extent, allowing mmap writes to
    go directly into the inline extent.
    
    3) btrfs_truncate_in_transaction truncates inline extents
    
    4) extent_map.c fixed to not merge inline extent mappings and hole
    mappings together
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 516b90224a1b..eef9c92f86d5 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1930,7 +1930,7 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 int btrfs_truncate_item(struct btrfs_trans_handle *trans,
 			struct btrfs_root *root,
 			struct btrfs_path *path,
-			u32 new_size)
+			u32 new_size, int from_end)
 {
 	int ret = 0;
 	int slot;
@@ -1946,13 +1946,17 @@ int btrfs_truncate_item(struct btrfs_trans_handle *trans,
 
 	slot_orig = path->slots[0];
 	leaf = path->nodes[0];
+	slot = path->slots[0];
+
+	old_size = btrfs_item_size_nr(leaf, slot);
+	if (old_size == new_size)
+		return 0;
 
 	nritems = btrfs_header_nritems(leaf);
 	data_end = leaf_data_end(root, leaf);
 
-	slot = path->slots[0];
 	old_data_start = btrfs_item_offset_nr(leaf, slot);
-	old_size = btrfs_item_size_nr(leaf, slot); BUG_ON(old_size <= new_size);
+
 	size_diff = old_size - new_size;
 
 	BUG_ON(slot < 0);
@@ -1984,9 +1988,45 @@ int btrfs_truncate_item(struct btrfs_trans_handle *trans,
 	}
 
 	/* shift the data */
-	memmove_extent_buffer(leaf, btrfs_leaf_data(leaf) +
-		      data_end + size_diff, btrfs_leaf_data(leaf) +
-		      data_end, old_data_start + new_size - data_end);
+	if (from_end) {
+		memmove_extent_buffer(leaf, btrfs_leaf_data(leaf) +
+			      data_end + size_diff, btrfs_leaf_data(leaf) +
+			      data_end, old_data_start + new_size - data_end);
+	} else {
+		struct btrfs_disk_key disk_key;
+		u64 offset;
+
+		btrfs_item_key(leaf, &disk_key, slot);
+
+		if (btrfs_disk_key_type(&disk_key) == BTRFS_EXTENT_DATA_KEY) {
+			unsigned long ptr;
+			struct btrfs_file_extent_item *fi;
+
+			fi = btrfs_item_ptr(leaf, slot,
+					    struct btrfs_file_extent_item);
+			fi = (struct btrfs_file_extent_item *)(
+			     (unsigned long)fi - size_diff);
+
+			if (btrfs_file_extent_type(leaf, fi) ==
+			    BTRFS_FILE_EXTENT_INLINE) {
+				ptr = btrfs_item_ptr_offset(leaf, slot);
+				memmove_extent_buffer(leaf, ptr,
+				        (unsigned long)fi,
+				        offsetof(struct btrfs_file_extent_item,
+						 disk_bytenr));
+			}
+		}
+
+		memmove_extent_buffer(leaf, btrfs_leaf_data(leaf) +
+			      data_end + size_diff, btrfs_leaf_data(leaf) +
+			      data_end, old_data_start - data_end);
+
+		offset = btrfs_disk_key_offset(&disk_key);
+		btrfs_set_disk_key_offset(&disk_key, offset + size_diff);
+		btrfs_set_item_key(leaf, &disk_key, slot);
+		if (slot == 0)
+			fixup_low_keys(trans, root, path, &disk_key, 1);
+	}
 
 	item = btrfs_item_nr(leaf, slot);
 	btrfs_set_item_size(leaf, item, new_size);

commit 5708b9591617486bf1aa5b1a97f2c0549ec87933
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Oct 25 15:43:18 2007 -0400

    Btrfs: Tune the automatic defrag code
    
    1) Forced defrag wasn't working properly (btrfsctl -d) because some
    cache only checks were incorrect.
    
    2) Defrag only the leaves unless in forced defrag mode.
    
    3) Don't use complex logic to figure out if a leaf is needs defrag
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 3eb5a9f30d14..516b90224a1b 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -161,34 +161,6 @@ static int close_blocks(u64 blocknr, u64 other, u32 blocksize)
 	return 0;
 }
 
-static int should_defrag_leaf(struct extent_buffer *leaf)
-{
-	struct btrfs_key key;
-	u32 nritems;
-
-	if (btrfs_buffer_defrag(leaf))
-		return 1;
-
-	nritems = btrfs_header_nritems(leaf);
-	if (nritems == 0)
-		return 0;
-
-	btrfs_item_key_to_cpu(leaf, &key, 0);
-	if (key.type == BTRFS_DIR_ITEM_KEY)
-		return 1;
-
-
-	btrfs_item_key_to_cpu(leaf, &key, nritems - 1);
-	if (key.type == BTRFS_DIR_ITEM_KEY)
-		return 1;
-	if (nritems > 4) {
-		btrfs_item_key_to_cpu(leaf, &key, nritems / 2);
-		if (key.type == BTRFS_DIR_ITEM_KEY)
-			return 1;
-	}
-	return 0;
-}
-
 int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 		       struct btrfs_root *root, struct extent_buffer *parent,
 		       int start_slot, int cache_only, u64 *last_ret,
@@ -208,6 +180,10 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 	int uptodate;
 	u32 blocksize;
 
+	parent_level = btrfs_header_level(parent);
+	if (cache_only && parent_level != 1)
+		return 0;
+
 	if (trans->transaction != root->fs_info->running_transaction) {
 		printk(KERN_CRIT "trans %Lu running %Lu\n", trans->transid,
 		       root->fs_info->running_transaction->transid);
@@ -218,7 +194,6 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 		       root->fs_info->generation);
 		WARN_ON(1);
 	}
-	parent_level = btrfs_header_level(parent);
 
 	parent_nritems = btrfs_header_nritems(parent);
 	blocksize = btrfs_level_size(root, parent_level - 1);
@@ -227,27 +202,26 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 	if (parent_nritems == 1)
 		return 0;
 
-	if (root != root->fs_info->extent_root) {
-		struct btrfs_key first_key;
-		struct btrfs_key last_key;
-
-		btrfs_node_key_to_cpu(parent, &first_key, 0);
-		btrfs_node_key_to_cpu(parent, &last_key, parent_nritems - 1);
-		if (first_key.objectid != last_key.objectid)
-			return 0;
-	}
-
 	for (i = start_slot; i < end_slot; i++) {
 		int close = 1;
 
+		if (!parent->map_token) {
+			map_extent_buffer(parent,
+					btrfs_node_key_ptr_offset(i),
+					sizeof(struct btrfs_key_ptr),
+					&parent->map_token, &parent->kaddr,
+					&parent->map_start, &parent->map_len,
+					KM_USER1);
+		}
 		blocknr = btrfs_node_blockptr(parent, i);
 		if (last_block == 0)
 			last_block = blocknr;
+
 		if (i > 0) {
 			other = btrfs_node_blockptr(parent, i - 1);
 			close = close_blocks(blocknr, other, blocksize);
 		}
-		if (close && i < end_slot - 1) {
+		if (close && i < end_slot - 2) {
 			other = btrfs_node_blockptr(parent, i + 1);
 			close = close_blocks(blocknr, other, blocksize);
 		}
@@ -255,15 +229,18 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 			last_block = blocknr;
 			continue;
 		}
+		if (parent->map_token) {
+			unmap_extent_buffer(parent, parent->map_token,
+					    KM_USER1);
+			parent->map_token = NULL;
+		}
 
 		cur = btrfs_find_tree_block(root, blocknr, blocksize);
 		if (cur)
 			uptodate = btrfs_buffer_uptodate(cur);
 		else
 			uptodate = 0;
-		if (!cur || !uptodate ||
-		    (parent_level != 1 && !btrfs_buffer_defrag(cur)) ||
-		    (parent_level == 1 && !should_defrag_leaf(cur))) {
+		if (!cur || !uptodate) {
 			if (cache_only) {
 				free_extent_buffer(cur);
 				continue;
@@ -287,11 +264,17 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 			break;
 		}
 		search_start = tmp->start;
+		last_block = tmp->start;
 		*last_ret = search_start;
 		if (parent_level == 1)
 			btrfs_clear_buffer_defrag(tmp);
 		free_extent_buffer(tmp);
 	}
+	if (parent->map_token) {
+		unmap_extent_buffer(parent, parent->map_token,
+				    KM_USER1);
+		parent->map_token = NULL;
+	}
 	return err;
 }
 

commit cc0c55384796b422133ff1f21646835b31590f88
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Oct 25 15:42:57 2007 -0400

    Btrfs: Fix split_leaf to detect when it is extending an item
    
    When making room for a new item, it is ok to create an empty leaf, but
    when making room to extend an item, split_leaf needs to make sure it
    keeps the item we're extending in the path and make sure we don't end up
    with an empty leaf.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 53e40b5c084d..3eb5a9f30d14 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -26,7 +26,7 @@ static int split_node(struct btrfs_trans_handle *trans, struct btrfs_root
 		      *root, struct btrfs_path *path, int level);
 static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 		      *root, struct btrfs_key *ins_key,
-		      struct btrfs_path *path, int data_size);
+		      struct btrfs_path *path, int data_size, int extend);
 static int push_node_left(struct btrfs_trans_handle *trans,
 			  struct btrfs_root *root, struct extent_buffer *dst,
 			  struct extent_buffer *src);
@@ -1049,7 +1049,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 			if (ins_len > 0 && btrfs_leaf_free_space(root, b) <
 			    sizeof(struct btrfs_item) + ins_len) {
 				int sret = split_leaf(trans, root, key,
-						      p, ins_len);
+						      p, ins_len, ret == 0);
 				BUG_ON(sret > 0);
 				if (sret)
 					return sret;
@@ -1755,7 +1755,7 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
  */
 static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 		      *root, struct btrfs_key *ins_key,
-		      struct btrfs_path *path, int data_size)
+		      struct btrfs_path *path, int data_size, int extend)
 {
 	struct extent_buffer *l;
 	u32 nritems;
@@ -1768,9 +1768,13 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 	int i;
 	int ret = 0;
 	int wret;
-	int double_split = 0;
+	int double_split;
+	int num_doubles = 0;
 	struct btrfs_disk_key disk_key;
 
+	if (extend)
+		space_needed = data_size;
+
 	/* first try to make some room by pushing left and right */
 	if (ins_key->type != BTRFS_DIR_ITEM_KEY) {
 		wret = push_leaf_right(trans, root, path, data_size);
@@ -1785,12 +1789,8 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 		l = path->nodes[0];
 
 		/* did the pushes work? */
-		if (btrfs_leaf_free_space(root, l) >=
-		    sizeof(struct btrfs_item) + data_size) {
+		if (btrfs_leaf_free_space(root, l) >= space_needed)
 			return 0;
-		}
-	} else {
-		l = path->nodes[0];
 	}
 
 	if (!path->nodes[1]) {
@@ -1798,6 +1798,9 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 		if (ret)
 			return ret;
 	}
+again:
+	double_split = 0;
+	l = path->nodes[0];
 	slot = path->slots[0];
 	nritems = btrfs_header_nritems(l);
 	mid = (nritems + 1)/ 2;
@@ -1815,7 +1818,6 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 	write_extent_buffer(right, root->fs_info->fsid,
 			    (unsigned long)btrfs_header_fsid(right),
 			    BTRFS_FSID_SIZE);
-
 	if (mid <= slot) {
 		if (nritems == 1 ||
 		    leaf_space_used(l, mid, nritems - mid) + space_needed >
@@ -1844,7 +1846,7 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 	} else {
 		if (leaf_space_used(l, 0, mid + 1) + space_needed >
 			BTRFS_LEAF_DATA_SIZE(root)) {
-			if (slot == 0) {
+			if (!extend && slot == 0) {
 				btrfs_cpu_key_to_disk(&disk_key, ins_key);
 				btrfs_set_header_nritems(right, 0);
 				wret = insert_ptr(trans, root, path,
@@ -1863,12 +1865,15 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 						ret = wret;
 				}
 				return ret;
-			}
-			mid = slot;
-			if (mid != nritems &&
-			    leaf_space_used(l, mid, nritems - mid) +
-			    space_needed > BTRFS_LEAF_DATA_SIZE(root)) {
-				double_split = 1;
+			} else if (extend && slot == 0) {
+				mid = 1;
+			} else {
+				mid = slot;
+				if (mid != nritems &&
+				    leaf_space_used(l, mid, nritems - mid) +
+				    space_needed > BTRFS_LEAF_DATA_SIZE(root)) {
+					double_split = 1;
+				}
 			}
 		}
 	}
@@ -1931,39 +1936,11 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 
 	BUG_ON(path->slots[0] < 0);
 
-	if (!double_split) {
-		return ret;
-	}
-
-	right = btrfs_alloc_free_block(trans, root, root->leafsize,
-				       l->start, 0);
-	if (IS_ERR(right))
-		return PTR_ERR(right);
-
-	memset_extent_buffer(right, 0, 0, sizeof(struct btrfs_header));
-	btrfs_set_header_bytenr(right, right->start);
-	btrfs_set_header_generation(right, trans->transid);
-	btrfs_set_header_owner(right, root->root_key.objectid);
-	btrfs_set_header_level(right, 0);
-	write_extent_buffer(right, root->fs_info->fsid,
-			    (unsigned long)btrfs_header_fsid(right),
-			    BTRFS_FSID_SIZE);
-
-	btrfs_cpu_key_to_disk(&disk_key, ins_key);
-	btrfs_set_header_nritems(right, 0);
-	wret = insert_ptr(trans, root, path,
-			  &disk_key, right->start,
-			  path->slots[1], 1);
-	if (wret)
-		ret = wret;
-	if (path->slots[1] == 0) {
-		wret = fixup_low_keys(trans, root, path, &disk_key, 1);
-		if (wret)
-			ret = wret;
+	if (double_split) {
+		BUG_ON(num_doubles != 0);
+		num_doubles++;
+		goto again;
 	}
-	free_extent_buffer(path->nodes[0]);
-	path->nodes[0] = right;
-	path->slots[0] = 0;
 	return ret;
 }
 
@@ -1992,8 +1969,7 @@ int btrfs_truncate_item(struct btrfs_trans_handle *trans,
 
 	slot = path->slots[0];
 	old_data_start = btrfs_item_offset_nr(leaf, slot);
-	old_size = btrfs_item_size_nr(leaf, slot);
-	BUG_ON(old_size <= new_size);
+	old_size = btrfs_item_size_nr(leaf, slot); BUG_ON(old_size <= new_size);
 	size_diff = old_size - new_size;
 
 	BUG_ON(slot < 0);

commit 5ee78ac70f6c01f2a5acf662c2af975665710edc
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Oct 19 14:01:21 2007 -0400

    Btrfs: Fix split_leaf to avoid incorrect double splits
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index e8466940fa30..53e40b5c084d 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1865,7 +1865,11 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 				return ret;
 			}
 			mid = slot;
-			double_split = 1;
+			if (mid != nritems &&
+			    leaf_space_used(l, mid, nritems - mid) +
+			    space_needed > BTRFS_LEAF_DATA_SIZE(root)) {
+				double_split = 1;
+			}
 		}
 	}
 	nritems = nritems - mid;

commit 3685f791659c9f21b763ee1702ac8ca58bc20f81
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Oct 19 09:23:27 2007 -0400

    Btrfs: CPU usage optimizations in push and the extent_map code
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 17a322663f3b..e8466940fa30 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1588,6 +1588,11 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 	if (!path->nodes[1])
 		return 1;
 
+	right_nritems = btrfs_header_nritems(right);
+	if (right_nritems == 0) {
+		return 1;
+	}
+
 	left = read_tree_block(root, btrfs_node_blockptr(path->nodes[1],
 			       slot - 1), root->leafsize);
 	free_space = btrfs_leaf_free_space(root, left);
@@ -1604,18 +1609,13 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 		free_extent_buffer(left);
 		return 1;
 	}
+
 	free_space = btrfs_leaf_free_space(root, left);
 	if (free_space < data_size + sizeof(struct btrfs_item)) {
 		free_extent_buffer(left);
 		return 1;
 	}
 
-	right_nritems = btrfs_header_nritems(right);
-	if (right_nritems == 0) {
-		free_extent_buffer(left);
-		return 1;
-	}
-
 	for (i = 0; i < right_nritems - 1; i++) {
 		item = btrfs_item_nr(right, i);
 		if (!right->map_token) {
@@ -1772,21 +1772,25 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 	struct btrfs_disk_key disk_key;
 
 	/* first try to make some room by pushing left and right */
-	wret = push_leaf_left(trans, root, path, data_size);
-	if (wret < 0) {
-		return wret;
-	}
-	if (wret) {
+	if (ins_key->type != BTRFS_DIR_ITEM_KEY) {
 		wret = push_leaf_right(trans, root, path, data_size);
-		if (wret < 0)
+		if (wret < 0) {
 			return wret;
-	}
-	l = path->nodes[0];
+		}
+		if (wret) {
+			wret = push_leaf_left(trans, root, path, data_size);
+			if (wret < 0)
+				return wret;
+		}
+		l = path->nodes[0];
 
-	/* did the pushes work? */
-	if (btrfs_leaf_free_space(root, l) >=
-	    sizeof(struct btrfs_item) + data_size) {
-		return 0;
+		/* did the pushes work? */
+		if (btrfs_leaf_free_space(root, l) >=
+		    sizeof(struct btrfs_item) + data_size) {
+			return 0;
+		}
+	} else {
+		l = path->nodes[0];
 	}
 
 	if (!path->nodes[1]) {
@@ -2388,13 +2392,13 @@ int btrfs_del_item(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 			slot = path->slots[1];
 			extent_buffer_get(leaf);
 
-			wret = push_leaf_left(trans, root, path, 1);
+			wret = push_leaf_right(trans, root, path, 1);
 			if (wret < 0 && wret != -ENOSPC)
 				ret = wret;
 
 			if (path->nodes[0] == leaf &&
 			    btrfs_header_nritems(leaf)) {
-				wret = push_leaf_right(trans, root, path, 1);
+				wret = push_leaf_left(trans, root, path, 1);
 				if (wret < 0 && wret != -ENOSPC)
 					ret = wret;
 			}

commit ae2f5411c4ce7180cca8418853db50c8e52d40db
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Fri Oct 19 09:22:59 2007 -0400

    btrfs: 32-bit type problems
    
    An assorted set of casts to get rid of the warnings on 32-bit archs.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 54967099a8eb..17a322663f3b 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1389,7 +1389,7 @@ int btrfs_leaf_free_space(struct btrfs_root *root, struct extent_buffer *leaf)
 	ret = BTRFS_LEAF_DATA_SIZE(root) - leaf_space_used(leaf, 0, nritems);
 	if (ret < 0) {
 		printk("leaf free space ret %d, leaf data size %lu, used %d nritems %d\n",
-		       ret, BTRFS_LEAF_DATA_SIZE(root),
+		       ret, (unsigned long) BTRFS_LEAF_DATA_SIZE(root),
 		       leaf_space_used(leaf, 0, nritems), nritems);
 	}
 	return ret;

commit 7936ca3883e5fef8ce5cc367a4356ad5fed67180
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Oct 19 09:22:41 2007 -0400

    Btrfs: Default to 8k max packed tails
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 74fec6b83a8b..54967099a8eb 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2380,7 +2380,7 @@ int btrfs_del_item(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 		}
 
 		/* delete the leaf if it is mostly empty */
-		if (0 && used < BTRFS_LEAF_DATA_SIZE(root) / 3) {
+		if (used < BTRFS_LEAF_DATA_SIZE(root) / 3) {
 			/* push_leaf_left fixes the path.
 			 * make sure the path still points to our leaf
 			 * for possible call to del_ptr below

commit a6b6e75e096f436f0cc56edf5bca96301e194491
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Oct 15 16:22:39 2007 -0400

    Btrfs: Defrag only leaves, and only when the parent node has a single objectid
    
    This allows us to defrag huge directories, but skip the expensive defrag
    case in more common usage, where it does not help as much.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 984f4745440e..74fec6b83a8b 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -16,6 +16,7 @@
  * Boston, MA 021110-1307, USA.
  */
 
+#include <linux/sched.h>
 #include "ctree.h"
 #include "disk-io.h"
 #include "transaction.h"
@@ -190,7 +191,8 @@ static int should_defrag_leaf(struct extent_buffer *leaf)
 
 int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 		       struct btrfs_root *root, struct extent_buffer *parent,
-		       int cache_only, u64 *last_ret)
+		       int start_slot, int cache_only, u64 *last_ret,
+		       struct btrfs_key *progress)
 {
 	struct extent_buffer *cur;
 	struct extent_buffer *tmp;
@@ -199,7 +201,6 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 	u64 last_block = 0;
 	u64 other;
 	u32 parent_nritems;
-	int start_slot;
 	int end_slot;
 	int i;
 	int err = 0;
@@ -221,15 +222,24 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 
 	parent_nritems = btrfs_header_nritems(parent);
 	blocksize = btrfs_level_size(root, parent_level - 1);
-
-	start_slot = 0;
 	end_slot = parent_nritems;
 
 	if (parent_nritems == 1)
 		return 0;
 
+	if (root != root->fs_info->extent_root) {
+		struct btrfs_key first_key;
+		struct btrfs_key last_key;
+
+		btrfs_node_key_to_cpu(parent, &first_key, 0);
+		btrfs_node_key_to_cpu(parent, &last_key, parent_nritems - 1);
+		if (first_key.objectid != last_key.objectid)
+			return 0;
+	}
+
 	for (i = start_slot; i < end_slot; i++) {
 		int close = 1;
+
 		blocknr = btrfs_node_blockptr(parent, i);
 		if (last_block == 0)
 			last_block = blocknr;
@@ -898,7 +908,7 @@ static void reada_for_search(struct btrfs_root *root, struct btrfs_path *path,
 	u32 blocksize;
 	u32 nscan = 0;
 
-	if (level == 0)
+	if (level != 1)
 		return;
 
 	if (!path->nodes[level])
@@ -2370,7 +2380,7 @@ int btrfs_del_item(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 		}
 
 		/* delete the leaf if it is mostly empty */
-		if (used < BTRFS_LEAF_DATA_SIZE(root) / 3) {
+		if (0 && used < BTRFS_LEAF_DATA_SIZE(root) / 3) {
 			/* push_leaf_left fixes the path.
 			 * make sure the path still points to our leaf
 			 * for possible call to del_ptr below

commit cf786e79e3f96f4bc037c69e1a7afb5fb85c6111
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Oct 15 16:22:11 2007 -0400

    Btrfs: Defrag: only walk into nodes with the defrag bit set
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index e1557e573d74..984f4745440e 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -280,7 +280,6 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 		*last_ret = search_start;
 		if (parent_level == 1)
 			btrfs_clear_buffer_defrag(tmp);
-		btrfs_set_buffer_defrag_done(tmp);
 		free_extent_buffer(tmp);
 	}
 	return err;

commit 0f1ebbd159baade0417f8f62f0cd6810cc950832
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Oct 15 16:19:46 2007 -0400

    Btrfs: Large block related defrag optimizations
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 0c6ed17ac1bc..e1557e573d74 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -217,11 +217,9 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 		       root->fs_info->generation);
 		WARN_ON(1);
 	}
-	if (btrfs_buffer_defrag_done(parent))
-		return 0;
+	parent_level = btrfs_header_level(parent);
 
 	parent_nritems = btrfs_header_nritems(parent);
-	parent_level = btrfs_header_level(parent);
 	blocksize = btrfs_level_size(root, parent_level - 1);
 
 	start_slot = 0;

commit 0f82731fc56448c2733f58e1f5db6c2cbfc90652
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Oct 15 16:18:56 2007 -0400

    Breakout BTRFS_SETGET_FUNCS into a separate C file, the inlines were too big.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 54a5d006c562..0c6ed17ac1bc 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -16,7 +16,6 @@
  * Boston, MA 021110-1307, USA.
  */
 
-#include <linux/highmem.h>
 #include "ctree.h"
 #include "disk-io.h"
 #include "transaction.h"

commit 810191ff3087e8143b41a944fcf4fd8c693f00e3
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Oct 15 16:18:55 2007 -0400

    Btrfs: extent_map optimizations to cut down on CPU usage
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index f691542dfa15..54a5d006c562 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -463,6 +463,7 @@ static int check_leaf(struct btrfs_root *root, struct btrfs_path *path,
 static int check_block(struct btrfs_root *root, struct btrfs_path *path,
 			int level)
 {
+	return 0;
 #if 0
 	struct extent_buffer *buf = path->nodes[level];
 

commit 3326d1b07c0cb6a2ff5b835b7a2cffa54124d074
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Oct 15 16:18:25 2007 -0400

    Btrfs: Allow tails larger than one page
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index bef61ee883e1..f691542dfa15 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1766,8 +1766,9 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 
 	/* first try to make some room by pushing left and right */
 	wret = push_leaf_left(trans, root, path, data_size);
-	if (wret < 0)
+	if (wret < 0) {
 		return wret;
+	}
 	if (wret) {
 		wret = push_leaf_right(trans, root, path, data_size);
 		if (wret < 0)
@@ -1777,8 +1778,9 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 
 	/* did the pushes work? */
 	if (btrfs_leaf_free_space(root, l) >=
-	    sizeof(struct btrfs_item) + data_size)
+	    sizeof(struct btrfs_item) + data_size) {
 		return 0;
+	}
 
 	if (!path->nodes[1]) {
 		ret = insert_new_root(trans, root, path, 1);
@@ -1822,7 +1824,11 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 				return ret;
 			}
 			mid = slot;
-			double_split = 1;
+			if (mid != nritems &&
+			    leaf_space_used(l, mid, nritems - mid) +
+			    space_needed > BTRFS_LEAF_DATA_SIZE(root)) {
+				double_split = 1;
+			}
 		}
 	} else {
 		if (leaf_space_used(l, 0, mid + 1) + space_needed >
@@ -1910,8 +1916,9 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 
 	BUG_ON(path->slots[0] < 0);
 
-	if (!double_split)
+	if (!double_split) {
 		return ret;
+	}
 
 	right = btrfs_alloc_free_block(trans, root, root->leafsize,
 				       l->start, 0);
@@ -2048,7 +2055,11 @@ int btrfs_extend_item(struct btrfs_trans_handle *trans,
 	old_data = btrfs_item_end_nr(leaf, slot);
 
 	BUG_ON(slot < 0);
-	BUG_ON(slot >= nritems);
+	if (slot >= nritems) {
+		btrfs_print_leaf(root, leaf);
+		printk("slot %d too large, nritems %d\n", slot, nritems);
+		BUG_ON(1);
+	}
 
 	/*
 	 * item0..itemN ... dataN.offset..dataN.size .. data0.size
@@ -2132,6 +2143,9 @@ int btrfs_insert_empty_item(struct btrfs_trans_handle *trans,
 
 	if (btrfs_leaf_free_space(root, leaf) <
 	    sizeof(struct btrfs_item) + data_size) {
+		btrfs_print_leaf(root, leaf);
+		printk("not enough freespace need %u have %d\n",
+		       data_size, btrfs_leaf_free_space(root, leaf));
 		BUG();
 	}
 

commit 4dc119046d0d8501afa4346472917fb05586ad9c
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Oct 15 16:18:14 2007 -0400

    Btrfs: Add an extent buffer LRU to reduce radix tree hits
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 08ddf1873ab1..bef61ee883e1 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -88,8 +88,6 @@ static int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 	if (IS_ERR(cow))
 		return PTR_ERR(cow);
 
-	cow->alloc_addr = (unsigned long)__builtin_return_address(0);
-
 	copy_extent_buffer(cow, buf, 0, 0, cow->len);
 	btrfs_set_header_bytenr(cow, cow->start);
 	btrfs_set_header_generation(cow, trans->transid);
@@ -151,7 +149,6 @@ int btrfs_cow_block(struct btrfs_trans_handle *trans,
 	search_start = buf->start & ~((u64)BTRFS_BLOCK_GROUP_SIZE - 1);
 	ret = __btrfs_cow_block(trans, root, buf, parent,
 				 parent_slot, cow_ret, search_start, 0);
-	(*cow_ret)->alloc_addr = (unsigned long)__builtin_return_address(0);
 	return ret;
 }
 

commit 6b80053d02be41886344b5007d04e345311ec0b5
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Oct 15 16:17:34 2007 -0400

    Btrfs: Add back the online defragging code
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 34de83630ae9..08ddf1873ab1 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -155,55 +155,49 @@ int btrfs_cow_block(struct btrfs_trans_handle *trans,
 	return ret;
 }
 
-#if 0
-static int close_blocks(u64 blocknr, u64 other)
+static int close_blocks(u64 blocknr, u64 other, u32 blocksize)
 {
-	if (blocknr < other && other - blocknr < 8)
+	if (blocknr < other && other - (blocknr + blocksize) < 32768)
 		return 1;
-	if (blocknr > other && blocknr - other < 8)
+	if (blocknr > other && blocknr - (other + blocksize) < 32768)
 		return 1;
 	return 0;
 }
 
-static int should_defrag_leaf(struct extent_buffer *eb)
+static int should_defrag_leaf(struct extent_buffer *leaf)
 {
-	return 0;
-	struct btrfs_leaf *leaf = btrfs_buffer_leaf(eb);
-	struct btrfs_disk_key *key;
+	struct btrfs_key key;
 	u32 nritems;
 
-	if (buffer_defrag(bh))
+	if (btrfs_buffer_defrag(leaf))
 		return 1;
 
-	nritems = btrfs_header_nritems(&leaf->header);
+	nritems = btrfs_header_nritems(leaf);
 	if (nritems == 0)
 		return 0;
 
-	key = &leaf->items[0].key;
-	if (btrfs_disk_key_type(key) == BTRFS_DIR_ITEM_KEY)
+	btrfs_item_key_to_cpu(leaf, &key, 0);
+	if (key.type == BTRFS_DIR_ITEM_KEY)
 		return 1;
 
-	key = &leaf->items[nritems-1].key;
-	if (btrfs_disk_key_type(key) == BTRFS_DIR_ITEM_KEY)
+
+	btrfs_item_key_to_cpu(leaf, &key, nritems - 1);
+	if (key.type == BTRFS_DIR_ITEM_KEY)
 		return 1;
 	if (nritems > 4) {
-		key = &leaf->items[nritems/2].key;
-		if (btrfs_disk_key_type(key) == BTRFS_DIR_ITEM_KEY)
+		btrfs_item_key_to_cpu(leaf, &key, nritems / 2);
+		if (key.type == BTRFS_DIR_ITEM_KEY)
 			return 1;
 	}
 	return 0;
 }
-#endif
 
 int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 		       struct btrfs_root *root, struct extent_buffer *parent,
 		       int cache_only, u64 *last_ret)
 {
-	return 0;
-#if 0
-	struct btrfs_node *parent_node;
-	struct extent_buffer *cur_eb;
-	struct extent_buffer *tmp_eb;
+	struct extent_buffer *cur;
+	struct extent_buffer *tmp;
 	u64 blocknr;
 	u64 search_start = *last_ret;
 	u64 last_block = 0;
@@ -214,6 +208,8 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 	int i;
 	int err = 0;
 	int parent_level;
+	int uptodate;
+	u32 blocksize;
 
 	if (trans->transaction != root->fs_info->running_transaction) {
 		printk(KERN_CRIT "trans %Lu running %Lu\n", trans->transid,
@@ -225,12 +221,12 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 		       root->fs_info->generation);
 		WARN_ON(1);
 	}
-	if (buffer_defrag_done(parent))
+	if (btrfs_buffer_defrag_done(parent))
 		return 0;
 
-	parent_node = btrfs_buffer_node(parent);
-	parent_nritems = btrfs_header_nritems(&parent_node->header);
-	parent_level = btrfs_header_level(&parent_node->header);
+	parent_nritems = btrfs_header_nritems(parent);
+	parent_level = btrfs_header_level(parent);
+	blocksize = btrfs_level_size(root, parent_level - 1);
 
 	start_slot = 0;
 	end_slot = parent_nritems;
@@ -240,56 +236,60 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 
 	for (i = start_slot; i < end_slot; i++) {
 		int close = 1;
-		blocknr = btrfs_node_blockptr(parent_node, i);
+		blocknr = btrfs_node_blockptr(parent, i);
 		if (last_block == 0)
 			last_block = blocknr;
 		if (i > 0) {
-			other = btrfs_node_blockptr(parent_node, i - 1);
-			close = close_blocks(blocknr, other);
+			other = btrfs_node_blockptr(parent, i - 1);
+			close = close_blocks(blocknr, other, blocksize);
 		}
 		if (close && i < end_slot - 1) {
-			other = btrfs_node_blockptr(parent_node, i + 1);
-			close = close_blocks(blocknr, other);
+			other = btrfs_node_blockptr(parent, i + 1);
+			close = close_blocks(blocknr, other, blocksize);
 		}
 		if (close) {
 			last_block = blocknr;
 			continue;
 		}
 
-		cur_bh = btrfs_find_tree_block(root, blocknr);
-		if (!cur_bh || !buffer_uptodate(cur_bh) ||
-		    buffer_locked(cur_bh) ||
-		    (parent_level != 1 && !buffer_defrag(cur_bh)) ||
-		    (parent_level == 1 && !should_defrag_leaf(cur_bh))) {
+		cur = btrfs_find_tree_block(root, blocknr, blocksize);
+		if (cur)
+			uptodate = btrfs_buffer_uptodate(cur);
+		else
+			uptodate = 0;
+		if (!cur || !uptodate ||
+		    (parent_level != 1 && !btrfs_buffer_defrag(cur)) ||
+		    (parent_level == 1 && !should_defrag_leaf(cur))) {
 			if (cache_only) {
-				brelse(cur_bh);
+				free_extent_buffer(cur);
 				continue;
 			}
-			if (!cur_bh || !buffer_uptodate(cur_bh) ||
-			    buffer_locked(cur_bh)) {
-				brelse(cur_bh);
-				cur_bh = read_tree_block(root, blocknr);
+			if (!cur) {
+				cur = read_tree_block(root, blocknr,
+							 blocksize);
+			} else if (!uptodate) {
+				btrfs_read_buffer(cur);
 			}
 		}
 		if (search_start == 0)
-			search_start = last_block & ~((u64)65535);
+			search_start = last_block;
 
-		err = __btrfs_cow_block(trans, root, cur_bh, parent, i,
-					&tmp_bh, search_start,
-					min(8, end_slot - i));
+		err = __btrfs_cow_block(trans, root, cur, parent, i,
+					&tmp, search_start,
+					min(16 * blocksize,
+					    (end_slot - i) * blocksize));
 		if (err) {
-			brelse(cur_bh);
+			free_extent_buffer(cur);
 			break;
 		}
-		search_start = bh_blocknr(tmp_bh);
+		search_start = tmp->start;
 		*last_ret = search_start;
 		if (parent_level == 1)
-			clear_buffer_defrag(tmp_bh);
-		set_buffer_defrag_done(tmp_bh);
-		brelse(tmp_bh);
+			btrfs_clear_buffer_defrag(tmp);
+		btrfs_set_buffer_defrag_done(tmp);
+		free_extent_buffer(tmp);
 	}
 	return err;
-#endif
 }
 
 /*
@@ -892,22 +892,17 @@ static int push_nodes_for_insert(struct btrfs_trans_handle *trans,
 static void reada_for_search(struct btrfs_root *root, struct btrfs_path *path,
 			     int level, int slot)
 {
-	return;
-#if 0
 	struct extent_buffer *node;
-	int i;
 	u32 nritems;
-	u64 bytenr;
 	u64 search;
-	u64 cluster_start;
-	int ret;
-	int nread = 0;
+	u64 lowest_read;
+	u64 highest_read;
+	u64 nread = 0;
 	int direction = path->reada;
-	int level;
-	struct radix_tree_root found;
-	unsigned long gang[8];
 	struct extent_buffer *eb;
-
+	u32 nr;
+	u32 blocksize;
+	u32 nscan = 0;
 
 	if (level == 0)
 		return;
@@ -917,42 +912,46 @@ static void reada_for_search(struct btrfs_root *root, struct btrfs_path *path,
 
 	node = path->nodes[level];
 	search = btrfs_node_blockptr(node, slot);
-	eb = btrfs_find_tree_block(root, search);
+	blocksize = btrfs_level_size(root, level - 1);
+	eb = btrfs_find_tree_block(root, search, blocksize);
 	if (eb) {
 		free_extent_buffer(eb);
 		return;
 	}
 
-	init_bit_radix(&found);
+	highest_read = search;
+	lowest_read = search;
+
 	nritems = btrfs_header_nritems(node);
-	level = btrfs_header_level(node) - 1;
-	for (i = slot; i < nritems; i++) {
-		bytenr = btrfs_node_blockptr(node, i);
-		set_radix_bit(&found, blocknr);
-	}
-	if (direction > 0) {
-		cluster_start = search - 4;
-		if (cluster_start > search)
-			cluster_start = 0;
-	} else
-		cluster_start = search + 4;
+	nr = slot;
 	while(1) {
-		ret = find_first_radix_bit(&found, gang, 0, ARRAY_SIZE(gang));
-		if (!ret)
-			break;
-		for (i = 0; i < ret; i++) {
-			blocknr = gang[i];
-			clear_radix_bit(&found, blocknr);
-			if (path->reada == 1 && nread > 16)
-				continue;
-			if (close_blocks(cluster_start, blocknr)) {
-				readahead_tree_block(root, blocknr);
-				nread++;
-				cluster_start = blocknr;
-			}
+		if (direction < 0) {
+			if (nr == 0)
+				break;
+			nr--;
+		} else if (direction > 0) {
+			nr++;
+			if (nr >= nritems)
+				break;
 		}
+		search = btrfs_node_blockptr(node, nr);
+		if ((search >= lowest_read && search <= highest_read) ||
+		    (search < lowest_read && lowest_read - search <= 32768) ||
+		    (search > highest_read && search - highest_read <= 32768)) {
+			readahead_tree_block(root, search, blocksize);
+			nread += blocksize;
+		}
+		nscan++;
+		if (path->reada < 2 && (nread > (256 * 1024) || nscan > 32))
+			break;
+		if(nread > (1024 * 1024) || nscan > 128)
+			break;
+
+		if (search < lowest_read)
+			lowest_read = search;
+		if (search > highest_read)
+			highest_read = search;
 	}
-#endif
 }
 /*
  * look for key in the tree.  path is filled in with nodes along the way

commit db94535db75e67fab12ccbb7f5ee548e33fed891
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Oct 15 16:15:53 2007 -0400

    Btrfs: Allow tree blocks larger than the page size
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 50ef351ef6b3..34de83630ae9 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -83,16 +83,15 @@ static int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 
 	WARN_ON(root->ref_cows && trans->transid != root->last_trans);
 
-	cow = btrfs_alloc_free_block(trans, root, search_start, empty_size);
+	cow = btrfs_alloc_free_block(trans, root, buf->len,
+				     search_start, empty_size);
 	if (IS_ERR(cow))
 		return PTR_ERR(cow);
 
 	cow->alloc_addr = (unsigned long)__builtin_return_address(0);
-	if (buf->len != root->sectorsize || cow->len != root->sectorsize)
-		WARN_ON(1);
 
 	copy_extent_buffer(cow, buf, 0, 0, cow->len);
-	btrfs_set_header_blocknr(cow, extent_buffer_blocknr(cow));
+	btrfs_set_header_bytenr(cow, cow->start);
 	btrfs_set_header_generation(cow, trans->transid);
 	btrfs_set_header_owner(cow, root->root_key.objectid);
 
@@ -110,16 +109,16 @@ static int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 		root->node = cow;
 		extent_buffer_get(cow);
 		if (buf != root->commit_root) {
-			btrfs_free_extent(trans, root,
-					  extent_buffer_blocknr(buf), 1, 1);
+			btrfs_free_extent(trans, root, buf->start,
+					  buf->len, 1);
 		}
 		free_extent_buffer(buf);
 	} else {
 		btrfs_set_node_blockptr(parent, parent_slot,
-					extent_buffer_blocknr(cow));
+					cow->start);
 		btrfs_mark_buffer_dirty(parent);
 		WARN_ON(btrfs_header_generation(parent) != trans->transid);
-		btrfs_free_extent(trans, root, extent_buffer_blocknr(buf),1,1);
+		btrfs_free_extent(trans, root, buf->start, buf->len, 1);
 	}
 	free_extent_buffer(buf);
 	btrfs_mark_buffer_dirty(cow);
@@ -149,13 +148,14 @@ int btrfs_cow_block(struct btrfs_trans_handle *trans,
 		return 0;
 	}
 
-	search_start = extent_buffer_blocknr(buf) & ~((u64)65535);
+	search_start = buf->start & ~((u64)BTRFS_BLOCK_GROUP_SIZE - 1);
 	ret = __btrfs_cow_block(trans, root, buf, parent,
 				 parent_slot, cow_ret, search_start, 0);
 	(*cow_ret)->alloc_addr = (unsigned long)__builtin_return_address(0);
 	return ret;
 }
 
+#if 0
 static int close_blocks(u64 blocknr, u64 other)
 {
 	if (blocknr < other && other - blocknr < 8)
@@ -165,7 +165,6 @@ static int close_blocks(u64 blocknr, u64 other)
 	return 0;
 }
 
-#if 0
 static int should_defrag_leaf(struct extent_buffer *eb)
 {
 	return 0;
@@ -355,7 +354,7 @@ static int check_node(struct btrfs_root *root, struct btrfs_path *path,
 		BUG_ON(memcmp(&parent_key, &node_key,
 			      sizeof(struct btrfs_disk_key)));
 		BUG_ON(btrfs_node_blockptr(parent, parent_slot) !=
-		       btrfs_header_blocknr(node));
+		       btrfs_header_bytenr(node));
 	}
 	BUG_ON(nritems > BTRFS_NODEPTRS_PER_BLOCK(root));
 	if (slot != 0) {
@@ -398,7 +397,7 @@ static int check_leaf(struct btrfs_root *root, struct btrfs_path *path,
 		BUG_ON(memcmp(&parent_key, &leaf_key,
 		       sizeof(struct btrfs_disk_key)));
 		BUG_ON(btrfs_node_blockptr(parent, parent_slot) !=
-		       btrfs_header_blocknr(leaf));
+		       btrfs_header_bytenr(leaf));
 	}
 #if 0
 	for (i = 0; nritems > 1 && i < nritems - 2; i++) {
@@ -467,14 +466,16 @@ static int check_leaf(struct btrfs_root *root, struct btrfs_path *path,
 static int check_block(struct btrfs_root *root, struct btrfs_path *path,
 			int level)
 {
+#if 0
 	struct extent_buffer *buf = path->nodes[level];
 
 	if (memcmp_extent_buffer(buf, root->fs_info->fsid,
 				 (unsigned long)btrfs_header_fsid(buf),
 				 BTRFS_FSID_SIZE)) {
 		printk("warning bad block %Lu\n", buf->start);
-		BUG();
+		return 1;
 	}
+#endif
 	if (level == 0)
 		return check_leaf(root, path, level);
 	return check_node(root, path, level);
@@ -585,7 +586,8 @@ static struct extent_buffer *read_node_slot(struct btrfs_root *root,
 		return NULL;
 	if (slot >= btrfs_header_nritems(parent))
 		return NULL;
-	return read_tree_block(root, btrfs_node_blockptr(parent, slot));
+	return read_tree_block(root, btrfs_node_blockptr(parent, slot),
+		       btrfs_level_size(root, btrfs_header_level(parent) - 1));
 }
 
 static int balance_level(struct btrfs_trans_handle *trans, struct btrfs_root
@@ -618,7 +620,6 @@ static int balance_level(struct btrfs_trans_handle *trans, struct btrfs_root
 	 */
 	if (!parent) {
 		struct extent_buffer *child;
-		u64 blocknr = extent_buffer_blocknr(mid);
 
 		if (btrfs_header_nritems(mid) != 1)
 			return 0;
@@ -632,9 +633,10 @@ static int balance_level(struct btrfs_trans_handle *trans, struct btrfs_root
 		wait_on_tree_block_writeback(root, mid);
 		/* once for the path */
 		free_extent_buffer(mid);
+		ret = btrfs_free_extent(trans, root, mid->start, mid->len, 1);
 		/* once for the root ptr */
 		free_extent_buffer(mid);
-		return btrfs_free_extent(trans, root, blocknr, 1, 1);
+		return ret;
 	}
 	if (btrfs_header_nritems(mid) >
 	    BTRFS_NODEPTRS_PER_BLOCK(root) / 4)
@@ -680,7 +682,9 @@ static int balance_level(struct btrfs_trans_handle *trans, struct btrfs_root
 		if (wret < 0 && wret != -ENOSPC)
 			ret = wret;
 		if (btrfs_header_nritems(right) == 0) {
-			u64 blocknr = extent_buffer_blocknr(right);
+			u64 bytenr = right->start;
+			u32 blocksize = right->len;
+
 			clean_tree_block(trans, root, right);
 			wait_on_tree_block_writeback(root, right);
 			free_extent_buffer(right);
@@ -689,7 +693,8 @@ static int balance_level(struct btrfs_trans_handle *trans, struct btrfs_root
 				       1);
 			if (wret)
 				ret = wret;
-			wret = btrfs_free_extent(trans, root, blocknr, 1, 1);
+			wret = btrfs_free_extent(trans, root, bytenr,
+						 blocksize, 1);
 			if (wret)
 				ret = wret;
 		} else {
@@ -719,7 +724,8 @@ static int balance_level(struct btrfs_trans_handle *trans, struct btrfs_root
 	}
 	if (btrfs_header_nritems(mid) == 0) {
 		/* we've managed to empty the middle node, drop it */
-		u64 blocknr = extent_buffer_blocknr(mid);
+		u64 bytenr = mid->start;
+		u32 blocksize = mid->len;
 		clean_tree_block(trans, root, mid);
 		wait_on_tree_block_writeback(root, mid);
 		free_extent_buffer(mid);
@@ -727,7 +733,7 @@ static int balance_level(struct btrfs_trans_handle *trans, struct btrfs_root
 		wret = del_ptr(trans, root, path, level + 1, pslot);
 		if (wret)
 			ret = wret;
-		wret = btrfs_free_extent(trans, root, blocknr, 1, 1);
+		wret = btrfs_free_extent(trans, root, bytenr, blocksize, 1);
 		if (wret)
 			ret = wret;
 	} else {
@@ -830,7 +836,6 @@ static int push_nodes_for_insert(struct btrfs_trans_handle *trans,
 				path->slots[level] = orig_slot;
 				free_extent_buffer(left);
 			}
-			check_node(root, path, level);
 			return 0;
 		}
 		free_extent_buffer(left);
@@ -874,12 +879,10 @@ static int push_nodes_for_insert(struct btrfs_trans_handle *trans,
 			} else {
 				free_extent_buffer(right);
 			}
-			check_node(root, path, level);
 			return 0;
 		}
 		free_extent_buffer(right);
 	}
-	check_node(root, path, level);
 	return 1;
 }
 
@@ -889,19 +892,23 @@ static int push_nodes_for_insert(struct btrfs_trans_handle *trans,
 static void reada_for_search(struct btrfs_root *root, struct btrfs_path *path,
 			     int level, int slot)
 {
+	return;
+#if 0
 	struct extent_buffer *node;
 	int i;
 	u32 nritems;
-	u64 blocknr;
+	u64 bytenr;
 	u64 search;
 	u64 cluster_start;
 	int ret;
 	int nread = 0;
 	int direction = path->reada;
+	int level;
 	struct radix_tree_root found;
 	unsigned long gang[8];
 	struct extent_buffer *eb;
 
+
 	if (level == 0)
 		return;
 
@@ -918,8 +925,9 @@ static void reada_for_search(struct btrfs_root *root, struct btrfs_path *path,
 
 	init_bit_radix(&found);
 	nritems = btrfs_header_nritems(node);
+	level = btrfs_header_level(node) - 1;
 	for (i = slot; i < nritems; i++) {
-		blocknr = btrfs_node_blockptr(node, i);
+		bytenr = btrfs_node_blockptr(node, i);
 		set_radix_bit(&found, blocknr);
 	}
 	if (direction > 0) {
@@ -944,6 +952,7 @@ static void reada_for_search(struct btrfs_root *root, struct btrfs_path *path,
 			}
 		}
 	}
+#endif
 }
 /*
  * look for key in the tree.  path is filled in with nodes along the way
@@ -963,7 +972,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 		      ins_len, int cow)
 {
 	struct extent_buffer *b;
-	u64 blocknr;
+	u64 bytenr;
 	int slot;
 	int ret;
 	int level;
@@ -1027,10 +1036,11 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 			/* this is only true while dropping a snapshot */
 			if (level == lowest_level)
 				break;
-			blocknr = btrfs_node_blockptr(b, slot);
+			bytenr = btrfs_node_blockptr(b, slot);
 			if (should_reada)
 				reada_for_search(root, p, level, slot);
-			b = read_tree_block(root, btrfs_node_blockptr(b, slot));
+			b = read_tree_block(root, bytenr,
+					    btrfs_level_size(root, level - 1));
 		} else {
 			p->slots[level] = slot;
 			if (ins_len > 0 && btrfs_leaf_free_space(root, b) <
@@ -1193,14 +1203,14 @@ static int insert_new_root(struct btrfs_trans_handle *trans,
 	BUG_ON(path->nodes[level]);
 	BUG_ON(path->nodes[level-1] != root->node);
 
-	c = btrfs_alloc_free_block(trans, root,
-				   extent_buffer_blocknr(root->node), 0);
+	c = btrfs_alloc_free_block(trans, root, root->nodesize,
+				   root->node->start, 0);
 	if (IS_ERR(c))
 		return PTR_ERR(c);
 	memset_extent_buffer(c, 0, 0, root->nodesize);
 	btrfs_set_header_nritems(c, 1);
 	btrfs_set_header_level(c, level);
-	btrfs_set_header_blocknr(c, extent_buffer_blocknr(c));
+	btrfs_set_header_bytenr(c, c->start);
 	btrfs_set_header_generation(c, trans->transid);
 	btrfs_set_header_owner(c, root->root_key.objectid);
 	lower = path->nodes[level-1];
@@ -1213,7 +1223,7 @@ static int insert_new_root(struct btrfs_trans_handle *trans,
 	else
 		btrfs_node_key(lower, &lower_key, 0);
 	btrfs_set_node_key(c, &lower_key, 0);
-	btrfs_set_node_blockptr(c, 0, extent_buffer_blocknr(lower));
+	btrfs_set_node_blockptr(c, 0, lower->start);
 
 	btrfs_mark_buffer_dirty(c);
 
@@ -1237,7 +1247,7 @@ static int insert_new_root(struct btrfs_trans_handle *trans,
  */
 static int insert_ptr(struct btrfs_trans_handle *trans, struct btrfs_root
 		      *root, struct btrfs_path *path, struct btrfs_disk_key
-		      *key, u64 blocknr, int slot, int level)
+		      *key, u64 bytenr, int slot, int level)
 {
 	struct extent_buffer *lower;
 	int nritems;
@@ -1256,10 +1266,9 @@ static int insert_ptr(struct btrfs_trans_handle *trans, struct btrfs_root
 			      (nritems - slot) * sizeof(struct btrfs_key_ptr));
 	}
 	btrfs_set_node_key(lower, key, slot);
-	btrfs_set_node_blockptr(lower, slot, blocknr);
+	btrfs_set_node_blockptr(lower, slot, bytenr);
 	btrfs_set_header_nritems(lower, nritems + 1);
 	btrfs_mark_buffer_dirty(lower);
-	check_node(root, path, level);
 	return 0;
 }
 
@@ -1300,14 +1309,14 @@ static int split_node(struct btrfs_trans_handle *trans, struct btrfs_root
 	}
 
 	c_nritems = btrfs_header_nritems(c);
-	split = btrfs_alloc_free_block(trans, root,
-				       extent_buffer_blocknr(c), 0);
+	split = btrfs_alloc_free_block(trans, root, root->nodesize,
+				       c->start, 0);
 	if (IS_ERR(split))
 		return PTR_ERR(split);
 
 	btrfs_set_header_flags(split, btrfs_header_flags(c));
 	btrfs_set_header_level(split, btrfs_header_level(c));
-	btrfs_set_header_blocknr(split, extent_buffer_blocknr(split));
+	btrfs_set_header_bytenr(split, split->start);
 	btrfs_set_header_generation(split, trans->transid);
 	btrfs_set_header_owner(split, root->root_key.objectid);
 	write_extent_buffer(split, root->fs_info->fsid,
@@ -1328,8 +1337,7 @@ static int split_node(struct btrfs_trans_handle *trans, struct btrfs_root
 	btrfs_mark_buffer_dirty(split);
 
 	btrfs_node_key(split, &disk_key, 0);
-	wret = insert_ptr(trans, root, path, &disk_key,
-			  extent_buffer_blocknr(split),
+	wret = insert_ptr(trans, root, path, &disk_key, split->start,
 			  path->slots[level + 1] + 1,
 			  level + 1);
 	if (wret)
@@ -1407,6 +1415,7 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 	u32 left_nritems;
 	u32 right_nritems;
 	u32 data_end;
+	u32 this_item_size;
 	int ret;
 
 	slot = path->slots[1];
@@ -1417,7 +1426,8 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 	if (slot >= btrfs_header_nritems(upper) - 1)
 		return 1;
 
-	right = read_tree_block(root, btrfs_node_blockptr(upper, slot + 1));
+	right = read_tree_block(root, btrfs_node_blockptr(upper, slot + 1),
+				root->leafsize);
 	free_space = btrfs_leaf_free_space(root, right);
 	if (free_space < data_size + sizeof(struct btrfs_item)) {
 		free_extent_buffer(right);
@@ -1445,13 +1455,27 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 
 	for (i = left_nritems - 1; i >= 1; i--) {
 		item = btrfs_item_nr(left, i);
+
 		if (path->slots[0] == i)
 			push_space += data_size + sizeof(*item);
-		if (btrfs_item_size(left, item) + sizeof(*item) + push_space >
-		    free_space)
+
+		if (!left->map_token) {
+			map_extent_buffer(left, (unsigned long)item,
+					sizeof(struct btrfs_item),
+					&left->map_token, &left->kaddr,
+					&left->map_start, &left->map_len,
+					KM_USER1);
+		}
+
+		this_item_size = btrfs_item_size(left, item);
+		if (this_item_size + sizeof(*item) + push_space > free_space)
 			break;
 		push_items++;
-		push_space += btrfs_item_size(left, item) + sizeof(*item);
+		push_space += this_item_size + sizeof(*item);
+	}
+	if (left->map_token) {
+		unmap_extent_buffer(left, left->map_token, KM_USER1);
+		left->map_token = NULL;
 	}
 
 	if (push_items == 0) {
@@ -1493,11 +1517,23 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 	right_nritems += push_items;
 	btrfs_set_header_nritems(right, right_nritems);
 	push_space = BTRFS_LEAF_DATA_SIZE(root);
+
 	for (i = 0; i < right_nritems; i++) {
 		item = btrfs_item_nr(right, i);
-		btrfs_set_item_offset(right, item, push_space -
-				      btrfs_item_size(right, item));
-		push_space = btrfs_item_offset(right, item);
+		if (!right->map_token) {
+			map_extent_buffer(right, (unsigned long)item,
+					sizeof(struct btrfs_item),
+					&right->map_token, &right->kaddr,
+					&right->map_start, &right->map_len,
+					KM_USER1);
+		}
+		push_space -= btrfs_item_size(right, item);
+		btrfs_set_item_offset(right, item, push_space);
+	}
+
+	if (right->map_token) {
+		unmap_extent_buffer(right, right->map_token, KM_USER1);
+		right->map_token = NULL;
 	}
 	left_nritems -= push_items;
 	btrfs_set_header_nritems(left, left_nritems);
@@ -1518,8 +1554,6 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 	} else {
 		free_extent_buffer(right);
 	}
-	if (path->nodes[1])
-		check_node(root, path, 1);
 	return 0;
 }
 /*
@@ -1542,6 +1576,8 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 	u32 right_nritems;
 	int ret = 0;
 	int wret;
+	u32 this_item_size;
+	u32 old_left_item_size;
 
 	slot = path->slots[1];
 	if (slot == 0)
@@ -1550,7 +1586,7 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 		return 1;
 
 	left = read_tree_block(root, btrfs_node_blockptr(path->nodes[1],
-							 slot - 1));
+			       slot - 1), root->leafsize);
 	free_space = btrfs_leaf_free_space(root, left);
 	if (free_space < data_size + sizeof(struct btrfs_item)) {
 		free_extent_buffer(left);
@@ -1579,14 +1615,30 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 
 	for (i = 0; i < right_nritems - 1; i++) {
 		item = btrfs_item_nr(right, i);
+		if (!right->map_token) {
+			map_extent_buffer(right, (unsigned long)item,
+					sizeof(struct btrfs_item),
+					&right->map_token, &right->kaddr,
+					&right->map_start, &right->map_len,
+					KM_USER1);
+		}
+
 		if (path->slots[0] == i)
 			push_space += data_size + sizeof(*item);
-		if (btrfs_item_size(right, item) + sizeof(*item) + push_space >
-		    free_space)
+
+		this_item_size = btrfs_item_size(right, item);
+		if (this_item_size + sizeof(*item) + push_space > free_space)
 			break;
+
 		push_items++;
-		push_space += btrfs_item_size(right, item) + sizeof(*item);
+		push_space += this_item_size + sizeof(*item);
+	}
+
+	if (right->map_token) {
+		unmap_extent_buffer(right, right->map_token, KM_USER1);
+		right->map_token = NULL;
 	}
+
 	if (push_items == 0) {
 		free_extent_buffer(left);
 		return 1;
@@ -1611,15 +1663,28 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 	old_left_nritems = btrfs_header_nritems(left);
 	BUG_ON(old_left_nritems < 0);
 
+	old_left_item_size = btrfs_item_offset_nr(left, old_left_nritems - 1);
 	for (i = old_left_nritems; i < old_left_nritems + push_items; i++) {
 		u32 ioff;
+
 		item = btrfs_item_nr(left, i);
+		if (!left->map_token) {
+			map_extent_buffer(left, (unsigned long)item,
+					sizeof(struct btrfs_item),
+					&left->map_token, &left->kaddr,
+					&left->map_start, &left->map_len,
+					KM_USER1);
+		}
+
 		ioff = btrfs_item_offset(left, item);
 		btrfs_set_item_offset(left, item,
-		      ioff - (BTRFS_LEAF_DATA_SIZE(root) -
-		      btrfs_item_offset_nr(left, old_left_nritems - 1)));
+		      ioff - (BTRFS_LEAF_DATA_SIZE(root) - old_left_item_size));
 	}
 	btrfs_set_header_nritems(left, old_left_nritems + push_items);
+	if (left->map_token) {
+		unmap_extent_buffer(left, left->map_token, KM_USER1);
+		left->map_token = NULL;
+	}
 
 	/* fixup right node */
 	push_space = btrfs_item_offset_nr(right, push_items - 1) -
@@ -1640,9 +1705,21 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 
 	for (i = 0; i < right_nritems; i++) {
 		item = btrfs_item_nr(right, i);
-		btrfs_set_item_offset(right, item, push_space -
-				      btrfs_item_size(right, item));
-		push_space = btrfs_item_offset(right, item);
+
+		if (!right->map_token) {
+			map_extent_buffer(right, (unsigned long)item,
+					sizeof(struct btrfs_item),
+					&right->map_token, &right->kaddr,
+					&right->map_start, &right->map_len,
+					KM_USER1);
+		}
+
+		push_space = push_space - btrfs_item_size(right, item);
+		btrfs_set_item_offset(right, item, push_space);
+	}
+	if (right->map_token) {
+		unmap_extent_buffer(right, right->map_token, KM_USER1);
+		right->map_token = NULL;
 	}
 
 	btrfs_mark_buffer_dirty(left);
@@ -1664,8 +1741,6 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 		path->slots[0] -= push_items;
 	}
 	BUG_ON(path->slots[0] < 0);
-	if (path->nodes[1])
-		check_node(root, path, 1);
 	return ret;
 }
 
@@ -1718,13 +1793,13 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 	nritems = btrfs_header_nritems(l);
 	mid = (nritems + 1)/ 2;
 
-	right = btrfs_alloc_free_block(trans, root,
-					      extent_buffer_blocknr(l), 0);
+	right = btrfs_alloc_free_block(trans, root, root->leafsize,
+				       l->start, 0);
 	if (IS_ERR(right))
 		return PTR_ERR(right);
 
 	memset_extent_buffer(right, 0, 0, sizeof(struct btrfs_header));
-	btrfs_set_header_blocknr(right, extent_buffer_blocknr(right));
+	btrfs_set_header_bytenr(right, right->start);
 	btrfs_set_header_generation(right, trans->transid);
 	btrfs_set_header_owner(right, root->root_key.objectid);
 	btrfs_set_header_level(right, 0);
@@ -1740,8 +1815,7 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 				btrfs_cpu_key_to_disk(&disk_key, ins_key);
 				btrfs_set_header_nritems(right, 0);
 				wret = insert_ptr(trans, root, path,
-						  &disk_key,
-						  extent_buffer_blocknr(right),
+						  &disk_key, right->start,
 						  path->slots[1] + 1, 1);
 				if (wret)
 					ret = wret;
@@ -1762,7 +1836,7 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 				btrfs_set_header_nritems(right, 0);
 				wret = insert_ptr(trans, root, path,
 						  &disk_key,
-						  extent_buffer_blocknr(right),
+						  right->start,
 						  path->slots[1], 1);
 				if (wret)
 					ret = wret;
@@ -1799,15 +1873,30 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 
 	for (i = 0; i < nritems; i++) {
 		struct btrfs_item *item = btrfs_item_nr(right, i);
-		u32 ioff = btrfs_item_offset(right, item);
+		u32 ioff;
+
+		if (!right->map_token) {
+			map_extent_buffer(right, (unsigned long)item,
+					sizeof(struct btrfs_item),
+					&right->map_token, &right->kaddr,
+					&right->map_start, &right->map_len,
+					KM_USER1);
+		}
+
+		ioff = btrfs_item_offset(right, item);
 		btrfs_set_item_offset(right, item, ioff + rt_data_off);
 	}
 
+	if (right->map_token) {
+		unmap_extent_buffer(right, right->map_token, KM_USER1);
+		right->map_token = NULL;
+	}
+
 	btrfs_set_header_nritems(l, mid);
 	ret = 0;
 	btrfs_item_key(right, &disk_key, 0);
-	wret = insert_ptr(trans, root, path, &disk_key,
-			  extent_buffer_blocknr(right), path->slots[1] + 1, 1);
+	wret = insert_ptr(trans, root, path, &disk_key, right->start,
+			  path->slots[1] + 1, 1);
 	if (wret)
 		ret = wret;
 
@@ -1824,19 +1913,17 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 		free_extent_buffer(right);
 
 	BUG_ON(path->slots[0] < 0);
-	check_node(root, path, 1);
-	check_leaf(root, path, 0);
 
 	if (!double_split)
 		return ret;
 
-	right = btrfs_alloc_free_block(trans, root,
-				       extent_buffer_blocknr(l), 0);
+	right = btrfs_alloc_free_block(trans, root, root->leafsize,
+				       l->start, 0);
 	if (IS_ERR(right))
 		return PTR_ERR(right);
 
 	memset_extent_buffer(right, 0, 0, sizeof(struct btrfs_header));
-	btrfs_set_header_blocknr(right, extent_buffer_blocknr(right));
+	btrfs_set_header_bytenr(right, right->start);
 	btrfs_set_header_generation(right, trans->transid);
 	btrfs_set_header_owner(right, root->root_key.objectid);
 	btrfs_set_header_level(right, 0);
@@ -1847,8 +1934,7 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 	btrfs_cpu_key_to_disk(&disk_key, ins_key);
 	btrfs_set_header_nritems(right, 0);
 	wret = insert_ptr(trans, root, path,
-			  &disk_key,
-			  extent_buffer_blocknr(right),
+			  &disk_key, right->start,
 			  path->slots[1], 1);
 	if (wret)
 		ret = wret;
@@ -1860,8 +1946,6 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 	free_extent_buffer(path->nodes[0]);
 	path->nodes[0] = right;
 	path->slots[0] = 0;
-	check_node(root, path, 1);
-	check_leaf(root, path, 0);
 	return ret;
 }
 
@@ -1904,9 +1988,24 @@ int btrfs_truncate_item(struct btrfs_trans_handle *trans,
 	for (i = slot; i < nritems; i++) {
 		u32 ioff;
 		item = btrfs_item_nr(leaf, i);
+
+		if (!leaf->map_token) {
+			map_extent_buffer(leaf, (unsigned long)item,
+					sizeof(struct btrfs_item),
+					&leaf->map_token, &leaf->kaddr,
+					&leaf->map_start, &leaf->map_len,
+					KM_USER1);
+		}
+
 		ioff = btrfs_item_offset(leaf, item);
 		btrfs_set_item_offset(leaf, item, ioff + size_diff);
 	}
+
+	if (leaf->map_token) {
+		unmap_extent_buffer(leaf, leaf->map_token, KM_USER1);
+		leaf->map_token = NULL;
+	}
+
 	/* shift the data */
 	memmove_extent_buffer(leaf, btrfs_leaf_data(leaf) +
 		      data_end + size_diff, btrfs_leaf_data(leaf) +
@@ -1921,7 +2020,6 @@ int btrfs_truncate_item(struct btrfs_trans_handle *trans,
 		btrfs_print_leaf(root, leaf);
 		BUG();
 	}
-	check_leaf(root, path, 0);
 	return ret;
 }
 
@@ -1963,10 +2061,23 @@ int btrfs_extend_item(struct btrfs_trans_handle *trans,
 	for (i = slot; i < nritems; i++) {
 		u32 ioff;
 		item = btrfs_item_nr(leaf, i);
+
+		if (!leaf->map_token) {
+			map_extent_buffer(leaf, (unsigned long)item,
+					sizeof(struct btrfs_item),
+					&leaf->map_token, &leaf->kaddr,
+					&leaf->map_start, &leaf->map_len,
+					KM_USER1);
+		}
 		ioff = btrfs_item_offset(leaf, item);
 		btrfs_set_item_offset(leaf, item, ioff - data_size);
 	}
 
+	if (leaf->map_token) {
+		unmap_extent_buffer(leaf, leaf->map_token, KM_USER1);
+		leaf->map_token = NULL;
+	}
+
 	/* shift the data */
 	memmove_extent_buffer(leaf, btrfs_leaf_data(leaf) +
 		      data_end - data_size, btrfs_leaf_data(leaf) +
@@ -1983,7 +2094,6 @@ int btrfs_extend_item(struct btrfs_trans_handle *trans,
 		btrfs_print_leaf(root, leaf);
 		BUG();
 	}
-	check_leaf(root, path, 0);
 	return ret;
 }
 
@@ -2046,12 +2156,26 @@ int btrfs_insert_empty_item(struct btrfs_trans_handle *trans,
 		 * item0..itemN ... dataN.offset..dataN.size .. data0.size
 		 */
 		/* first correct the data pointers */
+		WARN_ON(leaf->map_token);
 		for (i = slot; i < nritems; i++) {
 			u32 ioff;
+
 			item = btrfs_item_nr(leaf, i);
+			if (!leaf->map_token) {
+				map_extent_buffer(leaf, (unsigned long)item,
+					sizeof(struct btrfs_item),
+					&leaf->map_token, &leaf->kaddr,
+					&leaf->map_start, &leaf->map_len,
+					KM_USER1);
+			}
+
 			ioff = btrfs_item_offset(leaf, item);
 			btrfs_set_item_offset(leaf, item, ioff - data_size);
 		}
+		if (leaf->map_token) {
+			unmap_extent_buffer(leaf, leaf->map_token, KM_USER1);
+			leaf->map_token = NULL;
+		}
 
 		/* shift the items */
 		memmove_extent_buffer(leaf, btrfs_item_nr_offset(slot + 1),
@@ -2081,7 +2205,6 @@ int btrfs_insert_empty_item(struct btrfs_trans_handle *trans,
 		btrfs_print_leaf(root, leaf);
 		BUG();
 	}
-	check_leaf(root, path, 0);
 out:
 	return ret;
 }
@@ -2186,10 +2309,24 @@ int btrfs_del_item(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 
 		for (i = slot + 1; i < nritems; i++) {
 			u32 ioff;
+
 			item = btrfs_item_nr(leaf, i);
+			if (!leaf->map_token) {
+				map_extent_buffer(leaf, (unsigned long)item,
+					sizeof(struct btrfs_item),
+					&leaf->map_token, &leaf->kaddr,
+					&leaf->map_start, &leaf->map_len,
+					KM_USER1);
+			}
 			ioff = btrfs_item_offset(leaf, item);
 			btrfs_set_item_offset(leaf, item, ioff + dsize);
 		}
+
+		if (leaf->map_token) {
+			unmap_extent_buffer(leaf, leaf->map_token, KM_USER1);
+			leaf->map_token = NULL;
+		}
+
 		memmove_extent_buffer(leaf, btrfs_item_nr_offset(slot),
 			      btrfs_item_nr_offset(slot + 1),
 			      sizeof(struct btrfs_item) *
@@ -2209,8 +2346,7 @@ int btrfs_del_item(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 			if (wret)
 				ret = wret;
 			wret = btrfs_free_extent(trans, root,
-						 extent_buffer_blocknr(leaf),
-						 1, 1);
+						 leaf->start, leaf->len, 1);
 			if (wret)
 				ret = wret;
 		}
@@ -2247,7 +2383,8 @@ int btrfs_del_item(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 			}
 
 			if (btrfs_header_nritems(leaf) == 0) {
-				u64 blocknr = extent_buffer_blocknr(leaf);
+				u64 bytenr = leaf->start;
+				u32 blocksize = leaf->len;
 
 				clean_tree_block(trans, root, leaf);
 				wait_on_tree_block_writeback(root, leaf);
@@ -2257,8 +2394,8 @@ int btrfs_del_item(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 					ret = wret;
 
 				free_extent_buffer(leaf);
-				wret = btrfs_free_extent(trans, root, blocknr,
-							 1, 1);
+				wret = btrfs_free_extent(trans, root, bytenr,
+							 blocksize, 1);
 				if (wret)
 					ret = wret;
 			} else {
@@ -2281,7 +2418,7 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 {
 	int slot;
 	int level = 1;
-	u64 blocknr;
+	u64 bytenr;
 	struct extent_buffer *c;
 	struct extent_buffer *next = NULL;
 
@@ -2296,14 +2433,15 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 			continue;
 		}
 
-		blocknr = btrfs_node_blockptr(c, slot);
+		bytenr = btrfs_node_blockptr(c, slot);
 		if (next)
 			free_extent_buffer(next);
 
 		if (path->reada)
 			reada_for_search(root, path, level, slot);
 
-		next = read_tree_block(root, blocknr);
+		next = read_tree_block(root, bytenr,
+				       btrfs_level_size(root, level -1));
 		break;
 	}
 	path->slots[level] = slot;
@@ -2317,7 +2455,8 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 			break;
 		if (path->reada)
 			reada_for_search(root, path, level, 0);
-		next = read_tree_block(root, btrfs_node_blockptr(next, 0));
+		next = read_tree_block(root, btrfs_node_blockptr(next, 0),
+				       btrfs_level_size(root, level - 1));
 	}
 	return 0;
 }

commit f510cfecfc98759d75283823cfccf0cc0d59a4c6
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Oct 15 16:14:48 2007 -0400

    Btrfs: Fix extent_buffer and extent_state leaks
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 9427b79c5d79..50ef351ef6b3 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -87,6 +87,7 @@ static int __btrfs_cow_block(struct btrfs_trans_handle *trans,
 	if (IS_ERR(cow))
 		return PTR_ERR(cow);
 
+	cow->alloc_addr = (unsigned long)__builtin_return_address(0);
 	if (buf->len != root->sectorsize || cow->len != root->sectorsize)
 		WARN_ON(1);
 
@@ -132,6 +133,7 @@ int btrfs_cow_block(struct btrfs_trans_handle *trans,
 		    struct extent_buffer **cow_ret)
 {
 	u64 search_start;
+	int ret;
 	if (trans->transaction != root->fs_info->running_transaction) {
 		printk(KERN_CRIT "trans %Lu running %Lu\n", trans->transid,
 		       root->fs_info->running_transaction->transid);
@@ -148,8 +150,10 @@ int btrfs_cow_block(struct btrfs_trans_handle *trans,
 	}
 
 	search_start = extent_buffer_blocknr(buf) & ~((u64)65535);
-	return __btrfs_cow_block(trans, root, buf, parent,
+	ret = __btrfs_cow_block(trans, root, buf, parent,
 				 parent_slot, cow_ret, search_start, 0);
+	(*cow_ret)->alloc_addr = (unsigned long)__builtin_return_address(0);
+	return ret;
 }
 
 static int close_blocks(u64 blocknr, u64 other)
@@ -1013,8 +1017,10 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 				if (sret)
 					return sret;
 				b = p->nodes[level];
-				if (!b)
+				if (!b) {
+					btrfs_release_path(NULL, p);
 					goto again;
+				}
 				slot = p->slots[level];
 				BUG_ON(btrfs_header_nritems(b) == 1);
 			}

commit 479965d66e320f1a095bb76027171daa675a9c72
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Oct 15 16:14:27 2007 -0400

    Btrfs: Optimizations for the extent_buffer code
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index f60920e8a0e0..9427b79c5d79 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -464,22 +464,12 @@ static int check_block(struct btrfs_root *root, struct btrfs_path *path,
 			int level)
 {
 	struct extent_buffer *buf = path->nodes[level];
-	char fsid[BTRFS_FSID_SIZE];
 
-	read_extent_buffer(buf, fsid, (unsigned long)btrfs_header_fsid(buf),
-			   BTRFS_FSID_SIZE);
-
-	if (memcmp(fsid, root->fs_info->fsid, BTRFS_FSID_SIZE)) {
-		int i = 0;
+	if (memcmp_extent_buffer(buf, root->fs_info->fsid,
+				 (unsigned long)btrfs_header_fsid(buf),
+				 BTRFS_FSID_SIZE)) {
 		printk("warning bad block %Lu\n", buf->start);
-		if (!btrfs_buffer_uptodate(buf)) {
-			WARN_ON(1);
-		}
-		for (i = 0; i < BTRFS_FSID_SIZE; i++) {
-			printk("%x:%x ", root->fs_info->fsid[i], fsid[i]);
-		}
-		printk("\n");
-		// BUG();
+		BUG();
 	}
 	if (level == 0)
 		return check_leaf(root, path, level);
@@ -504,13 +494,14 @@ static int generic_bin_search(struct extent_buffer *eb, unsigned long p,
 	int high = max;
 	int mid;
 	int ret;
-	struct btrfs_disk_key *tmp;
+	struct btrfs_disk_key *tmp = NULL;
 	struct btrfs_disk_key unaligned;
 	unsigned long offset;
 	char *map_token = NULL;
 	char *kaddr = NULL;
 	unsigned long map_start = 0;
 	unsigned long map_len = 0;
+	int err;
 
 	while(low < high) {
 		mid = (low + high) / 2;
@@ -519,19 +510,24 @@ static int generic_bin_search(struct extent_buffer *eb, unsigned long p,
 		if (!map_token || offset < map_start ||
 		    (offset + sizeof(struct btrfs_disk_key)) >
 		    map_start + map_len) {
-			if (map_token)
+			if (map_token) {
 				unmap_extent_buffer(eb, map_token, KM_USER0);
-			map_extent_buffer(eb, offset, &map_token, &kaddr,
-					  &map_start, &map_len, KM_USER0);
+				map_token = NULL;
+			}
+			err = map_extent_buffer(eb, offset,
+						sizeof(struct btrfs_disk_key),
+						&map_token, &kaddr,
+						&map_start, &map_len, KM_USER0);
+
+			if (!err) {
+				tmp = (struct btrfs_disk_key *)(kaddr + offset -
+							map_start);
+			} else {
+				read_extent_buffer(eb, &unaligned,
+						   offset, sizeof(unaligned));
+				tmp = &unaligned;
+			}
 
-		}
-		if (offset + sizeof(struct btrfs_disk_key) >
-		    map_start + map_len) {
-			unmap_extent_buffer(eb, map_token, KM_USER0);
-			read_extent_buffer(eb, &unaligned,
-					   offset, sizeof(unaligned));
-			map_token = NULL;
-			tmp = &unaligned;
 		} else {
 			tmp = (struct btrfs_disk_key *)(kaddr + offset -
 							map_start);
@@ -544,7 +540,8 @@ static int generic_bin_search(struct extent_buffer *eb, unsigned long p,
 			high = mid;
 		else {
 			*slot = mid;
-			unmap_extent_buffer(eb, map_token, KM_USER0);
+			if (map_token)
+				unmap_extent_buffer(eb, map_token, KM_USER0);
 			return 0;
 		}
 	}

commit 5f39d397dfbe140a14edecd4e73c34ce23c4f9ee
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Oct 15 16:14:19 2007 -0400

    Btrfs: Create extent_buffer interface for large blocksizes
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index b41f48ade419..f60920e8a0e0 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -16,21 +16,24 @@
  * Boston, MA 021110-1307, USA.
  */
 
+#include <linux/highmem.h>
 #include "ctree.h"
 #include "disk-io.h"
 #include "transaction.h"
+#include "print-tree.h"
 
 static int split_node(struct btrfs_trans_handle *trans, struct btrfs_root
 		      *root, struct btrfs_path *path, int level);
 static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 		      *root, struct btrfs_key *ins_key,
 		      struct btrfs_path *path, int data_size);
-static int push_node_left(struct btrfs_trans_handle *trans, struct btrfs_root
-			  *root, struct buffer_head *dst, struct buffer_head
-			  *src);
-static int balance_node_right(struct btrfs_trans_handle *trans, struct
-			      btrfs_root *root, struct buffer_head *dst_buf,
-			      struct buffer_head *src_buf);
+static int push_node_left(struct btrfs_trans_handle *trans,
+			  struct btrfs_root *root, struct extent_buffer *dst,
+			  struct extent_buffer *src);
+static int balance_node_right(struct btrfs_trans_handle *trans,
+			      struct btrfs_root *root,
+			      struct extent_buffer *dst_buf,
+			      struct extent_buffer *src_buf);
 static int del_ptr(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 		   struct btrfs_path *path, int level, int slot);
 
@@ -62,40 +65,38 @@ void btrfs_release_path(struct btrfs_root *root, struct btrfs_path *p)
 	for (i = 0; i < BTRFS_MAX_LEVEL; i++) {
 		if (!p->nodes[i])
 			break;
-		btrfs_block_release(root, p->nodes[i]);
+		free_extent_buffer(p->nodes[i]);
 	}
 	memset(p, 0, sizeof(*p));
 }
 
-static int __btrfs_cow_block(struct btrfs_trans_handle *trans, struct btrfs_root
-			   *root, struct buffer_head *buf, struct buffer_head
-			   *parent, int parent_slot, struct buffer_head
-			   **cow_ret, u64 search_start, u64 empty_size)
+static int __btrfs_cow_block(struct btrfs_trans_handle *trans,
+			     struct btrfs_root *root,
+			     struct extent_buffer *buf,
+			     struct extent_buffer *parent, int parent_slot,
+			     struct extent_buffer **cow_ret,
+			     u64 search_start, u64 empty_size)
 {
-	struct buffer_head *cow;
-	struct btrfs_node *cow_node;
+	struct extent_buffer *cow;
 	int ret = 0;
 	int different_trans = 0;
 
 	WARN_ON(root->ref_cows && trans->transid != root->last_trans);
-	WARN_ON(!buffer_uptodate(buf));
+
 	cow = btrfs_alloc_free_block(trans, root, search_start, empty_size);
 	if (IS_ERR(cow))
 		return PTR_ERR(cow);
 
-	cow_node = btrfs_buffer_node(cow);
-	if (buf->b_size != root->blocksize || cow->b_size != root->blocksize)
+	if (buf->len != root->sectorsize || cow->len != root->sectorsize)
 		WARN_ON(1);
 
-	memcpy(cow_node, btrfs_buffer_node(buf), root->blocksize);
-	btrfs_set_header_blocknr(&cow_node->header, bh_blocknr(cow));
-	btrfs_set_header_generation(&cow_node->header, trans->transid);
-	btrfs_set_header_owner(&cow_node->header, root->root_key.objectid);
+	copy_extent_buffer(cow, buf, 0, 0, cow->len);
+	btrfs_set_header_blocknr(cow, extent_buffer_blocknr(cow));
+	btrfs_set_header_generation(cow, trans->transid);
+	btrfs_set_header_owner(cow, root->root_key.objectid);
 
-	WARN_ON(btrfs_header_generation(btrfs_buffer_header(buf)) >
-		trans->transid);
-	if (btrfs_header_generation(btrfs_buffer_header(buf)) !=
-				    trans->transid) {
+	WARN_ON(btrfs_header_generation(buf) > trans->transid);
+	if (btrfs_header_generation(buf) != trans->transid) {
 		different_trans = 1;
 		ret = btrfs_inc_ref(trans, root, buf);
 		if (ret)
@@ -106,29 +107,29 @@ static int __btrfs_cow_block(struct btrfs_trans_handle *trans, struct btrfs_root
 
 	if (buf == root->node) {
 		root->node = cow;
-		get_bh(cow);
+		extent_buffer_get(cow);
 		if (buf != root->commit_root) {
-			btrfs_free_extent(trans, root, bh_blocknr(buf), 1, 1);
+			btrfs_free_extent(trans, root,
+					  extent_buffer_blocknr(buf), 1, 1);
 		}
-		btrfs_block_release(root, buf);
+		free_extent_buffer(buf);
 	} else {
-		btrfs_set_node_blockptr(btrfs_buffer_node(parent), parent_slot,
-					bh_blocknr(cow));
+		btrfs_set_node_blockptr(parent, parent_slot,
+					extent_buffer_blocknr(cow));
 		btrfs_mark_buffer_dirty(parent);
-		WARN_ON(btrfs_header_generation(btrfs_buffer_header(parent)) !=
-				    trans->transid);
-		btrfs_free_extent(trans, root, bh_blocknr(buf), 1, 1);
+		WARN_ON(btrfs_header_generation(parent) != trans->transid);
+		btrfs_free_extent(trans, root, extent_buffer_blocknr(buf),1,1);
 	}
-	btrfs_block_release(root, buf);
+	free_extent_buffer(buf);
 	btrfs_mark_buffer_dirty(cow);
 	*cow_ret = cow;
 	return 0;
 }
 
-int btrfs_cow_block(struct btrfs_trans_handle *trans, struct btrfs_root
-			   *root, struct buffer_head *buf, struct buffer_head
-			   *parent, int parent_slot, struct buffer_head
-			   **cow_ret)
+int btrfs_cow_block(struct btrfs_trans_handle *trans,
+		    struct btrfs_root *root, struct extent_buffer *buf,
+		    struct extent_buffer *parent, int parent_slot,
+		    struct extent_buffer **cow_ret)
 {
 	u64 search_start;
 	if (trans->transaction != root->fs_info->running_transaction) {
@@ -141,13 +142,12 @@ int btrfs_cow_block(struct btrfs_trans_handle *trans, struct btrfs_root
 		       root->fs_info->generation);
 		WARN_ON(1);
 	}
-	if (btrfs_header_generation(btrfs_buffer_header(buf)) ==
-				    trans->transid) {
+	if (btrfs_header_generation(buf) == trans->transid) {
 		*cow_ret = buf;
 		return 0;
 	}
 
-	search_start = bh_blocknr(buf) & ~((u64)65535);
+	search_start = extent_buffer_blocknr(buf) & ~((u64)65535);
 	return __btrfs_cow_block(trans, root, buf, parent,
 				 parent_slot, cow_ret, search_start, 0);
 }
@@ -161,9 +161,11 @@ static int close_blocks(u64 blocknr, u64 other)
 	return 0;
 }
 
-static int should_defrag_leaf(struct buffer_head *bh)
+#if 0
+static int should_defrag_leaf(struct extent_buffer *eb)
 {
-	struct btrfs_leaf *leaf = btrfs_buffer_leaf(bh);
+	return 0;
+	struct btrfs_leaf *leaf = btrfs_buffer_leaf(eb);
 	struct btrfs_disk_key *key;
 	u32 nritems;
 
@@ -188,14 +190,17 @@ static int should_defrag_leaf(struct buffer_head *bh)
 	}
 	return 0;
 }
+#endif
 
 int btrfs_realloc_node(struct btrfs_trans_handle *trans,
-		       struct btrfs_root *root, struct buffer_head *parent,
+		       struct btrfs_root *root, struct extent_buffer *parent,
 		       int cache_only, u64 *last_ret)
 {
+	return 0;
+#if 0
 	struct btrfs_node *parent_node;
-	struct buffer_head *cur_bh;
-	struct buffer_head *tmp_bh;
+	struct extent_buffer *cur_eb;
+	struct extent_buffer *tmp_eb;
 	u64 blocknr;
 	u64 search_start = *last_ret;
 	u64 last_block = 0;
@@ -281,6 +286,7 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 		brelse(tmp_bh);
 	}
 	return err;
+#endif
 }
 
 /*
@@ -289,12 +295,12 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
  * which is the stop of the leaf data stack
  */
 static inline unsigned int leaf_data_end(struct btrfs_root *root,
-					 struct btrfs_leaf *leaf)
+					 struct extent_buffer *leaf)
 {
-	u32 nr = btrfs_header_nritems(&leaf->header);
+	u32 nr = btrfs_header_nritems(leaf);
 	if (nr == 0)
 		return BTRFS_LEAF_DATA_SIZE(root);
-	return btrfs_item_offset(leaf->items + nr - 1);
+	return btrfs_item_offset_nr(leaf, nr - 1);
 }
 
 /*
@@ -310,9 +316,9 @@ static int comp_keys(struct btrfs_disk_key *disk, struct btrfs_key *k2)
 		return 1;
 	if (k1.objectid < k2->objectid)
 		return -1;
-	if (k1.flags > k2->flags)
+	if (k1.type > k2->type)
 		return 1;
-	if (k1.flags < k2->flags)
+	if (k1.type < k2->type)
 		return -1;
 	if (k1.offset > k2->offset)
 		return 1;
@@ -324,37 +330,39 @@ static int comp_keys(struct btrfs_disk_key *disk, struct btrfs_key *k2)
 static int check_node(struct btrfs_root *root, struct btrfs_path *path,
 		      int level)
 {
-	struct btrfs_node *parent = NULL;
-	struct btrfs_node *node = btrfs_buffer_node(path->nodes[level]);
+	struct extent_buffer *parent = NULL;
+	struct extent_buffer *node = path->nodes[level];
+	struct btrfs_disk_key parent_key;
+	struct btrfs_disk_key node_key;
 	int parent_slot;
 	int slot;
 	struct btrfs_key cpukey;
-	u32 nritems = btrfs_header_nritems(&node->header);
+	u32 nritems = btrfs_header_nritems(node);
 
 	if (path->nodes[level + 1])
-		parent = btrfs_buffer_node(path->nodes[level + 1]);
+		parent = path->nodes[level + 1];
 
 	slot = path->slots[level];
-	BUG_ON(!buffer_uptodate(path->nodes[level]));
 	BUG_ON(nritems == 0);
 	if (parent) {
-		struct btrfs_disk_key *parent_key;
-
 		parent_slot = path->slots[level + 1];
-		parent_key = &parent->ptrs[parent_slot].key;
-		BUG_ON(memcmp(parent_key, &node->ptrs[0].key,
+		btrfs_node_key(parent, &parent_key, parent_slot);
+		btrfs_node_key(node, &node_key, 0);
+		BUG_ON(memcmp(&parent_key, &node_key,
 			      sizeof(struct btrfs_disk_key)));
 		BUG_ON(btrfs_node_blockptr(parent, parent_slot) !=
-		       btrfs_header_blocknr(&node->header));
+		       btrfs_header_blocknr(node));
 	}
 	BUG_ON(nritems > BTRFS_NODEPTRS_PER_BLOCK(root));
 	if (slot != 0) {
-		btrfs_disk_key_to_cpu(&cpukey, &node->ptrs[slot - 1].key);
-		BUG_ON(comp_keys(&node->ptrs[slot].key, &cpukey) <= 0);
+		btrfs_node_key_to_cpu(node, &cpukey, slot - 1);
+		btrfs_node_key(node, &node_key, slot);
+		BUG_ON(comp_keys(&node_key, &cpukey) <= 0);
 	}
 	if (slot < nritems - 1) {
-		btrfs_disk_key_to_cpu(&cpukey, &node->ptrs[slot + 1].key);
-		BUG_ON(comp_keys(&node->ptrs[slot].key, &cpukey) >= 0);
+		btrfs_node_key_to_cpu(node, &cpukey, slot + 1);
+		btrfs_node_key(node, &node_key, slot);
+		BUG_ON(comp_keys(&node_key, &cpukey) >= 0);
 	}
 	return 0;
 }
@@ -362,83 +370,172 @@ static int check_node(struct btrfs_root *root, struct btrfs_path *path,
 static int check_leaf(struct btrfs_root *root, struct btrfs_path *path,
 		      int level)
 {
-	struct btrfs_leaf *leaf = btrfs_buffer_leaf(path->nodes[level]);
-	struct btrfs_node *parent = NULL;
+	struct extent_buffer *leaf = path->nodes[level];
+	struct extent_buffer *parent = NULL;
 	int parent_slot;
-	int slot = path->slots[0];
 	struct btrfs_key cpukey;
+	struct btrfs_disk_key parent_key;
+	struct btrfs_disk_key leaf_key;
+	int slot = path->slots[0];
 
-	u32 nritems = btrfs_header_nritems(&leaf->header);
+	u32 nritems = btrfs_header_nritems(leaf);
 
 	if (path->nodes[level + 1])
-		parent = btrfs_buffer_node(path->nodes[level + 1]);
-
-	BUG_ON(btrfs_leaf_free_space(root, leaf) < 0);
+		parent = path->nodes[level + 1];
 
 	if (nritems == 0)
 		return 0;
 
 	if (parent) {
-		struct btrfs_disk_key *parent_key;
-
 		parent_slot = path->slots[level + 1];
-		parent_key = &parent->ptrs[parent_slot].key;
+		btrfs_node_key(parent, &parent_key, parent_slot);
+		btrfs_item_key(leaf, &leaf_key, 0);
 
-		BUG_ON(memcmp(parent_key, &leaf->items[0].key,
+		BUG_ON(memcmp(&parent_key, &leaf_key,
 		       sizeof(struct btrfs_disk_key)));
 		BUG_ON(btrfs_node_blockptr(parent, parent_slot) !=
-		       btrfs_header_blocknr(&leaf->header));
+		       btrfs_header_blocknr(leaf));
+	}
+#if 0
+	for (i = 0; nritems > 1 && i < nritems - 2; i++) {
+		btrfs_item_key_to_cpu(leaf, &cpukey, i + 1);
+		btrfs_item_key(leaf, &leaf_key, i);
+		if (comp_keys(&leaf_key, &cpukey) >= 0) {
+			btrfs_print_leaf(root, leaf);
+			printk("slot %d offset bad key\n", i);
+			BUG_ON(1);
+		}
+		if (btrfs_item_offset_nr(leaf, i) !=
+			btrfs_item_end_nr(leaf, i + 1)) {
+			btrfs_print_leaf(root, leaf);
+			printk("slot %d offset bad\n", i);
+			BUG_ON(1);
+		}
+		if (i == 0) {
+			if (btrfs_item_offset_nr(leaf, i) +
+			       btrfs_item_size_nr(leaf, i) !=
+			       BTRFS_LEAF_DATA_SIZE(root)) {
+				btrfs_print_leaf(root, leaf);
+				printk("slot %d first offset bad\n", i);
+				BUG_ON(1);
+			}
+		}
 	}
-	if (slot != 0) {
-		btrfs_disk_key_to_cpu(&cpukey, &leaf->items[slot - 1].key);
-		BUG_ON(comp_keys(&leaf->items[slot].key, &cpukey) <= 0);
-		BUG_ON(btrfs_item_offset(leaf->items + slot - 1) !=
-			btrfs_item_end(leaf->items + slot));
+	if (nritems > 0) {
+		if (btrfs_item_size_nr(leaf, nritems - 1) > 4096) {
+				btrfs_print_leaf(root, leaf);
+				printk("slot %d bad size \n", nritems - 1);
+				BUG_ON(1);
+		}
+	}
+#endif
+	if (slot != 0 && slot < nritems - 1) {
+		btrfs_item_key(leaf, &leaf_key, slot);
+		btrfs_item_key_to_cpu(leaf, &cpukey, slot - 1);
+		if (comp_keys(&leaf_key, &cpukey) <= 0) {
+			btrfs_print_leaf(root, leaf);
+			printk("slot %d offset bad key\n", slot);
+			BUG_ON(1);
+		}
+		if (btrfs_item_offset_nr(leaf, slot - 1) !=
+		       btrfs_item_end_nr(leaf, slot)) {
+			btrfs_print_leaf(root, leaf);
+			printk("slot %d offset bad\n", slot);
+			BUG_ON(1);
+		}
 	}
 	if (slot < nritems - 1) {
-		btrfs_disk_key_to_cpu(&cpukey, &leaf->items[slot + 1].key);
-		BUG_ON(comp_keys(&leaf->items[slot].key, &cpukey) >= 0);
-		BUG_ON(btrfs_item_offset(leaf->items + slot) !=
-			btrfs_item_end(leaf->items + slot + 1));
+		btrfs_item_key(leaf, &leaf_key, slot);
+		btrfs_item_key_to_cpu(leaf, &cpukey, slot + 1);
+		BUG_ON(comp_keys(&leaf_key, &cpukey) >= 0);
+		if (btrfs_item_offset_nr(leaf, slot) !=
+			btrfs_item_end_nr(leaf, slot + 1)) {
+			btrfs_print_leaf(root, leaf);
+			printk("slot %d offset bad\n", slot);
+			BUG_ON(1);
+		}
 	}
-	BUG_ON(btrfs_item_offset(leaf->items) +
-	       btrfs_item_size(leaf->items) != BTRFS_LEAF_DATA_SIZE(root));
+	BUG_ON(btrfs_item_offset_nr(leaf, 0) +
+	       btrfs_item_size_nr(leaf, 0) != BTRFS_LEAF_DATA_SIZE(root));
 	return 0;
 }
 
 static int check_block(struct btrfs_root *root, struct btrfs_path *path,
 			int level)
 {
-	struct btrfs_node *node = btrfs_buffer_node(path->nodes[level]);
-	if (memcmp(node->header.fsid, root->fs_info->disk_super->fsid,
-		   sizeof(node->header.fsid)))
-		BUG();
+	struct extent_buffer *buf = path->nodes[level];
+	char fsid[BTRFS_FSID_SIZE];
+
+	read_extent_buffer(buf, fsid, (unsigned long)btrfs_header_fsid(buf),
+			   BTRFS_FSID_SIZE);
+
+	if (memcmp(fsid, root->fs_info->fsid, BTRFS_FSID_SIZE)) {
+		int i = 0;
+		printk("warning bad block %Lu\n", buf->start);
+		if (!btrfs_buffer_uptodate(buf)) {
+			WARN_ON(1);
+		}
+		for (i = 0; i < BTRFS_FSID_SIZE; i++) {
+			printk("%x:%x ", root->fs_info->fsid[i], fsid[i]);
+		}
+		printk("\n");
+		// BUG();
+	}
 	if (level == 0)
 		return check_leaf(root, path, level);
 	return check_node(root, path, level);
 }
 
 /*
- * search for key in the array p.  items p are item_size apart
- * and there are 'max' items in p
+ * search for key in the extent_buffer.  The items start at offset p,
+ * and they are item_size apart.  There are 'max' items in p.
+ *
  * the slot in the array is returned via slot, and it points to
  * the place where you would insert key if it is not found in
  * the array.
  *
  * slot may point to max if the key is bigger than all of the keys
  */
-static int generic_bin_search(char *p, int item_size, struct btrfs_key *key,
-		       int max, int *slot)
+static int generic_bin_search(struct extent_buffer *eb, unsigned long p,
+			      int item_size, struct btrfs_key *key,
+			      int max, int *slot)
 {
 	int low = 0;
 	int high = max;
 	int mid;
 	int ret;
 	struct btrfs_disk_key *tmp;
+	struct btrfs_disk_key unaligned;
+	unsigned long offset;
+	char *map_token = NULL;
+	char *kaddr = NULL;
+	unsigned long map_start = 0;
+	unsigned long map_len = 0;
 
 	while(low < high) {
 		mid = (low + high) / 2;
-		tmp = (struct btrfs_disk_key *)(p + mid * item_size);
+		offset = p + mid * item_size;
+
+		if (!map_token || offset < map_start ||
+		    (offset + sizeof(struct btrfs_disk_key)) >
+		    map_start + map_len) {
+			if (map_token)
+				unmap_extent_buffer(eb, map_token, KM_USER0);
+			map_extent_buffer(eb, offset, &map_token, &kaddr,
+					  &map_start, &map_len, KM_USER0);
+
+		}
+		if (offset + sizeof(struct btrfs_disk_key) >
+		    map_start + map_len) {
+			unmap_extent_buffer(eb, map_token, KM_USER0);
+			read_extent_buffer(eb, &unaligned,
+					   offset, sizeof(unaligned));
+			map_token = NULL;
+			tmp = &unaligned;
+		} else {
+			tmp = (struct btrfs_disk_key *)(kaddr + offset -
+							map_start);
+		}
 		ret = comp_keys(tmp, key);
 
 		if (ret < 0)
@@ -447,10 +544,13 @@ static int generic_bin_search(char *p, int item_size, struct btrfs_key *key,
 			high = mid;
 		else {
 			*slot = mid;
+			unmap_extent_buffer(eb, map_token, KM_USER0);
 			return 0;
 		}
 	}
 	*slot = low;
+	if (map_token)
+		unmap_extent_buffer(eb, map_token, KM_USER0);
 	return 1;
 }
 
@@ -458,46 +558,42 @@ static int generic_bin_search(char *p, int item_size, struct btrfs_key *key,
  * simple bin_search frontend that does the right thing for
  * leaves vs nodes
  */
-static int bin_search(struct btrfs_node *c, struct btrfs_key *key, int *slot)
+static int bin_search(struct extent_buffer *eb, struct btrfs_key *key,
+		      int level, int *slot)
 {
-	if (btrfs_is_leaf(c)) {
-		struct btrfs_leaf *l = (struct btrfs_leaf *)c;
-		return generic_bin_search((void *)l->items,
+	if (level == 0) {
+		return generic_bin_search(eb,
+					  offsetof(struct btrfs_leaf, items),
 					  sizeof(struct btrfs_item),
-					  key, btrfs_header_nritems(&c->header),
+					  key, btrfs_header_nritems(eb),
 					  slot);
 	} else {
-		return generic_bin_search((void *)c->ptrs,
+		return generic_bin_search(eb,
+					  offsetof(struct btrfs_node, ptrs),
 					  sizeof(struct btrfs_key_ptr),
-					  key, btrfs_header_nritems(&c->header),
+					  key, btrfs_header_nritems(eb),
 					  slot);
 	}
 	return -1;
 }
 
-static struct buffer_head *read_node_slot(struct btrfs_root *root,
-				   struct buffer_head *parent_buf,
-				   int slot)
+static struct extent_buffer *read_node_slot(struct btrfs_root *root,
+				   struct extent_buffer *parent, int slot)
 {
-	struct btrfs_node *node = btrfs_buffer_node(parent_buf);
 	if (slot < 0)
 		return NULL;
-	if (slot >= btrfs_header_nritems(&node->header))
+	if (slot >= btrfs_header_nritems(parent))
 		return NULL;
-	return read_tree_block(root, btrfs_node_blockptr(node, slot));
+	return read_tree_block(root, btrfs_node_blockptr(parent, slot));
 }
 
 static int balance_level(struct btrfs_trans_handle *trans, struct btrfs_root
 			 *root, struct btrfs_path *path, int level)
 {
-	struct buffer_head *right_buf;
-	struct buffer_head *mid_buf;
-	struct buffer_head *left_buf;
-	struct buffer_head *parent_buf = NULL;
-	struct btrfs_node *right = NULL;
-	struct btrfs_node *mid;
-	struct btrfs_node *left = NULL;
-	struct btrfs_node *parent = NULL;
+	struct extent_buffer *right = NULL;
+	struct extent_buffer *mid;
+	struct extent_buffer *left = NULL;
+	struct extent_buffer *parent = NULL;
 	int ret = 0;
 	int wret;
 	int pslot;
@@ -508,60 +604,57 @@ static int balance_level(struct btrfs_trans_handle *trans, struct btrfs_root
 	if (level == 0)
 		return 0;
 
-	mid_buf = path->nodes[level];
-	mid = btrfs_buffer_node(mid_buf);
+	mid = path->nodes[level];
 	orig_ptr = btrfs_node_blockptr(mid, orig_slot);
 
 	if (level < BTRFS_MAX_LEVEL - 1)
-		parent_buf = path->nodes[level + 1];
+		parent = path->nodes[level + 1];
 	pslot = path->slots[level + 1];
 
 	/*
 	 * deal with the case where there is only one pointer in the root
 	 * by promoting the node below to a root
 	 */
-	if (!parent_buf) {
-		struct buffer_head *child;
-		u64 blocknr = bh_blocknr(mid_buf);
+	if (!parent) {
+		struct extent_buffer *child;
+		u64 blocknr = extent_buffer_blocknr(mid);
 
-		if (btrfs_header_nritems(&mid->header) != 1)
+		if (btrfs_header_nritems(mid) != 1)
 			return 0;
 
 		/* promote the child to a root */
-		child = read_node_slot(root, mid_buf, 0);
+		child = read_node_slot(root, mid, 0);
 		BUG_ON(!child);
 		root->node = child;
 		path->nodes[level] = NULL;
-		clean_tree_block(trans, root, mid_buf);
-		wait_on_buffer(mid_buf);
+		clean_tree_block(trans, root, mid);
+		wait_on_tree_block_writeback(root, mid);
 		/* once for the path */
-		btrfs_block_release(root, mid_buf);
+		free_extent_buffer(mid);
 		/* once for the root ptr */
-		btrfs_block_release(root, mid_buf);
+		free_extent_buffer(mid);
 		return btrfs_free_extent(trans, root, blocknr, 1, 1);
 	}
-	parent = btrfs_buffer_node(parent_buf);
-
-	if (btrfs_header_nritems(&mid->header) >
+	if (btrfs_header_nritems(mid) >
 	    BTRFS_NODEPTRS_PER_BLOCK(root) / 4)
 		return 0;
 
-	if (btrfs_header_nritems(&mid->header) < 2)
+	if (btrfs_header_nritems(mid) < 2)
 		err_on_enospc = 1;
 
-	left_buf = read_node_slot(root, parent_buf, pslot - 1);
-	if (left_buf) {
-		wret = btrfs_cow_block(trans, root, left_buf,
-				       parent_buf, pslot - 1, &left_buf);
+	left = read_node_slot(root, parent, pslot - 1);
+	if (left) {
+		wret = btrfs_cow_block(trans, root, left,
+				       parent, pslot - 1, &left);
 		if (wret) {
 			ret = wret;
 			goto enospc;
 		}
 	}
-	right_buf = read_node_slot(root, parent_buf, pslot + 1);
-	if (right_buf) {
-		wret = btrfs_cow_block(trans, root, right_buf,
-				       parent_buf, pslot + 1, &right_buf);
+	right = read_node_slot(root, parent, pslot + 1);
+	if (right) {
+		wret = btrfs_cow_block(trans, root, right,
+				       parent, pslot + 1, &right);
 		if (wret) {
 			ret = wret;
 			goto enospc;
@@ -569,30 +662,27 @@ static int balance_level(struct btrfs_trans_handle *trans, struct btrfs_root
 	}
 
 	/* first, try to make some room in the middle buffer */
-	if (left_buf) {
-		left = btrfs_buffer_node(left_buf);
-		orig_slot += btrfs_header_nritems(&left->header);
-		wret = push_node_left(trans, root, left_buf, mid_buf);
+	if (left) {
+		orig_slot += btrfs_header_nritems(left);
+		wret = push_node_left(trans, root, left, mid);
 		if (wret < 0)
 			ret = wret;
-		if (btrfs_header_nritems(&mid->header) < 2)
+		if (btrfs_header_nritems(mid) < 2)
 			err_on_enospc = 1;
 	}
 
 	/*
 	 * then try to empty the right most buffer into the middle
 	 */
-	if (right_buf) {
-		right = btrfs_buffer_node(right_buf);
-		wret = push_node_left(trans, root, mid_buf, right_buf);
+	if (right) {
+		wret = push_node_left(trans, root, mid, right);
 		if (wret < 0 && wret != -ENOSPC)
 			ret = wret;
-		if (btrfs_header_nritems(&right->header) == 0) {
-			u64 blocknr = bh_blocknr(right_buf);
-			clean_tree_block(trans, root, right_buf);
-			wait_on_buffer(right_buf);
-			btrfs_block_release(root, right_buf);
-			right_buf = NULL;
+		if (btrfs_header_nritems(right) == 0) {
+			u64 blocknr = extent_buffer_blocknr(right);
+			clean_tree_block(trans, root, right);
+			wait_on_tree_block_writeback(root, right);
+			free_extent_buffer(right);
 			right = NULL;
 			wret = del_ptr(trans, root, path, level + 1, pslot +
 				       1);
@@ -602,14 +692,13 @@ static int balance_level(struct btrfs_trans_handle *trans, struct btrfs_root
 			if (wret)
 				ret = wret;
 		} else {
-			btrfs_memcpy(root, parent,
-				     &parent->ptrs[pslot + 1].key,
-				     &right->ptrs[0].key,
-				     sizeof(struct btrfs_disk_key));
-			btrfs_mark_buffer_dirty(parent_buf);
+			struct btrfs_disk_key right_key;
+			btrfs_node_key(right, &right_key, 0);
+			btrfs_set_node_key(parent, &right_key, pslot + 1);
+			btrfs_mark_buffer_dirty(parent);
 		}
 	}
-	if (btrfs_header_nritems(&mid->header) == 1) {
+	if (btrfs_header_nritems(mid) == 1) {
 		/*
 		 * we're not allowed to leave a node with one item in the
 		 * tree during a delete.  A deletion from lower in the tree
@@ -619,21 +708,20 @@ static int balance_level(struct btrfs_trans_handle *trans, struct btrfs_root
 		 * otherwise we would have pulled some pointers from the
 		 * right
 		 */
-		BUG_ON(!left_buf);
-		wret = balance_node_right(trans, root, mid_buf, left_buf);
+		BUG_ON(!left);
+		wret = balance_node_right(trans, root, mid, left);
 		if (wret < 0) {
 			ret = wret;
 			goto enospc;
 		}
 		BUG_ON(wret == 1);
 	}
-	if (btrfs_header_nritems(&mid->header) == 0) {
+	if (btrfs_header_nritems(mid) == 0) {
 		/* we've managed to empty the middle node, drop it */
-		u64 blocknr = bh_blocknr(mid_buf);
-		clean_tree_block(trans, root, mid_buf);
-		wait_on_buffer(mid_buf);
-		btrfs_block_release(root, mid_buf);
-		mid_buf = NULL;
+		u64 blocknr = extent_buffer_blocknr(mid);
+		clean_tree_block(trans, root, mid);
+		wait_on_tree_block_writeback(root, mid);
+		free_extent_buffer(mid);
 		mid = NULL;
 		wret = del_ptr(trans, root, path, level + 1, pslot);
 		if (wret)
@@ -643,37 +731,36 @@ static int balance_level(struct btrfs_trans_handle *trans, struct btrfs_root
 			ret = wret;
 	} else {
 		/* update the parent key to reflect our changes */
-		btrfs_memcpy(root, parent,
-			     &parent->ptrs[pslot].key, &mid->ptrs[0].key,
-			     sizeof(struct btrfs_disk_key));
-		btrfs_mark_buffer_dirty(parent_buf);
+		struct btrfs_disk_key mid_key;
+		btrfs_node_key(mid, &mid_key, 0);
+		btrfs_set_node_key(parent, &mid_key, pslot);
+		btrfs_mark_buffer_dirty(parent);
 	}
 
 	/* update the path */
-	if (left_buf) {
-		if (btrfs_header_nritems(&left->header) > orig_slot) {
-			get_bh(left_buf);
-			path->nodes[level] = left_buf;
+	if (left) {
+		if (btrfs_header_nritems(left) > orig_slot) {
+			extent_buffer_get(left);
+			path->nodes[level] = left;
 			path->slots[level + 1] -= 1;
 			path->slots[level] = orig_slot;
-			if (mid_buf)
-				btrfs_block_release(root, mid_buf);
+			if (mid)
+				free_extent_buffer(mid);
 		} else {
-			orig_slot -= btrfs_header_nritems(&left->header);
+			orig_slot -= btrfs_header_nritems(left);
 			path->slots[level] = orig_slot;
 		}
 	}
 	/* double check we haven't messed things up */
 	check_block(root, path, level);
 	if (orig_ptr !=
-	    btrfs_node_blockptr(btrfs_buffer_node(path->nodes[level]),
-				path->slots[level]))
+	    btrfs_node_blockptr(path->nodes[level], path->slots[level]))
 		BUG();
 enospc:
-	if (right_buf)
-		btrfs_block_release(root, right_buf);
-	if (left_buf)
-		btrfs_block_release(root, left_buf);
+	if (right)
+		free_extent_buffer(right);
+	if (left)
+		free_extent_buffer(left);
 	return ret;
 }
 
@@ -682,14 +769,10 @@ static int push_nodes_for_insert(struct btrfs_trans_handle *trans,
 				struct btrfs_root *root,
 				struct btrfs_path *path, int level)
 {
-	struct buffer_head *right_buf;
-	struct buffer_head *mid_buf;
-	struct buffer_head *left_buf;
-	struct buffer_head *parent_buf = NULL;
-	struct btrfs_node *right = NULL;
-	struct btrfs_node *mid;
-	struct btrfs_node *left = NULL;
-	struct btrfs_node *parent = NULL;
+	struct extent_buffer *right = NULL;
+	struct extent_buffer *mid;
+	struct extent_buffer *left = NULL;
+	struct extent_buffer *parent = NULL;
 	int ret = 0;
 	int wret;
 	int pslot;
@@ -699,107 +782,101 @@ static int push_nodes_for_insert(struct btrfs_trans_handle *trans,
 	if (level == 0)
 		return 1;
 
-	mid_buf = path->nodes[level];
-	mid = btrfs_buffer_node(mid_buf);
+	mid = path->nodes[level];
 	orig_ptr = btrfs_node_blockptr(mid, orig_slot);
 
 	if (level < BTRFS_MAX_LEVEL - 1)
-		parent_buf = path->nodes[level + 1];
+		parent = path->nodes[level + 1];
 	pslot = path->slots[level + 1];
 
-	if (!parent_buf)
+	if (!parent)
 		return 1;
-	parent = btrfs_buffer_node(parent_buf);
 
-	left_buf = read_node_slot(root, parent_buf, pslot - 1);
+	left = read_node_slot(root, parent, pslot - 1);
 
 	/* first, try to make some room in the middle buffer */
-	if (left_buf) {
+	if (left) {
 		u32 left_nr;
-		left = btrfs_buffer_node(left_buf);
-		left_nr = btrfs_header_nritems(&left->header);
+		left_nr = btrfs_header_nritems(left);
 		if (left_nr >= BTRFS_NODEPTRS_PER_BLOCK(root) - 1) {
 			wret = 1;
 		} else {
-			ret = btrfs_cow_block(trans, root, left_buf, parent_buf,
-					      pslot - 1, &left_buf);
+			ret = btrfs_cow_block(trans, root, left, parent,
+					      pslot - 1, &left);
 			if (ret)
 				wret = 1;
 			else {
-				left = btrfs_buffer_node(left_buf);
 				wret = push_node_left(trans, root,
-						      left_buf, mid_buf);
+						      left, mid);
 			}
 		}
 		if (wret < 0)
 			ret = wret;
 		if (wret == 0) {
+			struct btrfs_disk_key disk_key;
 			orig_slot += left_nr;
-			btrfs_memcpy(root, parent,
-				     &parent->ptrs[pslot].key,
-				     &mid->ptrs[0].key,
-				     sizeof(struct btrfs_disk_key));
-			btrfs_mark_buffer_dirty(parent_buf);
-			if (btrfs_header_nritems(&left->header) > orig_slot) {
-				path->nodes[level] = left_buf;
+			btrfs_node_key(mid, &disk_key, 0);
+			btrfs_set_node_key(parent, &disk_key, pslot);
+			btrfs_mark_buffer_dirty(parent);
+			if (btrfs_header_nritems(left) > orig_slot) {
+				path->nodes[level] = left;
 				path->slots[level + 1] -= 1;
 				path->slots[level] = orig_slot;
-				btrfs_block_release(root, mid_buf);
+				free_extent_buffer(mid);
 			} else {
 				orig_slot -=
-					btrfs_header_nritems(&left->header);
+					btrfs_header_nritems(left);
 				path->slots[level] = orig_slot;
-				btrfs_block_release(root, left_buf);
+				free_extent_buffer(left);
 			}
 			check_node(root, path, level);
 			return 0;
 		}
-		btrfs_block_release(root, left_buf);
+		free_extent_buffer(left);
 	}
-	right_buf = read_node_slot(root, parent_buf, pslot + 1);
+	right= read_node_slot(root, parent, pslot + 1);
 
 	/*
 	 * then try to empty the right most buffer into the middle
 	 */
-	if (right_buf) {
+	if (right) {
 		u32 right_nr;
-		right = btrfs_buffer_node(right_buf);
-		right_nr = btrfs_header_nritems(&right->header);
+		right_nr = btrfs_header_nritems(right);
 		if (right_nr >= BTRFS_NODEPTRS_PER_BLOCK(root) - 1) {
 			wret = 1;
 		} else {
-			ret = btrfs_cow_block(trans, root, right_buf,
-					      parent_buf, pslot + 1,
-					      &right_buf);
+			ret = btrfs_cow_block(trans, root, right,
+					      parent, pslot + 1,
+					      &right);
 			if (ret)
 				wret = 1;
 			else {
-				right = btrfs_buffer_node(right_buf);
 				wret = balance_node_right(trans, root,
-							  right_buf, mid_buf);
+							  right, mid);
 			}
 		}
 		if (wret < 0)
 			ret = wret;
 		if (wret == 0) {
-			btrfs_memcpy(root, parent,
-				     &parent->ptrs[pslot + 1].key,
-				     &right->ptrs[0].key,
-				     sizeof(struct btrfs_disk_key));
-			btrfs_mark_buffer_dirty(parent_buf);
-			if (btrfs_header_nritems(&mid->header) <= orig_slot) {
-				path->nodes[level] = right_buf;
+			struct btrfs_disk_key disk_key;
+
+			btrfs_node_key(right, &disk_key, 0);
+			btrfs_set_node_key(parent, &disk_key, pslot + 1);
+			btrfs_mark_buffer_dirty(parent);
+
+			if (btrfs_header_nritems(mid) <= orig_slot) {
+				path->nodes[level] = right;
 				path->slots[level + 1] += 1;
 				path->slots[level] = orig_slot -
-					btrfs_header_nritems(&mid->header);
-				btrfs_block_release(root, mid_buf);
+					btrfs_header_nritems(mid);
+				free_extent_buffer(mid);
 			} else {
-				btrfs_block_release(root, right_buf);
+				free_extent_buffer(right);
 			}
 			check_node(root, path, level);
 			return 0;
 		}
-		btrfs_block_release(root, right_buf);
+		free_extent_buffer(right);
 	}
 	check_node(root, path, level);
 	return 1;
@@ -811,10 +888,9 @@ static int push_nodes_for_insert(struct btrfs_trans_handle *trans,
 static void reada_for_search(struct btrfs_root *root, struct btrfs_path *path,
 			     int level, int slot)
 {
-	struct btrfs_node *node;
+	struct extent_buffer *node;
 	int i;
 	u32 nritems;
-	u64 item_objectid;
 	u64 blocknr;
 	u64 search;
 	u64 cluster_start;
@@ -823,7 +899,7 @@ static void reada_for_search(struct btrfs_root *root, struct btrfs_path *path,
 	int direction = path->reada;
 	struct radix_tree_root found;
 	unsigned long gang[8];
-	struct buffer_head *bh;
+	struct extent_buffer *eb;
 
 	if (level == 0)
 		return;
@@ -831,18 +907,17 @@ static void reada_for_search(struct btrfs_root *root, struct btrfs_path *path,
 	if (!path->nodes[level])
 		return;
 
-	node = btrfs_buffer_node(path->nodes[level]);
+	node = path->nodes[level];
 	search = btrfs_node_blockptr(node, slot);
-	bh = btrfs_find_tree_block(root, search);
-	if (bh) {
-		brelse(bh);
+	eb = btrfs_find_tree_block(root, search);
+	if (eb) {
+		free_extent_buffer(eb);
 		return;
 	}
 
 	init_bit_radix(&found);
-	nritems = btrfs_header_nritems(&node->header);
+	nritems = btrfs_header_nritems(node);
 	for (i = slot; i < nritems; i++) {
-		item_objectid = btrfs_disk_key_objectid(&node->ptrs[i].key);
 		blocknr = btrfs_node_blockptr(node, i);
 		set_radix_bit(&found, blocknr);
 	}
@@ -886,8 +961,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 		      *root, struct btrfs_key *key, struct btrfs_path *p, int
 		      ins_len, int cow)
 {
-	struct buffer_head *b;
-	struct btrfs_node *c;
+	struct extent_buffer *b;
 	u64 blocknr;
 	int slot;
 	int ret;
@@ -901,10 +975,9 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 	WARN_ON(!mutex_is_locked(&root->fs_info->fs_mutex));
 again:
 	b = root->node;
-	get_bh(b);
+	extent_buffer_get(b);
 	while (b) {
-		c = btrfs_buffer_node(b);
-		level = btrfs_header_level(&c->header);
+		level = btrfs_header_level(b);
 		if (cow) {
 			int wret;
 			wret = btrfs_cow_block(trans, root, b,
@@ -912,32 +985,30 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 					       p->slots[level + 1],
 					       &b);
 			if (wret) {
-				btrfs_block_release(root, b);
+				free_extent_buffer(b);
 				return wret;
 			}
-			c = btrfs_buffer_node(b);
 		}
 		BUG_ON(!cow && ins_len);
-		if (level != btrfs_header_level(&c->header))
+		if (level != btrfs_header_level(b))
 			WARN_ON(1);
-		level = btrfs_header_level(&c->header);
+		level = btrfs_header_level(b);
 		p->nodes[level] = b;
 		ret = check_block(root, p, level);
 		if (ret)
 			return -1;
-		ret = bin_search(c, key, &slot);
-		if (!btrfs_is_leaf(c)) {
+		ret = bin_search(b, key, level, &slot);
+		if (level != 0) {
 			if (ret && slot > 0)
 				slot -= 1;
 			p->slots[level] = slot;
-			if (ins_len > 0 && btrfs_header_nritems(&c->header) >=
+			if (ins_len > 0 && btrfs_header_nritems(b) >=
 			    BTRFS_NODEPTRS_PER_BLOCK(root) - 1) {
 				int sret = split_node(trans, root, p, level);
 				BUG_ON(sret > 0);
 				if (sret)
 					return sret;
 				b = p->nodes[level];
-				c = btrfs_buffer_node(b);
 				slot = p->slots[level];
 			} else if (ins_len < 0) {
 				int sret = balance_level(trans, root, p,
@@ -947,22 +1018,19 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 				b = p->nodes[level];
 				if (!b)
 					goto again;
-				c = btrfs_buffer_node(b);
 				slot = p->slots[level];
-				BUG_ON(btrfs_header_nritems(&c->header) == 1);
+				BUG_ON(btrfs_header_nritems(b) == 1);
 			}
 			/* this is only true while dropping a snapshot */
 			if (level == lowest_level)
 				break;
-			blocknr = btrfs_node_blockptr(c, slot);
+			blocknr = btrfs_node_blockptr(b, slot);
 			if (should_reada)
 				reada_for_search(root, p, level, slot);
-			b = read_tree_block(root, btrfs_node_blockptr(c, slot));
-
+			b = read_tree_block(root, btrfs_node_blockptr(b, slot));
 		} else {
-			struct btrfs_leaf *l = (struct btrfs_leaf *)c;
 			p->slots[level] = slot;
-			if (ins_len > 0 && btrfs_leaf_free_space(root, l) <
+			if (ins_len > 0 && btrfs_leaf_free_space(root, b) <
 			    sizeof(struct btrfs_item) + ins_len) {
 				int sret = split_leaf(trans, root, key,
 						      p, ins_len);
@@ -986,19 +1054,20 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
  * If this fails to write a tree block, it returns -1, but continues
  * fixing up the blocks in ram so the tree is consistent.
  */
-static int fixup_low_keys(struct btrfs_trans_handle *trans, struct btrfs_root
-			  *root, struct btrfs_path *path, struct btrfs_disk_key
-			  *key, int level)
+static int fixup_low_keys(struct btrfs_trans_handle *trans,
+			  struct btrfs_root *root, struct btrfs_path *path,
+			  struct btrfs_disk_key *key, int level)
 {
 	int i;
 	int ret = 0;
+	struct extent_buffer *t;
+
 	for (i = level; i < BTRFS_MAX_LEVEL; i++) {
-		struct btrfs_node *t;
 		int tslot = path->slots[i];
 		if (!path->nodes[i])
 			break;
-		t = btrfs_buffer_node(path->nodes[i]);
-		btrfs_memcpy(root, t, &t->ptrs[tslot].key, key, sizeof(*key));
+		t = path->nodes[i];
+		btrfs_set_node_key(t, key, tslot);
 		btrfs_mark_buffer_dirty(path->nodes[i]);
 		if (tslot != 0)
 			break;
@@ -1014,18 +1083,16 @@ static int fixup_low_keys(struct btrfs_trans_handle *trans, struct btrfs_root
  * error, and > 0 if there was no room in the left hand block.
  */
 static int push_node_left(struct btrfs_trans_handle *trans, struct btrfs_root
-			  *root, struct buffer_head *dst_buf, struct
-			  buffer_head *src_buf)
+			  *root, struct extent_buffer *dst,
+			  struct extent_buffer *src)
 {
-	struct btrfs_node *src = btrfs_buffer_node(src_buf);
-	struct btrfs_node *dst = btrfs_buffer_node(dst_buf);
 	int push_items = 0;
 	int src_nritems;
 	int dst_nritems;
 	int ret = 0;
 
-	src_nritems = btrfs_header_nritems(&src->header);
-	dst_nritems = btrfs_header_nritems(&dst->header);
+	src_nritems = btrfs_header_nritems(src);
+	dst_nritems = btrfs_header_nritems(dst);
 	push_items = BTRFS_NODEPTRS_PER_BLOCK(root) - dst_nritems;
 
 	if (push_items <= 0) {
@@ -1035,17 +1102,21 @@ static int push_node_left(struct btrfs_trans_handle *trans, struct btrfs_root
 	if (src_nritems < push_items)
 		push_items = src_nritems;
 
-	btrfs_memcpy(root, dst, dst->ptrs + dst_nritems, src->ptrs,
-		     push_items * sizeof(struct btrfs_key_ptr));
+	copy_extent_buffer(dst, src,
+			   btrfs_node_key_ptr_offset(dst_nritems),
+			   btrfs_node_key_ptr_offset(0),
+		           push_items * sizeof(struct btrfs_key_ptr));
+
 	if (push_items < src_nritems) {
-		btrfs_memmove(root, src, src->ptrs, src->ptrs + push_items,
-			(src_nritems - push_items) *
-			sizeof(struct btrfs_key_ptr));
-	}
-	btrfs_set_header_nritems(&src->header, src_nritems - push_items);
-	btrfs_set_header_nritems(&dst->header, dst_nritems + push_items);
-	btrfs_mark_buffer_dirty(src_buf);
-	btrfs_mark_buffer_dirty(dst_buf);
+		memmove_extent_buffer(src, btrfs_node_key_ptr_offset(0),
+				      btrfs_node_key_ptr_offset(push_items),
+				      (src_nritems - push_items) *
+				      sizeof(struct btrfs_key_ptr));
+	}
+	btrfs_set_header_nritems(src, src_nritems - push_items);
+	btrfs_set_header_nritems(dst, dst_nritems + push_items);
+	btrfs_mark_buffer_dirty(src);
+	btrfs_mark_buffer_dirty(dst);
 	return ret;
 }
 
@@ -1058,24 +1129,22 @@ static int push_node_left(struct btrfs_trans_handle *trans, struct btrfs_root
  *
  * this will  only push up to 1/2 the contents of the left node over
  */
-static int balance_node_right(struct btrfs_trans_handle *trans, struct
-			      btrfs_root *root, struct buffer_head *dst_buf,
-			      struct buffer_head *src_buf)
+static int balance_node_right(struct btrfs_trans_handle *trans,
+			      struct btrfs_root *root,
+			      struct extent_buffer *dst,
+			      struct extent_buffer *src)
 {
-	struct btrfs_node *src = btrfs_buffer_node(src_buf);
-	struct btrfs_node *dst = btrfs_buffer_node(dst_buf);
 	int push_items = 0;
 	int max_push;
 	int src_nritems;
 	int dst_nritems;
 	int ret = 0;
 
-	src_nritems = btrfs_header_nritems(&src->header);
-	dst_nritems = btrfs_header_nritems(&dst->header);
+	src_nritems = btrfs_header_nritems(src);
+	dst_nritems = btrfs_header_nritems(dst);
 	push_items = BTRFS_NODEPTRS_PER_BLOCK(root) - dst_nritems;
-	if (push_items <= 0) {
+	if (push_items <= 0)
 		return 1;
-	}
 
 	max_push = src_nritems / 2 + 1;
 	/* don't try to empty the node */
@@ -1085,18 +1154,21 @@ static int balance_node_right(struct btrfs_trans_handle *trans, struct
 	if (max_push < push_items)
 		push_items = max_push;
 
-	btrfs_memmove(root, dst, dst->ptrs + push_items, dst->ptrs,
-		      dst_nritems * sizeof(struct btrfs_key_ptr));
+	memmove_extent_buffer(dst, btrfs_node_key_ptr_offset(push_items),
+				      btrfs_node_key_ptr_offset(0),
+				      (dst_nritems) *
+				      sizeof(struct btrfs_key_ptr));
 
-	btrfs_memcpy(root, dst, dst->ptrs,
-		     src->ptrs + src_nritems - push_items,
-		     push_items * sizeof(struct btrfs_key_ptr));
+	copy_extent_buffer(dst, src,
+			   btrfs_node_key_ptr_offset(0),
+			   btrfs_node_key_ptr_offset(src_nritems - push_items),
+		           push_items * sizeof(struct btrfs_key_ptr));
 
-	btrfs_set_header_nritems(&src->header, src_nritems - push_items);
-	btrfs_set_header_nritems(&dst->header, dst_nritems + push_items);
+	btrfs_set_header_nritems(src, src_nritems - push_items);
+	btrfs_set_header_nritems(dst, dst_nritems + push_items);
 
-	btrfs_mark_buffer_dirty(src_buf);
-	btrfs_mark_buffer_dirty(dst_buf);
+	btrfs_mark_buffer_dirty(src);
+	btrfs_mark_buffer_dirty(dst);
 	return ret;
 }
 
@@ -1107,45 +1179,46 @@ static int balance_node_right(struct btrfs_trans_handle *trans, struct
  *
  * returns zero on success or < 0 on failure.
  */
-static int insert_new_root(struct btrfs_trans_handle *trans, struct btrfs_root
-			   *root, struct btrfs_path *path, int level)
+static int insert_new_root(struct btrfs_trans_handle *trans,
+			   struct btrfs_root *root,
+			   struct btrfs_path *path, int level)
 {
-	struct buffer_head *t;
-	struct btrfs_node *lower;
-	struct btrfs_node *c;
-	struct btrfs_disk_key *lower_key;
+	struct extent_buffer *lower;
+	struct extent_buffer *c;
+	struct btrfs_disk_key lower_key;
 
 	BUG_ON(path->nodes[level]);
 	BUG_ON(path->nodes[level-1] != root->node);
 
-	t = btrfs_alloc_free_block(trans, root, root->node->b_blocknr, 0);
-	if (IS_ERR(t))
-		return PTR_ERR(t);
-	c = btrfs_buffer_node(t);
-	memset(c, 0, root->blocksize);
-	btrfs_set_header_nritems(&c->header, 1);
-	btrfs_set_header_level(&c->header, level);
-	btrfs_set_header_blocknr(&c->header, bh_blocknr(t));
-	btrfs_set_header_generation(&c->header, trans->transid);
-	btrfs_set_header_owner(&c->header, root->root_key.objectid);
-	lower = btrfs_buffer_node(path->nodes[level-1]);
-	memcpy(c->header.fsid, root->fs_info->disk_super->fsid,
-	       sizeof(c->header.fsid));
-	if (btrfs_is_leaf(lower))
-		lower_key = &((struct btrfs_leaf *)lower)->items[0].key;
+	c = btrfs_alloc_free_block(trans, root,
+				   extent_buffer_blocknr(root->node), 0);
+	if (IS_ERR(c))
+		return PTR_ERR(c);
+	memset_extent_buffer(c, 0, 0, root->nodesize);
+	btrfs_set_header_nritems(c, 1);
+	btrfs_set_header_level(c, level);
+	btrfs_set_header_blocknr(c, extent_buffer_blocknr(c));
+	btrfs_set_header_generation(c, trans->transid);
+	btrfs_set_header_owner(c, root->root_key.objectid);
+	lower = path->nodes[level-1];
+
+	write_extent_buffer(c, root->fs_info->fsid,
+			    (unsigned long)btrfs_header_fsid(c),
+			    BTRFS_FSID_SIZE);
+	if (level == 1)
+		btrfs_item_key(lower, &lower_key, 0);
 	else
-		lower_key = &lower->ptrs[0].key;
-	btrfs_memcpy(root, c, &c->ptrs[0].key, lower_key,
-		     sizeof(struct btrfs_disk_key));
-	btrfs_set_node_blockptr(c, 0, bh_blocknr(path->nodes[level - 1]));
+		btrfs_node_key(lower, &lower_key, 0);
+	btrfs_set_node_key(c, &lower_key, 0);
+	btrfs_set_node_blockptr(c, 0, extent_buffer_blocknr(lower));
 
-	btrfs_mark_buffer_dirty(t);
+	btrfs_mark_buffer_dirty(c);
 
 	/* the super has an extra ref to root->node */
-	btrfs_block_release(root, root->node);
-	root->node = t;
-	get_bh(t);
-	path->nodes[level] = t;
+	free_extent_buffer(root->node);
+	root->node = c;
+	extent_buffer_get(c);
+	path->nodes[level] = c;
 	path->slots[level] = 0;
 	return 0;
 }
@@ -1163,26 +1236,26 @@ static int insert_ptr(struct btrfs_trans_handle *trans, struct btrfs_root
 		      *root, struct btrfs_path *path, struct btrfs_disk_key
 		      *key, u64 blocknr, int slot, int level)
 {
-	struct btrfs_node *lower;
+	struct extent_buffer *lower;
 	int nritems;
 
 	BUG_ON(!path->nodes[level]);
-	lower = btrfs_buffer_node(path->nodes[level]);
-	nritems = btrfs_header_nritems(&lower->header);
+	lower = path->nodes[level];
+	nritems = btrfs_header_nritems(lower);
 	if (slot > nritems)
 		BUG();
 	if (nritems == BTRFS_NODEPTRS_PER_BLOCK(root))
 		BUG();
 	if (slot != nritems) {
-		btrfs_memmove(root, lower, lower->ptrs + slot + 1,
-			      lower->ptrs + slot,
+		memmove_extent_buffer(lower,
+			      btrfs_node_key_ptr_offset(slot + 1),
+			      btrfs_node_key_ptr_offset(slot),
 			      (nritems - slot) * sizeof(struct btrfs_key_ptr));
 	}
-	btrfs_memcpy(root, lower, &lower->ptrs[slot].key,
-		     key, sizeof(struct btrfs_disk_key));
+	btrfs_set_node_key(lower, key, slot);
 	btrfs_set_node_blockptr(lower, slot, blocknr);
-	btrfs_set_header_nritems(&lower->header, nritems + 1);
-	btrfs_mark_buffer_dirty(path->nodes[level]);
+	btrfs_set_header_nritems(lower, nritems + 1);
+	btrfs_mark_buffer_dirty(lower);
 	check_node(root, path, level);
 	return 0;
 }
@@ -1199,69 +1272,73 @@ static int insert_ptr(struct btrfs_trans_handle *trans, struct btrfs_root
 static int split_node(struct btrfs_trans_handle *trans, struct btrfs_root
 		      *root, struct btrfs_path *path, int level)
 {
-	struct buffer_head *t;
-	struct btrfs_node *c;
-	struct buffer_head *split_buffer;
-	struct btrfs_node *split;
+	struct extent_buffer *c;
+	struct extent_buffer *split;
+	struct btrfs_disk_key disk_key;
 	int mid;
 	int ret;
 	int wret;
 	u32 c_nritems;
 
-	t = path->nodes[level];
-	c = btrfs_buffer_node(t);
-	if (t == root->node) {
+	c = path->nodes[level];
+	if (c == root->node) {
 		/* trying to split the root, lets make a new one */
 		ret = insert_new_root(trans, root, path, level + 1);
 		if (ret)
 			return ret;
 	} else {
 		ret = push_nodes_for_insert(trans, root, path, level);
-		t = path->nodes[level];
-		c = btrfs_buffer_node(t);
-		if (!ret &&
-		    btrfs_header_nritems(&c->header) <
+		c = path->nodes[level];
+		if (!ret && btrfs_header_nritems(c) <
 		    BTRFS_NODEPTRS_PER_BLOCK(root) - 1)
 			return 0;
 		if (ret < 0)
 			return ret;
 	}
 
-	c_nritems = btrfs_header_nritems(&c->header);
-	split_buffer = btrfs_alloc_free_block(trans, root, t->b_blocknr, 0);
-	if (IS_ERR(split_buffer))
-		return PTR_ERR(split_buffer);
+	c_nritems = btrfs_header_nritems(c);
+	split = btrfs_alloc_free_block(trans, root,
+				       extent_buffer_blocknr(c), 0);
+	if (IS_ERR(split))
+		return PTR_ERR(split);
+
+	btrfs_set_header_flags(split, btrfs_header_flags(c));
+	btrfs_set_header_level(split, btrfs_header_level(c));
+	btrfs_set_header_blocknr(split, extent_buffer_blocknr(split));
+	btrfs_set_header_generation(split, trans->transid);
+	btrfs_set_header_owner(split, root->root_key.objectid);
+	write_extent_buffer(split, root->fs_info->fsid,
+			    (unsigned long)btrfs_header_fsid(split),
+			    BTRFS_FSID_SIZE);
 
-	split = btrfs_buffer_node(split_buffer);
-	btrfs_set_header_flags(&split->header, btrfs_header_flags(&c->header));
-	btrfs_set_header_level(&split->header, btrfs_header_level(&c->header));
-	btrfs_set_header_blocknr(&split->header, bh_blocknr(split_buffer));
-	btrfs_set_header_generation(&split->header, trans->transid);
-	btrfs_set_header_owner(&split->header, root->root_key.objectid);
-	memcpy(split->header.fsid, root->fs_info->disk_super->fsid,
-	       sizeof(split->header.fsid));
 	mid = (c_nritems + 1) / 2;
-	btrfs_memcpy(root, split, split->ptrs, c->ptrs + mid,
-		     (c_nritems - mid) * sizeof(struct btrfs_key_ptr));
-	btrfs_set_header_nritems(&split->header, c_nritems - mid);
-	btrfs_set_header_nritems(&c->header, mid);
+
+	copy_extent_buffer(split, c,
+			   btrfs_node_key_ptr_offset(0),
+			   btrfs_node_key_ptr_offset(mid),
+			   (c_nritems - mid) * sizeof(struct btrfs_key_ptr));
+	btrfs_set_header_nritems(split, c_nritems - mid);
+	btrfs_set_header_nritems(c, mid);
 	ret = 0;
 
-	btrfs_mark_buffer_dirty(t);
-	btrfs_mark_buffer_dirty(split_buffer);
-	wret = insert_ptr(trans, root, path, &split->ptrs[0].key,
-			  bh_blocknr(split_buffer), path->slots[level + 1] + 1,
+	btrfs_mark_buffer_dirty(c);
+	btrfs_mark_buffer_dirty(split);
+
+	btrfs_node_key(split, &disk_key, 0);
+	wret = insert_ptr(trans, root, path, &disk_key,
+			  extent_buffer_blocknr(split),
+			  path->slots[level + 1] + 1,
 			  level + 1);
 	if (wret)
 		ret = wret;
 
 	if (path->slots[level] >= mid) {
 		path->slots[level] -= mid;
-		btrfs_block_release(root, t);
-		path->nodes[level] = split_buffer;
+		free_extent_buffer(c);
+		path->nodes[level] = split;
 		path->slots[level + 1] += 1;
 	} else {
-		btrfs_block_release(root, split_buffer);
+		free_extent_buffer(split);
 	}
 	return ret;
 }
@@ -1271,16 +1348,16 @@ static int split_node(struct btrfs_trans_handle *trans, struct btrfs_root
  * and nr indicate which items in the leaf to check.  This totals up the
  * space used both by the item structs and the item data
  */
-static int leaf_space_used(struct btrfs_leaf *l, int start, int nr)
+static int leaf_space_used(struct extent_buffer *l, int start, int nr)
 {
 	int data_len;
-	int nritems = btrfs_header_nritems(&l->header);
+	int nritems = btrfs_header_nritems(l);
 	int end = min(nritems, start + nr) - 1;
 
 	if (!nr)
 		return 0;
-	data_len = btrfs_item_end(l->items + start);
-	data_len = data_len - btrfs_item_offset(l->items + end);
+	data_len = btrfs_item_end_nr(l, start);
+	data_len = data_len - btrfs_item_offset_nr(l, end);
 	data_len += sizeof(struct btrfs_item) * nr;
 	WARN_ON(data_len < 0);
 	return data_len;
@@ -1291,10 +1368,17 @@ static int leaf_space_used(struct btrfs_leaf *l, int start, int nr)
  * the start of the leaf data.  IOW, how much room
  * the leaf has left for both items and data
  */
-int btrfs_leaf_free_space(struct btrfs_root *root, struct btrfs_leaf *leaf)
+int btrfs_leaf_free_space(struct btrfs_root *root, struct extent_buffer *leaf)
 {
-	int nritems = btrfs_header_nritems(&leaf->header);
-	return BTRFS_LEAF_DATA_SIZE(root) - leaf_space_used(leaf, 0, nritems);
+	int nritems = btrfs_header_nritems(leaf);
+	int ret;
+	ret = BTRFS_LEAF_DATA_SIZE(root) - leaf_space_used(leaf, 0, nritems);
+	if (ret < 0) {
+		printk("leaf free space ret %d, leaf data size %lu, used %d nritems %d\n",
+		       ret, BTRFS_LEAF_DATA_SIZE(root),
+		       leaf_space_used(leaf, 0, nritems), nritems);
+	}
+	return ret;
 }
 
 /*
@@ -1307,12 +1391,10 @@ int btrfs_leaf_free_space(struct btrfs_root *root, struct btrfs_leaf *leaf)
 static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 			   *root, struct btrfs_path *path, int data_size)
 {
-	struct buffer_head *left_buf = path->nodes[0];
-	struct btrfs_leaf *left = btrfs_buffer_leaf(left_buf);
-	struct btrfs_leaf *right;
-	struct buffer_head *right_buf;
-	struct buffer_head *upper;
-	struct btrfs_node *upper_node;
+	struct extent_buffer *left = path->nodes[0];
+	struct extent_buffer *right;
+	struct extent_buffer *upper;
+	struct btrfs_disk_key disk_key;
 	int slot;
 	int i;
 	int free_space;
@@ -1321,6 +1403,7 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 	struct btrfs_item *item;
 	u32 left_nritems;
 	u32 right_nritems;
+	u32 data_end;
 	int ret;
 
 	slot = path->slots[1];
@@ -1328,102 +1411,109 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 		return 1;
 	}
 	upper = path->nodes[1];
-	upper_node = btrfs_buffer_node(upper);
-	if (slot >= btrfs_header_nritems(&upper_node->header) - 1) {
+	if (slot >= btrfs_header_nritems(upper) - 1)
 		return 1;
-	}
-	right_buf = read_tree_block(root,
-		    btrfs_node_blockptr(btrfs_buffer_node(upper), slot + 1));
-	right = btrfs_buffer_leaf(right_buf);
+
+	right = read_tree_block(root, btrfs_node_blockptr(upper, slot + 1));
 	free_space = btrfs_leaf_free_space(root, right);
 	if (free_space < data_size + sizeof(struct btrfs_item)) {
-		btrfs_block_release(root, right_buf);
+		free_extent_buffer(right);
 		return 1;
 	}
+
 	/* cow and double check */
-	ret = btrfs_cow_block(trans, root, right_buf, upper,
-			      slot + 1, &right_buf);
+	ret = btrfs_cow_block(trans, root, right, upper,
+			      slot + 1, &right);
 	if (ret) {
-		btrfs_block_release(root, right_buf);
+		free_extent_buffer(right);
 		return 1;
 	}
-	right = btrfs_buffer_leaf(right_buf);
 	free_space = btrfs_leaf_free_space(root, right);
 	if (free_space < data_size + sizeof(struct btrfs_item)) {
-		btrfs_block_release(root, right_buf);
+		free_extent_buffer(right);
 		return 1;
 	}
 
-	left_nritems = btrfs_header_nritems(&left->header);
+	left_nritems = btrfs_header_nritems(left);
 	if (left_nritems == 0) {
-		btrfs_block_release(root, right_buf);
+		free_extent_buffer(right);
 		return 1;
 	}
+
 	for (i = left_nritems - 1; i >= 1; i--) {
-		item = left->items + i;
+		item = btrfs_item_nr(left, i);
 		if (path->slots[0] == i)
 			push_space += data_size + sizeof(*item);
-		if (btrfs_item_size(item) + sizeof(*item) + push_space >
+		if (btrfs_item_size(left, item) + sizeof(*item) + push_space >
 		    free_space)
 			break;
 		push_items++;
-		push_space += btrfs_item_size(item) + sizeof(*item);
+		push_space += btrfs_item_size(left, item) + sizeof(*item);
 	}
+
 	if (push_items == 0) {
-		btrfs_block_release(root, right_buf);
+		free_extent_buffer(right);
 		return 1;
 	}
+
 	if (push_items == left_nritems)
 		WARN_ON(1);
-	right_nritems = btrfs_header_nritems(&right->header);
+
 	/* push left to right */
-	push_space = btrfs_item_end(left->items + left_nritems - push_items);
+	right_nritems = btrfs_header_nritems(right);
+	push_space = btrfs_item_end_nr(left, left_nritems - push_items);
 	push_space -= leaf_data_end(root, left);
+
 	/* make room in the right data area */
-	btrfs_memmove(root, right, btrfs_leaf_data(right) +
-		      leaf_data_end(root, right) - push_space,
-		      btrfs_leaf_data(right) +
-		      leaf_data_end(root, right), BTRFS_LEAF_DATA_SIZE(root) -
-		      leaf_data_end(root, right));
+	data_end = leaf_data_end(root, right);
+	memmove_extent_buffer(right,
+			      btrfs_leaf_data(right) + data_end - push_space,
+			      btrfs_leaf_data(right) + data_end,
+			      BTRFS_LEAF_DATA_SIZE(root) - data_end);
+
 	/* copy from the left data area */
-	btrfs_memcpy(root, right, btrfs_leaf_data(right) +
+	copy_extent_buffer(right, left, btrfs_leaf_data(right) +
 		     BTRFS_LEAF_DATA_SIZE(root) - push_space,
 		     btrfs_leaf_data(left) + leaf_data_end(root, left),
 		     push_space);
-	btrfs_memmove(root, right, right->items + push_items, right->items,
-		right_nritems * sizeof(struct btrfs_item));
+
+	memmove_extent_buffer(right, btrfs_item_nr_offset(push_items),
+			      btrfs_item_nr_offset(0),
+			      right_nritems * sizeof(struct btrfs_item));
+
 	/* copy the items from left to right */
-	btrfs_memcpy(root, right, right->items, left->items +
-		     left_nritems - push_items,
-		     push_items * sizeof(struct btrfs_item));
+	copy_extent_buffer(right, left, btrfs_item_nr_offset(0),
+		   btrfs_item_nr_offset(left_nritems - push_items),
+		   push_items * sizeof(struct btrfs_item));
 
 	/* update the item pointers */
 	right_nritems += push_items;
-	btrfs_set_header_nritems(&right->header, right_nritems);
+	btrfs_set_header_nritems(right, right_nritems);
 	push_space = BTRFS_LEAF_DATA_SIZE(root);
 	for (i = 0; i < right_nritems; i++) {
-		btrfs_set_item_offset(right->items + i, push_space -
-				      btrfs_item_size(right->items + i));
-		push_space = btrfs_item_offset(right->items + i);
+		item = btrfs_item_nr(right, i);
+		btrfs_set_item_offset(right, item, push_space -
+				      btrfs_item_size(right, item));
+		push_space = btrfs_item_offset(right, item);
 	}
 	left_nritems -= push_items;
-	btrfs_set_header_nritems(&left->header, left_nritems);
+	btrfs_set_header_nritems(left, left_nritems);
 
-	btrfs_mark_buffer_dirty(left_buf);
-	btrfs_mark_buffer_dirty(right_buf);
+	btrfs_mark_buffer_dirty(left);
+	btrfs_mark_buffer_dirty(right);
 
-	btrfs_memcpy(root, upper_node, &upper_node->ptrs[slot + 1].key,
-		&right->items[0].key, sizeof(struct btrfs_disk_key));
+	btrfs_item_key(right, &disk_key, 0);
+	btrfs_set_node_key(upper, &disk_key, slot + 1);
 	btrfs_mark_buffer_dirty(upper);
 
 	/* then fixup the leaf pointer in the path */
 	if (path->slots[0] >= left_nritems) {
 		path->slots[0] -= left_nritems;
-		btrfs_block_release(root, path->nodes[0]);
-		path->nodes[0] = right_buf;
+		free_extent_buffer(path->nodes[0]);
+		path->nodes[0] = right;
 		path->slots[1] += 1;
 	} else {
-		btrfs_block_release(root, right_buf);
+		free_extent_buffer(right);
 	}
 	if (path->nodes[1])
 		check_node(root, path, 1);
@@ -1436,10 +1526,9 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 			  *root, struct btrfs_path *path, int data_size)
 {
-	struct buffer_head *right_buf = path->nodes[0];
-	struct btrfs_leaf *right = btrfs_buffer_leaf(right_buf);
-	struct buffer_head *t;
-	struct btrfs_leaf *left;
+	struct btrfs_disk_key disk_key;
+	struct extent_buffer *right = path->nodes[0];
+	struct extent_buffer *left;
 	int slot;
 	int i;
 	int free_space;
@@ -1447,119 +1536,128 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 	int push_items = 0;
 	struct btrfs_item *item;
 	u32 old_left_nritems;
+	u32 right_nritems;
 	int ret = 0;
 	int wret;
 
 	slot = path->slots[1];
-	if (slot == 0) {
+	if (slot == 0)
 		return 1;
-	}
-	if (!path->nodes[1]) {
+	if (!path->nodes[1])
 		return 1;
-	}
-	t = read_tree_block(root,
-	    btrfs_node_blockptr(btrfs_buffer_node(path->nodes[1]), slot - 1));
-	left = btrfs_buffer_leaf(t);
+
+	left = read_tree_block(root, btrfs_node_blockptr(path->nodes[1],
+							 slot - 1));
 	free_space = btrfs_leaf_free_space(root, left);
 	if (free_space < data_size + sizeof(struct btrfs_item)) {
-		btrfs_block_release(root, t);
+		free_extent_buffer(left);
 		return 1;
 	}
 
 	/* cow and double check */
-	ret = btrfs_cow_block(trans, root, t, path->nodes[1], slot - 1, &t);
+	ret = btrfs_cow_block(trans, root, left,
+			      path->nodes[1], slot - 1, &left);
 	if (ret) {
 		/* we hit -ENOSPC, but it isn't fatal here */
-		btrfs_block_release(root, t);
+		free_extent_buffer(left);
 		return 1;
 	}
-	left = btrfs_buffer_leaf(t);
 	free_space = btrfs_leaf_free_space(root, left);
 	if (free_space < data_size + sizeof(struct btrfs_item)) {
-		btrfs_block_release(root, t);
+		free_extent_buffer(left);
 		return 1;
 	}
 
-	if (btrfs_header_nritems(&right->header) == 0) {
-		btrfs_block_release(root, t);
+	right_nritems = btrfs_header_nritems(right);
+	if (right_nritems == 0) {
+		free_extent_buffer(left);
 		return 1;
 	}
 
-	for (i = 0; i < btrfs_header_nritems(&right->header) - 1; i++) {
-		item = right->items + i;
+	for (i = 0; i < right_nritems - 1; i++) {
+		item = btrfs_item_nr(right, i);
 		if (path->slots[0] == i)
 			push_space += data_size + sizeof(*item);
-		if (btrfs_item_size(item) + sizeof(*item) + push_space >
+		if (btrfs_item_size(right, item) + sizeof(*item) + push_space >
 		    free_space)
 			break;
 		push_items++;
-		push_space += btrfs_item_size(item) + sizeof(*item);
+		push_space += btrfs_item_size(right, item) + sizeof(*item);
 	}
 	if (push_items == 0) {
-		btrfs_block_release(root, t);
+		free_extent_buffer(left);
 		return 1;
 	}
-	if (push_items == btrfs_header_nritems(&right->header))
+	if (push_items == btrfs_header_nritems(right))
 		WARN_ON(1);
+
 	/* push data from right to left */
-	btrfs_memcpy(root, left, left->items +
-		     btrfs_header_nritems(&left->header),
-		     right->items, push_items * sizeof(struct btrfs_item));
+	copy_extent_buffer(left, right,
+			   btrfs_item_nr_offset(btrfs_header_nritems(left)),
+			   btrfs_item_nr_offset(0),
+			   push_items * sizeof(struct btrfs_item));
+
 	push_space = BTRFS_LEAF_DATA_SIZE(root) -
-		     btrfs_item_offset(right->items + push_items -1);
-	btrfs_memcpy(root, left, btrfs_leaf_data(left) +
+		     btrfs_item_offset_nr(right, push_items -1);
+
+	copy_extent_buffer(left, right, btrfs_leaf_data(left) +
 		     leaf_data_end(root, left) - push_space,
 		     btrfs_leaf_data(right) +
-		     btrfs_item_offset(right->items + push_items - 1),
+		     btrfs_item_offset_nr(right, push_items - 1),
 		     push_space);
-	old_left_nritems = btrfs_header_nritems(&left->header);
+	old_left_nritems = btrfs_header_nritems(left);
 	BUG_ON(old_left_nritems < 0);
 
 	for (i = old_left_nritems; i < old_left_nritems + push_items; i++) {
-		u32 ioff = btrfs_item_offset(left->items + i);
-		btrfs_set_item_offset(left->items + i, ioff -
-				     (BTRFS_LEAF_DATA_SIZE(root) -
-				      btrfs_item_offset(left->items +
-						        old_left_nritems - 1)));
+		u32 ioff;
+		item = btrfs_item_nr(left, i);
+		ioff = btrfs_item_offset(left, item);
+		btrfs_set_item_offset(left, item,
+		      ioff - (BTRFS_LEAF_DATA_SIZE(root) -
+		      btrfs_item_offset_nr(left, old_left_nritems - 1)));
 	}
-	btrfs_set_header_nritems(&left->header, old_left_nritems + push_items);
+	btrfs_set_header_nritems(left, old_left_nritems + push_items);
 
 	/* fixup right node */
-	push_space = btrfs_item_offset(right->items + push_items - 1) -
-		     leaf_data_end(root, right);
-	btrfs_memmove(root, right, btrfs_leaf_data(right) +
-		      BTRFS_LEAF_DATA_SIZE(root) - push_space,
-		      btrfs_leaf_data(right) +
-		      leaf_data_end(root, right), push_space);
-	btrfs_memmove(root, right, right->items, right->items + push_items,
-		(btrfs_header_nritems(&right->header) - push_items) *
-		sizeof(struct btrfs_item));
-	btrfs_set_header_nritems(&right->header,
-				 btrfs_header_nritems(&right->header) -
-				 push_items);
+	push_space = btrfs_item_offset_nr(right, push_items - 1) -
+					  leaf_data_end(root, right);
+	memmove_extent_buffer(right, btrfs_leaf_data(right) +
+			      BTRFS_LEAF_DATA_SIZE(root) - push_space,
+			      btrfs_leaf_data(right) +
+			      leaf_data_end(root, right), push_space);
+
+	memmove_extent_buffer(right, btrfs_item_nr_offset(0),
+			      btrfs_item_nr_offset(push_items),
+			     (btrfs_header_nritems(right) - push_items) *
+			     sizeof(struct btrfs_item));
+
+	right_nritems = btrfs_header_nritems(right) - push_items;
+	btrfs_set_header_nritems(right, right_nritems);
 	push_space = BTRFS_LEAF_DATA_SIZE(root);
 
-	for (i = 0; i < btrfs_header_nritems(&right->header); i++) {
-		btrfs_set_item_offset(right->items + i, push_space -
-				      btrfs_item_size(right->items + i));
-		push_space = btrfs_item_offset(right->items + i);
+	for (i = 0; i < right_nritems; i++) {
+		item = btrfs_item_nr(right, i);
+		btrfs_set_item_offset(right, item, push_space -
+				      btrfs_item_size(right, item));
+		push_space = btrfs_item_offset(right, item);
 	}
 
-	btrfs_mark_buffer_dirty(t);
-	btrfs_mark_buffer_dirty(right_buf);
+	btrfs_mark_buffer_dirty(left);
+	btrfs_mark_buffer_dirty(right);
 
-	wret = fixup_low_keys(trans, root, path, &right->items[0].key, 1);
+	btrfs_item_key(right, &disk_key, 0);
+	wret = fixup_low_keys(trans, root, path, &disk_key, 1);
 	if (wret)
 		ret = wret;
 
 	/* then fixup the leaf pointer in the path */
 	if (path->slots[0] < push_items) {
 		path->slots[0] += old_left_nritems;
-		btrfs_block_release(root, path->nodes[0]);
-		path->nodes[0] = t;
+		free_extent_buffer(path->nodes[0]);
+		path->nodes[0] = left;
 		path->slots[1] -= 1;
 	} else {
-		btrfs_block_release(root, t);
+		free_extent_buffer(left);
 		path->slots[0] -= push_items;
 	}
 	BUG_ON(path->slots[0] < 0);
@@ -1578,13 +1676,11 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 		      *root, struct btrfs_key *ins_key,
 		      struct btrfs_path *path, int data_size)
 {
-	struct buffer_head *l_buf;
-	struct btrfs_leaf *l;
+	struct extent_buffer *l;
 	u32 nritems;
 	int mid;
 	int slot;
-	struct btrfs_leaf *right;
-	struct buffer_head *right_buffer;
+	struct extent_buffer *right;
 	int space_needed = data_size + sizeof(struct btrfs_item);
 	int data_copy_size;
 	int rt_data_off;
@@ -1603,8 +1699,7 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 		if (wret < 0)
 			return wret;
 	}
-	l_buf = path->nodes[0];
-	l = btrfs_buffer_leaf(l_buf);
+	l = path->nodes[0];
 
 	/* did the pushes work? */
 	if (btrfs_leaf_free_space(root, l) >=
@@ -1617,36 +1712,38 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 			return ret;
 	}
 	slot = path->slots[0];
-	nritems = btrfs_header_nritems(&l->header);
+	nritems = btrfs_header_nritems(l);
 	mid = (nritems + 1)/ 2;
 
-	right_buffer = btrfs_alloc_free_block(trans, root, l_buf->b_blocknr, 0);
-	if (IS_ERR(right_buffer))
-		return PTR_ERR(right_buffer);
-
-	right = btrfs_buffer_leaf(right_buffer);
-	memset(&right->header, 0, sizeof(right->header));
-	btrfs_set_header_blocknr(&right->header, bh_blocknr(right_buffer));
-	btrfs_set_header_generation(&right->header, trans->transid);
-	btrfs_set_header_owner(&right->header, root->root_key.objectid);
-	btrfs_set_header_level(&right->header, 0);
-	memcpy(right->header.fsid, root->fs_info->disk_super->fsid,
-	       sizeof(right->header.fsid));
+	right = btrfs_alloc_free_block(trans, root,
+					      extent_buffer_blocknr(l), 0);
+	if (IS_ERR(right))
+		return PTR_ERR(right);
+
+	memset_extent_buffer(right, 0, 0, sizeof(struct btrfs_header));
+	btrfs_set_header_blocknr(right, extent_buffer_blocknr(right));
+	btrfs_set_header_generation(right, trans->transid);
+	btrfs_set_header_owner(right, root->root_key.objectid);
+	btrfs_set_header_level(right, 0);
+	write_extent_buffer(right, root->fs_info->fsid,
+			    (unsigned long)btrfs_header_fsid(right),
+			    BTRFS_FSID_SIZE);
+
 	if (mid <= slot) {
 		if (nritems == 1 ||
 		    leaf_space_used(l, mid, nritems - mid) + space_needed >
 			BTRFS_LEAF_DATA_SIZE(root)) {
 			if (slot >= nritems) {
 				btrfs_cpu_key_to_disk(&disk_key, ins_key);
-				btrfs_set_header_nritems(&right->header, 0);
+				btrfs_set_header_nritems(right, 0);
 				wret = insert_ptr(trans, root, path,
 						  &disk_key,
-						  bh_blocknr(right_buffer),
+						  extent_buffer_blocknr(right),
 						  path->slots[1] + 1, 1);
 				if (wret)
 					ret = wret;
-				btrfs_block_release(root, path->nodes[0]);
-				path->nodes[0] = right_buffer;
+				free_extent_buffer(path->nodes[0]);
+				path->nodes[0] = right;
 				path->slots[0] = 0;
 				path->slots[1] += 1;
 				return ret;
@@ -1659,15 +1756,15 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 			BTRFS_LEAF_DATA_SIZE(root)) {
 			if (slot == 0) {
 				btrfs_cpu_key_to_disk(&disk_key, ins_key);
-				btrfs_set_header_nritems(&right->header, 0);
+				btrfs_set_header_nritems(right, 0);
 				wret = insert_ptr(trans, root, path,
 						  &disk_key,
-						  bh_blocknr(right_buffer),
+						  extent_buffer_blocknr(right),
 						  path->slots[1], 1);
 				if (wret)
 					ret = wret;
-				btrfs_block_release(root, path->nodes[0]);
-				path->nodes[0] = right_buffer;
+				free_extent_buffer(path->nodes[0]);
+				path->nodes[0] = right;
 				path->slots[0] = 0;
 				if (path->slots[1] == 0) {
 					wret = fixup_low_keys(trans, root,
@@ -1681,61 +1778,74 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 			double_split = 1;
 		}
 	}
-	btrfs_set_header_nritems(&right->header, nritems - mid);
-	data_copy_size = btrfs_item_end(l->items + mid) -
-			 leaf_data_end(root, l);
-	btrfs_memcpy(root, right, right->items, l->items + mid,
-		     (nritems - mid) * sizeof(struct btrfs_item));
-	btrfs_memcpy(root, right,
+	nritems = nritems - mid;
+	btrfs_set_header_nritems(right, nritems);
+	data_copy_size = btrfs_item_end_nr(l, mid) - leaf_data_end(root, l);
+
+	copy_extent_buffer(right, l, btrfs_item_nr_offset(0),
+			   btrfs_item_nr_offset(mid),
+			   nritems * sizeof(struct btrfs_item));
+
+	copy_extent_buffer(right, l,
 		     btrfs_leaf_data(right) + BTRFS_LEAF_DATA_SIZE(root) -
 		     data_copy_size, btrfs_leaf_data(l) +
 		     leaf_data_end(root, l), data_copy_size);
+
 	rt_data_off = BTRFS_LEAF_DATA_SIZE(root) -
-		      btrfs_item_end(l->items + mid);
+		      btrfs_item_end_nr(l, mid);
 
-	for (i = 0; i < btrfs_header_nritems(&right->header); i++) {
-		u32 ioff = btrfs_item_offset(right->items + i);
-		btrfs_set_item_offset(right->items + i, ioff + rt_data_off);
+	for (i = 0; i < nritems; i++) {
+		struct btrfs_item *item = btrfs_item_nr(right, i);
+		u32 ioff = btrfs_item_offset(right, item);
+		btrfs_set_item_offset(right, item, ioff + rt_data_off);
 	}
 
-	btrfs_set_header_nritems(&l->header, mid);
+	btrfs_set_header_nritems(l, mid);
 	ret = 0;
-	wret = insert_ptr(trans, root, path, &right->items[0].key,
-			  bh_blocknr(right_buffer), path->slots[1] + 1, 1);
+	btrfs_item_key(right, &disk_key, 0);
+	wret = insert_ptr(trans, root, path, &disk_key,
+			  extent_buffer_blocknr(right), path->slots[1] + 1, 1);
 	if (wret)
 		ret = wret;
-	btrfs_mark_buffer_dirty(right_buffer);
-	btrfs_mark_buffer_dirty(l_buf);
+
+	btrfs_mark_buffer_dirty(right);
+	btrfs_mark_buffer_dirty(l);
 	BUG_ON(path->slots[0] != slot);
+
 	if (mid <= slot) {
-		btrfs_block_release(root, path->nodes[0]);
-		path->nodes[0] = right_buffer;
+		free_extent_buffer(path->nodes[0]);
+		path->nodes[0] = right;
 		path->slots[0] -= mid;
 		path->slots[1] += 1;
 	} else
-		btrfs_block_release(root, right_buffer);
+		free_extent_buffer(right);
+
 	BUG_ON(path->slots[0] < 0);
 	check_node(root, path, 1);
+	check_leaf(root, path, 0);
 
 	if (!double_split)
 		return ret;
-	right_buffer = btrfs_alloc_free_block(trans, root, l_buf->b_blocknr, 0);
-	if (IS_ERR(right_buffer))
-		return PTR_ERR(right_buffer);
-
-	right = btrfs_buffer_leaf(right_buffer);
-	memset(&right->header, 0, sizeof(right->header));
-	btrfs_set_header_blocknr(&right->header, bh_blocknr(right_buffer));
-	btrfs_set_header_generation(&right->header, trans->transid);
-	btrfs_set_header_owner(&right->header, root->root_key.objectid);
-	btrfs_set_header_level(&right->header, 0);
-	memcpy(right->header.fsid, root->fs_info->disk_super->fsid,
-	       sizeof(right->header.fsid));
+
+	right = btrfs_alloc_free_block(trans, root,
+				       extent_buffer_blocknr(l), 0);
+	if (IS_ERR(right))
+		return PTR_ERR(right);
+
+	memset_extent_buffer(right, 0, 0, sizeof(struct btrfs_header));
+	btrfs_set_header_blocknr(right, extent_buffer_blocknr(right));
+	btrfs_set_header_generation(right, trans->transid);
+	btrfs_set_header_owner(right, root->root_key.objectid);
+	btrfs_set_header_level(right, 0);
+	write_extent_buffer(right, root->fs_info->fsid,
+			    (unsigned long)btrfs_header_fsid(right),
+			    BTRFS_FSID_SIZE);
+
 	btrfs_cpu_key_to_disk(&disk_key, ins_key);
-	btrfs_set_header_nritems(&right->header, 0);
+	btrfs_set_header_nritems(right, 0);
 	wret = insert_ptr(trans, root, path,
 			  &disk_key,
-			  bh_blocknr(right_buffer),
+			  extent_buffer_blocknr(right),
 			  path->slots[1], 1);
 	if (wret)
 		ret = wret;
@@ -1744,8 +1854,8 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 		if (wret)
 			ret = wret;
 	}
-	btrfs_block_release(root, path->nodes[0]);
-	path->nodes[0] = right_buffer;
+	free_extent_buffer(path->nodes[0]);
+	path->nodes[0] = right;
 	path->slots[0] = 0;
 	check_node(root, path, 1);
 	check_leaf(root, path, 0);
@@ -1760,8 +1870,8 @@ int btrfs_truncate_item(struct btrfs_trans_handle *trans,
 	int ret = 0;
 	int slot;
 	int slot_orig;
-	struct btrfs_leaf *leaf;
-	struct buffer_head *leaf_buf;
+	struct extent_buffer *leaf;
+	struct btrfs_item *item;
 	u32 nritems;
 	unsigned int data_end;
 	unsigned int old_data_start;
@@ -1770,15 +1880,14 @@ int btrfs_truncate_item(struct btrfs_trans_handle *trans,
 	int i;
 
 	slot_orig = path->slots[0];
-	leaf_buf = path->nodes[0];
-	leaf = btrfs_buffer_leaf(leaf_buf);
+	leaf = path->nodes[0];
 
-	nritems = btrfs_header_nritems(&leaf->header);
+	nritems = btrfs_header_nritems(leaf);
 	data_end = leaf_data_end(root, leaf);
 
 	slot = path->slots[0];
-	old_data_start = btrfs_item_offset(leaf->items + slot);
-	old_size = btrfs_item_size(leaf->items + slot);
+	old_data_start = btrfs_item_offset_nr(leaf, slot);
+	old_size = btrfs_item_size_nr(leaf, slot);
 	BUG_ON(old_size <= new_size);
 	size_diff = old_size - new_size;
 
@@ -1790,32 +1899,38 @@ int btrfs_truncate_item(struct btrfs_trans_handle *trans,
 	 */
 	/* first correct the data pointers */
 	for (i = slot; i < nritems; i++) {
-		u32 ioff = btrfs_item_offset(leaf->items + i);
-		btrfs_set_item_offset(leaf->items + i,
-				      ioff + size_diff);
+		u32 ioff;
+		item = btrfs_item_nr(leaf, i);
+		ioff = btrfs_item_offset(leaf, item);
+		btrfs_set_item_offset(leaf, item, ioff + size_diff);
 	}
 	/* shift the data */
-	btrfs_memmove(root, leaf, btrfs_leaf_data(leaf) +
+	memmove_extent_buffer(leaf, btrfs_leaf_data(leaf) +
 		      data_end + size_diff, btrfs_leaf_data(leaf) +
 		      data_end, old_data_start + new_size - data_end);
-	btrfs_set_item_size(leaf->items + slot, new_size);
-	btrfs_mark_buffer_dirty(leaf_buf);
+
+	item = btrfs_item_nr(leaf, slot);
+	btrfs_set_item_size(leaf, item, new_size);
+	btrfs_mark_buffer_dirty(leaf);
 
 	ret = 0;
-	if (btrfs_leaf_free_space(root, leaf) < 0)
+	if (btrfs_leaf_free_space(root, leaf) < 0) {
+		btrfs_print_leaf(root, leaf);
 		BUG();
+	}
 	check_leaf(root, path, 0);
 	return ret;
 }
 
-int btrfs_extend_item(struct btrfs_trans_handle *trans, struct btrfs_root
-		      *root, struct btrfs_path *path, u32 data_size)
+int btrfs_extend_item(struct btrfs_trans_handle *trans,
+		      struct btrfs_root *root, struct btrfs_path *path,
+		      u32 data_size)
 {
 	int ret = 0;
 	int slot;
 	int slot_orig;
-	struct btrfs_leaf *leaf;
-	struct buffer_head *leaf_buf;
+	struct extent_buffer *leaf;
+	struct btrfs_item *item;
 	u32 nritems;
 	unsigned int data_end;
 	unsigned int old_data;
@@ -1823,16 +1938,17 @@ int btrfs_extend_item(struct btrfs_trans_handle *trans, struct btrfs_root
 	int i;
 
 	slot_orig = path->slots[0];
-	leaf_buf = path->nodes[0];
-	leaf = btrfs_buffer_leaf(leaf_buf);
+	leaf = path->nodes[0];
 
-	nritems = btrfs_header_nritems(&leaf->header);
+	nritems = btrfs_header_nritems(leaf);
 	data_end = leaf_data_end(root, leaf);
 
-	if (btrfs_leaf_free_space(root, leaf) < data_size)
+	if (btrfs_leaf_free_space(root, leaf) < data_size) {
+		btrfs_print_leaf(root, leaf);
 		BUG();
+	}
 	slot = path->slots[0];
-	old_data = btrfs_item_end(leaf->items + slot);
+	old_data = btrfs_item_end_nr(leaf, slot);
 
 	BUG_ON(slot < 0);
 	BUG_ON(slot >= nritems);
@@ -1842,22 +1958,28 @@ int btrfs_extend_item(struct btrfs_trans_handle *trans, struct btrfs_root
 	 */
 	/* first correct the data pointers */
 	for (i = slot; i < nritems; i++) {
-		u32 ioff = btrfs_item_offset(leaf->items + i);
-		btrfs_set_item_offset(leaf->items + i,
-				      ioff - data_size);
+		u32 ioff;
+		item = btrfs_item_nr(leaf, i);
+		ioff = btrfs_item_offset(leaf, item);
+		btrfs_set_item_offset(leaf, item, ioff - data_size);
 	}
+
 	/* shift the data */
-	btrfs_memmove(root, leaf, btrfs_leaf_data(leaf) +
+	memmove_extent_buffer(leaf, btrfs_leaf_data(leaf) +
 		      data_end - data_size, btrfs_leaf_data(leaf) +
 		      data_end, old_data - data_end);
+
 	data_end = old_data;
-	old_size = btrfs_item_size(leaf->items + slot);
-	btrfs_set_item_size(leaf->items + slot, old_size + data_size);
-	btrfs_mark_buffer_dirty(leaf_buf);
+	old_size = btrfs_item_size_nr(leaf, slot);
+	item = btrfs_item_nr(leaf, slot);
+	btrfs_set_item_size(leaf, item, old_size + data_size);
+	btrfs_mark_buffer_dirty(leaf);
 
 	ret = 0;
-	if (btrfs_leaf_free_space(root, leaf) < 0)
+	if (btrfs_leaf_free_space(root, leaf) < 0) {
+		btrfs_print_leaf(root, leaf);
 		BUG();
+	}
 	check_leaf(root, path, 0);
 	return ret;
 }
@@ -1866,15 +1988,16 @@ int btrfs_extend_item(struct btrfs_trans_handle *trans, struct btrfs_root
  * Given a key and some data, insert an item into the tree.
  * This does all the path init required, making room in the tree if needed.
  */
-int btrfs_insert_empty_item(struct btrfs_trans_handle *trans, struct btrfs_root
-			    *root, struct btrfs_path *path, struct btrfs_key
-			    *cpu_key, u32 data_size)
+int btrfs_insert_empty_item(struct btrfs_trans_handle *trans,
+			    struct btrfs_root *root,
+			    struct btrfs_path *path,
+			    struct btrfs_key *cpu_key, u32 data_size)
 {
+	struct extent_buffer *leaf;
+	struct btrfs_item *item;
 	int ret = 0;
 	int slot;
 	int slot_orig;
-	struct btrfs_leaf *leaf;
-	struct buffer_head *leaf_buf;
 	u32 nritems;
 	unsigned int data_end;
 	struct btrfs_disk_key disk_key;
@@ -1884,6 +2007,7 @@ int btrfs_insert_empty_item(struct btrfs_trans_handle *trans, struct btrfs_root
 	/* create a root if there isn't one */
 	if (!root->node)
 		BUG();
+
 	ret = btrfs_search_slot(trans, root, cpu_key, path, data_size, 1);
 	if (ret == 0) {
 		return -EEXIST;
@@ -1892,57 +2016,68 @@ int btrfs_insert_empty_item(struct btrfs_trans_handle *trans, struct btrfs_root
 		goto out;
 
 	slot_orig = path->slots[0];
-	leaf_buf = path->nodes[0];
-	leaf = btrfs_buffer_leaf(leaf_buf);
+	leaf = path->nodes[0];
 
-	nritems = btrfs_header_nritems(&leaf->header);
+	nritems = btrfs_header_nritems(leaf);
 	data_end = leaf_data_end(root, leaf);
 
 	if (btrfs_leaf_free_space(root, leaf) <
 	    sizeof(struct btrfs_item) + data_size) {
 		BUG();
 	}
+
 	slot = path->slots[0];
 	BUG_ON(slot < 0);
+
 	if (slot != nritems) {
 		int i;
-		unsigned int old_data = btrfs_item_end(leaf->items + slot);
+		unsigned int old_data = btrfs_item_end_nr(leaf, slot);
 
+		if (old_data < data_end) {
+			btrfs_print_leaf(root, leaf);
+			printk("slot %d old_data %d data_end %d\n",
+			       slot, old_data, data_end);
+			BUG_ON(1);
+		}
 		/*
 		 * item0..itemN ... dataN.offset..dataN.size .. data0.size
 		 */
 		/* first correct the data pointers */
 		for (i = slot; i < nritems; i++) {
-			u32 ioff = btrfs_item_offset(leaf->items + i);
-			btrfs_set_item_offset(leaf->items + i,
-					      ioff - data_size);
+			u32 ioff;
+			item = btrfs_item_nr(leaf, i);
+			ioff = btrfs_item_offset(leaf, item);
+			btrfs_set_item_offset(leaf, item, ioff - data_size);
 		}
 
 		/* shift the items */
-		btrfs_memmove(root, leaf, leaf->items + slot + 1,
-			      leaf->items + slot,
+		memmove_extent_buffer(leaf, btrfs_item_nr_offset(slot + 1),
+			      btrfs_item_nr_offset(slot),
 			      (nritems - slot) * sizeof(struct btrfs_item));
 
 		/* shift the data */
-		btrfs_memmove(root, leaf, btrfs_leaf_data(leaf) +
+		memmove_extent_buffer(leaf, btrfs_leaf_data(leaf) +
 			      data_end - data_size, btrfs_leaf_data(leaf) +
 			      data_end, old_data - data_end);
 		data_end = old_data;
 	}
+
 	/* setup the item for the new data */
-	btrfs_memcpy(root, leaf, &leaf->items[slot].key, &disk_key,
-		     sizeof(struct btrfs_disk_key));
-	btrfs_set_item_offset(leaf->items + slot, data_end - data_size);
-	btrfs_set_item_size(leaf->items + slot, data_size);
-	btrfs_set_header_nritems(&leaf->header, nritems + 1);
-	btrfs_mark_buffer_dirty(leaf_buf);
+	btrfs_set_item_key(leaf, &disk_key, slot);
+	item = btrfs_item_nr(leaf, slot);
+	btrfs_set_item_offset(leaf, item, data_end - data_size);
+	btrfs_set_item_size(leaf, item, data_size);
+	btrfs_set_header_nritems(leaf, nritems + 1);
+	btrfs_mark_buffer_dirty(leaf);
 
 	ret = 0;
 	if (slot == 0)
 		ret = fixup_low_keys(trans, root, path, &disk_key, 1);
 
-	if (btrfs_leaf_free_space(root, leaf) < 0)
+	if (btrfs_leaf_free_space(root, leaf) < 0) {
+		btrfs_print_leaf(root, leaf);
 		BUG();
+	}
 	check_leaf(root, path, 0);
 out:
 	return ret;
@@ -1958,17 +2093,17 @@ int btrfs_insert_item(struct btrfs_trans_handle *trans, struct btrfs_root
 {
 	int ret = 0;
 	struct btrfs_path *path;
-	u8 *ptr;
+	struct extent_buffer *leaf;
+	unsigned long ptr;
 
 	path = btrfs_alloc_path();
 	BUG_ON(!path);
 	ret = btrfs_insert_empty_item(trans, root, path, cpu_key, data_size);
 	if (!ret) {
-		ptr = btrfs_item_ptr(btrfs_buffer_leaf(path->nodes[0]),
-				     path->slots[0], u8);
-		btrfs_memcpy(root, path->nodes[0]->b_data,
-			     ptr, data, data_size);
-		btrfs_mark_buffer_dirty(path->nodes[0]);
+		leaf = path->nodes[0];
+		ptr = btrfs_item_ptr_offset(leaf, path->slots[0]);
+		write_extent_buffer(leaf, data, ptr, data_size);
+		btrfs_mark_buffer_dirty(leaf);
 	}
 	btrfs_free_path(path);
 	return ret;
@@ -1984,30 +2119,30 @@ int btrfs_insert_item(struct btrfs_trans_handle *trans, struct btrfs_root
 static int del_ptr(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 		   struct btrfs_path *path, int level, int slot)
 {
-	struct btrfs_node *node;
-	struct buffer_head *parent = path->nodes[level];
+	struct extent_buffer *parent = path->nodes[level];
 	u32 nritems;
 	int ret = 0;
 	int wret;
 
-	node = btrfs_buffer_node(parent);
-	nritems = btrfs_header_nritems(&node->header);
+	nritems = btrfs_header_nritems(parent);
 	if (slot != nritems -1) {
-		btrfs_memmove(root, node, node->ptrs + slot,
-			      node->ptrs + slot + 1,
+		memmove_extent_buffer(parent,
+			      btrfs_node_key_ptr_offset(slot),
+			      btrfs_node_key_ptr_offset(slot + 1),
 			      sizeof(struct btrfs_key_ptr) *
 			      (nritems - slot - 1));
 	}
 	nritems--;
-	btrfs_set_header_nritems(&node->header, nritems);
+	btrfs_set_header_nritems(parent, nritems);
 	if (nritems == 0 && parent == root->node) {
-		struct btrfs_header *header = btrfs_buffer_header(root->node);
-		BUG_ON(btrfs_header_level(header) != 1);
+		BUG_ON(btrfs_header_level(root->node) != 1);
 		/* just turn the root into a leaf and break */
-		btrfs_set_header_level(header, 0);
+		btrfs_set_header_level(root->node, 0);
 	} else if (slot == 0) {
-		wret = fixup_low_keys(trans, root, path, &node->ptrs[0].key,
-				      level + 1);
+		struct btrfs_disk_key disk_key;
+
+		btrfs_node_key(parent, &disk_key, 0);
+		wret = fixup_low_keys(trans, root, path, &disk_key, level + 1);
 		if (wret)
 			ret = wret;
 	}
@@ -2023,59 +2158,67 @@ int btrfs_del_item(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 		   struct btrfs_path *path)
 {
 	int slot;
-	struct btrfs_leaf *leaf;
-	struct buffer_head *leaf_buf;
+	struct extent_buffer *leaf;
+	struct btrfs_item *item;
 	int doff;
 	int dsize;
 	int ret = 0;
 	int wret;
 	u32 nritems;
 
-	leaf_buf = path->nodes[0];
-	leaf = btrfs_buffer_leaf(leaf_buf);
+	leaf = path->nodes[0];
 	slot = path->slots[0];
-	doff = btrfs_item_offset(leaf->items + slot);
-	dsize = btrfs_item_size(leaf->items + slot);
-	nritems = btrfs_header_nritems(&leaf->header);
+	doff = btrfs_item_offset_nr(leaf, slot);
+	dsize = btrfs_item_size_nr(leaf, slot);
+	nritems = btrfs_header_nritems(leaf);
 
 	if (slot != nritems - 1) {
 		int i;
 		int data_end = leaf_data_end(root, leaf);
-		btrfs_memmove(root, leaf, btrfs_leaf_data(leaf) +
+
+		memmove_extent_buffer(leaf, btrfs_leaf_data(leaf) +
 			      data_end + dsize,
 			      btrfs_leaf_data(leaf) + data_end,
 			      doff - data_end);
+
 		for (i = slot + 1; i < nritems; i++) {
-			u32 ioff = btrfs_item_offset(leaf->items + i);
-			btrfs_set_item_offset(leaf->items + i, ioff + dsize);
+			u32 ioff;
+			item = btrfs_item_nr(leaf, i);
+			ioff = btrfs_item_offset(leaf, item);
+			btrfs_set_item_offset(leaf, item, ioff + dsize);
 		}
-		btrfs_memmove(root, leaf, leaf->items + slot,
-			      leaf->items + slot + 1,
+		memmove_extent_buffer(leaf, btrfs_item_nr_offset(slot),
+			      btrfs_item_nr_offset(slot + 1),
 			      sizeof(struct btrfs_item) *
 			      (nritems - slot - 1));
 	}
-	btrfs_set_header_nritems(&leaf->header, nritems - 1);
+	btrfs_set_header_nritems(leaf, nritems - 1);
 	nritems--;
+
 	/* delete the leaf if we've emptied it */
 	if (nritems == 0) {
-		if (leaf_buf == root->node) {
-			btrfs_set_header_level(&leaf->header, 0);
+		if (leaf == root->node) {
+			btrfs_set_header_level(leaf, 0);
 		} else {
-			clean_tree_block(trans, root, leaf_buf);
-			wait_on_buffer(leaf_buf);
+			clean_tree_block(trans, root, leaf);
+			wait_on_tree_block_writeback(root, leaf);
 			wret = del_ptr(trans, root, path, 1, path->slots[1]);
 			if (wret)
 				ret = wret;
 			wret = btrfs_free_extent(trans, root,
-						 bh_blocknr(leaf_buf), 1, 1);
+						 extent_buffer_blocknr(leaf),
+						 1, 1);
 			if (wret)
 				ret = wret;
 		}
 	} else {
 		int used = leaf_space_used(leaf, 0, nritems);
 		if (slot == 0) {
+			struct btrfs_disk_key disk_key;
+
+			btrfs_item_key(leaf, &disk_key, 0);
 			wret = fixup_low_keys(trans, root, path,
-					      &leaf->items[0].key, 1);
+					      &disk_key, 1);
 			if (wret)
 				ret = wret;
 		}
@@ -2087,34 +2230,40 @@ int btrfs_del_item(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 			 * for possible call to del_ptr below
 			 */
 			slot = path->slots[1];
-			get_bh(leaf_buf);
+			extent_buffer_get(leaf);
+
 			wret = push_leaf_left(trans, root, path, 1);
 			if (wret < 0 && wret != -ENOSPC)
 				ret = wret;
-			if (path->nodes[0] == leaf_buf &&
-			    btrfs_header_nritems(&leaf->header)) {
+
+			if (path->nodes[0] == leaf &&
+			    btrfs_header_nritems(leaf)) {
 				wret = push_leaf_right(trans, root, path, 1);
 				if (wret < 0 && wret != -ENOSPC)
 					ret = wret;
 			}
-			if (btrfs_header_nritems(&leaf->header) == 0) {
-				u64 blocknr = bh_blocknr(leaf_buf);
-				clean_tree_block(trans, root, leaf_buf);
-				wait_on_buffer(leaf_buf);
+
+			if (btrfs_header_nritems(leaf) == 0) {
+				u64 blocknr = extent_buffer_blocknr(leaf);
+
+				clean_tree_block(trans, root, leaf);
+				wait_on_tree_block_writeback(root, leaf);
+
 				wret = del_ptr(trans, root, path, 1, slot);
 				if (wret)
 					ret = wret;
-				btrfs_block_release(root, leaf_buf);
+
+				free_extent_buffer(leaf);
 				wret = btrfs_free_extent(trans, root, blocknr,
 							 1, 1);
 				if (wret)
 					ret = wret;
 			} else {
-				btrfs_mark_buffer_dirty(leaf_buf);
-				btrfs_block_release(root, leaf_buf);
+				btrfs_mark_buffer_dirty(leaf);
+				free_extent_buffer(leaf);
 			}
 		} else {
-			btrfs_mark_buffer_dirty(leaf_buf);
+			btrfs_mark_buffer_dirty(leaf);
 		}
 	}
 	return ret;
@@ -2130,25 +2279,27 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 	int slot;
 	int level = 1;
 	u64 blocknr;
-	struct buffer_head *c;
-	struct btrfs_node *c_node;
-	struct buffer_head *next = NULL;
+	struct extent_buffer *c;
+	struct extent_buffer *next = NULL;
 
 	while(level < BTRFS_MAX_LEVEL) {
 		if (!path->nodes[level])
 			return 1;
+
 		slot = path->slots[level] + 1;
 		c = path->nodes[level];
-		c_node = btrfs_buffer_node(c);
-		if (slot >= btrfs_header_nritems(&c_node->header)) {
+		if (slot >= btrfs_header_nritems(c)) {
 			level++;
 			continue;
 		}
-		blocknr = btrfs_node_blockptr(c_node, slot);
+
+		blocknr = btrfs_node_blockptr(c, slot);
 		if (next)
-			btrfs_block_release(root, next);
+			free_extent_buffer(next);
+
 		if (path->reada)
 			reada_for_search(root, path, level, slot);
+
 		next = read_tree_block(root, blocknr);
 		break;
 	}
@@ -2156,15 +2307,14 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 	while(1) {
 		level--;
 		c = path->nodes[level];
-		btrfs_block_release(root, c);
+		free_extent_buffer(c);
 		path->nodes[level] = next;
 		path->slots[level] = 0;
 		if (!level)
 			break;
 		if (path->reada)
 			reada_for_search(root, path, level, 0);
-		next = read_tree_block(root,
-		       btrfs_node_blockptr(btrfs_buffer_node(next), 0));
+		next = read_tree_block(root, btrfs_node_blockptr(next, 0));
 	}
 	return 0;
 }

commit 86479a04eef8f304a13aeb8b64bcc8e506a68268
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Sep 10 19:58:16 2007 -0400

    Add support for defragging files via btrfsctl -d.  Avoid OOM on extent tree
    defrag.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 89e282c35ce2..b41f48ade419 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -217,6 +217,9 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 		       root->fs_info->generation);
 		WARN_ON(1);
 	}
+	if (buffer_defrag_done(parent))
+		return 0;
+
 	parent_node = btrfs_buffer_node(parent);
 	parent_nritems = btrfs_header_nritems(&parent_node->header);
 	parent_level = btrfs_header_level(&parent_node->header);
@@ -274,6 +277,7 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 		*last_ret = search_start;
 		if (parent_level == 1)
 			clear_buffer_defrag(tmp_bh);
+		set_buffer_defrag_done(tmp_bh);
 		brelse(tmp_bh);
 	}
 	return err;

commit 252c38f0697b6889c03542fa3c96143d3db4d9d8
Author: Yan <yanzheng@21cn.com>
Date:   Wed Aug 29 09:11:44 2007 -0400

    Btrfs: ctree.c cleanups
    
    Fixup a few buffer_head release errors, and fix an off by one in
    balance_node_right.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index c0782a5b04c5..89e282c35ce2 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -266,8 +266,10 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 		err = __btrfs_cow_block(trans, root, cur_bh, parent, i,
 					&tmp_bh, search_start,
 					min(8, end_slot - i));
-		if (err)
+		if (err) {
+			brelse(cur_bh);
 			break;
+		}
 		search_start = bh_blocknr(tmp_bh);
 		*last_ret = search_start;
 		if (parent_level == 1)
@@ -881,7 +883,6 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 		      ins_len, int cow)
 {
 	struct buffer_head *b;
-	struct buffer_head *cow_buf;
 	struct btrfs_node *c;
 	u64 blocknr;
 	int slot;
@@ -905,12 +906,11 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 			wret = btrfs_cow_block(trans, root, b,
 					       p->nodes[level + 1],
 					       p->slots[level + 1],
-					       &cow_buf);
+					       &b);
 			if (wret) {
-				btrfs_block_release(root, cow_buf);
+				btrfs_block_release(root, b);
 				return wret;
 			}
-			b = cow_buf;
 			c = btrfs_buffer_node(b);
 		}
 		BUG_ON(!cow && ins_len);
@@ -1075,8 +1075,9 @@ static int balance_node_right(struct btrfs_trans_handle *trans, struct
 
 	max_push = src_nritems / 2 + 1;
 	/* don't try to empty the node */
-	if (max_push > src_nritems)
+	if (max_push >= src_nritems)
 		return 1;
+
 	if (max_push < push_items)
 		push_items = max_push;
 
@@ -1465,6 +1466,7 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 	ret = btrfs_cow_block(trans, root, t, path->nodes[1], slot - 1, &t);
 	if (ret) {
 		/* we hit -ENOSPC, but it isn't fatal here */
+		btrfs_block_release(root, t);
 		return 1;
 	}
 	left = btrfs_buffer_leaf(t);

commit 2cc58cf24f69be8632a3b29d653c318bf3bd8c84
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Aug 27 16:49:44 2007 -0400

    Btrfs: Do more extensive readahead during tree searches
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 51eea5ccb645..c0782a5b04c5 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -43,8 +43,10 @@ struct btrfs_path *btrfs_alloc_path(void)
 {
 	struct btrfs_path *path;
 	path = kmem_cache_alloc(btrfs_path_cachep, GFP_NOFS);
-	if (path)
+	if (path) {
 		btrfs_init_path(path);
+		path->reada = 1;
+	}
 	return path;
 }
 
@@ -159,6 +161,34 @@ static int close_blocks(u64 blocknr, u64 other)
 	return 0;
 }
 
+static int should_defrag_leaf(struct buffer_head *bh)
+{
+	struct btrfs_leaf *leaf = btrfs_buffer_leaf(bh);
+	struct btrfs_disk_key *key;
+	u32 nritems;
+
+	if (buffer_defrag(bh))
+		return 1;
+
+	nritems = btrfs_header_nritems(&leaf->header);
+	if (nritems == 0)
+		return 0;
+
+	key = &leaf->items[0].key;
+	if (btrfs_disk_key_type(key) == BTRFS_DIR_ITEM_KEY)
+		return 1;
+
+	key = &leaf->items[nritems-1].key;
+	if (btrfs_disk_key_type(key) == BTRFS_DIR_ITEM_KEY)
+		return 1;
+	if (nritems > 4) {
+		key = &leaf->items[nritems/2].key;
+		if (btrfs_disk_key_type(key) == BTRFS_DIR_ITEM_KEY)
+			return 1;
+	}
+	return 0;
+}
+
 int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 		       struct btrfs_root *root, struct buffer_head *parent,
 		       int cache_only, u64 *last_ret)
@@ -217,7 +247,9 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 
 		cur_bh = btrfs_find_tree_block(root, blocknr);
 		if (!cur_bh || !buffer_uptodate(cur_bh) ||
-		    buffer_locked(cur_bh) || !buffer_defrag(cur_bh)) {
+		    buffer_locked(cur_bh) ||
+		    (parent_level != 1 && !buffer_defrag(cur_bh)) ||
+		    (parent_level == 1 && !should_defrag_leaf(cur_bh))) {
 			if (cache_only) {
 				brelse(cur_bh);
 				continue;
@@ -297,6 +329,7 @@ static int check_node(struct btrfs_root *root, struct btrfs_path *path,
 		parent = btrfs_buffer_node(path->nodes[level + 1]);
 
 	slot = path->slots[level];
+	BUG_ON(!buffer_uptodate(path->nodes[level]));
 	BUG_ON(nritems == 0);
 	if (parent) {
 		struct btrfs_disk_key *parent_key;
@@ -511,9 +544,6 @@ static int balance_level(struct btrfs_trans_handle *trans, struct btrfs_root
 		err_on_enospc = 1;
 
 	left_buf = read_node_slot(root, parent_buf, pslot - 1);
-	right_buf = read_node_slot(root, parent_buf, pslot + 1);
-
-	/* first, try to make some room in the middle buffer */
 	if (left_buf) {
 		wret = btrfs_cow_block(trans, root, left_buf,
 				       parent_buf, pslot - 1, &left_buf);
@@ -521,6 +551,19 @@ static int balance_level(struct btrfs_trans_handle *trans, struct btrfs_root
 			ret = wret;
 			goto enospc;
 		}
+	}
+	right_buf = read_node_slot(root, parent_buf, pslot + 1);
+	if (right_buf) {
+		wret = btrfs_cow_block(trans, root, right_buf,
+				       parent_buf, pslot + 1, &right_buf);
+		if (wret) {
+			ret = wret;
+			goto enospc;
+		}
+	}
+
+	/* first, try to make some room in the middle buffer */
+	if (left_buf) {
 		left = btrfs_buffer_node(left_buf);
 		orig_slot += btrfs_header_nritems(&left->header);
 		wret = push_node_left(trans, root, left_buf, mid_buf);
@@ -534,13 +577,6 @@ static int balance_level(struct btrfs_trans_handle *trans, struct btrfs_root
 	 * then try to empty the right most buffer into the middle
 	 */
 	if (right_buf) {
-		wret = btrfs_cow_block(trans, root, right_buf,
-				       parent_buf, pslot + 1, &right_buf);
-		if (wret) {
-			ret = wret;
-			goto enospc;
-		}
-
 		right = btrfs_buffer_node(right_buf);
 		wret = push_node_left(trans, root, mid_buf, right_buf);
 		if (wret < 0 && wret != -ENOSPC)
@@ -817,7 +853,7 @@ static void reada_for_search(struct btrfs_root *root, struct btrfs_path *path,
 		for (i = 0; i < ret; i++) {
 			blocknr = gang[i];
 			clear_radix_bit(&found, blocknr);
-			if (nread > 32)
+			if (path->reada == 1 && nread > 16)
 				continue;
 			if (close_blocks(cluster_start, blocknr)) {
 				readahead_tree_block(root, blocknr);

commit 320206112895c72f98e57570ae194689dcd7fe56
Author: Yan <yanzheng@21cn.com>
Date:   Mon Aug 27 15:17:54 2007 -0400

    fix block readahead in btrfs_next_leaf
    
    Send the correct slot down to reada_for_search
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 7cf43da5e78e..51eea5ccb645 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2120,7 +2120,7 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 		if (!level)
 			break;
 		if (path->reada)
-			reada_for_search(root, path, level, slot);
+			reada_for_search(root, path, level, 0);
 		next = read_tree_block(root,
 		       btrfs_node_blockptr(btrfs_buffer_node(next), 0));
 	}

commit f2183bde1a918d338337955c8e8ba29bd8f5e7b1
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Aug 10 14:42:37 2007 -0400

    Btrfs: Add BH_Defrag to mark buffers that are in need of defragging
    
    This allows the tree walking code to defrag only the newly allocated
    buffers, it seems to be a good balance between perfect defragging and the
    performance hit of repeatedly reallocating blocks.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index ee1ae00d2827..7cf43da5e78e 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -175,6 +175,7 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 	int end_slot;
 	int i;
 	int err = 0;
+	int parent_level;
 
 	if (trans->transaction != root->fs_info->running_transaction) {
 		printk(KERN_CRIT "trans %Lu running %Lu\n", trans->transid,
@@ -188,6 +189,7 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 	}
 	parent_node = btrfs_buffer_node(parent);
 	parent_nritems = btrfs_header_nritems(&parent_node->header);
+	parent_level = btrfs_header_level(&parent_node->header);
 
 	start_slot = 0;
 	end_slot = parent_nritems;
@@ -215,13 +217,16 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 
 		cur_bh = btrfs_find_tree_block(root, blocknr);
 		if (!cur_bh || !buffer_uptodate(cur_bh) ||
-		    buffer_locked(cur_bh)) {
+		    buffer_locked(cur_bh) || !buffer_defrag(cur_bh)) {
 			if (cache_only) {
 				brelse(cur_bh);
 				continue;
 			}
-			brelse(cur_bh);
-			cur_bh = read_tree_block(root, blocknr);
+			if (!cur_bh || !buffer_uptodate(cur_bh) ||
+			    buffer_locked(cur_bh)) {
+				brelse(cur_bh);
+				cur_bh = read_tree_block(root, blocknr);
+			}
 		}
 		if (search_start == 0)
 			search_start = last_block & ~((u64)65535);
@@ -232,6 +237,9 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 		if (err)
 			break;
 		search_start = bh_blocknr(tmp_bh);
+		*last_ret = search_start;
+		if (parent_level == 1)
+			clear_buffer_defrag(tmp_bh);
 		brelse(tmp_bh);
 	}
 	return err;
@@ -811,16 +819,10 @@ static void reada_for_search(struct btrfs_root *root, struct btrfs_path *path,
 			clear_radix_bit(&found, blocknr);
 			if (nread > 32)
 				continue;
-			if (direction > 0 && cluster_start <= blocknr &&
-			    cluster_start + 8 > blocknr) {
-				cluster_start = blocknr;
+			if (close_blocks(cluster_start, blocknr)) {
 				readahead_tree_block(root, blocknr);
 				nread++;
-			} else if (direction < 0 && cluster_start >= blocknr &&
-				   blocknr + 8 > cluster_start) {
 				cluster_start = blocknr;
-				readahead_tree_block(root, blocknr);
-				nread++;
 			}
 		}
 	}

commit e9d0b13b5bbb58c9b840e407a8d181442f799966
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Aug 10 14:06:19 2007 -0400

    Btrfs: Btree defrag on the extent-mapping tree as well
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index c7e47e77723f..ee1ae00d2827 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -99,7 +99,6 @@ static int __btrfs_cow_block(struct btrfs_trans_handle *trans, struct btrfs_root
 		if (ret)
 			return ret;
 	} else {
-		WARN_ON(!root->ref_cows);
 		clean_tree_block(trans, root, buf);
 	}
 
@@ -162,13 +161,14 @@ static int close_blocks(u64 blocknr, u64 other)
 
 int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 		       struct btrfs_root *root, struct buffer_head *parent,
-		       int cache_only)
+		       int cache_only, u64 *last_ret)
 {
 	struct btrfs_node *parent_node;
 	struct buffer_head *cur_bh;
 	struct buffer_head *tmp_bh;
 	u64 blocknr;
-	u64 search_start = 0;
+	u64 search_start = *last_ret;
+	u64 last_block = 0;
 	u64 other;
 	u32 parent_nritems;
 	int start_slot;
@@ -198,6 +198,8 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 	for (i = start_slot; i < end_slot; i++) {
 		int close = 1;
 		blocknr = btrfs_node_blockptr(parent_node, i);
+		if (last_block == 0)
+			last_block = blocknr;
 		if (i > 0) {
 			other = btrfs_node_blockptr(parent_node, i - 1);
 			close = close_blocks(blocknr, other);
@@ -206,8 +208,10 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 			other = btrfs_node_blockptr(parent_node, i + 1);
 			close = close_blocks(blocknr, other);
 		}
-		if (close)
+		if (close) {
+			last_block = blocknr;
 			continue;
+		}
 
 		cur_bh = btrfs_find_tree_block(root, blocknr);
 		if (!cur_bh || !buffer_uptodate(cur_bh) ||
@@ -219,9 +223,9 @@ int btrfs_realloc_node(struct btrfs_trans_handle *trans,
 			brelse(cur_bh);
 			cur_bh = read_tree_block(root, blocknr);
 		}
-		if (search_start == 0) {
-			search_start = bh_blocknr(cur_bh) & ~((u64)65535);
-		}
+		if (search_start == 0)
+			search_start = last_block & ~((u64)65535);
+
 		err = __btrfs_cow_block(trans, root, cur_bh, parent, i,
 					&tmp_bh, search_start,
 					min(8, end_slot - i));

commit 6702ed490ca0bb44e17131818a5a18b773957c5a
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Aug 7 16:15:09 2007 -0400

    Btrfs: Add run time btree defrag, and an ioctl to force btree defrag
    
    This adds two types of btree defrag, a run time form that tries to
    defrag recently allocated blocks in the btree when they are still in ram,
    and an ioctl that forces defrag of all btree blocks.
    
    File data blocks are not defragged yet, but this can make a huge difference
    in sequential btree reads.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 7a08491e208e..c7e47e77723f 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -65,44 +65,44 @@ void btrfs_release_path(struct btrfs_root *root, struct btrfs_path *p)
 	memset(p, 0, sizeof(*p));
 }
 
-static int btrfs_cow_block(struct btrfs_trans_handle *trans, struct btrfs_root
+static int __btrfs_cow_block(struct btrfs_trans_handle *trans, struct btrfs_root
 			   *root, struct buffer_head *buf, struct buffer_head
 			   *parent, int parent_slot, struct buffer_head
-			   **cow_ret)
+			   **cow_ret, u64 search_start, u64 empty_size)
 {
 	struct buffer_head *cow;
 	struct btrfs_node *cow_node;
-	int ret;
+	int ret = 0;
+	int different_trans = 0;
 
+	WARN_ON(root->ref_cows && trans->transid != root->last_trans);
 	WARN_ON(!buffer_uptodate(buf));
-	if (trans->transaction != root->fs_info->running_transaction) {
-		printk(KERN_CRIT "trans %Lu running %Lu\n", trans->transid,
-		       root->fs_info->running_transaction->transid);
-		WARN_ON(1);
-	}
-	if (trans->transid != root->fs_info->generation) {
-		printk(KERN_CRIT "trans %Lu running %Lu\n", trans->transid,
-		       root->fs_info->generation);
-		WARN_ON(1);
-	}
-	if (btrfs_header_generation(btrfs_buffer_header(buf)) ==
-				    trans->transid) {
-		*cow_ret = buf;
-		return 0;
-	}
-	cow = btrfs_alloc_free_block(trans, root, buf->b_blocknr);
+	cow = btrfs_alloc_free_block(trans, root, search_start, empty_size);
 	if (IS_ERR(cow))
 		return PTR_ERR(cow);
+
 	cow_node = btrfs_buffer_node(cow);
 	if (buf->b_size != root->blocksize || cow->b_size != root->blocksize)
 		WARN_ON(1);
+
 	memcpy(cow_node, btrfs_buffer_node(buf), root->blocksize);
 	btrfs_set_header_blocknr(&cow_node->header, bh_blocknr(cow));
 	btrfs_set_header_generation(&cow_node->header, trans->transid);
 	btrfs_set_header_owner(&cow_node->header, root->root_key.objectid);
-	ret = btrfs_inc_ref(trans, root, buf);
-	if (ret)
-		return ret;
+
+	WARN_ON(btrfs_header_generation(btrfs_buffer_header(buf)) >
+		trans->transid);
+	if (btrfs_header_generation(btrfs_buffer_header(buf)) !=
+				    trans->transid) {
+		different_trans = 1;
+		ret = btrfs_inc_ref(trans, root, buf);
+		if (ret)
+			return ret;
+	} else {
+		WARN_ON(!root->ref_cows);
+		clean_tree_block(trans, root, buf);
+	}
+
 	if (buf == root->node) {
 		root->node = cow;
 		get_bh(cow);
@@ -114,6 +114,8 @@ static int btrfs_cow_block(struct btrfs_trans_handle *trans, struct btrfs_root
 		btrfs_set_node_blockptr(btrfs_buffer_node(parent), parent_slot,
 					bh_blocknr(cow));
 		btrfs_mark_buffer_dirty(parent);
+		WARN_ON(btrfs_header_generation(btrfs_buffer_header(parent)) !=
+				    trans->transid);
 		btrfs_free_extent(trans, root, bh_blocknr(buf), 1, 1);
 	}
 	btrfs_block_release(root, buf);
@@ -122,6 +124,115 @@ static int btrfs_cow_block(struct btrfs_trans_handle *trans, struct btrfs_root
 	return 0;
 }
 
+int btrfs_cow_block(struct btrfs_trans_handle *trans, struct btrfs_root
+			   *root, struct buffer_head *buf, struct buffer_head
+			   *parent, int parent_slot, struct buffer_head
+			   **cow_ret)
+{
+	u64 search_start;
+	if (trans->transaction != root->fs_info->running_transaction) {
+		printk(KERN_CRIT "trans %Lu running %Lu\n", trans->transid,
+		       root->fs_info->running_transaction->transid);
+		WARN_ON(1);
+	}
+	if (trans->transid != root->fs_info->generation) {
+		printk(KERN_CRIT "trans %Lu running %Lu\n", trans->transid,
+		       root->fs_info->generation);
+		WARN_ON(1);
+	}
+	if (btrfs_header_generation(btrfs_buffer_header(buf)) ==
+				    trans->transid) {
+		*cow_ret = buf;
+		return 0;
+	}
+
+	search_start = bh_blocknr(buf) & ~((u64)65535);
+	return __btrfs_cow_block(trans, root, buf, parent,
+				 parent_slot, cow_ret, search_start, 0);
+}
+
+static int close_blocks(u64 blocknr, u64 other)
+{
+	if (blocknr < other && other - blocknr < 8)
+		return 1;
+	if (blocknr > other && blocknr - other < 8)
+		return 1;
+	return 0;
+}
+
+int btrfs_realloc_node(struct btrfs_trans_handle *trans,
+		       struct btrfs_root *root, struct buffer_head *parent,
+		       int cache_only)
+{
+	struct btrfs_node *parent_node;
+	struct buffer_head *cur_bh;
+	struct buffer_head *tmp_bh;
+	u64 blocknr;
+	u64 search_start = 0;
+	u64 other;
+	u32 parent_nritems;
+	int start_slot;
+	int end_slot;
+	int i;
+	int err = 0;
+
+	if (trans->transaction != root->fs_info->running_transaction) {
+		printk(KERN_CRIT "trans %Lu running %Lu\n", trans->transid,
+		       root->fs_info->running_transaction->transid);
+		WARN_ON(1);
+	}
+	if (trans->transid != root->fs_info->generation) {
+		printk(KERN_CRIT "trans %Lu running %Lu\n", trans->transid,
+		       root->fs_info->generation);
+		WARN_ON(1);
+	}
+	parent_node = btrfs_buffer_node(parent);
+	parent_nritems = btrfs_header_nritems(&parent_node->header);
+
+	start_slot = 0;
+	end_slot = parent_nritems;
+
+	if (parent_nritems == 1)
+		return 0;
+
+	for (i = start_slot; i < end_slot; i++) {
+		int close = 1;
+		blocknr = btrfs_node_blockptr(parent_node, i);
+		if (i > 0) {
+			other = btrfs_node_blockptr(parent_node, i - 1);
+			close = close_blocks(blocknr, other);
+		}
+		if (close && i < end_slot - 1) {
+			other = btrfs_node_blockptr(parent_node, i + 1);
+			close = close_blocks(blocknr, other);
+		}
+		if (close)
+			continue;
+
+		cur_bh = btrfs_find_tree_block(root, blocknr);
+		if (!cur_bh || !buffer_uptodate(cur_bh) ||
+		    buffer_locked(cur_bh)) {
+			if (cache_only) {
+				brelse(cur_bh);
+				continue;
+			}
+			brelse(cur_bh);
+			cur_bh = read_tree_block(root, blocknr);
+		}
+		if (search_start == 0) {
+			search_start = bh_blocknr(cur_bh) & ~((u64)65535);
+		}
+		err = __btrfs_cow_block(trans, root, cur_bh, parent, i,
+					&tmp_bh, search_start,
+					min(8, end_slot - i));
+		if (err)
+			break;
+		search_start = bh_blocknr(tmp_bh);
+		brelse(tmp_bh);
+	}
+	return err;
+}
+
 /*
  * The leaf data grows from end-to-front in the node.
  * this returns the address of the start of the last item,
@@ -221,6 +332,7 @@ static int check_leaf(struct btrfs_root *root, struct btrfs_path *path,
 
 		parent_slot = path->slots[level + 1];
 		parent_key = &parent->ptrs[parent_slot].key;
+
 		BUG_ON(memcmp(parent_key, &leaf->items[0].key,
 		       sizeof(struct btrfs_disk_key)));
 		BUG_ON(btrfs_node_blockptr(parent, parent_slot) !=
@@ -643,7 +755,7 @@ static int push_nodes_for_insert(struct btrfs_trans_handle *trans,
  * readahead one full node of leaves
  */
 static void reada_for_search(struct btrfs_root *root, struct btrfs_path *path,
-			     int slot)
+			     int level, int slot)
 {
 	struct btrfs_node *node;
 	int i;
@@ -659,10 +771,13 @@ static void reada_for_search(struct btrfs_root *root, struct btrfs_path *path,
 	unsigned long gang[8];
 	struct buffer_head *bh;
 
-	if (!path->nodes[1])
+	if (level == 0)
+		return;
+
+	if (!path->nodes[level])
 		return;
 
-	node = btrfs_buffer_node(path->nodes[1]);
+	node = btrfs_buffer_node(path->nodes[level]);
 	search = btrfs_node_blockptr(node, slot);
 	bh = btrfs_find_tree_block(root, search);
 	if (bh) {
@@ -690,7 +805,7 @@ static void reada_for_search(struct btrfs_root *root, struct btrfs_path *path,
 		for (i = 0; i < ret; i++) {
 			blocknr = gang[i];
 			clear_radix_bit(&found, blocknr);
-			if (nread > 64)
+			if (nread > 32)
 				continue;
 			if (direction > 0 && cluster_start <= blocknr &&
 			    cluster_start + 8 > blocknr) {
@@ -726,7 +841,6 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 	struct buffer_head *b;
 	struct buffer_head *cow_buf;
 	struct btrfs_node *c;
-	struct btrfs_root_item *root_item = &root->root_item;
 	u64 blocknr;
 	int slot;
 	int ret;
@@ -734,11 +848,8 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 	int should_reada = p->reada;
 	u8 lowest_level = 0;
 
-	if (btrfs_root_refs(root_item) == 0 && root->ref_cows) {
-		lowest_level = root_item->drop_level;
-		WARN_ON(ins_len || cow);
-	}
-
+	lowest_level = p->lowest_level;
+	WARN_ON(lowest_level && ins_len);
 	WARN_ON(p->nodes[0] != NULL);
 	WARN_ON(!mutex_is_locked(&root->fs_info->fs_mutex));
 again:
@@ -798,8 +909,8 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 			if (level == lowest_level)
 				break;
 			blocknr = btrfs_node_blockptr(c, slot);
-			if (level == 1 && should_reada)
-				reada_for_search(root, p, slot);
+			if (should_reada)
+				reada_for_search(root, p, level, slot);
 			b = read_tree_block(root, btrfs_node_blockptr(c, slot));
 
 		} else {
@@ -960,7 +1071,7 @@ static int insert_new_root(struct btrfs_trans_handle *trans, struct btrfs_root
 	BUG_ON(path->nodes[level]);
 	BUG_ON(path->nodes[level-1] != root->node);
 
-	t = btrfs_alloc_free_block(trans, root, root->node->b_blocknr);
+	t = btrfs_alloc_free_block(trans, root, root->node->b_blocknr, 0);
 	if (IS_ERR(t))
 		return PTR_ERR(t);
 	c = btrfs_buffer_node(t);
@@ -1070,7 +1181,7 @@ static int split_node(struct btrfs_trans_handle *trans, struct btrfs_root
 	}
 
 	c_nritems = btrfs_header_nritems(&c->header);
-	split_buffer = btrfs_alloc_free_block(trans, root, t->b_blocknr);
+	split_buffer = btrfs_alloc_free_block(trans, root, t->b_blocknr, 0);
 	if (IS_ERR(split_buffer))
 		return PTR_ERR(split_buffer);
 
@@ -1461,7 +1572,7 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 	nritems = btrfs_header_nritems(&l->header);
 	mid = (nritems + 1)/ 2;
 
-	right_buffer = btrfs_alloc_free_block(trans, root, l_buf->b_blocknr);
+	right_buffer = btrfs_alloc_free_block(trans, root, l_buf->b_blocknr, 0);
 	if (IS_ERR(right_buffer))
 		return PTR_ERR(right_buffer);
 
@@ -1560,7 +1671,7 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 
 	if (!double_split)
 		return ret;
-	right_buffer = btrfs_alloc_free_block(trans, root, l_buf->b_blocknr);
+	right_buffer = btrfs_alloc_free_block(trans, root, l_buf->b_blocknr, 0);
 	if (IS_ERR(right_buffer))
 		return PTR_ERR(right_buffer);
 
@@ -1988,8 +2099,8 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 		blocknr = btrfs_node_blockptr(c_node, slot);
 		if (next)
 			btrfs_block_release(root, next);
-		if (level == 1 && path->reada)
-			reada_for_search(root, path, slot);
+		if (path->reada)
+			reada_for_search(root, path, level, slot);
 		next = read_tree_block(root, blocknr);
 		break;
 	}
@@ -2002,8 +2113,8 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 		path->slots[level] = 0;
 		if (!level)
 			break;
-		if (level == 1 && path->reada)
-			reada_for_search(root, path, slot);
+		if (path->reada)
+			reada_for_search(root, path, level, slot);
 		next = read_tree_block(root,
 		       btrfs_node_blockptr(btrfs_buffer_node(next), 0));
 	}

commit 3c69faecb8d83cb2ef085a98b196a3fecea67725
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Aug 7 15:52:22 2007 -0400

    Btrfs: Fold some btree readahead routines into something more generic.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index aa824e2c521f..7a08491e208e 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -639,6 +639,73 @@ static int push_nodes_for_insert(struct btrfs_trans_handle *trans,
 	return 1;
 }
 
+/*
+ * readahead one full node of leaves
+ */
+static void reada_for_search(struct btrfs_root *root, struct btrfs_path *path,
+			     int slot)
+{
+	struct btrfs_node *node;
+	int i;
+	u32 nritems;
+	u64 item_objectid;
+	u64 blocknr;
+	u64 search;
+	u64 cluster_start;
+	int ret;
+	int nread = 0;
+	int direction = path->reada;
+	struct radix_tree_root found;
+	unsigned long gang[8];
+	struct buffer_head *bh;
+
+	if (!path->nodes[1])
+		return;
+
+	node = btrfs_buffer_node(path->nodes[1]);
+	search = btrfs_node_blockptr(node, slot);
+	bh = btrfs_find_tree_block(root, search);
+	if (bh) {
+		brelse(bh);
+		return;
+	}
+
+	init_bit_radix(&found);
+	nritems = btrfs_header_nritems(&node->header);
+	for (i = slot; i < nritems; i++) {
+		item_objectid = btrfs_disk_key_objectid(&node->ptrs[i].key);
+		blocknr = btrfs_node_blockptr(node, i);
+		set_radix_bit(&found, blocknr);
+	}
+	if (direction > 0) {
+		cluster_start = search - 4;
+		if (cluster_start > search)
+			cluster_start = 0;
+	} else
+		cluster_start = search + 4;
+	while(1) {
+		ret = find_first_radix_bit(&found, gang, 0, ARRAY_SIZE(gang));
+		if (!ret)
+			break;
+		for (i = 0; i < ret; i++) {
+			blocknr = gang[i];
+			clear_radix_bit(&found, blocknr);
+			if (nread > 64)
+				continue;
+			if (direction > 0 && cluster_start <= blocknr &&
+			    cluster_start + 8 > blocknr) {
+				cluster_start = blocknr;
+				readahead_tree_block(root, blocknr);
+				nread++;
+			} else if (direction < 0 && cluster_start >= blocknr &&
+				   blocknr + 8 > cluster_start) {
+				cluster_start = blocknr;
+				readahead_tree_block(root, blocknr);
+				nread++;
+			}
+		}
+	}
+}
 /*
  * look for key in the tree.  path is filled in with nodes along the way
  * if key is found, we return zero and you can find the item in the leaf
@@ -660,9 +727,11 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 	struct buffer_head *cow_buf;
 	struct btrfs_node *c;
 	struct btrfs_root_item *root_item = &root->root_item;
+	u64 blocknr;
 	int slot;
 	int ret;
 	int level;
+	int should_reada = p->reada;
 	u8 lowest_level = 0;
 
 	if (btrfs_root_refs(root_item) == 0 && root->ref_cows) {
@@ -728,7 +797,11 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 			/* this is only true while dropping a snapshot */
 			if (level == lowest_level)
 				break;
+			blocknr = btrfs_node_blockptr(c, slot);
+			if (level == 1 && should_reada)
+				reada_for_search(root, p, slot);
 			b = read_tree_block(root, btrfs_node_blockptr(c, slot));
+
 		} else {
 			struct btrfs_leaf *l = (struct btrfs_leaf *)c;
 			p->slots[level] = slot;
@@ -1915,6 +1988,8 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 		blocknr = btrfs_node_blockptr(c_node, slot);
 		if (next)
 			btrfs_block_release(root, next);
+		if (level == 1 && path->reada)
+			reada_for_search(root, path, slot);
 		next = read_tree_block(root, blocknr);
 		break;
 	}
@@ -1927,6 +2002,8 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 		path->slots[level] = 0;
 		if (!level)
 			break;
+		if (level == 1 && path->reada)
+			reada_for_search(root, path, slot);
 		next = read_tree_block(root,
 		       btrfs_node_blockptr(btrfs_buffer_node(next), 0));
 	}

commit 9f3a742736cecda5a8778be70faa2f779458839f
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Aug 7 15:52:19 2007 -0400

    Btrfs: Do snapshot deletion in smaller chunks.
    
    Before, snapshot deletion was a single atomic unit.  This caused considerable
    lock contention and required an unbounded amount of space.  Now,
    the drop_progress field in the root item is used to indicate how far along
    snapshot deletion is, and to resume where it left off.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 19a30b7c3a28..aa824e2c521f 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -659,9 +659,16 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 	struct buffer_head *b;
 	struct buffer_head *cow_buf;
 	struct btrfs_node *c;
+	struct btrfs_root_item *root_item = &root->root_item;
 	int slot;
 	int ret;
 	int level;
+	u8 lowest_level = 0;
+
+	if (btrfs_root_refs(root_item) == 0 && root->ref_cows) {
+		lowest_level = root_item->drop_level;
+		WARN_ON(ins_len || cow);
+	}
 
 	WARN_ON(p->nodes[0] != NULL);
 	WARN_ON(!mutex_is_locked(&root->fs_info->fs_mutex));
@@ -718,6 +725,9 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 				slot = p->slots[level];
 				BUG_ON(btrfs_header_nritems(&c->header) == 1);
 			}
+			/* this is only true while dropping a snapshot */
+			if (level == lowest_level)
+				break;
 			b = read_tree_block(root, btrfs_node_blockptr(c, slot));
 		} else {
 			struct btrfs_leaf *l = (struct btrfs_leaf *)c;

commit a1f396304fb7e5f18e4ea81c294415375f1c814c
Author: Aneesh <aneesh.kumar@gmail.com>
Date:   Wed Jul 11 10:03:27 2007 -0400

    Btrfs: Some code cleanups
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index eace2ee76448..19a30b7c3a28 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -172,11 +172,13 @@ static int check_node(struct btrfs_root *root, struct btrfs_path *path,
 
 	if (path->nodes[level + 1])
 		parent = btrfs_buffer_node(path->nodes[level + 1]);
-	parent_slot = path->slots[level + 1];
+
 	slot = path->slots[level];
 	BUG_ON(nritems == 0);
 	if (parent) {
 		struct btrfs_disk_key *parent_key;
+
+		parent_slot = path->slots[level + 1];
 		parent_key = &parent->ptrs[parent_slot].key;
 		BUG_ON(memcmp(parent_key, &node->ptrs[0].key,
 			      sizeof(struct btrfs_disk_key)));
@@ -208,7 +210,7 @@ static int check_leaf(struct btrfs_root *root, struct btrfs_path *path,
 
 	if (path->nodes[level + 1])
 		parent = btrfs_buffer_node(path->nodes[level + 1]);
-	parent_slot = path->slots[level + 1];
+
 	BUG_ON(btrfs_leaf_free_space(root, leaf) < 0);
 
 	if (nritems == 0)
@@ -216,6 +218,8 @@ static int check_leaf(struct btrfs_root *root, struct btrfs_path *path,
 
 	if (parent) {
 		struct btrfs_disk_key *parent_key;
+
+		parent_slot = path->slots[level + 1];
 		parent_key = &parent->ptrs[parent_slot].key;
 		BUG_ON(memcmp(parent_key, &leaf->items[0].key,
 		       sizeof(struct btrfs_disk_key)));

commit ec6b910fb330f29e169c9f502c3ac209515af8d1
Author: Zach Brown <zach.brown@oracle.com>
Date:   Wed Jul 11 10:00:37 2007 -0400

    Btrfs: trivial include fixups
    
    Almost none of the files including module.h need to do so,
    remove them.
    
    Include sched.h in extent-tree.c to silence a warning about cond_resched()
    being undeclared.
    
    Signed-off-by: Zach Brown <zach.brown@oracle.com>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 9eb646529edc..eace2ee76448 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -16,7 +16,6 @@
  * Boston, MA 021110-1307, USA.
  */
 
-#include <linux/module.h>
 #include "ctree.h"
 #include "disk-io.h"
 #include "transaction.h"

commit ccd467d60e81b48cdbecae93532b66bcdedca91d
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Jun 28 15:57:36 2007 -0400

    Btrfs: crash recovery fixes
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 606a19b5916d..9eb646529edc 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -75,6 +75,17 @@ static int btrfs_cow_block(struct btrfs_trans_handle *trans, struct btrfs_root
 	struct btrfs_node *cow_node;
 	int ret;
 
+	WARN_ON(!buffer_uptodate(buf));
+	if (trans->transaction != root->fs_info->running_transaction) {
+		printk(KERN_CRIT "trans %Lu running %Lu\n", trans->transid,
+		       root->fs_info->running_transaction->transid);
+		WARN_ON(1);
+	}
+	if (trans->transid != root->fs_info->generation) {
+		printk(KERN_CRIT "trans %Lu running %Lu\n", trans->transid,
+		       root->fs_info->generation);
+		WARN_ON(1);
+	}
 	if (btrfs_header_generation(btrfs_buffer_header(buf)) ==
 				    trans->transid) {
 		*cow_ret = buf;
@@ -107,7 +118,7 @@ static int btrfs_cow_block(struct btrfs_trans_handle *trans, struct btrfs_root
 		btrfs_free_extent(trans, root, bh_blocknr(buf), 1, 1);
 	}
 	btrfs_block_release(root, buf);
-	mark_buffer_dirty(cow);
+	btrfs_mark_buffer_dirty(cow);
 	*cow_ret = cow;
 	return 0;
 }

commit 54aa1f4dfdacd60a19c4471220b24e581be6f774
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Jun 22 14:16:25 2007 -0400

    Btrfs: Audit callers and return codes to make sure -ENOSPC gets up the stack
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 498dea0854c1..606a19b5916d 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -73,6 +73,7 @@ static int btrfs_cow_block(struct btrfs_trans_handle *trans, struct btrfs_root
 {
 	struct buffer_head *cow;
 	struct btrfs_node *cow_node;
+	int ret;
 
 	if (btrfs_header_generation(btrfs_buffer_header(buf)) ==
 				    trans->transid) {
@@ -80,6 +81,8 @@ static int btrfs_cow_block(struct btrfs_trans_handle *trans, struct btrfs_root
 		return 0;
 	}
 	cow = btrfs_alloc_free_block(trans, root, buf->b_blocknr);
+	if (IS_ERR(cow))
+		return PTR_ERR(cow);
 	cow_node = btrfs_buffer_node(cow);
 	if (buf->b_size != root->blocksize || cow->b_size != root->blocksize)
 		WARN_ON(1);
@@ -87,7 +90,9 @@ static int btrfs_cow_block(struct btrfs_trans_handle *trans, struct btrfs_root
 	btrfs_set_header_blocknr(&cow_node->header, bh_blocknr(cow));
 	btrfs_set_header_generation(&cow_node->header, trans->transid);
 	btrfs_set_header_owner(&cow_node->header, root->root_key.objectid);
-	btrfs_inc_ref(trans, root, buf);
+	ret = btrfs_inc_ref(trans, root, buf);
+	if (ret)
+		return ret;
 	if (buf == root->node) {
 		root->node = cow;
 		get_bh(cow);
@@ -320,6 +325,7 @@ static int balance_level(struct btrfs_trans_handle *trans, struct btrfs_root
 	int wret;
 	int pslot;
 	int orig_slot = path->slots[level];
+	int err_on_enospc = 0;
 	u64 orig_ptr;
 
 	if (level == 0)
@@ -363,29 +369,43 @@ static int balance_level(struct btrfs_trans_handle *trans, struct btrfs_root
 	    BTRFS_NODEPTRS_PER_BLOCK(root) / 4)
 		return 0;
 
+	if (btrfs_header_nritems(&mid->header) < 2)
+		err_on_enospc = 1;
+
 	left_buf = read_node_slot(root, parent_buf, pslot - 1);
 	right_buf = read_node_slot(root, parent_buf, pslot + 1);
 
 	/* first, try to make some room in the middle buffer */
 	if (left_buf) {
-		btrfs_cow_block(trans, root, left_buf, parent_buf, pslot - 1,
-				&left_buf);
+		wret = btrfs_cow_block(trans, root, left_buf,
+				       parent_buf, pslot - 1, &left_buf);
+		if (wret) {
+			ret = wret;
+			goto enospc;
+		}
 		left = btrfs_buffer_node(left_buf);
 		orig_slot += btrfs_header_nritems(&left->header);
 		wret = push_node_left(trans, root, left_buf, mid_buf);
 		if (wret < 0)
 			ret = wret;
+		if (btrfs_header_nritems(&mid->header) < 2)
+			err_on_enospc = 1;
 	}
 
 	/*
 	 * then try to empty the right most buffer into the middle
 	 */
 	if (right_buf) {
-		btrfs_cow_block(trans, root, right_buf, parent_buf, pslot + 1,
-				&right_buf);
+		wret = btrfs_cow_block(trans, root, right_buf,
+				       parent_buf, pslot + 1, &right_buf);
+		if (wret) {
+			ret = wret;
+			goto enospc;
+		}
+
 		right = btrfs_buffer_node(right_buf);
 		wret = push_node_left(trans, root, mid_buf, right_buf);
-		if (wret < 0)
+		if (wret < 0 && wret != -ENOSPC)
 			ret = wret;
 		if (btrfs_header_nritems(&right->header) == 0) {
 			u64 blocknr = bh_blocknr(right_buf);
@@ -421,8 +441,10 @@ static int balance_level(struct btrfs_trans_handle *trans, struct btrfs_root
 		 */
 		BUG_ON(!left_buf);
 		wret = balance_node_right(trans, root, mid_buf, left_buf);
-		if (wret < 0)
+		if (wret < 0) {
 			ret = wret;
+			goto enospc;
+		}
 		BUG_ON(wret == 1);
 	}
 	if (btrfs_header_nritems(&mid->header) == 0) {
@@ -467,7 +489,7 @@ static int balance_level(struct btrfs_trans_handle *trans, struct btrfs_root
 	    btrfs_node_blockptr(btrfs_buffer_node(path->nodes[level]),
 				path->slots[level]))
 		BUG();
-
+enospc:
 	if (right_buf)
 		btrfs_block_release(root, right_buf);
 	if (left_buf)
@@ -519,10 +541,15 @@ static int push_nodes_for_insert(struct btrfs_trans_handle *trans,
 		if (left_nr >= BTRFS_NODEPTRS_PER_BLOCK(root) - 1) {
 			wret = 1;
 		} else {
-			btrfs_cow_block(trans, root, left_buf, parent_buf,
-					pslot - 1, &left_buf);
-			left = btrfs_buffer_node(left_buf);
-			wret = push_node_left(trans, root, left_buf, mid_buf);
+			ret = btrfs_cow_block(trans, root, left_buf, parent_buf,
+					      pslot - 1, &left_buf);
+			if (ret)
+				wret = 1;
+			else {
+				left = btrfs_buffer_node(left_buf);
+				wret = push_node_left(trans, root,
+						      left_buf, mid_buf);
+			}
 		}
 		if (wret < 0)
 			ret = wret;
@@ -561,11 +588,16 @@ static int push_nodes_for_insert(struct btrfs_trans_handle *trans,
 		if (right_nr >= BTRFS_NODEPTRS_PER_BLOCK(root) - 1) {
 			wret = 1;
 		} else {
-			btrfs_cow_block(trans, root, right_buf,
-					parent_buf, pslot + 1, &right_buf);
-			right = btrfs_buffer_node(right_buf);
-			wret = balance_node_right(trans, root,
-						  right_buf, mid_buf);
+			ret = btrfs_cow_block(trans, root, right_buf,
+					      parent_buf, pslot + 1,
+					      &right_buf);
+			if (ret)
+				wret = 1;
+			else {
+				right = btrfs_buffer_node(right_buf);
+				wret = balance_node_right(trans, root,
+							  right_buf, mid_buf);
+			}
 		}
 		if (wret < 0)
 			ret = wret;
@@ -631,6 +663,10 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 					       p->nodes[level + 1],
 					       p->slots[level + 1],
 					       &cow_buf);
+			if (wret) {
+				btrfs_block_release(root, cow_buf);
+				return wret;
+			}
 			b = cow_buf;
 			c = btrfs_buffer_node(b);
 		}
@@ -737,6 +773,7 @@ static int push_node_left(struct btrfs_trans_handle *trans, struct btrfs_root
 	src_nritems = btrfs_header_nritems(&src->header);
 	dst_nritems = btrfs_header_nritems(&dst->header);
 	push_items = BTRFS_NODEPTRS_PER_BLOCK(root) - dst_nritems;
+
 	if (push_items <= 0) {
 		return 1;
 	}
@@ -827,6 +864,8 @@ static int insert_new_root(struct btrfs_trans_handle *trans, struct btrfs_root
 	BUG_ON(path->nodes[level-1] != root->node);
 
 	t = btrfs_alloc_free_block(trans, root, root->node->b_blocknr);
+	if (IS_ERR(t))
+		return PTR_ERR(t);
 	c = btrfs_buffer_node(t);
 	memset(c, 0, root->blocksize);
 	btrfs_set_header_nritems(&c->header, 1);
@@ -929,10 +968,15 @@ static int split_node(struct btrfs_trans_handle *trans, struct btrfs_root
 		    btrfs_header_nritems(&c->header) <
 		    BTRFS_NODEPTRS_PER_BLOCK(root) - 1)
 			return 0;
+		if (ret < 0)
+			return ret;
 	}
 
 	c_nritems = btrfs_header_nritems(&c->header);
 	split_buffer = btrfs_alloc_free_block(trans, root, t->b_blocknr);
+	if (IS_ERR(split_buffer))
+		return PTR_ERR(split_buffer);
+
 	split = btrfs_buffer_node(split_buffer);
 	btrfs_set_header_flags(&split->header, btrfs_header_flags(&c->header));
 	btrfs_set_header_level(&split->header, btrfs_header_level(&c->header));
@@ -1022,6 +1066,7 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 	struct btrfs_item *item;
 	u32 left_nritems;
 	u32 right_nritems;
+	int ret;
 
 	slot = path->slots[1];
 	if (!path->nodes[1]) {
@@ -1041,7 +1086,12 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 		return 1;
 	}
 	/* cow and double check */
-	btrfs_cow_block(trans, root, right_buf, upper, slot + 1, &right_buf);
+	ret = btrfs_cow_block(trans, root, right_buf, upper,
+			      slot + 1, &right_buf);
+	if (ret) {
+		btrfs_block_release(root, right_buf);
+		return 1;
+	}
 	right = btrfs_buffer_leaf(right_buf);
 	free_space = btrfs_leaf_free_space(root, right);
 	if (free_space < data_size + sizeof(struct btrfs_item)) {
@@ -1162,7 +1212,11 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 	}
 
 	/* cow and double check */
-	btrfs_cow_block(trans, root, t, path->nodes[1], slot - 1, &t);
+	ret = btrfs_cow_block(trans, root, t, path->nodes[1], slot - 1, &t);
+	if (ret) {
+		/* we hit -ENOSPC, but it isn't fatal here */
+		return 1;
+	}
 	left = btrfs_buffer_leaf(t);
 	free_space = btrfs_leaf_free_space(root, left);
 	if (free_space < data_size + sizeof(struct btrfs_item)) {
@@ -1309,8 +1363,11 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 	slot = path->slots[0];
 	nritems = btrfs_header_nritems(&l->header);
 	mid = (nritems + 1)/ 2;
+
 	right_buffer = btrfs_alloc_free_block(trans, root, l_buf->b_blocknr);
-	BUG_ON(!right_buffer);
+	if (IS_ERR(right_buffer))
+		return PTR_ERR(right_buffer);
+
 	right = btrfs_buffer_leaf(right_buffer);
 	memset(&right->header, 0, sizeof(right->header));
 	btrfs_set_header_blocknr(&right->header, bh_blocknr(right_buffer));
@@ -1407,7 +1464,9 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 	if (!double_split)
 		return ret;
 	right_buffer = btrfs_alloc_free_block(trans, root, l_buf->b_blocknr);
-	BUG_ON(!right_buffer);
+	if (IS_ERR(right_buffer))
+		return PTR_ERR(right_buffer);
+
 	right = btrfs_buffer_leaf(right_buffer);
 	memset(&right->header, 0, sizeof(right->header));
 	btrfs_set_header_blocknr(&right->header, bh_blocknr(right_buffer));
@@ -1655,7 +1714,6 @@ int btrfs_insert_item(struct btrfs_trans_handle *trans, struct btrfs_root
 			     ptr, data, data_size);
 		btrfs_mark_buffer_dirty(path->nodes[0]);
 	}
-	btrfs_release_path(root, path);
 	btrfs_free_path(path);
 	return ret;
 }
@@ -1775,12 +1833,12 @@ int btrfs_del_item(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 			slot = path->slots[1];
 			get_bh(leaf_buf);
 			wret = push_leaf_left(trans, root, path, 1);
-			if (wret < 0)
+			if (wret < 0 && wret != -ENOSPC)
 				ret = wret;
 			if (path->nodes[0] == leaf_buf &&
 			    btrfs_header_nritems(&leaf->header)) {
 				wret = push_leaf_right(trans, root, path, 1);
-				if (wret < 0)
+				if (wret < 0 && wret != -ENOSPC)
 					ret = wret;
 			}
 			if (btrfs_header_nritems(&leaf->header) == 0) {

commit f1ace244c8c1e16eaa5c8b3b5339849651e31ede
Author: Aneesh <aneesh.kumar@linux.vnet.ibm.com>
Date:   Wed Jun 13 16:18:26 2007 -0400

    btrfs: Code cleanup
    Attaching below is some of the code cleanups that i came across while
    reading the code.
    
    a) alloc_path already calls init_path.
    b) Mention that btrfs_inode is the in memory copy.Ext4 have ext4_inode_info as
    the in memory copy ext4_inode as the disk copy
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 4736155af419..498dea0854c1 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1647,7 +1647,6 @@ int btrfs_insert_item(struct btrfs_trans_handle *trans, struct btrfs_root
 
 	path = btrfs_alloc_path();
 	BUG_ON(!path);
-	btrfs_init_path(path);
 	ret = btrfs_insert_empty_item(trans, root, path, cpu_key, data_size);
 	if (!ret) {
 		ptr = btrfs_item_ptr(btrfs_buffer_leaf(path->nodes[0]),

commit 6cbd55707802b98843f953d1ae6d8f5bcd9a76c0
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Jun 12 09:07:21 2007 -0400

    Btrfs: add GPLv2
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 20e01f95ce6e..4736155af419 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1,3 +1,21 @@
+/*
+ * Copyright (C) 2007 Oracle.  All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public
+ * License v2 as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public
+ * License along with this program; if not, write to the
+ * Free Software Foundation, Inc., 59 Temple Place - Suite 330,
+ * Boston, MA 021110-1307, USA.
+ */
+
 #include <linux/module.h>
 #include "ctree.h"
 #include "disk-io.h"

commit 098f59c2512426926722a96f82af127bd91fb5e4
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri May 11 11:33:21 2007 -0400

    Btrfs: patch queue: fix corruption when splitting large items
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 990d297a7a13..20e01f95ce6e 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -871,6 +871,7 @@ static int insert_ptr(struct btrfs_trans_handle *trans, struct btrfs_root
 	btrfs_set_node_blockptr(lower, slot, blocknr);
 	btrfs_set_header_nritems(&lower->header, nritems + 1);
 	btrfs_mark_buffer_dirty(path->nodes[level]);
+	check_node(root, path, level);
 	return 0;
 }
 
@@ -1101,6 +1102,8 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 	} else {
 		btrfs_block_release(root, right_buf);
 	}
+	if (path->nodes[1])
+		check_node(root, path, 1);
 	return 0;
 }
 /*
@@ -1216,6 +1219,7 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 
 	btrfs_mark_buffer_dirty(t);
 	btrfs_mark_buffer_dirty(right_buf);
+
 	wret = fixup_low_keys(trans, root, path, &right->items[0].key, 1);
 	if (wret)
 		ret = wret;
@@ -1231,6 +1235,8 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 		path->slots[0] -= push_items;
 	}
 	BUG_ON(path->slots[0] < 0);
+	if (path->nodes[1])
+		check_node(root, path, 1);
 	return ret;
 }
 
@@ -1326,13 +1332,12 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 				wret = insert_ptr(trans, root, path,
 						  &disk_key,
 						  bh_blocknr(right_buffer),
-						  path->slots[1] - 1, 1);
+						  path->slots[1], 1);
 				if (wret)
 					ret = wret;
 				btrfs_block_release(root, path->nodes[0]);
 				path->nodes[0] = right_buffer;
 				path->slots[0] = 0;
-				path->slots[1] -= 1;
 				if (path->slots[1] == 0) {
 					wret = fixup_low_keys(trans, root,
 					           path, &disk_key, 1);
@@ -1379,6 +1384,7 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 	} else
 		btrfs_block_release(root, right_buffer);
 	BUG_ON(path->slots[0] < 0);
+	check_node(root, path, 1);
 
 	if (!double_split)
 		return ret;

commit 8d7be552a733e53135a91d2d28f57ab87dc0e889
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu May 10 11:24:42 2007 -0400

    Btrfs: fix check_node and check_leaf to use less cpu
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index dbd3f636dd3d..990d297a7a13 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -130,15 +130,17 @@ static int comp_keys(struct btrfs_disk_key *disk, struct btrfs_key *k2)
 static int check_node(struct btrfs_root *root, struct btrfs_path *path,
 		      int level)
 {
-	int i;
 	struct btrfs_node *parent = NULL;
 	struct btrfs_node *node = btrfs_buffer_node(path->nodes[level]);
 	int parent_slot;
+	int slot;
+	struct btrfs_key cpukey;
 	u32 nritems = btrfs_header_nritems(&node->header);
 
 	if (path->nodes[level + 1])
 		parent = btrfs_buffer_node(path->nodes[level + 1]);
 	parent_slot = path->slots[level + 1];
+	slot = path->slots[level];
 	BUG_ON(nritems == 0);
 	if (parent) {
 		struct btrfs_disk_key *parent_key;
@@ -149,10 +151,13 @@ static int check_node(struct btrfs_root *root, struct btrfs_path *path,
 		       btrfs_header_blocknr(&node->header));
 	}
 	BUG_ON(nritems > BTRFS_NODEPTRS_PER_BLOCK(root));
-	for (i = 0; nritems > 1 && i < nritems - 2; i++) {
-		struct btrfs_key cpukey;
-		btrfs_disk_key_to_cpu(&cpukey, &node->ptrs[i + 1].key);
-		BUG_ON(comp_keys(&node->ptrs[i].key, &cpukey) >= 0);
+	if (slot != 0) {
+		btrfs_disk_key_to_cpu(&cpukey, &node->ptrs[slot - 1].key);
+		BUG_ON(comp_keys(&node->ptrs[slot].key, &cpukey) <= 0);
+	}
+	if (slot < nritems - 1) {
+		btrfs_disk_key_to_cpu(&cpukey, &node->ptrs[slot + 1].key);
+		BUG_ON(comp_keys(&node->ptrs[slot].key, &cpukey) >= 0);
 	}
 	return 0;
 }
@@ -160,10 +165,12 @@ static int check_node(struct btrfs_root *root, struct btrfs_path *path,
 static int check_leaf(struct btrfs_root *root, struct btrfs_path *path,
 		      int level)
 {
-	int i;
 	struct btrfs_leaf *leaf = btrfs_buffer_leaf(path->nodes[level]);
 	struct btrfs_node *parent = NULL;
 	int parent_slot;
+	int slot = path->slots[0];
+	struct btrfs_key cpukey;
+
 	u32 nritems = btrfs_header_nritems(&leaf->header);
 
 	if (path->nodes[level + 1])
@@ -182,19 +189,20 @@ static int check_leaf(struct btrfs_root *root, struct btrfs_path *path,
 		BUG_ON(btrfs_node_blockptr(parent, parent_slot) !=
 		       btrfs_header_blocknr(&leaf->header));
 	}
-	for (i = 0; nritems > 1 && i < nritems - 2; i++) {
-		struct btrfs_key cpukey;
-		btrfs_disk_key_to_cpu(&cpukey, &leaf->items[i + 1].key);
-		BUG_ON(comp_keys(&leaf->items[i].key,
-		                 &cpukey) >= 0);
-		BUG_ON(btrfs_item_offset(leaf->items + i) !=
-			btrfs_item_end(leaf->items + i + 1));
-		if (i == 0) {
-			BUG_ON(btrfs_item_offset(leaf->items + i) +
-			       btrfs_item_size(leaf->items + i) !=
-			       BTRFS_LEAF_DATA_SIZE(root));
-		}
+	if (slot != 0) {
+		btrfs_disk_key_to_cpu(&cpukey, &leaf->items[slot - 1].key);
+		BUG_ON(comp_keys(&leaf->items[slot].key, &cpukey) <= 0);
+		BUG_ON(btrfs_item_offset(leaf->items + slot - 1) !=
+			btrfs_item_end(leaf->items + slot));
+	}
+	if (slot < nritems - 1) {
+		btrfs_disk_key_to_cpu(&cpukey, &leaf->items[slot + 1].key);
+		BUG_ON(comp_keys(&leaf->items[slot].key, &cpukey) >= 0);
+		BUG_ON(btrfs_item_offset(leaf->items + slot) !=
+			btrfs_item_end(leaf->items + slot + 1));
 	}
+	BUG_ON(btrfs_item_offset(leaf->items) +
+	       btrfs_item_size(leaf->items) != BTRFS_LEAF_DATA_SIZE(root));
 	return 0;
 }
 

commit 31f3c99b73483f7b738a886c552050cbd6128ff3
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Apr 30 15:25:45 2007 -0400

    Btrfs: allocator improvements, inode block groups
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index b4783bf8bf4f..dbd3f636dd3d 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -61,7 +61,7 @@ static int btrfs_cow_block(struct btrfs_trans_handle *trans, struct btrfs_root
 		*cow_ret = buf;
 		return 0;
 	}
-	cow = btrfs_alloc_free_block(trans, root);
+	cow = btrfs_alloc_free_block(trans, root, buf->b_blocknr);
 	cow_node = btrfs_buffer_node(cow);
 	if (buf->b_size != root->blocksize || cow->b_size != root->blocksize)
 		WARN_ON(1);
@@ -800,7 +800,7 @@ static int insert_new_root(struct btrfs_trans_handle *trans, struct btrfs_root
 	BUG_ON(path->nodes[level]);
 	BUG_ON(path->nodes[level-1] != root->node);
 
-	t = btrfs_alloc_free_block(trans, root);
+	t = btrfs_alloc_free_block(trans, root, root->node->b_blocknr);
 	c = btrfs_buffer_node(t);
 	memset(c, 0, root->blocksize);
 	btrfs_set_header_nritems(&c->header, 1);
@@ -905,7 +905,7 @@ static int split_node(struct btrfs_trans_handle *trans, struct btrfs_root
 	}
 
 	c_nritems = btrfs_header_nritems(&c->header);
-	split_buffer = btrfs_alloc_free_block(trans, root);
+	split_buffer = btrfs_alloc_free_block(trans, root, t->b_blocknr);
 	split = btrfs_buffer_node(split_buffer);
 	btrfs_set_header_flags(&split->header, btrfs_header_flags(&c->header));
 	btrfs_set_header_level(&split->header, btrfs_header_level(&c->header));
@@ -1277,7 +1277,7 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 	slot = path->slots[0];
 	nritems = btrfs_header_nritems(&l->header);
 	mid = (nritems + 1)/ 2;
-	right_buffer = btrfs_alloc_free_block(trans, root);
+	right_buffer = btrfs_alloc_free_block(trans, root, l_buf->b_blocknr);
 	BUG_ON(!right_buffer);
 	right = btrfs_buffer_leaf(right_buffer);
 	memset(&right->header, 0, sizeof(right->header));
@@ -1374,7 +1374,7 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 
 	if (!double_split)
 		return ret;
-	right_buffer = btrfs_alloc_free_block(trans, root);
+	right_buffer = btrfs_alloc_free_block(trans, root, l_buf->b_blocknr);
 	BUG_ON(!right_buffer);
 	right = btrfs_buffer_leaf(right_buffer);
 	memset(&right->header, 0, sizeof(right->header));

commit 4d775673091d43b39fa9d086071009f98dec289e
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Apr 20 20:23:12 2007 -0400

    Btrfs: add owner and type fields to the extents aand block headers
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 1bae035f5171..b4783bf8bf4f 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -68,6 +68,7 @@ static int btrfs_cow_block(struct btrfs_trans_handle *trans, struct btrfs_root
 	memcpy(cow_node, btrfs_buffer_node(buf), root->blocksize);
 	btrfs_set_header_blocknr(&cow_node->header, bh_blocknr(cow));
 	btrfs_set_header_generation(&cow_node->header, trans->transid);
+	btrfs_set_header_owner(&cow_node->header, root->root_key.objectid);
 	btrfs_inc_ref(trans, root, buf);
 	if (buf == root->node) {
 		root->node = cow;
@@ -806,6 +807,7 @@ static int insert_new_root(struct btrfs_trans_handle *trans, struct btrfs_root
 	btrfs_set_header_level(&c->header, level);
 	btrfs_set_header_blocknr(&c->header, bh_blocknr(t));
 	btrfs_set_header_generation(&c->header, trans->transid);
+	btrfs_set_header_owner(&c->header, root->root_key.objectid);
 	lower = btrfs_buffer_node(path->nodes[level-1]);
 	memcpy(c->header.fsid, root->fs_info->disk_super->fsid,
 	       sizeof(c->header.fsid));
@@ -909,6 +911,7 @@ static int split_node(struct btrfs_trans_handle *trans, struct btrfs_root
 	btrfs_set_header_level(&split->header, btrfs_header_level(&c->header));
 	btrfs_set_header_blocknr(&split->header, bh_blocknr(split_buffer));
 	btrfs_set_header_generation(&split->header, trans->transid);
+	btrfs_set_header_owner(&split->header, root->root_key.objectid);
 	memcpy(split->header.fsid, root->fs_info->disk_super->fsid,
 	       sizeof(split->header.fsid));
 	mid = (c_nritems + 1) / 2;
@@ -1280,6 +1283,7 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 	memset(&right->header, 0, sizeof(right->header));
 	btrfs_set_header_blocknr(&right->header, bh_blocknr(right_buffer));
 	btrfs_set_header_generation(&right->header, trans->transid);
+	btrfs_set_header_owner(&right->header, root->root_key.objectid);
 	btrfs_set_header_level(&right->header, 0);
 	memcpy(right->header.fsid, root->fs_info->disk_super->fsid,
 	       sizeof(right->header.fsid));
@@ -1376,6 +1380,7 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 	memset(&right->header, 0, sizeof(right->header));
 	btrfs_set_header_blocknr(&right->header, bh_blocknr(right_buffer));
 	btrfs_set_header_generation(&right->header, trans->transid);
+	btrfs_set_header_owner(&right->header, root->root_key.objectid);
 	btrfs_set_header_level(&right->header, 0);
 	memcpy(right->header.fsid, root->fs_info->disk_super->fsid,
 	       sizeof(right->header.fsid));

commit 33ade1f826a7c348856a98930814f33ced6d1337
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Apr 20 13:48:57 2007 -0400

    Btrfs: node balance optimizations
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 744fd728e5d9..1bae035f5171 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -151,11 +151,6 @@ static int check_node(struct btrfs_root *root, struct btrfs_path *path,
 	for (i = 0; nritems > 1 && i < nritems - 2; i++) {
 		struct btrfs_key cpukey;
 		btrfs_disk_key_to_cpu(&cpukey, &node->ptrs[i + 1].key);
-if (comp_keys(&node->ptrs[i].key, &cpukey) >= 0) {
-	struct btrfs_key bad;
-	btrfs_disk_key_to_cpu(&bad, &node->ptrs[i].key);
-printk("check_node level %d i is %d bad comp %Lu %u %Lu, %Lu %u %Lu\n",level, i, bad.objectid, bad.flags, bad.offset, cpukey.objectid, cpukey.flags, cpukey.offset);
-}
 		BUG_ON(comp_keys(&node->ptrs[i].key, &cpukey) >= 0);
 	}
 	return 0;
@@ -492,11 +487,16 @@ static int push_nodes_for_insert(struct btrfs_trans_handle *trans,
 	/* first, try to make some room in the middle buffer */
 	if (left_buf) {
 		u32 left_nr;
-		btrfs_cow_block(trans, root, left_buf, parent_buf, pslot - 1,
-				&left_buf);
 		left = btrfs_buffer_node(left_buf);
 		left_nr = btrfs_header_nritems(&left->header);
-		wret = push_node_left(trans, root, left_buf, mid_buf);
+		if (left_nr >= BTRFS_NODEPTRS_PER_BLOCK(root) - 1) {
+			wret = 1;
+		} else {
+			btrfs_cow_block(trans, root, left_buf, parent_buf,
+					pslot - 1, &left_buf);
+			left = btrfs_buffer_node(left_buf);
+			wret = push_node_left(trans, root, left_buf, mid_buf);
+		}
 		if (wret < 0)
 			ret = wret;
 		if (wret == 0) {
@@ -528,10 +528,18 @@ static int push_nodes_for_insert(struct btrfs_trans_handle *trans,
 	 * then try to empty the right most buffer into the middle
 	 */
 	if (right_buf) {
-		btrfs_cow_block(trans, root, right_buf, parent_buf, pslot + 1,
-				&right_buf);
+		u32 right_nr;
 		right = btrfs_buffer_node(right_buf);
-		wret = balance_node_right(trans, root, right_buf, mid_buf);
+		right_nr = btrfs_header_nritems(&right->header);
+		if (right_nr >= BTRFS_NODEPTRS_PER_BLOCK(root) - 1) {
+			wret = 1;
+		} else {
+			btrfs_cow_block(trans, root, right_buf,
+					parent_buf, pslot + 1, &right_buf);
+			right = btrfs_buffer_node(right_buf);
+			wret = balance_node_right(trans, root,
+						  right_buf, mid_buf);
+		}
 		if (wret < 0)
 			ret = wret;
 		if (wret == 0) {

commit e66f709b157ee8557166c14b67c01bae978ac32e
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Apr 20 13:16:02 2007 -0400

    Btrfs: write barriers on commit, balance level before split
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 4efcd1bd63e5..744fd728e5d9 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -151,6 +151,11 @@ static int check_node(struct btrfs_root *root, struct btrfs_path *path,
 	for (i = 0; nritems > 1 && i < nritems - 2; i++) {
 		struct btrfs_key cpukey;
 		btrfs_disk_key_to_cpu(&cpukey, &node->ptrs[i + 1].key);
+if (comp_keys(&node->ptrs[i].key, &cpukey) >= 0) {
+	struct btrfs_key bad;
+	btrfs_disk_key_to_cpu(&bad, &node->ptrs[i].key);
+printk("check_node level %d i is %d bad comp %Lu %u %Lu, %Lu %u %Lu\n",level, i, bad.objectid, bad.flags, bad.offset, cpukey.objectid, cpukey.flags, cpukey.offset);
+}
 		BUG_ON(comp_keys(&node->ptrs[i].key, &cpukey) >= 0);
 	}
 	return 0;
@@ -448,6 +453,111 @@ static int balance_level(struct btrfs_trans_handle *trans, struct btrfs_root
 	return ret;
 }
 
+/* returns zero if the push worked, non-zero otherwise */
+static int push_nodes_for_insert(struct btrfs_trans_handle *trans,
+				struct btrfs_root *root,
+				struct btrfs_path *path, int level)
+{
+	struct buffer_head *right_buf;
+	struct buffer_head *mid_buf;
+	struct buffer_head *left_buf;
+	struct buffer_head *parent_buf = NULL;
+	struct btrfs_node *right = NULL;
+	struct btrfs_node *mid;
+	struct btrfs_node *left = NULL;
+	struct btrfs_node *parent = NULL;
+	int ret = 0;
+	int wret;
+	int pslot;
+	int orig_slot = path->slots[level];
+	u64 orig_ptr;
+
+	if (level == 0)
+		return 1;
+
+	mid_buf = path->nodes[level];
+	mid = btrfs_buffer_node(mid_buf);
+	orig_ptr = btrfs_node_blockptr(mid, orig_slot);
+
+	if (level < BTRFS_MAX_LEVEL - 1)
+		parent_buf = path->nodes[level + 1];
+	pslot = path->slots[level + 1];
+
+	if (!parent_buf)
+		return 1;
+	parent = btrfs_buffer_node(parent_buf);
+
+	left_buf = read_node_slot(root, parent_buf, pslot - 1);
+
+	/* first, try to make some room in the middle buffer */
+	if (left_buf) {
+		u32 left_nr;
+		btrfs_cow_block(trans, root, left_buf, parent_buf, pslot - 1,
+				&left_buf);
+		left = btrfs_buffer_node(left_buf);
+		left_nr = btrfs_header_nritems(&left->header);
+		wret = push_node_left(trans, root, left_buf, mid_buf);
+		if (wret < 0)
+			ret = wret;
+		if (wret == 0) {
+			orig_slot += left_nr;
+			btrfs_memcpy(root, parent,
+				     &parent->ptrs[pslot].key,
+				     &mid->ptrs[0].key,
+				     sizeof(struct btrfs_disk_key));
+			btrfs_mark_buffer_dirty(parent_buf);
+			if (btrfs_header_nritems(&left->header) > orig_slot) {
+				path->nodes[level] = left_buf;
+				path->slots[level + 1] -= 1;
+				path->slots[level] = orig_slot;
+				btrfs_block_release(root, mid_buf);
+			} else {
+				orig_slot -=
+					btrfs_header_nritems(&left->header);
+				path->slots[level] = orig_slot;
+				btrfs_block_release(root, left_buf);
+			}
+			check_node(root, path, level);
+			return 0;
+		}
+		btrfs_block_release(root, left_buf);
+	}
+	right_buf = read_node_slot(root, parent_buf, pslot + 1);
+
+	/*
+	 * then try to empty the right most buffer into the middle
+	 */
+	if (right_buf) {
+		btrfs_cow_block(trans, root, right_buf, parent_buf, pslot + 1,
+				&right_buf);
+		right = btrfs_buffer_node(right_buf);
+		wret = balance_node_right(trans, root, right_buf, mid_buf);
+		if (wret < 0)
+			ret = wret;
+		if (wret == 0) {
+			btrfs_memcpy(root, parent,
+				     &parent->ptrs[pslot + 1].key,
+				     &right->ptrs[0].key,
+				     sizeof(struct btrfs_disk_key));
+			btrfs_mark_buffer_dirty(parent_buf);
+			if (btrfs_header_nritems(&mid->header) <= orig_slot) {
+				path->nodes[level] = right_buf;
+				path->slots[level + 1] += 1;
+				path->slots[level] = orig_slot -
+					btrfs_header_nritems(&mid->header);
+				btrfs_block_release(root, mid_buf);
+			} else {
+				btrfs_block_release(root, right_buf);
+			}
+			check_node(root, path, level);
+			return 0;
+		}
+		btrfs_block_release(root, right_buf);
+	}
+	check_node(root, path, level);
+	return 1;
+}
+
 /*
  * look for key in the tree.  path is filled in with nodes along the way
  * if key is found, we return zero and you can find the item in the leaf
@@ -774,7 +884,16 @@ static int split_node(struct btrfs_trans_handle *trans, struct btrfs_root
 		ret = insert_new_root(trans, root, path, level + 1);
 		if (ret)
 			return ret;
+	} else {
+		ret = push_nodes_for_insert(trans, root, path, level);
+		t = path->nodes[level];
+		c = btrfs_buffer_node(t);
+		if (!ret &&
+		    btrfs_header_nritems(&c->header) <
+		    BTRFS_NODEPTRS_PER_BLOCK(root) - 1)
+			return 0;
 	}
+
 	c_nritems = btrfs_header_nritems(&c->header);
 	split_buffer = btrfs_alloc_free_block(trans, root);
 	split = btrfs_buffer_node(split_buffer);

commit a429e51371eee3c989160c003ee40bc3947c6a76
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Apr 18 16:15:28 2007 -0400

    Btrfs: working file_write, reorganized key flags
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 5c160c236773..4efcd1bd63e5 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -893,7 +893,11 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 	}
 
 	left_nritems = btrfs_header_nritems(&left->header);
-	for (i = left_nritems - 1; i >= 0; i--) {
+	if (left_nritems == 0) {
+		btrfs_block_release(root, right_buf);
+		return 1;
+	}
+	for (i = left_nritems - 1; i >= 1; i--) {
 		item = left->items + i;
 		if (path->slots[0] == i)
 			push_space += data_size + sizeof(*item);
@@ -907,6 +911,8 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 		btrfs_block_release(root, right_buf);
 		return 1;
 	}
+	if (push_items == left_nritems)
+		WARN_ON(1);
 	right_nritems = btrfs_header_nritems(&right->header);
 	/* push left to right */
 	push_space = btrfs_item_end(left->items + left_nritems - push_items);
@@ -943,6 +949,7 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 
 	btrfs_mark_buffer_dirty(left_buf);
 	btrfs_mark_buffer_dirty(right_buf);
+
 	btrfs_memcpy(root, upper_node, &upper_node->ptrs[slot + 1].key,
 		&right->items[0].key, sizeof(struct btrfs_disk_key));
 	btrfs_mark_buffer_dirty(upper);
@@ -1004,7 +1011,12 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 		return 1;
 	}
 
-	for (i = 0; i < btrfs_header_nritems(&right->header); i++) {
+	if (btrfs_header_nritems(&right->header) == 0) {
+		btrfs_block_release(root, t);
+		return 1;
+	}
+
+	for (i = 0; i < btrfs_header_nritems(&right->header) - 1; i++) {
 		item = right->items + i;
 		if (path->slots[0] == i)
 			push_space += data_size + sizeof(*item);
@@ -1018,6 +1030,8 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 		btrfs_block_release(root, t);
 		return 1;
 	}
+	if (push_items == btrfs_header_nritems(&right->header))
+		WARN_ON(1);
 	/* push data from right to left */
 	btrfs_memcpy(root, left, left->items +
 		     btrfs_header_nritems(&left->header),
@@ -1064,7 +1078,6 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 
 	btrfs_mark_buffer_dirty(t);
 	btrfs_mark_buffer_dirty(right_buf);
-
 	wret = fixup_low_keys(trans, root, path, &right->items[0].key, 1);
 	if (wret)
 		ret = wret;
@@ -1181,6 +1194,12 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 				path->nodes[0] = right_buffer;
 				path->slots[0] = 0;
 				path->slots[1] -= 1;
+				if (path->slots[1] == 0) {
+					wret = fixup_low_keys(trans, root,
+					           path, &disk_key, 1);
+					if (wret)
+						ret = wret;
+				}
 				return ret;
 			}
 			mid = slot;
@@ -1241,6 +1260,11 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 			  path->slots[1], 1);
 	if (wret)
 		ret = wret;
+	if (path->slots[1] == 0) {
+		wret = fixup_low_keys(trans, root, path, &disk_key, 1);
+		if (wret)
+			ret = wret;
+	}
 	btrfs_block_release(root, path->nodes[0]);
 	path->nodes[0] = right_buffer;
 	path->slots[0] = 0;

commit 70b2befd0c8a4064715d8b340270650cc9d15af8
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Apr 17 15:39:32 2007 -0400

    Btrfs: rework csums and extent item ordering
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 43d4779abdd1..5c160c236773 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -115,14 +115,14 @@ static int comp_keys(struct btrfs_disk_key *disk, struct btrfs_key *k2)
 		return 1;
 	if (k1.objectid < k2->objectid)
 		return -1;
-	if (k1.offset > k2->offset)
-		return 1;
-	if (k1.offset < k2->offset)
-		return -1;
 	if (k1.flags > k2->flags)
 		return 1;
 	if (k1.flags < k2->flags)
 		return -1;
+	if (k1.offset > k2->offset)
+		return 1;
+	if (k1.offset < k2->offset)
+		return -1;
 	return 0;
 }
 
@@ -1292,7 +1292,6 @@ int btrfs_truncate_item(struct btrfs_trans_handle *trans,
 				      ioff + size_diff);
 	}
 	/* shift the data */
-printk("truncate item, new_size %u old_size %u, diff %u, bufp %p, dst, %p, num %u, old_data_start %u, data_end %u\n", new_size, old_size, size_diff, leaf, btrfs_leaf_data(leaf) + data_end + size_diff, old_data_start-data_end, old_data_start, data_end);
 	btrfs_memmove(root, leaf, btrfs_leaf_data(leaf) +
 		      data_end + size_diff, btrfs_leaf_data(leaf) +
 		      data_end, old_data_start + new_size - data_end);

commit b18c6685810af8e6763760711aece31ccc7a8ea8
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Apr 17 13:26:50 2007 -0400

    Btrfs: progress on file_write
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 6b76fd967153..43d4779abdd1 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1249,6 +1249,63 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 	return ret;
 }
 
+int btrfs_truncate_item(struct btrfs_trans_handle *trans,
+			struct btrfs_root *root,
+			struct btrfs_path *path,
+			u32 new_size)
+{
+	int ret = 0;
+	int slot;
+	int slot_orig;
+	struct btrfs_leaf *leaf;
+	struct buffer_head *leaf_buf;
+	u32 nritems;
+	unsigned int data_end;
+	unsigned int old_data_start;
+	unsigned int old_size;
+	unsigned int size_diff;
+	int i;
+
+	slot_orig = path->slots[0];
+	leaf_buf = path->nodes[0];
+	leaf = btrfs_buffer_leaf(leaf_buf);
+
+	nritems = btrfs_header_nritems(&leaf->header);
+	data_end = leaf_data_end(root, leaf);
+
+	slot = path->slots[0];
+	old_data_start = btrfs_item_offset(leaf->items + slot);
+	old_size = btrfs_item_size(leaf->items + slot);
+	BUG_ON(old_size <= new_size);
+	size_diff = old_size - new_size;
+
+	BUG_ON(slot < 0);
+	BUG_ON(slot >= nritems);
+
+	/*
+	 * item0..itemN ... dataN.offset..dataN.size .. data0.size
+	 */
+	/* first correct the data pointers */
+	for (i = slot; i < nritems; i++) {
+		u32 ioff = btrfs_item_offset(leaf->items + i);
+		btrfs_set_item_offset(leaf->items + i,
+				      ioff + size_diff);
+	}
+	/* shift the data */
+printk("truncate item, new_size %u old_size %u, diff %u, bufp %p, dst, %p, num %u, old_data_start %u, data_end %u\n", new_size, old_size, size_diff, leaf, btrfs_leaf_data(leaf) + data_end + size_diff, old_data_start-data_end, old_data_start, data_end);
+	btrfs_memmove(root, leaf, btrfs_leaf_data(leaf) +
+		      data_end + size_diff, btrfs_leaf_data(leaf) +
+		      data_end, old_data_start + new_size - data_end);
+	btrfs_set_item_size(leaf->items + slot, new_size);
+	btrfs_mark_buffer_dirty(leaf_buf);
+
+	ret = 0;
+	if (btrfs_leaf_free_space(root, leaf) < 0)
+		BUG();
+	check_leaf(root, path, 0);
+	return ret;
+}
+
 int btrfs_extend_item(struct btrfs_trans_handle *trans, struct btrfs_root
 		      *root, struct btrfs_path *path, u32 data_size)
 {

commit 6567e837df07e43bffc08ac40858af8133a007bf
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Apr 16 09:22:45 2007 -0400

    Btrfs: early work to file_write in big extents
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index e2fc6f0d0243..6b76fd967153 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1249,6 +1249,60 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 	return ret;
 }
 
+int btrfs_extend_item(struct btrfs_trans_handle *trans, struct btrfs_root
+		      *root, struct btrfs_path *path, u32 data_size)
+{
+	int ret = 0;
+	int slot;
+	int slot_orig;
+	struct btrfs_leaf *leaf;
+	struct buffer_head *leaf_buf;
+	u32 nritems;
+	unsigned int data_end;
+	unsigned int old_data;
+	unsigned int old_size;
+	int i;
+
+	slot_orig = path->slots[0];
+	leaf_buf = path->nodes[0];
+	leaf = btrfs_buffer_leaf(leaf_buf);
+
+	nritems = btrfs_header_nritems(&leaf->header);
+	data_end = leaf_data_end(root, leaf);
+
+	if (btrfs_leaf_free_space(root, leaf) < data_size)
+		BUG();
+	slot = path->slots[0];
+	old_data = btrfs_item_end(leaf->items + slot);
+
+	BUG_ON(slot < 0);
+	BUG_ON(slot >= nritems);
+
+	/*
+	 * item0..itemN ... dataN.offset..dataN.size .. data0.size
+	 */
+	/* first correct the data pointers */
+	for (i = slot; i < nritems; i++) {
+		u32 ioff = btrfs_item_offset(leaf->items + i);
+		btrfs_set_item_offset(leaf->items + i,
+				      ioff - data_size);
+	}
+	/* shift the data */
+	btrfs_memmove(root, leaf, btrfs_leaf_data(leaf) +
+		      data_end - data_size, btrfs_leaf_data(leaf) +
+		      data_end, old_data - data_end);
+	data_end = old_data;
+	old_size = btrfs_item_size(leaf->items + slot);
+	btrfs_set_item_size(leaf->items + slot, old_size + data_size);
+	btrfs_mark_buffer_dirty(leaf_buf);
+
+	ret = 0;
+	if (btrfs_leaf_free_space(root, leaf) < 0)
+		BUG();
+	check_leaf(root, path, 0);
+	return ret;
+}
+
 /*
  * Given a key and some data, insert an item into the tree.
  * This does all the path init required, making room in the tree if needed.

commit 7eccb903a817e890c947ba4bc90c6a9af9b4219a
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Apr 11 15:53:25 2007 -0400

    Btrfs: create a logical->phsyical block number mapping scheme
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index f31ac843bf2e..e2fc6f0d0243 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -66,21 +66,21 @@ static int btrfs_cow_block(struct btrfs_trans_handle *trans, struct btrfs_root
 	if (buf->b_size != root->blocksize || cow->b_size != root->blocksize)
 		WARN_ON(1);
 	memcpy(cow_node, btrfs_buffer_node(buf), root->blocksize);
-	btrfs_set_header_blocknr(&cow_node->header, cow->b_blocknr);
+	btrfs_set_header_blocknr(&cow_node->header, bh_blocknr(cow));
 	btrfs_set_header_generation(&cow_node->header, trans->transid);
 	btrfs_inc_ref(trans, root, buf);
 	if (buf == root->node) {
 		root->node = cow;
 		get_bh(cow);
 		if (buf != root->commit_root) {
-			btrfs_free_extent(trans, root, buf->b_blocknr, 1, 1);
+			btrfs_free_extent(trans, root, bh_blocknr(buf), 1, 1);
 		}
 		btrfs_block_release(root, buf);
 	} else {
 		btrfs_set_node_blockptr(btrfs_buffer_node(parent), parent_slot,
-					cow->b_blocknr);
+					bh_blocknr(cow));
 		btrfs_mark_buffer_dirty(parent);
-		btrfs_free_extent(trans, root, buf->b_blocknr, 1, 1);
+		btrfs_free_extent(trans, root, bh_blocknr(buf), 1, 1);
 	}
 	btrfs_block_release(root, buf);
 	mark_buffer_dirty(cow);
@@ -312,7 +312,7 @@ static int balance_level(struct btrfs_trans_handle *trans, struct btrfs_root
 	 */
 	if (!parent_buf) {
 		struct buffer_head *child;
-		u64 blocknr = mid_buf->b_blocknr;
+		u64 blocknr = bh_blocknr(mid_buf);
 
 		if (btrfs_header_nritems(&mid->header) != 1)
 			return 0;
@@ -361,7 +361,7 @@ static int balance_level(struct btrfs_trans_handle *trans, struct btrfs_root
 		if (wret < 0)
 			ret = wret;
 		if (btrfs_header_nritems(&right->header) == 0) {
-			u64 blocknr = right_buf->b_blocknr;
+			u64 blocknr = bh_blocknr(right_buf);
 			clean_tree_block(trans, root, right_buf);
 			wait_on_buffer(right_buf);
 			btrfs_block_release(root, right_buf);
@@ -400,7 +400,7 @@ static int balance_level(struct btrfs_trans_handle *trans, struct btrfs_root
 	}
 	if (btrfs_header_nritems(&mid->header) == 0) {
 		/* we've managed to empty the middle node, drop it */
-		u64 blocknr = mid_buf->b_blocknr;
+		u64 blocknr = bh_blocknr(mid_buf);
 		clean_tree_block(trans, root, mid_buf);
 		wait_on_buffer(mid_buf);
 		btrfs_block_release(root, mid_buf);
@@ -686,7 +686,7 @@ static int insert_new_root(struct btrfs_trans_handle *trans, struct btrfs_root
 	memset(c, 0, root->blocksize);
 	btrfs_set_header_nritems(&c->header, 1);
 	btrfs_set_header_level(&c->header, level);
-	btrfs_set_header_blocknr(&c->header, t->b_blocknr);
+	btrfs_set_header_blocknr(&c->header, bh_blocknr(t));
 	btrfs_set_header_generation(&c->header, trans->transid);
 	lower = btrfs_buffer_node(path->nodes[level-1]);
 	memcpy(c->header.fsid, root->fs_info->disk_super->fsid,
@@ -697,7 +697,7 @@ static int insert_new_root(struct btrfs_trans_handle *trans, struct btrfs_root
 		lower_key = &lower->ptrs[0].key;
 	btrfs_memcpy(root, c, &c->ptrs[0].key, lower_key,
 		     sizeof(struct btrfs_disk_key));
-	btrfs_set_node_blockptr(c, 0, path->nodes[level - 1]->b_blocknr);
+	btrfs_set_node_blockptr(c, 0, bh_blocknr(path->nodes[level - 1]));
 
 	btrfs_mark_buffer_dirty(t);
 
@@ -780,7 +780,7 @@ static int split_node(struct btrfs_trans_handle *trans, struct btrfs_root
 	split = btrfs_buffer_node(split_buffer);
 	btrfs_set_header_flags(&split->header, btrfs_header_flags(&c->header));
 	btrfs_set_header_level(&split->header, btrfs_header_level(&c->header));
-	btrfs_set_header_blocknr(&split->header, split_buffer->b_blocknr);
+	btrfs_set_header_blocknr(&split->header, bh_blocknr(split_buffer));
 	btrfs_set_header_generation(&split->header, trans->transid);
 	memcpy(split->header.fsid, root->fs_info->disk_super->fsid,
 	       sizeof(split->header.fsid));
@@ -794,7 +794,7 @@ static int split_node(struct btrfs_trans_handle *trans, struct btrfs_root
 	btrfs_mark_buffer_dirty(t);
 	btrfs_mark_buffer_dirty(split_buffer);
 	wret = insert_ptr(trans, root, path, &split->ptrs[0].key,
-			  split_buffer->b_blocknr, path->slots[level + 1] + 1,
+			  bh_blocknr(split_buffer), path->slots[level + 1] + 1,
 			  level + 1);
 	if (wret)
 		ret = wret;
@@ -1138,7 +1138,7 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 	BUG_ON(!right_buffer);
 	right = btrfs_buffer_leaf(right_buffer);
 	memset(&right->header, 0, sizeof(right->header));
-	btrfs_set_header_blocknr(&right->header, right_buffer->b_blocknr);
+	btrfs_set_header_blocknr(&right->header, bh_blocknr(right_buffer));
 	btrfs_set_header_generation(&right->header, trans->transid);
 	btrfs_set_header_level(&right->header, 0);
 	memcpy(right->header.fsid, root->fs_info->disk_super->fsid,
@@ -1152,7 +1152,7 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 				btrfs_set_header_nritems(&right->header, 0);
 				wret = insert_ptr(trans, root, path,
 						  &disk_key,
-						  right_buffer->b_blocknr,
+						  bh_blocknr(right_buffer),
 						  path->slots[1] + 1, 1);
 				if (wret)
 					ret = wret;
@@ -1173,7 +1173,7 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 				btrfs_set_header_nritems(&right->header, 0);
 				wret = insert_ptr(trans, root, path,
 						  &disk_key,
-						  right_buffer->b_blocknr,
+						  bh_blocknr(right_buffer),
 						  path->slots[1] - 1, 1);
 				if (wret)
 					ret = wret;
@@ -1207,7 +1207,7 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 	btrfs_set_header_nritems(&l->header, mid);
 	ret = 0;
 	wret = insert_ptr(trans, root, path, &right->items[0].key,
-			  right_buffer->b_blocknr, path->slots[1] + 1, 1);
+			  bh_blocknr(right_buffer), path->slots[1] + 1, 1);
 	if (wret)
 		ret = wret;
 	btrfs_mark_buffer_dirty(right_buffer);
@@ -1228,7 +1228,7 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 	BUG_ON(!right_buffer);
 	right = btrfs_buffer_leaf(right_buffer);
 	memset(&right->header, 0, sizeof(right->header));
-	btrfs_set_header_blocknr(&right->header, right_buffer->b_blocknr);
+	btrfs_set_header_blocknr(&right->header, bh_blocknr(right_buffer));
 	btrfs_set_header_generation(&right->header, trans->transid);
 	btrfs_set_header_level(&right->header, 0);
 	memcpy(right->header.fsid, root->fs_info->disk_super->fsid,
@@ -1237,7 +1237,7 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 	btrfs_set_header_nritems(&right->header, 0);
 	wret = insert_ptr(trans, root, path,
 			  &disk_key,
-			  right_buffer->b_blocknr,
+			  bh_blocknr(right_buffer),
 			  path->slots[1], 1);
 	if (wret)
 		ret = wret;
@@ -1456,7 +1456,7 @@ int btrfs_del_item(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 			if (wret)
 				ret = wret;
 			wret = btrfs_free_extent(trans, root,
-						 leaf_buf->b_blocknr, 1, 1);
+						 bh_blocknr(leaf_buf), 1, 1);
 			if (wret)
 				ret = wret;
 		}
@@ -1487,7 +1487,7 @@ int btrfs_del_item(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 					ret = wret;
 			}
 			if (btrfs_header_nritems(&leaf->header) == 0) {
-				u64 blocknr = leaf_buf->b_blocknr;
+				u64 blocknr = bh_blocknr(leaf_buf);
 				clean_tree_block(trans, root, leaf_buf);
 				wait_on_buffer(leaf_buf);
 				wret = del_ptr(trans, root, path, 1, slot);

commit d0dbc6245cefa36e19dff49c557ccf05e3063e9c
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Apr 10 12:36:36 2007 -0400

    Btrfs: drop owner and parentid
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 39b551564fcf..f31ac843bf2e 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -688,8 +688,6 @@ static int insert_new_root(struct btrfs_trans_handle *trans, struct btrfs_root
 	btrfs_set_header_level(&c->header, level);
 	btrfs_set_header_blocknr(&c->header, t->b_blocknr);
 	btrfs_set_header_generation(&c->header, trans->transid);
-	btrfs_set_header_parentid(&c->header,
-	      btrfs_header_parentid(btrfs_buffer_header(root->node)));
 	lower = btrfs_buffer_node(path->nodes[level-1]);
 	memcpy(c->header.fsid, root->fs_info->disk_super->fsid,
 	       sizeof(c->header.fsid));
@@ -784,8 +782,6 @@ static int split_node(struct btrfs_trans_handle *trans, struct btrfs_root
 	btrfs_set_header_level(&split->header, btrfs_header_level(&c->header));
 	btrfs_set_header_blocknr(&split->header, split_buffer->b_blocknr);
 	btrfs_set_header_generation(&split->header, trans->transid);
-	btrfs_set_header_parentid(&split->header,
-	      btrfs_header_parentid(btrfs_buffer_header(root->node)));
 	memcpy(split->header.fsid, root->fs_info->disk_super->fsid,
 	       sizeof(split->header.fsid));
 	mid = (c_nritems + 1) / 2;
@@ -1145,8 +1141,6 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 	btrfs_set_header_blocknr(&right->header, right_buffer->b_blocknr);
 	btrfs_set_header_generation(&right->header, trans->transid);
 	btrfs_set_header_level(&right->header, 0);
-	btrfs_set_header_parentid(&right->header,
-	      btrfs_header_parentid(btrfs_buffer_header(root->node)));
 	memcpy(right->header.fsid, root->fs_info->disk_super->fsid,
 	       sizeof(right->header.fsid));
 	if (mid <= slot) {
@@ -1237,8 +1231,6 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 	btrfs_set_header_blocknr(&right->header, right_buffer->b_blocknr);
 	btrfs_set_header_generation(&right->header, trans->transid);
 	btrfs_set_header_level(&right->header, 0);
-	btrfs_set_header_parentid(&right->header,
-	      btrfs_header_parentid(btrfs_buffer_header(root->node)));
 	memcpy(right->header.fsid, root->fs_info->disk_super->fsid,
 	       sizeof(right->header.fsid));
 	btrfs_cpu_key_to_disk(&disk_key, ins_key);

commit 3eb0314dc1053b1ae617dcc8d6d93f776c5baa31
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Apr 5 14:28:50 2007 -0400

    Btrfs: uuids
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 864ee423b300..39b551564fcf 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -200,6 +200,10 @@ static int check_leaf(struct btrfs_root *root, struct btrfs_path *path,
 static int check_block(struct btrfs_root *root, struct btrfs_path *path,
 			int level)
 {
+	struct btrfs_node *node = btrfs_buffer_node(path->nodes[level]);
+	if (memcmp(node->header.fsid, root->fs_info->disk_super->fsid,
+		   sizeof(node->header.fsid)))
+		BUG();
 	if (level == 0)
 		return check_leaf(root, path, level);
 	return check_node(root, path, level);
@@ -687,6 +691,8 @@ static int insert_new_root(struct btrfs_trans_handle *trans, struct btrfs_root
 	btrfs_set_header_parentid(&c->header,
 	      btrfs_header_parentid(btrfs_buffer_header(root->node)));
 	lower = btrfs_buffer_node(path->nodes[level-1]);
+	memcpy(c->header.fsid, root->fs_info->disk_super->fsid,
+	       sizeof(c->header.fsid));
 	if (btrfs_is_leaf(lower))
 		lower_key = &((struct btrfs_leaf *)lower)->items[0].key;
 	else
@@ -780,6 +786,8 @@ static int split_node(struct btrfs_trans_handle *trans, struct btrfs_root
 	btrfs_set_header_generation(&split->header, trans->transid);
 	btrfs_set_header_parentid(&split->header,
 	      btrfs_header_parentid(btrfs_buffer_header(root->node)));
+	memcpy(split->header.fsid, root->fs_info->disk_super->fsid,
+	       sizeof(split->header.fsid));
 	mid = (c_nritems + 1) / 2;
 	btrfs_memcpy(root, split, split->ptrs, c->ptrs + mid,
 		     (c_nritems - mid) * sizeof(struct btrfs_key_ptr));
@@ -1139,6 +1147,8 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 	btrfs_set_header_level(&right->header, 0);
 	btrfs_set_header_parentid(&right->header,
 	      btrfs_header_parentid(btrfs_buffer_header(root->node)));
+	memcpy(right->header.fsid, root->fs_info->disk_super->fsid,
+	       sizeof(right->header.fsid));
 	if (mid <= slot) {
 		if (nritems == 1 ||
 		    leaf_space_used(l, mid, nritems - mid) + space_needed >
@@ -1229,6 +1239,8 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 	btrfs_set_header_level(&right->header, 0);
 	btrfs_set_header_parentid(&right->header,
 	      btrfs_header_parentid(btrfs_buffer_header(root->node)));
+	memcpy(right->header.fsid, root->fs_info->disk_super->fsid,
+	       sizeof(right->header.fsid));
 	btrfs_cpu_key_to_disk(&disk_key, ins_key);
 	btrfs_set_header_nritems(&right->header, 0);
 	wret = insert_ptr(trans, root, path,

commit d4dbff953e1f6f4079126c0404cc24f2ef14e925
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Apr 4 14:08:15 2007 -0400

    Btrfs: support for items bigger than 1/2 the blocksize
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 9ef65e2ecf51..864ee423b300 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -6,7 +6,8 @@
 static int split_node(struct btrfs_trans_handle *trans, struct btrfs_root
 		      *root, struct btrfs_path *path, int level);
 static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
-		      *root, struct btrfs_path *path, int data_size);
+		      *root, struct btrfs_key *ins_key,
+		      struct btrfs_path *path, int data_size);
 static int push_node_left(struct btrfs_trans_handle *trans, struct btrfs_root
 			  *root, struct buffer_head *dst, struct buffer_head
 			  *src);
@@ -101,19 +102,6 @@ static inline unsigned int leaf_data_end(struct btrfs_root *root,
 	return btrfs_item_offset(leaf->items + nr - 1);
 }
 
-/*
- * The space between the end of the leaf items and
- * the start of the leaf data.  IOW, how much room
- * the leaf has left for both items and data
- */
-int btrfs_leaf_free_space(struct btrfs_root *root, struct btrfs_leaf *leaf)
-{
-	int data_end = leaf_data_end(root, leaf);
-	int nritems = btrfs_header_nritems(&leaf->header);
-	char *items_end = (char *)(leaf->items + nritems + 1);
-	return (char *)(btrfs_leaf_data(leaf) + data_end) - (char *)items_end;
-}
-
 /*
  * compare two keys in a memcmp fashion
  */
@@ -510,8 +498,8 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 			if (ret && slot > 0)
 				slot -= 1;
 			p->slots[level] = slot;
-			if (ins_len > 0 && btrfs_header_nritems(&c->header) ==
-			    BTRFS_NODEPTRS_PER_BLOCK(root)) {
+			if (ins_len > 0 && btrfs_header_nritems(&c->header) >=
+			    BTRFS_NODEPTRS_PER_BLOCK(root) - 1) {
 				int sret = split_node(trans, root, p, level);
 				BUG_ON(sret > 0);
 				if (sret)
@@ -537,7 +525,8 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 			p->slots[level] = slot;
 			if (ins_len > 0 && btrfs_leaf_free_space(root, l) <
 			    sizeof(struct btrfs_item) + ins_len) {
-				int sret = split_leaf(trans, root, p, ins_len);
+				int sret = split_leaf(trans, root, key,
+						      p, ins_len);
 				BUG_ON(sret > 0);
 				if (sret)
 					return sret;
@@ -825,16 +814,29 @@ static int split_node(struct btrfs_trans_handle *trans, struct btrfs_root
 static int leaf_space_used(struct btrfs_leaf *l, int start, int nr)
 {
 	int data_len;
-	int end = start + nr - 1;
+	int nritems = btrfs_header_nritems(&l->header);
+	int end = min(nritems, start + nr) - 1;
 
 	if (!nr)
 		return 0;
 	data_len = btrfs_item_end(l->items + start);
 	data_len = data_len - btrfs_item_offset(l->items + end);
 	data_len += sizeof(struct btrfs_item) * nr;
+	WARN_ON(data_len < 0);
 	return data_len;
 }
 
+/*
+ * The space between the end of the leaf items and
+ * the start of the leaf data.  IOW, how much room
+ * the leaf has left for both items and data
+ */
+int btrfs_leaf_free_space(struct btrfs_root *root, struct btrfs_leaf *leaf)
+{
+	int nritems = btrfs_header_nritems(&leaf->header);
+	return BTRFS_LEAF_DATA_SIZE(root) - leaf_space_used(leaf, 0, nritems);
+}
+
 /*
  * push some data in the path leaf to the right, trying to free up at
  * least data_size bytes.  returns zero if the push worked, nonzero otherwise
@@ -1084,7 +1086,8 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
  * returns 0 if all went well and < 0 on failure.
  */
 static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
-		      *root, struct btrfs_path *path, int data_size)
+		      *root, struct btrfs_key *ins_key,
+		      struct btrfs_path *path, int data_size)
 {
 	struct buffer_head *l_buf;
 	struct btrfs_leaf *l;
@@ -1097,8 +1100,10 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 	int data_copy_size;
 	int rt_data_off;
 	int i;
-	int ret;
+	int ret = 0;
 	int wret;
+	int double_split = 0;
+	struct btrfs_disk_key disk_key;
 
 	/* first try to make some room by pushing left and right */
 	wret = push_leaf_left(trans, root, path, data_size);
@@ -1127,26 +1132,58 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 	mid = (nritems + 1)/ 2;
 	right_buffer = btrfs_alloc_free_block(trans, root);
 	BUG_ON(!right_buffer);
-	BUG_ON(mid == nritems);
 	right = btrfs_buffer_leaf(right_buffer);
 	memset(&right->header, 0, sizeof(right->header));
-	if (mid <= slot) {
-		/* FIXME, just alloc a new leaf here */
-		if (leaf_space_used(l, mid, nritems - mid) + space_needed >
-			BTRFS_LEAF_DATA_SIZE(root))
-			BUG();
-	} else {
-		/* FIXME, just alloc a new leaf here */
-		if (leaf_space_used(l, 0, mid + 1) + space_needed >
-			BTRFS_LEAF_DATA_SIZE(root))
-			BUG();
-	}
-	btrfs_set_header_nritems(&right->header, nritems - mid);
 	btrfs_set_header_blocknr(&right->header, right_buffer->b_blocknr);
 	btrfs_set_header_generation(&right->header, trans->transid);
 	btrfs_set_header_level(&right->header, 0);
 	btrfs_set_header_parentid(&right->header,
 	      btrfs_header_parentid(btrfs_buffer_header(root->node)));
+	if (mid <= slot) {
+		if (nritems == 1 ||
+		    leaf_space_used(l, mid, nritems - mid) + space_needed >
+			BTRFS_LEAF_DATA_SIZE(root)) {
+			if (slot >= nritems) {
+				btrfs_cpu_key_to_disk(&disk_key, ins_key);
+				btrfs_set_header_nritems(&right->header, 0);
+				wret = insert_ptr(trans, root, path,
+						  &disk_key,
+						  right_buffer->b_blocknr,
+						  path->slots[1] + 1, 1);
+				if (wret)
+					ret = wret;
+				btrfs_block_release(root, path->nodes[0]);
+				path->nodes[0] = right_buffer;
+				path->slots[0] = 0;
+				path->slots[1] += 1;
+				return ret;
+			}
+			mid = slot;
+			double_split = 1;
+		}
+	} else {
+		if (leaf_space_used(l, 0, mid + 1) + space_needed >
+			BTRFS_LEAF_DATA_SIZE(root)) {
+			if (slot == 0) {
+				btrfs_cpu_key_to_disk(&disk_key, ins_key);
+				btrfs_set_header_nritems(&right->header, 0);
+				wret = insert_ptr(trans, root, path,
+						  &disk_key,
+						  right_buffer->b_blocknr,
+						  path->slots[1] - 1, 1);
+				if (wret)
+					ret = wret;
+				btrfs_block_release(root, path->nodes[0]);
+				path->nodes[0] = right_buffer;
+				path->slots[0] = 0;
+				path->slots[1] -= 1;
+				return ret;
+			}
+			mid = slot;
+			double_split = 1;
+		}
+	}
+	btrfs_set_header_nritems(&right->header, nritems - mid);
 	data_copy_size = btrfs_item_end(l->items + mid) -
 			 leaf_data_end(root, l);
 	btrfs_memcpy(root, right, right->items, l->items + mid,
@@ -1180,6 +1217,31 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 	} else
 		btrfs_block_release(root, right_buffer);
 	BUG_ON(path->slots[0] < 0);
+
+	if (!double_split)
+		return ret;
+	right_buffer = btrfs_alloc_free_block(trans, root);
+	BUG_ON(!right_buffer);
+	right = btrfs_buffer_leaf(right_buffer);
+	memset(&right->header, 0, sizeof(right->header));
+	btrfs_set_header_blocknr(&right->header, right_buffer->b_blocknr);
+	btrfs_set_header_generation(&right->header, trans->transid);
+	btrfs_set_header_level(&right->header, 0);
+	btrfs_set_header_parentid(&right->header,
+	      btrfs_header_parentid(btrfs_buffer_header(root->node)));
+	btrfs_cpu_key_to_disk(&disk_key, ins_key);
+	btrfs_set_header_nritems(&right->header, 0);
+	wret = insert_ptr(trans, root, path,
+			  &disk_key,
+			  right_buffer->b_blocknr,
+			  path->slots[1], 1);
+	if (wret)
+		ret = wret;
+	btrfs_block_release(root, path->nodes[0]);
+	path->nodes[0] = right_buffer;
+	path->slots[0] = 0;
+	check_node(root, path, 1);
+	check_leaf(root, path, 0);
 	return ret;
 }
 
@@ -1220,9 +1282,9 @@ int btrfs_insert_empty_item(struct btrfs_trans_handle *trans, struct btrfs_root
 	data_end = leaf_data_end(root, leaf);
 
 	if (btrfs_leaf_free_space(root, leaf) <
-	    sizeof(struct btrfs_item) + data_size)
+	    sizeof(struct btrfs_item) + data_size) {
 		BUG();
-
+	}
 	slot = path->slots[0];
 	BUG_ON(slot < 0);
 	if (slot != nritems) {

commit df24a2b9c9bcef3348e4b1a8f206cd484a248d36
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Apr 4 09:36:31 2007 -0400

    Btrfs: early inline file data code
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 453ce835209a..9ef65e2ecf51 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -16,19 +16,24 @@ static int balance_node_right(struct btrfs_trans_handle *trans, struct
 static int del_ptr(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 		   struct btrfs_path *path, int level, int slot);
 
-struct btrfs_path *btrfs_alloc_path(void)
+inline void btrfs_init_path(struct btrfs_path *p)
 {
-	return kmem_cache_alloc(btrfs_path_cachep, GFP_NOFS);
+	memset(p, 0, sizeof(*p));
 }
 
-void btrfs_free_path(struct btrfs_path *p)
+struct btrfs_path *btrfs_alloc_path(void)
 {
-	kmem_cache_free(btrfs_path_cachep, p);
+	struct btrfs_path *path;
+	path = kmem_cache_alloc(btrfs_path_cachep, GFP_NOFS);
+	if (path)
+		btrfs_init_path(path);
+	return path;
 }
 
-inline void btrfs_init_path(struct btrfs_path *p)
+void btrfs_free_path(struct btrfs_path *p)
 {
-	memset(p, 0, sizeof(*p));
+	btrfs_release_path(NULL, p);
+	kmem_cache_free(btrfs_path_cachep, p);
 }
 
 void btrfs_release_path(struct btrfs_root *root, struct btrfs_path *p)
@@ -77,6 +82,7 @@ static int btrfs_cow_block(struct btrfs_trans_handle *trans, struct btrfs_root
 		btrfs_free_extent(trans, root, buf->b_blocknr, 1, 1);
 	}
 	btrfs_block_release(root, buf);
+	mark_buffer_dirty(cow);
 	*cow_ret = cow;
 	return 0;
 }

commit 2c90e5d658424bc71b111eb5a972240d5d06fe86
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Apr 2 10:50:19 2007 -0400

    Btrfs: still corruption hunting
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index a0dfa2d6cb9b..453ce835209a 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -16,6 +16,16 @@ static int balance_node_right(struct btrfs_trans_handle *trans, struct
 static int del_ptr(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 		   struct btrfs_path *path, int level, int slot);
 
+struct btrfs_path *btrfs_alloc_path(void)
+{
+	return kmem_cache_alloc(btrfs_path_cachep, GFP_NOFS);
+}
+
+void btrfs_free_path(struct btrfs_path *p)
+{
+	kmem_cache_free(btrfs_path_cachep, p);
+}
+
 inline void btrfs_init_path(struct btrfs_path *p)
 {
 	memset(p, 0, sizeof(*p));
@@ -47,17 +57,18 @@ static int btrfs_cow_block(struct btrfs_trans_handle *trans, struct btrfs_root
 	}
 	cow = btrfs_alloc_free_block(trans, root);
 	cow_node = btrfs_buffer_node(cow);
+	if (buf->b_size != root->blocksize || cow->b_size != root->blocksize)
+		WARN_ON(1);
 	memcpy(cow_node, btrfs_buffer_node(buf), root->blocksize);
 	btrfs_set_header_blocknr(&cow_node->header, cow->b_blocknr);
 	btrfs_set_header_generation(&cow_node->header, trans->transid);
-	*cow_ret = cow;
-	btrfs_mark_buffer_dirty(cow);
 	btrfs_inc_ref(trans, root, buf);
 	if (buf == root->node) {
 		root->node = cow;
 		get_bh(cow);
-		if (buf != root->commit_root)
+		if (buf != root->commit_root) {
 			btrfs_free_extent(trans, root, buf->b_blocknr, 1, 1);
+		}
 		btrfs_block_release(root, buf);
 	} else {
 		btrfs_set_node_blockptr(btrfs_buffer_node(parent), parent_slot,
@@ -66,6 +77,7 @@ static int btrfs_cow_block(struct btrfs_trans_handle *trans, struct btrfs_root
 		btrfs_free_extent(trans, root, buf->b_blocknr, 1, 1);
 	}
 	btrfs_block_release(root, buf);
+	*cow_ret = cow;
 	return 0;
 }
 
@@ -477,9 +489,12 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 					       p->slots[level + 1],
 					       &cow_buf);
 			b = cow_buf;
+			c = btrfs_buffer_node(b);
 		}
 		BUG_ON(!cow && ins_len);
-		c = btrfs_buffer_node(b);
+		if (level != btrfs_header_level(&c->header))
+			WARN_ON(1);
+		level = btrfs_header_level(&c->header);
 		p->nodes[level] = b;
 		ret = check_block(root, p, level);
 		if (ret)
@@ -1257,19 +1272,22 @@ int btrfs_insert_item(struct btrfs_trans_handle *trans, struct btrfs_root
 		      data_size)
 {
 	int ret = 0;
-	struct btrfs_path path;
+	struct btrfs_path *path;
 	u8 *ptr;
 
-	btrfs_init_path(&path);
-	ret = btrfs_insert_empty_item(trans, root, &path, cpu_key, data_size);
+	path = btrfs_alloc_path();
+	BUG_ON(!path);
+	btrfs_init_path(path);
+	ret = btrfs_insert_empty_item(trans, root, path, cpu_key, data_size);
 	if (!ret) {
-		ptr = btrfs_item_ptr(btrfs_buffer_leaf(path.nodes[0]),
-				     path.slots[0], u8);
-		btrfs_memcpy(root, path.nodes[0]->b_data,
+		ptr = btrfs_item_ptr(btrfs_buffer_leaf(path->nodes[0]),
+				     path->slots[0], u8);
+		btrfs_memcpy(root, path->nodes[0]->b_data,
 			     ptr, data, data_size);
-		btrfs_mark_buffer_dirty(path.nodes[0]);
+		btrfs_mark_buffer_dirty(path->nodes[0]);
 	}
-	btrfs_release_path(root, &path);
+	btrfs_release_path(root, path);
+	btrfs_free_path(path);
 	return ret;
 }
 

commit d6025579531b7ea170ba283b171ff7a6bf7d0e12
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Mar 30 14:27:56 2007 -0400

    Btrfs: corruption hunt continues
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index b7f3fcb72c62..a0dfa2d6cb9b 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -51,7 +51,7 @@ static int btrfs_cow_block(struct btrfs_trans_handle *trans, struct btrfs_root
 	btrfs_set_header_blocknr(&cow_node->header, cow->b_blocknr);
 	btrfs_set_header_generation(&cow_node->header, trans->transid);
 	*cow_ret = cow;
-	mark_buffer_dirty(cow);
+	btrfs_mark_buffer_dirty(cow);
 	btrfs_inc_ref(trans, root, buf);
 	if (buf == root->node) {
 		root->node = cow;
@@ -62,7 +62,7 @@ static int btrfs_cow_block(struct btrfs_trans_handle *trans, struct btrfs_root
 	} else {
 		btrfs_set_node_blockptr(btrfs_buffer_node(parent), parent_slot,
 					cow->b_blocknr);
-		mark_buffer_dirty(parent);
+		btrfs_mark_buffer_dirty(parent);
 		btrfs_free_extent(trans, root, buf->b_blocknr, 1, 1);
 	}
 	btrfs_block_release(root, buf);
@@ -312,11 +312,12 @@ static int balance_level(struct btrfs_trans_handle *trans, struct btrfs_root
 		BUG_ON(!child);
 		root->node = child;
 		path->nodes[level] = NULL;
+		clean_tree_block(trans, root, mid_buf);
+		wait_on_buffer(mid_buf);
 		/* once for the path */
 		btrfs_block_release(root, mid_buf);
 		/* once for the root ptr */
 		btrfs_block_release(root, mid_buf);
-		clean_tree_block(trans, root, mid_buf);
 		return btrfs_free_extent(trans, root, blocknr, 1, 1);
 	}
 	parent = btrfs_buffer_node(parent_buf);
@@ -351,8 +352,9 @@ static int balance_level(struct btrfs_trans_handle *trans, struct btrfs_root
 			ret = wret;
 		if (btrfs_header_nritems(&right->header) == 0) {
 			u64 blocknr = right_buf->b_blocknr;
-			btrfs_block_release(root, right_buf);
 			clean_tree_block(trans, root, right_buf);
+			wait_on_buffer(right_buf);
+			btrfs_block_release(root, right_buf);
 			right_buf = NULL;
 			right = NULL;
 			wret = del_ptr(trans, root, path, level + 1, pslot +
@@ -363,10 +365,11 @@ static int balance_level(struct btrfs_trans_handle *trans, struct btrfs_root
 			if (wret)
 				ret = wret;
 		} else {
-			memcpy(&parent->ptrs[pslot + 1].key,
-				&right->ptrs[0].key,
-				sizeof(struct btrfs_disk_key));
-			mark_buffer_dirty(parent_buf);
+			btrfs_memcpy(root, parent,
+				     &parent->ptrs[pslot + 1].key,
+				     &right->ptrs[0].key,
+				     sizeof(struct btrfs_disk_key));
+			btrfs_mark_buffer_dirty(parent_buf);
 		}
 	}
 	if (btrfs_header_nritems(&mid->header) == 1) {
@@ -388,8 +391,9 @@ static int balance_level(struct btrfs_trans_handle *trans, struct btrfs_root
 	if (btrfs_header_nritems(&mid->header) == 0) {
 		/* we've managed to empty the middle node, drop it */
 		u64 blocknr = mid_buf->b_blocknr;
-		btrfs_block_release(root, mid_buf);
 		clean_tree_block(trans, root, mid_buf);
+		wait_on_buffer(mid_buf);
+		btrfs_block_release(root, mid_buf);
 		mid_buf = NULL;
 		mid = NULL;
 		wret = del_ptr(trans, root, path, level + 1, pslot);
@@ -400,9 +404,10 @@ static int balance_level(struct btrfs_trans_handle *trans, struct btrfs_root
 			ret = wret;
 	} else {
 		/* update the parent key to reflect our changes */
-		memcpy(&parent->ptrs[pslot].key, &mid->ptrs[0].key,
-		       sizeof(struct btrfs_disk_key));
-		mark_buffer_dirty(parent_buf);
+		btrfs_memcpy(root, parent,
+			     &parent->ptrs[pslot].key, &mid->ptrs[0].key,
+			     sizeof(struct btrfs_disk_key));
+		btrfs_mark_buffer_dirty(parent_buf);
 	}
 
 	/* update the path */
@@ -544,8 +549,8 @@ static int fixup_low_keys(struct btrfs_trans_handle *trans, struct btrfs_root
 		if (!path->nodes[i])
 			break;
 		t = btrfs_buffer_node(path->nodes[i]);
-		memcpy(&t->ptrs[tslot].key, key, sizeof(*key));
-		mark_buffer_dirty(path->nodes[i]);
+		btrfs_memcpy(root, t, &t->ptrs[tslot].key, key, sizeof(*key));
+		btrfs_mark_buffer_dirty(path->nodes[i]);
 		if (tslot != 0)
 			break;
 	}
@@ -580,17 +585,17 @@ static int push_node_left(struct btrfs_trans_handle *trans, struct btrfs_root
 	if (src_nritems < push_items)
 		push_items = src_nritems;
 
-	memcpy(dst->ptrs + dst_nritems, src->ptrs,
-		push_items * sizeof(struct btrfs_key_ptr));
+	btrfs_memcpy(root, dst, dst->ptrs + dst_nritems, src->ptrs,
+		     push_items * sizeof(struct btrfs_key_ptr));
 	if (push_items < src_nritems) {
-		memmove(src->ptrs, src->ptrs + push_items,
+		btrfs_memmove(root, src, src->ptrs, src->ptrs + push_items,
 			(src_nritems - push_items) *
 			sizeof(struct btrfs_key_ptr));
 	}
 	btrfs_set_header_nritems(&src->header, src_nritems - push_items);
 	btrfs_set_header_nritems(&dst->header, dst_nritems + push_items);
-	mark_buffer_dirty(src_buf);
-	mark_buffer_dirty(dst_buf);
+	btrfs_mark_buffer_dirty(src_buf);
+	btrfs_mark_buffer_dirty(dst_buf);
 	return ret;
 }
 
@@ -629,16 +634,18 @@ static int balance_node_right(struct btrfs_trans_handle *trans, struct
 	if (max_push < push_items)
 		push_items = max_push;
 
-	memmove(dst->ptrs + push_items, dst->ptrs,
-		dst_nritems * sizeof(struct btrfs_key_ptr));
-	memcpy(dst->ptrs, src->ptrs + src_nritems - push_items,
-		push_items * sizeof(struct btrfs_key_ptr));
+	btrfs_memmove(root, dst, dst->ptrs + push_items, dst->ptrs,
+		      dst_nritems * sizeof(struct btrfs_key_ptr));
+
+	btrfs_memcpy(root, dst, dst->ptrs,
+		     src->ptrs + src_nritems - push_items,
+		     push_items * sizeof(struct btrfs_key_ptr));
 
 	btrfs_set_header_nritems(&src->header, src_nritems - push_items);
 	btrfs_set_header_nritems(&dst->header, dst_nritems + push_items);
 
-	mark_buffer_dirty(src_buf);
-	mark_buffer_dirty(dst_buf);
+	btrfs_mark_buffer_dirty(src_buf);
+	btrfs_mark_buffer_dirty(dst_buf);
 	return ret;
 }
 
@@ -674,10 +681,11 @@ static int insert_new_root(struct btrfs_trans_handle *trans, struct btrfs_root
 		lower_key = &((struct btrfs_leaf *)lower)->items[0].key;
 	else
 		lower_key = &lower->ptrs[0].key;
-	memcpy(&c->ptrs[0].key, lower_key, sizeof(struct btrfs_disk_key));
+	btrfs_memcpy(root, c, &c->ptrs[0].key, lower_key,
+		     sizeof(struct btrfs_disk_key));
 	btrfs_set_node_blockptr(c, 0, path->nodes[level - 1]->b_blocknr);
 
-	mark_buffer_dirty(t);
+	btrfs_mark_buffer_dirty(t);
 
 	/* the super has an extra ref to root->node */
 	btrfs_block_release(root, root->node);
@@ -712,13 +720,15 @@ static int insert_ptr(struct btrfs_trans_handle *trans, struct btrfs_root
 	if (nritems == BTRFS_NODEPTRS_PER_BLOCK(root))
 		BUG();
 	if (slot != nritems) {
-		memmove(lower->ptrs + slot + 1, lower->ptrs + slot,
-			(nritems - slot) * sizeof(struct btrfs_key_ptr));
+		btrfs_memmove(root, lower, lower->ptrs + slot + 1,
+			      lower->ptrs + slot,
+			      (nritems - slot) * sizeof(struct btrfs_key_ptr));
 	}
-	memcpy(&lower->ptrs[slot].key, key, sizeof(struct btrfs_disk_key));
+	btrfs_memcpy(root, lower, &lower->ptrs[slot].key,
+		     key, sizeof(struct btrfs_disk_key));
 	btrfs_set_node_blockptr(lower, slot, blocknr);
 	btrfs_set_header_nritems(&lower->header, nritems + 1);
-	mark_buffer_dirty(path->nodes[level]);
+	btrfs_mark_buffer_dirty(path->nodes[level]);
 	return 0;
 }
 
@@ -761,14 +771,14 @@ static int split_node(struct btrfs_trans_handle *trans, struct btrfs_root
 	btrfs_set_header_parentid(&split->header,
 	      btrfs_header_parentid(btrfs_buffer_header(root->node)));
 	mid = (c_nritems + 1) / 2;
-	memcpy(split->ptrs, c->ptrs + mid,
-		(c_nritems - mid) * sizeof(struct btrfs_key_ptr));
+	btrfs_memcpy(root, split, split->ptrs, c->ptrs + mid,
+		     (c_nritems - mid) * sizeof(struct btrfs_key_ptr));
 	btrfs_set_header_nritems(&split->header, c_nritems - mid);
 	btrfs_set_header_nritems(&c->header, mid);
 	ret = 0;
 
-	mark_buffer_dirty(t);
-	mark_buffer_dirty(split_buffer);
+	btrfs_mark_buffer_dirty(t);
+	btrfs_mark_buffer_dirty(split_buffer);
 	wret = insert_ptr(trans, root, path, &split->ptrs[0].key,
 			  split_buffer->b_blocknr, path->slots[level + 1] + 1,
 			  level + 1);
@@ -875,17 +885,22 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 	push_space = btrfs_item_end(left->items + left_nritems - push_items);
 	push_space -= leaf_data_end(root, left);
 	/* make room in the right data area */
-	memmove(btrfs_leaf_data(right) + leaf_data_end(root, right) -
-		push_space, btrfs_leaf_data(right) + leaf_data_end(root, right),
-		BTRFS_LEAF_DATA_SIZE(root) - leaf_data_end(root, right));
+	btrfs_memmove(root, right, btrfs_leaf_data(right) +
+		      leaf_data_end(root, right) - push_space,
+		      btrfs_leaf_data(right) +
+		      leaf_data_end(root, right), BTRFS_LEAF_DATA_SIZE(root) -
+		      leaf_data_end(root, right));
 	/* copy from the left data area */
-	memcpy(btrfs_leaf_data(right) + BTRFS_LEAF_DATA_SIZE(root) - push_space,
-		btrfs_leaf_data(left) + leaf_data_end(root, left), push_space);
-	memmove(right->items + push_items, right->items,
+	btrfs_memcpy(root, right, btrfs_leaf_data(right) +
+		     BTRFS_LEAF_DATA_SIZE(root) - push_space,
+		     btrfs_leaf_data(left) + leaf_data_end(root, left),
+		     push_space);
+	btrfs_memmove(root, right, right->items + push_items, right->items,
 		right_nritems * sizeof(struct btrfs_item));
 	/* copy the items from left to right */
-	memcpy(right->items, left->items + left_nritems - push_items,
-		push_items * sizeof(struct btrfs_item));
+	btrfs_memcpy(root, right, right->items, left->items +
+		     left_nritems - push_items,
+		     push_items * sizeof(struct btrfs_item));
 
 	/* update the item pointers */
 	right_nritems += push_items;
@@ -899,11 +914,11 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 	left_nritems -= push_items;
 	btrfs_set_header_nritems(&left->header, left_nritems);
 
-	mark_buffer_dirty(left_buf);
-	mark_buffer_dirty(right_buf);
-	memcpy(&upper_node->ptrs[slot + 1].key,
+	btrfs_mark_buffer_dirty(left_buf);
+	btrfs_mark_buffer_dirty(right_buf);
+	btrfs_memcpy(root, upper_node, &upper_node->ptrs[slot + 1].key,
 		&right->items[0].key, sizeof(struct btrfs_disk_key));
-	mark_buffer_dirty(upper);
+	btrfs_mark_buffer_dirty(upper);
 
 	/* then fixup the leaf pointer in the path */
 	if (path->slots[0] >= left_nritems) {
@@ -977,14 +992,16 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 		return 1;
 	}
 	/* push data from right to left */
-	memcpy(left->items + btrfs_header_nritems(&left->header),
-		right->items, push_items * sizeof(struct btrfs_item));
+	btrfs_memcpy(root, left, left->items +
+		     btrfs_header_nritems(&left->header),
+		     right->items, push_items * sizeof(struct btrfs_item));
 	push_space = BTRFS_LEAF_DATA_SIZE(root) -
 		     btrfs_item_offset(right->items + push_items -1);
-	memcpy(btrfs_leaf_data(left) + leaf_data_end(root, left) - push_space,
-		btrfs_leaf_data(right) +
-		btrfs_item_offset(right->items + push_items - 1),
-		push_space);
+	btrfs_memcpy(root, left, btrfs_leaf_data(left) +
+		     leaf_data_end(root, left) - push_space,
+		     btrfs_leaf_data(right) +
+		     btrfs_item_offset(right->items + push_items - 1),
+		     push_space);
 	old_left_nritems = btrfs_header_nritems(&left->header);
 	BUG_ON(old_left_nritems < 0);
 
@@ -1000,10 +1017,11 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 	/* fixup right node */
 	push_space = btrfs_item_offset(right->items + push_items - 1) -
 		     leaf_data_end(root, right);
-	memmove(btrfs_leaf_data(right) + BTRFS_LEAF_DATA_SIZE(root) -
-		push_space, btrfs_leaf_data(right) +
-		leaf_data_end(root, right), push_space);
-	memmove(right->items, right->items + push_items,
+	btrfs_memmove(root, right, btrfs_leaf_data(right) +
+		      BTRFS_LEAF_DATA_SIZE(root) - push_space,
+		      btrfs_leaf_data(right) +
+		      leaf_data_end(root, right), push_space);
+	btrfs_memmove(root, right, right->items, right->items + push_items,
 		(btrfs_header_nritems(&right->header) - push_items) *
 		sizeof(struct btrfs_item));
 	btrfs_set_header_nritems(&right->header,
@@ -1017,8 +1035,8 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 		push_space = btrfs_item_offset(right->items + i);
 	}
 
-	mark_buffer_dirty(t);
-	mark_buffer_dirty(right_buf);
+	btrfs_mark_buffer_dirty(t);
+	btrfs_mark_buffer_dirty(right_buf);
 
 	wret = fixup_low_keys(trans, root, path, &right->items[0].key, 1);
 	if (wret)
@@ -1110,11 +1128,12 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 	      btrfs_header_parentid(btrfs_buffer_header(root->node)));
 	data_copy_size = btrfs_item_end(l->items + mid) -
 			 leaf_data_end(root, l);
-	memcpy(right->items, l->items + mid,
-	       (nritems - mid) * sizeof(struct btrfs_item));
-	memcpy(btrfs_leaf_data(right) + BTRFS_LEAF_DATA_SIZE(root) -
-		data_copy_size, btrfs_leaf_data(l) +
-		leaf_data_end(root, l), data_copy_size);
+	btrfs_memcpy(root, right, right->items, l->items + mid,
+		     (nritems - mid) * sizeof(struct btrfs_item));
+	btrfs_memcpy(root, right,
+		     btrfs_leaf_data(right) + BTRFS_LEAF_DATA_SIZE(root) -
+		     data_copy_size, btrfs_leaf_data(l) +
+		     leaf_data_end(root, l), data_copy_size);
 	rt_data_off = BTRFS_LEAF_DATA_SIZE(root) -
 		      btrfs_item_end(l->items + mid);
 
@@ -1129,8 +1148,8 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 			  right_buffer->b_blocknr, path->slots[1] + 1, 1);
 	if (wret)
 		ret = wret;
-	mark_buffer_dirty(right_buffer);
-	mark_buffer_dirty(l_buf);
+	btrfs_mark_buffer_dirty(right_buffer);
+	btrfs_mark_buffer_dirty(l_buf);
 	BUG_ON(path->slots[0] != slot);
 	if (mid <= slot) {
 		btrfs_block_release(root, path->nodes[0]);
@@ -1200,22 +1219,23 @@ int btrfs_insert_empty_item(struct btrfs_trans_handle *trans, struct btrfs_root
 		}
 
 		/* shift the items */
-		memmove(leaf->items + slot + 1, leaf->items + slot,
-		        (nritems - slot) * sizeof(struct btrfs_item));
+		btrfs_memmove(root, leaf, leaf->items + slot + 1,
+			      leaf->items + slot,
+			      (nritems - slot) * sizeof(struct btrfs_item));
 
 		/* shift the data */
-		memmove(btrfs_leaf_data(leaf) + data_end - data_size,
-			btrfs_leaf_data(leaf) +
-		        data_end, old_data - data_end);
+		btrfs_memmove(root, leaf, btrfs_leaf_data(leaf) +
+			      data_end - data_size, btrfs_leaf_data(leaf) +
+			      data_end, old_data - data_end);
 		data_end = old_data;
 	}
 	/* setup the item for the new data */
-	memcpy(&leaf->items[slot].key, &disk_key,
-		sizeof(struct btrfs_disk_key));
+	btrfs_memcpy(root, leaf, &leaf->items[slot].key, &disk_key,
+		     sizeof(struct btrfs_disk_key));
 	btrfs_set_item_offset(leaf->items + slot, data_end - data_size);
 	btrfs_set_item_size(leaf->items + slot, data_size);
 	btrfs_set_header_nritems(&leaf->header, nritems + 1);
-	mark_buffer_dirty(leaf_buf);
+	btrfs_mark_buffer_dirty(leaf_buf);
 
 	ret = 0;
 	if (slot == 0)
@@ -1245,8 +1265,9 @@ int btrfs_insert_item(struct btrfs_trans_handle *trans, struct btrfs_root
 	if (!ret) {
 		ptr = btrfs_item_ptr(btrfs_buffer_leaf(path.nodes[0]),
 				     path.slots[0], u8);
-		memcpy(ptr, data, data_size);
-		mark_buffer_dirty(path.nodes[0]);
+		btrfs_memcpy(root, path.nodes[0]->b_data,
+			     ptr, data, data_size);
+		btrfs_mark_buffer_dirty(path.nodes[0]);
 	}
 	btrfs_release_path(root, &path);
 	return ret;
@@ -1271,8 +1292,10 @@ static int del_ptr(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 	node = btrfs_buffer_node(parent);
 	nritems = btrfs_header_nritems(&node->header);
 	if (slot != nritems -1) {
-		memmove(node->ptrs + slot, node->ptrs + slot + 1,
-			sizeof(struct btrfs_key_ptr) * (nritems - slot - 1));
+		btrfs_memmove(root, node, node->ptrs + slot,
+			      node->ptrs + slot + 1,
+			      sizeof(struct btrfs_key_ptr) *
+			      (nritems - slot - 1));
 	}
 	nritems--;
 	btrfs_set_header_nritems(&node->header, nritems);
@@ -1287,7 +1310,7 @@ static int del_ptr(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 		if (wret)
 			ret = wret;
 	}
-	mark_buffer_dirty(parent);
+	btrfs_mark_buffer_dirty(parent);
 	return ret;
 }
 
@@ -1317,16 +1340,18 @@ int btrfs_del_item(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 	if (slot != nritems - 1) {
 		int i;
 		int data_end = leaf_data_end(root, leaf);
-		memmove(btrfs_leaf_data(leaf) + data_end + dsize,
-			btrfs_leaf_data(leaf) + data_end,
-			doff - data_end);
+		btrfs_memmove(root, leaf, btrfs_leaf_data(leaf) +
+			      data_end + dsize,
+			      btrfs_leaf_data(leaf) + data_end,
+			      doff - data_end);
 		for (i = slot + 1; i < nritems; i++) {
 			u32 ioff = btrfs_item_offset(leaf->items + i);
 			btrfs_set_item_offset(leaf->items + i, ioff + dsize);
 		}
-		memmove(leaf->items + slot, leaf->items + slot + 1,
-			sizeof(struct btrfs_item) *
-			(nritems - slot - 1));
+		btrfs_memmove(root, leaf, leaf->items + slot,
+			      leaf->items + slot + 1,
+			      sizeof(struct btrfs_item) *
+			      (nritems - slot - 1));
 	}
 	btrfs_set_header_nritems(&leaf->header, nritems - 1);
 	nritems--;
@@ -1336,6 +1361,7 @@ int btrfs_del_item(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 			btrfs_set_header_level(&leaf->header, 0);
 		} else {
 			clean_tree_block(trans, root, leaf_buf);
+			wait_on_buffer(leaf_buf);
 			wret = del_ptr(trans, root, path, 1, path->slots[1]);
 			if (wret)
 				ret = wret;
@@ -1373,6 +1399,7 @@ int btrfs_del_item(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 			if (btrfs_header_nritems(&leaf->header) == 0) {
 				u64 blocknr = leaf_buf->b_blocknr;
 				clean_tree_block(trans, root, leaf_buf);
+				wait_on_buffer(leaf_buf);
 				wret = del_ptr(trans, root, path, 1, slot);
 				if (wret)
 					ret = wret;
@@ -1382,11 +1409,11 @@ int btrfs_del_item(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 				if (wret)
 					ret = wret;
 			} else {
-				mark_buffer_dirty(leaf_buf);
+				btrfs_mark_buffer_dirty(leaf_buf);
 				btrfs_block_release(root, leaf_buf);
 			}
 		} else {
-			mark_buffer_dirty(leaf_buf);
+			btrfs_mark_buffer_dirty(leaf_buf);
 		}
 	}
 	return ret;

commit 22b0ebda6c63a1ad66b6a9e806bd226a4a03a049
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Mar 30 08:47:31 2007 -0400

    Btrfs: hunting slab corruption
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 1eb333e2e18b..b7f3fcb72c62 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -457,6 +457,8 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 	int ret;
 	int level;
 
+	WARN_ON(p->nodes[0] != NULL);
+	WARN_ON(!mutex_is_locked(&root->fs_info->fs_mutex));
 again:
 	b = root->node;
 	get_bh(b);

commit f254e52c1ce550fdaa0d31f5e068f0d67c2485d4
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Mar 29 15:15:27 2007 -0400

    Btrfs: verify csums on read
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 48c611948d11..1eb333e2e18b 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -109,14 +109,14 @@ static int comp_keys(struct btrfs_disk_key *disk, struct btrfs_key *k2)
 		return 1;
 	if (k1.objectid < k2->objectid)
 		return -1;
-	if (k1.flags > k2->flags)
-		return 1;
-	if (k1.flags < k2->flags)
-		return -1;
 	if (k1.offset > k2->offset)
 		return 1;
 	if (k1.offset < k2->offset)
 		return -1;
+	if (k1.flags > k2->flags)
+		return 1;
+	if (k1.flags < k2->flags)
+		return -1;
 	return 0;
 }
 
@@ -1165,7 +1165,6 @@ int btrfs_insert_empty_item(struct btrfs_trans_handle *trans, struct btrfs_root
 		BUG();
 	ret = btrfs_search_slot(trans, root, cpu_key, path, data_size, 1);
 	if (ret == 0) {
-		btrfs_release_path(root, path);
 		return -EEXIST;
 	}
 	if (ret < 0)

commit 9a6f11ed8f421fb1cc7b37390e32316ff4701f5d
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Mar 27 09:06:38 2007 -0400

    Btrfs: split out level field in struct header
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index b33a6bfaf327..48c611948d11 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -753,6 +753,7 @@ static int split_node(struct btrfs_trans_handle *trans, struct btrfs_root
 	split_buffer = btrfs_alloc_free_block(trans, root);
 	split = btrfs_buffer_node(split_buffer);
 	btrfs_set_header_flags(&split->header, btrfs_header_flags(&c->header));
+	btrfs_set_header_level(&split->header, btrfs_header_level(&c->header));
 	btrfs_set_header_blocknr(&split->header, split_buffer->b_blocknr);
 	btrfs_set_header_generation(&split->header, trans->transid);
 	btrfs_set_header_parentid(&split->header,

commit 7f5c15160e1436a53d01f9190db11c2a3a4d788a
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Mar 23 15:56:19 2007 -0400

    Add generation number to btrfs_header, readdir fixes, hash collision fixes
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 052434a328ec..b33a6bfaf327 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1,6 +1,7 @@
 #include <linux/module.h>
 #include "ctree.h"
 #include "disk-io.h"
+#include "transaction.h"
 
 static int split_node(struct btrfs_trans_handle *trans, struct btrfs_root
 		      *root, struct btrfs_path *path, int level);
@@ -39,7 +40,8 @@ static int btrfs_cow_block(struct btrfs_trans_handle *trans, struct btrfs_root
 	struct buffer_head *cow;
 	struct btrfs_node *cow_node;
 
-	if (buffer_dirty(buf)) {
+	if (btrfs_header_generation(btrfs_buffer_header(buf)) ==
+				    trans->transid) {
 		*cow_ret = buf;
 		return 0;
 	}
@@ -47,6 +49,7 @@ static int btrfs_cow_block(struct btrfs_trans_handle *trans, struct btrfs_root
 	cow_node = btrfs_buffer_node(cow);
 	memcpy(cow_node, btrfs_buffer_node(buf), root->blocksize);
 	btrfs_set_header_blocknr(&cow_node->header, cow->b_blocknr);
+	btrfs_set_header_generation(&cow_node->header, trans->transid);
 	*cow_ret = cow;
 	mark_buffer_dirty(cow);
 	btrfs_inc_ref(trans, root, buf);
@@ -661,6 +664,7 @@ static int insert_new_root(struct btrfs_trans_handle *trans, struct btrfs_root
 	btrfs_set_header_nritems(&c->header, 1);
 	btrfs_set_header_level(&c->header, level);
 	btrfs_set_header_blocknr(&c->header, t->b_blocknr);
+	btrfs_set_header_generation(&c->header, trans->transid);
 	btrfs_set_header_parentid(&c->header,
 	      btrfs_header_parentid(btrfs_buffer_header(root->node)));
 	lower = btrfs_buffer_node(path->nodes[level-1]);
@@ -750,6 +754,7 @@ static int split_node(struct btrfs_trans_handle *trans, struct btrfs_root
 	split = btrfs_buffer_node(split_buffer);
 	btrfs_set_header_flags(&split->header, btrfs_header_flags(&c->header));
 	btrfs_set_header_blocknr(&split->header, split_buffer->b_blocknr);
+	btrfs_set_header_generation(&split->header, trans->transid);
 	btrfs_set_header_parentid(&split->header,
 	      btrfs_header_parentid(btrfs_buffer_header(root->node)));
 	mid = (c_nritems + 1) / 2;
@@ -1096,6 +1101,7 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 	}
 	btrfs_set_header_nritems(&right->header, nritems - mid);
 	btrfs_set_header_blocknr(&right->header, right_buffer->b_blocknr);
+	btrfs_set_header_generation(&right->header, trans->transid);
 	btrfs_set_header_level(&right->header, 0);
 	btrfs_set_header_parentid(&right->header,
 	      btrfs_header_parentid(btrfs_buffer_header(root->node)));

commit d571976292839cec05a2820b08f7629b145ed157
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Mar 23 10:01:08 2007 -0400

    btrfs_create, btrfs_write_super, btrfs_sync_fs
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index e690e2bb47d2..052434a328ec 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -39,7 +39,7 @@ static int btrfs_cow_block(struct btrfs_trans_handle *trans, struct btrfs_root
 	struct buffer_head *cow;
 	struct btrfs_node *cow_node;
 
-	if (!buffer_dirty(buf)) {
+	if (buffer_dirty(buf)) {
 		*cow_ret = buf;
 		return 0;
 	}
@@ -48,6 +48,7 @@ static int btrfs_cow_block(struct btrfs_trans_handle *trans, struct btrfs_root
 	memcpy(cow_node, btrfs_buffer_node(buf), root->blocksize);
 	btrfs_set_header_blocknr(&cow_node->header, cow->b_blocknr);
 	*cow_ret = cow;
+	mark_buffer_dirty(cow);
 	btrfs_inc_ref(trans, root, buf);
 	if (buf == root->node) {
 		root->node = cow;
@@ -58,7 +59,7 @@ static int btrfs_cow_block(struct btrfs_trans_handle *trans, struct btrfs_root
 	} else {
 		btrfs_set_node_blockptr(btrfs_buffer_node(parent), parent_slot,
 					cow->b_blocknr);
-		BUG_ON(!buffer_dirty(parent));
+		mark_buffer_dirty(parent);
 		btrfs_free_extent(trans, root, buf->b_blocknr, 1, 1);
 	}
 	btrfs_block_release(root, buf);
@@ -362,7 +363,7 @@ static int balance_level(struct btrfs_trans_handle *trans, struct btrfs_root
 			memcpy(&parent->ptrs[pslot + 1].key,
 				&right->ptrs[0].key,
 				sizeof(struct btrfs_disk_key));
-			BUG_ON(!buffer_dirty(parent_buf));
+			mark_buffer_dirty(parent_buf);
 		}
 	}
 	if (btrfs_header_nritems(&mid->header) == 1) {
@@ -398,7 +399,7 @@ static int balance_level(struct btrfs_trans_handle *trans, struct btrfs_root
 		/* update the parent key to reflect our changes */
 		memcpy(&parent->ptrs[pslot].key, &mid->ptrs[0].key,
 		       sizeof(struct btrfs_disk_key));
-		BUG_ON(!buffer_dirty(parent_buf));
+		mark_buffer_dirty(parent_buf);
 	}
 
 	/* update the path */
@@ -539,7 +540,7 @@ static int fixup_low_keys(struct btrfs_trans_handle *trans, struct btrfs_root
 			break;
 		t = btrfs_buffer_node(path->nodes[i]);
 		memcpy(&t->ptrs[tslot].key, key, sizeof(*key));
-		BUG_ON(!buffer_dirty(path->nodes[i]));
+		mark_buffer_dirty(path->nodes[i]);
 		if (tslot != 0)
 			break;
 	}
@@ -583,8 +584,8 @@ static int push_node_left(struct btrfs_trans_handle *trans, struct btrfs_root
 	}
 	btrfs_set_header_nritems(&src->header, src_nritems - push_items);
 	btrfs_set_header_nritems(&dst->header, dst_nritems + push_items);
-	BUG_ON(!buffer_dirty(src_buf));
-	BUG_ON(!buffer_dirty(dst_buf));
+	mark_buffer_dirty(src_buf);
+	mark_buffer_dirty(dst_buf);
 	return ret;
 }
 
@@ -631,8 +632,8 @@ static int balance_node_right(struct btrfs_trans_handle *trans, struct
 	btrfs_set_header_nritems(&src->header, src_nritems - push_items);
 	btrfs_set_header_nritems(&dst->header, dst_nritems + push_items);
 
-	BUG_ON(!buffer_dirty(src_buf));
-	BUG_ON(!buffer_dirty(dst_buf));
+	mark_buffer_dirty(src_buf);
+	mark_buffer_dirty(dst_buf);
 	return ret;
 }
 
@@ -669,6 +670,9 @@ static int insert_new_root(struct btrfs_trans_handle *trans, struct btrfs_root
 		lower_key = &lower->ptrs[0].key;
 	memcpy(&c->ptrs[0].key, lower_key, sizeof(struct btrfs_disk_key));
 	btrfs_set_node_blockptr(c, 0, path->nodes[level - 1]->b_blocknr);
+
+	mark_buffer_dirty(t);
+
 	/* the super has an extra ref to root->node */
 	btrfs_block_release(root, root->node);
 	root->node = t;
@@ -708,7 +712,7 @@ static int insert_ptr(struct btrfs_trans_handle *trans, struct btrfs_root
 	memcpy(&lower->ptrs[slot].key, key, sizeof(struct btrfs_disk_key));
 	btrfs_set_node_blockptr(lower, slot, blocknr);
 	btrfs_set_header_nritems(&lower->header, nritems + 1);
-	BUG_ON(!buffer_dirty(path->nodes[level]));
+	mark_buffer_dirty(path->nodes[level]);
 	return 0;
 }
 
@@ -755,7 +759,8 @@ static int split_node(struct btrfs_trans_handle *trans, struct btrfs_root
 	btrfs_set_header_nritems(&c->header, mid);
 	ret = 0;
 
-	BUG_ON(!buffer_dirty(t));
+	mark_buffer_dirty(t);
+	mark_buffer_dirty(split_buffer);
 	wret = insert_ptr(trans, root, path, &split->ptrs[0].key,
 			  split_buffer->b_blocknr, path->slots[level + 1] + 1,
 			  level + 1);
@@ -886,11 +891,11 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 	left_nritems -= push_items;
 	btrfs_set_header_nritems(&left->header, left_nritems);
 
-	BUG_ON(!buffer_dirty(left_buf));
-	BUG_ON(!buffer_dirty(right_buf));
+	mark_buffer_dirty(left_buf);
+	mark_buffer_dirty(right_buf);
 	memcpy(&upper_node->ptrs[slot + 1].key,
 		&right->items[0].key, sizeof(struct btrfs_disk_key));
-	BUG_ON(!buffer_dirty(upper));
+	mark_buffer_dirty(upper);
 
 	/* then fixup the leaf pointer in the path */
 	if (path->slots[0] >= left_nritems) {
@@ -1004,8 +1009,8 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 		push_space = btrfs_item_offset(right->items + i);
 	}
 
-	BUG_ON(!buffer_dirty(t));
-	BUG_ON(!buffer_dirty(right_buf));
+	mark_buffer_dirty(t);
+	mark_buffer_dirty(right_buf);
 
 	wret = fixup_low_keys(trans, root, path, &right->items[0].key, 1);
 	if (wret)
@@ -1115,8 +1120,8 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 			  right_buffer->b_blocknr, path->slots[1] + 1, 1);
 	if (wret)
 		ret = wret;
-	BUG_ON(!buffer_dirty(right_buffer));
-	BUG_ON(!buffer_dirty(l_buf));
+	mark_buffer_dirty(right_buffer);
+	mark_buffer_dirty(l_buf);
 	BUG_ON(path->slots[0] != slot);
 	if (mid <= slot) {
 		btrfs_block_release(root, path->nodes[0]);
@@ -1202,12 +1207,12 @@ int btrfs_insert_empty_item(struct btrfs_trans_handle *trans, struct btrfs_root
 	btrfs_set_item_offset(leaf->items + slot, data_end - data_size);
 	btrfs_set_item_size(leaf->items + slot, data_size);
 	btrfs_set_header_nritems(&leaf->header, nritems + 1);
+	mark_buffer_dirty(leaf_buf);
 
 	ret = 0;
 	if (slot == 0)
 		ret = fixup_low_keys(trans, root, path, &disk_key, 1);
 
-	BUG_ON(!buffer_dirty(leaf_buf));
 	if (btrfs_leaf_free_space(root, leaf) < 0)
 		BUG();
 	check_leaf(root, path, 0);
@@ -1233,6 +1238,7 @@ int btrfs_insert_item(struct btrfs_trans_handle *trans, struct btrfs_root
 		ptr = btrfs_item_ptr(btrfs_buffer_leaf(path.nodes[0]),
 				     path.slots[0], u8);
 		memcpy(ptr, data, data_size);
+		mark_buffer_dirty(path.nodes[0]);
 	}
 	btrfs_release_path(root, &path);
 	return ret;
@@ -1273,7 +1279,7 @@ static int del_ptr(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 		if (wret)
 			ret = wret;
 	}
-	BUG_ON(!buffer_dirty(parent));
+	mark_buffer_dirty(parent);
 	return ret;
 }
 
@@ -1368,8 +1374,11 @@ int btrfs_del_item(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 				if (wret)
 					ret = wret;
 			} else {
+				mark_buffer_dirty(leaf_buf);
 				btrfs_block_release(root, leaf_buf);
 			}
+		} else {
+			mark_buffer_dirty(leaf_buf);
 		}
 	}
 	return ret;

commit e20d96d64f9cf9288ffecc9ad4714e91c3b97ca8
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Mar 22 12:13:20 2007 -0400

    Mountable btrfs, with readdir
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 9fbd07c37fde..e690e2bb47d2 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -7,11 +7,11 @@ static int split_node(struct btrfs_trans_handle *trans, struct btrfs_root
 static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 		      *root, struct btrfs_path *path, int data_size);
 static int push_node_left(struct btrfs_trans_handle *trans, struct btrfs_root
-			  *root, struct btrfs_buffer *dst, struct btrfs_buffer
+			  *root, struct buffer_head *dst, struct buffer_head
 			  *src);
 static int balance_node_right(struct btrfs_trans_handle *trans, struct
-			      btrfs_root *root, struct btrfs_buffer *dst_buf,
-			      struct btrfs_buffer *src_buf);
+			      btrfs_root *root, struct buffer_head *dst_buf,
+			      struct buffer_head *src_buf);
 static int del_ptr(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 		   struct btrfs_path *path, int level, int slot);
 
@@ -32,32 +32,34 @@ void btrfs_release_path(struct btrfs_root *root, struct btrfs_path *p)
 }
 
 static int btrfs_cow_block(struct btrfs_trans_handle *trans, struct btrfs_root
-			   *root, struct btrfs_buffer *buf, struct btrfs_buffer
-			   *parent, int parent_slot, struct btrfs_buffer
+			   *root, struct buffer_head *buf, struct buffer_head
+			   *parent, int parent_slot, struct buffer_head
 			   **cow_ret)
 {
-	struct btrfs_buffer *cow;
+	struct buffer_head *cow;
+	struct btrfs_node *cow_node;
 
-	if (!list_empty(&buf->dirty)) {
+	if (!buffer_dirty(buf)) {
 		*cow_ret = buf;
 		return 0;
 	}
 	cow = btrfs_alloc_free_block(trans, root);
-	memcpy(&cow->node, &buf->node, root->blocksize);
-	btrfs_set_header_blocknr(&cow->node.header, cow->blocknr);
+	cow_node = btrfs_buffer_node(cow);
+	memcpy(cow_node, btrfs_buffer_node(buf), root->blocksize);
+	btrfs_set_header_blocknr(&cow_node->header, cow->b_blocknr);
 	*cow_ret = cow;
 	btrfs_inc_ref(trans, root, buf);
 	if (buf == root->node) {
 		root->node = cow;
-		cow->count++;
+		get_bh(cow);
 		if (buf != root->commit_root)
-			btrfs_free_extent(trans, root, buf->blocknr, 1, 1);
+			btrfs_free_extent(trans, root, buf->b_blocknr, 1, 1);
 		btrfs_block_release(root, buf);
 	} else {
-		btrfs_set_node_blockptr(&parent->node, parent_slot,
-					cow->blocknr);
-		BUG_ON(list_empty(&parent->dirty));
-		btrfs_free_extent(trans, root, buf->blocknr, 1, 1);
+		btrfs_set_node_blockptr(btrfs_buffer_node(parent), parent_slot,
+					cow->b_blocknr);
+		BUG_ON(!buffer_dirty(parent));
+		btrfs_free_extent(trans, root, buf->b_blocknr, 1, 1);
 	}
 	btrfs_block_release(root, buf);
 	return 0;
@@ -119,12 +121,12 @@ static int check_node(struct btrfs_root *root, struct btrfs_path *path,
 {
 	int i;
 	struct btrfs_node *parent = NULL;
-	struct btrfs_node *node = &path->nodes[level]->node;
+	struct btrfs_node *node = btrfs_buffer_node(path->nodes[level]);
 	int parent_slot;
 	u32 nritems = btrfs_header_nritems(&node->header);
 
 	if (path->nodes[level + 1])
-		parent = &path->nodes[level + 1]->node;
+		parent = btrfs_buffer_node(path->nodes[level + 1]);
 	parent_slot = path->slots[level + 1];
 	BUG_ON(nritems == 0);
 	if (parent) {
@@ -148,13 +150,13 @@ static int check_leaf(struct btrfs_root *root, struct btrfs_path *path,
 		      int level)
 {
 	int i;
-	struct btrfs_leaf *leaf = &path->nodes[level]->leaf;
+	struct btrfs_leaf *leaf = btrfs_buffer_leaf(path->nodes[level]);
 	struct btrfs_node *parent = NULL;
 	int parent_slot;
 	u32 nritems = btrfs_header_nritems(&leaf->header);
 
 	if (path->nodes[level + 1])
-		parent = &path->nodes[level + 1]->node;
+		parent = btrfs_buffer_node(path->nodes[level + 1]);
 	parent_slot = path->slots[level + 1];
 	BUG_ON(btrfs_leaf_free_space(root, leaf) < 0);
 
@@ -250,11 +252,11 @@ static int bin_search(struct btrfs_node *c, struct btrfs_key *key, int *slot)
 	return -1;
 }
 
-static struct btrfs_buffer *read_node_slot(struct btrfs_root *root,
-				   struct btrfs_buffer *parent_buf,
+static struct buffer_head *read_node_slot(struct btrfs_root *root,
+				   struct buffer_head *parent_buf,
 				   int slot)
 {
-	struct btrfs_node *node = &parent_buf->node;
+	struct btrfs_node *node = btrfs_buffer_node(parent_buf);
 	if (slot < 0)
 		return NULL;
 	if (slot >= btrfs_header_nritems(&node->header))
@@ -265,10 +267,10 @@ static struct btrfs_buffer *read_node_slot(struct btrfs_root *root,
 static int balance_level(struct btrfs_trans_handle *trans, struct btrfs_root
 			 *root, struct btrfs_path *path, int level)
 {
-	struct btrfs_buffer *right_buf;
-	struct btrfs_buffer *mid_buf;
-	struct btrfs_buffer *left_buf;
-	struct btrfs_buffer *parent_buf = NULL;
+	struct buffer_head *right_buf;
+	struct buffer_head *mid_buf;
+	struct buffer_head *left_buf;
+	struct buffer_head *parent_buf = NULL;
 	struct btrfs_node *right = NULL;
 	struct btrfs_node *mid;
 	struct btrfs_node *left = NULL;
@@ -283,7 +285,7 @@ static int balance_level(struct btrfs_trans_handle *trans, struct btrfs_root
 		return 0;
 
 	mid_buf = path->nodes[level];
-	mid = &mid_buf->node;
+	mid = btrfs_buffer_node(mid_buf);
 	orig_ptr = btrfs_node_blockptr(mid, orig_slot);
 
 	if (level < BTRFS_MAX_LEVEL - 1)
@@ -295,8 +297,8 @@ static int balance_level(struct btrfs_trans_handle *trans, struct btrfs_root
 	 * by promoting the node below to a root
 	 */
 	if (!parent_buf) {
-		struct btrfs_buffer *child;
-		u64 blocknr = mid_buf->blocknr;
+		struct buffer_head *child;
+		u64 blocknr = mid_buf->b_blocknr;
 
 		if (btrfs_header_nritems(&mid->header) != 1)
 			return 0;
@@ -313,7 +315,7 @@ static int balance_level(struct btrfs_trans_handle *trans, struct btrfs_root
 		clean_tree_block(trans, root, mid_buf);
 		return btrfs_free_extent(trans, root, blocknr, 1, 1);
 	}
-	parent = &parent_buf->node;
+	parent = btrfs_buffer_node(parent_buf);
 
 	if (btrfs_header_nritems(&mid->header) >
 	    BTRFS_NODEPTRS_PER_BLOCK(root) / 4)
@@ -326,7 +328,7 @@ static int balance_level(struct btrfs_trans_handle *trans, struct btrfs_root
 	if (left_buf) {
 		btrfs_cow_block(trans, root, left_buf, parent_buf, pslot - 1,
 				&left_buf);
-		left = &left_buf->node;
+		left = btrfs_buffer_node(left_buf);
 		orig_slot += btrfs_header_nritems(&left->header);
 		wret = push_node_left(trans, root, left_buf, mid_buf);
 		if (wret < 0)
@@ -339,12 +341,12 @@ static int balance_level(struct btrfs_trans_handle *trans, struct btrfs_root
 	if (right_buf) {
 		btrfs_cow_block(trans, root, right_buf, parent_buf, pslot + 1,
 				&right_buf);
-		right = &right_buf->node;
+		right = btrfs_buffer_node(right_buf);
 		wret = push_node_left(trans, root, mid_buf, right_buf);
 		if (wret < 0)
 			ret = wret;
 		if (btrfs_header_nritems(&right->header) == 0) {
-			u64 blocknr = right_buf->blocknr;
+			u64 blocknr = right_buf->b_blocknr;
 			btrfs_block_release(root, right_buf);
 			clean_tree_block(trans, root, right_buf);
 			right_buf = NULL;
@@ -360,7 +362,7 @@ static int balance_level(struct btrfs_trans_handle *trans, struct btrfs_root
 			memcpy(&parent->ptrs[pslot + 1].key,
 				&right->ptrs[0].key,
 				sizeof(struct btrfs_disk_key));
-			BUG_ON(list_empty(&parent_buf->dirty));
+			BUG_ON(!buffer_dirty(parent_buf));
 		}
 	}
 	if (btrfs_header_nritems(&mid->header) == 1) {
@@ -381,7 +383,7 @@ static int balance_level(struct btrfs_trans_handle *trans, struct btrfs_root
 	}
 	if (btrfs_header_nritems(&mid->header) == 0) {
 		/* we've managed to empty the middle node, drop it */
-		u64 blocknr = mid_buf->blocknr;
+		u64 blocknr = mid_buf->b_blocknr;
 		btrfs_block_release(root, mid_buf);
 		clean_tree_block(trans, root, mid_buf);
 		mid_buf = NULL;
@@ -396,13 +398,13 @@ static int balance_level(struct btrfs_trans_handle *trans, struct btrfs_root
 		/* update the parent key to reflect our changes */
 		memcpy(&parent->ptrs[pslot].key, &mid->ptrs[0].key,
 		       sizeof(struct btrfs_disk_key));
-		BUG_ON(list_empty(&parent_buf->dirty));
+		BUG_ON(!buffer_dirty(parent_buf));
 	}
 
 	/* update the path */
 	if (left_buf) {
 		if (btrfs_header_nritems(&left->header) > orig_slot) {
-			left_buf->count++; // released below
+			get_bh(left_buf);
 			path->nodes[level] = left_buf;
 			path->slots[level + 1] -= 1;
 			path->slots[level] = orig_slot;
@@ -415,8 +417,9 @@ static int balance_level(struct btrfs_trans_handle *trans, struct btrfs_root
 	}
 	/* double check we haven't messed things up */
 	check_block(root, path, level);
-	if (orig_ptr != btrfs_node_blockptr(&path->nodes[level]->node,
-					    path->slots[level]))
+	if (orig_ptr !=
+	    btrfs_node_blockptr(btrfs_buffer_node(path->nodes[level]),
+				path->slots[level]))
 		BUG();
 
 	if (right_buf)
@@ -443,8 +446,8 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 		      *root, struct btrfs_key *key, struct btrfs_path *p, int
 		      ins_len, int cow)
 {
-	struct btrfs_buffer *b;
-	struct btrfs_buffer *cow_buf;
+	struct buffer_head *b;
+	struct buffer_head *cow_buf;
 	struct btrfs_node *c;
 	int slot;
 	int ret;
@@ -452,18 +455,20 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 
 again:
 	b = root->node;
-	b->count++;
+	get_bh(b);
 	while (b) {
-		level = btrfs_header_level(&b->node.header);
+		c = btrfs_buffer_node(b);
+		level = btrfs_header_level(&c->header);
 		if (cow) {
 			int wret;
-			wret = btrfs_cow_block(trans, root, b, p->nodes[level +
-					       1], p->slots[level + 1],
+			wret = btrfs_cow_block(trans, root, b,
+					       p->nodes[level + 1],
+					       p->slots[level + 1],
 					       &cow_buf);
 			b = cow_buf;
 		}
 		BUG_ON(!cow && ins_len);
-		c = &b->node;
+		c = btrfs_buffer_node(b);
 		p->nodes[level] = b;
 		ret = check_block(root, p, level);
 		if (ret)
@@ -480,7 +485,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 				if (sret)
 					return sret;
 				b = p->nodes[level];
-				c = &b->node;
+				c = btrfs_buffer_node(b);
 				slot = p->slots[level];
 			} else if (ins_len < 0) {
 				int sret = balance_level(trans, root, p,
@@ -490,7 +495,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 				b = p->nodes[level];
 				if (!b)
 					goto again;
-				c = &b->node;
+				c = btrfs_buffer_node(b);
 				slot = p->slots[level];
 				BUG_ON(btrfs_header_nritems(&c->header) == 1);
 			}
@@ -505,11 +510,9 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
 				if (sret)
 					return sret;
 			}
-			BUG_ON(root->node->count == 1);
 			return ret;
 		}
 	}
-	BUG_ON(root->node->count == 1);
 	return 1;
 }
 
@@ -534,9 +537,9 @@ static int fixup_low_keys(struct btrfs_trans_handle *trans, struct btrfs_root
 		int tslot = path->slots[i];
 		if (!path->nodes[i])
 			break;
-		t = &path->nodes[i]->node;
+		t = btrfs_buffer_node(path->nodes[i]);
 		memcpy(&t->ptrs[tslot].key, key, sizeof(*key));
-		BUG_ON(list_empty(&path->nodes[i]->dirty));
+		BUG_ON(!buffer_dirty(path->nodes[i]));
 		if (tslot != 0)
 			break;
 	}
@@ -551,11 +554,11 @@ static int fixup_low_keys(struct btrfs_trans_handle *trans, struct btrfs_root
  * error, and > 0 if there was no room in the left hand block.
  */
 static int push_node_left(struct btrfs_trans_handle *trans, struct btrfs_root
-			  *root, struct btrfs_buffer *dst_buf, struct
-			  btrfs_buffer *src_buf)
+			  *root, struct buffer_head *dst_buf, struct
+			  buffer_head *src_buf)
 {
-	struct btrfs_node *src = &src_buf->node;
-	struct btrfs_node *dst = &dst_buf->node;
+	struct btrfs_node *src = btrfs_buffer_node(src_buf);
+	struct btrfs_node *dst = btrfs_buffer_node(dst_buf);
 	int push_items = 0;
 	int src_nritems;
 	int dst_nritems;
@@ -580,8 +583,8 @@ static int push_node_left(struct btrfs_trans_handle *trans, struct btrfs_root
 	}
 	btrfs_set_header_nritems(&src->header, src_nritems - push_items);
 	btrfs_set_header_nritems(&dst->header, dst_nritems + push_items);
-	BUG_ON(list_empty(&src_buf->dirty));
-	BUG_ON(list_empty(&dst_buf->dirty));
+	BUG_ON(!buffer_dirty(src_buf));
+	BUG_ON(!buffer_dirty(dst_buf));
 	return ret;
 }
 
@@ -595,11 +598,11 @@ static int push_node_left(struct btrfs_trans_handle *trans, struct btrfs_root
  * this will  only push up to 1/2 the contents of the left node over
  */
 static int balance_node_right(struct btrfs_trans_handle *trans, struct
-			      btrfs_root *root, struct btrfs_buffer *dst_buf,
-			      struct btrfs_buffer *src_buf)
+			      btrfs_root *root, struct buffer_head *dst_buf,
+			      struct buffer_head *src_buf)
 {
-	struct btrfs_node *src = &src_buf->node;
-	struct btrfs_node *dst = &dst_buf->node;
+	struct btrfs_node *src = btrfs_buffer_node(src_buf);
+	struct btrfs_node *dst = btrfs_buffer_node(dst_buf);
 	int push_items = 0;
 	int max_push;
 	int src_nritems;
@@ -628,8 +631,8 @@ static int balance_node_right(struct btrfs_trans_handle *trans, struct
 	btrfs_set_header_nritems(&src->header, src_nritems - push_items);
 	btrfs_set_header_nritems(&dst->header, dst_nritems + push_items);
 
-	BUG_ON(list_empty(&src_buf->dirty));
-	BUG_ON(list_empty(&dst_buf->dirty));
+	BUG_ON(!buffer_dirty(src_buf));
+	BUG_ON(!buffer_dirty(dst_buf));
 	return ret;
 }
 
@@ -643,7 +646,7 @@ static int balance_node_right(struct btrfs_trans_handle *trans, struct
 static int insert_new_root(struct btrfs_trans_handle *trans, struct btrfs_root
 			   *root, struct btrfs_path *path, int level)
 {
-	struct btrfs_buffer *t;
+	struct buffer_head *t;
 	struct btrfs_node *lower;
 	struct btrfs_node *c;
 	struct btrfs_disk_key *lower_key;
@@ -652,24 +655,24 @@ static int insert_new_root(struct btrfs_trans_handle *trans, struct btrfs_root
 	BUG_ON(path->nodes[level-1] != root->node);
 
 	t = btrfs_alloc_free_block(trans, root);
-	c = &t->node;
+	c = btrfs_buffer_node(t);
 	memset(c, 0, root->blocksize);
 	btrfs_set_header_nritems(&c->header, 1);
 	btrfs_set_header_level(&c->header, level);
-	btrfs_set_header_blocknr(&c->header, t->blocknr);
+	btrfs_set_header_blocknr(&c->header, t->b_blocknr);
 	btrfs_set_header_parentid(&c->header,
-	                       btrfs_header_parentid(&root->node->node.header));
-	lower = &path->nodes[level-1]->node;
+	      btrfs_header_parentid(btrfs_buffer_header(root->node)));
+	lower = btrfs_buffer_node(path->nodes[level-1]);
 	if (btrfs_is_leaf(lower))
 		lower_key = &((struct btrfs_leaf *)lower)->items[0].key;
 	else
 		lower_key = &lower->ptrs[0].key;
 	memcpy(&c->ptrs[0].key, lower_key, sizeof(struct btrfs_disk_key));
-	btrfs_set_node_blockptr(c, 0, path->nodes[level - 1]->blocknr);
+	btrfs_set_node_blockptr(c, 0, path->nodes[level - 1]->b_blocknr);
 	/* the super has an extra ref to root->node */
 	btrfs_block_release(root, root->node);
 	root->node = t;
-	t->count++;
+	get_bh(t);
 	path->nodes[level] = t;
 	path->slots[level] = 0;
 	return 0;
@@ -692,7 +695,7 @@ static int insert_ptr(struct btrfs_trans_handle *trans, struct btrfs_root
 	int nritems;
 
 	BUG_ON(!path->nodes[level]);
-	lower = &path->nodes[level]->node;
+	lower = btrfs_buffer_node(path->nodes[level]);
 	nritems = btrfs_header_nritems(&lower->header);
 	if (slot > nritems)
 		BUG();
@@ -705,7 +708,7 @@ static int insert_ptr(struct btrfs_trans_handle *trans, struct btrfs_root
 	memcpy(&lower->ptrs[slot].key, key, sizeof(struct btrfs_disk_key));
 	btrfs_set_node_blockptr(lower, slot, blocknr);
 	btrfs_set_header_nritems(&lower->header, nritems + 1);
-	BUG_ON(list_empty(&path->nodes[level]->dirty));
+	BUG_ON(!buffer_dirty(path->nodes[level]));
 	return 0;
 }
 
@@ -721,9 +724,9 @@ static int insert_ptr(struct btrfs_trans_handle *trans, struct btrfs_root
 static int split_node(struct btrfs_trans_handle *trans, struct btrfs_root
 		      *root, struct btrfs_path *path, int level)
 {
-	struct btrfs_buffer *t;
+	struct buffer_head *t;
 	struct btrfs_node *c;
-	struct btrfs_buffer *split_buffer;
+	struct buffer_head *split_buffer;
 	struct btrfs_node *split;
 	int mid;
 	int ret;
@@ -731,7 +734,7 @@ static int split_node(struct btrfs_trans_handle *trans, struct btrfs_root
 	u32 c_nritems;
 
 	t = path->nodes[level];
-	c = &t->node;
+	c = btrfs_buffer_node(t);
 	if (t == root->node) {
 		/* trying to split the root, lets make a new one */
 		ret = insert_new_root(trans, root, path, level + 1);
@@ -740,11 +743,11 @@ static int split_node(struct btrfs_trans_handle *trans, struct btrfs_root
 	}
 	c_nritems = btrfs_header_nritems(&c->header);
 	split_buffer = btrfs_alloc_free_block(trans, root);
-	split = &split_buffer->node;
+	split = btrfs_buffer_node(split_buffer);
 	btrfs_set_header_flags(&split->header, btrfs_header_flags(&c->header));
-	btrfs_set_header_blocknr(&split->header, split_buffer->blocknr);
+	btrfs_set_header_blocknr(&split->header, split_buffer->b_blocknr);
 	btrfs_set_header_parentid(&split->header,
-	                       btrfs_header_parentid(&root->node->node.header));
+	      btrfs_header_parentid(btrfs_buffer_header(root->node)));
 	mid = (c_nritems + 1) / 2;
 	memcpy(split->ptrs, c->ptrs + mid,
 		(c_nritems - mid) * sizeof(struct btrfs_key_ptr));
@@ -752,9 +755,9 @@ static int split_node(struct btrfs_trans_handle *trans, struct btrfs_root
 	btrfs_set_header_nritems(&c->header, mid);
 	ret = 0;
 
-	BUG_ON(list_empty(&t->dirty));
+	BUG_ON(!buffer_dirty(t));
 	wret = insert_ptr(trans, root, path, &split->ptrs[0].key,
-			  split_buffer->blocknr, path->slots[level + 1] + 1,
+			  split_buffer->b_blocknr, path->slots[level + 1] + 1,
 			  level + 1);
 	if (wret)
 		ret = wret;
@@ -798,11 +801,12 @@ static int leaf_space_used(struct btrfs_leaf *l, int start, int nr)
 static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 			   *root, struct btrfs_path *path, int data_size)
 {
-	struct btrfs_buffer *left_buf = path->nodes[0];
-	struct btrfs_leaf *left = &left_buf->leaf;
+	struct buffer_head *left_buf = path->nodes[0];
+	struct btrfs_leaf *left = btrfs_buffer_leaf(left_buf);
 	struct btrfs_leaf *right;
-	struct btrfs_buffer *right_buf;
-	struct btrfs_buffer *upper;
+	struct buffer_head *right_buf;
+	struct buffer_head *upper;
+	struct btrfs_node *upper_node;
 	int slot;
 	int i;
 	int free_space;
@@ -817,12 +821,13 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 		return 1;
 	}
 	upper = path->nodes[1];
-	if (slot >= btrfs_header_nritems(&upper->node.header) - 1) {
+	upper_node = btrfs_buffer_node(upper);
+	if (slot >= btrfs_header_nritems(&upper_node->header) - 1) {
 		return 1;
 	}
-	right_buf = read_tree_block(root, btrfs_node_blockptr(&upper->node,
-							      slot + 1));
-	right = &right_buf->leaf;
+	right_buf = read_tree_block(root,
+		    btrfs_node_blockptr(btrfs_buffer_node(upper), slot + 1));
+	right = btrfs_buffer_leaf(right_buf);
 	free_space = btrfs_leaf_free_space(root, right);
 	if (free_space < data_size + sizeof(struct btrfs_item)) {
 		btrfs_block_release(root, right_buf);
@@ -830,7 +835,7 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 	}
 	/* cow and double check */
 	btrfs_cow_block(trans, root, right_buf, upper, slot + 1, &right_buf);
-	right = &right_buf->leaf;
+	right = btrfs_buffer_leaf(right_buf);
 	free_space = btrfs_leaf_free_space(root, right);
 	if (free_space < data_size + sizeof(struct btrfs_item)) {
 		btrfs_block_release(root, right_buf);
@@ -881,11 +886,11 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 	left_nritems -= push_items;
 	btrfs_set_header_nritems(&left->header, left_nritems);
 
-	BUG_ON(list_empty(&left_buf->dirty));
-	BUG_ON(list_empty(&right_buf->dirty));
-	memcpy(&upper->node.ptrs[slot + 1].key,
+	BUG_ON(!buffer_dirty(left_buf));
+	BUG_ON(!buffer_dirty(right_buf));
+	memcpy(&upper_node->ptrs[slot + 1].key,
 		&right->items[0].key, sizeof(struct btrfs_disk_key));
-	BUG_ON(list_empty(&upper->dirty));
+	BUG_ON(!buffer_dirty(upper));
 
 	/* then fixup the leaf pointer in the path */
 	if (path->slots[0] >= left_nritems) {
@@ -905,9 +910,9 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 			  *root, struct btrfs_path *path, int data_size)
 {
-	struct btrfs_buffer *right_buf = path->nodes[0];
-	struct btrfs_leaf *right = &right_buf->leaf;
-	struct btrfs_buffer *t;
+	struct buffer_head *right_buf = path->nodes[0];
+	struct btrfs_leaf *right = btrfs_buffer_leaf(right_buf);
+	struct buffer_head *t;
 	struct btrfs_leaf *left;
 	int slot;
 	int i;
@@ -926,9 +931,9 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 	if (!path->nodes[1]) {
 		return 1;
 	}
-	t = read_tree_block(root, btrfs_node_blockptr(&path->nodes[1]->node,
-						      slot - 1));
-	left = &t->leaf;
+	t = read_tree_block(root,
+	    btrfs_node_blockptr(btrfs_buffer_node(path->nodes[1]), slot - 1));
+	left = btrfs_buffer_leaf(t);
 	free_space = btrfs_leaf_free_space(root, left);
 	if (free_space < data_size + sizeof(struct btrfs_item)) {
 		btrfs_block_release(root, t);
@@ -937,7 +942,7 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 
 	/* cow and double check */
 	btrfs_cow_block(trans, root, t, path->nodes[1], slot - 1, &t);
-	left = &t->leaf;
+	left = btrfs_buffer_leaf(t);
 	free_space = btrfs_leaf_free_space(root, left);
 	if (free_space < data_size + sizeof(struct btrfs_item)) {
 		btrfs_block_release(root, t);
@@ -999,8 +1004,8 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 		push_space = btrfs_item_offset(right->items + i);
 	}
 
-	BUG_ON(list_empty(&t->dirty));
-	BUG_ON(list_empty(&right_buf->dirty));
+	BUG_ON(!buffer_dirty(t));
+	BUG_ON(!buffer_dirty(right_buf));
 
 	wret = fixup_low_keys(trans, root, path, &right->items[0].key, 1);
 	if (wret)
@@ -1029,13 +1034,13 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 		      *root, struct btrfs_path *path, int data_size)
 {
-	struct btrfs_buffer *l_buf;
+	struct buffer_head *l_buf;
 	struct btrfs_leaf *l;
 	u32 nritems;
 	int mid;
 	int slot;
 	struct btrfs_leaf *right;
-	struct btrfs_buffer *right_buffer;
+	struct buffer_head *right_buffer;
 	int space_needed = data_size + sizeof(struct btrfs_item);
 	int data_copy_size;
 	int rt_data_off;
@@ -1053,7 +1058,7 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 			return wret;
 	}
 	l_buf = path->nodes[0];
-	l = &l_buf->leaf;
+	l = btrfs_buffer_leaf(l_buf);
 
 	/* did the pushes work? */
 	if (btrfs_leaf_free_space(root, l) >=
@@ -1071,7 +1076,7 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 	right_buffer = btrfs_alloc_free_block(trans, root);
 	BUG_ON(!right_buffer);
 	BUG_ON(mid == nritems);
-	right = &right_buffer->leaf;
+	right = btrfs_buffer_leaf(right_buffer);
 	memset(&right->header, 0, sizeof(right->header));
 	if (mid <= slot) {
 		/* FIXME, just alloc a new leaf here */
@@ -1085,10 +1090,10 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 			BUG();
 	}
 	btrfs_set_header_nritems(&right->header, nritems - mid);
-	btrfs_set_header_blocknr(&right->header, right_buffer->blocknr);
+	btrfs_set_header_blocknr(&right->header, right_buffer->b_blocknr);
 	btrfs_set_header_level(&right->header, 0);
 	btrfs_set_header_parentid(&right->header,
-	                       btrfs_header_parentid(&root->node->node.header));
+	      btrfs_header_parentid(btrfs_buffer_header(root->node)));
 	data_copy_size = btrfs_item_end(l->items + mid) -
 			 leaf_data_end(root, l);
 	memcpy(right->items, l->items + mid,
@@ -1107,11 +1112,11 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 	btrfs_set_header_nritems(&l->header, mid);
 	ret = 0;
 	wret = insert_ptr(trans, root, path, &right->items[0].key,
-			  right_buffer->blocknr, path->slots[1] + 1, 1);
+			  right_buffer->b_blocknr, path->slots[1] + 1, 1);
 	if (wret)
 		ret = wret;
-	BUG_ON(list_empty(&right_buffer->dirty));
-	BUG_ON(list_empty(&l_buf->dirty));
+	BUG_ON(!buffer_dirty(right_buffer));
+	BUG_ON(!buffer_dirty(l_buf));
 	BUG_ON(path->slots[0] != slot);
 	if (mid <= slot) {
 		btrfs_block_release(root, path->nodes[0]);
@@ -1136,7 +1141,7 @@ int btrfs_insert_empty_item(struct btrfs_trans_handle *trans, struct btrfs_root
 	int slot;
 	int slot_orig;
 	struct btrfs_leaf *leaf;
-	struct btrfs_buffer *leaf_buf;
+	struct buffer_head *leaf_buf;
 	u32 nritems;
 	unsigned int data_end;
 	struct btrfs_disk_key disk_key;
@@ -1156,7 +1161,7 @@ int btrfs_insert_empty_item(struct btrfs_trans_handle *trans, struct btrfs_root
 
 	slot_orig = path->slots[0];
 	leaf_buf = path->nodes[0];
-	leaf = &leaf_buf->leaf;
+	leaf = btrfs_buffer_leaf(leaf_buf);
 
 	nritems = btrfs_header_nritems(&leaf->header);
 	data_end = leaf_data_end(root, leaf);
@@ -1202,7 +1207,7 @@ int btrfs_insert_empty_item(struct btrfs_trans_handle *trans, struct btrfs_root
 	if (slot == 0)
 		ret = fixup_low_keys(trans, root, path, &disk_key, 1);
 
-	BUG_ON(list_empty(&leaf_buf->dirty));
+	BUG_ON(!buffer_dirty(leaf_buf));
 	if (btrfs_leaf_free_space(root, leaf) < 0)
 		BUG();
 	check_leaf(root, path, 0);
@@ -1225,7 +1230,8 @@ int btrfs_insert_item(struct btrfs_trans_handle *trans, struct btrfs_root
 	btrfs_init_path(&path);
 	ret = btrfs_insert_empty_item(trans, root, &path, cpu_key, data_size);
 	if (!ret) {
-		ptr = btrfs_item_ptr(&path.nodes[0]->leaf, path.slots[0], u8);
+		ptr = btrfs_item_ptr(btrfs_buffer_leaf(path.nodes[0]),
+				     path.slots[0], u8);
 		memcpy(ptr, data, data_size);
 	}
 	btrfs_release_path(root, &path);
@@ -1243,12 +1249,12 @@ static int del_ptr(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 		   struct btrfs_path *path, int level, int slot)
 {
 	struct btrfs_node *node;
-	struct btrfs_buffer *parent = path->nodes[level];
+	struct buffer_head *parent = path->nodes[level];
 	u32 nritems;
 	int ret = 0;
 	int wret;
 
-	node = &parent->node;
+	node = btrfs_buffer_node(parent);
 	nritems = btrfs_header_nritems(&node->header);
 	if (slot != nritems -1) {
 		memmove(node->ptrs + slot, node->ptrs + slot + 1,
@@ -1257,16 +1263,17 @@ static int del_ptr(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 	nritems--;
 	btrfs_set_header_nritems(&node->header, nritems);
 	if (nritems == 0 && parent == root->node) {
-		BUG_ON(btrfs_header_level(&root->node->node.header) != 1);
+		struct btrfs_header *header = btrfs_buffer_header(root->node);
+		BUG_ON(btrfs_header_level(header) != 1);
 		/* just turn the root into a leaf and break */
-		btrfs_set_header_level(&root->node->node.header, 0);
+		btrfs_set_header_level(header, 0);
 	} else if (slot == 0) {
 		wret = fixup_low_keys(trans, root, path, &node->ptrs[0].key,
 				      level + 1);
 		if (wret)
 			ret = wret;
 	}
-	BUG_ON(list_empty(&parent->dirty));
+	BUG_ON(!buffer_dirty(parent));
 	return ret;
 }
 
@@ -1279,7 +1286,7 @@ int btrfs_del_item(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 {
 	int slot;
 	struct btrfs_leaf *leaf;
-	struct btrfs_buffer *leaf_buf;
+	struct buffer_head *leaf_buf;
 	int doff;
 	int dsize;
 	int ret = 0;
@@ -1287,7 +1294,7 @@ int btrfs_del_item(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 	u32 nritems;
 
 	leaf_buf = path->nodes[0];
-	leaf = &leaf_buf->leaf;
+	leaf = btrfs_buffer_leaf(leaf_buf);
 	slot = path->slots[0];
 	doff = btrfs_item_offset(leaf->items + slot);
 	dsize = btrfs_item_size(leaf->items + slot);
@@ -1313,14 +1320,13 @@ int btrfs_del_item(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 	if (nritems == 0) {
 		if (leaf_buf == root->node) {
 			btrfs_set_header_level(&leaf->header, 0);
-			BUG_ON(list_empty(&leaf_buf->dirty));
 		} else {
 			clean_tree_block(trans, root, leaf_buf);
 			wret = del_ptr(trans, root, path, 1, path->slots[1]);
 			if (wret)
 				ret = wret;
 			wret = btrfs_free_extent(trans, root,
-						 leaf_buf->blocknr, 1, 1);
+						 leaf_buf->b_blocknr, 1, 1);
 			if (wret)
 				ret = wret;
 		}
@@ -1332,7 +1338,6 @@ int btrfs_del_item(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 			if (wret)
 				ret = wret;
 		}
-		BUG_ON(list_empty(&leaf_buf->dirty));
 
 		/* delete the leaf if it is mostly empty */
 		if (used < BTRFS_LEAF_DATA_SIZE(root) / 3) {
@@ -1341,7 +1346,7 @@ int btrfs_del_item(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 			 * for possible call to del_ptr below
 			 */
 			slot = path->slots[1];
-			leaf_buf->count++;
+			get_bh(leaf_buf);
 			wret = push_leaf_left(trans, root, path, 1);
 			if (wret < 0)
 				ret = wret;
@@ -1352,7 +1357,7 @@ int btrfs_del_item(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 					ret = wret;
 			}
 			if (btrfs_header_nritems(&leaf->header) == 0) {
-				u64 blocknr = leaf_buf->blocknr;
+				u64 blocknr = leaf_buf->b_blocknr;
 				clean_tree_block(trans, root, leaf_buf);
 				wret = del_ptr(trans, root, path, 1, slot);
 				if (wret)
@@ -1380,19 +1385,21 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 	int slot;
 	int level = 1;
 	u64 blocknr;
-	struct btrfs_buffer *c;
-	struct btrfs_buffer *next = NULL;
+	struct buffer_head *c;
+	struct btrfs_node *c_node;
+	struct buffer_head *next = NULL;
 
 	while(level < BTRFS_MAX_LEVEL) {
 		if (!path->nodes[level])
 			return 1;
 		slot = path->slots[level] + 1;
 		c = path->nodes[level];
-		if (slot >= btrfs_header_nritems(&c->node.header)) {
+		c_node = btrfs_buffer_node(c);
+		if (slot >= btrfs_header_nritems(&c_node->header)) {
 			level++;
 			continue;
 		}
-		blocknr = btrfs_node_blockptr(&c->node, slot);
+		blocknr = btrfs_node_blockptr(c_node, slot);
 		if (next)
 			btrfs_block_release(root, next);
 		next = read_tree_block(root, blocknr);
@@ -1408,7 +1415,7 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 		if (!level)
 			break;
 		next = read_tree_block(root,
-				       btrfs_node_blockptr(&next->node, 0));
+		       btrfs_node_blockptr(btrfs_buffer_node(next), 0));
 	}
 	return 0;
 }

commit 2e635a278354a1a7951e16cfea4c247d6d0e7c99
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Mar 21 11:12:56 2007 -0400

    Btrfs: initial move to kernel module land
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 32922643b5bc..9fbd07c37fde 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1,10 +1,6 @@
-#include <stdio.h>
-#include <stdlib.h>
-#include "kerncompat.h"
-#include "radix-tree.h"
+#include <linux/module.h>
 #include "ctree.h"
 #include "disk-io.h"
-#include "print-tree.h"
 
 static int split_node(struct btrfs_trans_handle *trans, struct btrfs_root
 		      *root, struct btrfs_path *path, int level);

commit 4068947886f2d2a4c192941397bacd014a04b804
Author: Chris Mason <chris.mason@oracle.com>
Date:   Sat Mar 17 14:29:23 2007 -0400

    Btrfs: minor comments
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 17a3ff2f1828..32922643b5bc 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -294,6 +294,10 @@ static int balance_level(struct btrfs_trans_handle *trans, struct btrfs_root
 		parent_buf = path->nodes[level + 1];
 	pslot = path->slots[level + 1];
 
+	/*
+	 * deal with the case where there is only one pointer in the root
+	 * by promoting the node below to a root
+	 */
 	if (!parent_buf) {
 		struct btrfs_buffer *child;
 		u64 blocknr = mid_buf->blocknr;
@@ -1043,6 +1047,7 @@ static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
 	int ret;
 	int wret;
 
+	/* first try to make some room by pushing left and right */
 	wret = push_leaf_left(trans, root, path, data_size);
 	if (wret < 0)
 		return wret;

commit e089f05c18ab36ed5fa7e2319052e03ab800d518
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Mar 16 16:20:31 2007 -0400

    Btrfs: transaction handles everywhere
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 13128b5ed65c..17a3ff2f1828 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -6,17 +6,18 @@
 #include "disk-io.h"
 #include "print-tree.h"
 
-static int split_node(struct btrfs_root *root, struct btrfs_path *path,
-		      int level);
-static int split_leaf(struct btrfs_root *root, struct btrfs_path *path,
-		      int data_size);
-static int push_node_left(struct btrfs_root *root, struct btrfs_buffer *dst,
-			  struct btrfs_buffer *src);
-static int balance_node_right(struct btrfs_root *root,
-			      struct btrfs_buffer *dst_buf,
+static int split_node(struct btrfs_trans_handle *trans, struct btrfs_root
+		      *root, struct btrfs_path *path, int level);
+static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
+		      *root, struct btrfs_path *path, int data_size);
+static int push_node_left(struct btrfs_trans_handle *trans, struct btrfs_root
+			  *root, struct btrfs_buffer *dst, struct btrfs_buffer
+			  *src);
+static int balance_node_right(struct btrfs_trans_handle *trans, struct
+			      btrfs_root *root, struct btrfs_buffer *dst_buf,
 			      struct btrfs_buffer *src_buf);
-static int del_ptr(struct btrfs_root *root, struct btrfs_path *path, int level,
-		   int slot);
+static int del_ptr(struct btrfs_trans_handle *trans, struct btrfs_root *root,
+		   struct btrfs_path *path, int level, int slot);
 
 inline void btrfs_init_path(struct btrfs_path *p)
 {
@@ -34,11 +35,10 @@ void btrfs_release_path(struct btrfs_root *root, struct btrfs_path *p)
 	memset(p, 0, sizeof(*p));
 }
 
-static int btrfs_cow_block(struct btrfs_root *root,
-		    struct btrfs_buffer *buf,
-		    struct btrfs_buffer *parent,
-		    int parent_slot,
-		    struct btrfs_buffer **cow_ret)
+static int btrfs_cow_block(struct btrfs_trans_handle *trans, struct btrfs_root
+			   *root, struct btrfs_buffer *buf, struct btrfs_buffer
+			   *parent, int parent_slot, struct btrfs_buffer
+			   **cow_ret)
 {
 	struct btrfs_buffer *cow;
 
@@ -46,22 +46,22 @@ static int btrfs_cow_block(struct btrfs_root *root,
 		*cow_ret = buf;
 		return 0;
 	}
-	cow = btrfs_alloc_free_block(root);
+	cow = btrfs_alloc_free_block(trans, root);
 	memcpy(&cow->node, &buf->node, root->blocksize);
 	btrfs_set_header_blocknr(&cow->node.header, cow->blocknr);
 	*cow_ret = cow;
-	btrfs_inc_ref(root, buf);
+	btrfs_inc_ref(trans, root, buf);
 	if (buf == root->node) {
 		root->node = cow;
 		cow->count++;
 		if (buf != root->commit_root)
-			btrfs_free_extent(root, buf->blocknr, 1, 1);
+			btrfs_free_extent(trans, root, buf->blocknr, 1, 1);
 		btrfs_block_release(root, buf);
 	} else {
 		btrfs_set_node_blockptr(&parent->node, parent_slot,
 					cow->blocknr);
 		BUG_ON(list_empty(&parent->dirty));
-		btrfs_free_extent(root, buf->blocknr, 1, 1);
+		btrfs_free_extent(trans, root, buf->blocknr, 1, 1);
 	}
 	btrfs_block_release(root, buf);
 	return 0;
@@ -266,8 +266,8 @@ static struct btrfs_buffer *read_node_slot(struct btrfs_root *root,
 	return read_tree_block(root, btrfs_node_blockptr(node, slot));
 }
 
-static int balance_level(struct btrfs_root *root, struct btrfs_path *path,
-			int level)
+static int balance_level(struct btrfs_trans_handle *trans, struct btrfs_root
+			 *root, struct btrfs_path *path, int level)
 {
 	struct btrfs_buffer *right_buf;
 	struct btrfs_buffer *mid_buf;
@@ -310,8 +310,8 @@ static int balance_level(struct btrfs_root *root, struct btrfs_path *path,
 		btrfs_block_release(root, mid_buf);
 		/* once for the root ptr */
 		btrfs_block_release(root, mid_buf);
-		clean_tree_block(root, mid_buf);
-		return btrfs_free_extent(root, blocknr, 1, 1);
+		clean_tree_block(trans, root, mid_buf);
+		return btrfs_free_extent(trans, root, blocknr, 1, 1);
 	}
 	parent = &parent_buf->node;
 
@@ -324,11 +324,11 @@ static int balance_level(struct btrfs_root *root, struct btrfs_path *path,
 
 	/* first, try to make some room in the middle buffer */
 	if (left_buf) {
-		btrfs_cow_block(root, left_buf, parent_buf,
-				pslot - 1, &left_buf);
+		btrfs_cow_block(trans, root, left_buf, parent_buf, pslot - 1,
+				&left_buf);
 		left = &left_buf->node;
 		orig_slot += btrfs_header_nritems(&left->header);
-		wret = push_node_left(root, left_buf, mid_buf);
+		wret = push_node_left(trans, root, left_buf, mid_buf);
 		if (wret < 0)
 			ret = wret;
 	}
@@ -337,22 +337,23 @@ static int balance_level(struct btrfs_root *root, struct btrfs_path *path,
 	 * then try to empty the right most buffer into the middle
 	 */
 	if (right_buf) {
-		btrfs_cow_block(root, right_buf, parent_buf,
-				pslot + 1, &right_buf);
+		btrfs_cow_block(trans, root, right_buf, parent_buf, pslot + 1,
+				&right_buf);
 		right = &right_buf->node;
-		wret = push_node_left(root, mid_buf, right_buf);
+		wret = push_node_left(trans, root, mid_buf, right_buf);
 		if (wret < 0)
 			ret = wret;
 		if (btrfs_header_nritems(&right->header) == 0) {
 			u64 blocknr = right_buf->blocknr;
 			btrfs_block_release(root, right_buf);
-			clean_tree_block(root, right_buf);
+			clean_tree_block(trans, root, right_buf);
 			right_buf = NULL;
 			right = NULL;
-			wret = del_ptr(root, path, level + 1, pslot + 1);
+			wret = del_ptr(trans, root, path, level + 1, pslot +
+				       1);
 			if (wret)
 				ret = wret;
-			wret = btrfs_free_extent(root, blocknr, 1, 1);
+			wret = btrfs_free_extent(trans, root, blocknr, 1, 1);
 			if (wret)
 				ret = wret;
 		} else {
@@ -373,7 +374,7 @@ static int balance_level(struct btrfs_root *root, struct btrfs_path *path,
 		 * right
 		 */
 		BUG_ON(!left_buf);
-		wret = balance_node_right(root, mid_buf, left_buf);
+		wret = balance_node_right(trans, root, mid_buf, left_buf);
 		if (wret < 0)
 			ret = wret;
 		BUG_ON(wret == 1);
@@ -382,13 +383,13 @@ static int balance_level(struct btrfs_root *root, struct btrfs_path *path,
 		/* we've managed to empty the middle node, drop it */
 		u64 blocknr = mid_buf->blocknr;
 		btrfs_block_release(root, mid_buf);
-		clean_tree_block(root, mid_buf);
+		clean_tree_block(trans, root, mid_buf);
 		mid_buf = NULL;
 		mid = NULL;
-		wret = del_ptr(root, path, level + 1, pslot);
+		wret = del_ptr(trans, root, path, level + 1, pslot);
 		if (wret)
 			ret = wret;
-		wret = btrfs_free_extent(root, blocknr, 1, 1);
+		wret = btrfs_free_extent(trans, root, blocknr, 1, 1);
 		if (wret)
 			ret = wret;
 	} else {
@@ -438,8 +439,9 @@ static int balance_level(struct btrfs_root *root, struct btrfs_path *path,
  * tree.  if ins_len < 0, nodes will be merged as we walk down the tree (if
  * possible)
  */
-int btrfs_search_slot(struct btrfs_root *root, struct btrfs_key *key,
-		struct btrfs_path *p, int ins_len, int cow)
+int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root
+		      *root, struct btrfs_key *key, struct btrfs_path *p, int
+		      ins_len, int cow)
 {
 	struct btrfs_buffer *b;
 	struct btrfs_buffer *cow_buf;
@@ -455,8 +457,9 @@ int btrfs_search_slot(struct btrfs_root *root, struct btrfs_key *key,
 		level = btrfs_header_level(&b->node.header);
 		if (cow) {
 			int wret;
-			wret = btrfs_cow_block(root, b, p->nodes[level + 1],
-					       p->slots[level + 1], &cow_buf);
+			wret = btrfs_cow_block(trans, root, b, p->nodes[level +
+					       1], p->slots[level + 1],
+					       &cow_buf);
 			b = cow_buf;
 		}
 		BUG_ON(!cow && ins_len);
@@ -472,7 +475,7 @@ int btrfs_search_slot(struct btrfs_root *root, struct btrfs_key *key,
 			p->slots[level] = slot;
 			if (ins_len > 0 && btrfs_header_nritems(&c->header) ==
 			    BTRFS_NODEPTRS_PER_BLOCK(root)) {
-				int sret = split_node(root, p, level);
+				int sret = split_node(trans, root, p, level);
 				BUG_ON(sret > 0);
 				if (sret)
 					return sret;
@@ -480,7 +483,8 @@ int btrfs_search_slot(struct btrfs_root *root, struct btrfs_key *key,
 				c = &b->node;
 				slot = p->slots[level];
 			} else if (ins_len < 0) {
-				int sret = balance_level(root, p, level);
+				int sret = balance_level(trans, root, p,
+							 level);
 				if (sret)
 					return sret;
 				b = p->nodes[level];
@@ -496,7 +500,7 @@ int btrfs_search_slot(struct btrfs_root *root, struct btrfs_key *key,
 			p->slots[level] = slot;
 			if (ins_len > 0 && btrfs_leaf_free_space(root, l) <
 			    sizeof(struct btrfs_item) + ins_len) {
-				int sret = split_leaf(root, p, ins_len);
+				int sret = split_leaf(trans, root, p, ins_len);
 				BUG_ON(sret > 0);
 				if (sret)
 					return sret;
@@ -519,9 +523,9 @@ int btrfs_search_slot(struct btrfs_root *root, struct btrfs_key *key,
  * If this fails to write a tree block, it returns -1, but continues
  * fixing up the blocks in ram so the tree is consistent.
  */
-static int fixup_low_keys(struct btrfs_root *root,
-			   struct btrfs_path *path, struct btrfs_disk_key *key,
-			   int level)
+static int fixup_low_keys(struct btrfs_trans_handle *trans, struct btrfs_root
+			  *root, struct btrfs_path *path, struct btrfs_disk_key
+			  *key, int level)
 {
 	int i;
 	int ret = 0;
@@ -546,8 +550,9 @@ static int fixup_low_keys(struct btrfs_root *root,
  * returns 0 if some ptrs were pushed left, < 0 if there was some horrible
  * error, and > 0 if there was no room in the left hand block.
  */
-static int push_node_left(struct btrfs_root *root, struct btrfs_buffer *dst_buf,
-			  struct btrfs_buffer *src_buf)
+static int push_node_left(struct btrfs_trans_handle *trans, struct btrfs_root
+			  *root, struct btrfs_buffer *dst_buf, struct
+			  btrfs_buffer *src_buf)
 {
 	struct btrfs_node *src = &src_buf->node;
 	struct btrfs_node *dst = &dst_buf->node;
@@ -589,8 +594,8 @@ static int push_node_left(struct btrfs_root *root, struct btrfs_buffer *dst_buf,
  *
  * this will  only push up to 1/2 the contents of the left node over
  */
-static int balance_node_right(struct btrfs_root *root,
-			      struct btrfs_buffer *dst_buf,
+static int balance_node_right(struct btrfs_trans_handle *trans, struct
+			      btrfs_root *root, struct btrfs_buffer *dst_buf,
 			      struct btrfs_buffer *src_buf)
 {
 	struct btrfs_node *src = &src_buf->node;
@@ -635,8 +640,8 @@ static int balance_node_right(struct btrfs_root *root,
  *
  * returns zero on success or < 0 on failure.
  */
-static int insert_new_root(struct btrfs_root *root,
-			   struct btrfs_path *path, int level)
+static int insert_new_root(struct btrfs_trans_handle *trans, struct btrfs_root
+			   *root, struct btrfs_path *path, int level)
 {
 	struct btrfs_buffer *t;
 	struct btrfs_node *lower;
@@ -646,7 +651,7 @@ static int insert_new_root(struct btrfs_root *root,
 	BUG_ON(path->nodes[level]);
 	BUG_ON(path->nodes[level-1] != root->node);
 
-	t = btrfs_alloc_free_block(root);
+	t = btrfs_alloc_free_block(trans, root);
 	c = &t->node;
 	memset(c, 0, root->blocksize);
 	btrfs_set_header_nritems(&c->header, 1);
@@ -679,9 +684,9 @@ static int insert_new_root(struct btrfs_root *root,
  *
  * returns zero on success and < 0 on any error
  */
-static int insert_ptr(struct btrfs_root *root,
-		struct btrfs_path *path, struct btrfs_disk_key *key,
-		u64 blocknr, int slot, int level)
+static int insert_ptr(struct btrfs_trans_handle *trans, struct btrfs_root
+		      *root, struct btrfs_path *path, struct btrfs_disk_key
+		      *key, u64 blocknr, int slot, int level)
 {
 	struct btrfs_node *lower;
 	int nritems;
@@ -713,8 +718,8 @@ static int insert_ptr(struct btrfs_root *root,
  *
  * returns 0 on success and < 0 on failure
  */
-static int split_node(struct btrfs_root *root, struct btrfs_path *path,
-		      int level)
+static int split_node(struct btrfs_trans_handle *trans, struct btrfs_root
+		      *root, struct btrfs_path *path, int level)
 {
 	struct btrfs_buffer *t;
 	struct btrfs_node *c;
@@ -729,12 +734,12 @@ static int split_node(struct btrfs_root *root, struct btrfs_path *path,
 	c = &t->node;
 	if (t == root->node) {
 		/* trying to split the root, lets make a new one */
-		ret = insert_new_root(root, path, level + 1);
+		ret = insert_new_root(trans, root, path, level + 1);
 		if (ret)
 			return ret;
 	}
 	c_nritems = btrfs_header_nritems(&c->header);
-	split_buffer = btrfs_alloc_free_block(root);
+	split_buffer = btrfs_alloc_free_block(trans, root);
 	split = &split_buffer->node;
 	btrfs_set_header_flags(&split->header, btrfs_header_flags(&c->header));
 	btrfs_set_header_blocknr(&split->header, split_buffer->blocknr);
@@ -748,7 +753,7 @@ static int split_node(struct btrfs_root *root, struct btrfs_path *path,
 	ret = 0;
 
 	BUG_ON(list_empty(&t->dirty));
-	wret = insert_ptr(root, path, &split->ptrs[0].key,
+	wret = insert_ptr(trans, root, path, &split->ptrs[0].key,
 			  split_buffer->blocknr, path->slots[level + 1] + 1,
 			  level + 1);
 	if (wret)
@@ -790,8 +795,8 @@ static int leaf_space_used(struct btrfs_leaf *l, int start, int nr)
  * returns 1 if the push failed because the other node didn't have enough
  * room, 0 if everything worked out and < 0 if there were major errors.
  */
-static int push_leaf_right(struct btrfs_root *root, struct btrfs_path *path,
-			   int data_size)
+static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
+			   *root, struct btrfs_path *path, int data_size)
 {
 	struct btrfs_buffer *left_buf = path->nodes[0];
 	struct btrfs_leaf *left = &left_buf->leaf;
@@ -824,7 +829,7 @@ static int push_leaf_right(struct btrfs_root *root, struct btrfs_path *path,
 		return 1;
 	}
 	/* cow and double check */
-	btrfs_cow_block(root, right_buf, upper, slot + 1, &right_buf);
+	btrfs_cow_block(trans, root, right_buf, upper, slot + 1, &right_buf);
 	right = &right_buf->leaf;
 	free_space = btrfs_leaf_free_space(root, right);
 	if (free_space < data_size + sizeof(struct btrfs_item)) {
@@ -897,8 +902,8 @@ static int push_leaf_right(struct btrfs_root *root, struct btrfs_path *path,
  * push some data in the path leaf to the left, trying to free up at
  * least data_size bytes.  returns zero if the push worked, nonzero otherwise
  */
-static int push_leaf_left(struct btrfs_root *root, struct btrfs_path *path,
-			  int data_size)
+static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
+			  *root, struct btrfs_path *path, int data_size)
 {
 	struct btrfs_buffer *right_buf = path->nodes[0];
 	struct btrfs_leaf *right = &right_buf->leaf;
@@ -931,7 +936,7 @@ static int push_leaf_left(struct btrfs_root *root, struct btrfs_path *path,
 	}
 
 	/* cow and double check */
-	btrfs_cow_block(root, t, path->nodes[1], slot - 1, &t);
+	btrfs_cow_block(trans, root, t, path->nodes[1], slot - 1, &t);
 	left = &t->leaf;
 	free_space = btrfs_leaf_free_space(root, left);
 	if (free_space < data_size + sizeof(struct btrfs_item)) {
@@ -997,7 +1002,7 @@ static int push_leaf_left(struct btrfs_root *root, struct btrfs_path *path,
 	BUG_ON(list_empty(&t->dirty));
 	BUG_ON(list_empty(&right_buf->dirty));
 
-	wret = fixup_low_keys(root, path, &right->items[0].key, 1);
+	wret = fixup_low_keys(trans, root, path, &right->items[0].key, 1);
 	if (wret)
 		ret = wret;
 
@@ -1021,8 +1026,8 @@ static int push_leaf_left(struct btrfs_root *root, struct btrfs_path *path,
  *
  * returns 0 if all went well and < 0 on failure.
  */
-static int split_leaf(struct btrfs_root *root, struct btrfs_path *path,
-		      int data_size)
+static int split_leaf(struct btrfs_trans_handle *trans, struct btrfs_root
+		      *root, struct btrfs_path *path, int data_size)
 {
 	struct btrfs_buffer *l_buf;
 	struct btrfs_leaf *l;
@@ -1038,11 +1043,11 @@ static int split_leaf(struct btrfs_root *root, struct btrfs_path *path,
 	int ret;
 	int wret;
 
-	wret = push_leaf_left(root, path, data_size);
+	wret = push_leaf_left(trans, root, path, data_size);
 	if (wret < 0)
 		return wret;
 	if (wret) {
-		wret = push_leaf_right(root, path, data_size);
+		wret = push_leaf_right(trans, root, path, data_size);
 		if (wret < 0)
 			return wret;
 	}
@@ -1055,14 +1060,14 @@ static int split_leaf(struct btrfs_root *root, struct btrfs_path *path,
 		return 0;
 
 	if (!path->nodes[1]) {
-		ret = insert_new_root(root, path, 1);
+		ret = insert_new_root(trans, root, path, 1);
 		if (ret)
 			return ret;
 	}
 	slot = path->slots[0];
 	nritems = btrfs_header_nritems(&l->header);
 	mid = (nritems + 1)/ 2;
-	right_buffer = btrfs_alloc_free_block(root);
+	right_buffer = btrfs_alloc_free_block(trans, root);
 	BUG_ON(!right_buffer);
 	BUG_ON(mid == nritems);
 	right = &right_buffer->leaf;
@@ -1100,7 +1105,7 @@ static int split_leaf(struct btrfs_root *root, struct btrfs_path *path,
 
 	btrfs_set_header_nritems(&l->header, mid);
 	ret = 0;
-	wret = insert_ptr(root, path, &right->items[0].key,
+	wret = insert_ptr(trans, root, path, &right->items[0].key,
 			  right_buffer->blocknr, path->slots[1] + 1, 1);
 	if (wret)
 		ret = wret;
@@ -1122,8 +1127,9 @@ static int split_leaf(struct btrfs_root *root, struct btrfs_path *path,
  * Given a key and some data, insert an item into the tree.
  * This does all the path init required, making room in the tree if needed.
  */
-int btrfs_insert_empty_item(struct btrfs_root *root, struct btrfs_path *path,
-			    struct btrfs_key *cpu_key, u32 data_size)
+int btrfs_insert_empty_item(struct btrfs_trans_handle *trans, struct btrfs_root
+			    *root, struct btrfs_path *path, struct btrfs_key
+			    *cpu_key, u32 data_size)
 {
 	int ret = 0;
 	int slot;
@@ -1139,7 +1145,7 @@ int btrfs_insert_empty_item(struct btrfs_root *root, struct btrfs_path *path,
 	/* create a root if there isn't one */
 	if (!root->node)
 		BUG();
-	ret = btrfs_search_slot(root, cpu_key, path, data_size, 1);
+	ret = btrfs_search_slot(trans, root, cpu_key, path, data_size, 1);
 	if (ret == 0) {
 		btrfs_release_path(root, path);
 		return -EEXIST;
@@ -1193,7 +1199,7 @@ int btrfs_insert_empty_item(struct btrfs_root *root, struct btrfs_path *path,
 
 	ret = 0;
 	if (slot == 0)
-		ret = fixup_low_keys(root, path, &disk_key, 1);
+		ret = fixup_low_keys(trans, root, path, &disk_key, 1);
 
 	BUG_ON(list_empty(&leaf_buf->dirty));
 	if (btrfs_leaf_free_space(root, leaf) < 0)
@@ -1207,15 +1213,16 @@ int btrfs_insert_empty_item(struct btrfs_root *root, struct btrfs_path *path,
  * Given a key and some data, insert an item into the tree.
  * This does all the path init required, making room in the tree if needed.
  */
-int btrfs_insert_item(struct btrfs_root *root, struct btrfs_key *cpu_key,
-			  void *data, u32 data_size)
+int btrfs_insert_item(struct btrfs_trans_handle *trans, struct btrfs_root
+		      *root, struct btrfs_key *cpu_key, void *data, u32
+		      data_size)
 {
 	int ret = 0;
 	struct btrfs_path path;
 	u8 *ptr;
 
 	btrfs_init_path(&path);
-	ret = btrfs_insert_empty_item(root, &path, cpu_key, data_size);
+	ret = btrfs_insert_empty_item(trans, root, &path, cpu_key, data_size);
 	if (!ret) {
 		ptr = btrfs_item_ptr(&path.nodes[0]->leaf, path.slots[0], u8);
 		memcpy(ptr, data, data_size);
@@ -1231,8 +1238,8 @@ int btrfs_insert_item(struct btrfs_root *root, struct btrfs_key *cpu_key,
  * continuing all the way the root if required.  The root is converted into
  * a leaf if all the nodes are emptied.
  */
-static int del_ptr(struct btrfs_root *root, struct btrfs_path *path, int level,
-		   int slot)
+static int del_ptr(struct btrfs_trans_handle *trans, struct btrfs_root *root,
+		   struct btrfs_path *path, int level, int slot)
 {
 	struct btrfs_node *node;
 	struct btrfs_buffer *parent = path->nodes[level];
@@ -1253,7 +1260,7 @@ static int del_ptr(struct btrfs_root *root, struct btrfs_path *path, int level,
 		/* just turn the root into a leaf and break */
 		btrfs_set_header_level(&root->node->node.header, 0);
 	} else if (slot == 0) {
-		wret = fixup_low_keys(root, path, &node->ptrs[0].key,
+		wret = fixup_low_keys(trans, root, path, &node->ptrs[0].key,
 				      level + 1);
 		if (wret)
 			ret = wret;
@@ -1266,7 +1273,8 @@ static int del_ptr(struct btrfs_root *root, struct btrfs_path *path, int level,
  * delete the item at the leaf level in path.  If that empties
  * the leaf, remove it from the tree
  */
-int btrfs_del_item(struct btrfs_root *root, struct btrfs_path *path)
+int btrfs_del_item(struct btrfs_trans_handle *trans, struct btrfs_root *root,
+		   struct btrfs_path *path)
 {
 	int slot;
 	struct btrfs_leaf *leaf;
@@ -1306,19 +1314,20 @@ int btrfs_del_item(struct btrfs_root *root, struct btrfs_path *path)
 			btrfs_set_header_level(&leaf->header, 0);
 			BUG_ON(list_empty(&leaf_buf->dirty));
 		} else {
-			clean_tree_block(root, leaf_buf);
-			wret = del_ptr(root, path, 1, path->slots[1]);
+			clean_tree_block(trans, root, leaf_buf);
+			wret = del_ptr(trans, root, path, 1, path->slots[1]);
 			if (wret)
 				ret = wret;
-			wret = btrfs_free_extent(root, leaf_buf->blocknr, 1, 1);
+			wret = btrfs_free_extent(trans, root,
+						 leaf_buf->blocknr, 1, 1);
 			if (wret)
 				ret = wret;
 		}
 	} else {
 		int used = leaf_space_used(leaf, 0, nritems);
 		if (slot == 0) {
-			wret = fixup_low_keys(root, path,
-						   &leaf->items[0].key, 1);
+			wret = fixup_low_keys(trans, root, path,
+					      &leaf->items[0].key, 1);
 			if (wret)
 				ret = wret;
 		}
@@ -1332,23 +1341,24 @@ int btrfs_del_item(struct btrfs_root *root, struct btrfs_path *path)
 			 */
 			slot = path->slots[1];
 			leaf_buf->count++;
-			wret = push_leaf_left(root, path, 1);
+			wret = push_leaf_left(trans, root, path, 1);
 			if (wret < 0)
 				ret = wret;
 			if (path->nodes[0] == leaf_buf &&
 			    btrfs_header_nritems(&leaf->header)) {
-				wret = push_leaf_right(root, path, 1);
+				wret = push_leaf_right(trans, root, path, 1);
 				if (wret < 0)
 					ret = wret;
 			}
 			if (btrfs_header_nritems(&leaf->header) == 0) {
 				u64 blocknr = leaf_buf->blocknr;
-				clean_tree_block(root, leaf_buf);
-				wret = del_ptr(root, path, 1, slot);
+				clean_tree_block(trans, root, leaf_buf);
+				wret = del_ptr(trans, root, path, 1, slot);
 				if (wret)
 					ret = wret;
 				btrfs_block_release(root, leaf_buf);
-				wret = btrfs_free_extent(root, blocknr, 1, 1);
+				wret = btrfs_free_extent(trans, root, blocknr,
+							 1, 1);
 				if (wret)
 					ret = wret;
 			} else {
@@ -1401,5 +1411,3 @@ int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 	}
 	return 0;
 }
-
-

commit 88fd146c27da0f34c512f47e2b3776a0762ecd81
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Mar 16 08:56:18 2007 -0400

    Btrfs: pin freed blocks from the FS tree too
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 68f66b320a88..13128b5ed65c 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -55,13 +55,13 @@ static int btrfs_cow_block(struct btrfs_root *root,
 		root->node = cow;
 		cow->count++;
 		if (buf != root->commit_root)
-			btrfs_free_extent(root, buf->blocknr, 1);
+			btrfs_free_extent(root, buf->blocknr, 1, 1);
 		btrfs_block_release(root, buf);
 	} else {
 		btrfs_set_node_blockptr(&parent->node, parent_slot,
 					cow->blocknr);
 		BUG_ON(list_empty(&parent->dirty));
-		btrfs_free_extent(root, buf->blocknr, 1);
+		btrfs_free_extent(root, buf->blocknr, 1, 1);
 	}
 	btrfs_block_release(root, buf);
 	return 0;
@@ -311,7 +311,7 @@ static int balance_level(struct btrfs_root *root, struct btrfs_path *path,
 		/* once for the root ptr */
 		btrfs_block_release(root, mid_buf);
 		clean_tree_block(root, mid_buf);
-		return btrfs_free_extent(root, blocknr, 1);
+		return btrfs_free_extent(root, blocknr, 1, 1);
 	}
 	parent = &parent_buf->node;
 
@@ -352,7 +352,7 @@ static int balance_level(struct btrfs_root *root, struct btrfs_path *path,
 			wret = del_ptr(root, path, level + 1, pslot + 1);
 			if (wret)
 				ret = wret;
-			wret = btrfs_free_extent(root, blocknr, 1);
+			wret = btrfs_free_extent(root, blocknr, 1, 1);
 			if (wret)
 				ret = wret;
 		} else {
@@ -388,7 +388,7 @@ static int balance_level(struct btrfs_root *root, struct btrfs_path *path,
 		wret = del_ptr(root, path, level + 1, pslot);
 		if (wret)
 			ret = wret;
-		wret = btrfs_free_extent(root, blocknr, 1);
+		wret = btrfs_free_extent(root, blocknr, 1, 1);
 		if (wret)
 			ret = wret;
 	} else {
@@ -1310,7 +1310,7 @@ int btrfs_del_item(struct btrfs_root *root, struct btrfs_path *path)
 			wret = del_ptr(root, path, 1, path->slots[1]);
 			if (wret)
 				ret = wret;
-			wret = btrfs_free_extent(root, leaf_buf->blocknr, 1);
+			wret = btrfs_free_extent(root, leaf_buf->blocknr, 1, 1);
 			if (wret)
 				ret = wret;
 		}
@@ -1348,7 +1348,7 @@ int btrfs_del_item(struct btrfs_root *root, struct btrfs_path *path)
 				if (wret)
 					ret = wret;
 				btrfs_block_release(root, leaf_buf);
-				wret = btrfs_free_extent(root, blocknr, 1);
+				wret = btrfs_free_extent(root, blocknr, 1, 1);
 				if (wret)
 					ret = wret;
 			} else {

commit a8a2ee0c600a213d13170c2f4d7bd0b304bbec19
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Mar 16 08:46:49 2007 -0400

    Btrfs: add a name_len to dir items, reorder key
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 7edfbd468042..68f66b320a88 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -107,14 +107,14 @@ static int comp_keys(struct btrfs_disk_key *disk, struct btrfs_key *k2)
 		return 1;
 	if (k1.objectid < k2->objectid)
 		return -1;
-	if (k1.offset > k2->offset)
-		return 1;
-	if (k1.offset < k2->offset)
-		return -1;
 	if (k1.flags > k2->flags)
 		return 1;
 	if (k1.flags < k2->flags)
 		return -1;
+	if (k1.offset > k2->offset)
+		return 1;
+	if (k1.offset < k2->offset)
+		return -1;
 	return 0;
 }
 

commit 62e2749e03a855d98855f9ce032dbe72d5fad148
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Mar 15 12:56:47 2007 -0400

    Btrfs: Use a chunk of the key flags to record the item type.
    Add (untested and simple) directory item code
    Fix comp_keys to use the new key ordering
    Add btrfs_insert_empty_item
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index ece8de7f38ef..7edfbd468042 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -107,14 +107,14 @@ static int comp_keys(struct btrfs_disk_key *disk, struct btrfs_key *k2)
 		return 1;
 	if (k1.objectid < k2->objectid)
 		return -1;
-	if (k1.flags > k2->flags)
-		return 1;
-	if (k1.flags < k2->flags)
-		return -1;
 	if (k1.offset > k2->offset)
 		return 1;
 	if (k1.offset < k2->offset)
 		return -1;
+	if (k1.flags > k2->flags)
+		return 1;
+	if (k1.flags < k2->flags)
+		return -1;
 	return 0;
 }
 
@@ -1122,8 +1122,8 @@ static int split_leaf(struct btrfs_root *root, struct btrfs_path *path,
  * Given a key and some data, insert an item into the tree.
  * This does all the path init required, making room in the tree if needed.
  */
-int btrfs_insert_item(struct btrfs_root *root, struct btrfs_key *cpu_key,
-			  void *data, int data_size)
+int btrfs_insert_empty_item(struct btrfs_root *root, struct btrfs_path *path,
+			    struct btrfs_key *cpu_key, u32 data_size)
 {
 	int ret = 0;
 	int slot;
@@ -1132,7 +1132,6 @@ int btrfs_insert_item(struct btrfs_root *root, struct btrfs_key *cpu_key,
 	struct btrfs_buffer *leaf_buf;
 	u32 nritems;
 	unsigned int data_end;
-	struct btrfs_path path;
 	struct btrfs_disk_key disk_key;
 
 	btrfs_cpu_key_to_disk(&disk_key, cpu_key);
@@ -1140,17 +1139,16 @@ int btrfs_insert_item(struct btrfs_root *root, struct btrfs_key *cpu_key,
 	/* create a root if there isn't one */
 	if (!root->node)
 		BUG();
-	btrfs_init_path(&path);
-	ret = btrfs_search_slot(root, cpu_key, &path, data_size, 1);
+	ret = btrfs_search_slot(root, cpu_key, path, data_size, 1);
 	if (ret == 0) {
-		btrfs_release_path(root, &path);
+		btrfs_release_path(root, path);
 		return -EEXIST;
 	}
 	if (ret < 0)
 		goto out;
 
-	slot_orig = path.slots[0];
-	leaf_buf = path.nodes[0];
+	slot_orig = path->slots[0];
+	leaf_buf = path->nodes[0];
 	leaf = &leaf_buf->leaf;
 
 	nritems = btrfs_header_nritems(&leaf->header);
@@ -1160,7 +1158,7 @@ int btrfs_insert_item(struct btrfs_root *root, struct btrfs_key *cpu_key,
 	    sizeof(struct btrfs_item) + data_size)
 		BUG();
 
-	slot = path.slots[0];
+	slot = path->slots[0];
 	BUG_ON(slot < 0);
 	if (slot != nritems) {
 		int i;
@@ -1186,23 +1184,42 @@ int btrfs_insert_item(struct btrfs_root *root, struct btrfs_key *cpu_key,
 		        data_end, old_data - data_end);
 		data_end = old_data;
 	}
-	/* copy the new data in */
+	/* setup the item for the new data */
 	memcpy(&leaf->items[slot].key, &disk_key,
 		sizeof(struct btrfs_disk_key));
 	btrfs_set_item_offset(leaf->items + slot, data_end - data_size);
 	btrfs_set_item_size(leaf->items + slot, data_size);
-	memcpy(btrfs_leaf_data(leaf) + data_end - data_size, data, data_size);
 	btrfs_set_header_nritems(&leaf->header, nritems + 1);
 
 	ret = 0;
 	if (slot == 0)
-		ret = fixup_low_keys(root, &path, &disk_key, 1);
+		ret = fixup_low_keys(root, path, &disk_key, 1);
 
 	BUG_ON(list_empty(&leaf_buf->dirty));
 	if (btrfs_leaf_free_space(root, leaf) < 0)
 		BUG();
-	check_leaf(root, &path, 0);
+	check_leaf(root, path, 0);
 out:
+	return ret;
+}
+
+/*
+ * Given a key and some data, insert an item into the tree.
+ * This does all the path init required, making room in the tree if needed.
+ */
+int btrfs_insert_item(struct btrfs_root *root, struct btrfs_key *cpu_key,
+			  void *data, u32 data_size)
+{
+	int ret = 0;
+	struct btrfs_path path;
+	u8 *ptr;
+
+	btrfs_init_path(&path);
+	ret = btrfs_insert_empty_item(root, &path, cpu_key, data_size);
+	if (!ret) {
+		ptr = btrfs_item_ptr(&path.nodes[0]->leaf, path.slots[0], u8);
+		memcpy(ptr, data, data_size);
+	}
 	btrfs_release_path(root, &path);
 	return ret;
 }

commit 123abc88c9087b9c5605566ee3491aaef17fd837
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Mar 14 14:14:43 2007 -0400

    Btrfs: variable block size support
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index dc620186e501..ece8de7f38ef 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -47,7 +47,7 @@ static int btrfs_cow_block(struct btrfs_root *root,
 		return 0;
 	}
 	cow = btrfs_alloc_free_block(root);
-	memcpy(&cow->node, &buf->node, sizeof(buf->node));
+	memcpy(&cow->node, &buf->node, root->blocksize);
 	btrfs_set_header_blocknr(&cow->node.header, cow->blocknr);
 	*cow_ret = cow;
 	btrfs_inc_ref(root, buf);
@@ -72,11 +72,12 @@ static int btrfs_cow_block(struct btrfs_root *root,
  * this returns the address of the start of the last item,
  * which is the stop of the leaf data stack
  */
-static inline unsigned int leaf_data_end(struct btrfs_leaf *leaf)
+static inline unsigned int leaf_data_end(struct btrfs_root *root,
+					 struct btrfs_leaf *leaf)
 {
 	u32 nr = btrfs_header_nritems(&leaf->header);
 	if (nr == 0)
-		return sizeof(leaf->data);
+		return BTRFS_LEAF_DATA_SIZE(root);
 	return btrfs_item_offset(leaf->items + nr - 1);
 }
 
@@ -85,12 +86,12 @@ static inline unsigned int leaf_data_end(struct btrfs_leaf *leaf)
  * the start of the leaf data.  IOW, how much room
  * the leaf has left for both items and data
  */
-int btrfs_leaf_free_space(struct btrfs_leaf *leaf)
+int btrfs_leaf_free_space(struct btrfs_root *root, struct btrfs_leaf *leaf)
 {
-	int data_end = leaf_data_end(leaf);
+	int data_end = leaf_data_end(root, leaf);
 	int nritems = btrfs_header_nritems(&leaf->header);
 	char *items_end = (char *)(leaf->items + nritems + 1);
-	return (char *)(leaf->data + data_end) - (char *)items_end;
+	return (char *)(btrfs_leaf_data(leaf) + data_end) - (char *)items_end;
 }
 
 /*
@@ -117,7 +118,8 @@ static int comp_keys(struct btrfs_disk_key *disk, struct btrfs_key *k2)
 	return 0;
 }
 
-static int check_node(struct btrfs_path *path, int level)
+static int check_node(struct btrfs_root *root, struct btrfs_path *path,
+		      int level)
 {
 	int i;
 	struct btrfs_node *parent = NULL;
@@ -131,22 +133,23 @@ static int check_node(struct btrfs_path *path, int level)
 	BUG_ON(nritems == 0);
 	if (parent) {
 		struct btrfs_disk_key *parent_key;
-		parent_key = &parent->keys[parent_slot];
-		BUG_ON(memcmp(parent_key, node->keys,
+		parent_key = &parent->ptrs[parent_slot].key;
+		BUG_ON(memcmp(parent_key, &node->ptrs[0].key,
 			      sizeof(struct btrfs_disk_key)));
 		BUG_ON(btrfs_node_blockptr(parent, parent_slot) !=
 		       btrfs_header_blocknr(&node->header));
 	}
-	BUG_ON(nritems > NODEPTRS_PER_BLOCK);
+	BUG_ON(nritems > BTRFS_NODEPTRS_PER_BLOCK(root));
 	for (i = 0; nritems > 1 && i < nritems - 2; i++) {
 		struct btrfs_key cpukey;
-		btrfs_disk_key_to_cpu(&cpukey, &node->keys[i + 1]);
-		BUG_ON(comp_keys(&node->keys[i], &cpukey) >= 0);
+		btrfs_disk_key_to_cpu(&cpukey, &node->ptrs[i + 1].key);
+		BUG_ON(comp_keys(&node->ptrs[i].key, &cpukey) >= 0);
 	}
 	return 0;
 }
 
-static int check_leaf(struct btrfs_path *path, int level)
+static int check_leaf(struct btrfs_root *root, struct btrfs_path *path,
+		      int level)
 {
 	int i;
 	struct btrfs_leaf *leaf = &path->nodes[level]->leaf;
@@ -157,14 +160,14 @@ static int check_leaf(struct btrfs_path *path, int level)
 	if (path->nodes[level + 1])
 		parent = &path->nodes[level + 1]->node;
 	parent_slot = path->slots[level + 1];
-	BUG_ON(btrfs_leaf_free_space(leaf) < 0);
+	BUG_ON(btrfs_leaf_free_space(root, leaf) < 0);
 
 	if (nritems == 0)
 		return 0;
 
 	if (parent) {
 		struct btrfs_disk_key *parent_key;
-		parent_key = &parent->keys[parent_slot];
+		parent_key = &parent->ptrs[parent_slot].key;
 		BUG_ON(memcmp(parent_key, &leaf->items[0].key,
 		       sizeof(struct btrfs_disk_key)));
 		BUG_ON(btrfs_node_blockptr(parent, parent_slot) !=
@@ -180,17 +183,18 @@ static int check_leaf(struct btrfs_path *path, int level)
 		if (i == 0) {
 			BUG_ON(btrfs_item_offset(leaf->items + i) +
 			       btrfs_item_size(leaf->items + i) !=
-			       LEAF_DATA_SIZE);
+			       BTRFS_LEAF_DATA_SIZE(root));
 		}
 	}
 	return 0;
 }
 
-static int check_block(struct btrfs_path *path, int level)
+static int check_block(struct btrfs_root *root, struct btrfs_path *path,
+			int level)
 {
 	if (level == 0)
-		return check_leaf(path, level);
-	return check_node(path, level);
+		return check_leaf(root, path, level);
+	return check_node(root, path, level);
 }
 
 /*
@@ -242,8 +246,8 @@ static int bin_search(struct btrfs_node *c, struct btrfs_key *key, int *slot)
 					  key, btrfs_header_nritems(&c->header),
 					  slot);
 	} else {
-		return generic_bin_search((void *)c->keys,
-					  sizeof(struct btrfs_disk_key),
+		return generic_bin_search((void *)c->ptrs,
+					  sizeof(struct btrfs_key_ptr),
 					  key, btrfs_header_nritems(&c->header),
 					  slot);
 	}
@@ -311,7 +315,8 @@ static int balance_level(struct btrfs_root *root, struct btrfs_path *path,
 	}
 	parent = &parent_buf->node;
 
-	if (btrfs_header_nritems(&mid->header) > NODEPTRS_PER_BLOCK / 4)
+	if (btrfs_header_nritems(&mid->header) >
+	    BTRFS_NODEPTRS_PER_BLOCK(root) / 4)
 		return 0;
 
 	left_buf = read_node_slot(root, parent_buf, pslot - 1);
@@ -351,7 +356,8 @@ static int balance_level(struct btrfs_root *root, struct btrfs_path *path,
 			if (wret)
 				ret = wret;
 		} else {
-			memcpy(parent->keys + pslot + 1, right->keys,
+			memcpy(&parent->ptrs[pslot + 1].key,
+				&right->ptrs[0].key,
 				sizeof(struct btrfs_disk_key));
 			BUG_ON(list_empty(&parent_buf->dirty));
 		}
@@ -387,7 +393,7 @@ static int balance_level(struct btrfs_root *root, struct btrfs_path *path,
 			ret = wret;
 	} else {
 		/* update the parent key to reflect our changes */
-		memcpy(parent->keys + pslot, mid->keys,
+		memcpy(&parent->ptrs[pslot].key, &mid->ptrs[0].key,
 		       sizeof(struct btrfs_disk_key));
 		BUG_ON(list_empty(&parent_buf->dirty));
 	}
@@ -407,7 +413,7 @@ static int balance_level(struct btrfs_root *root, struct btrfs_path *path,
 		}
 	}
 	/* double check we haven't messed things up */
-	check_block(path, level);
+	check_block(root, path, level);
 	if (orig_ptr != btrfs_node_blockptr(&path->nodes[level]->node,
 					    path->slots[level]))
 		BUG();
@@ -456,7 +462,7 @@ int btrfs_search_slot(struct btrfs_root *root, struct btrfs_key *key,
 		BUG_ON(!cow && ins_len);
 		c = &b->node;
 		p->nodes[level] = b;
-		ret = check_block(p, level);
+		ret = check_block(root, p, level);
 		if (ret)
 			return -1;
 		ret = bin_search(c, key, &slot);
@@ -465,7 +471,7 @@ int btrfs_search_slot(struct btrfs_root *root, struct btrfs_key *key,
 				slot -= 1;
 			p->slots[level] = slot;
 			if (ins_len > 0 && btrfs_header_nritems(&c->header) ==
-			    NODEPTRS_PER_BLOCK) {
+			    BTRFS_NODEPTRS_PER_BLOCK(root)) {
 				int sret = split_node(root, p, level);
 				BUG_ON(sret > 0);
 				if (sret)
@@ -488,7 +494,7 @@ int btrfs_search_slot(struct btrfs_root *root, struct btrfs_key *key,
 		} else {
 			struct btrfs_leaf *l = (struct btrfs_leaf *)c;
 			p->slots[level] = slot;
-			if (ins_len > 0 && btrfs_leaf_free_space(l) <
+			if (ins_len > 0 && btrfs_leaf_free_space(root, l) <
 			    sizeof(struct btrfs_item) + ins_len) {
 				int sret = split_leaf(root, p, ins_len);
 				BUG_ON(sret > 0);
@@ -525,7 +531,7 @@ static int fixup_low_keys(struct btrfs_root *root,
 		if (!path->nodes[i])
 			break;
 		t = &path->nodes[i]->node;
-		memcpy(t->keys + tslot, key, sizeof(*key));
+		memcpy(&t->ptrs[tslot].key, key, sizeof(*key));
 		BUG_ON(list_empty(&path->nodes[i]->dirty));
 		if (tslot != 0)
 			break;
@@ -552,7 +558,7 @@ static int push_node_left(struct btrfs_root *root, struct btrfs_buffer *dst_buf,
 
 	src_nritems = btrfs_header_nritems(&src->header);
 	dst_nritems = btrfs_header_nritems(&dst->header);
-	push_items = NODEPTRS_PER_BLOCK - dst_nritems;
+	push_items = BTRFS_NODEPTRS_PER_BLOCK(root) - dst_nritems;
 	if (push_items <= 0) {
 		return 1;
 	}
@@ -560,16 +566,12 @@ static int push_node_left(struct btrfs_root *root, struct btrfs_buffer *dst_buf,
 	if (src_nritems < push_items)
 		push_items = src_nritems;
 
-	memcpy(dst->keys + dst_nritems, src->keys,
-		push_items * sizeof(struct btrfs_disk_key));
-	memcpy(dst->blockptrs + dst_nritems, src->blockptrs,
-		push_items * sizeof(u64));
+	memcpy(dst->ptrs + dst_nritems, src->ptrs,
+		push_items * sizeof(struct btrfs_key_ptr));
 	if (push_items < src_nritems) {
-		memmove(src->keys, src->keys + push_items,
+		memmove(src->ptrs, src->ptrs + push_items,
 			(src_nritems - push_items) *
-			sizeof(struct btrfs_disk_key));
-		memmove(src->blockptrs, src->blockptrs + push_items,
-			(src_nritems - push_items) * sizeof(u64));
+			sizeof(struct btrfs_key_ptr));
 	}
 	btrfs_set_header_nritems(&src->header, src_nritems - push_items);
 	btrfs_set_header_nritems(&dst->header, dst_nritems + push_items);
@@ -601,7 +603,7 @@ static int balance_node_right(struct btrfs_root *root,
 
 	src_nritems = btrfs_header_nritems(&src->header);
 	dst_nritems = btrfs_header_nritems(&dst->header);
-	push_items = NODEPTRS_PER_BLOCK - dst_nritems;
+	push_items = BTRFS_NODEPTRS_PER_BLOCK(root) - dst_nritems;
 	if (push_items <= 0) {
 		return 1;
 	}
@@ -613,14 +615,10 @@ static int balance_node_right(struct btrfs_root *root,
 	if (max_push < push_items)
 		push_items = max_push;
 
-	memmove(dst->keys + push_items, dst->keys,
-		dst_nritems * sizeof(struct btrfs_disk_key));
-	memmove(dst->blockptrs + push_items, dst->blockptrs,
-		dst_nritems * sizeof(u64));
-	memcpy(dst->keys, src->keys + src_nritems - push_items,
-		push_items * sizeof(struct btrfs_disk_key));
-	memcpy(dst->blockptrs, src->blockptrs + src_nritems - push_items,
-		push_items * sizeof(u64));
+	memmove(dst->ptrs + push_items, dst->ptrs,
+		dst_nritems * sizeof(struct btrfs_key_ptr));
+	memcpy(dst->ptrs, src->ptrs + src_nritems - push_items,
+		push_items * sizeof(struct btrfs_key_ptr));
 
 	btrfs_set_header_nritems(&src->header, src_nritems - push_items);
 	btrfs_set_header_nritems(&dst->header, dst_nritems + push_items);
@@ -650,7 +648,7 @@ static int insert_new_root(struct btrfs_root *root,
 
 	t = btrfs_alloc_free_block(root);
 	c = &t->node;
-	memset(c, 0, sizeof(c));
+	memset(c, 0, root->blocksize);
 	btrfs_set_header_nritems(&c->header, 1);
 	btrfs_set_header_level(&c->header, level);
 	btrfs_set_header_blocknr(&c->header, t->blocknr);
@@ -660,8 +658,8 @@ static int insert_new_root(struct btrfs_root *root,
 	if (btrfs_is_leaf(lower))
 		lower_key = &((struct btrfs_leaf *)lower)->items[0].key;
 	else
-		lower_key = lower->keys;
-	memcpy(c->keys, lower_key, sizeof(struct btrfs_disk_key));
+		lower_key = &lower->ptrs[0].key;
+	memcpy(&c->ptrs[0].key, lower_key, sizeof(struct btrfs_disk_key));
 	btrfs_set_node_blockptr(c, 0, path->nodes[level - 1]->blocknr);
 	/* the super has an extra ref to root->node */
 	btrfs_block_release(root, root->node);
@@ -693,19 +691,15 @@ static int insert_ptr(struct btrfs_root *root,
 	nritems = btrfs_header_nritems(&lower->header);
 	if (slot > nritems)
 		BUG();
-	if (nritems == NODEPTRS_PER_BLOCK)
+	if (nritems == BTRFS_NODEPTRS_PER_BLOCK(root))
 		BUG();
 	if (slot != nritems) {
-		memmove(lower->keys + slot + 1, lower->keys + slot,
-			(nritems - slot) * sizeof(struct btrfs_disk_key));
-		memmove(lower->blockptrs + slot + 1, lower->blockptrs + slot,
-			(nritems - slot) * sizeof(u64));
+		memmove(lower->ptrs + slot + 1, lower->ptrs + slot,
+			(nritems - slot) * sizeof(struct btrfs_key_ptr));
 	}
-	memcpy(lower->keys + slot, key, sizeof(struct btrfs_disk_key));
+	memcpy(&lower->ptrs[slot].key, key, sizeof(struct btrfs_disk_key));
 	btrfs_set_node_blockptr(lower, slot, blocknr);
 	btrfs_set_header_nritems(&lower->header, nritems + 1);
-	if (lower->keys[1].objectid == 0)
-			BUG();
 	BUG_ON(list_empty(&path->nodes[level]->dirty));
 	return 0;
 }
@@ -747,17 +741,16 @@ static int split_node(struct btrfs_root *root, struct btrfs_path *path,
 	btrfs_set_header_parentid(&split->header,
 	                       btrfs_header_parentid(&root->node->node.header));
 	mid = (c_nritems + 1) / 2;
-	memcpy(split->keys, c->keys + mid,
-		(c_nritems - mid) * sizeof(struct btrfs_disk_key));
-	memcpy(split->blockptrs, c->blockptrs + mid,
-		(c_nritems - mid) * sizeof(u64));
+	memcpy(split->ptrs, c->ptrs + mid,
+		(c_nritems - mid) * sizeof(struct btrfs_key_ptr));
 	btrfs_set_header_nritems(&split->header, c_nritems - mid);
 	btrfs_set_header_nritems(&c->header, mid);
 	ret = 0;
 
 	BUG_ON(list_empty(&t->dirty));
-	wret = insert_ptr(root, path, split->keys, split_buffer->blocknr,
-			  path->slots[level + 1] + 1, level + 1);
+	wret = insert_ptr(root, path, &split->ptrs[0].key,
+			  split_buffer->blocknr, path->slots[level + 1] + 1,
+			  level + 1);
 	if (wret)
 		ret = wret;
 
@@ -825,7 +818,7 @@ static int push_leaf_right(struct btrfs_root *root, struct btrfs_path *path,
 	right_buf = read_tree_block(root, btrfs_node_blockptr(&upper->node,
 							      slot + 1));
 	right = &right_buf->leaf;
-	free_space = btrfs_leaf_free_space(right);
+	free_space = btrfs_leaf_free_space(root, right);
 	if (free_space < data_size + sizeof(struct btrfs_item)) {
 		btrfs_block_release(root, right_buf);
 		return 1;
@@ -833,7 +826,7 @@ static int push_leaf_right(struct btrfs_root *root, struct btrfs_path *path,
 	/* cow and double check */
 	btrfs_cow_block(root, right_buf, upper, slot + 1, &right_buf);
 	right = &right_buf->leaf;
-	free_space = btrfs_leaf_free_space(right);
+	free_space = btrfs_leaf_free_space(root, right);
 	if (free_space < data_size + sizeof(struct btrfs_item)) {
 		btrfs_block_release(root, right_buf);
 		return 1;
@@ -857,15 +850,14 @@ static int push_leaf_right(struct btrfs_root *root, struct btrfs_path *path,
 	right_nritems = btrfs_header_nritems(&right->header);
 	/* push left to right */
 	push_space = btrfs_item_end(left->items + left_nritems - push_items);
-	push_space -= leaf_data_end(left);
+	push_space -= leaf_data_end(root, left);
 	/* make room in the right data area */
-	memmove(right->data + leaf_data_end(right) - push_space,
-		right->data + leaf_data_end(right),
-		LEAF_DATA_SIZE - leaf_data_end(right));
+	memmove(btrfs_leaf_data(right) + leaf_data_end(root, right) -
+		push_space, btrfs_leaf_data(right) + leaf_data_end(root, right),
+		BTRFS_LEAF_DATA_SIZE(root) - leaf_data_end(root, right));
 	/* copy from the left data area */
-	memcpy(right->data + LEAF_DATA_SIZE - push_space,
-		left->data + leaf_data_end(left),
-		push_space);
+	memcpy(btrfs_leaf_data(right) + BTRFS_LEAF_DATA_SIZE(root) - push_space,
+		btrfs_leaf_data(left) + leaf_data_end(root, left), push_space);
 	memmove(right->items + push_items, right->items,
 		right_nritems * sizeof(struct btrfs_item));
 	/* copy the items from left to right */
@@ -875,7 +867,7 @@ static int push_leaf_right(struct btrfs_root *root, struct btrfs_path *path,
 	/* update the item pointers */
 	right_nritems += push_items;
 	btrfs_set_header_nritems(&right->header, right_nritems);
-	push_space = LEAF_DATA_SIZE;
+	push_space = BTRFS_LEAF_DATA_SIZE(root);
 	for (i = 0; i < right_nritems; i++) {
 		btrfs_set_item_offset(right->items + i, push_space -
 				      btrfs_item_size(right->items + i));
@@ -886,7 +878,7 @@ static int push_leaf_right(struct btrfs_root *root, struct btrfs_path *path,
 
 	BUG_ON(list_empty(&left_buf->dirty));
 	BUG_ON(list_empty(&right_buf->dirty));
-	memcpy(upper->node.keys + slot + 1,
+	memcpy(&upper->node.ptrs[slot + 1].key,
 		&right->items[0].key, sizeof(struct btrfs_disk_key));
 	BUG_ON(list_empty(&upper->dirty));
 
@@ -932,7 +924,7 @@ static int push_leaf_left(struct btrfs_root *root, struct btrfs_path *path,
 	t = read_tree_block(root, btrfs_node_blockptr(&path->nodes[1]->node,
 						      slot - 1));
 	left = &t->leaf;
-	free_space = btrfs_leaf_free_space(left);
+	free_space = btrfs_leaf_free_space(root, left);
 	if (free_space < data_size + sizeof(struct btrfs_item)) {
 		btrfs_block_release(root, t);
 		return 1;
@@ -941,7 +933,7 @@ static int push_leaf_left(struct btrfs_root *root, struct btrfs_path *path,
 	/* cow and double check */
 	btrfs_cow_block(root, t, path->nodes[1], slot - 1, &t);
 	left = &t->leaf;
-	free_space = btrfs_leaf_free_space(left);
+	free_space = btrfs_leaf_free_space(root, left);
 	if (free_space < data_size + sizeof(struct btrfs_item)) {
 		btrfs_block_release(root, t);
 		return 1;
@@ -964,17 +956,19 @@ static int push_leaf_left(struct btrfs_root *root, struct btrfs_path *path,
 	/* push data from right to left */
 	memcpy(left->items + btrfs_header_nritems(&left->header),
 		right->items, push_items * sizeof(struct btrfs_item));
-	push_space = LEAF_DATA_SIZE -
+	push_space = BTRFS_LEAF_DATA_SIZE(root) -
 		     btrfs_item_offset(right->items + push_items -1);
-	memcpy(left->data + leaf_data_end(left) - push_space,
-		right->data + btrfs_item_offset(right->items + push_items - 1),
+	memcpy(btrfs_leaf_data(left) + leaf_data_end(root, left) - push_space,
+		btrfs_leaf_data(right) +
+		btrfs_item_offset(right->items + push_items - 1),
 		push_space);
 	old_left_nritems = btrfs_header_nritems(&left->header);
 	BUG_ON(old_left_nritems < 0);
 
 	for (i = old_left_nritems; i < old_left_nritems + push_items; i++) {
-		u16 ioff = btrfs_item_offset(left->items + i);
-		btrfs_set_item_offset(left->items + i, ioff - (LEAF_DATA_SIZE -
+		u32 ioff = btrfs_item_offset(left->items + i);
+		btrfs_set_item_offset(left->items + i, ioff -
+				     (BTRFS_LEAF_DATA_SIZE(root) -
 				      btrfs_item_offset(left->items +
 						        old_left_nritems - 1)));
 	}
@@ -982,16 +976,17 @@ static int push_leaf_left(struct btrfs_root *root, struct btrfs_path *path,
 
 	/* fixup right node */
 	push_space = btrfs_item_offset(right->items + push_items - 1) -
-		     leaf_data_end(right);
-	memmove(right->data + LEAF_DATA_SIZE - push_space, right->data +
-		leaf_data_end(right), push_space);
+		     leaf_data_end(root, right);
+	memmove(btrfs_leaf_data(right) + BTRFS_LEAF_DATA_SIZE(root) -
+		push_space, btrfs_leaf_data(right) +
+		leaf_data_end(root, right), push_space);
 	memmove(right->items, right->items + push_items,
 		(btrfs_header_nritems(&right->header) - push_items) *
 		sizeof(struct btrfs_item));
 	btrfs_set_header_nritems(&right->header,
 				 btrfs_header_nritems(&right->header) -
 				 push_items);
-	push_space = LEAF_DATA_SIZE;
+	push_space = BTRFS_LEAF_DATA_SIZE(root);
 
 	for (i = 0; i < btrfs_header_nritems(&right->header); i++) {
 		btrfs_set_item_offset(right->items + i, push_space -
@@ -1051,12 +1046,12 @@ static int split_leaf(struct btrfs_root *root, struct btrfs_path *path,
 		if (wret < 0)
 			return wret;
 	}
-
 	l_buf = path->nodes[0];
 	l = &l_buf->leaf;
 
 	/* did the pushes work? */
-	if (btrfs_leaf_free_space(l) >= sizeof(struct btrfs_item) + data_size)
+	if (btrfs_leaf_free_space(root, l) >=
+	    sizeof(struct btrfs_item) + data_size)
 		return 0;
 
 	if (!path->nodes[1]) {
@@ -1071,16 +1066,16 @@ static int split_leaf(struct btrfs_root *root, struct btrfs_path *path,
 	BUG_ON(!right_buffer);
 	BUG_ON(mid == nritems);
 	right = &right_buffer->leaf;
-	memset(right, 0, sizeof(*right));
+	memset(&right->header, 0, sizeof(right->header));
 	if (mid <= slot) {
 		/* FIXME, just alloc a new leaf here */
 		if (leaf_space_used(l, mid, nritems - mid) + space_needed >
-			LEAF_DATA_SIZE)
+			BTRFS_LEAF_DATA_SIZE(root))
 			BUG();
 	} else {
 		/* FIXME, just alloc a new leaf here */
 		if (leaf_space_used(l, 0, mid + 1) + space_needed >
-			LEAF_DATA_SIZE)
+			BTRFS_LEAF_DATA_SIZE(root))
 			BUG();
 	}
 	btrfs_set_header_nritems(&right->header, nritems - mid);
@@ -1088,15 +1083,18 @@ static int split_leaf(struct btrfs_root *root, struct btrfs_path *path,
 	btrfs_set_header_level(&right->header, 0);
 	btrfs_set_header_parentid(&right->header,
 	                       btrfs_header_parentid(&root->node->node.header));
-	data_copy_size = btrfs_item_end(l->items + mid) - leaf_data_end(l);
+	data_copy_size = btrfs_item_end(l->items + mid) -
+			 leaf_data_end(root, l);
 	memcpy(right->items, l->items + mid,
 	       (nritems - mid) * sizeof(struct btrfs_item));
-	memcpy(right->data + LEAF_DATA_SIZE - data_copy_size,
-	       l->data + leaf_data_end(l), data_copy_size);
-	rt_data_off = LEAF_DATA_SIZE - btrfs_item_end(l->items + mid);
+	memcpy(btrfs_leaf_data(right) + BTRFS_LEAF_DATA_SIZE(root) -
+		data_copy_size, btrfs_leaf_data(l) +
+		leaf_data_end(root, l), data_copy_size);
+	rt_data_off = BTRFS_LEAF_DATA_SIZE(root) -
+		      btrfs_item_end(l->items + mid);
 
 	for (i = 0; i < btrfs_header_nritems(&right->header); i++) {
-		u16 ioff = btrfs_item_offset(right->items + i);
+		u32 ioff = btrfs_item_offset(right->items + i);
 		btrfs_set_item_offset(right->items + i, ioff + rt_data_off);
 	}
 
@@ -1156,9 +1154,9 @@ int btrfs_insert_item(struct btrfs_root *root, struct btrfs_key *cpu_key,
 	leaf = &leaf_buf->leaf;
 
 	nritems = btrfs_header_nritems(&leaf->header);
-	data_end = leaf_data_end(leaf);
+	data_end = leaf_data_end(root, leaf);
 
-	if (btrfs_leaf_free_space(leaf) <
+	if (btrfs_leaf_free_space(root, leaf) <
 	    sizeof(struct btrfs_item) + data_size)
 		BUG();
 
@@ -1173,7 +1171,7 @@ int btrfs_insert_item(struct btrfs_root *root, struct btrfs_key *cpu_key,
 		 */
 		/* first correct the data pointers */
 		for (i = slot; i < nritems; i++) {
-			u16 ioff = btrfs_item_offset(leaf->items + i);
+			u32 ioff = btrfs_item_offset(leaf->items + i);
 			btrfs_set_item_offset(leaf->items + i,
 					      ioff - data_size);
 		}
@@ -1183,7 +1181,8 @@ int btrfs_insert_item(struct btrfs_root *root, struct btrfs_key *cpu_key,
 		        (nritems - slot) * sizeof(struct btrfs_item));
 
 		/* shift the data */
-		memmove(leaf->data + data_end - data_size, leaf->data +
+		memmove(btrfs_leaf_data(leaf) + data_end - data_size,
+			btrfs_leaf_data(leaf) +
 		        data_end, old_data - data_end);
 		data_end = old_data;
 	}
@@ -1192,7 +1191,7 @@ int btrfs_insert_item(struct btrfs_root *root, struct btrfs_key *cpu_key,
 		sizeof(struct btrfs_disk_key));
 	btrfs_set_item_offset(leaf->items + slot, data_end - data_size);
 	btrfs_set_item_size(leaf->items + slot, data_size);
-	memcpy(leaf->data + data_end - data_size, data, data_size);
+	memcpy(btrfs_leaf_data(leaf) + data_end - data_size, data, data_size);
 	btrfs_set_header_nritems(&leaf->header, nritems + 1);
 
 	ret = 0;
@@ -1200,9 +1199,9 @@ int btrfs_insert_item(struct btrfs_root *root, struct btrfs_key *cpu_key,
 		ret = fixup_low_keys(root, &path, &disk_key, 1);
 
 	BUG_ON(list_empty(&leaf_buf->dirty));
-	if (btrfs_leaf_free_space(leaf) < 0)
+	if (btrfs_leaf_free_space(root, leaf) < 0)
 		BUG();
-	check_leaf(&path, 0);
+	check_leaf(root, &path, 0);
 out:
 	btrfs_release_path(root, &path);
 	return ret;
@@ -1227,11 +1226,8 @@ static int del_ptr(struct btrfs_root *root, struct btrfs_path *path, int level,
 	node = &parent->node;
 	nritems = btrfs_header_nritems(&node->header);
 	if (slot != nritems -1) {
-		memmove(node->keys + slot, node->keys + slot + 1,
-			sizeof(struct btrfs_disk_key) * (nritems - slot - 1));
-		memmove(node->blockptrs + slot,
-			node->blockptrs + slot + 1,
-			sizeof(u64) * (nritems - slot - 1));
+		memmove(node->ptrs + slot, node->ptrs + slot + 1,
+			sizeof(struct btrfs_key_ptr) * (nritems - slot - 1));
 	}
 	nritems--;
 	btrfs_set_header_nritems(&node->header, nritems);
@@ -1240,7 +1236,8 @@ static int del_ptr(struct btrfs_root *root, struct btrfs_path *path, int level,
 		/* just turn the root into a leaf and break */
 		btrfs_set_header_level(&root->node->node.header, 0);
 	} else if (slot == 0) {
-		wret = fixup_low_keys(root, path, node->keys, level + 1);
+		wret = fixup_low_keys(root, path, &node->ptrs[0].key,
+				      level + 1);
 		if (wret)
 			ret = wret;
 	}
@@ -1272,12 +1269,12 @@ int btrfs_del_item(struct btrfs_root *root, struct btrfs_path *path)
 
 	if (slot != nritems - 1) {
 		int i;
-		int data_end = leaf_data_end(leaf);
-		memmove(leaf->data + data_end + dsize,
-			leaf->data + data_end,
+		int data_end = leaf_data_end(root, leaf);
+		memmove(btrfs_leaf_data(leaf) + data_end + dsize,
+			btrfs_leaf_data(leaf) + data_end,
 			doff - data_end);
 		for (i = slot + 1; i < nritems; i++) {
-			u16 ioff = btrfs_item_offset(leaf->items + i);
+			u32 ioff = btrfs_item_offset(leaf->items + i);
 			btrfs_set_item_offset(leaf->items + i, ioff + dsize);
 		}
 		memmove(leaf->items + slot, leaf->items + slot + 1,
@@ -1311,7 +1308,7 @@ int btrfs_del_item(struct btrfs_root *root, struct btrfs_path *path)
 		BUG_ON(list_empty(&leaf_buf->dirty));
 
 		/* delete the leaf if it is mostly empty */
-		if (used < LEAF_DATA_SIZE / 3) {
+		if (used < BTRFS_LEAF_DATA_SIZE(root) / 3) {
 			/* push_leaf_left fixes the path.
 			 * make sure the path still points to our leaf
 			 * for possible call to del_ptr below

commit eaee50e881b88766d9a781c54c4142456cf5e527
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Mar 13 11:17:52 2007 -0400

    Btrfs: merge leaves before split
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index bbb632291709..dc620186e501 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1043,6 +1043,15 @@ static int split_leaf(struct btrfs_root *root, struct btrfs_path *path,
 	int ret;
 	int wret;
 
+	wret = push_leaf_left(root, path, data_size);
+	if (wret < 0)
+		return wret;
+	if (wret) {
+		wret = push_leaf_right(root, path, data_size);
+		if (wret < 0)
+			return wret;
+	}
+
 	l_buf = path->nodes[0];
 	l = &l_buf->leaf;
 

commit 9aca1d51323c0291a7358e0728b64ec4f17d1a77
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Mar 13 11:09:37 2007 -0400

    Btrfs: make some funcs static
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index c8529b394d5f..bbb632291709 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -34,7 +34,7 @@ void btrfs_release_path(struct btrfs_root *root, struct btrfs_path *p)
 	memset(p, 0, sizeof(*p));
 }
 
-int btrfs_cow_block(struct btrfs_root *root,
+static int btrfs_cow_block(struct btrfs_root *root,
 		    struct btrfs_buffer *buf,
 		    struct btrfs_buffer *parent,
 		    int parent_slot,
@@ -96,7 +96,7 @@ int btrfs_leaf_free_space(struct btrfs_leaf *leaf)
 /*
  * compare two keys in a memcmp fashion
  */
-int comp_keys(struct btrfs_disk_key *disk, struct btrfs_key *k2)
+static int comp_keys(struct btrfs_disk_key *disk, struct btrfs_key *k2)
 {
 	struct btrfs_key k1;
 
@@ -117,7 +117,7 @@ int comp_keys(struct btrfs_disk_key *disk, struct btrfs_key *k2)
 	return 0;
 }
 
-int check_node(struct btrfs_path *path, int level)
+static int check_node(struct btrfs_path *path, int level)
 {
 	int i;
 	struct btrfs_node *parent = NULL;
@@ -146,7 +146,7 @@ int check_node(struct btrfs_path *path, int level)
 	return 0;
 }
 
-int check_leaf(struct btrfs_path *path, int level)
+static int check_leaf(struct btrfs_path *path, int level)
 {
 	int i;
 	struct btrfs_leaf *leaf = &path->nodes[level]->leaf;
@@ -186,7 +186,7 @@ int check_leaf(struct btrfs_path *path, int level)
 	return 0;
 }
 
-int check_block(struct btrfs_path *path, int level)
+static int check_block(struct btrfs_path *path, int level)
 {
 	if (level == 0)
 		return check_leaf(path, level);
@@ -202,7 +202,7 @@ int check_block(struct btrfs_path *path, int level)
  *
  * slot may point to max if the key is bigger than all of the keys
  */
-int generic_bin_search(char *p, int item_size, struct btrfs_key *key,
+static int generic_bin_search(char *p, int item_size, struct btrfs_key *key,
 		       int max, int *slot)
 {
 	int low = 0;
@@ -233,7 +233,7 @@ int generic_bin_search(char *p, int item_size, struct btrfs_key *key,
  * simple bin_search frontend that does the right thing for
  * leaves vs nodes
  */
-int bin_search(struct btrfs_node *c, struct btrfs_key *key, int *slot)
+static int bin_search(struct btrfs_node *c, struct btrfs_key *key, int *slot)
 {
 	if (btrfs_is_leaf(c)) {
 		struct btrfs_leaf *l = (struct btrfs_leaf *)c;
@@ -250,7 +250,7 @@ int bin_search(struct btrfs_node *c, struct btrfs_key *key, int *slot)
 	return -1;
 }
 
-struct btrfs_buffer *read_node_slot(struct btrfs_root *root,
+static struct btrfs_buffer *read_node_slot(struct btrfs_root *root,
 				   struct btrfs_buffer *parent_buf,
 				   int slot)
 {

commit 234b63a091e1df6bd4261dd7b3a7490074830628
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Mar 13 10:46:10 2007 -0400

    rename funcs and structs to btrfs
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 762810731001..c8529b394d5f 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -6,47 +6,47 @@
 #include "disk-io.h"
 #include "print-tree.h"
 
-static int split_node(struct ctree_root *root, struct ctree_path *path,
+static int split_node(struct btrfs_root *root, struct btrfs_path *path,
 		      int level);
-static int split_leaf(struct ctree_root *root, struct ctree_path *path,
+static int split_leaf(struct btrfs_root *root, struct btrfs_path *path,
 		      int data_size);
-static int push_node_left(struct ctree_root *root, struct tree_buffer *dst,
-			  struct tree_buffer *src);
-static int balance_node_right(struct ctree_root *root,
-			      struct tree_buffer *dst_buf,
-			      struct tree_buffer *src_buf);
-static int del_ptr(struct ctree_root *root, struct ctree_path *path, int level,
+static int push_node_left(struct btrfs_root *root, struct btrfs_buffer *dst,
+			  struct btrfs_buffer *src);
+static int balance_node_right(struct btrfs_root *root,
+			      struct btrfs_buffer *dst_buf,
+			      struct btrfs_buffer *src_buf);
+static int del_ptr(struct btrfs_root *root, struct btrfs_path *path, int level,
 		   int slot);
 
-inline void init_path(struct ctree_path *p)
+inline void btrfs_init_path(struct btrfs_path *p)
 {
 	memset(p, 0, sizeof(*p));
 }
 
-void release_path(struct ctree_root *root, struct ctree_path *p)
+void btrfs_release_path(struct btrfs_root *root, struct btrfs_path *p)
 {
 	int i;
-	for (i = 0; i < MAX_LEVEL; i++) {
+	for (i = 0; i < BTRFS_MAX_LEVEL; i++) {
 		if (!p->nodes[i])
 			break;
-		tree_block_release(root, p->nodes[i]);
+		btrfs_block_release(root, p->nodes[i]);
 	}
 	memset(p, 0, sizeof(*p));
 }
 
-int btrfs_cow_block(struct ctree_root *root,
-		    struct tree_buffer *buf,
-		    struct tree_buffer *parent,
+int btrfs_cow_block(struct btrfs_root *root,
+		    struct btrfs_buffer *buf,
+		    struct btrfs_buffer *parent,
 		    int parent_slot,
-		    struct tree_buffer **cow_ret)
+		    struct btrfs_buffer **cow_ret)
 {
-	struct tree_buffer *cow;
+	struct btrfs_buffer *cow;
 
 	if (!list_empty(&buf->dirty)) {
 		*cow_ret = buf;
 		return 0;
 	}
-	cow = alloc_free_block(root);
+	cow = btrfs_alloc_free_block(root);
 	memcpy(&cow->node, &buf->node, sizeof(buf->node));
 	btrfs_set_header_blocknr(&cow->node.header, cow->blocknr);
 	*cow_ret = cow;
@@ -55,15 +55,15 @@ int btrfs_cow_block(struct ctree_root *root,
 		root->node = cow;
 		cow->count++;
 		if (buf != root->commit_root)
-			free_extent(root, buf->blocknr, 1);
-		tree_block_release(root, buf);
+			btrfs_free_extent(root, buf->blocknr, 1);
+		btrfs_block_release(root, buf);
 	} else {
 		btrfs_set_node_blockptr(&parent->node, parent_slot,
 					cow->blocknr);
 		BUG_ON(list_empty(&parent->dirty));
-		free_extent(root, buf->blocknr, 1);
+		btrfs_free_extent(root, buf->blocknr, 1);
 	}
-	tree_block_release(root, buf);
+	btrfs_block_release(root, buf);
 	return 0;
 }
 
@@ -72,7 +72,7 @@ int btrfs_cow_block(struct ctree_root *root,
  * this returns the address of the start of the last item,
  * which is the stop of the leaf data stack
  */
-static inline unsigned int leaf_data_end(struct leaf *leaf)
+static inline unsigned int leaf_data_end(struct btrfs_leaf *leaf)
 {
 	u32 nr = btrfs_header_nritems(&leaf->header);
 	if (nr == 0)
@@ -85,7 +85,7 @@ static inline unsigned int leaf_data_end(struct leaf *leaf)
  * the start of the leaf data.  IOW, how much room
  * the leaf has left for both items and data
  */
-int leaf_free_space(struct leaf *leaf)
+int btrfs_leaf_free_space(struct btrfs_leaf *leaf)
 {
 	int data_end = leaf_data_end(leaf);
 	int nritems = btrfs_header_nritems(&leaf->header);
@@ -117,11 +117,11 @@ int comp_keys(struct btrfs_disk_key *disk, struct btrfs_key *k2)
 	return 0;
 }
 
-int check_node(struct ctree_path *path, int level)
+int check_node(struct btrfs_path *path, int level)
 {
 	int i;
-	struct node *parent = NULL;
-	struct node *node = &path->nodes[level]->node;
+	struct btrfs_node *parent = NULL;
+	struct btrfs_node *node = &path->nodes[level]->node;
 	int parent_slot;
 	u32 nritems = btrfs_header_nritems(&node->header);
 
@@ -146,18 +146,18 @@ int check_node(struct ctree_path *path, int level)
 	return 0;
 }
 
-int check_leaf(struct ctree_path *path, int level)
+int check_leaf(struct btrfs_path *path, int level)
 {
 	int i;
-	struct leaf *leaf = &path->nodes[level]->leaf;
-	struct node *parent = NULL;
+	struct btrfs_leaf *leaf = &path->nodes[level]->leaf;
+	struct btrfs_node *parent = NULL;
 	int parent_slot;
 	u32 nritems = btrfs_header_nritems(&leaf->header);
 
 	if (path->nodes[level + 1])
 		parent = &path->nodes[level + 1]->node;
 	parent_slot = path->slots[level + 1];
-	BUG_ON(leaf_free_space(leaf) < 0);
+	BUG_ON(btrfs_leaf_free_space(leaf) < 0);
 
 	if (nritems == 0)
 		return 0;
@@ -186,7 +186,7 @@ int check_leaf(struct ctree_path *path, int level)
 	return 0;
 }
 
-int check_block(struct ctree_path *path, int level)
+int check_block(struct btrfs_path *path, int level)
 {
 	if (level == 0)
 		return check_leaf(path, level);
@@ -233,10 +233,10 @@ int generic_bin_search(char *p, int item_size, struct btrfs_key *key,
  * simple bin_search frontend that does the right thing for
  * leaves vs nodes
  */
-int bin_search(struct node *c, struct btrfs_key *key, int *slot)
+int bin_search(struct btrfs_node *c, struct btrfs_key *key, int *slot)
 {
 	if (btrfs_is_leaf(c)) {
-		struct leaf *l = (struct leaf *)c;
+		struct btrfs_leaf *l = (struct btrfs_leaf *)c;
 		return generic_bin_search((void *)l->items,
 					  sizeof(struct btrfs_item),
 					  key, btrfs_header_nritems(&c->header),
@@ -250,11 +250,11 @@ int bin_search(struct node *c, struct btrfs_key *key, int *slot)
 	return -1;
 }
 
-struct tree_buffer *read_node_slot(struct ctree_root *root,
-				   struct tree_buffer *parent_buf,
+struct btrfs_buffer *read_node_slot(struct btrfs_root *root,
+				   struct btrfs_buffer *parent_buf,
 				   int slot)
 {
-	struct node *node = &parent_buf->node;
+	struct btrfs_node *node = &parent_buf->node;
 	if (slot < 0)
 		return NULL;
 	if (slot >= btrfs_header_nritems(&node->header))
@@ -262,17 +262,17 @@ struct tree_buffer *read_node_slot(struct ctree_root *root,
 	return read_tree_block(root, btrfs_node_blockptr(node, slot));
 }
 
-static int balance_level(struct ctree_root *root, struct ctree_path *path,
+static int balance_level(struct btrfs_root *root, struct btrfs_path *path,
 			int level)
 {
-	struct tree_buffer *right_buf;
-	struct tree_buffer *mid_buf;
-	struct tree_buffer *left_buf;
-	struct tree_buffer *parent_buf = NULL;
-	struct node *right = NULL;
-	struct node *mid;
-	struct node *left = NULL;
-	struct node *parent = NULL;
+	struct btrfs_buffer *right_buf;
+	struct btrfs_buffer *mid_buf;
+	struct btrfs_buffer *left_buf;
+	struct btrfs_buffer *parent_buf = NULL;
+	struct btrfs_node *right = NULL;
+	struct btrfs_node *mid;
+	struct btrfs_node *left = NULL;
+	struct btrfs_node *parent = NULL;
 	int ret = 0;
 	int wret;
 	int pslot;
@@ -286,12 +286,12 @@ static int balance_level(struct ctree_root *root, struct ctree_path *path,
 	mid = &mid_buf->node;
 	orig_ptr = btrfs_node_blockptr(mid, orig_slot);
 
-	if (level < MAX_LEVEL - 1)
+	if (level < BTRFS_MAX_LEVEL - 1)
 		parent_buf = path->nodes[level + 1];
 	pslot = path->slots[level + 1];
 
 	if (!parent_buf) {
-		struct tree_buffer *child;
+		struct btrfs_buffer *child;
 		u64 blocknr = mid_buf->blocknr;
 
 		if (btrfs_header_nritems(&mid->header) != 1)
@@ -303,11 +303,11 @@ static int balance_level(struct ctree_root *root, struct ctree_path *path,
 		root->node = child;
 		path->nodes[level] = NULL;
 		/* once for the path */
-		tree_block_release(root, mid_buf);
+		btrfs_block_release(root, mid_buf);
 		/* once for the root ptr */
-		tree_block_release(root, mid_buf);
+		btrfs_block_release(root, mid_buf);
 		clean_tree_block(root, mid_buf);
-		return free_extent(root, blocknr, 1);
+		return btrfs_free_extent(root, blocknr, 1);
 	}
 	parent = &parent_buf->node;
 
@@ -340,14 +340,14 @@ static int balance_level(struct ctree_root *root, struct ctree_path *path,
 			ret = wret;
 		if (btrfs_header_nritems(&right->header) == 0) {
 			u64 blocknr = right_buf->blocknr;
-			tree_block_release(root, right_buf);
+			btrfs_block_release(root, right_buf);
 			clean_tree_block(root, right_buf);
 			right_buf = NULL;
 			right = NULL;
 			wret = del_ptr(root, path, level + 1, pslot + 1);
 			if (wret)
 				ret = wret;
-			wret = free_extent(root, blocknr, 1);
+			wret = btrfs_free_extent(root, blocknr, 1);
 			if (wret)
 				ret = wret;
 		} else {
@@ -375,14 +375,14 @@ static int balance_level(struct ctree_root *root, struct ctree_path *path,
 	if (btrfs_header_nritems(&mid->header) == 0) {
 		/* we've managed to empty the middle node, drop it */
 		u64 blocknr = mid_buf->blocknr;
-		tree_block_release(root, mid_buf);
+		btrfs_block_release(root, mid_buf);
 		clean_tree_block(root, mid_buf);
 		mid_buf = NULL;
 		mid = NULL;
 		wret = del_ptr(root, path, level + 1, pslot);
 		if (wret)
 			ret = wret;
-		wret = free_extent(root, blocknr, 1);
+		wret = btrfs_free_extent(root, blocknr, 1);
 		if (wret)
 			ret = wret;
 	} else {
@@ -400,7 +400,7 @@ static int balance_level(struct ctree_root *root, struct ctree_path *path,
 			path->slots[level + 1] -= 1;
 			path->slots[level] = orig_slot;
 			if (mid_buf)
-				tree_block_release(root, mid_buf);
+				btrfs_block_release(root, mid_buf);
 		} else {
 			orig_slot -= btrfs_header_nritems(&left->header);
 			path->slots[level] = orig_slot;
@@ -413,9 +413,9 @@ static int balance_level(struct ctree_root *root, struct ctree_path *path,
 		BUG();
 
 	if (right_buf)
-		tree_block_release(root, right_buf);
+		btrfs_block_release(root, right_buf);
 	if (left_buf)
-		tree_block_release(root, left_buf);
+		btrfs_block_release(root, left_buf);
 	return ret;
 }
 
@@ -432,12 +432,12 @@ static int balance_level(struct ctree_root *root, struct ctree_path *path,
  * tree.  if ins_len < 0, nodes will be merged as we walk down the tree (if
  * possible)
  */
-int search_slot(struct ctree_root *root, struct btrfs_key *key,
-		struct ctree_path *p, int ins_len, int cow)
+int btrfs_search_slot(struct btrfs_root *root, struct btrfs_key *key,
+		struct btrfs_path *p, int ins_len, int cow)
 {
-	struct tree_buffer *b;
-	struct tree_buffer *cow_buf;
-	struct node *c;
+	struct btrfs_buffer *b;
+	struct btrfs_buffer *cow_buf;
+	struct btrfs_node *c;
 	int slot;
 	int ret;
 	int level;
@@ -486,9 +486,9 @@ int search_slot(struct ctree_root *root, struct btrfs_key *key,
 			}
 			b = read_tree_block(root, btrfs_node_blockptr(c, slot));
 		} else {
-			struct leaf *l = (struct leaf *)c;
+			struct btrfs_leaf *l = (struct btrfs_leaf *)c;
 			p->slots[level] = slot;
-			if (ins_len > 0 && leaf_free_space(l) <
+			if (ins_len > 0 && btrfs_leaf_free_space(l) <
 			    sizeof(struct btrfs_item) + ins_len) {
 				int sret = split_leaf(root, p, ins_len);
 				BUG_ON(sret > 0);
@@ -513,14 +513,14 @@ int search_slot(struct ctree_root *root, struct btrfs_key *key,
  * If this fails to write a tree block, it returns -1, but continues
  * fixing up the blocks in ram so the tree is consistent.
  */
-static int fixup_low_keys(struct ctree_root *root,
-			   struct ctree_path *path, struct btrfs_disk_key *key,
+static int fixup_low_keys(struct btrfs_root *root,
+			   struct btrfs_path *path, struct btrfs_disk_key *key,
 			   int level)
 {
 	int i;
 	int ret = 0;
-	for (i = level; i < MAX_LEVEL; i++) {
-		struct node *t;
+	for (i = level; i < BTRFS_MAX_LEVEL; i++) {
+		struct btrfs_node *t;
 		int tslot = path->slots[i];
 		if (!path->nodes[i])
 			break;
@@ -540,11 +540,11 @@ static int fixup_low_keys(struct ctree_root *root,
  * returns 0 if some ptrs were pushed left, < 0 if there was some horrible
  * error, and > 0 if there was no room in the left hand block.
  */
-static int push_node_left(struct ctree_root *root, struct tree_buffer *dst_buf,
-			  struct tree_buffer *src_buf)
+static int push_node_left(struct btrfs_root *root, struct btrfs_buffer *dst_buf,
+			  struct btrfs_buffer *src_buf)
 {
-	struct node *src = &src_buf->node;
-	struct node *dst = &dst_buf->node;
+	struct btrfs_node *src = &src_buf->node;
+	struct btrfs_node *dst = &dst_buf->node;
 	int push_items = 0;
 	int src_nritems;
 	int dst_nritems;
@@ -587,12 +587,12 @@ static int push_node_left(struct ctree_root *root, struct tree_buffer *dst_buf,
  *
  * this will  only push up to 1/2 the contents of the left node over
  */
-static int balance_node_right(struct ctree_root *root,
-			      struct tree_buffer *dst_buf,
-			      struct tree_buffer *src_buf)
+static int balance_node_right(struct btrfs_root *root,
+			      struct btrfs_buffer *dst_buf,
+			      struct btrfs_buffer *src_buf)
 {
-	struct node *src = &src_buf->node;
-	struct node *dst = &dst_buf->node;
+	struct btrfs_node *src = &src_buf->node;
+	struct btrfs_node *dst = &dst_buf->node;
 	int push_items = 0;
 	int max_push;
 	int src_nritems;
@@ -637,18 +637,18 @@ static int balance_node_right(struct ctree_root *root,
  *
  * returns zero on success or < 0 on failure.
  */
-static int insert_new_root(struct ctree_root *root,
-			   struct ctree_path *path, int level)
+static int insert_new_root(struct btrfs_root *root,
+			   struct btrfs_path *path, int level)
 {
-	struct tree_buffer *t;
-	struct node *lower;
-	struct node *c;
+	struct btrfs_buffer *t;
+	struct btrfs_node *lower;
+	struct btrfs_node *c;
 	struct btrfs_disk_key *lower_key;
 
 	BUG_ON(path->nodes[level]);
 	BUG_ON(path->nodes[level-1] != root->node);
 
-	t = alloc_free_block(root);
+	t = btrfs_alloc_free_block(root);
 	c = &t->node;
 	memset(c, 0, sizeof(c));
 	btrfs_set_header_nritems(&c->header, 1);
@@ -658,13 +658,13 @@ static int insert_new_root(struct ctree_root *root,
 	                       btrfs_header_parentid(&root->node->node.header));
 	lower = &path->nodes[level-1]->node;
 	if (btrfs_is_leaf(lower))
-		lower_key = &((struct leaf *)lower)->items[0].key;
+		lower_key = &((struct btrfs_leaf *)lower)->items[0].key;
 	else
 		lower_key = lower->keys;
 	memcpy(c->keys, lower_key, sizeof(struct btrfs_disk_key));
 	btrfs_set_node_blockptr(c, 0, path->nodes[level - 1]->blocknr);
 	/* the super has an extra ref to root->node */
-	tree_block_release(root, root->node);
+	btrfs_block_release(root, root->node);
 	root->node = t;
 	t->count++;
 	path->nodes[level] = t;
@@ -681,11 +681,11 @@ static int insert_new_root(struct ctree_root *root,
  *
  * returns zero on success and < 0 on any error
  */
-static int insert_ptr(struct ctree_root *root,
-		struct ctree_path *path, struct btrfs_disk_key *key,
+static int insert_ptr(struct btrfs_root *root,
+		struct btrfs_path *path, struct btrfs_disk_key *key,
 		u64 blocknr, int slot, int level)
 {
-	struct node *lower;
+	struct btrfs_node *lower;
 	int nritems;
 
 	BUG_ON(!path->nodes[level]);
@@ -719,13 +719,13 @@ static int insert_ptr(struct ctree_root *root,
  *
  * returns 0 on success and < 0 on failure
  */
-static int split_node(struct ctree_root *root, struct ctree_path *path,
+static int split_node(struct btrfs_root *root, struct btrfs_path *path,
 		      int level)
 {
-	struct tree_buffer *t;
-	struct node *c;
-	struct tree_buffer *split_buffer;
-	struct node *split;
+	struct btrfs_buffer *t;
+	struct btrfs_node *c;
+	struct btrfs_buffer *split_buffer;
+	struct btrfs_node *split;
 	int mid;
 	int ret;
 	int wret;
@@ -740,7 +740,7 @@ static int split_node(struct ctree_root *root, struct ctree_path *path,
 			return ret;
 	}
 	c_nritems = btrfs_header_nritems(&c->header);
-	split_buffer = alloc_free_block(root);
+	split_buffer = btrfs_alloc_free_block(root);
 	split = &split_buffer->node;
 	btrfs_set_header_flags(&split->header, btrfs_header_flags(&c->header));
 	btrfs_set_header_blocknr(&split->header, split_buffer->blocknr);
@@ -763,11 +763,11 @@ static int split_node(struct ctree_root *root, struct ctree_path *path,
 
 	if (path->slots[level] >= mid) {
 		path->slots[level] -= mid;
-		tree_block_release(root, t);
+		btrfs_block_release(root, t);
 		path->nodes[level] = split_buffer;
 		path->slots[level + 1] += 1;
 	} else {
-		tree_block_release(root, split_buffer);
+		btrfs_block_release(root, split_buffer);
 	}
 	return ret;
 }
@@ -777,7 +777,7 @@ static int split_node(struct ctree_root *root, struct ctree_path *path,
  * and nr indicate which items in the leaf to check.  This totals up the
  * space used both by the item structs and the item data
  */
-static int leaf_space_used(struct leaf *l, int start, int nr)
+static int leaf_space_used(struct btrfs_leaf *l, int start, int nr)
 {
 	int data_len;
 	int end = start + nr - 1;
@@ -797,14 +797,14 @@ static int leaf_space_used(struct leaf *l, int start, int nr)
  * returns 1 if the push failed because the other node didn't have enough
  * room, 0 if everything worked out and < 0 if there were major errors.
  */
-static int push_leaf_right(struct ctree_root *root, struct ctree_path *path,
+static int push_leaf_right(struct btrfs_root *root, struct btrfs_path *path,
 			   int data_size)
 {
-	struct tree_buffer *left_buf = path->nodes[0];
-	struct leaf *left = &left_buf->leaf;
-	struct leaf *right;
-	struct tree_buffer *right_buf;
-	struct tree_buffer *upper;
+	struct btrfs_buffer *left_buf = path->nodes[0];
+	struct btrfs_leaf *left = &left_buf->leaf;
+	struct btrfs_leaf *right;
+	struct btrfs_buffer *right_buf;
+	struct btrfs_buffer *upper;
 	int slot;
 	int i;
 	int free_space;
@@ -825,17 +825,17 @@ static int push_leaf_right(struct ctree_root *root, struct ctree_path *path,
 	right_buf = read_tree_block(root, btrfs_node_blockptr(&upper->node,
 							      slot + 1));
 	right = &right_buf->leaf;
-	free_space = leaf_free_space(right);
+	free_space = btrfs_leaf_free_space(right);
 	if (free_space < data_size + sizeof(struct btrfs_item)) {
-		tree_block_release(root, right_buf);
+		btrfs_block_release(root, right_buf);
 		return 1;
 	}
 	/* cow and double check */
 	btrfs_cow_block(root, right_buf, upper, slot + 1, &right_buf);
 	right = &right_buf->leaf;
-	free_space = leaf_free_space(right);
+	free_space = btrfs_leaf_free_space(right);
 	if (free_space < data_size + sizeof(struct btrfs_item)) {
-		tree_block_release(root, right_buf);
+		btrfs_block_release(root, right_buf);
 		return 1;
 	}
 
@@ -851,7 +851,7 @@ static int push_leaf_right(struct ctree_root *root, struct ctree_path *path,
 		push_space += btrfs_item_size(item) + sizeof(*item);
 	}
 	if (push_items == 0) {
-		tree_block_release(root, right_buf);
+		btrfs_block_release(root, right_buf);
 		return 1;
 	}
 	right_nritems = btrfs_header_nritems(&right->header);
@@ -893,11 +893,11 @@ static int push_leaf_right(struct ctree_root *root, struct ctree_path *path,
 	/* then fixup the leaf pointer in the path */
 	if (path->slots[0] >= left_nritems) {
 		path->slots[0] -= left_nritems;
-		tree_block_release(root, path->nodes[0]);
+		btrfs_block_release(root, path->nodes[0]);
 		path->nodes[0] = right_buf;
 		path->slots[1] += 1;
 	} else {
-		tree_block_release(root, right_buf);
+		btrfs_block_release(root, right_buf);
 	}
 	return 0;
 }
@@ -905,13 +905,13 @@ static int push_leaf_right(struct ctree_root *root, struct ctree_path *path,
  * push some data in the path leaf to the left, trying to free up at
  * least data_size bytes.  returns zero if the push worked, nonzero otherwise
  */
-static int push_leaf_left(struct ctree_root *root, struct ctree_path *path,
+static int push_leaf_left(struct btrfs_root *root, struct btrfs_path *path,
 			  int data_size)
 {
-	struct tree_buffer *right_buf = path->nodes[0];
-	struct leaf *right = &right_buf->leaf;
-	struct tree_buffer *t;
-	struct leaf *left;
+	struct btrfs_buffer *right_buf = path->nodes[0];
+	struct btrfs_leaf *right = &right_buf->leaf;
+	struct btrfs_buffer *t;
+	struct btrfs_leaf *left;
 	int slot;
 	int i;
 	int free_space;
@@ -932,18 +932,18 @@ static int push_leaf_left(struct ctree_root *root, struct ctree_path *path,
 	t = read_tree_block(root, btrfs_node_blockptr(&path->nodes[1]->node,
 						      slot - 1));
 	left = &t->leaf;
-	free_space = leaf_free_space(left);
+	free_space = btrfs_leaf_free_space(left);
 	if (free_space < data_size + sizeof(struct btrfs_item)) {
-		tree_block_release(root, t);
+		btrfs_block_release(root, t);
 		return 1;
 	}
 
 	/* cow and double check */
 	btrfs_cow_block(root, t, path->nodes[1], slot - 1, &t);
 	left = &t->leaf;
-	free_space = leaf_free_space(left);
+	free_space = btrfs_leaf_free_space(left);
 	if (free_space < data_size + sizeof(struct btrfs_item)) {
-		tree_block_release(root, t);
+		btrfs_block_release(root, t);
 		return 1;
 	}
 
@@ -958,7 +958,7 @@ static int push_leaf_left(struct ctree_root *root, struct ctree_path *path,
 		push_space += btrfs_item_size(item) + sizeof(*item);
 	}
 	if (push_items == 0) {
-		tree_block_release(root, t);
+		btrfs_block_release(root, t);
 		return 1;
 	}
 	/* push data from right to left */
@@ -1009,11 +1009,11 @@ static int push_leaf_left(struct ctree_root *root, struct ctree_path *path,
 	/* then fixup the leaf pointer in the path */
 	if (path->slots[0] < push_items) {
 		path->slots[0] += old_left_nritems;
-		tree_block_release(root, path->nodes[0]);
+		btrfs_block_release(root, path->nodes[0]);
 		path->nodes[0] = t;
 		path->slots[1] -= 1;
 	} else {
-		tree_block_release(root, t);
+		btrfs_block_release(root, t);
 		path->slots[0] -= push_items;
 	}
 	BUG_ON(path->slots[0] < 0);
@@ -1026,16 +1026,16 @@ static int push_leaf_left(struct ctree_root *root, struct ctree_path *path,
  *
  * returns 0 if all went well and < 0 on failure.
  */
-static int split_leaf(struct ctree_root *root, struct ctree_path *path,
+static int split_leaf(struct btrfs_root *root, struct btrfs_path *path,
 		      int data_size)
 {
-	struct tree_buffer *l_buf;
-	struct leaf *l;
+	struct btrfs_buffer *l_buf;
+	struct btrfs_leaf *l;
 	u32 nritems;
 	int mid;
 	int slot;
-	struct leaf *right;
-	struct tree_buffer *right_buffer;
+	struct btrfs_leaf *right;
+	struct btrfs_buffer *right_buffer;
 	int space_needed = data_size + sizeof(struct btrfs_item);
 	int data_copy_size;
 	int rt_data_off;
@@ -1047,7 +1047,7 @@ static int split_leaf(struct ctree_root *root, struct ctree_path *path,
 	l = &l_buf->leaf;
 
 	/* did the pushes work? */
-	if (leaf_free_space(l) >= sizeof(struct btrfs_item) + data_size)
+	if (btrfs_leaf_free_space(l) >= sizeof(struct btrfs_item) + data_size)
 		return 0;
 
 	if (!path->nodes[1]) {
@@ -1058,7 +1058,7 @@ static int split_leaf(struct ctree_root *root, struct ctree_path *path,
 	slot = path->slots[0];
 	nritems = btrfs_header_nritems(&l->header);
 	mid = (nritems + 1)/ 2;
-	right_buffer = alloc_free_block(root);
+	right_buffer = btrfs_alloc_free_block(root);
 	BUG_ON(!right_buffer);
 	BUG_ON(mid == nritems);
 	right = &right_buffer->leaf;
@@ -1101,12 +1101,12 @@ static int split_leaf(struct ctree_root *root, struct ctree_path *path,
 	BUG_ON(list_empty(&l_buf->dirty));
 	BUG_ON(path->slots[0] != slot);
 	if (mid <= slot) {
-		tree_block_release(root, path->nodes[0]);
+		btrfs_block_release(root, path->nodes[0]);
 		path->nodes[0] = right_buffer;
 		path->slots[0] -= mid;
 		path->slots[1] += 1;
 	} else
-		tree_block_release(root, right_buffer);
+		btrfs_block_release(root, right_buffer);
 	BUG_ON(path->slots[0] < 0);
 	return ret;
 }
@@ -1115,17 +1115,17 @@ static int split_leaf(struct ctree_root *root, struct ctree_path *path,
  * Given a key and some data, insert an item into the tree.
  * This does all the path init required, making room in the tree if needed.
  */
-int insert_item(struct ctree_root *root, struct btrfs_key *cpu_key,
+int btrfs_insert_item(struct btrfs_root *root, struct btrfs_key *cpu_key,
 			  void *data, int data_size)
 {
 	int ret = 0;
 	int slot;
 	int slot_orig;
-	struct leaf *leaf;
-	struct tree_buffer *leaf_buf;
+	struct btrfs_leaf *leaf;
+	struct btrfs_buffer *leaf_buf;
 	u32 nritems;
 	unsigned int data_end;
-	struct ctree_path path;
+	struct btrfs_path path;
 	struct btrfs_disk_key disk_key;
 
 	btrfs_cpu_key_to_disk(&disk_key, cpu_key);
@@ -1133,10 +1133,10 @@ int insert_item(struct ctree_root *root, struct btrfs_key *cpu_key,
 	/* create a root if there isn't one */
 	if (!root->node)
 		BUG();
-	init_path(&path);
-	ret = search_slot(root, cpu_key, &path, data_size, 1);
+	btrfs_init_path(&path);
+	ret = btrfs_search_slot(root, cpu_key, &path, data_size, 1);
 	if (ret == 0) {
-		release_path(root, &path);
+		btrfs_release_path(root, &path);
 		return -EEXIST;
 	}
 	if (ret < 0)
@@ -1149,7 +1149,8 @@ int insert_item(struct ctree_root *root, struct btrfs_key *cpu_key,
 	nritems = btrfs_header_nritems(&leaf->header);
 	data_end = leaf_data_end(leaf);
 
-	if (leaf_free_space(leaf) <  sizeof(struct btrfs_item) + data_size)
+	if (btrfs_leaf_free_space(leaf) <
+	    sizeof(struct btrfs_item) + data_size)
 		BUG();
 
 	slot = path.slots[0];
@@ -1190,11 +1191,11 @@ int insert_item(struct ctree_root *root, struct btrfs_key *cpu_key,
 		ret = fixup_low_keys(root, &path, &disk_key, 1);
 
 	BUG_ON(list_empty(&leaf_buf->dirty));
-	if (leaf_free_space(leaf) < 0)
+	if (btrfs_leaf_free_space(leaf) < 0)
 		BUG();
 	check_leaf(&path, 0);
 out:
-	release_path(root, &path);
+	btrfs_release_path(root, &path);
 	return ret;
 }
 
@@ -1205,11 +1206,11 @@ int insert_item(struct ctree_root *root, struct btrfs_key *cpu_key,
  * continuing all the way the root if required.  The root is converted into
  * a leaf if all the nodes are emptied.
  */
-static int del_ptr(struct ctree_root *root, struct ctree_path *path, int level,
+static int del_ptr(struct btrfs_root *root, struct btrfs_path *path, int level,
 		   int slot)
 {
-	struct node *node;
-	struct tree_buffer *parent = path->nodes[level];
+	struct btrfs_node *node;
+	struct btrfs_buffer *parent = path->nodes[level];
 	u32 nritems;
 	int ret = 0;
 	int wret;
@@ -1242,11 +1243,11 @@ static int del_ptr(struct ctree_root *root, struct ctree_path *path, int level,
  * delete the item at the leaf level in path.  If that empties
  * the leaf, remove it from the tree
  */
-int del_item(struct ctree_root *root, struct ctree_path *path)
+int btrfs_del_item(struct btrfs_root *root, struct btrfs_path *path)
 {
 	int slot;
-	struct leaf *leaf;
-	struct tree_buffer *leaf_buf;
+	struct btrfs_leaf *leaf;
+	struct btrfs_buffer *leaf_buf;
 	int doff;
 	int dsize;
 	int ret = 0;
@@ -1286,7 +1287,7 @@ int del_item(struct ctree_root *root, struct ctree_path *path)
 			wret = del_ptr(root, path, 1, path->slots[1]);
 			if (wret)
 				ret = wret;
-			wret = free_extent(root, leaf_buf->blocknr, 1);
+			wret = btrfs_free_extent(root, leaf_buf->blocknr, 1);
 			if (wret)
 				ret = wret;
 		}
@@ -1323,12 +1324,12 @@ int del_item(struct ctree_root *root, struct ctree_path *path)
 				wret = del_ptr(root, path, 1, slot);
 				if (wret)
 					ret = wret;
-				tree_block_release(root, leaf_buf);
-				wret = free_extent(root, blocknr, 1);
+				btrfs_block_release(root, leaf_buf);
+				wret = btrfs_free_extent(root, blocknr, 1);
 				if (wret)
 					ret = wret;
 			} else {
-				tree_block_release(root, leaf_buf);
+				btrfs_block_release(root, leaf_buf);
 			}
 		}
 	}
@@ -1340,15 +1341,15 @@ int del_item(struct ctree_root *root, struct ctree_path *path)
  * returns 0 if it found something or 1 if there are no greater leaves.
  * returns < 0 on io errors.
  */
-int next_leaf(struct ctree_root *root, struct ctree_path *path)
+int btrfs_next_leaf(struct btrfs_root *root, struct btrfs_path *path)
 {
 	int slot;
 	int level = 1;
 	u64 blocknr;
-	struct tree_buffer *c;
-	struct tree_buffer *next = NULL;
+	struct btrfs_buffer *c;
+	struct btrfs_buffer *next = NULL;
 
-	while(level < MAX_LEVEL) {
+	while(level < BTRFS_MAX_LEVEL) {
 		if (!path->nodes[level])
 			return 1;
 		slot = path->slots[level] + 1;
@@ -1359,7 +1360,7 @@ int next_leaf(struct ctree_root *root, struct ctree_path *path)
 		}
 		blocknr = btrfs_node_blockptr(&c->node, slot);
 		if (next)
-			tree_block_release(root, next);
+			btrfs_block_release(root, next);
 		next = read_tree_block(root, blocknr);
 		break;
 	}
@@ -1367,7 +1368,7 @@ int next_leaf(struct ctree_root *root, struct ctree_path *path)
 	while(1) {
 		level--;
 		c = path->nodes[level];
-		tree_block_release(root, c);
+		btrfs_block_release(root, c);
 		path->nodes[level] = next;
 		path->slots[level] = 0;
 		if (!level)

commit 1d4f8a0c1eca5586134b56a4114a7cd5e85e3560
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Mar 13 09:28:32 2007 -0400

    Btrfs: node->blockptrs endian fixes
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 105556470055..762810731001 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -58,7 +58,8 @@ int btrfs_cow_block(struct ctree_root *root,
 			free_extent(root, buf->blocknr, 1);
 		tree_block_release(root, buf);
 	} else {
-		parent->node.blockptrs[parent_slot] = cow->blocknr;
+		btrfs_set_node_blockptr(&parent->node, parent_slot,
+					cow->blocknr);
 		BUG_ON(list_empty(&parent->dirty));
 		free_extent(root, buf->blocknr, 1);
 	}
@@ -133,7 +134,7 @@ int check_node(struct ctree_path *path, int level)
 		parent_key = &parent->keys[parent_slot];
 		BUG_ON(memcmp(parent_key, node->keys,
 			      sizeof(struct btrfs_disk_key)));
-		BUG_ON(parent->blockptrs[parent_slot] !=
+		BUG_ON(btrfs_node_blockptr(parent, parent_slot) !=
 		       btrfs_header_blocknr(&node->header));
 	}
 	BUG_ON(nritems > NODEPTRS_PER_BLOCK);
@@ -166,7 +167,7 @@ int check_leaf(struct ctree_path *path, int level)
 		parent_key = &parent->keys[parent_slot];
 		BUG_ON(memcmp(parent_key, &leaf->items[0].key,
 		       sizeof(struct btrfs_disk_key)));
-		BUG_ON(parent->blockptrs[parent_slot] !=
+		BUG_ON(btrfs_node_blockptr(parent, parent_slot) !=
 		       btrfs_header_blocknr(&leaf->header));
 	}
 	for (i = 0; nritems > 1 && i < nritems - 2; i++) {
@@ -258,7 +259,7 @@ struct tree_buffer *read_node_slot(struct ctree_root *root,
 		return NULL;
 	if (slot >= btrfs_header_nritems(&node->header))
 		return NULL;
-	return read_tree_block(root, node->blockptrs[slot]);
+	return read_tree_block(root, btrfs_node_blockptr(node, slot));
 }
 
 static int balance_level(struct ctree_root *root, struct ctree_path *path,
@@ -283,7 +284,7 @@ static int balance_level(struct ctree_root *root, struct ctree_path *path,
 
 	mid_buf = path->nodes[level];
 	mid = &mid_buf->node;
-	orig_ptr = mid->blockptrs[orig_slot];
+	orig_ptr = btrfs_node_blockptr(mid, orig_slot);
 
 	if (level < MAX_LEVEL - 1)
 		parent_buf = path->nodes[level + 1];
@@ -407,7 +408,8 @@ static int balance_level(struct ctree_root *root, struct ctree_path *path,
 	}
 	/* double check we haven't messed things up */
 	check_block(path, level);
-	if (orig_ptr != path->nodes[level]->node.blockptrs[path->slots[level]])
+	if (orig_ptr != btrfs_node_blockptr(&path->nodes[level]->node,
+					    path->slots[level]))
 		BUG();
 
 	if (right_buf)
@@ -482,7 +484,7 @@ int search_slot(struct ctree_root *root, struct btrfs_key *key,
 				slot = p->slots[level];
 				BUG_ON(btrfs_header_nritems(&c->header) == 1);
 			}
-			b = read_tree_block(root, c->blockptrs[slot]);
+			b = read_tree_block(root, btrfs_node_blockptr(c, slot));
 		} else {
 			struct leaf *l = (struct leaf *)c;
 			p->slots[level] = slot;
@@ -660,7 +662,7 @@ static int insert_new_root(struct ctree_root *root,
 	else
 		lower_key = lower->keys;
 	memcpy(c->keys, lower_key, sizeof(struct btrfs_disk_key));
-	c->blockptrs[0] = path->nodes[level-1]->blocknr;
+	btrfs_set_node_blockptr(c, 0, path->nodes[level - 1]->blocknr);
 	/* the super has an extra ref to root->node */
 	tree_block_release(root, root->node);
 	root->node = t;
@@ -700,7 +702,7 @@ static int insert_ptr(struct ctree_root *root,
 			(nritems - slot) * sizeof(u64));
 	}
 	memcpy(lower->keys + slot, key, sizeof(struct btrfs_disk_key));
-	lower->blockptrs[slot] = blocknr;
+	btrfs_set_node_blockptr(lower, slot, blocknr);
 	btrfs_set_header_nritems(&lower->header, nritems + 1);
 	if (lower->keys[1].objectid == 0)
 			BUG();
@@ -820,7 +822,8 @@ static int push_leaf_right(struct ctree_root *root, struct ctree_path *path,
 	if (slot >= btrfs_header_nritems(&upper->node.header) - 1) {
 		return 1;
 	}
-	right_buf = read_tree_block(root, upper->node.blockptrs[slot + 1]);
+	right_buf = read_tree_block(root, btrfs_node_blockptr(&upper->node,
+							      slot + 1));
 	right = &right_buf->leaf;
 	free_space = leaf_free_space(right);
 	if (free_space < data_size + sizeof(struct btrfs_item)) {
@@ -926,7 +929,8 @@ static int push_leaf_left(struct ctree_root *root, struct ctree_path *path,
 	if (!path->nodes[1]) {
 		return 1;
 	}
-	t = read_tree_block(root, path->nodes[1]->node.blockptrs[slot - 1]);
+	t = read_tree_block(root, btrfs_node_blockptr(&path->nodes[1]->node,
+						      slot - 1));
 	left = &t->leaf;
 	free_space = leaf_free_space(left);
 	if (free_space < data_size + sizeof(struct btrfs_item)) {
@@ -1353,7 +1357,7 @@ int next_leaf(struct ctree_root *root, struct ctree_path *path)
 			level++;
 			continue;
 		}
-		blocknr = c->node.blockptrs[slot];
+		blocknr = btrfs_node_blockptr(&c->node, slot);
 		if (next)
 			tree_block_release(root, next);
 		next = read_tree_block(root, blocknr);
@@ -1368,7 +1372,8 @@ int next_leaf(struct ctree_root *root, struct ctree_path *path)
 		path->slots[level] = 0;
 		if (!level)
 			break;
-		next = read_tree_block(root, next->node.blockptrs[0]);
+		next = read_tree_block(root,
+				       btrfs_node_blockptr(&next->node, 0));
 	}
 	return 0;
 }

commit 0783fcfc4dc19b8bb99bd51b7afa669ba4cbd377
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Mar 12 20:12:07 2007 -0400

    Btrfs: struct item endian fixes
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 489019ac04b8..105556470055 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -76,7 +76,7 @@ static inline unsigned int leaf_data_end(struct leaf *leaf)
 	u32 nr = btrfs_header_nritems(&leaf->header);
 	if (nr == 0)
 		return sizeof(leaf->data);
-	return leaf->items[nr-1].offset;
+	return btrfs_item_offset(leaf->items + nr - 1);
 }
 
 /*
@@ -174,11 +174,12 @@ int check_leaf(struct ctree_path *path, int level)
 		btrfs_disk_key_to_cpu(&cpukey, &leaf->items[i + 1].key);
 		BUG_ON(comp_keys(&leaf->items[i].key,
 		                 &cpukey) >= 0);
-		BUG_ON(leaf->items[i].offset != leaf->items[i + 1].offset +
-		    leaf->items[i + 1].size);
+		BUG_ON(btrfs_item_offset(leaf->items + i) !=
+			btrfs_item_end(leaf->items + i + 1));
 		if (i == 0) {
-			BUG_ON(leaf->items[i].offset + leaf->items[i].size !=
-				LEAF_DATA_SIZE);
+			BUG_ON(btrfs_item_offset(leaf->items + i) +
+			       btrfs_item_size(leaf->items + i) !=
+			       LEAF_DATA_SIZE);
 		}
 	}
 	return 0;
@@ -235,7 +236,8 @@ int bin_search(struct node *c, struct btrfs_key *key, int *slot)
 {
 	if (btrfs_is_leaf(c)) {
 		struct leaf *l = (struct leaf *)c;
-		return generic_bin_search((void *)l->items, sizeof(struct item),
+		return generic_bin_search((void *)l->items,
+					  sizeof(struct btrfs_item),
 					  key, btrfs_header_nritems(&c->header),
 					  slot);
 	} else {
@@ -485,7 +487,7 @@ int search_slot(struct ctree_root *root, struct btrfs_key *key,
 			struct leaf *l = (struct leaf *)c;
 			p->slots[level] = slot;
 			if (ins_len > 0 && leaf_free_space(l) <
-			    sizeof(struct item) + ins_len) {
+			    sizeof(struct btrfs_item) + ins_len) {
 				int sret = split_leaf(root, p, ins_len);
 				BUG_ON(sret > 0);
 				if (sret)
@@ -780,9 +782,9 @@ static int leaf_space_used(struct leaf *l, int start, int nr)
 
 	if (!nr)
 		return 0;
-	data_len = l->items[start].offset + l->items[start].size;
-	data_len = data_len - l->items[end].offset;
-	data_len += sizeof(struct item) * nr;
+	data_len = btrfs_item_end(l->items + start);
+	data_len = data_len - btrfs_item_offset(l->items + end);
+	data_len += sizeof(struct btrfs_item) * nr;
 	return data_len;
 }
 
@@ -806,7 +808,7 @@ static int push_leaf_right(struct ctree_root *root, struct ctree_path *path,
 	int free_space;
 	int push_space = 0;
 	int push_items = 0;
-	struct item *item;
+	struct btrfs_item *item;
 	u32 left_nritems;
 	u32 right_nritems;
 
@@ -821,7 +823,7 @@ static int push_leaf_right(struct ctree_root *root, struct ctree_path *path,
 	right_buf = read_tree_block(root, upper->node.blockptrs[slot + 1]);
 	right = &right_buf->leaf;
 	free_space = leaf_free_space(right);
-	if (free_space < data_size + sizeof(struct item)) {
+	if (free_space < data_size + sizeof(struct btrfs_item)) {
 		tree_block_release(root, right_buf);
 		return 1;
 	}
@@ -829,7 +831,7 @@ static int push_leaf_right(struct ctree_root *root, struct ctree_path *path,
 	btrfs_cow_block(root, right_buf, upper, slot + 1, &right_buf);
 	right = &right_buf->leaf;
 	free_space = leaf_free_space(right);
-	if (free_space < data_size + sizeof(struct item)) {
+	if (free_space < data_size + sizeof(struct btrfs_item)) {
 		tree_block_release(root, right_buf);
 		return 1;
 	}
@@ -839,10 +841,11 @@ static int push_leaf_right(struct ctree_root *root, struct ctree_path *path,
 		item = left->items + i;
 		if (path->slots[0] == i)
 			push_space += data_size + sizeof(*item);
-		if (item->size + sizeof(*item) + push_space > free_space)
+		if (btrfs_item_size(item) + sizeof(*item) + push_space >
+		    free_space)
 			break;
 		push_items++;
-		push_space += item->size + sizeof(*item);
+		push_space += btrfs_item_size(item) + sizeof(*item);
 	}
 	if (push_items == 0) {
 		tree_block_release(root, right_buf);
@@ -850,8 +853,7 @@ static int push_leaf_right(struct ctree_root *root, struct ctree_path *path,
 	}
 	right_nritems = btrfs_header_nritems(&right->header);
 	/* push left to right */
-	push_space = left->items[left_nritems - push_items].offset +
-		     left->items[left_nritems - push_items].size;
+	push_space = btrfs_item_end(left->items + left_nritems - push_items);
 	push_space -= leaf_data_end(left);
 	/* make room in the right data area */
 	memmove(right->data + leaf_data_end(right) - push_space,
@@ -862,18 +864,19 @@ static int push_leaf_right(struct ctree_root *root, struct ctree_path *path,
 		left->data + leaf_data_end(left),
 		push_space);
 	memmove(right->items + push_items, right->items,
-		right_nritems * sizeof(struct item));
+		right_nritems * sizeof(struct btrfs_item));
 	/* copy the items from left to right */
 	memcpy(right->items, left->items + left_nritems - push_items,
-		push_items * sizeof(struct item));
+		push_items * sizeof(struct btrfs_item));
 
 	/* update the item pointers */
 	right_nritems += push_items;
 	btrfs_set_header_nritems(&right->header, right_nritems);
 	push_space = LEAF_DATA_SIZE;
 	for (i = 0; i < right_nritems; i++) {
-		right->items[i].offset = push_space - right->items[i].size;
-		push_space = right->items[i].offset;
+		btrfs_set_item_offset(right->items + i, push_space -
+				      btrfs_item_size(right->items + i));
+		push_space = btrfs_item_offset(right->items + i);
 	}
 	left_nritems -= push_items;
 	btrfs_set_header_nritems(&left->header, left_nritems);
@@ -911,7 +914,7 @@ static int push_leaf_left(struct ctree_root *root, struct ctree_path *path,
 	int free_space;
 	int push_space = 0;
 	int push_items = 0;
-	struct item *item;
+	struct btrfs_item *item;
 	u32 old_left_nritems;
 	int ret = 0;
 	int wret;
@@ -926,7 +929,7 @@ static int push_leaf_left(struct ctree_root *root, struct ctree_path *path,
 	t = read_tree_block(root, path->nodes[1]->node.blockptrs[slot - 1]);
 	left = &t->leaf;
 	free_space = leaf_free_space(left);
-	if (free_space < data_size + sizeof(struct item)) {
+	if (free_space < data_size + sizeof(struct btrfs_item)) {
 		tree_block_release(root, t);
 		return 1;
 	}
@@ -935,7 +938,7 @@ static int push_leaf_left(struct ctree_root *root, struct ctree_path *path,
 	btrfs_cow_block(root, t, path->nodes[1], slot - 1, &t);
 	left = &t->leaf;
 	free_space = leaf_free_space(left);
-	if (free_space < data_size + sizeof(struct item)) {
+	if (free_space < data_size + sizeof(struct btrfs_item)) {
 		tree_block_release(root, t);
 		return 1;
 	}
@@ -944,10 +947,11 @@ static int push_leaf_left(struct ctree_root *root, struct ctree_path *path,
 		item = right->items + i;
 		if (path->slots[0] == i)
 			push_space += data_size + sizeof(*item);
-		if (item->size + sizeof(*item) + push_space > free_space)
+		if (btrfs_item_size(item) + sizeof(*item) + push_space >
+		    free_space)
 			break;
 		push_items++;
-		push_space += item->size + sizeof(*item);
+		push_space += btrfs_item_size(item) + sizeof(*item);
 	}
 	if (push_items == 0) {
 		tree_block_release(root, t);
@@ -955,35 +959,40 @@ static int push_leaf_left(struct ctree_root *root, struct ctree_path *path,
 	}
 	/* push data from right to left */
 	memcpy(left->items + btrfs_header_nritems(&left->header),
-		right->items, push_items * sizeof(struct item));
-	push_space = LEAF_DATA_SIZE - right->items[push_items -1].offset;
+		right->items, push_items * sizeof(struct btrfs_item));
+	push_space = LEAF_DATA_SIZE -
+		     btrfs_item_offset(right->items + push_items -1);
 	memcpy(left->data + leaf_data_end(left) - push_space,
-		right->data + right->items[push_items - 1].offset,
+		right->data + btrfs_item_offset(right->items + push_items - 1),
 		push_space);
 	old_left_nritems = btrfs_header_nritems(&left->header);
 	BUG_ON(old_left_nritems < 0);
 
-	for(i = old_left_nritems; i < old_left_nritems + push_items; i++) {
-		left->items[i].offset -= LEAF_DATA_SIZE -
-			left->items[old_left_nritems -1].offset;
+	for (i = old_left_nritems; i < old_left_nritems + push_items; i++) {
+		u16 ioff = btrfs_item_offset(left->items + i);
+		btrfs_set_item_offset(left->items + i, ioff - (LEAF_DATA_SIZE -
+				      btrfs_item_offset(left->items +
+						        old_left_nritems - 1)));
 	}
 	btrfs_set_header_nritems(&left->header, old_left_nritems + push_items);
 
 	/* fixup right node */
-	push_space = right->items[push_items-1].offset - leaf_data_end(right);
+	push_space = btrfs_item_offset(right->items + push_items - 1) -
+		     leaf_data_end(right);
 	memmove(right->data + LEAF_DATA_SIZE - push_space, right->data +
 		leaf_data_end(right), push_space);
 	memmove(right->items, right->items + push_items,
 		(btrfs_header_nritems(&right->header) - push_items) *
-		sizeof(struct item));
+		sizeof(struct btrfs_item));
 	btrfs_set_header_nritems(&right->header,
 				 btrfs_header_nritems(&right->header) -
 				 push_items);
 	push_space = LEAF_DATA_SIZE;
 
 	for (i = 0; i < btrfs_header_nritems(&right->header); i++) {
-		right->items[i].offset = push_space - right->items[i].size;
-		push_space = right->items[i].offset;
+		btrfs_set_item_offset(right->items + i, push_space -
+				      btrfs_item_size(right->items + i));
+		push_space = btrfs_item_offset(right->items + i);
 	}
 
 	BUG_ON(list_empty(&t->dirty));
@@ -1023,7 +1032,7 @@ static int split_leaf(struct ctree_root *root, struct ctree_path *path,
 	int slot;
 	struct leaf *right;
 	struct tree_buffer *right_buffer;
-	int space_needed = data_size + sizeof(struct item);
+	int space_needed = data_size + sizeof(struct btrfs_item);
 	int data_copy_size;
 	int rt_data_off;
 	int i;
@@ -1034,7 +1043,7 @@ static int split_leaf(struct ctree_root *root, struct ctree_path *path,
 	l = &l_buf->leaf;
 
 	/* did the pushes work? */
-	if (leaf_free_space(l) >= sizeof(struct item) + data_size)
+	if (leaf_free_space(l) >= sizeof(struct btrfs_item) + data_size)
 		return 0;
 
 	if (!path->nodes[1]) {
@@ -1066,17 +1075,17 @@ static int split_leaf(struct ctree_root *root, struct ctree_path *path,
 	btrfs_set_header_level(&right->header, 0);
 	btrfs_set_header_parentid(&right->header,
 	                       btrfs_header_parentid(&root->node->node.header));
-	data_copy_size = l->items[mid].offset + l->items[mid].size -
-			 leaf_data_end(l);
+	data_copy_size = btrfs_item_end(l->items + mid) - leaf_data_end(l);
 	memcpy(right->items, l->items + mid,
-	       (nritems - mid) * sizeof(struct item));
+	       (nritems - mid) * sizeof(struct btrfs_item));
 	memcpy(right->data + LEAF_DATA_SIZE - data_copy_size,
 	       l->data + leaf_data_end(l), data_copy_size);
-	rt_data_off = LEAF_DATA_SIZE -
-		     (l->items[mid].offset + l->items[mid].size);
+	rt_data_off = LEAF_DATA_SIZE - btrfs_item_end(l->items + mid);
 
-	for (i = 0; i < btrfs_header_nritems(&right->header); i++)
-		right->items[i].offset += rt_data_off;
+	for (i = 0; i < btrfs_header_nritems(&right->header); i++) {
+		u16 ioff = btrfs_item_offset(right->items + i);
+		btrfs_set_item_offset(right->items + i, ioff + rt_data_off);
+	}
 
 	btrfs_set_header_nritems(&l->header, mid);
 	ret = 0;
@@ -1136,26 +1145,28 @@ int insert_item(struct ctree_root *root, struct btrfs_key *cpu_key,
 	nritems = btrfs_header_nritems(&leaf->header);
 	data_end = leaf_data_end(leaf);
 
-	if (leaf_free_space(leaf) <  sizeof(struct item) + data_size)
+	if (leaf_free_space(leaf) <  sizeof(struct btrfs_item) + data_size)
 		BUG();
 
 	slot = path.slots[0];
 	BUG_ON(slot < 0);
 	if (slot != nritems) {
 		int i;
-		unsigned int old_data = leaf->items[slot].offset +
-					leaf->items[slot].size;
+		unsigned int old_data = btrfs_item_end(leaf->items + slot);
 
 		/*
 		 * item0..itemN ... dataN.offset..dataN.size .. data0.size
 		 */
 		/* first correct the data pointers */
-		for (i = slot; i < nritems; i++)
-			leaf->items[i].offset -= data_size;
+		for (i = slot; i < nritems; i++) {
+			u16 ioff = btrfs_item_offset(leaf->items + i);
+			btrfs_set_item_offset(leaf->items + i,
+					      ioff - data_size);
+		}
 
 		/* shift the items */
 		memmove(leaf->items + slot + 1, leaf->items + slot,
-		        (nritems - slot) * sizeof(struct item));
+		        (nritems - slot) * sizeof(struct btrfs_item));
 
 		/* shift the data */
 		memmove(leaf->data + data_end - data_size, leaf->data +
@@ -1165,8 +1176,8 @@ int insert_item(struct ctree_root *root, struct btrfs_key *cpu_key,
 	/* copy the new data in */
 	memcpy(&leaf->items[slot].key, &disk_key,
 		sizeof(struct btrfs_disk_key));
-	leaf->items[slot].offset = data_end - data_size;
-	leaf->items[slot].size = data_size;
+	btrfs_set_item_offset(leaf->items + slot, data_end - data_size);
+	btrfs_set_item_size(leaf->items + slot, data_size);
 	memcpy(leaf->data + data_end - data_size, data, data_size);
 	btrfs_set_header_nritems(&leaf->header, nritems + 1);
 
@@ -1241,8 +1252,8 @@ int del_item(struct ctree_root *root, struct ctree_path *path)
 	leaf_buf = path->nodes[0];
 	leaf = &leaf_buf->leaf;
 	slot = path->slots[0];
-	doff = leaf->items[slot].offset;
-	dsize = leaf->items[slot].size;
+	doff = btrfs_item_offset(leaf->items + slot);
+	dsize = btrfs_item_size(leaf->items + slot);
 	nritems = btrfs_header_nritems(&leaf->header);
 
 	if (slot != nritems - 1) {
@@ -1251,10 +1262,12 @@ int del_item(struct ctree_root *root, struct ctree_path *path)
 		memmove(leaf->data + data_end + dsize,
 			leaf->data + data_end,
 			doff - data_end);
-		for (i = slot + 1; i < nritems; i++)
-			leaf->items[i].offset += dsize;
+		for (i = slot + 1; i < nritems; i++) {
+			u16 ioff = btrfs_item_offset(leaf->items + i);
+			btrfs_set_item_offset(leaf->items + i, ioff + dsize);
+		}
 		memmove(leaf->items + slot, leaf->items + slot + 1,
-			sizeof(struct item) *
+			sizeof(struct btrfs_item) *
 			(nritems - slot - 1));
 	}
 	btrfs_set_header_nritems(&leaf->header, nritems - 1);

commit e2fa7227cdf132d72e7410dd0679dc573a1c2618
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Mar 12 16:22:34 2007 -0400

    Btrfs: struct key endian fixes
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index e43c827e0dfd..489019ac04b8 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -95,19 +95,23 @@ int leaf_free_space(struct leaf *leaf)
 /*
  * compare two keys in a memcmp fashion
  */
-int comp_keys(struct key *k1, struct key *k2)
+int comp_keys(struct btrfs_disk_key *disk, struct btrfs_key *k2)
 {
-	if (k1->objectid > k2->objectid)
+	struct btrfs_key k1;
+
+	btrfs_disk_key_to_cpu(&k1, disk);
+
+	if (k1.objectid > k2->objectid)
 		return 1;
-	if (k1->objectid < k2->objectid)
+	if (k1.objectid < k2->objectid)
 		return -1;
-	if (k1->flags > k2->flags)
+	if (k1.flags > k2->flags)
 		return 1;
-	if (k1->flags < k2->flags)
+	if (k1.flags < k2->flags)
 		return -1;
-	if (k1->offset > k2->offset)
+	if (k1.offset > k2->offset)
 		return 1;
-	if (k1->offset < k2->offset)
+	if (k1.offset < k2->offset)
 		return -1;
 	return 0;
 }
@@ -125,15 +129,18 @@ int check_node(struct ctree_path *path, int level)
 	parent_slot = path->slots[level + 1];
 	BUG_ON(nritems == 0);
 	if (parent) {
-		struct key *parent_key;
+		struct btrfs_disk_key *parent_key;
 		parent_key = &parent->keys[parent_slot];
-		BUG_ON(memcmp(parent_key, node->keys, sizeof(struct key)));
+		BUG_ON(memcmp(parent_key, node->keys,
+			      sizeof(struct btrfs_disk_key)));
 		BUG_ON(parent->blockptrs[parent_slot] !=
 		       btrfs_header_blocknr(&node->header));
 	}
 	BUG_ON(nritems > NODEPTRS_PER_BLOCK);
 	for (i = 0; nritems > 1 && i < nritems - 2; i++) {
-		BUG_ON(comp_keys(&node->keys[i], &node->keys[i+1]) >= 0);
+		struct btrfs_key cpukey;
+		btrfs_disk_key_to_cpu(&cpukey, &node->keys[i + 1]);
+		BUG_ON(comp_keys(&node->keys[i], &cpukey) >= 0);
 	}
 	return 0;
 }
@@ -155,16 +162,18 @@ int check_leaf(struct ctree_path *path, int level)
 		return 0;
 
 	if (parent) {
-		struct key *parent_key;
+		struct btrfs_disk_key *parent_key;
 		parent_key = &parent->keys[parent_slot];
 		BUG_ON(memcmp(parent_key, &leaf->items[0].key,
-		       sizeof(struct key)));
+		       sizeof(struct btrfs_disk_key)));
 		BUG_ON(parent->blockptrs[parent_slot] !=
 		       btrfs_header_blocknr(&leaf->header));
 	}
 	for (i = 0; nritems > 1 && i < nritems - 2; i++) {
+		struct btrfs_key cpukey;
+		btrfs_disk_key_to_cpu(&cpukey, &leaf->items[i + 1].key);
 		BUG_ON(comp_keys(&leaf->items[i].key,
-		                 &leaf->items[i+1].key) >= 0);
+		                 &cpukey) >= 0);
 		BUG_ON(leaf->items[i].offset != leaf->items[i + 1].offset +
 		    leaf->items[i + 1].size);
 		if (i == 0) {
@@ -191,18 +200,18 @@ int check_block(struct ctree_path *path, int level)
  *
  * slot may point to max if the key is bigger than all of the keys
  */
-int generic_bin_search(char *p, int item_size, struct key *key,
+int generic_bin_search(char *p, int item_size, struct btrfs_key *key,
 		       int max, int *slot)
 {
 	int low = 0;
 	int high = max;
 	int mid;
 	int ret;
-	struct key *tmp;
+	struct btrfs_disk_key *tmp;
 
 	while(low < high) {
 		mid = (low + high) / 2;
-		tmp = (struct key *)(p + mid * item_size);
+		tmp = (struct btrfs_disk_key *)(p + mid * item_size);
 		ret = comp_keys(tmp, key);
 
 		if (ret < 0)
@@ -222,7 +231,7 @@ int generic_bin_search(char *p, int item_size, struct key *key,
  * simple bin_search frontend that does the right thing for
  * leaves vs nodes
  */
-int bin_search(struct node *c, struct key *key, int *slot)
+int bin_search(struct node *c, struct btrfs_key *key, int *slot)
 {
 	if (btrfs_is_leaf(c)) {
 		struct leaf *l = (struct leaf *)c;
@@ -230,7 +239,8 @@ int bin_search(struct node *c, struct key *key, int *slot)
 					  key, btrfs_header_nritems(&c->header),
 					  slot);
 	} else {
-		return generic_bin_search((void *)c->keys, sizeof(struct key),
+		return generic_bin_search((void *)c->keys,
+					  sizeof(struct btrfs_disk_key),
 					  key, btrfs_header_nritems(&c->header),
 					  slot);
 	}
@@ -339,7 +349,7 @@ static int balance_level(struct ctree_root *root, struct ctree_path *path,
 				ret = wret;
 		} else {
 			memcpy(parent->keys + pslot + 1, right->keys,
-				sizeof(struct key));
+				sizeof(struct btrfs_disk_key));
 			BUG_ON(list_empty(&parent_buf->dirty));
 		}
 	}
@@ -374,7 +384,8 @@ static int balance_level(struct ctree_root *root, struct ctree_path *path,
 			ret = wret;
 	} else {
 		/* update the parent key to reflect our changes */
-		memcpy(parent->keys + pslot, mid->keys, sizeof(struct key));
+		memcpy(parent->keys + pslot, mid->keys,
+		       sizeof(struct btrfs_disk_key));
 		BUG_ON(list_empty(&parent_buf->dirty));
 	}
 
@@ -417,7 +428,7 @@ static int balance_level(struct ctree_root *root, struct ctree_path *path,
  * tree.  if ins_len < 0, nodes will be merged as we walk down the tree (if
  * possible)
  */
-int search_slot(struct ctree_root *root, struct key *key,
+int search_slot(struct ctree_root *root, struct btrfs_key *key,
 		struct ctree_path *p, int ins_len, int cow)
 {
 	struct tree_buffer *b;
@@ -499,7 +510,7 @@ int search_slot(struct ctree_root *root, struct key *key,
  * fixing up the blocks in ram so the tree is consistent.
  */
 static int fixup_low_keys(struct ctree_root *root,
-			   struct ctree_path *path, struct key *key,
+			   struct ctree_path *path, struct btrfs_disk_key *key,
 			   int level)
 {
 	int i;
@@ -546,12 +557,13 @@ static int push_node_left(struct ctree_root *root, struct tree_buffer *dst_buf,
 		push_items = src_nritems;
 
 	memcpy(dst->keys + dst_nritems, src->keys,
-		push_items * sizeof(struct key));
+		push_items * sizeof(struct btrfs_disk_key));
 	memcpy(dst->blockptrs + dst_nritems, src->blockptrs,
 		push_items * sizeof(u64));
 	if (push_items < src_nritems) {
 		memmove(src->keys, src->keys + push_items,
-			(src_nritems - push_items) * sizeof(struct key));
+			(src_nritems - push_items) *
+			sizeof(struct btrfs_disk_key));
 		memmove(src->blockptrs, src->blockptrs + push_items,
 			(src_nritems - push_items) * sizeof(u64));
 	}
@@ -598,11 +610,11 @@ static int balance_node_right(struct ctree_root *root,
 		push_items = max_push;
 
 	memmove(dst->keys + push_items, dst->keys,
-		dst_nritems * sizeof(struct key));
+		dst_nritems * sizeof(struct btrfs_disk_key));
 	memmove(dst->blockptrs + push_items, dst->blockptrs,
 		dst_nritems * sizeof(u64));
 	memcpy(dst->keys, src->keys + src_nritems - push_items,
-		push_items * sizeof(struct key));
+		push_items * sizeof(struct btrfs_disk_key));
 	memcpy(dst->blockptrs, src->blockptrs + src_nritems - push_items,
 		push_items * sizeof(u64));
 
@@ -627,7 +639,7 @@ static int insert_new_root(struct ctree_root *root,
 	struct tree_buffer *t;
 	struct node *lower;
 	struct node *c;
-	struct key *lower_key;
+	struct btrfs_disk_key *lower_key;
 
 	BUG_ON(path->nodes[level]);
 	BUG_ON(path->nodes[level-1] != root->node);
@@ -645,7 +657,7 @@ static int insert_new_root(struct ctree_root *root,
 		lower_key = &((struct leaf *)lower)->items[0].key;
 	else
 		lower_key = lower->keys;
-	memcpy(c->keys, lower_key, sizeof(struct key));
+	memcpy(c->keys, lower_key, sizeof(struct btrfs_disk_key));
 	c->blockptrs[0] = path->nodes[level-1]->blocknr;
 	/* the super has an extra ref to root->node */
 	tree_block_release(root, root->node);
@@ -666,7 +678,7 @@ static int insert_new_root(struct ctree_root *root,
  * returns zero on success and < 0 on any error
  */
 static int insert_ptr(struct ctree_root *root,
-		struct ctree_path *path, struct key *key,
+		struct ctree_path *path, struct btrfs_disk_key *key,
 		u64 blocknr, int slot, int level)
 {
 	struct node *lower;
@@ -681,11 +693,11 @@ static int insert_ptr(struct ctree_root *root,
 		BUG();
 	if (slot != nritems) {
 		memmove(lower->keys + slot + 1, lower->keys + slot,
-			(nritems - slot) * sizeof(struct key));
+			(nritems - slot) * sizeof(struct btrfs_disk_key));
 		memmove(lower->blockptrs + slot + 1, lower->blockptrs + slot,
 			(nritems - slot) * sizeof(u64));
 	}
-	memcpy(lower->keys + slot, key, sizeof(struct key));
+	memcpy(lower->keys + slot, key, sizeof(struct btrfs_disk_key));
 	lower->blockptrs[slot] = blocknr;
 	btrfs_set_header_nritems(&lower->header, nritems + 1);
 	if (lower->keys[1].objectid == 0)
@@ -732,7 +744,7 @@ static int split_node(struct ctree_root *root, struct ctree_path *path,
 	                       btrfs_header_parentid(&root->node->node.header));
 	mid = (c_nritems + 1) / 2;
 	memcpy(split->keys, c->keys + mid,
-		(c_nritems - mid) * sizeof(struct key));
+		(c_nritems - mid) * sizeof(struct btrfs_disk_key));
 	memcpy(split->blockptrs, c->blockptrs + mid,
 		(c_nritems - mid) * sizeof(u64));
 	btrfs_set_header_nritems(&split->header, c_nritems - mid);
@@ -869,7 +881,7 @@ static int push_leaf_right(struct ctree_root *root, struct ctree_path *path,
 	BUG_ON(list_empty(&left_buf->dirty));
 	BUG_ON(list_empty(&right_buf->dirty));
 	memcpy(upper->node.keys + slot + 1,
-		&right->items[0].key, sizeof(struct key));
+		&right->items[0].key, sizeof(struct btrfs_disk_key));
 	BUG_ON(list_empty(&upper->dirty));
 
 	/* then fixup the leaf pointer in the path */
@@ -1090,7 +1102,7 @@ static int split_leaf(struct ctree_root *root, struct ctree_path *path,
  * Given a key and some data, insert an item into the tree.
  * This does all the path init required, making room in the tree if needed.
  */
-int insert_item(struct ctree_root *root, struct key *key,
+int insert_item(struct ctree_root *root, struct btrfs_key *cpu_key,
 			  void *data, int data_size)
 {
 	int ret = 0;
@@ -1101,12 +1113,15 @@ int insert_item(struct ctree_root *root, struct key *key,
 	u32 nritems;
 	unsigned int data_end;
 	struct ctree_path path;
+	struct btrfs_disk_key disk_key;
+
+	btrfs_cpu_key_to_disk(&disk_key, cpu_key);
 
 	/* create a root if there isn't one */
 	if (!root->node)
 		BUG();
 	init_path(&path);
-	ret = search_slot(root, key, &path, data_size, 1);
+	ret = search_slot(root, cpu_key, &path, data_size, 1);
 	if (ret == 0) {
 		release_path(root, &path);
 		return -EEXIST;
@@ -1148,7 +1163,8 @@ int insert_item(struct ctree_root *root, struct key *key,
 		data_end = old_data;
 	}
 	/* copy the new data in */
-	memcpy(&leaf->items[slot].key, key, sizeof(struct key));
+	memcpy(&leaf->items[slot].key, &disk_key,
+		sizeof(struct btrfs_disk_key));
 	leaf->items[slot].offset = data_end - data_size;
 	leaf->items[slot].size = data_size;
 	memcpy(leaf->data + data_end - data_size, data, data_size);
@@ -1156,7 +1172,7 @@ int insert_item(struct ctree_root *root, struct key *key,
 
 	ret = 0;
 	if (slot == 0)
-		ret = fixup_low_keys(root, &path, key, 1);
+		ret = fixup_low_keys(root, &path, &disk_key, 1);
 
 	BUG_ON(list_empty(&leaf_buf->dirty));
 	if (leaf_free_space(leaf) < 0)
@@ -1187,7 +1203,7 @@ static int del_ptr(struct ctree_root *root, struct ctree_path *path, int level,
 	nritems = btrfs_header_nritems(&node->header);
 	if (slot != nritems -1) {
 		memmove(node->keys + slot, node->keys + slot + 1,
-			sizeof(struct key) * (nritems - slot - 1));
+			sizeof(struct btrfs_disk_key) * (nritems - slot - 1));
 		memmove(node->blockptrs + slot,
 			node->blockptrs + slot + 1,
 			sizeof(u64) * (nritems - slot - 1));

commit 7518a238ea0152dc849d1ed76d3cae8b44e12f46
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Mar 12 12:01:18 2007 -0400

    Btrfs: get/set for struct header fields
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 729d4ddb3746..e43c827e0dfd 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -48,7 +48,7 @@ int btrfs_cow_block(struct ctree_root *root,
 	}
 	cow = alloc_free_block(root);
 	memcpy(&cow->node, &buf->node, sizeof(buf->node));
-	cow->node.header.blocknr = cow->blocknr;
+	btrfs_set_header_blocknr(&cow->node.header, cow->blocknr);
 	*cow_ret = cow;
 	btrfs_inc_ref(root, buf);
 	if (buf == root->node) {
@@ -73,7 +73,7 @@ int btrfs_cow_block(struct ctree_root *root,
  */
 static inline unsigned int leaf_data_end(struct leaf *leaf)
 {
-	unsigned int nr = leaf->header.nritems;
+	u32 nr = btrfs_header_nritems(&leaf->header);
 	if (nr == 0)
 		return sizeof(leaf->data);
 	return leaf->items[nr-1].offset;
@@ -87,7 +87,7 @@ static inline unsigned int leaf_data_end(struct leaf *leaf)
 int leaf_free_space(struct leaf *leaf)
 {
 	int data_end = leaf_data_end(leaf);
-	int nritems = leaf->header.nritems;
+	int nritems = btrfs_header_nritems(&leaf->header);
 	char *items_end = (char *)(leaf->items + nritems + 1);
 	return (char *)(leaf->data + data_end) - (char *)items_end;
 }
@@ -118,18 +118,21 @@ int check_node(struct ctree_path *path, int level)
 	struct node *parent = NULL;
 	struct node *node = &path->nodes[level]->node;
 	int parent_slot;
+	u32 nritems = btrfs_header_nritems(&node->header);
 
 	if (path->nodes[level + 1])
 		parent = &path->nodes[level + 1]->node;
 	parent_slot = path->slots[level + 1];
-	if (parent && node->header.nritems > 0) {
+	BUG_ON(nritems == 0);
+	if (parent) {
 		struct key *parent_key;
 		parent_key = &parent->keys[parent_slot];
 		BUG_ON(memcmp(parent_key, node->keys, sizeof(struct key)));
-		BUG_ON(parent->blockptrs[parent_slot] != node->header.blocknr);
+		BUG_ON(parent->blockptrs[parent_slot] !=
+		       btrfs_header_blocknr(&node->header));
 	}
-	BUG_ON(node->header.nritems > NODEPTRS_PER_BLOCK);
-	for (i = 0; i < node->header.nritems - 2; i++) {
+	BUG_ON(nritems > NODEPTRS_PER_BLOCK);
+	for (i = 0; nritems > 1 && i < nritems - 2; i++) {
 		BUG_ON(comp_keys(&node->keys[i], &node->keys[i+1]) >= 0);
 	}
 	return 0;
@@ -141,18 +144,25 @@ int check_leaf(struct ctree_path *path, int level)
 	struct leaf *leaf = &path->nodes[level]->leaf;
 	struct node *parent = NULL;
 	int parent_slot;
+	u32 nritems = btrfs_header_nritems(&leaf->header);
 
 	if (path->nodes[level + 1])
 		parent = &path->nodes[level + 1]->node;
 	parent_slot = path->slots[level + 1];
-	if (parent && leaf->header.nritems > 0) {
+	BUG_ON(leaf_free_space(leaf) < 0);
+
+	if (nritems == 0)
+		return 0;
+
+	if (parent) {
 		struct key *parent_key;
 		parent_key = &parent->keys[parent_slot];
 		BUG_ON(memcmp(parent_key, &leaf->items[0].key,
 		       sizeof(struct key)));
-		BUG_ON(parent->blockptrs[parent_slot] != leaf->header.blocknr);
+		BUG_ON(parent->blockptrs[parent_slot] !=
+		       btrfs_header_blocknr(&leaf->header));
 	}
-	for (i = 0; i < leaf->header.nritems - 2; i++) {
+	for (i = 0; nritems > 1 && i < nritems - 2; i++) {
 		BUG_ON(comp_keys(&leaf->items[i].key,
 		                 &leaf->items[i+1].key) >= 0);
 		BUG_ON(leaf->items[i].offset != leaf->items[i + 1].offset +
@@ -162,7 +172,6 @@ int check_leaf(struct ctree_path *path, int level)
 				LEAF_DATA_SIZE);
 		}
 	}
-	BUG_ON(leaf_free_space(leaf) < 0);
 	return 0;
 }
 
@@ -215,13 +224,15 @@ int generic_bin_search(char *p, int item_size, struct key *key,
  */
 int bin_search(struct node *c, struct key *key, int *slot)
 {
-	if (is_leaf(c->header.flags)) {
+	if (btrfs_is_leaf(c)) {
 		struct leaf *l = (struct leaf *)c;
 		return generic_bin_search((void *)l->items, sizeof(struct item),
-					  key, c->header.nritems, slot);
+					  key, btrfs_header_nritems(&c->header),
+					  slot);
 	} else {
 		return generic_bin_search((void *)c->keys, sizeof(struct key),
-					  key, c->header.nritems, slot);
+					  key, btrfs_header_nritems(&c->header),
+					  slot);
 	}
 	return -1;
 }
@@ -233,7 +244,7 @@ struct tree_buffer *read_node_slot(struct ctree_root *root,
 	struct node *node = &parent_buf->node;
 	if (slot < 0)
 		return NULL;
-	if (slot >= node->header.nritems)
+	if (slot >= btrfs_header_nritems(&node->header))
 		return NULL;
 	return read_tree_block(root, node->blockptrs[slot]);
 }
@@ -270,7 +281,7 @@ static int balance_level(struct ctree_root *root, struct ctree_path *path,
 		struct tree_buffer *child;
 		u64 blocknr = mid_buf->blocknr;
 
-		if (mid->header.nritems != 1)
+		if (btrfs_header_nritems(&mid->header) != 1)
 			return 0;
 
 		/* promote the child to a root */
@@ -287,7 +298,7 @@ static int balance_level(struct ctree_root *root, struct ctree_path *path,
 	}
 	parent = &parent_buf->node;
 
-	if (mid->header.nritems > NODEPTRS_PER_BLOCK / 4)
+	if (btrfs_header_nritems(&mid->header) > NODEPTRS_PER_BLOCK / 4)
 		return 0;
 
 	left_buf = read_node_slot(root, parent_buf, pslot - 1);
@@ -298,7 +309,7 @@ static int balance_level(struct ctree_root *root, struct ctree_path *path,
 		btrfs_cow_block(root, left_buf, parent_buf,
 				pslot - 1, &left_buf);
 		left = &left_buf->node;
-		orig_slot += left->header.nritems;
+		orig_slot += btrfs_header_nritems(&left->header);
 		wret = push_node_left(root, left_buf, mid_buf);
 		if (wret < 0)
 			ret = wret;
@@ -314,7 +325,7 @@ static int balance_level(struct ctree_root *root, struct ctree_path *path,
 		wret = push_node_left(root, mid_buf, right_buf);
 		if (wret < 0)
 			ret = wret;
-		if (right->header.nritems == 0) {
+		if (btrfs_header_nritems(&right->header) == 0) {
 			u64 blocknr = right_buf->blocknr;
 			tree_block_release(root, right_buf);
 			clean_tree_block(root, right_buf);
@@ -332,7 +343,7 @@ static int balance_level(struct ctree_root *root, struct ctree_path *path,
 			BUG_ON(list_empty(&parent_buf->dirty));
 		}
 	}
-	if (mid->header.nritems == 1) {
+	if (btrfs_header_nritems(&mid->header) == 1) {
 		/*
 		 * we're not allowed to leave a node with one item in the
 		 * tree during a delete.  A deletion from lower in the tree
@@ -348,7 +359,7 @@ static int balance_level(struct ctree_root *root, struct ctree_path *path,
 			ret = wret;
 		BUG_ON(wret == 1);
 	}
-	if (mid->header.nritems == 0) {
+	if (btrfs_header_nritems(&mid->header) == 0) {
 		/* we've managed to empty the middle node, drop it */
 		u64 blocknr = mid_buf->blocknr;
 		tree_block_release(root, mid_buf);
@@ -369,7 +380,7 @@ static int balance_level(struct ctree_root *root, struct ctree_path *path,
 
 	/* update the path */
 	if (left_buf) {
-		if (left->header.nritems > orig_slot) {
+		if (btrfs_header_nritems(&left->header) > orig_slot) {
 			left_buf->count++; // released below
 			path->nodes[level] = left_buf;
 			path->slots[level + 1] -= 1;
@@ -377,7 +388,7 @@ static int balance_level(struct ctree_root *root, struct ctree_path *path,
 			if (mid_buf)
 				tree_block_release(root, mid_buf);
 		} else {
-			orig_slot -= left->header.nritems;
+			orig_slot -= btrfs_header_nritems(&left->header);
 			path->slots[level] = orig_slot;
 		}
 	}
@@ -420,7 +431,7 @@ int search_slot(struct ctree_root *root, struct key *key,
 	b = root->node;
 	b->count++;
 	while (b) {
-		level = node_level(b->node.header.flags);
+		level = btrfs_header_level(&b->node.header);
 		if (cow) {
 			int wret;
 			wret = btrfs_cow_block(root, b, p->nodes[level + 1],
@@ -434,12 +445,12 @@ int search_slot(struct ctree_root *root, struct key *key,
 		if (ret)
 			return -1;
 		ret = bin_search(c, key, &slot);
-		if (!is_leaf(c->header.flags)) {
+		if (!btrfs_is_leaf(c)) {
 			if (ret && slot > 0)
 				slot -= 1;
 			p->slots[level] = slot;
-			if (ins_len > 0 &&
-			    c->header.nritems == NODEPTRS_PER_BLOCK) {
+			if (ins_len > 0 && btrfs_header_nritems(&c->header) ==
+			    NODEPTRS_PER_BLOCK) {
 				int sret = split_node(root, p, level);
 				BUG_ON(sret > 0);
 				if (sret)
@@ -456,7 +467,7 @@ int search_slot(struct ctree_root *root, struct key *key,
 					goto again;
 				c = &b->node;
 				slot = p->slots[level];
-				BUG_ON(c->header.nritems == 1);
+				BUG_ON(btrfs_header_nritems(&c->header) == 1);
 			}
 			b = read_tree_block(root, c->blockptrs[slot]);
 		} else {
@@ -524,8 +535,8 @@ static int push_node_left(struct ctree_root *root, struct tree_buffer *dst_buf,
 	int dst_nritems;
 	int ret = 0;
 
-	src_nritems = src->header.nritems;
-	dst_nritems = dst->header.nritems;
+	src_nritems = btrfs_header_nritems(&src->header);
+	dst_nritems = btrfs_header_nritems(&dst->header);
 	push_items = NODEPTRS_PER_BLOCK - dst_nritems;
 	if (push_items <= 0) {
 		return 1;
@@ -544,9 +555,8 @@ static int push_node_left(struct ctree_root *root, struct tree_buffer *dst_buf,
 		memmove(src->blockptrs, src->blockptrs + push_items,
 			(src_nritems - push_items) * sizeof(u64));
 	}
-	src->header.nritems -= push_items;
-	dst->header.nritems += push_items;
-
+	btrfs_set_header_nritems(&src->header, src_nritems - push_items);
+	btrfs_set_header_nritems(&dst->header, dst_nritems + push_items);
 	BUG_ON(list_empty(&src_buf->dirty));
 	BUG_ON(list_empty(&dst_buf->dirty));
 	return ret;
@@ -573,8 +583,8 @@ static int balance_node_right(struct ctree_root *root,
 	int dst_nritems;
 	int ret = 0;
 
-	src_nritems = src->header.nritems;
-	dst_nritems = dst->header.nritems;
+	src_nritems = btrfs_header_nritems(&src->header);
+	dst_nritems = btrfs_header_nritems(&dst->header);
 	push_items = NODEPTRS_PER_BLOCK - dst_nritems;
 	if (push_items <= 0) {
 		return 1;
@@ -596,8 +606,8 @@ static int balance_node_right(struct ctree_root *root,
 	memcpy(dst->blockptrs, src->blockptrs + src_nritems - push_items,
 		push_items * sizeof(u64));
 
-	src->header.nritems -= push_items;
-	dst->header.nritems += push_items;
+	btrfs_set_header_nritems(&src->header, src_nritems - push_items);
+	btrfs_set_header_nritems(&dst->header, dst_nritems + push_items);
 
 	BUG_ON(list_empty(&src_buf->dirty));
 	BUG_ON(list_empty(&dst_buf->dirty));
@@ -625,12 +635,13 @@ static int insert_new_root(struct ctree_root *root,
 	t = alloc_free_block(root);
 	c = &t->node;
 	memset(c, 0, sizeof(c));
-	c->header.nritems = 1;
-	c->header.flags = node_level(level);
-	c->header.blocknr = t->blocknr;
-	c->header.parentid = root->node->node.header.parentid;
+	btrfs_set_header_nritems(&c->header, 1);
+	btrfs_set_header_level(&c->header, level);
+	btrfs_set_header_blocknr(&c->header, t->blocknr);
+	btrfs_set_header_parentid(&c->header,
+	                       btrfs_header_parentid(&root->node->node.header));
 	lower = &path->nodes[level-1]->node;
-	if (is_leaf(lower->header.flags))
+	if (btrfs_is_leaf(lower))
 		lower_key = &((struct leaf *)lower)->items[0].key;
 	else
 		lower_key = lower->keys;
@@ -663,7 +674,7 @@ static int insert_ptr(struct ctree_root *root,
 
 	BUG_ON(!path->nodes[level]);
 	lower = &path->nodes[level]->node;
-	nritems = lower->header.nritems;
+	nritems = btrfs_header_nritems(&lower->header);
 	if (slot > nritems)
 		BUG();
 	if (nritems == NODEPTRS_PER_BLOCK)
@@ -676,7 +687,7 @@ static int insert_ptr(struct ctree_root *root,
 	}
 	memcpy(lower->keys + slot, key, sizeof(struct key));
 	lower->blockptrs[slot] = blocknr;
-	lower->header.nritems++;
+	btrfs_set_header_nritems(&lower->header, nritems + 1);
 	if (lower->keys[1].objectid == 0)
 			BUG();
 	BUG_ON(list_empty(&path->nodes[level]->dirty));
@@ -702,6 +713,7 @@ static int split_node(struct ctree_root *root, struct ctree_path *path,
 	int mid;
 	int ret;
 	int wret;
+	u32 c_nritems;
 
 	t = path->nodes[level];
 	c = &t->node;
@@ -711,18 +723,20 @@ static int split_node(struct ctree_root *root, struct ctree_path *path,
 		if (ret)
 			return ret;
 	}
+	c_nritems = btrfs_header_nritems(&c->header);
 	split_buffer = alloc_free_block(root);
 	split = &split_buffer->node;
-	split->header.flags = c->header.flags;
-	split->header.blocknr = split_buffer->blocknr;
-	split->header.parentid = root->node->node.header.parentid;
-	mid = (c->header.nritems + 1) / 2;
+	btrfs_set_header_flags(&split->header, btrfs_header_flags(&c->header));
+	btrfs_set_header_blocknr(&split->header, split_buffer->blocknr);
+	btrfs_set_header_parentid(&split->header,
+	                       btrfs_header_parentid(&root->node->node.header));
+	mid = (c_nritems + 1) / 2;
 	memcpy(split->keys, c->keys + mid,
-		(c->header.nritems - mid) * sizeof(struct key));
+		(c_nritems - mid) * sizeof(struct key));
 	memcpy(split->blockptrs, c->blockptrs + mid,
-		(c->header.nritems - mid) * sizeof(u64));
-	split->header.nritems = c->header.nritems - mid;
-	c->header.nritems = mid;
+		(c_nritems - mid) * sizeof(u64));
+	btrfs_set_header_nritems(&split->header, c_nritems - mid);
+	btrfs_set_header_nritems(&c->header, mid);
 	ret = 0;
 
 	BUG_ON(list_empty(&t->dirty));
@@ -781,13 +795,15 @@ static int push_leaf_right(struct ctree_root *root, struct ctree_path *path,
 	int push_space = 0;
 	int push_items = 0;
 	struct item *item;
+	u32 left_nritems;
+	u32 right_nritems;
 
 	slot = path->slots[1];
 	if (!path->nodes[1]) {
 		return 1;
 	}
 	upper = path->nodes[1];
-	if (slot >= upper->node.header.nritems - 1) {
+	if (slot >= btrfs_header_nritems(&upper->node.header) - 1) {
 		return 1;
 	}
 	right_buf = read_tree_block(root, upper->node.blockptrs[slot + 1]);
@@ -806,7 +822,8 @@ static int push_leaf_right(struct ctree_root *root, struct ctree_path *path,
 		return 1;
 	}
 
-	for (i = left->header.nritems - 1; i >= 0; i--) {
+	left_nritems = btrfs_header_nritems(&left->header);
+	for (i = left_nritems - 1; i >= 0; i--) {
 		item = left->items + i;
 		if (path->slots[0] == i)
 			push_space += data_size + sizeof(*item);
@@ -819,9 +836,10 @@ static int push_leaf_right(struct ctree_root *root, struct ctree_path *path,
 		tree_block_release(root, right_buf);
 		return 1;
 	}
+	right_nritems = btrfs_header_nritems(&right->header);
 	/* push left to right */
-	push_space = left->items[left->header.nritems - push_items].offset +
-		     left->items[left->header.nritems - push_items].size;
+	push_space = left->items[left_nritems - push_items].offset +
+		     left->items[left_nritems - push_items].size;
 	push_space -= leaf_data_end(left);
 	/* make room in the right data area */
 	memmove(right->data + leaf_data_end(right) - push_space,
@@ -832,19 +850,21 @@ static int push_leaf_right(struct ctree_root *root, struct ctree_path *path,
 		left->data + leaf_data_end(left),
 		push_space);
 	memmove(right->items + push_items, right->items,
-		right->header.nritems * sizeof(struct item));
+		right_nritems * sizeof(struct item));
 	/* copy the items from left to right */
-	memcpy(right->items, left->items + left->header.nritems - push_items,
+	memcpy(right->items, left->items + left_nritems - push_items,
 		push_items * sizeof(struct item));
 
 	/* update the item pointers */
-	right->header.nritems += push_items;
+	right_nritems += push_items;
+	btrfs_set_header_nritems(&right->header, right_nritems);
 	push_space = LEAF_DATA_SIZE;
-	for (i = 0; i < right->header.nritems; i++) {
+	for (i = 0; i < right_nritems; i++) {
 		right->items[i].offset = push_space - right->items[i].size;
 		push_space = right->items[i].offset;
 	}
-	left->header.nritems -= push_items;
+	left_nritems -= push_items;
+	btrfs_set_header_nritems(&left->header, left_nritems);
 
 	BUG_ON(list_empty(&left_buf->dirty));
 	BUG_ON(list_empty(&right_buf->dirty));
@@ -853,8 +873,8 @@ static int push_leaf_right(struct ctree_root *root, struct ctree_path *path,
 	BUG_ON(list_empty(&upper->dirty));
 
 	/* then fixup the leaf pointer in the path */
-	if (path->slots[0] >= left->header.nritems) {
-		path->slots[0] -= left->header.nritems;
+	if (path->slots[0] >= left_nritems) {
+		path->slots[0] -= left_nritems;
 		tree_block_release(root, path->nodes[0]);
 		path->nodes[0] = right_buf;
 		path->slots[1] += 1;
@@ -880,7 +900,7 @@ static int push_leaf_left(struct ctree_root *root, struct ctree_path *path,
 	int push_space = 0;
 	int push_items = 0;
 	struct item *item;
-	int old_left_nritems;
+	u32 old_left_nritems;
 	int ret = 0;
 	int wret;
 
@@ -908,7 +928,7 @@ static int push_leaf_left(struct ctree_root *root, struct ctree_path *path,
 		return 1;
 	}
 
-	for (i = 0; i < right->header.nritems; i++) {
+	for (i = 0; i < btrfs_header_nritems(&right->header); i++) {
 		item = right->items + i;
 		if (path->slots[0] == i)
 			push_space += data_size + sizeof(*item);
@@ -922,31 +942,34 @@ static int push_leaf_left(struct ctree_root *root, struct ctree_path *path,
 		return 1;
 	}
 	/* push data from right to left */
-	memcpy(left->items + left->header.nritems,
+	memcpy(left->items + btrfs_header_nritems(&left->header),
 		right->items, push_items * sizeof(struct item));
 	push_space = LEAF_DATA_SIZE - right->items[push_items -1].offset;
 	memcpy(left->data + leaf_data_end(left) - push_space,
 		right->data + right->items[push_items - 1].offset,
 		push_space);
-	old_left_nritems = left->header.nritems;
+	old_left_nritems = btrfs_header_nritems(&left->header);
 	BUG_ON(old_left_nritems < 0);
 
 	for(i = old_left_nritems; i < old_left_nritems + push_items; i++) {
 		left->items[i].offset -= LEAF_DATA_SIZE -
 			left->items[old_left_nritems -1].offset;
 	}
-	left->header.nritems += push_items;
+	btrfs_set_header_nritems(&left->header, old_left_nritems + push_items);
 
 	/* fixup right node */
 	push_space = right->items[push_items-1].offset - leaf_data_end(right);
 	memmove(right->data + LEAF_DATA_SIZE - push_space, right->data +
 		leaf_data_end(right), push_space);
 	memmove(right->items, right->items + push_items,
-		(right->header.nritems - push_items) * sizeof(struct item));
-	right->header.nritems -= push_items;
+		(btrfs_header_nritems(&right->header) - push_items) *
+		sizeof(struct item));
+	btrfs_set_header_nritems(&right->header,
+				 btrfs_header_nritems(&right->header) -
+				 push_items);
 	push_space = LEAF_DATA_SIZE;
 
-	for (i = 0; i < right->header.nritems; i++) {
+	for (i = 0; i < btrfs_header_nritems(&right->header); i++) {
 		right->items[i].offset = push_space - right->items[i].size;
 		push_space = right->items[i].offset;
 	}
@@ -983,7 +1006,7 @@ static int split_leaf(struct ctree_root *root, struct ctree_path *path,
 {
 	struct tree_buffer *l_buf;
 	struct leaf *l;
-	int nritems;
+	u32 nritems;
 	int mid;
 	int slot;
 	struct leaf *right;
@@ -1008,7 +1031,7 @@ static int split_leaf(struct ctree_root *root, struct ctree_path *path,
 			return ret;
 	}
 	slot = path->slots[0];
-	nritems = l->header.nritems;
+	nritems = btrfs_header_nritems(&l->header);
 	mid = (nritems + 1)/ 2;
 	right_buffer = alloc_free_block(root);
 	BUG_ON(!right_buffer);
@@ -1026,10 +1049,11 @@ static int split_leaf(struct ctree_root *root, struct ctree_path *path,
 			LEAF_DATA_SIZE)
 			BUG();
 	}
-	right->header.nritems = nritems - mid;
-	right->header.blocknr = right_buffer->blocknr;
-	right->header.flags = node_level(0);
-	right->header.parentid = root->node->node.header.parentid;
+	btrfs_set_header_nritems(&right->header, nritems - mid);
+	btrfs_set_header_blocknr(&right->header, right_buffer->blocknr);
+	btrfs_set_header_level(&right->header, 0);
+	btrfs_set_header_parentid(&right->header,
+	                       btrfs_header_parentid(&root->node->node.header));
 	data_copy_size = l->items[mid].offset + l->items[mid].size -
 			 leaf_data_end(l);
 	memcpy(right->items, l->items + mid,
@@ -1039,10 +1063,10 @@ static int split_leaf(struct ctree_root *root, struct ctree_path *path,
 	rt_data_off = LEAF_DATA_SIZE -
 		     (l->items[mid].offset + l->items[mid].size);
 
-	for (i = 0; i < right->header.nritems; i++)
+	for (i = 0; i < btrfs_header_nritems(&right->header); i++)
 		right->items[i].offset += rt_data_off;
 
-	l->header.nritems = mid;
+	btrfs_set_header_nritems(&l->header, mid);
 	ret = 0;
 	wret = insert_ptr(root, path, &right->items[0].key,
 			  right_buffer->blocknr, path->slots[1] + 1, 1);
@@ -1074,7 +1098,7 @@ int insert_item(struct ctree_root *root, struct key *key,
 	int slot_orig;
 	struct leaf *leaf;
 	struct tree_buffer *leaf_buf;
-	unsigned int nritems;
+	u32 nritems;
 	unsigned int data_end;
 	struct ctree_path path;
 
@@ -1094,7 +1118,7 @@ int insert_item(struct ctree_root *root, struct key *key,
 	leaf_buf = path.nodes[0];
 	leaf = &leaf_buf->leaf;
 
-	nritems = leaf->header.nritems;
+	nritems = btrfs_header_nritems(&leaf->header);
 	data_end = leaf_data_end(leaf);
 
 	if (leaf_free_space(leaf) <  sizeof(struct item) + data_size)
@@ -1128,7 +1152,7 @@ int insert_item(struct ctree_root *root, struct key *key,
 	leaf->items[slot].offset = data_end - data_size;
 	leaf->items[slot].size = data_size;
 	memcpy(leaf->data + data_end - data_size, data, data_size);
-	leaf->header.nritems += 1;
+	btrfs_set_header_nritems(&leaf->header, nritems + 1);
 
 	ret = 0;
 	if (slot == 0)
@@ -1155,12 +1179,12 @@ static int del_ptr(struct ctree_root *root, struct ctree_path *path, int level,
 {
 	struct node *node;
 	struct tree_buffer *parent = path->nodes[level];
-	int nritems;
+	u32 nritems;
 	int ret = 0;
 	int wret;
 
 	node = &parent->node;
-	nritems = node->header.nritems;
+	nritems = btrfs_header_nritems(&node->header);
 	if (slot != nritems -1) {
 		memmove(node->keys + slot, node->keys + slot + 1,
 			sizeof(struct key) * (nritems - slot - 1));
@@ -1168,11 +1192,12 @@ static int del_ptr(struct ctree_root *root, struct ctree_path *path, int level,
 			node->blockptrs + slot + 1,
 			sizeof(u64) * (nritems - slot - 1));
 	}
-	node->header.nritems--;
-	if (node->header.nritems == 0 && parent == root->node) {
-		BUG_ON(node_level(root->node->node.header.flags) != 1);
+	nritems--;
+	btrfs_set_header_nritems(&node->header, nritems);
+	if (nritems == 0 && parent == root->node) {
+		BUG_ON(btrfs_header_level(&root->node->node.header) != 1);
 		/* just turn the root into a leaf and break */
-		root->node->node.header.flags = node_level(0);
+		btrfs_set_header_level(&root->node->node.header, 0);
 	} else if (slot == 0) {
 		wret = fixup_low_keys(root, path, node->keys, level + 1);
 		if (wret)
@@ -1195,30 +1220,33 @@ int del_item(struct ctree_root *root, struct ctree_path *path)
 	int dsize;
 	int ret = 0;
 	int wret;
+	u32 nritems;
 
 	leaf_buf = path->nodes[0];
 	leaf = &leaf_buf->leaf;
 	slot = path->slots[0];
 	doff = leaf->items[slot].offset;
 	dsize = leaf->items[slot].size;
+	nritems = btrfs_header_nritems(&leaf->header);
 
-	if (slot != leaf->header.nritems - 1) {
+	if (slot != nritems - 1) {
 		int i;
 		int data_end = leaf_data_end(leaf);
 		memmove(leaf->data + data_end + dsize,
 			leaf->data + data_end,
 			doff - data_end);
-		for (i = slot + 1; i < leaf->header.nritems; i++)
+		for (i = slot + 1; i < nritems; i++)
 			leaf->items[i].offset += dsize;
 		memmove(leaf->items + slot, leaf->items + slot + 1,
 			sizeof(struct item) *
-			(leaf->header.nritems - slot - 1));
+			(nritems - slot - 1));
 	}
-	leaf->header.nritems -= 1;
+	btrfs_set_header_nritems(&leaf->header, nritems - 1);
+	nritems--;
 	/* delete the leaf if we've emptied it */
-	if (leaf->header.nritems == 0) {
+	if (nritems == 0) {
 		if (leaf_buf == root->node) {
-			leaf->header.flags = node_level(0);
+			btrfs_set_header_level(&leaf->header, 0);
 			BUG_ON(list_empty(&leaf_buf->dirty));
 		} else {
 			clean_tree_block(root, leaf_buf);
@@ -1230,7 +1258,7 @@ int del_item(struct ctree_root *root, struct ctree_path *path)
 				ret = wret;
 		}
 	} else {
-		int used = leaf_space_used(leaf, 0, leaf->header.nritems);
+		int used = leaf_space_used(leaf, 0, nritems);
 		if (slot == 0) {
 			wret = fixup_low_keys(root, path,
 						   &leaf->items[0].key, 1);
@@ -1251,12 +1279,12 @@ int del_item(struct ctree_root *root, struct ctree_path *path)
 			if (wret < 0)
 				ret = wret;
 			if (path->nodes[0] == leaf_buf &&
-			    leaf->header.nritems) {
+			    btrfs_header_nritems(&leaf->header)) {
 				wret = push_leaf_right(root, path, 1);
 				if (wret < 0)
 					ret = wret;
 			}
-			if (leaf->header.nritems == 0) {
+			if (btrfs_header_nritems(&leaf->header) == 0) {
 				u64 blocknr = leaf_buf->blocknr;
 				clean_tree_block(root, leaf_buf);
 				wret = del_ptr(root, path, 1, slot);
@@ -1292,7 +1320,7 @@ int next_leaf(struct ctree_root *root, struct ctree_path *path)
 			return 1;
 		slot = path->slots[level] + 1;
 		c = path->nodes[level];
-		if (slot >= c->node.header.nritems) {
+		if (slot >= btrfs_header_nritems(&c->node.header)) {
 			level++;
 			continue;
 		}

commit 037e6390488af8ab96137e1e5cccc15ad14ef887
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Mar 7 11:50:24 2007 -0500

    Btrfs: get rid of add recursion
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 72816381d203..729d4ddb3746 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -995,15 +995,6 @@ static int split_leaf(struct ctree_root *root, struct ctree_path *path,
 	int ret;
 	int wret;
 
-	wret = push_leaf_left(root, path, data_size);
-	if (wret < 0)
-		return wret;
-	if (wret) {
-		wret = push_leaf_right(root, path, data_size);
-		if (wret < 0)
-			return wret;
-	}
-
 	l_buf = path->nodes[0];
 	l = &l_buf->leaf;
 

commit a28ec19775d62d673b034082128aca95780d3737
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Mar 6 20:08:01 2007 -0500

    Btrfs: Fixup reference counting on cows
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 1118986d7569..72816381d203 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -50,16 +50,17 @@ int btrfs_cow_block(struct ctree_root *root,
 	memcpy(&cow->node, &buf->node, sizeof(buf->node));
 	cow->node.header.blocknr = cow->blocknr;
 	*cow_ret = cow;
+	btrfs_inc_ref(root, buf);
 	if (buf == root->node) {
 		root->node = cow;
 		cow->count++;
+		if (buf != root->commit_root)
+			free_extent(root, buf->blocknr, 1);
 		tree_block_release(root, buf);
 	} else {
 		parent->node.blockptrs[parent_slot] = cow->blocknr;
 		BUG_ON(list_empty(&parent->dirty));
-	}
-	if (0 && root != root->extent_root && !is_leaf(cow->node.header.flags)) {
-		btrfs_inc_ref(root, cow);
+		free_extent(root, buf->blocknr, 1);
 	}
 	tree_block_release(root, buf);
 	return 0;
@@ -1018,7 +1019,6 @@ static int split_leaf(struct ctree_root *root, struct ctree_path *path,
 	slot = path->slots[0];
 	nritems = l->header.nritems;
 	mid = (nritems + 1)/ 2;
-
 	right_buffer = alloc_free_block(root);
 	BUG_ON(!right_buffer);
 	BUG_ON(mid == nritems);
@@ -1170,7 +1170,6 @@ static int del_ptr(struct ctree_root *root, struct ctree_path *path, int level,
 
 	node = &parent->node;
 	nritems = node->header.nritems;
-
 	if (slot != nritems -1) {
 		memmove(node->keys + slot, node->keys + slot + 1,
 			sizeof(struct key) * (nritems - slot - 1));

commit 02217ed299c6340a35696e0610047eb96826de2d
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Mar 2 16:08:05 2007 -0500

    Btrfs: early reference counting
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 3c5f4c2dd525..1118986d7569 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -34,6 +34,37 @@ void release_path(struct ctree_root *root, struct ctree_path *p)
 	memset(p, 0, sizeof(*p));
 }
 
+int btrfs_cow_block(struct ctree_root *root,
+		    struct tree_buffer *buf,
+		    struct tree_buffer *parent,
+		    int parent_slot,
+		    struct tree_buffer **cow_ret)
+{
+	struct tree_buffer *cow;
+
+	if (!list_empty(&buf->dirty)) {
+		*cow_ret = buf;
+		return 0;
+	}
+	cow = alloc_free_block(root);
+	memcpy(&cow->node, &buf->node, sizeof(buf->node));
+	cow->node.header.blocknr = cow->blocknr;
+	*cow_ret = cow;
+	if (buf == root->node) {
+		root->node = cow;
+		cow->count++;
+		tree_block_release(root, buf);
+	} else {
+		parent->node.blockptrs[parent_slot] = cow->blocknr;
+		BUG_ON(list_empty(&parent->dirty));
+	}
+	if (0 && root != root->extent_root && !is_leaf(cow->node.header.flags)) {
+		btrfs_inc_ref(root, cow);
+	}
+	tree_block_release(root, buf);
+	return 0;
+}
+
 /*
  * The leaf data grows from end-to-front in the node.
  * this returns the address of the start of the last item,
@@ -263,6 +294,8 @@ static int balance_level(struct ctree_root *root, struct ctree_path *path,
 
 	/* first, try to make some room in the middle buffer */
 	if (left_buf) {
+		btrfs_cow_block(root, left_buf, parent_buf,
+				pslot - 1, &left_buf);
 		left = &left_buf->node;
 		orig_slot += left->header.nritems;
 		wret = push_node_left(root, left_buf, mid_buf);
@@ -274,6 +307,8 @@ static int balance_level(struct ctree_root *root, struct ctree_path *path,
 	 * then try to empty the right most buffer into the middle
 	 */
 	if (right_buf) {
+		btrfs_cow_block(root, right_buf, parent_buf,
+				pslot + 1, &right_buf);
 		right = &right_buf->node;
 		wret = push_node_left(root, mid_buf, right_buf);
 		if (wret < 0)
@@ -293,9 +328,7 @@ static int balance_level(struct ctree_root *root, struct ctree_path *path,
 		} else {
 			memcpy(parent->keys + pslot + 1, right->keys,
 				sizeof(struct key));
-			wret = dirty_tree_block(root, parent_buf);
-			if (wret)
-				ret = wret;
+			BUG_ON(list_empty(&parent_buf->dirty));
 		}
 	}
 	if (mid->header.nritems == 1) {
@@ -330,9 +363,7 @@ static int balance_level(struct ctree_root *root, struct ctree_path *path,
 	} else {
 		/* update the parent key to reflect our changes */
 		memcpy(parent->keys + pslot, mid->keys, sizeof(struct key));
-		wret = dirty_tree_block(root, parent_buf);
-		if (wret)
-			ret = wret;
+		BUG_ON(list_empty(&parent_buf->dirty));
 	}
 
 	/* update the path */
@@ -375,9 +406,10 @@ static int balance_level(struct ctree_root *root, struct ctree_path *path,
  * possible)
  */
 int search_slot(struct ctree_root *root, struct key *key,
-		struct ctree_path *p, int ins_len)
+		struct ctree_path *p, int ins_len, int cow)
 {
 	struct tree_buffer *b;
+	struct tree_buffer *cow_buf;
 	struct node *c;
 	int slot;
 	int ret;
@@ -387,8 +419,15 @@ int search_slot(struct ctree_root *root, struct key *key,
 	b = root->node;
 	b->count++;
 	while (b) {
+		level = node_level(b->node.header.flags);
+		if (cow) {
+			int wret;
+			wret = btrfs_cow_block(root, b, p->nodes[level + 1],
+					       p->slots[level + 1], &cow_buf);
+			b = cow_buf;
+		}
+		BUG_ON(!cow && ins_len);
 		c = &b->node;
-		level = node_level(c->header.flags);
 		p->nodes[level] = b;
 		ret = check_block(p, level);
 		if (ret)
@@ -453,7 +492,6 @@ static int fixup_low_keys(struct ctree_root *root,
 {
 	int i;
 	int ret = 0;
-	int wret;
 	for (i = level; i < MAX_LEVEL; i++) {
 		struct node *t;
 		int tslot = path->slots[i];
@@ -461,9 +499,7 @@ static int fixup_low_keys(struct ctree_root *root,
 			break;
 		t = &path->nodes[i]->node;
 		memcpy(t->keys + tslot, key, sizeof(*key));
-		wret = dirty_tree_block(root, path->nodes[i]);
-		if (wret)
-			ret = wret;
+		BUG_ON(list_empty(&path->nodes[i]->dirty));
 		if (tslot != 0)
 			break;
 	}
@@ -486,7 +522,6 @@ static int push_node_left(struct ctree_root *root, struct tree_buffer *dst_buf,
 	int src_nritems;
 	int dst_nritems;
 	int ret = 0;
-	int wret;
 
 	src_nritems = src->header.nritems;
 	dst_nritems = dst->header.nritems;
@@ -511,13 +546,8 @@ static int push_node_left(struct ctree_root *root, struct tree_buffer *dst_buf,
 	src->header.nritems -= push_items;
 	dst->header.nritems += push_items;
 
-	wret = dirty_tree_block(root, src_buf);
-	if (wret < 0)
-		ret = wret;
-
-	wret = dirty_tree_block(root, dst_buf);
-	if (wret < 0)
-		ret = wret;
+	BUG_ON(list_empty(&src_buf->dirty));
+	BUG_ON(list_empty(&dst_buf->dirty));
 	return ret;
 }
 
@@ -541,7 +571,6 @@ static int balance_node_right(struct ctree_root *root,
 	int src_nritems;
 	int dst_nritems;
 	int ret = 0;
-	int wret;
 
 	src_nritems = src->header.nritems;
 	dst_nritems = dst->header.nritems;
@@ -569,13 +598,8 @@ static int balance_node_right(struct ctree_root *root,
 	src->header.nritems -= push_items;
 	dst->header.nritems += push_items;
 
-	wret = dirty_tree_block(root, src_buf);
-	if (wret < 0)
-		ret = wret;
-
-	wret = dirty_tree_block(root, dst_buf);
-	if (wret < 0)
-		ret = wret;
+	BUG_ON(list_empty(&src_buf->dirty));
+	BUG_ON(list_empty(&dst_buf->dirty));
 	return ret;
 }
 
@@ -615,7 +639,6 @@ static int insert_new_root(struct ctree_root *root,
 	tree_block_release(root, root->node);
 	root->node = t;
 	t->count++;
-	dirty_tree_block(root, t);
 	path->nodes[level] = t;
 	path->slots[level] = 0;
 	return 0;
@@ -655,7 +678,7 @@ static int insert_ptr(struct ctree_root *root,
 	lower->header.nritems++;
 	if (lower->keys[1].objectid == 0)
 			BUG();
-	dirty_tree_block(root, path->nodes[level]);
+	BUG_ON(list_empty(&path->nodes[level]->dirty));
 	return 0;
 }
 
@@ -701,12 +724,7 @@ static int split_node(struct ctree_root *root, struct ctree_path *path,
 	c->header.nritems = mid;
 	ret = 0;
 
-	wret = dirty_tree_block(root, t);
-	if (wret)
-		ret = wret;
-	wret = dirty_tree_block(root, split_buffer);
-	if (wret)
-		ret = wret;
+	BUG_ON(list_empty(&t->dirty));
 	wret = insert_ptr(root, path, split->keys, split_buffer->blocknr,
 			  path->slots[level + 1] + 1, level + 1);
 	if (wret)
@@ -778,6 +796,15 @@ static int push_leaf_right(struct ctree_root *root, struct ctree_path *path,
 		tree_block_release(root, right_buf);
 		return 1;
 	}
+	/* cow and double check */
+	btrfs_cow_block(root, right_buf, upper, slot + 1, &right_buf);
+	right = &right_buf->leaf;
+	free_space = leaf_free_space(right);
+	if (free_space < data_size + sizeof(struct item)) {
+		tree_block_release(root, right_buf);
+		return 1;
+	}
+
 	for (i = left->header.nritems - 1; i >= 0; i--) {
 		item = left->items + i;
 		if (path->slots[0] == i)
@@ -818,11 +845,12 @@ static int push_leaf_right(struct ctree_root *root, struct ctree_path *path,
 	}
 	left->header.nritems -= push_items;
 
-	dirty_tree_block(root, left_buf);
-	dirty_tree_block(root, right_buf);
+	BUG_ON(list_empty(&left_buf->dirty));
+	BUG_ON(list_empty(&right_buf->dirty));
 	memcpy(upper->node.keys + slot + 1,
 		&right->items[0].key, sizeof(struct key));
-	dirty_tree_block(root, upper);
+	BUG_ON(list_empty(&upper->dirty));
+
 	/* then fixup the leaf pointer in the path */
 	if (path->slots[0] >= left->header.nritems) {
 		path->slots[0] -= left->header.nritems;
@@ -869,6 +897,16 @@ static int push_leaf_left(struct ctree_root *root, struct ctree_path *path,
 		tree_block_release(root, t);
 		return 1;
 	}
+
+	/* cow and double check */
+	btrfs_cow_block(root, t, path->nodes[1], slot - 1, &t);
+	left = &t->leaf;
+	free_space = leaf_free_space(left);
+	if (free_space < data_size + sizeof(struct item)) {
+		tree_block_release(root, t);
+		return 1;
+	}
+
 	for (i = 0; i < right->header.nritems; i++) {
 		item = right->items + i;
 		if (path->slots[0] == i)
@@ -912,12 +950,8 @@ static int push_leaf_left(struct ctree_root *root, struct ctree_path *path,
 		push_space = right->items[i].offset;
 	}
 
-	wret = dirty_tree_block(root, t);
-	if (wret)
-		ret = wret;
-	wret = dirty_tree_block(root, right_buf);
-	if (wret)
-		ret = wret;
+	BUG_ON(list_empty(&t->dirty));
+	BUG_ON(list_empty(&right_buf->dirty));
 
 	wret = fixup_low_keys(root, path, &right->items[0].key, 1);
 	if (wret)
@@ -968,6 +1002,7 @@ static int split_leaf(struct ctree_root *root, struct ctree_path *path,
 		if (wret < 0)
 			return wret;
 	}
+
 	l_buf = path->nodes[0];
 	l = &l_buf->leaf;
 
@@ -1022,13 +1057,8 @@ static int split_leaf(struct ctree_root *root, struct ctree_path *path,
 			  right_buffer->blocknr, path->slots[1] + 1, 1);
 	if (wret)
 		ret = wret;
-	wret = dirty_tree_block(root, right_buffer);
-	if (wret)
-		ret = wret;
-	wret = dirty_tree_block(root, l_buf);
-	if (wret)
-		ret = wret;
-
+	BUG_ON(list_empty(&right_buffer->dirty));
+	BUG_ON(list_empty(&l_buf->dirty));
 	BUG_ON(path->slots[0] != slot);
 	if (mid <= slot) {
 		tree_block_release(root, path->nodes[0]);
@@ -1049,7 +1079,6 @@ int insert_item(struct ctree_root *root, struct key *key,
 			  void *data, int data_size)
 {
 	int ret = 0;
-	int wret;
 	int slot;
 	int slot_orig;
 	struct leaf *leaf;
@@ -1062,7 +1091,7 @@ int insert_item(struct ctree_root *root, struct key *key,
 	if (!root->node)
 		BUG();
 	init_path(&path);
-	ret = search_slot(root, key, &path, data_size);
+	ret = search_slot(root, key, &path, data_size, 1);
 	if (ret == 0) {
 		release_path(root, &path);
 		return -EEXIST;
@@ -1114,10 +1143,7 @@ int insert_item(struct ctree_root *root, struct key *key,
 	if (slot == 0)
 		ret = fixup_low_keys(root, &path, key, 1);
 
-	wret = dirty_tree_block(root, leaf_buf);
-	if (wret)
-		ret = wret;
-
+	BUG_ON(list_empty(&leaf_buf->dirty));
 	if (leaf_free_space(leaf) < 0)
 		BUG();
 	check_leaf(&path, 0);
@@ -1162,9 +1188,7 @@ static int del_ptr(struct ctree_root *root, struct ctree_path *path, int level,
 		if (wret)
 			ret = wret;
 	}
-	wret = dirty_tree_block(root, parent);
-	if (wret)
-		ret = wret;
+	BUG_ON(list_empty(&parent->dirty));
 	return ret;
 }
 
@@ -1205,7 +1229,7 @@ int del_item(struct ctree_root *root, struct ctree_path *path)
 	if (leaf->header.nritems == 0) {
 		if (leaf_buf == root->node) {
 			leaf->header.flags = node_level(0);
-			dirty_tree_block(root, leaf_buf);
+			BUG_ON(list_empty(&leaf_buf->dirty));
 		} else {
 			clean_tree_block(root, leaf_buf);
 			wret = del_ptr(root, path, 1, path->slots[1]);
@@ -1223,9 +1247,7 @@ int del_item(struct ctree_root *root, struct ctree_path *path)
 			if (wret)
 				ret = wret;
 		}
-		wret = dirty_tree_block(root, leaf_buf);
-		if (wret)
-			ret = wret;
+		BUG_ON(list_empty(&leaf_buf->dirty));
 
 		/* delete the leaf if it is mostly empty */
 		if (used < LEAF_DATA_SIZE / 3) {
@@ -1304,3 +1326,4 @@ int next_leaf(struct ctree_root *root, struct ctree_path *path)
 	return 0;
 }
 
+

commit f0930a37f1c096c3a8f6a17b1e251c7fdf4d4457
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Mar 2 09:47:58 2007 -0500

    Btrfs: Fix extent code to use merge during delete
    Remove implicit commit in del_item and insert_item
    Add implicit commit to close()
    Add commit op in random-test
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 57fa505fb9f6..3c5f4c2dd525 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1065,11 +1065,7 @@ int insert_item(struct ctree_root *root, struct key *key,
 	ret = search_slot(root, key, &path, data_size);
 	if (ret == 0) {
 		release_path(root, &path);
-		ret = -EEXIST;
-		wret = commit_transaction(root);
-		if (wret)
-			ret = wret;
-		return ret;
+		return -EEXIST;
 	}
 	if (ret < 0)
 		goto out;
@@ -1127,9 +1123,6 @@ int insert_item(struct ctree_root *root, struct key *key,
 	check_leaf(&path, 0);
 out:
 	release_path(root, &path);
-	wret = commit_transaction(root);
-	if (wret)
-		ret = wret;
 	return ret;
 }
 
@@ -1245,7 +1238,8 @@ int del_item(struct ctree_root *root, struct ctree_path *path)
 			wret = push_leaf_left(root, path, 1);
 			if (wret < 0)
 				ret = wret;
-			if (leaf->header.nritems) {
+			if (path->nodes[0] == leaf_buf &&
+			    leaf->header.nritems) {
 				wret = push_leaf_right(root, path, 1);
 				if (wret < 0)
 					ret = wret;
@@ -1265,9 +1259,6 @@ int del_item(struct ctree_root *root, struct ctree_path *path)
 			}
 		}
 	}
-	wret = commit_transaction(root);
-	if (wret)
-		ret = wret;
 	return ret;
 }
 

commit ed2ff2cba766dfe7976a0113f667c9a0a50dff02
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Mar 1 18:59:40 2007 -0500

    Btrfs: pretend page cache & commit code
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index afa5bc5c7c1a..57fa505fb9f6 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -250,6 +250,7 @@ static int balance_level(struct ctree_root *root, struct ctree_path *path,
 		tree_block_release(root, mid_buf);
 		/* once for the root ptr */
 		tree_block_release(root, mid_buf);
+		clean_tree_block(root, mid_buf);
 		return free_extent(root, blocknr, 1);
 	}
 	parent = &parent_buf->node;
@@ -280,6 +281,7 @@ static int balance_level(struct ctree_root *root, struct ctree_path *path,
 		if (right->header.nritems == 0) {
 			u64 blocknr = right_buf->blocknr;
 			tree_block_release(root, right_buf);
+			clean_tree_block(root, right_buf);
 			right_buf = NULL;
 			right = NULL;
 			wret = del_ptr(root, path, level + 1, pslot + 1);
@@ -291,7 +293,7 @@ static int balance_level(struct ctree_root *root, struct ctree_path *path,
 		} else {
 			memcpy(parent->keys + pslot + 1, right->keys,
 				sizeof(struct key));
-			wret = write_tree_block(root, parent_buf);
+			wret = dirty_tree_block(root, parent_buf);
 			if (wret)
 				ret = wret;
 		}
@@ -316,6 +318,7 @@ static int balance_level(struct ctree_root *root, struct ctree_path *path,
 		/* we've managed to empty the middle node, drop it */
 		u64 blocknr = mid_buf->blocknr;
 		tree_block_release(root, mid_buf);
+		clean_tree_block(root, mid_buf);
 		mid_buf = NULL;
 		mid = NULL;
 		wret = del_ptr(root, path, level + 1, pslot);
@@ -327,7 +330,7 @@ static int balance_level(struct ctree_root *root, struct ctree_path *path,
 	} else {
 		/* update the parent key to reflect our changes */
 		memcpy(parent->keys + pslot, mid->keys, sizeof(struct key));
-		wret = write_tree_block(root, parent_buf);
+		wret = dirty_tree_block(root, parent_buf);
 		if (wret)
 			ret = wret;
 	}
@@ -458,7 +461,7 @@ static int fixup_low_keys(struct ctree_root *root,
 			break;
 		t = &path->nodes[i]->node;
 		memcpy(t->keys + tslot, key, sizeof(*key));
-		wret = write_tree_block(root, path->nodes[i]);
+		wret = dirty_tree_block(root, path->nodes[i]);
 		if (wret)
 			ret = wret;
 		if (tslot != 0)
@@ -508,11 +511,11 @@ static int push_node_left(struct ctree_root *root, struct tree_buffer *dst_buf,
 	src->header.nritems -= push_items;
 	dst->header.nritems += push_items;
 
-	wret = write_tree_block(root, src_buf);
+	wret = dirty_tree_block(root, src_buf);
 	if (wret < 0)
 		ret = wret;
 
-	wret = write_tree_block(root, dst_buf);
+	wret = dirty_tree_block(root, dst_buf);
 	if (wret < 0)
 		ret = wret;
 	return ret;
@@ -566,11 +569,11 @@ static int balance_node_right(struct ctree_root *root,
 	src->header.nritems -= push_items;
 	dst->header.nritems += push_items;
 
-	wret = write_tree_block(root, src_buf);
+	wret = dirty_tree_block(root, src_buf);
 	if (wret < 0)
 		ret = wret;
 
-	wret = write_tree_block(root, dst_buf);
+	wret = dirty_tree_block(root, dst_buf);
 	if (wret < 0)
 		ret = wret;
 	return ret;
@@ -612,7 +615,7 @@ static int insert_new_root(struct ctree_root *root,
 	tree_block_release(root, root->node);
 	root->node = t;
 	t->count++;
-	write_tree_block(root, t);
+	dirty_tree_block(root, t);
 	path->nodes[level] = t;
 	path->slots[level] = 0;
 	return 0;
@@ -652,7 +655,7 @@ static int insert_ptr(struct ctree_root *root,
 	lower->header.nritems++;
 	if (lower->keys[1].objectid == 0)
 			BUG();
-	write_tree_block(root, path->nodes[level]);
+	dirty_tree_block(root, path->nodes[level]);
 	return 0;
 }
 
@@ -698,10 +701,10 @@ static int split_node(struct ctree_root *root, struct ctree_path *path,
 	c->header.nritems = mid;
 	ret = 0;
 
-	wret = write_tree_block(root, t);
+	wret = dirty_tree_block(root, t);
 	if (wret)
 		ret = wret;
-	wret = write_tree_block(root, split_buffer);
+	wret = dirty_tree_block(root, split_buffer);
 	if (wret)
 		ret = wret;
 	wret = insert_ptr(root, path, split->keys, split_buffer->blocknr,
@@ -815,11 +818,11 @@ static int push_leaf_right(struct ctree_root *root, struct ctree_path *path,
 	}
 	left->header.nritems -= push_items;
 
-	write_tree_block(root, left_buf);
-	write_tree_block(root, right_buf);
+	dirty_tree_block(root, left_buf);
+	dirty_tree_block(root, right_buf);
 	memcpy(upper->node.keys + slot + 1,
 		&right->items[0].key, sizeof(struct key));
-	write_tree_block(root, upper);
+	dirty_tree_block(root, upper);
 	/* then fixup the leaf pointer in the path */
 	if (path->slots[0] >= left->header.nritems) {
 		path->slots[0] -= left->header.nritems;
@@ -909,10 +912,10 @@ static int push_leaf_left(struct ctree_root *root, struct ctree_path *path,
 		push_space = right->items[i].offset;
 	}
 
-	wret = write_tree_block(root, t);
+	wret = dirty_tree_block(root, t);
 	if (wret)
 		ret = wret;
-	wret = write_tree_block(root, right_buf);
+	wret = dirty_tree_block(root, right_buf);
 	if (wret)
 		ret = wret;
 
@@ -1019,10 +1022,10 @@ static int split_leaf(struct ctree_root *root, struct ctree_path *path,
 			  right_buffer->blocknr, path->slots[1] + 1, 1);
 	if (wret)
 		ret = wret;
-	wret = write_tree_block(root, right_buffer);
+	wret = dirty_tree_block(root, right_buffer);
 	if (wret)
 		ret = wret;
-	wret = write_tree_block(root, l_buf);
+	wret = dirty_tree_block(root, l_buf);
 	if (wret)
 		ret = wret;
 
@@ -1062,12 +1065,14 @@ int insert_item(struct ctree_root *root, struct key *key,
 	ret = search_slot(root, key, &path, data_size);
 	if (ret == 0) {
 		release_path(root, &path);
-		return -EEXIST;
-	}
-	if (ret < 0) {
-		release_path(root, &path);
+		ret = -EEXIST;
+		wret = commit_transaction(root);
+		if (wret)
+			ret = wret;
 		return ret;
 	}
+	if (ret < 0)
+		goto out;
 
 	slot_orig = path.slots[0];
 	leaf_buf = path.nodes[0];
@@ -1113,14 +1118,18 @@ int insert_item(struct ctree_root *root, struct key *key,
 	if (slot == 0)
 		ret = fixup_low_keys(root, &path, key, 1);
 
-	wret = write_tree_block(root, leaf_buf);
+	wret = dirty_tree_block(root, leaf_buf);
 	if (wret)
 		ret = wret;
 
 	if (leaf_free_space(leaf) < 0)
 		BUG();
 	check_leaf(&path, 0);
+out:
 	release_path(root, &path);
+	wret = commit_transaction(root);
+	if (wret)
+		ret = wret;
 	return ret;
 }
 
@@ -1160,7 +1169,7 @@ static int del_ptr(struct ctree_root *root, struct ctree_path *path, int level,
 		if (wret)
 			ret = wret;
 	}
-	wret = write_tree_block(root, parent);
+	wret = dirty_tree_block(root, parent);
 	if (wret)
 		ret = wret;
 	return ret;
@@ -1203,8 +1212,9 @@ int del_item(struct ctree_root *root, struct ctree_path *path)
 	if (leaf->header.nritems == 0) {
 		if (leaf_buf == root->node) {
 			leaf->header.flags = node_level(0);
-			write_tree_block(root, leaf_buf);
+			dirty_tree_block(root, leaf_buf);
 		} else {
+			clean_tree_block(root, leaf_buf);
 			wret = del_ptr(root, path, 1, path->slots[1]);
 			if (wret)
 				ret = wret;
@@ -1220,7 +1230,7 @@ int del_item(struct ctree_root *root, struct ctree_path *path)
 			if (wret)
 				ret = wret;
 		}
-		wret = write_tree_block(root, leaf_buf);
+		wret = dirty_tree_block(root, leaf_buf);
 		if (wret)
 			ret = wret;
 
@@ -1242,6 +1252,7 @@ int del_item(struct ctree_root *root, struct ctree_path *path)
 			}
 			if (leaf->header.nritems == 0) {
 				u64 blocknr = leaf_buf->blocknr;
+				clean_tree_block(root, leaf_buf);
 				wret = del_ptr(root, path, 1, slot);
 				if (wret)
 					ret = wret;
@@ -1254,6 +1265,9 @@ int del_item(struct ctree_root *root, struct ctree_path *path)
 			}
 		}
 	}
+	wret = commit_transaction(root);
+	if (wret)
+		ret = wret;
 	return ret;
 }
 

commit 79f95c82dca7665f32bafd68b7cdf4a01fab0840
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Mar 1 15:16:26 2007 -0500

    Btrfs: Fixup the code to merge during path walks
    Add a bulk insert/remove test to random-test
    Add the quick-test code back as another regression test
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index df4a19d65206..afa5bc5c7c1a 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -12,6 +12,9 @@ static int split_leaf(struct ctree_root *root, struct ctree_path *path,
 		      int data_size);
 static int push_node_left(struct ctree_root *root, struct tree_buffer *dst,
 			  struct tree_buffer *src);
+static int balance_node_right(struct ctree_root *root,
+			      struct tree_buffer *dst_buf,
+			      struct tree_buffer *src_buf);
 static int del_ptr(struct ctree_root *root, struct ctree_path *path, int level,
 		   int slot);
 
@@ -217,15 +220,16 @@ static int balance_level(struct ctree_root *root, struct ctree_path *path,
 	int ret = 0;
 	int wret;
 	int pslot;
-	int used = 0;
-	int count;
 	int orig_slot = path->slots[level];
+	u64 orig_ptr;
 
 	if (level == 0)
 		return 0;
 
 	mid_buf = path->nodes[level];
 	mid = &mid_buf->node;
+	orig_ptr = mid->blockptrs[orig_slot];
+
 	if (level < MAX_LEVEL - 1)
 		parent_buf = path->nodes[level + 1];
 	pslot = path->slots[level + 1];
@@ -253,24 +257,26 @@ static int balance_level(struct ctree_root *root, struct ctree_path *path,
 	if (mid->header.nritems > NODEPTRS_PER_BLOCK / 4)
 		return 0;
 
-	// print_tree(root, root->node);
 	left_buf = read_node_slot(root, parent_buf, pslot - 1);
 	right_buf = read_node_slot(root, parent_buf, pslot + 1);
-	if (right_buf) {
-		right = &right_buf->node;
-		used = right->header.nritems;
-		count = 1;
-	}
+
+	/* first, try to make some room in the middle buffer */
 	if (left_buf) {
 		left = &left_buf->node;
-		used += left->header.nritems;
 		orig_slot += left->header.nritems;
-		count++;
+		wret = push_node_left(root, left_buf, mid_buf);
+		if (wret < 0)
+			ret = wret;
 	}
-	if (left_buf)
-		push_node_left(root, left_buf, mid_buf);
+
+	/*
+	 * then try to empty the right most buffer into the middle
+	 */
 	if (right_buf) {
-		push_node_left(root, mid_buf, right_buf);
+		right = &right_buf->node;
+		wret = push_node_left(root, mid_buf, right_buf);
+		if (wret < 0)
+			ret = wret;
 		if (right->header.nritems == 0) {
 			u64 blocknr = right_buf->blocknr;
 			tree_block_release(root, right_buf);
@@ -285,9 +291,29 @@ static int balance_level(struct ctree_root *root, struct ctree_path *path,
 		} else {
 			memcpy(parent->keys + pslot + 1, right->keys,
 				sizeof(struct key));
+			wret = write_tree_block(root, parent_buf);
+			if (wret)
+				ret = wret;
 		}
 	}
+	if (mid->header.nritems == 1) {
+		/*
+		 * we're not allowed to leave a node with one item in the
+		 * tree during a delete.  A deletion from lower in the tree
+		 * could try to delete the only pointer in this node.
+		 * So, pull some keys from the left.
+		 * There has to be a left pointer at this point because
+		 * otherwise we would have pulled some pointers from the
+		 * right
+		 */
+		BUG_ON(!left_buf);
+		wret = balance_node_right(root, mid_buf, left_buf);
+		if (wret < 0)
+			ret = wret;
+		BUG_ON(wret == 1);
+	}
 	if (mid->header.nritems == 0) {
+		/* we've managed to empty the middle node, drop it */
 		u64 blocknr = mid_buf->blocknr;
 		tree_block_release(root, mid_buf);
 		mid_buf = NULL;
@@ -298,11 +324,17 @@ static int balance_level(struct ctree_root *root, struct ctree_path *path,
 		wret = free_extent(root, blocknr, 1);
 		if (wret)
 			ret = wret;
-	} else
+	} else {
+		/* update the parent key to reflect our changes */
 		memcpy(parent->keys + pslot, mid->keys, sizeof(struct key));
+		wret = write_tree_block(root, parent_buf);
+		if (wret)
+			ret = wret;
+	}
 
+	/* update the path */
 	if (left_buf) {
-		if (left->header.nritems >= orig_slot) {
+		if (left->header.nritems > orig_slot) {
 			left_buf->count++; // released below
 			path->nodes[level] = left_buf;
 			path->slots[level + 1] -= 1;
@@ -314,12 +346,15 @@ static int balance_level(struct ctree_root *root, struct ctree_path *path,
 			path->slots[level] = orig_slot;
 		}
 	}
+	/* double check we haven't messed things up */
+	check_block(path, level);
+	if (orig_ptr != path->nodes[level]->node.blockptrs[path->slots[level]])
+		BUG();
 
 	if (right_buf)
 		tree_block_release(root, right_buf);
 	if (left_buf)
 		tree_block_release(root, left_buf);
-
 	return ret;
 }
 
@@ -378,6 +413,7 @@ int search_slot(struct ctree_root *root, struct key *key,
 					goto again;
 				c = &b->node;
 				slot = p->slots[level];
+				BUG_ON(c->header.nritems == 1);
 			}
 			b = read_tree_block(root, c->blockptrs[slot]);
 		} else {
@@ -433,13 +469,7 @@ static int fixup_low_keys(struct ctree_root *root,
 
 /*
  * try to push data from one node into the next node left in the
- * tree.  The src node is found at specified level in the path.
- * If some bytes were pushed, return 0, otherwise return 1.
- *
- * Lower nodes/leaves in the path are not touched, higher nodes may
- * be modified to reflect the push.
- *
- * The path is altered to reflect the push.
+ * tree.
  *
  * returns 0 if some ptrs were pushed left, < 0 if there was some horrible
  * error, and > 0 if there was no room in the left hand block.
@@ -463,7 +493,8 @@ static int push_node_left(struct ctree_root *root, struct tree_buffer *dst_buf,
 	}
 
 	if (src_nritems < push_items)
-		push_items =src_nritems;
+		push_items = src_nritems;
+
 	memcpy(dst->keys + dst_nritems, src->keys,
 		push_items * sizeof(struct key));
 	memcpy(dst->blockptrs + dst_nritems, src->blockptrs,
@@ -487,6 +518,64 @@ static int push_node_left(struct ctree_root *root, struct tree_buffer *dst_buf,
 	return ret;
 }
 
+/*
+ * try to push data from one node into the next node right in the
+ * tree.
+ *
+ * returns 0 if some ptrs were pushed, < 0 if there was some horrible
+ * error, and > 0 if there was no room in the right hand block.
+ *
+ * this will  only push up to 1/2 the contents of the left node over
+ */
+static int balance_node_right(struct ctree_root *root,
+			      struct tree_buffer *dst_buf,
+			      struct tree_buffer *src_buf)
+{
+	struct node *src = &src_buf->node;
+	struct node *dst = &dst_buf->node;
+	int push_items = 0;
+	int max_push;
+	int src_nritems;
+	int dst_nritems;
+	int ret = 0;
+	int wret;
+
+	src_nritems = src->header.nritems;
+	dst_nritems = dst->header.nritems;
+	push_items = NODEPTRS_PER_BLOCK - dst_nritems;
+	if (push_items <= 0) {
+		return 1;
+	}
+
+	max_push = src_nritems / 2 + 1;
+	/* don't try to empty the node */
+	if (max_push > src_nritems)
+		return 1;
+	if (max_push < push_items)
+		push_items = max_push;
+
+	memmove(dst->keys + push_items, dst->keys,
+		dst_nritems * sizeof(struct key));
+	memmove(dst->blockptrs + push_items, dst->blockptrs,
+		dst_nritems * sizeof(u64));
+	memcpy(dst->keys, src->keys + src_nritems - push_items,
+		push_items * sizeof(struct key));
+	memcpy(dst->blockptrs, src->blockptrs + src_nritems - push_items,
+		push_items * sizeof(u64));
+
+	src->header.nritems -= push_items;
+	dst->header.nritems += push_items;
+
+	wret = write_tree_block(root, src_buf);
+	if (wret < 0)
+		ret = wret;
+
+	wret = write_tree_block(root, dst_buf);
+	if (wret < 0)
+		ret = wret;
+	return ret;
+}
+
 /*
  * helper function to insert a new root level in the tree.
  * A new node is allocated, and a single item is inserted to

commit bb8039515d7c1b521ea22f095b43618ccc771885
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Mar 1 12:04:21 2007 -0500

    Btrfs: merge on the way down during deletes
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 273239932255..df4a19d65206 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -10,11 +10,10 @@ static int split_node(struct ctree_root *root, struct ctree_path *path,
 		      int level);
 static int split_leaf(struct ctree_root *root, struct ctree_path *path,
 		      int data_size);
-static int push_node_left(struct ctree_root *root, struct ctree_path *path,
-			  int level);
-static int push_node_right(struct ctree_root *root,
-		    struct ctree_path *path, int level);
-static int del_ptr(struct ctree_root *root, struct ctree_path *path, int level);
+static int push_node_left(struct ctree_root *root, struct tree_buffer *dst,
+			  struct tree_buffer *src);
+static int del_ptr(struct ctree_root *root, struct ctree_path *path, int level,
+		   int slot);
 
 inline void init_path(struct ctree_path *p)
 {
@@ -192,6 +191,138 @@ int bin_search(struct node *c, struct key *key, int *slot)
 	return -1;
 }
 
+struct tree_buffer *read_node_slot(struct ctree_root *root,
+				   struct tree_buffer *parent_buf,
+				   int slot)
+{
+	struct node *node = &parent_buf->node;
+	if (slot < 0)
+		return NULL;
+	if (slot >= node->header.nritems)
+		return NULL;
+	return read_tree_block(root, node->blockptrs[slot]);
+}
+
+static int balance_level(struct ctree_root *root, struct ctree_path *path,
+			int level)
+{
+	struct tree_buffer *right_buf;
+	struct tree_buffer *mid_buf;
+	struct tree_buffer *left_buf;
+	struct tree_buffer *parent_buf = NULL;
+	struct node *right = NULL;
+	struct node *mid;
+	struct node *left = NULL;
+	struct node *parent = NULL;
+	int ret = 0;
+	int wret;
+	int pslot;
+	int used = 0;
+	int count;
+	int orig_slot = path->slots[level];
+
+	if (level == 0)
+		return 0;
+
+	mid_buf = path->nodes[level];
+	mid = &mid_buf->node;
+	if (level < MAX_LEVEL - 1)
+		parent_buf = path->nodes[level + 1];
+	pslot = path->slots[level + 1];
+
+	if (!parent_buf) {
+		struct tree_buffer *child;
+		u64 blocknr = mid_buf->blocknr;
+
+		if (mid->header.nritems != 1)
+			return 0;
+
+		/* promote the child to a root */
+		child = read_node_slot(root, mid_buf, 0);
+		BUG_ON(!child);
+		root->node = child;
+		path->nodes[level] = NULL;
+		/* once for the path */
+		tree_block_release(root, mid_buf);
+		/* once for the root ptr */
+		tree_block_release(root, mid_buf);
+		return free_extent(root, blocknr, 1);
+	}
+	parent = &parent_buf->node;
+
+	if (mid->header.nritems > NODEPTRS_PER_BLOCK / 4)
+		return 0;
+
+	// print_tree(root, root->node);
+	left_buf = read_node_slot(root, parent_buf, pslot - 1);
+	right_buf = read_node_slot(root, parent_buf, pslot + 1);
+	if (right_buf) {
+		right = &right_buf->node;
+		used = right->header.nritems;
+		count = 1;
+	}
+	if (left_buf) {
+		left = &left_buf->node;
+		used += left->header.nritems;
+		orig_slot += left->header.nritems;
+		count++;
+	}
+	if (left_buf)
+		push_node_left(root, left_buf, mid_buf);
+	if (right_buf) {
+		push_node_left(root, mid_buf, right_buf);
+		if (right->header.nritems == 0) {
+			u64 blocknr = right_buf->blocknr;
+			tree_block_release(root, right_buf);
+			right_buf = NULL;
+			right = NULL;
+			wret = del_ptr(root, path, level + 1, pslot + 1);
+			if (wret)
+				ret = wret;
+			wret = free_extent(root, blocknr, 1);
+			if (wret)
+				ret = wret;
+		} else {
+			memcpy(parent->keys + pslot + 1, right->keys,
+				sizeof(struct key));
+		}
+	}
+	if (mid->header.nritems == 0) {
+		u64 blocknr = mid_buf->blocknr;
+		tree_block_release(root, mid_buf);
+		mid_buf = NULL;
+		mid = NULL;
+		wret = del_ptr(root, path, level + 1, pslot);
+		if (wret)
+			ret = wret;
+		wret = free_extent(root, blocknr, 1);
+		if (wret)
+			ret = wret;
+	} else
+		memcpy(parent->keys + pslot, mid->keys, sizeof(struct key));
+
+	if (left_buf) {
+		if (left->header.nritems >= orig_slot) {
+			left_buf->count++; // released below
+			path->nodes[level] = left_buf;
+			path->slots[level + 1] -= 1;
+			path->slots[level] = orig_slot;
+			if (mid_buf)
+				tree_block_release(root, mid_buf);
+		} else {
+			orig_slot -= left->header.nritems;
+			path->slots[level] = orig_slot;
+		}
+	}
+
+	if (right_buf)
+		tree_block_release(root, right_buf);
+	if (left_buf)
+		tree_block_release(root, left_buf);
+
+	return ret;
+}
+
 /*
  * look for key in the tree.  path is filled in with nodes along the way
  * if key is found, we return zero and you can find the item in the leaf
@@ -208,12 +339,14 @@ int bin_search(struct node *c, struct key *key, int *slot)
 int search_slot(struct ctree_root *root, struct key *key,
 		struct ctree_path *p, int ins_len)
 {
-	struct tree_buffer *b = root->node;
+	struct tree_buffer *b;
 	struct node *c;
 	int slot;
 	int ret;
 	int level;
 
+again:
+	b = root->node;
 	b->count++;
 	while (b) {
 		c = &b->node;
@@ -236,9 +369,17 @@ int search_slot(struct ctree_root *root, struct key *key,
 				b = p->nodes[level];
 				c = &b->node;
 				slot = p->slots[level];
+			} else if (ins_len < 0) {
+				int sret = balance_level(root, p, level);
+				if (sret)
+					return sret;
+				b = p->nodes[level];
+				if (!b)
+					goto again;
+				c = &b->node;
+				slot = p->slots[level];
 			}
 			b = read_tree_block(root, c->blockptrs[slot]);
-			continue;
 		} else {
 			struct leaf *l = (struct leaf *)c;
 			p->slots[level] = slot;
@@ -249,9 +390,11 @@ int search_slot(struct ctree_root *root, struct key *key,
 				if (sret)
 					return sret;
 			}
+			BUG_ON(root->node->count == 1);
 			return ret;
 		}
 	}
+	BUG_ON(root->node->count == 1);
 	return 1;
 }
 
@@ -301,163 +444,49 @@ static int fixup_low_keys(struct ctree_root *root,
  * returns 0 if some ptrs were pushed left, < 0 if there was some horrible
  * error, and > 0 if there was no room in the left hand block.
  */
-static int push_node_left(struct ctree_root *root, struct ctree_path *path,
-			  int level)
+static int push_node_left(struct ctree_root *root, struct tree_buffer *dst_buf,
+			  struct tree_buffer *src_buf)
 {
-	int slot;
-	struct node *left;
-	struct node *right;
+	struct node *src = &src_buf->node;
+	struct node *dst = &dst_buf->node;
 	int push_items = 0;
-	int left_nritems;
-	int right_nritems;
-	struct tree_buffer *t;
-	struct tree_buffer *right_buf;
+	int src_nritems;
+	int dst_nritems;
 	int ret = 0;
 	int wret;
 
-	if (level == MAX_LEVEL - 1 || path->nodes[level + 1] == 0)
-		return 1;
-	slot = path->slots[level + 1];
-	if (slot == 0)
-		return 1;
-
-	t = read_tree_block(root,
-		            path->nodes[level + 1]->node.blockptrs[slot - 1]);
-	left = &t->node;
-	right_buf = path->nodes[level];
-	right = &right_buf->node;
-	left_nritems = left->header.nritems;
-	right_nritems = right->header.nritems;
-	push_items = NODEPTRS_PER_BLOCK - (left_nritems + 1);
+	src_nritems = src->header.nritems;
+	dst_nritems = dst->header.nritems;
+	push_items = NODEPTRS_PER_BLOCK - dst_nritems;
 	if (push_items <= 0) {
-		tree_block_release(root, t);
 		return 1;
 	}
 
-	if (right_nritems < push_items)
-		push_items = right_nritems;
-	memcpy(left->keys + left_nritems, right->keys,
+	if (src_nritems < push_items)
+		push_items =src_nritems;
+	memcpy(dst->keys + dst_nritems, src->keys,
 		push_items * sizeof(struct key));
-	memcpy(left->blockptrs + left_nritems, right->blockptrs,
+	memcpy(dst->blockptrs + dst_nritems, src->blockptrs,
 		push_items * sizeof(u64));
-	memmove(right->keys, right->keys + push_items,
-		(right_nritems - push_items) * sizeof(struct key));
-	memmove(right->blockptrs, right->blockptrs + push_items,
-		(right_nritems - push_items) * sizeof(u64));
-	right->header.nritems -= push_items;
-	left->header.nritems += push_items;
-
-	/* adjust the pointers going up the tree */
-	wret = fixup_low_keys(root, path, right->keys, level + 1);
-	if (wret < 0)
-		ret = wret;
+	if (push_items < src_nritems) {
+		memmove(src->keys, src->keys + push_items,
+			(src_nritems - push_items) * sizeof(struct key));
+		memmove(src->blockptrs, src->blockptrs + push_items,
+			(src_nritems - push_items) * sizeof(u64));
+	}
+	src->header.nritems -= push_items;
+	dst->header.nritems += push_items;
 
-	wret = write_tree_block(root, t);
+	wret = write_tree_block(root, src_buf);
 	if (wret < 0)
 		ret = wret;
 
-	wret = write_tree_block(root, right_buf);
+	wret = write_tree_block(root, dst_buf);
 	if (wret < 0)
 		ret = wret;
-
-	/* then fixup the leaf pointer in the path */
-	if (path->slots[level] < push_items) {
-		path->slots[level] += left_nritems;
-		tree_block_release(root, path->nodes[level]);
-		path->nodes[level] = t;
-		path->slots[level + 1] -= 1;
-	} else {
-		path->slots[level] -= push_items;
-		tree_block_release(root, t);
-	}
 	return ret;
 }
 
-/*
- * try to push data from one node into the next node right in the
- * tree.  The src node is found at specified level in the path.
- * If some bytes were pushed, return 0, otherwise return 1.
- *
- * Lower nodes/leaves in the path are not touched, higher nodes may
- * be modified to reflect the push.
- *
- * The path is altered to reflect the push.
- *
- * returns 0 if some ptrs were pushed, < 0 if there was some horrible
- * error, and > 0 if there was no room in the right hand block.
- */
-static int push_node_right(struct ctree_root *root, struct ctree_path *path,
-			   int level)
-{
-	int slot;
-	struct tree_buffer *t;
-	struct tree_buffer *src_buffer;
-	struct node *dst;
-	struct node *src;
-	int push_items = 0;
-	int dst_nritems;
-	int src_nritems;
-
-	/* can't push from the root */
-	if (level == MAX_LEVEL - 1 || path->nodes[level + 1] == 0)
-		return 1;
-
-	/* only try to push inside the node higher up */
-	slot = path->slots[level + 1];
-	if (slot == NODEPTRS_PER_BLOCK - 1)
-		return 1;
-
-	if (slot >= path->nodes[level + 1]->node.header.nritems -1)
-		return 1;
-
-	t = read_tree_block(root,
-			    path->nodes[level + 1]->node.blockptrs[slot + 1]);
-	dst = &t->node;
-	src_buffer = path->nodes[level];
-	src = &src_buffer->node;
-	dst_nritems = dst->header.nritems;
-	src_nritems = src->header.nritems;
-	push_items = NODEPTRS_PER_BLOCK - (dst_nritems + 1);
-	if (push_items <= 0) {
-		tree_block_release(root, t);
-		return 1;
-	}
-
-	if (src_nritems < push_items)
-		push_items = src_nritems;
-	memmove(dst->keys + push_items, dst->keys,
-		dst_nritems * sizeof(struct key));
-	memcpy(dst->keys, src->keys + src_nritems - push_items,
-		push_items * sizeof(struct key));
-
-	memmove(dst->blockptrs + push_items, dst->blockptrs,
-		dst_nritems * sizeof(u64));
-	memcpy(dst->blockptrs, src->blockptrs + src_nritems - push_items,
-		push_items * sizeof(u64));
-
-	src->header.nritems -= push_items;
-	dst->header.nritems += push_items;
-
-	/* adjust the pointers going up the tree */
-	memcpy(path->nodes[level + 1]->node.keys + path->slots[level + 1] + 1,
-		dst->keys, sizeof(struct key));
-
-	write_tree_block(root, path->nodes[level + 1]);
-	write_tree_block(root, t);
-	write_tree_block(root, src_buffer);
-
-	/* then fixup the pointers in the path */
-	if (path->slots[level] >= src->header.nritems) {
-		path->slots[level] -= src->header.nritems;
-		tree_block_release(root, path->nodes[level]);
-		path->nodes[level] = t;
-		path->slots[level + 1] += 1;
-	} else {
-		tree_block_release(root, t);
-	}
-	return 0;
-}
-
 /*
  * helper function to insert a new root level in the tree.
  * A new node is allocated, and a single item is inserted to
@@ -558,16 +587,6 @@ static int split_node(struct ctree_root *root, struct ctree_path *path,
 	int ret;
 	int wret;
 
-	ret = push_node_left(root, path, level);
-	if (!ret)
-		return 0;
-	if (ret < 0)
-		return ret;
-	ret = push_node_right(root, path, level);
-	if (!ret)
-		return 0;
-	if (ret < 0)
-		return ret;
 	t = path->nodes[level];
 	c = &t->node;
 	if (t == root->node) {
@@ -1011,6 +1030,7 @@ int insert_item(struct ctree_root *root, struct key *key,
 
 	if (leaf_free_space(leaf) < 0)
 		BUG();
+	check_leaf(&path, 0);
 	release_path(root, &path);
 	return ret;
 }
@@ -1022,77 +1042,38 @@ int insert_item(struct ctree_root *root, struct key *key,
  * continuing all the way the root if required.  The root is converted into
  * a leaf if all the nodes are emptied.
  */
-static int del_ptr(struct ctree_root *root, struct ctree_path *path, int level)
+static int del_ptr(struct ctree_root *root, struct ctree_path *path, int level,
+		   int slot)
 {
-	int slot;
-	struct tree_buffer *t;
 	struct node *node;
+	struct tree_buffer *parent = path->nodes[level];
 	int nritems;
-	u64 blocknr;
-	int wret;
 	int ret = 0;
+	int wret;
 
-	while(1) {
-		t = path->nodes[level];
-		if (!t)
-			break;
-		node = &t->node;
-		slot = path->slots[level];
-		nritems = node->header.nritems;
-
-		if (slot != nritems -1) {
-			memmove(node->keys + slot, node->keys + slot + 1,
-				sizeof(struct key) * (nritems - slot - 1));
-			memmove(node->blockptrs + slot,
-				node->blockptrs + slot + 1,
-				sizeof(u64) * (nritems - slot - 1));
-		}
-		node->header.nritems--;
-		blocknr = t->blocknr;
-		write_tree_block(root, t);
-		if (node->header.nritems != 0) {
-			int tslot;
-			if (slot == 0) {
-				wret = fixup_low_keys(root, path,
-							   node->keys,
-							   level + 1);
-				if (wret)
-					ret = wret;
-			}
-			tslot = path->slots[level + 1];
-			t->count++;
-			wret = push_node_left(root, path, level);
-			if (wret < 0) {
-				ret = wret;
-				break;
-			}
-			if (node->header.nritems != 0) {
-				wret = push_node_right(root, path, level);
-				if (wret < 0) {
-					ret = wret;
-					break;
-				}
-			}
-			path->slots[level + 1] = tslot;
-			if (node->header.nritems != 0) {
-				tree_block_release(root, t);
-				break;
-			}
-			tree_block_release(root, t);
-		}
-		if (t == root->node) {
-			/* just turn the root into a leaf and break */
-			root->node->node.header.flags = node_level(0);
-			write_tree_block(root, t);
-			break;
-		}
-		level++;
-		wret = free_extent(root, blocknr, 1);
+	node = &parent->node;
+	nritems = node->header.nritems;
+
+	if (slot != nritems -1) {
+		memmove(node->keys + slot, node->keys + slot + 1,
+			sizeof(struct key) * (nritems - slot - 1));
+		memmove(node->blockptrs + slot,
+			node->blockptrs + slot + 1,
+			sizeof(u64) * (nritems - slot - 1));
+	}
+	node->header.nritems--;
+	if (node->header.nritems == 0 && parent == root->node) {
+		BUG_ON(node_level(root->node->node.header.flags) != 1);
+		/* just turn the root into a leaf and break */
+		root->node->node.header.flags = node_level(0);
+	} else if (slot == 0) {
+		wret = fixup_low_keys(root, path, node->keys, level + 1);
 		if (wret)
 			ret = wret;
-		if (!path->nodes[level])
-			BUG();
 	}
+	wret = write_tree_block(root, parent);
+	if (wret)
+		ret = wret;
 	return ret;
 }
 
@@ -1135,7 +1116,7 @@ int del_item(struct ctree_root *root, struct ctree_path *path)
 			leaf->header.flags = node_level(0);
 			write_tree_block(root, leaf_buf);
 		} else {
-			wret = del_ptr(root, path, 1);
+			wret = del_ptr(root, path, 1, path->slots[1]);
 			if (wret)
 				ret = wret;
 			wret = free_extent(root, leaf_buf->blocknr, 1);
@@ -1172,8 +1153,7 @@ int del_item(struct ctree_root *root, struct ctree_path *path)
 			}
 			if (leaf->header.nritems == 0) {
 				u64 blocknr = leaf_buf->blocknr;
-				path->slots[1] = slot;
-				wret = del_ptr(root, path, 1);
+				wret = del_ptr(root, path, 1, slot);
 				if (wret)
 					ret = wret;
 				tree_block_release(root, leaf_buf);

commit 0f70abe2b39d19171d4133d2ffdf77fb9113106a
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Feb 28 16:46:22 2007 -0500

    Btrfs: more return code checking
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index be2be0272513..273239932255 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1087,7 +1087,9 @@ static int del_ptr(struct ctree_root *root, struct ctree_path *path, int level)
 			break;
 		}
 		level++;
-		free_extent(root, blocknr, 1);
+		wret = free_extent(root, blocknr, 1);
+		if (wret)
+			ret = wret;
 		if (!path->nodes[level])
 			BUG();
 	}
@@ -1136,7 +1138,9 @@ int del_item(struct ctree_root *root, struct ctree_path *path)
 			wret = del_ptr(root, path, 1);
 			if (wret)
 				ret = wret;
-			free_extent(root, leaf_buf->blocknr, 1);
+			wret = free_extent(root, leaf_buf->blocknr, 1);
+			if (wret)
+				ret = wret;
 		}
 	} else {
 		int used = leaf_space_used(leaf, 0, leaf->header.nritems);
@@ -1173,7 +1177,9 @@ int del_item(struct ctree_root *root, struct ctree_path *path)
 				if (wret)
 					ret = wret;
 				tree_block_release(root, leaf_buf);
-				free_extent(root, blocknr, 1);
+				wret = free_extent(root, blocknr, 1);
+				if (wret)
+					ret = wret;
 			} else {
 				tree_block_release(root, leaf_buf);
 			}
@@ -1184,7 +1190,8 @@ int del_item(struct ctree_root *root, struct ctree_path *path)
 
 /*
  * walk up the tree as far as required to find the next leaf.
- * returns 0 if it found something or -1 if there are no greater leaves.
+ * returns 0 if it found something or 1 if there are no greater leaves.
+ * returns < 0 on io errors.
  */
 int next_leaf(struct ctree_root *root, struct ctree_path *path)
 {
@@ -1196,7 +1203,7 @@ int next_leaf(struct ctree_root *root, struct ctree_path *path)
 
 	while(level < MAX_LEVEL) {
 		if (!path->nodes[level])
-			return -1;
+			return 1;
 		slot = path->slots[level] + 1;
 		c = path->nodes[level];
 		if (slot >= c->node.header.nritems) {

commit aa5d6bed255d7f8c655a8f10d760f4247bc8385c
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Feb 28 16:35:06 2007 -0500

    Btrfs: return code checking
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 0aea94224ba3..be2be0272513 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -6,12 +6,15 @@
 #include "disk-io.h"
 #include "print-tree.h"
 
-int split_node(struct ctree_root *root, struct ctree_path *path, int level);
-int split_leaf(struct ctree_root *root, struct ctree_path *path, int data_size);
-int push_node_left(struct ctree_root *root, struct ctree_path *path, int level);
-int push_node_right(struct ctree_root *root,
+static int split_node(struct ctree_root *root, struct ctree_path *path,
+		      int level);
+static int split_leaf(struct ctree_root *root, struct ctree_path *path,
+		      int data_size);
+static int push_node_left(struct ctree_root *root, struct ctree_path *path,
+			  int level);
+static int push_node_right(struct ctree_root *root,
 		    struct ctree_path *path, int level);
-int del_ptr(struct ctree_root *root, struct ctree_path *path, int level);
+static int del_ptr(struct ctree_root *root, struct ctree_path *path, int level);
 
 inline void init_path(struct ctree_path *p)
 {
@@ -26,6 +29,7 @@ void release_path(struct ctree_root *root, struct ctree_path *p)
 			break;
 		tree_block_release(root, p->nodes[i]);
 	}
+	memset(p, 0, sizeof(*p));
 }
 
 /*
@@ -74,6 +78,67 @@ int comp_keys(struct key *k1, struct key *k2)
 	return 0;
 }
 
+int check_node(struct ctree_path *path, int level)
+{
+	int i;
+	struct node *parent = NULL;
+	struct node *node = &path->nodes[level]->node;
+	int parent_slot;
+
+	if (path->nodes[level + 1])
+		parent = &path->nodes[level + 1]->node;
+	parent_slot = path->slots[level + 1];
+	if (parent && node->header.nritems > 0) {
+		struct key *parent_key;
+		parent_key = &parent->keys[parent_slot];
+		BUG_ON(memcmp(parent_key, node->keys, sizeof(struct key)));
+		BUG_ON(parent->blockptrs[parent_slot] != node->header.blocknr);
+	}
+	BUG_ON(node->header.nritems > NODEPTRS_PER_BLOCK);
+	for (i = 0; i < node->header.nritems - 2; i++) {
+		BUG_ON(comp_keys(&node->keys[i], &node->keys[i+1]) >= 0);
+	}
+	return 0;
+}
+
+int check_leaf(struct ctree_path *path, int level)
+{
+	int i;
+	struct leaf *leaf = &path->nodes[level]->leaf;
+	struct node *parent = NULL;
+	int parent_slot;
+
+	if (path->nodes[level + 1])
+		parent = &path->nodes[level + 1]->node;
+	parent_slot = path->slots[level + 1];
+	if (parent && leaf->header.nritems > 0) {
+		struct key *parent_key;
+		parent_key = &parent->keys[parent_slot];
+		BUG_ON(memcmp(parent_key, &leaf->items[0].key,
+		       sizeof(struct key)));
+		BUG_ON(parent->blockptrs[parent_slot] != leaf->header.blocknr);
+	}
+	for (i = 0; i < leaf->header.nritems - 2; i++) {
+		BUG_ON(comp_keys(&leaf->items[i].key,
+		                 &leaf->items[i+1].key) >= 0);
+		BUG_ON(leaf->items[i].offset != leaf->items[i + 1].offset +
+		    leaf->items[i + 1].size);
+		if (i == 0) {
+			BUG_ON(leaf->items[i].offset + leaf->items[i].size !=
+				LEAF_DATA_SIZE);
+		}
+	}
+	BUG_ON(leaf_free_space(leaf) < 0);
+	return 0;
+}
+
+int check_block(struct ctree_path *path, int level)
+{
+	if (level == 0)
+		return check_leaf(path, level);
+	return check_node(path, level);
+}
+
 /*
  * search for key in the array p.  items p are item_size apart
  * and there are 'max' items in p
@@ -133,7 +198,8 @@ int bin_search(struct node *c, struct key *key, int *slot)
  * level of the path (level 0)
  *
  * If the key isn't found, the path points to the slot where it should
- * be inserted.
+ * be inserted, and 1 is returned.  If there are other errors during the
+ * search a negative error number is returned.
  *
  * if ins_len > 0, nodes and leaves will be split as we walk down the
  * tree.  if ins_len < 0, nodes will be merged as we walk down the tree (if
@@ -153,6 +219,9 @@ int search_slot(struct ctree_root *root, struct key *key,
 		c = &b->node;
 		level = node_level(c->header.flags);
 		p->nodes[level] = b;
+		ret = check_block(p, level);
+		if (ret)
+			return -1;
 		ret = bin_search(c, key, &slot);
 		if (!is_leaf(c->header.flags)) {
 			if (ret && slot > 0)
@@ -183,7 +252,7 @@ int search_slot(struct ctree_root *root, struct key *key,
 			return ret;
 		}
 	}
-	return -1;
+	return 1;
 }
 
 /*
@@ -192,12 +261,17 @@ int search_slot(struct ctree_root *root, struct key *key,
  * This is used after shifting pointers to the left, so it stops
  * fixing up pointers when a given leaf/node is not in slot 0 of the
  * higher levels
+ *
+ * If this fails to write a tree block, it returns -1, but continues
+ * fixing up the blocks in ram so the tree is consistent.
  */
-static void fixup_low_keys(struct ctree_root *root,
+static int fixup_low_keys(struct ctree_root *root,
 			   struct ctree_path *path, struct key *key,
 			   int level)
 {
 	int i;
+	int ret = 0;
+	int wret;
 	for (i = level; i < MAX_LEVEL; i++) {
 		struct node *t;
 		int tslot = path->slots[i];
@@ -205,10 +279,13 @@ static void fixup_low_keys(struct ctree_root *root,
 			break;
 		t = &path->nodes[i]->node;
 		memcpy(t->keys + tslot, key, sizeof(*key));
-		write_tree_block(root, path->nodes[i]);
+		wret = write_tree_block(root, path->nodes[i]);
+		if (wret)
+			ret = wret;
 		if (tslot != 0)
 			break;
 	}
+	return ret;
 }
 
 /*
@@ -220,8 +297,12 @@ static void fixup_low_keys(struct ctree_root *root,
  * be modified to reflect the push.
  *
  * The path is altered to reflect the push.
+ *
+ * returns 0 if some ptrs were pushed left, < 0 if there was some horrible
+ * error, and > 0 if there was no room in the left hand block.
  */
-int push_node_left(struct ctree_root *root, struct ctree_path *path, int level)
+static int push_node_left(struct ctree_root *root, struct ctree_path *path,
+			  int level)
 {
 	int slot;
 	struct node *left;
@@ -231,6 +312,8 @@ int push_node_left(struct ctree_root *root, struct ctree_path *path, int level)
 	int right_nritems;
 	struct tree_buffer *t;
 	struct tree_buffer *right_buf;
+	int ret = 0;
+	int wret;
 
 	if (level == MAX_LEVEL - 1 || path->nodes[level + 1] == 0)
 		return 1;
@@ -265,10 +348,17 @@ int push_node_left(struct ctree_root *root, struct ctree_path *path, int level)
 	left->header.nritems += push_items;
 
 	/* adjust the pointers going up the tree */
-	fixup_low_keys(root, path, right->keys, level + 1);
+	wret = fixup_low_keys(root, path, right->keys, level + 1);
+	if (wret < 0)
+		ret = wret;
 
-	write_tree_block(root, t);
-	write_tree_block(root, right_buf);
+	wret = write_tree_block(root, t);
+	if (wret < 0)
+		ret = wret;
+
+	wret = write_tree_block(root, right_buf);
+	if (wret < 0)
+		ret = wret;
 
 	/* then fixup the leaf pointer in the path */
 	if (path->slots[level] < push_items) {
@@ -280,7 +370,7 @@ int push_node_left(struct ctree_root *root, struct ctree_path *path, int level)
 		path->slots[level] -= push_items;
 		tree_block_release(root, t);
 	}
-	return 0;
+	return ret;
 }
 
 /*
@@ -292,8 +382,12 @@ int push_node_left(struct ctree_root *root, struct ctree_path *path, int level)
  * be modified to reflect the push.
  *
  * The path is altered to reflect the push.
+ *
+ * returns 0 if some ptrs were pushed, < 0 if there was some horrible
+ * error, and > 0 if there was no room in the right hand block.
  */
-int push_node_right(struct ctree_root *root, struct ctree_path *path, int level)
+static int push_node_right(struct ctree_root *root, struct ctree_path *path,
+			   int level)
 {
 	int slot;
 	struct tree_buffer *t;
@@ -368,6 +462,8 @@ int push_node_right(struct ctree_root *root, struct ctree_path *path, int level)
  * helper function to insert a new root level in the tree.
  * A new node is allocated, and a single item is inserted to
  * point to the existing root
+ *
+ * returns zero on success or < 0 on failure.
  */
 static int insert_new_root(struct ctree_root *root,
 			   struct ctree_path *path, int level)
@@ -410,8 +506,10 @@ static int insert_new_root(struct ctree_root *root,
  *
  * slot and level indicate where you want the key to go, and
  * blocknr is the block the key points to.
+ *
+ * returns zero on success and < 0 on any error
  */
-int insert_ptr(struct ctree_root *root,
+static int insert_ptr(struct ctree_root *root,
 		struct ctree_path *path, struct key *key,
 		u64 blocknr, int slot, int level)
 {
@@ -446,8 +544,11 @@ int insert_ptr(struct ctree_root *root,
  *
  * Before splitting this tries to make some room in the node by pushing
  * left and right, if either one works, it returns right away.
+ *
+ * returns 0 on success and < 0 on failure
  */
-int split_node(struct ctree_root *root, struct ctree_path *path, int level)
+static int split_node(struct ctree_root *root, struct ctree_path *path,
+		      int level)
 {
 	struct tree_buffer *t;
 	struct node *c;
@@ -455,13 +556,18 @@ int split_node(struct ctree_root *root, struct ctree_path *path, int level)
 	struct node *split;
 	int mid;
 	int ret;
+	int wret;
 
 	ret = push_node_left(root, path, level);
 	if (!ret)
 		return 0;
+	if (ret < 0)
+		return ret;
 	ret = push_node_right(root, path, level);
 	if (!ret)
 		return 0;
+	if (ret < 0)
+		return ret;
 	t = path->nodes[level];
 	c = &t->node;
 	if (t == root->node) {
@@ -482,10 +588,19 @@ int split_node(struct ctree_root *root, struct ctree_path *path, int level)
 		(c->header.nritems - mid) * sizeof(u64));
 	split->header.nritems = c->header.nritems - mid;
 	c->header.nritems = mid;
-	write_tree_block(root, t);
-	write_tree_block(root, split_buffer);
-	insert_ptr(root, path, split->keys, split_buffer->blocknr,
-		     path->slots[level + 1] + 1, level + 1);
+	ret = 0;
+
+	wret = write_tree_block(root, t);
+	if (wret)
+		ret = wret;
+	wret = write_tree_block(root, split_buffer);
+	if (wret)
+		ret = wret;
+	wret = insert_ptr(root, path, split->keys, split_buffer->blocknr,
+			  path->slots[level + 1] + 1, level + 1);
+	if (wret)
+		ret = wret;
+
 	if (path->slots[level] >= mid) {
 		path->slots[level] -= mid;
 		tree_block_release(root, t);
@@ -494,7 +609,7 @@ int split_node(struct ctree_root *root, struct ctree_path *path, int level)
 	} else {
 		tree_block_release(root, split_buffer);
 	}
-	return 0;
+	return ret;
 }
 
 /*
@@ -502,7 +617,7 @@ int split_node(struct ctree_root *root, struct ctree_path *path, int level)
  * and nr indicate which items in the leaf to check.  This totals up the
  * space used both by the item structs and the item data
  */
-int leaf_space_used(struct leaf *l, int start, int nr)
+static int leaf_space_used(struct leaf *l, int start, int nr)
 {
 	int data_len;
 	int end = start + nr - 1;
@@ -518,9 +633,12 @@ int leaf_space_used(struct leaf *l, int start, int nr)
 /*
  * push some data in the path leaf to the right, trying to free up at
  * least data_size bytes.  returns zero if the push worked, nonzero otherwise
+ *
+ * returns 1 if the push failed because the other node didn't have enough
+ * room, 0 if everything worked out and < 0 if there were major errors.
  */
-int push_leaf_right(struct ctree_root *root, struct ctree_path *path,
-		   int data_size)
+static int push_leaf_right(struct ctree_root *root, struct ctree_path *path,
+			   int data_size)
 {
 	struct tree_buffer *left_buf = path->nodes[0];
 	struct leaf *left = &left_buf->leaf;
@@ -609,8 +727,8 @@ int push_leaf_right(struct ctree_root *root, struct ctree_path *path,
  * push some data in the path leaf to the left, trying to free up at
  * least data_size bytes.  returns zero if the push worked, nonzero otherwise
  */
-int push_leaf_left(struct ctree_root *root, struct ctree_path *path,
-		   int data_size)
+static int push_leaf_left(struct ctree_root *root, struct ctree_path *path,
+			  int data_size)
 {
 	struct tree_buffer *right_buf = path->nodes[0];
 	struct leaf *right = &right_buf->leaf;
@@ -623,6 +741,8 @@ int push_leaf_left(struct ctree_root *root, struct ctree_path *path,
 	int push_items = 0;
 	struct item *item;
 	int old_left_nritems;
+	int ret = 0;
+	int wret;
 
 	slot = path->slots[1];
 	if (slot == 0) {
@@ -681,10 +801,16 @@ int push_leaf_left(struct ctree_root *root, struct ctree_path *path,
 		push_space = right->items[i].offset;
 	}
 
-	write_tree_block(root, t);
-	write_tree_block(root, right_buf);
+	wret = write_tree_block(root, t);
+	if (wret)
+		ret = wret;
+	wret = write_tree_block(root, right_buf);
+	if (wret)
+		ret = wret;
 
-	fixup_low_keys(root, path, &right->items[0].key, 1);
+	wret = fixup_low_keys(root, path, &right->items[0].key, 1);
+	if (wret)
+		ret = wret;
 
 	/* then fixup the leaf pointer in the path */
 	if (path->slots[0] < push_items) {
@@ -697,17 +823,20 @@ int push_leaf_left(struct ctree_root *root, struct ctree_path *path,
 		path->slots[0] -= push_items;
 	}
 	BUG_ON(path->slots[0] < 0);
-	return 0;
+	return ret;
 }
 
 /*
  * split the path's leaf in two, making sure there is at least data_size
  * available for the resulting leaf level of the path.
+ *
+ * returns 0 if all went well and < 0 on failure.
  */
-int split_leaf(struct ctree_root *root, struct ctree_path *path, int data_size)
+static int split_leaf(struct ctree_root *root, struct ctree_path *path,
+		      int data_size)
 {
-	struct tree_buffer *l_buf = path->nodes[0];
-	struct leaf *l = &l_buf->leaf;
+	struct tree_buffer *l_buf;
+	struct leaf *l;
 	int nritems;
 	int mid;
 	int slot;
@@ -718,14 +847,23 @@ int split_leaf(struct ctree_root *root, struct ctree_path *path, int data_size)
 	int rt_data_off;
 	int i;
 	int ret;
-
-	if (push_leaf_left(root, path, data_size) == 0 ||
-	    push_leaf_right(root, path, data_size) == 0) {
-		l_buf = path->nodes[0];
-		l = &l_buf->leaf;
-		if (leaf_free_space(l) >= sizeof(struct item) + data_size)
-			return 0;
+	int wret;
+
+	wret = push_leaf_left(root, path, data_size);
+	if (wret < 0)
+		return wret;
+	if (wret) {
+		wret = push_leaf_right(root, path, data_size);
+		if (wret < 0)
+			return wret;
 	}
+	l_buf = path->nodes[0];
+	l = &l_buf->leaf;
+
+	/* did the pushes work? */
+	if (leaf_free_space(l) >= sizeof(struct item) + data_size)
+		return 0;
+
 	if (!path->nodes[1]) {
 		ret = insert_new_root(root, path, 1);
 		if (ret)
@@ -768,10 +906,17 @@ int split_leaf(struct ctree_root *root, struct ctree_path *path, int data_size)
 		right->items[i].offset += rt_data_off;
 
 	l->header.nritems = mid;
-	ret = insert_ptr(root, path, &right->items[0].key,
+	ret = 0;
+	wret = insert_ptr(root, path, &right->items[0].key,
 			  right_buffer->blocknr, path->slots[1] + 1, 1);
-	write_tree_block(root, right_buffer);
-	write_tree_block(root, l_buf);
+	if (wret)
+		ret = wret;
+	wret = write_tree_block(root, right_buffer);
+	if (wret)
+		ret = wret;
+	wret = write_tree_block(root, l_buf);
+	if (wret)
+		ret = wret;
 
 	BUG_ON(path->slots[0] != slot);
 	if (mid <= slot) {
@@ -792,7 +937,8 @@ int split_leaf(struct ctree_root *root, struct ctree_path *path, int data_size)
 int insert_item(struct ctree_root *root, struct key *key,
 			  void *data, int data_size)
 {
-	int ret;
+	int ret = 0;
+	int wret;
 	int slot;
 	int slot_orig;
 	struct leaf *leaf;
@@ -810,6 +956,10 @@ int insert_item(struct ctree_root *root, struct key *key,
 		release_path(root, &path);
 		return -EEXIST;
 	}
+	if (ret < 0) {
+		release_path(root, &path);
+		return ret;
+	}
 
 	slot_orig = path.slots[0];
 	leaf_buf = path.nodes[0];
@@ -850,13 +1000,19 @@ int insert_item(struct ctree_root *root, struct key *key,
 	leaf->items[slot].size = data_size;
 	memcpy(leaf->data + data_end - data_size, data, data_size);
 	leaf->header.nritems += 1;
-	write_tree_block(root, leaf_buf);
+
+	ret = 0;
 	if (slot == 0)
-		fixup_low_keys(root, &path, key, 1);
+		ret = fixup_low_keys(root, &path, key, 1);
+
+	wret = write_tree_block(root, leaf_buf);
+	if (wret)
+		ret = wret;
+
 	if (leaf_free_space(leaf) < 0)
 		BUG();
 	release_path(root, &path);
-	return 0;
+	return ret;
 }
 
 /*
@@ -866,13 +1022,15 @@ int insert_item(struct ctree_root *root, struct key *key,
  * continuing all the way the root if required.  The root is converted into
  * a leaf if all the nodes are emptied.
  */
-int del_ptr(struct ctree_root *root, struct ctree_path *path, int level)
+static int del_ptr(struct ctree_root *root, struct ctree_path *path, int level)
 {
 	int slot;
 	struct tree_buffer *t;
 	struct node *node;
 	int nritems;
 	u64 blocknr;
+	int wret;
+	int ret = 0;
 
 	while(1) {
 		t = path->nodes[level];
@@ -894,13 +1052,27 @@ int del_ptr(struct ctree_root *root, struct ctree_path *path, int level)
 		write_tree_block(root, t);
 		if (node->header.nritems != 0) {
 			int tslot;
-			if (slot == 0)
-				fixup_low_keys(root, path, node->keys,
-					       level + 1);
+			if (slot == 0) {
+				wret = fixup_low_keys(root, path,
+							   node->keys,
+							   level + 1);
+				if (wret)
+					ret = wret;
+			}
 			tslot = path->slots[level + 1];
 			t->count++;
-			if (push_node_left(root, path, level))
-				push_node_right(root, path, level);
+			wret = push_node_left(root, path, level);
+			if (wret < 0) {
+				ret = wret;
+				break;
+			}
+			if (node->header.nritems != 0) {
+				wret = push_node_right(root, path, level);
+				if (wret < 0) {
+					ret = wret;
+					break;
+				}
+			}
 			path->slots[level + 1] = tslot;
 			if (node->header.nritems != 0) {
 				tree_block_release(root, t);
@@ -919,7 +1091,7 @@ int del_ptr(struct ctree_root *root, struct ctree_path *path, int level)
 		if (!path->nodes[level])
 			BUG();
 	}
-	return 0;
+	return ret;
 }
 
 /*
@@ -933,6 +1105,8 @@ int del_item(struct ctree_root *root, struct ctree_path *path)
 	struct tree_buffer *leaf_buf;
 	int doff;
 	int dsize;
+	int ret = 0;
+	int wret;
 
 	leaf_buf = path->nodes[0];
 	leaf = &leaf_buf->leaf;
@@ -959,14 +1133,23 @@ int del_item(struct ctree_root *root, struct ctree_path *path)
 			leaf->header.flags = node_level(0);
 			write_tree_block(root, leaf_buf);
 		} else {
-			del_ptr(root, path, 1);
+			wret = del_ptr(root, path, 1);
+			if (wret)
+				ret = wret;
 			free_extent(root, leaf_buf->blocknr, 1);
 		}
 	} else {
 		int used = leaf_space_used(leaf, 0, leaf->header.nritems);
-		if (slot == 0)
-			fixup_low_keys(root, path, &leaf->items[0].key, 1);
-		write_tree_block(root, leaf_buf);
+		if (slot == 0) {
+			wret = fixup_low_keys(root, path,
+						   &leaf->items[0].key, 1);
+			if (wret)
+				ret = wret;
+		}
+		wret = write_tree_block(root, leaf_buf);
+		if (wret)
+			ret = wret;
+
 		/* delete the leaf if it is mostly empty */
 		if (used < LEAF_DATA_SIZE / 3) {
 			/* push_leaf_left fixes the path.
@@ -975,13 +1158,20 @@ int del_item(struct ctree_root *root, struct ctree_path *path)
 			 */
 			slot = path->slots[1];
 			leaf_buf->count++;
-			push_leaf_left(root, path, 1);
-			if (leaf->header.nritems)
-				push_leaf_right(root, path, 1);
+			wret = push_leaf_left(root, path, 1);
+			if (wret < 0)
+				ret = wret;
+			if (leaf->header.nritems) {
+				wret = push_leaf_right(root, path, 1);
+				if (wret < 0)
+					ret = wret;
+			}
 			if (leaf->header.nritems == 0) {
 				u64 blocknr = leaf_buf->blocknr;
 				path->slots[1] = slot;
-				del_ptr(root, path, 1);
+				wret = del_ptr(root, path, 1);
+				if (wret)
+					ret = wret;
 				tree_block_release(root, leaf_buf);
 				free_extent(root, blocknr, 1);
 			} else {
@@ -989,7 +1179,7 @@ int del_item(struct ctree_root *root, struct ctree_path *path)
 			}
 		}
 	}
-	return 0;
+	return ret;
 }
 
 /*
@@ -1033,165 +1223,3 @@ int next_leaf(struct ctree_root *root, struct ctree_path *path)
 	return 0;
 }
 
-/* some sample code to insert,search & delete items */
-#if 0
-/* for testing only */
-int next_key(int i, int max_key) {
-	return rand() % max_key;
-	//return i;
-}
-int main() {
-	struct key ins;
-	struct key last = { (u64)-1, 0, 0};
-	char *buf;
-	int i;
-	int num;
-	int ret;
-	int run_size = 20000000;
-	int max_key =  100000000;
-	int tree_size = 0;
-	struct ctree_path path;
-	struct ctree_super_block super;
-	struct ctree_root *root;
-
-	radix_tree_init();
-
-
-	root = open_ctree("dbfile", &super);
-	srand(55);
-	for (i = 0; i < run_size; i++) {
-		buf = malloc(64);
-		num = next_key(i, max_key);
-		// num = i;
-		sprintf(buf, "string-%d", num);
-		if (i % 10000 == 0)
-			fprintf(stderr, "insert %d:%d\n", num, i);
-		ins.objectid = num;
-		ins.offset = 0;
-		ins.flags = 0;
-		ret = insert_item(root, &ins, buf, strlen(buf));
-		if (!ret)
-			tree_size++;
-		free(buf);
-	}
-	write_ctree_super(root, &super);
-	close_ctree(root);
-
-	root = open_ctree("dbfile", &super);
-	printf("starting search\n");
-	srand(55);
-	for (i = 0; i < run_size; i++) {
-		num = next_key(i, max_key);
-		ins.objectid = num;
-		init_path(&path);
-		if (i % 10000 == 0)
-			fprintf(stderr, "search %d:%d\n", num, i);
-		ret = search_slot(root, &ins, &path, 0);
-		if (ret) {
-			print_tree(root, root->node);
-			printf("unable to find %d\n", num);
-			exit(1);
-		}
-		release_path(root, &path);
-	}
-	write_ctree_super(root, &super);
-	close_ctree(root);
-	root = open_ctree("dbfile", &super);
-	printf("node %p level %d total ptrs %d free spc %lu\n", root->node,
-	        node_level(root->node->node.header.flags),
-		root->node->node.header.nritems,
-		NODEPTRS_PER_BLOCK - root->node->node.header.nritems);
-	printf("all searches good, deleting some items\n");
-	i = 0;
-	srand(55);
-	for (i = 0 ; i < run_size/4; i++) {
-		num = next_key(i, max_key);
-		ins.objectid = num;
-		init_path(&path);
-		ret = search_slot(root, &ins, &path, -1);
-		if (!ret) {
-			if (i % 10000 == 0)
-				fprintf(stderr, "del %d:%d\n", num, i);
-			ret = del_item(root, &path);
-			if (ret != 0)
-				BUG();
-			tree_size--;
-		}
-		release_path(root, &path);
-	}
-	write_ctree_super(root, &super);
-	close_ctree(root);
-	root = open_ctree("dbfile", &super);
-	srand(128);
-	for (i = 0; i < run_size; i++) {
-		buf = malloc(64);
-		num = next_key(i, max_key);
-		sprintf(buf, "string-%d", num);
-		ins.objectid = num;
-		if (i % 10000 == 0)
-			fprintf(stderr, "insert %d:%d\n", num, i);
-		ret = insert_item(root, &ins, buf, strlen(buf));
-		if (!ret)
-			tree_size++;
-		free(buf);
-	}
-	write_ctree_super(root, &super);
-	close_ctree(root);
-	root = open_ctree("dbfile", &super);
-	srand(128);
-	printf("starting search2\n");
-	for (i = 0; i < run_size; i++) {
-		num = next_key(i, max_key);
-		ins.objectid = num;
-		init_path(&path);
-		if (i % 10000 == 0)
-			fprintf(stderr, "search %d:%d\n", num, i);
-		ret = search_slot(root, &ins, &path, 0);
-		if (ret) {
-			print_tree(root, root->node);
-			printf("unable to find %d\n", num);
-			exit(1);
-		}
-		release_path(root, &path);
-	}
-	printf("starting big long delete run\n");
-	while(root->node && root->node->node.header.nritems > 0) {
-		struct leaf *leaf;
-		int slot;
-		ins.objectid = (u64)-1;
-		init_path(&path);
-		ret = search_slot(root, &ins, &path, -1);
-		if (ret == 0)
-			BUG();
-
-		leaf = &path.nodes[0]->leaf;
-		slot = path.slots[0];
-		if (slot != leaf->header.nritems)
-			BUG();
-		while(path.slots[0] > 0) {
-			path.slots[0] -= 1;
-			slot = path.slots[0];
-			leaf = &path.nodes[0]->leaf;
-
-			if (comp_keys(&last, &leaf->items[slot].key) <= 0)
-				BUG();
-			memcpy(&last, &leaf->items[slot].key, sizeof(last));
-			if (tree_size % 10000 == 0)
-				printf("big del %d:%d\n", tree_size, i);
-			ret = del_item(root, &path);
-			if (ret != 0) {
-				printf("del_item returned %d\n", ret);
-				BUG();
-			}
-			tree_size--;
-		}
-		release_path(root, &path);
-	}
-	printf("tree size is now %d\n", tree_size);
-	printf("map tree\n");
-	print_tree(root->extent_root, root->extent_root->node);
-	write_ctree_super(root, &super);
-	close_ctree(root);
-	return 0;
-}
-#endif

commit 8e19f2cd452853d1ca0895c2488c24d40de3d255
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Feb 28 09:27:02 2007 -0500

    Btrfs: Take out the merge-during-search-on-delete code, it is buggy.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 25dc7b2f7426..0aea94224ba3 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -167,29 +167,6 @@ int search_slot(struct ctree_root *root, struct key *key,
 				b = p->nodes[level];
 				c = &b->node;
 				slot = p->slots[level];
-			} else if (ins_len < 0 &&
-				   c->header.nritems <= NODEPTRS_PER_BLOCK/4) {
-				u64 blocknr = b->blocknr;
-				slot = p->slots[level +1];
-				b->count++;
-				if (push_node_left(root, p, level))
-					push_node_right(root, p, level);
-				if (c->header.nritems == 0 &&
-				    level < MAX_LEVEL - 1 &&
-				    p->nodes[level + 1]) {
-					int tslot = p->slots[level + 1];
-
-					p->slots[level + 1] = slot;
-					del_ptr(root, p, level + 1);
-					p->slots[level + 1] = tslot;
-					tree_block_release(root, b);
-					free_extent(root, blocknr, 1);
-				} else {
-					tree_block_release(root, b);
-				}
-				b = p->nodes[level];
-				c = &b->node;
-				slot = p->slots[level];
 			}
 			b = read_tree_block(root, c->blockptrs[slot]);
 			continue;
@@ -618,7 +595,6 @@ int push_leaf_right(struct ctree_root *root, struct ctree_path *path,
 		&right->items[0].key, sizeof(struct key));
 	write_tree_block(root, upper);
 	/* then fixup the leaf pointer in the path */
-	// FIXME use nritems in here somehow
 	if (path->slots[0] >= left->header.nritems) {
 		path->slots[0] -= left->header.nritems;
 		tree_block_release(root, path->nodes[0]);
@@ -847,8 +823,6 @@ int insert_item(struct ctree_root *root, struct key *key,
 
 	slot = path.slots[0];
 	BUG_ON(slot < 0);
-	if (slot == 0)
-		fixup_low_keys(root, &path, key, 1);
 	if (slot != nritems) {
 		int i;
 		unsigned int old_data = leaf->items[slot].offset +
@@ -877,6 +851,8 @@ int insert_item(struct ctree_root *root, struct key *key,
 	memcpy(leaf->data + data_end - data_size, data, data_size);
 	leaf->header.nritems += 1;
 	write_tree_block(root, leaf_buf);
+	if (slot == 0)
+		fixup_low_keys(root, &path, key, 1);
 	if (leaf_free_space(leaf) < 0)
 		BUG();
 	release_path(root, &path);
@@ -914,13 +890,23 @@ int del_ptr(struct ctree_root *root, struct ctree_path *path, int level)
 				sizeof(u64) * (nritems - slot - 1));
 		}
 		node->header.nritems--;
-		write_tree_block(root, t);
 		blocknr = t->blocknr;
+		write_tree_block(root, t);
 		if (node->header.nritems != 0) {
+			int tslot;
 			if (slot == 0)
 				fixup_low_keys(root, path, node->keys,
 					       level + 1);
-			break;
+			tslot = path->slots[level + 1];
+			t->count++;
+			if (push_node_left(root, path, level))
+				push_node_right(root, path, level);
+			path->slots[level + 1] = tslot;
+			if (node->header.nritems != 0) {
+				tree_block_release(root, t);
+				break;
+			}
+			tree_block_release(root, t);
 		}
 		if (t == root->node) {
 			/* just turn the root into a leaf and break */

commit fec577fb7f516e0d12ff821b1af272fd754e120a
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Feb 26 10:40:21 2007 -0500

    Btrfs: Add fsx-style randomized tree tester
    Add debug-tree command to print the tree
    Add extent-tree.c to the repo
    Comment ctree.h
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 7645ab3259ea..25dc7b2f7426 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1047,14 +1047,14 @@ int next_leaf(struct ctree_root *root, struct ctree_path *path)
 	return 0;
 }
 
+/* some sample code to insert,search & delete items */
+#if 0
 /* for testing only */
 int next_key(int i, int max_key) {
 	return rand() % max_key;
 	//return i;
 }
-
 int main() {
-	struct ctree_root *root;
 	struct key ins;
 	struct key last = { (u64)-1, 0, 0};
 	char *buf;
@@ -1066,6 +1066,7 @@ int main() {
 	int tree_size = 0;
 	struct ctree_path path;
 	struct ctree_super_block super;
+	struct ctree_root *root;
 
 	radix_tree_init();
 
@@ -1207,3 +1208,4 @@ int main() {
 	close_ctree(root);
 	return 0;
 }
+#endif

commit 97571fd0c939be8ae9cb57a8c57430a244ce13ae
Author: Chris Mason <chris.mason@oracle.com>
Date:   Sat Feb 24 13:39:08 2007 -0500

    Btrfs: cleanup & comment
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index ef8bfa837532..7645ab3259ea 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -110,6 +110,10 @@ int generic_bin_search(char *p, int item_size, struct key *key,
 	return 1;
 }
 
+/*
+ * simple bin_search frontend that does the right thing for
+ * leaves vs nodes
+ */
 int bin_search(struct node *c, struct key *key, int *slot)
 {
 	if (is_leaf(c->header.flags)) {
@@ -130,6 +134,10 @@ int bin_search(struct node *c, struct key *key, int *slot)
  *
  * If the key isn't found, the path points to the slot where it should
  * be inserted.
+ *
+ * if ins_len > 0, nodes and leaves will be split as we walk down the
+ * tree.  if ins_len < 0, nodes will be merged as we walk down the tree (if
+ * possible)
  */
 int search_slot(struct ctree_root *root, struct key *key,
 		struct ctree_path *p, int ins_len)
@@ -379,6 +387,11 @@ int push_node_right(struct ctree_root *root, struct ctree_path *path, int level)
 	return 0;
 }
 
+/*
+ * helper function to insert a new root level in the tree.
+ * A new node is allocated, and a single item is inserted to
+ * point to the existing root
+ */
 static int insert_new_root(struct ctree_root *root,
 			   struct ctree_path *path, int level)
 {
@@ -417,6 +430,7 @@ static int insert_new_root(struct ctree_root *root,
 /*
  * worker function to insert a single pointer in a node.
  * the node should have enough room for the pointer already
+ *
  * slot and level indicate where you want the key to go, and
  * blocknr is the block the key points to.
  */
@@ -449,6 +463,13 @@ int insert_ptr(struct ctree_root *root,
 	return 0;
 }
 
+/*
+ * split the node at the specified level in path in two.
+ * The path is corrected to point to the appropriate node after the split
+ *
+ * Before splitting this tries to make some room in the node by pushing
+ * left and right, if either one works, it returns right away.
+ */
 int split_node(struct ctree_root *root, struct ctree_path *path, int level)
 {
 	struct tree_buffer *t;
@@ -744,10 +765,12 @@ int split_leaf(struct ctree_root *root, struct ctree_path *path, int data_size)
 	right = &right_buffer->leaf;
 	memset(right, 0, sizeof(*right));
 	if (mid <= slot) {
+		/* FIXME, just alloc a new leaf here */
 		if (leaf_space_used(l, mid, nritems - mid) + space_needed >
 			LEAF_DATA_SIZE)
 			BUG();
 	} else {
+		/* FIXME, just alloc a new leaf here */
 		if (leaf_space_used(l, 0, mid + 1) + space_needed >
 			LEAF_DATA_SIZE)
 			BUG();
@@ -983,6 +1006,10 @@ int del_item(struct ctree_root *root, struct ctree_path *path)
 	return 0;
 }
 
+/*
+ * walk up the tree as far as required to find the next leaf.
+ * returns 0 if it found something or -1 if there are no greater leaves.
+ */
 int next_leaf(struct ctree_root *root, struct ctree_path *path)
 {
 	int slot;
@@ -1044,7 +1071,6 @@ int main() {
 
 
 	root = open_ctree("dbfile", &super);
-
 	srand(55);
 	for (i = 0; i < run_size; i++) {
 		buf = malloc(64);

commit 00ec4c5161e0adcf8be3cd844cb40239dc393d70
Author: Chris Mason <chris.mason@oracle.com>
Date:   Sat Feb 24 12:47:20 2007 -0500

    Btrfs: push_leaf_right
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index e497fd963118..ef8bfa837532 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -517,6 +517,97 @@ int leaf_space_used(struct leaf *l, int start, int nr)
 	return data_len;
 }
 
+/*
+ * push some data in the path leaf to the right, trying to free up at
+ * least data_size bytes.  returns zero if the push worked, nonzero otherwise
+ */
+int push_leaf_right(struct ctree_root *root, struct ctree_path *path,
+		   int data_size)
+{
+	struct tree_buffer *left_buf = path->nodes[0];
+	struct leaf *left = &left_buf->leaf;
+	struct leaf *right;
+	struct tree_buffer *right_buf;
+	struct tree_buffer *upper;
+	int slot;
+	int i;
+	int free_space;
+	int push_space = 0;
+	int push_items = 0;
+	struct item *item;
+
+	slot = path->slots[1];
+	if (!path->nodes[1]) {
+		return 1;
+	}
+	upper = path->nodes[1];
+	if (slot >= upper->node.header.nritems - 1) {
+		return 1;
+	}
+	right_buf = read_tree_block(root, upper->node.blockptrs[slot + 1]);
+	right = &right_buf->leaf;
+	free_space = leaf_free_space(right);
+	if (free_space < data_size + sizeof(struct item)) {
+		tree_block_release(root, right_buf);
+		return 1;
+	}
+	for (i = left->header.nritems - 1; i >= 0; i--) {
+		item = left->items + i;
+		if (path->slots[0] == i)
+			push_space += data_size + sizeof(*item);
+		if (item->size + sizeof(*item) + push_space > free_space)
+			break;
+		push_items++;
+		push_space += item->size + sizeof(*item);
+	}
+	if (push_items == 0) {
+		tree_block_release(root, right_buf);
+		return 1;
+	}
+	/* push left to right */
+	push_space = left->items[left->header.nritems - push_items].offset +
+		     left->items[left->header.nritems - push_items].size;
+	push_space -= leaf_data_end(left);
+	/* make room in the right data area */
+	memmove(right->data + leaf_data_end(right) - push_space,
+		right->data + leaf_data_end(right),
+		LEAF_DATA_SIZE - leaf_data_end(right));
+	/* copy from the left data area */
+	memcpy(right->data + LEAF_DATA_SIZE - push_space,
+		left->data + leaf_data_end(left),
+		push_space);
+	memmove(right->items + push_items, right->items,
+		right->header.nritems * sizeof(struct item));
+	/* copy the items from left to right */
+	memcpy(right->items, left->items + left->header.nritems - push_items,
+		push_items * sizeof(struct item));
+
+	/* update the item pointers */
+	right->header.nritems += push_items;
+	push_space = LEAF_DATA_SIZE;
+	for (i = 0; i < right->header.nritems; i++) {
+		right->items[i].offset = push_space - right->items[i].size;
+		push_space = right->items[i].offset;
+	}
+	left->header.nritems -= push_items;
+
+	write_tree_block(root, left_buf);
+	write_tree_block(root, right_buf);
+	memcpy(upper->node.keys + slot + 1,
+		&right->items[0].key, sizeof(struct key));
+	write_tree_block(root, upper);
+	/* then fixup the leaf pointer in the path */
+	// FIXME use nritems in here somehow
+	if (path->slots[0] >= left->header.nritems) {
+		path->slots[0] -= left->header.nritems;
+		tree_block_release(root, path->nodes[0]);
+		path->nodes[0] = right_buf;
+		path->slots[1] += 1;
+	} else {
+		tree_block_release(root, right_buf);
+	}
+	return 0;
+}
 /*
  * push some data in the path leaf to the left, trying to free up at
  * least data_size bytes.  returns zero if the push worked, nonzero otherwise
@@ -631,7 +722,8 @@ int split_leaf(struct ctree_root *root, struct ctree_path *path, int data_size)
 	int i;
 	int ret;
 
-	if (push_leaf_left(root, path, data_size) == 0) {
+	if (push_leaf_left(root, path, data_size) == 0 ||
+	    push_leaf_right(root, path, data_size) == 0) {
 		l_buf = path->nodes[0];
 		l = &l_buf->leaf;
 		if (leaf_free_space(l) >= sizeof(struct item) + data_size)
@@ -875,6 +967,8 @@ int del_item(struct ctree_root *root, struct ctree_path *path)
 			slot = path->slots[1];
 			leaf_buf->count++;
 			push_leaf_left(root, path, 1);
+			if (leaf->header.nritems)
+				push_leaf_right(root, path, 1);
 			if (leaf->header.nritems == 0) {
 				u64 blocknr = leaf_buf->blocknr;
 				path->slots[1] = slot;
@@ -929,7 +1023,7 @@ int next_leaf(struct ctree_root *root, struct ctree_path *path)
 /* for testing only */
 int next_key(int i, int max_key) {
 	return rand() % max_key;
-	// return i;
+	//return i;
 }
 
 int main() {
@@ -958,7 +1052,7 @@ int main() {
 		// num = i;
 		sprintf(buf, "string-%d", num);
 		if (i % 10000 == 0)
-			printf("insert %d:%d\n", num, i);
+			fprintf(stderr, "insert %d:%d\n", num, i);
 		ins.objectid = num;
 		ins.offset = 0;
 		ins.flags = 0;
@@ -978,7 +1072,7 @@ int main() {
 		ins.objectid = num;
 		init_path(&path);
 		if (i % 10000 == 0)
-			printf("search %d:%d\n", num, i);
+			fprintf(stderr, "search %d:%d\n", num, i);
 		ret = search_slot(root, &ins, &path, 0);
 		if (ret) {
 			print_tree(root, root->node);
@@ -1004,7 +1098,7 @@ int main() {
 		ret = search_slot(root, &ins, &path, -1);
 		if (!ret) {
 			if (i % 10000 == 0)
-				printf("del %d:%d\n", num, i);
+				fprintf(stderr, "del %d:%d\n", num, i);
 			ret = del_item(root, &path);
 			if (ret != 0)
 				BUG();
@@ -1022,7 +1116,7 @@ int main() {
 		sprintf(buf, "string-%d", num);
 		ins.objectid = num;
 		if (i % 10000 == 0)
-			printf("insert %d:%d\n", num, i);
+			fprintf(stderr, "insert %d:%d\n", num, i);
 		ret = insert_item(root, &ins, buf, strlen(buf));
 		if (!ret)
 			tree_size++;
@@ -1038,7 +1132,7 @@ int main() {
 		ins.objectid = num;
 		init_path(&path);
 		if (i % 10000 == 0)
-			printf("search %d:%d\n", num, i);
+			fprintf(stderr, "search %d:%d\n", num, i);
 		ret = search_slot(root, &ins, &path, 0);
 		if (ret) {
 			print_tree(root, root->node);
@@ -1082,6 +1176,7 @@ int main() {
 	}
 	printf("tree size is now %d\n", tree_size);
 	printf("map tree\n");
+	print_tree(root->extent_root, root->extent_root->node);
 	write_ctree_super(root, &super);
 	close_ctree(root);
 	return 0;

commit 5de08d7d50ba535b968d97101ed2ab07c2b8eb7d
Author: Chris Mason <chris.mason@oracle.com>
Date:   Sat Feb 24 06:24:44 2007 -0500

    Btrfs: Break up ctree.c a little
    Extent fixes
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index f0abcf1f3939..e497fd963118 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -4,23 +4,21 @@
 #include "radix-tree.h"
 #include "ctree.h"
 #include "disk-io.h"
-
-#define SEARCH_READ 0
-#define SEARCH_WRITE 1
-
-#define CTREE_EXTENT_PENDING 0
+#include "print-tree.h"
 
 int split_node(struct ctree_root *root, struct ctree_path *path, int level);
 int split_leaf(struct ctree_root *root, struct ctree_path *path, int data_size);
-struct tree_buffer *alloc_free_block(struct ctree_root *root);
-int free_extent(struct ctree_root *root, u64 blocknr, u64 num_blocks);
+int push_node_left(struct ctree_root *root, struct ctree_path *path, int level);
+int push_node_right(struct ctree_root *root,
+		    struct ctree_path *path, int level);
+int del_ptr(struct ctree_root *root, struct ctree_path *path, int level);
 
-static inline void init_path(struct ctree_path *p)
+inline void init_path(struct ctree_path *p)
 {
 	memset(p, 0, sizeof(*p));
 }
 
-static void release_path(struct ctree_root *root, struct ctree_path *p)
+void release_path(struct ctree_root *root, struct ctree_path *p)
 {
 	int i;
 	for (i = 0; i < MAX_LEVEL; i++) {
@@ -48,7 +46,7 @@ static inline unsigned int leaf_data_end(struct leaf *leaf)
  * the start of the leaf data.  IOW, how much room
  * the leaf has left for both items and data
  */
-static inline int leaf_free_space(struct leaf *leaf)
+int leaf_free_space(struct leaf *leaf)
 {
 	int data_end = leaf_data_end(leaf);
 	int nritems = leaf->header.nritems;
@@ -133,7 +131,8 @@ int bin_search(struct node *c, struct key *key, int *slot)
  * If the key isn't found, the path points to the slot where it should
  * be inserted.
  */
-int search_slot(struct ctree_root *root, struct key *key, struct ctree_path *p, int ins_len)
+int search_slot(struct ctree_root *root, struct key *key,
+		struct ctree_path *p, int ins_len)
 {
 	struct tree_buffer *b = root->node;
 	struct node *c;
@@ -151,7 +150,8 @@ int search_slot(struct ctree_root *root, struct key *key, struct ctree_path *p,
 			if (ret && slot > 0)
 				slot -= 1;
 			p->slots[level] = slot;
-			if (ins_len && c->header.nritems == NODEPTRS_PER_BLOCK) {
+			if (ins_len > 0 &&
+			    c->header.nritems == NODEPTRS_PER_BLOCK) {
 				int sret = split_node(root, p, level);
 				BUG_ON(sret > 0);
 				if (sret)
@@ -159,13 +159,37 @@ int search_slot(struct ctree_root *root, struct key *key, struct ctree_path *p,
 				b = p->nodes[level];
 				c = &b->node;
 				slot = p->slots[level];
+			} else if (ins_len < 0 &&
+				   c->header.nritems <= NODEPTRS_PER_BLOCK/4) {
+				u64 blocknr = b->blocknr;
+				slot = p->slots[level +1];
+				b->count++;
+				if (push_node_left(root, p, level))
+					push_node_right(root, p, level);
+				if (c->header.nritems == 0 &&
+				    level < MAX_LEVEL - 1 &&
+				    p->nodes[level + 1]) {
+					int tslot = p->slots[level + 1];
+
+					p->slots[level + 1] = slot;
+					del_ptr(root, p, level + 1);
+					p->slots[level + 1] = tslot;
+					tree_block_release(root, b);
+					free_extent(root, blocknr, 1);
+				} else {
+					tree_block_release(root, b);
+				}
+				b = p->nodes[level];
+				c = &b->node;
+				slot = p->slots[level];
 			}
 			b = read_tree_block(root, c->blockptrs[slot]);
 			continue;
 		} else {
 			struct leaf *l = (struct leaf *)c;
 			p->slots[level] = slot;
-			if (ins_len && leaf_free_space(l) <  sizeof(struct item) + ins_len) {
+			if (ins_len > 0 && leaf_free_space(l) <
+			    sizeof(struct item) + ins_len) {
 				int sret = split_leaf(root, p, ins_len);
 				BUG_ON(sret > 0);
 				if (sret)
@@ -355,7 +379,8 @@ int push_node_right(struct ctree_root *root, struct ctree_path *path, int level)
 	return 0;
 }
 
-static int insert_new_root(struct ctree_root *root, struct ctree_path *path, int level)
+static int insert_new_root(struct ctree_root *root,
+			   struct ctree_path *path, int level)
 {
 	struct tree_buffer *t;
 	struct node *lower;
@@ -463,7 +488,7 @@ int split_node(struct ctree_root *root, struct ctree_path *path, int level)
 	write_tree_block(root, split_buffer);
 	insert_ptr(root, path, split->keys, split_buffer->blocknr,
 		     path->slots[level + 1] + 1, level + 1);
-	if (path->slots[level] > mid) {
+	if (path->slots[level] >= mid) {
 		path->slots[level] -= mid;
 		tree_block_release(root, t);
 		path->nodes[level] = split_buffer;
@@ -744,8 +769,7 @@ int insert_item(struct ctree_root *root, struct key *key,
 }
 
 /*
- * delete the pointer from a given level in the path.  The path is not
- * fixed up, so after calling this it is not valid at that level.
+ * delete the pointer from a given node.
  *
  * If the delete empties a node, the node is removed from the tree,
  * continuing all the way the root if required.  The root is converted into
@@ -778,22 +802,10 @@ int del_ptr(struct ctree_root *root, struct ctree_path *path, int level)
 		write_tree_block(root, t);
 		blocknr = t->blocknr;
 		if (node->header.nritems != 0) {
-			int tslot;
 			if (slot == 0)
 				fixup_low_keys(root, path, node->keys,
 					       level + 1);
-			tslot = path->slots[level+1];
-			t->count++;
-			push_node_left(root, path, level);
-			if (node->header.nritems) {
-				push_node_right(root, path, level);
-			}
-			if (node->header.nritems) {
-				tree_block_release(root, t);
-				break;
-			}
-			tree_block_release(root, t);
-			path->slots[level+1] = tslot;
+			break;
 		}
 		if (t == root->node) {
 			/* just turn the root into a leaf and break */
@@ -850,12 +862,12 @@ int del_item(struct ctree_root *root, struct ctree_path *path)
 			free_extent(root, leaf_buf->blocknr, 1);
 		}
 	} else {
+		int used = leaf_space_used(leaf, 0, leaf->header.nritems);
 		if (slot == 0)
 			fixup_low_keys(root, path, &leaf->items[0].key, 1);
 		write_tree_block(root, leaf_buf);
 		/* delete the leaf if it is mostly empty */
-		if (leaf_space_used(leaf, 0, leaf->header.nritems) <
-		    LEAF_DATA_SIZE / 4) {
+		if (used < LEAF_DATA_SIZE / 3) {
 			/* push_leaf_left fixes the path.
 			 * make sure the path still points to our leaf
 			 * for possible call to del_ptr below
@@ -864,81 +876,19 @@ int del_item(struct ctree_root *root, struct ctree_path *path)
 			leaf_buf->count++;
 			push_leaf_left(root, path, 1);
 			if (leaf->header.nritems == 0) {
+				u64 blocknr = leaf_buf->blocknr;
 				path->slots[1] = slot;
 				del_ptr(root, path, 1);
+				tree_block_release(root, leaf_buf);
+				free_extent(root, blocknr, 1);
+			} else {
+				tree_block_release(root, leaf_buf);
 			}
-			tree_block_release(root, leaf_buf);
 		}
 	}
 	return 0;
 }
 
-static int del_pending_extents(struct ctree_root *extent_root)
-{
-	int ret;
-	struct key key;
-	struct tree_buffer *gang[4];
-	int i;
-	struct ctree_path path;
-
-	while(1) {
-		ret = radix_tree_gang_lookup_tag(&extent_root->cache_radix,
-						 (void **)gang, 0, ARRAY_SIZE(gang),
-						 CTREE_EXTENT_PENDING);
-		if (!ret)
-			break;
-		for (i = 0; i < ret; i++) {
-			key.objectid = gang[i]->blocknr;
-			key.flags = 0;
-			key.offset = 1;
-			init_path(&path);
-			ret = search_slot(extent_root, &key, &path, 0);
-			if (ret) {
-				BUG();
-				// FIXME undo it and return sane
-				return ret;
-			}
-			ret = del_item(extent_root, &path);
-			if (ret) {
-				BUG();
-				return ret;
-			}
-			release_path(extent_root, &path);
-			radix_tree_tag_clear(&extent_root->cache_radix, gang[i]->blocknr,
-						CTREE_EXTENT_PENDING);
-			tree_block_release(extent_root, gang[i]);
-		}
-	}
-	return 0;
-}
-
-int free_extent(struct ctree_root *root, u64 blocknr, u64 num_blocks)
-{
-	struct ctree_path path;
-	struct key key;
-	struct ctree_root *extent_root = root->extent_root;
-	struct tree_buffer *t;
-	int pending_ret;
-	int ret;
-
-	key.objectid = blocknr;
-	key.flags = 0;
-	key.offset = num_blocks;
-	if (root == extent_root) {
-		t = read_tree_block(root, key.objectid);
-		radix_tree_tag_set(&root->cache_radix, key.objectid, CTREE_EXTENT_PENDING);
-		return 0;
-	}
-	init_path(&path);
-	ret = search_slot(extent_root, &key, &path, 0);
-	if (ret)
-		BUG();
-	ret = del_item(extent_root, &path);
-	release_path(extent_root, &path);
-	pending_ret = del_pending_extents(root->extent_root);
-	return ret ? ret : pending_ret;
-}
-
 int next_leaf(struct ctree_root *root, struct ctree_path *path)
 {
 	int slot;
@@ -976,241 +926,10 @@ int next_leaf(struct ctree_root *root, struct ctree_path *path)
 	return 0;
 }
 
-int find_free_extent(struct ctree_root *orig_root, u64 num_blocks, u64 search_start,
-			 u64 search_end, struct key *ins)
-{
-	struct ctree_path path;
-	struct key *key;
-	int ret;
-	u64 hole_size = 0;
-	int slot = 0;
-	u64 last_block;
-	int start_found = 0;
-	struct leaf *l;
-	struct ctree_root * root = orig_root->extent_root;
-
-	init_path(&path);
-	ins->objectid = search_start;
-	ins->offset = 0;
-	ins->flags = 0;
-	ret = search_slot(root, ins, &path, 0);
-	while (1) {
-		l = &path.nodes[0]->leaf;
-		slot = path.slots[0];
-		if (!l) {
-			// FIXME allocate root
-		}
-		if (slot >= l->header.nritems) {
-			ret = next_leaf(root, &path);
-			if (ret == 0)
-				continue;
-			if (!start_found) {
-				ins->objectid = search_start;
-				ins->offset = num_blocks;
-				hole_size = search_end - search_start;
-				start_found = 1;
-				goto insert;
-			}
-			ins->objectid = last_block;
-			ins->offset = num_blocks;
-			hole_size = search_end - last_block;
-			goto insert;
-		}
-		key = &l->items[slot].key;
-		if (start_found) {
-			hole_size = key->objectid - last_block;
-			if (hole_size > num_blocks) {
-				ins->objectid = last_block;
-				ins->offset = num_blocks;
-				goto insert;
-			}
-		} else
-			start_found = 1;
-		last_block = key->objectid + key->offset;
-insert_failed:
-		path.slots[0]++;
-	}
-	// FIXME -ENOSPC
-insert:
-	if (orig_root->extent_root == orig_root) {
-		BUG_ON(num_blocks != 1);
-		if ((root->current_insert.objectid <= ins->objectid &&
-		    root->current_insert.objectid + root->current_insert.offset >
-		    ins->objectid) ||
-		   (root->current_insert.objectid > ins->objectid &&
-		    root->current_insert.objectid <= ins->objectid + ins->offset) ||
-		   radix_tree_tag_get(&root->cache_radix, ins->objectid,
-				      CTREE_EXTENT_PENDING)) {
-			last_block = ins->objectid + 1;
-			search_start = last_block;
-			goto insert_failed;
-		}
-	}
-	release_path(root, &path);
-	if (ins->offset != 1)
-		BUG();
-	return 0;
-}
-
-static int insert_pending_extents(struct ctree_root *extent_root)
-{
-	int ret;
-	struct key key;
-	struct extent_item item;
-	struct tree_buffer *gang[4];
-	int i;
-
-	// FIXME -ENOSPC
-	item.refs = 1;
-	item.owner = extent_root->node->node.header.parentid;
-	while(1) {
-		ret = radix_tree_gang_lookup_tag(&extent_root->cache_radix,
-						 (void **)gang, 0, ARRAY_SIZE(gang),
-						 CTREE_EXTENT_PENDING);
-		if (!ret)
-			break;
-		for (i = 0; i < ret; i++) {
-			key.objectid = gang[i]->blocknr;
-			key.flags = 0;
-			key.offset = 1;
-			ret = insert_item(extent_root, &key, &item, sizeof(item));
-			if (ret) {
-				BUG();
-				// FIXME undo it and return sane
-				return ret;
-			}
-			radix_tree_tag_clear(&extent_root->cache_radix, gang[i]->blocknr,
-						CTREE_EXTENT_PENDING);
-			tree_block_release(extent_root, gang[i]);
-		}
-	}
-	return 0;
-}
-
-int alloc_extent(struct ctree_root *root, u64 num_blocks, u64 search_start,
-			 u64 search_end, u64 owner, struct key *ins, struct tree_buffer **buf)
-{
-	int ret;
-	int pending_ret;
-	struct extent_item extent_item;
-
-	extent_item.refs = 1;
-	extent_item.owner = owner;
-
-	ret = find_free_extent(root, num_blocks, search_start, search_end, ins);
-	if (ret)
-		return ret;
-
-	if (root != root->extent_root) {
-		memcpy(&root->extent_root->current_insert, ins, sizeof(*ins));
-		ret = insert_item(root->extent_root, ins, &extent_item, sizeof(extent_item));
-		memset(&root->extent_root->current_insert, 0, sizeof(struct key));
-		pending_ret = insert_pending_extents(root->extent_root);
-		if (ret)
-			return ret;
-		if (pending_ret)
-			return pending_ret;
-		*buf = find_tree_block(root, ins->objectid);
-		return 0;
-	}
-	/* we're allocating an extent for the extent tree, don't recurse */
-	BUG_ON(ins->offset != 1);
-	*buf = find_tree_block(root, ins->objectid);
-	BUG_ON(!*buf);
-	radix_tree_tag_set(&root->cache_radix, ins->objectid, CTREE_EXTENT_PENDING);
-	(*buf)->count++;
-	return 0;
-
-}
-
-struct tree_buffer *alloc_free_block(struct ctree_root *root)
-{
-	struct key ins;
-	int ret;
-	struct tree_buffer *buf = NULL;
-
-	ret = alloc_extent(root, 1, 0, (unsigned long)-1, root->node->node.header.parentid,
-			   &ins, &buf);
-
-	if (ret) {
-		BUG();
-		return NULL;
-	}
-	if (root != root->extent_root)
-		BUG_ON(radix_tree_tag_get(&root->extent_root->cache_radix, buf->blocknr,
-					  CTREE_EXTENT_PENDING));
-	return buf;
-}
-
-void print_leaf(struct leaf *l)
-{
-	int i;
-	int nr = l->header.nritems;
-	struct item *item;
-	struct extent_item *ei;
-	printf("leaf %lu total ptrs %d free space %d\n", l->header.blocknr, nr,
-	       leaf_free_space(l));
-	fflush(stdout);
-	for (i = 0 ; i < nr ; i++) {
-		item = l->items + i;
-		printf("\titem %d key (%lu %u %lu) itemoff %d itemsize %d\n",
-			i,
-			item->key.objectid, item->key.flags, item->key.offset,
-			item->offset, item->size);
-		fflush(stdout);
-		printf("\t\titem data %.*s\n", item->size, l->data+item->offset);
-		ei = (struct extent_item *)(l->data + item->offset);
-		printf("\t\textent data %u %lu\n", ei->refs, ei->owner);
-		fflush(stdout);
-	}
-}
-void print_tree(struct ctree_root *root, struct tree_buffer *t)
-{
-	int i;
-	int nr;
-	struct node *c;
-
-	if (!t)
-		return;
-	c = &t->node;
-	nr = c->header.nritems;
-	if (c->header.blocknr != t->blocknr)
-		BUG();
-	if (is_leaf(c->header.flags)) {
-		print_leaf((struct leaf *)c);
-		return;
-	}
-	printf("node %lu level %d total ptrs %d free spc %lu\n", t->blocknr,
-	        node_level(c->header.flags), c->header.nritems,
-		NODEPTRS_PER_BLOCK - c->header.nritems);
-	fflush(stdout);
-	for (i = 0; i < nr; i++) {
-		printf("\tkey %d (%lu %u %lu) block %lu\n",
-		       i,
-		       c->keys[i].objectid, c->keys[i].flags, c->keys[i].offset,
-		       c->blockptrs[i]);
-		fflush(stdout);
-	}
-	for (i = 0; i < nr; i++) {
-		struct tree_buffer *next_buf = read_tree_block(root,
-							    c->blockptrs[i]);
-		struct node *next = &next_buf->node;
-		if (is_leaf(next->header.flags) &&
-		    node_level(c->header.flags) != 1)
-			BUG();
-		if (node_level(next->header.flags) !=
-			node_level(c->header.flags) - 1)
-			BUG();
-		print_tree(root, next_buf);
-		tree_block_release(root, next_buf);
-	}
-
-}
-
 /* for testing only */
 int next_key(int i, int max_key) {
-	// return rand() % max_key;
-	return i;
+	return rand() % max_key;
+	// return i;
 }
 
 int main() {
@@ -1221,8 +940,8 @@ int main() {
 	int i;
 	int num;
 	int ret;
-	int run_size = 10000;
-	int max_key = 100000000;
+	int run_size = 20000000;
+	int max_key =  100000000;
 	int tree_size = 0;
 	struct ctree_path path;
 	struct ctree_super_block super;
@@ -1231,11 +950,6 @@ int main() {
 
 
 	root = open_ctree("dbfile", &super);
-	printf("root tree\n");
-	print_tree(root, root->node);
-	printf("map tree\n");
-	print_tree(root->extent_root, root->extent_root->node);
-	fflush(stdout);
 
 	srand(55);
 	for (i = 0; i < run_size; i++) {
@@ -1243,13 +957,15 @@ int main() {
 		num = next_key(i, max_key);
 		// num = i;
 		sprintf(buf, "string-%d", num);
-		// printf("insert %d\n", num);
+		if (i % 10000 == 0)
+			printf("insert %d:%d\n", num, i);
 		ins.objectid = num;
 		ins.offset = 0;
 		ins.flags = 0;
 		ret = insert_item(root, &ins, buf, strlen(buf));
 		if (!ret)
 			tree_size++;
+		free(buf);
 	}
 	write_ctree_super(root, &super);
 	close_ctree(root);
@@ -1261,6 +977,8 @@ int main() {
 		num = next_key(i, max_key);
 		ins.objectid = num;
 		init_path(&path);
+		if (i % 10000 == 0)
+			printf("search %d:%d\n", num, i);
 		ret = search_slot(root, &ins, &path, 0);
 		if (ret) {
 			print_tree(root, root->node);
@@ -1283,39 +1001,32 @@ int main() {
 		num = next_key(i, max_key);
 		ins.objectid = num;
 		init_path(&path);
-		ret = search_slot(root, &ins, &path, 0);
-		if (ret)
-			continue;
-		ret = del_item(root, &path);
-		if (ret != 0)
-			BUG();
+		ret = search_slot(root, &ins, &path, -1);
+		if (!ret) {
+			if (i % 10000 == 0)
+				printf("del %d:%d\n", num, i);
+			ret = del_item(root, &path);
+			if (ret != 0)
+				BUG();
+			tree_size--;
+		}
 		release_path(root, &path);
-		tree_size--;
 	}
+	write_ctree_super(root, &super);
+	close_ctree(root);
+	root = open_ctree("dbfile", &super);
 	srand(128);
 	for (i = 0; i < run_size; i++) {
 		buf = malloc(64);
 		num = next_key(i, max_key);
 		sprintf(buf, "string-%d", num);
 		ins.objectid = num;
+		if (i % 10000 == 0)
+			printf("insert %d:%d\n", num, i);
 		ret = insert_item(root, &ins, buf, strlen(buf));
 		if (!ret)
 			tree_size++;
-		if (i >= 5) {
-			struct key ugh;
-			ugh.objectid = 5;
-			ugh.flags = 0;
-			ugh.offset = 0;
-			init_path(&path);
-			ret = search_slot(root, &ugh, &path, 0);
-			if (ret) {
-				print_tree(root, root->node);
-				printf("unable to find 5 %d\n", num);
-				exit(1);
-			}
-			release_path(root, &path);
-
-		}
+		free(buf);
 	}
 	write_ctree_super(root, &super);
 	close_ctree(root);
@@ -1326,6 +1037,8 @@ int main() {
 		num = next_key(i, max_key);
 		ins.objectid = num;
 		init_path(&path);
+		if (i % 10000 == 0)
+			printf("search %d:%d\n", num, i);
 		ret = search_slot(root, &ins, &path, 0);
 		if (ret) {
 			print_tree(root, root->node);
@@ -1340,7 +1053,7 @@ int main() {
 		int slot;
 		ins.objectid = (u64)-1;
 		init_path(&path);
-		ret = search_slot(root, &ins, &path, 0);
+		ret = search_slot(root, &ins, &path, -1);
 		if (ret == 0)
 			BUG();
 
@@ -1356,6 +1069,8 @@ int main() {
 			if (comp_keys(&last, &leaf->items[slot].key) <= 0)
 				BUG();
 			memcpy(&last, &leaf->items[slot].key, sizeof(last));
+			if (tree_size % 10000 == 0)
+				printf("big del %d:%d\n", tree_size, i);
 			ret = del_item(root, &path);
 			if (ret != 0) {
 				printf("del_item returned %d\n", ret);
@@ -1365,10 +1080,9 @@ int main() {
 		}
 		release_path(root, &path);
 	}
-	write_ctree_super(root, &super);
-	close_ctree(root);
 	printf("tree size is now %d\n", tree_size);
 	printf("map tree\n");
-	print_tree(root->extent_root, root->extent_root->node);
+	write_ctree_super(root, &super);
+	close_ctree(root);
 	return 0;
 }

commit 9a8dd1502de6aa683ae46cf0397e9b6e636416fb
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Feb 23 08:38:36 2007 -0500

    Btrfs: Block sized tree extents and extent deletion
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 1b4e82d8074d..f0abcf1f3939 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -8,9 +8,12 @@
 #define SEARCH_READ 0
 #define SEARCH_WRITE 1
 
-static int refill_alloc_extent(struct ctree_root *root);
+#define CTREE_EXTENT_PENDING 0
+
 int split_node(struct ctree_root *root, struct ctree_path *path, int level);
 int split_leaf(struct ctree_root *root, struct ctree_path *path, int data_size);
+struct tree_buffer *alloc_free_block(struct ctree_root *root);
+int free_extent(struct ctree_root *root, u64 blocknr, u64 num_blocks);
 
 static inline void init_path(struct ctree_path *p)
 {
@@ -682,8 +685,6 @@ int insert_item(struct ctree_root *root, struct key *key,
 	unsigned int data_end;
 	struct ctree_path path;
 
-	refill_alloc_extent(root);
-
 	/* create a root if there isn't one */
 	if (!root->node)
 		BUG();
@@ -756,6 +757,7 @@ int del_ptr(struct ctree_root *root, struct ctree_path *path, int level)
 	struct tree_buffer *t;
 	struct node *node;
 	int nritems;
+	u64 blocknr;
 
 	while(1) {
 		t = path->nodes[level];
@@ -774,6 +776,7 @@ int del_ptr(struct ctree_root *root, struct ctree_path *path, int level)
 		}
 		node->header.nritems--;
 		write_tree_block(root, t);
+		blocknr = t->blocknr;
 		if (node->header.nritems != 0) {
 			int tslot;
 			if (slot == 0)
@@ -799,6 +802,7 @@ int del_ptr(struct ctree_root *root, struct ctree_path *path, int level)
 			break;
 		}
 		level++;
+		free_extent(root, blocknr, 1);
 		if (!path->nodes[level])
 			BUG();
 	}
@@ -841,8 +845,10 @@ int del_item(struct ctree_root *root, struct ctree_path *path)
 		if (leaf_buf == root->node) {
 			leaf->header.flags = node_level(0);
 			write_tree_block(root, leaf_buf);
-		} else
+		} else {
 			del_ptr(root, path, 1);
+			free_extent(root, leaf_buf->blocknr, 1);
+		}
 	} else {
 		if (slot == 0)
 			fixup_low_keys(root, path, &leaf->items[0].key, 1);
@@ -867,6 +873,72 @@ int del_item(struct ctree_root *root, struct ctree_path *path)
 	return 0;
 }
 
+static int del_pending_extents(struct ctree_root *extent_root)
+{
+	int ret;
+	struct key key;
+	struct tree_buffer *gang[4];
+	int i;
+	struct ctree_path path;
+
+	while(1) {
+		ret = radix_tree_gang_lookup_tag(&extent_root->cache_radix,
+						 (void **)gang, 0, ARRAY_SIZE(gang),
+						 CTREE_EXTENT_PENDING);
+		if (!ret)
+			break;
+		for (i = 0; i < ret; i++) {
+			key.objectid = gang[i]->blocknr;
+			key.flags = 0;
+			key.offset = 1;
+			init_path(&path);
+			ret = search_slot(extent_root, &key, &path, 0);
+			if (ret) {
+				BUG();
+				// FIXME undo it and return sane
+				return ret;
+			}
+			ret = del_item(extent_root, &path);
+			if (ret) {
+				BUG();
+				return ret;
+			}
+			release_path(extent_root, &path);
+			radix_tree_tag_clear(&extent_root->cache_radix, gang[i]->blocknr,
+						CTREE_EXTENT_PENDING);
+			tree_block_release(extent_root, gang[i]);
+		}
+	}
+	return 0;
+}
+
+int free_extent(struct ctree_root *root, u64 blocknr, u64 num_blocks)
+{
+	struct ctree_path path;
+	struct key key;
+	struct ctree_root *extent_root = root->extent_root;
+	struct tree_buffer *t;
+	int pending_ret;
+	int ret;
+
+	key.objectid = blocknr;
+	key.flags = 0;
+	key.offset = num_blocks;
+	if (root == extent_root) {
+		t = read_tree_block(root, key.objectid);
+		radix_tree_tag_set(&root->cache_radix, key.objectid, CTREE_EXTENT_PENDING);
+		return 0;
+	}
+	init_path(&path);
+	ret = search_slot(extent_root, &key, &path, 0);
+	if (ret)
+		BUG();
+	ret = del_item(extent_root, &path);
+	release_path(extent_root, &path);
+	pending_ret = del_pending_extents(root->extent_root);
+	return ret ? ret : pending_ret;
+}
+
 int next_leaf(struct ctree_root *root, struct ctree_path *path)
 {
 	int slot;
@@ -904,8 +976,8 @@ int next_leaf(struct ctree_root *root, struct ctree_path *path)
 	return 0;
 }
 
-int alloc_extent(struct ctree_root *orig_root, u64 num_blocks, u64 search_start,
-		 u64 search_end, u64 owner, struct key *ins)
+int find_free_extent(struct ctree_root *orig_root, u64 num_blocks, u64 search_start,
+			 u64 search_end, struct key *ins)
 {
 	struct ctree_path path;
 	struct key *key;
@@ -915,15 +987,13 @@ int alloc_extent(struct ctree_root *orig_root, u64 num_blocks, u64 search_start,
 	u64 last_block;
 	int start_found = 0;
 	struct leaf *l;
-	struct extent_item extent_item;
 	struct ctree_root * root = orig_root->extent_root;
 
 	init_path(&path);
 	ins->objectid = search_start;
 	ins->offset = 0;
 	ins->flags = 0;
-
-	ret = search_slot(root, ins, &path, sizeof(struct extent_item));
+	ret = search_slot(root, ins, &path, 0);
 	while (1) {
 		l = &path.nodes[0]->leaf;
 		slot = path.slots[0];
@@ -938,6 +1008,7 @@ int alloc_extent(struct ctree_root *orig_root, u64 num_blocks, u64 search_start,
 				ins->objectid = search_start;
 				ins->offset = num_blocks;
 				hole_size = search_end - search_start;
+				start_found = 1;
 				goto insert;
 			}
 			ins->objectid = last_block;
@@ -956,51 +1027,119 @@ int alloc_extent(struct ctree_root *orig_root, u64 num_blocks, u64 search_start,
 		} else
 			start_found = 1;
 		last_block = key->objectid + key->offset;
+insert_failed:
 		path.slots[0]++;
 	}
 	// FIXME -ENOSPC
 insert:
+	if (orig_root->extent_root == orig_root) {
+		BUG_ON(num_blocks != 1);
+		if ((root->current_insert.objectid <= ins->objectid &&
+		    root->current_insert.objectid + root->current_insert.offset >
+		    ins->objectid) ||
+		   (root->current_insert.objectid > ins->objectid &&
+		    root->current_insert.objectid <= ins->objectid + ins->offset) ||
+		   radix_tree_tag_get(&root->cache_radix, ins->objectid,
+				      CTREE_EXTENT_PENDING)) {
+			last_block = ins->objectid + 1;
+			search_start = last_block;
+			goto insert_failed;
+		}
+	}
 	release_path(root, &path);
+	if (ins->offset != 1)
+		BUG();
+	return 0;
+}
+
+static int insert_pending_extents(struct ctree_root *extent_root)
+{
+	int ret;
+	struct key key;
+	struct extent_item item;
+	struct tree_buffer *gang[4];
+	int i;
+
+	// FIXME -ENOSPC
+	item.refs = 1;
+	item.owner = extent_root->node->node.header.parentid;
+	while(1) {
+		ret = radix_tree_gang_lookup_tag(&extent_root->cache_radix,
+						 (void **)gang, 0, ARRAY_SIZE(gang),
+						 CTREE_EXTENT_PENDING);
+		if (!ret)
+			break;
+		for (i = 0; i < ret; i++) {
+			key.objectid = gang[i]->blocknr;
+			key.flags = 0;
+			key.offset = 1;
+			ret = insert_item(extent_root, &key, &item, sizeof(item));
+			if (ret) {
+				BUG();
+				// FIXME undo it and return sane
+				return ret;
+			}
+			radix_tree_tag_clear(&extent_root->cache_radix, gang[i]->blocknr,
+						CTREE_EXTENT_PENDING);
+			tree_block_release(extent_root, gang[i]);
+		}
+	}
+	return 0;
+}
+
+int alloc_extent(struct ctree_root *root, u64 num_blocks, u64 search_start,
+			 u64 search_end, u64 owner, struct key *ins, struct tree_buffer **buf)
+{
+	int ret;
+	int pending_ret;
+	struct extent_item extent_item;
+
 	extent_item.refs = 1;
 	extent_item.owner = owner;
-	if (root == orig_root && root->reserve_extent->num_blocks == 0) {
-		root->reserve_extent->blocknr = ins->objectid;
-		root->reserve_extent->num_blocks = ins->offset;
-		root->reserve_extent->num_used = 0;
+
+	ret = find_free_extent(root, num_blocks, search_start, search_end, ins);
+	if (ret)
+		return ret;
+
+	if (root != root->extent_root) {
+		memcpy(&root->extent_root->current_insert, ins, sizeof(*ins));
+		ret = insert_item(root->extent_root, ins, &extent_item, sizeof(extent_item));
+		memset(&root->extent_root->current_insert, 0, sizeof(struct key));
+		pending_ret = insert_pending_extents(root->extent_root);
+		if (ret)
+			return ret;
+		if (pending_ret)
+			return pending_ret;
+		*buf = find_tree_block(root, ins->objectid);
+		return 0;
 	}
-	ret = insert_item(root->extent_root, ins, &extent_item, sizeof(extent_item));
-	return ret;
+	/* we're allocating an extent for the extent tree, don't recurse */
+	BUG_ON(ins->offset != 1);
+	*buf = find_tree_block(root, ins->objectid);
+	BUG_ON(!*buf);
+	radix_tree_tag_set(&root->cache_radix, ins->objectid, CTREE_EXTENT_PENDING);
+	(*buf)->count++;
+	return 0;
+
 }
 
-static int refill_alloc_extent(struct ctree_root *root)
+struct tree_buffer *alloc_free_block(struct ctree_root *root)
 {
-	struct alloc_extent *ae = root->alloc_extent;
-	struct key key;
+	struct key ins;
 	int ret;
-	int min_blocks = MAX_LEVEL * 2;
+	struct tree_buffer *buf = NULL;
 
-	if (ae->num_blocks > ae->num_used && ae->num_blocks - ae->num_used >
-	    min_blocks)
-		return 0;
-	ae = root->reserve_extent;
-	if (ae->num_blocks > ae->num_used) {
-		if (root->alloc_extent->num_blocks == 0) {
-			/* we should swap reserve/alloc_extent when alloc
-			 * fills up
-			 */
-			BUG();
-		}
-		if (ae->num_blocks - ae->num_used < min_blocks)
-			BUG();
-		return 0;
+	ret = alloc_extent(root, 1, 0, (unsigned long)-1, root->node->node.header.parentid,
+			   &ins, &buf);
+
+	if (ret) {
+		BUG();
+		return NULL;
 	}
-	ret = alloc_extent(root,
-			   min_blocks * 2, 0, (unsigned long)-1,
-			   root->node->node.header.parentid, &key);
-	ae->blocknr = key.objectid;
-	ae->num_blocks = key.offset;
-	ae->num_used = 0;
-	return ret;
+	if (root != root->extent_root)
+		BUG_ON(radix_tree_tag_get(&root->extent_root->cache_radix, buf->blocknr,
+					  CTREE_EXTENT_PENDING));
+	return buf;
 }
 
 void print_leaf(struct leaf *l)
@@ -1096,6 +1235,7 @@ int main() {
 	print_tree(root, root->node);
 	printf("map tree\n");
 	print_tree(root->extent_root, root->extent_root->node);
+	fflush(stdout);
 
 	srand(55);
 	for (i = 0; i < run_size; i++) {
@@ -1111,12 +1251,6 @@ int main() {
 		if (!ret)
 			tree_size++;
 	}
-	printf("root used: %lu\n", root->alloc_extent->num_used);
-	printf("root tree\n");
-	// print_tree(root, root->node);
-	printf("map tree\n");
-	printf("map used: %lu\n", root->extent_root->alloc_extent->num_used);
-	// print_tree(root->extent_root, root->extent_root->node);
 	write_ctree_super(root, &super);
 	close_ctree(root);
 
@@ -1167,12 +1301,27 @@ int main() {
 		ret = insert_item(root, &ins, buf, strlen(buf));
 		if (!ret)
 			tree_size++;
+		if (i >= 5) {
+			struct key ugh;
+			ugh.objectid = 5;
+			ugh.flags = 0;
+			ugh.offset = 0;
+			init_path(&path);
+			ret = search_slot(root, &ugh, &path, 0);
+			if (ret) {
+				print_tree(root, root->node);
+				printf("unable to find 5 %d\n", num);
+				exit(1);
+			}
+			release_path(root, &path);
+
+		}
 	}
 	write_ctree_super(root, &super);
 	close_ctree(root);
 	root = open_ctree("dbfile", &super);
-	printf("starting search2\n");
 	srand(128);
+	printf("starting search2\n");
 	for (i = 0; i < run_size; i++) {
 		num = next_key(i, max_key);
 		ins.objectid = num;
@@ -1219,5 +1368,7 @@ int main() {
 	write_ctree_super(root, &super);
 	close_ctree(root);
 	printf("tree size is now %d\n", tree_size);
+	printf("map tree\n");
+	print_tree(root->extent_root, root->extent_root->node);
 	return 0;
 }

commit 5c680ed620c2b69cf751aecf1a5e03ce2c89c7f3
Author: Chris Mason <chris.mason@oracle.com>
Date:   Thu Feb 22 11:39:13 2007 -0500

    Btrfs: switch to early splits
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 2891b582e26f..1b4e82d8074d 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -5,7 +5,12 @@
 #include "ctree.h"
 #include "disk-io.h"
 
+#define SEARCH_READ 0
+#define SEARCH_WRITE 1
+
 static int refill_alloc_extent(struct ctree_root *root);
+int split_node(struct ctree_root *root, struct ctree_path *path, int level);
+int split_leaf(struct ctree_root *root, struct ctree_path *path, int data_size);
 
 static inline void init_path(struct ctree_path *p)
 {
@@ -125,14 +130,14 @@ int bin_search(struct node *c, struct key *key, int *slot)
  * If the key isn't found, the path points to the slot where it should
  * be inserted.
  */
-int search_slot(struct ctree_root *root, struct key *key, struct ctree_path *p)
+int search_slot(struct ctree_root *root, struct key *key, struct ctree_path *p, int ins_len)
 {
 	struct tree_buffer *b = root->node;
 	struct node *c;
-
 	int slot;
 	int ret;
 	int level;
+
 	b->count++;
 	while (b) {
 		c = &b->node;
@@ -143,10 +148,26 @@ int search_slot(struct ctree_root *root, struct key *key, struct ctree_path *p)
 			if (ret && slot > 0)
 				slot -= 1;
 			p->slots[level] = slot;
+			if (ins_len && c->header.nritems == NODEPTRS_PER_BLOCK) {
+				int sret = split_node(root, p, level);
+				BUG_ON(sret > 0);
+				if (sret)
+					return sret;
+				b = p->nodes[level];
+				c = &b->node;
+				slot = p->slots[level];
+			}
 			b = read_tree_block(root, c->blockptrs[slot]);
 			continue;
 		} else {
+			struct leaf *l = (struct leaf *)c;
 			p->slots[level] = slot;
+			if (ins_len && leaf_free_space(l) <  sizeof(struct item) + ins_len) {
+				int sret = split_leaf(root, p, ins_len);
+				BUG_ON(sret > 0);
+				if (sret)
+					return sret;
+			}
 			return ret;
 		}
 	}
@@ -331,50 +352,54 @@ int push_node_right(struct ctree_root *root, struct ctree_path *path, int level)
 	return 0;
 }
 
+static int insert_new_root(struct ctree_root *root, struct ctree_path *path, int level)
+{
+	struct tree_buffer *t;
+	struct node *lower;
+	struct node *c;
+	struct key *lower_key;
+
+	BUG_ON(path->nodes[level]);
+	BUG_ON(path->nodes[level-1] != root->node);
+
+	t = alloc_free_block(root);
+	c = &t->node;
+	memset(c, 0, sizeof(c));
+	c->header.nritems = 1;
+	c->header.flags = node_level(level);
+	c->header.blocknr = t->blocknr;
+	c->header.parentid = root->node->node.header.parentid;
+	lower = &path->nodes[level-1]->node;
+	if (is_leaf(lower->header.flags))
+		lower_key = &((struct leaf *)lower)->items[0].key;
+	else
+		lower_key = lower->keys;
+	memcpy(c->keys, lower_key, sizeof(struct key));
+	c->blockptrs[0] = path->nodes[level-1]->blocknr;
+	/* the super has an extra ref to root->node */
+	tree_block_release(root, root->node);
+	root->node = t;
+	t->count++;
+	write_tree_block(root, t);
+	path->nodes[level] = t;
+	path->slots[level] = 0;
+	return 0;
+}
+
 /*
  * worker function to insert a single pointer in a node.
  * the node should have enough room for the pointer already
  * slot and level indicate where you want the key to go, and
  * blocknr is the block the key points to.
  */
-int __insert_ptr(struct ctree_root *root,
+int insert_ptr(struct ctree_root *root,
 		struct ctree_path *path, struct key *key,
 		u64 blocknr, int slot, int level)
 {
-	struct node *c;
 	struct node *lower;
-	struct key *lower_key;
 	int nritems;
-	/* need a new root */
-	if (!path->nodes[level]) {
-		struct tree_buffer *t;
-		t = alloc_free_block(root);
-		c = &t->node;
-		memset(c, 0, sizeof(c));
-		c->header.nritems = 2;
-		c->header.flags = node_level(level);
-		c->header.blocknr = t->blocknr;
-		c->header.parentid = root->node->node.header.parentid;
-		lower = &path->nodes[level-1]->node;
-		if (is_leaf(lower->header.flags))
-			lower_key = &((struct leaf *)lower)->items[0].key;
-		else
-			lower_key = lower->keys;
-		memcpy(c->keys, lower_key, sizeof(struct key));
-		memcpy(c->keys + 1, key, sizeof(struct key));
-		c->blockptrs[0] = path->nodes[level-1]->blocknr;
-		c->blockptrs[1] = blocknr;
-		/* the super has an extra ref to root->node */
-		tree_block_release(root, root->node);
-		root->node = t;
-		t->count++;
-		write_tree_block(root, t);
-		path->nodes[level] = t;
-		path->slots[level] = 0;
-		if (c->keys[1].objectid == 0)
-			BUG();
-		return 0;
-	}
+
+	BUG_ON(!path->nodes[level]);
 	lower = &path->nodes[level]->node;
 	nritems = lower->header.nritems;
 	if (slot > nritems)
@@ -396,93 +421,54 @@ int __insert_ptr(struct ctree_root *root,
 	return 0;
 }
 
-
-/*
- * insert a key,blocknr pair into the tree at a given level
- * If the node at that level in the path doesn't have room,
- * it is split or shifted as appropriate.
- */
-int insert_ptr(struct ctree_root *root,
-		struct ctree_path *path, struct key *key,
-		u64 blocknr, int level)
+int split_node(struct ctree_root *root, struct ctree_path *path, int level)
 {
-	struct tree_buffer *t = path->nodes[level];
-	struct node *c = &path->nodes[level]->node;
-	struct node *b;
-	struct tree_buffer *b_buffer;
-	struct tree_buffer *bal[MAX_LEVEL];
-	int bal_level = level;
+	struct tree_buffer *t;
+	struct node *c;
+	struct tree_buffer *split_buffer;
+	struct node *split;
 	int mid;
-	int bal_start = -1;
-
-	/*
-	 * check to see if we need to make room in the node for this
-	 * pointer.  If we do, keep walking the tree, making sure there
-	 * is enough room in each level for the required insertions.
-	 *
-	 * The bal array is filled in with any nodes to be inserted
-	 * due to splitting.  Once we've done all the splitting required
-	 * do the inserts based on the data in the bal array.
-	 */
-	memset(bal, 0, sizeof(bal));
-	while(t && t->node.header.nritems == NODEPTRS_PER_BLOCK) {
-		c = &t->node;
-		if (push_node_left(root, path,
-		   node_level(c->header.flags)) == 0)
-			break;
-		if (push_node_right(root, path,
-		   node_level(c->header.flags)) == 0)
-			break;
-		bal_start = bal_level;
-		if (bal_level == MAX_LEVEL - 1)
-			BUG();
-		b_buffer = alloc_free_block(root);
-		b = &b_buffer->node;
-		b->header.flags = c->header.flags;
-		b->header.blocknr = b_buffer->blocknr;
-		b->header.parentid = root->node->node.header.parentid;
-		mid = (c->header.nritems + 1) / 2;
-		memcpy(b->keys, c->keys + mid,
-			(c->header.nritems - mid) * sizeof(struct key));
-		memcpy(b->blockptrs, c->blockptrs + mid,
-			(c->header.nritems - mid) * sizeof(u64));
-		b->header.nritems = c->header.nritems - mid;
-		c->header.nritems = mid;
-
-		write_tree_block(root, t);
-		write_tree_block(root, b_buffer);
+	int ret;
 
-		bal[bal_level] = b_buffer;
-		if (bal_level == MAX_LEVEL - 1)
-			break;
-		bal_level += 1;
-		t = path->nodes[bal_level];
+	ret = push_node_left(root, path, level);
+	if (!ret)
+		return 0;
+	ret = push_node_right(root, path, level);
+	if (!ret)
+		return 0;
+	t = path->nodes[level];
+	c = &t->node;
+	if (t == root->node) {
+		/* trying to split the root, lets make a new one */
+		ret = insert_new_root(root, path, level + 1);
+		if (ret)
+			return ret;
 	}
-	/*
-	 * bal_start tells us the first level in the tree that needed to
-	 * be split.  Go through the bal array inserting the new nodes
-	 * as needed.  The path is fixed as we go.
-	 */
-	while(bal_start > 0) {
-		b_buffer = bal[bal_start];
-		c = &path->nodes[bal_start]->node;
-		__insert_ptr(root, path, b_buffer->node.keys, b_buffer->blocknr,
-				path->slots[bal_start + 1] + 1, bal_start + 1);
-		if (path->slots[bal_start] >= c->header.nritems) {
-			path->slots[bal_start] -= c->header.nritems;
-			tree_block_release(root, path->nodes[bal_start]);
-			path->nodes[bal_start] = b_buffer;
-			path->slots[bal_start + 1] += 1;
-		} else {
-			tree_block_release(root, b_buffer);
-		}
-		bal_start--;
-		if (!bal[bal_start])
-			break;
+	split_buffer = alloc_free_block(root);
+	split = &split_buffer->node;
+	split->header.flags = c->header.flags;
+	split->header.blocknr = split_buffer->blocknr;
+	split->header.parentid = root->node->node.header.parentid;
+	mid = (c->header.nritems + 1) / 2;
+	memcpy(split->keys, c->keys + mid,
+		(c->header.nritems - mid) * sizeof(struct key));
+	memcpy(split->blockptrs, c->blockptrs + mid,
+		(c->header.nritems - mid) * sizeof(u64));
+	split->header.nritems = c->header.nritems - mid;
+	c->header.nritems = mid;
+	write_tree_block(root, t);
+	write_tree_block(root, split_buffer);
+	insert_ptr(root, path, split->keys, split_buffer->blocknr,
+		     path->slots[level + 1] + 1, level + 1);
+	if (path->slots[level] > mid) {
+		path->slots[level] -= mid;
+		tree_block_release(root, t);
+		path->nodes[level] = split_buffer;
+		path->slots[level + 1] += 1;
+	} else {
+		tree_block_release(root, split_buffer);
 	}
-	/* Now that the tree has room, insert the requested pointer */
-	return __insert_ptr(root, path, key, blocknr, path->slots[level] + 1,
-			    level);
+	return 0;
 }
 
 /*
@@ -623,6 +609,11 @@ int split_leaf(struct ctree_root *root, struct ctree_path *path, int data_size)
 		if (leaf_free_space(l) >= sizeof(struct item) + data_size)
 			return 0;
 	}
+	if (!path->nodes[1]) {
+		ret = insert_new_root(root, path, 1);
+		if (ret)
+			return ret;
+	}
 	slot = path->slots[0];
 	nritems = l->header.nritems;
 	mid = (nritems + 1)/ 2;
@@ -659,8 +650,7 @@ int split_leaf(struct ctree_root *root, struct ctree_path *path, int data_size)
 
 	l->header.nritems = mid;
 	ret = insert_ptr(root, path, &right->items[0].key,
-			  right_buffer->blocknr, 1);
-
+			  right_buffer->blocknr, path->slots[1] + 1, 1);
 	write_tree_block(root, right_buffer);
 	write_tree_block(root, l_buf);
 
@@ -695,21 +685,10 @@ int insert_item(struct ctree_root *root, struct key *key,
 	refill_alloc_extent(root);
 
 	/* create a root if there isn't one */
-	if (!root->node) {
+	if (!root->node)
 		BUG();
-#if 0
-		struct tree_buffer *t;
-		t = alloc_free_block(root);
-		BUG_ON(!t);
-		t->node.header.nritems = 0;
-		t->node.header.flags = node_level(0);
-		t->node.header.blocknr = t->blocknr;
-		root->node = t;
-		write_tree_block(root, t);
-#endif
-	}
 	init_path(&path);
-	ret = search_slot(root, key, &path);
+	ret = search_slot(root, key, &path, data_size);
 	if (ret == 0) {
 		release_path(root, &path);
 		return -EEXIST;
@@ -719,12 +698,6 @@ int insert_item(struct ctree_root *root, struct key *key,
 	leaf_buf = path.nodes[0];
 	leaf = &leaf_buf->leaf;
 
-	/* make room if needed */
-	if (leaf_free_space(leaf) <  sizeof(struct item) + data_size) {
-		split_leaf(root, &path, data_size);
-		leaf_buf = path.nodes[0];
-		leaf = &path.nodes[0]->leaf;
-	}
 	nritems = leaf->header.nritems;
 	data_end = leaf_data_end(leaf);
 
@@ -950,7 +923,7 @@ int alloc_extent(struct ctree_root *orig_root, u64 num_blocks, u64 search_start,
 	ins->offset = 0;
 	ins->flags = 0;
 
-	ret = search_slot(root, ins, &path);
+	ret = search_slot(root, ins, &path, sizeof(struct extent_item));
 	while (1) {
 		l = &path.nodes[0]->leaf;
 		slot = path.slots[0];
@@ -1097,8 +1070,8 @@ void print_tree(struct ctree_root *root, struct tree_buffer *t)
 
 /* for testing only */
 int next_key(int i, int max_key) {
-	return rand() % max_key;
-	// return i;
+	// return rand() % max_key;
+	return i;
 }
 
 int main() {
@@ -1154,7 +1127,7 @@ int main() {
 		num = next_key(i, max_key);
 		ins.objectid = num;
 		init_path(&path);
-		ret = search_slot(root, &ins, &path);
+		ret = search_slot(root, &ins, &path, 0);
 		if (ret) {
 			print_tree(root, root->node);
 			printf("unable to find %d\n", num);
@@ -1176,7 +1149,7 @@ int main() {
 		num = next_key(i, max_key);
 		ins.objectid = num;
 		init_path(&path);
-		ret = search_slot(root, &ins, &path);
+		ret = search_slot(root, &ins, &path, 0);
 		if (ret)
 			continue;
 		ret = del_item(root, &path);
@@ -1204,7 +1177,7 @@ int main() {
 		num = next_key(i, max_key);
 		ins.objectid = num;
 		init_path(&path);
-		ret = search_slot(root, &ins, &path);
+		ret = search_slot(root, &ins, &path, 0);
 		if (ret) {
 			print_tree(root, root->node);
 			printf("unable to find %d\n", num);
@@ -1218,7 +1191,7 @@ int main() {
 		int slot;
 		ins.objectid = (u64)-1;
 		init_path(&path);
-		ret = search_slot(root, &ins, &path);
+		ret = search_slot(root, &ins, &path, 0);
 		if (ret == 0)
 			BUG();
 

commit cfaa72952fa7b44aa5d967cbc266110900552aef
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed Feb 21 17:04:57 2007 -0500

    Btrfs: extent fixes
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 2177744dedd3..2891b582e26f 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -354,6 +354,7 @@ int __insert_ptr(struct ctree_root *root,
 		c->header.nritems = 2;
 		c->header.flags = node_level(level);
 		c->header.blocknr = t->blocknr;
+		c->header.parentid = root->node->node.header.parentid;
 		lower = &path->nodes[level-1]->node;
 		if (is_leaf(lower->header.flags))
 			lower_key = &((struct leaf *)lower)->items[0].key;
@@ -363,7 +364,7 @@ int __insert_ptr(struct ctree_root *root,
 		memcpy(c->keys + 1, key, sizeof(struct key));
 		c->blockptrs[0] = path->nodes[level-1]->blocknr;
 		c->blockptrs[1] = blocknr;
-		/* the path has an extra ref to root->node */
+		/* the super has an extra ref to root->node */
 		tree_block_release(root, root->node);
 		root->node = t;
 		t->count++;
@@ -439,6 +440,7 @@ int insert_ptr(struct ctree_root *root,
 		b = &b_buffer->node;
 		b->header.flags = c->header.flags;
 		b->header.blocknr = b_buffer->blocknr;
+		b->header.parentid = root->node->node.header.parentid;
 		mid = (c->header.nritems + 1) / 2;
 		memcpy(b->keys, c->keys + mid,
 			(c->header.nritems - mid) * sizeof(struct key));
@@ -642,6 +644,7 @@ int split_leaf(struct ctree_root *root, struct ctree_path *path, int data_size)
 	right->header.nritems = nritems - mid;
 	right->header.blocknr = right_buffer->blocknr;
 	right->header.flags = node_level(0);
+	right->header.parentid = root->node->node.header.parentid;
 	data_copy_size = l->items[mid].offset + l->items[mid].size -
 			 leaf_data_end(l);
 	memcpy(right->items, l->items + mid,
@@ -689,8 +692,12 @@ int insert_item(struct ctree_root *root, struct key *key,
 	unsigned int data_end;
 	struct ctree_path path;
 
+	refill_alloc_extent(root);
+
 	/* create a root if there isn't one */
 	if (!root->node) {
+		BUG();
+#if 0
 		struct tree_buffer *t;
 		t = alloc_free_block(root);
 		BUG_ON(!t);
@@ -699,6 +706,7 @@ int insert_item(struct ctree_root *root, struct key *key,
 		t->node.header.blocknr = t->blocknr;
 		root->node = t;
 		write_tree_block(root, t);
+#endif
 	}
 	init_path(&path);
 	ret = search_slot(root, key, &path);
@@ -758,7 +766,6 @@ int insert_item(struct ctree_root *root, struct key *key,
 	if (leaf_free_space(leaf) < 0)
 		BUG();
 	release_path(root, &path);
-	refill_alloc_extent(root);
 	return 0;
 }
 
@@ -893,7 +900,7 @@ int next_leaf(struct ctree_root *root, struct ctree_path *path)
 	int level = 1;
 	u64 blocknr;
 	struct tree_buffer *c;
-	struct tree_buffer *next;
+	struct tree_buffer *next = NULL;
 
 	while(level < MAX_LEVEL) {
 		if (!path->nodes[level])
@@ -905,6 +912,8 @@ int next_leaf(struct ctree_root *root, struct ctree_path *path)
 			continue;
 		}
 		blocknr = c->node.blockptrs[slot];
+		if (next)
+			tree_block_release(root, next);
 		next = read_tree_block(root, blocknr);
 		break;
 	}
@@ -922,7 +931,7 @@ int next_leaf(struct ctree_root *root, struct ctree_path *path)
 	return 0;
 }
 
-int alloc_extent(struct ctree_root *root, u64 num_blocks, u64 search_start,
+int alloc_extent(struct ctree_root *orig_root, u64 num_blocks, u64 search_start,
 		 u64 search_end, u64 owner, struct key *ins)
 {
 	struct ctree_path path;
@@ -934,6 +943,7 @@ int alloc_extent(struct ctree_root *root, u64 num_blocks, u64 search_start,
 	int start_found = 0;
 	struct leaf *l;
 	struct extent_item extent_item;
+	struct ctree_root * root = orig_root->extent_root;
 
 	init_path(&path);
 	ins->objectid = search_start;
@@ -974,13 +984,18 @@ int alloc_extent(struct ctree_root *root, u64 num_blocks, u64 search_start,
 			start_found = 1;
 		last_block = key->objectid + key->offset;
 		path.slots[0]++;
-		printf("last block is not %lu\n", last_block);
 	}
 	// FIXME -ENOSPC
 insert:
+	release_path(root, &path);
 	extent_item.refs = 1;
 	extent_item.owner = owner;
-	ret = insert_item(root, ins, &extent_item, sizeof(extent_item));
+	if (root == orig_root && root->reserve_extent->num_blocks == 0) {
+		root->reserve_extent->blocknr = ins->objectid;
+		root->reserve_extent->num_blocks = ins->offset;
+		root->reserve_extent->num_used = 0;
+	}
+	ret = insert_item(root->extent_root, ins, &extent_item, sizeof(extent_item));
 	return ret;
 }
 
@@ -991,7 +1006,6 @@ static int refill_alloc_extent(struct ctree_root *root)
 	int ret;
 	int min_blocks = MAX_LEVEL * 2;
 
-	printf("refill alloc root %p, numused %lu total %lu\n", root, ae->num_used, ae->num_blocks);
 	if (ae->num_blocks > ae->num_used && ae->num_blocks - ae->num_used >
 	    min_blocks)
 		return 0;
@@ -1007,9 +1021,9 @@ static int refill_alloc_extent(struct ctree_root *root)
 			BUG();
 		return 0;
 	}
-	// FIXME, this recurses
-	ret = alloc_extent(root->extent_root,
-			   min_blocks * 2, 0, (unsigned long)-1, 0, &key);
+	ret = alloc_extent(root,
+			   min_blocks * 2, 0, (unsigned long)-1,
+			   root->node->node.header.parentid, &key);
 	ae->blocknr = key.objectid;
 	ae->num_blocks = key.offset;
 	ae->num_used = 0;
@@ -1021,6 +1035,7 @@ void print_leaf(struct leaf *l)
 	int i;
 	int nr = l->header.nritems;
 	struct item *item;
+	struct extent_item *ei;
 	printf("leaf %lu total ptrs %d free space %d\n", l->header.blocknr, nr,
 	       leaf_free_space(l));
 	fflush(stdout);
@@ -1032,6 +1047,8 @@ void print_leaf(struct leaf *l)
 			item->offset, item->size);
 		fflush(stdout);
 		printf("\t\titem data %.*s\n", item->size, l->data+item->offset);
+		ei = (struct extent_item *)(l->data + item->offset);
+		printf("\t\textent data %u %lu\n", ei->refs, ei->owner);
 		fflush(stdout);
 	}
 }
@@ -1080,8 +1097,8 @@ void print_tree(struct ctree_root *root, struct tree_buffer *t)
 
 /* for testing only */
 int next_key(int i, int max_key) {
-	// return rand() % max_key;
-	return i;
+	return rand() % max_key;
+	// return i;
 }
 
 int main() {
@@ -1092,15 +1109,20 @@ int main() {
 	int i;
 	int num;
 	int ret;
-	int run_size = 256;
+	int run_size = 10000;
 	int max_key = 100000000;
 	int tree_size = 0;
 	struct ctree_path path;
+	struct ctree_super_block super;
 
 	radix_tree_init();
 
 
-	root = open_ctree("dbfile");
+	root = open_ctree("dbfile", &super);
+	printf("root tree\n");
+	print_tree(root, root->node);
+	printf("map tree\n");
+	print_tree(root->extent_root, root->extent_root->node);
 
 	srand(55);
 	for (i = 0; i < run_size; i++) {
@@ -1112,22 +1134,20 @@ int main() {
 		ins.objectid = num;
 		ins.offset = 0;
 		ins.flags = 0;
-		printf("insert %d\n", i);
 		ret = insert_item(root, &ins, buf, strlen(buf));
 		if (!ret)
 			tree_size++;
-		printf("done insert %d\n", i);
 	}
 	printf("root used: %lu\n", root->alloc_extent->num_used);
 	printf("root tree\n");
-	print_tree(root, root->node);
+	// print_tree(root, root->node);
 	printf("map tree\n");
 	printf("map used: %lu\n", root->extent_root->alloc_extent->num_used);
-	print_tree(root->extent_root, root->extent_root->node);
-	exit(1);
-
+	// print_tree(root->extent_root, root->extent_root->node);
+	write_ctree_super(root, &super);
 	close_ctree(root);
-	root = open_ctree("dbfile");
+
+	root = open_ctree("dbfile", &super);
 	printf("starting search\n");
 	srand(55);
 	for (i = 0; i < run_size; i++) {
@@ -1142,8 +1162,9 @@ int main() {
 		}
 		release_path(root, &path);
 	}
+	write_ctree_super(root, &super);
 	close_ctree(root);
-	root = open_ctree("dbfile");
+	root = open_ctree("dbfile", &super);
 	printf("node %p level %d total ptrs %d free spc %lu\n", root->node,
 	        node_level(root->node->node.header.flags),
 		root->node->node.header.nritems,
@@ -1174,8 +1195,9 @@ int main() {
 		if (!ret)
 			tree_size++;
 	}
+	write_ctree_super(root, &super);
 	close_ctree(root);
-	root = open_ctree("dbfile");
+	root = open_ctree("dbfile", &super);
 	printf("starting search2\n");
 	srand(128);
 	for (i = 0; i < run_size; i++) {
@@ -1221,6 +1243,7 @@ int main() {
 		}
 		release_path(root, &path);
 	}
+	write_ctree_super(root, &super);
 	close_ctree(root);
 	printf("tree size is now %d\n", tree_size);
 	return 0;

commit d97e63b69ef21c02b67e20e41d9968b0e503572e
Author: Chris Mason <chris.mason@oracle.com>
Date:   Tue Feb 20 16:40:44 2007 -0500

    Btrfs: early extent mapping support
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 6b64f49a0279..2177744dedd3 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -5,6 +5,8 @@
 #include "ctree.h"
 #include "disk-io.h"
 
+static int refill_alloc_extent(struct ctree_root *root);
+
 static inline void init_path(struct ctree_path *p)
 {
 	memset(p, 0, sizeof(*p));
@@ -29,7 +31,7 @@ static inline unsigned int leaf_data_end(struct leaf *leaf)
 {
 	unsigned int nr = leaf->header.nritems;
 	if (nr == 0)
-		return ARRAY_SIZE(leaf->data);
+		return sizeof(leaf->data);
 	return leaf->items[nr-1].offset;
 }
 
@@ -421,7 +423,7 @@ int insert_ptr(struct ctree_root *root,
 	 * due to splitting.  Once we've done all the splitting required
 	 * do the inserts based on the data in the bal array.
 	 */
-	memset(bal, 0, ARRAY_SIZE(bal));
+	memset(bal, 0, sizeof(bal));
 	while(t && t->node.header.nritems == NODEPTRS_PER_BLOCK) {
 		c = &t->node;
 		if (push_node_left(root, path,
@@ -756,6 +758,7 @@ int insert_item(struct ctree_root *root, struct key *key,
 	if (leaf_free_space(leaf) < 0)
 		BUG();
 	release_path(root, &path);
+	refill_alloc_extent(root);
 	return 0;
 }
 
@@ -884,6 +887,135 @@ int del_item(struct ctree_root *root, struct ctree_path *path)
 	return 0;
 }
 
+int next_leaf(struct ctree_root *root, struct ctree_path *path)
+{
+	int slot;
+	int level = 1;
+	u64 blocknr;
+	struct tree_buffer *c;
+	struct tree_buffer *next;
+
+	while(level < MAX_LEVEL) {
+		if (!path->nodes[level])
+			return -1;
+		slot = path->slots[level] + 1;
+		c = path->nodes[level];
+		if (slot >= c->node.header.nritems) {
+			level++;
+			continue;
+		}
+		blocknr = c->node.blockptrs[slot];
+		next = read_tree_block(root, blocknr);
+		break;
+	}
+	path->slots[level] = slot;
+	while(1) {
+		level--;
+		c = path->nodes[level];
+		tree_block_release(root, c);
+		path->nodes[level] = next;
+		path->slots[level] = 0;
+		if (!level)
+			break;
+		next = read_tree_block(root, next->node.blockptrs[0]);
+	}
+	return 0;
+}
+
+int alloc_extent(struct ctree_root *root, u64 num_blocks, u64 search_start,
+		 u64 search_end, u64 owner, struct key *ins)
+{
+	struct ctree_path path;
+	struct key *key;
+	int ret;
+	u64 hole_size = 0;
+	int slot = 0;
+	u64 last_block;
+	int start_found = 0;
+	struct leaf *l;
+	struct extent_item extent_item;
+
+	init_path(&path);
+	ins->objectid = search_start;
+	ins->offset = 0;
+	ins->flags = 0;
+
+	ret = search_slot(root, ins, &path);
+	while (1) {
+		l = &path.nodes[0]->leaf;
+		slot = path.slots[0];
+		if (!l) {
+			// FIXME allocate root
+		}
+		if (slot >= l->header.nritems) {
+			ret = next_leaf(root, &path);
+			if (ret == 0)
+				continue;
+			if (!start_found) {
+				ins->objectid = search_start;
+				ins->offset = num_blocks;
+				hole_size = search_end - search_start;
+				goto insert;
+			}
+			ins->objectid = last_block;
+			ins->offset = num_blocks;
+			hole_size = search_end - last_block;
+			goto insert;
+		}
+		key = &l->items[slot].key;
+		if (start_found) {
+			hole_size = key->objectid - last_block;
+			if (hole_size > num_blocks) {
+				ins->objectid = last_block;
+				ins->offset = num_blocks;
+				goto insert;
+			}
+		} else
+			start_found = 1;
+		last_block = key->objectid + key->offset;
+		path.slots[0]++;
+		printf("last block is not %lu\n", last_block);
+	}
+	// FIXME -ENOSPC
+insert:
+	extent_item.refs = 1;
+	extent_item.owner = owner;
+	ret = insert_item(root, ins, &extent_item, sizeof(extent_item));
+	return ret;
+}
+
+static int refill_alloc_extent(struct ctree_root *root)
+{
+	struct alloc_extent *ae = root->alloc_extent;
+	struct key key;
+	int ret;
+	int min_blocks = MAX_LEVEL * 2;
+
+	printf("refill alloc root %p, numused %lu total %lu\n", root, ae->num_used, ae->num_blocks);
+	if (ae->num_blocks > ae->num_used && ae->num_blocks - ae->num_used >
+	    min_blocks)
+		return 0;
+	ae = root->reserve_extent;
+	if (ae->num_blocks > ae->num_used) {
+		if (root->alloc_extent->num_blocks == 0) {
+			/* we should swap reserve/alloc_extent when alloc
+			 * fills up
+			 */
+			BUG();
+		}
+		if (ae->num_blocks - ae->num_used < min_blocks)
+			BUG();
+		return 0;
+	}
+	// FIXME, this recurses
+	ret = alloc_extent(root->extent_root,
+			   min_blocks * 2, 0, (unsigned long)-1, 0, &key);
+	ae->blocknr = key.objectid;
+	ae->num_blocks = key.offset;
+	ae->num_used = 0;
+	return ret;
+}
+
 void print_leaf(struct leaf *l)
 {
 	int i;
@@ -948,8 +1080,8 @@ void print_tree(struct ctree_root *root, struct tree_buffer *t)
 
 /* for testing only */
 int next_key(int i, int max_key) {
-	return rand() % max_key;
-	// return i;
+	// return rand() % max_key;
+	return i;
 }
 
 int main() {
@@ -960,7 +1092,7 @@ int main() {
 	int i;
 	int num;
 	int ret;
-	int run_size = 25000;
+	int run_size = 256;
 	int max_key = 100000000;
 	int tree_size = 0;
 	struct ctree_path path;
@@ -980,10 +1112,20 @@ int main() {
 		ins.objectid = num;
 		ins.offset = 0;
 		ins.flags = 0;
+		printf("insert %d\n", i);
 		ret = insert_item(root, &ins, buf, strlen(buf));
 		if (!ret)
 			tree_size++;
+		printf("done insert %d\n", i);
 	}
+	printf("root used: %lu\n", root->alloc_extent->num_used);
+	printf("root tree\n");
+	print_tree(root, root->node);
+	printf("map tree\n");
+	printf("map used: %lu\n", root->extent_root->alloc_extent->num_used);
+	print_tree(root->extent_root, root->extent_root->node);
+	exit(1);
+
 	close_ctree(root);
 	root = open_ctree("dbfile");
 	printf("starting search\n");

commit 74123bd72a4e34c66e5ad95ce233dd352bebc572
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Feb 2 11:05:29 2007 -0500

    Btrfs: Commenting/cleanup
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 6f0522f21082..6b64f49a0279 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -20,6 +20,11 @@ static void release_path(struct ctree_root *root, struct ctree_path *p)
 	}
 }
 
+/*
+ * The leaf data grows from end-to-front in the node.
+ * this returns the address of the start of the last item,
+ * which is the stop of the leaf data stack
+ */
 static inline unsigned int leaf_data_end(struct leaf *leaf)
 {
 	unsigned int nr = leaf->header.nritems;
@@ -28,6 +33,11 @@ static inline unsigned int leaf_data_end(struct leaf *leaf)
 	return leaf->items[nr-1].offset;
 }
 
+/*
+ * The space between the end of the leaf items and
+ * the start of the leaf data.  IOW, how much room
+ * the leaf has left for both items and data
+ */
 static inline int leaf_free_space(struct leaf *leaf)
 {
 	int data_end = leaf_data_end(leaf);
@@ -36,6 +46,9 @@ static inline int leaf_free_space(struct leaf *leaf)
 	return (char *)(leaf->data + data_end) - (char *)items_end;
 }
 
+/*
+ * compare two keys in a memcmp fashion
+ */
 int comp_keys(struct key *k1, struct key *k2)
 {
 	if (k1->objectid > k2->objectid)
@@ -52,6 +65,16 @@ int comp_keys(struct key *k1, struct key *k2)
 		return -1;
 	return 0;
 }
+
+/*
+ * search for key in the array p.  items p are item_size apart
+ * and there are 'max' items in p
+ * the slot in the array is returned via slot, and it points to
+ * the place where you would insert key if it is not found in
+ * the array.
+ *
+ * slot may point to max if the key is bigger than all of the keys
+ */
 int generic_bin_search(char *p, int item_size, struct key *key,
 		       int max, int *slot)
 {
@@ -92,6 +115,14 @@ int bin_search(struct node *c, struct key *key, int *slot)
 	return -1;
 }
 
+/*
+ * look for key in the tree.  path is filled in with nodes along the way
+ * if key is found, we return zero and you can find the item in the leaf
+ * level of the path (level 0)
+ *
+ * If the key isn't found, the path points to the slot where it should
+ * be inserted.
+ */
 int search_slot(struct ctree_root *root, struct key *key, struct ctree_path *p)
 {
 	struct tree_buffer *b = root->node;
@@ -120,12 +151,18 @@ int search_slot(struct ctree_root *root, struct key *key, struct ctree_path *p)
 	return -1;
 }
 
+/*
+ * adjust the pointers going up the tree, starting at level
+ * making sure the right key of each node is points to 'key'.
+ * This is used after shifting pointers to the left, so it stops
+ * fixing up pointers when a given leaf/node is not in slot 0 of the
+ * higher levels
+ */
 static void fixup_low_keys(struct ctree_root *root,
 			   struct ctree_path *path, struct key *key,
 			   int level)
 {
 	int i;
-	/* adjust the pointers going up the tree */
 	for (i = level; i < MAX_LEVEL; i++) {
 		struct node *t;
 		int tslot = path->slots[i];
@@ -139,64 +176,16 @@ static void fixup_low_keys(struct ctree_root *root,
 	}
 }
 
-int __insert_ptr(struct ctree_root *root,
-		struct ctree_path *path, struct key *key,
-		u64 blocknr, int slot, int level)
-{
-	struct node *c;
-	struct node *lower;
-	struct key *lower_key;
-	int nritems;
-	/* need a new root */
-	if (!path->nodes[level]) {
-		struct tree_buffer *t;
-		t = alloc_free_block(root);
-		c = &t->node;
-		memset(c, 0, sizeof(c));
-		c->header.nritems = 2;
-		c->header.flags = node_level(level);
-		c->header.blocknr = t->blocknr;
-		lower = &path->nodes[level-1]->node;
-		if (is_leaf(lower->header.flags))
-			lower_key = &((struct leaf *)lower)->items[0].key;
-		else
-			lower_key = lower->keys;
-		memcpy(c->keys, lower_key, sizeof(struct key));
-		memcpy(c->keys + 1, key, sizeof(struct key));
-		c->blockptrs[0] = path->nodes[level-1]->blocknr;
-		c->blockptrs[1] = blocknr;
-		/* the path has an extra ref to root->node */
-		tree_block_release(root, root->node);
-		root->node = t;
-		t->count++;
-		write_tree_block(root, t);
-		path->nodes[level] = t;
-		path->slots[level] = 0;
-		if (c->keys[1].objectid == 0)
-			BUG();
-		return 0;
-	}
-	lower = &path->nodes[level]->node;
-	nritems = lower->header.nritems;
-	if (slot > nritems)
-		BUG();
-	if (nritems == NODEPTRS_PER_BLOCK)
-		BUG();
-	if (slot != nritems) {
-		memmove(lower->keys + slot + 1, lower->keys + slot,
-			(nritems - slot) * sizeof(struct key));
-		memmove(lower->blockptrs + slot + 1, lower->blockptrs + slot,
-			(nritems - slot) * sizeof(u64));
-	}
-	memcpy(lower->keys + slot, key, sizeof(struct key));
-	lower->blockptrs[slot] = blocknr;
-	lower->header.nritems++;
-	if (lower->keys[1].objectid == 0)
-			BUG();
-	write_tree_block(root, path->nodes[level]);
-	return 0;
-}
-
+/*
+ * try to push data from one node into the next node left in the
+ * tree.  The src node is found at specified level in the path.
+ * If some bytes were pushed, return 0, otherwise return 1.
+ *
+ * Lower nodes/leaves in the path are not touched, higher nodes may
+ * be modified to reflect the push.
+ *
+ * The path is altered to reflect the push.
+ */
 int push_node_left(struct ctree_root *root, struct ctree_path *path, int level)
 {
 	int slot;
@@ -259,6 +248,16 @@ int push_node_left(struct ctree_root *root, struct ctree_path *path, int level)
 	return 0;
 }
 
+/*
+ * try to push data from one node into the next node right in the
+ * tree.  The src node is found at specified level in the path.
+ * If some bytes were pushed, return 0, otherwise return 1.
+ *
+ * Lower nodes/leaves in the path are not touched, higher nodes may
+ * be modified to reflect the push.
+ *
+ * The path is altered to reflect the push.
+ */
 int push_node_right(struct ctree_root *root, struct ctree_path *path, int level)
 {
 	int slot;
@@ -270,8 +269,11 @@ int push_node_right(struct ctree_root *root, struct ctree_path *path, int level)
 	int dst_nritems;
 	int src_nritems;
 
+	/* can't push from the root */
 	if (level == MAX_LEVEL - 1 || path->nodes[level + 1] == 0)
 		return 1;
+
+	/* only try to push inside the node higher up */
 	slot = path->slots[level + 1];
 	if (slot == NODEPTRS_PER_BLOCK - 1)
 		return 1;
@@ -315,7 +317,7 @@ int push_node_right(struct ctree_root *root, struct ctree_path *path, int level)
 	write_tree_block(root, t);
 	write_tree_block(root, src_buffer);
 
-	/* then fixup the leaf pointer in the path */
+	/* then fixup the pointers in the path */
 	if (path->slots[level] >= src->header.nritems) {
 		path->slots[level] -= src->header.nritems;
 		tree_block_release(root, path->nodes[level]);
@@ -327,6 +329,76 @@ int push_node_right(struct ctree_root *root, struct ctree_path *path, int level)
 	return 0;
 }
 
+/*
+ * worker function to insert a single pointer in a node.
+ * the node should have enough room for the pointer already
+ * slot and level indicate where you want the key to go, and
+ * blocknr is the block the key points to.
+ */
+int __insert_ptr(struct ctree_root *root,
+		struct ctree_path *path, struct key *key,
+		u64 blocknr, int slot, int level)
+{
+	struct node *c;
+	struct node *lower;
+	struct key *lower_key;
+	int nritems;
+	/* need a new root */
+	if (!path->nodes[level]) {
+		struct tree_buffer *t;
+		t = alloc_free_block(root);
+		c = &t->node;
+		memset(c, 0, sizeof(c));
+		c->header.nritems = 2;
+		c->header.flags = node_level(level);
+		c->header.blocknr = t->blocknr;
+		lower = &path->nodes[level-1]->node;
+		if (is_leaf(lower->header.flags))
+			lower_key = &((struct leaf *)lower)->items[0].key;
+		else
+			lower_key = lower->keys;
+		memcpy(c->keys, lower_key, sizeof(struct key));
+		memcpy(c->keys + 1, key, sizeof(struct key));
+		c->blockptrs[0] = path->nodes[level-1]->blocknr;
+		c->blockptrs[1] = blocknr;
+		/* the path has an extra ref to root->node */
+		tree_block_release(root, root->node);
+		root->node = t;
+		t->count++;
+		write_tree_block(root, t);
+		path->nodes[level] = t;
+		path->slots[level] = 0;
+		if (c->keys[1].objectid == 0)
+			BUG();
+		return 0;
+	}
+	lower = &path->nodes[level]->node;
+	nritems = lower->header.nritems;
+	if (slot > nritems)
+		BUG();
+	if (nritems == NODEPTRS_PER_BLOCK)
+		BUG();
+	if (slot != nritems) {
+		memmove(lower->keys + slot + 1, lower->keys + slot,
+			(nritems - slot) * sizeof(struct key));
+		memmove(lower->blockptrs + slot + 1, lower->blockptrs + slot,
+			(nritems - slot) * sizeof(u64));
+	}
+	memcpy(lower->keys + slot, key, sizeof(struct key));
+	lower->blockptrs[slot] = blocknr;
+	lower->header.nritems++;
+	if (lower->keys[1].objectid == 0)
+			BUG();
+	write_tree_block(root, path->nodes[level]);
+	return 0;
+}
+
+
+/*
+ * insert a key,blocknr pair into the tree at a given level
+ * If the node at that level in the path doesn't have room,
+ * it is split or shifted as appropriate.
+ */
 int insert_ptr(struct ctree_root *root,
 		struct ctree_path *path, struct key *key,
 		u64 blocknr, int level)
@@ -340,6 +412,15 @@ int insert_ptr(struct ctree_root *root,
 	int mid;
 	int bal_start = -1;
 
+	/*
+	 * check to see if we need to make room in the node for this
+	 * pointer.  If we do, keep walking the tree, making sure there
+	 * is enough room in each level for the required insertions.
+	 *
+	 * The bal array is filled in with any nodes to be inserted
+	 * due to splitting.  Once we've done all the splitting required
+	 * do the inserts based on the data in the bal array.
+	 */
 	memset(bal, 0, ARRAY_SIZE(bal));
 	while(t && t->node.header.nritems == NODEPTRS_PER_BLOCK) {
 		c = &t->node;
@@ -373,6 +454,11 @@ int insert_ptr(struct ctree_root *root,
 		bal_level += 1;
 		t = path->nodes[bal_level];
 	}
+	/*
+	 * bal_start tells us the first level in the tree that needed to
+	 * be split.  Go through the bal array inserting the new nodes
+	 * as needed.  The path is fixed as we go.
+	 */
 	while(bal_start > 0) {
 		b_buffer = bal[bal_start];
 		c = &path->nodes[bal_start]->node;
@@ -390,10 +476,16 @@ int insert_ptr(struct ctree_root *root,
 		if (!bal[bal_start])
 			break;
 	}
+	/* Now that the tree has room, insert the requested pointer */
 	return __insert_ptr(root, path, key, blocknr, path->slots[level] + 1,
 			    level);
 }
 
+/*
+ * how many bytes are required to store the items in a leaf.  start
+ * and nr indicate which items in the leaf to check.  This totals up the
+ * space used both by the item structs and the item data
+ */
 int leaf_space_used(struct leaf *l, int start, int nr)
 {
 	int data_len;
@@ -407,6 +499,10 @@ int leaf_space_used(struct leaf *l, int start, int nr)
 	return data_len;
 }
 
+/*
+ * push some data in the path leaf to the left, trying to free up at
+ * least data_size bytes.  returns zero if the push worked, nonzero otherwise
+ */
 int push_leaf_left(struct ctree_root *root, struct ctree_path *path,
 		   int data_size)
 {
@@ -498,6 +594,10 @@ int push_leaf_left(struct ctree_root *root, struct ctree_path *path,
 	return 0;
 }
 
+/*
+ * split the path's leaf in two, making sure there is at least data_size
+ * available for the resulting leaf level of the path.
+ */
 int split_leaf(struct ctree_root *root, struct ctree_path *path, int data_size)
 {
 	struct tree_buffer *l_buf = path->nodes[0];
@@ -548,9 +648,10 @@ int split_leaf(struct ctree_root *root, struct ctree_path *path, int data_size)
 	       l->data + leaf_data_end(l), data_copy_size);
 	rt_data_off = LEAF_DATA_SIZE -
 		     (l->items[mid].offset + l->items[mid].size);
-	for (i = 0; i < right->header.nritems; i++) {
+
+	for (i = 0; i < right->header.nritems; i++)
 		right->items[i].offset += rt_data_off;
-	}
+
 	l->header.nritems = mid;
 	ret = insert_ptr(root, path, &right->items[0].key,
 			  right_buffer->blocknr, 1);
@@ -570,6 +671,10 @@ int split_leaf(struct ctree_root *root, struct ctree_path *path, int data_size)
 	return ret;
 }
 
+/*
+ * Given a key and some data, insert an item into the tree.
+ * This does all the path init required, making room in the tree if needed.
+ */
 int insert_item(struct ctree_root *root, struct key *key,
 			  void *data, int data_size)
 {
@@ -582,6 +687,7 @@ int insert_item(struct ctree_root *root, struct key *key,
 	unsigned int data_end;
 	struct ctree_path path;
 
+	/* create a root if there isn't one */
 	if (!root->node) {
 		struct tree_buffer *t;
 		t = alloc_free_block(root);
@@ -602,6 +708,8 @@ int insert_item(struct ctree_root *root, struct key *key,
 	slot_orig = path.slots[0];
 	leaf_buf = path.nodes[0];
 	leaf = &leaf_buf->leaf;
+
+	/* make room if needed */
 	if (leaf_free_space(leaf) <  sizeof(struct item) + data_size) {
 		split_leaf(root, &path, data_size);
 		leaf_buf = path.nodes[0];
@@ -638,6 +746,7 @@ int insert_item(struct ctree_root *root, struct key *key,
 		        data_end, old_data - data_end);
 		data_end = old_data;
 	}
+	/* copy the new data in */
 	memcpy(&leaf->items[slot].key, key, sizeof(struct key));
 	leaf->items[slot].offset = data_end - data_size;
 	leaf->items[slot].size = data_size;
@@ -650,6 +759,14 @@ int insert_item(struct ctree_root *root, struct key *key,
 	return 0;
 }
 
+/*
+ * delete the pointer from a given level in the path.  The path is not
+ * fixed up, so after calling this it is not valid at that level.
+ *
+ * If the delete empties a node, the node is removed from the tree,
+ * continuing all the way the root if required.  The root is converted into
+ * a leaf if all the nodes are emptied.
+ */
 int del_ptr(struct ctree_root *root, struct ctree_path *path, int level)
 {
 	int slot;
@@ -705,6 +822,10 @@ int del_ptr(struct ctree_root *root, struct ctree_path *path, int level)
 	return 0;
 }
 
+/*
+ * delete the item at the leaf level in path.  If that empties
+ * the leaf, remove it from the tree
+ */
 int del_item(struct ctree_root *root, struct ctree_path *path)
 {
 	int slot;
@@ -732,6 +853,7 @@ int del_item(struct ctree_root *root, struct ctree_path *path)
 			(leaf->header.nritems - slot - 1));
 	}
 	leaf->header.nritems -= 1;
+	/* delete the leaf if we've emptied it */
 	if (leaf->header.nritems == 0) {
 		if (leaf_buf == root->node) {
 			leaf->header.flags = node_level(0);
@@ -742,6 +864,7 @@ int del_item(struct ctree_root *root, struct ctree_path *path)
 		if (slot == 0)
 			fixup_low_keys(root, path, &leaf->items[0].key, 1);
 		write_tree_block(root, leaf_buf);
+		/* delete the leaf if it is mostly empty */
 		if (leaf_space_used(leaf, 0, leaf->header.nritems) <
 		    LEAF_DATA_SIZE / 4) {
 			/* push_leaf_left fixes the path.
@@ -837,7 +960,7 @@ int main() {
 	int i;
 	int num;
 	int ret;
-	int run_size = 1000000;
+	int run_size = 25000;
 	int max_key = 100000000;
 	int tree_size = 0;
 	struct ctree_path path;

commit eb60ceac0ebd11e9896ed1ca84fcad71a4b23601
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Feb 2 09:18:22 2007 -0500

    Btrfs: Add backing store, memory management
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 4bf5e92584bd..6f0522f21082 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -1,68 +1,25 @@
 #include <stdio.h>
 #include <stdlib.h>
 #include "kerncompat.h"
-
-#define BLOCKSIZE 4096
-
-struct key {
-	u64 objectid;
-	u32 flags;
-	u64 offset;
-} __attribute__ ((__packed__));
-
-struct header {
-	u64 fsid[2]; /* FS specific uuid */
-	u64 blocknum;
-	u64 parentid;
-	u32 csum;
-	u32 ham;
-	u16 nritems;
-	u16 flags;
-} __attribute__ ((__packed__));
-
-#define NODEPTRS_PER_BLOCK ((BLOCKSIZE - sizeof(struct header)) / \
-			    (sizeof(struct key) + sizeof(u64)))
-
-#define LEVEL_BITS 3
-#define MAX_LEVEL (1 << LEVEL_BITS)
-#define node_level(f) ((f) & (MAX_LEVEL-1))
-#define is_leaf(f) (node_level(f) == 0)
-
-struct ctree_root {
-	struct node *node;
-};
-
-struct item {
-	struct key key;
-	u16 offset;
-	u16 size;
-} __attribute__ ((__packed__));
-
-#define LEAF_DATA_SIZE (BLOCKSIZE - sizeof(struct header))
-struct leaf {
-	struct header header;
-	union {
-		struct item items[LEAF_DATA_SIZE/sizeof(struct item)];
-		u8 data[BLOCKSIZE-sizeof(struct header)];
-	};
-} __attribute__ ((__packed__));
-
-struct node {
-	struct header header;
-	struct key keys[NODEPTRS_PER_BLOCK];
-	u64 blockptrs[NODEPTRS_PER_BLOCK];
-} __attribute__ ((__packed__));
-
-struct ctree_path {
-	struct node *nodes[MAX_LEVEL];
-	int slots[MAX_LEVEL];
-};
+#include "radix-tree.h"
+#include "ctree.h"
+#include "disk-io.h"
 
 static inline void init_path(struct ctree_path *p)
 {
 	memset(p, 0, sizeof(*p));
 }
 
+static void release_path(struct ctree_root *root, struct ctree_path *p)
+{
+	int i;
+	for (i = 0; i < MAX_LEVEL; i++) {
+		if (!p->nodes[i])
+			break;
+		tree_block_release(root, p->nodes[i]);
+	}
+}
+
 static inline unsigned int leaf_data_end(struct leaf *leaf)
 {
 	unsigned int nr = leaf->header.nritems;
@@ -135,26 +92,25 @@ int bin_search(struct node *c, struct key *key, int *slot)
 	return -1;
 }
 
-void *read_block(u64 blocknum)
-{
-	return (void *)blocknum;
-}
-
 int search_slot(struct ctree_root *root, struct key *key, struct ctree_path *p)
 {
-	struct node *c = root->node;
+	struct tree_buffer *b = root->node;
+	struct node *c;
+
 	int slot;
 	int ret;
 	int level;
-	while (c) {
+	b->count++;
+	while (b) {
+		c = &b->node;
 		level = node_level(c->header.flags);
-		p->nodes[level] = c;
+		p->nodes[level] = b;
 		ret = bin_search(c, key, &slot);
 		if (!is_leaf(c->header.flags)) {
 			if (ret && slot > 0)
 				slot -= 1;
 			p->slots[level] = slot;
-			c = read_block(c->blockptrs[slot]);
+			b = read_tree_block(root, c->blockptrs[slot]);
 			continue;
 		} else {
 			p->slots[level] = slot;
@@ -164,17 +120,20 @@ int search_slot(struct ctree_root *root, struct key *key, struct ctree_path *p)
 	return -1;
 }
 
-static void fixup_low_keys(struct ctree_path *path, struct key *key,
-			     int level)
+static void fixup_low_keys(struct ctree_root *root,
+			   struct ctree_path *path, struct key *key,
+			   int level)
 {
 	int i;
 	/* adjust the pointers going up the tree */
 	for (i = level; i < MAX_LEVEL; i++) {
-		struct node *t = path->nodes[i];
+		struct node *t;
 		int tslot = path->slots[i];
-		if (!t)
+		if (!path->nodes[i])
 			break;
+		t = &path->nodes[i]->node;
 		memcpy(t->keys + tslot, key, sizeof(*key));
+		write_tree_block(root, path->nodes[i]);
 		if (tslot != 0)
 			break;
 	}
@@ -190,27 +149,34 @@ int __insert_ptr(struct ctree_root *root,
 	int nritems;
 	/* need a new root */
 	if (!path->nodes[level]) {
-		c = malloc(sizeof(struct node));
+		struct tree_buffer *t;
+		t = alloc_free_block(root);
+		c = &t->node;
 		memset(c, 0, sizeof(c));
 		c->header.nritems = 2;
 		c->header.flags = node_level(level);
-		lower = path->nodes[level-1];
+		c->header.blocknr = t->blocknr;
+		lower = &path->nodes[level-1]->node;
 		if (is_leaf(lower->header.flags))
 			lower_key = &((struct leaf *)lower)->items[0].key;
 		else
 			lower_key = lower->keys;
 		memcpy(c->keys, lower_key, sizeof(struct key));
 		memcpy(c->keys + 1, key, sizeof(struct key));
-		c->blockptrs[0] = (u64)lower;
+		c->blockptrs[0] = path->nodes[level-1]->blocknr;
 		c->blockptrs[1] = blocknr;
-		root->node = c;
-		path->nodes[level] = c;
+		/* the path has an extra ref to root->node */
+		tree_block_release(root, root->node);
+		root->node = t;
+		t->count++;
+		write_tree_block(root, t);
+		path->nodes[level] = t;
 		path->slots[level] = 0;
 		if (c->keys[1].objectid == 0)
 			BUG();
 		return 0;
 	}
-	lower = path->nodes[level];
+	lower = &path->nodes[level]->node;
 	nritems = lower->header.nritems;
 	if (slot > nritems)
 		BUG();
@@ -227,6 +193,7 @@ int __insert_ptr(struct ctree_root *root,
 	lower->header.nritems++;
 	if (lower->keys[1].objectid == 0)
 			BUG();
+	write_tree_block(root, path->nodes[level]);
 	return 0;
 }
 
@@ -238,6 +205,8 @@ int push_node_left(struct ctree_root *root, struct ctree_path *path, int level)
 	int push_items = 0;
 	int left_nritems;
 	int right_nritems;
+	struct tree_buffer *t;
+	struct tree_buffer *right_buf;
 
 	if (level == MAX_LEVEL - 1 || path->nodes[level + 1] == 0)
 		return 1;
@@ -245,13 +214,18 @@ int push_node_left(struct ctree_root *root, struct ctree_path *path, int level)
 	if (slot == 0)
 		return 1;
 
-	left = read_block(path->nodes[level + 1]->blockptrs[slot - 1]);
-	right = path->nodes[level];
+	t = read_tree_block(root,
+		            path->nodes[level + 1]->node.blockptrs[slot - 1]);
+	left = &t->node;
+	right_buf = path->nodes[level];
+	right = &right_buf->node;
 	left_nritems = left->header.nritems;
 	right_nritems = right->header.nritems;
 	push_items = NODEPTRS_PER_BLOCK - (left_nritems + 1);
-	if (push_items <= 0)
+	if (push_items <= 0) {
+		tree_block_release(root, t);
 		return 1;
+	}
 
 	if (right_nritems < push_items)
 		push_items = right_nritems;
@@ -267,15 +241,20 @@ int push_node_left(struct ctree_root *root, struct ctree_path *path, int level)
 	left->header.nritems += push_items;
 
 	/* adjust the pointers going up the tree */
-	fixup_low_keys(path, right->keys, level + 1);
+	fixup_low_keys(root, path, right->keys, level + 1);
+
+	write_tree_block(root, t);
+	write_tree_block(root, right_buf);
 
 	/* then fixup the leaf pointer in the path */
 	if (path->slots[level] < push_items) {
 		path->slots[level] += left_nritems;
-		path->nodes[level] = (struct node*)left;
+		tree_block_release(root, path->nodes[level]);
+		path->nodes[level] = t;
 		path->slots[level + 1] -= 1;
 	} else {
 		path->slots[level] -= push_items;
+		tree_block_release(root, t);
 	}
 	return 0;
 }
@@ -283,6 +262,8 @@ int push_node_left(struct ctree_root *root, struct ctree_path *path, int level)
 int push_node_right(struct ctree_root *root, struct ctree_path *path, int level)
 {
 	int slot;
+	struct tree_buffer *t;
+	struct tree_buffer *src_buffer;
 	struct node *dst;
 	struct node *src;
 	int push_items = 0;
@@ -295,16 +276,21 @@ int push_node_right(struct ctree_root *root, struct ctree_path *path, int level)
 	if (slot == NODEPTRS_PER_BLOCK - 1)
 		return 1;
 
-	if (slot >= path->nodes[level + 1]->header.nritems -1)
+	if (slot >= path->nodes[level + 1]->node.header.nritems -1)
 		return 1;
 
-	dst = read_block(path->nodes[level + 1]->blockptrs[slot + 1]);
-	src = path->nodes[level];
+	t = read_tree_block(root,
+			    path->nodes[level + 1]->node.blockptrs[slot + 1]);
+	dst = &t->node;
+	src_buffer = path->nodes[level];
+	src = &src_buffer->node;
 	dst_nritems = dst->header.nritems;
 	src_nritems = src->header.nritems;
 	push_items = NODEPTRS_PER_BLOCK - (dst_nritems + 1);
-	if (push_items <= 0)
+	if (push_items <= 0) {
+		tree_block_release(root, t);
 		return 1;
+	}
 
 	if (src_nritems < push_items)
 		push_items = src_nritems;
@@ -322,13 +308,21 @@ int push_node_right(struct ctree_root *root, struct ctree_path *path, int level)
 	dst->header.nritems += push_items;
 
 	/* adjust the pointers going up the tree */
-	memcpy(path->nodes[level + 1]->keys + path->slots[level + 1] + 1,
+	memcpy(path->nodes[level + 1]->node.keys + path->slots[level + 1] + 1,
 		dst->keys, sizeof(struct key));
+
+	write_tree_block(root, path->nodes[level + 1]);
+	write_tree_block(root, t);
+	write_tree_block(root, src_buffer);
+
 	/* then fixup the leaf pointer in the path */
 	if (path->slots[level] >= src->header.nritems) {
 		path->slots[level] -= src->header.nritems;
-		path->nodes[level] = (struct node*)dst;
+		tree_block_release(root, path->nodes[level]);
+		path->nodes[level] = t;
 		path->slots[level + 1] += 1;
+	} else {
+		tree_block_release(root, t);
 	}
 	return 0;
 }
@@ -337,15 +331,18 @@ int insert_ptr(struct ctree_root *root,
 		struct ctree_path *path, struct key *key,
 		u64 blocknr, int level)
 {
-	struct node *c = path->nodes[level];
+	struct tree_buffer *t = path->nodes[level];
+	struct node *c = &path->nodes[level]->node;
 	struct node *b;
-	struct node *bal[MAX_LEVEL];
+	struct tree_buffer *b_buffer;
+	struct tree_buffer *bal[MAX_LEVEL];
 	int bal_level = level;
 	int mid;
 	int bal_start = -1;
 
 	memset(bal, 0, ARRAY_SIZE(bal));
-	while(c && c->header.nritems == NODEPTRS_PER_BLOCK) {
+	while(t && t->node.header.nritems == NODEPTRS_PER_BLOCK) {
+		c = &t->node;
 		if (push_node_left(root, path,
 		   node_level(c->header.flags)) == 0)
 			break;
@@ -355,8 +352,10 @@ int insert_ptr(struct ctree_root *root,
 		bal_start = bal_level;
 		if (bal_level == MAX_LEVEL - 1)
 			BUG();
-		b = malloc(sizeof(struct node));
+		b_buffer = alloc_free_block(root);
+		b = &b_buffer->node;
 		b->header.flags = c->header.flags;
+		b->header.blocknr = b_buffer->blocknr;
 		mid = (c->header.nritems + 1) / 2;
 		memcpy(b->keys, c->keys + mid,
 			(c->header.nritems - mid) * sizeof(struct key));
@@ -364,21 +363,28 @@ int insert_ptr(struct ctree_root *root,
 			(c->header.nritems - mid) * sizeof(u64));
 		b->header.nritems = c->header.nritems - mid;
 		c->header.nritems = mid;
-		bal[bal_level] = b;
+
+		write_tree_block(root, t);
+		write_tree_block(root, b_buffer);
+
+		bal[bal_level] = b_buffer;
 		if (bal_level == MAX_LEVEL - 1)
 			break;
 		bal_level += 1;
-		c = path->nodes[bal_level];
+		t = path->nodes[bal_level];
 	}
 	while(bal_start > 0) {
-		b = bal[bal_start];
-		c = path->nodes[bal_start];
-		__insert_ptr(root, path, b->keys, (u64)b,
+		b_buffer = bal[bal_start];
+		c = &path->nodes[bal_start]->node;
+		__insert_ptr(root, path, b_buffer->node.keys, b_buffer->blocknr,
 				path->slots[bal_start + 1] + 1, bal_start + 1);
 		if (path->slots[bal_start] >= c->header.nritems) {
 			path->slots[bal_start] -= c->header.nritems;
-			path->nodes[bal_start] = b;
+			tree_block_release(root, path->nodes[bal_start]);
+			path->nodes[bal_start] = b_buffer;
 			path->slots[bal_start + 1] += 1;
+		} else {
+			tree_block_release(root, b_buffer);
 		}
 		bal_start--;
 		if (!bal[bal_start])
@@ -404,7 +410,9 @@ int leaf_space_used(struct leaf *l, int start, int nr)
 int push_leaf_left(struct ctree_root *root, struct ctree_path *path,
 		   int data_size)
 {
-	struct leaf *right = (struct leaf *)path->nodes[0];
+	struct tree_buffer *right_buf = path->nodes[0];
+	struct leaf *right = &right_buf->leaf;
+	struct tree_buffer *t;
 	struct leaf *left;
 	int slot;
 	int i;
@@ -421,9 +429,11 @@ int push_leaf_left(struct ctree_root *root, struct ctree_path *path,
 	if (!path->nodes[1]) {
 		return 1;
 	}
-	left = read_block(path->nodes[1]->blockptrs[slot - 1]);
+	t = read_tree_block(root, path->nodes[1]->node.blockptrs[slot - 1]);
+	left = &t->leaf;
 	free_space = leaf_free_space(left);
 	if (free_space < data_size + sizeof(struct item)) {
+		tree_block_release(root, t);
 		return 1;
 	}
 	for (i = 0; i < right->header.nritems; i++) {
@@ -436,6 +446,7 @@ int push_leaf_left(struct ctree_root *root, struct ctree_path *path,
 		push_space += item->size + sizeof(*item);
 	}
 	if (push_items == 0) {
+		tree_block_release(root, t);
 		return 1;
 	}
 	/* push data from right to left */
@@ -446,6 +457,8 @@ int push_leaf_left(struct ctree_root *root, struct ctree_path *path,
 		right->data + right->items[push_items - 1].offset,
 		push_space);
 	old_left_nritems = left->header.nritems;
+	BUG_ON(old_left_nritems < 0);
+
 	for(i = old_left_nritems; i < old_left_nritems + push_items; i++) {
 		left->items[i].offset -= LEAF_DATA_SIZE -
 			left->items[old_left_nritems -1].offset;
@@ -460,30 +473,40 @@ int push_leaf_left(struct ctree_root *root, struct ctree_path *path,
 		(right->header.nritems - push_items) * sizeof(struct item));
 	right->header.nritems -= push_items;
 	push_space = LEAF_DATA_SIZE;
+
 	for (i = 0; i < right->header.nritems; i++) {
 		right->items[i].offset = push_space - right->items[i].size;
 		push_space = right->items[i].offset;
 	}
-	fixup_low_keys(path, &right->items[0].key, 1);
+
+	write_tree_block(root, t);
+	write_tree_block(root, right_buf);
+
+	fixup_low_keys(root, path, &right->items[0].key, 1);
 
 	/* then fixup the leaf pointer in the path */
 	if (path->slots[0] < push_items) {
 		path->slots[0] += old_left_nritems;
-		path->nodes[0] = (struct node*)left;
+		tree_block_release(root, path->nodes[0]);
+		path->nodes[0] = t;
 		path->slots[1] -= 1;
 	} else {
+		tree_block_release(root, t);
 		path->slots[0] -= push_items;
 	}
+	BUG_ON(path->slots[0] < 0);
 	return 0;
 }
 
 int split_leaf(struct ctree_root *root, struct ctree_path *path, int data_size)
 {
-	struct leaf *l = (struct leaf *)path->nodes[0];
-	int nritems = l->header.nritems;
-	int mid = (nritems + 1)/ 2;
-	int slot = path->slots[0];
+	struct tree_buffer *l_buf = path->nodes[0];
+	struct leaf *l = &l_buf->leaf;
+	int nritems;
+	int mid;
+	int slot;
 	struct leaf *right;
+	struct tree_buffer *right_buffer;
 	int space_needed = data_size + sizeof(struct item);
 	int data_copy_size;
 	int rt_data_off;
@@ -491,9 +514,19 @@ int split_leaf(struct ctree_root *root, struct ctree_path *path, int data_size)
 	int ret;
 
 	if (push_leaf_left(root, path, data_size) == 0) {
-		return 0;
+		l_buf = path->nodes[0];
+		l = &l_buf->leaf;
+		if (leaf_free_space(l) >= sizeof(struct item) + data_size)
+			return 0;
 	}
-	right = malloc(sizeof(struct leaf));
+	slot = path->slots[0];
+	nritems = l->header.nritems;
+	mid = (nritems + 1)/ 2;
+
+	right_buffer = alloc_free_block(root);
+	BUG_ON(!right_buffer);
+	BUG_ON(mid == nritems);
+	right = &right_buffer->leaf;
 	memset(right, 0, sizeof(*right));
 	if (mid <= slot) {
 		if (leaf_space_used(l, mid, nritems - mid) + space_needed >
@@ -505,6 +538,8 @@ int split_leaf(struct ctree_root *root, struct ctree_path *path, int data_size)
 			BUG();
 	}
 	right->header.nritems = nritems - mid;
+	right->header.blocknr = right_buffer->blocknr;
+	right->header.flags = node_level(0);
 	data_copy_size = l->items[mid].offset + l->items[mid].size -
 			 leaf_data_end(l);
 	memcpy(right->items, l->items + mid,
@@ -518,12 +553,20 @@ int split_leaf(struct ctree_root *root, struct ctree_path *path, int data_size)
 	}
 	l->header.nritems = mid;
 	ret = insert_ptr(root, path, &right->items[0].key,
-			  (u64)right, 1);
+			  right_buffer->blocknr, 1);
+
+	write_tree_block(root, right_buffer);
+	write_tree_block(root, l_buf);
+
+	BUG_ON(path->slots[0] != slot);
 	if (mid <= slot) {
-		path->nodes[0] = (struct node *)right;
+		tree_block_release(root, path->nodes[0]);
+		path->nodes[0] = right_buffer;
 		path->slots[0] -= mid;
 		path->slots[1] += 1;
-	}
+	} else
+		tree_block_release(root, right_buffer);
+	BUG_ON(path->slots[0] < 0);
 	return ret;
 }
 
@@ -532,28 +575,48 @@ int insert_item(struct ctree_root *root, struct key *key,
 {
 	int ret;
 	int slot;
+	int slot_orig;
 	struct leaf *leaf;
+	struct tree_buffer *leaf_buf;
 	unsigned int nritems;
 	unsigned int data_end;
 	struct ctree_path path;
 
+	if (!root->node) {
+		struct tree_buffer *t;
+		t = alloc_free_block(root);
+		BUG_ON(!t);
+		t->node.header.nritems = 0;
+		t->node.header.flags = node_level(0);
+		t->node.header.blocknr = t->blocknr;
+		root->node = t;
+		write_tree_block(root, t);
+	}
 	init_path(&path);
 	ret = search_slot(root, key, &path);
-	if (ret == 0)
+	if (ret == 0) {
+		release_path(root, &path);
 		return -EEXIST;
+	}
 
-	leaf = (struct leaf *)path.nodes[0];
-	if (leaf_free_space(leaf) <  sizeof(struct item) + data_size)
+	slot_orig = path.slots[0];
+	leaf_buf = path.nodes[0];
+	leaf = &leaf_buf->leaf;
+	if (leaf_free_space(leaf) <  sizeof(struct item) + data_size) {
 		split_leaf(root, &path, data_size);
-	leaf = (struct leaf *)path.nodes[0];
+		leaf_buf = path.nodes[0];
+		leaf = &path.nodes[0]->leaf;
+	}
 	nritems = leaf->header.nritems;
 	data_end = leaf_data_end(leaf);
+
 	if (leaf_free_space(leaf) <  sizeof(struct item) + data_size)
 		BUG();
 
 	slot = path.slots[0];
+	BUG_ON(slot < 0);
 	if (slot == 0)
-		fixup_low_keys(&path, key, 1);
+		fixup_low_keys(root, &path, key, 1);
 	if (slot != nritems) {
 		int i;
 		unsigned int old_data = leaf->items[slot].offset +
@@ -580,21 +643,25 @@ int insert_item(struct ctree_root *root, struct key *key,
 	leaf->items[slot].size = data_size;
 	memcpy(leaf->data + data_end - data_size, data, data_size);
 	leaf->header.nritems += 1;
+	write_tree_block(root, leaf_buf);
 	if (leaf_free_space(leaf) < 0)
 		BUG();
+	release_path(root, &path);
 	return 0;
 }
 
 int del_ptr(struct ctree_root *root, struct ctree_path *path, int level)
 {
 	int slot;
+	struct tree_buffer *t;
 	struct node *node;
 	int nritems;
 
 	while(1) {
-		node = path->nodes[level];
-		if (!node)
+		t = path->nodes[level];
+		if (!t)
 			break;
+		node = &t->node;
 		slot = path->slots[level];
 		nritems = node->header.nritems;
 
@@ -606,28 +673,34 @@ int del_ptr(struct ctree_root *root, struct ctree_path *path, int level)
 				sizeof(u64) * (nritems - slot - 1));
 		}
 		node->header.nritems--;
+		write_tree_block(root, t);
 		if (node->header.nritems != 0) {
 			int tslot;
 			if (slot == 0)
-				fixup_low_keys(path, node->keys, level + 1);
+				fixup_low_keys(root, path, node->keys,
+					       level + 1);
 			tslot = path->slots[level+1];
+			t->count++;
 			push_node_left(root, path, level);
 			if (node->header.nritems) {
 				push_node_right(root, path, level);
 			}
-			if (node->header.nritems)
+			if (node->header.nritems) {
+				tree_block_release(root, t);
 				break;
+			}
+			tree_block_release(root, t);
 			path->slots[level+1] = tslot;
 		}
-		if (node == root->node) {
-			printf("root is now null!\n");
-			root->node = NULL;
+		if (t == root->node) {
+			/* just turn the root into a leaf and break */
+			root->node->node.header.flags = node_level(0);
+			write_tree_block(root, t);
 			break;
 		}
 		level++;
 		if (!path->nodes[level])
 			BUG();
-		free(node);
 	}
 	return 0;
 }
@@ -636,10 +709,12 @@ int del_item(struct ctree_root *root, struct ctree_path *path)
 {
 	int slot;
 	struct leaf *leaf;
+	struct tree_buffer *leaf_buf;
 	int doff;
 	int dsize;
 
-	leaf = (struct leaf *)path->nodes[0];
+	leaf_buf = path->nodes[0];
+	leaf = &leaf_buf->leaf;
 	slot = path->slots[0];
 	doff = leaf->items[slot].offset;
 	dsize = leaf->items[slot].size;
@@ -658,14 +733,15 @@ int del_item(struct ctree_root *root, struct ctree_path *path)
 	}
 	leaf->header.nritems -= 1;
 	if (leaf->header.nritems == 0) {
-		if (leaf == (struct leaf *)root->node)
-			root->node = NULL;
-		else
+		if (leaf_buf == root->node) {
+			leaf->header.flags = node_level(0);
+			write_tree_block(root, leaf_buf);
+		} else
 			del_ptr(root, path, 1);
-		free(leaf);
 	} else {
 		if (slot == 0)
-			fixup_low_keys(path, &leaf->items[0].key, 1);
+			fixup_low_keys(root, path, &leaf->items[0].key, 1);
+		write_tree_block(root, leaf_buf);
 		if (leaf_space_used(leaf, 0, leaf->header.nritems) <
 		    LEAF_DATA_SIZE / 4) {
 			/* push_leaf_left fixes the path.
@@ -673,12 +749,13 @@ int del_item(struct ctree_root *root, struct ctree_path *path)
 			 * for possible call to del_ptr below
 			 */
 			slot = path->slots[1];
+			leaf_buf->count++;
 			push_leaf_left(root, path, 1);
 			if (leaf->header.nritems == 0) {
-				free(leaf);
 				path->slots[1] = slot;
 				del_ptr(root, path, 1);
 			}
+			tree_block_release(root, leaf_buf);
 		}
 	}
 	return 0;
@@ -689,7 +766,7 @@ void print_leaf(struct leaf *l)
 	int i;
 	int nr = l->header.nritems;
 	struct item *item;
-	printf("leaf %p total ptrs %d free space %d\n", l, nr,
+	printf("leaf %lu total ptrs %d free space %d\n", l->header.blocknr, nr,
 	       leaf_free_space(l));
 	fflush(stdout);
 	for (i = 0 ; i < nr ; i++) {
@@ -703,38 +780,45 @@ void print_leaf(struct leaf *l)
 		fflush(stdout);
 	}
 }
-void print_tree(struct node *c)
+void print_tree(struct ctree_root *root, struct tree_buffer *t)
 {
 	int i;
 	int nr;
+	struct node *c;
 
-	if (!c)
+	if (!t)
 		return;
+	c = &t->node;
 	nr = c->header.nritems;
+	if (c->header.blocknr != t->blocknr)
+		BUG();
 	if (is_leaf(c->header.flags)) {
 		print_leaf((struct leaf *)c);
 		return;
 	}
-	printf("node %p level %d total ptrs %d free spc %lu\n", c,
+	printf("node %lu level %d total ptrs %d free spc %lu\n", t->blocknr,
 	        node_level(c->header.flags), c->header.nritems,
 		NODEPTRS_PER_BLOCK - c->header.nritems);
 	fflush(stdout);
 	for (i = 0; i < nr; i++) {
-		printf("\tkey %d (%lu %u %lu) block %lx\n",
+		printf("\tkey %d (%lu %u %lu) block %lu\n",
 		       i,
 		       c->keys[i].objectid, c->keys[i].flags, c->keys[i].offset,
 		       c->blockptrs[i]);
 		fflush(stdout);
 	}
 	for (i = 0; i < nr; i++) {
-		struct node *next = read_block(c->blockptrs[i]);
+		struct tree_buffer *next_buf = read_tree_block(root,
+							    c->blockptrs[i]);
+		struct node *next = &next_buf->node;
 		if (is_leaf(next->header.flags) &&
 		    node_level(c->header.flags) != 1)
 			BUG();
 		if (node_level(next->header.flags) !=
 			node_level(c->header.flags) - 1)
 			BUG();
-		print_tree(next);
+		print_tree(root, next_buf);
+		tree_block_release(root, next_buf);
 	}
 
 }
@@ -746,23 +830,24 @@ int next_key(int i, int max_key) {
 }
 
 int main() {
-	struct leaf *first_node = malloc(sizeof(struct leaf));
-	struct ctree_root root;
+	struct ctree_root *root;
 	struct key ins;
 	struct key last = { (u64)-1, 0, 0};
 	char *buf;
 	int i;
 	int num;
 	int ret;
-	int run_size = 100000;
+	int run_size = 1000000;
 	int max_key = 100000000;
 	int tree_size = 0;
 	struct ctree_path path;
 
+	radix_tree_init();
+
+
+	root = open_ctree("dbfile");
 
 	srand(55);
-	root.node = (struct node *)first_node;
-	memset(first_node, 0, sizeof(*first_node));
 	for (i = 0; i < run_size; i++) {
 		buf = malloc(64);
 		num = next_key(i, max_key);
@@ -772,39 +857,46 @@ int main() {
 		ins.objectid = num;
 		ins.offset = 0;
 		ins.flags = 0;
-		ret = insert_item(&root, &ins, buf, strlen(buf));
+		ret = insert_item(root, &ins, buf, strlen(buf));
 		if (!ret)
 			tree_size++;
 	}
+	close_ctree(root);
+	root = open_ctree("dbfile");
+	printf("starting search\n");
 	srand(55);
 	for (i = 0; i < run_size; i++) {
 		num = next_key(i, max_key);
 		ins.objectid = num;
 		init_path(&path);
-		ret = search_slot(&root, &ins, &path);
+		ret = search_slot(root, &ins, &path);
 		if (ret) {
-			print_tree(root.node);
+			print_tree(root, root->node);
 			printf("unable to find %d\n", num);
 			exit(1);
 		}
-	}
-	printf("node %p level %d total ptrs %d free spc %lu\n", root.node,
-	        node_level(root.node->header.flags), root.node->header.nritems,
-		NODEPTRS_PER_BLOCK - root.node->header.nritems);
-	// print_tree(root.node);
-	printf("all searches good\n");
+		release_path(root, &path);
+	}
+	close_ctree(root);
+	root = open_ctree("dbfile");
+	printf("node %p level %d total ptrs %d free spc %lu\n", root->node,
+	        node_level(root->node->node.header.flags),
+		root->node->node.header.nritems,
+		NODEPTRS_PER_BLOCK - root->node->node.header.nritems);
+	printf("all searches good, deleting some items\n");
 	i = 0;
 	srand(55);
 	for (i = 0 ; i < run_size/4; i++) {
 		num = next_key(i, max_key);
 		ins.objectid = num;
 		init_path(&path);
-		ret = search_slot(&root, &ins, &path);
+		ret = search_slot(root, &ins, &path);
 		if (ret)
 			continue;
-		ret = del_item(&root, &path);
+		ret = del_item(root, &path);
 		if (ret != 0)
 			BUG();
+		release_path(root, &path);
 		tree_size--;
 	}
 	srand(128);
@@ -813,38 +905,58 @@ int main() {
 		num = next_key(i, max_key);
 		sprintf(buf, "string-%d", num);
 		ins.objectid = num;
-		ret = insert_item(&root, &ins, buf, strlen(buf));
+		ret = insert_item(root, &ins, buf, strlen(buf));
 		if (!ret)
 			tree_size++;
 	}
-	while(root.node) {
+	close_ctree(root);
+	root = open_ctree("dbfile");
+	printf("starting search2\n");
+	srand(128);
+	for (i = 0; i < run_size; i++) {
+		num = next_key(i, max_key);
+		ins.objectid = num;
+		init_path(&path);
+		ret = search_slot(root, &ins, &path);
+		if (ret) {
+			print_tree(root, root->node);
+			printf("unable to find %d\n", num);
+			exit(1);
+		}
+		release_path(root, &path);
+	}
+	printf("starting big long delete run\n");
+	while(root->node && root->node->node.header.nritems > 0) {
 		struct leaf *leaf;
 		int slot;
 		ins.objectid = (u64)-1;
 		init_path(&path);
-		ret = search_slot(&root, &ins, &path);
+		ret = search_slot(root, &ins, &path);
 		if (ret == 0)
 			BUG();
 
-		leaf = (struct leaf *)(path.nodes[0]);
+		leaf = &path.nodes[0]->leaf;
 		slot = path.slots[0];
 		if (slot != leaf->header.nritems)
 			BUG();
 		while(path.slots[0] > 0) {
 			path.slots[0] -= 1;
 			slot = path.slots[0];
-			leaf = (struct leaf *)(path.nodes[0]);
+			leaf = &path.nodes[0]->leaf;
 
 			if (comp_keys(&last, &leaf->items[slot].key) <= 0)
 				BUG();
 			memcpy(&last, &leaf->items[slot].key, sizeof(last));
-			ret = del_item(&root, &path);
-			if (ret != 0)
+			ret = del_item(root, &path);
+			if (ret != 0) {
+				printf("del_item returned %d\n", ret);
 				BUG();
+			}
 			tree_size--;
 		}
+		release_path(root, &path);
 	}
-	print_tree(root.node);
+	close_ctree(root);
 	printf("tree size is now %d\n", tree_size);
 	return 0;
 }

commit 4920c9ac9a4bbc6bf9acd8c614987ee6b378e78f
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Jan 26 16:38:42 2007 -0500

    Btrfs: Faster deletes, add Makefile and kerncompat
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 893fd56960a0..4bf5e92584bd 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -615,9 +615,9 @@ int del_ptr(struct ctree_root *root, struct ctree_path *path, int level)
 			if (node->header.nritems) {
 				push_node_right(root, path, level);
 			}
-			path->slots[level+1] = tslot;
 			if (node->header.nritems)
 				break;
+			path->slots[level+1] = tslot;
 		}
 		if (node == root->node) {
 			printf("root is now null!\n");
@@ -632,22 +632,15 @@ int del_ptr(struct ctree_root *root, struct ctree_path *path, int level)
 	return 0;
 }
 
-int del_item(struct ctree_root *root, struct key *key)
+int del_item(struct ctree_root *root, struct ctree_path *path)
 {
-	int ret;
 	int slot;
 	struct leaf *leaf;
-	struct ctree_path path;
 	int doff;
 	int dsize;
 
-	init_path(&path);
-	ret = search_slot(root, key, &path);
-	if (ret != 0)
-		return -1;
-
-	leaf = (struct leaf *)path.nodes[0];
-	slot = path.slots[0];
+	leaf = (struct leaf *)path->nodes[0];
+	slot = path->slots[0];
 	doff = leaf->items[slot].offset;
 	dsize = leaf->items[slot].size;
 
@@ -665,23 +658,26 @@ int del_item(struct ctree_root *root, struct key *key)
 	}
 	leaf->header.nritems -= 1;
 	if (leaf->header.nritems == 0) {
+		if (leaf == (struct leaf *)root->node)
+			root->node = NULL;
+		else
+			del_ptr(root, path, 1);
 		free(leaf);
-		del_ptr(root, &path, 1);
 	} else {
 		if (slot == 0)
-			fixup_low_keys(&path, &leaf->items[0].key, 1);
+			fixup_low_keys(path, &leaf->items[0].key, 1);
 		if (leaf_space_used(leaf, 0, leaf->header.nritems) <
 		    LEAF_DATA_SIZE / 4) {
 			/* push_leaf_left fixes the path.
 			 * make sure the path still points to our leaf
 			 * for possible call to del_ptr below
 			 */
-			slot = path.slots[1];
-			push_leaf_left(root, &path, 1);
-			path.slots[1] = slot;
+			slot = path->slots[1];
+			push_leaf_left(root, path, 1);
 			if (leaf->header.nritems == 0) {
 				free(leaf);
-				del_ptr(root, &path, 1);
+				path->slots[1] = slot;
+				del_ptr(root, path, 1);
 			}
 		}
 	}
@@ -753,11 +749,12 @@ int main() {
 	struct leaf *first_node = malloc(sizeof(struct leaf));
 	struct ctree_root root;
 	struct key ins;
+	struct key last = { (u64)-1, 0, 0};
 	char *buf;
 	int i;
 	int num;
 	int ret;
-	int run_size = 10000000;
+	int run_size = 100000;
 	int max_key = 100000000;
 	int tree_size = 0;
 	struct ctree_path path;
@@ -783,8 +780,6 @@ int main() {
 	for (i = 0; i < run_size; i++) {
 		num = next_key(i, max_key);
 		ins.objectid = num;
-		ins.offset = 0;
-		ins.flags = 0;
 		init_path(&path);
 		ret = search_slot(&root, &ins, &path);
 		if (ret) {
@@ -800,11 +795,56 @@ int main() {
 	printf("all searches good\n");
 	i = 0;
 	srand(55);
+	for (i = 0 ; i < run_size/4; i++) {
+		num = next_key(i, max_key);
+		ins.objectid = num;
+		init_path(&path);
+		ret = search_slot(&root, &ins, &path);
+		if (ret)
+			continue;
+		ret = del_item(&root, &path);
+		if (ret != 0)
+			BUG();
+		tree_size--;
+	}
+	srand(128);
 	for (i = 0; i < run_size; i++) {
+		buf = malloc(64);
 		num = next_key(i, max_key);
+		sprintf(buf, "string-%d", num);
 		ins.objectid = num;
-		del_item(&root, &ins);
+		ret = insert_item(&root, &ins, buf, strlen(buf));
+		if (!ret)
+			tree_size++;
+	}
+	while(root.node) {
+		struct leaf *leaf;
+		int slot;
+		ins.objectid = (u64)-1;
+		init_path(&path);
+		ret = search_slot(&root, &ins, &path);
+		if (ret == 0)
+			BUG();
+
+		leaf = (struct leaf *)(path.nodes[0]);
+		slot = path.slots[0];
+		if (slot != leaf->header.nritems)
+			BUG();
+		while(path.slots[0] > 0) {
+			path.slots[0] -= 1;
+			slot = path.slots[0];
+			leaf = (struct leaf *)(path.nodes[0]);
+
+			if (comp_keys(&last, &leaf->items[slot].key) <= 0)
+				BUG();
+			memcpy(&last, &leaf->items[slot].key, sizeof(last));
+			ret = del_item(&root, &path);
+			if (ret != 0)
+				BUG();
+			tree_size--;
+		}
 	}
 	print_tree(root.node);
+	printf("tree size is now %d\n", tree_size);
 	return 0;
 }

commit be0e5c097fc206b863ce9fe6b3cfd6974b0110f4
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Jan 26 15:51:26 2007 -0500

    Btrfs: Initial checkin, basic working tree code
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
new file mode 100644
index 000000000000..893fd56960a0
--- /dev/null
+++ b/fs/btrfs/ctree.c
@@ -0,0 +1,810 @@
+#include <stdio.h>
+#include <stdlib.h>
+#include "kerncompat.h"
+
+#define BLOCKSIZE 4096
+
+struct key {
+	u64 objectid;
+	u32 flags;
+	u64 offset;
+} __attribute__ ((__packed__));
+
+struct header {
+	u64 fsid[2]; /* FS specific uuid */
+	u64 blocknum;
+	u64 parentid;
+	u32 csum;
+	u32 ham;
+	u16 nritems;
+	u16 flags;
+} __attribute__ ((__packed__));
+
+#define NODEPTRS_PER_BLOCK ((BLOCKSIZE - sizeof(struct header)) / \
+			    (sizeof(struct key) + sizeof(u64)))
+
+#define LEVEL_BITS 3
+#define MAX_LEVEL (1 << LEVEL_BITS)
+#define node_level(f) ((f) & (MAX_LEVEL-1))
+#define is_leaf(f) (node_level(f) == 0)
+
+struct ctree_root {
+	struct node *node;
+};
+
+struct item {
+	struct key key;
+	u16 offset;
+	u16 size;
+} __attribute__ ((__packed__));
+
+#define LEAF_DATA_SIZE (BLOCKSIZE - sizeof(struct header))
+struct leaf {
+	struct header header;
+	union {
+		struct item items[LEAF_DATA_SIZE/sizeof(struct item)];
+		u8 data[BLOCKSIZE-sizeof(struct header)];
+	};
+} __attribute__ ((__packed__));
+
+struct node {
+	struct header header;
+	struct key keys[NODEPTRS_PER_BLOCK];
+	u64 blockptrs[NODEPTRS_PER_BLOCK];
+} __attribute__ ((__packed__));
+
+struct ctree_path {
+	struct node *nodes[MAX_LEVEL];
+	int slots[MAX_LEVEL];
+};
+
+static inline void init_path(struct ctree_path *p)
+{
+	memset(p, 0, sizeof(*p));
+}
+
+static inline unsigned int leaf_data_end(struct leaf *leaf)
+{
+	unsigned int nr = leaf->header.nritems;
+	if (nr == 0)
+		return ARRAY_SIZE(leaf->data);
+	return leaf->items[nr-1].offset;
+}
+
+static inline int leaf_free_space(struct leaf *leaf)
+{
+	int data_end = leaf_data_end(leaf);
+	int nritems = leaf->header.nritems;
+	char *items_end = (char *)(leaf->items + nritems + 1);
+	return (char *)(leaf->data + data_end) - (char *)items_end;
+}
+
+int comp_keys(struct key *k1, struct key *k2)
+{
+	if (k1->objectid > k2->objectid)
+		return 1;
+	if (k1->objectid < k2->objectid)
+		return -1;
+	if (k1->flags > k2->flags)
+		return 1;
+	if (k1->flags < k2->flags)
+		return -1;
+	if (k1->offset > k2->offset)
+		return 1;
+	if (k1->offset < k2->offset)
+		return -1;
+	return 0;
+}
+int generic_bin_search(char *p, int item_size, struct key *key,
+		       int max, int *slot)
+{
+	int low = 0;
+	int high = max;
+	int mid;
+	int ret;
+	struct key *tmp;
+
+	while(low < high) {
+		mid = (low + high) / 2;
+		tmp = (struct key *)(p + mid * item_size);
+		ret = comp_keys(tmp, key);
+
+		if (ret < 0)
+			low = mid + 1;
+		else if (ret > 0)
+			high = mid;
+		else {
+			*slot = mid;
+			return 0;
+		}
+	}
+	*slot = low;
+	return 1;
+}
+
+int bin_search(struct node *c, struct key *key, int *slot)
+{
+	if (is_leaf(c->header.flags)) {
+		struct leaf *l = (struct leaf *)c;
+		return generic_bin_search((void *)l->items, sizeof(struct item),
+					  key, c->header.nritems, slot);
+	} else {
+		return generic_bin_search((void *)c->keys, sizeof(struct key),
+					  key, c->header.nritems, slot);
+	}
+	return -1;
+}
+
+void *read_block(u64 blocknum)
+{
+	return (void *)blocknum;
+}
+
+int search_slot(struct ctree_root *root, struct key *key, struct ctree_path *p)
+{
+	struct node *c = root->node;
+	int slot;
+	int ret;
+	int level;
+	while (c) {
+		level = node_level(c->header.flags);
+		p->nodes[level] = c;
+		ret = bin_search(c, key, &slot);
+		if (!is_leaf(c->header.flags)) {
+			if (ret && slot > 0)
+				slot -= 1;
+			p->slots[level] = slot;
+			c = read_block(c->blockptrs[slot]);
+			continue;
+		} else {
+			p->slots[level] = slot;
+			return ret;
+		}
+	}
+	return -1;
+}
+
+static void fixup_low_keys(struct ctree_path *path, struct key *key,
+			     int level)
+{
+	int i;
+	/* adjust the pointers going up the tree */
+	for (i = level; i < MAX_LEVEL; i++) {
+		struct node *t = path->nodes[i];
+		int tslot = path->slots[i];
+		if (!t)
+			break;
+		memcpy(t->keys + tslot, key, sizeof(*key));
+		if (tslot != 0)
+			break;
+	}
+}
+
+int __insert_ptr(struct ctree_root *root,
+		struct ctree_path *path, struct key *key,
+		u64 blocknr, int slot, int level)
+{
+	struct node *c;
+	struct node *lower;
+	struct key *lower_key;
+	int nritems;
+	/* need a new root */
+	if (!path->nodes[level]) {
+		c = malloc(sizeof(struct node));
+		memset(c, 0, sizeof(c));
+		c->header.nritems = 2;
+		c->header.flags = node_level(level);
+		lower = path->nodes[level-1];
+		if (is_leaf(lower->header.flags))
+			lower_key = &((struct leaf *)lower)->items[0].key;
+		else
+			lower_key = lower->keys;
+		memcpy(c->keys, lower_key, sizeof(struct key));
+		memcpy(c->keys + 1, key, sizeof(struct key));
+		c->blockptrs[0] = (u64)lower;
+		c->blockptrs[1] = blocknr;
+		root->node = c;
+		path->nodes[level] = c;
+		path->slots[level] = 0;
+		if (c->keys[1].objectid == 0)
+			BUG();
+		return 0;
+	}
+	lower = path->nodes[level];
+	nritems = lower->header.nritems;
+	if (slot > nritems)
+		BUG();
+	if (nritems == NODEPTRS_PER_BLOCK)
+		BUG();
+	if (slot != nritems) {
+		memmove(lower->keys + slot + 1, lower->keys + slot,
+			(nritems - slot) * sizeof(struct key));
+		memmove(lower->blockptrs + slot + 1, lower->blockptrs + slot,
+			(nritems - slot) * sizeof(u64));
+	}
+	memcpy(lower->keys + slot, key, sizeof(struct key));
+	lower->blockptrs[slot] = blocknr;
+	lower->header.nritems++;
+	if (lower->keys[1].objectid == 0)
+			BUG();
+	return 0;
+}
+
+int push_node_left(struct ctree_root *root, struct ctree_path *path, int level)
+{
+	int slot;
+	struct node *left;
+	struct node *right;
+	int push_items = 0;
+	int left_nritems;
+	int right_nritems;
+
+	if (level == MAX_LEVEL - 1 || path->nodes[level + 1] == 0)
+		return 1;
+	slot = path->slots[level + 1];
+	if (slot == 0)
+		return 1;
+
+	left = read_block(path->nodes[level + 1]->blockptrs[slot - 1]);
+	right = path->nodes[level];
+	left_nritems = left->header.nritems;
+	right_nritems = right->header.nritems;
+	push_items = NODEPTRS_PER_BLOCK - (left_nritems + 1);
+	if (push_items <= 0)
+		return 1;
+
+	if (right_nritems < push_items)
+		push_items = right_nritems;
+	memcpy(left->keys + left_nritems, right->keys,
+		push_items * sizeof(struct key));
+	memcpy(left->blockptrs + left_nritems, right->blockptrs,
+		push_items * sizeof(u64));
+	memmove(right->keys, right->keys + push_items,
+		(right_nritems - push_items) * sizeof(struct key));
+	memmove(right->blockptrs, right->blockptrs + push_items,
+		(right_nritems - push_items) * sizeof(u64));
+	right->header.nritems -= push_items;
+	left->header.nritems += push_items;
+
+	/* adjust the pointers going up the tree */
+	fixup_low_keys(path, right->keys, level + 1);
+
+	/* then fixup the leaf pointer in the path */
+	if (path->slots[level] < push_items) {
+		path->slots[level] += left_nritems;
+		path->nodes[level] = (struct node*)left;
+		path->slots[level + 1] -= 1;
+	} else {
+		path->slots[level] -= push_items;
+	}
+	return 0;
+}
+
+int push_node_right(struct ctree_root *root, struct ctree_path *path, int level)
+{
+	int slot;
+	struct node *dst;
+	struct node *src;
+	int push_items = 0;
+	int dst_nritems;
+	int src_nritems;
+
+	if (level == MAX_LEVEL - 1 || path->nodes[level + 1] == 0)
+		return 1;
+	slot = path->slots[level + 1];
+	if (slot == NODEPTRS_PER_BLOCK - 1)
+		return 1;
+
+	if (slot >= path->nodes[level + 1]->header.nritems -1)
+		return 1;
+
+	dst = read_block(path->nodes[level + 1]->blockptrs[slot + 1]);
+	src = path->nodes[level];
+	dst_nritems = dst->header.nritems;
+	src_nritems = src->header.nritems;
+	push_items = NODEPTRS_PER_BLOCK - (dst_nritems + 1);
+	if (push_items <= 0)
+		return 1;
+
+	if (src_nritems < push_items)
+		push_items = src_nritems;
+	memmove(dst->keys + push_items, dst->keys,
+		dst_nritems * sizeof(struct key));
+	memcpy(dst->keys, src->keys + src_nritems - push_items,
+		push_items * sizeof(struct key));
+
+	memmove(dst->blockptrs + push_items, dst->blockptrs,
+		dst_nritems * sizeof(u64));
+	memcpy(dst->blockptrs, src->blockptrs + src_nritems - push_items,
+		push_items * sizeof(u64));
+
+	src->header.nritems -= push_items;
+	dst->header.nritems += push_items;
+
+	/* adjust the pointers going up the tree */
+	memcpy(path->nodes[level + 1]->keys + path->slots[level + 1] + 1,
+		dst->keys, sizeof(struct key));
+	/* then fixup the leaf pointer in the path */
+	if (path->slots[level] >= src->header.nritems) {
+		path->slots[level] -= src->header.nritems;
+		path->nodes[level] = (struct node*)dst;
+		path->slots[level + 1] += 1;
+	}
+	return 0;
+}
+
+int insert_ptr(struct ctree_root *root,
+		struct ctree_path *path, struct key *key,
+		u64 blocknr, int level)
+{
+	struct node *c = path->nodes[level];
+	struct node *b;
+	struct node *bal[MAX_LEVEL];
+	int bal_level = level;
+	int mid;
+	int bal_start = -1;
+
+	memset(bal, 0, ARRAY_SIZE(bal));
+	while(c && c->header.nritems == NODEPTRS_PER_BLOCK) {
+		if (push_node_left(root, path,
+		   node_level(c->header.flags)) == 0)
+			break;
+		if (push_node_right(root, path,
+		   node_level(c->header.flags)) == 0)
+			break;
+		bal_start = bal_level;
+		if (bal_level == MAX_LEVEL - 1)
+			BUG();
+		b = malloc(sizeof(struct node));
+		b->header.flags = c->header.flags;
+		mid = (c->header.nritems + 1) / 2;
+		memcpy(b->keys, c->keys + mid,
+			(c->header.nritems - mid) * sizeof(struct key));
+		memcpy(b->blockptrs, c->blockptrs + mid,
+			(c->header.nritems - mid) * sizeof(u64));
+		b->header.nritems = c->header.nritems - mid;
+		c->header.nritems = mid;
+		bal[bal_level] = b;
+		if (bal_level == MAX_LEVEL - 1)
+			break;
+		bal_level += 1;
+		c = path->nodes[bal_level];
+	}
+	while(bal_start > 0) {
+		b = bal[bal_start];
+		c = path->nodes[bal_start];
+		__insert_ptr(root, path, b->keys, (u64)b,
+				path->slots[bal_start + 1] + 1, bal_start + 1);
+		if (path->slots[bal_start] >= c->header.nritems) {
+			path->slots[bal_start] -= c->header.nritems;
+			path->nodes[bal_start] = b;
+			path->slots[bal_start + 1] += 1;
+		}
+		bal_start--;
+		if (!bal[bal_start])
+			break;
+	}
+	return __insert_ptr(root, path, key, blocknr, path->slots[level] + 1,
+			    level);
+}
+
+int leaf_space_used(struct leaf *l, int start, int nr)
+{
+	int data_len;
+	int end = start + nr - 1;
+
+	if (!nr)
+		return 0;
+	data_len = l->items[start].offset + l->items[start].size;
+	data_len = data_len - l->items[end].offset;
+	data_len += sizeof(struct item) * nr;
+	return data_len;
+}
+
+int push_leaf_left(struct ctree_root *root, struct ctree_path *path,
+		   int data_size)
+{
+	struct leaf *right = (struct leaf *)path->nodes[0];
+	struct leaf *left;
+	int slot;
+	int i;
+	int free_space;
+	int push_space = 0;
+	int push_items = 0;
+	struct item *item;
+	int old_left_nritems;
+
+	slot = path->slots[1];
+	if (slot == 0) {
+		return 1;
+	}
+	if (!path->nodes[1]) {
+		return 1;
+	}
+	left = read_block(path->nodes[1]->blockptrs[slot - 1]);
+	free_space = leaf_free_space(left);
+	if (free_space < data_size + sizeof(struct item)) {
+		return 1;
+	}
+	for (i = 0; i < right->header.nritems; i++) {
+		item = right->items + i;
+		if (path->slots[0] == i)
+			push_space += data_size + sizeof(*item);
+		if (item->size + sizeof(*item) + push_space > free_space)
+			break;
+		push_items++;
+		push_space += item->size + sizeof(*item);
+	}
+	if (push_items == 0) {
+		return 1;
+	}
+	/* push data from right to left */
+	memcpy(left->items + left->header.nritems,
+		right->items, push_items * sizeof(struct item));
+	push_space = LEAF_DATA_SIZE - right->items[push_items -1].offset;
+	memcpy(left->data + leaf_data_end(left) - push_space,
+		right->data + right->items[push_items - 1].offset,
+		push_space);
+	old_left_nritems = left->header.nritems;
+	for(i = old_left_nritems; i < old_left_nritems + push_items; i++) {
+		left->items[i].offset -= LEAF_DATA_SIZE -
+			left->items[old_left_nritems -1].offset;
+	}
+	left->header.nritems += push_items;
+
+	/* fixup right node */
+	push_space = right->items[push_items-1].offset - leaf_data_end(right);
+	memmove(right->data + LEAF_DATA_SIZE - push_space, right->data +
+		leaf_data_end(right), push_space);
+	memmove(right->items, right->items + push_items,
+		(right->header.nritems - push_items) * sizeof(struct item));
+	right->header.nritems -= push_items;
+	push_space = LEAF_DATA_SIZE;
+	for (i = 0; i < right->header.nritems; i++) {
+		right->items[i].offset = push_space - right->items[i].size;
+		push_space = right->items[i].offset;
+	}
+	fixup_low_keys(path, &right->items[0].key, 1);
+
+	/* then fixup the leaf pointer in the path */
+	if (path->slots[0] < push_items) {
+		path->slots[0] += old_left_nritems;
+		path->nodes[0] = (struct node*)left;
+		path->slots[1] -= 1;
+	} else {
+		path->slots[0] -= push_items;
+	}
+	return 0;
+}
+
+int split_leaf(struct ctree_root *root, struct ctree_path *path, int data_size)
+{
+	struct leaf *l = (struct leaf *)path->nodes[0];
+	int nritems = l->header.nritems;
+	int mid = (nritems + 1)/ 2;
+	int slot = path->slots[0];
+	struct leaf *right;
+	int space_needed = data_size + sizeof(struct item);
+	int data_copy_size;
+	int rt_data_off;
+	int i;
+	int ret;
+
+	if (push_leaf_left(root, path, data_size) == 0) {
+		return 0;
+	}
+	right = malloc(sizeof(struct leaf));
+	memset(right, 0, sizeof(*right));
+	if (mid <= slot) {
+		if (leaf_space_used(l, mid, nritems - mid) + space_needed >
+			LEAF_DATA_SIZE)
+			BUG();
+	} else {
+		if (leaf_space_used(l, 0, mid + 1) + space_needed >
+			LEAF_DATA_SIZE)
+			BUG();
+	}
+	right->header.nritems = nritems - mid;
+	data_copy_size = l->items[mid].offset + l->items[mid].size -
+			 leaf_data_end(l);
+	memcpy(right->items, l->items + mid,
+	       (nritems - mid) * sizeof(struct item));
+	memcpy(right->data + LEAF_DATA_SIZE - data_copy_size,
+	       l->data + leaf_data_end(l), data_copy_size);
+	rt_data_off = LEAF_DATA_SIZE -
+		     (l->items[mid].offset + l->items[mid].size);
+	for (i = 0; i < right->header.nritems; i++) {
+		right->items[i].offset += rt_data_off;
+	}
+	l->header.nritems = mid;
+	ret = insert_ptr(root, path, &right->items[0].key,
+			  (u64)right, 1);
+	if (mid <= slot) {
+		path->nodes[0] = (struct node *)right;
+		path->slots[0] -= mid;
+		path->slots[1] += 1;
+	}
+	return ret;
+}
+
+int insert_item(struct ctree_root *root, struct key *key,
+			  void *data, int data_size)
+{
+	int ret;
+	int slot;
+	struct leaf *leaf;
+	unsigned int nritems;
+	unsigned int data_end;
+	struct ctree_path path;
+
+	init_path(&path);
+	ret = search_slot(root, key, &path);
+	if (ret == 0)
+		return -EEXIST;
+
+	leaf = (struct leaf *)path.nodes[0];
+	if (leaf_free_space(leaf) <  sizeof(struct item) + data_size)
+		split_leaf(root, &path, data_size);
+	leaf = (struct leaf *)path.nodes[0];
+	nritems = leaf->header.nritems;
+	data_end = leaf_data_end(leaf);
+	if (leaf_free_space(leaf) <  sizeof(struct item) + data_size)
+		BUG();
+
+	slot = path.slots[0];
+	if (slot == 0)
+		fixup_low_keys(&path, key, 1);
+	if (slot != nritems) {
+		int i;
+		unsigned int old_data = leaf->items[slot].offset +
+					leaf->items[slot].size;
+
+		/*
+		 * item0..itemN ... dataN.offset..dataN.size .. data0.size
+		 */
+		/* first correct the data pointers */
+		for (i = slot; i < nritems; i++)
+			leaf->items[i].offset -= data_size;
+
+		/* shift the items */
+		memmove(leaf->items + slot + 1, leaf->items + slot,
+		        (nritems - slot) * sizeof(struct item));
+
+		/* shift the data */
+		memmove(leaf->data + data_end - data_size, leaf->data +
+		        data_end, old_data - data_end);
+		data_end = old_data;
+	}
+	memcpy(&leaf->items[slot].key, key, sizeof(struct key));
+	leaf->items[slot].offset = data_end - data_size;
+	leaf->items[slot].size = data_size;
+	memcpy(leaf->data + data_end - data_size, data, data_size);
+	leaf->header.nritems += 1;
+	if (leaf_free_space(leaf) < 0)
+		BUG();
+	return 0;
+}
+
+int del_ptr(struct ctree_root *root, struct ctree_path *path, int level)
+{
+	int slot;
+	struct node *node;
+	int nritems;
+
+	while(1) {
+		node = path->nodes[level];
+		if (!node)
+			break;
+		slot = path->slots[level];
+		nritems = node->header.nritems;
+
+		if (slot != nritems -1) {
+			memmove(node->keys + slot, node->keys + slot + 1,
+				sizeof(struct key) * (nritems - slot - 1));
+			memmove(node->blockptrs + slot,
+				node->blockptrs + slot + 1,
+				sizeof(u64) * (nritems - slot - 1));
+		}
+		node->header.nritems--;
+		if (node->header.nritems != 0) {
+			int tslot;
+			if (slot == 0)
+				fixup_low_keys(path, node->keys, level + 1);
+			tslot = path->slots[level+1];
+			push_node_left(root, path, level);
+			if (node->header.nritems) {
+				push_node_right(root, path, level);
+			}
+			path->slots[level+1] = tslot;
+			if (node->header.nritems)
+				break;
+		}
+		if (node == root->node) {
+			printf("root is now null!\n");
+			root->node = NULL;
+			break;
+		}
+		level++;
+		if (!path->nodes[level])
+			BUG();
+		free(node);
+	}
+	return 0;
+}
+
+int del_item(struct ctree_root *root, struct key *key)
+{
+	int ret;
+	int slot;
+	struct leaf *leaf;
+	struct ctree_path path;
+	int doff;
+	int dsize;
+
+	init_path(&path);
+	ret = search_slot(root, key, &path);
+	if (ret != 0)
+		return -1;
+
+	leaf = (struct leaf *)path.nodes[0];
+	slot = path.slots[0];
+	doff = leaf->items[slot].offset;
+	dsize = leaf->items[slot].size;
+
+	if (slot != leaf->header.nritems - 1) {
+		int i;
+		int data_end = leaf_data_end(leaf);
+		memmove(leaf->data + data_end + dsize,
+			leaf->data + data_end,
+			doff - data_end);
+		for (i = slot + 1; i < leaf->header.nritems; i++)
+			leaf->items[i].offset += dsize;
+		memmove(leaf->items + slot, leaf->items + slot + 1,
+			sizeof(struct item) *
+			(leaf->header.nritems - slot - 1));
+	}
+	leaf->header.nritems -= 1;
+	if (leaf->header.nritems == 0) {
+		free(leaf);
+		del_ptr(root, &path, 1);
+	} else {
+		if (slot == 0)
+			fixup_low_keys(&path, &leaf->items[0].key, 1);
+		if (leaf_space_used(leaf, 0, leaf->header.nritems) <
+		    LEAF_DATA_SIZE / 4) {
+			/* push_leaf_left fixes the path.
+			 * make sure the path still points to our leaf
+			 * for possible call to del_ptr below
+			 */
+			slot = path.slots[1];
+			push_leaf_left(root, &path, 1);
+			path.slots[1] = slot;
+			if (leaf->header.nritems == 0) {
+				free(leaf);
+				del_ptr(root, &path, 1);
+			}
+		}
+	}
+	return 0;
+}
+
+void print_leaf(struct leaf *l)
+{
+	int i;
+	int nr = l->header.nritems;
+	struct item *item;
+	printf("leaf %p total ptrs %d free space %d\n", l, nr,
+	       leaf_free_space(l));
+	fflush(stdout);
+	for (i = 0 ; i < nr ; i++) {
+		item = l->items + i;
+		printf("\titem %d key (%lu %u %lu) itemoff %d itemsize %d\n",
+			i,
+			item->key.objectid, item->key.flags, item->key.offset,
+			item->offset, item->size);
+		fflush(stdout);
+		printf("\t\titem data %.*s\n", item->size, l->data+item->offset);
+		fflush(stdout);
+	}
+}
+void print_tree(struct node *c)
+{
+	int i;
+	int nr;
+
+	if (!c)
+		return;
+	nr = c->header.nritems;
+	if (is_leaf(c->header.flags)) {
+		print_leaf((struct leaf *)c);
+		return;
+	}
+	printf("node %p level %d total ptrs %d free spc %lu\n", c,
+	        node_level(c->header.flags), c->header.nritems,
+		NODEPTRS_PER_BLOCK - c->header.nritems);
+	fflush(stdout);
+	for (i = 0; i < nr; i++) {
+		printf("\tkey %d (%lu %u %lu) block %lx\n",
+		       i,
+		       c->keys[i].objectid, c->keys[i].flags, c->keys[i].offset,
+		       c->blockptrs[i]);
+		fflush(stdout);
+	}
+	for (i = 0; i < nr; i++) {
+		struct node *next = read_block(c->blockptrs[i]);
+		if (is_leaf(next->header.flags) &&
+		    node_level(c->header.flags) != 1)
+			BUG();
+		if (node_level(next->header.flags) !=
+			node_level(c->header.flags) - 1)
+			BUG();
+		print_tree(next);
+	}
+
+}
+
+/* for testing only */
+int next_key(int i, int max_key) {
+	return rand() % max_key;
+	// return i;
+}
+
+int main() {
+	struct leaf *first_node = malloc(sizeof(struct leaf));
+	struct ctree_root root;
+	struct key ins;
+	char *buf;
+	int i;
+	int num;
+	int ret;
+	int run_size = 10000000;
+	int max_key = 100000000;
+	int tree_size = 0;
+	struct ctree_path path;
+
+
+	srand(55);
+	root.node = (struct node *)first_node;
+	memset(first_node, 0, sizeof(*first_node));
+	for (i = 0; i < run_size; i++) {
+		buf = malloc(64);
+		num = next_key(i, max_key);
+		// num = i;
+		sprintf(buf, "string-%d", num);
+		// printf("insert %d\n", num);
+		ins.objectid = num;
+		ins.offset = 0;
+		ins.flags = 0;
+		ret = insert_item(&root, &ins, buf, strlen(buf));
+		if (!ret)
+			tree_size++;
+	}
+	srand(55);
+	for (i = 0; i < run_size; i++) {
+		num = next_key(i, max_key);
+		ins.objectid = num;
+		ins.offset = 0;
+		ins.flags = 0;
+		init_path(&path);
+		ret = search_slot(&root, &ins, &path);
+		if (ret) {
+			print_tree(root.node);
+			printf("unable to find %d\n", num);
+			exit(1);
+		}
+	}
+	printf("node %p level %d total ptrs %d free spc %lu\n", root.node,
+	        node_level(root.node->header.flags), root.node->header.nritems,
+		NODEPTRS_PER_BLOCK - root.node->header.nritems);
+	// print_tree(root.node);
+	printf("all searches good\n");
+	i = 0;
+	srand(55);
+	for (i = 0; i < run_size; i++) {
+		num = next_key(i, max_key);
+		ins.objectid = num;
+		del_item(&root, &ins);
+	}
+	print_tree(root.node);
+	return 0;
+}
