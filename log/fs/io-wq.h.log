commit 801dd57bd1d8c2c253f43635a3045bfa32a810b3
Author: Pavel Begunkov <asml.silence@gmail.com>
Date:   Mon Jun 15 10:33:14 2020 +0300

    io_uring: cancel by ->task not pid
    
    For an exiting process it tries to cancel all its inflight requests. Use
    req->task to match such instead of work.pid. We always have req->task
    set, and it will be valid because we're matching only current exiting
    task.
    
    Also, remove work.pid and everything related, it's useless now.
    
    Reported-by: Eric W. Biederman <ebiederm@xmission.com>
    Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/fs/io-wq.h b/fs/io-wq.h
index b72538fe5afd..071f1a997800 100644
--- a/fs/io-wq.h
+++ b/fs/io-wq.h
@@ -90,7 +90,6 @@ struct io_wq_work {
 	const struct cred *creds;
 	struct fs_struct *fs;
 	unsigned flags;
-	pid_t task_pid;
 };
 
 static inline struct io_wq_work *wq_next_work(struct io_wq_work *work)

commit 44e728b8aae0bb6d4229129083974f9dea43f50b
Author: Pavel Begunkov <asml.silence@gmail.com>
Date:   Mon Jun 15 10:24:04 2020 +0300

    io_uring: cancel all task's requests on exit
    
    If a process is going away, io_uring_flush() will cancel only 1
    request with a matching pid. Cancel all of them
    
    Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/fs/io-wq.h b/fs/io-wq.h
index 7d5bd431c5e3..b72538fe5afd 100644
--- a/fs/io-wq.h
+++ b/fs/io-wq.h
@@ -125,7 +125,6 @@ static inline bool io_wq_is_hashed(struct io_wq_work *work)
 
 void io_wq_cancel_all(struct io_wq *wq);
 enum io_wq_cancel io_wq_cancel_work(struct io_wq *wq, struct io_wq_work *cwork);
-enum io_wq_cancel io_wq_cancel_pid(struct io_wq *wq, pid_t pid);
 
 typedef bool (work_cancel_fn)(struct io_wq_work *, void *);
 

commit 4f26bda1522c35d2701fc219368c7101c17005c1
Author: Pavel Begunkov <asml.silence@gmail.com>
Date:   Mon Jun 15 10:24:03 2020 +0300

    io-wq: add an option to cancel all matched reqs
    
    This adds support for cancelling all io-wq works matching a predicate.
    It isn't used yet, so no change in observable behaviour.
    
    Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/fs/io-wq.h b/fs/io-wq.h
index 8e138fa88b9f..7d5bd431c5e3 100644
--- a/fs/io-wq.h
+++ b/fs/io-wq.h
@@ -130,7 +130,7 @@ enum io_wq_cancel io_wq_cancel_pid(struct io_wq *wq, pid_t pid);
 typedef bool (work_cancel_fn)(struct io_wq_work *, void *);
 
 enum io_wq_cancel io_wq_cancel_cb(struct io_wq *wq, work_cancel_fn *cancel,
-					void *data);
+					void *data, bool cancel_all);
 
 struct task_struct *io_wq_get_task(struct io_wq *wq);
 

commit 7cdaf587de7c6f494b8433fded19f7728e70e1ef
Author: Xiaoguang Wang <xiaoguang.wang@linux.alibaba.com>
Date:   Wed Jun 10 19:41:19 2020 +0800

    io_uring: avoid whole io_wq_work copy for requests completed inline
    
    If requests can be submitted and completed inline, we don't need to
    initialize whole io_wq_work in io_init_req(), which is an expensive
    operation, add a new 'REQ_F_WORK_INITIALIZED' to determine whether
    io_wq_work is initialized and add a helper io_req_init_async(), users
    must call io_req_init_async() for the first time touching any members
    of io_wq_work.
    
    I use /dev/nullb0 to evaluate performance improvement in my physical
    machine:
      modprobe null_blk nr_devices=1 completion_nsec=0
      sudo taskset -c 60 fio  -name=fiotest -filename=/dev/nullb0 -iodepth=128
      -thread -rw=read -ioengine=io_uring -direct=1 -bs=4k -size=100G -numjobs=1
      -time_based -runtime=120
    
    before this patch:
    Run status group 0 (all jobs):
       READ: bw=724MiB/s (759MB/s), 724MiB/s-724MiB/s (759MB/s-759MB/s),
       io=84.8GiB (91.1GB), run=120001-120001msec
    
    With this patch:
    Run status group 0 (all jobs):
       READ: bw=761MiB/s (798MB/s), 761MiB/s-761MiB/s (798MB/s-798MB/s),
       io=89.2GiB (95.8GB), run=120001-120001msec
    
    About 5% improvement.
    
    Signed-off-by: Xiaoguang Wang <xiaoguang.wang@linux.alibaba.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/fs/io-wq.h b/fs/io-wq.h
index 2db24d31fbc5..8e138fa88b9f 100644
--- a/fs/io-wq.h
+++ b/fs/io-wq.h
@@ -93,11 +93,6 @@ struct io_wq_work {
 	pid_t task_pid;
 };
 
-#define INIT_IO_WORK(work)					\
-	do {							\
-		*(work) = (struct io_wq_work){};		\
-	} while (0)						\
-
 static inline struct io_wq_work *wq_next_work(struct io_wq_work *work)
 {
 	if (!work->list.next)

commit f5fa38c59cb0b40633dee5cdf7465801be3e4928
Author: Pavel Begunkov <asml.silence@gmail.com>
Date:   Mon Jun 8 21:08:20 2020 +0300

    io_wq: add per-wq work handler instead of per work
    
    io_uring is the only user of io-wq, and now it uses only io-wq callback
    for all its requests, namely io_wq_submit_work(). Instead of storing
    work->runner callback in each instance of io_wq_work, keep it in io-wq
    itself.
    
    pros:
    - reduces io_wq_work size
    - more robust -- ->func won't be invalidated with mem{cpy,set}(req)
    - helps other work
    
    Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/fs/io-wq.h b/fs/io-wq.h
index 5ba12de7572f..2db24d31fbc5 100644
--- a/fs/io-wq.h
+++ b/fs/io-wq.h
@@ -85,7 +85,6 @@ static inline void wq_list_del(struct io_wq_work_list *list,
 
 struct io_wq_work {
 	struct io_wq_work_node list;
-	void (*func)(struct io_wq_work **);
 	struct files_struct *files;
 	struct mm_struct *mm;
 	const struct cred *creds;
@@ -94,9 +93,9 @@ struct io_wq_work {
 	pid_t task_pid;
 };
 
-#define INIT_IO_WORK(work, _func)				\
+#define INIT_IO_WORK(work)					\
 	do {							\
-		*(work) = (struct io_wq_work){ .func = _func };	\
+		*(work) = (struct io_wq_work){};		\
 	} while (0)						\
 
 static inline struct io_wq_work *wq_next_work(struct io_wq_work *work)
@@ -108,10 +107,12 @@ static inline struct io_wq_work *wq_next_work(struct io_wq_work *work)
 }
 
 typedef void (free_work_fn)(struct io_wq_work *);
+typedef void (io_wq_work_fn)(struct io_wq_work **);
 
 struct io_wq_data {
 	struct user_struct *user;
 
+	io_wq_work_fn *do_work;
 	free_work_fn *free_work;
 };
 

commit aa96bf8a9ee33457b7e3ea43e97dfa1e3a15ab20
Author: Jens Axboe <axboe@kernel.dk>
Date:   Fri Apr 3 11:26:26 2020 -0600

    io_uring: use io-wq manager as backup task if task is exiting
    
    If the original task is (or has) exited, then the task work will not get
    queued properly. Allow for using the io-wq manager task to queue this
    work for execution, and ensure that the io-wq manager notices and runs
    this work if woken up (or exiting).
    
    Reported-by: Dan Melnic <dmm@fb.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/fs/io-wq.h b/fs/io-wq.h
index 3ee7356d6be5..5ba12de7572f 100644
--- a/fs/io-wq.h
+++ b/fs/io-wq.h
@@ -136,6 +136,8 @@ typedef bool (work_cancel_fn)(struct io_wq_work *, void *);
 enum io_wq_cancel io_wq_cancel_cb(struct io_wq *wq, work_cancel_fn *cancel,
 					void *data);
 
+struct task_struct *io_wq_get_task(struct io_wq *wq);
+
 #if defined(CONFIG_IO_WQ)
 extern void io_wq_worker_sleeping(struct task_struct *);
 extern void io_wq_worker_running(struct task_struct *);

commit 86f3cd1b589a10dbdca98c52cc0cd0f56523c9b3
Author: Pavel Begunkov <asml.silence@gmail.com>
Date:   Mon Mar 23 22:57:22 2020 +0300

    io-wq: handle hashed writes in chains
    
    We always punt async buffered writes to an io-wq helper, as the core
    kernel does not have IOCB_NOWAIT support for that. Most buffered async
    writes complete very quickly, as it's just a copy operation. This means
    that doing multiple locking roundtrips on the shared wqe lock for each
    buffered write is wasteful. Additionally, buffered writes are hashed
    work items, which means that any buffered write to a given file is
    serialized.
    
    Keep identicaly hashed work items contiguously in @wqe->work_list, and
    track a tail for each hash bucket. On dequeue of a hashed item, splice
    all of the same hash in one go using the tracked tail. Until the batch
    is done, the caller doesn't have to synchronize with the wqe or worker
    locks again.
    
    Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/fs/io-wq.h b/fs/io-wq.h
index d2a5684bf673..3ee7356d6be5 100644
--- a/fs/io-wq.h
+++ b/fs/io-wq.h
@@ -28,6 +28,18 @@ struct io_wq_work_list {
 	struct io_wq_work_node *last;
 };
 
+static inline void wq_list_add_after(struct io_wq_work_node *node,
+				     struct io_wq_work_node *pos,
+				     struct io_wq_work_list *list)
+{
+	struct io_wq_work_node *next = pos->next;
+
+	pos->next = node;
+	node->next = next;
+	if (!next)
+		list->last = node;
+}
+
 static inline void wq_list_add_tail(struct io_wq_work_node *node,
 				    struct io_wq_work_list *list)
 {
@@ -40,17 +52,26 @@ static inline void wq_list_add_tail(struct io_wq_work_node *node,
 	}
 }
 
-static inline void wq_node_del(struct io_wq_work_list *list,
-			       struct io_wq_work_node *node,
+static inline void wq_list_cut(struct io_wq_work_list *list,
+			       struct io_wq_work_node *last,
 			       struct io_wq_work_node *prev)
 {
-	if (node == list->first)
-		WRITE_ONCE(list->first, node->next);
-	if (node == list->last)
+	/* first in the list, if prev==NULL */
+	if (!prev)
+		WRITE_ONCE(list->first, last->next);
+	else
+		prev->next = last->next;
+
+	if (last == list->last)
 		list->last = prev;
-	if (prev)
-		prev->next = node->next;
-	node->next = NULL;
+	last->next = NULL;
+}
+
+static inline void wq_list_del(struct io_wq_work_list *list,
+			       struct io_wq_work_node *node,
+			       struct io_wq_work_node *prev)
+{
+	wq_list_cut(list, node, prev);
 }
 
 #define wq_list_for_each(pos, prv, head)			\
@@ -78,6 +99,14 @@ struct io_wq_work {
 		*(work) = (struct io_wq_work){ .func = _func };	\
 	} while (0)						\
 
+static inline struct io_wq_work *wq_next_work(struct io_wq_work *work)
+{
+	if (!work->list.next)
+		return NULL;
+
+	return container_of(work->list.next, struct io_wq_work, list);
+}
+
 typedef void (free_work_fn)(struct io_wq_work *);
 
 struct io_wq_data {

commit 18a542ff19ad149fac9e5a36a4012e3cac7b3b3b
Author: Pavel Begunkov <asml.silence@gmail.com>
Date:   Mon Mar 23 00:23:29 2020 +0300

    io_uring: Fix ->data corruption on re-enqueue
    
    work->data and work->list are shared in union. io_wq_assign_next() sets
    ->data if a req having a linked_timeout, but then io-wq may want to use
    work->list, e.g. to do re-enqueue of a request, so corrupting ->data.
    
    ->data is not necessary, just remove it and extract linked_timeout
    through @link_list.
    
    Fixes: 60cf46ae6054 ("io-wq: hash dependent work")
    Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/fs/io-wq.h b/fs/io-wq.h
index 298b21f4a4d2..d2a5684bf673 100644
--- a/fs/io-wq.h
+++ b/fs/io-wq.h
@@ -63,10 +63,7 @@ static inline void wq_node_del(struct io_wq_work_list *list,
 } while (0)
 
 struct io_wq_work {
-	union {
-		struct io_wq_work_node list;
-		void *data;
-	};
+	struct io_wq_work_node list;
 	void (*func)(struct io_wq_work **);
 	struct files_struct *files;
 	struct mm_struct *mm;

commit 8766dd516c535abf04491dca674d0ef6c95d814f
Author: Pavel Begunkov <asml.silence@gmail.com>
Date:   Sat Mar 14 00:31:04 2020 +0300

    io-wq: split hashing and enqueueing
    
    It's a preparation patch removing io_wq_enqueue_hashed(), which
    now should be done by io_wq_hash_work() + io_wq_enqueue().
    
    Also, set hash value for dependant works, and do it as late as possible,
    because req->file can be unavailable before. This hash will be ignored
    by io-wq.
    
    Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/fs/io-wq.h b/fs/io-wq.h
index 2117b9a4f161..298b21f4a4d2 100644
--- a/fs/io-wq.h
+++ b/fs/io-wq.h
@@ -94,7 +94,12 @@ bool io_wq_get(struct io_wq *wq, struct io_wq_data *data);
 void io_wq_destroy(struct io_wq *wq);
 
 void io_wq_enqueue(struct io_wq *wq, struct io_wq_work *work);
-void io_wq_enqueue_hashed(struct io_wq *wq, struct io_wq_work *work, void *val);
+void io_wq_hash_work(struct io_wq_work *work, void *val);
+
+static inline bool io_wq_is_hashed(struct io_wq_work *work)
+{
+	return work->flags & IO_WQ_WORK_HASHED;
+}
 
 void io_wq_cancel_all(struct io_wq *wq);
 enum io_wq_cancel io_wq_cancel_work(struct io_wq *wq, struct io_wq_work *cwork);

commit e9fd939654f17651ff65e7e55aa6934d29eb4335
Author: Pavel Begunkov <asml.silence@gmail.com>
Date:   Wed Mar 4 16:14:12 2020 +0300

    io_uring/io-wq: forward submission ref to async
    
    First it changes io-wq interfaces. It replaces {get,put}_work() with
    free_work(), which guaranteed to be called exactly once. It also enforces
    free_work() callback to be non-NULL.
    
    io_uring follows the changes and instead of putting a submission reference
    in io_put_req_async_completion(), it will be done in io_free_work(). As
    removes io_get_work() with corresponding refcount_inc(), the ref balance
    is maintained.
    
    Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/fs/io-wq.h b/fs/io-wq.h
index a0978d6958f0..2117b9a4f161 100644
--- a/fs/io-wq.h
+++ b/fs/io-wq.h
@@ -81,14 +81,12 @@ struct io_wq_work {
 		*(work) = (struct io_wq_work){ .func = _func };	\
 	} while (0)						\
 
-typedef void (get_work_fn)(struct io_wq_work *);
-typedef void (put_work_fn)(struct io_wq_work *);
+typedef void (free_work_fn)(struct io_wq_work *);
 
 struct io_wq_data {
 	struct user_struct *user;
 
-	get_work_fn *get_work;
-	put_work_fn *put_work;
+	free_work_fn *free_work;
 };
 
 struct io_wq *io_wq_create(unsigned bounded, struct io_wq_data *data);

commit 5eae8619907a1389dbd1b4a1049caf52782c0916
Author: Pavel Begunkov <asml.silence@gmail.com>
Date:   Fri Feb 28 10:36:38 2020 +0300

    io_uring: remove IO_WQ_WORK_CB
    
    IO_WQ_WORK_CB is used only for linked timeouts, which will be armed
    before the work setup (i.e. mm, override creds, etc). The setup
    shouldn't take long, so it's ok to arm it a bit later and get rid
    of IO_WQ_WORK_CB.
    
    Make io-wq call work->func() only once, callbacks will handle the rest.
    i.e. the linked timeout handler will do the actual issue. And as a
    bonus, it removes an extra indirect call.
    
    Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/fs/io-wq.h b/fs/io-wq.h
index d500d88ab84e..a0978d6958f0 100644
--- a/fs/io-wq.h
+++ b/fs/io-wq.h
@@ -7,7 +7,6 @@ enum {
 	IO_WQ_WORK_CANCEL	= 1,
 	IO_WQ_WORK_HASHED	= 4,
 	IO_WQ_WORK_UNBOUND	= 32,
-	IO_WQ_WORK_CB		= 128,
 	IO_WQ_WORK_NO_CANCEL	= 256,
 	IO_WQ_WORK_CONCURRENT	= 512,
 

commit e85530ddda4f08d4f9ed6506d4a1f42e086e3b21
Author: Pavel Begunkov <asml.silence@gmail.com>
Date:   Fri Feb 28 10:36:37 2020 +0300

    io-wq: remove unused IO_WQ_WORK_HAS_MM
    
    IO_WQ_WORK_HAS_MM is set but never used, remove it.
    
    Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/fs/io-wq.h b/fs/io-wq.h
index e5e15f2c93ec..d500d88ab84e 100644
--- a/fs/io-wq.h
+++ b/fs/io-wq.h
@@ -5,7 +5,6 @@ struct io_wq;
 
 enum {
 	IO_WQ_WORK_CANCEL	= 1,
-	IO_WQ_WORK_HAS_MM	= 2,
 	IO_WQ_WORK_HASHED	= 4,
 	IO_WQ_WORK_UNBOUND	= 32,
 	IO_WQ_WORK_CB		= 128,

commit 80ad894382bf1d73eb688c29714fa10c0afcf2e7
Author: Pavel Begunkov <asml.silence@gmail.com>
Date:   Mon Mar 2 23:46:10 2020 +0300

    io-wq: remove io_wq_flush and IO_WQ_WORK_INTERNAL
    
    io_wq_flush() is buggy, during cancelation of a flush, the associated
    work may be passed to the caller's (i.e. io_uring) @match callback. That
    callback is expecting it to be embedded in struct io_kiocb. Cancelation
    of internal work probably doesn't make a lot of sense to begin with.
    
    As the flush helper is no longer used, just delete it and the associated
    work flag.
    
    Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/fs/io-wq.h b/fs/io-wq.h
index 33baba4370c5..e5e15f2c93ec 100644
--- a/fs/io-wq.h
+++ b/fs/io-wq.h
@@ -8,7 +8,6 @@ enum {
 	IO_WQ_WORK_HAS_MM	= 2,
 	IO_WQ_WORK_HASHED	= 4,
 	IO_WQ_WORK_UNBOUND	= 32,
-	IO_WQ_WORK_INTERNAL	= 64,
 	IO_WQ_WORK_CB		= 128,
 	IO_WQ_WORK_NO_CANCEL	= 256,
 	IO_WQ_WORK_CONCURRENT	= 512,
@@ -100,7 +99,6 @@ void io_wq_destroy(struct io_wq *wq);
 
 void io_wq_enqueue(struct io_wq *wq, struct io_wq_work *work);
 void io_wq_enqueue_hashed(struct io_wq *wq, struct io_wq_work *work, void *val);
-void io_wq_flush(struct io_wq *wq);
 
 void io_wq_cancel_all(struct io_wq *wq);
 enum io_wq_cancel io_wq_cancel_work(struct io_wq *wq, struct io_wq_work *cwork);

commit 2d141dd2caa78fbaf87b57c27769bdc14975ab3d
Author: Jens Axboe <axboe@kernel.dk>
Date:   Tue Feb 25 11:52:56 2020 -0700

    io-wq: ensure work->task_pid is cleared on init
    
    We use ->task_pid for exit cancellation, but we need to ensure it's
    cleared to zero for io_req_work_grab_env() to do the right thing. Take
    a suggestion from Bart and clear the whole thing, just setting the
    function passed in. This makes it more future proof as well.
    
    Fixes: 36282881a795 ("io-wq: add io_wq_cancel_pid() to cancel based on a specific pid")
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/fs/io-wq.h b/fs/io-wq.h
index ccc7d84af57d..33baba4370c5 100644
--- a/fs/io-wq.h
+++ b/fs/io-wq.h
@@ -79,16 +79,10 @@ struct io_wq_work {
 	pid_t task_pid;
 };
 
-#define INIT_IO_WORK(work, _func)			\
-	do {						\
-		(work)->list.next = NULL;		\
-		(work)->func = _func;			\
-		(work)->files = NULL;			\
-		(work)->mm = NULL;			\
-		(work)->creds = NULL;			\
-		(work)->fs = NULL;			\
-		(work)->flags = 0;			\
-	} while (0)					\
+#define INIT_IO_WORK(work, _func)				\
+	do {							\
+		*(work) = (struct io_wq_work){ .func = _func };	\
+	} while (0)						\
 
 typedef void (get_work_fn)(struct io_wq_work *);
 typedef void (put_work_fn)(struct io_wq_work *);

commit 36282881a795cbf717aca79392ae9cdf0fef59c9
Author: Jens Axboe <axboe@kernel.dk>
Date:   Sat Feb 8 19:16:39 2020 -0700

    io-wq: add io_wq_cancel_pid() to cancel based on a specific pid
    
    Add a helper that allows the caller to cancel work based on what mm
    it belongs to. This allows io_uring to cancel work from a given
    task or thread when it exits.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/fs/io-wq.h b/fs/io-wq.h
index f152ba677d8f..ccc7d84af57d 100644
--- a/fs/io-wq.h
+++ b/fs/io-wq.h
@@ -76,6 +76,7 @@ struct io_wq_work {
 	const struct cred *creds;
 	struct fs_struct *fs;
 	unsigned flags;
+	pid_t task_pid;
 };
 
 #define INIT_IO_WORK(work, _func)			\
@@ -109,6 +110,7 @@ void io_wq_flush(struct io_wq *wq);
 
 void io_wq_cancel_all(struct io_wq *wq);
 enum io_wq_cancel io_wq_cancel_work(struct io_wq *wq, struct io_wq_work *cwork);
+enum io_wq_cancel io_wq_cancel_pid(struct io_wq *wq, pid_t pid);
 
 typedef bool (work_cancel_fn)(struct io_wq_work *, void *);
 

commit 9392a27d88b9707145d713654eb26f0c29789e50
Author: Jens Axboe <axboe@kernel.dk>
Date:   Thu Feb 6 21:42:51 2020 -0700

    io-wq: add support for inheriting ->fs
    
    Some work items need this for relative path lookup, make it available
    like the other inherited credentials/mm/etc.
    
    Cc: stable@vger.kernel.org # 5.3+
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/fs/io-wq.h b/fs/io-wq.h
index 50b3378febf2..f152ba677d8f 100644
--- a/fs/io-wq.h
+++ b/fs/io-wq.h
@@ -74,6 +74,7 @@ struct io_wq_work {
 	struct files_struct *files;
 	struct mm_struct *mm;
 	const struct cred *creds;
+	struct fs_struct *fs;
 	unsigned flags;
 };
 
@@ -81,10 +82,11 @@ struct io_wq_work {
 	do {						\
 		(work)->list.next = NULL;		\
 		(work)->func = _func;			\
-		(work)->flags = 0;			\
 		(work)->files = NULL;			\
 		(work)->mm = NULL;			\
 		(work)->creds = NULL;			\
+		(work)->fs = NULL;			\
+		(work)->flags = 0;			\
 	} while (0)					\
 
 typedef void (get_work_fn)(struct io_wq_work *);

commit f86cd20c9454847a524ddbdcdec32c0380ed7c9b
Author: Jens Axboe <axboe@kernel.dk>
Date:   Wed Jan 29 13:46:44 2020 -0700

    io_uring: fix linked command file table usage
    
    We're not consistent in how the file table is grabbed and assigned if we
    have a command linked that requires the use of it.
    
    Add ->file_table to the io_op_defs[] array, and use that to determine
    when to grab the table instead of having the handlers set it if they
    need to defer. This also means we can kill the IO_WQ_WORK_NEEDS_FILES
    flag. We always initialize work->files, so io-wq can just check for
    that.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/fs/io-wq.h b/fs/io-wq.h
index c42602c58c56..50b3378febf2 100644
--- a/fs/io-wq.h
+++ b/fs/io-wq.h
@@ -7,7 +7,6 @@ enum {
 	IO_WQ_WORK_CANCEL	= 1,
 	IO_WQ_WORK_HAS_MM	= 2,
 	IO_WQ_WORK_HASHED	= 4,
-	IO_WQ_WORK_NEEDS_FILES	= 16,
 	IO_WQ_WORK_UNBOUND	= 32,
 	IO_WQ_WORK_INTERNAL	= 64,
 	IO_WQ_WORK_CB		= 128,

commit eba6f5a330cf042bb0001f0b5e8cbf21be1b25d6
Author: Pavel Begunkov <asml.silence@gmail.com>
Date:   Tue Jan 28 03:15:47 2020 +0300

    io-wq: allow grabbing existing io-wq
    
    Export a helper to attach to an existing io-wq, rather than setting up
    a new one. This is doable now that we have reference counted io_wq's.
    
    Reported-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/fs/io-wq.h b/fs/io-wq.h
index 167316ad447e..c42602c58c56 100644
--- a/fs/io-wq.h
+++ b/fs/io-wq.h
@@ -99,6 +99,7 @@ struct io_wq_data {
 };
 
 struct io_wq *io_wq_create(unsigned bounded, struct io_wq_data *data);
+bool io_wq_get(struct io_wq *wq, struct io_wq_data *data);
 void io_wq_destroy(struct io_wq *wq);
 
 void io_wq_enqueue(struct io_wq *wq, struct io_wq_work *work);

commit cccf0ee834559ae0b327b40290e14f6a2a017177
Author: Jens Axboe <axboe@kernel.dk>
Date:   Mon Jan 27 16:34:48 2020 -0700

    io_uring/io-wq: don't use static creds/mm assignments
    
    We currently setup the io_wq with a static set of mm and creds. Even for
    a single-use io-wq per io_uring, this is suboptimal as we have may have
    multiple enters of the ring. For sharing the io-wq backend, it doesn't
    work at all.
    
    Switch to passing in the creds and mm when the work item is setup. This
    means that async work is no longer deferred to the io_uring mm and creds,
    it is done with the current mm and creds.
    
    Flag this behavior with IORING_FEAT_CUR_PERSONALITY, so applications know
    they can rely on the current personality (mm and creds) being the same
    for direct issue and async issue.
    
    Reviewed-by: Stefan Metzmacher <metze@samba.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/fs/io-wq.h b/fs/io-wq.h
index 1cd039af8813..167316ad447e 100644
--- a/fs/io-wq.h
+++ b/fs/io-wq.h
@@ -7,7 +7,6 @@ enum {
 	IO_WQ_WORK_CANCEL	= 1,
 	IO_WQ_WORK_HAS_MM	= 2,
 	IO_WQ_WORK_HASHED	= 4,
-	IO_WQ_WORK_NEEDS_USER	= 8,
 	IO_WQ_WORK_NEEDS_FILES	= 16,
 	IO_WQ_WORK_UNBOUND	= 32,
 	IO_WQ_WORK_INTERNAL	= 64,
@@ -74,6 +73,8 @@ struct io_wq_work {
 	};
 	void (*func)(struct io_wq_work **);
 	struct files_struct *files;
+	struct mm_struct *mm;
+	const struct cred *creds;
 	unsigned flags;
 };
 
@@ -83,15 +84,15 @@ struct io_wq_work {
 		(work)->func = _func;			\
 		(work)->flags = 0;			\
 		(work)->files = NULL;			\
+		(work)->mm = NULL;			\
+		(work)->creds = NULL;			\
 	} while (0)					\
 
 typedef void (get_work_fn)(struct io_wq_work *);
 typedef void (put_work_fn)(struct io_wq_work *);
 
 struct io_wq_data {
-	struct mm_struct *mm;
 	struct user_struct *user;
-	const struct cred *creds;
 
 	get_work_fn *get_work;
 	put_work_fn *put_work;

commit 895e2ca0f693c672902191747b548bdc56f0c7de
Author: Jens Axboe <axboe@kernel.dk>
Date:   Tue Dec 17 08:46:33 2019 -0700

    io-wq: support concurrent non-blocking work
    
    io-wq assumes that work will complete fast (and not block), so it
    doesn't create a new worker when work is enqueued, if we already have
    at least one worker running. This is done on the assumption that if work
    is running, then it will complete fast.
    
    Add an option to force io-wq to fork a new worker for work queued. This
    is signaled by setting IO_WQ_WORK_CONCURRENT on the work item. For that
    case, io-wq will create a new worker, even though workers are already
    running.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/fs/io-wq.h b/fs/io-wq.h
index 04d60ad38dfc..1cd039af8813 100644
--- a/fs/io-wq.h
+++ b/fs/io-wq.h
@@ -13,6 +13,7 @@ enum {
 	IO_WQ_WORK_INTERNAL	= 64,
 	IO_WQ_WORK_CB		= 128,
 	IO_WQ_WORK_NO_CANCEL	= 256,
+	IO_WQ_WORK_CONCURRENT	= 512,
 
 	IO_WQ_HASH_SHIFT	= 24,	/* upper 8 bits are used for hash key */
 };

commit 0c9d5ccd26a004f59333c06fbbb98f9cb1eed93d
Author: Jens Axboe <axboe@kernel.dk>
Date:   Wed Dec 11 19:29:43 2019 -0700

    io-wq: add support for uncancellable work
    
    Not all work can be cancelled, some of it we may need to guarantee
    that it runs to completion. Allow the caller to set IO_WQ_WORK_NO_CANCEL
    on work that must not be cancelled. Note that the caller work function
    must also check for IO_WQ_WORK_NO_CANCEL on work that is marked
    IO_WQ_WORK_CANCEL.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/fs/io-wq.h b/fs/io-wq.h
index 3f5e356de980..04d60ad38dfc 100644
--- a/fs/io-wq.h
+++ b/fs/io-wq.h
@@ -12,6 +12,7 @@ enum {
 	IO_WQ_WORK_UNBOUND	= 32,
 	IO_WQ_WORK_INTERNAL	= 64,
 	IO_WQ_WORK_CB		= 128,
+	IO_WQ_WORK_NO_CANCEL	= 256,
 
 	IO_WQ_HASH_SHIFT	= 24,	/* upper 8 bits are used for hash key */
 };

commit 525b305d61ede489ce2118b000a5dabd6d869dac
Author: Jens Axboe <axboe@kernel.dk>
Date:   Tue Dec 17 14:13:37 2019 -0700

    io-wq: re-add io_wq_current_is_worker()
    
    This reverts commit 8cdda87a4414, we now have several use csaes for this
    helper. Reinstate it.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/fs/io-wq.h b/fs/io-wq.h
index fb993b2bd0ef..3f5e356de980 100644
--- a/fs/io-wq.h
+++ b/fs/io-wq.h
@@ -120,6 +120,10 @@ static inline void io_wq_worker_sleeping(struct task_struct *tsk)
 static inline void io_wq_worker_running(struct task_struct *tsk)
 {
 }
-#endif /* CONFIG_IO_WQ */
+#endif
 
-#endif /* INTERNAL_IO_WQ_H */
+static inline bool io_wq_current_is_worker(void)
+{
+	return in_task() && (current->flags & PF_IO_WORKER);
+}
+#endif

commit e995d5123ed433e37a8d63ac528737c912592e3d
Author: Jens Axboe <axboe@kernel.dk>
Date:   Sat Dec 7 21:06:46 2019 -0700

    io-wq: briefly spin for new work after finishing work
    
    To avoid going to sleep only to get woken shortly thereafter, spin
    briefly for new work upon completion of work.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/fs/io-wq.h b/fs/io-wq.h
index 7c333a28e2a7..fb993b2bd0ef 100644
--- a/fs/io-wq.h
+++ b/fs/io-wq.h
@@ -35,7 +35,8 @@ static inline void wq_list_add_tail(struct io_wq_work_node *node,
 				    struct io_wq_work_list *list)
 {
 	if (!list->first) {
-		list->first = list->last = node;
+		list->last = node;
+		WRITE_ONCE(list->first, node);
 	} else {
 		list->last->next = node;
 		list->last = node;
@@ -47,7 +48,7 @@ static inline void wq_node_del(struct io_wq_work_list *list,
 			       struct io_wq_work_node *prev)
 {
 	if (node == list->first)
-		list->first = node->next;
+		WRITE_ONCE(list->first, node->next);
 	if (node == list->last)
 		list->last = prev;
 	if (prev)
@@ -58,7 +59,7 @@ static inline void wq_node_del(struct io_wq_work_list *list,
 #define wq_list_for_each(pos, prv, head)			\
 	for (pos = (head)->first, prv = NULL; pos; prv = pos, pos = (pos)->next)
 
-#define wq_list_empty(list)	((list)->first == NULL)
+#define wq_list_empty(list)	(READ_ONCE((list)->first) == NULL)
 #define INIT_WQ_LIST(list)	do {				\
 	(list)->first = NULL;					\
 	(list)->last = NULL;					\

commit 08bdcc35f00c91b325195723cceaba0937a89ddf
Author: Jens Axboe <axboe@kernel.dk>
Date:   Wed Dec 4 17:19:44 2019 -0700

    io-wq: clear node->next on list deletion
    
    If someone removes a node from a list, and then later adds it back to
    a list, we can have invalid data in ->next. This can cause all sorts
    of issues. One such use case is the IORING_OP_POLL_ADD command, which
    will do just that if we race and get woken twice without any pending
    events. This is a pretty rare case, but can happen under extreme loads.
    Dan reports that he saw the following crash:
    
    BUG: kernel NULL pointer dereference, address: 0000000000000000
    PGD d283ce067 P4D d283ce067 PUD e5ca04067 PMD 0
    Oops: 0002 [#1] SMP
    CPU: 17 PID: 10726 Comm: tao:fast-fiber Kdump: loaded Not tainted 5.2.9-02851-gac7bc042d2d1 #116
    Hardware name: Quanta Twin Lakes MP/Twin Lakes Passive MP, BIOS F09_3A17 05/03/2019
    RIP: 0010:io_wqe_enqueue+0x3e/0xd0
    Code: 34 24 74 55 8b 47 58 48 8d 6f 50 85 c0 74 50 48 89 df e8 35 7c 75 00 48 83 7b 08 00 48 8b 14 24 0f 84 84 00 00 00 48 8b 4b 10 <48> 89 11 48 89 53 10 83 63 20 fe 48 89 c6 48 89 df e8 0c 7a 75 00
    RSP: 0000:ffffc90006858a08 EFLAGS: 00010082
    RAX: 0000000000000002 RBX: ffff889037492fc0 RCX: 0000000000000000
    RDX: ffff888e40cc11a8 RSI: ffff888e40cc11a8 RDI: ffff889037492fc0
    RBP: ffff889037493010 R08: 00000000000000c3 R09: ffffc90006858ab8
    R10: 0000000000000000 R11: 0000000000000000 R12: ffff888e40cc11a8
    R13: 0000000000000000 R14: 00000000000000c3 R15: ffff888e40cc1100
    FS:  00007fcddc9db700(0000) GS:ffff88903fa40000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 0000000000000000 CR3: 0000000e479f5003 CR4: 00000000007606e0
    DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
    PKRU: 55555554
    Call Trace:
     <IRQ>
     io_poll_wake+0x12f/0x2a0
     __wake_up_common+0x86/0x120
     __wake_up_common_lock+0x7a/0xc0
     sock_def_readable+0x3c/0x70
     tcp_rcv_established+0x557/0x630
     tcp_v6_do_rcv+0x118/0x3c0
     tcp_v6_rcv+0x97e/0x9d0
     ip6_protocol_deliver_rcu+0xe3/0x440
     ip6_input+0x3d/0xc0
     ? ip6_protocol_deliver_rcu+0x440/0x440
     ipv6_rcv+0x56/0xd0
     ? ip6_rcv_finish_core.isra.18+0x80/0x80
     __netif_receive_skb_one_core+0x50/0x70
     netif_receive_skb_internal+0x2f/0xa0
     napi_gro_receive+0x125/0x150
     mlx5e_handle_rx_cqe+0x1d9/0x5a0
     ? mlx5e_poll_tx_cq+0x305/0x560
     mlx5e_poll_rx_cq+0x49f/0x9c5
     mlx5e_napi_poll+0xee/0x640
     ? smp_reschedule_interrupt+0x16/0xd0
     ? reschedule_interrupt+0xf/0x20
     net_rx_action+0x286/0x3d0
     __do_softirq+0xca/0x297
     irq_exit+0x96/0xa0
     do_IRQ+0x54/0xe0
     common_interrupt+0xf/0xf
     </IRQ>
    RIP: 0033:0x7fdc627a2e3a
    Code: 31 c0 85 d2 0f 88 f6 00 00 00 55 48 89 e5 41 57 41 56 4c 63 f2 41 55 41 54 53 48 83 ec 18 48 85 ff 0f 84 c7 00 00 00 48 8b 07 <41> 89 d4 49 89 f5 48 89 fb 48 85 c0 0f 84 64 01 00 00 48 83 78 10
    
    when running a networked workload with about 5000 sockets being polled
    for. Fix this by clearing node->next when the node is being removed from
    the list.
    
    Fixes: 6206f0e180d4 ("io-wq: shrink io_wq_work a bit")
    Reported-by: Dan Melnic <dmm@fb.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/fs/io-wq.h b/fs/io-wq.h
index 892db0bb64b1..7c333a28e2a7 100644
--- a/fs/io-wq.h
+++ b/fs/io-wq.h
@@ -52,6 +52,7 @@ static inline void wq_node_del(struct io_wq_work_list *list,
 		list->last = prev;
 	if (prev)
 		prev->next = node->next;
+	node->next = NULL;
 }
 
 #define wq_list_for_each(pos, prv, head)			\

commit 8cdda87a4414092cd210e766189cf0353a844861
Author: Jackie Liu <liuyun01@kylinos.cn>
Date:   Mon Dec 2 17:14:53 2019 +0800

    io_uring: remove io_wq_current_is_worker
    
    Since commit b18fdf71e01f ("io_uring: simplify io_req_link_next()"),
    the io_wq_current_is_worker function is no longer needed, clean it
    up.
    
    Signed-off-by: Jackie Liu <liuyun01@kylinos.cn>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/fs/io-wq.h b/fs/io-wq.h
index dd0af0d7376c..892db0bb64b1 100644
--- a/fs/io-wq.h
+++ b/fs/io-wq.h
@@ -118,10 +118,6 @@ static inline void io_wq_worker_sleeping(struct task_struct *tsk)
 static inline void io_wq_worker_running(struct task_struct *tsk)
 {
 }
-#endif
+#endif /* CONFIG_IO_WQ */
 
-static inline bool io_wq_current_is_worker(void)
-{
-	return in_task() && (current->flags & PF_IO_WORKER);
-}
-#endif
+#endif /* INTERNAL_IO_WQ_H */

commit 0b8c0ec7eedcd8f9f1a1f238d87f9b512b09e71a
Author: Jens Axboe <axboe@kernel.dk>
Date:   Mon Dec 2 08:50:00 2019 -0700

    io_uring: use current task creds instead of allocating a new one
    
    syzbot reports:
    
    kasan: CONFIG_KASAN_INLINE enabled
    kasan: GPF could be caused by NULL-ptr deref or user memory access
    general protection fault: 0000 [#1] PREEMPT SMP KASAN
    CPU: 0 PID: 9217 Comm: io_uring-sq Not tainted 5.4.0-syzkaller #0
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS
    Google 01/01/2011
    RIP: 0010:creds_are_invalid kernel/cred.c:792 [inline]
    RIP: 0010:__validate_creds include/linux/cred.h:187 [inline]
    RIP: 0010:override_creds+0x9f/0x170 kernel/cred.c:550
    Code: ac 25 00 81 fb 64 65 73 43 0f 85 a3 37 00 00 e8 17 ab 25 00 49 8d 7c
    24 10 48 b8 00 00 00 00 00 fc ff df 48 89 fa 48 c1 ea 03 <0f> b6 04 02 84
    c0 74 08 3c 03 0f 8e 96 00 00 00 41 8b 5c 24 10 bf
    RSP: 0018:ffff88809c45fda0 EFLAGS: 00010202
    RAX: dffffc0000000000 RBX: 0000000043736564 RCX: ffffffff814f3318
    RDX: 0000000000000002 RSI: ffffffff814f3329 RDI: 0000000000000010
    RBP: ffff88809c45fdb8 R08: ffff8880a3aac240 R09: ffffed1014755849
    R10: ffffed1014755848 R11: ffff8880a3aac247 R12: 0000000000000000
    R13: ffff888098ab1600 R14: 0000000000000000 R15: 0000000000000000
    FS:  0000000000000000(0000) GS:ffff8880ae800000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 00007ffd51c40664 CR3: 0000000092641000 CR4: 00000000001406f0
    DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
    Call Trace:
      io_sq_thread+0x1c7/0xa20 fs/io_uring.c:3274
      kthread+0x361/0x430 kernel/kthread.c:255
      ret_from_fork+0x24/0x30 arch/x86/entry/entry_64.S:352
    Modules linked in:
    ---[ end trace f2e1a4307fbe2245 ]---
    RIP: 0010:creds_are_invalid kernel/cred.c:792 [inline]
    RIP: 0010:__validate_creds include/linux/cred.h:187 [inline]
    RIP: 0010:override_creds+0x9f/0x170 kernel/cred.c:550
    Code: ac 25 00 81 fb 64 65 73 43 0f 85 a3 37 00 00 e8 17 ab 25 00 49 8d 7c
    24 10 48 b8 00 00 00 00 00 fc ff df 48 89 fa 48 c1 ea 03 <0f> b6 04 02 84
    c0 74 08 3c 03 0f 8e 96 00 00 00 41 8b 5c 24 10 bf
    RSP: 0018:ffff88809c45fda0 EFLAGS: 00010202
    RAX: dffffc0000000000 RBX: 0000000043736564 RCX: ffffffff814f3318
    RDX: 0000000000000002 RSI: ffffffff814f3329 RDI: 0000000000000010
    RBP: ffff88809c45fdb8 R08: ffff8880a3aac240 R09: ffffed1014755849
    R10: ffffed1014755848 R11: ffff8880a3aac247 R12: 0000000000000000
    R13: ffff888098ab1600 R14: 0000000000000000 R15: 0000000000000000
    FS:  0000000000000000(0000) GS:ffff8880ae800000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 00007ffd51c40664 CR3: 0000000092641000 CR4: 00000000001406f0
    DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
    
    which is caused by slab fault injection triggering a failure in
    prepare_creds(). We don't actually need to create a copy of the creds
    as we're not modifying it, we just need a reference on the current task
    creds. This avoids the failure case as well, and propagates the const
    throughout the stack.
    
    Fixes: 181e448d8709 ("io_uring: async workers should inherit the user creds")
    Reported-by: syzbot+5320383e16029ba057ff@syzkaller.appspotmail.com
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/fs/io-wq.h b/fs/io-wq.h
index 600e0158cba7..dd0af0d7376c 100644
--- a/fs/io-wq.h
+++ b/fs/io-wq.h
@@ -87,7 +87,7 @@ typedef void (put_work_fn)(struct io_wq_work *);
 struct io_wq_data {
 	struct mm_struct *mm;
 	struct user_struct *user;
-	struct cred *creds;
+	const struct cred *creds;
 
 	get_work_fn *get_work;
 	put_work_fn *put_work;

commit 6206f0e180d4eddc0a178f57120ab1b913701f6e
Author: Jens Axboe <axboe@kernel.dk>
Date:   Tue Nov 26 11:59:32 2019 -0700

    io-wq: shrink io_wq_work a bit
    
    Currently we're using 40 bytes for the io_wq_work structure, and 16 of
    those is the doubly link list node. We don't need doubly linked lists,
    we always add to tail to keep things ordered, and any other use case
    is list traversal with deletion. For the deletion case, we can easily
    support any node deletion by keeping track of the previous entry.
    
    This shrinks io_wq_work to 32 bytes, and subsequently io_kiock from
    io_uring to 216 to 208 bytes.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/fs/io-wq.h b/fs/io-wq.h
index 5cd8c7697e88..600e0158cba7 100644
--- a/fs/io-wq.h
+++ b/fs/io-wq.h
@@ -22,18 +22,60 @@ enum io_wq_cancel {
 	IO_WQ_CANCEL_NOTFOUND,	/* work not found */
 };
 
+struct io_wq_work_node {
+	struct io_wq_work_node *next;
+};
+
+struct io_wq_work_list {
+	struct io_wq_work_node *first;
+	struct io_wq_work_node *last;
+};
+
+static inline void wq_list_add_tail(struct io_wq_work_node *node,
+				    struct io_wq_work_list *list)
+{
+	if (!list->first) {
+		list->first = list->last = node;
+	} else {
+		list->last->next = node;
+		list->last = node;
+	}
+}
+
+static inline void wq_node_del(struct io_wq_work_list *list,
+			       struct io_wq_work_node *node,
+			       struct io_wq_work_node *prev)
+{
+	if (node == list->first)
+		list->first = node->next;
+	if (node == list->last)
+		list->last = prev;
+	if (prev)
+		prev->next = node->next;
+}
+
+#define wq_list_for_each(pos, prv, head)			\
+	for (pos = (head)->first, prv = NULL; pos; prv = pos, pos = (pos)->next)
+
+#define wq_list_empty(list)	((list)->first == NULL)
+#define INIT_WQ_LIST(list)	do {				\
+	(list)->first = NULL;					\
+	(list)->last = NULL;					\
+} while (0)
+
 struct io_wq_work {
 	union {
-		struct list_head list;
+		struct io_wq_work_node list;
 		void *data;
 	};
 	void (*func)(struct io_wq_work **);
-	unsigned flags;
 	struct files_struct *files;
+	unsigned flags;
 };
 
 #define INIT_IO_WORK(work, _func)			\
 	do {						\
+		(work)->list.next = NULL;		\
 		(work)->func = _func;			\
 		(work)->flags = 0;			\
 		(work)->files = NULL;			\

commit 181e448d8709e517c9c7b523fcd209f24eb38ca7
Author: Jens Axboe <axboe@kernel.dk>
Date:   Mon Nov 25 08:52:30 2019 -0700

    io_uring: async workers should inherit the user creds
    
    If we don't inherit the original task creds, then we can confuse users
    like fuse that pass creds in the request header. See link below on
    identical aio issue.
    
    Link: https://lore.kernel.org/linux-fsdevel/26f0d78e-99ca-2f1b-78b9-433088053a61@scylladb.com/T/#u
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/fs/io-wq.h b/fs/io-wq.h
index bb8f1c8f8e24..5cd8c7697e88 100644
--- a/fs/io-wq.h
+++ b/fs/io-wq.h
@@ -45,6 +45,7 @@ typedef void (put_work_fn)(struct io_wq_work *);
 struct io_wq_data {
 	struct mm_struct *mm;
 	struct user_struct *user;
+	struct cred *creds;
 
 	get_work_fn *get_work;
 	put_work_fn *put_work;

commit 576a347b7af8abfbddc80783fb6629c2894d036e
Author: Jens Axboe <axboe@kernel.dk>
Date:   Mon Nov 25 08:49:20 2019 -0700

    io-wq: have io_wq_create() take a 'data' argument
    
    We currently pass in 4 arguments outside of the bounded size. In
    preparation for adding one more argument, let's bundle them up in
    a struct to make it more readable.
    
    No functional changes in this patch.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/fs/io-wq.h b/fs/io-wq.h
index b68b11bf3633..bb8f1c8f8e24 100644
--- a/fs/io-wq.h
+++ b/fs/io-wq.h
@@ -42,9 +42,15 @@ struct io_wq_work {
 typedef void (get_work_fn)(struct io_wq_work *);
 typedef void (put_work_fn)(struct io_wq_work *);
 
-struct io_wq *io_wq_create(unsigned bounded, struct mm_struct *mm,
-				struct user_struct *user,
-				get_work_fn *get_work, put_work_fn *put_work);
+struct io_wq_data {
+	struct mm_struct *mm;
+	struct user_struct *user;
+
+	get_work_fn *get_work;
+	put_work_fn *put_work;
+};
+
+struct io_wq *io_wq_create(unsigned bounded, struct io_wq_data *data);
 void io_wq_destroy(struct io_wq *wq);
 
 void io_wq_enqueue(struct io_wq *wq, struct io_wq_work *work);

commit b76da70fc3759df13e0991706451f1a2e06ba19e
Author: Jens Axboe <axboe@kernel.dk>
Date:   Wed Nov 20 13:05:32 2019 -0700

    io_uring: close lookup gap for dependent next work
    
    When we find new work to process within the work handler, we queue the
    linked timeout before we have issued the new work. This can be
    problematic for very short timeouts, as we have a window where the new
    work isn't visible.
    
    Allow the work handler to store a callback function for this in the work
    item, and flag it with IO_WQ_WORK_CB if the caller has done so. If that
    is set, then io-wq will call the callback when it has setup the new work
    item.
    
    Reported-by: Pavel Begunkov <asml.silence@gmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/fs/io-wq.h b/fs/io-wq.h
index 4b29f922f80c..b68b11bf3633 100644
--- a/fs/io-wq.h
+++ b/fs/io-wq.h
@@ -11,6 +11,7 @@ enum {
 	IO_WQ_WORK_NEEDS_FILES	= 16,
 	IO_WQ_WORK_UNBOUND	= 32,
 	IO_WQ_WORK_INTERNAL	= 64,
+	IO_WQ_WORK_CB		= 128,
 
 	IO_WQ_HASH_SHIFT	= 24,	/* upper 8 bits are used for hash key */
 };
@@ -22,7 +23,10 @@ enum io_wq_cancel {
 };
 
 struct io_wq_work {
-	struct list_head list;
+	union {
+		struct list_head list;
+		void *data;
+	};
 	void (*func)(struct io_wq_work **);
 	unsigned flags;
 	struct files_struct *files;

commit 7d7230652e7c788ef908536fd79f4cca077f269f
Author: Jens Axboe <axboe@kernel.dk>
Date:   Tue Nov 12 22:31:31 2019 -0700

    io_wq: add get/put_work handlers to io_wq_create()
    
    For cancellation, we need to ensure that the work item stays valid for
    as long as ->cur_work is valid. Right now we can't safely dereference
    the work item even under the wqe->lock, because while the ->cur_work
    pointer will remain valid, the work could be completing and be freed
    in parallel.
    
    Only invoke ->get/put_work() on items we know that the caller queued
    themselves. Add IO_WQ_WORK_INTERNAL for io-wq to use, which is needed
    when we're queueing a flush item, for instance.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/fs/io-wq.h b/fs/io-wq.h
index cc50754d028c..4b29f922f80c 100644
--- a/fs/io-wq.h
+++ b/fs/io-wq.h
@@ -10,6 +10,7 @@ enum {
 	IO_WQ_WORK_NEEDS_USER	= 8,
 	IO_WQ_WORK_NEEDS_FILES	= 16,
 	IO_WQ_WORK_UNBOUND	= 32,
+	IO_WQ_WORK_INTERNAL	= 64,
 
 	IO_WQ_HASH_SHIFT	= 24,	/* upper 8 bits are used for hash key */
 };
@@ -34,8 +35,12 @@ struct io_wq_work {
 		(work)->files = NULL;			\
 	} while (0)					\
 
+typedef void (get_work_fn)(struct io_wq_work *);
+typedef void (put_work_fn)(struct io_wq_work *);
+
 struct io_wq *io_wq_create(unsigned bounded, struct mm_struct *mm,
-				struct user_struct *user);
+				struct user_struct *user,
+				get_work_fn *get_work, put_work_fn *put_work);
 void io_wq_destroy(struct io_wq *wq);
 
 void io_wq_enqueue(struct io_wq *wq, struct io_wq_work *work);

commit 960e432dfa5927892a9b170d14de874597b84849
Author: Jens Axboe <axboe@kernel.dk>
Date:   Tue Nov 12 07:56:39 2019 -0700

    io_uring: use correct "is IO worker" helper
    
    Since we switched to io-wq, the dependent link optimization for when to
    pass back work inline has been broken. Fix this by providing a suitable
    io-wq helper for io_uring to use to detect when to do this.
    
    Fixes: 561fb04a6a22 ("io_uring: replace workqueue usage with io-wq")
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/fs/io-wq.h b/fs/io-wq.h
index 8cb345256f35..cc50754d028c 100644
--- a/fs/io-wq.h
+++ b/fs/io-wq.h
@@ -62,4 +62,8 @@ static inline void io_wq_worker_running(struct task_struct *tsk)
 }
 #endif
 
+static inline bool io_wq_current_is_worker(void)
+{
+	return in_task() && (current->flags & PF_IO_WORKER);
+}
 #endif

commit c5def4ab849494d3c97f6c9fc84b2ddb868fe78c
Author: Jens Axboe <axboe@kernel.dk>
Date:   Thu Nov 7 11:41:16 2019 -0700

    io-wq: add support for bounded vs unbunded work
    
    io_uring supports request types that basically have two different
    lifetimes:
    
    1) Bounded completion time. These are requests like disk reads or writes,
       which we know will finish in a finite amount of time.
    2) Unbounded completion time. These are generally networked IO, where we
       have no idea how long they will take to complete. Another example is
       POLL commands.
    
    This patch provides support for io-wq to handle these differently, so we
    don't starve bounded requests by tying up workers for too long. By default
    all work is bounded, unless otherwise specified in the work item.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/fs/io-wq.h b/fs/io-wq.h
index 3de192dc73fc..8cb345256f35 100644
--- a/fs/io-wq.h
+++ b/fs/io-wq.h
@@ -9,6 +9,7 @@ enum {
 	IO_WQ_WORK_HASHED	= 4,
 	IO_WQ_WORK_NEEDS_USER	= 8,
 	IO_WQ_WORK_NEEDS_FILES	= 16,
+	IO_WQ_WORK_UNBOUND	= 32,
 
 	IO_WQ_HASH_SHIFT	= 24,	/* upper 8 bits are used for hash key */
 };
@@ -33,7 +34,8 @@ struct io_wq_work {
 		(work)->files = NULL;			\
 	} while (0)					\
 
-struct io_wq *io_wq_create(unsigned concurrency, struct mm_struct *mm);
+struct io_wq *io_wq_create(unsigned bounded, struct mm_struct *mm,
+				struct user_struct *user);
 void io_wq_destroy(struct io_wq *wq);
 
 void io_wq_enqueue(struct io_wq *wq, struct io_wq_work *work);

commit 62755e35dfb2b113c52b81cd96d01c20971c8e02
Author: Jens Axboe <axboe@kernel.dk>
Date:   Mon Oct 28 21:49:21 2019 -0600

    io_uring: support for generic async request cancel
    
    This adds support for IORING_OP_ASYNC_CANCEL, which will attempt to
    cancel requests that have been punted to async context and are now
    in-flight. This works for regular read/write requests to files, as
    long as they haven't been started yet. For socket based IO (or things
    like accept4(2)), we can cancel work that is already running as well.
    
    To cancel a request, the sqe must have ->addr set to the user_data of
    the request it wishes to cancel. If the request is cancelled
    successfully, the original request is completed with -ECANCELED
    and the cancel request is completed with a result of 0. If the
    request was already running, the original may or may not complete
    in error. The cancel request will complete with -EALREADY for that
    case. And finally, if the request to cancel wasn't found, the cancel
    request is completed with -ENOENT.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/fs/io-wq.h b/fs/io-wq.h
index e93f764b1fa4..3de192dc73fc 100644
--- a/fs/io-wq.h
+++ b/fs/io-wq.h
@@ -43,6 +43,11 @@ void io_wq_flush(struct io_wq *wq);
 void io_wq_cancel_all(struct io_wq *wq);
 enum io_wq_cancel io_wq_cancel_work(struct io_wq *wq, struct io_wq_work *cwork);
 
+typedef bool (work_cancel_fn)(struct io_wq_work *, void *);
+
+enum io_wq_cancel io_wq_cancel_cb(struct io_wq *wq, work_cancel_fn *cancel,
+					void *data);
+
 #if defined(CONFIG_IO_WQ)
 extern void io_wq_worker_sleeping(struct task_struct *);
 extern void io_wq_worker_running(struct task_struct *);

commit fcb323cc53e29d9cc696d606bb42736b32dd9825
Author: Jens Axboe <axboe@kernel.dk>
Date:   Thu Oct 24 12:39:47 2019 -0600

    io_uring: io_uring: add support for async work inheriting files
    
    This is in preparation for adding opcodes that need to add new files
    in a process file table, system calls like open(2) or accept4(2).
    
    If an opcode needs this, it must set IO_WQ_WORK_NEEDS_FILES in the work
    item. If work that needs to get punted to async context have this
    set, the async worker will assume the original task file table before
    executing the work.
    
    Note that opcodes that need access to the current files of an
    application cannot be done through IORING_SETUP_SQPOLL.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/fs/io-wq.h b/fs/io-wq.h
index be8f22c8937b..e93f764b1fa4 100644
--- a/fs/io-wq.h
+++ b/fs/io-wq.h
@@ -8,6 +8,7 @@ enum {
 	IO_WQ_WORK_HAS_MM	= 2,
 	IO_WQ_WORK_HASHED	= 4,
 	IO_WQ_WORK_NEEDS_USER	= 8,
+	IO_WQ_WORK_NEEDS_FILES	= 16,
 
 	IO_WQ_HASH_SHIFT	= 24,	/* upper 8 bits are used for hash key */
 };
@@ -22,12 +23,14 @@ struct io_wq_work {
 	struct list_head list;
 	void (*func)(struct io_wq_work **);
 	unsigned flags;
+	struct files_struct *files;
 };
 
 #define INIT_IO_WORK(work, _func)			\
 	do {						\
 		(work)->func = _func;			\
 		(work)->flags = 0;			\
+		(work)->files = NULL;			\
 	} while (0)					\
 
 struct io_wq *io_wq_create(unsigned concurrency, struct mm_struct *mm);

commit 771b53d033e8663abdf59704806aa856b236dcdb
Author: Jens Axboe <axboe@kernel.dk>
Date:   Tue Oct 22 10:25:58 2019 -0600

    io-wq: small threadpool implementation for io_uring
    
    This adds support for io-wq, a smaller and specialized thread pool
    implementation. This is meant to replace workqueues for io_uring. Among
    the reasons for this addition are:
    
    - We can assign memory context smarter and more persistently if we
      manage the life time of threads.
    
    - We can drop various work-arounds we have in io_uring, like the
      async_list.
    
    - We can implement hashed work insertion, to manage concurrency of
      buffered writes without needing a) an extra workqueue, or b)
      needlessly making the concurrency of said workqueue very low
      which hurts performance of multiple buffered file writers.
    
    - We can implement cancel through signals, for cancelling
      interruptible work like read/write (or send/recv) to/from sockets.
    
    - We need the above cancel for being able to assign and use file tables
      from a process.
    
    - We can implement a more thorough cancel operation in general.
    
    - We need it to move towards a syslet/threadlet model for even faster
      async execution. For that we need to take ownership of the used
      threads.
    
    This list is just off the top of my head. Performance should be the
    same, or better, at least that's what I've seen in my testing. io-wq
    supports basic NUMA functionality, setting up a pool per node.
    
    io-wq hooks up to the scheduler schedule in/out just like workqueue
    and uses that to drive the need for more/less workers.
    
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/fs/io-wq.h b/fs/io-wq.h
new file mode 100644
index 000000000000..be8f22c8937b
--- /dev/null
+++ b/fs/io-wq.h
@@ -0,0 +1,55 @@
+#ifndef INTERNAL_IO_WQ_H
+#define INTERNAL_IO_WQ_H
+
+struct io_wq;
+
+enum {
+	IO_WQ_WORK_CANCEL	= 1,
+	IO_WQ_WORK_HAS_MM	= 2,
+	IO_WQ_WORK_HASHED	= 4,
+	IO_WQ_WORK_NEEDS_USER	= 8,
+
+	IO_WQ_HASH_SHIFT	= 24,	/* upper 8 bits are used for hash key */
+};
+
+enum io_wq_cancel {
+	IO_WQ_CANCEL_OK,	/* cancelled before started */
+	IO_WQ_CANCEL_RUNNING,	/* found, running, and attempted cancelled */
+	IO_WQ_CANCEL_NOTFOUND,	/* work not found */
+};
+
+struct io_wq_work {
+	struct list_head list;
+	void (*func)(struct io_wq_work **);
+	unsigned flags;
+};
+
+#define INIT_IO_WORK(work, _func)			\
+	do {						\
+		(work)->func = _func;			\
+		(work)->flags = 0;			\
+	} while (0)					\
+
+struct io_wq *io_wq_create(unsigned concurrency, struct mm_struct *mm);
+void io_wq_destroy(struct io_wq *wq);
+
+void io_wq_enqueue(struct io_wq *wq, struct io_wq_work *work);
+void io_wq_enqueue_hashed(struct io_wq *wq, struct io_wq_work *work, void *val);
+void io_wq_flush(struct io_wq *wq);
+
+void io_wq_cancel_all(struct io_wq *wq);
+enum io_wq_cancel io_wq_cancel_work(struct io_wq *wq, struct io_wq_work *cwork);
+
+#if defined(CONFIG_IO_WQ)
+extern void io_wq_worker_sleeping(struct task_struct *);
+extern void io_wq_worker_running(struct task_struct *);
+#else
+static inline void io_wq_worker_sleeping(struct task_struct *tsk)
+{
+}
+static inline void io_wq_worker_running(struct task_struct *tsk)
+{
+}
+#endif
+
+#endif
