commit 3c4c4075fc61f5c37a0112b1dc8398025dc3e26a
Author: David Howells <dhowells@redhat.com>
Date:   Wed May 27 15:51:30 2020 +0100

    afs: Fix the by-UUID server tree to allow servers with the same UUID
    
    Whilst it shouldn't happen, it is possible for multiple fileservers to
    share a UUID, particularly if an entire cell has been duplicated, UUIDs and
    all.  In such a case, it's not necessarily possible to map the effect of
    the CB.InitCallBackState3 incoming RPC to a specific server unambiguously
    by UUID and thus to a specific cell.
    
    Indeed, there's a problem whereby multiple server records may need to
    occupy the same spot in the rb_tree rooted in the afs_net struct.
    
    Fix this by allowing servers to form a list, with the head of the list in
    the tree.  When the front entry in the list is removed, the second in the
    list just replaces it.  afs_init_callback_state() then just goes down the
    line, poking each server in the list.
    
    This means that some servers will be unnecessarily poked, unfortunately.
    An alternative would be to route by call parameters.
    
    Reported-by: Jeffrey Altman <jaltman@auristor.com>
    Signed-off-by: David Howells <dhowells@redhat.com>
    Fixes: d2ddc776a458 ("afs: Overhaul volume and server record caching and fileserver rotation")

diff --git a/fs/afs/callback.c b/fs/afs/callback.c
index b4cb9bb63f0a..7d9b23d981bf 100644
--- a/fs/afs/callback.c
+++ b/fs/afs/callback.c
@@ -21,11 +21,17 @@
 #include "internal.h"
 
 /*
- * allow the fileserver to request callback state (re-)initialisation
+ * Allow the fileserver to request callback state (re-)initialisation.
+ * Unfortunately, UUIDs are not guaranteed unique.
  */
 void afs_init_callback_state(struct afs_server *server)
 {
-	server->cb_s_break++;
+	rcu_read_lock();
+	do {
+		server->cb_s_break++;
+		server = rcu_dereference(server->uuid_next);
+	} while (0);
+	rcu_read_unlock();
 }
 
 /*

commit 20325960f8750165964a6891a733e4cc15d19076
Author: David Howells <dhowells@redhat.com>
Date:   Thu Apr 30 01:03:49 2020 +0100

    afs: Reorganise volume and server trees to be rooted on the cell
    
    Reorganise afs_volume objects such that they're in a tree keyed on volume
    ID, rooted at on an afs_cell object rather than being in multiple trees,
    each of which is rooted on an afs_server object.
    
    afs_server structs become per-cell and acquire a pointer to the cell.
    
    The process of breaking a callback then starts with finding the server by
    its network address, following that to the cell and then looking up each
    volume ID in the volume tree.
    
    This is simpler than the afs_vol_interest/afs_cb_interest N:M mapping web
    and allows those structs and the code for maintaining them to be simplified
    or removed.
    
    It does make a couple of things a bit more tricky, though:
    
     (1) Operations now start with a volume, not a server, so there can be more
         than one answer as to whether or not the server we'll end up using
         supports the FS.InlineBulkStatus RPC.
    
     (2) CB RPC operations that specify the server UUID.  There's still a tree
         of servers by UUID on the afs_net struct, but the UUIDs in it aren't
         guaranteed unique.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/callback.c b/fs/afs/callback.c
index 282dbac84435..b4cb9bb63f0a 100644
--- a/fs/afs/callback.c
+++ b/fs/afs/callback.c
@@ -20,185 +20,6 @@
 #include <linux/sched.h>
 #include "internal.h"
 
-/*
- * Create volume and callback interests on a server.
- */
-static struct afs_cb_interest *afs_create_interest(struct afs_server *server,
-						   struct afs_vnode *vnode)
-{
-	struct afs_vol_interest *new_vi, *vi;
-	struct afs_cb_interest *new;
-	struct rb_node *parent, **pp;
-
-	new_vi = kzalloc(sizeof(struct afs_vol_interest), GFP_KERNEL);
-	if (!new_vi)
-		return NULL;
-
-	new = kzalloc(sizeof(struct afs_cb_interest), GFP_KERNEL);
-	if (!new) {
-		kfree(new_vi);
-		return NULL;
-	}
-
-	new_vi->usage = 1;
-	new_vi->vid = vnode->volume->vid;
-	INIT_HLIST_HEAD(&new_vi->cb_interests);
-
-	refcount_set(&new->usage, 1);
-	new->sb = vnode->vfs_inode.i_sb;
-	new->server = afs_get_server(server, afs_server_trace_get_new_cbi);
-	INIT_HLIST_NODE(&new->cb_vlink);
-
-	write_seqlock(&server->cb_break_lock);
-
-	pp = &server->cb_volumes.rb_node;
-	while ((parent = *pp)) {
-		vi = rb_entry(parent, struct afs_vol_interest, srv_node);
-		if (vi->vid < new_vi->vid) {
-			pp = &(*pp)->rb_left;
-		} else if (vi->vid > new_vi->vid) {
-			pp = &(*pp)->rb_right;
-		} else {
-			vi->usage++;
-			goto found_vi;
-		}
-	}
-
-	vi = new_vi;
-	new_vi = NULL;
-	rb_link_node_rcu(&vi->srv_node, parent, pp);
-	rb_insert_color(&vi->srv_node, &server->cb_volumes);
-
-found_vi:
-	new->vol_interest = vi;
-	hlist_add_head(&new->cb_vlink, &vi->cb_interests);
-
-	write_sequnlock(&server->cb_break_lock);
-	kfree(new_vi);
-	return new;
-}
-
-/*
- * Set up an interest-in-callbacks record for a volume on a server and
- * register it with the server.
- * - Called with vnode->io_lock held.
- */
-int afs_register_server_cb_interest(struct afs_vnode *vnode,
-				    struct afs_server_list *slist,
-				    unsigned int index)
-{
-	struct afs_server_entry *entry = &slist->servers[index];
-	struct afs_cb_interest *cbi, *vcbi, *new, *old;
-	struct afs_server *server = entry->server;
-
-again:
-	vcbi = rcu_dereference_protected(vnode->cb_interest,
-					 lockdep_is_held(&vnode->io_lock));
-	if (vcbi && likely(vcbi == entry->cb_interest))
-		return 0;
-
-	read_lock(&slist->lock);
-	cbi = afs_get_cb_interest(entry->cb_interest);
-	read_unlock(&slist->lock);
-
-	if (vcbi) {
-		if (vcbi == cbi) {
-			afs_put_cb_interest(afs_v2net(vnode), cbi);
-			return 0;
-		}
-
-		/* Use a new interest in the server list for the same server
-		 * rather than an old one that's still attached to a vnode.
-		 */
-		if (cbi && vcbi->server == cbi->server) {
-			write_seqlock(&vnode->cb_lock);
-			old = rcu_dereference_protected(vnode->cb_interest,
-							lockdep_is_held(&vnode->cb_lock.lock));
-			rcu_assign_pointer(vnode->cb_interest, cbi);
-			write_sequnlock(&vnode->cb_lock);
-			afs_put_cb_interest(afs_v2net(vnode), old);
-			return 0;
-		}
-
-		/* Re-use the one attached to the vnode. */
-		if (!cbi && vcbi->server == server) {
-			write_lock(&slist->lock);
-			if (entry->cb_interest) {
-				write_unlock(&slist->lock);
-				afs_put_cb_interest(afs_v2net(vnode), cbi);
-				goto again;
-			}
-
-			entry->cb_interest = cbi;
-			write_unlock(&slist->lock);
-			return 0;
-		}
-	}
-
-	if (!cbi) {
-		new = afs_create_interest(server, vnode);
-		if (!new)
-			return -ENOMEM;
-
-		write_lock(&slist->lock);
-		if (!entry->cb_interest) {
-			entry->cb_interest = afs_get_cb_interest(new);
-			cbi = new;
-			new = NULL;
-		} else {
-			cbi = afs_get_cb_interest(entry->cb_interest);
-		}
-		write_unlock(&slist->lock);
-		afs_put_cb_interest(afs_v2net(vnode), new);
-	}
-
-	ASSERT(cbi);
-
-	/* Change the server the vnode is using.  This entails scrubbing any
-	 * interest the vnode had in the previous server it was using.
-	 */
-	write_seqlock(&vnode->cb_lock);
-
-	old = rcu_dereference_protected(vnode->cb_interest,
-					lockdep_is_held(&vnode->cb_lock.lock));
-	rcu_assign_pointer(vnode->cb_interest, cbi);
-	vnode->cb_s_break = cbi->server->cb_s_break;
-	vnode->cb_v_break = vnode->volume->cb_v_break;
-	clear_bit(AFS_VNODE_CB_PROMISED, &vnode->flags);
-
-	write_sequnlock(&vnode->cb_lock);
-	afs_put_cb_interest(afs_v2net(vnode), old);
-	return 0;
-}
-
-/*
- * Remove an interest on a server.
- */
-void afs_put_cb_interest(struct afs_net *net, struct afs_cb_interest *cbi)
-{
-	struct afs_vol_interest *vi;
-
-	if (cbi && refcount_dec_and_test(&cbi->usage)) {
-		if (!hlist_unhashed(&cbi->cb_vlink)) {
-			write_seqlock(&cbi->server->cb_break_lock);
-
-			hlist_del_init(&cbi->cb_vlink);
-			vi = cbi->vol_interest;
-			cbi->vol_interest = NULL;
-			if (--vi->usage == 0)
-				rb_erase(&vi->srv_node, &cbi->server->cb_volumes);
-			else
-				vi = NULL;
-
-			write_sequnlock(&cbi->server->cb_break_lock);
-			if (vi)
-				kfree_rcu(vi, rcu);
-			afs_put_server(net, cbi->server, afs_server_trace_put_cbi);
-		}
-		kfree_rcu(cbi, rcu);
-	}
-}
-
 /*
  * allow the fileserver to request callback state (re-)initialisation
  */
@@ -236,12 +57,12 @@ void afs_break_callback(struct afs_vnode *vnode, enum afs_cb_break_reason reason
 }
 
 /*
- * Look up a volume interest by volume ID under RCU conditions.
+ * Look up a volume by volume ID under RCU conditions.
  */
-static struct afs_vol_interest *afs_lookup_vol_interest_rcu(struct afs_server *server,
-							    afs_volid_t vid)
+static struct afs_volume *afs_lookup_volume_rcu(struct afs_cell *cell,
+						afs_volid_t vid)
 {
-	struct afs_vol_interest *vi = NULL;
+	struct afs_volume *volume = NULL;
 	struct rb_node *p;
 	int seq = 0;
 
@@ -250,28 +71,25 @@ static struct afs_vol_interest *afs_lookup_vol_interest_rcu(struct afs_server *s
 		 * under just the RCU read lock, so we have to check for
 		 * changes.
 		 */
-		read_seqbegin_or_lock(&server->cb_break_lock, &seq);
+		read_seqbegin_or_lock(&cell->volume_lock, &seq);
 
-		p = rcu_dereference_raw(server->cb_volumes.rb_node);
+		p = rcu_dereference_raw(cell->volumes.rb_node);
 		while (p) {
-			vi = rb_entry(p, struct afs_vol_interest, srv_node);
+			volume = rb_entry(p, struct afs_volume, cell_node);
 
-			if (vi->vid < vid)
+			if (volume->vid < vid)
 				p = rcu_dereference_raw(p->rb_left);
-			else if (vi->vid > vid)
+			else if (volume->vid > vid)
 				p = rcu_dereference_raw(p->rb_right);
 			else
 				break;
-			/* We want to repeat the search, this time with the
-			 * lock properly locked.
-			 */
-			vi = NULL;
+			volume = NULL;
 		}
 
-	} while (need_seqretry(&server->cb_break_lock, seq));
+	} while (need_seqretry(&cell->volume_lock, seq));
 
-	done_seqretry(&server->cb_break_lock, seq);
-	return vi;
+	done_seqretry(&cell->volume_lock, seq);
+	return volume;
 }
 
 /*
@@ -280,42 +98,37 @@ static struct afs_vol_interest *afs_lookup_vol_interest_rcu(struct afs_server *s
  *   - the backing file is changed
  *   - a lock is released
  */
-static void afs_break_one_callback(struct afs_server *server,
-				   struct afs_fid *fid,
-				   struct afs_vol_interest *vi)
+static void afs_break_one_callback(struct afs_volume *volume,
+				   struct afs_fid *fid)
 {
-	struct afs_cb_interest *cbi;
+	struct super_block *sb;
 	struct afs_vnode *vnode;
 	struct inode *inode;
 
-	/* Step through all interested superblocks.  There may be more than one
-	 * because of cell aliasing.
-	 */
-	hlist_for_each_entry_rcu(cbi, &vi->cb_interests, cb_vlink) {
-		if (fid->vnode == 0 && fid->unique == 0) {
-			/* The callback break applies to an entire volume. */
-			struct afs_super_info *as = AFS_FS_S(cbi->sb);
-			struct afs_volume *volume = as->volume;
+	if (fid->vnode == 0 && fid->unique == 0) {
+		/* The callback break applies to an entire volume. */
+		write_lock(&volume->cb_v_break_lock);
+		volume->cb_v_break++;
+		trace_afs_cb_break(fid, volume->cb_v_break,
+				   afs_cb_break_for_volume_callback, false);
+		write_unlock(&volume->cb_v_break_lock);
+		return;
+	}
 
-			write_lock(&volume->cb_v_break_lock);
-			volume->cb_v_break++;
-			trace_afs_cb_break(fid, volume->cb_v_break,
-					   afs_cb_break_for_volume_callback, false);
-			write_unlock(&volume->cb_v_break_lock);
-		} else {
-			/* See if we can find a matching inode - even an I_NEW
-			 * inode needs to be marked as it can have its callback
-			 * broken before we finish setting up the local inode.
-			 */
-			inode = find_inode_rcu(cbi->sb, fid->vnode,
-					       afs_ilookup5_test_by_fid, fid);
-			if (inode) {
-				vnode = AFS_FS_I(inode);
-				afs_break_callback(vnode, afs_cb_break_for_callback);
-			} else {
-				trace_afs_cb_miss(fid, afs_cb_break_for_callback);
-			}
-		}
+	/* See if we can find a matching inode - even an I_NEW inode needs to
+	 * be marked as it can have its callback broken before we finish
+	 * setting up the local inode.
+	 */
+	sb = rcu_dereference(volume->sb);
+	if (!sb)
+		return;
+
+	inode = find_inode_rcu(sb, fid->vnode, afs_ilookup5_test_by_fid, fid);
+	if (inode) {
+		vnode = AFS_FS_I(inode);
+		afs_break_callback(vnode, afs_cb_break_for_callback);
+	} else {
+		trace_afs_cb_miss(fid, afs_cb_break_for_callback);
 	}
 }
 
@@ -324,11 +137,11 @@ static void afs_break_some_callbacks(struct afs_server *server,
 				     size_t *_count)
 {
 	struct afs_callback_break *residue = cbb;
-	struct afs_vol_interest *vi;
+	struct afs_volume *volume;
 	afs_volid_t vid = cbb->fid.vid;
 	size_t i;
 
-	vi = afs_lookup_vol_interest_rcu(server, vid);
+	volume = afs_lookup_volume_rcu(server->cell, vid);
 
 	/* TODO: Find all matching volumes if we couldn't match the server and
 	 * break them anyway.
@@ -341,8 +154,8 @@ static void afs_break_some_callbacks(struct afs_server *server,
 			       cbb->fid.vnode,
 			       cbb->fid.unique);
 			--*_count;
-			if (vi)
-				afs_break_one_callback(server, &cbb->fid, vi);
+			if (volume)
+				afs_break_one_callback(volume, &cbb->fid);
 		} else {
 			*residue++ = *cbb;
 		}
@@ -367,16 +180,3 @@ void afs_break_callbacks(struct afs_server *server, size_t count,
 	rcu_read_unlock();
 	return;
 }
-
-/*
- * Clear the callback interests in a server list.
- */
-void afs_clear_callback_interests(struct afs_net *net, struct afs_server_list *slist)
-{
-	int i;
-
-	for (i = 0; i < slist->nr_servers; i++) {
-		afs_put_cb_interest(net, slist->servers[i].cb_interest);
-		slist->servers[i].cb_interest = NULL;
-	}
-}

commit e49c7b2f6de7ff81ca34c56e4eeb4fa740c099f2
Author: David Howells <dhowells@redhat.com>
Date:   Fri Apr 10 20:51:51 2020 +0100

    afs: Build an abstraction around an "operation" concept
    
    Turn the afs_operation struct into the main way that most fileserver
    operations are managed.  Various things are added to the struct, including
    the following:
    
     (1) All the parameters and results of the relevant operations are moved
         into it, removing corresponding fields from the afs_call struct.
         afs_call gets a pointer to the op.
    
     (2) The target volume is made the main focus of the operation, rather than
         the target vnode(s), and a bunch of op->vnode->volume are made
         op->volume instead.
    
     (3) Two vnode records are defined (op->file[]) for the vnode(s) involved
         in most operations.  The vnode record (struct afs_vnode_param)
         contains:
    
            - The vnode pointer.
    
            - The fid of the vnode to be included in the parameters or that was
              returned in the reply (eg. FS.MakeDir).
    
            - The status and callback information that may be returned in the
              reply about the vnode.
    
            - Callback break and data version tracking for detecting
              simultaneous third-parth changes.
    
     (4) Pointers to dentries to be updated with new inodes.
    
     (5) An operations table pointer.  The table includes pointers to functions
         for issuing AFS and YFS-variant RPCs, handling the success and abort
         of an operation and handling post-I/O-lock local editing of a
         directory.
    
    To make this work, the following function restructuring is made:
    
     (A) The rotation loop that issues calls to fileservers that can be found
         in each function that wants to issue an RPC (such as afs_mkdir()) is
         extracted out into common code, in a new file called fs_operation.c.
    
     (B) The rotation loops, such as the one in afs_mkdir(), are replaced with
         a much smaller piece of code that allocates an operation, sets the
         parameters and then calls out to the common code to do the actual
         work.
    
     (C) The code for handling the success and failure of an operation are
         moved into operation functions (as (5) above) and these are called
         from the core code at appropriate times.
    
     (D) The pseudo inode getting stuff used by the dynamic root code is moved
         over into dynroot.c.
    
     (E) struct afs_iget_data is absorbed into the operation struct and
         afs_iget() expects to be given an op pointer and a vnode record.
    
     (F) Point (E) doesn't work for the root dir of a volume, but we know the
         FID in advance (it's always vnode 1, unique 1), so a separate inode
         getter, afs_root_iget(), is provided to special-case that.
    
     (G) The inode status init/update functions now also take an op and a vnode
         record.
    
     (H) The RPC marshalling functions now, for the most part, just take an
         afs_operation struct as their only argument.  All the data they need
         is held there.  The result delivery functions write their answers
         there as well.
    
     (I) The call is attached to the operation and then the operation core does
         the waiting.
    
    And then the new operation code is, for the moment, made to just initialise
    the operation, get the appropriate vnode I/O locks and do the same rotation
    loop as before.
    
    This lays the foundation for the following changes in the future:
    
     (*) Overhauling the rotation (again).
    
     (*) Support for asynchronous I/O, where the fileserver rotation must be
         done asynchronously also.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/callback.c b/fs/afs/callback.c
index b16781e1683e..282dbac84435 100644
--- a/fs/afs/callback.c
+++ b/fs/afs/callback.c
@@ -46,7 +46,6 @@ static struct afs_cb_interest *afs_create_interest(struct afs_server *server,
 
 	refcount_set(&new->usage, 1);
 	new->sb = vnode->vfs_inode.i_sb;
-	new->vid = vnode->volume->vid;
 	new->server = afs_get_server(server, afs_server_trace_get_new_cbi);
 	INIT_HLIST_NODE(&new->cb_vlink);
 
@@ -286,7 +285,6 @@ static void afs_break_one_callback(struct afs_server *server,
 				   struct afs_vol_interest *vi)
 {
 	struct afs_cb_interest *cbi;
-	struct afs_iget_data data;
 	struct afs_vnode *vnode;
 	struct inode *inode;
 
@@ -305,15 +303,12 @@ static void afs_break_one_callback(struct afs_server *server,
 					   afs_cb_break_for_volume_callback, false);
 			write_unlock(&volume->cb_v_break_lock);
 		} else {
-			data.volume = NULL;
-			data.fid = *fid;
-
 			/* See if we can find a matching inode - even an I_NEW
 			 * inode needs to be marked as it can have its callback
 			 * broken before we finish setting up the local inode.
 			 */
 			inode = find_inode_rcu(cbi->sb, fid->vnode,
-					       afs_iget5_test, &data);
+					       afs_ilookup5_test_by_fid, fid);
 			if (inode) {
 				vnode = AFS_FS_I(inode);
 				afs_break_callback(vnode, afs_cb_break_for_callback);

commit 8230fd8217b7ea76f838ae88e4a5a8e54f37099f
Author: David Howells <dhowells@redhat.com>
Date:   Fri Mar 27 15:02:44 2020 +0000

    afs: Make callback processing more efficient.
    
    afs_vol_interest objects represent the volume IDs currently being accessed
    from a fileserver.  These hold lists of afs_cb_interest objects that
    repesent the superblocks using that volume ID on that server.
    
    When a callback notification from the server telling of a modification by
    another client arrives, the volume ID specified in the notification is
    looked up in the server's afs_vol_interest list.  Through the
    afs_cb_interest list, the relevant superblocks can be iterated over and the
    specific inode looked up and marked in each one.
    
    Make the following efficiency improvements:
    
     (1) Hold rcu_read_lock() over the entire processing rather than locking it
         each time.
    
     (2) Do all the callbacks for each vid together rather than individually.
         Each volume then only needs to be looked up once.
    
     (3) afs_vol_interest objects are now stored in an rb_tree rather than a
         flat list to reduce the lookup step count.
    
     (4) afs_vol_interest lookup is now done with RCU, but because it's in an
         rb_tree which may rotate under us, a seqlock is used so that if it
         changes during the walk, we repeat the walk with a lock held.
    
    With this and the preceding patch which adds RCU-based lookups in the inode
    cache, target volumes/vnodes can be taken without the need to take any
    locks, except on the target itself.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/callback.c b/fs/afs/callback.c
index 0dcbd40732d1..b16781e1683e 100644
--- a/fs/afs/callback.c
+++ b/fs/afs/callback.c
@@ -28,7 +28,7 @@ static struct afs_cb_interest *afs_create_interest(struct afs_server *server,
 {
 	struct afs_vol_interest *new_vi, *vi;
 	struct afs_cb_interest *new;
-	struct hlist_node **pp;
+	struct rb_node *parent, **pp;
 
 	new_vi = kzalloc(sizeof(struct afs_vol_interest), GFP_KERNEL);
 	if (!new_vi)
@@ -42,7 +42,6 @@ static struct afs_cb_interest *afs_create_interest(struct afs_server *server,
 
 	new_vi->usage = 1;
 	new_vi->vid = vnode->volume->vid;
-	INIT_HLIST_NODE(&new_vi->srv_link);
 	INIT_HLIST_HEAD(&new_vi->cb_interests);
 
 	refcount_set(&new->usage, 1);
@@ -51,31 +50,31 @@ static struct afs_cb_interest *afs_create_interest(struct afs_server *server,
 	new->server = afs_get_server(server, afs_server_trace_get_new_cbi);
 	INIT_HLIST_NODE(&new->cb_vlink);
 
-	write_lock(&server->cb_break_lock);
+	write_seqlock(&server->cb_break_lock);
 
-	for (pp = &server->cb_volumes.first; *pp; pp = &(*pp)->next) {
-		vi = hlist_entry(*pp, struct afs_vol_interest, srv_link);
-		if (vi->vid < new_vi->vid)
-			continue;
-		if (vi->vid > new_vi->vid)
-			break;
-		vi->usage++;
-		goto found_vi;
+	pp = &server->cb_volumes.rb_node;
+	while ((parent = *pp)) {
+		vi = rb_entry(parent, struct afs_vol_interest, srv_node);
+		if (vi->vid < new_vi->vid) {
+			pp = &(*pp)->rb_left;
+		} else if (vi->vid > new_vi->vid) {
+			pp = &(*pp)->rb_right;
+		} else {
+			vi->usage++;
+			goto found_vi;
+		}
 	}
 
-	new_vi->srv_link.pprev = pp;
-	new_vi->srv_link.next = *pp;
-	if (*pp)
-		(*pp)->pprev = &new_vi->srv_link.next;
-	*pp = &new_vi->srv_link;
 	vi = new_vi;
 	new_vi = NULL;
-found_vi:
+	rb_link_node_rcu(&vi->srv_node, parent, pp);
+	rb_insert_color(&vi->srv_node, &server->cb_volumes);
 
+found_vi:
 	new->vol_interest = vi;
 	hlist_add_head(&new->cb_vlink, &vi->cb_interests);
 
-	write_unlock(&server->cb_break_lock);
+	write_sequnlock(&server->cb_break_lock);
 	kfree(new_vi);
 	return new;
 }
@@ -182,17 +181,17 @@ void afs_put_cb_interest(struct afs_net *net, struct afs_cb_interest *cbi)
 
 	if (cbi && refcount_dec_and_test(&cbi->usage)) {
 		if (!hlist_unhashed(&cbi->cb_vlink)) {
-			write_lock(&cbi->server->cb_break_lock);
+			write_seqlock(&cbi->server->cb_break_lock);
 
 			hlist_del_init(&cbi->cb_vlink);
 			vi = cbi->vol_interest;
 			cbi->vol_interest = NULL;
 			if (--vi->usage == 0)
-				hlist_del(&vi->srv_link);
+				rb_erase(&vi->srv_node, &cbi->server->cb_volumes);
 			else
 				vi = NULL;
 
-			write_unlock(&cbi->server->cb_break_lock);
+			write_sequnlock(&cbi->server->cb_break_lock);
 			if (vi)
 				kfree_rcu(vi, rcu);
 			afs_put_server(net, cbi->server, afs_server_trace_put_cbi);
@@ -237,6 +236,45 @@ void afs_break_callback(struct afs_vnode *vnode, enum afs_cb_break_reason reason
 	write_sequnlock(&vnode->cb_lock);
 }
 
+/*
+ * Look up a volume interest by volume ID under RCU conditions.
+ */
+static struct afs_vol_interest *afs_lookup_vol_interest_rcu(struct afs_server *server,
+							    afs_volid_t vid)
+{
+	struct afs_vol_interest *vi = NULL;
+	struct rb_node *p;
+	int seq = 0;
+
+	do {
+		/* Unfortunately, rbtree walking doesn't give reliable results
+		 * under just the RCU read lock, so we have to check for
+		 * changes.
+		 */
+		read_seqbegin_or_lock(&server->cb_break_lock, &seq);
+
+		p = rcu_dereference_raw(server->cb_volumes.rb_node);
+		while (p) {
+			vi = rb_entry(p, struct afs_vol_interest, srv_node);
+
+			if (vi->vid < vid)
+				p = rcu_dereference_raw(p->rb_left);
+			else if (vi->vid > vid)
+				p = rcu_dereference_raw(p->rb_right);
+			else
+				break;
+			/* We want to repeat the search, this time with the
+			 * lock properly locked.
+			 */
+			vi = NULL;
+		}
+
+	} while (need_seqretry(&server->cb_break_lock, seq));
+
+	done_seqretry(&server->cb_break_lock, seq);
+	return vi;
+}
+
 /*
  * allow the fileserver to explicitly break one callback
  * - happens when
@@ -244,37 +282,18 @@ void afs_break_callback(struct afs_vnode *vnode, enum afs_cb_break_reason reason
  *   - a lock is released
  */
 static void afs_break_one_callback(struct afs_server *server,
-				   struct afs_fid *fid)
+				   struct afs_fid *fid,
+				   struct afs_vol_interest *vi)
 {
-	struct afs_vol_interest *vi;
 	struct afs_cb_interest *cbi;
 	struct afs_iget_data data;
 	struct afs_vnode *vnode;
 	struct inode *inode;
 
-	rcu_read_lock();
-	read_lock(&server->cb_break_lock);
-	hlist_for_each_entry(vi, &server->cb_volumes, srv_link) {
-		if (vi->vid < fid->vid)
-			continue;
-		if (vi->vid > fid->vid) {
-			vi = NULL;
-			break;
-		}
-		//atomic_inc(&vi->usage);
-		break;
-	}
-
-	/* TODO: Find all matching volumes if we couldn't match the server and
-	 * break them anyway.
-	 */
-	if (!vi)
-		goto out;
-
 	/* Step through all interested superblocks.  There may be more than one
 	 * because of cell aliasing.
 	 */
-	hlist_for_each_entry(cbi, &vi->cb_interests, cb_vlink) {
+	hlist_for_each_entry_rcu(cbi, &vi->cb_interests, cb_vlink) {
 		if (fid->vnode == 0 && fid->unique == 0) {
 			/* The callback break applies to an entire volume. */
 			struct afs_super_info *as = AFS_FS_S(cbi->sb);
@@ -303,10 +322,36 @@ static void afs_break_one_callback(struct afs_server *server,
 			}
 		}
 	}
+}
 
-out:
-	read_unlock(&server->cb_break_lock);
-	rcu_read_unlock();
+static void afs_break_some_callbacks(struct afs_server *server,
+				     struct afs_callback_break *cbb,
+				     size_t *_count)
+{
+	struct afs_callback_break *residue = cbb;
+	struct afs_vol_interest *vi;
+	afs_volid_t vid = cbb->fid.vid;
+	size_t i;
+
+	vi = afs_lookup_vol_interest_rcu(server, vid);
+
+	/* TODO: Find all matching volumes if we couldn't match the server and
+	 * break them anyway.
+	 */
+
+	for (i = *_count; i > 0; cbb++, i--) {
+		if (cbb->fid.vid == vid) {
+			_debug("- Fid { vl=%08llx n=%llu u=%u }",
+			       cbb->fid.vid,
+			       cbb->fid.vnode,
+			       cbb->fid.unique);
+			--*_count;
+			if (vi)
+				afs_break_one_callback(server, &cbb->fid, vi);
+		} else {
+			*residue++ = *cbb;
+		}
+	}
 }
 
 /*
@@ -319,17 +364,12 @@ void afs_break_callbacks(struct afs_server *server, size_t count,
 
 	ASSERT(server != NULL);
 
-	/* TODO: Sort the callback break list by volume ID */
+	rcu_read_lock();
 
-	for (; count > 0; callbacks++, count--) {
-		_debug("- Fid { vl=%08llx n=%llu u=%u }",
-		       callbacks->fid.vid,
-		       callbacks->fid.vnode,
-		       callbacks->fid.unique);
-		afs_break_one_callback(server, &callbacks->fid);
-	}
+	while (count > 0)
+		afs_break_some_callbacks(server, callbacks, &count);
 
-	_leave("");
+	rcu_read_unlock();
 	return;
 }
 

commit 3f19b2ab97a97b413c24b66c67ae16daa4f56c35
Author: David Howells <dhowells@redhat.com>
Date:   Fri Dec 1 11:40:16 2017 +0000

    vfs, afs, ext4: Make the inode hash table RCU searchable
    
    Make the inode hash table RCU searchable so that searches that want to
    access or modify an inode without taking a ref on that inode can do so
    without taking the inode hash table lock.
    
    The main thing this requires is some RCU annotation on the list
    manipulation operations.  Inodes are already freed by RCU in most cases.
    
    Users of this interface must take care as the inode may be still under
    construction or may be being torn down around them.
    
    There are at least three instances where this can be of use:
    
     (1) Testing whether the inode number iunique() is going to return is
         currently unique (the iunique_lock is still held).
    
     (2) Ext4 date stamp updating.
    
     (3) AFS callback breaking.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: Konstantin Khlebnikov <khlebnikov@yandex-team.ru>
    cc: linux-ext4@vger.kernel.org
    cc: linux-afs@lists.infradead.org

diff --git a/fs/afs/callback.c b/fs/afs/callback.c
index 2dca8df1a18d..0dcbd40732d1 100644
--- a/fs/afs/callback.c
+++ b/fs/afs/callback.c
@@ -252,6 +252,7 @@ static void afs_break_one_callback(struct afs_server *server,
 	struct afs_vnode *vnode;
 	struct inode *inode;
 
+	rcu_read_lock();
 	read_lock(&server->cb_break_lock);
 	hlist_for_each_entry(vi, &server->cb_volumes, srv_link) {
 		if (vi->vid < fid->vid)
@@ -287,12 +288,16 @@ static void afs_break_one_callback(struct afs_server *server,
 		} else {
 			data.volume = NULL;
 			data.fid = *fid;
-			inode = ilookup5_nowait(cbi->sb, fid->vnode,
-						afs_iget5_test, &data);
+
+			/* See if we can find a matching inode - even an I_NEW
+			 * inode needs to be marked as it can have its callback
+			 * broken before we finish setting up the local inode.
+			 */
+			inode = find_inode_rcu(cbi->sb, fid->vnode,
+					       afs_iget5_test, &data);
 			if (inode) {
 				vnode = AFS_FS_I(inode);
 				afs_break_callback(vnode, afs_cb_break_for_callback);
-				iput(inode);
 			} else {
 				trace_afs_cb_miss(fid, afs_cb_break_for_callback);
 			}
@@ -301,6 +306,7 @@ static void afs_break_one_callback(struct afs_server *server,
 
 out:
 	read_unlock(&server->cb_break_lock);
+	rcu_read_unlock();
 }
 
 /*

commit cd340703c29bdf90f7b32f7eba0ffd6f361b57bb
Author: Marc Dionne <marc.dionne@auristor.com>
Date:   Thu Nov 21 15:26:15 2019 +0000

    afs: Fix possible assert with callbacks from yfs servers
    
    Servers sending callback breaks to the YFS_CM_SERVICE service may
    send up to YFSCBMAX (1024) fids in a single RPC.  Anything over
    AFSCBMAX (50) will cause the assert in afs_break_callbacks to trigger.
    
    Remove the assert, as the count has already been checked against
    the appropriate max values in afs_deliver_cb_callback and
    afs_deliver_yfs_cb_callback.
    
    Fixes: 35dbfba3111a ("afs: Implement the YFS cache manager service")
    Signed-off-by: Marc Dionne <marc.dionne@auristor.com>
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/afs/callback.c b/fs/afs/callback.c
index 6cdd7047c809..2dca8df1a18d 100644
--- a/fs/afs/callback.c
+++ b/fs/afs/callback.c
@@ -312,7 +312,6 @@ void afs_break_callbacks(struct afs_server *server, size_t count,
 	_enter("%p,%zu,", server, count);
 
 	ASSERT(server != NULL);
-	ASSERTCMP(count, <=, AFSCBMAX);
 
 	/* TODO: Sort the callback break list by volume ID */
 

commit 452181936931f0f08923aba5e04e1e9ef58c389f
Author: David Howells <dhowells@redhat.com>
Date:   Thu Jun 20 18:12:17 2019 +0100

    afs: Trace afs_server usage
    
    Add a tracepoint (afs_server) to track the afs_server object usage count.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/callback.c b/fs/afs/callback.c
index d66cd0364243..6cdd7047c809 100644
--- a/fs/afs/callback.c
+++ b/fs/afs/callback.c
@@ -48,7 +48,7 @@ static struct afs_cb_interest *afs_create_interest(struct afs_server *server,
 	refcount_set(&new->usage, 1);
 	new->sb = vnode->vfs_inode.i_sb;
 	new->vid = vnode->volume->vid;
-	new->server = afs_get_server(server);
+	new->server = afs_get_server(server, afs_server_trace_get_new_cbi);
 	INIT_HLIST_NODE(&new->cb_vlink);
 
 	write_lock(&server->cb_break_lock);
@@ -195,7 +195,7 @@ void afs_put_cb_interest(struct afs_net *net, struct afs_cb_interest *cbi)
 			write_unlock(&cbi->server->cb_break_lock);
 			if (vi)
 				kfree_rcu(vi, rcu);
-			afs_put_server(net, cbi->server);
+			afs_put_server(net, cbi->server, afs_server_trace_put_cbi);
 		}
 		kfree_rcu(cbi, rcu);
 	}

commit 051d25250b55c215a2254a0130d46fbd38bcbcc0
Author: David Howells <dhowells@redhat.com>
Date:   Thu Jun 20 18:12:16 2019 +0100

    afs: Add some callback management tracepoints
    
    Add a couple of tracepoints to track callback management:
    
     (1) afs_cb_miss - Logs when we were unable to apply a callback, either due
         to the inode being discarded or due to a competing thread applying a
         callback first.
    
     (2) afs_cb_break - Logs when we attempted to clear the noted callback
         promise, either due to the server explicitly breaking the callback,
         the callback promise lapsing or a local event obsoleting it.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/callback.c b/fs/afs/callback.c
index 915010464572..d66cd0364243 100644
--- a/fs/afs/callback.c
+++ b/fs/afs/callback.c
@@ -212,7 +212,7 @@ void afs_init_callback_state(struct afs_server *server)
 /*
  * actually break a callback
  */
-void __afs_break_callback(struct afs_vnode *vnode)
+void __afs_break_callback(struct afs_vnode *vnode, enum afs_cb_break_reason reason)
 {
 	_enter("");
 
@@ -223,13 +223,17 @@ void __afs_break_callback(struct afs_vnode *vnode)
 
 		if (vnode->lock_state == AFS_VNODE_LOCK_WAITING_FOR_CB)
 			afs_lock_may_be_available(vnode);
+
+		trace_afs_cb_break(&vnode->fid, vnode->cb_break, reason, true);
+	} else {
+		trace_afs_cb_break(&vnode->fid, vnode->cb_break, reason, false);
 	}
 }
 
-void afs_break_callback(struct afs_vnode *vnode)
+void afs_break_callback(struct afs_vnode *vnode, enum afs_cb_break_reason reason)
 {
 	write_seqlock(&vnode->cb_lock);
-	__afs_break_callback(vnode);
+	__afs_break_callback(vnode, reason);
 	write_sequnlock(&vnode->cb_lock);
 }
 
@@ -277,6 +281,8 @@ static void afs_break_one_callback(struct afs_server *server,
 
 			write_lock(&volume->cb_v_break_lock);
 			volume->cb_v_break++;
+			trace_afs_cb_break(fid, volume->cb_v_break,
+					   afs_cb_break_for_volume_callback, false);
 			write_unlock(&volume->cb_v_break_lock);
 		} else {
 			data.volume = NULL;
@@ -285,8 +291,10 @@ static void afs_break_one_callback(struct afs_server *server,
 						afs_iget5_test, &data);
 			if (inode) {
 				vnode = AFS_FS_I(inode);
-				afs_break_callback(vnode);
+				afs_break_callback(vnode, afs_cb_break_for_callback);
 				iput(inode);
+			} else {
+				trace_afs_cb_miss(fid, afs_cb_break_for_callback);
 			}
 		}
 	}

commit 90fa9b64523a645a97edc0bdcf2d74759957eeee
Author: David Howells <dhowells@redhat.com>
Date:   Thu Jun 20 16:49:35 2019 +0100

    afs: Fix uninitialised spinlock afs_volume::cb_break_lock
    
    Fix the cb_break_lock spinlock in afs_volume struct by initialising it when
    the volume record is allocated.
    
    Also rename the lock to cb_v_break_lock to distinguish it from the lock of
    the same name in the afs_server struct.
    
    Without this, the following trace may be observed when a volume-break
    callback is received:
    
      INFO: trying to register non-static key.
      the code is fine but needs lockdep annotation.
      turning off the locking correctness validator.
      CPU: 2 PID: 50 Comm: kworker/2:1 Not tainted 5.2.0-rc1-fscache+ #3045
      Hardware name: ASUS All Series/H97-PLUS, BIOS 2306 10/09/2014
      Workqueue: afs SRXAFSCB_CallBack
      Call Trace:
       dump_stack+0x67/0x8e
       register_lock_class+0x23b/0x421
       ? check_usage_forwards+0x13c/0x13c
       __lock_acquire+0x89/0xf73
       lock_acquire+0x13b/0x166
       ? afs_break_callbacks+0x1b2/0x3dd
       _raw_write_lock+0x2c/0x36
       ? afs_break_callbacks+0x1b2/0x3dd
       afs_break_callbacks+0x1b2/0x3dd
       ? trace_event_raw_event_afs_server+0x61/0xac
       SRXAFSCB_CallBack+0x11f/0x16c
       process_one_work+0x2c5/0x4ee
       ? worker_thread+0x234/0x2ac
       worker_thread+0x1d8/0x2ac
       ? cancel_delayed_work_sync+0xf/0xf
       kthread+0x11f/0x127
       ? kthread_park+0x76/0x76
       ret_from_fork+0x24/0x30
    
    Fixes: 68251f0a6818 ("afs: Fix whole-volume callback handling")
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/callback.c b/fs/afs/callback.c
index d441bef72163..915010464572 100644
--- a/fs/afs/callback.c
+++ b/fs/afs/callback.c
@@ -275,9 +275,9 @@ static void afs_break_one_callback(struct afs_server *server,
 			struct afs_super_info *as = AFS_FS_S(cbi->sb);
 			struct afs_volume *volume = as->volume;
 
-			write_lock(&volume->cb_break_lock);
+			write_lock(&volume->cb_v_break_lock);
 			volume->cb_v_break++;
-			write_unlock(&volume->cb_break_lock);
+			write_unlock(&volume->cb_v_break_lock);
 		} else {
 			data.volume = NULL;
 			data.fid = *fid;

commit f642404a0436a50912c218009ccc7856d48d784c
Author: David Howells <dhowells@redhat.com>
Date:   Mon May 13 16:14:32 2019 +0100

    afs: Make vnode->cb_interest RCU safe
    
    Use RCU-based freeing for afs_cb_interest struct objects and use RCU on
    vnode->cb_interest.  Use that change to allow afs_check_validity() to use
    read_seqbegin_or_lock() instead of read_seqlock_excl().
    
    This also requires the caller of afs_check_validity() to hold the RCU read
    lock across the call.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/callback.c b/fs/afs/callback.c
index 4876079aa643..d441bef72163 100644
--- a/fs/afs/callback.c
+++ b/fs/afs/callback.c
@@ -94,15 +94,15 @@ int afs_register_server_cb_interest(struct afs_vnode *vnode,
 	struct afs_server *server = entry->server;
 
 again:
-	if (vnode->cb_interest &&
-	    likely(vnode->cb_interest == entry->cb_interest))
+	vcbi = rcu_dereference_protected(vnode->cb_interest,
+					 lockdep_is_held(&vnode->io_lock));
+	if (vcbi && likely(vcbi == entry->cb_interest))
 		return 0;
 
 	read_lock(&slist->lock);
 	cbi = afs_get_cb_interest(entry->cb_interest);
 	read_unlock(&slist->lock);
 
-	vcbi = vnode->cb_interest;
 	if (vcbi) {
 		if (vcbi == cbi) {
 			afs_put_cb_interest(afs_v2net(vnode), cbi);
@@ -114,8 +114,9 @@ int afs_register_server_cb_interest(struct afs_vnode *vnode,
 		 */
 		if (cbi && vcbi->server == cbi->server) {
 			write_seqlock(&vnode->cb_lock);
-			old = vnode->cb_interest;
-			vnode->cb_interest = cbi;
+			old = rcu_dereference_protected(vnode->cb_interest,
+							lockdep_is_held(&vnode->cb_lock.lock));
+			rcu_assign_pointer(vnode->cb_interest, cbi);
 			write_sequnlock(&vnode->cb_lock);
 			afs_put_cb_interest(afs_v2net(vnode), old);
 			return 0;
@@ -160,8 +161,9 @@ int afs_register_server_cb_interest(struct afs_vnode *vnode,
 	 */
 	write_seqlock(&vnode->cb_lock);
 
-	old = vnode->cb_interest;
-	vnode->cb_interest = cbi;
+	old = rcu_dereference_protected(vnode->cb_interest,
+					lockdep_is_held(&vnode->cb_lock.lock));
+	rcu_assign_pointer(vnode->cb_interest, cbi);
 	vnode->cb_s_break = cbi->server->cb_s_break;
 	vnode->cb_v_break = vnode->volume->cb_v_break;
 	clear_bit(AFS_VNODE_CB_PROMISED, &vnode->flags);
@@ -191,10 +193,11 @@ void afs_put_cb_interest(struct afs_net *net, struct afs_cb_interest *cbi)
 				vi = NULL;
 
 			write_unlock(&cbi->server->cb_break_lock);
-			kfree(vi);
+			if (vi)
+				kfree_rcu(vi, rcu);
 			afs_put_server(net, cbi->server);
 		}
-		kfree(cbi);
+		kfree_rcu(cbi, rcu);
 	}
 }
 

commit c7226e407b6065d3bda8bd9dc627663d2c505ea3
Author: David Howells <dhowells@redhat.com>
Date:   Fri May 10 23:03:31 2019 +0100

    afs: Fix lock-wait/callback-break double locking
    
    __afs_break_callback() holds vnode->lock around its call of
    afs_lock_may_be_available() - which also takes that lock.
    
    Fix this by not taking the lock in __afs_break_callback().
    
    Also, there's no point checking the granted_locks and pending_locks queues;
    it's sufficient to check lock_state, so move that check out of
    afs_lock_may_be_available() into __afs_break_callback() to replace the
    queue checks.
    
    Fixes: e8d6c554126b ("AFS: implement file locking")
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/callback.c b/fs/afs/callback.c
index 128f2dbe256a..4876079aa643 100644
--- a/fs/afs/callback.c
+++ b/fs/afs/callback.c
@@ -218,14 +218,8 @@ void __afs_break_callback(struct afs_vnode *vnode)
 		vnode->cb_break++;
 		afs_clear_permits(vnode);
 
-		spin_lock(&vnode->lock);
-
-		_debug("break callback");
-
-		if (list_empty(&vnode->granted_locks) &&
-		    !list_empty(&vnode->pending_locks))
+		if (vnode->lock_state == AFS_VNODE_LOCK_WAITING_FOR_CB)
 			afs_lock_may_be_available(vnode);
-		spin_unlock(&vnode->lock);
 	}
 }
 

commit eeba1e9cf31d064284dd1fa7bd6cfe01395bd03d
Author: David Howells <dhowells@redhat.com>
Date:   Sat Apr 13 08:37:37 2019 +0100

    afs: Fix in-progess ops to ignore server-level callback invalidation
    
    The in-kernel afs filesystem client counts the number of server-level
    callback invalidation events (CB.InitCallBackState* RPC operations) that it
    receives from the server.  This is stored in cb_s_break in various
    structures, including afs_server and afs_vnode.
    
    If an inode is examined by afs_validate(), say, the afs_server copy is
    compared, along with other break counters, to those in afs_vnode, and if
    one or more of the counters do not match, it is considered that the
    server's callback promise is broken.  At points where this happens,
    AFS_VNODE_CB_PROMISED is cleared to indicate that the status must be
    refetched from the server.
    
    afs_validate() issues an FS.FetchStatus operation to get updated metadata -
    and based on the updated data_version may invalidate the pagecache too.
    
    However, the break counters are also used to determine whether to note a
    new callback in the vnode (which would set the AFS_VNODE_CB_PROMISED flag)
    and whether to cache the permit data included in the YFSFetchStatus record
    by the server.
    
    
    The problem comes when the server sends us a CB.InitCallBackState op.  The
    first such instance doesn't cause cb_s_break to be incremented, but rather
    causes AFS_SERVER_FL_NEW to be cleared - but thereafter, say some hours
    after last use and all the volumes have been automatically unmounted and
    the server has forgotten about the client[*], this *will* likely cause an
    increment.
    
     [*] There are other circumstances too, such as the server restarting or
         needing to make space in its callback table.
    
    Note that the server won't send us a CB.InitCallBackState op until we talk
    to it again.
    
    So what happens is:
    
     (1) A mount for a new volume is attempted, a inode is created for the root
         vnode and vnode->cb_s_break and AFS_VNODE_CB_PROMISED aren't set
         immediately, as we don't have a nominated server to talk to yet - and
         we may iterate through a few to find one.
    
     (2) Before the operation happens, afs_fetch_status(), say, notes in the
         cursor (fc.cb_break) the break counter sum from the vnode, volume and
         server counters, but the server->cb_s_break is currently 0.
    
     (3) We send FS.FetchStatus to the server.  The server sends us back
         CB.InitCallBackState.  We increment server->cb_s_break.
    
     (4) Our FS.FetchStatus completes.  The reply includes a callback record.
    
     (5) xdr_decode_AFSCallBack()/xdr_decode_YFSCallBack() check to see whether
         the callback promise was broken by checking the break counter sum from
         step (2) against the current sum.
    
         This fails because of step (3), so we don't set the callback record
         and, importantly, don't set AFS_VNODE_CB_PROMISED on the vnode.
    
    This does not preclude the syscall from progressing, and we don't loop here
    rechecking the status, but rather assume it's good enough for one round
    only and will need to be rechecked next time.
    
     (6) afs_validate() it triggered on the vnode, probably called from
         d_revalidate() checking the parent directory.
    
     (7) afs_validate() notes that AFS_VNODE_CB_PROMISED isn't set, so doesn't
         update vnode->cb_s_break and assumes the vnode to be invalid.
    
     (8) afs_validate() needs to calls afs_fetch_status().  Go back to step (2)
         and repeat, every time the vnode is validated.
    
    This primarily affects volume root dir vnodes.  Everything subsequent to
    those inherit an already incremented cb_s_break upon mounting.
    
    
    The issue is that we assume that the callback record and the cached permit
    information in a reply from the server can't be trusted after getting a
    server break - but this is wrong since the server makes sure things are
    done in the right order, holding up our ops if necessary[*].
    
     [*] There is an extremely unlikely scenario where a reply from before the
         CB.InitCallBackState could get its delivery deferred till after - at
         which point we think we have a promise when we don't.  This, however,
         requires unlucky mass packet loss to one call.
    
    AFS_SERVER_FL_NEW tries to paper over the cracks for the initial mount from
    a server we've never contacted before, but this should be unnecessary.
    It's also further insulated from the problem on an initial mount by
    querying the server first with FS.GetCapabilities, which triggers the
    CB.InitCallBackState.
    
    
    Fix this by
    
     (1) Remove AFS_SERVER_FL_NEW.
    
     (2) In afs_calc_vnode_cb_break(), don't include cb_s_break in the
         calculation.
    
     (3) In afs_cb_is_broken(), don't include cb_s_break in the check.
    
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/callback.c b/fs/afs/callback.c
index 1c7955f5cdaf..128f2dbe256a 100644
--- a/fs/afs/callback.c
+++ b/fs/afs/callback.c
@@ -203,8 +203,7 @@ void afs_put_cb_interest(struct afs_net *net, struct afs_cb_interest *cbi)
  */
 void afs_init_callback_state(struct afs_server *server)
 {
-	if (!test_and_clear_bit(AFS_SERVER_FL_NEW, &server->flags))
-		server->cb_s_break++;
+	server->cb_s_break++;
 }
 
 /*

commit 30062bd13e3659a309d249a06d5f4ebb4a5c5251
Author: David Howells <dhowells@redhat.com>
Date:   Sat Oct 20 00:57:58 2018 +0100

    afs: Implement YFS support in the fs client
    
    Implement support for talking to YFS-variant fileservers in the cache
    manager and the filesystem client.  These implement upgraded services on
    the same port as their AFS services.
    
    YFS fileservers provide expanded capabilities over AFS.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/callback.c b/fs/afs/callback.c
index df9bfee698ad..1c7955f5cdaf 100644
--- a/fs/afs/callback.c
+++ b/fs/afs/callback.c
@@ -210,12 +210,10 @@ void afs_init_callback_state(struct afs_server *server)
 /*
  * actually break a callback
  */
-void afs_break_callback(struct afs_vnode *vnode)
+void __afs_break_callback(struct afs_vnode *vnode)
 {
 	_enter("");
 
-	write_seqlock(&vnode->cb_lock);
-
 	clear_bit(AFS_VNODE_NEW_CONTENT, &vnode->flags);
 	if (test_and_clear_bit(AFS_VNODE_CB_PROMISED, &vnode->flags)) {
 		vnode->cb_break++;
@@ -230,7 +228,12 @@ void afs_break_callback(struct afs_vnode *vnode)
 			afs_lock_may_be_available(vnode);
 		spin_unlock(&vnode->lock);
 	}
+}
 
+void afs_break_callback(struct afs_vnode *vnode)
+{
+	write_seqlock(&vnode->cb_lock);
+	__afs_break_callback(vnode);
 	write_sequnlock(&vnode->cb_lock);
 }
 

commit 06aeb2971457b33c1123af9f307a55f3dc4052c9
Author: David Howells <dhowells@redhat.com>
Date:   Sat Oct 20 00:57:58 2018 +0100

    afs: Remove callback details from afs_callback_break struct
    
    Remove unnecessary details of a broken callback, such as version, expiry
    and type, from the afs_callback_break struct as they're not actually used
    and make the list take more memory.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/callback.c b/fs/afs/callback.c
index 8698198ad427..df9bfee698ad 100644
--- a/fs/afs/callback.c
+++ b/fs/afs/callback.c
@@ -310,14 +310,10 @@ void afs_break_callbacks(struct afs_server *server, size_t count,
 	/* TODO: Sort the callback break list by volume ID */
 
 	for (; count > 0; callbacks++, count--) {
-		_debug("- Fid { vl=%08llx n=%llu u=%u }  CB { v=%u x=%u t=%u }",
+		_debug("- Fid { vl=%08llx n=%llu u=%u }",
 		       callbacks->fid.vid,
 		       callbacks->fid.vnode,
-		       callbacks->fid.unique,
-		       callbacks->cb.version,
-		       callbacks->cb.expiry,
-		       callbacks->cb.type
-		       );
+		       callbacks->fid.unique);
 		afs_break_one_callback(server, &callbacks->fid);
 	}
 

commit 3b6492df4153b8550d347dfc581856138678a231
Author: David Howells <dhowells@redhat.com>
Date:   Sat Oct 20 00:57:57 2018 +0100

    afs: Increase to 64-bit volume ID and 96-bit vnode ID for YFS
    
    Increase the sizes of the volume ID to 64 bits and the vnode ID (inode
    number equivalent) to 96 bits to allow the support of YFS.
    
    This requires the iget comparator to check the vnode->fid rather than i_ino
    and i_generation as i_ino is not sufficiently capacious.  It also requires
    this data to be placed into the vnode cache key for fscache.
    
    For the moment, just discard the top 32 bits of the vnode ID when returning
    it though stat.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/callback.c b/fs/afs/callback.c
index 5f261fbf2182..8698198ad427 100644
--- a/fs/afs/callback.c
+++ b/fs/afs/callback.c
@@ -310,7 +310,7 @@ void afs_break_callbacks(struct afs_server *server, size_t count,
 	/* TODO: Sort the callback break list by volume ID */
 
 	for (; count > 0; callbacks++, count--) {
-		_debug("- Fid { vl=%08x n=%u u=%u }  CB { v=%u x=%u t=%u }",
+		_debug("- Fid { vl=%08llx n=%llu u=%u }  CB { v=%u x=%u t=%u }",
 		       callbacks->fid.vid,
 		       callbacks->fid.vnode,
 		       callbacks->fid.unique,

commit 47ea0f2ebffd400d36ab5946ec8d6d6e08a67d53
Author: David Howells <dhowells@redhat.com>
Date:   Fri Jun 15 15:24:50 2018 +0100

    afs: Optimise callback breaking by not repeating volume lookup
    
    At the moment, afs_break_callbacks calls afs_break_one_callback() for each
    separate FID it was given, and the latter looks up the volume individually
    for each one.
    
    However, this is inefficient if two or more FIDs have the same vid as we
    could reuse the volume.  This is complicated by cell aliasing whereby we
    may have multiple cells sharing a volume and can therefore have multiple
    callback interests for any particular volume ID.
    
    At the moment afs_break_one_callback() scans the entire list of volumes
    we're getting from a server and breaks the appropriate callback in every
    matching volume, regardless of cell.  This scan is done for every FID.
    
    Optimise callback breaking by the following means:
    
     (1) Sort the FID list by vid so that all FIDs belonging to the same volume
         are clumped together.
    
         This is done through the use of an indirection table as we cannot do
         an insertion sort on the afs_callback_break array as we decode FIDs
         into it as we subsequently also have to decode callback info into it
         that corresponds by array index only.
    
         We also don't really want to bubblesort afterwards if we can avoid it.
    
     (2) Sort the server->cb_interests array by vid so that all the matching
         volumes are grouped together.  This permits the scan to stop after
         finding a record that has a higher vid.
    
     (3) When breaking FIDs, we try to keep server->cb_break_lock as long as
         possible, caching the start point in the array for that volume group
         as long as possible.
    
         It might make sense to add another layer in that list and have a
         refcounted volume ID anchor that has the matching interests attached
         to it rather than being in the list.  This would allow the lock to be
         dropped without losing the cursor.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/callback.c b/fs/afs/callback.c
index 571437dcb252..5f261fbf2182 100644
--- a/fs/afs/callback.c
+++ b/fs/afs/callback.c
@@ -20,6 +20,66 @@
 #include <linux/sched.h>
 #include "internal.h"
 
+/*
+ * Create volume and callback interests on a server.
+ */
+static struct afs_cb_interest *afs_create_interest(struct afs_server *server,
+						   struct afs_vnode *vnode)
+{
+	struct afs_vol_interest *new_vi, *vi;
+	struct afs_cb_interest *new;
+	struct hlist_node **pp;
+
+	new_vi = kzalloc(sizeof(struct afs_vol_interest), GFP_KERNEL);
+	if (!new_vi)
+		return NULL;
+
+	new = kzalloc(sizeof(struct afs_cb_interest), GFP_KERNEL);
+	if (!new) {
+		kfree(new_vi);
+		return NULL;
+	}
+
+	new_vi->usage = 1;
+	new_vi->vid = vnode->volume->vid;
+	INIT_HLIST_NODE(&new_vi->srv_link);
+	INIT_HLIST_HEAD(&new_vi->cb_interests);
+
+	refcount_set(&new->usage, 1);
+	new->sb = vnode->vfs_inode.i_sb;
+	new->vid = vnode->volume->vid;
+	new->server = afs_get_server(server);
+	INIT_HLIST_NODE(&new->cb_vlink);
+
+	write_lock(&server->cb_break_lock);
+
+	for (pp = &server->cb_volumes.first; *pp; pp = &(*pp)->next) {
+		vi = hlist_entry(*pp, struct afs_vol_interest, srv_link);
+		if (vi->vid < new_vi->vid)
+			continue;
+		if (vi->vid > new_vi->vid)
+			break;
+		vi->usage++;
+		goto found_vi;
+	}
+
+	new_vi->srv_link.pprev = pp;
+	new_vi->srv_link.next = *pp;
+	if (*pp)
+		(*pp)->pprev = &new_vi->srv_link.next;
+	*pp = &new_vi->srv_link;
+	vi = new_vi;
+	new_vi = NULL;
+found_vi:
+
+	new->vol_interest = vi;
+	hlist_add_head(&new->cb_vlink, &vi->cb_interests);
+
+	write_unlock(&server->cb_break_lock);
+	kfree(new_vi);
+	return new;
+}
+
 /*
  * Set up an interest-in-callbacks record for a volume on a server and
  * register it with the server.
@@ -77,20 +137,10 @@ int afs_register_server_cb_interest(struct afs_vnode *vnode,
 	}
 
 	if (!cbi) {
-		new = kzalloc(sizeof(struct afs_cb_interest), GFP_KERNEL);
+		new = afs_create_interest(server, vnode);
 		if (!new)
 			return -ENOMEM;
 
-		refcount_set(&new->usage, 1);
-		new->sb = vnode->vfs_inode.i_sb;
-		new->vid = vnode->volume->vid;
-		new->server = afs_get_server(server);
-		INIT_LIST_HEAD(&new->cb_link);
-
-		write_lock(&server->cb_break_lock);
-		list_add_tail(&new->cb_link, &server->cb_interests);
-		write_unlock(&server->cb_break_lock);
-
 		write_lock(&slist->lock);
 		if (!entry->cb_interest) {
 			entry->cb_interest = afs_get_cb_interest(new);
@@ -126,11 +176,22 @@ int afs_register_server_cb_interest(struct afs_vnode *vnode,
  */
 void afs_put_cb_interest(struct afs_net *net, struct afs_cb_interest *cbi)
 {
+	struct afs_vol_interest *vi;
+
 	if (cbi && refcount_dec_and_test(&cbi->usage)) {
-		if (!list_empty(&cbi->cb_link)) {
+		if (!hlist_unhashed(&cbi->cb_vlink)) {
 			write_lock(&cbi->server->cb_break_lock);
-			list_del_init(&cbi->cb_link);
+
+			hlist_del_init(&cbi->cb_vlink);
+			vi = cbi->vol_interest;
+			cbi->vol_interest = NULL;
+			if (--vi->usage == 0)
+				hlist_del(&vi->srv_link);
+			else
+				vi = NULL;
+
 			write_unlock(&cbi->server->cb_break_lock);
+			kfree(vi);
 			afs_put_server(net, cbi->server);
 		}
 		kfree(cbi);
@@ -182,20 +243,34 @@ void afs_break_callback(struct afs_vnode *vnode)
 static void afs_break_one_callback(struct afs_server *server,
 				   struct afs_fid *fid)
 {
+	struct afs_vol_interest *vi;
 	struct afs_cb_interest *cbi;
 	struct afs_iget_data data;
 	struct afs_vnode *vnode;
 	struct inode *inode;
 
 	read_lock(&server->cb_break_lock);
+	hlist_for_each_entry(vi, &server->cb_volumes, srv_link) {
+		if (vi->vid < fid->vid)
+			continue;
+		if (vi->vid > fid->vid) {
+			vi = NULL;
+			break;
+		}
+		//atomic_inc(&vi->usage);
+		break;
+	}
+
+	/* TODO: Find all matching volumes if we couldn't match the server and
+	 * break them anyway.
+	 */
+	if (!vi)
+		goto out;
 
 	/* Step through all interested superblocks.  There may be more than one
 	 * because of cell aliasing.
 	 */
-	list_for_each_entry(cbi, &server->cb_interests, cb_link) {
-		if (cbi->vid != fid->vid)
-			continue;
-
+	hlist_for_each_entry(cbi, &vi->cb_interests, cb_vlink) {
 		if (fid->vnode == 0 && fid->unique == 0) {
 			/* The callback break applies to an entire volume. */
 			struct afs_super_info *as = AFS_FS_S(cbi->sb);
@@ -217,6 +292,7 @@ static void afs_break_one_callback(struct afs_server *server,
 		}
 	}
 
+out:
 	read_unlock(&server->cb_break_lock);
 }
 

commit 68251f0a6818f3be19b1471f36c956ca97c1427d
Author: David Howells <dhowells@redhat.com>
Date:   Sat May 12 22:31:33 2018 +0100

    afs: Fix whole-volume callback handling
    
    It's possible for an AFS file server to issue a whole-volume notification
    that callbacks on all the vnodes in the file have been broken.  This is
    done for R/O and backup volumes (which don't have per-file callbacks) and
    for things like a volume being taken offline.
    
    Fix callback handling to detect whole-volume notifications, to track it
    across operations and to check it during inode validation.
    
    Fixes: c435ee34551e ("afs: Overhaul the callback handling")
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/callback.c b/fs/afs/callback.c
index 09332945d322..571437dcb252 100644
--- a/fs/afs/callback.c
+++ b/fs/afs/callback.c
@@ -113,6 +113,7 @@ int afs_register_server_cb_interest(struct afs_vnode *vnode,
 	old = vnode->cb_interest;
 	vnode->cb_interest = cbi;
 	vnode->cb_s_break = cbi->server->cb_s_break;
+	vnode->cb_v_break = vnode->volume->cb_v_break;
 	clear_bit(AFS_VNODE_CB_PROMISED, &vnode->flags);
 
 	write_sequnlock(&vnode->cb_lock);
@@ -195,13 +196,24 @@ static void afs_break_one_callback(struct afs_server *server,
 		if (cbi->vid != fid->vid)
 			continue;
 
-		data.volume = NULL;
-		data.fid = *fid;
-		inode = ilookup5_nowait(cbi->sb, fid->vnode, afs_iget5_test, &data);
-		if (inode) {
-			vnode = AFS_FS_I(inode);
-			afs_break_callback(vnode);
-			iput(inode);
+		if (fid->vnode == 0 && fid->unique == 0) {
+			/* The callback break applies to an entire volume. */
+			struct afs_super_info *as = AFS_FS_S(cbi->sb);
+			struct afs_volume *volume = as->volume;
+
+			write_lock(&volume->cb_break_lock);
+			volume->cb_v_break++;
+			write_unlock(&volume->cb_break_lock);
+		} else {
+			data.volume = NULL;
+			data.fid = *fid;
+			inode = ilookup5_nowait(cbi->sb, fid->vnode,
+						afs_iget5_test, &data);
+			if (inode) {
+				vnode = AFS_FS_I(inode);
+				afs_break_callback(vnode);
+				iput(inode);
+			}
 		}
 	}
 
@@ -219,6 +231,8 @@ void afs_break_callbacks(struct afs_server *server, size_t count,
 	ASSERT(server != NULL);
 	ASSERTCMP(count, <=, AFSCBMAX);
 
+	/* TODO: Sort the callback break list by volume ID */
+
 	for (; count > 0; callbacks++, count--) {
 		_debug("- Fid { vl=%08x n=%u u=%u }  CB { v=%u x=%u t=%u }",
 		       callbacks->fid.vid,

commit d4a96bec7a7362834ef5c31d7b2cc9bf36eb0570
Author: David Howells <dhowells@redhat.com>
Date:   Thu May 10 08:43:04 2018 +0100

    afs: Fix refcounting in callback registration
    
    The refcounting on afs_cb_interest struct objects in
    afs_register_server_cb_interest() is wrong as it uses the server list
    entry's call back interest pointer without regard for the fact that it
    might be replaced at any time and the object thrown away.
    
    Fix this by:
    
     (1) Put a lock on the afs_server_list struct that can be used to
         mediate access to the callback interest pointers in the servers array.
    
     (2) Keep a ref on the callback interest that we get from the entry.
    
     (3) Dropping the old reference held by vnode->cb_interest if we replace
         the pointer.
    
    Fixes: c435ee34551e ("afs: Overhaul the callback handling")
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/callback.c b/fs/afs/callback.c
index abd9a84f4e88..09332945d322 100644
--- a/fs/afs/callback.c
+++ b/fs/afs/callback.c
@@ -23,36 +23,55 @@
 /*
  * Set up an interest-in-callbacks record for a volume on a server and
  * register it with the server.
- * - Called with volume->server_sem held.
+ * - Called with vnode->io_lock held.
  */
 int afs_register_server_cb_interest(struct afs_vnode *vnode,
-				    struct afs_server_entry *entry)
+				    struct afs_server_list *slist,
+				    unsigned int index)
 {
-	struct afs_cb_interest *cbi = entry->cb_interest, *vcbi, *new, *x;
+	struct afs_server_entry *entry = &slist->servers[index];
+	struct afs_cb_interest *cbi, *vcbi, *new, *old;
 	struct afs_server *server = entry->server;
 
 again:
+	if (vnode->cb_interest &&
+	    likely(vnode->cb_interest == entry->cb_interest))
+		return 0;
+
+	read_lock(&slist->lock);
+	cbi = afs_get_cb_interest(entry->cb_interest);
+	read_unlock(&slist->lock);
+
 	vcbi = vnode->cb_interest;
 	if (vcbi) {
-		if (vcbi == cbi)
+		if (vcbi == cbi) {
+			afs_put_cb_interest(afs_v2net(vnode), cbi);
 			return 0;
+		}
 
+		/* Use a new interest in the server list for the same server
+		 * rather than an old one that's still attached to a vnode.
+		 */
 		if (cbi && vcbi->server == cbi->server) {
 			write_seqlock(&vnode->cb_lock);
-			vnode->cb_interest = afs_get_cb_interest(cbi);
+			old = vnode->cb_interest;
+			vnode->cb_interest = cbi;
 			write_sequnlock(&vnode->cb_lock);
-			afs_put_cb_interest(afs_v2net(vnode), cbi);
+			afs_put_cb_interest(afs_v2net(vnode), old);
 			return 0;
 		}
 
+		/* Re-use the one attached to the vnode. */
 		if (!cbi && vcbi->server == server) {
-			afs_get_cb_interest(vcbi);
-			x = cmpxchg(&entry->cb_interest, cbi, vcbi);
-			if (x != cbi) {
-				cbi = x;
-				afs_put_cb_interest(afs_v2net(vnode), vcbi);
+			write_lock(&slist->lock);
+			if (entry->cb_interest) {
+				write_unlock(&slist->lock);
+				afs_put_cb_interest(afs_v2net(vnode), cbi);
 				goto again;
 			}
+
+			entry->cb_interest = cbi;
+			write_unlock(&slist->lock);
 			return 0;
 		}
 	}
@@ -72,13 +91,16 @@ int afs_register_server_cb_interest(struct afs_vnode *vnode,
 		list_add_tail(&new->cb_link, &server->cb_interests);
 		write_unlock(&server->cb_break_lock);
 
-		x = cmpxchg(&entry->cb_interest, cbi, new);
-		if (x == cbi) {
+		write_lock(&slist->lock);
+		if (!entry->cb_interest) {
+			entry->cb_interest = afs_get_cb_interest(new);
 			cbi = new;
+			new = NULL;
 		} else {
-			cbi = x;
-			afs_put_cb_interest(afs_v2net(vnode), new);
+			cbi = afs_get_cb_interest(entry->cb_interest);
 		}
+		write_unlock(&slist->lock);
+		afs_put_cb_interest(afs_v2net(vnode), new);
 	}
 
 	ASSERT(cbi);
@@ -88,11 +110,13 @@ int afs_register_server_cb_interest(struct afs_vnode *vnode,
 	 */
 	write_seqlock(&vnode->cb_lock);
 
-	vnode->cb_interest = afs_get_cb_interest(cbi);
+	old = vnode->cb_interest;
+	vnode->cb_interest = cbi;
 	vnode->cb_s_break = cbi->server->cb_s_break;
 	clear_bit(AFS_VNODE_CB_PROMISED, &vnode->flags);
 
 	write_sequnlock(&vnode->cb_lock);
+	afs_put_cb_interest(afs_v2net(vnode), old);
 	return 0;
 }
 

commit 5a8132761609bd7e42db642d6f157140d5bf2ae8
Author: David Howells <dhowells@redhat.com>
Date:   Fri Apr 6 14:17:26 2018 +0100

    afs: Do better accretion of small writes on newly created content
    
    Processes like ld that do lots of small writes that aren't necessarily
    contiguous result in a lot of small StoreData operations to the server, the
    idea being that if someone else changes the data on the server, we only
    write our changes over that and not the space between.  Further, we don't
    want to write back empty space if we can avoid it to make it easier for the
    server to do sparse files.
    
    However, making lots of tiny RPC ops is a lot less efficient for the server
    than one big one because each op requires allocation of resources and the
    taking of locks, so we want to compromise a bit.
    
    Reduce the load by the following:
    
     (1) If a file is just created locally or has just been truncated with
         O_TRUNC locally, allow subsequent writes to the file to be merged with
         intervening space if that space doesn't cross an entire intervening
         page.
    
     (2) Don't flush the file on ->flush() but rather on ->release() if the
         file was open for writing.
    
    Just linking vmlinux.o, without this patch, looking in /proc/fs/afs/stats:
    
            file-wr : n=441 nb=513581204
    
    and after the patch:
    
            file-wr : n=62 nb=513668555
    
    there were 379 fewer StoreData RPC operations at the expense of an extra
    87K being written.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/callback.c b/fs/afs/callback.c
index 6049ca837498..abd9a84f4e88 100644
--- a/fs/afs/callback.c
+++ b/fs/afs/callback.c
@@ -130,6 +130,7 @@ void afs_break_callback(struct afs_vnode *vnode)
 
 	write_seqlock(&vnode->cb_lock);
 
+	clear_bit(AFS_VNODE_NEW_CONTENT, &vnode->flags);
 	if (test_and_clear_bit(AFS_VNODE_CB_PROMISED, &vnode->flags)) {
 		vnode->cb_break++;
 		afs_clear_permits(vnode);

commit 5cf9dd55a0ec26428f2824aadd16bfa305a5b603
Author: David Howells <dhowells@redhat.com>
Date:   Mon Apr 9 21:12:31 2018 +0100

    afs: Prospectively look up extra files when doing a single lookup
    
    When afs_lookup() is called, prospectively look up the next 50 uncached
    fids also from that same directory and cache the results, rather than just
    looking up the one file requested.
    
    This allows us to use the FS.InlineBulkStatus RPC op to increase efficiency
    by fetching up to 50 file statuses at a time.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/callback.c b/fs/afs/callback.c
index bf082c719645..6049ca837498 100644
--- a/fs/afs/callback.c
+++ b/fs/afs/callback.c
@@ -187,7 +187,7 @@ static void afs_break_one_callback(struct afs_server *server,
  * allow the fileserver to break callback promises
  */
 void afs_break_callbacks(struct afs_server *server, size_t count,
-			 struct afs_callback callbacks[])
+			 struct afs_callback_break *callbacks)
 {
 	_enter("%p,%zu,", server, count);
 
@@ -199,9 +199,9 @@ void afs_break_callbacks(struct afs_server *server, size_t count,
 		       callbacks->fid.vid,
 		       callbacks->fid.vnode,
 		       callbacks->fid.unique,
-		       callbacks->version,
-		       callbacks->expiry,
-		       callbacks->type
+		       callbacks->cb.version,
+		       callbacks->cb.expiry,
+		       callbacks->cb.type
 		       );
 		afs_break_one_callback(server, &callbacks->fid);
 	}

commit fe342cf77bc3c3ba89e8bb1e4eddbe614df4efa4
Author: David Howells <dhowells@redhat.com>
Date:   Mon Apr 9 21:12:31 2018 +0100

    afs: Fix checker warnings
    
    Fix warnings raised by checker, including:
    
     (*) Warnings raised by unequal comparison for the purposes of sorting,
         where the endianness doesn't matter:
    
    fs/afs/addr_list.c:246:21: warning: restricted __be16 degrades to integer
    fs/afs/addr_list.c:246:30: warning: restricted __be16 degrades to integer
    fs/afs/addr_list.c:248:21: warning: restricted __be32 degrades to integer
    fs/afs/addr_list.c:248:49: warning: restricted __be32 degrades to integer
    fs/afs/addr_list.c:283:21: warning: restricted __be16 degrades to integer
    fs/afs/addr_list.c:283:30: warning: restricted __be16 degrades to integer
    
     (*) afs_set_cb_interest() is not actually used and can be removed.
    
     (*) afs_cell_gc_delay() should be provided with a sysctl.
    
     (*) afs_cell_destroy() needs to use rcu_access_pointer() to read
         cell->vl_addrs.
    
     (*) afs_init_fs_cursor() should be static.
    
     (*) struct afs_vnode::permit_cache needs to be marked __rcu.
    
     (*) afs_server_rcu() needs to use rcu_access_pointer().
    
     (*) afs_destroy_server() should use rcu_access_pointer() on
         server->addresses as the server object is no longer accessible.
    
     (*) afs_find_server() casts __be16/__be32 values to int in order to
         directly compare them for the purpose of finding a match in a list,
         but is should also annotate the cast with __force to avoid checker
         warnings.
    
     (*) afs_check_permit() accesses vnode->permit_cache outside of the RCU
         readlock, though it doesn't then access the value; the extraneous
         access is deleted.
    
    False positives:
    
     (*) Conditional locking around the code in xdr_decode_AFSFetchStatus.  This
         can be dealt with in a separate patch.
    
    fs/afs/fsclient.c:148:9: warning: context imbalance in 'xdr_decode_AFSFetchStatus' - different lock contexts for basic block
    
     (*) Incorrect handling of seq-retry lock context balance:
    
    fs/afs/inode.c:455:38: warning: context imbalance in 'afs_getattr' - different
    lock contexts for basic block
    fs/afs/server.c:52:17: warning: context imbalance in 'afs_find_server' - different lock contexts for basic block
    fs/afs/server.c:128:17: warning: context imbalance in 'afs_find_server_by_uuid' - different lock contexts for basic block
    
    Errors:
    
     (*) afs_lookup_cell_rcu() needs to break out of the seq-retry loop, not go
         round again if it successfully found the workstation cell.
    
     (*) Fix UUID decode in afs_deliver_cb_probe_uuid().
    
     (*) afs_cache_permit() has a missing rcu_read_unlock() before one of the
         jumps to the someone_else_changed_it label.  Move the unlock to after
         the label.
    
     (*) afs_vl_get_addrs_u() is using ntohl() rather than htonl() when
         encoding to XDR.
    
     (*) afs_deliver_yfsvl_get_endpoints() is using htonl() rather than ntohl()
         when decoding from XDR.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/callback.c b/fs/afs/callback.c
index f4291b576054..bf082c719645 100644
--- a/fs/afs/callback.c
+++ b/fs/afs/callback.c
@@ -96,26 +96,6 @@ int afs_register_server_cb_interest(struct afs_vnode *vnode,
 	return 0;
 }
 
-/*
- * Set a vnode's interest on a server.
- */
-void afs_set_cb_interest(struct afs_vnode *vnode, struct afs_cb_interest *cbi)
-{
-	struct afs_cb_interest *old_cbi = NULL;
-
-	if (vnode->cb_interest == cbi)
-		return;
-
-	write_seqlock(&vnode->cb_lock);
-	if (vnode->cb_interest != cbi) {
-		afs_get_cb_interest(cbi);
-		old_cbi = vnode->cb_interest;
-		vnode->cb_interest = cbi;
-	}
-	write_sequnlock(&vnode->cb_lock);
-	afs_put_cb_interest(afs_v2net(vnode), cbi);
-}
-
 /*
  * Remove an interest on a server.
  */

commit d2ddc776a4581d900fc3bdc7803b403daae64d88
Author: David Howells <dhowells@redhat.com>
Date:   Thu Nov 2 15:27:50 2017 +0000

    afs: Overhaul volume and server record caching and fileserver rotation
    
    The current code assumes that volumes and servers are per-cell and are
    never shared, but this is not enforced, and, indeed, public cells do exist
    that are aliases of each other.  Further, an organisation can, say, set up
    a public cell and a private cell with overlapping, but not identical, sets
    of servers.  The difference is purely in the database attached to the VL
    servers.
    
    The current code will malfunction if it sees a server in two cells as it
    assumes global address -> server record mappings and that each server is in
    just one cell.
    
    Further, each server may have multiple addresses - and may have addresses
    of different families (IPv4 and IPv6, say).
    
    To this end, the following structural changes are made:
    
     (1) Server record management is overhauled:
    
         (a) Server records are made independent of cell.  The namespace keeps
             track of them, volume records have lists of them and each vnode
             has a server on which its callback interest currently resides.
    
         (b) The cell record no longer keeps a list of servers known to be in
             that cell.
    
         (c) The server records are now kept in a flat list because there's no
             single address to sort on.
    
         (d) Server records are now keyed by their UUID within the namespace.
    
         (e) The addresses for a server are obtained with the VL.GetAddrsU
             rather than with VL.GetEntryByName, using the server's UUID as a
             parameter.
    
         (f) Cached server records are garbage collected after a period of
             non-use and are counted out of existence before purging is allowed
             to complete.  This protects the work functions against rmmod.
    
         (g) The servers list is now in /proc/fs/afs/servers.
    
     (2) Volume record management is overhauled:
    
         (a) An RCU-replaceable server list is introduced.  This tracks both
             servers and their coresponding callback interests.
    
         (b) The superblock is now keyed on cell record and numeric volume ID.
    
         (c) The volume record is now tied to the superblock which mounts it,
             and is activated when mounted and deactivated when unmounted.
             This makes it easier to handle the cache cookie without causing a
             double-use in fscache.
    
         (d) The volume record is loaded from the VLDB using VL.GetEntryByNameU
             to get the server UUID list.
    
         (e) The volume name is updated if it is seen to have changed when the
             volume is updated (the update is keyed on the volume ID).
    
     (3) The vlocation record is got rid of and VLDB records are no longer
         cached.  Sufficient information is stored in the volume record, though
         an update to a volume record is now no longer shared between related
         volumes (volumes come in bundles of three: R/W, R/O and backup).
    
    and the following procedural changes are made:
    
     (1) The fileserver cursor introduced previously is now fleshed out and
         used to iterate over fileservers and their addresses.
    
     (2) Volume status is checked during iteration, and the server list is
         replaced if a change is detected.
    
     (3) Server status is checked during iteration, and the address list is
         replaced if a change is detected.
    
     (4) The abort code is saved into the address list cursor and -ECONNABORTED
         returned in afs_make_call() if a remote abort happened rather than
         translating the abort into an error message.  This allows actions to
         be taken depending on the abort code more easily.
    
         (a) If a VMOVED abort is seen then this is handled by rechecking the
             volume and restarting the iteration.
    
         (b) If a VBUSY, VRESTARTING or VSALVAGING abort is seen then this is
             handled by sleeping for a short period and retrying and/or trying
             other servers that might serve that volume.  A message is also
             displayed once until the condition has cleared.
    
         (c) If a VOFFLINE abort is seen, then this is handled as VBUSY for the
             moment.
    
         (d) If a VNOVOL abort is seen, the volume is rechecked in the VLDB to
             see if it has been deleted; if not, the fileserver is probably
             indicating that the volume couldn't be attached and needs
             salvaging.
    
         (e) If statfs() sees one of these aborts, it does not sleep, but
             rather returns an error, so as not to block the umount program.
    
     (5) The fileserver iteration functions in vnode.c are now merged into
         their callers and more heavily macroised around the cursor.  vnode.c
         is removed.
    
     (6) Operations on a particular vnode are serialised on that vnode because
         the server will lock that vnode whilst it operates on it, so a second
         op sent will just have to wait.
    
     (7) Fileservers are probed with FS.GetCapabilities before being used.
         This is where service upgrade will be done.
    
     (8) A callback interest on a fileserver is set up before an FS operation
         is performed and passed through to afs_make_call() so that it can be
         set on the vnode if the operation returns a callback.  The callback
         interest is passed through to afs_iget() also so that it can be set
         there too.
    
    In general, record updating is done on an as-needed basis when we try to
    access servers, volumes or vnodes rather than offloading it to work items
    and special threads.
    
    Notes:
    
     (1) Pre AFS-3.4 servers are no longer supported, though this can be added
         back if necessary (AFS-3.4 was released in 1998).
    
     (2) VBUSY is retried forever for the moment at intervals of 1s.
    
     (3) /proc/fs/afs/<cell>/servers no longer exists.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/callback.c b/fs/afs/callback.c
index 82f4c7a3b7b6..f4291b576054 100644
--- a/fs/afs/callback.c
+++ b/fs/afs/callback.c
@@ -26,10 +26,10 @@
  * - Called with volume->server_sem held.
  */
 int afs_register_server_cb_interest(struct afs_vnode *vnode,
-				    struct afs_cb_interest **ppcbi,
-				    struct afs_server *server)
+				    struct afs_server_entry *entry)
 {
-	struct afs_cb_interest *cbi = *ppcbi, *vcbi, *new, *x;
+	struct afs_cb_interest *cbi = entry->cb_interest, *vcbi, *new, *x;
+	struct afs_server *server = entry->server;
 
 again:
 	vcbi = vnode->cb_interest;
@@ -47,7 +47,7 @@ int afs_register_server_cb_interest(struct afs_vnode *vnode,
 
 		if (!cbi && vcbi->server == server) {
 			afs_get_cb_interest(vcbi);
-			x = cmpxchg(ppcbi, cbi, vcbi);
+			x = cmpxchg(&entry->cb_interest, cbi, vcbi);
 			if (x != cbi) {
 				cbi = x;
 				afs_put_cb_interest(afs_v2net(vnode), vcbi);
@@ -72,7 +72,7 @@ int afs_register_server_cb_interest(struct afs_vnode *vnode,
 		list_add_tail(&new->cb_link, &server->cb_interests);
 		write_unlock(&server->cb_break_lock);
 
-		x = cmpxchg(ppcbi, cbi, new);
+		x = cmpxchg(&entry->cb_interest, cbi, new);
 		if (x == cbi) {
 			cbi = new;
 		} else {
@@ -137,7 +137,7 @@ void afs_put_cb_interest(struct afs_net *net, struct afs_cb_interest *cbi)
  */
 void afs_init_callback_state(struct afs_server *server)
 {
-	if (!test_and_clear_bit(AFS_SERVER_NEW, &server->flags))
+	if (!test_and_clear_bit(AFS_SERVER_FL_NEW, &server->flags))
 		server->cb_s_break++;
 }
 
@@ -233,12 +233,12 @@ void afs_break_callbacks(struct afs_server *server, size_t count,
 /*
  * Clear the callback interests in a server list.
  */
-void afs_clear_callback_interests(struct afs_net *net, struct afs_volume *volume)
+void afs_clear_callback_interests(struct afs_net *net, struct afs_server_list *slist)
 {
 	int i;
 
-	for (i = 0; i < ARRAY_SIZE(volume->cb_interests); i++) {
-		afs_put_cb_interest(net, volume->cb_interests[i]);
-		volume->cb_interests[i] = NULL;
+	for (i = 0; i < slist->nr_servers; i++) {
+		afs_put_cb_interest(net, slist->servers[i].cb_interest);
+		slist->servers[i].cb_interest = NULL;
 	}
 }

commit c435ee34551e1f5a02a253ca8e235287efd2727c
Author: David Howells <dhowells@redhat.com>
Date:   Thu Nov 2 15:27:49 2017 +0000

    afs: Overhaul the callback handling
    
    Overhaul the AFS callback handling by the following means:
    
     (1) Don't give up callback promises on vnodes that we are no longer using,
         rather let them just expire on the server or let the server break
         them.  This is actually more efficient for the server as the callback
         lookup is expensive if there are lots of extant callbacks.
    
     (2) Only give up the callback promises we have from a server when the
         server record is destroyed.  Then we can just give up *all* the
         callback promises on it in one go.
    
     (3) Servers can end up being shared between cells if cells are aliased, so
         don't add all the vnodes being backed by a particular server into a
         big FID-indexed tree on that server as there may be duplicates.
    
         Instead have each volume instance (~= superblock) register an interest
         in a server as it starts to make use of it and use this to allow the
         processor for callbacks from the server to find the superblock and
         thence the inode corresponding to the FID being broken by means of
         ilookup_nowait().
    
     (4) Rather than iterating over the entire callback list when a mass-break
         comes in from the server, maintain a counter of mass-breaks in
         afs_server (cb_seq) and make afs_validate() check it against the copy
         in afs_vnode.
    
         It would be nice not to have to take a read_lock whilst doing this,
         but that's tricky without using RCU.
    
     (5) Save a ref on the fileserver we're using for a call in the afs_call
         struct so that we can access its cb_s_break during call decoding.
    
     (6) Write-lock around callback and status storage in a vnode and read-lock
         around getattr so that we don't see the status mid-update.
    
    This has the following consequences:
    
     (1) Data invalidation isn't seen until someone calls afs_validate() on a
         vnode.  Unfortunately, we need to use a key to query the server, but
         getting one from a background thread is tricky without caching loads
         of keys all over the place.
    
     (2) Mass invalidation isn't seen until someone calls afs_validate().
    
     (3) Callback breaking is going to hit the inode_hash_lock quite a bit.
         Could this be replaced with rcu_read_lock() since inodes are destroyed
         under RCU conditions.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/callback.c b/fs/afs/callback.c
index d12dffb76b67..82f4c7a3b7b6 100644
--- a/fs/afs/callback.c
+++ b/fs/afs/callback.c
@@ -20,116 +20,151 @@
 #include <linux/sched.h>
 #include "internal.h"
 
-#if 0
-unsigned afs_vnode_update_timeout = 10;
-#endif  /*  0  */
-
-#define afs_breakring_space(server) \
-	CIRC_SPACE((server)->cb_break_head, (server)->cb_break_tail,	\
-		   ARRAY_SIZE((server)->cb_break))
-
-struct workqueue_struct *afs_callback_update_worker;
-
 /*
- * allow the fileserver to request callback state (re-)initialisation
+ * Set up an interest-in-callbacks record for a volume on a server and
+ * register it with the server.
+ * - Called with volume->server_sem held.
  */
-void afs_init_callback_state(struct afs_server *server)
+int afs_register_server_cb_interest(struct afs_vnode *vnode,
+				    struct afs_cb_interest **ppcbi,
+				    struct afs_server *server)
 {
-	struct afs_vnode *vnode;
-
-	_enter("{%p}", server);
+	struct afs_cb_interest *cbi = *ppcbi, *vcbi, *new, *x;
+
+again:
+	vcbi = vnode->cb_interest;
+	if (vcbi) {
+		if (vcbi == cbi)
+			return 0;
+
+		if (cbi && vcbi->server == cbi->server) {
+			write_seqlock(&vnode->cb_lock);
+			vnode->cb_interest = afs_get_cb_interest(cbi);
+			write_sequnlock(&vnode->cb_lock);
+			afs_put_cb_interest(afs_v2net(vnode), cbi);
+			return 0;
+		}
 
-	spin_lock(&server->cb_lock);
+		if (!cbi && vcbi->server == server) {
+			afs_get_cb_interest(vcbi);
+			x = cmpxchg(ppcbi, cbi, vcbi);
+			if (x != cbi) {
+				cbi = x;
+				afs_put_cb_interest(afs_v2net(vnode), vcbi);
+				goto again;
+			}
+			return 0;
+		}
+	}
 
-	/* kill all the promises on record from this server */
-	while (!RB_EMPTY_ROOT(&server->cb_promises)) {
-		vnode = rb_entry(server->cb_promises.rb_node,
-				 struct afs_vnode, cb_promise);
-		_debug("UNPROMISE { vid=%x:%u uq=%u}",
-		       vnode->fid.vid, vnode->fid.vnode, vnode->fid.unique);
-		rb_erase(&vnode->cb_promise, &server->cb_promises);
-		vnode->cb_promised = false;
+	if (!cbi) {
+		new = kzalloc(sizeof(struct afs_cb_interest), GFP_KERNEL);
+		if (!new)
+			return -ENOMEM;
+
+		refcount_set(&new->usage, 1);
+		new->sb = vnode->vfs_inode.i_sb;
+		new->vid = vnode->volume->vid;
+		new->server = afs_get_server(server);
+		INIT_LIST_HEAD(&new->cb_link);
+
+		write_lock(&server->cb_break_lock);
+		list_add_tail(&new->cb_link, &server->cb_interests);
+		write_unlock(&server->cb_break_lock);
+
+		x = cmpxchg(ppcbi, cbi, new);
+		if (x == cbi) {
+			cbi = new;
+		} else {
+			cbi = x;
+			afs_put_cb_interest(afs_v2net(vnode), new);
+		}
 	}
 
-	spin_unlock(&server->cb_lock);
-	_leave("");
+	ASSERT(cbi);
+
+	/* Change the server the vnode is using.  This entails scrubbing any
+	 * interest the vnode had in the previous server it was using.
+	 */
+	write_seqlock(&vnode->cb_lock);
+
+	vnode->cb_interest = afs_get_cb_interest(cbi);
+	vnode->cb_s_break = cbi->server->cb_s_break;
+	clear_bit(AFS_VNODE_CB_PROMISED, &vnode->flags);
+
+	write_sequnlock(&vnode->cb_lock);
+	return 0;
 }
 
 /*
- * handle the data invalidation side of a callback being broken
+ * Set a vnode's interest on a server.
  */
-void afs_broken_callback_work(struct work_struct *work)
+void afs_set_cb_interest(struct afs_vnode *vnode, struct afs_cb_interest *cbi)
 {
-	struct afs_vnode *vnode =
-		container_of(work, struct afs_vnode, cb_broken_work);
-
-	_enter("");
+	struct afs_cb_interest *old_cbi = NULL;
 
-	if (test_bit(AFS_VNODE_DELETED, &vnode->flags))
+	if (vnode->cb_interest == cbi)
 		return;
 
-	/* we're only interested in dealing with a broken callback on *this*
-	 * vnode and only if no-one else has dealt with it yet */
-	if (!mutex_trylock(&vnode->validate_lock))
-		return; /* someone else is dealing with it */
-
-	if (test_bit(AFS_VNODE_CB_BROKEN, &vnode->flags)) {
-		if (S_ISDIR(vnode->vfs_inode.i_mode))
-			afs_clear_permits(vnode);
-
-		if (afs_vnode_fetch_status(vnode, NULL, NULL) < 0)
-			goto out;
-
-		if (test_bit(AFS_VNODE_DELETED, &vnode->flags))
-			goto out;
-
-		/* if the vnode's data version number changed then its contents
-		 * are different */
-		if (test_and_clear_bit(AFS_VNODE_ZAP_DATA, &vnode->flags))
-			afs_zap_data(vnode);
+	write_seqlock(&vnode->cb_lock);
+	if (vnode->cb_interest != cbi) {
+		afs_get_cb_interest(cbi);
+		old_cbi = vnode->cb_interest;
+		vnode->cb_interest = cbi;
 	}
+	write_sequnlock(&vnode->cb_lock);
+	afs_put_cb_interest(afs_v2net(vnode), cbi);
+}
 
-out:
-	mutex_unlock(&vnode->validate_lock);
-
-	/* avoid the potential race whereby the mutex_trylock() in this
-	 * function happens again between the clear_bit() and the
-	 * mutex_unlock() */
-	if (test_bit(AFS_VNODE_CB_BROKEN, &vnode->flags)) {
-		_debug("requeue");
-		queue_work(afs_callback_update_worker, &vnode->cb_broken_work);
+/*
+ * Remove an interest on a server.
+ */
+void afs_put_cb_interest(struct afs_net *net, struct afs_cb_interest *cbi)
+{
+	if (cbi && refcount_dec_and_test(&cbi->usage)) {
+		if (!list_empty(&cbi->cb_link)) {
+			write_lock(&cbi->server->cb_break_lock);
+			list_del_init(&cbi->cb_link);
+			write_unlock(&cbi->server->cb_break_lock);
+			afs_put_server(net, cbi->server);
+		}
+		kfree(cbi);
 	}
-	_leave("");
+}
+
+/*
+ * allow the fileserver to request callback state (re-)initialisation
+ */
+void afs_init_callback_state(struct afs_server *server)
+{
+	if (!test_and_clear_bit(AFS_SERVER_NEW, &server->flags))
+		server->cb_s_break++;
 }
 
 /*
  * actually break a callback
  */
-static void afs_break_callback(struct afs_server *server,
-			       struct afs_vnode *vnode)
+void afs_break_callback(struct afs_vnode *vnode)
 {
 	_enter("");
 
-	set_bit(AFS_VNODE_CB_BROKEN, &vnode->flags);
+	write_seqlock(&vnode->cb_lock);
+
+	if (test_and_clear_bit(AFS_VNODE_CB_PROMISED, &vnode->flags)) {
+		vnode->cb_break++;
+		afs_clear_permits(vnode);
 
-	if (vnode->cb_promised) {
 		spin_lock(&vnode->lock);
 
 		_debug("break callback");
 
-		spin_lock(&server->cb_lock);
-		if (vnode->cb_promised) {
-			rb_erase(&vnode->cb_promise, &server->cb_promises);
-			vnode->cb_promised = false;
-		}
-		spin_unlock(&server->cb_lock);
-
-		queue_work(afs_callback_update_worker, &vnode->cb_broken_work);
 		if (list_empty(&vnode->granted_locks) &&
 		    !list_empty(&vnode->pending_locks))
 			afs_lock_may_be_available(vnode);
 		spin_unlock(&vnode->lock);
 	}
+
+	write_sequnlock(&vnode->cb_lock);
 }
 
 /*
@@ -141,49 +176,31 @@ static void afs_break_callback(struct afs_server *server,
 static void afs_break_one_callback(struct afs_server *server,
 				   struct afs_fid *fid)
 {
+	struct afs_cb_interest *cbi;
+	struct afs_iget_data data;
 	struct afs_vnode *vnode;
-	struct rb_node *p;
-
-	_debug("find");
-	spin_lock(&server->fs_lock);
-	p = server->fs_vnodes.rb_node;
-	while (p) {
-		vnode = rb_entry(p, struct afs_vnode, server_rb);
-		if (fid->vid < vnode->fid.vid)
-			p = p->rb_left;
-		else if (fid->vid > vnode->fid.vid)
-			p = p->rb_right;
-		else if (fid->vnode < vnode->fid.vnode)
-			p = p->rb_left;
-		else if (fid->vnode > vnode->fid.vnode)
-			p = p->rb_right;
-		else if (fid->unique < vnode->fid.unique)
-			p = p->rb_left;
-		else if (fid->unique > vnode->fid.unique)
-			p = p->rb_right;
-		else
-			goto found;
-	}
+	struct inode *inode;
 
-	/* not found so we just ignore it (it may have moved to another
-	 * server) */
-not_available:
-	_debug("not avail");
-	spin_unlock(&server->fs_lock);
-	_leave("");
-	return;
-
-found:
-	_debug("found");
-	ASSERTCMP(server, ==, vnode->server);
+	read_lock(&server->cb_break_lock);
 
-	if (!igrab(AFS_VNODE_TO_I(vnode)))
-		goto not_available;
-	spin_unlock(&server->fs_lock);
+	/* Step through all interested superblocks.  There may be more than one
+	 * because of cell aliasing.
+	 */
+	list_for_each_entry(cbi, &server->cb_interests, cb_link) {
+		if (cbi->vid != fid->vid)
+			continue;
+
+		data.volume = NULL;
+		data.fid = *fid;
+		inode = ilookup5_nowait(cbi->sb, fid->vnode, afs_iget5_test, &data);
+		if (inode) {
+			vnode = AFS_FS_I(inode);
+			afs_break_callback(vnode);
+			iput(inode);
+		}
+	}
 
-	afs_break_callback(server, vnode);
-	iput(&vnode->vfs_inode);
-	_leave("");
+	read_unlock(&server->cb_break_lock);
 }
 
 /*
@@ -214,243 +231,14 @@ void afs_break_callbacks(struct afs_server *server, size_t count,
 }
 
 /*
- * record the callback for breaking
- * - the caller must hold server->cb_lock
- */
-static void afs_do_give_up_callback(struct afs_server *server,
-				    struct afs_vnode *vnode)
-{
-	struct afs_callback *cb;
-
-	_enter("%p,%p", server, vnode);
-
-	cb = &server->cb_break[server->cb_break_head];
-	cb->fid		= vnode->fid;
-	cb->version	= vnode->cb_version;
-	cb->expiry	= vnode->cb_expiry;
-	cb->type	= vnode->cb_type;
-	smp_wmb();
-	server->cb_break_head =
-		(server->cb_break_head + 1) &
-		(ARRAY_SIZE(server->cb_break) - 1);
-
-	/* defer the breaking of callbacks to try and collect as many as
-	 * possible to ship in one operation */
-	switch (atomic_inc_return(&server->cb_break_n)) {
-	case 1 ... AFSCBMAX - 1:
-		queue_delayed_work(afs_callback_update_worker,
-				   &server->cb_break_work, HZ * 2);
-		break;
-	case AFSCBMAX:
-		afs_flush_callback_breaks(server);
-		break;
-	default:
-		break;
-	}
-
-	ASSERT(server->cb_promises.rb_node != NULL);
-	rb_erase(&vnode->cb_promise, &server->cb_promises);
-	vnode->cb_promised = false;
-	_leave("");
-}
-
-/*
- * discard the callback on a deleted item
- */
-void afs_discard_callback_on_delete(struct afs_vnode *vnode)
-{
-	struct afs_server *server = vnode->server;
-
-	_enter("%d", vnode->cb_promised);
-
-	if (!vnode->cb_promised) {
-		_leave(" [not promised]");
-		return;
-	}
-
-	ASSERT(server != NULL);
-
-	spin_lock(&server->cb_lock);
-	if (vnode->cb_promised) {
-		ASSERT(server->cb_promises.rb_node != NULL);
-		rb_erase(&vnode->cb_promise, &server->cb_promises);
-		vnode->cb_promised = false;
-	}
-	spin_unlock(&server->cb_lock);
-	_leave("");
-}
-
-/*
- * give up the callback registered for a vnode on the file server when the
- * inode is being cleared
+ * Clear the callback interests in a server list.
  */
-void afs_give_up_callback(struct afs_vnode *vnode)
+void afs_clear_callback_interests(struct afs_net *net, struct afs_volume *volume)
 {
-	struct afs_server *server = vnode->server;
-
-	DECLARE_WAITQUEUE(myself, current);
-
-	_enter("%d", vnode->cb_promised);
-
-	_debug("GIVE UP INODE %p", &vnode->vfs_inode);
-
-	if (!vnode->cb_promised) {
-		_leave(" [not promised]");
-		return;
-	}
-
-	ASSERT(server != NULL);
+	int i;
 
-	spin_lock(&server->cb_lock);
-	if (vnode->cb_promised && afs_breakring_space(server) == 0) {
-		add_wait_queue(&server->cb_break_waitq, &myself);
-		for (;;) {
-			set_current_state(TASK_UNINTERRUPTIBLE);
-			if (!vnode->cb_promised ||
-			    afs_breakring_space(server) != 0)
-				break;
-			spin_unlock(&server->cb_lock);
-			schedule();
-			spin_lock(&server->cb_lock);
-		}
-		remove_wait_queue(&server->cb_break_waitq, &myself);
-		__set_current_state(TASK_RUNNING);
+	for (i = 0; i < ARRAY_SIZE(volume->cb_interests); i++) {
+		afs_put_cb_interest(net, volume->cb_interests[i]);
+		volume->cb_interests[i] = NULL;
 	}
-
-	/* of course, it's always possible for the server to break this vnode's
-	 * callback first... */
-	if (vnode->cb_promised)
-		afs_do_give_up_callback(server, vnode);
-
-	spin_unlock(&server->cb_lock);
-	_leave("");
-}
-
-/*
- * dispatch a deferred give up callbacks operation
- */
-void afs_dispatch_give_up_callbacks(struct work_struct *work)
-{
-	struct afs_server *server =
-		container_of(work, struct afs_server, cb_break_work.work);
-
-	_enter("");
-
-	/* tell the fileserver to discard the callback promises it has
-	 * - in the event of ENOMEM or some other error, we just forget that we
-	 *   had callbacks entirely, and the server will call us later to break
-	 *   them
-	 */
-	afs_fs_give_up_callbacks(server->cell->net, server, true);
-}
-
-/*
- * flush the outstanding callback breaks on a server
- */
-void afs_flush_callback_breaks(struct afs_server *server)
-{
-	mod_delayed_work(afs_callback_update_worker, &server->cb_break_work, 0);
-}
-
-#if 0
-/*
- * update a bunch of callbacks
- */
-static void afs_callback_updater(struct work_struct *work)
-{
-	struct afs_server *server;
-	struct afs_vnode *vnode, *xvnode;
-	time64_t now;
-	long timeout;
-	int ret;
-
-	server = container_of(work, struct afs_server, updater);
-
-	_enter("");
-
-	now = ktime_get_real_seconds();
-
-	/* find the first vnode to update */
-	spin_lock(&server->cb_lock);
-	for (;;) {
-		if (RB_EMPTY_ROOT(&server->cb_promises)) {
-			spin_unlock(&server->cb_lock);
-			_leave(" [nothing]");
-			return;
-		}
-
-		vnode = rb_entry(rb_first(&server->cb_promises),
-				 struct afs_vnode, cb_promise);
-		if (atomic_read(&vnode->usage) > 0)
-			break;
-		rb_erase(&vnode->cb_promise, &server->cb_promises);
-		vnode->cb_promised = false;
-	}
-
-	timeout = vnode->update_at - now;
-	if (timeout > 0) {
-		queue_delayed_work(afs_vnode_update_worker,
-				   &afs_vnode_update, timeout * HZ);
-		spin_unlock(&server->cb_lock);
-		_leave(" [nothing]");
-		return;
-	}
-
-	list_del_init(&vnode->update);
-	atomic_inc(&vnode->usage);
-	spin_unlock(&server->cb_lock);
-
-	/* we can now perform the update */
-	_debug("update %s", vnode->vldb.name);
-	vnode->state = AFS_VL_UPDATING;
-	vnode->upd_rej_cnt = 0;
-	vnode->upd_busy_cnt = 0;
-
-	ret = afs_vnode_update_record(vl, &vldb);
-	switch (ret) {
-	case 0:
-		afs_vnode_apply_update(vl, &vldb);
-		vnode->state = AFS_VL_UPDATING;
-		break;
-	case -ENOMEDIUM:
-		vnode->state = AFS_VL_VOLUME_DELETED;
-		break;
-	default:
-		vnode->state = AFS_VL_UNCERTAIN;
-		break;
-	}
-
-	/* and then reschedule */
-	_debug("reschedule");
-	vnode->update_at = ktime_get_real_seconds() +
-			afs_vnode_update_timeout;
-
-	spin_lock(&server->cb_lock);
-
-	if (!list_empty(&server->cb_promises)) {
-		/* next update in 10 minutes, but wait at least 1 second more
-		 * than the newest record already queued so that we don't spam
-		 * the VL server suddenly with lots of requests
-		 */
-		xvnode = list_entry(server->cb_promises.prev,
-				    struct afs_vnode, update);
-		if (vnode->update_at <= xvnode->update_at)
-			vnode->update_at = xvnode->update_at + 1;
-		xvnode = list_entry(server->cb_promises.next,
-				    struct afs_vnode, update);
-		timeout = xvnode->update_at - now;
-		if (timeout < 0)
-			timeout = 0;
-	} else {
-		timeout = afs_vnode_update_timeout;
-	}
-
-	list_add_tail(&vnode->update, &server->cb_promises);
-
-	_debug("timeout %ld", timeout);
-	queue_delayed_work(afs_vnode_update_worker,
-			   &afs_vnode_update, timeout * HZ);
-	spin_unlock(&server->cb_lock);
-	afs_put_vnode(vl);
 }
-#endif

commit f044c8847bb61eff5e1e95b6f6bb950e7f4a73a4
Author: David Howells <dhowells@redhat.com>
Date:   Thu Nov 2 15:27:45 2017 +0000

    afs: Lay the groundwork for supporting network namespaces
    
    Lay the groundwork for supporting network namespaces (netns) to the AFS
    filesystem by moving various global features to a network-namespace struct
    (afs_net) and providing an instance of this as a temporary global variable
    that everything uses via accessor functions for the moment.
    
    The following changes have been made:
    
     (1) Store the netns in the superblock info.  This will be obtained from
         the mounter's nsproxy on a manual mount and inherited from the parent
         superblock on an automount.
    
     (2) The cell list is made per-netns.  It can be viewed through
         /proc/net/afs/cells and also be modified by writing commands to that
         file.
    
     (3) The local workstation cell is set per-ns in /proc/net/afs/rootcell.
         This is unset by default.
    
     (4) The 'rootcell' module parameter, which sets a cell and VL server list
         modifies the init net namespace, thereby allowing an AFS root fs to be
         theoretically used.
    
     (5) The volume location lists and the file lock manager are made
         per-netns.
    
     (6) The AF_RXRPC socket and associated I/O bits are made per-ns.
    
    The various workqueues remain global for the moment.
    
    Changes still to be made:
    
     (1) /proc/fs/afs/ should be moved to /proc/net/afs/ and a symlink emplaced
         from the old name.
    
     (2) A per-netns subsys needs to be registered for AFS into which it can
         store its per-netns data.
    
     (3) Rather than the AF_RXRPC socket being opened on module init, it needs
         to be opened on the creation of a superblock in that netns.
    
     (4) The socket needs to be closed when the last superblock using it is
         destroyed and all outstanding client calls on it have been completed.
         This prevents a reference loop on the namespace.
    
     (5) It is possible that several namespaces will want to use AFS, in which
         case each one will need its own UDP port.  These can either be set
         through /proc/net/afs/cm_port or the kernel can pick one at random.
         The init_ns gets 7001 by default.
    
    Other issues that need resolving:
    
     (1) The DNS keyring needs net-namespacing.
    
     (2) Where do upcalls go (eg. DNS request-key upcall)?
    
     (3) Need something like open_socket_in_file_ns() syscall so that AFS
         command line tools attempting to operate on an AFS file/volume have
         their RPC calls go to the right place.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/callback.c b/fs/afs/callback.c
index 25d404d22cae..d12dffb76b67 100644
--- a/fs/afs/callback.c
+++ b/fs/afs/callback.c
@@ -28,9 +28,7 @@ unsigned afs_vnode_update_timeout = 10;
 	CIRC_SPACE((server)->cb_break_head, (server)->cb_break_tail,	\
 		   ARRAY_SIZE((server)->cb_break))
 
-//static void afs_callback_updater(struct work_struct *);
-
-static struct workqueue_struct *afs_callback_update_worker;
+struct workqueue_struct *afs_callback_update_worker;
 
 /*
  * allow the fileserver to request callback state (re-)initialisation
@@ -343,7 +341,7 @@ void afs_dispatch_give_up_callbacks(struct work_struct *work)
 	 *   had callbacks entirely, and the server will call us later to break
 	 *   them
 	 */
-	afs_fs_give_up_callbacks(server, true);
+	afs_fs_give_up_callbacks(server->cell->net, server, true);
 }
 
 /*
@@ -456,21 +454,3 @@ static void afs_callback_updater(struct work_struct *work)
 	afs_put_vnode(vl);
 }
 #endif
-
-/*
- * initialise the callback update process
- */
-int __init afs_callback_update_init(void)
-{
-	afs_callback_update_worker = alloc_ordered_workqueue("kafs_callbackd",
-							     WQ_MEM_RECLAIM);
-	return afs_callback_update_worker ? 0 : -ENOMEM;
-}
-
-/*
- * shut down the callback update process
- */
-void afs_callback_update_kill(void)
-{
-	destroy_workqueue(afs_callback_update_worker);
-}

commit 8a79790bf0b7da216627ffb85f52cfb4adbf1e4e
Author: Tina Ruchandani <ruchandani.tina@gmail.com>
Date:   Thu Mar 16 16:27:46 2017 +0000

    afs: Migrate vlocation fields to 64-bit
    
    get_seconds() returns real wall-clock seconds. On 32-bit systems
    this value will overflow in year 2038 and beyond. This patch changes
    afs's vlocation record to use ktime_get_real_seconds() instead, for the
    fields time_of_death and update_at.
    
    Signed-off-by: Tina Ruchandani <ruchandani.tina@gmail.com>
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/callback.c b/fs/afs/callback.c
index b29447e03ede..25d404d22cae 100644
--- a/fs/afs/callback.c
+++ b/fs/afs/callback.c
@@ -362,7 +362,7 @@ static void afs_callback_updater(struct work_struct *work)
 {
 	struct afs_server *server;
 	struct afs_vnode *vnode, *xvnode;
-	time_t now;
+	time64_t now;
 	long timeout;
 	int ret;
 
@@ -370,7 +370,7 @@ static void afs_callback_updater(struct work_struct *work)
 
 	_enter("");
 
-	now = get_seconds();
+	now = ktime_get_real_seconds();
 
 	/* find the first vnode to update */
 	spin_lock(&server->cb_lock);
@@ -424,7 +424,8 @@ static void afs_callback_updater(struct work_struct *work)
 
 	/* and then reschedule */
 	_debug("reschedule");
-	vnode->update_at = get_seconds() + afs_vnode_update_timeout;
+	vnode->update_at = ktime_get_real_seconds() +
+			afs_vnode_update_timeout;
 
 	spin_lock(&server->cb_lock);
 

commit 56ff9c837778ba2cf76f29c966856a9341e5939d
Author: David Howells <dhowells@redhat.com>
Date:   Thu Jan 5 10:38:36 2017 +0000

    afs: Kill afs_wait_mode
    
    The afs_wait_mode struct isn't really necessary.  Client calls only use one
    of a choice of two (synchronous or the asynchronous) and incoming calls
    don't use the wait at all.  Replace with a boolean parameter.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/callback.c b/fs/afs/callback.c
index 1e9d2f84e5b5..b29447e03ede 100644
--- a/fs/afs/callback.c
+++ b/fs/afs/callback.c
@@ -343,7 +343,7 @@ void afs_dispatch_give_up_callbacks(struct work_struct *work)
 	 *   had callbacks entirely, and the server will call us later to break
 	 *   them
 	 */
-	afs_fs_give_up_callbacks(server, &afs_async_call);
+	afs_fs_give_up_callbacks(server, true);
 }
 
 /*

commit 4c136dae62f7a62383a9d6563c8613e732bfefaf
Author: Bhaktipriya Shridhar <bhaktipriya96@gmail.com>
Date:   Sun Sep 4 20:54:11 2016 +0530

    fs/afs/callback: Remove deprecated create_singlethread_workqueue
    
    The workqueue "afs_callback_update_worker" queues multiple work items
    viz  &vnode->cb_broken_work, &server->cb_break_work which require strict
    execution ordering. Hence, an ordered dedicated workqueue has been used.
    
    Since the workqueue is being used on a memory reclaim path, WQ_MEM_RECLAIM
    has been set to ensure forward progress under memory pressure.
    
    Signed-off-by: Bhaktipriya Shridhar <bhaktipriya96@gmail.com>
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/callback.c b/fs/afs/callback.c
index 7ef637d7f3a5..1e9d2f84e5b5 100644
--- a/fs/afs/callback.c
+++ b/fs/afs/callback.c
@@ -461,8 +461,8 @@ static void afs_callback_updater(struct work_struct *work)
  */
 int __init afs_callback_update_init(void)
 {
-	afs_callback_update_worker =
-		create_singlethread_workqueue("kafs_callbackd");
+	afs_callback_update_worker = alloc_ordered_workqueue("kafs_callbackd",
+							     WQ_MEM_RECLAIM);
 	return afs_callback_update_worker ? 0 : -ENOMEM;
 }
 

commit 41f63c5359d14ca995172b8f6eaffd93f60fec54
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Aug 3 10:30:47 2012 -0700

    workqueue: use mod_delayed_work() instead of cancel + queue
    
    Convert delayed_work users doing cancel_delayed_work() followed by
    queue_delayed_work() to mod_delayed_work().
    
    Most conversions are straight-forward.  Ones worth mentioning are,
    
    * drivers/edac/edac_mc.c: edac_mc_workq_setup() converted to always
      use mod_delayed_work() and cancel loop in
      edac_mc_reset_delay_period() is dropped.
    
    * drivers/platform/x86/thinkpad_acpi.c: No need to remember whether
      watchdog is active or not.  @fan_watchdog_active and related code
      dropped.
    
    * drivers/power/charger-manager.c: Seemingly a lot of
      delayed_work_pending() abuse going on here.
      [delayed_]work_pending() are unsynchronized and racy when used like
      this.  I converted one instance in fullbatt_handler().  Please
      conver the rest so that it invokes workqueue APIs for the intended
      target state rather than trying to game work item pending state
      transitions.  e.g. if timer should be modified - call
      mod_delayed_work(), canceled - call cancel_delayed_work[_sync]().
    
    * drivers/thermal/thermal_sys.c: thermal_zone_device_set_polling()
      simplified.  Note that round_jiffies() calls in this function are
      meaningless.  round_jiffies() work on absolute jiffies not delta
      delay used by delayed_work.
    
    v2: Tomi pointed out that __cancel_delayed_work() users can't be
        safely converted to mod_delayed_work().  They could be calling it
        from irq context and if that happens while delayed_work_timer_fn()
        is running, it could deadlock.  __cancel_delayed_work() users are
        dropped.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Henrique de Moraes Holschuh <hmh@hmh.eng.br>
    Acked-by: Dmitry Torokhov <dmitry.torokhov@gmail.com>
    Acked-by: Anton Vorontsov <cbouatmailru@gmail.com>
    Acked-by: David Howells <dhowells@redhat.com>
    Cc: Tomi Valkeinen <tomi.valkeinen@ti.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Doug Thompson <dougthompson@xmission.com>
    Cc: David Airlie <airlied@linux.ie>
    Cc: Roland Dreier <roland@kernel.org>
    Cc: "John W. Linville" <linville@tuxdriver.com>
    Cc: Zhang Rui <rui.zhang@intel.com>
    Cc: Len Brown <len.brown@intel.com>
    Cc: "J. Bruce Fields" <bfields@fieldses.org>
    Cc: Johannes Berg <johannes@sipsolutions.net>

diff --git a/fs/afs/callback.c b/fs/afs/callback.c
index 587ef5123cd8..7ef637d7f3a5 100644
--- a/fs/afs/callback.c
+++ b/fs/afs/callback.c
@@ -351,9 +351,7 @@ void afs_dispatch_give_up_callbacks(struct work_struct *work)
  */
 void afs_flush_callback_breaks(struct afs_server *server)
 {
-	cancel_delayed_work(&server->cb_break_work);
-	queue_delayed_work(afs_callback_update_worker,
-			   &server->cb_break_work, 0);
+	mod_delayed_work(afs_callback_update_worker, &server->cb_break_work, 0);
 }
 
 #if 0

commit 44d1b980c72db0faf35adb082fb2208351803028
Author: David Woodhouse <dwmw2@infradead.org>
Date:   Thu Jun 5 22:46:18 2008 -0700

    Fix various old email addresses for dwmw2
    
    Although if people have questions about ARCnet, perhaps it's _better_
    for them to be mailing dwmw2@cam.ac.uk about it...
    
    Signed-off-by: David Woodhouse <dwmw2@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/afs/callback.c b/fs/afs/callback.c
index a78d5b236bb1..587ef5123cd8 100644
--- a/fs/afs/callback.c
+++ b/fs/afs/callback.c
@@ -8,7 +8,7 @@
  * along with this program; if not, write to the Free Software
  * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
  *
- * Authors: David Woodhouse <dwmw2@cambridge.redhat.com>
+ * Authors: David Woodhouse <dwmw2@infradead.org>
  *          David Howells <dhowells@redhat.com>
  *
  */

commit c1206a2c6d87def4af5379e6fef64e928d882ab2
Author: Adrian Bunk <bunk@stusta.de>
Date:   Tue Oct 16 23:26:41 2007 -0700

    fs/afs/: possible cleanups
    
    This patch contains the following possible cleanups:
    - make the following needlessly global functions static:
      - rxrpc.c: afs_send_pages()
      - vlocation.c: afs_vlocation_queue_for_updates()
      - write.c: afs_writepages_region()
    - make the following needlessly global variables static:
      - mntpt.c: afs_mntpt_expiry_timeout
      - proc.c: afs_vlocation_states[]
      - server.c: afs_server_timeout
      - vlocation.c: afs_vlocation_timeout
      - vlocation.c: afs_vlocation_update_timeout
    - #if 0 the following unused function:
      - cell.c: afs_get_cell_maybe()
    - #if 0 the following unused variables:
      - callback.c: afs_vnode_update_timeout
      - cmservice.c: struct afs_cm_workqueue
    
    Signed-off-by: Adrian Bunk <bunk@stusta.de>
    Acked-by: David Howells <dhowells@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/afs/callback.c b/fs/afs/callback.c
index b8243945818d..a78d5b236bb1 100644
--- a/fs/afs/callback.c
+++ b/fs/afs/callback.c
@@ -20,7 +20,9 @@
 #include <linux/sched.h>
 #include "internal.h"
 
+#if 0
 unsigned afs_vnode_update_timeout = 10;
+#endif  /*  0  */
 
 #define afs_breakring_space(server) \
 	CIRC_SPACE((server)->cb_break_head, (server)->cb_break_tail,	\

commit e8d6c554126b830217c5e9f549e0e21f865a0a8a
Author: David Howells <dhowells@redhat.com>
Date:   Sun Jul 15 23:40:12 2007 -0700

    AFS: implement file locking
    
    Implement file locking for AFS.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/afs/callback.c b/fs/afs/callback.c
index bacf518c6fa8..b8243945818d 100644
--- a/fs/afs/callback.c
+++ b/fs/afs/callback.c
@@ -125,6 +125,9 @@ static void afs_break_callback(struct afs_server *server,
 		spin_unlock(&server->cb_lock);
 
 		queue_work(afs_callback_update_worker, &vnode->cb_broken_work);
+		if (list_empty(&vnode->granted_locks) &&
+		    !list_empty(&vnode->pending_locks))
+			afs_lock_may_be_available(vnode);
 		spin_unlock(&vnode->lock);
 	}
 }

commit e8edc6e03a5c8562dc70a6d969f732bdb355a7e7
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Mon May 21 01:22:52 2007 +0400

    Detach sched.h from mm.h
    
    First thing mm.h does is including sched.h solely for can_do_mlock() inline
    function which has "current" dereference inside. By dealing with can_do_mlock()
    mm.h can be detached from sched.h which is good. See below, why.
    
    This patch
    a) removes unconditional inclusion of sched.h from mm.h
    b) makes can_do_mlock() normal function in mm/mlock.c
    c) exports can_do_mlock() to not break compilation
    d) adds sched.h inclusions back to files that were getting it indirectly.
    e) adds less bloated headers to some files (asm/signal.h, jiffies.h) that were
       getting them indirectly
    
    Net result is:
    a) mm.h users would get less code to open, read, preprocess, parse, ... if
       they don't need sched.h
    b) sched.h stops being dependency for significant number of files:
       on x86_64 allmodconfig touching sched.h results in recompile of 4083 files,
       after patch it's only 3744 (-8.3%).
    
    Cross-compile tested on
    
            all arm defconfigs, all mips defconfigs, all powerpc defconfigs,
            alpha alpha-up
            arm
            i386 i386-up i386-defconfig i386-allnoconfig
            ia64 ia64-up
            m68k
            mips
            parisc parisc-up
            powerpc powerpc-up
            s390 s390-up
            sparc sparc-up
            sparc64 sparc64-up
            um-x86_64
            x86_64 x86_64-up x86_64-defconfig x86_64-allnoconfig
    
    as well as my two usual configs.
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/afs/callback.c b/fs/afs/callback.c
index f64e40fefc02..bacf518c6fa8 100644
--- a/fs/afs/callback.c
+++ b/fs/afs/callback.c
@@ -17,6 +17,7 @@
 #include <linux/module.h>
 #include <linux/init.h>
 #include <linux/circ_buf.h>
+#include <linux/sched.h>
 #include "internal.h"
 
 unsigned afs_vnode_update_timeout = 10;

commit 416351f28d2b31d15ff73e9aff699b2163704c95
Author: David Howells <dhowells@redhat.com>
Date:   Wed May 9 02:33:45 2007 -0700

    AFS: AFS fixups
    
    Make some miscellaneous changes to the AFS filesystem:
    
     (1) Assert RCU barriers on module exit to make sure RCU has finished with
         callbacks in this module.
    
     (2) Correctly handle the AFS server returning a zero-length read.
    
     (3) Split out data zapping calls into one function (afs_zap_data).
    
     (4) Rename some afs_file_*() functions to afs_*() where they apply to
         non-regular files too.
    
     (5) Be consistent about the presentation of volume ID:vnode ID in debugging
         output.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/afs/callback.c b/fs/afs/callback.c
index 9bdbf36a9aa9..f64e40fefc02 100644
--- a/fs/afs/callback.c
+++ b/fs/afs/callback.c
@@ -44,7 +44,7 @@ void afs_init_callback_state(struct afs_server *server)
 	while (!RB_EMPTY_ROOT(&server->cb_promises)) {
 		vnode = rb_entry(server->cb_promises.rb_node,
 				 struct afs_vnode, cb_promise);
-		_debug("UNPROMISE { vid=%x vn=%u uq=%u}",
+		_debug("UNPROMISE { vid=%x:%u uq=%u}",
 		       vnode->fid.vid, vnode->fid.vnode, vnode->fid.unique);
 		rb_erase(&vnode->cb_promise, &server->cb_promises);
 		vnode->cb_promised = false;
@@ -84,11 +84,8 @@ void afs_broken_callback_work(struct work_struct *work)
 
 		/* if the vnode's data version number changed then its contents
 		 * are different */
-		if (test_and_clear_bit(AFS_VNODE_ZAP_DATA, &vnode->flags)) {
-			_debug("zap data {%x:%u}",
-			       vnode->fid.vid, vnode->fid.vnode);
-			invalidate_remote_inode(&vnode->vfs_inode);
-		}
+		if (test_and_clear_bit(AFS_VNODE_ZAP_DATA, &vnode->flags))
+			afs_zap_data(vnode);
 	}
 
 out:

commit fbb3fcba72ff52ee3d9990c9dec0c95cd9d0ce17
Author: David Howells <dhowells@redhat.com>
Date:   Thu May 3 03:12:46 2007 -0700

    [AFS]: Fix use of __exit functions from __init path
    
    Fix use of __exit functions from __init path.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/fs/afs/callback.c b/fs/afs/callback.c
index 639399f0ab6f..9bdbf36a9aa9 100644
--- a/fs/afs/callback.c
+++ b/fs/afs/callback.c
@@ -468,7 +468,7 @@ int __init afs_callback_update_init(void)
 /*
  * shut down the callback update process
  */
-void __exit afs_callback_update_kill(void)
+void afs_callback_update_kill(void)
 {
 	destroy_workqueue(afs_callback_update_worker);
 }

commit 260a980317dac80182dd76140cf67c6e81d6d3dd
Author: David Howells <dhowells@redhat.com>
Date:   Thu Apr 26 15:59:35 2007 -0700

    [AFS]: Add "directory write" support.
    
    Add support for the create, link, symlink, unlink, mkdir, rmdir and
    rename VFS operations to the in-kernel AFS filesystem.
    
    Also:
    
     (1) Fix dentry and inode revalidation.  d_revalidate should only look at
         state of the dentry.  Revalidation of the contents of an inode pointed to
         by a dentry is now separate.
    
     (2) Fix afs_lookup() to hash negative dentries as well as positive ones.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/fs/afs/callback.c b/fs/afs/callback.c
index e674bebbb8b1..639399f0ab6f 100644
--- a/fs/afs/callback.c
+++ b/fs/afs/callback.c
@@ -44,7 +44,8 @@ void afs_init_callback_state(struct afs_server *server)
 	while (!RB_EMPTY_ROOT(&server->cb_promises)) {
 		vnode = rb_entry(server->cb_promises.rb_node,
 				 struct afs_vnode, cb_promise);
-		printk("\nUNPROMISE on %p\n", vnode);
+		_debug("UNPROMISE { vid=%x vn=%u uq=%u}",
+		       vnode->fid.vid, vnode->fid.vnode, vnode->fid.unique);
 		rb_erase(&vnode->cb_promise, &server->cb_promises);
 		vnode->cb_promised = false;
 	}
@@ -68,7 +69,7 @@ void afs_broken_callback_work(struct work_struct *work)
 
 	/* we're only interested in dealing with a broken callback on *this*
 	 * vnode and only if no-one else has dealt with it yet */
-	if (!mutex_trylock(&vnode->cb_broken_lock))
+	if (!mutex_trylock(&vnode->validate_lock))
 		return; /* someone else is dealing with it */
 
 	if (test_bit(AFS_VNODE_CB_BROKEN, &vnode->flags)) {
@@ -84,13 +85,14 @@ void afs_broken_callback_work(struct work_struct *work)
 		/* if the vnode's data version number changed then its contents
 		 * are different */
 		if (test_and_clear_bit(AFS_VNODE_ZAP_DATA, &vnode->flags)) {
-			_debug("zap data");
+			_debug("zap data {%x:%u}",
+			       vnode->fid.vid, vnode->fid.vnode);
 			invalidate_remote_inode(&vnode->vfs_inode);
 		}
 	}
 
 out:
-	mutex_unlock(&vnode->cb_broken_lock);
+	mutex_unlock(&vnode->validate_lock);
 
 	/* avoid the potential race whereby the mutex_trylock() in this
 	 * function happens again between the clear_bit() and the
@@ -251,6 +253,32 @@ static void afs_do_give_up_callback(struct afs_server *server,
 	_leave("");
 }
 
+/*
+ * discard the callback on a deleted item
+ */
+void afs_discard_callback_on_delete(struct afs_vnode *vnode)
+{
+	struct afs_server *server = vnode->server;
+
+	_enter("%d", vnode->cb_promised);
+
+	if (!vnode->cb_promised) {
+		_leave(" [not promised]");
+		return;
+	}
+
+	ASSERT(server != NULL);
+
+	spin_lock(&server->cb_lock);
+	if (vnode->cb_promised) {
+		ASSERT(server->cb_promises.rb_node != NULL);
+		rb_erase(&vnode->cb_promise, &server->cb_promises);
+		vnode->cb_promised = false;
+	}
+	spin_unlock(&server->cb_lock);
+	_leave("");
+}
+
 /*
  * give up the callback registered for a vnode on the file server when the
  * inode is being cleared

commit 00d3b7a4533e367b0dc2812a706db8f9f071c27f
Author: David Howells <dhowells@redhat.com>
Date:   Thu Apr 26 15:57:07 2007 -0700

    [AFS]: Add security support.
    
    Add security support to the AFS filesystem.  Kerberos IV tickets are added as
    RxRPC keys are added to the session keyring with the klog program.  open() and
    other VFS operations then find this ticket with request_key() and either use
    it immediately (eg: mkdir, unlink) or attach it to a file descriptor (open).
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/fs/afs/callback.c b/fs/afs/callback.c
index 611215547142..e674bebbb8b1 100644
--- a/fs/afs/callback.c
+++ b/fs/afs/callback.c
@@ -72,7 +72,10 @@ void afs_broken_callback_work(struct work_struct *work)
 		return; /* someone else is dealing with it */
 
 	if (test_bit(AFS_VNODE_CB_BROKEN, &vnode->flags)) {
-		if (afs_vnode_fetch_status(vnode) < 0)
+		if (S_ISDIR(vnode->vfs_inode.i_mode))
+			afs_clear_permits(vnode);
+
+		if (afs_vnode_fetch_status(vnode, NULL, NULL) < 0)
 			goto out;
 
 		if (test_bit(AFS_VNODE_DELETED, &vnode->flags))

commit 08e0e7c82eeadec6f4871a386b86bf0f0fbcb4eb
Author: David Howells <dhowells@redhat.com>
Date:   Thu Apr 26 15:55:03 2007 -0700

    [AF_RXRPC]: Make the in-kernel AFS filesystem use AF_RXRPC.
    
    Make the in-kernel AFS filesystem use AF_RXRPC instead of the old RxRPC code.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/fs/afs/callback.c b/fs/afs/callback.c
index 26a48fea42f4..611215547142 100644
--- a/fs/afs/callback.c
+++ b/fs/afs/callback.c
@@ -1,5 +1,5 @@
 /*
- * Copyright (c) 2002 Red Hat, Inc. All rights reserved.
+ * Copyright (c) 2002, 2007 Red Hat, Inc. All rights reserved.
  *
  * This software may be freely redistributed under the terms of the
  * GNU General Public License.
@@ -16,83 +16,182 @@
 #include <linux/kernel.h>
 #include <linux/module.h>
 #include <linux/init.h>
-#include "server.h"
-#include "vnode.h"
+#include <linux/circ_buf.h>
 #include "internal.h"
-#include "cmservice.h"
+
+unsigned afs_vnode_update_timeout = 10;
+
+#define afs_breakring_space(server) \
+	CIRC_SPACE((server)->cb_break_head, (server)->cb_break_tail,	\
+		   ARRAY_SIZE((server)->cb_break))
+
+//static void afs_callback_updater(struct work_struct *);
+
+static struct workqueue_struct *afs_callback_update_worker;
 
 /*
  * allow the fileserver to request callback state (re-)initialisation
  */
-int SRXAFSCM_InitCallBackState(struct afs_server *server)
+void afs_init_callback_state(struct afs_server *server)
 {
-	struct list_head callbacks;
+	struct afs_vnode *vnode;
 
-	_enter("%p", server);
+	_enter("{%p}", server);
 
-	INIT_LIST_HEAD(&callbacks);
-
-	/* transfer the callback list from the server to a temp holding area */
 	spin_lock(&server->cb_lock);
 
-	list_add(&callbacks, &server->cb_promises);
-	list_del_init(&server->cb_promises);
+	/* kill all the promises on record from this server */
+	while (!RB_EMPTY_ROOT(&server->cb_promises)) {
+		vnode = rb_entry(server->cb_promises.rb_node,
+				 struct afs_vnode, cb_promise);
+		printk("\nUNPROMISE on %p\n", vnode);
+		rb_erase(&vnode->cb_promise, &server->cb_promises);
+		vnode->cb_promised = false;
+	}
 
-	/* munch our way through the list, grabbing the inode, dropping all the
-	 * locks and regetting them in the right order
-	 */
-	while (!list_empty(&callbacks)) {
-		struct afs_vnode *vnode;
-		struct inode *inode;
+	spin_unlock(&server->cb_lock);
+	_leave("");
+}
 
-		vnode = list_entry(callbacks.next, struct afs_vnode, cb_link);
-		list_del_init(&vnode->cb_link);
+/*
+ * handle the data invalidation side of a callback being broken
+ */
+void afs_broken_callback_work(struct work_struct *work)
+{
+	struct afs_vnode *vnode =
+		container_of(work, struct afs_vnode, cb_broken_work);
 
-		/* try and grab the inode - may fail */
-		inode = igrab(AFS_VNODE_TO_I(vnode));
-		if (inode) {
-			int release = 0;
+	_enter("");
 
-			spin_unlock(&server->cb_lock);
-			spin_lock(&vnode->lock);
+	if (test_bit(AFS_VNODE_DELETED, &vnode->flags))
+		return;
 
-			if (vnode->cb_server == server) {
-				vnode->cb_server = NULL;
-				afs_kafstimod_del_timer(&vnode->cb_timeout);
-				spin_lock(&afs_cb_hash_lock);
-				list_del_init(&vnode->cb_hash_link);
-				spin_unlock(&afs_cb_hash_lock);
-				release = 1;
-			}
+	/* we're only interested in dealing with a broken callback on *this*
+	 * vnode and only if no-one else has dealt with it yet */
+	if (!mutex_trylock(&vnode->cb_broken_lock))
+		return; /* someone else is dealing with it */
 
-			spin_unlock(&vnode->lock);
+	if (test_bit(AFS_VNODE_CB_BROKEN, &vnode->flags)) {
+		if (afs_vnode_fetch_status(vnode) < 0)
+			goto out;
 
-			iput(inode);
-			afs_put_server(server);
+		if (test_bit(AFS_VNODE_DELETED, &vnode->flags))
+			goto out;
 
-			spin_lock(&server->cb_lock);
+		/* if the vnode's data version number changed then its contents
+		 * are different */
+		if (test_and_clear_bit(AFS_VNODE_ZAP_DATA, &vnode->flags)) {
+			_debug("zap data");
+			invalidate_remote_inode(&vnode->vfs_inode);
 		}
 	}
 
-	spin_unlock(&server->cb_lock);
+out:
+	mutex_unlock(&vnode->cb_broken_lock);
 
-	_leave(" = 0");
-	return 0;
+	/* avoid the potential race whereby the mutex_trylock() in this
+	 * function happens again between the clear_bit() and the
+	 * mutex_unlock() */
+	if (test_bit(AFS_VNODE_CB_BROKEN, &vnode->flags)) {
+		_debug("requeue");
+		queue_work(afs_callback_update_worker, &vnode->cb_broken_work);
+	}
+	_leave("");
+}
+
+/*
+ * actually break a callback
+ */
+static void afs_break_callback(struct afs_server *server,
+			       struct afs_vnode *vnode)
+{
+	_enter("");
+
+	set_bit(AFS_VNODE_CB_BROKEN, &vnode->flags);
+
+	if (vnode->cb_promised) {
+		spin_lock(&vnode->lock);
+
+		_debug("break callback");
+
+		spin_lock(&server->cb_lock);
+		if (vnode->cb_promised) {
+			rb_erase(&vnode->cb_promise, &server->cb_promises);
+			vnode->cb_promised = false;
+		}
+		spin_unlock(&server->cb_lock);
+
+		queue_work(afs_callback_update_worker, &vnode->cb_broken_work);
+		spin_unlock(&vnode->lock);
+	}
+}
+
+/*
+ * allow the fileserver to explicitly break one callback
+ * - happens when
+ *   - the backing file is changed
+ *   - a lock is released
+ */
+static void afs_break_one_callback(struct afs_server *server,
+				   struct afs_fid *fid)
+{
+	struct afs_vnode *vnode;
+	struct rb_node *p;
+
+	_debug("find");
+	spin_lock(&server->fs_lock);
+	p = server->fs_vnodes.rb_node;
+	while (p) {
+		vnode = rb_entry(p, struct afs_vnode, server_rb);
+		if (fid->vid < vnode->fid.vid)
+			p = p->rb_left;
+		else if (fid->vid > vnode->fid.vid)
+			p = p->rb_right;
+		else if (fid->vnode < vnode->fid.vnode)
+			p = p->rb_left;
+		else if (fid->vnode > vnode->fid.vnode)
+			p = p->rb_right;
+		else if (fid->unique < vnode->fid.unique)
+			p = p->rb_left;
+		else if (fid->unique > vnode->fid.unique)
+			p = p->rb_right;
+		else
+			goto found;
+	}
+
+	/* not found so we just ignore it (it may have moved to another
+	 * server) */
+not_available:
+	_debug("not avail");
+	spin_unlock(&server->fs_lock);
+	_leave("");
+	return;
+
+found:
+	_debug("found");
+	ASSERTCMP(server, ==, vnode->server);
+
+	if (!igrab(AFS_VNODE_TO_I(vnode)))
+		goto not_available;
+	spin_unlock(&server->fs_lock);
+
+	afs_break_callback(server, vnode);
+	iput(&vnode->vfs_inode);
+	_leave("");
 }
 
 /*
  * allow the fileserver to break callback promises
  */
-int SRXAFSCM_CallBack(struct afs_server *server, size_t count,
-		      struct afs_callback callbacks[])
+void afs_break_callbacks(struct afs_server *server, size_t count,
+			 struct afs_callback callbacks[])
 {
-	_enter("%p,%u,", server, count);
+	_enter("%p,%zu,", server, count);
 
-	for (; count > 0; callbacks++, count--) {
-		struct afs_vnode *vnode = NULL;
-		struct inode *inode = NULL;
-		int valid = 0;
+	ASSERT(server != NULL);
+	ASSERTCMP(count, <=, AFSCBMAX);
 
+	for (; count > 0; callbacks++, count--) {
 		_debug("- Fid { vl=%08x n=%u u=%u }  CB { v=%u x=%u t=%u }",
 		       callbacks->fid.vid,
 		       callbacks->fid.vnode,
@@ -101,66 +200,244 @@ int SRXAFSCM_CallBack(struct afs_server *server, size_t count,
 		       callbacks->expiry,
 		       callbacks->type
 		       );
+		afs_break_one_callback(server, &callbacks->fid);
+	}
+
+	_leave("");
+	return;
+}
 
-		/* find the inode for this fid */
-		spin_lock(&afs_cb_hash_lock);
+/*
+ * record the callback for breaking
+ * - the caller must hold server->cb_lock
+ */
+static void afs_do_give_up_callback(struct afs_server *server,
+				    struct afs_vnode *vnode)
+{
+	struct afs_callback *cb;
 
-		list_for_each_entry(vnode,
-				    &afs_cb_hash(server, &callbacks->fid),
-				    cb_hash_link) {
-			if (memcmp(&vnode->fid, &callbacks->fid,
-				   sizeof(struct afs_fid)) != 0)
-				continue;
+	_enter("%p,%p", server, vnode);
 
-			/* right vnode, but is it same server? */
-			if (vnode->cb_server != server)
-				break; /* no */
+	cb = &server->cb_break[server->cb_break_head];
+	cb->fid		= vnode->fid;
+	cb->version	= vnode->cb_version;
+	cb->expiry	= vnode->cb_expiry;
+	cb->type	= vnode->cb_type;
+	smp_wmb();
+	server->cb_break_head =
+		(server->cb_break_head + 1) &
+		(ARRAY_SIZE(server->cb_break) - 1);
 
-			/* try and nail the inode down */
-			inode = igrab(AFS_VNODE_TO_I(vnode));
-			break;
+	/* defer the breaking of callbacks to try and collect as many as
+	 * possible to ship in one operation */
+	switch (atomic_inc_return(&server->cb_break_n)) {
+	case 1 ... AFSCBMAX - 1:
+		queue_delayed_work(afs_callback_update_worker,
+				   &server->cb_break_work, HZ * 2);
+		break;
+	case AFSCBMAX:
+		afs_flush_callback_breaks(server);
+		break;
+	default:
+		break;
+	}
+
+	ASSERT(server->cb_promises.rb_node != NULL);
+	rb_erase(&vnode->cb_promise, &server->cb_promises);
+	vnode->cb_promised = false;
+	_leave("");
+}
+
+/*
+ * give up the callback registered for a vnode on the file server when the
+ * inode is being cleared
+ */
+void afs_give_up_callback(struct afs_vnode *vnode)
+{
+	struct afs_server *server = vnode->server;
+
+	DECLARE_WAITQUEUE(myself, current);
+
+	_enter("%d", vnode->cb_promised);
+
+	_debug("GIVE UP INODE %p", &vnode->vfs_inode);
+
+	if (!vnode->cb_promised) {
+		_leave(" [not promised]");
+		return;
+	}
+
+	ASSERT(server != NULL);
+
+	spin_lock(&server->cb_lock);
+	if (vnode->cb_promised && afs_breakring_space(server) == 0) {
+		add_wait_queue(&server->cb_break_waitq, &myself);
+		for (;;) {
+			set_current_state(TASK_UNINTERRUPTIBLE);
+			if (!vnode->cb_promised ||
+			    afs_breakring_space(server) != 0)
+				break;
+			spin_unlock(&server->cb_lock);
+			schedule();
+			spin_lock(&server->cb_lock);
 		}
+		remove_wait_queue(&server->cb_break_waitq, &myself);
+		__set_current_state(TASK_RUNNING);
+	}
+
+	/* of course, it's always possible for the server to break this vnode's
+	 * callback first... */
+	if (vnode->cb_promised)
+		afs_do_give_up_callback(server, vnode);
+
+	spin_unlock(&server->cb_lock);
+	_leave("");
+}
+
+/*
+ * dispatch a deferred give up callbacks operation
+ */
+void afs_dispatch_give_up_callbacks(struct work_struct *work)
+{
+	struct afs_server *server =
+		container_of(work, struct afs_server, cb_break_work.work);
+
+	_enter("");
+
+	/* tell the fileserver to discard the callback promises it has
+	 * - in the event of ENOMEM or some other error, we just forget that we
+	 *   had callbacks entirely, and the server will call us later to break
+	 *   them
+	 */
+	afs_fs_give_up_callbacks(server, &afs_async_call);
+}
+
+/*
+ * flush the outstanding callback breaks on a server
+ */
+void afs_flush_callback_breaks(struct afs_server *server)
+{
+	cancel_delayed_work(&server->cb_break_work);
+	queue_delayed_work(afs_callback_update_worker,
+			   &server->cb_break_work, 0);
+}
+
+#if 0
+/*
+ * update a bunch of callbacks
+ */
+static void afs_callback_updater(struct work_struct *work)
+{
+	struct afs_server *server;
+	struct afs_vnode *vnode, *xvnode;
+	time_t now;
+	long timeout;
+	int ret;
+
+	server = container_of(work, struct afs_server, updater);
+
+	_enter("");
 
-		spin_unlock(&afs_cb_hash_lock);
-
-		if (inode) {
-			/* we've found the record for this vnode */
-			spin_lock(&vnode->lock);
-			if (vnode->cb_server == server) {
-				/* the callback _is_ on the calling server */
-				vnode->cb_server = NULL;
-				valid = 1;
-
-				afs_kafstimod_del_timer(&vnode->cb_timeout);
-				vnode->flags |= AFS_VNODE_CHANGED;
-
-				spin_lock(&server->cb_lock);
-				list_del_init(&vnode->cb_link);
-				spin_unlock(&server->cb_lock);
-
-				spin_lock(&afs_cb_hash_lock);
-				list_del_init(&vnode->cb_hash_link);
-				spin_unlock(&afs_cb_hash_lock);
-			}
-			spin_unlock(&vnode->lock);
-
-			if (valid) {
-				invalidate_remote_inode(inode);
-				afs_put_server(server);
-			}
-			iput(inode);
+	now = get_seconds();
+
+	/* find the first vnode to update */
+	spin_lock(&server->cb_lock);
+	for (;;) {
+		if (RB_EMPTY_ROOT(&server->cb_promises)) {
+			spin_unlock(&server->cb_lock);
+			_leave(" [nothing]");
+			return;
 		}
+
+		vnode = rb_entry(rb_first(&server->cb_promises),
+				 struct afs_vnode, cb_promise);
+		if (atomic_read(&vnode->usage) > 0)
+			break;
+		rb_erase(&vnode->cb_promise, &server->cb_promises);
+		vnode->cb_promised = false;
+	}
+
+	timeout = vnode->update_at - now;
+	if (timeout > 0) {
+		queue_delayed_work(afs_vnode_update_worker,
+				   &afs_vnode_update, timeout * HZ);
+		spin_unlock(&server->cb_lock);
+		_leave(" [nothing]");
+		return;
+	}
+
+	list_del_init(&vnode->update);
+	atomic_inc(&vnode->usage);
+	spin_unlock(&server->cb_lock);
+
+	/* we can now perform the update */
+	_debug("update %s", vnode->vldb.name);
+	vnode->state = AFS_VL_UPDATING;
+	vnode->upd_rej_cnt = 0;
+	vnode->upd_busy_cnt = 0;
+
+	ret = afs_vnode_update_record(vl, &vldb);
+	switch (ret) {
+	case 0:
+		afs_vnode_apply_update(vl, &vldb);
+		vnode->state = AFS_VL_UPDATING;
+		break;
+	case -ENOMEDIUM:
+		vnode->state = AFS_VL_VOLUME_DELETED;
+		break;
+	default:
+		vnode->state = AFS_VL_UNCERTAIN;
+		break;
+	}
+
+	/* and then reschedule */
+	_debug("reschedule");
+	vnode->update_at = get_seconds() + afs_vnode_update_timeout;
+
+	spin_lock(&server->cb_lock);
+
+	if (!list_empty(&server->cb_promises)) {
+		/* next update in 10 minutes, but wait at least 1 second more
+		 * than the newest record already queued so that we don't spam
+		 * the VL server suddenly with lots of requests
+		 */
+		xvnode = list_entry(server->cb_promises.prev,
+				    struct afs_vnode, update);
+		if (vnode->update_at <= xvnode->update_at)
+			vnode->update_at = xvnode->update_at + 1;
+		xvnode = list_entry(server->cb_promises.next,
+				    struct afs_vnode, update);
+		timeout = xvnode->update_at - now;
+		if (timeout < 0)
+			timeout = 0;
+	} else {
+		timeout = afs_vnode_update_timeout;
 	}
 
-	_leave(" = 0");
-	return 0;
+	list_add_tail(&vnode->update, &server->cb_promises);
+
+	_debug("timeout %ld", timeout);
+	queue_delayed_work(afs_vnode_update_worker,
+			   &afs_vnode_update, timeout * HZ);
+	spin_unlock(&server->cb_lock);
+	afs_put_vnode(vl);
+}
+#endif
+
+/*
+ * initialise the callback update process
+ */
+int __init afs_callback_update_init(void)
+{
+	afs_callback_update_worker =
+		create_singlethread_workqueue("kafs_callbackd");
+	return afs_callback_update_worker ? 0 : -ENOMEM;
 }
 
 /*
- * allow the fileserver to see if the cache manager is still alive
+ * shut down the callback update process
  */
-int SRXAFSCM_Probe(struct afs_server *server)
+void __exit afs_callback_update_kill(void)
 {
-	_debug("SRXAFSCM_Probe(%p)\n", server);
-	return 0;
+	destroy_workqueue(afs_callback_update_worker);
 }

commit ec26815ad847dbf74a1e27aa5515fb7d5dc6ee6f
Author: David Howells <dhowells@redhat.com>
Date:   Thu Apr 26 15:49:28 2007 -0700

    [AFS]: Clean up the AFS sources
    
    Clean up the AFS sources.
    
    Also remove references to AFS keys.  RxRPC keys are used instead.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/fs/afs/callback.c b/fs/afs/callback.c
index 9cb206e9d4be..26a48fea42f4 100644
--- a/fs/afs/callback.c
+++ b/fs/afs/callback.c
@@ -21,7 +21,6 @@
 #include "internal.h"
 #include "cmservice.h"
 
-/*****************************************************************************/
 /*
  * allow the fileserver to request callback state (re-)initialisation
  */
@@ -79,9 +78,8 @@ int SRXAFSCM_InitCallBackState(struct afs_server *server)
 
 	_leave(" = 0");
 	return 0;
-} /* end SRXAFSCM_InitCallBackState() */
+}
 
-/*****************************************************************************/
 /*
  * allow the fileserver to break callback promises
  */
@@ -156,9 +154,8 @@ int SRXAFSCM_CallBack(struct afs_server *server, size_t count,
 
 	_leave(" = 0");
 	return 0;
-} /* end SRXAFSCM_CallBack() */
+}
 
-/*****************************************************************************/
 /*
  * allow the fileserver to see if the cache manager is still alive
  */
@@ -166,4 +163,4 @@ int SRXAFSCM_Probe(struct afs_server *server)
 {
 	_debug("SRXAFSCM_Probe(%p)\n", server);
 	return 0;
-} /* end SRXAFSCM_Probe() */
+}

commit 9f56f743e8dcc470322c7efd5d307a32665ba05c
Author: Adrian Bunk <bunk@stusta.de>
Date:   Tue Nov 8 16:56:17 2005 +0100

    fs/afs/callback.c should #include "cmservice.h"
    
    Every file should include the headers containing the prototypes for
    it's global functions.
    
    Signed-off-by: Adrian Bunk <bunk@stusta.de>

diff --git a/fs/afs/callback.c b/fs/afs/callback.c
index 2fd62f89ae01..9cb206e9d4be 100644
--- a/fs/afs/callback.c
+++ b/fs/afs/callback.c
@@ -19,6 +19,7 @@
 #include "server.h"
 #include "vnode.h"
 #include "internal.h"
+#include "cmservice.h"
 
 /*****************************************************************************/
 /*

commit 1da177e4c3f41524e886b7f1b8a0c1fc7321cac2
Author: Linus Torvalds <torvalds@ppc970.osdl.org>
Date:   Sat Apr 16 15:20:36 2005 -0700

    Linux-2.6.12-rc2
    
    Initial git repository build. I'm not bothering with the full history,
    even though we have it. We can create a separate "historical" git
    archive of that later if we want to, and in the meantime it's about
    3.2GB when imported into git - space that would just make the early
    git days unnecessarily complicated, when we don't have a lot of good
    infrastructure for it.
    
    Let it rip!

diff --git a/fs/afs/callback.c b/fs/afs/callback.c
new file mode 100644
index 000000000000..2fd62f89ae01
--- /dev/null
+++ b/fs/afs/callback.c
@@ -0,0 +1,168 @@
+/*
+ * Copyright (c) 2002 Red Hat, Inc. All rights reserved.
+ *
+ * This software may be freely redistributed under the terms of the
+ * GNU General Public License.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ * Authors: David Woodhouse <dwmw2@cambridge.redhat.com>
+ *          David Howells <dhowells@redhat.com>
+ *
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include "server.h"
+#include "vnode.h"
+#include "internal.h"
+
+/*****************************************************************************/
+/*
+ * allow the fileserver to request callback state (re-)initialisation
+ */
+int SRXAFSCM_InitCallBackState(struct afs_server *server)
+{
+	struct list_head callbacks;
+
+	_enter("%p", server);
+
+	INIT_LIST_HEAD(&callbacks);
+
+	/* transfer the callback list from the server to a temp holding area */
+	spin_lock(&server->cb_lock);
+
+	list_add(&callbacks, &server->cb_promises);
+	list_del_init(&server->cb_promises);
+
+	/* munch our way through the list, grabbing the inode, dropping all the
+	 * locks and regetting them in the right order
+	 */
+	while (!list_empty(&callbacks)) {
+		struct afs_vnode *vnode;
+		struct inode *inode;
+
+		vnode = list_entry(callbacks.next, struct afs_vnode, cb_link);
+		list_del_init(&vnode->cb_link);
+
+		/* try and grab the inode - may fail */
+		inode = igrab(AFS_VNODE_TO_I(vnode));
+		if (inode) {
+			int release = 0;
+
+			spin_unlock(&server->cb_lock);
+			spin_lock(&vnode->lock);
+
+			if (vnode->cb_server == server) {
+				vnode->cb_server = NULL;
+				afs_kafstimod_del_timer(&vnode->cb_timeout);
+				spin_lock(&afs_cb_hash_lock);
+				list_del_init(&vnode->cb_hash_link);
+				spin_unlock(&afs_cb_hash_lock);
+				release = 1;
+			}
+
+			spin_unlock(&vnode->lock);
+
+			iput(inode);
+			afs_put_server(server);
+
+			spin_lock(&server->cb_lock);
+		}
+	}
+
+	spin_unlock(&server->cb_lock);
+
+	_leave(" = 0");
+	return 0;
+} /* end SRXAFSCM_InitCallBackState() */
+
+/*****************************************************************************/
+/*
+ * allow the fileserver to break callback promises
+ */
+int SRXAFSCM_CallBack(struct afs_server *server, size_t count,
+		      struct afs_callback callbacks[])
+{
+	_enter("%p,%u,", server, count);
+
+	for (; count > 0; callbacks++, count--) {
+		struct afs_vnode *vnode = NULL;
+		struct inode *inode = NULL;
+		int valid = 0;
+
+		_debug("- Fid { vl=%08x n=%u u=%u }  CB { v=%u x=%u t=%u }",
+		       callbacks->fid.vid,
+		       callbacks->fid.vnode,
+		       callbacks->fid.unique,
+		       callbacks->version,
+		       callbacks->expiry,
+		       callbacks->type
+		       );
+
+		/* find the inode for this fid */
+		spin_lock(&afs_cb_hash_lock);
+
+		list_for_each_entry(vnode,
+				    &afs_cb_hash(server, &callbacks->fid),
+				    cb_hash_link) {
+			if (memcmp(&vnode->fid, &callbacks->fid,
+				   sizeof(struct afs_fid)) != 0)
+				continue;
+
+			/* right vnode, but is it same server? */
+			if (vnode->cb_server != server)
+				break; /* no */
+
+			/* try and nail the inode down */
+			inode = igrab(AFS_VNODE_TO_I(vnode));
+			break;
+		}
+
+		spin_unlock(&afs_cb_hash_lock);
+
+		if (inode) {
+			/* we've found the record for this vnode */
+			spin_lock(&vnode->lock);
+			if (vnode->cb_server == server) {
+				/* the callback _is_ on the calling server */
+				vnode->cb_server = NULL;
+				valid = 1;
+
+				afs_kafstimod_del_timer(&vnode->cb_timeout);
+				vnode->flags |= AFS_VNODE_CHANGED;
+
+				spin_lock(&server->cb_lock);
+				list_del_init(&vnode->cb_link);
+				spin_unlock(&server->cb_lock);
+
+				spin_lock(&afs_cb_hash_lock);
+				list_del_init(&vnode->cb_hash_link);
+				spin_unlock(&afs_cb_hash_lock);
+			}
+			spin_unlock(&vnode->lock);
+
+			if (valid) {
+				invalidate_remote_inode(inode);
+				afs_put_server(server);
+			}
+			iput(inode);
+		}
+	}
+
+	_leave(" = 0");
+	return 0;
+} /* end SRXAFSCM_CallBack() */
+
+/*****************************************************************************/
+/*
+ * allow the fileserver to see if the cache manager is still alive
+ */
+int SRXAFSCM_Probe(struct afs_server *server)
+{
+	_debug("SRXAFSCM_Probe(%p)\n", server);
+	return 0;
+} /* end SRXAFSCM_Probe() */
