commit 9daa0a27a0bce6596be287fb1df372ff80bb1087
Merge: 0b166a57e622 8409f67b6437
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jun 5 16:26:36 2020 -0700

    Merge tag 'afs-next-20200604' of git://git.kernel.org/pub/scm/linux/kernel/git/dhowells/linux-fs
    
    Pull AFS updates from David Howells:
     "There's some core VFS changes which affect a couple of filesystems:
    
       - Make the inode hash table RCU safe and providing some RCU-safe
         accessor functions. The search can then be done without taking the
         inode_hash_lock. Care must be taken because the object may be being
         deleted and no wait is made.
    
       - Allow iunique() to avoid taking the inode_hash_lock.
    
       - Allow AFS's callback processing to avoid taking the inode_hash_lock
         when using the inode table to find an inode to notify.
    
       - Improve Ext4's time updating. Konstantin Khlebnikov said "For now,
         I've plugged this issue with try-lock in ext4 lazy time update.
         This solution is much better."
    
      Then there's a set of changes to make a number of improvements to the
      AFS driver:
    
       - Improve callback (ie. third party change notification) processing
         by:
    
          (a) Relying more on the fact we're doing this under RCU and by
              using fewer locks. This makes use of the RCU-based inode
              searching outlined above.
    
          (b) Moving to keeping volumes in a tree indexed by volume ID
              rather than a flat list.
    
          (c) Making the server and volume records logically part of the
              cell. This means that a server record now points directly at
              the cell and the tree of volumes is there. This removes an N:M
              mapping table, simplifying things.
    
       - Improve keeping NAT or firewall channels open for the server
         callbacks to reach the client by actively polling the fileserver on
         a timed basis, instead of only doing it when we have an operation
         to process.
    
       - Improving detection of delayed or lost callbacks by including the
         parent directory in the list of file IDs to be queried when doing a
         bulk status fetch from lookup. We can then check to see if our copy
         of the directory has changed under us without us getting notified.
    
       - Determine aliasing of cells (such as a cell that is pointed to be a
         DNS alias). This allows us to avoid having ambiguity due to
         apparently different cells using the same volume and file servers.
    
       - Improve the fileserver rotation to do more probing when it detects
         that all of the addresses to a server are listed as non-responsive.
         It's possible that an address that previously stopped responding
         has become responsive again.
    
      Beyond that, lay some foundations for making some calls asynchronous:
    
       - Turn the fileserver cursor struct into a general operation struct
         and hang the parameters off of that rather than keeping them in
         local variables and hang results off of that rather than the call
         struct.
    
       - Implement some general operation handling code and simplify the
         callers of operations that affect a volume or a volume component
         (such as a file). Most of the operation is now done by core code.
    
       - Operations are supplied with a table of operations to issue
         different variants of RPCs and to manage the completion, where all
         the required data is held in the operation object, thereby allowing
         these to be called from a workqueue.
    
       - Put the standard "if (begin), while(select), call op, end" sequence
         into a canned function that just emulates the current behaviour for
         now.
    
      There are also some fixes interspersed:
    
       - Don't let the EACCES from ICMP6 mapping reach the user as such,
         since it's confusing as to whether it's a filesystem error. Convert
         it to EHOSTUNREACH.
    
       - Don't use the epoch value acquired through probing a server. If we
         have two servers with the same UUID but in different cells, it's
         hard to draw conclusions from them having different epoch values.
    
       - Don't interpret the argument to the CB.ProbeUuid RPC as a
         fileserver UUID and look up a fileserver from it.
    
       - Deal with servers in different cells having the same UUIDs. In the
         event that a CB.InitCallBackState3 RPC is received, we have to
         break the callback promises for every server record matching that
         UUID.
    
       - Don't let afs_statfs return values that go below 0.
    
       - Don't use running fileserver probe state to make server selection
         and address selection decisions on. Only make decisions on final
         state as the running state is cleared at the start of probing"
    
    Acked-by: Al Viro <viro@zeniv.linux.org.uk> (fs/inode.c part)
    
    * tag 'afs-next-20200604' of git://git.kernel.org/pub/scm/linux/kernel/git/dhowells/linux-fs: (27 commits)
      afs: Adjust the fileserver rotation algorithm to reprobe/retry more quickly
      afs: Show more a bit more server state in /proc/net/afs/servers
      afs: Don't use probe running state to make decisions outside probe code
      afs: Fix afs_statfs() to not let the values go below zero
      afs: Fix the by-UUID server tree to allow servers with the same UUID
      afs: Reorganise volume and server trees to be rooted on the cell
      afs: Add a tracepoint to track the lifetime of the afs_volume struct
      afs: Detect cell aliases 3 - YFS Cells with a canonical cell name op
      afs: Detect cell aliases 2 - Cells with no root volumes
      afs: Detect cell aliases 1 - Cells with root volumes
      afs: Implement client support for the YFSVL.GetCellName RPC op
      afs: Retain more of the VLDB record for alias detection
      afs: Fix handling of CB.ProbeUuid cache manager op
      afs: Don't get epoch from a server because it may be ambiguous
      afs: Build an abstraction around an "operation" concept
      afs: Rename struct afs_fs_cursor to afs_operation
      afs: Remove the error argument from afs_protocol_error()
      afs: Set error flag rather than return error from file status decode
      afs: Make callback processing more efficient.
      afs: Show more information in /proc/net/afs/servers
      ...

commit 20325960f8750165964a6891a733e4cc15d19076
Author: David Howells <dhowells@redhat.com>
Date:   Thu Apr 30 01:03:49 2020 +0100

    afs: Reorganise volume and server trees to be rooted on the cell
    
    Reorganise afs_volume objects such that they're in a tree keyed on volume
    ID, rooted at on an afs_cell object rather than being in multiple trees,
    each of which is rooted on an afs_server object.
    
    afs_server structs become per-cell and acquire a pointer to the cell.
    
    The process of breaking a callback then starts with finding the server by
    its network address, following that to the cell and then looking up each
    volume ID in the volume tree.
    
    This is simpler than the afs_vol_interest/afs_cb_interest N:M mapping web
    and allows those structs and the code for maintaining them to be simplified
    or removed.
    
    It does make a couple of things a bit more tricky, though:
    
     (1) Operations now start with a volume, not a server, so there can be more
         than one answer as to whether or not the server we'll end up using
         supports the FS.InlineBulkStatus RPC.
    
     (2) CB RPC operations that specify the server UUID.  There's still a tree
         of servers by UUID on the afs_net struct, but the UUIDs in it aren't
         guaranteed unique.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index bd4d8e5efe59..b7fb5f98f80c 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -184,7 +184,6 @@ void afs_put_call(struct afs_call *call)
 			call->type->destructor(call);
 
 		afs_unuse_server_notime(call->net, call->server, afs_server_trace_put_call);
-		afs_put_cb_interest(call->net, call->cbi);
 		afs_put_addrlist(call->alist);
 		kfree(call->request);
 
@@ -550,9 +549,9 @@ static void afs_deliver_to_call(struct afs_call *call)
 		case 0:
 			afs_queue_call_work(call);
 			if (state == AFS_CALL_CL_PROC_REPLY) {
-				if (call->cbi)
+				if (call->op)
 					set_bit(AFS_SERVER_FL_MAY_HAVE_CB,
-						&call->cbi->server->flags);
+						&call->op->server->flags);
 				goto call_complete;
 			}
 			ASSERTCMP(state, >, AFS_CALL_CL_PROC_REPLY);

commit e49c7b2f6de7ff81ca34c56e4eeb4fa740c099f2
Author: David Howells <dhowells@redhat.com>
Date:   Fri Apr 10 20:51:51 2020 +0100

    afs: Build an abstraction around an "operation" concept
    
    Turn the afs_operation struct into the main way that most fileserver
    operations are managed.  Various things are added to the struct, including
    the following:
    
     (1) All the parameters and results of the relevant operations are moved
         into it, removing corresponding fields from the afs_call struct.
         afs_call gets a pointer to the op.
    
     (2) The target volume is made the main focus of the operation, rather than
         the target vnode(s), and a bunch of op->vnode->volume are made
         op->volume instead.
    
     (3) Two vnode records are defined (op->file[]) for the vnode(s) involved
         in most operations.  The vnode record (struct afs_vnode_param)
         contains:
    
            - The vnode pointer.
    
            - The fid of the vnode to be included in the parameters or that was
              returned in the reply (eg. FS.MakeDir).
    
            - The status and callback information that may be returned in the
              reply about the vnode.
    
            - Callback break and data version tracking for detecting
              simultaneous third-parth changes.
    
     (4) Pointers to dentries to be updated with new inodes.
    
     (5) An operations table pointer.  The table includes pointers to functions
         for issuing AFS and YFS-variant RPCs, handling the success and abort
         of an operation and handling post-I/O-lock local editing of a
         directory.
    
    To make this work, the following function restructuring is made:
    
     (A) The rotation loop that issues calls to fileservers that can be found
         in each function that wants to issue an RPC (such as afs_mkdir()) is
         extracted out into common code, in a new file called fs_operation.c.
    
     (B) The rotation loops, such as the one in afs_mkdir(), are replaced with
         a much smaller piece of code that allocates an operation, sets the
         parameters and then calls out to the common code to do the actual
         work.
    
     (C) The code for handling the success and failure of an operation are
         moved into operation functions (as (5) above) and these are called
         from the core code at appropriate times.
    
     (D) The pseudo inode getting stuff used by the dynamic root code is moved
         over into dynroot.c.
    
     (E) struct afs_iget_data is absorbed into the operation struct and
         afs_iget() expects to be given an op pointer and a vnode record.
    
     (F) Point (E) doesn't work for the root dir of a volume, but we know the
         FID in advance (it's always vnode 1, unique 1), so a separate inode
         getter, afs_root_iget(), is provided to special-case that.
    
     (G) The inode status init/update functions now also take an op and a vnode
         record.
    
     (H) The RPC marshalling functions now, for the most part, just take an
         afs_operation struct as their only argument.  All the data they need
         is held there.  The result delivery functions write their answers
         there as well.
    
     (I) The call is attached to the operation and then the operation core does
         the waiting.
    
    And then the new operation code is, for the moment, made to just initialise
    the operation, get the appropriate vnode I/O locks and do the same rotation
    loop as before.
    
    This lays the foundation for the following changes in the future:
    
     (*) Overhauling the rotation (again).
    
     (*) Support for asynchronous I/O, where the fileserver rotation must be
         done asynchronously also.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 00b87bac4fec..bd4d8e5efe59 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -283,18 +283,19 @@ static void afs_load_bvec(struct afs_call *call, struct msghdr *msg,
 			  struct bio_vec *bv, pgoff_t first, pgoff_t last,
 			  unsigned offset)
 {
+	struct afs_operation *op = call->op;
 	struct page *pages[AFS_BVEC_MAX];
 	unsigned int nr, n, i, to, bytes = 0;
 
 	nr = min_t(pgoff_t, last - first + 1, AFS_BVEC_MAX);
-	n = find_get_pages_contig(call->mapping, first, nr, pages);
+	n = find_get_pages_contig(op->store.mapping, first, nr, pages);
 	ASSERTCMP(n, ==, nr);
 
 	msg->msg_flags |= MSG_MORE;
 	for (i = 0; i < nr; i++) {
 		to = PAGE_SIZE;
 		if (first + i >= last) {
-			to = call->last_to;
+			to = op->store.last_to;
 			msg->msg_flags &= ~MSG_MORE;
 		}
 		bv[i].bv_page = pages[i];
@@ -324,13 +325,14 @@ static void afs_notify_end_request_tx(struct sock *sock,
  */
 static int afs_send_pages(struct afs_call *call, struct msghdr *msg)
 {
+	struct afs_operation *op = call->op;
 	struct bio_vec bv[AFS_BVEC_MAX];
 	unsigned int bytes, nr, loop, offset;
-	pgoff_t first = call->first, last = call->last;
+	pgoff_t first = op->store.first, last = op->store.last;
 	int ret;
 
-	offset = call->first_offset;
-	call->first_offset = 0;
+	offset = op->store.first_offset;
+	op->store.first_offset = 0;
 
 	do {
 		afs_load_bvec(call, msg, bv, first, last, offset);
@@ -340,7 +342,7 @@ static int afs_send_pages(struct afs_call *call, struct msghdr *msg)
 		bytes = msg->msg_iter.count;
 		nr = msg->msg_iter.nr_segs;
 
-		ret = rxrpc_kernel_send_data(call->net->socket, call->rxcall, msg,
+		ret = rxrpc_kernel_send_data(op->net->socket, call->rxcall, msg,
 					     bytes, afs_notify_end_request_tx);
 		for (loop = 0; loop < nr; loop++)
 			put_page(bv[loop].bv_page);
@@ -350,7 +352,7 @@ static int afs_send_pages(struct afs_call *call, struct msghdr *msg)
 		first += nr;
 	} while (first <= last);
 
-	trace_afs_sent_pages(call, call->first, last, first, ret);
+	trace_afs_sent_pages(call, op->store.first, last, first, ret);
 	return ret;
 }
 
@@ -385,16 +387,18 @@ void afs_make_call(struct afs_addr_cursor *ac, struct afs_call *call, gfp_t gfp)
 	 */
 	tx_total_len = call->request_size;
 	if (call->send_pages) {
-		if (call->last == call->first) {
-			tx_total_len += call->last_to - call->first_offset;
+		struct afs_operation *op = call->op;
+
+		if (op->store.last == op->store.first) {
+			tx_total_len += op->store.last_to - op->store.first_offset;
 		} else {
 			/* It looks mathematically like you should be able to
 			 * combine the following lines with the ones above, but
 			 * unsigned arithmetic is fun when it wraps...
 			 */
-			tx_total_len += PAGE_SIZE - call->first_offset;
-			tx_total_len += call->last_to;
-			tx_total_len += (call->last - call->first - 1) * PAGE_SIZE;
+			tx_total_len += PAGE_SIZE - op->store.first_offset;
+			tx_total_len += op->store.last_to;
+			tx_total_len += (op->store.last - op->store.first - 1) * PAGE_SIZE;
 		}
 	}
 

commit 7126ead910aa9fcc9e16e9e7a8c9179658261f1d
Author: David Howells <dhowells@redhat.com>
Date:   Wed Apr 8 16:49:08 2020 +0100

    afs: Remove the error argument from afs_protocol_error()
    
    Remove the error argument from afs_protocol_error() as it's always
    -EBADMSG.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index c84d571782d7..00b87bac4fec 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -961,11 +961,11 @@ int afs_extract_data(struct afs_call *call, bool want_more)
 /*
  * Log protocol error production.
  */
-noinline int afs_protocol_error(struct afs_call *call, int error,
+noinline int afs_protocol_error(struct afs_call *call,
 				enum afs_eproto_cause cause)
 {
-	trace_afs_protocol_error(call, error, cause);
+	trace_afs_protocol_error(call, cause);
 	if (call)
 		call->unmarshalling_error = true;
-	return error;
+	return -EBADMSG;
 }

commit 38355eec6a7d2b8f2f313f9174736dc877744e59
Author: David Howells <dhowells@redhat.com>
Date:   Wed Apr 8 16:13:20 2020 +0100

    afs: Set error flag rather than return error from file status decode
    
    Set a flag in the call struct to indicate an unmarshalling error rather
    than return and handle an error from the decoding of file statuses.  This
    flag is checked on a successful return from the delivery function.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index ab2962fff1fb..c84d571782d7 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -540,6 +540,8 @@ static void afs_deliver_to_call(struct afs_call *call)
 
 		ret = call->type->deliver(call);
 		state = READ_ONCE(call->state);
+		if (ret == 0 && call->unmarshalling_error)
+			ret = -EBADMSG;
 		switch (ret) {
 		case 0:
 			afs_queue_call_work(call);
@@ -963,5 +965,7 @@ noinline int afs_protocol_error(struct afs_call *call, int error,
 				enum afs_eproto_cause cause)
 {
 	trace_afs_protocol_error(call, error, cause);
+	if (call)
+		call->unmarshalling_error = true;
 	return error;
 }

commit 977e5f8ed0ab2786755f8d2a96b78a3c7320f7c4
Author: David Howells <dhowells@redhat.com>
Date:   Fri Apr 17 17:31:26 2020 +0100

    afs: Split the usage count on struct afs_server
    
    Split the usage count on the afs_server struct to have an active count that
    registers who's actually using it separately from the reference count on
    the object.
    
    This allows a future patch to dispatch polling probes without advancing the
    "unuse" time into the future each time we emit a probe, which would
    otherwise prevent unused server records from expiring.
    
    Included in this:
    
     (1) The latter part of afs_destroy_server() in which the RCU destruction
         of afs_server objects is invoked and the outstanding server count is
         decremented is split out into __afs_put_server().
    
     (2) afs_put_server() now calls __afs_put_server() rather then setting the
         management timer.
    
     (3) The calls begun by afs_fs_give_up_all_callbacks() and
         afs_fs_get_capabilities() can now take a ref on the server record, so
         afs_destroy_server() can just drop its ref and needn't wait for the
         completion of these calls.  They'll put the ref when they're done.
    
     (4) Because of (3), afs_fs_probe_done() no longer needs to wake up
         afs_destroy_server() with server->probe_outstanding.
    
     (5) afs_gc_servers can be simplified.  It only needs to check if
         server->active is 0 rather than playing games with the refcount.
    
     (6) afs_manage_servers() can propose a server for gc if usage == 0 rather
         than if ref == 1.  The gc is effected by (5).
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 1ecc67da6c1a..ab2962fff1fb 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -183,7 +183,7 @@ void afs_put_call(struct afs_call *call)
 		if (call->type->destructor)
 			call->type->destructor(call);
 
-		afs_put_server(call->net, call->server, afs_server_trace_put_call);
+		afs_unuse_server_notime(call->net, call->server, afs_server_trace_put_call);
 		afs_put_cb_interest(call->net, call->cbi);
 		afs_put_addrlist(call->alist);
 		kfree(call->request);

commit 298cd88a66a02c899772ffafbf648786ceb5ab95
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu May 28 07:12:35 2020 +0200

    rxrpc: add rxrpc_sock_set_min_security_level
    
    Add a helper to directly set the RXRPC_MIN_SECURITY_LEVEL sockopt from
    kernel space without going through a fake uaccess.
    
    Thanks to David Howells for the documentation updates.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: David Howells <dhowells@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 1ecc67da6c1a..e313dae01674 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -37,7 +37,6 @@ int afs_open_socket(struct afs_net *net)
 {
 	struct sockaddr_rxrpc srx;
 	struct socket *socket;
-	unsigned int min_level;
 	int ret;
 
 	_enter("");
@@ -57,9 +56,8 @@ int afs_open_socket(struct afs_net *net)
 	srx.transport.sin6.sin6_family	= AF_INET6;
 	srx.transport.sin6.sin6_port	= htons(AFS_CM_PORT);
 
-	min_level = RXRPC_SECURITY_ENCRYPT;
-	ret = kernel_setsockopt(socket, SOL_RXRPC, RXRPC_MIN_SECURITY_LEVEL,
-				(void *)&min_level, sizeof(min_level));
+	ret = rxrpc_sock_set_min_security_level(socket->sk,
+						RXRPC_SECURITY_ENCRYPT);
 	if (ret < 0)
 		goto error_2;
 

commit 7d7587db0d7fd1138f2afcffdc46a8e15630b944
Author: David Howells <dhowells@redhat.com>
Date:   Thu Mar 12 21:40:06 2020 +0000

    afs: Fix client call Rx-phase signal handling
    
    Fix the handling of signals in client rxrpc calls made by the afs
    filesystem.  Ignore signals completely, leaving call abandonment or
    connection loss to be detected by timeouts inside AF_RXRPC.
    
    Allowing a filesystem call to be interrupted after the entire request has
    been transmitted and an abort sent means that the server may or may not
    have done the action - and we don't know.  It may even be worse than that
    for older servers.
    
    Fixes: bc5e3a546d55 ("rxrpc: Use MSG_WAITALL to tell sendmsg() to temporarily ignore signals")
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 972e3aafa361..1ecc67da6c1a 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -604,11 +604,7 @@ static void afs_deliver_to_call(struct afs_call *call)
 long afs_wait_for_call_to_complete(struct afs_call *call,
 				   struct afs_addr_cursor *ac)
 {
-	signed long rtt2, timeout;
 	long ret;
-	bool stalled = false;
-	u64 rtt;
-	u32 life, last_life;
 	bool rxrpc_complete = false;
 
 	DECLARE_WAITQUEUE(myself, current);
@@ -619,14 +615,6 @@ long afs_wait_for_call_to_complete(struct afs_call *call,
 	if (ret < 0)
 		goto out;
 
-	rtt = rxrpc_kernel_get_rtt(call->net->socket, call->rxcall);
-	rtt2 = nsecs_to_jiffies64(rtt) * 2;
-	if (rtt2 < 2)
-		rtt2 = 2;
-
-	timeout = rtt2;
-	rxrpc_kernel_check_life(call->net->socket, call->rxcall, &last_life);
-
 	add_wait_queue(&call->waitq, &myself);
 	for (;;) {
 		set_current_state(TASK_UNINTERRUPTIBLE);
@@ -637,37 +625,19 @@ long afs_wait_for_call_to_complete(struct afs_call *call,
 			call->need_attention = false;
 			__set_current_state(TASK_RUNNING);
 			afs_deliver_to_call(call);
-			timeout = rtt2;
 			continue;
 		}
 
 		if (afs_check_call_state(call, AFS_CALL_COMPLETE))
 			break;
 
-		if (!rxrpc_kernel_check_life(call->net->socket, call->rxcall, &life)) {
+		if (!rxrpc_kernel_check_life(call->net->socket, call->rxcall)) {
 			/* rxrpc terminated the call. */
 			rxrpc_complete = true;
 			break;
 		}
 
-		if (call->intr && timeout == 0 &&
-		    life == last_life && signal_pending(current)) {
-			if (stalled)
-				break;
-			__set_current_state(TASK_RUNNING);
-			rxrpc_kernel_probe_life(call->net->socket, call->rxcall);
-			timeout = rtt2;
-			stalled = true;
-			continue;
-		}
-
-		if (life != last_life) {
-			timeout = rtt2;
-			last_life = life;
-			stalled = false;
-		}
-
-		timeout = schedule_timeout(timeout);
+		schedule();
 	}
 
 	remove_wait_queue(&call->waitq, &myself);

commit dde9f095583b3f375ba23979045ee10dfcebec2f
Author: David Howells <dhowells@redhat.com>
Date:   Fri Mar 13 13:46:08 2020 +0000

    afs: Fix handling of an abort from a service handler
    
    When an AFS service handler function aborts a call, AF_RXRPC marks the call
    as complete - which means that it's not going to get any more packets from
    the receiver.  This is a problem because reception of the final ACK is what
    triggers afs_deliver_to_call() to drop the final ref on the afs_call
    object.
    
    Instead, aborted AFS service calls may then just sit around waiting for
    ever or until they're displaced by a new call on the same connection
    channel or a connection-level abort.
    
    Fix this by calling afs_set_call_complete() to finalise the afs_call struct
    representing the call.
    
    However, we then need to drop the ref that stops the call from being
    deallocated.  We can do this in afs_set_call_complete(), as the work queue
    is holding a separate ref of its own, but then we shouldn't do it in
    afs_process_async_call() and afs_delete_async_call().
    
    call->drop_ref is set to indicate that a ref needs dropping for a call and
    this is dealt with when we transition a call to AFS_CALL_COMPLETE.
    
    But then we also need to get rid of the ref that pins an asynchronous
    client call.  We can do this by the same mechanism, setting call->drop_ref
    for an async client call too.
    
    We can also get rid of call->incoming since nothing ever sets it and only
    one thing ever checks it (futilely).
    
    
    A trace of the rxrpc_call and afs_call struct ref counting looks like:
    
              <idle>-0     [001] ..s5   164.764892: rxrpc_call: c=00000002 SEE u=3 sp=rxrpc_new_incoming_call+0x473/0xb34 a=00000000442095b5
              <idle>-0     [001] .Ns5   164.766001: rxrpc_call: c=00000002 QUE u=4 sp=rxrpc_propose_ACK+0xbe/0x551 a=00000000442095b5
              <idle>-0     [001] .Ns4   164.766005: rxrpc_call: c=00000002 PUT u=3 sp=rxrpc_new_incoming_call+0xa3f/0xb34 a=00000000442095b5
              <idle>-0     [001] .Ns7   164.766433: afs_call: c=00000002 WAKE  u=2 o=11 sp=rxrpc_notify_socket+0x196/0x33c
         kworker/1:2-1810  [001] ...1   164.768409: rxrpc_call: c=00000002 SEE u=3 sp=rxrpc_process_call+0x25/0x7ae a=00000000442095b5
         kworker/1:2-1810  [001] ...1   164.769439: rxrpc_tx_packet: c=00000002 e9f1a7a8:95786a88:00000008:09c5 00000001 00000000 02 22 ACK CallAck
         kworker/1:2-1810  [001] ...1   164.769459: rxrpc_call: c=00000002 PUT u=2 sp=rxrpc_process_call+0x74f/0x7ae a=00000000442095b5
         kworker/1:2-1810  [001] ...1   164.770794: afs_call: c=00000002 QUEUE u=3 o=12 sp=afs_deliver_to_call+0x449/0x72c
         kworker/1:2-1810  [001] ...1   164.770829: afs_call: c=00000002 PUT   u=2 o=12 sp=afs_process_async_call+0xdb/0x11e
         kworker/1:2-1810  [001] ...2   164.771084: rxrpc_abort: c=00000002 95786a88:00000008 s=0 a=1 e=1 K-1
         kworker/1:2-1810  [001] ...1   164.771461: rxrpc_tx_packet: c=00000002 e9f1a7a8:95786a88:00000008:09c5 00000002 00000000 04 00 ABORT CallAbort
         kworker/1:2-1810  [001] ...1   164.771466: afs_call: c=00000002 PUT   u=1 o=12 sp=SRXAFSCB_ProbeUuid+0xc1/0x106
    
    The abort generated in SRXAFSCB_ProbeUuid(), labelled "K-1", indicates that
    the local filesystem/cache manager didn't recognise the UUID as its own.
    
    Fixes: 2067b2b3f484 ("afs: Fix the CB.ProbeUuid service handler to reply correctly")
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 907d5948564a..972e3aafa361 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -18,7 +18,6 @@ struct workqueue_struct *afs_async_calls;
 
 static void afs_wake_up_call_waiter(struct sock *, struct rxrpc_call *, unsigned long);
 static void afs_wake_up_async_call(struct sock *, struct rxrpc_call *, unsigned long);
-static void afs_delete_async_call(struct work_struct *);
 static void afs_process_async_call(struct work_struct *);
 static void afs_rx_new_call(struct sock *, struct rxrpc_call *, unsigned long);
 static void afs_rx_discard_new_call(struct rxrpc_call *, unsigned long);
@@ -402,8 +401,10 @@ void afs_make_call(struct afs_addr_cursor *ac, struct afs_call *call, gfp_t gfp)
 	/* If the call is going to be asynchronous, we need an extra ref for
 	 * the call to hold itself so the caller need not hang on to its ref.
 	 */
-	if (call->async)
+	if (call->async) {
 		afs_get_call(call, afs_call_trace_get);
+		call->drop_ref = true;
+	}
 
 	/* create a call */
 	rxcall = rxrpc_kernel_begin_call(call->net->socket, srx, call->key,
@@ -585,8 +586,6 @@ static void afs_deliver_to_call(struct afs_call *call)
 done:
 	if (call->type->done)
 		call->type->done(call);
-	if (state == AFS_CALL_COMPLETE && call->incoming)
-		afs_put_call(call);
 out:
 	_leave("");
 	return;
@@ -745,21 +744,6 @@ static void afs_wake_up_async_call(struct sock *sk, struct rxrpc_call *rxcall,
 	}
 }
 
-/*
- * Delete an asynchronous call.  The work item carries a ref to the call struct
- * that we need to release.
- */
-static void afs_delete_async_call(struct work_struct *work)
-{
-	struct afs_call *call = container_of(work, struct afs_call, async_work);
-
-	_enter("");
-
-	afs_put_call(call);
-
-	_leave("");
-}
-
 /*
  * Perform I/O processing on an asynchronous call.  The work item carries a ref
  * to the call struct that we either need to release or to pass on.
@@ -775,16 +759,6 @@ static void afs_process_async_call(struct work_struct *work)
 		afs_deliver_to_call(call);
 	}
 
-	if (call->state == AFS_CALL_COMPLETE) {
-		/* We have two refs to release - one from the alloc and one
-		 * queued with the work item - and we can't just deallocate the
-		 * call because the work item may be queued again.
-		 */
-		call->async_work.func = afs_delete_async_call;
-		if (!queue_work(afs_async_calls, &call->async_work))
-			afs_put_call(call);
-	}
-
 	afs_put_call(call);
 	_leave("");
 }
@@ -811,6 +785,7 @@ void afs_charge_preallocation(struct work_struct *work)
 			if (!call)
 				break;
 
+			call->drop_ref = true;
 			call->async = true;
 			call->state = AFS_CALL_SV_AWAIT_OP_ID;
 			init_waitqueue_head(&call->waitq);

commit 4636cf184d6d9a92a56c2554681ea520dd4fe49a
Author: David Howells <dhowells@redhat.com>
Date:   Fri Mar 13 13:36:01 2020 +0000

    afs: Fix some tracing details
    
    Fix a couple of tracelines to indicate the usage count after the atomic op,
    not the usage count before it to be consistent with other afs and rxrpc
    trace lines.
    
    Change the wording of the afs_call_trace_work trace ID label from "WORK" to
    "QUEUE" to reflect the fact that it's queueing work, not doing work.
    
    Fixes: 341f741f04be ("afs: Refcount the afs_call struct")
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 4c28712bb7f6..907d5948564a 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -169,7 +169,7 @@ void afs_put_call(struct afs_call *call)
 	int n = atomic_dec_return(&call->usage);
 	int o = atomic_read(&net->nr_outstanding_calls);
 
-	trace_afs_call(call, afs_call_trace_put, n + 1, o,
+	trace_afs_call(call, afs_call_trace_put, n, o,
 		       __builtin_return_address(0));
 
 	ASSERTCMP(n, >=, 0);
@@ -736,7 +736,7 @@ static void afs_wake_up_async_call(struct sock *sk, struct rxrpc_call *rxcall,
 
 	u = atomic_fetch_add_unless(&call->usage, 1, 0);
 	if (u != 0) {
-		trace_afs_call(call, afs_call_trace_wake, u,
+		trace_afs_call(call, afs_call_trace_wake, u + 1,
 			       atomic_read(&call->net->nr_outstanding_calls),
 			       __builtin_return_address(0));
 

commit e138aa7d3271ac1b0690ae2c9b04d51468dce1d6
Author: David Howells <dhowells@redhat.com>
Date:   Fri Mar 13 09:22:09 2020 +0000

    rxrpc: Fix call interruptibility handling
    
    Fix the interruptibility of kernel-initiated client calls so that they're
    either only interruptible when they're waiting for a call slot to come
    available or they're not interruptible at all.  Either way, they're not
    interruptible during transmission.
    
    This should help prevent StoreData calls from being interrupted when
    writeback is in progress.  It doesn't, however, handle interruption during
    the receive phase.
    
    Userspace-initiated calls are still interruptable.  After the signal has
    been handled, sendmsg() will return the amount of data copied out of the
    buffer and userspace can perform another sendmsg() call to continue
    transmission.
    
    Fixes: bc5e3a546d55 ("rxrpc: Use MSG_WAITALL to tell sendmsg() to temporarily ignore signals")
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 58d396592250..4c28712bb7f6 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -413,7 +413,8 @@ void afs_make_call(struct afs_addr_cursor *ac, struct afs_call *call, gfp_t gfp)
 					  afs_wake_up_async_call :
 					  afs_wake_up_call_waiter),
 					 call->upgrade,
-					 call->intr,
+					 (call->intr ? RXRPC_PREINTERRUPTIBLE :
+					  RXRPC_UNINTERRUPTIBLE),
 					 call->debug_id);
 	if (IS_ERR(rxcall)) {
 		ret = PTR_ERR(rxcall);

commit 4a55d362ffe7caf099a01f6d2ed49a6ea03a4a88
Merge: 50b8b3f85a01 4fe171bb81b1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Nov 30 10:57:22 2019 -0800

    Merge tag 'afs-next-20191121' of git://git.kernel.org/pub/scm/linux/kernel/git/dhowells/linux-fs
    
    Pull AFS updates from David Howells:
     "Minor cleanups and fix:
    
       - Minor fix to make some debugging statements display information
         from the correct iov_iter.
    
       - Rename some members and variables to make things more obvious or
         consistent.
    
       - Provide a helper to wrap increments of the usage count on the
         afs_read struct.
    
       - Use scnprintf() to print into a stack buffer rather than sprintf().
    
       - Remove some set but unused variables"
    
    * tag 'afs-next-20191121' of git://git.kernel.org/pub/scm/linux/kernel/git/dhowells/linux-fs:
      afs: Remove set but not used variable 'ret'
      afs: Remove set but not used variables 'before', 'after'
      afs: xattr: use scnprintf
      afs: Introduce an afs_get_read() refcount helper
      afs: Rename desc -> req in afs_fetch_data()
      afs: Switch the naming of call->iter and call->_iter
      afs: Use call->_iter not &call->iter in debugging statements

commit fc276122496df13341c1429fbd430d02462815c8
Author: David Howells <dhowells@redhat.com>
Date:   Thu Nov 21 09:12:17 2019 +0000

    afs: Switch the naming of call->iter and call->_iter
    
    Change the name of call->iter to call->def_iter to represent the default
    iterator.
    
    Change the name of call->_iter to call->iter to represent the iterator
    actually being used.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 0e5269374ac1..aa6f5ff83733 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -152,7 +152,7 @@ static struct afs_call *afs_alloc_call(struct afs_net *net,
 	INIT_WORK(&call->async_work, afs_process_async_call);
 	init_waitqueue_head(&call->waitq);
 	spin_lock_init(&call->state_lock);
-	call->_iter = &call->iter;
+	call->iter = &call->def_iter;
 
 	o = atomic_inc_return(&net->nr_outstanding_calls);
 	trace_afs_call(call, afs_call_trace_alloc, 1, o,
@@ -513,12 +513,12 @@ static void afs_deliver_to_call(struct afs_call *call)
 	       state == AFS_CALL_SV_AWAIT_ACK
 	       ) {
 		if (state == AFS_CALL_SV_AWAIT_ACK) {
-			iov_iter_kvec(&call->iter, READ, NULL, 0, 0);
+			iov_iter_kvec(&call->def_iter, READ, NULL, 0, 0);
 			ret = rxrpc_kernel_recv_data(call->net->socket,
-						     call->rxcall, &call->iter,
+						     call->rxcall, &call->def_iter,
 						     false, &remote_abort,
 						     &call->service_id);
-			trace_afs_receive_data(call, &call->iter, false, ret);
+			trace_afs_receive_data(call, &call->def_iter, false, ret);
 
 			if (ret == -EINPROGRESS || ret == -EAGAIN)
 				return;
@@ -858,7 +858,7 @@ static int afs_deliver_cm_op_id(struct afs_call *call)
 {
 	int ret;
 
-	_enter("{%zu}", iov_iter_count(call->_iter));
+	_enter("{%zu}", iov_iter_count(call->iter));
 
 	/* the operation ID forms the first four bytes of the request data */
 	ret = afs_extract_data(call, true);
@@ -974,7 +974,7 @@ void afs_send_simple_reply(struct afs_call *call, const void *buf, size_t len)
 int afs_extract_data(struct afs_call *call, bool want_more)
 {
 	struct afs_net *net = call->net;
-	struct iov_iter *iter = call->_iter;
+	struct iov_iter *iter = call->iter;
 	enum afs_call_state state;
 	u32 remote_abort = 0;
 	int ret;

commit c74386d50fbaf4a54fd3fe560f1abc709c0cff4b
Author: David Howells <dhowells@redhat.com>
Date:   Tue Nov 19 21:00:36 2019 +0000

    afs: Fix missing timeout reset
    
    In afs_wait_for_call_to_complete(), rather than immediately aborting an
    operation if a signal occurs, the code attempts to wait for it to
    complete, using a schedule timeout of 2*RTT (or min 2 jiffies) and a
    check that we're still receiving relevant packets from the server before
    we consider aborting the call.  We may even ping the server to check on
    the status of the call.
    
    However, there's a missing timeout reset in the event that we do
    actually get a packet to process, such that if we then get a couple of
    short stalls, we then time out when progress is actually being made.
    
    Fix this by resetting the timeout any time we get something to process.
    If it's the failure of the call then the call state will get changed and
    we'll exit the loop shortly thereafter.
    
    A symptom of this is data fetches and stores failing with EINTR when
    they really shouldn't.
    
    Fixes: bc5e3a546d55 ("rxrpc: Use MSG_WAITALL to tell sendmsg() to temporarily ignore signals")
    Signed-off-by: David Howells <dhowells@redhat.com>
    Reviewed-by: Marc Dionne <marc.dionne@auristor.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 0e5269374ac1..61498d9f06ef 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -637,6 +637,7 @@ long afs_wait_for_call_to_complete(struct afs_call *call,
 			call->need_attention = false;
 			__set_current_state(TASK_RUNNING);
 			afs_deliver_to_call(call);
+			timeout = rtt2;
 			continue;
 		}
 

commit 8dda9957e3a1c871dfbabf84c4760f9b26032442
Merge: 25cd6f355dab 1eda8bab70ca
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jul 10 20:55:33 2019 -0700

    Merge tag 'afs-next-20190628' of git://git.kernel.org/pub/scm/linux/kernel/git/dhowells/linux-fs
    
    Pull afs updates from David Howells:
     "A set of minor changes for AFS:
    
       - Remove an unnecessary check in afs_unlink()
    
       - Add a tracepoint for tracking callback management
    
       - Add a tracepoint for afs_server object usage
    
       - Use struct_size()
    
       - Add mappings for AFS UAE abort codes to Linux error codes, using
         symbolic names rather than hex numbers in the .c file"
    
    * tag 'afs-next-20190628' of git://git.kernel.org/pub/scm/linux/kernel/git/dhowells/linux-fs:
      afs: Add support for the UAE error table
      fs/afs: use struct_size() in kzalloc()
      afs: Trace afs_server usage
      afs: Add some callback management tracepoints
      afs: afs_unlink() doesn't need to check dentry->d_inode

commit 452181936931f0f08923aba5e04e1e9ef58c389f
Author: David Howells <dhowells@redhat.com>
Date:   Thu Jun 20 18:12:17 2019 +0100

    afs: Trace afs_server usage
    
    Add a tracepoint (afs_server) to track the afs_server object usage count.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 4fa5ce92b9b9..32d1721f41db 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -188,7 +188,7 @@ void afs_put_call(struct afs_call *call)
 		if (call->type->destructor)
 			call->type->destructor(call);
 
-		afs_put_server(call->net, call->server);
+		afs_put_server(call->net, call->server, afs_server_trace_put_call);
 		afs_put_cb_interest(call->net, call->cbi);
 		afs_put_addrlist(call->alist);
 		kfree(call->request);

commit 2874c5fd284268364ece81a7bd936f3c8168e567
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 27 08:55:01 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 152
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license as published by
      the free software foundation either version 2 of the license or at
      your option any later version
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-or-later
    
    has been chosen to replace the boilerplate/reference in 3029 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190527070032.746973796@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 4fa5ce92b9b9..d1dde2834b6d 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -1,12 +1,8 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
 /* Maintain an RxRPC server socket to do AFS communications through
  *
  * Copyright (C) 2007 Red Hat, Inc. All Rights Reserved.
  * Written by David Howells (dhowells@redhat.com)
- *
- * This program is free software; you can redistribute it and/or
- * modify it under the terms of the GNU General Public License
- * as published by the Free Software Foundation; either version
- * 2 of the License, or (at your option) any later version.
  */
 
 #include <linux/slab.h>

commit 4571577f16c82f8db8133b228cdca5fd61042c77
Author: David Howells <dhowells@redhat.com>
Date:   Tue May 14 11:44:26 2019 +0100

    afs: Always get the reply time
    
    Always ask for the reply time from AF_RXRPC as it's used to calculate the
    callback expiry time and lock expiry times, so it's needed by most FS
    operations.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index d63e0c9c09e7..4fa5ce92b9b9 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -534,11 +534,11 @@ static void afs_deliver_to_call(struct afs_call *call)
 			return;
 		}
 
-		if (call->want_reply_time &&
+		if (!call->have_reply_time &&
 		    rxrpc_kernel_get_reply_time(call->net->socket,
 						call->rxcall,
 						&call->reply_time))
-			call->want_reply_time = false;
+			call->have_reply_time = true;
 
 		ret = call->type->deliver(call);
 		state = READ_ONCE(call->state);

commit ffba718e935402e7f42b8cd5d1e00e4a3907d361
Author: David Howells <dhowells@redhat.com>
Date:   Thu May 9 22:22:50 2019 +0100

    afs: Get rid of afs_call::reply[]
    
    Replace the afs_call::reply[] array with a bunch of typed members so that
    the compiler can use type-checking on them.  It's also easier for the eye
    to see what's going on.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 02a39e6adf63..d63e0c9c09e7 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -188,7 +188,7 @@ void afs_put_call(struct afs_call *call)
 		if (call->type->destructor)
 			call->type->destructor(call);
 
-		afs_put_server(call->net, call->cm_server);
+		afs_put_server(call->net, call->server);
 		afs_put_cb_interest(call->net, call->cbi);
 		afs_put_addrlist(call->alist);
 		kfree(call->request);
@@ -696,10 +696,9 @@ long afs_wait_for_call_to_complete(struct afs_call *call,
 	ret = ac->error;
 	switch (ret) {
 	case 0:
-		if (call->ret_reply0) {
-			ret = (long)call->reply[0];
-			call->reply[0] = NULL;
-		}
+		ret = call->ret0;
+		call->ret0 = 0;
+
 		/* Fall through */
 	case -ECONNABORTED:
 		ac->responded = true;

commit 20b8391fff56f64893233a772a81adc392a69121
Author: David Howells <dhowells@redhat.com>
Date:   Wed May 8 16:16:31 2019 +0100

    afs: Make some RPC operations non-interruptible
    
    Make certain RPC operations non-interruptible, including:
    
     (*) Set attributes
     (*) Store data
    
         We don't want to get interrupted during a flush on close, flush on
         unlock, writeback or an inode update, leaving us in a state where we
         still need to do the writeback or update.
    
     (*) Extend lock
     (*) Release lock
    
         We don't want to get lock extension interrupted as the file locks on
         the server are time-limited.  Interruption during lock release is less
         of an issue since the lock is time-limited, but it's better to
         complete the release to avoid a several-minute wait to recover it.
    
         *Setting* the lock isn't a problem if it's interrupted since we can
          just return to the user and tell them they were interrupted - at
          which point they can elect to retry.
    
     (*) Silly unlink
    
         We want to remove silly unlink files if we can, rather than leaving
         them for the salvager to clear up.
    
    Note that whilst these calls are no longer interruptible, they do have
    timeouts on them, so if the server stops responding the call will fail with
    something like ETIME or ECONNRESET.
    
    Without this, the following:
    
            kAFS: Unexpected error from FS.StoreData -512
    
    appears in dmesg when a pending store data gets interrupted and some
    processes may just hang.
    
    Additionally, make the code that checks/updates the server record ignore
    failure due to interruption if the main call is uninterruptible and if the
    server has an address list.  The next op will check it again since the
    expiration time on the old list has past.
    
    Fixes: d2ddc776a458 ("afs: Overhaul volume and server record caching and fileserver rotation")
    Reported-by: Jonathan Billings <jsbillings@jsbillings.org>
    Reported-by: Marc Dionne <marc.dionne@auristor.com>
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 87763379952d..02a39e6adf63 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -417,7 +417,7 @@ void afs_make_call(struct afs_addr_cursor *ac, struct afs_call *call, gfp_t gfp)
 					  afs_wake_up_async_call :
 					  afs_wake_up_call_waiter),
 					 call->upgrade,
-					 true,
+					 call->intr,
 					 call->debug_id);
 	if (IS_ERR(rxcall)) {
 		ret = PTR_ERR(rxcall);
@@ -653,7 +653,7 @@ long afs_wait_for_call_to_complete(struct afs_call *call,
 			break;
 		}
 
-		if (timeout == 0 &&
+		if (call->intr && timeout == 0 &&
 		    life == last_life && signal_pending(current)) {
 			if (stalled)
 				break;

commit b960a34b73e4c1c972623bc2076e24b97588d09e
Author: David Howells <dhowells@redhat.com>
Date:   Thu May 9 08:21:21 2019 +0100

    rxrpc: Allow the kernel to mark a call as being non-interruptible
    
    Allow kernel services using AF_RXRPC to indicate that a call should be
    non-interruptible.  This allows kafs to make things like lock-extension and
    writeback data storage calls non-interruptible.
    
    If this is set, signals will be ignored for operations on that call where
    possible - such as waiting to get a call channel on an rxrpc connection.
    
    It doesn't prevent UDP sendmsg from being interrupted, but that will be
    handled by packet retransmission.
    
    rxrpc_kernel_recv_data() isn't affected by this since that never waits,
    preferring instead to return -EAGAIN and leave the waiting to the caller.
    
    Userspace initiated calls can't be set to be uninterruptible at this time.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 4974defb4592..87763379952d 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -417,6 +417,7 @@ void afs_make_call(struct afs_addr_cursor *ac, struct afs_call *call, gfp_t gfp)
 					  afs_wake_up_async_call :
 					  afs_wake_up_call_waiter),
 					 call->upgrade,
+					 true,
 					 call->debug_id);
 	if (IS_ERR(rxcall)) {
 		ret = PTR_ERR(rxcall);

commit 94f699c9cdb11b8f53cb70624b69aeae16f26db2
Author: David Howells <dhowells@redhat.com>
Date:   Thu May 16 13:21:59 2019 +0100

    afs: Fix the maximum lifespan of VL and probe calls
    
    If an older AFS server doesn't support an operation, it may accept the call
    and then sit on it forever, happily responding to pings that make kafs
    think that the call is still alive.
    
    Fix this by setting the maximum lifespan of Volume Location service calls
    in particular and probe calls in general so that they don't run on
    endlessly if they're not supported.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index a34a89c75c6a..4974defb4592 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -426,6 +426,10 @@ void afs_make_call(struct afs_addr_cursor *ac, struct afs_call *call, gfp_t gfp)
 
 	call->rxcall = rxcall;
 
+	if (call->max_lifespan)
+		rxrpc_kernel_set_max_life(call->net->socket, rxcall,
+					  call->max_lifespan);
+
 	/* send the request */
 	iov[0].iov_base	= call->request;
 	iov[0].iov_len	= call->request_size;

commit e5fef2a9732580c5bd30c0097f5e9091a3d58ce5
Merge: 149e703cb8bf f5e4546347bc
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 7 20:51:58 2019 -0700

    Merge tag 'afs-next-20190507' of git://git.kernel.org/pub/scm/linux/kernel/git/dhowells/linux-fs
    
    Pull AFS updates from David Howells:
     "A set of fix and development patches for AFS for 5.2.
    
      Summary:
    
       - Fix the AFS file locking so that sqlite can run on an AFS mount and
         also so that firefox and gnome can use a homedir that's mounted
         through AFS.
    
         This required emulation of fine-grained locking when the server
         will only support whole-file locks and no upgrade/downgrade. Four
         modes are provided, settable by mount parameter:
    
           "flock=local"   - No reference to the server
    
           "flock=openafs" - Fine-grained locks are local-only, whole-file
                             locks require sufficient server locks
    
           "flock=strict"  - All locks require sufficient server locks
    
           "flock=write"   - Always get an exclusive server lock
    
         If the volume is a read-only or backup volume, then flock=local for
         that volume.
    
       - Log extra information for a couple of cases where the client mucks
         up somehow: AFS vnode with undefined type and dir check failure -
         in both cases we seem to end up with unfilled data, but the issues
         happen infrequently and are difficult to reproduce at will.
    
       - Implement silly rename for unlink() and rename().
    
       - Set i_blocks so that du can get some information about usage.
    
       - Fix xattr handlers to return the right amount of data and to not
         overflow buffers.
    
       - Implement getting/setting raw AFS and YFS ACLs as xattrs"
    
    * tag 'afs-next-20190507' of git://git.kernel.org/pub/scm/linux/kernel/git/dhowells/linux-fs:
      afs: Implement YFS ACL setting
      afs: Get YFS ACLs and information through xattrs
      afs: implement acl setting
      afs: Get an AFS3 ACL as an xattr
      afs: Fix getting the afs.fid xattr
      afs: Fix the afs.cell and afs.volume xattr handlers
      afs: Calculate i_blocks based on file size
      afs: Log more information for "kAFS: AFS vnode with undefined type\n"
      afs: Provide mount-time configurable byte-range file locking emulation
      afs: Add more tracepoints
      afs: Implement sillyrename for unlink and rename
      afs: Add directory reload tracepoint
      afs: Handle lock rpc ops failing on a file that got deleted
      afs: Improve dir check failure reports
      afs: Add file locking tracepoints
      afs: Further fix file locking
      afs: Fix AFS file locking to allow fine grained locks
      afs: Calculate lock extend timer from set/extend reply reception
      afs: Split wait from afs_make_call()

commit b4b52b881cf08e13d110eac811d4becc0775abbf
Merge: 2d60d96b6f00 ccaa75187a5f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 7 12:48:10 2019 -0700

    Merge tag 'Wimplicit-fallthrough-5.2-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/gustavoars/linux
    
    Pull Wimplicit-fallthrough updates from Gustavo A. R. Silva:
     "Mark switch cases where we are expecting to fall through.
    
      This is part of the ongoing efforts to enable -Wimplicit-fallthrough.
    
      Most of them have been baking in linux-next for a whole development
      cycle. And with Stephen Rothwell's help, we've had linux-next
      nag-emails going out for newly introduced code that triggers
      -Wimplicit-fallthrough to avoid gaining more of these cases while we
      work to remove the ones that are already present.
    
      We are getting close to completing this work. Currently, there are
      only 32 of 2311 of these cases left to be addressed in linux-next. I'm
      auditing every case; I take a look into the code and analyze it in
      order to determine if I'm dealing with an actual bug or a false
      positive, as explained here:
    
          https://lore.kernel.org/lkml/c2fad584-1705-a5f2-d63c-824e9b96cf50@embeddedor.com/
    
      While working on this, I've found and fixed the several missing
      break/return bugs, some of them introduced more than 5 years ago.
    
      Once this work is finished, we'll be able to universally enable
      "-Wimplicit-fallthrough" to avoid any of these kinds of bugs from
      entering the kernel again"
    
    * tag 'Wimplicit-fallthrough-5.2-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/gustavoars/linux: (27 commits)
      memstick: mark expected switch fall-throughs
      drm/nouveau/nvkm: mark expected switch fall-throughs
      NFC: st21nfca: Fix fall-through warnings
      NFC: pn533: mark expected switch fall-throughs
      block: Mark expected switch fall-throughs
      ASN.1: mark expected switch fall-through
      lib/cmdline.c: mark expected switch fall-throughs
      lib: zstd: Mark expected switch fall-throughs
      scsi: sym53c8xx_2: sym_nvram: Mark expected switch fall-through
      scsi: sym53c8xx_2: sym_hipd: mark expected switch fall-throughs
      scsi: ppa: mark expected switch fall-through
      scsi: osst: mark expected switch fall-throughs
      scsi: lpfc: lpfc_scsi: Mark expected switch fall-throughs
      scsi: lpfc: lpfc_nvme: Mark expected switch fall-through
      scsi: lpfc: lpfc_nportdisc: Mark expected switch fall-through
      scsi: lpfc: lpfc_hbadisc: Mark expected switch fall-throughs
      scsi: lpfc: lpfc_els: Mark expected switch fall-throughs
      scsi: lpfc: lpfc_ct: Mark expected switch fall-throughs
      scsi: imm: mark expected switch fall-throughs
      scsi: csiostor: csio_wr: mark expected switch fall-through
      ...

commit 0b9bf3812ad1f0d937584e300826285694f53e2b
Author: David Howells <dhowells@redhat.com>
Date:   Thu Apr 25 14:26:50 2019 +0100

    afs: Split wait from afs_make_call()
    
    Split the call to afs_wait_for_call_to_complete() from afs_make_call() to
    make it easier to handle asynchronous calls and to make it easier to
    convert a synchronous call to an asynchronous one in future, for instance
    when someone tries to interrupt an operation by pressing Ctrl-C.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 15c7e82d80cb..bc5aa590f348 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -21,7 +21,6 @@
 struct workqueue_struct *afs_async_calls;
 
 static void afs_wake_up_call_waiter(struct sock *, struct rxrpc_call *, unsigned long);
-static long afs_wait_for_call_to_complete(struct afs_call *, struct afs_addr_cursor *);
 static void afs_wake_up_async_call(struct sock *, struct rxrpc_call *, unsigned long);
 static void afs_delete_async_call(struct work_struct *);
 static void afs_process_async_call(struct work_struct *);
@@ -361,10 +360,10 @@ static int afs_send_pages(struct afs_call *call, struct msghdr *msg)
 }
 
 /*
- * initiate a call
+ * Initiate a call and synchronously queue up the parameters for dispatch.  Any
+ * error is stored into the call struct, which the caller must check for.
  */
-long afs_make_call(struct afs_addr_cursor *ac, struct afs_call *call,
-		   gfp_t gfp, bool async)
+void afs_make_call(struct afs_addr_cursor *ac, struct afs_call *call, gfp_t gfp)
 {
 	struct sockaddr_rxrpc *srx = &ac->alist->addrs[ac->index];
 	struct rxrpc_call *rxcall;
@@ -382,7 +381,6 @@ long afs_make_call(struct afs_addr_cursor *ac, struct afs_call *call,
 	       call, call->type->name, key_serial(call->key),
 	       atomic_read(&call->net->nr_outstanding_calls));
 
-	call->async = async;
 	call->addr_ix = ac->index;
 	call->alist = afs_get_addrlist(ac->alist);
 
@@ -415,7 +413,7 @@ long afs_make_call(struct afs_addr_cursor *ac, struct afs_call *call,
 	rxcall = rxrpc_kernel_begin_call(call->net->socket, srx, call->key,
 					 (unsigned long)call,
 					 tx_total_len, gfp,
-					 (async ?
+					 (call->async ?
 					  afs_wake_up_async_call :
 					  afs_wake_up_call_waiter),
 					 call->upgrade,
@@ -453,13 +451,11 @@ long afs_make_call(struct afs_addr_cursor *ac, struct afs_call *call,
 
 	/* Note that at this point, we may have received the reply or an abort
 	 * - and an asynchronous call may already have completed.
+	 *
+	 * afs_wait_for_call_to_complete(call, ac)
+	 * must be called to synchronously clean up.
 	 */
-	if (call->async) {
-		afs_put_call(call);
-		return -EINPROGRESS;
-	}
-
-	return afs_wait_for_call_to_complete(call, ac);
+	return;
 
 error_do_abort:
 	if (ret != -ECONNABORTED) {
@@ -495,9 +491,7 @@ long afs_make_call(struct afs_addr_cursor *ac, struct afs_call *call,
 
 	ac->error = ret;
 	call->state = AFS_CALL_COMPLETE;
-	afs_put_call(call);
 	_leave(" = %d", ret);
-	return ret;
 }
 
 /*
@@ -604,10 +598,10 @@ static void afs_deliver_to_call(struct afs_call *call)
 }
 
 /*
- * wait synchronously for a call to complete
+ * Wait synchronously for a call to complete and clean up the call struct.
  */
-static long afs_wait_for_call_to_complete(struct afs_call *call,
-					  struct afs_addr_cursor *ac)
+long afs_wait_for_call_to_complete(struct afs_call *call,
+				   struct afs_addr_cursor *ac)
 {
 	signed long rtt2, timeout;
 	long ret;
@@ -620,6 +614,10 @@ static long afs_wait_for_call_to_complete(struct afs_call *call,
 
 	_enter("");
 
+	ret = call->error;
+	if (ret < 0)
+		goto out;
+
 	rtt = rxrpc_kernel_get_rtt(call->net->socket, call->rxcall);
 	rtt2 = nsecs_to_jiffies64(rtt) * 2;
 	if (rtt2 < 2)
@@ -703,6 +701,7 @@ static long afs_wait_for_call_to_complete(struct afs_call *call,
 		break;
 	}
 
+out:
 	_debug("call complete");
 	afs_put_call(call);
 	_leave(" = %p", (void *)ret);

commit 8022c4b95c3793d7ba28ab0701ea15b5deb46e02
Author: David Howells <dhowells@redhat.com>
Date:   Sat Apr 13 08:37:37 2019 +0100

    afs: Differentiate abort due to unmarshalling from other errors
    
    Differentiate an abort due to an unmarshalling error from an abort due to
    other errors, such as ENETUNREACH.  It doesn't make sense to set abort code
    RXGEN_*_UNMARSHAL in such a case, so use RX_USER_ABORT instead.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index c14001b42d20..15c7e82d80cb 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -572,13 +572,17 @@ static void afs_deliver_to_call(struct afs_call *call)
 		case -ENODATA:
 		case -EBADMSG:
 		case -EMSGSIZE:
-		default:
 			abort_code = RXGEN_CC_UNMARSHAL;
 			if (state != AFS_CALL_CL_AWAIT_REPLY)
 				abort_code = RXGEN_SS_UNMARSHAL;
 			rxrpc_kernel_abort_call(call->net->socket, call->rxcall,
 						abort_code, ret, "KUM");
 			goto local_abort;
+		default:
+			abort_code = RX_USER_ABORT;
+			rxrpc_kernel_abort_call(call->net->socket, call->rxcall,
+						abort_code, ret, "KER");
+			goto local_abort;
 		}
 	}
 

commit f7f1dd3162efc7ffdbcdb9da1ad1599f8ab51296
Author: Marc Dionne <marc.dionne@auristor.com>
Date:   Fri Apr 12 16:34:02 2019 +0100

    afs: Check for rxrpc call completion in wait loop
    
    Check the state of the rxrpc call backing an afs call in each iteration of
    the call wait loop in case the rxrpc call has already been terminated at
    the rxrpc layer.
    
    Interrupt the wait loop and mark the afs call as complete if the rxrpc
    layer call is complete.
    
    There were cases where rxrpc errors were not passed up to afs, which could
    result in this loop waiting forever for an afs call to transition to
    AFS_CALL_COMPLETE while the rx call was already complete.
    
    Signed-off-by: Marc Dionne <marc.dionne@auristor.com>
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 5cb11aff9298..c14001b42d20 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -610,6 +610,7 @@ static long afs_wait_for_call_to_complete(struct afs_call *call,
 	bool stalled = false;
 	u64 rtt;
 	u32 life, last_life;
+	bool rxrpc_complete = false;
 
 	DECLARE_WAITQUEUE(myself, current);
 
@@ -639,7 +640,12 @@ static long afs_wait_for_call_to_complete(struct afs_call *call,
 		if (afs_check_call_state(call, AFS_CALL_COMPLETE))
 			break;
 
-		rxrpc_kernel_check_life(call->net->socket, call->rxcall, &life);
+		if (!rxrpc_kernel_check_life(call->net->socket, call->rxcall, &life)) {
+			/* rxrpc terminated the call. */
+			rxrpc_complete = true;
+			break;
+		}
+
 		if (timeout == 0 &&
 		    life == last_life && signal_pending(current)) {
 			if (stalled)
@@ -663,12 +669,16 @@ static long afs_wait_for_call_to_complete(struct afs_call *call,
 	remove_wait_queue(&call->waitq, &myself);
 	__set_current_state(TASK_RUNNING);
 
-	/* Kill off the call if it's still live. */
 	if (!afs_check_call_state(call, AFS_CALL_COMPLETE)) {
-		_debug("call interrupted");
-		if (rxrpc_kernel_abort_call(call->net->socket, call->rxcall,
-					    RX_USER_ABORT, -EINTR, "KWI"))
-			afs_set_call_complete(call, -EINTR, 0);
+		if (rxrpc_complete) {
+			afs_set_call_complete(call, call->error, call->abort_code);
+		} else {
+			/* Kill off the call if it's still live. */
+			_debug("call interrupted");
+			if (rxrpc_kernel_abort_call(call->net->socket, call->rxcall,
+						    RX_USER_ABORT, -EINTR, "KWI"))
+				afs_set_call_complete(call, -EINTR, 0);
+		}
 	}
 
 	spin_lock_bh(&call->state_lock);

commit 4611da30d679a4b0a2c2b5d4d7b3fbbafc922df7
Author: Marc Dionne <marc.dionne@auristor.com>
Date:   Fri Apr 12 16:33:47 2019 +0100

    rxrpc: Make rxrpc_kernel_check_life() indicate if call completed
    
    Make rxrpc_kernel_check_life() pass back the life counter through the
    argument list and return true if the call has not yet completed.
    
    Suggested-by: Marc Dionne <marc.dionne@auristor.com>
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 2c588f9bbbda..5cb11aff9298 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -621,7 +621,7 @@ static long afs_wait_for_call_to_complete(struct afs_call *call,
 		rtt2 = 2;
 
 	timeout = rtt2;
-	last_life = rxrpc_kernel_check_life(call->net->socket, call->rxcall);
+	rxrpc_kernel_check_life(call->net->socket, call->rxcall, &last_life);
 
 	add_wait_queue(&call->waitq, &myself);
 	for (;;) {
@@ -639,7 +639,7 @@ static long afs_wait_for_call_to_complete(struct afs_call *call,
 		if (afs_check_call_state(call, AFS_CALL_COMPLETE))
 			break;
 
-		life = rxrpc_kernel_check_life(call->net->socket, call->rxcall);
+		rxrpc_kernel_check_life(call->net->socket, call->rxcall, &life);
 		if (timeout == 0 &&
 		    life == last_life && signal_pending(current)) {
 			if (stalled)

commit e690c9e3f4fb2cc589414c5bf502dcfe04e685de
Author: Gustavo A. R. Silva <gustavo@embeddedor.com>
Date:   Thu Jan 10 15:52:25 2019 -0600

    afs: Mark expected switch fall-throughs
    
    In preparation to enabling -Wimplicit-fallthrough, mark switch cases
    where we are expecting to fall through.
    
    Notice that in many cases I placed a /* Fall through */ comment
    at the bottom of the case, which what GCC is expecting to find.
    
    In other cases I had to tweak a bit the format of the comments.
    
    This patch suppresses ALL missing-break-in-switch false positives
    in fs/afs
    
    Addresses-Coverity-ID: 115042 ("Missing break in switch")
    Addresses-Coverity-ID: 115043 ("Missing break in switch")
    Addresses-Coverity-ID: 115045 ("Missing break in switch")
    Addresses-Coverity-ID: 1357430 ("Missing break in switch")
    Addresses-Coverity-ID: 115047 ("Missing break in switch")
    Addresses-Coverity-ID: 115050 ("Missing break in switch")
    Addresses-Coverity-ID: 115051 ("Missing break in switch")
    Addresses-Coverity-ID: 1467806 ("Missing break in switch")
    Addresses-Coverity-ID: 1467807 ("Missing break in switch")
    Addresses-Coverity-ID: 1467811 ("Missing break in switch")
    Addresses-Coverity-ID: 115041 ("Missing break in switch")
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Gustavo A. R. Silva <gustavo@embeddedor.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 2c588f9bbbda..b3a8dfa784c4 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -909,6 +909,7 @@ void afs_send_empty_reply(struct afs_call *call)
 		_debug("oom");
 		rxrpc_kernel_abort_call(net->socket, call->rxcall,
 					RX_USER_ABORT, -ENOMEM, "KOO");
+		/* Fall through */
 	default:
 		_leave(" [error]");
 		return;

commit 34fa47612bfe5d7de7fcaf658a6952b6aeec3b13
Author: David Howells <dhowells@redhat.com>
Date:   Thu Jan 10 15:40:50 2019 +0000

    afs: Fix race in async call refcounting
    
    There's a race between afs_make_call() and afs_wake_up_async_call() in the
    case that an error is returned from rxrpc_kernel_send_data() after it has
    queued the final packet.
    
    afs_make_call() will try and clean up the mess, but the call state may have
    been moved on thereby causing afs_process_async_call() to also try and to
    delete the call.
    
    Fix this by:
    
     (1) Getting an extra ref for an asynchronous call for the call itself to
         hold.  This makes sure the call doesn't evaporate on us accidentally
         and will allow the call to be retained by the caller in a future
         patch.  The ref is released on leaving afs_make_call() or
         afs_wait_for_call_to_complete().
    
     (2) In the event of an error from rxrpc_kernel_send_data():
    
         (a) Don't set the call state to AFS_CALL_COMPLETE until *after* the
             call has been aborted and ended.  This prevents
             afs_deliver_to_call() from doing anything with any notifications
             it gets.
    
         (b) Explicitly end the call immediately to prevent further callbacks.
    
         (c) Cancel any queued async_work and wait for the work if it's
             executing.  This allows us to be sure the race won't recur when we
             change the state.  We put the work queue's ref on the call if we
             managed to cancel it.
    
         (d) Put the call's ref that we got in (1).  This belongs to us as long
             as the call is in state AFS_CALL_CL_REQUESTING.
    
    Fixes: 341f741f04be ("afs: Refcount the afs_call struct")
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 4830e0a6bf1d..2c588f9bbbda 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -23,6 +23,7 @@ struct workqueue_struct *afs_async_calls;
 static void afs_wake_up_call_waiter(struct sock *, struct rxrpc_call *, unsigned long);
 static long afs_wait_for_call_to_complete(struct afs_call *, struct afs_addr_cursor *);
 static void afs_wake_up_async_call(struct sock *, struct rxrpc_call *, unsigned long);
+static void afs_delete_async_call(struct work_struct *);
 static void afs_process_async_call(struct work_struct *);
 static void afs_rx_new_call(struct sock *, struct rxrpc_call *, unsigned long);
 static void afs_rx_discard_new_call(struct rxrpc_call *, unsigned long);
@@ -404,6 +405,12 @@ long afs_make_call(struct afs_addr_cursor *ac, struct afs_call *call,
 		}
 	}
 
+	/* If the call is going to be asynchronous, we need an extra ref for
+	 * the call to hold itself so the caller need not hang on to its ref.
+	 */
+	if (call->async)
+		afs_get_call(call, afs_call_trace_get);
+
 	/* create a call */
 	rxcall = rxrpc_kernel_begin_call(call->net->socket, srx, call->key,
 					 (unsigned long)call,
@@ -444,15 +451,17 @@ long afs_make_call(struct afs_addr_cursor *ac, struct afs_call *call,
 			goto error_do_abort;
 	}
 
-	/* at this point, an async call may no longer exist as it may have
-	 * already completed */
-	if (call->async)
+	/* Note that at this point, we may have received the reply or an abort
+	 * - and an asynchronous call may already have completed.
+	 */
+	if (call->async) {
+		afs_put_call(call);
 		return -EINPROGRESS;
+	}
 
 	return afs_wait_for_call_to_complete(call, ac);
 
 error_do_abort:
-	call->state = AFS_CALL_COMPLETE;
 	if (ret != -ECONNABORTED) {
 		rxrpc_kernel_abort_call(call->net->socket, rxcall,
 					RX_USER_ABORT, ret, "KSD");
@@ -469,8 +478,24 @@ long afs_make_call(struct afs_addr_cursor *ac, struct afs_call *call,
 error_kill_call:
 	if (call->type->done)
 		call->type->done(call);
-	afs_put_call(call);
+
+	/* We need to dispose of the extra ref we grabbed for an async call.
+	 * The call, however, might be queued on afs_async_calls and we need to
+	 * make sure we don't get any more notifications that might requeue it.
+	 */
+	if (call->rxcall) {
+		rxrpc_kernel_end_call(call->net->socket, call->rxcall);
+		call->rxcall = NULL;
+	}
+	if (call->async) {
+		if (cancel_work_sync(&call->async_work))
+			afs_put_call(call);
+		afs_put_call(call);
+	}
+
 	ac->error = ret;
+	call->state = AFS_CALL_COMPLETE;
+	afs_put_call(call);
 	_leave(" = %d", ret);
 	return ret;
 }

commit 7a75b0079a1d54e342c502c3c8107ba97e05d3d3
Author: David Howells <dhowells@redhat.com>
Date:   Thu Jan 10 15:14:29 2019 +0000

    afs: Provide a function to get a ref on a call
    
    Provide a function to get a reference on an afs_call struct.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index a7b44863d502..4830e0a6bf1d 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -203,20 +203,26 @@ void afs_put_call(struct afs_call *call)
 	}
 }
 
+static struct afs_call *afs_get_call(struct afs_call *call,
+				     enum afs_call_trace why)
+{
+	int u = atomic_inc_return(&call->usage);
+
+	trace_afs_call(call, why, u,
+		       atomic_read(&call->net->nr_outstanding_calls),
+		       __builtin_return_address(0));
+	return call;
+}
+
 /*
  * Queue the call for actual work.
  */
 static void afs_queue_call_work(struct afs_call *call)
 {
 	if (call->type->work) {
-		int u = atomic_inc_return(&call->usage);
-
-		trace_afs_call(call, afs_call_trace_work, u,
-			       atomic_read(&call->net->nr_outstanding_calls),
-			       __builtin_return_address(0));
-
 		INIT_WORK(&call->work, call->type->work);
 
+		afs_get_call(call, afs_call_trace_work);
 		if (!queue_work(afs_wq, &call->work))
 			afs_put_call(call);
 	}

commit 7150ceaacb27f7b3bf494e72cd4be4e11612dfff
Author: David Howells <dhowells@redhat.com>
Date:   Mon Nov 12 22:33:22 2018 +0000

    rxrpc: Fix life check
    
    The life-checking function, which is used by kAFS to make sure that a call
    is still live in the event of a pending signal, only samples the received
    packet serial number counter; it doesn't actually provoke a change in the
    counter, rather relying on the server to happen to give us a packet in the
    time window.
    
    Fix this by adding a function to force a ping to be transmitted.
    
    kAFS then keeps track of whether there's been a stall, and if so, uses the
    new function to ping the server, resetting the timeout to allow the reply
    to come back.
    
    If there's a stall, a ping and the call is *still* stalled in the same
    place after another period, then the call will be aborted.
    
    Fixes: bc5e3a546d55 ("rxrpc: Use MSG_WAITALL to tell sendmsg() to temporarily ignore signals")
    Fixes: f4d15fb6f99a ("rxrpc: Provide functions for allowing cleaner handling of signals")
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 59970886690f..a7b44863d502 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -576,6 +576,7 @@ static long afs_wait_for_call_to_complete(struct afs_call *call,
 {
 	signed long rtt2, timeout;
 	long ret;
+	bool stalled = false;
 	u64 rtt;
 	u32 life, last_life;
 
@@ -609,12 +610,20 @@ static long afs_wait_for_call_to_complete(struct afs_call *call,
 
 		life = rxrpc_kernel_check_life(call->net->socket, call->rxcall);
 		if (timeout == 0 &&
-		    life == last_life && signal_pending(current))
+		    life == last_life && signal_pending(current)) {
+			if (stalled)
 				break;
+			__set_current_state(TASK_RUNNING);
+			rxrpc_kernel_probe_life(call->net->socket, call->rxcall);
+			timeout = rtt2;
+			stalled = true;
+			continue;
+		}
 
 		if (life != last_life) {
 			timeout = rtt2;
 			last_life = life;
+			stalled = false;
 		}
 
 		timeout = schedule_timeout(timeout);

commit 3bf0fb6f33dd545693da5e65f5b1b9b9f0bfc35e
Author: David Howells <dhowells@redhat.com>
Date:   Sat Oct 20 00:57:59 2018 +0100

    afs: Probe multiple fileservers simultaneously
    
    Send probes to all the unprobed fileservers in a fileserver list on all
    addresses simultaneously in an attempt to find out the fastest route whilst
    not getting stuck for 20s on any server or address that we don't get a
    reply from.
    
    This alleviates the problem whereby attempting to access a new server can
    take a long time because the rotation algorithm ends up rotating through
    all servers and addresses until it finds one that responds.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 42e1ea7372e9..59970886690f 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -43,7 +43,6 @@ int afs_open_socket(struct afs_net *net)
 	struct sockaddr_rxrpc srx;
 	struct socket *socket;
 	unsigned int min_level;
-	u16 service_upgrade[2];
 	int ret;
 
 	_enter("");
@@ -82,13 +81,12 @@ int afs_open_socket(struct afs_net *net)
 	if (ret < 0)
 		goto error_2;
 
-	service_upgrade[0] = CM_SERVICE;
-	service_upgrade[1] = YFS_CM_SERVICE;
-	ret = kernel_setsockopt(socket, SOL_RXRPC, RXRPC_UPGRADEABLE_SERVICE,
-				(void *)service_upgrade, sizeof(service_upgrade));
-	if (ret < 0)
-		goto error_2;
-
+	/* Ideally, we'd turn on service upgrade here, but we can't because
+	 * OpenAFS is buggy and leaks the userStatus field from packet to
+	 * packet and between FS packets and CB packets - so if we try to do an
+	 * upgrade on an FS packet, OpenAFS will leak that into the CB packet
+	 * it sends back to us.
+	 */
 
 	rxrpc_kernel_new_call_notification(socket, afs_rx_new_call,
 					   afs_rx_discard_new_call);
@@ -192,6 +190,7 @@ void afs_put_call(struct afs_call *call)
 
 		afs_put_server(call->net, call->cm_server);
 		afs_put_cb_interest(call->net, call->cbi);
+		afs_put_addrlist(call->alist);
 		kfree(call->request);
 
 		trace_afs_call(call, afs_call_trace_free, 0, o,
@@ -205,21 +204,22 @@ void afs_put_call(struct afs_call *call)
 }
 
 /*
- * Queue the call for actual work.  Returns 0 unconditionally for convenience.
+ * Queue the call for actual work.
  */
-int afs_queue_call_work(struct afs_call *call)
+static void afs_queue_call_work(struct afs_call *call)
 {
-	int u = atomic_inc_return(&call->usage);
+	if (call->type->work) {
+		int u = atomic_inc_return(&call->usage);
 
-	trace_afs_call(call, afs_call_trace_work, u,
-		       atomic_read(&call->net->nr_outstanding_calls),
-		       __builtin_return_address(0));
+		trace_afs_call(call, afs_call_trace_work, u,
+			       atomic_read(&call->net->nr_outstanding_calls),
+			       __builtin_return_address(0));
 
-	INIT_WORK(&call->work, call->type->work);
+		INIT_WORK(&call->work, call->type->work);
 
-	if (!queue_work(afs_wq, &call->work))
-		afs_put_call(call);
-	return 0;
+		if (!queue_work(afs_wq, &call->work))
+			afs_put_call(call);
+	}
 }
 
 /*
@@ -376,6 +376,8 @@ long afs_make_call(struct afs_addr_cursor *ac, struct afs_call *call,
 	       atomic_read(&call->net->nr_outstanding_calls));
 
 	call->async = async;
+	call->addr_ix = ac->index;
+	call->alist = afs_get_addrlist(ac->alist);
 
 	/* Work out the length we're going to transmit.  This is awkward for
 	 * calls such as FS.StoreData where there's an extra injection of data
@@ -407,6 +409,7 @@ long afs_make_call(struct afs_addr_cursor *ac, struct afs_call *call,
 					 call->debug_id);
 	if (IS_ERR(rxcall)) {
 		ret = PTR_ERR(rxcall);
+		call->error = ret;
 		goto error_kill_call;
 	}
 
@@ -458,6 +461,8 @@ long afs_make_call(struct afs_addr_cursor *ac, struct afs_call *call,
 	call->error = ret;
 	trace_afs_call_done(call);
 error_kill_call:
+	if (call->type->done)
+		call->type->done(call);
 	afs_put_call(call);
 	ac->error = ret;
 	_leave(" = %d", ret);
@@ -509,6 +514,7 @@ static void afs_deliver_to_call(struct afs_call *call)
 		state = READ_ONCE(call->state);
 		switch (ret) {
 		case 0:
+			afs_queue_call_work(call);
 			if (state == AFS_CALL_CL_PROC_REPLY) {
 				if (call->cbi)
 					set_bit(AFS_SERVER_FL_MAY_HAVE_CB,
@@ -546,6 +552,8 @@ static void afs_deliver_to_call(struct afs_call *call)
 	}
 
 done:
+	if (call->type->done)
+		call->type->done(call);
 	if (state == AFS_CALL_COMPLETE && call->incoming)
 		afs_put_call(call);
 out:

commit 2feeaf8433c8e68de3d0a06a0ffe7742bcd13c1a
Author: David Howells <dhowells@redhat.com>
Date:   Sat Oct 20 00:57:59 2018 +0100

    afs: Eliminate the address pointer from the address list cursor
    
    Eliminate the address pointer from the address list cursor as it's
    redundant (ac->addrs[ac->index] can be used to find the same address) and
    address lists must be replaced rather than being rearranged, so is of
    limited value.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 444ba0d511ef..42e1ea7372e9 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -359,7 +359,7 @@ static int afs_send_pages(struct afs_call *call, struct msghdr *msg)
 long afs_make_call(struct afs_addr_cursor *ac, struct afs_call *call,
 		   gfp_t gfp, bool async)
 {
-	struct sockaddr_rxrpc *srx = ac->addr;
+	struct sockaddr_rxrpc *srx = &ac->alist->addrs[ac->index];
 	struct rxrpc_call *rxcall;
 	struct msghdr msg;
 	struct kvec iov[1];

commit 12d8e95a911eb3921a1fecf1e4d913654f6f4fb5
Author: David Howells <dhowells@redhat.com>
Date:   Sat Oct 20 00:57:58 2018 +0100

    afs: Calc callback expiry in op reply delivery
    
    Calculate the callback expiration time at the point of operation reply
    delivery, using the reply time queried from AF_RXRPC on that call as a
    base.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index ce98e133caa6..444ba0d511ef 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -499,6 +499,12 @@ static void afs_deliver_to_call(struct afs_call *call)
 			return;
 		}
 
+		if (call->want_reply_time &&
+		    rxrpc_kernel_get_reply_time(call->net->socket,
+						call->rxcall,
+						&call->reply_time))
+			call->want_reply_time = false;
+
 		ret = call->type->deliver(call);
 		state = READ_ONCE(call->state);
 		switch (ret) {

commit 35dbfba3111a5ef0663bb89185ce8dfdbef63f8d
Author: David Howells <dhowells@redhat.com>
Date:   Sat Oct 20 00:57:58 2018 +0100

    afs: Implement the YFS cache manager service
    
    Implement the YFS cache manager service which gives extra capabilities on
    top of AFS.  This is done by listening for an additional service on the
    same port and indicating that anyone requesting an upgrade should be
    upgraded to the YFS port.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index e6ab824ab761..ce98e133caa6 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -16,6 +16,7 @@
 #include <net/af_rxrpc.h>
 #include "internal.h"
 #include "afs_cm.h"
+#include "protocol_yfs.h"
 
 struct workqueue_struct *afs_async_calls;
 
@@ -42,6 +43,7 @@ int afs_open_socket(struct afs_net *net)
 	struct sockaddr_rxrpc srx;
 	struct socket *socket;
 	unsigned int min_level;
+	u16 service_upgrade[2];
 	int ret;
 
 	_enter("");
@@ -75,6 +77,19 @@ int afs_open_socket(struct afs_net *net)
 	if (ret < 0)
 		goto error_2;
 
+	srx.srx_service = YFS_CM_SERVICE;
+	ret = kernel_bind(socket, (struct sockaddr *) &srx, sizeof(srx));
+	if (ret < 0)
+		goto error_2;
+
+	service_upgrade[0] = CM_SERVICE;
+	service_upgrade[1] = YFS_CM_SERVICE;
+	ret = kernel_setsockopt(socket, SOL_RXRPC, RXRPC_UPGRADEABLE_SERVICE,
+				(void *)service_upgrade, sizeof(service_upgrade));
+	if (ret < 0)
+		goto error_2;
+
+
 	rxrpc_kernel_new_call_notification(socket, afs_rx_new_call,
 					   afs_rx_discard_new_call);
 

commit f51375cd9e1ad75e9e38186aa0d3749ade7d52a5
Author: David Howells <dhowells@redhat.com>
Date:   Sat Oct 20 00:57:57 2018 +0100

    afs: Add a couple of tracepoints to log I/O errors
    
    Add a couple of tracepoints to log the production of I/O errors within the AFS
    filesystem.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 947ae3ab389b..e6ab824ab761 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -915,7 +915,7 @@ int afs_extract_data(struct afs_call *call, bool want_more)
 			break;
 		case AFS_CALL_COMPLETE:
 			kdebug("prem complete %d", call->error);
-			return -EIO;
+			return afs_io_error(call, afs_io_error_extract);
 		default:
 			break;
 		}

commit 4ac15ea53622272c01954461b4814892b7481b40
Author: David Howells <dhowells@redhat.com>
Date:   Sat Oct 20 00:57:57 2018 +0100

    afs: Handle EIO from delivery function
    
    Fix afs_deliver_to_call() to handle -EIO being returned by the operation
    delivery function, indicating that the call found itself in the wrong
    state, by printing an error and aborting the call.
    
    Currently, an assertion failure will occur.  This can happen, say, if the
    delivery function falls off the end without calling afs_extract_data() with
    the want_more parameter set to false to collect the end of the Rx phase of
    a call.
    
    The assertion failure looks like:
    
            AFS: Assertion failed
            4 == 7 is false
            0x4 == 0x7 is false
            ------------[ cut here ]------------
            kernel BUG at fs/afs/rxrpc.c:462!
    
    and is matched in the trace buffer by a line like:
    
    kworker/7:3-3226 [007] ...1 85158.030203: afs_io_error: c=0003be0c r=-5 CM_REPLY
    
    Fixes: 98bf40cd99fc ("afs: Protect call->state changes against signals")
    Reported-by: Marc Dionne <marc.dionne@auristor.com>
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index a3904a8315de..947ae3ab389b 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -499,7 +499,6 @@ static void afs_deliver_to_call(struct afs_call *call)
 		case -EINPROGRESS:
 		case -EAGAIN:
 			goto out;
-		case -EIO:
 		case -ECONNABORTED:
 			ASSERTCMP(state, ==, AFS_CALL_COMPLETE);
 			goto done;
@@ -508,6 +507,10 @@ static void afs_deliver_to_call(struct afs_call *call)
 			rxrpc_kernel_abort_call(call->net->socket, call->rxcall,
 						abort_code, ret, "KIV");
 			goto local_abort;
+		case -EIO:
+			pr_err("kAFS: Call %u in bad state %u\n",
+			       call->debug_id, state);
+			/* Fall through */
 		case -ENODATA:
 		case -EBADMSG:
 		case -EMSGSIZE:

commit 12bdcf333fe13ece2a487a699b1a0f4c5dbb594b
Author: David Howells <dhowells@redhat.com>
Date:   Sat Oct 20 00:57:56 2018 +0100

    afs: Set up the iov_iter before calling afs_extract_data()
    
    afs_extract_data sets up a temporary iov_iter and passes it to AF_RXRPC
    each time it is called to describe the remaining buffer to be filled.
    
    Instead:
    
     (1) Put an iterator in the afs_call struct.
    
     (2) Set the iterator for each marshalling stage to load data into the
         appropriate places.  A number of convenience functions are provided to
         this end (eg. afs_extract_to_buf()).
    
         This iterator is then passed to afs_extract_data().
    
     (3) Use the new ITER_DISCARD iterator to discard any excess data provided
         by FetchData.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 9757c2d953f1..a3904a8315de 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -143,6 +143,7 @@ static struct afs_call *afs_alloc_call(struct afs_net *net,
 	INIT_WORK(&call->async_work, afs_process_async_call);
 	init_waitqueue_head(&call->waitq);
 	spin_lock_init(&call->state_lock);
+	call->_iter = &call->iter;
 
 	o = atomic_inc_return(&net->nr_outstanding_calls);
 	trace_afs_call(call, afs_call_trace_alloc, 1, o,
@@ -233,6 +234,7 @@ struct afs_call *afs_alloc_flat_call(struct afs_net *net,
 			goto nomem_free;
 	}
 
+	afs_extract_to_buf(call, call->reply_max);
 	call->operation_ID = type->op;
 	init_waitqueue_head(&call->waitq);
 	return call;
@@ -465,14 +467,12 @@ static void afs_deliver_to_call(struct afs_call *call)
 	       state == AFS_CALL_SV_AWAIT_ACK
 	       ) {
 		if (state == AFS_CALL_SV_AWAIT_ACK) {
-			struct iov_iter iter;
-
-			iov_iter_kvec(&iter, READ, NULL, 0, 0);
+			iov_iter_kvec(&call->iter, READ, NULL, 0, 0);
 			ret = rxrpc_kernel_recv_data(call->net->socket,
-						     call->rxcall, &iter, false,
-						     &remote_abort,
+						     call->rxcall, &call->iter,
+						     false, &remote_abort,
 						     &call->service_id);
-			trace_afs_recv_data(call, 0, 0, false, ret);
+			trace_afs_receive_data(call, &call->iter, false, ret);
 
 			if (ret == -EINPROGRESS || ret == -EAGAIN)
 				return;
@@ -516,7 +516,7 @@ static void afs_deliver_to_call(struct afs_call *call)
 			if (state != AFS_CALL_CL_AWAIT_REPLY)
 				abort_code = RXGEN_SS_UNMARSHAL;
 			rxrpc_kernel_abort_call(call->net->socket, call->rxcall,
-						abort_code, -EBADMSG, "KUM");
+						abort_code, ret, "KUM");
 			goto local_abort;
 		}
 	}
@@ -727,6 +727,7 @@ void afs_charge_preallocation(struct work_struct *work)
 			call->async = true;
 			call->state = AFS_CALL_SV_AWAIT_OP_ID;
 			init_waitqueue_head(&call->waitq);
+			afs_extract_to_tmp(call);
 		}
 
 		if (rxrpc_kernel_charge_accept(net->socket,
@@ -772,18 +773,15 @@ static int afs_deliver_cm_op_id(struct afs_call *call)
 {
 	int ret;
 
-	_enter("{%zu}", call->offset);
-
-	ASSERTCMP(call->offset, <, 4);
+	_enter("{%zu}", iov_iter_count(call->_iter));
 
 	/* the operation ID forms the first four bytes of the request data */
-	ret = afs_extract_data(call, &call->tmp, 4, true);
+	ret = afs_extract_data(call, true);
 	if (ret < 0)
 		return ret;
 
 	call->operation_ID = ntohl(call->tmp);
 	afs_set_call_state(call, AFS_CALL_SV_AWAIT_OP_ID, AFS_CALL_SV_AWAIT_REQUEST);
-	call->offset = 0;
 
 	/* ask the cache manager to route the call (it'll change the call type
 	 * if successful) */
@@ -887,30 +885,19 @@ void afs_send_simple_reply(struct afs_call *call, const void *buf, size_t len)
 /*
  * Extract a piece of data from the received data socket buffers.
  */
-int afs_extract_data(struct afs_call *call, void *buf, size_t count,
-		     bool want_more)
+int afs_extract_data(struct afs_call *call, bool want_more)
 {
 	struct afs_net *net = call->net;
-	struct iov_iter iter;
-	struct kvec iov;
+	struct iov_iter *iter = call->_iter;
 	enum afs_call_state state;
 	u32 remote_abort = 0;
 	int ret;
 
-	_enter("{%s,%zu},,%zu,%d",
-	       call->type->name, call->offset, count, want_more);
-
-	ASSERTCMP(call->offset, <=, count);
-
-	iov.iov_base = buf + call->offset;
-	iov.iov_len = count - call->offset;
-	iov_iter_kvec(&iter, READ, &iov, 1, count - call->offset);
+	_enter("{%s,%zu},%d", call->type->name, iov_iter_count(iter), want_more);
 
-	ret = rxrpc_kernel_recv_data(net->socket, call->rxcall, &iter,
+	ret = rxrpc_kernel_recv_data(net->socket, call->rxcall, iter,
 				     want_more, &remote_abort,
 				     &call->service_id);
-	call->offset += (count - call->offset) - iov_iter_count(&iter);
-	trace_afs_recv_data(call, count, call->offset, want_more, ret);
 	if (ret == 0 || ret == -EAGAIN)
 		return ret;
 

commit 160cb9574b550426122422444b8f19d614505f81
Author: David Howells <dhowells@redhat.com>
Date:   Sat Oct 20 00:57:56 2018 +0100

    afs: Better tracing of protocol errors
    
    Include the site of detection of AFS protocol errors in trace lines to
    better be able to determine what went wrong.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 639c16882e93..9757c2d953f1 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -939,8 +939,9 @@ int afs_extract_data(struct afs_call *call, void *buf, size_t count,
 /*
  * Log protocol error production.
  */
-noinline int afs_protocol_error(struct afs_call *call, int error)
+noinline int afs_protocol_error(struct afs_call *call, int error,
+				enum afs_eproto_cause cause)
 {
-	trace_afs_protocol_error(call, error, __builtin_return_address(0));
+	trace_afs_protocol_error(call, error, cause);
 	return error;
 }

commit aa563d7bca6e882ec2bdae24603c8f016401a144
Author: David Howells <dhowells@redhat.com>
Date:   Sat Oct 20 00:57:56 2018 +0100

    iov_iter: Separate type from direction and use accessor functions
    
    In the iov_iter struct, separate the iterator type from the iterator
    direction and use accessor functions to access them in most places.
    
    Convert a bunch of places to use switch-statements to access them rather
    then chains of bitwise-AND statements.  This makes it easier to add further
    iterator types.  Also, this can be more efficient as to implement a switch
    of small contiguous integers, the compiler can use ~50% fewer compare
    instructions than it has to use bitwise-and instructions.
    
    Further, cease passing the iterator type into the iterator setup function.
    The iterator function can set that itself.  Only the direction is required.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 77a83790a31f..639c16882e93 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -286,7 +286,7 @@ static void afs_load_bvec(struct afs_call *call, struct msghdr *msg,
 		offset = 0;
 	}
 
-	iov_iter_bvec(&msg->msg_iter, WRITE | ITER_BVEC, bv, nr, bytes);
+	iov_iter_bvec(&msg->msg_iter, WRITE, bv, nr, bytes);
 }
 
 /*
@@ -401,8 +401,7 @@ long afs_make_call(struct afs_addr_cursor *ac, struct afs_call *call,
 
 	msg.msg_name		= NULL;
 	msg.msg_namelen		= 0;
-	iov_iter_kvec(&msg.msg_iter, WRITE | ITER_KVEC, iov, 1,
-		      call->request_size);
+	iov_iter_kvec(&msg.msg_iter, WRITE, iov, 1, call->request_size);
 	msg.msg_control		= NULL;
 	msg.msg_controllen	= 0;
 	msg.msg_flags		= MSG_WAITALL | (call->send_pages ? MSG_MORE : 0);
@@ -432,7 +431,7 @@ long afs_make_call(struct afs_addr_cursor *ac, struct afs_call *call,
 		rxrpc_kernel_abort_call(call->net->socket, rxcall,
 					RX_USER_ABORT, ret, "KSD");
 	} else {
-		iov_iter_kvec(&msg.msg_iter, READ | ITER_KVEC, NULL, 0, 0);
+		iov_iter_kvec(&msg.msg_iter, READ, NULL, 0, 0);
 		rxrpc_kernel_recv_data(call->net->socket, rxcall,
 				       &msg.msg_iter, false,
 				       &call->abort_code, &call->service_id);
@@ -468,7 +467,7 @@ static void afs_deliver_to_call(struct afs_call *call)
 		if (state == AFS_CALL_SV_AWAIT_ACK) {
 			struct iov_iter iter;
 
-			iov_iter_kvec(&iter, READ | ITER_KVEC, NULL, 0, 0);
+			iov_iter_kvec(&iter, READ, NULL, 0, 0);
 			ret = rxrpc_kernel_recv_data(call->net->socket,
 						     call->rxcall, &iter, false,
 						     &remote_abort,
@@ -825,7 +824,7 @@ void afs_send_empty_reply(struct afs_call *call)
 
 	msg.msg_name		= NULL;
 	msg.msg_namelen		= 0;
-	iov_iter_kvec(&msg.msg_iter, WRITE | ITER_KVEC, NULL, 0, 0);
+	iov_iter_kvec(&msg.msg_iter, WRITE, NULL, 0, 0);
 	msg.msg_control		= NULL;
 	msg.msg_controllen	= 0;
 	msg.msg_flags		= 0;
@@ -864,7 +863,7 @@ void afs_send_simple_reply(struct afs_call *call, const void *buf, size_t len)
 	iov[0].iov_len		= len;
 	msg.msg_name		= NULL;
 	msg.msg_namelen		= 0;
-	iov_iter_kvec(&msg.msg_iter, WRITE | ITER_KVEC, iov, 1, len);
+	iov_iter_kvec(&msg.msg_iter, WRITE, iov, 1, len);
 	msg.msg_control		= NULL;
 	msg.msg_controllen	= 0;
 	msg.msg_flags		= 0;
@@ -905,7 +904,7 @@ int afs_extract_data(struct afs_call *call, void *buf, size_t count,
 
 	iov.iov_base = buf + call->offset;
 	iov.iov_len = count - call->offset;
-	iov_iter_kvec(&iter, ITER_KVEC | READ, &iov, 1, count - call->offset);
+	iov_iter_kvec(&iter, READ, &iov, 1, count - call->offset);
 
 	ret = rxrpc_kernel_recv_data(net->socket, call->rxcall, &iter,
 				     want_more, &remote_abort,

commit f0a7d1883d9f78ae7bf15fc258bf9a2b20f35b76
Author: David Howells <dhowells@redhat.com>
Date:   Mon Oct 15 12:43:02 2018 +0100

    afs: Fix clearance of reply
    
    The recent patch to fix the afs_server struct leak didn't actually fix the
    bug, but rather fixed some of the symptoms.  The problem is that an
    asynchronous call that holds a resource pointed to by call->reply[0] will
    find the pointer cleared in the call destructor, thereby preventing the
    resource from being cleaned up.
    
    In the case of the server record leak, the afs_fs_get_capabilities()
    function in devel code sets up a call with reply[0] pointing at the server
    record that should be altered when the result is obtained, but this was
    being cleared before the destructor was called, so the put in the
    destructor does nothing and the record is leaked.
    
    Commit f014ffb025c1 removed the additional ref obtained by
    afs_install_server(), but the removal of this ref is actually used by the
    garbage collector to mark a server record as being defunct after the record
    has expired through lack of use.
    
    The offending clearance of call->reply[0] upon completion in
    afs_process_async_call() has been there from the origin of the code, but
    none of the asynchronous calls actually use that pointer currently, so it
    should be safe to remove (note that synchronous calls don't involve this
    function).
    
    Fix this by the following means:
    
     (1) Revert commit f014ffb025c1.
    
     (2) Remove the clearance of reply[0] from afs_process_async_call().
    
    Without this, afs_manage_servers() will suffer an assertion failure if it
    sees a server record that didn't get used because the usage count is not 1.
    
    Fixes: f014ffb025c1 ("afs: Fix afs_server struct leak")
    Fixes: 08e0e7c82eea ("[AF_RXRPC]: Make the in-kernel AFS filesystem use AF_RXRPC.")
    Signed-off-by: David Howells <dhowells@redhat.com>
    Cc: stable <stable@vger.kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 35f2ae30f31f..77a83790a31f 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -690,8 +690,6 @@ static void afs_process_async_call(struct work_struct *work)
 	}
 
 	if (call->state == AFS_CALL_COMPLETE) {
-		call->reply[0] = NULL;
-
 		/* We have two refs to release - one from the alloc and one
 		 * queued with the work item - and we can't just deallocate the
 		 * call because the work item may be queued again.

commit 9a76aba02a37718242d7cdc294f0a3901928aa57
Merge: 0a957467c5fd 26a1ccc6c117
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Aug 15 15:04:25 2018 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next
    
    Pull networking updates from David Miller:
     "Highlights:
    
       - Gustavo A. R. Silva keeps working on the implicit switch fallthru
         changes.
    
       - Support 802.11ax High-Efficiency wireless in cfg80211 et al, From
         Luca Coelho.
    
       - Re-enable ASPM in r8169, from Kai-Heng Feng.
    
       - Add virtual XFRM interfaces, which avoids all of the limitations of
         existing IPSEC tunnels. From Steffen Klassert.
    
       - Convert GRO over to use a hash table, so that when we have many
         flows active we don't traverse a long list during accumluation.
    
       - Many new self tests for routing, TC, tunnels, etc. Too many
         contributors to mention them all, but I'm really happy to keep
         seeing this stuff.
    
       - Hardware timestamping support for dpaa_eth/fsl-fman from Yangbo Lu.
    
       - Lots of cleanups and fixes in L2TP code from Guillaume Nault.
    
       - Add IPSEC offload support to netdevsim, from Shannon Nelson.
    
       - Add support for slotting with non-uniform distribution to netem
         packet scheduler, from Yousuk Seung.
    
       - Add UDP GSO support to mlx5e, from Boris Pismenny.
    
       - Support offloading of Team LAG in NFP, from John Hurley.
    
       - Allow to configure TX queue selection based upon RX queue, from
         Amritha Nambiar.
    
       - Support ethtool ring size configuration in aquantia, from Anton
         Mikaev.
    
       - Support DSCP and flowlabel per-transport in SCTP, from Xin Long.
    
       - Support list based batching and stack traversal of SKBs, this is
         very exciting work. From Edward Cree.
    
       - Busyloop optimizations in vhost_net, from Toshiaki Makita.
    
       - Introduce the ETF qdisc, which allows time based transmissions. IGB
         can offload this in hardware. From Vinicius Costa Gomes.
    
       - Add parameter support to devlink, from Moshe Shemesh.
    
       - Several multiplication and division optimizations for BPF JIT in
         nfp driver, from Jiong Wang.
    
       - Lots of prepatory work to make more of the packet scheduler layer
         lockless, when possible, from Vlad Buslov.
    
       - Add ACK filter and NAT awareness to sch_cake packet scheduler, from
         Toke Høiland-Jørgensen.
    
       - Support regions and region snapshots in devlink, from Alex Vesker.
    
       - Allow to attach XDP programs to both HW and SW at the same time on
         a given device, with initial support in nfp. From Jakub Kicinski.
    
       - Add TLS RX offload and support in mlx5, from Ilya Lesokhin.
    
       - Use PHYLIB in r8169 driver, from Heiner Kallweit.
    
       - All sorts of changes to support Spectrum 2 in mlxsw driver, from
         Ido Schimmel.
    
       - PTP support in mv88e6xxx DSA driver, from Andrew Lunn.
    
       - Make TCP_USER_TIMEOUT socket option more accurate, from Jon
         Maxwell.
    
       - Support for templates in packet scheduler classifier, from Jiri
         Pirko.
    
       - IPV6 support in RDS, from Ka-Cheong Poon.
    
       - Native tproxy support in nf_tables, from Máté Eckl.
    
       - Maintain IP fragment queue in an rbtree, but optimize properly for
         in-order frags. From Peter Oskolkov.
    
       - Improvde handling of ACKs on hole repairs, from Yuchung Cheng"
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next: (1996 commits)
      bpf: test: fix spelling mistake "REUSEEPORT" -> "REUSEPORT"
      hv/netvsc: Fix NULL dereference at single queue mode fallback
      net: filter: mark expected switch fall-through
      xen-netfront: fix warn message as irq device name has '/'
      cxgb4: Add new T5 PCI device ids 0x50af and 0x50b0
      net: dsa: mv88e6xxx: missing unlock on error path
      rds: fix building with IPV6=m
      inet/connection_sock: prefer _THIS_IP_ to current_text_addr
      net: dsa: mv88e6xxx: bitwise vs logical bug
      net: sock_diag: Fix spectre v1 gadget in __sock_diag_cmd()
      ieee802154: hwsim: using right kind of iteration
      net: hns3: Add vlan filter setting by ethtool command -K
      net: hns3: Set tx ring' tc info when netdev is up
      net: hns3: Remove tx ring BD len register in hns3_enet
      net: hns3: Fix desc num set to default when setting channel
      net: hns3: Fix for phy link issue when using marvell phy driver
      net: hns3: Fix for information of phydev lost problem when down/up
      net: hns3: Fix for command format parsing error in hclge_is_all_function_id_zero
      net: hns3: Add support for serdes loopback selftest
      bnxt_en: take coredump_record structure off stack
      ...

commit eb9950eb31f56e57582a61c92073336d04a26542
Author: David Howells <dhowells@redhat.com>
Date:   Fri Aug 3 17:06:56 2018 +0100

    rxrpc: Push iov_iter up from rxrpc_kernel_recv_data() to caller
    
    Push iov_iter up from rxrpc_kernel_recv_data() to its caller to allow
    non-contiguous iovs to be passed down, thereby permitting file reading to
    be simplified in the AFS filesystem in a future patch.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index a1b18082991b..19db5f672a9d 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -346,7 +346,6 @@ long afs_make_call(struct afs_addr_cursor *ac, struct afs_call *call,
 	struct rxrpc_call *rxcall;
 	struct msghdr msg;
 	struct kvec iov[1];
-	size_t offset;
 	s64 tx_total_len;
 	int ret;
 
@@ -433,10 +432,10 @@ long afs_make_call(struct afs_addr_cursor *ac, struct afs_call *call,
 		rxrpc_kernel_abort_call(call->net->socket, rxcall,
 					RX_USER_ABORT, ret, "KSD");
 	} else {
-		offset = 0;
-		rxrpc_kernel_recv_data(call->net->socket, rxcall, NULL,
-				       0, &offset, false, &call->abort_code,
-				       &call->service_id);
+		iov_iter_kvec(&msg.msg_iter, READ | ITER_KVEC, NULL, 0, 0);
+		rxrpc_kernel_recv_data(call->net->socket, rxcall,
+				       &msg.msg_iter, false,
+				       &call->abort_code, &call->service_id);
 		ac->abort_code = call->abort_code;
 		ac->responded = true;
 	}
@@ -467,13 +466,14 @@ static void afs_deliver_to_call(struct afs_call *call)
 	       state == AFS_CALL_SV_AWAIT_ACK
 	       ) {
 		if (state == AFS_CALL_SV_AWAIT_ACK) {
-			size_t offset = 0;
+			struct iov_iter iter;
+
+			iov_iter_kvec(&iter, READ | ITER_KVEC, NULL, 0, 0);
 			ret = rxrpc_kernel_recv_data(call->net->socket,
-						     call->rxcall,
-						     NULL, 0, &offset, false,
+						     call->rxcall, &iter, false,
 						     &remote_abort,
 						     &call->service_id);
-			trace_afs_recv_data(call, 0, offset, false, ret);
+			trace_afs_recv_data(call, 0, 0, false, ret);
 
 			if (ret == -EINPROGRESS || ret == -EAGAIN)
 				return;
@@ -894,6 +894,8 @@ int afs_extract_data(struct afs_call *call, void *buf, size_t count,
 		     bool want_more)
 {
 	struct afs_net *net = call->net;
+	struct iov_iter iter;
+	struct kvec iov;
 	enum afs_call_state state;
 	u32 remote_abort = 0;
 	int ret;
@@ -903,10 +905,14 @@ int afs_extract_data(struct afs_call *call, void *buf, size_t count,
 
 	ASSERTCMP(call->offset, <=, count);
 
-	ret = rxrpc_kernel_recv_data(net->socket, call->rxcall,
-				     buf, count, &call->offset,
+	iov.iov_base = buf + call->offset;
+	iov.iov_len = count - call->offset;
+	iov_iter_kvec(&iter, ITER_KVEC | READ, &iov, 1, count - call->offset);
+
+	ret = rxrpc_kernel_recv_data(net->socket, call->rxcall, &iter,
 				     want_more, &remote_abort,
 				     &call->service_id);
+	call->offset += (count - call->offset) - iov_iter_count(&iter);
 	trace_afs_recv_data(call, count, call->offset, want_more, ret);
 	if (ret == 0 || ret == -EAGAIN)
 		return ret;

commit bfc18e389c7a09fbbbed6bf4032396685b14246e
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Jun 21 13:13:04 2018 +0100

    atomics/treewide: Rename __atomic_add_unless() => atomic_fetch_add_unless()
    
    While __atomic_add_unless() was originally intended as a building-block
    for atomic_add_unless(), it's now used in a number of places around the
    kernel. It's the only common atomic operation named __atomic*(), rather
    than atomic_*(), and for consistency it would be better named
    atomic_fetch_add_unless().
    
    This lack of consistency is slightly confusing, and gets in the way of
    scripting atomics. Given that, let's clean things up and promote it to
    an official part of the atomics API, in the form of
    atomic_fetch_add_unless().
    
    This patch converts definitions and invocations over to the new name,
    including the instrumented version, using the following script:
    
      ----
      git grep -w __atomic_add_unless | while read line; do
      sed -i '{s/\<__atomic_add_unless\>/atomic_fetch_add_unless/}' "${line%%:*}";
      done
      git grep -w __arch_atomic_add_unless | while read line; do
      sed -i '{s/\<__arch_atomic_add_unless\>/arch_atomic_fetch_add_unless/}' "${line%%:*}";
      done
      ----
    
    Note that we do not have atomic{64,_long}_fetch_add_unless(), which will
    be introduced by later patches.
    
    There should be no functional change as a result of this patch.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Palmer Dabbelt <palmer@sifive.com>
    Cc: Boqun Feng <boqun.feng@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lore.kernel.org/lkml/20180621121321.4761-2-mark.rutland@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index a1b18082991b..183cc5418722 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -648,7 +648,7 @@ static void afs_wake_up_async_call(struct sock *sk, struct rxrpc_call *rxcall,
 	trace_afs_notify_call(rxcall, call);
 	call->need_attention = true;
 
-	u = __atomic_add_unless(&call->usage, 1, 0);
+	u = atomic_fetch_add_unless(&call->usage, 1, 0);
 	if (u != 0) {
 		trace_afs_call(call, afs_call_trace_wake, u,
 			       atomic_read(&call->net->nr_outstanding_calls),

commit de52cf922a4a17d0a4cd34d697db62a01c1bd092
Merge: 5b86d4ff5dce 4776cab43fd3
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sat Jun 2 18:08:11 2018 -0400

    Merge tag 'afs-fixes-20180514' into afs-proc
    
    backmerge AFS fixes that went into mainline and deal with
    the conflict in fs/afs/fsclient.c
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit 5b86d4ff5dce3271dff54119e06174dc22422903
Author: David Howells <dhowells@redhat.com>
Date:   Fri May 18 11:46:15 2018 +0100

    afs: Implement network namespacing
    
    Implement network namespacing within AFS, but don't yet let mounts occur
    outside the init namespace.  An additional patch will be required propagate
    the network namespace across automounts.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 5c6263972ec9..e58fa0e15798 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -45,7 +45,7 @@ int afs_open_socket(struct afs_net *net)
 
 	_enter("");
 
-	ret = sock_create_kern(&init_net, AF_RXRPC, SOCK_DGRAM, PF_INET6, &socket);
+	ret = sock_create_kern(net->net, AF_RXRPC, SOCK_DGRAM, PF_INET6, &socket);
 	if (ret < 0)
 		goto error_1;
 

commit 4776cab43fd3111618112737a257dc3ef368eddd
Author: David Howells <dhowells@redhat.com>
Date:   Thu May 10 23:10:40 2018 +0100

    afs: Fix the non-encryption of calls
    
    Some AFS servers refuse to accept unencrypted traffic, so can't be accessed
    with kAFS.  Set the AF_RXRPC security level to encrypt client calls to deal
    with this.
    
    Note that incoming service calls are set by the remote client and so aren't
    affected by this.
    
    This requires an AF_RXRPC patch to pass the value set by setsockopt to calls
    begun by the kernel.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index d0eee5d32c94..08735948f15d 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -41,6 +41,7 @@ int afs_open_socket(struct afs_net *net)
 {
 	struct sockaddr_rxrpc srx;
 	struct socket *socket;
+	unsigned int min_level;
 	int ret;
 
 	_enter("");
@@ -60,6 +61,12 @@ int afs_open_socket(struct afs_net *net)
 	srx.transport.sin6.sin6_family	= AF_INET6;
 	srx.transport.sin6.sin6_port	= htons(AFS_CM_PORT);
 
+	min_level = RXRPC_SECURITY_ENCRYPT;
+	ret = kernel_setsockopt(socket, SOL_RXRPC, RXRPC_MIN_SECURITY_LEVEL,
+				(void *)&min_level, sizeof(min_level));
+	if (ret < 0)
+		goto error_2;
+
 	ret = kernel_bind(socket, (struct sockaddr *) &srx, sizeof(srx));
 	if (ret == -EADDRINUSE) {
 		srx.transport.sin6.sin6_port = 0;

commit a86b06d1ccd218a6a50d6a3a88fbd2abcd0eaa94
Author: David Howells <dhowells@redhat.com>
Date:   Fri May 11 23:45:40 2018 +0100

    afs: Fix the handling of an unfound server in CM operations
    
    If the client cache manager operations that need the server record
    (CB.Callback, CB.InitCallBackState, and CB.InitCallBackState3) can't find
    the server record, they abort the call from the file server with
    RX_CALL_DEAD when they should return okay.
    
    Fixes: c35eccb1f614 ("[AFS]: Implement the CB.InitCallBackState3 operation.")
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 1f6235a6e9ae..d0eee5d32c94 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -497,11 +497,6 @@ static void afs_deliver_to_call(struct afs_call *call)
 		case -ECONNABORTED:
 			ASSERTCMP(state, ==, AFS_CALL_COMPLETE);
 			goto done;
-		case -ENOTCONN:
-			abort_code = RX_CALL_DEAD;
-			rxrpc_kernel_abort_call(call->net->socket, call->rxcall,
-						abort_code, ret, "KNC");
-			goto local_abort;
 		case -ENOTSUPP:
 			abort_code = RXGEN_OPCODE;
 			rxrpc_kernel_abort_call(call->net->socket, call->rxcall,

commit f2686b09269ec1a6f23028b5675d87c3b4579a4c
Author: David Howells <dhowells@redhat.com>
Date:   Thu May 10 14:12:50 2018 +0100

    afs: Fix giving up callbacks on server destruction
    
    When a server record is destroyed, we want to send a message to the server
    telling it that we're giving up all the callbacks it has promised us.
    
    Apply two fixes to this:
    
     (1) Only send the FS.GiveUpAllCallBacks message if we actually got a
         callback from that server.  We assume this to be the case if we
         performed at least one successful FS operation on that server.
    
     (2) Send it to the address last used for that server rather than always
         picking the first address in the list (which might be unreachable).
    
    Fixes: d2ddc776a458 ("afs: Overhaul volume and server record caching and fileserver rotation")
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 5c6263972ec9..1f6235a6e9ae 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -482,8 +482,12 @@ static void afs_deliver_to_call(struct afs_call *call)
 		state = READ_ONCE(call->state);
 		switch (ret) {
 		case 0:
-			if (state == AFS_CALL_CL_PROC_REPLY)
+			if (state == AFS_CALL_CL_PROC_REPLY) {
+				if (call->cbi)
+					set_bit(AFS_SERVER_FL_MAY_HAVE_CB,
+						&call->cbi->server->flags);
 				goto call_complete;
+			}
 			ASSERTCMP(state, >, AFS_CALL_CL_PROC_REPLY);
 			goto done;
 		case -EINPROGRESS:

commit 5f702c8e124f967146a735a19f0b00a2469487d1
Author: David Howells <dhowells@redhat.com>
Date:   Fri Apr 6 14:17:25 2018 +0100

    afs: Trace protocol errors
    
    Trace protocol errors detected in afs.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index f7ae54b6a393..5c6263972ec9 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -926,3 +926,12 @@ int afs_extract_data(struct afs_call *call, void *buf, size_t count,
 	afs_set_call_complete(call, ret, remote_abort);
 	return ret;
 }
+
+/*
+ * Log protocol error production.
+ */
+noinline int afs_protocol_error(struct afs_call *call, int error)
+{
+	trace_afs_protocol_error(call, error, __builtin_return_address(0));
+	return error;
+}

commit 5bb053bef82523a8fd78d650bca81c9f114fa276
Merge: bb2407a72197 159f02977b2f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 3 14:04:18 2018 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next
    
    Pull networking updates from David Miller:
    
     1) Support offloading wireless authentication to userspace via
        NL80211_CMD_EXTERNAL_AUTH, from Srinivas Dasari.
    
     2) A lot of work on network namespace setup/teardown from Kirill Tkhai.
        Setup and cleanup of namespaces now all run asynchronously and thus
        performance is significantly increased.
    
     3) Add rx/tx timestamping support to mv88e6xxx driver, from Brandon
        Streiff.
    
     4) Support zerocopy on RDS sockets, from Sowmini Varadhan.
    
     5) Use denser instruction encoding in x86 eBPF JIT, from Daniel
        Borkmann.
    
     6) Support hw offload of vlan filtering in mvpp2 dreiver, from Maxime
        Chevallier.
    
     7) Support grafting of child qdiscs in mlxsw driver, from Nogah
        Frankel.
    
     8) Add packet forwarding tests to selftests, from Ido Schimmel.
    
     9) Deal with sub-optimal GSO packets better in BBR congestion control,
        from Eric Dumazet.
    
    10) Support 5-tuple hashing in ipv6 multipath routing, from David Ahern.
    
    11) Add path MTU tests to selftests, from Stefano Brivio.
    
    12) Various bits of IPSEC offloading support for mlx5, from Aviad
        Yehezkel, Yossi Kuperman, and Saeed Mahameed.
    
    13) Support RSS spreading on ntuple filters in SFC driver, from Edward
        Cree.
    
    14) Lots of sockmap work from John Fastabend. Applications can use eBPF
        to filter sendmsg and sendpage operations.
    
    15) In-kernel receive TLS support, from Dave Watson.
    
    16) Add XDP support to ixgbevf, this is significant because it should
        allow optimized XDP usage in various cloud environments. From Tony
        Nguyen.
    
    17) Add new Intel E800 series "ice" ethernet driver, from Anirudh
        Venkataramanan et al.
    
    18) IP fragmentation match offload support in nfp driver, from Pieter
        Jansen van Vuuren.
    
    19) Support XDP redirect in i40e driver, from Björn Töpel.
    
    20) Add BPF_RAW_TRACEPOINT program type for accessing the arguments of
        tracepoints in their raw form, from Alexei Starovoitov.
    
    21) Lots of striding RQ improvements to mlx5 driver with many
        performance improvements, from Tariq Toukan.
    
    22) Use rhashtable for inet frag reassembly, from Eric Dumazet.
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next: (1678 commits)
      net: mvneta: improve suspend/resume
      net: mvneta: split rxq/txq init and txq deinit into SW and HW parts
      ipv6: frags: fix /proc/sys/net/ipv6/ip6frag_low_thresh
      net: bgmac: Fix endian access in bgmac_dma_tx_ring_free()
      net: bgmac: Correctly annotate register space
      route: check sysctl_fib_multipath_use_neigh earlier than hash
      fix typo in command value in drivers/net/phy/mdio-bitbang.
      sky2: Increase D3 delay to sky2 stops working after suspend
      net/mlx5e: Set EQE based as default TX interrupt moderation mode
      ibmvnic: Disable irqs before exiting reset from closed state
      net: sched: do not emit messages while holding spinlock
      vlan: also check phy_driver ts_info for vlan's real device
      Bluetooth: Mark expected switch fall-throughs
      Bluetooth: Set HCI_QUIRK_SIMULTANEOUS_DISCOVERY for BTUSB_QCA_ROME
      Bluetooth: btrsi: remove unused including <linux/version.h>
      Bluetooth: hci_bcm: Remove DMI quirk for the MINIX Z83-4
      sh_eth: kill useless check in __sh_eth_get_regs()
      sh_eth: add sh_eth_cpu_data::no_xdfar flag
      ipv6: factorize sk_wmem_alloc updates done by __ip6_append_data()
      ipv4: factorize sk_wmem_alloc updates done by __ip_append_data()
      ...

commit a25e21f0bcd25673b91b97b9805db33350feec0f
Author: David Howells <dhowells@redhat.com>
Date:   Tue Mar 27 23:03:00 2018 +0100

    rxrpc, afs: Use debug_ids rather than pointers in traces
    
    In rxrpc and afs, use the debug_ids that are monotonically allocated to
    various objects as they're allocated rather than pointers as kernel
    pointers are now hashed making them less useful.  Further, the debug ids
    aren't reused anywhere nearly as quickly.
    
    In addition, allow kernel services that use rxrpc, such as afs, to take
    numbers from the rxrpc counter, assign them to their own call struct and
    pass them in to rxrpc for both client and service calls so that the trace
    lines for each will have the same ID tag.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index e1126659f043..b819900916e6 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -131,6 +131,7 @@ static struct afs_call *afs_alloc_call(struct afs_net *net,
 
 	call->type = type;
 	call->net = net;
+	call->debug_id = atomic_inc_return(&rxrpc_debug_id);
 	atomic_set(&call->usage, 1);
 	INIT_WORK(&call->async_work, afs_process_async_call);
 	init_waitqueue_head(&call->waitq);
@@ -169,11 +170,12 @@ void afs_put_call(struct afs_call *call)
 		afs_put_server(call->net, call->cm_server);
 		afs_put_cb_interest(call->net, call->cbi);
 		kfree(call->request);
-		kfree(call);
 
-		o = atomic_dec_return(&net->nr_outstanding_calls);
 		trace_afs_call(call, afs_call_trace_free, 0, o,
 			       __builtin_return_address(0));
+		kfree(call);
+
+		o = atomic_dec_return(&net->nr_outstanding_calls);
 		if (o == 0)
 			wake_up_atomic_t(&net->nr_outstanding_calls);
 	}
@@ -378,7 +380,8 @@ long afs_make_call(struct afs_addr_cursor *ac, struct afs_call *call,
 					 (async ?
 					  afs_wake_up_async_call :
 					  afs_wake_up_call_waiter),
-					 call->upgrade);
+					 call->upgrade,
+					 call->debug_id);
 	if (IS_ERR(rxcall)) {
 		ret = PTR_ERR(rxcall);
 		goto error_kill_call;
@@ -727,7 +730,8 @@ void afs_charge_preallocation(struct work_struct *work)
 					       afs_wake_up_async_call,
 					       afs_rx_attach,
 					       (unsigned long)call,
-					       GFP_KERNEL) < 0)
+					       GFP_KERNEL,
+					       call->debug_id) < 0)
 			break;
 		call = NULL;
 	}

commit ab1fbe32477619b99921a203d16de9e66526b22a
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Mar 15 11:42:28 2018 +0100

    sched/wait, fs/afs: Convert wait_on_atomic_t() usage to the new wait_var_event() API
    
    The old wait_on_atomic_t() is going to get removed, use the more
    flexible wait_var_event() API instead.
    
    No change in functionality.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index e1126659f043..e613dd754383 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -103,8 +103,8 @@ void afs_close_socket(struct afs_net *net)
 	}
 
 	_debug("outstanding %u", atomic_read(&net->nr_outstanding_calls));
-	wait_on_atomic_t(&net->nr_outstanding_calls, atomic_t_wait,
-			 TASK_UNINTERRUPTIBLE);
+	wait_var_event(&net->nr_outstanding_calls,
+		       !atomic_read(&net->nr_outstanding_calls));
 	_debug("no outstanding calls");
 
 	kernel_sock_shutdown(net->socket, SHUT_RDWR);
@@ -175,7 +175,7 @@ void afs_put_call(struct afs_call *call)
 		trace_afs_call(call, afs_call_trace_free, 0, o,
 			       __builtin_return_address(0));
 		if (o == 0)
-			wake_up_atomic_t(&net->nr_outstanding_calls);
+			wake_up_var(&net->nr_outstanding_calls);
 	}
 }
 

commit 7888da95832d50a87bbfdb9f40620ddc66f94b3c
Author: Dan Carpenter <dan.carpenter@oracle.com>
Date:   Tue Jan 2 10:02:19 2018 +0000

    afs: Potential uninitialized variable in afs_extract_data()
    
    Smatch warns that:
    
        fs/afs/rxrpc.c:922 afs_extract_data()
        error: uninitialized symbol 'remote_abort'.
    
    Smatch is right that "remote_abort" might be uninitialized when we pass
    it to afs_set_call_complete().  I don't know if that function uses the
    uninitialized variable.  Anyway, the comment for rxrpc_kernel_recv_data(),
    says that "*_abort should also be initialised to 0." and this patch does
    that.
    
    Signed-off-by: Dan Carpenter <dan.carpenter@oracle.com>
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index ea1460b9b71a..e1126659f043 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -885,7 +885,7 @@ int afs_extract_data(struct afs_call *call, void *buf, size_t count,
 {
 	struct afs_net *net = call->net;
 	enum afs_call_state state;
-	u32 remote_abort;
+	u32 remote_abort = 0;
 	int ret;
 
 	_enter("{%s,%zu},,%zu,%d",

commit 98bf40cd99fcfed0705812b6cbdbb3b441a42970
Author: David Howells <dhowells@redhat.com>
Date:   Thu Nov 2 15:27:53 2017 +0000

    afs: Protect call->state changes against signals
    
    Protect call->state changes against the call being prematurely terminated
    due to a signal.
    
    What can happen is that a signal causes afs_wait_for_call_to_complete() to
    abort an afs_call because it's not yet complete whilst afs_deliver_to_call()
    is delivering data to that call.
    
    If the data delivery causes the state to change, this may overwrite the state
    of the afs_call, making it not-yet-complete again - but no further
    notifications will be forthcoming from AF_RXRPC as the rxrpc call has been
    aborted and completed, so kAFS will just hang in various places waiting for
    that call or on page bits that need clearing by that call.
    
    A tracepoint to monitor call state changes is also provided.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index bd44ae8b63d8..ea1460b9b71a 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -134,6 +134,7 @@ static struct afs_call *afs_alloc_call(struct afs_net *net,
 	atomic_set(&call->usage, 1);
 	INIT_WORK(&call->async_work, afs_process_async_call);
 	init_waitqueue_head(&call->waitq);
+	spin_lock_init(&call->state_lock);
 
 	o = atomic_inc_return(&net->nr_outstanding_calls);
 	trace_afs_call(call, afs_call_trace_alloc, 1, o,
@@ -288,8 +289,7 @@ static void afs_notify_end_request_tx(struct sock *sock,
 {
 	struct afs_call *call = (struct afs_call *)call_user_ID;
 
-	if (call->state == AFS_CALL_REQUESTING)
-		call->state = AFS_CALL_AWAIT_REPLY;
+	afs_set_call_state(call, AFS_CALL_CL_REQUESTING, AFS_CALL_CL_AWAIT_REPLY);
 }
 
 /*
@@ -444,82 +444,87 @@ long afs_make_call(struct afs_addr_cursor *ac, struct afs_call *call,
  */
 static void afs_deliver_to_call(struct afs_call *call)
 {
-	u32 abort_code;
+	enum afs_call_state state;
+	u32 abort_code, remote_abort = 0;
 	int ret;
 
 	_enter("%s", call->type->name);
 
-	while (call->state == AFS_CALL_AWAIT_REPLY ||
-	       call->state == AFS_CALL_AWAIT_OP_ID ||
-	       call->state == AFS_CALL_AWAIT_REQUEST ||
-	       call->state == AFS_CALL_AWAIT_ACK
+	while (state = READ_ONCE(call->state),
+	       state == AFS_CALL_CL_AWAIT_REPLY ||
+	       state == AFS_CALL_SV_AWAIT_OP_ID ||
+	       state == AFS_CALL_SV_AWAIT_REQUEST ||
+	       state == AFS_CALL_SV_AWAIT_ACK
 	       ) {
-		if (call->state == AFS_CALL_AWAIT_ACK) {
+		if (state == AFS_CALL_SV_AWAIT_ACK) {
 			size_t offset = 0;
 			ret = rxrpc_kernel_recv_data(call->net->socket,
 						     call->rxcall,
 						     NULL, 0, &offset, false,
-						     &call->abort_code,
+						     &remote_abort,
 						     &call->service_id);
 			trace_afs_recv_data(call, 0, offset, false, ret);
 
 			if (ret == -EINPROGRESS || ret == -EAGAIN)
 				return;
-			if (ret < 0)
-				call->error = ret;
-			if (ret < 0 || ret == 1)
+			if (ret < 0 || ret == 1) {
+				if (ret == 1)
+					ret = 0;
 				goto call_complete;
+			}
 			return;
 		}
 
 		ret = call->type->deliver(call);
+		state = READ_ONCE(call->state);
 		switch (ret) {
 		case 0:
-			if (call->state == AFS_CALL_AWAIT_REPLY)
+			if (state == AFS_CALL_CL_PROC_REPLY)
 				goto call_complete;
+			ASSERTCMP(state, >, AFS_CALL_CL_PROC_REPLY);
 			goto done;
 		case -EINPROGRESS:
 		case -EAGAIN:
 			goto out;
+		case -EIO:
 		case -ECONNABORTED:
-			goto save_error;
+			ASSERTCMP(state, ==, AFS_CALL_COMPLETE);
+			goto done;
 		case -ENOTCONN:
 			abort_code = RX_CALL_DEAD;
 			rxrpc_kernel_abort_call(call->net->socket, call->rxcall,
 						abort_code, ret, "KNC");
-			goto save_error;
+			goto local_abort;
 		case -ENOTSUPP:
 			abort_code = RXGEN_OPCODE;
 			rxrpc_kernel_abort_call(call->net->socket, call->rxcall,
 						abort_code, ret, "KIV");
-			goto save_error;
+			goto local_abort;
 		case -ENODATA:
 		case -EBADMSG:
 		case -EMSGSIZE:
 		default:
 			abort_code = RXGEN_CC_UNMARSHAL;
-			if (call->state != AFS_CALL_AWAIT_REPLY)
+			if (state != AFS_CALL_CL_AWAIT_REPLY)
 				abort_code = RXGEN_SS_UNMARSHAL;
 			rxrpc_kernel_abort_call(call->net->socket, call->rxcall,
 						abort_code, -EBADMSG, "KUM");
-			goto save_error;
+			goto local_abort;
 		}
 	}
 
 done:
-	if (call->state == AFS_CALL_COMPLETE && call->incoming)
+	if (state == AFS_CALL_COMPLETE && call->incoming)
 		afs_put_call(call);
 out:
 	_leave("");
 	return;
 
-save_error:
-	call->error = ret;
+local_abort:
+	abort_code = 0;
 call_complete:
-	if (call->state != AFS_CALL_COMPLETE) {
-		call->state = AFS_CALL_COMPLETE;
-		trace_afs_call_done(call);
-	}
+	afs_set_call_complete(call, ret, remote_abort);
+	state = AFS_CALL_COMPLETE;
 	goto done;
 }
 
@@ -551,14 +556,15 @@ static long afs_wait_for_call_to_complete(struct afs_call *call,
 		set_current_state(TASK_UNINTERRUPTIBLE);
 
 		/* deliver any messages that are in the queue */
-		if (call->state < AFS_CALL_COMPLETE && call->need_attention) {
+		if (!afs_check_call_state(call, AFS_CALL_COMPLETE) &&
+		    call->need_attention) {
 			call->need_attention = false;
 			__set_current_state(TASK_RUNNING);
 			afs_deliver_to_call(call);
 			continue;
 		}
 
-		if (call->state == AFS_CALL_COMPLETE)
+		if (afs_check_call_state(call, AFS_CALL_COMPLETE))
 			break;
 
 		life = rxrpc_kernel_check_life(call->net->socket, call->rxcall);
@@ -578,17 +584,17 @@ static long afs_wait_for_call_to_complete(struct afs_call *call,
 	__set_current_state(TASK_RUNNING);
 
 	/* Kill off the call if it's still live. */
-	if (call->state < AFS_CALL_COMPLETE) {
+	if (!afs_check_call_state(call, AFS_CALL_COMPLETE)) {
 		_debug("call interrupted");
 		if (rxrpc_kernel_abort_call(call->net->socket, call->rxcall,
-					    RX_USER_ABORT, -EINTR, "KWI")) {
-			call->error = -ERESTARTSYS;
-			trace_afs_call_done(call);
-		}
+					    RX_USER_ABORT, -EINTR, "KWI"))
+			afs_set_call_complete(call, -EINTR, 0);
 	}
 
+	spin_lock_bh(&call->state_lock);
 	ac->abort_code = call->abort_code;
 	ac->error = call->error;
+	spin_unlock_bh(&call->state_lock);
 
 	ret = ac->error;
 	switch (ret) {
@@ -713,7 +719,7 @@ void afs_charge_preallocation(struct work_struct *work)
 				break;
 
 			call->async = true;
-			call->state = AFS_CALL_AWAIT_OP_ID;
+			call->state = AFS_CALL_SV_AWAIT_OP_ID;
 			init_waitqueue_head(&call->waitq);
 		}
 
@@ -769,7 +775,7 @@ static int afs_deliver_cm_op_id(struct afs_call *call)
 		return ret;
 
 	call->operation_ID = ntohl(call->tmp);
-	call->state = AFS_CALL_AWAIT_REQUEST;
+	afs_set_call_state(call, AFS_CALL_SV_AWAIT_OP_ID, AFS_CALL_SV_AWAIT_REQUEST);
 	call->offset = 0;
 
 	/* ask the cache manager to route the call (it'll change the call type
@@ -794,8 +800,7 @@ static void afs_notify_end_reply_tx(struct sock *sock,
 {
 	struct afs_call *call = (struct afs_call *)call_user_ID;
 
-	if (call->state == AFS_CALL_REPLYING)
-		call->state = AFS_CALL_AWAIT_ACK;
+	afs_set_call_state(call, AFS_CALL_SV_REPLYING, AFS_CALL_SV_AWAIT_ACK);
 }
 
 /*
@@ -879,6 +884,8 @@ int afs_extract_data(struct afs_call *call, void *buf, size_t count,
 		     bool want_more)
 {
 	struct afs_net *net = call->net;
+	enum afs_call_state state;
+	u32 remote_abort;
 	int ret;
 
 	_enter("{%s,%zu},,%zu,%d",
@@ -888,29 +895,30 @@ int afs_extract_data(struct afs_call *call, void *buf, size_t count,
 
 	ret = rxrpc_kernel_recv_data(net->socket, call->rxcall,
 				     buf, count, &call->offset,
-				     want_more, &call->abort_code,
+				     want_more, &remote_abort,
 				     &call->service_id);
 	trace_afs_recv_data(call, count, call->offset, want_more, ret);
 	if (ret == 0 || ret == -EAGAIN)
 		return ret;
 
+	state = READ_ONCE(call->state);
 	if (ret == 1) {
-		switch (call->state) {
-		case AFS_CALL_AWAIT_REPLY:
-			call->state = AFS_CALL_COMPLETE;
-			trace_afs_call_done(call);
+		switch (state) {
+		case AFS_CALL_CL_AWAIT_REPLY:
+			afs_set_call_state(call, state, AFS_CALL_CL_PROC_REPLY);
 			break;
-		case AFS_CALL_AWAIT_REQUEST:
-			call->state = AFS_CALL_REPLYING;
+		case AFS_CALL_SV_AWAIT_REQUEST:
+			afs_set_call_state(call, state, AFS_CALL_SV_REPLYING);
 			break;
+		case AFS_CALL_COMPLETE:
+			kdebug("prem complete %d", call->error);
+			return -EIO;
 		default:
 			break;
 		}
 		return 0;
 	}
 
-	call->error = ret;
-	call->state = AFS_CALL_COMPLETE;
-	trace_afs_call_done(call);
+	afs_set_call_complete(call, ret, remote_abort);
 	return ret;
 }

commit 83732ec5146916bd49b3036b0ea7dedb7831b90e
Author: Marc Dionne <marc.dionne@auristor.com>
Date:   Thu Nov 2 15:27:52 2017 +0000

    afs: Use a dynamic port if 7001 is in use
    
    It is not required that the afs client operate on port 7001.
    The port could be in use because another kernel or userspace
    client has already bound to it.
    
    If the port is in use, just fallback to using a dynamic port.
    
    Signed-off-by: Marc Dionne <marc.dionne@auristor.com>
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 06a51d70b82b..bd44ae8b63d8 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -61,6 +61,10 @@ int afs_open_socket(struct afs_net *net)
 	srx.transport.sin6.sin6_port	= htons(AFS_CM_PORT);
 
 	ret = kernel_bind(socket, (struct sockaddr *) &srx, sizeof(srx));
+	if (ret == -EADDRINUSE) {
+		srx.transport.sin6.sin6_port = 0;
+		ret = kernel_bind(socket, (struct sockaddr *) &srx, sizeof(srx));
+	}
 	if (ret < 0)
 		goto error_2;
 

commit 2c099014a0a456012c1778e80adce839bf956b77
Author: David Howells <dhowells@redhat.com>
Date:   Thu Nov 2 15:27:51 2017 +0000

    afs: Trace the sending of pages
    
    Add a pair of tracepoints to log the sending of pages for an FS.StoreData
    or FS.StoreData64 operation.
    
    Tracepoint afs_send_pages notes each set of pages added to the operation.
    There may be several of these per operation as we get up at most 8
    contiguous pages in one go because the bvec we're using is on the stack.
    
    Tracepoint afs_sent_pages notes the end of adding data from a whole run of
    pages to the operation and the completion of the request phase.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 59cc58022c4d..06a51d70b82b 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -303,6 +303,8 @@ static int afs_send_pages(struct afs_call *call, struct msghdr *msg)
 
 	do {
 		afs_load_bvec(call, msg, bv, first, last, offset);
+		trace_afs_send_pages(call, msg, first, last, offset);
+
 		offset = 0;
 		bytes = msg->msg_iter.count;
 		nr = msg->msg_iter.nr_segs;
@@ -317,6 +319,7 @@ static int afs_send_pages(struct afs_call *call, struct msghdr *msg)
 		first += nr;
 	} while (first <= last);
 
+	trace_afs_sent_pages(call, call->first, last, first, ret);
 	return ret;
 }
 

commit 025db80c9e421efef11f2b83b7f78a11476f06db
Author: David Howells <dhowells@redhat.com>
Date:   Thu Nov 2 15:27:51 2017 +0000

    afs: Trace the initiation and completion of client calls
    
    Add tracepoints to trace the initiation and completion of client calls
    within the kafs filesystem.
    
    The afs_make_vl_call tracepoint watches calls to the volume location
    database server.
    
    The afs_make_fs_call tracepoint watches calls to the file server.
    
    The afs_call_done tracepoint watches for call completion.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 1d075696bf55..59cc58022c4d 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -219,6 +219,7 @@ struct afs_call *afs_alloc_flat_call(struct afs_net *net,
 			goto nomem_free;
 	}
 
+	call->operation_ID = type->op;
 	init_waitqueue_head(&call->waitq);
 	return call;
 
@@ -422,6 +423,8 @@ long afs_make_call(struct afs_addr_cursor *ac, struct afs_call *call,
 		ac->abort_code = call->abort_code;
 		ac->responded = true;
 	}
+	call->error = ret;
+	trace_afs_call_done(call);
 error_kill_call:
 	afs_put_call(call);
 	ac->error = ret;
@@ -455,10 +458,10 @@ static void afs_deliver_to_call(struct afs_call *call)
 
 			if (ret == -EINPROGRESS || ret == -EAGAIN)
 				return;
-			if (ret == 1 || ret < 0) {
-				call->state = AFS_CALL_COMPLETE;
-				goto done;
-			}
+			if (ret < 0)
+				call->error = ret;
+			if (ret < 0 || ret == 1)
+				goto call_complete;
 			return;
 		}
 
@@ -466,7 +469,7 @@ static void afs_deliver_to_call(struct afs_call *call)
 		switch (ret) {
 		case 0:
 			if (call->state == AFS_CALL_AWAIT_REPLY)
-				call->state = AFS_CALL_COMPLETE;
+				goto call_complete;
 			goto done;
 		case -EINPROGRESS:
 		case -EAGAIN:
@@ -505,7 +508,11 @@ static void afs_deliver_to_call(struct afs_call *call)
 
 save_error:
 	call->error = ret;
-	call->state = AFS_CALL_COMPLETE;
+call_complete:
+	if (call->state != AFS_CALL_COMPLETE) {
+		call->state = AFS_CALL_COMPLETE;
+		trace_afs_call_done(call);
+	}
 	goto done;
 }
 
@@ -567,8 +574,10 @@ static long afs_wait_for_call_to_complete(struct afs_call *call,
 	if (call->state < AFS_CALL_COMPLETE) {
 		_debug("call interrupted");
 		if (rxrpc_kernel_abort_call(call->net->socket, call->rxcall,
-					    RX_USER_ABORT, -EINTR, "KWI"))
+					    RX_USER_ABORT, -EINTR, "KWI")) {
 			call->error = -ERESTARTSYS;
+			trace_afs_call_done(call);
+		}
 	}
 
 	ac->abort_code = call->abort_code;
@@ -882,6 +891,7 @@ int afs_extract_data(struct afs_call *call, void *buf, size_t count,
 		switch (call->state) {
 		case AFS_CALL_AWAIT_REPLY:
 			call->state = AFS_CALL_COMPLETE;
+			trace_afs_call_done(call);
 			break;
 		case AFS_CALL_AWAIT_REQUEST:
 			call->state = AFS_CALL_REPLYING;
@@ -894,5 +904,6 @@ int afs_extract_data(struct afs_call *call, void *buf, size_t count,
 
 	call->error = ret;
 	call->state = AFS_CALL_COMPLETE;
+	trace_afs_call_done(call);
 	return ret;
 }

commit 1199db603511d7463d9d3840f96f61967affc766
Author: David Howells <dhowells@redhat.com>
Date:   Thu Nov 2 15:27:51 2017 +0000

    afs: Fix total-length calculation for multiple-page send
    
    Fix the total-length calculation in afs_make_call() when the operation
    being dispatched has data from a series of pages attached.
    
    Despite the patched code looking like that it should reduce mathematically
    to the current code, it doesn't because the 32-bit unsigned arithmetic
    being used to calculate the page-offset-difference doesn't correctly extend
    to a 64-bit value when the result is effectively negative.
    
    Without this, some FS.StoreData operations that span multiple pages fail,
    reporting too little or too much data.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 21f43d3acb91..1d075696bf55 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -350,8 +350,17 @@ long afs_make_call(struct afs_addr_cursor *ac, struct afs_call *call,
 	 */
 	tx_total_len = call->request_size;
 	if (call->send_pages) {
-		tx_total_len += call->last_to - call->first_offset;
-		tx_total_len += (call->last - call->first) * PAGE_SIZE;
+		if (call->last == call->first) {
+			tx_total_len += call->last_to - call->first_offset;
+		} else {
+			/* It looks mathematically like you should be able to
+			 * combine the following lines with the ones above, but
+			 * unsigned arithmetic is fun when it wraps...
+			 */
+			tx_total_len += PAGE_SIZE - call->first_offset;
+			tx_total_len += call->last_to;
+			tx_total_len += (call->last - call->first - 1) * PAGE_SIZE;
+		}
 	}
 
 	/* create a call */

commit 5f0fc8ba6a1eec510a1e43def48697985d948a2c
Author: David Howells <dhowells@redhat.com>
Date:   Thu Nov 2 15:27:51 2017 +0000

    afs: Only progress call state at end of Tx phase from rxrpc callback
    
    Only progress the AFS call state at the end of Tx phase from the callback
    passed to rxrpc_kernel_send_data() rather than setting it before the last
    data send call.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 1bbd5854507d..21f43d3acb91 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -381,13 +381,6 @@ long afs_make_call(struct afs_addr_cursor *ac, struct afs_call *call,
 	msg.msg_controllen	= 0;
 	msg.msg_flags		= MSG_WAITALL | (call->send_pages ? MSG_MORE : 0);
 
-	/* We have to change the state *before* sending the last packet as
-	 * rxrpc might give us the reply before it returns from sending the
-	 * request.  Further, if the send fails, we may already have been given
-	 * a notification and may have collected it.
-	 */
-	if (!call->send_pages)
-		call->state = AFS_CALL_AWAIT_REPLY;
 	ret = rxrpc_kernel_send_data(call->net->socket, rxcall,
 				     &msg, call->request_size,
 				     afs_notify_end_request_tx);
@@ -799,7 +792,6 @@ void afs_send_empty_reply(struct afs_call *call)
 	msg.msg_controllen	= 0;
 	msg.msg_flags		= 0;
 
-	call->state = AFS_CALL_AWAIT_ACK;
 	switch (rxrpc_kernel_send_data(net->socket, call->rxcall, &msg, 0,
 				       afs_notify_end_reply_tx)) {
 	case 0:
@@ -839,7 +831,6 @@ void afs_send_simple_reply(struct afs_call *call, const void *buf, size_t len)
 	msg.msg_controllen	= 0;
 	msg.msg_flags		= 0;
 
-	call->state = AFS_CALL_AWAIT_ACK;
 	n = rxrpc_kernel_send_data(net->socket, call->rxcall, &msg, len,
 				   afs_notify_end_reply_tx);
 	if (n >= 0) {

commit d2ddc776a4581d900fc3bdc7803b403daae64d88
Author: David Howells <dhowells@redhat.com>
Date:   Thu Nov 2 15:27:50 2017 +0000

    afs: Overhaul volume and server record caching and fileserver rotation
    
    The current code assumes that volumes and servers are per-cell and are
    never shared, but this is not enforced, and, indeed, public cells do exist
    that are aliases of each other.  Further, an organisation can, say, set up
    a public cell and a private cell with overlapping, but not identical, sets
    of servers.  The difference is purely in the database attached to the VL
    servers.
    
    The current code will malfunction if it sees a server in two cells as it
    assumes global address -> server record mappings and that each server is in
    just one cell.
    
    Further, each server may have multiple addresses - and may have addresses
    of different families (IPv4 and IPv6, say).
    
    To this end, the following structural changes are made:
    
     (1) Server record management is overhauled:
    
         (a) Server records are made independent of cell.  The namespace keeps
             track of them, volume records have lists of them and each vnode
             has a server on which its callback interest currently resides.
    
         (b) The cell record no longer keeps a list of servers known to be in
             that cell.
    
         (c) The server records are now kept in a flat list because there's no
             single address to sort on.
    
         (d) Server records are now keyed by their UUID within the namespace.
    
         (e) The addresses for a server are obtained with the VL.GetAddrsU
             rather than with VL.GetEntryByName, using the server's UUID as a
             parameter.
    
         (f) Cached server records are garbage collected after a period of
             non-use and are counted out of existence before purging is allowed
             to complete.  This protects the work functions against rmmod.
    
         (g) The servers list is now in /proc/fs/afs/servers.
    
     (2) Volume record management is overhauled:
    
         (a) An RCU-replaceable server list is introduced.  This tracks both
             servers and their coresponding callback interests.
    
         (b) The superblock is now keyed on cell record and numeric volume ID.
    
         (c) The volume record is now tied to the superblock which mounts it,
             and is activated when mounted and deactivated when unmounted.
             This makes it easier to handle the cache cookie without causing a
             double-use in fscache.
    
         (d) The volume record is loaded from the VLDB using VL.GetEntryByNameU
             to get the server UUID list.
    
         (e) The volume name is updated if it is seen to have changed when the
             volume is updated (the update is keyed on the volume ID).
    
     (3) The vlocation record is got rid of and VLDB records are no longer
         cached.  Sufficient information is stored in the volume record, though
         an update to a volume record is now no longer shared between related
         volumes (volumes come in bundles of three: R/W, R/O and backup).
    
    and the following procedural changes are made:
    
     (1) The fileserver cursor introduced previously is now fleshed out and
         used to iterate over fileservers and their addresses.
    
     (2) Volume status is checked during iteration, and the server list is
         replaced if a change is detected.
    
     (3) Server status is checked during iteration, and the address list is
         replaced if a change is detected.
    
     (4) The abort code is saved into the address list cursor and -ECONNABORTED
         returned in afs_make_call() if a remote abort happened rather than
         translating the abort into an error message.  This allows actions to
         be taken depending on the abort code more easily.
    
         (a) If a VMOVED abort is seen then this is handled by rechecking the
             volume and restarting the iteration.
    
         (b) If a VBUSY, VRESTARTING or VSALVAGING abort is seen then this is
             handled by sleeping for a short period and retrying and/or trying
             other servers that might serve that volume.  A message is also
             displayed once until the condition has cleared.
    
         (c) If a VOFFLINE abort is seen, then this is handled as VBUSY for the
             moment.
    
         (d) If a VNOVOL abort is seen, the volume is rechecked in the VLDB to
             see if it has been deleted; if not, the fileserver is probably
             indicating that the volume couldn't be attached and needs
             salvaging.
    
         (e) If statfs() sees one of these aborts, it does not sleep, but
             rather returns an error, so as not to block the umount program.
    
     (5) The fileserver iteration functions in vnode.c are now merged into
         their callers and more heavily macroised around the cursor.  vnode.c
         is removed.
    
     (6) Operations on a particular vnode are serialised on that vnode because
         the server will lock that vnode whilst it operates on it, so a second
         op sent will just have to wait.
    
     (7) Fileservers are probed with FS.GetCapabilities before being used.
         This is where service upgrade will be done.
    
     (8) A callback interest on a fileserver is set up before an FS operation
         is performed and passed through to afs_make_call() so that it can be
         set on the vnode if the operation returns a callback.  The callback
         interest is passed through to afs_iget() also so that it can be set
         there too.
    
    In general, record updating is done on an as-needed basis when we try to
    access servers, volumes or vnodes rather than offloading it to work items
    and special threads.
    
    Notes:
    
     (1) Pre AFS-3.4 servers are no longer supported, though this can be added
         back if necessary (AFS-3.4 was released in 1998).
    
     (2) VBUSY is retried forever for the moment at intervals of 1s.
    
     (3) /proc/fs/afs/<cell>/servers no longer exists.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 5ddfb7c4cf78..1bbd5854507d 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -20,7 +20,7 @@
 struct workqueue_struct *afs_async_calls;
 
 static void afs_wake_up_call_waiter(struct sock *, struct rxrpc_call *, unsigned long);
-static long afs_wait_for_call_to_complete(struct afs_call *);
+static long afs_wait_for_call_to_complete(struct afs_call *, struct afs_addr_cursor *);
 static void afs_wake_up_async_call(struct sock *, struct rxrpc_call *, unsigned long);
 static void afs_process_async_call(struct work_struct *);
 static void afs_rx_new_call(struct sock *, struct rxrpc_call *, unsigned long);
@@ -162,6 +162,7 @@ void afs_put_call(struct afs_call *call)
 			call->type->destructor(call);
 
 		afs_put_server(call->net, call->cm_server);
+		afs_put_cb_interest(call->net, call->cbi);
 		kfree(call->request);
 		kfree(call);
 
@@ -330,7 +331,6 @@ long afs_make_call(struct afs_addr_cursor *ac, struct afs_call *call,
 	struct kvec iov[1];
 	size_t offset;
 	s64 tx_total_len;
-	u32 abort_code;
 	int ret;
 
 	_enter(",{%pISp},", &srx->transport);
@@ -362,7 +362,6 @@ long afs_make_call(struct afs_addr_cursor *ac, struct afs_call *call,
 					  afs_wake_up_async_call :
 					  afs_wake_up_call_waiter),
 					 call->upgrade);
-	call->key = NULL;
 	if (IS_ERR(rxcall)) {
 		ret = PTR_ERR(rxcall);
 		goto error_kill_call;
@@ -406,7 +405,7 @@ long afs_make_call(struct afs_addr_cursor *ac, struct afs_call *call,
 	if (call->async)
 		return -EINPROGRESS;
 
-	return afs_wait_for_call_to_complete(call);
+	return afs_wait_for_call_to_complete(call, ac);
 
 error_do_abort:
 	call->state = AFS_CALL_COMPLETE;
@@ -414,15 +413,16 @@ long afs_make_call(struct afs_addr_cursor *ac, struct afs_call *call,
 		rxrpc_kernel_abort_call(call->net->socket, rxcall,
 					RX_USER_ABORT, ret, "KSD");
 	} else {
-		abort_code = 0;
 		offset = 0;
 		rxrpc_kernel_recv_data(call->net->socket, rxcall, NULL,
 				       0, &offset, false, &call->abort_code,
 				       &call->service_id);
-		ret = afs_abort_to_error(call->abort_code);
+		ac->abort_code = call->abort_code;
+		ac->responded = true;
 	}
 error_kill_call:
 	afs_put_call(call);
+	ac->error = ret;
 	_leave(" = %d", ret);
 	return ret;
 }
@@ -510,7 +510,8 @@ static void afs_deliver_to_call(struct afs_call *call)
 /*
  * wait synchronously for a call to complete
  */
-static long afs_wait_for_call_to_complete(struct afs_call *call)
+static long afs_wait_for_call_to_complete(struct afs_call *call,
+					  struct afs_addr_cursor *ac)
 {
 	signed long rtt2, timeout;
 	long ret;
@@ -563,16 +564,25 @@ static long afs_wait_for_call_to_complete(struct afs_call *call)
 	/* Kill off the call if it's still live. */
 	if (call->state < AFS_CALL_COMPLETE) {
 		_debug("call interrupted");
-		rxrpc_kernel_abort_call(call->net->socket, call->rxcall,
-					RX_USER_ABORT, -EINTR, "KWI");
+		if (rxrpc_kernel_abort_call(call->net->socket, call->rxcall,
+					    RX_USER_ABORT, -EINTR, "KWI"))
+			call->error = -ERESTARTSYS;
 	}
 
-	ret = call->error;
-	if (ret < 0) {
-		ret = afs_abort_to_error(call->abort_code);
-	} else if (ret == 0 && call->ret_reply0) {
-		ret = (long)call->reply[0];
-		call->reply[0] = NULL;
+	ac->abort_code = call->abort_code;
+	ac->error = call->error;
+
+	ret = ac->error;
+	switch (ret) {
+	case 0:
+		if (call->ret_reply0) {
+			ret = (long)call->reply[0];
+			call->reply[0] = NULL;
+		}
+		/* Fall through */
+	case -ECONNABORTED:
+		ac->responded = true;
+		break;
 	}
 
 	_debug("call complete");
@@ -882,10 +892,7 @@ int afs_extract_data(struct afs_call *call, void *buf, size_t count,
 		return 0;
 	}
 
-	if (ret == -ECONNABORTED)
-		call->error = afs_abort_to_error(call->abort_code);
-	else
-		call->error = ret;
+	call->error = ret;
 	call->state = AFS_CALL_COMPLETE;
 	return ret;
 }

commit 8b2a464ced77fe35be72ab7d38152a9439daf8d3
Author: David Howells <dhowells@redhat.com>
Date:   Thu Nov 2 15:27:50 2017 +0000

    afs: Add an address list concept
    
    Add an RCU replaceable address list structure to hold a list of server
    addresses.  The list also holds the
    
    To this end:
    
     (1) A cell's VL server address list can be loaded directly via insmod or
         echo to /proc/fs/afs/cells or dynamically from a DNS query for AFSDB
         or SRV records.
    
     (2) Anyone wanting to use a cell's VL server address must wait until the
         cell record comes online and has tried to obtain some addresses.
    
     (3) An FS server's address list, for the moment, has a single entry that
         is the key to the server list.  This will change in the future when a
         server is instead keyed on its UUID and the VL.GetAddrsU operation is
         used.
    
     (4) An 'address cursor' concept is introduced to handle iteration through
         the address list.  This is passed to the afs_make_call() as, in the
         future, stuff (such as abort code) that doesn't outlast the call will
         be returned in it.
    
    In the future, we might want to annotate the list with information about
    how each address fares.  We might then want to propagate such annotations
    over address list replacement.
    
    Whilst we're at it, we allow IPv6 addresses to be specified in
    colon-delimited lists by enclosing them in square brackets.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index ac1e25f957b1..5ddfb7c4cf78 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -321,9 +321,10 @@ static int afs_send_pages(struct afs_call *call, struct msghdr *msg)
 /*
  * initiate a call
  */
-long afs_make_call(struct sockaddr_rxrpc *srx, struct afs_call *call,
+long afs_make_call(struct afs_addr_cursor *ac, struct afs_call *call,
 		   gfp_t gfp, bool async)
 {
+	struct sockaddr_rxrpc *srx = ac->addr;
 	struct rxrpc_call *rxcall;
 	struct msghdr msg;
 	struct kvec iov[1];

commit d0676a16781d0972969dff8b3f3f819599cc4b07
Author: David Howells <dhowells@redhat.com>
Date:   Thu Nov 2 15:27:49 2017 +0000

    afs: Rename struct afs_call server member to cm_server
    
    Rename the server member of struct afs_call to cm_server as we're only
    going to be using it for incoming calls for the Cache Manager service.
    This makes it easier to differentiate from the pointer to the target server
    for the client, which will point to a different structure to allow for
    callback handling.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index d57f09b5d27b..ac1e25f957b1 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -161,6 +161,7 @@ void afs_put_call(struct afs_call *call)
 		if (call->type->destructor)
 			call->type->destructor(call);
 
+		afs_put_server(call->net, call->cm_server);
 		kfree(call->request);
 		kfree(call);
 

commit 33cd7f2b76717ac8dda566d8b4f518e803ae2618
Author: David Howells <dhowells@redhat.com>
Date:   Thu Nov 2 15:27:48 2017 +0000

    afs: Potentially return call->reply[0] from afs_make_call()
    
    If call->ret_reply0 is set, return call->reply[0] on success.  Change the
    return type of afs_make_call() to long so that this can be passed back
    without bit loss and then cast to a pointer if required.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index fc49193e12c4..d57f09b5d27b 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -20,7 +20,7 @@
 struct workqueue_struct *afs_async_calls;
 
 static void afs_wake_up_call_waiter(struct sock *, struct rxrpc_call *, unsigned long);
-static int afs_wait_for_call_to_complete(struct afs_call *);
+static long afs_wait_for_call_to_complete(struct afs_call *);
 static void afs_wake_up_async_call(struct sock *, struct rxrpc_call *, unsigned long);
 static void afs_process_async_call(struct work_struct *);
 static void afs_rx_new_call(struct sock *, struct rxrpc_call *, unsigned long);
@@ -320,8 +320,8 @@ static int afs_send_pages(struct afs_call *call, struct msghdr *msg)
 /*
  * initiate a call
  */
-int afs_make_call(struct sockaddr_rxrpc *srx, struct afs_call *call,
-		  gfp_t gfp, bool async)
+long afs_make_call(struct sockaddr_rxrpc *srx, struct afs_call *call,
+		   gfp_t gfp, bool async)
 {
 	struct rxrpc_call *rxcall;
 	struct msghdr msg;
@@ -415,9 +415,9 @@ int afs_make_call(struct sockaddr_rxrpc *srx, struct afs_call *call,
 		abort_code = 0;
 		offset = 0;
 		rxrpc_kernel_recv_data(call->net->socket, rxcall, NULL,
-				       0, &offset, false, &abort_code,
+				       0, &offset, false, &call->abort_code,
 				       &call->service_id);
-		ret = afs_abort_to_error(abort_code);
+		ret = afs_abort_to_error(call->abort_code);
 	}
 error_kill_call:
 	afs_put_call(call);
@@ -468,7 +468,7 @@ static void afs_deliver_to_call(struct afs_call *call)
 		case -EAGAIN:
 			goto out;
 		case -ECONNABORTED:
-			goto call_complete;
+			goto save_error;
 		case -ENOTCONN:
 			abort_code = RX_CALL_DEAD;
 			rxrpc_kernel_abort_call(call->net->socket, call->rxcall,
@@ -501,7 +501,6 @@ static void afs_deliver_to_call(struct afs_call *call)
 
 save_error:
 	call->error = ret;
-call_complete:
 	call->state = AFS_CALL_COMPLETE;
 	goto done;
 }
@@ -509,10 +508,10 @@ static void afs_deliver_to_call(struct afs_call *call)
 /*
  * wait synchronously for a call to complete
  */
-static int afs_wait_for_call_to_complete(struct afs_call *call)
+static long afs_wait_for_call_to_complete(struct afs_call *call)
 {
 	signed long rtt2, timeout;
-	int ret;
+	long ret;
 	u64 rtt;
 	u32 life, last_life;
 
@@ -567,9 +566,16 @@ static int afs_wait_for_call_to_complete(struct afs_call *call)
 	}
 
 	ret = call->error;
+	if (ret < 0) {
+		ret = afs_abort_to_error(call->abort_code);
+	} else if (ret == 0 && call->ret_reply0) {
+		ret = (long)call->reply[0];
+		call->reply[0] = NULL;
+	}
+
 	_debug("call complete");
 	afs_put_call(call);
-	_leave(" = %d", ret);
+	_leave(" = %p", (void *)ret);
 	return ret;
 }
 

commit 97e3043ad82c93b7c2e3c4bfc518f7401f175821
Author: David Howells <dhowells@redhat.com>
Date:   Thu Nov 2 15:27:48 2017 +0000

    afs: Condense afs_call's reply{,2,3,4} into an array
    
    Condense struct afs_call's reply anchor members - reply{,2,3,4} - into an
    array.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 5f06cf720340..fc49193e12c4 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -639,7 +639,7 @@ static void afs_process_async_call(struct work_struct *work)
 	}
 
 	if (call->state == AFS_CALL_COMPLETE) {
-		call->reply = NULL;
+		call->reply[0] = NULL;
 
 		/* We have two refs to release - one from the alloc and one
 		 * queued with the work item - and we can't just deallocate the

commit f780c8ea0e73c0d371ffbebff91bb7555697219f
Author: David Howells <dhowells@redhat.com>
Date:   Thu Nov 2 15:27:48 2017 +0000

    afs: Consolidate abort_to_error translators
    
    The AFS abort code space is shared across all services, so there's no need
    for separate abort_to_error translators for each service.
    
    Consolidate them into a single function and remove the function pointers
    for them.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 5d2c1a34ffd5..5f06cf720340 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -31,7 +31,6 @@ static int afs_deliver_cm_op_id(struct afs_call *);
 static const struct afs_call_type afs_RXCMxxxx = {
 	.name		= "CB.xxxx",
 	.deliver	= afs_deliver_cm_op_id,
-	.abort_to_error	= afs_abort_to_error,
 };
 
 /*
@@ -418,7 +417,7 @@ int afs_make_call(struct sockaddr_rxrpc *srx, struct afs_call *call,
 		rxrpc_kernel_recv_data(call->net->socket, rxcall, NULL,
 				       0, &offset, false, &abort_code,
 				       &call->service_id);
-		ret = call->type->abort_to_error(abort_code);
+		ret = afs_abort_to_error(abort_code);
 	}
 error_kill_call:
 	afs_put_call(call);
@@ -876,7 +875,7 @@ int afs_extract_data(struct afs_call *call, void *buf, size_t count,
 	}
 
 	if (ret == -ECONNABORTED)
-		call->error = call->type->abort_to_error(call->abort_code);
+		call->error = afs_abort_to_error(call->abort_code);
 	else
 		call->error = ret;
 	call->state = AFS_CALL_COMPLETE;

commit 3838d3ecdea496699a8c13c183d4df5dfe8e1a3e
Author: David Howells <dhowells@redhat.com>
Date:   Thu Nov 2 15:27:47 2017 +0000

    afs: Allow IPv6 address specification of VL servers
    
    Allow VL server specifications to be given IPv6 addresses as well as IPv4
    addresses, for example as:
    
            echo add foo.org 1111:2222:3333:0:4444:5555:6666:7777 >/proc/fs/afs/cells
    
    Note that ':' is the expected separator for separating IPv4 addresses, but
    if a ',' is detected or no '.' is detected in the string, the delimiter is
    switched to ','.
    
    This also works with DNS AFSDB or SRV record strings fetched by upcall from
    userspace.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index c108effb54be..5d2c1a34ffd5 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -46,21 +46,20 @@ int afs_open_socket(struct afs_net *net)
 
 	_enter("");
 
-	ret = sock_create_kern(&init_net, AF_RXRPC, SOCK_DGRAM, PF_INET, &socket);
+	ret = sock_create_kern(&init_net, AF_RXRPC, SOCK_DGRAM, PF_INET6, &socket);
 	if (ret < 0)
 		goto error_1;
 
 	socket->sk->sk_allocation = GFP_NOFS;
 
 	/* bind the callback manager's address to make this a server socket */
+	memset(&srx, 0, sizeof(srx));
 	srx.srx_family			= AF_RXRPC;
 	srx.srx_service			= CM_SERVICE;
 	srx.transport_type		= SOCK_DGRAM;
-	srx.transport_len		= sizeof(srx.transport.sin);
-	srx.transport.sin.sin_family	= AF_INET;
-	srx.transport.sin.sin_port	= htons(AFS_CM_PORT);
-	memset(&srx.transport.sin.sin_addr, 0,
-	       sizeof(srx.transport.sin.sin_addr));
+	srx.transport_len		= sizeof(srx.transport.sin6);
+	srx.transport.sin6.sin6_family	= AF_INET6;
+	srx.transport.sin6.sin6_port	= htons(AFS_CM_PORT);
 
 	ret = kernel_bind(socket, (struct sockaddr *) &srx, sizeof(srx));
 	if (ret < 0)

commit 4d9df9868f31df6725481135c10ac6419ce58d44
Author: David Howells <dhowells@redhat.com>
Date:   Thu Nov 2 15:27:47 2017 +0000

    afs: Keep and pass sockaddr_rxrpc addresses rather than in_addr
    
    Keep and pass sockaddr_rxrpc addresses around rather than keeping and
    passing in_addr addresses to allow for the use of IPv6 and non-standard
    port numbers in future.
    
    This also allows the port and service_id fields to be removed from the
    afs_call struct.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 656ceb285b85..c108effb54be 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -322,10 +322,9 @@ static int afs_send_pages(struct afs_call *call, struct msghdr *msg)
 /*
  * initiate a call
  */
-int afs_make_call(struct in_addr *addr, struct afs_call *call, gfp_t gfp,
-		  bool async)
+int afs_make_call(struct sockaddr_rxrpc *srx, struct afs_call *call,
+		  gfp_t gfp, bool async)
 {
-	struct sockaddr_rxrpc srx;
 	struct rxrpc_call *rxcall;
 	struct msghdr msg;
 	struct kvec iov[1];
@@ -334,7 +333,7 @@ int afs_make_call(struct in_addr *addr, struct afs_call *call, gfp_t gfp,
 	u32 abort_code;
 	int ret;
 
-	_enter("%x,{%d},", addr->s_addr, ntohs(call->port));
+	_enter(",{%pISp},", &srx->transport);
 
 	ASSERT(call->type != NULL);
 	ASSERT(call->type->name != NULL);
@@ -345,15 +344,6 @@ int afs_make_call(struct in_addr *addr, struct afs_call *call, gfp_t gfp,
 
 	call->async = async;
 
-	memset(&srx, 0, sizeof(srx));
-	srx.srx_family = AF_RXRPC;
-	srx.srx_service = call->service_id;
-	srx.transport_type = SOCK_DGRAM;
-	srx.transport_len = sizeof(srx.transport.sin);
-	srx.transport.sin.sin_family = AF_INET;
-	srx.transport.sin.sin_port = call->port;
-	memcpy(&srx.transport.sin.sin_addr, addr, 4);
-
 	/* Work out the length we're going to transmit.  This is awkward for
 	 * calls such as FS.StoreData where there's an extra injection of data
 	 * after the initial fixed part.
@@ -365,7 +355,7 @@ int afs_make_call(struct in_addr *addr, struct afs_call *call, gfp_t gfp,
 	}
 
 	/* create a call */
-	rxcall = rxrpc_kernel_begin_call(call->net->socket, &srx, call->key,
+	rxcall = rxrpc_kernel_begin_call(call->net->socket, srx, call->key,
 					 (unsigned long)call,
 					 tx_total_len, gfp,
 					 (async ?

commit f044c8847bb61eff5e1e95b6f6bb950e7f4a73a4
Author: David Howells <dhowells@redhat.com>
Date:   Thu Nov 2 15:27:45 2017 +0000

    afs: Lay the groundwork for supporting network namespaces
    
    Lay the groundwork for supporting network namespaces (netns) to the AFS
    filesystem by moving various global features to a network-namespace struct
    (afs_net) and providing an instance of this as a temporary global variable
    that everything uses via accessor functions for the moment.
    
    The following changes have been made:
    
     (1) Store the netns in the superblock info.  This will be obtained from
         the mounter's nsproxy on a manual mount and inherited from the parent
         superblock on an automount.
    
     (2) The cell list is made per-netns.  It can be viewed through
         /proc/net/afs/cells and also be modified by writing commands to that
         file.
    
     (3) The local workstation cell is set per-ns in /proc/net/afs/rootcell.
         This is unset by default.
    
     (4) The 'rootcell' module parameter, which sets a cell and VL server list
         modifies the init net namespace, thereby allowing an AFS root fs to be
         theoretically used.
    
     (5) The volume location lists and the file lock manager are made
         per-netns.
    
     (6) The AF_RXRPC socket and associated I/O bits are made per-ns.
    
    The various workqueues remain global for the moment.
    
    Changes still to be made:
    
     (1) /proc/fs/afs/ should be moved to /proc/net/afs/ and a symlink emplaced
         from the old name.
    
     (2) A per-netns subsys needs to be registered for AFS into which it can
         store its per-netns data.
    
     (3) Rather than the AF_RXRPC socket being opened on module init, it needs
         to be opened on the creation of a superblock in that netns.
    
     (4) The socket needs to be closed when the last superblock using it is
         destroyed and all outstanding client calls on it have been completed.
         This prevents a reference loop on the namespace.
    
     (5) It is possible that several namespaces will want to use AFS, in which
         case each one will need its own UDP port.  These can either be set
         through /proc/net/afs/cm_port or the kernel can pick one at random.
         The init_ns gets 7001 by default.
    
    Other issues that need resolving:
    
     (1) The DNS keyring needs net-namespacing.
    
     (2) Where do upcalls go (eg. DNS request-key upcall)?
    
     (3) Need something like open_socket_in_file_ns() syscall so that AFS
         command line tools attempting to operate on an AFS file/volume have
         their RPC calls go to the right place.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 77f5420a1a24..656ceb285b85 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -17,10 +17,7 @@
 #include "internal.h"
 #include "afs_cm.h"
 
-struct socket *afs_socket; /* my RxRPC socket */
-static struct workqueue_struct *afs_async_calls;
-static struct afs_call *afs_spare_incoming_call;
-atomic_t afs_outstanding_calls;
+struct workqueue_struct *afs_async_calls;
 
 static void afs_wake_up_call_waiter(struct sock *, struct rxrpc_call *, unsigned long);
 static int afs_wait_for_call_to_complete(struct afs_call *);
@@ -37,15 +34,11 @@ static const struct afs_call_type afs_RXCMxxxx = {
 	.abort_to_error	= afs_abort_to_error,
 };
 
-static void afs_charge_preallocation(struct work_struct *);
-
-static DECLARE_WORK(afs_charge_preallocation_work, afs_charge_preallocation);
-
 /*
  * open an RxRPC socket and bind it to be a server for callback notifications
  * - the socket is left in blocking mode and non-blocking ops use MSG_DONTWAIT
  */
-int afs_open_socket(void)
+int afs_open_socket(struct afs_net *net)
 {
 	struct sockaddr_rxrpc srx;
 	struct socket *socket;
@@ -53,11 +46,6 @@ int afs_open_socket(void)
 
 	_enter("");
 
-	ret = -ENOMEM;
-	afs_async_calls = alloc_workqueue("kafsd", WQ_MEM_RECLAIM, 0);
-	if (!afs_async_calls)
-		goto error_0;
-
 	ret = sock_create_kern(&init_net, AF_RXRPC, SOCK_DGRAM, PF_INET, &socket);
 	if (ret < 0)
 		goto error_1;
@@ -85,16 +73,14 @@ int afs_open_socket(void)
 	if (ret < 0)
 		goto error_2;
 
-	afs_socket = socket;
-	afs_charge_preallocation(NULL);
+	net->socket = socket;
+	afs_charge_preallocation(&net->charge_preallocation_work);
 	_leave(" = 0");
 	return 0;
 
 error_2:
 	sock_release(socket);
 error_1:
-	destroy_workqueue(afs_async_calls);
-error_0:
 	_leave(" = %d", ret);
 	return ret;
 }
@@ -102,36 +88,36 @@ int afs_open_socket(void)
 /*
  * close the RxRPC socket AFS was using
  */
-void afs_close_socket(void)
+void afs_close_socket(struct afs_net *net)
 {
 	_enter("");
 
-	kernel_listen(afs_socket, 0);
+	kernel_listen(net->socket, 0);
 	flush_workqueue(afs_async_calls);
 
-	if (afs_spare_incoming_call) {
-		afs_put_call(afs_spare_incoming_call);
-		afs_spare_incoming_call = NULL;
+	if (net->spare_incoming_call) {
+		afs_put_call(net->spare_incoming_call);
+		net->spare_incoming_call = NULL;
 	}
 
-	_debug("outstanding %u", atomic_read(&afs_outstanding_calls));
-	wait_on_atomic_t(&afs_outstanding_calls, atomic_t_wait,
+	_debug("outstanding %u", atomic_read(&net->nr_outstanding_calls));
+	wait_on_atomic_t(&net->nr_outstanding_calls, atomic_t_wait,
 			 TASK_UNINTERRUPTIBLE);
 	_debug("no outstanding calls");
 
-	kernel_sock_shutdown(afs_socket, SHUT_RDWR);
+	kernel_sock_shutdown(net->socket, SHUT_RDWR);
 	flush_workqueue(afs_async_calls);
-	sock_release(afs_socket);
+	sock_release(net->socket);
 
 	_debug("dework");
-	destroy_workqueue(afs_async_calls);
 	_leave("");
 }
 
 /*
  * Allocate a call.
  */
-static struct afs_call *afs_alloc_call(const struct afs_call_type *type,
+static struct afs_call *afs_alloc_call(struct afs_net *net,
+				       const struct afs_call_type *type,
 				       gfp_t gfp)
 {
 	struct afs_call *call;
@@ -142,11 +128,12 @@ static struct afs_call *afs_alloc_call(const struct afs_call_type *type,
 		return NULL;
 
 	call->type = type;
+	call->net = net;
 	atomic_set(&call->usage, 1);
 	INIT_WORK(&call->async_work, afs_process_async_call);
 	init_waitqueue_head(&call->waitq);
 
-	o = atomic_inc_return(&afs_outstanding_calls);
+	o = atomic_inc_return(&net->nr_outstanding_calls);
 	trace_afs_call(call, afs_call_trace_alloc, 1, o,
 		       __builtin_return_address(0));
 	return call;
@@ -157,8 +144,9 @@ static struct afs_call *afs_alloc_call(const struct afs_call_type *type,
  */
 void afs_put_call(struct afs_call *call)
 {
+	struct afs_net *net = call->net;
 	int n = atomic_dec_return(&call->usage);
-	int o = atomic_read(&afs_outstanding_calls);
+	int o = atomic_read(&net->nr_outstanding_calls);
 
 	trace_afs_call(call, afs_call_trace_put, n + 1, o,
 		       __builtin_return_address(0));
@@ -169,7 +157,7 @@ void afs_put_call(struct afs_call *call)
 		ASSERT(call->type->name != NULL);
 
 		if (call->rxcall) {
-			rxrpc_kernel_end_call(afs_socket, call->rxcall);
+			rxrpc_kernel_end_call(net->socket, call->rxcall);
 			call->rxcall = NULL;
 		}
 		if (call->type->destructor)
@@ -178,11 +166,11 @@ void afs_put_call(struct afs_call *call)
 		kfree(call->request);
 		kfree(call);
 
-		o = atomic_dec_return(&afs_outstanding_calls);
+		o = atomic_dec_return(&net->nr_outstanding_calls);
 		trace_afs_call(call, afs_call_trace_free, 0, o,
 			       __builtin_return_address(0));
 		if (o == 0)
-			wake_up_atomic_t(&afs_outstanding_calls);
+			wake_up_atomic_t(&net->nr_outstanding_calls);
 	}
 }
 
@@ -194,7 +182,7 @@ int afs_queue_call_work(struct afs_call *call)
 	int u = atomic_inc_return(&call->usage);
 
 	trace_afs_call(call, afs_call_trace_work, u,
-		       atomic_read(&afs_outstanding_calls),
+		       atomic_read(&call->net->nr_outstanding_calls),
 		       __builtin_return_address(0));
 
 	INIT_WORK(&call->work, call->type->work);
@@ -207,12 +195,13 @@ int afs_queue_call_work(struct afs_call *call)
 /*
  * allocate a call with flat request and reply buffers
  */
-struct afs_call *afs_alloc_flat_call(const struct afs_call_type *type,
+struct afs_call *afs_alloc_flat_call(struct afs_net *net,
+				     const struct afs_call_type *type,
 				     size_t request_size, size_t reply_max)
 {
 	struct afs_call *call;
 
-	call = afs_alloc_call(type, GFP_NOFS);
+	call = afs_alloc_call(net, type, GFP_NOFS);
 	if (!call)
 		goto nomem_call;
 
@@ -317,7 +306,7 @@ static int afs_send_pages(struct afs_call *call, struct msghdr *msg)
 		bytes = msg->msg_iter.count;
 		nr = msg->msg_iter.nr_segs;
 
-		ret = rxrpc_kernel_send_data(afs_socket, call->rxcall, msg,
+		ret = rxrpc_kernel_send_data(call->net->socket, call->rxcall, msg,
 					     bytes, afs_notify_end_request_tx);
 		for (loop = 0; loop < nr; loop++)
 			put_page(bv[loop].bv_page);
@@ -352,7 +341,7 @@ int afs_make_call(struct in_addr *addr, struct afs_call *call, gfp_t gfp,
 
 	_debug("____MAKE %p{%s,%x} [%d]____",
 	       call, call->type->name, key_serial(call->key),
-	       atomic_read(&afs_outstanding_calls));
+	       atomic_read(&call->net->nr_outstanding_calls));
 
 	call->async = async;
 
@@ -376,7 +365,7 @@ int afs_make_call(struct in_addr *addr, struct afs_call *call, gfp_t gfp,
 	}
 
 	/* create a call */
-	rxcall = rxrpc_kernel_begin_call(afs_socket, &srx, call->key,
+	rxcall = rxrpc_kernel_begin_call(call->net->socket, &srx, call->key,
 					 (unsigned long)call,
 					 tx_total_len, gfp,
 					 (async ?
@@ -410,7 +399,7 @@ int afs_make_call(struct in_addr *addr, struct afs_call *call, gfp_t gfp,
 	 */
 	if (!call->send_pages)
 		call->state = AFS_CALL_AWAIT_REPLY;
-	ret = rxrpc_kernel_send_data(afs_socket, rxcall,
+	ret = rxrpc_kernel_send_data(call->net->socket, rxcall,
 				     &msg, call->request_size,
 				     afs_notify_end_request_tx);
 	if (ret < 0)
@@ -432,13 +421,14 @@ int afs_make_call(struct in_addr *addr, struct afs_call *call, gfp_t gfp,
 error_do_abort:
 	call->state = AFS_CALL_COMPLETE;
 	if (ret != -ECONNABORTED) {
-		rxrpc_kernel_abort_call(afs_socket, rxcall, RX_USER_ABORT,
-					ret, "KSD");
+		rxrpc_kernel_abort_call(call->net->socket, rxcall,
+					RX_USER_ABORT, ret, "KSD");
 	} else {
 		abort_code = 0;
 		offset = 0;
-		rxrpc_kernel_recv_data(afs_socket, rxcall, NULL, 0, &offset,
-				       false, &abort_code, &call->service_id);
+		rxrpc_kernel_recv_data(call->net->socket, rxcall, NULL,
+				       0, &offset, false, &abort_code,
+				       &call->service_id);
 		ret = call->type->abort_to_error(abort_code);
 	}
 error_kill_call:
@@ -464,7 +454,8 @@ static void afs_deliver_to_call(struct afs_call *call)
 	       ) {
 		if (call->state == AFS_CALL_AWAIT_ACK) {
 			size_t offset = 0;
-			ret = rxrpc_kernel_recv_data(afs_socket, call->rxcall,
+			ret = rxrpc_kernel_recv_data(call->net->socket,
+						     call->rxcall,
 						     NULL, 0, &offset, false,
 						     &call->abort_code,
 						     &call->service_id);
@@ -492,12 +483,12 @@ static void afs_deliver_to_call(struct afs_call *call)
 			goto call_complete;
 		case -ENOTCONN:
 			abort_code = RX_CALL_DEAD;
-			rxrpc_kernel_abort_call(afs_socket, call->rxcall,
+			rxrpc_kernel_abort_call(call->net->socket, call->rxcall,
 						abort_code, ret, "KNC");
 			goto save_error;
 		case -ENOTSUPP:
 			abort_code = RXGEN_OPCODE;
-			rxrpc_kernel_abort_call(afs_socket, call->rxcall,
+			rxrpc_kernel_abort_call(call->net->socket, call->rxcall,
 						abort_code, ret, "KIV");
 			goto save_error;
 		case -ENODATA:
@@ -507,7 +498,7 @@ static void afs_deliver_to_call(struct afs_call *call)
 			abort_code = RXGEN_CC_UNMARSHAL;
 			if (call->state != AFS_CALL_AWAIT_REPLY)
 				abort_code = RXGEN_SS_UNMARSHAL;
-			rxrpc_kernel_abort_call(afs_socket, call->rxcall,
+			rxrpc_kernel_abort_call(call->net->socket, call->rxcall,
 						abort_code, -EBADMSG, "KUM");
 			goto save_error;
 		}
@@ -541,13 +532,13 @@ static int afs_wait_for_call_to_complete(struct afs_call *call)
 
 	_enter("");
 
-	rtt = rxrpc_kernel_get_rtt(afs_socket, call->rxcall);
+	rtt = rxrpc_kernel_get_rtt(call->net->socket, call->rxcall);
 	rtt2 = nsecs_to_jiffies64(rtt) * 2;
 	if (rtt2 < 2)
 		rtt2 = 2;
 
 	timeout = rtt2;
-	last_life = rxrpc_kernel_check_life(afs_socket, call->rxcall);
+	last_life = rxrpc_kernel_check_life(call->net->socket, call->rxcall);
 
 	add_wait_queue(&call->waitq, &myself);
 	for (;;) {
@@ -564,7 +555,7 @@ static int afs_wait_for_call_to_complete(struct afs_call *call)
 		if (call->state == AFS_CALL_COMPLETE)
 			break;
 
-		life = rxrpc_kernel_check_life(afs_socket, call->rxcall);
+		life = rxrpc_kernel_check_life(call->net->socket, call->rxcall);
 		if (timeout == 0 &&
 		    life == last_life && signal_pending(current))
 				break;
@@ -583,7 +574,7 @@ static int afs_wait_for_call_to_complete(struct afs_call *call)
 	/* Kill off the call if it's still live. */
 	if (call->state < AFS_CALL_COMPLETE) {
 		_debug("call interrupted");
-		rxrpc_kernel_abort_call(afs_socket, call->rxcall,
+		rxrpc_kernel_abort_call(call->net->socket, call->rxcall,
 					RX_USER_ABORT, -EINTR, "KWI");
 	}
 
@@ -621,7 +612,7 @@ static void afs_wake_up_async_call(struct sock *sk, struct rxrpc_call *rxcall,
 	u = __atomic_add_unless(&call->usage, 1, 0);
 	if (u != 0) {
 		trace_afs_call(call, afs_call_trace_wake, u,
-			       atomic_read(&afs_outstanding_calls),
+			       atomic_read(&call->net->nr_outstanding_calls),
 			       __builtin_return_address(0));
 
 		if (!queue_work(afs_async_calls, &call->async_work))
@@ -685,13 +676,15 @@ static void afs_rx_attach(struct rxrpc_call *rxcall, unsigned long user_call_ID)
 /*
  * Charge the incoming call preallocation.
  */
-static void afs_charge_preallocation(struct work_struct *work)
+void afs_charge_preallocation(struct work_struct *work)
 {
-	struct afs_call *call = afs_spare_incoming_call;
+	struct afs_net *net =
+		container_of(work, struct afs_net, charge_preallocation_work);
+	struct afs_call *call = net->spare_incoming_call;
 
 	for (;;) {
 		if (!call) {
-			call = afs_alloc_call(&afs_RXCMxxxx, GFP_KERNEL);
+			call = afs_alloc_call(net, &afs_RXCMxxxx, GFP_KERNEL);
 			if (!call)
 				break;
 
@@ -700,7 +693,7 @@ static void afs_charge_preallocation(struct work_struct *work)
 			init_waitqueue_head(&call->waitq);
 		}
 
-		if (rxrpc_kernel_charge_accept(afs_socket,
+		if (rxrpc_kernel_charge_accept(net->socket,
 					       afs_wake_up_async_call,
 					       afs_rx_attach,
 					       (unsigned long)call,
@@ -708,7 +701,7 @@ static void afs_charge_preallocation(struct work_struct *work)
 			break;
 		call = NULL;
 	}
-	afs_spare_incoming_call = call;
+	net->spare_incoming_call = call;
 }
 
 /*
@@ -729,7 +722,9 @@ static void afs_rx_discard_new_call(struct rxrpc_call *rxcall,
 static void afs_rx_new_call(struct sock *sk, struct rxrpc_call *rxcall,
 			    unsigned long user_call_ID)
 {
-	queue_work(afs_wq, &afs_charge_preallocation_work);
+	struct afs_net *net = afs_sock2net(sk);
+
+	queue_work(afs_wq, &net->charge_preallocation_work);
 }
 
 /*
@@ -784,11 +779,12 @@ static void afs_notify_end_reply_tx(struct sock *sock,
  */
 void afs_send_empty_reply(struct afs_call *call)
 {
+	struct afs_net *net = call->net;
 	struct msghdr msg;
 
 	_enter("");
 
-	rxrpc_kernel_set_tx_length(afs_socket, call->rxcall, 0);
+	rxrpc_kernel_set_tx_length(net->socket, call->rxcall, 0);
 
 	msg.msg_name		= NULL;
 	msg.msg_namelen		= 0;
@@ -798,7 +794,7 @@ void afs_send_empty_reply(struct afs_call *call)
 	msg.msg_flags		= 0;
 
 	call->state = AFS_CALL_AWAIT_ACK;
-	switch (rxrpc_kernel_send_data(afs_socket, call->rxcall, &msg, 0,
+	switch (rxrpc_kernel_send_data(net->socket, call->rxcall, &msg, 0,
 				       afs_notify_end_reply_tx)) {
 	case 0:
 		_leave(" [replied]");
@@ -806,7 +802,7 @@ void afs_send_empty_reply(struct afs_call *call)
 
 	case -ENOMEM:
 		_debug("oom");
-		rxrpc_kernel_abort_call(afs_socket, call->rxcall,
+		rxrpc_kernel_abort_call(net->socket, call->rxcall,
 					RX_USER_ABORT, -ENOMEM, "KOO");
 	default:
 		_leave(" [error]");
@@ -819,13 +815,14 @@ void afs_send_empty_reply(struct afs_call *call)
  */
 void afs_send_simple_reply(struct afs_call *call, const void *buf, size_t len)
 {
+	struct afs_net *net = call->net;
 	struct msghdr msg;
 	struct kvec iov[1];
 	int n;
 
 	_enter("");
 
-	rxrpc_kernel_set_tx_length(afs_socket, call->rxcall, len);
+	rxrpc_kernel_set_tx_length(net->socket, call->rxcall, len);
 
 	iov[0].iov_base		= (void *) buf;
 	iov[0].iov_len		= len;
@@ -837,7 +834,7 @@ void afs_send_simple_reply(struct afs_call *call, const void *buf, size_t len)
 	msg.msg_flags		= 0;
 
 	call->state = AFS_CALL_AWAIT_ACK;
-	n = rxrpc_kernel_send_data(afs_socket, call->rxcall, &msg, len,
+	n = rxrpc_kernel_send_data(net->socket, call->rxcall, &msg, len,
 				   afs_notify_end_reply_tx);
 	if (n >= 0) {
 		/* Success */
@@ -847,7 +844,7 @@ void afs_send_simple_reply(struct afs_call *call, const void *buf, size_t len)
 
 	if (n == -ENOMEM) {
 		_debug("oom");
-		rxrpc_kernel_abort_call(afs_socket, call->rxcall,
+		rxrpc_kernel_abort_call(net->socket, call->rxcall,
 					RX_USER_ABORT, -ENOMEM, "KOO");
 	}
 	_leave(" [error]");
@@ -859,6 +856,7 @@ void afs_send_simple_reply(struct afs_call *call, const void *buf, size_t len)
 int afs_extract_data(struct afs_call *call, void *buf, size_t count,
 		     bool want_more)
 {
+	struct afs_net *net = call->net;
 	int ret;
 
 	_enter("{%s,%zu},,%zu,%d",
@@ -866,7 +864,7 @@ int afs_extract_data(struct afs_call *call, void *buf, size_t count,
 
 	ASSERTCMP(call->offset, <=, count);
 
-	ret = rxrpc_kernel_recv_data(afs_socket, call->rxcall,
+	ret = rxrpc_kernel_recv_data(net->socket, call->rxcall,
 				     buf, count, &call->offset,
 				     want_more, &call->abort_code,
 				     &call->service_id);

commit 5e4def20381678ba3ce0a4e117f97e378ecd81bc
Author: David Howells <dhowells@redhat.com>
Date:   Thu Nov 2 15:27:44 2017 +0000

    Pass mode to wait_on_atomic_t() action funcs and provide default actions
    
    Make wait_on_atomic_t() pass the TASK_* mode onto its action function as an
    extra argument and make it 'unsigned int throughout.
    
    Also, consolidate a bunch of identical action functions into a default
    function that can do the appropriate thing for the mode.
    
    Also, change the argument name in the bit_wait*() function declarations to
    reflect the fact that it's the mode and not the bit number.
    
    [Peter Z gives this a grudging ACK, but thinks that the whole atomic_t wait
    should be done differently, though he's not immediately sure as to how]
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    cc: Ingo Molnar <mingo@kernel.org>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index bb1e2caa1720..77f5420a1a24 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -41,12 +41,6 @@ static void afs_charge_preallocation(struct work_struct *);
 
 static DECLARE_WORK(afs_charge_preallocation_work, afs_charge_preallocation);
 
-static int afs_wait_atomic_t(atomic_t *p)
-{
-	schedule();
-	return 0;
-}
-
 /*
  * open an RxRPC socket and bind it to be a server for callback notifications
  * - the socket is left in blocking mode and non-blocking ops use MSG_DONTWAIT
@@ -121,7 +115,7 @@ void afs_close_socket(void)
 	}
 
 	_debug("outstanding %u", atomic_read(&afs_outstanding_calls));
-	wait_on_atomic_t(&afs_outstanding_calls, afs_wait_atomic_t,
+	wait_on_atomic_t(&afs_outstanding_calls, atomic_t_wait,
 			 TASK_UNINTERRUPTIBLE);
 	_debug("no outstanding calls");
 

commit bc5e3a546d553e5223851fc199e69040eb70f68b
Author: David Howells <dhowells@redhat.com>
Date:   Wed Oct 18 11:07:31 2017 +0100

    rxrpc: Use MSG_WAITALL to tell sendmsg() to temporarily ignore signals
    
    Make AF_RXRPC accept MSG_WAITALL as a flag to sendmsg() to tell it to
    ignore signals whilst loading up the message queue, provided progress is
    being made in emptying the queue at the other side.
    
    Progress is defined as the base of the transmit window having being
    advanced within 2 RTT periods.  If the period is exceeded with no progress,
    sendmsg() will return anyway, indicating how much data has been copied, if
    any.
    
    Once the supplied buffer is entirely decanted, the sendmsg() will return.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 172a4f9747ac..bb1e2caa1720 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -407,7 +407,7 @@ int afs_make_call(struct in_addr *addr, struct afs_call *call, gfp_t gfp,
 		      call->request_size);
 	msg.msg_control		= NULL;
 	msg.msg_controllen	= 0;
-	msg.msg_flags		= (call->send_pages ? MSG_MORE : 0);
+	msg.msg_flags		= MSG_WAITALL | (call->send_pages ? MSG_MORE : 0);
 
 	/* We have to change the state *before* sending the last packet as
 	 * rxrpc might give us the reply before it returns from sending the
@@ -538,15 +538,26 @@ static void afs_deliver_to_call(struct afs_call *call)
  */
 static int afs_wait_for_call_to_complete(struct afs_call *call)
 {
+	signed long rtt2, timeout;
 	int ret;
+	u64 rtt;
+	u32 life, last_life;
 
 	DECLARE_WAITQUEUE(myself, current);
 
 	_enter("");
 
+	rtt = rxrpc_kernel_get_rtt(afs_socket, call->rxcall);
+	rtt2 = nsecs_to_jiffies64(rtt) * 2;
+	if (rtt2 < 2)
+		rtt2 = 2;
+
+	timeout = rtt2;
+	last_life = rxrpc_kernel_check_life(afs_socket, call->rxcall);
+
 	add_wait_queue(&call->waitq, &myself);
 	for (;;) {
-		set_current_state(TASK_INTERRUPTIBLE);
+		set_current_state(TASK_UNINTERRUPTIBLE);
 
 		/* deliver any messages that are in the queue */
 		if (call->state < AFS_CALL_COMPLETE && call->need_attention) {
@@ -556,10 +567,20 @@ static int afs_wait_for_call_to_complete(struct afs_call *call)
 			continue;
 		}
 
-		if (call->state == AFS_CALL_COMPLETE ||
-		    signal_pending(current))
+		if (call->state == AFS_CALL_COMPLETE)
 			break;
-		schedule();
+
+		life = rxrpc_kernel_check_life(afs_socket, call->rxcall);
+		if (timeout == 0 &&
+		    life == last_life && signal_pending(current))
+				break;
+
+		if (life != last_life) {
+			timeout = rtt2;
+			last_life = life;
+		}
+
+		timeout = schedule_timeout(timeout);
 	}
 
 	remove_wait_queue(&call->waitq, &myself);

commit a68f4a27f55f1d54e35c270aff89383da4b1b656
Author: David Howells <dhowells@redhat.com>
Date:   Wed Oct 18 11:36:39 2017 +0100

    rxrpc: Support service upgrade from a kernel service
    
    Provide support for a kernel service to make use of the service upgrade
    facility.  This involves:
    
     (1) Pass an upgrade request flag to rxrpc_kernel_begin_call().
    
     (2) Make rxrpc_kernel_recv_data() return the call's current service ID so
         that the caller can detect service upgrade and see what the service
         was upgraded to.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 0bf191f0dbaf..172a4f9747ac 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -387,7 +387,8 @@ int afs_make_call(struct in_addr *addr, struct afs_call *call, gfp_t gfp,
 					 tx_total_len, gfp,
 					 (async ?
 					  afs_wake_up_async_call :
-					  afs_wake_up_call_waiter));
+					  afs_wake_up_call_waiter),
+					 call->upgrade);
 	call->key = NULL;
 	if (IS_ERR(rxcall)) {
 		ret = PTR_ERR(rxcall);
@@ -443,7 +444,7 @@ int afs_make_call(struct in_addr *addr, struct afs_call *call, gfp_t gfp,
 		abort_code = 0;
 		offset = 0;
 		rxrpc_kernel_recv_data(afs_socket, rxcall, NULL, 0, &offset,
-				       false, &abort_code);
+				       false, &abort_code, &call->service_id);
 		ret = call->type->abort_to_error(abort_code);
 	}
 error_kill_call:
@@ -471,7 +472,8 @@ static void afs_deliver_to_call(struct afs_call *call)
 			size_t offset = 0;
 			ret = rxrpc_kernel_recv_data(afs_socket, call->rxcall,
 						     NULL, 0, &offset, false,
-						     &call->abort_code);
+						     &call->abort_code,
+						     &call->service_id);
 			trace_afs_recv_data(call, 0, offset, false, ret);
 
 			if (ret == -EINPROGRESS || ret == -EAGAIN)
@@ -851,7 +853,8 @@ int afs_extract_data(struct afs_call *call, void *buf, size_t count,
 
 	ret = rxrpc_kernel_recv_data(afs_socket, call->rxcall,
 				     buf, count, &call->offset,
-				     want_more, &call->abort_code);
+				     want_more, &call->abort_code,
+				     &call->service_id);
 	trace_afs_recv_data(call, count, call->offset, want_more, ret);
 	if (ret == 0 || ret == -EAGAIN)
 		return ret;

commit e833251ad813168253fef9915aaf6a8c883337b0
Author: David Howells <dhowells@redhat.com>
Date:   Tue Aug 29 10:18:56 2017 +0100

    rxrpc: Add notification of end-of-Tx phase
    
    Add a callback to rxrpc_kernel_send_data() so that a kernel service can get
    a notification that the AF_RXRPC call has transitioned out the Tx phase and
    is now waiting for a reply or a final ACK.
    
    This is called from AF_RXRPC with the call state lock held so the
    notification is guaranteed to come before any reply is passed back.
    
    Further, modify the AFS filesystem to make use of this so that we don't have
    to change the afs_call state before sending the last bit of data.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 10743043d431..0bf191f0dbaf 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -291,6 +291,19 @@ static void afs_load_bvec(struct afs_call *call, struct msghdr *msg,
 	iov_iter_bvec(&msg->msg_iter, WRITE | ITER_BVEC, bv, nr, bytes);
 }
 
+/*
+ * Advance the AFS call state when the RxRPC call ends the transmit phase.
+ */
+static void afs_notify_end_request_tx(struct sock *sock,
+				      struct rxrpc_call *rxcall,
+				      unsigned long call_user_ID)
+{
+	struct afs_call *call = (struct afs_call *)call_user_ID;
+
+	if (call->state == AFS_CALL_REQUESTING)
+		call->state = AFS_CALL_AWAIT_REPLY;
+}
+
 /*
  * attach the data from a bunch of pages on an inode to a call
  */
@@ -310,14 +323,8 @@ static int afs_send_pages(struct afs_call *call, struct msghdr *msg)
 		bytes = msg->msg_iter.count;
 		nr = msg->msg_iter.nr_segs;
 
-		/* Have to change the state *before* sending the last
-		 * packet as RxRPC might give us the reply before it
-		 * returns from sending the request.
-		 */
-		if (first + nr - 1 >= last)
-			call->state = AFS_CALL_AWAIT_REPLY;
-		ret = rxrpc_kernel_send_data(afs_socket, call->rxcall,
-					     msg, bytes);
+		ret = rxrpc_kernel_send_data(afs_socket, call->rxcall, msg,
+					     bytes, afs_notify_end_request_tx);
 		for (loop = 0; loop < nr; loop++)
 			put_page(bv[loop].bv_page);
 		if (ret < 0)
@@ -409,7 +416,8 @@ int afs_make_call(struct in_addr *addr, struct afs_call *call, gfp_t gfp,
 	if (!call->send_pages)
 		call->state = AFS_CALL_AWAIT_REPLY;
 	ret = rxrpc_kernel_send_data(afs_socket, rxcall,
-				     &msg, call->request_size);
+				     &msg, call->request_size,
+				     afs_notify_end_request_tx);
 	if (ret < 0)
 		goto error_do_abort;
 
@@ -740,6 +748,20 @@ static int afs_deliver_cm_op_id(struct afs_call *call)
 	return call->type->deliver(call);
 }
 
+/*
+ * Advance the AFS call state when an RxRPC service call ends the transmit
+ * phase.
+ */
+static void afs_notify_end_reply_tx(struct sock *sock,
+				    struct rxrpc_call *rxcall,
+				    unsigned long call_user_ID)
+{
+	struct afs_call *call = (struct afs_call *)call_user_ID;
+
+	if (call->state == AFS_CALL_REPLYING)
+		call->state = AFS_CALL_AWAIT_ACK;
+}
+
 /*
  * send an empty reply
  */
@@ -759,7 +781,8 @@ void afs_send_empty_reply(struct afs_call *call)
 	msg.msg_flags		= 0;
 
 	call->state = AFS_CALL_AWAIT_ACK;
-	switch (rxrpc_kernel_send_data(afs_socket, call->rxcall, &msg, 0)) {
+	switch (rxrpc_kernel_send_data(afs_socket, call->rxcall, &msg, 0,
+				       afs_notify_end_reply_tx)) {
 	case 0:
 		_leave(" [replied]");
 		return;
@@ -797,7 +820,8 @@ void afs_send_simple_reply(struct afs_call *call, const void *buf, size_t len)
 	msg.msg_flags		= 0;
 
 	call->state = AFS_CALL_AWAIT_ACK;
-	n = rxrpc_kernel_send_data(afs_socket, call->rxcall, &msg, len);
+	n = rxrpc_kernel_send_data(afs_socket, call->rxcall, &msg, len,
+				   afs_notify_end_reply_tx);
 	if (n >= 0) {
 		/* Success */
 		_leave(" [replied]");

commit ddc6c70f07bb1f6dd39a2c6c430f7b4fa95199c8
Author: David Howells <dhowells@redhat.com>
Date:   Fri Jul 21 10:07:10 2017 +0100

    rxrpc: Move the packet.h include file into net/rxrpc/
    
    Move the protocol description header file into net/rxrpc/ and rename it to
    protocol.h.  It's no longer necessary to expose it as packets are no longer
    exposed to kernel services (such as AFS) that use the facility.
    
    The abort codes are transferred to the UAPI header instead as we pass these
    back to userspace and also to kernel services.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 02781e78ffb6..10743043d431 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -14,7 +14,6 @@
 
 #include <net/sock.h>
 #include <net/af_rxrpc.h>
-#include <rxrpc/packet.h>
 #include "internal.h"
 #include "afs_cm.h"
 

commit e754eba685aac2a9b5538176fa2d254ad25f464d
Author: David Howells <dhowells@redhat.com>
Date:   Wed Jun 7 12:40:03 2017 +0100

    rxrpc: Provide a cmsg to specify the amount of Tx data for a call
    
    Provide a control message that can be specified on the first sendmsg() of a
    client call or the first sendmsg() of a service response to indicate the
    total length of the data to be transmitted for that call.
    
    Currently, because the length of the payload of an encrypted DATA packet is
    encrypted in front of the data, the packet cannot be encrypted until we
    know how much data it will hold.
    
    By specifying the length at the beginning of the transmit phase, each DATA
    packet length can be set before we start loading data from userspace (where
    several sendmsg() calls may contribute to a particular packet).
    
    An error will be returned if too little or too much data is presented in
    the Tx phase.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index d5990eb160bd..02781e78ffb6 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -341,6 +341,7 @@ int afs_make_call(struct in_addr *addr, struct afs_call *call, gfp_t gfp,
 	struct msghdr msg;
 	struct kvec iov[1];
 	size_t offset;
+	s64 tx_total_len;
 	u32 abort_code;
 	int ret;
 
@@ -364,9 +365,20 @@ int afs_make_call(struct in_addr *addr, struct afs_call *call, gfp_t gfp,
 	srx.transport.sin.sin_port = call->port;
 	memcpy(&srx.transport.sin.sin_addr, addr, 4);
 
+	/* Work out the length we're going to transmit.  This is awkward for
+	 * calls such as FS.StoreData where there's an extra injection of data
+	 * after the initial fixed part.
+	 */
+	tx_total_len = call->request_size;
+	if (call->send_pages) {
+		tx_total_len += call->last_to - call->first_offset;
+		tx_total_len += (call->last - call->first) * PAGE_SIZE;
+	}
+
 	/* create a call */
 	rxcall = rxrpc_kernel_begin_call(afs_socket, &srx, call->key,
-					 (unsigned long) call, gfp,
+					 (unsigned long)call,
+					 tx_total_len, gfp,
 					 (async ?
 					  afs_wake_up_async_call :
 					  afs_wake_up_call_waiter));
@@ -738,6 +750,8 @@ void afs_send_empty_reply(struct afs_call *call)
 
 	_enter("");
 
+	rxrpc_kernel_set_tx_length(afs_socket, call->rxcall, 0);
+
 	msg.msg_name		= NULL;
 	msg.msg_namelen		= 0;
 	iov_iter_kvec(&msg.msg_iter, WRITE | ITER_KVEC, NULL, 0, 0);
@@ -772,6 +786,8 @@ void afs_send_simple_reply(struct afs_call *call, const void *buf, size_t len)
 
 	_enter("");
 
+	rxrpc_kernel_set_tx_length(afs_socket, call->rxcall, len);
+
 	iov[0].iov_base		= (void *) buf;
 	iov[0].iov_len		= len;
 	msg.msg_name		= NULL;

commit 3a92789af0d625caff1e0bf5701aec8edf0d057d
Author: David Howells <dhowells@redhat.com>
Date:   Thu Apr 6 10:11:56 2017 +0100

    rxrpc: Use negative error codes in rxrpc_call struct
    
    Use negative error codes in struct rxrpc_call::error because that's what
    the kernel normally deals with and to make the code consistent.  We only
    turn them positive when transcribing into a cmsg for userspace recvmsg.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 8f76b13d5549..d5990eb160bd 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -419,7 +419,7 @@ int afs_make_call(struct in_addr *addr, struct afs_call *call, gfp_t gfp,
 	call->state = AFS_CALL_COMPLETE;
 	if (ret != -ECONNABORTED) {
 		rxrpc_kernel_abort_call(afs_socket, rxcall, RX_USER_ABORT,
-					-ret, "KSD");
+					ret, "KSD");
 	} else {
 		abort_code = 0;
 		offset = 0;
@@ -478,12 +478,12 @@ static void afs_deliver_to_call(struct afs_call *call)
 		case -ENOTCONN:
 			abort_code = RX_CALL_DEAD;
 			rxrpc_kernel_abort_call(afs_socket, call->rxcall,
-						abort_code, -ret, "KNC");
+						abort_code, ret, "KNC");
 			goto save_error;
 		case -ENOTSUPP:
 			abort_code = RXGEN_OPCODE;
 			rxrpc_kernel_abort_call(afs_socket, call->rxcall,
-						abort_code, -ret, "KIV");
+						abort_code, ret, "KIV");
 			goto save_error;
 		case -ENODATA:
 		case -EBADMSG:
@@ -493,7 +493,7 @@ static void afs_deliver_to_call(struct afs_call *call)
 			if (call->state != AFS_CALL_AWAIT_REPLY)
 				abort_code = RXGEN_SS_UNMARSHAL;
 			rxrpc_kernel_abort_call(afs_socket, call->rxcall,
-						abort_code, EBADMSG, "KUM");
+						abort_code, -EBADMSG, "KUM");
 			goto save_error;
 		}
 	}
@@ -754,7 +754,7 @@ void afs_send_empty_reply(struct afs_call *call)
 	case -ENOMEM:
 		_debug("oom");
 		rxrpc_kernel_abort_call(afs_socket, call->rxcall,
-					RX_USER_ABORT, ENOMEM, "KOO");
+					RX_USER_ABORT, -ENOMEM, "KOO");
 	default:
 		_leave(" [error]");
 		return;
@@ -792,7 +792,7 @@ void afs_send_simple_reply(struct afs_call *call, const void *buf, size_t len)
 	if (n == -ENOMEM) {
 		_debug("oom");
 		rxrpc_kernel_abort_call(afs_socket, call->rxcall,
-					RX_USER_ABORT, ENOMEM, "KOO");
+					RX_USER_ABORT, -ENOMEM, "KOO");
 	}
 	_leave(" [error]");
 }

commit 954cd6dc02a65065aecb7150962c0870c5b0e322
Author: David Howells <dhowells@redhat.com>
Date:   Thu Mar 16 16:27:49 2017 +0000

    afs: Fix abort on signal while waiting for call completion
    
    Fix the way in which a call that's in progress and being waited for is
    aborted in the case that EINTR is detected.  We should be sending
    RX_USER_ABORT rather than RX_CALL_DEAD as the abort code.
    
    Note that since the only two ways out of the loop are if the call completes
    or if a signal happens, the kill-the-call clause after the loop has
    finished can only happen in the case of EINTR.  This means that we only
    have one abort case to deal with, not two, and the "KWC" case can never
    happen and so can be deleted.
    
    Note further that simply aborting the call isn't necessarily the best thing
    here since at this point: the request has been entirely sent and it's
    likely the server will do the operation anyway - whether we abort it or
    not.  In future, we should punt the handling of the remainder of the call
    off to a background thread.
    
    Reported-by: Marc Dionne <marc.c.dionne@auristor.com>
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index b12da6aa5412..8f76b13d5549 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -517,7 +517,6 @@ static void afs_deliver_to_call(struct afs_call *call)
  */
 static int afs_wait_for_call_to_complete(struct afs_call *call)
 {
-	const char *abort_why;
 	int ret;
 
 	DECLARE_WAITQUEUE(myself, current);
@@ -536,13 +535,8 @@ static int afs_wait_for_call_to_complete(struct afs_call *call)
 			continue;
 		}
 
-		abort_why = "KWC";
-		ret = call->error;
-		if (call->state == AFS_CALL_COMPLETE)
-			break;
-		abort_why = "KWI";
-		ret = -EINTR;
-		if (signal_pending(current))
+		if (call->state == AFS_CALL_COMPLETE ||
+		    signal_pending(current))
 			break;
 		schedule();
 	}
@@ -550,15 +544,14 @@ static int afs_wait_for_call_to_complete(struct afs_call *call)
 	remove_wait_queue(&call->waitq, &myself);
 	__set_current_state(TASK_RUNNING);
 
-	/* kill the call */
+	/* Kill off the call if it's still live. */
 	if (call->state < AFS_CALL_COMPLETE) {
-		_debug("call incomplete");
+		_debug("call interrupted");
 		rxrpc_kernel_abort_call(afs_socket, call->rxcall,
-					RX_CALL_DEAD, -ret, abort_why);
-	} else if (call->error < 0) {
-		ret = call->error;
+					RX_USER_ABORT, -EINTR, "KWI");
 	}
 
+	ret = call->error;
 	_debug("call complete");
 	afs_put_call(call);
 	_leave(" = %d", ret);

commit 445783d0ec173a52bef2e9b129de7d716a19b9fa
Author: David Howells <dhowells@redhat.com>
Date:   Thu Mar 16 16:27:48 2017 +0000

    afs: Fix an off-by-one error in afs_send_pages()
    
    afs_send_pages() should only put the call into the AFS_CALL_AWAIT_REPLY
    state if it has sent all the pages - but the check it makes is incorrect
    and sometimes it will finish the loop early.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 22d26b369070..b12da6aa5412 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -315,7 +315,7 @@ static int afs_send_pages(struct afs_call *call, struct msghdr *msg)
 		 * packet as RxRPC might give us the reply before it
 		 * returns from sending the request.
 		 */
-		if (first + nr >= last)
+		if (first + nr - 1 >= last)
 			call->state = AFS_CALL_AWAIT_REPLY;
 		ret = rxrpc_kernel_send_data(afs_socket, call->rxcall,
 					     msg, bytes);

commit 70af0e3bd65142f9e674961c975451638a7ce1d5
Author: David Howells <dhowells@redhat.com>
Date:   Thu Mar 16 16:27:47 2017 +0000

    afs: Better abort and net error handling
    
    If we receive a network error, a remote abort or a protocol error whilst
    we're still transmitting data, make sure we return an appropriate error to
    the caller rather than ESHUTDOWN or ECONNABORTED.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index bf7761fe6ef5..22d26b369070 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -340,6 +340,8 @@ int afs_make_call(struct in_addr *addr, struct afs_call *call, gfp_t gfp,
 	struct rxrpc_call *rxcall;
 	struct msghdr msg;
 	struct kvec iov[1];
+	size_t offset;
+	u32 abort_code;
 	int ret;
 
 	_enter("%x,{%d},", addr->s_addr, ntohs(call->port));
@@ -388,9 +390,11 @@ int afs_make_call(struct in_addr *addr, struct afs_call *call, gfp_t gfp,
 	msg.msg_controllen	= 0;
 	msg.msg_flags		= (call->send_pages ? MSG_MORE : 0);
 
-	/* have to change the state *before* sending the last packet as RxRPC
-	 * might give us the reply before it returns from sending the
-	 * request */
+	/* We have to change the state *before* sending the last packet as
+	 * rxrpc might give us the reply before it returns from sending the
+	 * request.  Further, if the send fails, we may already have been given
+	 * a notification and may have collected it.
+	 */
 	if (!call->send_pages)
 		call->state = AFS_CALL_AWAIT_REPLY;
 	ret = rxrpc_kernel_send_data(afs_socket, rxcall,
@@ -412,7 +416,17 @@ int afs_make_call(struct in_addr *addr, struct afs_call *call, gfp_t gfp,
 	return afs_wait_for_call_to_complete(call);
 
 error_do_abort:
-	rxrpc_kernel_abort_call(afs_socket, rxcall, RX_USER_ABORT, -ret, "KSD");
+	call->state = AFS_CALL_COMPLETE;
+	if (ret != -ECONNABORTED) {
+		rxrpc_kernel_abort_call(afs_socket, rxcall, RX_USER_ABORT,
+					-ret, "KSD");
+	} else {
+		abort_code = 0;
+		offset = 0;
+		rxrpc_kernel_recv_data(afs_socket, rxcall, NULL, 0, &offset,
+				       false, &abort_code);
+		ret = call->type->abort_to_error(abort_code);
+	}
 error_kill_call:
 	afs_put_call(call);
 	_leave(" = %d", ret);
@@ -459,16 +473,18 @@ static void afs_deliver_to_call(struct afs_call *call)
 		case -EINPROGRESS:
 		case -EAGAIN:
 			goto out;
+		case -ECONNABORTED:
+			goto call_complete;
 		case -ENOTCONN:
 			abort_code = RX_CALL_DEAD;
 			rxrpc_kernel_abort_call(afs_socket, call->rxcall,
 						abort_code, -ret, "KNC");
-			goto do_abort;
+			goto save_error;
 		case -ENOTSUPP:
 			abort_code = RXGEN_OPCODE;
 			rxrpc_kernel_abort_call(afs_socket, call->rxcall,
 						abort_code, -ret, "KIV");
-			goto do_abort;
+			goto save_error;
 		case -ENODATA:
 		case -EBADMSG:
 		case -EMSGSIZE:
@@ -478,7 +494,7 @@ static void afs_deliver_to_call(struct afs_call *call)
 				abort_code = RXGEN_SS_UNMARSHAL;
 			rxrpc_kernel_abort_call(afs_socket, call->rxcall,
 						abort_code, EBADMSG, "KUM");
-			goto do_abort;
+			goto save_error;
 		}
 	}
 
@@ -489,8 +505,9 @@ static void afs_deliver_to_call(struct afs_call *call)
 	_leave("");
 	return;
 
-do_abort:
+save_error:
 	call->error = ret;
+call_complete:
 	call->state = AFS_CALL_COMPLETE;
 	goto done;
 }
@@ -538,6 +555,8 @@ static int afs_wait_for_call_to_complete(struct afs_call *call)
 		_debug("call incomplete");
 		rxrpc_kernel_abort_call(afs_socket, call->rxcall,
 					RX_CALL_DEAD, -ret, abort_why);
+	} else if (call->error < 0) {
+		ret = call->error;
 	}
 
 	_debug("call complete");

commit 1157f153f37a8586765034470e4f00a4a6c4ce6f
Author: David Howells <dhowells@redhat.com>
Date:   Thu Mar 16 16:27:47 2017 +0000

    afs: Invalid op ID should abort with RXGEN_OPCODE
    
    When we are given an invalid operation ID, we should abort that with
    RXGEN_OPCODE rather than RX_INVALID_OPERATION.
    
    Also map RXGEN_OPCODE to -ENOTSUPP.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index bf45307ff201..bf7761fe6ef5 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -465,7 +465,7 @@ static void afs_deliver_to_call(struct afs_call *call)
 						abort_code, -ret, "KNC");
 			goto do_abort;
 		case -ENOTSUPP:
-			abort_code = RX_INVALID_OPERATION;
+			abort_code = RXGEN_OPCODE;
 			rxrpc_kernel_abort_call(afs_socket, call->rxcall,
 						abort_code, -ret, "KIV");
 			goto do_abort;

commit 2f5705a5c805e7f761f2228820656bb9363a3d8c
Author: David Howells <dhowells@redhat.com>
Date:   Thu Mar 16 16:27:46 2017 +0000

    afs: Use a bvec rather than a kvec in afs_send_pages()
    
    Use a bvec rather than a kvec in afs_send_pages() as we don't then have to
    call kmap() in advance.  This allows us to pass the array of contiguous
    pages that we extracted through to rxrpc in one go rather than passing a
    single page at a time.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 419ef05dcb5e..bf45307ff201 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -259,67 +259,74 @@ void afs_flat_call_destructor(struct afs_call *call)
 	call->buffer = NULL;
 }
 
+#define AFS_BVEC_MAX 8
+
+/*
+ * Load the given bvec with the next few pages.
+ */
+static void afs_load_bvec(struct afs_call *call, struct msghdr *msg,
+			  struct bio_vec *bv, pgoff_t first, pgoff_t last,
+			  unsigned offset)
+{
+	struct page *pages[AFS_BVEC_MAX];
+	unsigned int nr, n, i, to, bytes = 0;
+
+	nr = min_t(pgoff_t, last - first + 1, AFS_BVEC_MAX);
+	n = find_get_pages_contig(call->mapping, first, nr, pages);
+	ASSERTCMP(n, ==, nr);
+
+	msg->msg_flags |= MSG_MORE;
+	for (i = 0; i < nr; i++) {
+		to = PAGE_SIZE;
+		if (first + i >= last) {
+			to = call->last_to;
+			msg->msg_flags &= ~MSG_MORE;
+		}
+		bv[i].bv_page = pages[i];
+		bv[i].bv_len = to - offset;
+		bv[i].bv_offset = offset;
+		bytes += to - offset;
+		offset = 0;
+	}
+
+	iov_iter_bvec(&msg->msg_iter, WRITE | ITER_BVEC, bv, nr, bytes);
+}
+
 /*
  * attach the data from a bunch of pages on an inode to a call
  */
 static int afs_send_pages(struct afs_call *call, struct msghdr *msg)
 {
-	struct page *pages[8];
-	unsigned count, n, loop, offset, to;
+	struct bio_vec bv[AFS_BVEC_MAX];
+	unsigned int bytes, nr, loop, offset;
 	pgoff_t first = call->first, last = call->last;
 	int ret;
 
-	_enter("");
-
 	offset = call->first_offset;
 	call->first_offset = 0;
 
 	do {
-		_debug("attach %lx-%lx", first, last);
-
-		count = last - first + 1;
-		if (count > ARRAY_SIZE(pages))
-			count = ARRAY_SIZE(pages);
-		n = find_get_pages_contig(call->mapping, first, count, pages);
-		ASSERTCMP(n, ==, count);
-
-		loop = 0;
-		do {
-			struct bio_vec bvec = {.bv_page = pages[loop],
-					       .bv_offset = offset};
-			msg->msg_flags = 0;
-			to = PAGE_SIZE;
-			if (first + loop >= last)
-				to = call->last_to;
-			else
-				msg->msg_flags = MSG_MORE;
-			bvec.bv_len = to - offset;
-			offset = 0;
-
-			_debug("- range %u-%u%s",
-			       offset, to, msg->msg_flags ? " [more]" : "");
-			iov_iter_bvec(&msg->msg_iter, WRITE | ITER_BVEC,
-				      &bvec, 1, to - offset);
-
-			/* have to change the state *before* sending the last
-			 * packet as RxRPC might give us the reply before it
-			 * returns from sending the request */
-			if (first + loop >= last)
-				call->state = AFS_CALL_AWAIT_REPLY;
-			ret = rxrpc_kernel_send_data(afs_socket, call->rxcall,
-						     msg, to - offset);
-			if (ret < 0)
-				break;
-		} while (++loop < count);
-		first += count;
-
-		for (loop = 0; loop < count; loop++)
-			put_page(pages[loop]);
+		afs_load_bvec(call, msg, bv, first, last, offset);
+		offset = 0;
+		bytes = msg->msg_iter.count;
+		nr = msg->msg_iter.nr_segs;
+
+		/* Have to change the state *before* sending the last
+		 * packet as RxRPC might give us the reply before it
+		 * returns from sending the request.
+		 */
+		if (first + nr >= last)
+			call->state = AFS_CALL_AWAIT_REPLY;
+		ret = rxrpc_kernel_send_data(afs_socket, call->rxcall,
+					     msg, bytes);
+		for (loop = 0; loop < nr; loop++)
+			put_page(bv[loop].bv_page);
 		if (ret < 0)
 			break;
+
+		first += nr;
 	} while (first <= last);
 
-	_leave(" = %d", ret);
 	return ret;
 }
 

commit 1827adb11ad26b2290dc9fe2aaf54976b2439865
Merge: 78769912f680 5eca1c10cbaa
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Mar 3 10:16:38 2017 -0800

    Merge branch 'WIP.sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull sched.h split-up from Ingo Molnar:
     "The point of these changes is to significantly reduce the
      <linux/sched.h> header footprint, to speed up the kernel build and to
      have a cleaner header structure.
    
      After these changes the new <linux/sched.h>'s typical preprocessed
      size goes down from a previous ~0.68 MB (~22K lines) to ~0.45 MB (~15K
      lines), which is around 40% faster to build on typical configs.
    
      Not much changed from the last version (-v2) posted three weeks ago: I
      eliminated quirks, backmerged fixes plus I rebased it to an upstream
      SHA1 from yesterday that includes most changes queued up in -next plus
      all sched.h changes that were pending from Andrew.
    
      I've re-tested the series both on x86 and on cross-arch defconfigs,
      and did a bisectability test at a number of random points.
    
      I tried to test as many build configurations as possible, but some
      build breakage is probably still left - but it should be mostly
      limited to architectures that have no cross-compiler binaries
      available on kernel.org, and non-default configurations"
    
    * 'WIP.sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (146 commits)
      sched/headers: Clean up <linux/sched.h>
      sched/headers: Remove #ifdefs from <linux/sched.h>
      sched/headers: Remove the <linux/topology.h> include from <linux/sched.h>
      sched/headers, hrtimer: Remove the <linux/wait.h> include from <linux/hrtimer.h>
      sched/headers, x86/apic: Remove the <linux/pm.h> header inclusion from <asm/apic.h>
      sched/headers, timers: Remove the <linux/sysctl.h> include from <linux/timer.h>
      sched/headers: Remove <linux/magic.h> from <linux/sched/task_stack.h>
      sched/headers: Remove <linux/sched.h> from <linux/sched/init.h>
      sched/core: Remove unused prefetch_stack()
      sched/headers: Remove <linux/rculist.h> from <linux/sched.h>
      sched/headers: Remove the 'init_pid_ns' prototype from <linux/sched.h>
      sched/headers: Remove <linux/signal.h> from <linux/sched.h>
      sched/headers: Remove <linux/rwsem.h> from <linux/sched.h>
      sched/headers: Remove the runqueue_is_locked() prototype
      sched/headers: Remove <linux/sched.h> from <linux/sched/hotplug.h>
      sched/headers: Remove <linux/sched.h> from <linux/sched/debug.h>
      sched/headers: Remove <linux/sched.h> from <linux/sched/nohz.h>
      sched/headers: Remove <linux/sched.h> from <linux/sched/stat.h>
      sched/headers: Remove the <linux/gfp.h> include from <linux/sched.h>
      sched/headers: Remove <linux/rtmutex.h> from <linux/sched.h>
      ...

commit 69fd110eb650ea7baa82158f3b89a7d86da1d056
Merge: 821fd6f6cb65 4038a2a37e35
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 2 15:16:38 2017 -0800

    Merge branch 'work.sendmsg' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs
    
    Pull vfs sendmsg updates from Al Viro:
     "More sendmsg work.
    
      This is a fairly separate isolated stuff (there's a continuation
      around lustre, but that one was too late to soak in -next), thus the
      separate pull request"
    
    * 'work.sendmsg' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs:
      ncpfs: switch to sock_sendmsg()
      ncpfs: don't mess with manually advancing iovec on send
      ncpfs: sendmsg does *not* bugger iovec these days
      ceph_tcp_sendpage(): use ITER_BVEC sendmsg
      afs_send_pages(): use ITER_BVEC
      rds: remove dead code
      ceph: switch to sock_recvmsg()
      usbip_recv(): switch to sock_recvmsg()
      iscsi_target: deal with short writes on the tx side
      [nbd] pass iov_iter to nbd_xmit()
      [nbd] switch sock_xmit() to sock_{send,recv}msg()
      [drbd] use sock_sendmsg()

commit 174cd4b1e5fbd0d74c68cf3a74f5bd4923485512
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Feb 2 19:15:33 2017 +0100

    sched/headers: Prepare to move signal wakeup & sigpending methods from <linux/sched.h> into <linux/sched/signal.h>
    
    Fix up affected files that include this signal functionality via sched.h.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 95f42872b787..4f6b00efc27c 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -10,6 +10,8 @@
  */
 
 #include <linux/slab.h>
+#include <linux/sched/signal.h>
+
 #include <net/sock.h>
 #include <net/af_rxrpc.h>
 #include <rxrpc/packet.h>

commit 341f741f04beceebcb30daa12ae2e5e52e64e532
Author: David Howells <dhowells@redhat.com>
Date:   Thu Jan 5 10:38:36 2017 +0000

    afs: Refcount the afs_call struct
    
    A static checker warning occurs in the AFS filesystem:
    
            fs/afs/cmservice.c:155 SRXAFSCB_CallBack()
            error: dereferencing freed memory 'call'
    
    due to the reply being sent before we access the server it points to.  The
    act of sending the reply causes the call to be freed if an error occurs
    (but not if it doesn't).
    
    On top of this, the lifetime handling of afs_call structs is fragile
    because they get passed around through workqueues without any sort of
    refcounting.
    
    Deal with the issues by:
    
     (1) Fix the maybe/maybe not nature of the reply sending functions with
         regards to whether they release the call struct.
    
     (2) Refcount the afs_call struct and sort out places that need to get/put
         references.
    
     (3) Pass a ref through the work queue and release (or pass on) that ref in
         the work function.  Care has to be taken because a work queue may
         already own a ref to the call.
    
     (4) Do the cleaning up in the put function only.
    
     (5) Simplify module cleanup by always incrementing afs_outstanding_calls
         whenever a call is allocated.
    
     (6) Set the backlog to 0 with kernel_listen() at the beginning of the
         process of closing the socket to prevent new incoming calls from
         occurring and to remove the contribution of preallocated calls from
         afs_outstanding_calls before we wait on it.
    
    A tracepoint is also added to monitor the afs_call refcount and lifetime.
    
    Reported-by: Dan Carpenter <dan.carpenter@oracle.com>
    Signed-off-by: David Howells <dhowells@redhat.com>
    Fixes: 08e0e7c82eea: "[AF_RXRPC]: Make the in-kernel AFS filesystem use AF_RXRPC."

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index ec1e41f929d1..95f42872b787 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -19,9 +19,8 @@
 struct socket *afs_socket; /* my RxRPC socket */
 static struct workqueue_struct *afs_async_calls;
 static struct afs_call *afs_spare_incoming_call;
-static atomic_t afs_outstanding_calls;
+atomic_t afs_outstanding_calls;
 
-static void afs_free_call(struct afs_call *);
 static void afs_wake_up_call_waiter(struct sock *, struct rxrpc_call *, unsigned long);
 static int afs_wait_for_call_to_complete(struct afs_call *);
 static void afs_wake_up_async_call(struct sock *, struct rxrpc_call *, unsigned long);
@@ -112,9 +111,11 @@ void afs_close_socket(void)
 {
 	_enter("");
 
+	kernel_listen(afs_socket, 0);
+	flush_workqueue(afs_async_calls);
+
 	if (afs_spare_incoming_call) {
-		atomic_inc(&afs_outstanding_calls);
-		afs_free_call(afs_spare_incoming_call);
+		afs_put_call(afs_spare_incoming_call);
 		afs_spare_incoming_call = NULL;
 	}
 
@@ -123,7 +124,6 @@ void afs_close_socket(void)
 			 TASK_UNINTERRUPTIBLE);
 	_debug("no outstanding calls");
 
-	flush_workqueue(afs_async_calls);
 	kernel_sock_shutdown(afs_socket, SHUT_RDWR);
 	flush_workqueue(afs_async_calls);
 	sock_release(afs_socket);
@@ -134,44 +134,79 @@ void afs_close_socket(void)
 }
 
 /*
- * free a call
+ * Allocate a call.
  */
-static void afs_free_call(struct afs_call *call)
+static struct afs_call *afs_alloc_call(const struct afs_call_type *type,
+				       gfp_t gfp)
 {
-	_debug("DONE %p{%s} [%d]",
-	       call, call->type->name, atomic_read(&afs_outstanding_calls));
+	struct afs_call *call;
+	int o;
 
-	ASSERTCMP(call->rxcall, ==, NULL);
-	ASSERT(!work_pending(&call->async_work));
-	ASSERT(call->type->name != NULL);
+	call = kzalloc(sizeof(*call), gfp);
+	if (!call)
+		return NULL;
 
-	kfree(call->request);
-	kfree(call);
+	call->type = type;
+	atomic_set(&call->usage, 1);
+	INIT_WORK(&call->async_work, afs_process_async_call);
+	init_waitqueue_head(&call->waitq);
 
-	if (atomic_dec_and_test(&afs_outstanding_calls))
-		wake_up_atomic_t(&afs_outstanding_calls);
+	o = atomic_inc_return(&afs_outstanding_calls);
+	trace_afs_call(call, afs_call_trace_alloc, 1, o,
+		       __builtin_return_address(0));
+	return call;
 }
 
 /*
- * End a call but do not free it
+ * Dispose of a reference on a call.
  */
-static void afs_end_call_nofree(struct afs_call *call)
+void afs_put_call(struct afs_call *call)
 {
-	if (call->rxcall) {
-		rxrpc_kernel_end_call(afs_socket, call->rxcall);
-		call->rxcall = NULL;
+	int n = atomic_dec_return(&call->usage);
+	int o = atomic_read(&afs_outstanding_calls);
+
+	trace_afs_call(call, afs_call_trace_put, n + 1, o,
+		       __builtin_return_address(0));
+
+	ASSERTCMP(n, >=, 0);
+	if (n == 0) {
+		ASSERT(!work_pending(&call->async_work));
+		ASSERT(call->type->name != NULL);
+
+		if (call->rxcall) {
+			rxrpc_kernel_end_call(afs_socket, call->rxcall);
+			call->rxcall = NULL;
+		}
+		if (call->type->destructor)
+			call->type->destructor(call);
+
+		kfree(call->request);
+		kfree(call);
+
+		o = atomic_dec_return(&afs_outstanding_calls);
+		trace_afs_call(call, afs_call_trace_free, 0, o,
+			       __builtin_return_address(0));
+		if (o == 0)
+			wake_up_atomic_t(&afs_outstanding_calls);
 	}
-	if (call->type->destructor)
-		call->type->destructor(call);
 }
 
 /*
- * End a call and free it
+ * Queue the call for actual work.  Returns 0 unconditionally for convenience.
  */
-static void afs_end_call(struct afs_call *call)
+int afs_queue_call_work(struct afs_call *call)
 {
-	afs_end_call_nofree(call);
-	afs_free_call(call);
+	int u = atomic_inc_return(&call->usage);
+
+	trace_afs_call(call, afs_call_trace_work, u,
+		       atomic_read(&afs_outstanding_calls),
+		       __builtin_return_address(0));
+
+	INIT_WORK(&call->work, call->type->work);
+
+	if (!queue_work(afs_wq, &call->work))
+		afs_put_call(call);
+	return 0;
 }
 
 /*
@@ -182,25 +217,19 @@ struct afs_call *afs_alloc_flat_call(const struct afs_call_type *type,
 {
 	struct afs_call *call;
 
-	call = kzalloc(sizeof(*call), GFP_NOFS);
+	call = afs_alloc_call(type, GFP_NOFS);
 	if (!call)
 		goto nomem_call;
 
-	_debug("CALL %p{%s} [%d]",
-	       call, type->name, atomic_read(&afs_outstanding_calls));
-	atomic_inc(&afs_outstanding_calls);
-
-	call->type = type;
-	call->request_size = request_size;
-	call->reply_max = reply_max;
-
 	if (request_size) {
+		call->request_size = request_size;
 		call->request = kmalloc(request_size, GFP_NOFS);
 		if (!call->request)
 			goto nomem_free;
 	}
 
 	if (reply_max) {
+		call->reply_max = reply_max;
 		call->buffer = kmalloc(reply_max, GFP_NOFS);
 		if (!call->buffer)
 			goto nomem_free;
@@ -210,7 +239,7 @@ struct afs_call *afs_alloc_flat_call(const struct afs_call_type *type,
 	return call;
 
 nomem_free:
-	afs_free_call(call);
+	afs_put_call(call);
 nomem_call:
 	return NULL;
 }
@@ -315,7 +344,6 @@ int afs_make_call(struct in_addr *addr, struct afs_call *call, gfp_t gfp,
 	       atomic_read(&afs_outstanding_calls));
 
 	call->async = async;
-	INIT_WORK(&call->async_work, afs_process_async_call);
 
 	memset(&srx, 0, sizeof(srx));
 	srx.srx_family = AF_RXRPC;
@@ -378,7 +406,7 @@ int afs_make_call(struct in_addr *addr, struct afs_call *call, gfp_t gfp,
 error_do_abort:
 	rxrpc_kernel_abort_call(afs_socket, rxcall, RX_USER_ABORT, -ret, "KSD");
 error_kill_call:
-	afs_end_call(call);
+	afs_put_call(call);
 	_leave(" = %d", ret);
 	return ret;
 }
@@ -448,7 +476,7 @@ static void afs_deliver_to_call(struct afs_call *call)
 
 done:
 	if (call->state == AFS_CALL_COMPLETE && call->incoming)
-		afs_end_call(call);
+		afs_put_call(call);
 out:
 	_leave("");
 	return;
@@ -505,7 +533,7 @@ static int afs_wait_for_call_to_complete(struct afs_call *call)
 	}
 
 	_debug("call complete");
-	afs_end_call(call);
+	afs_put_call(call);
 	_leave(" = %d", ret);
 	return ret;
 }
@@ -529,14 +557,25 @@ static void afs_wake_up_async_call(struct sock *sk, struct rxrpc_call *rxcall,
 				   unsigned long call_user_ID)
 {
 	struct afs_call *call = (struct afs_call *)call_user_ID;
+	int u;
 
 	trace_afs_notify_call(rxcall, call);
 	call->need_attention = true;
-	queue_work(afs_async_calls, &call->async_work);
+
+	u = __atomic_add_unless(&call->usage, 1, 0);
+	if (u != 0) {
+		trace_afs_call(call, afs_call_trace_wake, u,
+			       atomic_read(&afs_outstanding_calls),
+			       __builtin_return_address(0));
+
+		if (!queue_work(afs_async_calls, &call->async_work))
+			afs_put_call(call);
+	}
 }
 
 /*
- * delete an asynchronous call
+ * Delete an asynchronous call.  The work item carries a ref to the call struct
+ * that we need to release.
  */
 static void afs_delete_async_call(struct work_struct *work)
 {
@@ -544,13 +583,14 @@ static void afs_delete_async_call(struct work_struct *work)
 
 	_enter("");
 
-	afs_free_call(call);
+	afs_put_call(call);
 
 	_leave("");
 }
 
 /*
- * perform processing on an asynchronous call
+ * Perform I/O processing on an asynchronous call.  The work item carries a ref
+ * to the call struct that we either need to release or to pass on.
  */
 static void afs_process_async_call(struct work_struct *work)
 {
@@ -566,15 +606,16 @@ static void afs_process_async_call(struct work_struct *work)
 	if (call->state == AFS_CALL_COMPLETE) {
 		call->reply = NULL;
 
-		/* kill the call */
-		afs_end_call_nofree(call);
-
-		/* we can't just delete the call because the work item may be
-		 * queued */
+		/* We have two refs to release - one from the alloc and one
+		 * queued with the work item - and we can't just deallocate the
+		 * call because the work item may be queued again.
+		 */
 		call->async_work.func = afs_delete_async_call;
-		queue_work(afs_async_calls, &call->async_work);
+		if (!queue_work(afs_async_calls, &call->async_work))
+			afs_put_call(call);
 	}
 
+	afs_put_call(call);
 	_leave("");
 }
 
@@ -594,12 +635,10 @@ static void afs_charge_preallocation(struct work_struct *work)
 
 	for (;;) {
 		if (!call) {
-			call = kzalloc(sizeof(struct afs_call), GFP_KERNEL);
+			call = afs_alloc_call(&afs_RXCMxxxx, GFP_KERNEL);
 			if (!call)
 				break;
 
-			INIT_WORK(&call->async_work, afs_process_async_call);
-			call->type = &afs_RXCMxxxx;
 			call->async = true;
 			call->state = AFS_CALL_AWAIT_OP_ID;
 			init_waitqueue_head(&call->waitq);
@@ -624,9 +663,8 @@ static void afs_rx_discard_new_call(struct rxrpc_call *rxcall,
 {
 	struct afs_call *call = (struct afs_call *)user_call_ID;
 
-	atomic_inc(&afs_outstanding_calls);
 	call->rxcall = NULL;
-	afs_free_call(call);
+	afs_put_call(call);
 }
 
 /*
@@ -635,7 +673,6 @@ static void afs_rx_discard_new_call(struct rxrpc_call *rxcall,
 static void afs_rx_new_call(struct sock *sk, struct rxrpc_call *rxcall,
 			    unsigned long user_call_ID)
 {
-	atomic_inc(&afs_outstanding_calls);
 	queue_work(afs_wq, &afs_charge_preallocation_work);
 }
 
@@ -699,7 +736,6 @@ void afs_send_empty_reply(struct afs_call *call)
 		rxrpc_kernel_abort_call(afs_socket, call->rxcall,
 					RX_USER_ABORT, ENOMEM, "KOO");
 	default:
-		afs_end_call(call);
 		_leave(" [error]");
 		return;
 	}
@@ -738,7 +774,6 @@ void afs_send_simple_reply(struct afs_call *call, const void *buf, size_t len)
 		rxrpc_kernel_abort_call(afs_socket, call->rxcall,
 					RX_USER_ABORT, ENOMEM, "KOO");
 	}
-	afs_end_call(call);
 	_leave(" [error]");
 }
 

commit 56ff9c837778ba2cf76f29c966856a9341e5939d
Author: David Howells <dhowells@redhat.com>
Date:   Thu Jan 5 10:38:36 2017 +0000

    afs: Kill afs_wait_mode
    
    The afs_wait_mode struct isn't really necessary.  Client calls only use one
    of a choice of two (synchronous or the asynchronous) and incoming calls
    don't use the wait at all.  Replace with a boolean parameter.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index f26344a8c029..ec1e41f929d1 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -25,29 +25,11 @@ static void afs_free_call(struct afs_call *);
 static void afs_wake_up_call_waiter(struct sock *, struct rxrpc_call *, unsigned long);
 static int afs_wait_for_call_to_complete(struct afs_call *);
 static void afs_wake_up_async_call(struct sock *, struct rxrpc_call *, unsigned long);
-static int afs_dont_wait_for_call_to_complete(struct afs_call *);
 static void afs_process_async_call(struct work_struct *);
 static void afs_rx_new_call(struct sock *, struct rxrpc_call *, unsigned long);
 static void afs_rx_discard_new_call(struct rxrpc_call *, unsigned long);
 static int afs_deliver_cm_op_id(struct afs_call *);
 
-/* synchronous call management */
-const struct afs_wait_mode afs_sync_call = {
-	.notify_rx	= afs_wake_up_call_waiter,
-	.wait		= afs_wait_for_call_to_complete,
-};
-
-/* asynchronous call management */
-const struct afs_wait_mode afs_async_call = {
-	.notify_rx	= afs_wake_up_async_call,
-	.wait		= afs_dont_wait_for_call_to_complete,
-};
-
-/* asynchronous incoming call management */
-static const struct afs_wait_mode afs_async_incoming_call = {
-	.notify_rx	= afs_wake_up_async_call,
-};
-
 /* asynchronous incoming call initial processing */
 static const struct afs_call_type afs_RXCMxxxx = {
 	.name		= "CB.xxxx",
@@ -315,7 +297,7 @@ static int afs_send_pages(struct afs_call *call, struct msghdr *msg,
  * initiate a call
  */
 int afs_make_call(struct in_addr *addr, struct afs_call *call, gfp_t gfp,
-		  const struct afs_wait_mode *wait_mode)
+		  bool async)
 {
 	struct sockaddr_rxrpc srx;
 	struct rxrpc_call *rxcall;
@@ -332,7 +314,7 @@ int afs_make_call(struct in_addr *addr, struct afs_call *call, gfp_t gfp,
 	       call, call->type->name, key_serial(call->key),
 	       atomic_read(&afs_outstanding_calls));
 
-	call->wait_mode = wait_mode;
+	call->async = async;
 	INIT_WORK(&call->async_work, afs_process_async_call);
 
 	memset(&srx, 0, sizeof(srx));
@@ -347,7 +329,9 @@ int afs_make_call(struct in_addr *addr, struct afs_call *call, gfp_t gfp,
 	/* create a call */
 	rxcall = rxrpc_kernel_begin_call(afs_socket, &srx, call->key,
 					 (unsigned long) call, gfp,
-					 wait_mode->notify_rx);
+					 (async ?
+					  afs_wake_up_async_call :
+					  afs_wake_up_call_waiter));
 	call->key = NULL;
 	if (IS_ERR(rxcall)) {
 		ret = PTR_ERR(rxcall);
@@ -386,7 +370,10 @@ int afs_make_call(struct in_addr *addr, struct afs_call *call, gfp_t gfp,
 
 	/* at this point, an async call may no longer exist as it may have
 	 * already completed */
-	return wait_mode->wait(call);
+	if (call->async)
+		return -EINPROGRESS;
+
+	return afs_wait_for_call_to_complete(call);
 
 error_do_abort:
 	rxrpc_kernel_abort_call(afs_socket, rxcall, RX_USER_ABORT, -ret, "KSD");
@@ -548,17 +535,6 @@ static void afs_wake_up_async_call(struct sock *sk, struct rxrpc_call *rxcall,
 	queue_work(afs_async_calls, &call->async_work);
 }
 
-/*
- * put a call into asynchronous mode
- * - mustn't touch the call descriptor as the call my have completed by the
- *   time we get here
- */
-static int afs_dont_wait_for_call_to_complete(struct afs_call *call)
-{
-	_enter("");
-	return -EINPROGRESS;
-}
-
 /*
  * delete an asynchronous call
  */
@@ -587,10 +563,7 @@ static void afs_process_async_call(struct work_struct *work)
 		afs_deliver_to_call(call);
 	}
 
-	if (call->state == AFS_CALL_COMPLETE && call->wait_mode) {
-		if (call->wait_mode->async_complete)
-			call->wait_mode->async_complete(call->reply,
-							call->error);
+	if (call->state == AFS_CALL_COMPLETE) {
 		call->reply = NULL;
 
 		/* kill the call */
@@ -626,10 +599,10 @@ static void afs_charge_preallocation(struct work_struct *work)
 				break;
 
 			INIT_WORK(&call->async_work, afs_process_async_call);
-			call->wait_mode = &afs_async_incoming_call;
 			call->type = &afs_RXCMxxxx;
-			init_waitqueue_head(&call->waitq);
+			call->async = true;
 			call->state = AFS_CALL_AWAIT_OP_ID;
+			init_waitqueue_head(&call->waitq);
 		}
 
 		if (rxrpc_kernel_charge_accept(afs_socket,

commit 8e8d7f13b6d5a93b3d2cf9a4ceaaf923809fd5ac
Author: David Howells <dhowells@redhat.com>
Date:   Thu Jan 5 10:38:34 2017 +0000

    afs: Add some tracepoints
    
    Add three tracepoints to the AFS filesystem:
    
     (1) The afs_recv_data tracepoint logs data segments that are extracted
         from the data received from the peer through afs_extract_data().
    
     (2) The afs_notify_call tracepoint logs notification from AF_RXRPC of data
         coming in to an asynchronous call.
    
     (3) The afs_cb_call tracepoint logs incoming calls that have had their
         operation ID extracted and mapped into a supported cache manager
         service call.
    
    To make (3) work, the name strings in the afs_call_type struct objects have
    to be annotated with __tracepoint_string.  This is done with the CM_NAME()
    macro.
    
    Further, the AFS call state enum needs a name so that it can be used to
    declare parameter types.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 25f05a8d21b1..f26344a8c029 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -416,6 +416,8 @@ static void afs_deliver_to_call(struct afs_call *call)
 			ret = rxrpc_kernel_recv_data(afs_socket, call->rxcall,
 						     NULL, 0, &offset, false,
 						     &call->abort_code);
+			trace_afs_recv_data(call, 0, offset, false, ret);
+
 			if (ret == -EINPROGRESS || ret == -EAGAIN)
 				return;
 			if (ret == 1 || ret < 0) {
@@ -541,6 +543,7 @@ static void afs_wake_up_async_call(struct sock *sk, struct rxrpc_call *rxcall,
 {
 	struct afs_call *call = (struct afs_call *)call_user_ID;
 
+	trace_afs_notify_call(rxcall, call);
 	call->need_attention = true;
 	queue_work(afs_async_calls, &call->async_work);
 }
@@ -689,6 +692,8 @@ static int afs_deliver_cm_op_id(struct afs_call *call)
 	if (!afs_cm_incoming_call(call))
 		return -ENOTSUPP;
 
+	trace_afs_cb_call(call);
+
 	/* pass responsibility for the remainer of this message off to the
 	 * cache manager op */
 	return call->type->deliver(call);
@@ -780,6 +785,7 @@ int afs_extract_data(struct afs_call *call, void *buf, size_t count,
 	ret = rxrpc_kernel_recv_data(afs_socket, call->rxcall,
 				     buf, count, &call->offset,
 				     want_more, &call->abort_code);
+	trace_afs_recv_data(call, count, call->offset, want_more, ret);
 	if (ret == 0 || ret == -EAGAIN)
 		return ret;
 

commit 39c6aceae961776a11a3767553b0e295fc9d413b
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sat Jan 9 20:36:51 2016 -0500

    afs_send_pages(): use ITER_BVEC
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 25f05a8d21b1..bfba8f0c3d43 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -249,8 +249,7 @@ void afs_flat_call_destructor(struct afs_call *call)
 /*
  * attach the data from a bunch of pages on an inode to a call
  */
-static int afs_send_pages(struct afs_call *call, struct msghdr *msg,
-			  struct kvec *iov)
+static int afs_send_pages(struct afs_call *call, struct msghdr *msg)
 {
 	struct page *pages[8];
 	unsigned count, n, loop, offset, to;
@@ -273,20 +272,21 @@ static int afs_send_pages(struct afs_call *call, struct msghdr *msg,
 
 		loop = 0;
 		do {
+			struct bio_vec bvec = {.bv_page = pages[loop],
+					       .bv_offset = offset};
 			msg->msg_flags = 0;
 			to = PAGE_SIZE;
 			if (first + loop >= last)
 				to = call->last_to;
 			else
 				msg->msg_flags = MSG_MORE;
-			iov->iov_base = kmap(pages[loop]) + offset;
-			iov->iov_len = to - offset;
+			bvec.bv_len = to - offset;
 			offset = 0;
 
 			_debug("- range %u-%u%s",
 			       offset, to, msg->msg_flags ? " [more]" : "");
-			iov_iter_kvec(&msg->msg_iter, WRITE | ITER_KVEC,
-				      iov, 1, to - offset);
+			iov_iter_bvec(&msg->msg_iter, WRITE | ITER_BVEC,
+				      &bvec, 1, to - offset);
 
 			/* have to change the state *before* sending the last
 			 * packet as RxRPC might give us the reply before it
@@ -295,7 +295,6 @@ static int afs_send_pages(struct afs_call *call, struct msghdr *msg,
 				call->state = AFS_CALL_AWAIT_REPLY;
 			ret = rxrpc_kernel_send_data(afs_socket, call->rxcall,
 						     msg, to - offset);
-			kunmap(pages[loop]);
 			if (ret < 0)
 				break;
 		} while (++loop < count);
@@ -379,7 +378,7 @@ int afs_make_call(struct in_addr *addr, struct afs_call *call, gfp_t gfp,
 		goto error_do_abort;
 
 	if (call->send_pages) {
-		ret = afs_send_pages(call, &msg, iov);
+		ret = afs_send_pages(call, &msg);
 		if (ret < 0)
 			goto error_do_abort;
 	}

commit 50a2c95381a7d0e453d7bdfde81d0c5f8351ba54
Author: David Howells <dhowells@redhat.com>
Date:   Thu Oct 13 08:27:10 2016 +0100

    afs: call->operation_ID sometimes used as __be32 sometimes as u32
    
    call->operation_ID is sometimes being used as __be32 sometimes is being
    used as u32.  Be consistent and settle on using as u32.
    
    Signed-off-by: David Howells <dhowells@redhat.com.

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 477928b25940..25f05a8d21b1 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -676,10 +676,11 @@ static int afs_deliver_cm_op_id(struct afs_call *call)
 	ASSERTCMP(call->offset, <, 4);
 
 	/* the operation ID forms the first four bytes of the request data */
-	ret = afs_extract_data(call, &call->operation_ID, 4, true);
+	ret = afs_extract_data(call, &call->tmp, 4, true);
 	if (ret < 0)
 		return ret;
 
+	call->operation_ID = ntohl(call->tmp);
 	call->state = AFS_CALL_AWAIT_REQUEST;
 	call->offset = 0;
 

commit 9008f998a2e992991a5d60656d4573ba4c516c58
Author: David Howells <dhowells@redhat.com>
Date:   Thu Oct 6 08:11:50 2016 +0100

    afs: Check for fatal error when in waiting for ack state
    
    When it's in the waiting-for-ACK state, the AFS filesystem needs to check
    the result of rxrpc_kernel_recv_data() any time it is notified to see if it
    is indicating a fatal error.  If this is the case, it needs to mark the
    call completed otherwise the call just sits there and never goes away.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 59bdaa7527b6..477928b25940 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -418,7 +418,7 @@ static void afs_deliver_to_call(struct afs_call *call)
 						     &call->abort_code);
 			if (ret == -EINPROGRESS || ret == -EAGAIN)
 				return;
-			if (ret == 1) {
+			if (ret == 1 || ret < 0) {
 				call->state = AFS_CALL_COMPLETE;
 				goto done;
 			}

commit 248f219cb8bcbfbd7f132752d44afa2df7c241d1
Author: David Howells <dhowells@redhat.com>
Date:   Thu Sep 8 11:10:12 2016 +0100

    rxrpc: Rewrite the data and ack handling code
    
    Rewrite the data and ack handling code such that:
    
     (1) Parsing of received ACK and ABORT packets and the distribution and the
         filing of DATA packets happens entirely within the data_ready context
         called from the UDP socket.  This allows us to process and discard ACK
         and ABORT packets much more quickly (they're no longer stashed on a
         queue for a background thread to process).
    
     (2) We avoid calling skb_clone(), pskb_pull() and pskb_trim().  We instead
         keep track of the offset and length of the content of each packet in
         the sk_buff metadata.  This means we don't do any allocation in the
         receive path.
    
     (3) Jumbo DATA packet parsing is now done in data_ready context.  Rather
         than cloning the packet once for each subpacket and pulling/trimming
         it, we file the packet multiple times with an annotation for each
         indicating which subpacket is there.  From that we can directly
         calculate the offset and length.
    
     (4) A call's receive queue can be accessed without taking locks (memory
         barriers do have to be used, though).
    
     (5) Incoming calls are set up from preallocated resources and immediately
         made live.  They can than have packets queued upon them and ACKs
         generated.  If insufficient resources exist, DATA packet #1 is given a
         BUSY reply and other DATA packets are discarded).
    
     (6) sk_buffs no longer take a ref on their parent call.
    
    To make this work, the following changes are made:
    
     (1) Each call's receive buffer is now a circular buffer of sk_buff
         pointers (rxtx_buffer) rather than a number of sk_buff_heads spread
         between the call and the socket.  This permits each sk_buff to be in
         the buffer multiple times.  The receive buffer is reused for the
         transmit buffer.
    
     (2) A circular buffer of annotations (rxtx_annotations) is kept parallel
         to the data buffer.  Transmission phase annotations indicate whether a
         buffered packet has been ACK'd or not and whether it needs
         retransmission.
    
         Receive phase annotations indicate whether a slot holds a whole packet
         or a jumbo subpacket and, if the latter, which subpacket.  They also
         note whether the packet has been decrypted in place.
    
     (3) DATA packet window tracking is much simplified.  Each phase has just
         two numbers representing the window (rx_hard_ack/rx_top and
         tx_hard_ack/tx_top).
    
         The hard_ack number is the sequence number before base of the window,
         representing the last packet the other side says it has consumed.
         hard_ack starts from 0 and the first packet is sequence number 1.
    
         The top number is the sequence number of the highest-numbered packet
         residing in the buffer.  Packets between hard_ack+1 and top are
         soft-ACK'd to indicate they've been received, but not yet consumed.
    
         Four macros, before(), before_eq(), after() and after_eq() are added
         to compare sequence numbers within the window.  This allows for the
         top of the window to wrap when the hard-ack sequence number gets close
         to the limit.
    
         Two flags, RXRPC_CALL_RX_LAST and RXRPC_CALL_TX_LAST, are added also
         to indicate when rx_top and tx_top point at the packets with the
         LAST_PACKET bit set, indicating the end of the phase.
    
     (4) Calls are queued on the socket 'receive queue' rather than packets.
         This means that we don't need have to invent dummy packets to queue to
         indicate abnormal/terminal states and we don't have to keep metadata
         packets (such as ABORTs) around
    
     (5) The offset and length of a (sub)packet's content are now passed to
         the verify_packet security op.  This is currently expected to decrypt
         the packet in place and validate it.
    
         However, there's now nowhere to store the revised offset and length of
         the actual data within the decrypted blob (there may be a header and
         padding to skip) because an sk_buff may represent multiple packets, so
         a locate_data security op is added to retrieve these details from the
         sk_buff content when needed.
    
     (6) recvmsg() now has to handle jumbo subpackets, where each subpacket is
         individually secured and needs to be individually decrypted.  The code
         to do this is broken out into rxrpc_recvmsg_data() and shared with the
         kernel API.  It now iterates over the call's receive buffer rather
         than walking the socket receive queue.
    
    Additional changes:
    
     (1) The timers are condensed to a single timer that is set for the soonest
         of three timeouts (delayed ACK generation, DATA retransmission and
         call lifespan).
    
     (2) Transmission of ACK and ABORT packets is effected immediately from
         process-context socket ops/kernel API calls that cause them instead of
         them being punted off to a background work item.  The data_ready
         handler still has to defer to the background, though.
    
     (3) A shutdown op is added to the AF_RXRPC socket so that the AFS
         filesystem can shut down the socket and flush its own work items
         before closing the socket to deal with any in-progress service calls.
    
    Future additional changes that will need to be considered:
    
     (1) Make sure that a call doesn't hog the front of the queue by receiving
         data from the network as fast as userspace is consuming it to the
         exclusion of other calls.
    
     (2) Transmit delayed ACKs from within recvmsg() when we've consumed
         sufficiently more packets to avoid the background work item needing to
         run.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 720ef05a24fe..59bdaa7527b6 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -55,10 +55,8 @@ static const struct afs_call_type afs_RXCMxxxx = {
 	.abort_to_error	= afs_abort_to_error,
 };
 
-static void afs_collect_incoming_call(struct work_struct *);
 static void afs_charge_preallocation(struct work_struct *);
 
-static DECLARE_WORK(afs_collect_incoming_call_work, afs_collect_incoming_call);
 static DECLARE_WORK(afs_charge_preallocation_work, afs_charge_preallocation);
 
 static int afs_wait_atomic_t(atomic_t *p)
@@ -143,6 +141,8 @@ void afs_close_socket(void)
 			 TASK_UNINTERRUPTIBLE);
 	_debug("no outstanding calls");
 
+	flush_workqueue(afs_async_calls);
+	kernel_sock_shutdown(afs_socket, SHUT_RDWR);
 	flush_workqueue(afs_async_calls);
 	sock_release(afs_socket);
 
@@ -602,51 +602,6 @@ static void afs_process_async_call(struct work_struct *work)
 	_leave("");
 }
 
-/*
- * accept the backlog of incoming calls
- */
-static void afs_collect_incoming_call(struct work_struct *work)
-{
-	struct rxrpc_call *rxcall;
-	struct afs_call *call = NULL;
-
-	_enter("");
-
-	do {
-		if (!call) {
-			call = kzalloc(sizeof(struct afs_call), GFP_KERNEL);
-			if (!call) {
-				rxrpc_kernel_reject_call(afs_socket);
-				return;
-			}
-
-			INIT_WORK(&call->async_work, afs_process_async_call);
-			call->wait_mode = &afs_async_incoming_call;
-			call->type = &afs_RXCMxxxx;
-			init_waitqueue_head(&call->waitq);
-			call->state = AFS_CALL_AWAIT_OP_ID;
-
-			_debug("CALL %p{%s} [%d]",
-			       call, call->type->name,
-			       atomic_read(&afs_outstanding_calls));
-			atomic_inc(&afs_outstanding_calls);
-		}
-
-		rxcall = rxrpc_kernel_accept_call(afs_socket,
-						  (unsigned long)call,
-						  afs_wake_up_async_call);
-		if (!IS_ERR(rxcall)) {
-			call->rxcall = rxcall;
-			call->need_attention = true;
-			queue_work(afs_async_calls, &call->async_work);
-			call = NULL;
-		}
-	} while (!call);
-
-	if (call)
-		afs_free_call(call);
-}
-
 static void afs_rx_attach(struct rxrpc_call *rxcall, unsigned long user_call_ID)
 {
 	struct afs_call *call = (struct afs_call *)user_call_ID;
@@ -704,7 +659,7 @@ static void afs_rx_discard_new_call(struct rxrpc_call *rxcall,
 static void afs_rx_new_call(struct sock *sk, struct rxrpc_call *rxcall,
 			    unsigned long user_call_ID)
 {
-	queue_work(afs_wq, &afs_collect_incoming_call_work);
+	atomic_inc(&afs_outstanding_calls);
 	queue_work(afs_wq, &afs_charge_preallocation_work);
 }
 

commit 00e907127e6f86d0f9b122d9b4347a8aa09a8b61
Author: David Howells <dhowells@redhat.com>
Date:   Thu Sep 8 11:10:12 2016 +0100

    rxrpc: Preallocate peers, conns and calls for incoming service requests
    
    Make it possible for the data_ready handler called from the UDP transport
    socket to completely instantiate an rxrpc_call structure and make it
    immediately live by preallocating all the memory it might need.  The idea
    is to cut out the background thread usage as much as possible.
    
    [Note that the preallocated structs are not actually used in this patch -
     that will be done in a future patch.]
    
    If insufficient resources are available in the preallocation buffers, it
    will be possible to discard the DATA packet in the data_ready handler or
    schedule a BUSY packet without the need to schedule an attempt at
    allocation in a background thread.
    
    To this end:
    
     (1) Preallocate rxrpc_peer, rxrpc_connection and rxrpc_call structs to a
         maximum number each of the listen backlog size.  The backlog size is
         limited to a maxmimum of 32.  Only this many of each can be in the
         preallocation buffer.
    
     (2) For userspace sockets, the preallocation is charged initially by
         listen() and will be recharged by accepting or rejecting pending
         new incoming calls.
    
     (3) For kernel services {,re,dis}charging of the preallocation buffers is
         handled manually.  Two notifier callbacks have to be provided before
         kernel_listen() is invoked:
    
         (a) An indication that a new call has been instantiated.  This can be
             used to trigger background recharging.
    
         (b) An indication that a call is being discarded.  This is used when
             the socket is being released.
    
         A function, rxrpc_kernel_charge_accept() is called by the kernel
         service to preallocate a single call.  It should be passed the user ID
         to be used for that call and a callback to associate the rxrpc call
         with the kernel service's side of the ID.
    
     (4) Discard the preallocation when the socket is closed.
    
     (5) Temporarily bump the refcount on the call allocated in
         rxrpc_incoming_call() so that rxrpc_release_call() can ditch the
         preallocation ref on service calls unconditionally.  This will no
         longer be necessary once the preallocation is used.
    
    Note that this does not yet control the number of active service calls on a
    client - that will come in a later patch.
    
    A future development would be to provide a setsockopt() call that allows a
    userspace server to manually charge the preallocation buffer.  This would
    allow user call IDs to be provided in advance and the awkward manual accept
    stage to be bypassed.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 53750dece80e..720ef05a24fe 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -18,6 +18,7 @@
 
 struct socket *afs_socket; /* my RxRPC socket */
 static struct workqueue_struct *afs_async_calls;
+static struct afs_call *afs_spare_incoming_call;
 static atomic_t afs_outstanding_calls;
 
 static void afs_free_call(struct afs_call *);
@@ -26,7 +27,8 @@ static int afs_wait_for_call_to_complete(struct afs_call *);
 static void afs_wake_up_async_call(struct sock *, struct rxrpc_call *, unsigned long);
 static int afs_dont_wait_for_call_to_complete(struct afs_call *);
 static void afs_process_async_call(struct work_struct *);
-static void afs_rx_new_call(struct sock *);
+static void afs_rx_new_call(struct sock *, struct rxrpc_call *, unsigned long);
+static void afs_rx_discard_new_call(struct rxrpc_call *, unsigned long);
 static int afs_deliver_cm_op_id(struct afs_call *);
 
 /* synchronous call management */
@@ -54,8 +56,10 @@ static const struct afs_call_type afs_RXCMxxxx = {
 };
 
 static void afs_collect_incoming_call(struct work_struct *);
+static void afs_charge_preallocation(struct work_struct *);
 
 static DECLARE_WORK(afs_collect_incoming_call_work, afs_collect_incoming_call);
+static DECLARE_WORK(afs_charge_preallocation_work, afs_charge_preallocation);
 
 static int afs_wait_atomic_t(atomic_t *p)
 {
@@ -100,13 +104,15 @@ int afs_open_socket(void)
 	if (ret < 0)
 		goto error_2;
 
-	rxrpc_kernel_new_call_notification(socket, afs_rx_new_call);
+	rxrpc_kernel_new_call_notification(socket, afs_rx_new_call,
+					   afs_rx_discard_new_call);
 
 	ret = kernel_listen(socket, INT_MAX);
 	if (ret < 0)
 		goto error_2;
 
 	afs_socket = socket;
+	afs_charge_preallocation(NULL);
 	_leave(" = 0");
 	return 0;
 
@@ -126,6 +132,12 @@ void afs_close_socket(void)
 {
 	_enter("");
 
+	if (afs_spare_incoming_call) {
+		atomic_inc(&afs_outstanding_calls);
+		afs_free_call(afs_spare_incoming_call);
+		afs_spare_incoming_call = NULL;
+	}
+
 	_debug("outstanding %u", atomic_read(&afs_outstanding_calls));
 	wait_on_atomic_t(&afs_outstanding_calls, afs_wait_atomic_t,
 			 TASK_UNINTERRUPTIBLE);
@@ -635,12 +647,65 @@ static void afs_collect_incoming_call(struct work_struct *work)
 		afs_free_call(call);
 }
 
+static void afs_rx_attach(struct rxrpc_call *rxcall, unsigned long user_call_ID)
+{
+	struct afs_call *call = (struct afs_call *)user_call_ID;
+
+	call->rxcall = rxcall;
+}
+
+/*
+ * Charge the incoming call preallocation.
+ */
+static void afs_charge_preallocation(struct work_struct *work)
+{
+	struct afs_call *call = afs_spare_incoming_call;
+
+	for (;;) {
+		if (!call) {
+			call = kzalloc(sizeof(struct afs_call), GFP_KERNEL);
+			if (!call)
+				break;
+
+			INIT_WORK(&call->async_work, afs_process_async_call);
+			call->wait_mode = &afs_async_incoming_call;
+			call->type = &afs_RXCMxxxx;
+			init_waitqueue_head(&call->waitq);
+			call->state = AFS_CALL_AWAIT_OP_ID;
+		}
+
+		if (rxrpc_kernel_charge_accept(afs_socket,
+					       afs_wake_up_async_call,
+					       afs_rx_attach,
+					       (unsigned long)call,
+					       GFP_KERNEL) < 0)
+			break;
+		call = NULL;
+	}
+	afs_spare_incoming_call = call;
+}
+
+/*
+ * Discard a preallocated call when a socket is shut down.
+ */
+static void afs_rx_discard_new_call(struct rxrpc_call *rxcall,
+				    unsigned long user_call_ID)
+{
+	struct afs_call *call = (struct afs_call *)user_call_ID;
+
+	atomic_inc(&afs_outstanding_calls);
+	call->rxcall = NULL;
+	afs_free_call(call);
+}
+
 /*
  * Notification of an incoming call.
  */
-static void afs_rx_new_call(struct sock *sk)
+static void afs_rx_new_call(struct sock *sk, struct rxrpc_call *rxcall,
+			    unsigned long user_call_ID)
 {
 	queue_work(afs_wq, &afs_collect_incoming_call_work);
+	queue_work(afs_wq, &afs_charge_preallocation_work);
 }
 
 /*

commit 5a42976d4fe5d7fddce133de995c742c87b1b7e3
Author: David Howells <dhowells@redhat.com>
Date:   Tue Sep 6 22:19:51 2016 +0100

    rxrpc: Add tracepoint for working out where aborts happen
    
    Add a tracepoint for working out where local aborts happen.  Each
    tracepoint call is labelled with a 3-letter code so that they can be
    distinguished - and the DATA sequence number is added too where available.
    
    rxrpc_kernel_abort_call() also takes a 3-letter code so that AFS can
    indicate the circumstances when it aborts a call.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 37608be52abd..53750dece80e 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -377,7 +377,7 @@ int afs_make_call(struct in_addr *addr, struct afs_call *call, gfp_t gfp,
 	return wait_mode->wait(call);
 
 error_do_abort:
-	rxrpc_kernel_abort_call(afs_socket, rxcall, RX_USER_ABORT);
+	rxrpc_kernel_abort_call(afs_socket, rxcall, RX_USER_ABORT, -ret, "KSD");
 error_kill_call:
 	afs_end_call(call);
 	_leave(" = %d", ret);
@@ -425,12 +425,12 @@ static void afs_deliver_to_call(struct afs_call *call)
 		case -ENOTCONN:
 			abort_code = RX_CALL_DEAD;
 			rxrpc_kernel_abort_call(afs_socket, call->rxcall,
-						abort_code);
+						abort_code, -ret, "KNC");
 			goto do_abort;
 		case -ENOTSUPP:
 			abort_code = RX_INVALID_OPERATION;
 			rxrpc_kernel_abort_call(afs_socket, call->rxcall,
-						abort_code);
+						abort_code, -ret, "KIV");
 			goto do_abort;
 		case -ENODATA:
 		case -EBADMSG:
@@ -440,7 +440,7 @@ static void afs_deliver_to_call(struct afs_call *call)
 			if (call->state != AFS_CALL_AWAIT_REPLY)
 				abort_code = RXGEN_SS_UNMARSHAL;
 			rxrpc_kernel_abort_call(afs_socket, call->rxcall,
-						abort_code);
+						abort_code, EBADMSG, "KUM");
 			goto do_abort;
 		}
 	}
@@ -463,6 +463,7 @@ static void afs_deliver_to_call(struct afs_call *call)
  */
 static int afs_wait_for_call_to_complete(struct afs_call *call)
 {
+	const char *abort_why;
 	int ret;
 
 	DECLARE_WAITQUEUE(myself, current);
@@ -481,9 +482,11 @@ static int afs_wait_for_call_to_complete(struct afs_call *call)
 			continue;
 		}
 
+		abort_why = "KWC";
 		ret = call->error;
 		if (call->state == AFS_CALL_COMPLETE)
 			break;
+		abort_why = "KWI";
 		ret = -EINTR;
 		if (signal_pending(current))
 			break;
@@ -497,7 +500,7 @@ static int afs_wait_for_call_to_complete(struct afs_call *call)
 	if (call->state < AFS_CALL_COMPLETE) {
 		_debug("call incomplete");
 		rxrpc_kernel_abort_call(afs_socket, call->rxcall,
-					RX_CALL_DEAD);
+					RX_CALL_DEAD, -ret, abort_why);
 	}
 
 	_debug("call complete");
@@ -695,7 +698,7 @@ void afs_send_empty_reply(struct afs_call *call)
 	case -ENOMEM:
 		_debug("oom");
 		rxrpc_kernel_abort_call(afs_socket, call->rxcall,
-					RX_USER_ABORT);
+					RX_USER_ABORT, ENOMEM, "KOO");
 	default:
 		afs_end_call(call);
 		_leave(" [error]");
@@ -734,7 +737,7 @@ void afs_send_simple_reply(struct afs_call *call, const void *buf, size_t len)
 	if (n == -ENOMEM) {
 		_debug("oom");
 		rxrpc_kernel_abort_call(afs_socket, call->rxcall,
-					RX_USER_ABORT);
+					RX_USER_ABORT, ENOMEM, "KOO");
 	}
 	afs_end_call(call);
 	_leave(" [error]");

commit 69ad052aec6bb9a3f376b6ae117dfde28ed337c8
Author: Bhaktipriya Shridhar <bhaktipriya96@gmail.com>
Date:   Sun Sep 4 20:53:42 2016 +0530

    fs/afs/rxrpc: Remove deprecated create_singlethread_workqueue
    
    The workqueue "afs_async_calls" queues work item
    &call->async_work per afs_call. Since there could be multiple calls and since
    these calls can be run concurrently, alloc_workqueue has been used to replace
    the deprecated create_singlethread_workqueue instance.
    
    The WQ_MEM_RECLAIM flag has been set to ensure forward progress under
    memory pressure because the workqueue is being used on a memory reclaim
    path.
    
    Since there are fixed number of work items, explicit concurrency
    limit is unnecessary here.
    
    Signed-off-by: Bhaktipriya Shridhar <bhaktipriya96@gmail.com>
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 244896baf241..37608be52abd 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -76,7 +76,7 @@ int afs_open_socket(void)
 	_enter("");
 
 	ret = -ENOMEM;
-	afs_async_calls = create_singlethread_workqueue("kafsd");
+	afs_async_calls = alloc_workqueue("kafsd", WQ_MEM_RECLAIM, 0);
 	if (!afs_async_calls)
 		goto error_0;
 

commit d001648ec7cf8b21ae9eec8b9ba4a18295adfb14
Author: David Howells <dhowells@redhat.com>
Date:   Tue Aug 30 20:42:14 2016 +0100

    rxrpc: Don't expose skbs to in-kernel users [ver #2]
    
    Don't expose skbs to in-kernel users, such as the AFS filesystem, but
    instead provide a notification hook the indicates that a call needs
    attention and another that indicates that there's a new call to be
    collected.
    
    This makes the following possibilities more achievable:
    
     (1) Call refcounting can be made simpler if skbs don't hold refs to calls.
    
     (2) skbs referring to non-data events will be able to be freed much sooner
         rather than being queued for AFS to pick up as rxrpc_kernel_recv_data
         will be able to consult the call state.
    
     (3) We can shortcut the receive phase when a call is remotely aborted
         because we don't have to go through all the packets to get to the one
         cancelling the operation.
    
     (4) It makes it easier to do encryption/decryption directly between AFS's
         buffers and sk_buffs.
    
     (5) Encryption/decryption can more easily be done in the AFS's thread
         contexts - usually that of the userspace process that issued a syscall
         - rather than in one of rxrpc's background threads on a workqueue.
    
     (6) AFS will be able to wait synchronously on a call inside AF_RXRPC.
    
    To make this work, the following interface function has been added:
    
         int rxrpc_kernel_recv_data(
                    struct socket *sock, struct rxrpc_call *call,
                    void *buffer, size_t bufsize, size_t *_offset,
                    bool want_more, u32 *_abort_code);
    
    This is the recvmsg equivalent.  It allows the caller to find out about the
    state of a specific call and to transfer received data into a buffer
    piecemeal.
    
    afs_extract_data() and rxrpc_kernel_recv_data() now do all the extraction
    logic between them.  They don't wait synchronously yet because the socket
    lock needs to be dealt with.
    
    Five interface functions have been removed:
    
            rxrpc_kernel_is_data_last()
            rxrpc_kernel_get_abort_code()
            rxrpc_kernel_get_error_number()
            rxrpc_kernel_free_skb()
            rxrpc_kernel_data_consumed()
    
    As a temporary hack, sk_buffs going to an in-kernel call are queued on the
    rxrpc_call struct (->knlrecv_queue) rather than being handed over to the
    in-kernel user.  To process the queue internally, a temporary function,
    temp_deliver_data() has been added.  This will be replaced with common code
    between the rxrpc_recvmsg() path and the kernel_rxrpc_recv_data() path in a
    future patch.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 7b0d18900f50..244896baf241 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -19,31 +19,31 @@
 struct socket *afs_socket; /* my RxRPC socket */
 static struct workqueue_struct *afs_async_calls;
 static atomic_t afs_outstanding_calls;
-static atomic_t afs_outstanding_skbs;
 
-static void afs_wake_up_call_waiter(struct afs_call *);
+static void afs_free_call(struct afs_call *);
+static void afs_wake_up_call_waiter(struct sock *, struct rxrpc_call *, unsigned long);
 static int afs_wait_for_call_to_complete(struct afs_call *);
-static void afs_wake_up_async_call(struct afs_call *);
+static void afs_wake_up_async_call(struct sock *, struct rxrpc_call *, unsigned long);
 static int afs_dont_wait_for_call_to_complete(struct afs_call *);
-static void afs_process_async_call(struct afs_call *);
-static void afs_rx_interceptor(struct sock *, unsigned long, struct sk_buff *);
-static int afs_deliver_cm_op_id(struct afs_call *, struct sk_buff *, bool);
+static void afs_process_async_call(struct work_struct *);
+static void afs_rx_new_call(struct sock *);
+static int afs_deliver_cm_op_id(struct afs_call *);
 
 /* synchronous call management */
 const struct afs_wait_mode afs_sync_call = {
-	.rx_wakeup	= afs_wake_up_call_waiter,
+	.notify_rx	= afs_wake_up_call_waiter,
 	.wait		= afs_wait_for_call_to_complete,
 };
 
 /* asynchronous call management */
 const struct afs_wait_mode afs_async_call = {
-	.rx_wakeup	= afs_wake_up_async_call,
+	.notify_rx	= afs_wake_up_async_call,
 	.wait		= afs_dont_wait_for_call_to_complete,
 };
 
 /* asynchronous incoming call management */
 static const struct afs_wait_mode afs_async_incoming_call = {
-	.rx_wakeup	= afs_wake_up_async_call,
+	.notify_rx	= afs_wake_up_async_call,
 };
 
 /* asynchronous incoming call initial processing */
@@ -55,16 +55,8 @@ static const struct afs_call_type afs_RXCMxxxx = {
 
 static void afs_collect_incoming_call(struct work_struct *);
 
-static struct sk_buff_head afs_incoming_calls;
 static DECLARE_WORK(afs_collect_incoming_call_work, afs_collect_incoming_call);
 
-static void afs_async_workfn(struct work_struct *work)
-{
-	struct afs_call *call = container_of(work, struct afs_call, async_work);
-
-	call->async_workfn(call);
-}
-
 static int afs_wait_atomic_t(atomic_t *p)
 {
 	schedule();
@@ -83,8 +75,6 @@ int afs_open_socket(void)
 
 	_enter("");
 
-	skb_queue_head_init(&afs_incoming_calls);
-
 	ret = -ENOMEM;
 	afs_async_calls = create_singlethread_workqueue("kafsd");
 	if (!afs_async_calls)
@@ -110,12 +100,12 @@ int afs_open_socket(void)
 	if (ret < 0)
 		goto error_2;
 
+	rxrpc_kernel_new_call_notification(socket, afs_rx_new_call);
+
 	ret = kernel_listen(socket, INT_MAX);
 	if (ret < 0)
 		goto error_2;
 
-	rxrpc_kernel_intercept_rx_messages(socket, afs_rx_interceptor);
-
 	afs_socket = socket;
 	_leave(" = 0");
 	return 0;
@@ -136,51 +126,19 @@ void afs_close_socket(void)
 {
 	_enter("");
 
+	_debug("outstanding %u", atomic_read(&afs_outstanding_calls));
 	wait_on_atomic_t(&afs_outstanding_calls, afs_wait_atomic_t,
 			 TASK_UNINTERRUPTIBLE);
 	_debug("no outstanding calls");
 
+	flush_workqueue(afs_async_calls);
 	sock_release(afs_socket);
 
 	_debug("dework");
 	destroy_workqueue(afs_async_calls);
-
-	ASSERTCMP(atomic_read(&afs_outstanding_skbs), ==, 0);
 	_leave("");
 }
 
-/*
- * Note that the data in a socket buffer is now consumed.
- */
-void afs_data_consumed(struct afs_call *call, struct sk_buff *skb)
-{
-	if (!skb) {
-		_debug("DLVR NULL [%d]", atomic_read(&afs_outstanding_skbs));
-		dump_stack();
-	} else {
-		_debug("DLVR %p{%u} [%d]",
-		       skb, skb->mark, atomic_read(&afs_outstanding_skbs));
-		rxrpc_kernel_data_consumed(call->rxcall, skb);
-	}
-}
-
-/*
- * free a socket buffer
- */
-static void afs_free_skb(struct sk_buff *skb)
-{
-	if (!skb) {
-		_debug("FREE NULL [%d]", atomic_read(&afs_outstanding_skbs));
-		dump_stack();
-	} else {
-		_debug("FREE %p{%u} [%d]",
-		       skb, skb->mark, atomic_read(&afs_outstanding_skbs));
-		if (atomic_dec_return(&afs_outstanding_skbs) == -1)
-			BUG();
-		rxrpc_kernel_free_skb(skb);
-	}
-}
-
 /*
  * free a call
  */
@@ -191,7 +149,6 @@ static void afs_free_call(struct afs_call *call)
 
 	ASSERTCMP(call->rxcall, ==, NULL);
 	ASSERT(!work_pending(&call->async_work));
-	ASSERT(skb_queue_empty(&call->rx_queue));
 	ASSERT(call->type->name != NULL);
 
 	kfree(call->request);
@@ -227,7 +184,7 @@ static void afs_end_call(struct afs_call *call)
  * allocate a call with flat request and reply buffers
  */
 struct afs_call *afs_alloc_flat_call(const struct afs_call_type *type,
-				     size_t request_size, size_t reply_size)
+				     size_t request_size, size_t reply_max)
 {
 	struct afs_call *call;
 
@@ -241,7 +198,7 @@ struct afs_call *afs_alloc_flat_call(const struct afs_call_type *type,
 
 	call->type = type;
 	call->request_size = request_size;
-	call->reply_max = reply_size;
+	call->reply_max = reply_max;
 
 	if (request_size) {
 		call->request = kmalloc(request_size, GFP_NOFS);
@@ -249,14 +206,13 @@ struct afs_call *afs_alloc_flat_call(const struct afs_call_type *type,
 			goto nomem_free;
 	}
 
-	if (reply_size) {
-		call->buffer = kmalloc(reply_size, GFP_NOFS);
+	if (reply_max) {
+		call->buffer = kmalloc(reply_max, GFP_NOFS);
 		if (!call->buffer)
 			goto nomem_free;
 	}
 
 	init_waitqueue_head(&call->waitq);
-	skb_queue_head_init(&call->rx_queue);
 	return call;
 
 nomem_free:
@@ -354,7 +310,6 @@ int afs_make_call(struct in_addr *addr, struct afs_call *call, gfp_t gfp,
 	struct msghdr msg;
 	struct kvec iov[1];
 	int ret;
-	struct sk_buff *skb;
 
 	_enter("%x,{%d},", addr->s_addr, ntohs(call->port));
 
@@ -366,8 +321,7 @@ int afs_make_call(struct in_addr *addr, struct afs_call *call, gfp_t gfp,
 	       atomic_read(&afs_outstanding_calls));
 
 	call->wait_mode = wait_mode;
-	call->async_workfn = afs_process_async_call;
-	INIT_WORK(&call->async_work, afs_async_workfn);
+	INIT_WORK(&call->async_work, afs_process_async_call);
 
 	memset(&srx, 0, sizeof(srx));
 	srx.srx_family = AF_RXRPC;
@@ -380,7 +334,8 @@ int afs_make_call(struct in_addr *addr, struct afs_call *call, gfp_t gfp,
 
 	/* create a call */
 	rxcall = rxrpc_kernel_begin_call(afs_socket, &srx, call->key,
-					 (unsigned long) call, gfp);
+					 (unsigned long) call, gfp,
+					 wait_mode->notify_rx);
 	call->key = NULL;
 	if (IS_ERR(rxcall)) {
 		ret = PTR_ERR(rxcall);
@@ -423,150 +378,84 @@ int afs_make_call(struct in_addr *addr, struct afs_call *call, gfp_t gfp,
 
 error_do_abort:
 	rxrpc_kernel_abort_call(afs_socket, rxcall, RX_USER_ABORT);
-	while ((skb = skb_dequeue(&call->rx_queue)))
-		afs_free_skb(skb);
 error_kill_call:
 	afs_end_call(call);
 	_leave(" = %d", ret);
 	return ret;
 }
 
-/*
- * Handles intercepted messages that were arriving in the socket's Rx queue.
- *
- * Called from the AF_RXRPC call processor in waitqueue process context.  For
- * each call, it is guaranteed this will be called in order of packet to be
- * delivered.
- */
-static void afs_rx_interceptor(struct sock *sk, unsigned long user_call_ID,
-			       struct sk_buff *skb)
-{
-	struct afs_call *call = (struct afs_call *) user_call_ID;
-
-	_enter("%p,,%u", call, skb->mark);
-
-	_debug("ICPT %p{%u} [%d]",
-	       skb, skb->mark, atomic_read(&afs_outstanding_skbs));
-
-	ASSERTCMP(sk, ==, afs_socket->sk);
-	atomic_inc(&afs_outstanding_skbs);
-
-	if (!call) {
-		/* its an incoming call for our callback service */
-		skb_queue_tail(&afs_incoming_calls, skb);
-		queue_work(afs_wq, &afs_collect_incoming_call_work);
-	} else {
-		/* route the messages directly to the appropriate call */
-		skb_queue_tail(&call->rx_queue, skb);
-		call->wait_mode->rx_wakeup(call);
-	}
-
-	_leave("");
-}
-
 /*
  * deliver messages to a call
  */
 static void afs_deliver_to_call(struct afs_call *call)
 {
-	struct sk_buff *skb;
-	bool last;
 	u32 abort_code;
 	int ret;
 
-	_enter("");
-
-	while ((call->state == AFS_CALL_AWAIT_REPLY ||
-		call->state == AFS_CALL_AWAIT_OP_ID ||
-		call->state == AFS_CALL_AWAIT_REQUEST ||
-		call->state == AFS_CALL_AWAIT_ACK) &&
-	       (skb = skb_dequeue(&call->rx_queue))) {
-		switch (skb->mark) {
-		case RXRPC_SKB_MARK_DATA:
-			_debug("Rcv DATA");
-			last = rxrpc_kernel_is_data_last(skb);
-			ret = call->type->deliver(call, skb, last);
-			switch (ret) {
-			case -EAGAIN:
-				if (last) {
-					_debug("short data");
-					goto unmarshal_error;
-				}
-				break;
-			case 0:
-				ASSERT(last);
-				if (call->state == AFS_CALL_AWAIT_REPLY)
-					call->state = AFS_CALL_COMPLETE;
-				break;
-			case -ENOTCONN:
-				abort_code = RX_CALL_DEAD;
-				goto do_abort;
-			case -ENOTSUPP:
-				abort_code = RX_INVALID_OPERATION;
-				goto do_abort;
-			default:
-			unmarshal_error:
-				abort_code = RXGEN_CC_UNMARSHAL;
-				if (call->state != AFS_CALL_AWAIT_REPLY)
-					abort_code = RXGEN_SS_UNMARSHAL;
-			do_abort:
-				rxrpc_kernel_abort_call(afs_socket,
-							call->rxcall,
-							abort_code);
-				call->error = ret;
-				call->state = AFS_CALL_ERROR;
-				break;
+	_enter("%s", call->type->name);
+
+	while (call->state == AFS_CALL_AWAIT_REPLY ||
+	       call->state == AFS_CALL_AWAIT_OP_ID ||
+	       call->state == AFS_CALL_AWAIT_REQUEST ||
+	       call->state == AFS_CALL_AWAIT_ACK
+	       ) {
+		if (call->state == AFS_CALL_AWAIT_ACK) {
+			size_t offset = 0;
+			ret = rxrpc_kernel_recv_data(afs_socket, call->rxcall,
+						     NULL, 0, &offset, false,
+						     &call->abort_code);
+			if (ret == -EINPROGRESS || ret == -EAGAIN)
+				return;
+			if (ret == 1) {
+				call->state = AFS_CALL_COMPLETE;
+				goto done;
 			}
-			break;
-		case RXRPC_SKB_MARK_FINAL_ACK:
-			_debug("Rcv ACK");
-			call->state = AFS_CALL_COMPLETE;
-			break;
-		case RXRPC_SKB_MARK_BUSY:
-			_debug("Rcv BUSY");
-			call->error = -EBUSY;
-			call->state = AFS_CALL_BUSY;
-			break;
-		case RXRPC_SKB_MARK_REMOTE_ABORT:
-			abort_code = rxrpc_kernel_get_abort_code(skb);
-			call->error = call->type->abort_to_error(abort_code);
-			call->state = AFS_CALL_ABORTED;
-			_debug("Rcv ABORT %u -> %d", abort_code, call->error);
-			break;
-		case RXRPC_SKB_MARK_LOCAL_ABORT:
-			abort_code = rxrpc_kernel_get_abort_code(skb);
-			call->error = call->type->abort_to_error(abort_code);
-			call->state = AFS_CALL_ABORTED;
-			_debug("Loc ABORT %u -> %d", abort_code, call->error);
-			break;
-		case RXRPC_SKB_MARK_NET_ERROR:
-			call->error = -rxrpc_kernel_get_error_number(skb);
-			call->state = AFS_CALL_ERROR;
-			_debug("Rcv NET ERROR %d", call->error);
-			break;
-		case RXRPC_SKB_MARK_LOCAL_ERROR:
-			call->error = -rxrpc_kernel_get_error_number(skb);
-			call->state = AFS_CALL_ERROR;
-			_debug("Rcv LOCAL ERROR %d", call->error);
-			break;
-		default:
-			BUG();
-			break;
+			return;
 		}
 
-		afs_free_skb(skb);
-	}
-
-	/* make sure the queue is empty if the call is done with (we might have
-	 * aborted the call early because of an unmarshalling error) */
-	if (call->state >= AFS_CALL_COMPLETE) {
-		while ((skb = skb_dequeue(&call->rx_queue)))
-			afs_free_skb(skb);
-		if (call->incoming)
-			afs_end_call(call);
+		ret = call->type->deliver(call);
+		switch (ret) {
+		case 0:
+			if (call->state == AFS_CALL_AWAIT_REPLY)
+				call->state = AFS_CALL_COMPLETE;
+			goto done;
+		case -EINPROGRESS:
+		case -EAGAIN:
+			goto out;
+		case -ENOTCONN:
+			abort_code = RX_CALL_DEAD;
+			rxrpc_kernel_abort_call(afs_socket, call->rxcall,
+						abort_code);
+			goto do_abort;
+		case -ENOTSUPP:
+			abort_code = RX_INVALID_OPERATION;
+			rxrpc_kernel_abort_call(afs_socket, call->rxcall,
+						abort_code);
+			goto do_abort;
+		case -ENODATA:
+		case -EBADMSG:
+		case -EMSGSIZE:
+		default:
+			abort_code = RXGEN_CC_UNMARSHAL;
+			if (call->state != AFS_CALL_AWAIT_REPLY)
+				abort_code = RXGEN_SS_UNMARSHAL;
+			rxrpc_kernel_abort_call(afs_socket, call->rxcall,
+						abort_code);
+			goto do_abort;
+		}
 	}
 
+done:
+	if (call->state == AFS_CALL_COMPLETE && call->incoming)
+		afs_end_call(call);
+out:
 	_leave("");
+	return;
+
+do_abort:
+	call->error = ret;
+	call->state = AFS_CALL_COMPLETE;
+	goto done;
 }
 
 /*
@@ -574,7 +463,6 @@ static void afs_deliver_to_call(struct afs_call *call)
  */
 static int afs_wait_for_call_to_complete(struct afs_call *call)
 {
-	struct sk_buff *skb;
 	int ret;
 
 	DECLARE_WAITQUEUE(myself, current);
@@ -586,14 +474,15 @@ static int afs_wait_for_call_to_complete(struct afs_call *call)
 		set_current_state(TASK_INTERRUPTIBLE);
 
 		/* deliver any messages that are in the queue */
-		if (!skb_queue_empty(&call->rx_queue)) {
+		if (call->state < AFS_CALL_COMPLETE && call->need_attention) {
+			call->need_attention = false;
 			__set_current_state(TASK_RUNNING);
 			afs_deliver_to_call(call);
 			continue;
 		}
 
 		ret = call->error;
-		if (call->state >= AFS_CALL_COMPLETE)
+		if (call->state == AFS_CALL_COMPLETE)
 			break;
 		ret = -EINTR;
 		if (signal_pending(current))
@@ -607,9 +496,8 @@ static int afs_wait_for_call_to_complete(struct afs_call *call)
 	/* kill the call */
 	if (call->state < AFS_CALL_COMPLETE) {
 		_debug("call incomplete");
-		rxrpc_kernel_abort_call(afs_socket, call->rxcall, RX_CALL_DEAD);
-		while ((skb = skb_dequeue(&call->rx_queue)))
-			afs_free_skb(skb);
+		rxrpc_kernel_abort_call(afs_socket, call->rxcall,
+					RX_CALL_DEAD);
 	}
 
 	_debug("call complete");
@@ -621,17 +509,24 @@ static int afs_wait_for_call_to_complete(struct afs_call *call)
 /*
  * wake up a waiting call
  */
-static void afs_wake_up_call_waiter(struct afs_call *call)
+static void afs_wake_up_call_waiter(struct sock *sk, struct rxrpc_call *rxcall,
+				    unsigned long call_user_ID)
 {
+	struct afs_call *call = (struct afs_call *)call_user_ID;
+
+	call->need_attention = true;
 	wake_up(&call->waitq);
 }
 
 /*
  * wake up an asynchronous call
  */
-static void afs_wake_up_async_call(struct afs_call *call)
+static void afs_wake_up_async_call(struct sock *sk, struct rxrpc_call *rxcall,
+				   unsigned long call_user_ID)
 {
-	_enter("");
+	struct afs_call *call = (struct afs_call *)call_user_ID;
+
+	call->need_attention = true;
 	queue_work(afs_async_calls, &call->async_work);
 }
 
@@ -649,8 +544,10 @@ static int afs_dont_wait_for_call_to_complete(struct afs_call *call)
 /*
  * delete an asynchronous call
  */
-static void afs_delete_async_call(struct afs_call *call)
+static void afs_delete_async_call(struct work_struct *work)
 {
+	struct afs_call *call = container_of(work, struct afs_call, async_work);
+
 	_enter("");
 
 	afs_free_call(call);
@@ -660,17 +557,19 @@ static void afs_delete_async_call(struct afs_call *call)
 
 /*
  * perform processing on an asynchronous call
- * - on a multiple-thread workqueue this work item may try to run on several
- *   CPUs at the same time
  */
-static void afs_process_async_call(struct afs_call *call)
+static void afs_process_async_call(struct work_struct *work)
 {
+	struct afs_call *call = container_of(work, struct afs_call, async_work);
+
 	_enter("");
 
-	if (!skb_queue_empty(&call->rx_queue))
+	if (call->state < AFS_CALL_COMPLETE && call->need_attention) {
+		call->need_attention = false;
 		afs_deliver_to_call(call);
+	}
 
-	if (call->state >= AFS_CALL_COMPLETE && call->wait_mode) {
+	if (call->state == AFS_CALL_COMPLETE && call->wait_mode) {
 		if (call->wait_mode->async_complete)
 			call->wait_mode->async_complete(call->reply,
 							call->error);
@@ -681,45 +580,13 @@ static void afs_process_async_call(struct afs_call *call)
 
 		/* we can't just delete the call because the work item may be
 		 * queued */
-		call->async_workfn = afs_delete_async_call;
+		call->async_work.func = afs_delete_async_call;
 		queue_work(afs_async_calls, &call->async_work);
 	}
 
 	_leave("");
 }
 
-/*
- * Empty a socket buffer into a flat reply buffer.
- */
-int afs_transfer_reply(struct afs_call *call, struct sk_buff *skb, bool last)
-{
-	size_t len = skb->len;
-
-	if (len > call->reply_max - call->reply_size) {
-		_leave(" = -EBADMSG [%zu > %u]",
-		       len, call->reply_max - call->reply_size);
-		return -EBADMSG;
-	}
-
-	if (len > 0) {
-		if (skb_copy_bits(skb, 0, call->buffer + call->reply_size,
-				  len) < 0)
-			BUG();
-		call->reply_size += len;
-	}
-
-	afs_data_consumed(call, skb);
-	if (!last)
-		return -EAGAIN;
-
-	if (call->reply_size != call->reply_max) {
-		_leave(" = -EBADMSG [%u != %u]",
-		       call->reply_size, call->reply_max);
-		return -EBADMSG;
-	}
-	return 0;
-}
-
 /*
  * accept the backlog of incoming calls
  */
@@ -727,14 +594,10 @@ static void afs_collect_incoming_call(struct work_struct *work)
 {
 	struct rxrpc_call *rxcall;
 	struct afs_call *call = NULL;
-	struct sk_buff *skb;
-
-	while ((skb = skb_dequeue(&afs_incoming_calls))) {
-		_debug("new call");
 
-		/* don't need the notification */
-		afs_free_skb(skb);
+	_enter("");
 
+	do {
 		if (!call) {
 			call = kzalloc(sizeof(struct afs_call), GFP_KERNEL);
 			if (!call) {
@@ -742,12 +605,10 @@ static void afs_collect_incoming_call(struct work_struct *work)
 				return;
 			}
 
-			call->async_workfn = afs_process_async_call;
-			INIT_WORK(&call->async_work, afs_async_workfn);
+			INIT_WORK(&call->async_work, afs_process_async_call);
 			call->wait_mode = &afs_async_incoming_call;
 			call->type = &afs_RXCMxxxx;
 			init_waitqueue_head(&call->waitq);
-			skb_queue_head_init(&call->rx_queue);
 			call->state = AFS_CALL_AWAIT_OP_ID;
 
 			_debug("CALL %p{%s} [%d]",
@@ -757,46 +618,47 @@ static void afs_collect_incoming_call(struct work_struct *work)
 		}
 
 		rxcall = rxrpc_kernel_accept_call(afs_socket,
-						  (unsigned long) call);
+						  (unsigned long)call,
+						  afs_wake_up_async_call);
 		if (!IS_ERR(rxcall)) {
 			call->rxcall = rxcall;
+			call->need_attention = true;
+			queue_work(afs_async_calls, &call->async_work);
 			call = NULL;
 		}
-	}
+	} while (!call);
 
 	if (call)
 		afs_free_call(call);
 }
 
+/*
+ * Notification of an incoming call.
+ */
+static void afs_rx_new_call(struct sock *sk)
+{
+	queue_work(afs_wq, &afs_collect_incoming_call_work);
+}
+
 /*
  * Grab the operation ID from an incoming cache manager call.  The socket
  * buffer is discarded on error or if we don't yet have sufficient data.
  */
-static int afs_deliver_cm_op_id(struct afs_call *call, struct sk_buff *skb,
-				bool last)
+static int afs_deliver_cm_op_id(struct afs_call *call)
 {
-	size_t len = skb->len;
-	void *oibuf = (void *) &call->operation_ID;
+	int ret;
 
-	_enter("{%u},{%zu},%d", call->offset, len, last);
+	_enter("{%zu}", call->offset);
 
 	ASSERTCMP(call->offset, <, 4);
 
 	/* the operation ID forms the first four bytes of the request data */
-	len = min_t(size_t, len, 4 - call->offset);
-	if (skb_copy_bits(skb, 0, oibuf + call->offset, len) < 0)
-		BUG();
-	if (!pskb_pull(skb, len))
-		BUG();
-	call->offset += len;
-
-	if (call->offset < 4) {
-		afs_data_consumed(call, skb);
-		_leave(" = -EAGAIN");
-		return -EAGAIN;
-	}
+	ret = afs_extract_data(call, &call->operation_ID, 4, true);
+	if (ret < 0)
+		return ret;
 
 	call->state = AFS_CALL_AWAIT_REQUEST;
+	call->offset = 0;
 
 	/* ask the cache manager to route the call (it'll change the call type
 	 * if successful) */
@@ -805,7 +667,7 @@ static int afs_deliver_cm_op_id(struct afs_call *call, struct sk_buff *skb,
 
 	/* pass responsibility for the remainer of this message off to the
 	 * cache manager op */
-	return call->type->deliver(call, skb, last);
+	return call->type->deliver(call);
 }
 
 /*
@@ -881,25 +743,40 @@ void afs_send_simple_reply(struct afs_call *call, const void *buf, size_t len)
 /*
  * Extract a piece of data from the received data socket buffers.
  */
-int afs_extract_data(struct afs_call *call, struct sk_buff *skb,
-		     bool last, void *buf, size_t count)
+int afs_extract_data(struct afs_call *call, void *buf, size_t count,
+		     bool want_more)
 {
-	size_t len = skb->len;
+	int ret;
 
-	_enter("{%u},{%zu},%d,,%zu", call->offset, len, last, count);
+	_enter("{%s,%zu},,%zu,%d",
+	       call->type->name, call->offset, count, want_more);
 
-	ASSERTCMP(call->offset, <, count);
+	ASSERTCMP(call->offset, <=, count);
 
-	len = min_t(size_t, len, count - call->offset);
-	if (skb_copy_bits(skb, 0, buf + call->offset, len) < 0 ||
-	    !pskb_pull(skb, len))
-		BUG();
-	call->offset += len;
+	ret = rxrpc_kernel_recv_data(afs_socket, call->rxcall,
+				     buf, count, &call->offset,
+				     want_more, &call->abort_code);
+	if (ret == 0 || ret == -EAGAIN)
+		return ret;
 
-	if (call->offset < count) {
-		afs_data_consumed(call, skb);
-		_leave(" = -EAGAIN");
-		return -EAGAIN;
+	if (ret == 1) {
+		switch (call->state) {
+		case AFS_CALL_AWAIT_REPLY:
+			call->state = AFS_CALL_COMPLETE;
+			break;
+		case AFS_CALL_AWAIT_REQUEST:
+			call->state = AFS_CALL_REPLYING;
+			break;
+		default:
+			break;
+		}
+		return 0;
 	}
-	return 0;
+
+	if (ret == -ECONNABORTED)
+		call->error = call->type->abort_to_error(call->abort_code);
+	else
+		call->error = ret;
+	call->state = AFS_CALL_COMPLETE;
+	return ret;
 }

commit 4de48af663d88d8c9a2550e60725f5a5c660970b
Author: David Howells <dhowells@redhat.com>
Date:   Tue Aug 30 12:00:48 2016 +0100

    rxrpc: Pass struct socket * to more rxrpc kernel interface functions
    
    Pass struct socket * to more rxrpc kernel interface functions.  They should
    be starting from this rather than the socket pointer in the rxrpc_call
    struct if they need to access the socket.
    
    I have left:
    
            rxrpc_kernel_is_data_last()
            rxrpc_kernel_get_abort_code()
            rxrpc_kernel_get_error_number()
            rxrpc_kernel_free_skb()
            rxrpc_kernel_data_consumed()
    
    unmodified as they're all about to be removed (and, in any case, don't
    touch the socket).
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index a1916750e2f9..7b0d18900f50 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -207,7 +207,7 @@ static void afs_free_call(struct afs_call *call)
 static void afs_end_call_nofree(struct afs_call *call)
 {
 	if (call->rxcall) {
-		rxrpc_kernel_end_call(call->rxcall);
+		rxrpc_kernel_end_call(afs_socket, call->rxcall);
 		call->rxcall = NULL;
 	}
 	if (call->type->destructor)
@@ -325,8 +325,8 @@ static int afs_send_pages(struct afs_call *call, struct msghdr *msg,
 			 * returns from sending the request */
 			if (first + loop >= last)
 				call->state = AFS_CALL_AWAIT_REPLY;
-			ret = rxrpc_kernel_send_data(call->rxcall, msg,
-						     to - offset);
+			ret = rxrpc_kernel_send_data(afs_socket, call->rxcall,
+						     msg, to - offset);
 			kunmap(pages[loop]);
 			if (ret < 0)
 				break;
@@ -406,7 +406,8 @@ int afs_make_call(struct in_addr *addr, struct afs_call *call, gfp_t gfp,
 	 * request */
 	if (!call->send_pages)
 		call->state = AFS_CALL_AWAIT_REPLY;
-	ret = rxrpc_kernel_send_data(rxcall, &msg, call->request_size);
+	ret = rxrpc_kernel_send_data(afs_socket, rxcall,
+				     &msg, call->request_size);
 	if (ret < 0)
 		goto error_do_abort;
 
@@ -421,7 +422,7 @@ int afs_make_call(struct in_addr *addr, struct afs_call *call, gfp_t gfp,
 	return wait_mode->wait(call);
 
 error_do_abort:
-	rxrpc_kernel_abort_call(rxcall, RX_USER_ABORT);
+	rxrpc_kernel_abort_call(afs_socket, rxcall, RX_USER_ABORT);
 	while ((skb = skb_dequeue(&call->rx_queue)))
 		afs_free_skb(skb);
 error_kill_call:
@@ -509,7 +510,8 @@ static void afs_deliver_to_call(struct afs_call *call)
 				if (call->state != AFS_CALL_AWAIT_REPLY)
 					abort_code = RXGEN_SS_UNMARSHAL;
 			do_abort:
-				rxrpc_kernel_abort_call(call->rxcall,
+				rxrpc_kernel_abort_call(afs_socket,
+							call->rxcall,
 							abort_code);
 				call->error = ret;
 				call->state = AFS_CALL_ERROR;
@@ -605,7 +607,7 @@ static int afs_wait_for_call_to_complete(struct afs_call *call)
 	/* kill the call */
 	if (call->state < AFS_CALL_COMPLETE) {
 		_debug("call incomplete");
-		rxrpc_kernel_abort_call(call->rxcall, RX_CALL_DEAD);
+		rxrpc_kernel_abort_call(afs_socket, call->rxcall, RX_CALL_DEAD);
 		while ((skb = skb_dequeue(&call->rx_queue)))
 			afs_free_skb(skb);
 	}
@@ -823,14 +825,15 @@ void afs_send_empty_reply(struct afs_call *call)
 	msg.msg_flags		= 0;
 
 	call->state = AFS_CALL_AWAIT_ACK;
-	switch (rxrpc_kernel_send_data(call->rxcall, &msg, 0)) {
+	switch (rxrpc_kernel_send_data(afs_socket, call->rxcall, &msg, 0)) {
 	case 0:
 		_leave(" [replied]");
 		return;
 
 	case -ENOMEM:
 		_debug("oom");
-		rxrpc_kernel_abort_call(call->rxcall, RX_USER_ABORT);
+		rxrpc_kernel_abort_call(afs_socket, call->rxcall,
+					RX_USER_ABORT);
 	default:
 		afs_end_call(call);
 		_leave(" [error]");
@@ -859,7 +862,7 @@ void afs_send_simple_reply(struct afs_call *call, const void *buf, size_t len)
 	msg.msg_flags		= 0;
 
 	call->state = AFS_CALL_AWAIT_ACK;
-	n = rxrpc_kernel_send_data(call->rxcall, &msg, len);
+	n = rxrpc_kernel_send_data(afs_socket, call->rxcall, &msg, len);
 	if (n >= 0) {
 		/* Success */
 		_leave(" [replied]");
@@ -868,7 +871,8 @@ void afs_send_simple_reply(struct afs_call *call, const void *buf, size_t len)
 
 	if (n == -ENOMEM) {
 		_debug("oom");
-		rxrpc_kernel_abort_call(call->rxcall, RX_USER_ABORT);
+		rxrpc_kernel_abort_call(afs_socket, call->rxcall,
+					RX_USER_ABORT);
 	}
 	afs_end_call(call);
 	_leave(" [error]");

commit 8324f0bcfbfc645cf248e4b93ab58341b7d3b135
Author: David Howells <dhowells@redhat.com>
Date:   Tue Aug 30 09:49:29 2016 +0100

    rxrpc: Provide a way for AFS to ask for the peer address of a call
    
    Provide a function so that kernel users, such as AFS, can ask for the peer
    address of a call:
    
       void rxrpc_kernel_get_peer(struct rxrpc_call *call,
                                  struct sockaddr_rxrpc *_srx);
    
    In the future the kernel service won't get sk_buffs to look inside.
    Further, this allows us to hide any canonicalisation inside AF_RXRPC for
    when IPv6 support is added.
    
    Also propagate this through to afs_find_server() and issue a warning if we
    can't handle the address family yet.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 14d04c848465..a1916750e2f9 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -16,7 +16,7 @@
 #include "internal.h"
 #include "afs_cm.h"
 
-static struct socket *afs_socket; /* my RxRPC socket */
+struct socket *afs_socket; /* my RxRPC socket */
 static struct workqueue_struct *afs_async_calls;
 static atomic_t afs_outstanding_calls;
 static atomic_t afs_outstanding_skbs;

commit 372ee16386bbf6dc5eeb0387e1ede963debba82a
Author: David Howells <dhowells@redhat.com>
Date:   Wed Aug 3 14:11:40 2016 +0100

    rxrpc: Fix races between skb free, ACK generation and replying
    
    Inside the kafs filesystem it is possible to occasionally have a call
    processed and terminated before we've had a chance to check whether we need
    to clean up the rx queue for that call because afs_send_simple_reply() ends
    the call when it is done, but this is done in a workqueue item that might
    happen to run to completion before afs_deliver_to_call() completes.
    
    Further, it is possible for rxrpc_kernel_send_data() to be called to send a
    reply before the last request-phase data skb is released.  The rxrpc skb
    destructor is where the ACK processing is done and the call state is
    advanced upon release of the last skb.  ACK generation is also deferred to
    a work item because it's possible that the skb destructor is not called in
    a context where kernel_sendmsg() can be invoked.
    
    To this end, the following changes are made:
    
     (1) kernel_rxrpc_data_consumed() is added.  This should be called whenever
         an skb is emptied so as to crank the ACK and call states.  This does
         not release the skb, however.  kernel_rxrpc_free_skb() must now be
         called to achieve that.  These together replace
         rxrpc_kernel_data_delivered().
    
     (2) kernel_rxrpc_data_consumed() is wrapped by afs_data_consumed().
    
         This makes afs_deliver_to_call() easier to work as the skb can simply
         be discarded unconditionally here without trying to work out what the
         return value of the ->deliver() function means.
    
         The ->deliver() functions can, via afs_data_complete(),
         afs_transfer_reply() and afs_extract_data() mark that an skb has been
         consumed (thereby cranking the state) without the need to
         conditionally free the skb to make sure the state is correct on an
         incoming call for when the call processor tries to send the reply.
    
     (3) rxrpc_recvmsg() now has to call kernel_rxrpc_data_consumed() when it
         has finished with a packet and MSG_PEEK isn't set.
    
     (4) rxrpc_packet_destructor() no longer calls rxrpc_hard_ACK_data().
    
         Because of this, we no longer need to clear the destructor and put the
         call before we free the skb in cases where we don't want the ACK/call
         state to be cranked.
    
     (5) The ->deliver() call-type callbacks are made to return -EAGAIN rather
         than 0 if they expect more data (afs_extract_data() returns -EAGAIN to
         the delivery function already), and the caller is now responsible for
         producing an abort if that was the last packet.
    
     (6) There are many bits of unmarshalling code where:
    
                    ret = afs_extract_data(call, skb, last, ...);
                    switch (ret) {
                    case 0:         break;
                    case -EAGAIN:   return 0;
                    default:        return ret;
                    }
    
         is to be found.  As -EAGAIN can now be passed back to the caller, we
         now just return if ret < 0:
    
                    ret = afs_extract_data(call, skb, last, ...);
                    if (ret < 0)
                            return ret;
    
     (7) Checks for trailing data and empty final data packets has been
         consolidated as afs_data_complete().  So:
    
                    if (skb->len > 0)
                            return -EBADMSG;
                    if (!last)
                            return 0;
    
         becomes:
    
                    ret = afs_data_complete(call, skb, last);
                    if (ret < 0)
                            return ret;
    
     (8) afs_transfer_reply() now checks the amount of data it has against the
         amount of data desired and the amount of data in the skb and returns
         an error to induce an abort if we don't get exactly what we want.
    
    Without these changes, the following oops can occasionally be observed,
    particularly if some printks are inserted into the delivery path:
    
    general protection fault: 0000 [#1] SMP
    Modules linked in: kafs(E) af_rxrpc(E) [last unloaded: af_rxrpc]
    CPU: 0 PID: 1305 Comm: kworker/u8:3 Tainted: G            E   4.7.0-fsdevel+ #1303
    Hardware name: ASUS All Series/H97-PLUS, BIOS 2306 10/09/2014
    Workqueue: kafsd afs_async_workfn [kafs]
    task: ffff88040be041c0 ti: ffff88040c070000 task.ti: ffff88040c070000
    RIP: 0010:[<ffffffff8108fd3c>]  [<ffffffff8108fd3c>] __lock_acquire+0xcf/0x15a1
    RSP: 0018:ffff88040c073bc0  EFLAGS: 00010002
    RAX: 6b6b6b6b6b6b6b6b RBX: 0000000000000000 RCX: ffff88040d29a710
    RDX: 0000000000000000 RSI: 0000000000000000 RDI: ffff88040d29a710
    RBP: ffff88040c073c70 R08: 0000000000000001 R09: 0000000000000001
    R10: 0000000000000001 R11: 0000000000000000 R12: 0000000000000000
    R13: 0000000000000000 R14: ffff88040be041c0 R15: ffffffff814c928f
    FS:  0000000000000000(0000) GS:ffff88041fa00000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 00007fa4595f4750 CR3: 0000000001c14000 CR4: 00000000001406f0
    Stack:
     0000000000000006 000000000be04930 0000000000000000 ffff880400000000
     ffff880400000000 ffffffff8108f847 ffff88040be041c0 ffffffff81050446
     ffff8803fc08a920 ffff8803fc08a958 ffff88040be041c0 ffff88040c073c38
    Call Trace:
     [<ffffffff8108f847>] ? mark_held_locks+0x5e/0x74
     [<ffffffff81050446>] ? __local_bh_enable_ip+0x9b/0xa1
     [<ffffffff8108f9ca>] ? trace_hardirqs_on_caller+0x16d/0x189
     [<ffffffff810915f4>] lock_acquire+0x122/0x1b6
     [<ffffffff810915f4>] ? lock_acquire+0x122/0x1b6
     [<ffffffff814c928f>] ? skb_dequeue+0x18/0x61
     [<ffffffff81609dbf>] _raw_spin_lock_irqsave+0x35/0x49
     [<ffffffff814c928f>] ? skb_dequeue+0x18/0x61
     [<ffffffff814c928f>] skb_dequeue+0x18/0x61
     [<ffffffffa009aa92>] afs_deliver_to_call+0x344/0x39d [kafs]
     [<ffffffffa009ab37>] afs_process_async_call+0x4c/0xd5 [kafs]
     [<ffffffffa0099e9c>] afs_async_workfn+0xe/0x10 [kafs]
     [<ffffffff81063a3a>] process_one_work+0x29d/0x57c
     [<ffffffff81064ac2>] worker_thread+0x24a/0x385
     [<ffffffff81064878>] ? rescuer_thread+0x2d0/0x2d0
     [<ffffffff810696f5>] kthread+0xf3/0xfb
     [<ffffffff8160a6ff>] ret_from_fork+0x1f/0x40
     [<ffffffff81069602>] ? kthread_create_on_node+0x1cf/0x1cf
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 4832de84d52c..14d04c848465 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -150,10 +150,9 @@ void afs_close_socket(void)
 }
 
 /*
- * note that the data in a socket buffer is now delivered and that the buffer
- * should be freed
+ * Note that the data in a socket buffer is now consumed.
  */
-static void afs_data_delivered(struct sk_buff *skb)
+void afs_data_consumed(struct afs_call *call, struct sk_buff *skb)
 {
 	if (!skb) {
 		_debug("DLVR NULL [%d]", atomic_read(&afs_outstanding_skbs));
@@ -161,9 +160,7 @@ static void afs_data_delivered(struct sk_buff *skb)
 	} else {
 		_debug("DLVR %p{%u} [%d]",
 		       skb, skb->mark, atomic_read(&afs_outstanding_skbs));
-		if (atomic_dec_return(&afs_outstanding_skbs) == -1)
-			BUG();
-		rxrpc_kernel_data_delivered(skb);
+		rxrpc_kernel_data_consumed(call->rxcall, skb);
 	}
 }
 
@@ -489,9 +486,15 @@ static void afs_deliver_to_call(struct afs_call *call)
 			last = rxrpc_kernel_is_data_last(skb);
 			ret = call->type->deliver(call, skb, last);
 			switch (ret) {
+			case -EAGAIN:
+				if (last) {
+					_debug("short data");
+					goto unmarshal_error;
+				}
+				break;
 			case 0:
-				if (last &&
-				    call->state == AFS_CALL_AWAIT_REPLY)
+				ASSERT(last);
+				if (call->state == AFS_CALL_AWAIT_REPLY)
 					call->state = AFS_CALL_COMPLETE;
 				break;
 			case -ENOTCONN:
@@ -501,6 +504,7 @@ static void afs_deliver_to_call(struct afs_call *call)
 				abort_code = RX_INVALID_OPERATION;
 				goto do_abort;
 			default:
+			unmarshal_error:
 				abort_code = RXGEN_CC_UNMARSHAL;
 				if (call->state != AFS_CALL_AWAIT_REPLY)
 					abort_code = RXGEN_SS_UNMARSHAL;
@@ -511,9 +515,7 @@ static void afs_deliver_to_call(struct afs_call *call)
 				call->state = AFS_CALL_ERROR;
 				break;
 			}
-			afs_data_delivered(skb);
-			skb = NULL;
-			continue;
+			break;
 		case RXRPC_SKB_MARK_FINAL_ACK:
 			_debug("Rcv ACK");
 			call->state = AFS_CALL_COMPLETE;
@@ -685,15 +687,35 @@ static void afs_process_async_call(struct afs_call *call)
 }
 
 /*
- * empty a socket buffer into a flat reply buffer
+ * Empty a socket buffer into a flat reply buffer.
  */
-void afs_transfer_reply(struct afs_call *call, struct sk_buff *skb)
+int afs_transfer_reply(struct afs_call *call, struct sk_buff *skb, bool last)
 {
 	size_t len = skb->len;
 
-	if (skb_copy_bits(skb, 0, call->buffer + call->reply_size, len) < 0)
-		BUG();
-	call->reply_size += len;
+	if (len > call->reply_max - call->reply_size) {
+		_leave(" = -EBADMSG [%zu > %u]",
+		       len, call->reply_max - call->reply_size);
+		return -EBADMSG;
+	}
+
+	if (len > 0) {
+		if (skb_copy_bits(skb, 0, call->buffer + call->reply_size,
+				  len) < 0)
+			BUG();
+		call->reply_size += len;
+	}
+
+	afs_data_consumed(call, skb);
+	if (!last)
+		return -EAGAIN;
+
+	if (call->reply_size != call->reply_max) {
+		_leave(" = -EBADMSG [%u != %u]",
+		       call->reply_size, call->reply_max);
+		return -EBADMSG;
+	}
+	return 0;
 }
 
 /*
@@ -745,7 +767,8 @@ static void afs_collect_incoming_call(struct work_struct *work)
 }
 
 /*
- * grab the operation ID from an incoming cache manager call
+ * Grab the operation ID from an incoming cache manager call.  The socket
+ * buffer is discarded on error or if we don't yet have sufficient data.
  */
 static int afs_deliver_cm_op_id(struct afs_call *call, struct sk_buff *skb,
 				bool last)
@@ -766,12 +789,9 @@ static int afs_deliver_cm_op_id(struct afs_call *call, struct sk_buff *skb,
 	call->offset += len;
 
 	if (call->offset < 4) {
-		if (last) {
-			_leave(" = -EBADMSG [op ID short]");
-			return -EBADMSG;
-		}
-		_leave(" = 0 [incomplete]");
-		return 0;
+		afs_data_consumed(call, skb);
+		_leave(" = -EAGAIN");
+		return -EAGAIN;
 	}
 
 	call->state = AFS_CALL_AWAIT_REQUEST;
@@ -855,7 +875,7 @@ void afs_send_simple_reply(struct afs_call *call, const void *buf, size_t len)
 }
 
 /*
- * extract a piece of data from the received data socket buffers
+ * Extract a piece of data from the received data socket buffers.
  */
 int afs_extract_data(struct afs_call *call, struct sk_buff *skb,
 		     bool last, void *buf, size_t count)
@@ -873,10 +893,7 @@ int afs_extract_data(struct afs_call *call, struct sk_buff *skb,
 	call->offset += len;
 
 	if (call->offset < count) {
-		if (last) {
-			_leave(" = -EBADMSG [%d < %zu]", call->offset, count);
-			return -EBADMSG;
-		}
+		afs_data_consumed(call, skb);
 		_leave(" = -EAGAIN");
 		return -EAGAIN;
 	}

commit 0e119b41b7f23e08799fa8b1c9c1360d7da75815
Author: David Howells <dhowells@redhat.com>
Date:   Fri Jun 10 22:30:37 2016 +0100

    rxrpc: Limit the listening backlog
    
    Limit the socket incoming call backlog queue size so that a remote client
    can't pump in sufficient new calls that the server runs out of memory.  Note
    that this is partially theoretical at the moment since whilst the number of
    calls is limited, the number of packets trying to set up new calls is not.
    This will be addressed in a later patch.
    
    If the caller of listen() specifies a backlog INT_MAX, then they get the
    current maximum; anything else greater than max_backlog or anything
    negative incurs EINVAL.
    
    The limit on the maximum queue size can be set by:
    
            echo N >/proc/sys/net/rxrpc/max_backlog
    
    where 4<=N<=32.
    
    Further, set the default backlog to 0, requiring listen() to be called
    before we start actually queueing new calls.  Whilst this kind of is a
    change in the UAPI, the caller can't actually *accept* new calls anyway
    unless they've first called listen() to put the socket into the LISTENING
    state - thus the aforementioned new calls would otherwise just sit there,
    eating up kernel memory.  (Note that sockets that don't have a non-zero
    service ID bound don't get incoming calls anyway.)
    
    Given that the default backlog is now 0, make the AFS filesystem call
    kernel_listen() to set the maximum backlog for itself.
    
    Possible improvements include:
    
     (1) Trimming a too-large backlog to max_backlog when listen is called.
    
     (2) Trimming the backlog value whenever the value is used so that changes
         to max_backlog are applied to an open socket automatically.  Note that
         the AFS filesystem opens one socket and keeps it open for extended
         periods, so would miss out on changes to max_backlog.
    
     (3) Having a separate setting for the AFS filesystem.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 63cd9f939f19..4832de84d52c 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -85,18 +85,14 @@ int afs_open_socket(void)
 
 	skb_queue_head_init(&afs_incoming_calls);
 
+	ret = -ENOMEM;
 	afs_async_calls = create_singlethread_workqueue("kafsd");
-	if (!afs_async_calls) {
-		_leave(" = -ENOMEM [wq]");
-		return -ENOMEM;
-	}
+	if (!afs_async_calls)
+		goto error_0;
 
 	ret = sock_create_kern(&init_net, AF_RXRPC, SOCK_DGRAM, PF_INET, &socket);
-	if (ret < 0) {
-		destroy_workqueue(afs_async_calls);
-		_leave(" = %d [socket]", ret);
-		return ret;
-	}
+	if (ret < 0)
+		goto error_1;
 
 	socket->sk->sk_allocation = GFP_NOFS;
 
@@ -111,18 +107,26 @@ int afs_open_socket(void)
 	       sizeof(srx.transport.sin.sin_addr));
 
 	ret = kernel_bind(socket, (struct sockaddr *) &srx, sizeof(srx));
-	if (ret < 0) {
-		sock_release(socket);
-		destroy_workqueue(afs_async_calls);
-		_leave(" = %d [bind]", ret);
-		return ret;
-	}
+	if (ret < 0)
+		goto error_2;
+
+	ret = kernel_listen(socket, INT_MAX);
+	if (ret < 0)
+		goto error_2;
 
 	rxrpc_kernel_intercept_rx_messages(socket, afs_rx_interceptor);
 
 	afs_socket = socket;
 	_leave(" = 0");
 	return 0;
+
+error_2:
+	sock_release(socket);
+error_1:
+	destroy_workqueue(afs_async_calls);
+error_0:
+	_leave(" = %d", ret);
+	return ret;
 }
 
 /*

commit dc44b3a09aec9ac57c1e7410677c87c0e6453624
Author: David Howells <dhowells@redhat.com>
Date:   Thu Apr 7 17:23:30 2016 +0100

    rxrpc: Differentiate local and remote abort codes in structs
    
    In the rxrpc_connection and rxrpc_call structs, there's one field to hold
    the abort code, no matter whether that value was generated locally to be
    sent or was received from the peer via an abort packet.
    
    Split the abort code fields in two for cleanliness sake and add an error
    field to hold the Linux error number to the rxrpc_call struct too
    (sometimes this is generated in a context where we can't return it to
    userspace directly).
    
    Furthermore, add a skb mark to indicate a packet that caused a local abort
    to be generated so that recvmsg() can pick up the correct abort code.  A
    future addition will need to be to indicate to userspace the difference
    between aborts via a control message.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index b4d337ad6e36..63cd9f939f19 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -430,9 +430,11 @@ int afs_make_call(struct in_addr *addr, struct afs_call *call, gfp_t gfp,
 }
 
 /*
- * handles intercepted messages that were arriving in the socket's Rx queue
- * - called with the socket receive queue lock held to ensure message ordering
- * - called with softirqs disabled
+ * Handles intercepted messages that were arriving in the socket's Rx queue.
+ *
+ * Called from the AF_RXRPC call processor in waitqueue process context.  For
+ * each call, it is guaranteed this will be called in order of packet to be
+ * delivered.
  */
 static void afs_rx_interceptor(struct sock *sk, unsigned long user_call_ID,
 			       struct sk_buff *skb)
@@ -523,6 +525,12 @@ static void afs_deliver_to_call(struct afs_call *call)
 			call->state = AFS_CALL_ABORTED;
 			_debug("Rcv ABORT %u -> %d", abort_code, call->error);
 			break;
+		case RXRPC_SKB_MARK_LOCAL_ABORT:
+			abort_code = rxrpc_kernel_get_abort_code(skb);
+			call->error = call->type->abort_to_error(abort_code);
+			call->state = AFS_CALL_ABORTED;
+			_debug("Loc ABORT %u -> %d", abort_code, call->error);
+			break;
 		case RXRPC_SKB_MARK_NET_ERROR:
 			call->error = -rxrpc_kernel_get_error_number(skb);
 			call->state = AFS_CALL_ERROR;

commit 2f02f7aea7b6c9a9312846c006e076ae6ad026a4
Author: David Howells <dhowells@redhat.com>
Date:   Thu Apr 7 17:23:03 2016 +0100

    afs: Wait for outstanding async calls before closing rxrpc socket
    
    The afs filesystem needs to wait for any outstanding asynchronous calls
    (such as FS.GiveUpCallBacks cleaning up the callbacks lodged with a server)
    to complete before closing the AF_RXRPC socket when unloading the module.
    
    This may occur if the module is removed too quickly after unmounting all
    filesystems.  This will produce an error report that looks like:
    
            AFS: Assertion failed
            1 == 0 is false
            0x1 == 0x0 is false
            ------------[ cut here ]------------
            kernel BUG at ../fs/afs/rxrpc.c:135!
            ...
            RIP: 0010:[<ffffffffa004111c>] afs_close_socket+0xec/0x107 [kafs]
            ...
            Call Trace:
             [<ffffffffa004a160>] afs_exit+0x1f/0x57 [kafs]
             [<ffffffff810c30a0>] SyS_delete_module+0xec/0x17d
             [<ffffffff81610417>] entry_SYSCALL_64_fastpath+0x12/0x6b
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index b50642870a43..b4d337ad6e36 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -65,6 +65,12 @@ static void afs_async_workfn(struct work_struct *work)
 	call->async_workfn(call);
 }
 
+static int afs_wait_atomic_t(atomic_t *p)
+{
+	schedule();
+	return 0;
+}
+
 /*
  * open an RxRPC socket and bind it to be a server for callback notifications
  * - the socket is left in blocking mode and non-blocking ops use MSG_DONTWAIT
@@ -126,13 +132,16 @@ void afs_close_socket(void)
 {
 	_enter("");
 
+	wait_on_atomic_t(&afs_outstanding_calls, afs_wait_atomic_t,
+			 TASK_UNINTERRUPTIBLE);
+	_debug("no outstanding calls");
+
 	sock_release(afs_socket);
 
 	_debug("dework");
 	destroy_workqueue(afs_async_calls);
 
 	ASSERTCMP(atomic_read(&afs_outstanding_skbs), ==, 0);
-	ASSERTCMP(atomic_read(&afs_outstanding_calls), ==, 0);
 	_leave("");
 }
 
@@ -178,8 +187,6 @@ static void afs_free_call(struct afs_call *call)
 {
 	_debug("DONE %p{%s} [%d]",
 	       call, call->type->name, atomic_read(&afs_outstanding_calls));
-	if (atomic_dec_return(&afs_outstanding_calls) == -1)
-		BUG();
 
 	ASSERTCMP(call->rxcall, ==, NULL);
 	ASSERT(!work_pending(&call->async_work));
@@ -188,6 +195,9 @@ static void afs_free_call(struct afs_call *call)
 
 	kfree(call->request);
 	kfree(call);
+
+	if (atomic_dec_and_test(&afs_outstanding_calls))
+		wake_up_atomic_t(&afs_outstanding_calls);
 }
 
 /*

commit eeb1bd5c40edb0e2fd925c8535e2fdebdbc5cef2
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Fri May 8 21:08:05 2015 -0500

    net: Add a struct net parameter to sock_create_kern
    
    This is long overdue, and is part of cleaning up how we allocate kernel
    sockets that don't reference count struct net.
    
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 3a57a1b0fb51..b50642870a43 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -85,7 +85,7 @@ int afs_open_socket(void)
 		return -ENOMEM;
 	}
 
-	ret = sock_create_kern(AF_RXRPC, SOCK_DGRAM, PF_INET, &socket);
+	ret = sock_create_kern(&init_net, AF_RXRPC, SOCK_DGRAM, PF_INET, &socket);
 	if (ret < 0) {
 		destroy_workqueue(afs_async_calls);
 		_leave(" = %d [socket]", ret);

commit bfd4e9562c8769feadba9b5262200448656b8048
Author: David Howells <dhowells@redhat.com>
Date:   Wed Apr 1 16:03:46 2015 +0100

    AFS: afs_send_empty_reply() doesn't require an iovec array
    
    afs_send_empty_reply() doesn't require an iovec array with which to initialise
    the msghdr, but can pass NULL instead.
    
    Suggested-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index dbc732e9a5c0..3a57a1b0fb51 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -770,15 +770,12 @@ static int afs_deliver_cm_op_id(struct afs_call *call, struct sk_buff *skb,
 void afs_send_empty_reply(struct afs_call *call)
 {
 	struct msghdr msg;
-	struct kvec iov[1];
 
 	_enter("");
 
-	iov[0].iov_base		= NULL;
-	iov[0].iov_len		= 0;
 	msg.msg_name		= NULL;
 	msg.msg_namelen		= 0;
-	iov_iter_kvec(&msg.msg_iter, WRITE | ITER_KVEC, iov, 0, 0);	/* WTF? */
+	iov_iter_kvec(&msg.msg_iter, WRITE | ITER_KVEC, NULL, 0, 0);
 	msg.msg_control		= NULL;
 	msg.msg_controllen	= 0;
 	msg.msg_flags		= 0;

commit 2e90b1c45e34240eeeacab0b37d5f8f739462bdc
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Thu Nov 27 21:50:31 2014 -0500

    rxrpc: make the users of rxrpc_kernel_send_data() set kvec-backed msg_iter properly
    
    Use iov_iter_kvec() there, get rid of set_fs() games - now that
    rxrpc_send_data() uses iov_iter primitives, it'll handle ITER_KVEC just
    fine.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 06e14bfb3496..dbc732e9a5c0 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -306,8 +306,8 @@ static int afs_send_pages(struct afs_call *call, struct msghdr *msg,
 
 			_debug("- range %u-%u%s",
 			       offset, to, msg->msg_flags ? " [more]" : "");
-			iov_iter_init(&msg->msg_iter, WRITE,
-				      (struct iovec *) iov, 1, to - offset);
+			iov_iter_kvec(&msg->msg_iter, WRITE | ITER_KVEC,
+				      iov, 1, to - offset);
 
 			/* have to change the state *before* sending the last
 			 * packet as RxRPC might give us the reply before it
@@ -384,7 +384,7 @@ int afs_make_call(struct in_addr *addr, struct afs_call *call, gfp_t gfp,
 
 	msg.msg_name		= NULL;
 	msg.msg_namelen		= 0;
-	iov_iter_init(&msg.msg_iter, WRITE, (struct iovec *)iov, 1,
+	iov_iter_kvec(&msg.msg_iter, WRITE | ITER_KVEC, iov, 1,
 		      call->request_size);
 	msg.msg_control		= NULL;
 	msg.msg_controllen	= 0;
@@ -770,7 +770,7 @@ static int afs_deliver_cm_op_id(struct afs_call *call, struct sk_buff *skb,
 void afs_send_empty_reply(struct afs_call *call)
 {
 	struct msghdr msg;
-	struct iovec iov[1];
+	struct kvec iov[1];
 
 	_enter("");
 
@@ -778,7 +778,7 @@ void afs_send_empty_reply(struct afs_call *call)
 	iov[0].iov_len		= 0;
 	msg.msg_name		= NULL;
 	msg.msg_namelen		= 0;
-	iov_iter_init(&msg.msg_iter, WRITE, iov, 0, 0);	/* WTF? */
+	iov_iter_kvec(&msg.msg_iter, WRITE | ITER_KVEC, iov, 0, 0);	/* WTF? */
 	msg.msg_control		= NULL;
 	msg.msg_controllen	= 0;
 	msg.msg_flags		= 0;
@@ -805,7 +805,7 @@ void afs_send_empty_reply(struct afs_call *call)
 void afs_send_simple_reply(struct afs_call *call, const void *buf, size_t len)
 {
 	struct msghdr msg;
-	struct iovec iov[1];
+	struct kvec iov[1];
 	int n;
 
 	_enter("");
@@ -814,7 +814,7 @@ void afs_send_simple_reply(struct afs_call *call, const void *buf, size_t len)
 	iov[0].iov_len		= len;
 	msg.msg_name		= NULL;
 	msg.msg_namelen		= 0;
-	iov_iter_init(&msg.msg_iter, WRITE, iov, 1, len);
+	iov_iter_kvec(&msg.msg_iter, WRITE | ITER_KVEC, iov, 1, len);
 	msg.msg_control		= NULL;
 	msg.msg_controllen	= 0;
 	msg.msg_flags		= 0;

commit c0371da6047abd261bc483c744dbc7d81a116172
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Mon Nov 24 10:42:55 2014 -0500

    put iov_iter into msghdr
    
    Note that the code _using_ ->msg_iter at that point will be very
    unhappy with anything other than unshifted iovec-backed iov_iter.
    We still need to convert users to proper primitives.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 03a3beb17004..06e14bfb3496 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -306,8 +306,8 @@ static int afs_send_pages(struct afs_call *call, struct msghdr *msg,
 
 			_debug("- range %u-%u%s",
 			       offset, to, msg->msg_flags ? " [more]" : "");
-			msg->msg_iov = (struct iovec *) iov;
-			msg->msg_iovlen = 1;
+			iov_iter_init(&msg->msg_iter, WRITE,
+				      (struct iovec *) iov, 1, to - offset);
 
 			/* have to change the state *before* sending the last
 			 * packet as RxRPC might give us the reply before it
@@ -384,8 +384,8 @@ int afs_make_call(struct in_addr *addr, struct afs_call *call, gfp_t gfp,
 
 	msg.msg_name		= NULL;
 	msg.msg_namelen		= 0;
-	msg.msg_iov		= (struct iovec *) iov;
-	msg.msg_iovlen		= 1;
+	iov_iter_init(&msg.msg_iter, WRITE, (struct iovec *)iov, 1,
+		      call->request_size);
 	msg.msg_control		= NULL;
 	msg.msg_controllen	= 0;
 	msg.msg_flags		= (call->send_pages ? MSG_MORE : 0);
@@ -778,8 +778,7 @@ void afs_send_empty_reply(struct afs_call *call)
 	iov[0].iov_len		= 0;
 	msg.msg_name		= NULL;
 	msg.msg_namelen		= 0;
-	msg.msg_iov		= iov;
-	msg.msg_iovlen		= 0;
+	iov_iter_init(&msg.msg_iter, WRITE, iov, 0, 0);	/* WTF? */
 	msg.msg_control		= NULL;
 	msg.msg_controllen	= 0;
 	msg.msg_flags		= 0;
@@ -815,8 +814,7 @@ void afs_send_simple_reply(struct afs_call *call, const void *buf, size_t len)
 	iov[0].iov_len		= len;
 	msg.msg_name		= NULL;
 	msg.msg_namelen		= 0;
-	msg.msg_iov		= iov;
-	msg.msg_iovlen		= 1;
+	iov_iter_init(&msg.msg_iter, WRITE, iov, 1, len);
 	msg.msg_control		= NULL;
 	msg.msg_controllen	= 0;
 	msg.msg_flags		= 0;

commit 656f88ddf1ec3abf2cd20b8b4028c44e8e95f56d
Author: David Howells <dhowells@redhat.com>
Date:   Wed May 21 15:55:26 2014 +0100

    AFS: Pass an afs_call* to call->async_workfn() instead of a work_struct*
    
    call->async_workfn() can take an afs_call* arg rather than a work_struct* as
    the functions assigned there are now called from afs_async_workfn() which has
    to call container_of() anyway.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Reviewed-by: Nathaniel Wesley Filardo <nwf@cs.jhu.edu>
    Reviewed-by: Tejun Heo <tj@kernel.org>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 5a05014ea7b0..03a3beb17004 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -25,7 +25,7 @@ static void afs_wake_up_call_waiter(struct afs_call *);
 static int afs_wait_for_call_to_complete(struct afs_call *);
 static void afs_wake_up_async_call(struct afs_call *);
 static int afs_dont_wait_for_call_to_complete(struct afs_call *);
-static void afs_process_async_call(struct work_struct *);
+static void afs_process_async_call(struct afs_call *);
 static void afs_rx_interceptor(struct sock *, unsigned long, struct sk_buff *);
 static int afs_deliver_cm_op_id(struct afs_call *, struct sk_buff *, bool);
 
@@ -62,7 +62,7 @@ static void afs_async_workfn(struct work_struct *work)
 {
 	struct afs_call *call = container_of(work, struct afs_call, async_work);
 
-	call->async_workfn(work);
+	call->async_workfn(call);
 }
 
 /*
@@ -623,11 +623,8 @@ static int afs_dont_wait_for_call_to_complete(struct afs_call *call)
 /*
  * delete an asynchronous call
  */
-static void afs_delete_async_call(struct work_struct *work)
+static void afs_delete_async_call(struct afs_call *call)
 {
-	struct afs_call *call =
-		container_of(work, struct afs_call, async_work);
-
 	_enter("");
 
 	afs_free_call(call);
@@ -640,11 +637,8 @@ static void afs_delete_async_call(struct work_struct *work)
  * - on a multiple-thread workqueue this work item may try to run on several
  *   CPUs at the same time
  */
-static void afs_process_async_call(struct work_struct *work)
+static void afs_process_async_call(struct afs_call *call)
 {
-	struct afs_call *call =
-		container_of(work, struct afs_call, async_work);
-
 	_enter("");
 
 	if (!skb_queue_empty(&call->rx_queue))

commit 150a6b478982475c60fa25b7060ab990ece5483d
Author: Nathaniel Wesley Filardo <nwf@cs.jhu.edu>
Date:   Wed May 21 14:58:26 2014 +0100

    AFS: Fix kafs module unloading
    
    At present, it is not possible to successfully unload the kafs module if there
    are outstanding async outgoing calls (those made with afs_make_call()).  This
    appears to be due to the changes introduced by:
    
            commit 059499453a9abd1857d442b44da8b4c126dc72a8
            Author: Tejun Heo <tj@kernel.org>
            Date:   Fri Mar 7 10:24:50 2014 -0500
            Subject: afs: don't use PREPARE_WORK
    
    which didn't go far enough.  The problem is due to:
    
     (1) The aforementioned commit introduced a separate handler function pointer
         in the call, call->async_workfn, in addition to the original workqueue
         item, call->async_work, for asynchronous operations because workqueues
         subsystem cannot handle the workqueue item pointer being changed whilst
         the item is queued or being processed.
    
     (2) afs_async_workfn() was introduced in that commit to be the callback for
         call->async_work.  Its sole purpose is to run whatever call->async_workfn
         points to.
    
     (3) call->async_workfn is only used from afs_async_workfn(), which is only
         set on async_work by afs_collect_incoming_call() - ie. for incoming
         calls.
    
     (4) call->async_workfn is *not* set by afs_make_call() when outgoing calls are
         made, and call->async_work is set afs_process_async_call() - and not
         afs_async_workfn().
    
     (5) afs_process_async_call() now changes call->async_workfn rather than
         call->async_work to point to afs_delete_async_call() to clean up, but this
         is only effective for incoming calls because call->async_work does not
         point to afs_async_workfn() for outgoing calls.
    
     (6) Because, for incoming calls, call->async_work remains pointing to
         afs_process_async_call() this results in an infinite loop.
    
    Instead, make the workqueue uniformly vector through call->async_workfn, via
    afs_async_workfn() and simply initialise call->async_workfn to point to
    afs_process_async_call() in afs_make_call().
    
    Signed-off-by: Nathaniel Wesley Filardo <nwf@cs.jhu.edu>
    Signed-off-by: David Howells <dhowells@redhat.com>
    Reviewed-by: Tejun Heo <tj@kernel.org>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 1a1110b1a7ff..5a05014ea7b0 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -58,6 +58,13 @@ static void afs_collect_incoming_call(struct work_struct *);
 static struct sk_buff_head afs_incoming_calls;
 static DECLARE_WORK(afs_collect_incoming_call_work, afs_collect_incoming_call);
 
+static void afs_async_workfn(struct work_struct *work)
+{
+	struct afs_call *call = container_of(work, struct afs_call, async_work);
+
+	call->async_workfn(work);
+}
+
 /*
  * open an RxRPC socket and bind it to be a server for callback notifications
  * - the socket is left in blocking mode and non-blocking ops use MSG_DONTWAIT
@@ -348,7 +355,8 @@ int afs_make_call(struct in_addr *addr, struct afs_call *call, gfp_t gfp,
 	       atomic_read(&afs_outstanding_calls));
 
 	call->wait_mode = wait_mode;
-	INIT_WORK(&call->async_work, afs_process_async_call);
+	call->async_workfn = afs_process_async_call;
+	INIT_WORK(&call->async_work, afs_async_workfn);
 
 	memset(&srx, 0, sizeof(srx));
 	srx.srx_family = AF_RXRPC;
@@ -672,13 +680,6 @@ void afs_transfer_reply(struct afs_call *call, struct sk_buff *skb)
 	call->reply_size += len;
 }
 
-static void afs_async_workfn(struct work_struct *work)
-{
-	struct afs_call *call = container_of(work, struct afs_call, async_work);
-
-	call->async_workfn(work);
-}
-
 /*
  * accept the backlog of incoming calls
  */

commit 6cf12869f5c1a837f18af5f8b2308fa243772735
Author: Nathaniel Wesley Filardo <nwf@cs.jhu.edu>
Date:   Wed May 21 16:04:11 2014 +0100

    AFS: Part of afs_end_call() is identical to code elsewhere, so split it
    
    Split afs_end_call() into two pieces, one of which is identical to code in
    afs_process_async_call().  Replace the latter with a call to the first part of
    afs_end_call().
    
    Signed-off-by: Nathaniel Wesley Filardo <nwf@cs.jhu.edu>
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 9226a6674d7f..1a1110b1a7ff 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -184,15 +184,24 @@ static void afs_free_call(struct afs_call *call)
 }
 
 /*
- * End a call
+ * End a call but do not free it
  */
-static void afs_end_call(struct afs_call *call)
+static void afs_end_call_nofree(struct afs_call *call)
 {
 	if (call->rxcall) {
 		rxrpc_kernel_end_call(call->rxcall);
 		call->rxcall = NULL;
 	}
-	call->type->destructor(call);
+	if (call->type->destructor)
+		call->type->destructor(call);
+}
+
+/*
+ * End a call and free it
+ */
+static void afs_end_call(struct afs_call *call)
+{
+	afs_end_call_nofree(call);
 	afs_free_call(call);
 }
 
@@ -640,10 +649,7 @@ static void afs_process_async_call(struct work_struct *work)
 		call->reply = NULL;
 
 		/* kill the call */
-		rxrpc_kernel_end_call(call->rxcall);
-		call->rxcall = NULL;
-		if (call->type->destructor)
-			call->type->destructor(call);
+		afs_end_call_nofree(call);
 
 		/* we can't just delete the call because the work item may be
 		 * queued */

commit 6c67c7c38cf32c2a9cbccb6b21aadf61a85fbfb4
Author: David Howells <dhowells@redhat.com>
Date:   Wed May 21 14:48:05 2014 +0100

    AFS: Fix cache manager service handlers
    
    Fix the cache manager RPC service handlers.  The afs_send_empty_reply() and
    afs_send_simple_reply() functions:
    
     (a) Kill the call and free up the buffers associated with it if they fail.
    
     (b) Return with call intact if it they succeed.
    
    However, none of the callers actually check the result or clean up if
    successful - and may use the now non-existent data if it fails.
    
    This was detected by Dan Carpenter using a static checker:
    
            The patch 08e0e7c82eea: "[AF_RXRPC]: Make the in-kernel AFS
            filesystem use AF_RXRPC." from Apr 26, 2007, leads to the following
            static checker warning:
            "fs/afs/cmservice.c:155 SRXAFSCB_CallBack()
                     warn: 'call' was already freed."
    
    Reported-by: Dan Carpenter <dan.carpenter@oracle.com>
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index ef943df73b8c..9226a6674d7f 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -183,6 +183,19 @@ static void afs_free_call(struct afs_call *call)
 	kfree(call);
 }
 
+/*
+ * End a call
+ */
+static void afs_end_call(struct afs_call *call)
+{
+	if (call->rxcall) {
+		rxrpc_kernel_end_call(call->rxcall);
+		call->rxcall = NULL;
+	}
+	call->type->destructor(call);
+	afs_free_call(call);
+}
+
 /*
  * allocate a call with flat request and reply buffers
  */
@@ -383,11 +396,8 @@ int afs_make_call(struct in_addr *addr, struct afs_call *call, gfp_t gfp,
 	rxrpc_kernel_abort_call(rxcall, RX_USER_ABORT);
 	while ((skb = skb_dequeue(&call->rx_queue)))
 		afs_free_skb(skb);
-	rxrpc_kernel_end_call(rxcall);
-	call->rxcall = NULL;
 error_kill_call:
-	call->type->destructor(call);
-	afs_free_call(call);
+	afs_end_call(call);
 	_leave(" = %d", ret);
 	return ret;
 }
@@ -509,12 +519,8 @@ static void afs_deliver_to_call(struct afs_call *call)
 	if (call->state >= AFS_CALL_COMPLETE) {
 		while ((skb = skb_dequeue(&call->rx_queue)))
 			afs_free_skb(skb);
-		if (call->incoming) {
-			rxrpc_kernel_end_call(call->rxcall);
-			call->rxcall = NULL;
-			call->type->destructor(call);
-			afs_free_call(call);
-		}
+		if (call->incoming)
+			afs_end_call(call);
 	}
 
 	_leave("");
@@ -564,10 +570,7 @@ static int afs_wait_for_call_to_complete(struct afs_call *call)
 	}
 
 	_debug("call complete");
-	rxrpc_kernel_end_call(call->rxcall);
-	call->rxcall = NULL;
-	call->type->destructor(call);
-	afs_free_call(call);
+	afs_end_call(call);
 	_leave(" = %d", ret);
 	return ret;
 }
@@ -790,10 +793,7 @@ void afs_send_empty_reply(struct afs_call *call)
 		_debug("oom");
 		rxrpc_kernel_abort_call(call->rxcall, RX_USER_ABORT);
 	default:
-		rxrpc_kernel_end_call(call->rxcall);
-		call->rxcall = NULL;
-		call->type->destructor(call);
-		afs_free_call(call);
+		afs_end_call(call);
 		_leave(" [error]");
 		return;
 	}
@@ -823,17 +823,16 @@ void afs_send_simple_reply(struct afs_call *call, const void *buf, size_t len)
 	call->state = AFS_CALL_AWAIT_ACK;
 	n = rxrpc_kernel_send_data(call->rxcall, &msg, len);
 	if (n >= 0) {
+		/* Success */
 		_leave(" [replied]");
 		return;
 	}
+
 	if (n == -ENOMEM) {
 		_debug("oom");
 		rxrpc_kernel_abort_call(call->rxcall, RX_USER_ABORT);
 	}
-	rxrpc_kernel_end_call(call->rxcall);
-	call->rxcall = NULL;
-	call->type->destructor(call);
-	afs_free_call(call);
+	afs_end_call(call);
 	_leave(" [error]");
 }
 

commit 059499453a9abd1857d442b44da8b4c126dc72a8
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Mar 7 10:24:50 2014 -0500

    afs: don't use PREPARE_WORK
    
    PREPARE_[DELAYED_]WORK() are being phased out.  They have few users
    and a nasty surprise in terms of reentrancy guarantee as workqueue
    considers work items to be different if they don't have the same work
    function.
    
    afs_call->async_work is multiplexed with multiple work functions.
    Introduce afs_async_workfn() which invokes afs_call->async_workfn and
    always use it as the work function and update the users to set the
    ->async_workfn field instead of overriding the work function using
    PREPARE_WORK().
    
    It would probably be best to route this with other related updates
    through the workqueue tree.
    
    Compile tested.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: David Howells <dhowells@redhat.com>
    Cc: linux-afs@lists.infradead.org

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 8ad8c2a0703a..ef943df73b8c 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -644,7 +644,7 @@ static void afs_process_async_call(struct work_struct *work)
 
 		/* we can't just delete the call because the work item may be
 		 * queued */
-		PREPARE_WORK(&call->async_work, afs_delete_async_call);
+		call->async_workfn = afs_delete_async_call;
 		queue_work(afs_async_calls, &call->async_work);
 	}
 
@@ -663,6 +663,13 @@ void afs_transfer_reply(struct afs_call *call, struct sk_buff *skb)
 	call->reply_size += len;
 }
 
+static void afs_async_workfn(struct work_struct *work)
+{
+	struct afs_call *call = container_of(work, struct afs_call, async_work);
+
+	call->async_workfn(work);
+}
+
 /*
  * accept the backlog of incoming calls
  */
@@ -685,7 +692,8 @@ static void afs_collect_incoming_call(struct work_struct *work)
 				return;
 			}
 
-			INIT_WORK(&call->async_work, afs_process_async_call);
+			call->async_workfn = afs_process_async_call;
+			INIT_WORK(&call->async_work, afs_async_workfn);
 			call->wait_mode = &afs_async_incoming_call;
 			call->type = &afs_RXCMxxxx;
 			init_waitqueue_head(&call->waitq);

commit c0173863528a8c9212c53e080d63a1aaae5ef4f4
Author: Anton Blanchard <anton@samba.org>
Date:   Fri Mar 16 10:28:19 2012 +0000

    afs: Remote abort can cause BUG in rxrpc code
    
    When writing files to afs I sometimes hit a BUG:
    
    kernel BUG at fs/afs/rxrpc.c:179!
    
    With a backtrace of:
    
            afs_free_call
            afs_make_call
            afs_fs_store_data
            afs_vnode_store_data
            afs_write_back_from_locked_page
            afs_writepages_region
            afs_writepages
    
    The cause is:
    
            ASSERT(skb_queue_empty(&call->rx_queue));
    
    Looking at a tcpdump of the session the abort happens because we
    are exceeding our disk quota:
    
            rx abort fs reply store-data error diskquota exceeded (32)
    
    So the abort error is valid. We hit the BUG because we haven't
    freed all the resources for the call.
    
    By freeing any skbs in call->rx_queue before calling afs_free_call
    we avoid hitting leaking memory and avoid hitting the BUG.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: David Howells <dhowells@redhat.com>
    Cc: <stable@kernel.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index e45a323aebb4..8ad8c2a0703a 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -314,6 +314,7 @@ int afs_make_call(struct in_addr *addr, struct afs_call *call, gfp_t gfp,
 	struct msghdr msg;
 	struct kvec iov[1];
 	int ret;
+	struct sk_buff *skb;
 
 	_enter("%x,{%d},", addr->s_addr, ntohs(call->port));
 
@@ -380,6 +381,8 @@ int afs_make_call(struct in_addr *addr, struct afs_call *call, gfp_t gfp,
 
 error_do_abort:
 	rxrpc_kernel_abort_call(rxcall, RX_USER_ABORT);
+	while ((skb = skb_dequeue(&call->rx_queue)))
+		afs_free_skb(skb);
 	rxrpc_kernel_end_call(rxcall);
 	call->rxcall = NULL;
 error_kill_call:

commit 0ad53eeefcbb2620b6a71ffdaad4add20b450b8b
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Jan 14 15:56:37 2011 +0000

    afs: add afs_wq and use it instead of the system workqueue
    
    flush_scheduled_work() is going away.  afs needs to make sure all the
    works it has queued have finished before being unloaded and there can
    be arbitrary number of pending works.  Add afs_wq and use it as the
    flush domain instead of the system workqueue.
    
    Also, convert cancel_delayed_work() + flush_scheduled_work() to
    cancel_delayed_work_sync() in afs_mntpt_kill_timer().
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: David Howells <dhowells@redhat.com>
    Cc: linux-afs@lists.infradead.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 654d8fdbf01f..e45a323aebb4 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -410,7 +410,7 @@ static void afs_rx_interceptor(struct sock *sk, unsigned long user_call_ID,
 	if (!call) {
 		/* its an incoming call for our callback service */
 		skb_queue_tail(&afs_incoming_calls, skb);
-		schedule_work(&afs_collect_incoming_call_work);
+		queue_work(afs_wq, &afs_collect_incoming_call_work);
 	} else {
 		/* route the messages directly to the appropriate call */
 		skb_queue_tail(&call->rx_queue, skb);

commit bebf8cfaea1df1a104b993b995bb385e998a4dc8
Author: Dan Carpenter <error27@gmail.com>
Date:   Tue Aug 10 18:03:27 2010 -0700

    afs: destroy work queue on init failure
    
    We can clean up the work queue on this error path.  This function is
    called from afs_init().
    
    Signed-off-by: Dan Carpenter <error27@gmail.com>
    Acked-by: David Howells <dhowells@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 67cf810e0fd6..654d8fdbf01f 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -100,6 +100,7 @@ int afs_open_socket(void)
 	ret = kernel_bind(socket, (struct sockaddr *) &srx, sizeof(srx));
 	if (ret < 0) {
 		sock_release(socket);
+		destroy_workqueue(afs_async_calls);
 		_leave(" = %d [bind]", ret);
 		return ret;
 	}

commit 5a0e3ad6af8660be21ca98a971cd00f331318c05
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Mar 24 17:04:11 2010 +0900

    include cleanup: Update gfp.h and slab.h includes to prepare for breaking implicit slab.h inclusion from percpu.h
    
    percpu.h is included by sched.h and module.h and thus ends up being
    included when building most .c files.  percpu.h includes slab.h which
    in turn includes gfp.h making everything defined by the two files
    universally available and complicating inclusion dependencies.
    
    percpu.h -> slab.h dependency is about to be removed.  Prepare for
    this change by updating users of gfp and slab facilities include those
    headers directly instead of assuming availability.  As this conversion
    needs to touch large number of source files, the following script is
    used as the basis of conversion.
    
      http://userweb.kernel.org/~tj/misc/slabh-sweep.py
    
    The script does the followings.
    
    * Scan files for gfp and slab usages and update includes such that
      only the necessary includes are there.  ie. if only gfp is used,
      gfp.h, if slab is used, slab.h.
    
    * When the script inserts a new include, it looks at the include
      blocks and try to put the new include such that its order conforms
      to its surrounding.  It's put in the include block which contains
      core kernel includes, in the same order that the rest are ordered -
      alphabetical, Christmas tree, rev-Xmas-tree or at the end if there
      doesn't seem to be any matching order.
    
    * If the script can't find a place to put a new include (mostly
      because the file doesn't have fitting include block), it prints out
      an error message indicating which .h file needs to be added to the
      file.
    
    The conversion was done in the following steps.
    
    1. The initial automatic conversion of all .c files updated slightly
       over 4000 files, deleting around 700 includes and adding ~480 gfp.h
       and ~3000 slab.h inclusions.  The script emitted errors for ~400
       files.
    
    2. Each error was manually checked.  Some didn't need the inclusion,
       some needed manual addition while adding it to implementation .h or
       embedding .c file was more appropriate for others.  This step added
       inclusions to around 150 files.
    
    3. The script was run again and the output was compared to the edits
       from #2 to make sure no file was left behind.
    
    4. Several build tests were done and a couple of problems were fixed.
       e.g. lib/decompress_*.c used malloc/free() wrappers around slab
       APIs requiring slab.h to be added manually.
    
    5. The script was run on all .h files but without automatically
       editing them as sprinkling gfp.h and slab.h inclusions around .h
       files could easily lead to inclusion dependency hell.  Most gfp.h
       inclusion directives were ignored as stuff from gfp.h was usually
       wildly available and often used in preprocessor macros.  Each
       slab.h inclusion directive was examined and added manually as
       necessary.
    
    6. percpu.h was updated not to include slab.h.
    
    7. Build test were done on the following configurations and failures
       were fixed.  CONFIG_GCOV_KERNEL was turned off for all tests (as my
       distributed build env didn't work with gcov compiles) and a few
       more options had to be turned off depending on archs to make things
       build (like ipr on powerpc/64 which failed due to missing writeq).
    
       * x86 and x86_64 UP and SMP allmodconfig and a custom test config.
       * powerpc and powerpc64 SMP allmodconfig
       * sparc and sparc64 SMP allmodconfig
       * ia64 SMP allmodconfig
       * s390 SMP allmodconfig
       * alpha SMP allmodconfig
       * um on x86_64 SMP allmodconfig
    
    8. percpu.h modifications were reverted so that it could be applied as
       a separate patch and serve as bisection point.
    
    Given the fact that I had only a couple of failures from tests on step
    6, I'm fairly confident about the coverage of this conversion patch.
    If there is a breakage, it's likely to be something in one of the arch
    headers which should be easily discoverable easily on most builds of
    the specific arch.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Guess-its-ok-by: Christoph Lameter <cl@linux-foundation.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Lee Schermerhorn <Lee.Schermerhorn@hp.com>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index bde3f19c0995..67cf810e0fd6 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -9,6 +9,7 @@
  * 2 of the License, or (at your option) any later version.
  */
 
+#include <linux/slab.h>
 #include <net/sock.h>
 #include <net/af_rxrpc.h>
 #include <rxrpc/packet.h>

commit c1206a2c6d87def4af5379e6fef64e928d882ab2
Author: Adrian Bunk <bunk@stusta.de>
Date:   Tue Oct 16 23:26:41 2007 -0700

    fs/afs/: possible cleanups
    
    This patch contains the following possible cleanups:
    - make the following needlessly global functions static:
      - rxrpc.c: afs_send_pages()
      - vlocation.c: afs_vlocation_queue_for_updates()
      - write.c: afs_writepages_region()
    - make the following needlessly global variables static:
      - mntpt.c: afs_mntpt_expiry_timeout
      - proc.c: afs_vlocation_states[]
      - server.c: afs_server_timeout
      - vlocation.c: afs_vlocation_timeout
      - vlocation.c: afs_vlocation_update_timeout
    - #if 0 the following unused function:
      - cell.c: afs_get_cell_maybe()
    - #if 0 the following unused variables:
      - callback.c: afs_vnode_update_timeout
      - cmservice.c: struct afs_cm_workqueue
    
    Signed-off-by: Adrian Bunk <bunk@stusta.de>
    Acked-by: David Howells <dhowells@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 8ccee9ee1d9d..bde3f19c0995 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -239,7 +239,8 @@ void afs_flat_call_destructor(struct afs_call *call)
 /*
  * attach the data from a bunch of pages on an inode to a call
  */
-int afs_send_pages(struct afs_call *call, struct msghdr *msg, struct kvec *iov)
+static int afs_send_pages(struct afs_call *call, struct msghdr *msg,
+			  struct kvec *iov)
 {
 	struct page *pages[8];
 	unsigned count, n, loop, offset, to;

commit bd6dc742a4b1945861795a66dc27c65365c5f28e
Author: David Howells <dhowells@redhat.com>
Date:   Fri Jul 20 10:59:41 2007 +0100

    AFS: Use patched rxrpc_kernel_send_data() correctly
    
    Fix afs_send_simple_reply() to accept a greater-than-zero return value from
    rxrpc_kernel_send_data() as being a successful return rather than thinking it
    an error and aborting the call.
    
    rxrpc_kernel_send_data() previously returned zero incorrectly when it worked
    successfully, but has been patched to return the number of bytes it
    transmitted.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 1b36f45076ad..8ccee9ee1d9d 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -792,6 +792,7 @@ void afs_send_simple_reply(struct afs_call *call, const void *buf, size_t len)
 {
 	struct msghdr msg;
 	struct iovec iov[1];
+	int n;
 
 	_enter("");
 
@@ -806,22 +807,20 @@ void afs_send_simple_reply(struct afs_call *call, const void *buf, size_t len)
 	msg.msg_flags		= 0;
 
 	call->state = AFS_CALL_AWAIT_ACK;
-	switch (rxrpc_kernel_send_data(call->rxcall, &msg, len)) {
-	case 0:
+	n = rxrpc_kernel_send_data(call->rxcall, &msg, len);
+	if (n >= 0) {
 		_leave(" [replied]");
 		return;
-
-	case -ENOMEM:
+	}
+	if (n == -ENOMEM) {
 		_debug("oom");
 		rxrpc_kernel_abort_call(call->rxcall, RX_USER_ABORT);
-	default:
-		rxrpc_kernel_end_call(call->rxcall);
-		call->rxcall = NULL;
-		call->type->destructor(call);
-		afs_free_call(call);
-		_leave(" [error]");
-		return;
 	}
+	rxrpc_kernel_end_call(call->rxcall);
+	call->rxcall = NULL;
+	call->type->destructor(call);
+	afs_free_call(call);
+	_leave(" [error]");
 }
 
 /*

commit 5bbf5d39f88af9e8173f7331a3741a8b3ff16a1c
Author: David Howells <dhowells@redhat.com>
Date:   Thu May 10 03:15:23 2007 -0700

    AFS: further write support fixes
    
    Further fixes for AFS write support:
    
     (1) The afs_send_pages() outer loop must do an extra iteration if it ends
         with 'first == last' because 'last' is inclusive in the page set
         otherwise it fails to send the last page and complete the RxRPC op under
         some circumstances.
    
     (2) Similarly, the outer loop in afs_pages_written_back() must also do an
         extra iteration if it ends with 'first == last', otherwise it fails to
         clear PG_writeback on the last page under some circumstances.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 04189c47d6a0..1b36f45076ad 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -294,7 +294,7 @@ int afs_send_pages(struct afs_call *call, struct msghdr *msg, struct kvec *iov)
 			put_page(pages[loop]);
 		if (ret < 0)
 			break;
-	} while (first < last);
+	} while (first <= last);
 
 	_leave(" = %d", ret);
 	return ret;

commit 31143d5d515ece617ffccb7df5ff75e4d1dfa120
Author: David Howells <dhowells@redhat.com>
Date:   Wed May 9 02:33:46 2007 -0700

    AFS: implement basic file write support
    
    Implement support for writing to regular AFS files, including:
    
     (1) write
    
     (2) truncate
    
     (3) fsync, fdatasync
    
     (4) chmod, chown, chgrp, utime.
    
    AFS writeback attempts to batch writes into as chunks as large as it can manage
    up to the point that it writes back 65535 pages in one chunk or it meets a
    locked page.
    
    Furthermore, if a page has been written to using a particular key, then should
    another write to that page use some other key, the first write will be flushed
    before the second is allowed to take place.  If the first write fails due to a
    security error, then the page will be scrapped and reread before the second
    write takes place.
    
    If a page is dirty and the callback on it is broken by the server, then the
    dirty data is not discarded (same behaviour as NFS).
    
    Shared-writable mappings are not supported by this patch.
    
    [akpm@linux-foundation.org: fix a bunch of warnings]
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 222c1a3abbb8..04189c47d6a0 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -236,6 +236,70 @@ void afs_flat_call_destructor(struct afs_call *call)
 	call->buffer = NULL;
 }
 
+/*
+ * attach the data from a bunch of pages on an inode to a call
+ */
+int afs_send_pages(struct afs_call *call, struct msghdr *msg, struct kvec *iov)
+{
+	struct page *pages[8];
+	unsigned count, n, loop, offset, to;
+	pgoff_t first = call->first, last = call->last;
+	int ret;
+
+	_enter("");
+
+	offset = call->first_offset;
+	call->first_offset = 0;
+
+	do {
+		_debug("attach %lx-%lx", first, last);
+
+		count = last - first + 1;
+		if (count > ARRAY_SIZE(pages))
+			count = ARRAY_SIZE(pages);
+		n = find_get_pages_contig(call->mapping, first, count, pages);
+		ASSERTCMP(n, ==, count);
+
+		loop = 0;
+		do {
+			msg->msg_flags = 0;
+			to = PAGE_SIZE;
+			if (first + loop >= last)
+				to = call->last_to;
+			else
+				msg->msg_flags = MSG_MORE;
+			iov->iov_base = kmap(pages[loop]) + offset;
+			iov->iov_len = to - offset;
+			offset = 0;
+
+			_debug("- range %u-%u%s",
+			       offset, to, msg->msg_flags ? " [more]" : "");
+			msg->msg_iov = (struct iovec *) iov;
+			msg->msg_iovlen = 1;
+
+			/* have to change the state *before* sending the last
+			 * packet as RxRPC might give us the reply before it
+			 * returns from sending the request */
+			if (first + loop >= last)
+				call->state = AFS_CALL_AWAIT_REPLY;
+			ret = rxrpc_kernel_send_data(call->rxcall, msg,
+						     to - offset);
+			kunmap(pages[loop]);
+			if (ret < 0)
+				break;
+		} while (++loop < count);
+		first += count;
+
+		for (loop = 0; loop < count; loop++)
+			put_page(pages[loop]);
+		if (ret < 0)
+			break;
+	} while (first < last);
+
+	_leave(" = %d", ret);
+	return ret;
+}
+
 /*
  * initiate a call
  */
@@ -253,8 +317,9 @@ int afs_make_call(struct in_addr *addr, struct afs_call *call, gfp_t gfp,
 	ASSERT(call->type != NULL);
 	ASSERT(call->type->name != NULL);
 
-	_debug("MAKE %p{%s} [%d]",
-	       call, call->type->name, atomic_read(&afs_outstanding_calls));
+	_debug("____MAKE %p{%s,%x} [%d]____",
+	       call, call->type->name, key_serial(call->key),
+	       atomic_read(&afs_outstanding_calls));
 
 	call->wait_mode = wait_mode;
 	INIT_WORK(&call->async_work, afs_process_async_call);
@@ -289,16 +354,23 @@ int afs_make_call(struct in_addr *addr, struct afs_call *call, gfp_t gfp,
 	msg.msg_iovlen		= 1;
 	msg.msg_control		= NULL;
 	msg.msg_controllen	= 0;
-	msg.msg_flags		= 0;
+	msg.msg_flags		= (call->send_pages ? MSG_MORE : 0);
 
 	/* have to change the state *before* sending the last packet as RxRPC
 	 * might give us the reply before it returns from sending the
 	 * request */
-	call->state = AFS_CALL_AWAIT_REPLY;
+	if (!call->send_pages)
+		call->state = AFS_CALL_AWAIT_REPLY;
 	ret = rxrpc_kernel_send_data(rxcall, &msg, call->request_size);
 	if (ret < 0)
 		goto error_do_abort;
 
+	if (call->send_pages) {
+		ret = afs_send_pages(call, &msg, iov);
+		if (ret < 0)
+			goto error_do_abort;
+	}
+
 	/* at this point, an async call may no longer exist as it may have
 	 * already completed */
 	return wait_mode->wait(call);

commit b1bdb691c3c38b4fbaf99fa8474f5cfa99b2d774
Author: David Howells <dhowells@redhat.com>
Date:   Fri Apr 27 15:28:45 2007 -0700

    [AF_RXRPC/AFS]: Arch-specific fixes.
    
    Fixes for various arch compilation problems:
    
     (*) Missing module exports.
    
     (*) Variable name collision when rxkad and af_rxrpc both built in
         (rxrpc_debug).
    
     (*) Large constant representation problem (AFS_UUID_TO_UNIX_TIME).
    
     (*) Configuration dependencies.
    
     (*) printk() format warnings.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index e7b047328a39..222c1a3abbb8 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -772,7 +772,7 @@ int afs_extract_data(struct afs_call *call, struct sk_buff *skb,
 
 	if (call->offset < count) {
 		if (last) {
-			_leave(" = -EBADMSG [%d < %lu]", call->offset, count);
+			_leave(" = -EBADMSG [%d < %zu]", call->offset, count);
 			return -EBADMSG;
 		}
 		_leave(" = -EAGAIN");

commit b908fe6b2d1294d93b0d0badf6bf4f9a2cd7d729
Author: David Howells <dhowells@redhat.com>
Date:   Thu Apr 26 15:58:17 2007 -0700

    [AFS]: Add support for the CB.GetCapabilities operation.
    
    Add support for the CB.GetCapabilities operation with which the fileserver can
    ask the client for the following information:
    
     (1) The list of network interfaces it has available as IPv4 address + netmask
         plus the MTUs.
    
     (2) The client's UUID.
    
     (3) The extended capabilities of the client, for which the only current one
         is unified error mapping (abort code interpretation).
    
    To support this, the patch adds the following routines to AFS:
    
     (1) A function to iterate through all the network interfaces using RTNETLINK
         to extract IPv4 addresses and MTUs.
    
     (2) A function to iterate through all the network interfaces using RTNETLINK
         to pull out the MAC address of the lowest index interface to use in UUID
         construction.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index e86c527d87a1..e7b047328a39 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -713,6 +713,45 @@ void afs_send_empty_reply(struct afs_call *call)
 	}
 }
 
+/*
+ * send a simple reply
+ */
+void afs_send_simple_reply(struct afs_call *call, const void *buf, size_t len)
+{
+	struct msghdr msg;
+	struct iovec iov[1];
+
+	_enter("");
+
+	iov[0].iov_base		= (void *) buf;
+	iov[0].iov_len		= len;
+	msg.msg_name		= NULL;
+	msg.msg_namelen		= 0;
+	msg.msg_iov		= iov;
+	msg.msg_iovlen		= 1;
+	msg.msg_control		= NULL;
+	msg.msg_controllen	= 0;
+	msg.msg_flags		= 0;
+
+	call->state = AFS_CALL_AWAIT_ACK;
+	switch (rxrpc_kernel_send_data(call->rxcall, &msg, len)) {
+	case 0:
+		_leave(" [replied]");
+		return;
+
+	case -ENOMEM:
+		_debug("oom");
+		rxrpc_kernel_abort_call(call->rxcall, RX_USER_ABORT);
+	default:
+		rxrpc_kernel_end_call(call->rxcall);
+		call->rxcall = NULL;
+		call->type->destructor(call);
+		afs_free_call(call);
+		_leave(" [error]");
+		return;
+	}
+}
+
 /*
  * extract a piece of data from the received data socket buffers
  */

commit 00d3b7a4533e367b0dc2812a706db8f9f071c27f
Author: David Howells <dhowells@redhat.com>
Date:   Thu Apr 26 15:57:07 2007 -0700

    [AFS]: Add security support.
    
    Add security support to the AFS filesystem.  Kerberos IV tickets are added as
    RxRPC keys are added to the session keyring with the klog program.  open() and
    other VFS operations then find this ticket with request_key() and either use
    it immediately (eg: mkdir, unlink) or attach it to a file descriptor (open).
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index b92774231b3c..e86c527d87a1 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -17,6 +17,8 @@
 
 static struct socket *afs_socket; /* my RxRPC socket */
 static struct workqueue_struct *afs_async_calls;
+static atomic_t afs_outstanding_calls;
+static atomic_t afs_outstanding_skbs;
 
 static void afs_wake_up_call_waiter(struct afs_call *);
 static int afs_wait_for_call_to_complete(struct afs_call *);
@@ -45,6 +47,7 @@ static const struct afs_wait_mode afs_async_incoming_call = {
 
 /* asynchronous incoming call initial processing */
 static const struct afs_call_type afs_RXCMxxxx = {
+	.name		= "CB.xxxx",
 	.deliver	= afs_deliver_cm_op_id,
 	.abort_to_error	= afs_abort_to_error,
 };
@@ -118,9 +121,66 @@ void afs_close_socket(void)
 
 	_debug("dework");
 	destroy_workqueue(afs_async_calls);
+
+	ASSERTCMP(atomic_read(&afs_outstanding_skbs), ==, 0);
+	ASSERTCMP(atomic_read(&afs_outstanding_calls), ==, 0);
 	_leave("");
 }
 
+/*
+ * note that the data in a socket buffer is now delivered and that the buffer
+ * should be freed
+ */
+static void afs_data_delivered(struct sk_buff *skb)
+{
+	if (!skb) {
+		_debug("DLVR NULL [%d]", atomic_read(&afs_outstanding_skbs));
+		dump_stack();
+	} else {
+		_debug("DLVR %p{%u} [%d]",
+		       skb, skb->mark, atomic_read(&afs_outstanding_skbs));
+		if (atomic_dec_return(&afs_outstanding_skbs) == -1)
+			BUG();
+		rxrpc_kernel_data_delivered(skb);
+	}
+}
+
+/*
+ * free a socket buffer
+ */
+static void afs_free_skb(struct sk_buff *skb)
+{
+	if (!skb) {
+		_debug("FREE NULL [%d]", atomic_read(&afs_outstanding_skbs));
+		dump_stack();
+	} else {
+		_debug("FREE %p{%u} [%d]",
+		       skb, skb->mark, atomic_read(&afs_outstanding_skbs));
+		if (atomic_dec_return(&afs_outstanding_skbs) == -1)
+			BUG();
+		rxrpc_kernel_free_skb(skb);
+	}
+}
+
+/*
+ * free a call
+ */
+static void afs_free_call(struct afs_call *call)
+{
+	_debug("DONE %p{%s} [%d]",
+	       call, call->type->name, atomic_read(&afs_outstanding_calls));
+	if (atomic_dec_return(&afs_outstanding_calls) == -1)
+		BUG();
+
+	ASSERTCMP(call->rxcall, ==, NULL);
+	ASSERT(!work_pending(&call->async_work));
+	ASSERT(skb_queue_empty(&call->rx_queue));
+	ASSERT(call->type->name != NULL);
+
+	kfree(call->request);
+	kfree(call);
+}
+
 /*
  * allocate a call with flat request and reply buffers
  */
@@ -133,30 +193,32 @@ struct afs_call *afs_alloc_flat_call(const struct afs_call_type *type,
 	if (!call)
 		goto nomem_call;
 
+	_debug("CALL %p{%s} [%d]",
+	       call, type->name, atomic_read(&afs_outstanding_calls));
+	atomic_inc(&afs_outstanding_calls);
+
+	call->type = type;
+	call->request_size = request_size;
+	call->reply_max = reply_size;
+
 	if (request_size) {
 		call->request = kmalloc(request_size, GFP_NOFS);
 		if (!call->request)
-			goto nomem_request;
+			goto nomem_free;
 	}
 
 	if (reply_size) {
 		call->buffer = kmalloc(reply_size, GFP_NOFS);
 		if (!call->buffer)
-			goto nomem_buffer;
+			goto nomem_free;
 	}
 
-	call->type = type;
-	call->request_size = request_size;
-	call->reply_max = reply_size;
-
 	init_waitqueue_head(&call->waitq);
 	skb_queue_head_init(&call->rx_queue);
 	return call;
 
-nomem_buffer:
-	kfree(call->request);
-nomem_request:
-	kfree(call);
+nomem_free:
+	afs_free_call(call);
 nomem_call:
 	return NULL;
 }
@@ -188,6 +250,12 @@ int afs_make_call(struct in_addr *addr, struct afs_call *call, gfp_t gfp,
 
 	_enter("%x,{%d},", addr->s_addr, ntohs(call->port));
 
+	ASSERT(call->type != NULL);
+	ASSERT(call->type->name != NULL);
+
+	_debug("MAKE %p{%s} [%d]",
+	       call, call->type->name, atomic_read(&afs_outstanding_calls));
+
 	call->wait_mode = wait_mode;
 	INIT_WORK(&call->async_work, afs_process_async_call);
 
@@ -203,6 +271,7 @@ int afs_make_call(struct in_addr *addr, struct afs_call *call, gfp_t gfp,
 	/* create a call */
 	rxcall = rxrpc_kernel_begin_call(afs_socket, &srx, call->key,
 					 (unsigned long) call, gfp);
+	call->key = NULL;
 	if (IS_ERR(rxcall)) {
 		ret = PTR_ERR(rxcall);
 		goto error_kill_call;
@@ -237,10 +306,10 @@ int afs_make_call(struct in_addr *addr, struct afs_call *call, gfp_t gfp,
 error_do_abort:
 	rxrpc_kernel_abort_call(rxcall, RX_USER_ABORT);
 	rxrpc_kernel_end_call(rxcall);
+	call->rxcall = NULL;
 error_kill_call:
 	call->type->destructor(call);
-	ASSERT(skb_queue_empty(&call->rx_queue));
-	kfree(call);
+	afs_free_call(call);
 	_leave(" = %d", ret);
 	return ret;
 }
@@ -257,15 +326,19 @@ static void afs_rx_interceptor(struct sock *sk, unsigned long user_call_ID,
 
 	_enter("%p,,%u", call, skb->mark);
 
+	_debug("ICPT %p{%u} [%d]",
+	       skb, skb->mark, atomic_read(&afs_outstanding_skbs));
+
 	ASSERTCMP(sk, ==, afs_socket->sk);
+	atomic_inc(&afs_outstanding_skbs);
 
 	if (!call) {
 		/* its an incoming call for our callback service */
-		__skb_queue_tail(&afs_incoming_calls, skb);
+		skb_queue_tail(&afs_incoming_calls, skb);
 		schedule_work(&afs_collect_incoming_call_work);
 	} else {
 		/* route the messages directly to the appropriate call */
-		__skb_queue_tail(&call->rx_queue, skb);
+		skb_queue_tail(&call->rx_queue, skb);
 		call->wait_mode->rx_wakeup(call);
 	}
 
@@ -317,9 +390,9 @@ static void afs_deliver_to_call(struct afs_call *call)
 				call->state = AFS_CALL_ERROR;
 				break;
 			}
-			rxrpc_kernel_data_delivered(skb);
+			afs_data_delivered(skb);
 			skb = NULL;
-			break;
+			continue;
 		case RXRPC_SKB_MARK_FINAL_ACK:
 			_debug("Rcv ACK");
 			call->state = AFS_CALL_COMPLETE;
@@ -350,19 +423,19 @@ static void afs_deliver_to_call(struct afs_call *call)
 			break;
 		}
 
-		rxrpc_kernel_free_skb(skb);
+		afs_free_skb(skb);
 	}
 
 	/* make sure the queue is empty if the call is done with (we might have
 	 * aborted the call early because of an unmarshalling error) */
 	if (call->state >= AFS_CALL_COMPLETE) {
 		while ((skb = skb_dequeue(&call->rx_queue)))
-			rxrpc_kernel_free_skb(skb);
+			afs_free_skb(skb);
 		if (call->incoming) {
 			rxrpc_kernel_end_call(call->rxcall);
+			call->rxcall = NULL;
 			call->type->destructor(call);
-			ASSERT(skb_queue_empty(&call->rx_queue));
-			kfree(call);
+			afs_free_call(call);
 		}
 	}
 
@@ -409,14 +482,14 @@ static int afs_wait_for_call_to_complete(struct afs_call *call)
 		_debug("call incomplete");
 		rxrpc_kernel_abort_call(call->rxcall, RX_CALL_DEAD);
 		while ((skb = skb_dequeue(&call->rx_queue)))
-			rxrpc_kernel_free_skb(skb);
+			afs_free_skb(skb);
 	}
 
 	_debug("call complete");
 	rxrpc_kernel_end_call(call->rxcall);
+	call->rxcall = NULL;
 	call->type->destructor(call);
-	ASSERT(skb_queue_empty(&call->rx_queue));
-	kfree(call);
+	afs_free_call(call);
 	_leave(" = %d", ret);
 	return ret;
 }
@@ -459,9 +532,7 @@ static void afs_delete_async_call(struct work_struct *work)
 
 	_enter("");
 
-	ASSERT(skb_queue_empty(&call->rx_queue));
-	ASSERT(!work_pending(&call->async_work));
-	kfree(call);
+	afs_free_call(call);
 
 	_leave("");
 }
@@ -489,6 +560,7 @@ static void afs_process_async_call(struct work_struct *work)
 
 		/* kill the call */
 		rxrpc_kernel_end_call(call->rxcall);
+		call->rxcall = NULL;
 		if (call->type->destructor)
 			call->type->destructor(call);
 
@@ -526,7 +598,7 @@ static void afs_collect_incoming_call(struct work_struct *work)
 		_debug("new call");
 
 		/* don't need the notification */
-		rxrpc_kernel_free_skb(skb);
+		afs_free_skb(skb);
 
 		if (!call) {
 			call = kzalloc(sizeof(struct afs_call), GFP_KERNEL);
@@ -541,6 +613,11 @@ static void afs_collect_incoming_call(struct work_struct *work)
 			init_waitqueue_head(&call->waitq);
 			skb_queue_head_init(&call->rx_queue);
 			call->state = AFS_CALL_AWAIT_OP_ID;
+
+			_debug("CALL %p{%s} [%d]",
+			       call, call->type->name,
+			       atomic_read(&afs_outstanding_calls));
+			atomic_inc(&afs_outstanding_calls);
 		}
 
 		rxcall = rxrpc_kernel_accept_call(afs_socket,
@@ -551,7 +628,8 @@ static void afs_collect_incoming_call(struct work_struct *work)
 		}
 	}
 
-	kfree(call);
+	if (call)
+		afs_free_call(call);
 }
 
 /*
@@ -629,8 +707,7 @@ void afs_send_empty_reply(struct afs_call *call)
 		rxrpc_kernel_end_call(call->rxcall);
 		call->rxcall = NULL;
 		call->type->destructor(call);
-		ASSERT(skb_queue_empty(&call->rx_queue));
-		kfree(call);
+		afs_free_call(call);
 		_leave(" [error]");
 		return;
 	}

commit 08e0e7c82eeadec6f4871a386b86bf0f0fbcb4eb
Author: David Howells <dhowells@redhat.com>
Date:   Thu Apr 26 15:55:03 2007 -0700

    [AF_RXRPC]: Make the in-kernel AFS filesystem use AF_RXRPC.
    
    Make the in-kernel AFS filesystem use AF_RXRPC instead of the old RxRPC code.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
new file mode 100644
index 000000000000..b92774231b3c
--- /dev/null
+++ b/fs/afs/rxrpc.c
@@ -0,0 +1,666 @@
+/* Maintain an RxRPC server socket to do AFS communications through
+ *
+ * Copyright (C) 2007 Red Hat, Inc. All Rights Reserved.
+ * Written by David Howells (dhowells@redhat.com)
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version
+ * 2 of the License, or (at your option) any later version.
+ */
+
+#include <net/sock.h>
+#include <net/af_rxrpc.h>
+#include <rxrpc/packet.h>
+#include "internal.h"
+#include "afs_cm.h"
+
+static struct socket *afs_socket; /* my RxRPC socket */
+static struct workqueue_struct *afs_async_calls;
+
+static void afs_wake_up_call_waiter(struct afs_call *);
+static int afs_wait_for_call_to_complete(struct afs_call *);
+static void afs_wake_up_async_call(struct afs_call *);
+static int afs_dont_wait_for_call_to_complete(struct afs_call *);
+static void afs_process_async_call(struct work_struct *);
+static void afs_rx_interceptor(struct sock *, unsigned long, struct sk_buff *);
+static int afs_deliver_cm_op_id(struct afs_call *, struct sk_buff *, bool);
+
+/* synchronous call management */
+const struct afs_wait_mode afs_sync_call = {
+	.rx_wakeup	= afs_wake_up_call_waiter,
+	.wait		= afs_wait_for_call_to_complete,
+};
+
+/* asynchronous call management */
+const struct afs_wait_mode afs_async_call = {
+	.rx_wakeup	= afs_wake_up_async_call,
+	.wait		= afs_dont_wait_for_call_to_complete,
+};
+
+/* asynchronous incoming call management */
+static const struct afs_wait_mode afs_async_incoming_call = {
+	.rx_wakeup	= afs_wake_up_async_call,
+};
+
+/* asynchronous incoming call initial processing */
+static const struct afs_call_type afs_RXCMxxxx = {
+	.deliver	= afs_deliver_cm_op_id,
+	.abort_to_error	= afs_abort_to_error,
+};
+
+static void afs_collect_incoming_call(struct work_struct *);
+
+static struct sk_buff_head afs_incoming_calls;
+static DECLARE_WORK(afs_collect_incoming_call_work, afs_collect_incoming_call);
+
+/*
+ * open an RxRPC socket and bind it to be a server for callback notifications
+ * - the socket is left in blocking mode and non-blocking ops use MSG_DONTWAIT
+ */
+int afs_open_socket(void)
+{
+	struct sockaddr_rxrpc srx;
+	struct socket *socket;
+	int ret;
+
+	_enter("");
+
+	skb_queue_head_init(&afs_incoming_calls);
+
+	afs_async_calls = create_singlethread_workqueue("kafsd");
+	if (!afs_async_calls) {
+		_leave(" = -ENOMEM [wq]");
+		return -ENOMEM;
+	}
+
+	ret = sock_create_kern(AF_RXRPC, SOCK_DGRAM, PF_INET, &socket);
+	if (ret < 0) {
+		destroy_workqueue(afs_async_calls);
+		_leave(" = %d [socket]", ret);
+		return ret;
+	}
+
+	socket->sk->sk_allocation = GFP_NOFS;
+
+	/* bind the callback manager's address to make this a server socket */
+	srx.srx_family			= AF_RXRPC;
+	srx.srx_service			= CM_SERVICE;
+	srx.transport_type		= SOCK_DGRAM;
+	srx.transport_len		= sizeof(srx.transport.sin);
+	srx.transport.sin.sin_family	= AF_INET;
+	srx.transport.sin.sin_port	= htons(AFS_CM_PORT);
+	memset(&srx.transport.sin.sin_addr, 0,
+	       sizeof(srx.transport.sin.sin_addr));
+
+	ret = kernel_bind(socket, (struct sockaddr *) &srx, sizeof(srx));
+	if (ret < 0) {
+		sock_release(socket);
+		_leave(" = %d [bind]", ret);
+		return ret;
+	}
+
+	rxrpc_kernel_intercept_rx_messages(socket, afs_rx_interceptor);
+
+	afs_socket = socket;
+	_leave(" = 0");
+	return 0;
+}
+
+/*
+ * close the RxRPC socket AFS was using
+ */
+void afs_close_socket(void)
+{
+	_enter("");
+
+	sock_release(afs_socket);
+
+	_debug("dework");
+	destroy_workqueue(afs_async_calls);
+	_leave("");
+}
+
+/*
+ * allocate a call with flat request and reply buffers
+ */
+struct afs_call *afs_alloc_flat_call(const struct afs_call_type *type,
+				     size_t request_size, size_t reply_size)
+{
+	struct afs_call *call;
+
+	call = kzalloc(sizeof(*call), GFP_NOFS);
+	if (!call)
+		goto nomem_call;
+
+	if (request_size) {
+		call->request = kmalloc(request_size, GFP_NOFS);
+		if (!call->request)
+			goto nomem_request;
+	}
+
+	if (reply_size) {
+		call->buffer = kmalloc(reply_size, GFP_NOFS);
+		if (!call->buffer)
+			goto nomem_buffer;
+	}
+
+	call->type = type;
+	call->request_size = request_size;
+	call->reply_max = reply_size;
+
+	init_waitqueue_head(&call->waitq);
+	skb_queue_head_init(&call->rx_queue);
+	return call;
+
+nomem_buffer:
+	kfree(call->request);
+nomem_request:
+	kfree(call);
+nomem_call:
+	return NULL;
+}
+
+/*
+ * clean up a call with flat buffer
+ */
+void afs_flat_call_destructor(struct afs_call *call)
+{
+	_enter("");
+
+	kfree(call->request);
+	call->request = NULL;
+	kfree(call->buffer);
+	call->buffer = NULL;
+}
+
+/*
+ * initiate a call
+ */
+int afs_make_call(struct in_addr *addr, struct afs_call *call, gfp_t gfp,
+		  const struct afs_wait_mode *wait_mode)
+{
+	struct sockaddr_rxrpc srx;
+	struct rxrpc_call *rxcall;
+	struct msghdr msg;
+	struct kvec iov[1];
+	int ret;
+
+	_enter("%x,{%d},", addr->s_addr, ntohs(call->port));
+
+	call->wait_mode = wait_mode;
+	INIT_WORK(&call->async_work, afs_process_async_call);
+
+	memset(&srx, 0, sizeof(srx));
+	srx.srx_family = AF_RXRPC;
+	srx.srx_service = call->service_id;
+	srx.transport_type = SOCK_DGRAM;
+	srx.transport_len = sizeof(srx.transport.sin);
+	srx.transport.sin.sin_family = AF_INET;
+	srx.transport.sin.sin_port = call->port;
+	memcpy(&srx.transport.sin.sin_addr, addr, 4);
+
+	/* create a call */
+	rxcall = rxrpc_kernel_begin_call(afs_socket, &srx, call->key,
+					 (unsigned long) call, gfp);
+	if (IS_ERR(rxcall)) {
+		ret = PTR_ERR(rxcall);
+		goto error_kill_call;
+	}
+
+	call->rxcall = rxcall;
+
+	/* send the request */
+	iov[0].iov_base	= call->request;
+	iov[0].iov_len	= call->request_size;
+
+	msg.msg_name		= NULL;
+	msg.msg_namelen		= 0;
+	msg.msg_iov		= (struct iovec *) iov;
+	msg.msg_iovlen		= 1;
+	msg.msg_control		= NULL;
+	msg.msg_controllen	= 0;
+	msg.msg_flags		= 0;
+
+	/* have to change the state *before* sending the last packet as RxRPC
+	 * might give us the reply before it returns from sending the
+	 * request */
+	call->state = AFS_CALL_AWAIT_REPLY;
+	ret = rxrpc_kernel_send_data(rxcall, &msg, call->request_size);
+	if (ret < 0)
+		goto error_do_abort;
+
+	/* at this point, an async call may no longer exist as it may have
+	 * already completed */
+	return wait_mode->wait(call);
+
+error_do_abort:
+	rxrpc_kernel_abort_call(rxcall, RX_USER_ABORT);
+	rxrpc_kernel_end_call(rxcall);
+error_kill_call:
+	call->type->destructor(call);
+	ASSERT(skb_queue_empty(&call->rx_queue));
+	kfree(call);
+	_leave(" = %d", ret);
+	return ret;
+}
+
+/*
+ * handles intercepted messages that were arriving in the socket's Rx queue
+ * - called with the socket receive queue lock held to ensure message ordering
+ * - called with softirqs disabled
+ */
+static void afs_rx_interceptor(struct sock *sk, unsigned long user_call_ID,
+			       struct sk_buff *skb)
+{
+	struct afs_call *call = (struct afs_call *) user_call_ID;
+
+	_enter("%p,,%u", call, skb->mark);
+
+	ASSERTCMP(sk, ==, afs_socket->sk);
+
+	if (!call) {
+		/* its an incoming call for our callback service */
+		__skb_queue_tail(&afs_incoming_calls, skb);
+		schedule_work(&afs_collect_incoming_call_work);
+	} else {
+		/* route the messages directly to the appropriate call */
+		__skb_queue_tail(&call->rx_queue, skb);
+		call->wait_mode->rx_wakeup(call);
+	}
+
+	_leave("");
+}
+
+/*
+ * deliver messages to a call
+ */
+static void afs_deliver_to_call(struct afs_call *call)
+{
+	struct sk_buff *skb;
+	bool last;
+	u32 abort_code;
+	int ret;
+
+	_enter("");
+
+	while ((call->state == AFS_CALL_AWAIT_REPLY ||
+		call->state == AFS_CALL_AWAIT_OP_ID ||
+		call->state == AFS_CALL_AWAIT_REQUEST ||
+		call->state == AFS_CALL_AWAIT_ACK) &&
+	       (skb = skb_dequeue(&call->rx_queue))) {
+		switch (skb->mark) {
+		case RXRPC_SKB_MARK_DATA:
+			_debug("Rcv DATA");
+			last = rxrpc_kernel_is_data_last(skb);
+			ret = call->type->deliver(call, skb, last);
+			switch (ret) {
+			case 0:
+				if (last &&
+				    call->state == AFS_CALL_AWAIT_REPLY)
+					call->state = AFS_CALL_COMPLETE;
+				break;
+			case -ENOTCONN:
+				abort_code = RX_CALL_DEAD;
+				goto do_abort;
+			case -ENOTSUPP:
+				abort_code = RX_INVALID_OPERATION;
+				goto do_abort;
+			default:
+				abort_code = RXGEN_CC_UNMARSHAL;
+				if (call->state != AFS_CALL_AWAIT_REPLY)
+					abort_code = RXGEN_SS_UNMARSHAL;
+			do_abort:
+				rxrpc_kernel_abort_call(call->rxcall,
+							abort_code);
+				call->error = ret;
+				call->state = AFS_CALL_ERROR;
+				break;
+			}
+			rxrpc_kernel_data_delivered(skb);
+			skb = NULL;
+			break;
+		case RXRPC_SKB_MARK_FINAL_ACK:
+			_debug("Rcv ACK");
+			call->state = AFS_CALL_COMPLETE;
+			break;
+		case RXRPC_SKB_MARK_BUSY:
+			_debug("Rcv BUSY");
+			call->error = -EBUSY;
+			call->state = AFS_CALL_BUSY;
+			break;
+		case RXRPC_SKB_MARK_REMOTE_ABORT:
+			abort_code = rxrpc_kernel_get_abort_code(skb);
+			call->error = call->type->abort_to_error(abort_code);
+			call->state = AFS_CALL_ABORTED;
+			_debug("Rcv ABORT %u -> %d", abort_code, call->error);
+			break;
+		case RXRPC_SKB_MARK_NET_ERROR:
+			call->error = -rxrpc_kernel_get_error_number(skb);
+			call->state = AFS_CALL_ERROR;
+			_debug("Rcv NET ERROR %d", call->error);
+			break;
+		case RXRPC_SKB_MARK_LOCAL_ERROR:
+			call->error = -rxrpc_kernel_get_error_number(skb);
+			call->state = AFS_CALL_ERROR;
+			_debug("Rcv LOCAL ERROR %d", call->error);
+			break;
+		default:
+			BUG();
+			break;
+		}
+
+		rxrpc_kernel_free_skb(skb);
+	}
+
+	/* make sure the queue is empty if the call is done with (we might have
+	 * aborted the call early because of an unmarshalling error) */
+	if (call->state >= AFS_CALL_COMPLETE) {
+		while ((skb = skb_dequeue(&call->rx_queue)))
+			rxrpc_kernel_free_skb(skb);
+		if (call->incoming) {
+			rxrpc_kernel_end_call(call->rxcall);
+			call->type->destructor(call);
+			ASSERT(skb_queue_empty(&call->rx_queue));
+			kfree(call);
+		}
+	}
+
+	_leave("");
+}
+
+/*
+ * wait synchronously for a call to complete
+ */
+static int afs_wait_for_call_to_complete(struct afs_call *call)
+{
+	struct sk_buff *skb;
+	int ret;
+
+	DECLARE_WAITQUEUE(myself, current);
+
+	_enter("");
+
+	add_wait_queue(&call->waitq, &myself);
+	for (;;) {
+		set_current_state(TASK_INTERRUPTIBLE);
+
+		/* deliver any messages that are in the queue */
+		if (!skb_queue_empty(&call->rx_queue)) {
+			__set_current_state(TASK_RUNNING);
+			afs_deliver_to_call(call);
+			continue;
+		}
+
+		ret = call->error;
+		if (call->state >= AFS_CALL_COMPLETE)
+			break;
+		ret = -EINTR;
+		if (signal_pending(current))
+			break;
+		schedule();
+	}
+
+	remove_wait_queue(&call->waitq, &myself);
+	__set_current_state(TASK_RUNNING);
+
+	/* kill the call */
+	if (call->state < AFS_CALL_COMPLETE) {
+		_debug("call incomplete");
+		rxrpc_kernel_abort_call(call->rxcall, RX_CALL_DEAD);
+		while ((skb = skb_dequeue(&call->rx_queue)))
+			rxrpc_kernel_free_skb(skb);
+	}
+
+	_debug("call complete");
+	rxrpc_kernel_end_call(call->rxcall);
+	call->type->destructor(call);
+	ASSERT(skb_queue_empty(&call->rx_queue));
+	kfree(call);
+	_leave(" = %d", ret);
+	return ret;
+}
+
+/*
+ * wake up a waiting call
+ */
+static void afs_wake_up_call_waiter(struct afs_call *call)
+{
+	wake_up(&call->waitq);
+}
+
+/*
+ * wake up an asynchronous call
+ */
+static void afs_wake_up_async_call(struct afs_call *call)
+{
+	_enter("");
+	queue_work(afs_async_calls, &call->async_work);
+}
+
+/*
+ * put a call into asynchronous mode
+ * - mustn't touch the call descriptor as the call my have completed by the
+ *   time we get here
+ */
+static int afs_dont_wait_for_call_to_complete(struct afs_call *call)
+{
+	_enter("");
+	return -EINPROGRESS;
+}
+
+/*
+ * delete an asynchronous call
+ */
+static void afs_delete_async_call(struct work_struct *work)
+{
+	struct afs_call *call =
+		container_of(work, struct afs_call, async_work);
+
+	_enter("");
+
+	ASSERT(skb_queue_empty(&call->rx_queue));
+	ASSERT(!work_pending(&call->async_work));
+	kfree(call);
+
+	_leave("");
+}
+
+/*
+ * perform processing on an asynchronous call
+ * - on a multiple-thread workqueue this work item may try to run on several
+ *   CPUs at the same time
+ */
+static void afs_process_async_call(struct work_struct *work)
+{
+	struct afs_call *call =
+		container_of(work, struct afs_call, async_work);
+
+	_enter("");
+
+	if (!skb_queue_empty(&call->rx_queue))
+		afs_deliver_to_call(call);
+
+	if (call->state >= AFS_CALL_COMPLETE && call->wait_mode) {
+		if (call->wait_mode->async_complete)
+			call->wait_mode->async_complete(call->reply,
+							call->error);
+		call->reply = NULL;
+
+		/* kill the call */
+		rxrpc_kernel_end_call(call->rxcall);
+		if (call->type->destructor)
+			call->type->destructor(call);
+
+		/* we can't just delete the call because the work item may be
+		 * queued */
+		PREPARE_WORK(&call->async_work, afs_delete_async_call);
+		queue_work(afs_async_calls, &call->async_work);
+	}
+
+	_leave("");
+}
+
+/*
+ * empty a socket buffer into a flat reply buffer
+ */
+void afs_transfer_reply(struct afs_call *call, struct sk_buff *skb)
+{
+	size_t len = skb->len;
+
+	if (skb_copy_bits(skb, 0, call->buffer + call->reply_size, len) < 0)
+		BUG();
+	call->reply_size += len;
+}
+
+/*
+ * accept the backlog of incoming calls
+ */
+static void afs_collect_incoming_call(struct work_struct *work)
+{
+	struct rxrpc_call *rxcall;
+	struct afs_call *call = NULL;
+	struct sk_buff *skb;
+
+	while ((skb = skb_dequeue(&afs_incoming_calls))) {
+		_debug("new call");
+
+		/* don't need the notification */
+		rxrpc_kernel_free_skb(skb);
+
+		if (!call) {
+			call = kzalloc(sizeof(struct afs_call), GFP_KERNEL);
+			if (!call) {
+				rxrpc_kernel_reject_call(afs_socket);
+				return;
+			}
+
+			INIT_WORK(&call->async_work, afs_process_async_call);
+			call->wait_mode = &afs_async_incoming_call;
+			call->type = &afs_RXCMxxxx;
+			init_waitqueue_head(&call->waitq);
+			skb_queue_head_init(&call->rx_queue);
+			call->state = AFS_CALL_AWAIT_OP_ID;
+		}
+
+		rxcall = rxrpc_kernel_accept_call(afs_socket,
+						  (unsigned long) call);
+		if (!IS_ERR(rxcall)) {
+			call->rxcall = rxcall;
+			call = NULL;
+		}
+	}
+
+	kfree(call);
+}
+
+/*
+ * grab the operation ID from an incoming cache manager call
+ */
+static int afs_deliver_cm_op_id(struct afs_call *call, struct sk_buff *skb,
+				bool last)
+{
+	size_t len = skb->len;
+	void *oibuf = (void *) &call->operation_ID;
+
+	_enter("{%u},{%zu},%d", call->offset, len, last);
+
+	ASSERTCMP(call->offset, <, 4);
+
+	/* the operation ID forms the first four bytes of the request data */
+	len = min_t(size_t, len, 4 - call->offset);
+	if (skb_copy_bits(skb, 0, oibuf + call->offset, len) < 0)
+		BUG();
+	if (!pskb_pull(skb, len))
+		BUG();
+	call->offset += len;
+
+	if (call->offset < 4) {
+		if (last) {
+			_leave(" = -EBADMSG [op ID short]");
+			return -EBADMSG;
+		}
+		_leave(" = 0 [incomplete]");
+		return 0;
+	}
+
+	call->state = AFS_CALL_AWAIT_REQUEST;
+
+	/* ask the cache manager to route the call (it'll change the call type
+	 * if successful) */
+	if (!afs_cm_incoming_call(call))
+		return -ENOTSUPP;
+
+	/* pass responsibility for the remainer of this message off to the
+	 * cache manager op */
+	return call->type->deliver(call, skb, last);
+}
+
+/*
+ * send an empty reply
+ */
+void afs_send_empty_reply(struct afs_call *call)
+{
+	struct msghdr msg;
+	struct iovec iov[1];
+
+	_enter("");
+
+	iov[0].iov_base		= NULL;
+	iov[0].iov_len		= 0;
+	msg.msg_name		= NULL;
+	msg.msg_namelen		= 0;
+	msg.msg_iov		= iov;
+	msg.msg_iovlen		= 0;
+	msg.msg_control		= NULL;
+	msg.msg_controllen	= 0;
+	msg.msg_flags		= 0;
+
+	call->state = AFS_CALL_AWAIT_ACK;
+	switch (rxrpc_kernel_send_data(call->rxcall, &msg, 0)) {
+	case 0:
+		_leave(" [replied]");
+		return;
+
+	case -ENOMEM:
+		_debug("oom");
+		rxrpc_kernel_abort_call(call->rxcall, RX_USER_ABORT);
+	default:
+		rxrpc_kernel_end_call(call->rxcall);
+		call->rxcall = NULL;
+		call->type->destructor(call);
+		ASSERT(skb_queue_empty(&call->rx_queue));
+		kfree(call);
+		_leave(" [error]");
+		return;
+	}
+}
+
+/*
+ * extract a piece of data from the received data socket buffers
+ */
+int afs_extract_data(struct afs_call *call, struct sk_buff *skb,
+		     bool last, void *buf, size_t count)
+{
+	size_t len = skb->len;
+
+	_enter("{%u},{%zu},%d,,%zu", call->offset, len, last, count);
+
+	ASSERTCMP(call->offset, <, count);
+
+	len = min_t(size_t, len, count - call->offset);
+	if (skb_copy_bits(skb, 0, buf + call->offset, len) < 0 ||
+	    !pskb_pull(skb, len))
+		BUG();
+	call->offset += len;
+
+	if (call->offset < count) {
+		if (last) {
+			_leave(" = -EBADMSG [%d < %lu]", call->offset, count);
+			return -EBADMSG;
+		}
+		_leave(" = -EAGAIN");
+		return -EAGAIN;
+	}
+	return 0;
+}
