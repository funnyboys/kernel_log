commit d919b33dafb3e222d23671b2bb06d119aede625f
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Mon Apr 6 20:09:01 2020 -0700

    proc: faster open/read/close with "permanent" files
    
    Now that "struct proc_ops" exist we can start putting there stuff which
    could not fly with VFS "struct file_operations"...
    
    Most of fs/proc/inode.c file is dedicated to make open/read/.../close
    reliable in the event of disappearing /proc entries which usually happens
    if module is getting removed.  Files like /proc/cpuinfo which never
    disappear simply do not need such protection.
    
    Save 2 atomic ops, 1 allocation, 1 free per open/read/close sequence for such
    "permanent" files.
    
    Enable "permanent" flag for
    
            /proc/cpuinfo
            /proc/kmsg
            /proc/modules
            /proc/slabinfo
            /proc/stat
            /proc/sysvipc/*
            /proc/swaps
    
    More will come once I figure out foolproof way to prevent out module
    authors from marking their stuff "permanent" for performance reasons
    when it is not.
    
    This should help with scalability: benchmark is "read /proc/cpuinfo R times
    by N threads scattered over the system".
    
            N       R       t, s (before)   t, s (after)
            -----------------------------------------------------
            64      4096    1.582458        1.530502        -3.2%
            256     4096    6.371926        6.125168        -3.9%
            1024    4096    25.64888        24.47528        -4.6%
    
    Benchmark source:
    
    #include <chrono>
    #include <iostream>
    #include <thread>
    #include <vector>
    
    #include <sys/types.h>
    #include <sys/stat.h>
    #include <fcntl.h>
    #include <unistd.h>
    
    const int NR_CPUS = sysconf(_SC_NPROCESSORS_ONLN);
    int N;
    const char *filename;
    int R;
    
    int xxx = 0;
    
    int glue(int n)
    {
            cpu_set_t m;
            CPU_ZERO(&m);
            CPU_SET(n, &m);
            return sched_setaffinity(0, sizeof(cpu_set_t), &m);
    }
    
    void f(int n)
    {
            glue(n % NR_CPUS);
    
            while (*(volatile int *)&xxx == 0) {
            }
    
            for (int i = 0; i < R; i++) {
                    int fd = open(filename, O_RDONLY);
                    char buf[4096];
                    ssize_t rv = read(fd, buf, sizeof(buf));
                    asm volatile ("" :: "g" (rv));
                    close(fd);
            }
    }
    
    int main(int argc, char *argv[])
    {
            if (argc < 4) {
                    std::cerr << "usage: " << argv[0] << ' ' << "N /proc/filename R
    ";
                    return 1;
            }
    
            N = atoi(argv[1]);
            filename = argv[2];
            R = atoi(argv[3]);
    
            for (int i = 0; i < NR_CPUS; i++) {
                    if (glue(i) == 0)
                            break;
            }
    
            std::vector<std::thread> T;
            T.reserve(N);
            for (int i = 0; i < N; i++) {
                    T.emplace_back(f, i);
            }
    
            auto t0 = std::chrono::system_clock::now();
            {
                    *(volatile int *)&xxx = 1;
                    for (auto& t: T) {
                            t.join();
                    }
            }
            auto t1 = std::chrono::system_clock::now();
            std::chrono::duration<double> dt = t1 - t0;
            std::cout << dt.count() << '
    ';
    
            return 0;
    }
    
    P.S.:
    Explicit randomization marker is added because adding non-function pointer
    will silently disable structure layout randomization.
    
    [akpm@linux-foundation.org: coding style fixes]
    Reported-by: kbuild test robot <lkp@intel.com>
    Reported-by: Dan Carpenter <dan.carpenter@oracle.com>
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Joe Perches <joe@perches.com>
    Link: http://lkml.kernel.org/r/20200222201539.GA22576@avx2
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/proc/stat.c b/fs/proc/stat.c
index 0449edf460f5..46b3293015fe 100644
--- a/fs/proc/stat.c
+++ b/fs/proc/stat.c
@@ -224,6 +224,7 @@ static int stat_open(struct inode *inode, struct file *file)
 }
 
 static const struct proc_ops stat_proc_ops = {
+	.proc_flags	= PROC_ENTRY_PERMANENT,
 	.proc_open	= stat_open,
 	.proc_read	= seq_read,
 	.proc_lseek	= seq_lseek,

commit 97a32539b9568bb653683349e5a76d02ff3c3e2c
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Mon Feb 3 17:37:17 2020 -0800

    proc: convert everything to "struct proc_ops"
    
    The most notable change is DEFINE_SHOW_ATTRIBUTE macro split in
    seq_file.h.
    
    Conversion rule is:
    
            llseek          => proc_lseek
            unlocked_ioctl  => proc_ioctl
    
            xxx             => proc_xxx
    
            delete ".owner = THIS_MODULE" line
    
    [akpm@linux-foundation.org: fix drivers/isdn/capi/kcapi_proc.c]
    [sfr@canb.auug.org.au: fix kernel/sched/psi.c]
      Link: http://lkml.kernel.org/r/20200122180545.36222f50@canb.auug.org.au
    Link: http://lkml.kernel.org/r/20191225172546.GB13378@avx2
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/proc/stat.c b/fs/proc/stat.c
index fd931d3e77be..0449edf460f5 100644
--- a/fs/proc/stat.c
+++ b/fs/proc/stat.c
@@ -223,16 +223,16 @@ static int stat_open(struct inode *inode, struct file *file)
 	return single_open_size(file, show_stat, NULL, size);
 }
 
-static const struct file_operations proc_stat_operations = {
-	.open		= stat_open,
-	.read		= seq_read,
-	.llseek		= seq_lseek,
-	.release	= single_release,
+static const struct proc_ops stat_proc_ops = {
+	.proc_open	= stat_open,
+	.proc_read	= seq_read,
+	.proc_lseek	= seq_lseek,
+	.proc_release	= single_release,
 };
 
 static int __init proc_stat_init(void)
 {
-	proc_create("stat", 0, NULL, &proc_stat_operations);
+	proc_create("stat", 0, NULL, &stat_proc_ops);
 	return 0;
 }
 fs_initcall(proc_stat_init);

commit 346da4d2c7ea39de65487b249aaa4733317a40ec
Author: Flavio Leitner <fbl@sysclose.org>
Date:   Thu Dec 5 03:03:44 2019 +0100

    sched/cputime, proc/stat: Fix incorrect guest nice cpustat value
    
    The value being used for guest_nice should be CPUTIME_GUEST_NICE
    and not CPUTIME_USER.
    
    Fixes: 26dae145a76c ("procfs: Use all-in-one vtime aware kcpustat accessor")
    Signed-off-by: Flavio Leitner <fbl@sysclose.org>
    Signed-off-by: Frederic Weisbecker <frederic@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20191205020344.14940-1-frederic@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/fs/proc/stat.c b/fs/proc/stat.c
index 37bdbec5b402..fd931d3e77be 100644
--- a/fs/proc/stat.c
+++ b/fs/proc/stat.c
@@ -134,7 +134,7 @@ static int show_stat(struct seq_file *p, void *v)
 		softirq		+= cpustat[CPUTIME_SOFTIRQ];
 		steal		+= cpustat[CPUTIME_STEAL];
 		guest		+= cpustat[CPUTIME_GUEST];
-		guest_nice	+= cpustat[CPUTIME_USER];
+		guest_nice	+= cpustat[CPUTIME_GUEST_NICE];
 		sum		+= kstat_cpu_irqs_sum(i);
 		sum		+= arch_irq_stat_cpu(i);
 
@@ -175,7 +175,7 @@ static int show_stat(struct seq_file *p, void *v)
 		softirq		= cpustat[CPUTIME_SOFTIRQ];
 		steal		= cpustat[CPUTIME_STEAL];
 		guest		= cpustat[CPUTIME_GUEST];
-		guest_nice	= cpustat[CPUTIME_USER];
+		guest_nice	= cpustat[CPUTIME_GUEST_NICE];
 		seq_printf(p, "cpu%d", i);
 		seq_put_decimal_ull(p, " ", nsec_to_clock_t(user));
 		seq_put_decimal_ull(p, " ", nsec_to_clock_t(nice));

commit 26dae145a76c3615588f263885904c6e567ff116
Author: Frederic Weisbecker <frederic@kernel.org>
Date:   Thu Nov 21 03:44:27 2019 +0100

    procfs: Use all-in-one vtime aware kcpustat accessor
    
    Now that we can read also user and guest time safely under vtime, use
    the relevant accessor to fix frozen kcpustat values on nohz_full CPUs.
    
    Reported-by: Yauheni Kaliuta <yauheni.kaliuta@redhat.com>
    Signed-off-by: Frederic Weisbecker <frederic@kernel.org>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Wanpeng Li <wanpengli@tencent.com>
    Link: https://lkml.kernel.org/r/20191121024430.19938-4-frederic@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/fs/proc/stat.c b/fs/proc/stat.c
index 5c6bd0ae3802..37bdbec5b402 100644
--- a/fs/proc/stat.c
+++ b/fs/proc/stat.c
@@ -120,20 +120,23 @@ static int show_stat(struct seq_file *p, void *v)
 	getboottime64(&boottime);
 
 	for_each_possible_cpu(i) {
-		struct kernel_cpustat *kcs = &kcpustat_cpu(i);
-
-		user += kcs->cpustat[CPUTIME_USER];
-		nice += kcs->cpustat[CPUTIME_NICE];
-		system += kcpustat_field(kcs, CPUTIME_SYSTEM, i);
-		idle += get_idle_time(kcs, i);
-		iowait += get_iowait_time(kcs, i);
-		irq += kcs->cpustat[CPUTIME_IRQ];
-		softirq += kcs->cpustat[CPUTIME_SOFTIRQ];
-		steal += kcs->cpustat[CPUTIME_STEAL];
-		guest += kcs->cpustat[CPUTIME_GUEST];
-		guest_nice += kcs->cpustat[CPUTIME_GUEST_NICE];
-		sum += kstat_cpu_irqs_sum(i);
-		sum += arch_irq_stat_cpu(i);
+		struct kernel_cpustat kcpustat;
+		u64 *cpustat = kcpustat.cpustat;
+
+		kcpustat_cpu_fetch(&kcpustat, i);
+
+		user		+= cpustat[CPUTIME_USER];
+		nice		+= cpustat[CPUTIME_NICE];
+		system		+= cpustat[CPUTIME_SYSTEM];
+		idle		+= get_idle_time(&kcpustat, i);
+		iowait		+= get_iowait_time(&kcpustat, i);
+		irq		+= cpustat[CPUTIME_IRQ];
+		softirq		+= cpustat[CPUTIME_SOFTIRQ];
+		steal		+= cpustat[CPUTIME_STEAL];
+		guest		+= cpustat[CPUTIME_GUEST];
+		guest_nice	+= cpustat[CPUTIME_USER];
+		sum		+= kstat_cpu_irqs_sum(i);
+		sum		+= arch_irq_stat_cpu(i);
 
 		for (j = 0; j < NR_SOFTIRQS; j++) {
 			unsigned int softirq_stat = kstat_softirqs_cpu(j, i);
@@ -157,19 +160,22 @@ static int show_stat(struct seq_file *p, void *v)
 	seq_putc(p, '\n');
 
 	for_each_online_cpu(i) {
-		struct kernel_cpustat *kcs = &kcpustat_cpu(i);
+		struct kernel_cpustat kcpustat;
+		u64 *cpustat = kcpustat.cpustat;
+
+		kcpustat_cpu_fetch(&kcpustat, i);
 
 		/* Copy values here to work around gcc-2.95.3, gcc-2.96 */
-		user = kcs->cpustat[CPUTIME_USER];
-		nice = kcs->cpustat[CPUTIME_NICE];
-		system = kcpustat_field(kcs, CPUTIME_SYSTEM, i);
-		idle = get_idle_time(kcs, i);
-		iowait = get_iowait_time(kcs, i);
-		irq = kcs->cpustat[CPUTIME_IRQ];
-		softirq = kcs->cpustat[CPUTIME_SOFTIRQ];
-		steal = kcs->cpustat[CPUTIME_STEAL];
-		guest = kcs->cpustat[CPUTIME_GUEST];
-		guest_nice = kcs->cpustat[CPUTIME_GUEST_NICE];
+		user		= cpustat[CPUTIME_USER];
+		nice		= cpustat[CPUTIME_NICE];
+		system		= cpustat[CPUTIME_SYSTEM];
+		idle		= get_idle_time(&kcpustat, i);
+		iowait		= get_iowait_time(&kcpustat, i);
+		irq		= cpustat[CPUTIME_IRQ];
+		softirq		= cpustat[CPUTIME_SOFTIRQ];
+		steal		= cpustat[CPUTIME_STEAL];
+		guest		= cpustat[CPUTIME_GUEST];
+		guest_nice	= cpustat[CPUTIME_USER];
 		seq_printf(p, "cpu%d", i);
 		seq_put_decimal_ull(p, " ", nsec_to_clock_t(user));
 		seq_put_decimal_ull(p, " ", nsec_to_clock_t(nice));

commit ae37fe5c07508e1c3dcdd41c9127e5d50d31013d
Author: Frederic Weisbecker <frederic@kernel.org>
Date:   Wed Oct 16 04:56:58 2019 +0200

    procfs: Use vtime aware kcpustat accessor to fetch CPUTIME_SYSTEM
    
    Now that we have a vtime safe kcpustat accessor for CPUTIME_SYSTEM, use
    it to start fixing frozen kcpustat values on nohz_full CPUs.
    
    Reported-by: Yauheni Kaliuta <yauheni.kaliuta@redhat.com>
    Signed-off-by: Frederic Weisbecker <frederic@kernel.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Jacek Anaszewski <jacek.anaszewski@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Pavel Machek <pavel@ucw.cz>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rafael J . Wysocki <rjw@rjwysocki.net>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: Wanpeng Li <wanpengli@tencent.com>
    Link: https://lkml.kernel.org/r/20191016025700.31277-13-frederic@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/fs/proc/stat.c b/fs/proc/stat.c
index 80c305f206bb..5c6bd0ae3802 100644
--- a/fs/proc/stat.c
+++ b/fs/proc/stat.c
@@ -124,7 +124,7 @@ static int show_stat(struct seq_file *p, void *v)
 
 		user += kcs->cpustat[CPUTIME_USER];
 		nice += kcs->cpustat[CPUTIME_NICE];
-		system += kcs->cpustat[CPUTIME_SYSTEM];
+		system += kcpustat_field(kcs, CPUTIME_SYSTEM, i);
 		idle += get_idle_time(kcs, i);
 		iowait += get_iowait_time(kcs, i);
 		irq += kcs->cpustat[CPUTIME_IRQ];
@@ -162,7 +162,7 @@ static int show_stat(struct seq_file *p, void *v)
 		/* Copy values here to work around gcc-2.95.3, gcc-2.96 */
 		user = kcs->cpustat[CPUTIME_USER];
 		nice = kcs->cpustat[CPUTIME_NICE];
-		system = kcs->cpustat[CPUTIME_SYSTEM];
+		system = kcpustat_field(kcs, CPUTIME_SYSTEM, i);
 		idle = get_idle_time(kcs, i);
 		iowait = get_iowait_time(kcs, i);
 		irq = kcs->cpustat[CPUTIME_IRQ];

commit 8dcd175bc3d50b78413c56d5b17d4bddd77412ef
Merge: afe6fe7036c6 fff04900ea79
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Mar 6 10:31:36 2019 -0800

    Merge branch 'akpm' (patches from Andrew)
    
    Merge misc updates from Andrew Morton:
    
     - a few misc things
    
     - ocfs2 updates
    
     - most of MM
    
    * emailed patches from Andrew Morton <akpm@linux-foundation.org>: (159 commits)
      tools/testing/selftests/proc/proc-self-syscall.c: remove duplicate include
      proc: more robust bulk read test
      proc: test /proc/*/maps, smaps, smaps_rollup, statm
      proc: use seq_puts() everywhere
      proc: read kernel cpu stat pointer once
      proc: remove unused argument in proc_pid_lookup()
      fs/proc/thread_self.c: code cleanup for proc_setup_thread_self()
      fs/proc/self.c: code cleanup for proc_setup_self()
      proc: return exit code 4 for skipped tests
      mm,mremap: bail out earlier in mremap_to under map pressure
      mm/sparse: fix a bad comparison
      mm/memory.c: do_fault: avoid usage of stale vm_area_struct
      writeback: fix inode cgroup switching comment
      mm/huge_memory.c: fix "orig_pud" set but not used
      mm/hotplug: fix an imbalance with DEBUG_PAGEALLOC
      mm/memcontrol.c: fix bad line in comment
      mm/cma.c: cma_declare_contiguous: correct err handling
      mm/page_ext.c: fix an imbalance with kmemleak
      mm/compaction: pass pgdat to too_many_isolated() instead of zone
      mm: remove zone_lru_lock() function, access ->lru_lock directly
      ...

commit 5713f35c0575a1137b705e13d10f8ee58f2ec7e8
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Tue Mar 5 15:50:32 2019 -0800

    proc: read kernel cpu stat pointer once
    
    Help gcc generate better code:
    
            $ ./scripts/bloat-o-meter ../vmlinux-000 ../vmlinux-001
            add/remove: 2/2 grow/shrink: 0/1 up/down: 92/-142 (-50)
            Function                                     old     new   delta
            get_iowait_time.isra                           -      46     +46
            get_idle_time.isra                             -      46     +46
            show_stat                                   1489    1477     -12
            get_iowait_time                               65       -     -65
            get_idle_time                                 65       -     -65
    
    Link: http://lkml.kernel.org/r/20190114195907.GA9680@avx2
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/proc/stat.c b/fs/proc/stat.c
index 535eda7857cf..49aa0a2b0d9e 100644
--- a/fs/proc/stat.c
+++ b/fs/proc/stat.c
@@ -23,21 +23,21 @@
 
 #ifdef arch_idle_time
 
-static u64 get_idle_time(int cpu)
+static u64 get_idle_time(struct kernel_cpustat *kcs, int cpu)
 {
 	u64 idle;
 
-	idle = kcpustat_cpu(cpu).cpustat[CPUTIME_IDLE];
+	idle = kcs->cpustat[CPUTIME_IDLE];
 	if (cpu_online(cpu) && !nr_iowait_cpu(cpu))
 		idle += arch_idle_time(cpu);
 	return idle;
 }
 
-static u64 get_iowait_time(int cpu)
+static u64 get_iowait_time(struct kernel_cpustat *kcs, int cpu)
 {
 	u64 iowait;
 
-	iowait = kcpustat_cpu(cpu).cpustat[CPUTIME_IOWAIT];
+	iowait = kcs->cpustat[CPUTIME_IOWAIT];
 	if (cpu_online(cpu) && nr_iowait_cpu(cpu))
 		iowait += arch_idle_time(cpu);
 	return iowait;
@@ -45,7 +45,7 @@ static u64 get_iowait_time(int cpu)
 
 #else
 
-static u64 get_idle_time(int cpu)
+static u64 get_idle_time(struct kernel_cpustat *kcs, int cpu)
 {
 	u64 idle, idle_usecs = -1ULL;
 
@@ -54,14 +54,14 @@ static u64 get_idle_time(int cpu)
 
 	if (idle_usecs == -1ULL)
 		/* !NO_HZ or cpu offline so we can rely on cpustat.idle */
-		idle = kcpustat_cpu(cpu).cpustat[CPUTIME_IDLE];
+		idle = kcs->cpustat[CPUTIME_IDLE];
 	else
 		idle = idle_usecs * NSEC_PER_USEC;
 
 	return idle;
 }
 
-static u64 get_iowait_time(int cpu)
+static u64 get_iowait_time(struct kernel_cpustat *kcs, int cpu)
 {
 	u64 iowait, iowait_usecs = -1ULL;
 
@@ -70,7 +70,7 @@ static u64 get_iowait_time(int cpu)
 
 	if (iowait_usecs == -1ULL)
 		/* !NO_HZ or cpu offline so we can rely on cpustat.iowait */
-		iowait = kcpustat_cpu(cpu).cpustat[CPUTIME_IOWAIT];
+		iowait = kcs->cpustat[CPUTIME_IOWAIT];
 	else
 		iowait = iowait_usecs * NSEC_PER_USEC;
 
@@ -95,16 +95,18 @@ static int show_stat(struct seq_file *p, void *v)
 	getboottime64(&boottime);
 
 	for_each_possible_cpu(i) {
-		user += kcpustat_cpu(i).cpustat[CPUTIME_USER];
-		nice += kcpustat_cpu(i).cpustat[CPUTIME_NICE];
-		system += kcpustat_cpu(i).cpustat[CPUTIME_SYSTEM];
-		idle += get_idle_time(i);
-		iowait += get_iowait_time(i);
-		irq += kcpustat_cpu(i).cpustat[CPUTIME_IRQ];
-		softirq += kcpustat_cpu(i).cpustat[CPUTIME_SOFTIRQ];
-		steal += kcpustat_cpu(i).cpustat[CPUTIME_STEAL];
-		guest += kcpustat_cpu(i).cpustat[CPUTIME_GUEST];
-		guest_nice += kcpustat_cpu(i).cpustat[CPUTIME_GUEST_NICE];
+		struct kernel_cpustat *kcs = &kcpustat_cpu(i);
+
+		user += kcs->cpustat[CPUTIME_USER];
+		nice += kcs->cpustat[CPUTIME_NICE];
+		system += kcs->cpustat[CPUTIME_SYSTEM];
+		idle += get_idle_time(kcs, i);
+		iowait += get_iowait_time(kcs, i);
+		irq += kcs->cpustat[CPUTIME_IRQ];
+		softirq += kcs->cpustat[CPUTIME_SOFTIRQ];
+		steal += kcs->cpustat[CPUTIME_STEAL];
+		guest += kcs->cpustat[CPUTIME_GUEST];
+		guest_nice += kcs->cpustat[CPUTIME_GUEST_NICE];
 		sum += kstat_cpu_irqs_sum(i);
 		sum += arch_irq_stat_cpu(i);
 
@@ -130,17 +132,19 @@ static int show_stat(struct seq_file *p, void *v)
 	seq_putc(p, '\n');
 
 	for_each_online_cpu(i) {
+		struct kernel_cpustat *kcs = &kcpustat_cpu(i);
+
 		/* Copy values here to work around gcc-2.95.3, gcc-2.96 */
-		user = kcpustat_cpu(i).cpustat[CPUTIME_USER];
-		nice = kcpustat_cpu(i).cpustat[CPUTIME_NICE];
-		system = kcpustat_cpu(i).cpustat[CPUTIME_SYSTEM];
-		idle = get_idle_time(i);
-		iowait = get_iowait_time(i);
-		irq = kcpustat_cpu(i).cpustat[CPUTIME_IRQ];
-		softirq = kcpustat_cpu(i).cpustat[CPUTIME_SOFTIRQ];
-		steal = kcpustat_cpu(i).cpustat[CPUTIME_STEAL];
-		guest = kcpustat_cpu(i).cpustat[CPUTIME_GUEST];
-		guest_nice = kcpustat_cpu(i).cpustat[CPUTIME_GUEST_NICE];
+		user = kcs->cpustat[CPUTIME_USER];
+		nice = kcs->cpustat[CPUTIME_NICE];
+		system = kcs->cpustat[CPUTIME_SYSTEM];
+		idle = get_idle_time(kcs, i);
+		iowait = get_iowait_time(kcs, i);
+		irq = kcs->cpustat[CPUTIME_IRQ];
+		softirq = kcs->cpustat[CPUTIME_SOFTIRQ];
+		steal = kcs->cpustat[CPUTIME_STEAL];
+		guest = kcs->cpustat[CPUTIME_GUEST];
+		guest_nice = kcs->cpustat[CPUTIME_GUEST_NICE];
 		seq_printf(p, "cpu%d", i);
 		seq_put_decimal_ull(p, " ", nsec_to_clock_t(user));
 		seq_put_decimal_ull(p, " ", nsec_to_clock_t(nice));

commit c2da3f1b711173b72378258496b49f74db7479de
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Feb 8 14:48:04 2019 +0100

    proc/stat: Make the interrupt statistics more efficient
    
    Waiman reported that on large systems with a large amount of interrupts the
    readout of /proc/stat takes a long time to sum up the interrupt
    statistics. In principle this is not a problem. but for unknown reasons
    some enterprise quality software reads /proc/stat with a high frequency.
    
    The reason for this is that interrupt statistics are accounted per cpu. So
    the /proc/stat logic has to sum up the interrupt stats for each interrupt.
    
    The interrupt core provides now a per interrupt summary counter which can
    be used to avoid the summation loops completely except for interrupts
    marked PER_CPU which are only a small fraction of the interrupt space if at
    all.
    
    Another simplification is to iterate only over the active interrupts and
    skip the potentially large gaps in the interrupt number space and just
    print zeros for the gaps without going into the interrupt core in the first
    place.
    
    Waiman provided test results from a 4-socket IvyBridge-EX system (60-core
    120-thread, 3016 irqs) excuting a test program which reads /proc/stat
    50,000 times:
    
       Before: 18.436s (sys 18.380s)
       After:   3.769s (sys  3.742s)
    
    Reported-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexey Dobriyan <adobriyan@gmail.com>
    Reviewed-by: Waiman Long <longman@redhat.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Reviewed-by: Davidlohr Bueso <dbueso@suse.de>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: linux-fsdevel@vger.kernel.org
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Miklos Szeredi <miklos@szeredi.hu>
    Cc: Daniel Colascione <dancol@google.com>
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: Randy Dunlap <rdunlap@infradead.org>
    Link: https://lkml.kernel.org/r/20190208135021.013828701@linutronix.de

diff --git a/fs/proc/stat.c b/fs/proc/stat.c
index 535eda7857cf..76175211b304 100644
--- a/fs/proc/stat.c
+++ b/fs/proc/stat.c
@@ -79,6 +79,31 @@ static u64 get_iowait_time(int cpu)
 
 #endif
 
+static void show_irq_gap(struct seq_file *p, unsigned int gap)
+{
+	static const char zeros[] = " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0";
+
+	while (gap > 0) {
+		unsigned int inc;
+
+		inc = min_t(unsigned int, gap, ARRAY_SIZE(zeros) / 2);
+		seq_write(p, zeros, 2 * inc);
+		gap -= inc;
+	}
+}
+
+static void show_all_irqs(struct seq_file *p)
+{
+	unsigned int i, next = 0;
+
+	for_each_active_irq(i) {
+		show_irq_gap(p, i - next);
+		seq_put_decimal_ull(p, " ", kstat_irqs_usr(i));
+		next = i + 1;
+	}
+	show_irq_gap(p, nr_irqs - next);
+}
+
 static int show_stat(struct seq_file *p, void *v)
 {
 	int i, j;
@@ -156,9 +181,7 @@ static int show_stat(struct seq_file *p, void *v)
 	}
 	seq_put_decimal_ull(p, "intr ", (unsigned long long)sum);
 
-	/* sum again ? it could be updated? */
-	for_each_irq_nr(j)
-		seq_put_decimal_ull(p, " ", kstat_irqs_usr(j));
+	show_all_irqs(p);
 
 	seq_printf(p,
 		"\nctxt %llu\n"

commit 9a27e97aaab9a25fac2e8976e98e42ab7f4a8fac
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Tue Aug 21 21:54:41 2018 -0700

    proc: use "unsigned int" in /proc/stat hook
    
    Number of CPUs is never high enough to force 64-bit arithmetic.
    Save couple of bytes on x86_64.
    
    Link: http://lkml.kernel.org/r/20180627200710.GC18434@avx2
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/proc/stat.c b/fs/proc/stat.c
index 59749dfaef67..535eda7857cf 100644
--- a/fs/proc/stat.c
+++ b/fs/proc/stat.c
@@ -183,7 +183,7 @@ static int show_stat(struct seq_file *p, void *v)
 
 static int stat_open(struct inode *inode, struct file *file)
 {
-	size_t size = 1024 + 128 * num_online_cpus();
+	unsigned int size = 1024 + 128 * num_online_cpus();
 
 	/* minimum size to display an interrupt count : 2 bytes */
 	size += 2 * nr_irqs;

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/fs/proc/stat.c b/fs/proc/stat.c
index bd4e55f4aa20..59749dfaef67 100644
--- a/fs/proc/stat.c
+++ b/fs/proc/stat.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 #include <linux/cpumask.h>
 #include <linux/fs.h>
 #include <linux/init.h>

commit 32ef5517c298042ed58408545f475df43afe1f24
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sun Feb 5 11:48:36 2017 +0100

    sched/headers: Prepare to move cputime functionality from <linux/sched.h> into <linux/sched/cputime.h>
    
    Introduce a trivial, mostly empty <linux/sched/cputime.h> header
    to prepare for the moving of cputime functionality out of sched.h.
    
    Update all code that relies on these facilities.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/fs/proc/stat.c b/fs/proc/stat.c
index b95556e036bb..bd4e55f4aa20 100644
--- a/fs/proc/stat.c
+++ b/fs/proc/stat.c
@@ -10,7 +10,7 @@
 #include <linux/slab.h>
 #include <linux/time.h>
 #include <linux/irqnr.h>
-#include <linux/cputime.h>
+#include <linux/sched/cputime.h>
 #include <linux/tick.h>
 
 #ifndef arch_irq_stat_cpu

commit 03441a3482a31462c93509939a388877e3cd9261
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:35 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/stat.h>
    
    We are going to split <linux/sched/stat.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/stat.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/fs/proc/stat.c b/fs/proc/stat.c
index e47c3e8c4dfe..b95556e036bb 100644
--- a/fs/proc/stat.c
+++ b/fs/proc/stat.c
@@ -5,6 +5,7 @@
 #include <linux/kernel_stat.h>
 #include <linux/proc_fs.h>
 #include <linux/sched.h>
+#include <linux/sched/stat.h>
 #include <linux/seq_file.h>
 #include <linux/slab.h>
 #include <linux/time.h>

commit 42b425b3360a20ed08e0d735623f389bdf5e4500
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Jan 31 04:09:47 2017 +0100

    s390, sched/cputime: Make arch_cpu_idle_time() to return nsecs
    
    This way we don't need to deal with cputime_t details from the core code.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Wanpeng Li <wanpeng.li@hotmail.com>
    Link: http://lkml.kernel.org/r/1485832191-26889-32-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/fs/proc/stat.c b/fs/proc/stat.c
index 44475a44cbf1..e47c3e8c4dfe 100644
--- a/fs/proc/stat.c
+++ b/fs/proc/stat.c
@@ -27,7 +27,7 @@ static u64 get_idle_time(int cpu)
 
 	idle = kcpustat_cpu(cpu).cpustat[CPUTIME_IDLE];
 	if (cpu_online(cpu) && !nr_iowait_cpu(cpu))
-		idle += cputime_to_nsecs(arch_idle_time(cpu));
+		idle += arch_idle_time(cpu);
 	return idle;
 }
 
@@ -37,7 +37,7 @@ static u64 get_iowait_time(int cpu)
 
 	iowait = kcpustat_cpu(cpu).cpustat[CPUTIME_IOWAIT];
 	if (cpu_online(cpu) && nr_iowait_cpu(cpu))
-		iowait += cputime_to_nsecs(arch_idle_time(cpu));
+		iowait += arch_idle_time(cpu);
 	return iowait;
 }
 

commit 7fb1327ee9b92fca27662f9b9d60c7c3376d6c69
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Jan 31 04:09:19 2017 +0100

    sched/cputime: Convert kcpustat to nsecs
    
    Kernel CPU stats are stored in cputime_t which is an architecture
    defined type, and hence a bit opaque and requiring accessors and mutators
    for any operation.
    
    Converting them to nsecs simplifies the code and is one step toward
    the removal of cputime_t in the core code.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    Cc: Wanpeng Li <wanpeng.li@hotmail.com>
    Link: http://lkml.kernel.org/r/1485832191-26889-4-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/fs/proc/stat.c b/fs/proc/stat.c
index d700c42b3572..44475a44cbf1 100644
--- a/fs/proc/stat.c
+++ b/fs/proc/stat.c
@@ -21,23 +21,23 @@
 
 #ifdef arch_idle_time
 
-static cputime64_t get_idle_time(int cpu)
+static u64 get_idle_time(int cpu)
 {
-	cputime64_t idle;
+	u64 idle;
 
 	idle = kcpustat_cpu(cpu).cpustat[CPUTIME_IDLE];
 	if (cpu_online(cpu) && !nr_iowait_cpu(cpu))
-		idle += arch_idle_time(cpu);
+		idle += cputime_to_nsecs(arch_idle_time(cpu));
 	return idle;
 }
 
-static cputime64_t get_iowait_time(int cpu)
+static u64 get_iowait_time(int cpu)
 {
-	cputime64_t iowait;
+	u64 iowait;
 
 	iowait = kcpustat_cpu(cpu).cpustat[CPUTIME_IOWAIT];
 	if (cpu_online(cpu) && nr_iowait_cpu(cpu))
-		iowait += arch_idle_time(cpu);
+		iowait += cputime_to_nsecs(arch_idle_time(cpu));
 	return iowait;
 }
 
@@ -45,32 +45,32 @@ static cputime64_t get_iowait_time(int cpu)
 
 static u64 get_idle_time(int cpu)
 {
-	u64 idle, idle_time = -1ULL;
+	u64 idle, idle_usecs = -1ULL;
 
 	if (cpu_online(cpu))
-		idle_time = get_cpu_idle_time_us(cpu, NULL);
+		idle_usecs = get_cpu_idle_time_us(cpu, NULL);
 
-	if (idle_time == -1ULL)
+	if (idle_usecs == -1ULL)
 		/* !NO_HZ or cpu offline so we can rely on cpustat.idle */
 		idle = kcpustat_cpu(cpu).cpustat[CPUTIME_IDLE];
 	else
-		idle = usecs_to_cputime64(idle_time);
+		idle = idle_usecs * NSEC_PER_USEC;
 
 	return idle;
 }
 
 static u64 get_iowait_time(int cpu)
 {
-	u64 iowait, iowait_time = -1ULL;
+	u64 iowait, iowait_usecs = -1ULL;
 
 	if (cpu_online(cpu))
-		iowait_time = get_cpu_iowait_time_us(cpu, NULL);
+		iowait_usecs = get_cpu_iowait_time_us(cpu, NULL);
 
-	if (iowait_time == -1ULL)
+	if (iowait_usecs == -1ULL)
 		/* !NO_HZ or cpu offline so we can rely on cpustat.iowait */
 		iowait = kcpustat_cpu(cpu).cpustat[CPUTIME_IOWAIT];
 	else
-		iowait = usecs_to_cputime64(iowait_time);
+		iowait = iowait_usecs * NSEC_PER_USEC;
 
 	return iowait;
 }
@@ -115,16 +115,16 @@ static int show_stat(struct seq_file *p, void *v)
 	}
 	sum += arch_irq_stat();
 
-	seq_put_decimal_ull(p, "cpu  ", cputime64_to_clock_t(user));
-	seq_put_decimal_ull(p, " ", cputime64_to_clock_t(nice));
-	seq_put_decimal_ull(p, " ", cputime64_to_clock_t(system));
-	seq_put_decimal_ull(p, " ", cputime64_to_clock_t(idle));
-	seq_put_decimal_ull(p, " ", cputime64_to_clock_t(iowait));
-	seq_put_decimal_ull(p, " ", cputime64_to_clock_t(irq));
-	seq_put_decimal_ull(p, " ", cputime64_to_clock_t(softirq));
-	seq_put_decimal_ull(p, " ", cputime64_to_clock_t(steal));
-	seq_put_decimal_ull(p, " ", cputime64_to_clock_t(guest));
-	seq_put_decimal_ull(p, " ", cputime64_to_clock_t(guest_nice));
+	seq_put_decimal_ull(p, "cpu  ", nsec_to_clock_t(user));
+	seq_put_decimal_ull(p, " ", nsec_to_clock_t(nice));
+	seq_put_decimal_ull(p, " ", nsec_to_clock_t(system));
+	seq_put_decimal_ull(p, " ", nsec_to_clock_t(idle));
+	seq_put_decimal_ull(p, " ", nsec_to_clock_t(iowait));
+	seq_put_decimal_ull(p, " ", nsec_to_clock_t(irq));
+	seq_put_decimal_ull(p, " ", nsec_to_clock_t(softirq));
+	seq_put_decimal_ull(p, " ", nsec_to_clock_t(steal));
+	seq_put_decimal_ull(p, " ", nsec_to_clock_t(guest));
+	seq_put_decimal_ull(p, " ", nsec_to_clock_t(guest_nice));
 	seq_putc(p, '\n');
 
 	for_each_online_cpu(i) {
@@ -140,16 +140,16 @@ static int show_stat(struct seq_file *p, void *v)
 		guest = kcpustat_cpu(i).cpustat[CPUTIME_GUEST];
 		guest_nice = kcpustat_cpu(i).cpustat[CPUTIME_GUEST_NICE];
 		seq_printf(p, "cpu%d", i);
-		seq_put_decimal_ull(p, " ", cputime64_to_clock_t(user));
-		seq_put_decimal_ull(p, " ", cputime64_to_clock_t(nice));
-		seq_put_decimal_ull(p, " ", cputime64_to_clock_t(system));
-		seq_put_decimal_ull(p, " ", cputime64_to_clock_t(idle));
-		seq_put_decimal_ull(p, " ", cputime64_to_clock_t(iowait));
-		seq_put_decimal_ull(p, " ", cputime64_to_clock_t(irq));
-		seq_put_decimal_ull(p, " ", cputime64_to_clock_t(softirq));
-		seq_put_decimal_ull(p, " ", cputime64_to_clock_t(steal));
-		seq_put_decimal_ull(p, " ", cputime64_to_clock_t(guest));
-		seq_put_decimal_ull(p, " ", cputime64_to_clock_t(guest_nice));
+		seq_put_decimal_ull(p, " ", nsec_to_clock_t(user));
+		seq_put_decimal_ull(p, " ", nsec_to_clock_t(nice));
+		seq_put_decimal_ull(p, " ", nsec_to_clock_t(system));
+		seq_put_decimal_ull(p, " ", nsec_to_clock_t(idle));
+		seq_put_decimal_ull(p, " ", nsec_to_clock_t(iowait));
+		seq_put_decimal_ull(p, " ", nsec_to_clock_t(irq));
+		seq_put_decimal_ull(p, " ", nsec_to_clock_t(softirq));
+		seq_put_decimal_ull(p, " ", nsec_to_clock_t(steal));
+		seq_put_decimal_ull(p, " ", nsec_to_clock_t(guest));
+		seq_put_decimal_ull(p, " ", nsec_to_clock_t(guest_nice));
 		seq_putc(p, '\n');
 	}
 	seq_put_decimal_ull(p, "intr ", (unsigned long long)sum);

commit 75ba1d07fd6a494851db5132612944a9d4773f9c
Author: Joe Perches <joe@perches.com>
Date:   Fri Oct 7 17:02:20 2016 -0700

    seq/proc: modify seq_put_decimal_[u]ll to take a const char *, not char
    
    Allow some seq_puts removals by taking a string instead of a single
    char.
    
    [akpm@linux-foundation.org: update vmstat_show(), per Joe]
    Link: http://lkml.kernel.org/r/667e1cf3d436de91a5698170a1e98d882905e956.1470704995.git.joe@perches.com
    Signed-off-by: Joe Perches <joe@perches.com>
    Cc: Joe Perches <joe@perches.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/proc/stat.c b/fs/proc/stat.c
index 7907e456ac4f..d700c42b3572 100644
--- a/fs/proc/stat.c
+++ b/fs/proc/stat.c
@@ -115,17 +115,16 @@ static int show_stat(struct seq_file *p, void *v)
 	}
 	sum += arch_irq_stat();
 
-	seq_puts(p, "cpu ");
-	seq_put_decimal_ull(p, ' ', cputime64_to_clock_t(user));
-	seq_put_decimal_ull(p, ' ', cputime64_to_clock_t(nice));
-	seq_put_decimal_ull(p, ' ', cputime64_to_clock_t(system));
-	seq_put_decimal_ull(p, ' ', cputime64_to_clock_t(idle));
-	seq_put_decimal_ull(p, ' ', cputime64_to_clock_t(iowait));
-	seq_put_decimal_ull(p, ' ', cputime64_to_clock_t(irq));
-	seq_put_decimal_ull(p, ' ', cputime64_to_clock_t(softirq));
-	seq_put_decimal_ull(p, ' ', cputime64_to_clock_t(steal));
-	seq_put_decimal_ull(p, ' ', cputime64_to_clock_t(guest));
-	seq_put_decimal_ull(p, ' ', cputime64_to_clock_t(guest_nice));
+	seq_put_decimal_ull(p, "cpu  ", cputime64_to_clock_t(user));
+	seq_put_decimal_ull(p, " ", cputime64_to_clock_t(nice));
+	seq_put_decimal_ull(p, " ", cputime64_to_clock_t(system));
+	seq_put_decimal_ull(p, " ", cputime64_to_clock_t(idle));
+	seq_put_decimal_ull(p, " ", cputime64_to_clock_t(iowait));
+	seq_put_decimal_ull(p, " ", cputime64_to_clock_t(irq));
+	seq_put_decimal_ull(p, " ", cputime64_to_clock_t(softirq));
+	seq_put_decimal_ull(p, " ", cputime64_to_clock_t(steal));
+	seq_put_decimal_ull(p, " ", cputime64_to_clock_t(guest));
+	seq_put_decimal_ull(p, " ", cputime64_to_clock_t(guest_nice));
 	seq_putc(p, '\n');
 
 	for_each_online_cpu(i) {
@@ -141,23 +140,23 @@ static int show_stat(struct seq_file *p, void *v)
 		guest = kcpustat_cpu(i).cpustat[CPUTIME_GUEST];
 		guest_nice = kcpustat_cpu(i).cpustat[CPUTIME_GUEST_NICE];
 		seq_printf(p, "cpu%d", i);
-		seq_put_decimal_ull(p, ' ', cputime64_to_clock_t(user));
-		seq_put_decimal_ull(p, ' ', cputime64_to_clock_t(nice));
-		seq_put_decimal_ull(p, ' ', cputime64_to_clock_t(system));
-		seq_put_decimal_ull(p, ' ', cputime64_to_clock_t(idle));
-		seq_put_decimal_ull(p, ' ', cputime64_to_clock_t(iowait));
-		seq_put_decimal_ull(p, ' ', cputime64_to_clock_t(irq));
-		seq_put_decimal_ull(p, ' ', cputime64_to_clock_t(softirq));
-		seq_put_decimal_ull(p, ' ', cputime64_to_clock_t(steal));
-		seq_put_decimal_ull(p, ' ', cputime64_to_clock_t(guest));
-		seq_put_decimal_ull(p, ' ', cputime64_to_clock_t(guest_nice));
+		seq_put_decimal_ull(p, " ", cputime64_to_clock_t(user));
+		seq_put_decimal_ull(p, " ", cputime64_to_clock_t(nice));
+		seq_put_decimal_ull(p, " ", cputime64_to_clock_t(system));
+		seq_put_decimal_ull(p, " ", cputime64_to_clock_t(idle));
+		seq_put_decimal_ull(p, " ", cputime64_to_clock_t(iowait));
+		seq_put_decimal_ull(p, " ", cputime64_to_clock_t(irq));
+		seq_put_decimal_ull(p, " ", cputime64_to_clock_t(softirq));
+		seq_put_decimal_ull(p, " ", cputime64_to_clock_t(steal));
+		seq_put_decimal_ull(p, " ", cputime64_to_clock_t(guest));
+		seq_put_decimal_ull(p, " ", cputime64_to_clock_t(guest_nice));
 		seq_putc(p, '\n');
 	}
-	seq_printf(p, "intr %llu", (unsigned long long)sum);
+	seq_put_decimal_ull(p, "intr ", (unsigned long long)sum);
 
 	/* sum again ? it could be updated? */
 	for_each_irq_nr(j)
-		seq_put_decimal_ull(p, ' ', kstat_irqs_usr(j));
+		seq_put_decimal_ull(p, " ", kstat_irqs_usr(j));
 
 	seq_printf(p,
 		"\nctxt %llu\n"
@@ -171,10 +170,10 @@ static int show_stat(struct seq_file *p, void *v)
 		nr_running(),
 		nr_iowait());
 
-	seq_printf(p, "softirq %llu", (unsigned long long)sum_softirq);
+	seq_put_decimal_ull(p, "softirq ", (unsigned long long)sum_softirq);
 
 	for (i = 0; i < NR_SOFTIRQS; i++)
-		seq_put_decimal_ull(p, ' ', per_softirq_sums[i]);
+		seq_put_decimal_ull(p, " ", per_softirq_sums[i]);
 	seq_putc(p, '\n');
 
 	return 0;

commit 519ded5a89ec0e46e5b0867ba9f5752239b73898
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Tue Aug 2 14:03:22 2016 -0700

    procfs: avoid 32-bit time_t in /proc/*/stat
    
    /proc/stat shows (among lots of other things) the current boottime (i.e.
    number of seconds since boot).  While a 32-bit number is sufficient for
    this particular case, we want to get rid of the 'struct timespec'
    suffers from a 32-bit overflow in 2038.
    
    This changes the code to use a struct timespec64, which is known to be
    safe in all cases.
    
    Link: http://lkml.kernel.org/r/20160617201247.2292101-1-arnd@arndb.de
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/proc/stat.c b/fs/proc/stat.c
index 510413eb25b8..7907e456ac4f 100644
--- a/fs/proc/stat.c
+++ b/fs/proc/stat.c
@@ -80,19 +80,17 @@ static u64 get_iowait_time(int cpu)
 static int show_stat(struct seq_file *p, void *v)
 {
 	int i, j;
-	unsigned long jif;
 	u64 user, nice, system, idle, iowait, irq, softirq, steal;
 	u64 guest, guest_nice;
 	u64 sum = 0;
 	u64 sum_softirq = 0;
 	unsigned int per_softirq_sums[NR_SOFTIRQS] = {0};
-	struct timespec boottime;
+	struct timespec64 boottime;
 
 	user = nice = system = idle = iowait =
 		irq = softirq = steal = 0;
 	guest = guest_nice = 0;
-	getboottime(&boottime);
-	jif = boottime.tv_sec;
+	getboottime64(&boottime);
 
 	for_each_possible_cpu(i) {
 		user += kcpustat_cpu(i).cpustat[CPUTIME_USER];
@@ -163,12 +161,12 @@ static int show_stat(struct seq_file *p, void *v)
 
 	seq_printf(p,
 		"\nctxt %llu\n"
-		"btime %lu\n"
+		"btime %llu\n"
 		"processes %lu\n"
 		"procs_running %lu\n"
 		"procs_blocked %lu\n",
 		nr_context_switches(),
-		(unsigned long)jif,
+		(unsigned long long)boottime.tv_sec,
 		total_forks,
 		nr_running(),
 		nr_iowait());

commit c291ee622165cb2c8d4e7af63fffd499354a23be
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Dec 11 23:01:41 2014 +0100

    genirq: Prevent proc race against freeing of irq descriptors
    
    Since the rework of the sparse interrupt code to actually free the
    unused interrupt descriptors there exists a race between the /proc
    interfaces to the irq subsystem and the code which frees the interrupt
    descriptor.
    
    CPU0                            CPU1
                                    show_interrupts()
                                      desc = irq_to_desc(X);
    free_desc(desc)
      remove_from_radix_tree();
      kfree(desc);
                                      raw_spinlock_irq(&desc->lock);
    
    /proc/interrupts is the only interface which can actively corrupt
    kernel memory via the lock access. /proc/stat can only read from freed
    memory. Extremly hard to trigger, but possible.
    
    The interfaces in /proc/irq/N/ are not affected by this because the
    removal of the proc file is serialized in procfs against concurrent
    readers/writers. The removal happens before the descriptor is freed.
    
    For architectures which have CONFIG_SPARSE_IRQ=n this is a non issue
    as the descriptor is never freed. It's merely cleared out with the irq
    descriptor lock held. So any concurrent proc access will either see
    the old correct value or the cleared out ones.
    
    Protect the lookup and access to the irq descriptor in
    show_interrupts() with the sparse_irq_lock.
    
    Provide kstat_irqs_usr() which is protecting the lookup and access
    with sparse_irq_lock and switch /proc/stat to use it.
    
    Document the existing kstat_irqs interfaces so it's clear that the
    caller needs to take care about protection. The users of these
    interfaces are either not affected due to SPARSE_IRQ=n or already
    protected against removal.
    
    Fixes: 1f5a5b87f78f "genirq: Implement a sane sparse_irq allocator"
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable@vger.kernel.org

diff --git a/fs/proc/stat.c b/fs/proc/stat.c
index bf2d03f8fd3e..510413eb25b8 100644
--- a/fs/proc/stat.c
+++ b/fs/proc/stat.c
@@ -159,7 +159,7 @@ static int show_stat(struct seq_file *p, void *v)
 
 	/* sum again ? it could be updated? */
 	for_each_irq_nr(j)
-		seq_put_decimal_ull(p, ' ', kstat_irqs(j));
+		seq_put_decimal_ull(p, ' ', kstat_irqs_usr(j));
 
 	seq_printf(p,
 		"\nctxt %llu\n"

commit f74373a5cc7a0155d232c4e999648c7a95435bb2
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Wed Jul 2 15:22:37 2014 -0700

    /proc/stat: convert to single_open_size()
    
    These two patches are supposed to "fix" failed order-4 memory
    allocations which have been observed when reading /proc/stat.  The
    problem has been observed on s390 as well as on x86.
    
    To address the problem change the seq_file memory allocations to
    fallback to use vmalloc, so that allocations also work if memory is
    fragmented.
    
    This approach seems to be simpler and less intrusive than changing
    /proc/stat to use an interator.  Also it "fixes" other users as well,
    which use seq_file's single_open() interface.
    
    This patch (of 2):
    
    Use seq_file's single_open_size() to preallocate a buffer that is large
    enough to hold the whole output, instead of open coding it.  Also
    calculate the requested size using the number of online cpus instead of
    possible cpus, since the size of the output only depends on the number
    of online cpus.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Cc: Ian Kent <raven@themaw.net>
    Cc: Hendrik Brueckner <brueckner@linux.vnet.ibm.com>
    Cc: Thorsten Diehl <thorsten.diehl@de.ibm.com>
    Cc: Andrea Righi <andrea@betterlinux.com>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Stefan Bader <stefan.bader@canonical.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/proc/stat.c b/fs/proc/stat.c
index 9d231e9e5f0e..bf2d03f8fd3e 100644
--- a/fs/proc/stat.c
+++ b/fs/proc/stat.c
@@ -184,29 +184,11 @@ static int show_stat(struct seq_file *p, void *v)
 
 static int stat_open(struct inode *inode, struct file *file)
 {
-	size_t size = 1024 + 128 * num_possible_cpus();
-	char *buf;
-	struct seq_file *m;
-	int res;
+	size_t size = 1024 + 128 * num_online_cpus();
 
 	/* minimum size to display an interrupt count : 2 bytes */
 	size += 2 * nr_irqs;
-
-	/* don't ask for more than the kmalloc() max size */
-	if (size > KMALLOC_MAX_SIZE)
-		size = KMALLOC_MAX_SIZE;
-	buf = kmalloc(size, GFP_KERNEL);
-	if (!buf)
-		return -ENOMEM;
-
-	res = single_open(file, show_stat, NULL);
-	if (!res) {
-		m = file->private_data;
-		m->buf = buf;
-		m->size = ksize(buf);
-	} else
-		kfree(buf);
-	return res;
+	return single_open_size(file, show_stat, NULL, size);
 }
 
 static const struct file_operations proc_stat_operations = {

commit bfc3f0281e08066fa8111c3972cff6edc1049864
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Mar 5 16:33:42 2014 +0100

    cputime: Default implementation of nsecs -> cputime conversion
    
    The architectures that override cputime_t (s390, ppc) don't provide
    any version of nsecs_to_cputime(). Indeed this cputime_t implementation
    by backend only happens when CONFIG_VIRT_CPU_ACCOUNTING_NATIVE=y under
    which the core code doesn't make any use of nsecs_to_cputime().
    
    At least for now.
    
    We are going to make a broader use of it so lets provide a default
    version with a per usecs granularity. It should be good enough for most
    usecases.
    
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/fs/proc/stat.c b/fs/proc/stat.c
index 6f599c62f0cc..9d231e9e5f0e 100644
--- a/fs/proc/stat.c
+++ b/fs/proc/stat.c
@@ -9,7 +9,7 @@
 #include <linux/slab.h>
 #include <linux/time.h>
 #include <linux/irqnr.h>
-#include <asm/cputime.h>
+#include <linux/cputime.h>
 #include <linux/tick.h>
 
 #ifndef arch_irq_stat_cpu

commit abaf3787ac26ba33e2f75e76b1174c32254c25b0
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Thu Jan 23 15:55:45 2014 -0800

    fs/proc: don't use module_init for non-modular core code
    
    PROC_FS is a bool, so this code is either present or absent.  It will
    never be modular, so using module_init as an alias for __initcall is
    rather misleading.
    
    Fix this up now, so that we can relocate module_init from init.h into
    module.h in the future.  If we don't do this, we'd have to add module.h to
    obviously non-modular code, and that would be ugly at best.
    
    Note that direct use of __initcall is discouraged, vs.  one of the
    priority categorized subgroups.  As __initcall gets mapped onto
    device_initcall, our use of fs_initcall (which makes sense for fs code)
    will thus change these registrations from level 6-device to level 5-fs
    (i.e.  slightly earlier).  However no observable impact of that small
    difference has been observed during testing, or is expected.
    
    Also note that this change uncovers a missing semicolon bug in the
    registration of vmcore_init as an initcall.
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/proc/stat.c b/fs/proc/stat.c
index 1cf86c0e8689..6f599c62f0cc 100644
--- a/fs/proc/stat.c
+++ b/fs/proc/stat.c
@@ -221,4 +221,4 @@ static int __init proc_stat_init(void)
 	proc_create("stat", 0, NULL, &proc_stat_operations);
 	return 0;
 }
-module_init(proc_stat_init);
+fs_initcall(proc_stat_init);

commit 9e5e8deca74603357626471a9b44f05dea9e32b1
Author: Christoph Lameter <cl@linux.com>
Date:   Thu Jan 10 19:14:17 2013 +0000

    stat: Use size_t for sizes instead of unsigned
    
    On some platforms (such as IA64) the large page size may results in
    slab allocations to be allowed of numbers that do not fit in 32 bit.
    
    Acked-by: Glauber Costa <glommer@parallels.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/fs/proc/stat.c b/fs/proc/stat.c
index e296572c73ed..1cf86c0e8689 100644
--- a/fs/proc/stat.c
+++ b/fs/proc/stat.c
@@ -184,7 +184,7 @@ static int show_stat(struct seq_file *p, void *v)
 
 static int stat_open(struct inode *inode, struct file *file)
 {
-	unsigned size = 1024 + 128 * num_possible_cpus();
+	size_t size = 1024 + 128 * num_possible_cpus();
 	char *buf;
 	struct seq_file *m;
 	int res;

commit 7386cdbf2f57ea8cff3c9fde93f206e58b9fe13f
Author: Michal Hocko <mhocko@suse.cz>
Date:   Wed Oct 10 11:51:09 2012 +0530

    nohz: Fix idle ticks in cpu summary line of /proc/stat
    
    Git commit 09a1d34f8535ecf9 "nohz: Make idle/iowait counter update
    conditional" introduced a bug in regard to cpu hotplug. The effect is
    that the number of idle ticks in the cpu summary line in /proc/stat is
    still counting ticks for offline cpus.
    
    Reproduction is easy, just start a workload that keeps all cpus busy,
    switch off one or more cpus and then watch the idle field in top.
    On a dual-core with one cpu 100% busy and one offline cpu you will get
    something like this:
    
    %Cpu(s): 48.7 us,  1.3 sy,  0.0 ni, 50.0 id,  0.0 wa,  0.0 hi,  0.0 si,
    %0.0 st
    
    The problem is that an offline cpu still has ts->idle_active == 1.
    To fix this we should make sure that the cpu is online when calling
    get_cpu_idle_time_us and get_cpu_iowait_time_us.
    
    [Srivatsa: Rebased to current mainline]
    
    Reported-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Michal Hocko <mhocko@suse.cz>
    Reviewed-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Link: http://lkml.kernel.org/r/20121010061820.8999.57245.stgit@srivatsabhat.in.ibm.com
    Cc: deepthi@linux.vnet.ibm.com
    Cc: stable@vger.kernel.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/fs/proc/stat.c b/fs/proc/stat.c
index 64c3b3172367..e296572c73ed 100644
--- a/fs/proc/stat.c
+++ b/fs/proc/stat.c
@@ -45,10 +45,13 @@ static cputime64_t get_iowait_time(int cpu)
 
 static u64 get_idle_time(int cpu)
 {
-	u64 idle, idle_time = get_cpu_idle_time_us(cpu, NULL);
+	u64 idle, idle_time = -1ULL;
+
+	if (cpu_online(cpu))
+		idle_time = get_cpu_idle_time_us(cpu, NULL);
 
 	if (idle_time == -1ULL)
-		/* !NO_HZ so we can rely on cpustat.idle */
+		/* !NO_HZ or cpu offline so we can rely on cpustat.idle */
 		idle = kcpustat_cpu(cpu).cpustat[CPUTIME_IDLE];
 	else
 		idle = usecs_to_cputime64(idle_time);
@@ -58,10 +61,13 @@ static u64 get_idle_time(int cpu)
 
 static u64 get_iowait_time(int cpu)
 {
-	u64 iowait, iowait_time = get_cpu_iowait_time_us(cpu, NULL);
+	u64 iowait, iowait_time = -1ULL;
+
+	if (cpu_online(cpu))
+		iowait_time = get_cpu_iowait_time_us(cpu, NULL);
 
 	if (iowait_time == -1ULL)
-		/* !NO_HZ so we can rely on cpustat.iowait */
+		/* !NO_HZ or cpu offline so we can rely on cpustat.iowait */
 		iowait = kcpustat_cpu(cpu).cpustat[CPUTIME_IOWAIT];
 	else
 		iowait = usecs_to_cputime64(iowait_time);

commit cb85a6ed67e979c59a29b7b4e8217e755b951cf4
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Fri Mar 30 12:23:08 2012 +0200

    proc: stats: Use arch_idle_time for idle and iowait times if available
    
    Git commit a25cac5198d4ff28 "proc: Consider NO_HZ when printing idle and
    iowait times" changes the code for /proc/stat to use get_cpu_idle_time_us
    and get_cpu_iowait_time_us if the system is running with nohz enabled.
    For architectures which define arch_idle_time (currently s390 only)
    this is a change for the worse. The result of arch_idle_time is supposed
    to be the exact sleep time of the target cpu and should be used instead
    of the value kept by the scheduler.
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Reviewed-by: Michal Hocko <mhocko@suse.cz>
    Reviewed-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Link: http://lkml.kernel.org/r/20120330122308.18720283@de.ibm.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/fs/proc/stat.c b/fs/proc/stat.c
index 6a0c62d6e442..64c3b3172367 100644
--- a/fs/proc/stat.c
+++ b/fs/proc/stat.c
@@ -18,19 +18,39 @@
 #ifndef arch_irq_stat
 #define arch_irq_stat() 0
 #endif
-#ifndef arch_idle_time
-#define arch_idle_time(cpu) 0
-#endif
+
+#ifdef arch_idle_time
+
+static cputime64_t get_idle_time(int cpu)
+{
+	cputime64_t idle;
+
+	idle = kcpustat_cpu(cpu).cpustat[CPUTIME_IDLE];
+	if (cpu_online(cpu) && !nr_iowait_cpu(cpu))
+		idle += arch_idle_time(cpu);
+	return idle;
+}
+
+static cputime64_t get_iowait_time(int cpu)
+{
+	cputime64_t iowait;
+
+	iowait = kcpustat_cpu(cpu).cpustat[CPUTIME_IOWAIT];
+	if (cpu_online(cpu) && nr_iowait_cpu(cpu))
+		iowait += arch_idle_time(cpu);
+	return iowait;
+}
+
+#else
 
 static u64 get_idle_time(int cpu)
 {
 	u64 idle, idle_time = get_cpu_idle_time_us(cpu, NULL);
 
-	if (idle_time == -1ULL) {
+	if (idle_time == -1ULL)
 		/* !NO_HZ so we can rely on cpustat.idle */
 		idle = kcpustat_cpu(cpu).cpustat[CPUTIME_IDLE];
-		idle += arch_idle_time(cpu);
-	} else
+	else
 		idle = usecs_to_cputime64(idle_time);
 
 	return idle;
@@ -49,6 +69,8 @@ static u64 get_iowait_time(int cpu)
 	return iowait;
 }
 
+#endif
+
 static int show_stat(struct seq_file *p, void *v)
 {
 	int i, j;

commit 1ac101a5d675aca2426c5cd460c73fb95acb8391
Author: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
Date:   Fri Mar 23 15:02:54 2012 -0700

    procfs: add num_to_str() to speed up /proc/stat
    
    == stat_check.py
    num = 0
    with open("/proc/stat") as f:
            while num < 1000 :
                    data = f.read()
                    f.seek(0, 0)
                    num = num + 1
    ==
    
    perf shows
    
        20.39%  stat_check.py  [kernel.kallsyms]    [k] format_decode
        13.41%  stat_check.py  [kernel.kallsyms]    [k] number
        12.61%  stat_check.py  [kernel.kallsyms]    [k] vsnprintf
        10.85%  stat_check.py  [kernel.kallsyms]    [k] memcpy
         4.85%  stat_check.py  [kernel.kallsyms]    [k] radix_tree_lookup
         4.43%  stat_check.py  [kernel.kallsyms]    [k] seq_printf
    
    This patch removes most of calls to vsnprintf() by adding num_to_str()
    and seq_print_decimal_ull(), which prints decimal numbers without rich
    functions provided by printf().
    
    On my 8cpu box.
    == Before patch ==
    [root@bluextal test]# time ./stat_check.py
    
    real    0m0.150s
    user    0m0.026s
    sys     0m0.121s
    
    == After patch ==
    [root@bluextal test]# time ./stat_check.py
    
    real    0m0.055s
    user    0m0.022s
    sys     0m0.030s
    
    [akpm@linux-foundation.org: remove incorrect comment, use less statck in num_to_str(), move comment from .h to .c, simplify seq_put_decimal_ull()]
    [andrea@betterlinux.com: avoid breaking the ABI in /proc/stat]
    Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Andrea Righi <andrea@betterlinux.com>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Glauber Costa <glommer@parallels.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Paul Turner <pjt@google.com>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/proc/stat.c b/fs/proc/stat.c
index ac446114cd48..6a0c62d6e442 100644
--- a/fs/proc/stat.c
+++ b/fs/proc/stat.c
@@ -89,18 +89,19 @@ static int show_stat(struct seq_file *p, void *v)
 	}
 	sum += arch_irq_stat();
 
-	seq_printf(p, "cpu  %llu %llu %llu %llu %llu %llu %llu %llu %llu "
-		"%llu\n",
-		(unsigned long long)cputime64_to_clock_t(user),
-		(unsigned long long)cputime64_to_clock_t(nice),
-		(unsigned long long)cputime64_to_clock_t(system),
-		(unsigned long long)cputime64_to_clock_t(idle),
-		(unsigned long long)cputime64_to_clock_t(iowait),
-		(unsigned long long)cputime64_to_clock_t(irq),
-		(unsigned long long)cputime64_to_clock_t(softirq),
-		(unsigned long long)cputime64_to_clock_t(steal),
-		(unsigned long long)cputime64_to_clock_t(guest),
-		(unsigned long long)cputime64_to_clock_t(guest_nice));
+	seq_puts(p, "cpu ");
+	seq_put_decimal_ull(p, ' ', cputime64_to_clock_t(user));
+	seq_put_decimal_ull(p, ' ', cputime64_to_clock_t(nice));
+	seq_put_decimal_ull(p, ' ', cputime64_to_clock_t(system));
+	seq_put_decimal_ull(p, ' ', cputime64_to_clock_t(idle));
+	seq_put_decimal_ull(p, ' ', cputime64_to_clock_t(iowait));
+	seq_put_decimal_ull(p, ' ', cputime64_to_clock_t(irq));
+	seq_put_decimal_ull(p, ' ', cputime64_to_clock_t(softirq));
+	seq_put_decimal_ull(p, ' ', cputime64_to_clock_t(steal));
+	seq_put_decimal_ull(p, ' ', cputime64_to_clock_t(guest));
+	seq_put_decimal_ull(p, ' ', cputime64_to_clock_t(guest_nice));
+	seq_putc(p, '\n');
+
 	for_each_online_cpu(i) {
 		/* Copy values here to work around gcc-2.95.3, gcc-2.96 */
 		user = kcpustat_cpu(i).cpustat[CPUTIME_USER];
@@ -113,26 +114,24 @@ static int show_stat(struct seq_file *p, void *v)
 		steal = kcpustat_cpu(i).cpustat[CPUTIME_STEAL];
 		guest = kcpustat_cpu(i).cpustat[CPUTIME_GUEST];
 		guest_nice = kcpustat_cpu(i).cpustat[CPUTIME_GUEST_NICE];
-		seq_printf(p,
-			"cpu%d %llu %llu %llu %llu %llu %llu %llu %llu %llu "
-			"%llu\n",
-			i,
-			(unsigned long long)cputime64_to_clock_t(user),
-			(unsigned long long)cputime64_to_clock_t(nice),
-			(unsigned long long)cputime64_to_clock_t(system),
-			(unsigned long long)cputime64_to_clock_t(idle),
-			(unsigned long long)cputime64_to_clock_t(iowait),
-			(unsigned long long)cputime64_to_clock_t(irq),
-			(unsigned long long)cputime64_to_clock_t(softirq),
-			(unsigned long long)cputime64_to_clock_t(steal),
-			(unsigned long long)cputime64_to_clock_t(guest),
-			(unsigned long long)cputime64_to_clock_t(guest_nice));
+		seq_printf(p, "cpu%d", i);
+		seq_put_decimal_ull(p, ' ', cputime64_to_clock_t(user));
+		seq_put_decimal_ull(p, ' ', cputime64_to_clock_t(nice));
+		seq_put_decimal_ull(p, ' ', cputime64_to_clock_t(system));
+		seq_put_decimal_ull(p, ' ', cputime64_to_clock_t(idle));
+		seq_put_decimal_ull(p, ' ', cputime64_to_clock_t(iowait));
+		seq_put_decimal_ull(p, ' ', cputime64_to_clock_t(irq));
+		seq_put_decimal_ull(p, ' ', cputime64_to_clock_t(softirq));
+		seq_put_decimal_ull(p, ' ', cputime64_to_clock_t(steal));
+		seq_put_decimal_ull(p, ' ', cputime64_to_clock_t(guest));
+		seq_put_decimal_ull(p, ' ', cputime64_to_clock_t(guest_nice));
+		seq_putc(p, '\n');
 	}
 	seq_printf(p, "intr %llu", (unsigned long long)sum);
 
 	/* sum again ? it could be updated? */
 	for_each_irq_nr(j)
-		seq_printf(p, " %u", kstat_irqs(j));
+		seq_put_decimal_ull(p, ' ', kstat_irqs(j));
 
 	seq_printf(p,
 		"\nctxt %llu\n"
@@ -149,7 +148,7 @@ static int show_stat(struct seq_file *p, void *v)
 	seq_printf(p, "softirq %llu", (unsigned long long)sum_softirq);
 
 	for (i = 0; i < NR_SOFTIRQS; i++)
-		seq_printf(p, " %u", per_softirq_sums[i]);
+		seq_put_decimal_ull(p, ' ', per_softirq_sums[i]);
 	seq_putc(p, '\n');
 
 	return 0;

commit 59a32e2ce5eb809967cac4e718bc527beca83c59
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Fri Mar 23 15:02:53 2012 -0700

    proc: speed up /proc/stat handling
    
    On a typical 16 cpus machine, "cat /proc/stat" gives more than 4096 bytes,
    and is slow :
    
      # strace -T -o /tmp/STRACE cat /proc/stat | wc -c
      5826
      # grep "cpu " /tmp/STRACE
      read(0, "cpu  1949310 19 2144714 12117253"..., 32768) = 5826 <0.001504>
    
    Thats partly because show_stat() must be called twice since initial
    buffer size is too small (4096 bytes for less than 32 possible cpus)
    
    Fix this by :
    
     1) Taking into account nr_irqs in the initial buffer sizing.
    
     2) Using ksize() to allow better filling of initial buffer.
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Glauber Costa <glommer@parallels.com>
    Cc: Russell King - ARM Linux <linux@arm.linux.org.uk>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Paul Turner <pjt@google.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/proc/stat.c b/fs/proc/stat.c
index 121f77cfef76..ac446114cd48 100644
--- a/fs/proc/stat.c
+++ b/fs/proc/stat.c
@@ -157,11 +157,14 @@ static int show_stat(struct seq_file *p, void *v)
 
 static int stat_open(struct inode *inode, struct file *file)
 {
-	unsigned size = 4096 * (1 + num_possible_cpus() / 32);
+	unsigned size = 1024 + 128 * num_possible_cpus();
 	char *buf;
 	struct seq_file *m;
 	int res;
 
+	/* minimum size to display an interrupt count : 2 bytes */
+	size += 2 * nr_irqs;
+
 	/* don't ask for more than the kmalloc() max size */
 	if (size > KMALLOC_MAX_SIZE)
 		size = KMALLOC_MAX_SIZE;
@@ -173,7 +176,7 @@ static int stat_open(struct inode *inode, struct file *file)
 	if (!res) {
 		m = file->private_data;
 		m->buf = buf;
-		m->size = size;
+		m->size = ksize(buf);
 	} else
 		kfree(buf);
 	return res;

commit f7e6746ebae984ea67b0a1a1e23c7e6698240631
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sat Jan 14 00:01:01 2012 +0000

    sched/accounting, proc: Fix /proc/stat interrupts sum
    
    Commit 3292beb340c7688 ("sched/accounting: Change cpustat fields to an array")
    deleted the code which provides us with the sum of all interrupts in the
    system, causing vmstat to report zero interrupts occuring in the system.
    
    Fix this by restoring the code.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Tested-by: Russell King <rmk+kernel@arm.linux.org.uk> # [on ARM]
    Tested-by: Tony Luck <tony.luck@intel.com>
    Tested-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Glauber Costa <glommer@parallels.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Paul Tuner <pjt@google.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/fs/proc/stat.c b/fs/proc/stat.c
index d76ca6ae2b1b..121f77cfef76 100644
--- a/fs/proc/stat.c
+++ b/fs/proc/stat.c
@@ -77,6 +77,8 @@ static int show_stat(struct seq_file *p, void *v)
 		steal += kcpustat_cpu(i).cpustat[CPUTIME_STEAL];
 		guest += kcpustat_cpu(i).cpustat[CPUTIME_GUEST];
 		guest_nice += kcpustat_cpu(i).cpustat[CPUTIME_GUEST_NICE];
+		sum += kstat_cpu_irqs_sum(i);
+		sum += arch_irq_stat_cpu(i);
 
 		for (j = 0; j < NR_SOFTIRQS; j++) {
 			unsigned int softirq_stat = kstat_softirqs_cpu(j, i);

commit 0db49b72bce26341274b74fd968501489a361ae3
Merge: 35b740e4662e 1ac9bc6943ed
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jan 6 08:33:28 2012 -0800

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (40 commits)
      sched/tracing: Add a new tracepoint for sleeptime
      sched: Disable scheduler warnings during oopses
      sched: Fix cgroup movement of waking process
      sched: Fix cgroup movement of newly created process
      sched: Fix cgroup movement of forking process
      sched: Remove cfs bandwidth period check in tg_set_cfs_period()
      sched: Fix load-balance lock-breaking
      sched: Replace all_pinned with a generic flags field
      sched: Only queue remote wakeups when crossing cache boundaries
      sched: Add missing rcu_dereference() around ->real_parent usage
      [S390] fix cputime overflow in uptime_proc_show
      [S390] cputime: add sparse checking and cleanup
      sched: Mark parent and real_parent as __rcu
      sched, nohz: Fix missing RCU read lock
      sched, nohz: Set the NOHZ_BALANCE_KICK flag for idle load balancer
      sched, nohz: Fix the idle cpu check in nohz_idle_balance
      sched: Use jump_labels for sched_feat
      sched/accounting: Fix parameter passing in task_group_account_field
      sched/accounting: Fix user/system tick double accounting
      sched/accounting: Re-use scheduler statistics for the root cgroup
      ...
    
    Fix up conflicts in
     - arch/ia64/include/asm/cputime.h, include/asm-generic/cputime.h
            usecs_to_cputime64() vs the sparse cleanups
     - kernel/sched/fair.c, kernel/time/tick-sched.c
            scheduler changes in multiple branches

commit 34845636a184f3be91a531098192592cbe6db587
Author: Andreas Schwab <schwab@linux-m68k.org>
Date:   Wed Dec 28 15:57:15 2011 -0800

    procfs: do not confuse jiffies with cputime64_t
    
    Commit 2a95ea6c0d129b4 ("procfs: do not overflow get_{idle,iowait}_time
    for nohz") did not take into account that one some architectures jiffies
    and cputime use different units.
    
    This causes get_idle_time() to return numbers in the wrong units, making
    the idle time fields in /proc/stat wrong.
    
    Instead of converting the usec value returned by
    get_cpu_{idle,iowait}_time_us to units of jiffies, use the new function
    usecs_to_cputime64 to convert it to the correct unit of cputime64_t.
    
    Signed-off-by: Andreas Schwab <schwab@linux-m68k.org>
    Acked-by: Michal Hocko <mhocko@suse.cz>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: "Artem S. Tashkinov" <t.artem@mailcity.com>
    Cc: Dave Jones <davej@redhat.com>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/proc/stat.c b/fs/proc/stat.c
index 2a30d67dd6b8..0855e6f20391 100644
--- a/fs/proc/stat.c
+++ b/fs/proc/stat.c
@@ -32,7 +32,7 @@ static cputime64_t get_idle_time(int cpu)
 		idle = kstat_cpu(cpu).cpustat.idle;
 		idle = cputime64_add(idle, arch_idle_time(cpu));
 	} else
-		idle = nsecs_to_jiffies64(1000 * idle_time);
+		idle = usecs_to_cputime64(idle_time);
 
 	return idle;
 }
@@ -46,7 +46,7 @@ static cputime64_t get_iowait_time(int cpu)
 		/* !NO_HZ so we can rely on cpustat.iowait */
 		iowait = kstat_cpu(cpu).cpustat.iowait;
 	else
-		iowait = nsecs_to_jiffies64(1000 * iowait_time);
+		iowait = usecs_to_cputime64(iowait_time);
 
 	return iowait;
 }

commit 6a54aebf6978e9f296a4d3da3e40af425163c22e
Merge: 067491b7313c dc47ce90c3a8
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Dec 15 08:21:21 2011 +0100

    Merge commit 'v3.2-rc5' into sched/core
    
    Merge reason: Pick up the latest fixes.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 2a95ea6c0d129b4568fb64e1deda16ceb20e6636
Author: Michal Hocko <mhocko@suse.cz>
Date:   Thu Dec 8 14:34:32 2011 -0800

    procfs: do not overflow get_{idle,iowait}_time for nohz
    
    Since commit a25cac5198d4 ("proc: Consider NO_HZ when printing idle and
    iowait times") we are reporting idle/io_wait time also while a CPU is
    tickless.  We rely on get_{idle,iowait}_time functions to retrieve
    proper data.
    
    These functions, however, use usecs_to_cputime to translate micro
    seconds time to cputime64_t.  This is just an alias to usecs_to_jiffies
    which reduces the data type from u64 to unsigned int and also checks
    whether the given parameter overflows jiffies_to_usecs(MAX_JIFFY_OFFSET)
    and returns MAX_JIFFY_OFFSET in that case.
    
    When we overflow depends on CONFIG_HZ but especially for CONFIG_HZ_300
    it is quite low (1431649781) so we are getting MAX_JIFFY_OFFSET for
    >3000s! until we overflow unsigned int.  Just for reference
    CONFIG_HZ_100 has an overflow window around 20s, CONFIG_HZ_250 ~8s and
    CONFIG_HZ_1000 ~2s.
    
    This results in a bug when people saw [h]top going mad reporting 100%
    CPU usage even though there was basically no CPU load.  The reason was
    simply that /proc/stat stopped reporting idle/io_wait changes (and
    reported MAX_JIFFY_OFFSET) and so the only change happening was for user
    system time.
    
    Let's use nsecs_to_jiffies64 instead which doesn't reduce the precision
    to 32b type and it is much more appropriate for cumulative time values
    (unlike usecs_to_jiffies which intended for timeout calculations).
    
    Signed-off-by: Michal Hocko <mhocko@suse.cz>
    Tested-by: Artem S. Tashkinov <t.artem@mailcity.com>
    Cc: Dave Jones <davej@redhat.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/proc/stat.c b/fs/proc/stat.c
index 42b274da92c3..2a30d67dd6b8 100644
--- a/fs/proc/stat.c
+++ b/fs/proc/stat.c
@@ -32,7 +32,7 @@ static cputime64_t get_idle_time(int cpu)
 		idle = kstat_cpu(cpu).cpustat.idle;
 		idle = cputime64_add(idle, arch_idle_time(cpu));
 	} else
-		idle = usecs_to_cputime(idle_time);
+		idle = nsecs_to_jiffies64(1000 * idle_time);
 
 	return idle;
 }
@@ -46,7 +46,7 @@ static cputime64_t get_iowait_time(int cpu)
 		/* !NO_HZ so we can rely on cpustat.iowait */
 		iowait = kstat_cpu(cpu).cpustat.iowait;
 	else
-		iowait = usecs_to_cputime(iowait_time);
+		iowait = nsecs_to_jiffies64(1000 * iowait_time);
 
 	return iowait;
 }

commit 3292beb340c76884427faa1f5d6085719477d889
Author: Glauber Costa <glommer@parallels.com>
Date:   Mon Nov 28 14:45:17 2011 -0200

    sched/accounting: Change cpustat fields to an array
    
    This patch changes fields in cpustat from a structure, to an
    u64 array. Math gets easier, and the code is more flexible.
    
    Signed-off-by: Glauber Costa <glommer@parallels.com>
    Reviewed-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Paul Tuner <pjt@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1322498719-2255-2-git-send-email-glommer@parallels.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/fs/proc/stat.c b/fs/proc/stat.c
index 42b274da92c3..8a6ab666e9f8 100644
--- a/fs/proc/stat.c
+++ b/fs/proc/stat.c
@@ -22,29 +22,27 @@
 #define arch_idle_time(cpu) 0
 #endif
 
-static cputime64_t get_idle_time(int cpu)
+static u64 get_idle_time(int cpu)
 {
-	u64 idle_time = get_cpu_idle_time_us(cpu, NULL);
-	cputime64_t idle;
+	u64 idle, idle_time = get_cpu_idle_time_us(cpu, NULL);
 
 	if (idle_time == -1ULL) {
 		/* !NO_HZ so we can rely on cpustat.idle */
-		idle = kstat_cpu(cpu).cpustat.idle;
-		idle = cputime64_add(idle, arch_idle_time(cpu));
+		idle = kcpustat_cpu(cpu).cpustat[CPUTIME_IDLE];
+		idle += arch_idle_time(cpu);
 	} else
 		idle = usecs_to_cputime(idle_time);
 
 	return idle;
 }
 
-static cputime64_t get_iowait_time(int cpu)
+static u64 get_iowait_time(int cpu)
 {
-	u64 iowait_time = get_cpu_iowait_time_us(cpu, NULL);
-	cputime64_t iowait;
+	u64 iowait, iowait_time = get_cpu_iowait_time_us(cpu, NULL);
 
 	if (iowait_time == -1ULL)
 		/* !NO_HZ so we can rely on cpustat.iowait */
-		iowait = kstat_cpu(cpu).cpustat.iowait;
+		iowait = kcpustat_cpu(cpu).cpustat[CPUTIME_IOWAIT];
 	else
 		iowait = usecs_to_cputime(iowait_time);
 
@@ -55,33 +53,30 @@ static int show_stat(struct seq_file *p, void *v)
 {
 	int i, j;
 	unsigned long jif;
-	cputime64_t user, nice, system, idle, iowait, irq, softirq, steal;
-	cputime64_t guest, guest_nice;
+	u64 user, nice, system, idle, iowait, irq, softirq, steal;
+	u64 guest, guest_nice;
 	u64 sum = 0;
 	u64 sum_softirq = 0;
 	unsigned int per_softirq_sums[NR_SOFTIRQS] = {0};
 	struct timespec boottime;
 
 	user = nice = system = idle = iowait =
-		irq = softirq = steal = cputime64_zero;
-	guest = guest_nice = cputime64_zero;
+		irq = softirq = steal = 0;
+	guest = guest_nice = 0;
 	getboottime(&boottime);
 	jif = boottime.tv_sec;
 
 	for_each_possible_cpu(i) {
-		user = cputime64_add(user, kstat_cpu(i).cpustat.user);
-		nice = cputime64_add(nice, kstat_cpu(i).cpustat.nice);
-		system = cputime64_add(system, kstat_cpu(i).cpustat.system);
-		idle = cputime64_add(idle, get_idle_time(i));
-		iowait = cputime64_add(iowait, get_iowait_time(i));
-		irq = cputime64_add(irq, kstat_cpu(i).cpustat.irq);
-		softirq = cputime64_add(softirq, kstat_cpu(i).cpustat.softirq);
-		steal = cputime64_add(steal, kstat_cpu(i).cpustat.steal);
-		guest = cputime64_add(guest, kstat_cpu(i).cpustat.guest);
-		guest_nice = cputime64_add(guest_nice,
-			kstat_cpu(i).cpustat.guest_nice);
-		sum += kstat_cpu_irqs_sum(i);
-		sum += arch_irq_stat_cpu(i);
+		user += kcpustat_cpu(i).cpustat[CPUTIME_USER];
+		nice += kcpustat_cpu(i).cpustat[CPUTIME_NICE];
+		system += kcpustat_cpu(i).cpustat[CPUTIME_SYSTEM];
+		idle += get_idle_time(i);
+		iowait += get_iowait_time(i);
+		irq += kcpustat_cpu(i).cpustat[CPUTIME_IRQ];
+		softirq += kcpustat_cpu(i).cpustat[CPUTIME_SOFTIRQ];
+		steal += kcpustat_cpu(i).cpustat[CPUTIME_STEAL];
+		guest += kcpustat_cpu(i).cpustat[CPUTIME_GUEST];
+		guest_nice += kcpustat_cpu(i).cpustat[CPUTIME_GUEST_NICE];
 
 		for (j = 0; j < NR_SOFTIRQS; j++) {
 			unsigned int softirq_stat = kstat_softirqs_cpu(j, i);
@@ -106,16 +101,16 @@ static int show_stat(struct seq_file *p, void *v)
 		(unsigned long long)cputime64_to_clock_t(guest_nice));
 	for_each_online_cpu(i) {
 		/* Copy values here to work around gcc-2.95.3, gcc-2.96 */
-		user = kstat_cpu(i).cpustat.user;
-		nice = kstat_cpu(i).cpustat.nice;
-		system = kstat_cpu(i).cpustat.system;
+		user = kcpustat_cpu(i).cpustat[CPUTIME_USER];
+		nice = kcpustat_cpu(i).cpustat[CPUTIME_NICE];
+		system = kcpustat_cpu(i).cpustat[CPUTIME_SYSTEM];
 		idle = get_idle_time(i);
 		iowait = get_iowait_time(i);
-		irq = kstat_cpu(i).cpustat.irq;
-		softirq = kstat_cpu(i).cpustat.softirq;
-		steal = kstat_cpu(i).cpustat.steal;
-		guest = kstat_cpu(i).cpustat.guest;
-		guest_nice = kstat_cpu(i).cpustat.guest_nice;
+		irq = kcpustat_cpu(i).cpustat[CPUTIME_IRQ];
+		softirq = kcpustat_cpu(i).cpustat[CPUTIME_SOFTIRQ];
+		steal = kcpustat_cpu(i).cpustat[CPUTIME_STEAL];
+		guest = kcpustat_cpu(i).cpustat[CPUTIME_GUEST];
+		guest_nice = kcpustat_cpu(i).cpustat[CPUTIME_GUEST_NICE];
 		seq_printf(p,
 			"cpu%d %llu %llu %llu %llu %llu %llu %llu %llu %llu "
 			"%llu\n",

commit a25cac5198d4ff2842ccca63b423962848ad24b2
Author: Michal Hocko <mhocko@suse.cz>
Date:   Wed Aug 24 09:40:25 2011 +0200

    proc: Consider NO_HZ when printing idle and iowait times
    
    show_stat handler of the /proc/stat file relies on kstat_cpu(cpu)
    statistics when priting information about idle and iowait times.
    This is OK if we are not using tickless kernel (CONFIG_NO_HZ) because
    counters are updated periodically.
    With NO_HZ things got more tricky because we are not doing idle/iowait
    accounting while we are tickless so the value might get outdated.
    Users of /proc/stat will notice that by unchanged idle/iowait values
    which is then interpreted as 0% idle/iowait time. From the user space
    POV this is an unexpected behavior and a change of the interface.
    
    Let's fix this by using get_cpu_{idle,iowait}_time_us which accounts the
    total idle/iowait time since boot and it doesn't rely on sampling or any
    other periodic activity. Fall back to the previous behavior if NO_HZ is
    disabled or not configured.
    
    Signed-off-by: Michal Hocko <mhocko@suse.cz>
    Cc: Dave Jones <davej@redhat.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Link: http://lkml.kernel.org/r/39181366adac1b39cb6aa3cd53ff0f7c78d32676.1314172057.git.mhocko@suse.cz
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/fs/proc/stat.c b/fs/proc/stat.c
index 9758b654a1bc..42b274da92c3 100644
--- a/fs/proc/stat.c
+++ b/fs/proc/stat.c
@@ -10,6 +10,7 @@
 #include <linux/time.h>
 #include <linux/irqnr.h>
 #include <asm/cputime.h>
+#include <linux/tick.h>
 
 #ifndef arch_irq_stat_cpu
 #define arch_irq_stat_cpu(cpu) 0
@@ -21,6 +22,35 @@
 #define arch_idle_time(cpu) 0
 #endif
 
+static cputime64_t get_idle_time(int cpu)
+{
+	u64 idle_time = get_cpu_idle_time_us(cpu, NULL);
+	cputime64_t idle;
+
+	if (idle_time == -1ULL) {
+		/* !NO_HZ so we can rely on cpustat.idle */
+		idle = kstat_cpu(cpu).cpustat.idle;
+		idle = cputime64_add(idle, arch_idle_time(cpu));
+	} else
+		idle = usecs_to_cputime(idle_time);
+
+	return idle;
+}
+
+static cputime64_t get_iowait_time(int cpu)
+{
+	u64 iowait_time = get_cpu_iowait_time_us(cpu, NULL);
+	cputime64_t iowait;
+
+	if (iowait_time == -1ULL)
+		/* !NO_HZ so we can rely on cpustat.iowait */
+		iowait = kstat_cpu(cpu).cpustat.iowait;
+	else
+		iowait = usecs_to_cputime(iowait_time);
+
+	return iowait;
+}
+
 static int show_stat(struct seq_file *p, void *v)
 {
 	int i, j;
@@ -42,9 +72,8 @@ static int show_stat(struct seq_file *p, void *v)
 		user = cputime64_add(user, kstat_cpu(i).cpustat.user);
 		nice = cputime64_add(nice, kstat_cpu(i).cpustat.nice);
 		system = cputime64_add(system, kstat_cpu(i).cpustat.system);
-		idle = cputime64_add(idle, kstat_cpu(i).cpustat.idle);
-		idle = cputime64_add(idle, arch_idle_time(i));
-		iowait = cputime64_add(iowait, kstat_cpu(i).cpustat.iowait);
+		idle = cputime64_add(idle, get_idle_time(i));
+		iowait = cputime64_add(iowait, get_iowait_time(i));
 		irq = cputime64_add(irq, kstat_cpu(i).cpustat.irq);
 		softirq = cputime64_add(softirq, kstat_cpu(i).cpustat.softirq);
 		steal = cputime64_add(steal, kstat_cpu(i).cpustat.steal);
@@ -76,14 +105,12 @@ static int show_stat(struct seq_file *p, void *v)
 		(unsigned long long)cputime64_to_clock_t(guest),
 		(unsigned long long)cputime64_to_clock_t(guest_nice));
 	for_each_online_cpu(i) {
-
 		/* Copy values here to work around gcc-2.95.3, gcc-2.96 */
 		user = kstat_cpu(i).cpustat.user;
 		nice = kstat_cpu(i).cpustat.nice;
 		system = kstat_cpu(i).cpustat.system;
-		idle = kstat_cpu(i).cpustat.idle;
-		idle = cputime64_add(idle, arch_idle_time(i));
-		iowait = kstat_cpu(i).cpustat.iowait;
+		idle = get_idle_time(i);
+		iowait = get_iowait_time(i);
 		irq = kstat_cpu(i).cpustat.irq;
 		softirq = kstat_cpu(i).cpustat.softirq;
 		steal = kstat_cpu(i).cpustat.steal;

commit a4dbf0ec2aa3e8aca6e63f598095750c232d50f1
Author: Yuanhan Liu <yuanhan.liu@linux.intel.com>
Date:   Thu May 26 16:25:51 2011 -0700

    proc/stat: use defined macro KMALLOC_MAX_SIZE
    
    There is a macro for the max size kmalloc can allocate, so use it instead
    of a hardcoded number.
    
    Signed-off-by: Yuanhan Liu <yuanhan.liu@linux.intel.com>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/proc/stat.c b/fs/proc/stat.c
index 1cffa2b8a2fc..9758b654a1bc 100644
--- a/fs/proc/stat.c
+++ b/fs/proc/stat.c
@@ -138,9 +138,9 @@ static int stat_open(struct inode *inode, struct file *file)
 	struct seq_file *m;
 	int res;
 
-	/* don't ask for more than the kmalloc() max size, currently 128 KB */
-	if (size > 128 * 1024)
-		size = 128 * 1024;
+	/* don't ask for more than the kmalloc() max size */
+	if (size > KMALLOC_MAX_SIZE)
+		size = KMALLOC_MAX_SIZE;
 	buf = kmalloc(size, GFP_KERNEL);
 	if (!buf)
 		return -ENOMEM;

commit 9d6de12f70d2fb1487c4f482a21fed25fe74e0fd
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Wed Jan 12 17:00:32 2011 -0800

    proc: use seq_puts()/seq_putc() where possible
    
    For string without format specifiers, use seq_puts().
    For seq_printf("\n"), use seq_putc('\n').
    
       text    data     bss     dec     hex filename
      61866     488     112   62466    f402 fs/proc/proc.o
      61729     488     112   62329    f379 fs/proc/proc.o
      ----------------------------------------------------
                               -139
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/proc/stat.c b/fs/proc/stat.c
index e15a19c93bae..1cffa2b8a2fc 100644
--- a/fs/proc/stat.c
+++ b/fs/proc/stat.c
@@ -126,7 +126,7 @@ static int show_stat(struct seq_file *p, void *v)
 
 	for (i = 0; i < NR_SOFTIRQS; i++)
 		seq_printf(p, " %u", per_softirq_sums[i]);
-	seq_printf(p, "\n");
+	seq_putc(p, '\n');
 
 	return 0;
 }

commit 478735e38887077ac77a9756121b6ce0cb956e2f
Author: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
Date:   Wed Oct 27 15:34:15 2010 -0700

    /proc/stat: fix scalability of irq sum of all cpu
    
    In /proc/stat, the number of per-IRQ event is shown by making a sum each
    irq's events on all cpus.  But we can make use of kstat_irqs().
    
    kstat_irqs() do the same calculation, If !CONFIG_GENERIC_HARDIRQ,
    it's not a big cost. (Both of the number of cpus and irqs are small.)
    
    If a system is very big and CONFIG_GENERIC_HARDIRQ, it does
    
            for_each_irq()
                    for_each_cpu()
                            - look up a radix tree
                            - read desc->irq_stat[cpu]
    This seems not efficient. This patch adds kstat_irqs() for
    CONFIG_GENRIC_HARDIRQ and change the calculation as
    
            for_each_irq()
                    look up radix tree
                    for_each_cpu()
                            - read desc->irq_stat[cpu]
    
    This reduces cost.
    
    A test on (4096cpusp, 256 nodes, 4592 irqs) host (by Jack Steiner)
    
    %time cat /proc/stat > /dev/null
    
    Before Patch:    2.459 sec
    After Patch :     .561 sec
    
    [akpm@linux-foundation.org: unexport kstat_irqs, coding-style tweaks]
    [akpm@linux-foundation.org: fix unused variable 'per_irq_sum']
    Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Tested-by: Jack Steiner <steiner@sgi.com>
    Acked-by: Jack Steiner <steiner@sgi.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/proc/stat.c b/fs/proc/stat.c
index b80c620565bf..e15a19c93bae 100644
--- a/fs/proc/stat.c
+++ b/fs/proc/stat.c
@@ -31,7 +31,6 @@ static int show_stat(struct seq_file *p, void *v)
 	u64 sum_softirq = 0;
 	unsigned int per_softirq_sums[NR_SOFTIRQS] = {0};
 	struct timespec boottime;
-	unsigned int per_irq_sum;
 
 	user = nice = system = idle = iowait =
 		irq = softirq = steal = cputime64_zero;
@@ -108,13 +107,8 @@ static int show_stat(struct seq_file *p, void *v)
 	seq_printf(p, "intr %llu", (unsigned long long)sum);
 
 	/* sum again ? it could be updated? */
-	for_each_irq_nr(j) {
-		per_irq_sum = 0;
-		for_each_possible_cpu(i)
-			per_irq_sum += kstat_irqs_cpu(j, i);
-
-		seq_printf(p, " %u", per_irq_sum);
-	}
+	for_each_irq_nr(j)
+		seq_printf(p, " %u", kstat_irqs(j));
 
 	seq_printf(p,
 		"\nctxt %llu\n"

commit f2c66cd8eeddedb440f33bc0f5cec1ed7ae376cb
Author: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
Date:   Wed Oct 27 15:34:13 2010 -0700

    /proc/stat: scalability of irq num per cpu
    
    /proc/stat shows the total number of all interrupts to each cpu.  But when
    the number of IRQs are very large, it take very long time and 'cat
    /proc/stat' takes more than 10 secs.  This is because sum of all irq
    events are counted when /proc/stat is read.  This patch adds "sum of all
    irq" counter percpu and reduce read costs.
    
    The cost of reading /proc/stat is important because it's used by major
    applications as 'top', 'ps', 'w', etc....
    
    A test on a mechin (4096cpu, 256 nodes, 4592 irqs) shows
    
     %time cat /proc/stat > /dev/null
     Before Patch:  12.627 sec
     After  Patch:  2.459 sec
    
    Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Tested-by: Jack Steiner <steiner@sgi.com>
    Acked-by: Jack Steiner <steiner@sgi.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/proc/stat.c b/fs/proc/stat.c
index bf31b03fc275..b80c620565bf 100644
--- a/fs/proc/stat.c
+++ b/fs/proc/stat.c
@@ -52,9 +52,7 @@ static int show_stat(struct seq_file *p, void *v)
 		guest = cputime64_add(guest, kstat_cpu(i).cpustat.guest);
 		guest_nice = cputime64_add(guest_nice,
 			kstat_cpu(i).cpustat.guest_nice);
-		for_each_irq_nr(j) {
-			sum += kstat_irqs_cpu(j, i);
-		}
+		sum += kstat_cpu_irqs_sum(i);
 		sum += arch_irq_stat_cpu(i);
 
 		for (j = 0; j < NR_SOFTIRQS; j++) {

commit 5a0e3ad6af8660be21ca98a971cd00f331318c05
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Mar 24 17:04:11 2010 +0900

    include cleanup: Update gfp.h and slab.h includes to prepare for breaking implicit slab.h inclusion from percpu.h
    
    percpu.h is included by sched.h and module.h and thus ends up being
    included when building most .c files.  percpu.h includes slab.h which
    in turn includes gfp.h making everything defined by the two files
    universally available and complicating inclusion dependencies.
    
    percpu.h -> slab.h dependency is about to be removed.  Prepare for
    this change by updating users of gfp and slab facilities include those
    headers directly instead of assuming availability.  As this conversion
    needs to touch large number of source files, the following script is
    used as the basis of conversion.
    
      http://userweb.kernel.org/~tj/misc/slabh-sweep.py
    
    The script does the followings.
    
    * Scan files for gfp and slab usages and update includes such that
      only the necessary includes are there.  ie. if only gfp is used,
      gfp.h, if slab is used, slab.h.
    
    * When the script inserts a new include, it looks at the include
      blocks and try to put the new include such that its order conforms
      to its surrounding.  It's put in the include block which contains
      core kernel includes, in the same order that the rest are ordered -
      alphabetical, Christmas tree, rev-Xmas-tree or at the end if there
      doesn't seem to be any matching order.
    
    * If the script can't find a place to put a new include (mostly
      because the file doesn't have fitting include block), it prints out
      an error message indicating which .h file needs to be added to the
      file.
    
    The conversion was done in the following steps.
    
    1. The initial automatic conversion of all .c files updated slightly
       over 4000 files, deleting around 700 includes and adding ~480 gfp.h
       and ~3000 slab.h inclusions.  The script emitted errors for ~400
       files.
    
    2. Each error was manually checked.  Some didn't need the inclusion,
       some needed manual addition while adding it to implementation .h or
       embedding .c file was more appropriate for others.  This step added
       inclusions to around 150 files.
    
    3. The script was run again and the output was compared to the edits
       from #2 to make sure no file was left behind.
    
    4. Several build tests were done and a couple of problems were fixed.
       e.g. lib/decompress_*.c used malloc/free() wrappers around slab
       APIs requiring slab.h to be added manually.
    
    5. The script was run on all .h files but without automatically
       editing them as sprinkling gfp.h and slab.h inclusions around .h
       files could easily lead to inclusion dependency hell.  Most gfp.h
       inclusion directives were ignored as stuff from gfp.h was usually
       wildly available and often used in preprocessor macros.  Each
       slab.h inclusion directive was examined and added manually as
       necessary.
    
    6. percpu.h was updated not to include slab.h.
    
    7. Build test were done on the following configurations and failures
       were fixed.  CONFIG_GCOV_KERNEL was turned off for all tests (as my
       distributed build env didn't work with gcov compiles) and a few
       more options had to be turned off depending on archs to make things
       build (like ipr on powerpc/64 which failed due to missing writeq).
    
       * x86 and x86_64 UP and SMP allmodconfig and a custom test config.
       * powerpc and powerpc64 SMP allmodconfig
       * sparc and sparc64 SMP allmodconfig
       * ia64 SMP allmodconfig
       * s390 SMP allmodconfig
       * alpha SMP allmodconfig
       * um on x86_64 SMP allmodconfig
    
    8. percpu.h modifications were reverted so that it could be applied as
       a separate patch and serve as bisection point.
    
    Given the fact that I had only a couple of failures from tests on step
    6, I'm fairly confident about the coverage of this conversion patch.
    If there is a breakage, it's likely to be something in one of the arch
    headers which should be easily discoverable easily on most builds of
    the specific arch.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Guess-its-ok-by: Christoph Lameter <cl@linux-foundation.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Lee Schermerhorn <Lee.Schermerhorn@hp.com>

diff --git a/fs/proc/stat.c b/fs/proc/stat.c
index b9b7aad2003d..bf31b03fc275 100644
--- a/fs/proc/stat.c
+++ b/fs/proc/stat.c
@@ -1,6 +1,5 @@
 #include <linux/cpumask.h>
 #include <linux/fs.h>
-#include <linux/gfp.h>
 #include <linux/init.h>
 #include <linux/interrupt.h>
 #include <linux/kernel_stat.h>

commit ce0e7b28fb75cb003cfc8d0238613aaf1c55e797
Author: Ryota Ozaki <ozaki.ryota@gmail.com>
Date:   Sat Oct 24 01:20:10 2009 +0900

    sched, cpuacct: Fix niced guest time accounting
    
    CPU time of a guest is always accounted in 'user' time
    without concern for the nice value of its counterpart
    process although the guest is scheduled under the nice
    value.
    
    This patch fixes the defect and accounts cpu time of
    a niced guest in 'nice' time as same as a niced process.
    
    And also the patch adds 'guest_nice' to cpuacct. The
    value provides niced guest cpu time which is like 'nice'
    to 'user'.
    
    The original discussions can be found here:
    
      http://www.mail-archive.com/kvm@vger.kernel.org/msg23982.html
      http://www.mail-archive.com/kvm@vger.kernel.org/msg23860.html
    
    Signed-off-by: Ryota Ozaki <ozaki.ryota@gmail.com>
    Acked-by: Avi Kivity <avi@redhat.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1256314810-7897-1-git-send-email-ozaki.ryota@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/fs/proc/stat.c b/fs/proc/stat.c
index 7cc726c6d70a..b9b7aad2003d 100644
--- a/fs/proc/stat.c
+++ b/fs/proc/stat.c
@@ -27,7 +27,7 @@ static int show_stat(struct seq_file *p, void *v)
 	int i, j;
 	unsigned long jif;
 	cputime64_t user, nice, system, idle, iowait, irq, softirq, steal;
-	cputime64_t guest;
+	cputime64_t guest, guest_nice;
 	u64 sum = 0;
 	u64 sum_softirq = 0;
 	unsigned int per_softirq_sums[NR_SOFTIRQS] = {0};
@@ -36,7 +36,7 @@ static int show_stat(struct seq_file *p, void *v)
 
 	user = nice = system = idle = iowait =
 		irq = softirq = steal = cputime64_zero;
-	guest = cputime64_zero;
+	guest = guest_nice = cputime64_zero;
 	getboottime(&boottime);
 	jif = boottime.tv_sec;
 
@@ -51,6 +51,8 @@ static int show_stat(struct seq_file *p, void *v)
 		softirq = cputime64_add(softirq, kstat_cpu(i).cpustat.softirq);
 		steal = cputime64_add(steal, kstat_cpu(i).cpustat.steal);
 		guest = cputime64_add(guest, kstat_cpu(i).cpustat.guest);
+		guest_nice = cputime64_add(guest_nice,
+			kstat_cpu(i).cpustat.guest_nice);
 		for_each_irq_nr(j) {
 			sum += kstat_irqs_cpu(j, i);
 		}
@@ -65,7 +67,8 @@ static int show_stat(struct seq_file *p, void *v)
 	}
 	sum += arch_irq_stat();
 
-	seq_printf(p, "cpu  %llu %llu %llu %llu %llu %llu %llu %llu %llu\n",
+	seq_printf(p, "cpu  %llu %llu %llu %llu %llu %llu %llu %llu %llu "
+		"%llu\n",
 		(unsigned long long)cputime64_to_clock_t(user),
 		(unsigned long long)cputime64_to_clock_t(nice),
 		(unsigned long long)cputime64_to_clock_t(system),
@@ -74,7 +77,8 @@ static int show_stat(struct seq_file *p, void *v)
 		(unsigned long long)cputime64_to_clock_t(irq),
 		(unsigned long long)cputime64_to_clock_t(softirq),
 		(unsigned long long)cputime64_to_clock_t(steal),
-		(unsigned long long)cputime64_to_clock_t(guest));
+		(unsigned long long)cputime64_to_clock_t(guest),
+		(unsigned long long)cputime64_to_clock_t(guest_nice));
 	for_each_online_cpu(i) {
 
 		/* Copy values here to work around gcc-2.95.3, gcc-2.96 */
@@ -88,8 +92,10 @@ static int show_stat(struct seq_file *p, void *v)
 		softirq = kstat_cpu(i).cpustat.softirq;
 		steal = kstat_cpu(i).cpustat.steal;
 		guest = kstat_cpu(i).cpustat.guest;
+		guest_nice = kstat_cpu(i).cpustat.guest_nice;
 		seq_printf(p,
-			"cpu%d %llu %llu %llu %llu %llu %llu %llu %llu %llu\n",
+			"cpu%d %llu %llu %llu %llu %llu %llu %llu %llu %llu "
+			"%llu\n",
 			i,
 			(unsigned long long)cputime64_to_clock_t(user),
 			(unsigned long long)cputime64_to_clock_t(nice),
@@ -99,7 +105,8 @@ static int show_stat(struct seq_file *p, void *v)
 			(unsigned long long)cputime64_to_clock_t(irq),
 			(unsigned long long)cputime64_to_clock_t(softirq),
 			(unsigned long long)cputime64_to_clock_t(steal),
-			(unsigned long long)cputime64_to_clock_t(guest));
+			(unsigned long long)cputime64_to_clock_t(guest),
+			(unsigned long long)cputime64_to_clock_t(guest_nice));
 	}
 	seq_printf(p, "intr %llu", (unsigned long long)sum);
 

commit d3d64df21d3d0de675a0d3ffa7c10514f3644b30
Author: Keika Kobayashi <kobayashi.kk@ncos.nec.co.jp>
Date:   Wed Jun 17 16:25:55 2009 -0700

    proc: export statistics for softirq to /proc
    
    Export statistics for softirq in /proc/softirqs and /proc/stat.
    
    1. /proc/softirqs
    Implement /proc/softirqs which shows the number of softirq
    for each CPU like /proc/interrupts.
    
    2. /proc/stat
    Add the "softirq" line to /proc/stat.
    This line shows the number of softirq for all cpu.
    The first column is the total of all softirqs and
    each subsequent column is the total for particular softirq.
    
    [kosaki.motohiro@jp.fujitsu.com: remove redundant for_each_possible_cpu() loop]
    Signed-off-by: Keika Kobayashi <kobayashi.kk@ncos.nec.co.jp>
    Reviewed-by: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Eric Dumazet <dada1@cosmosbay.com>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/proc/stat.c b/fs/proc/stat.c
index 81e4eb60972e..7cc726c6d70a 100644
--- a/fs/proc/stat.c
+++ b/fs/proc/stat.c
@@ -29,6 +29,8 @@ static int show_stat(struct seq_file *p, void *v)
 	cputime64_t user, nice, system, idle, iowait, irq, softirq, steal;
 	cputime64_t guest;
 	u64 sum = 0;
+	u64 sum_softirq = 0;
+	unsigned int per_softirq_sums[NR_SOFTIRQS] = {0};
 	struct timespec boottime;
 	unsigned int per_irq_sum;
 
@@ -53,6 +55,13 @@ static int show_stat(struct seq_file *p, void *v)
 			sum += kstat_irqs_cpu(j, i);
 		}
 		sum += arch_irq_stat_cpu(i);
+
+		for (j = 0; j < NR_SOFTIRQS; j++) {
+			unsigned int softirq_stat = kstat_softirqs_cpu(j, i);
+
+			per_softirq_sums[j] += softirq_stat;
+			sum_softirq += softirq_stat;
+		}
 	}
 	sum += arch_irq_stat();
 
@@ -115,6 +124,12 @@ static int show_stat(struct seq_file *p, void *v)
 		nr_running(),
 		nr_iowait());
 
+	seq_printf(p, "softirq %llu", (unsigned long long)sum_softirq);
+
+	for (i = 0; i < NR_SOFTIRQS; i++)
+		seq_printf(p, " %u", per_softirq_sums[i]);
+	seq_printf(p, "\n");
+
 	return 0;
 }
 

commit e1c805309d19c69d4ebeac38724076fa86feacdf
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Thu Apr 23 13:58:08 2009 +0200

    [S390] /proc/stat idle field for idle cpus
    
    The cpu idle field in the output of /proc/stat is too small for cpus
    that have been idle for more than a tick. Add the architecture hook
    arch_idle_time that allows to add the not accounted idle time of a
    sleeping cpu without waking the cpu.
    
    The s390 implementation of arch_idle_time uses the already existing
    s390_idle_data per_cpu variable to find the sleep time of a neighboring
    idle cpu.
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/fs/proc/stat.c b/fs/proc/stat.c
index f75efa22df5e..81e4eb60972e 100644
--- a/fs/proc/stat.c
+++ b/fs/proc/stat.c
@@ -18,6 +18,9 @@
 #ifndef arch_irq_stat
 #define arch_irq_stat() 0
 #endif
+#ifndef arch_idle_time
+#define arch_idle_time(cpu) 0
+#endif
 
 static int show_stat(struct seq_file *p, void *v)
 {
@@ -40,6 +43,7 @@ static int show_stat(struct seq_file *p, void *v)
 		nice = cputime64_add(nice, kstat_cpu(i).cpustat.nice);
 		system = cputime64_add(system, kstat_cpu(i).cpustat.system);
 		idle = cputime64_add(idle, kstat_cpu(i).cpustat.idle);
+		idle = cputime64_add(idle, arch_idle_time(i));
 		iowait = cputime64_add(iowait, kstat_cpu(i).cpustat.iowait);
 		irq = cputime64_add(irq, kstat_cpu(i).cpustat.irq);
 		softirq = cputime64_add(softirq, kstat_cpu(i).cpustat.softirq);
@@ -69,6 +73,7 @@ static int show_stat(struct seq_file *p, void *v)
 		nice = kstat_cpu(i).cpustat.nice;
 		system = kstat_cpu(i).cpustat.system;
 		idle = kstat_cpu(i).cpustat.idle;
+		idle = cputime64_add(idle, arch_idle_time(i));
 		iowait = kstat_cpu(i).cpustat.iowait;
 		irq = kstat_cpu(i).cpustat.irq;
 		softirq = kstat_cpu(i).cpustat.softirq;

commit 26ddd8d5cac8a563953d5febe8c6e40909f7bce1
Author: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
Date:   Fri Dec 26 14:24:10 2008 +0900

    proc: remove ifdef CONFIG_SPARSE_IRQ from stat.c
    
    Impact: cleanup
    
    irq_desc can be NULL when CONFIG_SPARSE_IRQ=y only.
    therefore, NULL checking can move into kstat_irqs_cpu() of SPARSE_IRQ version.
    
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Acked-by: "Yinghai Lu" <yinghai@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/fs/proc/stat.c b/fs/proc/stat.c
index 3bb1cf1e7425..f75efa22df5e 100644
--- a/fs/proc/stat.c
+++ b/fs/proc/stat.c
@@ -9,6 +9,7 @@
 #include <linux/seq_file.h>
 #include <linux/slab.h>
 #include <linux/time.h>
+#include <linux/irqnr.h>
 #include <asm/cputime.h>
 
 #ifndef arch_irq_stat_cpu
@@ -45,10 +46,6 @@ static int show_stat(struct seq_file *p, void *v)
 		steal = cputime64_add(steal, kstat_cpu(i).cpustat.steal);
 		guest = cputime64_add(guest, kstat_cpu(i).cpustat.guest);
 		for_each_irq_nr(j) {
-#ifdef CONFIG_SPARSE_IRQ
-			if (!irq_to_desc(j))
-				continue;
-#endif
 			sum += kstat_irqs_cpu(j, i);
 		}
 		sum += arch_irq_stat_cpu(i);
@@ -95,12 +92,6 @@ static int show_stat(struct seq_file *p, void *v)
 	/* sum again ? it could be updated? */
 	for_each_irq_nr(j) {
 		per_irq_sum = 0;
-#ifdef CONFIG_SPARSE_IRQ
-		if (!irq_to_desc(j)) {
-			seq_printf(p, " %u", per_irq_sum);
-			continue;
-		}
-#endif
 		for_each_possible_cpu(i)
 			per_irq_sum += kstat_irqs_cpu(j, i);
 

commit 13bd41bc227a48d6cf8992a3286bf6eba3c71a0c
Author: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
Date:   Tue Dec 16 00:23:34 2008 -0800

    proc: enclose desc variable of show_stat() in CONFIG_SPARSE_IRQ
    
    Impact: restructure code to fix compiler warning
    
    commit 240d367b4e6c6e3c5075e034db14dba60a6f5fa7 moved desc usage point
    into #ifdef CONFIG_SPARSE_IRQ.
    
    Eliminate the desc variable, otherwise following warning happens:
    
     fs/proc/stat.c: In function 'show_stat':
     fs/proc/stat.c:31: warning: unused variable 'desc'
    
    [ akpm: cleaned up the patch to remove #ifdef ]
    
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/fs/proc/stat.c b/fs/proc/stat.c
index 3cb9492801c0..3bb1cf1e7425 100644
--- a/fs/proc/stat.c
+++ b/fs/proc/stat.c
@@ -27,7 +27,6 @@ static int show_stat(struct seq_file *p, void *v)
 	u64 sum = 0;
 	struct timespec boottime;
 	unsigned int per_irq_sum;
-	struct irq_desc *desc;
 
 	user = nice = system = idle = iowait =
 		irq = softirq = steal = cputime64_zero;
@@ -47,8 +46,7 @@ static int show_stat(struct seq_file *p, void *v)
 		guest = cputime64_add(guest, kstat_cpu(i).cpustat.guest);
 		for_each_irq_nr(j) {
 #ifdef CONFIG_SPARSE_IRQ
-			desc = irq_to_desc(j);
-			if (!desc)
+			if (!irq_to_desc(j))
 				continue;
 #endif
 			sum += kstat_irqs_cpu(j, i);
@@ -98,8 +96,7 @@ static int show_stat(struct seq_file *p, void *v)
 	for_each_irq_nr(j) {
 		per_irq_sum = 0;
 #ifdef CONFIG_SPARSE_IRQ
-		desc = irq_to_desc(j);
-		if (!desc) {
+		if (!irq_to_desc(j)) {
 			seq_printf(p, " %u", per_irq_sum);
 			continue;
 		}

commit 240d367b4e6c6e3c5075e034db14dba60a6f5fa7
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Mon Dec 8 14:06:17 2008 -0800

    sparseirq: fix Alpha build failure
    
    Impact: build fix on Alpha
    
    -tip testing found this build failure on the Alpha defconfig:
    
    /home/mingo/tip/fs/proc/stat.c: In function 'show_stat':
    /home/mingo/tip/fs/proc/stat.c:48: error: implicit declaration of function 'for_each_irq_desc'
    /home/mingo/tip/fs/proc/stat.c:48: error: expected ';' before '{' token
    
    can not use irq_desc() in stat.c on older architectures.
    
    Signed-off-by: Yinghai Lu <yinghai@kernel.orgg>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/fs/proc/stat.c b/fs/proc/stat.c
index a13431ab7c65..3cb9492801c0 100644
--- a/fs/proc/stat.c
+++ b/fs/proc/stat.c
@@ -45,9 +45,12 @@ static int show_stat(struct seq_file *p, void *v)
 		softirq = cputime64_add(softirq, kstat_cpu(i).cpustat.softirq);
 		steal = cputime64_add(steal, kstat_cpu(i).cpustat.steal);
 		guest = cputime64_add(guest, kstat_cpu(i).cpustat.guest);
-		for_each_irq_desc(j, desc) {
+		for_each_irq_nr(j) {
+#ifdef CONFIG_SPARSE_IRQ
+			desc = irq_to_desc(j);
 			if (!desc)
 				continue;
+#endif
 			sum += kstat_irqs_cpu(j, i);
 		}
 		sum += arch_irq_stat_cpu(i);
@@ -92,14 +95,17 @@ static int show_stat(struct seq_file *p, void *v)
 	seq_printf(p, "intr %llu", (unsigned long long)sum);
 
 	/* sum again ? it could be updated? */
-	for (j = 0; j < NR_IRQS; j++) {
-		desc = irq_to_desc(j);
+	for_each_irq_nr(j) {
 		per_irq_sum = 0;
-
-		if (desc) {
-			for_each_possible_cpu(i)
-				per_irq_sum += kstat_irqs_cpu(j, i);
+#ifdef CONFIG_SPARSE_IRQ
+		desc = irq_to_desc(j);
+		if (!desc) {
+			seq_printf(p, " %u", per_irq_sum);
+			continue;
 		}
+#endif
+		for_each_possible_cpu(i)
+			per_irq_sum += kstat_irqs_cpu(j, i);
 
 		seq_printf(p, " %u", per_irq_sum);
 	}

commit 0b8f1efad30bd58f89961b82dfe68b9edf8fd2ac
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Fri Dec 5 18:58:31 2008 -0800

    sparse irq_desc[] array: core kernel and x86 changes
    
    Impact: new feature
    
    Problem on distro kernels: irq_desc[NR_IRQS] takes megabytes of RAM with
    NR_CPUS set to large values. The goal is to be able to scale up to much
    larger NR_IRQS value without impacting the (important) common case.
    
    To solve this, we generalize irq_desc[NR_IRQS] to an (optional) array of
    irq_desc pointers.
    
    When CONFIG_SPARSE_IRQ=y is used, we use kzalloc_node to get irq_desc,
    this also makes the IRQ descriptors NUMA-local (to the site that calls
    request_irq()).
    
    This gets rid of the irq_cfg[] static array on x86 as well: irq_cfg now
    uses desc->chip_data for x86 to store irq_cfg.
    
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/fs/proc/stat.c b/fs/proc/stat.c
index 81904f07679d..a13431ab7c65 100644
--- a/fs/proc/stat.c
+++ b/fs/proc/stat.c
@@ -27,6 +27,7 @@ static int show_stat(struct seq_file *p, void *v)
 	u64 sum = 0;
 	struct timespec boottime;
 	unsigned int per_irq_sum;
+	struct irq_desc *desc;
 
 	user = nice = system = idle = iowait =
 		irq = softirq = steal = cputime64_zero;
@@ -44,10 +45,11 @@ static int show_stat(struct seq_file *p, void *v)
 		softirq = cputime64_add(softirq, kstat_cpu(i).cpustat.softirq);
 		steal = cputime64_add(steal, kstat_cpu(i).cpustat.steal);
 		guest = cputime64_add(guest, kstat_cpu(i).cpustat.guest);
-
-		for_each_irq_nr(j)
+		for_each_irq_desc(j, desc) {
+			if (!desc)
+				continue;
 			sum += kstat_irqs_cpu(j, i);
-
+		}
 		sum += arch_irq_stat_cpu(i);
 	}
 	sum += arch_irq_stat();
@@ -90,11 +92,14 @@ static int show_stat(struct seq_file *p, void *v)
 	seq_printf(p, "intr %llu", (unsigned long long)sum);
 
 	/* sum again ? it could be updated? */
-	for_each_irq_nr(j) {
+	for (j = 0; j < NR_IRQS; j++) {
+		desc = irq_to_desc(j);
 		per_irq_sum = 0;
 
-		for_each_possible_cpu(i)
-			per_irq_sum += kstat_irqs_cpu(j, i);
+		if (desc) {
+			for_each_possible_cpu(i)
+				per_irq_sum += kstat_irqs_cpu(j, i);
+		}
 
 		seq_printf(p, " %u", per_irq_sum);
 	}

commit df8106dbb59a8c167ea16631059ecb5f7d77da13
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Sun Oct 5 00:01:56 2008 +0400

    proc: move /proc/stat to fs/proc/stat.c
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>

diff --git a/fs/proc/stat.c b/fs/proc/stat.c
new file mode 100644
index 000000000000..81904f07679d
--- /dev/null
+++ b/fs/proc/stat.c
@@ -0,0 +1,153 @@
+#include <linux/cpumask.h>
+#include <linux/fs.h>
+#include <linux/gfp.h>
+#include <linux/init.h>
+#include <linux/interrupt.h>
+#include <linux/kernel_stat.h>
+#include <linux/proc_fs.h>
+#include <linux/sched.h>
+#include <linux/seq_file.h>
+#include <linux/slab.h>
+#include <linux/time.h>
+#include <asm/cputime.h>
+
+#ifndef arch_irq_stat_cpu
+#define arch_irq_stat_cpu(cpu) 0
+#endif
+#ifndef arch_irq_stat
+#define arch_irq_stat() 0
+#endif
+
+static int show_stat(struct seq_file *p, void *v)
+{
+	int i, j;
+	unsigned long jif;
+	cputime64_t user, nice, system, idle, iowait, irq, softirq, steal;
+	cputime64_t guest;
+	u64 sum = 0;
+	struct timespec boottime;
+	unsigned int per_irq_sum;
+
+	user = nice = system = idle = iowait =
+		irq = softirq = steal = cputime64_zero;
+	guest = cputime64_zero;
+	getboottime(&boottime);
+	jif = boottime.tv_sec;
+
+	for_each_possible_cpu(i) {
+		user = cputime64_add(user, kstat_cpu(i).cpustat.user);
+		nice = cputime64_add(nice, kstat_cpu(i).cpustat.nice);
+		system = cputime64_add(system, kstat_cpu(i).cpustat.system);
+		idle = cputime64_add(idle, kstat_cpu(i).cpustat.idle);
+		iowait = cputime64_add(iowait, kstat_cpu(i).cpustat.iowait);
+		irq = cputime64_add(irq, kstat_cpu(i).cpustat.irq);
+		softirq = cputime64_add(softirq, kstat_cpu(i).cpustat.softirq);
+		steal = cputime64_add(steal, kstat_cpu(i).cpustat.steal);
+		guest = cputime64_add(guest, kstat_cpu(i).cpustat.guest);
+
+		for_each_irq_nr(j)
+			sum += kstat_irqs_cpu(j, i);
+
+		sum += arch_irq_stat_cpu(i);
+	}
+	sum += arch_irq_stat();
+
+	seq_printf(p, "cpu  %llu %llu %llu %llu %llu %llu %llu %llu %llu\n",
+		(unsigned long long)cputime64_to_clock_t(user),
+		(unsigned long long)cputime64_to_clock_t(nice),
+		(unsigned long long)cputime64_to_clock_t(system),
+		(unsigned long long)cputime64_to_clock_t(idle),
+		(unsigned long long)cputime64_to_clock_t(iowait),
+		(unsigned long long)cputime64_to_clock_t(irq),
+		(unsigned long long)cputime64_to_clock_t(softirq),
+		(unsigned long long)cputime64_to_clock_t(steal),
+		(unsigned long long)cputime64_to_clock_t(guest));
+	for_each_online_cpu(i) {
+
+		/* Copy values here to work around gcc-2.95.3, gcc-2.96 */
+		user = kstat_cpu(i).cpustat.user;
+		nice = kstat_cpu(i).cpustat.nice;
+		system = kstat_cpu(i).cpustat.system;
+		idle = kstat_cpu(i).cpustat.idle;
+		iowait = kstat_cpu(i).cpustat.iowait;
+		irq = kstat_cpu(i).cpustat.irq;
+		softirq = kstat_cpu(i).cpustat.softirq;
+		steal = kstat_cpu(i).cpustat.steal;
+		guest = kstat_cpu(i).cpustat.guest;
+		seq_printf(p,
+			"cpu%d %llu %llu %llu %llu %llu %llu %llu %llu %llu\n",
+			i,
+			(unsigned long long)cputime64_to_clock_t(user),
+			(unsigned long long)cputime64_to_clock_t(nice),
+			(unsigned long long)cputime64_to_clock_t(system),
+			(unsigned long long)cputime64_to_clock_t(idle),
+			(unsigned long long)cputime64_to_clock_t(iowait),
+			(unsigned long long)cputime64_to_clock_t(irq),
+			(unsigned long long)cputime64_to_clock_t(softirq),
+			(unsigned long long)cputime64_to_clock_t(steal),
+			(unsigned long long)cputime64_to_clock_t(guest));
+	}
+	seq_printf(p, "intr %llu", (unsigned long long)sum);
+
+	/* sum again ? it could be updated? */
+	for_each_irq_nr(j) {
+		per_irq_sum = 0;
+
+		for_each_possible_cpu(i)
+			per_irq_sum += kstat_irqs_cpu(j, i);
+
+		seq_printf(p, " %u", per_irq_sum);
+	}
+
+	seq_printf(p,
+		"\nctxt %llu\n"
+		"btime %lu\n"
+		"processes %lu\n"
+		"procs_running %lu\n"
+		"procs_blocked %lu\n",
+		nr_context_switches(),
+		(unsigned long)jif,
+		total_forks,
+		nr_running(),
+		nr_iowait());
+
+	return 0;
+}
+
+static int stat_open(struct inode *inode, struct file *file)
+{
+	unsigned size = 4096 * (1 + num_possible_cpus() / 32);
+	char *buf;
+	struct seq_file *m;
+	int res;
+
+	/* don't ask for more than the kmalloc() max size, currently 128 KB */
+	if (size > 128 * 1024)
+		size = 128 * 1024;
+	buf = kmalloc(size, GFP_KERNEL);
+	if (!buf)
+		return -ENOMEM;
+
+	res = single_open(file, show_stat, NULL);
+	if (!res) {
+		m = file->private_data;
+		m->buf = buf;
+		m->size = size;
+	} else
+		kfree(buf);
+	return res;
+}
+
+static const struct file_operations proc_stat_operations = {
+	.open		= stat_open,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= single_release,
+};
+
+static int __init proc_stat_init(void)
+{
+	proc_create("stat", 0, NULL, &proc_stat_operations);
+	return 0;
+}
+module_init(proc_stat_init);
