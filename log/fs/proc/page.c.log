commit 97a32539b9568bb653683349e5a76d02ff3c3e2c
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Mon Feb 3 17:37:17 2020 -0800

    proc: convert everything to "struct proc_ops"
    
    The most notable change is DEFINE_SHOW_ATTRIBUTE macro split in
    seq_file.h.
    
    Conversion rule is:
    
            llseek          => proc_lseek
            unlocked_ioctl  => proc_ioctl
    
            xxx             => proc_xxx
    
            delete ".owner = THIS_MODULE" line
    
    [akpm@linux-foundation.org: fix drivers/isdn/capi/kcapi_proc.c]
    [sfr@canb.auug.org.au: fix kernel/sched/psi.c]
      Link: http://lkml.kernel.org/r/20200122180545.36222f50@canb.auug.org.au
    Link: http://lkml.kernel.org/r/20191225172546.GB13378@avx2
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/proc/page.c b/fs/proc/page.c
index 7a84f6fcfa68..f909243d4a66 100644
--- a/fs/proc/page.c
+++ b/fs/proc/page.c
@@ -89,9 +89,9 @@ static ssize_t kpagecount_read(struct file *file, char __user *buf,
 	return ret;
 }
 
-static const struct file_operations proc_kpagecount_operations = {
-	.llseek = mem_lseek,
-	.read = kpagecount_read,
+static const struct proc_ops kpagecount_proc_ops = {
+	.proc_lseek	= mem_lseek,
+	.proc_read	= kpagecount_read,
 };
 
 /* /proc/kpageflags - an array exposing page flags
@@ -263,9 +263,9 @@ static ssize_t kpageflags_read(struct file *file, char __user *buf,
 	return ret;
 }
 
-static const struct file_operations proc_kpageflags_operations = {
-	.llseek = mem_lseek,
-	.read = kpageflags_read,
+static const struct proc_ops kpageflags_proc_ops = {
+	.proc_lseek	= mem_lseek,
+	.proc_read	= kpageflags_read,
 };
 
 #ifdef CONFIG_MEMCG
@@ -317,18 +317,18 @@ static ssize_t kpagecgroup_read(struct file *file, char __user *buf,
 	return ret;
 }
 
-static const struct file_operations proc_kpagecgroup_operations = {
-	.llseek = mem_lseek,
-	.read = kpagecgroup_read,
+static const struct proc_ops kpagecgroup_proc_ops = {
+	.proc_lseek	= mem_lseek,
+	.proc_read	= kpagecgroup_read,
 };
 #endif /* CONFIG_MEMCG */
 
 static int __init proc_page_init(void)
 {
-	proc_create("kpagecount", S_IRUSR, NULL, &proc_kpagecount_operations);
-	proc_create("kpageflags", S_IRUSR, NULL, &proc_kpageflags_operations);
+	proc_create("kpagecount", S_IRUSR, NULL, &kpagecount_proc_ops);
+	proc_create("kpageflags", S_IRUSR, NULL, &kpageflags_proc_ops);
 #ifdef CONFIG_MEMCG
-	proc_create("kpagecgroup", S_IRUSR, NULL, &proc_kpagecgroup_operations);
+	proc_create("kpagecgroup", S_IRUSR, NULL, &kpagecgroup_proc_ops);
 #endif
 	return 0;
 }

commit abec749facfff7fece8732be2f8f0bf56d78950b
Author: David Hildenbrand <david@redhat.com>
Date:   Mon Feb 3 17:33:52 2020 -0800

    fs/proc/page.c: allow inspection of last section and fix end detection
    
    If max_pfn does not fall onto a section boundary, it is possible to
    inspect PFNs up to max_pfn, and PFNs above max_pfn, however, max_pfn
    itself can't be inspected.  We can have a valid (and online) memmap at and
    above max_pfn if max_pfn is not aligned to a section boundary.  The whole
    early section has a memmap and is marked online.  Being able to inspect
    the state of these PFNs is valuable for debugging, especially because
    max_pfn can change on memory hotplug and expose these memmaps.
    
    Also, querying page flags via "./page-types -r -a 0x144001,"
    (tools/vm/page-types.c) inside a x86-64 guest with 4160MB under QEMU
    results in an (almost) endless loop in user space, because the end is not
    detected properly when starting after max_pfn.
    
    Instead, let's allow to inspect all pages in the highest section and
    return 0 directly if we try to access pages above that section.
    
    While at it, check the count before adjusting it, to avoid masking user
    errors.
    
    Link: http://lkml.kernel.org/r/20191211163201.17179-3-david@redhat.com
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: Oscar Salvador <osalvador@suse.de>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Cc: Bob Picco <bob.picco@oracle.com>
    Cc: Daniel Jordan <daniel.m.jordan@oracle.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: Pavel Tatashin <pasha.tatashin@oracle.com>
    Cc: Steven Sistare <steven.sistare@oracle.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/proc/page.c b/fs/proc/page.c
index 7c952ee732e6..7a84f6fcfa68 100644
--- a/fs/proc/page.c
+++ b/fs/proc/page.c
@@ -21,6 +21,21 @@
 #define KPMMASK (KPMSIZE - 1)
 #define KPMBITS (KPMSIZE * BITS_PER_BYTE)
 
+static inline unsigned long get_max_dump_pfn(void)
+{
+#ifdef CONFIG_SPARSEMEM
+	/*
+	 * The memmap of early sections is completely populated and marked
+	 * online even if max_pfn does not fall on a section boundary -
+	 * pfn_to_online_page() will succeed on all pages. Allow inspecting
+	 * these memmaps.
+	 */
+	return round_up(max_pfn, PAGES_PER_SECTION);
+#else
+	return max_pfn;
+#endif
+}
+
 /* /proc/kpagecount - an array exposing page counts
  *
  * Each entry is a u64 representing the corresponding
@@ -29,6 +44,7 @@
 static ssize_t kpagecount_read(struct file *file, char __user *buf,
 			     size_t count, loff_t *ppos)
 {
+	const unsigned long max_dump_pfn = get_max_dump_pfn();
 	u64 __user *out = (u64 __user *)buf;
 	struct page *ppage;
 	unsigned long src = *ppos;
@@ -37,9 +53,11 @@ static ssize_t kpagecount_read(struct file *file, char __user *buf,
 	u64 pcount;
 
 	pfn = src / KPMSIZE;
-	count = min_t(size_t, count, (max_pfn * KPMSIZE) - src);
 	if (src & KPMMASK || count & KPMMASK)
 		return -EINVAL;
+	if (src >= max_dump_pfn * KPMSIZE)
+		return 0;
+	count = min_t(unsigned long, count, (max_dump_pfn * KPMSIZE) - src);
 
 	while (count > 0) {
 		/*
@@ -206,6 +224,7 @@ u64 stable_page_flags(struct page *page)
 static ssize_t kpageflags_read(struct file *file, char __user *buf,
 			     size_t count, loff_t *ppos)
 {
+	const unsigned long max_dump_pfn = get_max_dump_pfn();
 	u64 __user *out = (u64 __user *)buf;
 	struct page *ppage;
 	unsigned long src = *ppos;
@@ -213,9 +232,11 @@ static ssize_t kpageflags_read(struct file *file, char __user *buf,
 	ssize_t ret = 0;
 
 	pfn = src / KPMSIZE;
-	count = min_t(unsigned long, count, (max_pfn * KPMSIZE) - src);
 	if (src & KPMMASK || count & KPMMASK)
 		return -EINVAL;
+	if (src >= max_dump_pfn * KPMSIZE)
+		return 0;
+	count = min_t(unsigned long, count, (max_dump_pfn * KPMSIZE) - src);
 
 	while (count > 0) {
 		/*
@@ -251,6 +272,7 @@ static const struct file_operations proc_kpageflags_operations = {
 static ssize_t kpagecgroup_read(struct file *file, char __user *buf,
 				size_t count, loff_t *ppos)
 {
+	const unsigned long max_dump_pfn = get_max_dump_pfn();
 	u64 __user *out = (u64 __user *)buf;
 	struct page *ppage;
 	unsigned long src = *ppos;
@@ -259,9 +281,11 @@ static ssize_t kpagecgroup_read(struct file *file, char __user *buf,
 	u64 ino;
 
 	pfn = src / KPMSIZE;
-	count = min_t(unsigned long, count, (max_pfn * KPMSIZE) - src);
 	if (src & KPMMASK || count & KPMMASK)
 		return -EINVAL;
+	if (src >= max_dump_pfn * KPMSIZE)
+		return 0;
+	count = min_t(unsigned long, count, (max_dump_pfn * KPMSIZE) - src);
 
 	while (count > 0) {
 		/*

commit aad5f69bc161af489dbb5934868bd347282f0764
Author: David Hildenbrand <david@redhat.com>
Date:   Fri Oct 18 20:19:20 2019 -0700

    fs/proc/page.c: don't access uninitialized memmaps in fs/proc/page.c
    
    There are three places where we access uninitialized memmaps, namely:
    - /proc/kpagecount
    - /proc/kpageflags
    - /proc/kpagecgroup
    
    We have initialized memmaps either when the section is online or when the
    page was initialized to the ZONE_DEVICE.  Uninitialized memmaps contain
    garbage and in the worst case trigger kernel BUGs, especially with
    CONFIG_PAGE_POISONING.
    
    For example, not onlining a DIMM during boot and calling /proc/kpagecount
    with CONFIG_PAGE_POISONING:
    
      :/# cat /proc/kpagecount > tmp.test
      BUG: unable to handle page fault for address: fffffffffffffffe
      #PF: supervisor read access in kernel mode
      #PF: error_code(0x0000) - not-present page
      PGD 114616067 P4D 114616067 PUD 114618067 PMD 0
      Oops: 0000 [#1] SMP NOPTI
      CPU: 0 PID: 469 Comm: cat Not tainted 5.4.0-rc1-next-20191004+ #11
      Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS rel-1.12.1-0-ga5cab58e9a3f-prebuilt.qemu.4
      RIP: 0010:kpagecount_read+0xce/0x1e0
      Code: e8 09 83 e0 3f 48 0f a3 02 73 2d 4c 89 e7 48 c1 e7 06 48 03 3d ab 51 01 01 74 1d 48 8b 57 08 480
      RSP: 0018:ffffa14e409b7e78 EFLAGS: 00010202
      RAX: fffffffffffffffe RBX: 0000000000020000 RCX: 0000000000000000
      RDX: 0000000000000001 RSI: 00007f76b5595000 RDI: fffff35645000000
      RBP: 00007f76b5595000 R08: 0000000000000001 R09: 0000000000000000
      R10: 0000000000000000 R11: 0000000000000000 R12: 0000000000140000
      R13: 0000000000020000 R14: 00007f76b5595000 R15: ffffa14e409b7f08
      FS:  00007f76b577d580(0000) GS:ffff8f41bd400000(0000) knlGS:0000000000000000
      CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
      CR2: fffffffffffffffe CR3: 0000000078960000 CR4: 00000000000006f0
      Call Trace:
       proc_reg_read+0x3c/0x60
       vfs_read+0xc5/0x180
       ksys_read+0x68/0xe0
       do_syscall_64+0x5c/0xa0
       entry_SYSCALL_64_after_hwframe+0x49/0xbe
    
    For now, let's drop support for ZONE_DEVICE from the three pseudo files
    in order to fix this.  To distinguish offline memory (with garbage
    memmap) from ZONE_DEVICE memory with properly initialized memmaps, we
    would have to check get_dev_pagemap() and pfn_zone_device_reserved()
    right now.  The usage of both (especially, special casing devmem) is
    frowned upon and needs to be reworked.
    
    The fundamental issue we have is:
    
            if (pfn_to_online_page(pfn)) {
                    /* memmap initialized */
            } else if (pfn_valid(pfn)) {
                    /*
                     * ???
                     * a) offline memory. memmap garbage.
                     * b) devmem: memmap initialized to ZONE_DEVICE.
                     * c) devmem: reserved for driver. memmap garbage.
                     * (d) devmem: memmap currently initializing - garbage)
                     */
            }
    
    We'll leave the pfn_zone_device_reserved() check in stable_page_flags()
    in place as that function is also used from memory failure.  We now no
    longer dump information about pages that are not in use anymore -
    offline.
    
    Link: http://lkml.kernel.org/r/20191009142435.3975-2-david@redhat.com
    Fixes: f1dd2cd13c4b ("mm, memory_hotplug: do not associate hotadded memory to zones until online")      [visible after d0dc12e86b319]
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Reported-by: Qian Cai <cai@lca.pw>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Cc: Toshiki Fukasawa <t-fukasawa@vx.jp.nec.com>
    Cc: Pankaj gupta <pagupta@redhat.com>
    Cc: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Cc: Anthony Yznaga <anthony.yznaga@oracle.com>
    Cc: "Aneesh Kumar K.V" <aneesh.kumar@linux.ibm.com>
    Cc: <stable@vger.kernel.org>    [4.13+]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/proc/page.c b/fs/proc/page.c
index 544d1ee15aee..7c952ee732e6 100644
--- a/fs/proc/page.c
+++ b/fs/proc/page.c
@@ -42,10 +42,12 @@ static ssize_t kpagecount_read(struct file *file, char __user *buf,
 		return -EINVAL;
 
 	while (count > 0) {
-		if (pfn_valid(pfn))
-			ppage = pfn_to_page(pfn);
-		else
-			ppage = NULL;
+		/*
+		 * TODO: ZONE_DEVICE support requires to identify
+		 * memmaps that were actually initialized.
+		 */
+		ppage = pfn_to_online_page(pfn);
+
 		if (!ppage || PageSlab(ppage) || page_has_type(ppage))
 			pcount = 0;
 		else
@@ -216,10 +218,11 @@ static ssize_t kpageflags_read(struct file *file, char __user *buf,
 		return -EINVAL;
 
 	while (count > 0) {
-		if (pfn_valid(pfn))
-			ppage = pfn_to_page(pfn);
-		else
-			ppage = NULL;
+		/*
+		 * TODO: ZONE_DEVICE support requires to identify
+		 * memmaps that were actually initialized.
+		 */
+		ppage = pfn_to_online_page(pfn);
 
 		if (put_user(stable_page_flags(ppage), out)) {
 			ret = -EFAULT;
@@ -261,10 +264,11 @@ static ssize_t kpagecgroup_read(struct file *file, char __user *buf,
 		return -EINVAL;
 
 	while (count > 0) {
-		if (pfn_valid(pfn))
-			ppage = pfn_to_page(pfn);
-		else
-			ppage = NULL;
+		/*
+		 * TODO: ZONE_DEVICE support requires to identify
+		 * memmaps that were actually initialized.
+		 */
+		ppage = pfn_to_online_page(pfn);
 
 		if (ppage)
 			ino = page_cgroup_ino(ppage);

commit ca215086b14b89a0e70fc211314944aa6ce50020
Author: David Hildenbrand <david@redhat.com>
Date:   Tue Mar 5 15:42:23 2019 -0800

    mm: convert PG_balloon to PG_offline
    
    PG_balloon was introduced to implement page migration/compaction for
    pages inflated in virtio-balloon.  Nowadays, it is only a marker that a
    page is part of virtio-balloon and therefore logically offline.
    
    We also want to make use of this flag in other balloon drivers - for
    inflated pages or when onlining a section but keeping some pages offline
    (e.g.  used right now by XEN and Hyper-V via set_online_page_callback()).
    
    We are going to expose this flag to dump tools like makedumpfile.  But
    instead of exposing PG_balloon, let's generalize the concept of marking
    pages as logically offline, so it can be reused for other purposes later
    on.
    
    Rename PG_balloon to PG_offline.  This is an indicator that the page is
    logically offline, the content stale and that it should not be touched
    (e.g.  a hypervisor would have to allocate backing storage in order for
    the guest to dump an unused page).  We can then e.g.  exclude such pages
    from dumps.
    
    We replace and reuse KPF_BALLOON (23), as this shouldn't really harm
    (and for now the semantics stay the same).  In following patches, we
    will make use of this bit also in other balloon drivers.  While at it,
    document PGTABLE.
    
    [akpm@linux-foundation.org: fix comment text, per David]
    Link: http://lkml.kernel.org/r/20181119101616.8901-3-david@redhat.com
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Acked-by: Konstantin Khlebnikov <koct9i@gmail.com>
    Acked-by: Michael S. Tsirkin <mst@redhat.com>
    Acked-by: Pankaj gupta <pagupta@redhat.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Cc: Christian Hansen <chansen3@cisco.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Pavel Tatashin <pasha.tatashin@oracle.com>
    Cc: Alexander Duyck <alexander.h.duyck@linux.intel.com>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: Miles Chen <miles.chen@mediatek.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Kazuhito Hagio <k-hagio@ab.jp.nec.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Baoquan He <bhe@redhat.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Dave Young <dyoung@redhat.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Haiyang Zhang <haiyangz@microsoft.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Julien Freche <jfreche@vmware.com>
    Cc: Kairui Song <kasong@redhat.com>
    Cc: "K. Y. Srinivasan" <kys@microsoft.com>
    Cc: Len Brown <len.brown@intel.com>
    Cc: Lianbo Jiang <lijiang@redhat.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Nadav Amit <namit@vmware.com>
    Cc: Omar Sandoval <osandov@fb.com>
    Cc: Pavel Machek <pavel@ucw.cz>
    Cc: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: Stefano Stabellini <sstabellini@kernel.org>
    Cc: Stephen Hemminger <sthemmin@microsoft.com>
    Cc: Vitaly Kuznetsov <vkuznets@redhat.com>
    Cc: Xavier Deguillard <xdeguillard@vmware.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/proc/page.c b/fs/proc/page.c
index 40b05e0d4274..544d1ee15aee 100644
--- a/fs/proc/page.c
+++ b/fs/proc/page.c
@@ -152,8 +152,8 @@ u64 stable_page_flags(struct page *page)
 	else if (page_count(page) == 0 && is_free_buddy_page(page))
 		u |= 1 << KPF_BUDDY;
 
-	if (PageBalloon(page))
-		u |= 1 << KPF_BALLOON;
+	if (PageOffline(page))
+		u |= 1 << KPF_OFFLINE;
 	if (PageTable(page))
 		u |= 1 << KPF_PGTABLE;
 

commit 144552ff8995dd34d049a203d636b259ab751137
Author: Anthony Yznaga <anthony.yznaga@oracle.com>
Date:   Fri Dec 28 00:37:31 2018 -0800

    /proc/kpagecount: return 0 for special pages that are never mapped
    
    Certain pages that are never mapped to userspace have a type indicated in
    the page_type field of their struct pages (e.g.  PG_buddy).  page_type
    overlaps with _mapcount so set the count to 0 and avoid calling
    page_mapcount() for these pages.
    
    [anthony.yznaga@oracle.com: incorporate feedback from Matthew Wilcox]
      Link: http://lkml.kernel.org/r/1544481313-27318-1-git-send-email-anthony.yznaga@oracle.com
    Link: http://lkml.kernel.org/r/1543963526-27917-1-git-send-email-anthony.yznaga@oracle.com
    Signed-off-by: Anthony Yznaga <anthony.yznaga@oracle.com>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Matthew Wilcox <willy@infradead.org>
    Reviewed-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Alexander Duyck <alexander.h.duyck@linux.intel.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Miles Chen <miles.chen@mediatek.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/proc/page.c b/fs/proc/page.c
index 6c517b11acf8..40b05e0d4274 100644
--- a/fs/proc/page.c
+++ b/fs/proc/page.c
@@ -46,7 +46,7 @@ static ssize_t kpagecount_read(struct file *file, char __user *buf,
 			ppage = pfn_to_page(pfn);
 		else
 			ppage = NULL;
-		if (!ppage || PageSlab(ppage))
+		if (!ppage || PageSlab(ppage) || page_has_type(ppage))
 			pcount = 0;
 		else
 			pcount = page_mapcount(ppage);

commit 57c8a661d95dff48dd9c2f2496139082bbaf241a
Author: Mike Rapoport <rppt@linux.vnet.ibm.com>
Date:   Tue Oct 30 15:09:49 2018 -0700

    mm: remove include/linux/bootmem.h
    
    Move remaining definitions and declarations from include/linux/bootmem.h
    into include/linux/memblock.h and remove the redundant header.
    
    The includes were replaced with the semantic patch below and then
    semi-automated removal of duplicated '#include <linux/memblock.h>
    
    @@
    @@
    - #include <linux/bootmem.h>
    + #include <linux/memblock.h>
    
    [sfr@canb.auug.org.au: dma-direct: fix up for the removal of linux/bootmem.h]
      Link: http://lkml.kernel.org/r/20181002185342.133d1680@canb.auug.org.au
    [sfr@canb.auug.org.au: powerpc: fix up for removal of linux/bootmem.h]
      Link: http://lkml.kernel.org/r/20181005161406.73ef8727@canb.auug.org.au
    [sfr@canb.auug.org.au: x86/kaslr, ACPI/NUMA: fix for linux/bootmem.h removal]
      Link: http://lkml.kernel.org/r/20181008190341.5e396491@canb.auug.org.au
    Link: http://lkml.kernel.org/r/1536927045-23536-30-git-send-email-rppt@linux.vnet.ibm.com
    Signed-off-by: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "James E.J. Bottomley" <jejb@parisc-linux.org>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Ley Foon Tan <lftan@altera.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Palmer Dabbelt <palmer@sifive.com>
    Cc: Paul Burton <paul.burton@mips.com>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Serge Semin <fancer.lancer@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/proc/page.c b/fs/proc/page.c
index 792c78a49174..6c517b11acf8 100644
--- a/fs/proc/page.c
+++ b/fs/proc/page.c
@@ -1,5 +1,5 @@
 // SPDX-License-Identifier: GPL-2.0
-#include <linux/bootmem.h>
+#include <linux/memblock.h>
 #include <linux/compiler.h>
 #include <linux/fs.h>
 #include <linux/init.h>

commit 1d40a5ea01d53251c23c7be541d3f4a656cfc537
Author: Matthew Wilcox <mawilcox@microsoft.com>
Date:   Thu Jun 7 17:08:23 2018 -0700

    mm: mark pages in use for page tables
    
    Define a new PageTable bit in the page_type and use it to mark pages in
    use as page tables.  This can be helpful when debugging crashdumps or
    analysing memory fragmentation.  Add a KPF flag to report these pages to
    userspace and update page-types.c to interpret that flag.
    
    Note that only pages currently accounted as NR_PAGETABLES are tracked as
    PageTable; this does not include pgd/p4d/pud/pmd pages.  Those will be the
    subject of a later patch.
    
    Link: http://lkml.kernel.org/r/20180518194519.3820-4-willy@infradead.org
    Signed-off-by: Matthew Wilcox <mawilcox@microsoft.com>
    Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Jérôme Glisse <jglisse@redhat.com>
    Cc: Lai Jiangshan <jiangshanlai@gmail.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Randy Dunlap <rdunlap@infradead.org>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/proc/page.c b/fs/proc/page.c
index 1491918a33c3..792c78a49174 100644
--- a/fs/proc/page.c
+++ b/fs/proc/page.c
@@ -154,6 +154,8 @@ u64 stable_page_flags(struct page *page)
 
 	if (PageBalloon(page))
 		u |= 1 << KPF_BALLOON;
+	if (PageTable(page))
+		u |= 1 << KPF_PGTABLE;
 
 	if (page_is_idle(page))
 		u |= 1 << KPF_IDLE;

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/fs/proc/page.c b/fs/proc/page.c
index 2726536489b1..1491918a33c3 100644
--- a/fs/proc/page.c
+++ b/fs/proc/page.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 #include <linux/bootmem.h>
 #include <linux/compiler.h>
 #include <linux/fs.h>

commit b6789123bccba8b5feb9901ed2e8c3c39181979d
Author: Hugh Dickins <hughd@google.com>
Date:   Tue Feb 7 11:11:16 2017 -0800

    mm: fix KPF_SWAPCACHE in /proc/kpageflags
    
    Commit 6326fec1122c ("mm: Use owner_priv bit for PageSwapCache, valid
    when PageSwapBacked") aliased PG_swapcache to PG_owner_priv_1 (and
    depending on PageSwapBacked being true).
    
    As a result, the KPF_SWAPCACHE bit in '/proc/kpageflags' should now be
    synthesized, instead of being shown on unrelated pages which just happen
    to have PG_owner_priv_1 set.
    
    Signed-off-by: Hugh Dickins <hughd@google.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/proc/page.c b/fs/proc/page.c
index a2066e6dee90..2726536489b1 100644
--- a/fs/proc/page.c
+++ b/fs/proc/page.c
@@ -173,7 +173,8 @@ u64 stable_page_flags(struct page *page)
 	u |= kpf_copy_bit(k, KPF_ACTIVE,	PG_active);
 	u |= kpf_copy_bit(k, KPF_RECLAIM,	PG_reclaim);
 
-	u |= kpf_copy_bit(k, KPF_SWAPCACHE,	PG_swapcache);
+	if (PageSwapCache(page))
+		u |= 1 << KPF_SWAPCACHE;
 	u |= kpf_copy_bit(k, KPF_SWAPBACKED,	PG_swapbacked);
 
 	u |= kpf_copy_bit(k, KPF_UNEVICTABLE,	PG_unevictable);

commit 7c0f6ba682b9c7632072ffbedf8d328c8f3c42ba
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Dec 24 11:46:01 2016 -0800

    Replace <asm/uaccess.h> with <linux/uaccess.h> globally
    
    This was entirely automated, using the script by Al:
    
      PATT='^[[:blank:]]*#[[:blank:]]*include[[:blank:]]*<asm/uaccess.h>'
      sed -i -e "s!$PATT!#include <linux/uaccess.h>!" \
            $(git grep -l "$PATT"|grep -v ^include/linux/uaccess.h)
    
    to do the replacement at the end of the merge window.
    
    Requested-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/proc/page.c b/fs/proc/page.c
index 3ecd445e830d..a2066e6dee90 100644
--- a/fs/proc/page.c
+++ b/fs/proc/page.c
@@ -13,7 +13,7 @@
 #include <linux/mmu_notifier.h>
 #include <linux/page_idle.h>
 #include <linux/kernel-page-flags.h>
-#include <asm/uaccess.h>
+#include <linux/uaccess.h>
 #include "internal.h"
 
 #define KPMSIZE sizeof(u64)

commit 0139aa7b7fa12ceef095d99dc36606a5b10ab83a
Author: Joonsoo Kim <iamjoonsoo.kim@lge.com>
Date:   Thu May 19 17:10:49 2016 -0700

    mm: rename _count, field of the struct page, to _refcount
    
    Many developers already know that field for reference count of the
    struct page is _count and atomic type.  They would try to handle it
    directly and this could break the purpose of page reference count
    tracepoint.  To prevent direct _count modification, this patch rename it
    to _refcount and add warning message on the code.  After that, developer
    who need to handle reference count will find that field should not be
    accessed directly.
    
    [akpm@linux-foundation.org: fix comments, per Vlastimil]
    [akpm@linux-foundation.org: Documentation/vm/transhuge.txt too]
    [sfr@canb.auug.org.au: sync ethernet driver changes]
    Signed-off-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Johannes Berg <johannes@sipsolutions.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Sunil Goutham <sgoutham@cavium.com>
    Cc: Chris Metcalf <cmetcalf@mellanox.com>
    Cc: Manish Chopra <manish.chopra@qlogic.com>
    Cc: Yuval Mintz <yuval.mintz@qlogic.com>
    Cc: Tariq Toukan <tariqt@mellanox.com>
    Cc: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/proc/page.c b/fs/proc/page.c
index 712f1b9992cc..3ecd445e830d 100644
--- a/fs/proc/page.c
+++ b/fs/proc/page.c
@@ -142,7 +142,7 @@ u64 stable_page_flags(struct page *page)
 
 
 	/*
-	 * Caveats on high order pages: page->_count will only be set
+	 * Caveats on high order pages: page->_refcount will only be set
 	 * -1 on the head page; SLUB/SLQB do the same for PG_slab;
 	 * SLOB won't set PG_slab at all on compound pages.
 	 */

commit 0a71649cb724ab97df26baa7731ac31d2364bfe5
Author: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
Date:   Thu Mar 17 14:17:44 2016 -0700

    /proc/kpageflags: return KPF_SLAB for slab tail pages
    
    Currently /proc/kpageflags returns just KPF_COMPOUND_TAIL for slab tail
    pages, which is inconvenient when grasping how slab pages are
    distributed (userspace always needs to check which kind of tail pages by
    itself).  This patch sets KPF_SLAB for such pages.
    
    With this patch:
    
      $ grep Slab /proc/meminfo ; tools/vm/page-types -b slab
      Slab:              64880 kB
                   flags      page-count       MB  symbolic-flags                     long-symbolic-flags
      0x0000000000000080           16220       63  _______S__________________________________ slab
                   total           16220       63
    
    16220 pages equals to 64880 kB, so returned result is consistent with the
    global counter.
    
    Signed-off-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Reviewed-by: Vladimir Davydov <vdavydov@virtuozzo.com>
    Cc: Konstantin Khlebnikov <koct9i@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/proc/page.c b/fs/proc/page.c
index 0be626d85331..712f1b9992cc 100644
--- a/fs/proc/page.c
+++ b/fs/proc/page.c
@@ -160,6 +160,8 @@ u64 stable_page_flags(struct page *page)
 	u |= kpf_copy_bit(k, KPF_LOCKED,	PG_locked);
 
 	u |= kpf_copy_bit(k, KPF_SLAB,		PG_slab);
+	if (PageTail(page) && PageSlab(compound_head(page)))
+		u |= 1 << KPF_SLAB;
 
 	u |= kpf_copy_bit(k, KPF_ERROR,		PG_error);
 	u |= kpf_copy_bit(k, KPF_DIRTY,		PG_dirty);

commit 832fc1de01aea28255cb11d270679b7f1273f0d7
Author: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
Date:   Thu Mar 17 14:17:41 2016 -0700

    /proc/kpageflags: return KPF_BUDDY for "tail" buddy pages
    
    Currently /proc/kpageflags returns nothing for "tail" buddy pages, which
    is inconvenient when grasping how free pages are distributed.  This
    patch sets KPF_BUDDY for such pages.
    
    With this patch:
    
      $ grep MemFree /proc/meminfo ; tools/vm/page-types -b buddy
      MemFree:         3134992 kB
                   flags      page-count       MB  symbolic-flags                     long-symbolic-flags
      0x0000000000000400          779272     3044  __________B_______________________________ buddy
      0x0000000000000c00            4385       17  __________BM______________________________ buddy,mmap
                   total          783657     3061
    
    783657 pages is 3134628 kB (roughly consistent with the global counter,)
    so it's OK.
    
    [akpm@linux-foundation.org: update comment, per Naoya]
    Signed-off-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Reviewed-by: Vladimir Davydov <vdavydov@virtuozzo.com>>
    Cc: Konstantin Khlebnikov <koct9i@gmail.com>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/proc/page.c b/fs/proc/page.c
index b2855eea5405..0be626d85331 100644
--- a/fs/proc/page.c
+++ b/fs/proc/page.c
@@ -103,9 +103,9 @@ u64 stable_page_flags(struct page *page)
 	 * pseudo flags for the well known (anonymous) memory mapped pages
 	 *
 	 * Note that page->_mapcount is overloaded in SLOB/SLUB/SLQB, so the
-	 * simple test in page_mapcount() is not enough.
+	 * simple test in page_mapped() is not enough.
 	 */
-	if (!PageSlab(page) && page_mapcount(page))
+	if (!PageSlab(page) && page_mapped(page))
 		u |= 1 << KPF_MMAP;
 	if (PageAnon(page))
 		u |= 1 << KPF_ANON;
@@ -148,6 +148,8 @@ u64 stable_page_flags(struct page *page)
 	 */
 	if (PageBuddy(page))
 		u |= 1 << KPF_BUDDY;
+	else if (page_count(page) == 0 && is_free_buddy_page(page))
+		u |= 1 << KPF_BUDDY;
 
 	if (PageBalloon(page))
 		u |= 1 << KPF_BALLOON;

commit e1534ae95004d6a307839a44eed40389d608c935
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Fri Jan 15 16:53:46 2016 -0800

    mm: differentiate page_mapped() from page_mapcount() for compound pages
    
    Let's define page_mapped() to be true for compound pages if any
    sub-pages of the compound page is mapped (with PMD or PTE).
    
    On other hand page_mapcount() return mapcount for this particular small
    page.
    
    This will make cases like page_get_anon_vma() behave correctly once we
    allow huge pages to be mapped with PTE.
    
    Most users outside core-mm should use page_mapcount() instead of
    page_mapped().
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Tested-by: Sasha Levin <sasha.levin@oracle.com>
    Tested-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Acked-by: Jerome Marchand <jmarchan@redhat.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: Steve Capper <steve.capper@linaro.org>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/proc/page.c b/fs/proc/page.c
index 93484034a03d..b2855eea5405 100644
--- a/fs/proc/page.c
+++ b/fs/proc/page.c
@@ -103,9 +103,9 @@ u64 stable_page_flags(struct page *page)
 	 * pseudo flags for the well known (anonymous) memory mapped pages
 	 *
 	 * Note that page->_mapcount is overloaded in SLOB/SLUB/SLQB, so the
-	 * simple test in page_mapped() is not enough.
+	 * simple test in page_mapcount() is not enough.
 	 */
-	if (!PageSlab(page) && page_mapped(page))
+	if (!PageSlab(page) && page_mapcount(page))
 		u |= 1 << KPF_MMAP;
 	if (PageAnon(page))
 		u |= 1 << KPF_ANON;

commit d3691d2c6d3e72624c987bbef6f322631bbb2d5d
Author: Vladimir Davydov <vdavydov@parallels.com>
Date:   Wed Sep 9 15:35:51 2015 -0700

    proc: add cond_resched to /proc/kpage* read/write loop
    
    Reading/writing a /proc/kpage* file may take long on machines with a lot
    of RAM installed.
    
    Signed-off-by: Vladimir Davydov <vdavydov@parallels.com>
    Suggested-by: Andres Lagar-Cavilla <andreslc@google.com>
    Reviewed-by: Andres Lagar-Cavilla <andreslc@google.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Raghavendra K T <raghavendra.kt@linux.vnet.ibm.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Greg Thelen <gthelen@google.com>
    Cc: Michel Lespinasse <walken@google.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Pavel Emelyanov <xemul@parallels.com>
    Cc: Cyrill Gorcunov <gorcunov@openvz.org>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/proc/page.c b/fs/proc/page.c
index 0b8286450a93..93484034a03d 100644
--- a/fs/proc/page.c
+++ b/fs/proc/page.c
@@ -58,6 +58,8 @@ static ssize_t kpagecount_read(struct file *file, char __user *buf,
 		pfn++;
 		out++;
 		count -= KPMSIZE;
+
+		cond_resched();
 	}
 
 	*ppos += (char __user *)out - buf;
@@ -219,6 +221,8 @@ static ssize_t kpageflags_read(struct file *file, char __user *buf,
 		pfn++;
 		out++;
 		count -= KPMSIZE;
+
+		cond_resched();
 	}
 
 	*ppos += (char __user *)out - buf;
@@ -267,6 +271,8 @@ static ssize_t kpagecgroup_read(struct file *file, char __user *buf,
 		pfn++;
 		out++;
 		count -= KPMSIZE;
+
+		cond_resched();
 	}
 
 	*ppos += (char __user *)out - buf;

commit f074a8f49eb87cde95ac9d040ad5e7ea4f029738
Author: Vladimir Davydov <vdavydov@parallels.com>
Date:   Wed Sep 9 15:35:48 2015 -0700

    proc: export idle flag via kpageflags
    
    As noted by Minchan, a benefit of reading idle flag from /proc/kpageflags
    is that one can easily filter dirty and/or unevictable pages while
    estimating the size of unused memory.
    
    Note that idle flag read from /proc/kpageflags may be stale in case the
    page was accessed via a PTE, because it would be too costly to iterate
    over all page mappings on each /proc/kpageflags read to provide an
    up-to-date value.  To make sure the flag is up-to-date one has to read
    /sys/kernel/mm/page_idle/bitmap first.
    
    Signed-off-by: Vladimir Davydov <vdavydov@parallels.com>
    Reviewed-by: Andres Lagar-Cavilla <andreslc@google.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Raghavendra K T <raghavendra.kt@linux.vnet.ibm.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Greg Thelen <gthelen@google.com>
    Cc: Michel Lespinasse <walken@google.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Pavel Emelyanov <xemul@parallels.com>
    Cc: Cyrill Gorcunov <gorcunov@openvz.org>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/proc/page.c b/fs/proc/page.c
index c2d29edcaa6b..0b8286450a93 100644
--- a/fs/proc/page.c
+++ b/fs/proc/page.c
@@ -150,6 +150,9 @@ u64 stable_page_flags(struct page *page)
 	if (PageBalloon(page))
 		u |= 1 << KPF_BALLOON;
 
+	if (page_is_idle(page))
+		u |= 1 << KPF_IDLE;
+
 	u |= kpf_copy_bit(k, KPF_LOCKED,	PG_locked);
 
 	u |= kpf_copy_bit(k, KPF_SLAB,		PG_slab);

commit 33c3fc71c8cfa3cc3a98beaa901c069c177dc295
Author: Vladimir Davydov <vdavydov@parallels.com>
Date:   Wed Sep 9 15:35:45 2015 -0700

    mm: introduce idle page tracking
    
    Knowing the portion of memory that is not used by a certain application or
    memory cgroup (idle memory) can be useful for partitioning the system
    efficiently, e.g.  by setting memory cgroup limits appropriately.
    Currently, the only means to estimate the amount of idle memory provided
    by the kernel is /proc/PID/{clear_refs,smaps}: the user can clear the
    access bit for all pages mapped to a particular process by writing 1 to
    clear_refs, wait for some time, and then count smaps:Referenced.  However,
    this method has two serious shortcomings:
    
     - it does not count unmapped file pages
     - it affects the reclaimer logic
    
    To overcome these drawbacks, this patch introduces two new page flags,
    Idle and Young, and a new sysfs file, /sys/kernel/mm/page_idle/bitmap.
    A page's Idle flag can only be set from userspace by setting bit in
    /sys/kernel/mm/page_idle/bitmap at the offset corresponding to the page,
    and it is cleared whenever the page is accessed either through page tables
    (it is cleared in page_referenced() in this case) or using the read(2)
    system call (mark_page_accessed()). Thus by setting the Idle flag for
    pages of a particular workload, which can be found e.g.  by reading
    /proc/PID/pagemap, waiting for some time to let the workload access its
    working set, and then reading the bitmap file, one can estimate the amount
    of pages that are not used by the workload.
    
    The Young page flag is used to avoid interference with the memory
    reclaimer.  A page's Young flag is set whenever the Access bit of a page
    table entry pointing to the page is cleared by writing to the bitmap file.
    If page_referenced() is called on a Young page, it will add 1 to its
    return value, therefore concealing the fact that the Access bit was
    cleared.
    
    Note, since there is no room for extra page flags on 32 bit, this feature
    uses extended page flags when compiled on 32 bit.
    
    [akpm@linux-foundation.org: fix build]
    [akpm@linux-foundation.org: kpageidle requires an MMU]
    [akpm@linux-foundation.org: decouple from page-flags rework]
    Signed-off-by: Vladimir Davydov <vdavydov@parallels.com>
    Reviewed-by: Andres Lagar-Cavilla <andreslc@google.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Raghavendra K T <raghavendra.kt@linux.vnet.ibm.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Greg Thelen <gthelen@google.com>
    Cc: Michel Lespinasse <walken@google.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Pavel Emelyanov <xemul@parallels.com>
    Cc: Cyrill Gorcunov <gorcunov@openvz.org>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/proc/page.c b/fs/proc/page.c
index 70d23245dd43..c2d29edcaa6b 100644
--- a/fs/proc/page.c
+++ b/fs/proc/page.c
@@ -10,12 +10,15 @@
 #include <linux/seq_file.h>
 #include <linux/hugetlb.h>
 #include <linux/memcontrol.h>
+#include <linux/mmu_notifier.h>
+#include <linux/page_idle.h>
 #include <linux/kernel-page-flags.h>
 #include <asm/uaccess.h>
 #include "internal.h"
 
 #define KPMSIZE sizeof(u64)
 #define KPMMASK (KPMSIZE - 1)
+#define KPMBITS (KPMSIZE * BITS_PER_BYTE)
 
 /* /proc/kpagecount - an array exposing page counts
  *

commit 80ae2fdceba8313b0433f899bdd9c6c463291a17
Author: Vladimir Davydov <vdavydov@parallels.com>
Date:   Wed Sep 9 15:35:38 2015 -0700

    proc: add kpagecgroup file
    
    /proc/kpagecgroup contains a 64-bit inode number of the memory cgroup each
    page is charged to, indexed by PFN.  Having this information is useful for
    estimating a cgroup working set size.
    
    The file is present if CONFIG_PROC_PAGE_MONITOR && CONFIG_MEMCG.
    
    Signed-off-by: Vladimir Davydov <vdavydov@parallels.com>
    Reviewed-by: Andres Lagar-Cavilla <andreslc@google.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Raghavendra K T <raghavendra.kt@linux.vnet.ibm.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Greg Thelen <gthelen@google.com>
    Cc: Michel Lespinasse <walken@google.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Pavel Emelyanov <xemul@parallels.com>
    Cc: Cyrill Gorcunov <gorcunov@openvz.org>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/proc/page.c b/fs/proc/page.c
index 7eee2d8b97d9..70d23245dd43 100644
--- a/fs/proc/page.c
+++ b/fs/proc/page.c
@@ -9,6 +9,7 @@
 #include <linux/proc_fs.h>
 #include <linux/seq_file.h>
 #include <linux/hugetlb.h>
+#include <linux/memcontrol.h>
 #include <linux/kernel-page-flags.h>
 #include <asm/uaccess.h>
 #include "internal.h"
@@ -225,10 +226,62 @@ static const struct file_operations proc_kpageflags_operations = {
 	.read = kpageflags_read,
 };
 
+#ifdef CONFIG_MEMCG
+static ssize_t kpagecgroup_read(struct file *file, char __user *buf,
+				size_t count, loff_t *ppos)
+{
+	u64 __user *out = (u64 __user *)buf;
+	struct page *ppage;
+	unsigned long src = *ppos;
+	unsigned long pfn;
+	ssize_t ret = 0;
+	u64 ino;
+
+	pfn = src / KPMSIZE;
+	count = min_t(unsigned long, count, (max_pfn * KPMSIZE) - src);
+	if (src & KPMMASK || count & KPMMASK)
+		return -EINVAL;
+
+	while (count > 0) {
+		if (pfn_valid(pfn))
+			ppage = pfn_to_page(pfn);
+		else
+			ppage = NULL;
+
+		if (ppage)
+			ino = page_cgroup_ino(ppage);
+		else
+			ino = 0;
+
+		if (put_user(ino, out)) {
+			ret = -EFAULT;
+			break;
+		}
+
+		pfn++;
+		out++;
+		count -= KPMSIZE;
+	}
+
+	*ppos += (char __user *)out - buf;
+	if (!ret)
+		ret = (char __user *)out - buf;
+	return ret;
+}
+
+static const struct file_operations proc_kpagecgroup_operations = {
+	.llseek = mem_lseek,
+	.read = kpagecgroup_read,
+};
+#endif /* CONFIG_MEMCG */
+
 static int __init proc_page_init(void)
 {
 	proc_create("kpagecount", S_IRUSR, NULL, &proc_kpagecount_operations);
 	proc_create("kpageflags", S_IRUSR, NULL, &proc_kpageflags_operations);
+#ifdef CONFIG_MEMCG
+	proc_create("kpagecgroup", S_IRUSR, NULL, &proc_kpagecgroup_operations);
+#endif
 	return 0;
 }
 fs_initcall(proc_page_init);

commit 56873f43abdcd574b25105867a990f067747b2f4
Author: Wang, Yalin <Yalin.Wang@sonymobile.com>
Date:   Wed Feb 11 15:24:51 2015 -0800

    mm:add KPF_ZERO_PAGE flag for /proc/kpageflags
    
    Add KPF_ZERO_PAGE flag for zero_page, so that userspace processes can
    detect zero_page in /proc/kpageflags, and then do memory analysis more
    accurately.
    
    Signed-off-by: Yalin Wang <yalin.wang@sonymobile.com>
    Acked-by: Kirill A. Shutemov <kirill@shutemov.name>
    Cc: Konstantin Khlebnikov <koct9i@gmail.com>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/proc/page.c b/fs/proc/page.c
index 1e3187da1fed..7eee2d8b97d9 100644
--- a/fs/proc/page.c
+++ b/fs/proc/page.c
@@ -5,6 +5,7 @@
 #include <linux/ksm.h>
 #include <linux/mm.h>
 #include <linux/mmzone.h>
+#include <linux/huge_mm.h>
 #include <linux/proc_fs.h>
 #include <linux/seq_file.h>
 #include <linux/hugetlb.h>
@@ -121,9 +122,18 @@ u64 stable_page_flags(struct page *page)
 	 * just checks PG_head/PG_tail, so we need to check PageLRU/PageAnon
 	 * to make sure a given page is a thp, not a non-huge compound page.
 	 */
-	else if (PageTransCompound(page) && (PageLRU(compound_head(page)) ||
-					     PageAnon(compound_head(page))))
-		u |= 1 << KPF_THP;
+	else if (PageTransCompound(page)) {
+		struct page *head = compound_head(page);
+
+		if (PageLRU(head) || PageAnon(head))
+			u |= 1 << KPF_THP;
+		else if (is_huge_zero_page(head)) {
+			u |= 1 << KPF_ZERO_PAGE;
+			u |= 1 << KPF_THP;
+		}
+	} else if (is_zero_pfn(page_to_pfn(page)))
+		u |= 1 << KPF_ZERO_PAGE;
+
 
 	/*
 	 * Caveats on high order pages: page->_count will only be set

commit 09316c09dde33aae14f34489d9e3d243ec0d5938
Author: Konstantin Khlebnikov <k.khlebnikov@samsung.com>
Date:   Thu Oct 9 15:29:32 2014 -0700

    mm/balloon_compaction: add vmstat counters and kpageflags bit
    
    Always mark pages with PageBalloon even if balloon compaction is disabled
    and expose this mark in /proc/kpageflags as KPF_BALLOON.
    
    Also this patch adds three counters into /proc/vmstat: "balloon_inflate",
    "balloon_deflate" and "balloon_migrate".  They accumulate balloon
    activity.  Current size of balloon is (balloon_inflate - balloon_deflate)
    pages.
    
    All generic balloon code now gathered under option CONFIG_MEMORY_BALLOON.
    It should be selected by ballooning driver which wants use this feature.
    Currently virtio-balloon is the only user.
    
    Signed-off-by: Konstantin Khlebnikov <k.khlebnikov@samsung.com>
    Cc: Rafael Aquini <aquini@redhat.com>
    Cc: Andrey Ryabinin <ryabinin.a.a@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/proc/page.c b/fs/proc/page.c
index e647c55275d9..1e3187da1fed 100644
--- a/fs/proc/page.c
+++ b/fs/proc/page.c
@@ -133,6 +133,9 @@ u64 stable_page_flags(struct page *page)
 	if (PageBuddy(page))
 		u |= 1 << KPF_BUDDY;
 
+	if (PageBalloon(page))
+		u |= 1 << KPF_BALLOON;
+
 	u |= kpf_copy_bit(k, KPF_LOCKED,	PG_locked);
 
 	u |= kpf_copy_bit(k, KPF_SLAB,		PG_slab);

commit 668f9abbd4334e6c29fa8acd71635c4f9101caa7
Author: David Rientjes <rientjes@google.com>
Date:   Mon Mar 3 15:38:18 2014 -0800

    mm: close PageTail race
    
    Commit bf6bddf1924e ("mm: introduce compaction and migration for
    ballooned pages") introduces page_count(page) into memory compaction
    which dereferences page->first_page if PageTail(page).
    
    This results in a very rare NULL pointer dereference on the
    aforementioned page_count(page).  Indeed, anything that does
    compound_head(), including page_count() is susceptible to racing with
    prep_compound_page() and seeing a NULL or dangling page->first_page
    pointer.
    
    This patch uses Andrea's implementation of compound_trans_head() that
    deals with such a race and makes it the default compound_head()
    implementation.  This includes a read memory barrier that ensures that
    if PageTail(head) is true that we return a head page that is neither
    NULL nor dangling.  The patch then adds a store memory barrier to
    prep_compound_page() to ensure page->first_page is set.
    
    This is the safest way to ensure we see the head page that we are
    expecting, PageTail(page) is already in the unlikely() path and the
    memory barriers are unfortunately required.
    
    Hugetlbfs is the exception, we don't enforce a store memory barrier
    during init since no race is possible.
    
    Signed-off-by: David Rientjes <rientjes@google.com>
    Cc: Holger Kiehl <Holger.Kiehl@dwd.de>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Rafael Aquini <aquini@redhat.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/proc/page.c b/fs/proc/page.c
index 02174a610315..e647c55275d9 100644
--- a/fs/proc/page.c
+++ b/fs/proc/page.c
@@ -121,9 +121,8 @@ u64 stable_page_flags(struct page *page)
 	 * just checks PG_head/PG_tail, so we need to check PageLRU/PageAnon
 	 * to make sure a given page is a thp, not a non-huge compound page.
 	 */
-	else if (PageTransCompound(page) &&
-		 (PageLRU(compound_trans_head(page)) ||
-		  PageAnon(compound_trans_head(page))))
+	else if (PageTransCompound(page) && (PageLRU(compound_head(page)) ||
+					     PageAnon(compound_head(page))))
 		u |= 1 << KPF_THP;
 
 	/*

commit abaf3787ac26ba33e2f75e76b1174c32254c25b0
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Thu Jan 23 15:55:45 2014 -0800

    fs/proc: don't use module_init for non-modular core code
    
    PROC_FS is a bool, so this code is either present or absent.  It will
    never be modular, so using module_init as an alias for __initcall is
    rather misleading.
    
    Fix this up now, so that we can relocate module_init from init.h into
    module.h in the future.  If we don't do this, we'd have to add module.h to
    obviously non-modular code, and that would be ugly at best.
    
    Note that direct use of __initcall is discouraged, vs.  one of the
    priority categorized subgroups.  As __initcall gets mapped onto
    device_initcall, our use of fs_initcall (which makes sense for fs code)
    will thus change these registrations from level 6-device to level 5-fs
    (i.e.  slightly earlier).  However no observable impact of that small
    difference has been observed during testing, or is expected.
    
    Also note that this change uncovers a missing semicolon bug in the
    registration of vmcore_init as an initcall.
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/proc/page.c b/fs/proc/page.c
index cab84b6272ed..02174a610315 100644
--- a/fs/proc/page.c
+++ b/fs/proc/page.c
@@ -219,4 +219,4 @@ static int __init proc_page_init(void)
 	proc_create("kpageflags", S_IRUSR, NULL, &proc_kpageflags_operations);
 	return 0;
 }
-module_init(proc_page_init);
+fs_initcall(proc_page_init);

commit e3bba3c3c90cd434c1ccb9e5dc704a96baf9541c
Author: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
Date:   Thu Jan 23 15:52:53 2014 -0800

    fs/proc/page.c: add PageAnon check to surely detect thp
    
    stable_page_flags() checks !PageHuge && PageTransCompound && PageLRU to
    know that a specified page is thp or not.  But sometimes it's not enough
    and we fail to detect thp when the thp is on pagevec.  This happens only
    for a few seconds after LRU list operations, but it makes it difficult
    to control our applications depending on this flag.
    
    So this patch adds another check PageAnon to detect thps on pagevec.  It
    might not give the future extensibility for thp pagecache, but it's OK
    at least for now.
    
    Signed-off-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/proc/page.c b/fs/proc/page.c
index b8730d9ebaee..cab84b6272ed 100644
--- a/fs/proc/page.c
+++ b/fs/proc/page.c
@@ -118,10 +118,12 @@ u64 stable_page_flags(struct page *page)
 	/*
 	 * PageTransCompound can be true for non-huge compound pages (slab
 	 * pages or pages allocated by drivers with __GFP_COMP) because it
-	 * just checks PG_head/PG_tail, so we need to check PageLRU to make
-	 * sure a given page is a thp, not a non-huge compound page.
+	 * just checks PG_head/PG_tail, so we need to check PageLRU/PageAnon
+	 * to make sure a given page is a thp, not a non-huge compound page.
 	 */
-	else if (PageTransCompound(page) && PageLRU(compound_trans_head(page)))
+	else if (PageTransCompound(page) &&
+		 (PageLRU(compound_trans_head(page)) ||
+		  PageAnon(compound_trans_head(page))))
 		u |= 1 << KPF_THP;
 
 	/*

commit 7a71932d5676b7410ab64d149bad8bde6b0d8632
Author: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
Date:   Mon Oct 8 16:33:47 2012 -0700

    kpageflags: fix wrong KPF_THP on non-huge compound pages
    
    KPF_THP can be set on non-huge compound pages (like slab pages or pages
    allocated by drivers with __GFP_COMP) because PageTransCompound only
    checks PG_head and PG_tail.  Obviously this is a bug and breaks user space
    applications which look for thp via /proc/kpageflags.
    
    This patch rules out setting KPF_THP wrongly by additionally checking
    PageLRU on the head pages.
    
    Signed-off-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Acked-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Reviewed-by: Fengguang Wu <fengguang.wu@intel.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/proc/page.c b/fs/proc/page.c
index 7fcd0d60a968..b8730d9ebaee 100644
--- a/fs/proc/page.c
+++ b/fs/proc/page.c
@@ -115,7 +115,13 @@ u64 stable_page_flags(struct page *page)
 		u |= 1 << KPF_COMPOUND_TAIL;
 	if (PageHuge(page))
 		u |= 1 << KPF_HUGE;
-	else if (PageTransCompound(page))
+	/*
+	 * PageTransCompound can be true for non-huge compound pages (slab
+	 * pages or pages allocated by drivers with __GFP_COMP) because it
+	 * just checks PG_head/PG_tail, so we need to check PageLRU to make
+	 * sure a given page is a thp, not a non-huge compound page.
+	 */
+	else if (PageTransCompound(page) && PageLRU(compound_trans_head(page)))
 		u |= 1 << KPF_THP;
 
 	/*

commit e873c49fbfdd595481976b915850e682441bcbec
Author: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
Date:   Wed Mar 21 16:33:58 2012 -0700

    pagemap: export KPF_THP
    
    This flag shows that a given page is a subpage of a transparent hugepage.
    It helps us debug and test the kernel by showing physical address of thp.
    
    Signed-off-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Reviewed-by: Wu Fengguang <fengguang.wu@intel.com>
    Reviewed-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Acked-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/proc/page.c b/fs/proc/page.c
index 6d8e6a9e93ab..7fcd0d60a968 100644
--- a/fs/proc/page.c
+++ b/fs/proc/page.c
@@ -115,6 +115,8 @@ u64 stable_page_flags(struct page *page)
 		u |= 1 << KPF_COMPOUND_TAIL;
 	if (PageHuge(page))
 		u |= 1 << KPF_HUGE;
+	else if (PageTransCompound(page))
+		u |= 1 << KPF_THP;
 
 	/*
 	 * Caveats on high order pages: page->_count will only be set

commit 5f24ce5fd34c3ca1b3d10d30da754732da64d5c0
Author: Andrea Arcangeli <aarcange@redhat.com>
Date:   Thu Jan 13 15:47:00 2011 -0800

    thp: remove PG_buddy
    
    PG_buddy can be converted to _mapcount == -2.  So the PG_compound_lock can
    be added to page->flags without overflowing (because of the sparse section
    bits increasing) with CONFIG_X86_PAE=y and CONFIG_X86_PAT=y.  This also
    has to move the memory hotplug code from _mapcount to lru.next to avoid
    any risk of clashes.  We can't use lru.next for PG_buddy removal, but
    memory hotplug can use lru.next even more easily than the mapcount
    instead.
    
    Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/proc/page.c b/fs/proc/page.c
index b06c674624e6..6d8e6a9e93ab 100644
--- a/fs/proc/page.c
+++ b/fs/proc/page.c
@@ -116,15 +116,17 @@ u64 stable_page_flags(struct page *page)
 	if (PageHuge(page))
 		u |= 1 << KPF_HUGE;
 
-	u |= kpf_copy_bit(k, KPF_LOCKED,	PG_locked);
-
 	/*
-	 * Caveats on high order pages:
-	 * PG_buddy will only be set on the head page; SLUB/SLQB do the same
-	 * for PG_slab; SLOB won't set PG_slab at all on compound pages.
+	 * Caveats on high order pages: page->_count will only be set
+	 * -1 on the head page; SLUB/SLQB do the same for PG_slab;
+	 * SLOB won't set PG_slab at all on compound pages.
 	 */
+	if (PageBuddy(page))
+		u |= 1 << KPF_BUDDY;
+
+	u |= kpf_copy_bit(k, KPF_LOCKED,	PG_locked);
+
 	u |= kpf_copy_bit(k, KPF_SLAB,		PG_slab);
-	u |= kpf_copy_bit(k, KPF_BUDDY,		PG_buddy);
 
 	u |= kpf_copy_bit(k, KPF_ERROR,		PG_error);
 	u |= kpf_copy_bit(k, KPF_DIRTY,		PG_dirty);

commit a6fc86d2b43bf1086557f023a24adf91db915559
Author: Petr Holasek <pholasek@redhat.com>
Date:   Wed Jan 12 17:00:34 2011 -0800

    kpagecount: add slab page checking because _mapcount is in a union
    
    Add a PageSlab() check before adding the _mapcount value to /kpagecount.
    page->_mapcount is in a union with the SLAB structure so for pages
    controlled by SLAB, page_mapcount() returns nonsense.
    
    Signed-off-by: Petr Holasek <pholasek@redhat.com>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Cc: Matt Mackall <mpm@selenic.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/proc/page.c b/fs/proc/page.c
index 3b8b45660331..b06c674624e6 100644
--- a/fs/proc/page.c
+++ b/fs/proc/page.c
@@ -40,7 +40,7 @@ static ssize_t kpagecount_read(struct file *file, char __user *buf,
 			ppage = pfn_to_page(pfn);
 		else
 			ppage = NULL;
-		if (!ppage)
+		if (!ppage || PageSlab(ppage))
 			pcount = 0;
 		else
 			pcount = page_mapcount(ppage);

commit ed430fec756ad65f7cfba24f8ad17c3d5a403290
Author: Takashi Iwai <tiwai@suse.de>
Date:   Thu Sep 9 16:37:36 2010 -0700

    proc: export uncached bit properly in /proc/kpageflags
    
    Fix the left-over old ifdef for PG_uncached in /proc/kpageflags.  Now it's
    used by x86, too.
    
    Signed-off-by: Takashi Iwai <tiwai@suse.de>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/proc/page.c b/fs/proc/page.c
index 180cf5a0bd67..3b8b45660331 100644
--- a/fs/proc/page.c
+++ b/fs/proc/page.c
@@ -146,7 +146,7 @@ u64 stable_page_flags(struct page *page)
 	u |= kpf_copy_bit(k, KPF_HWPOISON,	PG_hwpoison);
 #endif
 
-#ifdef CONFIG_IA64_UNCACHED_ALLOCATOR
+#ifdef CONFIG_ARCH_USES_PG_UNCACHED
 	u |= kpf_copy_bit(k, KPF_UNCACHED,	PG_uncached);
 #endif
 

commit 1a9b5b7fe0c5dad8a635288882d36785dea742f9
Author: Wu Fengguang <fengguang.wu@intel.com>
Date:   Wed Dec 16 12:19:59 2009 +0100

    mm: export stable page flags
    
    Rename get_uflags() to stable_page_flags() and make it a global function
    for use in the hwpoison page flags filter, which need to compare user
    page flags with the value provided by user space.
    
    Also move KPF_* to kernel-page-flags.h for use by user space tools.
    
    Acked-by: Matt Mackall <mpm@selenic.com>
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    CC: Nick Piggin <npiggin@suse.de>
    CC: Christoph Lameter <cl@linux-foundation.org>
    Signed-off-by: Wu Fengguang <fengguang.wu@intel.com>
    Signed-off-by: Andi Kleen <ak@linux.intel.com>

diff --git a/fs/proc/page.c b/fs/proc/page.c
index 5033ce0d254b..180cf5a0bd67 100644
--- a/fs/proc/page.c
+++ b/fs/proc/page.c
@@ -8,6 +8,7 @@
 #include <linux/proc_fs.h>
 #include <linux/seq_file.h>
 #include <linux/hugetlb.h>
+#include <linux/kernel-page-flags.h>
 #include <asm/uaccess.h>
 #include "internal.h"
 
@@ -71,52 +72,12 @@ static const struct file_operations proc_kpagecount_operations = {
  * physical page flags.
  */
 
-/* These macros are used to decouple internal flags from exported ones */
-
-#define KPF_LOCKED		0
-#define KPF_ERROR		1
-#define KPF_REFERENCED		2
-#define KPF_UPTODATE		3
-#define KPF_DIRTY		4
-#define KPF_LRU			5
-#define KPF_ACTIVE		6
-#define KPF_SLAB		7
-#define KPF_WRITEBACK		8
-#define KPF_RECLAIM		9
-#define KPF_BUDDY		10
-
-/* 11-20: new additions in 2.6.31 */
-#define KPF_MMAP		11
-#define KPF_ANON		12
-#define KPF_SWAPCACHE		13
-#define KPF_SWAPBACKED		14
-#define KPF_COMPOUND_HEAD	15
-#define KPF_COMPOUND_TAIL	16
-#define KPF_HUGE		17
-#define KPF_UNEVICTABLE		18
-#define KPF_HWPOISON		19
-#define KPF_NOPAGE		20
-
-#define KPF_KSM			21
-
-/* kernel hacking assistances
- * WARNING: subject to change, never rely on them!
- */
-#define KPF_RESERVED		32
-#define KPF_MLOCKED		33
-#define KPF_MAPPEDTODISK	34
-#define KPF_PRIVATE		35
-#define KPF_PRIVATE_2		36
-#define KPF_OWNER_PRIVATE	37
-#define KPF_ARCH		38
-#define KPF_UNCACHED		39
-
 static inline u64 kpf_copy_bit(u64 kflags, int ubit, int kbit)
 {
 	return ((kflags >> kbit) & 1) << ubit;
 }
 
-static u64 get_uflags(struct page *page)
+u64 stable_page_flags(struct page *page)
 {
 	u64 k;
 	u64 u;
@@ -219,7 +180,7 @@ static ssize_t kpageflags_read(struct file *file, char __user *buf,
 		else
 			ppage = NULL;
 
-		if (put_user(get_uflags(ppage), out)) {
+		if (put_user(stable_page_flags(ppage), out)) {
 			ret = -EFAULT;
 			break;
 		}

commit 253fb02d62571e5455eedc9e39b9d660e86a40f0
Author: Wu Fengguang <fengguang.wu@intel.com>
Date:   Wed Oct 7 16:32:27 2009 -0700

    pagemap: export KPF_HWPOISON
    
    This flag indicates a hardware detected memory corruption on the page.
    Any future access of the page data may bring down the machine.
    
    Signed-off-by: Wu Fengguang <fengguang.wu@intel.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/proc/page.c b/fs/proc/page.c
index 2281c2cbfe2b..5033ce0d254b 100644
--- a/fs/proc/page.c
+++ b/fs/proc/page.c
@@ -94,6 +94,7 @@ static const struct file_operations proc_kpagecount_operations = {
 #define KPF_COMPOUND_TAIL	16
 #define KPF_HUGE		17
 #define KPF_UNEVICTABLE		18
+#define KPF_HWPOISON		19
 #define KPF_NOPAGE		20
 
 #define KPF_KSM			21
@@ -180,6 +181,10 @@ static u64 get_uflags(struct page *page)
 	u |= kpf_copy_bit(k, KPF_UNEVICTABLE,	PG_unevictable);
 	u |= kpf_copy_bit(k, KPF_MLOCKED,	PG_mlocked);
 
+#ifdef CONFIG_MEMORY_FAILURE
+	u |= kpf_copy_bit(k, KPF_HWPOISON,	PG_hwpoison);
+#endif
+
 #ifdef CONFIG_IA64_UNCACHED_ALLOCATOR
 	u |= kpf_copy_bit(k, KPF_UNCACHED,	PG_uncached);
 #endif

commit 9a840895147b12de5cdd633c600b38686840ee53
Author: Hugh Dickins <hugh.dickins@tiscali.co.uk>
Date:   Mon Sep 21 17:02:01 2009 -0700

    ksm: identify PageKsm pages
    
    KSM will need to identify its kernel merged pages unambiguously, and
    /proc/kpageflags will probably like to do so too.
    
    Since KSM will only be substituting anonymous pages, statistics are best
    preserved by making a PageKsm page a special PageAnon page: one with no
    anon_vma.
    
    But KSM then needs its own page_add_ksm_rmap() - keep it in ksm.h near
    PageKsm; and do_wp_page() must COW them, unlike singly mapped PageAnons.
    
    Signed-off-by: Hugh Dickins <hugh.dickins@tiscali.co.uk>
    Signed-off-by: Chris Wright <chrisw@redhat.com>
    Signed-off-by: Izik Eidus <ieidus@redhat.com>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Cc: Hugh Dickins <hugh.dickins@tiscali.co.uk>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Cc: Avi Kivity <avi@redhat.com>
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/proc/page.c b/fs/proc/page.c
index 2707c6c7a20f..2281c2cbfe2b 100644
--- a/fs/proc/page.c
+++ b/fs/proc/page.c
@@ -2,6 +2,7 @@
 #include <linux/compiler.h>
 #include <linux/fs.h>
 #include <linux/init.h>
+#include <linux/ksm.h>
 #include <linux/mm.h>
 #include <linux/mmzone.h>
 #include <linux/proc_fs.h>
@@ -95,6 +96,8 @@ static const struct file_operations proc_kpagecount_operations = {
 #define KPF_UNEVICTABLE		18
 #define KPF_NOPAGE		20
 
+#define KPF_KSM			21
+
 /* kernel hacking assistances
  * WARNING: subject to change, never rely on them!
  */
@@ -137,6 +140,8 @@ static u64 get_uflags(struct page *page)
 		u |= 1 << KPF_MMAP;
 	if (PageAnon(page))
 		u |= 1 << KPF_ANON;
+	if (PageKsm(page))
+		u |= 1 << KPF_KSM;
 
 	/*
 	 * compound pages: export both head/tail info

commit 6837765963f1723e80ca97b1fae660f3a60d77df
Author: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
Date:   Tue Jun 16 15:32:51 2009 -0700

    mm: remove CONFIG_UNEVICTABLE_LRU config option
    
    Currently, nobody wants to turn UNEVICTABLE_LRU off.  Thus this
    configurability is unnecessary.
    
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Andi Kleen <andi@firstfloor.org>
    Acked-by: Minchan Kim <minchan.kim@gmail.com>
    Cc: David Woodhouse <dwmw2@infradead.org>
    Cc: Matt Mackall <mpm@selenic.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/proc/page.c b/fs/proc/page.c
index 9d926bd279a4..2707c6c7a20f 100644
--- a/fs/proc/page.c
+++ b/fs/proc/page.c
@@ -172,10 +172,8 @@ static u64 get_uflags(struct page *page)
 	u |= kpf_copy_bit(k, KPF_SWAPCACHE,	PG_swapcache);
 	u |= kpf_copy_bit(k, KPF_SWAPBACKED,	PG_swapbacked);
 
-#ifdef CONFIG_UNEVICTABLE_LRU
 	u |= kpf_copy_bit(k, KPF_UNEVICTABLE,	PG_unevictable);
 	u |= kpf_copy_bit(k, KPF_MLOCKED,	PG_mlocked);
-#endif
 
 #ifdef CONFIG_IA64_UNCACHED_ALLOCATOR
 	u |= kpf_copy_bit(k, KPF_UNCACHED,	PG_uncached);

commit 177975495914efb372f7edee28ba9a0fdb754149
Author: Wu Fengguang <fengguang.wu@intel.com>
Date:   Tue Jun 16 15:32:24 2009 -0700

    proc: export more page flags in /proc/kpageflags
    
    Export all page flags faithfully in /proc/kpageflags.
    
            11. KPF_MMAP            (pseudo flag) memory mapped page
            12. KPF_ANON            (pseudo flag) memory mapped page (anonymous)
            13. KPF_SWAPCACHE       page is in swap cache
            14. KPF_SWAPBACKED      page is swap/RAM backed
            15. KPF_COMPOUND_HEAD   (*)
            16. KPF_COMPOUND_TAIL   (*)
            17. KPF_HUGE            hugeTLB pages
            18. KPF_UNEVICTABLE     page is in the unevictable LRU list
            19. KPF_HWPOISON(TBD)   hardware detected corruption
            20. KPF_NOPAGE          (pseudo flag) no page frame at the address
            32-39.                  more obscure flags for kernel developers
    
            (*) For compound pages, exporting _both_ head/tail info enables
                users to tell where a compound page starts/ends, and its order.
    
    The accompanying page-types tool will handle the details like decoupling
    overloaded flags and hiding obscure flags to normal users.
    
    Thanks to KOSAKI and Andi for their valuable recommendations!
    
    Signed-off-by: Wu Fengguang <fengguang.wu@intel.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Matt Mackall <mpm@selenic.com>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/proc/page.c b/fs/proc/page.c
index e73e911b7d0c..9d926bd279a4 100644
--- a/fs/proc/page.c
+++ b/fs/proc/page.c
@@ -72,19 +72,124 @@ static const struct file_operations proc_kpagecount_operations = {
 
 /* These macros are used to decouple internal flags from exported ones */
 
-#define KPF_LOCKED     0
-#define KPF_ERROR      1
-#define KPF_REFERENCED 2
-#define KPF_UPTODATE   3
-#define KPF_DIRTY      4
-#define KPF_LRU        5
-#define KPF_ACTIVE     6
-#define KPF_SLAB       7
-#define KPF_WRITEBACK  8
-#define KPF_RECLAIM    9
-#define KPF_BUDDY     10
-
-#define kpf_copy_bit(flags, dstpos, srcpos) (((flags >> srcpos) & 1) << dstpos)
+#define KPF_LOCKED		0
+#define KPF_ERROR		1
+#define KPF_REFERENCED		2
+#define KPF_UPTODATE		3
+#define KPF_DIRTY		4
+#define KPF_LRU			5
+#define KPF_ACTIVE		6
+#define KPF_SLAB		7
+#define KPF_WRITEBACK		8
+#define KPF_RECLAIM		9
+#define KPF_BUDDY		10
+
+/* 11-20: new additions in 2.6.31 */
+#define KPF_MMAP		11
+#define KPF_ANON		12
+#define KPF_SWAPCACHE		13
+#define KPF_SWAPBACKED		14
+#define KPF_COMPOUND_HEAD	15
+#define KPF_COMPOUND_TAIL	16
+#define KPF_HUGE		17
+#define KPF_UNEVICTABLE		18
+#define KPF_NOPAGE		20
+
+/* kernel hacking assistances
+ * WARNING: subject to change, never rely on them!
+ */
+#define KPF_RESERVED		32
+#define KPF_MLOCKED		33
+#define KPF_MAPPEDTODISK	34
+#define KPF_PRIVATE		35
+#define KPF_PRIVATE_2		36
+#define KPF_OWNER_PRIVATE	37
+#define KPF_ARCH		38
+#define KPF_UNCACHED		39
+
+static inline u64 kpf_copy_bit(u64 kflags, int ubit, int kbit)
+{
+	return ((kflags >> kbit) & 1) << ubit;
+}
+
+static u64 get_uflags(struct page *page)
+{
+	u64 k;
+	u64 u;
+
+	/*
+	 * pseudo flag: KPF_NOPAGE
+	 * it differentiates a memory hole from a page with no flags
+	 */
+	if (!page)
+		return 1 << KPF_NOPAGE;
+
+	k = page->flags;
+	u = 0;
+
+	/*
+	 * pseudo flags for the well known (anonymous) memory mapped pages
+	 *
+	 * Note that page->_mapcount is overloaded in SLOB/SLUB/SLQB, so the
+	 * simple test in page_mapped() is not enough.
+	 */
+	if (!PageSlab(page) && page_mapped(page))
+		u |= 1 << KPF_MMAP;
+	if (PageAnon(page))
+		u |= 1 << KPF_ANON;
+
+	/*
+	 * compound pages: export both head/tail info
+	 * they together define a compound page's start/end pos and order
+	 */
+	if (PageHead(page))
+		u |= 1 << KPF_COMPOUND_HEAD;
+	if (PageTail(page))
+		u |= 1 << KPF_COMPOUND_TAIL;
+	if (PageHuge(page))
+		u |= 1 << KPF_HUGE;
+
+	u |= kpf_copy_bit(k, KPF_LOCKED,	PG_locked);
+
+	/*
+	 * Caveats on high order pages:
+	 * PG_buddy will only be set on the head page; SLUB/SLQB do the same
+	 * for PG_slab; SLOB won't set PG_slab at all on compound pages.
+	 */
+	u |= kpf_copy_bit(k, KPF_SLAB,		PG_slab);
+	u |= kpf_copy_bit(k, KPF_BUDDY,		PG_buddy);
+
+	u |= kpf_copy_bit(k, KPF_ERROR,		PG_error);
+	u |= kpf_copy_bit(k, KPF_DIRTY,		PG_dirty);
+	u |= kpf_copy_bit(k, KPF_UPTODATE,	PG_uptodate);
+	u |= kpf_copy_bit(k, KPF_WRITEBACK,	PG_writeback);
+
+	u |= kpf_copy_bit(k, KPF_LRU,		PG_lru);
+	u |= kpf_copy_bit(k, KPF_REFERENCED,	PG_referenced);
+	u |= kpf_copy_bit(k, KPF_ACTIVE,	PG_active);
+	u |= kpf_copy_bit(k, KPF_RECLAIM,	PG_reclaim);
+
+	u |= kpf_copy_bit(k, KPF_SWAPCACHE,	PG_swapcache);
+	u |= kpf_copy_bit(k, KPF_SWAPBACKED,	PG_swapbacked);
+
+#ifdef CONFIG_UNEVICTABLE_LRU
+	u |= kpf_copy_bit(k, KPF_UNEVICTABLE,	PG_unevictable);
+	u |= kpf_copy_bit(k, KPF_MLOCKED,	PG_mlocked);
+#endif
+
+#ifdef CONFIG_IA64_UNCACHED_ALLOCATOR
+	u |= kpf_copy_bit(k, KPF_UNCACHED,	PG_uncached);
+#endif
+
+	u |= kpf_copy_bit(k, KPF_RESERVED,	PG_reserved);
+	u |= kpf_copy_bit(k, KPF_MAPPEDTODISK,	PG_mappedtodisk);
+	u |= kpf_copy_bit(k, KPF_PRIVATE,	PG_private);
+	u |= kpf_copy_bit(k, KPF_PRIVATE_2,	PG_private_2);
+	u |= kpf_copy_bit(k, KPF_OWNER_PRIVATE,	PG_owner_priv_1);
+	u |= kpf_copy_bit(k, KPF_ARCH,		PG_arch_1);
+
+	return u;
+};
 
 static ssize_t kpageflags_read(struct file *file, char __user *buf,
 			     size_t count, loff_t *ppos)
@@ -94,7 +199,6 @@ static ssize_t kpageflags_read(struct file *file, char __user *buf,
 	unsigned long src = *ppos;
 	unsigned long pfn;
 	ssize_t ret = 0;
-	u64 kflags, uflags;
 
 	pfn = src / KPMSIZE;
 	count = min_t(unsigned long, count, (max_pfn * KPMSIZE) - src);
@@ -106,24 +210,8 @@ static ssize_t kpageflags_read(struct file *file, char __user *buf,
 			ppage = pfn_to_page(pfn);
 		else
 			ppage = NULL;
-		if (!ppage)
-			kflags = 0;
-		else
-			kflags = ppage->flags;
-
-		uflags = kpf_copy_bit(kflags, KPF_LOCKED, PG_locked) |
-			kpf_copy_bit(kflags, KPF_ERROR, PG_error) |
-			kpf_copy_bit(kflags, KPF_REFERENCED, PG_referenced) |
-			kpf_copy_bit(kflags, KPF_UPTODATE, PG_uptodate) |
-			kpf_copy_bit(kflags, KPF_DIRTY, PG_dirty) |
-			kpf_copy_bit(kflags, KPF_LRU, PG_lru) |
-			kpf_copy_bit(kflags, KPF_ACTIVE, PG_active) |
-			kpf_copy_bit(kflags, KPF_SLAB, PG_slab) |
-			kpf_copy_bit(kflags, KPF_WRITEBACK, PG_writeback) |
-			kpf_copy_bit(kflags, KPF_RECLAIM, PG_reclaim) |
-			kpf_copy_bit(kflags, KPF_BUDDY, PG_buddy);
-
-		if (put_user(uflags, out)) {
+
+		if (put_user(get_uflags(ppage), out)) {
 			ret = -EFAULT;
 			break;
 		}

commit ed7ce0f1022942301776f93159c981b09382ddea
Author: Wu Fengguang <fengguang.wu@intel.com>
Date:   Tue Jun 16 15:32:23 2009 -0700

    proc: kpagecount/kpageflags code cleanup
    
    Move increments of pfn/out to bottom of the loop.
    
    Signed-off-by: Wu Fengguang <fengguang.wu@intel.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Acked-by: Matt Mackall <mpm@selenic.com>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/proc/page.c b/fs/proc/page.c
index 38dd88b7ce84..e73e911b7d0c 100644
--- a/fs/proc/page.c
+++ b/fs/proc/page.c
@@ -12,6 +12,7 @@
 
 #define KPMSIZE sizeof(u64)
 #define KPMMASK (KPMSIZE - 1)
+
 /* /proc/kpagecount - an array exposing page counts
  *
  * Each entry is a u64 representing the corresponding
@@ -33,20 +34,22 @@ static ssize_t kpagecount_read(struct file *file, char __user *buf,
 		return -EINVAL;
 
 	while (count > 0) {
-		ppage = NULL;
 		if (pfn_valid(pfn))
 			ppage = pfn_to_page(pfn);
-		pfn++;
+		else
+			ppage = NULL;
 		if (!ppage)
 			pcount = 0;
 		else
 			pcount = page_mapcount(ppage);
 
-		if (put_user(pcount, out++)) {
+		if (put_user(pcount, out)) {
 			ret = -EFAULT;
 			break;
 		}
 
+		pfn++;
+		out++;
 		count -= KPMSIZE;
 	}
 
@@ -99,10 +102,10 @@ static ssize_t kpageflags_read(struct file *file, char __user *buf,
 		return -EINVAL;
 
 	while (count > 0) {
-		ppage = NULL;
 		if (pfn_valid(pfn))
 			ppage = pfn_to_page(pfn);
-		pfn++;
+		else
+			ppage = NULL;
 		if (!ppage)
 			kflags = 0;
 		else
@@ -120,11 +123,13 @@ static ssize_t kpageflags_read(struct file *file, char __user *buf,
 			kpf_copy_bit(kflags, KPF_RECLAIM, PG_reclaim) |
 			kpf_copy_bit(kflags, KPF_BUDDY, PG_buddy);
 
-		if (put_user(uflags, out++)) {
+		if (put_user(uflags, out)) {
 			ret = -EFAULT;
 			break;
 		}
 
+		pfn++;
+		out++;
 		count -= KPMSIZE;
 	}
 

commit 20a0307c0396c2edb651401d2f2db193dda2f3c9
Author: Wu Fengguang <fengguang.wu@intel.com>
Date:   Tue Jun 16 15:32:22 2009 -0700

    mm: introduce PageHuge() for testing huge/gigantic pages
    
    A series of patches to enhance the /proc/pagemap interface and to add a
    userspace executable which can be used to present the pagemap data.
    
    Export 10 more flags to end users (and more for kernel developers):
    
            11. KPF_MMAP            (pseudo flag) memory mapped page
            12. KPF_ANON            (pseudo flag) memory mapped page (anonymous)
            13. KPF_SWAPCACHE       page is in swap cache
            14. KPF_SWAPBACKED      page is swap/RAM backed
            15. KPF_COMPOUND_HEAD   (*)
            16. KPF_COMPOUND_TAIL   (*)
            17. KPF_HUGE            hugeTLB pages
            18. KPF_UNEVICTABLE     page is in the unevictable LRU list
            19. KPF_HWPOISON        hardware detected corruption
            20. KPF_NOPAGE          (pseudo flag) no page frame at the address
    
            (*) For compound pages, exporting _both_ head/tail info enables
                users to tell where a compound page starts/ends, and its order.
    
    a simple demo of the page-types tool
    
    # ./page-types -h
    page-types [options]
                -r|--raw                  Raw mode, for kernel developers
                -a|--addr    addr-spec    Walk a range of pages
                -b|--bits    bits-spec    Walk pages with specified bits
                -l|--list                 Show page details in ranges
                -L|--list-each            Show page details one by one
                -N|--no-summary           Don't show summay info
                -h|--help                 Show this usage message
    addr-spec:
                N                         one page at offset N (unit: pages)
                N+M                       pages range from N to N+M-1
                N,M                       pages range from N to M-1
                N,                        pages range from N to end
                ,M                        pages range from 0 to M
    bits-spec:
                bit1,bit2                 (flags & (bit1|bit2)) != 0
                bit1,bit2=bit1            (flags & (bit1|bit2)) == bit1
                bit1,~bit2                (flags & (bit1|bit2)) == bit1
                =bit1,bit2                flags == (bit1|bit2)
    bit-names:
              locked              error         referenced           uptodate
               dirty                lru             active               slab
           writeback            reclaim              buddy               mmap
           anonymous          swapcache         swapbacked      compound_head
       compound_tail               huge        unevictable           hwpoison
              nopage           reserved(r)         mlocked(r)    mappedtodisk(r)
             private(r)       private_2(r)   owner_private(r)            arch(r)
            uncached(r)       readahead(o)       slob_free(o)     slub_frozen(o)
          slub_debug(o)
                                       (r) raw mode bits  (o) overloaded bits
    
    # ./page-types
                 flags      page-count       MB  symbolic-flags                     long-symbolic-flags
    0x0000000000000000          487369     1903  _________________________________
    0x0000000000000014               5        0  __R_D____________________________  referenced,dirty
    0x0000000000000020               1        0  _____l___________________________  lru
    0x0000000000000024              34        0  __R__l___________________________  referenced,lru
    0x0000000000000028            3838       14  ___U_l___________________________  uptodate,lru
    0x0001000000000028              48        0  ___U_l_______________________I___  uptodate,lru,readahead
    0x000000000000002c            6478       25  __RU_l___________________________  referenced,uptodate,lru
    0x000100000000002c              47        0  __RU_l_______________________I___  referenced,uptodate,lru,readahead
    0x0000000000000040            8344       32  ______A__________________________  active
    0x0000000000000060               1        0  _____lA__________________________  lru,active
    0x0000000000000068             348        1  ___U_lA__________________________  uptodate,lru,active
    0x0001000000000068              12        0  ___U_lA______________________I___  uptodate,lru,active,readahead
    0x000000000000006c             988        3  __RU_lA__________________________  referenced,uptodate,lru,active
    0x000100000000006c              48        0  __RU_lA______________________I___  referenced,uptodate,lru,active,readahead
    0x0000000000004078               1        0  ___UDlA_______b__________________  uptodate,dirty,lru,active,swapbacked
    0x000000000000407c              34        0  __RUDlA_______b__________________  referenced,uptodate,dirty,lru,active,swapbacked
    0x0000000000000400             503        1  __________B______________________  buddy
    0x0000000000000804               1        0  __R________M_____________________  referenced,mmap
    0x0000000000000828            1029        4  ___U_l_____M_____________________  uptodate,lru,mmap
    0x0001000000000828              43        0  ___U_l_____M_________________I___  uptodate,lru,mmap,readahead
    0x000000000000082c             382        1  __RU_l_____M_____________________  referenced,uptodate,lru,mmap
    0x000100000000082c              12        0  __RU_l_____M_________________I___  referenced,uptodate,lru,mmap,readahead
    0x0000000000000868             192        0  ___U_lA____M_____________________  uptodate,lru,active,mmap
    0x0001000000000868              12        0  ___U_lA____M_________________I___  uptodate,lru,active,mmap,readahead
    0x000000000000086c             800        3  __RU_lA____M_____________________  referenced,uptodate,lru,active,mmap
    0x000100000000086c              31        0  __RU_lA____M_________________I___  referenced,uptodate,lru,active,mmap,readahead
    0x0000000000004878               2        0  ___UDlA____M__b__________________  uptodate,dirty,lru,active,mmap,swapbacked
    0x0000000000001000             492        1  ____________a____________________  anonymous
    0x0000000000005808               4        0  ___U_______Ma_b__________________  uptodate,mmap,anonymous,swapbacked
    0x0000000000005868            2839       11  ___U_lA____Ma_b__________________  uptodate,lru,active,mmap,anonymous,swapbacked
    0x000000000000586c              30        0  __RU_lA____Ma_b__________________  referenced,uptodate,lru,active,mmap,anonymous,swapbacked
                 total          513968     2007
    
    # ./page-types -r
                 flags      page-count       MB  symbolic-flags                     long-symbolic-flags
    0x0000000000000000          468002     1828  _________________________________
    0x0000000100000000           19102       74  _____________________r___________  reserved
    0x0000000000008000              41        0  _______________H_________________  compound_head
    0x0000000000010000             188        0  ________________T________________  compound_tail
    0x0000000000008014               1        0  __R_D__________H_________________  referenced,dirty,compound_head
    0x0000000000010014               4        0  __R_D___________T________________  referenced,dirty,compound_tail
    0x0000000000000020               1        0  _____l___________________________  lru
    0x0000000800000024              34        0  __R__l__________________P________  referenced,lru,private
    0x0000000000000028            3794       14  ___U_l___________________________  uptodate,lru
    0x0001000000000028              46        0  ___U_l_______________________I___  uptodate,lru,readahead
    0x0000000400000028              44        0  ___U_l_________________d_________  uptodate,lru,mappedtodisk
    0x0001000400000028               2        0  ___U_l_________________d_____I___  uptodate,lru,mappedtodisk,readahead
    0x000000000000002c            6434       25  __RU_l___________________________  referenced,uptodate,lru
    0x000100000000002c              47        0  __RU_l_______________________I___  referenced,uptodate,lru,readahead
    0x000000040000002c              14        0  __RU_l_________________d_________  referenced,uptodate,lru,mappedtodisk
    0x000000080000002c              30        0  __RU_l__________________P________  referenced,uptodate,lru,private
    0x0000000800000040            8124       31  ______A_________________P________  active,private
    0x0000000000000040             219        0  ______A__________________________  active
    0x0000000800000060               1        0  _____lA_________________P________  lru,active,private
    0x0000000000000068             322        1  ___U_lA__________________________  uptodate,lru,active
    0x0001000000000068              12        0  ___U_lA______________________I___  uptodate,lru,active,readahead
    0x0000000400000068              13        0  ___U_lA________________d_________  uptodate,lru,active,mappedtodisk
    0x0000000800000068              12        0  ___U_lA_________________P________  uptodate,lru,active,private
    0x000000000000006c             977        3  __RU_lA__________________________  referenced,uptodate,lru,active
    0x000100000000006c              48        0  __RU_lA______________________I___  referenced,uptodate,lru,active,readahead
    0x000000040000006c               5        0  __RU_lA________________d_________  referenced,uptodate,lru,active,mappedtodisk
    0x000000080000006c               3        0  __RU_lA_________________P________  referenced,uptodate,lru,active,private
    0x0000000c0000006c               3        0  __RU_lA________________dP________  referenced,uptodate,lru,active,mappedtodisk,private
    0x0000000c00000068               1        0  ___U_lA________________dP________  uptodate,lru,active,mappedtodisk,private
    0x0000000000004078               1        0  ___UDlA_______b__________________  uptodate,dirty,lru,active,swapbacked
    0x000000000000407c              34        0  __RUDlA_______b__________________  referenced,uptodate,dirty,lru,active,swapbacked
    0x0000000000000400             538        2  __________B______________________  buddy
    0x0000000000000804               1        0  __R________M_____________________  referenced,mmap
    0x0000000000000828            1029        4  ___U_l_____M_____________________  uptodate,lru,mmap
    0x0001000000000828              43        0  ___U_l_____M_________________I___  uptodate,lru,mmap,readahead
    0x000000000000082c             382        1  __RU_l_____M_____________________  referenced,uptodate,lru,mmap
    0x000100000000082c              12        0  __RU_l_____M_________________I___  referenced,uptodate,lru,mmap,readahead
    0x0000000000000868             192        0  ___U_lA____M_____________________  uptodate,lru,active,mmap
    0x0001000000000868              12        0  ___U_lA____M_________________I___  uptodate,lru,active,mmap,readahead
    0x000000000000086c             800        3  __RU_lA____M_____________________  referenced,uptodate,lru,active,mmap
    0x000100000000086c              31        0  __RU_lA____M_________________I___  referenced,uptodate,lru,active,mmap,readahead
    0x0000000000004878               2        0  ___UDlA____M__b__________________  uptodate,dirty,lru,active,mmap,swapbacked
    0x0000000000001000             492        1  ____________a____________________  anonymous
    0x0000000000005008               2        0  ___U________a_b__________________  uptodate,anonymous,swapbacked
    0x0000000000005808               4        0  ___U_______Ma_b__________________  uptodate,mmap,anonymous,swapbacked
    0x000000000000580c               1        0  __RU_______Ma_b__________________  referenced,uptodate,mmap,anonymous,swapbacked
    0x0000000000005868            2839       11  ___U_lA____Ma_b__________________  uptodate,lru,active,mmap,anonymous,swapbacked
    0x000000000000586c              29        0  __RU_lA____Ma_b__________________  referenced,uptodate,lru,active,mmap,anonymous,swapbacked
                 total          513968     2007
    
    # ./page-types --raw --list --no-summary --bits reserved
    offset  count   flags
    0       15      _____________________r___________
    31      4       _____________________r___________
    159     97      _____________________r___________
    4096    2067    _____________________r___________
    6752    2390    _____________________r___________
    9355    3       _____________________r___________
    9728    14526   _____________________r___________
    
    This patch:
    
    Introduce PageHuge(), which identifies huge/gigantic pages by their
    dedicated compound destructor functions.
    
    Also move prep_compound_gigantic_page() to hugetlb.c and make
    __free_pages_ok() non-static.
    
    Signed-off-by: Wu Fengguang <fengguang.wu@intel.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Matt Mackall <mpm@selenic.com>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/proc/page.c b/fs/proc/page.c
index e9983837d08d..38dd88b7ce84 100644
--- a/fs/proc/page.c
+++ b/fs/proc/page.c
@@ -6,6 +6,7 @@
 #include <linux/mmzone.h>
 #include <linux/proc_fs.h>
 #include <linux/seq_file.h>
+#include <linux/hugetlb.h>
 #include <asm/uaccess.h>
 #include "internal.h"
 

commit ad3bdefe877afb47480418fdb05ecd42842de65e
Author: Wu Fengguang <fengguang.wu@intel.com>
Date:   Wed Mar 11 09:00:04 2009 +0800

    proc: fix kflags to uflags copying in /proc/kpageflags
    
    Fix kpf_copy_bit(src,dst) to be kpf_copy_bit(dst,src) to match the
    actual call patterns, e.g. kpf_copy_bit(kflags, KPF_LOCKED, PG_locked).
    
    This misplacement of src/dst only affected reporting of PG_writeback,
    PG_reclaim and PG_buddy. For others kflags==uflags so not affected.
    
    Signed-off-by: Wu Fengguang <fengguang.wu@intel.com>
    Reviewed-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: stable@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/proc/page.c b/fs/proc/page.c
index 2d1345112a42..e9983837d08d 100644
--- a/fs/proc/page.c
+++ b/fs/proc/page.c
@@ -80,7 +80,7 @@ static const struct file_operations proc_kpagecount_operations = {
 #define KPF_RECLAIM    9
 #define KPF_BUDDY     10
 
-#define kpf_copy_bit(flags, srcpos, dstpos) (((flags >> srcpos) & 1) << dstpos)
+#define kpf_copy_bit(flags, dstpos, srcpos) (((flags >> srcpos) & 1) << dstpos)
 
 static ssize_t kpageflags_read(struct file *file, char __user *buf,
 			     size_t count, loff_t *ppos)

commit e07a4b9217d1e97d2f3a62b6b070efdc61212110
Author: Helge Bahmann <helge.bahmann@secunet.com>
Date:   Fri Feb 20 16:24:12 2009 +0300

    proc: fix PG_locked reporting in /proc/kpageflags
    
    Expr always evaluates to zero.
    
    Cc: Matt Mackall <mpm@selenic.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>

diff --git a/fs/proc/page.c b/fs/proc/page.c
index 767d95a6d1b1..2d1345112a42 100644
--- a/fs/proc/page.c
+++ b/fs/proc/page.c
@@ -107,7 +107,7 @@ static ssize_t kpageflags_read(struct file *file, char __user *buf,
 		else
 			kflags = ppage->flags;
 
-		uflags = kpf_copy_bit(KPF_LOCKED, PG_locked, kflags) |
+		uflags = kpf_copy_bit(kflags, KPF_LOCKED, PG_locked) |
 			kpf_copy_bit(kflags, KPF_ERROR, PG_error) |
 			kpf_copy_bit(kflags, KPF_REFERENCED, PG_referenced) |
 			kpf_copy_bit(kflags, KPF_UPTODATE, PG_uptodate) |

commit 6d80e53f0056178c63fa8fbf3e8de40fb4df5f50
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Mon Oct 6 14:26:12 2008 +0400

    proc: move pagecount stuff to fs/proc/page.c
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>

diff --git a/fs/proc/page.c b/fs/proc/page.c
new file mode 100644
index 000000000000..767d95a6d1b1
--- /dev/null
+++ b/fs/proc/page.c
@@ -0,0 +1,147 @@
+#include <linux/bootmem.h>
+#include <linux/compiler.h>
+#include <linux/fs.h>
+#include <linux/init.h>
+#include <linux/mm.h>
+#include <linux/mmzone.h>
+#include <linux/proc_fs.h>
+#include <linux/seq_file.h>
+#include <asm/uaccess.h>
+#include "internal.h"
+
+#define KPMSIZE sizeof(u64)
+#define KPMMASK (KPMSIZE - 1)
+/* /proc/kpagecount - an array exposing page counts
+ *
+ * Each entry is a u64 representing the corresponding
+ * physical page count.
+ */
+static ssize_t kpagecount_read(struct file *file, char __user *buf,
+			     size_t count, loff_t *ppos)
+{
+	u64 __user *out = (u64 __user *)buf;
+	struct page *ppage;
+	unsigned long src = *ppos;
+	unsigned long pfn;
+	ssize_t ret = 0;
+	u64 pcount;
+
+	pfn = src / KPMSIZE;
+	count = min_t(size_t, count, (max_pfn * KPMSIZE) - src);
+	if (src & KPMMASK || count & KPMMASK)
+		return -EINVAL;
+
+	while (count > 0) {
+		ppage = NULL;
+		if (pfn_valid(pfn))
+			ppage = pfn_to_page(pfn);
+		pfn++;
+		if (!ppage)
+			pcount = 0;
+		else
+			pcount = page_mapcount(ppage);
+
+		if (put_user(pcount, out++)) {
+			ret = -EFAULT;
+			break;
+		}
+
+		count -= KPMSIZE;
+	}
+
+	*ppos += (char __user *)out - buf;
+	if (!ret)
+		ret = (char __user *)out - buf;
+	return ret;
+}
+
+static const struct file_operations proc_kpagecount_operations = {
+	.llseek = mem_lseek,
+	.read = kpagecount_read,
+};
+
+/* /proc/kpageflags - an array exposing page flags
+ *
+ * Each entry is a u64 representing the corresponding
+ * physical page flags.
+ */
+
+/* These macros are used to decouple internal flags from exported ones */
+
+#define KPF_LOCKED     0
+#define KPF_ERROR      1
+#define KPF_REFERENCED 2
+#define KPF_UPTODATE   3
+#define KPF_DIRTY      4
+#define KPF_LRU        5
+#define KPF_ACTIVE     6
+#define KPF_SLAB       7
+#define KPF_WRITEBACK  8
+#define KPF_RECLAIM    9
+#define KPF_BUDDY     10
+
+#define kpf_copy_bit(flags, srcpos, dstpos) (((flags >> srcpos) & 1) << dstpos)
+
+static ssize_t kpageflags_read(struct file *file, char __user *buf,
+			     size_t count, loff_t *ppos)
+{
+	u64 __user *out = (u64 __user *)buf;
+	struct page *ppage;
+	unsigned long src = *ppos;
+	unsigned long pfn;
+	ssize_t ret = 0;
+	u64 kflags, uflags;
+
+	pfn = src / KPMSIZE;
+	count = min_t(unsigned long, count, (max_pfn * KPMSIZE) - src);
+	if (src & KPMMASK || count & KPMMASK)
+		return -EINVAL;
+
+	while (count > 0) {
+		ppage = NULL;
+		if (pfn_valid(pfn))
+			ppage = pfn_to_page(pfn);
+		pfn++;
+		if (!ppage)
+			kflags = 0;
+		else
+			kflags = ppage->flags;
+
+		uflags = kpf_copy_bit(KPF_LOCKED, PG_locked, kflags) |
+			kpf_copy_bit(kflags, KPF_ERROR, PG_error) |
+			kpf_copy_bit(kflags, KPF_REFERENCED, PG_referenced) |
+			kpf_copy_bit(kflags, KPF_UPTODATE, PG_uptodate) |
+			kpf_copy_bit(kflags, KPF_DIRTY, PG_dirty) |
+			kpf_copy_bit(kflags, KPF_LRU, PG_lru) |
+			kpf_copy_bit(kflags, KPF_ACTIVE, PG_active) |
+			kpf_copy_bit(kflags, KPF_SLAB, PG_slab) |
+			kpf_copy_bit(kflags, KPF_WRITEBACK, PG_writeback) |
+			kpf_copy_bit(kflags, KPF_RECLAIM, PG_reclaim) |
+			kpf_copy_bit(kflags, KPF_BUDDY, PG_buddy);
+
+		if (put_user(uflags, out++)) {
+			ret = -EFAULT;
+			break;
+		}
+
+		count -= KPMSIZE;
+	}
+
+	*ppos += (char __user *)out - buf;
+	if (!ret)
+		ret = (char __user *)out - buf;
+	return ret;
+}
+
+static const struct file_operations proc_kpageflags_operations = {
+	.llseek = mem_lseek,
+	.read = kpageflags_read,
+};
+
+static int __init proc_page_init(void)
+{
+	proc_create("kpagecount", S_IRUSR, NULL, &proc_kpagecount_operations);
+	proc_create("kpageflags", S_IRUSR, NULL, &proc_kpageflags_operations);
+	return 0;
+}
+module_init(proc_page_init);
