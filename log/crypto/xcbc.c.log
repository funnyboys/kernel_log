commit a39c66cc2f6108c8346dc882bdcf72861aaca956
Author: Eric Biggers <ebiggers@google.com>
Date:   Thu Jan 2 20:04:38 2020 -0800

    crypto: shash - convert shash_free_instance() to new style
    
    Convert shash_free_instance() and its users to the new way of freeing
    instances, where a ->free() method is installed to the instance struct
    itself.  This replaces the weakly-typed method crypto_template::free().
    
    This will allow removing support for the old way of freeing instances.
    
    Also give shash_free_instance() a more descriptive name to reflect that
    it's only for instances with a single spawn, not for any instance.
    
    Signed-off-by: Eric Biggers <ebiggers@google.com>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/crypto/xcbc.c b/crypto/xcbc.c
index 9265e00ea663..598ec88abf0f 100644
--- a/crypto/xcbc.c
+++ b/crypto/xcbc.c
@@ -239,10 +239,12 @@ static int xcbc_create(struct crypto_template *tmpl, struct rtattr **tb)
 	inst->alg.final = crypto_xcbc_digest_final;
 	inst->alg.setkey = crypto_xcbc_digest_setkey;
 
+	inst->free = shash_free_singlespawn_instance;
+
 	err = shash_register_instance(tmpl, inst);
 	if (err) {
 err_free_inst:
-		shash_free_instance(shash_crypto_instance(inst));
+		shash_free_singlespawn_instance(inst);
 	}
 	return err;
 }
@@ -250,7 +252,6 @@ static int xcbc_create(struct crypto_template *tmpl, struct rtattr **tb)
 static struct crypto_template crypto_xcbc_tmpl = {
 	.name = "xcbc",
 	.create = xcbc_create,
-	.free = shash_free_instance,
 	.module = THIS_MODULE,
 };
 

commit d5ed3b65f7012a6592809f7f928f3e3660df8fd9
Author: Eric Biggers <ebiggers@google.com>
Date:   Thu Jan 2 19:59:05 2020 -0800

    crypto: cipher - make crypto_spawn_cipher() take a crypto_cipher_spawn
    
    Now that all users of single-block cipher spawns have been converted to
    use 'struct crypto_cipher_spawn' rather than the less specifically typed
    'struct crypto_spawn', make crypto_spawn_cipher() take a pointer to a
    'struct crypto_cipher_spawn' rather than a 'struct crypto_spawn'.
    
    Signed-off-by: Eric Biggers <ebiggers@google.com>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/crypto/xcbc.c b/crypto/xcbc.c
index 9b97fa511f10..9265e00ea663 100644
--- a/crypto/xcbc.c
+++ b/crypto/xcbc.c
@@ -167,7 +167,7 @@ static int xcbc_init_tfm(struct crypto_tfm *tfm)
 {
 	struct crypto_cipher *cipher;
 	struct crypto_instance *inst = (void *)tfm->__crt_alg;
-	struct crypto_spawn *spawn = crypto_instance_ctx(inst);
+	struct crypto_cipher_spawn *spawn = crypto_instance_ctx(inst);
 	struct xcbc_tfm_ctx *ctx = crypto_tfm_ctx(tfm);
 
 	cipher = crypto_spawn_cipher(spawn);

commit 1e212a6a562f781f00cba6c7ece93817857e0f32
Author: Eric Biggers <ebiggers@google.com>
Date:   Thu Jan 2 19:59:04 2020 -0800

    crypto: xcbc - use crypto_grab_cipher() and simplify error paths
    
    Make the xcbc template use the new function crypto_grab_cipher() to
    initialize its cipher spawn.
    
    This is needed to make all spawns be initialized in a consistent way.
    
    This required making xcbc_create() allocate the instance directly rather
    than use shash_alloc_instance().
    
    Also simplify the error handling by taking advantage of crypto_drop_*()
    now accepting (as a no-op) spawns that haven't been initialized yet, and
    by taking advantage of crypto_grab_*() now handling ERR_PTR() names.
    
    Signed-off-by: Eric Biggers <ebiggers@google.com>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/crypto/xcbc.c b/crypto/xcbc.c
index 0bb26e8f6f5a..9b97fa511f10 100644
--- a/crypto/xcbc.c
+++ b/crypto/xcbc.c
@@ -188,6 +188,7 @@ static void xcbc_exit_tfm(struct crypto_tfm *tfm)
 static int xcbc_create(struct crypto_template *tmpl, struct rtattr **tb)
 {
 	struct shash_instance *inst;
+	struct crypto_cipher_spawn *spawn;
 	struct crypto_alg *alg;
 	unsigned long alignmask;
 	int err;
@@ -196,28 +197,24 @@ static int xcbc_create(struct crypto_template *tmpl, struct rtattr **tb)
 	if (err)
 		return err;
 
-	alg = crypto_get_attr_alg(tb, CRYPTO_ALG_TYPE_CIPHER,
-				  CRYPTO_ALG_TYPE_MASK);
-	if (IS_ERR(alg))
-		return PTR_ERR(alg);
+	inst = kzalloc(sizeof(*inst) + sizeof(*spawn), GFP_KERNEL);
+	if (!inst)
+		return -ENOMEM;
+	spawn = shash_instance_ctx(inst);
 
-	switch(alg->cra_blocksize) {
-	case XCBC_BLOCKSIZE:
-		break;
-	default:
-		goto out_put_alg;
-	}
+	err = crypto_grab_cipher(spawn, shash_crypto_instance(inst),
+				 crypto_attr_alg_name(tb[1]), 0, 0);
+	if (err)
+		goto err_free_inst;
+	alg = crypto_spawn_cipher_alg(spawn);
 
-	inst = shash_alloc_instance("xcbc", alg);
-	err = PTR_ERR(inst);
-	if (IS_ERR(inst))
-		goto out_put_alg;
+	err = -EINVAL;
+	if (alg->cra_blocksize != XCBC_BLOCKSIZE)
+		goto err_free_inst;
 
-	err = crypto_init_spawn(shash_instance_ctx(inst), alg,
-				shash_crypto_instance(inst),
-				CRYPTO_ALG_TYPE_MASK);
+	err = crypto_inst_setname(shash_crypto_instance(inst), tmpl->name, alg);
 	if (err)
-		goto out_free_inst;
+		goto err_free_inst;
 
 	alignmask = alg->cra_alignmask | 3;
 	inst->alg.base.cra_alignmask = alignmask;
@@ -244,12 +241,9 @@ static int xcbc_create(struct crypto_template *tmpl, struct rtattr **tb)
 
 	err = shash_register_instance(tmpl, inst);
 	if (err) {
-out_free_inst:
+err_free_inst:
 		shash_free_instance(shash_crypto_instance(inst));
 	}
-
-out_put_alg:
-	crypto_mod_put(alg);
 	return err;
 }
 

commit 1ccea77e2a2687cae171b7987eb44730ec8c6d5f
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun May 19 15:51:43 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 13
    
    Based on 2 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license as published by
      the free software foundation either version 2 of the license or at
      your option any later version this program is distributed in the
      hope that it will be useful but without any warranty without even
      the implied warranty of merchantability or fitness for a particular
      purpose see the gnu general public license for more details you
      should have received a copy of the gnu general public license along
      with this program if not see http www gnu org licenses
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license as published by
      the free software foundation either version 2 of the license or at
      your option any later version this program is distributed in the
      hope that it will be useful but without any warranty without even
      the implied warranty of merchantability or fitness for a particular
      purpose see the gnu general public license for more details [based]
      [from] [clk] [highbank] [c] you should have received a copy of the
      gnu general public license along with this program if not see http
      www gnu org licenses
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-or-later
    
    has been chosen to replace the boilerplate/reference in 355 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Jilayne Lovejoy <opensource@jilayne.com>
    Reviewed-by: Steve Winslow <swinslow@gmail.com>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190519154041.837383322@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/crypto/xcbc.c b/crypto/xcbc.c
index 94ca694ef091..0bb26e8f6f5a 100644
--- a/crypto/xcbc.c
+++ b/crypto/xcbc.c
@@ -1,19 +1,7 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
 /*
  * Copyright (C)2006 USAGI/WIDE Project
  *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License as published by
- * the Free Software Foundation; either version 2 of the License, or
- * (at your option) any later version.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program.  If not, see <http://www.gnu.org/licenses/>.
- *
  * Author:
  * 	Kazunori Miyazawa <miyazawa@linux-ipv6.org>
  */

commit c4741b23059794bd99beef0f700103b0d983b3fd
Author: Eric Biggers <ebiggers@google.com>
Date:   Thu Apr 11 21:57:42 2019 -0700

    crypto: run initcalls for generic implementations earlier
    
    Use subsys_initcall for registration of all templates and generic
    algorithm implementations, rather than module_init.  Then change
    cryptomgr to use arch_initcall, to place it before the subsys_initcalls.
    
    This is needed so that when both a generic and optimized implementation
    of an algorithm are built into the kernel (not loadable modules), the
    generic implementation is registered before the optimized one.
    Otherwise, the self-tests for the optimized implementation are unable to
    allocate the generic implementation for the new comparison fuzz tests.
    
    Note that on arm, a side effect of this change is that self-tests for
    generic implementations may run before the unaligned access handler has
    been installed.  So, unaligned accesses will crash the kernel.  This is
    arguably a good thing as it makes it easier to detect that type of bug.
    
    Signed-off-by: Eric Biggers <ebiggers@google.com>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/crypto/xcbc.c b/crypto/xcbc.c
index c055f57fab11..94ca694ef091 100644
--- a/crypto/xcbc.c
+++ b/crypto/xcbc.c
@@ -282,7 +282,7 @@ static void __exit crypto_xcbc_module_exit(void)
 	crypto_unregister_template(&crypto_xcbc_tmpl);
 }
 
-module_init(crypto_xcbc_module_init);
+subsys_initcall(crypto_xcbc_module_init);
 module_exit(crypto_xcbc_module_exit);
 
 MODULE_LICENSE("GPL");

commit 3bdd23f886c08a0d649c535e1e2cf083ec600036
Author: Kees Cook <keescook@chromium.org>
Date:   Tue Aug 7 14:18:35 2018 -0700

    crypto: xcbc - Remove VLA usage
    
    In the quest to remove all stack VLA usage from the kernel[1], this uses
    the maximum blocksize and adds a sanity check. For xcbc, the blocksize
    must always be 16, so use that, since it's already being enforced during
    instantiation.
    
    [1] https://lkml.kernel.org/r/CA+55aFzCG-zNmZwX4A2FQpadafLfEzK6CC=qPXydAacU1RqZWA@mail.gmail.com
    
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/crypto/xcbc.c b/crypto/xcbc.c
index 25c75af50d3f..c055f57fab11 100644
--- a/crypto/xcbc.c
+++ b/crypto/xcbc.c
@@ -57,15 +57,17 @@ struct xcbc_desc_ctx {
 	u8 ctx[];
 };
 
+#define XCBC_BLOCKSIZE	16
+
 static int crypto_xcbc_digest_setkey(struct crypto_shash *parent,
 				     const u8 *inkey, unsigned int keylen)
 {
 	unsigned long alignmask = crypto_shash_alignmask(parent);
 	struct xcbc_tfm_ctx *ctx = crypto_shash_ctx(parent);
-	int bs = crypto_shash_blocksize(parent);
 	u8 *consts = PTR_ALIGN(&ctx->ctx[0], alignmask + 1);
 	int err = 0;
-	u8 key1[bs];
+	u8 key1[XCBC_BLOCKSIZE];
+	int bs = sizeof(key1);
 
 	if ((err = crypto_cipher_setkey(ctx->child, inkey, keylen)))
 		return err;
@@ -212,7 +214,7 @@ static int xcbc_create(struct crypto_template *tmpl, struct rtattr **tb)
 		return PTR_ERR(alg);
 
 	switch(alg->cra_blocksize) {
-	case 16:
+	case XCBC_BLOCKSIZE:
 		break;
 	default:
 		goto out_put_alg;

commit 1af39daaad4a868a3abd220e0ec9643473398c96
Author: Martin Kepplinger <martink@posteo.de>
Date:   Tue Nov 14 10:25:15 2017 +0100

    crypto: replace FSF address with web source in license notices
    
    A few years ago the FSF moved and "59 Temple Place" is wrong. Having this
    still in our source files feels old and unmaintained.
    
    Let's take the license statement serious and not confuse users.
    
    As https://www.gnu.org/licenses/gpl-howto.html suggests, we replace the
    postal address with "<http://www.gnu.org/licenses/>".
    
    Signed-off-by: Martin Kepplinger <martink@posteo.de>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/crypto/xcbc.c b/crypto/xcbc.c
index df90b332554c..25c75af50d3f 100644
--- a/crypto/xcbc.c
+++ b/crypto/xcbc.c
@@ -12,8 +12,7 @@
  * GNU General Public License for more details.
  *
  * You should have received a copy of the GNU General Public License
- * along with this program; if not, write to the Free Software
- * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
  *
  * Author:
  * 	Kazunori Miyazawa <miyazawa@linux-ipv6.org>

commit 4943ba16bbc2db05115707b3ff7b4874e9e3c560
Author: Kees Cook <keescook@chromium.org>
Date:   Mon Nov 24 16:32:38 2014 -0800

    crypto: include crypto- module prefix in template
    
    This adds the module loading prefix "crypto-" to the template lookup
    as well.
    
    For example, attempting to load 'vfat(blowfish)' via AF_ALG now correctly
    includes the "crypto-" prefix at every level, correctly rejecting "vfat":
    
            net-pf-38
            algif-hash
            crypto-vfat(blowfish)
            crypto-vfat(blowfish)-all
            crypto-vfat
    
    Reported-by: Mathias Krause <minipli@googlemail.com>
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Acked-by: Mathias Krause <minipli@googlemail.com>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/crypto/xcbc.c b/crypto/xcbc.c
index a5fbdf3738cf..df90b332554c 100644
--- a/crypto/xcbc.c
+++ b/crypto/xcbc.c
@@ -286,3 +286,4 @@ module_exit(crypto_xcbc_module_exit);
 
 MODULE_LICENSE("GPL");
 MODULE_DESCRIPTION("XCBC keyed hash algorithm");
+MODULE_ALIAS_CRYPTO("xcbc");

commit 4bb33cc8901898af80d5d4a9917067aa0839922a
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Fri May 27 14:41:48 2011 -0400

    crypto: add module.h to those files that are explicitly using it
    
    Part of the include cleanups means that the implicit
    inclusion of module.h via device.h is going away.  So
    fix things up in advance.
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/crypto/xcbc.c b/crypto/xcbc.c
index bb7b67fba349..a5fbdf3738cf 100644
--- a/crypto/xcbc.c
+++ b/crypto/xcbc.c
@@ -22,6 +22,7 @@
 #include <crypto/internal/hash.h>
 #include <linux/err.h>
 #include <linux/kernel.h>
+#include <linux/module.h>
 
 static u_int32_t ks[12] = {0x01010101, 0x01010101, 0x01010101, 0x01010101,
 			   0x02020202, 0x02020202, 0x02020202, 0x02020202,

commit 36f87a4a29cb8cd291169483079fde34bad4ef16
Author: Steffen Klassert <steffen.klassert@secunet.com>
Date:   Thu Aug 20 17:58:04 2009 +1000

    crypto: xcbc - Fix alignment calculation of xcbc_tfm_ctx
    
    The alignment calculation of xcbc_tfm_ctx uses alg->cra_alignmask
    and not alg->cra_alignmask + 1 as it should. This led to frequent
    crashes during the selftest of xcbc(aes-asm) on x86_64
    machines. This patch fixes this. Also we use the alignmask
    of xcbc and not the alignmask of the underlying algorithm
    for the alignmnent calculation in xcbc_create now.
    
    Signed-off-by: Steffen Klassert <steffen.klassert@secunet.com>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/crypto/xcbc.c b/crypto/xcbc.c
index 1e30b31f33c6..bb7b67fba349 100644
--- a/crypto/xcbc.c
+++ b/crypto/xcbc.c
@@ -199,6 +199,7 @@ static int xcbc_create(struct crypto_template *tmpl, struct rtattr **tb)
 {
 	struct shash_instance *inst;
 	struct crypto_alg *alg;
+	unsigned long alignmask;
 	int err;
 
 	err = crypto_check_attr_type(tb, CRYPTO_ALG_TYPE_SHASH);
@@ -228,19 +229,20 @@ static int xcbc_create(struct crypto_template *tmpl, struct rtattr **tb)
 	if (err)
 		goto out_free_inst;
 
+	alignmask = alg->cra_alignmask | 3;
+	inst->alg.base.cra_alignmask = alignmask;
 	inst->alg.base.cra_priority = alg->cra_priority;
 	inst->alg.base.cra_blocksize = alg->cra_blocksize;
-	inst->alg.base.cra_alignmask = alg->cra_alignmask | 3;
 
 	inst->alg.digestsize = alg->cra_blocksize;
 	inst->alg.descsize = ALIGN(sizeof(struct xcbc_desc_ctx),
 				   crypto_tfm_ctx_alignment()) +
-			     (alg->cra_alignmask &
+			     (alignmask &
 			      ~(crypto_tfm_ctx_alignment() - 1)) +
 			     alg->cra_blocksize * 2;
 
 	inst->alg.base.cra_ctxsize = ALIGN(sizeof(struct xcbc_tfm_ctx),
-					   alg->cra_alignmask) +
+					   alignmask + 1) +
 				     alg->cra_blocksize * 2;
 	inst->alg.base.cra_init = xcbc_init_tfm;
 	inst->alg.base.cra_exit = xcbc_exit_tfm;

commit ac95301f271f32901e4007096aa3516def49eed2
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Wed Jul 22 14:37:15 2009 +0800

    crypto: xcbc - Fix shash conversion
    
    Although xcbc was converted to shash, it didn't obey the new
    requirement that all hash state must be stored in the descriptor
    rather than the transform.
    
    This patch fixes this issue and also optimises away the rekeying
    by precomputing K2 and K3 within setkey.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/crypto/xcbc.c b/crypto/xcbc.c
index 9d502e67a5c0..1e30b31f33c6 100644
--- a/crypto/xcbc.c
+++ b/crypto/xcbc.c
@@ -26,69 +26,67 @@
 static u_int32_t ks[12] = {0x01010101, 0x01010101, 0x01010101, 0x01010101,
 			   0x02020202, 0x02020202, 0x02020202, 0x02020202,
 			   0x03030303, 0x03030303, 0x03030303, 0x03030303};
+
 /*
  * +------------------------
  * | <parent tfm>
  * +------------------------
- * | crypto_xcbc_ctx
+ * | xcbc_tfm_ctx
  * +------------------------
- * | odds (block size)
+ * | consts (block size * 2)
  * +------------------------
- * | prev (block size)
+ */
+struct xcbc_tfm_ctx {
+	struct crypto_cipher *child;
+	u8 ctx[];
+};
+
+/*
  * +------------------------
- * | key (block size)
+ * | <shash desc>
  * +------------------------
- * | consts (block size * 3)
+ * | xcbc_desc_ctx
+ * +------------------------
+ * | odds (block size)
+ * +------------------------
+ * | prev (block size)
  * +------------------------
  */
-struct crypto_xcbc_ctx {
-	struct crypto_cipher *child;
-	u8 *odds;
-	u8 *prev;
-	u8 *key;
-	u8 *consts;
-	unsigned int keylen;
+struct xcbc_desc_ctx {
 	unsigned int len;
+	u8 ctx[];
 };
 
-static int _crypto_xcbc_digest_setkey(struct crypto_shash *parent,
-				      struct crypto_xcbc_ctx *ctx)
+static int crypto_xcbc_digest_setkey(struct crypto_shash *parent,
+				     const u8 *inkey, unsigned int keylen)
 {
+	unsigned long alignmask = crypto_shash_alignmask(parent);
+	struct xcbc_tfm_ctx *ctx = crypto_shash_ctx(parent);
 	int bs = crypto_shash_blocksize(parent);
+	u8 *consts = PTR_ALIGN(&ctx->ctx[0], alignmask + 1);
 	int err = 0;
 	u8 key1[bs];
 
-	if ((err = crypto_cipher_setkey(ctx->child, ctx->key, ctx->keylen)))
-	    return err;
+	if ((err = crypto_cipher_setkey(ctx->child, inkey, keylen)))
+		return err;
 
-	crypto_cipher_encrypt_one(ctx->child, key1, ctx->consts);
+	crypto_cipher_encrypt_one(ctx->child, consts, (u8 *)ks + bs);
+	crypto_cipher_encrypt_one(ctx->child, consts + bs, (u8 *)ks + bs * 2);
+	crypto_cipher_encrypt_one(ctx->child, key1, (u8 *)ks);
 
 	return crypto_cipher_setkey(ctx->child, key1, bs);
-}
-
-static int crypto_xcbc_digest_setkey(struct crypto_shash *parent,
-				     const u8 *inkey, unsigned int keylen)
-{
-	struct crypto_xcbc_ctx *ctx = crypto_shash_ctx(parent);
-
-	if (keylen != crypto_cipher_blocksize(ctx->child))
-		return -EINVAL;
 
-	ctx->keylen = keylen;
-	memcpy(ctx->key, inkey, keylen);
-	ctx->consts = (u8*)ks;
-
-	return _crypto_xcbc_digest_setkey(parent, ctx);
 }
 
 static int crypto_xcbc_digest_init(struct shash_desc *pdesc)
 {
-	struct crypto_xcbc_ctx *ctx = crypto_shash_ctx(pdesc->tfm);
+	unsigned long alignmask = crypto_shash_alignmask(pdesc->tfm);
+	struct xcbc_desc_ctx *ctx = shash_desc_ctx(pdesc);
 	int bs = crypto_shash_blocksize(pdesc->tfm);
+	u8 *prev = PTR_ALIGN(&ctx->ctx[0], alignmask + 1) + bs;
 
 	ctx->len = 0;
-	memset(ctx->odds, 0, bs);
-	memset(ctx->prev, 0, bs);
+	memset(prev, 0, bs);
 
 	return 0;
 }
@@ -97,39 +95,43 @@ static int crypto_xcbc_digest_update(struct shash_desc *pdesc, const u8 *p,
 				     unsigned int len)
 {
 	struct crypto_shash *parent = pdesc->tfm;
-	struct crypto_xcbc_ctx *ctx = crypto_shash_ctx(parent);
-	struct crypto_cipher *tfm = ctx->child;
+	unsigned long alignmask = crypto_shash_alignmask(parent);
+	struct xcbc_tfm_ctx *tctx = crypto_shash_ctx(parent);
+	struct xcbc_desc_ctx *ctx = shash_desc_ctx(pdesc);
+	struct crypto_cipher *tfm = tctx->child;
 	int bs = crypto_shash_blocksize(parent);
+	u8 *odds = PTR_ALIGN(&ctx->ctx[0], alignmask + 1);
+	u8 *prev = odds + bs;
 
 	/* checking the data can fill the block */
 	if ((ctx->len + len) <= bs) {
-		memcpy(ctx->odds + ctx->len, p, len);
+		memcpy(odds + ctx->len, p, len);
 		ctx->len += len;
 		return 0;
 	}
 
 	/* filling odds with new data and encrypting it */
-	memcpy(ctx->odds + ctx->len, p, bs - ctx->len);
+	memcpy(odds + ctx->len, p, bs - ctx->len);
 	len -= bs - ctx->len;
 	p += bs - ctx->len;
 
-	crypto_xor(ctx->prev, ctx->odds, bs);
-	crypto_cipher_encrypt_one(tfm, ctx->prev, ctx->prev);
+	crypto_xor(prev, odds, bs);
+	crypto_cipher_encrypt_one(tfm, prev, prev);
 
 	/* clearing the length */
 	ctx->len = 0;
 
 	/* encrypting the rest of data */
 	while (len > bs) {
-		crypto_xor(ctx->prev, p, bs);
-		crypto_cipher_encrypt_one(tfm, ctx->prev, ctx->prev);
+		crypto_xor(prev, p, bs);
+		crypto_cipher_encrypt_one(tfm, prev, prev);
 		p += bs;
 		len -= bs;
 	}
 
 	/* keeping the surplus of blocksize */
 	if (len) {
-		memcpy(ctx->odds, p, len);
+		memcpy(odds, p, len);
 		ctx->len = len;
 	}
 
@@ -139,29 +141,20 @@ static int crypto_xcbc_digest_update(struct shash_desc *pdesc, const u8 *p,
 static int crypto_xcbc_digest_final(struct shash_desc *pdesc, u8 *out)
 {
 	struct crypto_shash *parent = pdesc->tfm;
-	struct crypto_xcbc_ctx *ctx = crypto_shash_ctx(parent);
-	struct crypto_cipher *tfm = ctx->child;
+	unsigned long alignmask = crypto_shash_alignmask(parent);
+	struct xcbc_tfm_ctx *tctx = crypto_shash_ctx(parent);
+	struct xcbc_desc_ctx *ctx = shash_desc_ctx(pdesc);
+	struct crypto_cipher *tfm = tctx->child;
 	int bs = crypto_shash_blocksize(parent);
-	int err = 0;
+	u8 *consts = PTR_ALIGN(&tctx->ctx[0], alignmask + 1);
+	u8 *odds = PTR_ALIGN(&ctx->ctx[0], alignmask + 1);
+	u8 *prev = odds + bs;
+	unsigned int offset = 0;
 
-	if (ctx->len == bs) {
-		u8 key2[bs];
-
-		if ((err = crypto_cipher_setkey(tfm, ctx->key, ctx->keylen)) != 0)
-			return err;
-
-		crypto_cipher_encrypt_one(tfm, key2,
-					  (u8 *)(ctx->consts + bs));
-
-		crypto_xor(ctx->prev, ctx->odds, bs);
-		crypto_xor(ctx->prev, key2, bs);
-		_crypto_xcbc_digest_setkey(parent, ctx);
-
-		crypto_cipher_encrypt_one(tfm, out, ctx->prev);
-	} else {
-		u8 key3[bs];
+	if (ctx->len != bs) {
 		unsigned int rlen;
-		u8 *p = ctx->odds + ctx->len;
+		u8 *p = odds + ctx->len;
+
 		*p = 0x80;
 		p++;
 
@@ -169,19 +162,13 @@ static int crypto_xcbc_digest_final(struct shash_desc *pdesc, u8 *out)
 		if (rlen)
 			memset(p, 0, rlen);
 
-		if ((err = crypto_cipher_setkey(tfm, ctx->key, ctx->keylen)) != 0)
-			return err;
-
-		crypto_cipher_encrypt_one(tfm, key3,
-					  (u8 *)(ctx->consts + bs * 2));
+		offset += bs;
+	}
 
-		crypto_xor(ctx->prev, ctx->odds, bs);
-		crypto_xor(ctx->prev, key3, bs);
+	crypto_xor(prev, odds, bs);
+	crypto_xor(prev, consts + offset, bs);
 
-		_crypto_xcbc_digest_setkey(parent, ctx);
-
-		crypto_cipher_encrypt_one(tfm, out, ctx->prev);
-	}
+	crypto_cipher_encrypt_one(tfm, out, prev);
 
 	return 0;
 }
@@ -191,31 +178,20 @@ static int xcbc_init_tfm(struct crypto_tfm *tfm)
 	struct crypto_cipher *cipher;
 	struct crypto_instance *inst = (void *)tfm->__crt_alg;
 	struct crypto_spawn *spawn = crypto_instance_ctx(inst);
-	struct crypto_xcbc_ctx *ctx = crypto_tfm_ctx(tfm);
-	int bs = crypto_tfm_alg_blocksize(tfm);
+	struct xcbc_tfm_ctx *ctx = crypto_tfm_ctx(tfm);
 
 	cipher = crypto_spawn_cipher(spawn);
 	if (IS_ERR(cipher))
 		return PTR_ERR(cipher);
 
-	switch(bs) {
-	case 16:
-		break;
-	default:
-		return -EINVAL;
-	}
-
 	ctx->child = cipher;
-	ctx->odds = (u8*)(ctx+1);
-	ctx->prev = ctx->odds + bs;
-	ctx->key = ctx->prev + bs;
 
 	return 0;
 };
 
 static void xcbc_exit_tfm(struct crypto_tfm *tfm)
 {
-	struct crypto_xcbc_ctx *ctx = crypto_tfm_ctx(tfm);
+	struct xcbc_tfm_ctx *ctx = crypto_tfm_ctx(tfm);
 	crypto_free_cipher(ctx->child);
 }
 
@@ -254,12 +230,18 @@ static int xcbc_create(struct crypto_template *tmpl, struct rtattr **tb)
 
 	inst->alg.base.cra_priority = alg->cra_priority;
 	inst->alg.base.cra_blocksize = alg->cra_blocksize;
-	inst->alg.base.cra_alignmask = alg->cra_alignmask;
+	inst->alg.base.cra_alignmask = alg->cra_alignmask | 3;
 
 	inst->alg.digestsize = alg->cra_blocksize;
-	inst->alg.base.cra_ctxsize = sizeof(struct crypto_xcbc_ctx) +
-				     ALIGN(alg->cra_blocksize * 3,
-					   sizeof(void *));
+	inst->alg.descsize = ALIGN(sizeof(struct xcbc_desc_ctx),
+				   crypto_tfm_ctx_alignment()) +
+			     (alg->cra_alignmask &
+			      ~(crypto_tfm_ctx_alignment() - 1)) +
+			     alg->cra_blocksize * 2;
+
+	inst->alg.base.cra_ctxsize = ALIGN(sizeof(struct xcbc_tfm_ctx),
+					   alg->cra_alignmask) +
+				     alg->cra_blocksize * 2;
 	inst->alg.base.cra_init = xcbc_init_tfm;
 	inst->alg.base.cra_exit = xcbc_exit_tfm;
 

commit b588ef6e69bfc0944a17dc673ee166a00fa23de2
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Wed Jul 22 13:04:37 2009 +0800

    crypto: xcbc - Use crypto_xor
    
    This patch replaces the local xor function with the generic
    crypto_xor function.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/crypto/xcbc.c b/crypto/xcbc.c
index 3b991bf2fd92..9d502e67a5c0 100644
--- a/crypto/xcbc.c
+++ b/crypto/xcbc.c
@@ -47,19 +47,10 @@ struct crypto_xcbc_ctx {
 	u8 *prev;
 	u8 *key;
 	u8 *consts;
-	void (*xor)(u8 *a, const u8 *b, unsigned int bs);
 	unsigned int keylen;
 	unsigned int len;
 };
 
-static void xor_128(u8 *a, const u8 *b, unsigned int bs)
-{
-	((u32 *)a)[0] ^= ((u32 *)b)[0];
-	((u32 *)a)[1] ^= ((u32 *)b)[1];
-	((u32 *)a)[2] ^= ((u32 *)b)[2];
-	((u32 *)a)[3] ^= ((u32 *)b)[3];
-}
-
 static int _crypto_xcbc_digest_setkey(struct crypto_shash *parent,
 				      struct crypto_xcbc_ctx *ctx)
 {
@@ -122,7 +113,7 @@ static int crypto_xcbc_digest_update(struct shash_desc *pdesc, const u8 *p,
 	len -= bs - ctx->len;
 	p += bs - ctx->len;
 
-	ctx->xor(ctx->prev, ctx->odds, bs);
+	crypto_xor(ctx->prev, ctx->odds, bs);
 	crypto_cipher_encrypt_one(tfm, ctx->prev, ctx->prev);
 
 	/* clearing the length */
@@ -130,7 +121,7 @@ static int crypto_xcbc_digest_update(struct shash_desc *pdesc, const u8 *p,
 
 	/* encrypting the rest of data */
 	while (len > bs) {
-		ctx->xor(ctx->prev, p, bs);
+		crypto_xor(ctx->prev, p, bs);
 		crypto_cipher_encrypt_one(tfm, ctx->prev, ctx->prev);
 		p += bs;
 		len -= bs;
@@ -162,8 +153,8 @@ static int crypto_xcbc_digest_final(struct shash_desc *pdesc, u8 *out)
 		crypto_cipher_encrypt_one(tfm, key2,
 					  (u8 *)(ctx->consts + bs));
 
-		ctx->xor(ctx->prev, ctx->odds, bs);
-		ctx->xor(ctx->prev, key2, bs);
+		crypto_xor(ctx->prev, ctx->odds, bs);
+		crypto_xor(ctx->prev, key2, bs);
 		_crypto_xcbc_digest_setkey(parent, ctx);
 
 		crypto_cipher_encrypt_one(tfm, out, ctx->prev);
@@ -184,8 +175,8 @@ static int crypto_xcbc_digest_final(struct shash_desc *pdesc, u8 *out)
 		crypto_cipher_encrypt_one(tfm, key3,
 					  (u8 *)(ctx->consts + bs * 2));
 
-		ctx->xor(ctx->prev, ctx->odds, bs);
-		ctx->xor(ctx->prev, key3, bs);
+		crypto_xor(ctx->prev, ctx->odds, bs);
+		crypto_xor(ctx->prev, key3, bs);
 
 		_crypto_xcbc_digest_setkey(parent, ctx);
 
@@ -209,7 +200,6 @@ static int xcbc_init_tfm(struct crypto_tfm *tfm)
 
 	switch(bs) {
 	case 16:
-		ctx->xor = xor_128;
 		break;
 	default:
 		return -EINVAL;

commit b5ebd44eb7559ea6135d536bafd02323d2ef0547
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Wed Jul 15 16:53:33 2009 +0800

    crypto: xcbc - Fix incorrect error value when creating instance
    
    If shash_alloc_instance() fails, we return the wrong error value.
    This patch fixes it.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/crypto/xcbc.c b/crypto/xcbc.c
index b2cc8551b99f..3b991bf2fd92 100644
--- a/crypto/xcbc.c
+++ b/crypto/xcbc.c
@@ -252,6 +252,7 @@ static int xcbc_create(struct crypto_template *tmpl, struct rtattr **tb)
 	}
 
 	inst = shash_alloc_instance("xcbc", alg);
+	err = PTR_ERR(inst);
 	if (IS_ERR(inst))
 		goto out_put_alg;
 

commit 3106caab617c75c9a47706af3a8017318207be2d
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Sun Jul 12 12:48:32 2009 +0800

    crypto: xcbc - Switch to shash
    
    This patch converts the xcbc algorithm to the new shash type.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/crypto/xcbc.c b/crypto/xcbc.c
index b63b633e549c..b2cc8551b99f 100644
--- a/crypto/xcbc.c
+++ b/crypto/xcbc.c
@@ -19,15 +19,9 @@
  * 	Kazunori Miyazawa <miyazawa@linux-ipv6.org>
  */
 
-#include <crypto/scatterwalk.h>
-#include <linux/crypto.h>
+#include <crypto/internal/hash.h>
 #include <linux/err.h>
-#include <linux/hardirq.h>
 #include <linux/kernel.h>
-#include <linux/mm.h>
-#include <linux/rtnetlink.h>
-#include <linux/slab.h>
-#include <linux/scatterlist.h>
 
 static u_int32_t ks[12] = {0x01010101, 0x01010101, 0x01010101, 0x01010101,
 			   0x02020202, 0x02020202, 0x02020202, 0x02020202,
@@ -66,10 +60,10 @@ static void xor_128(u8 *a, const u8 *b, unsigned int bs)
 	((u32 *)a)[3] ^= ((u32 *)b)[3];
 }
 
-static int _crypto_xcbc_digest_setkey(struct crypto_hash *parent,
+static int _crypto_xcbc_digest_setkey(struct crypto_shash *parent,
 				      struct crypto_xcbc_ctx *ctx)
 {
-	int bs = crypto_hash_blocksize(parent);
+	int bs = crypto_shash_blocksize(parent);
 	int err = 0;
 	u8 key1[bs];
 
@@ -81,10 +75,10 @@ static int _crypto_xcbc_digest_setkey(struct crypto_hash *parent,
 	return crypto_cipher_setkey(ctx->child, key1, bs);
 }
 
-static int crypto_xcbc_digest_setkey(struct crypto_hash *parent,
+static int crypto_xcbc_digest_setkey(struct crypto_shash *parent,
 				     const u8 *inkey, unsigned int keylen)
 {
-	struct crypto_xcbc_ctx *ctx = crypto_hash_ctx_aligned(parent);
+	struct crypto_xcbc_ctx *ctx = crypto_shash_ctx(parent);
 
 	if (keylen != crypto_cipher_blocksize(ctx->child))
 		return -EINVAL;
@@ -96,10 +90,10 @@ static int crypto_xcbc_digest_setkey(struct crypto_hash *parent,
 	return _crypto_xcbc_digest_setkey(parent, ctx);
 }
 
-static int crypto_xcbc_digest_init(struct hash_desc *pdesc)
+static int crypto_xcbc_digest_init(struct shash_desc *pdesc)
 {
-	struct crypto_xcbc_ctx *ctx = crypto_hash_ctx_aligned(pdesc->tfm);
-	int bs = crypto_hash_blocksize(pdesc->tfm);
+	struct crypto_xcbc_ctx *ctx = crypto_shash_ctx(pdesc->tfm);
+	int bs = crypto_shash_blocksize(pdesc->tfm);
 
 	ctx->len = 0;
 	memset(ctx->odds, 0, bs);
@@ -108,102 +102,55 @@ static int crypto_xcbc_digest_init(struct hash_desc *pdesc)
 	return 0;
 }
 
-static int crypto_xcbc_digest_update2(struct hash_desc *pdesc,
-				      struct scatterlist *sg,
-				      unsigned int nbytes)
+static int crypto_xcbc_digest_update(struct shash_desc *pdesc, const u8 *p,
+				     unsigned int len)
 {
-	struct crypto_hash *parent = pdesc->tfm;
-	struct crypto_xcbc_ctx *ctx = crypto_hash_ctx_aligned(parent);
+	struct crypto_shash *parent = pdesc->tfm;
+	struct crypto_xcbc_ctx *ctx = crypto_shash_ctx(parent);
 	struct crypto_cipher *tfm = ctx->child;
-	int bs = crypto_hash_blocksize(parent);
-
-	for (;;) {
-		struct page *pg = sg_page(sg);
-		unsigned int offset = sg->offset;
-		unsigned int slen = sg->length;
-
-		if (unlikely(slen > nbytes))
-			slen = nbytes;
-
-		nbytes -= slen;
-
-		while (slen > 0) {
-			unsigned int len = min(slen, ((unsigned int)(PAGE_SIZE)) - offset);
-			char *p = crypto_kmap(pg, 0) + offset;
-
-			/* checking the data can fill the block */
-			if ((ctx->len + len) <= bs) {
-				memcpy(ctx->odds + ctx->len, p, len);
-				ctx->len += len;
-				slen -= len;
-
-				/* checking the rest of the page */
-				if (len + offset >= PAGE_SIZE) {
-					offset = 0;
-					pg++;
-				} else
-					offset += len;
-
-				crypto_kunmap(p, 0);
-				crypto_yield(pdesc->flags);
-				continue;
-			}
-
-			/* filling odds with new data and encrypting it */
-			memcpy(ctx->odds + ctx->len, p, bs - ctx->len);
-			len -= bs - ctx->len;
-			p += bs - ctx->len;
-
-			ctx->xor(ctx->prev, ctx->odds, bs);
-			crypto_cipher_encrypt_one(tfm, ctx->prev, ctx->prev);
-
-			/* clearing the length */
-			ctx->len = 0;
-
-			/* encrypting the rest of data */
-			while (len > bs) {
-				ctx->xor(ctx->prev, p, bs);
-				crypto_cipher_encrypt_one(tfm, ctx->prev,
-							  ctx->prev);
-				p += bs;
-				len -= bs;
-			}
-
-			/* keeping the surplus of blocksize */
-			if (len) {
-				memcpy(ctx->odds, p, len);
-				ctx->len = len;
-			}
-			crypto_kunmap(p, 0);
-			crypto_yield(pdesc->flags);
-			slen -= min(slen, ((unsigned int)(PAGE_SIZE)) - offset);
-			offset = 0;
-			pg++;
-		}
-
-		if (!nbytes)
-			break;
-		sg = scatterwalk_sg_next(sg);
+	int bs = crypto_shash_blocksize(parent);
+
+	/* checking the data can fill the block */
+	if ((ctx->len + len) <= bs) {
+		memcpy(ctx->odds + ctx->len, p, len);
+		ctx->len += len;
+		return 0;
 	}
 
-	return 0;
-}
+	/* filling odds with new data and encrypting it */
+	memcpy(ctx->odds + ctx->len, p, bs - ctx->len);
+	len -= bs - ctx->len;
+	p += bs - ctx->len;
 
-static int crypto_xcbc_digest_update(struct hash_desc *pdesc,
-				     struct scatterlist *sg,
-				     unsigned int nbytes)
-{
-	if (WARN_ON_ONCE(in_irq()))
-		return -EDEADLK;
-	return crypto_xcbc_digest_update2(pdesc, sg, nbytes);
+	ctx->xor(ctx->prev, ctx->odds, bs);
+	crypto_cipher_encrypt_one(tfm, ctx->prev, ctx->prev);
+
+	/* clearing the length */
+	ctx->len = 0;
+
+	/* encrypting the rest of data */
+	while (len > bs) {
+		ctx->xor(ctx->prev, p, bs);
+		crypto_cipher_encrypt_one(tfm, ctx->prev, ctx->prev);
+		p += bs;
+		len -= bs;
+	}
+
+	/* keeping the surplus of blocksize */
+	if (len) {
+		memcpy(ctx->odds, p, len);
+		ctx->len = len;
+	}
+
+	return 0;
 }
 
-static int crypto_xcbc_digest_final(struct hash_desc *pdesc, u8 *out)
+static int crypto_xcbc_digest_final(struct shash_desc *pdesc, u8 *out)
 {
-	struct crypto_hash *parent = pdesc->tfm;
-	struct crypto_xcbc_ctx *ctx = crypto_hash_ctx_aligned(parent);
+	struct crypto_shash *parent = pdesc->tfm;
+	struct crypto_xcbc_ctx *ctx = crypto_shash_ctx(parent);
 	struct crypto_cipher *tfm = ctx->child;
-	int bs = crypto_hash_blocksize(parent);
+	int bs = crypto_shash_blocksize(parent);
 	int err = 0;
 
 	if (ctx->len == bs) {
@@ -248,24 +195,13 @@ static int crypto_xcbc_digest_final(struct hash_desc *pdesc, u8 *out)
 	return 0;
 }
 
-static int crypto_xcbc_digest(struct hash_desc *pdesc,
-		  struct scatterlist *sg, unsigned int nbytes, u8 *out)
-{
-	if (WARN_ON_ONCE(in_irq()))
-		return -EDEADLK;
-
-	crypto_xcbc_digest_init(pdesc);
-	crypto_xcbc_digest_update2(pdesc, sg, nbytes);
-	return crypto_xcbc_digest_final(pdesc, out);
-}
-
 static int xcbc_init_tfm(struct crypto_tfm *tfm)
 {
 	struct crypto_cipher *cipher;
 	struct crypto_instance *inst = (void *)tfm->__crt_alg;
 	struct crypto_spawn *spawn = crypto_instance_ctx(inst);
-	struct crypto_xcbc_ctx *ctx = crypto_hash_ctx_aligned(__crypto_hash_cast(tfm));
-	int bs = crypto_hash_blocksize(__crypto_hash_cast(tfm));
+	struct crypto_xcbc_ctx *ctx = crypto_tfm_ctx(tfm);
+	int bs = crypto_tfm_alg_blocksize(tfm);
 
 	cipher = crypto_spawn_cipher(spawn);
 	if (IS_ERR(cipher))
@@ -289,70 +225,73 @@ static int xcbc_init_tfm(struct crypto_tfm *tfm)
 
 static void xcbc_exit_tfm(struct crypto_tfm *tfm)
 {
-	struct crypto_xcbc_ctx *ctx = crypto_hash_ctx_aligned(__crypto_hash_cast(tfm));
+	struct crypto_xcbc_ctx *ctx = crypto_tfm_ctx(tfm);
 	crypto_free_cipher(ctx->child);
 }
 
-static struct crypto_instance *xcbc_alloc(struct rtattr **tb)
+static int xcbc_create(struct crypto_template *tmpl, struct rtattr **tb)
 {
-	struct crypto_instance *inst;
+	struct shash_instance *inst;
 	struct crypto_alg *alg;
 	int err;
 
-	err = crypto_check_attr_type(tb, CRYPTO_ALG_TYPE_HASH);
+	err = crypto_check_attr_type(tb, CRYPTO_ALG_TYPE_SHASH);
 	if (err)
-		return ERR_PTR(err);
+		return err;
 
 	alg = crypto_get_attr_alg(tb, CRYPTO_ALG_TYPE_CIPHER,
 				  CRYPTO_ALG_TYPE_MASK);
 	if (IS_ERR(alg))
-		return ERR_CAST(alg);
+		return PTR_ERR(alg);
 
 	switch(alg->cra_blocksize) {
 	case 16:
 		break;
 	default:
-		inst = ERR_PTR(-EINVAL);
 		goto out_put_alg;
 	}
 
-	inst = crypto_alloc_instance("xcbc", alg);
+	inst = shash_alloc_instance("xcbc", alg);
 	if (IS_ERR(inst))
 		goto out_put_alg;
 
-	inst->alg.cra_flags = CRYPTO_ALG_TYPE_HASH;
-	inst->alg.cra_priority = alg->cra_priority;
-	inst->alg.cra_blocksize = alg->cra_blocksize;
-	inst->alg.cra_alignmask = alg->cra_alignmask;
-	inst->alg.cra_type = &crypto_hash_type;
-
-	inst->alg.cra_hash.digestsize = alg->cra_blocksize;
-	inst->alg.cra_ctxsize = sizeof(struct crypto_xcbc_ctx) +
-				ALIGN(inst->alg.cra_blocksize * 3, sizeof(void *));
-	inst->alg.cra_init = xcbc_init_tfm;
-	inst->alg.cra_exit = xcbc_exit_tfm;
-
-	inst->alg.cra_hash.init = crypto_xcbc_digest_init;
-	inst->alg.cra_hash.update = crypto_xcbc_digest_update;
-	inst->alg.cra_hash.final = crypto_xcbc_digest_final;
-	inst->alg.cra_hash.digest = crypto_xcbc_digest;
-	inst->alg.cra_hash.setkey = crypto_xcbc_digest_setkey;
+	err = crypto_init_spawn(shash_instance_ctx(inst), alg,
+				shash_crypto_instance(inst),
+				CRYPTO_ALG_TYPE_MASK);
+	if (err)
+		goto out_free_inst;
+
+	inst->alg.base.cra_priority = alg->cra_priority;
+	inst->alg.base.cra_blocksize = alg->cra_blocksize;
+	inst->alg.base.cra_alignmask = alg->cra_alignmask;
+
+	inst->alg.digestsize = alg->cra_blocksize;
+	inst->alg.base.cra_ctxsize = sizeof(struct crypto_xcbc_ctx) +
+				     ALIGN(alg->cra_blocksize * 3,
+					   sizeof(void *));
+	inst->alg.base.cra_init = xcbc_init_tfm;
+	inst->alg.base.cra_exit = xcbc_exit_tfm;
+
+	inst->alg.init = crypto_xcbc_digest_init;
+	inst->alg.update = crypto_xcbc_digest_update;
+	inst->alg.final = crypto_xcbc_digest_final;
+	inst->alg.setkey = crypto_xcbc_digest_setkey;
+
+	err = shash_register_instance(tmpl, inst);
+	if (err) {
+out_free_inst:
+		shash_free_instance(shash_crypto_instance(inst));
+	}
 
 out_put_alg:
 	crypto_mod_put(alg);
-	return inst;
-}
-
-static void xcbc_free(struct crypto_instance *inst)
-{
-	crypto_drop_spawn(crypto_instance_ctx(inst));
-	kfree(inst);
+	return err;
 }
 
 static struct crypto_template crypto_xcbc_tmpl = {
 	.name = "xcbc",
-	.alloc = xcbc_alloc,
-	.free = xcbc_free,
+	.create = xcbc_create,
+	.free = shash_free_instance,
 	.module = THIS_MODULE,
 };
 

commit 1edcf2e1ee2babb011cfca80ad9d202e9c491669
Author: Joy Latten <latten@austin.ibm.com>
Date:   Wed Apr 2 14:36:09 2008 +0800

    [CRYPTO] xcbc: Fix crash when ipsec uses xcbc-mac with big data chunk
    
    The kernel crashes when ipsec passes a udp packet of about 14XX bytes
    of data to aes-xcbc-mac.
    
    It seems the first xxxx bytes of the data are in first sg entry,
    and remaining xx bytes are in next sg entry. But we don't
    check next sg entry to see if we need to go look the page up.
    
    I noticed in hmac.c, we do a scatterwalk_sg_next(), to do this check
    and possible lookup, thus xcbc.c needs to use this routine too.
    
    A 15-hour run of an ipsec stress test sending streams of tcp and
    udp packets of various sizes,  using this patch and
    aes-xcbc-mac completed successfully, so hopefully this fixes the
    problem.
    
    Signed-off-by: Joy Latten <latten@austin.ibm.com>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/crypto/xcbc.c b/crypto/xcbc.c
index 2feb0f239c38..b63b633e549c 100644
--- a/crypto/xcbc.c
+++ b/crypto/xcbc.c
@@ -116,13 +116,11 @@ static int crypto_xcbc_digest_update2(struct hash_desc *pdesc,
 	struct crypto_xcbc_ctx *ctx = crypto_hash_ctx_aligned(parent);
 	struct crypto_cipher *tfm = ctx->child;
 	int bs = crypto_hash_blocksize(parent);
-	unsigned int i = 0;
 
-	do {
-
-		struct page *pg = sg_page(&sg[i]);
-		unsigned int offset = sg[i].offset;
-		unsigned int slen = sg[i].length;
+	for (;;) {
+		struct page *pg = sg_page(sg);
+		unsigned int offset = sg->offset;
+		unsigned int slen = sg->length;
 
 		if (unlikely(slen > nbytes))
 			slen = nbytes;
@@ -182,8 +180,11 @@ static int crypto_xcbc_digest_update2(struct hash_desc *pdesc,
 			offset = 0;
 			pg++;
 		}
-		i++;
-	} while (nbytes>0);
+
+		if (!nbytes)
+			break;
+		sg = scatterwalk_sg_next(sg);
+	}
 
 	return 0;
 }

commit 2f40a178e70030c4712fe63807c883f34c3645eb
Author: Joy Latten <latten@austin.ibm.com>
Date:   Thu Mar 6 19:28:44 2008 +0800

    [CRYPTO] xcbc: Fix crash with IPsec
    
    When using aes-xcbc-mac for authentication in IPsec,
    the kernel crashes. It seems this algorithm doesn't
    account for the space IPsec may make in scatterlist for authtag.
    Thus when crypto_xcbc_digest_update2() gets called,
    nbytes may be less than sg[i].length.
    Since nbytes is an unsigned number, it wraps
    at the end of the loop allowing us to go back
    into loop and causing crash in memcpy.
    
    I used update function in digest.c to model this fix.
    Please let me know if it looks ok.
    
    Signed-off-by: Joy Latten <latten@austin.ibm.com>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/crypto/xcbc.c b/crypto/xcbc.c
index 86727403e5ab..2feb0f239c38 100644
--- a/crypto/xcbc.c
+++ b/crypto/xcbc.c
@@ -124,6 +124,11 @@ static int crypto_xcbc_digest_update2(struct hash_desc *pdesc,
 		unsigned int offset = sg[i].offset;
 		unsigned int slen = sg[i].length;
 
+		if (unlikely(slen > nbytes))
+			slen = nbytes;
+
+		nbytes -= slen;
+
 		while (slen > 0) {
 			unsigned int len = min(slen, ((unsigned int)(PAGE_SIZE)) - offset);
 			char *p = crypto_kmap(pg, 0) + offset;
@@ -177,7 +182,6 @@ static int crypto_xcbc_digest_update2(struct hash_desc *pdesc,
 			offset = 0;
 			pg++;
 		}
-		nbytes-=sg[i].length;
 		i++;
 	} while (nbytes>0);
 

commit e231c2ee64eb1c5cd3c63c31da9dac7d888dcf7f
Author: David Howells <dhowells@redhat.com>
Date:   Thu Feb 7 00:15:26 2008 -0800

    Convert ERR_PTR(PTR_ERR(p)) instances to ERR_CAST(p)
    
    Convert instances of ERR_PTR(PTR_ERR(p)) to ERR_CAST(p) using:
    
    perl -spi -e 's/ERR_PTR[(]PTR_ERR[(](.*)[)][)]/ERR_CAST(\1)/' `grep -rl 'ERR_PTR[(]*PTR_ERR' fs crypto net security`
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/crypto/xcbc.c b/crypto/xcbc.c
index a82959df678c..86727403e5ab 100644
--- a/crypto/xcbc.c
+++ b/crypto/xcbc.c
@@ -301,7 +301,7 @@ static struct crypto_instance *xcbc_alloc(struct rtattr **tb)
 	alg = crypto_get_attr_alg(tb, CRYPTO_ALG_TYPE_CIPHER,
 				  CRYPTO_ALG_TYPE_MASK);
 	if (IS_ERR(alg))
-		return ERR_PTR(PTR_ERR(alg));
+		return ERR_CAST(alg);
 
 	switch(alg->cra_blocksize) {
 	case 16:

commit 94765b9e4cba8e3c51c292338db16aa174894d30
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Tue Jan 1 15:49:17 2008 +1100

    [CRYPTO] xcbc: Remove bogus hash/cipher test
    
    When setting the digest size xcbc tests to see if the underlying algorithm
    is a hash.  This is silly because we don't allow it to be a hash and we've
    specifically requested for a cipher.
    
    This patch removes the bogus test.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/crypto/xcbc.c b/crypto/xcbc.c
index e3d9503a034b..a82959df678c 100644
--- a/crypto/xcbc.c
+++ b/crypto/xcbc.c
@@ -321,10 +321,7 @@ static struct crypto_instance *xcbc_alloc(struct rtattr **tb)
 	inst->alg.cra_alignmask = alg->cra_alignmask;
 	inst->alg.cra_type = &crypto_hash_type;
 
-	inst->alg.cra_hash.digestsize =
-		(alg->cra_flags & CRYPTO_ALG_TYPE_MASK) ==
-		CRYPTO_ALG_TYPE_HASH ? alg->cra_hash.digestsize :
-				       alg->cra_blocksize;
+	inst->alg.cra_hash.digestsize = alg->cra_blocksize;
 	inst->alg.cra_ctxsize = sizeof(struct crypto_xcbc_ctx) +
 				ALIGN(inst->alg.cra_blocksize * 3, sizeof(void *));
 	inst->alg.cra_init = xcbc_init_tfm;

commit 1b87887d6c232becba77835b29a424cf78442b7d
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Tue Jan 1 15:44:50 2008 +1100

    [CRYPTO] xcbc: Fix algorithm leak when block size check fails
    
    When the underlying algorithm has a block size other than 16 we abort
    without freeing it.  In fact, we try to return the algorithm itself
    as an error!
    
    This patch plugs the leak and makes it return -EINVAL instead.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/crypto/xcbc.c b/crypto/xcbc.c
index 789cdeee6b7d..e3d9503a034b 100644
--- a/crypto/xcbc.c
+++ b/crypto/xcbc.c
@@ -307,7 +307,8 @@ static struct crypto_instance *xcbc_alloc(struct rtattr **tb)
 	case 16:
 		break;
 	default:
-		return ERR_PTR(PTR_ERR(alg));
+		inst = ERR_PTR(-EINVAL);
+		goto out_put_alg;
 	}
 
 	inst = crypto_alloc_instance("xcbc", alg);

commit 42c271c6c538857cb13c5ead5184d264d745f675
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Fri Dec 7 18:52:49 2007 +0800

    [CRYPTO] scatterwalk: Move scatterwalk.h to linux/crypto
    
    The scatterwalk infrastructure is used by algorithms so it needs to
    move out of crypto for future users that may live in drivers/crypto
    or asm/*/crypto.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/crypto/xcbc.c b/crypto/xcbc.c
index ac68f3b62fde..789cdeee6b7d 100644
--- a/crypto/xcbc.c
+++ b/crypto/xcbc.c
@@ -19,6 +19,7 @@
  * 	Kazunori Miyazawa <miyazawa@linux-ipv6.org>
  */
 
+#include <crypto/scatterwalk.h>
 #include <linux/crypto.h>
 #include <linux/err.h>
 #include <linux/hardirq.h>
@@ -27,7 +28,6 @@
 #include <linux/rtnetlink.h>
 #include <linux/slab.h>
 #include <linux/scatterlist.h>
-#include "internal.h"
 
 static u_int32_t ks[12] = {0x01010101, 0x01010101, 0x01010101, 0x01010101,
 			   0x02020202, 0x02020202, 0x02020202, 0x02020202,

commit 78c2f0b8c285c5305b3e67b0595200541e15eb43
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Mon Oct 22 19:40:16 2007 +0200

    [SG] Update crypto/ to sg helpers
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/crypto/xcbc.c b/crypto/xcbc.c
index 9f502b86e0ea..ac68f3b62fde 100644
--- a/crypto/xcbc.c
+++ b/crypto/xcbc.c
@@ -120,7 +120,7 @@ static int crypto_xcbc_digest_update2(struct hash_desc *pdesc,
 
 	do {
 
-		struct page *pg = sg[i].page;
+		struct page *pg = sg_page(&sg[i]);
 		unsigned int offset = sg[i].offset;
 		unsigned int slen = sg[i].length;
 

commit ebc610e5bc76df073221e64e86c3f7533a09ea40
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Mon Jan 1 18:37:02 2007 +1100

    [CRYPTO] templates: Pass type/mask when creating instances
    
    This patch passes the type/mask along when constructing instances of
    templates.  This is in preparation for templates that may support
    multiple types of instances depending on what is requested.  For example,
    the planned software async crypto driver will use this construct.
    
    For the moment this allows us to check whether the instance constructed
    is of the correct type and avoid returning success if the type does not
    match.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/crypto/xcbc.c b/crypto/xcbc.c
index 53e8ccbf0f5f..9f502b86e0ea 100644
--- a/crypto/xcbc.c
+++ b/crypto/xcbc.c
@@ -288,12 +288,18 @@ static void xcbc_exit_tfm(struct crypto_tfm *tfm)
 	crypto_free_cipher(ctx->child);
 }
 
-static struct crypto_instance *xcbc_alloc(void *param, unsigned int len)
+static struct crypto_instance *xcbc_alloc(struct rtattr **tb)
 {
 	struct crypto_instance *inst;
 	struct crypto_alg *alg;
-	alg = crypto_get_attr_alg(param, len, CRYPTO_ALG_TYPE_CIPHER,
-				  CRYPTO_ALG_TYPE_HASH_MASK | CRYPTO_ALG_ASYNC);
+	int err;
+
+	err = crypto_check_attr_type(tb, CRYPTO_ALG_TYPE_HASH);
+	if (err)
+		return ERR_PTR(err);
+
+	alg = crypto_get_attr_alg(tb, CRYPTO_ALG_TYPE_CIPHER,
+				  CRYPTO_ALG_TYPE_MASK);
 	if (IS_ERR(alg))
 		return ERR_PTR(PTR_ERR(alg));
 

commit 6b701dde8e0584f3bf0b6857d0e92f7ed15ed6f9
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Sun Dec 24 09:59:42 2006 +1100

    [CRYPTO] xcbc: Use new cipher interface
    
    This patch changes xcbc to use the new cipher encryt_one interface.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/crypto/xcbc.c b/crypto/xcbc.c
index d7b4be0a057d..53e8ccbf0f5f 100644
--- a/crypto/xcbc.c
+++ b/crypto/xcbc.c
@@ -48,7 +48,7 @@ static u_int32_t ks[12] = {0x01010101, 0x01010101, 0x01010101, 0x01010101,
  * +------------------------
  */
 struct crypto_xcbc_ctx {
-	struct crypto_tfm *child;
+	struct crypto_cipher *child;
 	u8 *odds;
 	u8 *prev;
 	u8 *key;
@@ -76,8 +76,7 @@ static int _crypto_xcbc_digest_setkey(struct crypto_hash *parent,
 	if ((err = crypto_cipher_setkey(ctx->child, ctx->key, ctx->keylen)))
 	    return err;
 
-	ctx->child->__crt_alg->cra_cipher.cia_encrypt(ctx->child, key1,
-			ctx->consts);
+	crypto_cipher_encrypt_one(ctx->child, key1, ctx->consts);
 
 	return crypto_cipher_setkey(ctx->child, key1, bs);
 }
@@ -87,7 +86,7 @@ static int crypto_xcbc_digest_setkey(struct crypto_hash *parent,
 {
 	struct crypto_xcbc_ctx *ctx = crypto_hash_ctx_aligned(parent);
 
-	if (keylen != crypto_tfm_alg_blocksize(ctx->child))
+	if (keylen != crypto_cipher_blocksize(ctx->child))
 		return -EINVAL;
 
 	ctx->keylen = keylen;
@@ -115,7 +114,7 @@ static int crypto_xcbc_digest_update2(struct hash_desc *pdesc,
 {
 	struct crypto_hash *parent = pdesc->tfm;
 	struct crypto_xcbc_ctx *ctx = crypto_hash_ctx_aligned(parent);
-	struct crypto_tfm *tfm = ctx->child;
+	struct crypto_cipher *tfm = ctx->child;
 	int bs = crypto_hash_blocksize(parent);
 	unsigned int i = 0;
 
@@ -143,7 +142,7 @@ static int crypto_xcbc_digest_update2(struct hash_desc *pdesc,
 					offset += len;
 
 				crypto_kunmap(p, 0);
-				crypto_yield(tfm->crt_flags);
+				crypto_yield(pdesc->flags);
 				continue;
 			}
 
@@ -153,7 +152,7 @@ static int crypto_xcbc_digest_update2(struct hash_desc *pdesc,
 			p += bs - ctx->len;
 
 			ctx->xor(ctx->prev, ctx->odds, bs);
-			tfm->__crt_alg->cra_cipher.cia_encrypt(tfm, ctx->prev, ctx->prev);
+			crypto_cipher_encrypt_one(tfm, ctx->prev, ctx->prev);
 
 			/* clearing the length */
 			ctx->len = 0;
@@ -161,7 +160,8 @@ static int crypto_xcbc_digest_update2(struct hash_desc *pdesc,
 			/* encrypting the rest of data */
 			while (len > bs) {
 				ctx->xor(ctx->prev, p, bs);
-				tfm->__crt_alg->cra_cipher.cia_encrypt(tfm, ctx->prev, ctx->prev);
+				crypto_cipher_encrypt_one(tfm, ctx->prev,
+							  ctx->prev);
 				p += bs;
 				len -= bs;
 			}
@@ -172,7 +172,7 @@ static int crypto_xcbc_digest_update2(struct hash_desc *pdesc,
 				ctx->len = len;
 			}
 			crypto_kunmap(p, 0);
-			crypto_yield(tfm->crt_flags);
+			crypto_yield(pdesc->flags);
 			slen -= min(slen, ((unsigned int)(PAGE_SIZE)) - offset);
 			offset = 0;
 			pg++;
@@ -197,7 +197,7 @@ static int crypto_xcbc_digest_final(struct hash_desc *pdesc, u8 *out)
 {
 	struct crypto_hash *parent = pdesc->tfm;
 	struct crypto_xcbc_ctx *ctx = crypto_hash_ctx_aligned(parent);
-	struct crypto_tfm *tfm = ctx->child;
+	struct crypto_cipher *tfm = ctx->child;
 	int bs = crypto_hash_blocksize(parent);
 	int err = 0;
 
@@ -207,13 +207,14 @@ static int crypto_xcbc_digest_final(struct hash_desc *pdesc, u8 *out)
 		if ((err = crypto_cipher_setkey(tfm, ctx->key, ctx->keylen)) != 0)
 			return err;
 
-		tfm->__crt_alg->cra_cipher.cia_encrypt(tfm, key2, (const u8*)(ctx->consts+bs));
+		crypto_cipher_encrypt_one(tfm, key2,
+					  (u8 *)(ctx->consts + bs));
 
 		ctx->xor(ctx->prev, ctx->odds, bs);
 		ctx->xor(ctx->prev, key2, bs);
 		_crypto_xcbc_digest_setkey(parent, ctx);
 
-		tfm->__crt_alg->cra_cipher.cia_encrypt(tfm, out, ctx->prev);
+		crypto_cipher_encrypt_one(tfm, out, ctx->prev);
 	} else {
 		u8 key3[bs];
 		unsigned int rlen;
@@ -228,14 +229,15 @@ static int crypto_xcbc_digest_final(struct hash_desc *pdesc, u8 *out)
 		if ((err = crypto_cipher_setkey(tfm, ctx->key, ctx->keylen)) != 0)
 			return err;
 
-		tfm->__crt_alg->cra_cipher.cia_encrypt(tfm, key3, (const u8*)(ctx->consts+bs*2));
+		crypto_cipher_encrypt_one(tfm, key3,
+					  (u8 *)(ctx->consts + bs * 2));
 
 		ctx->xor(ctx->prev, ctx->odds, bs);
 		ctx->xor(ctx->prev, key3, bs);
 
 		_crypto_xcbc_digest_setkey(parent, ctx);
 
-		tfm->__crt_alg->cra_cipher.cia_encrypt(tfm, out, ctx->prev);
+		crypto_cipher_encrypt_one(tfm, out, ctx->prev);
 	}
 
 	return 0;

commit 2e306ee016fd4750289e65c3b1856db569f1f3f2
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Sun Dec 17 10:05:58 2006 +1100

    [CRYPTO] api: Add type-safe spawns
    
    This patch allows spawns of specific types (e.g., cipher) to be allocated.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/crypto/xcbc.c b/crypto/xcbc.c
index 317e9f08fc04..d7b4be0a057d 100644
--- a/crypto/xcbc.c
+++ b/crypto/xcbc.c
@@ -254,14 +254,15 @@ static int crypto_xcbc_digest(struct hash_desc *pdesc,
 
 static int xcbc_init_tfm(struct crypto_tfm *tfm)
 {
+	struct crypto_cipher *cipher;
 	struct crypto_instance *inst = (void *)tfm->__crt_alg;
 	struct crypto_spawn *spawn = crypto_instance_ctx(inst);
 	struct crypto_xcbc_ctx *ctx = crypto_hash_ctx_aligned(__crypto_hash_cast(tfm));
 	int bs = crypto_hash_blocksize(__crypto_hash_cast(tfm));
 
-	tfm = crypto_spawn_tfm(spawn);
-	if (IS_ERR(tfm))
-		return PTR_ERR(tfm);
+	cipher = crypto_spawn_cipher(spawn);
+	if (IS_ERR(cipher))
+		return PTR_ERR(cipher);
 
 	switch(bs) {
 	case 16:
@@ -271,7 +272,7 @@ static int xcbc_init_tfm(struct crypto_tfm *tfm)
 		return -EINVAL;
 	}
 
-	ctx->child = crypto_cipher_cast(tfm);
+	ctx->child = cipher;
 	ctx->odds = (u8*)(ctx+1);
 	ctx->prev = ctx->odds + bs;
 	ctx->key = ctx->prev + bs;

commit fb469840b8c34b2f95b40a64b271f245cc1075b7
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Sun Dec 10 10:45:28 2006 +1100

    [CRYPTO] all: Check for usage in hard IRQ context
    
    Using blkcipher/hash crypto operations in hard IRQ context can lead
    to random memory corruption due to the reuse of kmap_atomic slots.
    Since crypto operations were never meant to be used in hard IRQ
    contexts, this patch checks for such usage and returns an error
    before kmap_atomic is performed.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/crypto/xcbc.c b/crypto/xcbc.c
index 9347eb6bcf69..317e9f08fc04 100644
--- a/crypto/xcbc.c
+++ b/crypto/xcbc.c
@@ -21,6 +21,7 @@
 
 #include <linux/crypto.h>
 #include <linux/err.h>
+#include <linux/hardirq.h>
 #include <linux/kernel.h>
 #include <linux/mm.h>
 #include <linux/rtnetlink.h>
@@ -108,9 +109,9 @@ static int crypto_xcbc_digest_init(struct hash_desc *pdesc)
 	return 0;
 }
 
-static int crypto_xcbc_digest_update(struct hash_desc *pdesc,
-				     struct scatterlist *sg,
-				     unsigned int nbytes)
+static int crypto_xcbc_digest_update2(struct hash_desc *pdesc,
+				      struct scatterlist *sg,
+				      unsigned int nbytes)
 {
 	struct crypto_hash *parent = pdesc->tfm;
 	struct crypto_xcbc_ctx *ctx = crypto_hash_ctx_aligned(parent);
@@ -183,6 +184,15 @@ static int crypto_xcbc_digest_update(struct hash_desc *pdesc,
 	return 0;
 }
 
+static int crypto_xcbc_digest_update(struct hash_desc *pdesc,
+				     struct scatterlist *sg,
+				     unsigned int nbytes)
+{
+	if (WARN_ON_ONCE(in_irq()))
+		return -EDEADLK;
+	return crypto_xcbc_digest_update2(pdesc, sg, nbytes);
+}
+
 static int crypto_xcbc_digest_final(struct hash_desc *pdesc, u8 *out)
 {
 	struct crypto_hash *parent = pdesc->tfm;
@@ -234,8 +244,11 @@ static int crypto_xcbc_digest_final(struct hash_desc *pdesc, u8 *out)
 static int crypto_xcbc_digest(struct hash_desc *pdesc,
 		  struct scatterlist *sg, unsigned int nbytes, u8 *out)
 {
+	if (WARN_ON_ONCE(in_irq()))
+		return -EDEADLK;
+
 	crypto_xcbc_digest_init(pdesc);
-	crypto_xcbc_digest_update(pdesc, sg, nbytes);
+	crypto_xcbc_digest_update2(pdesc, sg, nbytes);
 	return crypto_xcbc_digest_final(pdesc, out);
 }
 

commit 5b37538a514cf4c8746be9d09e8a9f564e7df939
Author: Adrian Bunk <bunk@stusta.de>
Date:   Fri Nov 17 13:43:04 2006 +1100

    [CRYPTO] xcbc: Make needlessly global code static
    
    On Tue, Nov 14, 2006 at 01:41:25AM -0800, Andrew Morton wrote:
    >...
    > Changes since 2.6.19-rc5-mm2:
    >...
    >  git-cryptodev.patch
    >...
    >  git trees
    >...
    
    This patch makes some needlessly global code static.
    
    Signed-off-by: Adrian Bunk <bunk@stusta.de>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/crypto/xcbc.c b/crypto/xcbc.c
index f5929501bd48..9347eb6bcf69 100644
--- a/crypto/xcbc.c
+++ b/crypto/xcbc.c
@@ -28,9 +28,9 @@
 #include <linux/scatterlist.h>
 #include "internal.h"
 
-u_int32_t ks[12] = {0x01010101, 0x01010101, 0x01010101, 0x01010101,
-		    0x02020202, 0x02020202, 0x02020202, 0x02020202,
-		    0x03030303, 0x03030303, 0x03030303, 0x03030303};
+static u_int32_t ks[12] = {0x01010101, 0x01010101, 0x01010101, 0x01010101,
+			   0x02020202, 0x02020202, 0x02020202, 0x02020202,
+			   0x03030303, 0x03030303, 0x03030303, 0x03030303};
 /*
  * +------------------------
  * | <parent tfm>
@@ -96,7 +96,7 @@ static int crypto_xcbc_digest_setkey(struct crypto_hash *parent,
 	return _crypto_xcbc_digest_setkey(parent, ctx);
 }
 
-int crypto_xcbc_digest_init(struct hash_desc *pdesc)
+static int crypto_xcbc_digest_init(struct hash_desc *pdesc)
 {
 	struct crypto_xcbc_ctx *ctx = crypto_hash_ctx_aligned(pdesc->tfm);
 	int bs = crypto_hash_blocksize(pdesc->tfm);
@@ -108,7 +108,9 @@ int crypto_xcbc_digest_init(struct hash_desc *pdesc)
 	return 0;
 }
 
-int crypto_xcbc_digest_update(struct hash_desc *pdesc, struct scatterlist *sg, unsigned int nbytes)
+static int crypto_xcbc_digest_update(struct hash_desc *pdesc,
+				     struct scatterlist *sg,
+				     unsigned int nbytes)
 {
 	struct crypto_hash *parent = pdesc->tfm;
 	struct crypto_xcbc_ctx *ctx = crypto_hash_ctx_aligned(parent);
@@ -181,7 +183,7 @@ int crypto_xcbc_digest_update(struct hash_desc *pdesc, struct scatterlist *sg, u
 	return 0;
 }
 
-int crypto_xcbc_digest_final(struct hash_desc *pdesc, u8 *out)
+static int crypto_xcbc_digest_final(struct hash_desc *pdesc, u8 *out)
 {
 	struct crypto_hash *parent = pdesc->tfm;
 	struct crypto_xcbc_ctx *ctx = crypto_hash_ctx_aligned(parent);

commit 333b0d7eeacbd47159daf23757aa81368470c409
Author: Kazunori MIYAZAWA <miyazawa@linux-ipv6.org>
Date:   Sat Oct 28 13:15:24 2006 +1000

    [CRYPTO] xcbc: New algorithm
    
    This is core code of XCBC.
    
    XCBC is an algorithm that forms a MAC algorithm out of a cipher algorithm.
    For example, AES-XCBC-MAC is a MAC algorithm based on the AES cipher
    algorithm.
    
    Signed-off-by: Kazunori MIYAZAWA <miyazawa@linux-ipv6.org>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/crypto/xcbc.c b/crypto/xcbc.c
new file mode 100644
index 000000000000..f5929501bd48
--- /dev/null
+++ b/crypto/xcbc.c
@@ -0,0 +1,346 @@
+/*
+ * Copyright (C)2006 USAGI/WIDE Project
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
+ *
+ * Author:
+ * 	Kazunori Miyazawa <miyazawa@linux-ipv6.org>
+ */
+
+#include <linux/crypto.h>
+#include <linux/err.h>
+#include <linux/kernel.h>
+#include <linux/mm.h>
+#include <linux/rtnetlink.h>
+#include <linux/slab.h>
+#include <linux/scatterlist.h>
+#include "internal.h"
+
+u_int32_t ks[12] = {0x01010101, 0x01010101, 0x01010101, 0x01010101,
+		    0x02020202, 0x02020202, 0x02020202, 0x02020202,
+		    0x03030303, 0x03030303, 0x03030303, 0x03030303};
+/*
+ * +------------------------
+ * | <parent tfm>
+ * +------------------------
+ * | crypto_xcbc_ctx
+ * +------------------------
+ * | odds (block size)
+ * +------------------------
+ * | prev (block size)
+ * +------------------------
+ * | key (block size)
+ * +------------------------
+ * | consts (block size * 3)
+ * +------------------------
+ */
+struct crypto_xcbc_ctx {
+	struct crypto_tfm *child;
+	u8 *odds;
+	u8 *prev;
+	u8 *key;
+	u8 *consts;
+	void (*xor)(u8 *a, const u8 *b, unsigned int bs);
+	unsigned int keylen;
+	unsigned int len;
+};
+
+static void xor_128(u8 *a, const u8 *b, unsigned int bs)
+{
+	((u32 *)a)[0] ^= ((u32 *)b)[0];
+	((u32 *)a)[1] ^= ((u32 *)b)[1];
+	((u32 *)a)[2] ^= ((u32 *)b)[2];
+	((u32 *)a)[3] ^= ((u32 *)b)[3];
+}
+
+static int _crypto_xcbc_digest_setkey(struct crypto_hash *parent,
+				      struct crypto_xcbc_ctx *ctx)
+{
+	int bs = crypto_hash_blocksize(parent);
+	int err = 0;
+	u8 key1[bs];
+
+	if ((err = crypto_cipher_setkey(ctx->child, ctx->key, ctx->keylen)))
+	    return err;
+
+	ctx->child->__crt_alg->cra_cipher.cia_encrypt(ctx->child, key1,
+			ctx->consts);
+
+	return crypto_cipher_setkey(ctx->child, key1, bs);
+}
+
+static int crypto_xcbc_digest_setkey(struct crypto_hash *parent,
+				     const u8 *inkey, unsigned int keylen)
+{
+	struct crypto_xcbc_ctx *ctx = crypto_hash_ctx_aligned(parent);
+
+	if (keylen != crypto_tfm_alg_blocksize(ctx->child))
+		return -EINVAL;
+
+	ctx->keylen = keylen;
+	memcpy(ctx->key, inkey, keylen);
+	ctx->consts = (u8*)ks;
+
+	return _crypto_xcbc_digest_setkey(parent, ctx);
+}
+
+int crypto_xcbc_digest_init(struct hash_desc *pdesc)
+{
+	struct crypto_xcbc_ctx *ctx = crypto_hash_ctx_aligned(pdesc->tfm);
+	int bs = crypto_hash_blocksize(pdesc->tfm);
+
+	ctx->len = 0;
+	memset(ctx->odds, 0, bs);
+	memset(ctx->prev, 0, bs);
+
+	return 0;
+}
+
+int crypto_xcbc_digest_update(struct hash_desc *pdesc, struct scatterlist *sg, unsigned int nbytes)
+{
+	struct crypto_hash *parent = pdesc->tfm;
+	struct crypto_xcbc_ctx *ctx = crypto_hash_ctx_aligned(parent);
+	struct crypto_tfm *tfm = ctx->child;
+	int bs = crypto_hash_blocksize(parent);
+	unsigned int i = 0;
+
+	do {
+
+		struct page *pg = sg[i].page;
+		unsigned int offset = sg[i].offset;
+		unsigned int slen = sg[i].length;
+
+		while (slen > 0) {
+			unsigned int len = min(slen, ((unsigned int)(PAGE_SIZE)) - offset);
+			char *p = crypto_kmap(pg, 0) + offset;
+
+			/* checking the data can fill the block */
+			if ((ctx->len + len) <= bs) {
+				memcpy(ctx->odds + ctx->len, p, len);
+				ctx->len += len;
+				slen -= len;
+
+				/* checking the rest of the page */
+				if (len + offset >= PAGE_SIZE) {
+					offset = 0;
+					pg++;
+				} else
+					offset += len;
+
+				crypto_kunmap(p, 0);
+				crypto_yield(tfm->crt_flags);
+				continue;
+			}
+
+			/* filling odds with new data and encrypting it */
+			memcpy(ctx->odds + ctx->len, p, bs - ctx->len);
+			len -= bs - ctx->len;
+			p += bs - ctx->len;
+
+			ctx->xor(ctx->prev, ctx->odds, bs);
+			tfm->__crt_alg->cra_cipher.cia_encrypt(tfm, ctx->prev, ctx->prev);
+
+			/* clearing the length */
+			ctx->len = 0;
+
+			/* encrypting the rest of data */
+			while (len > bs) {
+				ctx->xor(ctx->prev, p, bs);
+				tfm->__crt_alg->cra_cipher.cia_encrypt(tfm, ctx->prev, ctx->prev);
+				p += bs;
+				len -= bs;
+			}
+
+			/* keeping the surplus of blocksize */
+			if (len) {
+				memcpy(ctx->odds, p, len);
+				ctx->len = len;
+			}
+			crypto_kunmap(p, 0);
+			crypto_yield(tfm->crt_flags);
+			slen -= min(slen, ((unsigned int)(PAGE_SIZE)) - offset);
+			offset = 0;
+			pg++;
+		}
+		nbytes-=sg[i].length;
+		i++;
+	} while (nbytes>0);
+
+	return 0;
+}
+
+int crypto_xcbc_digest_final(struct hash_desc *pdesc, u8 *out)
+{
+	struct crypto_hash *parent = pdesc->tfm;
+	struct crypto_xcbc_ctx *ctx = crypto_hash_ctx_aligned(parent);
+	struct crypto_tfm *tfm = ctx->child;
+	int bs = crypto_hash_blocksize(parent);
+	int err = 0;
+
+	if (ctx->len == bs) {
+		u8 key2[bs];
+
+		if ((err = crypto_cipher_setkey(tfm, ctx->key, ctx->keylen)) != 0)
+			return err;
+
+		tfm->__crt_alg->cra_cipher.cia_encrypt(tfm, key2, (const u8*)(ctx->consts+bs));
+
+		ctx->xor(ctx->prev, ctx->odds, bs);
+		ctx->xor(ctx->prev, key2, bs);
+		_crypto_xcbc_digest_setkey(parent, ctx);
+
+		tfm->__crt_alg->cra_cipher.cia_encrypt(tfm, out, ctx->prev);
+	} else {
+		u8 key3[bs];
+		unsigned int rlen;
+		u8 *p = ctx->odds + ctx->len;
+		*p = 0x80;
+		p++;
+
+		rlen = bs - ctx->len -1;
+		if (rlen)
+			memset(p, 0, rlen);
+
+		if ((err = crypto_cipher_setkey(tfm, ctx->key, ctx->keylen)) != 0)
+			return err;
+
+		tfm->__crt_alg->cra_cipher.cia_encrypt(tfm, key3, (const u8*)(ctx->consts+bs*2));
+
+		ctx->xor(ctx->prev, ctx->odds, bs);
+		ctx->xor(ctx->prev, key3, bs);
+
+		_crypto_xcbc_digest_setkey(parent, ctx);
+
+		tfm->__crt_alg->cra_cipher.cia_encrypt(tfm, out, ctx->prev);
+	}
+
+	return 0;
+}
+
+static int crypto_xcbc_digest(struct hash_desc *pdesc,
+		  struct scatterlist *sg, unsigned int nbytes, u8 *out)
+{
+	crypto_xcbc_digest_init(pdesc);
+	crypto_xcbc_digest_update(pdesc, sg, nbytes);
+	return crypto_xcbc_digest_final(pdesc, out);
+}
+
+static int xcbc_init_tfm(struct crypto_tfm *tfm)
+{
+	struct crypto_instance *inst = (void *)tfm->__crt_alg;
+	struct crypto_spawn *spawn = crypto_instance_ctx(inst);
+	struct crypto_xcbc_ctx *ctx = crypto_hash_ctx_aligned(__crypto_hash_cast(tfm));
+	int bs = crypto_hash_blocksize(__crypto_hash_cast(tfm));
+
+	tfm = crypto_spawn_tfm(spawn);
+	if (IS_ERR(tfm))
+		return PTR_ERR(tfm);
+
+	switch(bs) {
+	case 16:
+		ctx->xor = xor_128;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	ctx->child = crypto_cipher_cast(tfm);
+	ctx->odds = (u8*)(ctx+1);
+	ctx->prev = ctx->odds + bs;
+	ctx->key = ctx->prev + bs;
+
+	return 0;
+};
+
+static void xcbc_exit_tfm(struct crypto_tfm *tfm)
+{
+	struct crypto_xcbc_ctx *ctx = crypto_hash_ctx_aligned(__crypto_hash_cast(tfm));
+	crypto_free_cipher(ctx->child);
+}
+
+static struct crypto_instance *xcbc_alloc(void *param, unsigned int len)
+{
+	struct crypto_instance *inst;
+	struct crypto_alg *alg;
+	alg = crypto_get_attr_alg(param, len, CRYPTO_ALG_TYPE_CIPHER,
+				  CRYPTO_ALG_TYPE_HASH_MASK | CRYPTO_ALG_ASYNC);
+	if (IS_ERR(alg))
+		return ERR_PTR(PTR_ERR(alg));
+
+	switch(alg->cra_blocksize) {
+	case 16:
+		break;
+	default:
+		return ERR_PTR(PTR_ERR(alg));
+	}
+
+	inst = crypto_alloc_instance("xcbc", alg);
+	if (IS_ERR(inst))
+		goto out_put_alg;
+
+	inst->alg.cra_flags = CRYPTO_ALG_TYPE_HASH;
+	inst->alg.cra_priority = alg->cra_priority;
+	inst->alg.cra_blocksize = alg->cra_blocksize;
+	inst->alg.cra_alignmask = alg->cra_alignmask;
+	inst->alg.cra_type = &crypto_hash_type;
+
+	inst->alg.cra_hash.digestsize =
+		(alg->cra_flags & CRYPTO_ALG_TYPE_MASK) ==
+		CRYPTO_ALG_TYPE_HASH ? alg->cra_hash.digestsize :
+				       alg->cra_blocksize;
+	inst->alg.cra_ctxsize = sizeof(struct crypto_xcbc_ctx) +
+				ALIGN(inst->alg.cra_blocksize * 3, sizeof(void *));
+	inst->alg.cra_init = xcbc_init_tfm;
+	inst->alg.cra_exit = xcbc_exit_tfm;
+
+	inst->alg.cra_hash.init = crypto_xcbc_digest_init;
+	inst->alg.cra_hash.update = crypto_xcbc_digest_update;
+	inst->alg.cra_hash.final = crypto_xcbc_digest_final;
+	inst->alg.cra_hash.digest = crypto_xcbc_digest;
+	inst->alg.cra_hash.setkey = crypto_xcbc_digest_setkey;
+
+out_put_alg:
+	crypto_mod_put(alg);
+	return inst;
+}
+
+static void xcbc_free(struct crypto_instance *inst)
+{
+	crypto_drop_spawn(crypto_instance_ctx(inst));
+	kfree(inst);
+}
+
+static struct crypto_template crypto_xcbc_tmpl = {
+	.name = "xcbc",
+	.alloc = xcbc_alloc,
+	.free = xcbc_free,
+	.module = THIS_MODULE,
+};
+
+static int __init crypto_xcbc_module_init(void)
+{
+	return crypto_register_template(&crypto_xcbc_tmpl);
+}
+
+static void __exit crypto_xcbc_module_exit(void)
+{
+	crypto_unregister_template(&crypto_xcbc_tmpl);
+}
+
+module_init(crypto_xcbc_module_init);
+module_exit(crypto_xcbc_module_exit);
+
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("XCBC keyed hash algorithm");
