commit 6af2ed53f0402c09b36d2b38698e18a25ca732a7
Author: Janakarajan Natarajan <Janakarajan.Natarajan@amd.com>
Date:   Tue Nov 5 17:16:55 2019 +0000

    cpupower: mperf_monitor: Update cpupower to use the RDPRU instruction
    
    AMD Zen 2 introduces the RDPRU instruction which can be used to access some
    processor registers which are typically only accessible in privilege level
    0. ECX specifies the register to read and EDX:EAX will contain the value read.
    
    ECX: 0 - Register MPERF
         1 - Register APERF
    
    This has the added advantage of not having to use the msr module, since the
    userspace to kernel transitions which occur during each read_msr() might
    cause APERF and MPERF to go out of sync.
    
    Signed-off-by: Janakarajan Natarajan <Janakarajan.Natarajan@amd.com>
    Acked-by: Thomas Renninger <trenn@suse.de>
    Signed-off-by: Shuah Khan <skhan@linuxfoundation.org>

diff --git a/tools/power/cpupower/utils/idle_monitor/mperf_monitor.c b/tools/power/cpupower/utils/idle_monitor/mperf_monitor.c
index afb2e6f8edd3..e7d48cb563c0 100644
--- a/tools/power/cpupower/utils/idle_monitor/mperf_monitor.c
+++ b/tools/power/cpupower/utils/idle_monitor/mperf_monitor.c
@@ -19,6 +19,10 @@
 #define MSR_APERF	0xE8
 #define MSR_MPERF	0xE7
 
+#define RDPRU ".byte 0x0f, 0x01, 0xfd"
+#define RDPRU_ECX_MPERF	0
+#define RDPRU_ECX_APERF	1
+
 #define MSR_TSC	0x10
 
 #define MSR_AMD_HWCR 0xc0010015
@@ -89,6 +93,8 @@ static int mperf_get_tsc(unsigned long long *tsc)
 static int get_aperf_mperf(int cpu, unsigned long long *aval,
 				    unsigned long long *mval)
 {
+	unsigned long low_a, high_a;
+	unsigned long low_m, high_m;
 	int ret;
 
 	/*
@@ -101,6 +107,20 @@ static int get_aperf_mperf(int cpu, unsigned long long *aval,
 			return 1;
 	}
 
+	if (cpupower_cpu_info.caps & CPUPOWER_CAP_AMD_RDPRU) {
+		asm volatile(RDPRU
+			     : "=a" (low_a), "=d" (high_a)
+			     : "c" (RDPRU_ECX_APERF));
+		asm volatile(RDPRU
+			     : "=a" (low_m), "=d" (high_m)
+			     : "c" (RDPRU_ECX_MPERF));
+
+		*aval = ((low_a) | (high_a) << 32);
+		*mval = ((low_m) | (high_m) << 32);
+
+		return 0;
+	}
+
 	ret  = read_msr(cpu, MSR_APERF, aval);
 	ret |= read_msr(cpu, MSR_MPERF, mval);
 

commit 7adafe541fe5e015261a92d39db8b163db477337
Author: Janakarajan Natarajan <Janakarajan.Natarajan@amd.com>
Date:   Tue Nov 5 17:16:54 2019 +0000

    cpupower: mperf_monitor: Introduce per_cpu_schedule flag
    
    The per_cpu_schedule flag is used to move the cpupower process to the cpu
    on which we are looking to read the APERF/MPERF registers.
    
    This prevents IPIs from being generated by read_msr()s as we are already
    on the cpu of interest.
    
    Ex: If cpupower is running on CPU 0 and we execute
    
        read_msr(20, MSR_APERF, val) then,
        read_msr(20, MSR_MPERF, val)
    
        the msr module will generate an IPI from CPU 0 to CPU 20 to query
        for the MSR_APERF and then the MSR_MPERF in separate IPIs.
    
    This delay, caused by IPI latency, between reading the APERF and MPERF
    registers may cause both of them to go out of sync.
    
    The use of the per_cpu_schedule flag reduces the probability of this
    from happening. It comes at the cost of a negligible increase in cpu
    consumption caused by the migration of cpupower across each of the
    cpus of the system.
    
    Signed-off-by: Janakarajan Natarajan <Janakarajan.Natarajan@amd.com>
    Acked-by: Thomas Renninger <trenn@suse.de>
    Signed-off-by: Shuah Khan <skhan@linuxfoundation.org>

diff --git a/tools/power/cpupower/utils/idle_monitor/mperf_monitor.c b/tools/power/cpupower/utils/idle_monitor/mperf_monitor.c
index 7cae74202a4d..afb2e6f8edd3 100644
--- a/tools/power/cpupower/utils/idle_monitor/mperf_monitor.c
+++ b/tools/power/cpupower/utils/idle_monitor/mperf_monitor.c
@@ -86,15 +86,35 @@ static int mperf_get_tsc(unsigned long long *tsc)
 	return ret;
 }
 
+static int get_aperf_mperf(int cpu, unsigned long long *aval,
+				    unsigned long long *mval)
+{
+	int ret;
+
+	/*
+	 * Running on the cpu from which we read the registers will
+	 * prevent APERF/MPERF from going out of sync because of IPI
+	 * latency introduced by read_msr()s.
+	 */
+	if (mperf_monitor.flags.per_cpu_schedule) {
+		if (bind_cpu(cpu))
+			return 1;
+	}
+
+	ret  = read_msr(cpu, MSR_APERF, aval);
+	ret |= read_msr(cpu, MSR_MPERF, mval);
+
+	return ret;
+}
+
 static int mperf_init_stats(unsigned int cpu)
 {
-	unsigned long long val;
+	unsigned long long aval, mval;
 	int ret;
 
-	ret = read_msr(cpu, MSR_APERF, &val);
-	aperf_previous_count[cpu] = val;
-	ret |= read_msr(cpu, MSR_MPERF, &val);
-	mperf_previous_count[cpu] = val;
+	ret = get_aperf_mperf(cpu, &aval, &mval);
+	aperf_previous_count[cpu] = aval;
+	mperf_previous_count[cpu] = mval;
 	is_valid[cpu] = !ret;
 
 	return 0;
@@ -102,13 +122,12 @@ static int mperf_init_stats(unsigned int cpu)
 
 static int mperf_measure_stats(unsigned int cpu)
 {
-	unsigned long long val;
+	unsigned long long aval, mval;
 	int ret;
 
-	ret = read_msr(cpu, MSR_APERF, &val);
-	aperf_current_count[cpu] = val;
-	ret |= read_msr(cpu, MSR_MPERF, &val);
-	mperf_current_count[cpu] = val;
+	ret = get_aperf_mperf(cpu, &aval, &mval);
+	aperf_current_count[cpu] = aval;
+	mperf_current_count[cpu] = mval;
 	is_valid[cpu] = !ret;
 
 	return 0;
@@ -305,6 +324,9 @@ struct cpuidle_monitor *mperf_register(void)
 	if (init_maxfreq_mode())
 		return NULL;
 
+	if (cpupower_cpu_info.vendor == X86_VENDOR_AMD)
+		mperf_monitor.flags.per_cpu_schedule = 1;
+
 	/* Free this at program termination */
 	is_valid = calloc(cpu_count, sizeof(int));
 	mperf_previous_count = calloc(cpu_count, sizeof(unsigned long long));

commit d3f5d2a192a299f56579ae6e6283f9011b00208f
Author: Janakarajan Natarajan <Janakarajan.Natarajan@amd.com>
Date:   Tue Nov 5 17:16:52 2019 +0000

    cpupower: Move needs_root variable into a sub-struct
    
    Move the needs_root variable into a sub-struct. This is in preparation
    for adding a new flag for cpuidle_monitor.
    
    Update all uses of the needs_root variable to reflect this change.
    
    Signed-off-by: Janakarajan Natarajan <Janakarajan.Natarajan@amd.com>
    Acked-by: Thomas Renninger <trenn@suse.de>
    Signed-off-by: Shuah Khan <skhan@linuxfoundation.org>

diff --git a/tools/power/cpupower/utils/idle_monitor/mperf_monitor.c b/tools/power/cpupower/utils/idle_monitor/mperf_monitor.c
index 44806a6dae11..7cae74202a4d 100644
--- a/tools/power/cpupower/utils/idle_monitor/mperf_monitor.c
+++ b/tools/power/cpupower/utils/idle_monitor/mperf_monitor.c
@@ -333,7 +333,7 @@ struct cpuidle_monitor mperf_monitor = {
 	.stop			= mperf_stop,
 	.do_register		= mperf_register,
 	.unregister		= mperf_unregister,
-	.needs_root		= 1,
+	.flags.needs_root	= 1,
 	.overflow_s		= 922000000 /* 922337203 seconds TSC overflow
 					       at 20GHz */
 };

commit 4f19048fd0a0036e02443237952db5bfa5b5cdf0
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 27 08:55:14 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 166
    
    Based on 1 normalized pattern(s):
    
      licensed under the terms of the gnu gpl license version 2
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 62 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Richard Fontana <rfontana@redhat.com>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190527070033.929121379@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/tools/power/cpupower/utils/idle_monitor/mperf_monitor.c b/tools/power/cpupower/utils/idle_monitor/mperf_monitor.c
index f2a7e9cfd577..44806a6dae11 100644
--- a/tools/power/cpupower/utils/idle_monitor/mperf_monitor.c
+++ b/tools/power/cpupower/utils/idle_monitor/mperf_monitor.c
@@ -1,7 +1,6 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  *  (C) 2010,2011       Thomas Renninger <trenn@suse.de>, Novell Inc.
- *
- *  Licensed under the terms of the GNU GPL License version 2.
  */
 
 #if defined(__i386__) || defined(__x86_64__)

commit 995d5f64b62f20f05b8e0972f07ec4d6c23333c9
Author: Pu Wen <puwen@hygon.cn>
Date:   Thu Oct 4 09:21:43 2018 +0800

    tools/cpupower: Add Hygon Dhyana support
    
    The tool cpupower is useful to get CPU frequency information and monitor
    power stats on the Hygon Dhyana platform. So add Hygon Dhyana support to
    it by checking vendor and family to share the code path of AMD family
    17h.
    
    Signed-off-by: Pu Wen <puwen@hygon.cn>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Acked-by: Shuah Khan (Samsung OSG) <shuah@kernel.org>
    CC: Prarit Bhargava <prarit@redhat.com>
    CC: Shuah Khan <shuah@kernel.org>
    CC: Thomas Gleixner <tglx@linutronix.de>
    CC: Thomas Renninger <trenn@suse.com>
    CC: linux-pm@vger.kernel.org
    Link: http://lkml.kernel.org/r/5ce86123a7b9dad925ac583d88d2f921040e859b.1538583282.git.puwen@hygon.cn

diff --git a/tools/power/cpupower/utils/idle_monitor/mperf_monitor.c b/tools/power/cpupower/utils/idle_monitor/mperf_monitor.c
index d7c2a6d13dea..f2a7e9cfd577 100644
--- a/tools/power/cpupower/utils/idle_monitor/mperf_monitor.c
+++ b/tools/power/cpupower/utils/idle_monitor/mperf_monitor.c
@@ -241,7 +241,8 @@ static int init_maxfreq_mode(void)
 	if (!(cpupower_cpu_info.caps & CPUPOWER_CAP_INV_TSC))
 		goto use_sysfs;
 
-	if (cpupower_cpu_info.vendor == X86_VENDOR_AMD) {
+	if (cpupower_cpu_info.vendor == X86_VENDOR_AMD ||
+	    cpupower_cpu_info.vendor == X86_VENDOR_HYGON) {
 		/* MSR_AMD_HWCR tells us whether TSC runs at P0/mperf
 		 * freq.
 		 * A test whether hwcr is accessable/available would be:

commit d0e4a193c33adaa4f91128d5393aa3589c2f3e9e
Author: Prarit Bhargava <prarit@redhat.com>
Date:   Mon Jul 31 07:56:06 2017 -0400

    tools/power/cpupower: allow running without cpu0
    
    Linux-3.7 added CONFIG_BOOTPARAM_HOTPLUG_CPU0,
    allowing systems to offline cpu0.
    
    But when cpu0 is offline, cpupower monitor will not display all
    processor and Mperf information:
    
    [root@intel-skylake-dh-03 cpupower]# ./cpupower monitor
    WARNING: at least one cpu is offline
        |Idle_Stats
    CPU | POLL | C1-S | C1E- | C3-S | C6-S | C7s- | C8-S
       4|  0.00|  0.00|  0.00|  0.00|  0.90|  0.00| 96.13
       1|  0.00|  0.00|  5.49|  0.00|  0.01|  0.00| 92.26
       5|  0.00|  0.00|  0.00|  0.00|  0.46|  0.00| 99.50
       2| 45.42|  0.00|  0.00|  0.00| 22.94|  0.00| 28.84
       6|  0.00| 37.54|  0.00|  0.00|  0.00|  0.00|  0.00
       3|  0.00|  0.00|  0.00|  0.00|  0.30|  0.00| 91.99
       7|  0.00|  0.00|  0.00|  0.00|  4.70|  0.00|  0.70
    
    This patch replaces the hard-coded use of cpu0 in cpupower with the
    current cpu, allowing it to run without a cpu0.
    
    After the patch is applied,
    
    [root@intel-skylake-dh-03 cpupower]# ./cpupower monitor
    WARNING: at least one cpu is offline
        |Nehalem                    || Mperf              || Idle_Stats
    CPU | C3   | C6   | PC3  | PC6  || C0   | Cx   | Freq || POLL | C1-S | C1E- | C3-S | C6-S | C7s- | C8-S
       4|  0.01|  1.27|  0.00|  0.00||  0.04| 99.96|  3957||  0.00|  0.00|  0.00|  0.00|  1.43|  0.00| 98.52
       1|  0.00| 98.82|  0.00|  0.00||  0.05| 99.95|  3361||  0.00|  0.00|  0.01|  0.00|  0.03|  0.00| 99.88
       5|  0.00| 98.82|  0.00|  0.00||  0.09| 99.91|  3917||  0.00|  0.00|  0.00|  0.00| 99.38|  0.00|  0.50
       2|  0.33|  0.00|  0.00|  0.00||  0.00|100.00|  3890||  0.00|  0.00|  0.00|  0.00|  0.00|  0.00|100.00
       6|  0.33|  0.00|  0.00|  0.00||  0.01| 99.99|  3903||  0.00|  0.00|  0.00|  0.00|  0.00|  0.00| 99.99
       3|  0.01|  0.71|  0.00|  0.00||  0.06| 99.94|  3678||  0.00|  0.00|  0.00|  0.00|  0.80|  0.00| 99.13
       7|  0.01|  0.71|  0.00|  0.00||  0.03| 99.97|  3538||  0.00|  0.69| 11.70|  0.00|  0.00|  0.00| 87.57
    
    There are some minor cleanups included in this patch.
    
    Signed-off-by: Prarit Bhargava <prarit@redhat.com>
    Signed-off-by: Thomas Renninger <trenn@suse.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/tools/power/cpupower/utils/idle_monitor/mperf_monitor.c b/tools/power/cpupower/utils/idle_monitor/mperf_monitor.c
index c83f1606970b..d7c2a6d13dea 100644
--- a/tools/power/cpupower/utils/idle_monitor/mperf_monitor.c
+++ b/tools/power/cpupower/utils/idle_monitor/mperf_monitor.c
@@ -80,7 +80,8 @@ static int *is_valid;
 static int mperf_get_tsc(unsigned long long *tsc)
 {
 	int ret;
-	ret = read_msr(0, MSR_TSC, tsc);
+
+	ret = read_msr(base_cpu, MSR_TSC, tsc);
 	if (ret)
 		dprint("Reading TSC MSR failed, returning %llu\n", *tsc);
 	return ret;

commit 47b98c74fab2d05fd724a6d9fd0efc8987ae3911
Author: Herton R. Krzesinski <herton@redhat.com>
Date:   Sat May 30 02:21:31 2015 +0200

    cpupower: mperf monitor: fix output in MAX_FREQ_SYSFS mode
    
    There is clearly wrong output when mperf monitor runs in MAX_FREQ_SYSFS mode:
    average frequency shows in kHz unit (despite the intended output to be in MHz),
    and percentages for C state information are all wrong (including high/negative
    values shown).
    
    The problem is that the max_frequency read on initialization isn't used where it
    should have been used on mperf_get_count_percent (to estimate the number of
    ticks in the given time period), and the value we read from sysfs is in kHz, so
    we must divide it to get the MHz value to use in current calculations.
    
    While at it, also I fixed another small issues in the debug output of
    max_frequency value in mperf_get_count_freq.
    
    Signed-off-by: Herton R. Krzesinski <herton@redhat.com>
    Acked-by: Thomas Renninger <trenn@suse.de>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/tools/power/cpupower/utils/idle_monitor/mperf_monitor.c b/tools/power/cpupower/utils/idle_monitor/mperf_monitor.c
index 90a8c4f071e7..c83f1606970b 100644
--- a/tools/power/cpupower/utils/idle_monitor/mperf_monitor.c
+++ b/tools/power/cpupower/utils/idle_monitor/mperf_monitor.c
@@ -135,7 +135,7 @@ static int mperf_get_count_percent(unsigned int id, double *percent,
 		dprint("%s: TSC Ref - mperf_diff: %llu, tsc_diff: %llu\n",
 		       mperf_cstates[id].name, mperf_diff, tsc_diff);
 	} else if (max_freq_mode == MAX_FREQ_SYSFS) {
-		timediff = timespec_diff_us(time_start, time_end);
+		timediff = max_frequency * timespec_diff_us(time_start, time_end);
 		*percent = 100.0 * mperf_diff / timediff;
 		dprint("%s: MAXFREQ - mperf_diff: %llu, time_diff: %llu\n",
 		       mperf_cstates[id].name, mperf_diff, timediff);
@@ -176,7 +176,7 @@ static int mperf_get_count_freq(unsigned int id, unsigned long long *count,
 	dprint("%s: Average freq based on %s maximum frequency:\n",
 	       mperf_cstates[id].name,
 	       (max_freq_mode == MAX_FREQ_TSC_REF) ? "TSC calculated" : "sysfs read");
-	dprint("%max_frequency: %lu", max_frequency);
+	dprint("max_frequency: %lu\n", max_frequency);
 	dprint("aperf_diff: %llu\n", aperf_diff);
 	dprint("mperf_diff: %llu\n", mperf_diff);
 	dprint("avg freq:   %llu\n", *count);
@@ -279,6 +279,7 @@ static int init_maxfreq_mode(void)
 		return -1;
 	}
 	max_freq_mode = MAX_FREQ_SYSFS;
+	max_frequency /= 1000; /* Default automatically to MHz value */
 	return 0;
 }
 

commit 97fa1c5ca680bdee2c650e3aadf2a839b92f3f0e
Author: Himangi Saraogi <himangi774@gmail.com>
Date:   Tue Jul 29 18:12:18 2014 +0200

    cpupower: mperf monitor: Correct use of ! and &
    
    In commit ae91d60ba88ef0bdb1b5e9b2363bd52fc45d2af7, a bug was fixed that
    involved converting !x & y to !(x & y).  The code below shows the same
    pattern, and thus should perhaps be fixed in the same way.
    
    The Coccinelle semantic patch that makes this change is as follows:
    
    // <smpl>
    @@ expression E1,E2; @@
    (
      !E1 & !E2
    |
    - !E1 & E2
    + !(E1 & E2)
    )
    // </smpl>
    
    Signed-off-by: Himangi Saraogi <himangi774@gmail.com>
    Signed-off-by: Thomas Renninger <trenn@suse.de>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/tools/power/cpupower/utils/idle_monitor/mperf_monitor.c b/tools/power/cpupower/utils/idle_monitor/mperf_monitor.c
index 5650ab5a2c20..90a8c4f071e7 100644
--- a/tools/power/cpupower/utils/idle_monitor/mperf_monitor.c
+++ b/tools/power/cpupower/utils/idle_monitor/mperf_monitor.c
@@ -237,7 +237,7 @@ static int init_maxfreq_mode(void)
 	unsigned long long hwcr;
 	unsigned long min;
 
-	if (!cpupower_cpu_info.caps & CPUPOWER_CAP_INV_TSC)
+	if (!(cpupower_cpu_info.caps & CPUPOWER_CAP_INV_TSC))
 		goto use_sysfs;
 
 	if (cpupower_cpu_info.vendor == X86_VENDOR_AMD) {

commit 2dfc818b35cbea59188cc86e86e0a0efce2b0dbe
Author: Thomas Renninger <trenn@suse.de>
Date:   Fri Aug 12 01:11:35 2011 +0200

    cpupower: mperf monitor - Use TSC to calculate max frequency if possible
    
    Which makes the implementation independent from cpufreq drivers.
    Therefore this would also work on a Xen kernel where the hypervisor
    is doing frequency switching and idle entering.
    
    Signed-off-by: Thomas Renninger <trenn@suse.de>
    Signed-off-by: Dominik Brodowski <linux@dominikbrodowski.net>

diff --git a/tools/power/cpupower/utils/idle_monitor/mperf_monitor.c b/tools/power/cpupower/utils/idle_monitor/mperf_monitor.c
index 63ca87a05e5f..5650ab5a2c20 100644
--- a/tools/power/cpupower/utils/idle_monitor/mperf_monitor.c
+++ b/tools/power/cpupower/utils/idle_monitor/mperf_monitor.c
@@ -22,12 +22,15 @@
 
 #define MSR_TSC	0x10
 
+#define MSR_AMD_HWCR 0xc0010015
+
 enum mperf_id { C0 = 0, Cx, AVG_FREQ, MPERF_CSTATE_COUNT };
 
 static int mperf_get_count_percent(unsigned int self_id, double *percent,
 				   unsigned int cpu);
 static int mperf_get_count_freq(unsigned int id, unsigned long long *count,
 				unsigned int cpu);
+static struct timespec time_start, time_end;
 
 static cstate_t mperf_cstates[MPERF_CSTATE_COUNT] = {
 	{
@@ -54,19 +57,33 @@ static cstate_t mperf_cstates[MPERF_CSTATE_COUNT] = {
 	},
 };
 
+enum MAX_FREQ_MODE { MAX_FREQ_SYSFS, MAX_FREQ_TSC_REF };
+static int max_freq_mode;
+/*
+ * The max frequency mperf is ticking at (in C0), either retrieved via:
+ *   1) calculated after measurements if we know TSC ticks at mperf/P0 frequency
+ *   2) cpufreq /sys/devices/.../cpu0/cpufreq/cpuinfo_max_freq at init time
+ * 1. Is preferred as it also works without cpufreq subsystem (e.g. on Xen)
+ */
+static unsigned long max_frequency;
+
 static unsigned long long tsc_at_measure_start;
 static unsigned long long tsc_at_measure_end;
-static unsigned long max_frequency;
 static unsigned long long *mperf_previous_count;
 static unsigned long long *aperf_previous_count;
 static unsigned long long *mperf_current_count;
 static unsigned long long *aperf_current_count;
+
 /* valid flag for all CPUs. If a MSR read failed it will be zero */
 static int *is_valid;
 
 static int mperf_get_tsc(unsigned long long *tsc)
 {
-	return read_msr(0, MSR_TSC, tsc);
+	int ret;
+	ret = read_msr(0, MSR_TSC, tsc);
+	if (ret)
+		dprint("Reading TSC MSR failed, returning %llu\n", *tsc);
+	return ret;
 }
 
 static int mperf_init_stats(unsigned int cpu)
@@ -97,36 +114,11 @@ static int mperf_measure_stats(unsigned int cpu)
 	return 0;
 }
 
-/*
- * get_average_perf()
- *
- * Returns the average performance (also considers boosted frequencies)
- *
- * Input:
- *   aperf_diff: Difference of the aperf register over a time period
- *   mperf_diff: Difference of the mperf register over the same time period
- *   max_freq:   Maximum frequency (P0)
- *
- * Returns:
- *   Average performance over the time period
- */
-static unsigned long get_average_perf(unsigned long long aperf_diff,
-				      unsigned long long mperf_diff)
-{
-	unsigned int perf_percent = 0;
-	if (((unsigned long)(-1) / 100) < aperf_diff) {
-		int shift_count = 7;
-		aperf_diff >>= shift_count;
-		mperf_diff >>= shift_count;
-	}
-	perf_percent = (aperf_diff * 100) / mperf_diff;
-	return (max_frequency * perf_percent) / 100;
-}
-
 static int mperf_get_count_percent(unsigned int id, double *percent,
 				   unsigned int cpu)
 {
 	unsigned long long aperf_diff, mperf_diff, tsc_diff;
+	unsigned long long timediff;
 
 	if (!is_valid[cpu])
 		return -1;
@@ -136,11 +128,19 @@ static int mperf_get_count_percent(unsigned int id, double *percent,
 
 	mperf_diff = mperf_current_count[cpu] - mperf_previous_count[cpu];
 	aperf_diff = aperf_current_count[cpu] - aperf_previous_count[cpu];
-	tsc_diff = tsc_at_measure_end - tsc_at_measure_start;
 
-	*percent = 100.0 * mperf_diff / tsc_diff;
-	dprint("%s: mperf_diff: %llu, tsc_diff: %llu\n",
-	       mperf_cstates[id].name, mperf_diff, tsc_diff);
+	if (max_freq_mode == MAX_FREQ_TSC_REF) {
+		tsc_diff = tsc_at_measure_end - tsc_at_measure_start;
+		*percent = 100.0 * mperf_diff / tsc_diff;
+		dprint("%s: TSC Ref - mperf_diff: %llu, tsc_diff: %llu\n",
+		       mperf_cstates[id].name, mperf_diff, tsc_diff);
+	} else if (max_freq_mode == MAX_FREQ_SYSFS) {
+		timediff = timespec_diff_us(time_start, time_end);
+		*percent = 100.0 * mperf_diff / timediff;
+		dprint("%s: MAXFREQ - mperf_diff: %llu, time_diff: %llu\n",
+		       mperf_cstates[id].name, mperf_diff, timediff);
+	} else
+		return -1;
 
 	if (id == Cx)
 		*percent = 100.0 - *percent;
@@ -154,7 +154,7 @@ static int mperf_get_count_percent(unsigned int id, double *percent,
 static int mperf_get_count_freq(unsigned int id, unsigned long long *count,
 				unsigned int cpu)
 {
-	unsigned long long aperf_diff, mperf_diff;
+	unsigned long long aperf_diff, mperf_diff, time_diff, tsc_diff;
 
 	if (id != AVG_FREQ)
 		return 1;
@@ -165,11 +165,21 @@ static int mperf_get_count_freq(unsigned int id, unsigned long long *count,
 	mperf_diff = mperf_current_count[cpu] - mperf_previous_count[cpu];
 	aperf_diff = aperf_current_count[cpu] - aperf_previous_count[cpu];
 
-	/* Return MHz for now, might want to return KHz if column width is more
-	   generic */
-	*count = get_average_perf(aperf_diff, mperf_diff) / 1000;
-	dprint("%s: %llu\n", mperf_cstates[id].name, *count);
+	if (max_freq_mode == MAX_FREQ_TSC_REF) {
+		/* Calculate max_freq from TSC count */
+		tsc_diff = tsc_at_measure_end - tsc_at_measure_start;
+		time_diff = timespec_diff_us(time_start, time_end);
+		max_frequency = tsc_diff / time_diff;
+	}
 
+	*count = max_frequency * ((double)aperf_diff / mperf_diff);
+	dprint("%s: Average freq based on %s maximum frequency:\n",
+	       mperf_cstates[id].name,
+	       (max_freq_mode == MAX_FREQ_TSC_REF) ? "TSC calculated" : "sysfs read");
+	dprint("%max_frequency: %lu", max_frequency);
+	dprint("aperf_diff: %llu\n", aperf_diff);
+	dprint("mperf_diff: %llu\n", mperf_diff);
+	dprint("avg freq:   %llu\n", *count);
 	return 0;
 }
 
@@ -178,6 +188,7 @@ static int mperf_start(void)
 	int cpu;
 	unsigned long long dbg;
 
+	clock_gettime(CLOCK_REALTIME, &time_start);
 	mperf_get_tsc(&tsc_at_measure_start);
 
 	for (cpu = 0; cpu < cpu_count; cpu++)
@@ -193,32 +204,104 @@ static int mperf_stop(void)
 	unsigned long long dbg;
 	int cpu;
 
-	mperf_get_tsc(&tsc_at_measure_end);
-
 	for (cpu = 0; cpu < cpu_count; cpu++)
 		mperf_measure_stats(cpu);
 
+	mperf_get_tsc(&tsc_at_measure_end);
+	clock_gettime(CLOCK_REALTIME, &time_end);
+
 	mperf_get_tsc(&dbg);
 	dprint("TSC diff: %llu\n", dbg - tsc_at_measure_end);
 
 	return 0;
 }
 
-struct cpuidle_monitor mperf_monitor;
-
-struct cpuidle_monitor *mperf_register(void)
+/*
+ * Mperf register is defined to tick at P0 (maximum) frequency
+ *
+ * Instead of reading out P0 which can be tricky to read out from HW,
+ * we use TSC counter if it reliably ticks at P0/mperf frequency.
+ *
+ * Still try to fall back to:
+ * /sys/devices/system/cpu/cpu0/cpufreq/cpuinfo_max_freq
+ * on older Intel HW without invariant TSC feature.
+ * Or on AMD machines where TSC does not tick at P0 (do not exist yet, but
+ * it's still double checked (MSR_AMD_HWCR)).
+ *
+ * On these machines the user would still get useful mperf
+ * stats when acpi-cpufreq driver is loaded.
+ */
+static int init_maxfreq_mode(void)
 {
+	int ret;
+	unsigned long long hwcr;
 	unsigned long min;
 
-	if (!(cpupower_cpu_info.caps & CPUPOWER_CAP_APERF))
-		return NULL;
-
-	/* Assume min/max all the same on all cores */
+	if (!cpupower_cpu_info.caps & CPUPOWER_CAP_INV_TSC)
+		goto use_sysfs;
+
+	if (cpupower_cpu_info.vendor == X86_VENDOR_AMD) {
+		/* MSR_AMD_HWCR tells us whether TSC runs at P0/mperf
+		 * freq.
+		 * A test whether hwcr is accessable/available would be:
+		 * (cpupower_cpu_info.family > 0x10 ||
+		 *   cpupower_cpu_info.family == 0x10 &&
+		 *   cpupower_cpu_info.model >= 0x2))
+		 * This should be the case for all aperf/mperf
+		 * capable AMD machines and is therefore safe to test here.
+		 * Compare with Linus kernel git commit: acf01734b1747b1ec4
+		 */
+		ret = read_msr(0, MSR_AMD_HWCR, &hwcr);
+		/*
+		 * If the MSR read failed, assume a Xen system that did
+		 * not explicitly provide access to it and assume TSC works
+		*/
+		if (ret != 0) {
+			dprint("TSC read 0x%x failed - assume TSC working\n",
+			       MSR_AMD_HWCR);
+			return 0;
+		} else if (1 & (hwcr >> 24)) {
+			max_freq_mode = MAX_FREQ_TSC_REF;
+			return 0;
+		} else { /* Use sysfs max frequency if available */ }
+	} else if (cpupower_cpu_info.vendor == X86_VENDOR_INTEL) {
+		/*
+		 * On Intel we assume mperf (in C0) is ticking at same
+		 * rate than TSC
+		 */
+		max_freq_mode = MAX_FREQ_TSC_REF;
+		return 0;
+	}
+use_sysfs:
 	if (cpufreq_get_hardware_limits(0, &min, &max_frequency)) {
 		dprint("Cannot retrieve max freq from cpufreq kernel "
 		       "subsystem\n");
-		return NULL;
+		return -1;
 	}
+	max_freq_mode = MAX_FREQ_SYSFS;
+	return 0;
+}
+
+/*
+ * This monitor provides:
+ *
+ * 1) Average frequency a CPU resided in
+ *    This always works if the CPU has aperf/mperf capabilities
+ *
+ * 2) C0 and Cx (any sleep state) time a CPU resided in
+ *    Works if mperf timer stops ticking in sleep states which
+ *    seem to be the case on all current HW.
+ * Both is directly retrieved from HW registers and is independent
+ * from kernel statistics.
+ */
+struct cpuidle_monitor mperf_monitor;
+struct cpuidle_monitor *mperf_register(void)
+{
+	if (!(cpupower_cpu_info.caps & CPUPOWER_CAP_APERF))
+		return NULL;
+
+	if (init_maxfreq_mode())
+		return NULL;
 
 	/* Free this at program termination */
 	is_valid = calloc(cpu_count, sizeof(int));

commit b510b54127a4d4112a9a3f200339719bcb463c15
Author: Dominik Brodowski <linux@dominikbrodowski.net>
Date:   Tue Apr 19 19:58:59 2011 +0200

    cpupowerutils: idle_monitor - ConfigStyle bugfixes
    
    Signed-off-by: Dominik Brodowski <linux@dominikbrodowski.net>

diff --git a/tools/power/cpupower/utils/idle_monitor/mperf_monitor.c b/tools/power/cpupower/utils/idle_monitor/mperf_monitor.c
index f8545e40e232..63ca87a05e5f 100644
--- a/tools/power/cpupower/utils/idle_monitor/mperf_monitor.c
+++ b/tools/power/cpupower/utils/idle_monitor/mperf_monitor.c
@@ -79,7 +79,7 @@ static int mperf_init_stats(unsigned int cpu)
 	ret |= read_msr(cpu, MSR_MPERF, &val);
 	mperf_previous_count[cpu] = val;
 	is_valid[cpu] = !ret;
-	
+
 	return 0;
 }
 
@@ -93,7 +93,7 @@ static int mperf_measure_stats(unsigned int cpu)
 	ret |= read_msr(cpu, MSR_MPERF, &val);
 	mperf_current_count[cpu] = val;
 	is_valid[cpu] = !ret;
-	
+
 	return 0;
 }
 
@@ -145,14 +145,14 @@ static int mperf_get_count_percent(unsigned int id, double *percent,
 	if (id == Cx)
 		*percent = 100.0 - *percent;
 
-	dprint("%s: previous: %llu - current: %llu - (%u)\n", mperf_cstates[id].name,
-	       mperf_diff, aperf_diff, cpu);
+	dprint("%s: previous: %llu - current: %llu - (%u)\n",
+		mperf_cstates[id].name, mperf_diff, aperf_diff, cpu);
 	dprint("%s: %f\n", mperf_cstates[id].name, *percent);
 	return 0;
 }
 
 static int mperf_get_count_freq(unsigned int id, unsigned long long *count,
-			      unsigned int cpu)
+				unsigned int cpu)
 {
 	unsigned long long aperf_diff, mperf_diff;
 
@@ -206,8 +206,8 @@ static int mperf_stop(void)
 
 struct cpuidle_monitor mperf_monitor;
 
-struct cpuidle_monitor* mperf_register(void) {
-
+struct cpuidle_monitor *mperf_register(void)
+{
 	unsigned long min;
 
 	if (!(cpupower_cpu_info.caps & CPUPOWER_CAP_APERF))
@@ -221,21 +221,18 @@ struct cpuidle_monitor* mperf_register(void) {
 	}
 
 	/* Free this at program termination */
-	is_valid = calloc(cpu_count, sizeof (int));
-	mperf_previous_count = calloc (cpu_count,
-				       sizeof(unsigned long long));
-	aperf_previous_count = calloc (cpu_count,
-				       sizeof(unsigned long long));
-	mperf_current_count = calloc (cpu_count,
-				      sizeof(unsigned long long));
-	aperf_current_count = calloc (cpu_count,
-				      sizeof(unsigned long long));
-	
+	is_valid = calloc(cpu_count, sizeof(int));
+	mperf_previous_count = calloc(cpu_count, sizeof(unsigned long long));
+	aperf_previous_count = calloc(cpu_count, sizeof(unsigned long long));
+	mperf_current_count = calloc(cpu_count, sizeof(unsigned long long));
+	aperf_current_count = calloc(cpu_count, sizeof(unsigned long long));
+
 	mperf_monitor.name_len = strlen(mperf_monitor.name);
 	return &mperf_monitor;
 }
 
-void mperf_unregister(void) {
+void mperf_unregister(void)
+{
 	free(mperf_previous_count);
 	free(aperf_previous_count);
 	free(mperf_current_count);

commit 7fe2f6399a84760a9af8896ac152728250f82adb
Author: Dominik Brodowski <linux@dominikbrodowski.net>
Date:   Wed Mar 30 16:30:11 2011 +0200

    cpupowerutils - cpufrequtils extended with quite some features
    
    CPU power consumption vs performance tuning is no longer
    limited to CPU frequency switching anymore: deep sleep states,
    traditional dynamic frequency scaling and hidden turbo/boost
    frequencies are tied close together and depend on each other.
    The first two exist on different architectures like PPC, Itanium and
    ARM, the latter (so far) only on X86. On X86 the APU (CPU+GPU) will
    only run most efficiently if CPU and GPU has proper power management
    in place.
    
    Users and Developers want to have *one* tool to get an overview what
    their system supports and to monitor and debug CPU power management
    in detail. The tool should compile and work on as many architectures
    as possible.
    
    Once this tool stabilizes a bit, it is intended to replace the
    Intel-specific tools in tools/power/x86
    
    Signed-off-by: Dominik Brodowski <linux@dominikbrodowski.net>

diff --git a/tools/power/cpupower/utils/idle_monitor/mperf_monitor.c b/tools/power/cpupower/utils/idle_monitor/mperf_monitor.c
new file mode 100644
index 000000000000..f8545e40e232
--- /dev/null
+++ b/tools/power/cpupower/utils/idle_monitor/mperf_monitor.c
@@ -0,0 +1,258 @@
+/*
+ *  (C) 2010,2011       Thomas Renninger <trenn@suse.de>, Novell Inc.
+ *
+ *  Licensed under the terms of the GNU GPL License version 2.
+ */
+
+#if defined(__i386__) || defined(__x86_64__)
+
+#include <stdio.h>
+#include <stdint.h>
+#include <stdlib.h>
+#include <string.h>
+#include <limits.h>
+
+#include <cpufreq.h>
+
+#include "helpers/helpers.h"
+#include "idle_monitor/cpupower-monitor.h"
+
+#define MSR_APERF	0xE8
+#define MSR_MPERF	0xE7
+
+#define MSR_TSC	0x10
+
+enum mperf_id { C0 = 0, Cx, AVG_FREQ, MPERF_CSTATE_COUNT };
+
+static int mperf_get_count_percent(unsigned int self_id, double *percent,
+				   unsigned int cpu);
+static int mperf_get_count_freq(unsigned int id, unsigned long long *count,
+				unsigned int cpu);
+
+static cstate_t mperf_cstates[MPERF_CSTATE_COUNT] = {
+	{
+		.name			= "C0",
+		.desc			= N_("Processor Core not idle"),
+		.id			= C0,
+		.range			= RANGE_THREAD,
+		.get_count_percent	= mperf_get_count_percent,
+	},
+	{
+		.name			= "Cx",
+		.desc			= N_("Processor Core in an idle state"),
+		.id			= Cx,
+		.range			= RANGE_THREAD,
+		.get_count_percent	= mperf_get_count_percent,
+	},
+
+	{
+		.name			= "Freq",
+		.desc			= N_("Average Frequency (including boost) in MHz"),
+		.id			= AVG_FREQ,
+		.range			= RANGE_THREAD,
+		.get_count		= mperf_get_count_freq,
+	},
+};
+
+static unsigned long long tsc_at_measure_start;
+static unsigned long long tsc_at_measure_end;
+static unsigned long max_frequency;
+static unsigned long long *mperf_previous_count;
+static unsigned long long *aperf_previous_count;
+static unsigned long long *mperf_current_count;
+static unsigned long long *aperf_current_count;
+/* valid flag for all CPUs. If a MSR read failed it will be zero */
+static int *is_valid;
+
+static int mperf_get_tsc(unsigned long long *tsc)
+{
+	return read_msr(0, MSR_TSC, tsc);
+}
+
+static int mperf_init_stats(unsigned int cpu)
+{
+	unsigned long long val;
+	int ret;
+
+	ret = read_msr(cpu, MSR_APERF, &val);
+	aperf_previous_count[cpu] = val;
+	ret |= read_msr(cpu, MSR_MPERF, &val);
+	mperf_previous_count[cpu] = val;
+	is_valid[cpu] = !ret;
+	
+	return 0;
+}
+
+static int mperf_measure_stats(unsigned int cpu)
+{
+	unsigned long long val;
+	int ret;
+
+	ret = read_msr(cpu, MSR_APERF, &val);
+	aperf_current_count[cpu] = val;
+	ret |= read_msr(cpu, MSR_MPERF, &val);
+	mperf_current_count[cpu] = val;
+	is_valid[cpu] = !ret;
+	
+	return 0;
+}
+
+/*
+ * get_average_perf()
+ *
+ * Returns the average performance (also considers boosted frequencies)
+ *
+ * Input:
+ *   aperf_diff: Difference of the aperf register over a time period
+ *   mperf_diff: Difference of the mperf register over the same time period
+ *   max_freq:   Maximum frequency (P0)
+ *
+ * Returns:
+ *   Average performance over the time period
+ */
+static unsigned long get_average_perf(unsigned long long aperf_diff,
+				      unsigned long long mperf_diff)
+{
+	unsigned int perf_percent = 0;
+	if (((unsigned long)(-1) / 100) < aperf_diff) {
+		int shift_count = 7;
+		aperf_diff >>= shift_count;
+		mperf_diff >>= shift_count;
+	}
+	perf_percent = (aperf_diff * 100) / mperf_diff;
+	return (max_frequency * perf_percent) / 100;
+}
+
+static int mperf_get_count_percent(unsigned int id, double *percent,
+				   unsigned int cpu)
+{
+	unsigned long long aperf_diff, mperf_diff, tsc_diff;
+
+	if (!is_valid[cpu])
+		return -1;
+
+	if (id != C0 && id != Cx)
+		return -1;
+
+	mperf_diff = mperf_current_count[cpu] - mperf_previous_count[cpu];
+	aperf_diff = aperf_current_count[cpu] - aperf_previous_count[cpu];
+	tsc_diff = tsc_at_measure_end - tsc_at_measure_start;
+
+	*percent = 100.0 * mperf_diff / tsc_diff;
+	dprint("%s: mperf_diff: %llu, tsc_diff: %llu\n",
+	       mperf_cstates[id].name, mperf_diff, tsc_diff);
+
+	if (id == Cx)
+		*percent = 100.0 - *percent;
+
+	dprint("%s: previous: %llu - current: %llu - (%u)\n", mperf_cstates[id].name,
+	       mperf_diff, aperf_diff, cpu);
+	dprint("%s: %f\n", mperf_cstates[id].name, *percent);
+	return 0;
+}
+
+static int mperf_get_count_freq(unsigned int id, unsigned long long *count,
+			      unsigned int cpu)
+{
+	unsigned long long aperf_diff, mperf_diff;
+
+	if (id != AVG_FREQ)
+		return 1;
+
+	if (!is_valid[cpu])
+		return -1;
+
+	mperf_diff = mperf_current_count[cpu] - mperf_previous_count[cpu];
+	aperf_diff = aperf_current_count[cpu] - aperf_previous_count[cpu];
+
+	/* Return MHz for now, might want to return KHz if column width is more
+	   generic */
+	*count = get_average_perf(aperf_diff, mperf_diff) / 1000;
+	dprint("%s: %llu\n", mperf_cstates[id].name, *count);
+
+	return 0;
+}
+
+static int mperf_start(void)
+{
+	int cpu;
+	unsigned long long dbg;
+
+	mperf_get_tsc(&tsc_at_measure_start);
+
+	for (cpu = 0; cpu < cpu_count; cpu++)
+		mperf_init_stats(cpu);
+
+	mperf_get_tsc(&dbg);
+	dprint("TSC diff: %llu\n", dbg - tsc_at_measure_start);
+	return 0;
+}
+
+static int mperf_stop(void)
+{
+	unsigned long long dbg;
+	int cpu;
+
+	mperf_get_tsc(&tsc_at_measure_end);
+
+	for (cpu = 0; cpu < cpu_count; cpu++)
+		mperf_measure_stats(cpu);
+
+	mperf_get_tsc(&dbg);
+	dprint("TSC diff: %llu\n", dbg - tsc_at_measure_end);
+
+	return 0;
+}
+
+struct cpuidle_monitor mperf_monitor;
+
+struct cpuidle_monitor* mperf_register(void) {
+
+	unsigned long min;
+
+	if (!(cpupower_cpu_info.caps & CPUPOWER_CAP_APERF))
+		return NULL;
+
+	/* Assume min/max all the same on all cores */
+	if (cpufreq_get_hardware_limits(0, &min, &max_frequency)) {
+		dprint("Cannot retrieve max freq from cpufreq kernel "
+		       "subsystem\n");
+		return NULL;
+	}
+
+	/* Free this at program termination */
+	is_valid = calloc(cpu_count, sizeof (int));
+	mperf_previous_count = calloc (cpu_count,
+				       sizeof(unsigned long long));
+	aperf_previous_count = calloc (cpu_count,
+				       sizeof(unsigned long long));
+	mperf_current_count = calloc (cpu_count,
+				      sizeof(unsigned long long));
+	aperf_current_count = calloc (cpu_count,
+				      sizeof(unsigned long long));
+	
+	mperf_monitor.name_len = strlen(mperf_monitor.name);
+	return &mperf_monitor;
+}
+
+void mperf_unregister(void) {
+	free(mperf_previous_count);
+	free(aperf_previous_count);
+	free(mperf_current_count);
+	free(aperf_current_count);
+	free(is_valid);
+}
+
+struct cpuidle_monitor mperf_monitor = {
+	.name			= "Mperf",
+	.hw_states_num		= MPERF_CSTATE_COUNT,
+	.hw_states		= mperf_cstates,
+	.start			= mperf_start,
+	.stop			= mperf_stop,
+	.do_register		= mperf_register,
+	.unregister		= mperf_unregister,
+	.needs_root		= 1,
+	.overflow_s		= 922000000 /* 922337203 seconds TSC overflow
+					       at 20GHz */
+};
+#endif /* #if defined(__i386__) || defined(__x86_64__) */
