commit bf99c936f9478a05d51e9f101f90de70bee9a89c
Author: Andrii Nakryiko <andriin@fb.com>
Date:   Fri May 29 00:54:21 2020 -0700

    libbpf: Add BPF ring buffer support
    
    Declaring and instantiating BPF ring buffer doesn't require any changes to
    libbpf, as it's just another type of maps. So using existing BTF-defined maps
    syntax with __uint(type, BPF_MAP_TYPE_RINGBUF) and __uint(max_elements,
    <size-of-ring-buf>) is all that's necessary to create and use BPF ring buffer.
    
    This patch adds BPF ring buffer consumer to libbpf. It is very similar to
    perf_buffer implementation in terms of API, but also attempts to fix some
    minor problems and inconveniences with existing perf_buffer API.
    
    ring_buffer support both single ring buffer use case (with just using
    ring_buffer__new()), as well as allows to add more ring buffers, each with its
    own callback and context. This allows to efficiently poll and consume
    multiple, potentially completely independent, ring buffers, using single
    epoll instance.
    
    The latter is actually a problem in practice for applications
    that are using multiple sets of perf buffers. They have to create multiple
    instances for struct perf_buffer and poll them independently or in a loop,
    each approach having its own problems (e.g., inability to use a common poll
    timeout). struct ring_buffer eliminates this problem by aggregating many
    independent ring buffer instances under the single "ring buffer manager".
    
    Second, perf_buffer's callback can't return error, so applications that need
    to stop polling due to error in data or data signalling the end, have to use
    extra mechanisms to signal that polling has to stop. ring_buffer's callback
    can return error, which will be passed through back to user code and can be
    acted upon appropariately.
    
    Two APIs allow to consume ring buffer data:
      - ring_buffer__poll(), which will wait for data availability notification
        and will consume data only from reported ring buffer(s); this API allows
        to efficiently use resources by reading data only when it becomes
        available;
      - ring_buffer__consume(), will attempt to read new records regardless of
        data availablity notification sub-system. This API is useful for cases
        when lowest latency is required, in expense of burning CPU resources.
    
    Signed-off-by: Andrii Nakryiko <andriin@fb.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Link: https://lore.kernel.org/bpf/20200529075424.3139988-3-andriin@fb.com
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/tools/lib/bpf/libbpf_probes.c b/tools/lib/bpf/libbpf_probes.c
index 2c92059c0c90..10cd8d1891f5 100644
--- a/tools/lib/bpf/libbpf_probes.c
+++ b/tools/lib/bpf/libbpf_probes.c
@@ -238,6 +238,11 @@ bool bpf_probe_map_type(enum bpf_map_type map_type, __u32 ifindex)
 		if (btf_fd < 0)
 			return false;
 		break;
+	case BPF_MAP_TYPE_RINGBUF:
+		key_size = 0;
+		value_size = 0;
+		max_entries = 4096;
+		break;
 	case BPF_MAP_TYPE_UNSPEC:
 	case BPF_MAP_TYPE_HASH:
 	case BPF_MAP_TYPE_ARRAY:

commit fc611f47f2188ade2b48ff6902d5cce8baac0c58
Author: KP Singh <kpsingh@google.com>
Date:   Sun Mar 29 01:43:49 2020 +0100

    bpf: Introduce BPF_PROG_TYPE_LSM
    
    Introduce types and configs for bpf programs that can be attached to
    LSM hooks. The programs can be enabled by the config option
    CONFIG_BPF_LSM.
    
    Signed-off-by: KP Singh <kpsingh@google.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Reviewed-by: Brendan Jackman <jackmanb@google.com>
    Reviewed-by: Florent Revest <revest@google.com>
    Reviewed-by: Thomas Garnier <thgarnie@google.com>
    Acked-by: Yonghong Song <yhs@fb.com>
    Acked-by: Andrii Nakryiko <andriin@fb.com>
    Acked-by: James Morris <jamorris@linux.microsoft.com>
    Link: https://lore.kernel.org/bpf/20200329004356.27286-2-kpsingh@chromium.org

diff --git a/tools/lib/bpf/libbpf_probes.c b/tools/lib/bpf/libbpf_probes.c
index b782ebef6ac9..2c92059c0c90 100644
--- a/tools/lib/bpf/libbpf_probes.c
+++ b/tools/lib/bpf/libbpf_probes.c
@@ -108,6 +108,7 @@ probe_load(enum bpf_prog_type prog_type, const struct bpf_insn *insns,
 	case BPF_PROG_TYPE_TRACING:
 	case BPF_PROG_TYPE_STRUCT_OPS:
 	case BPF_PROG_TYPE_EXT:
+	case BPF_PROG_TYPE_LSM:
 	default:
 		break;
 	}

commit 2db6eab18b9778d55f48c804f8efebd7097e7958
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Mon Jan 20 16:53:47 2020 -0800

    libbpf: Add support for program extensions
    
    Add minimal support for program extensions. bpf_object_open_opts() needs to be
    called with attach_prog_fd = target_prog_fd and BPF program extension needs to
    have in .c file section definition like SEC("freplace/func_to_be_replaced").
    libbpf will search for "func_to_be_replaced" in the target_prog_fd's BTF and
    will pass it in attach_btf_id to the kernel. This approach works for tests, but
    more compex use case may need to request function name (and attach_btf_id that
    kernel sees) to be more dynamic. Such API will be added in future patches.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: John Fastabend <john.fastabend@gmail.com>
    Acked-by: Andrii Nakryiko <andriin@fb.com>
    Acked-by: Toke Høiland-Jørgensen <toke@redhat.com>
    Link: https://lore.kernel.org/bpf/20200121005348.2769920-3-ast@kernel.org

diff --git a/tools/lib/bpf/libbpf_probes.c b/tools/lib/bpf/libbpf_probes.c
index 8cc992bc532a..b782ebef6ac9 100644
--- a/tools/lib/bpf/libbpf_probes.c
+++ b/tools/lib/bpf/libbpf_probes.c
@@ -107,6 +107,7 @@ probe_load(enum bpf_prog_type prog_type, const struct bpf_insn *insns,
 	case BPF_PROG_TYPE_CGROUP_SOCKOPT:
 	case BPF_PROG_TYPE_TRACING:
 	case BPF_PROG_TYPE_STRUCT_OPS:
+	case BPF_PROG_TYPE_EXT:
 	default:
 		break;
 	}

commit 1d1a3bcffe360a56fd8cc287ed74d4c3066daf42
Author: Andrii Nakryiko <andriin@fb.com>
Date:   Fri Jan 10 10:19:16 2020 -0800

    libbpf: Poison kernel-only integer types
    
    It's been a recurring issue with types like u32 slipping into libbpf source
    code accidentally. This is not detected during builds inside kernel source
    tree, but becomes a compilation error in libbpf's Github repo. Libbpf is
    supposed to use only __{s,u}{8,16,32,64} typedefs, so poison {s,u}{8,16,32,64}
    explicitly in every .c file. Doing that in a bit more centralized way, e.g.,
    inside libbpf_internal.h breaks selftests, which are both using kernel u32 and
    libbpf_internal.h.
    
    This patch also fixes a new u32 occurence in libbpf.c, added recently.
    
    Fixes: 590a00888250 ("bpf: libbpf: Add STRUCT_OPS support")
    Signed-off-by: Andrii Nakryiko <andriin@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Link: https://lore.kernel.org/bpf/20200110181916.271446-1-andriin@fb.com

diff --git a/tools/lib/bpf/libbpf_probes.c b/tools/lib/bpf/libbpf_probes.c
index 320697f8e4c7..8cc992bc532a 100644
--- a/tools/lib/bpf/libbpf_probes.c
+++ b/tools/lib/bpf/libbpf_probes.c
@@ -17,6 +17,9 @@
 #include "libbpf.h"
 #include "libbpf_internal.h"
 
+/* make sure libbpf doesn't use kernel-only integer typedefs */
+#pragma GCC poison u8 u16 u32 u64 s8 s16 s32 s64
+
 static bool grep(const char *buffer, const char *pattern)
 {
 	return !!strstr(buffer, pattern);

commit 590a0088825016ca7ec53f1aef7e84e1211778d8
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Wed Jan 8 16:35:14 2020 -0800

    bpf: libbpf: Add STRUCT_OPS support
    
    This patch adds BPF STRUCT_OPS support to libbpf.
    
    The only sec_name convention is SEC(".struct_ops") to identify the
    struct_ops implemented in BPF,
    e.g. To implement a tcp_congestion_ops:
    
    SEC(".struct_ops")
    struct tcp_congestion_ops dctcp = {
            .init           = (void *)dctcp_init,  /* <-- a bpf_prog */
            /* ... some more func prts ... */
            .name           = "bpf_dctcp",
    };
    
    Each struct_ops is defined as a global variable under SEC(".struct_ops")
    as above.  libbpf creates a map for each variable and the variable name
    is the map's name.  Multiple struct_ops is supported under
    SEC(".struct_ops").
    
    In the bpf_object__open phase, libbpf will look for the SEC(".struct_ops")
    section and find out what is the btf-type the struct_ops is
    implementing.  Note that the btf-type here is referring to
    a type in the bpf_prog.o's btf.  A "struct bpf_map" is added
    by bpf_object__add_map() as other maps do.  It will then
    collect (through SHT_REL) where are the bpf progs that the
    func ptrs are referring to.  No btf_vmlinux is needed in
    the open phase.
    
    In the bpf_object__load phase, the map-fields, which depend
    on the btf_vmlinux, are initialized (in bpf_map__init_kern_struct_ops()).
    It will also set the prog->type, prog->attach_btf_id, and
    prog->expected_attach_type.  Thus, the prog's properties do
    not rely on its section name.
    [ Currently, the bpf_prog's btf-type ==> btf_vmlinux's btf-type matching
      process is as simple as: member-name match + btf-kind match + size match.
      If these matching conditions fail, libbpf will reject.
      The current targeting support is "struct tcp_congestion_ops" which
      most of its members are function pointers.
      The member ordering of the bpf_prog's btf-type can be different from
      the btf_vmlinux's btf-type. ]
    
    Then, all obj->maps are created as usual (in bpf_object__create_maps()).
    
    Once the maps are created and prog's properties are all set,
    the libbpf will proceed to load all the progs.
    
    bpf_map__attach_struct_ops() is added to register a struct_ops
    map to a kernel subsystem.
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/bpf/20200109003514.3856730-1-kafai@fb.com

diff --git a/tools/lib/bpf/libbpf_probes.c b/tools/lib/bpf/libbpf_probes.c
index 221e6ad97012..320697f8e4c7 100644
--- a/tools/lib/bpf/libbpf_probes.c
+++ b/tools/lib/bpf/libbpf_probes.c
@@ -103,6 +103,7 @@ probe_load(enum bpf_prog_type prog_type, const struct bpf_insn *insns,
 	case BPF_PROG_TYPE_CGROUP_SYSCTL:
 	case BPF_PROG_TYPE_CGROUP_SOCKOPT:
 	case BPF_PROG_TYPE_TRACING:
+	case BPF_PROG_TYPE_STRUCT_OPS:
 	default:
 		break;
 	}
@@ -251,6 +252,7 @@ bool bpf_probe_map_type(enum bpf_map_type map_type, __u32 ifindex)
 	case BPF_MAP_TYPE_XSKMAP:
 	case BPF_MAP_TYPE_SOCKHASH:
 	case BPF_MAP_TYPE_REUSEPORT_SOCKARRAY:
+	case BPF_MAP_TYPE_STRUCT_OPS:
 	default:
 		break;
 	}

commit 5ff051200308ab6f4c64c0fe52187bf4a1234dac
Author: Michal Rostecki <mrostecki@opensuse.org>
Date:   Wed Jan 8 17:23:52 2020 +0100

    libbpf: Add probe for large INSN limit
    
    Introduce a new probe which checks whether kernel has large maximum
    program size which was increased in the following commit:
    
    c04c0d2b968a ("bpf: increase complexity limit and maximum program size")
    
    Based on the similar check in Cilium[0], authored by Daniel Borkmann.
    
      [0] https://github.com/cilium/cilium/commit/657d0f585afd26232cfa5d4e70b6f64d2ea91596
    
    Co-authored-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Michal Rostecki <mrostecki@opensuse.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Reviewed-by: Quentin Monnet <quentin.monnet@netronome.com>
    Link: https://lore.kernel.org/bpf/20200108162428.25014-2-mrostecki@opensuse.org

diff --git a/tools/lib/bpf/libbpf_probes.c b/tools/lib/bpf/libbpf_probes.c
index a9eb8b322671..221e6ad97012 100644
--- a/tools/lib/bpf/libbpf_probes.c
+++ b/tools/lib/bpf/libbpf_probes.c
@@ -321,3 +321,24 @@ bool bpf_probe_helper(enum bpf_func_id id, enum bpf_prog_type prog_type,
 
 	return res;
 }
+
+/*
+ * Probe for availability of kernel commit (5.3):
+ *
+ * c04c0d2b968a ("bpf: increase complexity limit and maximum program size")
+ */
+bool bpf_probe_large_insn_limit(__u32 ifindex)
+{
+	struct bpf_insn insns[BPF_MAXINSNS + 1];
+	int i;
+
+	for (i = 0; i < BPF_MAXINSNS; i++)
+		insns[i] = BPF_MOV64_IMM(BPF_REG_0, 1);
+	insns[BPF_MAXINSNS] = BPF_EXIT_INSN();
+
+	errno = 0;
+	probe_load(BPF_PROG_TYPE_SCHED_CLS, insns, ARRAY_SIZE(insns), NULL, 0,
+		   ifindex);
+
+	return errno != E2BIG && errno != EINVAL;
+}

commit 12a8654b2e5aab37b22c9608d008f9f0565862c0
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Wed Oct 30 15:32:12 2019 -0700

    libbpf: Add support for prog_tracing
    
    Cleanup libbpf from expected_attach_type == attach_btf_id hack
    and introduce BPF_PROG_TYPE_TRACING.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Andrii Nakryiko <andriin@fb.com>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Link: https://lore.kernel.org/bpf/20191030223212.953010-3-ast@kernel.org

diff --git a/tools/lib/bpf/libbpf_probes.c b/tools/lib/bpf/libbpf_probes.c
index 4b0b0364f5fc..a9eb8b322671 100644
--- a/tools/lib/bpf/libbpf_probes.c
+++ b/tools/lib/bpf/libbpf_probes.c
@@ -102,6 +102,7 @@ probe_load(enum bpf_prog_type prog_type, const struct bpf_insn *insns,
 	case BPF_PROG_TYPE_FLOW_DISSECTOR:
 	case BPF_PROG_TYPE_CGROUP_SYSCTL:
 	case BPF_PROG_TYPE_CGROUP_SOCKOPT:
+	case BPF_PROG_TYPE_TRACING:
 	default:
 		break;
 	}

commit e42346192c9f179a9a47845e4663ad2f453d2b7b
Author: Toke Høiland-Jørgensen <toke@redhat.com>
Date:   Fri Jul 26 18:06:57 2019 +0200

    tools/libbpf_probes: Add new devmap_hash type
    
    This adds the definition for BPF_MAP_TYPE_DEVMAP_HASH to libbpf_probes.c in
    tools/lib/bpf.
    
    Signed-off-by: Toke Høiland-Jørgensen <toke@redhat.com>
    Acked-by: Yonghong Song <yhs@fb.com>
    Acked-by: Jesper Dangaard Brouer <brouer@redhat.com
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/tools/lib/bpf/libbpf_probes.c b/tools/lib/bpf/libbpf_probes.c
index ace1a0708d99..4b0b0364f5fc 100644
--- a/tools/lib/bpf/libbpf_probes.c
+++ b/tools/lib/bpf/libbpf_probes.c
@@ -244,6 +244,7 @@ bool bpf_probe_map_type(enum bpf_map_type map_type, __u32 ifindex)
 	case BPF_MAP_TYPE_ARRAY_OF_MAPS:
 	case BPF_MAP_TYPE_HASH_OF_MAPS:
 	case BPF_MAP_TYPE_DEVMAP:
+	case BPF_MAP_TYPE_DEVMAP_HASH:
 	case BPF_MAP_TYPE_SOCKMAP:
 	case BPF_MAP_TYPE_CPUMAP:
 	case BPF_MAP_TYPE_XSKMAP:

commit 4cdbfb59c44a0df58ab321e4ddd9710cd0823584
Author: Stanislav Fomichev <sdf@google.com>
Date:   Thu Jun 27 13:38:49 2019 -0700

    libbpf: support sockopt hooks
    
    Make libbpf aware of new sockopt hooks so it can derive prog type
    and hook point from the section names.
    
    Cc: Andrii Nakryiko <andriin@fb.com>
    Cc: Martin Lau <kafai@fb.com>
    Signed-off-by: Stanislav Fomichev <sdf@google.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/tools/lib/bpf/libbpf_probes.c b/tools/lib/bpf/libbpf_probes.c
index 6635a31a7a16..ace1a0708d99 100644
--- a/tools/lib/bpf/libbpf_probes.c
+++ b/tools/lib/bpf/libbpf_probes.c
@@ -101,6 +101,7 @@ probe_load(enum bpf_prog_type prog_type, const struct bpf_insn *insns,
 	case BPF_PROG_TYPE_SK_REUSEPORT:
 	case BPF_PROG_TYPE_FLOW_DISSECTOR:
 	case BPF_PROG_TYPE_CGROUP_SYSCTL:
+	case BPF_PROG_TYPE_CGROUP_SOCKOPT:
 	default:
 		break;
 	}

commit cfd4921049269ee6765b4a1cb820b95d0df5dda5
Author: Michal Rostecki <mrostecki@opensuse.org>
Date:   Wed May 29 20:31:09 2019 +0200

    libbpf: Return btf_fd for load_sk_storage_btf
    
    Before this change, function load_sk_storage_btf expected that
    libbpf__probe_raw_btf was returning a BTF descriptor, but in fact it was
    returning an information about whether the probe was successful (0 or
    1). load_sk_storage_btf was using that value as an argument of the close
    function, which was resulting in closing stdout and thus terminating the
    process which called that function.
    
    That bug was visible in bpftool. `bpftool feature` subcommand was always
    exiting too early (because of closed stdout) and it didn't display all
    requested probes. `bpftool -j feature` or `bpftool -p feature` were not
    returning a valid json object.
    
    This change renames the libbpf__probe_raw_btf function to
    libbpf__load_raw_btf, which now returns a BTF descriptor, as expected in
    load_sk_storage_btf.
    
    v2:
    - Fix typo in the commit message.
    
    v3:
    - Simplify BTF descriptor handling in bpf_object__probe_btf_* functions.
    - Rename libbpf__probe_raw_btf function to libbpf__load_raw_btf and
    return a BTF descriptor.
    
    v4:
    - Fix typo in the commit message.
    
    Fixes: d7c4b3980c18 ("libbpf: detect supported kernel BTF features and sanitize BTF")
    Signed-off-by: Michal Rostecki <mrostecki@opensuse.org>
    Acked-by: Andrii Nakryiko <andriin@fb.com>
    Acked-by: Song Liu <songliubraving@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/tools/lib/bpf/libbpf_probes.c b/tools/lib/bpf/libbpf_probes.c
index 5e2aa83f637a..6635a31a7a16 100644
--- a/tools/lib/bpf/libbpf_probes.c
+++ b/tools/lib/bpf/libbpf_probes.c
@@ -133,8 +133,8 @@ bool bpf_probe_prog_type(enum bpf_prog_type prog_type, __u32 ifindex)
 	return errno != EINVAL && errno != EOPNOTSUPP;
 }
 
-int libbpf__probe_raw_btf(const char *raw_types, size_t types_len,
-			  const char *str_sec, size_t str_len)
+int libbpf__load_raw_btf(const char *raw_types, size_t types_len,
+			 const char *str_sec, size_t str_len)
 {
 	struct btf_header hdr = {
 		.magic = BTF_MAGIC,
@@ -157,14 +157,9 @@ int libbpf__probe_raw_btf(const char *raw_types, size_t types_len,
 	memcpy(raw_btf + hdr.hdr_len + hdr.type_len, str_sec, hdr.str_len);
 
 	btf_fd = bpf_load_btf(raw_btf, btf_len, NULL, 0, false);
-	if (btf_fd < 0) {
-		free(raw_btf);
-		return 0;
-	}
 
-	close(btf_fd);
 	free(raw_btf);
-	return 1;
+	return btf_fd;
 }
 
 static int load_sk_storage_btf(void)
@@ -190,7 +185,7 @@ static int load_sk_storage_btf(void)
 		BTF_MEMBER_ENC(23, 2, 32),/* struct bpf_spin_lock l; */
 	};
 
-	return libbpf__probe_raw_btf((char *)types, sizeof(types),
+	return libbpf__load_raw_btf((char *)types, sizeof(types),
 				     strs, sizeof(strs));
 }
 

commit d7c4b3980c18e81c0470f5df6d96d832f446d26f
Author: Andrii Nakryiko <andriin@fb.com>
Date:   Fri May 10 14:13:15 2019 -0700

    libbpf: detect supported kernel BTF features and sanitize BTF
    
    Depending on used versions of libbpf, Clang, and kernel, it's possible to
    have valid BPF object files with valid BTF information, that still won't
    load successfully due to Clang emitting newer BTF features (e.g.,
    BTF_KIND_FUNC, .BTF.ext's line_info/func_info, BTF_KIND_DATASEC, etc), that
    are not yet supported by older kernel.
    
    This patch adds detection of BTF features and sanitizes BPF object's BTF
    by substituting various supported BTF kinds, which have compatible layout:
      - BTF_KIND_FUNC -> BTF_KIND_TYPEDEF
      - BTF_KIND_FUNC_PROTO -> BTF_KIND_ENUM
      - BTF_KIND_VAR -> BTF_KIND_INT
      - BTF_KIND_DATASEC -> BTF_KIND_STRUCT
    
    Replacement is done in such a way as to preserve as much information as
    possible (names, sizes, etc) where possible without violating kernel's
    validation rules.
    
    v2->v3:
      - remove duplicate #defines from libbpf_util.h
    
    v1->v2:
      - add internal libbpf_internal.h w/ common stuff
      - switch SK storage BTF to use new libbpf__probe_raw_btf()
    
    Reported-by: Alexei Starovoitov <ast@fb.com>
    Signed-off-by: Andrii Nakryiko <andriin@fb.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/tools/lib/bpf/libbpf_probes.c b/tools/lib/bpf/libbpf_probes.c
index a2c64a9ce1a6..5e2aa83f637a 100644
--- a/tools/lib/bpf/libbpf_probes.c
+++ b/tools/lib/bpf/libbpf_probes.c
@@ -15,6 +15,7 @@
 
 #include "bpf.h"
 #include "libbpf.h"
+#include "libbpf_internal.h"
 
 static bool grep(const char *buffer, const char *pattern)
 {
@@ -132,21 +133,43 @@ bool bpf_probe_prog_type(enum bpf_prog_type prog_type, __u32 ifindex)
 	return errno != EINVAL && errno != EOPNOTSUPP;
 }
 
-static int load_btf(void)
+int libbpf__probe_raw_btf(const char *raw_types, size_t types_len,
+			  const char *str_sec, size_t str_len)
 {
-#define BTF_INFO_ENC(kind, kind_flag, vlen) \
-	((!!(kind_flag) << 31) | ((kind) << 24) | ((vlen) & BTF_MAX_VLEN))
-#define BTF_TYPE_ENC(name, info, size_or_type) \
-	(name), (info), (size_or_type)
-#define BTF_INT_ENC(encoding, bits_offset, nr_bits) \
-	((encoding) << 24 | (bits_offset) << 16 | (nr_bits))
-#define BTF_TYPE_INT_ENC(name, encoding, bits_offset, bits, sz) \
-	BTF_TYPE_ENC(name, BTF_INFO_ENC(BTF_KIND_INT, 0, 0), sz), \
-	BTF_INT_ENC(encoding, bits_offset, bits)
-#define BTF_MEMBER_ENC(name, type, bits_offset) \
-	(name), (type), (bits_offset)
-
-	const char btf_str_sec[] = "\0bpf_spin_lock\0val\0cnt\0l";
+	struct btf_header hdr = {
+		.magic = BTF_MAGIC,
+		.version = BTF_VERSION,
+		.hdr_len = sizeof(struct btf_header),
+		.type_len = types_len,
+		.str_off = types_len,
+		.str_len = str_len,
+	};
+	int btf_fd, btf_len;
+	__u8 *raw_btf;
+
+	btf_len = hdr.hdr_len + hdr.type_len + hdr.str_len;
+	raw_btf = malloc(btf_len);
+	if (!raw_btf)
+		return -ENOMEM;
+
+	memcpy(raw_btf, &hdr, sizeof(hdr));
+	memcpy(raw_btf + hdr.hdr_len, raw_types, hdr.type_len);
+	memcpy(raw_btf + hdr.hdr_len + hdr.type_len, str_sec, hdr.str_len);
+
+	btf_fd = bpf_load_btf(raw_btf, btf_len, NULL, 0, false);
+	if (btf_fd < 0) {
+		free(raw_btf);
+		return 0;
+	}
+
+	close(btf_fd);
+	free(raw_btf);
+	return 1;
+}
+
+static int load_sk_storage_btf(void)
+{
+	const char strs[] = "\0bpf_spin_lock\0val\0cnt\0l";
 	/* struct bpf_spin_lock {
 	 *   int val;
 	 * };
@@ -155,7 +178,7 @@ static int load_btf(void)
 	 *   struct bpf_spin_lock l;
 	 * };
 	 */
-	__u32 btf_raw_types[] = {
+	__u32 types[] = {
 		/* int */
 		BTF_TYPE_INT_ENC(0, BTF_INT_SIGNED, 0, 32, 4),  /* [1] */
 		/* struct bpf_spin_lock */                      /* [2] */
@@ -166,23 +189,9 @@ static int load_btf(void)
 		BTF_MEMBER_ENC(19, 1, 0), /* int cnt; */
 		BTF_MEMBER_ENC(23, 2, 32),/* struct bpf_spin_lock l; */
 	};
-	struct btf_header btf_hdr = {
-		.magic = BTF_MAGIC,
-		.version = BTF_VERSION,
-		.hdr_len = sizeof(struct btf_header),
-		.type_len = sizeof(btf_raw_types),
-		.str_off = sizeof(btf_raw_types),
-		.str_len = sizeof(btf_str_sec),
-	};
-	__u8 raw_btf[sizeof(struct btf_header) + sizeof(btf_raw_types) +
-		     sizeof(btf_str_sec)];
-
-	memcpy(raw_btf, &btf_hdr, sizeof(btf_hdr));
-	memcpy(raw_btf + sizeof(btf_hdr), btf_raw_types, sizeof(btf_raw_types));
-	memcpy(raw_btf + sizeof(btf_hdr) + sizeof(btf_raw_types),
-	       btf_str_sec, sizeof(btf_str_sec));
 
-	return bpf_load_btf(raw_btf, sizeof(raw_btf), 0, 0, 0);
+	return libbpf__probe_raw_btf((char *)types, sizeof(types),
+				     strs, sizeof(strs));
 }
 
 bool bpf_probe_map_type(enum bpf_map_type map_type, __u32 ifindex)
@@ -222,7 +231,7 @@ bool bpf_probe_map_type(enum bpf_map_type map_type, __u32 ifindex)
 		value_size = 8;
 		max_entries = 0;
 		map_flags = BPF_F_NO_PREALLOC;
-		btf_fd = load_btf();
+		btf_fd = load_sk_storage_btf();
 		if (btf_fd < 0)
 			return false;
 		break;

commit a19f89f3667c950ad13c1560e4abd8aa8526b6b1
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Fri Apr 26 16:39:44 2019 -0700

    bpf: Support BPF_MAP_TYPE_SK_STORAGE in bpf map probing
    
    This patch supports probing for the new BPF_MAP_TYPE_SK_STORAGE.
    BPF_MAP_TYPE_SK_STORAGE enforces BTF usage, so the new probe
    requires to create and load a BTF also.
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/tools/lib/bpf/libbpf_probes.c b/tools/lib/bpf/libbpf_probes.c
index 80ee922f290c..a2c64a9ce1a6 100644
--- a/tools/lib/bpf/libbpf_probes.c
+++ b/tools/lib/bpf/libbpf_probes.c
@@ -9,6 +9,7 @@
 #include <net/if.h>
 #include <sys/utsname.h>
 
+#include <linux/btf.h>
 #include <linux/filter.h>
 #include <linux/kernel.h>
 
@@ -131,11 +132,65 @@ bool bpf_probe_prog_type(enum bpf_prog_type prog_type, __u32 ifindex)
 	return errno != EINVAL && errno != EOPNOTSUPP;
 }
 
+static int load_btf(void)
+{
+#define BTF_INFO_ENC(kind, kind_flag, vlen) \
+	((!!(kind_flag) << 31) | ((kind) << 24) | ((vlen) & BTF_MAX_VLEN))
+#define BTF_TYPE_ENC(name, info, size_or_type) \
+	(name), (info), (size_or_type)
+#define BTF_INT_ENC(encoding, bits_offset, nr_bits) \
+	((encoding) << 24 | (bits_offset) << 16 | (nr_bits))
+#define BTF_TYPE_INT_ENC(name, encoding, bits_offset, bits, sz) \
+	BTF_TYPE_ENC(name, BTF_INFO_ENC(BTF_KIND_INT, 0, 0), sz), \
+	BTF_INT_ENC(encoding, bits_offset, bits)
+#define BTF_MEMBER_ENC(name, type, bits_offset) \
+	(name), (type), (bits_offset)
+
+	const char btf_str_sec[] = "\0bpf_spin_lock\0val\0cnt\0l";
+	/* struct bpf_spin_lock {
+	 *   int val;
+	 * };
+	 * struct val {
+	 *   int cnt;
+	 *   struct bpf_spin_lock l;
+	 * };
+	 */
+	__u32 btf_raw_types[] = {
+		/* int */
+		BTF_TYPE_INT_ENC(0, BTF_INT_SIGNED, 0, 32, 4),  /* [1] */
+		/* struct bpf_spin_lock */                      /* [2] */
+		BTF_TYPE_ENC(1, BTF_INFO_ENC(BTF_KIND_STRUCT, 0, 1), 4),
+		BTF_MEMBER_ENC(15, 1, 0), /* int val; */
+		/* struct val */                                /* [3] */
+		BTF_TYPE_ENC(15, BTF_INFO_ENC(BTF_KIND_STRUCT, 0, 2), 8),
+		BTF_MEMBER_ENC(19, 1, 0), /* int cnt; */
+		BTF_MEMBER_ENC(23, 2, 32),/* struct bpf_spin_lock l; */
+	};
+	struct btf_header btf_hdr = {
+		.magic = BTF_MAGIC,
+		.version = BTF_VERSION,
+		.hdr_len = sizeof(struct btf_header),
+		.type_len = sizeof(btf_raw_types),
+		.str_off = sizeof(btf_raw_types),
+		.str_len = sizeof(btf_str_sec),
+	};
+	__u8 raw_btf[sizeof(struct btf_header) + sizeof(btf_raw_types) +
+		     sizeof(btf_str_sec)];
+
+	memcpy(raw_btf, &btf_hdr, sizeof(btf_hdr));
+	memcpy(raw_btf + sizeof(btf_hdr), btf_raw_types, sizeof(btf_raw_types));
+	memcpy(raw_btf + sizeof(btf_hdr) + sizeof(btf_raw_types),
+	       btf_str_sec, sizeof(btf_str_sec));
+
+	return bpf_load_btf(raw_btf, sizeof(raw_btf), 0, 0, 0);
+}
+
 bool bpf_probe_map_type(enum bpf_map_type map_type, __u32 ifindex)
 {
 	int key_size, value_size, max_entries, map_flags;
+	__u32 btf_key_type_id = 0, btf_value_type_id = 0;
 	struct bpf_create_map_attr attr = {};
-	int fd = -1, fd_inner;
+	int fd = -1, btf_fd = -1, fd_inner;
 
 	key_size	= sizeof(__u32);
 	value_size	= sizeof(__u32);
@@ -161,6 +216,16 @@ bool bpf_probe_map_type(enum bpf_map_type map_type, __u32 ifindex)
 	case BPF_MAP_TYPE_STACK:
 		key_size	= 0;
 		break;
+	case BPF_MAP_TYPE_SK_STORAGE:
+		btf_key_type_id = 1;
+		btf_value_type_id = 3;
+		value_size = 8;
+		max_entries = 0;
+		map_flags = BPF_F_NO_PREALLOC;
+		btf_fd = load_btf();
+		if (btf_fd < 0)
+			return false;
+		break;
 	case BPF_MAP_TYPE_UNSPEC:
 	case BPF_MAP_TYPE_HASH:
 	case BPF_MAP_TYPE_ARRAY:
@@ -206,11 +271,18 @@ bool bpf_probe_map_type(enum bpf_map_type map_type, __u32 ifindex)
 		attr.max_entries = max_entries;
 		attr.map_flags = map_flags;
 		attr.map_ifindex = ifindex;
+		if (btf_fd >= 0) {
+			attr.btf_fd = btf_fd;
+			attr.btf_key_type_id = btf_key_type_id;
+			attr.btf_value_type_id = btf_value_type_id;
+		}
 
 		fd = bpf_create_map_xattr(&attr);
 	}
 	if (fd >= 0)
 		close(fd);
+	if (btf_fd >= 0)
+		close(btf_fd);
 
 	return fd >= 0;
 }

commit 4635b0ae4d26f87cf68dbab6740955dd1ad67cf4
Author: Matt Mullins <mmullins@fb.com>
Date:   Fri Apr 26 11:49:50 2019 -0700

    tools: sync bpf.h
    
    This adds BPF_PROG_TYPE_RAW_TRACEPOINT_WRITABLE, and fixes up the
    
            error: enumeration value ‘BPF_PROG_TYPE_RAW_TRACEPOINT_WRITABLE’ not handled in switch [-Werror=switch-enum]
    
    build errors it would otherwise cause in libbpf.
    
    Signed-off-by: Matt Mullins <mmullins@fb.com>
    Acked-by: Yonghong Song <yhs@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/tools/lib/bpf/libbpf_probes.c b/tools/lib/bpf/libbpf_probes.c
index 0f25541632e3..80ee922f290c 100644
--- a/tools/lib/bpf/libbpf_probes.c
+++ b/tools/lib/bpf/libbpf_probes.c
@@ -93,6 +93,7 @@ probe_load(enum bpf_prog_type prog_type, const struct bpf_insn *insns,
 	case BPF_PROG_TYPE_CGROUP_DEVICE:
 	case BPF_PROG_TYPE_SK_MSG:
 	case BPF_PROG_TYPE_RAW_TRACEPOINT:
+	case BPF_PROG_TYPE_RAW_TRACEPOINT_WRITABLE:
 	case BPF_PROG_TYPE_LWT_SEG6LOCAL:
 	case BPF_PROG_TYPE_LIRC_MODE2:
 	case BPF_PROG_TYPE_SK_REUSEPORT:

commit 063cc9f06ee6cac12dbc68a504bc4ff56bde790d
Author: Andrey Ignatov <rdna@fb.com>
Date:   Fri Mar 8 09:15:26 2019 -0800

    libbpf: Support sysctl hook
    
    Support BPF_PROG_TYPE_CGROUP_SYSCTL program in libbpf: identifying
    program and attach types by section name, probe.
    
    Signed-off-by: Andrey Ignatov <rdna@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/tools/lib/bpf/libbpf_probes.c b/tools/lib/bpf/libbpf_probes.c
index 8c3a1c04dcb2..0f25541632e3 100644
--- a/tools/lib/bpf/libbpf_probes.c
+++ b/tools/lib/bpf/libbpf_probes.c
@@ -97,6 +97,7 @@ probe_load(enum bpf_prog_type prog_type, const struct bpf_insn *insns,
 	case BPF_PROG_TYPE_LIRC_MODE2:
 	case BPF_PROG_TYPE_SK_REUSEPORT:
 	case BPF_PROG_TYPE_FLOW_DISSECTOR:
+	case BPF_PROG_TYPE_CGROUP_SYSCTL:
 	default:
 		break;
 	}

commit 2d3ea5e85dd867712ba8747cb01c2d88376ead5c
Author: Quentin Monnet <quentin.monnet@netronome.com>
Date:   Thu Jan 17 15:27:55 2019 +0000

    tools: bpftool: add probes for eBPF helper functions
    
    Similarly to what was done for program types and map types, add a set of
    probes to test the availability of the different eBPF helper functions
    on the current system.
    
    For each known program type, all known helpers are tested, in order to
    establish a compatibility matrix. Output is provided as a set of lists
    of available helpers, one per program type.
    
    Sample output:
    
        # bpftool feature probe kernel
        ...
        Scanning eBPF helper functions...
        eBPF helpers supported for program type socket_filter:
                - bpf_map_lookup_elem
                - bpf_map_update_elem
                - bpf_map_delete_elem
        ...
        eBPF helpers supported for program type kprobe:
                - bpf_map_lookup_elem
                - bpf_map_update_elem
                - bpf_map_delete_elem
        ...
    
        # bpftool --json --pretty feature probe kernel
        {
            ...
            "helpers": {
                "socket_filter_available_helpers": ["bpf_map_lookup_elem", \
                        "bpf_map_update_elem","bpf_map_delete_elem", ...
                ],
                "kprobe_available_helpers": ["bpf_map_lookup_elem", \
                        "bpf_map_update_elem","bpf_map_delete_elem", ...
                ],
                ...
            }
        }
    
    v5:
    - In libbpf.map, move global symbol to the new LIBBPF_0.0.2 section.
    
    v4:
    - Use "enum bpf_func_id" instead of "__u32" in bpf_probe_helper()
      declaration for the type of the argument used to pass the id of
      the helper to probe.
    - Undef BPF_HELPER_MAKE_ENTRY after using it.
    
    v3:
    - Do not pass kernel version from bpftool to libbpf probes (kernel
      version for testing program with kprobes is retrieved directly from
      libbpf).
    - Dump one list of available helpers per program type (instead of one
      list of compatible program types per helper).
    
    v2:
    - Move probes from bpftool to libbpf.
    - Test all program types for each helper, print a list of working prog
      types for each helper.
    - Fall back on include/uapi/linux/bpf.h for names and ids of helpers.
    - Remove C-style macros output from this patch.
    
    Signed-off-by: Quentin Monnet <quentin.monnet@netronome.com>
    Reviewed-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Stanislav Fomichev <sdf@google.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/tools/lib/bpf/libbpf_probes.c b/tools/lib/bpf/libbpf_probes.c
index f511bd317b87..8c3a1c04dcb2 100644
--- a/tools/lib/bpf/libbpf_probes.c
+++ b/tools/lib/bpf/libbpf_probes.c
@@ -2,7 +2,11 @@
 /* Copyright (c) 2019 Netronome Systems, Inc. */
 
 #include <errno.h>
+#include <fcntl.h>
+#include <string.h>
+#include <stdlib.h>
 #include <unistd.h>
+#include <net/if.h>
 #include <sys/utsname.h>
 
 #include <linux/filter.h>
@@ -11,6 +15,37 @@
 #include "bpf.h"
 #include "libbpf.h"
 
+static bool grep(const char *buffer, const char *pattern)
+{
+	return !!strstr(buffer, pattern);
+}
+
+static int get_vendor_id(int ifindex)
+{
+	char ifname[IF_NAMESIZE], path[64], buf[8];
+	ssize_t len;
+	int fd;
+
+	if (!if_indextoname(ifindex, ifname))
+		return -1;
+
+	snprintf(path, sizeof(path), "/sys/class/net/%s/device/vendor", ifname);
+
+	fd = open(path, O_RDONLY);
+	if (fd < 0)
+		return -1;
+
+	len = read(fd, buf, sizeof(buf));
+	close(fd);
+	if (len < 0)
+		return -1;
+	if (len >= (ssize_t)sizeof(buf))
+		return -1;
+	buf[len] = '\0';
+
+	return strtol(buf, NULL, 0);
+}
+
 static int get_kernel_version(void)
 {
 	int version, subversion, patchlevel;
@@ -177,3 +212,31 @@ bool bpf_probe_map_type(enum bpf_map_type map_type, __u32 ifindex)
 
 	return fd >= 0;
 }
+
+bool bpf_probe_helper(enum bpf_func_id id, enum bpf_prog_type prog_type,
+		      __u32 ifindex)
+{
+	struct bpf_insn insns[2] = {
+		BPF_EMIT_CALL(id),
+		BPF_EXIT_INSN()
+	};
+	char buf[4096] = {};
+	bool res;
+
+	probe_load(prog_type, insns, ARRAY_SIZE(insns), buf, sizeof(buf),
+		   ifindex);
+	res = !grep(buf, "invalid func ") && !grep(buf, "unknown func ");
+
+	if (ifindex) {
+		switch (get_vendor_id(ifindex)) {
+		case 0x19ee: /* Netronome specific */
+			res = res && !grep(buf, "not supported by FW") &&
+				!grep(buf, "unsupported function id");
+			break;
+		default:
+			break;
+		}
+	}
+
+	return res;
+}

commit f99e166397f0298fe78bce24c55c6d074f9bf196
Author: Quentin Monnet <quentin.monnet@netronome.com>
Date:   Thu Jan 17 15:27:54 2019 +0000

    tools: bpftool: add probes for eBPF map types
    
    Add new probes for eBPF map types, to detect what are the ones available
    on the system. Try creating one map of each type, and see if the kernel
    complains.
    
    Sample output:
    
        # bpftool feature probe kernel
        ...
        Scanning eBPF map types...
        eBPF map_type hash is available
        eBPF map_type array is available
        eBPF map_type prog_array is available
        ...
    
        # bpftool --json --pretty feature probe kernel
        {
            ...
            "map_types": {
                "have_hash_map_type": true,
                "have_array_map_type": true,
                "have_prog_array_map_type": true,
                ...
            }
        }
    
    v5:
    - In libbpf.map, move global symbol to the new LIBBPF_0.0.2 section.
    
    v3:
    - Use a switch with all enum values for setting specific map parameters,
      so that gcc complains at compile time (-Wswitch-enum) if new map types
      were added to the kernel but libbpf was not updated.
    
    v2:
    - Move probes from bpftool to libbpf.
    - Remove C-style macros output from this patch.
    
    Signed-off-by: Quentin Monnet <quentin.monnet@netronome.com>
    Reviewed-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Stanislav Fomichev <sdf@google.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/tools/lib/bpf/libbpf_probes.c b/tools/lib/bpf/libbpf_probes.c
index 056c0c186f2a..f511bd317b87 100644
--- a/tools/lib/bpf/libbpf_probes.c
+++ b/tools/lib/bpf/libbpf_probes.c
@@ -93,3 +93,87 @@ bool bpf_probe_prog_type(enum bpf_prog_type prog_type, __u32 ifindex)
 
 	return errno != EINVAL && errno != EOPNOTSUPP;
 }
+
+bool bpf_probe_map_type(enum bpf_map_type map_type, __u32 ifindex)
+{
+	int key_size, value_size, max_entries, map_flags;
+	struct bpf_create_map_attr attr = {};
+	int fd = -1, fd_inner;
+
+	key_size	= sizeof(__u32);
+	value_size	= sizeof(__u32);
+	max_entries	= 1;
+	map_flags	= 0;
+
+	switch (map_type) {
+	case BPF_MAP_TYPE_STACK_TRACE:
+		value_size	= sizeof(__u64);
+		break;
+	case BPF_MAP_TYPE_LPM_TRIE:
+		key_size	= sizeof(__u64);
+		value_size	= sizeof(__u64);
+		map_flags	= BPF_F_NO_PREALLOC;
+		break;
+	case BPF_MAP_TYPE_CGROUP_STORAGE:
+	case BPF_MAP_TYPE_PERCPU_CGROUP_STORAGE:
+		key_size	= sizeof(struct bpf_cgroup_storage_key);
+		value_size	= sizeof(__u64);
+		max_entries	= 0;
+		break;
+	case BPF_MAP_TYPE_QUEUE:
+	case BPF_MAP_TYPE_STACK:
+		key_size	= 0;
+		break;
+	case BPF_MAP_TYPE_UNSPEC:
+	case BPF_MAP_TYPE_HASH:
+	case BPF_MAP_TYPE_ARRAY:
+	case BPF_MAP_TYPE_PROG_ARRAY:
+	case BPF_MAP_TYPE_PERF_EVENT_ARRAY:
+	case BPF_MAP_TYPE_PERCPU_HASH:
+	case BPF_MAP_TYPE_PERCPU_ARRAY:
+	case BPF_MAP_TYPE_CGROUP_ARRAY:
+	case BPF_MAP_TYPE_LRU_HASH:
+	case BPF_MAP_TYPE_LRU_PERCPU_HASH:
+	case BPF_MAP_TYPE_ARRAY_OF_MAPS:
+	case BPF_MAP_TYPE_HASH_OF_MAPS:
+	case BPF_MAP_TYPE_DEVMAP:
+	case BPF_MAP_TYPE_SOCKMAP:
+	case BPF_MAP_TYPE_CPUMAP:
+	case BPF_MAP_TYPE_XSKMAP:
+	case BPF_MAP_TYPE_SOCKHASH:
+	case BPF_MAP_TYPE_REUSEPORT_SOCKARRAY:
+	default:
+		break;
+	}
+
+	if (map_type == BPF_MAP_TYPE_ARRAY_OF_MAPS ||
+	    map_type == BPF_MAP_TYPE_HASH_OF_MAPS) {
+		/* TODO: probe for device, once libbpf has a function to create
+		 * map-in-map for offload
+		 */
+		if (ifindex)
+			return false;
+
+		fd_inner = bpf_create_map(BPF_MAP_TYPE_HASH,
+					  sizeof(__u32), sizeof(__u32), 1, 0);
+		if (fd_inner < 0)
+			return false;
+		fd = bpf_create_map_in_map(map_type, NULL, sizeof(__u32),
+					   fd_inner, 1, 0);
+		close(fd_inner);
+	} else {
+		/* Note: No other restriction on map type probes for offload */
+		attr.map_type = map_type;
+		attr.key_size = key_size;
+		attr.value_size = value_size;
+		attr.max_entries = max_entries;
+		attr.map_flags = map_flags;
+		attr.map_ifindex = ifindex;
+
+		fd = bpf_create_map_xattr(&attr);
+	}
+	if (fd >= 0)
+		close(fd);
+
+	return fd >= 0;
+}

commit 1bf4b05810fe38c5f09973295e8d4234a4fd5d87
Author: Quentin Monnet <quentin.monnet@netronome.com>
Date:   Thu Jan 17 15:27:53 2019 +0000

    tools: bpftool: add probes for eBPF program types
    
    Introduce probes for supported BPF program types in libbpf, and call it
    from bpftool to test what types are available on the system. The probe
    simply consists in loading a very basic program of that type and see if
    the verifier complains or not.
    
    Sample output:
    
        # bpftool feature probe kernel
        ...
        Scanning eBPF program types...
        eBPF program_type socket_filter is available
        eBPF program_type kprobe is available
        eBPF program_type sched_cls is available
        ...
    
        # bpftool --json --pretty feature probe kernel
        {
            ...
            "program_types": {
                "have_socket_filter_prog_type": true,
                "have_kprobe_prog_type": true,
                "have_sched_cls_prog_type": true,
                ...
            }
        }
    
    v5:
    - In libbpf.map, move global symbol to a new LIBBPF_0.0.2 section.
    - Rename (non-API function) prog_load() as probe_load().
    
    v3:
    - Get kernel version for checking kprobes availability from libbpf
      instead of from bpftool. Do not pass kernel_version as an argument
      when calling libbpf probes.
    - Use a switch with all enum values for setting specific program
      parameters just before probing, so that gcc complains at compile time
      (-Wswitch-enum) if new prog types were added to the kernel but libbpf
      was not updated.
    - Add a comment in libbpf.h about setrlimit() usage to allow many
      consecutive probe attempts.
    
    v2:
    - Move probes from bpftool to libbpf.
    - Remove C-style macros output from this patch.
    
    Signed-off-by: Quentin Monnet <quentin.monnet@netronome.com>
    Reviewed-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Stanislav Fomichev <sdf@google.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/tools/lib/bpf/libbpf_probes.c b/tools/lib/bpf/libbpf_probes.c
new file mode 100644
index 000000000000..056c0c186f2a
--- /dev/null
+++ b/tools/lib/bpf/libbpf_probes.c
@@ -0,0 +1,95 @@
+// SPDX-License-Identifier: (LGPL-2.1 OR BSD-2-Clause)
+/* Copyright (c) 2019 Netronome Systems, Inc. */
+
+#include <errno.h>
+#include <unistd.h>
+#include <sys/utsname.h>
+
+#include <linux/filter.h>
+#include <linux/kernel.h>
+
+#include "bpf.h"
+#include "libbpf.h"
+
+static int get_kernel_version(void)
+{
+	int version, subversion, patchlevel;
+	struct utsname utsn;
+
+	/* Return 0 on failure, and attempt to probe with empty kversion */
+	if (uname(&utsn))
+		return 0;
+
+	if (sscanf(utsn.release, "%d.%d.%d",
+		   &version, &subversion, &patchlevel) != 3)
+		return 0;
+
+	return (version << 16) + (subversion << 8) + patchlevel;
+}
+
+static void
+probe_load(enum bpf_prog_type prog_type, const struct bpf_insn *insns,
+	   size_t insns_cnt, char *buf, size_t buf_len, __u32 ifindex)
+{
+	struct bpf_load_program_attr xattr = {};
+	int fd;
+
+	switch (prog_type) {
+	case BPF_PROG_TYPE_CGROUP_SOCK_ADDR:
+		xattr.expected_attach_type = BPF_CGROUP_INET4_CONNECT;
+		break;
+	case BPF_PROG_TYPE_KPROBE:
+		xattr.kern_version = get_kernel_version();
+		break;
+	case BPF_PROG_TYPE_UNSPEC:
+	case BPF_PROG_TYPE_SOCKET_FILTER:
+	case BPF_PROG_TYPE_SCHED_CLS:
+	case BPF_PROG_TYPE_SCHED_ACT:
+	case BPF_PROG_TYPE_TRACEPOINT:
+	case BPF_PROG_TYPE_XDP:
+	case BPF_PROG_TYPE_PERF_EVENT:
+	case BPF_PROG_TYPE_CGROUP_SKB:
+	case BPF_PROG_TYPE_CGROUP_SOCK:
+	case BPF_PROG_TYPE_LWT_IN:
+	case BPF_PROG_TYPE_LWT_OUT:
+	case BPF_PROG_TYPE_LWT_XMIT:
+	case BPF_PROG_TYPE_SOCK_OPS:
+	case BPF_PROG_TYPE_SK_SKB:
+	case BPF_PROG_TYPE_CGROUP_DEVICE:
+	case BPF_PROG_TYPE_SK_MSG:
+	case BPF_PROG_TYPE_RAW_TRACEPOINT:
+	case BPF_PROG_TYPE_LWT_SEG6LOCAL:
+	case BPF_PROG_TYPE_LIRC_MODE2:
+	case BPF_PROG_TYPE_SK_REUSEPORT:
+	case BPF_PROG_TYPE_FLOW_DISSECTOR:
+	default:
+		break;
+	}
+
+	xattr.prog_type = prog_type;
+	xattr.insns = insns;
+	xattr.insns_cnt = insns_cnt;
+	xattr.license = "GPL";
+	xattr.prog_ifindex = ifindex;
+
+	fd = bpf_load_program_xattr(&xattr, buf, buf_len);
+	if (fd >= 0)
+		close(fd);
+}
+
+bool bpf_probe_prog_type(enum bpf_prog_type prog_type, __u32 ifindex)
+{
+	struct bpf_insn insns[2] = {
+		BPF_MOV64_IMM(BPF_REG_0, 0),
+		BPF_EXIT_INSN()
+	};
+
+	if (ifindex && prog_type == BPF_PROG_TYPE_SCHED_CLS)
+		/* nfp returns -EINVAL on exit(0) with TC offload */
+		insns[0].imm = 2;
+
+	errno = 0;
+	probe_load(prog_type, insns, ARRAY_SIZE(insns), NULL, 0, ifindex);
+
+	return errno != EINVAL && errno != EOPNOTSUPP;
+}
