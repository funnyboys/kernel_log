commit 3b7313f2d7bbff44ea951c6602bee58c0148cf21
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Mon May 4 13:56:31 2020 -0300

    perf sched: Rename perf_evsel__*() operating on 'struct evsel *' to evsel__*()
    
    As those is a 'struct evsel' methods, not part of tools/lib/perf/, aka
    libperf, to whom the perf_ prefix belongs.
    
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index b99398031131..459e4229945e 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1848,7 +1848,7 @@ static inline void print_sched_time(unsigned long long nsecs, int width)
  * returns runtime data for event, allocating memory for it the
  * first time it is used.
  */
-static struct evsel_runtime *perf_evsel__get_runtime(struct evsel *evsel)
+static struct evsel_runtime *evsel__get_runtime(struct evsel *evsel)
 {
 	struct evsel_runtime *r = evsel->priv;
 
@@ -1863,10 +1863,9 @@ static struct evsel_runtime *perf_evsel__get_runtime(struct evsel *evsel)
 /*
  * save last time event was seen per cpu
  */
-static void perf_evsel__save_time(struct evsel *evsel,
-				  u64 timestamp, u32 cpu)
+static void evsel__save_time(struct evsel *evsel, u64 timestamp, u32 cpu)
 {
-	struct evsel_runtime *r = perf_evsel__get_runtime(evsel);
+	struct evsel_runtime *r = evsel__get_runtime(evsel);
 
 	if (r == NULL)
 		return;
@@ -1890,9 +1889,9 @@ static void perf_evsel__save_time(struct evsel *evsel,
 }
 
 /* returns last time this event was seen on the given cpu */
-static u64 perf_evsel__get_time(struct evsel *evsel, u32 cpu)
+static u64 evsel__get_time(struct evsel *evsel, u32 cpu)
 {
-	struct evsel_runtime *r = perf_evsel__get_runtime(evsel);
+	struct evsel_runtime *r = evsel__get_runtime(evsel);
 
 	if ((r == NULL) || (r->last_time == NULL) || (cpu >= r->ncpu))
 		return 0;
@@ -2548,7 +2547,7 @@ static int timehist_sched_change_event(struct perf_tool *tool,
 		goto out;
 	}
 
-	tprev = perf_evsel__get_time(evsel, sample->cpu);
+	tprev = evsel__get_time(evsel, sample->cpu);
 
 	/*
 	 * If start time given:
@@ -2631,7 +2630,7 @@ static int timehist_sched_change_event(struct perf_tool *tool,
 		tr->ready_to_run = 0;
 	}
 
-	perf_evsel__save_time(evsel, sample->time, sample->cpu);
+	evsel__save_time(evsel, sample->time, sample->cpu);
 
 	return rc;
 }
@@ -2941,7 +2940,7 @@ static int timehist_check_attr(struct perf_sched *sched,
 	struct evsel_runtime *er;
 
 	list_for_each_entry(evsel, &evlist->core.entries, core.node) {
-		er = perf_evsel__get_runtime(evsel);
+		er = evsel__get_runtime(evsel);
 		if (er == NULL) {
 			pr_err("Failed to allocate memory for evsel runtime data\n");
 			return -1;

commit efc0cdc9ed5e6cfb060ff7b77834cad9d3c97d1d
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Wed Apr 29 16:26:57 2020 -0300

    perf evsel: Rename perf_evsel__{str,int}val() and other tracepoint field metehods to to evsel__*()
    
    As those are not 'struct evsel' methods, not part of tools/lib/perf/,
    aka libperf, to whom the perf_ prefix belongs.
    
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 5c005260bf13..b99398031131 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -811,8 +811,8 @@ replay_wakeup_event(struct perf_sched *sched,
 		    struct evsel *evsel, struct perf_sample *sample,
 		    struct machine *machine __maybe_unused)
 {
-	const char *comm = perf_evsel__strval(evsel, sample, "comm");
-	const u32 pid	 = perf_evsel__intval(evsel, sample, "pid");
+	const char *comm = evsel__strval(evsel, sample, "comm");
+	const u32 pid	 = evsel__intval(evsel, sample, "pid");
 	struct task_desc *waker, *wakee;
 
 	if (verbose > 0) {
@@ -833,11 +833,11 @@ static int replay_switch_event(struct perf_sched *sched,
 			       struct perf_sample *sample,
 			       struct machine *machine __maybe_unused)
 {
-	const char *prev_comm  = perf_evsel__strval(evsel, sample, "prev_comm"),
-		   *next_comm  = perf_evsel__strval(evsel, sample, "next_comm");
-	const u32 prev_pid = perf_evsel__intval(evsel, sample, "prev_pid"),
-		  next_pid = perf_evsel__intval(evsel, sample, "next_pid");
-	const u64 prev_state = perf_evsel__intval(evsel, sample, "prev_state");
+	const char *prev_comm  = evsel__strval(evsel, sample, "prev_comm"),
+		   *next_comm  = evsel__strval(evsel, sample, "next_comm");
+	const u32 prev_pid = evsel__intval(evsel, sample, "prev_pid"),
+		  next_pid = evsel__intval(evsel, sample, "next_pid");
+	const u64 prev_state = evsel__intval(evsel, sample, "prev_state");
 	struct task_desc *prev, __maybe_unused *next;
 	u64 timestamp0, timestamp = sample->time;
 	int cpu = sample->cpu;
@@ -1106,9 +1106,9 @@ static int latency_switch_event(struct perf_sched *sched,
 				struct perf_sample *sample,
 				struct machine *machine)
 {
-	const u32 prev_pid = perf_evsel__intval(evsel, sample, "prev_pid"),
-		  next_pid = perf_evsel__intval(evsel, sample, "next_pid");
-	const u64 prev_state = perf_evsel__intval(evsel, sample, "prev_state");
+	const u32 prev_pid = evsel__intval(evsel, sample, "prev_pid"),
+		  next_pid = evsel__intval(evsel, sample, "next_pid");
+	const u64 prev_state = evsel__intval(evsel, sample, "prev_state");
 	struct work_atoms *out_events, *in_events;
 	struct thread *sched_out, *sched_in;
 	u64 timestamp0, timestamp = sample->time;
@@ -1176,8 +1176,8 @@ static int latency_runtime_event(struct perf_sched *sched,
 				 struct perf_sample *sample,
 				 struct machine *machine)
 {
-	const u32 pid	   = perf_evsel__intval(evsel, sample, "pid");
-	const u64 runtime  = perf_evsel__intval(evsel, sample, "runtime");
+	const u32 pid	   = evsel__intval(evsel, sample, "pid");
+	const u64 runtime  = evsel__intval(evsel, sample, "runtime");
 	struct thread *thread = machine__findnew_thread(machine, -1, pid);
 	struct work_atoms *atoms = thread_atoms_search(&sched->atom_root, thread, &sched->cmp_pid);
 	u64 timestamp = sample->time;
@@ -1211,7 +1211,7 @@ static int latency_wakeup_event(struct perf_sched *sched,
 				struct perf_sample *sample,
 				struct machine *machine)
 {
-	const u32 pid	  = perf_evsel__intval(evsel, sample, "pid");
+	const u32 pid	  = evsel__intval(evsel, sample, "pid");
 	struct work_atoms *atoms;
 	struct work_atom *atom;
 	struct thread *wakee;
@@ -1272,7 +1272,7 @@ static int latency_migrate_task_event(struct perf_sched *sched,
 				      struct perf_sample *sample,
 				      struct machine *machine)
 {
-	const u32 pid = perf_evsel__intval(evsel, sample, "pid");
+	const u32 pid = evsel__intval(evsel, sample, "pid");
 	u64 timestamp = sample->time;
 	struct work_atoms *atoms;
 	struct work_atom *atom;
@@ -1526,7 +1526,7 @@ map__findnew_thread(struct perf_sched *sched, struct machine *machine, pid_t pid
 static int map_switch_event(struct perf_sched *sched, struct evsel *evsel,
 			    struct perf_sample *sample, struct machine *machine)
 {
-	const u32 next_pid = perf_evsel__intval(evsel, sample, "next_pid");
+	const u32 next_pid = evsel__intval(evsel, sample, "next_pid");
 	struct thread *sched_in;
 	struct thread_runtime *tr;
 	int new_shortname;
@@ -1670,8 +1670,8 @@ static int process_sched_switch_event(struct perf_tool *tool,
 {
 	struct perf_sched *sched = container_of(tool, struct perf_sched, tool);
 	int this_cpu = sample->cpu, err = 0;
-	u32 prev_pid = perf_evsel__intval(evsel, sample, "prev_pid"),
-	    next_pid = perf_evsel__intval(evsel, sample, "next_pid");
+	u32 prev_pid = evsel__intval(evsel, sample, "prev_pid"),
+	    next_pid = evsel__intval(evsel, sample, "next_pid");
 
 	if (sched->curr_pid[this_cpu] != (u32)-1) {
 		/*
@@ -2004,8 +2004,8 @@ static void timehist_print_sample(struct perf_sched *sched,
 				  u64 t, int state)
 {
 	struct thread_runtime *tr = thread__priv(thread);
-	const char *next_comm = perf_evsel__strval(evsel, sample, "next_comm");
-	const u32 next_pid = perf_evsel__intval(evsel, sample, "next_pid");
+	const char *next_comm = evsel__strval(evsel, sample, "next_comm");
+	const u32 next_pid = evsel__intval(evsel, sample, "next_pid");
 	u32 max_cpus = sched->max_cpu + 1;
 	char tstr[64];
 	char nstr[30];
@@ -2137,7 +2137,7 @@ static bool is_idle_sample(struct perf_sample *sample,
 {
 	/* pid 0 == swapper == idle task */
 	if (strcmp(evsel__name(evsel), "sched:sched_switch") == 0)
-		return perf_evsel__intval(evsel, sample, "prev_pid") == 0;
+		return evsel__intval(evsel, sample, "prev_pid") == 0;
 
 	return sample->pid == 0;
 }
@@ -2334,7 +2334,7 @@ static struct thread *timehist_get_thread(struct perf_sched *sched,
 			itr->last_thread = thread;
 
 			/* copy task callchain when entering to idle */
-			if (perf_evsel__intval(evsel, sample, "next_pid") == 0)
+			if (evsel__intval(evsel, sample, "next_pid") == 0)
 				save_idle_callchain(sched, itr, sample);
 		}
 	}
@@ -2357,8 +2357,8 @@ static bool timehist_skip_sample(struct perf_sched *sched,
 	if (sched->idle_hist) {
 		if (strcmp(evsel__name(evsel), "sched:sched_switch"))
 			rc = true;
-		else if (perf_evsel__intval(evsel, sample, "prev_pid") != 0 &&
-			 perf_evsel__intval(evsel, sample, "next_pid") != 0)
+		else if (evsel__intval(evsel, sample, "prev_pid") != 0 &&
+			 evsel__intval(evsel, sample, "next_pid") != 0)
 			rc = true;
 	}
 
@@ -2409,7 +2409,7 @@ static int timehist_sched_wakeup_event(struct perf_tool *tool,
 	struct thread *thread;
 	struct thread_runtime *tr = NULL;
 	/* want pid of awakened task not pid in sample */
-	const u32 pid = perf_evsel__intval(evsel, sample, "pid");
+	const u32 pid = evsel__intval(evsel, sample, "pid");
 
 	thread = machine__findnew_thread(machine, 0, pid);
 	if (thread == NULL)
@@ -2445,8 +2445,8 @@ static void timehist_print_migration_event(struct perf_sched *sched,
 		return;
 
 	max_cpus = sched->max_cpu + 1;
-	ocpu = perf_evsel__intval(evsel, sample, "orig_cpu");
-	dcpu = perf_evsel__intval(evsel, sample, "dest_cpu");
+	ocpu = evsel__intval(evsel, sample, "orig_cpu");
+	dcpu = evsel__intval(evsel, sample, "dest_cpu");
 
 	thread = machine__findnew_thread(machine, sample->pid, sample->tid);
 	if (thread == NULL)
@@ -2493,7 +2493,7 @@ static int timehist_migrate_task_event(struct perf_tool *tool,
 	struct thread *thread;
 	struct thread_runtime *tr = NULL;
 	/* want pid of migrated task not pid in sample */
-	const u32 pid = perf_evsel__intval(evsel, sample, "pid");
+	const u32 pid = evsel__intval(evsel, sample, "pid");
 
 	thread = machine__findnew_thread(machine, 0, pid);
 	if (thread == NULL)
@@ -2524,8 +2524,7 @@ static int timehist_sched_change_event(struct perf_tool *tool,
 	struct thread_runtime *tr = NULL;
 	u64 tprev, t = sample->time;
 	int rc = 0;
-	int state = perf_evsel__intval(evsel, sample, "prev_state");
-
+	int state = evsel__intval(evsel, sample, "prev_state");
 
 	if (machine__resolve(machine, &al, sample) < 0) {
 		pr_err("problem processing %d event. skipping it\n",

commit 8ab2e96d8ff188006f1e3346a56443cd07fe1858
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Wed Apr 29 16:07:09 2020 -0300

    perf evsel: Rename *perf_evsel__*name() to *evsel__*name()
    
    As they are 'struct evsel' methods or related routines, not part of
    tools/lib/perf/, aka libperf, to whom the perf_ prefix belongs.
    
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 82fcc2c15fe4..5c005260bf13 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -2136,7 +2136,7 @@ static bool is_idle_sample(struct perf_sample *sample,
 			   struct evsel *evsel)
 {
 	/* pid 0 == swapper == idle task */
-	if (strcmp(perf_evsel__name(evsel), "sched:sched_switch") == 0)
+	if (strcmp(evsel__name(evsel), "sched:sched_switch") == 0)
 		return perf_evsel__intval(evsel, sample, "prev_pid") == 0;
 
 	return sample->pid == 0;
@@ -2355,7 +2355,7 @@ static bool timehist_skip_sample(struct perf_sched *sched,
 	}
 
 	if (sched->idle_hist) {
-		if (strcmp(perf_evsel__name(evsel), "sched:sched_switch"))
+		if (strcmp(evsel__name(evsel), "sched:sched_switch"))
 			rc = true;
 		else if (perf_evsel__intval(evsel, sample, "prev_pid") != 0 &&
 			 perf_evsel__intval(evsel, sample, "next_pid") != 0)

commit c30d630d1bcfad8d2f70ff0cbb2a86d5a43bc152
Author: David Ahern <dsahern@gmail.com>
Date:   Wed Dec 4 10:39:25 2019 -0700

    perf sched timehist: Add support for filtering on CPU
    
    Allow user to limit output to one or more CPUs. Really helpful on
    systems with a large number of cpus.
    
    Committer testing:
    
      # perf sched record -a sleep 1
      [ perf record: Woken up 1 times to write data ]
      [ perf record: Captured and wrote 1.765 MB perf.data (1412 samples) ]
      [root@quaco ~]# perf sched timehist | head
      Samples do not have callchains.
                 time    cpu  task name                       wait time  sch delay   run time
                              [tid/pid]                          (msec)     (msec)     (msec)
      --------------- ------  ------------------------------  ---------  ---------  ---------
         66307.802686 [0000]  perf[13086]                         0.000      0.000      0.000
         66307.802700 [0000]  migration/0[12]                     0.000      0.001      0.014
         66307.802766 [0001]  perf[13086]                         0.000      0.000      0.000
         66307.802774 [0001]  migration/1[15]                     0.000      0.001      0.007
         66307.802841 [0002]  perf[13086]                         0.000      0.000      0.000
         66307.802849 [0002]  migration/2[20]                     0.000      0.001      0.008
         66307.802913 [0003]  perf[13086]                         0.000      0.000      0.000
      #
      # perf sched timehist --cpu 2 | head
      Samples do not have callchains.
                 time    cpu  task name                       wait time  sch delay   run time
                              [tid/pid]                          (msec)     (msec)     (msec)
      --------------- ------  ------------------------------  ---------  ---------  ---------
         66307.802841 [0002]  perf[13086]                         0.000      0.000      0.000
         66307.802849 [0002]  migration/2[20]                     0.000      0.001      0.008
         66307.964485 [0002]  <idle>                              0.000      0.000    161.635
         66307.964811 [0002]  CPU 0/KVM[3589/3561]                0.000      0.056      0.325
         66307.965477 [0002]  <idle>                              0.325      0.000      0.666
         66307.965553 [0002]  CPU 0/KVM[3589/3561]                0.666      0.024      0.076
         66307.966456 [0002]  <idle>                              0.076      0.000      0.903
      #
    
    Signed-off-by: David Ahern <dsahern@gmail.com>
    Tested-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Link: http://lore.kernel.org/lkml/20191204173925.66976-1-dsahern@kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 8a12d71364c3..82fcc2c15fe4 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -51,6 +51,9 @@
 #define SYM_LEN			129
 #define MAX_PID			1024000
 
+static const char *cpu_list;
+static DECLARE_BITMAP(cpu_bitmap, MAX_NR_CPUS);
+
 struct sched_atom;
 
 struct task_desc {
@@ -2008,6 +2011,9 @@ static void timehist_print_sample(struct perf_sched *sched,
 	char nstr[30];
 	u64 wait_time;
 
+	if (cpu_list && !test_bit(sample->cpu, cpu_bitmap))
+		return;
+
 	timestamp__scnprintf_usec(t, tstr, sizeof(tstr));
 	printf("%15s [%04d] ", tstr, sample->cpu);
 
@@ -2994,6 +3000,12 @@ static int perf_sched__timehist(struct perf_sched *sched)
 	if (IS_ERR(session))
 		return PTR_ERR(session);
 
+	if (cpu_list) {
+		err = perf_session__cpu_bitmap(session, cpu_list, cpu_bitmap);
+		if (err < 0)
+			goto out;
+	}
+
 	evlist = session->evlist;
 
 	symbol__init(&session->header.env);
@@ -3429,6 +3441,7 @@ int cmd_sched(int argc, const char **argv)
 		   "analyze events only for given process id(s)"),
 	OPT_STRING('t', "tid", &symbol_conf.tid_list_str, "tid[,tid...]",
 		   "analyze events only for given thread id(s)"),
+	OPT_STRING('C', "cpu", &cpu_list, "cpu", "list of cpus to profile"),
 	OPT_PARENT(sched_options)
 	};
 

commit 5f0fef8ac3e7a5707751493293ba8ff2ffc0f8a4
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Mon Nov 4 12:14:32 2019 -0300

    perf callchain: Use 'struct map_symbol' in 'struct callchain_cursor_node'
    
    To ease passing around map+symbol, just like done for other parts of the
    tree recently.
    
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 5cacc4f84c8d..8a12d71364c3 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -2172,7 +2172,7 @@ static void save_task_callchain(struct perf_sched *sched,
 		if (node == NULL)
 			break;
 
-		sym = node->sym;
+		sym = node->ms.sym;
 		if (sym) {
 			if (!strcmp(sym->name, "schedule") ||
 			    !strcmp(sym->name, "__schedule") ||

commit ca1252779f48ece225c6003e01c675abb91cf1b4
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Tue Sep 24 15:41:51 2019 -0300

    perf evsel: Introduce evsel_fprintf.h
    
    We already had evsel_fprintf.c, add its counterpart, so that we can
    reduce evsel.h a bit more.
    
    We needed a new perf_event_attr_fprintf.c file so as to have a separate
    object to link with the python binding in tools/perf/util/python-ext-sources
    and not drag symbol_conf, etc into the python binding.
    
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Link: https://lkml.kernel.org/n/tip-06bdmt1062d9unzgqmxwlv88@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index f9706306fea0..5cacc4f84c8d 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -6,6 +6,7 @@
 #include "util/cpumap.h"
 #include "util/evlist.h"
 #include "util/evsel.h"
+#include "util/evsel_fprintf.h"
 #include "util/symbol.h"
 #include "util/thread.h"
 #include "util/header.h"

commit 9620bc361ac6e292ad2d6997b2f59f41f4e17862
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Wed Sep 25 15:06:59 2019 -0300

    perf evsel: Remove need for symbol_conf in evsel_fprintf.c
    
    So that we an later link it to the python binding without having to
    drag the symbol object files.
    
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Link: https://lkml.kernel.org/n/tip-8823tveyasocnuoelq4qopwf@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 079e67a36904..f9706306fea0 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -2055,7 +2055,7 @@ static void timehist_print_sample(struct perf_sched *sched,
 			    EVSEL__PRINT_SYM | EVSEL__PRINT_ONELINE |
 			    EVSEL__PRINT_CALLCHAIN_ARROW |
 			    EVSEL__PRINT_SKIP_IGNORED,
-			    &callchain_cursor, stdout);
+			    &callchain_cursor, symbol_conf.bt_stop_list,  stdout);
 
 out:
 	printf("\n");

commit 6ef81c55a2b6584cb642917f5fdf3632ef44b670
Author: Mamatha Inamdar <mamatha4@linux.vnet.ibm.com>
Date:   Thu Aug 22 12:50:49 2019 +0530

    perf session: Return error code for perf_session__new() function on failure
    
    This patch is to return error code of perf_new_session function on
    failure instead of NULL.
    
    Test Results:
    
    Before Fix:
    
      $ perf c2c report -input
      failed to open nput: No such file or directory
    
      $ echo $?
      0
      $
    
    After Fix:
    
      $ perf c2c report -input
      failed to open nput: No such file or directory
    
      $ echo $?
      254
      $
    
    Committer notes:
    
    Fix 'perf tests topology' case, where we use that TEST_ASSERT_VAL(...,
    session), i.e. we need to pass zero in case of failure, which was the
    case before when NULL was returned by perf_session__new() for failure,
    but now we need to negate the result of IS_ERR(session) to respect that
    TEST_ASSERT_VAL) expectation of zero meaning failure.
    
    Reported-by: Nageswara R Sastry <rnsastry@linux.vnet.ibm.com>
    Signed-off-by: Mamatha Inamdar <mamatha4@linux.vnet.ibm.com>
    Tested-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Tested-by: Nageswara R Sastry <rnsastry@linux.vnet.ibm.com>
    Acked-by: Ravi Bangoria <ravi.bangoria@linux.ibm.com>
    Reviewed-by: Jiri Olsa <jolsa@redhat.com>
    Reviewed-by: Mukesh Ojha <mojha@codeaurora.org>
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Alexey Budankov <alexey.budankov@linux.intel.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Jeremie Galarneau <jeremie.galarneau@efficios.com>
    Cc: Kate Stewart <kstewart@linuxfoundation.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Shawn Landden <shawn@git.icu>
    Cc: Song Liu <songliubraving@fb.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tzvetomir Stoyanov <tstoyanov@vmware.com>
    Link: http://lore.kernel.org/lkml/20190822071223.17892.45782.stgit@localhost.localdomain
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index f0b828c201cc..079e67a36904 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -40,6 +40,7 @@
 #include <api/fs/fs.h>
 #include <perf/cpumap.h>
 #include <linux/time64.h>
+#include <linux/err.h>
 
 #include <linux/ctype.h>
 
@@ -1797,9 +1798,9 @@ static int perf_sched__read_events(struct perf_sched *sched)
 	int rc = -1;
 
 	session = perf_session__new(&data, false, &sched->tool);
-	if (session == NULL) {
-		pr_debug("No Memory for session\n");
-		return -1;
+	if (IS_ERR(session)) {
+		pr_debug("Error creating perf session");
+		return PTR_ERR(session);
 	}
 
 	symbol__init(&session->header.env);
@@ -2989,8 +2990,8 @@ static int perf_sched__timehist(struct perf_sched *sched)
 	symbol_conf.use_callchain = sched->show_callchain;
 
 	session = perf_session__new(&data, false, &sched->tool);
-	if (session == NULL)
-		return -ENOMEM;
+	if (IS_ERR(session))
+		return PTR_ERR(session);
 
 	evlist = session->evlist;
 

commit f12be047d981bb802d8cf78eb220db3ee97f0513
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Wed Sep 18 10:11:20 2019 -0300

    perf sched: Add missing event.h include directive
    
    We use what is defined there, were getting it by luck, indirectly, fix
    it.
    
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Link: https://lkml.kernel.org/n/tip-e1cdt9557ctpvs3jb9c16qe6@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 511e19a7aafa..f0b828c201cc 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -24,6 +24,7 @@
 #include "util/trace-event.h"
 
 #include "util/debug.h"
+#include "util/event.h"
 
 #include <linux/kernel.h>
 #include <linux/log2.h>

commit 87ffb6c6407023419ae6b2770142b0754d9cbaa1
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Tue Sep 10 16:29:02 2019 +0100

    perf env: Remove needless cpumap.h header
    
    Only a 'struct perf_cmp_map' forward allocation is necessary, fix the
    places that need the header but were getting it indirectly, by luck,
    from env.h.
    
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Link: https://lkml.kernel.org/n/tip-3sj3n534zghxhk7ygzeaqlx9@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index ec96d64aec69..511e19a7aafa 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -3,6 +3,7 @@
 #include "perf.h"
 #include "perf-sys.h"
 
+#include "util/cpumap.h"
 #include "util/evlist.h"
 #include "util/evsel.h"
 #include "util/symbol.h"
@@ -36,6 +37,7 @@
 #include <pthread.h>
 #include <math.h>
 #include <api/fs/fs.h>
+#include <perf/cpumap.h>
 #include <linux/time64.h>
 
 #include <linux/ctype.h>

commit fa0d98462fae5d4951f22f3ac1090d48c53396d1
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Fri Aug 30 12:52:25 2019 -0300

    perf tools: Remove needless evlist.h include directives
    
    Remove the last unneeded use of cache.h in a header, we can check where
    it is really needed, i.e. we can remove it and be sure that it isn't
    being obtained indirectly.
    
    This is an old file, used by now incorrectly in many places, so it was
    providing includes needed indirectly, fixup this fallout.
    
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Link: https://lkml.kernel.org/n/tip-3x3l8gihoaeh7714os861ia7@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 91d0a9b10581..ec96d64aec69 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -4,7 +4,6 @@
 #include "perf-sys.h"
 
 #include "util/evlist.h"
-#include "util/cache.h"
 #include "util/evsel.h"
 #include "util/symbol.h"
 #include "util/thread.h"
@@ -19,6 +18,7 @@
 #include "util/callchain.h"
 #include "util/time-utils.h"
 
+#include <subcmd/pager.h>
 #include <subcmd/parse-options.h>
 #include "util/trace-event.h"
 

commit 91854f9a077e18e43ed30ebe9c61f8089bec9166
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Thu Aug 29 14:59:50 2019 -0300

    perf tools: Move everything related to sys_perf_event_open() to perf-sys.h
    
    And remove unneeded include directives from perf-sys.h to prune the
    header dependency tree.
    
    Fixup the fallout in places where definitions were being used without
    the needed include directives that were being satisfied because they
    were in perf-sys.h.
    
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Link: https://lkml.kernel.org/n/tip-7b1zvugiwak4ibfa3j6ott7f@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 025151dcb651..91d0a9b10581 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1,6 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0
 #include "builtin.h"
 #include "perf.h"
+#include "perf-sys.h"
 
 #include "util/evlist.h"
 #include "util/cache.h"

commit 5290ed6955ebc481d5cd62f7175e8514931058bc
Author: Jiri Olsa <jolsa@kernel.org>
Date:   Sun Aug 25 20:17:46 2019 +0200

    libperf: Add PERF_RECORD_LOST 'struct lost_event' to perf/event.h
    
    Move the lost_event event definition to libperf's event.h header
    include.
    
    In order to keep libperf simple, we switch 'u64/u32/u16/u8' types used
    events to their generic '__u*' versions.
    
    Perf added 'u*' types mainly to ease up printing __u64 values as stated
    in the linux/types.h comment:
    
      /*
       * We define u64 as uint64_t for every architecture
       * so that we can print it with "%"PRIx64 without getting warnings.
       *
       * typedef __u64 u64;
       * typedef __s64 s64;
       */
    
    Add and use new PRI_lu64 and PRI_lx64 macros for that.  Use extra '_' to
    ease up the reading and differentiate them from standard PRI*64 macros.
    
    Signed-off-by: Jiri Olsa <jolsa@kernel.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Michael Petlan <mpetlan@redhat.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20190825181752.722-7-jolsa@kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 0d6b4c3b1a51..025151dcb651 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -2643,7 +2643,7 @@ static int process_lost(struct perf_tool *tool __maybe_unused,
 
 	timestamp__scnprintf_usec(sample->time, tstr, sizeof(tstr));
 	printf("%15s ", tstr);
-	printf("lost %" PRIu64 " events on cpu %d\n", event->lost.lost, sample->cpu);
+	printf("lost %" PRI_lu64 " events on cpu %d\n", event->lost.lost, sample->cpu);
 
 	return 0;
 }

commit 9c3516d1b850ea938b074df33e4c86d721c77720
Author: Jiri Olsa <jolsa@kernel.org>
Date:   Sun Jul 21 13:24:30 2019 +0200

    libperf: Add perf_cpu_map__new()/perf_cpu_map__read() functions
    
    Moving the following functions from tools/perf:
    
      cpu_map__new()
      cpu_map__read()
    
    to libperf with the following names:
    
      perf_cpu_map__new()
      perf_cpu_map__read()
    
    Committer notes:
    
    Fixed up this one:
    
      tools/perf/arch/arm/util/cs-etm.c
    
    Signed-off-by: Jiri Olsa <jolsa@kernel.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Alexey Budankov <alexey.budankov@linux.intel.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Michael Petlan <mpetlan@redhat.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20190721112506.12306-44-jolsa@kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 897d11c8ca2e..0d6b4c3b1a51 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -3183,7 +3183,7 @@ static int setup_map_cpus(struct perf_sched *sched)
 	if (!sched->map.cpus_str)
 		return 0;
 
-	map = cpu_map__new(sched->map.cpus_str);
+	map = perf_cpu_map__new(sched->map.cpus_str);
 	if (!map) {
 		pr_err("failed to get cpus map from %s\n", sched->map.cpus_str);
 		return -1;
@@ -3217,7 +3217,7 @@ static int setup_color_cpus(struct perf_sched *sched)
 	if (!sched->map.color_cpus_str)
 		return 0;
 
-	map = cpu_map__new(sched->map.color_cpus_str);
+	map = perf_cpu_map__new(sched->map.color_cpus_str);
 	if (!map) {
 		pr_err("failed to get thread map from %s\n", sched->map.color_cpus_str);
 		return -1;

commit ce9036a6e3bdfac6c7ccf8221aec9bcf9c2d355e
Author: Jiri Olsa <jolsa@kernel.org>
Date:   Sun Jul 21 13:24:23 2019 +0200

    libperf: Include perf_evlist in evlist object
    
    Include perf_evlist in the evlist object, will continue to move other
    generic things into libperf's perf_evlist.
    
    Signed-off-by: Jiri Olsa <jolsa@kernel.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Alexey Budankov <alexey.budankov@linux.intel.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Michael Petlan <mpetlan@redhat.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20190721112506.12306-37-jolsa@kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 70247f1b23da..897d11c8ca2e 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -2929,7 +2929,7 @@ static int timehist_check_attr(struct perf_sched *sched,
 	struct evsel *evsel;
 	struct evsel_runtime *er;
 
-	list_for_each_entry(evsel, &evlist->entries, core.node) {
+	list_for_each_entry(evsel, &evlist->core.entries, core.node) {
 		er = perf_evsel__get_runtime(evsel);
 		if (er == NULL) {
 			pr_err("Failed to allocate memory for evsel runtime data\n");

commit b27c4ece725a7f2225f76ad05dc6f3f5463fe893
Author: Jiri Olsa <jolsa@kernel.org>
Date:   Sun Jul 21 13:24:22 2019 +0200

    libperf: Include perf_evsel in evsel object
    
    Including perf_evsel in evsel object, will continue to move other
    generic things into libperf's perf_evsel struct.
    
    Signed-off-by: Jiri Olsa <jolsa@kernel.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Alexey Budankov <alexey.budankov@linux.intel.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Michael Petlan <mpetlan@redhat.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20190721112506.12306-36-jolsa@kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index c02ecb295f23..70247f1b23da 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -2929,7 +2929,7 @@ static int timehist_check_attr(struct perf_sched *sched,
 	struct evsel *evsel;
 	struct evsel_runtime *er;
 
-	list_for_each_entry(evsel, &evlist->entries, node) {
+	list_for_each_entry(evsel, &evlist->entries, core.node) {
 		er = perf_evsel__get_runtime(evsel);
 		if (er == NULL) {
 			pr_err("Failed to allocate memory for evsel runtime data\n");

commit 63503dba87acfab49280d3b05df6705a6f327e8a
Author: Jiri Olsa <jolsa@kernel.org>
Date:   Sun Jul 21 13:23:52 2019 +0200

    perf evlist: Rename struct perf_evlist to struct evlist
    
    Rename struct perf_evlist to struct evlist, so we don't have a name
    clash when we add struct perf_evlist in libperf.
    
    Committer notes:
    
    Added fixes to build on arm64, from Jiri and from me
    (tools/perf/util/cs-etm.c)
    
    Signed-off-by: Jiri Olsa <jolsa@kernel.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Alexey Budankov <alexey.budankov@linux.intel.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Michael Petlan <mpetlan@redhat.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20190721112506.12306-6-jolsa@kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 55779f496d27..c02ecb295f23 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -2924,7 +2924,7 @@ static int perf_timehist__process_sample(struct perf_tool *tool,
 }
 
 static int timehist_check_attr(struct perf_sched *sched,
-			       struct perf_evlist *evlist)
+			       struct evlist *evlist)
 {
 	struct evsel *evsel;
 	struct evsel_runtime *er;
@@ -2963,7 +2963,7 @@ static int perf_sched__timehist(struct perf_sched *sched)
 	};
 
 	struct perf_session *session;
-	struct perf_evlist *evlist;
+	struct evlist *evlist;
 	int err = -1;
 
 	/*

commit 32dcd021d004038ca12ac17319da5aa4756e9312
Author: Jiri Olsa <jolsa@kernel.org>
Date:   Sun Jul 21 13:23:51 2019 +0200

    perf evsel: Rename struct perf_evsel to struct evsel
    
    Rename struct perf_evsel to struct evsel, so we don't have a name clash
    when we add struct perf_evsel in libperf.
    
    Committer notes:
    
    Added fixes for arm64, provided by Jiri.
    
    Signed-off-by: Jiri Olsa <jolsa@kernel.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Alexey Budankov <alexey.budankov@linux.intel.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Michael Petlan <mpetlan@redhat.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20190721112506.12306-5-jolsa@kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index ac6a0c5d6d6b..55779f496d27 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -133,13 +133,13 @@ typedef int (*sort_fn_t)(struct work_atoms *, struct work_atoms *);
 struct perf_sched;
 
 struct trace_sched_handler {
-	int (*switch_event)(struct perf_sched *sched, struct perf_evsel *evsel,
+	int (*switch_event)(struct perf_sched *sched, struct evsel *evsel,
 			    struct perf_sample *sample, struct machine *machine);
 
-	int (*runtime_event)(struct perf_sched *sched, struct perf_evsel *evsel,
+	int (*runtime_event)(struct perf_sched *sched, struct evsel *evsel,
 			     struct perf_sample *sample, struct machine *machine);
 
-	int (*wakeup_event)(struct perf_sched *sched, struct perf_evsel *evsel,
+	int (*wakeup_event)(struct perf_sched *sched, struct evsel *evsel,
 			    struct perf_sample *sample, struct machine *machine);
 
 	/* PERF_RECORD_FORK event, not sched_process_fork tracepoint */
@@ -147,7 +147,7 @@ struct trace_sched_handler {
 			  struct machine *machine);
 
 	int (*migrate_task_event)(struct perf_sched *sched,
-				  struct perf_evsel *evsel,
+				  struct evsel *evsel,
 				  struct perf_sample *sample,
 				  struct machine *machine);
 };
@@ -799,7 +799,7 @@ static void test_calibrations(struct perf_sched *sched)
 
 static int
 replay_wakeup_event(struct perf_sched *sched,
-		    struct perf_evsel *evsel, struct perf_sample *sample,
+		    struct evsel *evsel, struct perf_sample *sample,
 		    struct machine *machine __maybe_unused)
 {
 	const char *comm = perf_evsel__strval(evsel, sample, "comm");
@@ -820,7 +820,7 @@ replay_wakeup_event(struct perf_sched *sched,
 }
 
 static int replay_switch_event(struct perf_sched *sched,
-			       struct perf_evsel *evsel,
+			       struct evsel *evsel,
 			       struct perf_sample *sample,
 			       struct machine *machine __maybe_unused)
 {
@@ -1093,7 +1093,7 @@ add_sched_in_event(struct work_atoms *atoms, u64 timestamp)
 }
 
 static int latency_switch_event(struct perf_sched *sched,
-				struct perf_evsel *evsel,
+				struct evsel *evsel,
 				struct perf_sample *sample,
 				struct machine *machine)
 {
@@ -1163,7 +1163,7 @@ static int latency_switch_event(struct perf_sched *sched,
 }
 
 static int latency_runtime_event(struct perf_sched *sched,
-				 struct perf_evsel *evsel,
+				 struct evsel *evsel,
 				 struct perf_sample *sample,
 				 struct machine *machine)
 {
@@ -1198,7 +1198,7 @@ static int latency_runtime_event(struct perf_sched *sched,
 }
 
 static int latency_wakeup_event(struct perf_sched *sched,
-				struct perf_evsel *evsel,
+				struct evsel *evsel,
 				struct perf_sample *sample,
 				struct machine *machine)
 {
@@ -1259,7 +1259,7 @@ static int latency_wakeup_event(struct perf_sched *sched,
 }
 
 static int latency_migrate_task_event(struct perf_sched *sched,
-				      struct perf_evsel *evsel,
+				      struct evsel *evsel,
 				      struct perf_sample *sample,
 				      struct machine *machine)
 {
@@ -1470,7 +1470,7 @@ static void perf_sched__sort_lat(struct perf_sched *sched)
 }
 
 static int process_sched_wakeup_event(struct perf_tool *tool,
-				      struct perf_evsel *evsel,
+				      struct evsel *evsel,
 				      struct perf_sample *sample,
 				      struct machine *machine)
 {
@@ -1514,7 +1514,7 @@ map__findnew_thread(struct perf_sched *sched, struct machine *machine, pid_t pid
 	return thread;
 }
 
-static int map_switch_event(struct perf_sched *sched, struct perf_evsel *evsel,
+static int map_switch_event(struct perf_sched *sched, struct evsel *evsel,
 			    struct perf_sample *sample, struct machine *machine)
 {
 	const u32 next_pid = perf_evsel__intval(evsel, sample, "next_pid");
@@ -1655,7 +1655,7 @@ static int map_switch_event(struct perf_sched *sched, struct perf_evsel *evsel,
 }
 
 static int process_sched_switch_event(struct perf_tool *tool,
-				      struct perf_evsel *evsel,
+				      struct evsel *evsel,
 				      struct perf_sample *sample,
 				      struct machine *machine)
 {
@@ -1681,7 +1681,7 @@ static int process_sched_switch_event(struct perf_tool *tool,
 }
 
 static int process_sched_runtime_event(struct perf_tool *tool,
-				       struct perf_evsel *evsel,
+				       struct evsel *evsel,
 				       struct perf_sample *sample,
 				       struct machine *machine)
 {
@@ -1711,7 +1711,7 @@ static int perf_sched__process_fork_event(struct perf_tool *tool,
 }
 
 static int process_sched_migrate_task_event(struct perf_tool *tool,
-					    struct perf_evsel *evsel,
+					    struct evsel *evsel,
 					    struct perf_sample *sample,
 					    struct machine *machine)
 {
@@ -1724,14 +1724,14 @@ static int process_sched_migrate_task_event(struct perf_tool *tool,
 }
 
 typedef int (*tracepoint_handler)(struct perf_tool *tool,
-				  struct perf_evsel *evsel,
+				  struct evsel *evsel,
 				  struct perf_sample *sample,
 				  struct machine *machine);
 
 static int perf_sched__process_tracepoint_sample(struct perf_tool *tool __maybe_unused,
 						 union perf_event *event __maybe_unused,
 						 struct perf_sample *sample,
-						 struct perf_evsel *evsel,
+						 struct evsel *evsel,
 						 struct machine *machine)
 {
 	int err = 0;
@@ -1777,7 +1777,7 @@ static int perf_sched__process_comm(struct perf_tool *tool __maybe_unused,
 
 static int perf_sched__read_events(struct perf_sched *sched)
 {
-	const struct perf_evsel_str_handler handlers[] = {
+	const struct evsel_str_handler handlers[] = {
 		{ "sched:sched_switch",	      process_sched_switch_event, },
 		{ "sched:sched_stat_runtime", process_sched_runtime_event, },
 		{ "sched:sched_wakeup",	      process_sched_wakeup_event, },
@@ -1839,7 +1839,7 @@ static inline void print_sched_time(unsigned long long nsecs, int width)
  * returns runtime data for event, allocating memory for it the
  * first time it is used.
  */
-static struct evsel_runtime *perf_evsel__get_runtime(struct perf_evsel *evsel)
+static struct evsel_runtime *perf_evsel__get_runtime(struct evsel *evsel)
 {
 	struct evsel_runtime *r = evsel->priv;
 
@@ -1854,7 +1854,7 @@ static struct evsel_runtime *perf_evsel__get_runtime(struct perf_evsel *evsel)
 /*
  * save last time event was seen per cpu
  */
-static void perf_evsel__save_time(struct perf_evsel *evsel,
+static void perf_evsel__save_time(struct evsel *evsel,
 				  u64 timestamp, u32 cpu)
 {
 	struct evsel_runtime *r = perf_evsel__get_runtime(evsel);
@@ -1881,7 +1881,7 @@ static void perf_evsel__save_time(struct perf_evsel *evsel,
 }
 
 /* returns last time this event was seen on the given cpu */
-static u64 perf_evsel__get_time(struct perf_evsel *evsel, u32 cpu)
+static u64 perf_evsel__get_time(struct evsel *evsel, u32 cpu)
 {
 	struct evsel_runtime *r = perf_evsel__get_runtime(evsel);
 
@@ -1988,7 +1988,7 @@ static char task_state_char(struct thread *thread, int state)
 }
 
 static void timehist_print_sample(struct perf_sched *sched,
-				  struct perf_evsel *evsel,
+				  struct evsel *evsel,
 				  struct perf_sample *sample,
 				  struct addr_location *al,
 				  struct thread *thread,
@@ -2121,7 +2121,7 @@ static void timehist_update_runtime_stats(struct thread_runtime *r,
 }
 
 static bool is_idle_sample(struct perf_sample *sample,
-			   struct perf_evsel *evsel)
+			   struct evsel *evsel)
 {
 	/* pid 0 == swapper == idle task */
 	if (strcmp(perf_evsel__name(evsel), "sched:sched_switch") == 0)
@@ -2132,7 +2132,7 @@ static bool is_idle_sample(struct perf_sample *sample,
 
 static void save_task_callchain(struct perf_sched *sched,
 				struct perf_sample *sample,
-				struct perf_evsel *evsel,
+				struct evsel *evsel,
 				struct machine *machine)
 {
 	struct callchain_cursor *cursor = &callchain_cursor;
@@ -2286,7 +2286,7 @@ static void save_idle_callchain(struct perf_sched *sched,
 static struct thread *timehist_get_thread(struct perf_sched *sched,
 					  struct perf_sample *sample,
 					  struct machine *machine,
-					  struct perf_evsel *evsel)
+					  struct evsel *evsel)
 {
 	struct thread *thread;
 
@@ -2332,7 +2332,7 @@ static struct thread *timehist_get_thread(struct perf_sched *sched,
 
 static bool timehist_skip_sample(struct perf_sched *sched,
 				 struct thread *thread,
-				 struct perf_evsel *evsel,
+				 struct evsel *evsel,
 				 struct perf_sample *sample)
 {
 	bool rc = false;
@@ -2354,7 +2354,7 @@ static bool timehist_skip_sample(struct perf_sched *sched,
 }
 
 static void timehist_print_wakeup_event(struct perf_sched *sched,
-					struct perf_evsel *evsel,
+					struct evsel *evsel,
 					struct perf_sample *sample,
 					struct machine *machine,
 					struct thread *awakened)
@@ -2389,7 +2389,7 @@ static void timehist_print_wakeup_event(struct perf_sched *sched,
 
 static int timehist_sched_wakeup_event(struct perf_tool *tool,
 				       union perf_event *event __maybe_unused,
-				       struct perf_evsel *evsel,
+				       struct evsel *evsel,
 				       struct perf_sample *sample,
 				       struct machine *machine)
 {
@@ -2419,7 +2419,7 @@ static int timehist_sched_wakeup_event(struct perf_tool *tool,
 }
 
 static void timehist_print_migration_event(struct perf_sched *sched,
-					struct perf_evsel *evsel,
+					struct evsel *evsel,
 					struct perf_sample *sample,
 					struct machine *machine,
 					struct thread *migrated)
@@ -2473,7 +2473,7 @@ static void timehist_print_migration_event(struct perf_sched *sched,
 
 static int timehist_migrate_task_event(struct perf_tool *tool,
 				       union perf_event *event __maybe_unused,
-				       struct perf_evsel *evsel,
+				       struct evsel *evsel,
 				       struct perf_sample *sample,
 				       struct machine *machine)
 {
@@ -2501,7 +2501,7 @@ static int timehist_migrate_task_event(struct perf_tool *tool,
 
 static int timehist_sched_change_event(struct perf_tool *tool,
 				       union perf_event *event,
-				       struct perf_evsel *evsel,
+				       struct evsel *evsel,
 				       struct perf_sample *sample,
 				       struct machine *machine)
 {
@@ -2627,7 +2627,7 @@ static int timehist_sched_change_event(struct perf_tool *tool,
 
 static int timehist_sched_switch_event(struct perf_tool *tool,
 			     union perf_event *event,
-			     struct perf_evsel *evsel,
+			     struct evsel *evsel,
 			     struct perf_sample *sample,
 			     struct machine *machine __maybe_unused)
 {
@@ -2897,14 +2897,14 @@ static void timehist_print_summary(struct perf_sched *sched,
 
 typedef int (*sched_handler)(struct perf_tool *tool,
 			  union perf_event *event,
-			  struct perf_evsel *evsel,
+			  struct evsel *evsel,
 			  struct perf_sample *sample,
 			  struct machine *machine);
 
 static int perf_timehist__process_sample(struct perf_tool *tool,
 					 union perf_event *event,
 					 struct perf_sample *sample,
-					 struct perf_evsel *evsel,
+					 struct evsel *evsel,
 					 struct machine *machine)
 {
 	struct perf_sched *sched = container_of(tool, struct perf_sched, tool);
@@ -2926,7 +2926,7 @@ static int perf_timehist__process_sample(struct perf_tool *tool,
 static int timehist_check_attr(struct perf_sched *sched,
 			       struct perf_evlist *evlist)
 {
-	struct perf_evsel *evsel;
+	struct evsel *evsel;
 	struct evsel_runtime *er;
 
 	list_for_each_entry(evsel, &evlist->entries, node) {
@@ -2948,12 +2948,12 @@ static int timehist_check_attr(struct perf_sched *sched,
 
 static int perf_sched__timehist(struct perf_sched *sched)
 {
-	const struct perf_evsel_str_handler handlers[] = {
+	const struct evsel_str_handler handlers[] = {
 		{ "sched:sched_switch",       timehist_sched_switch_event, },
 		{ "sched:sched_wakeup",	      timehist_sched_wakeup_event, },
 		{ "sched:sched_wakeup_new",   timehist_sched_wakeup_event, },
 	};
-	const struct perf_evsel_str_handler migrate_handlers[] = {
+	const struct evsel_str_handler migrate_handlers[] = {
 		{ "sched:sched_migrate_task", timehist_migrate_task_event, },
 	};
 	struct perf_data data = {

commit 9749b90e566ca1a235fc8e2118f99c5690969342
Author: Jiri Olsa <jolsa@kernel.org>
Date:   Sun Jul 21 13:23:50 2019 +0200

    perf tools: Rename struct thread_map to struct perf_thread_map
    
    Rename struct thread_map to struct perf_thread_map, so it could be part
    of libperf.
    
    Signed-off-by: Jiri Olsa <jolsa@kernel.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Alexey Budankov <alexey.budankov@linux.intel.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Michael Petlan <mpetlan@redhat.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20190721112506.12306-4-jolsa@kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 51dd48f20972..ac6a0c5d6d6b 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -159,7 +159,7 @@ struct perf_sched_map {
 	DECLARE_BITMAP(comp_cpus_mask, MAX_CPUS);
 	int			*comp_cpus;
 	bool			 comp;
-	struct thread_map	*color_pids;
+	struct perf_thread_map *color_pids;
 	const char		*color_pids_str;
 	struct perf_cpu_map	*color_cpus;
 	const char		*color_cpus_str;
@@ -3195,7 +3195,7 @@ static int setup_map_cpus(struct perf_sched *sched)
 
 static int setup_color_pids(struct perf_sched *sched)
 {
-	struct thread_map *map;
+	struct perf_thread_map *map;
 
 	if (!sched->map.color_pids_str)
 		return 0;

commit f854839ba2a546a888159667c5ade96793e5cd10
Author: Jiri Olsa <jolsa@kernel.org>
Date:   Sun Jul 21 13:23:49 2019 +0200

    perf cpu_map: Rename struct cpu_map to struct perf_cpu_map
    
    Rename struct cpu_map to struct perf_cpu_map, so it could be part of
    libperf.
    
    Committer notes:
    
    Added fixes for arm64, provided by Jiri.
    
    Signed-off-by: Jiri Olsa <jolsa@kernel.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Alexey Budankov <alexey.budankov@linux.intel.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Michael Petlan <mpetlan@redhat.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20190721112506.12306-3-jolsa@kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 56d1907b1215..51dd48f20972 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -161,9 +161,9 @@ struct perf_sched_map {
 	bool			 comp;
 	struct thread_map	*color_pids;
 	const char		*color_pids_str;
-	struct cpu_map		*color_cpus;
+	struct perf_cpu_map	*color_cpus;
 	const char		*color_cpus_str;
-	struct cpu_map		*cpus;
+	struct perf_cpu_map	*cpus;
 	const char		*cpus_str;
 };
 
@@ -3170,7 +3170,7 @@ static int perf_sched__lat(struct perf_sched *sched)
 
 static int setup_map_cpus(struct perf_sched *sched)
 {
-	struct cpu_map *map;
+	struct perf_cpu_map *map;
 
 	sched->max_cpu  = sysconf(_SC_NPROCESSORS_CONF);
 
@@ -3212,7 +3212,7 @@ static int setup_color_pids(struct perf_sched *sched)
 
 static int setup_color_cpus(struct perf_sched *sched)
 {
-	struct cpu_map *map;
+	struct perf_cpu_map *map;
 
 	if (!sched->map.color_cpus_str)
 		return 0;

commit 7f7c536f23e6afaa5d5d4b0e0958b0be8922491f
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Thu Jul 4 11:32:27 2019 -0300

    tools lib: Adopt zalloc()/zfree() from tools/perf
    
    Eroding a bit more the tools/perf/util/util.h hodpodge header.
    
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Link: https://lkml.kernel.org/n/tip-natazosyn9rwjka25tvcnyi0@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 1519989961ff..56d1907b1215 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -2,7 +2,6 @@
 #include "builtin.h"
 #include "perf.h"
 
-#include "util/util.h"
 #include "util/evlist.h"
 #include "util/cache.h"
 #include "util/evsel.h"
@@ -26,6 +25,7 @@
 
 #include <linux/kernel.h>
 #include <linux/log2.h>
+#include <linux/zalloc.h>
 #include <sys/prctl.h>
 #include <sys/resource.h>
 #include <inttypes.h>

commit 3052ba56bcb589046eca6a931bd897742653d2cb
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Tue Jun 25 17:27:31 2019 -0300

    tools perf: Move from sane_ctype.h obtained from git to the Linux's original
    
    We got the sane_ctype.h headers from git and kept using it so far, but
    since that code originally came from the kernel sources to the git
    sources, perhaps its better to just use the one in the kernel, so that
    we can leverage tools/perf/check_headers.sh to be notified when our copy
    gets out of sync, i.e. when fixes or goodies are added to the code we've
    copied.
    
    This will help with things like tools/lib/string.c where we want to have
    more things in common with the kernel, such as strim(), skip_spaces(),
    etc so as to go on removing the things that we have in tools/perf/util/
    and instead using the code in the kernel, indirectly and removing things
    like EXPORT_SYMBOL(), etc, getting notified when fixes and improvements
    are made to the original code.
    
    Hopefully this also should help with reducing the difference of code
    hosted in tools/ to the one in the kernel proper.
    
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Link: https://lkml.kernel.org/n/tip-7k9868l713wqtgo01xxygn12@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 79577b67c898..1519989961ff 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -37,7 +37,7 @@
 #include <api/fs/fs.h>
 #include <linux/time64.h>
 
-#include "sane_ctype.h"
+#include <linux/ctype.h>
 
 #define PR_SET_NAME		15               /* Set process name */
 #define MAX_CPUS		4096

commit 6a9fa4e3bddedc027b691b6470c500d51d04e56c
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Tue Jun 25 17:31:26 2019 -0300

    perf string: Move 'dots' and 'graph_dotted_line' out of sane_ctype.h
    
    Those are not in that file in the git repo, lets move it from there so
    that we get that sane ctype code fully isolated to allow getting it in
    sync either with the git sources or better with the kernel sources
    (include/linux/ctype.h + lib/ctype.h), that way we can use
    check_headers.h to get notified when changes are made in the original
    code so that we can cherry-pick.
    
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Link: https://lkml.kernel.org/n/tip-ioh5sghn3943j0rxg6lb2dgs@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 275f2d92a7bf..79577b67c898 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -15,6 +15,7 @@
 #include "util/thread_map.h"
 #include "util/color.h"
 #include "util/stat.h"
+#include "util/string2.h"
 #include "util/callchain.h"
 #include "util/time-utils.h"
 

commit 2d4f27999b8877409f326682fd8cc40c52f47cea
Author: Jiri Olsa <jolsa@kernel.org>
Date:   Thu Feb 21 10:41:30 2019 +0100

    perf data: Add global path holder
    
    Add a 'path' member to 'struct perf_data'. It will keep the configured
    path for the data (const char *). The path in struct perf_data_file is
    now dynamically allocated (duped) from it.
    
    This scheme is useful/used in following patches where struct
    perf_data::path holds the 'configure' directory path and struct
    perf_data_file::path holds the allocated path for specific files.
    
    Also it actually makes the code little simpler.
    
    Signed-off-by: Jiri Olsa <jolsa@kernel.org>
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Alexey Budankov <alexey.budankov@linux.intel.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Link: http://lkml.kernel.org/r/20190221094145.9151-3-jolsa@kernel.org
    [ Fixup data-convert-bt.c missing conversion ]
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 640558e9352e..275f2d92a7bf 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1785,11 +1785,9 @@ static int perf_sched__read_events(struct perf_sched *sched)
 	};
 	struct perf_session *session;
 	struct perf_data data = {
-		.file      = {
-			.path = input_name,
-		},
-		.mode      = PERF_DATA_MODE_READ,
-		.force     = sched->force,
+		.path  = input_name,
+		.mode  = PERF_DATA_MODE_READ,
+		.force = sched->force,
 	};
 	int rc = -1;
 
@@ -2958,11 +2956,9 @@ static int perf_sched__timehist(struct perf_sched *sched)
 		{ "sched:sched_migrate_task", timehist_migrate_task_event, },
 	};
 	struct perf_data data = {
-		.file      = {
-			.path = input_name,
-		},
-		.mode      = PERF_DATA_MODE_READ,
-		.force     = sched->force,
+		.path  = input_name,
+		.mode  = PERF_DATA_MODE_READ,
+		.force = sched->force,
 	};
 
 	struct perf_session *session;

commit cb4c13a5137766c3666ae106e1a5549316992379
Author: Davidlohr Bueso <dave@stgolabs.net>
Date:   Thu Dec 6 11:18:19 2018 -0800

    perf sched: Use cached rbtrees
    
    At the cost of an extra pointer, we can avoid the O(logN) cost of
    finding the first element in the tree (smallest node), which is
    something heavily required for perf-sched.
    
    Signed-off-by: Davidlohr Bueso <dbueso@suse.de>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Link: http://lkml.kernel.org/r/20181206191819.30182-8-dave@stgolabs.net
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 2e0f0c65964a..640558e9352e 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -213,7 +213,7 @@ struct perf_sched {
 	u64		 all_runtime;
 	u64		 all_count;
 	u64		 cpu_last_switched[MAX_CPUS];
-	struct rb_root	 atom_root, sorted_atom_root, merged_atom_root;
+	struct rb_root_cached atom_root, sorted_atom_root, merged_atom_root;
 	struct list_head sort_list, cmp_pid;
 	bool force;
 	bool skip_merge;
@@ -271,7 +271,7 @@ struct evsel_runtime {
 struct idle_thread_runtime {
 	struct thread_runtime	tr;
 	struct thread		*last_thread;
-	struct rb_root		sorted_root;
+	struct rb_root_cached	sorted_root;
 	struct callchain_root	callchain;
 	struct callchain_cursor	cursor;
 };
@@ -950,10 +950,10 @@ thread_lat_cmp(struct list_head *list, struct work_atoms *l, struct work_atoms *
 }
 
 static struct work_atoms *
-thread_atoms_search(struct rb_root *root, struct thread *thread,
+thread_atoms_search(struct rb_root_cached *root, struct thread *thread,
 			 struct list_head *sort_list)
 {
-	struct rb_node *node = root->rb_node;
+	struct rb_node *node = root->rb_root.rb_node;
 	struct work_atoms key = { .thread = thread };
 
 	while (node) {
@@ -976,10 +976,11 @@ thread_atoms_search(struct rb_root *root, struct thread *thread,
 }
 
 static void
-__thread_latency_insert(struct rb_root *root, struct work_atoms *data,
+__thread_latency_insert(struct rb_root_cached *root, struct work_atoms *data,
 			 struct list_head *sort_list)
 {
-	struct rb_node **new = &(root->rb_node), *parent = NULL;
+	struct rb_node **new = &(root->rb_root.rb_node), *parent = NULL;
+	bool leftmost = true;
 
 	while (*new) {
 		struct work_atoms *this;
@@ -992,12 +993,14 @@ __thread_latency_insert(struct rb_root *root, struct work_atoms *data,
 
 		if (cmp > 0)
 			new = &((*new)->rb_left);
-		else
+		else {
 			new = &((*new)->rb_right);
+			leftmost = false;
+		}
 	}
 
 	rb_link_node(&data->node, parent, new);
-	rb_insert_color(&data->node, root);
+	rb_insert_color_cached(&data->node, root, leftmost);
 }
 
 static int thread_atoms_insert(struct perf_sched *sched, struct thread *thread)
@@ -1447,15 +1450,15 @@ static int sort_dimension__add(const char *tok, struct list_head *list)
 static void perf_sched__sort_lat(struct perf_sched *sched)
 {
 	struct rb_node *node;
-	struct rb_root *root = &sched->atom_root;
+	struct rb_root_cached *root = &sched->atom_root;
 again:
 	for (;;) {
 		struct work_atoms *data;
-		node = rb_first(root);
+		node = rb_first_cached(root);
 		if (!node)
 			break;
 
-		rb_erase(node, root);
+		rb_erase_cached(node, root);
 		data = rb_entry(node, struct work_atoms, node);
 		__thread_latency_insert(&sched->sorted_atom_root, data, &sched->sort_list);
 	}
@@ -2762,12 +2765,12 @@ static size_t callchain__fprintf_folded(FILE *fp, struct callchain_node *node)
 	return ret;
 }
 
-static size_t timehist_print_idlehist_callchain(struct rb_root *root)
+static size_t timehist_print_idlehist_callchain(struct rb_root_cached *root)
 {
 	size_t ret = 0;
 	FILE *fp = stdout;
 	struct callchain_node *chain;
-	struct rb_node *rb_node = rb_first(root);
+	struct rb_node *rb_node = rb_first_cached(root);
 
 	printf("  %16s  %8s  %s\n", "Idle time (msec)", "Count", "Callchains");
 	printf("  %.16s  %.8s  %.50s\n", graph_dotted_line, graph_dotted_line,
@@ -2868,7 +2871,7 @@ static void timehist_print_summary(struct perf_sched *sched,
 			if (itr == NULL)
 				continue;
 
-			callchain_param.sort(&itr->sorted_root, &itr->callchain,
+			callchain_param.sort(&itr->sorted_root.rb_root, &itr->callchain,
 					     0, &callchain_param);
 
 			printf("  CPU %2d:", i);
@@ -3074,11 +3077,12 @@ static void print_bad_events(struct perf_sched *sched)
 	}
 }
 
-static void __merge_work_atoms(struct rb_root *root, struct work_atoms *data)
+static void __merge_work_atoms(struct rb_root_cached *root, struct work_atoms *data)
 {
-	struct rb_node **new = &(root->rb_node), *parent = NULL;
+	struct rb_node **new = &(root->rb_root.rb_node), *parent = NULL;
 	struct work_atoms *this;
 	const char *comm = thread__comm_str(data->thread), *this_comm;
+	bool leftmost = true;
 
 	while (*new) {
 		int cmp;
@@ -3092,6 +3096,7 @@ static void __merge_work_atoms(struct rb_root *root, struct work_atoms *data)
 			new = &((*new)->rb_left);
 		} else if (cmp < 0) {
 			new = &((*new)->rb_right);
+			leftmost = false;
 		} else {
 			this->num_merged++;
 			this->total_runtime += data->total_runtime;
@@ -3109,7 +3114,7 @@ static void __merge_work_atoms(struct rb_root *root, struct work_atoms *data)
 
 	data->num_merged++;
 	rb_link_node(&data->node, parent, new);
-	rb_insert_color(&data->node, root);
+	rb_insert_color_cached(&data->node, root, leftmost);
 }
 
 static void perf_sched__merge_lat(struct perf_sched *sched)
@@ -3120,8 +3125,8 @@ static void perf_sched__merge_lat(struct perf_sched *sched)
 	if (sched->skip_merge)
 		return;
 
-	while ((node = rb_first(&sched->atom_root))) {
-		rb_erase(node, &sched->atom_root);
+	while ((node = rb_first_cached(&sched->atom_root))) {
+		rb_erase_cached(node, &sched->atom_root);
 		data = rb_entry(node, struct work_atoms, node);
 		__merge_work_atoms(&sched->merged_atom_root, data);
 	}
@@ -3143,7 +3148,7 @@ static int perf_sched__lat(struct perf_sched *sched)
 	printf("  Task                  |   Runtime ms  | Switches | Average delay ms | Maximum delay ms | Maximum delay at       |\n");
 	printf(" -----------------------------------------------------------------------------------------------------------------\n");
 
-	next = rb_first(&sched->sorted_atom_root);
+	next = rb_first_cached(&sched->sorted_atom_root);
 
 	while (next) {
 		struct work_atoms *work_list;

commit 49b8e2beceda79ae25938faf68fad0ad62534852
Author: Rasmus Villemoes <linux@rasmusvillemoes.dk>
Date:   Sat Nov 3 00:06:23 2018 +0100

    perf tools: Replace automatic const char[] variables by statics
    
    An automatic const char[] variable gets initialized at runtime, just
    like any other automatic variable. For long strings, that uses a lot of
    stack and wastes time building the string; e.g. for the "No %s
    allocation events..." case one has:
    
      444516:       48 b8 4e 6f 20 25 73 20 61 6c   movabs $0x6c61207325206f4e,%rax # "No %s al"
      ...
      444674:       48 89 45 80                     mov    %rax,-0x80(%rbp)
      444678:       48 b8 6c 6f 63 61 74 69 6f 6e   movabs $0x6e6f697461636f6c,%rax # "location"
      444682:       48 89 45 88                     mov    %rax,-0x78(%rbp)
      444686:       48 b8 20 65 76 65 6e 74 73 20   movabs $0x2073746e65766520,%rax # " events "
      444690:       66 44 89 55 c4                  mov    %r10w,-0x3c(%rbp)
      444695:       48 89 45 90                     mov    %rax,-0x70(%rbp)
      444699:       48 b8 66 6f 75 6e 64 2e 20 20   movabs $0x20202e646e756f66,%rax
    
    Make them all static so that the compiler just references objects in .rodata.
    
    Committer testing:
    
    Ok, using dwarves's codiff tool:
    
        $ codiff --functions /tmp/perf.before ~/bin/perf
      builtin-sched.c:
        cmd_sched                 |  -48
       1 function changed, 48 bytes removed, diff: -48
    
      builtin-report.c:
        cmd_report                |  -32
       1 function changed, 32 bytes removed, diff: -32
    
      builtin-kmem.c:
        cmd_kmem                  |  -64
        build_alloc_func_list     |  -50
       2 functions changed, 114 bytes removed, diff: -114
    
      builtin-c2c.c:
        perf_c2c__report          | -390
       1 function changed, 390 bytes removed, diff: -390
    
      ui/browsers/header.c:
        tui__header_window        | -104
       1 function changed, 104 bytes removed, diff: -104
    
      /home/acme/bin/perf:
       9 functions changed, 688 bytes removed, diff: -688
    
    Signed-off-by: Rasmus Villemoes <linux@rasmusvillemoes.dk>
    Acked-by: Jiri Olsa <jolsa@kernel.org>
    Tested-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20181102230624.20064-1-linux@rasmusvillemoes.dk
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index cbf39dab19c1..2e0f0c65964a 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -3336,7 +3336,7 @@ static int __cmd_record(int argc, const char **argv)
 
 int cmd_sched(int argc, const char **argv)
 {
-	const char default_sort_order[] = "avg, max, switch, runtime";
+	static const char default_sort_order[] = "avg, max, switch, runtime";
 	struct perf_sched sched = {
 		.tool = {
 			.sample		 = perf_sched__process_tracepoint_sample,

commit 4c50563d818622bb0f99b378b4e6683f189b39fb
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Mon May 28 16:07:56 2018 -0300

    perf sched: Use sched->show_callchain where appropriate
    
    Instead of using symbol_conf.use_callchain, reducing its usage a bit
    more.
    
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Wang Nan <wangnan0@huawei.com>
    Link: https://lkml.kernel.org/n/tip-edgwb1b2mpbrdeg0w64wp7ms@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 97f9e755e8e6..cbf39dab19c1 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -2143,7 +2143,7 @@ static void save_task_callchain(struct perf_sched *sched,
 		return;
 	}
 
-	if (!symbol_conf.use_callchain || sample->callchain == NULL)
+	if (!sched->show_callchain || sample->callchain == NULL)
 		return;
 
 	if (thread__resolve_callchain(thread, cursor, evsel, sample,
@@ -2271,10 +2271,11 @@ static struct thread *get_idle_thread(int cpu)
 	return idle_threads[cpu];
 }
 
-static void save_idle_callchain(struct idle_thread_runtime *itr,
+static void save_idle_callchain(struct perf_sched *sched,
+				struct idle_thread_runtime *itr,
 				struct perf_sample *sample)
 {
-	if (!symbol_conf.use_callchain || sample->callchain == NULL)
+	if (!sched->show_callchain || sample->callchain == NULL)
 		return;
 
 	callchain_cursor__copy(&itr->cursor, &callchain_cursor);
@@ -2320,7 +2321,7 @@ static struct thread *timehist_get_thread(struct perf_sched *sched,
 
 			/* copy task callchain when entering to idle */
 			if (perf_evsel__intval(evsel, sample, "next_pid") == 0)
-				save_idle_callchain(itr, sample);
+				save_idle_callchain(sched, itr, sample);
 		}
 	}
 
@@ -2849,7 +2850,7 @@ static void timehist_print_summary(struct perf_sched *sched,
 			printf("    CPU %2d idle entire time window\n", i);
 	}
 
-	if (sched->idle_hist && symbol_conf.use_callchain) {
+	if (sched->idle_hist && sched->show_callchain) {
 		callchain_param.mode  = CHAIN_FOLDED;
 		callchain_param.value = CCVAL_PERIOD;
 

commit 27de9b2bd996de0ca4079c42c81c85158e10145c
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Mon May 28 16:00:29 2018 -0300

    perf evsel: Add has_callchain() helper to make code more compact/clear
    
    Its common to have the (evsel->attr.sample_type & PERF_SAMPLE_CALLCHAIN),
    so add an evsel__has_callchain(evsel) helper.
    
    This will actually get more uses as we check that instead of
    symbol_conf.use_callchain in places where that produces the same result
    but makes this decision to be more fine grained, per evsel.
    
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Wang Nan <wangnan0@huawei.com>
    Link: https://lkml.kernel.org/n/tip-145340oytbthatpfeaq1do18@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 4dfdee668b0c..97f9e755e8e6 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -2933,8 +2933,7 @@ static int timehist_check_attr(struct perf_sched *sched,
 			return -1;
 		}
 
-		if (sched->show_callchain &&
-		    !(evsel->attr.sample_type & PERF_SAMPLE_CALLCHAIN)) {
+		if (sched->show_callchain && !evsel__has_callchain(evsel)) {
 			pr_info("Samples do not have callchains.\n");
 			sched->show_callchain = 0;
 			symbol_conf.use_callchain = 0;

commit 99a3c3a91382a7e5e841a98467a8409b47b540f0
Author: Changbin Du <changbin.du@intel.com>
Date:   Tue Mar 6 11:37:37 2018 +0800

    perf sched map: Re-annotate shortname if thread comm changed
    
    This is to show the real name of thread that created via fork-exec.  See
    below example for shortname *A0*.
    
    $ sudo ./perf sched map
                  *A0   80393.050639 secs A0 => perf:22368
              *.   A0   80393.050748 secs .  => swapper:0
               .  *.    80393.050887 secs
          *B0  .   .    80393.052735 secs B0 => rcu_sched:8
          *.   .   .    80393.052743 secs
           .  *C0  .    80393.056264 secs C0 => kworker/2:1H:287
           .  *A0  .    80393.056270 secs
           .  *D0  .    80393.056769 secs D0 => ksoftirqd/2:22
    -      .  *A0  .    80393.056804 secs
    +      .  *A0  .    80393.056804 secs A0 => pi:22368
           .  *.   .    80393.056854 secs
          *B0  .   .    80393.060727 secs
          ...
    
    Signed-off-by: Changbin Du <changbin.du@intel.com>
    Acked-by: Jiri Olsa <jolsa@kernel.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1520307457-23668-3-git-send-email-changbin.du@intel.com
    [ Optimally pack struct thread_runtime when adding the new bool member ]
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 0a632a6b6228..4dfdee668b0c 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -256,6 +256,8 @@ struct thread_runtime {
 	int last_state;
 
 	char shortname[3];
+	bool comm_changed;
+
 	u64 migrations;
 };
 
@@ -1626,7 +1628,7 @@ static int map_switch_event(struct perf_sched *sched, struct perf_evsel *evsel,
 
 	timestamp__scnprintf_usec(timestamp, stimestamp, sizeof(stimestamp));
 	color_fprintf(stdout, color, "  %12s secs ", stimestamp);
-	if (new_shortname || (verbose > 0 && sched_in->tid)) {
+	if (new_shortname || tr->comm_changed || (verbose > 0 && sched_in->tid)) {
 		const char *pid_color = color;
 
 		if (thread__has_color(sched_in))
@@ -1634,6 +1636,7 @@ static int map_switch_event(struct perf_sched *sched, struct perf_evsel *evsel,
 
 		color_fprintf(stdout, pid_color, "%s => %s:%d",
 		       tr->shortname, thread__comm_str(sched_in), sched_in->tid);
+		tr->comm_changed = false;
 	}
 
 	if (sched->map.comp && new_cpu)
@@ -1737,6 +1740,37 @@ static int perf_sched__process_tracepoint_sample(struct perf_tool *tool __maybe_
 	return err;
 }
 
+static int perf_sched__process_comm(struct perf_tool *tool __maybe_unused,
+				    union perf_event *event,
+				    struct perf_sample *sample,
+				    struct machine *machine)
+{
+	struct thread *thread;
+	struct thread_runtime *tr;
+	int err;
+
+	err = perf_event__process_comm(tool, event, sample, machine);
+	if (err)
+		return err;
+
+	thread = machine__find_thread(machine, sample->pid, sample->tid);
+	if (!thread) {
+		pr_err("Internal error: can't find thread\n");
+		return -1;
+	}
+
+	tr = thread__get_runtime(thread);
+	if (tr == NULL) {
+		thread__put(thread);
+		return -1;
+	}
+
+	tr->comm_changed = true;
+	thread__put(thread);
+
+	return 0;
+}
+
 static int perf_sched__read_events(struct perf_sched *sched)
 {
 	const struct perf_evsel_str_handler handlers[] = {
@@ -3306,7 +3340,7 @@ int cmd_sched(int argc, const char **argv)
 	struct perf_sched sched = {
 		.tool = {
 			.sample		 = perf_sched__process_tracepoint_sample,
-			.comm		 = perf_event__process_comm,
+			.comm		 = perf_sched__process_comm,
 			.namespaces	 = perf_event__process_namespaces,
 			.lost		 = perf_event__process_lost,
 			.fork		 = perf_sched__process_fork_event,

commit 8640da9f4fea88c8fbb44ff63fde4000203cb7d1
Author: Changbin Du <changbin.du@intel.com>
Date:   Tue Mar 6 11:37:36 2018 +0800

    perf sched: Move thread::shortname to thread_runtime
    
    The thread::shortname only used by sched command, so move it to sched
    private structure.
    
    Signed-off-by: Changbin Du <changbin.du@intel.com>
    Acked-by: Jiri Olsa <jolsa@kernel.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1520307457-23668-2-git-send-email-changbin.du@intel.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 83283fedb00f..0a632a6b6228 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -254,6 +254,8 @@ struct thread_runtime {
 	u64 total_delay_time;
 
 	int last_state;
+
+	char shortname[3];
 	u64 migrations;
 };
 
@@ -897,6 +899,37 @@ struct sort_dimension {
 	struct list_head	list;
 };
 
+/*
+ * handle runtime stats saved per thread
+ */
+static struct thread_runtime *thread__init_runtime(struct thread *thread)
+{
+	struct thread_runtime *r;
+
+	r = zalloc(sizeof(struct thread_runtime));
+	if (!r)
+		return NULL;
+
+	init_stats(&r->run_stats);
+	thread__set_priv(thread, r);
+
+	return r;
+}
+
+static struct thread_runtime *thread__get_runtime(struct thread *thread)
+{
+	struct thread_runtime *tr;
+
+	tr = thread__priv(thread);
+	if (tr == NULL) {
+		tr = thread__init_runtime(thread);
+		if (tr == NULL)
+			pr_debug("Failed to malloc memory for runtime data.\n");
+	}
+
+	return tr;
+}
+
 static int
 thread_lat_cmp(struct list_head *list, struct work_atoms *l, struct work_atoms *r)
 {
@@ -1480,6 +1513,7 @@ static int map_switch_event(struct perf_sched *sched, struct perf_evsel *evsel,
 {
 	const u32 next_pid = perf_evsel__intval(evsel, sample, "next_pid");
 	struct thread *sched_in;
+	struct thread_runtime *tr;
 	int new_shortname;
 	u64 timestamp0, timestamp = sample->time;
 	s64 delta;
@@ -1519,22 +1553,28 @@ static int map_switch_event(struct perf_sched *sched, struct perf_evsel *evsel,
 	if (sched_in == NULL)
 		return -1;
 
+	tr = thread__get_runtime(sched_in);
+	if (tr == NULL) {
+		thread__put(sched_in);
+		return -1;
+	}
+
 	sched->curr_thread[this_cpu] = thread__get(sched_in);
 
 	printf("  ");
 
 	new_shortname = 0;
-	if (!sched_in->shortname[0]) {
+	if (!tr->shortname[0]) {
 		if (!strcmp(thread__comm_str(sched_in), "swapper")) {
 			/*
 			 * Don't allocate a letter-number for swapper:0
 			 * as a shortname. Instead, we use '.' for it.
 			 */
-			sched_in->shortname[0] = '.';
-			sched_in->shortname[1] = ' ';
+			tr->shortname[0] = '.';
+			tr->shortname[1] = ' ';
 		} else {
-			sched_in->shortname[0] = sched->next_shortname1;
-			sched_in->shortname[1] = sched->next_shortname2;
+			tr->shortname[0] = sched->next_shortname1;
+			tr->shortname[1] = sched->next_shortname2;
 
 			if (sched->next_shortname1 < 'Z') {
 				sched->next_shortname1++;
@@ -1552,6 +1592,7 @@ static int map_switch_event(struct perf_sched *sched, struct perf_evsel *evsel,
 	for (i = 0; i < cpus_nr; i++) {
 		int cpu = sched->map.comp ? sched->map.comp_cpus[i] : i;
 		struct thread *curr_thread = sched->curr_thread[cpu];
+		struct thread_runtime *curr_tr;
 		const char *pid_color = color;
 		const char *cpu_color = color;
 
@@ -1569,9 +1610,14 @@ static int map_switch_event(struct perf_sched *sched, struct perf_evsel *evsel,
 		else
 			color_fprintf(stdout, cpu_color, "*");
 
-		if (sched->curr_thread[cpu])
-			color_fprintf(stdout, pid_color, "%2s ", sched->curr_thread[cpu]->shortname);
-		else
+		if (sched->curr_thread[cpu]) {
+			curr_tr = thread__get_runtime(sched->curr_thread[cpu]);
+			if (curr_tr == NULL) {
+				thread__put(sched_in);
+				return -1;
+			}
+			color_fprintf(stdout, pid_color, "%2s ", curr_tr->shortname);
+		} else
 			color_fprintf(stdout, color, "   ");
 	}
 
@@ -1587,7 +1633,7 @@ static int map_switch_event(struct perf_sched *sched, struct perf_evsel *evsel,
 			pid_color = COLOR_PIDS;
 
 		color_fprintf(stdout, pid_color, "%s => %s:%d",
-		       sched_in->shortname, thread__comm_str(sched_in), sched_in->tid);
+		       tr->shortname, thread__comm_str(sched_in), sched_in->tid);
 	}
 
 	if (sched->map.comp && new_cpu)
@@ -2200,37 +2246,6 @@ static void save_idle_callchain(struct idle_thread_runtime *itr,
 	callchain_cursor__copy(&itr->cursor, &callchain_cursor);
 }
 
-/*
- * handle runtime stats saved per thread
- */
-static struct thread_runtime *thread__init_runtime(struct thread *thread)
-{
-	struct thread_runtime *r;
-
-	r = zalloc(sizeof(struct thread_runtime));
-	if (!r)
-		return NULL;
-
-	init_stats(&r->run_stats);
-	thread__set_priv(thread, r);
-
-	return r;
-}
-
-static struct thread_runtime *thread__get_runtime(struct thread *thread)
-{
-	struct thread_runtime *tr;
-
-	tr = thread__priv(thread);
-	if (tr == NULL) {
-		tr = thread__init_runtime(thread);
-		if (tr == NULL)
-			pr_debug("Failed to malloc memory for runtime data.\n");
-	}
-
-	return tr;
-}
-
 static struct thread *timehist_get_thread(struct perf_sched *sched,
 					  struct perf_sample *sample,
 					  struct machine *machine,

commit 15bcdc9477b03eb035052412c3a087e11e855e76
Merge: 340b5319c98e e4880bc5dfb1
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Nov 7 10:30:18 2017 +0100

    Merge branch 'linus' into perf/core, to fix conflicts
    
    Conflicts:
            tools/perf/arch/arm/annotate/instructions.c
            tools/perf/arch/arm64/annotate/instructions.c
            tools/perf/arch/powerpc/annotate/instructions.c
            tools/perf/arch/s390/annotate/instructions.c
            tools/perf/arch/x86/tests/intel-cqm.c
            tools/perf/ui/tui/progress.c
            tools/perf/util/zlib.c
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 322b4def8411..f380d91ee609 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 #include "builtin.h"
 #include "perf.h"
 

commit eae8ad8042d82775da1ddf3faa915b32854d9cf4
Author: Jiri Olsa <jolsa@kernel.org>
Date:   Mon Jan 23 22:25:41 2017 +0100

    perf tools: Add struct perf_data_file
    
    Add struct perf_data_file to represent a single file within a perf_data
    struct.
    
    Signed-off-by: Jiri Olsa <jolsa@kernel.org>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Changbin Du <changbin.du@intel.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Jin Yao <yao.jin@linux.intel.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Wang Nan <wangnan0@huawei.com>
    Link: http://lkml.kernel.org/n/tip-c3f9p4xzykr845ktqcek6p4t@git.kernel.org
    [ Fixup recent changes in 'perf script --per-event-dump' ]
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index cb5410511f69..47e54348b5ed 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1701,9 +1701,11 @@ static int perf_sched__read_events(struct perf_sched *sched)
 	};
 	struct perf_session *session;
 	struct perf_data data = {
-		.path = input_name,
-		.mode = PERF_DATA_MODE_READ,
-		.force = sched->force,
+		.file      = {
+			.path = input_name,
+		},
+		.mode      = PERF_DATA_MODE_READ,
+		.force     = sched->force,
 	};
 	int rc = -1;
 
@@ -2903,9 +2905,11 @@ static int perf_sched__timehist(struct perf_sched *sched)
 		{ "sched:sched_migrate_task", timehist_migrate_task_event, },
 	};
 	struct perf_data data = {
-		.path = input_name,
-		.mode = PERF_DATA_MODE_READ,
-		.force = sched->force,
+		.file      = {
+			.path = input_name,
+		},
+		.mode      = PERF_DATA_MODE_READ,
+		.force     = sched->force,
 	};
 
 	struct perf_session *session;

commit 8ceb41d7e305f186543c58178d2e1fe34f708948
Author: Jiri Olsa <jolsa@kernel.org>
Date:   Mon Jan 23 22:07:59 2017 +0100

    perf tools: Rename struct perf_data_file to perf_data
    
    Rename struct perf_data_file to perf_data, because we will add the
    possibility to have multiple files under perf.data, so the 'perf_data'
    name fits better.
    
    Signed-off-by: Jiri Olsa <jolsa@kernel.org>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Changbin Du <changbin.du@intel.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Jin Yao <yao.jin@linux.intel.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Wang Nan <wangnan0@huawei.com>
    Link: http://lkml.kernel.org/n/tip-39wn4d77phel3dgkzo3lyan0@git.kernel.org
    [ Fixup recent changes in 'perf script --per-event-dump' ]
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index b7e8812ee80c..cb5410511f69 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1700,14 +1700,14 @@ static int perf_sched__read_events(struct perf_sched *sched)
 		{ "sched:sched_migrate_task", process_sched_migrate_task_event, },
 	};
 	struct perf_session *session;
-	struct perf_data_file file = {
+	struct perf_data data = {
 		.path = input_name,
 		.mode = PERF_DATA_MODE_READ,
 		.force = sched->force,
 	};
 	int rc = -1;
 
-	session = perf_session__new(&file, false, &sched->tool);
+	session = perf_session__new(&data, false, &sched->tool);
 	if (session == NULL) {
 		pr_debug("No Memory for session\n");
 		return -1;
@@ -2902,7 +2902,7 @@ static int perf_sched__timehist(struct perf_sched *sched)
 	const struct perf_evsel_str_handler migrate_handlers[] = {
 		{ "sched:sched_migrate_task", timehist_migrate_task_event, },
 	};
-	struct perf_data_file file = {
+	struct perf_data data = {
 		.path = input_name,
 		.mode = PERF_DATA_MODE_READ,
 		.force = sched->force,
@@ -2930,7 +2930,7 @@ static int perf_sched__timehist(struct perf_sched *sched)
 
 	symbol_conf.use_callchain = sched->show_callchain;
 
-	session = perf_session__new(&file, false, &sched->tool);
+	session = perf_session__new(&data, false, &sched->tool);
 	if (session == NULL)
 		return -ENOMEM;
 

commit 0f59d7a352c11712de0f226b46cb82775b4fcece
Author: David Ahern <dsahern@gmail.com>
Date:   Fri Sep 1 10:49:12 2017 -0700

    perf sched timehist: Add pid and tid options
    
    Add options to only show event for specific pid(s) and tid(s).
    
    Signed-off-by: David Ahern <dsahern@gmail.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Link: http://lkml.kernel.org/r/1504288152-19690-1-git-send-email-dsahern@gmail.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 322b4def8411..b7e8812ee80c 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -3363,6 +3363,10 @@ int cmd_sched(int argc, const char **argv)
 	OPT_STRING(0, "time", &sched.time_str, "str",
 		   "Time span for analysis (start,stop)"),
 	OPT_BOOLEAN(0, "state", &sched.show_state, "Show task state when sched-out"),
+	OPT_STRING('p', "pid", &symbol_conf.pid_list_str, "pid[,pid...]",
+		   "analyze events only for given process id(s)"),
+	OPT_STRING('t', "tid", &symbol_conf.tid_list_str, "tid[,tid...]",
+		   "analyze events only for given thread id(s)"),
 	OPT_PARENT(sched_options)
 	};
 

commit 62d94b00f80b0ecb7fa9eea0539c59e9f82b0fcd
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Tue Jun 27 11:22:31 2017 -0300

    perf tools: Replace error() with pr_err()
    
    To consolidate the error reporting facility.
    
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Wang Nan <wangnan0@huawei.com>
    Link: http://lkml.kernel.org/n/tip-b41iot1094katoffdf19w9zk@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 39996c53995a..322b4def8411 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -2066,7 +2066,7 @@ static void save_task_callchain(struct perf_sched *sched,
 	if (thread__resolve_callchain(thread, cursor, evsel, sample,
 				      NULL, NULL, sched->max_stack + 2) != 0) {
 		if (verbose > 0)
-			error("Failed to resolve callchain. Skipping\n");
+			pr_err("Failed to resolve callchain. Skipping\n");
 
 		return;
 	}

commit a43783aeec5fac8ef372ff8c0a5bbb3056fc0604
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Tue Apr 18 10:46:11 2017 -0300

    perf tools: Include errno.h where needed
    
    Removing it from util.h, part of an effort to disentangle the includes
    hell, that makes changes to util.h or something included by it to cause
    a complete rebuild of the tools.
    
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Wang Nan <wangnan0@huawei.com>
    Link: http://lkml.kernel.org/n/tip-ztrjy52q1rqcchuy3rubfgt2@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index aefab93d7d2f..39996c53995a 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -28,6 +28,7 @@
 #include <sys/resource.h>
 #include <inttypes.h>
 
+#include <errno.h>
 #include <semaphore.h>
 #include <pthread.h>
 #include <math.h>

commit 3d689ed6099a1a11c38bb78aff7498e78e287e0b
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Mon Apr 17 16:10:49 2017 -0300

    perf tools: Move sane ctype stuff from util.h to sane_ctype.h
    
    More stuff that came from git, out of the hodge-podge that is util.h
    
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Wang Nan <wangnan0@huawei.com>
    Link: http://lkml.kernel.org/n/tip-e3lana4gctz3ub4hn4y29hkw@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index d4677fb7f7f5..aefab93d7d2f 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -34,6 +34,8 @@
 #include <api/fs/fs.h>
 #include <linux/time64.h>
 
+#include "sane_ctype.h"
+
 #define PR_SET_NAME		15               /* Set process name */
 #define MAX_CPUS		4096
 #define COMM_LEN		20

commit fd20e8111cc0e51ce12fb8ee17c863088fe95065
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Mon Apr 17 15:23:08 2017 -0300

    perf tools: Including missing inttypes.h header
    
    Needed to use the PRI[xu](32,64) formatting macros.
    
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Wang Nan <wangnan0@huawei.com>
    Link: http://lkml.kernel.org/n/tip-wkbho8kaw24q67dd11q0j39f@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 5cd60882c8d9..d4677fb7f7f5 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -26,6 +26,7 @@
 #include <linux/log2.h>
 #include <sys/prctl.h>
 #include <sys/resource.h>
+#include <inttypes.h>
 
 #include <semaphore.h>
 #include <pthread.h>

commit 877a7a11050ee4d465364c57f8fbf78f6b1a2559
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Mon Apr 17 11:39:06 2017 -0300

    perf tools: Add include <linux/kernel.h> where ARRAY_SIZE() is used
    
    To pave the way for further cleanups where linux/kernel.h may stop being
    included in some header.
    
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Wang Nan <wangnan0@huawei.com>
    Link: http://lkml.kernel.org/n/tip-qqxan6tfsl6qx3l0v3nwgjvk@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 79833e226789..5cd60882c8d9 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -22,6 +22,7 @@
 
 #include "util/debug.h"
 
+#include <linux/kernel.h>
 #include <linux/log2.h>
 #include <sys/prctl.h>
 #include <sys/resource.h>

commit b0ad8ea66445d64a469df0c710947f4cdb8ef16b
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Mon Mar 27 11:47:20 2017 -0300

    perf tools: Remove unused 'prefix' from builtin functions
    
    We got it from the git sources but never used it for anything, with the
    place where this would be somehow used remaining:
    
      static int run_builtin(struct cmd_struct *p, int argc, const char **argv)
      {
            prefix = NULL;
            if (p->option & RUN_SETUP)
                    prefix = NULL; /* setup_perf_directory(); */
    
    Ditch it.
    
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Wang Nan <wangnan0@huawei.com>
    Link: http://lkml.kernel.org/n/tip-uw5swz05vol0qpr32c5lpvus@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index b92c4d97192c..79833e226789 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -3272,10 +3272,10 @@ static int __cmd_record(int argc, const char **argv)
 
 	BUG_ON(i != rec_argc);
 
-	return cmd_record(i, rec_argv, NULL);
+	return cmd_record(i, rec_argv);
 }
 
-int cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)
+int cmd_sched(int argc, const char **argv)
 {
 	const char default_sort_order[] = "avg, max, switch, runtime";
 	struct perf_sched sched = {
@@ -3412,7 +3412,7 @@ int cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)
 	 * Aliased to 'perf script' for now:
 	 */
 	if (!strcmp(argv[0], "script"))
-		return cmd_script(argc, argv, prefix);
+		return cmd_script(argc, argv);
 
 	if (!strncmp(argv[0], "rec", 3)) {
 		return __cmd_record(argc, argv);

commit 292c4a8f985b35b3738d5900fe256c4fed4cd3f5
Author: Brendan Gregg <bgregg@netflix.com>
Date:   Tue Mar 14 01:56:29 2017 +0000

    perf sched timehist: Add --next option
    
    The --next option shows the next task for each context switch, providing
    more context for the sequence of scheduler events.
    
      $ perf sched timehist --next | head
      Samples do not have callchains.
           time  cpu task name  waittime schdelay run time
                     [tid/pid]     (msec) (msec) (msec)
      ---------- --- ---------- --------- ------ -----
      374.793792 [0] <idle>         0.000  0.000 0.000 next: rngd[1524]
      374.793801 [0] rngd[1524]     0.000  0.000 0.009 next: swapper/0[0]
      374.794048 [7] <idle>         0.000  0.000 0.000 next: yes[30884]
      374.794066 [7] yes[30884]     0.000  0.000 0.018 next: swapper/7[0]
      374.794126 [2] <idle>         0.000  0.000 0.000 next: rngd[1524]
      374.794140 [2] rngd[1524]     0.325  0.006 0.013 next: swapper/2[0]
      374.794281 [3] <idle>         0.000  0.000 0.000 next: perf[31070]
    
    Signed-off-by: Brendan Gregg <bgregg@netflix.com>
    Tested-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1489456589-32555-1-git-send-email-bgregg@netflix.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 16170e9b47e6..b92c4d97192c 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -221,6 +221,7 @@ struct perf_sched {
 	unsigned int	max_stack;
 	bool		show_cpu_visual;
 	bool		show_wakeups;
+	bool		show_next;
 	bool		show_migrations;
 	bool		show_state;
 	u64		skipped_samples;
@@ -1897,14 +1898,18 @@ static char task_state_char(struct thread *thread, int state)
 }
 
 static void timehist_print_sample(struct perf_sched *sched,
+				  struct perf_evsel *evsel,
 				  struct perf_sample *sample,
 				  struct addr_location *al,
 				  struct thread *thread,
 				  u64 t, int state)
 {
 	struct thread_runtime *tr = thread__priv(thread);
+	const char *next_comm = perf_evsel__strval(evsel, sample, "next_comm");
+	const u32 next_pid = perf_evsel__intval(evsel, sample, "next_pid");
 	u32 max_cpus = sched->max_cpu + 1;
 	char tstr[64];
+	char nstr[30];
 	u64 wait_time;
 
 	timestamp__scnprintf_usec(t, tstr, sizeof(tstr));
@@ -1937,7 +1942,12 @@ static void timehist_print_sample(struct perf_sched *sched,
 	if (sched->show_state)
 		printf(" %5c ", task_state_char(thread, state));
 
-	if (sched->show_wakeups)
+	if (sched->show_next) {
+		snprintf(nstr, sizeof(nstr), "next: %s[%d]", next_comm, next_pid);
+		printf(" %-*s", comm_width, nstr);
+	}
+
+	if (sched->show_wakeups && !sched->show_next)
 		printf("  %-*s", comm_width, "");
 
 	if (thread->tid == 0)
@@ -2531,7 +2541,7 @@ static int timehist_sched_change_event(struct perf_tool *tool,
 	}
 
 	if (!sched->summary_only)
-		timehist_print_sample(sched, sample, &al, thread, t, state);
+		timehist_print_sample(sched, evsel, sample, &al, thread, t, state);
 
 out:
 	if (sched->hist_time.start == 0 && t >= ptime->start)
@@ -3341,6 +3351,7 @@ int cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)
 	OPT_BOOLEAN('S', "with-summary", &sched.summary,
 		    "Show all syscalls and summary with statistics"),
 	OPT_BOOLEAN('w', "wakeups", &sched.show_wakeups, "Show wakeup events"),
+	OPT_BOOLEAN('n', "next", &sched.show_next, "Show next task"),
 	OPT_BOOLEAN('M', "migrations", &sched.show_migrations, "Show migration events"),
 	OPT_BOOLEAN('V', "cpu-visual", &sched.show_cpu_visual, "Add CPU visual"),
 	OPT_BOOLEAN('I', "idle-hist", &sched.idle_hist, "Show idle events only"),
@@ -3438,10 +3449,14 @@ int cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)
 			if (argc)
 				usage_with_options(timehist_usage, timehist_options);
 		}
-		if (sched.show_wakeups && sched.summary_only) {
-			pr_err(" Error: -s and -w are mutually exclusive.\n");
+		if ((sched.show_wakeups || sched.show_next) &&
+		    sched.summary_only) {
+			pr_err(" Error: -s and -[n|w] are mutually exclusive.\n");
 			parse_options_usage(timehist_usage, timehist_options, "s", true);
-			parse_options_usage(NULL, timehist_options, "w", true);
+			if (sched.show_wakeups)
+				parse_options_usage(NULL, timehist_options, "w", true);
+			if (sched.show_next)
+				parse_options_usage(NULL, timehist_options, "n", true);
 			return -EINVAL;
 		}
 

commit f3b3614a284deb124018155a618a7b19694c8b5c
Author: Hari Bathini <hbathini@linux.vnet.ibm.com>
Date:   Wed Mar 8 02:11:43 2017 +0530

    perf tools: Add PERF_RECORD_NAMESPACES to include namespaces related info
    
    Introduce a new option to record PERF_RECORD_NAMESPACES events emitted
    by the kernel when fork, clone, setns or unshare are invoked. And update
    perf-record documentation with the new option to record namespace
    events.
    
    Committer notes:
    
    Combined it with a later patch to allow printing it via 'perf report -D'
    and be able to test the feature introduced in this patch. Had to move
    here also perf_ns__name(), that was introduced in another later patch.
    
    Also used PRIu64 and PRIx64 to fix the build in some enfironments wrt:
    
      util/event.c:1129:39: error: format '%lx' expects argument of type 'long unsigned int', but argument 6 has type 'long long unsigned int' [-Werror=format=]
         ret  += fprintf(fp, "%u/%s: %lu/0x%lx%s", idx
                                             ^
    Testing it:
    
      # perf record --namespaces -a
      ^C[ perf record: Woken up 1 times to write data ]
      [ perf record: Captured and wrote 1.083 MB perf.data (423 samples) ]
      #
      # perf report -D
      <SNIP>
      3 2028902078892 0x115140 [0xa0]: PERF_RECORD_NAMESPACES 14783/14783 - nr_namespaces: 7
                    [0/net: 3/0xf0000081, 1/uts: 3/0xeffffffe, 2/ipc: 3/0xefffffff, 3/pid: 3/0xeffffffc,
                     4/user: 3/0xeffffffd, 5/mnt: 3/0xf0000000, 6/cgroup: 3/0xeffffffb]
    
      0x1151e0 [0x30]: event: 9
      .
      . ... raw event: size 48 bytes
      .  0000:  09 00 00 00 02 00 30 00 c4 71 82 68 0c 7f 00 00  ......0..q.h....
      .  0010:  a9 39 00 00 a9 39 00 00 94 28 fe 63 d8 01 00 00  .9...9...(.c....
      .  0020:  03 00 00 00 00 00 00 00 ce c4 02 00 00 00 00 00  ................
      <SNIP>
            NAMESPACES events:          1
      <SNIP>
      #
    
    Signed-off-by: Hari Bathini <hbathini@linux.vnet.ibm.com>
    Acked-by: Jiri Olsa <jolsa@kernel.org>
    Tested-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Alexei Starovoitov <ast@fb.com>
    Cc: Ananth N Mavinakayanahalli <ananth@linux.vnet.ibm.com>
    Cc: Aravinda Prasad <aravinda@linux.vnet.ibm.com>
    Cc: Brendan Gregg <brendan.d.gregg@gmail.com>
    Cc: Daniel Borkmann <daniel@iogearbox.net>
    Cc: Eric Biederman <ebiederm@xmission.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sargun Dhillon <sargun@sargun.me>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/148891930386.25309.18412039920746995488.stgit@hbathini.in.ibm.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index b94cf0de715a..16170e9b47e6 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -3272,6 +3272,7 @@ int cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)
 		.tool = {
 			.sample		 = perf_sched__process_tracepoint_sample,
 			.comm		 = perf_event__process_comm,
+			.namespaces	 = perf_event__process_namespaces,
 			.lost		 = perf_event__process_lost,
 			.fork		 = perf_sched__process_fork_event,
 			.ordered_events = true,

commit bb963e16507ca7670f0bb47ccaada8874b2ba6a1
Author: Namhyung Kim <namhyung@kernel.org>
Date:   Fri Feb 17 17:17:38 2017 +0900

    perf utils: Check verbose flag properly
    
    It now can have negative value to suppress the message entirely.  So it
    needs to check it being positive.
    
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: kernel-team@lge.com
    Link: http://lkml.kernel.org/r/20170217081742.17417-3-namhyung@kernel.org
    [ Adjust fuzz on tools/perf/util/pmu.c, add > 0 checks in many other places ]
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 270eb2d8ca6b..b94cf0de715a 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -460,7 +460,7 @@ static struct task_desc *register_pid(struct perf_sched *sched,
 	BUG_ON(!sched->tasks);
 	sched->tasks[task->nr] = task;
 
-	if (verbose)
+	if (verbose > 0)
 		printf("registered task #%ld, PID %ld (%s)\n", sched->nr_tasks, pid, comm);
 
 	return task;
@@ -794,7 +794,7 @@ replay_wakeup_event(struct perf_sched *sched,
 	const u32 pid	 = perf_evsel__intval(evsel, sample, "pid");
 	struct task_desc *waker, *wakee;
 
-	if (verbose) {
+	if (verbose > 0) {
 		printf("sched_wakeup event %p\n", evsel);
 
 		printf(" ... pid %d woke up %s/%d\n", sample->tid, comm, pid);
@@ -822,7 +822,7 @@ static int replay_switch_event(struct perf_sched *sched,
 	int cpu = sample->cpu;
 	s64 delta;
 
-	if (verbose)
+	if (verbose > 0)
 		printf("sched_switch event %p\n", evsel);
 
 	if (cpu >= MAX_CPUS || cpu < 0)
@@ -870,7 +870,7 @@ static int replay_fork_event(struct perf_sched *sched,
 		goto out_put;
 	}
 
-	if (verbose) {
+	if (verbose > 0) {
 		printf("fork event\n");
 		printf("... parent: %s/%d\n", thread__comm_str(parent), parent->tid);
 		printf("...  child: %s/%d\n", thread__comm_str(child), child->tid);
@@ -1573,7 +1573,7 @@ static int map_switch_event(struct perf_sched *sched, struct perf_evsel *evsel,
 
 	timestamp__scnprintf_usec(timestamp, stimestamp, sizeof(stimestamp));
 	color_fprintf(stdout, color, "  %12s secs ", stimestamp);
-	if (new_shortname || (verbose && sched_in->tid)) {
+	if (new_shortname || (verbose > 0 && sched_in->tid)) {
 		const char *pid_color = color;
 
 		if (thread__has_color(sched_in))
@@ -2050,7 +2050,7 @@ static void save_task_callchain(struct perf_sched *sched,
 
 	if (thread__resolve_callchain(thread, cursor, evsel, sample,
 				      NULL, NULL, sched->max_stack + 2) != 0) {
-		if (verbose)
+		if (verbose > 0)
 			error("Failed to resolve callchain. Skipping\n");
 
 		return;

commit a7c3899c06865c75f8887f33d9043f6e8e780e71
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Mon Feb 13 16:52:15 2017 -0300

    perf symbols: No need to check if sym->name is NULL
    
    As it is an array, so will always evaluate to 'true', as reported by
    clang:
    
      builtin-sched.c:2070:19: error: address of array 'sym->name' will always evaluate to 'true' [-Werror,-Wpointer-bool-conversion]
                      if (sym && sym->name) {
                              ~~ ~~~~~^~~~
      1 warning generated.
    
    So just ditch all those useless checks.
    
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Wang Nan <wangnan0@huawei.com>
    Link: http://lkml.kernel.org/n/tip-ydpm927col06paixb775jjx5@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index daceb3202200..270eb2d8ca6b 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -2067,7 +2067,7 @@ static void save_task_callchain(struct perf_sched *sched,
 			break;
 
 		sym = node->sym;
-		if (sym && sym->name) {
+		if (sym) {
 			if (!strcmp(sym->name, "schedule") ||
 			    !strcmp(sym->name, "__schedule") ||
 			    !strcmp(sym->name, "preempt_schedule"))

commit 587782c52a83b35673201fd9a54364fa2b189b33
Author: Namhyung Kim <namhyung@kernel.org>
Date:   Fri Jan 13 19:45:23 2017 +0900

    perf sched timehist: Show total wait times for summary
    
    When --state option is given, the summary will show total run, sleep,
    iowait, preempt and delay time instead of statistics of runtime.
    
      $ perf sched timehist -s --state
    
      Wait-time summary
                comm  parent sched-in run-time  sleep iowait preempt delay
                              (count)   (msec) (msec) (msec)  (msec) (msec)
      ---------------------------------------------------------------------
           systemd[1]      0        3    0.497   1.685 0.000   0.000 0.061
       ksoftirqd/0[3]      2       21    0.434 989.948 0.000   0.000 0.325
       rcu_preempt[7]      2       28    0.386 993.211 0.000   0.000 0.712
      migration/0[10]      2       12    0.126  50.174 0.000   0.000 0.044
       watchdog/0[11]      2        1    0.009   0.000 0.000   0.000 0.016
      migration/1[13]      2        2    0.029  11.755 0.000   0.000 0.007
      <SNIP>
    
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Tested-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20170113104523.31212-3-namhyung@kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index a8ac76602187..daceb3202200 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -241,6 +241,10 @@ struct thread_runtime {
 
 	struct stats run_stats;
 	u64 total_run_time;
+	u64 total_sleep_time;
+	u64 total_iowait_time;
+	u64 total_preempt_time;
+	u64 total_delay_time;
 
 	int last_state;
 	u64 migrations;
@@ -2008,7 +2012,12 @@ static void timehist_update_runtime_stats(struct thread_runtime *r,
 	}
 
 	update_stats(&r->run_stats, r->dt_run);
-	r->total_run_time += r->dt_run;
+
+	r->total_run_time     += r->dt_run;
+	r->total_delay_time   += r->dt_delay;
+	r->total_sleep_time   += r->dt_sleep;
+	r->total_iowait_time  += r->dt_iowait;
+	r->total_preempt_time += r->dt_preempt;
 }
 
 static bool is_idle_sample(struct perf_sample *sample,
@@ -2593,7 +2602,26 @@ static void print_thread_runtime(struct thread *t,
 	printf("\n");
 }
 
+static void print_thread_waittime(struct thread *t,
+				  struct thread_runtime *r)
+{
+	printf("%*s   %5d  %9" PRIu64 " ",
+	       comm_width, timehist_get_commstr(t), t->ppid,
+	       (u64) r->run_stats.n);
+
+	print_sched_time(r->total_run_time, 8);
+	print_sched_time(r->total_sleep_time, 6);
+	printf(" ");
+	print_sched_time(r->total_iowait_time, 6);
+	printf(" ");
+	print_sched_time(r->total_preempt_time, 6);
+	printf(" ");
+	print_sched_time(r->total_delay_time, 6);
+	printf("\n");
+}
+
 struct total_run_stats {
+	struct perf_sched *sched;
 	u64  sched_count;
 	u64  task_count;
 	u64  total_run_time;
@@ -2612,7 +2640,11 @@ static int __show_thread_runtime(struct thread *t, void *priv)
 		stats->task_count++;
 		stats->sched_count += r->run_stats.n;
 		stats->total_run_time += r->total_run_time;
-		print_thread_runtime(t, r);
+
+		if (stats->sched->show_state)
+			print_thread_waittime(t, r);
+		else
+			print_thread_runtime(t, r);
 	}
 
 	return 0;
@@ -2700,18 +2732,24 @@ static void timehist_print_summary(struct perf_sched *sched,
 	u64 hist_time = sched->hist_time.end - sched->hist_time.start;
 
 	memset(&totals, 0, sizeof(totals));
+	totals.sched = sched;
 
 	if (sched->idle_hist) {
 		printf("\nIdle-time summary\n");
 		printf("%*s  parent  sched-out  ", comm_width, "comm");
 		printf("  idle-time   min-idle    avg-idle    max-idle  stddev  migrations\n");
+	} else if (sched->show_state) {
+		printf("\nWait-time summary\n");
+		printf("%*s  parent   sched-in  ", comm_width, "comm");
+		printf("   run-time      sleep      iowait     preempt       delay\n");
 	} else {
 		printf("\nRuntime summary\n");
 		printf("%*s  parent   sched-in  ", comm_width, "comm");
 		printf("   run-time    min-run     avg-run     max-run  stddev  migrations\n");
 	}
 	printf("%*s            (count)  ", comm_width, "");
-	printf("     (msec)     (msec)      (msec)      (msec)       %%\n");
+	printf("     (msec)     (msec)      (msec)      (msec)       %s\n",
+	       sched->show_state ? "(msec)" : "%");
 	printf("%.117s\n", graph_dotted_line);
 
 	machine__for_each_thread(m, show_thread_runtime, &totals);

commit 414e050c68ec2a3dd815544ea48ff6016f1a7a11
Author: Namhyung Kim <namhyung@kernel.org>
Date:   Fri Jan 13 19:45:22 2017 +0900

    perf sched timehist: Add --state option
    
    The --state option is to show task state when switched out.  The state
    is printed as a single character like in the /proc but I added 'I' for
    idle state rather than 'R'.
    
      $ perf sched timehist --state | head
      Samples do not have callchains.
          time cpu task name              wait time sch delay run time state
                   [tid/pid]                 (msec)    (msec)   (msec)
      -------- --- ----------------------- -------- ------------------ -----
      1.753791 [3] <idle>                     0.000     0.000    0.000     I
      1.753834 [1] perf[27469]                0.000     0.000    0.000     S
      1.753904 [3] perf[27470]                0.000     0.006    0.112     S
      1.753914 [1] <idle>                     0.000     0.000    0.079     I
      1.753915 [3] migration/3[23]            0.000     0.002    0.011     S
      1.754287 [2] <idle>                     0.000     0.000    0.000     I
      1.754335 [2] transmission[1773/1739]    0.000     0.004    0.047     S
    
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Tested-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20170113104523.31212-2-namhyung@kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 6d3c3e84881a..a8ac76602187 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -222,6 +222,7 @@ struct perf_sched {
 	bool		show_cpu_visual;
 	bool		show_wakeups;
 	bool		show_migrations;
+	bool		show_state;
 	u64		skipped_samples;
 	const char	*time_str;
 	struct perf_time_interval ptime;
@@ -1840,6 +1841,9 @@ static void timehist_header(struct perf_sched *sched)
 	printf(" %-*s  %9s  %9s  %9s", comm_width,
 		"task name", "wait time", "sch delay", "run time");
 
+	if (sched->show_state)
+		printf("  %s", "state");
+
 	printf("\n");
 
 	/*
@@ -1850,9 +1854,14 @@ static void timehist_header(struct perf_sched *sched)
 	if (sched->show_cpu_visual)
 		printf(" %*s ", ncpus, "");
 
-	printf(" %-*s  %9s  %9s  %9s\n", comm_width,
+	printf(" %-*s  %9s  %9s  %9s", comm_width,
 	       "[tid/pid]", "(msec)", "(msec)", "(msec)");
 
+	if (sched->show_state)
+		printf("  %5s", "");
+
+	printf("\n");
+
 	/*
 	 * separator
 	 */
@@ -1865,14 +1874,29 @@ static void timehist_header(struct perf_sched *sched)
 		graph_dotted_line, graph_dotted_line, graph_dotted_line,
 		graph_dotted_line);
 
+	if (sched->show_state)
+		printf("  %.5s", graph_dotted_line);
+
 	printf("\n");
 }
 
+static char task_state_char(struct thread *thread, int state)
+{
+	static const char state_to_char[] = TASK_STATE_TO_CHAR_STR;
+	unsigned bit = state ? ffs(state) : 0;
+
+	/* 'I' for idle */
+	if (thread->tid == 0)
+		return 'I';
+
+	return bit < sizeof(state_to_char) - 1 ? state_to_char[bit] : '?';
+}
+
 static void timehist_print_sample(struct perf_sched *sched,
 				  struct perf_sample *sample,
 				  struct addr_location *al,
 				  struct thread *thread,
-				  u64 t)
+				  u64 t, int state)
 {
 	struct thread_runtime *tr = thread__priv(thread);
 	u32 max_cpus = sched->max_cpu + 1;
@@ -1906,6 +1930,9 @@ static void timehist_print_sample(struct perf_sched *sched,
 	print_sched_time(tr->dt_delay, 6);
 	print_sched_time(tr->dt_run, 6);
 
+	if (sched->show_state)
+		printf(" %5c ", task_state_char(thread, state));
+
 	if (sched->show_wakeups)
 		printf("  %-*s", comm_width, "");
 
@@ -2406,6 +2433,8 @@ static int timehist_sched_change_event(struct perf_tool *tool,
 	struct thread_runtime *tr = NULL;
 	u64 tprev, t = sample->time;
 	int rc = 0;
+	int state = perf_evsel__intval(evsel, sample, "prev_state");
+
 
 	if (machine__resolve(machine, &al, sample) < 0) {
 		pr_err("problem processing %d event. skipping it\n",
@@ -2493,7 +2522,7 @@ static int timehist_sched_change_event(struct perf_tool *tool,
 	}
 
 	if (!sched->summary_only)
-		timehist_print_sample(sched, sample, &al, thread, t);
+		timehist_print_sample(sched, sample, &al, thread, t, state);
 
 out:
 	if (sched->hist_time.start == 0 && t >= ptime->start)
@@ -2506,7 +2535,7 @@ static int timehist_sched_change_event(struct perf_tool *tool,
 		tr->last_time = sample->time;
 
 		/* last state is used to determine where to account wait time */
-		tr->last_state = perf_evsel__intval(evsel, sample, "prev_state");
+		tr->last_state = state;
 
 		/* sched out event for task so reset ready to run time */
 		tr->ready_to_run = 0;
@@ -3278,6 +3307,7 @@ int cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)
 	OPT_BOOLEAN('I', "idle-hist", &sched.idle_hist, "Show idle events only"),
 	OPT_STRING(0, "time", &sched.time_str, "str",
 		   "Time span for analysis (start,stop)"),
+	OPT_BOOLEAN(0, "state", &sched.show_state, "Show task state when sched-out"),
 	OPT_PARENT(sched_options)
 	};
 

commit 941bdea79e194dec7e7b42780aa5620020548f8a
Author: Namhyung Kim <namhyung@kernel.org>
Date:   Fri Jan 13 19:45:21 2017 +0900

    perf sched timehist: Account thread wait time separately
    
    Separate thread wait time into 3 parts - sleep, iowait and preempt based
    on the prev_state of the last event.
    
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20170113104523.31212-1-namhyung@kernel.org
    [ Fix the build on centos:5 where 'wait' shadows a global declaration ]
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 5b134b0d1ff3..6d3c3e84881a 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -77,6 +77,22 @@ struct sched_atom {
 
 #define TASK_STATE_TO_CHAR_STR "RSDTtZXxKWP"
 
+/* task state bitmask, copied from include/linux/sched.h */
+#define TASK_RUNNING		0
+#define TASK_INTERRUPTIBLE	1
+#define TASK_UNINTERRUPTIBLE	2
+#define __TASK_STOPPED		4
+#define __TASK_TRACED		8
+/* in tsk->exit_state */
+#define EXIT_DEAD		16
+#define EXIT_ZOMBIE		32
+#define EXIT_TRACE		(EXIT_ZOMBIE | EXIT_DEAD)
+/* in tsk->state again */
+#define TASK_DEAD		64
+#define TASK_WAKEKILL		128
+#define TASK_WAKING		256
+#define TASK_PARKED		512
+
 enum thread_state {
 	THREAD_SLEEPING = 0,
 	THREAD_WAIT_CPU,
@@ -216,13 +232,16 @@ struct perf_sched {
 struct thread_runtime {
 	u64 last_time;      /* time of previous sched in/out event */
 	u64 dt_run;         /* run time */
-	u64 dt_wait;        /* time between CPU access (off cpu) */
+	u64 dt_sleep;       /* time between CPU access by sleep (off cpu) */
+	u64 dt_iowait;      /* time between CPU access by iowait (off cpu) */
+	u64 dt_preempt;     /* time between CPU access by preempt (off cpu) */
 	u64 dt_delay;       /* time between wakeup and sched-in */
 	u64 ready_to_run;   /* time of wakeup */
 
 	struct stats run_stats;
 	u64 total_run_time;
 
+	int last_state;
 	u64 migrations;
 };
 
@@ -1858,6 +1877,7 @@ static void timehist_print_sample(struct perf_sched *sched,
 	struct thread_runtime *tr = thread__priv(thread);
 	u32 max_cpus = sched->max_cpu + 1;
 	char tstr[64];
+	u64 wait_time;
 
 	timestamp__scnprintf_usec(t, tstr, sizeof(tstr));
 	printf("%15s [%04d] ", tstr, sample->cpu);
@@ -1880,7 +1900,9 @@ static void timehist_print_sample(struct perf_sched *sched,
 
 	printf(" %-*s ", comm_width, timehist_get_commstr(thread));
 
-	print_sched_time(tr->dt_wait, 6);
+	wait_time = tr->dt_sleep + tr->dt_iowait + tr->dt_preempt;
+	print_sched_time(wait_time, 6);
+
 	print_sched_time(tr->dt_delay, 6);
 	print_sched_time(tr->dt_run, 6);
 
@@ -1930,8 +1952,11 @@ static void timehist_update_runtime_stats(struct thread_runtime *r,
 					 u64 t, u64 tprev)
 {
 	r->dt_delay   = 0;
-	r->dt_wait    = 0;
+	r->dt_sleep   = 0;
+	r->dt_iowait  = 0;
+	r->dt_preempt = 0;
 	r->dt_run     = 0;
+
 	if (tprev) {
 		r->dt_run = t - tprev;
 		if (r->ready_to_run) {
@@ -1943,8 +1968,16 @@ static void timehist_update_runtime_stats(struct thread_runtime *r,
 
 		if (r->last_time > tprev)
 			pr_debug("time travel: last sched out time for task > previous sched_switch event\n");
-		else if (r->last_time)
-			r->dt_wait = tprev - r->last_time;
+		else if (r->last_time) {
+			u64 dt_wait = tprev - r->last_time;
+
+			if (r->last_state == TASK_RUNNING)
+				r->dt_preempt = dt_wait;
+			else if (r->last_state == TASK_UNINTERRUPTIBLE)
+				r->dt_iowait = dt_wait;
+			else
+				r->dt_sleep = dt_wait;
+		}
 	}
 
 	update_stats(&r->run_stats, r->dt_run);
@@ -2447,8 +2480,10 @@ static int timehist_sched_change_event(struct perf_tool *tool,
 			 * time.  we only care total run time and run stat.
 			 */
 			last_tr->dt_run = 0;
-			last_tr->dt_wait = 0;
 			last_tr->dt_delay = 0;
+			last_tr->dt_sleep = 0;
+			last_tr->dt_iowait = 0;
+			last_tr->dt_preempt = 0;
 
 			if (itr->cursor.nr)
 				callchain_append(&itr->callchain, &itr->cursor, t - tprev);
@@ -2470,6 +2505,9 @@ static int timehist_sched_change_event(struct perf_tool *tool,
 		/* time of this sched_switch event becomes last time task seen */
 		tr->last_time = sample->time;
 
+		/* last state is used to determine where to account wait time */
+		tr->last_state = perf_evsel__intval(evsel, sample, "prev_state");
+
 		/* sched out event for task so reset ready to run time */
 		tr->ready_to_run = 0;
 	}

commit 9396c9cb0d9534ca67bda8a34b2413a2ca1c67e9
Author: Namhyung Kim <namhyung@kernel.org>
Date:   Thu Dec 22 15:03:50 2016 +0900

    perf sched timehist: Show total scheduling time
    
    Show length of analyzed sample time and rate of idle task running.
    This also takes care of time range given by --time option.
    
      $ perf sched timehist -sI | tail
      Samples do not have callchains.
      Idle stats:
          CPU  0 idle for    930.316  msec  ( 92.93%)
          CPU  1 idle for    963.614  msec  ( 96.25%)
          CPU  2 idle for    885.482  msec  ( 88.45%)
          CPU  3 idle for    938.635  msec  ( 93.76%)
    
          Total number of unique tasks: 118
      Total number of context switches: 2337
                 Total run time (msec): 3718.048
          Total scheduling time (msec): 1001.131  (x 4)
    
    Suggested-by: David Ahern <dsahern@gmail.com>
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20161222060350.17655-3-namhyung@kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index d53e706a6f17..5b134b0d1ff3 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -209,6 +209,7 @@ struct perf_sched {
 	u64		skipped_samples;
 	const char	*time_str;
 	struct perf_time_interval ptime;
+	struct perf_time_interval hist_time;
 };
 
 /* per thread run time data */
@@ -2460,6 +2461,11 @@ static int timehist_sched_change_event(struct perf_tool *tool,
 		timehist_print_sample(sched, sample, &al, thread, t);
 
 out:
+	if (sched->hist_time.start == 0 && t >= ptime->start)
+		sched->hist_time.start = t;
+	if (ptime->end == 0 || t <= ptime->end)
+		sched->hist_time.end = t;
+
 	if (tr) {
 		/* time of this sched_switch event becomes last time task seen */
 		tr->last_time = sample->time;
@@ -2624,6 +2630,7 @@ static void timehist_print_summary(struct perf_sched *sched,
 	struct thread *t;
 	struct thread_runtime *r;
 	int i;
+	u64 hist_time = sched->hist_time.end - sched->hist_time.start;
 
 	memset(&totals, 0, sizeof(totals));
 
@@ -2665,7 +2672,7 @@ static void timehist_print_summary(struct perf_sched *sched,
 			totals.sched_count += r->run_stats.n;
 			printf("    CPU %2d idle for ", i);
 			print_sched_time(r->total_run_time, 6);
-			printf(" msec\n");
+			printf(" msec  (%6.2f%%)\n", 100.0 * r->total_run_time / hist_time);
 		} else
 			printf("    CPU %2d idle entire time window\n", i);
 	}
@@ -2701,12 +2708,16 @@ static void timehist_print_summary(struct perf_sched *sched,
 
 	printf("\n"
 	       "    Total number of unique tasks: %" PRIu64 "\n"
-	       "Total number of context switches: %" PRIu64 "\n"
-	       "           Total run time (msec): ",
+	       "Total number of context switches: %" PRIu64 "\n",
 	       totals.task_count, totals.sched_count);
 
+	printf("           Total run time (msec): ");
 	print_sched_time(totals.total_run_time, 2);
 	printf("\n");
+
+	printf("    Total scheduling time (msec): ");
+	print_sched_time(hist_time, 2);
+	printf(" (x %d)\n", sched->max_cpu);
 }
 
 typedef int (*sched_handler)(struct perf_tool *tool,

commit bdd75729e5d279d734e8d3fb41ef4818ac1598ab
Author: Namhyung Kim <namhyung@kernel.org>
Date:   Thu Dec 22 15:03:49 2016 +0900

    perf sched timehist: Fix invalid period calculation
    
    When --time option is given with a value outside recorded time, the last
    sample time (tprev) was set to that value and run time calculation might
    be incorrect.  This is a problem of the first samples for each cpus
    since it would skip the runtime update when tprev is 0.  But with --time
    option it had non-zero (which is invalid) value so the calculation is
    also incorrect.
    
    For example, let's see the followging:
    
      $ perf sched timehist
                 time    cpu  task name                       wait time  sch delay   run time
                              [tid/pid]                          (msec)     (msec)     (msec)
      --------------- ------  ------------------------------  ---------  ---------  ---------
          3195.968367 [0003]  <idle>                              0.000      0.000      0.000
          3195.968386 [0002]  Timer[4306/4277]                    0.000      0.000      0.018
          3195.968397 [0002]  Web Content[4277]                   0.000      0.000      0.000
          3195.968595 [0001]  JS Helper[4302/4277]                0.000      0.000      0.000
          3195.969217 [0000]  <idle>                              0.000      0.000      0.621
          3195.969251 [0001]  kworker/1:1H[291]                   0.000      0.000      0.033
    
    The sample starts at 3195.968367 but when I gave a time interval from
    3194 to 3196 (in sec) it will calculate the whole 2 second as runtime.
    In below, 2 cpus accounted it as runtime, other 2 cpus accounted it as
    idle time.
    
    Before:
    
      $ perf sched timehist --time 3194,3196 -s | tail
      Idle stats:
          CPU  0 idle for   1995.991  msec
          CPU  1 idle for     20.793  msec
          CPU  2 idle for     30.191  msec
          CPU  3 idle for   1999.852  msec
    
          Total number of unique tasks: 23
      Total number of context switches: 128
                 Total run time (msec): 3724.940
    
    After:
    
      $ perf sched timehist --time 3194,3196 -s | tail
      Idle stats:
          CPU  0 idle for     10.811  msec
          CPU  1 idle for     20.793  msec
          CPU  2 idle for     30.191  msec
          CPU  3 idle for     18.337  msec
    
          Total number of unique tasks: 23
      Total number of context switches: 128
                 Total run time (msec): 18.139
    
    Committer notes:
    
    Further testing:
    
    Before:
    
      Idle stats:
          CPU  0 idle for    229.785  msec
          CPU  1 idle for    937.944  msec
          CPU  2 idle for    188.931  msec
          CPU  3 idle for    986.185  msec
    
      After:
    
      # perf sched timehist --time 40602,40603 -s | tail
    
      Idle stats:
          CPU  0 idle for    229.785  msec
          CPU  1 idle for    175.407  msec
          CPU  2 idle for    188.931  msec
          CPU  3 idle for    223.657  msec
    
          Total number of unique tasks: 68
      Total number of context switches: 814
                 Total run time (msec): 97.688
    
      # for cpu in `seq 0 3` ; do echo -n "CPU $cpu idle for " ; perf sched timehist --time 40602,40603 | grep "\[000${cpu}\].*\<idle\>" | tr -s ' ' | cut -d' ' -f7 | awk '{entries++ ; s+=$1} END {print s " msec (entries: " entries ")"}' ; done
      CPU 0 idle for 229.721 msec (entries: 123)
      CPU 1 idle for 175.381 msec (entries: 65)
      CPU 2 idle for 188.903 msec (entries: 56)
      CPU 3 idle for 223.61 msec (entries: 102)
    
    Difference due to the idle stats being accounted at nanoseconds precision while
    the <idle> entries in 'perf sched timehist' are trucated at msec.usec.
    
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Tested-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Fixes: 853b74071110 ("perf sched timehist: Add option to specify time window of interest")
    Link: http://lkml.kernel.org/r/20161222060350.17655-2-namhyung@kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 5052caa91caa..d53e706a6f17 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -2405,7 +2405,7 @@ static int timehist_sched_change_event(struct perf_tool *tool,
 	if (ptime->start && ptime->start > t)
 		goto out;
 
-	if (ptime->start > tprev)
+	if (tprev && ptime->start > tprev)
 		tprev = ptime->start;
 
 	/*

commit 4fa0d1aa2711a5053fb2422331bdf6bce99b5f87
Author: Namhyung Kim <namhyung@kernel.org>
Date:   Thu Dec 22 15:03:48 2016 +0900

    perf sched timehist: Remove hardcoded 'comm_width' check at print_summary
    
    Now that the default 'comm_width' value is 30, no need to check that at
    print_summary,
    
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20161222060350.17655-1-namhyung@kernel.org
    [ Split from a larger patch ]
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index c65f16cbae62..5052caa91caa 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -2627,9 +2627,6 @@ static void timehist_print_summary(struct perf_sched *sched,
 
 	memset(&totals, 0, sizeof(totals));
 
-	if (comm_width < 30)
-		comm_width = 30;
-
 	if (sched->idle_hist) {
 		printf("\nIdle-time summary\n");
 		printf("%*s  parent  sched-out  ", comm_width, "comm");

commit 9b8087d72086b249ff744cee237ad12cc5e9255d
Author: Namhyung Kim <namhyung@kernel.org>
Date:   Thu Dec 22 15:03:48 2016 +0900

    perf sched timehist: Enlarge default 'comm_width'
    
    Current default value is 20 but it's easily changed to a bigger value as
    task has a long name and different tid and pid.  And it makes the output
    not aligned.  So change it to have a large value as summary shows.
    
    Committer notes:
    
    Before:
    
      # perf sched record
      ^C
      # perf sched timehist
      <SNIP>
        40602.770537 [0001]  rcuos/2[29]               7.970      0.002      0.020
        40602.771512 [0003]  <idle>                    0.003      0.000      0.986
        40602.771586 [0001]  <idle>                    0.020      0.000      1.049
        40602.771606 [0001]  qemu-system-x86[3593/3510]      0.000      0.002      0.020
        40602.771629 [0003]  qemu-system-x86[3510]           0.000      0.003      0.116
        40602.771776 [0000]  <idle>                          0.001      0.000      1.892
      <SNIP>
    
    After:
    
      # perf sched timehist
      <SNIP>
       40602.770537 [0001]  rcuos/2[29]                         7.970      0.002      0.020
       40602.771512 [0003]  <idle>                              0.003      0.000      0.986
       40602.771586 [0001]  <idle>                              0.020      0.000      1.049
       40602.771606 [0001]  qemu-system-x86[3593/3510]          0.000      0.002      0.020
       40602.771629 [0003]  qemu-system-x86[3510]               0.000      0.003      0.116
      <SNIP>
    
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Tested-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20161222060350.17655-1-namhyung@kernel.org
    [ Split from a larger patch ]
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 020f9c9cef7b..c65f16cbae62 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1775,7 +1775,7 @@ static u64 perf_evsel__get_time(struct perf_evsel *evsel, u32 cpu)
 	return r->last_time[cpu];
 }
 
-static int comm_width = 20;
+static int comm_width = 30;
 
 static char *timehist_get_commstr(struct thread *thread)
 {

commit 0e6758e8231d5905c2e267566e9bd679a68a7b00
Author: Namhyung Kim <namhyung@kernel.org>
Date:   Thu Dec 22 15:03:48 2016 +0900

    perf sched timehist: Honour 'comm_width' when aligning the headers
    
    Current default value is 20, but that may change in the future, so make
    places where we have 20 hardcoded use 'comm_width'.
    
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20161222060350.17655-1-namhyung@kernel.org
    [ Split from a larger patch ]
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index c1c07bfe132c..020f9c9cef7b 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1817,7 +1817,7 @@ static void timehist_header(struct perf_sched *sched)
 		printf(" ");
 	}
 
-	printf(" %-20s  %9s  %9s  %9s",
+	printf(" %-*s  %9s  %9s  %9s", comm_width,
 		"task name", "wait time", "sch delay", "run time");
 
 	printf("\n");
@@ -1830,7 +1830,8 @@ static void timehist_header(struct perf_sched *sched)
 	if (sched->show_cpu_visual)
 		printf(" %*s ", ncpus, "");
 
-	printf(" %-20s  %9s  %9s  %9s\n", "[tid/pid]", "(msec)", "(msec)", "(msec)");
+	printf(" %-*s  %9s  %9s  %9s\n", comm_width,
+	       "[tid/pid]", "(msec)", "(msec)", "(msec)");
 
 	/*
 	 * separator
@@ -1840,7 +1841,7 @@ static void timehist_header(struct perf_sched *sched)
 	if (sched->show_cpu_visual)
 		printf(" %.*s ", ncpus, graph_dotted_line);
 
-	printf(" %.20s  %.9s  %.9s  %.9s",
+	printf(" %.*s  %.9s  %.9s  %.9s", comm_width,
 		graph_dotted_line, graph_dotted_line, graph_dotted_line,
 		graph_dotted_line);
 

commit ba957ebb54893acaf3dc846031073a63f021cee1
Author: Namhyung Kim <namhyung@kernel.org>
Date:   Thu Dec 8 23:47:55 2016 +0900

    perf sched timehist: Show callchains for idle stat
    
    When --idle-hist option is used with --summary, it now shows idle stats
    with callchains like below:
    
      Idle stats by callchain:
      CPU  0:   902.195 msec
      Idle time (msec)    Count Callchains
      ----------------  ------- --------------------------------------------------
               370.589       69 futex_wait_queue_me <- futex_wait <- do_futex <- sys_futex <- entry_SYSCALL_64_fastpath
               178.799       17 worker_thread <- kthread <- ret_from_fork
               128.352       17 schedule_timeout <- rcu_gp_kthread <- kthread <- ret_from_fork
               125.111       19 schedule_hrtimeout_range_clock <- schedule_hrtimeout_range <- poll_schedule_timeout <- do_select <- core_sys_select
                71.599       50 schedule_hrtimeout_range_clock <- schedule_hrtimeout_range <- poll_schedule_timeout <- do_sys_poll <- sys_poll
                23.146        1 rcu_gp_kthread <- kthread <- ret_from_fork
                 4.510        1 schedule_hrtimeout_range_clock <- schedule_hrtimeout_range <- ep_poll <- sys_epoll_wait <- do_syscall_64
                 0.085        1 schedule_hrtimeout_range_clock <- schedule_hrtimeout_range <- poll_schedule_timeout <- do_sys_poll <- do_restart_poll
      ...
    
    Committer notes:
    
    Extra testing:
    
      # uname -a
      Linux jouet 4.8.8-300.fc25.x86_64 #1 SMP Tue Nov 15 18:10:06 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
    
    1) Run 'perf sched record -g'
    
    2) Run 'perf sched timehist --idle --summary'
    
    <SNIP>
      Idle stats by callchain:
      CPU  0: 13456.840 msec
      Idle time (msec) Count Callchains
      ---------------- ----- --------------------------------------------------
              5386.637  3283 schedule_hrtimeout_range_clock <- schedule_hrtimeout_range <- poll_schedule_timeout <- do_sys_poll <- sys_poll
              2750.238  2299 futex_wait_queue_me <- futex_wait <- do_futex <- sys_futex <- do_syscall_64
              1275.672  1287 schedule_hrtimeout_range_clock <- schedule_hrtimeout_range <- ep_poll <- sys_epoll_wait <- entry_SYSCALL_64_fastpath
               936.322   452 worker_thread <- kthread <- ret_from_fork
               741.311   385 rcu_nocb_kthread <- kthread <- ret_from_fork
               729.385   248 schedule_hrtimeout_range_clock <- schedule_hrtimeout_range <- poll_schedule_timeout <- do_sys_poll <- sys_ppoll
               365.386   229 irq_thread <- kthread <- ret_from_fork
               338.934   265 futex_wait_queue_me <- futex_wait <- do_futex <- sys_futex <- entry_SYSCALL_64_fastpath
               219.488   201 schedule_timeout <- rcu_gp_kthread <- kthread <- ret_from_fork
               186.839   410 schedule_hrtimeout_range_clock <- schedule_hrtimeout_range <- ep_poll <- sys_epoll_wait <- do_syscall_64
               142.541    59 kvm_vcpu_block <- kvm_arch_vcpu_ioctl_run <- kvm_vcpu_ioctl <- do_vfs_ioctl <- sys_ioctl
                83.887    92 smpboot_thread_fn <- kthread <- ret_from_fork
                62.722    96 do_exit <- do_group_exit <- 0x2a5594 <- entry_SYSCALL_64_fastpath
                47.894    83 pipe_wait <- pipe_read <- __vfs_read <- vfs_read <- sys_read
                46.554    61 rcu_gp_kthread <- kthread <- ret_from_fork
                34.337    21 schedule_timeout <- intel_fbc_work_fn <- process_one_work <- worker_thread <- kthread
                29.521    14 schedule_hrtimeout_range_clock <- schedule_hrtimeout_range <- poll_schedule_timeout <- do_select <- core_sys_select
                20.274    10 schedule_timeout <- io_schedule_timeout <- bit_wait_io <- __wait_on_bit <- out_of_line_wait_on_bit
                15.085    55 schedule_timeout <- unix_stream_read_generic <- unix_stream_recvmsg <- sock_recvmsg <- SYSC_recvfrom
    <SNIP>
    
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Tested-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Acked-by: David Ahern <dsahern@gmail.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20161208144755.16673-7-namhyung@kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 0b14265432df..c1c07bfe132c 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -2448,6 +2448,9 @@ static int timehist_sched_change_event(struct perf_tool *tool,
 			last_tr->dt_wait = 0;
 			last_tr->dt_delay = 0;
 
+			if (itr->cursor.nr)
+				callchain_append(&itr->callchain, &itr->cursor, t - tprev);
+
 			itr->last_thread = NULL;
 		}
 	}
@@ -2557,6 +2560,60 @@ static int show_deadthread_runtime(struct thread *t, void *priv)
 	return __show_thread_runtime(t, priv);
 }
 
+static size_t callchain__fprintf_folded(FILE *fp, struct callchain_node *node)
+{
+	const char *sep = " <- ";
+	struct callchain_list *chain;
+	size_t ret = 0;
+	char bf[1024];
+	bool first;
+
+	if (node == NULL)
+		return 0;
+
+	ret = callchain__fprintf_folded(fp, node->parent);
+	first = (ret == 0);
+
+	list_for_each_entry(chain, &node->val, list) {
+		if (chain->ip >= PERF_CONTEXT_MAX)
+			continue;
+		if (chain->ms.sym && chain->ms.sym->ignore)
+			continue;
+		ret += fprintf(fp, "%s%s", first ? "" : sep,
+			       callchain_list__sym_name(chain, bf, sizeof(bf),
+							false));
+		first = false;
+	}
+
+	return ret;
+}
+
+static size_t timehist_print_idlehist_callchain(struct rb_root *root)
+{
+	size_t ret = 0;
+	FILE *fp = stdout;
+	struct callchain_node *chain;
+	struct rb_node *rb_node = rb_first(root);
+
+	printf("  %16s  %8s  %s\n", "Idle time (msec)", "Count", "Callchains");
+	printf("  %.16s  %.8s  %.50s\n", graph_dotted_line, graph_dotted_line,
+	       graph_dotted_line);
+
+	while (rb_node) {
+		chain = rb_entry(rb_node, struct callchain_node, rb_node);
+		rb_node = rb_next(rb_node);
+
+		ret += fprintf(fp, "  ");
+		print_sched_time(chain->hit, 12);
+		ret += 16;  /* print_sched_time returns 2nd arg + 4 */
+		ret += fprintf(fp, " %8d  ", chain->count);
+		ret += callchain__fprintf_folded(fp, chain);
+		ret += fprintf(fp, "\n");
+	}
+
+	return ret;
+}
+
 static void timehist_print_summary(struct perf_sched *sched,
 				   struct perf_session *session)
 {
@@ -2615,6 +2672,35 @@ static void timehist_print_summary(struct perf_sched *sched,
 			printf("    CPU %2d idle entire time window\n", i);
 	}
 
+	if (sched->idle_hist && symbol_conf.use_callchain) {
+		callchain_param.mode  = CHAIN_FOLDED;
+		callchain_param.value = CCVAL_PERIOD;
+
+		callchain_register_param(&callchain_param);
+
+		printf("\nIdle stats by callchain:\n");
+		for (i = 0; i < idle_max_cpu; ++i) {
+			struct idle_thread_runtime *itr;
+
+			t = idle_threads[i];
+			if (!t)
+				continue;
+
+			itr = thread__priv(t);
+			if (itr == NULL)
+				continue;
+
+			callchain_param.sort(&itr->sorted_root, &itr->callchain,
+					     0, &callchain_param);
+
+			printf("  CPU %2d:", i);
+			print_sched_time(itr->tr.total_run_time, 6);
+			printf(" msec\n");
+			timehist_print_idlehist_callchain(&itr->sorted_root);
+			printf("\n");
+		}
+	}
+
 	printf("\n"
 	       "    Total number of unique tasks: %" PRIu64 "\n"
 	       "Total number of context switches: %" PRIu64 "\n"

commit 07235f84ece6b66f43334881806aad3467cf3d84
Author: Namhyung Kim <namhyung@kernel.org>
Date:   Thu Dec 8 23:47:54 2016 +0900

    perf sched timehist: Add -I/--idle-hist option
    
    The --idle-hist option is to analyze system idle state so which process
    makes cpu to go idle.  If this option is specified, non-idle events will
    be skipped and processes switching to/from idle will be shown.
    
    This option is mostly useful when used with --summary(-only) option.  In
    the idle-time summary view, idle time is accounted to previous thread
    which is run before idle task.
    
    The example output looks like following:
    
      Idle-time summary
                      comm parent sched-out idle-time min-idle avg-idle max-idle stddev migrations
                                    (count)    (msec)   (msec)   (msec)   (msec)      %
      --------------------------------------------------------------------------------------------
            rcu_preempt[7]      2        95   550.872    0.011    5.798   23.146   7.63      0
           migration/1[16]      2         1    15.558   15.558   15.558   15.558   0.00      0
            khugepaged[39]      2         1     3.062    3.062    3.062    3.062   0.00      0
         kworker/0:1H[124]      2         2     4.728    0.611    2.364    4.116  74.12      0
      systemd-journal[167]      1         1     4.510    4.510    4.510    4.510   0.00      0
        kworker/u16:3[558]      2        13    74.737    0.080    5.749   12.960  21.96      0
       irq/34-iwlwifi[628]      2        21   118.403    0.032    5.638   23.990  24.00      0
        kworker/u17:0[673]      2         1     3.523    3.523    3.523    3.523   0.00      0
          dbus-daemon[722]      1         1     6.743    6.743    6.743    6.743   0.00      0
              ifplugd[741]      1         1    58.826   58.826   58.826   58.826   0.00      0
      wpa_supplicant[1490]      1         1    13.302   13.302   13.302   13.302   0.00      0
         wpa_actiond[1492]      1         2     4.064    0.168    2.032    3.896  91.72      0
             dockerd[1500]      1         1     0.055    0.055    0.055    0.055   0.00      0
      ...
    
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Tested-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Acked-by: David Ahern <dsahern@gmail.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20161208144755.16673-6-namhyung@kernel.org
    Link: http://lkml.kernel.org/r/20161213080632.19099-2-namhyung@kernel.org
    [ Merged fix sent by Namhyumg, as posted in the second Link: tag ]
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index c8e7848e71a7..0b14265432df 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -2421,7 +2421,36 @@ static int timehist_sched_change_event(struct perf_tool *tool,
 			t = ptime->end;
 	}
 
-	timehist_update_runtime_stats(tr, t, tprev);
+	if (!sched->idle_hist || thread->tid == 0) {
+		timehist_update_runtime_stats(tr, t, tprev);
+
+		if (sched->idle_hist) {
+			struct idle_thread_runtime *itr = (void *)tr;
+			struct thread_runtime *last_tr;
+
+			BUG_ON(thread->tid != 0);
+
+			if (itr->last_thread == NULL)
+				goto out;
+
+			/* add current idle time as last thread's runtime */
+			last_tr = thread__get_runtime(itr->last_thread);
+			if (last_tr == NULL)
+				goto out;
+
+			timehist_update_runtime_stats(last_tr, t, tprev);
+			/*
+			 * remove delta time of last thread as it's not updated
+			 * and otherwise it will show an invalid value next
+			 * time.  we only care total run time and run stat.
+			 */
+			last_tr->dt_run = 0;
+			last_tr->dt_wait = 0;
+			last_tr->dt_delay = 0;
+
+			itr->last_thread = NULL;
+		}
+	}
 
 	if (!sched->summary_only)
 		timehist_print_sample(sched, sample, &al, thread, t);
@@ -2543,9 +2572,15 @@ static void timehist_print_summary(struct perf_sched *sched,
 	if (comm_width < 30)
 		comm_width = 30;
 
-	printf("\nRuntime summary\n");
-	printf("%*s  parent   sched-in  ", comm_width, "comm");
-	printf("   run-time    min-run     avg-run     max-run  stddev  migrations\n");
+	if (sched->idle_hist) {
+		printf("\nIdle-time summary\n");
+		printf("%*s  parent  sched-out  ", comm_width, "comm");
+		printf("  idle-time   min-idle    avg-idle    max-idle  stddev  migrations\n");
+	} else {
+		printf("\nRuntime summary\n");
+		printf("%*s  parent   sched-in  ", comm_width, "comm");
+		printf("   run-time    min-run     avg-run     max-run  stddev  migrations\n");
+	}
 	printf("%*s            (count)  ", comm_width, "");
 	printf("     (msec)     (msec)      (msec)      (msec)       %%\n");
 	printf("%.117s\n", graph_dotted_line);
@@ -2561,7 +2596,7 @@ static void timehist_print_summary(struct perf_sched *sched,
 		printf("<no terminated tasks>\n");
 
 	/* CPU idle stats not tracked when samples were skipped */
-	if (sched->skipped_samples)
+	if (sched->skipped_samples && !sched->idle_hist)
 		return;
 
 	printf("\nIdle stats:\n");
@@ -3107,6 +3142,7 @@ int cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)
 	OPT_BOOLEAN('w', "wakeups", &sched.show_wakeups, "Show wakeup events"),
 	OPT_BOOLEAN('M', "migrations", &sched.show_migrations, "Show migration events"),
 	OPT_BOOLEAN('V', "cpu-visual", &sched.show_cpu_visual, "Add CPU visual"),
+	OPT_BOOLEAN('I', "idle-hist", &sched.idle_hist, "Show idle events only"),
 	OPT_STRING(0, "time", &sched.time_str, "str",
 		   "Time span for analysis (start,stop)"),
 	OPT_PARENT(sched_options)

commit a4b2b6f56e0cfe729cf89318d44b6a875b31d95a
Author: Namhyung Kim <namhyung@kernel.org>
Date:   Thu Dec 8 23:47:53 2016 +0900

    perf sched timehist: Skip non-idle events when necessary
    
    Sometimes it only focuses on idle-related events like upcoming idle-hist
    feature.  In this case we don't want to see other event to reduce noise.
    
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Acked-by: David Ahern <dsahern@gmail.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20161208144755.16673-5-namhyung@kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index dc83b803ca54..c8e7848e71a7 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -2190,7 +2190,9 @@ static struct thread *timehist_get_thread(struct perf_sched *sched,
 }
 
 static bool timehist_skip_sample(struct perf_sched *sched,
-				 struct thread *thread)
+				 struct thread *thread,
+				 struct perf_evsel *evsel,
+				 struct perf_sample *sample)
 {
 	bool rc = false;
 
@@ -2199,10 +2201,19 @@ static bool timehist_skip_sample(struct perf_sched *sched,
 		sched->skipped_samples++;
 	}
 
+	if (sched->idle_hist) {
+		if (strcmp(perf_evsel__name(evsel), "sched:sched_switch"))
+			rc = true;
+		else if (perf_evsel__intval(evsel, sample, "prev_pid") != 0 &&
+			 perf_evsel__intval(evsel, sample, "next_pid") != 0)
+			rc = true;
+	}
+
 	return rc;
 }
 
 static void timehist_print_wakeup_event(struct perf_sched *sched,
+					struct perf_evsel *evsel,
 					struct perf_sample *sample,
 					struct machine *machine,
 					struct thread *awakened)
@@ -2215,8 +2226,8 @@ static void timehist_print_wakeup_event(struct perf_sched *sched,
 		return;
 
 	/* show wakeup unless both awakee and awaker are filtered */
-	if (timehist_skip_sample(sched, thread) &&
-	    timehist_skip_sample(sched, awakened)) {
+	if (timehist_skip_sample(sched, thread, evsel, sample) &&
+	    timehist_skip_sample(sched, awakened, evsel, sample)) {
 		return;
 	}
 
@@ -2261,7 +2272,7 @@ static int timehist_sched_wakeup_event(struct perf_tool *tool,
 	/* show wakeups if requested */
 	if (sched->show_wakeups &&
 	    !perf_time__skip_sample(&sched->ptime, sample->time))
-		timehist_print_wakeup_event(sched, sample, machine, thread);
+		timehist_print_wakeup_event(sched, evsel, sample, machine, thread);
 
 	return 0;
 }
@@ -2288,8 +2299,8 @@ static void timehist_print_migration_event(struct perf_sched *sched,
 	if (thread == NULL)
 		return;
 
-	if (timehist_skip_sample(sched, thread) &&
-	    timehist_skip_sample(sched, migrated)) {
+	if (timehist_skip_sample(sched, thread, evsel, sample) &&
+	    timehist_skip_sample(sched, migrated, evsel, sample)) {
 		return;
 	}
 
@@ -2374,7 +2385,7 @@ static int timehist_sched_change_event(struct perf_tool *tool,
 		goto out;
 	}
 
-	if (timehist_skip_sample(sched, thread))
+	if (timehist_skip_sample(sched, thread, evsel, sample))
 		goto out;
 
 	tr = thread__get_runtime(thread);

commit 699b5b920db04a6ff5c03a519e4c182aeb350952
Author: Namhyung Kim <namhyung@kernel.org>
Date:   Thu Dec 8 23:47:52 2016 +0900

    perf sched timehist: Save callchain when entering idle
    
    In order to investigate the idleness reason, it is necessary to keep the
    callchains when entering idle.  This can be identified by the
    sched:sched_switch event having the next_pid field as 0.
    
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Acked-by: David Ahern <dsahern@gmail.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20161208144755.16673-4-namhyung@kernel.org
    Link: http://lkml.kernel.org/r/20161213080632.19099-1-namhyung@kernel.org
    [ Merged fix from Namhyung, see second Link: tag ]
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index e108b0f6a246..dc83b803ca54 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -200,6 +200,7 @@ struct perf_sched {
 	/* options for timehist command */
 	bool		summary;
 	bool		summary_only;
+	bool		idle_hist;
 	bool		show_callchain;
 	unsigned int	max_stack;
 	bool		show_cpu_visual;
@@ -2101,6 +2102,15 @@ static struct thread *get_idle_thread(int cpu)
 	return idle_threads[cpu];
 }
 
+static void save_idle_callchain(struct idle_thread_runtime *itr,
+				struct perf_sample *sample)
+{
+	if (!symbol_conf.use_callchain || sample->callchain == NULL)
+		return;
+
+	callchain_cursor__copy(&itr->cursor, &callchain_cursor);
+}
+
 /*
  * handle runtime stats saved per thread
  */
@@ -2154,6 +2164,26 @@ static struct thread *timehist_get_thread(struct perf_sched *sched,
 		}
 
 		save_task_callchain(sched, sample, evsel, machine);
+		if (sched->idle_hist) {
+			struct thread *idle;
+			struct idle_thread_runtime *itr;
+
+			idle = get_idle_thread(sample->cpu);
+			if (idle == NULL) {
+				pr_err("Failed to get idle thread for cpu %d.\n", sample->cpu);
+				return NULL;
+			}
+
+			itr = thread__priv(idle);
+			if (itr == NULL)
+				return NULL;
+
+			itr->last_thread = thread;
+
+			/* copy task callchain when entering to idle */
+			if (perf_evsel__intval(evsel, sample, "next_pid") == 0)
+				save_idle_callchain(itr, sample);
+		}
 	}
 
 	return thread;

commit 3bc2fa9cb829ccf6527e7117d9af769d93ee6d39
Author: Namhyung Kim <namhyung@kernel.org>
Date:   Thu Dec 8 23:47:51 2016 +0900

    perf sched timehist: Introduce struct idle_time_data
    
    The struct idle_time_data is to keep idle stats with callchains entering
    to the idle task.  The normal thread_runtime calculation is done
    transparently since it extends the struct thread_runtime.
    
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Acked-by: David Ahern <dsahern@gmail.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20161208144755.16673-3-namhyung@kernel.org
    [ Align struct field names ]
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 966eddce1609..e108b0f6a246 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -230,6 +230,15 @@ struct evsel_runtime {
 	u32 ncpu;       /* highest cpu slot allocated */
 };
 
+/* per cpu idle time data */
+struct idle_thread_runtime {
+	struct thread_runtime	tr;
+	struct thread		*last_thread;
+	struct rb_root		sorted_root;
+	struct callchain_root	callchain;
+	struct callchain_cursor	cursor;
+};
+
 /* track idle times per cpu */
 static struct thread **idle_threads;
 static int idle_max_cpu;
@@ -1997,13 +2006,31 @@ static void save_task_callchain(struct perf_sched *sched,
 	}
 }
 
+static int init_idle_thread(struct thread *thread)
+{
+	struct idle_thread_runtime *itr;
+
+	thread__set_comm(thread, idle_comm, 0);
+
+	itr = zalloc(sizeof(*itr));
+	if (itr == NULL)
+		return -ENOMEM;
+
+	init_stats(&itr->tr.run_stats);
+	callchain_init(&itr->callchain);
+	callchain_cursor_reset(&itr->cursor);
+	thread__set_priv(thread, itr);
+
+	return 0;
+}
+
 /*
  * Track idle stats per cpu by maintaining a local thread
  * struct for the idle task on each cpu.
  */
 static int init_idle_threads(int ncpu)
 {
-	int i;
+	int i, ret;
 
 	idle_threads = zalloc(ncpu * sizeof(struct thread *));
 	if (!idle_threads)
@@ -2017,7 +2044,9 @@ static int init_idle_threads(int ncpu)
 		if (idle_threads[i] == NULL)
 			return -ENOMEM;
 
-		thread__set_comm(idle_threads[i], idle_comm, 0);
+		ret = init_idle_thread(idle_threads[i]);
+		if (ret < 0)
+			return ret;
 	}
 
 	return 0;
@@ -2064,8 +2093,8 @@ static struct thread *get_idle_thread(int cpu)
 		idle_threads[cpu] = thread__new(0, 0);
 
 		if (idle_threads[cpu]) {
-			idle_threads[cpu]->tid = 0;
-			thread__set_comm(idle_threads[cpu], idle_comm, 0);
+			if (init_idle_thread(idle_threads[cpu]) < 0)
+				return NULL;
 		}
 	}
 

commit 96039c7c52e03b7d6dd773664e74b79e3ae9856a
Author: Namhyung Kim <namhyung@kernel.org>
Date:   Thu Dec 8 23:47:50 2016 +0900

    perf sched timehist: Split is_idle_sample()
    
    The is_idle_sample() function actually does more than determining
    whether sample come from idle task.  Split the callchain part into
    save_task_callchain() to make it clearer.
    
    Also checking prev_pid from trace data looks preferred than just
    checking sample->pid since it's possible, although rare, to have invalid
    0 pid/tid on scheduling an exiting task.
    
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Acked-by: David Ahern <dsahern@gmail.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20161208144755.16673-2-namhyung@kernel.org
    [ Remove some needless () in some return statements ]
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 1a3f1be93372..966eddce1609 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1939,39 +1939,40 @@ static void timehist_update_runtime_stats(struct thread_runtime *r,
 	r->total_run_time += r->dt_run;
 }
 
-static bool is_idle_sample(struct perf_sched *sched,
-			   struct perf_sample *sample,
-			   struct perf_evsel *evsel,
-			   struct machine *machine)
+static bool is_idle_sample(struct perf_sample *sample,
+			   struct perf_evsel *evsel)
 {
-	struct thread *thread;
-	struct callchain_cursor *cursor = &callchain_cursor;
-
 	/* pid 0 == swapper == idle task */
-	if (sample->pid == 0)
-		return true;
+	if (strcmp(perf_evsel__name(evsel), "sched:sched_switch") == 0)
+		return perf_evsel__intval(evsel, sample, "prev_pid") == 0;
 
-	if (strcmp(perf_evsel__name(evsel), "sched:sched_switch") == 0) {
-		if (perf_evsel__intval(evsel, sample, "prev_pid") == 0)
-			return true;
-	}
+	return sample->pid == 0;
+}
+
+static void save_task_callchain(struct perf_sched *sched,
+				struct perf_sample *sample,
+				struct perf_evsel *evsel,
+				struct machine *machine)
+{
+	struct callchain_cursor *cursor = &callchain_cursor;
+	struct thread *thread;
 
 	/* want main thread for process - has maps */
 	thread = machine__findnew_thread(machine, sample->pid, sample->pid);
 	if (thread == NULL) {
 		pr_debug("Failed to get thread for pid %d.\n", sample->pid);
-		return false;
+		return;
 	}
 
 	if (!symbol_conf.use_callchain || sample->callchain == NULL)
-		return false;
+		return;
 
 	if (thread__resolve_callchain(thread, cursor, evsel, sample,
 				      NULL, NULL, sched->max_stack + 2) != 0) {
 		if (verbose)
 			error("Failed to resolve callchain. Skipping\n");
 
-		return false;
+		return;
 	}
 
 	callchain_cursor_commit(cursor);
@@ -1994,8 +1995,6 @@ static bool is_idle_sample(struct perf_sched *sched,
 
 		callchain_cursor_advance(cursor);
 	}
-
-	return false;
 }
 
 /*
@@ -2111,7 +2110,7 @@ static struct thread *timehist_get_thread(struct perf_sched *sched,
 {
 	struct thread *thread;
 
-	if (is_idle_sample(sched, sample, evsel, machine)) {
+	if (is_idle_sample(sample, evsel)) {
 		thread = get_idle_thread(sample->cpu);
 		if (thread == NULL)
 			pr_err("Failed to get idle thread for cpu %d.\n", sample->cpu);
@@ -2124,6 +2123,8 @@ static struct thread *timehist_get_thread(struct perf_sched *sched,
 			pr_debug("Failed to get thread for tid %d. skipping sample.\n",
 				 sample->tid);
 		}
+
+		save_task_callchain(sched, sample, evsel, machine);
 	}
 
 	return thread;

commit b336352b417edd8c058c9e09d136728b6e2ff56d
Author: Namhyung Kim <namhyung@kernel.org>
Date:   Tue Dec 6 12:40:05 2016 +0900

    perf sched timehist: Cleanup idle_max_cpu handling
    
    It treats the idle_max_cpu little bit confusingly IMHO.  Let's make it
    more straight forward.
    
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Acked-by: David Ahern <dsahern@gmail.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20161206034010.6499-6-namhyung@kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index c8b3e6c330c3..1a3f1be93372 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -2010,7 +2010,7 @@ static int init_idle_threads(int ncpu)
 	if (!idle_threads)
 		return -ENOMEM;
 
-	idle_max_cpu = ncpu - 1;
+	idle_max_cpu = ncpu;
 
 	/* allocate the actual thread struct if needed */
 	for (i = 0; i < ncpu; ++i) {
@@ -2031,7 +2031,7 @@ static void free_idle_threads(void)
 	if (idle_threads == NULL)
 		return;
 
-	for (i = 0; i <= idle_max_cpu; ++i) {
+	for (i = 0; i < idle_max_cpu; ++i) {
 		if ((idle_threads[i]))
 			thread__delete(idle_threads[i]);
 	}
@@ -2054,8 +2054,7 @@ static struct thread *get_idle_thread(int cpu)
 			return NULL;
 
 		idle_threads = (struct thread **) p;
-		i = idle_max_cpu ? idle_max_cpu + 1 : 0;
-		for (; i < j; ++i)
+		for (i = idle_max_cpu; i < j; ++i)
 			idle_threads[i] = NULL;
 
 		idle_max_cpu = j;
@@ -2495,7 +2494,7 @@ static void timehist_print_summary(struct perf_sched *sched,
 		return;
 
 	printf("\nIdle stats:\n");
-	for (i = 0; i <= idle_max_cpu; ++i) {
+	for (i = 0; i < idle_max_cpu; ++i) {
 		t = idle_threads[i];
 		if (!t)
 			continue;

commit 5d92d96a947a5d0d83710d11750bb29a0c1b985d
Author: Namhyung Kim <namhyung@kernel.org>
Date:   Tue Dec 6 12:40:03 2016 +0900

    perf sched timehist: Handle zero sample->tid properly
    
    Sometimes samples have tid of 0 but non-0 pid.  It ends up having a new
    thread of 0 tid/pid (instead of referring idle task) since tid is used
    to search matching task.  But I guess it's wrong to use 0 as a tid when
    pid is set.  This patch uses tid only if it has a non-zero value or same
    as pid (of 0).
    
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20161206034010.6499-4-namhyung@kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 634d8cfb4693..c8b3e6c330c3 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -2118,7 +2118,9 @@ static struct thread *timehist_get_thread(struct perf_sched *sched,
 			pr_err("Failed to get idle thread for cpu %d.\n", sample->cpu);
 
 	} else {
-		thread = machine__findnew_thread(machine, sample->pid, sample->tid);
+		/* there were samples with tid 0 but non-zero pid */
+		thread = machine__findnew_thread(machine, sample->pid,
+						 sample->tid ?: sample->pid);
 		if (thread == NULL) {
 			pr_debug("Failed to get thread for tid %d. skipping sample.\n",
 				 sample->tid);

commit 6fa94258ce2673adc707b2ec5668464f2cf83ed3
Author: Namhyung Kim <namhyung@kernel.org>
Date:   Tue Dec 6 12:40:01 2016 +0900

    perf sched: Cleanup option processing
    
    The -D/--dump-raw-trace option is in the parent option so no need to
    repeat it.  Also move -f/--force option to parent as it's common to
    handle data file.
    
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Acked-by: David Ahern <dsahern@gmail.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20161206034010.6499-2-namhyung@kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 84c39f177bef..634d8cfb4693 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -2583,6 +2583,7 @@ static int perf_sched__timehist(struct perf_sched *sched)
 	struct perf_data_file file = {
 		.path = input_name,
 		.mode = PERF_DATA_MODE_READ,
+		.force = sched->force,
 	};
 
 	struct perf_session *session;
@@ -2988,6 +2989,7 @@ int cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)
 		    "be more verbose (show symbol address, etc)"),
 	OPT_BOOLEAN('D', "dump-raw-trace", &dump_trace,
 		    "dump raw trace in ASCII"),
+	OPT_BOOLEAN('f', "force", &sched.force, "don't complain, do it"),
 	OPT_END()
 	};
 	const struct option latency_options[] = {
@@ -2995,8 +2997,6 @@ int cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)
 		   "sort by key(s): runtime, switch, avg, max"),
 	OPT_INTEGER('C', "CPU", &sched.profile_cpu,
 		    "CPU to profile on"),
-	OPT_BOOLEAN('D', "dump-raw-trace", &dump_trace,
-		    "dump raw trace in ASCII"),
 	OPT_BOOLEAN('p', "pids", &sched.skip_merge,
 		    "latency stats per pid instead of per comm"),
 	OPT_PARENT(sched_options)
@@ -3004,9 +3004,6 @@ int cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)
 	const struct option replay_options[] = {
 	OPT_UINTEGER('r', "repeat", &sched.replay_repeat,
 		     "repeat the workload replay N times (-1: infinite)"),
-	OPT_BOOLEAN('D', "dump-raw-trace", &dump_trace,
-		    "dump raw trace in ASCII"),
-	OPT_BOOLEAN('f', "force", &sched.force, "don't complain, do it"),
 	OPT_PARENT(sched_options)
 	};
 	const struct option map_options[] = {

commit f45bf8d3939d8f32f06ec12e56d2b8651c5437b4
Author: David Ahern <dsahern@gmail.com>
Date:   Tue Nov 29 13:39:48 2016 -0700

    perf sched timehist: Improve error message when analyzing wrong file
    
    Arnaldo reported an unhelpful error message when running perf sched
    timehist on a file that did not contain sched tracepoints:
    
        [root@jouet ~]# perf sched timehist
        No trace sample to read. Did you call 'perf record -R'?
    
        [root@jouet ~]# perf evlist -v
        cycles:ppp: size: 112, { sample_period, sample_freq }: 4000, sample_type: IP|TID|TIME|CALLCHAIN|CPU|PERIOD, disabled: 1, inherit: 1, mmap: 1, comm: 1, freq: 1, task: 1, precise_ip: 3, sample_id_all: 1, exclude_guest: 1, mmap2: 1, comm_exec: 1
    
    Change the has_traces check to look for the sched_switch event. Analysis
    for perf sched timehist requires at least this event.
    
    Now when analyzing a file without sched tracepoints you get:
    
        root@f21-vbox:/tmp$ perf sched timehist
        No sched_switch events found. Have you run 'perf sched record'?
    
    Signed-off-by: David Ahern <dsahern@gmail.com>
    Reported-and-Tested-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Acked-by: Namhyung Kim <namhyung@kernel.org>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1480451988-43673-1-git-send-email-dsahern@gmail.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 870d94cd20ba..84c39f177bef 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -2629,8 +2629,12 @@ static int perf_sched__timehist(struct perf_sched *sched)
 	if (perf_session__set_tracepoints_handlers(session, handlers))
 		goto out;
 
-	if (!perf_session__has_traces(session, "record -R"))
+	/* sched_switch event at a minimum needs to exist */
+	if (!perf_evlist__find_tracepoint_by_name(session->evlist,
+						  "sched:sched_switch")) {
+		pr_err("No sched_switch events found. Have you run 'perf sched record'?\n");
 		goto out;
+	}
 
 	if (sched->show_migrations &&
 	    perf_session__set_tracepoints_handlers(session, migrate_handlers))

commit 853b74071110bed344bad1ca9d8de27731b1c574
Author: David Ahern <dsa@cumulusnetworks.com>
Date:   Tue Nov 29 10:15:44 2016 -0700

    perf sched timehist: Add option to specify time window of interest
    
    Add option to allow user to control analysis window. e.g., collect data
    for time window and analyze a segment of interest within that window.
    
    Committer notes:
    
    Testing it:
    
      # perf sched record -a usleep 1
      [ perf record: Woken up 1 times to write data ]
      [ perf record: Captured and wrote 1.593 MB perf.data (25 samples) ]
      #
      # perf sched timehist | head -18
      Samples do not have callchains.
              time    cpu   task name       wait time  sch delay  run time
                            [tid/pid]          (msec)     (msec)    (msec)
      ------------- ------  --------------- ---------  ---------  --------
       19818.635579 [0002]  <idle>              0.000      0.000     0.000
       19818.635613 [0000]  perf[9116]          0.000      0.000     0.000
       19818.635676 [0000]  <idle>              0.000      0.000     0.063
       19818.635678 [0000]  rcuos/2[29]         0.000      0.002     0.001
       19818.635696 [0002]  perf[9117]          0.000      0.004     0.116
       19818.635702 [0000]  <idle>              0.001      0.000     0.024
       19818.635709 [0002]  migration/2[25]     0.000      0.003     0.012
       19818.636263 [0000]  usleep[9117]        0.005      0.000     0.560
       19818.636316 [0000]  <idle>              0.560      0.000     0.053
       19818.636358 [0002]  <idle>              0.129      0.000     0.649
       19818.636358 [0000]  usleep[9117]        0.053      0.002     0.042
      #
    
      # perf sched timehist --time 19818.635696,
      Samples do not have callchains.
               time    cpu  task name       wait time  sch delay  run time
                            [tid/pid]          (msec)     (msec)    (msec)
      ------------- ------  ---------------  --------  --------- ---------
       19818.635696 [0002]  perf[9117]          0.000      0.120     0.000
       19818.635702 [0000]  <idle>              0.019      0.000     0.006
       19818.635709 [0002]  migration/2[25]     0.000      0.003     0.012
       19818.636263 [0000]  usleep[9117]        0.005      0.000     0.560
       19818.636316 [0000]  <idle>              0.560      0.000     0.053
       19818.636358 [0002]  <idle>              0.129      0.000     0.649
       19818.636358 [0000]  usleep[9117]        0.053      0.002     0.042
      #
      # perf sched timehist --time 19818.635696,19818.635709
      Samples do not have callchains.
               time    cpu  task name       wait time  sch delay  run time
                            [tid/pid]          (msec)     (msec)    (msec)
      ------------- ------  --------------- ---------  --------- ---------
       19818.635696 [0002]  perf[9117]          0.000      0.120     0.000
       19818.635702 [0000]  <idle>              0.019      0.000     0.006
       19818.635709 [0002]  migration/2[25]     0.000      0.003     0.012
       19818.635709 [0000]  usleep[9117]        0.005      0.000     0.006
      #
    
    Signed-off-by: David Ahern <dsahern@gmail.com>
    Tested-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Acked-by: Namhyung Kim <namhyung@kernel.org>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1480439746-42695-5-git-send-email-dsahern@gmail.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 4f9e7cba4ebf..870d94cd20ba 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -15,6 +15,7 @@
 #include "util/color.h"
 #include "util/stat.h"
 #include "util/callchain.h"
+#include "util/time-utils.h"
 
 #include <subcmd/parse-options.h>
 #include "util/trace-event.h"
@@ -205,6 +206,8 @@ struct perf_sched {
 	bool		show_wakeups;
 	bool		show_migrations;
 	u64		skipped_samples;
+	const char	*time_str;
+	struct perf_time_interval ptime;
 };
 
 /* per thread run time data */
@@ -1837,13 +1840,14 @@ static void timehist_header(struct perf_sched *sched)
 static void timehist_print_sample(struct perf_sched *sched,
 				  struct perf_sample *sample,
 				  struct addr_location *al,
-				  struct thread *thread)
+				  struct thread *thread,
+				  u64 t)
 {
 	struct thread_runtime *tr = thread__priv(thread);
 	u32 max_cpus = sched->max_cpu + 1;
 	char tstr[64];
 
-	timestamp__scnprintf_usec(sample->time, tstr, sizeof(tstr));
+	timestamp__scnprintf_usec(t, tstr, sizeof(tstr));
 	printf("%15s [%04d] ", tstr, sample->cpu);
 
 	if (sched->show_cpu_visual) {
@@ -2194,7 +2198,8 @@ static int timehist_sched_wakeup_event(struct perf_tool *tool,
 		tr->ready_to_run = sample->time;
 
 	/* show wakeups if requested */
-	if (sched->show_wakeups)
+	if (sched->show_wakeups &&
+	    !perf_time__skip_sample(&sched->ptime, sample->time))
 		timehist_print_wakeup_event(sched, sample, machine, thread);
 
 	return 0;
@@ -2288,10 +2293,11 @@ static int timehist_sched_change_event(struct perf_tool *tool,
 				       struct machine *machine)
 {
 	struct perf_sched *sched = container_of(tool, struct perf_sched, tool);
+	struct perf_time_interval *ptime = &sched->ptime;
 	struct addr_location al;
 	struct thread *thread;
 	struct thread_runtime *tr = NULL;
-	u64 tprev;
+	u64 tprev, t = sample->time;
 	int rc = 0;
 
 	if (machine__resolve(machine, &al, sample) < 0) {
@@ -2318,9 +2324,35 @@ static int timehist_sched_change_event(struct perf_tool *tool,
 
 	tprev = perf_evsel__get_time(evsel, sample->cpu);
 
-	timehist_update_runtime_stats(tr, sample->time, tprev);
+	/*
+	 * If start time given:
+	 * - sample time is under window user cares about - skip sample
+	 * - tprev is under window user cares about  - reset to start of window
+	 */
+	if (ptime->start && ptime->start > t)
+		goto out;
+
+	if (ptime->start > tprev)
+		tprev = ptime->start;
+
+	/*
+	 * If end time given:
+	 * - previous sched event is out of window - we are done
+	 * - sample time is beyond window user cares about - reset it
+	 *   to close out stats for time window interest
+	 */
+	if (ptime->end) {
+		if (tprev > ptime->end)
+			goto out;
+
+		if (t > ptime->end)
+			t = ptime->end;
+	}
+
+	timehist_update_runtime_stats(tr, t, tprev);
+
 	if (!sched->summary_only)
-		timehist_print_sample(sched, sample, &al, thread);
+		timehist_print_sample(sched, sample, &al, thread, t);
 
 out:
 	if (tr) {
@@ -2583,6 +2615,11 @@ static int perf_sched__timehist(struct perf_sched *sched)
 
 	symbol__init(&session->header.env);
 
+	if (perf_time__parse_str(&sched->ptime, sched->time_str) != 0) {
+		pr_err("Invalid time string\n");
+		return -EINVAL;
+	}
+
 	if (timehist_check_attr(sched, evlist) != 0)
 		goto out;
 
@@ -2997,6 +3034,8 @@ int cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)
 	OPT_BOOLEAN('w', "wakeups", &sched.show_wakeups, "Show wakeup events"),
 	OPT_BOOLEAN('M', "migrations", &sched.show_migrations, "Show migration events"),
 	OPT_BOOLEAN('V', "cpu-visual", &sched.show_cpu_visual, "Add CPU visual"),
+	OPT_STRING(0, "time", &sched.time_str, "str",
+		   "Time span for analysis (start,stop)"),
 	OPT_PARENT(sched_options)
 	};
 

commit 350f54fab2afd7f819e64fbeecdf980086fa59f2
Author: David Ahern <dsahern@gmail.com>
Date:   Fri Nov 25 09:28:41 2016 -0700

    perf sched timehist: Handle cpu migration events
    
    Add handlers for sched:sched_migrate_task event. Total number of
    migrations is added to summary display and -M/--migrations can be used
    to show migration events.
    
    Signed-off-by: David Ahern <dsahern@gmail.com>
    Tested-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Link: http://lkml.kernel.org/r/1480091321-35591-1-git-send-email-dsa@cumulusnetworks.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index a49a032f5b15..4f9e7cba4ebf 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -203,6 +203,7 @@ struct perf_sched {
 	unsigned int	max_stack;
 	bool		show_cpu_visual;
 	bool		show_wakeups;
+	bool		show_migrations;
 	u64		skipped_samples;
 };
 
@@ -216,6 +217,8 @@ struct thread_runtime {
 
 	struct stats run_stats;
 	u64 total_run_time;
+
+	u64 migrations;
 };
 
 /* per event run time data */
@@ -2197,6 +2200,87 @@ static int timehist_sched_wakeup_event(struct perf_tool *tool,
 	return 0;
 }
 
+static void timehist_print_migration_event(struct perf_sched *sched,
+					struct perf_evsel *evsel,
+					struct perf_sample *sample,
+					struct machine *machine,
+					struct thread *migrated)
+{
+	struct thread *thread;
+	char tstr[64];
+	u32 max_cpus = sched->max_cpu + 1;
+	u32 ocpu, dcpu;
+
+	if (sched->summary_only)
+		return;
+
+	max_cpus = sched->max_cpu + 1;
+	ocpu = perf_evsel__intval(evsel, sample, "orig_cpu");
+	dcpu = perf_evsel__intval(evsel, sample, "dest_cpu");
+
+	thread = machine__findnew_thread(machine, sample->pid, sample->tid);
+	if (thread == NULL)
+		return;
+
+	if (timehist_skip_sample(sched, thread) &&
+	    timehist_skip_sample(sched, migrated)) {
+		return;
+	}
+
+	timestamp__scnprintf_usec(sample->time, tstr, sizeof(tstr));
+	printf("%15s [%04d] ", tstr, sample->cpu);
+
+	if (sched->show_cpu_visual) {
+		u32 i;
+		char c;
+
+		printf("  ");
+		for (i = 0; i < max_cpus; ++i) {
+			c = (i == sample->cpu) ? 'm' : ' ';
+			printf("%c", c);
+		}
+		printf("  ");
+	}
+
+	printf(" %-*s ", comm_width, timehist_get_commstr(thread));
+
+	/* dt spacer */
+	printf("  %9s  %9s  %9s ", "", "", "");
+
+	printf("migrated: %s", timehist_get_commstr(migrated));
+	printf(" cpu %d => %d", ocpu, dcpu);
+
+	printf("\n");
+}
+
+static int timehist_migrate_task_event(struct perf_tool *tool,
+				       union perf_event *event __maybe_unused,
+				       struct perf_evsel *evsel,
+				       struct perf_sample *sample,
+				       struct machine *machine)
+{
+	struct perf_sched *sched = container_of(tool, struct perf_sched, tool);
+	struct thread *thread;
+	struct thread_runtime *tr = NULL;
+	/* want pid of migrated task not pid in sample */
+	const u32 pid = perf_evsel__intval(evsel, sample, "pid");
+
+	thread = machine__findnew_thread(machine, 0, pid);
+	if (thread == NULL)
+		return -1;
+
+	tr = thread__get_runtime(thread);
+	if (tr == NULL)
+		return -1;
+
+	tr->migrations++;
+
+	/* show migrations if requested */
+	timehist_print_migration_event(sched, evsel, sample, machine, thread);
+
+	return 0;
+}
+
 static int timehist_sched_change_event(struct perf_tool *tool,
 				       union perf_event *event,
 				       struct perf_evsel *evsel,
@@ -2295,6 +2379,7 @@ static void print_thread_runtime(struct thread *t,
 	print_sched_time(r->run_stats.max, 6);
 	printf("  ");
 	printf("%5.2f", stddev);
+	printf("   %5" PRIu64, r->migrations);
 	printf("\n");
 }
 
@@ -2356,10 +2441,10 @@ static void timehist_print_summary(struct perf_sched *sched,
 
 	printf("\nRuntime summary\n");
 	printf("%*s  parent   sched-in  ", comm_width, "comm");
-	printf("   run-time    min-run     avg-run     max-run  stddev\n");
+	printf("   run-time    min-run     avg-run     max-run  stddev  migrations\n");
 	printf("%*s            (count)  ", comm_width, "");
 	printf("     (msec)     (msec)      (msec)      (msec)       %%\n");
-	printf("%.105s\n", graph_dotted_line);
+	printf("%.117s\n", graph_dotted_line);
 
 	machine__for_each_thread(m, show_thread_runtime, &totals);
 	task_count = totals.task_count;
@@ -2460,6 +2545,9 @@ static int perf_sched__timehist(struct perf_sched *sched)
 		{ "sched:sched_wakeup",	      timehist_sched_wakeup_event, },
 		{ "sched:sched_wakeup_new",   timehist_sched_wakeup_event, },
 	};
+	const struct perf_evsel_str_handler migrate_handlers[] = {
+		{ "sched:sched_migrate_task", timehist_migrate_task_event, },
+	};
 	struct perf_data_file file = {
 		.path = input_name,
 		.mode = PERF_DATA_MODE_READ,
@@ -2507,6 +2595,10 @@ static int perf_sched__timehist(struct perf_sched *sched)
 	if (!perf_session__has_traces(session, "record -R"))
 		goto out;
 
+	if (sched->show_migrations &&
+	    perf_session__set_tracepoints_handlers(session, migrate_handlers))
+		goto out;
+
 	/* pre-allocate struct for per-CPU idle stats */
 	sched->max_cpu = session->header.env.nr_cpus_online;
 	if (sched->max_cpu == 0)
@@ -2903,6 +2995,7 @@ int cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)
 	OPT_BOOLEAN('S', "with-summary", &sched.summary,
 		    "Show all syscalls and summary with statistics"),
 	OPT_BOOLEAN('w', "wakeups", &sched.show_wakeups, "Show wakeup events"),
+	OPT_BOOLEAN('M', "migrations", &sched.show_migrations, "Show migration events"),
 	OPT_BOOLEAN('V', "cpu-visual", &sched.show_cpu_visual, "Add CPU visual"),
 	OPT_PARENT(sched_options)
 	};

commit 8388deb3ba4d36ffcae91a2a01cb2ea6f27553e6
Author: Namhyung Kim <namhyung@kernel.org>
Date:   Thu Nov 24 10:11:14 2016 +0900

    perf sched timehist: Enlarge max stack depth by 2
    
    When it records callchains, they will always have 2 scheduler functions
    (__schedule + schedule or __schedule + preempt_schedule) and get
    ignored.  So it should collect 2 more functions to show the expected
    number of callchains to user.
    
    Committer Notes:
    
    Example of final result, using the same perf.data file as in the
    previous cset comment, but this time redirecting the output of 'perf
    sched timehist' to a file instead of copy'n'pasting from xterm:
    
      [root@jouet experimental]# perf sched timehist > /tmp/bla
      [root@jouet experimental]# cat /tmp/bla
          time  cpu task name        wait time sch delay run time
                     [tid/pid]            (msec) (msec) (msec)
      -------- ----  -------------------- ------ ------ -----
      6.494998 [01] <idle>                0.000  0.000  0.000
      6.495027 [02] perf[519]             0.000  0.000  0.000 schedule_hrtimeout_range_clock <- schedule_hrtimeout_range <- poll_schedule_timeout <- do_sys_poll <- sys_poll
      6.495096 [03] <idle>                0.000  0.000  0.000
      6.495100 [03] rcuos/0[9]            0.000  0.005  0.003 rcu_nocb_kthread <- kthread <- ret_from_fork
      6.495113 [01] perf[520]             0.000  0.008  0.114 preempt_schedule_common <- _cond_resched <- wait_for_completion <- stop_one_cpu <- sched_exec <- do_execveat_common.isra.35
      6.495121 [00] <idle>                0.000  0.000  0.000
      6.495129 [01] migration/1[17]       0.000  0.003  0.016 smpboot_thread_fn <- kthread <- ret_from_fork
      6.496085 [02] <idle>                0.000  0.000  1.057
      6.496096 [02] kworker/u16:1[31169]  0.000  0.004  0.011 worker_thread <- kthread <- ret_from_fork
      6.496096 [03] <idle>                0.003  0.000  0.996
      6.496169 [02] <idle>                0.011  0.000  0.072
      6.496171 [00] ls[520]               0.008  0.000  1.049 do_exit <- do_group_exit <- [unknown] <- entry_SYSCALL_64_fastpath
      6.496172 [03] gnome-terminal-[4391] 0.000  0.003  0.076 schedule_hrtimeout_range_clock <- schedule_hrtimeout_range <- poll_schedule_timeout <- do_sys_poll <- sys_poll
    
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Tested-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Stephane Eranian <eranian@google.com>
    Link: http://lkml.kernel.org/r/20161124011114.7102-3-namhyung@kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 06be809a02ab..a49a032f5b15 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1960,7 +1960,7 @@ static bool is_idle_sample(struct perf_sched *sched,
 		return false;
 
 	if (thread__resolve_callchain(thread, cursor, evsel, sample,
-				      NULL, NULL, sched->max_stack) != 0) {
+				      NULL, NULL, sched->max_stack + 2) != 0) {
 		if (verbose)
 			error("Failed to resolve callchain. Skipping\n");
 

commit cdeb01bf7863718bbbbdac2a2c3a12b62366757a
Author: Namhyung Kim <namhyung@kernel.org>
Date:   Thu Nov 24 10:11:12 2016 +0900

    perf sched timehist: Mark schedule function in callchains
    
    The sched_switch event always captured from the scheduler function.  So
    it'd be great omit them from the callchain.  This patch marks the
    functions to be omitted by later patch.
    
    Committer notes:
    
    Testing it:
    
    Before:
    
      [root@jouet experimental]# perf sched record -g ls
      Dockerfile  perf.data  x-mips64
      [ perf record: Woken up 1 times to write data ]
      [ perf record: Captured and wrote 1.355 MB perf.data (29 samples) ]
      [root@jouet experimental]# perf sched timehist
          time  cpu  task name         wait time sch delay run time
                     [tid/pid]             (msec) (msec) (msec)
      ----------- -----  ----------------- ------ ------ ------
      6.494998 [001] <idle>                0.000  0.000  0.000
      6.495027 [002] perf[519]             0.000  0.000  0.000 __schedule <- schedule <- schedule_hrtimeout_range_clock <- schedule_hrtimeou
      6.495096 [003] <idle>                0.000  0.000  0.000
      6.495100 [003] rcuos/0[9]            0.000  0.005  0.003 __schedule <- schedule <- rcu_nocb_kthread <- kthread <- ret_from_fork
      6.495113 [001] perf[520]             0.000  0.008  0.114 __schedule <- preempt_schedule_common <- _cond_resched <- wait_for_completion
      6.495121 [000] <idle>                0.000  0.000  0.000
      6.495129 [001] migration/1[17]       0.000  0.003  0.016 __schedule <- schedule <- smpboot_thread_fn <- kthread <- ret_from_fork
      6.496085 [002] <idle>                0.000  0.000  1.057
      6.496096 [002] kworker/u16:1[31169]  0.000  0.004  0.011 __schedule <- schedule <- worker_thread <- kthread <- ret_from_fork
      6.496096 [003] <idle>                0.003  0.000  0.996
      6.496169 [002] <idle>                0.011  0.000  0.072
      6.496171 [000] ls[520]               0.008  0.000  1.049 __schedule <- schedule <- do_exit <- do_group_exit <- [unknown]
      6.496172 [003] gnome-terminal-[4391] 0.000  0.003  0.076 __schedule <- schedule <- schedule_hrtimeout_range_clock <- schedule_hrtimeo
    
    After:
    
      [root@jouet experimental]# perf sched timehist
          time  cpu  task name         wait time sch delay run time
                     [tid/pid]            (msec)  (msec)  (msec)
      ----------- -----  ----------------- -----  -----  ------
      6.494998 [001] <idle>                0.000  0.000  0.000
      6.495027 [002] perf[519]             0.000  0.000  0.000 schedule_hrtimeout_range_clock <- schedule_hrtimeout_range <- poll_schedule_t
      6.495096 [003] <idle>                0.000  0.000  0.000
      6.495100 [003] rcuos/0[9]            0.000  0.005  0.003 rcu_nocb_kthread <- kthread <- ret_from_fork
      6.495113 [001] perf[520]             0.000  0.008  0.114 preempt_schedule_common <- _cond_resched <- wait_for_completion <- stop_one_c
      6.495121 [000] <idle>                0.000  0.000  0.000
      6.495129 [001] migration/1[17]       0.000  0.003  0.016 smpboot_thread_fn <- kthread <- ret_from_fork
      6.496085 [002] <idle>                0.000  0.000  1.057
      6.496096 [002] kworker/u16:1[31169]  0.000  0.004  0.011 worker_thread <- kthread <- ret_from_fork
      6.496096 [003] <idle>                0.003  0.000  0.996
      6.496169 [002] <idle>                0.011  0.000  0.072
      6.496171 [000] ls[520]               0.008  0.000  1.049 do_exit <- do_group_exit <- [unknown]
      6.496172 [003] gnome-terminal-[4391] 0.000  0.003  0.076 schedule_hrtimeout_range_clock <- schedule_hrtimeout_range <- poll_schedule_
      [root@jouet experimental]#
    
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Tested-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Stephane Eranian <eranian@google.com>
    Link: http://lkml.kernel.org/r/20161124011114.7102-1-namhyung@kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 43fcc13e402d..06be809a02ab 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1966,7 +1966,28 @@ static bool is_idle_sample(struct perf_sched *sched,
 
 		return false;
 	}
+
 	callchain_cursor_commit(cursor);
+
+	while (true) {
+		struct callchain_cursor_node *node;
+		struct symbol *sym;
+
+		node = callchain_cursor_current(cursor);
+		if (node == NULL)
+			break;
+
+		sym = node->sym;
+		if (sym && sym->name) {
+			if (!strcmp(sym->name, "schedule") ||
+			    !strcmp(sym->name, "__schedule") ||
+			    !strcmp(sym->name, "preempt_schedule"))
+				sym->ignore = 1;
+		}
+
+		callchain_cursor_advance(cursor);
+	}
+
 	return false;
 }
 

commit 2d9bbf6eb3825739efa9e91c256ce7ead60d8367
Author: Namhyung Kim <namhyung@kernel.org>
Date:   Thu Nov 24 10:11:13 2016 +0900

    perf callchain: Add option to skip ignore symbol when printing callchains
    
    For tracepoint events, callchains always contain certain functions.
    Sometimes it'd be better to skip those functions as they have no value.
    
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Tested-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Stephane Eranian <eranian@google.com>
    Link: http://lkml.kernel.org/r/20161124011114.7102-2-namhyung@kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 829468defa07..43fcc13e402d 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1876,7 +1876,8 @@ static void timehist_print_sample(struct perf_sched *sched,
 
 	sample__fprintf_sym(sample, al, 0,
 			    EVSEL__PRINT_SYM | EVSEL__PRINT_ONELINE |
-			    EVSEL__PRINT_CALLCHAIN_ARROW,
+			    EVSEL__PRINT_CALLCHAIN_ARROW |
+			    EVSEL__PRINT_SKIP_IGNORED,
 			    &callchain_cursor, stdout);
 
 out:

commit a407b0678bc1c39d70af5fdbe6421c164b69a8c0
Author: David Ahern <dsahern@gmail.com>
Date:   Wed Nov 16 15:06:33 2016 +0900

    perf sched timehist: Add -V/--cpu-visual option
    
    The -V option provides a visual aid for sched switches by cpu:
    
      $ perf sched timehist -V
                 time    cpu  0123456789abc  task name              b/n time  sch delay   run time
                                             [tid/pid]                (msec)     (msec)     (msec)
      --------------- ------  -------------  --------------------  ---------  ---------  ---------
      ...
       2412598.429696 [0009]           i     <idle>                    0.000      0.000      0.000
       2412598.429767 [0002]    s            perf[7219]                0.000      0.000      0.000
       2412598.429783 [0009]           s     perf[7220]                0.000      0.006      0.087
       2412598.429794 [0010]            i    <idle>                    0.000      0.000      0.000
       2412598.429795 [0009]           s     migration/9[53]           0.000      0.003      0.011
       2412598.430370 [0010]            s    sleep[7220]               0.011      0.000      0.576
       2412598.432584 [0003]     i           <idle>                    0.000      0.000      0.000
      ...
    
    Committer notes:
    
    'i' marks idle time, 's' are scheduler events.
    
    Signed-off-by: David Ahern <dsahern@gmail.com>
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Jiri Olsa <jolsa@kernel.org>
    Tested-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Stephane Eranian <eranian@google.com>
    Link: http://lkml.kernel.org/r/20161116060634.28477-8-namhyung@kernel.org
    [ Add documentation based on above commit message ]
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 1f8731640809..829468defa07 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -201,6 +201,7 @@ struct perf_sched {
 	bool		summary_only;
 	bool		show_callchain;
 	unsigned int	max_stack;
+	bool		show_cpu_visual;
 	bool		show_wakeups;
 	u64		skipped_samples;
 };
@@ -1783,10 +1784,23 @@ static char *timehist_get_commstr(struct thread *thread)
 	return str;
 }
 
-static void timehist_header(void)
+static void timehist_header(struct perf_sched *sched)
 {
+	u32 ncpus = sched->max_cpu + 1;
+	u32 i, j;
+
 	printf("%15s %6s ", "time", "cpu");
 
+	if (sched->show_cpu_visual) {
+		printf(" ");
+		for (i = 0, j = 0; i < ncpus; ++i) {
+			printf("%x", j++);
+			if (j > 15)
+				j = 0;
+		}
+		printf(" ");
+	}
+
 	printf(" %-20s  %9s  %9s  %9s",
 		"task name", "wait time", "sch delay", "run time");
 
@@ -1797,6 +1811,9 @@ static void timehist_header(void)
 	 */
 	printf("%15s %-6s ", "", "");
 
+	if (sched->show_cpu_visual)
+		printf(" %*s ", ncpus, "");
+
 	printf(" %-20s  %9s  %9s  %9s\n", "[tid/pid]", "(msec)", "(msec)", "(msec)");
 
 	/*
@@ -1804,6 +1821,9 @@ static void timehist_header(void)
 	 */
 	printf("%.15s %.6s ", graph_dotted_line, graph_dotted_line);
 
+	if (sched->show_cpu_visual)
+		printf(" %.*s ", ncpus, graph_dotted_line);
+
 	printf(" %.20s  %.9s  %.9s  %.9s",
 		graph_dotted_line, graph_dotted_line, graph_dotted_line,
 		graph_dotted_line);
@@ -1817,11 +1837,28 @@ static void timehist_print_sample(struct perf_sched *sched,
 				  struct thread *thread)
 {
 	struct thread_runtime *tr = thread__priv(thread);
+	u32 max_cpus = sched->max_cpu + 1;
 	char tstr[64];
 
 	timestamp__scnprintf_usec(sample->time, tstr, sizeof(tstr));
 	printf("%15s [%04d] ", tstr, sample->cpu);
 
+	if (sched->show_cpu_visual) {
+		u32 i;
+		char c;
+
+		printf(" ");
+		for (i = 0; i < max_cpus; ++i) {
+			/* flag idle times with 'i'; others are sched events */
+			if (i == sample->cpu)
+				c = (thread->tid == 0) ? 'i' : 's';
+			else
+				c = ' ';
+			printf("%c", c);
+		}
+		printf(" ");
+	}
+
 	printf(" %-*s ", comm_width, timehist_get_commstr(thread));
 
 	print_sched_time(tr->dt_wait, 6);
@@ -2095,6 +2132,8 @@ static void timehist_print_wakeup_event(struct perf_sched *sched,
 
 	timestamp__scnprintf_usec(sample->time, tstr, sizeof(tstr));
 	printf("%15s [%04d] ", tstr, sample->cpu);
+	if (sched->show_cpu_visual)
+		printf(" %*s ", sched->max_cpu + 1, "");
 
 	printf(" %-*s ", comm_width, timehist_get_commstr(thread));
 
@@ -2458,7 +2497,7 @@ static int perf_sched__timehist(struct perf_sched *sched)
 		sched->summary = sched->summary_only;
 
 	if (!sched->summary_only)
-		timehist_header();
+		timehist_header(sched);
 
 	err = perf_session__process_events(session);
 	if (err) {
@@ -2842,6 +2881,7 @@ int cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)
 	OPT_BOOLEAN('S', "with-summary", &sched.summary,
 		    "Show all syscalls and summary with statistics"),
 	OPT_BOOLEAN('w', "wakeups", &sched.show_wakeups, "Show wakeup events"),
+	OPT_BOOLEAN('V', "cpu-visual", &sched.show_cpu_visual, "Add CPU visual"),
 	OPT_PARENT(sched_options)
 	};
 

commit 6c973c90852c3fa4b4e76a061ce89dcd373efccc
Author: David Ahern <dsahern@gmail.com>
Date:   Wed Nov 16 15:06:32 2016 +0900

    perf sched timehist: Add call graph options
    
    If callchains were recorded they are appended to the line with a default stack depth of 5:
    
      1.874569 [0011] gcc[31949]       0.014 0.000 1.148 wait_for_completion_killable <- do_fork <- sys_vfork <- stub_vfork <- __vfork
      1.874591 [0010] gcc[31951]       0.000 0.000 0.024 __cond_resched <- _cond_resched <- wait_for_completion <- stop_one_cpu <- sched_exec
      1.874603 [0010] migration/10[59] 3.350 0.004 0.011 smpboot_thread_fn <- kthread <- ret_from_fork
      1.874604 [0011] <idle>           1.148 0.000 0.035 cpu_startup_entry <- start_secondary
      1.874723 [0005] <idle>           0.016 0.000 1.383 cpu_startup_entry <- start_secondary
      1.874746 [0005] gcc[31949]       0.153 0.078 0.022 do_wait sys_wait4 <- system_call_fastpath <- __GI___waitpid
    
     --no-call-graph can be used to not show the callchains. --max-stack is used
    to control the number of frames shown (default of 5). -x/--excl options can
    be used to collapse redundant callchains to get more relevant data on screen.
    
    Signed-off-by: David Ahern <dsahern@gmail.com>
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Jiri Olsa <jolsa@kernel.org>
    Tested-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Stephane Eranian <eranian@google.com>
    Link: http://lkml.kernel.org/r/20161116060634.28477-7-namhyung@kernel.org
    [ Add documentation based on above commit message ]
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 8fb7bcc2cb76..1f8731640809 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -14,6 +14,7 @@
 #include "util/thread_map.h"
 #include "util/color.h"
 #include "util/stat.h"
+#include "util/callchain.h"
 
 #include <subcmd/parse-options.h>
 #include "util/trace-event.h"
@@ -198,6 +199,8 @@ struct perf_sched {
 	/* options for timehist command */
 	bool		summary;
 	bool		summary_only;
+	bool		show_callchain;
+	unsigned int	max_stack;
 	bool		show_wakeups;
 	u64		skipped_samples;
 };
@@ -1810,6 +1813,7 @@ static void timehist_header(void)
 
 static void timehist_print_sample(struct perf_sched *sched,
 				  struct perf_sample *sample,
+				  struct addr_location *al,
 				  struct thread *thread)
 {
 	struct thread_runtime *tr = thread__priv(thread);
@@ -1827,6 +1831,18 @@ static void timehist_print_sample(struct perf_sched *sched,
 	if (sched->show_wakeups)
 		printf("  %-*s", comm_width, "");
 
+	if (thread->tid == 0)
+		goto out;
+
+	if (sched->show_callchain)
+		printf("  ");
+
+	sample__fprintf_sym(sample, al, 0,
+			    EVSEL__PRINT_SYM | EVSEL__PRINT_ONELINE |
+			    EVSEL__PRINT_CALLCHAIN_ARROW,
+			    &callchain_cursor, stdout);
+
+out:
 	printf("\n");
 }
 
@@ -1878,9 +1894,14 @@ static void timehist_update_runtime_stats(struct thread_runtime *r,
 	r->total_run_time += r->dt_run;
 }
 
-static bool is_idle_sample(struct perf_sample *sample,
-			   struct perf_evsel *evsel)
+static bool is_idle_sample(struct perf_sched *sched,
+			   struct perf_sample *sample,
+			   struct perf_evsel *evsel,
+			   struct machine *machine)
 {
+	struct thread *thread;
+	struct callchain_cursor *cursor = &callchain_cursor;
+
 	/* pid 0 == swapper == idle task */
 	if (sample->pid == 0)
 		return true;
@@ -1889,6 +1910,25 @@ static bool is_idle_sample(struct perf_sample *sample,
 		if (perf_evsel__intval(evsel, sample, "prev_pid") == 0)
 			return true;
 	}
+
+	/* want main thread for process - has maps */
+	thread = machine__findnew_thread(machine, sample->pid, sample->pid);
+	if (thread == NULL) {
+		pr_debug("Failed to get thread for pid %d.\n", sample->pid);
+		return false;
+	}
+
+	if (!symbol_conf.use_callchain || sample->callchain == NULL)
+		return false;
+
+	if (thread__resolve_callchain(thread, cursor, evsel, sample,
+				      NULL, NULL, sched->max_stack) != 0) {
+		if (verbose)
+			error("Failed to resolve callchain. Skipping\n");
+
+		return false;
+	}
+	callchain_cursor_commit(cursor);
 	return false;
 }
 
@@ -1999,13 +2039,14 @@ static struct thread_runtime *thread__get_runtime(struct thread *thread)
 	return tr;
 }
 
-static struct thread *timehist_get_thread(struct perf_sample *sample,
+static struct thread *timehist_get_thread(struct perf_sched *sched,
+					  struct perf_sample *sample,
 					  struct machine *machine,
 					  struct perf_evsel *evsel)
 {
 	struct thread *thread;
 
-	if (is_idle_sample(sample, evsel)) {
+	if (is_idle_sample(sched, sample, evsel, machine)) {
 		thread = get_idle_thread(sample->cpu);
 		if (thread == NULL)
 			pr_err("Failed to get idle thread for cpu %d.\n", sample->cpu);
@@ -2115,7 +2156,7 @@ static int timehist_sched_change_event(struct perf_tool *tool,
 		goto out;
 	}
 
-	thread = timehist_get_thread(sample, machine, evsel);
+	thread = timehist_get_thread(sched, sample, machine, evsel);
 	if (thread == NULL) {
 		rc = -1;
 		goto out;
@@ -2134,7 +2175,7 @@ static int timehist_sched_change_event(struct perf_tool *tool,
 
 	timehist_update_runtime_stats(tr, sample->time, tprev);
 	if (!sched->summary_only)
-		timehist_print_sample(sched, sample, thread);
+		timehist_print_sample(sched, sample, &al, thread);
 
 out:
 	if (tr) {
@@ -2327,6 +2368,30 @@ static int perf_timehist__process_sample(struct perf_tool *tool,
 	return err;
 }
 
+static int timehist_check_attr(struct perf_sched *sched,
+			       struct perf_evlist *evlist)
+{
+	struct perf_evsel *evsel;
+	struct evsel_runtime *er;
+
+	list_for_each_entry(evsel, &evlist->entries, node) {
+		er = perf_evsel__get_runtime(evsel);
+		if (er == NULL) {
+			pr_err("Failed to allocate memory for evsel runtime data\n");
+			return -1;
+		}
+
+		if (sched->show_callchain &&
+		    !(evsel->attr.sample_type & PERF_SAMPLE_CALLCHAIN)) {
+			pr_info("Samples do not have callchains.\n");
+			sched->show_callchain = 0;
+			symbol_conf.use_callchain = 0;
+		}
+	}
+
+	return 0;
+}
+
 static int perf_sched__timehist(struct perf_sched *sched)
 {
 	const struct perf_evsel_str_handler handlers[] = {
@@ -2359,6 +2424,8 @@ static int perf_sched__timehist(struct perf_sched *sched)
 	sched->tool.ordered_events = true;
 	sched->tool.ordering_requires_timestamps = true;
 
+	symbol_conf.use_callchain = sched->show_callchain;
+
 	session = perf_session__new(&file, false, &sched->tool);
 	if (session == NULL)
 		return -ENOMEM;
@@ -2367,6 +2434,9 @@ static int perf_sched__timehist(struct perf_sched *sched)
 
 	symbol__init(&session->header.env);
 
+	if (timehist_check_attr(sched, evlist) != 0)
+		goto out;
+
 	setup_pager();
 
 	/* setup per-evsel handlers */
@@ -2714,6 +2784,8 @@ int cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)
 		.next_shortname1      = 'A',
 		.next_shortname2      = '0',
 		.skip_merge           = 0,
+		.show_callchain	      = 1,
+		.max_stack            = 5,
 	};
 	const struct option sched_options[] = {
 	OPT_STRING('i', "input", &input_name, "file",
@@ -2759,6 +2831,10 @@ int cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)
 		   "file", "vmlinux pathname"),
 	OPT_STRING(0, "kallsyms", &symbol_conf.kallsyms_name,
 		   "file", "kallsyms pathname"),
+	OPT_BOOLEAN('g', "call-graph", &sched.show_callchain,
+		    "Display call chains if present (default on)"),
+	OPT_UINTEGER(0, "max-stack", &sched.max_stack,
+		   "Maximum number of functions to display backtrace."),
 	OPT_STRING(0, "symfs", &symbol_conf.symfs, "directory",
 		    "Look for files with symbols relative to this directory"),
 	OPT_BOOLEAN('s', "summary", &sched.summary_only,

commit fc1469f1b20777929a47f6f8e55bca12c6f142ac
Author: David Ahern <dsahern@gmail.com>
Date:   Wed Nov 16 15:06:31 2016 +0900

    perf sched timehist: Add -w/--wakeups option
    
    The -w option is to show wakeup events with timehist.
    
      $ perf sched timehist -w
                 time    cpu  task name              b/n time  sch delay   run time
                              [tid/pid]                (msec)     (msec)     (msec)
      --------------- ------  --------------------  ---------  ---------  ---------
       2412598.429689 [0002]  perf[7219]                                             awakened: perf[7220]
       2412598.429696 [0009]  <idle>                    0.000      0.000      0.000
       2412598.429767 [0002]  perf[7219]                0.000      0.000      0.000
       2412598.429780 [0009]  perf[7220]                                             awakened: migration/9[53]
      ...
    
    Signed-off-by: David Ahern <dsahern@gmail.com>
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Jiri Olsa <jolsa@kernel.org>
    Tested-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Stephane Eranian <eranian@google.com>
    Link: http://lkml.kernel.org/r/20161116060634.28477-6-namhyung@kernel.org
    [ Add documentation based on above commit message ]
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 1e7d81ad5ec6..8fb7bcc2cb76 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -198,6 +198,7 @@ struct perf_sched {
 	/* options for timehist command */
 	bool		summary;
 	bool		summary_only;
+	bool		show_wakeups;
 	u64		skipped_samples;
 };
 
@@ -1807,7 +1808,8 @@ static void timehist_header(void)
 	printf("\n");
 }
 
-static void timehist_print_sample(struct perf_sample *sample,
+static void timehist_print_sample(struct perf_sched *sched,
+				  struct perf_sample *sample,
 				  struct thread *thread)
 {
 	struct thread_runtime *tr = thread__priv(thread);
@@ -1821,6 +1823,10 @@ static void timehist_print_sample(struct perf_sample *sample,
 	print_sched_time(tr->dt_wait, 6);
 	print_sched_time(tr->dt_delay, 6);
 	print_sched_time(tr->dt_run, 6);
+
+	if (sched->show_wakeups)
+		printf("  %-*s", comm_width, "");
+
 	printf("\n");
 }
 
@@ -2028,12 +2034,44 @@ static bool timehist_skip_sample(struct perf_sched *sched,
 	return rc;
 }
 
-static int timehist_sched_wakeup_event(struct perf_tool *tool __maybe_unused,
+static void timehist_print_wakeup_event(struct perf_sched *sched,
+					struct perf_sample *sample,
+					struct machine *machine,
+					struct thread *awakened)
+{
+	struct thread *thread;
+	char tstr[64];
+
+	thread = machine__findnew_thread(machine, sample->pid, sample->tid);
+	if (thread == NULL)
+		return;
+
+	/* show wakeup unless both awakee and awaker are filtered */
+	if (timehist_skip_sample(sched, thread) &&
+	    timehist_skip_sample(sched, awakened)) {
+		return;
+	}
+
+	timestamp__scnprintf_usec(sample->time, tstr, sizeof(tstr));
+	printf("%15s [%04d] ", tstr, sample->cpu);
+
+	printf(" %-*s ", comm_width, timehist_get_commstr(thread));
+
+	/* dt spacer */
+	printf("  %9s  %9s  %9s ", "", "", "");
+
+	printf("awakened: %s", timehist_get_commstr(awakened));
+
+	printf("\n");
+}
+
+static int timehist_sched_wakeup_event(struct perf_tool *tool,
 				       union perf_event *event __maybe_unused,
 				       struct perf_evsel *evsel,
 				       struct perf_sample *sample,
 				       struct machine *machine)
 {
+	struct perf_sched *sched = container_of(tool, struct perf_sched, tool);
 	struct thread *thread;
 	struct thread_runtime *tr = NULL;
 	/* want pid of awakened task not pid in sample */
@@ -2050,6 +2088,10 @@ static int timehist_sched_wakeup_event(struct perf_tool *tool __maybe_unused,
 	if (tr->ready_to_run == 0)
 		tr->ready_to_run = sample->time;
 
+	/* show wakeups if requested */
+	if (sched->show_wakeups)
+		timehist_print_wakeup_event(sched, sample, machine, thread);
+
 	return 0;
 }
 
@@ -2059,12 +2101,12 @@ static int timehist_sched_change_event(struct perf_tool *tool,
 				       struct perf_sample *sample,
 				       struct machine *machine)
 {
+	struct perf_sched *sched = container_of(tool, struct perf_sched, tool);
 	struct addr_location al;
 	struct thread *thread;
 	struct thread_runtime *tr = NULL;
 	u64 tprev;
 	int rc = 0;
-	struct perf_sched *sched = container_of(tool, struct perf_sched, tool);
 
 	if (machine__resolve(machine, &al, sample) < 0) {
 		pr_err("problem processing %d event. skipping it\n",
@@ -2092,7 +2134,7 @@ static int timehist_sched_change_event(struct perf_tool *tool,
 
 	timehist_update_runtime_stats(tr, sample->time, tprev);
 	if (!sched->summary_only)
-		timehist_print_sample(sample, thread);
+		timehist_print_sample(sched, sample, thread);
 
 out:
 	if (tr) {
@@ -2723,6 +2765,7 @@ int cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)
 		    "Show only syscall summary with statistics"),
 	OPT_BOOLEAN('S', "with-summary", &sched.summary,
 		    "Show all syscalls and summary with statistics"),
+	OPT_BOOLEAN('w', "wakeups", &sched.show_wakeups, "Show wakeup events"),
 	OPT_PARENT(sched_options)
 	};
 
@@ -2814,6 +2857,13 @@ int cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)
 			if (argc)
 				usage_with_options(timehist_usage, timehist_options);
 		}
+		if (sched.show_wakeups && sched.summary_only) {
+			pr_err(" Error: -s and -w are mutually exclusive.\n");
+			parse_options_usage(timehist_usage, timehist_options, "s", true);
+			parse_options_usage(NULL, timehist_options, "w", true);
+			return -EINVAL;
+		}
+
 		return perf_sched__timehist(&sched);
 	} else {
 		usage_with_options(sched_usage, sched_options);

commit 52df138caaa8daaaf4e0dc64428ef06f303a8dbf
Author: David Ahern <dsahern@gmail.com>
Date:   Wed Nov 16 15:06:30 2016 +0900

    perf sched timehist: Add summary options
    
    The -s/--summary option is to show process runtime statistics.  And the
     -S/--with-summary option is to show the stats with the normal output.
    
      $ perf sched timehist -s
    
      Runtime summary
                                comm  parent   sched-in     run-time    min-run     avg-run     max-run  stddev
                                                (count)       (msec)     (msec)      (msec)      (msec)       %
      ---------------------------------------------------------------------------------------------------------
                      ksoftirqd/0[3]       2          2        0.011      0.004       0.005       0.006   14.87
                      rcu_preempt[7]       2         11        0.071      0.002       0.006       0.017   20.23
                      watchdog/0[11]       2          1        0.002      0.002       0.002       0.002    0.00
                      watchdog/1[12]       2          1        0.004      0.004       0.004       0.004    0.00
      ...
    
      Terminated tasks:
                         sleep[7220]    7219          3        0.770      0.087       0.256       0.576   62.28
    
      Idle stats:
          CPU  0 idle for   2352.006  msec
          CPU  1 idle for   2764.497  msec
          CPU  2 idle for   2998.229  msec
          CPU  3 idle for   2967.800  msec
    
          Total number of unique tasks: 52
      Total number of context switches: 2532
                 Total run time (msec): 218.036
    
    Signed-off-by: David Ahern <dsahern@gmail.com>
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Jiri Olsa <jolsa@kernel.org>
    Tested-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Stephane Eranian <eranian@google.com>
    Link: http://lkml.kernel.org/r/20161116060634.28477-5-namhyung@kernel.org
    [ Add documentation from last commit, so that docs comes with the cset that introduces the feature ]
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index c0ac0c9557e8..1e7d81ad5ec6 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -194,6 +194,11 @@ struct perf_sched {
 	bool force;
 	bool skip_merge;
 	struct perf_sched_map map;
+
+	/* options for timehist command */
+	bool		summary;
+	bool		summary_only;
+	u64		skipped_samples;
 };
 
 /* per thread run time data */
@@ -2010,12 +2015,15 @@ static struct thread *timehist_get_thread(struct perf_sample *sample,
 	return thread;
 }
 
-static bool timehist_skip_sample(struct thread *thread)
+static bool timehist_skip_sample(struct perf_sched *sched,
+				 struct thread *thread)
 {
 	bool rc = false;
 
-	if (thread__is_filtered(thread))
+	if (thread__is_filtered(thread)) {
 		rc = true;
+		sched->skipped_samples++;
+	}
 
 	return rc;
 }
@@ -2045,7 +2053,7 @@ static int timehist_sched_wakeup_event(struct perf_tool *tool __maybe_unused,
 	return 0;
 }
 
-static int timehist_sched_change_event(struct perf_tool *tool __maybe_unused,
+static int timehist_sched_change_event(struct perf_tool *tool,
 				       union perf_event *event,
 				       struct perf_evsel *evsel,
 				       struct perf_sample *sample,
@@ -2056,6 +2064,7 @@ static int timehist_sched_change_event(struct perf_tool *tool __maybe_unused,
 	struct thread_runtime *tr = NULL;
 	u64 tprev;
 	int rc = 0;
+	struct perf_sched *sched = container_of(tool, struct perf_sched, tool);
 
 	if (machine__resolve(machine, &al, sample) < 0) {
 		pr_err("problem processing %d event. skipping it\n",
@@ -2070,7 +2079,7 @@ static int timehist_sched_change_event(struct perf_tool *tool __maybe_unused,
 		goto out;
 	}
 
-	if (timehist_skip_sample(thread))
+	if (timehist_skip_sample(sched, thread))
 		goto out;
 
 	tr = thread__get_runtime(thread);
@@ -2082,7 +2091,8 @@ static int timehist_sched_change_event(struct perf_tool *tool __maybe_unused,
 	tprev = perf_evsel__get_time(evsel, sample->cpu);
 
 	timehist_update_runtime_stats(tr, sample->time, tprev);
-	timehist_print_sample(sample, thread);
+	if (!sched->summary_only)
+		timehist_print_sample(sample, thread);
 
 out:
 	if (tr) {
@@ -2122,6 +2132,131 @@ static int process_lost(struct perf_tool *tool __maybe_unused,
 }
 
 
+static void print_thread_runtime(struct thread *t,
+				 struct thread_runtime *r)
+{
+	double mean = avg_stats(&r->run_stats);
+	float stddev;
+
+	printf("%*s   %5d  %9" PRIu64 " ",
+	       comm_width, timehist_get_commstr(t), t->ppid,
+	       (u64) r->run_stats.n);
+
+	print_sched_time(r->total_run_time, 8);
+	stddev = rel_stddev_stats(stddev_stats(&r->run_stats), mean);
+	print_sched_time(r->run_stats.min, 6);
+	printf(" ");
+	print_sched_time((u64) mean, 6);
+	printf(" ");
+	print_sched_time(r->run_stats.max, 6);
+	printf("  ");
+	printf("%5.2f", stddev);
+	printf("\n");
+}
+
+struct total_run_stats {
+	u64  sched_count;
+	u64  task_count;
+	u64  total_run_time;
+};
+
+static int __show_thread_runtime(struct thread *t, void *priv)
+{
+	struct total_run_stats *stats = priv;
+	struct thread_runtime *r;
+
+	if (thread__is_filtered(t))
+		return 0;
+
+	r = thread__priv(t);
+	if (r && r->run_stats.n) {
+		stats->task_count++;
+		stats->sched_count += r->run_stats.n;
+		stats->total_run_time += r->total_run_time;
+		print_thread_runtime(t, r);
+	}
+
+	return 0;
+}
+
+static int show_thread_runtime(struct thread *t, void *priv)
+{
+	if (t->dead)
+		return 0;
+
+	return __show_thread_runtime(t, priv);
+}
+
+static int show_deadthread_runtime(struct thread *t, void *priv)
+{
+	if (!t->dead)
+		return 0;
+
+	return __show_thread_runtime(t, priv);
+}
+
+static void timehist_print_summary(struct perf_sched *sched,
+				   struct perf_session *session)
+{
+	struct machine *m = &session->machines.host;
+	struct total_run_stats totals;
+	u64 task_count;
+	struct thread *t;
+	struct thread_runtime *r;
+	int i;
+
+	memset(&totals, 0, sizeof(totals));
+
+	if (comm_width < 30)
+		comm_width = 30;
+
+	printf("\nRuntime summary\n");
+	printf("%*s  parent   sched-in  ", comm_width, "comm");
+	printf("   run-time    min-run     avg-run     max-run  stddev\n");
+	printf("%*s            (count)  ", comm_width, "");
+	printf("     (msec)     (msec)      (msec)      (msec)       %%\n");
+	printf("%.105s\n", graph_dotted_line);
+
+	machine__for_each_thread(m, show_thread_runtime, &totals);
+	task_count = totals.task_count;
+	if (!task_count)
+		printf("<no still running tasks>\n");
+
+	printf("\nTerminated tasks:\n");
+	machine__for_each_thread(m, show_deadthread_runtime, &totals);
+	if (task_count == totals.task_count)
+		printf("<no terminated tasks>\n");
+
+	/* CPU idle stats not tracked when samples were skipped */
+	if (sched->skipped_samples)
+		return;
+
+	printf("\nIdle stats:\n");
+	for (i = 0; i <= idle_max_cpu; ++i) {
+		t = idle_threads[i];
+		if (!t)
+			continue;
+
+		r = thread__priv(t);
+		if (r && r->run_stats.n) {
+			totals.sched_count += r->run_stats.n;
+			printf("    CPU %2d idle for ", i);
+			print_sched_time(r->total_run_time, 6);
+			printf(" msec\n");
+		} else
+			printf("    CPU %2d idle entire time window\n", i);
+	}
+
+	printf("\n"
+	       "    Total number of unique tasks: %" PRIu64 "\n"
+	       "Total number of context switches: %" PRIu64 "\n"
+	       "           Total run time (msec): ",
+	       totals.task_count, totals.sched_count);
+
+	print_sched_time(totals.total_run_time, 2);
+	printf("\n");
+}
+
 typedef int (*sched_handler)(struct perf_tool *tool,
 			  union perf_event *event,
 			  struct perf_evsel *evsel,
@@ -2163,6 +2298,7 @@ static int perf_sched__timehist(struct perf_sched *sched)
 	};
 
 	struct perf_session *session;
+	struct perf_evlist *evlist;
 	int err = -1;
 
 	/*
@@ -2185,6 +2321,8 @@ static int perf_sched__timehist(struct perf_sched *sched)
 	if (session == NULL)
 		return -ENOMEM;
 
+	evlist = session->evlist;
+
 	symbol__init(&session->header.env);
 
 	setup_pager();
@@ -2203,7 +2341,12 @@ static int perf_sched__timehist(struct perf_sched *sched)
 	if (init_idle_threads(sched->max_cpu))
 		goto out;
 
-	timehist_header();
+	/* summary_only implies summary option, but don't overwrite summary if set */
+	if (sched->summary_only)
+		sched->summary = sched->summary_only;
+
+	if (!sched->summary_only)
+		timehist_header();
 
 	err = perf_session__process_events(session);
 	if (err) {
@@ -2211,6 +2354,13 @@ static int perf_sched__timehist(struct perf_sched *sched)
 		goto out;
 	}
 
+	sched->nr_events      = evlist->stats.nr_events[0];
+	sched->nr_lost_events = evlist->stats.total_lost;
+	sched->nr_lost_chunks = evlist->stats.nr_events[PERF_RECORD_LOST];
+
+	if (sched->summary)
+		timehist_print_summary(sched, session);
+
 out:
 	free_idle_threads();
 	perf_session__delete(session);
@@ -2569,6 +2719,10 @@ int cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)
 		   "file", "kallsyms pathname"),
 	OPT_STRING(0, "symfs", &symbol_conf.symfs, "directory",
 		    "Look for files with symbols relative to this directory"),
+	OPT_BOOLEAN('s', "summary", &sched.summary_only,
+		    "Show only syscall summary with statistics"),
+	OPT_BOOLEAN('S', "with-summary", &sched.summary,
+		    "Show all syscalls and summary with statistics"),
 	OPT_PARENT(sched_options)
 	};
 

commit 49394a2a24c78ce034fe0e732b1b54922c23fd92
Author: David Ahern <dsahern@gmail.com>
Date:   Wed Nov 16 15:06:29 2016 +0900

    perf sched timehist: Introduce timehist command
    
    'perf sched timehist' provides an analysis of scheduling events.
    
    Example usage:
        perf sched record -- sleep 1
        perf sched timehist
    
    By default it shows the individual schedule events, including the wait
    time (time between sched-out and next sched-in events for the task), the
    task scheduling delay (time between wakeup and actually running) and run
    time for the task:
    
                time    cpu  task name             wait time  sch delay   run time
                             [tid/pid]                (msec)     (msec)     (msec)
      -------------- ------  --------------------  ---------  ---------  ---------
        79371.874569 [0011]  gcc[31949]                0.014      0.000      1.148
        79371.874591 [0010]  gcc[31951]                0.000      0.000      0.024
        79371.874603 [0010]  migration/10[59]          3.350      0.004      0.011
        79371.874604 [0011]  <idle>                    1.148      0.000      0.035
        79371.874723 [0005]  <idle>                    0.016      0.000      1.383
        79371.874746 [0005]  gcc[31949]                0.153      0.078      0.022
    ...
    
    Times are in msec.usec.
    
    Committer note:
    
    Add above explanation as the 'perf sched timehist' entry for 'man
    perf-sched'.
    
    Signed-off-by: David Ahern <dsahern@gmail.com>
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Jiri Olsa <jolsa@kernel.org>
    Tested-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Stephane Eranian <eranian@google.com>
    Link: http://lkml.kernel.org/r/20161116060634.28477-4-namhyung@kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index fb3441211e4b..c0ac0c9557e8 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -13,12 +13,14 @@
 #include "util/cloexec.h"
 #include "util/thread_map.h"
 #include "util/color.h"
+#include "util/stat.h"
 
 #include <subcmd/parse-options.h>
 #include "util/trace-event.h"
 
 #include "util/debug.h"
 
+#include <linux/log2.h>
 #include <sys/prctl.h>
 #include <sys/resource.h>
 
@@ -194,6 +196,29 @@ struct perf_sched {
 	struct perf_sched_map map;
 };
 
+/* per thread run time data */
+struct thread_runtime {
+	u64 last_time;      /* time of previous sched in/out event */
+	u64 dt_run;         /* run time */
+	u64 dt_wait;        /* time between CPU access (off cpu) */
+	u64 dt_delay;       /* time between wakeup and sched-in */
+	u64 ready_to_run;   /* time of wakeup */
+
+	struct stats run_stats;
+	u64 total_run_time;
+};
+
+/* per event run time data */
+struct evsel_runtime {
+	u64 *last_time; /* time this event was last seen per cpu */
+	u32 ncpu;       /* highest cpu slot allocated */
+};
+
+/* track idle times per cpu */
+static struct thread **idle_threads;
+static int idle_max_cpu;
+static char idle_comm[] = "<idle>";
+
 static u64 get_nsecs(void)
 {
 	struct timespec ts;
@@ -1654,6 +1679,546 @@ static int perf_sched__read_events(struct perf_sched *sched)
 	return rc;
 }
 
+/*
+ * scheduling times are printed as msec.usec
+ */
+static inline void print_sched_time(unsigned long long nsecs, int width)
+{
+	unsigned long msecs;
+	unsigned long usecs;
+
+	msecs  = nsecs / NSEC_PER_MSEC;
+	nsecs -= msecs * NSEC_PER_MSEC;
+	usecs  = nsecs / NSEC_PER_USEC;
+	printf("%*lu.%03lu ", width, msecs, usecs);
+}
+
+/*
+ * returns runtime data for event, allocating memory for it the
+ * first time it is used.
+ */
+static struct evsel_runtime *perf_evsel__get_runtime(struct perf_evsel *evsel)
+{
+	struct evsel_runtime *r = evsel->priv;
+
+	if (r == NULL) {
+		r = zalloc(sizeof(struct evsel_runtime));
+		evsel->priv = r;
+	}
+
+	return r;
+}
+
+/*
+ * save last time event was seen per cpu
+ */
+static void perf_evsel__save_time(struct perf_evsel *evsel,
+				  u64 timestamp, u32 cpu)
+{
+	struct evsel_runtime *r = perf_evsel__get_runtime(evsel);
+
+	if (r == NULL)
+		return;
+
+	if ((cpu >= r->ncpu) || (r->last_time == NULL)) {
+		int i, n = __roundup_pow_of_two(cpu+1);
+		void *p = r->last_time;
+
+		p = realloc(r->last_time, n * sizeof(u64));
+		if (!p)
+			return;
+
+		r->last_time = p;
+		for (i = r->ncpu; i < n; ++i)
+			r->last_time[i] = (u64) 0;
+
+		r->ncpu = n;
+	}
+
+	r->last_time[cpu] = timestamp;
+}
+
+/* returns last time this event was seen on the given cpu */
+static u64 perf_evsel__get_time(struct perf_evsel *evsel, u32 cpu)
+{
+	struct evsel_runtime *r = perf_evsel__get_runtime(evsel);
+
+	if ((r == NULL) || (r->last_time == NULL) || (cpu >= r->ncpu))
+		return 0;
+
+	return r->last_time[cpu];
+}
+
+static int comm_width = 20;
+
+static char *timehist_get_commstr(struct thread *thread)
+{
+	static char str[32];
+	const char *comm = thread__comm_str(thread);
+	pid_t tid = thread->tid;
+	pid_t pid = thread->pid_;
+	int n;
+
+	if (pid == 0)
+		n = scnprintf(str, sizeof(str), "%s", comm);
+
+	else if (tid != pid)
+		n = scnprintf(str, sizeof(str), "%s[%d/%d]", comm, tid, pid);
+
+	else
+		n = scnprintf(str, sizeof(str), "%s[%d]", comm, tid);
+
+	if (n > comm_width)
+		comm_width = n;
+
+	return str;
+}
+
+static void timehist_header(void)
+{
+	printf("%15s %6s ", "time", "cpu");
+
+	printf(" %-20s  %9s  %9s  %9s",
+		"task name", "wait time", "sch delay", "run time");
+
+	printf("\n");
+
+	/*
+	 * units row
+	 */
+	printf("%15s %-6s ", "", "");
+
+	printf(" %-20s  %9s  %9s  %9s\n", "[tid/pid]", "(msec)", "(msec)", "(msec)");
+
+	/*
+	 * separator
+	 */
+	printf("%.15s %.6s ", graph_dotted_line, graph_dotted_line);
+
+	printf(" %.20s  %.9s  %.9s  %.9s",
+		graph_dotted_line, graph_dotted_line, graph_dotted_line,
+		graph_dotted_line);
+
+	printf("\n");
+}
+
+static void timehist_print_sample(struct perf_sample *sample,
+				  struct thread *thread)
+{
+	struct thread_runtime *tr = thread__priv(thread);
+	char tstr[64];
+
+	timestamp__scnprintf_usec(sample->time, tstr, sizeof(tstr));
+	printf("%15s [%04d] ", tstr, sample->cpu);
+
+	printf(" %-*s ", comm_width, timehist_get_commstr(thread));
+
+	print_sched_time(tr->dt_wait, 6);
+	print_sched_time(tr->dt_delay, 6);
+	print_sched_time(tr->dt_run, 6);
+	printf("\n");
+}
+
+/*
+ * Explanation of delta-time stats:
+ *
+ *            t = time of current schedule out event
+ *        tprev = time of previous sched out event
+ *                also time of schedule-in event for current task
+ *    last_time = time of last sched change event for current task
+ *                (i.e, time process was last scheduled out)
+ * ready_to_run = time of wakeup for current task
+ *
+ * -----|------------|------------|------------|------
+ *    last         ready        tprev          t
+ *    time         to run
+ *
+ *      |-------- dt_wait --------|
+ *                   |- dt_delay -|-- dt_run --|
+ *
+ *   dt_run = run time of current task
+ *  dt_wait = time between last schedule out event for task and tprev
+ *            represents time spent off the cpu
+ * dt_delay = time between wakeup and schedule-in of task
+ */
+
+static void timehist_update_runtime_stats(struct thread_runtime *r,
+					 u64 t, u64 tprev)
+{
+	r->dt_delay   = 0;
+	r->dt_wait    = 0;
+	r->dt_run     = 0;
+	if (tprev) {
+		r->dt_run = t - tprev;
+		if (r->ready_to_run) {
+			if (r->ready_to_run > tprev)
+				pr_debug("time travel: wakeup time for task > previous sched_switch event\n");
+			else
+				r->dt_delay = tprev - r->ready_to_run;
+		}
+
+		if (r->last_time > tprev)
+			pr_debug("time travel: last sched out time for task > previous sched_switch event\n");
+		else if (r->last_time)
+			r->dt_wait = tprev - r->last_time;
+	}
+
+	update_stats(&r->run_stats, r->dt_run);
+	r->total_run_time += r->dt_run;
+}
+
+static bool is_idle_sample(struct perf_sample *sample,
+			   struct perf_evsel *evsel)
+{
+	/* pid 0 == swapper == idle task */
+	if (sample->pid == 0)
+		return true;
+
+	if (strcmp(perf_evsel__name(evsel), "sched:sched_switch") == 0) {
+		if (perf_evsel__intval(evsel, sample, "prev_pid") == 0)
+			return true;
+	}
+	return false;
+}
+
+/*
+ * Track idle stats per cpu by maintaining a local thread
+ * struct for the idle task on each cpu.
+ */
+static int init_idle_threads(int ncpu)
+{
+	int i;
+
+	idle_threads = zalloc(ncpu * sizeof(struct thread *));
+	if (!idle_threads)
+		return -ENOMEM;
+
+	idle_max_cpu = ncpu - 1;
+
+	/* allocate the actual thread struct if needed */
+	for (i = 0; i < ncpu; ++i) {
+		idle_threads[i] = thread__new(0, 0);
+		if (idle_threads[i] == NULL)
+			return -ENOMEM;
+
+		thread__set_comm(idle_threads[i], idle_comm, 0);
+	}
+
+	return 0;
+}
+
+static void free_idle_threads(void)
+{
+	int i;
+
+	if (idle_threads == NULL)
+		return;
+
+	for (i = 0; i <= idle_max_cpu; ++i) {
+		if ((idle_threads[i]))
+			thread__delete(idle_threads[i]);
+	}
+
+	free(idle_threads);
+}
+
+static struct thread *get_idle_thread(int cpu)
+{
+	/*
+	 * expand/allocate array of pointers to local thread
+	 * structs if needed
+	 */
+	if ((cpu >= idle_max_cpu) || (idle_threads == NULL)) {
+		int i, j = __roundup_pow_of_two(cpu+1);
+		void *p;
+
+		p = realloc(idle_threads, j * sizeof(struct thread *));
+		if (!p)
+			return NULL;
+
+		idle_threads = (struct thread **) p;
+		i = idle_max_cpu ? idle_max_cpu + 1 : 0;
+		for (; i < j; ++i)
+			idle_threads[i] = NULL;
+
+		idle_max_cpu = j;
+	}
+
+	/* allocate a new thread struct if needed */
+	if (idle_threads[cpu] == NULL) {
+		idle_threads[cpu] = thread__new(0, 0);
+
+		if (idle_threads[cpu]) {
+			idle_threads[cpu]->tid = 0;
+			thread__set_comm(idle_threads[cpu], idle_comm, 0);
+		}
+	}
+
+	return idle_threads[cpu];
+}
+
+/*
+ * handle runtime stats saved per thread
+ */
+static struct thread_runtime *thread__init_runtime(struct thread *thread)
+{
+	struct thread_runtime *r;
+
+	r = zalloc(sizeof(struct thread_runtime));
+	if (!r)
+		return NULL;
+
+	init_stats(&r->run_stats);
+	thread__set_priv(thread, r);
+
+	return r;
+}
+
+static struct thread_runtime *thread__get_runtime(struct thread *thread)
+{
+	struct thread_runtime *tr;
+
+	tr = thread__priv(thread);
+	if (tr == NULL) {
+		tr = thread__init_runtime(thread);
+		if (tr == NULL)
+			pr_debug("Failed to malloc memory for runtime data.\n");
+	}
+
+	return tr;
+}
+
+static struct thread *timehist_get_thread(struct perf_sample *sample,
+					  struct machine *machine,
+					  struct perf_evsel *evsel)
+{
+	struct thread *thread;
+
+	if (is_idle_sample(sample, evsel)) {
+		thread = get_idle_thread(sample->cpu);
+		if (thread == NULL)
+			pr_err("Failed to get idle thread for cpu %d.\n", sample->cpu);
+
+	} else {
+		thread = machine__findnew_thread(machine, sample->pid, sample->tid);
+		if (thread == NULL) {
+			pr_debug("Failed to get thread for tid %d. skipping sample.\n",
+				 sample->tid);
+		}
+	}
+
+	return thread;
+}
+
+static bool timehist_skip_sample(struct thread *thread)
+{
+	bool rc = false;
+
+	if (thread__is_filtered(thread))
+		rc = true;
+
+	return rc;
+}
+
+static int timehist_sched_wakeup_event(struct perf_tool *tool __maybe_unused,
+				       union perf_event *event __maybe_unused,
+				       struct perf_evsel *evsel,
+				       struct perf_sample *sample,
+				       struct machine *machine)
+{
+	struct thread *thread;
+	struct thread_runtime *tr = NULL;
+	/* want pid of awakened task not pid in sample */
+	const u32 pid = perf_evsel__intval(evsel, sample, "pid");
+
+	thread = machine__findnew_thread(machine, 0, pid);
+	if (thread == NULL)
+		return -1;
+
+	tr = thread__get_runtime(thread);
+	if (tr == NULL)
+		return -1;
+
+	if (tr->ready_to_run == 0)
+		tr->ready_to_run = sample->time;
+
+	return 0;
+}
+
+static int timehist_sched_change_event(struct perf_tool *tool __maybe_unused,
+				       union perf_event *event,
+				       struct perf_evsel *evsel,
+				       struct perf_sample *sample,
+				       struct machine *machine)
+{
+	struct addr_location al;
+	struct thread *thread;
+	struct thread_runtime *tr = NULL;
+	u64 tprev;
+	int rc = 0;
+
+	if (machine__resolve(machine, &al, sample) < 0) {
+		pr_err("problem processing %d event. skipping it\n",
+		       event->header.type);
+		rc = -1;
+		goto out;
+	}
+
+	thread = timehist_get_thread(sample, machine, evsel);
+	if (thread == NULL) {
+		rc = -1;
+		goto out;
+	}
+
+	if (timehist_skip_sample(thread))
+		goto out;
+
+	tr = thread__get_runtime(thread);
+	if (tr == NULL) {
+		rc = -1;
+		goto out;
+	}
+
+	tprev = perf_evsel__get_time(evsel, sample->cpu);
+
+	timehist_update_runtime_stats(tr, sample->time, tprev);
+	timehist_print_sample(sample, thread);
+
+out:
+	if (tr) {
+		/* time of this sched_switch event becomes last time task seen */
+		tr->last_time = sample->time;
+
+		/* sched out event for task so reset ready to run time */
+		tr->ready_to_run = 0;
+	}
+
+	perf_evsel__save_time(evsel, sample->time, sample->cpu);
+
+	return rc;
+}
+
+static int timehist_sched_switch_event(struct perf_tool *tool,
+			     union perf_event *event,
+			     struct perf_evsel *evsel,
+			     struct perf_sample *sample,
+			     struct machine *machine __maybe_unused)
+{
+	return timehist_sched_change_event(tool, event, evsel, sample, machine);
+}
+
+static int process_lost(struct perf_tool *tool __maybe_unused,
+			union perf_event *event,
+			struct perf_sample *sample,
+			struct machine *machine __maybe_unused)
+{
+	char tstr[64];
+
+	timestamp__scnprintf_usec(sample->time, tstr, sizeof(tstr));
+	printf("%15s ", tstr);
+	printf("lost %" PRIu64 " events on cpu %d\n", event->lost.lost, sample->cpu);
+
+	return 0;
+}
+
+
+typedef int (*sched_handler)(struct perf_tool *tool,
+			  union perf_event *event,
+			  struct perf_evsel *evsel,
+			  struct perf_sample *sample,
+			  struct machine *machine);
+
+static int perf_timehist__process_sample(struct perf_tool *tool,
+					 union perf_event *event,
+					 struct perf_sample *sample,
+					 struct perf_evsel *evsel,
+					 struct machine *machine)
+{
+	struct perf_sched *sched = container_of(tool, struct perf_sched, tool);
+	int err = 0;
+	int this_cpu = sample->cpu;
+
+	if (this_cpu > sched->max_cpu)
+		sched->max_cpu = this_cpu;
+
+	if (evsel->handler != NULL) {
+		sched_handler f = evsel->handler;
+
+		err = f(tool, event, evsel, sample, machine);
+	}
+
+	return err;
+}
+
+static int perf_sched__timehist(struct perf_sched *sched)
+{
+	const struct perf_evsel_str_handler handlers[] = {
+		{ "sched:sched_switch",       timehist_sched_switch_event, },
+		{ "sched:sched_wakeup",	      timehist_sched_wakeup_event, },
+		{ "sched:sched_wakeup_new",   timehist_sched_wakeup_event, },
+	};
+	struct perf_data_file file = {
+		.path = input_name,
+		.mode = PERF_DATA_MODE_READ,
+	};
+
+	struct perf_session *session;
+	int err = -1;
+
+	/*
+	 * event handlers for timehist option
+	 */
+	sched->tool.sample	 = perf_timehist__process_sample;
+	sched->tool.mmap	 = perf_event__process_mmap;
+	sched->tool.comm	 = perf_event__process_comm;
+	sched->tool.exit	 = perf_event__process_exit;
+	sched->tool.fork	 = perf_event__process_fork;
+	sched->tool.lost	 = process_lost;
+	sched->tool.attr	 = perf_event__process_attr;
+	sched->tool.tracing_data = perf_event__process_tracing_data;
+	sched->tool.build_id	 = perf_event__process_build_id;
+
+	sched->tool.ordered_events = true;
+	sched->tool.ordering_requires_timestamps = true;
+
+	session = perf_session__new(&file, false, &sched->tool);
+	if (session == NULL)
+		return -ENOMEM;
+
+	symbol__init(&session->header.env);
+
+	setup_pager();
+
+	/* setup per-evsel handlers */
+	if (perf_session__set_tracepoints_handlers(session, handlers))
+		goto out;
+
+	if (!perf_session__has_traces(session, "record -R"))
+		goto out;
+
+	/* pre-allocate struct for per-CPU idle stats */
+	sched->max_cpu = session->header.env.nr_cpus_online;
+	if (sched->max_cpu == 0)
+		sched->max_cpu = 4;
+	if (init_idle_threads(sched->max_cpu))
+		goto out;
+
+	timehist_header();
+
+	err = perf_session__process_events(session);
+	if (err) {
+		pr_err("Failed to process events, error %d", err);
+		goto out;
+	}
+
+out:
+	free_idle_threads();
+	perf_session__delete(session);
+
+	return err;
+}
+
+
 static void print_bad_events(struct perf_sched *sched)
 {
 	if (sched->nr_unordered_timestamps && sched->nr_timestamps) {
@@ -1970,8 +2535,6 @@ int cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)
 	const struct option latency_options[] = {
 	OPT_STRING('s', "sort", &sched.sort_order, "key[,key2...]",
 		   "sort by key(s): runtime, switch, avg, max"),
-	OPT_INCR('v', "verbose", &verbose,
-		    "be more verbose (show symbol address, etc)"),
 	OPT_INTEGER('C', "CPU", &sched.profile_cpu,
 		    "CPU to profile on"),
 	OPT_BOOLEAN('D', "dump-raw-trace", &dump_trace,
@@ -1983,8 +2546,6 @@ int cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)
 	const struct option replay_options[] = {
 	OPT_UINTEGER('r', "repeat", &sched.replay_repeat,
 		     "repeat the workload replay N times (-1: infinite)"),
-	OPT_INCR('v', "verbose", &verbose,
-		    "be more verbose (show symbol address, etc)"),
 	OPT_BOOLEAN('D', "dump-raw-trace", &dump_trace,
 		    "dump raw trace in ASCII"),
 	OPT_BOOLEAN('f', "force", &sched.force, "don't complain, do it"),
@@ -2001,6 +2562,16 @@ int cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)
                     "display given CPUs in map"),
 	OPT_PARENT(sched_options)
 	};
+	const struct option timehist_options[] = {
+	OPT_STRING('k', "vmlinux", &symbol_conf.vmlinux_name,
+		   "file", "vmlinux pathname"),
+	OPT_STRING(0, "kallsyms", &symbol_conf.kallsyms_name,
+		   "file", "kallsyms pathname"),
+	OPT_STRING(0, "symfs", &symbol_conf.symfs, "directory",
+		    "Look for files with symbols relative to this directory"),
+	OPT_PARENT(sched_options)
+	};
+
 	const char * const latency_usage[] = {
 		"perf sched latency [<options>]",
 		NULL
@@ -2013,8 +2584,13 @@ int cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)
 		"perf sched map [<options>]",
 		NULL
 	};
+	const char * const timehist_usage[] = {
+		"perf sched timehist [<options>]",
+		NULL
+	};
 	const char *const sched_subcommands[] = { "record", "latency", "map",
-						  "replay", "script", NULL };
+						  "replay", "script",
+						  "timehist", NULL };
 	const char *sched_usage[] = {
 		NULL,
 		NULL
@@ -2077,6 +2653,14 @@ int cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)
 				usage_with_options(replay_usage, replay_options);
 		}
 		return perf_sched__replay(&sched);
+	} else if (!strcmp(argv[0], "timehist")) {
+		if (argc) {
+			argc = parse_options(argc, argv, timehist_options,
+					     timehist_usage, 0);
+			if (argc)
+				usage_with_options(timehist_usage, timehist_options);
+		}
+		return perf_sched__timehist(&sched);
 	} else {
 		usage_with_options(sched_usage, sched_options);
 	}

commit 99620a5d0cc8e2dd9aedb629a6e81825f0db020e
Author: Namhyung Kim <namhyung@kernel.org>
Date:   Mon Oct 24 11:02:45 2016 +0900

    perf tools: Introduce timestamp__scnprintf_usec()
    
    Joonwoo reported that there's a mismatch between timestamps in script
    and sched commands.  This was because of difference in printing the
    timestamp.  Factor out the code and share it so that they can be in
    sync.  Also I found that sched map has similar problem, fix it too.
    
    Committer notes:
    
    Fixed the max_lat_at bug introduced by Namhyung's original patch, as
    pointed out by Joonwoo, and made it a function following the scnprintf()
    model, i.e. returning the number of bytes formatted, and receiving as
    the first parameter the object from where the data to the formatting is
    obtained, renaming it from:
    
       char *timestamp_in_usec(char *bf, size_t size, u64 timestamp)
    
    to
    
       int timestamp__scnprintf_usec(u64 timestamp, char *bf, size_t size)
    
    Reported-by: Joonwoo Park <joonwoop@codeaurora.org>
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20161024020246.14928-3-namhyung@kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 1f33d15314a5..fb3441211e4b 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1191,6 +1191,7 @@ static void output_lat_thread(struct perf_sched *sched, struct work_atoms *work_
 	int i;
 	int ret;
 	u64 avg;
+	char max_lat_at[32];
 
 	if (!work_list->nb_atoms)
 		return;
@@ -1212,12 +1213,13 @@ static void output_lat_thread(struct perf_sched *sched, struct work_atoms *work_
 		printf(" ");
 
 	avg = work_list->total_lat / work_list->nb_atoms;
+	timestamp__scnprintf_usec(work_list->max_lat_at, max_lat_at, sizeof(max_lat_at));
 
-	printf("|%11.3f ms |%9" PRIu64 " | avg:%9.3f ms | max:%9.3f ms | max at: %13.6f s\n",
+	printf("|%11.3f ms |%9" PRIu64 " | avg:%9.3f ms | max:%9.3f ms | max at: %13s s\n",
 	      (double)work_list->total_runtime / NSEC_PER_MSEC,
 		 work_list->nb_atoms, (double)avg / NSEC_PER_MSEC,
 		 (double)work_list->max_lat / NSEC_PER_MSEC,
-		 (double)work_list->max_lat_at / NSEC_PER_SEC);
+		 max_lat_at);
 }
 
 static int pid_cmp(struct work_atoms *l, struct work_atoms *r)
@@ -1402,6 +1404,7 @@ static int map_switch_event(struct perf_sched *sched, struct perf_evsel *evsel,
 	int cpus_nr;
 	bool new_cpu = false;
 	const char *color = PERF_COLOR_NORMAL;
+	char stimestamp[32];
 
 	BUG_ON(this_cpu >= MAX_CPUS || this_cpu < 0);
 
@@ -1492,7 +1495,8 @@ static int map_switch_event(struct perf_sched *sched, struct perf_evsel *evsel,
 	if (sched->map.cpus && !cpu_map__has(sched->map.cpus, this_cpu))
 		goto out;
 
-	color_fprintf(stdout, color, "  %12.6f secs ", (double)timestamp / NSEC_PER_SEC);
+	timestamp__scnprintf_usec(timestamp, stimestamp, sizeof(stimestamp));
+	color_fprintf(stdout, color, "  %12s secs ", stimestamp);
 	if (new_shortname || (verbose && sched_in->tid)) {
 		const char *pid_color = color;
 

commit e107f129e2e0e75ddf1cd7995a9f5ffff2307766
Author: Namhyung Kim <namhyung@kernel.org>
Date:   Mon Oct 24 11:02:44 2016 +0900

    perf sched map: Always show task comm with -v
    
    I'd like to see the name of tasks with perf sched map, but it only shows
    name of new tasks and then use short names after all.  This is not good
    for long running tasks since it's hard for users to track the short
    names.  This patch makes it show the names (except the idle task) when
    -v option is used.  Probably we may make it as default behavior.
    
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Acked-by: Jiri Olsa <jolsa@kernel.org>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20161024020246.14928-2-namhyung@kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index a8ad85914466..1f33d15314a5 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1493,7 +1493,7 @@ static int map_switch_event(struct perf_sched *sched, struct perf_evsel *evsel,
 		goto out;
 
 	color_fprintf(stdout, color, "  %12.6f secs ", (double)timestamp / NSEC_PER_SEC);
-	if (new_shortname) {
+	if (new_shortname || (verbose && sched_in->tid)) {
 		const char *pid_color = color;
 
 		if (thread__has_color(sched_in))

commit 1208bb274ba547012579d5b68c693e0b36682b74
Author: Namhyung Kim <namhyung@kernel.org>
Date:   Mon Oct 24 11:02:43 2016 +0900

    perf sched map: Apply cpu color when there's an activity
    
    Applying cpu color always doesn't help readability IMHO.  Instead it
    might be better to applying the color when there's an activity on those
    CPUs.
    
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Acked-by: Jiri Olsa <jolsa@kernel.org>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20161024020246.14928-1-namhyung@kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 8ca1b5409289..a8ad85914466 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1479,7 +1479,7 @@ static int map_switch_event(struct perf_sched *sched, struct perf_evsel *evsel,
 			cpu_color = COLOR_CPUS;
 
 		if (cpu != this_cpu)
-			color_fprintf(stdout, cpu_color, " ");
+			color_fprintf(stdout, color, " ");
 		else
 			color_fprintf(stdout, cpu_color, "*");
 

commit 77f02f44460ab1480af2ae6145a1a85b9fe0b8ac
Author: Namhyung Kim <namhyung@kernel.org>
Date:   Mon Oct 24 12:00:03 2016 +0900

    perf sched: Make common options cascading
    
    The -i and -v options can be used in subcommands so enable cascading the
    sched_options.  This fixes the following inconvenience in 'perf sched':
    
      $ perf sched -i perf.data.sched  map
      ... (it works well) ...
    
      $ perf sched map  -i perf.data.sched
        Error: unknown switch `i'
    
       Usage: perf sched map [<options>]
    
              --color-cpus <cpus>
                                highlight given CPUs in map
              --color-pids <pids>
                                highlight given pids in map
              --compact         map output in compact mode
              --cpus <cpus>     display given CPUs in map
    
    With this patch, the second command line works with the perf.data.sched
    data file.
    
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Tested-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Acked-by: Jiri Olsa <jolsa@kernel.org>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Wang Nan <wangnan0@huawei.com>
    Link: http://lkml.kernel.org/r/20161024030003.28534-2-namhyung@kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index f5503ca22e1c..8ca1b5409289 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1954,6 +1954,15 @@ int cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)
 		.next_shortname2      = '0',
 		.skip_merge           = 0,
 	};
+	const struct option sched_options[] = {
+	OPT_STRING('i', "input", &input_name, "file",
+		    "input file name"),
+	OPT_INCR('v', "verbose", &verbose,
+		    "be more verbose (show symbol address, etc)"),
+	OPT_BOOLEAN('D', "dump-raw-trace", &dump_trace,
+		    "dump raw trace in ASCII"),
+	OPT_END()
+	};
 	const struct option latency_options[] = {
 	OPT_STRING('s', "sort", &sched.sort_order, "key[,key2...]",
 		   "sort by key(s): runtime, switch, avg, max"),
@@ -1965,7 +1974,7 @@ int cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)
 		    "dump raw trace in ASCII"),
 	OPT_BOOLEAN('p', "pids", &sched.skip_merge,
 		    "latency stats per pid instead of per comm"),
-	OPT_END()
+	OPT_PARENT(sched_options)
 	};
 	const struct option replay_options[] = {
 	OPT_UINTEGER('r', "repeat", &sched.replay_repeat,
@@ -1975,16 +1984,7 @@ int cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)
 	OPT_BOOLEAN('D', "dump-raw-trace", &dump_trace,
 		    "dump raw trace in ASCII"),
 	OPT_BOOLEAN('f', "force", &sched.force, "don't complain, do it"),
-	OPT_END()
-	};
-	const struct option sched_options[] = {
-	OPT_STRING('i', "input", &input_name, "file",
-		    "input file name"),
-	OPT_INCR('v', "verbose", &verbose,
-		    "be more verbose (show symbol address, etc)"),
-	OPT_BOOLEAN('D', "dump-raw-trace", &dump_trace,
-		    "dump raw trace in ASCII"),
-	OPT_END()
+	OPT_PARENT(sched_options)
 	};
 	const struct option map_options[] = {
 	OPT_BOOLEAN(0, "compact", &sched.map.comp,
@@ -1995,7 +1995,7 @@ int cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)
                     "highlight given CPUs in map"),
 	OPT_STRING(0, "cpus", &sched.map.cpus_str, "cpus",
                     "display given CPUs in map"),
-	OPT_END()
+	OPT_PARENT(sched_options)
 	};
 	const char * const latency_usage[] = {
 		"perf sched latency [<options>]",

commit 4fc76e495b60da343b94dadeea1001f878ceb955
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Mon Aug 8 12:23:49 2016 -0300

    perf sched: Use linux/time64.h
    
    Probably the next step is to introduce linux/time.h and use
    timespec_to_ns(), etc.
    
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Wang Nan <wangnan0@huawei.com>
    Link: http://lkml.kernel.org/n/tip-4nqhskn27fn93cz3ukbc8drf@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 0dfe8df2ab9b..f5503ca22e1c 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -26,6 +26,7 @@
 #include <pthread.h>
 #include <math.h>
 #include <api/fs/fs.h>
+#include <linux/time64.h>
 
 #define PR_SET_NAME		15               /* Set process name */
 #define MAX_CPUS		4096
@@ -199,7 +200,7 @@ static u64 get_nsecs(void)
 
 	clock_gettime(CLOCK_MONOTONIC, &ts);
 
-	return ts.tv_sec * 1000000000ULL + ts.tv_nsec;
+	return ts.tv_sec * NSEC_PER_SEC + ts.tv_nsec;
 }
 
 static void burn_nsecs(struct perf_sched *sched, u64 nsecs)
@@ -223,7 +224,7 @@ static void sleep_nsecs(u64 nsecs)
 
 static void calibrate_run_measurement_overhead(struct perf_sched *sched)
 {
-	u64 T0, T1, delta, min_delta = 1000000000ULL;
+	u64 T0, T1, delta, min_delta = NSEC_PER_SEC;
 	int i;
 
 	for (i = 0; i < 10; i++) {
@@ -240,7 +241,7 @@ static void calibrate_run_measurement_overhead(struct perf_sched *sched)
 
 static void calibrate_sleep_measurement_overhead(struct perf_sched *sched)
 {
-	u64 T0, T1, delta, min_delta = 1000000000ULL;
+	u64 T0, T1, delta, min_delta = NSEC_PER_SEC;
 	int i;
 
 	for (i = 0; i < 10; i++) {
@@ -452,8 +453,8 @@ static u64 get_cpu_usage_nsec_parent(void)
 	err = getrusage(RUSAGE_SELF, &ru);
 	BUG_ON(err);
 
-	sum =  ru.ru_utime.tv_sec*1e9 + ru.ru_utime.tv_usec*1e3;
-	sum += ru.ru_stime.tv_sec*1e9 + ru.ru_stime.tv_usec*1e3;
+	sum =  ru.ru_utime.tv_sec * NSEC_PER_SEC + ru.ru_utime.tv_usec * NSEC_PER_USEC;
+	sum += ru.ru_stime.tv_sec * NSEC_PER_SEC + ru.ru_stime.tv_usec * NSEC_PER_USEC;
 
 	return sum;
 }
@@ -667,12 +668,12 @@ static void run_one_test(struct perf_sched *sched)
 		sched->run_avg = delta;
 	sched->run_avg = (sched->run_avg * (sched->replay_repeat - 1) + delta) / sched->replay_repeat;
 
-	printf("#%-3ld: %0.3f, ", sched->nr_runs, (double)delta / 1000000.0);
+	printf("#%-3ld: %0.3f, ", sched->nr_runs, (double)delta / NSEC_PER_MSEC);
 
-	printf("ravg: %0.2f, ", (double)sched->run_avg / 1e6);
+	printf("ravg: %0.2f, ", (double)sched->run_avg / NSEC_PER_MSEC);
 
 	printf("cpu: %0.2f / %0.2f",
-		(double)sched->cpu_usage / 1e6, (double)sched->runavg_cpu_usage / 1e6);
+		(double)sched->cpu_usage / NSEC_PER_MSEC, (double)sched->runavg_cpu_usage / NSEC_PER_MSEC);
 
 #if 0
 	/*
@@ -680,8 +681,8 @@ static void run_one_test(struct perf_sched *sched)
 	 * accurate than the sched->sum_exec_runtime based statistics:
 	 */
 	printf(" [%0.2f / %0.2f]",
-		(double)sched->parent_cpu_usage/1e6,
-		(double)sched->runavg_parent_cpu_usage/1e6);
+		(double)sched->parent_cpu_usage / NSEC_PER_MSEC,
+		(double)sched->runavg_parent_cpu_usage / NSEC_PER_MSEC);
 #endif
 
 	printf("\n");
@@ -696,13 +697,13 @@ static void test_calibrations(struct perf_sched *sched)
 	u64 T0, T1;
 
 	T0 = get_nsecs();
-	burn_nsecs(sched, 1e6);
+	burn_nsecs(sched, NSEC_PER_MSEC);
 	T1 = get_nsecs();
 
 	printf("the run test took %" PRIu64 " nsecs\n", T1 - T0);
 
 	T0 = get_nsecs();
-	sleep_nsecs(1e6);
+	sleep_nsecs(NSEC_PER_MSEC);
 	T1 = get_nsecs();
 
 	printf("the sleep test took %" PRIu64 " nsecs\n", T1 - T0);
@@ -1213,10 +1214,10 @@ static void output_lat_thread(struct perf_sched *sched, struct work_atoms *work_
 	avg = work_list->total_lat / work_list->nb_atoms;
 
 	printf("|%11.3f ms |%9" PRIu64 " | avg:%9.3f ms | max:%9.3f ms | max at: %13.6f s\n",
-	      (double)work_list->total_runtime / 1e6,
-		 work_list->nb_atoms, (double)avg / 1e6,
-		 (double)work_list->max_lat / 1e6,
-		 (double)work_list->max_lat_at / 1e9);
+	      (double)work_list->total_runtime / NSEC_PER_MSEC,
+		 work_list->nb_atoms, (double)avg / NSEC_PER_MSEC,
+		 (double)work_list->max_lat / NSEC_PER_MSEC,
+		 (double)work_list->max_lat_at / NSEC_PER_SEC);
 }
 
 static int pid_cmp(struct work_atoms *l, struct work_atoms *r)
@@ -1491,7 +1492,7 @@ static int map_switch_event(struct perf_sched *sched, struct perf_evsel *evsel,
 	if (sched->map.cpus && !cpu_map__has(sched->map.cpus, this_cpu))
 		goto out;
 
-	color_fprintf(stdout, color, "  %12.6f secs ", (double)timestamp/1e9);
+	color_fprintf(stdout, color, "  %12.6f secs ", (double)timestamp / NSEC_PER_SEC);
 	if (new_shortname) {
 		const char *pid_color = color;
 
@@ -1753,7 +1754,7 @@ static int perf_sched__lat(struct perf_sched *sched)
 
 	printf(" -----------------------------------------------------------------------------------------------------------------\n");
 	printf("  TOTAL:                |%11.3f ms |%9" PRIu64 " |\n",
-		(double)sched->all_runtime / 1e6, sched->all_count);
+		(double)sched->all_runtime / NSEC_PER_MSEC, sched->all_count);
 
 	printf(" ---------------------------------------------------\n");
 

commit c8b5f2c96d1bf6cefcbe12f67dce0b892fe20512
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Wed Jul 6 11:56:20 2016 -0300

    tools: Introduce str_error_r()
    
    The tools so far have been using the strerror_r() GNU variant, that
    returns a string, be it the buffer passed or something else.
    
    But that, besides being tricky in cases where we expect that the
    function using strerror_r() returns the error formatted in a provided
    buffer (we have to check if it returned something else and copy that
    instead), breaks the build on systems not using glibc, like Alpine
    Linux, where musl libc is used.
    
    So, introduce yet another wrapper, str_error_r(), that has the GNU
    interface, but uses the portable XSI variant of strerror_r(), so that
    users rest asured that the provided buffer is used and it is what is
    returned.
    
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Wang Nan <wangnan0@huawei.com>
    Link: http://lkml.kernel.org/n/tip-d4t42fnf48ytlk8rjxs822tf@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index afa057666c2a..0dfe8df2ab9b 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -494,7 +494,7 @@ static int self_open_counters(struct perf_sched *sched, unsigned long cur_task)
 		}
 		pr_err("Error: sys_perf_event_open() syscall returned "
 		       "with %d (%s)\n%s", fd,
-		       strerror_r(errno, sbuf, sizeof(sbuf)), info);
+		       str_error_r(errno, sbuf, sizeof(sbuf)), info);
 		exit(EXIT_FAILURE);
 	}
 	return fd;

commit 73643bb6a21c85509c7ae4c316f502c5a19cce65
Author: Jiri Olsa <jolsa@kernel.org>
Date:   Tue Apr 12 15:29:31 2016 +0200

    perf sched map: Display only given cpus
    
    Introducing --cpus option that will display only given cpus. Could be
    used together with color-cpus option.
    
      $ perf sched map  --cpus 0,1
            *A0   309999.786924 secs A0 => rcu_sched:7
            *.    309999.786930 secs
        *B0  .    309999.786931 secs B0 => rcuos/2:25
         B0 *A0   309999.786947 secs
    
    Signed-off-by: Jiri Olsa <jolsa@kernel.org>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1460467771-26532-9-git-send-email-jolsa@kernel.org
    [ Added entry to man page ]
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 7de04b297c14..afa057666c2a 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -135,6 +135,8 @@ struct perf_sched_map {
 	const char		*color_pids_str;
 	struct cpu_map		*color_cpus;
 	const char		*color_cpus_str;
+	struct cpu_map		*cpus;
+	const char		*cpus_str;
 };
 
 struct perf_sched {
@@ -1469,6 +1471,9 @@ static int map_switch_event(struct perf_sched *sched, struct perf_evsel *evsel,
 		if (curr_thread && thread__has_color(curr_thread))
 			pid_color = COLOR_PIDS;
 
+		if (sched->map.cpus && !cpu_map__has(sched->map.cpus, cpu))
+			continue;
+
 		if (sched->map.color_cpus && cpu_map__has(sched->map.color_cpus, cpu))
 			cpu_color = COLOR_CPUS;
 
@@ -1483,6 +1488,9 @@ static int map_switch_event(struct perf_sched *sched, struct perf_evsel *evsel,
 			color_fprintf(stdout, color, "   ");
 	}
 
+	if (sched->map.cpus && !cpu_map__has(sched->map.cpus, this_cpu))
+		goto out;
+
 	color_fprintf(stdout, color, "  %12.6f secs ", (double)timestamp/1e9);
 	if (new_shortname) {
 		const char *pid_color = color;
@@ -1497,6 +1505,7 @@ static int map_switch_event(struct perf_sched *sched, struct perf_evsel *evsel,
 	if (sched->map.comp && new_cpu)
 		color_fprintf(stdout, color, " (CPU %d)", this_cpu);
 
+out:
 	color_fprintf(stdout, color, "\n");
 
 	thread__put(sched_in);
@@ -1756,6 +1765,8 @@ static int perf_sched__lat(struct perf_sched *sched)
 
 static int setup_map_cpus(struct perf_sched *sched)
 {
+	struct cpu_map *map;
+
 	sched->max_cpu  = sysconf(_SC_NPROCESSORS_CONF);
 
 	if (sched->map.comp) {
@@ -1764,6 +1775,16 @@ static int setup_map_cpus(struct perf_sched *sched)
 			return -1;
 	}
 
+	if (!sched->map.cpus_str)
+		return 0;
+
+	map = cpu_map__new(sched->map.cpus_str);
+	if (!map) {
+		pr_err("failed to get cpus map from %s\n", sched->map.cpus_str);
+		return -1;
+	}
+
+	sched->map.cpus = map;
 	return 0;
 }
 
@@ -1971,6 +1992,8 @@ int cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)
 		   "highlight given pids in map"),
 	OPT_STRING(0, "color-cpus", &sched.map.color_cpus_str, "cpus",
                     "highlight given CPUs in map"),
+	OPT_STRING(0, "cpus", &sched.map.cpus_str, "cpus",
+                    "display given CPUs in map"),
 	OPT_END()
 	};
 	const char * const latency_usage[] = {

commit cf294f24f8c83bca6aa8e96b5cc4f78bed887f92
Author: Jiri Olsa <jolsa@kernel.org>
Date:   Tue Apr 12 15:29:30 2016 +0200

    perf sched map: Color given cpus
    
    Adding --color-cpus option to display selected cpus with background
    color (red by default).  It helps on navigating through the perf sched
    map output.
    
    Signed-off-by: Jiri Olsa <jolsa@kernel.org>
    Tested-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1460467771-26532-8-git-send-email-jolsa@kernel.org
    [ Added entry to man page ]
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index b5361a1d20e1..7de04b297c14 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -125,6 +125,7 @@ struct trace_sched_handler {
 };
 
 #define COLOR_PIDS PERF_COLOR_BLUE
+#define COLOR_CPUS PERF_COLOR_BG_RED
 
 struct perf_sched_map {
 	DECLARE_BITMAP(comp_cpus_mask, MAX_CPUS);
@@ -132,6 +133,8 @@ struct perf_sched_map {
 	bool			 comp;
 	struct thread_map	*color_pids;
 	const char		*color_pids_str;
+	struct cpu_map		*color_cpus;
+	const char		*color_cpus_str;
 };
 
 struct perf_sched {
@@ -1461,14 +1464,18 @@ static int map_switch_event(struct perf_sched *sched, struct perf_evsel *evsel,
 		int cpu = sched->map.comp ? sched->map.comp_cpus[i] : i;
 		struct thread *curr_thread = sched->curr_thread[cpu];
 		const char *pid_color = color;
+		const char *cpu_color = color;
 
 		if (curr_thread && thread__has_color(curr_thread))
 			pid_color = COLOR_PIDS;
 
+		if (sched->map.color_cpus && cpu_map__has(sched->map.color_cpus, cpu))
+			cpu_color = COLOR_CPUS;
+
 		if (cpu != this_cpu)
-			color_fprintf(stdout, color, " ");
+			color_fprintf(stdout, cpu_color, " ");
 		else
-			color_fprintf(stdout, color, "*");
+			color_fprintf(stdout, cpu_color, "*");
 
 		if (sched->curr_thread[cpu])
 			color_fprintf(stdout, pid_color, "%2s ", sched->curr_thread[cpu]->shortname);
@@ -1753,7 +1760,8 @@ static int setup_map_cpus(struct perf_sched *sched)
 
 	if (sched->map.comp) {
 		sched->map.comp_cpus = zalloc(sched->max_cpu * sizeof(int));
-		return sched->map.comp_cpus ? 0 : -1;
+		if (!sched->map.comp_cpus)
+			return -1;
 	}
 
 	return 0;
@@ -1776,6 +1784,23 @@ static int setup_color_pids(struct perf_sched *sched)
 	return 0;
 }
 
+static int setup_color_cpus(struct perf_sched *sched)
+{
+	struct cpu_map *map;
+
+	if (!sched->map.color_cpus_str)
+		return 0;
+
+	map = cpu_map__new(sched->map.color_cpus_str);
+	if (!map) {
+		pr_err("failed to get thread map from %s\n", sched->map.color_cpus_str);
+		return -1;
+	}
+
+	sched->map.color_cpus = map;
+	return 0;
+}
+
 static int perf_sched__map(struct perf_sched *sched)
 {
 	if (setup_map_cpus(sched))
@@ -1784,6 +1809,9 @@ static int perf_sched__map(struct perf_sched *sched)
 	if (setup_color_pids(sched))
 		return -1;
 
+	if (setup_color_cpus(sched))
+		return -1;
+
 	setup_pager();
 	if (perf_sched__read_events(sched))
 		return -1;
@@ -1941,6 +1969,8 @@ int cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)
 		    "map output in compact mode"),
 	OPT_STRING(0, "color-pids", &sched.map.color_pids_str, "pids",
 		   "highlight given pids in map"),
+	OPT_STRING(0, "color-cpus", &sched.map.color_cpus_str, "cpus",
+                    "highlight given CPUs in map"),
 	OPT_END()
 	};
 	const char * const latency_usage[] = {

commit a151a37a760aab41c115af8d5016e449228e8d2e
Author: Jiri Olsa <jolsa@kernel.org>
Date:   Tue Apr 12 15:29:29 2016 +0200

    perf sched map: Color given pids
    
    Adding --color-pids option to display selected pids in color (blue by
    default). It helps on navigating through the 'perf sched map' output.
    
    Signed-off-by: Jiri Olsa <jolsa@kernel.org>
    Tested-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1460467771-26532-7-git-send-email-jolsa@kernel.org
    [ Added entry to man page ]
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 9ef28973f198..b5361a1d20e1 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -11,6 +11,7 @@
 #include "util/session.h"
 #include "util/tool.h"
 #include "util/cloexec.h"
+#include "util/thread_map.h"
 #include "util/color.h"
 
 #include <subcmd/parse-options.h>
@@ -123,10 +124,14 @@ struct trace_sched_handler {
 				  struct machine *machine);
 };
 
+#define COLOR_PIDS PERF_COLOR_BLUE
+
 struct perf_sched_map {
 	DECLARE_BITMAP(comp_cpus_mask, MAX_CPUS);
 	int			*comp_cpus;
 	bool			 comp;
+	struct thread_map	*color_pids;
+	const char		*color_pids_str;
 };
 
 struct perf_sched {
@@ -1347,6 +1352,38 @@ static int process_sched_wakeup_event(struct perf_tool *tool,
 	return 0;
 }
 
+union map_priv {
+	void	*ptr;
+	bool	 color;
+};
+
+static bool thread__has_color(struct thread *thread)
+{
+	union map_priv priv = {
+		.ptr = thread__priv(thread),
+	};
+
+	return priv.color;
+}
+
+static struct thread*
+map__findnew_thread(struct perf_sched *sched, struct machine *machine, pid_t pid, pid_t tid)
+{
+	struct thread *thread = machine__findnew_thread(machine, pid, tid);
+	union map_priv priv = {
+		.color = false,
+	};
+
+	if (!sched->map.color_pids || !thread || thread__priv(thread))
+		return thread;
+
+	if (thread_map__has(sched->map.color_pids, tid))
+		priv.color = true;
+
+	thread__set_priv(thread, priv.ptr);
+	return thread;
+}
+
 static int map_switch_event(struct perf_sched *sched, struct perf_evsel *evsel,
 			    struct perf_sample *sample, struct machine *machine)
 {
@@ -1386,7 +1423,7 @@ static int map_switch_event(struct perf_sched *sched, struct perf_evsel *evsel,
 		return -1;
 	}
 
-	sched_in = machine__findnew_thread(machine, -1, next_pid);
+	sched_in = map__findnew_thread(sched, machine, -1, next_pid);
 	if (sched_in == NULL)
 		return -1;
 
@@ -1422,6 +1459,11 @@ static int map_switch_event(struct perf_sched *sched, struct perf_evsel *evsel,
 
 	for (i = 0; i < cpus_nr; i++) {
 		int cpu = sched->map.comp ? sched->map.comp_cpus[i] : i;
+		struct thread *curr_thread = sched->curr_thread[cpu];
+		const char *pid_color = color;
+
+		if (curr_thread && thread__has_color(curr_thread))
+			pid_color = COLOR_PIDS;
 
 		if (cpu != this_cpu)
 			color_fprintf(stdout, color, " ");
@@ -1429,14 +1471,19 @@ static int map_switch_event(struct perf_sched *sched, struct perf_evsel *evsel,
 			color_fprintf(stdout, color, "*");
 
 		if (sched->curr_thread[cpu])
-			color_fprintf(stdout, color, "%2s ", sched->curr_thread[cpu]->shortname);
+			color_fprintf(stdout, pid_color, "%2s ", sched->curr_thread[cpu]->shortname);
 		else
 			color_fprintf(stdout, color, "   ");
 	}
 
 	color_fprintf(stdout, color, "  %12.6f secs ", (double)timestamp/1e9);
 	if (new_shortname) {
-		color_fprintf(stdout, color, "%s => %s:%d",
+		const char *pid_color = color;
+
+		if (thread__has_color(sched_in))
+			pid_color = COLOR_PIDS;
+
+		color_fprintf(stdout, pid_color, "%s => %s:%d",
 		       sched_in->shortname, thread__comm_str(sched_in), sched_in->tid);
 	}
 
@@ -1712,11 +1759,31 @@ static int setup_map_cpus(struct perf_sched *sched)
 	return 0;
 }
 
+static int setup_color_pids(struct perf_sched *sched)
+{
+	struct thread_map *map;
+
+	if (!sched->map.color_pids_str)
+		return 0;
+
+	map = thread_map__new_by_tid_str(sched->map.color_pids_str);
+	if (!map) {
+		pr_err("failed to get thread map from %s\n", sched->map.color_pids_str);
+		return -1;
+	}
+
+	sched->map.color_pids = map;
+	return 0;
+}
+
 static int perf_sched__map(struct perf_sched *sched)
 {
 	if (setup_map_cpus(sched))
 		return -1;
 
+	if (setup_color_pids(sched))
+		return -1;
+
 	setup_pager();
 	if (perf_sched__read_events(sched))
 		return -1;
@@ -1872,6 +1939,8 @@ int cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)
 	const struct option map_options[] = {
 	OPT_BOOLEAN(0, "compact", &sched.map.comp,
 		    "map output in compact mode"),
+	OPT_STRING(0, "color-pids", &sched.map.color_pids_str, "pids",
+		   "highlight given pids in map"),
 	OPT_END()
 	};
 	const char * const latency_usage[] = {
@@ -1935,7 +2004,7 @@ int cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)
 		return perf_sched__lat(&sched);
 	} else if (!strcmp(argv[0], "map")) {
 		if (argc) {
-			argc = parse_options(argc, argv, map_options, replay_usage, 0);
+			argc = parse_options(argc, argv, map_options, map_usage, 0);
 			if (argc)
 				usage_with_options(map_usage, map_options);
 		}

commit 8cd91195e5efc5166fc48eec6cf83ef93133b7b6
Author: Jiri Olsa <jolsa@kernel.org>
Date:   Tue Apr 12 15:29:27 2016 +0200

    perf sched: Use color_fprintf for output
    
    As preparation for next patch.
    
    Signed-off-by: Jiri Olsa <jolsa@kernel.org>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1460467771-26532-5-git-send-email-jolsa@kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 64dd94667055..9ef28973f198 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -11,6 +11,7 @@
 #include "util/session.h"
 #include "util/tool.h"
 #include "util/cloexec.h"
+#include "util/color.h"
 
 #include <subcmd/parse-options.h>
 #include "util/trace-event.h"
@@ -1357,6 +1358,7 @@ static int map_switch_event(struct perf_sched *sched, struct perf_evsel *evsel,
 	int i, this_cpu = sample->cpu;
 	int cpus_nr;
 	bool new_cpu = false;
+	const char *color = PERF_COLOR_NORMAL;
 
 	BUG_ON(this_cpu >= MAX_CPUS || this_cpu < 0);
 
@@ -1422,26 +1424,26 @@ static int map_switch_event(struct perf_sched *sched, struct perf_evsel *evsel,
 		int cpu = sched->map.comp ? sched->map.comp_cpus[i] : i;
 
 		if (cpu != this_cpu)
-			printf(" ");
+			color_fprintf(stdout, color, " ");
 		else
-			printf("*");
+			color_fprintf(stdout, color, "*");
 
 		if (sched->curr_thread[cpu])
-			printf("%2s ", sched->curr_thread[cpu]->shortname);
+			color_fprintf(stdout, color, "%2s ", sched->curr_thread[cpu]->shortname);
 		else
-			printf("   ");
+			color_fprintf(stdout, color, "   ");
 	}
 
-	printf("  %12.6f secs ", (double)timestamp/1e9);
+	color_fprintf(stdout, color, "  %12.6f secs ", (double)timestamp/1e9);
 	if (new_shortname) {
-		printf("%s => %s:%d",
+		color_fprintf(stdout, color, "%s => %s:%d",
 		       sched_in->shortname, thread__comm_str(sched_in), sched_in->tid);
 	}
 
 	if (sched->map.comp && new_cpu)
-		printf(" (CPU %d)", this_cpu);
+		color_fprintf(stdout, color, " (CPU %d)", this_cpu);
 
-	printf("\n");
+	color_fprintf(stdout, color, "\n");
 
 	thread__put(sched_in);
 

commit 99623c628f5425f09b5321cf621af1da29c0c47d
Author: Jiri Olsa <jolsa@kernel.org>
Date:   Tue Apr 12 15:29:26 2016 +0200

    perf sched: Add compact display option
    
    Add compact map display that does not output the whole cpu matrix, only
    cpus that got event.
    
      $ perf sched map --compact
        *A0   1082427.094098 secs A0 => perf:19404 (CPU 2)
         A0 *.    1082427.094127 secs .  => swapper:0 (CPU 1)
         A0  .  *B0   1082427.094174 secs B0 => rcuos/2:25 (CPU 3)
         A0  .  *.    1082427.094177 secs
        *C0  .   .    1082427.094187 secs C0 => migration/2:21
         C0 *A0  .    1082427.094193 secs
        *.   A0  .    1082427.094195 secs
        *D0  A0  .    1082427.094402 secs D0 => rngd:968
        *.   A0  .    1082427.094406 secs
         .  *E0  .    1082427.095221 secs E0 => kworker/1:1:5333
         .   E0 *F0   1082427.095227 secs F0 => xterm:3342
    
    It helps to display sane output for small thread loads on big cpu
    servers.
    
    Signed-off-by: Jiri Olsa <jolsa@kernel.org>
    Tested-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1460467771-26532-4-git-send-email-jolsa@kernel.org
    [ Add entry in 'perf sched' man page ]
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 871b55ae22a4..64dd94667055 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -122,6 +122,12 @@ struct trace_sched_handler {
 				  struct machine *machine);
 };
 
+struct perf_sched_map {
+	DECLARE_BITMAP(comp_cpus_mask, MAX_CPUS);
+	int			*comp_cpus;
+	bool			 comp;
+};
+
 struct perf_sched {
 	struct perf_tool tool;
 	const char	 *sort_order;
@@ -173,6 +179,7 @@ struct perf_sched {
 	struct list_head sort_list, cmp_pid;
 	bool force;
 	bool skip_merge;
+	struct perf_sched_map map;
 };
 
 static u64 get_nsecs(void)
@@ -1347,13 +1354,24 @@ static int map_switch_event(struct perf_sched *sched, struct perf_evsel *evsel,
 	int new_shortname;
 	u64 timestamp0, timestamp = sample->time;
 	s64 delta;
-	int cpu, this_cpu = sample->cpu;
+	int i, this_cpu = sample->cpu;
+	int cpus_nr;
+	bool new_cpu = false;
 
 	BUG_ON(this_cpu >= MAX_CPUS || this_cpu < 0);
 
 	if (this_cpu > sched->max_cpu)
 		sched->max_cpu = this_cpu;
 
+	if (sched->map.comp) {
+		cpus_nr = bitmap_weight(sched->map.comp_cpus_mask, MAX_CPUS);
+		if (!test_and_set_bit(this_cpu, sched->map.comp_cpus_mask)) {
+			sched->map.comp_cpus[cpus_nr++] = this_cpu;
+			new_cpu = true;
+		}
+	} else
+		cpus_nr = sched->max_cpu;
+
 	timestamp0 = sched->cpu_last_switched[this_cpu];
 	sched->cpu_last_switched[this_cpu] = timestamp;
 	if (timestamp0)
@@ -1400,7 +1418,9 @@ static int map_switch_event(struct perf_sched *sched, struct perf_evsel *evsel,
 		new_shortname = 1;
 	}
 
-	for (cpu = 0; cpu <= sched->max_cpu; cpu++) {
+	for (i = 0; i < cpus_nr; i++) {
+		int cpu = sched->map.comp ? sched->map.comp_cpus[i] : i;
+
 		if (cpu != this_cpu)
 			printf(" ");
 		else
@@ -1414,12 +1434,15 @@ static int map_switch_event(struct perf_sched *sched, struct perf_evsel *evsel,
 
 	printf("  %12.6f secs ", (double)timestamp/1e9);
 	if (new_shortname) {
-		printf("%s => %s:%d\n",
+		printf("%s => %s:%d",
 		       sched_in->shortname, thread__comm_str(sched_in), sched_in->tid);
-	} else {
-		printf("\n");
 	}
 
+	if (sched->map.comp && new_cpu)
+		printf(" (CPU %d)", this_cpu);
+
+	printf("\n");
+
 	thread__put(sched_in);
 
 	return 0;
@@ -1675,9 +1698,22 @@ static int perf_sched__lat(struct perf_sched *sched)
 	return 0;
 }
 
+static int setup_map_cpus(struct perf_sched *sched)
+{
+	sched->max_cpu  = sysconf(_SC_NPROCESSORS_CONF);
+
+	if (sched->map.comp) {
+		sched->map.comp_cpus = zalloc(sched->max_cpu * sizeof(int));
+		return sched->map.comp_cpus ? 0 : -1;
+	}
+
+	return 0;
+}
+
 static int perf_sched__map(struct perf_sched *sched)
 {
-	sched->max_cpu = sysconf(_SC_NPROCESSORS_CONF);
+	if (setup_map_cpus(sched))
+		return -1;
 
 	setup_pager();
 	if (perf_sched__read_events(sched))
@@ -1831,6 +1867,11 @@ int cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)
 		    "dump raw trace in ASCII"),
 	OPT_END()
 	};
+	const struct option map_options[] = {
+	OPT_BOOLEAN(0, "compact", &sched.map.comp,
+		    "map output in compact mode"),
+	OPT_END()
+	};
 	const char * const latency_usage[] = {
 		"perf sched latency [<options>]",
 		NULL
@@ -1839,6 +1880,10 @@ int cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)
 		"perf sched replay [<options>]",
 		NULL
 	};
+	const char * const map_usage[] = {
+		"perf sched map [<options>]",
+		NULL
+	};
 	const char *const sched_subcommands[] = { "record", "latency", "map",
 						  "replay", "script", NULL };
 	const char *sched_usage[] = {
@@ -1887,6 +1932,11 @@ int cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)
 		setup_sorting(&sched, latency_options, latency_usage);
 		return perf_sched__lat(&sched);
 	} else if (!strcmp(argv[0], "map")) {
+		if (argc) {
+			argc = parse_options(argc, argv, map_options, replay_usage, 0);
+			if (argc)
+				usage_with_options(map_usage, map_options);
+		}
 		sched.tp_handler = &map_ops;
 		setup_sorting(&sched, latency_options, latency_usage);
 		return perf_sched__map(&sched);

commit 4b6ab94eabe4f55371cff4569750bb3996c55db6
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Tue Dec 15 09:39:39 2015 -0600

    perf subcmd: Create subcmd library
    
    Move the subcommand-related files from perf to a new library named
    libsubcmd.a.
    
    Since we're moving files anyway, go ahead and rename 'exec_cmd.*' to
    'exec-cmd.*' to be consistent with the naming of all the other files.
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/c0a838d4c878ab17fee50998811612b2281355c1.1450193761.git.jpoimboe@redhat.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index e3d3e32c0a93..871b55ae22a4 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -12,7 +12,7 @@
 #include "util/tool.h"
 #include "util/cloexec.h"
 
-#include "util/parse-options.h"
+#include <subcmd/parse-options.h>
 #include "util/trace-event.h"
 
 #include "util/debug.h"

commit 0014de172d228e450377d1fd079d94e67128d27f
Author: Jiri Olsa <jolsa@kernel.org>
Date:   Mon Nov 2 12:10:25 2015 +0100

    perf sched latency: Fix thread pid reuse issue
    
    The latency subcommand holds a tree of working atoms sorted by thread's
    pid/tid. If there's new thread with same pid and tid, the old working atom is
    found and assert bug condition is hit in search function:
    
      thread_atoms_search: Assertion `!(thread != atoms->thread)' failed
    
    Changing the sort function to use thread object pointers together with pid and
    tid check. This way new thread will never find old one with same pid/tid.
    
    Link: http://lkml.kernel.org/n/tip-o4doazhhv0zax5zshkg8hnys@git.kernel.org
    Reported-by: Mohit Agrawal <moagrawa@redhat.com>
    Signed-off-by: Jiri Olsa <jolsa@kernel.org>
    Acked-by: Namhyung Kim <namhyung@kernel.org>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1446462625-15807-1-git-send-email-jolsa@kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 0ee6d900e100..e3d3e32c0a93 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1203,12 +1203,13 @@ static void output_lat_thread(struct perf_sched *sched, struct work_atoms *work_
 
 static int pid_cmp(struct work_atoms *l, struct work_atoms *r)
 {
+	if (l->thread == r->thread)
+		return 0;
 	if (l->thread->tid < r->thread->tid)
 		return -1;
 	if (l->thread->tid > r->thread->tid)
 		return 1;
-
-	return 0;
+	return (int)(l->thread - r->thread);
 }
 
 static int avg_cmp(struct work_atoms *l, struct work_atoms *r)

commit c71183697250b356be6c7c1abc2e9a74073e1dca
Author: Namhyung Kim <namhyung@kernel.org>
Date:   Sun Oct 25 00:49:27 2015 +0900

    perf tools: Introduce usage_with_options_msg()
    
    Now usage_with_options() setup a pager before printing message so normal
    printf() or pr_err() will not be shown.  The usage_with_options_msg()
    can be used to print some help message before usage strings.
    
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Acked-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1445701767-12731-4-git-send-email-namhyung@kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 33962612a5e9..0ee6d900e100 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1728,8 +1728,8 @@ static void setup_sorting(struct perf_sched *sched, const struct option *options
 	for (tok = strtok_r(str, ", ", &tmp);
 			tok; tok = strtok_r(NULL, ", ", &tmp)) {
 		if (sort_dimension__add(tok, &sched->sort_list) < 0) {
-			error("Unknown --sort key: `%s'", tok);
-			usage_with_options(usage_msg, options);
+			usage_with_options_msg(usage_msg, options,
+					"Unknown --sort key: `%s'", tok);
 		}
 	}
 

commit 2f80dd4488c204a4850554746eb31f25f5a84405
Author: Josef Bacik <jbacik@fb.com>
Date:   Fri May 22 09:18:40 2015 -0400

    perf sched: Add option to merge like comms to lat output
    
    Sometimes when debugging large multi-threaded applications it is helpful
    to collate all of the latency numbers into one bulk record to get an
    idea of what is going on.
    
    This patch does this by merging any entries that belong to the same comm
    into one entry and then spits out those totals.
    
    I've also slightly changed the output so you can see how many threads
    were merged in the processing.  Here is the new default output format
    
     -----------------------------------------------------------------------------------------------------------
      Task                 | Runtime ms  | Switches | Average delay ms | Maximum delay ms | Maximum delay at    |
     -----------------------------------------------------------------------------------------------------------
      chrome:(23)          |  740.878 ms |     2612 | avg:    0.022 ms | max:    0.845 ms | max at: 7935.254223 s
      pulseaudio:1523      |   94.440 ms |      597 | avg:    0.027 ms | max:    0.110 ms | max at: 7934.668372 s
      threaded-ml:6042     |   72.554 ms |      386 | avg:    0.035 ms | max:    1.186 ms | max at: 7935.330911 s
      Chrome_IOThread:3832 |   52.388 ms |      456 | avg:    0.021 ms | max:    1.365 ms | max at: 7935.330602 s
      Chrome_ChildIOT:(7)  |   50.694 ms |      743 | avg:    0.021 ms | max:    1.448 ms | max at: 7935.256659 s
      Compositor:5510      |   30.012 ms |      192 | avg:    0.019 ms | max:    0.131 ms | max at: 7936.636815 s
      plugin_audio_th:6043 |   24.828 ms |      314 | avg:    0.018 ms | max:    0.143 ms | max at: 7936.205994 s
      CompositorTileW:(2)  |   14.099 ms |       45 | avg:    0.022 ms | max:    0.153 ms | max at: 7937.521800 s
    
    the (#) after the task is the number of tasks merged, and then if there were
    no tasks merged it just shows the pid.  Here is the same trace file with the -p
    option to print the per-pid latency numbers
    
     -----------------------------------------------------------------------------------------------------------
      Task                 | Runtime ms  | Switches | Average delay ms | Maximum delay ms | Maximum delay at    |
     -----------------------------------------------------------------------------------------------------------
      chrome:5500          |  386.872 ms |      387 | avg:    0.023 ms | max:    0.241 ms | max at: 7936.001694 s
      pulseaudio:1523      |   94.440 ms |      597 | avg:    0.027 ms | max:    0.110 ms | max at: 7934.668372 s
      threaded-ml:6042     |   72.554 ms |      386 | avg:    0.035 ms | max:    1.186 ms | max at: 7935.330911 s
      chrome:10226         |   69.710 ms |      251 | avg:    0.023 ms | max:    0.764 ms | max at: 7935.992305 s
      chrome:4267          |   64.551 ms |      418 | avg:    0.021 ms | max:    0.294 ms | max at: 7937.862427 s
      chrome:4827          |   62.268 ms |       54 | avg:    0.029 ms | max:    0.666 ms | max at: 7935.992813 s
      Chrome_IOThread:3832 |   52.388 ms |      456 | avg:    0.021 ms | max:    1.365 ms | max at: 7935.330602 s
      chrome:3776          |   46.150 ms |      349 | avg:    0.023 ms | max:    0.845 ms | max at: 7935.254223 s
    
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: kernel-team@fb.com
    Link: http://lkml.kernel.org/r/1432300720-30478-1-git-send-email-jbacik@fb.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 79273ecf92eb..33962612a5e9 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -95,6 +95,7 @@ struct work_atoms {
 	u64			total_lat;
 	u64			nb_atoms;
 	u64			total_runtime;
+	int			num_merged;
 };
 
 typedef int (*sort_fn_t)(struct work_atoms *, struct work_atoms *);
@@ -168,9 +169,10 @@ struct perf_sched {
 	u64		 all_runtime;
 	u64		 all_count;
 	u64		 cpu_last_switched[MAX_CPUS];
-	struct rb_root	 atom_root, sorted_atom_root;
+	struct rb_root	 atom_root, sorted_atom_root, merged_atom_root;
 	struct list_head sort_list, cmp_pid;
 	bool force;
+	bool skip_merge;
 };
 
 static u64 get_nsecs(void)
@@ -1182,7 +1184,10 @@ static void output_lat_thread(struct perf_sched *sched, struct work_atoms *work_
 	sched->all_runtime += work_list->total_runtime;
 	sched->all_count   += work_list->nb_atoms;
 
-	ret = printf("  %s:%d ", thread__comm_str(work_list->thread), work_list->thread->tid);
+	if (work_list->num_merged > 1)
+		ret = printf("  %s:(%d) ", thread__comm_str(work_list->thread), work_list->num_merged);
+	else
+		ret = printf("  %s:%d ", thread__comm_str(work_list->thread), work_list->thread->tid);
 
 	for (i = 0; i < 24 - ret; i++)
 		printf(" ");
@@ -1302,17 +1307,22 @@ static int sort_dimension__add(const char *tok, struct list_head *list)
 static void perf_sched__sort_lat(struct perf_sched *sched)
 {
 	struct rb_node *node;
-
+	struct rb_root *root = &sched->atom_root;
+again:
 	for (;;) {
 		struct work_atoms *data;
-		node = rb_first(&sched->atom_root);
+		node = rb_first(root);
 		if (!node)
 			break;
 
-		rb_erase(node, &sched->atom_root);
+		rb_erase(node, root);
 		data = rb_entry(node, struct work_atoms, node);
 		__thread_latency_insert(&sched->sorted_atom_root, data, &sched->sort_list);
 	}
+	if (root == &sched->atom_root) {
+		root = &sched->merged_atom_root;
+		goto again;
+	}
 }
 
 static int process_sched_wakeup_event(struct perf_tool *tool,
@@ -1572,6 +1582,59 @@ static void print_bad_events(struct perf_sched *sched)
 	}
 }
 
+static void __merge_work_atoms(struct rb_root *root, struct work_atoms *data)
+{
+	struct rb_node **new = &(root->rb_node), *parent = NULL;
+	struct work_atoms *this;
+	const char *comm = thread__comm_str(data->thread), *this_comm;
+
+	while (*new) {
+		int cmp;
+
+		this = container_of(*new, struct work_atoms, node);
+		parent = *new;
+
+		this_comm = thread__comm_str(this->thread);
+		cmp = strcmp(comm, this_comm);
+		if (cmp > 0) {
+			new = &((*new)->rb_left);
+		} else if (cmp < 0) {
+			new = &((*new)->rb_right);
+		} else {
+			this->num_merged++;
+			this->total_runtime += data->total_runtime;
+			this->nb_atoms += data->nb_atoms;
+			this->total_lat += data->total_lat;
+			list_splice(&data->work_list, &this->work_list);
+			if (this->max_lat < data->max_lat) {
+				this->max_lat = data->max_lat;
+				this->max_lat_at = data->max_lat_at;
+			}
+			zfree(&data);
+			return;
+		}
+	}
+
+	data->num_merged++;
+	rb_link_node(&data->node, parent, new);
+	rb_insert_color(&data->node, root);
+}
+
+static void perf_sched__merge_lat(struct perf_sched *sched)
+{
+	struct work_atoms *data;
+	struct rb_node *node;
+
+	if (sched->skip_merge)
+		return;
+
+	while ((node = rb_first(&sched->atom_root))) {
+		rb_erase(node, &sched->atom_root);
+		data = rb_entry(node, struct work_atoms, node);
+		__merge_work_atoms(&sched->merged_atom_root, data);
+	}
+}
+
 static int perf_sched__lat(struct perf_sched *sched)
 {
 	struct rb_node *next;
@@ -1581,6 +1644,7 @@ static int perf_sched__lat(struct perf_sched *sched)
 	if (perf_sched__read_events(sched))
 		return -1;
 
+	perf_sched__merge_lat(sched);
 	perf_sched__sort_lat(sched);
 
 	printf("\n -----------------------------------------------------------------------------------------------------------------\n");
@@ -1732,6 +1796,7 @@ int cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)
 		.profile_cpu	      = -1,
 		.next_shortname1      = 'A',
 		.next_shortname2      = '0',
+		.skip_merge           = 0,
 	};
 	const struct option latency_options[] = {
 	OPT_STRING('s', "sort", &sched.sort_order, "key[,key2...]",
@@ -1742,6 +1807,8 @@ int cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)
 		    "CPU to profile on"),
 	OPT_BOOLEAN('D', "dump-raw-trace", &dump_trace,
 		    "dump raw trace in ASCII"),
+	OPT_BOOLEAN('p', "pids", &sched.skip_merge,
+		    "latency stats per pid instead of per comm"),
 	OPT_END()
 	};
 	const struct option replay_options[] = {

commit b91fc39f4ad7503419dd617df78401fa36266cb3
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Mon Apr 6 20:43:22 2015 -0300

    perf machine: Protect the machine->threads with a rwlock
    
    In addition to using refcounts for the struct thread lifetime
    management, we need to protect access to machine->threads from
    concurrent access.
    
    That happens in 'perf top', where a thread processes events, inserting
    and deleting entries from that rb_tree while another thread decays
    hist_entries, that end up dropping references and ultimately deleting
    threads from the rb_tree and releasing its resources when no further
    hist_entry (or other data structures, like in 'perf sched') references
    it.
    
    So the rule is the same for refcounts + protected trees in the kernel,
    get the tree lock, find object, bump the refcount, drop the tree lock,
    return, use object, drop the refcount if no more use of it is needed,
    keep it if storing it in some other data structure, drop when releasing
    that data structure.
    
    I.e. pair "t = machine__find(new)_thread()" with a "thread__put(t)", and
    "perf_event__preprocess_sample(&al)" with "addr_location__put(&al)".
    
    The addr_location__put() one is because as we return references to
    several data structures, we may end up adding more reference counting
    for the other data structures and then we'll drop it at
    addr_location__put() time.
    
    Acked-by: David Ahern <dsahern@gmail.com>
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Don Zickus <dzickus@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Stephane Eranian <eranian@google.com>
    Link: http://lkml.kernel.org/n/tip-bs9rt4n0jw3hi9f3zxyy3xln@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 5275bab70313..79273ecf92eb 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -770,7 +770,7 @@ static int replay_fork_event(struct perf_sched *sched,
 	if (child == NULL || parent == NULL) {
 		pr_debug("thread does not exist on fork event: child %p, parent %p\n",
 				 child, parent);
-		return 0;
+		goto out_put;
 	}
 
 	if (verbose) {
@@ -781,6 +781,9 @@ static int replay_fork_event(struct perf_sched *sched,
 
 	register_pid(sched, parent->tid, thread__comm_str(parent));
 	register_pid(sched, child->tid, thread__comm_str(child));
+out_put:
+	thread__put(child);
+	thread__put(parent);
 	return 0;
 }
 
@@ -957,7 +960,7 @@ static int latency_switch_event(struct perf_sched *sched,
 	struct work_atoms *out_events, *in_events;
 	struct thread *sched_out, *sched_in;
 	u64 timestamp0, timestamp = sample->time;
-	int cpu = sample->cpu;
+	int cpu = sample->cpu, err = -1;
 	s64 delta;
 
 	BUG_ON(cpu >= MAX_CPUS || cpu < 0);
@@ -976,15 +979,17 @@ static int latency_switch_event(struct perf_sched *sched,
 
 	sched_out = machine__findnew_thread(machine, -1, prev_pid);
 	sched_in = machine__findnew_thread(machine, -1, next_pid);
+	if (sched_out == NULL || sched_in == NULL)
+		goto out_put;
 
 	out_events = thread_atoms_search(&sched->atom_root, sched_out, &sched->cmp_pid);
 	if (!out_events) {
 		if (thread_atoms_insert(sched, sched_out))
-			return -1;
+			goto out_put;
 		out_events = thread_atoms_search(&sched->atom_root, sched_out, &sched->cmp_pid);
 		if (!out_events) {
 			pr_err("out-event: Internal tree error");
-			return -1;
+			goto out_put;
 		}
 	}
 	if (add_sched_out_event(out_events, sched_out_state(prev_state), timestamp))
@@ -993,22 +998,25 @@ static int latency_switch_event(struct perf_sched *sched,
 	in_events = thread_atoms_search(&sched->atom_root, sched_in, &sched->cmp_pid);
 	if (!in_events) {
 		if (thread_atoms_insert(sched, sched_in))
-			return -1;
+			goto out_put;
 		in_events = thread_atoms_search(&sched->atom_root, sched_in, &sched->cmp_pid);
 		if (!in_events) {
 			pr_err("in-event: Internal tree error");
-			return -1;
+			goto out_put;
 		}
 		/*
 		 * Take came in we have not heard about yet,
 		 * add in an initial atom in runnable state:
 		 */
 		if (add_sched_out_event(in_events, 'R', timestamp))
-			return -1;
+			goto out_put;
 	}
 	add_sched_in_event(in_events, timestamp);
-
-	return 0;
+	err = 0;
+out_put:
+	thread__put(sched_out);
+	thread__put(sched_in);
+	return err;
 }
 
 static int latency_runtime_event(struct perf_sched *sched,
@@ -1021,23 +1029,29 @@ static int latency_runtime_event(struct perf_sched *sched,
 	struct thread *thread = machine__findnew_thread(machine, -1, pid);
 	struct work_atoms *atoms = thread_atoms_search(&sched->atom_root, thread, &sched->cmp_pid);
 	u64 timestamp = sample->time;
-	int cpu = sample->cpu;
+	int cpu = sample->cpu, err = -1;
+
+	if (thread == NULL)
+		return -1;
 
 	BUG_ON(cpu >= MAX_CPUS || cpu < 0);
 	if (!atoms) {
 		if (thread_atoms_insert(sched, thread))
-			return -1;
+			goto out_put;
 		atoms = thread_atoms_search(&sched->atom_root, thread, &sched->cmp_pid);
 		if (!atoms) {
 			pr_err("in-event: Internal tree error");
-			return -1;
+			goto out_put;
 		}
 		if (add_sched_out_event(atoms, 'R', timestamp))
-			return -1;
+			goto out_put;
 	}
 
 	add_runtime_event(atoms, runtime, timestamp);
-	return 0;
+	err = 0;
+out_put:
+	thread__put(thread);
+	return err;
 }
 
 static int latency_wakeup_event(struct perf_sched *sched,
@@ -1050,19 +1064,22 @@ static int latency_wakeup_event(struct perf_sched *sched,
 	struct work_atom *atom;
 	struct thread *wakee;
 	u64 timestamp = sample->time;
+	int err = -1;
 
 	wakee = machine__findnew_thread(machine, -1, pid);
+	if (wakee == NULL)
+		return -1;
 	atoms = thread_atoms_search(&sched->atom_root, wakee, &sched->cmp_pid);
 	if (!atoms) {
 		if (thread_atoms_insert(sched, wakee))
-			return -1;
+			goto out_put;
 		atoms = thread_atoms_search(&sched->atom_root, wakee, &sched->cmp_pid);
 		if (!atoms) {
 			pr_err("wakeup-event: Internal tree error");
-			return -1;
+			goto out_put;
 		}
 		if (add_sched_out_event(atoms, 'S', timestamp))
-			return -1;
+			goto out_put;
 	}
 
 	BUG_ON(list_empty(&atoms->work_list));
@@ -1081,17 +1098,21 @@ static int latency_wakeup_event(struct perf_sched *sched,
 	 * skip in this case.
 	 */
 	if (sched->profile_cpu == -1 && atom->state != THREAD_SLEEPING)
-		return 0;
+		goto out_ok;
 
 	sched->nr_timestamps++;
 	if (atom->sched_out_time > timestamp) {
 		sched->nr_unordered_timestamps++;
-		return 0;
+		goto out_ok;
 	}
 
 	atom->state = THREAD_WAIT_CPU;
 	atom->wake_up_time = timestamp;
-	return 0;
+out_ok:
+	err = 0;
+out_put:
+	thread__put(wakee);
+	return err;
 }
 
 static int latency_migrate_task_event(struct perf_sched *sched,
@@ -1104,6 +1125,7 @@ static int latency_migrate_task_event(struct perf_sched *sched,
 	struct work_atoms *atoms;
 	struct work_atom *atom;
 	struct thread *migrant;
+	int err = -1;
 
 	/*
 	 * Only need to worry about migration when profiling one CPU.
@@ -1112,18 +1134,20 @@ static int latency_migrate_task_event(struct perf_sched *sched,
 		return 0;
 
 	migrant = machine__findnew_thread(machine, -1, pid);
+	if (migrant == NULL)
+		return -1;
 	atoms = thread_atoms_search(&sched->atom_root, migrant, &sched->cmp_pid);
 	if (!atoms) {
 		if (thread_atoms_insert(sched, migrant))
-			return -1;
+			goto out_put;
 		register_pid(sched, migrant->tid, thread__comm_str(migrant));
 		atoms = thread_atoms_search(&sched->atom_root, migrant, &sched->cmp_pid);
 		if (!atoms) {
 			pr_err("migration-event: Internal tree error");
-			return -1;
+			goto out_put;
 		}
 		if (add_sched_out_event(atoms, 'R', timestamp))
-			return -1;
+			goto out_put;
 	}
 
 	BUG_ON(list_empty(&atoms->work_list));
@@ -1135,8 +1159,10 @@ static int latency_migrate_task_event(struct perf_sched *sched,
 
 	if (atom->sched_out_time > timestamp)
 		sched->nr_unordered_timestamps++;
-
-	return 0;
+	err = 0;
+out_put:
+	thread__put(migrant);
+	return err;
 }
 
 static void output_lat_thread(struct perf_sched *sched, struct work_atoms *work_list)
@@ -1330,8 +1356,10 @@ static int map_switch_event(struct perf_sched *sched, struct perf_evsel *evsel,
 	}
 
 	sched_in = machine__findnew_thread(machine, -1, next_pid);
+	if (sched_in == NULL)
+		return -1;
 
-	sched->curr_thread[this_cpu] = sched_in;
+	sched->curr_thread[this_cpu] = thread__get(sched_in);
 
 	printf("  ");
 
@@ -1381,6 +1409,8 @@ static int map_switch_event(struct perf_sched *sched, struct perf_evsel *evsel,
 		printf("\n");
 	}
 
+	thread__put(sched_in);
+
 	return 0;
 }
 

commit ff5f3bbd40bfb8632f826f1f83223d95363f36af
Author: Yunlong Song <yunlong.song@huawei.com>
Date:   Tue Mar 31 21:46:36 2015 +0800

    perf sched replay: Use replay_repeat to calculate the runavg of cpu usage instead of the default value 10
    
    Since sched->replay_repeat is set to 10 as default, the sched->run_avg,
    sched->runavg_cpu_usage, and sched->runavg_parent_cpu_usage all use
    10 to calculate their value.
    
    However, the replay_repeat can be changed to other value by using -r
    option, so the calculation above should use replay_repeat to achieve
    more accurate results instead of the default value 10.
    
    Signed-off-by: Yunlong Song <yunlong.song@huawei.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Wang Nan <wangnan0@huawei.com>
    Link: http://lkml.kernel.org/r/1427809596-29559-10-git-send-email-yunlong.song@huawei.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 7b7b798b22b2..5275bab70313 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -607,13 +607,13 @@ static void wait_for_tasks(struct perf_sched *sched)
 	cpu_usage_1 = get_cpu_usage_nsec_parent();
 	if (!sched->runavg_cpu_usage)
 		sched->runavg_cpu_usage = sched->cpu_usage;
-	sched->runavg_cpu_usage = (sched->runavg_cpu_usage * 9 + sched->cpu_usage) / 10;
+	sched->runavg_cpu_usage = (sched->runavg_cpu_usage * (sched->replay_repeat - 1) + sched->cpu_usage) / sched->replay_repeat;
 
 	sched->parent_cpu_usage = cpu_usage_1 - cpu_usage_0;
 	if (!sched->runavg_parent_cpu_usage)
 		sched->runavg_parent_cpu_usage = sched->parent_cpu_usage;
-	sched->runavg_parent_cpu_usage = (sched->runavg_parent_cpu_usage * 9 +
-					 sched->parent_cpu_usage)/10;
+	sched->runavg_parent_cpu_usage = (sched->runavg_parent_cpu_usage * (sched->replay_repeat - 1) +
+					 sched->parent_cpu_usage)/sched->replay_repeat;
 
 	ret = pthread_mutex_lock(&sched->start_work_mutex);
 	BUG_ON(ret);
@@ -645,7 +645,7 @@ static void run_one_test(struct perf_sched *sched)
 	sched->sum_fluct += fluct;
 	if (!sched->run_avg)
 		sched->run_avg = delta;
-	sched->run_avg = (sched->run_avg * 9 + delta) / 10;
+	sched->run_avg = (sched->run_avg * (sched->replay_repeat - 1) + delta) / sched->replay_repeat;
 
 	printf("#%-3ld: %0.3f, ", sched->nr_runs, (double)delta / 1000000.0);
 

commit f0dd330fdf07d295ac468660cf60341796d5d501
Author: Yunlong Song <yunlong.song@huawei.com>
Date:   Tue Mar 31 21:46:35 2015 +0800

    perf sched replay: Support using -f to override perf.data file ownership
    
    Enable to use perf.data when it is not owned by current user or root.
    
    Example:
    
     $ ls -al perf.data
     -rw------- 1 Yunlong.Song Yunlong.Song 5321918 Mar 25 15:14 perf.data
     $ sudo id
     uid=0(root) gid=0(root) groups=0(root),64(pkcs11)
    
    Before this patch:
    
     $ sudo perf sched replay -f
     run measurement overhead: 98 nsecs
     sleep measurement overhead: 52909 nsecs
     the run test took 1000015 nsecs
     the sleep test took 1054253 nsecs
     File perf.data not owned by current user or root (use -f to override)
    
    As shown above, the -f option does not work at all.
    
    After this patch:
    
     $ sudo perf sched replay -f
     run measurement overhead: 221 nsecs
     sleep measurement overhead: 40514 nsecs
     the run test took 1000003 nsecs
     the sleep test took 1056098 nsecs
     nr_run_events:        10
     nr_sleep_events:      1562
     nr_wakeup_events:     5
     task      0 (                  :1:         1), nr_events: 1
     task      1 (                  :2:         2), nr_events: 1
     task      2 (                  :3:         3), nr_events: 1
     ...
     ...
     task   1549 (             :163132:    163132), nr_events: 1
     task   1550 (             :163540:    163540), nr_events: 1
     task   1551 (           <unknown>:         0), nr_events: 10
     ------------------------------------------------------------
     #1  : 50.198, ravg: 50.20, cpu: 2335.18 / 2335.18
     #2  : 219.099, ravg: 67.09, cpu: 2835.11 / 2385.17
     #3  : 238.626, ravg: 84.24, cpu: 3278.26 / 2474.48
     #4  : 200.364, ravg: 95.85, cpu: 2977.41 / 2524.77
     #5  : 176.882, ravg: 103.96, cpu: 2801.35 / 2552.43
     #6  : 191.093, ravg: 112.67, cpu: 2813.70 / 2578.56
     #7  : 189.448, ravg: 120.35, cpu: 2809.21 / 2601.62
     #8  : 200.637, ravg: 128.38, cpu: 2849.91 / 2626.45
     #9  : 248.338, ravg: 140.37, cpu: 4380.61 / 2801.87
     #10 : 511.139, ravg: 177.45, cpu: 3077.73 / 2829.45
    
    As shown above, the -f option really works now.
    
    Besides for replay, -f option can also work for latency and map.
    
    Signed-off-by: Yunlong Song <yunlong.song@huawei.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Wang Nan <wangnan0@huawei.com>
    Link: http://lkml.kernel.org/r/1427809596-29559-9-git-send-email-yunlong.song@huawei.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 5ab58c6d2467..7b7b798b22b2 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1487,6 +1487,7 @@ static int perf_sched__read_events(struct perf_sched *sched)
 	struct perf_data_file file = {
 		.path = input_name,
 		.mode = PERF_DATA_MODE_READ,
+		.force = sched->force,
 	};
 	int rc = -1;
 

commit 939cda521a24ae4dbf3beec983abd519bce56231
Author: Yunlong Song <yunlong.song@huawei.com>
Date:   Tue Mar 31 21:46:34 2015 +0800

    perf sched replay: Fix the EMFILE error caused by the limitation of the maximum open files
    
    The soft maximum number of open files for a calling process is 1024,
    which is defined as INR_OPEN_CUR in include/uapi/linux/fs.h, and the
    hard maximum number of open files for a calling process is 4096, which
    is defined as INR_OPEN_MAX in include/uapi/linux/fs.h.
    
    Both INR_OPEN_CUR and INR_OPEN_MAX are used to limit the value of
    RLIMIT_NOFILE in include/asm-generic/resource.h.
    
    And the soft maximum number finally decides the limitation of the
    maximum files which are allowed to be opened.
    
    That is to say a process can use at most 1024 file descriptors for its
    o pened files, or an EMFILE error will happen.
    
    This error can be fixed by increasing the soft maximum number, under the
    constraint that the soft maximum number can not exceed the hard maximum
    number, or both soft and hard maximum number should be increased
    simultaneously with privilege.
    
    For perf sched replay, it uses sys_perf_event_open to create the file
    descriptor for each of the tasks in order to handle information of perf
    events.
    
    That is to say each task needs a unique file descriptor. In x86_64,
    there may be over 1024 or 4096 tasks correspoinding to the record in
    perf.data, which causes that no enough file descriptors can be used.
    
    As a result, EMFILE error happens and stops the replay process. To solve
    this problem, we adaptively increase the soft and hard maximum number of
    open files with a '-f' option.
    
    Example:
    
    Test environment: x86_64 with 160 cores
    
     $ cat /proc/sys/kernel/pid_max
     163840
     $ cat /proc/sys/fs/file-max
     6815744
     $ ulimit -Sn
     1024
     $ ulimit -Hn
     4096
    
    Before this patch:
    
     $ perf sched replay
     ...
     task   1549 (             :163132:    163132), nr_events: 1
     task   1550 (             :163540:    163540), nr_events: 1
     task   1551 (           <unknown>:         0), nr_events: 10
     Error: sys_perf_event_open() syscall returned with -1 (Too many open
     files)
    
    After this patch:
    
     $ perf sched replay
     ...
     task   1549 (             :163132:    163132), nr_events: 1
     task   1550 (             :163540:    163540), nr_events: 1
     task   1551 (           <unknown>:         0), nr_events: 10
     Error: sys_perf_event_open() syscall returned with -1 (Too many open
     files)
     Have a try with -f option
    
     $ perf sched replay -f
     ...
     task   1549 (             :163132:    163132), nr_events: 1
     task   1550 (             :163540:    163540), nr_events: 1
     task   1551 (           <unknown>:         0), nr_events: 10
     ------------------------------------------------------------
     #1  : 54.401, ravg: 54.40, cpu: 3285.21 / 3285.21
     #2  : 199.548, ravg: 68.92, cpu: 4999.65 / 3456.66
     #3  : 170.483, ravg: 79.07, cpu: 1349.94 / 3245.99
     #4  : 192.034, ravg: 90.37, cpu: 1322.88 / 3053.67
     #5  : 182.929, ravg: 99.62, cpu: 1406.51 / 2888.96
     #6  : 152.974, ravg: 104.96, cpu: 1167.54 / 2716.82
     #7  : 155.579, ravg: 110.02, cpu: 2992.53 / 2744.39
     #8  : 130.557, ravg: 112.08, cpu: 1126.43 / 2582.59
     #9  : 138.520, ravg: 114.72, cpu: 1253.22 / 2449.65
     #10 : 134.328, ravg: 116.68, cpu: 1587.95 / 2363.48
    
    Signed-off-by: Yunlong Song <yunlong.song@huawei.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Wang Nan <wangnan0@huawei.com>
    Link: http://lkml.kernel.org/r/1427809596-29559-8-git-send-email-yunlong.song@huawei.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 3261300c08f0..5ab58c6d2467 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -170,6 +170,7 @@ struct perf_sched {
 	u64		 cpu_last_switched[MAX_CPUS];
 	struct rb_root	 atom_root, sorted_atom_root;
 	struct list_head sort_list, cmp_pid;
+	bool force;
 };
 
 static u64 get_nsecs(void)
@@ -437,24 +438,43 @@ static u64 get_cpu_usage_nsec_parent(void)
 	return sum;
 }
 
-static int self_open_counters(void)
+static int self_open_counters(struct perf_sched *sched, unsigned long cur_task)
 {
 	struct perf_event_attr attr;
-	char sbuf[STRERR_BUFSIZE];
+	char sbuf[STRERR_BUFSIZE], info[STRERR_BUFSIZE];
 	int fd;
+	struct rlimit limit;
+	bool need_privilege = false;
 
 	memset(&attr, 0, sizeof(attr));
 
 	attr.type = PERF_TYPE_SOFTWARE;
 	attr.config = PERF_COUNT_SW_TASK_CLOCK;
 
+force_again:
 	fd = sys_perf_event_open(&attr, 0, -1, -1,
 				 perf_event_open_cloexec_flag());
 
 	if (fd < 0) {
+		if (errno == EMFILE) {
+			if (sched->force) {
+				BUG_ON(getrlimit(RLIMIT_NOFILE, &limit) == -1);
+				limit.rlim_cur += sched->nr_tasks - cur_task;
+				if (limit.rlim_cur > limit.rlim_max) {
+					limit.rlim_max = limit.rlim_cur;
+					need_privilege = true;
+				}
+				if (setrlimit(RLIMIT_NOFILE, &limit) == -1) {
+					if (need_privilege && errno == EPERM)
+						strcpy(info, "Need privilege\n");
+				} else
+					goto force_again;
+			} else
+				strcpy(info, "Have a try with -f option\n");
+		}
 		pr_err("Error: sys_perf_event_open() syscall returned "
-		       "with %d (%s)\n", fd,
-		       strerror_r(errno, sbuf, sizeof(sbuf)));
+		       "with %d (%s)\n%s", fd,
+		       strerror_r(errno, sbuf, sizeof(sbuf)), info);
 		exit(EXIT_FAILURE);
 	}
 	return fd;
@@ -542,7 +562,7 @@ static void create_tasks(struct perf_sched *sched)
 		BUG_ON(parms == NULL);
 		parms->task = task = sched->tasks[i];
 		parms->sched = sched;
-		parms->fd = self_open_counters();
+		parms->fd = self_open_counters(sched, i);
 		sem_init(&task->sleep_sem, 0, 0);
 		sem_init(&task->ready_for_work, 0, 0);
 		sem_init(&task->work_done_sem, 0, 0);
@@ -1700,6 +1720,7 @@ int cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)
 		    "be more verbose (show symbol address, etc)"),
 	OPT_BOOLEAN('D', "dump-raw-trace", &dump_trace,
 		    "dump raw trace in ASCII"),
+	OPT_BOOLEAN('f', "force", &sched.force, "don't complain, do it"),
 	OPT_END()
 	};
 	const struct option sched_options[] = {

commit 1aff59be53ef37aa9943fb5f772f03148f789bb6
Author: Yunlong Song <yunlong.song@huawei.com>
Date:   Tue Mar 31 21:46:33 2015 +0800

    perf sched replay: Handle the dead halt of sem_wait when create_tasks() fails for any task
    
    Since there is sem_wait for each task in the wait_for_tasks(), e.g.
    sem_wait(&task->work_done_sem).
    
    The sem_wait can continue only when work_done_sem is greater than 0, or
    it will be blocked.
    
    For perf sched replay, one task may sem_post the work_done_sem of
    another task, which causes the work_done_sem of that task processed in a
    reasonable sequence, e.g. sem_post, sem_wait, sem_wait, sem_post...
    
    This sequence simulates the sched process of the running tasks at the
    time when perf sched record runs.
    
    As a result, all the tasks are required and their threads must be
    successfully created.
    
    If any one (task A) of the tasks fails to create its thread, then
    another task (task B), whose work_done_sem needs sem_post from that
    failed task A, may likely block itself due to seg_wait.
    
    And this is a dead halt, since task B's thread_func cannot continue at
    all.
    
    To solve this problem, perf sched replay should exit once any task fails
    to create its thread.
    
    Example:
    
    Test environment: x86_64 with 160 cores
    
    Before this patch:
    
     $ perf sched replay
     ...
     Error: sys_perf_event_open() syscall returned with -1 (Too many open
     files)
     ------------------------------------------------------------    <- dead halt
    
    After this patch:
    
     $ perf sched replay
     ...
     task   1551 (           <unknown>:         0), nr_events: 10
     Error: sys_perf_event_open() syscall returned with -1 (Too many open
     files)
     $
    
    As shown above, perf sched replay finishes the process after printing an
    error message and does not block itself.
    
    Signed-off-by: Yunlong Song <yunlong.song@huawei.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Wang Nan <wangnan0@huawei.com>
    Link: http://lkml.kernel.org/r/1427809596-29559-7-git-send-email-yunlong.song@huawei.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 7fe3b3cb4cc8..3261300c08f0 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -451,10 +451,12 @@ static int self_open_counters(void)
 	fd = sys_perf_event_open(&attr, 0, -1, -1,
 				 perf_event_open_cloexec_flag());
 
-	if (fd < 0)
+	if (fd < 0) {
 		pr_err("Error: sys_perf_event_open() syscall returned "
 		       "with %d (%s)\n", fd,
 		       strerror_r(errno, sbuf, sizeof(sbuf)));
+		exit(EXIT_FAILURE);
+	}
 	return fd;
 }
 

commit 08097abc11bcee21355dd857852a807b2a30b79f
Author: Yunlong Song <yunlong.song@huawei.com>
Date:   Tue Mar 31 21:46:32 2015 +0800

    perf sched replay: Fix the segmentation fault problem caused by pr_err in threads
    
    The pr_err in self_open_counters() prints error message to stderr.
    Unlike stdout, stderr uses memory buffer on the stack of each calling
    process.
    
    The pr_err in self_open_counters() works in a thread called thread_func
    created in function create_tasks, which concurrently creates
    sched->nr_tasks threads.
    
    If the error happens and pr_err prints the error message in each of
    these threads, the stack size of the perf process (default is 8192
    kbytes) will quickly run out and the segmentation fault will happen
    then.
    
    To solve this problem, pr_err with self_open_counters() should be moved
    from newly created threads to the old main thread of the perf process.
    Then the pr_err can work in a stable situation without the strange
    segmentation fault problem.
    
    Example:
    
    Test environment: x86_64 with 160 cores
    
    Before this patch:
    
     $ perf sched replay
     ...
     task   1549 (             :163132:    163132), nr_events: 1
     task   1550 (             :163540:    163540), nr_events: 1
     task   1551 (           <unknown>:         0), nr_events: 10
     Segmentation fault
    
    After this patch:
    
     $ perf sched replay
     ...
     task   1549 (             :163132:    163132), nr_events: 1
     task   1550 (             :163540:    163540), nr_events: 1
     task   1551 (           <unknown>:         0), nr_events: 10
     ...
    
    As shown above, the result continues without any segmentation fault.
    
    Signed-off-by: Yunlong Song <yunlong.song@huawei.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Wang Nan <wangnan0@huawei.com>
    Link: http://lkml.kernel.org/r/1427809596-29559-6-git-send-email-yunlong.song@huawei.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index dd714818fa4d..7fe3b3cb4cc8 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -472,6 +472,7 @@ static u64 get_cpu_usage_nsec_self(int fd)
 struct sched_thread_parms {
 	struct task_desc  *task;
 	struct perf_sched *sched;
+	int fd;
 };
 
 static void *thread_func(void *ctx)
@@ -482,13 +483,12 @@ static void *thread_func(void *ctx)
 	u64 cpu_usage_0, cpu_usage_1;
 	unsigned long i, ret;
 	char comm2[22];
-	int fd;
+	int fd = parms->fd;
 
 	zfree(&parms);
 
 	sprintf(comm2, ":%s", this_task->comm);
 	prctl(PR_SET_NAME, comm2);
-	fd = self_open_counters();
 	if (fd < 0)
 		return NULL;
 again:
@@ -540,6 +540,7 @@ static void create_tasks(struct perf_sched *sched)
 		BUG_ON(parms == NULL);
 		parms->task = task = sched->tasks[i];
 		parms->sched = sched;
+		parms->fd = self_open_counters();
 		sem_init(&task->sleep_sem, 0, 0);
 		sem_init(&task->ready_for_work, 0, 0);
 		sem_init(&task->work_done_sem, 0, 0);

commit 3a423a5c36d1a28a258beaa7db855568b82d07ab
Author: Yunlong Song <yunlong.song@huawei.com>
Date:   Tue Mar 31 21:46:31 2015 +0800

    perf sched replay: Realloc the memory of pid_to_task stepwise to adapt to the different pid_max configurations
    
    Although the memory of pid_to_task can be allocated via calloc according
    to the value of /proc/sys/kernel/pid_max, it cannot handle the case when
    pid_max is changed after 'perf sched record' has created its perf.data.
    
    If the new pid_max configured in 'perf sched replay' is smaller than the
    old pid_max configured in 'perf sched record', then it will cause the
    assertion failure problem.
    
    To solve this problem, we realloc the memory of pid_to_task stepwise
    once the passed-in pid parameter in register_pid is larger than the
    current pid_max.
    
    Example:
    
    Test environment: x86_64 with 160 cores
    
     $ cat /proc/sys/kernel/pid_max
     163840
     $ perf sched record ls
     $ echo 5000 > /proc/sys/kernel/pid_max
     $ cat /proc/sys/kernel/pid_max
     5000
    
    Before this patch:
    
     $ perf sched replay
     run measurement overhead: 221 nsecs
     sleep measurement overhead: 55356 nsecs
     the run test took 1000011 nsecs
     the sleep test took 1060940 nsecs
     perf: builtin-sched.c:337: register_pid: Assertion `!(pid >= (unsigned
     long)pid_max)' failed.
     Aborted
    
    After this patch:
    
     $ perf sched replay
     run measurement overhead: 221 nsecs
     sleep measurement overhead: 55611 nsecs
     the run test took 1000026 nsecs
     the sleep test took 1060486 nsecs
     nr_run_events:        10
     nr_sleep_events:      1562
     nr_wakeup_events:     5
     task      0 (                  :1:         1), nr_events: 1
     task      1 (                  :2:         2), nr_events: 1
     task      2 (                  :3:         3), nr_events: 1
     task      3 (                  :5:         5), nr_events: 1
     ...
    
    Signed-off-by: Yunlong Song <yunlong.song@huawei.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Wang Nan <wangnan0@huawei.com>
    Link: http://lkml.kernel.org/r/1427809596-29559-5-git-send-email-yunlong.song@huawei.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 20d887b222e4..dd714818fa4d 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -334,7 +334,12 @@ static struct task_desc *register_pid(struct perf_sched *sched,
 			pid_max = MAX_PID;
 		BUG_ON((sched->pid_to_task = calloc(pid_max, sizeof(struct task_desc *))) == NULL);
 	}
-	BUG_ON(pid >= (unsigned long)pid_max);
+	if (pid >= (unsigned long)pid_max) {
+		BUG_ON((sched->pid_to_task = realloc(sched->pid_to_task, (pid + 1) *
+			sizeof(struct task_desc *))) == NULL);
+		while (pid >= (unsigned long)pid_max)
+			sched->pid_to_task[pid_max++] = NULL;
+	}
 
 	task = sched->pid_to_task[pid];
 

commit cb06ac256a16fc1a5ab063107c2b35b3b9e95102
Author: Yunlong Song <yunlong.song@huawei.com>
Date:   Tue Mar 31 21:46:30 2015 +0800

    perf sched replay: Alloc the memory of pid_to_task dynamically to adapt to the unexpected change of pid_max
    
    The current memory allocation of struct task_desc *pid_to_task[MAX_PID]
    is in a permanent and preset way, and it has two problems:
    
    Problem 1: If the pid_max, which is the max number of pids in the
    system, is much smaller than MAX_PID (1024*1000), then it causes a waste
    of stack memory. This may happen in the case where the number of cpu
    cores is much smaller than 1000.
    
    Problem 2: If the pid_max is changed from the default value to a value
    larger than MAX_PID, then it will cause assertion failure problem. The
    maximum value of pid_max can be set to pid_max_max (see pidmap_init
    defined in kernel/pid.c), which equals to PID_MAX_LIMIT. In x86_64,
    PID_MAX_LIMIT is 4*1024*1024 (defined in include/linux/threads.h). This
    value is much larger than MAX_PID, and will take up 32768 Kbytes
    (4*1024*1024*8/1024) for memory allocation of pid_to_task, which is much
    larger than the default 8192 Kbytes of the stack size of calling
    process.
    
    Due to these two problems, we use calloc to allocate the memory of
    pid_to_task dynamically.
    
    Example:
    
    Test environment: x86_64 with 160 cores
    
     $ cat /proc/sys/kernel/pid_max
     163840
     $ echo 1025000 > /proc/sys/kernel/pid_max
     $ cat /proc/sys/kernel/pid_max
     1025000
    
    Run some applications until the pid of some process is greater than
    the value of MAX_PID (1024*1000).
    
    Before this patch:
    
     $ perf sched replay
     run measurement overhead: 221 nsecs
     sleep measurement overhead: 55480 nsecs
     the run test took 1000008 nsecs
     the sleep test took 1063151 nsecs
     perf: builtin-sched.c:330: register_pid: Assertion `!(pid >= 1024000)'
     failed.
     Aborted
    
    After this patch:
    
     $ perf sched replay
     run measurement overhead: 221 nsecs
     sleep measurement overhead: 55435 nsecs
     the run test took 1000004 nsecs
     the sleep test took 1059312 nsecs
     nr_run_events:        10
     nr_sleep_events:      1562
     nr_wakeup_events:     5
     task      0 (                  :1:         1), nr_events: 1
     task      1 (                  :2:         2), nr_events: 1
     task      2 (                  :3:         3), nr_events: 1
     task      3 (                  :5:         5), nr_events: 1
     ...
    
    Signed-off-by: Yunlong Song <yunlong.song@huawei.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Wang Nan <wangnan0@huawei.com>
    Link: http://lkml.kernel.org/r/1427809596-29559-4-git-send-email-yunlong.song@huawei.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index c46610447ede..20d887b222e4 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -23,6 +23,7 @@
 #include <semaphore.h>
 #include <pthread.h>
 #include <math.h>
+#include <api/fs/fs.h>
 
 #define PR_SET_NAME		15               /* Set process name */
 #define MAX_CPUS		4096
@@ -124,7 +125,7 @@ struct perf_sched {
 	struct perf_tool tool;
 	const char	 *sort_order;
 	unsigned long	 nr_tasks;
-	struct task_desc *pid_to_task[MAX_PID];
+	struct task_desc **pid_to_task;
 	struct task_desc **tasks;
 	const struct trace_sched_handler *tp_handler;
 	pthread_mutex_t	 start_work_mutex;
@@ -326,8 +327,14 @@ static struct task_desc *register_pid(struct perf_sched *sched,
 				      unsigned long pid, const char *comm)
 {
 	struct task_desc *task;
+	static int pid_max;
 
-	BUG_ON(pid >= MAX_PID);
+	if (sched->pid_to_task == NULL) {
+		if (sysctl__read_int("kernel/pid_max", &pid_max) < 0)
+			pid_max = MAX_PID;
+		BUG_ON((sched->pid_to_task = calloc(pid_max, sizeof(struct task_desc *))) == NULL);
+	}
+	BUG_ON(pid >= (unsigned long)pid_max);
 
 	task = sched->pid_to_task[pid];
 

commit a35e27d0e5d801ff75481a8f639bb4d59ea1aafa
Author: Yunlong Song <yunlong.song@huawei.com>
Date:   Tue Mar 31 21:46:29 2015 +0800

    perf sched replay: Increase the MAX_PID value to fix assertion failure problem
    
    Current MAX_PID is only 65536, which will cause assertion failure problem
    when CPU cores are more than 64 in x86_64.
    
    This is because the pid_max value in x86_64 is at least
    PIDS_PER_CPU_DEFAULT * num_possible_cpus() (see function pidmap_init
    defined in kernel/pid.c), where PIDS_PER_CPU_DEFAULT is 1024 (defined in
    include/linux/threads.h).
    
    Thus for MAX_PID = 65536, the correspoinding CPU cores are
    65536/1024=64.  This is obviously not enough at all for x86_64, and will
    cause an assertion failure problem due to BUG_ON(pid >= MAX_PID) in the
    codes.
    
    We increase MAX_PID value from 65536 to 1024*1000, which can be used in
    x86_64 with 1000 cores.
    
    This number is finally decided according to the limitation of stack size
    of calling process.
    
    Use 'ulimit -a', the result shows the stack size of any process is 8192
    Kbytes, which is defined in include/uapi/linux/resource.h (#define
    _STK_LIM (8*1024*1024)).
    
    Thus we choose a large enough value for MAX_PID, and make it satisfy to
    the limitation of the stack size, i.e., making the perf process take up
    a memory space just smaller than 8192 Kbytes.
    
    We have calculated and tested that 1024*1000 is OK for MAX_PID.
    
    This means perf sched replay can now be used with at most 1000 cores in
    x86_64 without any assertion failure problem.
    
    Example:
    
    Test environment: x86_64 with 160 cores
    
     $ cat /proc/sys/kernel/pid_max
     163840
    
    Before this patch:
    
     $ perf sched replay
     run measurement overhead: 240 nsecs
     sleep measurement overhead: 55379 nsecs
     the run test took 1000004 nsecs
     the sleep test took 1059424 nsecs
     perf: builtin-sched.c:330: register_pid: Assertion `!(pid >= 65536)'
     failed.
     Aborted
    
    After this patch:
    
     $ perf sched replay
     run measurement overhead: 221 nsecs
     sleep measurement overhead: 55397 nsecs
     the run test took 999920 nsecs
     the sleep test took 1053313 nsecs
     nr_run_events:        10
     nr_sleep_events:      1562
     nr_wakeup_events:     5
     task      0 (                  :1:         1), nr_events: 1
     task      1 (                  :2:         2), nr_events: 1
     task      2 (                  :3:         3), nr_events: 1
     task      3 (                  :5:         5), nr_events: 1
     ...
    
    Signed-off-by: Yunlong Song <yunlong.song@huawei.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Wang Nan <wangnan0@huawei.com>
    Link: http://lkml.kernel.org/r/1427809596-29559-3-git-send-email-yunlong.song@huawei.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index a1893e8dfe17..c46610447ede 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -28,7 +28,7 @@
 #define MAX_CPUS		4096
 #define COMM_LEN		20
 #define SYM_LEN			129
-#define MAX_PID			65536
+#define MAX_PID			1024000
 
 struct sched_atom;
 

commit 0755bc4dc77a876aa60d4b3d33b5f6506f21f91b
Author: Yunlong Song <yunlong.song@huawei.com>
Date:   Tue Mar 31 21:46:28 2015 +0800

    perf sched replay: Use struct task_desc instead of struct task_task for correct meaning
    
    There is no struct task_task at all, thus it is a typo error in the old
    commits, now fix it to what it should be in order to avoid unnecessary
    misunderstanding.
    
    Signed-off-by: Yunlong Song <yunlong.song@huawei.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Wang Nan <wangnan0@huawei.com>
    Link: http://lkml.kernel.org/r/1427809596-29559-2-git-send-email-yunlong.song@huawei.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 3b3a5bb97059..a1893e8dfe17 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -346,7 +346,7 @@ static struct task_desc *register_pid(struct perf_sched *sched,
 
 	sched->pid_to_task[pid] = task;
 	sched->nr_tasks++;
-	sched->tasks = realloc(sched->tasks, sched->nr_tasks * sizeof(struct task_task *));
+	sched->tasks = realloc(sched->tasks, sched->nr_tasks * sizeof(struct task_desc *));
 	BUG_ON(!sched->tasks);
 	sched->tasks[task->nr] = task;
 

commit b7b61cbebd789a3dbca522e3fdb727fe5c95593f
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Tue Mar 3 11:58:45 2015 -0300

    perf ordered_events: Shorten function signatures
    
    By keeping pointers to machines, evlist and tool in ordered_events.
    
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Don Zickus <dzickus@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Stephane Eranian <eranian@google.com>
    Link: http://lkml.kernel.org/n/tip-0c6huyaf59mqtm2ek9pmposl@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index a3ebf1d3c29d..3b3a5bb97059 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1467,7 +1467,7 @@ static int perf_sched__read_events(struct perf_sched *sched)
 		goto out_delete;
 
 	if (perf_session__has_traces(session, "record -R")) {
-		int err = perf_session__process_events(session, &sched->tool);
+		int err = perf_session__process_events(session);
 		if (err) {
 			pr_err("Failed to process events, error %d", err);
 			goto out_delete;

commit ae536acfacb65a4a9858c32b12361e09f84f4157
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Mon Mar 2 22:28:41 2015 -0300

    perf sched: No need to keep the session around
    
    We were keeping the session around just because we kept pointers to
    struct thread instances, but now we reference count them, so no need
    for deferring the perf_session__delete call to after we traverse the
    work_list entries.
    
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Don Zickus <dzickus@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Stephane Eranian <eranian@google.com>
    Link: http://lkml.kernel.org/n/tip-9agtck6jdr3rebdp39z1lo0e@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index e00e2eaf89da..a3ebf1d3c29d 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1439,8 +1439,7 @@ static int perf_sched__process_tracepoint_sample(struct perf_tool *tool __maybe_
 	return err;
 }
 
-static int perf_sched__read_events(struct perf_sched *sched,
-				   struct perf_session **psession)
+static int perf_sched__read_events(struct perf_sched *sched)
 {
 	const struct perf_evsel_str_handler handlers[] = {
 		{ "sched:sched_switch",	      process_sched_switch_event, },
@@ -1454,6 +1453,7 @@ static int perf_sched__read_events(struct perf_sched *sched,
 		.path = input_name,
 		.mode = PERF_DATA_MODE_READ,
 	};
+	int rc = -1;
 
 	session = perf_session__new(&file, false, &sched->tool);
 	if (session == NULL) {
@@ -1478,16 +1478,10 @@ static int perf_sched__read_events(struct perf_sched *sched,
 		sched->nr_lost_chunks = session->evlist->stats.nr_events[PERF_RECORD_LOST];
 	}
 
-	if (psession)
-		*psession = session;
-	else
-		perf_session__delete(session);
-
-	return 0;
-
+	rc = 0;
 out_delete:
 	perf_session__delete(session);
-	return -1;
+	return rc;
 }
 
 static void print_bad_events(struct perf_sched *sched)
@@ -1515,12 +1509,10 @@ static void print_bad_events(struct perf_sched *sched)
 static int perf_sched__lat(struct perf_sched *sched)
 {
 	struct rb_node *next;
-	struct perf_session *session;
 
 	setup_pager();
 
-	/* save session -- references to threads are held in work_list */
-	if (perf_sched__read_events(sched, &session))
+	if (perf_sched__read_events(sched))
 		return -1;
 
 	perf_sched__sort_lat(sched);
@@ -1537,6 +1529,7 @@ static int perf_sched__lat(struct perf_sched *sched)
 		work_list = rb_entry(next, struct work_atoms, node);
 		output_lat_thread(sched, work_list);
 		next = rb_next(next);
+		thread__zput(work_list->thread);
 	}
 
 	printf(" -----------------------------------------------------------------------------------------------------------------\n");
@@ -1548,7 +1541,6 @@ static int perf_sched__lat(struct perf_sched *sched)
 	print_bad_events(sched);
 	printf("\n");
 
-	perf_session__delete(session);
 	return 0;
 }
 
@@ -1557,7 +1549,7 @@ static int perf_sched__map(struct perf_sched *sched)
 	sched->max_cpu = sysconf(_SC_NPROCESSORS_CONF);
 
 	setup_pager();
-	if (perf_sched__read_events(sched, NULL))
+	if (perf_sched__read_events(sched))
 		return -1;
 	print_bad_events(sched);
 	return 0;
@@ -1572,7 +1564,7 @@ static int perf_sched__replay(struct perf_sched *sched)
 
 	test_calibrations(sched);
 
-	if (perf_sched__read_events(sched, NULL))
+	if (perf_sched__read_events(sched))
 		return -1;
 
 	printf("nr_run_events:        %ld\n", sched->nr_run_events);

commit f3b623b8490af7a9b819cbcf2d99ab4597ece94b
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Mon Mar 2 22:21:35 2015 -0300

    perf tools: Reference count struct thread
    
    We need to do that to stop accumulating entries in the dead_threads
    linked list, i.e. we were keeping references to threads in struct hists
    that continue to exist even after a thread exited and was removed from
    the machine threads rbtree.
    
    We still keep the dead_threads list, but just for debugging, allowing us
    to iterate at any given point over the threads that still are referenced
    by things like struct hist_entry.
    
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Don Zickus <dzickus@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Stephane Eranian <eranian@google.com>
    Link: http://lkml.kernel.org/n/tip-3ejvfyed0r7ue61dkurzjux4@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 7ce296618717..e00e2eaf89da 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -831,7 +831,7 @@ static int thread_atoms_insert(struct perf_sched *sched, struct thread *thread)
 		return -1;
 	}
 
-	atoms->thread = thread;
+	atoms->thread = thread__get(thread);
 	INIT_LIST_HEAD(&atoms->work_list);
 	__thread_latency_insert(&sched->atom_root, atoms, &sched->cmp_pid);
 	return 0;

commit 75be989a7a18e9666efd92b846ee48bed79e8086
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Sat Feb 14 14:50:11 2015 -0300

    perf evlist: Adopt events_stats from perf_session
    
    For tools that don't deal with perf.data files, thus do not need to
    use perf_session.
    
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Don Zickus <dzickus@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Link: http://lkml.kernel.org/n/tip-kglq67gvauq9tak02a4se00r@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 891c3930080e..7ce296618717 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1473,9 +1473,9 @@ static int perf_sched__read_events(struct perf_sched *sched,
 			goto out_delete;
 		}
 
-		sched->nr_events      = session->stats.nr_events[0];
-		sched->nr_lost_events = session->stats.total_lost;
-		sched->nr_lost_chunks = session->stats.nr_events[PERF_RECORD_LOST];
+		sched->nr_events      = session->evlist->stats.nr_events[0];
+		sched->nr_lost_events = session->evlist->stats.total_lost;
+		sched->nr_lost_chunks = session->evlist->stats.nr_events[PERF_RECORD_LOST];
 	}
 
 	if (psession)

commit b3f25b6e04495ace0851cf0b88514f41c7807af6
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Thu Oct 9 11:40:15 2014 -0300

    perf sched: Stop updating hists stats, not used
    
    Not used here, remove to reduce perf_evsel/hists structs interaction.
    
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Don Zickus <dzickus@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Jean Pihet <jean.pihet@linaro.org>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Link: http://lkml.kernel.org/n/tip-cb7wkk4a3jpoovzim914ih3c@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 9c9287fbf8e9..891c3930080e 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1431,9 +1431,6 @@ static int perf_sched__process_tracepoint_sample(struct perf_tool *tool __maybe_
 {
 	int err = 0;
 
-	evsel->hists.stats.total_period += sample->period;
-	hists__inc_nr_samples(&evsel->hists, true);
-
 	if (evsel->handler != NULL) {
 		tracepoint_handler f = evsel->handler;
 		err = f(tool, evsel, sample, machine);

commit fb74fbda42dc5bcbd9bae5d75bfb6755948db21d
Author: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
Date:   Thu Aug 14 02:22:47 2014 +0000

    perf sched: Use strerror_r instead of strerror
    
    Use strerror_r instead of strerror in error message for thread-safety.
    
    Signed-off-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Naohiro Aota <naota@elisp.net>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20140814022247.3545.4564.stgit@kbuild-fedora.novalocal
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index f5874a27b346..9c9287fbf8e9 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -428,6 +428,7 @@ static u64 get_cpu_usage_nsec_parent(void)
 static int self_open_counters(void)
 {
 	struct perf_event_attr attr;
+	char sbuf[STRERR_BUFSIZE];
 	int fd;
 
 	memset(&attr, 0, sizeof(attr));
@@ -440,7 +441,8 @@ static int self_open_counters(void)
 
 	if (fd < 0)
 		pr_err("Error: sys_perf_event_open() syscall returned "
-		       "with %d (%s)\n", fd, strerror(errno));
+		       "with %d (%s)\n", fd,
+		       strerror_r(errno, sbuf, sizeof(sbuf)));
 	return fd;
 }
 

commit 0a7e6d1b6844bec2d6817615a693c7fce447b80d
Author: Namhyung Kim <namhyung@kernel.org>
Date:   Tue Aug 12 15:40:45 2014 +0900

    perf tools: Check recorded kernel version when finding vmlinux
    
    Currently vmlinux_path__init() only tries to find vmlinux file from
    current directory, /boot and some canonical directories with version
    number of the running kernel.  This can be a problem when reporting old
    data recorded on a kernel version not running currently.
    
    We can use --symfs option for this but it's annoying for user to do it
    always.  As we already have the info in the perf.data file, it can be
    changed to use it for the search automatically.
    
    Before:
    
      $ perf report
      ...
      # Samples: 4K of event 'cpu-clock'
      # Event count (approx.): 1067250000
      #
      # Overhead  Command     Shared Object      Symbol
      # ........  ..........  .................  ..............................
          71.87%     swapper  [kernel.kallsyms]  [k] recover_probed_instruction
    
    After:
    
      # Overhead  Command     Shared Object      Symbol
      # ........  ..........  .................  ....................
          71.87%     swapper  [kernel.kallsyms]  [k] native_safe_halt
    
    This requires to change signature of symbol__init() to receive struct
    perf_session_env *.
    
    Reported-by: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Namhyung Kim <namhyung.kim@lge.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Stephane Eranian <eranian@google.com>
    Link: http://lkml.kernel.org/r/1407825645-24586-14-git-send-email-namhyung@kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index dcd9ebf5a7df..f5874a27b346 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1462,7 +1462,7 @@ static int perf_sched__read_events(struct perf_sched *sched,
 		return -1;
 	}
 
-	symbol__init();
+	symbol__init(&session->header.env);
 
 	if (perf_session__set_tracepoints_handlers(session, handlers))
 		goto out_delete;

commit 0493410612486cadaa4e076caf4df3fa9cd20fde
Author: Namhyung Kim <namhyung@kernel.org>
Date:   Tue Aug 12 15:40:41 2014 +0900

    perf sched: Move call to symbol__init() after creating session
    
    This is a preparation of fixing dso__load_kernel_sym().  It needs a
    session info before calling symbol__init().
    
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Acked-by: Jiri Olsa <jolsa@kernel.org>
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Namhyung Kim <namhyung.kim@lge.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1407825645-24586-10-git-send-email-namhyung@kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 7c16aeb6b675..dcd9ebf5a7df 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1462,6 +1462,8 @@ static int perf_sched__read_events(struct perf_sched *sched,
 		return -1;
 	}
 
+	symbol__init();
+
 	if (perf_session__set_tracepoints_handlers(session, handlers))
 		goto out_delete;
 
@@ -1747,7 +1749,6 @@ int cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)
 	if (!strcmp(argv[0], "script"))
 		return cmd_script(argc, argv, prefix);
 
-	symbol__init();
 	if (!strncmp(argv[0], "rec", 3)) {
 		return __cmd_record(argc, argv);
 	} else if (!strncmp(argv[0], "lat", 3)) {

commit 0a8cb85c200c4082ed7e57efd90dd9d18c8d40b6
Author: Jiri Olsa <jolsa@kernel.org>
Date:   Sun Jul 6 14:18:21 2014 +0200

    perf tools: Rename ordered_samples bool to ordered_events
    
    The time ordering is generic for all kinds of events, so using generic
    name 'ordered_events' for ordered_samples bool in perf_tool struct.
    
    No functional change was intended.
    
    Signed-off-by: Jiri Olsa <jolsa@kernel.org>
    Acked-by: David Ahern <dsahern@gmail.com>
    Cc: Corey Ashford <cjashfor@linux.vnet.ibm.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Jean Pihet <jean.pihet@linaro.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/n/tip-07mrqzcuhsks9wfmxrzsvemz@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index f83c08c0dd87..7c16aeb6b675 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1662,7 +1662,7 @@ int cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)
 			.comm		 = perf_event__process_comm,
 			.lost		 = perf_event__process_lost,
 			.fork		 = perf_sched__process_fork_event,
-			.ordered_samples = true,
+			.ordered_events = true,
 		},
 		.cmp_pid	      = LIST_HEAD_INIT(sched.cmp_pid),
 		.sort_list	      = LIST_HEAD_INIT(sched.sort_list),

commit 57480d2cd93579d665e57e144e1e63f7f02ef058
Author: Yann Droneaud <ydroneaud@opteya.com>
Date:   Mon Jun 30 22:28:47 2014 +0200

    perf tools: Enable close-on-exec flag on perf file descriptor
    
    In commit a21b0b354d4a ('perf: Introduce a flag to enable
    close-on-exec in perf_event_open()'), flag PERF_FLAG_FD_CLOEXEC
    was added to perf_event_open(2) syscall to allows userspace
    to atomically enable close-on-exec behavor when creating
    the file descriptor.
    
    This patch makes perf tools use the new flag if supported
    by the kernel, so that the event file descriptors got
    automatically closed if perf tool exec a sub-command.
    
    Signed-off-by: Yann Droneaud <ydroneaud@opteya.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/n/1404160127-7475-1-git-send-email-ydroneaud@opteya.com
    Signed-off-by: Jiri Olsa <jolsa@kernel.org>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index b7f555add0c8..f83c08c0dd87 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -10,6 +10,7 @@
 #include "util/header.h"
 #include "util/session.h"
 #include "util/tool.h"
+#include "util/cloexec.h"
 
 #include "util/parse-options.h"
 #include "util/trace-event.h"
@@ -434,7 +435,8 @@ static int self_open_counters(void)
 	attr.type = PERF_TYPE_SOFTWARE;
 	attr.config = PERF_COUNT_SW_TASK_CLOCK;
 
-	fd = sys_perf_event_open(&attr, 0, -1, -1, 0);
+	fd = sys_perf_event_open(&attr, 0, -1, -1,
+				 perf_event_open_cloexec_flag());
 
 	if (fd < 0)
 		pr_err("Error: sys_perf_event_open() syscall returned "

commit 1fcb8768636d38cb6fdfeef83a5ee596c4bd9c56
Author: Adrian Hunter <adrian.hunter@intel.com>
Date:   Mon Jul 14 13:02:25 2014 +0300

    perf machine: Fix the value used for unknown pids
    
    The value used for unknown pids cannot be zero because that is used by
    the "idle" task.
    
    Use -1 instead.  Also handle the unknown pid case when creating map
    groups.
    
    Note that, threads with an unknown pid should not occur because fork (or
    synthesized) events precede the thread's existence.
    
    Signed-off-by: Adrian Hunter <adrian.hunter@intel.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Namhyung Kim <namhyung@gmail.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Link: http://lkml.kernel.org/r/1405332185-4050-2-git-send-email-adrian.hunter@intel.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index c38d06c04775..b7f555add0c8 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -935,8 +935,8 @@ static int latency_switch_event(struct perf_sched *sched,
 		return -1;
 	}
 
-	sched_out = machine__findnew_thread(machine, 0, prev_pid);
-	sched_in = machine__findnew_thread(machine, 0, next_pid);
+	sched_out = machine__findnew_thread(machine, -1, prev_pid);
+	sched_in = machine__findnew_thread(machine, -1, next_pid);
 
 	out_events = thread_atoms_search(&sched->atom_root, sched_out, &sched->cmp_pid);
 	if (!out_events) {
@@ -979,7 +979,7 @@ static int latency_runtime_event(struct perf_sched *sched,
 {
 	const u32 pid	   = perf_evsel__intval(evsel, sample, "pid");
 	const u64 runtime  = perf_evsel__intval(evsel, sample, "runtime");
-	struct thread *thread = machine__findnew_thread(machine, 0, pid);
+	struct thread *thread = machine__findnew_thread(machine, -1, pid);
 	struct work_atoms *atoms = thread_atoms_search(&sched->atom_root, thread, &sched->cmp_pid);
 	u64 timestamp = sample->time;
 	int cpu = sample->cpu;
@@ -1012,7 +1012,7 @@ static int latency_wakeup_event(struct perf_sched *sched,
 	struct thread *wakee;
 	u64 timestamp = sample->time;
 
-	wakee = machine__findnew_thread(machine, 0, pid);
+	wakee = machine__findnew_thread(machine, -1, pid);
 	atoms = thread_atoms_search(&sched->atom_root, wakee, &sched->cmp_pid);
 	if (!atoms) {
 		if (thread_atoms_insert(sched, wakee))
@@ -1072,7 +1072,7 @@ static int latency_migrate_task_event(struct perf_sched *sched,
 	if (sched->profile_cpu == -1)
 		return 0;
 
-	migrant = machine__findnew_thread(machine, 0, pid);
+	migrant = machine__findnew_thread(machine, -1, pid);
 	atoms = thread_atoms_search(&sched->atom_root, migrant, &sched->cmp_pid);
 	if (!atoms) {
 		if (thread_atoms_insert(sched, migrant))
@@ -1290,7 +1290,7 @@ static int map_switch_event(struct perf_sched *sched, struct perf_evsel *evsel,
 		return -1;
 	}
 
-	sched_in = machine__findnew_thread(machine, 0, next_pid);
+	sched_in = machine__findnew_thread(machine, -1, next_pid);
 
 	sched->curr_thread[this_cpu] = sched_in;
 

commit 1844dbcbe78503e0f4a8996d69da725d5e7a5177
Author: Namhyung Kim <namhyung@kernel.org>
Date:   Wed May 28 14:12:18 2014 +0900

    perf tools: Introduce hists__inc_nr_samples()
    
    There're some duplicate code for counting number of samples.  Add
    hists__inc_nr_samples() and reuse it.
    
    Suggested-by: Jiri Olsa <jolsa@kernel.org>
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Link: http://lkml.kernel.org/r/1401335910-16832-2-git-send-email-namhyung@kernel.org
    Signed-off-by: Jiri Olsa <jolsa@kernel.org>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index d7176830b9b2..c38d06c04775 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1428,7 +1428,7 @@ static int perf_sched__process_tracepoint_sample(struct perf_tool *tool __maybe_
 	int err = 0;
 
 	evsel->hists.stats.total_period += sample->period;
-	hists__inc_nr_events(&evsel->hists, PERF_RECORD_SAMPLE);
+	hists__inc_nr_samples(&evsel->hists, true);
 
 	if (evsel->handler != NULL) {
 		tracepoint_handler f = evsel->handler;

commit 9d372ca59bcb9339b4a34a9bf978a1fc15b68b03
Author: Dongsheng Yang <yangds.fnst@cn.fujitsu.com>
Date:   Fri May 16 14:37:05 2014 +0900

    perf sched: Cleanup, remove unused variables in map_switch_event()
    
    In map_switch_event(), we don't care the previous process currently,
    this patch remove the infomation we get but not used.
    
    Signed-off-by: Dongsheng Yang <yangds.fnst@cn.fujitsu.com>
    Link: http://lkml.kernel.org/r/1400218625-14613-1-git-send-email-yangds.fnst@cn.fujitsu.com
    Signed-off-by: Jiri Olsa <jolsa@kernel.org>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 0b4fe533e1a1..d7176830b9b2 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1266,9 +1266,8 @@ static int process_sched_wakeup_event(struct perf_tool *tool,
 static int map_switch_event(struct perf_sched *sched, struct perf_evsel *evsel,
 			    struct perf_sample *sample, struct machine *machine)
 {
-	const u32 prev_pid = perf_evsel__intval(evsel, sample, "prev_pid"),
-		  next_pid = perf_evsel__intval(evsel, sample, "next_pid");
-	struct thread *sched_out __maybe_unused, *sched_in;
+	const u32 next_pid = perf_evsel__intval(evsel, sample, "next_pid");
+	struct thread *sched_in;
 	int new_shortname;
 	u64 timestamp0, timestamp = sample->time;
 	s64 delta;
@@ -1291,7 +1290,6 @@ static int map_switch_event(struct perf_sched *sched, struct perf_evsel *evsel,
 		return -1;
 	}
 
-	sched_out = machine__findnew_thread(machine, 0, prev_pid);
 	sched_in = machine__findnew_thread(machine, 0, next_pid);
 
 	sched->curr_thread[this_cpu] = sched_in;

commit 67d6259dd021006ade25d67b045ad2089b5aba96
Author: Dongsheng Yang <yangds.fnst@cn.fujitsu.com>
Date:   Tue May 13 10:38:21 2014 +0900

    perf sched: Remove nr_state_machine_bugs in perf latency
    
    As we do not use .success in sched_wakeup event any more, then
    we can not guarantee that the task when wakeup event happen is
    out of run queue. So the message of nr_state_machine_bugs is
    not correct.
    
    Signed-off-by: Dongsheng Yang <yangds.fnst@cn.fujitsu.com>
    Link: http://lkml.kernel.org/r/1399945101-21736-1-git-send-email-yangds.fnst@cn.fujitsu.com
    Signed-off-by: Jiri Olsa <jolsa@kernel.org>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index a3320f1cda69..0b4fe533e1a1 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -149,7 +149,6 @@ struct perf_sched {
 	unsigned long	 nr_runs;
 	unsigned long	 nr_timestamps;
 	unsigned long	 nr_unordered_timestamps;
-	unsigned long	 nr_state_machine_bugs;
 	unsigned long	 nr_context_switch_bugs;
 	unsigned long	 nr_events;
 	unsigned long	 nr_lost_chunks;
@@ -1032,12 +1031,18 @@ static int latency_wakeup_event(struct perf_sched *sched,
 	atom = list_entry(atoms->work_list.prev, struct work_atom, list);
 
 	/*
+	 * As we do not guarantee the wakeup event happens when
+	 * task is out of run queue, also may happen when task is
+	 * on run queue and wakeup only change ->state to TASK_RUNNING,
+	 * then we should not set the ->wake_up_time when wake up a
+	 * task which is on run queue.
+	 *
 	 * You WILL be missing events if you've recorded only
 	 * one CPU, or are only looking at only one, so don't
-	 * make useless noise.
+	 * skip in this case.
 	 */
 	if (sched->profile_cpu == -1 && atom->state != THREAD_SLEEPING)
-		sched->nr_state_machine_bugs++;
+		return 0;
 
 	sched->nr_timestamps++;
 	if (atom->sched_out_time > timestamp) {
@@ -1496,14 +1501,6 @@ static void print_bad_events(struct perf_sched *sched)
 			(double)sched->nr_lost_events/(double)sched->nr_events * 100.0,
 			sched->nr_lost_events, sched->nr_events, sched->nr_lost_chunks);
 	}
-	if (sched->nr_state_machine_bugs && sched->nr_timestamps) {
-		printf("  INFO: %.3f%% state machine bugs (%ld out of %ld)",
-			(double)sched->nr_state_machine_bugs/(double)sched->nr_timestamps*100.0,
-			sched->nr_state_machine_bugs, sched->nr_timestamps);
-		if (sched->nr_lost_events)
-			printf(" (due to lost events?)");
-		printf("\n");
-	}
 	if (sched->nr_context_switch_bugs && sched->nr_timestamps) {
 		printf("  INFO: %.3f%% context switch bugs (%ld out of %ld)",
 			(double)sched->nr_context_switch_bugs/(double)sched->nr_timestamps*100.0,

commit 0680ee7db16de9c02d1d4b1a935a5daf754fe8a1
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon May 12 20:19:46 2014 +0200

    perf tools: Remove usage of trace_sched_wakeup(.success)
    
    trace_sched_wakeup(.success) is a dead argument and has been for ages,
    the only reason its still there is because of brain dead software, which
    apparently includes perf tools
    
    There's a few more instances in pearly snake shit, but that's not
    supported as far as I care anyhow, so let that bitrot.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20140512181946.GG13467@laptop.programming.kicks-ass.net
    Signed-off-by: Jiri Olsa <jolsa@kernel.org>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 2579215f5743..a3320f1cda69 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1007,17 +1007,12 @@ static int latency_wakeup_event(struct perf_sched *sched,
 				struct perf_sample *sample,
 				struct machine *machine)
 {
-	const u32 pid	  = perf_evsel__intval(evsel, sample, "pid"),
-		  success = perf_evsel__intval(evsel, sample, "success");
+	const u32 pid	  = perf_evsel__intval(evsel, sample, "pid");
 	struct work_atoms *atoms;
 	struct work_atom *atom;
 	struct thread *wakee;
 	u64 timestamp = sample->time;
 
-	/* Note for later, it may be interesting to observe the failing cases */
-	if (!success)
-		return 0;
-
 	wakee = machine__findnew_thread(machine, 0, pid);
 	atoms = thread_atoms_search(&sched->atom_root, wakee, &sched->cmp_pid);
 	if (!atoms) {

commit 6bcab4e1eaa1f669d003051ef3b87a963d8763bb
Author: Dongsheng <yangds.fnst@cn.fujitsu.com>
Date:   Tue May 6 14:39:01 2014 +0900

    perf tools: Clarify the output of perf sched map.
    
    In output of perf sched map, any shortname of thread will be explained
    at the first time when it appear.
    
    Example:
                  *A0       228836.978985 secs A0 => perf:23032
              *.   A0       228836.979016 secs B0 => swapper:0
               .  *C0       228836.979099 secs C0 => migration/3:22
      *A0      .   C0       228836.979115 secs
       A0      .  *.        228836.979115 secs
    
    But B0, which is explained as swapper:0 did not appear in the
    left part of output. Instead, we use '.' as the shortname of
    swapper:0. So the comment of "B0 => swapper:0" is not easy to
    understand.
    
    This patch clarify the output of perf sched map with not allocating
    one letter-number shortname for swapper:0 and print ". => swapper:0"
    as the explanation for swapper:0.
    
    Example:
                  *A0       228836.978985 secs A0 => perf:23032
              * .  A0       228836.979016 secs .  => swapper:0
                . *B0       228836.979099 secs B0 => migration/3:22
      *A0       .  B0       228836.979115 secs
       A0       . * .       228836.979115 secs
       A0     *C0   .       228836.979225 secs C0 => ksoftirqd/2:18
       A0     *D0   .       228836.979236 secs D0 => rcu_sched:7
    
    Signed-off-by: Dongsheng <yangds.fnst@cn.fujitsu.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Link: http://lkml.kernel.org/r/1399354741-19522-1-git-send-email-yangds.fnst@cn.fujitsu.com
    [ small style fixes to make checkpatch happy ]
    Signed-off-by: Jiri Olsa <jolsa@kernel.org>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 4f0dd21b5923..2579215f5743 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1300,17 +1300,25 @@ static int map_switch_event(struct perf_sched *sched, struct perf_evsel *evsel,
 
 	new_shortname = 0;
 	if (!sched_in->shortname[0]) {
-		sched_in->shortname[0] = sched->next_shortname1;
-		sched_in->shortname[1] = sched->next_shortname2;
-
-		if (sched->next_shortname1 < 'Z') {
-			sched->next_shortname1++;
+		if (!strcmp(thread__comm_str(sched_in), "swapper")) {
+			/*
+			 * Don't allocate a letter-number for swapper:0
+			 * as a shortname. Instead, we use '.' for it.
+			 */
+			sched_in->shortname[0] = '.';
+			sched_in->shortname[1] = ' ';
 		} else {
-			sched->next_shortname1='A';
-			if (sched->next_shortname2 < '9') {
-				sched->next_shortname2++;
+			sched_in->shortname[0] = sched->next_shortname1;
+			sched_in->shortname[1] = sched->next_shortname2;
+
+			if (sched->next_shortname1 < 'Z') {
+				sched->next_shortname1++;
 			} else {
-				sched->next_shortname2='0';
+				sched->next_shortname1 = 'A';
+				if (sched->next_shortname2 < '9')
+					sched->next_shortname2++;
+				else
+					sched->next_shortname2 = '0';
 			}
 		}
 		new_shortname = 1;
@@ -1322,12 +1330,9 @@ static int map_switch_event(struct perf_sched *sched, struct perf_evsel *evsel,
 		else
 			printf("*");
 
-		if (sched->curr_thread[cpu]) {
-			if (sched->curr_thread[cpu]->tid)
-				printf("%2s ", sched->curr_thread[cpu]->shortname);
-			else
-				printf(".  ");
-		} else
+		if (sched->curr_thread[cpu])
+			printf("%2s ", sched->curr_thread[cpu]->shortname);
+		else
 			printf("   ");
 	}
 

commit e936e8e459e14af7432a775f8139a79b71e41afc
Author: Dongsheng <yangds.fnst@cn.fujitsu.com>
Date:   Mon May 5 16:05:54 2014 +0900

    perf tools: Adapt the TASK_STATE_TO_CHAR_STR to new value in kernel space.
    
    Currently, TASK_STATE_TO_CHAR_STR in kernel space is already expanded to RSDTtZXxKWP,
    but it is still RSDTtZX in perf sched tool.
    
    This patch update TASK_STATE_TO_CHAR_STR to the new value in kernel space.
    
    Signed-off-by: Dongsheng <yangds.fnst@cn.fujitsu.com>
    Link: http://lkml.kernel.org/r/6d2f55dc1e02c1e29a5d70bfeb9d6e8863caf2aa.1399273302.git.yangds.fnst@cn.fujitsu.com
    Signed-off-by: Jiri Olsa <jolsa@kernel.org>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 7eae5011b690..4f0dd21b5923 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -66,7 +66,7 @@ struct sched_atom {
 	struct task_desc	*wakee;
 };
 
-#define TASK_STATE_TO_CHAR_STR "RSDTtZX"
+#define TASK_STATE_TO_CHAR_STR "RSDTtZXxKWP"
 
 enum thread_state {
 	THREAD_SLEEPING = 0,

commit 7fff959783949b2f50454c49e325697073f48dc0
Author: Dongsheng <yangds.fnst@cn.fujitsu.com>
Date:   Mon May 5 16:05:53 2014 +0900

    perf tools: Add missing event for perf sched record.
    
    We should record and process sched:sched_wakeup_new event in
    perf sched tool, but currently, there is the process function
    for it, without recording it in record subcommand.
    
    This patch add -e sched:sched_wakeup_new to perf sched record.
    
    Signed-off-by: Dongsheng <yangds.fnst@cn.fujitsu.com>
    Link: http://lkml.kernel.org/r/710c6edd2162b2cea1711443f54de47c0210d9fd.1399273302.git.yangds.fnst@cn.fujitsu.com
    Signed-off-by: Jiri Olsa <jolsa@kernel.org>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index d3fb0ed7240a..7eae5011b690 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1635,6 +1635,7 @@ static int __cmd_record(int argc, const char **argv)
 		"-e", "sched:sched_stat_runtime",
 		"-e", "sched:sched_process_fork",
 		"-e", "sched:sched_wakeup",
+		"-e", "sched:sched_wakeup_new",
 		"-e", "sched:sched_migrate_task",
 	};
 

commit a83edb2dfc5989fbadc594109c933bae528a2809
Author: Ramkumar Ramachandra <artagnon@gmail.com>
Date:   Fri Mar 14 23:17:54 2014 -0400

    perf sched: Introduce --list-cmds for use by scripts
    
    Signed-off-by: Ramkumar Ramachandra <artagnon@gmail.com>
    Acked-by: David Ahern <dsahern@gmail.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Link: http://lkml.kernel.org/r/1394853474-31019-5-git-send-email-artagnon@gmail.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Signed-off-by: Jiri Olsa <jolsa@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 9ac0a495c954..d3fb0ed7240a 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1713,8 +1713,10 @@ int cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)
 		"perf sched replay [<options>]",
 		NULL
 	};
-	const char * const sched_usage[] = {
-		"perf sched [<options>] {record|latency|map|replay|script}",
+	const char *const sched_subcommands[] = { "record", "latency", "map",
+						  "replay", "script", NULL };
+	const char *sched_usage[] = {
+		NULL,
 		NULL
 	};
 	struct trace_sched_handler lat_ops  = {
@@ -1736,8 +1738,8 @@ int cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)
 	for (i = 0; i < ARRAY_SIZE(sched.curr_pid); i++)
 		sched.curr_pid[i] = -1;
 
-	argc = parse_options(argc, argv, sched_options, sched_usage,
-			     PARSE_OPT_STOP_AT_NON_OPTION);
+	argc = parse_options_subcommand(argc, argv, sched_options, sched_subcommands,
+					sched_usage, PARSE_OPT_STOP_AT_NON_OPTION);
 	if (!argc)
 		usage_with_options(sched_usage, sched_options);
 

commit 80790e0b7ef768b6591fdf764b62c572b76a5d80
Author: Ramkumar Ramachandra <artagnon@gmail.com>
Date:   Mon Mar 17 10:18:21 2014 -0400

    perf sched: Fixup header alignment in 'latency' output
    
    Before:
    
     ---------------------------------------------------------------------------------------------------------------
      Task                  |   Runtime ms  | Switches | Average delay ms | Maximum delay ms | Maximum delay at     |
     ---------------------------------------------------------------------------------------------------------------
      ...                   |               |          |                  |                  |
      git:24540             |    336.622 ms |       10 | avg:    0.032 ms | max:    0.062 ms | max at: 115610.111046 s
      git:24541             |      0.457 ms |        1 | avg:    0.000 ms | max:    0.000 ms | max at:  0.000000 s
     -----------------------------------------------------------------------------------------
      TOTAL:                |    396.542 ms |      353 |
     ---------------------------------------------------
    
    After:
    
     -----------------------------------------------------------------------------------------------------------------
      Task                  |   Runtime ms  | Switches | Average delay ms | Maximum delay ms | Maximum delay at       |
     -----------------------------------------------------------------------------------------------------------------
      ...                   |               |          |                  |                  |
      git:24540             |    336.622 ms |       10 | avg:    0.032 ms | max:    0.062 ms | max at: 115610.111046 s
      git:24541             |      0.457 ms |        1 | avg:    0.000 ms | max:    0.000 ms | max at:      0.000000 s
     -----------------------------------------------------------------------------------------------------------------
      TOTAL:                |    396.542 ms |      353 |
     ---------------------------------------------------
    
    Signed-off-by: Ramkumar Ramachandra <artagnon@gmail.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Link: http://lkml.kernel.org/r/1395065901-25740-1-git-send-email-artagnon@gmail.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 6a76a07b6789..9ac0a495c954 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1124,7 +1124,7 @@ static void output_lat_thread(struct perf_sched *sched, struct work_atoms *work_
 
 	avg = work_list->total_lat / work_list->nb_atoms;
 
-	printf("|%11.3f ms |%9" PRIu64 " | avg:%9.3f ms | max:%9.3f ms | max at: %9.6f s\n",
+	printf("|%11.3f ms |%9" PRIu64 " | avg:%9.3f ms | max:%9.3f ms | max at: %13.6f s\n",
 	      (double)work_list->total_runtime / 1e6,
 		 work_list->nb_atoms, (double)avg / 1e6,
 		 (double)work_list->max_lat / 1e6,
@@ -1527,9 +1527,9 @@ static int perf_sched__lat(struct perf_sched *sched)
 
 	perf_sched__sort_lat(sched);
 
-	printf("\n ---------------------------------------------------------------------------------------------------------------\n");
-	printf("  Task                  |   Runtime ms  | Switches | Average delay ms | Maximum delay ms | Maximum delay at     |\n");
-	printf(" ---------------------------------------------------------------------------------------------------------------\n");
+	printf("\n -----------------------------------------------------------------------------------------------------------------\n");
+	printf("  Task                  |   Runtime ms  | Switches | Average delay ms | Maximum delay ms | Maximum delay at       |\n");
+	printf(" -----------------------------------------------------------------------------------------------------------------\n");
 
 	next = rb_first(&sched->sorted_atom_root);
 
@@ -1541,7 +1541,7 @@ static int perf_sched__lat(struct perf_sched *sched)
 		next = rb_next(next);
 	}
 
-	printf(" -----------------------------------------------------------------------------------------\n");
+	printf(" -----------------------------------------------------------------------------------------------------------------\n");
 	printf("  TOTAL:                |%11.3f ms |%9" PRIu64 " |\n",
 		(double)sched->all_runtime / 1e6, sched->all_count);
 

commit 74cf249d5cf7de84c88cca69a2f13b13d500ff94
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Fri Dec 27 16:55:14 2013 -0300

    perf tools: Use zfree to help detect use after free bugs
    
    Several areas already used this technique, so do some audit to
    consistently use it elsewhere.
    
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Link: http://lkml.kernel.org/n/tip-9sbere0kkplwe45ak6rk4a1f@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 0f3c65518a2c..6a76a07b6789 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -469,7 +469,7 @@ static void *thread_func(void *ctx)
 	char comm2[22];
 	int fd;
 
-	free(parms);
+	zfree(&parms);
 
 	sprintf(comm2, ":%s", this_task->comm);
 	prctl(PR_SET_NAME, comm2);

commit 744a971940520cf0818e1fe882b64892c528e6de
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Wed Nov 6 10:17:38 2013 -0300

    perf evsel: Ditch evsel->handler.data field
    
    Not needed since this cset:
    
      fcf65bf149af: perf evsel: Cache associated event_format
    
    So lets trim this struct a bit.
    
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Link: http://lkml.kernel.org/n/tip-j8setslokt0goiwxq9dogzqm@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index a81ab1828aa5..0f3c65518a2c 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1427,8 +1427,8 @@ static int perf_sched__process_tracepoint_sample(struct perf_tool *tool __maybe_
 	evsel->hists.stats.total_period += sample->period;
 	hists__inc_nr_events(&evsel->hists, PERF_RECORD_SAMPLE);
 
-	if (evsel->handler.func != NULL) {
-		tracepoint_handler f = evsel->handler.func;
+	if (evsel->handler != NULL) {
+		tracepoint_handler f = evsel->handler;
 		err = f(tool, evsel, sample, machine);
 	}
 

commit b9c5143a012a543c4ee872498d6dbae5c10beb2e
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Sep 11 14:46:56 2013 +0200

    perf tools: Use an accessor to read thread comm
    
    As the thread comm is going to be implemented by way of a more
    complicated data structure than just a pointer to a string from the
    thread struct, convert the readers of comm to use an accessor instead of
    accessing it directly.
    
    The accessor will be later overriden to support an enhanced comm
    implementation.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Tested-by: Jiri Olsa <jolsa@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Stephane Eranian <eranian@google.com>
    Link: http://lkml.kernel.org/n/tip-wr683zwy94hmj4ibogmnv9ce@git.kernel.org
    [ Rename thread__comm_curr() to thread__comm_str() ]
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    [ Fixed up some minor const pointer issues ]
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index ddb5dc15be17..a81ab1828aa5 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -737,12 +737,12 @@ static int replay_fork_event(struct perf_sched *sched,
 
 	if (verbose) {
 		printf("fork event\n");
-		printf("... parent: %s/%d\n", parent->comm, parent->tid);
-		printf("...  child: %s/%d\n", child->comm, child->tid);
+		printf("... parent: %s/%d\n", thread__comm_str(parent), parent->tid);
+		printf("...  child: %s/%d\n", thread__comm_str(child), child->tid);
 	}
 
-	register_pid(sched, parent->tid, parent->comm);
-	register_pid(sched, child->tid, child->comm);
+	register_pid(sched, parent->tid, thread__comm_str(parent));
+	register_pid(sched, child->tid, thread__comm_str(child));
 	return 0;
 }
 
@@ -1077,7 +1077,7 @@ static int latency_migrate_task_event(struct perf_sched *sched,
 	if (!atoms) {
 		if (thread_atoms_insert(sched, migrant))
 			return -1;
-		register_pid(sched, migrant->tid, migrant->comm);
+		register_pid(sched, migrant->tid, thread__comm_str(migrant));
 		atoms = thread_atoms_search(&sched->atom_root, migrant, &sched->cmp_pid);
 		if (!atoms) {
 			pr_err("migration-event: Internal tree error");
@@ -1111,13 +1111,13 @@ static void output_lat_thread(struct perf_sched *sched, struct work_atoms *work_
 	/*
 	 * Ignore idle threads:
 	 */
-	if (!strcmp(work_list->thread->comm, "swapper"))
+	if (!strcmp(thread__comm_str(work_list->thread), "swapper"))
 		return;
 
 	sched->all_runtime += work_list->total_runtime;
 	sched->all_count   += work_list->nb_atoms;
 
-	ret = printf("  %s:%d ", work_list->thread->comm, work_list->thread->tid);
+	ret = printf("  %s:%d ", thread__comm_str(work_list->thread), work_list->thread->tid);
 
 	for (i = 0; i < 24 - ret; i++)
 		printf(" ");
@@ -1334,7 +1334,7 @@ static int map_switch_event(struct perf_sched *sched, struct perf_evsel *evsel,
 	printf("  %12.6f secs ", (double)timestamp/1e9);
 	if (new_shortname) {
 		printf("%s => %s:%d\n",
-			sched_in->shortname, sched_in->comm, sched_in->tid);
+		       sched_in->shortname, thread__comm_str(sched_in), sched_in->tid);
 	} else {
 		printf("\n");
 	}

commit 156a2b022907687f28c72d1ba601015f295cd99e
Author: Adrian Hunter <adrian.hunter@intel.com>
Date:   Tue Oct 22 10:34:16 2013 +0300

    perf sched: Optimize build time
    
    builtin-sched.c took a log time to build with -O6 optimization. This
    turned out to be caused by:
    
            .curr_pid = { [0 ... MAX_CPUS - 1] = -1 },
    
    Fix by initializing curr_pid programmatically.
    
    This addresses the problem cured in f36f83f947ed using a smaller hammer.
    
    Signed-off-by: Adrian Hunter <adrian.hunter@intel.com>
    Acked-by: David Ahern <dsahern@gmail.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Namhyung Kim <namhyung@gmail.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Stephane Eranian <eranian@google.com>
    Link: http://lkml.kernel.org/r/1382427258-17495-13-git-send-email-adrian.hunter@intel.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 5a338566195e..ddb5dc15be17 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1670,7 +1670,6 @@ int cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)
 		.sort_list	      = LIST_HEAD_INIT(sched.sort_list),
 		.start_work_mutex     = PTHREAD_MUTEX_INITIALIZER,
 		.work_done_wait_mutex = PTHREAD_MUTEX_INITIALIZER,
-		.curr_pid	      = { [0 ... MAX_CPUS - 1] = -1 },
 		.sort_order	      = default_sort_order,
 		.replay_repeat	      = 10,
 		.profile_cpu	      = -1,
@@ -1732,6 +1731,10 @@ int cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)
 		.switch_event	    = replay_switch_event,
 		.fork_event	    = replay_fork_event,
 	};
+	unsigned int i;
+
+	for (i = 0; i < ARRAY_SIZE(sched.curr_pid); i++)
+		sched.curr_pid[i] = -1;
 
 	argc = parse_options(argc, argv, sched_options, sched_usage,
 			     PARSE_OPT_STOP_AT_NON_OPTION);

commit 8a39df8faa1cb130f136d5e404332c16fbb936c0
Author: Adrian Hunter <adrian.hunter@intel.com>
Date:   Tue Oct 22 10:34:15 2013 +0300

    perf sched: Make struct perf_sched sched a local variable
    
    Change "struct perf_sched sched" from being global to being local.
    
    The build slowdown cured by f36f83f947ed is dealt with in the following
    patch, by programatically setting perf_sched.curr_pid.
    
    Signed-off-by: Adrian Hunter <adrian.hunter@intel.com>
    Acked-by: David Ahern <dsahern@gmail.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Namhyung Kim <namhyung@gmail.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Stephane Eranian <eranian@google.com>
    Link: http://lkml.kernel.org/r/1382427258-17495-12-git-send-email-adrian.hunter@intel.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 5a46b102eb08..5a338566195e 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1655,29 +1655,28 @@ static int __cmd_record(int argc, const char **argv)
 	return cmd_record(i, rec_argv, NULL);
 }
 
-static const char default_sort_order[] = "avg, max, switch, runtime";
-static struct perf_sched sched = {
-	.tool = {
-		.sample		 = perf_sched__process_tracepoint_sample,
-		.comm		 = perf_event__process_comm,
-		.lost		 = perf_event__process_lost,
-		.fork		 = perf_sched__process_fork_event,
-		.ordered_samples = true,
-	},
-	.cmp_pid	      = LIST_HEAD_INIT(sched.cmp_pid),
-	.sort_list	      = LIST_HEAD_INIT(sched.sort_list),
-	.start_work_mutex     = PTHREAD_MUTEX_INITIALIZER,
-	.work_done_wait_mutex = PTHREAD_MUTEX_INITIALIZER,
-	.curr_pid	      = { [0 ... MAX_CPUS - 1] = -1 },
-	.sort_order	      = default_sort_order,
-	.replay_repeat	      = 10,
-	.profile_cpu	      = -1,
-	.next_shortname1      = 'A',
-	.next_shortname2      = '0',
-};
-
 int cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)
 {
+	const char default_sort_order[] = "avg, max, switch, runtime";
+	struct perf_sched sched = {
+		.tool = {
+			.sample		 = perf_sched__process_tracepoint_sample,
+			.comm		 = perf_event__process_comm,
+			.lost		 = perf_event__process_lost,
+			.fork		 = perf_sched__process_fork_event,
+			.ordered_samples = true,
+		},
+		.cmp_pid	      = LIST_HEAD_INIT(sched.cmp_pid),
+		.sort_list	      = LIST_HEAD_INIT(sched.sort_list),
+		.start_work_mutex     = PTHREAD_MUTEX_INITIALIZER,
+		.work_done_wait_mutex = PTHREAD_MUTEX_INITIALIZER,
+		.curr_pid	      = { [0 ... MAX_CPUS - 1] = -1 },
+		.sort_order	      = default_sort_order,
+		.replay_repeat	      = 10,
+		.profile_cpu	      = -1,
+		.next_shortname1      = 'A',
+		.next_shortname2      = '0',
+	};
 	const struct option latency_options[] = {
 	OPT_STRING('s', "sort", &sched.sort_order, "key[,key2...]",
 		   "sort by key(s): runtime, switch, avg, max"),

commit f5fc14124c5cefdd052a2b2a6a3f0ed531540113
Author: Jiri Olsa <jolsa@redhat.com>
Date:   Tue Oct 15 16:27:32 2013 +0200

    perf tools: Add data object to handle perf data file
    
    This patch is adding 'struct perf_data_file' object as a placeholder for
    all attributes regarding perf.data file handling. Changing
    perf_session__new to take it as an argument.
    
    The rest of the functionality will be added later to keep this change
    simple enough, because all the places using perf_session are changed
    now.
    
    Signed-off-by: Jiri Olsa <jolsa@redhat.com>
    Acked-by: Namhyung Kim <namhyung@kernel.org>
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Corey Ashford <cjashfor@linux.vnet.ibm.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1381847254-28809-2-git-send-email-jolsa@redhat.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index d8c51b2f263f..5a46b102eb08 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1446,8 +1446,12 @@ static int perf_sched__read_events(struct perf_sched *sched,
 		{ "sched:sched_migrate_task", process_sched_migrate_task_event, },
 	};
 	struct perf_session *session;
+	struct perf_data_file file = {
+		.path = input_name,
+		.mode = PERF_DATA_MODE_READ,
+	};
 
-	session = perf_session__new(input_name, O_RDONLY, 0, false, &sched->tool);
+	session = perf_session__new(&file, false, &sched->tool);
 	if (session == NULL) {
 		pr_debug("No Memory for session\n");
 		return -1;

commit 314add6b1f045b59ca39683bd0cbc5310cd203f2
Author: Adrian Hunter <adrian.hunter@intel.com>
Date:   Tue Aug 27 11:23:03 2013 +0300

    perf tools: change machine__findnew_thread() to set thread pid
    
    Add a new parameter for 'pid' to machine__findnew_thread().
    Change callers to pass 'pid' when it is known.
    
    Note that callers sometimes want to find the main thread
    which has the memory maps.  The main thread has tid == pid
    so the usage in that case is:
    
            machine__findnew_thread(machine, pid, pid)
    
    whereas the usage to find the specific thread is:
    
            machine__findnew_thread(machine, pid, tid)
    
    Signed-off-by: Adrian Hunter <adrian.hunter@intel.com>
    Acked-by: David Ahern <dsahern@gmail.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Namhyung Kim <namhyung@gmail.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Link: http://lkml.kernel.org/r/1377591794-30553-2-git-send-email-adrian.hunter@intel.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index f809cc7fb7d9..d8c51b2f263f 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -724,8 +724,10 @@ static int replay_fork_event(struct perf_sched *sched,
 {
 	struct thread *child, *parent;
 
-	child = machine__findnew_thread(machine, event->fork.tid);
-	parent = machine__findnew_thread(machine, event->fork.ptid);
+	child = machine__findnew_thread(machine, event->fork.pid,
+					event->fork.tid);
+	parent = machine__findnew_thread(machine, event->fork.ppid,
+					 event->fork.ptid);
 
 	if (child == NULL || parent == NULL) {
 		pr_debug("thread does not exist on fork event: child %p, parent %p\n",
@@ -934,8 +936,8 @@ static int latency_switch_event(struct perf_sched *sched,
 		return -1;
 	}
 
-	sched_out = machine__findnew_thread(machine, prev_pid);
-	sched_in = machine__findnew_thread(machine, next_pid);
+	sched_out = machine__findnew_thread(machine, 0, prev_pid);
+	sched_in = machine__findnew_thread(machine, 0, next_pid);
 
 	out_events = thread_atoms_search(&sched->atom_root, sched_out, &sched->cmp_pid);
 	if (!out_events) {
@@ -978,7 +980,7 @@ static int latency_runtime_event(struct perf_sched *sched,
 {
 	const u32 pid	   = perf_evsel__intval(evsel, sample, "pid");
 	const u64 runtime  = perf_evsel__intval(evsel, sample, "runtime");
-	struct thread *thread = machine__findnew_thread(machine, pid);
+	struct thread *thread = machine__findnew_thread(machine, 0, pid);
 	struct work_atoms *atoms = thread_atoms_search(&sched->atom_root, thread, &sched->cmp_pid);
 	u64 timestamp = sample->time;
 	int cpu = sample->cpu;
@@ -1016,7 +1018,7 @@ static int latency_wakeup_event(struct perf_sched *sched,
 	if (!success)
 		return 0;
 
-	wakee = machine__findnew_thread(machine, pid);
+	wakee = machine__findnew_thread(machine, 0, pid);
 	atoms = thread_atoms_search(&sched->atom_root, wakee, &sched->cmp_pid);
 	if (!atoms) {
 		if (thread_atoms_insert(sched, wakee))
@@ -1070,7 +1072,7 @@ static int latency_migrate_task_event(struct perf_sched *sched,
 	if (sched->profile_cpu == -1)
 		return 0;
 
-	migrant = machine__findnew_thread(machine, pid);
+	migrant = machine__findnew_thread(machine, 0, pid);
 	atoms = thread_atoms_search(&sched->atom_root, migrant, &sched->cmp_pid);
 	if (!atoms) {
 		if (thread_atoms_insert(sched, migrant))
@@ -1289,8 +1291,8 @@ static int map_switch_event(struct perf_sched *sched, struct perf_evsel *evsel,
 		return -1;
 	}
 
-	sched_out = machine__findnew_thread(machine, prev_pid);
-	sched_in = machine__findnew_thread(machine, next_pid);
+	sched_out = machine__findnew_thread(machine, 0, prev_pid);
+	sched_in = machine__findnew_thread(machine, 0, next_pid);
 
 	sched->curr_thread[this_cpu] = sched_in;
 

commit cb627505ae028a0cd88cc29ed72a4c168a08751d
Author: David Ahern <dsahern@gmail.com>
Date:   Wed Aug 7 22:50:47 2013 -0400

    perf sched: Remove sched_process_fork tracepoint
    
    The PERF_RECORD_FORK event is already collected as part of the use of
    cmd_record and those events are analyzed as part of the libperf
    machinery.  Using the fork tracepoint as well just duplicates the event
    load.
    
    Signed-off-by: David Ahern <dsahern@gmail.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Link: http://lkml.kernel.org/r/1375930261-77273-6-git-send-email-dsahern@gmail.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 42f45873d8f0..f809cc7fb7d9 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -109,8 +109,9 @@ struct trace_sched_handler {
 	int (*wakeup_event)(struct perf_sched *sched, struct perf_evsel *evsel,
 			    struct perf_sample *sample, struct machine *machine);
 
-	int (*fork_event)(struct perf_sched *sched, struct perf_evsel *evsel,
-			  struct perf_sample *sample);
+	/* PERF_RECORD_FORK event, not sched_process_fork tracepoint */
+	int (*fork_event)(struct perf_sched *sched, union perf_event *event,
+			  struct machine *machine);
 
 	int (*migrate_task_event)(struct perf_sched *sched,
 				  struct perf_evsel *evsel,
@@ -717,22 +718,29 @@ static int replay_switch_event(struct perf_sched *sched,
 	return 0;
 }
 
-static int replay_fork_event(struct perf_sched *sched, struct perf_evsel *evsel,
-			     struct perf_sample *sample)
+static int replay_fork_event(struct perf_sched *sched,
+			     union perf_event *event,
+			     struct machine *machine)
 {
-	const char *parent_comm = perf_evsel__strval(evsel, sample, "parent_comm"),
-		   *child_comm  = perf_evsel__strval(evsel, sample, "child_comm");
-	const u32 parent_pid  = perf_evsel__intval(evsel, sample, "parent_pid"),
-		  child_pid  = perf_evsel__intval(evsel, sample, "child_pid");
+	struct thread *child, *parent;
+
+	child = machine__findnew_thread(machine, event->fork.tid);
+	parent = machine__findnew_thread(machine, event->fork.ptid);
+
+	if (child == NULL || parent == NULL) {
+		pr_debug("thread does not exist on fork event: child %p, parent %p\n",
+				 child, parent);
+		return 0;
+	}
 
 	if (verbose) {
-		printf("sched_fork event %p\n", evsel);
-		printf("... parent: %s/%d\n", parent_comm, parent_pid);
-		printf("...  child: %s/%d\n", child_comm, child_pid);
+		printf("fork event\n");
+		printf("... parent: %s/%d\n", parent->comm, parent->tid);
+		printf("...  child: %s/%d\n", child->comm, child->tid);
 	}
 
-	register_pid(sched, parent_pid, parent_comm);
-	register_pid(sched, child_pid, child_comm);
+	register_pid(sched, parent->tid, parent->comm);
+	register_pid(sched, child->tid, child->comm);
 	return 0;
 }
 
@@ -824,14 +832,6 @@ static int thread_atoms_insert(struct perf_sched *sched, struct thread *thread)
 	return 0;
 }
 
-static int latency_fork_event(struct perf_sched *sched __maybe_unused,
-			      struct perf_evsel *evsel __maybe_unused,
-			      struct perf_sample *sample __maybe_unused)
-{
-	/* should insert the newcomer */
-	return 0;
-}
-
 static char sched_out_state(u64 prev_state)
 {
 	const char *str = TASK_STATE_TO_CHAR_STR;
@@ -1379,15 +1379,19 @@ static int process_sched_runtime_event(struct perf_tool *tool,
 	return 0;
 }
 
-static int process_sched_fork_event(struct perf_tool *tool,
-				    struct perf_evsel *evsel,
-				    struct perf_sample *sample,
-				    struct machine *machine __maybe_unused)
+static int perf_sched__process_fork_event(struct perf_tool *tool,
+					  union perf_event *event,
+					  struct perf_sample *sample,
+					  struct machine *machine)
 {
 	struct perf_sched *sched = container_of(tool, struct perf_sched, tool);
 
+	/* run the fork event through the perf machineruy */
+	perf_event__process_fork(tool, event, sample, machine);
+
+	/* and then run additional processing needed for this command */
 	if (sched->tp_handler->fork_event)
-		return sched->tp_handler->fork_event(sched, evsel, sample);
+		return sched->tp_handler->fork_event(sched, event, machine);
 
 	return 0;
 }
@@ -1437,7 +1441,6 @@ static int perf_sched__read_events(struct perf_sched *sched,
 		{ "sched:sched_stat_runtime", process_sched_runtime_event, },
 		{ "sched:sched_wakeup",	      process_sched_wakeup_event, },
 		{ "sched:sched_wakeup_new",   process_sched_wakeup_event, },
-		{ "sched:sched_process_fork", process_sched_fork_event, },
 		{ "sched:sched_migrate_task", process_sched_migrate_task_event, },
 	};
 	struct perf_session *session;
@@ -1652,7 +1655,7 @@ static struct perf_sched sched = {
 		.sample		 = perf_sched__process_tracepoint_sample,
 		.comm		 = perf_event__process_comm,
 		.lost		 = perf_event__process_lost,
-		.fork		 = perf_event__process_fork,
+		.fork		 = perf_sched__process_fork_event,
 		.ordered_samples = true,
 	},
 	.cmp_pid	      = LIST_HEAD_INIT(sched.cmp_pid),
@@ -1714,7 +1717,6 @@ int cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)
 		.wakeup_event	    = latency_wakeup_event,
 		.switch_event	    = latency_switch_event,
 		.runtime_event	    = latency_runtime_event,
-		.fork_event	    = latency_fork_event,
 		.migrate_task_event = latency_migrate_task_event,
 	};
 	struct trace_sched_handler map_ops  = {

commit 4a957e4df1a212c447fd162d18dc7ee6320c1621
Author: David Ahern <dsahern@gmail.com>
Date:   Wed Aug 7 22:50:46 2013 -0400

    perf sched: Remove sched_process_exit tracepoint
    
    Event is not needed nor analyzed. Since perf-sched leverages perf-record
    to capture the sched data, we already capture task events like EXIT.
    
    Signed-off-by: David Ahern <dsahern@gmail.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Link: http://lkml.kernel.org/r/1375930261-77273-5-git-send-email-dsahern@gmail.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 52850240583a..42f45873d8f0 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1392,15 +1392,6 @@ static int process_sched_fork_event(struct perf_tool *tool,
 	return 0;
 }
 
-static int process_sched_exit_event(struct perf_tool *tool __maybe_unused,
-				    struct perf_evsel *evsel,
-				    struct perf_sample *sample __maybe_unused,
-				    struct machine *machine __maybe_unused)
-{
-	pr_debug("sched_exit event %p\n", evsel);
-	return 0;
-}
-
 static int process_sched_migrate_task_event(struct perf_tool *tool,
 					    struct perf_evsel *evsel,
 					    struct perf_sample *sample,
@@ -1447,7 +1438,6 @@ static int perf_sched__read_events(struct perf_sched *sched,
 		{ "sched:sched_wakeup",	      process_sched_wakeup_event, },
 		{ "sched:sched_wakeup_new",   process_sched_wakeup_event, },
 		{ "sched:sched_process_fork", process_sched_fork_event, },
-		{ "sched:sched_process_exit", process_sched_exit_event, },
 		{ "sched:sched_migrate_task", process_sched_migrate_task_event, },
 	};
 	struct perf_session *session;
@@ -1634,7 +1624,6 @@ static int __cmd_record(int argc, const char **argv)
 		"-e", "sched:sched_stat_sleep",
 		"-e", "sched:sched_stat_iowait",
 		"-e", "sched:sched_stat_runtime",
-		"-e", "sched:sched_process_exit",
 		"-e", "sched:sched_process_fork",
 		"-e", "sched:sched_wakeup",
 		"-e", "sched:sched_migrate_task",

commit ffb273dd7e3bd72e7d964fc0a0f6d441aceb7dae
Author: David Ahern <dsahern@gmail.com>
Date:   Wed Aug 7 22:50:45 2013 -0400

    perf sched: Remove thread lookup in sample handler
    
    Not used in the function, so no sense in doing the lookup here. Thread
    look up will be done in the timehist command, and no sense in doing it
    twice.
    
    Signed-off-by: David Ahern <dsahern@gmail.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Link: http://lkml.kernel.org/r/1375930261-77273-4-git-send-email-dsahern@gmail.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 34ce57d24721..52850240583a 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1425,15 +1425,8 @@ static int perf_sched__process_tracepoint_sample(struct perf_tool *tool __maybe_
 						 struct perf_evsel *evsel,
 						 struct machine *machine)
 {
-	struct thread *thread = machine__findnew_thread(machine, sample->tid);
 	int err = 0;
 
-	if (thread == NULL) {
-		pr_debug("problem processing %s event, skipping it.\n",
-			 perf_evsel__name(evsel));
-		return -1;
-	}
-
 	evsel->hists.stats.total_period += sample->period;
 	hists__inc_nr_events(&evsel->hists, PERF_RECORD_SAMPLE);
 

commit ad9def7ca020ef5b54968c89194f52d18ef1ef49
Author: David Ahern <dsahern@gmail.com>
Date:   Wed Aug 7 22:50:44 2013 -0400

    perf sched: Simplify arguments to read_events
    
    Destroy argument is not necessary. If session is not returned to caller,
    then clean it up.
    
    Signed-off-by: David Ahern <dsahern@gmail.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Link: http://lkml.kernel.org/r/1375930261-77273-3-git-send-email-dsahern@gmail.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 948183adb6e5..34ce57d24721 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1445,7 +1445,7 @@ static int perf_sched__process_tracepoint_sample(struct perf_tool *tool __maybe_
 	return err;
 }
 
-static int perf_sched__read_events(struct perf_sched *sched, bool destroy,
+static int perf_sched__read_events(struct perf_sched *sched,
 				   struct perf_session **psession)
 {
 	const struct perf_evsel_str_handler handlers[] = {
@@ -1480,11 +1480,10 @@ static int perf_sched__read_events(struct perf_sched *sched, bool destroy,
 		sched->nr_lost_chunks = session->stats.nr_events[PERF_RECORD_LOST];
 	}
 
-	if (destroy)
-		perf_session__delete(session);
-
 	if (psession)
 		*psession = session;
+	else
+		perf_session__delete(session);
 
 	return 0;
 
@@ -1529,8 +1528,11 @@ static int perf_sched__lat(struct perf_sched *sched)
 	struct perf_session *session;
 
 	setup_pager();
-	if (perf_sched__read_events(sched, false, &session))
+
+	/* save session -- references to threads are held in work_list */
+	if (perf_sched__read_events(sched, &session))
 		return -1;
+
 	perf_sched__sort_lat(sched);
 
 	printf("\n ---------------------------------------------------------------------------------------------------------------\n");
@@ -1565,7 +1567,7 @@ static int perf_sched__map(struct perf_sched *sched)
 	sched->max_cpu = sysconf(_SC_NPROCESSORS_CONF);
 
 	setup_pager();
-	if (perf_sched__read_events(sched, true, NULL))
+	if (perf_sched__read_events(sched, NULL))
 		return -1;
 	print_bad_events(sched);
 	return 0;
@@ -1580,7 +1582,7 @@ static int perf_sched__replay(struct perf_sched *sched)
 
 	test_calibrations(sched);
 
-	if (perf_sched__read_events(sched, true, NULL))
+	if (perf_sched__read_events(sched, NULL))
 		return -1;
 
 	printf("nr_run_events:        %ld\n", sched->nr_run_events);

commit 380512345e13c3af64e59627f1b993c4faa94a84
Author: Adrian Hunter <adrian.hunter@intel.com>
Date:   Thu Jul 4 16:20:31 2013 +0300

    perf tools: struct thread has a tid not a pid
    
    As evident from 'machine__process_fork_event()' and
    'machine__process_exit_event()' the 'pid' member of struct thread is
    actually the tid.
    
    Rename 'pid' to 'tid' in struct thread accordingly.
    
    Signed-off-by: Adrian Hunter <adrian.hunter@intel.com>
    Acked-by: David Ahern <dsahern@gmail.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Namhyung Kim <namhyung@gmail.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Link: http://lkml.kernel.org/r/1372944040-32690-13-git-send-email-adrian.hunter@intel.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index fba4a940ba31..948183adb6e5 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1075,7 +1075,7 @@ static int latency_migrate_task_event(struct perf_sched *sched,
 	if (!atoms) {
 		if (thread_atoms_insert(sched, migrant))
 			return -1;
-		register_pid(sched, migrant->pid, migrant->comm);
+		register_pid(sched, migrant->tid, migrant->comm);
 		atoms = thread_atoms_search(&sched->atom_root, migrant, &sched->cmp_pid);
 		if (!atoms) {
 			pr_err("migration-event: Internal tree error");
@@ -1115,7 +1115,7 @@ static void output_lat_thread(struct perf_sched *sched, struct work_atoms *work_
 	sched->all_runtime += work_list->total_runtime;
 	sched->all_count   += work_list->nb_atoms;
 
-	ret = printf("  %s:%d ", work_list->thread->comm, work_list->thread->pid);
+	ret = printf("  %s:%d ", work_list->thread->comm, work_list->thread->tid);
 
 	for (i = 0; i < 24 - ret; i++)
 		printf(" ");
@@ -1131,9 +1131,9 @@ static void output_lat_thread(struct perf_sched *sched, struct work_atoms *work_
 
 static int pid_cmp(struct work_atoms *l, struct work_atoms *r)
 {
-	if (l->thread->pid < r->thread->pid)
+	if (l->thread->tid < r->thread->tid)
 		return -1;
-	if (l->thread->pid > r->thread->pid)
+	if (l->thread->tid > r->thread->tid)
 		return 1;
 
 	return 0;
@@ -1321,7 +1321,7 @@ static int map_switch_event(struct perf_sched *sched, struct perf_evsel *evsel,
 			printf("*");
 
 		if (sched->curr_thread[cpu]) {
-			if (sched->curr_thread[cpu]->pid)
+			if (sched->curr_thread[cpu]->tid)
 				printf("%2s ", sched->curr_thread[cpu]->shortname);
 			else
 				printf(".  ");
@@ -1332,7 +1332,7 @@ static int map_switch_event(struct perf_sched *sched, struct perf_evsel *evsel,
 	printf("  %12.6f secs ", (double)timestamp/1e9);
 	if (new_shortname) {
 		printf("%s => %s:%d\n",
-			sched_in->shortname, sched_in->comm, sched_in->pid);
+			sched_in->shortname, sched_in->comm, sched_in->tid);
 	} else {
 		printf("\n");
 	}

commit f36f83f947ede547833e462696893f866df77324
Author: Namhyung Kim <namhyung.kim@lge.com>
Date:   Tue Jun 4 14:46:19 2013 +0900

    perf sched: Move struct perf_sched definition out of cmd_sched()
    
    For some reason it consumed quite amount of compile time when declared
    as local variable, and it disappeared when moved out of the function.
    Moving other variables/tables didn't help.
    
    On my system this single-file-change build time reduced from 11s to 3s.
    
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Stephane Eranian <eranian@google.com>
    Link: http://lkml.kernel.org/r/1370324779-16921-1-git-send-email-namhyung@kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index fed9ae432c16..fba4a940ba31 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1662,28 +1662,29 @@ static int __cmd_record(int argc, const char **argv)
 	return cmd_record(i, rec_argv, NULL);
 }
 
+static const char default_sort_order[] = "avg, max, switch, runtime";
+static struct perf_sched sched = {
+	.tool = {
+		.sample		 = perf_sched__process_tracepoint_sample,
+		.comm		 = perf_event__process_comm,
+		.lost		 = perf_event__process_lost,
+		.fork		 = perf_event__process_fork,
+		.ordered_samples = true,
+	},
+	.cmp_pid	      = LIST_HEAD_INIT(sched.cmp_pid),
+	.sort_list	      = LIST_HEAD_INIT(sched.sort_list),
+	.start_work_mutex     = PTHREAD_MUTEX_INITIALIZER,
+	.work_done_wait_mutex = PTHREAD_MUTEX_INITIALIZER,
+	.curr_pid	      = { [0 ... MAX_CPUS - 1] = -1 },
+	.sort_order	      = default_sort_order,
+	.replay_repeat	      = 10,
+	.profile_cpu	      = -1,
+	.next_shortname1      = 'A',
+	.next_shortname2      = '0',
+};
+
 int cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)
 {
-	const char default_sort_order[] = "avg, max, switch, runtime";
-	struct perf_sched sched = {
-		.tool = {
-			.sample		 = perf_sched__process_tracepoint_sample,
-			.comm		 = perf_event__process_comm,
-			.lost		 = perf_event__process_lost,
-			.fork		 = perf_event__process_fork,
-			.ordered_samples = true,
-		},
-		.cmp_pid	      = LIST_HEAD_INIT(sched.cmp_pid),
-		.sort_list	      = LIST_HEAD_INIT(sched.sort_list),
-		.start_work_mutex     = PTHREAD_MUTEX_INITIALIZER,
-		.work_done_wait_mutex = PTHREAD_MUTEX_INITIALIZER,
-		.curr_pid	      = { [0 ... MAX_CPUS - 1] = -1 },
-		.sort_order	      = default_sort_order,
-		.replay_repeat	      = 10,
-		.profile_cpu	      = -1,
-		.next_shortname1      = 'A',
-		.next_shortname2      = '0',
-	};
 	const struct option latency_options[] = {
 	OPT_STRING('s', "sort", &sched.sort_order, "key[,key2...]",
 		   "sort by key(s): runtime, switch, avg, max"),

commit 4a4d371a4dfbd3b84a7eab8d535d4c7c3647b09e
Author: Jiri Olsa <jolsa@redhat.com>
Date:   Wed Jun 5 13:37:21 2013 +0200

    perf record: Remove -f/--force option
    
    It no longer have any affect on the processing and is marked as obsolete
    anyway.
    
    Signed-off-by: Jiri Olsa <jolsa@redhat.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Corey Ashford <cjashfor@linux.vnet.ibm.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/n/tip-tvwyspiqr4getzfib2lw06ty@git.kernel.org
    Link: http://lkml.kernel.org/r/1372307120-737-1-git-send-email-namhyung@kernel.org
    [ combined patch removing the -f usage in various sub-commands, such as 'perf sched', etc, by Namhyung Kim ]
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 2da2a6ca22bf..fed9ae432c16 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1632,7 +1632,6 @@ static int __cmd_record(int argc, const char **argv)
 		"record",
 		"-a",
 		"-R",
-		"-f",
 		"-m", "1024",
 		"-c", "1",
 		"-e", "sched:sched_switch",

commit 1c6763cb9962d6d83a8470934c20ed525ef62355
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Wed Mar 27 10:34:49 2013 -0300

    Revert "perf sched: Handle PERF_RECORD_EXIT events"
    
    This reverts commit 0439539f72ea222fbfe511b47318b9c1815a7108.
    
    This caused this segfault:
    
    [root@sandy linux]# perf sched rec
    ^C[ perf record: Woken up 1 times to write data ]
    [ perf record: Captured and wrote 1.306 MB perf.data (~57062 samples) ]
    perf
    [root@sandy linux]# perf sched lat
    perf: builtin-sched.c:781: thread_atoms_search: Assertion `!(thread != atoms->thread)' failed.
    Aborted (core dumped)
    [root@sandy linux]#
    
    Further investigation is needed to check that even with machine__remove_thread()
    not really deleting the thread referenced in the PERF_RECORD_EXIT (it goes to
    machine->dead_threads, because references may still exist to them in things like
     hist, etc) some event later comes for this dead thread and then
    machine__findnew_thread() will create a new thead instance that will not be the
    same as the one referenced by work_atoms->thread in thread_atoms_search().
    
    For now just revert this patch to get the 'perf sched lat' back working.
    
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Namhyung Kim <namhyung@gmail.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    echo Link: http://lkml.kernel.org/n/tip-`ranpwd -l 24`@git.kernel.org
    Link: http://lkml.kernel.org/n/tip-hg4s6e5txiwqe00h8rdg1sin@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 138229439a93..2da2a6ca22bf 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1671,7 +1671,6 @@ int cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)
 			.sample		 = perf_sched__process_tracepoint_sample,
 			.comm		 = perf_event__process_comm,
 			.lost		 = perf_event__process_lost,
-			.exit		 = perf_event__process_exit,
 			.fork		 = perf_event__process_fork,
 			.ordered_samples = true,
 		},

commit 28a6b6aa54878a6a239e901698b3fc111bbcc54f
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Tue Dec 18 16:24:46 2012 -0300

    perf session: There is no need for a per session hists instance
    
    It was being used just for its stats member, so ditch session->hists and
    use just what is needed, session->stats.
    
    This completes the move support multiple events in the hists layer, the
    last user of session->hists was 'perf diff' but Jiri Olsa has fixed that
    some time ago.
    
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Namhyung Kim <namhyung@gmail.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Link: http://lkml.kernel.org/n/tip-pimk92kek8kcp4dmb1jakoro@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index cc28b85dabd5..138229439a93 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1475,9 +1475,9 @@ static int perf_sched__read_events(struct perf_sched *sched, bool destroy,
 			goto out_delete;
 		}
 
-		sched->nr_events      = session->hists.stats.nr_events[0];
-		sched->nr_lost_events = session->hists.stats.total_lost;
-		sched->nr_lost_chunks = session->hists.stats.nr_events[PERF_RECORD_LOST];
+		sched->nr_events      = session->stats.nr_events[0];
+		sched->nr_lost_events = session->stats.total_lost;
+		sched->nr_lost_chunks = session->stats.nr_events[PERF_RECORD_LOST];
 	}
 
 	if (destroy)

commit 70cb4e963f77dae90ae2aa3dd9385a43737c469f
Author: Feng Tang <feng.tang@intel.com>
Date:   Tue Oct 30 11:56:02 2012 +0800

    perf tools: Add a global variable "const char *input_name"
    
    Currently many perf commands annotate/evlist/report/script/lock etc all
    support "-i" option to chose a specific perf data, and all of them
    create a local "input_name" to save the file name for that perf data.
    
    Since most of these commands need it, we can add a global variable for
    it, also it can some other benefits:
    
    1. When calling script browser inside hists/annotation browser, it needs
    to know the perf data file name to run that script.
    
    2. For further feature like runtime switching to another perf data file,
    this variable can also help.
    
    Signed-off-by: Feng Tang <feng.tang@intel.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1351569369-26732-2-git-send-email-feng.tang@intel.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 30e53360d3c2..cc28b85dabd5 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -120,7 +120,6 @@ struct trace_sched_handler {
 
 struct perf_sched {
 	struct perf_tool tool;
-	const char	 *input_name;
 	const char	 *sort_order;
 	unsigned long	 nr_tasks;
 	struct task_desc *pid_to_task[MAX_PID];
@@ -1460,7 +1459,7 @@ static int perf_sched__read_events(struct perf_sched *sched, bool destroy,
 	};
 	struct perf_session *session;
 
-	session = perf_session__new(sched->input_name, O_RDONLY, 0, false, &sched->tool);
+	session = perf_session__new(input_name, O_RDONLY, 0, false, &sched->tool);
 	if (session == NULL) {
 		pr_debug("No Memory for session\n");
 		return -1;
@@ -1708,7 +1707,7 @@ int cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)
 	OPT_END()
 	};
 	const struct option sched_options[] = {
-	OPT_STRING('i', "input", &sched.input_name, "file",
+	OPT_STRING('i', "input", &input_name, "file",
 		    "input file name"),
 	OPT_INCR('v', "verbose", &verbose,
 		    "be more verbose (show symbol address, etc)"),

commit 0439539f72ea222fbfe511b47318b9c1815a7108
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Sat Oct 6 15:53:41 2012 -0300

    perf sched: Handle PERF_RECORD_EXIT events
    
    Noticed sched wasn't handling those events while introducing
    perf_event__process_{fork,exit}.
    
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Namhyung Kim <namhyung@gmail.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Link: http://lkml.kernel.org/n/tip-035rzjtnv9ri8sssi7ojjjq0@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 3a25cd83b56e..30e53360d3c2 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1672,6 +1672,7 @@ int cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)
 			.sample		 = perf_sched__process_tracepoint_sample,
 			.comm		 = perf_event__process_comm,
 			.lost		 = perf_event__process_lost,
+			.exit		 = perf_event__process_exit,
 			.fork		 = perf_event__process_fork,
 			.ordered_samples = true,
 		},

commit f62d3f0f4596f983ec00495d91c8ddb30268d878
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Sat Oct 6 15:44:59 2012 -0300

    perf event: No need to create a thread when handling PERF_RECORD_EXIT
    
    When we were processing a PERF_RECORD_EXIT event we first used
    machine__findnew_thread for both the thread exiting and for its parent,
    only to use just the thread struct associated with the one exiting, and
    to just delete it.
    
    If it existed, i.e. not created at this very moment in
    machine__findnew_thread, it will be moved to the machine->dead_threads
    linked list, because we may have hist_entries pointing to it, but if it
    was created just do be deleted, it will just sit there with no
    references at all.
    
    Use the new machine__find_thread() method so that if it is not there, we
    don't create it.
    
    As a bonus the parent thread will also not be created at this point.
    
    Create process_fork() and process_exit() helpers to use this and make
    the builtins use it instead of the generic process_task(), ditched by
    this patch.
    
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Namhyung Kim <namhyung@gmail.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Link: http://lkml.kernel.org/n/tip-z7n2y98ebjyrvmytaope4vdl@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 3488ead3b60c..3a25cd83b56e 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1672,7 +1672,7 @@ int cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)
 			.sample		 = perf_sched__process_tracepoint_sample,
 			.comm		 = perf_event__process_comm,
 			.lost		 = perf_event__process_lost,
-			.fork		 = perf_event__process_task,
+			.fork		 = perf_event__process_fork,
 			.ordered_samples = true,
 		},
 		.cmp_pid	      = LIST_HEAD_INIT(sched.cmp_pid),

commit 73ee3b276864af1ca042258e67237e8454d7b7b6
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Mon Oct 1 13:58:17 2012 -0300

    perf sched: Look up thread using tid instead of pid
    
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Namhyung Kim <namhyung@gmail.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Link: http://lkml.kernel.org/n/tip-zdu8up6vahogckg2uft7wh3n@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 9b9e32eaa805..3488ead3b60c 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1426,7 +1426,7 @@ static int perf_sched__process_tracepoint_sample(struct perf_tool *tool __maybe_
 						 struct perf_evsel *evsel,
 						 struct machine *machine)
 {
-	struct thread *thread = machine__findnew_thread(machine, sample->pid);
+	struct thread *thread = machine__findnew_thread(machine, sample->tid);
 	int err = 0;
 
 	if (thread == NULL) {

commit 60b7d14af46ef07778e89556429ec9ab5a7fad0b
Author: Namhyung Kim <namhyung.kim@lge.com>
Date:   Wed Sep 12 11:11:06 2012 +0900

    perf sched: Fixup for the die() removal
    
    The commit a116e05dcf61 ("perf sched: Remove die() calls") replaced
    die() call to pr_debug + return -1, but it should be pr_err otherwise
    it'll not show up unless -v option is given.  Fix it.
    
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1347415866-303-2-git-send-email-namhyung@kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index af305f57bd22..9b9e32eaa805 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -438,8 +438,8 @@ static int self_open_counters(void)
 	fd = sys_perf_event_open(&attr, 0, -1, -1, 0);
 
 	if (fd < 0)
-		pr_debug("Error: sys_perf_event_open() syscall returned"
-			 "with %d (%s)\n", fd, strerror(errno));
+		pr_err("Error: sys_perf_event_open() syscall returned "
+		       "with %d (%s)\n", fd, strerror(errno));
 	return fd;
 }
 
@@ -700,7 +700,7 @@ static int replay_switch_event(struct perf_sched *sched,
 		delta = 0;
 
 	if (delta < 0) {
-		pr_debug("hm, delta: %" PRIu64 " < 0 ?\n", delta);
+		pr_err("hm, delta: %" PRIu64 " < 0 ?\n", delta);
 		return -1;
 	}
 
@@ -990,7 +990,7 @@ static int latency_runtime_event(struct perf_sched *sched,
 			return -1;
 		atoms = thread_atoms_search(&sched->atom_root, thread, &sched->cmp_pid);
 		if (!atoms) {
-			pr_debug("in-event: Internal tree error");
+			pr_err("in-event: Internal tree error");
 			return -1;
 		}
 		if (add_sched_out_event(atoms, 'R', timestamp))
@@ -1024,7 +1024,7 @@ static int latency_wakeup_event(struct perf_sched *sched,
 			return -1;
 		atoms = thread_atoms_search(&sched->atom_root, wakee, &sched->cmp_pid);
 		if (!atoms) {
-			pr_debug("wakeup-event: Internal tree error");
+			pr_err("wakeup-event: Internal tree error");
 			return -1;
 		}
 		if (add_sched_out_event(atoms, 'S', timestamp))
@@ -1079,7 +1079,7 @@ static int latency_migrate_task_event(struct perf_sched *sched,
 		register_pid(sched, migrant->pid, migrant->comm);
 		atoms = thread_atoms_search(&sched->atom_root, migrant, &sched->cmp_pid);
 		if (!atoms) {
-			pr_debug("migration-event: Internal tree error");
+			pr_err("migration-event: Internal tree error");
 			return -1;
 		}
 		if (add_sched_out_event(atoms, 'R', timestamp))
@@ -1286,7 +1286,7 @@ static int map_switch_event(struct perf_sched *sched, struct perf_evsel *evsel,
 		delta = 0;
 
 	if (delta < 0) {
-		pr_debug("hm, delta: %" PRIu64 " < 0 ?\n", delta);
+		pr_err("hm, delta: %" PRIu64 " < 0 ?\n", delta);
 		return -1;
 	}
 

commit 9ec3f4e437ede2f3b5087d412abe16a0219b3b99
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Tue Sep 11 19:29:17 2012 -0300

    perf sched: Don't read all tracepoint variables in advance
    
    Do it just at the actual consumer of these fields, that way we avoid
    needless lookups:
    
      [root@sandy ~]# perf sched record sleep 30s
      [ perf record: Woken up 1 times to write data ]
      [ perf record: Captured and wrote 8.585 MB perf.data (~375063 samples) ]
    
    Before:
    
      [root@sandy ~]# perf stat -r 10 perf sched lat > /dev/null
    
       Performance counter stats for 'perf sched lat' (10 runs):
    
              103.592215 task-clock                #    0.993 CPUs utilized            ( +-  0.33% )
                      12 context-switches          #    0.114 K/sec                    ( +-  3.29% )
                       0 cpu-migrations            #    0.000 K/sec
                   7,605 page-faults               #    0.073 M/sec                    ( +-  0.00% )
             345,796,112 cycles                    #    3.338 GHz                      ( +-  0.07% ) [82.90%]
             106,876,796 stalled-cycles-frontend   #   30.91% frontend cycles idle     ( +-  0.38% ) [83.23%]
              62,060,877 stalled-cycles-backend    #   17.95% backend  cycles idle     ( +-  0.80% ) [67.14%]
             628,246,586 instructions              #    1.82  insns per cycle
                                                   #    0.17  stalled cycles per insn  ( +-  0.04% ) [83.64%]
             134,962,057 branches                  # 1302.820 M/sec                    ( +-  0.10% ) [83.64%]
               1,233,037 branch-misses             #    0.91% of all branches          ( +-  0.29% ) [83.41%]
    
             0.104333272 seconds time elapsed                                          ( +-  0.33% )
    
      [root@sandy ~]# perf stat -r 10 perf sched lat > /dev/null
    
       Performance counter stats for 'perf sched lat' (10 runs):
    
             98.848272 task-clock                #    0.993 CPUs utilized            ( +-  0.48% )
                    11 context-switches          #    0.112 K/sec                    ( +-  2.83% )
                     0 cpu-migrations            #    0.003 K/sec                    ( +- 50.92% )
                 7,604 page-faults               #    0.077 M/sec                    ( +-  0.00% )
           332,216,085 cycles                    #    3.361 GHz                      ( +-  0.14% ) [82.87%]
           100,623,710 stalled-cycles-frontend   #   30.29% frontend cycles idle     ( +-  0.53% ) [82.95%]
            58,788,692 stalled-cycles-backend    #   17.70% backend  cycles idle     ( +-  0.59% ) [67.15%]
           609,402,433 instructions              #    1.83  insns per cycle
                                                 #    0.17  stalled cycles per insn  ( +-  0.04% ) [83.76%]
           131,277,138 branches                  # 1328.067 M/sec                    ( +-  0.06% ) [83.77%]
             1,117,871 branch-misses             #    0.85% of all branches          ( +-  0.32% ) [83.51%]
    
           0.099580430 seconds time elapsed                                          ( +-  0.48% )
    
      [root@sandy ~]#
    
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Namhyung Kim <namhyung@gmail.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Link: http://lkml.kernel.org/n/tip-kracdpw8wqlr0xjh75uk8g11@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 0df5e7a08c63..af305f57bd22 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -97,73 +97,25 @@ struct work_atoms {
 
 typedef int (*sort_fn_t)(struct work_atoms *, struct work_atoms *);
 
-struct trace_switch_event {
-	char *prev_comm;
-	u32  prev_pid;
-	u32  prev_prio;
-	u64  prev_state;
-	char *next_comm;
-	u32  next_pid;
-	u32  next_prio;
-};
-
-struct trace_runtime_event {
-	char *comm;
-	u32  pid;
-	u64  runtime;
-	u64  vruntime;
-};
+struct perf_sched;
 
-struct trace_wakeup_event {
-	char *comm;
-	u32  pid;
-	u32  prio;
-	u32  success;
-	u32  cpu;
-};
+struct trace_sched_handler {
+	int (*switch_event)(struct perf_sched *sched, struct perf_evsel *evsel,
+			    struct perf_sample *sample, struct machine *machine);
 
-struct trace_fork_event {
-	char *parent_comm;
-	u32  parent_pid;
-	char *child_comm;
-	u32   child_pid;
-};
+	int (*runtime_event)(struct perf_sched *sched, struct perf_evsel *evsel,
+			     struct perf_sample *sample, struct machine *machine);
 
-struct trace_migrate_task_event {
-	char *comm;
-	u32  pid;
-	u32  prio;
-	u32  cpu;
-};
+	int (*wakeup_event)(struct perf_sched *sched, struct perf_evsel *evsel,
+			    struct perf_sample *sample, struct machine *machine);
 
-struct perf_sched;
-
-struct trace_sched_handler {
-	int (*switch_event)(struct perf_sched *sched,
-			    struct trace_switch_event *event,
-			    struct machine *machine,
-			    struct perf_evsel *evsel,
-			    struct perf_sample *sample);
-
-	int (*runtime_event)(struct perf_sched *sched,
-			     struct trace_runtime_event *event,
-			     struct machine *machine,
-			     struct perf_sample *sample);
-
-	int (*wakeup_event)(struct perf_sched *sched,
-			    struct trace_wakeup_event *event,
-			    struct machine *machine,
-			    struct perf_evsel *evsel,
-			    struct perf_sample *sample);
-
-	int (*fork_event)(struct perf_sched *sched,
-			  struct trace_fork_event *event,
-			  struct perf_evsel *evsel);
+	int (*fork_event)(struct perf_sched *sched, struct perf_evsel *evsel,
+			  struct perf_sample *sample);
 
 	int (*migrate_task_event)(struct perf_sched *sched,
-				  struct trace_migrate_task_event *event,
-				  struct machine *machine,
-				  struct perf_sample *sample);
+				  struct perf_evsel *evsel,
+				  struct perf_sample *sample,
+				  struct machine *machine);
 };
 
 struct perf_sched {
@@ -700,33 +652,36 @@ static void test_calibrations(struct perf_sched *sched)
 
 static int
 replay_wakeup_event(struct perf_sched *sched,
-		    struct trace_wakeup_event *wakeup_event,
-		    struct machine *machine __maybe_unused,
-		    struct perf_evsel *evsel, struct perf_sample *sample)
+		    struct perf_evsel *evsel, struct perf_sample *sample,
+		    struct machine *machine __maybe_unused)
 {
+	const char *comm = perf_evsel__strval(evsel, sample, "comm");
+	const u32 pid	 = perf_evsel__intval(evsel, sample, "pid");
 	struct task_desc *waker, *wakee;
 
 	if (verbose) {
 		printf("sched_wakeup event %p\n", evsel);
 
-		printf(" ... pid %d woke up %s/%d\n",
-		       sample->tid, wakeup_event->comm, wakeup_event->pid);
+		printf(" ... pid %d woke up %s/%d\n", sample->tid, comm, pid);
 	}
 
 	waker = register_pid(sched, sample->tid, "<unknown>");
-	wakee = register_pid(sched, wakeup_event->pid, wakeup_event->comm);
+	wakee = register_pid(sched, pid, comm);
 
 	add_sched_event_wakeup(sched, waker, sample->time, wakee);
 	return 0;
 }
 
-static int
-replay_switch_event(struct perf_sched *sched,
-		    struct trace_switch_event *switch_event,
-		    struct machine *machine __maybe_unused,
-		    struct perf_evsel *evsel,
-		    struct perf_sample *sample)
+static int replay_switch_event(struct perf_sched *sched,
+			       struct perf_evsel *evsel,
+			       struct perf_sample *sample,
+			       struct machine *machine __maybe_unused)
 {
+	const char *prev_comm  = perf_evsel__strval(evsel, sample, "prev_comm"),
+		   *next_comm  = perf_evsel__strval(evsel, sample, "next_comm");
+	const u32 prev_pid = perf_evsel__intval(evsel, sample, "prev_pid"),
+		  next_pid = perf_evsel__intval(evsel, sample, "next_pid");
+	const u64 prev_state = perf_evsel__intval(evsel, sample, "prev_state");
 	struct task_desc *prev, __maybe_unused *next;
 	u64 timestamp0, timestamp = sample->time;
 	int cpu = sample->cpu;
@@ -749,35 +704,36 @@ replay_switch_event(struct perf_sched *sched,
 		return -1;
 	}
 
-	if (verbose) {
-		printf(" ... switch from %s/%d to %s/%d [ran %" PRIu64 " nsecs]\n",
-			switch_event->prev_comm, switch_event->prev_pid,
-			switch_event->next_comm, switch_event->next_pid,
-			delta);
-	}
+	pr_debug(" ... switch from %s/%d to %s/%d [ran %" PRIu64 " nsecs]\n",
+		 prev_comm, prev_pid, next_comm, next_pid, delta);
 
-	prev = register_pid(sched, switch_event->prev_pid, switch_event->prev_comm);
-	next = register_pid(sched, switch_event->next_pid, switch_event->next_comm);
+	prev = register_pid(sched, prev_pid, prev_comm);
+	next = register_pid(sched, next_pid, next_comm);
 
 	sched->cpu_last_switched[cpu] = timestamp;
 
 	add_sched_event_run(sched, prev, timestamp, delta);
-	add_sched_event_sleep(sched, prev, timestamp, switch_event->prev_state);
+	add_sched_event_sleep(sched, prev, timestamp, prev_state);
 
 	return 0;
 }
 
-static int
-replay_fork_event(struct perf_sched *sched, struct trace_fork_event *fork_event,
-		  struct perf_evsel *evsel)
+static int replay_fork_event(struct perf_sched *sched, struct perf_evsel *evsel,
+			     struct perf_sample *sample)
 {
+	const char *parent_comm = perf_evsel__strval(evsel, sample, "parent_comm"),
+		   *child_comm  = perf_evsel__strval(evsel, sample, "child_comm");
+	const u32 parent_pid  = perf_evsel__intval(evsel, sample, "parent_pid"),
+		  child_pid  = perf_evsel__intval(evsel, sample, "child_pid");
+
 	if (verbose) {
 		printf("sched_fork event %p\n", evsel);
-		printf("... parent: %s/%d\n", fork_event->parent_comm, fork_event->parent_pid);
-		printf("...  child: %s/%d\n", fork_event->child_comm, fork_event->child_pid);
+		printf("... parent: %s/%d\n", parent_comm, parent_pid);
+		printf("...  child: %s/%d\n", child_comm, child_pid);
 	}
-	register_pid(sched, fork_event->parent_pid, fork_event->parent_comm);
-	register_pid(sched, fork_event->child_pid, fork_event->child_comm);
+
+	register_pid(sched, parent_pid, parent_comm);
+	register_pid(sched, child_pid, child_comm);
 	return 0;
 }
 
@@ -870,18 +826,18 @@ static int thread_atoms_insert(struct perf_sched *sched, struct thread *thread)
 }
 
 static int latency_fork_event(struct perf_sched *sched __maybe_unused,
-			      struct trace_fork_event *fork_event __maybe_unused,
-			      struct perf_evsel *evsel __maybe_unused)
+			      struct perf_evsel *evsel __maybe_unused,
+			      struct perf_sample *sample __maybe_unused)
 {
 	/* should insert the newcomer */
 	return 0;
 }
 
-static char sched_out_state(struct trace_switch_event *switch_event)
+static char sched_out_state(u64 prev_state)
 {
 	const char *str = TASK_STATE_TO_CHAR_STR;
 
-	return str[switch_event->prev_state];
+	return str[prev_state];
 }
 
 static int
@@ -951,13 +907,14 @@ add_sched_in_event(struct work_atoms *atoms, u64 timestamp)
 	atoms->nb_atoms++;
 }
 
-static int
-latency_switch_event(struct perf_sched *sched,
-		     struct trace_switch_event *switch_event,
-		     struct machine *machine,
-		     struct perf_evsel *evsel __maybe_unused,
-		     struct perf_sample *sample)
+static int latency_switch_event(struct perf_sched *sched,
+				struct perf_evsel *evsel,
+				struct perf_sample *sample,
+				struct machine *machine)
 {
+	const u32 prev_pid = perf_evsel__intval(evsel, sample, "prev_pid"),
+		  next_pid = perf_evsel__intval(evsel, sample, "next_pid");
+	const u64 prev_state = perf_evsel__intval(evsel, sample, "prev_state");
 	struct work_atoms *out_events, *in_events;
 	struct thread *sched_out, *sched_in;
 	u64 timestamp0, timestamp = sample->time;
@@ -978,8 +935,8 @@ latency_switch_event(struct perf_sched *sched,
 		return -1;
 	}
 
-	sched_out = machine__findnew_thread(machine, switch_event->prev_pid);
-	sched_in = machine__findnew_thread(machine, switch_event->next_pid);
+	sched_out = machine__findnew_thread(machine, prev_pid);
+	sched_in = machine__findnew_thread(machine, next_pid);
 
 	out_events = thread_atoms_search(&sched->atom_root, sched_out, &sched->cmp_pid);
 	if (!out_events) {
@@ -991,7 +948,7 @@ latency_switch_event(struct perf_sched *sched,
 			return -1;
 		}
 	}
-	if (add_sched_out_event(out_events, sched_out_state(switch_event), timestamp))
+	if (add_sched_out_event(out_events, sched_out_state(prev_state), timestamp))
 		return -1;
 
 	in_events = thread_atoms_search(&sched->atom_root, sched_in, &sched->cmp_pid);
@@ -1015,12 +972,14 @@ latency_switch_event(struct perf_sched *sched,
 	return 0;
 }
 
-static int
-latency_runtime_event(struct perf_sched *sched,
-		      struct trace_runtime_event *runtime_event,
-		      struct machine *machine, struct perf_sample *sample)
+static int latency_runtime_event(struct perf_sched *sched,
+				 struct perf_evsel *evsel,
+				 struct perf_sample *sample,
+				 struct machine *machine)
 {
-	struct thread *thread = machine__findnew_thread(machine, runtime_event->pid);
+	const u32 pid	   = perf_evsel__intval(evsel, sample, "pid");
+	const u64 runtime  = perf_evsel__intval(evsel, sample, "runtime");
+	struct thread *thread = machine__findnew_thread(machine, pid);
 	struct work_atoms *atoms = thread_atoms_search(&sched->atom_root, thread, &sched->cmp_pid);
 	u64 timestamp = sample->time;
 	int cpu = sample->cpu;
@@ -1038,27 +997,27 @@ latency_runtime_event(struct perf_sched *sched,
 			return -1;
 	}
 
-	add_runtime_event(atoms, runtime_event->runtime, timestamp);
+	add_runtime_event(atoms, runtime, timestamp);
 	return 0;
 }
 
-static int
-latency_wakeup_event(struct perf_sched *sched,
-		     struct trace_wakeup_event *wakeup_event,
-		     struct machine *machine,
-		     struct perf_evsel *evsel __maybe_unused,
-		     struct perf_sample *sample)
+static int latency_wakeup_event(struct perf_sched *sched,
+				struct perf_evsel *evsel,
+				struct perf_sample *sample,
+				struct machine *machine)
 {
+	const u32 pid	  = perf_evsel__intval(evsel, sample, "pid"),
+		  success = perf_evsel__intval(evsel, sample, "success");
 	struct work_atoms *atoms;
 	struct work_atom *atom;
 	struct thread *wakee;
 	u64 timestamp = sample->time;
 
 	/* Note for later, it may be interesting to observe the failing cases */
-	if (!wakeup_event->success)
+	if (!success)
 		return 0;
 
-	wakee = machine__findnew_thread(machine, wakeup_event->pid);
+	wakee = machine__findnew_thread(machine, pid);
 	atoms = thread_atoms_search(&sched->atom_root, wakee, &sched->cmp_pid);
 	if (!atoms) {
 		if (thread_atoms_insert(sched, wakee))
@@ -1095,11 +1054,12 @@ latency_wakeup_event(struct perf_sched *sched,
 	return 0;
 }
 
-static int
-latency_migrate_task_event(struct perf_sched *sched,
-			   struct trace_migrate_task_event *migrate_task_event,
-			   struct machine *machine, struct perf_sample *sample)
+static int latency_migrate_task_event(struct perf_sched *sched,
+				      struct perf_evsel *evsel,
+				      struct perf_sample *sample,
+				      struct machine *machine)
 {
+	const u32 pid = perf_evsel__intval(evsel, sample, "pid");
 	u64 timestamp = sample->time;
 	struct work_atoms *atoms;
 	struct work_atom *atom;
@@ -1111,7 +1071,7 @@ latency_migrate_task_event(struct perf_sched *sched,
 	if (sched->profile_cpu == -1)
 		return 0;
 
-	migrant = machine__findnew_thread(machine, migrate_task_event->pid);
+	migrant = machine__findnew_thread(machine, pid);
 	atoms = thread_atoms_search(&sched->atom_root, migrant, &sched->cmp_pid);
 	if (!atoms) {
 		if (thread_atoms_insert(sched, migrant))
@@ -1296,28 +1256,17 @@ static int process_sched_wakeup_event(struct perf_tool *tool,
 {
 	struct perf_sched *sched = container_of(tool, struct perf_sched, tool);
 
-	if (sched->tp_handler->wakeup_event) {
-		struct trace_wakeup_event event = {
-			.comm	 = perf_evsel__strval(evsel, sample, "comm"),
-			.pid	 = perf_evsel__intval(evsel, sample, "pid"),
-			.prio	 = perf_evsel__intval(evsel, sample, "prio"),
-			.success = perf_evsel__intval(evsel, sample, "success"),
-			.cpu	 = perf_evsel__intval(evsel, sample, "cpu"),
-		};
-
-		return sched->tp_handler->wakeup_event(sched, &event, machine, evsel, sample);
-	}
+	if (sched->tp_handler->wakeup_event)
+		return sched->tp_handler->wakeup_event(sched, evsel, sample, machine);
 
 	return 0;
 }
 
-static int
-map_switch_event(struct perf_sched *sched,
-		 struct trace_switch_event *switch_event,
-		 struct machine *machine,
-		 struct perf_evsel *evsel __maybe_unused,
-		 struct perf_sample *sample)
+static int map_switch_event(struct perf_sched *sched, struct perf_evsel *evsel,
+			    struct perf_sample *sample, struct machine *machine)
 {
+	const u32 prev_pid = perf_evsel__intval(evsel, sample, "prev_pid"),
+		  next_pid = perf_evsel__intval(evsel, sample, "next_pid");
 	struct thread *sched_out __maybe_unused, *sched_in;
 	int new_shortname;
 	u64 timestamp0, timestamp = sample->time;
@@ -1341,8 +1290,8 @@ map_switch_event(struct perf_sched *sched,
 		return -1;
 	}
 
-	sched_out = machine__findnew_thread(machine, switch_event->prev_pid);
-	sched_in = machine__findnew_thread(machine, switch_event->next_pid);
+	sched_out = machine__findnew_thread(machine, prev_pid);
+	sched_in = machine__findnew_thread(machine, next_pid);
 
 	sched->curr_thread[this_cpu] = sched_in;
 
@@ -1411,19 +1360,8 @@ static int process_sched_switch_event(struct perf_tool *tool,
 			sched->nr_context_switch_bugs++;
 	}
 
-	if (sched->tp_handler->switch_event) {
-		struct trace_switch_event event = {
-			.prev_comm  = perf_evsel__strval(evsel, sample, "prev_comm"),
-			.prev_pid   = prev_pid,
-			.prev_prio  = perf_evsel__intval(evsel, sample, "prev_prio"),
-			.prev_state = perf_evsel__intval(evsel, sample, "prev_state"),
-			.next_comm  = perf_evsel__strval(evsel, sample, "next_comm"),
-			.next_pid   = next_pid,
-			.next_prio  = perf_evsel__intval(evsel, sample, "next_prio"),
-		};
-
-		err = sched->tp_handler->switch_event(sched, &event, machine, evsel, sample);
-	}
+	if (sched->tp_handler->switch_event)
+		err = sched->tp_handler->switch_event(sched, evsel, sample, machine);
 
 	sched->curr_pid[this_cpu] = next_pid;
 	return err;
@@ -1436,15 +1374,8 @@ static int process_sched_runtime_event(struct perf_tool *tool,
 {
 	struct perf_sched *sched = container_of(tool, struct perf_sched, tool);
 
-	if (sched->tp_handler->runtime_event) {
-		struct trace_runtime_event event = {
-			.comm	  = perf_evsel__strval(evsel, sample, "comm"),
-			.pid	  = perf_evsel__intval(evsel, sample, "pid"),
-			.runtime  = perf_evsel__intval(evsel, sample, "runtime"),
-			.vruntime = perf_evsel__intval(evsel, sample, "vruntime"),
-		};
-		return sched->tp_handler->runtime_event(sched, &event, machine, sample);
-	}
+	if (sched->tp_handler->runtime_event)
+		return sched->tp_handler->runtime_event(sched, evsel, sample, machine);
 
 	return 0;
 }
@@ -1456,15 +1387,8 @@ static int process_sched_fork_event(struct perf_tool *tool,
 {
 	struct perf_sched *sched = container_of(tool, struct perf_sched, tool);
 
-	if (sched->tp_handler->fork_event) {
-		struct trace_fork_event event = {
-			.parent_comm = perf_evsel__strval(evsel, sample, "parent_comm"),
-			.child_comm  = perf_evsel__strval(evsel, sample, "child_comm"),
-			.parent_pid  = perf_evsel__intval(evsel, sample, "parent_pid"),
-			.child_pid  = perf_evsel__intval(evsel, sample, "child_pid"),
-		};
-		return sched->tp_handler->fork_event(sched, &event, evsel);
-	}
+	if (sched->tp_handler->fork_event)
+		return sched->tp_handler->fork_event(sched, evsel, sample);
 
 	return 0;
 }
@@ -1485,15 +1409,8 @@ static int process_sched_migrate_task_event(struct perf_tool *tool,
 {
 	struct perf_sched *sched = container_of(tool, struct perf_sched, tool);
 
-	if (sched->tp_handler->migrate_task_event) {
-		struct trace_migrate_task_event event = {
-			.comm = perf_evsel__strval(evsel, sample, "comm"),
-			.pid  = perf_evsel__intval(evsel, sample, "pid"),
-			.prio = perf_evsel__intval(evsel, sample, "prio"),
-			.cpu  = perf_evsel__intval(evsel, sample, "cpu"),
-		};
-		return sched->tp_handler->migrate_task_event(sched, &event, machine, sample);
-	}
+	if (sched->tp_handler->migrate_task_event)
+		return sched->tp_handler->migrate_task_event(sched, evsel, sample, machine);
 
 	return 0;
 }

commit 2b7fcbc5a9c719a306af1c4986a9f5c2cbfcec65
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Tue Sep 11 19:29:17 2012 -0300

    perf sched: Use perf_evsel__{int,str}val
    
    This patch also stops reading the common fields, as they were not being used except
    for one ->common_pid case that was replaced by sample->tid, i.e. the info is already
    in the perf_sample struct.
    
    Also it only fills the _event structures when there is a handler.
    
      [root@sandy ~]# perf sched record sleep 30s
      [ perf record: Woken up 1 times to write data ]
      [ perf record: Captured and wrote 8.585 MB perf.data (~375063 samples) ]
    
    Before:
    
      [root@sandy ~]# perf stat -r 10 perf sched lat > /dev/null
    
       Performance counter stats for 'perf sched lat' (10 runs):
    
              129.117838 task-clock                #    0.994 CPUs utilized            ( +-  0.28% )
                      14 context-switches          #    0.111 K/sec                    ( +-  2.10% )
                       0 cpu-migrations            #    0.002 K/sec                    ( +- 66.67% )
                   7,654 page-faults               #    0.059 M/sec                    ( +-  0.67% )
             438,121,661 cycles                    #    3.393 GHz                      ( +-  0.06% ) [83.06%]
             150,808,605 stalled-cycles-frontend   #   34.42% frontend cycles idle     ( +-  0.14% ) [83.10%]
              80,748,941 stalled-cycles-backend    #   18.43% backend  cycles idle     ( +-  0.64% ) [66.73%]
             758,605,879 instructions              #    1.73  insns per cycle
                                                   #    0.20  stalled cycles per insn  ( +-  0.08% ) [83.54%]
             162,164,321 branches                  # 1255.940 M/sec                    ( +-  0.10% ) [83.70%]
               1,609,903 branch-misses             #    0.99% of all branches          ( +-  0.08% ) [83.62%]
    
             0.129949153 seconds time elapsed                                          ( +-  0.28% )
    
    After:
    
      [root@sandy ~]# perf stat -r 10 perf sched lat > /dev/null
    
       Performance counter stats for 'perf sched lat' (10 runs):
    
              103.592215 task-clock                #    0.993 CPUs utilized            ( +-  0.33% )
                      12 context-switches          #    0.114 K/sec                    ( +-  3.29% )
                       0 cpu-migrations            #    0.000 K/sec
                   7,605 page-faults               #    0.073 M/sec                    ( +-  0.00% )
             345,796,112 cycles                    #    3.338 GHz                      ( +-  0.07% ) [82.90%]
             106,876,796 stalled-cycles-frontend   #   30.91% frontend cycles idle     ( +-  0.38% ) [83.23%]
              62,060,877 stalled-cycles-backend    #   17.95% backend  cycles idle     ( +-  0.80% ) [67.14%]
             628,246,586 instructions              #    1.82  insns per cycle
                                                   #    0.17  stalled cycles per insn  ( +-  0.04% ) [83.64%]
             134,962,057 branches                  # 1302.820 M/sec                    ( +-  0.10% ) [83.64%]
               1,233,037 branch-misses             #    0.91% of all branches          ( +-  0.29% ) [83.41%]
    
             0.104333272 seconds time elapsed                                          ( +-  0.33% )
    
      [root@sandy ~]#
    
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Namhyung Kim <namhyung@gmail.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Link: http://lkml.kernel.org/n/tip-weu9t63zkrfrazkn0gxj48xy@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 79f88fa3f7a3..0df5e7a08c63 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -98,82 +98,40 @@ struct work_atoms {
 typedef int (*sort_fn_t)(struct work_atoms *, struct work_atoms *);
 
 struct trace_switch_event {
-	u32  size;
-
-	u16  common_type;
-	u8   common_flags;
-	u8   common_preempt_count;
-	u32  common_pid;
-	u32  common_tgid;
-
-	char prev_comm[16];
+	char *prev_comm;
 	u32  prev_pid;
 	u32  prev_prio;
 	u64  prev_state;
-	char next_comm[16];
+	char *next_comm;
 	u32  next_pid;
 	u32  next_prio;
 };
 
 struct trace_runtime_event {
-	u32  size;
-
-	u16  common_type;
-	u8   common_flags;
-	u8   common_preempt_count;
-	u32  common_pid;
-	u32  common_tgid;
-
-	char comm[16];
+	char *comm;
 	u32  pid;
 	u64  runtime;
 	u64  vruntime;
 };
 
 struct trace_wakeup_event {
-	u32  size;
-
-	u16  common_type;
-	u8   common_flags;
-	u8   common_preempt_count;
-	u32  common_pid;
-	u32  common_tgid;
-
-	char comm[16];
+	char *comm;
 	u32  pid;
-
 	u32  prio;
 	u32  success;
 	u32  cpu;
 };
 
 struct trace_fork_event {
-	u32  size;
-
-	u16  common_type;
-	u8   common_flags;
-	u8   common_preempt_count;
-	u32  common_pid;
-	u32  common_tgid;
-
-	char parent_comm[16];
+	char *parent_comm;
 	u32  parent_pid;
-	char child_comm[16];
+	char *child_comm;
 	u32   child_pid;
 };
 
 struct trace_migrate_task_event {
-	u32  size;
-
-	u16  common_type;
-	u8   common_flags;
-	u8   common_preempt_count;
-	u32  common_pid;
-	u32  common_tgid;
-
-	char comm[16];
+	char *comm;
 	u32  pid;
-
 	u32  prio;
 	u32  cpu;
 };
@@ -184,7 +142,7 @@ struct trace_sched_handler {
 	int (*switch_event)(struct perf_sched *sched,
 			    struct trace_switch_event *event,
 			    struct machine *machine,
-			    struct event_format *tp_format,
+			    struct perf_evsel *evsel,
 			    struct perf_sample *sample);
 
 	int (*runtime_event)(struct perf_sched *sched,
@@ -195,12 +153,12 @@ struct trace_sched_handler {
 	int (*wakeup_event)(struct perf_sched *sched,
 			    struct trace_wakeup_event *event,
 			    struct machine *machine,
-			    struct event_format *tp_format,
+			    struct perf_evsel *evsel,
 			    struct perf_sample *sample);
 
 	int (*fork_event)(struct perf_sched *sched,
 			  struct trace_fork_event *event,
-			  struct event_format *tp_format);
+			  struct perf_evsel *evsel);
 
 	int (*migrate_task_event)(struct perf_sched *sched,
 				  struct trace_migrate_task_event *event,
@@ -740,40 +698,22 @@ static void test_calibrations(struct perf_sched *sched)
 	printf("the sleep test took %" PRIu64 " nsecs\n", T1 - T0);
 }
 
-#define FILL_FIELD(ptr, field, event, data)	\
-	ptr.field = (typeof(ptr.field)) raw_field_value(event, #field, data)
-
-#define FILL_ARRAY(ptr, array, event, data)			\
-do {								\
-	void *__array = raw_field_ptr(event, #array, data);	\
-	memcpy(ptr.array, __array, sizeof(ptr.array));	\
-} while(0)
-
-#define FILL_COMMON_FIELDS(ptr, event, data)			\
-do {								\
-	FILL_FIELD(ptr, common_type, event, data);		\
-	FILL_FIELD(ptr, common_flags, event, data);		\
-	FILL_FIELD(ptr, common_preempt_count, event, data);	\
-	FILL_FIELD(ptr, common_pid, event, data);		\
-	FILL_FIELD(ptr, common_tgid, event, data);		\
-} while (0)
-
 static int
 replay_wakeup_event(struct perf_sched *sched,
 		    struct trace_wakeup_event *wakeup_event,
 		    struct machine *machine __maybe_unused,
-		    struct event_format *event, struct perf_sample *sample)
+		    struct perf_evsel *evsel, struct perf_sample *sample)
 {
 	struct task_desc *waker, *wakee;
 
 	if (verbose) {
-		printf("sched_wakeup event %p\n", event);
+		printf("sched_wakeup event %p\n", evsel);
 
 		printf(" ... pid %d woke up %s/%d\n",
-		       wakeup_event->common_pid, wakeup_event->comm, wakeup_event->pid);
+		       sample->tid, wakeup_event->comm, wakeup_event->pid);
 	}
 
-	waker = register_pid(sched, wakeup_event->common_pid, "<unknown>");
+	waker = register_pid(sched, sample->tid, "<unknown>");
 	wakee = register_pid(sched, wakeup_event->pid, wakeup_event->comm);
 
 	add_sched_event_wakeup(sched, waker, sample->time, wakee);
@@ -784,7 +724,7 @@ static int
 replay_switch_event(struct perf_sched *sched,
 		    struct trace_switch_event *switch_event,
 		    struct machine *machine __maybe_unused,
-		    struct event_format *event,
+		    struct perf_evsel *evsel,
 		    struct perf_sample *sample)
 {
 	struct task_desc *prev, __maybe_unused *next;
@@ -793,7 +733,7 @@ replay_switch_event(struct perf_sched *sched,
 	s64 delta;
 
 	if (verbose)
-		printf("sched_switch event %p\n", event);
+		printf("sched_switch event %p\n", evsel);
 
 	if (cpu >= MAX_CPUS || cpu < 0)
 		return 0;
@@ -829,10 +769,10 @@ replay_switch_event(struct perf_sched *sched,
 
 static int
 replay_fork_event(struct perf_sched *sched, struct trace_fork_event *fork_event,
-		  struct event_format *event)
+		  struct perf_evsel *evsel)
 {
 	if (verbose) {
-		printf("sched_fork event %p\n", event);
+		printf("sched_fork event %p\n", evsel);
 		printf("... parent: %s/%d\n", fork_event->parent_comm, fork_event->parent_pid);
 		printf("...  child: %s/%d\n", fork_event->child_comm, fork_event->child_pid);
 	}
@@ -931,7 +871,7 @@ static int thread_atoms_insert(struct perf_sched *sched, struct thread *thread)
 
 static int latency_fork_event(struct perf_sched *sched __maybe_unused,
 			      struct trace_fork_event *fork_event __maybe_unused,
-			      struct event_format *event __maybe_unused)
+			      struct perf_evsel *evsel __maybe_unused)
 {
 	/* should insert the newcomer */
 	return 0;
@@ -1015,7 +955,7 @@ static int
 latency_switch_event(struct perf_sched *sched,
 		     struct trace_switch_event *switch_event,
 		     struct machine *machine,
-		     struct event_format *event __maybe_unused,
+		     struct perf_evsel *evsel __maybe_unused,
 		     struct perf_sample *sample)
 {
 	struct work_atoms *out_events, *in_events;
@@ -1106,7 +1046,7 @@ static int
 latency_wakeup_event(struct perf_sched *sched,
 		     struct trace_wakeup_event *wakeup_event,
 		     struct machine *machine,
-		     struct event_format *event __maybe_unused,
+		     struct perf_evsel *evsel __maybe_unused,
 		     struct perf_sample *sample)
 {
 	struct work_atoms *atoms;
@@ -1350,34 +1290,32 @@ static void perf_sched__sort_lat(struct perf_sched *sched)
 }
 
 static int process_sched_wakeup_event(struct perf_tool *tool,
-				      struct event_format *event,
+				      struct perf_evsel *evsel,
 				      struct perf_sample *sample,
 				      struct machine *machine)
 {
 	struct perf_sched *sched = container_of(tool, struct perf_sched, tool);
-	void *data = sample->raw_data;
-	struct trace_wakeup_event wakeup_event;
-	int err = 0;
-
-	FILL_COMMON_FIELDS(wakeup_event, event, data);
 
-	FILL_ARRAY(wakeup_event, comm, event, data);
-	FILL_FIELD(wakeup_event, pid, event, data);
-	FILL_FIELD(wakeup_event, prio, event, data);
-	FILL_FIELD(wakeup_event, success, event, data);
-	FILL_FIELD(wakeup_event, cpu, event, data);
+	if (sched->tp_handler->wakeup_event) {
+		struct trace_wakeup_event event = {
+			.comm	 = perf_evsel__strval(evsel, sample, "comm"),
+			.pid	 = perf_evsel__intval(evsel, sample, "pid"),
+			.prio	 = perf_evsel__intval(evsel, sample, "prio"),
+			.success = perf_evsel__intval(evsel, sample, "success"),
+			.cpu	 = perf_evsel__intval(evsel, sample, "cpu"),
+		};
 
-	if (sched->tp_handler->wakeup_event)
-		err = sched->tp_handler->wakeup_event(sched, &wakeup_event, machine, event, sample);
+		return sched->tp_handler->wakeup_event(sched, &event, machine, evsel, sample);
+	}
 
-	return err;
+	return 0;
 }
 
 static int
 map_switch_event(struct perf_sched *sched,
 		 struct trace_switch_event *switch_event,
 		 struct machine *machine,
-		 struct event_format *event __maybe_unused,
+		 struct perf_evsel *evsel __maybe_unused,
 		 struct perf_sample *sample)
 {
 	struct thread *sched_out __maybe_unused, *sched_in;
@@ -1455,120 +1393,113 @@ map_switch_event(struct perf_sched *sched,
 }
 
 static int process_sched_switch_event(struct perf_tool *tool,
-				      struct event_format *event,
+				      struct perf_evsel *evsel,
 				      struct perf_sample *sample,
 				      struct machine *machine)
 {
 	struct perf_sched *sched = container_of(tool, struct perf_sched, tool);
 	int this_cpu = sample->cpu, err = 0;
-	void *data = sample->raw_data;
-	struct trace_switch_event switch_event;
-
-	FILL_COMMON_FIELDS(switch_event, event, data);
-
-	FILL_ARRAY(switch_event, prev_comm, event, data);
-	FILL_FIELD(switch_event, prev_pid, event, data);
-	FILL_FIELD(switch_event, prev_prio, event, data);
-	FILL_FIELD(switch_event, prev_state, event, data);
-	FILL_ARRAY(switch_event, next_comm, event, data);
-	FILL_FIELD(switch_event, next_pid, event, data);
-	FILL_FIELD(switch_event, next_prio, event, data);
+	u32 prev_pid = perf_evsel__intval(evsel, sample, "prev_pid"),
+	    next_pid = perf_evsel__intval(evsel, sample, "next_pid");
 
 	if (sched->curr_pid[this_cpu] != (u32)-1) {
 		/*
 		 * Are we trying to switch away a PID that is
 		 * not current?
 		 */
-		if (sched->curr_pid[this_cpu] != switch_event.prev_pid)
+		if (sched->curr_pid[this_cpu] != prev_pid)
 			sched->nr_context_switch_bugs++;
 	}
-	if (sched->tp_handler->switch_event)
-		err = sched->tp_handler->switch_event(sched, &switch_event, machine, event, sample);
 
-	sched->curr_pid[this_cpu] = switch_event.next_pid;
+	if (sched->tp_handler->switch_event) {
+		struct trace_switch_event event = {
+			.prev_comm  = perf_evsel__strval(evsel, sample, "prev_comm"),
+			.prev_pid   = prev_pid,
+			.prev_prio  = perf_evsel__intval(evsel, sample, "prev_prio"),
+			.prev_state = perf_evsel__intval(evsel, sample, "prev_state"),
+			.next_comm  = perf_evsel__strval(evsel, sample, "next_comm"),
+			.next_pid   = next_pid,
+			.next_prio  = perf_evsel__intval(evsel, sample, "next_prio"),
+		};
+
+		err = sched->tp_handler->switch_event(sched, &event, machine, evsel, sample);
+	}
+
+	sched->curr_pid[this_cpu] = next_pid;
 	return err;
 }
 
 static int process_sched_runtime_event(struct perf_tool *tool,
-				       struct event_format *event,
+				       struct perf_evsel *evsel,
 				       struct perf_sample *sample,
 				       struct machine *machine)
 {
 	struct perf_sched *sched = container_of(tool, struct perf_sched, tool);
-	void *data = sample->raw_data;
-	struct trace_runtime_event runtime_event;
-	int err = 0;
 
-	FILL_ARRAY(runtime_event, comm, event, data);
-	FILL_FIELD(runtime_event, pid, event, data);
-	FILL_FIELD(runtime_event, runtime, event, data);
-	FILL_FIELD(runtime_event, vruntime, event, data);
-
-	if (sched->tp_handler->runtime_event)
-		err = sched->tp_handler->runtime_event(sched, &runtime_event, machine, sample);
+	if (sched->tp_handler->runtime_event) {
+		struct trace_runtime_event event = {
+			.comm	  = perf_evsel__strval(evsel, sample, "comm"),
+			.pid	  = perf_evsel__intval(evsel, sample, "pid"),
+			.runtime  = perf_evsel__intval(evsel, sample, "runtime"),
+			.vruntime = perf_evsel__intval(evsel, sample, "vruntime"),
+		};
+		return sched->tp_handler->runtime_event(sched, &event, machine, sample);
+	}
 
-	return err;
+	return 0;
 }
 
 static int process_sched_fork_event(struct perf_tool *tool,
-				    struct event_format *event,
+				    struct perf_evsel *evsel,
 				    struct perf_sample *sample,
 				    struct machine *machine __maybe_unused)
 {
 	struct perf_sched *sched = container_of(tool, struct perf_sched, tool);
-	void *data = sample->raw_data;
-	struct trace_fork_event fork_event;
-	int err = 0;
-
-	FILL_COMMON_FIELDS(fork_event, event, data);
 
-	FILL_ARRAY(fork_event, parent_comm, event, data);
-	FILL_FIELD(fork_event, parent_pid, event, data);
-	FILL_ARRAY(fork_event, child_comm, event, data);
-	FILL_FIELD(fork_event, child_pid, event, data);
-
-	if (sched->tp_handler->fork_event)
-		err = sched->tp_handler->fork_event(sched, &fork_event, event);
+	if (sched->tp_handler->fork_event) {
+		struct trace_fork_event event = {
+			.parent_comm = perf_evsel__strval(evsel, sample, "parent_comm"),
+			.child_comm  = perf_evsel__strval(evsel, sample, "child_comm"),
+			.parent_pid  = perf_evsel__intval(evsel, sample, "parent_pid"),
+			.child_pid  = perf_evsel__intval(evsel, sample, "child_pid"),
+		};
+		return sched->tp_handler->fork_event(sched, &event, evsel);
+	}
 
-	return err;
+	return 0;
 }
 
 static int process_sched_exit_event(struct perf_tool *tool __maybe_unused,
-				    struct event_format *event,
+				    struct perf_evsel *evsel,
 				    struct perf_sample *sample __maybe_unused,
 				    struct machine *machine __maybe_unused)
 {
-	if (verbose)
-		printf("sched_exit event %p\n", event);
-
+	pr_debug("sched_exit event %p\n", evsel);
 	return 0;
 }
 
 static int process_sched_migrate_task_event(struct perf_tool *tool,
-					    struct event_format *event,
+					    struct perf_evsel *evsel,
 					    struct perf_sample *sample,
 					    struct machine *machine)
 {
 	struct perf_sched *sched = container_of(tool, struct perf_sched, tool);
-	void *data = sample->raw_data;
-	struct trace_migrate_task_event migrate_task_event;
-	int err = 0;
 
-	FILL_COMMON_FIELDS(migrate_task_event, event, data);
-
-	FILL_ARRAY(migrate_task_event, comm, event, data);
-	FILL_FIELD(migrate_task_event, pid, event, data);
-	FILL_FIELD(migrate_task_event, prio, event, data);
-	FILL_FIELD(migrate_task_event, cpu, event, data);
-
-	if (sched->tp_handler->migrate_task_event)
-		err = sched->tp_handler->migrate_task_event(sched, &migrate_task_event, machine, sample);
+	if (sched->tp_handler->migrate_task_event) {
+		struct trace_migrate_task_event event = {
+			.comm = perf_evsel__strval(evsel, sample, "comm"),
+			.pid  = perf_evsel__intval(evsel, sample, "pid"),
+			.prio = perf_evsel__intval(evsel, sample, "prio"),
+			.cpu  = perf_evsel__intval(evsel, sample, "cpu"),
+		};
+		return sched->tp_handler->migrate_task_event(sched, &event, machine, sample);
+	}
 
-	return err;
+	return 0;
 }
 
 typedef int (*tracepoint_handler)(struct perf_tool *tool,
-				  struct event_format *tp_format,
+				  struct perf_evsel *evsel,
 				  struct perf_sample *sample,
 				  struct machine *machine);
 
@@ -1592,7 +1523,7 @@ static int perf_sched__process_tracepoint_sample(struct perf_tool *tool __maybe_
 
 	if (evsel->handler.func != NULL) {
 		tracepoint_handler f = evsel->handler.func;
-		err = f(tool, evsel->tp_format, sample, machine);
+		err = f(tool, evsel, sample, machine);
 	}
 
 	return err;

commit 0e9b07e574e544c1e840c59dabf39fef120620ae
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Tue Sep 11 17:29:27 2012 -0300

    perf sched: Use perf_tool as ancestor
    
    So that we can remove all the globals.
    
    Before:
    
       text    data     bss     dec     hex filename
    1586833  110368 1438600 3135801  2fd939 /tmp/oldperf
    
    After:
    
       text    data     bss     dec     hex filename
    1629329   93568  848328 2571225  273bd9 /root/bin/perf
    
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Namhyung Kim <namhyung@gmail.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Link: http://lkml.kernel.org/n/tip-oph40vikij0crjz4eyapneov@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index af11b1aa1bd7..79f88fa3f7a3 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -23,26 +23,12 @@
 #include <pthread.h>
 #include <math.h>
 
-static const char		*input_name;
-
-static char			default_sort_order[] = "avg, max, switch, runtime";
-static const char		*sort_order = default_sort_order;
-
-static int			profile_cpu = -1;
-
 #define PR_SET_NAME		15               /* Set process name */
 #define MAX_CPUS		4096
-
-static u64			run_measurement_overhead;
-static u64			sleep_measurement_overhead;
-
 #define COMM_LEN		20
 #define SYM_LEN			129
-
 #define MAX_PID			65536
 
-static unsigned long		nr_tasks;
-
 struct sched_atom;
 
 struct task_desc {
@@ -80,44 +66,6 @@ struct sched_atom {
 	struct task_desc	*wakee;
 };
 
-static struct task_desc		*pid_to_task[MAX_PID];
-
-static struct task_desc		**tasks;
-
-static pthread_mutex_t		start_work_mutex = PTHREAD_MUTEX_INITIALIZER;
-static u64			start_time;
-
-static pthread_mutex_t		work_done_wait_mutex = PTHREAD_MUTEX_INITIALIZER;
-
-static unsigned long		nr_run_events;
-static unsigned long		nr_sleep_events;
-static unsigned long		nr_wakeup_events;
-
-static unsigned long		nr_sleep_corrections;
-static unsigned long		nr_run_events_optimized;
-
-static unsigned long		targetless_wakeups;
-static unsigned long		multitarget_wakeups;
-
-static u64			cpu_usage;
-static u64			runavg_cpu_usage;
-static u64			parent_cpu_usage;
-static u64			runavg_parent_cpu_usage;
-
-static unsigned long		nr_runs;
-static u64			sum_runtime;
-static u64			sum_fluct;
-static u64			run_avg;
-
-static unsigned int		replay_repeat = 10;
-static unsigned long		nr_timestamps;
-static unsigned long		nr_unordered_timestamps;
-static unsigned long		nr_state_machine_bugs;
-static unsigned long		nr_context_switch_bugs;
-static unsigned long		nr_events;
-static unsigned long		nr_lost_chunks;
-static unsigned long		nr_lost_events;
-
 #define TASK_STATE_TO_CHAR_STR "RSDTtZX"
 
 enum thread_state {
@@ -149,11 +97,169 @@ struct work_atoms {
 
 typedef int (*sort_fn_t)(struct work_atoms *, struct work_atoms *);
 
-static struct rb_root		atom_root, sorted_atom_root;
+struct trace_switch_event {
+	u32  size;
+
+	u16  common_type;
+	u8   common_flags;
+	u8   common_preempt_count;
+	u32  common_pid;
+	u32  common_tgid;
+
+	char prev_comm[16];
+	u32  prev_pid;
+	u32  prev_prio;
+	u64  prev_state;
+	char next_comm[16];
+	u32  next_pid;
+	u32  next_prio;
+};
+
+struct trace_runtime_event {
+	u32  size;
+
+	u16  common_type;
+	u8   common_flags;
+	u8   common_preempt_count;
+	u32  common_pid;
+	u32  common_tgid;
+
+	char comm[16];
+	u32  pid;
+	u64  runtime;
+	u64  vruntime;
+};
+
+struct trace_wakeup_event {
+	u32  size;
 
-static u64			all_runtime;
-static u64			all_count;
+	u16  common_type;
+	u8   common_flags;
+	u8   common_preempt_count;
+	u32  common_pid;
+	u32  common_tgid;
 
+	char comm[16];
+	u32  pid;
+
+	u32  prio;
+	u32  success;
+	u32  cpu;
+};
+
+struct trace_fork_event {
+	u32  size;
+
+	u16  common_type;
+	u8   common_flags;
+	u8   common_preempt_count;
+	u32  common_pid;
+	u32  common_tgid;
+
+	char parent_comm[16];
+	u32  parent_pid;
+	char child_comm[16];
+	u32   child_pid;
+};
+
+struct trace_migrate_task_event {
+	u32  size;
+
+	u16  common_type;
+	u8   common_flags;
+	u8   common_preempt_count;
+	u32  common_pid;
+	u32  common_tgid;
+
+	char comm[16];
+	u32  pid;
+
+	u32  prio;
+	u32  cpu;
+};
+
+struct perf_sched;
+
+struct trace_sched_handler {
+	int (*switch_event)(struct perf_sched *sched,
+			    struct trace_switch_event *event,
+			    struct machine *machine,
+			    struct event_format *tp_format,
+			    struct perf_sample *sample);
+
+	int (*runtime_event)(struct perf_sched *sched,
+			     struct trace_runtime_event *event,
+			     struct machine *machine,
+			     struct perf_sample *sample);
+
+	int (*wakeup_event)(struct perf_sched *sched,
+			    struct trace_wakeup_event *event,
+			    struct machine *machine,
+			    struct event_format *tp_format,
+			    struct perf_sample *sample);
+
+	int (*fork_event)(struct perf_sched *sched,
+			  struct trace_fork_event *event,
+			  struct event_format *tp_format);
+
+	int (*migrate_task_event)(struct perf_sched *sched,
+				  struct trace_migrate_task_event *event,
+				  struct machine *machine,
+				  struct perf_sample *sample);
+};
+
+struct perf_sched {
+	struct perf_tool tool;
+	const char	 *input_name;
+	const char	 *sort_order;
+	unsigned long	 nr_tasks;
+	struct task_desc *pid_to_task[MAX_PID];
+	struct task_desc **tasks;
+	const struct trace_sched_handler *tp_handler;
+	pthread_mutex_t	 start_work_mutex;
+	pthread_mutex_t	 work_done_wait_mutex;
+	int		 profile_cpu;
+/*
+ * Track the current task - that way we can know whether there's any
+ * weird events, such as a task being switched away that is not current.
+ */
+	int		 max_cpu;
+	u32		 curr_pid[MAX_CPUS];
+	struct thread	 *curr_thread[MAX_CPUS];
+	char		 next_shortname1;
+	char		 next_shortname2;
+	unsigned int	 replay_repeat;
+	unsigned long	 nr_run_events;
+	unsigned long	 nr_sleep_events;
+	unsigned long	 nr_wakeup_events;
+	unsigned long	 nr_sleep_corrections;
+	unsigned long	 nr_run_events_optimized;
+	unsigned long	 targetless_wakeups;
+	unsigned long	 multitarget_wakeups;
+	unsigned long	 nr_runs;
+	unsigned long	 nr_timestamps;
+	unsigned long	 nr_unordered_timestamps;
+	unsigned long	 nr_state_machine_bugs;
+	unsigned long	 nr_context_switch_bugs;
+	unsigned long	 nr_events;
+	unsigned long	 nr_lost_chunks;
+	unsigned long	 nr_lost_events;
+	u64		 run_measurement_overhead;
+	u64		 sleep_measurement_overhead;
+	u64		 start_time;
+	u64		 cpu_usage;
+	u64		 runavg_cpu_usage;
+	u64		 parent_cpu_usage;
+	u64		 runavg_parent_cpu_usage;
+	u64		 sum_runtime;
+	u64		 sum_fluct;
+	u64		 run_avg;
+	u64		 all_runtime;
+	u64		 all_count;
+	u64		 cpu_last_switched[MAX_CPUS];
+	struct rb_root	 atom_root, sorted_atom_root;
+	struct list_head sort_list, cmp_pid;
+};
 
 static u64 get_nsecs(void)
 {
@@ -164,13 +270,13 @@ static u64 get_nsecs(void)
 	return ts.tv_sec * 1000000000ULL + ts.tv_nsec;
 }
 
-static void burn_nsecs(u64 nsecs)
+static void burn_nsecs(struct perf_sched *sched, u64 nsecs)
 {
 	u64 T0 = get_nsecs(), T1;
 
 	do {
 		T1 = get_nsecs();
-	} while (T1 + run_measurement_overhead < T0 + nsecs);
+	} while (T1 + sched->run_measurement_overhead < T0 + nsecs);
 }
 
 static void sleep_nsecs(u64 nsecs)
@@ -183,24 +289,24 @@ static void sleep_nsecs(u64 nsecs)
 	nanosleep(&ts, NULL);
 }
 
-static void calibrate_run_measurement_overhead(void)
+static void calibrate_run_measurement_overhead(struct perf_sched *sched)
 {
 	u64 T0, T1, delta, min_delta = 1000000000ULL;
 	int i;
 
 	for (i = 0; i < 10; i++) {
 		T0 = get_nsecs();
-		burn_nsecs(0);
+		burn_nsecs(sched, 0);
 		T1 = get_nsecs();
 		delta = T1-T0;
 		min_delta = min(min_delta, delta);
 	}
-	run_measurement_overhead = min_delta;
+	sched->run_measurement_overhead = min_delta;
 
 	printf("run measurement overhead: %" PRIu64 " nsecs\n", min_delta);
 }
 
-static void calibrate_sleep_measurement_overhead(void)
+static void calibrate_sleep_measurement_overhead(struct perf_sched *sched)
 {
 	u64 T0, T1, delta, min_delta = 1000000000ULL;
 	int i;
@@ -213,7 +319,7 @@ static void calibrate_sleep_measurement_overhead(void)
 		min_delta = min(min_delta, delta);
 	}
 	min_delta -= 10000;
-	sleep_measurement_overhead = min_delta;
+	sched->sleep_measurement_overhead = min_delta;
 
 	printf("sleep measurement overhead: %" PRIu64 " nsecs\n", min_delta);
 }
@@ -246,8 +352,8 @@ static struct sched_atom *last_event(struct task_desc *task)
 	return task->atoms[task->nr_events - 1];
 }
 
-static void
-add_sched_event_run(struct task_desc *task, u64 timestamp, u64 duration)
+static void add_sched_event_run(struct perf_sched *sched, struct task_desc *task,
+				u64 timestamp, u64 duration)
 {
 	struct sched_atom *event, *curr_event = last_event(task);
 
@@ -256,7 +362,7 @@ add_sched_event_run(struct task_desc *task, u64 timestamp, u64 duration)
 	 * to it:
 	 */
 	if (curr_event && curr_event->type == SCHED_EVENT_RUN) {
-		nr_run_events_optimized++;
+		sched->nr_run_events_optimized++;
 		curr_event->duration += duration;
 		return;
 	}
@@ -266,12 +372,11 @@ add_sched_event_run(struct task_desc *task, u64 timestamp, u64 duration)
 	event->type = SCHED_EVENT_RUN;
 	event->duration = duration;
 
-	nr_run_events++;
+	sched->nr_run_events++;
 }
 
-static void
-add_sched_event_wakeup(struct task_desc *task, u64 timestamp,
-		       struct task_desc *wakee)
+static void add_sched_event_wakeup(struct perf_sched *sched, struct task_desc *task,
+				   u64 timestamp, struct task_desc *wakee)
 {
 	struct sched_atom *event, *wakee_event;
 
@@ -281,11 +386,11 @@ add_sched_event_wakeup(struct task_desc *task, u64 timestamp,
 
 	wakee_event = last_event(wakee);
 	if (!wakee_event || wakee_event->type != SCHED_EVENT_SLEEP) {
-		targetless_wakeups++;
+		sched->targetless_wakeups++;
 		return;
 	}
 	if (wakee_event->wait_sem) {
-		multitarget_wakeups++;
+		sched->multitarget_wakeups++;
 		return;
 	}
 
@@ -294,89 +399,89 @@ add_sched_event_wakeup(struct task_desc *task, u64 timestamp,
 	wakee_event->specific_wait = 1;
 	event->wait_sem = wakee_event->wait_sem;
 
-	nr_wakeup_events++;
+	sched->nr_wakeup_events++;
 }
 
-static void
-add_sched_event_sleep(struct task_desc *task, u64 timestamp,
-		      u64 task_state __maybe_unused)
+static void add_sched_event_sleep(struct perf_sched *sched, struct task_desc *task,
+				  u64 timestamp, u64 task_state __maybe_unused)
 {
 	struct sched_atom *event = get_new_event(task, timestamp);
 
 	event->type = SCHED_EVENT_SLEEP;
 
-	nr_sleep_events++;
+	sched->nr_sleep_events++;
 }
 
-static struct task_desc *register_pid(unsigned long pid, const char *comm)
+static struct task_desc *register_pid(struct perf_sched *sched,
+				      unsigned long pid, const char *comm)
 {
 	struct task_desc *task;
 
 	BUG_ON(pid >= MAX_PID);
 
-	task = pid_to_task[pid];
+	task = sched->pid_to_task[pid];
 
 	if (task)
 		return task;
 
 	task = zalloc(sizeof(*task));
 	task->pid = pid;
-	task->nr = nr_tasks;
+	task->nr = sched->nr_tasks;
 	strcpy(task->comm, comm);
 	/*
 	 * every task starts in sleeping state - this gets ignored
 	 * if there's no wakeup pointing to this sleep state:
 	 */
-	add_sched_event_sleep(task, 0, 0);
+	add_sched_event_sleep(sched, task, 0, 0);
 
-	pid_to_task[pid] = task;
-	nr_tasks++;
-	tasks = realloc(tasks, nr_tasks*sizeof(struct task_task *));
-	BUG_ON(!tasks);
-	tasks[task->nr] = task;
+	sched->pid_to_task[pid] = task;
+	sched->nr_tasks++;
+	sched->tasks = realloc(sched->tasks, sched->nr_tasks * sizeof(struct task_task *));
+	BUG_ON(!sched->tasks);
+	sched->tasks[task->nr] = task;
 
 	if (verbose)
-		printf("registered task #%ld, PID %ld (%s)\n", nr_tasks, pid, comm);
+		printf("registered task #%ld, PID %ld (%s)\n", sched->nr_tasks, pid, comm);
 
 	return task;
 }
 
 
-static void print_task_traces(void)
+static void print_task_traces(struct perf_sched *sched)
 {
 	struct task_desc *task;
 	unsigned long i;
 
-	for (i = 0; i < nr_tasks; i++) {
-		task = tasks[i];
+	for (i = 0; i < sched->nr_tasks; i++) {
+		task = sched->tasks[i];
 		printf("task %6ld (%20s:%10ld), nr_events: %ld\n",
 			task->nr, task->comm, task->pid, task->nr_events);
 	}
 }
 
-static void add_cross_task_wakeups(void)
+static void add_cross_task_wakeups(struct perf_sched *sched)
 {
 	struct task_desc *task1, *task2;
 	unsigned long i, j;
 
-	for (i = 0; i < nr_tasks; i++) {
-		task1 = tasks[i];
+	for (i = 0; i < sched->nr_tasks; i++) {
+		task1 = sched->tasks[i];
 		j = i + 1;
-		if (j == nr_tasks)
+		if (j == sched->nr_tasks)
 			j = 0;
-		task2 = tasks[j];
-		add_sched_event_wakeup(task1, 0, task2);
+		task2 = sched->tasks[j];
+		add_sched_event_wakeup(sched, task1, 0, task2);
 	}
 }
 
-static void process_sched_event(struct task_desc *this_task __maybe_unused,
-				struct sched_atom *atom)
+static void perf_sched__process_event(struct perf_sched *sched,
+				      struct sched_atom *atom)
 {
 	int ret = 0;
 
 	switch (atom->type) {
 		case SCHED_EVENT_RUN:
-			burn_nsecs(atom->duration);
+			burn_nsecs(sched, atom->duration);
 			break;
 		case SCHED_EVENT_SLEEP:
 			if (atom->wait_sem)
@@ -439,14 +544,23 @@ static u64 get_cpu_usage_nsec_self(int fd)
 	return runtime;
 }
 
+struct sched_thread_parms {
+	struct task_desc  *task;
+	struct perf_sched *sched;
+};
+
 static void *thread_func(void *ctx)
 {
-	struct task_desc *this_task = ctx;
+	struct sched_thread_parms *parms = ctx;
+	struct task_desc *this_task = parms->task;
+	struct perf_sched *sched = parms->sched;
 	u64 cpu_usage_0, cpu_usage_1;
 	unsigned long i, ret;
 	char comm2[22];
 	int fd;
 
+	free(parms);
+
 	sprintf(comm2, ":%s", this_task->comm);
 	prctl(PR_SET_NAME, comm2);
 	fd = self_open_counters();
@@ -455,16 +569,16 @@ static void *thread_func(void *ctx)
 again:
 	ret = sem_post(&this_task->ready_for_work);
 	BUG_ON(ret);
-	ret = pthread_mutex_lock(&start_work_mutex);
+	ret = pthread_mutex_lock(&sched->start_work_mutex);
 	BUG_ON(ret);
-	ret = pthread_mutex_unlock(&start_work_mutex);
+	ret = pthread_mutex_unlock(&sched->start_work_mutex);
 	BUG_ON(ret);
 
 	cpu_usage_0 = get_cpu_usage_nsec_self(fd);
 
 	for (i = 0; i < this_task->nr_events; i++) {
 		this_task->curr_event = i;
-		process_sched_event(this_task, this_task->atoms[i]);
+		perf_sched__process_event(sched, this_task->atoms[i]);
 	}
 
 	cpu_usage_1 = get_cpu_usage_nsec_self(fd);
@@ -472,15 +586,15 @@ static void *thread_func(void *ctx)
 	ret = sem_post(&this_task->work_done_sem);
 	BUG_ON(ret);
 
-	ret = pthread_mutex_lock(&work_done_wait_mutex);
+	ret = pthread_mutex_lock(&sched->work_done_wait_mutex);
 	BUG_ON(ret);
-	ret = pthread_mutex_unlock(&work_done_wait_mutex);
+	ret = pthread_mutex_unlock(&sched->work_done_wait_mutex);
 	BUG_ON(ret);
 
 	goto again;
 }
 
-static void create_tasks(void)
+static void create_tasks(struct perf_sched *sched)
 {
 	struct task_desc *task;
 	pthread_attr_t attr;
@@ -492,128 +606,129 @@ static void create_tasks(void)
 	err = pthread_attr_setstacksize(&attr,
 			(size_t) max(16 * 1024, PTHREAD_STACK_MIN));
 	BUG_ON(err);
-	err = pthread_mutex_lock(&start_work_mutex);
+	err = pthread_mutex_lock(&sched->start_work_mutex);
 	BUG_ON(err);
-	err = pthread_mutex_lock(&work_done_wait_mutex);
+	err = pthread_mutex_lock(&sched->work_done_wait_mutex);
 	BUG_ON(err);
-	for (i = 0; i < nr_tasks; i++) {
-		task = tasks[i];
+	for (i = 0; i < sched->nr_tasks; i++) {
+		struct sched_thread_parms *parms = malloc(sizeof(*parms));
+		BUG_ON(parms == NULL);
+		parms->task = task = sched->tasks[i];
+		parms->sched = sched;
 		sem_init(&task->sleep_sem, 0, 0);
 		sem_init(&task->ready_for_work, 0, 0);
 		sem_init(&task->work_done_sem, 0, 0);
 		task->curr_event = 0;
-		err = pthread_create(&task->thread, &attr, thread_func, task);
+		err = pthread_create(&task->thread, &attr, thread_func, parms);
 		BUG_ON(err);
 	}
 }
 
-static void wait_for_tasks(void)
+static void wait_for_tasks(struct perf_sched *sched)
 {
 	u64 cpu_usage_0, cpu_usage_1;
 	struct task_desc *task;
 	unsigned long i, ret;
 
-	start_time = get_nsecs();
-	cpu_usage = 0;
-	pthread_mutex_unlock(&work_done_wait_mutex);
+	sched->start_time = get_nsecs();
+	sched->cpu_usage = 0;
+	pthread_mutex_unlock(&sched->work_done_wait_mutex);
 
-	for (i = 0; i < nr_tasks; i++) {
-		task = tasks[i];
+	for (i = 0; i < sched->nr_tasks; i++) {
+		task = sched->tasks[i];
 		ret = sem_wait(&task->ready_for_work);
 		BUG_ON(ret);
 		sem_init(&task->ready_for_work, 0, 0);
 	}
-	ret = pthread_mutex_lock(&work_done_wait_mutex);
+	ret = pthread_mutex_lock(&sched->work_done_wait_mutex);
 	BUG_ON(ret);
 
 	cpu_usage_0 = get_cpu_usage_nsec_parent();
 
-	pthread_mutex_unlock(&start_work_mutex);
+	pthread_mutex_unlock(&sched->start_work_mutex);
 
-	for (i = 0; i < nr_tasks; i++) {
-		task = tasks[i];
+	for (i = 0; i < sched->nr_tasks; i++) {
+		task = sched->tasks[i];
 		ret = sem_wait(&task->work_done_sem);
 		BUG_ON(ret);
 		sem_init(&task->work_done_sem, 0, 0);
-		cpu_usage += task->cpu_usage;
+		sched->cpu_usage += task->cpu_usage;
 		task->cpu_usage = 0;
 	}
 
 	cpu_usage_1 = get_cpu_usage_nsec_parent();
-	if (!runavg_cpu_usage)
-		runavg_cpu_usage = cpu_usage;
-	runavg_cpu_usage = (runavg_cpu_usage*9 + cpu_usage)/10;
+	if (!sched->runavg_cpu_usage)
+		sched->runavg_cpu_usage = sched->cpu_usage;
+	sched->runavg_cpu_usage = (sched->runavg_cpu_usage * 9 + sched->cpu_usage) / 10;
 
-	parent_cpu_usage = cpu_usage_1 - cpu_usage_0;
-	if (!runavg_parent_cpu_usage)
-		runavg_parent_cpu_usage = parent_cpu_usage;
-	runavg_parent_cpu_usage = (runavg_parent_cpu_usage*9 +
-				   parent_cpu_usage)/10;
+	sched->parent_cpu_usage = cpu_usage_1 - cpu_usage_0;
+	if (!sched->runavg_parent_cpu_usage)
+		sched->runavg_parent_cpu_usage = sched->parent_cpu_usage;
+	sched->runavg_parent_cpu_usage = (sched->runavg_parent_cpu_usage * 9 +
+					 sched->parent_cpu_usage)/10;
 
-	ret = pthread_mutex_lock(&start_work_mutex);
+	ret = pthread_mutex_lock(&sched->start_work_mutex);
 	BUG_ON(ret);
 
-	for (i = 0; i < nr_tasks; i++) {
-		task = tasks[i];
+	for (i = 0; i < sched->nr_tasks; i++) {
+		task = sched->tasks[i];
 		sem_init(&task->sleep_sem, 0, 0);
 		task->curr_event = 0;
 	}
 }
 
-static void run_one_test(void)
+static void run_one_test(struct perf_sched *sched)
 {
 	u64 T0, T1, delta, avg_delta, fluct;
 
 	T0 = get_nsecs();
-	wait_for_tasks();
+	wait_for_tasks(sched);
 	T1 = get_nsecs();
 
 	delta = T1 - T0;
-	sum_runtime += delta;
-	nr_runs++;
+	sched->sum_runtime += delta;
+	sched->nr_runs++;
 
-	avg_delta = sum_runtime / nr_runs;
+	avg_delta = sched->sum_runtime / sched->nr_runs;
 	if (delta < avg_delta)
 		fluct = avg_delta - delta;
 	else
 		fluct = delta - avg_delta;
-	sum_fluct += fluct;
-	if (!run_avg)
-		run_avg = delta;
-	run_avg = (run_avg*9 + delta)/10;
+	sched->sum_fluct += fluct;
+	if (!sched->run_avg)
+		sched->run_avg = delta;
+	sched->run_avg = (sched->run_avg * 9 + delta) / 10;
 
-	printf("#%-3ld: %0.3f, ",
-		nr_runs, (double)delta/1000000.0);
+	printf("#%-3ld: %0.3f, ", sched->nr_runs, (double)delta / 1000000.0);
 
-	printf("ravg: %0.2f, ",
-		(double)run_avg/1e6);
+	printf("ravg: %0.2f, ", (double)sched->run_avg / 1e6);
 
 	printf("cpu: %0.2f / %0.2f",
-		(double)cpu_usage/1e6, (double)runavg_cpu_usage/1e6);
+		(double)sched->cpu_usage / 1e6, (double)sched->runavg_cpu_usage / 1e6);
 
 #if 0
 	/*
 	 * rusage statistics done by the parent, these are less
-	 * accurate than the sum_exec_runtime based statistics:
+	 * accurate than the sched->sum_exec_runtime based statistics:
 	 */
 	printf(" [%0.2f / %0.2f]",
-		(double)parent_cpu_usage/1e6,
-		(double)runavg_parent_cpu_usage/1e6);
+		(double)sched->parent_cpu_usage/1e6,
+		(double)sched->runavg_parent_cpu_usage/1e6);
 #endif
 
 	printf("\n");
 
-	if (nr_sleep_corrections)
-		printf(" (%ld sleep corrections)\n", nr_sleep_corrections);
-	nr_sleep_corrections = 0;
+	if (sched->nr_sleep_corrections)
+		printf(" (%ld sleep corrections)\n", sched->nr_sleep_corrections);
+	sched->nr_sleep_corrections = 0;
 }
 
-static void test_calibrations(void)
+static void test_calibrations(struct perf_sched *sched)
 {
 	u64 T0, T1;
 
 	T0 = get_nsecs();
-	burn_nsecs(1e6);
+	burn_nsecs(sched, 1e6);
 	T1 = get_nsecs();
 
 	printf("the run test took %" PRIu64 " nsecs\n", T1 - T0);
@@ -643,115 +758,9 @@ do {								\
 	FILL_FIELD(ptr, common_tgid, event, data);		\
 } while (0)
 
-
-
-struct trace_switch_event {
-	u32 size;
-
-	u16 common_type;
-	u8 common_flags;
-	u8 common_preempt_count;
-	u32 common_pid;
-	u32 common_tgid;
-
-	char prev_comm[16];
-	u32 prev_pid;
-	u32 prev_prio;
-	u64 prev_state;
-	char next_comm[16];
-	u32 next_pid;
-	u32 next_prio;
-};
-
-struct trace_runtime_event {
-	u32 size;
-
-	u16 common_type;
-	u8 common_flags;
-	u8 common_preempt_count;
-	u32 common_pid;
-	u32 common_tgid;
-
-	char comm[16];
-	u32 pid;
-	u64 runtime;
-	u64 vruntime;
-};
-
-struct trace_wakeup_event {
-	u32 size;
-
-	u16 common_type;
-	u8 common_flags;
-	u8 common_preempt_count;
-	u32 common_pid;
-	u32 common_tgid;
-
-	char comm[16];
-	u32 pid;
-
-	u32 prio;
-	u32 success;
-	u32 cpu;
-};
-
-struct trace_fork_event {
-	u32 size;
-
-	u16 common_type;
-	u8 common_flags;
-	u8 common_preempt_count;
-	u32 common_pid;
-	u32 common_tgid;
-
-	char parent_comm[16];
-	u32 parent_pid;
-	char child_comm[16];
-	u32 child_pid;
-};
-
-struct trace_migrate_task_event {
-	u32 size;
-
-	u16 common_type;
-	u8 common_flags;
-	u8 common_preempt_count;
-	u32 common_pid;
-	u32 common_tgid;
-
-	char comm[16];
-	u32 pid;
-
-	u32 prio;
-	u32 cpu;
-};
-
-struct trace_sched_handler {
-	int (*switch_event)(struct trace_switch_event *event,
-			    struct machine *machine,
-			    struct event_format *tp_format,
-			    struct perf_sample *sample);
-
-	int (*runtime_event)(struct trace_runtime_event *event,
-			     struct machine *machine,
-			     struct perf_sample *sample);
-
-	int (*wakeup_event)(struct trace_wakeup_event *event,
-			    struct machine *machine,
-			    struct event_format *tp_format,
-			    struct perf_sample *sample);
-
-	int (*fork_event)(struct trace_fork_event *event,
-			  struct event_format *tp_format);
-
-	int (*migrate_task_event)(struct trace_migrate_task_event *event,
-				  struct machine *machine,
-				  struct perf_sample *sample);
-};
-
-
 static int
-replay_wakeup_event(struct trace_wakeup_event *wakeup_event,
+replay_wakeup_event(struct perf_sched *sched,
+		    struct trace_wakeup_event *wakeup_event,
 		    struct machine *machine __maybe_unused,
 		    struct event_format *event, struct perf_sample *sample)
 {
@@ -761,22 +770,19 @@ replay_wakeup_event(struct trace_wakeup_event *wakeup_event,
 		printf("sched_wakeup event %p\n", event);
 
 		printf(" ... pid %d woke up %s/%d\n",
-			wakeup_event->common_pid,
-			wakeup_event->comm,
-			wakeup_event->pid);
+		       wakeup_event->common_pid, wakeup_event->comm, wakeup_event->pid);
 	}
 
-	waker = register_pid(wakeup_event->common_pid, "<unknown>");
-	wakee = register_pid(wakeup_event->pid, wakeup_event->comm);
+	waker = register_pid(sched, wakeup_event->common_pid, "<unknown>");
+	wakee = register_pid(sched, wakeup_event->pid, wakeup_event->comm);
 
-	add_sched_event_wakeup(waker, sample->time, wakee);
+	add_sched_event_wakeup(sched, waker, sample->time, wakee);
 	return 0;
 }
 
-static u64 cpu_last_switched[MAX_CPUS];
-
 static int
-replay_switch_event(struct trace_switch_event *switch_event,
+replay_switch_event(struct perf_sched *sched,
+		    struct trace_switch_event *switch_event,
 		    struct machine *machine __maybe_unused,
 		    struct event_format *event,
 		    struct perf_sample *sample)
@@ -792,7 +798,7 @@ replay_switch_event(struct trace_switch_event *switch_event,
 	if (cpu >= MAX_CPUS || cpu < 0)
 		return 0;
 
-	timestamp0 = cpu_last_switched[cpu];
+	timestamp0 = sched->cpu_last_switched[cpu];
 	if (timestamp0)
 		delta = timestamp - timestamp0;
 	else
@@ -810,20 +816,19 @@ replay_switch_event(struct trace_switch_event *switch_event,
 			delta);
 	}
 
-	prev = register_pid(switch_event->prev_pid, switch_event->prev_comm);
-	next = register_pid(switch_event->next_pid, switch_event->next_comm);
+	prev = register_pid(sched, switch_event->prev_pid, switch_event->prev_comm);
+	next = register_pid(sched, switch_event->next_pid, switch_event->next_comm);
 
-	cpu_last_switched[cpu] = timestamp;
+	sched->cpu_last_switched[cpu] = timestamp;
 
-	add_sched_event_run(prev, timestamp, delta);
-	add_sched_event_sleep(prev, timestamp, switch_event->prev_state);
+	add_sched_event_run(sched, prev, timestamp, delta);
+	add_sched_event_sleep(sched, prev, timestamp, switch_event->prev_state);
 
 	return 0;
 }
 
-
 static int
-replay_fork_event(struct trace_fork_event *fork_event,
+replay_fork_event(struct perf_sched *sched, struct trace_fork_event *fork_event,
 		  struct event_format *event)
 {
 	if (verbose) {
@@ -831,25 +836,17 @@ replay_fork_event(struct trace_fork_event *fork_event,
 		printf("... parent: %s/%d\n", fork_event->parent_comm, fork_event->parent_pid);
 		printf("...  child: %s/%d\n", fork_event->child_comm, fork_event->child_pid);
 	}
-	register_pid(fork_event->parent_pid, fork_event->parent_comm);
-	register_pid(fork_event->child_pid, fork_event->child_comm);
+	register_pid(sched, fork_event->parent_pid, fork_event->parent_comm);
+	register_pid(sched, fork_event->child_pid, fork_event->child_comm);
 	return 0;
 }
 
-static struct trace_sched_handler replay_ops  = {
-	.wakeup_event		= replay_wakeup_event,
-	.switch_event		= replay_switch_event,
-	.fork_event		= replay_fork_event,
-};
-
 struct sort_dimension {
 	const char		*name;
 	sort_fn_t		cmp;
 	struct list_head	list;
 };
 
-static LIST_HEAD(cmp_pid);
-
 static int
 thread_lat_cmp(struct list_head *list, struct work_atoms *l, struct work_atoms *r)
 {
@@ -918,7 +915,7 @@ __thread_latency_insert(struct rb_root *root, struct work_atoms *data,
 	rb_insert_color(&data->node, root);
 }
 
-static int thread_atoms_insert(struct thread *thread)
+static int thread_atoms_insert(struct perf_sched *sched, struct thread *thread)
 {
 	struct work_atoms *atoms = zalloc(sizeof(*atoms));
 	if (!atoms) {
@@ -928,11 +925,12 @@ static int thread_atoms_insert(struct thread *thread)
 
 	atoms->thread = thread;
 	INIT_LIST_HEAD(&atoms->work_list);
-	__thread_latency_insert(&atom_root, atoms, &cmp_pid);
+	__thread_latency_insert(&sched->atom_root, atoms, &sched->cmp_pid);
 	return 0;
 }
 
-static int latency_fork_event(struct trace_fork_event *fork_event __maybe_unused,
+static int latency_fork_event(struct perf_sched *sched __maybe_unused,
+			      struct trace_fork_event *fork_event __maybe_unused,
 			      struct event_format *event __maybe_unused)
 {
 	/* should insert the newcomer */
@@ -1014,7 +1012,8 @@ add_sched_in_event(struct work_atoms *atoms, u64 timestamp)
 }
 
 static int
-latency_switch_event(struct trace_switch_event *switch_event,
+latency_switch_event(struct perf_sched *sched,
+		     struct trace_switch_event *switch_event,
 		     struct machine *machine,
 		     struct event_format *event __maybe_unused,
 		     struct perf_sample *sample)
@@ -1027,8 +1026,8 @@ latency_switch_event(struct trace_switch_event *switch_event,
 
 	BUG_ON(cpu >= MAX_CPUS || cpu < 0);
 
-	timestamp0 = cpu_last_switched[cpu];
-	cpu_last_switched[cpu] = timestamp;
+	timestamp0 = sched->cpu_last_switched[cpu];
+	sched->cpu_last_switched[cpu] = timestamp;
 	if (timestamp0)
 		delta = timestamp - timestamp0;
 	else
@@ -1042,11 +1041,11 @@ latency_switch_event(struct trace_switch_event *switch_event,
 	sched_out = machine__findnew_thread(machine, switch_event->prev_pid);
 	sched_in = machine__findnew_thread(machine, switch_event->next_pid);
 
-	out_events = thread_atoms_search(&atom_root, sched_out, &cmp_pid);
+	out_events = thread_atoms_search(&sched->atom_root, sched_out, &sched->cmp_pid);
 	if (!out_events) {
-		if (thread_atoms_insert(sched_out))
+		if (thread_atoms_insert(sched, sched_out))
 			return -1;
-		out_events = thread_atoms_search(&atom_root, sched_out, &cmp_pid);
+		out_events = thread_atoms_search(&sched->atom_root, sched_out, &sched->cmp_pid);
 		if (!out_events) {
 			pr_err("out-event: Internal tree error");
 			return -1;
@@ -1055,11 +1054,11 @@ latency_switch_event(struct trace_switch_event *switch_event,
 	if (add_sched_out_event(out_events, sched_out_state(switch_event), timestamp))
 		return -1;
 
-	in_events = thread_atoms_search(&atom_root, sched_in, &cmp_pid);
+	in_events = thread_atoms_search(&sched->atom_root, sched_in, &sched->cmp_pid);
 	if (!in_events) {
-		if (thread_atoms_insert(sched_in))
+		if (thread_atoms_insert(sched, sched_in))
 			return -1;
-		in_events = thread_atoms_search(&atom_root, sched_in, &cmp_pid);
+		in_events = thread_atoms_search(&sched->atom_root, sched_in, &sched->cmp_pid);
 		if (!in_events) {
 			pr_err("in-event: Internal tree error");
 			return -1;
@@ -1077,19 +1076,20 @@ latency_switch_event(struct trace_switch_event *switch_event,
 }
 
 static int
-latency_runtime_event(struct trace_runtime_event *runtime_event,
+latency_runtime_event(struct perf_sched *sched,
+		      struct trace_runtime_event *runtime_event,
 		      struct machine *machine, struct perf_sample *sample)
 {
 	struct thread *thread = machine__findnew_thread(machine, runtime_event->pid);
-	struct work_atoms *atoms = thread_atoms_search(&atom_root, thread, &cmp_pid);
+	struct work_atoms *atoms = thread_atoms_search(&sched->atom_root, thread, &sched->cmp_pid);
 	u64 timestamp = sample->time;
 	int cpu = sample->cpu;
 
 	BUG_ON(cpu >= MAX_CPUS || cpu < 0);
 	if (!atoms) {
-		if (thread_atoms_insert(thread))
+		if (thread_atoms_insert(sched, thread))
 			return -1;
-		atoms = thread_atoms_search(&atom_root, thread, &cmp_pid);
+		atoms = thread_atoms_search(&sched->atom_root, thread, &sched->cmp_pid);
 		if (!atoms) {
 			pr_debug("in-event: Internal tree error");
 			return -1;
@@ -1103,7 +1103,8 @@ latency_runtime_event(struct trace_runtime_event *runtime_event,
 }
 
 static int
-latency_wakeup_event(struct trace_wakeup_event *wakeup_event,
+latency_wakeup_event(struct perf_sched *sched,
+		     struct trace_wakeup_event *wakeup_event,
 		     struct machine *machine,
 		     struct event_format *event __maybe_unused,
 		     struct perf_sample *sample)
@@ -1118,11 +1119,11 @@ latency_wakeup_event(struct trace_wakeup_event *wakeup_event,
 		return 0;
 
 	wakee = machine__findnew_thread(machine, wakeup_event->pid);
-	atoms = thread_atoms_search(&atom_root, wakee, &cmp_pid);
+	atoms = thread_atoms_search(&sched->atom_root, wakee, &sched->cmp_pid);
 	if (!atoms) {
-		if (thread_atoms_insert(wakee))
+		if (thread_atoms_insert(sched, wakee))
 			return -1;
-		atoms = thread_atoms_search(&atom_root, wakee, &cmp_pid);
+		atoms = thread_atoms_search(&sched->atom_root, wakee, &sched->cmp_pid);
 		if (!atoms) {
 			pr_debug("wakeup-event: Internal tree error");
 			return -1;
@@ -1140,12 +1141,12 @@ latency_wakeup_event(struct trace_wakeup_event *wakeup_event,
 	 * one CPU, or are only looking at only one, so don't
 	 * make useless noise.
 	 */
-	if (profile_cpu == -1 && atom->state != THREAD_SLEEPING)
-		nr_state_machine_bugs++;
+	if (sched->profile_cpu == -1 && atom->state != THREAD_SLEEPING)
+		sched->nr_state_machine_bugs++;
 
-	nr_timestamps++;
+	sched->nr_timestamps++;
 	if (atom->sched_out_time > timestamp) {
-		nr_unordered_timestamps++;
+		sched->nr_unordered_timestamps++;
 		return 0;
 	}
 
@@ -1155,7 +1156,8 @@ latency_wakeup_event(struct trace_wakeup_event *wakeup_event,
 }
 
 static int
-latency_migrate_task_event(struct trace_migrate_task_event *migrate_task_event,
+latency_migrate_task_event(struct perf_sched *sched,
+			   struct trace_migrate_task_event *migrate_task_event,
 			   struct machine *machine, struct perf_sample *sample)
 {
 	u64 timestamp = sample->time;
@@ -1166,16 +1168,16 @@ latency_migrate_task_event(struct trace_migrate_task_event *migrate_task_event,
 	/*
 	 * Only need to worry about migration when profiling one CPU.
 	 */
-	if (profile_cpu == -1)
+	if (sched->profile_cpu == -1)
 		return 0;
 
 	migrant = machine__findnew_thread(machine, migrate_task_event->pid);
-	atoms = thread_atoms_search(&atom_root, migrant, &cmp_pid);
+	atoms = thread_atoms_search(&sched->atom_root, migrant, &sched->cmp_pid);
 	if (!atoms) {
-		if (thread_atoms_insert(migrant))
+		if (thread_atoms_insert(sched, migrant))
 			return -1;
-		register_pid(migrant->pid, migrant->comm);
-		atoms = thread_atoms_search(&atom_root, migrant, &cmp_pid);
+		register_pid(sched, migrant->pid, migrant->comm);
+		atoms = thread_atoms_search(&sched->atom_root, migrant, &sched->cmp_pid);
 		if (!atoms) {
 			pr_debug("migration-event: Internal tree error");
 			return -1;
@@ -1189,23 +1191,15 @@ latency_migrate_task_event(struct trace_migrate_task_event *migrate_task_event,
 	atom = list_entry(atoms->work_list.prev, struct work_atom, list);
 	atom->sched_in_time = atom->sched_out_time = atom->wake_up_time = timestamp;
 
-	nr_timestamps++;
+	sched->nr_timestamps++;
 
 	if (atom->sched_out_time > timestamp)
-		nr_unordered_timestamps++;
+		sched->nr_unordered_timestamps++;
 
 	return 0;
 }
 
-static struct trace_sched_handler lat_ops  = {
-	.wakeup_event		= latency_wakeup_event,
-	.switch_event		= latency_switch_event,
-	.runtime_event		= latency_runtime_event,
-	.fork_event		= latency_fork_event,
-	.migrate_task_event	= latency_migrate_task_event,
-};
-
-static void output_lat_thread(struct work_atoms *work_list)
+static void output_lat_thread(struct perf_sched *sched, struct work_atoms *work_list)
 {
 	int i;
 	int ret;
@@ -1219,8 +1213,8 @@ static void output_lat_thread(struct work_atoms *work_list)
 	if (!strcmp(work_list->thread->comm, "swapper"))
 		return;
 
-	all_runtime += work_list->total_runtime;
-	all_count += work_list->nb_atoms;
+	sched->all_runtime += work_list->total_runtime;
+	sched->all_count   += work_list->nb_atoms;
 
 	ret = printf("  %s:%d ", work_list->thread->comm, work_list->thread->pid);
 
@@ -1246,11 +1240,6 @@ static int pid_cmp(struct work_atoms *l, struct work_atoms *r)
 	return 0;
 }
 
-static struct sort_dimension pid_sort_dimension = {
-	.name			= "pid",
-	.cmp			= pid_cmp,
-};
-
 static int avg_cmp(struct work_atoms *l, struct work_atoms *r)
 {
 	u64 avgl, avgr;
@@ -1272,11 +1261,6 @@ static int avg_cmp(struct work_atoms *l, struct work_atoms *r)
 	return 0;
 }
 
-static struct sort_dimension avg_sort_dimension = {
-	.name			= "avg",
-	.cmp			= avg_cmp,
-};
-
 static int max_cmp(struct work_atoms *l, struct work_atoms *r)
 {
 	if (l->max_lat < r->max_lat)
@@ -1287,11 +1271,6 @@ static int max_cmp(struct work_atoms *l, struct work_atoms *r)
 	return 0;
 }
 
-static struct sort_dimension max_sort_dimension = {
-	.name			= "max",
-	.cmp			= max_cmp,
-};
-
 static int switch_cmp(struct work_atoms *l, struct work_atoms *r)
 {
 	if (l->nb_atoms < r->nb_atoms)
@@ -1302,11 +1281,6 @@ static int switch_cmp(struct work_atoms *l, struct work_atoms *r)
 	return 0;
 }
 
-static struct sort_dimension switch_sort_dimension = {
-	.name			= "switch",
-	.cmp			= switch_cmp,
-};
-
 static int runtime_cmp(struct work_atoms *l, struct work_atoms *r)
 {
 	if (l->total_runtime < r->total_runtime)
@@ -1317,28 +1291,38 @@ static int runtime_cmp(struct work_atoms *l, struct work_atoms *r)
 	return 0;
 }
 
-static struct sort_dimension runtime_sort_dimension = {
-	.name			= "runtime",
-	.cmp			= runtime_cmp,
-};
-
-static struct sort_dimension *available_sorts[] = {
-	&pid_sort_dimension,
-	&avg_sort_dimension,
-	&max_sort_dimension,
-	&switch_sort_dimension,
-	&runtime_sort_dimension,
-};
-
-#define NB_AVAILABLE_SORTS	(int)(sizeof(available_sorts) / sizeof(struct sort_dimension *))
-
-static LIST_HEAD(sort_list);
-
 static int sort_dimension__add(const char *tok, struct list_head *list)
 {
-	int i;
+	size_t i;
+	static struct sort_dimension avg_sort_dimension = {
+		.name = "avg",
+		.cmp  = avg_cmp,
+	};
+	static struct sort_dimension max_sort_dimension = {
+		.name = "max",
+		.cmp  = max_cmp,
+	};
+	static struct sort_dimension pid_sort_dimension = {
+		.name = "pid",
+		.cmp  = pid_cmp,
+	};
+	static struct sort_dimension runtime_sort_dimension = {
+		.name = "runtime",
+		.cmp  = runtime_cmp,
+	};
+	static struct sort_dimension switch_sort_dimension = {
+		.name = "switch",
+		.cmp  = switch_cmp,
+	};
+	struct sort_dimension *available_sorts[] = {
+		&pid_sort_dimension,
+		&avg_sort_dimension,
+		&max_sort_dimension,
+		&switch_sort_dimension,
+		&runtime_sort_dimension,
+	};
 
-	for (i = 0; i < NB_AVAILABLE_SORTS; i++) {
+	for (i = 0; i < ARRAY_SIZE(available_sorts); i++) {
 		if (!strcmp(available_sorts[i]->name, tok)) {
 			list_add_tail(&available_sorts[i]->list, list);
 
@@ -1349,31 +1333,28 @@ static int sort_dimension__add(const char *tok, struct list_head *list)
 	return -1;
 }
 
-static void setup_sorting(void);
-
-static void sort_lat(void)
+static void perf_sched__sort_lat(struct perf_sched *sched)
 {
 	struct rb_node *node;
 
 	for (;;) {
 		struct work_atoms *data;
-		node = rb_first(&atom_root);
+		node = rb_first(&sched->atom_root);
 		if (!node)
 			break;
 
-		rb_erase(node, &atom_root);
+		rb_erase(node, &sched->atom_root);
 		data = rb_entry(node, struct work_atoms, node);
-		__thread_latency_insert(&sorted_atom_root, data, &sort_list);
+		__thread_latency_insert(&sched->sorted_atom_root, data, &sched->sort_list);
 	}
 }
 
-static struct trace_sched_handler *trace_handler;
-
-static int process_sched_wakeup_event(struct perf_tool *tool __maybe_unused,
+static int process_sched_wakeup_event(struct perf_tool *tool,
 				      struct event_format *event,
 				      struct perf_sample *sample,
 				      struct machine *machine)
 {
+	struct perf_sched *sched = container_of(tool, struct perf_sched, tool);
 	void *data = sample->raw_data;
 	struct trace_wakeup_event wakeup_event;
 	int err = 0;
@@ -1386,27 +1367,15 @@ static int process_sched_wakeup_event(struct perf_tool *tool __maybe_unused,
 	FILL_FIELD(wakeup_event, success, event, data);
 	FILL_FIELD(wakeup_event, cpu, event, data);
 
-	if (trace_handler->wakeup_event)
-		err = trace_handler->wakeup_event(&wakeup_event, machine, event, sample);
+	if (sched->tp_handler->wakeup_event)
+		err = sched->tp_handler->wakeup_event(sched, &wakeup_event, machine, event, sample);
 
 	return err;
 }
 
-/*
- * Track the current task - that way we can know whether there's any
- * weird events, such as a task being switched away that is not current.
- */
-static int max_cpu;
-
-static u32 curr_pid[MAX_CPUS] = { [0 ... MAX_CPUS-1] = -1 };
-
-static struct thread *curr_thread[MAX_CPUS];
-
-static char next_shortname1 = 'A';
-static char next_shortname2 = '0';
-
 static int
-map_switch_event(struct trace_switch_event *switch_event,
+map_switch_event(struct perf_sched *sched,
+		 struct trace_switch_event *switch_event,
 		 struct machine *machine,
 		 struct event_format *event __maybe_unused,
 		 struct perf_sample *sample)
@@ -1419,11 +1388,11 @@ map_switch_event(struct trace_switch_event *switch_event,
 
 	BUG_ON(this_cpu >= MAX_CPUS || this_cpu < 0);
 
-	if (this_cpu > max_cpu)
-		max_cpu = this_cpu;
+	if (this_cpu > sched->max_cpu)
+		sched->max_cpu = this_cpu;
 
-	timestamp0 = cpu_last_switched[this_cpu];
-	cpu_last_switched[this_cpu] = timestamp;
+	timestamp0 = sched->cpu_last_switched[this_cpu];
+	sched->cpu_last_switched[this_cpu] = timestamp;
 	if (timestamp0)
 		delta = timestamp - timestamp0;
 	else
@@ -1437,37 +1406,37 @@ map_switch_event(struct trace_switch_event *switch_event,
 	sched_out = machine__findnew_thread(machine, switch_event->prev_pid);
 	sched_in = machine__findnew_thread(machine, switch_event->next_pid);
 
-	curr_thread[this_cpu] = sched_in;
+	sched->curr_thread[this_cpu] = sched_in;
 
 	printf("  ");
 
 	new_shortname = 0;
 	if (!sched_in->shortname[0]) {
-		sched_in->shortname[0] = next_shortname1;
-		sched_in->shortname[1] = next_shortname2;
+		sched_in->shortname[0] = sched->next_shortname1;
+		sched_in->shortname[1] = sched->next_shortname2;
 
-		if (next_shortname1 < 'Z') {
-			next_shortname1++;
+		if (sched->next_shortname1 < 'Z') {
+			sched->next_shortname1++;
 		} else {
-			next_shortname1='A';
-			if (next_shortname2 < '9') {
-				next_shortname2++;
+			sched->next_shortname1='A';
+			if (sched->next_shortname2 < '9') {
+				sched->next_shortname2++;
 			} else {
-				next_shortname2='0';
+				sched->next_shortname2='0';
 			}
 		}
 		new_shortname = 1;
 	}
 
-	for (cpu = 0; cpu <= max_cpu; cpu++) {
+	for (cpu = 0; cpu <= sched->max_cpu; cpu++) {
 		if (cpu != this_cpu)
 			printf(" ");
 		else
 			printf("*");
 
-		if (curr_thread[cpu]) {
-			if (curr_thread[cpu]->pid)
-				printf("%2s ", curr_thread[cpu]->shortname);
+		if (sched->curr_thread[cpu]) {
+			if (sched->curr_thread[cpu]->pid)
+				printf("%2s ", sched->curr_thread[cpu]->shortname);
 			else
 				printf(".  ");
 		} else
@@ -1485,11 +1454,12 @@ map_switch_event(struct trace_switch_event *switch_event,
 	return 0;
 }
 
-static int process_sched_switch_event(struct perf_tool *tool __maybe_unused,
+static int process_sched_switch_event(struct perf_tool *tool,
 				      struct event_format *event,
 				      struct perf_sample *sample,
 				      struct machine *machine)
 {
+	struct perf_sched *sched = container_of(tool, struct perf_sched, tool);
 	int this_cpu = sample->cpu, err = 0;
 	void *data = sample->raw_data;
 	struct trace_switch_event switch_event;
@@ -1504,26 +1474,27 @@ static int process_sched_switch_event(struct perf_tool *tool __maybe_unused,
 	FILL_FIELD(switch_event, next_pid, event, data);
 	FILL_FIELD(switch_event, next_prio, event, data);
 
-	if (curr_pid[this_cpu] != (u32)-1) {
+	if (sched->curr_pid[this_cpu] != (u32)-1) {
 		/*
 		 * Are we trying to switch away a PID that is
 		 * not current?
 		 */
-		if (curr_pid[this_cpu] != switch_event.prev_pid)
-			nr_context_switch_bugs++;
+		if (sched->curr_pid[this_cpu] != switch_event.prev_pid)
+			sched->nr_context_switch_bugs++;
 	}
-	if (trace_handler->switch_event)
-		err = trace_handler->switch_event(&switch_event, machine, event, sample);
+	if (sched->tp_handler->switch_event)
+		err = sched->tp_handler->switch_event(sched, &switch_event, machine, event, sample);
 
-	curr_pid[this_cpu] = switch_event.next_pid;
+	sched->curr_pid[this_cpu] = switch_event.next_pid;
 	return err;
 }
 
-static int process_sched_runtime_event(struct perf_tool *tool __maybe_unused,
+static int process_sched_runtime_event(struct perf_tool *tool,
 				       struct event_format *event,
 				       struct perf_sample *sample,
 				       struct machine *machine)
 {
+	struct perf_sched *sched = container_of(tool, struct perf_sched, tool);
 	void *data = sample->raw_data;
 	struct trace_runtime_event runtime_event;
 	int err = 0;
@@ -1533,17 +1504,18 @@ static int process_sched_runtime_event(struct perf_tool *tool __maybe_unused,
 	FILL_FIELD(runtime_event, runtime, event, data);
 	FILL_FIELD(runtime_event, vruntime, event, data);
 
-	if (trace_handler->runtime_event)
-		err = trace_handler->runtime_event(&runtime_event, machine, sample);
+	if (sched->tp_handler->runtime_event)
+		err = sched->tp_handler->runtime_event(sched, &runtime_event, machine, sample);
 
 	return err;
 }
 
-static int process_sched_fork_event(struct perf_tool *tool __maybe_unused,
+static int process_sched_fork_event(struct perf_tool *tool,
 				    struct event_format *event,
 				    struct perf_sample *sample,
 				    struct machine *machine __maybe_unused)
 {
+	struct perf_sched *sched = container_of(tool, struct perf_sched, tool);
 	void *data = sample->raw_data;
 	struct trace_fork_event fork_event;
 	int err = 0;
@@ -1555,8 +1527,8 @@ static int process_sched_fork_event(struct perf_tool *tool __maybe_unused,
 	FILL_ARRAY(fork_event, child_comm, event, data);
 	FILL_FIELD(fork_event, child_pid, event, data);
 
-	if (trace_handler->fork_event)
-		err = trace_handler->fork_event(&fork_event, event);
+	if (sched->tp_handler->fork_event)
+		err = sched->tp_handler->fork_event(sched, &fork_event, event);
 
 	return err;
 }
@@ -1572,11 +1544,12 @@ static int process_sched_exit_event(struct perf_tool *tool __maybe_unused,
 	return 0;
 }
 
-static int process_sched_migrate_task_event(struct perf_tool *tool __maybe_unused,
+static int process_sched_migrate_task_event(struct perf_tool *tool,
 					    struct event_format *event,
 					    struct perf_sample *sample,
 					    struct machine *machine)
 {
+	struct perf_sched *sched = container_of(tool, struct perf_sched, tool);
 	void *data = sample->raw_data;
 	struct trace_migrate_task_event migrate_task_event;
 	int err = 0;
@@ -1588,8 +1561,8 @@ static int process_sched_migrate_task_event(struct perf_tool *tool __maybe_unuse
 	FILL_FIELD(migrate_task_event, prio, event, data);
 	FILL_FIELD(migrate_task_event, cpu, event, data);
 
-	if (trace_handler->migrate_task_event)
-		err = trace_handler->migrate_task_event(&migrate_task_event, machine, sample);
+	if (sched->tp_handler->migrate_task_event)
+		err = sched->tp_handler->migrate_task_event(sched, &migrate_task_event, machine, sample);
 
 	return err;
 }
@@ -1625,15 +1598,8 @@ static int perf_sched__process_tracepoint_sample(struct perf_tool *tool __maybe_
 	return err;
 }
 
-static struct perf_tool perf_sched = {
-	.sample		 = perf_sched__process_tracepoint_sample,
-	.comm		 = perf_event__process_comm,
-	.lost		 = perf_event__process_lost,
-	.fork		 = perf_event__process_task,
-	.ordered_samples = true,
-};
-
-static int read_events(bool destroy, struct perf_session **psession)
+static int perf_sched__read_events(struct perf_sched *sched, bool destroy,
+				   struct perf_session **psession)
 {
 	const struct perf_evsel_str_handler handlers[] = {
 		{ "sched:sched_switch",	      process_sched_switch_event, },
@@ -1646,7 +1612,7 @@ static int read_events(bool destroy, struct perf_session **psession)
 	};
 	struct perf_session *session;
 
-	session = perf_session__new(input_name, O_RDONLY, 0, false, &perf_sched);
+	session = perf_session__new(sched->input_name, O_RDONLY, 0, false, &sched->tool);
 	if (session == NULL) {
 		pr_debug("No Memory for session\n");
 		return -1;
@@ -1656,15 +1622,15 @@ static int read_events(bool destroy, struct perf_session **psession)
 		goto out_delete;
 
 	if (perf_session__has_traces(session, "record -R")) {
-		int err = perf_session__process_events(session, &perf_sched);
+		int err = perf_session__process_events(session, &sched->tool);
 		if (err) {
 			pr_err("Failed to process events, error %d", err);
 			goto out_delete;
 		}
 
-		nr_events      = session->hists.stats.nr_events[0];
-		nr_lost_events = session->hists.stats.total_lost;
-		nr_lost_chunks = session->hists.stats.nr_events[PERF_RECORD_LOST];
+		sched->nr_events      = session->hists.stats.nr_events[0];
+		sched->nr_lost_events = session->hists.stats.total_lost;
+		sched->nr_lost_chunks = session->hists.stats.nr_events[PERF_RECORD_LOST];
 	}
 
 	if (destroy)
@@ -1680,213 +1646,158 @@ static int read_events(bool destroy, struct perf_session **psession)
 	return -1;
 }
 
-static void print_bad_events(void)
+static void print_bad_events(struct perf_sched *sched)
 {
-	if (nr_unordered_timestamps && nr_timestamps) {
+	if (sched->nr_unordered_timestamps && sched->nr_timestamps) {
 		printf("  INFO: %.3f%% unordered timestamps (%ld out of %ld)\n",
-			(double)nr_unordered_timestamps/(double)nr_timestamps*100.0,
-			nr_unordered_timestamps, nr_timestamps);
+			(double)sched->nr_unordered_timestamps/(double)sched->nr_timestamps*100.0,
+			sched->nr_unordered_timestamps, sched->nr_timestamps);
 	}
-	if (nr_lost_events && nr_events) {
+	if (sched->nr_lost_events && sched->nr_events) {
 		printf("  INFO: %.3f%% lost events (%ld out of %ld, in %ld chunks)\n",
-			(double)nr_lost_events/(double)nr_events*100.0,
-			nr_lost_events, nr_events, nr_lost_chunks);
+			(double)sched->nr_lost_events/(double)sched->nr_events * 100.0,
+			sched->nr_lost_events, sched->nr_events, sched->nr_lost_chunks);
 	}
-	if (nr_state_machine_bugs && nr_timestamps) {
+	if (sched->nr_state_machine_bugs && sched->nr_timestamps) {
 		printf("  INFO: %.3f%% state machine bugs (%ld out of %ld)",
-			(double)nr_state_machine_bugs/(double)nr_timestamps*100.0,
-			nr_state_machine_bugs, nr_timestamps);
-		if (nr_lost_events)
+			(double)sched->nr_state_machine_bugs/(double)sched->nr_timestamps*100.0,
+			sched->nr_state_machine_bugs, sched->nr_timestamps);
+		if (sched->nr_lost_events)
 			printf(" (due to lost events?)");
 		printf("\n");
 	}
-	if (nr_context_switch_bugs && nr_timestamps) {
+	if (sched->nr_context_switch_bugs && sched->nr_timestamps) {
 		printf("  INFO: %.3f%% context switch bugs (%ld out of %ld)",
-			(double)nr_context_switch_bugs/(double)nr_timestamps*100.0,
-			nr_context_switch_bugs, nr_timestamps);
-		if (nr_lost_events)
+			(double)sched->nr_context_switch_bugs/(double)sched->nr_timestamps*100.0,
+			sched->nr_context_switch_bugs, sched->nr_timestamps);
+		if (sched->nr_lost_events)
 			printf(" (due to lost events?)");
 		printf("\n");
 	}
 }
 
-static int __cmd_lat(void)
+static int perf_sched__lat(struct perf_sched *sched)
 {
 	struct rb_node *next;
 	struct perf_session *session;
 
 	setup_pager();
-	if (read_events(false, &session))
+	if (perf_sched__read_events(sched, false, &session))
 		return -1;
-	sort_lat();
+	perf_sched__sort_lat(sched);
 
 	printf("\n ---------------------------------------------------------------------------------------------------------------\n");
 	printf("  Task                  |   Runtime ms  | Switches | Average delay ms | Maximum delay ms | Maximum delay at     |\n");
 	printf(" ---------------------------------------------------------------------------------------------------------------\n");
 
-	next = rb_first(&sorted_atom_root);
+	next = rb_first(&sched->sorted_atom_root);
 
 	while (next) {
 		struct work_atoms *work_list;
 
 		work_list = rb_entry(next, struct work_atoms, node);
-		output_lat_thread(work_list);
+		output_lat_thread(sched, work_list);
 		next = rb_next(next);
 	}
 
 	printf(" -----------------------------------------------------------------------------------------\n");
 	printf("  TOTAL:                |%11.3f ms |%9" PRIu64 " |\n",
-		(double)all_runtime/1e6, all_count);
+		(double)sched->all_runtime / 1e6, sched->all_count);
 
 	printf(" ---------------------------------------------------\n");
 
-	print_bad_events();
+	print_bad_events(sched);
 	printf("\n");
 
 	perf_session__delete(session);
 	return 0;
 }
 
-static struct trace_sched_handler map_ops  = {
-	.wakeup_event		= NULL,
-	.switch_event		= map_switch_event,
-	.runtime_event		= NULL,
-	.fork_event		= NULL,
-};
-
-static int __cmd_map(void)
+static int perf_sched__map(struct perf_sched *sched)
 {
-	max_cpu = sysconf(_SC_NPROCESSORS_CONF);
+	sched->max_cpu = sysconf(_SC_NPROCESSORS_CONF);
 
 	setup_pager();
-	if (read_events(true, NULL))
+	if (perf_sched__read_events(sched, true, NULL))
 		return -1;
-	print_bad_events();
+	print_bad_events(sched);
 	return 0;
 }
 
-static int __cmd_replay(void)
+static int perf_sched__replay(struct perf_sched *sched)
 {
 	unsigned long i;
 
-	calibrate_run_measurement_overhead();
-	calibrate_sleep_measurement_overhead();
+	calibrate_run_measurement_overhead(sched);
+	calibrate_sleep_measurement_overhead(sched);
 
-	test_calibrations();
+	test_calibrations(sched);
 
-	if (read_events(true, NULL))
+	if (perf_sched__read_events(sched, true, NULL))
 		return -1;
 
-	printf("nr_run_events:        %ld\n", nr_run_events);
-	printf("nr_sleep_events:      %ld\n", nr_sleep_events);
-	printf("nr_wakeup_events:     %ld\n", nr_wakeup_events);
+	printf("nr_run_events:        %ld\n", sched->nr_run_events);
+	printf("nr_sleep_events:      %ld\n", sched->nr_sleep_events);
+	printf("nr_wakeup_events:     %ld\n", sched->nr_wakeup_events);
 
-	if (targetless_wakeups)
-		printf("target-less wakeups:  %ld\n", targetless_wakeups);
-	if (multitarget_wakeups)
-		printf("multi-target wakeups: %ld\n", multitarget_wakeups);
-	if (nr_run_events_optimized)
+	if (sched->targetless_wakeups)
+		printf("target-less wakeups:  %ld\n", sched->targetless_wakeups);
+	if (sched->multitarget_wakeups)
+		printf("multi-target wakeups: %ld\n", sched->multitarget_wakeups);
+	if (sched->nr_run_events_optimized)
 		printf("run atoms optimized: %ld\n",
-			nr_run_events_optimized);
+			sched->nr_run_events_optimized);
 
-	print_task_traces();
-	add_cross_task_wakeups();
+	print_task_traces(sched);
+	add_cross_task_wakeups(sched);
 
-	create_tasks();
+	create_tasks(sched);
 	printf("------------------------------------------------------------\n");
-	for (i = 0; i < replay_repeat; i++)
-		run_one_test();
+	for (i = 0; i < sched->replay_repeat; i++)
+		run_one_test(sched);
 
 	return 0;
 }
 
-
-static const char * const sched_usage[] = {
-	"perf sched [<options>] {record|latency|map|replay|script}",
-	NULL
-};
-
-static const struct option sched_options[] = {
-	OPT_STRING('i', "input", &input_name, "file",
-		    "input file name"),
-	OPT_INCR('v', "verbose", &verbose,
-		    "be more verbose (show symbol address, etc)"),
-	OPT_BOOLEAN('D', "dump-raw-trace", &dump_trace,
-		    "dump raw trace in ASCII"),
-	OPT_END()
-};
-
-static const char * const latency_usage[] = {
-	"perf sched latency [<options>]",
-	NULL
-};
-
-static const struct option latency_options[] = {
-	OPT_STRING('s', "sort", &sort_order, "key[,key2...]",
-		   "sort by key(s): runtime, switch, avg, max"),
-	OPT_INCR('v', "verbose", &verbose,
-		    "be more verbose (show symbol address, etc)"),
-	OPT_INTEGER('C', "CPU", &profile_cpu,
-		    "CPU to profile on"),
-	OPT_BOOLEAN('D', "dump-raw-trace", &dump_trace,
-		    "dump raw trace in ASCII"),
-	OPT_END()
-};
-
-static const char * const replay_usage[] = {
-	"perf sched replay [<options>]",
-	NULL
-};
-
-static const struct option replay_options[] = {
-	OPT_UINTEGER('r', "repeat", &replay_repeat,
-		     "repeat the workload replay N times (-1: infinite)"),
-	OPT_INCR('v', "verbose", &verbose,
-		    "be more verbose (show symbol address, etc)"),
-	OPT_BOOLEAN('D', "dump-raw-trace", &dump_trace,
-		    "dump raw trace in ASCII"),
-	OPT_END()
-};
-
-static void setup_sorting(void)
+static void setup_sorting(struct perf_sched *sched, const struct option *options,
+			  const char * const usage_msg[])
 {
-	char *tmp, *tok, *str = strdup(sort_order);
+	char *tmp, *tok, *str = strdup(sched->sort_order);
 
 	for (tok = strtok_r(str, ", ", &tmp);
 			tok; tok = strtok_r(NULL, ", ", &tmp)) {
-		if (sort_dimension__add(tok, &sort_list) < 0) {
+		if (sort_dimension__add(tok, &sched->sort_list) < 0) {
 			error("Unknown --sort key: `%s'", tok);
-			usage_with_options(latency_usage, latency_options);
+			usage_with_options(usage_msg, options);
 		}
 	}
 
 	free(str);
 
-	sort_dimension__add("pid", &cmp_pid);
+	sort_dimension__add("pid", &sched->cmp_pid);
 }
 
-static const char *record_args[] = {
-	"record",
-	"-a",
-	"-R",
-	"-f",
-	"-m", "1024",
-	"-c", "1",
-	"-e", "sched:sched_switch",
-	"-e", "sched:sched_stat_wait",
-	"-e", "sched:sched_stat_sleep",
-	"-e", "sched:sched_stat_iowait",
-	"-e", "sched:sched_stat_runtime",
-	"-e", "sched:sched_process_exit",
-	"-e", "sched:sched_process_fork",
-	"-e", "sched:sched_wakeup",
-	"-e", "sched:sched_migrate_task",
-};
-
 static int __cmd_record(int argc, const char **argv)
 {
 	unsigned int rec_argc, i, j;
 	const char **rec_argv;
+	const char * const record_args[] = {
+		"record",
+		"-a",
+		"-R",
+		"-f",
+		"-m", "1024",
+		"-c", "1",
+		"-e", "sched:sched_switch",
+		"-e", "sched:sched_stat_wait",
+		"-e", "sched:sched_stat_sleep",
+		"-e", "sched:sched_stat_iowait",
+		"-e", "sched:sched_stat_runtime",
+		"-e", "sched:sched_process_exit",
+		"-e", "sched:sched_process_fork",
+		"-e", "sched:sched_wakeup",
+		"-e", "sched:sched_migrate_task",
+	};
 
 	rec_argc = ARRAY_SIZE(record_args) + argc - 1;
 	rec_argv = calloc(rec_argc + 1, sizeof(char *));
@@ -1907,6 +1818,83 @@ static int __cmd_record(int argc, const char **argv)
 
 int cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)
 {
+	const char default_sort_order[] = "avg, max, switch, runtime";
+	struct perf_sched sched = {
+		.tool = {
+			.sample		 = perf_sched__process_tracepoint_sample,
+			.comm		 = perf_event__process_comm,
+			.lost		 = perf_event__process_lost,
+			.fork		 = perf_event__process_task,
+			.ordered_samples = true,
+		},
+		.cmp_pid	      = LIST_HEAD_INIT(sched.cmp_pid),
+		.sort_list	      = LIST_HEAD_INIT(sched.sort_list),
+		.start_work_mutex     = PTHREAD_MUTEX_INITIALIZER,
+		.work_done_wait_mutex = PTHREAD_MUTEX_INITIALIZER,
+		.curr_pid	      = { [0 ... MAX_CPUS - 1] = -1 },
+		.sort_order	      = default_sort_order,
+		.replay_repeat	      = 10,
+		.profile_cpu	      = -1,
+		.next_shortname1      = 'A',
+		.next_shortname2      = '0',
+	};
+	const struct option latency_options[] = {
+	OPT_STRING('s', "sort", &sched.sort_order, "key[,key2...]",
+		   "sort by key(s): runtime, switch, avg, max"),
+	OPT_INCR('v', "verbose", &verbose,
+		    "be more verbose (show symbol address, etc)"),
+	OPT_INTEGER('C', "CPU", &sched.profile_cpu,
+		    "CPU to profile on"),
+	OPT_BOOLEAN('D', "dump-raw-trace", &dump_trace,
+		    "dump raw trace in ASCII"),
+	OPT_END()
+	};
+	const struct option replay_options[] = {
+	OPT_UINTEGER('r', "repeat", &sched.replay_repeat,
+		     "repeat the workload replay N times (-1: infinite)"),
+	OPT_INCR('v', "verbose", &verbose,
+		    "be more verbose (show symbol address, etc)"),
+	OPT_BOOLEAN('D', "dump-raw-trace", &dump_trace,
+		    "dump raw trace in ASCII"),
+	OPT_END()
+	};
+	const struct option sched_options[] = {
+	OPT_STRING('i', "input", &sched.input_name, "file",
+		    "input file name"),
+	OPT_INCR('v', "verbose", &verbose,
+		    "be more verbose (show symbol address, etc)"),
+	OPT_BOOLEAN('D', "dump-raw-trace", &dump_trace,
+		    "dump raw trace in ASCII"),
+	OPT_END()
+	};
+	const char * const latency_usage[] = {
+		"perf sched latency [<options>]",
+		NULL
+	};
+	const char * const replay_usage[] = {
+		"perf sched replay [<options>]",
+		NULL
+	};
+	const char * const sched_usage[] = {
+		"perf sched [<options>] {record|latency|map|replay|script}",
+		NULL
+	};
+	struct trace_sched_handler lat_ops  = {
+		.wakeup_event	    = latency_wakeup_event,
+		.switch_event	    = latency_switch_event,
+		.runtime_event	    = latency_runtime_event,
+		.fork_event	    = latency_fork_event,
+		.migrate_task_event = latency_migrate_task_event,
+	};
+	struct trace_sched_handler map_ops  = {
+		.switch_event	    = map_switch_event,
+	};
+	struct trace_sched_handler replay_ops  = {
+		.wakeup_event	    = replay_wakeup_event,
+		.switch_event	    = replay_switch_event,
+		.fork_event	    = replay_fork_event,
+	};
+
 	argc = parse_options(argc, argv, sched_options, sched_usage,
 			     PARSE_OPT_STOP_AT_NON_OPTION);
 	if (!argc)
@@ -1922,26 +1910,26 @@ int cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)
 	if (!strncmp(argv[0], "rec", 3)) {
 		return __cmd_record(argc, argv);
 	} else if (!strncmp(argv[0], "lat", 3)) {
-		trace_handler = &lat_ops;
+		sched.tp_handler = &lat_ops;
 		if (argc > 1) {
 			argc = parse_options(argc, argv, latency_options, latency_usage, 0);
 			if (argc)
 				usage_with_options(latency_usage, latency_options);
 		}
-		setup_sorting();
-		return __cmd_lat();
+		setup_sorting(&sched, latency_options, latency_usage);
+		return perf_sched__lat(&sched);
 	} else if (!strcmp(argv[0], "map")) {
-		trace_handler = &map_ops;
-		setup_sorting();
-		return __cmd_map();
+		sched.tp_handler = &map_ops;
+		setup_sorting(&sched, latency_options, latency_usage);
+		return perf_sched__map(&sched);
 	} else if (!strncmp(argv[0], "rep", 3)) {
-		trace_handler = &replay_ops;
+		sched.tp_handler = &replay_ops;
 		if (argc) {
 			argc = parse_options(argc, argv, replay_options, replay_usage, 0);
 			if (argc)
 				usage_with_options(replay_usage, replay_options);
 		}
-		return __cmd_replay();
+		return perf_sched__replay(&sched);
 	} else {
 		usage_with_options(sched_usage, sched_options);
 	}

commit 4218e6734197f3842fc9b6362f12973918d913aa
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Tue Sep 11 13:18:47 2012 -0300

    perf sched: Remove unused thread parameter
    
    From the tracepoint handling routines.
    
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Namhyung Kim <namhyung@gmail.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Link: http://lkml.kernel.org/n/tip-mcqd9mv34z6he0wqiz4a3mh9@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 82e8ec2c43b7..af11b1aa1bd7 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1372,8 +1372,7 @@ static struct trace_sched_handler *trace_handler;
 static int process_sched_wakeup_event(struct perf_tool *tool __maybe_unused,
 				      struct event_format *event,
 				      struct perf_sample *sample,
-				      struct machine *machine,
-				      struct thread *thread __maybe_unused)
+				      struct machine *machine)
 {
 	void *data = sample->raw_data;
 	struct trace_wakeup_event wakeup_event;
@@ -1489,8 +1488,7 @@ map_switch_event(struct trace_switch_event *switch_event,
 static int process_sched_switch_event(struct perf_tool *tool __maybe_unused,
 				      struct event_format *event,
 				      struct perf_sample *sample,
-				      struct machine *machine,
-				      struct thread *thread __maybe_unused)
+				      struct machine *machine)
 {
 	int this_cpu = sample->cpu, err = 0;
 	void *data = sample->raw_data;
@@ -1524,8 +1522,7 @@ static int process_sched_switch_event(struct perf_tool *tool __maybe_unused,
 static int process_sched_runtime_event(struct perf_tool *tool __maybe_unused,
 				       struct event_format *event,
 				       struct perf_sample *sample,
-				       struct machine *machine,
-				       struct thread *thread __maybe_unused)
+				       struct machine *machine)
 {
 	void *data = sample->raw_data;
 	struct trace_runtime_event runtime_event;
@@ -1545,8 +1542,7 @@ static int process_sched_runtime_event(struct perf_tool *tool __maybe_unused,
 static int process_sched_fork_event(struct perf_tool *tool __maybe_unused,
 				    struct event_format *event,
 				    struct perf_sample *sample,
-				    struct machine *machine __maybe_unused,
-				    struct thread *thread __maybe_unused)
+				    struct machine *machine __maybe_unused)
 {
 	void *data = sample->raw_data;
 	struct trace_fork_event fork_event;
@@ -1568,8 +1564,7 @@ static int process_sched_fork_event(struct perf_tool *tool __maybe_unused,
 static int process_sched_exit_event(struct perf_tool *tool __maybe_unused,
 				    struct event_format *event,
 				    struct perf_sample *sample __maybe_unused,
-				    struct machine *machine __maybe_unused,
-				    struct thread *thread __maybe_unused)
+				    struct machine *machine __maybe_unused)
 {
 	if (verbose)
 		printf("sched_exit event %p\n", event);
@@ -1580,8 +1575,7 @@ static int process_sched_exit_event(struct perf_tool *tool __maybe_unused,
 static int process_sched_migrate_task_event(struct perf_tool *tool __maybe_unused,
 					    struct event_format *event,
 					    struct perf_sample *sample,
-					    struct machine *machine,
-					    struct thread *thread __maybe_unused)
+					    struct machine *machine)
 {
 	void *data = sample->raw_data;
 	struct trace_migrate_task_event migrate_task_event;
@@ -1603,8 +1597,7 @@ static int process_sched_migrate_task_event(struct perf_tool *tool __maybe_unuse
 typedef int (*tracepoint_handler)(struct perf_tool *tool,
 				  struct event_format *tp_format,
 				  struct perf_sample *sample,
-				  struct machine *machine,
-				  struct thread *thread);
+				  struct machine *machine);
 
 static int perf_sched__process_tracepoint_sample(struct perf_tool *tool __maybe_unused,
 						 union perf_event *event __maybe_unused,
@@ -1626,7 +1619,7 @@ static int perf_sched__process_tracepoint_sample(struct perf_tool *tool __maybe_
 
 	if (evsel->handler.func != NULL) {
 		tracepoint_handler f = evsel->handler.func;
-		err = f(tool, evsel->tp_format, sample, machine, thread);
+		err = f(tool, evsel->tp_format, sample, machine);
 	}
 
 	return err;

commit 1d037ca1648b775277fc96401ec2aa233724906c
Author: Irina Tirdea <irina.tirdea@gmail.com>
Date:   Tue Sep 11 01:15:03 2012 +0300

    perf tools: Use __maybe_used for unused variables
    
    perf defines both __used and __unused variables to use for marking
    unused variables. The variable __used is defined to
    __attribute__((__unused__)), which contradicts the kernel definition to
    __attribute__((__used__)) for new gcc versions. On Android, __used is
    also defined in system headers and this leads to warnings like: warning:
    '__used__' attribute ignored
    
    __unused is not defined in the kernel and is not a standard definition.
    If __unused is included everywhere instead of __used, this leads to
    conflicts with glibc headers, since glibc has a variables with this name
    in its headers.
    
    The best approach is to use __maybe_unused, the definition used in the
    kernel for __attribute__((unused)). In this way there is only one
    definition in perf sources (instead of 2 definitions that point to the
    same thing: __used and __unused) and it works on both Linux and Android.
    This patch simply replaces all instances of __used and __unused with
    __maybe_unused.
    
    Signed-off-by: Irina Tirdea <irina.tirdea@intel.com>
    Acked-by: Pekka Enberg <penberg@kernel.org>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Namhyung Kim <namhyung.kim@lge.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/1347315303-29906-7-git-send-email-irina.tirdea@intel.com
    [ committer note: fixed up conflict with a116e05 in builtin-sched.c ]
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 782f66d3610e..82e8ec2c43b7 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -299,7 +299,7 @@ add_sched_event_wakeup(struct task_desc *task, u64 timestamp,
 
 static void
 add_sched_event_sleep(struct task_desc *task, u64 timestamp,
-		      u64 task_state __used)
+		      u64 task_state __maybe_unused)
 {
 	struct sched_atom *event = get_new_event(task, timestamp);
 
@@ -369,8 +369,8 @@ static void add_cross_task_wakeups(void)
 	}
 }
 
-static void
-process_sched_event(struct task_desc *this_task __used, struct sched_atom *atom)
+static void process_sched_event(struct task_desc *this_task __maybe_unused,
+				struct sched_atom *atom)
 {
 	int ret = 0;
 
@@ -752,7 +752,7 @@ struct trace_sched_handler {
 
 static int
 replay_wakeup_event(struct trace_wakeup_event *wakeup_event,
-		    struct machine *machine __used,
+		    struct machine *machine __maybe_unused,
 		    struct event_format *event, struct perf_sample *sample)
 {
 	struct task_desc *waker, *wakee;
@@ -777,11 +777,11 @@ static u64 cpu_last_switched[MAX_CPUS];
 
 static int
 replay_switch_event(struct trace_switch_event *switch_event,
-		    struct machine *machine __used,
+		    struct machine *machine __maybe_unused,
 		    struct event_format *event,
 		    struct perf_sample *sample)
 {
-	struct task_desc *prev, __used *next;
+	struct task_desc *prev, __maybe_unused *next;
 	u64 timestamp0, timestamp = sample->time;
 	int cpu = sample->cpu;
 	s64 delta;
@@ -932,15 +932,13 @@ static int thread_atoms_insert(struct thread *thread)
 	return 0;
 }
 
-static int
-latency_fork_event(struct trace_fork_event *fork_event __used,
-		   struct event_format *event __used)
+static int latency_fork_event(struct trace_fork_event *fork_event __maybe_unused,
+			      struct event_format *event __maybe_unused)
 {
 	/* should insert the newcomer */
 	return 0;
 }
 
-__used
 static char sched_out_state(struct trace_switch_event *switch_event)
 {
 	const char *str = TASK_STATE_TO_CHAR_STR;
@@ -971,7 +969,8 @@ add_sched_out_event(struct work_atoms *atoms,
 }
 
 static void
-add_runtime_event(struct work_atoms *atoms, u64 delta, u64 timestamp __used)
+add_runtime_event(struct work_atoms *atoms, u64 delta,
+		  u64 timestamp __maybe_unused)
 {
 	struct work_atom *atom;
 
@@ -1017,7 +1016,7 @@ add_sched_in_event(struct work_atoms *atoms, u64 timestamp)
 static int
 latency_switch_event(struct trace_switch_event *switch_event,
 		     struct machine *machine,
-		     struct event_format *event __used,
+		     struct event_format *event __maybe_unused,
 		     struct perf_sample *sample)
 {
 	struct work_atoms *out_events, *in_events;
@@ -1105,7 +1104,8 @@ latency_runtime_event(struct trace_runtime_event *runtime_event,
 
 static int
 latency_wakeup_event(struct trace_wakeup_event *wakeup_event,
-		     struct machine *machine, struct event_format *event __used,
+		     struct machine *machine,
+		     struct event_format *event __maybe_unused,
 		     struct perf_sample *sample)
 {
 	struct work_atoms *atoms;
@@ -1369,12 +1369,11 @@ static void sort_lat(void)
 
 static struct trace_sched_handler *trace_handler;
 
-static int
-process_sched_wakeup_event(struct perf_tool *tool __used,
-			   struct event_format *event,
-			   struct perf_sample *sample,
-			   struct machine *machine,
-			   struct thread *thread __used)
+static int process_sched_wakeup_event(struct perf_tool *tool __maybe_unused,
+				      struct event_format *event,
+				      struct perf_sample *sample,
+				      struct machine *machine,
+				      struct thread *thread __maybe_unused)
 {
 	void *data = sample->raw_data;
 	struct trace_wakeup_event wakeup_event;
@@ -1410,10 +1409,10 @@ static char next_shortname2 = '0';
 static int
 map_switch_event(struct trace_switch_event *switch_event,
 		 struct machine *machine,
-		 struct event_format *event __used,
+		 struct event_format *event __maybe_unused,
 		 struct perf_sample *sample)
 {
-	struct thread *sched_out __used, *sched_in;
+	struct thread *sched_out __maybe_unused, *sched_in;
 	int new_shortname;
 	u64 timestamp0, timestamp = sample->time;
 	s64 delta;
@@ -1487,12 +1486,11 @@ map_switch_event(struct trace_switch_event *switch_event,
 	return 0;
 }
 
-static int
-process_sched_switch_event(struct perf_tool *tool __used,
-			   struct event_format *event,
-			   struct perf_sample *sample,
-			   struct machine *machine,
-			   struct thread *thread __used)
+static int process_sched_switch_event(struct perf_tool *tool __maybe_unused,
+				      struct event_format *event,
+				      struct perf_sample *sample,
+				      struct machine *machine,
+				      struct thread *thread __maybe_unused)
 {
 	int this_cpu = sample->cpu, err = 0;
 	void *data = sample->raw_data;
@@ -1523,12 +1521,11 @@ process_sched_switch_event(struct perf_tool *tool __used,
 	return err;
 }
 
-static int
-process_sched_runtime_event(struct perf_tool *tool __used,
-			    struct event_format *event,
-			    struct perf_sample *sample,
-			    struct machine *machine,
-			    struct thread *thread __used)
+static int process_sched_runtime_event(struct perf_tool *tool __maybe_unused,
+				       struct event_format *event,
+				       struct perf_sample *sample,
+				       struct machine *machine,
+				       struct thread *thread __maybe_unused)
 {
 	void *data = sample->raw_data;
 	struct trace_runtime_event runtime_event;
@@ -1545,12 +1542,11 @@ process_sched_runtime_event(struct perf_tool *tool __used,
 	return err;
 }
 
-static int
-process_sched_fork_event(struct perf_tool *tool __used,
-			 struct event_format *event,
-			 struct perf_sample *sample,
-			 struct machine *machine __used,
-			 struct thread *thread __used)
+static int process_sched_fork_event(struct perf_tool *tool __maybe_unused,
+				    struct event_format *event,
+				    struct perf_sample *sample,
+				    struct machine *machine __maybe_unused,
+				    struct thread *thread __maybe_unused)
 {
 	void *data = sample->raw_data;
 	struct trace_fork_event fork_event;
@@ -1569,12 +1565,11 @@ process_sched_fork_event(struct perf_tool *tool __used,
 	return err;
 }
 
-static int
-process_sched_exit_event(struct perf_tool *tool __used,
-			 struct event_format *event,
-			 struct perf_sample *sample __used,
-			 struct machine *machine __used,
-			 struct thread *thread __used)
+static int process_sched_exit_event(struct perf_tool *tool __maybe_unused,
+				    struct event_format *event,
+				    struct perf_sample *sample __maybe_unused,
+				    struct machine *machine __maybe_unused,
+				    struct thread *thread __maybe_unused)
 {
 	if (verbose)
 		printf("sched_exit event %p\n", event);
@@ -1582,12 +1577,11 @@ process_sched_exit_event(struct perf_tool *tool __used,
 	return 0;
 }
 
-static int
-process_sched_migrate_task_event(struct perf_tool *tool __used,
-				 struct event_format *event,
-				 struct perf_sample *sample,
-				 struct machine *machine,
-				 struct thread *thread __used)
+static int process_sched_migrate_task_event(struct perf_tool *tool __maybe_unused,
+					    struct event_format *event,
+					    struct perf_sample *sample,
+					    struct machine *machine,
+					    struct thread *thread __maybe_unused)
 {
 	void *data = sample->raw_data;
 	struct trace_migrate_task_event migrate_task_event;
@@ -1612,8 +1606,8 @@ typedef int (*tracepoint_handler)(struct perf_tool *tool,
 				  struct machine *machine,
 				  struct thread *thread);
 
-static int perf_sched__process_tracepoint_sample(struct perf_tool *tool __used,
-						 union perf_event *event __used,
+static int perf_sched__process_tracepoint_sample(struct perf_tool *tool __maybe_unused,
+						 union perf_event *event __maybe_unused,
 						 struct perf_sample *sample,
 						 struct perf_evsel *evsel,
 						 struct machine *machine)
@@ -1918,7 +1912,7 @@ static int __cmd_record(int argc, const char **argv)
 	return cmd_record(i, rec_argv, NULL);
 }
 
-int cmd_sched(int argc, const char **argv, const char *prefix __used)
+int cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)
 {
 	argc = parse_options(argc, argv, sched_options, sched_usage,
 			     PARSE_OPT_STOP_AT_NON_OPTION);

commit a116e05dcf61c8d758e0f0aed40325534aee2c13
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Sat Sep 8 22:53:06 2012 -0300

    perf sched: Remove die() calls
    
    Just use pr_err() + return -1 and perf_session__process_events to abort
    when some event would call die(), then let the perf's main() exit doing
    whatever it needs.
    
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Namhyung Kim <namhyung@gmail.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Link: http://lkml.kernel.org/n/tip-88cwdogxqomsy9tfr8r0as58@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index a25a023965bb..782f66d3610e 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -423,8 +423,8 @@ static int self_open_counters(void)
 	fd = sys_perf_event_open(&attr, 0, -1, -1, 0);
 
 	if (fd < 0)
-		die("Error: sys_perf_event_open() syscall returned"
-		    "with %d (%s)\n", fd, strerror(errno));
+		pr_debug("Error: sys_perf_event_open() syscall returned"
+			 "with %d (%s)\n", fd, strerror(errno));
 	return fd;
 }
 
@@ -450,7 +450,8 @@ static void *thread_func(void *ctx)
 	sprintf(comm2, ":%s", this_task->comm);
 	prctl(PR_SET_NAME, comm2);
 	fd = self_open_counters();
-
+	if (fd < 0)
+		return NULL;
 again:
 	ret = sem_post(&this_task->ready_for_work);
 	BUG_ON(ret);
@@ -726,30 +727,30 @@ struct trace_migrate_task_event {
 };
 
 struct trace_sched_handler {
-	void (*switch_event)(struct trace_switch_event *,
-			     struct machine *,
-			     struct event_format *,
-			     struct perf_sample *sample);
-
-	void (*runtime_event)(struct trace_runtime_event *,
-			      struct machine *,
-			      struct perf_sample *sample);
+	int (*switch_event)(struct trace_switch_event *event,
+			    struct machine *machine,
+			    struct event_format *tp_format,
+			    struct perf_sample *sample);
 
-	void (*wakeup_event)(struct trace_wakeup_event *,
-			     struct machine *,
-			     struct event_format *,
+	int (*runtime_event)(struct trace_runtime_event *event,
+			     struct machine *machine,
 			     struct perf_sample *sample);
 
-	void (*fork_event)(struct trace_fork_event *,
-			   struct event_format *event);
+	int (*wakeup_event)(struct trace_wakeup_event *event,
+			    struct machine *machine,
+			    struct event_format *tp_format,
+			    struct perf_sample *sample);
 
-	void (*migrate_task_event)(struct trace_migrate_task_event *,
-				   struct machine *machine,
-				   struct perf_sample *sample);
+	int (*fork_event)(struct trace_fork_event *event,
+			  struct event_format *tp_format);
+
+	int (*migrate_task_event)(struct trace_migrate_task_event *event,
+				  struct machine *machine,
+				  struct perf_sample *sample);
 };
 
 
-static void
+static int
 replay_wakeup_event(struct trace_wakeup_event *wakeup_event,
 		    struct machine *machine __used,
 		    struct event_format *event, struct perf_sample *sample)
@@ -769,11 +770,12 @@ replay_wakeup_event(struct trace_wakeup_event *wakeup_event,
 	wakee = register_pid(wakeup_event->pid, wakeup_event->comm);
 
 	add_sched_event_wakeup(waker, sample->time, wakee);
+	return 0;
 }
 
 static u64 cpu_last_switched[MAX_CPUS];
 
-static void
+static int
 replay_switch_event(struct trace_switch_event *switch_event,
 		    struct machine *machine __used,
 		    struct event_format *event,
@@ -788,7 +790,7 @@ replay_switch_event(struct trace_switch_event *switch_event,
 		printf("sched_switch event %p\n", event);
 
 	if (cpu >= MAX_CPUS || cpu < 0)
-		return;
+		return 0;
 
 	timestamp0 = cpu_last_switched[cpu];
 	if (timestamp0)
@@ -796,8 +798,10 @@ replay_switch_event(struct trace_switch_event *switch_event,
 	else
 		delta = 0;
 
-	if (delta < 0)
-		die("hm, delta: %" PRIu64 " < 0 ?\n", delta);
+	if (delta < 0) {
+		pr_debug("hm, delta: %" PRIu64 " < 0 ?\n", delta);
+		return -1;
+	}
 
 	if (verbose) {
 		printf(" ... switch from %s/%d to %s/%d [ran %" PRIu64 " nsecs]\n",
@@ -813,10 +817,12 @@ replay_switch_event(struct trace_switch_event *switch_event,
 
 	add_sched_event_run(prev, timestamp, delta);
 	add_sched_event_sleep(prev, timestamp, switch_event->prev_state);
+
+	return 0;
 }
 
 
-static void
+static int
 replay_fork_event(struct trace_fork_event *fork_event,
 		  struct event_format *event)
 {
@@ -827,6 +833,7 @@ replay_fork_event(struct trace_fork_event *fork_event,
 	}
 	register_pid(fork_event->parent_pid, fork_event->parent_comm);
 	register_pid(fork_event->child_pid, fork_event->child_comm);
+	return 0;
 }
 
 static struct trace_sched_handler replay_ops  = {
@@ -911,22 +918,26 @@ __thread_latency_insert(struct rb_root *root, struct work_atoms *data,
 	rb_insert_color(&data->node, root);
 }
 
-static void thread_atoms_insert(struct thread *thread)
+static int thread_atoms_insert(struct thread *thread)
 {
 	struct work_atoms *atoms = zalloc(sizeof(*atoms));
-	if (!atoms)
-		die("No memory");
+	if (!atoms) {
+		pr_err("No memory at %s\n", __func__);
+		return -1;
+	}
 
 	atoms->thread = thread;
 	INIT_LIST_HEAD(&atoms->work_list);
 	__thread_latency_insert(&atom_root, atoms, &cmp_pid);
+	return 0;
 }
 
-static void
+static int
 latency_fork_event(struct trace_fork_event *fork_event __used,
 		   struct event_format *event __used)
 {
 	/* should insert the newcomer */
+	return 0;
 }
 
 __used
@@ -937,14 +948,16 @@ static char sched_out_state(struct trace_switch_event *switch_event)
 	return str[switch_event->prev_state];
 }
 
-static void
+static int
 add_sched_out_event(struct work_atoms *atoms,
 		    char run_state,
 		    u64 timestamp)
 {
 	struct work_atom *atom = zalloc(sizeof(*atom));
-	if (!atom)
-		die("Non memory");
+	if (!atom) {
+		pr_err("Non memory at %s", __func__);
+		return -1;
+	}
 
 	atom->sched_out_time = timestamp;
 
@@ -954,6 +967,7 @@ add_sched_out_event(struct work_atoms *atoms,
 	}
 
 	list_add_tail(&atom->list, &atoms->work_list);
+	return 0;
 }
 
 static void
@@ -1000,7 +1014,7 @@ add_sched_in_event(struct work_atoms *atoms, u64 timestamp)
 	atoms->nb_atoms++;
 }
 
-static void
+static int
 latency_switch_event(struct trace_switch_event *switch_event,
 		     struct machine *machine,
 		     struct event_format *event __used,
@@ -1021,38 +1035,49 @@ latency_switch_event(struct trace_switch_event *switch_event,
 	else
 		delta = 0;
 
-	if (delta < 0)
-		die("hm, delta: %" PRIu64 " < 0 ?\n", delta);
-
+	if (delta < 0) {
+		pr_err("hm, delta: %" PRIu64 " < 0 ?\n", delta);
+		return -1;
+	}
 
 	sched_out = machine__findnew_thread(machine, switch_event->prev_pid);
 	sched_in = machine__findnew_thread(machine, switch_event->next_pid);
 
 	out_events = thread_atoms_search(&atom_root, sched_out, &cmp_pid);
 	if (!out_events) {
-		thread_atoms_insert(sched_out);
+		if (thread_atoms_insert(sched_out))
+			return -1;
 		out_events = thread_atoms_search(&atom_root, sched_out, &cmp_pid);
-		if (!out_events)
-			die("out-event: Internal tree error");
+		if (!out_events) {
+			pr_err("out-event: Internal tree error");
+			return -1;
+		}
 	}
-	add_sched_out_event(out_events, sched_out_state(switch_event), timestamp);
+	if (add_sched_out_event(out_events, sched_out_state(switch_event), timestamp))
+		return -1;
 
 	in_events = thread_atoms_search(&atom_root, sched_in, &cmp_pid);
 	if (!in_events) {
-		thread_atoms_insert(sched_in);
+		if (thread_atoms_insert(sched_in))
+			return -1;
 		in_events = thread_atoms_search(&atom_root, sched_in, &cmp_pid);
-		if (!in_events)
-			die("in-event: Internal tree error");
+		if (!in_events) {
+			pr_err("in-event: Internal tree error");
+			return -1;
+		}
 		/*
 		 * Take came in we have not heard about yet,
 		 * add in an initial atom in runnable state:
 		 */
-		add_sched_out_event(in_events, 'R', timestamp);
+		if (add_sched_out_event(in_events, 'R', timestamp))
+			return -1;
 	}
 	add_sched_in_event(in_events, timestamp);
+
+	return 0;
 }
 
-static void
+static int
 latency_runtime_event(struct trace_runtime_event *runtime_event,
 		      struct machine *machine, struct perf_sample *sample)
 {
@@ -1063,17 +1088,22 @@ latency_runtime_event(struct trace_runtime_event *runtime_event,
 
 	BUG_ON(cpu >= MAX_CPUS || cpu < 0);
 	if (!atoms) {
-		thread_atoms_insert(thread);
+		if (thread_atoms_insert(thread))
+			return -1;
 		atoms = thread_atoms_search(&atom_root, thread, &cmp_pid);
-		if (!atoms)
-			die("in-event: Internal tree error");
-		add_sched_out_event(atoms, 'R', timestamp);
+		if (!atoms) {
+			pr_debug("in-event: Internal tree error");
+			return -1;
+		}
+		if (add_sched_out_event(atoms, 'R', timestamp))
+			return -1;
 	}
 
 	add_runtime_event(atoms, runtime_event->runtime, timestamp);
+	return 0;
 }
 
-static void
+static int
 latency_wakeup_event(struct trace_wakeup_event *wakeup_event,
 		     struct machine *machine, struct event_format *event __used,
 		     struct perf_sample *sample)
@@ -1085,16 +1115,20 @@ latency_wakeup_event(struct trace_wakeup_event *wakeup_event,
 
 	/* Note for later, it may be interesting to observe the failing cases */
 	if (!wakeup_event->success)
-		return;
+		return 0;
 
 	wakee = machine__findnew_thread(machine, wakeup_event->pid);
 	atoms = thread_atoms_search(&atom_root, wakee, &cmp_pid);
 	if (!atoms) {
-		thread_atoms_insert(wakee);
+		if (thread_atoms_insert(wakee))
+			return -1;
 		atoms = thread_atoms_search(&atom_root, wakee, &cmp_pid);
-		if (!atoms)
-			die("wakeup-event: Internal tree error");
-		add_sched_out_event(atoms, 'S', timestamp);
+		if (!atoms) {
+			pr_debug("wakeup-event: Internal tree error");
+			return -1;
+		}
+		if (add_sched_out_event(atoms, 'S', timestamp))
+			return -1;
 	}
 
 	BUG_ON(list_empty(&atoms->work_list));
@@ -1112,14 +1146,15 @@ latency_wakeup_event(struct trace_wakeup_event *wakeup_event,
 	nr_timestamps++;
 	if (atom->sched_out_time > timestamp) {
 		nr_unordered_timestamps++;
-		return;
+		return 0;
 	}
 
 	atom->state = THREAD_WAIT_CPU;
 	atom->wake_up_time = timestamp;
+	return 0;
 }
 
-static void
+static int
 latency_migrate_task_event(struct trace_migrate_task_event *migrate_task_event,
 			   struct machine *machine, struct perf_sample *sample)
 {
@@ -1132,17 +1167,21 @@ latency_migrate_task_event(struct trace_migrate_task_event *migrate_task_event,
 	 * Only need to worry about migration when profiling one CPU.
 	 */
 	if (profile_cpu == -1)
-		return;
+		return 0;
 
 	migrant = machine__findnew_thread(machine, migrate_task_event->pid);
 	atoms = thread_atoms_search(&atom_root, migrant, &cmp_pid);
 	if (!atoms) {
-		thread_atoms_insert(migrant);
+		if (thread_atoms_insert(migrant))
+			return -1;
 		register_pid(migrant->pid, migrant->comm);
 		atoms = thread_atoms_search(&atom_root, migrant, &cmp_pid);
-		if (!atoms)
-			die("migration-event: Internal tree error");
-		add_sched_out_event(atoms, 'R', timestamp);
+		if (!atoms) {
+			pr_debug("migration-event: Internal tree error");
+			return -1;
+		}
+		if (add_sched_out_event(atoms, 'R', timestamp))
+			return -1;
 	}
 
 	BUG_ON(list_empty(&atoms->work_list));
@@ -1154,6 +1193,8 @@ latency_migrate_task_event(struct trace_migrate_task_event *migrate_task_event,
 
 	if (atom->sched_out_time > timestamp)
 		nr_unordered_timestamps++;
+
+	return 0;
 }
 
 static struct trace_sched_handler lat_ops  = {
@@ -1328,7 +1369,7 @@ static void sort_lat(void)
 
 static struct trace_sched_handler *trace_handler;
 
-static void
+static int
 process_sched_wakeup_event(struct perf_tool *tool __used,
 			   struct event_format *event,
 			   struct perf_sample *sample,
@@ -1337,6 +1378,7 @@ process_sched_wakeup_event(struct perf_tool *tool __used,
 {
 	void *data = sample->raw_data;
 	struct trace_wakeup_event wakeup_event;
+	int err = 0;
 
 	FILL_COMMON_FIELDS(wakeup_event, event, data);
 
@@ -1347,7 +1389,9 @@ process_sched_wakeup_event(struct perf_tool *tool __used,
 	FILL_FIELD(wakeup_event, cpu, event, data);
 
 	if (trace_handler->wakeup_event)
-		trace_handler->wakeup_event(&wakeup_event, machine, event, sample);
+		err = trace_handler->wakeup_event(&wakeup_event, machine, event, sample);
+
+	return err;
 }
 
 /*
@@ -1363,7 +1407,7 @@ static struct thread *curr_thread[MAX_CPUS];
 static char next_shortname1 = 'A';
 static char next_shortname2 = '0';
 
-static void
+static int
 map_switch_event(struct trace_switch_event *switch_event,
 		 struct machine *machine,
 		 struct event_format *event __used,
@@ -1387,9 +1431,10 @@ map_switch_event(struct trace_switch_event *switch_event,
 	else
 		delta = 0;
 
-	if (delta < 0)
-		die("hm, delta: %" PRIu64 " < 0 ?\n", delta);
-
+	if (delta < 0) {
+		pr_debug("hm, delta: %" PRIu64 " < 0 ?\n", delta);
+		return -1;
+	}
 
 	sched_out = machine__findnew_thread(machine, switch_event->prev_pid);
 	sched_in = machine__findnew_thread(machine, switch_event->next_pid);
@@ -1438,16 +1483,18 @@ map_switch_event(struct trace_switch_event *switch_event,
 	} else {
 		printf("\n");
 	}
+
+	return 0;
 }
 
-static void
+static int
 process_sched_switch_event(struct perf_tool *tool __used,
 			   struct event_format *event,
 			   struct perf_sample *sample,
 			   struct machine *machine,
 			   struct thread *thread __used)
 {
-	int this_cpu = sample->cpu;
+	int this_cpu = sample->cpu, err = 0;
 	void *data = sample->raw_data;
 	struct trace_switch_event switch_event;
 
@@ -1470,12 +1517,13 @@ process_sched_switch_event(struct perf_tool *tool __used,
 			nr_context_switch_bugs++;
 	}
 	if (trace_handler->switch_event)
-		trace_handler->switch_event(&switch_event, machine, event, sample);
+		err = trace_handler->switch_event(&switch_event, machine, event, sample);
 
 	curr_pid[this_cpu] = switch_event.next_pid;
+	return err;
 }
 
-static void
+static int
 process_sched_runtime_event(struct perf_tool *tool __used,
 			    struct event_format *event,
 			    struct perf_sample *sample,
@@ -1484,6 +1532,7 @@ process_sched_runtime_event(struct perf_tool *tool __used,
 {
 	void *data = sample->raw_data;
 	struct trace_runtime_event runtime_event;
+	int err = 0;
 
 	FILL_ARRAY(runtime_event, comm, event, data);
 	FILL_FIELD(runtime_event, pid, event, data);
@@ -1491,10 +1540,12 @@ process_sched_runtime_event(struct perf_tool *tool __used,
 	FILL_FIELD(runtime_event, vruntime, event, data);
 
 	if (trace_handler->runtime_event)
-		trace_handler->runtime_event(&runtime_event, machine, sample);
+		err = trace_handler->runtime_event(&runtime_event, machine, sample);
+
+	return err;
 }
 
-static void
+static int
 process_sched_fork_event(struct perf_tool *tool __used,
 			 struct event_format *event,
 			 struct perf_sample *sample,
@@ -1503,6 +1554,7 @@ process_sched_fork_event(struct perf_tool *tool __used,
 {
 	void *data = sample->raw_data;
 	struct trace_fork_event fork_event;
+	int err = 0;
 
 	FILL_COMMON_FIELDS(fork_event, event, data);
 
@@ -1512,10 +1564,12 @@ process_sched_fork_event(struct perf_tool *tool __used,
 	FILL_FIELD(fork_event, child_pid, event, data);
 
 	if (trace_handler->fork_event)
-		trace_handler->fork_event(&fork_event, event);
+		err = trace_handler->fork_event(&fork_event, event);
+
+	return err;
 }
 
-static void
+static int
 process_sched_exit_event(struct perf_tool *tool __used,
 			 struct event_format *event,
 			 struct perf_sample *sample __used,
@@ -1524,9 +1578,11 @@ process_sched_exit_event(struct perf_tool *tool __used,
 {
 	if (verbose)
 		printf("sched_exit event %p\n", event);
+
+	return 0;
 }
 
-static void
+static int
 process_sched_migrate_task_event(struct perf_tool *tool __used,
 				 struct event_format *event,
 				 struct perf_sample *sample,
@@ -1535,6 +1591,7 @@ process_sched_migrate_task_event(struct perf_tool *tool __used,
 {
 	void *data = sample->raw_data;
 	struct trace_migrate_task_event migrate_task_event;
+	int err = 0;
 
 	FILL_COMMON_FIELDS(migrate_task_event, event, data);
 
@@ -1544,13 +1601,16 @@ process_sched_migrate_task_event(struct perf_tool *tool __used,
 	FILL_FIELD(migrate_task_event, cpu, event, data);
 
 	if (trace_handler->migrate_task_event)
-		trace_handler->migrate_task_event(&migrate_task_event, machine, sample);
+		err = trace_handler->migrate_task_event(&migrate_task_event, machine, sample);
+
+	return err;
 }
 
-typedef void (*tracepoint_handler)(struct perf_tool *tool, struct event_format *event,
-				   struct perf_sample *sample,
-				   struct machine *machine,
-				   struct thread *thread);
+typedef int (*tracepoint_handler)(struct perf_tool *tool,
+				  struct event_format *tp_format,
+				  struct perf_sample *sample,
+				  struct machine *machine,
+				  struct thread *thread);
 
 static int perf_sched__process_tracepoint_sample(struct perf_tool *tool __used,
 						 union perf_event *event __used,
@@ -1559,6 +1619,7 @@ static int perf_sched__process_tracepoint_sample(struct perf_tool *tool __used,
 						 struct machine *machine)
 {
 	struct thread *thread = machine__findnew_thread(machine, sample->pid);
+	int err = 0;
 
 	if (thread == NULL) {
 		pr_debug("problem processing %s event, skipping it.\n",
@@ -1571,10 +1632,10 @@ static int perf_sched__process_tracepoint_sample(struct perf_tool *tool __used,
 
 	if (evsel->handler.func != NULL) {
 		tracepoint_handler f = evsel->handler.func;
-		f(tool, evsel->tp_format, sample, machine, thread);
+		err = f(tool, evsel->tp_format, sample, machine, thread);
 	}
 
-	return 0;
+	return err;
 }
 
 static struct perf_tool perf_sched = {
@@ -1585,9 +1646,8 @@ static struct perf_tool perf_sched = {
 	.ordered_samples = true,
 };
 
-static void read_events(bool destroy, struct perf_session **psession)
+static int read_events(bool destroy, struct perf_session **psession)
 {
-	int err = -EINVAL;
 	const struct perf_evsel_str_handler handlers[] = {
 		{ "sched:sched_switch",	      process_sched_switch_event, },
 		{ "sched:sched_stat_runtime", process_sched_runtime_event, },
@@ -1600,16 +1660,20 @@ static void read_events(bool destroy, struct perf_session **psession)
 	struct perf_session *session;
 
 	session = perf_session__new(input_name, O_RDONLY, 0, false, &perf_sched);
-	if (session == NULL)
-		die("No Memory");
+	if (session == NULL) {
+		pr_debug("No Memory for session\n");
+		return -1;
+	}
 
-	err = perf_session__set_tracepoints_handlers(session, handlers);
-	assert(err == 0);
+	if (perf_session__set_tracepoints_handlers(session, handlers))
+		goto out_delete;
 
 	if (perf_session__has_traces(session, "record -R")) {
-		err = perf_session__process_events(session, &perf_sched);
-		if (err)
-			die("Failed to process events, error %d", err);
+		int err = perf_session__process_events(session, &perf_sched);
+		if (err) {
+			pr_err("Failed to process events, error %d", err);
+			goto out_delete;
+		}
 
 		nr_events      = session->hists.stats.nr_events[0];
 		nr_lost_events = session->hists.stats.total_lost;
@@ -1621,6 +1685,12 @@ static void read_events(bool destroy, struct perf_session **psession)
 
 	if (psession)
 		*psession = session;
+
+	return 0;
+
+out_delete:
+	perf_session__delete(session);
+	return -1;
 }
 
 static void print_bad_events(void)
@@ -1653,13 +1723,14 @@ static void print_bad_events(void)
 	}
 }
 
-static void __cmd_lat(void)
+static int __cmd_lat(void)
 {
 	struct rb_node *next;
 	struct perf_session *session;
 
 	setup_pager();
-	read_events(false, &session);
+	if (read_events(false, &session))
+		return -1;
 	sort_lat();
 
 	printf("\n ---------------------------------------------------------------------------------------------------------------\n");
@@ -1686,6 +1757,7 @@ static void __cmd_lat(void)
 	printf("\n");
 
 	perf_session__delete(session);
+	return 0;
 }
 
 static struct trace_sched_handler map_ops  = {
@@ -1695,16 +1767,18 @@ static struct trace_sched_handler map_ops  = {
 	.fork_event		= NULL,
 };
 
-static void __cmd_map(void)
+static int __cmd_map(void)
 {
 	max_cpu = sysconf(_SC_NPROCESSORS_CONF);
 
 	setup_pager();
-	read_events(true, NULL);
+	if (read_events(true, NULL))
+		return -1;
 	print_bad_events();
+	return 0;
 }
 
-static void __cmd_replay(void)
+static int __cmd_replay(void)
 {
 	unsigned long i;
 
@@ -1713,7 +1787,8 @@ static void __cmd_replay(void)
 
 	test_calibrations();
 
-	read_events(true, NULL);
+	if (read_events(true, NULL))
+		return -1;
 
 	printf("nr_run_events:        %ld\n", nr_run_events);
 	printf("nr_sleep_events:      %ld\n", nr_sleep_events);
@@ -1734,6 +1809,8 @@ static void __cmd_replay(void)
 	printf("------------------------------------------------------------\n");
 	for (i = 0; i < replay_repeat; i++)
 		run_one_test();
+
+	return 0;
 }
 
 
@@ -1865,11 +1942,11 @@ int cmd_sched(int argc, const char **argv, const char *prefix __used)
 				usage_with_options(latency_usage, latency_options);
 		}
 		setup_sorting();
-		__cmd_lat();
+		return __cmd_lat();
 	} else if (!strcmp(argv[0], "map")) {
 		trace_handler = &map_ops;
 		setup_sorting();
-		__cmd_map();
+		return __cmd_map();
 	} else if (!strncmp(argv[0], "rep", 3)) {
 		trace_handler = &replay_ops;
 		if (argc) {
@@ -1877,7 +1954,7 @@ int cmd_sched(int argc, const char **argv, const char *prefix __used)
 			if (argc)
 				usage_with_options(replay_usage, replay_options);
 		}
-		__cmd_replay();
+		return __cmd_replay();
 	} else {
 		usage_with_options(sched_usage, sched_options);
 	}

commit 7f7f8d0bea5d6bb985f4ae84ca3daff34802fd32
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Tue Aug 7 11:33:42 2012 -0300

    perf sched: Use perf_sample
    
    To reduce the number of parameters passed to the various event handling
    functions.
    
    Cc: Andrey Wagin <avagin@gmail.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Namhyung Kim <namhyung@gmail.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Link: http://lkml.kernel.org/n/tip-fc537qykjjqzvyol5fecx6ug@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 30ef82aca885..a25a023965bb 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -729,46 +729,30 @@ struct trace_sched_handler {
 	void (*switch_event)(struct trace_switch_event *,
 			     struct machine *,
 			     struct event_format *,
-			     int cpu,
-			     u64 timestamp,
-			     struct thread *thread);
+			     struct perf_sample *sample);
 
 	void (*runtime_event)(struct trace_runtime_event *,
 			      struct machine *,
-			      struct event_format *,
-			      int cpu,
-			      u64 timestamp,
-			      struct thread *thread);
+			      struct perf_sample *sample);
 
 	void (*wakeup_event)(struct trace_wakeup_event *,
 			     struct machine *,
 			     struct event_format *,
-			     int cpu,
-			     u64 timestamp,
-			     struct thread *thread);
+			     struct perf_sample *sample);
 
 	void (*fork_event)(struct trace_fork_event *,
-			   struct event_format *,
-			   int cpu,
-			   u64 timestamp,
-			   struct thread *thread);
+			   struct event_format *event);
 
 	void (*migrate_task_event)(struct trace_migrate_task_event *,
-			   struct machine *machine,
-			   struct event_format *,
-			   int cpu,
-			   u64 timestamp,
-			   struct thread *thread);
+				   struct machine *machine,
+				   struct perf_sample *sample);
 };
 
 
 static void
 replay_wakeup_event(struct trace_wakeup_event *wakeup_event,
 		    struct machine *machine __used,
-		    struct event_format *event,
-		    int cpu __used,
-		    u64 timestamp __used,
-		    struct thread *thread __used)
+		    struct event_format *event, struct perf_sample *sample)
 {
 	struct task_desc *waker, *wakee;
 
@@ -784,7 +768,7 @@ replay_wakeup_event(struct trace_wakeup_event *wakeup_event,
 	waker = register_pid(wakeup_event->common_pid, "<unknown>");
 	wakee = register_pid(wakeup_event->pid, wakeup_event->comm);
 
-	add_sched_event_wakeup(waker, timestamp, wakee);
+	add_sched_event_wakeup(waker, sample->time, wakee);
 }
 
 static u64 cpu_last_switched[MAX_CPUS];
@@ -793,12 +777,11 @@ static void
 replay_switch_event(struct trace_switch_event *switch_event,
 		    struct machine *machine __used,
 		    struct event_format *event,
-		    int cpu,
-		    u64 timestamp,
-		    struct thread *thread __used)
+		    struct perf_sample *sample)
 {
 	struct task_desc *prev, __used *next;
-	u64 timestamp0;
+	u64 timestamp0, timestamp = sample->time;
+	int cpu = sample->cpu;
 	s64 delta;
 
 	if (verbose)
@@ -835,10 +818,7 @@ replay_switch_event(struct trace_switch_event *switch_event,
 
 static void
 replay_fork_event(struct trace_fork_event *fork_event,
-		  struct event_format *event,
-		  int cpu __used,
-		  u64 timestamp __used,
-		  struct thread *thread __used)
+		  struct event_format *event)
 {
 	if (verbose) {
 		printf("sched_fork event %p\n", event);
@@ -944,10 +924,7 @@ static void thread_atoms_insert(struct thread *thread)
 
 static void
 latency_fork_event(struct trace_fork_event *fork_event __used,
-		   struct event_format *event __used,
-		   int cpu __used,
-		   u64 timestamp __used,
-		   struct thread *thread __used)
+		   struct event_format *event __used)
 {
 	/* should insert the newcomer */
 }
@@ -1027,13 +1004,12 @@ static void
 latency_switch_event(struct trace_switch_event *switch_event,
 		     struct machine *machine,
 		     struct event_format *event __used,
-		     int cpu,
-		     u64 timestamp,
-		     struct thread *thread __used)
+		     struct perf_sample *sample)
 {
 	struct work_atoms *out_events, *in_events;
 	struct thread *sched_out, *sched_in;
-	u64 timestamp0;
+	u64 timestamp0, timestamp = sample->time;
+	int cpu = sample->cpu;
 	s64 delta;
 
 	BUG_ON(cpu >= MAX_CPUS || cpu < 0);
@@ -1078,14 +1054,12 @@ latency_switch_event(struct trace_switch_event *switch_event,
 
 static void
 latency_runtime_event(struct trace_runtime_event *runtime_event,
-		     struct machine *machine,
-		     struct event_format *event __used,
-		     int cpu,
-		     u64 timestamp,
-		     struct thread *this_thread __used)
+		      struct machine *machine, struct perf_sample *sample)
 {
 	struct thread *thread = machine__findnew_thread(machine, runtime_event->pid);
 	struct work_atoms *atoms = thread_atoms_search(&atom_root, thread, &cmp_pid);
+	u64 timestamp = sample->time;
+	int cpu = sample->cpu;
 
 	BUG_ON(cpu >= MAX_CPUS || cpu < 0);
 	if (!atoms) {
@@ -1101,15 +1075,13 @@ latency_runtime_event(struct trace_runtime_event *runtime_event,
 
 static void
 latency_wakeup_event(struct trace_wakeup_event *wakeup_event,
-		     struct machine *machine,
-		     struct event_format *__event __used,
-		     int cpu __used,
-		     u64 timestamp,
-		     struct thread *thread __used)
+		     struct machine *machine, struct event_format *event __used,
+		     struct perf_sample *sample)
 {
 	struct work_atoms *atoms;
 	struct work_atom *atom;
 	struct thread *wakee;
+	u64 timestamp = sample->time;
 
 	/* Note for later, it may be interesting to observe the failing cases */
 	if (!wakeup_event->success)
@@ -1149,12 +1121,9 @@ latency_wakeup_event(struct trace_wakeup_event *wakeup_event,
 
 static void
 latency_migrate_task_event(struct trace_migrate_task_event *migrate_task_event,
-		     struct machine *machine,
-		     struct event_format *__event __used,
-		     int cpu __used,
-		     u64 timestamp,
-		     struct thread *thread __used)
+			   struct machine *machine, struct perf_sample *sample)
 {
+	u64 timestamp = sample->time;
 	struct work_atoms *atoms;
 	struct work_atom *atom;
 	struct thread *migrant;
@@ -1364,7 +1333,7 @@ process_sched_wakeup_event(struct perf_tool *tool __used,
 			   struct event_format *event,
 			   struct perf_sample *sample,
 			   struct machine *machine,
-			   struct thread *thread)
+			   struct thread *thread __used)
 {
 	void *data = sample->raw_data;
 	struct trace_wakeup_event wakeup_event;
@@ -1378,8 +1347,7 @@ process_sched_wakeup_event(struct perf_tool *tool __used,
 	FILL_FIELD(wakeup_event, cpu, event, data);
 
 	if (trace_handler->wakeup_event)
-		trace_handler->wakeup_event(&wakeup_event, machine, event,
-					    sample->cpu, sample->time, thread);
+		trace_handler->wakeup_event(&wakeup_event, machine, event, sample);
 }
 
 /*
@@ -1399,15 +1367,13 @@ static void
 map_switch_event(struct trace_switch_event *switch_event,
 		 struct machine *machine,
 		 struct event_format *event __used,
-		 int this_cpu,
-		 u64 timestamp,
-		 struct thread *thread __used)
+		 struct perf_sample *sample)
 {
 	struct thread *sched_out __used, *sched_in;
 	int new_shortname;
-	u64 timestamp0;
+	u64 timestamp0, timestamp = sample->time;
 	s64 delta;
-	int cpu;
+	int cpu, this_cpu = sample->cpu;
 
 	BUG_ON(this_cpu >= MAX_CPUS || this_cpu < 0);
 
@@ -1479,7 +1445,7 @@ process_sched_switch_event(struct perf_tool *tool __used,
 			   struct event_format *event,
 			   struct perf_sample *sample,
 			   struct machine *machine,
-			   struct thread *thread)
+			   struct thread *thread __used)
 {
 	int this_cpu = sample->cpu;
 	void *data = sample->raw_data;
@@ -1504,8 +1470,7 @@ process_sched_switch_event(struct perf_tool *tool __used,
 			nr_context_switch_bugs++;
 	}
 	if (trace_handler->switch_event)
-		trace_handler->switch_event(&switch_event, machine, event,
-					    this_cpu, sample->time, thread);
+		trace_handler->switch_event(&switch_event, machine, event, sample);
 
 	curr_pid[this_cpu] = switch_event.next_pid;
 }
@@ -1515,7 +1480,7 @@ process_sched_runtime_event(struct perf_tool *tool __used,
 			    struct event_format *event,
 			    struct perf_sample *sample,
 			    struct machine *machine,
-			    struct thread *thread)
+			    struct thread *thread __used)
 {
 	void *data = sample->raw_data;
 	struct trace_runtime_event runtime_event;
@@ -1526,8 +1491,7 @@ process_sched_runtime_event(struct perf_tool *tool __used,
 	FILL_FIELD(runtime_event, vruntime, event, data);
 
 	if (trace_handler->runtime_event)
-		trace_handler->runtime_event(&runtime_event, machine, event,
-					     sample->cpu, sample->time, thread);
+		trace_handler->runtime_event(&runtime_event, machine, sample);
 }
 
 static void
@@ -1535,7 +1499,7 @@ process_sched_fork_event(struct perf_tool *tool __used,
 			 struct event_format *event,
 			 struct perf_sample *sample,
 			 struct machine *machine __used,
-			 struct thread *thread)
+			 struct thread *thread __used)
 {
 	void *data = sample->raw_data;
 	struct trace_fork_event fork_event;
@@ -1548,8 +1512,7 @@ process_sched_fork_event(struct perf_tool *tool __used,
 	FILL_FIELD(fork_event, child_pid, event, data);
 
 	if (trace_handler->fork_event)
-		trace_handler->fork_event(&fork_event, event,
-					  sample->cpu, sample->time, thread);
+		trace_handler->fork_event(&fork_event, event);
 }
 
 static void
@@ -1568,7 +1531,7 @@ process_sched_migrate_task_event(struct perf_tool *tool __used,
 				 struct event_format *event,
 				 struct perf_sample *sample,
 				 struct machine *machine,
-				 struct thread *thread)
+				 struct thread *thread __used)
 {
 	void *data = sample->raw_data;
 	struct trace_migrate_task_event migrate_task_event;
@@ -1581,9 +1544,7 @@ process_sched_migrate_task_event(struct perf_tool *tool __used,
 	FILL_FIELD(migrate_task_event, cpu, event, data);
 
 	if (trace_handler->migrate_task_event)
-		trace_handler->migrate_task_event(&migrate_task_event, machine,
-						  event, sample->cpu,
-						  sample->time, thread);
+		trace_handler->migrate_task_event(&migrate_task_event, machine, sample);
 }
 
 typedef void (*tracepoint_handler)(struct perf_tool *tool, struct event_format *event,

commit fcf65bf149afa91b875ffde4455967cb63ee0be9
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Tue Aug 7 09:58:03 2012 -0300

    perf evsel: Cache associated event_format
    
    We already lookup the associated event_format when reading the perf.data
    header, so that we can cache the tracepoint name in evsel->name, so do
    it a little further and save the event_format itself, so that we can
    avoid relookups in tools that need to access it.
    
    Change the tools to take the most obvious advantage, when they were
    using pevent_find_event directly. More work is needed for further
    removing the need of a pointer to pevent, such as when asking for event
    field values ("common_pid" and the other common fields and per
    event_format fields).
    
    This is something that was planned but only got actually done when
    Andrey Wagin needed to do this lookup at perf_tool->sample() time, when
    we don't have access to pevent (session->pevent) to use with
    pevent_find_event().
    
    Cc: Andrey Wagin <avagin@gmail.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Namhyung Kim <namhyung@gmail.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Link: http://lkml.kernel.org/n/tip-txkvew2ckko0b594ae8fbnyk@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 7a9ad2b1ee76..30ef82aca885 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -43,11 +43,6 @@ static u64			sleep_measurement_overhead;
 
 static unsigned long		nr_tasks;
 
-struct perf_sched {
-	struct perf_tool    tool;
-	struct perf_session *session;
-};
-
 struct sched_atom;
 
 struct task_desc {
@@ -1596,14 +1591,12 @@ typedef void (*tracepoint_handler)(struct perf_tool *tool, struct event_format *
 				   struct machine *machine,
 				   struct thread *thread);
 
-static int perf_sched__process_tracepoint_sample(struct perf_tool *tool,
+static int perf_sched__process_tracepoint_sample(struct perf_tool *tool __used,
 						 union perf_event *event __used,
 						 struct perf_sample *sample,
 						 struct perf_evsel *evsel,
 						 struct machine *machine)
 {
-	struct perf_sched *sched = container_of(tool, struct perf_sched, tool);
-	struct pevent *pevent = sched->session->pevent;
 	struct thread *thread = machine__findnew_thread(machine, sample->pid);
 
 	if (thread == NULL) {
@@ -1617,25 +1610,18 @@ static int perf_sched__process_tracepoint_sample(struct perf_tool *tool,
 
 	if (evsel->handler.func != NULL) {
 		tracepoint_handler f = evsel->handler.func;
-
-		if (evsel->handler.data == NULL)
-			evsel->handler.data = pevent_find_event(pevent,
-							  evsel->attr.config);
-
-		f(tool, evsel->handler.data, sample, machine, thread);
+		f(tool, evsel->tp_format, sample, machine, thread);
 	}
 
 	return 0;
 }
 
-static struct perf_sched perf_sched = {
-	.tool = {
-		.sample		 = perf_sched__process_tracepoint_sample,
-		.comm		 = perf_event__process_comm,
-		.lost		 = perf_event__process_lost,
-		.fork		 = perf_event__process_task,
-		.ordered_samples = true,
-	},
+static struct perf_tool perf_sched = {
+	.sample		 = perf_sched__process_tracepoint_sample,
+	.comm		 = perf_event__process_comm,
+	.lost		 = perf_event__process_lost,
+	.fork		 = perf_event__process_task,
+	.ordered_samples = true,
 };
 
 static void read_events(bool destroy, struct perf_session **psession)
@@ -1652,18 +1638,15 @@ static void read_events(bool destroy, struct perf_session **psession)
 	};
 	struct perf_session *session;
 
-	session = perf_session__new(input_name, O_RDONLY, 0, false,
-				    &perf_sched.tool);
+	session = perf_session__new(input_name, O_RDONLY, 0, false, &perf_sched);
 	if (session == NULL)
 		die("No Memory");
 
-	perf_sched.session = session;
-
 	err = perf_session__set_tracepoints_handlers(session, handlers);
 	assert(err == 0);
 
 	if (perf_session__has_traces(session, "record -R")) {
-		err = perf_session__process_events(session, &perf_sched.tool);
+		err = perf_session__process_events(session, &perf_sched);
 		if (err)
 			die("Failed to process events, error %d", err);
 

commit da3789628f88684d3f0fb4e6a6bc086c395ac3cb
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Wed Jun 27 13:08:42 2012 -0300

    perf tools: Stop using a global trace events description list
    
    The pevent thing is per perf.data file, so I made it stop being static
    and become a perf_session member, so tools processing perf.data files
    use perf_session and _there_ we read the trace events description into
    session->pevent and then change everywhere to stop using that single
    global pevent variable and use the per session one.
    
    Note that it _doesn't_ fall backs to trace__event_id, as we're not
    interested at all in what is present in the
    /sys/kernel/debug/tracing/events in the workstation doing the analysis,
    just in what is in the perf.data file.
    
    This patch also introduces perf_session__set_tracepoints_handlers that
    is the perf perf.data/session way to associate handlers to tracepoint
    events by resolving their IDs using the events descriptions stored in a
    perf.data file. Make 'perf sched' use it.
    
    Reported-by: Dmitry Antipov <dmitry.antipov@linaro.org>
    Tested-by: Dmitry Antipov <dmitry.antipov@linaro.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Namhyung Kim <namhyung@gmail.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: linaro-dev@lists.linaro.org
    Cc: patches@linaro.org
    Link: http://lkml.kernel.org/r/20120625232016.GA28525@infradead.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 9fe77b185338..7a9ad2b1ee76 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -43,6 +43,11 @@ static u64			sleep_measurement_overhead;
 
 static unsigned long		nr_tasks;
 
+struct perf_sched {
+	struct perf_tool    tool;
+	struct perf_session *session;
+};
+
 struct sched_atom;
 
 struct task_desc {
@@ -1597,6 +1602,8 @@ static int perf_sched__process_tracepoint_sample(struct perf_tool *tool,
 						 struct perf_evsel *evsel,
 						 struct machine *machine)
 {
+	struct perf_sched *sched = container_of(tool, struct perf_sched, tool);
+	struct pevent *pevent = sched->session->pevent;
 	struct thread *thread = machine__findnew_thread(machine, sample->pid);
 
 	if (thread == NULL) {
@@ -1612,7 +1619,8 @@ static int perf_sched__process_tracepoint_sample(struct perf_tool *tool,
 		tracepoint_handler f = evsel->handler.func;
 
 		if (evsel->handler.data == NULL)
-			evsel->handler.data = trace_find_event(evsel->attr.config);
+			evsel->handler.data = pevent_find_event(pevent,
+							  evsel->attr.config);
 
 		f(tool, evsel->handler.data, sample, machine, thread);
 	}
@@ -1620,12 +1628,14 @@ static int perf_sched__process_tracepoint_sample(struct perf_tool *tool,
 	return 0;
 }
 
-static struct perf_tool perf_sched = {
-	.sample			= perf_sched__process_tracepoint_sample,
-	.comm			= perf_event__process_comm,
-	.lost			= perf_event__process_lost,
-	.fork			= perf_event__process_task,
-	.ordered_samples	= true,
+static struct perf_sched perf_sched = {
+	.tool = {
+		.sample		 = perf_sched__process_tracepoint_sample,
+		.comm		 = perf_event__process_comm,
+		.lost		 = perf_event__process_lost,
+		.fork		 = perf_event__process_task,
+		.ordered_samples = true,
+	},
 };
 
 static void read_events(bool destroy, struct perf_session **psession)
@@ -1640,16 +1650,20 @@ static void read_events(bool destroy, struct perf_session **psession)
 		{ "sched:sched_process_exit", process_sched_exit_event, },
 		{ "sched:sched_migrate_task", process_sched_migrate_task_event, },
 	};
-	struct perf_session *session = perf_session__new(input_name, O_RDONLY,
-							 0, false, &perf_sched);
+	struct perf_session *session;
+
+	session = perf_session__new(input_name, O_RDONLY, 0, false,
+				    &perf_sched.tool);
 	if (session == NULL)
 		die("No Memory");
 
-	err = perf_evlist__set_tracepoints_handlers_array(session->evlist, handlers);
+	perf_sched.session = session;
+
+	err = perf_session__set_tracepoints_handlers(session, handlers);
 	assert(err == 0);
 
 	if (perf_session__has_traces(session, "record -R")) {
-		err = perf_session__process_events(session, &perf_sched);
+		err = perf_session__process_events(session, &perf_sched.tool);
 		if (err)
 			die("Failed to process events, error %d", err);
 

commit 22c8b84320ecf9ecbb6587d46a259b828e9ece5e
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Tue Jun 12 13:55:13 2012 -0300

    perf tools: Don't access evsel->name directly
    
    One needs to use perf_evsel__name() so that if needed the name gets
    synthesized and stored in evsel->name, from where perf_evsel__name()
    will serve from them on.
    
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Namhyung Kim <namhyung@gmail.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Link: http://lkml.kernel.org/n/tip-ml7zbenjmri9bghmrea0jm0d@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index b125e07eb399..9fe77b185338 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1601,7 +1601,7 @@ static int perf_sched__process_tracepoint_sample(struct perf_tool *tool,
 
 	if (thread == NULL) {
 		pr_debug("problem processing %s event, skipping it.\n",
-			 evsel->name);
+			 perf_evsel__name(evsel));
 		return -1;
 	}
 

commit aaf045f72335653b24784d6042be8e4aee114403
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Apr 6 00:47:56 2012 +0200

    perf: Have perf use the new libtraceevent.a library
    
    The event parsing code in perf was originally copied from trace-cmd
    but never was kept up-to-date with the changes that was done there.
    The trace-cmd libtraceevent.a code is much more mature than what is
    currently in perf.
    
    This updates the code to use wrappers to handle the calls to the
    new event parsing code. The new code requires a handle to be pass
    around, which removes the global event variables and allows
    more than one event structure to be read from different files
    (and different machines).
    
    But perf still has the old global events and the code throughout
    perf does not yet have a nice way to pass around a handle.
    A global 'pevent' has been made for perf and the old calls have
    been created as wrappers to the new event parsing code that uses
    the global pevent.
    
    With this change, perf can later incorporate the pevent handle into
    the perf structures and allow more than one file to be read and
    compared, that contains different events.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Arun Sharma <asharma@fb.com>
    Cc: Namhyung Kim <namhyung.kim@lge.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 1cad3af4bf4c..b125e07eb399 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -728,34 +728,34 @@ struct trace_migrate_task_event {
 struct trace_sched_handler {
 	void (*switch_event)(struct trace_switch_event *,
 			     struct machine *,
-			     struct event *,
+			     struct event_format *,
 			     int cpu,
 			     u64 timestamp,
 			     struct thread *thread);
 
 	void (*runtime_event)(struct trace_runtime_event *,
 			      struct machine *,
-			      struct event *,
+			      struct event_format *,
 			      int cpu,
 			      u64 timestamp,
 			      struct thread *thread);
 
 	void (*wakeup_event)(struct trace_wakeup_event *,
 			     struct machine *,
-			     struct event *,
+			     struct event_format *,
 			     int cpu,
 			     u64 timestamp,
 			     struct thread *thread);
 
 	void (*fork_event)(struct trace_fork_event *,
-			   struct event *,
+			   struct event_format *,
 			   int cpu,
 			   u64 timestamp,
 			   struct thread *thread);
 
 	void (*migrate_task_event)(struct trace_migrate_task_event *,
 			   struct machine *machine,
-			   struct event *,
+			   struct event_format *,
 			   int cpu,
 			   u64 timestamp,
 			   struct thread *thread);
@@ -765,7 +765,7 @@ struct trace_sched_handler {
 static void
 replay_wakeup_event(struct trace_wakeup_event *wakeup_event,
 		    struct machine *machine __used,
-		    struct event *event,
+		    struct event_format *event,
 		    int cpu __used,
 		    u64 timestamp __used,
 		    struct thread *thread __used)
@@ -792,7 +792,7 @@ static u64 cpu_last_switched[MAX_CPUS];
 static void
 replay_switch_event(struct trace_switch_event *switch_event,
 		    struct machine *machine __used,
-		    struct event *event,
+		    struct event_format *event,
 		    int cpu,
 		    u64 timestamp,
 		    struct thread *thread __used)
@@ -835,7 +835,7 @@ replay_switch_event(struct trace_switch_event *switch_event,
 
 static void
 replay_fork_event(struct trace_fork_event *fork_event,
-		  struct event *event,
+		  struct event_format *event,
 		  int cpu __used,
 		  u64 timestamp __used,
 		  struct thread *thread __used)
@@ -944,7 +944,7 @@ static void thread_atoms_insert(struct thread *thread)
 
 static void
 latency_fork_event(struct trace_fork_event *fork_event __used,
-		   struct event *event __used,
+		   struct event_format *event __used,
 		   int cpu __used,
 		   u64 timestamp __used,
 		   struct thread *thread __used)
@@ -1026,7 +1026,7 @@ add_sched_in_event(struct work_atoms *atoms, u64 timestamp)
 static void
 latency_switch_event(struct trace_switch_event *switch_event,
 		     struct machine *machine,
-		     struct event *event __used,
+		     struct event_format *event __used,
 		     int cpu,
 		     u64 timestamp,
 		     struct thread *thread __used)
@@ -1079,7 +1079,7 @@ latency_switch_event(struct trace_switch_event *switch_event,
 static void
 latency_runtime_event(struct trace_runtime_event *runtime_event,
 		     struct machine *machine,
-		     struct event *event __used,
+		     struct event_format *event __used,
 		     int cpu,
 		     u64 timestamp,
 		     struct thread *this_thread __used)
@@ -1102,7 +1102,7 @@ latency_runtime_event(struct trace_runtime_event *runtime_event,
 static void
 latency_wakeup_event(struct trace_wakeup_event *wakeup_event,
 		     struct machine *machine,
-		     struct event *__event __used,
+		     struct event_format *__event __used,
 		     int cpu __used,
 		     u64 timestamp,
 		     struct thread *thread __used)
@@ -1150,7 +1150,7 @@ latency_wakeup_event(struct trace_wakeup_event *wakeup_event,
 static void
 latency_migrate_task_event(struct trace_migrate_task_event *migrate_task_event,
 		     struct machine *machine,
-		     struct event *__event __used,
+		     struct event_format *__event __used,
 		     int cpu __used,
 		     u64 timestamp,
 		     struct thread *thread __used)
@@ -1361,7 +1361,7 @@ static struct trace_sched_handler *trace_handler;
 
 static void
 process_sched_wakeup_event(struct perf_tool *tool __used,
-			   struct event *event,
+			   struct event_format *event,
 			   struct perf_sample *sample,
 			   struct machine *machine,
 			   struct thread *thread)
@@ -1398,7 +1398,7 @@ static char next_shortname2 = '0';
 static void
 map_switch_event(struct trace_switch_event *switch_event,
 		 struct machine *machine,
-		 struct event *event __used,
+		 struct event_format *event __used,
 		 int this_cpu,
 		 u64 timestamp,
 		 struct thread *thread __used)
@@ -1476,7 +1476,7 @@ map_switch_event(struct trace_switch_event *switch_event,
 
 static void
 process_sched_switch_event(struct perf_tool *tool __used,
-			   struct event *event,
+			   struct event_format *event,
 			   struct perf_sample *sample,
 			   struct machine *machine,
 			   struct thread *thread)
@@ -1512,7 +1512,7 @@ process_sched_switch_event(struct perf_tool *tool __used,
 
 static void
 process_sched_runtime_event(struct perf_tool *tool __used,
-			    struct event *event,
+			    struct event_format *event,
 			    struct perf_sample *sample,
 			    struct machine *machine,
 			    struct thread *thread)
@@ -1532,7 +1532,7 @@ process_sched_runtime_event(struct perf_tool *tool __used,
 
 static void
 process_sched_fork_event(struct perf_tool *tool __used,
-			 struct event *event,
+			 struct event_format *event,
 			 struct perf_sample *sample,
 			 struct machine *machine __used,
 			 struct thread *thread)
@@ -1554,7 +1554,7 @@ process_sched_fork_event(struct perf_tool *tool __used,
 
 static void
 process_sched_exit_event(struct perf_tool *tool __used,
-			 struct event *event,
+			 struct event_format *event,
 			 struct perf_sample *sample __used,
 			 struct machine *machine __used,
 			 struct thread *thread __used)
@@ -1565,7 +1565,7 @@ process_sched_exit_event(struct perf_tool *tool __used,
 
 static void
 process_sched_migrate_task_event(struct perf_tool *tool __used,
-				 struct event *event,
+				 struct event_format *event,
 				 struct perf_sample *sample,
 				 struct machine *machine,
 				 struct thread *thread)
@@ -1586,7 +1586,7 @@ process_sched_migrate_task_event(struct perf_tool *tool __used,
 						  sample->time, thread);
 }
 
-typedef void (*tracepoint_handler)(struct perf_tool *tool, struct event *event,
+typedef void (*tracepoint_handler)(struct perf_tool *tool, struct event_format *event,
 				   struct perf_sample *sample,
 				   struct machine *machine,
 				   struct thread *thread);

commit 7b78f13603c6fcb64e020a0bbe31a651ea2b657b
Author: Markus Trippelsdorf <markus@trippelsdorf.de>
Date:   Wed Apr 4 10:45:27 2012 +0200

    perf tools: Fix getrusage() related build failure on glibc trunk
    
    On a system running glibc trunk perf doesn't build:
    
        CC builtin-sched.o
    builtin-sched.c: In function ‘get_cpu_usage_nsec_parent’: builtin-sched.c:399:16: error: storage size of ‘ru’ isn’t known builtin-sched.c:403:2: error: implicit declaration of function ‘getrusage’ [-Werror=implicit-function-declaration]
        [...]
    
    Fix it by including sys/resource.h.
    
    Signed-off-by: Markus Trippelsdorf <markus@trippelsdorf.de>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20120404084527.GA294@x4
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index fb8b5f83b4a0..1cad3af4bf4c 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -17,6 +17,7 @@
 #include "util/debug.h"
 
 #include <sys/prctl.h>
+#include <sys/resource.h>
 
 #include <semaphore.h>
 #include <pthread.h>

commit efad14150a0b4429f37da7245001a8096ef7ee38
Author: Robert Richter <robert.richter@amd.com>
Date:   Wed Dec 7 10:02:54 2011 +0100

    perf report: Accept fifos as input file
    
    The default input file for perf report is not handled the same way as
    perf record does it for its output file. This leads to unexpected
    behavior of perf report, etc. E.g.:
    
     # perf record -a -e cpu-cycles sleep 2 | perf report | cat
     failed to open perf.data: No such file or directory  (try 'perf record' first)
    
    While perf record writes to a fifo, perf report expects perf.data to be
    read. This patch changes this to accept fifos as input file.
    
    Applies to the following commands:
    
     perf annotate
     perf buildid-list
     perf evlist
     perf kmem
     perf lock
     perf report
     perf sched
     perf script
     perf timechart
    
    Also fixes char const* -> const char* type declaration for filename
    strings.
    
    v2:
    * Prevent potential null pointer access to input_name in
      builtin-report.c. Needed due to removal of patch "perf report: Setup
      browser if stdout is a pipe"
    
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Link: http://lkml.kernel.org/r/1323248577-11268-5-git-send-email-robert.richter@amd.com
    Signed-off-by: Robert Richter <robert.richter@amd.com>
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 6284ed2317f2..fb8b5f83b4a0 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -22,7 +22,7 @@
 #include <pthread.h>
 #include <math.h>
 
-static char			const *input_name = "perf.data";
+static const char		*input_name;
 
 static char			default_sort_order[] = "avg, max, switch, runtime";
 static const char		*sort_order = default_sort_order;

commit ee29be625bd7b115d45eba4b0526ff3e24bf3ca0
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Mon Nov 28 17:57:40 2011 -0200

    perf tools: Save some loops using perf_evlist__id2evsel
    
    Since we already ask for PERF_SAMPLE_ID and use it to quickly find the
    associated evsel, add handler func + data to struct perf_evsel to avoid
    using chains of if(strcmp(event_name)) and also to avoid all the linear
    list searches via trace_event_find.
    
    To demonstrate the technique convert 'perf sched' to it:
    
     # perf sched record sleep 5m
    
    And then:
    
     Performance counter stats for '/tmp/oldperf sched lat':
    
            646.929438 task-clock                #    0.999 CPUs utilized
                     9 context-switches          #    0.000 M/sec
                     0 CPU-migrations            #    0.000 M/sec
                20,901 page-faults               #    0.032 M/sec
         1,290,144,450 cycles                    #    1.994 GHz
       <not supported> stalled-cycles-frontend
       <not supported> stalled-cycles-backend
         1,606,158,439 instructions              #    1.24  insns per cycle
           339,088,395 branches                  #  524.151 M/sec
             4,550,735 branch-misses             #    1.34% of all branches
    
           0.647524759 seconds time elapsed
    
    Versus:
    
     Performance counter stats for 'perf sched lat':
    
            473.564691 task-clock                #    0.999 CPUs utilized
                     9 context-switches          #    0.000 M/sec
                     0 CPU-migrations            #    0.000 M/sec
                20,903 page-faults               #    0.044 M/sec
           944,367,984 cycles                    #    1.994 GHz
       <not supported> stalled-cycles-frontend
       <not supported> stalled-cycles-backend
         1,442,385,571 instructions              #    1.53  insns per cycle
           308,383,106 branches                  #  651.195 M/sec
             4,481,784 branch-misses             #    1.45% of all branches
    
           0.474215751 seconds time elapsed
    
    [root@emilia ~]#
    
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Link: http://lkml.kernel.org/n/tip-1kbzpl74lwi6lavpqke2u2p3@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 0ee868e6f63b..6284ed2317f2 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -2,6 +2,7 @@
 #include "perf.h"
 
 #include "util/util.h"
+#include "util/evlist.h"
 #include "util/cache.h"
 #include "util/evsel.h"
 #include "util/symbol.h"
@@ -1358,12 +1359,13 @@ static void sort_lat(void)
 static struct trace_sched_handler *trace_handler;
 
 static void
-process_sched_wakeup_event(void *data, struct machine *machine,
+process_sched_wakeup_event(struct perf_tool *tool __used,
 			   struct event *event,
-			   int cpu __used,
-			   u64 timestamp __used,
-			   struct thread *thread __used)
+			   struct perf_sample *sample,
+			   struct machine *machine,
+			   struct thread *thread)
 {
+	void *data = sample->raw_data;
 	struct trace_wakeup_event wakeup_event;
 
 	FILL_COMMON_FIELDS(wakeup_event, event, data);
@@ -1376,7 +1378,7 @@ process_sched_wakeup_event(void *data, struct machine *machine,
 
 	if (trace_handler->wakeup_event)
 		trace_handler->wakeup_event(&wakeup_event, machine, event,
-					    cpu, timestamp, thread);
+					    sample->cpu, sample->time, thread);
 }
 
 /*
@@ -1471,14 +1473,15 @@ map_switch_event(struct trace_switch_event *switch_event,
 	}
 }
 
-
 static void
-process_sched_switch_event(void *data, struct machine *machine,
+process_sched_switch_event(struct perf_tool *tool __used,
 			   struct event *event,
-			   int this_cpu,
-			   u64 timestamp __used,
-			   struct thread *thread __used)
+			   struct perf_sample *sample,
+			   struct machine *machine,
+			   struct thread *thread)
 {
+	int this_cpu = sample->cpu;
+	void *data = sample->raw_data;
 	struct trace_switch_event switch_event;
 
 	FILL_COMMON_FIELDS(switch_event, event, data);
@@ -1501,18 +1504,19 @@ process_sched_switch_event(void *data, struct machine *machine,
 	}
 	if (trace_handler->switch_event)
 		trace_handler->switch_event(&switch_event, machine, event,
-					    this_cpu, timestamp, thread);
+					    this_cpu, sample->time, thread);
 
 	curr_pid[this_cpu] = switch_event.next_pid;
 }
 
 static void
-process_sched_runtime_event(void *data, struct machine *machine,
-			   struct event *event,
-			   int cpu __used,
-			   u64 timestamp __used,
-			   struct thread *thread __used)
+process_sched_runtime_event(struct perf_tool *tool __used,
+			    struct event *event,
+			    struct perf_sample *sample,
+			    struct machine *machine,
+			    struct thread *thread)
 {
+	void *data = sample->raw_data;
 	struct trace_runtime_event runtime_event;
 
 	FILL_ARRAY(runtime_event, comm, event, data);
@@ -1521,16 +1525,18 @@ process_sched_runtime_event(void *data, struct machine *machine,
 	FILL_FIELD(runtime_event, vruntime, event, data);
 
 	if (trace_handler->runtime_event)
-		trace_handler->runtime_event(&runtime_event, machine, event, cpu, timestamp, thread);
+		trace_handler->runtime_event(&runtime_event, machine, event,
+					     sample->cpu, sample->time, thread);
 }
 
 static void
-process_sched_fork_event(void *data,
+process_sched_fork_event(struct perf_tool *tool __used,
 			 struct event *event,
-			 int cpu __used,
-			 u64 timestamp __used,
-			 struct thread *thread __used)
+			 struct perf_sample *sample,
+			 struct machine *machine __used,
+			 struct thread *thread)
 {
+	void *data = sample->raw_data;
 	struct trace_fork_event fork_event;
 
 	FILL_COMMON_FIELDS(fork_event, event, data);
@@ -1542,13 +1548,14 @@ process_sched_fork_event(void *data,
 
 	if (trace_handler->fork_event)
 		trace_handler->fork_event(&fork_event, event,
-					  cpu, timestamp, thread);
+					  sample->cpu, sample->time, thread);
 }
 
 static void
-process_sched_exit_event(struct event *event,
-			 int cpu __used,
-			 u64 timestamp __used,
+process_sched_exit_event(struct perf_tool *tool __used,
+			 struct event *event,
+			 struct perf_sample *sample __used,
+			 struct machine *machine __used,
 			 struct thread *thread __used)
 {
 	if (verbose)
@@ -1556,12 +1563,13 @@ process_sched_exit_event(struct event *event,
 }
 
 static void
-process_sched_migrate_task_event(void *data, struct machine *machine,
-			   struct event *event,
-			   int cpu __used,
-			   u64 timestamp __used,
-			   struct thread *thread __used)
+process_sched_migrate_task_event(struct perf_tool *tool __used,
+				 struct event *event,
+				 struct perf_sample *sample,
+				 struct machine *machine,
+				 struct thread *thread)
 {
+	void *data = sample->raw_data;
 	struct trace_migrate_task_event migrate_task_event;
 
 	FILL_COMMON_FIELDS(migrate_task_event, event, data);
@@ -1573,67 +1581,46 @@ process_sched_migrate_task_event(void *data, struct machine *machine,
 
 	if (trace_handler->migrate_task_event)
 		trace_handler->migrate_task_event(&migrate_task_event, machine,
-						 event, cpu, timestamp, thread);
+						  event, sample->cpu,
+						  sample->time, thread);
 }
 
-static void process_raw_event(union perf_event *raw_event __used,
-			      struct machine *machine, void *data, int cpu,
-			      u64 timestamp, struct thread *thread)
-{
-	struct event *event;
-	int type;
-
-
-	type = trace_parse_common_type(data);
-	event = trace_find_event(type);
-
-	if (!strcmp(event->name, "sched_switch"))
-		process_sched_switch_event(data, machine, event, cpu, timestamp, thread);
-	if (!strcmp(event->name, "sched_stat_runtime"))
-		process_sched_runtime_event(data, machine, event, cpu, timestamp, thread);
-	if (!strcmp(event->name, "sched_wakeup"))
-		process_sched_wakeup_event(data, machine, event, cpu, timestamp, thread);
-	if (!strcmp(event->name, "sched_wakeup_new"))
-		process_sched_wakeup_event(data, machine, event, cpu, timestamp, thread);
-	if (!strcmp(event->name, "sched_process_fork"))
-		process_sched_fork_event(data, event, cpu, timestamp, thread);
-	if (!strcmp(event->name, "sched_process_exit"))
-		process_sched_exit_event(event, cpu, timestamp, thread);
-	if (!strcmp(event->name, "sched_migrate_task"))
-		process_sched_migrate_task_event(data, machine, event, cpu, timestamp, thread);
-}
+typedef void (*tracepoint_handler)(struct perf_tool *tool, struct event *event,
+				   struct perf_sample *sample,
+				   struct machine *machine,
+				   struct thread *thread);
 
-static int process_sample_event(struct perf_tool *tool __used,
-				union perf_event *event,
-				struct perf_sample *sample,
-				struct perf_evsel *evsel,
-				struct machine *machine)
+static int perf_sched__process_tracepoint_sample(struct perf_tool *tool,
+						 union perf_event *event __used,
+						 struct perf_sample *sample,
+						 struct perf_evsel *evsel,
+						 struct machine *machine)
 {
-	struct thread *thread;
-
-	if (!(evsel->attr.sample_type & PERF_SAMPLE_RAW))
-		return 0;
+	struct thread *thread = machine__findnew_thread(machine, sample->pid);
 
-	thread = machine__findnew_thread(machine, sample->pid);
 	if (thread == NULL) {
-		pr_debug("problem processing %d event, skipping it.\n",
-			 event->header.type);
+		pr_debug("problem processing %s event, skipping it.\n",
+			 evsel->name);
 		return -1;
 	}
 
-	dump_printf(" ... thread: %s:%d\n", thread->comm, thread->pid);
+	evsel->hists.stats.total_period += sample->period;
+	hists__inc_nr_events(&evsel->hists, PERF_RECORD_SAMPLE);
 
-	if (profile_cpu != -1 && profile_cpu != (int)sample->cpu)
-		return 0;
+	if (evsel->handler.func != NULL) {
+		tracepoint_handler f = evsel->handler.func;
 
-	process_raw_event(event, machine, sample->raw_data, sample->cpu,
-			  sample->time, thread);
+		if (evsel->handler.data == NULL)
+			evsel->handler.data = trace_find_event(evsel->attr.config);
+
+		f(tool, evsel->handler.data, sample, machine, thread);
+	}
 
 	return 0;
 }
 
 static struct perf_tool perf_sched = {
-	.sample			= process_sample_event,
+	.sample			= perf_sched__process_tracepoint_sample,
 	.comm			= perf_event__process_comm,
 	.lost			= perf_event__process_lost,
 	.fork			= perf_event__process_task,
@@ -1643,11 +1630,23 @@ static struct perf_tool perf_sched = {
 static void read_events(bool destroy, struct perf_session **psession)
 {
 	int err = -EINVAL;
+	const struct perf_evsel_str_handler handlers[] = {
+		{ "sched:sched_switch",	      process_sched_switch_event, },
+		{ "sched:sched_stat_runtime", process_sched_runtime_event, },
+		{ "sched:sched_wakeup",	      process_sched_wakeup_event, },
+		{ "sched:sched_wakeup_new",   process_sched_wakeup_event, },
+		{ "sched:sched_process_fork", process_sched_fork_event, },
+		{ "sched:sched_process_exit", process_sched_exit_event, },
+		{ "sched:sched_migrate_task", process_sched_migrate_task_event, },
+	};
 	struct perf_session *session = perf_session__new(input_name, O_RDONLY,
 							 0, false, &perf_sched);
 	if (session == NULL)
 		die("No Memory");
 
+	err = perf_evlist__set_tracepoints_handlers_array(session->evlist, handlers);
+	assert(err == 0);
+
 	if (perf_session__has_traces(session, "record -R")) {
 		err = perf_session__process_events(session, &perf_sched);
 		if (err)

commit 45694aa7702bc44d538a3bcb51bb2bb96cf190c0
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Mon Nov 28 08:30:20 2011 -0200

    perf tools: Rename perf_event_ops to perf_tool
    
    To better reflect that it became the base class for all tools, that must
    be in each tool struct and where common stuff will be put.
    
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Link: http://lkml.kernel.org/n/tip-qgpc4msetqlwr8y2k7537cxe@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 6a771f822e5d..0ee868e6f63b 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -8,6 +8,7 @@
 #include "util/thread.h"
 #include "util/header.h"
 #include "util/session.h"
+#include "util/tool.h"
 
 #include "util/parse-options.h"
 #include "util/trace-event.h"
@@ -1602,7 +1603,7 @@ static void process_raw_event(union perf_event *raw_event __used,
 		process_sched_migrate_task_event(data, machine, event, cpu, timestamp, thread);
 }
 
-static int process_sample_event(struct perf_event_ops *ops __used,
+static int process_sample_event(struct perf_tool *tool __used,
 				union perf_event *event,
 				struct perf_sample *sample,
 				struct perf_evsel *evsel,
@@ -1631,7 +1632,7 @@ static int process_sample_event(struct perf_event_ops *ops __used,
 	return 0;
 }
 
-static struct perf_event_ops event_ops = {
+static struct perf_tool perf_sched = {
 	.sample			= process_sample_event,
 	.comm			= perf_event__process_comm,
 	.lost			= perf_event__process_lost,
@@ -1643,12 +1644,12 @@ static void read_events(bool destroy, struct perf_session **psession)
 {
 	int err = -EINVAL;
 	struct perf_session *session = perf_session__new(input_name, O_RDONLY,
-							 0, false, &event_ops);
+							 0, false, &perf_sched);
 	if (session == NULL)
 		die("No Memory");
 
 	if (perf_session__has_traces(session, "record -R")) {
-		err = perf_session__process_events(session, &event_ops);
+		err = perf_session__process_events(session, &perf_sched);
 		if (err)
 			die("Failed to process events, error %d", err);
 

commit 743eb868657bdb1b26c7b24077ca21c67c82c777
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Mon Nov 28 07:56:39 2011 -0200

    perf tools: Resolve machine earlier and pass it to perf_event_ops
    
    Reducing the exposure of perf_session further, so that we can use the
    classes in cases where no perf.data file is created.
    
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Link: http://lkml.kernel.org/n/tip-stua66dcscsezzrcdugvbmvd@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index b11d6283fedf..6a771f822e5d 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -724,21 +724,21 @@ struct trace_migrate_task_event {
 
 struct trace_sched_handler {
 	void (*switch_event)(struct trace_switch_event *,
-			     struct perf_session *,
+			     struct machine *,
 			     struct event *,
 			     int cpu,
 			     u64 timestamp,
 			     struct thread *thread);
 
 	void (*runtime_event)(struct trace_runtime_event *,
-			      struct perf_session *,
+			      struct machine *,
 			      struct event *,
 			      int cpu,
 			      u64 timestamp,
 			      struct thread *thread);
 
 	void (*wakeup_event)(struct trace_wakeup_event *,
-			     struct perf_session *,
+			     struct machine *,
 			     struct event *,
 			     int cpu,
 			     u64 timestamp,
@@ -751,7 +751,7 @@ struct trace_sched_handler {
 			   struct thread *thread);
 
 	void (*migrate_task_event)(struct trace_migrate_task_event *,
-			   struct perf_session *session,
+			   struct machine *machine,
 			   struct event *,
 			   int cpu,
 			   u64 timestamp,
@@ -761,7 +761,7 @@ struct trace_sched_handler {
 
 static void
 replay_wakeup_event(struct trace_wakeup_event *wakeup_event,
-		    struct perf_session *session __used,
+		    struct machine *machine __used,
 		    struct event *event,
 		    int cpu __used,
 		    u64 timestamp __used,
@@ -788,7 +788,7 @@ static u64 cpu_last_switched[MAX_CPUS];
 
 static void
 replay_switch_event(struct trace_switch_event *switch_event,
-		    struct perf_session *session __used,
+		    struct machine *machine __used,
 		    struct event *event,
 		    int cpu,
 		    u64 timestamp,
@@ -1022,7 +1022,7 @@ add_sched_in_event(struct work_atoms *atoms, u64 timestamp)
 
 static void
 latency_switch_event(struct trace_switch_event *switch_event,
-		     struct perf_session *session,
+		     struct machine *machine,
 		     struct event *event __used,
 		     int cpu,
 		     u64 timestamp,
@@ -1046,8 +1046,8 @@ latency_switch_event(struct trace_switch_event *switch_event,
 		die("hm, delta: %" PRIu64 " < 0 ?\n", delta);
 
 
-	sched_out = perf_session__findnew(session, switch_event->prev_pid);
-	sched_in = perf_session__findnew(session, switch_event->next_pid);
+	sched_out = machine__findnew_thread(machine, switch_event->prev_pid);
+	sched_in = machine__findnew_thread(machine, switch_event->next_pid);
 
 	out_events = thread_atoms_search(&atom_root, sched_out, &cmp_pid);
 	if (!out_events) {
@@ -1075,13 +1075,13 @@ latency_switch_event(struct trace_switch_event *switch_event,
 
 static void
 latency_runtime_event(struct trace_runtime_event *runtime_event,
-		     struct perf_session *session,
+		     struct machine *machine,
 		     struct event *event __used,
 		     int cpu,
 		     u64 timestamp,
 		     struct thread *this_thread __used)
 {
-	struct thread *thread = perf_session__findnew(session, runtime_event->pid);
+	struct thread *thread = machine__findnew_thread(machine, runtime_event->pid);
 	struct work_atoms *atoms = thread_atoms_search(&atom_root, thread, &cmp_pid);
 
 	BUG_ON(cpu >= MAX_CPUS || cpu < 0);
@@ -1098,7 +1098,7 @@ latency_runtime_event(struct trace_runtime_event *runtime_event,
 
 static void
 latency_wakeup_event(struct trace_wakeup_event *wakeup_event,
-		     struct perf_session *session,
+		     struct machine *machine,
 		     struct event *__event __used,
 		     int cpu __used,
 		     u64 timestamp,
@@ -1112,7 +1112,7 @@ latency_wakeup_event(struct trace_wakeup_event *wakeup_event,
 	if (!wakeup_event->success)
 		return;
 
-	wakee = perf_session__findnew(session, wakeup_event->pid);
+	wakee = machine__findnew_thread(machine, wakeup_event->pid);
 	atoms = thread_atoms_search(&atom_root, wakee, &cmp_pid);
 	if (!atoms) {
 		thread_atoms_insert(wakee);
@@ -1146,7 +1146,7 @@ latency_wakeup_event(struct trace_wakeup_event *wakeup_event,
 
 static void
 latency_migrate_task_event(struct trace_migrate_task_event *migrate_task_event,
-		     struct perf_session *session,
+		     struct machine *machine,
 		     struct event *__event __used,
 		     int cpu __used,
 		     u64 timestamp,
@@ -1162,7 +1162,7 @@ latency_migrate_task_event(struct trace_migrate_task_event *migrate_task_event,
 	if (profile_cpu == -1)
 		return;
 
-	migrant = perf_session__findnew(session, migrate_task_event->pid);
+	migrant = machine__findnew_thread(machine, migrate_task_event->pid);
 	atoms = thread_atoms_search(&atom_root, migrant, &cmp_pid);
 	if (!atoms) {
 		thread_atoms_insert(migrant);
@@ -1357,7 +1357,7 @@ static void sort_lat(void)
 static struct trace_sched_handler *trace_handler;
 
 static void
-process_sched_wakeup_event(void *data, struct perf_session *session,
+process_sched_wakeup_event(void *data, struct machine *machine,
 			   struct event *event,
 			   int cpu __used,
 			   u64 timestamp __used,
@@ -1374,7 +1374,7 @@ process_sched_wakeup_event(void *data, struct perf_session *session,
 	FILL_FIELD(wakeup_event, cpu, event, data);
 
 	if (trace_handler->wakeup_event)
-		trace_handler->wakeup_event(&wakeup_event, session, event,
+		trace_handler->wakeup_event(&wakeup_event, machine, event,
 					    cpu, timestamp, thread);
 }
 
@@ -1393,7 +1393,7 @@ static char next_shortname2 = '0';
 
 static void
 map_switch_event(struct trace_switch_event *switch_event,
-		 struct perf_session *session,
+		 struct machine *machine,
 		 struct event *event __used,
 		 int this_cpu,
 		 u64 timestamp,
@@ -1421,8 +1421,8 @@ map_switch_event(struct trace_switch_event *switch_event,
 		die("hm, delta: %" PRIu64 " < 0 ?\n", delta);
 
 
-	sched_out = perf_session__findnew(session, switch_event->prev_pid);
-	sched_in = perf_session__findnew(session, switch_event->next_pid);
+	sched_out = machine__findnew_thread(machine, switch_event->prev_pid);
+	sched_in = machine__findnew_thread(machine, switch_event->next_pid);
 
 	curr_thread[this_cpu] = sched_in;
 
@@ -1472,7 +1472,7 @@ map_switch_event(struct trace_switch_event *switch_event,
 
 
 static void
-process_sched_switch_event(void *data, struct perf_session *session,
+process_sched_switch_event(void *data, struct machine *machine,
 			   struct event *event,
 			   int this_cpu,
 			   u64 timestamp __used,
@@ -1499,14 +1499,14 @@ process_sched_switch_event(void *data, struct perf_session *session,
 			nr_context_switch_bugs++;
 	}
 	if (trace_handler->switch_event)
-		trace_handler->switch_event(&switch_event, session, event,
+		trace_handler->switch_event(&switch_event, machine, event,
 					    this_cpu, timestamp, thread);
 
 	curr_pid[this_cpu] = switch_event.next_pid;
 }
 
 static void
-process_sched_runtime_event(void *data, struct perf_session *session,
+process_sched_runtime_event(void *data, struct machine *machine,
 			   struct event *event,
 			   int cpu __used,
 			   u64 timestamp __used,
@@ -1520,7 +1520,7 @@ process_sched_runtime_event(void *data, struct perf_session *session,
 	FILL_FIELD(runtime_event, vruntime, event, data);
 
 	if (trace_handler->runtime_event)
-		trace_handler->runtime_event(&runtime_event, session, event, cpu, timestamp, thread);
+		trace_handler->runtime_event(&runtime_event, machine, event, cpu, timestamp, thread);
 }
 
 static void
@@ -1555,7 +1555,7 @@ process_sched_exit_event(struct event *event,
 }
 
 static void
-process_sched_migrate_task_event(void *data, struct perf_session *session,
+process_sched_migrate_task_event(void *data, struct machine *machine,
 			   struct event *event,
 			   int cpu __used,
 			   u64 timestamp __used,
@@ -1571,12 +1571,12 @@ process_sched_migrate_task_event(void *data, struct perf_session *session,
 	FILL_FIELD(migrate_task_event, cpu, event, data);
 
 	if (trace_handler->migrate_task_event)
-		trace_handler->migrate_task_event(&migrate_task_event, session,
+		trace_handler->migrate_task_event(&migrate_task_event, machine,
 						 event, cpu, timestamp, thread);
 }
 
 static void process_raw_event(union perf_event *raw_event __used,
-			      struct perf_session *session, void *data, int cpu,
+			      struct machine *machine, void *data, int cpu,
 			      u64 timestamp, struct thread *thread)
 {
 	struct event *event;
@@ -1587,33 +1587,33 @@ static void process_raw_event(union perf_event *raw_event __used,
 	event = trace_find_event(type);
 
 	if (!strcmp(event->name, "sched_switch"))
-		process_sched_switch_event(data, session, event, cpu, timestamp, thread);
+		process_sched_switch_event(data, machine, event, cpu, timestamp, thread);
 	if (!strcmp(event->name, "sched_stat_runtime"))
-		process_sched_runtime_event(data, session, event, cpu, timestamp, thread);
+		process_sched_runtime_event(data, machine, event, cpu, timestamp, thread);
 	if (!strcmp(event->name, "sched_wakeup"))
-		process_sched_wakeup_event(data, session, event, cpu, timestamp, thread);
+		process_sched_wakeup_event(data, machine, event, cpu, timestamp, thread);
 	if (!strcmp(event->name, "sched_wakeup_new"))
-		process_sched_wakeup_event(data, session, event, cpu, timestamp, thread);
+		process_sched_wakeup_event(data, machine, event, cpu, timestamp, thread);
 	if (!strcmp(event->name, "sched_process_fork"))
 		process_sched_fork_event(data, event, cpu, timestamp, thread);
 	if (!strcmp(event->name, "sched_process_exit"))
 		process_sched_exit_event(event, cpu, timestamp, thread);
 	if (!strcmp(event->name, "sched_migrate_task"))
-		process_sched_migrate_task_event(data, session, event, cpu, timestamp, thread);
+		process_sched_migrate_task_event(data, machine, event, cpu, timestamp, thread);
 }
 
 static int process_sample_event(struct perf_event_ops *ops __used,
 				union perf_event *event,
 				struct perf_sample *sample,
 				struct perf_evsel *evsel,
-				struct perf_session *session)
+				struct machine *machine)
 {
 	struct thread *thread;
 
 	if (!(evsel->attr.sample_type & PERF_SAMPLE_RAW))
 		return 0;
 
-	thread = perf_session__findnew(session, sample->pid);
+	thread = machine__findnew_thread(machine, sample->pid);
 	if (thread == NULL) {
 		pr_debug("problem processing %d event, skipping it.\n",
 			 event->header.type);
@@ -1625,7 +1625,7 @@ static int process_sample_event(struct perf_event_ops *ops __used,
 	if (profile_cpu != -1 && profile_cpu != (int)sample->cpu)
 		return 0;
 
-	process_raw_event(event, session, sample->raw_data, sample->cpu,
+	process_raw_event(event, machine, sample->raw_data, sample->cpu,
 			  sample->time, thread);
 
 	return 0;

commit d20deb64e0490ee9442b5181bc08a62d2cadcb90
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Fri Nov 25 08:19:45 2011 -0200

    perf tools: Pass tool context in the the perf_event_ops functions
    
    So that we don't need to have that many globals.
    
    Next steps will remove the 'session' pointer, that in most cases is
    not needed.
    
    Then we can rename perf_event_ops to 'perf_tool' that better describes
    this class hierarchy.
    
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Link: http://lkml.kernel.org/n/tip-wp4djox7x6w1i2bab1pt4xxp@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index d51af0beab13..b11d6283fedf 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1602,7 +1602,8 @@ static void process_raw_event(union perf_event *raw_event __used,
 		process_sched_migrate_task_event(data, session, event, cpu, timestamp, thread);
 }
 
-static int process_sample_event(union perf_event *event,
+static int process_sample_event(struct perf_event_ops *ops __used,
+				union perf_event *event,
 				struct perf_sample *sample,
 				struct perf_evsel *evsel,
 				struct perf_session *session)

commit e3f42609628a20da92ecbc2d81053cc82c90a071
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Wed Nov 16 17:02:54 2011 -0200

    perf tools: Use evsel->attr.sample_type instead of session->sample_type
    
    Eventually session->sample_type will go away as we want to support
    multiple sample types per session, so use it from the evsel which is a
    step in that direction.
    
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Link: http://lkml.kernel.org/n/tip-0vwdpjcwbjezw459lw5n3ew1@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 5177964943e7..d51af0beab13 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -3,6 +3,7 @@
 
 #include "util/util.h"
 #include "util/cache.h"
+#include "util/evsel.h"
 #include "util/symbol.h"
 #include "util/thread.h"
 #include "util/header.h"
@@ -1603,12 +1604,12 @@ static void process_raw_event(union perf_event *raw_event __used,
 
 static int process_sample_event(union perf_event *event,
 				struct perf_sample *sample,
-				struct perf_evsel *evsel __used,
+				struct perf_evsel *evsel,
 				struct perf_session *session)
 {
 	struct thread *thread;
 
-	if (!(session->sample_type & PERF_SAMPLE_RAW))
+	if (!(evsel->attr.sample_type & PERF_SAMPLE_RAW))
 		return 0;
 
 	thread = perf_session__findnew(session, sample->pid);

commit 580cabed88ebc631e740b16010f2fa6ba882652f
Author: Jiri Olsa <jolsa@redhat.com>
Date:   Tue Aug 9 14:46:51 2011 +0200

    perf sched: Usage leftover from trace -> script rename
    
    The 'perf sched' command usage still showing 'trace' command instead of
    the 'script' command.
    
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20110809124651.GD2056@jolsa.brq.redhat.com
    Signed-off-by: Jiri Olsa <jolsa@redhat.com>
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 3d5702c8b512..5177964943e7 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1777,7 +1777,7 @@ static void __cmd_replay(void)
 
 
 static const char * const sched_usage[] = {
-	"perf sched [<options>] {record|latency|map|replay|trace}",
+	"perf sched [<options>] {record|latency|map|replay|script}",
 	NULL
 };
 

commit 4c09bafae37d870ab8efc50faeeb4855cb55b5b7
Author: Jiri Olsa <jolsa@redhat.com>
Date:   Mon Aug 8 23:03:34 2011 +0200

    perf sched: Do not delete session object prematurely
    
    The session object is released prematurely when processing events for
    latency command. The session's thread objects are used within the
    output_lat_thread function.
    
    Runnning following commands:
    
     # perf sched record
     # perf sched latency
    
    the latter displays incorrect data and might cause access violation.
    
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1312837414-3819-1-git-send-email-jolsa@redhat.com
    Signed-off-by: Jiri Olsa <jolsa@redhat.com>
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index dcfe8873c9a1..3d5702c8b512 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1637,23 +1637,29 @@ static struct perf_event_ops event_ops = {
 	.ordered_samples	= true,
 };
 
-static int read_events(void)
+static void read_events(bool destroy, struct perf_session **psession)
 {
 	int err = -EINVAL;
 	struct perf_session *session = perf_session__new(input_name, O_RDONLY,
 							 0, false, &event_ops);
 	if (session == NULL)
-		return -ENOMEM;
+		die("No Memory");
 
 	if (perf_session__has_traces(session, "record -R")) {
 		err = perf_session__process_events(session, &event_ops);
+		if (err)
+			die("Failed to process events, error %d", err);
+
 		nr_events      = session->hists.stats.nr_events[0];
 		nr_lost_events = session->hists.stats.total_lost;
 		nr_lost_chunks = session->hists.stats.nr_events[PERF_RECORD_LOST];
 	}
 
-	perf_session__delete(session);
-	return err;
+	if (destroy)
+		perf_session__delete(session);
+
+	if (psession)
+		*psession = session;
 }
 
 static void print_bad_events(void)
@@ -1689,9 +1695,10 @@ static void print_bad_events(void)
 static void __cmd_lat(void)
 {
 	struct rb_node *next;
+	struct perf_session *session;
 
 	setup_pager();
-	read_events();
+	read_events(false, &session);
 	sort_lat();
 
 	printf("\n ---------------------------------------------------------------------------------------------------------------\n");
@@ -1717,6 +1724,7 @@ static void __cmd_lat(void)
 	print_bad_events();
 	printf("\n");
 
+	perf_session__delete(session);
 }
 
 static struct trace_sched_handler map_ops  = {
@@ -1731,7 +1739,7 @@ static void __cmd_map(void)
 	max_cpu = sysconf(_SC_NPROCESSORS_CONF);
 
 	setup_pager();
-	read_events();
+	read_events(true, NULL);
 	print_bad_events();
 }
 
@@ -1744,7 +1752,7 @@ static void __cmd_replay(void)
 
 	test_calibrations();
 
-	read_events();
+	read_events(true, NULL);
 
 	printf("nr_run_events:        %ld\n", nr_run_events);
 	printf("nr_sleep_events:      %ld\n", nr_sleep_events);

commit 9e69c210822c4035708a6111567c96364ca244d5
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Tue Mar 15 15:44:01 2011 -0300

    perf session: Pass evsel in event_ops->sample()
    
    Resolving the sample->id to an evsel since the most advanced tools,
    report and annotate, and the others will too when they evolve to
    properly support multi-event perf.data files.
    
    Good also because it does an extra validation, checking that the ID is
    valid when present. When that is not the case, the overhead is just a
    branch + function call (perf_evlist__id2evsel).
    
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Tom Zanussi <tzanussi@gmail.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index a32f411faeac..dcfe8873c9a1 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1603,6 +1603,7 @@ static void process_raw_event(union perf_event *raw_event __used,
 
 static int process_sample_event(union perf_event *event,
 				struct perf_sample *sample,
+				struct perf_evsel *evsel __used,
 				struct perf_session *session)
 {
 	struct thread *thread;

commit fb7d0b3cefb80a105f7fd26bbc62e0cbf9192822
Author: Kyle McMartin <kyle@mcmartin.ca>
Date:   Mon Jan 24 11:13:04 2011 -0500

    perf tool: Fix gcc 4.6.0 issues
    
    GCC 4.6.0 in Fedora rawhide turned up some compile errors in tools/perf
    due to the -Werror=unused-but-set-variable flag.
    
    I've gone through and annotated some of the assignments that had side
    effects (ie: return value from a function) with the __used annotation,
    and in some cases, just removed unused code.
    
    In a few cases, we were assigning something useful, but not using it in
    later parts of the function.
    
    kyle@dreadnought:~/src% gcc --version
    gcc (GCC) 4.6.0 20110122 (Red Hat 4.6.0-0.3)
    
    Cc: Ingo Molnar <mingo@redhat.com>
    LKML-Reference: <20110124161304.GK27353@bombadil.infradead.org>
    Signed-off-by: Kyle McMartin <kyle@redhat.com>
    [ committer note: Fixed up the annotation fixes, as that code moved recently ]
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index ae2621182927..a32f411faeac 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -369,11 +369,6 @@ static void
 process_sched_event(struct task_desc *this_task __used, struct sched_atom *atom)
 {
 	int ret = 0;
-	u64 now;
-	long long delta;
-
-	now = get_nsecs();
-	delta = start_time + atom->timestamp - now;
 
 	switch (atom->type) {
 		case SCHED_EVENT_RUN:
@@ -562,7 +557,7 @@ static void wait_for_tasks(void)
 
 static void run_one_test(void)
 {
-	u64 T0, T1, delta, avg_delta, fluct, std_dev;
+	u64 T0, T1, delta, avg_delta, fluct;
 
 	T0 = get_nsecs();
 	wait_for_tasks();
@@ -578,7 +573,6 @@ static void run_one_test(void)
 	else
 		fluct = delta - avg_delta;
 	sum_fluct += fluct;
-	std_dev = sum_fluct / nr_runs / sqrt(nr_runs);
 	if (!run_avg)
 		run_avg = delta;
 	run_avg = (run_avg*9 + delta)/10;
@@ -799,7 +793,7 @@ replay_switch_event(struct trace_switch_event *switch_event,
 		    u64 timestamp,
 		    struct thread *thread __used)
 {
-	struct task_desc *prev, *next;
+	struct task_desc *prev, __used *next;
 	u64 timestamp0;
 	s64 delta;
 
@@ -1404,7 +1398,7 @@ map_switch_event(struct trace_switch_event *switch_event,
 		 u64 timestamp,
 		 struct thread *thread __used)
 {
-	struct thread *sched_out, *sched_in;
+	struct thread *sched_out __used, *sched_in;
 	int new_shortname;
 	u64 timestamp0;
 	s64 delta;

commit 8115d60c323dd9931b95221c0a392aeddc1d6ef3
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Sat Jan 29 14:01:45 2011 -0200

    perf tools: Kill event_t typedef, use 'union perf_event' instead
    
    And move the event_t methods to the perf_event__ too.
    
    No code changes, just namespace consistency.
    
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Tom Zanussi <tzanussi@gmail.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index ff993c8b175d..ae2621182927 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1580,9 +1580,9 @@ process_sched_migrate_task_event(void *data, struct perf_session *session,
 						 event, cpu, timestamp, thread);
 }
 
-static void
-process_raw_event(event_t *raw_event __used, struct perf_session *session,
-		  void *data, int cpu, u64 timestamp, struct thread *thread)
+static void process_raw_event(union perf_event *raw_event __used,
+			      struct perf_session *session, void *data, int cpu,
+			      u64 timestamp, struct thread *thread)
 {
 	struct event *event;
 	int type;
@@ -1607,7 +1607,8 @@ process_raw_event(event_t *raw_event __used, struct perf_session *session,
 		process_sched_migrate_task_event(data, session, event, cpu, timestamp, thread);
 }
 
-static int process_sample_event(event_t *event, struct perf_sample *sample,
+static int process_sample_event(union perf_event *event,
+				struct perf_sample *sample,
 				struct perf_session *session)
 {
 	struct thread *thread;
@@ -1635,9 +1636,9 @@ static int process_sample_event(event_t *event, struct perf_sample *sample,
 
 static struct perf_event_ops event_ops = {
 	.sample			= process_sample_event,
-	.comm			= event__process_comm,
-	.lost			= event__process_lost,
-	.fork			= event__process_task,
+	.comm			= perf_event__process_comm,
+	.lost			= perf_event__process_lost,
+	.fork			= perf_event__process_task,
 	.ordered_samples	= true,
 };
 

commit 8d50e5b4171a69cf48ca94a1e7c14033d0b4771d
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Sat Jan 29 13:02:00 2011 -0200

    perf tools: Rename 'struct sample_data' to 'struct perf_sample'
    
    Making the namespace more uniform.
    
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Tom Zanussi <tzanussi@gmail.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 29acb894e035..ff993c8b175d 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1607,7 +1607,7 @@ process_raw_event(event_t *raw_event __used, struct perf_session *session,
 		process_sched_migrate_task_event(data, session, event, cpu, timestamp, thread);
 }
 
-static int process_sample_event(event_t *event, struct sample_data *sample,
+static int process_sample_event(event_t *event, struct perf_sample *sample,
 				struct perf_session *session)
 {
 	struct thread *thread;

commit 9486aa38771661e96fbb51c549b9901b5df609d8
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Sat Jan 22 20:37:02 2011 -0200

    perf tools: Fix 64 bit integer format strings
    
    Using %L[uxd] has issues in some architectures, like on ppc64.  Fix it
    by making our 64 bit integers typedefs of stdint.h types and using
    PRI[ux]64 like, for instance, git does.
    
    Reported by Denis Kirjanov that provided a patch for one case, I went
    and changed all cases.
    
    Reported-by: Denis Kirjanov <dkirjanov@kernel.org>
    Tested-by: Denis Kirjanov <dkirjanov@kernel.org>
    LKML-Reference: <20110120093246.GA8031@hera.kernel.org>
    Cc: Denis Kirjanov <dkirjanov@kernel.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Pingtian Han <phan@redhat.com>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Tom Zanussi <tzanussi@gmail.com>
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 29e7ffd85690..29acb894e035 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -193,7 +193,7 @@ static void calibrate_run_measurement_overhead(void)
 	}
 	run_measurement_overhead = min_delta;
 
-	printf("run measurement overhead: %Ld nsecs\n", min_delta);
+	printf("run measurement overhead: %" PRIu64 " nsecs\n", min_delta);
 }
 
 static void calibrate_sleep_measurement_overhead(void)
@@ -211,7 +211,7 @@ static void calibrate_sleep_measurement_overhead(void)
 	min_delta -= 10000;
 	sleep_measurement_overhead = min_delta;
 
-	printf("sleep measurement overhead: %Ld nsecs\n", min_delta);
+	printf("sleep measurement overhead: %" PRIu64 " nsecs\n", min_delta);
 }
 
 static struct sched_atom *
@@ -617,13 +617,13 @@ static void test_calibrations(void)
 	burn_nsecs(1e6);
 	T1 = get_nsecs();
 
-	printf("the run test took %Ld nsecs\n", T1-T0);
+	printf("the run test took %" PRIu64 " nsecs\n", T1 - T0);
 
 	T0 = get_nsecs();
 	sleep_nsecs(1e6);
 	T1 = get_nsecs();
 
-	printf("the sleep test took %Ld nsecs\n", T1-T0);
+	printf("the sleep test took %" PRIu64 " nsecs\n", T1 - T0);
 }
 
 #define FILL_FIELD(ptr, field, event, data)	\
@@ -816,10 +816,10 @@ replay_switch_event(struct trace_switch_event *switch_event,
 		delta = 0;
 
 	if (delta < 0)
-		die("hm, delta: %Ld < 0 ?\n", delta);
+		die("hm, delta: %" PRIu64 " < 0 ?\n", delta);
 
 	if (verbose) {
-		printf(" ... switch from %s/%d to %s/%d [ran %Ld nsecs]\n",
+		printf(" ... switch from %s/%d to %s/%d [ran %" PRIu64 " nsecs]\n",
 			switch_event->prev_comm, switch_event->prev_pid,
 			switch_event->next_comm, switch_event->next_pid,
 			delta);
@@ -1048,7 +1048,7 @@ latency_switch_event(struct trace_switch_event *switch_event,
 		delta = 0;
 
 	if (delta < 0)
-		die("hm, delta: %Ld < 0 ?\n", delta);
+		die("hm, delta: %" PRIu64 " < 0 ?\n", delta);
 
 
 	sched_out = perf_session__findnew(session, switch_event->prev_pid);
@@ -1221,7 +1221,7 @@ static void output_lat_thread(struct work_atoms *work_list)
 
 	avg = work_list->total_lat / work_list->nb_atoms;
 
-	printf("|%11.3f ms |%9llu | avg:%9.3f ms | max:%9.3f ms | max at: %9.6f s\n",
+	printf("|%11.3f ms |%9" PRIu64 " | avg:%9.3f ms | max:%9.3f ms | max at: %9.6f s\n",
 	      (double)work_list->total_runtime / 1e6,
 		 work_list->nb_atoms, (double)avg / 1e6,
 		 (double)work_list->max_lat / 1e6,
@@ -1423,7 +1423,7 @@ map_switch_event(struct trace_switch_event *switch_event,
 		delta = 0;
 
 	if (delta < 0)
-		die("hm, delta: %Ld < 0 ?\n", delta);
+		die("hm, delta: %" PRIu64 " < 0 ?\n", delta);
 
 
 	sched_out = perf_session__findnew(session, switch_event->prev_pid);
@@ -1713,7 +1713,7 @@ static void __cmd_lat(void)
 	}
 
 	printf(" -----------------------------------------------------------------------------------------\n");
-	printf("  TOTAL:                |%11.3f ms |%9Ld |\n",
+	printf("  TOTAL:                |%11.3f ms |%9" PRIu64 " |\n",
 		(double)all_runtime/1e6, all_count);
 
 	printf(" ---------------------------------------------------\n");

commit 9710118bd4e7f3406865171cb9b9c94547c1c2f9
Author: Stephane Eranian <eranian@google.com>
Date:   Wed Jan 12 10:29:05 2011 +0100

    perf sched: Fix list of events, dropping unsupported ':r' modifier
    
    Looks to me like the :r modifier is not supported anymore, so remove it from
    the list of events.
    
    Cc: Corey Ashford <cjashfor@linux.vnet.ibm.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Robert Richter <robert.richter@amd.com>
    LKML-Reference: <AANLkTim=jawJyBj0iFd0r4-LCKzvjFW+NddzJMD5GUB9@mail.gmail.com>
    Signed-off-by: Stephane Eranian <eranian@google.com>
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index abd4b8497bc4..29e7ffd85690 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1843,15 +1843,15 @@ static const char *record_args[] = {
 	"-f",
 	"-m", "1024",
 	"-c", "1",
-	"-e", "sched:sched_switch:r",
-	"-e", "sched:sched_stat_wait:r",
-	"-e", "sched:sched_stat_sleep:r",
-	"-e", "sched:sched_stat_iowait:r",
-	"-e", "sched:sched_stat_runtime:r",
-	"-e", "sched:sched_process_exit:r",
-	"-e", "sched:sched_process_fork:r",
-	"-e", "sched:sched_wakeup:r",
-	"-e", "sched:sched_migrate_task:r",
+	"-e", "sched:sched_switch",
+	"-e", "sched:sched_stat_wait",
+	"-e", "sched:sched_stat_sleep",
+	"-e", "sched:sched_stat_iowait",
+	"-e", "sched:sched_stat_runtime",
+	"-e", "sched:sched_process_exit",
+	"-e", "sched:sched_process_fork",
+	"-e", "sched:sched_wakeup",
+	"-e", "sched:sched_migrate_task",
 };
 
 static int __cmd_record(int argc, const char **argv)

commit 12f7e0364375ba1ba55abcc5ac082b68fb526c80
Author: Jiri Pirko <jpirko@redhat.com>
Date:   Mon Jan 10 14:14:23 2011 -0200

    perf sched: Use PTHREAD_STACK_MIN to avoid pthread_attr_setstacksize() fail
    
    on ppc64:
    /usr/include/bits/local_lim.h:#define PTHREAD_STACK_MIN 131072
    
    therefore following set of commands:
    
    gives:
    perf.2.6.37test: builtin-sched.c:493: create_tasks: Assertion `!(err)' failed.
    
    So make sure we do not set stack size lower than PTHREAD_STACK_MIN.
    
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    LKML-Reference: <20110110160417.GB2685@psychotron.brq.redhat.com>
    Signed-off-by: Jiri Pirko <jpirko@redhat.com>
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 54024d2a017e..abd4b8497bc4 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -489,7 +489,8 @@ static void create_tasks(void)
 
 	err = pthread_attr_init(&attr);
 	BUG_ON(err);
-	err = pthread_attr_setstacksize(&attr, (size_t)(16*1024));
+	err = pthread_attr_setstacksize(&attr,
+			(size_t) max(16 * 1024, PTHREAD_STACK_MIN));
 	BUG_ON(err);
 	err = pthread_mutex_lock(&start_work_mutex);
 	BUG_ON(err);

commit e462dc553ea5e09d4713e7c35a11ed331dc6f369
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Mon Jan 10 10:48:47 2011 -0200

    perf sched: Fix allocation result check
    
    Bug introduced in ce47dc56.
    
    Reported-by: Mike Galbraith <efault@gmx.de>
    Cc: Chris Samuel <chris@csamuel.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Tom Zanussi <tzanussi@gmail.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 7a4ebeb8b016..54024d2a017e 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1861,7 +1861,7 @@ static int __cmd_record(int argc, const char **argv)
 	rec_argc = ARRAY_SIZE(record_args) + argc - 1;
 	rec_argv = calloc(rec_argc + 1, sizeof(char *));
 
-	if (rec_argv)
+	if (rec_argv == NULL)
 		return -ENOMEM;
 
 	for (i = 0; i < ARRAY_SIZE(record_args); i++)

commit 21ef97f05a7da5bc23b26cb34d6746f83ca9bf20
Author: Ian Munsie <imunsie@au1.ibm.com>
Date:   Fri Dec 10 14:09:16 2010 +1100

    perf session: Fallback to unordered processing if no sample_id_all
    
    If we are running the new perf on an old kernel without support for
    sample_id_all, we should fall back to the old unordered processing of
    events. If we didn't than we would *always* process events without
    timestamps out of order, whether or not we hit a reordering race. In
    other words, instead of there being a chance of not attributing samples
    correctly, we would guarantee that samples would not be attributed.
    
    While processing all events without timestamps before events with
    timestamps may seem like an intuitive solution, it falls down as
    PERF_RECORD_EXIT events would also be processed before any samples.
    Even with a workaround for that case, samples before/after an exec would
    not be attributed correctly.
    
    This patch allows commands to indicate whether they need to fall back to
    unordered processing, so that commands that do not care about timestamps
    on every event will not be affected. If we do fallback, this will print
    out a warning if report -D was invoked.
    
    This patch adds the test in perf_session__new so that we only need to
    test once per session. Commands that do not use an event_ops (such as
    record and top) can simply pass NULL in it's place.
    
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    LKML-Reference: <1291951882-sup-6069@au1.ibm.com>
    Signed-off-by: Ian Munsie <imunsie@au1.ibm.com>
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index c7753940aea0..7a4ebeb8b016 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1643,7 +1643,8 @@ static struct perf_event_ops event_ops = {
 static int read_events(void)
 {
 	int err = -EINVAL;
-	struct perf_session *session = perf_session__new(input_name, O_RDONLY, 0, false);
+	struct perf_session *session = perf_session__new(input_name, O_RDONLY,
+							 0, false, &event_ops);
 	if (session == NULL)
 		return -ENOMEM;
 

commit ce47dc56a2241dc035160a85bc5e34283cdd622c
Author: Chris Samuel <chris@csamuel.org>
Date:   Sat Nov 13 13:35:06 2010 +1100

    perf tools: Catch a few uncheck calloc/malloc's
    
    There were a few stray calloc()'s and malloc()'s which were not having
    their return values checked for success.
    
    As the calling code either already coped with failure or didn't actually
    care we just return -ENOMEM at that point.
    
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Chris Samuel <chris@csamuel.org>
    LKML-Reference: <4CDDF95A.1050400@csamuel.org>
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 73d1e30b0fed..c7753940aea0 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1860,6 +1860,9 @@ static int __cmd_record(int argc, const char **argv)
 	rec_argc = ARRAY_SIZE(record_args) + argc - 1;
 	rec_argv = calloc(rec_argc + 1, sizeof(char *));
 
+	if (rec_argv)
+		return -ENOMEM;
+
 	for (i = 0; i < ARRAY_SIZE(record_args); i++)
 		rec_argv[i] = strdup(record_args[i]);
 

commit 640c03ce837fe8d4b56342aba376ea0da3960459
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Thu Dec 2 14:10:21 2010 -0200

    perf session: Parse sample earlier
    
    At perf_session__process_event, so that we reduce the number of lines in eache
    tool sample processing routine that now receives a sample_data pointer already
    parsed.
    
    This will also be useful in the next patch, where we'll allow sample the
    identity fields in MMAP, FORK, EXIT, etc, when it will be possible to see (cpu,
    timestamp) just after before every event.
    
    Also validate callchains in perf_session__process_event, i.e. as early as
    possible, and keep a counter of the number of events discarded due to invalid
    callchains, warning the user about it if it happens.
    
    There is an assumption that was kept that all events have the same sample_type,
    that will be dealt with in the future, when this preexisting limitation will be
    removed.
    
    Tested-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Ian Munsie <imunsie@au1.ibm.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Frédéric Weisbecker <fweisbec@gmail.com>
    Cc: Ian Munsie <imunsie@au1.ibm.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Stephane Eranian <eranian@google.com>
    LKML-Reference: <1291318772-30880-4-git-send-email-acme@infradead.org>
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 26523c939791..73d1e30b0fed 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1606,25 +1606,15 @@ process_raw_event(event_t *raw_event __used, struct perf_session *session,
 		process_sched_migrate_task_event(data, session, event, cpu, timestamp, thread);
 }
 
-static int process_sample_event(event_t *event, struct perf_session *session)
+static int process_sample_event(event_t *event, struct sample_data *sample,
+				struct perf_session *session)
 {
-	struct sample_data data;
 	struct thread *thread;
 
 	if (!(session->sample_type & PERF_SAMPLE_RAW))
 		return 0;
 
-	memset(&data, 0, sizeof(data));
-	data.time = -1;
-	data.cpu = -1;
-	data.period = -1;
-
-	event__parse_sample(event, session->sample_type, &data);
-
-	dump_printf("(IP, %d): %d/%d: %#Lx period: %Ld\n", event->header.misc,
-		    data.pid, data.tid, data.ip, data.period);
-
-	thread = perf_session__findnew(session, data.pid);
+	thread = perf_session__findnew(session, sample->pid);
 	if (thread == NULL) {
 		pr_debug("problem processing %d event, skipping it.\n",
 			 event->header.type);
@@ -1633,10 +1623,11 @@ static int process_sample_event(event_t *event, struct perf_session *session)
 
 	dump_printf(" ... thread: %s:%d\n", thread->comm, thread->pid);
 
-	if (profile_cpu != -1 && profile_cpu != (int)data.cpu)
+	if (profile_cpu != -1 && profile_cpu != (int)sample->cpu)
 		return 0;
 
-	process_raw_event(event, session, data.raw_data, data.cpu, data.time, thread);
+	process_raw_event(event, session, sample->raw_data, sample->cpu,
+			  sample->time, thread);
 
 	return 0;
 }

commit 133dc4c39c57eeef2577ca5b4ed24765b7a78ce2
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Nov 16 18:45:39 2010 +0100

    perf: Rename 'perf trace' to 'perf script'
    
    Free the perf trace name space and rename the trace to 'script' which is a
    better match for the scripting engine.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 55f3b5dcc731..26523c939791 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1888,10 +1888,10 @@ int cmd_sched(int argc, const char **argv, const char *prefix __used)
 		usage_with_options(sched_usage, sched_options);
 
 	/*
-	 * Aliased to 'perf trace' for now:
+	 * Aliased to 'perf script' for now:
 	 */
-	if (!strcmp(argv[0], "trace"))
-		return cmd_trace(argc, argv, prefix);
+	if (!strcmp(argv[0], "script"))
+		return cmd_script(argc, argv, prefix);
 
 	symbol__init();
 	if (!strncmp(argv[0], "rec", 3)) {

commit af64865ba6825e10fe62a3105fea5cadffeb52c6
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Jun 1 00:03:40 2010 +0200

    perf: Use event__process_task from perf sched
    
    perf sched uses event__process_comm(), which means it can resolve
    comms from:
    
    - tasks that have exec'ed (kernel comm events)
    - tasks that were running when perf record started the actual
      recording (synthetized comm events)
    
    But perf sched can't resolve the pids of tasks that were created
    after the recording started.
    
    To solve this, we need to inherit the comms on fork events using
    event__process_task().
    
    This fixes various unresolved pids in perf sched, easily visible
    with:
            perf sched record perf bench sched messaging
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Tom Zanussi <tzanussi@gmail.com>
    Cc: Stephane Eranian <eranian@google.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index f67bce2a83b4..55f3b5dcc731 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1645,6 +1645,7 @@ static struct perf_event_ops event_ops = {
 	.sample			= process_sample_event,
 	.comm			= event__process_comm,
 	.lost			= event__process_lost,
+	.fork			= event__process_task,
 	.ordered_samples	= true,
 };
 

commit edb7c60e27c1baff38d82440dc52eaffac9a45f4
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Mon May 17 16:22:41 2010 -0300

    perf options: Type check all the remaining OPT_ variants
    
    OPT_SET_INT was renamed to OPT_SET_UINT since the only use in these
    tools is to set something that has an enum type, that is builtin
    compatible with unsigned int.
    
    Several string constifications were done to make OPT_STRING require a
    const char * type.
    
    Cc: Frédéric Weisbecker <fweisbec@gmail.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Tom Zanussi <tzanussi@gmail.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index c80acdf927a7..f67bce2a83b4 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -22,7 +22,7 @@
 static char			const *input_name = "perf.data";
 
 static char			default_sort_order[] = "avg, max, switch, runtime";
-static char			*sort_order = default_sort_order;
+static const char		*sort_order = default_sort_order;
 
 static int			profile_cpu = -1;
 

commit 1967936d688c475b85d34d84e09858cf514c893c
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Mon May 17 15:39:16 2010 -0300

    perf options: Check v type in OPT_U?INTEGER
    
    To avoid problems like the one fixed by Stephane Eranian in 3de29ca, now
    we'll got this instead:
    
            bench/sched-messaging.c:259: error: negative width in bit-field ‘<anonymous>’
            bench/sched-messaging.c:261: error: negative width in bit-field ‘<anonymous>’
    
    Which is rather cryptic, but is how BUILD_BUG_ON_ZERO works, so kernel
    hackers should be already used to this.
    
    With it in place found some problems, fixed by changing the affected
    variables to sensible types or changed some OPT_INTEGER to OPT_UINTEGER.
    
    Next csets will go thru converting each of the remaining OPT_ so that
    review can be made easier by grouping changes per type per patch.
    
    Cc: Frédéric Weisbecker <fweisbec@gmail.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Tom Zanussi <tzanussi@gmail.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index be7bc9264710..c80acdf927a7 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -105,7 +105,7 @@ static u64			sum_runtime;
 static u64			sum_fluct;
 static u64			run_avg;
 
-static unsigned long		replay_repeat = 10;
+static unsigned int		replay_repeat = 10;
 static unsigned long		nr_timestamps;
 static unsigned long		nr_unordered_timestamps;
 static unsigned long		nr_state_machine_bugs;
@@ -1816,8 +1816,8 @@ static const char * const replay_usage[] = {
 };
 
 static const struct option replay_options[] = {
-	OPT_INTEGER('r', "repeat", &replay_repeat,
-		    "repeat the workload replay N times (-1: infinite)"),
+	OPT_UINTEGER('r', "repeat", &replay_repeat,
+		     "repeat the workload replay N times (-1: infinite)"),
 	OPT_INCR('v', "verbose", &verbose,
 		    "be more verbose (show symbol address, etc)"),
 	OPT_BOOLEAN('D', "dump-raw-trace", &dump_trace,

commit cee75ac7ecc27084accdb9d9d6fde65a09f047ae
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Fri May 14 13:16:55 2010 -0300

    perf hist: Clarify events_stats fields usage
    
    The events_stats.total field is too generic, rename it to .total_period,
    and also add a comment explaining that it is the sum of all the .period
    fields in samples, that is needed because we use auto-freq to avoid
    sampling artifacts.
    
    Ditto for events_stats.lost, that is the sum of all lost_event.lost
    fields, i.e. the number of events the kernel dropped.
    
    Looking at the users, builtin-sched.c can make use of these fields and
    stop doing it again.
    
    Cc: Frédéric Weisbecker <fweisbec@gmail.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Tom Zanussi <tzanussi@gmail.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index aef6ed0e119c..be7bc9264710 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1641,19 +1641,10 @@ static int process_sample_event(event_t *event, struct perf_session *session)
 	return 0;
 }
 
-static int process_lost_event(event_t *event __used,
-			      struct perf_session *session __used)
-{
-	nr_lost_chunks++;
-	nr_lost_events += event->lost.lost;
-
-	return 0;
-}
-
 static struct perf_event_ops event_ops = {
 	.sample			= process_sample_event,
 	.comm			= event__process_comm,
-	.lost			= process_lost_event,
+	.lost			= event__process_lost,
 	.ordered_samples	= true,
 };
 
@@ -1664,8 +1655,12 @@ static int read_events(void)
 	if (session == NULL)
 		return -ENOMEM;
 
-	if (perf_session__has_traces(session, "record -R"))
+	if (perf_session__has_traces(session, "record -R")) {
 		err = perf_session__process_events(session, &event_ops);
+		nr_events      = session->hists.stats.nr_events[0];
+		nr_lost_events = session->hists.stats.total_lost;
+		nr_lost_chunks = session->hists.stats.nr_events[PERF_RECORD_LOST];
+	}
 
 	perf_session__delete(session);
 	return err;

commit 454c407ec17a0c63e4023ac0877d687945a7df4a
Author: Tom Zanussi <tzanussi@gmail.com>
Date:   Sat May 1 01:41:20 2010 -0500

    perf: add perf-inject builtin
    
    Currently, perf 'live mode' writes build-ids at the end of the
    session, which isn't actually useful for processing live mode events.
    
    What would be better would be to have the build-ids sent before any of
    the samples that reference them, which can be done by processing the
    event stream and retrieving the build-ids on the first hit.  Doing
    that in perf-record itself, however, is off-limits.
    
    This patch introduces perf-inject, which does the same job while
    leaving perf-record untouched.  Normal mode perf still records the
    build-ids at the end of the session as it should, but for live mode,
    perf-inject can be injected in between the record and report steps
    e.g.:
    
    perf record -o - ./hackbench 10 | perf inject -v -b | perf report -v -i -
    
    perf-inject reads a perf-record event stream and repipes it to stdout.
    At any point the processing code can inject other events into the
    event stream - in this case build-ids (-b option) are read and
    injected as needed into the event stream.
    
    Build-ids are just the first user of perf-inject - potentially
    anything that needs userspace processing to augment the trace stream
    with additional information could make use of this facility.
    
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Frédéric Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <1272696080-16435-3-git-send-email-tzanussi@gmail.com>
    Signed-off-by: Tom Zanussi <tzanussi@gmail.com>
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 94453f1e4e01..aef6ed0e119c 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1660,7 +1660,7 @@ static struct perf_event_ops event_ops = {
 static int read_events(void)
 {
 	int err = -EINVAL;
-	struct perf_session *session = perf_session__new(input_name, O_RDONLY, 0);
+	struct perf_session *session = perf_session__new(input_name, O_RDONLY, 0, false);
 	if (session == NULL)
 		return -ENOMEM;
 

commit a64eae703b390185abe1d15fa932b48f04fa7fbb
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Apr 24 00:29:40 2010 +0200

    perf: Use generic sample reordering in perf sched
    
    Use the new generic sample events reordering from perf sched,
    this drops the need of multiplexing the buffers on record time,
    improving the scalability of perf sched.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Hitoshi Mitake <mitake@dcl.info.waseda.ac.jp>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Masami Hiramatsu <mhiramat@redhat.com>
    Cc: Tom Zanussi <tzanussi@gmail.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 09ddc8e6d8e1..94453f1e4e01 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1651,9 +1651,10 @@ static int process_lost_event(event_t *event __used,
 }
 
 static struct perf_event_ops event_ops = {
-	.sample	= process_sample_event,
-	.comm	= event__process_comm,
-	.lost	= process_lost_event,
+	.sample			= process_sample_event,
+	.comm			= event__process_comm,
+	.lost			= process_lost_event,
+	.ordered_samples	= true,
 };
 
 static int read_events(void)
@@ -1850,7 +1851,6 @@ static const char *record_args[] = {
 	"record",
 	"-a",
 	"-R",
-	"-M",
 	"-f",
 	"-m", "1024",
 	"-c", "1",

commit c05556421742eb47f80301767653a4bcb19de9de
Author: Ian Munsie <imunsie@au.ibm.com>
Date:   Tue Apr 13 18:37:33 2010 +1000

    perf: Fix endianness argument compatibility with OPT_BOOLEAN() and introduce OPT_INCR()
    
    Parsing an option from the command line with OPT_BOOLEAN on a
    bool data type would not work on a big-endian machine due to the
    manner in which the boolean was being cast into an int and
    incremented. For example, running 'perf probe --list' on a
    PowerPC machine would fail to properly set the list_events bool
    and would therefore print out the usage information and
    terminate.
    
    This patch makes OPT_BOOLEAN work as expected with a bool
    datatype. For cases where the original OPT_BOOLEAN was
    intentionally being used to increment an int each time it was
    passed in on the command line, this patch introduces OPT_INCR
    with the old behaviour of OPT_BOOLEAN (the verbose variable is
    currently the only such example of this).
    
    I have reviewed every use of OPT_BOOLEAN to verify that a true
    C99 bool was passed. Where integers were used, I verified that
    they were only being used for boolean logic and changed them to
    bools to ensure that they would not be mistakenly used as ints.
    The major exception was the verbose variable which now uses
    OPT_INCR instead of OPT_BOOLEAN.
    
    Signed-off-by: Ian Munsie <imunsie@au.ibm.com>
    Acked-by: David S. Miller <davem@davemloft.net>
    Cc: <stable@kernel.org> # NOTE: wont apply to .3[34].x cleanly, please backport
    Cc: Git development list <git@vger.kernel.org>
    Cc: Ian Munsie <imunsie@au1.ibm.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Hitoshi Mitake <mitake@dcl.info.waseda.ac.jp>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Eric B Munson <ebmunson@us.ibm.com>
    Cc: Valdis.Kletnieks@vt.edu
    Cc: WANG Cong <amwang@redhat.com>
    Cc: Thiago Farina <tfransosi@gmail.com>
    Cc: Masami Hiramatsu <mhiramat@redhat.com>
    Cc: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
    Cc: Jaswinder Singh Rajput <jaswinderrajput@gmail.com>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: OGAWA Hirofumi <hirofumi@mail.parknet.co.jp>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Tom Zanussi <tzanussi@gmail.com>
    Cc: Anton Blanchard <anton@samba.org>
    Cc: John Kacur <jkacur@redhat.com>
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    LKML-Reference: <1271147857-11604-1-git-send-email-imunsie@au.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 5e59c0c40c40..09ddc8e6d8e1 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1790,7 +1790,7 @@ static const char * const sched_usage[] = {
 static const struct option sched_options[] = {
 	OPT_STRING('i', "input", &input_name, "file",
 		    "input file name"),
-	OPT_BOOLEAN('v', "verbose", &verbose,
+	OPT_INCR('v', "verbose", &verbose,
 		    "be more verbose (show symbol address, etc)"),
 	OPT_BOOLEAN('D', "dump-raw-trace", &dump_trace,
 		    "dump raw trace in ASCII"),
@@ -1805,7 +1805,7 @@ static const char * const latency_usage[] = {
 static const struct option latency_options[] = {
 	OPT_STRING('s', "sort", &sort_order, "key[,key2...]",
 		   "sort by key(s): runtime, switch, avg, max"),
-	OPT_BOOLEAN('v', "verbose", &verbose,
+	OPT_INCR('v', "verbose", &verbose,
 		    "be more verbose (show symbol address, etc)"),
 	OPT_INTEGER('C', "CPU", &profile_cpu,
 		    "CPU to profile on"),
@@ -1822,7 +1822,7 @@ static const char * const replay_usage[] = {
 static const struct option replay_options[] = {
 	OPT_INTEGER('r', "repeat", &replay_repeat,
 		    "repeat the workload replay N times (-1: infinite)"),
-	OPT_BOOLEAN('v', "verbose", &verbose,
+	OPT_INCR('v', "verbose", &verbose,
 		    "be more verbose (show symbol address, etc)"),
 	OPT_BOOLEAN('D', "dump-raw-trace", &dump_trace,
 		    "dump raw trace in ASCII"),

commit eed05fe70f96b04ebeb218b07ae8898e605f9b23
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Mon Apr 5 12:53:45 2010 -0300

    perf tools: Reorganize some structs to save space
    
    Using 'pahole --packable' I found some structs that could be reorganized
    to eliminate alignment holes, in some cases getting them to be cacheline
    multiples.
    
    [acme@doppio linux-2.6-tip]$ codiff perf.old ~/bin/perf
    builtin-annotate.c:
      struct perf_session    |   -8
      struct perf_header     |   -8
     2 structs changed
    
    builtin-diff.c:
      struct sample_data         |   -8
     1 struct changed
      diff__process_sample_event |   -8
     1 function changed, 8 bytes removed, diff: -8
    
    builtin-sched.c:
      struct sched_atom      |   -8
     1 struct changed
    
    builtin-timechart.c:
      struct per_pid         |   -8
     1 struct changed
      cmd_timechart          |  -16
     1 function changed, 16 bytes removed, diff: -16
    
    builtin-probe.c:
      struct perf_probe_point |   -8
      struct perf_probe_event |   -8
     2 structs changed
      opt_add_probe_event     |   -3
     1 function changed, 3 bytes removed, diff: -3
    
    util/probe-finder.c:
      struct probe_finder      |   -8
     1 struct changed
      find_kprobe_trace_events |  -16
     1 function changed, 16 bytes removed, diff: -16
    
    /home/acme/bin/perf:
     4 functions changed, 43 bytes removed, diff: -43
    [acme@doppio linux-2.6-tip]$
    
    Cc: Frédéric Weisbecker <fweisbec@gmail.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Mackerras <paulus@samba.org>
    LKML-Reference: <new-submission>
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 4f5a03e43444..5e59c0c40c40 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -68,10 +68,10 @@ enum sched_event_type {
 
 struct sched_atom {
 	enum sched_event_type	type;
+	int			specific_wait;
 	u64			timestamp;
 	u64			duration;
 	unsigned long		nr;
-	int			specific_wait;
 	sem_t			*wait_sem;
 	struct task_desc	*wakee;
 };

commit 0d755034dbd01e240eadf2d31f4f75d3088ccd21
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Thu Jan 14 12:23:09 2010 -0200

    perf tools: Don't cast RIP to pointers
    
    Since they can come from another architecture with bigger
    pointers, i.e. processing a 64-bit perf.data on a 32-bit arch.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frédéric Weisbecker <fweisbec@gmail.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Mackerras <paulus@samba.org>
    LKML-Reference: <1263478990-8200-1-git-send-email-acme@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 702322f8fec1..4f5a03e43444 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1621,11 +1621,8 @@ static int process_sample_event(event_t *event, struct perf_session *session)
 
 	event__parse_sample(event, session->sample_type, &data);
 
-	dump_printf("(IP, %d): %d/%d: %p period: %Ld\n",
-		event->header.misc,
-		data.pid, data.tid,
-		(void *)(long)data.ip,
-		(long long)data.period);
+	dump_printf("(IP, %d): %d/%d: %#Lx period: %Ld\n", event->header.misc,
+		    data.pid, data.tid, data.ip, data.period);
 
 	thread = perf_session__findnew(session, data.pid);
 	if (thread == NULL) {

commit 55aa640f54280da25046acd2075842d464f451e6
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Sun Dec 27 21:37:05 2009 -0200

    perf session: Remove redundant prefix & suffix from perf_event_ops
    
    Since now all that we have are perf event handlers, leave just
    the name of the event.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frédéric Weisbecker <fweisbec@gmail.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Mackerras <paulus@samba.org>
    LKML-Reference: <1261957026-15580-9-git-send-email-acme@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index e862e71f4e68..702322f8fec1 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1654,9 +1654,9 @@ static int process_lost_event(event_t *event __used,
 }
 
 static struct perf_event_ops event_ops = {
-	.process_sample_event	= process_sample_event,
-	.process_comm_event	= event__process_comm,
-	.process_lost_event	= process_lost_event,
+	.sample	= process_sample_event,
+	.comm	= event__process_comm,
+	.lost	= process_lost_event,
 };
 
 static int read_events(void)

commit d549c7690190d9739005e19604faad6da4b802ac
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Sun Dec 27 21:37:02 2009 -0200

    perf session: Remove sample_type_check from event_ops
    
    This is really something tools need to do before asking for the
    events to be processed, leaving perf_session__process_events to
    do just that, process events.
    
    Also add a msg parameter to perf_session__has_traces() so that
    the right message can be printed, fixing a regression added by
    me in the previous cset (right timechart message) and also
    fixing 'perf kmem', that was not asking if 'perf kmem record'
    was ran.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frédéric Weisbecker <fweisbec@gmail.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Mackerras <paulus@samba.org>
    LKML-Reference: <1261957026-15580-6-git-send-email-acme@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index d65098c42990..e862e71f4e68 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1657,17 +1657,18 @@ static struct perf_event_ops event_ops = {
 	.process_sample_event	= process_sample_event,
 	.process_comm_event	= event__process_comm,
 	.process_lost_event	= process_lost_event,
-	.sample_type_check	= perf_session__has_traces,
 };
 
 static int read_events(void)
 {
-	int err;
+	int err = -EINVAL;
 	struct perf_session *session = perf_session__new(input_name, O_RDONLY, 0);
 	if (session == NULL)
 		return -ENOMEM;
 
-	err = perf_session__process_events(session, &event_ops);
+	if (perf_session__has_traces(session, "record -R"))
+		err = perf_session__process_events(session, &event_ops);
+
 	perf_session__delete(session);
 	return err;
 }

commit 27295592c22e71bbd38110c302da8dbb43912a60
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Sun Dec 27 21:37:01 2009 -0200

    perf session: Share the common trace sample_check routine as perf_session__has_traces
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frédéric Weisbecker <fweisbec@gmail.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Mackerras <paulus@samba.org>
    LKML-Reference: <1261957026-15580-5-git-send-email-acme@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 80209df6cfe8..d65098c42990 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1653,23 +1653,11 @@ static int process_lost_event(event_t *event __used,
 	return 0;
 }
 
-static int sample_type_check(struct perf_session *session __used)
-{
-	if (!(session->sample_type & PERF_SAMPLE_RAW)) {
-		fprintf(stderr,
-			"No trace sample to read. Did you call perf record "
-			"without -R?");
-		return -1;
-	}
-
-	return 0;
-}
-
 static struct perf_event_ops event_ops = {
 	.process_sample_event	= process_sample_event,
 	.process_comm_event	= event__process_comm,
 	.process_lost_event	= process_lost_event,
-	.sample_type_check	= sample_type_check,
+	.sample_type_check	= perf_session__has_traces,
 };
 
 static int read_events(void)

commit 75be6cf48738aec68aac49b428423569492cfba3
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Tue Dec 15 20:04:39 2009 -0200

    perf symbols: Make symbol_conf global
    
    This simplifies a lot of functions, less stuff to be done by
    tool writers.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frédéric Weisbecker <fweisbec@gmail.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Mackerras <paulus@samba.org>
    LKML-Reference: <1260914682-29652-1-git-send-email-acme@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index d67f274adba0..80209df6cfe8 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1675,8 +1675,7 @@ static struct perf_event_ops event_ops = {
 static int read_events(void)
 {
 	int err;
-	struct perf_session *session = perf_session__new(input_name, O_RDONLY,
-							 0, NULL);
+	struct perf_session *session = perf_session__new(input_name, O_RDONLY, 0);
 	if (session == NULL)
 		return -ENOMEM;
 
@@ -1912,7 +1911,7 @@ int cmd_sched(int argc, const char **argv, const char *prefix __used)
 	if (!strcmp(argv[0], "trace"))
 		return cmd_trace(argc, argv, prefix);
 
-	symbol__init(0);
+	symbol__init();
 	if (!strncmp(argv[0], "rec", 3)) {
 		return __cmd_record(argc, argv);
 	} else if (!strncmp(argv[0], "lat", 3)) {

commit c019879bcc5692ec9267c1cedad91f1794d0b693
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Mon Dec 14 14:23:00 2009 -0200

    perf session: Adopt the sample_type variable
    
    All tools had copies, and perf diff would have to specify a
    sample_type_check method just for copying it.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frédéric Weisbecker <fweisbec@gmail.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Mackerras <paulus@samba.org>
    LKML-Reference: <1260807780-19377-2-git-send-email-acme@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 412ae924640a..d67f274adba0 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -21,8 +21,6 @@
 
 static char			const *input_name = "perf.data";
 
-static u64			sample_type;
-
 static char			default_sort_order[] = "avg, max, switch, runtime";
 static char			*sort_order = default_sort_order;
 
@@ -1613,7 +1611,7 @@ static int process_sample_event(event_t *event, struct perf_session *session)
 	struct sample_data data;
 	struct thread *thread;
 
-	if (!(sample_type & PERF_SAMPLE_RAW))
+	if (!(session->sample_type & PERF_SAMPLE_RAW))
 		return 0;
 
 	memset(&data, 0, sizeof(data));
@@ -1621,7 +1619,7 @@ static int process_sample_event(event_t *event, struct perf_session *session)
 	data.cpu = -1;
 	data.period = -1;
 
-	event__parse_sample(event, sample_type, &data);
+	event__parse_sample(event, session->sample_type, &data);
 
 	dump_printf("(IP, %d): %d/%d: %p period: %Ld\n",
 		event->header.misc,
@@ -1655,11 +1653,9 @@ static int process_lost_event(event_t *event __used,
 	return 0;
 }
 
-static int sample_type_check(u64 type, struct perf_session *session __used)
+static int sample_type_check(struct perf_session *session __used)
 {
-	sample_type = type;
-
-	if (!(sample_type & PERF_SAMPLE_RAW)) {
+	if (!(session->sample_type & PERF_SAMPLE_RAW)) {
 		fprintf(stderr,
 			"No trace sample to read. Did you call perf record "
 			"without -R?");

commit 4e4f06e4c8f17ea96f7dd76251cab99511026401
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Mon Dec 14 13:10:39 2009 -0200

    perf session: Move the hist_entries rb tree to perf_session
    
    As we'll need to sort multiple times for multiple perf sessions,
    so that we can then do a diff.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frédéric Weisbecker <fweisbec@gmail.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Mackerras <paulus@samba.org>
    LKML-Reference: <1260803439-16783-1-git-send-email-acme@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index bce05dff6dfe..412ae924640a 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1655,7 +1655,7 @@ static int process_lost_event(event_t *event __used,
 	return 0;
 }
 
-static int sample_type_check(u64 type)
+static int sample_type_check(u64 type, struct perf_session *session __used)
 {
 	sample_type = type;
 

commit 4aa65636411ccb12f006a6ad593930655c445ff6
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Sun Dec 13 19:50:29 2009 -0200

    perf session: Move kmaps to perf_session
    
    There is still some more work to do to disentangle map creation
    from DSO loading, but this happens only for the kernel, and for
    the early adopters of perf diff, where this disentanglement
    matters most, we'll be testing different kernels, so no problem
    here.
    
    Further clarification: right now we create the kernel maps for
    the various modules and discontiguous kernel text maps when
    loading the DSO, we should do it as a two step process, first
    creating the maps, for multiple mappings with the same DSO
    store, then doing the dso load just once, for the first hit on
    one of the maps sharing this DSO backing store.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frédéric Weisbecker <fweisbec@gmail.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Mackerras <paulus@samba.org>
    LKML-Reference: <1260741029-4430-6-git-send-email-acme@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 8d58d9e07a7b..bce05dff6dfe 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1679,8 +1679,8 @@ static struct perf_event_ops event_ops = {
 static int read_events(void)
 {
 	int err;
-	struct perf_session *session = perf_session__new(input_name, O_RDONLY, 0);
-
+	struct perf_session *session = perf_session__new(input_name, O_RDONLY,
+							 0, NULL);
 	if (session == NULL)
 		return -ENOMEM;
 

commit b3165f414416a717f72a376720564012af5a2e01
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Sun Dec 13 19:50:28 2009 -0200

    perf session: Move the global threads list to perf_session
    
    So that we can process two perf.data files.
    
    We still need to add a O_MMAP mode for perf_session so that we
    can do all the mmap stuff in it.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frédéric Weisbecker <fweisbec@gmail.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Mackerras <paulus@samba.org>
    LKML-Reference: <1260741029-4430-5-git-send-email-acme@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 1e4e508339a8..8d58d9e07a7b 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -730,18 +730,21 @@ struct trace_migrate_task_event {
 
 struct trace_sched_handler {
 	void (*switch_event)(struct trace_switch_event *,
+			     struct perf_session *,
 			     struct event *,
 			     int cpu,
 			     u64 timestamp,
 			     struct thread *thread);
 
 	void (*runtime_event)(struct trace_runtime_event *,
+			      struct perf_session *,
 			      struct event *,
 			      int cpu,
 			      u64 timestamp,
 			      struct thread *thread);
 
 	void (*wakeup_event)(struct trace_wakeup_event *,
+			     struct perf_session *,
 			     struct event *,
 			     int cpu,
 			     u64 timestamp,
@@ -754,6 +757,7 @@ struct trace_sched_handler {
 			   struct thread *thread);
 
 	void (*migrate_task_event)(struct trace_migrate_task_event *,
+			   struct perf_session *session,
 			   struct event *,
 			   int cpu,
 			   u64 timestamp,
@@ -763,6 +767,7 @@ struct trace_sched_handler {
 
 static void
 replay_wakeup_event(struct trace_wakeup_event *wakeup_event,
+		    struct perf_session *session __used,
 		    struct event *event,
 		    int cpu __used,
 		    u64 timestamp __used,
@@ -789,6 +794,7 @@ static u64 cpu_last_switched[MAX_CPUS];
 
 static void
 replay_switch_event(struct trace_switch_event *switch_event,
+		    struct perf_session *session __used,
 		    struct event *event,
 		    int cpu,
 		    u64 timestamp,
@@ -1022,6 +1028,7 @@ add_sched_in_event(struct work_atoms *atoms, u64 timestamp)
 
 static void
 latency_switch_event(struct trace_switch_event *switch_event,
+		     struct perf_session *session,
 		     struct event *event __used,
 		     int cpu,
 		     u64 timestamp,
@@ -1045,8 +1052,8 @@ latency_switch_event(struct trace_switch_event *switch_event,
 		die("hm, delta: %Ld < 0 ?\n", delta);
 
 
-	sched_out = threads__findnew(switch_event->prev_pid);
-	sched_in = threads__findnew(switch_event->next_pid);
+	sched_out = perf_session__findnew(session, switch_event->prev_pid);
+	sched_in = perf_session__findnew(session, switch_event->next_pid);
 
 	out_events = thread_atoms_search(&atom_root, sched_out, &cmp_pid);
 	if (!out_events) {
@@ -1074,12 +1081,13 @@ latency_switch_event(struct trace_switch_event *switch_event,
 
 static void
 latency_runtime_event(struct trace_runtime_event *runtime_event,
+		     struct perf_session *session,
 		     struct event *event __used,
 		     int cpu,
 		     u64 timestamp,
 		     struct thread *this_thread __used)
 {
-	struct thread *thread = threads__findnew(runtime_event->pid);
+	struct thread *thread = perf_session__findnew(session, runtime_event->pid);
 	struct work_atoms *atoms = thread_atoms_search(&atom_root, thread, &cmp_pid);
 
 	BUG_ON(cpu >= MAX_CPUS || cpu < 0);
@@ -1096,6 +1104,7 @@ latency_runtime_event(struct trace_runtime_event *runtime_event,
 
 static void
 latency_wakeup_event(struct trace_wakeup_event *wakeup_event,
+		     struct perf_session *session,
 		     struct event *__event __used,
 		     int cpu __used,
 		     u64 timestamp,
@@ -1109,7 +1118,7 @@ latency_wakeup_event(struct trace_wakeup_event *wakeup_event,
 	if (!wakeup_event->success)
 		return;
 
-	wakee = threads__findnew(wakeup_event->pid);
+	wakee = perf_session__findnew(session, wakeup_event->pid);
 	atoms = thread_atoms_search(&atom_root, wakee, &cmp_pid);
 	if (!atoms) {
 		thread_atoms_insert(wakee);
@@ -1143,6 +1152,7 @@ latency_wakeup_event(struct trace_wakeup_event *wakeup_event,
 
 static void
 latency_migrate_task_event(struct trace_migrate_task_event *migrate_task_event,
+		     struct perf_session *session,
 		     struct event *__event __used,
 		     int cpu __used,
 		     u64 timestamp,
@@ -1158,7 +1168,7 @@ latency_migrate_task_event(struct trace_migrate_task_event *migrate_task_event,
 	if (profile_cpu == -1)
 		return;
 
-	migrant = threads__findnew(migrate_task_event->pid);
+	migrant = perf_session__findnew(session, migrate_task_event->pid);
 	atoms = thread_atoms_search(&atom_root, migrant, &cmp_pid);
 	if (!atoms) {
 		thread_atoms_insert(migrant);
@@ -1353,7 +1363,7 @@ static void sort_lat(void)
 static struct trace_sched_handler *trace_handler;
 
 static void
-process_sched_wakeup_event(void *data,
+process_sched_wakeup_event(void *data, struct perf_session *session,
 			   struct event *event,
 			   int cpu __used,
 			   u64 timestamp __used,
@@ -1370,7 +1380,8 @@ process_sched_wakeup_event(void *data,
 	FILL_FIELD(wakeup_event, cpu, event, data);
 
 	if (trace_handler->wakeup_event)
-		trace_handler->wakeup_event(&wakeup_event, event, cpu, timestamp, thread);
+		trace_handler->wakeup_event(&wakeup_event, session, event,
+					    cpu, timestamp, thread);
 }
 
 /*
@@ -1388,6 +1399,7 @@ static char next_shortname2 = '0';
 
 static void
 map_switch_event(struct trace_switch_event *switch_event,
+		 struct perf_session *session,
 		 struct event *event __used,
 		 int this_cpu,
 		 u64 timestamp,
@@ -1415,8 +1427,8 @@ map_switch_event(struct trace_switch_event *switch_event,
 		die("hm, delta: %Ld < 0 ?\n", delta);
 
 
-	sched_out = threads__findnew(switch_event->prev_pid);
-	sched_in = threads__findnew(switch_event->next_pid);
+	sched_out = perf_session__findnew(session, switch_event->prev_pid);
+	sched_in = perf_session__findnew(session, switch_event->next_pid);
 
 	curr_thread[this_cpu] = sched_in;
 
@@ -1466,7 +1478,7 @@ map_switch_event(struct trace_switch_event *switch_event,
 
 
 static void
-process_sched_switch_event(void *data,
+process_sched_switch_event(void *data, struct perf_session *session,
 			   struct event *event,
 			   int this_cpu,
 			   u64 timestamp __used,
@@ -1493,13 +1505,14 @@ process_sched_switch_event(void *data,
 			nr_context_switch_bugs++;
 	}
 	if (trace_handler->switch_event)
-		trace_handler->switch_event(&switch_event, event, this_cpu, timestamp, thread);
+		trace_handler->switch_event(&switch_event, session, event,
+					    this_cpu, timestamp, thread);
 
 	curr_pid[this_cpu] = switch_event.next_pid;
 }
 
 static void
-process_sched_runtime_event(void *data,
+process_sched_runtime_event(void *data, struct perf_session *session,
 			   struct event *event,
 			   int cpu __used,
 			   u64 timestamp __used,
@@ -1513,7 +1526,7 @@ process_sched_runtime_event(void *data,
 	FILL_FIELD(runtime_event, vruntime, event, data);
 
 	if (trace_handler->runtime_event)
-		trace_handler->runtime_event(&runtime_event, event, cpu, timestamp, thread);
+		trace_handler->runtime_event(&runtime_event, session, event, cpu, timestamp, thread);
 }
 
 static void
@@ -1533,7 +1546,8 @@ process_sched_fork_event(void *data,
 	FILL_FIELD(fork_event, child_pid, event, data);
 
 	if (trace_handler->fork_event)
-		trace_handler->fork_event(&fork_event, event, cpu, timestamp, thread);
+		trace_handler->fork_event(&fork_event, event,
+					  cpu, timestamp, thread);
 }
 
 static void
@@ -1547,7 +1561,7 @@ process_sched_exit_event(struct event *event,
 }
 
 static void
-process_sched_migrate_task_event(void *data,
+process_sched_migrate_task_event(void *data, struct perf_session *session,
 			   struct event *event,
 			   int cpu __used,
 			   u64 timestamp __used,
@@ -1563,12 +1577,13 @@ process_sched_migrate_task_event(void *data,
 	FILL_FIELD(migrate_task_event, cpu, event, data);
 
 	if (trace_handler->migrate_task_event)
-		trace_handler->migrate_task_event(&migrate_task_event, event, cpu, timestamp, thread);
+		trace_handler->migrate_task_event(&migrate_task_event, session,
+						 event, cpu, timestamp, thread);
 }
 
 static void
-process_raw_event(event_t *raw_event __used, void *data,
-		  int cpu, u64 timestamp, struct thread *thread)
+process_raw_event(event_t *raw_event __used, struct perf_session *session,
+		  void *data, int cpu, u64 timestamp, struct thread *thread)
 {
 	struct event *event;
 	int type;
@@ -1578,23 +1593,22 @@ process_raw_event(event_t *raw_event __used, void *data,
 	event = trace_find_event(type);
 
 	if (!strcmp(event->name, "sched_switch"))
-		process_sched_switch_event(data, event, cpu, timestamp, thread);
+		process_sched_switch_event(data, session, event, cpu, timestamp, thread);
 	if (!strcmp(event->name, "sched_stat_runtime"))
-		process_sched_runtime_event(data, event, cpu, timestamp, thread);
+		process_sched_runtime_event(data, session, event, cpu, timestamp, thread);
 	if (!strcmp(event->name, "sched_wakeup"))
-		process_sched_wakeup_event(data, event, cpu, timestamp, thread);
+		process_sched_wakeup_event(data, session, event, cpu, timestamp, thread);
 	if (!strcmp(event->name, "sched_wakeup_new"))
-		process_sched_wakeup_event(data, event, cpu, timestamp, thread);
+		process_sched_wakeup_event(data, session, event, cpu, timestamp, thread);
 	if (!strcmp(event->name, "sched_process_fork"))
 		process_sched_fork_event(data, event, cpu, timestamp, thread);
 	if (!strcmp(event->name, "sched_process_exit"))
 		process_sched_exit_event(event, cpu, timestamp, thread);
 	if (!strcmp(event->name, "sched_migrate_task"))
-		process_sched_migrate_task_event(data, event, cpu, timestamp, thread);
+		process_sched_migrate_task_event(data, session, event, cpu, timestamp, thread);
 }
 
-static int process_sample_event(event_t *event,
-				struct perf_session *session __used)
+static int process_sample_event(event_t *event, struct perf_session *session)
 {
 	struct sample_data data;
 	struct thread *thread;
@@ -1615,7 +1629,7 @@ static int process_sample_event(event_t *event,
 		(void *)(long)data.ip,
 		(long long)data.period);
 
-	thread = threads__findnew(data.pid);
+	thread = perf_session__findnew(session, data.pid);
 	if (thread == NULL) {
 		pr_debug("problem processing %d event, skipping it.\n",
 			 event->header.type);
@@ -1627,7 +1641,7 @@ static int process_sample_event(event_t *event,
 	if (profile_cpu != -1 && profile_cpu != (int)data.cpu)
 		return 0;
 
-	process_raw_event(event, data.raw_data, data.cpu, data.time, thread);
+	process_raw_event(event, session, data.raw_data, data.cpu, data.time, thread);
 
 	return 0;
 }

commit ec913369733923dbfd6bdff5953a918107059701
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Sun Dec 13 19:50:27 2009 -0200

    perf session: Reduce the number of parms to perf_session__process_events
    
    By having the cwd/cwdlen in the perf_session struct and
    full_paths in perf_event_ops.
    
    Now its just a matter of passing the ops.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frédéric Weisbecker <fweisbec@gmail.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Mackerras <paulus@samba.org>
    LKML-Reference: <1260741029-4430-4-git-send-email-acme@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 847ed51248b1..1e4e508339a8 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1670,8 +1670,7 @@ static int read_events(void)
 	if (session == NULL)
 		return -ENOMEM;
 
-	err = perf_session__process_events(session, &event_ops, 0,
-					   &event__cwdlen, &event__cwd);
+	err = perf_session__process_events(session, &event_ops);
 	perf_session__delete(session);
 	return err;
 }

commit 13df45ca1c9a430d5f53862854070fcc324e015c
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Sun Dec 13 19:50:26 2009 -0200

    perf session: Register the idle thread in perf_session__process_events
    
    No need for all tools to register it and then immediately call
    perf_session__process_events.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frédéric Weisbecker <fweisbec@gmail.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Mackerras <paulus@samba.org>
    LKML-Reference: <1260741029-4430-3-git-send-email-acme@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index b5b447236261..847ed51248b1 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1670,8 +1670,6 @@ static int read_events(void)
 	if (session == NULL)
 		return -ENOMEM;
 
-	register_idle_thread();
-
 	err = perf_session__process_events(session, &event_ops, 0,
 					   &event__cwdlen, &event__cwd);
 	perf_session__delete(session);

commit 301a0b020210360c6e441c7765521248bc87d58e
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Sun Dec 13 19:50:25 2009 -0200

    perf session: Ditch register_perf_file_handler
    
    Pass the event_ops to perf_session__process_events instead.
    
    Also move the event_ops definition to session.h, starting to
    move things around to their right place, trimming the many
    unneeded headers we have.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frédéric Weisbecker <fweisbec@gmail.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Mackerras <paulus@samba.org>
    LKML-Reference: <1260741029-4430-2-git-send-email-acme@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 48ab283ed86b..b5b447236261 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -12,7 +12,6 @@
 #include "util/trace-event.h"
 
 #include "util/debug.h"
-#include "util/data_map.h"
 
 #include <sys/prctl.h>
 
@@ -1656,7 +1655,7 @@ static int sample_type_check(u64 type)
 	return 0;
 }
 
-static struct perf_file_handler file_handler = {
+static struct perf_event_ops event_ops = {
 	.process_sample_event	= process_sample_event,
 	.process_comm_event	= event__process_comm,
 	.process_lost_event	= process_lost_event,
@@ -1672,9 +1671,9 @@ static int read_events(void)
 		return -ENOMEM;
 
 	register_idle_thread();
-	register_perf_file_handler(&file_handler);
 
-	err = perf_session__process_events(session, 0, &event__cwdlen, &event__cwd);
+	err = perf_session__process_events(session, &event_ops, 0,
+					   &event__cwdlen, &event__cwd);
 	perf_session__delete(session);
 	return err;
 }

commit d8f66248d6f25f7c935cc5307c43bf394db07272
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Sun Dec 13 19:50:24 2009 -0200

    perf session: Pass the perf_session to the event handling operations
    
    They will need it to get the right threads list, etc.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frédéric Weisbecker <fweisbec@gmail.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Mackerras <paulus@samba.org>
    LKML-Reference: <1260741029-4430-1-git-send-email-acme@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 65021fe1361e..48ab283ed86b 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1594,7 +1594,8 @@ process_raw_event(event_t *raw_event __used, void *data,
 		process_sched_migrate_task_event(data, event, cpu, timestamp, thread);
 }
 
-static int process_sample_event(event_t *event)
+static int process_sample_event(event_t *event,
+				struct perf_session *session __used)
 {
 	struct sample_data data;
 	struct thread *thread;
@@ -1632,7 +1633,8 @@ static int process_sample_event(event_t *event)
 	return 0;
 }
 
-static int process_lost_event(event_t *event __used)
+static int process_lost_event(event_t *event __used,
+			      struct perf_session *session __used)
 {
 	nr_lost_chunks++;
 	nr_lost_events += event->lost.lost;

commit 94c744b6c0c6c5802a85ebfebbec429ac5851f2b
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Fri Dec 11 21:24:02 2009 -0200

    perf tools: Introduce perf_session class
    
    That does all the initialization boilerplate, opening the file,
    reading the header, checking if it is valid, etc.
    
    And that will as well have the threads list, kmap (now) global
    variable, etc, so that we can handle two (or more) perf.data files
    describing sessions to compare.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frédéric Weisbecker <fweisbec@gmail.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Mackerras <paulus@samba.org>
    LKML-Reference: <1260573842-19720-1-git-send-email-acme@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 7cca7c15b40a..65021fe1361e 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -6,6 +6,7 @@
 #include "util/symbol.h"
 #include "util/thread.h"
 #include "util/header.h"
+#include "util/session.h"
 
 #include "util/parse-options.h"
 #include "util/trace-event.h"
@@ -21,7 +22,6 @@
 
 static char			const *input_name = "perf.data";
 
-static struct perf_header	*header;
 static u64			sample_type;
 
 static char			default_sort_order[] = "avg, max, switch, runtime";
@@ -1663,11 +1663,18 @@ static struct perf_file_handler file_handler = {
 
 static int read_events(void)
 {
+	int err;
+	struct perf_session *session = perf_session__new(input_name, O_RDONLY, 0);
+
+	if (session == NULL)
+		return -ENOMEM;
+
 	register_idle_thread();
 	register_perf_file_handler(&file_handler);
 
-	return mmap_dispatch_perf_file(&header, input_name, 0, 0,
-				       &event__cwdlen, &event__cwd);
+	err = perf_session__process_events(session, 0, &event__cwdlen, &event__cwd);
+	perf_session__delete(session);
+	return err;
 }
 
 static void print_bad_events(void)

commit 3786310afe738070be31c439b8deeaeb69b9735d
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Dec 9 21:40:08 2009 +0100

    perf sched: Add max delay time snapshot
    
    When we have a maximum latency reported for a task, we need a
    convenient way to find the matching location to the raw traces
    or to perf sched map that shows where the task has been
    eventually scheduled in. This gives a pointer to retrieve the
    events that occured during this max latency.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Reviewed-by: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Paul Mackerras <paulus@samba.org>
    LKML-Reference: <1260391208-6808-1-git-send-regression-fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index b12b23ac06f3..7cca7c15b40a 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -140,6 +140,7 @@ struct work_atoms {
 	struct thread		*thread;
 	struct rb_node		node;
 	u64			max_lat;
+	u64			max_lat_at;
 	u64			total_lat;
 	u64			nb_atoms;
 	u64			total_runtime;
@@ -1013,8 +1014,10 @@ add_sched_in_event(struct work_atoms *atoms, u64 timestamp)
 
 	delta = atom->sched_in_time - atom->wake_up_time;
 	atoms->total_lat += delta;
-	if (delta > atoms->max_lat)
+	if (delta > atoms->max_lat) {
 		atoms->max_lat = delta;
+		atoms->max_lat_at = timestamp;
+	}
 	atoms->nb_atoms++;
 }
 
@@ -1210,10 +1213,11 @@ static void output_lat_thread(struct work_atoms *work_list)
 
 	avg = work_list->total_lat / work_list->nb_atoms;
 
-	printf("|%11.3f ms |%9llu | avg:%9.3f ms | max:%9.3f ms |\n",
+	printf("|%11.3f ms |%9llu | avg:%9.3f ms | max:%9.3f ms | max at: %9.6f s\n",
 	      (double)work_list->total_runtime / 1e6,
 		 work_list->nb_atoms, (double)avg / 1e6,
-		 (double)work_list->max_lat / 1e6);
+		 (double)work_list->max_lat / 1e6,
+		 (double)work_list->max_lat_at / 1e9);
 }
 
 static int pid_cmp(struct work_atoms *l, struct work_atoms *r)
@@ -1704,9 +1708,9 @@ static void __cmd_lat(void)
 	read_events();
 	sort_lat();
 
-	printf("\n -----------------------------------------------------------------------------------------\n");
-	printf("  Task                  |   Runtime ms  | Switches | Average delay ms | Maximum delay ms |\n");
-	printf(" -----------------------------------------------------------------------------------------\n");
+	printf("\n ---------------------------------------------------------------------------------------------------------------\n");
+	printf("  Task                  |   Runtime ms  | Switches | Average delay ms | Maximum delay ms | Maximum delay at     |\n");
+	printf(" ---------------------------------------------------------------------------------------------------------------\n");
 
 	next = rb_first(&sorted_atom_root);
 

commit c0c9e72150c07b4a6766cd24a6f685ec2dc9c343
Author: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
Date:   Wed Dec 9 17:51:30 2009 +0800

    perf sched: Fix for getting task's execution time
    
    In current code, task's execute time is got by reading
    '/proc/<pid>/sched' file, it's wrong if the task is created
    by pthread_create(), because every thread task has same pid.
    
    This way also has two demerits:
    
     1: 'perf sched replay' can't work if the kernel is not
        compiled with the 'CONFIG_SCHED_DEBUG' option
    
     2: perf tool should depend on proc file system
    
    So, this patch uses PERF_COUNT_SW_TASK_CLOCK to get task's
    execution time instead of reading /proc file.
    
    Changelog v2 -> v3:
    use PERF_COUNT_SW_TASK_CLOCK instead of rusage() as Ingo's
    suggestion
    
    Reported-by: Torok Edwin <edwintorok@gmail.com>
    Signed-off-by: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
    Cc: Xiao Guangrong <ericxiao.gr@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Paul Mackerras <paulus@samba.org>
    LKML-Reference: <4B1F7322.80103@cn.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 19f43faa9f81..b12b23ac06f3 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -13,7 +13,6 @@
 #include "util/debug.h"
 #include "util/data_map.h"
 
-#include <sys/types.h>
 #include <sys/prctl.h>
 
 #include <semaphore.h>
@@ -414,34 +413,33 @@ static u64 get_cpu_usage_nsec_parent(void)
 	return sum;
 }
 
-static u64 get_cpu_usage_nsec_self(void)
+static int self_open_counters(void)
 {
-	char filename [] = "/proc/1234567890/sched";
-	unsigned long msecs, nsecs;
-	char *line = NULL;
-	u64 total = 0;
-	size_t len = 0;
-	ssize_t chars;
-	FILE *file;
-	int ret;
+	struct perf_event_attr attr;
+	int fd;
 
-	sprintf(filename, "/proc/%d/sched", getpid());
-	file = fopen(filename, "r");
-	BUG_ON(!file);
+	memset(&attr, 0, sizeof(attr));
 
-	while ((chars = getline(&line, &len, file)) != -1) {
-		ret = sscanf(line, "se.sum_exec_runtime : %ld.%06ld\n",
-			&msecs, &nsecs);
-		if (ret == 2) {
-			total = msecs*1e6 + nsecs;
-			break;
-		}
-	}
-	if (line)
-		free(line);
-	fclose(file);
+	attr.type = PERF_TYPE_SOFTWARE;
+	attr.config = PERF_COUNT_SW_TASK_CLOCK;
 
-	return total;
+	fd = sys_perf_event_open(&attr, 0, -1, -1, 0);
+
+	if (fd < 0)
+		die("Error: sys_perf_event_open() syscall returned"
+		    "with %d (%s)\n", fd, strerror(errno));
+	return fd;
+}
+
+static u64 get_cpu_usage_nsec_self(int fd)
+{
+	u64 runtime;
+	int ret;
+
+	ret = read(fd, &runtime, sizeof(runtime));
+	BUG_ON(ret != sizeof(runtime));
+
+	return runtime;
 }
 
 static void *thread_func(void *ctx)
@@ -450,9 +448,11 @@ static void *thread_func(void *ctx)
 	u64 cpu_usage_0, cpu_usage_1;
 	unsigned long i, ret;
 	char comm2[22];
+	int fd;
 
 	sprintf(comm2, ":%s", this_task->comm);
 	prctl(PR_SET_NAME, comm2);
+	fd = self_open_counters();
 
 again:
 	ret = sem_post(&this_task->ready_for_work);
@@ -462,16 +462,15 @@ static void *thread_func(void *ctx)
 	ret = pthread_mutex_unlock(&start_work_mutex);
 	BUG_ON(ret);
 
-	cpu_usage_0 = get_cpu_usage_nsec_self();
+	cpu_usage_0 = get_cpu_usage_nsec_self(fd);
 
 	for (i = 0; i < this_task->nr_events; i++) {
 		this_task->curr_event = i;
 		process_sched_event(this_task, this_task->atoms[i]);
 	}
 
-	cpu_usage_1 = get_cpu_usage_nsec_self();
+	cpu_usage_1 = get_cpu_usage_nsec_self(fd);
 	this_task->cpu_usage = cpu_usage_1 - cpu_usage_0;
-
 	ret = sem_post(&this_task->work_done_sem);
 	BUG_ON(ret);
 

commit f48f669d42e133db839af16656fd720107ef6742
Author: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
Date:   Mon Dec 7 13:04:04 2009 +0800

    perf_event: Eliminate raw->size
    
    raw->size is not used, this patch just cleans it up.
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: OGAWA Hirofumi <hirofumi@mail.parknet.co.jp>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    LKML-Reference: <4B1C8CC4.4050007@cn.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 4655e16b929b..19f43faa9f81 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -628,11 +628,6 @@ static void test_calibrations(void)
 	printf("the sleep test took %Ld nsecs\n", T1-T0);
 }
 
-struct raw_event_sample {
-	u32 size;
-	char data[0];
-};
-
 #define FILL_FIELD(ptr, field, event, data)	\
 	ptr.field = (typeof(ptr.field)) raw_field_value(event, #field, data)
 
@@ -1356,7 +1351,7 @@ static void sort_lat(void)
 static struct trace_sched_handler *trace_handler;
 
 static void
-process_sched_wakeup_event(struct raw_event_sample *raw,
+process_sched_wakeup_event(void *data,
 			   struct event *event,
 			   int cpu __used,
 			   u64 timestamp __used,
@@ -1364,13 +1359,13 @@ process_sched_wakeup_event(struct raw_event_sample *raw,
 {
 	struct trace_wakeup_event wakeup_event;
 
-	FILL_COMMON_FIELDS(wakeup_event, event, raw->data);
+	FILL_COMMON_FIELDS(wakeup_event, event, data);
 
-	FILL_ARRAY(wakeup_event, comm, event, raw->data);
-	FILL_FIELD(wakeup_event, pid, event, raw->data);
-	FILL_FIELD(wakeup_event, prio, event, raw->data);
-	FILL_FIELD(wakeup_event, success, event, raw->data);
-	FILL_FIELD(wakeup_event, cpu, event, raw->data);
+	FILL_ARRAY(wakeup_event, comm, event, data);
+	FILL_FIELD(wakeup_event, pid, event, data);
+	FILL_FIELD(wakeup_event, prio, event, data);
+	FILL_FIELD(wakeup_event, success, event, data);
+	FILL_FIELD(wakeup_event, cpu, event, data);
 
 	if (trace_handler->wakeup_event)
 		trace_handler->wakeup_event(&wakeup_event, event, cpu, timestamp, thread);
@@ -1469,7 +1464,7 @@ map_switch_event(struct trace_switch_event *switch_event,
 
 
 static void
-process_sched_switch_event(struct raw_event_sample *raw,
+process_sched_switch_event(void *data,
 			   struct event *event,
 			   int this_cpu,
 			   u64 timestamp __used,
@@ -1477,15 +1472,15 @@ process_sched_switch_event(struct raw_event_sample *raw,
 {
 	struct trace_switch_event switch_event;
 
-	FILL_COMMON_FIELDS(switch_event, event, raw->data);
+	FILL_COMMON_FIELDS(switch_event, event, data);
 
-	FILL_ARRAY(switch_event, prev_comm, event, raw->data);
-	FILL_FIELD(switch_event, prev_pid, event, raw->data);
-	FILL_FIELD(switch_event, prev_prio, event, raw->data);
-	FILL_FIELD(switch_event, prev_state, event, raw->data);
-	FILL_ARRAY(switch_event, next_comm, event, raw->data);
-	FILL_FIELD(switch_event, next_pid, event, raw->data);
-	FILL_FIELD(switch_event, next_prio, event, raw->data);
+	FILL_ARRAY(switch_event, prev_comm, event, data);
+	FILL_FIELD(switch_event, prev_pid, event, data);
+	FILL_FIELD(switch_event, prev_prio, event, data);
+	FILL_FIELD(switch_event, prev_state, event, data);
+	FILL_ARRAY(switch_event, next_comm, event, data);
+	FILL_FIELD(switch_event, next_pid, event, data);
+	FILL_FIELD(switch_event, next_prio, event, data);
 
 	if (curr_pid[this_cpu] != (u32)-1) {
 		/*
@@ -1502,7 +1497,7 @@ process_sched_switch_event(struct raw_event_sample *raw,
 }
 
 static void
-process_sched_runtime_event(struct raw_event_sample *raw,
+process_sched_runtime_event(void *data,
 			   struct event *event,
 			   int cpu __used,
 			   u64 timestamp __used,
@@ -1510,17 +1505,17 @@ process_sched_runtime_event(struct raw_event_sample *raw,
 {
 	struct trace_runtime_event runtime_event;
 
-	FILL_ARRAY(runtime_event, comm, event, raw->data);
-	FILL_FIELD(runtime_event, pid, event, raw->data);
-	FILL_FIELD(runtime_event, runtime, event, raw->data);
-	FILL_FIELD(runtime_event, vruntime, event, raw->data);
+	FILL_ARRAY(runtime_event, comm, event, data);
+	FILL_FIELD(runtime_event, pid, event, data);
+	FILL_FIELD(runtime_event, runtime, event, data);
+	FILL_FIELD(runtime_event, vruntime, event, data);
 
 	if (trace_handler->runtime_event)
 		trace_handler->runtime_event(&runtime_event, event, cpu, timestamp, thread);
 }
 
 static void
-process_sched_fork_event(struct raw_event_sample *raw,
+process_sched_fork_event(void *data,
 			 struct event *event,
 			 int cpu __used,
 			 u64 timestamp __used,
@@ -1528,12 +1523,12 @@ process_sched_fork_event(struct raw_event_sample *raw,
 {
 	struct trace_fork_event fork_event;
 
-	FILL_COMMON_FIELDS(fork_event, event, raw->data);
+	FILL_COMMON_FIELDS(fork_event, event, data);
 
-	FILL_ARRAY(fork_event, parent_comm, event, raw->data);
-	FILL_FIELD(fork_event, parent_pid, event, raw->data);
-	FILL_ARRAY(fork_event, child_comm, event, raw->data);
-	FILL_FIELD(fork_event, child_pid, event, raw->data);
+	FILL_ARRAY(fork_event, parent_comm, event, data);
+	FILL_FIELD(fork_event, parent_pid, event, data);
+	FILL_ARRAY(fork_event, child_comm, event, data);
+	FILL_FIELD(fork_event, child_pid, event, data);
 
 	if (trace_handler->fork_event)
 		trace_handler->fork_event(&fork_event, event, cpu, timestamp, thread);
@@ -1550,7 +1545,7 @@ process_sched_exit_event(struct event *event,
 }
 
 static void
-process_sched_migrate_task_event(struct raw_event_sample *raw,
+process_sched_migrate_task_event(void *data,
 			   struct event *event,
 			   int cpu __used,
 			   u64 timestamp __used,
@@ -1558,46 +1553,42 @@ process_sched_migrate_task_event(struct raw_event_sample *raw,
 {
 	struct trace_migrate_task_event migrate_task_event;
 
-	FILL_COMMON_FIELDS(migrate_task_event, event, raw->data);
+	FILL_COMMON_FIELDS(migrate_task_event, event, data);
 
-	FILL_ARRAY(migrate_task_event, comm, event, raw->data);
-	FILL_FIELD(migrate_task_event, pid, event, raw->data);
-	FILL_FIELD(migrate_task_event, prio, event, raw->data);
-	FILL_FIELD(migrate_task_event, cpu, event, raw->data);
+	FILL_ARRAY(migrate_task_event, comm, event, data);
+	FILL_FIELD(migrate_task_event, pid, event, data);
+	FILL_FIELD(migrate_task_event, prio, event, data);
+	FILL_FIELD(migrate_task_event, cpu, event, data);
 
 	if (trace_handler->migrate_task_event)
 		trace_handler->migrate_task_event(&migrate_task_event, event, cpu, timestamp, thread);
 }
 
 static void
-process_raw_event(event_t *raw_event __used, u32 size, void *data,
+process_raw_event(event_t *raw_event __used, void *data,
 		  int cpu, u64 timestamp, struct thread *thread)
 {
-	struct raw_event_sample *raw;
 	struct event *event;
 	int type;
 
-	raw = malloc_or_die(sizeof(*raw)+size);
-	raw->size = size;
-	memcpy(raw->data, data, size);
 
-	type = trace_parse_common_type(raw->data);
+	type = trace_parse_common_type(data);
 	event = trace_find_event(type);
 
 	if (!strcmp(event->name, "sched_switch"))
-		process_sched_switch_event(raw, event, cpu, timestamp, thread);
+		process_sched_switch_event(data, event, cpu, timestamp, thread);
 	if (!strcmp(event->name, "sched_stat_runtime"))
-		process_sched_runtime_event(raw, event, cpu, timestamp, thread);
+		process_sched_runtime_event(data, event, cpu, timestamp, thread);
 	if (!strcmp(event->name, "sched_wakeup"))
-		process_sched_wakeup_event(raw, event, cpu, timestamp, thread);
+		process_sched_wakeup_event(data, event, cpu, timestamp, thread);
 	if (!strcmp(event->name, "sched_wakeup_new"))
-		process_sched_wakeup_event(raw, event, cpu, timestamp, thread);
+		process_sched_wakeup_event(data, event, cpu, timestamp, thread);
 	if (!strcmp(event->name, "sched_process_fork"))
-		process_sched_fork_event(raw, event, cpu, timestamp, thread);
+		process_sched_fork_event(data, event, cpu, timestamp, thread);
 	if (!strcmp(event->name, "sched_process_exit"))
 		process_sched_exit_event(event, cpu, timestamp, thread);
 	if (!strcmp(event->name, "sched_migrate_task"))
-		process_sched_migrate_task_event(raw, event, cpu, timestamp, thread);
+		process_sched_migrate_task_event(data, event, cpu, timestamp, thread);
 }
 
 static int process_sample_event(event_t *event)
@@ -1633,8 +1624,7 @@ static int process_sample_event(event_t *event)
 	if (profile_cpu != -1 && profile_cpu != (int)data.cpu)
 		return 0;
 
-	process_raw_event(event, data.raw_size, data.raw_data, data.cpu,
-			  data.time, thread);
+	process_raw_event(event, data.raw_data, data.cpu, data.time, thread);
 
 	return 0;
 }

commit d8bd9e0aedabcb47887712497bc386a06ddcbd12
Author: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
Date:   Mon Dec 7 12:06:29 2009 +0800

    perf_event: Fix raw event processing
    
    We use 'data.raw_data' parameter to call process_raw_event(),
    but data.raw_data buffer not include data size. it can make perf
    tool crash.
    
    This bug was introduced by commit 180f95e29a ("perf: Make common
    SAMPLE_EVENT parser").
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: Eduard - Gabriel Munteanu <eduard.munteanu@linux360.ro>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: OGAWA Hirofumi <hirofumi@mail.parknet.co.jp>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    LKML-Reference: <4B1C7F45.5080105@cn.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 7481ebdb17ef..4655e16b929b 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1570,13 +1570,17 @@ process_sched_migrate_task_event(struct raw_event_sample *raw,
 }
 
 static void
-process_raw_event(event_t *raw_event __used, void *more_data,
+process_raw_event(event_t *raw_event __used, u32 size, void *data,
 		  int cpu, u64 timestamp, struct thread *thread)
 {
-	struct raw_event_sample *raw = more_data;
+	struct raw_event_sample *raw;
 	struct event *event;
 	int type;
 
+	raw = malloc_or_die(sizeof(*raw)+size);
+	raw->size = size;
+	memcpy(raw->data, data, size);
+
 	type = trace_parse_common_type(raw->data);
 	event = trace_find_event(type);
 
@@ -1629,7 +1633,8 @@ static int process_sample_event(event_t *event)
 	if (profile_cpu != -1 && profile_cpu != (int)data.cpu)
 		return 0;
 
-	process_raw_event(event, data.raw_data, data.cpu, data.time, thread);
+	process_raw_event(event, data.raw_size, data.raw_data, data.cpu,
+			  data.time, thread);
 
 	return 0;
 }

commit c0777c5aa835a97ccc77d82e55388940f0140a61
Author: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
Date:   Mon Dec 7 12:04:49 2009 +0800

    perf/sched: Fix 'perf sched trace'
    
    If we use 'perf sched trace', it will call symbol__init() again,
    and can lead to a perf tool crash:
    
     [root@localhost perf]# ./perf sched trace
     *** glibc detected *** ./perf: free(): invalid next size (normal): 0x094c1898 ***
     ======= Backtrace: =========
     /lib/libc.so.6[0xb7602404]
     /lib/libc.so.6(cfree+0x96)[0xb76043b6]
     ./perf[0x80730fe]
     ./perf[0x8074c97]
     ./perf[0x805eb59]
     ./perf[0x80536fd]
     ./perf[0x804b618]
     ./perf[0x804bdc3]
     /lib/libc.so.6(__libc_start_main+0xe5)[0xb75a9735]
     ./perf[0x804af81]
     ======= Memory map: ========
     08048000-08158000 r-xp 00000000 fe:00 556831     /home/eric/....
     08158000-08168000 rw-p 0010f000 fe:00 556831     /home/eric/...
     08168000-085fe000 rw-p 00000000 00:00 0
     094ab000-094cc000 rw-p 00000000 00:00 0          [heap]
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
    LKML-Reference: <4B1C7EE1.8030906@cn.fujitsu.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 45c46c790493..7481ebdb17ef 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1888,13 +1888,18 @@ static int __cmd_record(int argc, const char **argv)
 
 int cmd_sched(int argc, const char **argv, const char *prefix __used)
 {
-	symbol__init(0);
-
 	argc = parse_options(argc, argv, sched_options, sched_usage,
 			     PARSE_OPT_STOP_AT_NON_OPTION);
 	if (!argc)
 		usage_with_options(sched_usage, sched_options);
 
+	/*
+	 * Aliased to 'perf trace' for now:
+	 */
+	if (!strcmp(argv[0], "trace"))
+		return cmd_trace(argc, argv, prefix);
+
+	symbol__init(0);
 	if (!strncmp(argv[0], "rec", 3)) {
 		return __cmd_record(argc, argv);
 	} else if (!strncmp(argv[0], "lat", 3)) {
@@ -1918,11 +1923,6 @@ int cmd_sched(int argc, const char **argv, const char *prefix __used)
 				usage_with_options(replay_usage, replay_options);
 		}
 		__cmd_replay();
-	} else if (!strcmp(argv[0], "trace")) {
-		/*
-		 * Aliased to 'perf trace' for now:
-		 */
-		return cmd_trace(argc, argv, prefix);
 	} else {
 		usage_with_options(sched_usage, sched_options);
 	}

commit 180f95e29aa8782c019caa64ede2a28d8ab62564
Author: OGAWA Hirofumi <hirofumi@mail.parknet.co.jp>
Date:   Sun Dec 6 20:08:24 2009 +0900

    perf: Make common SAMPLE_EVENT parser
    
    Currently, sample event data is parsed for each commands, and it
    is assuming that the data is not including other data. (E.g.
    timechart, trace, etc. can't parse the event if it has
    PERF_SAMPLE_CALLCHAIN)
    
    So, even if we record the superset data for multiple commands at
    a time, commands can't parse. etc.
    
    To fix it, this makes common sample event parser, and use it to
    parse sample event correctly. (PERF_SAMPLE_READ is unsupported
    for now though, it seems to be not using.)
    
    Signed-off-by: OGAWA Hirofumi <hirofumi@mail.parknet.co.jp>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <87hbs48imv.fsf@devron.myhome.or.jp>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 26b782f26ee1..45c46c790493 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1598,40 +1598,26 @@ process_raw_event(event_t *raw_event __used, void *more_data,
 
 static int process_sample_event(event_t *event)
 {
+	struct sample_data data;
 	struct thread *thread;
-	u64 ip = event->ip.ip;
-	u64 timestamp = -1;
-	u32 cpu = -1;
-	u64 period = 1;
-	void *more_data = event->ip.__more_data;
 
 	if (!(sample_type & PERF_SAMPLE_RAW))
 		return 0;
 
-	thread = threads__findnew(event->ip.pid);
+	memset(&data, 0, sizeof(data));
+	data.time = -1;
+	data.cpu = -1;
+	data.period = -1;
 
-	if (sample_type & PERF_SAMPLE_TIME) {
-		timestamp = *(u64 *)more_data;
-		more_data += sizeof(u64);
-	}
-
-	if (sample_type & PERF_SAMPLE_CPU) {
-		cpu = *(u32 *)more_data;
-		more_data += sizeof(u32);
-		more_data += sizeof(u32); /* reserved */
-	}
-
-	if (sample_type & PERF_SAMPLE_PERIOD) {
-		period = *(u64 *)more_data;
-		more_data += sizeof(u64);
-	}
+	event__parse_sample(event, sample_type, &data);
 
 	dump_printf("(IP, %d): %d/%d: %p period: %Ld\n",
 		event->header.misc,
-		event->ip.pid, event->ip.tid,
-		(void *)(long)ip,
-		(long long)period);
+		data.pid, data.tid,
+		(void *)(long)data.ip,
+		(long long)data.period);
 
+	thread = threads__findnew(data.pid);
 	if (thread == NULL) {
 		pr_debug("problem processing %d event, skipping it.\n",
 			 event->header.type);
@@ -1640,10 +1626,10 @@ static int process_sample_event(event_t *event)
 
 	dump_printf(" ... thread: %s:%d\n", thread->comm, thread->pid);
 
-	if (profile_cpu != -1 && profile_cpu != (int) cpu)
+	if (profile_cpu != -1 && profile_cpu != (int)data.cpu)
 		return 0;
 
-	process_raw_event(event, more_data, cpu, timestamp, thread);
+	process_raw_event(event, data.raw_data, data.cpu, data.time, thread);
 
 	return 0;
 }

commit 62daacb51a2bf8480e6f6b3696b03f102fc15eb0
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Fri Nov 27 16:29:22 2009 -0200

    perf tools: Reorganize event processing routines, lotsa dups killed
    
    While implementing event__preprocess_sample, that will do all of
    the symbol lookup in one convenient function, I noticed that
    util/process_event.[ch] were not being used at all, then started
    looking if there were other functions that could be shared
    and...
    
    All those functions really don't need to receive offset + head,
    the only thing they did was common to all of them, so do it at
    one place instead.
    
    Stats about number of each type of event processed now is done
    in a central place.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frédéric Weisbecker <fweisbec@gmail.com>
    Cc: John Kacur <jkacur@redhat.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Mackerras <paulus@samba.org>
    LKML-Reference: <1259346563-12568-11-git-send-email-acme@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 19eb708a706b..26b782f26ee1 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -22,8 +22,6 @@
 
 static char			const *input_name = "perf.data";
 
-static unsigned long		total_comm = 0;
-
 static struct perf_header	*header;
 static u64			sample_type;
 
@@ -32,9 +30,6 @@ static char			*sort_order = default_sort_order;
 
 static int			profile_cpu = -1;
 
-static char			*cwd;
-static int			cwdlen;
-
 #define PR_SET_NAME		15               /* Set process name */
 #define MAX_CPUS		4096
 
@@ -633,27 +628,6 @@ static void test_calibrations(void)
 	printf("the sleep test took %Ld nsecs\n", T1-T0);
 }
 
-static int
-process_comm_event(event_t *event, unsigned long offset, unsigned long head)
-{
-	struct thread *thread = threads__findnew(event->comm.tid);
-
-	dump_printf("%p [%p]: perf_event_comm: %s:%d\n",
-		(void *)(offset + head),
-		(void *)(long)(event->header.size),
-		event->comm.comm, event->comm.pid);
-
-	if (thread == NULL ||
-	    thread__set_comm(thread, event->comm.comm)) {
-		dump_printf("problem processing perf_event_comm, skipping event.\n");
-		return -1;
-	}
-	total_comm++;
-
-	return 0;
-}
-
-
 struct raw_event_sample {
 	u32 size;
 	char data[0];
@@ -1622,8 +1596,7 @@ process_raw_event(event_t *raw_event __used, void *more_data,
 		process_sched_migrate_task_event(raw, event, cpu, timestamp, thread);
 }
 
-static int
-process_sample_event(event_t *event, unsigned long offset, unsigned long head)
+static int process_sample_event(event_t *event)
 {
 	struct thread *thread;
 	u64 ip = event->ip.ip;
@@ -1653,9 +1626,7 @@ process_sample_event(event_t *event, unsigned long offset, unsigned long head)
 		more_data += sizeof(u64);
 	}
 
-	dump_printf("%p [%p]: PERF_RECORD_SAMPLE (IP, %d): %d/%d: %p period: %Ld\n",
-		(void *)(offset + head),
-		(void *)(long)(event->header.size),
+	dump_printf("(IP, %d): %d/%d: %p period: %Ld\n",
 		event->header.misc,
 		event->ip.pid, event->ip.tid,
 		(void *)(long)ip,
@@ -1677,10 +1648,7 @@ process_sample_event(event_t *event, unsigned long offset, unsigned long head)
 	return 0;
 }
 
-static int
-process_lost_event(event_t *event __used,
-		   unsigned long offset __used,
-		   unsigned long head __used)
+static int process_lost_event(event_t *event __used)
 {
 	nr_lost_chunks++;
 	nr_lost_events += event->lost.lost;
@@ -1704,7 +1672,7 @@ static int sample_type_check(u64 type)
 
 static struct perf_file_handler file_handler = {
 	.process_sample_event	= process_sample_event,
-	.process_comm_event	= process_comm_event,
+	.process_comm_event	= event__process_comm,
 	.process_lost_event	= process_lost_event,
 	.sample_type_check	= sample_type_check,
 };
@@ -1715,7 +1683,7 @@ static int read_events(void)
 	register_perf_file_handler(&file_handler);
 
 	return mmap_dispatch_perf_file(&header, input_name, 0, 0,
-				       &cwdlen, &cwd);
+				       &event__cwdlen, &event__cwd);
 }
 
 static void print_bad_events(void)

commit 364794845cbc49e638b83d7ef739524291e1e961
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Tue Nov 24 12:05:16 2009 -0200

    perf tools: Introduce zalloc() for the common calloc(1, N) case
    
    This way we type less characters and it looks more like the
    kzalloc kernel counterpart.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frédéric Weisbecker <fweisbec@gmail.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Mackerras <paulus@samba.org>
    LKML-Reference: <1259071517-3242-3-git-send-email-acme@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index dbf089b12def..19eb708a706b 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -225,7 +225,7 @@ static void calibrate_sleep_measurement_overhead(void)
 static struct sched_atom *
 get_new_event(struct task_desc *task, u64 timestamp)
 {
-	struct sched_atom *event = calloc(1, sizeof(*event));
+	struct sched_atom *event = zalloc(sizeof(*event));
 	unsigned long idx = task->nr_events;
 	size_t size;
 
@@ -293,7 +293,7 @@ add_sched_event_wakeup(struct task_desc *task, u64 timestamp,
 		return;
 	}
 
-	wakee_event->wait_sem = calloc(1, sizeof(*wakee_event->wait_sem));
+	wakee_event->wait_sem = zalloc(sizeof(*wakee_event->wait_sem));
 	sem_init(wakee_event->wait_sem, 0, 0);
 	wakee_event->specific_wait = 1;
 	event->wait_sem = wakee_event->wait_sem;
@@ -323,7 +323,7 @@ static struct task_desc *register_pid(unsigned long pid, const char *comm)
 	if (task)
 		return task;
 
-	task = calloc(1, sizeof(*task));
+	task = zalloc(sizeof(*task));
 	task->pid = pid;
 	task->nr = nr_tasks;
 	strcpy(task->comm, comm);
@@ -962,9 +962,7 @@ __thread_latency_insert(struct rb_root *root, struct work_atoms *data,
 
 static void thread_atoms_insert(struct thread *thread)
 {
-	struct work_atoms *atoms;
-
-	atoms = calloc(sizeof(*atoms), 1);
+	struct work_atoms *atoms = zalloc(sizeof(*atoms));
 	if (!atoms)
 		die("No memory");
 
@@ -996,9 +994,7 @@ add_sched_out_event(struct work_atoms *atoms,
 		    char run_state,
 		    u64 timestamp)
 {
-	struct work_atom *atom;
-
-	atom = calloc(sizeof(*atom), 1);
+	struct work_atom *atom = zalloc(sizeof(*atom));
 	if (!atom)
 		die("Non memory");
 

commit b32d133aec5dc882cf783a293f393bfb3f4379e1
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Tue Nov 24 12:05:15 2009 -0200

    perf symbols: Simplify symbol machinery setup
    
    And also express its configuration toggles via a struct.
    
    Now all one has to do is to call symbol__init(NULL) if the
    defaults are OK, or pass a struct symbol_conf pointer with the
    desired configuration.
    
    If a tool uses kernel_maps__find_symbol() to look at the kernel
    and modules mappings for a symbol but didn't call symbol__init()
    first, that will generate a one time warning too, alerting the
    subcommand developer that symbol__init() must be called.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frédéric Weisbecker <fweisbec@gmail.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Mackerras <paulus@samba.org>
    LKML-Reference: <1259071517-3242-2-git-send-email-acme@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 260f57a72ee0..dbf089b12def 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1718,7 +1718,7 @@ static int read_events(void)
 	register_idle_thread();
 	register_perf_file_handler(&file_handler);
 
-	return mmap_dispatch_perf_file(&header, input_name, NULL, false, 0, 0,
+	return mmap_dispatch_perf_file(&header, input_name, 0, 0,
 				       &cwdlen, &cwd);
 }
 

commit cc612d8199089413719397c9d92e5823da578eac
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Mon Nov 23 16:39:10 2009 -0200

    perf symbols: Look for vmlinux in more places
    
    Now that we can check the buildid to see if it really matches,
    this can be done safely:
    
      vmlinux
      /boot/vmlinux
      /boot/vmlinux-<uts.release>
      /lib/modules/<uts.release>/build/vmlinux
      /usr/lib/debug/lib/modules/%s/vmlinux
    
    More can be added - if you know about distros that put the
    vmlinux somewhere else please let us know.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frédéric Weisbecker <fweisbec@gmail.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Mackerras <paulus@samba.org>
    LKML-Reference: <1259001550-8194-1-git-send-email-acme@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index df44b756cecc..260f57a72ee0 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1718,7 +1718,8 @@ static int read_events(void)
 	register_idle_thread();
 	register_perf_file_handler(&file_handler);
 
-	return mmap_dispatch_perf_file(&header, input_name, 0, 0, &cwdlen, &cwd);
+	return mmap_dispatch_perf_file(&header, input_name, NULL, false, 0, 0,
+				       &cwdlen, &cwd);
 }
 
 static void print_bad_events(void)

commit 00a192b395b0606ad0265243844b3cd68e73420a
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Fri Oct 30 16:28:24 2009 -0200

    perf tools: Simplify the symbol priv area mechanism
    
    Before we were storing this in the DSO, but in fact this is a
    property of the 'symbol' class, not something that will vary
    among DSOs, so move it to a global variable and initialize it
    using the existing symbol__init routine.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Mike Galbraith <efault@gmx.de>
    LKML-Reference: <1256927305-4628-2-git-send-email-acme@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 9a48d9626be4..df44b756cecc 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1937,7 +1937,7 @@ static int __cmd_record(int argc, const char **argv)
 
 int cmd_sched(int argc, const char **argv, const char *prefix __used)
 {
-	symbol__init();
+	symbol__init(0);
 
 	argc = parse_options(argc, argv, sched_options, sched_usage,
 			     PARSE_OPT_STOP_AT_NON_OPTION);

commit 6beba7adbe092e63dfe8d09fbd1e3ec140474a13
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Wed Oct 21 17:34:06 2009 -0200

    perf tools: Unify debug messages mechanisms
    
    We were using eprintf in some places, that looks at a global
    'verbose' level, and at other places passing a 'v' parameter to
    specify the verbosity level, unify it by introducing
    pr_{err,warning,debug,etc}, just like in the kernel.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Mike Galbraith <efault@gmx.de>
    LKML-Reference: <1256153646-10097-1-git-send-email-acme@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 807ca66e7a8c..9a48d9626be4 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1666,8 +1666,8 @@ process_sample_event(event_t *event, unsigned long offset, unsigned long head)
 		(long long)period);
 
 	if (thread == NULL) {
-		eprintf("problem processing %d event, skipping it.\n",
-			event->header.type);
+		pr_debug("problem processing %d event, skipping it.\n",
+			 event->header.type);
 		return -1;
 	}
 

commit 5a116dd2797677cad48fee2f42267e3cb69f5502
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Oct 17 17:12:33 2009 +0200

    perf tools: Use kernel bitmap library
    
    Use the kernel bitmap library for internal perf tools uses.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    LKML-Reference: <1255792354-11304-1-git-send-email-fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 57ad3f458ef5..807ca66e7a8c 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -38,8 +38,6 @@ static int			cwdlen;
 #define PR_SET_NAME		15               /* Set process name */
 #define MAX_CPUS		4096
 
-#define BUG_ON(x)		assert(!(x))
-
 static u64			run_measurement_overhead;
 static u64			sleep_measurement_overhead;
 

commit f39cdf25bf77219676ec5360980ac40b1a7e144a
Author: Julia Lawall <julia@diku.dk>
Date:   Sat Oct 17 08:43:17 2009 +0200

    perf tools: Move dereference after NULL test
    
    In each case, if the NULL test on thread is needed, then the
    dereference should be after the NULL test.
    
    A simplified version of the semantic match that detects this
    problem is as follows (http://coccinelle.lip6.fr/):
    
    // <smpl>
    @match exists@
    expression x, E;
    identifier fld;
    @@
    
    * x->fld
      ... when != \(x = E\|&x\)
    * x == NULL
    // </smpl>
    
    Signed-off-by: Julia Lawall <julia@diku.dk>
    LKML-Reference: <Pine.LNX.4.64.0910170842500.9213@ask.diku.dk>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index c9c68563e964..57ad3f458ef5 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1667,14 +1667,14 @@ process_sample_event(event_t *event, unsigned long offset, unsigned long head)
 		(void *)(long)ip,
 		(long long)period);
 
-	dump_printf(" ... thread: %s:%d\n", thread->comm, thread->pid);
-
 	if (thread == NULL) {
 		eprintf("problem processing %d event, skipping it.\n",
 			event->header.type);
 		return -1;
 	}
 
+	dump_printf(" ... thread: %s:%d\n", thread->comm, thread->pid);
+
 	if (profile_cpu != -1 && profile_cpu != (int) cpu)
 		return 0;
 

commit b226f744d40b052ac126c4cb16c76f66e5185128
Merge: d5b889f2ecec a3ccf63ee643
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Oct 15 08:44:42 2009 +0200

    Merge branch 'linus' into perf/core
    
    Merge reason: pick up tools/perf/ changes from upstream.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit d5b889f2ecec7849e851ddd31c34bdfb3482b5de
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Tue Oct 13 11:16:29 2009 -0300

    perf tools: Move threads & last_match to threads.c
    
    This was just being copy'n'pasted all over.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Mike Galbraith <efault@gmx.de>
    LKML-Reference: <20091013141629.GD21809@ghostprotocols.net>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 387a44234368..73bdad029730 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -24,9 +24,6 @@ static char			const *input_name = "perf.data";
 
 static unsigned long		total_comm = 0;
 
-static struct rb_root		threads;
-static struct thread		*last_match;
-
 static struct perf_header	*header;
 static u64			sample_type;
 
@@ -641,9 +638,7 @@ static void test_calibrations(void)
 static int
 process_comm_event(event_t *event, unsigned long offset, unsigned long head)
 {
-	struct thread *thread;
-
-	thread = threads__findnew(event->comm.tid, &threads, &last_match);
+	struct thread *thread = threads__findnew(event->comm.tid);
 
 	dump_printf("%p [%p]: perf_event_comm: %s:%d\n",
 		(void *)(offset + head),
@@ -1086,8 +1081,8 @@ latency_switch_event(struct trace_switch_event *switch_event,
 		die("hm, delta: %Ld < 0 ?\n", delta);
 
 
-	sched_out = threads__findnew(switch_event->prev_pid, &threads, &last_match);
-	sched_in = threads__findnew(switch_event->next_pid, &threads, &last_match);
+	sched_out = threads__findnew(switch_event->prev_pid);
+	sched_in = threads__findnew(switch_event->next_pid);
 
 	out_events = thread_atoms_search(&atom_root, sched_out, &cmp_pid);
 	if (!out_events) {
@@ -1120,13 +1115,10 @@ latency_runtime_event(struct trace_runtime_event *runtime_event,
 		     u64 timestamp,
 		     struct thread *this_thread __used)
 {
-	struct work_atoms *atoms;
-	struct thread *thread;
+	struct thread *thread = threads__findnew(runtime_event->pid);
+	struct work_atoms *atoms = thread_atoms_search(&atom_root, thread, &cmp_pid);
 
 	BUG_ON(cpu >= MAX_CPUS || cpu < 0);
-
-	thread = threads__findnew(runtime_event->pid, &threads, &last_match);
-	atoms = thread_atoms_search(&atom_root, thread, &cmp_pid);
 	if (!atoms) {
 		thread_atoms_insert(thread);
 		atoms = thread_atoms_search(&atom_root, thread, &cmp_pid);
@@ -1153,7 +1145,7 @@ latency_wakeup_event(struct trace_wakeup_event *wakeup_event,
 	if (!wakeup_event->success)
 		return;
 
-	wakee = threads__findnew(wakeup_event->pid, &threads, &last_match);
+	wakee = threads__findnew(wakeup_event->pid);
 	atoms = thread_atoms_search(&atom_root, wakee, &cmp_pid);
 	if (!atoms) {
 		thread_atoms_insert(wakee);
@@ -1202,7 +1194,7 @@ latency_migrate_task_event(struct trace_migrate_task_event *migrate_task_event,
 	if (profile_cpu == -1)
 		return;
 
-	migrant = threads__findnew(migrate_task_event->pid, &threads, &last_match);
+	migrant = threads__findnew(migrate_task_event->pid);
 	atoms = thread_atoms_search(&atom_root, migrant, &cmp_pid);
 	if (!atoms) {
 		thread_atoms_insert(migrant);
@@ -1458,8 +1450,8 @@ map_switch_event(struct trace_switch_event *switch_event,
 		die("hm, delta: %Ld < 0 ?\n", delta);
 
 
-	sched_out = threads__findnew(switch_event->prev_pid, &threads, &last_match);
-	sched_in = threads__findnew(switch_event->next_pid, &threads, &last_match);
+	sched_out = threads__findnew(switch_event->prev_pid);
+	sched_in = threads__findnew(switch_event->next_pid);
 
 	curr_thread[this_cpu] = sched_in;
 
@@ -1649,7 +1641,7 @@ process_sample_event(event_t *event, unsigned long offset, unsigned long head)
 	if (!(sample_type & PERF_SAMPLE_RAW))
 		return 0;
 
-	thread = threads__findnew(event->ip.pid, &threads, &last_match);
+	thread = threads__findnew(event->ip.pid);
 
 	if (sample_type & PERF_SAMPLE_TIME) {
 		timestamp = *(u64 *)more_data;
@@ -1725,7 +1717,7 @@ static struct perf_file_handler file_handler = {
 
 static int read_events(void)
 {
-	register_idle_thread(&threads, &last_match);
+	register_idle_thread();
 	register_perf_file_handler(&file_handler);
 
 	return mmap_dispatch_perf_file(&header, input_name, 0, 0, &cwdlen, &cwd);

commit 55ffb7a6bd45d0083ffb132381cb46964a4afe01
Author: Mike Galbraith <efault@gmx.de>
Date:   Sat Oct 10 14:46:04 2009 +0200

    perf sched: Add -C option to measure on a specific CPU
    
    To refresh, trying to sched record only one CPU results in bogus
    latencies as below.
    
    I fixed^Wmade it stop doing the bad thing today, by
    following task migration events properly.
    
    Before:
    
      marge:/root/tmp # taskset -c 1 perf sched record -C 0 -- sleep 10
      marge:/root/tmp # perf sched lat
       -----------------------------------------------------------------------------------------
        Task                  |   Runtime ms  | Switches | Average delay ms | Maximum delay ms |
       -----------------------------------------------------------------------------------------
        Xorg:4943             |      1.290 ms |        1 | avg: 1670.132 ms | max: 1670.132 ms |
        hald-addon-stor:3569  |      0.091 ms |        3 | avg:  658.609 ms | max: 1975.797 ms |
        hald-addon-stor:3573  |      0.209 ms |        4 | avg:  499.138 ms | max: 1990.565 ms |
        audispd:4270          |      0.012 ms |        1 | avg:    0.015 ms | max:    0.015 ms |
      ....
    
      marge:/root/tmp # perf sched trace|grep 'Xorg:4943'
               swapper-0     [000]   401.184013288: sched_stat_runtime: task: Xorg:4943 runtime: 1233188 [ns], vruntime: 19105169779 [ns]
       rt2870TimerQHan-4947  [000]   402.854140127: sched_stat_wait: task: Xorg:4943 wait: 580073 [ns]
       rt2870TimerQHan-4947  [000]   402.854141770: sched_migrate_task: task Xorg:4943 [140] from: 1  to: 0
       rt2870TimerQHan-4947  [000]   402.854143854: sched_stat_wait: task: Xorg:4943 wait: 0 [ns]
       rt2870TimerQHan-4947  [000]   402.854145397: sched_switch: task rt2870TimerQHan:4947 [140] (D) ==> Xorg:4943 [140]
                  Xorg-4943  [000]   402.854193133: sched_stat_runtime: task: Xorg:4943 runtime: 56546 [ns], vruntime: 11766332500 [ns]
                  Xorg-4943  [000]   402.854196842: sched_switch: task Xorg:4943 [140] (S) ==> swapper:0 [140]
    
    After:
    
      marge:/root/tmp # taskset -c 1 perf sched record -C 0 -- sleep 10
      marge:/root/tmp # perf sched lat
       -----------------------------------------------------------------------------------------
        Task                  |   Runtime ms  | Switches | Average delay ms | Maximum delay ms |
       -----------------------------------------------------------------------------------------
        amarokapp:11150       |    271.297 ms |      878 | avg:    0.130 ms | max:    1.057 ms |
        konsole:5965          |      1.370 ms |       12 | avg:    0.092 ms | max:    0.855 ms |
        Xorg:4943             |    179.980 ms |     1109 | avg:    0.087 ms | max:    1.206 ms |
        hald-addon-stor:3574  |      0.212 ms |        9 | avg:    0.040 ms | max:    0.169 ms |
        hald-addon-stor:3570  |      0.223 ms |        9 | avg:    0.037 ms | max:    0.223 ms |
        klauncher:5864        |      0.550 ms |        8 | avg:    0.032 ms | max:    0.048 ms |
    
    The 'Maximum delay ms' results are now sane.
    
    Signed-off-by: Mike Galbraith <efault@gmx.de>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 6b00529ce348..387a44234368 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -33,6 +33,8 @@ static u64			sample_type;
 static char			default_sort_order[] = "avg, max, switch, runtime";
 static char			*sort_order = default_sort_order;
 
+static int			profile_cpu = -1;
+
 static char			*cwd;
 static int			cwdlen;
 
@@ -75,6 +77,7 @@ enum sched_event_type {
 	SCHED_EVENT_RUN,
 	SCHED_EVENT_SLEEP,
 	SCHED_EVENT_WAKEUP,
+	SCHED_EVENT_MIGRATION,
 };
 
 struct sched_atom {
@@ -399,6 +402,8 @@ process_sched_event(struct task_desc *this_task __used, struct sched_atom *atom)
 				ret = sem_post(atom->wait_sem);
 			BUG_ON(ret);
 			break;
+		case SCHED_EVENT_MIGRATION:
+			break;
 		default:
 			BUG_ON(1);
 	}
@@ -746,6 +751,22 @@ struct trace_fork_event {
 	u32 child_pid;
 };
 
+struct trace_migrate_task_event {
+	u32 size;
+
+	u16 common_type;
+	u8 common_flags;
+	u8 common_preempt_count;
+	u32 common_pid;
+	u32 common_tgid;
+
+	char comm[16];
+	u32 pid;
+
+	u32 prio;
+	u32 cpu;
+};
+
 struct trace_sched_handler {
 	void (*switch_event)(struct trace_switch_event *,
 			     struct event *,
@@ -770,6 +791,12 @@ struct trace_sched_handler {
 			   int cpu,
 			   u64 timestamp,
 			   struct thread *thread);
+
+	void (*migrate_task_event)(struct trace_migrate_task_event *,
+			   struct event *,
+			   int cpu,
+			   u64 timestamp,
+			   struct thread *thread);
 };
 
 
@@ -1140,7 +1167,12 @@ latency_wakeup_event(struct trace_wakeup_event *wakeup_event,
 
 	atom = list_entry(atoms->work_list.prev, struct work_atom, list);
 
-	if (atom->state != THREAD_SLEEPING)
+	/*
+	 * You WILL be missing events if you've recorded only
+	 * one CPU, or are only looking at only one, so don't
+	 * make useless noise.
+	 */
+	if (profile_cpu == -1 && atom->state != THREAD_SLEEPING)
 		nr_state_machine_bugs++;
 
 	nr_timestamps++;
@@ -1153,11 +1185,51 @@ latency_wakeup_event(struct trace_wakeup_event *wakeup_event,
 	atom->wake_up_time = timestamp;
 }
 
+static void
+latency_migrate_task_event(struct trace_migrate_task_event *migrate_task_event,
+		     struct event *__event __used,
+		     int cpu __used,
+		     u64 timestamp,
+		     struct thread *thread __used)
+{
+	struct work_atoms *atoms;
+	struct work_atom *atom;
+	struct thread *migrant;
+
+	/*
+	 * Only need to worry about migration when profiling one CPU.
+	 */
+	if (profile_cpu == -1)
+		return;
+
+	migrant = threads__findnew(migrate_task_event->pid, &threads, &last_match);
+	atoms = thread_atoms_search(&atom_root, migrant, &cmp_pid);
+	if (!atoms) {
+		thread_atoms_insert(migrant);
+		register_pid(migrant->pid, migrant->comm);
+		atoms = thread_atoms_search(&atom_root, migrant, &cmp_pid);
+		if (!atoms)
+			die("migration-event: Internal tree error");
+		add_sched_out_event(atoms, 'R', timestamp);
+	}
+
+	BUG_ON(list_empty(&atoms->work_list));
+
+	atom = list_entry(atoms->work_list.prev, struct work_atom, list);
+	atom->sched_in_time = atom->sched_out_time = atom->wake_up_time = timestamp;
+
+	nr_timestamps++;
+
+	if (atom->sched_out_time > timestamp)
+		nr_unordered_timestamps++;
+}
+
 static struct trace_sched_handler lat_ops  = {
 	.wakeup_event		= latency_wakeup_event,
 	.switch_event		= latency_switch_event,
 	.runtime_event		= latency_runtime_event,
 	.fork_event		= latency_fork_event,
+	.migrate_task_event	= latency_migrate_task_event,
 };
 
 static void output_lat_thread(struct work_atoms *work_list)
@@ -1517,6 +1589,26 @@ process_sched_exit_event(struct event *event,
 		printf("sched_exit event %p\n", event);
 }
 
+static void
+process_sched_migrate_task_event(struct raw_event_sample *raw,
+			   struct event *event,
+			   int cpu __used,
+			   u64 timestamp __used,
+			   struct thread *thread __used)
+{
+	struct trace_migrate_task_event migrate_task_event;
+
+	FILL_COMMON_FIELDS(migrate_task_event, event, raw->data);
+
+	FILL_ARRAY(migrate_task_event, comm, event, raw->data);
+	FILL_FIELD(migrate_task_event, pid, event, raw->data);
+	FILL_FIELD(migrate_task_event, prio, event, raw->data);
+	FILL_FIELD(migrate_task_event, cpu, event, raw->data);
+
+	if (trace_handler->migrate_task_event)
+		trace_handler->migrate_task_event(&migrate_task_event, event, cpu, timestamp, thread);
+}
+
 static void
 process_raw_event(event_t *raw_event __used, void *more_data,
 		  int cpu, u64 timestamp, struct thread *thread)
@@ -1540,6 +1632,8 @@ process_raw_event(event_t *raw_event __used, void *more_data,
 		process_sched_fork_event(raw, event, cpu, timestamp, thread);
 	if (!strcmp(event->name, "sched_process_exit"))
 		process_sched_exit_event(event, cpu, timestamp, thread);
+	if (!strcmp(event->name, "sched_migrate_task"))
+		process_sched_migrate_task_event(raw, event, cpu, timestamp, thread);
 }
 
 static int
@@ -1589,6 +1683,9 @@ process_sample_event(event_t *event, unsigned long offset, unsigned long head)
 		return -1;
 	}
 
+	if (profile_cpu != -1 && profile_cpu != (int) cpu)
+		return 0;
+
 	process_raw_event(event, more_data, cpu, timestamp, thread);
 
 	return 0;
@@ -1771,6 +1868,8 @@ static const struct option latency_options[] = {
 		   "sort by key(s): runtime, switch, avg, max"),
 	OPT_BOOLEAN('v', "verbose", &verbose,
 		    "be more verbose (show symbol address, etc)"),
+	OPT_INTEGER('C', "CPU", &profile_cpu,
+		    "CPU to profile on"),
 	OPT_BOOLEAN('D', "dump-raw-trace", &dump_trace,
 		    "dump raw trace in ASCII"),
 	OPT_END()

commit cbef79a82a64ec13e745ce2b0274154ae1e47243
Author: Randy Dunlap <randy.dunlap@oracle.com>
Date:   Mon Oct 5 13:17:29 2009 -0700

    perf tools: Fix const char type propagation
    
    The following perf build warnings/errors in function
    argument types:
    
      builtin-sched.c:1894: warning: passing argument 1 of 'sort_dimension__add' discards qualifiers from pointer target type
      util/trace-event-parse.c:685: warning: passing argument 2 of 'read_expected' discards qualifiers from pointer target type
      util/trace-event-parse.c:741: warning: passing argument 4 of 'test_type_token' discards qualifiers from pointer target type
      util/trace-event-parse.c:706: warning: passing argument 2 of 'read_expected_item' discards qualifiers from pointer target type
    
    ... trigger because older GCC is not able to prove that
    sort_dimension__add() does not change the string.
    
    Some goes for test_type_token().
    
    Fix this by improving type consistency.
    
    Signed-off-by: Randy Dunlap <randy.dunlap@oracle.com>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Mackerras <paulus@samba.org>
    LKML-Reference: <20091005131729.78444bfb.randy.dunlap@oracle.com>
    [ Also remove ugly type cast now unnecessary. ]
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index ea9c15c0cdfe..ce2d5be4f30e 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1287,7 +1287,7 @@ static struct sort_dimension *available_sorts[] = {
 
 static LIST_HEAD(sort_list);
 
-static int sort_dimension__add(char *tok, struct list_head *list)
+static int sort_dimension__add(const char *tok, struct list_head *list)
 {
 	int i;
 
@@ -1917,7 +1917,7 @@ static void setup_sorting(void)
 
 	free(str);
 
-	sort_dimension__add((char *)"pid", &cmp_pid);
+	sort_dimension__add("pid", &cmp_pid);
 }
 
 static const char *record_args[] = {

commit 97ea1a7fa62af0d8d49a0fc12796b0073537c9d8
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Oct 8 21:04:17 2009 +0200

    perf tools: Fix thread comm resolution in perf sched
    
    This reverts commit 9a92b479b2f088ee2d3194243f4c8e59b1b8c9c2 ("perf
    tools: Improve thread comm resolution in perf sched") and fixes the
    real bug.
    
    The bug was elsewhere:
    
    We are failing to resolve thread names in perf sched because the
    table of threads we are building, on top of comm events, has a per
    process granularity. But perf sched, unlike the other perf tools,
    needs a per thread granularity as we are profiling every tasks
    individually.
    
    So fix it by building our threads table using the tid instead of
    the pid as the thread identifier.
    
    v2: Revert the previous fix - it is not really needed
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    LKML-Reference: <1255028657-11158-1-git-send-email-fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 25b91e784332..6b00529ce348 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -638,7 +638,7 @@ process_comm_event(event_t *event, unsigned long offset, unsigned long head)
 {
 	struct thread *thread;
 
-	thread = threads__findnew(event->comm.pid, &threads, &last_match);
+	thread = threads__findnew(event->comm.tid, &threads, &last_match);
 
 	dump_printf("%p [%p]: perf_event_comm: %s:%d\n",
 		(void *)(offset + head),
@@ -1034,36 +1034,6 @@ add_sched_in_event(struct work_atoms *atoms, u64 timestamp)
 	atoms->nb_atoms++;
 }
 
-static struct thread *
-threads__findnew_from_ctx(u32 pid, struct trace_switch_event *switch_event)
-{
-	struct thread *th;
-
-	th = threads__findnew_nocomm(pid, &threads, &last_match);
-	if (th->comm)
-		return th;
-
-	if (pid == switch_event->prev_pid)
-		thread__set_comm(th, switch_event->prev_comm);
-	else
-		thread__set_comm(th, switch_event->next_comm);
-	return th;
-}
-
-static struct thread *
-threads__findnew_from_wakeup(struct trace_wakeup_event *wakeup_event)
-{
-	struct thread *th;
-
-	th =  threads__findnew_nocomm(wakeup_event->pid, &threads, &last_match);
-	if (th->comm)
-		return th;
-
-	thread__set_comm(th, wakeup_event->comm);
-
-	return th;
-}
-
 static void
 latency_switch_event(struct trace_switch_event *switch_event,
 		     struct event *event __used,
@@ -1089,10 +1059,8 @@ latency_switch_event(struct trace_switch_event *switch_event,
 		die("hm, delta: %Ld < 0 ?\n", delta);
 
 
-	sched_out = threads__findnew_from_ctx(switch_event->prev_pid,
-					      switch_event);
-	sched_in = threads__findnew_from_ctx(switch_event->next_pid,
-					     switch_event);
+	sched_out = threads__findnew(switch_event->prev_pid, &threads, &last_match);
+	sched_in = threads__findnew(switch_event->next_pid, &threads, &last_match);
 
 	out_events = thread_atoms_search(&atom_root, sched_out, &cmp_pid);
 	if (!out_events) {
@@ -1158,7 +1126,7 @@ latency_wakeup_event(struct trace_wakeup_event *wakeup_event,
 	if (!wakeup_event->success)
 		return;
 
-	wakee = threads__findnew_from_wakeup(wakeup_event);
+	wakee = threads__findnew(wakeup_event->pid, &threads, &last_match);
 	atoms = thread_atoms_search(&atom_root, wakee, &cmp_pid);
 	if (!atoms) {
 		thread_atoms_insert(wakee);
@@ -1418,10 +1386,8 @@ map_switch_event(struct trace_switch_event *switch_event,
 		die("hm, delta: %Ld < 0 ?\n", delta);
 
 
-	sched_out = threads__findnew_from_ctx(switch_event->prev_pid,
-					      switch_event);
-	sched_in = threads__findnew_from_ctx(switch_event->next_pid,
-					     switch_event);
+	sched_out = threads__findnew(switch_event->prev_pid, &threads, &last_match);
+	sched_in = threads__findnew(switch_event->next_pid, &threads, &last_match);
 
 	curr_thread[this_cpu] = sched_in;
 

commit 9a92b479b2f088ee2d3194243f4c8e59b1b8c9c2
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Oct 8 16:37:12 2009 +0200

    perf tools: Improve thread comm resolution in perf sched
    
    When we get sched traces that involve a task that was already
    created before opening the event, we won't have the comm event for
    it.
    
    So if we can't find the comm event for a given thread, we look at
    the traces that may contain these informations.
    
    Before:
    
     ata/1:371             |      0.000 ms |        1 | avg: 3988.693 ms | max: 3988.693 ms |
     kondemand/1:421       |      0.096 ms |        3 | avg:  345.346 ms | max: 1035.989 ms |
     kondemand/0:420       |      0.025 ms |        3 | avg:  421.332 ms | max:  964.014 ms |
     :5124:5124            |      0.103 ms |        5 | avg:   74.082 ms | max:  277.194 ms |
     :6244:6244            |      0.691 ms |        9 | avg:  125.655 ms | max:  271.306 ms |
     firefox:5080          |      0.924 ms |        5 | avg:   53.833 ms | max:  257.828 ms |
     npviewer.bin:6225     |     21.871 ms |       53 | avg:   22.462 ms | max:  220.835 ms |
     :6245:6245            |      9.631 ms |       21 | avg:   41.864 ms | max:  213.349 ms |
    
    After:
    
     ata/1:371             |      0.000 ms |        1 | avg: 3988.693 ms | max: 3988.693 ms |
     kondemand/1:421       |      0.096 ms |        3 | avg:  345.346 ms | max: 1035.989 ms |
     kondemand/0:420       |      0.025 ms |        3 | avg:  421.332 ms | max:  964.014 ms |
     firefox:5124          |      0.103 ms |        5 | avg:   74.082 ms | max:  277.194 ms |
     npviewer.bin:6244     |      0.691 ms |        9 | avg:  125.655 ms | max:  271.306 ms |
     firefox:5080          |      0.924 ms |        5 | avg:   53.833 ms | max:  257.828 ms |
     npviewer.bin:6225     |     21.871 ms |       53 | avg:   22.462 ms | max:  220.835 ms |
     npviewer.bin:6245     |      9.631 ms |       21 | avg:   41.864 ms | max:  213.349 ms |
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    LKML-Reference: <1255012632-7882-1-git-send-email-fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index e1df7055ab82..25b91e784332 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1034,6 +1034,36 @@ add_sched_in_event(struct work_atoms *atoms, u64 timestamp)
 	atoms->nb_atoms++;
 }
 
+static struct thread *
+threads__findnew_from_ctx(u32 pid, struct trace_switch_event *switch_event)
+{
+	struct thread *th;
+
+	th = threads__findnew_nocomm(pid, &threads, &last_match);
+	if (th->comm)
+		return th;
+
+	if (pid == switch_event->prev_pid)
+		thread__set_comm(th, switch_event->prev_comm);
+	else
+		thread__set_comm(th, switch_event->next_comm);
+	return th;
+}
+
+static struct thread *
+threads__findnew_from_wakeup(struct trace_wakeup_event *wakeup_event)
+{
+	struct thread *th;
+
+	th =  threads__findnew_nocomm(wakeup_event->pid, &threads, &last_match);
+	if (th->comm)
+		return th;
+
+	thread__set_comm(th, wakeup_event->comm);
+
+	return th;
+}
+
 static void
 latency_switch_event(struct trace_switch_event *switch_event,
 		     struct event *event __used,
@@ -1059,8 +1089,10 @@ latency_switch_event(struct trace_switch_event *switch_event,
 		die("hm, delta: %Ld < 0 ?\n", delta);
 
 
-	sched_out = threads__findnew(switch_event->prev_pid, &threads, &last_match);
-	sched_in = threads__findnew(switch_event->next_pid, &threads, &last_match);
+	sched_out = threads__findnew_from_ctx(switch_event->prev_pid,
+					      switch_event);
+	sched_in = threads__findnew_from_ctx(switch_event->next_pid,
+					     switch_event);
 
 	out_events = thread_atoms_search(&atom_root, sched_out, &cmp_pid);
 	if (!out_events) {
@@ -1126,7 +1158,7 @@ latency_wakeup_event(struct trace_wakeup_event *wakeup_event,
 	if (!wakeup_event->success)
 		return;
 
-	wakee = threads__findnew(wakeup_event->pid, &threads, &last_match);
+	wakee = threads__findnew_from_wakeup(wakeup_event);
 	atoms = thread_atoms_search(&atom_root, wakee, &cmp_pid);
 	if (!atoms) {
 		thread_atoms_insert(wakee);
@@ -1386,8 +1418,10 @@ map_switch_event(struct trace_switch_event *switch_event,
 		die("hm, delta: %Ld < 0 ?\n", delta);
 
 
-	sched_out = threads__findnew(switch_event->prev_pid, &threads, &last_match);
-	sched_in = threads__findnew(switch_event->next_pid, &threads, &last_match);
+	sched_out = threads__findnew_from_ctx(switch_event->prev_pid,
+					      switch_event);
+	sched_in = threads__findnew_from_ctx(switch_event->next_pid,
+					     switch_event);
 
 	curr_thread[this_cpu] = sched_in;
 

commit 016e92fbc9ef33689cf654f343a94383d43235e7
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Oct 7 12:47:31 2009 +0200

    perf tools: Unify perf.data mapping and events handling
    
    This librarizes the perf.data file mapping and handling in various
    perf tools, roughly reducing the amount of code and fixing the
    places that mmap from beginning of the file whereas we want to mmap
    from the beginning of the data, leading to page fault because the
    mmap window is too small since the trace info are written in the
    file too.
    
    TODO:
    
     - convert perf timechart too
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arjan van de Ven <arjan@infradead.org>
    LKML-Reference: <20091007104729.GD5043@nowhere>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 18871380b015..e1df7055ab82 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -11,6 +11,7 @@
 #include "util/trace-event.h"
 
 #include "util/debug.h"
+#include "util/data_map.h"
 
 #include <sys/types.h>
 #include <sys/prctl.h>
@@ -20,9 +21,6 @@
 #include <math.h>
 
 static char			const *input_name = "perf.data";
-static int			input;
-static unsigned long		page_size;
-static unsigned long		mmap_window = 32;
 
 static unsigned long		total_comm = 0;
 
@@ -35,6 +33,9 @@ static u64			sample_type;
 static char			default_sort_order[] = "avg, max, switch, runtime";
 static char			*sort_order = default_sort_order;
 
+static char			*cwd;
+static int			cwdlen;
+
 #define PR_SET_NAME		15               /* Set process name */
 #define MAX_CPUS		4096
 
@@ -1594,129 +1595,43 @@ process_sample_event(event_t *event, unsigned long offset, unsigned long head)
 }
 
 static int
-process_event(event_t *event, unsigned long offset, unsigned long head)
+process_lost_event(event_t *event __used,
+		   unsigned long offset __used,
+		   unsigned long head __used)
 {
-	trace_event(event);
-
-	nr_events++;
-	switch (event->header.type) {
-	case PERF_RECORD_MMAP:
-		return 0;
-	case PERF_RECORD_LOST:
-		nr_lost_chunks++;
-		nr_lost_events += event->lost.lost;
-		return 0;
-
-	case PERF_RECORD_COMM:
-		return process_comm_event(event, offset, head);
+	nr_lost_chunks++;
+	nr_lost_events += event->lost.lost;
 
-	case PERF_RECORD_EXIT ... PERF_RECORD_READ:
-		return 0;
+	return 0;
+}
 
-	case PERF_RECORD_SAMPLE:
-		return process_sample_event(event, offset, head);
+static int sample_type_check(u64 type)
+{
+	sample_type = type;
 
-	case PERF_RECORD_MAX:
-	default:
+	if (!(sample_type & PERF_SAMPLE_RAW)) {
+		fprintf(stderr,
+			"No trace sample to read. Did you call perf record "
+			"without -R?");
 		return -1;
 	}
 
 	return 0;
 }
 
+static struct perf_file_handler file_handler = {
+	.process_sample_event	= process_sample_event,
+	.process_comm_event	= process_comm_event,
+	.process_lost_event	= process_lost_event,
+	.sample_type_check	= sample_type_check,
+};
+
 static int read_events(void)
 {
-	int ret, rc = EXIT_FAILURE;
-	unsigned long offset = 0;
-	unsigned long head = 0;
-	struct stat perf_stat;
-	event_t *event;
-	uint32_t size;
-	char *buf;
-
 	register_idle_thread(&threads, &last_match);
+	register_perf_file_handler(&file_handler);
 
-	input = open(input_name, O_RDONLY);
-	if (input < 0) {
-		perror("failed to open file");
-		exit(-1);
-	}
-
-	ret = fstat(input, &perf_stat);
-	if (ret < 0) {
-		perror("failed to stat file");
-		exit(-1);
-	}
-
-	if (!perf_stat.st_size) {
-		fprintf(stderr, "zero-sized file, nothing to do!\n");
-		exit(0);
-	}
-	header = perf_header__read(input);
-	head = header->data_offset;
-	sample_type = perf_header__sample_type(header);
-
-	if (!(sample_type & PERF_SAMPLE_RAW))
-		die("No trace sample to read. Did you call perf record "
-		    "without -R?");
-
-	if (load_kernel() < 0) {
-		perror("failed to load kernel symbols");
-		return EXIT_FAILURE;
-	}
-
-remap:
-	buf = (char *)mmap(NULL, page_size * mmap_window, PROT_READ,
-			   MAP_SHARED, input, offset);
-	if (buf == MAP_FAILED) {
-		perror("failed to mmap file");
-		exit(-1);
-	}
-
-more:
-	event = (event_t *)(buf + head);
-
-	size = event->header.size;
-	if (!size)
-		size = 8;
-
-	if (head + event->header.size >= page_size * mmap_window) {
-		unsigned long shift = page_size * (head / page_size);
-		int res;
-
-		res = munmap(buf, page_size * mmap_window);
-		assert(res == 0);
-
-		offset += shift;
-		head -= shift;
-		goto remap;
-	}
-
-	size = event->header.size;
-
-
-	if (!size || process_event(event, offset, head) < 0) {
-
-		/*
-		 * assume we lost track of the stream, check alignment, and
-		 * increment a single u64 in the hope to catch on again 'soon'.
-		 */
-
-		if (unlikely(head & 7))
-			head &= ~7ULL;
-
-		size = 8;
-	}
-
-	head += size;
-
-	if (offset + head < (unsigned long)perf_stat.st_size)
-		goto more;
-
-	rc = EXIT_SUCCESS;
-	close(input);
-
-	return rc;
+	return mmap_dispatch_perf_file(&header, input_name, 0, 0, &cwdlen, &cwd);
 }
 
 static void print_bad_events(void)
@@ -1934,7 +1849,6 @@ static int __cmd_record(int argc, const char **argv)
 int cmd_sched(int argc, const char **argv, const char *prefix __used)
 {
 	symbol__init();
-	page_size = getpagesize();
 
 	argc = parse_options(argc, argv, sched_options, sched_usage,
 			     PARSE_OPT_STOP_AT_NON_OPTION);

commit 03456a158d9067d2f657bec170506009db81756d
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Oct 6 23:36:47 2009 +0200

    perf tools: Merge trace.info content into perf.data
    
    This drops the trace.info file and move its contents into the
    common perf.data file.
    
    This is done by creating a new trace_info section into this file. A
    user of perf headers needs to call perf_header__set_trace_info() to
    save the trace meta informations into the perf.data file.
    
    A file created by perf after his patch is unsupported by previous
    version because the size of the headers have increased.
    
    That said, it's two new fields that have been added in the end of
    the headers, and those could be ignored by previous versions if
    they just handled the dynamic header size and then ignore the
    unknow part. The offsets guarantee the compatibility. We'll do a
    -stable fix for that.
    
    But current previous versions handle the header size using its
    static size, not dynamic, then it's not backward compatible with
    trace records.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    LKML-Reference: <20091006213643.GA5343@nowhere>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 4470f2535706..18871380b015 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1634,7 +1634,6 @@ static int read_events(void)
 	uint32_t size;
 	char *buf;
 
-	trace_report();
 	register_idle_thread(&threads, &last_match);
 
 	input = open(input_name, O_RDONLY);

commit a80deb622dba7dfb65d9e27b6b74b7c1963c3635
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Mon Sep 28 15:23:51 2009 -0300

    perf sched: Remove dead code
    
    Several variables are not used at all, cut'n'paste leftovers.
    
    Also check if the sample_type is RAW earlier, to avoid needless
    searches.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frédéric Weisbecker <fweisbec@gmail.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index ea9c15c0cdfe..4470f2535706 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1544,16 +1544,15 @@ process_raw_event(event_t *raw_event __used, void *more_data,
 static int
 process_sample_event(event_t *event, unsigned long offset, unsigned long head)
 {
-	char level;
-	int show = 0;
-	struct dso *dso = NULL;
 	struct thread *thread;
 	u64 ip = event->ip.ip;
 	u64 timestamp = -1;
 	u32 cpu = -1;
 	u64 period = 1;
 	void *more_data = event->ip.__more_data;
-	int cpumode;
+
+	if (!(sample_type & PERF_SAMPLE_RAW))
+		return 0;
 
 	thread = threads__findnew(event->ip.pid, &threads, &last_match);
 
@@ -1589,32 +1588,7 @@ process_sample_event(event_t *event, unsigned long offset, unsigned long head)
 		return -1;
 	}
 
-	cpumode = event->header.misc & PERF_RECORD_MISC_CPUMODE_MASK;
-
-	if (cpumode == PERF_RECORD_MISC_KERNEL) {
-		show = SHOW_KERNEL;
-		level = 'k';
-
-		dso = kernel_dso;
-
-		dump_printf(" ...... dso: %s\n", dso->name);
-
-	} else if (cpumode == PERF_RECORD_MISC_USER) {
-
-		show = SHOW_USER;
-		level = '.';
-
-	} else {
-		show = SHOW_HV;
-		level = 'H';
-
-		dso = hypervisor_dso;
-
-		dump_printf(" ...... dso: [hypervisor]\n");
-	}
-
-	if (sample_type & PERF_SAMPLE_RAW)
-		process_raw_event(event, more_data, cpu, timestamp, thread);
+	process_raw_event(event, more_data, cpu, timestamp, thread);
 
 	return 0;
 }

commit cdd6c482c9ff9c55475ee7392ec8f672eddb7be6
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Sep 21 12:02:48 2009 +0200

    perf: Do the big rename: Performance Counters -> Performance Events
    
    Bye-bye Performance Counters, welcome Performance Events!
    
    In the past few months the perfcounters subsystem has grown out its
    initial role of counting hardware events, and has become (and is
    becoming) a much broader generic event enumeration, reporting, logging,
    monitoring, analysis facility.
    
    Naming its core object 'perf_counter' and naming the subsystem
    'perfcounters' has become more and more of a misnomer. With pending
    code like hw-breakpoints support the 'counter' name is less and
    less appropriate.
    
    All in one, we've decided to rename the subsystem to 'performance
    events' and to propagate this rename through all fields, variables
    and API names. (in an ABI compatible fashion)
    
    The word 'event' is also a bit shorter than 'counter' - which makes
    it slightly more convenient to write/handle as well.
    
    Thanks goes to Stephane Eranian who first observed this misnomer and
    suggested a rename.
    
    User-space tooling and ABI compatibility is not affected - this patch
    should be function-invariant. (Also, defconfigs were not touched to
    keep the size down.)
    
    This patch has been generated via the following script:
    
      FILES=$(find * -type f | grep -vE 'oprofile|[^K]config')
    
      sed -i \
        -e 's/PERF_EVENT_/PERF_RECORD_/g' \
        -e 's/PERF_COUNTER/PERF_EVENT/g' \
        -e 's/perf_counter/perf_event/g' \
        -e 's/nb_counters/nb_events/g' \
        -e 's/swcounter/swevent/g' \
        -e 's/tpcounter_event/tp_event/g' \
        $FILES
    
      for N in $(find . -name perf_counter.[ch]); do
        M=$(echo $N | sed 's/perf_counter/perf_event/g')
        mv $N $M
      done
    
      FILES=$(find . -name perf_event.*)
    
      sed -i \
        -e 's/COUNTER_MASK/REG_MASK/g' \
        -e 's/COUNTER/EVENT/g' \
        -e 's/\<event\>/event_id/g' \
        -e 's/counter/event/g' \
        -e 's/Counter/Event/g' \
        $FILES
    
    ... to keep it as correct as possible. This script can also be
    used by anyone who has pending perfcounters patches - it converts
    a Linux kernel tree over to the new naming. We tried to time this
    change to the point in time where the amount of pending patches
    is the smallest: the end of the merge window.
    
    Namespace clashes were fixed up in a preparatory patch - and some
    stylistic fallout will be fixed up in a subsequent patch.
    
    ( NOTE: 'counters' are still the proper terminology when we deal
      with hardware registers - and these sed scripts are a bit
      over-eager in renaming them. I've undone some of that, but
      in case there's something left where 'counter' would be
      better than 'event' we can undo that on an individual basis
      instead of touching an otherwise nicely automated patch. )
    
    Suggested-by: Stephane Eranian <eranian@google.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Paul Mackerras <paulus@samba.org>
    Reviewed-by: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Kyle McMartin <kyle@mcmartin.ca>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: <linux-arch@vger.kernel.org>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 275d79c6627a..ea9c15c0cdfe 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1573,7 +1573,7 @@ process_sample_event(event_t *event, unsigned long offset, unsigned long head)
 		more_data += sizeof(u64);
 	}
 
-	dump_printf("%p [%p]: PERF_EVENT_SAMPLE (IP, %d): %d/%d: %p period: %Ld\n",
+	dump_printf("%p [%p]: PERF_RECORD_SAMPLE (IP, %d): %d/%d: %p period: %Ld\n",
 		(void *)(offset + head),
 		(void *)(long)(event->header.size),
 		event->header.misc,
@@ -1589,9 +1589,9 @@ process_sample_event(event_t *event, unsigned long offset, unsigned long head)
 		return -1;
 	}
 
-	cpumode = event->header.misc & PERF_EVENT_MISC_CPUMODE_MASK;
+	cpumode = event->header.misc & PERF_RECORD_MISC_CPUMODE_MASK;
 
-	if (cpumode == PERF_EVENT_MISC_KERNEL) {
+	if (cpumode == PERF_RECORD_MISC_KERNEL) {
 		show = SHOW_KERNEL;
 		level = 'k';
 
@@ -1599,7 +1599,7 @@ process_sample_event(event_t *event, unsigned long offset, unsigned long head)
 
 		dump_printf(" ...... dso: %s\n", dso->name);
 
-	} else if (cpumode == PERF_EVENT_MISC_USER) {
+	} else if (cpumode == PERF_RECORD_MISC_USER) {
 
 		show = SHOW_USER;
 		level = '.';
@@ -1626,23 +1626,23 @@ process_event(event_t *event, unsigned long offset, unsigned long head)
 
 	nr_events++;
 	switch (event->header.type) {
-	case PERF_EVENT_MMAP:
+	case PERF_RECORD_MMAP:
 		return 0;
-	case PERF_EVENT_LOST:
+	case PERF_RECORD_LOST:
 		nr_lost_chunks++;
 		nr_lost_events += event->lost.lost;
 		return 0;
 
-	case PERF_EVENT_COMM:
+	case PERF_RECORD_COMM:
 		return process_comm_event(event, offset, head);
 
-	case PERF_EVENT_EXIT ... PERF_EVENT_READ:
+	case PERF_RECORD_EXIT ... PERF_RECORD_READ:
 		return 0;
 
-	case PERF_EVENT_SAMPLE:
+	case PERF_RECORD_SAMPLE:
 		return process_sample_event(event, offset, head);
 
-	case PERF_EVENT_MAX:
+	case PERF_RECORD_MAX:
 	default:
 		return -1;
 	}

commit 4b77a7297795229eca96c41e1709a3c87909fabe
Author: Mike Galbraith <efault@gmx.de>
Date:   Fri Sep 18 08:22:24 2009 +0200

    perf sched: Add --input=file option to builtin-sched.c
    
    perf sched record passes unparsed args on to perf record, so
    specifying an output file via perf sched record -o FILE (cmd) just
    works.  Ergo, provide an option to specify input file as well.
    
    Also add the missing 'map' command to help.
    
    Signed-off-by: Mike Galbraith <efault@gmx.de>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1253254944.20589.11.camel@marge.simson.net>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 9e04827d16be..275d79c6627a 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1859,11 +1859,13 @@ static void __cmd_replay(void)
 
 
 static const char * const sched_usage[] = {
-	"perf sched [<options>] {record|latency|replay|trace}",
+	"perf sched [<options>] {record|latency|map|replay|trace}",
 	NULL
 };
 
 static const struct option sched_options[] = {
+	OPT_STRING('i', "input", &input_name, "file",
+		    "input file name"),
 	OPT_BOOLEAN('v', "verbose", &verbose,
 		    "be more verbose (show symbol address, etc)"),
 	OPT_BOOLEAN('D', "dump-raw-trace", &dump_trace,

commit 40749d0ff49f99c3661b336fe5e5625207bd925a
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Sep 17 18:24:55 2009 +0200

    perf sched: Determine the number of CPUs automatically
    
    For 'perf sched map' output, determine max_cpu automatically,
    instead of the static default of 15.
    
    [ v2: use sysconf() pointed out by Arjan van de Ven <arjan@infradead.org> ]
    
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index f67e351b050b..9e04827d16be 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1347,7 +1347,7 @@ process_sched_wakeup_event(struct raw_event_sample *raw,
  * Track the current task - that way we can know whether there's any
  * weird events, such as a task being switched away that is not current.
  */
-static int max_cpu = 15;
+static int max_cpu;
 
 static u32 curr_pid[MAX_CPUS] = { [0 ... MAX_CPUS-1] = -1 };
 
@@ -1818,6 +1818,8 @@ static struct trace_sched_handler map_ops  = {
 
 static void __cmd_map(void)
 {
+	max_cpu = sysconf(_SC_NPROCESSORS_CONF);
+
 	setup_pager();
 	read_events();
 	print_bad_events();

commit 0ec04e16d08b69d8da46abbcfa3e3f2cd9738852
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Sep 16 17:40:48 2009 +0200

    perf sched: Add 'perf sched map' scheduling event map printout
    
    This prints a textual context-switching outline of workload
    captured via perf sched record.
    
    For example, on a 16 CPU box it outputs:
    
       N1  O1  .   .   .   S1  .   .   .   B0  .  *I0  C1  .   M1  .    23002.773423 secs
       N1  O1  .  *Q0  .   S1  .   .   .   B0  .   I0  C1  .   M1  .    23002.773423 secs
       N1  O1  .   Q0  .   S1  .   .   .   B0  .  *R1  C1  .   M1  .    23002.773485 secs
       N1  O1  .   Q0  .   S1  .  *S0  .   B0  .   R1  C1  .   M1  .    23002.773478 secs
      *L0  O1  .   Q0  .   S1  .   S0  .   B0  .   R1  C1  .   M1  .    23002.773523 secs
       L0  O1  .  *.   .   S1  .   S0  .   B0  .   R1  C1  .   M1  .    23002.773531 secs
       L0  O1  .   .   .   S1  .   S0  .   B0  .   R1  C1 *T1  M1  .    23002.773547 secs T1 => irqbalance:2089
       L0  O1  .   .   .   S1  .   S0  .  *P0  .   R1  C1  T1  M1  .    23002.773549 secs
      *N1  O1  .   .   .   S1  .   S0  .   P0  .   R1  C1  T1  M1  .    23002.773566 secs
       N1  O1  .   .   .  *J0  .   S0  .   P0  .   R1  C1  T1  M1  .    23002.773571 secs
       N1  O1  .   .   .   J0  .   S0 *B0  P0  .   R1  C1  T1  M1  .    23002.773592 secs
       N1  O1  .   .   .   J0  .  *U0  B0  P0  .   R1  C1  T1  M1  .    23002.773582 secs
       N1  O1  .   .   .  *S1  .   U0  B0  P0  .   R1  C1  T1  M1  .    23002.773604 secs
       N1  O1  .   .   .   S1  .   U0  B0 *.   .   R1  C1  T1  M1  .    23002.773615 secs
       N1  O1  .   .   .   S1  .   U0  B0  .   .  *K0  C1  T1  M1  .    23002.773631 secs
       N1  O1  .  *M0  .   S1  .   U0  B0  .   .   K0  C1  T1  M1  .    23002.773624 secs
       N1  O1  .   M0  .   S1  .   U0 *.   .   .   K0  C1  T1  M1  .    23002.773644 secs
       N1  O1  .   M0  .   S1  .   U0  .   .   .  *R1  C1  T1  M1  .    23002.773662 secs
       N1  O1  .   M0  .   S1  .  *.   .   .   .   R1  C1  T1  M1  .    23002.773648 secs
       N1  O1  .  *.   .   S1  .   .   .   .   .   R1  C1  T1  M1  .    23002.773680 secs
       N1  O1  .   .   .  *L0  .   .   .   .   .   R1  C1  T1  M1  .    23002.773717 secs
      *N0  O1  .   .   .   L0  .   .   .   .   .   R1  C1  T1  M1  .    23002.773709 secs
      *N1  O1  .   .   .   L0  .   .   .   .   .   R1  C1  T1  M1  .    23002.773747 secs
    
    Columns stand for individual CPUs, from CPU0 to CPU15, and the
    two-letter shortcuts stand for tasks that are running on a CPU.
    
    '*' denotes the CPU that had the event.
    
    A dot signals an idle CPU.
    
    New tasks are assigned new two-letter shortcuts - when they occur
    first they are printed. In the above example 'T1' stood for irqbalance:
    
          T1 => irqbalance:2089
    
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index da8f67483ae7..f67e351b050b 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -159,8 +159,6 @@ static struct rb_root		atom_root, sorted_atom_root;
 static u64			all_runtime;
 static u64			all_count;
 
-static int read_events(void);
-
 
 static u64 get_nsecs(void)
 {
@@ -634,38 +632,6 @@ static void test_calibrations(void)
 	printf("the sleep test took %Ld nsecs\n", T1-T0);
 }
 
-static void __cmd_replay(void)
-{
-	unsigned long i;
-
-	calibrate_run_measurement_overhead();
-	calibrate_sleep_measurement_overhead();
-
-	test_calibrations();
-
-	read_events();
-
-	printf("nr_run_events:        %ld\n", nr_run_events);
-	printf("nr_sleep_events:      %ld\n", nr_sleep_events);
-	printf("nr_wakeup_events:     %ld\n", nr_wakeup_events);
-
-	if (targetless_wakeups)
-		printf("target-less wakeups:  %ld\n", targetless_wakeups);
-	if (multitarget_wakeups)
-		printf("multi-target wakeups: %ld\n", multitarget_wakeups);
-	if (nr_run_events_optimized)
-		printf("run atoms optimized: %ld\n",
-			nr_run_events_optimized);
-
-	print_task_traces();
-	add_cross_task_wakeups();
-
-	create_tasks();
-	printf("------------------------------------------------------------\n");
-	for (i = 0; i < replay_repeat; i++)
-		run_one_test();
-}
-
 static int
 process_comm_event(event_t *event, unsigned long offset, unsigned long head)
 {
@@ -1354,64 +1320,6 @@ static void sort_lat(void)
 	}
 }
 
-static void __cmd_lat(void)
-{
-	struct rb_node *next;
-
-	setup_pager();
-	read_events();
-	sort_lat();
-
-	printf("\n -----------------------------------------------------------------------------------------\n");
-	printf("  Task                  |   Runtime ms  | Switches | Average delay ms | Maximum delay ms |\n");
-	printf(" -----------------------------------------------------------------------------------------\n");
-
-	next = rb_first(&sorted_atom_root);
-
-	while (next) {
-		struct work_atoms *work_list;
-
-		work_list = rb_entry(next, struct work_atoms, node);
-		output_lat_thread(work_list);
-		next = rb_next(next);
-	}
-
-	printf(" -----------------------------------------------------------------------------------------\n");
-	printf("  TOTAL:                |%11.3f ms |%9Ld |\n",
-		(double)all_runtime/1e6, all_count);
-
-	printf(" ---------------------------------------------------\n");
-	if (nr_unordered_timestamps && nr_timestamps) {
-		printf("  INFO: %.3f%% unordered timestamps (%ld out of %ld)\n",
-			(double)nr_unordered_timestamps/(double)nr_timestamps*100.0,
-			nr_unordered_timestamps, nr_timestamps);
-	} else {
-	}
-	if (nr_lost_events && nr_events) {
-		printf("  INFO: %.3f%% lost events (%ld out of %ld, in %ld chunks)\n",
-			(double)nr_lost_events/(double)nr_events*100.0,
-			nr_lost_events, nr_events, nr_lost_chunks);
-	}
-	if (nr_state_machine_bugs && nr_timestamps) {
-		printf("  INFO: %.3f%% state machine bugs (%ld out of %ld)",
-			(double)nr_state_machine_bugs/(double)nr_timestamps*100.0,
-			nr_state_machine_bugs, nr_timestamps);
-		if (nr_lost_events)
-			printf(" (due to lost events?)");
-		printf("\n");
-	}
-	if (nr_context_switch_bugs && nr_timestamps) {
-		printf("  INFO: %.3f%% context switch bugs (%ld out of %ld)",
-			(double)nr_context_switch_bugs/(double)nr_timestamps*100.0,
-			nr_context_switch_bugs, nr_timestamps);
-		if (nr_lost_events)
-			printf(" (due to lost events?)");
-		printf("\n");
-	}
-	printf("\n");
-
-}
-
 static struct trace_sched_handler *trace_handler;
 
 static void
@@ -1431,19 +1339,106 @@ process_sched_wakeup_event(struct raw_event_sample *raw,
 	FILL_FIELD(wakeup_event, success, event, raw->data);
 	FILL_FIELD(wakeup_event, cpu, event, raw->data);
 
-	trace_handler->wakeup_event(&wakeup_event, event, cpu, timestamp, thread);
+	if (trace_handler->wakeup_event)
+		trace_handler->wakeup_event(&wakeup_event, event, cpu, timestamp, thread);
 }
 
 /*
  * Track the current task - that way we can know whether there's any
  * weird events, such as a task being switched away that is not current.
  */
+static int max_cpu = 15;
+
 static u32 curr_pid[MAX_CPUS] = { [0 ... MAX_CPUS-1] = -1 };
 
+static struct thread *curr_thread[MAX_CPUS];
+
+static char next_shortname1 = 'A';
+static char next_shortname2 = '0';
+
+static void
+map_switch_event(struct trace_switch_event *switch_event,
+		 struct event *event __used,
+		 int this_cpu,
+		 u64 timestamp,
+		 struct thread *thread __used)
+{
+	struct thread *sched_out, *sched_in;
+	int new_shortname;
+	u64 timestamp0;
+	s64 delta;
+	int cpu;
+
+	BUG_ON(this_cpu >= MAX_CPUS || this_cpu < 0);
+
+	if (this_cpu > max_cpu)
+		max_cpu = this_cpu;
+
+	timestamp0 = cpu_last_switched[this_cpu];
+	cpu_last_switched[this_cpu] = timestamp;
+	if (timestamp0)
+		delta = timestamp - timestamp0;
+	else
+		delta = 0;
+
+	if (delta < 0)
+		die("hm, delta: %Ld < 0 ?\n", delta);
+
+
+	sched_out = threads__findnew(switch_event->prev_pid, &threads, &last_match);
+	sched_in = threads__findnew(switch_event->next_pid, &threads, &last_match);
+
+	curr_thread[this_cpu] = sched_in;
+
+	printf("  ");
+
+	new_shortname = 0;
+	if (!sched_in->shortname[0]) {
+		sched_in->shortname[0] = next_shortname1;
+		sched_in->shortname[1] = next_shortname2;
+
+		if (next_shortname1 < 'Z') {
+			next_shortname1++;
+		} else {
+			next_shortname1='A';
+			if (next_shortname2 < '9') {
+				next_shortname2++;
+			} else {
+				next_shortname2='0';
+			}
+		}
+		new_shortname = 1;
+	}
+
+	for (cpu = 0; cpu <= max_cpu; cpu++) {
+		if (cpu != this_cpu)
+			printf(" ");
+		else
+			printf("*");
+
+		if (curr_thread[cpu]) {
+			if (curr_thread[cpu]->pid)
+				printf("%2s ", curr_thread[cpu]->shortname);
+			else
+				printf(".  ");
+		} else
+			printf("   ");
+	}
+
+	printf("  %12.6f secs ", (double)timestamp/1e9);
+	if (new_shortname) {
+		printf("%s => %s:%d\n",
+			sched_in->shortname, sched_in->comm, sched_in->pid);
+	} else {
+		printf("\n");
+	}
+}
+
+
 static void
 process_sched_switch_event(struct raw_event_sample *raw,
 			   struct event *event,
-			   int cpu,
+			   int this_cpu,
 			   u64 timestamp __used,
 			   struct thread *thread __used)
 {
@@ -1459,17 +1454,18 @@ process_sched_switch_event(struct raw_event_sample *raw,
 	FILL_FIELD(switch_event, next_pid, event, raw->data);
 	FILL_FIELD(switch_event, next_prio, event, raw->data);
 
-	if (curr_pid[cpu] != (u32)-1) {
+	if (curr_pid[this_cpu] != (u32)-1) {
 		/*
 		 * Are we trying to switch away a PID that is
 		 * not current?
 		 */
-		if (curr_pid[cpu] != switch_event.prev_pid)
+		if (curr_pid[this_cpu] != switch_event.prev_pid)
 			nr_context_switch_bugs++;
 	}
-	curr_pid[cpu] = switch_event.next_pid;
+	if (trace_handler->switch_event)
+		trace_handler->switch_event(&switch_event, event, this_cpu, timestamp, thread);
 
-	trace_handler->switch_event(&switch_event, event, cpu, timestamp, thread);
+	curr_pid[this_cpu] = switch_event.next_pid;
 }
 
 static void
@@ -1486,7 +1482,8 @@ process_sched_runtime_event(struct raw_event_sample *raw,
 	FILL_FIELD(runtime_event, runtime, event, raw->data);
 	FILL_FIELD(runtime_event, vruntime, event, raw->data);
 
-	trace_handler->runtime_event(&runtime_event, event, cpu, timestamp, thread);
+	if (trace_handler->runtime_event)
+		trace_handler->runtime_event(&runtime_event, event, cpu, timestamp, thread);
 }
 
 static void
@@ -1505,7 +1502,8 @@ process_sched_fork_event(struct raw_event_sample *raw,
 	FILL_ARRAY(fork_event, child_comm, event, raw->data);
 	FILL_FIELD(fork_event, child_pid, event, raw->data);
 
-	trace_handler->fork_event(&fork_event, event, cpu, timestamp, thread);
+	if (trace_handler->fork_event)
+		trace_handler->fork_event(&fork_event, event, cpu, timestamp, thread);
 }
 
 static void
@@ -1748,6 +1746,116 @@ static int read_events(void)
 	return rc;
 }
 
+static void print_bad_events(void)
+{
+	if (nr_unordered_timestamps && nr_timestamps) {
+		printf("  INFO: %.3f%% unordered timestamps (%ld out of %ld)\n",
+			(double)nr_unordered_timestamps/(double)nr_timestamps*100.0,
+			nr_unordered_timestamps, nr_timestamps);
+	}
+	if (nr_lost_events && nr_events) {
+		printf("  INFO: %.3f%% lost events (%ld out of %ld, in %ld chunks)\n",
+			(double)nr_lost_events/(double)nr_events*100.0,
+			nr_lost_events, nr_events, nr_lost_chunks);
+	}
+	if (nr_state_machine_bugs && nr_timestamps) {
+		printf("  INFO: %.3f%% state machine bugs (%ld out of %ld)",
+			(double)nr_state_machine_bugs/(double)nr_timestamps*100.0,
+			nr_state_machine_bugs, nr_timestamps);
+		if (nr_lost_events)
+			printf(" (due to lost events?)");
+		printf("\n");
+	}
+	if (nr_context_switch_bugs && nr_timestamps) {
+		printf("  INFO: %.3f%% context switch bugs (%ld out of %ld)",
+			(double)nr_context_switch_bugs/(double)nr_timestamps*100.0,
+			nr_context_switch_bugs, nr_timestamps);
+		if (nr_lost_events)
+			printf(" (due to lost events?)");
+		printf("\n");
+	}
+}
+
+static void __cmd_lat(void)
+{
+	struct rb_node *next;
+
+	setup_pager();
+	read_events();
+	sort_lat();
+
+	printf("\n -----------------------------------------------------------------------------------------\n");
+	printf("  Task                  |   Runtime ms  | Switches | Average delay ms | Maximum delay ms |\n");
+	printf(" -----------------------------------------------------------------------------------------\n");
+
+	next = rb_first(&sorted_atom_root);
+
+	while (next) {
+		struct work_atoms *work_list;
+
+		work_list = rb_entry(next, struct work_atoms, node);
+		output_lat_thread(work_list);
+		next = rb_next(next);
+	}
+
+	printf(" -----------------------------------------------------------------------------------------\n");
+	printf("  TOTAL:                |%11.3f ms |%9Ld |\n",
+		(double)all_runtime/1e6, all_count);
+
+	printf(" ---------------------------------------------------\n");
+
+	print_bad_events();
+	printf("\n");
+
+}
+
+static struct trace_sched_handler map_ops  = {
+	.wakeup_event		= NULL,
+	.switch_event		= map_switch_event,
+	.runtime_event		= NULL,
+	.fork_event		= NULL,
+};
+
+static void __cmd_map(void)
+{
+	setup_pager();
+	read_events();
+	print_bad_events();
+}
+
+static void __cmd_replay(void)
+{
+	unsigned long i;
+
+	calibrate_run_measurement_overhead();
+	calibrate_sleep_measurement_overhead();
+
+	test_calibrations();
+
+	read_events();
+
+	printf("nr_run_events:        %ld\n", nr_run_events);
+	printf("nr_sleep_events:      %ld\n", nr_sleep_events);
+	printf("nr_wakeup_events:     %ld\n", nr_wakeup_events);
+
+	if (targetless_wakeups)
+		printf("target-less wakeups:  %ld\n", targetless_wakeups);
+	if (multitarget_wakeups)
+		printf("multi-target wakeups: %ld\n", multitarget_wakeups);
+	if (nr_run_events_optimized)
+		printf("run atoms optimized: %ld\n",
+			nr_run_events_optimized);
+
+	print_task_traces();
+	add_cross_task_wakeups();
+
+	create_tasks();
+	printf("------------------------------------------------------------\n");
+	for (i = 0; i < replay_repeat; i++)
+		run_one_test();
+}
+
+
 static const char * const sched_usage[] = {
 	"perf sched [<options>] {record|latency|replay|trace}",
 	NULL
@@ -1867,6 +1975,10 @@ int cmd_sched(int argc, const char **argv, const char *prefix __used)
 		}
 		setup_sorting();
 		__cmd_lat();
+	} else if (!strcmp(argv[0], "map")) {
+		trace_handler = &map_ops;
+		setup_sorting();
+		__cmd_map();
 	} else if (!strncmp(argv[0], "rep", 3)) {
 		trace_handler = &replay_ops;
 		if (argc) {

commit 80ed0987f363d7eb50193df3e6f6d71451f74bc3
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Sep 16 14:12:36 2009 +0200

    perf sched: Make idle thread and comm/pid names more consistent
    
    Peter noticed that we have 3 ways of referring to the idle thread:
    
     [idle]:0
     swapper:0
     swapper-0
    
    Standardize on 'swapper:0'.
    
    Reported-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 2d542368de3c..da8f67483ae7 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1204,13 +1204,13 @@ static void output_lat_thread(struct work_atoms *work_list)
 	/*
 	 * Ignore idle threads:
 	 */
-	if (!work_list->thread->pid)
+	if (!strcmp(work_list->thread->comm, "swapper"))
 		return;
 
 	all_runtime += work_list->total_runtime;
 	all_count += work_list->nb_atoms;
 
-	ret = printf("  %s-%d ", work_list->thread->comm, work_list->thread->pid);
+	ret = printf("  %s:%d ", work_list->thread->comm, work_list->thread->pid);
 
 	for (i = 0; i < 24 - ret; i++)
 		printf(" ");

commit c8a37751043427c6e4397a2cbfd617cb5f215c72
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Sep 16 14:07:00 2009 +0200

    perf sched: Sanity check context switch events
    
    Use 'perf sched latency' to track the current task based on
    context-switch events, and flag the cases where there's some
    impossible transition: such as a PID being switched out that
    was not switched in.
    
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 1f0f9be34faa..2d542368de3c 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -119,6 +119,7 @@ static unsigned long		replay_repeat = 10;
 static unsigned long		nr_timestamps;
 static unsigned long		nr_unordered_timestamps;
 static unsigned long		nr_state_machine_bugs;
+static unsigned long		nr_context_switch_bugs;
 static unsigned long		nr_events;
 static unsigned long		nr_lost_chunks;
 static unsigned long		nr_lost_events;
@@ -1399,6 +1400,14 @@ static void __cmd_lat(void)
 			printf(" (due to lost events?)");
 		printf("\n");
 	}
+	if (nr_context_switch_bugs && nr_timestamps) {
+		printf("  INFO: %.3f%% context switch bugs (%ld out of %ld)",
+			(double)nr_context_switch_bugs/(double)nr_timestamps*100.0,
+			nr_context_switch_bugs, nr_timestamps);
+		if (nr_lost_events)
+			printf(" (due to lost events?)");
+		printf("\n");
+	}
 	printf("\n");
 
 }
@@ -1425,10 +1434,16 @@ process_sched_wakeup_event(struct raw_event_sample *raw,
 	trace_handler->wakeup_event(&wakeup_event, event, cpu, timestamp, thread);
 }
 
+/*
+ * Track the current task - that way we can know whether there's any
+ * weird events, such as a task being switched away that is not current.
+ */
+static u32 curr_pid[MAX_CPUS] = { [0 ... MAX_CPUS-1] = -1 };
+
 static void
 process_sched_switch_event(struct raw_event_sample *raw,
 			   struct event *event,
-			   int cpu __used,
+			   int cpu,
 			   u64 timestamp __used,
 			   struct thread *thread __used)
 {
@@ -1444,6 +1459,16 @@ process_sched_switch_event(struct raw_event_sample *raw,
 	FILL_FIELD(switch_event, next_pid, event, raw->data);
 	FILL_FIELD(switch_event, next_prio, event, raw->data);
 
+	if (curr_pid[cpu] != (u32)-1) {
+		/*
+		 * Are we trying to switch away a PID that is
+		 * not current?
+		 */
+		if (curr_pid[cpu] != switch_event.prev_pid)
+			nr_context_switch_bugs++;
+	}
+	curr_pid[cpu] = switch_event.next_pid;
+
 	trace_handler->switch_event(&switch_event, event, cpu, timestamp, thread);
 }
 

commit dc02bf7178c8e2cb3d442ae19027b736d51c7dd5
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Sep 16 13:45:00 2009 +0200

    perf sched: Account for lost events, increase default buffering
    
    Output such lost event and state machine weirdness stats:
    
       TOTAL:                |  14974.910 ms |    46384 |
      ---------------------------------------------------
       INFO: 8.865% lost events (19132 out of 215819, in 8 chunks)
       INFO: 0.198% state machine bugs (49 out of 24708) (due to lost events?)
    
    And increase buffering to -m 1024 (4 MB) by default. Since we
    use output multiplexing that kind of space is needed.
    
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index adcb563ec4d2..1f0f9be34faa 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -117,7 +117,11 @@ static u64			run_avg;
 
 static unsigned long		replay_repeat = 10;
 static unsigned long		nr_timestamps;
-static unsigned long		unordered_timestamps;
+static unsigned long		nr_unordered_timestamps;
+static unsigned long		nr_state_machine_bugs;
+static unsigned long		nr_events;
+static unsigned long		nr_lost_chunks;
+static unsigned long		nr_lost_events;
 
 #define TASK_STATE_TO_CHAR_STR "RSDTtZX"
 
@@ -668,14 +672,14 @@ process_comm_event(event_t *event, unsigned long offset, unsigned long head)
 
 	thread = threads__findnew(event->comm.pid, &threads, &last_match);
 
-	dump_printf("%p [%p]: PERF_EVENT_COMM: %s:%d\n",
+	dump_printf("%p [%p]: perf_event_comm: %s:%d\n",
 		(void *)(offset + head),
 		(void *)(long)(event->header.size),
 		event->comm.comm, event->comm.pid);
 
 	if (thread == NULL ||
 	    thread__set_comm(thread, event->comm.comm)) {
-		dump_printf("problem processing PERF_EVENT_COMM, skipping event.\n");
+		dump_printf("problem processing perf_event_comm, skipping event.\n");
 		return -1;
 	}
 	total_comm++;
@@ -1168,14 +1172,12 @@ latency_wakeup_event(struct trace_wakeup_event *wakeup_event,
 
 	atom = list_entry(atoms->work_list.prev, struct work_atom, list);
 
-	if (atom->state != THREAD_SLEEPING) {
-		printf("boo2\n");
-		return;
-	}
+	if (atom->state != THREAD_SLEEPING)
+		nr_state_machine_bugs++;
 
 	nr_timestamps++;
 	if (atom->sched_out_time > timestamp) {
-		unordered_timestamps++;
+		nr_unordered_timestamps++;
 		return;
 	}
 
@@ -1214,7 +1216,7 @@ static void output_lat_thread(struct work_atoms *work_list)
 
 	avg = work_list->total_lat / work_list->nb_atoms;
 
-	printf("|%9.3f ms |%9llu | avg:%9.3f ms | max:%9.3f ms |\n",
+	printf("|%11.3f ms |%9llu | avg:%9.3f ms | max:%9.3f ms |\n",
 	      (double)work_list->total_runtime / 1e6,
 		 work_list->nb_atoms, (double)avg / 1e6,
 		 (double)work_list->max_lat / 1e6);
@@ -1359,9 +1361,9 @@ static void __cmd_lat(void)
 	read_events();
 	sort_lat();
 
-	printf("\n ---------------------------------------------------------------------------------------\n");
-	printf("  Task                  |  Runtime ms | Switches | Average delay ms | Maximum delay ms |\n");
-	printf(" ---------------------------------------------------------------------------------------\n");
+	printf("\n -----------------------------------------------------------------------------------------\n");
+	printf("  Task                  |   Runtime ms  | Switches | Average delay ms | Maximum delay ms |\n");
+	printf(" -----------------------------------------------------------------------------------------\n");
 
 	next = rb_first(&sorted_atom_root);
 
@@ -1373,18 +1375,32 @@ static void __cmd_lat(void)
 		next = rb_next(next);
 	}
 
-	printf(" ---------------------------------------------------------------------------------------\n");
-	printf("  TOTAL:                |%9.3f ms |%9Ld |",
+	printf(" -----------------------------------------------------------------------------------------\n");
+	printf("  TOTAL:                |%11.3f ms |%9Ld |\n",
 		(double)all_runtime/1e6, all_count);
 
-	if (unordered_timestamps && nr_timestamps) {
-		printf(" INFO: %.2f%% unordered events.\n",
-			(double)unordered_timestamps/(double)nr_timestamps*100.0);
+	printf(" ---------------------------------------------------\n");
+	if (nr_unordered_timestamps && nr_timestamps) {
+		printf("  INFO: %.3f%% unordered timestamps (%ld out of %ld)\n",
+			(double)nr_unordered_timestamps/(double)nr_timestamps*100.0,
+			nr_unordered_timestamps, nr_timestamps);
 	} else {
+	}
+	if (nr_lost_events && nr_events) {
+		printf("  INFO: %.3f%% lost events (%ld out of %ld, in %ld chunks)\n",
+			(double)nr_lost_events/(double)nr_events*100.0,
+			nr_lost_events, nr_events, nr_lost_chunks);
+	}
+	if (nr_state_machine_bugs && nr_timestamps) {
+		printf("  INFO: %.3f%% state machine bugs (%ld out of %ld)",
+			(double)nr_state_machine_bugs/(double)nr_timestamps*100.0,
+			nr_state_machine_bugs, nr_timestamps);
+		if (nr_lost_events)
+			printf(" (due to lost events?)");
 		printf("\n");
 	}
+	printf("\n");
 
-	printf(" -------------------------------------------------\n\n");
 }
 
 static struct trace_sched_handler *trace_handler;
@@ -1585,8 +1601,13 @@ process_event(event_t *event, unsigned long offset, unsigned long head)
 {
 	trace_event(event);
 
+	nr_events++;
 	switch (event->header.type) {
-	case PERF_EVENT_MMAP ... PERF_EVENT_LOST:
+	case PERF_EVENT_MMAP:
+		return 0;
+	case PERF_EVENT_LOST:
+		nr_lost_chunks++;
+		nr_lost_events += event->lost.lost;
 		return 0;
 
 	case PERF_EVENT_COMM:
@@ -1768,6 +1789,7 @@ static const char *record_args[] = {
 	"-R",
 	"-M",
 	"-f",
+	"-m", "1024",
 	"-c", "1",
 	"-e", "sched:sched_switch:r",
 	"-e", "sched:sched_stat_wait:r",

commit 39aeb52f99f2380c1f16036deed2f7bb8b2e0559
Author: mingo <mingo@europe.(none)>
Date:   Mon Sep 14 20:04:48 2009 +0200

    perf sched: Add support for sched:sched_stat_runtime events
    
    This allows more precise 'perf sched latency' output:
    
     ---------------------------------------------------------------------------------------
      Task                  |  Runtime ms | Switches | Average delay ms | Maximum delay ms |
     ---------------------------------------------------------------------------------------
      ksoftirqd/0-4         |    0.010 ms |        2 | avg:    2.476 ms | max:    2.977 ms |
      perf-12328            |   15.844 ms |       66 | avg:    1.118 ms | max:    9.979 ms |
      bdi-default-235       |    0.009 ms |        1 | avg:    0.998 ms | max:    0.998 ms |
      events/1-8            |    0.020 ms |        2 | avg:    0.998 ms | max:    0.998 ms |
      events/0-7            |    0.018 ms |        2 | avg:    0.992 ms | max:    0.996 ms |
      sleep-12329           |    0.742 ms |        3 | avg:    0.906 ms | max:    2.289 ms |
      sshd-12122            |    0.163 ms |        2 | avg:    0.283 ms | max:    0.562 ms |
      loop-getpid-lon-12322 | 1023.636 ms |       69 | avg:    0.208 ms | max:    5.996 ms |
      loop-getpid-lon-12321 | 1038.638 ms |        5 | avg:    0.073 ms | max:    0.171 ms |
      migration/1-5         |    0.000 ms |        1 | avg:    0.006 ms | max:    0.006 ms |
     ---------------------------------------------------------------------------------------
      TOTAL:                | 2079.078 ms |      153 |
     -------------------------------------------------
    
    Also, streamline the code a bit more, add asserts for various state
    machine failures (they should be debugged if they occur) and fix
    a few odd ends.
    
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 93ef7b215aba..adcb563ec4d2 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -50,7 +50,7 @@ static u64			sleep_measurement_overhead;
 
 static unsigned long		nr_tasks;
 
-struct sched_event;
+struct sched_atom;
 
 struct task_desc {
 	unsigned long		nr;
@@ -59,7 +59,7 @@ struct task_desc {
 
 	unsigned long		nr_events;
 	unsigned long		curr_event;
-	struct sched_event	**events;
+	struct sched_atom	**atoms;
 
 	pthread_t		thread;
 	sem_t			sleep_sem;
@@ -76,7 +76,7 @@ enum sched_event_type {
 	SCHED_EVENT_WAKEUP,
 };
 
-struct sched_event {
+struct sched_atom {
 	enum sched_event_type	type;
 	u64			timestamp;
 	u64			duration;
@@ -137,8 +137,8 @@ struct work_atom {
 	u64			runtime;
 };
 
-struct task_atoms {
-	struct list_head	atom_list;
+struct work_atoms {
+	struct list_head	work_list;
 	struct thread		*thread;
 	struct rb_node		node;
 	u64			max_lat;
@@ -147,7 +147,7 @@ struct task_atoms {
 	u64			total_runtime;
 };
 
-typedef int (*sort_fn_t)(struct task_atoms *, struct task_atoms *);
+typedef int (*sort_fn_t)(struct work_atoms *, struct work_atoms *);
 
 static struct rb_root		atom_root, sorted_atom_root;
 
@@ -220,10 +220,10 @@ static void calibrate_sleep_measurement_overhead(void)
 	printf("sleep measurement overhead: %Ld nsecs\n", min_delta);
 }
 
-static struct sched_event *
+static struct sched_atom *
 get_new_event(struct task_desc *task, u64 timestamp)
 {
-	struct sched_event *event = calloc(1, sizeof(*event));
+	struct sched_atom *event = calloc(1, sizeof(*event));
 	unsigned long idx = task->nr_events;
 	size_t size;
 
@@ -231,27 +231,27 @@ get_new_event(struct task_desc *task, u64 timestamp)
 	event->nr = idx;
 
 	task->nr_events++;
-	size = sizeof(struct sched_event *) * task->nr_events;
-	task->events = realloc(task->events, size);
-	BUG_ON(!task->events);
+	size = sizeof(struct sched_atom *) * task->nr_events;
+	task->atoms = realloc(task->atoms, size);
+	BUG_ON(!task->atoms);
 
-	task->events[idx] = event;
+	task->atoms[idx] = event;
 
 	return event;
 }
 
-static struct sched_event *last_event(struct task_desc *task)
+static struct sched_atom *last_event(struct task_desc *task)
 {
 	if (!task->nr_events)
 		return NULL;
 
-	return task->events[task->nr_events - 1];
+	return task->atoms[task->nr_events - 1];
 }
 
 static void
 add_sched_event_run(struct task_desc *task, u64 timestamp, u64 duration)
 {
-	struct sched_event *event, *curr_event = last_event(task);
+	struct sched_atom *event, *curr_event = last_event(task);
 
 	/*
 	 * optimize an existing RUN event by merging this one
@@ -275,7 +275,7 @@ static void
 add_sched_event_wakeup(struct task_desc *task, u64 timestamp,
 		       struct task_desc *wakee)
 {
-	struct sched_event *event, *wakee_event;
+	struct sched_atom *event, *wakee_event;
 
 	event = get_new_event(task, timestamp);
 	event->type = SCHED_EVENT_WAKEUP;
@@ -303,7 +303,7 @@ static void
 add_sched_event_sleep(struct task_desc *task, u64 timestamp,
 		      u64 task_state __used)
 {
-	struct sched_event *event = get_new_event(task, timestamp);
+	struct sched_atom *event = get_new_event(task, timestamp);
 
 	event->type = SCHED_EVENT_SLEEP;
 
@@ -372,27 +372,27 @@ static void add_cross_task_wakeups(void)
 }
 
 static void
-process_sched_event(struct task_desc *this_task __used, struct sched_event *event)
+process_sched_event(struct task_desc *this_task __used, struct sched_atom *atom)
 {
 	int ret = 0;
 	u64 now;
 	long long delta;
 
 	now = get_nsecs();
-	delta = start_time + event->timestamp - now;
+	delta = start_time + atom->timestamp - now;
 
-	switch (event->type) {
+	switch (atom->type) {
 		case SCHED_EVENT_RUN:
-			burn_nsecs(event->duration);
+			burn_nsecs(atom->duration);
 			break;
 		case SCHED_EVENT_SLEEP:
-			if (event->wait_sem)
-				ret = sem_wait(event->wait_sem);
+			if (atom->wait_sem)
+				ret = sem_wait(atom->wait_sem);
 			BUG_ON(ret);
 			break;
 		case SCHED_EVENT_WAKEUP:
-			if (event->wait_sem)
-				ret = sem_post(event->wait_sem);
+			if (atom->wait_sem)
+				ret = sem_post(atom->wait_sem);
 			BUG_ON(ret);
 			break;
 		default:
@@ -467,7 +467,7 @@ static void *thread_func(void *ctx)
 
 	for (i = 0; i < this_task->nr_events; i++) {
 		this_task->curr_event = i;
-		process_sched_event(this_task, this_task->events[i]);
+		process_sched_event(this_task, this_task->atoms[i]);
 	}
 
 	cpu_usage_1 = get_cpu_usage_nsec_self();
@@ -649,7 +649,7 @@ static void __cmd_replay(void)
 	if (multitarget_wakeups)
 		printf("multi-target wakeups: %ld\n", multitarget_wakeups);
 	if (nr_run_events_optimized)
-		printf("run events optimized: %ld\n",
+		printf("run atoms optimized: %ld\n",
 			nr_run_events_optimized);
 
 	print_task_traces();
@@ -727,6 +727,20 @@ struct trace_switch_event {
 	u32 next_prio;
 };
 
+struct trace_runtime_event {
+	u32 size;
+
+	u16 common_type;
+	u8 common_flags;
+	u8 common_preempt_count;
+	u32 common_pid;
+	u32 common_tgid;
+
+	char comm[16];
+	u32 pid;
+	u64 runtime;
+	u64 vruntime;
+};
 
 struct trace_wakeup_event {
 	u32 size;
@@ -767,6 +781,12 @@ struct trace_sched_handler {
 			     u64 timestamp,
 			     struct thread *thread);
 
+	void (*runtime_event)(struct trace_runtime_event *,
+			      struct event *,
+			      int cpu,
+			      u64 timestamp,
+			      struct thread *thread);
+
 	void (*wakeup_event)(struct trace_wakeup_event *,
 			     struct event *,
 			     int cpu,
@@ -881,7 +901,7 @@ struct sort_dimension {
 static LIST_HEAD(cmp_pid);
 
 static int
-thread_lat_cmp(struct list_head *list, struct task_atoms *l, struct task_atoms *r)
+thread_lat_cmp(struct list_head *list, struct work_atoms *l, struct work_atoms *r)
 {
 	struct sort_dimension *sort;
 	int ret = 0;
@@ -897,18 +917,18 @@ thread_lat_cmp(struct list_head *list, struct task_atoms *l, struct task_atoms *
 	return ret;
 }
 
-static struct task_atoms *
+static struct work_atoms *
 thread_atoms_search(struct rb_root *root, struct thread *thread,
 			 struct list_head *sort_list)
 {
 	struct rb_node *node = root->rb_node;
-	struct task_atoms key = { .thread = thread };
+	struct work_atoms key = { .thread = thread };
 
 	while (node) {
-		struct task_atoms *atoms;
+		struct work_atoms *atoms;
 		int cmp;
 
-		atoms = container_of(node, struct task_atoms, node);
+		atoms = container_of(node, struct work_atoms, node);
 
 		cmp = thread_lat_cmp(sort_list, &key, atoms);
 		if (cmp > 0)
@@ -924,16 +944,16 @@ thread_atoms_search(struct rb_root *root, struct thread *thread,
 }
 
 static void
-__thread_latency_insert(struct rb_root *root, struct task_atoms *data,
+__thread_latency_insert(struct rb_root *root, struct work_atoms *data,
 			 struct list_head *sort_list)
 {
 	struct rb_node **new = &(root->rb_node), *parent = NULL;
 
 	while (*new) {
-		struct task_atoms *this;
+		struct work_atoms *this;
 		int cmp;
 
-		this = container_of(*new, struct task_atoms, node);
+		this = container_of(*new, struct work_atoms, node);
 		parent = *new;
 
 		cmp = thread_lat_cmp(sort_list, data, this);
@@ -950,14 +970,14 @@ __thread_latency_insert(struct rb_root *root, struct task_atoms *data,
 
 static void thread_atoms_insert(struct thread *thread)
 {
-	struct task_atoms *atoms;
+	struct work_atoms *atoms;
 
 	atoms = calloc(sizeof(*atoms), 1);
 	if (!atoms)
 		die("No memory");
 
 	atoms->thread = thread;
-	INIT_LIST_HEAD(&atoms->atom_list);
+	INIT_LIST_HEAD(&atoms->work_list);
 	__thread_latency_insert(&atom_root, atoms, &cmp_pid);
 }
 
@@ -980,10 +1000,9 @@ static char sched_out_state(struct trace_switch_event *switch_event)
 }
 
 static void
-lat_sched_out(struct task_atoms *atoms,
-	      struct trace_switch_event *switch_event __used,
-	      u64 delta,
-	      u64 timestamp)
+add_sched_out_event(struct work_atoms *atoms,
+		    char run_state,
+		    u64 timestamp)
 {
 	struct work_atom *atom;
 
@@ -993,25 +1012,37 @@ lat_sched_out(struct task_atoms *atoms,
 
 	atom->sched_out_time = timestamp;
 
-	if (sched_out_state(switch_event) == 'R') {
+	if (run_state == 'R') {
 		atom->state = THREAD_WAIT_CPU;
 		atom->wake_up_time = atom->sched_out_time;
 	}
 
-	atom->runtime = delta;
-	list_add_tail(&atom->list, &atoms->atom_list);
+	list_add_tail(&atom->list, &atoms->work_list);
+}
+
+static void
+add_runtime_event(struct work_atoms *atoms, u64 delta, u64 timestamp __used)
+{
+	struct work_atom *atom;
+
+	BUG_ON(list_empty(&atoms->work_list));
+
+	atom = list_entry(atoms->work_list.prev, struct work_atom, list);
+
+	atom->runtime += delta;
+	atoms->total_runtime += delta;
 }
 
 static void
-lat_sched_in(struct task_atoms *atoms, u64 timestamp)
+add_sched_in_event(struct work_atoms *atoms, u64 timestamp)
 {
 	struct work_atom *atom;
 	u64 delta;
 
-	if (list_empty(&atoms->atom_list))
+	if (list_empty(&atoms->work_list))
 		return;
 
-	atom = list_entry(atoms->atom_list.prev, struct work_atom, list);
+	atom = list_entry(atoms->work_list.prev, struct work_atom, list);
 
 	if (atom->state != THREAD_WAIT_CPU)
 		return;
@@ -1029,7 +1060,6 @@ lat_sched_in(struct task_atoms *atoms, u64 timestamp)
 	if (delta > atoms->max_lat)
 		atoms->max_lat = delta;
 	atoms->nb_atoms++;
-	atoms->total_runtime += atom->runtime;
 }
 
 static void
@@ -1039,13 +1069,12 @@ latency_switch_event(struct trace_switch_event *switch_event,
 		     u64 timestamp,
 		     struct thread *thread __used)
 {
-	struct task_atoms *out_atoms, *in_atoms;
+	struct work_atoms *out_events, *in_events;
 	struct thread *sched_out, *sched_in;
 	u64 timestamp0;
 	s64 delta;
 
-	if (cpu >= MAX_CPUS || cpu < 0)
-		return;
+	BUG_ON(cpu >= MAX_CPUS || cpu < 0);
 
 	timestamp0 = cpu_last_switched[cpu];
 	cpu_last_switched[cpu] = timestamp;
@@ -1061,34 +1090,63 @@ latency_switch_event(struct trace_switch_event *switch_event,
 	sched_out = threads__findnew(switch_event->prev_pid, &threads, &last_match);
 	sched_in = threads__findnew(switch_event->next_pid, &threads, &last_match);
 
-	in_atoms = thread_atoms_search(&atom_root, sched_in, &cmp_pid);
-	if (!in_atoms) {
+	out_events = thread_atoms_search(&atom_root, sched_out, &cmp_pid);
+	if (!out_events) {
+		thread_atoms_insert(sched_out);
+		out_events = thread_atoms_search(&atom_root, sched_out, &cmp_pid);
+		if (!out_events)
+			die("out-event: Internal tree error");
+	}
+	add_sched_out_event(out_events, sched_out_state(switch_event), timestamp);
+
+	in_events = thread_atoms_search(&atom_root, sched_in, &cmp_pid);
+	if (!in_events) {
 		thread_atoms_insert(sched_in);
-		in_atoms = thread_atoms_search(&atom_root, sched_in, &cmp_pid);
-		if (!in_atoms)
-			die("in-atom: Internal tree error");
+		in_events = thread_atoms_search(&atom_root, sched_in, &cmp_pid);
+		if (!in_events)
+			die("in-event: Internal tree error");
+		/*
+		 * Take came in we have not heard about yet,
+		 * add in an initial atom in runnable state:
+		 */
+		add_sched_out_event(in_events, 'R', timestamp);
 	}
+	add_sched_in_event(in_events, timestamp);
+}
 
-	out_atoms = thread_atoms_search(&atom_root, sched_out, &cmp_pid);
-	if (!out_atoms) {
-		thread_atoms_insert(sched_out);
-		out_atoms = thread_atoms_search(&atom_root, sched_out, &cmp_pid);
-		if (!out_atoms)
-			die("out-atom: Internal tree error");
+static void
+latency_runtime_event(struct trace_runtime_event *runtime_event,
+		     struct event *event __used,
+		     int cpu,
+		     u64 timestamp,
+		     struct thread *this_thread __used)
+{
+	struct work_atoms *atoms;
+	struct thread *thread;
+
+	BUG_ON(cpu >= MAX_CPUS || cpu < 0);
+
+	thread = threads__findnew(runtime_event->pid, &threads, &last_match);
+	atoms = thread_atoms_search(&atom_root, thread, &cmp_pid);
+	if (!atoms) {
+		thread_atoms_insert(thread);
+		atoms = thread_atoms_search(&atom_root, thread, &cmp_pid);
+		if (!atoms)
+			die("in-event: Internal tree error");
+		add_sched_out_event(atoms, 'R', timestamp);
 	}
 
-	lat_sched_in(in_atoms, timestamp);
-	lat_sched_out(out_atoms, switch_event, delta, timestamp);
+	add_runtime_event(atoms, runtime_event->runtime, timestamp);
 }
 
 static void
 latency_wakeup_event(struct trace_wakeup_event *wakeup_event,
-		     struct event *event __used,
+		     struct event *__event __used,
 		     int cpu __used,
 		     u64 timestamp,
 		     struct thread *thread __used)
 {
-	struct task_atoms *atoms;
+	struct work_atoms *atoms;
 	struct work_atom *atom;
 	struct thread *wakee;
 
@@ -1100,16 +1158,20 @@ latency_wakeup_event(struct trace_wakeup_event *wakeup_event,
 	atoms = thread_atoms_search(&atom_root, wakee, &cmp_pid);
 	if (!atoms) {
 		thread_atoms_insert(wakee);
-		return;
+		atoms = thread_atoms_search(&atom_root, wakee, &cmp_pid);
+		if (!atoms)
+			die("wakeup-event: Internal tree error");
+		add_sched_out_event(atoms, 'S', timestamp);
 	}
 
-	if (list_empty(&atoms->atom_list))
-		return;
+	BUG_ON(list_empty(&atoms->work_list));
 
-	atom = list_entry(atoms->atom_list.prev, struct work_atom, list);
+	atom = list_entry(atoms->work_list.prev, struct work_atom, list);
 
-	if (atom->state != THREAD_SLEEPING)
+	if (atom->state != THREAD_SLEEPING) {
+		printf("boo2\n");
 		return;
+	}
 
 	nr_timestamps++;
 	if (atom->sched_out_time > timestamp) {
@@ -1124,40 +1186,41 @@ latency_wakeup_event(struct trace_wakeup_event *wakeup_event,
 static struct trace_sched_handler lat_ops  = {
 	.wakeup_event		= latency_wakeup_event,
 	.switch_event		= latency_switch_event,
+	.runtime_event		= latency_runtime_event,
 	.fork_event		= latency_fork_event,
 };
 
-static void output_lat_thread(struct task_atoms *atom_list)
+static void output_lat_thread(struct work_atoms *work_list)
 {
 	int i;
 	int ret;
 	u64 avg;
 
-	if (!atom_list->nb_atoms)
+	if (!work_list->nb_atoms)
 		return;
 	/*
 	 * Ignore idle threads:
 	 */
-	if (!atom_list->thread->pid)
+	if (!work_list->thread->pid)
 		return;
 
-	all_runtime += atom_list->total_runtime;
-	all_count += atom_list->nb_atoms;
+	all_runtime += work_list->total_runtime;
+	all_count += work_list->nb_atoms;
 
-	ret = printf("  %s-%d ", atom_list->thread->comm, atom_list->thread->pid);
+	ret = printf("  %s-%d ", work_list->thread->comm, work_list->thread->pid);
 
 	for (i = 0; i < 24 - ret; i++)
 		printf(" ");
 
-	avg = atom_list->total_lat / atom_list->nb_atoms;
+	avg = work_list->total_lat / work_list->nb_atoms;
 
 	printf("|%9.3f ms |%9llu | avg:%9.3f ms | max:%9.3f ms |\n",
-	      (double)atom_list->total_runtime / 1e6,
-		 atom_list->nb_atoms, (double)avg / 1e6,
-		 (double)atom_list->max_lat / 1e6);
+	      (double)work_list->total_runtime / 1e6,
+		 work_list->nb_atoms, (double)avg / 1e6,
+		 (double)work_list->max_lat / 1e6);
 }
 
-static int pid_cmp(struct task_atoms *l, struct task_atoms *r)
+static int pid_cmp(struct work_atoms *l, struct work_atoms *r)
 {
 	if (l->thread->pid < r->thread->pid)
 		return -1;
@@ -1172,7 +1235,7 @@ static struct sort_dimension pid_sort_dimension = {
 	.cmp			= pid_cmp,
 };
 
-static int avg_cmp(struct task_atoms *l, struct task_atoms *r)
+static int avg_cmp(struct work_atoms *l, struct work_atoms *r)
 {
 	u64 avgl, avgr;
 
@@ -1198,7 +1261,7 @@ static struct sort_dimension avg_sort_dimension = {
 	.cmp			= avg_cmp,
 };
 
-static int max_cmp(struct task_atoms *l, struct task_atoms *r)
+static int max_cmp(struct work_atoms *l, struct work_atoms *r)
 {
 	if (l->max_lat < r->max_lat)
 		return -1;
@@ -1213,7 +1276,7 @@ static struct sort_dimension max_sort_dimension = {
 	.cmp			= max_cmp,
 };
 
-static int switch_cmp(struct task_atoms *l, struct task_atoms *r)
+static int switch_cmp(struct work_atoms *l, struct work_atoms *r)
 {
 	if (l->nb_atoms < r->nb_atoms)
 		return -1;
@@ -1228,7 +1291,7 @@ static struct sort_dimension switch_sort_dimension = {
 	.cmp			= switch_cmp,
 };
 
-static int runtime_cmp(struct task_atoms *l, struct task_atoms *r)
+static int runtime_cmp(struct work_atoms *l, struct work_atoms *r)
 {
 	if (l->total_runtime < r->total_runtime)
 		return -1;
@@ -1277,13 +1340,13 @@ static void sort_lat(void)
 	struct rb_node *node;
 
 	for (;;) {
-		struct task_atoms *data;
+		struct work_atoms *data;
 		node = rb_first(&atom_root);
 		if (!node)
 			break;
 
 		rb_erase(node, &atom_root);
-		data = rb_entry(node, struct task_atoms, node);
+		data = rb_entry(node, struct work_atoms, node);
 		__thread_latency_insert(&sorted_atom_root, data, &sort_list);
 	}
 }
@@ -1303,10 +1366,10 @@ static void __cmd_lat(void)
 	next = rb_first(&sorted_atom_root);
 
 	while (next) {
-		struct task_atoms *atom_list;
+		struct work_atoms *work_list;
 
-		atom_list = rb_entry(next, struct task_atoms, node);
-		output_lat_thread(atom_list);
+		work_list = rb_entry(next, struct work_atoms, node);
+		output_lat_thread(work_list);
 		next = rb_next(next);
 	}
 
@@ -1368,6 +1431,23 @@ process_sched_switch_event(struct raw_event_sample *raw,
 	trace_handler->switch_event(&switch_event, event, cpu, timestamp, thread);
 }
 
+static void
+process_sched_runtime_event(struct raw_event_sample *raw,
+			   struct event *event,
+			   int cpu __used,
+			   u64 timestamp __used,
+			   struct thread *thread __used)
+{
+	struct trace_runtime_event runtime_event;
+
+	FILL_ARRAY(runtime_event, comm, event, raw->data);
+	FILL_FIELD(runtime_event, pid, event, raw->data);
+	FILL_FIELD(runtime_event, runtime, event, raw->data);
+	FILL_FIELD(runtime_event, vruntime, event, raw->data);
+
+	trace_handler->runtime_event(&runtime_event, event, cpu, timestamp, thread);
+}
+
 static void
 process_sched_fork_event(struct raw_event_sample *raw,
 			 struct event *event,
@@ -1410,6 +1490,8 @@ process_raw_event(event_t *raw_event __used, void *more_data,
 
 	if (!strcmp(event->name, "sched_switch"))
 		process_sched_switch_event(raw, event, cpu, timestamp, thread);
+	if (!strcmp(event->name, "sched_stat_runtime"))
+		process_sched_runtime_event(raw, event, cpu, timestamp, thread);
 	if (!strcmp(event->name, "sched_wakeup"))
 		process_sched_wakeup_event(raw, event, cpu, timestamp, thread);
 	if (!strcmp(event->name, "sched_wakeup_new"))

commit 08f69e6c2e59b3d73343f8c9ecf758e0133dbc22
Author: mingo <mingo@europe.(none)>
Date:   Mon Sep 14 18:30:44 2009 +0200

    perf sched: Print PIDs too
    
    Often it's useful to know the PID of the task as well - print it
    out too.
    
    ( While at it, reformat the output to be a bit more
      paste-into-commit-logs friendly. )
    
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index f856a02cd4fc..93ef7b215aba 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1144,9 +1144,9 @@ static void output_lat_thread(struct task_atoms *atom_list)
 	all_runtime += atom_list->total_runtime;
 	all_count += atom_list->nb_atoms;
 
-	ret = printf(" %s ", atom_list->thread->comm);
+	ret = printf("  %s-%d ", atom_list->thread->comm, atom_list->thread->pid);
 
-	for (i = 0; i < 19 - ret; i++)
+	for (i = 0; i < 24 - ret; i++)
 		printf(" ");
 
 	avg = atom_list->total_lat / atom_list->nb_atoms;
@@ -1296,9 +1296,9 @@ static void __cmd_lat(void)
 	read_events();
 	sort_lat();
 
-	printf("-----------------------------------------------------------------------------------\n");
-	printf(" Task              |  Runtime ms | Switches | Average delay ms | Maximum delay ms |\n");
-	printf("-----------------------------------------------------------------------------------\n");
+	printf("\n ---------------------------------------------------------------------------------------\n");
+	printf("  Task                  |  Runtime ms | Switches | Average delay ms | Maximum delay ms |\n");
+	printf(" ---------------------------------------------------------------------------------------\n");
 
 	next = rb_first(&sorted_atom_root);
 
@@ -1310,8 +1310,8 @@ static void __cmd_lat(void)
 		next = rb_next(next);
 	}
 
-	printf("-----------------------------------------------------------------------------------\n");
-	printf(" TOTAL:            |%9.3f ms |%9Ld |",
+	printf(" ---------------------------------------------------------------------------------------\n");
+	printf("  TOTAL:                |%9.3f ms |%9Ld |",
 		(double)all_runtime/1e6, all_count);
 
 	if (unordered_timestamps && nr_timestamps) {
@@ -1321,7 +1321,7 @@ static void __cmd_lat(void)
 		printf("\n");
 	}
 
-	printf("---------------------------------------------\n");
+	printf(" -------------------------------------------------\n\n");
 }
 
 static struct trace_sched_handler *trace_handler;

commit d11533893b31ab7806ff04bfa69ae646068610ce
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Sep 14 18:22:53 2009 +0200

    perf sched: Fix 'perf sched latency' output on 32-bit systems
    
    Before:
    
      -----------------------------------------------------------------------------------
       Task              |  Runtime ms | Switches | Average delay ms | Maximum delay ms |
      -----------------------------------------------------------------------------------
       perf              |4853313.251 ms |       10 | avg:    0.046 ms | max:    0.337 ms |
       flush-8:0         |2426659.202 ms |        5 | avg:    0.015 ms | max:    0.016 ms |
       sleep             |485331.966 ms |        1 | avg:    0.012 ms | max:    0.012 ms |
       ksoftirqd/1       |485331.320 ms |        1 | avg:    0.005 ms | max:    0.005 ms |
      -----------------------------------------------------------------------------------
       TOTAL:            |8250635.739 ms |       17 |
      ---------------------------------------------
    
    After:
    
      -----------------------------------------------------------------------------------
       Task              |  Runtime ms | Switches | Average delay ms | Maximum delay ms |
      -----------------------------------------------------------------------------------
       perf              |    0.206 ms |       10 | avg:    0.046 ms | max:    0.337 ms |
       flush-8:0         |    2.680 ms |        5 | avg:    0.015 ms | max:    0.016 ms |
       sleep             |    0.662 ms |        1 | avg:    0.012 ms | max:    0.012 ms |
       ksoftirqd/1       |    0.015 ms |        1 | avg:    0.005 ms | max:    0.005 ms |
      -----------------------------------------------------------------------------------
       TOTAL:            |    3.563 ms |       17 |
      ---------------------------------------------
    
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 2ce87ef5a3e6..f856a02cd4fc 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -805,7 +805,7 @@ replay_wakeup_event(struct trace_wakeup_event *wakeup_event,
 	add_sched_event_wakeup(waker, timestamp, wakee);
 }
 
-static unsigned long cpu_last_switched[MAX_CPUS];
+static u64 cpu_last_switched[MAX_CPUS];
 
 static void
 replay_switch_event(struct trace_switch_event *switch_event,

commit ea57c4f5203d82c7844c54cdef54e972cf4e9d1f
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Sep 13 18:15:54 2009 +0200

    perf tools: Implement counter output multiplexing
    
    Finish the -M/--multiplex option implementation:
    
     - separate it out from group_fd
    
     - correctly set it via the ioctl and dont mmap counters that
       are multiplexed
    
     - modify the perf record event loop to deal with buffer-less
       counters.
    
     - remove the -g option from perf sched record
    
     - account for unordered events in perf sched latency
    
     - (add -f to perf sched record to ease measurements)
    
     - skip idle threads (pid==0) in latency output
    
    The result is better latency output by 'perf sched latency':
    
     -----------------------------------------------------------------------------------
      Task              |  Runtime ms | Switches | Average delay ms | Maximum delay ms |
     -----------------------------------------------------------------------------------
      ksoftirqd/8       |    0.071 ms |        2 | avg:    0.458 ms | max:    0.913 ms |
      at-spi-registry   |    0.609 ms |       19 | avg:    0.013 ms | max:    0.023 ms |
      perf              |    3.316 ms |       16 | avg:    0.013 ms | max:    0.054 ms |
      Xorg              |    0.392 ms |       19 | avg:    0.011 ms | max:    0.018 ms |
      sleep             |    0.537 ms |        2 | avg:    0.009 ms | max:    0.009 ms |
     -----------------------------------------------------------------------------------
      TOTAL:            |    4.925 ms |       58 |
     ---------------------------------------------
    
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 3e003237c42f..2ce87ef5a3e6 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -116,6 +116,8 @@ static u64			sum_fluct;
 static u64			run_avg;
 
 static unsigned long		replay_repeat = 10;
+static unsigned long		nr_timestamps;
+static unsigned long		unordered_timestamps;
 
 #define TASK_STATE_TO_CHAR_STR "RSDTtZX"
 
@@ -1109,8 +1111,11 @@ latency_wakeup_event(struct trace_wakeup_event *wakeup_event,
 	if (atom->state != THREAD_SLEEPING)
 		return;
 
-	if (atom->sched_out_time > timestamp)
+	nr_timestamps++;
+	if (atom->sched_out_time > timestamp) {
+		unordered_timestamps++;
 		return;
+	}
 
 	atom->state = THREAD_WAIT_CPU;
 	atom->wake_up_time = timestamp;
@@ -1130,6 +1135,11 @@ static void output_lat_thread(struct task_atoms *atom_list)
 
 	if (!atom_list->nb_atoms)
 		return;
+	/*
+	 * Ignore idle threads:
+	 */
+	if (!atom_list->thread->pid)
+		return;
 
 	all_runtime += atom_list->total_runtime;
 	all_count += atom_list->nb_atoms;
@@ -1301,8 +1311,16 @@ static void __cmd_lat(void)
 	}
 
 	printf("-----------------------------------------------------------------------------------\n");
-	printf(" TOTAL:            |%9.3f ms |%9Ld |\n",
+	printf(" TOTAL:            |%9.3f ms |%9Ld |",
 		(double)all_runtime/1e6, all_count);
+
+	if (unordered_timestamps && nr_timestamps) {
+		printf(" INFO: %.2f%% unordered events.\n",
+			(double)unordered_timestamps/(double)nr_timestamps*100.0);
+	} else {
+		printf("\n");
+	}
+
 	printf("---------------------------------------------\n");
 }
 
@@ -1667,12 +1685,13 @@ static const char *record_args[] = {
 	"-a",
 	"-R",
 	"-M",
-	"-g",
+	"-f",
 	"-c", "1",
 	"-e", "sched:sched_switch:r",
 	"-e", "sched:sched_stat_wait:r",
 	"-e", "sched:sched_stat_sleep:r",
 	"-e", "sched:sched_stat_iowait:r",
+	"-e", "sched:sched_stat_runtime:r",
 	"-e", "sched:sched_process_exit:r",
 	"-e", "sched:sched_process_fork:r",
 	"-e", "sched:sched_wakeup:r",

commit aa1ab9d26ae9fe2566a9036e3cb83e7d555b3987
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Mon Sep 14 03:01:12 2009 +0200

    perf tools: Fix processing of randomly serialized sched traces
    
    Currently it's possible to meet such too high latency results
    with 'perf sched latency'.
    
     -----------------------------------------------------------------------------------
     Task              |  Runtime ms | Switches | Average delay ms | Maximum delay ms |
     -----------------------------------------------------------------------------------
     xfce4-panel       |    0.222 ms |        2 | avg: 4718.345 ms | max: 9436.493 ms |
     scsi_eh_3         |    3.962 ms |       36 | avg:   55.957 ms | max: 1977.829 ms |
    
    The origin is on traces that are sometimes badly serialized across cpus.
    For example the raw traces that raised such results for xfce4-panel:
    
    (1)          [init]-0     [000]  1494.663899990: sched_switch: task swapper:0 [140] (R) ==> xfce4-panel:4569 [120]
    (2)     xfce4-panel-4569  [000]  1494.663928373: sched_switch: task xfce4-panel:4569 [120] (S) ==> swapper:0 [140]
    (3)            Xorg-4276  [001]  1494.663860125: sched_wakeup: task xfce4-panel:4569 [120] success=1 [000]
    (4)            Xorg-4276  [001]  1504.098252756: sched_wakeup: task xfce4-panel:4569 [120] success=1 [000]
    (5)            perf-5219  [000]  1504.100353302: sched_switch: task perf:5219 [120] (S) ==> xfce4-panel:4569 [120]
    
    The traces are processed in the order they arrive. Then in (2),
    xfce4-panel sleeps, it is first waken up in (3) and eventually
    scheduled in (5).
    
    The latency reported is then 1504 - 1495 = 9 secs, as reported by perf
    sched. But this is wrong, we are confident in the fact the traces are
    nicely serialized while we should actually more trust the timestamps.
    
    If we reorder by timestamps we get:
    
    (1)            Xorg-4276  [001]  1494.663860125: sched_wakeup: task xfce4-panel:4569 [120] success=1 [000]
    (2)          [init]-0     [000]  1494.663899990: sched_switch: task swapper:0 [140] (R) ==> xfce4-panel:4569 [120]
    (3)     xfce4-panel-4569  [000]  1494.663928373: sched_switch: task xfce4-panel:4569 [120] (S) ==> swapper:0 [140]
    (4)            Xorg-4276  [001]  1504.098252756: sched_wakeup: task xfce4-panel:4569 [120] success=1 [000]
    (5)            perf-5219  [000]  1504.100353302: sched_switch: task perf:5219 [120] (S) ==> xfce4-panel:4569 [120]
    
    Now the trace make more sense, xfce4-panel is sleeping. Then it is
    woken up in (1), scheduled in (2)
    It goes to sleep in (3), woken up in (4) and scheduled in (5).
    
    Now, latency captured between (1) and (2) is of 39 us.
    And between (4) and (5) it is 2.1 ms.
    
    Such pattern of bad serializing is the origin of the high latencies
    reported by perf sched.
    
    Basically, we need to check whether wake up time is higher than
    schedule out time. If it's not the case, we need to tag the current
    work atom as invalid.
    
    Beside that, we may need to work later on a better ordering of the
    traces given by the kernel.
    
    After this patch:
    
    xfce4-session     |    0.221 ms |        1 | avg:    0.538 ms | max:    0.538 ms |
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 686af633b35b..3e003237c42f 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -129,6 +129,7 @@ enum thread_state {
 struct work_atom {
 	struct list_head	list;
 	enum thread_state	state;
+	u64			sched_out_time;
 	u64			wake_up_time;
 	u64			sched_in_time;
 	u64			runtime;
@@ -988,9 +989,11 @@ lat_sched_out(struct task_atoms *atoms,
 	if (!atom)
 		die("Non memory");
 
+	atom->sched_out_time = timestamp;
+
 	if (sched_out_state(switch_event) == 'R') {
 		atom->state = THREAD_WAIT_CPU;
-		atom->wake_up_time = timestamp;
+		atom->wake_up_time = atom->sched_out_time;
 	}
 
 	atom->runtime = delta;
@@ -1106,6 +1109,9 @@ latency_wakeup_event(struct trace_wakeup_event *wakeup_event,
 	if (atom->state != THREAD_SLEEPING)
 		return;
 
+	if (atom->sched_out_time > timestamp)
+		return;
+
 	atom->state = THREAD_WAIT_CPU;
 	atom->wake_up_time = timestamp;
 }

commit d13025222cdb0043e2239b3b819389358bb31bc0
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Mon Sep 14 08:57:15 2009 +0200

    perf tools: Add an option to multiplex counters in a single channel
    
    Add an option to multiplex counters output in the channel of
    the group leader, ie: the first counter opened:
    
            -M --multiplex
    
    The effect is better serialized samples. This is especially
    useful for tracepoint samples that need to be well serialized
    for their post-processing.
    
    Also make use of this option in 'perf sched'.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 8db0fd222f80..686af633b35b 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1660,6 +1660,8 @@ static const char *record_args[] = {
 	"record",
 	"-a",
 	"-R",
+	"-M",
+	"-g",
 	"-c", "1",
 	"-e", "sched:sched_switch:r",
 	"-e", "sched:sched_stat_wait:r",

commit c13f0d3c8165e9592102687fa999da0a0d9c3724
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Sep 13 16:51:04 2009 +0200

    perf sched: Add 'perf sched trace', improve documentation
    
    Alias 'perf sched trace' to 'perf trace', for workflow completeness.
    
    Add a bit of documentation for perf sched.
    
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index ede40c1429a8..8db0fd222f80 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1597,7 +1597,7 @@ static int read_events(void)
 }
 
 static const char * const sched_usage[] = {
-	"perf sched [<options>] {record|latency|replay}",
+	"perf sched [<options>] {record|latency|replay|trace}",
 	NULL
 };
 
@@ -1719,6 +1719,11 @@ int cmd_sched(int argc, const char **argv, const char *prefix __used)
 				usage_with_options(replay_usage, replay_options);
 		}
 		__cmd_replay();
+	} else if (!strcmp(argv[0], "trace")) {
+		/*
+		 * Aliased to 'perf trace' for now:
+		 */
+		return cmd_trace(argc, argv, prefix);
 	} else {
 		usage_with_options(sched_usage, sched_options);
 	}

commit 1fc35b29b4098aa3bf9fc9acb4c1615d0b5dd95d
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Sep 13 09:44:29 2009 +0200

    perf sched: Implement the 'perf sched record' subcommand
    
    Implement the 'perf sched record' subcommand that adds a
    default list of events, turns on raw sampling and system-wide
    tracing and passes off the rest of the command to perf record.
    
    This is more convenient than having to specify the events all
    the time.
    
    Before:
    
     $ perf record -a -R -e sched:sched_switch:r -e sched:sched_stat_wait:r -e sched:sched_stat_sleep:r -e sched:sched_stat_iowait:r -e sched:sched_process_exit:r -e sched:sched_process_fork:r -e sched:sched_wakeup:r -e sched:sched_migrate_task:r -c 1 sleep 1
    
    After:
    
     $ perf sched record -f sleep 1
    
    Also fix an assumption in the event string parser that assumed
    that strings passed in can be modified. (In this case they wont
    be as they come from a readonly constant section.)
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index b72544f2b964..ede40c1429a8 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1656,6 +1656,40 @@ static void setup_sorting(void)
 	sort_dimension__add((char *)"pid", &cmp_pid);
 }
 
+static const char *record_args[] = {
+	"record",
+	"-a",
+	"-R",
+	"-c", "1",
+	"-e", "sched:sched_switch:r",
+	"-e", "sched:sched_stat_wait:r",
+	"-e", "sched:sched_stat_sleep:r",
+	"-e", "sched:sched_stat_iowait:r",
+	"-e", "sched:sched_process_exit:r",
+	"-e", "sched:sched_process_fork:r",
+	"-e", "sched:sched_wakeup:r",
+	"-e", "sched:sched_migrate_task:r",
+};
+
+static int __cmd_record(int argc, const char **argv)
+{
+	unsigned int rec_argc, i, j;
+	const char **rec_argv;
+
+	rec_argc = ARRAY_SIZE(record_args) + argc - 1;
+	rec_argv = calloc(rec_argc + 1, sizeof(char *));
+
+	for (i = 0; i < ARRAY_SIZE(record_args); i++)
+		rec_argv[i] = strdup(record_args[i]);
+
+	for (j = 1; j < (unsigned int)argc; j++, i++)
+		rec_argv[i] = argv[j];
+
+	BUG_ON(i != rec_argc);
+
+	return cmd_record(i, rec_argv, NULL);
+}
+
 int cmd_sched(int argc, const char **argv, const char *prefix __used)
 {
 	symbol__init();
@@ -1666,7 +1700,9 @@ int cmd_sched(int argc, const char **argv, const char *prefix __used)
 	if (!argc)
 		usage_with_options(sched_usage, sched_options);
 
-	if (!strncmp(argv[0], "lat", 3)) {
+	if (!strncmp(argv[0], "rec", 3)) {
+		return __cmd_record(argc, argv);
+	} else if (!strncmp(argv[0], "lat", 3)) {
 		trace_handler = &lat_ops;
 		if (argc > 1) {
 			argc = parse_options(argc, argv, latency_options, latency_usage, 0);
@@ -1687,6 +1723,5 @@ int cmd_sched(int argc, const char **argv, const char *prefix __used)
 		usage_with_options(sched_usage, sched_options);
 	}
 
-
 	return 0;
 }

commit b5fae128e41021889777f8ead810cbd2a8b249fc
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Sep 11 12:12:54 2009 +0200

    perf sched: Clean up PID sorting logic
    
    Use a sort list for thread atoms insertion as well - instead of
    hardcoded for PID.
    
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index cc2dbd5b50eb..b72544f2b964 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -144,7 +144,7 @@ struct task_atoms {
 	u64			total_runtime;
 };
 
-typedef int (*sort_thread_lat)(struct task_atoms *, struct task_atoms *);
+typedef int (*sort_fn_t)(struct task_atoms *, struct task_atoms *);
 
 static struct rb_root		atom_root, sorted_atom_root;
 
@@ -869,41 +869,22 @@ static struct trace_sched_handler replay_ops  = {
 	.fork_event		= replay_fork_event,
 };
 
-static struct task_atoms *
-thread_atoms_search(struct rb_root *root, struct thread *thread)
-{
-	struct rb_node *node = root->rb_node;
-
-	while (node) {
-		struct task_atoms *atoms;
-
-		atoms = container_of(node, struct task_atoms, node);
-		if (thread->pid > atoms->thread->pid)
-			node = node->rb_left;
-		else if (thread->pid < atoms->thread->pid)
-			node = node->rb_right;
-		else {
-			return atoms;
-		}
-	}
-	return NULL;
-}
-
 struct sort_dimension {
 	const char		*name;
-	sort_thread_lat		cmp;
+	sort_fn_t		cmp;
 	struct list_head	list;
 };
 
 static LIST_HEAD(cmp_pid);
 
 static int
-thread_lat_cmp(struct list_head *list, struct task_atoms *l,
-	       struct task_atoms *r)
+thread_lat_cmp(struct list_head *list, struct task_atoms *l, struct task_atoms *r)
 {
 	struct sort_dimension *sort;
 	int ret = 0;
 
+	BUG_ON(list_empty(list));
+
 	list_for_each_entry(sort, list, list) {
 		ret = sort->cmp(l, r);
 		if (ret)
@@ -913,6 +894,32 @@ thread_lat_cmp(struct list_head *list, struct task_atoms *l,
 	return ret;
 }
 
+static struct task_atoms *
+thread_atoms_search(struct rb_root *root, struct thread *thread,
+			 struct list_head *sort_list)
+{
+	struct rb_node *node = root->rb_node;
+	struct task_atoms key = { .thread = thread };
+
+	while (node) {
+		struct task_atoms *atoms;
+		int cmp;
+
+		atoms = container_of(node, struct task_atoms, node);
+
+		cmp = thread_lat_cmp(sort_list, &key, atoms);
+		if (cmp > 0)
+			node = node->rb_left;
+		else if (cmp < 0)
+			node = node->rb_right;
+		else {
+			BUG_ON(thread != atoms->thread);
+			return atoms;
+		}
+	}
+	return NULL;
+}
+
 static void
 __thread_latency_insert(struct rb_root *root, struct task_atoms *data,
 			 struct list_head *sort_list)
@@ -1049,18 +1056,18 @@ latency_switch_event(struct trace_switch_event *switch_event,
 	sched_out = threads__findnew(switch_event->prev_pid, &threads, &last_match);
 	sched_in = threads__findnew(switch_event->next_pid, &threads, &last_match);
 
-	in_atoms = thread_atoms_search(&atom_root, sched_in);
+	in_atoms = thread_atoms_search(&atom_root, sched_in, &cmp_pid);
 	if (!in_atoms) {
 		thread_atoms_insert(sched_in);
-		in_atoms = thread_atoms_search(&atom_root, sched_in);
+		in_atoms = thread_atoms_search(&atom_root, sched_in, &cmp_pid);
 		if (!in_atoms)
 			die("in-atom: Internal tree error");
 	}
 
-	out_atoms = thread_atoms_search(&atom_root, sched_out);
+	out_atoms = thread_atoms_search(&atom_root, sched_out, &cmp_pid);
 	if (!out_atoms) {
 		thread_atoms_insert(sched_out);
-		out_atoms = thread_atoms_search(&atom_root, sched_out);
+		out_atoms = thread_atoms_search(&atom_root, sched_out, &cmp_pid);
 		if (!out_atoms)
 			die("out-atom: Internal tree error");
 	}
@@ -1085,7 +1092,7 @@ latency_wakeup_event(struct trace_wakeup_event *wakeup_event,
 		return;
 
 	wakee = threads__findnew(wakeup_event->pid, &threads, &last_match);
-	atoms = thread_atoms_search(&atom_root, wakee);
+	atoms = thread_atoms_search(&atom_root, wakee, &cmp_pid);
 	if (!atoms) {
 		thread_atoms_insert(wakee);
 		return;
@@ -1136,7 +1143,6 @@ static void output_lat_thread(struct task_atoms *atom_list)
 
 static int pid_cmp(struct task_atoms *l, struct task_atoms *r)
 {
-
 	if (l->thread->pid < r->thread->pid)
 		return -1;
 	if (l->thread->pid > r->thread->pid)
@@ -1146,8 +1152,8 @@ static int pid_cmp(struct task_atoms *l, struct task_atoms *r)
 }
 
 static struct sort_dimension pid_sort_dimension = {
-	.name = "pid",
-	.cmp = pid_cmp,
+	.name			= "pid",
+	.cmp			= pid_cmp,
 };
 
 static int avg_cmp(struct task_atoms *l, struct task_atoms *r)
@@ -1172,8 +1178,8 @@ static int avg_cmp(struct task_atoms *l, struct task_atoms *r)
 }
 
 static struct sort_dimension avg_sort_dimension = {
-	.name 	= "avg",
-	.cmp	= avg_cmp,
+	.name			= "avg",
+	.cmp			= avg_cmp,
 };
 
 static int max_cmp(struct task_atoms *l, struct task_atoms *r)
@@ -1187,8 +1193,8 @@ static int max_cmp(struct task_atoms *l, struct task_atoms *r)
 }
 
 static struct sort_dimension max_sort_dimension = {
-	.name 	= "max",
-	.cmp	= max_cmp,
+	.name			= "max",
+	.cmp			= max_cmp,
 };
 
 static int switch_cmp(struct task_atoms *l, struct task_atoms *r)
@@ -1202,8 +1208,8 @@ static int switch_cmp(struct task_atoms *l, struct task_atoms *r)
 }
 
 static struct sort_dimension switch_sort_dimension = {
-	.name 	= "switch",
-	.cmp	= switch_cmp,
+	.name			= "switch",
+	.cmp			= switch_cmp,
 };
 
 static int runtime_cmp(struct task_atoms *l, struct task_atoms *r)
@@ -1217,8 +1223,8 @@ static int runtime_cmp(struct task_atoms *l, struct task_atoms *r)
 }
 
 static struct sort_dimension runtime_sort_dimension = {
-	.name 	= "runtime",
-	.cmp	= runtime_cmp,
+	.name			= "runtime",
+	.cmp			= runtime_cmp,
 };
 
 static struct sort_dimension *available_sorts[] = {
@@ -1666,8 +1672,8 @@ int cmd_sched(int argc, const char **argv, const char *prefix __used)
 			argc = parse_options(argc, argv, latency_options, latency_usage, 0);
 			if (argc)
 				usage_with_options(latency_usage, latency_options);
-			setup_sorting();
 		}
+		setup_sorting();
 		__cmd_lat();
 	} else if (!strncmp(argv[0], "rep", 3)) {
 		trace_handler = &replay_ops;

commit b1ffe8f3e0c96f5527a89e24410d6b0e59b3554a
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Sep 11 12:12:54 2009 +0200

    perf sched: Finish latency => atom rename and misc cleanups
    
    - Rename 'latency' field/variable names to the better 'atom' ones
    
     - Reduce the number of #include lines and consolidate them
    
     - Gather file scope variables at the top of the file
    
     - Remove unused bits
    
    No change in functionality.
    
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index e01cc63b98cc..cc2dbd5b50eb 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1,4 +1,5 @@
 #include "builtin.h"
+#include "perf.h"
 
 #include "util/util.h"
 #include "util/cache.h"
@@ -7,15 +8,16 @@
 #include "util/header.h"
 
 #include "util/parse-options.h"
+#include "util/trace-event.h"
 
-#include "perf.h"
 #include "util/debug.h"
 
-#include "util/trace-event.h"
 #include <sys/types.h>
+#include <sys/prctl.h>
 
-
-#define MAX_CPUS 4096
+#include <semaphore.h>
+#include <pthread.h>
+#include <math.h>
 
 static char			const *input_name = "perf.data";
 static int			input;
@@ -33,44 +35,126 @@ static u64			sample_type;
 static char			default_sort_order[] = "avg, max, switch, runtime";
 static char			*sort_order = default_sort_order;
 
+#define PR_SET_NAME		15               /* Set process name */
+#define MAX_CPUS		4096
 
-/*
- * Scheduler benchmarks
- */
-#include <sys/resource.h>
-#include <sys/types.h>
-#include <sys/stat.h>
-#include <sys/time.h>
-#include <sys/prctl.h>
+#define BUG_ON(x)		assert(!(x))
 
-#include <linux/unistd.h>
+static u64			run_measurement_overhead;
+static u64			sleep_measurement_overhead;
 
-#include <semaphore.h>
-#include <pthread.h>
-#include <signal.h>
-#include <values.h>
-#include <string.h>
-#include <unistd.h>
-#include <stdlib.h>
-#include <assert.h>
-#include <fcntl.h>
-#include <time.h>
-#include <math.h>
+#define COMM_LEN		20
+#define SYM_LEN			129
 
-#include <stdio.h>
+#define MAX_PID			65536
 
-#define PR_SET_NAME	15               /* Set process name */
+static unsigned long		nr_tasks;
 
-#define BUG_ON(x)	assert(!(x))
+struct sched_event;
 
-#define DEBUG		0
+struct task_desc {
+	unsigned long		nr;
+	unsigned long		pid;
+	char			comm[COMM_LEN];
 
-typedef unsigned long long nsec_t;
+	unsigned long		nr_events;
+	unsigned long		curr_event;
+	struct sched_event	**events;
+
+	pthread_t		thread;
+	sem_t			sleep_sem;
 
-static nsec_t run_measurement_overhead;
-static nsec_t sleep_measurement_overhead;
+	sem_t			ready_for_work;
+	sem_t			work_done_sem;
+
+	u64			cpu_usage;
+};
+
+enum sched_event_type {
+	SCHED_EVENT_RUN,
+	SCHED_EVENT_SLEEP,
+	SCHED_EVENT_WAKEUP,
+};
+
+struct sched_event {
+	enum sched_event_type	type;
+	u64			timestamp;
+	u64			duration;
+	unsigned long		nr;
+	int			specific_wait;
+	sem_t			*wait_sem;
+	struct task_desc	*wakee;
+};
+
+static struct task_desc		*pid_to_task[MAX_PID];
+
+static struct task_desc		**tasks;
+
+static pthread_mutex_t		start_work_mutex = PTHREAD_MUTEX_INITIALIZER;
+static u64			start_time;
+
+static pthread_mutex_t		work_done_wait_mutex = PTHREAD_MUTEX_INITIALIZER;
 
-static nsec_t get_nsecs(void)
+static unsigned long		nr_run_events;
+static unsigned long		nr_sleep_events;
+static unsigned long		nr_wakeup_events;
+
+static unsigned long		nr_sleep_corrections;
+static unsigned long		nr_run_events_optimized;
+
+static unsigned long		targetless_wakeups;
+static unsigned long		multitarget_wakeups;
+
+static u64			cpu_usage;
+static u64			runavg_cpu_usage;
+static u64			parent_cpu_usage;
+static u64			runavg_parent_cpu_usage;
+
+static unsigned long		nr_runs;
+static u64			sum_runtime;
+static u64			sum_fluct;
+static u64			run_avg;
+
+static unsigned long		replay_repeat = 10;
+
+#define TASK_STATE_TO_CHAR_STR "RSDTtZX"
+
+enum thread_state {
+	THREAD_SLEEPING = 0,
+	THREAD_WAIT_CPU,
+	THREAD_SCHED_IN,
+	THREAD_IGNORE
+};
+
+struct work_atom {
+	struct list_head	list;
+	enum thread_state	state;
+	u64			wake_up_time;
+	u64			sched_in_time;
+	u64			runtime;
+};
+
+struct task_atoms {
+	struct list_head	atom_list;
+	struct thread		*thread;
+	struct rb_node		node;
+	u64			max_lat;
+	u64			total_lat;
+	u64			nb_atoms;
+	u64			total_runtime;
+};
+
+typedef int (*sort_thread_lat)(struct task_atoms *, struct task_atoms *);
+
+static struct rb_root		atom_root, sorted_atom_root;
+
+static u64			all_runtime;
+static u64			all_count;
+
+static int read_events(void);
+
+
+static u64 get_nsecs(void)
 {
 	struct timespec ts;
 
@@ -79,16 +163,16 @@ static nsec_t get_nsecs(void)
 	return ts.tv_sec * 1000000000ULL + ts.tv_nsec;
 }
 
-static void burn_nsecs(nsec_t nsecs)
+static void burn_nsecs(u64 nsecs)
 {
-	nsec_t T0 = get_nsecs(), T1;
+	u64 T0 = get_nsecs(), T1;
 
 	do {
 		T1 = get_nsecs();
 	} while (T1 + run_measurement_overhead < T0 + nsecs);
 }
 
-static void sleep_nsecs(nsec_t nsecs)
+static void sleep_nsecs(u64 nsecs)
 {
 	struct timespec ts;
 
@@ -100,7 +184,7 @@ static void sleep_nsecs(nsec_t nsecs)
 
 static void calibrate_run_measurement_overhead(void)
 {
-	nsec_t T0, T1, delta, min_delta = 1000000000ULL;
+	u64 T0, T1, delta, min_delta = 1000000000ULL;
 	int i;
 
 	for (i = 0; i < 10; i++) {
@@ -117,7 +201,7 @@ static void calibrate_run_measurement_overhead(void)
 
 static void calibrate_sleep_measurement_overhead(void)
 {
-	nsec_t T0, T1, delta, min_delta = 1000000000ULL;
+	u64 T0, T1, delta, min_delta = 1000000000ULL;
 	int i;
 
 	for (i = 0; i < 10; i++) {
@@ -133,67 +217,8 @@ static void calibrate_sleep_measurement_overhead(void)
 	printf("sleep measurement overhead: %Ld nsecs\n", min_delta);
 }
 
-#define COMM_LEN	20
-#define SYM_LEN		129
-
-#define MAX_PID		65536
-
-static unsigned long nr_tasks;
-
-struct sched_event;
-
-struct task_desc {
-	unsigned long		nr;
-	unsigned long		pid;
-	char			comm[COMM_LEN];
-
-	unsigned long		nr_events;
-	unsigned long		curr_event;
-	struct sched_event	**events;
-
-	pthread_t		thread;
-	sem_t			sleep_sem;
-
-	sem_t			ready_for_work;
-	sem_t			work_done_sem;
-
-	nsec_t			cpu_usage;
-};
-
-enum sched_event_type {
-	SCHED_EVENT_RUN,
-	SCHED_EVENT_SLEEP,
-	SCHED_EVENT_WAKEUP,
-};
-
-struct sched_event {
-	enum sched_event_type	type;
-	nsec_t			timestamp;
-	nsec_t			duration;
-	unsigned long		nr;
-	int			specific_wait;
-	sem_t			*wait_sem;
-	struct task_desc	*wakee;
-};
-
-static struct task_desc		*pid_to_task[MAX_PID];
-
-static struct task_desc		**tasks;
-
-static pthread_mutex_t		start_work_mutex = PTHREAD_MUTEX_INITIALIZER;
-static nsec_t			start_time;
-
-static pthread_mutex_t		work_done_wait_mutex = PTHREAD_MUTEX_INITIALIZER;
-
-static unsigned long		nr_run_events;
-static unsigned long		nr_sleep_events;
-static unsigned long		nr_wakeup_events;
-
-static unsigned long		nr_sleep_corrections;
-static unsigned long		nr_run_events_optimized;
-
 static struct sched_event *
-get_new_event(struct task_desc *task, nsec_t timestamp)
+get_new_event(struct task_desc *task, u64 timestamp)
 {
 	struct sched_event *event = calloc(1, sizeof(*event));
 	unsigned long idx = task->nr_events;
@@ -221,7 +246,7 @@ static struct sched_event *last_event(struct task_desc *task)
 }
 
 static void
-add_sched_event_run(struct task_desc *task, nsec_t timestamp, u64 duration)
+add_sched_event_run(struct task_desc *task, u64 timestamp, u64 duration)
 {
 	struct sched_event *event, *curr_event = last_event(task);
 
@@ -243,11 +268,8 @@ add_sched_event_run(struct task_desc *task, nsec_t timestamp, u64 duration)
 	nr_run_events++;
 }
 
-static unsigned long		targetless_wakeups;
-static unsigned long		multitarget_wakeups;
-
 static void
-add_sched_event_wakeup(struct task_desc *task, nsec_t timestamp,
+add_sched_event_wakeup(struct task_desc *task, u64 timestamp,
 		       struct task_desc *wakee)
 {
 	struct sched_event *event, *wakee_event;
@@ -275,7 +297,7 @@ add_sched_event_wakeup(struct task_desc *task, nsec_t timestamp,
 }
 
 static void
-add_sched_event_sleep(struct task_desc *task, nsec_t timestamp,
+add_sched_event_sleep(struct task_desc *task, u64 timestamp,
 		      u64 task_state __used)
 {
 	struct sched_event *event = get_new_event(task, timestamp);
@@ -350,7 +372,7 @@ static void
 process_sched_event(struct task_desc *this_task __used, struct sched_event *event)
 {
 	int ret = 0;
-	nsec_t now;
+	u64 now;
 	long long delta;
 
 	now = get_nsecs();
@@ -375,10 +397,10 @@ process_sched_event(struct task_desc *this_task __used, struct sched_event *even
 	}
 }
 
-static nsec_t get_cpu_usage_nsec_parent(void)
+static u64 get_cpu_usage_nsec_parent(void)
 {
 	struct rusage ru;
-	nsec_t sum;
+	u64 sum;
 	int err;
 
 	err = getrusage(RUSAGE_SELF, &ru);
@@ -390,12 +412,12 @@ static nsec_t get_cpu_usage_nsec_parent(void)
 	return sum;
 }
 
-static nsec_t get_cpu_usage_nsec_self(void)
+static u64 get_cpu_usage_nsec_self(void)
 {
 	char filename [] = "/proc/1234567890/sched";
 	unsigned long msecs, nsecs;
 	char *line = NULL;
-	nsec_t total = 0;
+	u64 total = 0;
 	size_t len = 0;
 	ssize_t chars;
 	FILE *file;
@@ -423,7 +445,7 @@ static nsec_t get_cpu_usage_nsec_self(void)
 static void *thread_func(void *ctx)
 {
 	struct task_desc *this_task = ctx;
-	nsec_t cpu_usage_0, cpu_usage_1;
+	u64 cpu_usage_0, cpu_usage_1;
 	unsigned long i, ret;
 	char comm2[22];
 
@@ -485,14 +507,9 @@ static void create_tasks(void)
 	}
 }
 
-static nsec_t			cpu_usage;
-static nsec_t			runavg_cpu_usage;
-static nsec_t			parent_cpu_usage;
-static nsec_t			runavg_parent_cpu_usage;
-
 static void wait_for_tasks(void)
 {
-	nsec_t cpu_usage_0, cpu_usage_1;
+	u64 cpu_usage_0, cpu_usage_1;
 	struct task_desc *task;
 	unsigned long i, ret;
 
@@ -543,16 +560,9 @@ static void wait_for_tasks(void)
 	}
 }
 
-static int read_events(void);
-
-static unsigned long nr_runs;
-static nsec_t sum_runtime;
-static nsec_t sum_fluct;
-static nsec_t run_avg;
-
 static void run_one_test(void)
 {
-	nsec_t T0, T1, delta, avg_delta, fluct, std_dev;
+	u64 T0, T1, delta, avg_delta, fluct, std_dev;
 
 	T0 = get_nsecs();
 	wait_for_tasks();
@@ -576,10 +586,6 @@ static void run_one_test(void)
 	printf("#%-3ld: %0.3f, ",
 		nr_runs, (double)delta/1000000.0);
 
-#if 0
-	printf("%0.2f +- %0.2f, ",
-		(double)avg_delta/1e6, (double)std_dev/1e6);
-#endif
 	printf("ravg: %0.2f, ",
 		(double)run_avg/1e6);
 
@@ -605,7 +611,7 @@ static void run_one_test(void)
 
 static void test_calibrations(void)
 {
-	nsec_t T0, T1;
+	u64 T0, T1;
 
 	T0 = get_nsecs();
 	burn_nsecs(1e6);
@@ -620,8 +626,6 @@ static void test_calibrations(void)
 	printf("the sleep test took %Ld nsecs\n", T1-T0);
 }
 
-static unsigned long replay_repeat = 10;
-
 static void __cmd_replay(void)
 {
 	unsigned long i;
@@ -865,47 +869,8 @@ static struct trace_sched_handler replay_ops  = {
 	.fork_event		= replay_fork_event,
 };
 
-#define TASK_STATE_TO_CHAR_STR "RSDTtZX"
-
-enum thread_state {
-	THREAD_SLEEPING = 0,
-	THREAD_WAIT_CPU,
-	THREAD_SCHED_IN,
-	THREAD_IGNORE
-};
-
-struct work_atom {
-	struct list_head	list;
-	enum thread_state	state;
-	u64			wake_up_time;
-	u64			sched_in_time;
-	u64			runtime;
-};
-
-struct task_atoms {
-	struct list_head	snapshot_list;
-	struct thread		*thread;
-	struct rb_node		node;
-	u64			max_lat;
-	u64			total_lat;
-	u64			nb_atoms;
-	u64			total_runtime;
-};
-
-typedef int (*sort_thread_lat)(struct task_atoms *, struct task_atoms *);
-
-struct sort_dimension {
-	const char 		*name;
-	sort_thread_lat		cmp;
-	struct list_head 	list;
-};
-
-static LIST_HEAD(cmp_pid);
-
-static struct rb_root lat_snapshot_root, sorted_lat_snapshot_root;
-
 static struct task_atoms *
-thread_atom_list_search(struct rb_root *root, struct thread *thread)
+thread_atoms_search(struct rb_root *root, struct thread *thread)
 {
 	struct rb_node *node = root->rb_node;
 
@@ -924,6 +889,14 @@ thread_atom_list_search(struct rb_root *root, struct thread *thread)
 	return NULL;
 }
 
+struct sort_dimension {
+	const char		*name;
+	sort_thread_lat		cmp;
+	struct list_head	list;
+};
+
+static LIST_HEAD(cmp_pid);
+
 static int
 thread_lat_cmp(struct list_head *list, struct task_atoms *l,
 	       struct task_atoms *r)
@@ -965,16 +938,17 @@ __thread_latency_insert(struct rb_root *root, struct task_atoms *data,
 	rb_insert_color(&data->node, root);
 }
 
-static void thread_atom_list_insert(struct thread *thread)
+static void thread_atoms_insert(struct thread *thread)
 {
 	struct task_atoms *atoms;
+
 	atoms = calloc(sizeof(*atoms), 1);
 	if (!atoms)
 		die("No memory");
 
 	atoms->thread = thread;
-	INIT_LIST_HEAD(&atoms->snapshot_list);
-	__thread_latency_insert(&lat_snapshot_root, atoms, &cmp_pid);
+	INIT_LIST_HEAD(&atoms->atom_list);
+	__thread_latency_insert(&atom_root, atoms, &cmp_pid);
 }
 
 static void
@@ -1001,50 +975,49 @@ lat_sched_out(struct task_atoms *atoms,
 	      u64 delta,
 	      u64 timestamp)
 {
-	struct work_atom *snapshot;
+	struct work_atom *atom;
 
-	snapshot = calloc(sizeof(*snapshot), 1);
-	if (!snapshot)
+	atom = calloc(sizeof(*atom), 1);
+	if (!atom)
 		die("Non memory");
 
 	if (sched_out_state(switch_event) == 'R') {
-		snapshot->state = THREAD_WAIT_CPU;
-		snapshot->wake_up_time = timestamp;
+		atom->state = THREAD_WAIT_CPU;
+		atom->wake_up_time = timestamp;
 	}
 
-	snapshot->runtime = delta;
-	list_add_tail(&snapshot->list, &atoms->snapshot_list);
+	atom->runtime = delta;
+	list_add_tail(&atom->list, &atoms->atom_list);
 }
 
 static void
 lat_sched_in(struct task_atoms *atoms, u64 timestamp)
 {
-	struct work_atom *snapshot;
+	struct work_atom *atom;
 	u64 delta;
 
-	if (list_empty(&atoms->snapshot_list))
+	if (list_empty(&atoms->atom_list))
 		return;
 
-	snapshot = list_entry(atoms->snapshot_list.prev, struct work_atom,
-			      list);
+	atom = list_entry(atoms->atom_list.prev, struct work_atom, list);
 
-	if (snapshot->state != THREAD_WAIT_CPU)
+	if (atom->state != THREAD_WAIT_CPU)
 		return;
 
-	if (timestamp < snapshot->wake_up_time) {
-		snapshot->state = THREAD_IGNORE;
+	if (timestamp < atom->wake_up_time) {
+		atom->state = THREAD_IGNORE;
 		return;
 	}
 
-	snapshot->state = THREAD_SCHED_IN;
-	snapshot->sched_in_time = timestamp;
+	atom->state = THREAD_SCHED_IN;
+	atom->sched_in_time = timestamp;
 
-	delta = snapshot->sched_in_time - snapshot->wake_up_time;
+	delta = atom->sched_in_time - atom->wake_up_time;
 	atoms->total_lat += delta;
 	if (delta > atoms->max_lat)
 		atoms->max_lat = delta;
 	atoms->nb_atoms++;
-	atoms->total_runtime += snapshot->runtime;
+	atoms->total_runtime += atom->runtime;
 }
 
 static void
@@ -1076,20 +1049,20 @@ latency_switch_event(struct trace_switch_event *switch_event,
 	sched_out = threads__findnew(switch_event->prev_pid, &threads, &last_match);
 	sched_in = threads__findnew(switch_event->next_pid, &threads, &last_match);
 
-	in_atoms = thread_atom_list_search(&lat_snapshot_root, sched_in);
+	in_atoms = thread_atoms_search(&atom_root, sched_in);
 	if (!in_atoms) {
-		thread_atom_list_insert(sched_in);
-		in_atoms = thread_atom_list_search(&lat_snapshot_root, sched_in);
+		thread_atoms_insert(sched_in);
+		in_atoms = thread_atoms_search(&atom_root, sched_in);
 		if (!in_atoms)
-			die("Internal latency tree error");
+			die("in-atom: Internal tree error");
 	}
 
-	out_atoms = thread_atom_list_search(&lat_snapshot_root, sched_out);
+	out_atoms = thread_atoms_search(&atom_root, sched_out);
 	if (!out_atoms) {
-		thread_atom_list_insert(sched_out);
-		out_atoms = thread_atom_list_search(&lat_snapshot_root, sched_out);
+		thread_atoms_insert(sched_out);
+		out_atoms = thread_atoms_search(&atom_root, sched_out);
 		if (!out_atoms)
-			die("Internal latency tree error");
+			die("out-atom: Internal tree error");
 	}
 
 	lat_sched_in(in_atoms, timestamp);
@@ -1104,7 +1077,7 @@ latency_wakeup_event(struct trace_wakeup_event *wakeup_event,
 		     struct thread *thread __used)
 {
 	struct task_atoms *atoms;
-	struct work_atom *snapshot;
+	struct work_atom *atom;
 	struct thread *wakee;
 
 	/* Note for later, it may be interesting to observe the failing cases */
@@ -1112,23 +1085,22 @@ latency_wakeup_event(struct trace_wakeup_event *wakeup_event,
 		return;
 
 	wakee = threads__findnew(wakeup_event->pid, &threads, &last_match);
-	atoms = thread_atom_list_search(&lat_snapshot_root, wakee);
+	atoms = thread_atoms_search(&atom_root, wakee);
 	if (!atoms) {
-		thread_atom_list_insert(wakee);
+		thread_atoms_insert(wakee);
 		return;
 	}
 
-	if (list_empty(&atoms->snapshot_list))
+	if (list_empty(&atoms->atom_list))
 		return;
 
-	snapshot = list_entry(atoms->snapshot_list.prev, struct work_atom,
-			      list);
+	atom = list_entry(atoms->atom_list.prev, struct work_atom, list);
 
-	if (snapshot->state != THREAD_SLEEPING)
+	if (atom->state != THREAD_SLEEPING)
 		return;
 
-	snapshot->state = THREAD_WAIT_CPU;
-	snapshot->wake_up_time = timestamp;
+	atom->state = THREAD_WAIT_CPU;
+	atom->wake_up_time = timestamp;
 }
 
 static struct trace_sched_handler lat_ops  = {
@@ -1137,9 +1109,6 @@ static struct trace_sched_handler lat_ops  = {
 	.fork_event		= latency_fork_event,
 };
 
-static u64 all_runtime;
-static u64 all_count;
-
 static void output_lat_thread(struct task_atoms *atom_list)
 {
 	int i;
@@ -1287,13 +1256,13 @@ static void sort_lat(void)
 
 	for (;;) {
 		struct task_atoms *data;
-		node = rb_first(&lat_snapshot_root);
+		node = rb_first(&atom_root);
 		if (!node)
 			break;
 
-		rb_erase(node, &lat_snapshot_root);
+		rb_erase(node, &atom_root);
 		data = rb_entry(node, struct task_atoms, node);
-		__thread_latency_insert(&sorted_lat_snapshot_root, data, &sort_list);
+		__thread_latency_insert(&sorted_atom_root, data, &sort_list);
 	}
 }
 
@@ -1309,7 +1278,7 @@ static void __cmd_lat(void)
 	printf(" Task              |  Runtime ms | Switches | Average delay ms | Maximum delay ms |\n");
 	printf("-----------------------------------------------------------------------------------\n");
 
-	next = rb_first(&sorted_lat_snapshot_root);
+	next = rb_first(&sorted_atom_root);
 
 	while (next) {
 		struct task_atoms *atom_list;

commit f2858d8ad9858e63c87257553c5721cba5db95ae
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Sep 11 12:12:54 2009 +0200

    perf sched: Add 'perf sched latency' and 'perf sched replay'
    
    Separate the option parsing cleanly and add two variants:
    
     - 'perf sched latency' (can be abbreviated via 'perf sched lat')
     - 'perf sched replay'  (can be abbreviated via 'perf sched rep')
    
    Also add a repeat count option to replay and add a separation
    set of options for replay.
    
    Do the sorting setup only in the latency sub-command.
    
    Display separate help screens for 'perf sched' and
    'perf sched replay -h' - i.e. further separation of the
    sub-commands.
    
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 10fcd49e298b..e01cc63b98cc 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -30,9 +30,6 @@ static struct thread		*last_match;
 static struct perf_header	*header;
 static u64			sample_type;
 
-static int			replay_mode;
-static int			lat_mode;
-
 static char			default_sort_order[] = "avg, max, switch, runtime";
 static char			*sort_order = default_sort_order;
 
@@ -623,9 +620,11 @@ static void test_calibrations(void)
 	printf("the sleep test took %Ld nsecs\n", T1-T0);
 }
 
+static unsigned long replay_repeat = 10;
+
 static void __cmd_replay(void)
 {
-	long nr_iterations = 10, i;
+	unsigned long i;
 
 	calibrate_run_measurement_overhead();
 	calibrate_sleep_measurement_overhead();
@@ -651,7 +650,7 @@ static void __cmd_replay(void)
 
 	create_tasks();
 	printf("------------------------------------------------------------\n");
-	for (i = 0; i < nr_iterations; i++)
+	for (i = 0; i < replay_repeat; i++)
 		run_one_test();
 }
 
@@ -1623,21 +1622,45 @@ static int read_events(void)
 }
 
 static const char * const sched_usage[] = {
-	"perf sched [<options>] <command>",
+	"perf sched [<options>] {record|latency|replay}",
 	NULL
 };
 
-static const struct option options[] = {
+static const struct option sched_options[] = {
+	OPT_BOOLEAN('v', "verbose", &verbose,
+		    "be more verbose (show symbol address, etc)"),
 	OPT_BOOLEAN('D', "dump-raw-trace", &dump_trace,
 		    "dump raw trace in ASCII"),
-	OPT_BOOLEAN('r', "replay", &replay_mode,
-		    "replay sched behaviour from traces"),
-	OPT_BOOLEAN('l', "latency", &lat_mode,
-		    "measure various latencies"),
+	OPT_END()
+};
+
+static const char * const latency_usage[] = {
+	"perf sched latency [<options>]",
+	NULL
+};
+
+static const struct option latency_options[] = {
 	OPT_STRING('s', "sort", &sort_order, "key[,key2...]",
 		   "sort by key(s): runtime, switch, avg, max"),
 	OPT_BOOLEAN('v', "verbose", &verbose,
 		    "be more verbose (show symbol address, etc)"),
+	OPT_BOOLEAN('D', "dump-raw-trace", &dump_trace,
+		    "dump raw trace in ASCII"),
+	OPT_END()
+};
+
+static const char * const replay_usage[] = {
+	"perf sched replay [<options>]",
+	NULL
+};
+
+static const struct option replay_options[] = {
+	OPT_INTEGER('r', "repeat", &replay_repeat,
+		    "repeat the workload replay N times (-1: infinite)"),
+	OPT_BOOLEAN('v', "verbose", &verbose,
+		    "be more verbose (show symbol address, etc)"),
+	OPT_BOOLEAN('D', "dump-raw-trace", &dump_trace,
+		    "dump raw trace in ASCII"),
 	OPT_END()
 };
 
@@ -1649,7 +1672,7 @@ static void setup_sorting(void)
 			tok; tok = strtok_r(NULL, ", ", &tmp)) {
 		if (sort_dimension__add(tok, &sort_list) < 0) {
 			error("Unknown --sort key: `%s'", tok);
-			usage_with_options(sched_usage, options);
+			usage_with_options(latency_usage, latency_options);
 		}
 	}
 
@@ -1663,29 +1686,32 @@ int cmd_sched(int argc, const char **argv, const char *prefix __used)
 	symbol__init();
 	page_size = getpagesize();
 
-	argc = parse_options(argc, argv, options, sched_usage, 0);
-	if (argc) {
-		/*
-		 * Special case: if there's an argument left then assume tha
-		 * it's a symbol filter:
-		 */
-		if (argc > 1)
-			usage_with_options(sched_usage, options);
-	}
+	argc = parse_options(argc, argv, sched_options, sched_usage,
+			     PARSE_OPT_STOP_AT_NON_OPTION);
+	if (!argc)
+		usage_with_options(sched_usage, sched_options);
 
-	if (replay_mode)
-		trace_handler = &replay_ops;
-	else if (lat_mode)
+	if (!strncmp(argv[0], "lat", 3)) {
 		trace_handler = &lat_ops;
-	else
-		usage_with_options(sched_usage, options);
-
-	setup_sorting();
-
-	if (replay_mode)
-		__cmd_replay();
-	else if (lat_mode)
+		if (argc > 1) {
+			argc = parse_options(argc, argv, latency_options, latency_usage, 0);
+			if (argc)
+				usage_with_options(latency_usage, latency_options);
+			setup_sorting();
+		}
 		__cmd_lat();
+	} else if (!strncmp(argv[0], "rep", 3)) {
+		trace_handler = &replay_ops;
+		if (argc) {
+			argc = parse_options(argc, argv, replay_options, replay_usage, 0);
+			if (argc)
+				usage_with_options(replay_usage, replay_options);
+		}
+		__cmd_replay();
+	} else {
+		usage_with_options(sched_usage, sched_options);
+	}
+
 
 	return 0;
 }

commit daa1d7a5eafc0a3a91a9add6a9a9f1bcaed63108
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sun Sep 13 03:36:29 2009 +0200

    perf sched: Implement multidimensional sorting
    
    Implement multidimensional sorting on perf sched so that
    you can sort either by number of switches, latency average,
    latency maximum, runtime.
    
    perf sched -l -s avg,max  (this is the default)
    
    -----------------------------------------------------------------------------------
     Task              |  Runtime ms | Switches | Average delay ms | Maximum delay ms |
    -----------------------------------------------------------------------------------
     gnome-power-man   |    0.113 ms |        1 | avg: 4998.531 ms | max: 4998.531 ms |
     xfdesktop         |    1.190 ms |        7 | avg:  136.475 ms | max:  940.933 ms |
     xfce-mcs-manage   |    2.194 ms |       22 | avg:   38.534 ms | max:  735.174 ms |
     notification-da   |    2.749 ms |       31 | avg:   27.436 ms | max:  731.791 ms |
     xfce4-session     |    3.343 ms |       28 | avg:   26.796 ms | max:  734.891 ms |
     xfwm4             |    3.159 ms |       22 | avg:   12.406 ms | max:  241.333 ms |
     xchat             |   42.789 ms |      214 | avg:   11.886 ms | max:  100.349 ms |
     xfce4-terminal    |    5.386 ms |       22 | avg:   11.414 ms | max:  241.611 ms |
     firefox           |  151.992 ms |      123 | avg:    9.543 ms | max:  153.717 ms |
     xfce4-panel       |   24.324 ms |       47 | avg:    8.189 ms | max:  242.352 ms |
     :5090             |    6.932 ms |      111 | avg:    8.131 ms | max:  102.665 ms |
     events/0          |    0.758 ms |       12 | avg:    1.964 ms | max:   21.879 ms |
     Xorg              |  280.558 ms |      340 | avg:    1.864 ms | max:   99.526 ms |
     geany             |   63.391 ms |      295 | avg:    1.099 ms | max:    9.334 ms |
     reiserfs/0        |    0.039 ms |        2 | avg:    0.854 ms | max:    1.487 ms |
     kondemand/0       |    8.251 ms |      245 | avg:    0.691 ms | max:   34.372 ms |
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 67a0ba88aecf..10fcd49e298b 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -33,6 +33,9 @@ static u64			sample_type;
 static int			replay_mode;
 static int			lat_mode;
 
+static char			default_sort_order[] = "avg, max, switch, runtime";
+static char			*sort_order = default_sort_order;
+
 
 /*
  * Scheduler benchmarks
@@ -890,7 +893,17 @@ struct task_atoms {
 	u64			total_runtime;
 };
 
-static struct rb_root lat_snapshot_root;
+typedef int (*sort_thread_lat)(struct task_atoms *, struct task_atoms *);
+
+struct sort_dimension {
+	const char 		*name;
+	sort_thread_lat		cmp;
+	struct list_head 	list;
+};
+
+static LIST_HEAD(cmp_pid);
+
+static struct rb_root lat_snapshot_root, sorted_lat_snapshot_root;
 
 static struct task_atoms *
 thread_atom_list_search(struct rb_root *root, struct thread *thread)
@@ -901,9 +914,9 @@ thread_atom_list_search(struct rb_root *root, struct thread *thread)
 		struct task_atoms *atoms;
 
 		atoms = container_of(node, struct task_atoms, node);
-		if (thread->pid < atoms->thread->pid)
+		if (thread->pid > atoms->thread->pid)
 			node = node->rb_left;
-		else if (thread->pid > atoms->thread->pid)
+		else if (thread->pid < atoms->thread->pid)
 			node = node->rb_right;
 		else {
 			return atoms;
@@ -912,22 +925,41 @@ thread_atom_list_search(struct rb_root *root, struct thread *thread)
 	return NULL;
 }
 
+static int
+thread_lat_cmp(struct list_head *list, struct task_atoms *l,
+	       struct task_atoms *r)
+{
+	struct sort_dimension *sort;
+	int ret = 0;
+
+	list_for_each_entry(sort, list, list) {
+		ret = sort->cmp(l, r);
+		if (ret)
+			return ret;
+	}
+
+	return ret;
+}
+
 static void
-__thread_latency_insert(struct rb_root *root, struct task_atoms *data)
+__thread_latency_insert(struct rb_root *root, struct task_atoms *data,
+			 struct list_head *sort_list)
 {
 	struct rb_node **new = &(root->rb_node), *parent = NULL;
 
 	while (*new) {
 		struct task_atoms *this;
+		int cmp;
 
 		this = container_of(*new, struct task_atoms, node);
 		parent = *new;
-		if (data->thread->pid < this->thread->pid)
+
+		cmp = thread_lat_cmp(sort_list, data, this);
+
+		if (cmp > 0)
 			new = &((*new)->rb_left);
-		else if (data->thread->pid > this->thread->pid)
-			new = &((*new)->rb_right);
 		else
-			die("Double thread insertion\n");
+			new = &((*new)->rb_right);
 	}
 
 	rb_link_node(&data->node, parent, new);
@@ -943,7 +975,7 @@ static void thread_atom_list_insert(struct thread *thread)
 
 	atoms->thread = thread;
 	INIT_LIST_HEAD(&atoms->snapshot_list);
-	__thread_latency_insert(&lat_snapshot_root, atoms);
+	__thread_latency_insert(&lat_snapshot_root, atoms, &cmp_pid);
 }
 
 static void
@@ -1134,18 +1166,151 @@ static void output_lat_thread(struct task_atoms *atom_list)
 		 (double)atom_list->max_lat / 1e6);
 }
 
+static int pid_cmp(struct task_atoms *l, struct task_atoms *r)
+{
+
+	if (l->thread->pid < r->thread->pid)
+		return -1;
+	if (l->thread->pid > r->thread->pid)
+		return 1;
+
+	return 0;
+}
+
+static struct sort_dimension pid_sort_dimension = {
+	.name = "pid",
+	.cmp = pid_cmp,
+};
+
+static int avg_cmp(struct task_atoms *l, struct task_atoms *r)
+{
+	u64 avgl, avgr;
+
+	if (!l->nb_atoms)
+		return -1;
+
+	if (!r->nb_atoms)
+		return 1;
+
+	avgl = l->total_lat / l->nb_atoms;
+	avgr = r->total_lat / r->nb_atoms;
+
+	if (avgl < avgr)
+		return -1;
+	if (avgl > avgr)
+		return 1;
+
+	return 0;
+}
+
+static struct sort_dimension avg_sort_dimension = {
+	.name 	= "avg",
+	.cmp	= avg_cmp,
+};
+
+static int max_cmp(struct task_atoms *l, struct task_atoms *r)
+{
+	if (l->max_lat < r->max_lat)
+		return -1;
+	if (l->max_lat > r->max_lat)
+		return 1;
+
+	return 0;
+}
+
+static struct sort_dimension max_sort_dimension = {
+	.name 	= "max",
+	.cmp	= max_cmp,
+};
+
+static int switch_cmp(struct task_atoms *l, struct task_atoms *r)
+{
+	if (l->nb_atoms < r->nb_atoms)
+		return -1;
+	if (l->nb_atoms > r->nb_atoms)
+		return 1;
+
+	return 0;
+}
+
+static struct sort_dimension switch_sort_dimension = {
+	.name 	= "switch",
+	.cmp	= switch_cmp,
+};
+
+static int runtime_cmp(struct task_atoms *l, struct task_atoms *r)
+{
+	if (l->total_runtime < r->total_runtime)
+		return -1;
+	if (l->total_runtime > r->total_runtime)
+		return 1;
+
+	return 0;
+}
+
+static struct sort_dimension runtime_sort_dimension = {
+	.name 	= "runtime",
+	.cmp	= runtime_cmp,
+};
+
+static struct sort_dimension *available_sorts[] = {
+	&pid_sort_dimension,
+	&avg_sort_dimension,
+	&max_sort_dimension,
+	&switch_sort_dimension,
+	&runtime_sort_dimension,
+};
+
+#define NB_AVAILABLE_SORTS	(int)(sizeof(available_sorts) / sizeof(struct sort_dimension *))
+
+static LIST_HEAD(sort_list);
+
+static int sort_dimension__add(char *tok, struct list_head *list)
+{
+	int i;
+
+	for (i = 0; i < NB_AVAILABLE_SORTS; i++) {
+		if (!strcmp(available_sorts[i]->name, tok)) {
+			list_add_tail(&available_sorts[i]->list, list);
+
+			return 0;
+		}
+	}
+
+	return -1;
+}
+
+static void setup_sorting(void);
+
+static void sort_lat(void)
+{
+	struct rb_node *node;
+
+	for (;;) {
+		struct task_atoms *data;
+		node = rb_first(&lat_snapshot_root);
+		if (!node)
+			break;
+
+		rb_erase(node, &lat_snapshot_root);
+		data = rb_entry(node, struct task_atoms, node);
+		__thread_latency_insert(&sorted_lat_snapshot_root, data, &sort_list);
+	}
+}
+
 static void __cmd_lat(void)
 {
 	struct rb_node *next;
 
 	setup_pager();
 	read_events();
+	sort_lat();
 
 	printf("-----------------------------------------------------------------------------------\n");
 	printf(" Task              |  Runtime ms | Switches | Average delay ms | Maximum delay ms |\n");
 	printf("-----------------------------------------------------------------------------------\n");
 
-	next = rb_first(&lat_snapshot_root);
+	next = rb_first(&sorted_lat_snapshot_root);
 
 	while (next) {
 		struct task_atoms *atom_list;
@@ -1469,11 +1634,30 @@ static const struct option options[] = {
 		    "replay sched behaviour from traces"),
 	OPT_BOOLEAN('l', "latency", &lat_mode,
 		    "measure various latencies"),
+	OPT_STRING('s', "sort", &sort_order, "key[,key2...]",
+		   "sort by key(s): runtime, switch, avg, max"),
 	OPT_BOOLEAN('v', "verbose", &verbose,
 		    "be more verbose (show symbol address, etc)"),
 	OPT_END()
 };
 
+static void setup_sorting(void)
+{
+	char *tmp, *tok, *str = strdup(sort_order);
+
+	for (tok = strtok_r(str, ", ", &tmp);
+			tok; tok = strtok_r(NULL, ", ", &tmp)) {
+		if (sort_dimension__add(tok, &sort_list) < 0) {
+			error("Unknown --sort key: `%s'", tok);
+			usage_with_options(sched_usage, options);
+		}
+	}
+
+	free(str);
+
+	sort_dimension__add((char *)"pid", &cmp_pid);
+}
+
 int cmd_sched(int argc, const char **argv, const char *prefix __used)
 {
 	symbol__init();
@@ -1496,6 +1680,8 @@ int cmd_sched(int argc, const char **argv, const char *prefix __used)
 	else
 		usage_with_options(sched_usage, options);
 
+	setup_sorting();
+
 	if (replay_mode)
 		__cmd_replay();
 	else if (lat_mode)

commit 7362262687b21b0d04927a7615c162a3d064849e
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sun Sep 13 01:59:05 2009 +0200

    perf sched: Fix nsec to msec conversion
    
    We are dividing a time in ns by 1e9. This is a nsec to sec
    conversion. What we want is msecs. Fix it by dividing by 1e6.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 435702702789..67a0ba88aecf 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1129,9 +1129,9 @@ static void output_lat_thread(struct task_atoms *atom_list)
 	avg = atom_list->total_lat / atom_list->nb_atoms;
 
 	printf("|%9.3f ms |%9llu | avg:%9.3f ms | max:%9.3f ms |\n",
-	      (double)atom_list->total_runtime / 1e9,
-		 atom_list->nb_atoms, (double)avg / 1e9,
-		 (double)atom_list->max_lat / 1e9);
+	      (double)atom_list->total_runtime / 1e6,
+		 atom_list->nb_atoms, (double)avg / 1e6,
+		 (double)atom_list->max_lat / 1e6);
 }
 
 static void __cmd_lat(void)
@@ -1157,7 +1157,7 @@ static void __cmd_lat(void)
 
 	printf("-----------------------------------------------------------------------------------\n");
 	printf(" TOTAL:            |%9.3f ms |%9Ld |\n",
-		(double)all_runtime/1e9, all_count);
+		(double)all_runtime/1e6, all_count);
 	printf("---------------------------------------------\n");
 }
 

commit 66685678a03d0d8e8ba015472f280fb4f12f84c1
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sun Sep 13 01:56:25 2009 +0200

    perf sched: Export the total, max latency and total runtime to thread atoms list
    
    Add a field in the thread atom list that keeps track of the
    total and max latencies and also the total runtime. This makes
    a faster output and also prepares for sorting.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 61a80e8c9d0d..435702702789 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -884,6 +884,10 @@ struct task_atoms {
 	struct list_head	snapshot_list;
 	struct thread		*thread;
 	struct rb_node		node;
+	u64			max_lat;
+	u64			total_lat;
+	u64			nb_atoms;
+	u64			total_runtime;
 };
 
 static struct rb_root lat_snapshot_root;
@@ -985,6 +989,7 @@ static void
 lat_sched_in(struct task_atoms *atoms, u64 timestamp)
 {
 	struct work_atom *snapshot;
+	u64 delta;
 
 	if (list_empty(&atoms->snapshot_list))
 		return;
@@ -1002,6 +1007,13 @@ lat_sched_in(struct task_atoms *atoms, u64 timestamp)
 
 	snapshot->state = THREAD_SCHED_IN;
 	snapshot->sched_in_time = timestamp;
+
+	delta = snapshot->sched_in_time - snapshot->wake_up_time;
+	atoms->total_lat += delta;
+	if (delta > atoms->max_lat)
+		atoms->max_lat = delta;
+	atoms->nb_atoms++;
+	atoms->total_runtime += snapshot->runtime;
 }
 
 static void
@@ -1099,43 +1111,27 @@ static u64 all_count;
 
 static void output_lat_thread(struct task_atoms *atom_list)
 {
-	struct work_atom *atom;
-	int count = 0;
 	int i;
 	int ret;
-	u64 max = 0, avg;
-	u64 total = 0, delta;
-	u64 total_runtime = 0;
-
-	list_for_each_entry(atom, &atom_list->snapshot_list, list) {
-		total_runtime += atom->runtime;
-
-		if (atom->state != THREAD_SCHED_IN)
-			continue;
-
-		count++;
+	u64 avg;
 
-		delta = atom->sched_in_time - atom->wake_up_time;
-		if (delta > max)
-			max = delta;
-		total += delta;
-	}
-
-	all_runtime += total_runtime;
-	all_count += count;
-
-	if (!count)
+	if (!atom_list->nb_atoms)
 		return;
 
+	all_runtime += atom_list->total_runtime;
+	all_count += atom_list->nb_atoms;
+
 	ret = printf(" %s ", atom_list->thread->comm);
 
 	for (i = 0; i < 19 - ret; i++)
 		printf(" ");
 
-	avg = total / count;
+	avg = atom_list->total_lat / atom_list->nb_atoms;
 
-	printf("|%9.3f ms |%9d | avg:%9.3f ms | max:%9.3f ms |\n",
-		(double)total_runtime/1e9, count, (double)avg/1e9, (double)max/1e9);
+	printf("|%9.3f ms |%9llu | avg:%9.3f ms | max:%9.3f ms |\n",
+	      (double)atom_list->total_runtime / 1e9,
+		 atom_list->nb_atoms, (double)avg / 1e9,
+		 (double)atom_list->max_lat / 1e9);
 }
 
 static void __cmd_lat(void)

commit c6ced61112f1e6139914149fab65695801a74f0f
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sun Sep 13 00:46:19 2009 +0200

    perf sched: Add involuntarily sleeping task in work atoms
    
    Currently in perf sched, we are measuring the scheduler wakeup
    latencies.
    
    Now we also want measure the time a task wait to be scheduled
    after it gets preempted.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 7e57a986c056..61a80e8c9d0d 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -866,8 +866,8 @@ static struct trace_sched_handler replay_ops  = {
 #define TASK_STATE_TO_CHAR_STR "RSDTtZX"
 
 enum thread_state {
-	THREAD_SLEEPING,
-	THREAD_WAKED_UP,
+	THREAD_SLEEPING = 0,
+	THREAD_WAIT_CPU,
 	THREAD_SCHED_IN,
 	THREAD_IGNORE
 };
@@ -962,7 +962,9 @@ static char sched_out_state(struct trace_switch_event *switch_event)
 
 static void
 lat_sched_out(struct task_atoms *atoms,
-	     struct trace_switch_event *switch_event __used, u64 delta)
+	      struct trace_switch_event *switch_event __used,
+	      u64 delta,
+	      u64 timestamp)
 {
 	struct work_atom *snapshot;
 
@@ -970,6 +972,11 @@ lat_sched_out(struct task_atoms *atoms,
 	if (!snapshot)
 		die("Non memory");
 
+	if (sched_out_state(switch_event) == 'R') {
+		snapshot->state = THREAD_WAIT_CPU;
+		snapshot->wake_up_time = timestamp;
+	}
+
 	snapshot->runtime = delta;
 	list_add_tail(&snapshot->list, &atoms->snapshot_list);
 }
@@ -985,7 +992,7 @@ lat_sched_in(struct task_atoms *atoms, u64 timestamp)
 	snapshot = list_entry(atoms->snapshot_list.prev, struct work_atom,
 			      list);
 
-	if (snapshot->state != THREAD_WAKED_UP)
+	if (snapshot->state != THREAD_WAIT_CPU)
 		return;
 
 	if (timestamp < snapshot->wake_up_time) {
@@ -1043,7 +1050,7 @@ latency_switch_event(struct trace_switch_event *switch_event,
 	}
 
 	lat_sched_in(in_atoms, timestamp);
-	lat_sched_out(out_atoms, switch_event, delta);
+	lat_sched_out(out_atoms, switch_event, delta, timestamp);
 }
 
 static void
@@ -1077,7 +1084,7 @@ latency_wakeup_event(struct trace_wakeup_event *wakeup_event,
 	if (snapshot->state != THREAD_SLEEPING)
 		return;
 
-	snapshot->state = THREAD_WAKED_UP;
+	snapshot->state = THREAD_WAIT_CPU;
 	snapshot->wake_up_time = timestamp;
 }
 

commit 175622053069afbd366ba3c6030b5af82f378d40
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Sep 12 23:11:32 2009 +0200

    perf sched: Rename struct lat_snapshot to struct work atoms
    
    To measures the latencies, we capture the sched atoms data into
    a specific structure named struct lat_snapshot.
    
    As this structure can be used for other purposes of scheduler
    profiling and mirrors what happens in a thread work atom, lets
    rename it to struct work_atom and propagate this renaming in
    other functions and structures names to keep it coherent.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 727cc5b852c2..7e57a986c056 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -872,7 +872,7 @@ enum thread_state {
 	THREAD_IGNORE
 };
 
-struct lat_snapshot {
+struct work_atom {
 	struct list_head	list;
 	enum thread_state	state;
 	u64			wake_up_time;
@@ -880,7 +880,7 @@ struct lat_snapshot {
 	u64			runtime;
 };
 
-struct thread_latency {
+struct task_atoms {
 	struct list_head	snapshot_list;
 	struct thread		*thread;
 	struct rb_node		node;
@@ -888,35 +888,35 @@ struct thread_latency {
 
 static struct rb_root lat_snapshot_root;
 
-static struct thread_latency *
-thread_latency_search(struct rb_root *root, struct thread *thread)
+static struct task_atoms *
+thread_atom_list_search(struct rb_root *root, struct thread *thread)
 {
 	struct rb_node *node = root->rb_node;
 
 	while (node) {
-		struct thread_latency *lat;
+		struct task_atoms *atoms;
 
-		lat = container_of(node, struct thread_latency, node);
-		if (thread->pid < lat->thread->pid)
+		atoms = container_of(node, struct task_atoms, node);
+		if (thread->pid < atoms->thread->pid)
 			node = node->rb_left;
-		else if (thread->pid > lat->thread->pid)
+		else if (thread->pid > atoms->thread->pid)
 			node = node->rb_right;
 		else {
-			return lat;
+			return atoms;
 		}
 	}
 	return NULL;
 }
 
 static void
-__thread_latency_insert(struct rb_root *root, struct thread_latency *data)
+__thread_latency_insert(struct rb_root *root, struct task_atoms *data)
 {
 	struct rb_node **new = &(root->rb_node), *parent = NULL;
 
 	while (*new) {
-		struct thread_latency *this;
+		struct task_atoms *this;
 
-		this = container_of(*new, struct thread_latency, node);
+		this = container_of(*new, struct task_atoms, node);
 		parent = *new;
 		if (data->thread->pid < this->thread->pid)
 			new = &((*new)->rb_left);
@@ -930,16 +930,16 @@ __thread_latency_insert(struct rb_root *root, struct thread_latency *data)
 	rb_insert_color(&data->node, root);
 }
 
-static void thread_latency_insert(struct thread *thread)
+static void thread_atom_list_insert(struct thread *thread)
 {
-	struct thread_latency *lat;
-	lat = calloc(sizeof(*lat), 1);
-	if (!lat)
+	struct task_atoms *atoms;
+	atoms = calloc(sizeof(*atoms), 1);
+	if (!atoms)
 		die("No memory");
 
-	lat->thread = thread;
-	INIT_LIST_HEAD(&lat->snapshot_list);
-	__thread_latency_insert(&lat_snapshot_root, lat);
+	atoms->thread = thread;
+	INIT_LIST_HEAD(&atoms->snapshot_list);
+	__thread_latency_insert(&lat_snapshot_root, atoms);
 }
 
 static void
@@ -961,28 +961,28 @@ static char sched_out_state(struct trace_switch_event *switch_event)
 }
 
 static void
-lat_sched_out(struct thread_latency *lat,
+lat_sched_out(struct task_atoms *atoms,
 	     struct trace_switch_event *switch_event __used, u64 delta)
 {
-	struct lat_snapshot *snapshot;
+	struct work_atom *snapshot;
 
 	snapshot = calloc(sizeof(*snapshot), 1);
 	if (!snapshot)
 		die("Non memory");
 
 	snapshot->runtime = delta;
-	list_add_tail(&snapshot->list, &lat->snapshot_list);
+	list_add_tail(&snapshot->list, &atoms->snapshot_list);
 }
 
 static void
-lat_sched_in(struct thread_latency *lat, u64 timestamp)
+lat_sched_in(struct task_atoms *atoms, u64 timestamp)
 {
-	struct lat_snapshot *snapshot;
+	struct work_atom *snapshot;
 
-	if (list_empty(&lat->snapshot_list))
+	if (list_empty(&atoms->snapshot_list))
 		return;
 
-	snapshot = list_entry(lat->snapshot_list.prev, struct lat_snapshot,
+	snapshot = list_entry(atoms->snapshot_list.prev, struct work_atom,
 			      list);
 
 	if (snapshot->state != THREAD_WAKED_UP)
@@ -1004,7 +1004,7 @@ latency_switch_event(struct trace_switch_event *switch_event,
 		     u64 timestamp,
 		     struct thread *thread __used)
 {
-	struct thread_latency *out_lat, *in_lat;
+	struct task_atoms *out_atoms, *in_atoms;
 	struct thread *sched_out, *sched_in;
 	u64 timestamp0;
 	s64 delta;
@@ -1026,24 +1026,24 @@ latency_switch_event(struct trace_switch_event *switch_event,
 	sched_out = threads__findnew(switch_event->prev_pid, &threads, &last_match);
 	sched_in = threads__findnew(switch_event->next_pid, &threads, &last_match);
 
-	in_lat = thread_latency_search(&lat_snapshot_root, sched_in);
-	if (!in_lat) {
-		thread_latency_insert(sched_in);
-		in_lat = thread_latency_search(&lat_snapshot_root, sched_in);
-		if (!in_lat)
+	in_atoms = thread_atom_list_search(&lat_snapshot_root, sched_in);
+	if (!in_atoms) {
+		thread_atom_list_insert(sched_in);
+		in_atoms = thread_atom_list_search(&lat_snapshot_root, sched_in);
+		if (!in_atoms)
 			die("Internal latency tree error");
 	}
 
-	out_lat = thread_latency_search(&lat_snapshot_root, sched_out);
-	if (!out_lat) {
-		thread_latency_insert(sched_out);
-		out_lat = thread_latency_search(&lat_snapshot_root, sched_out);
-		if (!out_lat)
+	out_atoms = thread_atom_list_search(&lat_snapshot_root, sched_out);
+	if (!out_atoms) {
+		thread_atom_list_insert(sched_out);
+		out_atoms = thread_atom_list_search(&lat_snapshot_root, sched_out);
+		if (!out_atoms)
 			die("Internal latency tree error");
 	}
 
-	lat_sched_in(in_lat, timestamp);
-	lat_sched_out(out_lat, switch_event, delta);
+	lat_sched_in(in_atoms, timestamp);
+	lat_sched_out(out_atoms, switch_event, delta);
 }
 
 static void
@@ -1053,8 +1053,8 @@ latency_wakeup_event(struct trace_wakeup_event *wakeup_event,
 		     u64 timestamp,
 		     struct thread *thread __used)
 {
-	struct thread_latency *lat;
-	struct lat_snapshot *snapshot;
+	struct task_atoms *atoms;
+	struct work_atom *snapshot;
 	struct thread *wakee;
 
 	/* Note for later, it may be interesting to observe the failing cases */
@@ -1062,16 +1062,16 @@ latency_wakeup_event(struct trace_wakeup_event *wakeup_event,
 		return;
 
 	wakee = threads__findnew(wakeup_event->pid, &threads, &last_match);
-	lat = thread_latency_search(&lat_snapshot_root, wakee);
-	if (!lat) {
-		thread_latency_insert(wakee);
+	atoms = thread_atom_list_search(&lat_snapshot_root, wakee);
+	if (!atoms) {
+		thread_atom_list_insert(wakee);
 		return;
 	}
 
-	if (list_empty(&lat->snapshot_list))
+	if (list_empty(&atoms->snapshot_list))
 		return;
 
-	snapshot = list_entry(lat->snapshot_list.prev, struct lat_snapshot,
+	snapshot = list_entry(atoms->snapshot_list.prev, struct work_atom,
 			      list);
 
 	if (snapshot->state != THREAD_SLEEPING)
@@ -1090,9 +1090,9 @@ static struct trace_sched_handler lat_ops  = {
 static u64 all_runtime;
 static u64 all_count;
 
-static void output_lat_thread(struct thread_latency *lat)
+static void output_lat_thread(struct task_atoms *atom_list)
 {
-	struct lat_snapshot *shot;
+	struct work_atom *atom;
 	int count = 0;
 	int i;
 	int ret;
@@ -1100,15 +1100,15 @@ static void output_lat_thread(struct thread_latency *lat)
 	u64 total = 0, delta;
 	u64 total_runtime = 0;
 
-	list_for_each_entry(shot, &lat->snapshot_list, list) {
-		total_runtime += shot->runtime;
+	list_for_each_entry(atom, &atom_list->snapshot_list, list) {
+		total_runtime += atom->runtime;
 
-		if (shot->state != THREAD_SCHED_IN)
+		if (atom->state != THREAD_SCHED_IN)
 			continue;
 
 		count++;
 
-		delta = shot->sched_in_time - shot->wake_up_time;
+		delta = atom->sched_in_time - atom->wake_up_time;
 		if (delta > max)
 			max = delta;
 		total += delta;
@@ -1120,7 +1120,7 @@ static void output_lat_thread(struct thread_latency *lat)
 	if (!count)
 		return;
 
-	ret = printf(" %s ", lat->thread->comm);
+	ret = printf(" %s ", atom_list->thread->comm);
 
 	for (i = 0; i < 19 - ret; i++)
 		printf(" ");
@@ -1145,10 +1145,10 @@ static void __cmd_lat(void)
 	next = rb_first(&lat_snapshot_root);
 
 	while (next) {
-		struct thread_latency *lat;
+		struct task_atoms *atom_list;
 
-		lat = rb_entry(next, struct thread_latency, node);
-		output_lat_thread(lat);
+		atom_list = rb_entry(next, struct task_atoms, node);
+		output_lat_thread(atom_list);
 		next = rb_next(next);
 	}
 

commit 3e304147cdb404ce6d1dd0e50cb19f52142bb363
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sat Sep 12 10:08:34 2009 +0200

    perf sched: Output runtime and context switch totals
    
    After:
    
    -----------------------------------------------------------------------------------
     Task              |  Runtime ms | Switches | Average delay ms | Maximum delay ms |
    -----------------------------------------------------------------------------------
     make              |    0.678 ms |       13 | avg:    0.018 ms | max:    0.050 ms |
     gcc               |    0.014 ms |        2 | avg:    0.320 ms | max:    0.627 ms |
     gcc               |    0.000 ms |        2 | avg:    0.185 ms | max:    0.369 ms |
    ...
    -----------------------------------------------------------------------------------
     TOTAL:            |   21.316 ms |       63 |
    ---------------------------------------------
    
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index c382f530d4c6..727cc5b852c2 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1087,6 +1087,9 @@ static struct trace_sched_handler lat_ops  = {
 	.fork_event		= latency_fork_event,
 };
 
+static u64 all_runtime;
+static u64 all_count;
+
 static void output_lat_thread(struct thread_latency *lat)
 {
 	struct lat_snapshot *shot;
@@ -1111,6 +1114,9 @@ static void output_lat_thread(struct thread_latency *lat)
 		total += delta;
 	}
 
+	all_runtime += total_runtime;
+	all_count += count;
+
 	if (!count)
 		return;
 
@@ -1133,7 +1139,7 @@ static void __cmd_lat(void)
 	read_events();
 
 	printf("-----------------------------------------------------------------------------------\n");
-	printf(" Task              |  runtime ms | switches | average delay ms | maximum delay ms |\n");
+	printf(" Task              |  Runtime ms | Switches | Average delay ms | Maximum delay ms |\n");
 	printf("-----------------------------------------------------------------------------------\n");
 
 	next = rb_first(&lat_snapshot_root);
@@ -1147,6 +1153,9 @@ static void __cmd_lat(void)
 	}
 
 	printf("-----------------------------------------------------------------------------------\n");
+	printf(" TOTAL:            |%9.3f ms |%9Ld |\n",
+		(double)all_runtime/1e9, all_count);
+	printf("---------------------------------------------\n");
 }
 
 static struct trace_sched_handler *trace_handler;

commit ea92ed5a8f4e6c638efe7de2efe8a875d580ad3f
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sat Sep 12 10:08:34 2009 +0200

    perf sched: Add runtime stats
    
    Extend the latency tracking structure with scheduling atom
    runtime info - and sum it up during per task display.
    
    (Also clean up a few details.)
    
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index a084c284e198..c382f530d4c6 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -243,8 +243,8 @@ add_sched_event_run(struct task_desc *task, nsec_t timestamp, u64 duration)
 	nr_run_events++;
 }
 
-static unsigned long targetless_wakeups;
-static unsigned long multitarget_wakeups;
+static unsigned long		targetless_wakeups;
+static unsigned long		multitarget_wakeups;
 
 static void
 add_sched_event_wakeup(struct task_desc *task, nsec_t timestamp,
@@ -485,10 +485,10 @@ static void create_tasks(void)
 	}
 }
 
-static nsec_t cpu_usage;
-static nsec_t runavg_cpu_usage;
-static nsec_t parent_cpu_usage;
-static nsec_t runavg_parent_cpu_usage;
+static nsec_t			cpu_usage;
+static nsec_t			runavg_cpu_usage;
+static nsec_t			parent_cpu_usage;
+static nsec_t			runavg_parent_cpu_usage;
 
 static void wait_for_tasks(void)
 {
@@ -858,9 +858,9 @@ replay_fork_event(struct trace_fork_event *fork_event,
 }
 
 static struct trace_sched_handler replay_ops  = {
-	.wakeup_event = replay_wakeup_event,
-	.switch_event = replay_switch_event,
-	.fork_event = replay_fork_event,
+	.wakeup_event		= replay_wakeup_event,
+	.switch_event		= replay_switch_event,
+	.fork_event		= replay_fork_event,
 };
 
 #define TASK_STATE_TO_CHAR_STR "RSDTtZX"
@@ -877,6 +877,7 @@ struct lat_snapshot {
 	enum thread_state	state;
 	u64			wake_up_time;
 	u64			sched_in_time;
+	u64			runtime;
 };
 
 struct thread_latency {
@@ -951,6 +952,7 @@ latency_fork_event(struct trace_fork_event *fork_event __used,
 	/* should insert the newcomer */
 }
 
+__used
 static char sched_out_state(struct trace_switch_event *switch_event)
 {
 	const char *str = TASK_STATE_TO_CHAR_STR;
@@ -960,17 +962,15 @@ static char sched_out_state(struct trace_switch_event *switch_event)
 
 static void
 lat_sched_out(struct thread_latency *lat,
-	     struct trace_switch_event *switch_event)
+	     struct trace_switch_event *switch_event __used, u64 delta)
 {
 	struct lat_snapshot *snapshot;
 
-	if (sched_out_state(switch_event) == 'R')
-		return;
-
 	snapshot = calloc(sizeof(*snapshot), 1);
 	if (!snapshot)
 		die("Non memory");
 
+	snapshot->runtime = delta;
 	list_add_tail(&snapshot->list, &lat->snapshot_list);
 }
 
@@ -997,16 +997,31 @@ lat_sched_in(struct thread_latency *lat, u64 timestamp)
 	snapshot->sched_in_time = timestamp;
 }
 
-
 static void
 latency_switch_event(struct trace_switch_event *switch_event,
 		     struct event *event __used,
-		     int cpu __used,
+		     int cpu,
 		     u64 timestamp,
 		     struct thread *thread __used)
 {
 	struct thread_latency *out_lat, *in_lat;
 	struct thread *sched_out, *sched_in;
+	u64 timestamp0;
+	s64 delta;
+
+	if (cpu >= MAX_CPUS || cpu < 0)
+		return;
+
+	timestamp0 = cpu_last_switched[cpu];
+	cpu_last_switched[cpu] = timestamp;
+	if (timestamp0)
+		delta = timestamp - timestamp0;
+	else
+		delta = 0;
+
+	if (delta < 0)
+		die("hm, delta: %Ld < 0 ?\n", delta);
+
 
 	sched_out = threads__findnew(switch_event->prev_pid, &threads, &last_match);
 	sched_in = threads__findnew(switch_event->next_pid, &threads, &last_match);
@@ -1028,7 +1043,7 @@ latency_switch_event(struct trace_switch_event *switch_event,
 	}
 
 	lat_sched_in(in_lat, timestamp);
-	lat_sched_out(out_lat, switch_event);
+	lat_sched_out(out_lat, switch_event, delta);
 }
 
 static void
@@ -1067,9 +1082,9 @@ latency_wakeup_event(struct trace_wakeup_event *wakeup_event,
 }
 
 static struct trace_sched_handler lat_ops  = {
-	.wakeup_event = latency_wakeup_event,
-	.switch_event = latency_switch_event,
-	.fork_event = latency_fork_event,
+	.wakeup_event		= latency_wakeup_event,
+	.switch_event		= latency_switch_event,
+	.fork_event		= latency_fork_event,
 };
 
 static void output_lat_thread(struct thread_latency *lat)
@@ -1080,8 +1095,11 @@ static void output_lat_thread(struct thread_latency *lat)
 	int ret;
 	u64 max = 0, avg;
 	u64 total = 0, delta;
+	u64 total_runtime = 0;
 
 	list_for_each_entry(shot, &lat->snapshot_list, list) {
+		total_runtime += shot->runtime;
+
 		if (shot->state != THREAD_SCHED_IN)
 			continue;
 
@@ -1104,7 +1122,7 @@ static void output_lat_thread(struct thread_latency *lat)
 	avg = total / count;
 
 	printf("|%9.3f ms |%9d | avg:%9.3f ms | max:%9.3f ms |\n",
-		0.0, count, (double)avg/1e9, (double)max/1e9);
+		(double)total_runtime/1e9, count, (double)avg/1e9, (double)max/1e9);
 }
 
 static void __cmd_lat(void)

commit d9340c1db3f52460a8335eeb127a2728c5bba6ce
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sat Sep 12 10:08:34 2009 +0200

    perf sched: Display time in milliseconds, reorganize output
    
    After:
    
    -----------------------------------------------------------------------------------
     Task              |  runtime ms | switches | average delay ms | maximum delay ms |
    -----------------------------------------------------------------------------------
     migration/0       |    0.000 ms |        1 | avg:    0.047 ms | max:    0.047 ms |
     ksoftirqd/0       |    0.000 ms |        1 | avg:    0.039 ms | max:    0.039 ms |
     migration/1       |    0.000 ms |        3 | avg:    0.013 ms | max:    0.016 ms |
     migration/3       |    0.000 ms |        2 | avg:    0.003 ms | max:    0.004 ms |
     migration/4       |    0.000 ms |        1 | avg:    0.022 ms | max:    0.022 ms |
     distccd           |    0.000 ms |        1 | avg:    0.004 ms | max:    0.004 ms |
     distccd           |    0.000 ms |        1 | avg:    0.014 ms | max:    0.014 ms |
     distccd           |    0.000 ms |        2 | avg:    0.000 ms | max:    0.000 ms |
     distccd           |    0.000 ms |        2 | avg:    0.012 ms | max:    0.019 ms |
     distccd           |    0.000 ms |        1 | avg:    0.002 ms | max:    0.002 ms |
     as                |    0.000 ms |        2 | avg:    0.019 ms | max:    0.019 ms |
     as                |    0.000 ms |        3 | avg:    0.015 ms | max:    0.017 ms |
     as                |    0.000 ms |        1 | avg:    0.009 ms | max:    0.009 ms |
     perf              |    0.000 ms |        1 | avg:    0.001 ms | max:    0.001 ms |
     gcc               |    0.000 ms |        1 | avg:    0.021 ms | max:    0.021 ms |
     run-mozilla.sh    |    0.000 ms |        2 | avg:    0.010 ms | max:    0.017 ms |
     mozilla-plugin-   |    0.000 ms |        1 | avg:    0.006 ms | max:    0.006 ms |
     gcc               |    0.000 ms |        2 | avg:    0.013 ms | max:    0.013 ms |
    -----------------------------------------------------------------------------------
    
    (The runtime ms column is not filled in yet.)
    
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 84699cf036ab..a084c284e198 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -1096,14 +1096,15 @@ static void output_lat_thread(struct thread_latency *lat)
 	if (!count)
 		return;
 
-	ret = printf("%s", lat->thread->comm);
+	ret = printf(" %s ", lat->thread->comm);
 
-	for (i = 0; i < 25 - ret; i++)
+	for (i = 0; i < 19 - ret; i++)
 		printf(" ");
 
 	avg = total / count;
 
-	printf("%5d        %10llu       %10llu      %10llu\n", count, total, avg, max);
+	printf("|%9.3f ms |%9d | avg:%9.3f ms | max:%9.3f ms |\n",
+		0.0, count, (double)avg/1e9, (double)max/1e9);
 }
 
 static void __cmd_lat(void)
@@ -1113,11 +1114,9 @@ static void __cmd_lat(void)
 	setup_pager();
 	read_events();
 
-	printf(" Tasks");
-	printf("                     count");
-	printf("          total");
-	printf("              avg");
-	printf("            max\n\n");
+	printf("-----------------------------------------------------------------------------------\n");
+	printf(" Task              |  runtime ms | switches | average delay ms | maximum delay ms |\n");
+	printf("-----------------------------------------------------------------------------------\n");
 
 	next = rb_first(&lat_snapshot_root);
 
@@ -1128,6 +1127,8 @@ static void __cmd_lat(void)
 		output_lat_thread(lat);
 		next = rb_next(next);
 	}
+
+	printf("-----------------------------------------------------------------------------------\n");
 }
 
 static struct trace_sched_handler *trace_handler;

commit 46f392c97f9fd772426ed3361c5179a0d44b8c3f
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sat Sep 12 10:08:34 2009 +0200

    perf sched: Clean up latency and replay sub-commands
    
    - Separate the latency and the replay commands more cleanly
    
     - Use consistent naming
    
     - Display help page on 'perf sched' outlining comments,
       instead of aborting
    
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 4f9e943181a3..84699cf036ab 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -543,24 +543,7 @@ static void wait_for_tasks(void)
 	}
 }
 
-static int __cmd_sched(void);
-
-static void parse_trace(void)
-{
-	__cmd_sched();
-
-	printf("nr_run_events:        %ld\n", nr_run_events);
-	printf("nr_sleep_events:      %ld\n", nr_sleep_events);
-	printf("nr_wakeup_events:     %ld\n", nr_wakeup_events);
-
-	if (targetless_wakeups)
-		printf("target-less wakeups:  %ld\n", targetless_wakeups);
-	if (multitarget_wakeups)
-		printf("multi-target wakeups: %ld\n", multitarget_wakeups);
-	if (nr_run_events_optimized)
-		printf("run events optimized: %ld\n",
-			nr_run_events_optimized);
-}
+static int read_events(void);
 
 static unsigned long nr_runs;
 static nsec_t sum_runtime;
@@ -637,6 +620,38 @@ static void test_calibrations(void)
 	printf("the sleep test took %Ld nsecs\n", T1-T0);
 }
 
+static void __cmd_replay(void)
+{
+	long nr_iterations = 10, i;
+
+	calibrate_run_measurement_overhead();
+	calibrate_sleep_measurement_overhead();
+
+	test_calibrations();
+
+	read_events();
+
+	printf("nr_run_events:        %ld\n", nr_run_events);
+	printf("nr_sleep_events:      %ld\n", nr_sleep_events);
+	printf("nr_wakeup_events:     %ld\n", nr_wakeup_events);
+
+	if (targetless_wakeups)
+		printf("target-less wakeups:  %ld\n", targetless_wakeups);
+	if (multitarget_wakeups)
+		printf("multi-target wakeups: %ld\n", multitarget_wakeups);
+	if (nr_run_events_optimized)
+		printf("run events optimized: %ld\n",
+			nr_run_events_optimized);
+
+	print_task_traces();
+	add_cross_task_wakeups();
+
+	create_tasks();
+	printf("------------------------------------------------------------\n");
+	for (i = 0; i < nr_iterations; i++)
+		run_one_test();
+}
+
 static int
 process_comm_event(event_t *event, unsigned long offset, unsigned long head)
 {
@@ -1091,10 +1106,13 @@ static void output_lat_thread(struct thread_latency *lat)
 	printf("%5d        %10llu       %10llu      %10llu\n", count, total, avg, max);
 }
 
-static void output_lat_results(void)
+static void __cmd_lat(void)
 {
 	struct rb_node *next;
 
+	setup_pager();
+	read_events();
+
 	printf(" Tasks");
 	printf("                     count");
 	printf("          total");
@@ -1312,7 +1330,7 @@ process_event(event_t *event, unsigned long offset, unsigned long head)
 	return 0;
 }
 
-static int __cmd_sched(void)
+static int read_events(void)
 {
 	int ret, rc = EXIT_FAILURE;
 	unsigned long offset = 0;
@@ -1408,8 +1426,8 @@ static int __cmd_sched(void)
 	return rc;
 }
 
-static const char * const annotate_usage[] = {
-	"perf trace [<options>] <command>",
+static const char * const sched_usage[] = {
+	"perf sched [<options>] <command>",
 	NULL
 };
 
@@ -1427,49 +1445,30 @@ static const struct option options[] = {
 
 int cmd_sched(int argc, const char **argv, const char *prefix __used)
 {
-	long nr_iterations = 10, i;
-
 	symbol__init();
 	page_size = getpagesize();
 
-	argc = parse_options(argc, argv, options, annotate_usage, 0);
+	argc = parse_options(argc, argv, options, sched_usage, 0);
 	if (argc) {
 		/*
 		 * Special case: if there's an argument left then assume tha
 		 * it's a symbol filter:
 		 */
 		if (argc > 1)
-			usage_with_options(annotate_usage, options);
+			usage_with_options(sched_usage, options);
 	}
 
-//	setup_pager();
-
 	if (replay_mode)
 		trace_handler = &replay_ops;
 	else if (lat_mode)
 		trace_handler = &lat_ops;
-	else /* We may need a default subcommand (perf trace?) */
-		die("Please select a sub command (-r)\n");
-
-	if (replay_mode) {
-		calibrate_run_measurement_overhead();
-		calibrate_sleep_measurement_overhead();
-
-		test_calibrations();
-
-		parse_trace();
-		print_task_traces();
-		add_cross_task_wakeups();
-
-		create_tasks();
-		printf("------------------------------------------------------------\n");
-		for (i = 0; i < nr_iterations; i++)
-			run_one_test();
-	} else if (lat_mode) {
-		setup_pager();
-		__cmd_sched();
-		output_lat_results();
-	}
+	else
+		usage_with_options(sched_usage, options);
+
+	if (replay_mode)
+		__cmd_replay();
+	else if (lat_mode)
+		__cmd_lat();
 
 	return 0;
 }

commit cdce9d738b91e1ec686e4ef6ca46d46e2891e208
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Sep 12 08:06:14 2009 +0200

    perf sched: Add sched latency profiling
    
    Add the -l --latency option that reports statistics about the
    scheduler latencies.
    
    For now, the latencies are measured in the following sequence
    scope:
    
    - task A is sleeping (D or S state)
    - task B wakes up A
             ^
             |
             |
    
       latency timeframe
    
             |
             |
             v
    - task A is scheduled in
    
    Start by recording every scheduler events:
    
            perf record -e sched:*
    
    and then fetch the results:
    
            perf sched -l
    
     Tasks                     count          total              avg            max
    
    migration/0                  2             39849            19924           28826
    ksoftirqd/0                  7            756383           108054          373014
    migration/1                  5             45391             9078           10452
    ksoftirqd/1                  2            399055           199527          359130
    events/0                     8           4780110           597513         4500250
    events/1                     9           6353057           705895         2986012
    kblockd/0                   42          37805097           900121         5077684
    
    The snapshot are in nanoseconds.
    
    - Count: number of snapshots taken for the given task
    - Total: total latencies in nanosec
    - Avg  : average of latency between wake up and sched in
    - Max  : max snapshot latency
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 756fe62eb046..4f9e943181a3 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -31,6 +31,7 @@ static struct perf_header	*header;
 static u64			sample_type;
 
 static int			replay_mode;
+static int			lat_mode;
 
 
 /*
@@ -847,6 +848,269 @@ static struct trace_sched_handler replay_ops  = {
 	.fork_event = replay_fork_event,
 };
 
+#define TASK_STATE_TO_CHAR_STR "RSDTtZX"
+
+enum thread_state {
+	THREAD_SLEEPING,
+	THREAD_WAKED_UP,
+	THREAD_SCHED_IN,
+	THREAD_IGNORE
+};
+
+struct lat_snapshot {
+	struct list_head	list;
+	enum thread_state	state;
+	u64			wake_up_time;
+	u64			sched_in_time;
+};
+
+struct thread_latency {
+	struct list_head	snapshot_list;
+	struct thread		*thread;
+	struct rb_node		node;
+};
+
+static struct rb_root lat_snapshot_root;
+
+static struct thread_latency *
+thread_latency_search(struct rb_root *root, struct thread *thread)
+{
+	struct rb_node *node = root->rb_node;
+
+	while (node) {
+		struct thread_latency *lat;
+
+		lat = container_of(node, struct thread_latency, node);
+		if (thread->pid < lat->thread->pid)
+			node = node->rb_left;
+		else if (thread->pid > lat->thread->pid)
+			node = node->rb_right;
+		else {
+			return lat;
+		}
+	}
+	return NULL;
+}
+
+static void
+__thread_latency_insert(struct rb_root *root, struct thread_latency *data)
+{
+	struct rb_node **new = &(root->rb_node), *parent = NULL;
+
+	while (*new) {
+		struct thread_latency *this;
+
+		this = container_of(*new, struct thread_latency, node);
+		parent = *new;
+		if (data->thread->pid < this->thread->pid)
+			new = &((*new)->rb_left);
+		else if (data->thread->pid > this->thread->pid)
+			new = &((*new)->rb_right);
+		else
+			die("Double thread insertion\n");
+	}
+
+	rb_link_node(&data->node, parent, new);
+	rb_insert_color(&data->node, root);
+}
+
+static void thread_latency_insert(struct thread *thread)
+{
+	struct thread_latency *lat;
+	lat = calloc(sizeof(*lat), 1);
+	if (!lat)
+		die("No memory");
+
+	lat->thread = thread;
+	INIT_LIST_HEAD(&lat->snapshot_list);
+	__thread_latency_insert(&lat_snapshot_root, lat);
+}
+
+static void
+latency_fork_event(struct trace_fork_event *fork_event __used,
+		   struct event *event __used,
+		   int cpu __used,
+		   u64 timestamp __used,
+		   struct thread *thread __used)
+{
+	/* should insert the newcomer */
+}
+
+static char sched_out_state(struct trace_switch_event *switch_event)
+{
+	const char *str = TASK_STATE_TO_CHAR_STR;
+
+	return str[switch_event->prev_state];
+}
+
+static void
+lat_sched_out(struct thread_latency *lat,
+	     struct trace_switch_event *switch_event)
+{
+	struct lat_snapshot *snapshot;
+
+	if (sched_out_state(switch_event) == 'R')
+		return;
+
+	snapshot = calloc(sizeof(*snapshot), 1);
+	if (!snapshot)
+		die("Non memory");
+
+	list_add_tail(&snapshot->list, &lat->snapshot_list);
+}
+
+static void
+lat_sched_in(struct thread_latency *lat, u64 timestamp)
+{
+	struct lat_snapshot *snapshot;
+
+	if (list_empty(&lat->snapshot_list))
+		return;
+
+	snapshot = list_entry(lat->snapshot_list.prev, struct lat_snapshot,
+			      list);
+
+	if (snapshot->state != THREAD_WAKED_UP)
+		return;
+
+	if (timestamp < snapshot->wake_up_time) {
+		snapshot->state = THREAD_IGNORE;
+		return;
+	}
+
+	snapshot->state = THREAD_SCHED_IN;
+	snapshot->sched_in_time = timestamp;
+}
+
+
+static void
+latency_switch_event(struct trace_switch_event *switch_event,
+		     struct event *event __used,
+		     int cpu __used,
+		     u64 timestamp,
+		     struct thread *thread __used)
+{
+	struct thread_latency *out_lat, *in_lat;
+	struct thread *sched_out, *sched_in;
+
+	sched_out = threads__findnew(switch_event->prev_pid, &threads, &last_match);
+	sched_in = threads__findnew(switch_event->next_pid, &threads, &last_match);
+
+	in_lat = thread_latency_search(&lat_snapshot_root, sched_in);
+	if (!in_lat) {
+		thread_latency_insert(sched_in);
+		in_lat = thread_latency_search(&lat_snapshot_root, sched_in);
+		if (!in_lat)
+			die("Internal latency tree error");
+	}
+
+	out_lat = thread_latency_search(&lat_snapshot_root, sched_out);
+	if (!out_lat) {
+		thread_latency_insert(sched_out);
+		out_lat = thread_latency_search(&lat_snapshot_root, sched_out);
+		if (!out_lat)
+			die("Internal latency tree error");
+	}
+
+	lat_sched_in(in_lat, timestamp);
+	lat_sched_out(out_lat, switch_event);
+}
+
+static void
+latency_wakeup_event(struct trace_wakeup_event *wakeup_event,
+		     struct event *event __used,
+		     int cpu __used,
+		     u64 timestamp,
+		     struct thread *thread __used)
+{
+	struct thread_latency *lat;
+	struct lat_snapshot *snapshot;
+	struct thread *wakee;
+
+	/* Note for later, it may be interesting to observe the failing cases */
+	if (!wakeup_event->success)
+		return;
+
+	wakee = threads__findnew(wakeup_event->pid, &threads, &last_match);
+	lat = thread_latency_search(&lat_snapshot_root, wakee);
+	if (!lat) {
+		thread_latency_insert(wakee);
+		return;
+	}
+
+	if (list_empty(&lat->snapshot_list))
+		return;
+
+	snapshot = list_entry(lat->snapshot_list.prev, struct lat_snapshot,
+			      list);
+
+	if (snapshot->state != THREAD_SLEEPING)
+		return;
+
+	snapshot->state = THREAD_WAKED_UP;
+	snapshot->wake_up_time = timestamp;
+}
+
+static struct trace_sched_handler lat_ops  = {
+	.wakeup_event = latency_wakeup_event,
+	.switch_event = latency_switch_event,
+	.fork_event = latency_fork_event,
+};
+
+static void output_lat_thread(struct thread_latency *lat)
+{
+	struct lat_snapshot *shot;
+	int count = 0;
+	int i;
+	int ret;
+	u64 max = 0, avg;
+	u64 total = 0, delta;
+
+	list_for_each_entry(shot, &lat->snapshot_list, list) {
+		if (shot->state != THREAD_SCHED_IN)
+			continue;
+
+		count++;
+
+		delta = shot->sched_in_time - shot->wake_up_time;
+		if (delta > max)
+			max = delta;
+		total += delta;
+	}
+
+	if (!count)
+		return;
+
+	ret = printf("%s", lat->thread->comm);
+
+	for (i = 0; i < 25 - ret; i++)
+		printf(" ");
+
+	avg = total / count;
+
+	printf("%5d        %10llu       %10llu      %10llu\n", count, total, avg, max);
+}
+
+static void output_lat_results(void)
+{
+	struct rb_node *next;
+
+	printf(" Tasks");
+	printf("                     count");
+	printf("          total");
+	printf("              avg");
+	printf("            max\n\n");
+
+	next = rb_first(&lat_snapshot_root);
+
+	while (next) {
+		struct thread_latency *lat;
+
+		lat = rb_entry(next, struct thread_latency, node);
+		output_lat_thread(lat);
+		next = rb_next(next);
+	}
+}
 
 static struct trace_sched_handler *trace_handler;
 
@@ -1154,6 +1418,8 @@ static const struct option options[] = {
 		    "dump raw trace in ASCII"),
 	OPT_BOOLEAN('r', "replay", &replay_mode,
 		    "replay sched behaviour from traces"),
+	OPT_BOOLEAN('l', "latency", &lat_mode,
+		    "measure various latencies"),
 	OPT_BOOLEAN('v', "verbose", &verbose,
 		    "be more verbose (show symbol address, etc)"),
 	OPT_END()
@@ -1180,22 +1446,30 @@ int cmd_sched(int argc, const char **argv, const char *prefix __used)
 
 	if (replay_mode)
 		trace_handler = &replay_ops;
-	else /* We may need a default subcommand */
+	else if (lat_mode)
+		trace_handler = &lat_ops;
+	else /* We may need a default subcommand (perf trace?) */
 		die("Please select a sub command (-r)\n");
 
-	calibrate_run_measurement_overhead();
-	calibrate_sleep_measurement_overhead();
+	if (replay_mode) {
+		calibrate_run_measurement_overhead();
+		calibrate_sleep_measurement_overhead();
 
-	test_calibrations();
+		test_calibrations();
 
-	parse_trace();
-	print_task_traces();
-	add_cross_task_wakeups();
+		parse_trace();
+		print_task_traces();
+		add_cross_task_wakeups();
 
-	create_tasks();
-	printf("------------------------------------------------------------\n");
-	for (i = 0; i < nr_iterations; i++)
-		run_one_test();
+		create_tasks();
+		printf("------------------------------------------------------------\n");
+		for (i = 0; i < nr_iterations; i++)
+			run_one_test();
+	} else if (lat_mode) {
+		setup_pager();
+		__cmd_sched();
+		output_lat_results();
+	}
 
 	return 0;
 }

commit 419ab0d6a959f41ec7fde807fe311aaafb05c3be
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Sep 12 03:59:01 2009 +0200

    perf sched: Make it easier to plug in new sub profilers
    
    Create a sched event structure of handlers in which various
    sched events reader can plug their own callbacks.
    
    This makes easier the addition of new perf sched sub commands.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 0215936696ed..756fe62eb046 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -14,6 +14,9 @@
 #include "util/trace-event.h"
 #include <sys/types.h>
 
+
+#define MAX_CPUS 4096
+
 static char			const *input_name = "perf.data";
 static int			input;
 static unsigned long		page_size;
@@ -27,6 +30,8 @@ static struct thread		*last_match;
 static struct perf_header	*header;
 static u64			sample_type;
 
+static int			replay_mode;
+
 
 /*
  * Scheduler benchmarks
@@ -677,6 +682,27 @@ do {								\
 	FILL_FIELD(ptr, common_tgid, event, data);		\
 } while (0)
 
+
+
+struct trace_switch_event {
+	u32 size;
+
+	u16 common_type;
+	u8 common_flags;
+	u8 common_preempt_count;
+	u32 common_pid;
+	u32 common_tgid;
+
+	char prev_comm[16];
+	u32 prev_pid;
+	u32 prev_prio;
+	u64 prev_state;
+	char next_comm[16];
+	u32 next_pid;
+	u32 next_prio;
+};
+
+
 struct trace_wakeup_event {
 	u32 size;
 
@@ -694,78 +720,79 @@ struct trace_wakeup_event {
 	u32 cpu;
 };
 
-static void
-process_sched_wakeup_event(struct raw_event_sample *raw, struct event *event,
-		  int cpu __used, u64 timestamp __used, struct thread *thread __used)
-{
-	struct task_desc *waker, *wakee;
-	struct trace_wakeup_event wakeup_event;
+struct trace_fork_event {
+	u32 size;
 
-	FILL_COMMON_FIELDS(wakeup_event, event, raw->data);
+	u16 common_type;
+	u8 common_flags;
+	u8 common_preempt_count;
+	u32 common_pid;
+	u32 common_tgid;
+
+	char parent_comm[16];
+	u32 parent_pid;
+	char child_comm[16];
+	u32 child_pid;
+};
+
+struct trace_sched_handler {
+	void (*switch_event)(struct trace_switch_event *,
+			     struct event *,
+			     int cpu,
+			     u64 timestamp,
+			     struct thread *thread);
+
+	void (*wakeup_event)(struct trace_wakeup_event *,
+			     struct event *,
+			     int cpu,
+			     u64 timestamp,
+			     struct thread *thread);
+
+	void (*fork_event)(struct trace_fork_event *,
+			   struct event *,
+			   int cpu,
+			   u64 timestamp,
+			   struct thread *thread);
+};
 
-	FILL_ARRAY(wakeup_event, comm, event, raw->data);
-	FILL_FIELD(wakeup_event, pid, event, raw->data);
-	FILL_FIELD(wakeup_event, prio, event, raw->data);
-	FILL_FIELD(wakeup_event, success, event, raw->data);
-	FILL_FIELD(wakeup_event, cpu, event, raw->data);
 
+static void
+replay_wakeup_event(struct trace_wakeup_event *wakeup_event,
+		    struct event *event,
+		    int cpu __used,
+		    u64 timestamp __used,
+		    struct thread *thread __used)
+{
+	struct task_desc *waker, *wakee;
 
 	if (verbose) {
 		printf("sched_wakeup event %p\n", event);
 
 		printf(" ... pid %d woke up %s/%d\n",
-			wakeup_event.common_pid,
-			wakeup_event.comm,
-			wakeup_event.pid);
+			wakeup_event->common_pid,
+			wakeup_event->comm,
+			wakeup_event->pid);
 	}
 
-	waker = register_pid(wakeup_event.common_pid, "<unknown>");
-	wakee = register_pid(wakeup_event.pid, wakeup_event.comm);
+	waker = register_pid(wakeup_event->common_pid, "<unknown>");
+	wakee = register_pid(wakeup_event->pid, wakeup_event->comm);
 
 	add_sched_event_wakeup(waker, timestamp, wakee);
 }
 
-struct trace_switch_event {
-	u32 size;
-
-	u16 common_type;
-	u8 common_flags;
-	u8 common_preempt_count;
-	u32 common_pid;
-	u32 common_tgid;
-
-	char prev_comm[16];
-	u32 prev_pid;
-	u32 prev_prio;
-	u64 prev_state;
-	char next_comm[16];
-	u32 next_pid;
-	u32 next_prio;
-};
-
-#define MAX_CPUS 4096
-
-unsigned long cpu_last_switched[MAX_CPUS];
+static unsigned long cpu_last_switched[MAX_CPUS];
 
 static void
-process_sched_switch_event(struct raw_event_sample *raw, struct event *event,
-		  int cpu __used, u64 timestamp __used, struct thread *thread __used)
+replay_switch_event(struct trace_switch_event *switch_event,
+		    struct event *event,
+		    int cpu,
+		    u64 timestamp,
+		    struct thread *thread __used)
 {
-	struct trace_switch_event switch_event;
 	struct task_desc *prev, *next;
 	u64 timestamp0;
 	s64 delta;
 
-	FILL_COMMON_FIELDS(switch_event, event, raw->data);
-
-	FILL_ARRAY(switch_event, prev_comm, event, raw->data);
-	FILL_FIELD(switch_event, prev_pid, event, raw->data);
-	FILL_FIELD(switch_event, prev_prio, event, raw->data);
-	FILL_FIELD(switch_event, prev_state, event, raw->data);
-	FILL_ARRAY(switch_event, next_comm, event, raw->data);
-	FILL_FIELD(switch_event, next_pid, event, raw->data);
-	FILL_FIELD(switch_event, next_prio, event, raw->data);
-
 	if (verbose)
 		printf("sched_switch event %p\n", event);
 
@@ -783,38 +810,94 @@ process_sched_switch_event(struct raw_event_sample *raw, struct event *event,
 
 	if (verbose) {
 		printf(" ... switch from %s/%d to %s/%d [ran %Ld nsecs]\n",
-			switch_event.prev_comm, switch_event.prev_pid,
-			switch_event.next_comm, switch_event.next_pid,
+			switch_event->prev_comm, switch_event->prev_pid,
+			switch_event->next_comm, switch_event->next_pid,
 			delta);
 	}
 
-	prev = register_pid(switch_event.prev_pid, switch_event.prev_comm);
-	next = register_pid(switch_event.next_pid, switch_event.next_comm);
+	prev = register_pid(switch_event->prev_pid, switch_event->prev_comm);
+	next = register_pid(switch_event->next_pid, switch_event->next_comm);
 
 	cpu_last_switched[cpu] = timestamp;
 
 	add_sched_event_run(prev, timestamp, delta);
-	add_sched_event_sleep(prev, timestamp, switch_event.prev_state);
+	add_sched_event_sleep(prev, timestamp, switch_event->prev_state);
 }
 
-struct trace_fork_event {
-	u32 size;
 
-	u16 common_type;
-	u8 common_flags;
-	u8 common_preempt_count;
-	u32 common_pid;
-	u32 common_tgid;
+static void
+replay_fork_event(struct trace_fork_event *fork_event,
+		  struct event *event,
+		  int cpu __used,
+		  u64 timestamp __used,
+		  struct thread *thread __used)
+{
+	if (verbose) {
+		printf("sched_fork event %p\n", event);
+		printf("... parent: %s/%d\n", fork_event->parent_comm, fork_event->parent_pid);
+		printf("...  child: %s/%d\n", fork_event->child_comm, fork_event->child_pid);
+	}
+	register_pid(fork_event->parent_pid, fork_event->parent_comm);
+	register_pid(fork_event->child_pid, fork_event->child_comm);
+}
 
-	char parent_comm[16];
-	u32 parent_pid;
-	char child_comm[16];
-	u32 child_pid;
+static struct trace_sched_handler replay_ops  = {
+	.wakeup_event = replay_wakeup_event,
+	.switch_event = replay_switch_event,
+	.fork_event = replay_fork_event,
 };
 
+
+static struct trace_sched_handler *trace_handler;
+
 static void
-process_sched_fork_event(struct raw_event_sample *raw, struct event *event,
-		  int cpu __used, u64 timestamp __used, struct thread *thread __used)
+process_sched_wakeup_event(struct raw_event_sample *raw,
+			   struct event *event,
+			   int cpu __used,
+			   u64 timestamp __used,
+			   struct thread *thread __used)
+{
+	struct trace_wakeup_event wakeup_event;
+
+	FILL_COMMON_FIELDS(wakeup_event, event, raw->data);
+
+	FILL_ARRAY(wakeup_event, comm, event, raw->data);
+	FILL_FIELD(wakeup_event, pid, event, raw->data);
+	FILL_FIELD(wakeup_event, prio, event, raw->data);
+	FILL_FIELD(wakeup_event, success, event, raw->data);
+	FILL_FIELD(wakeup_event, cpu, event, raw->data);
+
+	trace_handler->wakeup_event(&wakeup_event, event, cpu, timestamp, thread);
+}
+
+static void
+process_sched_switch_event(struct raw_event_sample *raw,
+			   struct event *event,
+			   int cpu __used,
+			   u64 timestamp __used,
+			   struct thread *thread __used)
+{
+	struct trace_switch_event switch_event;
+
+	FILL_COMMON_FIELDS(switch_event, event, raw->data);
+
+	FILL_ARRAY(switch_event, prev_comm, event, raw->data);
+	FILL_FIELD(switch_event, prev_pid, event, raw->data);
+	FILL_FIELD(switch_event, prev_prio, event, raw->data);
+	FILL_FIELD(switch_event, prev_state, event, raw->data);
+	FILL_ARRAY(switch_event, next_comm, event, raw->data);
+	FILL_FIELD(switch_event, next_pid, event, raw->data);
+	FILL_FIELD(switch_event, next_prio, event, raw->data);
+
+	trace_handler->switch_event(&switch_event, event, cpu, timestamp, thread);
+}
+
+static void
+process_sched_fork_event(struct raw_event_sample *raw,
+			 struct event *event,
+			 int cpu __used,
+			 u64 timestamp __used,
+			 struct thread *thread __used)
 {
 	struct trace_fork_event fork_event;
 
@@ -825,17 +908,14 @@ process_sched_fork_event(struct raw_event_sample *raw, struct event *event,
 	FILL_ARRAY(fork_event, child_comm, event, raw->data);
 	FILL_FIELD(fork_event, child_pid, event, raw->data);
 
-	if (verbose) {
-		printf("sched_fork event %p\n", event);
-		printf("... parent: %s/%d\n", fork_event.parent_comm, fork_event.parent_pid);
-		printf("...  child: %s/%d\n", fork_event.child_comm, fork_event.child_pid);
-	}
-	register_pid(fork_event.parent_pid, fork_event.parent_comm);
-	register_pid(fork_event.child_pid, fork_event.child_comm);
+	trace_handler->fork_event(&fork_event, event, cpu, timestamp, thread);
 }
 
-static void process_sched_exit_event(struct event *event,
-		  int cpu __used, u64 timestamp __used, struct thread *thread __used)
+static void
+process_sched_exit_event(struct event *event,
+			 int cpu __used,
+			 u64 timestamp __used,
+			 struct thread *thread __used)
 {
 	if (verbose)
 		printf("sched_exit event %p\n", event);
@@ -1072,6 +1152,8 @@ static const char * const annotate_usage[] = {
 static const struct option options[] = {
 	OPT_BOOLEAN('D', "dump-raw-trace", &dump_trace,
 		    "dump raw trace in ASCII"),
+	OPT_BOOLEAN('r', "replay", &replay_mode,
+		    "replay sched behaviour from traces"),
 	OPT_BOOLEAN('v', "verbose", &verbose,
 		    "be more verbose (show symbol address, etc)"),
 	OPT_END()
@@ -1096,6 +1178,11 @@ int cmd_sched(int argc, const char **argv, const char *prefix __used)
 
 //	setup_pager();
 
+	if (replay_mode)
+		trace_handler = &replay_ops;
+	else /* We may need a default subcommand */
+		die("Please select a sub command (-r)\n");
+
 	calibrate_run_measurement_overhead();
 	calibrate_sleep_measurement_overhead();
 

commit 46538818023e8ea94f656acfa1e38297e2df20e2
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Sep 12 02:43:45 2009 +0200

    perf sched: Fix bad event alignment
    
    perf sched raises the following error when it meets a sched
    switch event:
    
    perf: builtin-sched.c:286: register_pid: Assertion `!(pid >= 65536)' failed.
    Abandon
    
    Currently in x86-64, the sched switch events have a hole in the
    middle of the structure:
    
            u16 common_type;
            u8 common_flags;
            u8 common_preempt_count;
            u32 common_pid;
            u32 common_tgid;
    
            char prev_comm[16];
            u32 prev_pid;
            u32 prev_prio;
                            <--- there
            u64 prev_state;
            char next_comm[16];
            u32 next_pid;
            u32 next_prio;
    
    Gcc inserts a 4 bytes hole there for prev_state to be u64
    aligned. And the events are exported to userspace with this
    hole.
    
    But in userspace, from perf sched, we fetch it using a
    structure that has a new field in the beginning: u32 size. This
    is because our trace is exported with its size as a field. But
    now that we have this new field, the hole in the middle
    disappears because it makes prev_state becoming well aligned.
    
    And since we are using a pointer to the raw trace using this
    struct, instead of reading prev_state, we are reading the hole.
    
    We could fix it by keeping the size seperate from the struct
    but actually there a lot of other potential problems: some
    fields may be saved as long in a 64 bits system and later read
    as long in a 32 bits system. Also this direct cast doesn't care
    about the endianness differences between the host traced
    machine and the machine in which we do the post processing.
    
    So instead of using such dangerous direct casts, fetch the
    values using the trace parsing API that already takes care of
    all these problems.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index de93a2604528..0215936696ed 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -653,6 +653,30 @@ process_comm_event(event_t *event, unsigned long offset, unsigned long head)
 	return 0;
 }
 
+
+struct raw_event_sample {
+	u32 size;
+	char data[0];
+};
+
+#define FILL_FIELD(ptr, field, event, data)	\
+	ptr.field = (typeof(ptr.field)) raw_field_value(event, #field, data)
+
+#define FILL_ARRAY(ptr, array, event, data)			\
+do {								\
+	void *__array = raw_field_ptr(event, #array, data);	\
+	memcpy(ptr.array, __array, sizeof(ptr.array));	\
+} while(0)
+
+#define FILL_COMMON_FIELDS(ptr, event, data)			\
+do {								\
+	FILL_FIELD(ptr, common_type, event, data);		\
+	FILL_FIELD(ptr, common_flags, event, data);		\
+	FILL_FIELD(ptr, common_preempt_count, event, data);	\
+	FILL_FIELD(ptr, common_pid, event, data);		\
+	FILL_FIELD(ptr, common_tgid, event, data);		\
+} while (0)
+
 struct trace_wakeup_event {
 	u32 size;
 
@@ -671,22 +695,32 @@ struct trace_wakeup_event {
 };
 
 static void
-process_sched_wakeup_event(struct trace_wakeup_event *wakeup_event, struct event *event,
+process_sched_wakeup_event(struct raw_event_sample *raw, struct event *event,
 		  int cpu __used, u64 timestamp __used, struct thread *thread __used)
 {
 	struct task_desc *waker, *wakee;
+	struct trace_wakeup_event wakeup_event;
+
+	FILL_COMMON_FIELDS(wakeup_event, event, raw->data);
+
+	FILL_ARRAY(wakeup_event, comm, event, raw->data);
+	FILL_FIELD(wakeup_event, pid, event, raw->data);
+	FILL_FIELD(wakeup_event, prio, event, raw->data);
+	FILL_FIELD(wakeup_event, success, event, raw->data);
+	FILL_FIELD(wakeup_event, cpu, event, raw->data);
+
 
 	if (verbose) {
 		printf("sched_wakeup event %p\n", event);
 
 		printf(" ... pid %d woke up %s/%d\n",
-			wakeup_event->common_pid,
-			wakeup_event->comm,
-			wakeup_event->pid);
+			wakeup_event.common_pid,
+			wakeup_event.comm,
+			wakeup_event.pid);
 	}
 
-	waker = register_pid(wakeup_event->common_pid, "<unknown>");
-	wakee = register_pid(wakeup_event->pid, wakeup_event->comm);
+	waker = register_pid(wakeup_event.common_pid, "<unknown>");
+	wakee = register_pid(wakeup_event.pid, wakeup_event.comm);
 
 	add_sched_event_wakeup(waker, timestamp, wakee);
 }
@@ -714,13 +748,24 @@ struct trace_switch_event {
 unsigned long cpu_last_switched[MAX_CPUS];
 
 static void
-process_sched_switch_event(struct trace_switch_event *switch_event, struct event *event,
+process_sched_switch_event(struct raw_event_sample *raw, struct event *event,
 		  int cpu __used, u64 timestamp __used, struct thread *thread __used)
 {
+	struct trace_switch_event switch_event;
 	struct task_desc *prev, *next;
 	u64 timestamp0;
 	s64 delta;
 
+	FILL_COMMON_FIELDS(switch_event, event, raw->data);
+
+	FILL_ARRAY(switch_event, prev_comm, event, raw->data);
+	FILL_FIELD(switch_event, prev_pid, event, raw->data);
+	FILL_FIELD(switch_event, prev_prio, event, raw->data);
+	FILL_FIELD(switch_event, prev_state, event, raw->data);
+	FILL_ARRAY(switch_event, next_comm, event, raw->data);
+	FILL_FIELD(switch_event, next_pid, event, raw->data);
+	FILL_FIELD(switch_event, next_prio, event, raw->data);
+
 	if (verbose)
 		printf("sched_switch event %p\n", event);
 
@@ -738,18 +783,18 @@ process_sched_switch_event(struct trace_switch_event *switch_event, struct event
 
 	if (verbose) {
 		printf(" ... switch from %s/%d to %s/%d [ran %Ld nsecs]\n",
-			switch_event->prev_comm, switch_event->prev_pid,
-			switch_event->next_comm, switch_event->next_pid,
+			switch_event.prev_comm, switch_event.prev_pid,
+			switch_event.next_comm, switch_event.next_pid,
 			delta);
 	}
 
-	prev = register_pid(switch_event->prev_pid, switch_event->prev_comm);
-	next = register_pid(switch_event->next_pid, switch_event->next_comm);
+	prev = register_pid(switch_event.prev_pid, switch_event.prev_comm);
+	next = register_pid(switch_event.next_pid, switch_event.next_comm);
 
 	cpu_last_switched[cpu] = timestamp;
 
 	add_sched_event_run(prev, timestamp, delta);
-	add_sched_event_sleep(prev, timestamp, switch_event->prev_state);
+	add_sched_event_sleep(prev, timestamp, switch_event.prev_state);
 }
 
 struct trace_fork_event {
@@ -768,16 +813,25 @@ struct trace_fork_event {
 };
 
 static void
-process_sched_fork_event(struct trace_fork_event *fork_event, struct event *event,
+process_sched_fork_event(struct raw_event_sample *raw, struct event *event,
 		  int cpu __used, u64 timestamp __used, struct thread *thread __used)
 {
+	struct trace_fork_event fork_event;
+
+	FILL_COMMON_FIELDS(fork_event, event, raw->data);
+
+	FILL_ARRAY(fork_event, parent_comm, event, raw->data);
+	FILL_FIELD(fork_event, parent_pid, event, raw->data);
+	FILL_ARRAY(fork_event, child_comm, event, raw->data);
+	FILL_FIELD(fork_event, child_pid, event, raw->data);
+
 	if (verbose) {
 		printf("sched_fork event %p\n", event);
-		printf("... parent: %s/%d\n", fork_event->parent_comm, fork_event->parent_pid);
-		printf("...  child: %s/%d\n", fork_event->child_comm, fork_event->child_pid);
+		printf("... parent: %s/%d\n", fork_event.parent_comm, fork_event.parent_pid);
+		printf("...  child: %s/%d\n", fork_event.child_comm, fork_event.child_pid);
 	}
-	register_pid(fork_event->parent_pid, fork_event->parent_comm);
-	register_pid(fork_event->child_pid, fork_event->child_comm);
+	register_pid(fork_event.parent_pid, fork_event.parent_comm);
+	register_pid(fork_event.child_pid, fork_event.child_comm);
 }
 
 static void process_sched_exit_event(struct event *event,
@@ -791,10 +845,7 @@ static void
 process_raw_event(event_t *raw_event __used, void *more_data,
 		  int cpu, u64 timestamp, struct thread *thread)
 {
-	struct {
-		u32 size;
-		char data[0];
-	} *raw = more_data;
+	struct raw_event_sample *raw = more_data;
 	struct event *event;
 	int type;
 
@@ -802,13 +853,13 @@ process_raw_event(event_t *raw_event __used, void *more_data,
 	event = trace_find_event(type);
 
 	if (!strcmp(event->name, "sched_switch"))
-		process_sched_switch_event(more_data, event, cpu, timestamp, thread);
+		process_sched_switch_event(raw, event, cpu, timestamp, thread);
 	if (!strcmp(event->name, "sched_wakeup"))
-		process_sched_wakeup_event(more_data, event, cpu, timestamp, thread);
+		process_sched_wakeup_event(raw, event, cpu, timestamp, thread);
 	if (!strcmp(event->name, "sched_wakeup_new"))
-		process_sched_wakeup_event(more_data, event, cpu, timestamp, thread);
+		process_sched_wakeup_event(raw, event, cpu, timestamp, thread);
 	if (!strcmp(event->name, "sched_process_fork"))
-		process_sched_fork_event(more_data, event, cpu, timestamp, thread);
+		process_sched_fork_event(raw, event, cpu, timestamp, thread);
 	if (!strcmp(event->name, "sched_process_exit"))
 		process_sched_exit_event(event, cpu, timestamp, thread);
 }

commit ad236fd23b6d6372dcacd549983cce051d2ccff6
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Sep 11 12:12:54 2009 +0200

    perf sched: Tighten up the code
    
    Various small cleanups - removal of debug printks and dead
    functions, etc.
    
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 6ec4f51d536b..de93a2604528 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -61,29 +61,6 @@ static u64			sample_type;
 
 typedef unsigned long long nsec_t;
 
-#define printk(x...)		do { printf(x); fflush(stdout); } while (0)
-
-nsec_t prev_printk;
-
-#define __dprintk(x,y...) do {						 \
-	nsec_t __now = get_nsecs(), __delta = __now - prev_printk;	 \
-									 \
-	prev_printk = __now;						 \
-									 \
-	printf("%.3f [%Ld] [%.3f]: " x, (double)__now/1e6, __now, (double)__delta/1e6, y);\
-} while (0)
-
-#if !DEBUG
-# define dprintk(x...)	do { } while (0)
-#else
-# define dprintk(x...)	__dprintk(x)
-#endif
-
-#define __DP()		__dprintk("parent: line %d\n", __LINE__)
-#define DP()		dprintk("parent: line %d\n", __LINE__)
-#define D()		dprintk("task %ld: line %d\n", this_task->nr, __LINE__)
-
-
 static nsec_t run_measurement_overhead;
 static nsec_t sleep_measurement_overhead;
 
@@ -129,7 +106,7 @@ static void calibrate_run_measurement_overhead(void)
 	}
 	run_measurement_overhead = min_delta;
 
-	printk("run measurement overhead: %Ld nsecs\n", min_delta);
+	printf("run measurement overhead: %Ld nsecs\n", min_delta);
 }
 
 static void calibrate_sleep_measurement_overhead(void)
@@ -147,7 +124,7 @@ static void calibrate_sleep_measurement_overhead(void)
 	min_delta -= 10000;
 	sleep_measurement_overhead = min_delta;
 
-	printk("sleep measurement overhead: %Ld nsecs\n", min_delta);
+	printf("sleep measurement overhead: %Ld nsecs\n", min_delta);
 }
 
 #define COMM_LEN	20
@@ -293,7 +270,7 @@ add_sched_event_wakeup(struct task_desc *task, nsec_t timestamp,
 
 static void
 add_sched_event_sleep(struct task_desc *task, nsec_t timestamp,
-		      unsigned long uninterruptible __used)
+		      u64 task_state __used)
 {
 	struct sched_event *event = get_new_event(task, timestamp);
 
@@ -329,77 +306,13 @@ static struct task_desc *register_pid(unsigned long pid, const char *comm)
 	BUG_ON(!tasks);
 	tasks[task->nr] = task;
 
-	printk("registered task #%ld, PID %ld (%s)\n", nr_tasks, pid, comm);
+	if (verbose)
+		printf("registered task #%ld, PID %ld (%s)\n", nr_tasks, pid, comm);
 
 	return task;
 }
 
 
-static int first_trace_line = 1;
-
-static nsec_t first_timestamp;
-static nsec_t prev_timestamp;
-
-void parse_line(char *line);
-
-void parse_line(char *line)
-{
-	unsigned long param1 = 0, param2 = 0;
-	char comm[COMM_LEN], comm2[COMM_LEN];
-	unsigned long pid, pid2, timestamp0;
-	struct task_desc *task, *task2;
-	char func_str[SYM_LEN];
-	nsec_t timestamp;
-	int ret;
-
-	//"   <idle> 0     0D.s3    0us+: try_to_wake_up <events/0 9> (1 0)"
-	ret = sscanf(line, "%20s %5ld %*s %ldus%*c:"
-			   " %128s <%20s %ld> (%ld %ld)\n",
-		comm, &pid, &timestamp0,
-		func_str, comm2, &pid2, &param1, &param2);
-	dprintk("ret: %d\n", ret);
-	if (ret != 8)
-		return;
-
-	timestamp = timestamp0 * 1000LL;
-
-	if (first_trace_line) {
-		first_trace_line = 0;
-		first_timestamp = timestamp;
-	}
-
-	timestamp -= first_timestamp;
-	BUG_ON(timestamp < prev_timestamp);
-	prev_timestamp = timestamp;
-
-	dprintk("parsed: %s - %ld %Ld: %s - <%s %ld> (%ld %ld)\n",
-		comm,
-		pid,
-		timestamp,
-		func_str,
-		comm2,
-		pid2,
-		param1,
-		param2);
-
-	task = register_pid(pid, comm);
-	task2 = register_pid(pid2, comm2);
-
-	if (!strcmp(func_str, "update_curr")) {
-		dprintk("%Ld: task %ld runs for %ld nsecs\n",
-			timestamp, task->nr, param1);
-		add_sched_event_run(task, timestamp, param1);
-	} else if (!strcmp(func_str, "try_to_wake_up")) {
-		dprintk("%Ld: task %ld wakes up task %ld\n",
-			timestamp, task->nr, task2->nr);
-		add_sched_event_wakeup(task, timestamp, task2);
-	} else if (!strcmp(func_str, "deactivate_task")) {
-		dprintk("%Ld: task %ld goes to sleep (uninterruptible: %ld)\n",
-			timestamp, task->nr, param1);
-		add_sched_event_sleep(task, timestamp, param1);
-	}
-}
-
 static void print_task_traces(void)
 {
 	struct task_desc *task;
@@ -407,7 +320,7 @@ static void print_task_traces(void)
 
 	for (i = 0; i < nr_tasks; i++) {
 		task = tasks[i];
-		printk("task %6ld (%20s:%10ld), nr_events: %ld\n",
+		printf("task %6ld (%20s:%10ld), nr_events: %ld\n",
 			task->nr, task->comm, task->pid, task->nr_events);
 	}
 }
@@ -437,40 +350,16 @@ process_sched_event(struct task_desc *this_task __used, struct sched_event *even
 	now = get_nsecs();
 	delta = start_time + event->timestamp - now;
 
-	dprintk("task %ld, event #%ld, %Ld, delta: %.3f (%Ld)\n",
-		this_task->nr, event->nr, event->timestamp,
-		(double)delta/1e6, delta);
-
-	if (0 && delta > 0) {
-		dprintk("%.3f: task %ld FIX %.3f\n",
-			(double)event->timestamp/1e6,
-			this_task->nr,
-			(double)delta/1e6);
-		sleep_nsecs(start_time + event->timestamp - now);
-		nr_sleep_corrections++;
-	}
-
 	switch (event->type) {
 		case SCHED_EVENT_RUN:
-			dprintk("%.3f: task %ld RUN for %.3f\n",
-				(double)event->timestamp/1e6,
-				this_task->nr,
-				(double)event->duration/1e6);
 			burn_nsecs(event->duration);
 			break;
 		case SCHED_EVENT_SLEEP:
-			dprintk("%.3f: task %ld %s SLEEP\n",
-				(double)event->timestamp/1e6,
-				this_task->nr, event->wait_sem ? "" : "SKIP");
 			if (event->wait_sem)
 				ret = sem_wait(event->wait_sem);
 			BUG_ON(ret);
 			break;
 		case SCHED_EVENT_WAKEUP:
-			dprintk("%.3f: task %ld WAKEUP => task %ld\n",
-				(double)event->timestamp/1e6,
-				this_task->nr,
-				event->wakee->nr);
 			if (event->wait_sem)
 				ret = sem_post(event->wait_sem);
 			BUG_ON(ret);
@@ -511,14 +400,10 @@ static nsec_t get_cpu_usage_nsec_self(void)
 	BUG_ON(!file);
 
 	while ((chars = getline(&line, &len, file)) != -1) {
-		dprintk("got line with length %zu :\n", chars);
-		dprintk("%s", line);
 		ret = sscanf(line, "se.sum_exec_runtime : %ld.%06ld\n",
 			&msecs, &nsecs);
 		if (ret == 2) {
 			total = msecs*1e6 + nsecs;
-			dprintk("total: (%ld.%06ld) %Ld\n",
-				msecs, nsecs, total);
 			break;
 		}
 	}
@@ -536,19 +421,16 @@ static void *thread_func(void *ctx)
 	unsigned long i, ret;
 	char comm2[22];
 
-	dprintk("task %ld started up.\n", this_task->nr);
 	sprintf(comm2, ":%s", this_task->comm);
 	prctl(PR_SET_NAME, comm2);
 
 again:
 	ret = sem_post(&this_task->ready_for_work);
 	BUG_ON(ret);
-	D();
 	ret = pthread_mutex_lock(&start_work_mutex);
 	BUG_ON(ret);
 	ret = pthread_mutex_unlock(&start_work_mutex);
 	BUG_ON(ret);
-	D();
 
 	cpu_usage_0 = get_cpu_usage_nsec_self();
 
@@ -560,19 +442,13 @@ static void *thread_func(void *ctx)
 	cpu_usage_1 = get_cpu_usage_nsec_self();
 	this_task->cpu_usage = cpu_usage_1 - cpu_usage_0;
 
-	dprintk("task %ld cpu usage: %0.3f msecs\n",
-		this_task->nr, (double)this_task->cpu_usage / 1e6);
-
-	D();
 	ret = sem_post(&this_task->work_done_sem);
 	BUG_ON(ret);
-	D();
 
 	ret = pthread_mutex_lock(&work_done_wait_mutex);
 	BUG_ON(ret);
 	ret = pthread_mutex_unlock(&work_done_wait_mutex);
 	BUG_ON(ret);
-	D();
 
 	goto again;
 }
@@ -614,9 +490,7 @@ static void wait_for_tasks(void)
 	struct task_desc *task;
 	unsigned long i, ret;
 
-	DP();
 	start_time = get_nsecs();
-	DP();
 	cpu_usage = 0;
 	pthread_mutex_unlock(&work_done_wait_mutex);
 
@@ -633,24 +507,6 @@ static void wait_for_tasks(void)
 
 	pthread_mutex_unlock(&start_work_mutex);
 
-#if 0
-	for (i = 0; i < nr_tasks; i++) {
-		unsigned long missed;
-
-		task = tasks[i];
-		while (task->curr_event + 1 < task->nr_events) {
-			dprintk("parent waiting for %ld (%ld != %ld)\n",
-				i, task->curr_event, task->nr_events);
-			sleep_nsecs(100000000);
-		}
-		missed = task->nr_events - 1 - task->curr_event;
-		if (missed)
-			printk("task %ld missed events: %ld\n", i, missed);
-		ret = sem_post(&task->sleep_sem);
-		BUG_ON(ret);
-	}
-#endif
-	DP();
 	for (i = 0; i < nr_tasks; i++) {
 		task = tasks[i];
 		ret = sem_wait(&task->work_done_sem);
@@ -687,16 +543,16 @@ static void parse_trace(void)
 {
 	__cmd_sched();
 
-	printk("nr_run_events:        %ld\n", nr_run_events);
-	printk("nr_sleep_events:      %ld\n", nr_sleep_events);
-	printk("nr_wakeup_events:     %ld\n", nr_wakeup_events);
+	printf("nr_run_events:        %ld\n", nr_run_events);
+	printf("nr_sleep_events:      %ld\n", nr_sleep_events);
+	printf("nr_wakeup_events:     %ld\n", nr_wakeup_events);
 
 	if (targetless_wakeups)
-		printk("target-less wakeups:  %ld\n", targetless_wakeups);
+		printf("target-less wakeups:  %ld\n", targetless_wakeups);
 	if (multitarget_wakeups)
-		printk("multi-target wakeups: %ld\n", multitarget_wakeups);
+		printf("multi-target wakeups: %ld\n", multitarget_wakeups);
 	if (nr_run_events_optimized)
-		printk("run events optimized: %ld\n",
+		printf("run events optimized: %ld\n",
 			nr_run_events_optimized);
 }
 
@@ -728,17 +584,17 @@ static void run_one_test(void)
 		run_avg = delta;
 	run_avg = (run_avg*9 + delta)/10;
 
-	printk("#%-3ld: %0.3f, ",
+	printf("#%-3ld: %0.3f, ",
 		nr_runs, (double)delta/1000000.0);
 
 #if 0
-	printk("%0.2f +- %0.2f, ",
+	printf("%0.2f +- %0.2f, ",
 		(double)avg_delta/1e6, (double)std_dev/1e6);
 #endif
-	printk("ravg: %0.2f, ",
+	printf("ravg: %0.2f, ",
 		(double)run_avg/1e6);
 
-	printk("cpu: %0.2f / %0.2f",
+	printf("cpu: %0.2f / %0.2f",
 		(double)cpu_usage/1e6, (double)runavg_cpu_usage/1e6);
 
 #if 0
@@ -746,15 +602,15 @@ static void run_one_test(void)
 	 * rusage statistics done by the parent, these are less
 	 * accurate than the sum_exec_runtime based statistics:
 	 */
-	printk(" [%0.2f / %0.2f]",
+	printf(" [%0.2f / %0.2f]",
 		(double)parent_cpu_usage/1e6,
 		(double)runavg_parent_cpu_usage/1e6);
 #endif
 
-	printk("\n");
+	printf("\n");
 
 	if (nr_sleep_corrections)
-		printk(" (%ld sleep corrections)\n", nr_sleep_corrections);
+		printf(" (%ld sleep corrections)\n", nr_sleep_corrections);
 	nr_sleep_corrections = 0;
 }
 
@@ -766,13 +622,13 @@ static void test_calibrations(void)
 	burn_nsecs(1e6);
 	T1 = get_nsecs();
 
-	printk("the run test took %Ld nsecs\n", T1-T0);
+	printf("the run test took %Ld nsecs\n", T1-T0);
 
 	T0 = get_nsecs();
 	sleep_nsecs(1e6);
 	T1 = get_nsecs();
 
-	printk("the sleep test took %Ld nsecs\n", T1-T0);
+	printf("the sleep test took %Ld nsecs\n", T1-T0);
 }
 
 static int
@@ -820,12 +676,14 @@ process_sched_wakeup_event(struct trace_wakeup_event *wakeup_event, struct event
 {
 	struct task_desc *waker, *wakee;
 
-	printf("sched_wakeup event %p\n", event);
+	if (verbose) {
+		printf("sched_wakeup event %p\n", event);
 
-	printf(" ... pid %d woke up %s/%d\n",
-		wakeup_event->common_pid,
-		wakeup_event->comm,
-		wakeup_event->pid);
+		printf(" ... pid %d woke up %s/%d\n",
+			wakeup_event->common_pid,
+			wakeup_event->comm,
+			wakeup_event->pid);
+	}
 
 	waker = register_pid(wakeup_event->common_pid, "<unknown>");
 	wakee = register_pid(wakeup_event->pid, wakeup_event->comm);
@@ -863,7 +721,9 @@ process_sched_switch_event(struct trace_switch_event *switch_event, struct event
 	u64 timestamp0;
 	s64 delta;
 
-	printf("sched_switch event %p\n", event);
+	if (verbose)
+		printf("sched_switch event %p\n", event);
+
 	if (cpu >= MAX_CPUS || cpu < 0)
 		return;
 
@@ -876,10 +736,12 @@ process_sched_switch_event(struct trace_switch_event *switch_event, struct event
 	if (delta < 0)
 		die("hm, delta: %Ld < 0 ?\n", delta);
 
-	printf(" ... switch from %s/%d to %s/%d [ran %Ld nsecs]\n",
-		switch_event->prev_comm, switch_event->prev_pid,
-		switch_event->next_comm, switch_event->next_pid,
-		delta);
+	if (verbose) {
+		printf(" ... switch from %s/%d to %s/%d [ran %Ld nsecs]\n",
+			switch_event->prev_comm, switch_event->prev_pid,
+			switch_event->next_comm, switch_event->next_pid,
+			delta);
+	}
 
 	prev = register_pid(switch_event->prev_pid, switch_event->prev_comm);
 	next = register_pid(switch_event->next_pid, switch_event->next_comm);
@@ -887,6 +749,7 @@ process_sched_switch_event(struct trace_switch_event *switch_event, struct event
 	cpu_last_switched[cpu] = timestamp;
 
 	add_sched_event_run(prev, timestamp, delta);
+	add_sched_event_sleep(prev, timestamp, switch_event->prev_state);
 }
 
 struct trace_fork_event {
@@ -908,9 +771,11 @@ static void
 process_sched_fork_event(struct trace_fork_event *fork_event, struct event *event,
 		  int cpu __used, u64 timestamp __used, struct thread *thread __used)
 {
-	printf("sched_fork event %p\n", event);
-	printf("... parent: %s/%d\n", fork_event->parent_comm, fork_event->parent_pid);
-	printf("...  child: %s/%d\n", fork_event->child_comm, fork_event->child_pid);
+	if (verbose) {
+		printf("sched_fork event %p\n", event);
+		printf("... parent: %s/%d\n", fork_event->parent_comm, fork_event->parent_pid);
+		printf("...  child: %s/%d\n", fork_event->child_comm, fork_event->child_pid);
+	}
 	register_pid(fork_event->parent_pid, fork_event->parent_comm);
 	register_pid(fork_event->child_pid, fork_event->child_comm);
 }
@@ -918,11 +783,12 @@ process_sched_fork_event(struct trace_fork_event *fork_event, struct event *even
 static void process_sched_exit_event(struct event *event,
 		  int cpu __used, u64 timestamp __used, struct thread *thread __used)
 {
-	printf("sched_exit event %p\n", event);
+	if (verbose)
+		printf("sched_exit event %p\n", event);
 }
 
 static void
-process_raw_event(event_t *raw_event, void *more_data,
+process_raw_event(event_t *raw_event __used, void *more_data,
 		  int cpu, u64 timestamp, struct thread *thread)
 {
 	struct {
@@ -935,14 +801,6 @@ process_raw_event(event_t *raw_event, void *more_data,
 	type = trace_parse_common_type(raw->data);
 	event = trace_find_event(type);
 
-	/*
-	 * FIXME: better resolve from pid from the struct trace_entry
-	 * field, although it should be the same than this perf
-	 * event pid
-	 */
-	printf("id %d, type: %d, event: %s\n",
-		raw_event->header.type, type, event->name);
-
 	if (!strcmp(event->name, "sched_switch"))
 		process_sched_switch_event(more_data, event, cpu, timestamp, thread);
 	if (!strcmp(event->name, "sched_wakeup"))
@@ -1197,7 +1055,7 @@ int cmd_sched(int argc, const char **argv, const char *prefix __used)
 	add_cross_task_wakeups();
 
 	create_tasks();
-	printk("------------------------------------------------------------\n");
+	printf("------------------------------------------------------------\n");
 	for (i = 0; i < nr_iterations; i++)
 		run_one_test();
 

commit fbf9482911825f965829567aea8acff3bbc5279c
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Sep 11 12:12:54 2009 +0200

    perf sched: Implement the scheduling workload replay engine
    
    Integrate the schedbench.c bits with the raw trace events
    that we get from the perf machinery, and activate the
    workload replayer/simulator.
    
    Example of a captured 'make -j' workload:
    
    $ perf sched
    
      run measurement overhead: 90 nsecs
      sleep measurement overhead: 2724743 nsecs
      the run test took 1000081 nsecs
      the sleep test took 2981111 nsecs
      version = 0.5
      ...
      nr_run_events:        70
      nr_sleep_events:      66
      nr_wakeup_events:     9
      target-less wakeups:  71
      multi-target wakeups: 47
      run events optimized: 139
      task      0 (                perf:      6607), nr_events: 2
      task      1 (                perf:      6608), nr_events: 6
      task      2 (                    :         0), nr_events: 1
      task      3 (                make:      6609), nr_events: 5
      task      4 (                  sh:      6610), nr_events: 4
      task      5 (                make:      6611), nr_events: 6
      task      6 (                  sh:      6612), nr_events: 4
      task      7 (                make:      6613), nr_events: 5
      task      8 (        migration/11:        25), nr_events: 1
      task      9 (        migration/13:        29), nr_events: 1
      task     10 (        migration/15:        33), nr_events: 1
      task     11 (         migration/9:        21), nr_events: 1
      task     12 (                  sh:      6614), nr_events: 4
      task     13 (                make:      6615), nr_events: 5
      task     14 (                  sh:      6616), nr_events: 4
      task     15 (                make:      6617), nr_events: 7
      task     16 (         migration/3:         9), nr_events: 1
      task     17 (         migration/5:        13), nr_events: 1
      task     18 (         migration/7:        17), nr_events: 1
      task     19 (         migration/1:         5), nr_events: 1
      task     20 (                  sh:      6618), nr_events: 4
      task     21 (                make:      6619), nr_events: 5
      task     22 (                  sh:      6620), nr_events: 4
      task     23 (                make:      6621), nr_events: 10
      task     24 (                  sh:      6623), nr_events: 3
      task     25 (                 gcc:      6624), nr_events: 4
      task     26 (                 gcc:      6625), nr_events: 4
      task     27 (                 gcc:      6626), nr_events: 5
      task     28 (            collect2:      6627), nr_events: 5
      task     29 (                  sh:      6622), nr_events: 1
      task     30 (                make:      6628), nr_events: 7
      task     31 (                  sh:      6630), nr_events: 4
      task     32 (                 gcc:      6631), nr_events: 4
      task     33 (                  sh:      6629), nr_events: 1
      task     34 (                 gcc:      6632), nr_events: 4
      task     35 (                 gcc:      6633), nr_events: 4
      task     36 (            collect2:      6634), nr_events: 4
      task     37 (                make:      6635), nr_events: 8
      task     38 (                  sh:      6637), nr_events: 4
      task     39 (                  sh:      6636), nr_events: 1
      task     40 (                 gcc:      6638), nr_events: 4
      task     41 (                 gcc:      6639), nr_events: 4
      task     42 (                 gcc:      6640), nr_events: 4
      task     43 (            collect2:      6641), nr_events: 4
      task     44 (                make:      6642), nr_events: 6
      task     45 (                  sh:      6643), nr_events: 5
      task     46 (                  sh:      6644), nr_events: 3
      task     47 (                  sh:      6645), nr_events: 4
      task     48 (                make:      6646), nr_events: 6
      task     49 (                  sh:      6647), nr_events: 3
      task     50 (                make:      6648), nr_events: 5
      task     51 (                  sh:      6649), nr_events: 5
      task     52 (                  sh:      6650), nr_events: 6
      task     53 (                make:      6651), nr_events: 4
      task     54 (                make:      6652), nr_events: 5
      task     55 (                make:      6653), nr_events: 4
      task     56 (                make:      6654), nr_events: 4
      task     57 (                make:      6655), nr_events: 5
      task     58 (                  sh:      6656), nr_events: 4
      task     59 (                 gcc:      6657), nr_events: 9
      task     60 (         ksoftirqd/3:        10), nr_events: 1
      task     61 (                 gcc:      6658), nr_events: 4
      task     62 (                make:      6659), nr_events: 5
      task     63 (                  sh:      6660), nr_events: 3
      task     64 (                 gcc:      6661), nr_events: 5
      task     65 (            collect2:      6662), nr_events: 4
      ------------------------------------------------------------
      #1  : 256.745, ravg: 256.74, cpu: 0.00 / 0.00
      #2  : 439.372, ravg: 275.01, cpu: 0.00 / 0.00
      #3  : 411.971, ravg: 288.70, cpu: 0.00 / 0.00
      #4  : 385.500, ravg: 298.38, cpu: 0.00 / 0.00
      #5  : 366.526, ravg: 305.20, cpu: 0.00 / 0.00
      #6  : 381.281, ravg: 312.81, cpu: 0.00 / 0.00
      #7  : 410.756, ravg: 322.60, cpu: 0.00 / 0.00
      #8  : 368.009, ravg: 327.14, cpu: 0.00 / 0.00
      #9  : 408.098, ravg: 335.24, cpu: 0.00 / 0.00
      #10 : 368.582, ravg: 338.57, cpu: 0.00 / 0.00
    
    I.e. we successfully analyzed the trace, replayed it
    via real threads and measured the replayed workload's
    scheduling properties.
    
    This is how it looked like in 'top' output:
    
       PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND
      7164 mingo     20   0 1434m 8080  888 R 57.0  0.1   0:02.04 :perf
      7165 mingo     20   0 1434m 8080  888 R 41.8  0.1   0:01.52 :perf
      7228 mingo     20   0 1434m 8080  888 R 39.8  0.1   0:01.44 :gcc
      7225 mingo     20   0 1434m 8080  888 R 33.8  0.1   0:01.26 :gcc
      7202 mingo     20   0 1434m 8080  888 R 31.2  0.1   0:01.16 :sh
      7222 mingo     20   0 1434m 8080  888 R 25.2  0.1   0:00.96 :sh
      7211 mingo     20   0 1434m 8080  888 R 21.9  0.1   0:00.82 :sh
      7213 mingo     20   0 1434m 8080  888 D 19.2  0.1   0:00.74 :sh
      7194 mingo     20   0 1434m 8080  888 D 18.6  0.1   0:00.72 :make
    
    There's still various kinks in it - more patches to come.
    
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index c66e6a321371..6ec4f51d536b 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -57,7 +57,7 @@ static u64			sample_type;
 
 #define BUG_ON(x)	assert(!(x))
 
-#define DEBUG		1
+#define DEBUG		0
 
 typedef unsigned long long nsec_t;
 
@@ -238,15 +238,14 @@ static struct sched_event *last_event(struct task_desc *task)
 }
 
 static void
-add_sched_event_run(struct task_desc *task, nsec_t timestamp,
-		    unsigned long duration)
+add_sched_event_run(struct task_desc *task, nsec_t timestamp, u64 duration)
 {
 	struct sched_event *event, *curr_event = last_event(task);
 
 	/*
- 	 * optimize an existing RUN event by merging this one
- 	 * to it:
- 	 */
+	 * optimize an existing RUN event by merging this one
+	 * to it:
+	 */
 	if (curr_event && curr_event->type == SCHED_EVENT_RUN) {
 		nr_run_events_optimized++;
 		curr_event->duration += duration;
@@ -376,7 +375,7 @@ void parse_line(char *line)
 	dprintk("parsed: %s - %ld %Ld: %s - <%s %ld> (%ld %ld)\n",
 		comm,
 		pid,
-		timestamp, 
+		timestamp,
 		func_str,
 		comm2,
 		pid2,
@@ -429,7 +428,7 @@ static void add_cross_task_wakeups(void)
 }
 
 static void
-process_sched_event(struct task_desc *this_task, struct sched_event *event)
+process_sched_event(struct task_desc *this_task __used, struct sched_event *event)
 {
 	int ret = 0;
 	nsec_t now;
@@ -744,9 +743,9 @@ static void run_one_test(void)
 
 #if 0
 	/*
- 	 * rusage statistics done by the parent, these are less
- 	 * accurate than the sum_exec_runtime based statistics:
- 	 */
+	 * rusage statistics done by the parent, these are less
+	 * accurate than the sum_exec_runtime based statistics:
+	 */
 	printk(" [%0.2f / %0.2f]",
 		(double)parent_cpu_usage/1e6,
 		(double)runavg_parent_cpu_usage/1e6);
@@ -798,16 +797,128 @@ process_comm_event(event_t *event, unsigned long offset, unsigned long head)
 	return 0;
 }
 
-static void process_sched_wakeup_event(struct event *event,
+struct trace_wakeup_event {
+	u32 size;
+
+	u16 common_type;
+	u8 common_flags;
+	u8 common_preempt_count;
+	u32 common_pid;
+	u32 common_tgid;
+
+	char comm[16];
+	u32 pid;
+
+	u32 prio;
+	u32 success;
+	u32 cpu;
+};
+
+static void
+process_sched_wakeup_event(struct trace_wakeup_event *wakeup_event, struct event *event,
 		  int cpu __used, u64 timestamp __used, struct thread *thread __used)
 {
+	struct task_desc *waker, *wakee;
+
 	printf("sched_wakeup event %p\n", event);
+
+	printf(" ... pid %d woke up %s/%d\n",
+		wakeup_event->common_pid,
+		wakeup_event->comm,
+		wakeup_event->pid);
+
+	waker = register_pid(wakeup_event->common_pid, "<unknown>");
+	wakee = register_pid(wakeup_event->pid, wakeup_event->comm);
+
+	add_sched_event_wakeup(waker, timestamp, wakee);
 }
 
-static void process_sched_switch_event(struct event *event,
+struct trace_switch_event {
+	u32 size;
+
+	u16 common_type;
+	u8 common_flags;
+	u8 common_preempt_count;
+	u32 common_pid;
+	u32 common_tgid;
+
+	char prev_comm[16];
+	u32 prev_pid;
+	u32 prev_prio;
+	u64 prev_state;
+	char next_comm[16];
+	u32 next_pid;
+	u32 next_prio;
+};
+
+#define MAX_CPUS 4096
+
+unsigned long cpu_last_switched[MAX_CPUS];
+
+static void
+process_sched_switch_event(struct trace_switch_event *switch_event, struct event *event,
 		  int cpu __used, u64 timestamp __used, struct thread *thread __used)
 {
+	struct task_desc *prev, *next;
+	u64 timestamp0;
+	s64 delta;
+
 	printf("sched_switch event %p\n", event);
+	if (cpu >= MAX_CPUS || cpu < 0)
+		return;
+
+	timestamp0 = cpu_last_switched[cpu];
+	if (timestamp0)
+		delta = timestamp - timestamp0;
+	else
+		delta = 0;
+
+	if (delta < 0)
+		die("hm, delta: %Ld < 0 ?\n", delta);
+
+	printf(" ... switch from %s/%d to %s/%d [ran %Ld nsecs]\n",
+		switch_event->prev_comm, switch_event->prev_pid,
+		switch_event->next_comm, switch_event->next_pid,
+		delta);
+
+	prev = register_pid(switch_event->prev_pid, switch_event->prev_comm);
+	next = register_pid(switch_event->next_pid, switch_event->next_comm);
+
+	cpu_last_switched[cpu] = timestamp;
+
+	add_sched_event_run(prev, timestamp, delta);
+}
+
+struct trace_fork_event {
+	u32 size;
+
+	u16 common_type;
+	u8 common_flags;
+	u8 common_preempt_count;
+	u32 common_pid;
+	u32 common_tgid;
+
+	char parent_comm[16];
+	u32 parent_pid;
+	char child_comm[16];
+	u32 child_pid;
+};
+
+static void
+process_sched_fork_event(struct trace_fork_event *fork_event, struct event *event,
+		  int cpu __used, u64 timestamp __used, struct thread *thread __used)
+{
+	printf("sched_fork event %p\n", event);
+	printf("... parent: %s/%d\n", fork_event->parent_comm, fork_event->parent_pid);
+	printf("...  child: %s/%d\n", fork_event->child_comm, fork_event->child_pid);
+	register_pid(fork_event->parent_pid, fork_event->parent_comm);
+	register_pid(fork_event->child_pid, fork_event->child_comm);
+}
+
+static void process_sched_exit_event(struct event *event,
+		  int cpu __used, u64 timestamp __used, struct thread *thread __used)
+{
+	printf("sched_exit event %p\n", event);
 }
 
 static void
@@ -833,9 +944,15 @@ process_raw_event(event_t *raw_event, void *more_data,
 		raw_event->header.type, type, event->name);
 
 	if (!strcmp(event->name, "sched_switch"))
-		process_sched_switch_event(event, cpu, timestamp, thread);
+		process_sched_switch_event(more_data, event, cpu, timestamp, thread);
 	if (!strcmp(event->name, "sched_wakeup"))
-		process_sched_wakeup_event(event, cpu, timestamp, thread);
+		process_sched_wakeup_event(more_data, event, cpu, timestamp, thread);
+	if (!strcmp(event->name, "sched_wakeup_new"))
+		process_sched_wakeup_event(more_data, event, cpu, timestamp, thread);
+	if (!strcmp(event->name, "sched_process_fork"))
+		process_sched_fork_event(more_data, event, cpu, timestamp, thread);
+	if (!strcmp(event->name, "sched_process_exit"))
+		process_sched_exit_event(event, cpu, timestamp, thread);
 }
 
 static int
@@ -1053,7 +1170,7 @@ static const struct option options[] = {
 
 int cmd_sched(int argc, const char **argv, const char *prefix __used)
 {
-	long nr_iterations = LONG_MAX, i;
+	long nr_iterations = 10, i;
 
 	symbol__init();
 	page_size = getpagesize();
@@ -1068,8 +1185,7 @@ int cmd_sched(int argc, const char **argv, const char *prefix __used)
 			usage_with_options(annotate_usage, options);
 	}
 
-
-	setup_pager();
+//	setup_pager();
 
 	calibrate_run_measurement_overhead();
 	calibrate_sleep_measurement_overhead();

commit ec156764d424dd67283c2cd5e9f6f1b8388364ac
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Sep 11 12:12:54 2009 +0200

    perf sched: Import schedbench.c
    
    Import the schedbench.c tool that i wrote some time ago to
    simulate scheduler behavior but never finished. It's a good
    basis for perf sched nevertheless.
    
    Most of its guts are not hooked up to the perf event loop
    yet - that will be done in the patches to come.
    
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
index 60228d9179ac..c66e6a321371 100644
--- a/tools/perf/builtin-sched.c
+++ b/tools/perf/builtin-sched.c
@@ -12,22 +12,770 @@
 #include "util/debug.h"
 
 #include "util/trace-event.h"
+#include <sys/types.h>
 
-static char		const *input_name = "perf.data";
-static int		input;
-static unsigned long	page_size;
-static unsigned long	mmap_window = 32;
+static char			const *input_name = "perf.data";
+static int			input;
+static unsigned long		page_size;
+static unsigned long		mmap_window = 32;
 
-static unsigned long	total = 0;
-static unsigned long	total_comm = 0;
+static unsigned long		total_comm = 0;
 
-static struct rb_root	threads;
-static struct thread	*last_match;
+static struct rb_root		threads;
+static struct thread		*last_match;
 
-static struct perf_header *header;
-static u64		sample_type;
+static struct perf_header	*header;
+static u64			sample_type;
 
 
+/*
+ * Scheduler benchmarks
+ */
+#include <sys/resource.h>
+#include <sys/types.h>
+#include <sys/stat.h>
+#include <sys/time.h>
+#include <sys/prctl.h>
+
+#include <linux/unistd.h>
+
+#include <semaphore.h>
+#include <pthread.h>
+#include <signal.h>
+#include <values.h>
+#include <string.h>
+#include <unistd.h>
+#include <stdlib.h>
+#include <assert.h>
+#include <fcntl.h>
+#include <time.h>
+#include <math.h>
+
+#include <stdio.h>
+
+#define PR_SET_NAME	15               /* Set process name */
+
+#define BUG_ON(x)	assert(!(x))
+
+#define DEBUG		1
+
+typedef unsigned long long nsec_t;
+
+#define printk(x...)		do { printf(x); fflush(stdout); } while (0)
+
+nsec_t prev_printk;
+
+#define __dprintk(x,y...) do {						 \
+	nsec_t __now = get_nsecs(), __delta = __now - prev_printk;	 \
+									 \
+	prev_printk = __now;						 \
+									 \
+	printf("%.3f [%Ld] [%.3f]: " x, (double)__now/1e6, __now, (double)__delta/1e6, y);\
+} while (0)
+
+#if !DEBUG
+# define dprintk(x...)	do { } while (0)
+#else
+# define dprintk(x...)	__dprintk(x)
+#endif
+
+#define __DP()		__dprintk("parent: line %d\n", __LINE__)
+#define DP()		dprintk("parent: line %d\n", __LINE__)
+#define D()		dprintk("task %ld: line %d\n", this_task->nr, __LINE__)
+
+
+static nsec_t run_measurement_overhead;
+static nsec_t sleep_measurement_overhead;
+
+static nsec_t get_nsecs(void)
+{
+	struct timespec ts;
+
+	clock_gettime(CLOCK_MONOTONIC, &ts);
+
+	return ts.tv_sec * 1000000000ULL + ts.tv_nsec;
+}
+
+static void burn_nsecs(nsec_t nsecs)
+{
+	nsec_t T0 = get_nsecs(), T1;
+
+	do {
+		T1 = get_nsecs();
+	} while (T1 + run_measurement_overhead < T0 + nsecs);
+}
+
+static void sleep_nsecs(nsec_t nsecs)
+{
+	struct timespec ts;
+
+	ts.tv_nsec = nsecs % 999999999;
+	ts.tv_sec = nsecs / 999999999;
+
+	nanosleep(&ts, NULL);
+}
+
+static void calibrate_run_measurement_overhead(void)
+{
+	nsec_t T0, T1, delta, min_delta = 1000000000ULL;
+	int i;
+
+	for (i = 0; i < 10; i++) {
+		T0 = get_nsecs();
+		burn_nsecs(0);
+		T1 = get_nsecs();
+		delta = T1-T0;
+		min_delta = min(min_delta, delta);
+	}
+	run_measurement_overhead = min_delta;
+
+	printk("run measurement overhead: %Ld nsecs\n", min_delta);
+}
+
+static void calibrate_sleep_measurement_overhead(void)
+{
+	nsec_t T0, T1, delta, min_delta = 1000000000ULL;
+	int i;
+
+	for (i = 0; i < 10; i++) {
+		T0 = get_nsecs();
+		sleep_nsecs(10000);
+		T1 = get_nsecs();
+		delta = T1-T0;
+		min_delta = min(min_delta, delta);
+	}
+	min_delta -= 10000;
+	sleep_measurement_overhead = min_delta;
+
+	printk("sleep measurement overhead: %Ld nsecs\n", min_delta);
+}
+
+#define COMM_LEN	20
+#define SYM_LEN		129
+
+#define MAX_PID		65536
+
+static unsigned long nr_tasks;
+
+struct sched_event;
+
+struct task_desc {
+	unsigned long		nr;
+	unsigned long		pid;
+	char			comm[COMM_LEN];
+
+	unsigned long		nr_events;
+	unsigned long		curr_event;
+	struct sched_event	**events;
+
+	pthread_t		thread;
+	sem_t			sleep_sem;
+
+	sem_t			ready_for_work;
+	sem_t			work_done_sem;
+
+	nsec_t			cpu_usage;
+};
+
+enum sched_event_type {
+	SCHED_EVENT_RUN,
+	SCHED_EVENT_SLEEP,
+	SCHED_EVENT_WAKEUP,
+};
+
+struct sched_event {
+	enum sched_event_type	type;
+	nsec_t			timestamp;
+	nsec_t			duration;
+	unsigned long		nr;
+	int			specific_wait;
+	sem_t			*wait_sem;
+	struct task_desc	*wakee;
+};
+
+static struct task_desc		*pid_to_task[MAX_PID];
+
+static struct task_desc		**tasks;
+
+static pthread_mutex_t		start_work_mutex = PTHREAD_MUTEX_INITIALIZER;
+static nsec_t			start_time;
+
+static pthread_mutex_t		work_done_wait_mutex = PTHREAD_MUTEX_INITIALIZER;
+
+static unsigned long		nr_run_events;
+static unsigned long		nr_sleep_events;
+static unsigned long		nr_wakeup_events;
+
+static unsigned long		nr_sleep_corrections;
+static unsigned long		nr_run_events_optimized;
+
+static struct sched_event *
+get_new_event(struct task_desc *task, nsec_t timestamp)
+{
+	struct sched_event *event = calloc(1, sizeof(*event));
+	unsigned long idx = task->nr_events;
+	size_t size;
+
+	event->timestamp = timestamp;
+	event->nr = idx;
+
+	task->nr_events++;
+	size = sizeof(struct sched_event *) * task->nr_events;
+	task->events = realloc(task->events, size);
+	BUG_ON(!task->events);
+
+	task->events[idx] = event;
+
+	return event;
+}
+
+static struct sched_event *last_event(struct task_desc *task)
+{
+	if (!task->nr_events)
+		return NULL;
+
+	return task->events[task->nr_events - 1];
+}
+
+static void
+add_sched_event_run(struct task_desc *task, nsec_t timestamp,
+		    unsigned long duration)
+{
+	struct sched_event *event, *curr_event = last_event(task);
+
+	/*
+ 	 * optimize an existing RUN event by merging this one
+ 	 * to it:
+ 	 */
+	if (curr_event && curr_event->type == SCHED_EVENT_RUN) {
+		nr_run_events_optimized++;
+		curr_event->duration += duration;
+		return;
+	}
+
+	event = get_new_event(task, timestamp);
+
+	event->type = SCHED_EVENT_RUN;
+	event->duration = duration;
+
+	nr_run_events++;
+}
+
+static unsigned long targetless_wakeups;
+static unsigned long multitarget_wakeups;
+
+static void
+add_sched_event_wakeup(struct task_desc *task, nsec_t timestamp,
+		       struct task_desc *wakee)
+{
+	struct sched_event *event, *wakee_event;
+
+	event = get_new_event(task, timestamp);
+	event->type = SCHED_EVENT_WAKEUP;
+	event->wakee = wakee;
+
+	wakee_event = last_event(wakee);
+	if (!wakee_event || wakee_event->type != SCHED_EVENT_SLEEP) {
+		targetless_wakeups++;
+		return;
+	}
+	if (wakee_event->wait_sem) {
+		multitarget_wakeups++;
+		return;
+	}
+
+	wakee_event->wait_sem = calloc(1, sizeof(*wakee_event->wait_sem));
+	sem_init(wakee_event->wait_sem, 0, 0);
+	wakee_event->specific_wait = 1;
+	event->wait_sem = wakee_event->wait_sem;
+
+	nr_wakeup_events++;
+}
+
+static void
+add_sched_event_sleep(struct task_desc *task, nsec_t timestamp,
+		      unsigned long uninterruptible __used)
+{
+	struct sched_event *event = get_new_event(task, timestamp);
+
+	event->type = SCHED_EVENT_SLEEP;
+
+	nr_sleep_events++;
+}
+
+static struct task_desc *register_pid(unsigned long pid, const char *comm)
+{
+	struct task_desc *task;
+
+	BUG_ON(pid >= MAX_PID);
+
+	task = pid_to_task[pid];
+
+	if (task)
+		return task;
+
+	task = calloc(1, sizeof(*task));
+	task->pid = pid;
+	task->nr = nr_tasks;
+	strcpy(task->comm, comm);
+	/*
+	 * every task starts in sleeping state - this gets ignored
+	 * if there's no wakeup pointing to this sleep state:
+	 */
+	add_sched_event_sleep(task, 0, 0);
+
+	pid_to_task[pid] = task;
+	nr_tasks++;
+	tasks = realloc(tasks, nr_tasks*sizeof(struct task_task *));
+	BUG_ON(!tasks);
+	tasks[task->nr] = task;
+
+	printk("registered task #%ld, PID %ld (%s)\n", nr_tasks, pid, comm);
+
+	return task;
+}
+
+
+static int first_trace_line = 1;
+
+static nsec_t first_timestamp;
+static nsec_t prev_timestamp;
+
+void parse_line(char *line);
+
+void parse_line(char *line)
+{
+	unsigned long param1 = 0, param2 = 0;
+	char comm[COMM_LEN], comm2[COMM_LEN];
+	unsigned long pid, pid2, timestamp0;
+	struct task_desc *task, *task2;
+	char func_str[SYM_LEN];
+	nsec_t timestamp;
+	int ret;
+
+	//"   <idle> 0     0D.s3    0us+: try_to_wake_up <events/0 9> (1 0)"
+	ret = sscanf(line, "%20s %5ld %*s %ldus%*c:"
+			   " %128s <%20s %ld> (%ld %ld)\n",
+		comm, &pid, &timestamp0,
+		func_str, comm2, &pid2, &param1, &param2);
+	dprintk("ret: %d\n", ret);
+	if (ret != 8)
+		return;
+
+	timestamp = timestamp0 * 1000LL;
+
+	if (first_trace_line) {
+		first_trace_line = 0;
+		first_timestamp = timestamp;
+	}
+
+	timestamp -= first_timestamp;
+	BUG_ON(timestamp < prev_timestamp);
+	prev_timestamp = timestamp;
+
+	dprintk("parsed: %s - %ld %Ld: %s - <%s %ld> (%ld %ld)\n",
+		comm,
+		pid,
+		timestamp, 
+		func_str,
+		comm2,
+		pid2,
+		param1,
+		param2);
+
+	task = register_pid(pid, comm);
+	task2 = register_pid(pid2, comm2);
+
+	if (!strcmp(func_str, "update_curr")) {
+		dprintk("%Ld: task %ld runs for %ld nsecs\n",
+			timestamp, task->nr, param1);
+		add_sched_event_run(task, timestamp, param1);
+	} else if (!strcmp(func_str, "try_to_wake_up")) {
+		dprintk("%Ld: task %ld wakes up task %ld\n",
+			timestamp, task->nr, task2->nr);
+		add_sched_event_wakeup(task, timestamp, task2);
+	} else if (!strcmp(func_str, "deactivate_task")) {
+		dprintk("%Ld: task %ld goes to sleep (uninterruptible: %ld)\n",
+			timestamp, task->nr, param1);
+		add_sched_event_sleep(task, timestamp, param1);
+	}
+}
+
+static void print_task_traces(void)
+{
+	struct task_desc *task;
+	unsigned long i;
+
+	for (i = 0; i < nr_tasks; i++) {
+		task = tasks[i];
+		printk("task %6ld (%20s:%10ld), nr_events: %ld\n",
+			task->nr, task->comm, task->pid, task->nr_events);
+	}
+}
+
+static void add_cross_task_wakeups(void)
+{
+	struct task_desc *task1, *task2;
+	unsigned long i, j;
+
+	for (i = 0; i < nr_tasks; i++) {
+		task1 = tasks[i];
+		j = i + 1;
+		if (j == nr_tasks)
+			j = 0;
+		task2 = tasks[j];
+		add_sched_event_wakeup(task1, 0, task2);
+	}
+}
+
+static void
+process_sched_event(struct task_desc *this_task, struct sched_event *event)
+{
+	int ret = 0;
+	nsec_t now;
+	long long delta;
+
+	now = get_nsecs();
+	delta = start_time + event->timestamp - now;
+
+	dprintk("task %ld, event #%ld, %Ld, delta: %.3f (%Ld)\n",
+		this_task->nr, event->nr, event->timestamp,
+		(double)delta/1e6, delta);
+
+	if (0 && delta > 0) {
+		dprintk("%.3f: task %ld FIX %.3f\n",
+			(double)event->timestamp/1e6,
+			this_task->nr,
+			(double)delta/1e6);
+		sleep_nsecs(start_time + event->timestamp - now);
+		nr_sleep_corrections++;
+	}
+
+	switch (event->type) {
+		case SCHED_EVENT_RUN:
+			dprintk("%.3f: task %ld RUN for %.3f\n",
+				(double)event->timestamp/1e6,
+				this_task->nr,
+				(double)event->duration/1e6);
+			burn_nsecs(event->duration);
+			break;
+		case SCHED_EVENT_SLEEP:
+			dprintk("%.3f: task %ld %s SLEEP\n",
+				(double)event->timestamp/1e6,
+				this_task->nr, event->wait_sem ? "" : "SKIP");
+			if (event->wait_sem)
+				ret = sem_wait(event->wait_sem);
+			BUG_ON(ret);
+			break;
+		case SCHED_EVENT_WAKEUP:
+			dprintk("%.3f: task %ld WAKEUP => task %ld\n",
+				(double)event->timestamp/1e6,
+				this_task->nr,
+				event->wakee->nr);
+			if (event->wait_sem)
+				ret = sem_post(event->wait_sem);
+			BUG_ON(ret);
+			break;
+		default:
+			BUG_ON(1);
+	}
+}
+
+static nsec_t get_cpu_usage_nsec_parent(void)
+{
+	struct rusage ru;
+	nsec_t sum;
+	int err;
+
+	err = getrusage(RUSAGE_SELF, &ru);
+	BUG_ON(err);
+
+	sum =  ru.ru_utime.tv_sec*1e9 + ru.ru_utime.tv_usec*1e3;
+	sum += ru.ru_stime.tv_sec*1e9 + ru.ru_stime.tv_usec*1e3;
+
+	return sum;
+}
+
+static nsec_t get_cpu_usage_nsec_self(void)
+{
+	char filename [] = "/proc/1234567890/sched";
+	unsigned long msecs, nsecs;
+	char *line = NULL;
+	nsec_t total = 0;
+	size_t len = 0;
+	ssize_t chars;
+	FILE *file;
+	int ret;
+
+	sprintf(filename, "/proc/%d/sched", getpid());
+	file = fopen(filename, "r");
+	BUG_ON(!file);
+
+	while ((chars = getline(&line, &len, file)) != -1) {
+		dprintk("got line with length %zu :\n", chars);
+		dprintk("%s", line);
+		ret = sscanf(line, "se.sum_exec_runtime : %ld.%06ld\n",
+			&msecs, &nsecs);
+		if (ret == 2) {
+			total = msecs*1e6 + nsecs;
+			dprintk("total: (%ld.%06ld) %Ld\n",
+				msecs, nsecs, total);
+			break;
+		}
+	}
+	if (line)
+		free(line);
+	fclose(file);
+
+	return total;
+}
+
+static void *thread_func(void *ctx)
+{
+	struct task_desc *this_task = ctx;
+	nsec_t cpu_usage_0, cpu_usage_1;
+	unsigned long i, ret;
+	char comm2[22];
+
+	dprintk("task %ld started up.\n", this_task->nr);
+	sprintf(comm2, ":%s", this_task->comm);
+	prctl(PR_SET_NAME, comm2);
+
+again:
+	ret = sem_post(&this_task->ready_for_work);
+	BUG_ON(ret);
+	D();
+	ret = pthread_mutex_lock(&start_work_mutex);
+	BUG_ON(ret);
+	ret = pthread_mutex_unlock(&start_work_mutex);
+	BUG_ON(ret);
+	D();
+
+	cpu_usage_0 = get_cpu_usage_nsec_self();
+
+	for (i = 0; i < this_task->nr_events; i++) {
+		this_task->curr_event = i;
+		process_sched_event(this_task, this_task->events[i]);
+	}
+
+	cpu_usage_1 = get_cpu_usage_nsec_self();
+	this_task->cpu_usage = cpu_usage_1 - cpu_usage_0;
+
+	dprintk("task %ld cpu usage: %0.3f msecs\n",
+		this_task->nr, (double)this_task->cpu_usage / 1e6);
+
+	D();
+	ret = sem_post(&this_task->work_done_sem);
+	BUG_ON(ret);
+	D();
+
+	ret = pthread_mutex_lock(&work_done_wait_mutex);
+	BUG_ON(ret);
+	ret = pthread_mutex_unlock(&work_done_wait_mutex);
+	BUG_ON(ret);
+	D();
+
+	goto again;
+}
+
+static void create_tasks(void)
+{
+	struct task_desc *task;
+	pthread_attr_t attr;
+	unsigned long i;
+	int err;
+
+	err = pthread_attr_init(&attr);
+	BUG_ON(err);
+	err = pthread_attr_setstacksize(&attr, (size_t)(16*1024));
+	BUG_ON(err);
+	err = pthread_mutex_lock(&start_work_mutex);
+	BUG_ON(err);
+	err = pthread_mutex_lock(&work_done_wait_mutex);
+	BUG_ON(err);
+	for (i = 0; i < nr_tasks; i++) {
+		task = tasks[i];
+		sem_init(&task->sleep_sem, 0, 0);
+		sem_init(&task->ready_for_work, 0, 0);
+		sem_init(&task->work_done_sem, 0, 0);
+		task->curr_event = 0;
+		err = pthread_create(&task->thread, &attr, thread_func, task);
+		BUG_ON(err);
+	}
+}
+
+static nsec_t cpu_usage;
+static nsec_t runavg_cpu_usage;
+static nsec_t parent_cpu_usage;
+static nsec_t runavg_parent_cpu_usage;
+
+static void wait_for_tasks(void)
+{
+	nsec_t cpu_usage_0, cpu_usage_1;
+	struct task_desc *task;
+	unsigned long i, ret;
+
+	DP();
+	start_time = get_nsecs();
+	DP();
+	cpu_usage = 0;
+	pthread_mutex_unlock(&work_done_wait_mutex);
+
+	for (i = 0; i < nr_tasks; i++) {
+		task = tasks[i];
+		ret = sem_wait(&task->ready_for_work);
+		BUG_ON(ret);
+		sem_init(&task->ready_for_work, 0, 0);
+	}
+	ret = pthread_mutex_lock(&work_done_wait_mutex);
+	BUG_ON(ret);
+
+	cpu_usage_0 = get_cpu_usage_nsec_parent();
+
+	pthread_mutex_unlock(&start_work_mutex);
+
+#if 0
+	for (i = 0; i < nr_tasks; i++) {
+		unsigned long missed;
+
+		task = tasks[i];
+		while (task->curr_event + 1 < task->nr_events) {
+			dprintk("parent waiting for %ld (%ld != %ld)\n",
+				i, task->curr_event, task->nr_events);
+			sleep_nsecs(100000000);
+		}
+		missed = task->nr_events - 1 - task->curr_event;
+		if (missed)
+			printk("task %ld missed events: %ld\n", i, missed);
+		ret = sem_post(&task->sleep_sem);
+		BUG_ON(ret);
+	}
+#endif
+	DP();
+	for (i = 0; i < nr_tasks; i++) {
+		task = tasks[i];
+		ret = sem_wait(&task->work_done_sem);
+		BUG_ON(ret);
+		sem_init(&task->work_done_sem, 0, 0);
+		cpu_usage += task->cpu_usage;
+		task->cpu_usage = 0;
+	}
+
+	cpu_usage_1 = get_cpu_usage_nsec_parent();
+	if (!runavg_cpu_usage)
+		runavg_cpu_usage = cpu_usage;
+	runavg_cpu_usage = (runavg_cpu_usage*9 + cpu_usage)/10;
+
+	parent_cpu_usage = cpu_usage_1 - cpu_usage_0;
+	if (!runavg_parent_cpu_usage)
+		runavg_parent_cpu_usage = parent_cpu_usage;
+	runavg_parent_cpu_usage = (runavg_parent_cpu_usage*9 +
+				   parent_cpu_usage)/10;
+
+	ret = pthread_mutex_lock(&start_work_mutex);
+	BUG_ON(ret);
+
+	for (i = 0; i < nr_tasks; i++) {
+		task = tasks[i];
+		sem_init(&task->sleep_sem, 0, 0);
+		task->curr_event = 0;
+	}
+}
+
+static int __cmd_sched(void);
+
+static void parse_trace(void)
+{
+	__cmd_sched();
+
+	printk("nr_run_events:        %ld\n", nr_run_events);
+	printk("nr_sleep_events:      %ld\n", nr_sleep_events);
+	printk("nr_wakeup_events:     %ld\n", nr_wakeup_events);
+
+	if (targetless_wakeups)
+		printk("target-less wakeups:  %ld\n", targetless_wakeups);
+	if (multitarget_wakeups)
+		printk("multi-target wakeups: %ld\n", multitarget_wakeups);
+	if (nr_run_events_optimized)
+		printk("run events optimized: %ld\n",
+			nr_run_events_optimized);
+}
+
+static unsigned long nr_runs;
+static nsec_t sum_runtime;
+static nsec_t sum_fluct;
+static nsec_t run_avg;
+
+static void run_one_test(void)
+{
+	nsec_t T0, T1, delta, avg_delta, fluct, std_dev;
+
+	T0 = get_nsecs();
+	wait_for_tasks();
+	T1 = get_nsecs();
+
+	delta = T1 - T0;
+	sum_runtime += delta;
+	nr_runs++;
+
+	avg_delta = sum_runtime / nr_runs;
+	if (delta < avg_delta)
+		fluct = avg_delta - delta;
+	else
+		fluct = delta - avg_delta;
+	sum_fluct += fluct;
+	std_dev = sum_fluct / nr_runs / sqrt(nr_runs);
+	if (!run_avg)
+		run_avg = delta;
+	run_avg = (run_avg*9 + delta)/10;
+
+	printk("#%-3ld: %0.3f, ",
+		nr_runs, (double)delta/1000000.0);
+
+#if 0
+	printk("%0.2f +- %0.2f, ",
+		(double)avg_delta/1e6, (double)std_dev/1e6);
+#endif
+	printk("ravg: %0.2f, ",
+		(double)run_avg/1e6);
+
+	printk("cpu: %0.2f / %0.2f",
+		(double)cpu_usage/1e6, (double)runavg_cpu_usage/1e6);
+
+#if 0
+	/*
+ 	 * rusage statistics done by the parent, these are less
+ 	 * accurate than the sum_exec_runtime based statistics:
+ 	 */
+	printk(" [%0.2f / %0.2f]",
+		(double)parent_cpu_usage/1e6,
+		(double)runavg_parent_cpu_usage/1e6);
+#endif
+
+	printk("\n");
+
+	if (nr_sleep_corrections)
+		printk(" (%ld sleep corrections)\n", nr_sleep_corrections);
+	nr_sleep_corrections = 0;
+}
+
+static void test_calibrations(void)
+{
+	nsec_t T0, T1;
+
+	T0 = get_nsecs();
+	burn_nsecs(1e6);
+	T1 = get_nsecs();
+
+	printk("the run test took %Ld nsecs\n", T1-T0);
+
+	T0 = get_nsecs();
+	sleep_nsecs(1e6);
+	T1 = get_nsecs();
+
+	printk("the sleep test took %Ld nsecs\n", T1-T0);
+}
+
 static int
 process_comm_event(event_t *event, unsigned long offset, unsigned long head)
 {
@@ -50,6 +798,46 @@ process_comm_event(event_t *event, unsigned long offset, unsigned long head)
 	return 0;
 }
 
+static void process_sched_wakeup_event(struct event *event,
+		  int cpu __used, u64 timestamp __used, struct thread *thread __used)
+{
+	printf("sched_wakeup event %p\n", event);
+}
+
+static void process_sched_switch_event(struct event *event,
+		  int cpu __used, u64 timestamp __used, struct thread *thread __used)
+{
+	printf("sched_switch event %p\n", event);
+}
+
+static void
+process_raw_event(event_t *raw_event, void *more_data,
+		  int cpu, u64 timestamp, struct thread *thread)
+{
+	struct {
+		u32 size;
+		char data[0];
+	} *raw = more_data;
+	struct event *event;
+	int type;
+
+	type = trace_parse_common_type(raw->data);
+	event = trace_find_event(type);
+
+	/*
+	 * FIXME: better resolve from pid from the struct trace_entry
+	 * field, although it should be the same than this perf
+	 * event pid
+	 */
+	printf("id %d, type: %d, event: %s\n",
+		raw_event->header.type, type, event->name);
+
+	if (!strcmp(event->name, "sched_switch"))
+		process_sched_switch_event(event, cpu, timestamp, thread);
+	if (!strcmp(event->name, "sched_wakeup"))
+		process_sched_wakeup_event(event, cpu, timestamp, thread);
+}
+
 static int
 process_sample_event(event_t *event, unsigned long offset, unsigned long head)
 {
@@ -122,20 +910,8 @@ process_sample_event(event_t *event, unsigned long offset, unsigned long head)
 		dump_printf(" ...... dso: [hypervisor]\n");
 	}
 
-	if (sample_type & PERF_SAMPLE_RAW) {
-		struct {
-			u32 size;
-			char data[0];
-		} *raw = more_data;
-
-		/*
-		 * FIXME: better resolve from pid from the struct trace_entry
-		 * field, although it should be the same than this perf
-		 * event pid
-		 */
-		print_event(cpu, raw->data, raw->size, timestamp, thread->comm);
-	}
-	total += period;
+	if (sample_type & PERF_SAMPLE_RAW)
+		process_raw_event(event, more_data, cpu, timestamp, thread);
 
 	return 0;
 }
@@ -277,6 +1053,8 @@ static const struct option options[] = {
 
 int cmd_sched(int argc, const char **argv, const char *prefix __used)
 {
+	long nr_iterations = LONG_MAX, i;
+
 	symbol__init();
 	page_size = getpagesize();
 
@@ -293,5 +1071,19 @@ int cmd_sched(int argc, const char **argv, const char *prefix __used)
 
 	setup_pager();
 
-	return __cmd_sched();
+	calibrate_run_measurement_overhead();
+	calibrate_sleep_measurement_overhead();
+
+	test_calibrations();
+
+	parse_trace();
+	print_task_traces();
+	add_cross_task_wakeups();
+
+	create_tasks();
+	printk("------------------------------------------------------------\n");
+	for (i = 0; i < nr_iterations; i++)
+		run_one_test();
+
+	return 0;
 }

commit 0a02ad9331dd4db56c29c60db2e99c4daaad8a48
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Sep 11 12:12:54 2009 +0200

    perf: Add 'perf sched' tool
    
    This turn-key tool allows scheduler measurements to be
    conducted and the results be displayed numerically.
    
    First baby step towards that goal: clone the new command off of
    perf trace.
    
    Fix a few other details along the way:
    
     - add (minimal) perf trace documentation
    
     - reorder a few places
    
     - list perf trace in the mainporcelain list as well
       as it's a very useful utility.
    
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/tools/perf/builtin-sched.c b/tools/perf/builtin-sched.c
new file mode 100644
index 000000000000..60228d9179ac
--- /dev/null
+++ b/tools/perf/builtin-sched.c
@@ -0,0 +1,297 @@
+#include "builtin.h"
+
+#include "util/util.h"
+#include "util/cache.h"
+#include "util/symbol.h"
+#include "util/thread.h"
+#include "util/header.h"
+
+#include "util/parse-options.h"
+
+#include "perf.h"
+#include "util/debug.h"
+
+#include "util/trace-event.h"
+
+static char		const *input_name = "perf.data";
+static int		input;
+static unsigned long	page_size;
+static unsigned long	mmap_window = 32;
+
+static unsigned long	total = 0;
+static unsigned long	total_comm = 0;
+
+static struct rb_root	threads;
+static struct thread	*last_match;
+
+static struct perf_header *header;
+static u64		sample_type;
+
+
+static int
+process_comm_event(event_t *event, unsigned long offset, unsigned long head)
+{
+	struct thread *thread;
+
+	thread = threads__findnew(event->comm.pid, &threads, &last_match);
+
+	dump_printf("%p [%p]: PERF_EVENT_COMM: %s:%d\n",
+		(void *)(offset + head),
+		(void *)(long)(event->header.size),
+		event->comm.comm, event->comm.pid);
+
+	if (thread == NULL ||
+	    thread__set_comm(thread, event->comm.comm)) {
+		dump_printf("problem processing PERF_EVENT_COMM, skipping event.\n");
+		return -1;
+	}
+	total_comm++;
+
+	return 0;
+}
+
+static int
+process_sample_event(event_t *event, unsigned long offset, unsigned long head)
+{
+	char level;
+	int show = 0;
+	struct dso *dso = NULL;
+	struct thread *thread;
+	u64 ip = event->ip.ip;
+	u64 timestamp = -1;
+	u32 cpu = -1;
+	u64 period = 1;
+	void *more_data = event->ip.__more_data;
+	int cpumode;
+
+	thread = threads__findnew(event->ip.pid, &threads, &last_match);
+
+	if (sample_type & PERF_SAMPLE_TIME) {
+		timestamp = *(u64 *)more_data;
+		more_data += sizeof(u64);
+	}
+
+	if (sample_type & PERF_SAMPLE_CPU) {
+		cpu = *(u32 *)more_data;
+		more_data += sizeof(u32);
+		more_data += sizeof(u32); /* reserved */
+	}
+
+	if (sample_type & PERF_SAMPLE_PERIOD) {
+		period = *(u64 *)more_data;
+		more_data += sizeof(u64);
+	}
+
+	dump_printf("%p [%p]: PERF_EVENT_SAMPLE (IP, %d): %d/%d: %p period: %Ld\n",
+		(void *)(offset + head),
+		(void *)(long)(event->header.size),
+		event->header.misc,
+		event->ip.pid, event->ip.tid,
+		(void *)(long)ip,
+		(long long)period);
+
+	dump_printf(" ... thread: %s:%d\n", thread->comm, thread->pid);
+
+	if (thread == NULL) {
+		eprintf("problem processing %d event, skipping it.\n",
+			event->header.type);
+		return -1;
+	}
+
+	cpumode = event->header.misc & PERF_EVENT_MISC_CPUMODE_MASK;
+
+	if (cpumode == PERF_EVENT_MISC_KERNEL) {
+		show = SHOW_KERNEL;
+		level = 'k';
+
+		dso = kernel_dso;
+
+		dump_printf(" ...... dso: %s\n", dso->name);
+
+	} else if (cpumode == PERF_EVENT_MISC_USER) {
+
+		show = SHOW_USER;
+		level = '.';
+
+	} else {
+		show = SHOW_HV;
+		level = 'H';
+
+		dso = hypervisor_dso;
+
+		dump_printf(" ...... dso: [hypervisor]\n");
+	}
+
+	if (sample_type & PERF_SAMPLE_RAW) {
+		struct {
+			u32 size;
+			char data[0];
+		} *raw = more_data;
+
+		/*
+		 * FIXME: better resolve from pid from the struct trace_entry
+		 * field, although it should be the same than this perf
+		 * event pid
+		 */
+		print_event(cpu, raw->data, raw->size, timestamp, thread->comm);
+	}
+	total += period;
+
+	return 0;
+}
+
+static int
+process_event(event_t *event, unsigned long offset, unsigned long head)
+{
+	trace_event(event);
+
+	switch (event->header.type) {
+	case PERF_EVENT_MMAP ... PERF_EVENT_LOST:
+		return 0;
+
+	case PERF_EVENT_COMM:
+		return process_comm_event(event, offset, head);
+
+	case PERF_EVENT_EXIT ... PERF_EVENT_READ:
+		return 0;
+
+	case PERF_EVENT_SAMPLE:
+		return process_sample_event(event, offset, head);
+
+	case PERF_EVENT_MAX:
+	default:
+		return -1;
+	}
+
+	return 0;
+}
+
+static int __cmd_sched(void)
+{
+	int ret, rc = EXIT_FAILURE;
+	unsigned long offset = 0;
+	unsigned long head = 0;
+	struct stat perf_stat;
+	event_t *event;
+	uint32_t size;
+	char *buf;
+
+	trace_report();
+	register_idle_thread(&threads, &last_match);
+
+	input = open(input_name, O_RDONLY);
+	if (input < 0) {
+		perror("failed to open file");
+		exit(-1);
+	}
+
+	ret = fstat(input, &perf_stat);
+	if (ret < 0) {
+		perror("failed to stat file");
+		exit(-1);
+	}
+
+	if (!perf_stat.st_size) {
+		fprintf(stderr, "zero-sized file, nothing to do!\n");
+		exit(0);
+	}
+	header = perf_header__read(input);
+	head = header->data_offset;
+	sample_type = perf_header__sample_type(header);
+
+	if (!(sample_type & PERF_SAMPLE_RAW))
+		die("No trace sample to read. Did you call perf record "
+		    "without -R?");
+
+	if (load_kernel() < 0) {
+		perror("failed to load kernel symbols");
+		return EXIT_FAILURE;
+	}
+
+remap:
+	buf = (char *)mmap(NULL, page_size * mmap_window, PROT_READ,
+			   MAP_SHARED, input, offset);
+	if (buf == MAP_FAILED) {
+		perror("failed to mmap file");
+		exit(-1);
+	}
+
+more:
+	event = (event_t *)(buf + head);
+
+	size = event->header.size;
+	if (!size)
+		size = 8;
+
+	if (head + event->header.size >= page_size * mmap_window) {
+		unsigned long shift = page_size * (head / page_size);
+		int res;
+
+		res = munmap(buf, page_size * mmap_window);
+		assert(res == 0);
+
+		offset += shift;
+		head -= shift;
+		goto remap;
+	}
+
+	size = event->header.size;
+
+
+	if (!size || process_event(event, offset, head) < 0) {
+
+		/*
+		 * assume we lost track of the stream, check alignment, and
+		 * increment a single u64 in the hope to catch on again 'soon'.
+		 */
+
+		if (unlikely(head & 7))
+			head &= ~7ULL;
+
+		size = 8;
+	}
+
+	head += size;
+
+	if (offset + head < (unsigned long)perf_stat.st_size)
+		goto more;
+
+	rc = EXIT_SUCCESS;
+	close(input);
+
+	return rc;
+}
+
+static const char * const annotate_usage[] = {
+	"perf trace [<options>] <command>",
+	NULL
+};
+
+static const struct option options[] = {
+	OPT_BOOLEAN('D', "dump-raw-trace", &dump_trace,
+		    "dump raw trace in ASCII"),
+	OPT_BOOLEAN('v', "verbose", &verbose,
+		    "be more verbose (show symbol address, etc)"),
+	OPT_END()
+};
+
+int cmd_sched(int argc, const char **argv, const char *prefix __used)
+{
+	symbol__init();
+	page_size = getpagesize();
+
+	argc = parse_options(argc, argv, options, annotate_usage, 0);
+	if (argc) {
+		/*
+		 * Special case: if there's an argument left then assume tha
+		 * it's a symbol filter:
+		 */
+		if (argc > 1)
+			usage_with_options(annotate_usage, options);
+	}
+
+
+	setup_pager();
+
+	return __cmd_sched();
+}
