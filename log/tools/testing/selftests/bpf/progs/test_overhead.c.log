commit da07f52d3caf6c24c6dbffb5500f379d819e04bd
Merge: 93d43e58683e f85c1598ddfe
Author: David S. Miller <davem@davemloft.net>
Date:   Fri May 15 13:48:59 2020 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net
    
    Move the bpf verifier trace check into the new switch statement in
    HEAD.
    
    Resolve the overlapping changes in hinic, where bug fixes overlap
    the addition of VF support.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 6d74f64b922b8394dccc52576659cb0dc0a1da7b
Author: Yonghong Song <yhs@fb.com>
Date:   Wed May 13 22:32:07 2020 -0700

    selftests/bpf: Enforce returning 0 for fentry/fexit programs
    
    There are a few fentry/fexit programs returning non-0.
    The tests with these programs will break with the previous
    patch which enfoced return-0 rules. Fix them properly.
    
    Fixes: ac065870d928 ("selftests/bpf: Add BPF_PROG, BPF_KPROBE, and BPF_KRETPROBE macros")
    Signed-off-by: Yonghong Song <yhs@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Andrii Nakryiko <andriin@fb.com>
    Link: https://lore.kernel.org/bpf/20200514053207.1298479-1-yhs@fb.com

diff --git a/tools/testing/selftests/bpf/progs/test_overhead.c b/tools/testing/selftests/bpf/progs/test_overhead.c
index 56a50b25cd33..abb7344b531f 100644
--- a/tools/testing/selftests/bpf/progs/test_overhead.c
+++ b/tools/testing/selftests/bpf/progs/test_overhead.c
@@ -30,13 +30,13 @@ int prog3(struct bpf_raw_tracepoint_args *ctx)
 SEC("fentry/__set_task_comm")
 int BPF_PROG(prog4, struct task_struct *tsk, const char *buf, bool exec)
 {
-	return !tsk;
+	return 0;
 }
 
 SEC("fexit/__set_task_comm")
 int BPF_PROG(prog5, struct task_struct *tsk, const char *buf, bool exec)
 {
-	return !tsk;
+	return 0;
 }
 
 char _license[] SEC("license") = "GPL";

commit 4eaf0b5c5e04c21a866431bd763ab4b1f24c4d16
Author: Andrii Nakryiko <andriin@fb.com>
Date:   Tue May 12 12:24:44 2020 -0700

    selftest/bpf: Fmod_ret prog and implement test_overhead as part of bench
    
    Add fmod_ret BPF program to existing test_overhead selftest. Also re-implement
    user-space benchmarking part into benchmark runner to compare results. Results
    with ./bench are consistently somewhat lower than test_overhead's, but relative
    performance of various types of BPF programs stay consisten (e.g., kretprobe is
    noticeably slower). This slowdown seems to be coming from the fact that
    test_overhead is single-threaded, while benchmark always spins off at least
    one thread for producer. This has been confirmed by hacking multi-threaded
    test_overhead variant and also single-threaded bench variant. Resutls are
    below. run_bench_rename.sh script from benchs/ subdirectory was used to
    produce results for ./bench.
    
    Single-threaded implementations
    ===============================
    
    /* bench: single-threaded, atomics */
    base      :    4.622 ± 0.049M/s
    kprobe    :    3.673 ± 0.052M/s
    kretprobe :    2.625 ± 0.052M/s
    rawtp     :    4.369 ± 0.089M/s
    fentry    :    4.201 ± 0.558M/s
    fexit     :    4.309 ± 0.148M/s
    fmodret   :    4.314 ± 0.203M/s
    
    /* selftest: single-threaded, no atomics */
    task_rename base        4555K events per sec
    task_rename kprobe      3643K events per sec
    task_rename kretprobe   2506K events per sec
    task_rename raw_tp      4303K events per sec
    task_rename fentry      4307K events per sec
    task_rename fexit       4010K events per sec
    task_rename fmod_ret    3984K events per sec
    
    Multi-threaded implementations
    ==============================
    
    /* bench: multi-threaded w/ atomics */
    base      :    3.910 ± 0.023M/s
    kprobe    :    3.048 ± 0.037M/s
    kretprobe :    2.300 ± 0.015M/s
    rawtp     :    3.687 ± 0.034M/s
    fentry    :    3.740 ± 0.087M/s
    fexit     :    3.510 ± 0.009M/s
    fmodret   :    3.485 ± 0.050M/s
    
    /* selftest: multi-threaded w/ atomics */
    task_rename base        3872K events per sec
    task_rename kprobe      3068K events per sec
    task_rename kretprobe   2350K events per sec
    task_rename raw_tp      3731K events per sec
    task_rename fentry      3639K events per sec
    task_rename fexit       3558K events per sec
    task_rename fmod_ret    3511K events per sec
    
    /* selftest: multi-threaded, no atomics */
    task_rename base        3945K events per sec
    task_rename kprobe      3298K events per sec
    task_rename kretprobe   2451K events per sec
    task_rename raw_tp      3718K events per sec
    task_rename fentry      3782K events per sec
    task_rename fexit       3543K events per sec
    task_rename fmod_ret    3526K events per sec
    
    Note that the fact that ./bench benchmark always uses atomic increments for
    counting, while test_overhead doesn't, doesn't influence test results all that
    much.
    
    Signed-off-by: Andrii Nakryiko <andriin@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: John Fastabend <john.fastabend@gmail.com>
    Acked-by: Yonghong Song <yhs@fb.com>
    Link: https://lore.kernel.org/bpf/20200512192445.2351848-4-andriin@fb.com

diff --git a/tools/testing/selftests/bpf/progs/test_overhead.c b/tools/testing/selftests/bpf/progs/test_overhead.c
index 56a50b25cd33..450bf819beac 100644
--- a/tools/testing/selftests/bpf/progs/test_overhead.c
+++ b/tools/testing/selftests/bpf/progs/test_overhead.c
@@ -39,4 +39,10 @@ int BPF_PROG(prog5, struct task_struct *tsk, const char *buf, bool exec)
 	return !tsk;
 }
 
+SEC("fmod_ret/__set_task_comm")
+int BPF_PROG(prog6, struct task_struct *tsk, const char *buf, bool exec)
+{
+	return !tsk;
+}
+
 char _license[] SEC("license") = "GPL";

commit df8ff35311c8d10d90b4604c02b32c361dc997aa
Author: Andrii Nakryiko <andriin@fb.com>
Date:   Sat Feb 29 15:11:12 2020 -0800

    libbpf: Merge selftests' bpf_trace_helpers.h into libbpf's bpf_tracing.h
    
    Move BPF_PROG, BPF_KPROBE, and BPF_KRETPROBE macro into libbpf's bpf_tracing.h
    header to make it available for non-selftests users.
    
    Signed-off-by: Andrii Nakryiko <andriin@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/bpf/20200229231112.1240137-5-andriin@fb.com

diff --git a/tools/testing/selftests/bpf/progs/test_overhead.c b/tools/testing/selftests/bpf/progs/test_overhead.c
index f43714c69cc8..56a50b25cd33 100644
--- a/tools/testing/selftests/bpf/progs/test_overhead.c
+++ b/tools/testing/selftests/bpf/progs/test_overhead.c
@@ -6,7 +6,6 @@
 #include <linux/ptrace.h>
 #include <bpf/bpf_helpers.h>
 #include <bpf/bpf_tracing.h>
-#include "bpf_trace_helpers.h"
 
 struct task_struct;
 

commit 396f544ed5e5a9c40de5663b774f643644cba059
Author: Andrii Nakryiko <andriin@fb.com>
Date:   Sat Feb 29 15:11:11 2020 -0800

    selftests/bpf: Fix BPF_KRETPROBE macro and use it in attach_probe test
    
    For kretprobes, there is no point in capturing input arguments from pt_regs,
    as they are going to be, most probably, clobbered by the time probed kernel
    function returns. So switch BPF_KRETPROBE to accept zero or one argument
    (optional return result).
    
    Fixes: ac065870d928 ("selftests/bpf: Add BPF_PROG, BPF_KPROBE, and BPF_KRETPROBE macros")
    Signed-off-by: Andrii Nakryiko <andriin@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/bpf/20200229231112.1240137-4-andriin@fb.com

diff --git a/tools/testing/selftests/bpf/progs/test_overhead.c b/tools/testing/selftests/bpf/progs/test_overhead.c
index bfe9fbcb9684..f43714c69cc8 100644
--- a/tools/testing/selftests/bpf/progs/test_overhead.c
+++ b/tools/testing/selftests/bpf/progs/test_overhead.c
@@ -17,11 +17,9 @@ int BPF_KPROBE(prog1, struct task_struct *tsk, const char *buf, bool exec)
 }
 
 SEC("kretprobe/__set_task_comm")
-int BPF_KRETPROBE(prog2,
-		  struct task_struct *tsk, const char *buf, bool exec,
-		  int ret)
+int BPF_KRETPROBE(prog2, int ret)
 {
-	return !PT_REGS_PARM1(ctx) && ret;
+	return ret;
 }
 
 SEC("raw_tp/task_rename")

commit 3e689141e64df91b009a289c0559adedfe62f511
Author: Toke Høiland-Jørgensen <toke@redhat.com>
Date:   Mon Jan 20 14:06:45 2020 +0100

    selftests: Use consistent include paths for libbpf
    
    Fix all selftests to include libbpf header files with the bpf/ prefix, to
    be consistent with external users of the library. Also ensure that all
    includes of exported libbpf header files (those that are exported on 'make
    install' of the library) use bracketed includes instead of quoted.
    
    To not break the build, keep the old include path until everything has been
    changed to the new one; a subsequent patch will remove that.
    
    Fixes: 6910d7d3867a ("selftests/bpf: Ensure bpf_helper_defs.h are taken from selftests dir")
    Signed-off-by: Toke Høiland-Jørgensen <toke@redhat.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Andrii Nakryiko <andriin@fb.com>
    Link: https://lore.kernel.org/bpf/157952560568.1683545.9649335788846513446.stgit@toke.dk

diff --git a/tools/testing/selftests/bpf/progs/test_overhead.c b/tools/testing/selftests/bpf/progs/test_overhead.c
index 48748297b860..bfe9fbcb9684 100644
--- a/tools/testing/selftests/bpf/progs/test_overhead.c
+++ b/tools/testing/selftests/bpf/progs/test_overhead.c
@@ -4,8 +4,8 @@
 #include <stddef.h>
 #include <linux/bpf.h>
 #include <linux/ptrace.h>
-#include "bpf_helpers.h"
-#include "bpf_tracing.h"
+#include <bpf/bpf_helpers.h>
+#include <bpf/bpf_tracing.h>
 #include "bpf_trace_helpers.h"
 
 struct task_struct;

commit ac065870d92824ee0bc275ab71fd8901dbde5055
Author: Andrii Nakryiko <andriin@fb.com>
Date:   Fri Jan 10 13:16:34 2020 -0800

    selftests/bpf: Add BPF_PROG, BPF_KPROBE, and BPF_KRETPROBE macros
    
    Streamline BPF_TRACE_x macro by moving out return type and section attribute
    definition out of macro itself. That makes those function look in source code
    similar to other BPF programs. Additionally, simplify its usage by determining
    number of arguments automatically (so just single BPF_TRACE vs a family of
    BPF_TRACE_1, BPF_TRACE_2, etc). Also, allow more natural function argument
    syntax without commas inbetween argument type and name.
    
    Given this helper is useful not only for tracing tp_btf/fenty/fexit programs,
    but could be used for LSM programs and others following the same pattern,
    rename BPF_TRACE macro into more generic BPF_PROG. Existing BPF_TRACE_x
    usages in selftests are converted to new BPF_PROG macro.
    
    Following the same pattern, define BPF_KPROBE and BPF_KRETPROBE macros for
    nicer usage of kprobe/kretprobe arguments, respectively. BPF_KRETPROBE, adopts
    same convention used by fexit programs, that last defined argument is probed
    function's return result.
    
    v4->v5:
    - fix test_overhead test (__set_task_comm is void) (Alexei);
    
    v3->v4:
    - rebased and fixed one more BPF_TRACE_x occurence (Alexei);
    
    v2->v3:
    - rename to shorter and as generic BPF_PROG (Alexei);
    
    v1->v2:
    - verified GCC handles pragmas as expected;
    - added descriptions to macros;
    - converted new STRUCT_OPS selftest to BPF_HANDLER (worked as expected);
    - added original context as 'ctx' parameter, for cases where it has to be
      passed into BPF helpers. This might cause an accidental naming collision,
      unfortunately, but at least it's easy to work around. Fortunately, this
      situation produces quite legible compilation error:
    
    progs/bpf_dctcp.c:46:6: error: redefinition of 'ctx' with a different type: 'int' vs 'unsigned long long *'
            int ctx = 123;
                ^
    progs/bpf_dctcp.c:42:6: note: previous definition is here
    void BPF_HANDLER(dctcp_init, struct sock *sk)
         ^
    ./bpf_trace_helpers.h:58:32: note: expanded from macro 'BPF_HANDLER'
    ____##name(unsigned long long *ctx, ##args)
    
    Signed-off-by: Andrii Nakryiko <andriin@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Link: https://lore.kernel.org/bpf/20200110211634.1614739-1-andriin@fb.com

diff --git a/tools/testing/selftests/bpf/progs/test_overhead.c b/tools/testing/selftests/bpf/progs/test_overhead.c
index 96c0124a04ba..48748297b860 100644
--- a/tools/testing/selftests/bpf/progs/test_overhead.c
+++ b/tools/testing/selftests/bpf/progs/test_overhead.c
@@ -1,39 +1,45 @@
 // SPDX-License-Identifier: GPL-2.0
 /* Copyright (c) 2019 Facebook */
+#include <stdbool.h>
+#include <stddef.h>
 #include <linux/bpf.h>
+#include <linux/ptrace.h>
 #include "bpf_helpers.h"
 #include "bpf_tracing.h"
 #include "bpf_trace_helpers.h"
 
+struct task_struct;
+
 SEC("kprobe/__set_task_comm")
-int prog1(struct pt_regs *ctx)
+int BPF_KPROBE(prog1, struct task_struct *tsk, const char *buf, bool exec)
 {
-	return 0;
+	return !tsk;
 }
 
 SEC("kretprobe/__set_task_comm")
-int prog2(struct pt_regs *ctx)
+int BPF_KRETPROBE(prog2,
+		  struct task_struct *tsk, const char *buf, bool exec,
+		  int ret)
 {
-	return 0;
+	return !PT_REGS_PARM1(ctx) && ret;
 }
 
 SEC("raw_tp/task_rename")
 int prog3(struct bpf_raw_tracepoint_args *ctx)
 {
-	return 0;
+	return !ctx->args[0];
 }
 
-struct task_struct;
-BPF_TRACE_3("fentry/__set_task_comm", prog4,
-	    struct task_struct *, tsk, const char *, buf, __u8, exec)
+SEC("fentry/__set_task_comm")
+int BPF_PROG(prog4, struct task_struct *tsk, const char *buf, bool exec)
 {
-	return 0;
+	return !tsk;
 }
 
-BPF_TRACE_3("fexit/__set_task_comm", prog5,
-	    struct task_struct *, tsk, const char *, buf, __u8, exec)
+SEC("fexit/__set_task_comm")
+int BPF_PROG(prog5, struct task_struct *tsk, const char *buf, bool exec)
 {
-	return 0;
+	return !tsk;
 }
 
 char _license[] SEC("license") = "GPL";

commit f9a7cf6eb17cd0110c8c47d9e7969fc2716e5772
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Sat Nov 23 12:25:04 2019 -0800

    bpf: Introduce BPF_TRACE_x helper for the tracing tests
    
    For BPF_PROG_TYPE_TRACING, the bpf_prog's ctx is an array of u64.
    This patch borrows the idea from BPF_CALL_x in filter.h to
    convert a u64 to the arg type of the traced function.
    
    The new BPF_TRACE_x has an arg to specify the return type of a bpf_prog.
    It will be used in the future TCP-ops bpf_prog that may return "void".
    
    The new macros are defined in the new header file "bpf_trace_helpers.h".
    It is under selftests/bpf/ for now.  It could be moved to libbpf later
    after seeing more upcoming non-tracing use cases.
    
    The tests are changed to use these new macros also.  Hence,
    the k[s]u8/16/32/64 are no longer needed and they are removed
    from the bpf_helpers.h.
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/bpf/20191123202504.1502696-1-kafai@fb.com

diff --git a/tools/testing/selftests/bpf/progs/test_overhead.c b/tools/testing/selftests/bpf/progs/test_overhead.c
index ef06b2693f96..96c0124a04ba 100644
--- a/tools/testing/selftests/bpf/progs/test_overhead.c
+++ b/tools/testing/selftests/bpf/progs/test_overhead.c
@@ -3,6 +3,7 @@
 #include <linux/bpf.h>
 #include "bpf_helpers.h"
 #include "bpf_tracing.h"
+#include "bpf_trace_helpers.h"
 
 SEC("kprobe/__set_task_comm")
 int prog1(struct pt_regs *ctx)
@@ -22,20 +23,15 @@ int prog3(struct bpf_raw_tracepoint_args *ctx)
 	return 0;
 }
 
-struct __set_task_comm_args {
-	struct task_struct *tsk;
-	const char *buf;
-	ku8 exec;
-};
-
-SEC("fentry/__set_task_comm")
-int prog4(struct __set_task_comm_args *ctx)
+struct task_struct;
+BPF_TRACE_3("fentry/__set_task_comm", prog4,
+	    struct task_struct *, tsk, const char *, buf, __u8, exec)
 {
 	return 0;
 }
 
-SEC("fexit/__set_task_comm")
-int prog5(struct __set_task_comm_args *ctx)
+BPF_TRACE_3("fexit/__set_task_comm", prog5,
+	    struct task_struct *, tsk, const char *, buf, __u8, exec)
 {
 	return 0;
 }

commit c4781e37c6a22c39cb4a57411d14f42aca124f04
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Thu Nov 21 17:15:15 2019 -0800

    selftests/bpf: Add BPF trampoline performance test
    
    Add a test that benchmarks different ways of attaching BPF program to a kernel function.
    Here are the results for 2.4Ghz x86 cpu on a kernel without mitigations:
    $ ./test_progs -n 49 -v|grep events
    task_rename base        2743K events per sec
    task_rename kprobe      2419K events per sec
    task_rename kretprobe   1876K events per sec
    task_rename raw_tp      2578K events per sec
    task_rename fentry      2710K events per sec
    task_rename fexit       2685K events per sec
    
    On a kernel with retpoline:
    $ ./test_progs -n 49 -v|grep events
    task_rename base        2401K events per sec
    task_rename kprobe      1930K events per sec
    task_rename kretprobe   1485K events per sec
    task_rename raw_tp      2053K events per sec
    task_rename fentry      2351K events per sec
    task_rename fexit       2185K events per sec
    
    All 5 approaches:
    - kprobe/kretprobe in __set_task_comm()
    - raw tracepoint in trace_task_rename()
    - fentry/fexit in __set_task_comm()
    are roughly equivalent.
    
    __set_task_comm() by itself is quite fast, so any extra instructions add up.
    Until BPF trampoline was introduced the fastest mechanism was raw tracepoint.
    kprobe via ftrace was second best. kretprobe is slow due to trap. New
    fentry/fexit methods via BPF trampoline are clearly the fastest and the
    difference is more pronounced with retpoline on, since BPF trampoline doesn't
    use indirect jumps.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: John Fastabend <john.fastabend@gmail.com>
    Link: https://lore.kernel.org/bpf/20191122011515.255371-1-ast@kernel.org

diff --git a/tools/testing/selftests/bpf/progs/test_overhead.c b/tools/testing/selftests/bpf/progs/test_overhead.c
new file mode 100644
index 000000000000..ef06b2693f96
--- /dev/null
+++ b/tools/testing/selftests/bpf/progs/test_overhead.c
@@ -0,0 +1,43 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Copyright (c) 2019 Facebook */
+#include <linux/bpf.h>
+#include "bpf_helpers.h"
+#include "bpf_tracing.h"
+
+SEC("kprobe/__set_task_comm")
+int prog1(struct pt_regs *ctx)
+{
+	return 0;
+}
+
+SEC("kretprobe/__set_task_comm")
+int prog2(struct pt_regs *ctx)
+{
+	return 0;
+}
+
+SEC("raw_tp/task_rename")
+int prog3(struct bpf_raw_tracepoint_args *ctx)
+{
+	return 0;
+}
+
+struct __set_task_comm_args {
+	struct task_struct *tsk;
+	const char *buf;
+	ku8 exec;
+};
+
+SEC("fentry/__set_task_comm")
+int prog4(struct __set_task_comm_args *ctx)
+{
+	return 0;
+}
+
+SEC("fexit/__set_task_comm")
+int prog5(struct __set_task_comm_args *ctx)
+{
+	return 0;
+}
+
+char _license[] SEC("license") = "GPL";
