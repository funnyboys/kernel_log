commit 4eaf0b5c5e04c21a866431bd763ab4b1f24c4d16
Author: Andrii Nakryiko <andriin@fb.com>
Date:   Tue May 12 12:24:44 2020 -0700

    selftest/bpf: Fmod_ret prog and implement test_overhead as part of bench
    
    Add fmod_ret BPF program to existing test_overhead selftest. Also re-implement
    user-space benchmarking part into benchmark runner to compare results. Results
    with ./bench are consistently somewhat lower than test_overhead's, but relative
    performance of various types of BPF programs stay consisten (e.g., kretprobe is
    noticeably slower). This slowdown seems to be coming from the fact that
    test_overhead is single-threaded, while benchmark always spins off at least
    one thread for producer. This has been confirmed by hacking multi-threaded
    test_overhead variant and also single-threaded bench variant. Resutls are
    below. run_bench_rename.sh script from benchs/ subdirectory was used to
    produce results for ./bench.
    
    Single-threaded implementations
    ===============================
    
    /* bench: single-threaded, atomics */
    base      :    4.622 ± 0.049M/s
    kprobe    :    3.673 ± 0.052M/s
    kretprobe :    2.625 ± 0.052M/s
    rawtp     :    4.369 ± 0.089M/s
    fentry    :    4.201 ± 0.558M/s
    fexit     :    4.309 ± 0.148M/s
    fmodret   :    4.314 ± 0.203M/s
    
    /* selftest: single-threaded, no atomics */
    task_rename base        4555K events per sec
    task_rename kprobe      3643K events per sec
    task_rename kretprobe   2506K events per sec
    task_rename raw_tp      4303K events per sec
    task_rename fentry      4307K events per sec
    task_rename fexit       4010K events per sec
    task_rename fmod_ret    3984K events per sec
    
    Multi-threaded implementations
    ==============================
    
    /* bench: multi-threaded w/ atomics */
    base      :    3.910 ± 0.023M/s
    kprobe    :    3.048 ± 0.037M/s
    kretprobe :    2.300 ± 0.015M/s
    rawtp     :    3.687 ± 0.034M/s
    fentry    :    3.740 ± 0.087M/s
    fexit     :    3.510 ± 0.009M/s
    fmodret   :    3.485 ± 0.050M/s
    
    /* selftest: multi-threaded w/ atomics */
    task_rename base        3872K events per sec
    task_rename kprobe      3068K events per sec
    task_rename kretprobe   2350K events per sec
    task_rename raw_tp      3731K events per sec
    task_rename fentry      3639K events per sec
    task_rename fexit       3558K events per sec
    task_rename fmod_ret    3511K events per sec
    
    /* selftest: multi-threaded, no atomics */
    task_rename base        3945K events per sec
    task_rename kprobe      3298K events per sec
    task_rename kretprobe   2451K events per sec
    task_rename raw_tp      3718K events per sec
    task_rename fentry      3782K events per sec
    task_rename fexit       3543K events per sec
    task_rename fmod_ret    3526K events per sec
    
    Note that the fact that ./bench benchmark always uses atomic increments for
    counting, while test_overhead doesn't, doesn't influence test results all that
    much.
    
    Signed-off-by: Andrii Nakryiko <andriin@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: John Fastabend <john.fastabend@gmail.com>
    Acked-by: Yonghong Song <yhs@fb.com>
    Link: https://lore.kernel.org/bpf/20200512192445.2351848-4-andriin@fb.com

diff --git a/tools/testing/selftests/bpf/prog_tests/test_overhead.c b/tools/testing/selftests/bpf/prog_tests/test_overhead.c
index 465b371a561d..2702df2b2343 100644
--- a/tools/testing/selftests/bpf/prog_tests/test_overhead.c
+++ b/tools/testing/selftests/bpf/prog_tests/test_overhead.c
@@ -61,9 +61,10 @@ void test_test_overhead(void)
 	const char *raw_tp_name = "raw_tp/task_rename";
 	const char *fentry_name = "fentry/__set_task_comm";
 	const char *fexit_name = "fexit/__set_task_comm";
+	const char *fmodret_name = "fmod_ret/__set_task_comm";
 	const char *kprobe_func = "__set_task_comm";
 	struct bpf_program *kprobe_prog, *kretprobe_prog, *raw_tp_prog;
-	struct bpf_program *fentry_prog, *fexit_prog;
+	struct bpf_program *fentry_prog, *fexit_prog, *fmodret_prog;
 	struct bpf_object *obj;
 	struct bpf_link *link;
 	int err, duration = 0;
@@ -96,6 +97,10 @@ void test_test_overhead(void)
 	if (CHECK(!fexit_prog, "find_probe",
 		  "prog '%s' not found\n", fexit_name))
 		goto cleanup;
+	fmodret_prog = bpf_object__find_program_by_title(obj, fmodret_name);
+	if (CHECK(!fmodret_prog, "find_probe",
+		  "prog '%s' not found\n", fmodret_name))
+		goto cleanup;
 
 	err = bpf_object__load(obj);
 	if (CHECK(err, "obj_load", "err %d\n", err))
@@ -142,6 +147,13 @@ void test_test_overhead(void)
 		goto cleanup;
 	test_run("fexit");
 	bpf_link__destroy(link);
+
+	/* attach fmod_ret */
+	link = bpf_program__attach_trace(fmodret_prog);
+	if (CHECK(IS_ERR(link), "attach fmod_ret", "err %ld\n", PTR_ERR(link)))
+		goto cleanup;
+	test_run("fmod_ret");
+	bpf_link__destroy(link);
 cleanup:
 	prctl(PR_SET_NAME, comm, 0L, 0L, 0L);
 	bpf_object__close(obj);

commit e43002242a47e8d7b2f9446f54de982f09e7dbd1
Author: Stanislav Fomichev <sdf@google.com>
Date:   Wed Jan 8 11:21:32 2020 -0800

    selftests/bpf: Restore original comm in test_overhead
    
    test_overhead changes task comm in order to estimate BPF trampoline
    overhead but never sets the comm back to the original one.
    We have the tests (like core_reloc.c) that have 'test_progs'
    as hard-coded expected comm, so let's try to preserve the
    original comm.
    
    Currently, everything works because the order of execution is:
    first core_recloc, then test_overhead; but let's make it a bit
    future-proof.
    
    Other related changes: use 'test_overhead' as new comm instead of
    'test' to make it easy to debug and drop '\n' at the end.
    
    Signed-off-by: Stanislav Fomichev <sdf@google.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Petar Penkov <ppenkov@google.com>
    Acked-by: Song Liu <songliubraving@fb.com>
    Link: https://lore.kernel.org/bpf/20200108192132.189221-1-sdf@google.com

diff --git a/tools/testing/selftests/bpf/prog_tests/test_overhead.c b/tools/testing/selftests/bpf/prog_tests/test_overhead.c
index c32aa28bd93f..465b371a561d 100644
--- a/tools/testing/selftests/bpf/prog_tests/test_overhead.c
+++ b/tools/testing/selftests/bpf/prog_tests/test_overhead.c
@@ -2,6 +2,7 @@
 /* Copyright (c) 2019 Facebook */
 #define _GNU_SOURCE
 #include <sched.h>
+#include <sys/prctl.h>
 #include <test_progs.h>
 
 #define MAX_CNT 100000
@@ -17,7 +18,7 @@ static __u64 time_get_ns(void)
 static int test_task_rename(const char *prog)
 {
 	int i, fd, duration = 0, err;
-	char buf[] = "test\n";
+	char buf[] = "test_overhead";
 	__u64 start_time;
 
 	fd = open("/proc/self/comm", O_WRONLY|O_TRUNC);
@@ -66,6 +67,10 @@ void test_test_overhead(void)
 	struct bpf_object *obj;
 	struct bpf_link *link;
 	int err, duration = 0;
+	char comm[16] = {};
+
+	if (CHECK_FAIL(prctl(PR_GET_NAME, comm, 0L, 0L, 0L)))
+		return;
 
 	obj = bpf_object__open_file("./test_overhead.o", NULL);
 	if (CHECK(IS_ERR(obj), "obj_open_file", "err %ld\n", PTR_ERR(obj)))
@@ -138,5 +143,6 @@ void test_test_overhead(void)
 	test_run("fexit");
 	bpf_link__destroy(link);
 cleanup:
+	prctl(PR_SET_NAME, comm, 0L, 0L, 0L);
 	bpf_object__close(obj);
 }

commit c4781e37c6a22c39cb4a57411d14f42aca124f04
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Thu Nov 21 17:15:15 2019 -0800

    selftests/bpf: Add BPF trampoline performance test
    
    Add a test that benchmarks different ways of attaching BPF program to a kernel function.
    Here are the results for 2.4Ghz x86 cpu on a kernel without mitigations:
    $ ./test_progs -n 49 -v|grep events
    task_rename base        2743K events per sec
    task_rename kprobe      2419K events per sec
    task_rename kretprobe   1876K events per sec
    task_rename raw_tp      2578K events per sec
    task_rename fentry      2710K events per sec
    task_rename fexit       2685K events per sec
    
    On a kernel with retpoline:
    $ ./test_progs -n 49 -v|grep events
    task_rename base        2401K events per sec
    task_rename kprobe      1930K events per sec
    task_rename kretprobe   1485K events per sec
    task_rename raw_tp      2053K events per sec
    task_rename fentry      2351K events per sec
    task_rename fexit       2185K events per sec
    
    All 5 approaches:
    - kprobe/kretprobe in __set_task_comm()
    - raw tracepoint in trace_task_rename()
    - fentry/fexit in __set_task_comm()
    are roughly equivalent.
    
    __set_task_comm() by itself is quite fast, so any extra instructions add up.
    Until BPF trampoline was introduced the fastest mechanism was raw tracepoint.
    kprobe via ftrace was second best. kretprobe is slow due to trap. New
    fentry/fexit methods via BPF trampoline are clearly the fastest and the
    difference is more pronounced with retpoline on, since BPF trampoline doesn't
    use indirect jumps.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: John Fastabend <john.fastabend@gmail.com>
    Link: https://lore.kernel.org/bpf/20191122011515.255371-1-ast@kernel.org

diff --git a/tools/testing/selftests/bpf/prog_tests/test_overhead.c b/tools/testing/selftests/bpf/prog_tests/test_overhead.c
new file mode 100644
index 000000000000..c32aa28bd93f
--- /dev/null
+++ b/tools/testing/selftests/bpf/prog_tests/test_overhead.c
@@ -0,0 +1,142 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/* Copyright (c) 2019 Facebook */
+#define _GNU_SOURCE
+#include <sched.h>
+#include <test_progs.h>
+
+#define MAX_CNT 100000
+
+static __u64 time_get_ns(void)
+{
+	struct timespec ts;
+
+	clock_gettime(CLOCK_MONOTONIC, &ts);
+	return ts.tv_sec * 1000000000ull + ts.tv_nsec;
+}
+
+static int test_task_rename(const char *prog)
+{
+	int i, fd, duration = 0, err;
+	char buf[] = "test\n";
+	__u64 start_time;
+
+	fd = open("/proc/self/comm", O_WRONLY|O_TRUNC);
+	if (CHECK(fd < 0, "open /proc", "err %d", errno))
+		return -1;
+	start_time = time_get_ns();
+	for (i = 0; i < MAX_CNT; i++) {
+		err = write(fd, buf, sizeof(buf));
+		if (err < 0) {
+			CHECK(err < 0, "task rename", "err %d", errno);
+			close(fd);
+			return -1;
+		}
+	}
+	printf("task_rename %s\t%lluK events per sec\n", prog,
+	       MAX_CNT * 1000000ll / (time_get_ns() - start_time));
+	close(fd);
+	return 0;
+}
+
+static void test_run(const char *prog)
+{
+	test_task_rename(prog);
+}
+
+static void setaffinity(void)
+{
+	cpu_set_t cpuset;
+	int cpu = 0;
+
+	CPU_ZERO(&cpuset);
+	CPU_SET(cpu, &cpuset);
+	sched_setaffinity(0, sizeof(cpuset), &cpuset);
+}
+
+void test_test_overhead(void)
+{
+	const char *kprobe_name = "kprobe/__set_task_comm";
+	const char *kretprobe_name = "kretprobe/__set_task_comm";
+	const char *raw_tp_name = "raw_tp/task_rename";
+	const char *fentry_name = "fentry/__set_task_comm";
+	const char *fexit_name = "fexit/__set_task_comm";
+	const char *kprobe_func = "__set_task_comm";
+	struct bpf_program *kprobe_prog, *kretprobe_prog, *raw_tp_prog;
+	struct bpf_program *fentry_prog, *fexit_prog;
+	struct bpf_object *obj;
+	struct bpf_link *link;
+	int err, duration = 0;
+
+	obj = bpf_object__open_file("./test_overhead.o", NULL);
+	if (CHECK(IS_ERR(obj), "obj_open_file", "err %ld\n", PTR_ERR(obj)))
+		return;
+
+	kprobe_prog = bpf_object__find_program_by_title(obj, kprobe_name);
+	if (CHECK(!kprobe_prog, "find_probe",
+		  "prog '%s' not found\n", kprobe_name))
+		goto cleanup;
+	kretprobe_prog = bpf_object__find_program_by_title(obj, kretprobe_name);
+	if (CHECK(!kretprobe_prog, "find_probe",
+		  "prog '%s' not found\n", kretprobe_name))
+		goto cleanup;
+	raw_tp_prog = bpf_object__find_program_by_title(obj, raw_tp_name);
+	if (CHECK(!raw_tp_prog, "find_probe",
+		  "prog '%s' not found\n", raw_tp_name))
+		goto cleanup;
+	fentry_prog = bpf_object__find_program_by_title(obj, fentry_name);
+	if (CHECK(!fentry_prog, "find_probe",
+		  "prog '%s' not found\n", fentry_name))
+		goto cleanup;
+	fexit_prog = bpf_object__find_program_by_title(obj, fexit_name);
+	if (CHECK(!fexit_prog, "find_probe",
+		  "prog '%s' not found\n", fexit_name))
+		goto cleanup;
+
+	err = bpf_object__load(obj);
+	if (CHECK(err, "obj_load", "err %d\n", err))
+		goto cleanup;
+
+	setaffinity();
+
+	/* base line run */
+	test_run("base");
+
+	/* attach kprobe */
+	link = bpf_program__attach_kprobe(kprobe_prog, false /* retprobe */,
+					  kprobe_func);
+	if (CHECK(IS_ERR(link), "attach_kprobe", "err %ld\n", PTR_ERR(link)))
+		goto cleanup;
+	test_run("kprobe");
+	bpf_link__destroy(link);
+
+	/* attach kretprobe */
+	link = bpf_program__attach_kprobe(kretprobe_prog, true /* retprobe */,
+					  kprobe_func);
+	if (CHECK(IS_ERR(link), "attach kretprobe", "err %ld\n", PTR_ERR(link)))
+		goto cleanup;
+	test_run("kretprobe");
+	bpf_link__destroy(link);
+
+	/* attach raw_tp */
+	link = bpf_program__attach_raw_tracepoint(raw_tp_prog, "task_rename");
+	if (CHECK(IS_ERR(link), "attach fentry", "err %ld\n", PTR_ERR(link)))
+		goto cleanup;
+	test_run("raw_tp");
+	bpf_link__destroy(link);
+
+	/* attach fentry */
+	link = bpf_program__attach_trace(fentry_prog);
+	if (CHECK(IS_ERR(link), "attach fentry", "err %ld\n", PTR_ERR(link)))
+		goto cleanup;
+	test_run("fentry");
+	bpf_link__destroy(link);
+
+	/* attach fexit */
+	link = bpf_program__attach_trace(fexit_prog);
+	if (CHECK(IS_ERR(link), "attach fexit", "err %ld\n", PTR_ERR(link)))
+		goto cleanup;
+	test_run("fexit");
+	bpf_link__destroy(link);
+cleanup:
+	bpf_object__close(obj);
+}
