commit d94ecfc399715f06da347922e7979c088b1d8834
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Jun 5 19:44:09 2020 +0800

    blk-mq: split out a __blk_mq_get_driver_tag helper
    
    Allocation of the driver tag in the case of using a scheduler shares very
    little code with the "normal" tag allocation.  Split out a new helper to
    streamline this path, and untangle it from the complex normal tag
    allocation.
    
    This way also avoids to fail driver tag allocation because of inactive hctx
    during cpu hotplug, and fixes potential hang risk.
    
    Fixes: bf0beec0607d ("blk-mq: drain I/O when all CPUs in a hctx are offline")
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Tested-by: John Garry <john.garry@huawei.com>
    Cc: Dongli Zhang <dongli.zhang@oracle.com>
    Cc: Hannes Reinecke <hare@suse.de>
    Cc: Daniel Wagner <dwagner@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index a139b0631817..b3ce0f3a2ad2 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -44,7 +44,6 @@ bool blk_mq_dispatch_rq_list(struct request_queue *, struct list_head *, bool);
 void blk_mq_add_to_requeue_list(struct request *rq, bool at_head,
 				bool kick_requeue_list);
 void blk_mq_flush_busy_ctxs(struct blk_mq_hw_ctx *hctx, struct list_head *list);
-bool blk_mq_get_driver_tag(struct request *rq);
 struct request *blk_mq_dequeue_from_ctx(struct blk_mq_hw_ctx *hctx,
 					struct blk_mq_ctx *start);
 

commit 766473681c131f2da81d62472864c8c97e021373
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri May 29 15:53:12 2020 +0200

    blk-mq: use BLK_MQ_NO_TAG in more places
    
    Replace various magic -1 constants for tags with BLK_MQ_NO_TAG.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Hannes Reinecke <hare@suse.de>
    Reviewed-by: Johannes Thumshirn <johannes.thumshirn@wdc.com>
    Reviewed-by: Bart Van Assche <bvanassche@acm.org>
    Reviewed-by: Daniel Wagner <dwagner@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 10bfdfb494fa..a139b0631817 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -201,7 +201,7 @@ static inline void __blk_mq_put_driver_tag(struct blk_mq_hw_ctx *hctx,
 					   struct request *rq)
 {
 	blk_mq_put_tag(hctx->tags, rq->mq_ctx, rq->tag);
-	rq->tag = -1;
+	rq->tag = BLK_MQ_NO_TAG;
 
 	if (rq->rq_flags & RQF_MQ_INFLIGHT) {
 		rq->rq_flags &= ~RQF_MQ_INFLIGHT;
@@ -211,7 +211,7 @@ static inline void __blk_mq_put_driver_tag(struct blk_mq_hw_ctx *hctx,
 
 static inline void blk_mq_put_driver_tag(struct request *rq)
 {
-	if (rq->tag == -1 || rq->internal_tag == -1)
+	if (rq->tag == BLK_MQ_NO_TAG || rq->internal_tag == BLK_MQ_NO_TAG)
 		return;
 
 	__blk_mq_put_driver_tag(rq->mq_hctx, rq);

commit cae740a04b4d6d5166f19ee5faf04ea2a1f34b3d
Author: John Garry <john.garry@huawei.com>
Date:   Wed Feb 26 20:10:15 2020 +0800

    blk-mq: Remove some unused function arguments
    
    The struct blk_mq_hw_ctx pointer argument in blk_mq_put_tag(),
    blk_mq_poll_nsecs(), and blk_mq_poll_hybrid_sleep() is unused, so remove
    it.
    
    Overall obj code size shows a minor reduction, before:
       text    data     bss     dec     hex filename
      27306    1312       0   28618    6fca block/blk-mq.o
       4303     272       0    4575    11df block/blk-mq-tag.o
    
    after:
      27282    1312       0   28594    6fb2 block/blk-mq.o
       4311     272       0    4583    11e7 block/blk-mq-tag.o
    
    Reviewed-by: Johannes Thumshirn <johannes.thumshirn@wdc.com>
    Reviewed-by: Hannes Reinecke <hare@suse.de>
    Signed-off-by: John Garry <john.garry@huawei.com>
    --
    This minor patch had been carried as part of the blk-mq shared tags RFC,
    I'd rather not carry it anymore as it required rebasing, so now or never..
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index c0fa34378eb2..10bfdfb494fa 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -200,7 +200,7 @@ static inline bool blk_mq_get_dispatch_budget(struct blk_mq_hw_ctx *hctx)
 static inline void __blk_mq_put_driver_tag(struct blk_mq_hw_ctx *hctx,
 					   struct request *rq)
 {
-	blk_mq_put_tag(hctx, hctx->tags, rq->mq_ctx, rq->tag);
+	blk_mq_put_tag(hctx->tags, rq->mq_ctx, rq->tag);
 	rq->tag = -1;
 
 	if (rq->rq_flags & RQF_MQ_INFLIGHT) {

commit 01e99aeca3979600302913cef3f89076786f32c8
Author: Ming Lei <ming.lei@redhat.com>
Date:   Tue Feb 25 09:04:32 2020 +0800

    blk-mq: insert passthrough request into hctx->dispatch directly
    
    For some reason, device may be in one situation which can't handle
    FS request, so STS_RESOURCE is always returned and the FS request
    will be added to hctx->dispatch. However passthrough request may
    be required at that time for fixing the problem. If passthrough
    request is added to scheduler queue, there isn't any chance for
    blk-mq to dispatch it given we prioritize requests in hctx->dispatch.
    Then the FS IO request may never be completed, and IO hang is caused.
    
    So passthrough request has to be added to hctx->dispatch directly
    for fixing the IO hang.
    
    Fix this issue by inserting passthrough request into hctx->dispatch
    directly together withing adding FS request to the tail of
    hctx->dispatch in blk_mq_dispatch_rq_list(). Actually we add FS request
    to tail of hctx->dispatch at default, see blk_mq_request_bypass_insert().
    
    Then it becomes consistent with original legacy IO request
    path, in which passthrough request is always added to q->queue_head.
    
    Cc: Dongli Zhang <dongli.zhang@oracle.com>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Ewan D. Milne <emilne@redhat.com>
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index eaaca8fc1c28..c0fa34378eb2 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -66,7 +66,8 @@ int blk_mq_alloc_rqs(struct blk_mq_tag_set *set, struct blk_mq_tags *tags,
  */
 void __blk_mq_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
 				bool at_head);
-void blk_mq_request_bypass_insert(struct request *rq, bool run_queue);
+void blk_mq_request_bypass_insert(struct request *rq, bool at_head,
+				  bool run_queue);
 void blk_mq_insert_requests(struct blk_mq_hw_ctx *hctx, struct blk_mq_ctx *ctx,
 				struct list_head *list);
 

commit 27a46989a82c71028f2ba15a3f2c8f30451fda33
Author: Pavel Begunkov <asml.silence@gmail.com>
Date:   Mon Sep 30 11:25:49 2019 +0300

    blk-mq: Inline status checkers
    
    blk_mq_request_completed() and blk_mq_request_started() are
    short, inline it.
    
    Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 32c62c64e6c2..eaaca8fc1c28 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -128,15 +128,6 @@ extern void blk_mq_hctx_kobj_init(struct blk_mq_hw_ctx *hctx);
 
 void blk_mq_release(struct request_queue *q);
 
-/**
- * blk_mq_rq_state() - read the current MQ_RQ_* state of a request
- * @rq: target request.
- */
-static inline enum mq_rq_state blk_mq_rq_state(struct request *rq)
-{
-	return READ_ONCE(rq->state);
-}
-
 static inline struct blk_mq_ctx *__blk_mq_get_ctx(struct request_queue *q,
 					   unsigned int cpu)
 {

commit b49773e7bcf316f238f6709ad9e1999dcc3ed433
Author: Damien Le Moal <damien.lemoal@wdc.com>
Date:   Thu Jul 11 01:18:31 2019 +0900

    block: Disable write plugging for zoned block devices
    
    Simultaneously writing to a sequential zone of a zoned block device
    from multiple contexts requires mutual exclusion for BIO issuing to
    ensure that writes happen sequentially. However, even for a well
    behaved user correctly implementing such synchronization, BIO plugging
    may interfere and result in BIOs from the different contextx to be
    reordered if plugging is done outside of the mutual exclusion section,
    e.g. the plug was started by a function higher in the call chain than
    the function issuing BIOs.
    
             Context A                     Context B
    
       | blk_start_plug()
       | ...
       | seq_write_zone()
         | mutex_lock(zone)
         | bio-0->bi_iter.bi_sector = zone->wp
         | zone->wp += bio_sectors(bio-0)
         | submit_bio(bio-0)
         | bio-1->bi_iter.bi_sector = zone->wp
         | zone->wp += bio_sectors(bio-1)
         | submit_bio(bio-1)
         | mutex_unlock(zone)
         | return
       | -----------------------> | seq_write_zone()
                                    | mutex_lock(zone)
                                    | bio-2->bi_iter.bi_sector = zone->wp
                                    | zone->wp += bio_sectors(bio-2)
                                    | submit_bio(bio-2)
                                    | mutex_unlock(zone)
       | <------------------------- |
       | blk_finish_plug()
    
    In the above example, despite the mutex synchronization ensuring the
    correct BIO issuing order 0, 1, 2, context A BIOs 0 and 1 end up being
    issued after BIO 2 of context B, when the plug is released with
    blk_finish_plug().
    
    While this problem can be addressed using the blk_flush_plug_list()
    function (in the above example, the call must be inserted before the
    zone mutex lock is released), a simple generic solution in the block
    layer avoid this additional code in all zoned block device user code.
    The simple generic solution implemented with this patch is to introduce
    the internal helper function blk_mq_plug() to access the current
    context plug on BIO submission. This helper returns the current plug
    only if the target device is not a zoned block device or if the BIO to
    be plugged is not a write operation. Otherwise, the caller context plug
    is ignored and NULL returned, resulting is all writes to zoned block
    device to never be plugged.
    
    Signed-off-by: Damien Le Moal <damien.lemoal@wdc.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index f4bf5161333e..32c62c64e6c2 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -233,4 +233,36 @@ static inline void blk_mq_clear_mq_map(struct blk_mq_queue_map *qmap)
 		qmap->mq_map[cpu] = 0;
 }
 
+/*
+ * blk_mq_plug() - Get caller context plug
+ * @q: request queue
+ * @bio : the bio being submitted by the caller context
+ *
+ * Plugging, by design, may delay the insertion of BIOs into the elevator in
+ * order to increase BIO merging opportunities. This however can cause BIO
+ * insertion order to change from the order in which submit_bio() is being
+ * executed in the case of multiple contexts concurrently issuing BIOs to a
+ * device, even if these context are synchronized to tightly control BIO issuing
+ * order. While this is not a problem with regular block devices, this ordering
+ * change can cause write BIO failures with zoned block devices as these
+ * require sequential write patterns to zones. Prevent this from happening by
+ * ignoring the plug state of a BIO issuing context if the target request queue
+ * is for a zoned block device and the BIO to plug is a write operation.
+ *
+ * Return current->plug if the bio can be plugged and NULL otherwise
+ */
+static inline struct blk_plug *blk_mq_plug(struct request_queue *q,
+					   struct bio *bio)
+{
+	/*
+	 * For regular block devices or read operations, use the context plug
+	 * which may be NULL if blk_start_plug() was not executed.
+	 */
+	if (!blk_queue_is_zoned(q) || !op_is_write(bio_op(bio)))
+		return current->plug;
+
+	/* Zoned block device write operation case: do not plug the BIO */
+	return NULL;
+}
+
 #endif

commit c05f42206f4de12b6807270fc669b45472f1bdb7
Author: Bart Van Assche <bvanassche@acm.org>
Date:   Mon Jul 1 08:47:29 2019 -0700

    blk-mq: remove blk_mq_put_ctx()
    
    No code that occurs between blk_mq_get_ctx() and blk_mq_put_ctx() depends
    on preemption being disabled for its correctness. Since removing the CPU
    preemption calls does not measurably affect performance, simplify the
    blk-mq code by removing the blk_mq_put_ctx() function and also by not
    disabling preemption in blk_mq_get_ctx().
    
    Cc: Hannes Reinecke <hare@suse.com>
    Cc: Omar Sandoval <osandov@fb.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Bart Van Assche <bvanassche@acm.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 633a5a77ee8b..f4bf5161333e 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -151,12 +151,7 @@ static inline struct blk_mq_ctx *__blk_mq_get_ctx(struct request_queue *q,
  */
 static inline struct blk_mq_ctx *blk_mq_get_ctx(struct request_queue *q)
 {
-	return __blk_mq_get_ctx(q, get_cpu());
-}
-
-static inline void blk_mq_put_ctx(struct blk_mq_ctx *ctx)
-{
-	put_cpu();
+	return __blk_mq_get_ctx(q, raw_smp_processor_id());
 }
 
 struct blk_mq_alloc_data {

commit c7e2d94b3d1634988a95ac4d77a72dc7487ece06
Author: Ming Lei <ming.lei@redhat.com>
Date:   Tue Apr 30 09:52:25 2019 +0800

    blk-mq: free hw queue's resource in hctx's release handler
    
    Once blk_cleanup_queue() returns, tags shouldn't be used any more,
    because blk_mq_free_tag_set() may be called. Commit 45a9c9d909b2
    ("blk-mq: Fix a use-after-free") fixes this issue exactly.
    
    However, that commit introduces another issue. Before 45a9c9d909b2,
    we are allowed to run queue during cleaning up queue if the queue's
    kobj refcount is held. After that commit, queue can't be run during
    queue cleaning up, otherwise oops can be triggered easily because
    some fields of hctx are freed by blk_mq_free_queue() in blk_cleanup_queue().
    
    We have invented ways for addressing this kind of issue before, such as:
    
            8dc765d438f1 ("SCSI: fix queue cleanup race before queue initialization is done")
            c2856ae2f315 ("blk-mq: quiesce queue before freeing queue")
    
    But still can't cover all cases, recently James reports another such
    kind of issue:
    
            https://marc.info/?l=linux-scsi&m=155389088124782&w=2
    
    This issue can be quite hard to address by previous way, given
    scsi_run_queue() may run requeues for other LUNs.
    
    Fixes the above issue by freeing hctx's resources in its release handler, and this
    way is safe becasue tags isn't needed for freeing such hctx resource.
    
    This approach follows typical design pattern wrt. kobject's release handler.
    
    Cc: Dongli Zhang <dongli.zhang@oracle.com>
    Cc: James Smart <james.smart@broadcom.com>
    Cc: Bart Van Assche <bart.vanassche@wdc.com>
    Cc: linux-scsi@vger.kernel.org,
    Cc: Martin K . Petersen <martin.petersen@oracle.com>,
    Cc: Christoph Hellwig <hch@lst.de>,
    Cc: James E . J . Bottomley <jejb@linux.vnet.ibm.com>,
    Reported-by: James Smart <james.smart@broadcom.com>
    Fixes: 45a9c9d909b2 ("blk-mq: Fix a use-after-free")
    Cc: stable@vger.kernel.org
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Tested-by: James Smart <james.smart@broadcom.com>
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 423ea88ab6fb..633a5a77ee8b 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -37,7 +37,7 @@ struct blk_mq_ctx {
 	struct kobject		kobj;
 } ____cacheline_aligned_in_smp;
 
-void blk_mq_free_queue(struct request_queue *q);
+void blk_mq_exit_queue(struct request_queue *q);
 int blk_mq_update_nr_requests(struct request_queue *q, unsigned int nr);
 void blk_mq_wake_waiters(struct request_queue *q);
 bool blk_mq_dispatch_rq_list(struct request_queue *, struct list_head *, bool);

commit fd9c40f64c514bdc585a21e2e33fa5f83ca8811b
Author: Bart Van Assche <bvanassche@acm.org>
Date:   Thu Apr 4 10:08:43 2019 -0700

    block: Revert v5.0 blk_mq_request_issue_directly() changes
    
    blk_mq_try_issue_directly() can return BLK_STS*_RESOURCE for requests that
    have been queued. If that happens when blk_mq_try_issue_directly() is called
    by the dm-mpath driver then dm-mpath will try to resubmit a request that is
    already queued and a kernel crash follows. Since it is nontrivial to fix
    blk_mq_request_issue_directly(), revert the blk_mq_request_issue_directly()
    changes that went into kernel v5.0.
    
    This patch reverts the following commits:
    * d6a51a97c0b2 ("blk-mq: replace and kill blk_mq_request_issue_directly") # v5.0.
    * 5b7a6f128aad ("blk-mq: issue directly with bypass 'false' in blk_mq_sched_insert_requests") # v5.0.
    * 7f556a44e61d ("blk-mq: refactor the code of issue request directly") # v5.0.
    
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Ming Lei <ming.lei@redhat.com>
    Cc: Jianchao Wang <jianchao.w.wang@oracle.com>
    Cc: Hannes Reinecke <hare@suse.com>
    Cc: Johannes Thumshirn <jthumshirn@suse.de>
    Cc: James Smart <james.smart@broadcom.com>
    Cc: Dongli Zhang <dongli.zhang@oracle.com>
    Cc: Laurence Oberman <loberman@redhat.com>
    Cc: <stable@vger.kernel.org>
    Reported-by: Laurence Oberman <loberman@redhat.com>
    Tested-by: Laurence Oberman <loberman@redhat.com>
    Fixes: 7f556a44e61d ("blk-mq: refactor the code of issue request directly") # v5.0.
    Signed-off-by: Bart Van Assche <bvanassche@acm.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index d704fc7766f4..423ea88ab6fb 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -70,10 +70,8 @@ void blk_mq_request_bypass_insert(struct request *rq, bool run_queue);
 void blk_mq_insert_requests(struct blk_mq_hw_ctx *hctx, struct blk_mq_ctx *ctx,
 				struct list_head *list);
 
-blk_status_t blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
-						struct request *rq,
-						blk_qc_t *cookie,
-						bool bypass, bool last);
+/* Used by blk_insert_cloned_request() to issue request directly */
+blk_status_t blk_mq_request_issue_directly(struct request *rq, bool last);
 void blk_mq_try_issue_list_directly(struct blk_mq_hw_ctx *hctx,
 				    struct list_head *list);
 

commit 13f063815265c5397ee92d84436804bc9fb6b58b
Author: Yufen Yu <yuyufen@huawei.com>
Date:   Sun Mar 24 17:57:07 2019 +0800

    blk-mq: use blk_mq_put_driver_tag() to put tag
    
    Expect arguments, blk_mq_put_driver_tag_hctx() and blk_mq_put_driver_tag()
    is same. We can just use argument 'request' to put tag by blk_mq_put_driver_tag().
    Then we can remove the unused blk_mq_put_driver_tag_hctx().
    
    Signed-off-by: Yufen Yu <yuyufen@huawei.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 0ed8e5a8729f..d704fc7766f4 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -224,15 +224,6 @@ static inline void __blk_mq_put_driver_tag(struct blk_mq_hw_ctx *hctx,
 	}
 }
 
-static inline void blk_mq_put_driver_tag_hctx(struct blk_mq_hw_ctx *hctx,
-				       struct request *rq)
-{
-	if (rq->tag == -1 || rq->internal_tag == -1)
-		return;
-
-	__blk_mq_put_driver_tag(hctx, rq);
-}
-
 static inline void blk_mq_put_driver_tag(struct request *rq)
 {
 	if (rq->tag == -1 || rq->internal_tag == -1)

commit e6c987120e24cb913cb7bd4e675129a30fa49e0d
Author: Bart Van Assche <bvanassche@acm.org>
Date:   Wed Mar 20 13:14:37 2019 -0700

    block: Unexport blk_mq_add_to_requeue_list()
    
    This function is not used outside the block layer core. Hence unexport it.
    
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Bart Van Assche <bvanassche@acm.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index c11353a3749d..0ed8e5a8729f 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -41,6 +41,8 @@ void blk_mq_free_queue(struct request_queue *q);
 int blk_mq_update_nr_requests(struct request_queue *q, unsigned int nr);
 void blk_mq_wake_waiters(struct request_queue *q);
 bool blk_mq_dispatch_rq_list(struct request_queue *, struct list_head *, bool);
+void blk_mq_add_to_requeue_list(struct request *rq, bool at_head,
+				bool kick_requeue_list);
 void blk_mq_flush_busy_ctxs(struct blk_mq_hw_ctx *hctx, struct list_head *list);
 bool blk_mq_get_driver_tag(struct request *rq);
 struct request *blk_mq_dequeue_from_ctx(struct blk_mq_hw_ctx *hctx,

commit 6fb845f0e78de19eaaf6a2d351702474e44b6a9e
Merge: 56d18f62f556 d13937116f1e
Author: Jens Axboe <axboe@kernel.dk>
Date:   Fri Feb 15 08:43:59 2019 -0700

    Merge tag 'v5.0-rc6' into for-5.1/block
    
    Pull in 5.0-rc6 to avoid a dumb merge conflict with fs/iomap.c.
    This is needed since io_uring is now based on the block branch,
    to avoid a conflict between the multi-page bvecs and the bits
    of io_uring that touch the core block parts.
    
    * tag 'v5.0-rc6': (525 commits)
      Linux 5.0-rc6
      x86/mm: Make set_pmd_at() paravirt aware
      MAINTAINERS: Update the ocores i2c bus driver maintainer, etc
      blk-mq: remove duplicated definition of blk_mq_freeze_queue
      Blk-iolatency: warn on negative inflight IO counter
      blk-iolatency: fix IO hang due to negative inflight counter
      MAINTAINERS: unify reference to xen-devel list
      x86/mm/cpa: Fix set_mce_nospec()
      futex: Handle early deadlock return correctly
      futex: Fix barrier comment
      net: dsa: b53: Fix for failure when irq is not defined in dt
      blktrace: Show requests without sector
      mips: cm: reprime error cause
      mips: loongson64: remove unreachable(), fix loongson_poweroff().
      sit: check if IPv6 enabled before calling ip6_err_gen_icmpv6_unreach()
      geneve: should not call rt6_lookup() when ipv6 was disabled
      KVM: nVMX: unconditionally cancel preemption timer in free_nested (CVE-2019-7221)
      KVM: x86: work around leak of uninitialized stack contents (CVE-2019-7222)
      kvm: fix kvm_ioctl_create_device() reference counting (CVE-2019-6974)
      signal: Better detection of synchronous signals
      ...

commit 2698484178ca5cbfdde189b1d8809e1528f82a10
Author: Liu Bo <bo.liu@linux.alibaba.com>
Date:   Fri Jan 25 08:12:49 2019 +0800

    blk-mq: remove duplicated definition of blk_mq_freeze_queue
    
    As the prototype has been defined in "include/linux/blk-mq.h", the one
    in "block/blk-mq.h" can be removed then.
    
    Signed-off-by: Liu Bo <bo.liu@linux.alibaba.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index d943d46b0785..d0b3dd54ef8d 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -36,7 +36,6 @@ struct blk_mq_ctx {
 	struct kobject		kobj;
 } ____cacheline_aligned_in_smp;
 
-void blk_mq_freeze_queue(struct request_queue *q);
 void blk_mq_free_queue(struct request_queue *q);
 int blk_mq_update_nr_requests(struct request_queue *q, unsigned int nr);
 void blk_mq_wake_waiters(struct request_queue *q);

commit bb94aea1444b9859faa8d72aff3713fcd11c6696
Author: Jianchao Wang <jianchao.w.wang@oracle.com>
Date:   Thu Jan 24 18:25:33 2019 +0800

    blk-mq: save default hctx into ctx->hctxs for not-supported type
    
    Currently, we check whether the hctx type is supported every time
    in hot path. Actually, this is not necessary, we could save the
    default hctx into ctx->hctxs if the type is not supported when
    map swqueues and use it directly with ctx->hctxs[type].
    
    We also needn't check whether the poll is enabled or not, because
    the caller would clear the REQ_HIPRI in that case.
    
    Signed-off-by: Jianchao Wang <jianchao.w.wang@oracle.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 9fb06261518e..14b7efb94f75 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -106,15 +106,12 @@ static inline struct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *q,
 {
 	enum hctx_type type = HCTX_TYPE_DEFAULT;
 
-	if ((flags & REQ_HIPRI) &&
-	    q->tag_set->nr_maps > HCTX_TYPE_POLL && 
-	    q->tag_set->map[HCTX_TYPE_POLL].nr_queues &&
-	    test_bit(QUEUE_FLAG_POLL, &q->queue_flags))
+	/*
+	 * The caller ensure that if REQ_HIPRI, poll must be enabled.
+	 */
+	if (flags & REQ_HIPRI)
 		type = HCTX_TYPE_POLL;
-
-	else if (((flags & REQ_OP_MASK) == REQ_OP_READ) &&
-	         q->tag_set->nr_maps > HCTX_TYPE_READ &&
-		 q->tag_set->map[HCTX_TYPE_READ].nr_queues)
+	else if ((flags & REQ_OP_MASK) == REQ_OP_READ)
 		type = HCTX_TYPE_READ;
 	
 	return ctx->hctxs[type];

commit 8ccdf4a3775229314c8bd365ac88c2cbdf36be13
Author: Jianchao Wang <jianchao.w.wang@oracle.com>
Date:   Thu Jan 24 18:25:32 2019 +0800

    blk-mq: save queue mapping result into ctx directly
    
    Currently, the queue mapping result is saved in a two-dimensional
    array. In the hot path, to get a hctx, we need do following:
    
      q->queue_hw_ctx[q->tag_set->map[type].mq_map[cpu]]
    
    This isn't very efficient. We could save the queue mapping result into
    ctx directly with different hctx type, like,
    
      ctx->hctxs[type]
    
    Signed-off-by: Jianchao Wang <jianchao.w.wang@oracle.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index d943d46b0785..9fb06261518e 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -23,6 +23,7 @@ struct blk_mq_ctx {
 
 	unsigned int		cpu;
 	unsigned short		index_hw[HCTX_MAX_TYPES];
+	struct blk_mq_hw_ctx 	*hctxs[HCTX_MAX_TYPES];
 
 	/* incremented at dispatch time */
 	unsigned long		rq_dispatched[2];
@@ -97,11 +98,11 @@ static inline struct blk_mq_hw_ctx *blk_mq_map_queue_type(struct request_queue *
  * blk_mq_map_queue() - map (cmd_flags,type) to hardware queue
  * @q: request queue
  * @flags: request command flags
- * @cpu: CPU
+ * @cpu: cpu ctx
  */
 static inline struct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *q,
 						     unsigned int flags,
-						     unsigned int cpu)
+						     struct blk_mq_ctx *ctx)
 {
 	enum hctx_type type = HCTX_TYPE_DEFAULT;
 
@@ -116,7 +117,7 @@ static inline struct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *q,
 		 q->tag_set->map[HCTX_TYPE_READ].nr_queues)
 		type = HCTX_TYPE_READ;
 	
-	return blk_mq_map_queue_type(q, type, cpu);
+	return ctx->hctxs[type];
 }
 
 /*

commit c16d6b5a9f47d0e581882269fca1d73be60208b2
Author: Ming Lei <ming.lei@redhat.com>
Date:   Mon Dec 17 08:44:05 2018 -0700

    blk-mq: fix dispatch from sw queue
    
    When a request is added to rq list of sw queue(ctx), the rq may be from
    a different type of hctx, especially after multi queue mapping is
    introduced.
    
    So when dispach request from sw queue via blk_mq_flush_busy_ctxs() or
    blk_mq_dequeue_from_ctx(), one request belonging to other queue type of
    hctx can be dispatched to current hctx in case that read queue or poll
    queue is enabled.
    
    This patch fixes this issue by introducing per-queue-type list.
    
    Cc: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    
    Changed by me to not use separately cacheline aligned lists, just
    place them all in the same cacheline where we had just the one list
    and lock before.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index d1ed096723fb..d943d46b0785 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -18,8 +18,8 @@ struct blk_mq_ctxs {
 struct blk_mq_ctx {
 	struct {
 		spinlock_t		lock;
-		struct list_head	rq_list;
-	}  ____cacheline_aligned_in_smp;
+		struct list_head	rq_lists[HCTX_MAX_TYPES];
+	} ____cacheline_aligned_in_smp;
 
 	unsigned int		cpu;
 	unsigned short		index_hw[HCTX_MAX_TYPES];

commit 5aceaeb26394538858a9dbae5830d628469a44cf
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Dec 17 12:16:26 2018 +0100

    blk-mq: only dispatch to non-defauly queue maps if they have queues
    
    We should check if a given queue map actually has queues enabled before
    dispatching to it.  This allows drivers to not initialize optional but
    not used map types, which subsequently will allow fixing problems with
    queue map rebuilds for that case.
    
    Reviewed-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index b63a0de8a07a..d1ed096723fb 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -105,14 +105,17 @@ static inline struct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *q,
 {
 	enum hctx_type type = HCTX_TYPE_DEFAULT;
 
-	if (q->tag_set->nr_maps > HCTX_TYPE_POLL &&
-	    ((flags & REQ_HIPRI) && test_bit(QUEUE_FLAG_POLL, &q->queue_flags)))
+	if ((flags & REQ_HIPRI) &&
+	    q->tag_set->nr_maps > HCTX_TYPE_POLL && 
+	    q->tag_set->map[HCTX_TYPE_POLL].nr_queues &&
+	    test_bit(QUEUE_FLAG_POLL, &q->queue_flags))
 		type = HCTX_TYPE_POLL;
 
-	else if (q->tag_set->nr_maps > HCTX_TYPE_READ &&
-		 ((flags & REQ_OP_MASK) == REQ_OP_READ))
+	else if (((flags & REQ_OP_MASK) == REQ_OP_READ) &&
+	         q->tag_set->nr_maps > HCTX_TYPE_READ &&
+		 q->tag_set->map[HCTX_TYPE_READ].nr_queues)
 		type = HCTX_TYPE_READ;
-
+	
 	return blk_mq_map_queue_type(q, type, cpu);
 }
 

commit d6a51a97c0b2e21fec224746c2683ff739bcf4ae
Author: Jianchao Wang <jianchao.w.wang@oracle.com>
Date:   Fri Dec 14 09:28:20 2018 +0800

    blk-mq: replace and kill blk_mq_request_issue_directly
    
    Replace blk_mq_request_issue_directly with blk_mq_try_issue_directly
    in blk_insert_cloned_request and kill it as nobody uses it any more.
    
    Signed-off-by: Jianchao Wang <jianchao.w.wang@oracle.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 0c9c9ea2fefe..b63a0de8a07a 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -68,8 +68,10 @@ void blk_mq_request_bypass_insert(struct request *rq, bool run_queue);
 void blk_mq_insert_requests(struct blk_mq_hw_ctx *hctx, struct blk_mq_ctx *ctx,
 				struct list_head *list);
 
-/* Used by blk_insert_cloned_request() to issue request directly */
-blk_status_t blk_mq_request_issue_directly(struct request *rq, bool last);
+blk_status_t blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
+						struct request *rq,
+						blk_qc_t *cookie,
+						bool bypass, bool last);
 void blk_mq_try_issue_list_directly(struct blk_mq_hw_ctx *hctx,
 				    struct list_head *list);
 

commit e016b78201a2d9ff40f3f0da072292689af24c7f
Author: Mikulas Patocka <mpatocka@redhat.com>
Date:   Thu Dec 6 11:41:21 2018 -0500

    block: return just one value from part_in_flight
    
    The previous patches deleted all the code that needed the second value
    returned from part_in_flight - now the kernel only uses the first value.
    
    Consequently, part_in_flight (and blk_mq_in_flight) may be changed so that
    it only returns one value.
    
    This patch just refactors the code, there's no functional change.
    
    Signed-off-by: Mikulas Patocka <mpatocka@redhat.com>
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index a664ea44ffd4..0c9c9ea2fefe 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -187,8 +187,7 @@ static inline bool blk_mq_hw_queue_mapped(struct blk_mq_hw_ctx *hctx)
 	return hctx->nr_ctx && hctx->tags;
 }
 
-void blk_mq_in_flight(struct request_queue *q, struct hd_struct *part,
-		      unsigned int inflight[2]);
+unsigned int blk_mq_in_flight(struct request_queue *q, struct hd_struct *part);
 void blk_mq_in_flight_rw(struct request_queue *q, struct hd_struct *part,
 			 unsigned int inflight[2]);
 

commit e20ba6e1da029136ded295f33076483d65ddf50a
Author: Christoph Hellwig <hch@lst.de>
Date:   Sun Dec 2 17:46:16 2018 +0100

    block: move queues types to the block layer
    
    Having another indirect all in the fast path doesn't really help
    in our post-spectre world.  Also having too many queue type is just
    going to create confusion, so I'd rather manage them centrally.
    
    Note that the queue type naming and ordering changes a bit - the
    first index now is the default queue for everything not explicitly
    marked, the optional ones are read and poll queues.
    
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 7291e5379358..a664ea44ffd4 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -81,16 +81,14 @@ extern int blk_mq_hw_queue_to_node(struct blk_mq_queue_map *qmap, unsigned int);
 /*
  * blk_mq_map_queue_type() - map (hctx_type,cpu) to hardware queue
  * @q: request queue
- * @hctx_type: the hctx type index
+ * @type: the hctx type index
  * @cpu: CPU
  */
 static inline struct blk_mq_hw_ctx *blk_mq_map_queue_type(struct request_queue *q,
-							  unsigned int hctx_type,
+							  enum hctx_type type,
 							  unsigned int cpu)
 {
-	struct blk_mq_tag_set *set = q->tag_set;
-
-	return q->queue_hw_ctx[set->map[hctx_type].mq_map[cpu]];
+	return q->queue_hw_ctx[q->tag_set->map[type].mq_map[cpu]];
 }
 
 /*
@@ -103,12 +101,17 @@ static inline struct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *q,
 						     unsigned int flags,
 						     unsigned int cpu)
 {
-	int hctx_type = 0;
+	enum hctx_type type = HCTX_TYPE_DEFAULT;
+
+	if (q->tag_set->nr_maps > HCTX_TYPE_POLL &&
+	    ((flags & REQ_HIPRI) && test_bit(QUEUE_FLAG_POLL, &q->queue_flags)))
+		type = HCTX_TYPE_POLL;
 
-	if (q->mq_ops->rq_flags_to_type)
-		hctx_type = q->mq_ops->rq_flags_to_type(q, flags);
+	else if (q->tag_set->nr_maps > HCTX_TYPE_READ &&
+		 ((flags & REQ_OP_MASK) == REQ_OP_READ))
+		type = HCTX_TYPE_READ;
 
-	return blk_mq_map_queue_type(q, hctx_type, cpu);
+	return blk_mq_map_queue_type(q, type, cpu);
 }
 
 /*

commit be94f058f2bde6f0b0ee9059a35daa8e15be308f
Author: Jens Axboe <axboe@kernel.dk>
Date:   Sat Nov 24 10:15:46 2018 -0700

    blk-mq: use bd->last == true for list inserts
    
    If we are issuing a list of requests, we know if we're at the last one.
    If we fail issuing, ensure that we call ->commits_rqs() to flush any
    potential previous requests.
    
    Reviewed-by: Omar Sandoval <osandov@fb.com>
    Reviewed-by: Ming Lei <ming.lei@redhat.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 9ae8e9f8f8b1..7291e5379358 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -69,7 +69,7 @@ void blk_mq_insert_requests(struct blk_mq_hw_ctx *hctx, struct blk_mq_ctx *ctx,
 				struct list_head *list);
 
 /* Used by blk_insert_cloned_request() to issue request directly */
-blk_status_t blk_mq_request_issue_directly(struct request *rq);
+blk_status_t blk_mq_request_issue_directly(struct request *rq, bool last);
 void blk_mq_try_issue_list_directly(struct blk_mq_hw_ctx *hctx,
 				    struct list_head *list);
 

commit 1db4909e76f64a85f4aaa187f0f683f5c85a471d
Author: Ming Lei <ming.lei@redhat.com>
Date:   Tue Nov 20 09:44:35 2018 +0800

    blk-mq: not embed .mq_kobj and ctx->kobj into queue instance
    
    Even though .mq_kobj, ctx->kobj and q->kobj share same lifetime
    from block layer's view, actually they don't because userspace may
    grab one kobject anytime via sysfs.
    
    This patch fixes the issue by the following approach:
    
    1) introduce 'struct blk_mq_ctxs' for holding .mq_kobj and managing
    all ctxs
    
    2) free all allocated ctxs and the 'blk_mq_ctxs' instance in release
    handler of .mq_kobj
    
    3) grab one ref of .mq_kobj before initializing each ctx->kobj, so that
    .mq_kobj is always released after all ctxs are freed.
    
    This patch fixes kernel panic issue during booting when DEBUG_KOBJECT_RELEASE
    is enabled.
    
    Reported-by: Guenter Roeck <linux@roeck-us.net>
    Cc: "jianchao.wang" <jianchao.w.wang@oracle.com>
    Tested-by: Guenter Roeck <linux@roeck-us.net>
    Reviewed-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index facb6e9ddce4..9ae8e9f8f8b1 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -7,6 +7,11 @@
 
 struct blk_mq_tag_set;
 
+struct blk_mq_ctxs {
+	struct kobject kobj;
+	struct blk_mq_ctx __percpu	*queue_ctx;
+};
+
 /**
  * struct blk_mq_ctx - State for a software queue facing the submitting CPUs
  */
@@ -27,6 +32,7 @@ struct blk_mq_ctx {
 	unsigned long		____cacheline_aligned_in_smp rq_completed[2];
 
 	struct request_queue	*queue;
+	struct blk_mq_ctxs      *ctxs;
 	struct kobject		kobj;
 } ____cacheline_aligned_in_smp;
 

commit ea4f995ee8b8f0578b3319949f2edd5d812fdb0a
Author: Jens Axboe <axboe@kernel.dk>
Date:   Mon Oct 29 15:06:13 2018 -0600

    blk-mq: cache request hardware queue mapping
    
    We call blk_mq_map_queue() a lot, at least two times for each
    request per IO, sometimes more. Since we now have an indirect
    call as well in that function. cache the mapping so we don't
    have to re-call blk_mq_map_queue() for the same request
    multiple times.
    
    Reviewed-by: Keith Busch <keith.busch@intel.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 053862270125..facb6e9ddce4 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -223,13 +223,10 @@ static inline void blk_mq_put_driver_tag_hctx(struct blk_mq_hw_ctx *hctx,
 
 static inline void blk_mq_put_driver_tag(struct request *rq)
 {
-	struct blk_mq_hw_ctx *hctx;
-
 	if (rq->tag == -1 || rq->internal_tag == -1)
 		return;
 
-	hctx = blk_mq_map_queue(rq->q, rq->cmd_flags, rq->mq_ctx->cpu);
-	__blk_mq_put_driver_tag(hctx, rq);
+	__blk_mq_put_driver_tag(rq->mq_hctx, rq);
 }
 
 static inline void blk_mq_clear_mq_map(struct blk_mq_queue_map *qmap)

commit b3c661b15d5ab11d982e58bee23e05c1780528a1
Author: Jens Axboe <axboe@kernel.dk>
Date:   Tue Oct 30 10:36:06 2018 -0600

    blk-mq: support multiple hctx maps
    
    Add support for the tag set carrying multiple queue maps, and
    for the driver to inform blk-mq how many it wishes to support
    through setting set->nr_maps.
    
    This adds an mq_ops helper for drivers that support more than 1
    map, mq_ops->rq_flags_to_type(). The function takes request/bio
    flags and CPU, and returns a queue map index for that. We then
    use the type information in blk_mq_map_queue() to index the map
    set.
    
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Reviewed-by: Keith Busch <keith.busch@intel.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 1821f448f7c4..053862270125 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -72,20 +72,37 @@ void blk_mq_try_issue_list_directly(struct blk_mq_hw_ctx *hctx,
  */
 extern int blk_mq_hw_queue_to_node(struct blk_mq_queue_map *qmap, unsigned int);
 
-static inline struct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *q,
-						     unsigned int flags,
-						     unsigned int cpu)
+/*
+ * blk_mq_map_queue_type() - map (hctx_type,cpu) to hardware queue
+ * @q: request queue
+ * @hctx_type: the hctx type index
+ * @cpu: CPU
+ */
+static inline struct blk_mq_hw_ctx *blk_mq_map_queue_type(struct request_queue *q,
+							  unsigned int hctx_type,
+							  unsigned int cpu)
 {
 	struct blk_mq_tag_set *set = q->tag_set;
 
-	return q->queue_hw_ctx[set->map[0].mq_map[cpu]];
+	return q->queue_hw_ctx[set->map[hctx_type].mq_map[cpu]];
 }
 
-static inline struct blk_mq_hw_ctx *blk_mq_map_queue_type(struct request_queue *q,
-							  unsigned int hctx_type,
-							  unsigned int cpu)
+/*
+ * blk_mq_map_queue() - map (cmd_flags,type) to hardware queue
+ * @q: request queue
+ * @flags: request command flags
+ * @cpu: CPU
+ */
+static inline struct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *q,
+						     unsigned int flags,
+						     unsigned int cpu)
 {
-	return blk_mq_map_queue(q, hctx_type, cpu);
+	int hctx_type = 0;
+
+	if (q->mq_ops->rq_flags_to_type)
+		hctx_type = q->mq_ops->rq_flags_to_type(q, flags);
+
+	return blk_mq_map_queue_type(q, hctx_type, cpu);
 }
 
 /*

commit f31967f0e455d08d3ea1d2f849bf62dafc92dbf4
Author: Jens Axboe <axboe@kernel.dk>
Date:   Mon Oct 29 13:13:29 2018 -0600

    blk-mq: allow software queue to map to multiple hardware queues
    
    The mapping used to be dependent on just the CPU location, but
    now it's a tuple of (type, cpu) instead. This is a prep patch
    for allowing a single software queue to map to multiple hardware
    queues. No functional changes in this patch.
    
    This changes the software queue count to an unsigned short
    to save a bit of space. We can still support 64K-1 CPUs,
    which should be enough. Add a check to catch a wrap.
    
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Reviewed-by: Keith Busch <keith.busch@intel.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 6a8f8b60d8ba..1821f448f7c4 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -17,7 +17,7 @@ struct blk_mq_ctx {
 	}  ____cacheline_aligned_in_smp;
 
 	unsigned int		cpu;
-	unsigned int		index_hw;
+	unsigned short		index_hw[HCTX_MAX_TYPES];
 
 	/* incremented at dispatch time */
 	unsigned long		rq_dispatched[2];

commit f9afca4d367b8c915f28d29fcaba7460640403ff
Author: Jens Axboe <axboe@kernel.dk>
Date:   Mon Oct 29 13:11:38 2018 -0600

    blk-mq: pass in request/bio flags to queue mapping
    
    Prep patch for being able to place request based not just on
    CPU location, but also on the type of request.
    
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Reviewed-by: Keith Busch <keith.busch@intel.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index d9facfb9ca51..6a8f8b60d8ba 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -73,6 +73,7 @@ void blk_mq_try_issue_list_directly(struct blk_mq_hw_ctx *hctx,
 extern int blk_mq_hw_queue_to_node(struct blk_mq_queue_map *qmap, unsigned int);
 
 static inline struct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *q,
+						     unsigned int flags,
 						     unsigned int cpu)
 {
 	struct blk_mq_tag_set *set = q->tag_set;
@@ -84,7 +85,7 @@ static inline struct blk_mq_hw_ctx *blk_mq_map_queue_type(struct request_queue *
 							  unsigned int hctx_type,
 							  unsigned int cpu)
 {
-	return blk_mq_map_queue(q, cpu);
+	return blk_mq_map_queue(q, hctx_type, cpu);
 }
 
 /*
@@ -135,6 +136,7 @@ struct blk_mq_alloc_data {
 	struct request_queue *q;
 	blk_mq_req_flags_t flags;
 	unsigned int shallow_depth;
+	unsigned int cmd_flags;
 
 	/* input & output parameter */
 	struct blk_mq_ctx *ctx;
@@ -209,7 +211,7 @@ static inline void blk_mq_put_driver_tag(struct request *rq)
 	if (rq->tag == -1 || rq->internal_tag == -1)
 		return;
 
-	hctx = blk_mq_map_queue(rq->q, rq->mq_ctx->cpu);
+	hctx = blk_mq_map_queue(rq->q, rq->cmd_flags, rq->mq_ctx->cpu);
 	__blk_mq_put_driver_tag(hctx, rq);
 }
 

commit ff2c56609d9b1f0739ae3a3bfdb78191d01e4192
Author: Jens Axboe <axboe@kernel.dk>
Date:   Mon Oct 29 13:07:33 2018 -0600

    blk-mq: provide dummy blk_mq_map_queue_type() helper
    
    Doesn't do anything right now, but it's needed as a prep patch
    to get the interfaces right.
    
    While in there, correct the blk_mq_map_queue() CPU type to an unsigned
    int.
    
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Reviewed-by: Keith Busch <keith.busch@intel.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 889f0069dd80..d9facfb9ca51 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -73,13 +73,20 @@ void blk_mq_try_issue_list_directly(struct blk_mq_hw_ctx *hctx,
 extern int blk_mq_hw_queue_to_node(struct blk_mq_queue_map *qmap, unsigned int);
 
 static inline struct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *q,
-		int cpu)
+						     unsigned int cpu)
 {
 	struct blk_mq_tag_set *set = q->tag_set;
 
 	return q->queue_hw_ctx[set->map[0].mq_map[cpu]];
 }
 
+static inline struct blk_mq_hw_ctx *blk_mq_map_queue_type(struct request_queue *q,
+							  unsigned int hctx_type,
+							  unsigned int cpu)
+{
+	return blk_mq_map_queue(q, cpu);
+}
+
 /*
  * sysfs helpers
  */

commit ed76e329d74a4b15ac0f5fd3adbd52ec0178a134
Author: Jens Axboe <axboe@kernel.dk>
Date:   Mon Oct 29 13:06:14 2018 -0600

    blk-mq: abstract out queue map
    
    This is in preparation for allowing multiple sets of maps per
    queue, if so desired.
    
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Reviewed-by: Bart Van Assche <bvanassche@acm.org>
    Reviewed-by: Keith Busch <keith.busch@intel.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 9536be06d022..889f0069dd80 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -70,14 +70,14 @@ void blk_mq_try_issue_list_directly(struct blk_mq_hw_ctx *hctx,
 /*
  * CPU -> queue mappings
  */
-extern int blk_mq_hw_queue_to_node(unsigned int *map, unsigned int);
+extern int blk_mq_hw_queue_to_node(struct blk_mq_queue_map *qmap, unsigned int);
 
 static inline struct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *q,
 		int cpu)
 {
 	struct blk_mq_tag_set *set = q->tag_set;
 
-	return q->queue_hw_ctx[set->mq_map[cpu]];
+	return q->queue_hw_ctx[set->map[0].mq_map[cpu]];
 }
 
 /*
@@ -206,12 +206,12 @@ static inline void blk_mq_put_driver_tag(struct request *rq)
 	__blk_mq_put_driver_tag(hctx, rq);
 }
 
-static inline void blk_mq_clear_mq_map(struct blk_mq_tag_set *set)
+static inline void blk_mq_clear_mq_map(struct blk_mq_queue_map *qmap)
 {
 	int cpu;
 
 	for_each_possible_cpu(cpu)
-		set->mq_map[cpu] = 0;
+		qmap->mq_map[cpu] = 0;
 }
 
 #endif

commit a8908939af569ce2419f43fd56eeaf003bc3d85d
Author: Jens Axboe <axboe@kernel.dk>
Date:   Tue Oct 16 14:23:06 2018 -0600

    blk-mq: kill q->mq_map
    
    It's just a pointer to set->mq_map, use that instead. Move the
    assignment a bit earlier, so we always know it's valid.
    
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Reviewed-by: Bart Van Assche <bvanassche@acm.org>
    Reviewed-by: Keith Busch <keith.busch@intel.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 9497b47e2526..9536be06d022 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -75,7 +75,9 @@ extern int blk_mq_hw_queue_to_node(unsigned int *map, unsigned int);
 static inline struct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *q,
 		int cpu)
 {
-	return q->queue_hw_ctx[q->mq_map[cpu]];
+	struct blk_mq_tag_set *set = q->tag_set;
+
+	return q->queue_hw_ctx[set->mq_map[cpu]];
 }
 
 /*

commit 6ce3dd6eec114930cf2035a8bcb1e80477ed79a8
Author: Ming Lei <ming.lei@redhat.com>
Date:   Tue Jul 10 09:03:31 2018 +0800

    blk-mq: issue directly if hw queue isn't busy in case of 'none'
    
    In case of 'none' io scheduler, when hw queue isn't busy, it isn't
    necessary to enqueue request to sw queue and dequeue it from
    sw queue because request may be submitted to hw queue asap without
    extra cost, meantime there shouldn't be much request in sw queue,
    and we don't need to worry about effect on IO merge.
    
    There are still some single hw queue SCSI HBAs(HPSA, megaraid_sas, ...)
    which may connect high performance devices, so 'none' is often required
    for obtaining good performance.
    
    This patch improves IOPS and decreases CPU unilization on megaraid_sas,
    per Kashyap's test.
    
    Cc: Kashyap Desai <kashyap.desai@broadcom.com>
    Cc: Laurence Oberman <loberman@redhat.com>
    Cc: Omar Sandoval <osandov@fb.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Bart Van Assche <bart.vanassche@wdc.com>
    Cc: Hannes Reinecke <hare@suse.de>
    Reported-by: Kashyap Desai <kashyap.desai@broadcom.com>
    Tested-by: Kashyap Desai <kashyap.desai@broadcom.com>
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index bc2b24735ed4..9497b47e2526 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -64,6 +64,8 @@ void blk_mq_insert_requests(struct blk_mq_hw_ctx *hctx, struct blk_mq_ctx *ctx,
 
 /* Used by blk_insert_cloned_request() to issue request directly */
 blk_status_t blk_mq_request_issue_directly(struct request *rq);
+void blk_mq_try_issue_list_directly(struct blk_mq_hw_ctx *hctx,
+				    struct list_head *list);
 
 /*
  * CPU -> queue mappings

commit 0da73d00ca111a6175825a00d94dbeae185f6d7e
Author: Minwoo Im <minwoo.im.dev@gmail.com>
Date:   Mon Jul 2 23:46:43 2018 +0900

    blk-mq: code clean-up by adding an API to clear set->mq_map
    
    set->mq_map is now currently cleared if something goes wrong when
    establishing a queue map in blk-mq-pci.c.  It's also cleared before
    updating a queue map in blk_mq_update_queue_map().
    
    This patch provides an API to clear set->mq_map to make it clear.
    
    Signed-off-by: Minwoo Im <minwoo.im.dev@gmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 23659f41bf2c..bc2b24735ed4 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -202,4 +202,12 @@ static inline void blk_mq_put_driver_tag(struct request *rq)
 	__blk_mq_put_driver_tag(hctx, rq);
 }
 
+static inline void blk_mq_clear_mq_map(struct blk_mq_tag_set *set)
+{
+	int cpu;
+
+	for_each_possible_cpu(cpu)
+		set->mq_map[cpu] = 0;
+}
+
 #endif

commit 8ab6bb9ee8d04ba56b9eb19cc7e4f56d0a43ad1a
Author: Ming Lei <ming.lei@redhat.com>
Date:   Mon Jun 25 19:31:45 2018 +0800

    blk-mq: cleanup blk_mq_get_driver_tag()
    
    We never pass 'wait' as true to blk_mq_get_driver_tag(), and hence
    we never change '**hctx' as well. The last use of these went away
    with the flush cleanup, commit 0c2a6fe4dc3e.
    
    So cleanup the usage and remove the two extra parameters.
    
    Cc: Bart Van Assche <bart.vanassche@wdc.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Tested-by: Andrew Jones <drjones@redhat.com>
    Reviewed-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 89231e439b2f..23659f41bf2c 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -36,8 +36,7 @@ int blk_mq_update_nr_requests(struct request_queue *q, unsigned int nr);
 void blk_mq_wake_waiters(struct request_queue *q);
 bool blk_mq_dispatch_rq_list(struct request_queue *, struct list_head *, bool);
 void blk_mq_flush_busy_ctxs(struct blk_mq_hw_ctx *hctx, struct list_head *list);
-bool blk_mq_get_driver_tag(struct request *rq, struct blk_mq_hw_ctx **hctx,
-				bool wait);
+bool blk_mq_get_driver_tag(struct request *rq);
 struct request *blk_mq_dequeue_from_ctx(struct blk_mq_hw_ctx *hctx,
 					struct blk_mq_ctx *start);
 

commit 12f5b93145450c750f315657ef239a314811aeeb
Author: Keith Busch <keith.busch@intel.com>
Date:   Tue May 29 15:52:28 2018 +0200

    blk-mq: Remove generation seqeunce
    
    This patch simplifies the timeout handling by relying on the request
    reference counting to ensure the iterator is operating on an inflight
    and truly timed out request. Since the reference counting prevents the
    tag from being reallocated, the block layer no longer needs to prevent
    drivers from completing their requests while the timeout handler is
    operating on it: a driver completing a request is allowed to proceed to
    the next state without additional syncronization with the block layer.
    
    This also removes any need for generation sequence numbers since the
    request lifetime is prevented from being reallocated as a new sequence
    while timeout handling is operating on it.
    
    To enables this a refcount is added to struct request so that request
    users can be sure they're operating on the same request without it
    changing while they're processing it.  The request's tag won't be
    released for reuse until both the timeout handler and the completion
    are done with it.
    
    Signed-off-by: Keith Busch <keith.busch@intel.com>
    [hch: slight cleanups, added back submission side hctx lock, use cmpxchg
     for completions]
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index e1bb420dc5d6..89231e439b2f 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -30,20 +30,6 @@ struct blk_mq_ctx {
 	struct kobject		kobj;
 } ____cacheline_aligned_in_smp;
 
-/*
- * Bits for request->gstate.  The lower two bits carry MQ_RQ_* state value
- * and the upper bits the generation number.
- */
-enum mq_rq_state {
-	MQ_RQ_IDLE		= 0,
-	MQ_RQ_IN_FLIGHT		= 1,
-	MQ_RQ_COMPLETE		= 2,
-
-	MQ_RQ_STATE_BITS	= 2,
-	MQ_RQ_STATE_MASK	= (1 << MQ_RQ_STATE_BITS) - 1,
-	MQ_RQ_GEN_INC		= 1 << MQ_RQ_STATE_BITS,
-};
-
 void blk_mq_freeze_queue(struct request_queue *q);
 void blk_mq_free_queue(struct request_queue *q);
 int blk_mq_update_nr_requests(struct request_queue *q, unsigned int nr);
@@ -107,33 +93,9 @@ void blk_mq_release(struct request_queue *q);
  * blk_mq_rq_state() - read the current MQ_RQ_* state of a request
  * @rq: target request.
  */
-static inline int blk_mq_rq_state(struct request *rq)
+static inline enum mq_rq_state blk_mq_rq_state(struct request *rq)
 {
-	return READ_ONCE(rq->gstate) & MQ_RQ_STATE_MASK;
-}
-
-/**
- * blk_mq_rq_update_state() - set the current MQ_RQ_* state of a request
- * @rq: target request.
- * @state: new state to set.
- *
- * Set @rq's state to @state.  The caller is responsible for ensuring that
- * there are no other updaters.  A request can transition into IN_FLIGHT
- * only from IDLE and doing so increments the generation number.
- */
-static inline void blk_mq_rq_update_state(struct request *rq,
-					  enum mq_rq_state state)
-{
-	u64 old_val = READ_ONCE(rq->gstate);
-	u64 new_val = (old_val & ~MQ_RQ_STATE_MASK) | state;
-
-	if (state == MQ_RQ_IN_FLIGHT) {
-		WARN_ON_ONCE((old_val & MQ_RQ_STATE_MASK) != MQ_RQ_IDLE);
-		new_val += MQ_RQ_GEN_INC;
-	}
-
-	/* avoid exposing interim values */
-	WRITE_ONCE(rq->gstate, new_val);
+	return READ_ONCE(rq->state);
 }
 
 static inline struct blk_mq_ctx *__blk_mq_get_ctx(struct request_queue *q,

commit bf0ddaba65ddbb2715af97041da8e7a45b2d8628
Author: Omar Sandoval <osandov@fb.com>
Date:   Thu Apr 26 00:21:59 2018 -0700

    blk-mq: fix sysfs inflight counter
    
    When the blk-mq inflight implementation was added, /proc/diskstats was
    converted to use it, but /sys/block/$dev/inflight was not. Fix it by
    adding another helper to count in-flight requests by data direction.
    
    Fixes: f299b7c7a9de ("blk-mq: provide internal in-flight variant")
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 89b5cd3a6c70..e1bb420dc5d6 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -188,7 +188,9 @@ static inline bool blk_mq_hw_queue_mapped(struct blk_mq_hw_ctx *hctx)
 }
 
 void blk_mq_in_flight(struct request_queue *q, struct hd_struct *part,
-			unsigned int inflight[2]);
+		      unsigned int inflight[2]);
+void blk_mq_in_flight_rw(struct request_queue *q, struct hd_struct *part,
+			 unsigned int inflight[2]);
 
 static inline void blk_mq_put_dispatch_budget(struct blk_mq_hw_ctx *hctx)
 {

commit fe644072dfee069d97a66ea9a80f4bc461499e6a
Author: Linus Walleij <linus.walleij@linaro.org>
Date:   Fri Apr 20 10:29:51 2018 +0200

    block: mq: Add some minor doc for core structs
    
    As it came up in discussion on the mailing list that the semantic
    meaning of 'blk_mq_ctx' and 'blk_mq_hw_ctx' isn't completely
    obvious to everyone, let's add some minimal kerneldoc for a
    starter.
    
    Signed-off-by: Linus Walleij <linus.walleij@linaro.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 88c558f71819..89b5cd3a6c70 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -7,6 +7,9 @@
 
 struct blk_mq_tag_set;
 
+/**
+ * struct blk_mq_ctx - State for a software queue facing the submitting CPUs
+ */
 struct blk_mq_ctx {
 	struct {
 		spinlock_t		lock;

commit c77ff7fd03ddca8face268c4cf093c0edf4bcf1f
Author: Bart Van Assche <bart.vanassche@wdc.com>
Date:   Fri Jan 19 08:58:54 2018 -0800

    blk-mq: Rename blk_mq_request_direct_issue() into blk_mq_request_issue_directly()
    
    Most blk-mq functions have a name that follows the pattern blk_mq_${action}.
    However, the function name blk_mq_request_direct_issue is an exception.
    Hence rename this function. This patch does not change any functionality.
    
    Reviewed-by: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: Bart Van Assche <bart.vanassche@wdc.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index e3ebc93646ca..88c558f71819 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -75,7 +75,7 @@ void blk_mq_insert_requests(struct blk_mq_hw_ctx *hctx, struct blk_mq_ctx *ctx,
 				struct list_head *list);
 
 /* Used by blk_insert_cloned_request() to issue request directly */
-blk_status_t blk_mq_request_direct_issue(struct request *rq);
+blk_status_t blk_mq_request_issue_directly(struct request *rq);
 
 /*
  * CPU -> queue mappings

commit 396eaf21ee17c476e8f66249fb1f4a39003d0ab4
Author: Ming Lei <ming.lei@redhat.com>
Date:   Wed Jan 17 11:25:57 2018 -0500

    blk-mq: improve DM's blk-mq IO merging via blk_insert_cloned_request feedback
    
    blk_insert_cloned_request() is called in the fast path of a dm-rq driver
    (e.g. blk-mq request-based DM mpath).  blk_insert_cloned_request() uses
    blk_mq_request_bypass_insert() to directly append the request to the
    blk-mq hctx->dispatch_list of the underlying queue.
    
    1) This way isn't efficient enough because the hctx spinlock is always
    used.
    
    2) With blk_insert_cloned_request(), we completely bypass underlying
    queue's elevator and depend on the upper-level dm-rq driver's elevator
    to schedule IO.  But dm-rq currently can't get the underlying queue's
    dispatch feedback at all.  Without knowing whether a request was issued
    or not (e.g. due to underlying queue being busy) the dm-rq elevator will
    not be able to provide effective IO merging (as a side-effect of dm-rq
    currently blindly destaging a request from its elevator only to requeue
    it after a delay, which kills any opportunity for merging).  This
    obviously causes very bad sequential IO performance.
    
    Fix this by updating blk_insert_cloned_request() to use
    blk_mq_request_direct_issue().  blk_mq_request_direct_issue() allows a
    request to be issued directly to the underlying queue and returns the
    dispatch feedback (blk_status_t).  If blk_mq_request_direct_issue()
    returns BLK_SYS_RESOURCE the dm-rq driver will now use DM_MAPIO_REQUEUE
    to _not_ destage the request.  Whereby preserving the opportunity to
    merge IO.
    
    With this, request-based DM's blk-mq sequential IO performance is vastly
    improved (as much as 3X in mpath/virtio-scsi testing).
    
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    [blk-mq.c changes heavily influenced by Ming Lei's initial solution, but
    they were refactored to make them less fragile and easier to read/review]
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 8591a54d989b..e3ebc93646ca 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -74,6 +74,9 @@ void blk_mq_request_bypass_insert(struct request *rq, bool run_queue);
 void blk_mq_insert_requests(struct blk_mq_hw_ctx *hctx, struct blk_mq_ctx *ctx,
 				struct list_head *list);
 
+/* Used by blk_insert_cloned_request() to issue request directly */
+blk_status_t blk_mq_request_direct_issue(struct request *rq);
+
 /*
  * CPU -> queue mappings
  */

commit 5a61c36398d0626bad377a7f5b9391b21e16e91d
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jan 9 08:29:52 2018 -0800

    blk-mq: remove REQ_ATOM_STARTED
    
    After the recent updates to use generation number and state based
    synchronization, we can easily replace REQ_ATOM_STARTED usages by
    adding an extra state to distinguish completed but not yet freed
    state.
    
    Add MQ_RQ_COMPLETE and replace REQ_ATOM_STARTED usages with
    blk_mq_rq_state() tests.  REQ_ATOM_STARTED no longer has any users
    left and is removed.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 6b2d61629d48..8591a54d989b 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -34,6 +34,7 @@ struct blk_mq_ctx {
 enum mq_rq_state {
 	MQ_RQ_IDLE		= 0,
 	MQ_RQ_IN_FLIGHT		= 1,
+	MQ_RQ_COMPLETE		= 2,
 
 	MQ_RQ_STATE_BITS	= 2,
 	MQ_RQ_STATE_MASK	= (1 << MQ_RQ_STATE_BITS) - 1,

commit 358f70da49d77c43f2ca11b5da584213b2add29c
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jan 9 08:29:50 2018 -0800

    blk-mq: make blk_abort_request() trigger timeout path
    
    With issue/complete and timeout paths now using the generation number
    and state based synchronization, blk_abort_request() is the only one
    which depends on REQ_ATOM_COMPLETE for arbitrating completion.
    
    There's no reason for blk_abort_request() to be a completely separate
    path.  This patch makes blk_abort_request() piggyback on the timeout
    path instead of trying to terminate the request directly.
    
    This removes the last dependency on REQ_ATOM_COMPLETE in blk-mq.
    
    Note that this makes blk_abort_request() asynchronous - it initiates
    abortion but the actual termination will happen after a short while,
    even when the caller owns the request.  AFAICS, SCSI and ATA should be
    fine with that and I think mtip32xx and dasd should be safe but not
    completely sure.  It'd be great if people who know the drivers take a
    look.
    
    v2: - Add comment explaining the lack of synchronization around
          ->deadline update as requested by Bart.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Asai Thambi SP <asamymuthupa@micron.com>
    Cc: Stefan Haberland <sth@linux.vnet.ibm.com>
    Cc: Jan Hoeppner <hoeppner@linux.vnet.ibm.com>
    Cc: Bart Van Assche <Bart.VanAssche@wdc.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index cf01f6f8c73d..6b2d61629d48 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -94,8 +94,6 @@ extern int blk_mq_sysfs_register(struct request_queue *q);
 extern void blk_mq_sysfs_unregister(struct request_queue *q);
 extern void blk_mq_hctx_kobj_init(struct blk_mq_hw_ctx *hctx);
 
-extern void blk_mq_rq_timed_out(struct request *req, bool reserved);
-
 void blk_mq_release(struct request_queue *q);
 
 /**

commit 1d9bd5161ba32db5665a617edc8b0723880f543e
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jan 9 08:29:48 2018 -0800

    blk-mq: replace timeout synchronization with a RCU and generation based scheme
    
    Currently, blk-mq timeout path synchronizes against the usual
    issue/completion path using a complex scheme involving atomic
    bitflags, REQ_ATOM_*, memory barriers and subtle memory coherence
    rules.  Unfortunately, it contains quite a few holes.
    
    There's a complex dancing around REQ_ATOM_STARTED and
    REQ_ATOM_COMPLETE between issue/completion and timeout paths; however,
    they don't have a synchronization point across request recycle
    instances and it isn't clear what the barriers add.
    blk_mq_check_expired() can easily read STARTED from N-2'th iteration,
    deadline from N-1'th, blk_mark_rq_complete() against Nth instance.
    
    In fact, it's pretty easy to make blk_mq_check_expired() terminate a
    later instance of a request.  If we induce 5 sec delay before
    time_after_eq() test in blk_mq_check_expired(), shorten the timeout to
    2s, and issue back-to-back large IOs, blk-mq starts timing out
    requests spuriously pretty quickly.  Nothing actually timed out.  It
    just made the call on a recycle instance of a request and then
    terminated a later instance long after the original instance finished.
    The scenario isn't theoretical either.
    
    This patch replaces the broken synchronization mechanism with a RCU
    and generation number based one.
    
    1. Each request has a u64 generation + state value, which can be
       updated only by the request owner.  Whenever a request becomes
       in-flight, the generation number gets bumped up too.  This provides
       the basis for the timeout path to distinguish different recycle
       instances of the request.
    
       Also, marking a request in-flight and setting its deadline are
       protected with a seqcount so that the timeout path can fetch both
       values coherently.
    
    2. The timeout path fetches the generation, state and deadline.  If
       the verdict is timeout, it records the generation into a dedicated
       request abortion field and does RCU wait.
    
    3. The completion path is also protected by RCU (from the previous
       patch) and checks whether the current generation number and state
       match the abortion field.  If so, it skips completion.
    
    4. The timeout path, after RCU wait, scans requests again and
       terminates the ones whose generation and state still match the ones
       requested for abortion.
    
       By now, the timeout path knows that either the generation number
       and state changed if it lost the race or the completion will yield
       to it and can safely timeout the request.
    
    While it's more lines of code, it's conceptually simpler, doesn't
    depend on direct use of subtle memory ordering or coherence, and
    hopefully doesn't terminate the wrong instance.
    
    While this change makes REQ_ATOM_COMPLETE synchronization unnecessary
    between issue/complete and timeout paths, REQ_ATOM_COMPLETE isn't
    removed yet as it's still used in other places.  Future patches will
    move all state tracking to the new mechanism and remove all bitops in
    the hot paths.
    
    Note that this patch adds a comment explaining a race condition in
    BLK_EH_RESET_TIMER path.  The race has always been there and this
    patch doesn't change it.  It's just documenting the existing race.
    
    v2: - Fixed BLK_EH_RESET_TIMER handling as pointed out by Jianchao.
        - s/request->gstate_seqc/request->gstate_seq/ as suggested by Peter.
        - READ_ONCE() added in blk_mq_rq_update_state() as suggested by Peter.
    
    v3: - Fixed possible extended seqcount / u64_stats_sync read looping
          spotted by Peter.
        - MQ_RQ_IDLE was incorrectly being set in complete_request instead
          of free_request.  Fixed.
    
    v4: - Rebased on top of hctx_lock() refactoring patch.
        - Added comment explaining the use of hctx_lock() in completion path.
    
    v5: - Added comments requested by Bart.
        - Note the addition of BLK_EH_RESET_TIMER race condition in the
          commit message.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: "jianchao.wang" <jianchao.w.wang@oracle.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Bart Van Assche <Bart.VanAssche@wdc.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 6c7c3ff5bf62..cf01f6f8c73d 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -27,6 +27,19 @@ struct blk_mq_ctx {
 	struct kobject		kobj;
 } ____cacheline_aligned_in_smp;
 
+/*
+ * Bits for request->gstate.  The lower two bits carry MQ_RQ_* state value
+ * and the upper bits the generation number.
+ */
+enum mq_rq_state {
+	MQ_RQ_IDLE		= 0,
+	MQ_RQ_IN_FLIGHT		= 1,
+
+	MQ_RQ_STATE_BITS	= 2,
+	MQ_RQ_STATE_MASK	= (1 << MQ_RQ_STATE_BITS) - 1,
+	MQ_RQ_GEN_INC		= 1 << MQ_RQ_STATE_BITS,
+};
+
 void blk_mq_freeze_queue(struct request_queue *q);
 void blk_mq_free_queue(struct request_queue *q);
 int blk_mq_update_nr_requests(struct request_queue *q, unsigned int nr);
@@ -85,6 +98,39 @@ extern void blk_mq_rq_timed_out(struct request *req, bool reserved);
 
 void blk_mq_release(struct request_queue *q);
 
+/**
+ * blk_mq_rq_state() - read the current MQ_RQ_* state of a request
+ * @rq: target request.
+ */
+static inline int blk_mq_rq_state(struct request *rq)
+{
+	return READ_ONCE(rq->gstate) & MQ_RQ_STATE_MASK;
+}
+
+/**
+ * blk_mq_rq_update_state() - set the current MQ_RQ_* state of a request
+ * @rq: target request.
+ * @state: new state to set.
+ *
+ * Set @rq's state to @state.  The caller is responsible for ensuring that
+ * there are no other updaters.  A request can transition into IN_FLIGHT
+ * only from IDLE and doing so increments the generation number.
+ */
+static inline void blk_mq_rq_update_state(struct request *rq,
+					  enum mq_rq_state state)
+{
+	u64 old_val = READ_ONCE(rq->gstate);
+	u64 new_val = (old_val & ~MQ_RQ_STATE_MASK) | state;
+
+	if (state == MQ_RQ_IN_FLIGHT) {
+		WARN_ON_ONCE((old_val & MQ_RQ_STATE_MASK) != MQ_RQ_IDLE);
+		new_val += MQ_RQ_GEN_INC;
+	}
+
+	/* avoid exposing interim values */
+	WRITE_ONCE(rq->gstate, new_val);
+}
+
 static inline struct blk_mq_ctx *__blk_mq_get_ctx(struct request_queue *q,
 					   unsigned int cpu)
 {

commit e2c5923c349c1738fe8fda980874d93f6fb2e5b6
Merge: abc36be23635 a04b5de5050a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Nov 14 15:32:19 2017 -0800

    Merge branch 'for-4.15/block' of git://git.kernel.dk/linux-block
    
    Pull core block layer updates from Jens Axboe:
     "This is the main pull request for block storage for 4.15-rc1.
    
      Nothing out of the ordinary in here, and no API changes or anything
      like that. Just various new features for drivers, core changes, etc.
      In particular, this pull request contains:
    
       - A patch series from Bart, closing the whole on blk/scsi-mq queue
         quescing.
    
       - A series from Christoph, building towards hidden gendisks (for
         multipath) and ability to move bio chains around.
    
       - NVMe
            - Support for native multipath for NVMe (Christoph).
            - Userspace notifications for AENs (Keith).
            - Command side-effects support (Keith).
            - SGL support (Chaitanya Kulkarni)
            - FC fixes and improvements (James Smart)
            - Lots of fixes and tweaks (Various)
    
       - bcache
            - New maintainer (Michael Lyle)
            - Writeback control improvements (Michael)
            - Various fixes (Coly, Elena, Eric, Liang, et al)
    
       - lightnvm updates, mostly centered around the pblk interface
         (Javier, Hans, and Rakesh).
    
       - Removal of unused bio/bvec kmap atomic interfaces (me, Christoph)
    
       - Writeback series that fix the much discussed hundreds of millions
         of sync-all units. This goes all the way, as discussed previously
         (me).
    
       - Fix for missing wakeup on writeback timer adjustments (Yafang
         Shao).
    
       - Fix laptop mode on blk-mq (me).
    
       - {mq,name} tupple lookup for IO schedulers, allowing us to have
         alias names. This means you can use 'deadline' on both !mq and on
         mq (where it's called mq-deadline). (me).
    
       - blktrace race fix, oopsing on sg load (me).
    
       - blk-mq optimizations (me).
    
       - Obscure waitqueue race fix for kyber (Omar).
    
       - NBD fixes (Josef).
    
       - Disable writeback throttling by default on bfq, like we do on cfq
         (Luca Miccio).
    
       - Series from Ming that enable us to treat flush requests on blk-mq
         like any other request. This is a really nice cleanup.
    
       - Series from Ming that improves merging on blk-mq with schedulers,
         getting us closer to flipping the switch on scsi-mq again.
    
       - BFQ updates (Paolo).
    
       - blk-mq atomic flags memory ordering fixes (Peter Z).
    
       - Loop cgroup support (Shaohua).
    
       - Lots of minor fixes from lots of different folks, both for core and
         driver code"
    
    * 'for-4.15/block' of git://git.kernel.dk/linux-block: (294 commits)
      nvme: fix visibility of "uuid" ns attribute
      blk-mq: fixup some comment typos and lengths
      ide: ide-atapi: fix compile error with defining macro DEBUG
      blk-mq: improve tag waiting setup for non-shared tags
      brd: remove unused brd_mutex
      blk-mq: only run the hardware queue if IO is pending
      block: avoid null pointer dereference on null disk
      fs: guard_bio_eod() needs to consider partitions
      xtensa/simdisk: fix compile error
      nvme: expose subsys attribute to sysfs
      nvme: create 'slaves' and 'holders' entries for hidden controllers
      block: create 'slaves' and 'holders' entries for hidden gendisks
      nvme: also expose the namespace identification sysfs files for mpath nodes
      nvme: implement multipath access to nvme subsystems
      nvme: track shared namespaces
      nvme: introduce a nvme_ns_ids structure
      nvme: track subsystems
      block, nvme: Introduce blk_mq_req_flags_t
      block, scsi: Make SCSI quiesce and resume work reliably
      block: Add the QUEUE_FLAG_PREEMPT_ONLY request queue flag
      ...

commit 79f720a751cad613620d0237e3b44f89f4a69181
Author: Jens Axboe <axboe@kernel.dk>
Date:   Fri Nov 10 09:13:21 2017 -0700

    blk-mq: only run the hardware queue if IO is pending
    
    Currently we are inconsistent in when we decide to run the queue. Using
    blk_mq_run_hw_queues() we check if the hctx has pending IO before
    running it, but we don't do that from the individual queue run function,
    blk_mq_run_hw_queue(). This results in a lot of extra and pointless
    queue runs, potentially, on flush requests and (much worse) on tag
    starvation situations. This is observable just looking at top output,
    with lots of kworkers active. For the !async runs, it just adds to the
    CPU overhead of blk-mq.
    
    Move the has-pending check into the run function instead of having
    callers do it.
    
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 99a19c5523e2..dcf379a892dd 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -26,14 +26,12 @@ struct blk_mq_ctx {
 	struct kobject		kobj;
 } ____cacheline_aligned_in_smp;
 
-void blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async);
 void blk_mq_freeze_queue(struct request_queue *q);
 void blk_mq_free_queue(struct request_queue *q);
 int blk_mq_update_nr_requests(struct request_queue *q, unsigned int nr);
 void blk_mq_wake_waiters(struct request_queue *q);
 bool blk_mq_dispatch_rq_list(struct request_queue *, struct list_head *, bool);
 void blk_mq_flush_busy_ctxs(struct blk_mq_hw_ctx *hctx, struct list_head *list);
-bool blk_mq_hctx_has_pending(struct blk_mq_hw_ctx *hctx);
 bool blk_mq_get_driver_tag(struct request *rq, struct blk_mq_hw_ctx **hctx,
 				bool wait);
 struct request *blk_mq_dequeue_from_ctx(struct blk_mq_hw_ctx *hctx,

commit 9a95e4ef709533efac4aafcb8bddf73f96db50ed
Author: Bart Van Assche <bart.vanassche@wdc.com>
Date:   Thu Nov 9 10:49:59 2017 -0800

    block, nvme: Introduce blk_mq_req_flags_t
    
    Several block layer and NVMe core functions accept a combination
    of BLK_MQ_REQ_* flags through the 'flags' argument but there is
    no verification at compile time whether the right type of block
    layer flags is passed. Make it possible for sparse to verify this.
    This patch does not change any functionality.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@wdc.com>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Tested-by: Oleksandr Natalenko <oleksandr@natalenko.name>
    Cc: linux-nvme@lists.infradead.org
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Johannes Thumshirn <jthumshirn@suse.de>
    Cc: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 2502f40ccdc0..99a19c5523e2 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -111,7 +111,7 @@ static inline void blk_mq_put_ctx(struct blk_mq_ctx *ctx)
 struct blk_mq_alloc_data {
 	/* input parameter */
 	struct request_queue *q;
-	unsigned int flags;
+	blk_mq_req_flags_t flags;
 	unsigned int shallow_depth;
 
 	/* input & output parameter */

commit 244c65a3ccaa06fd15cc940315606674d3108b2f
Author: Ming Lei <ming.lei@redhat.com>
Date:   Sat Nov 4 12:39:57 2017 -0600

    blk-mq: move blk_mq_put_driver_tag*() into blk-mq.h
    
    We need this helper to put the driver tag for flush rq, since we will
    not share tag in the flush request sequence in the following patch
    in case that I/O scheduler is applied.
    
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 9fffec0ad8e9..2502f40ccdc0 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -2,6 +2,7 @@
 #define INT_BLK_MQ_H
 
 #include "blk-stat.h"
+#include "blk-mq-tag.h"
 
 struct blk_mq_tag_set;
 
@@ -156,4 +157,36 @@ static inline bool blk_mq_get_dispatch_budget(struct blk_mq_hw_ctx *hctx)
 	return true;
 }
 
+static inline void __blk_mq_put_driver_tag(struct blk_mq_hw_ctx *hctx,
+					   struct request *rq)
+{
+	blk_mq_put_tag(hctx, hctx->tags, rq->mq_ctx, rq->tag);
+	rq->tag = -1;
+
+	if (rq->rq_flags & RQF_MQ_INFLIGHT) {
+		rq->rq_flags &= ~RQF_MQ_INFLIGHT;
+		atomic_dec(&hctx->nr_active);
+	}
+}
+
+static inline void blk_mq_put_driver_tag_hctx(struct blk_mq_hw_ctx *hctx,
+				       struct request *rq)
+{
+	if (rq->tag == -1 || rq->internal_tag == -1)
+		return;
+
+	__blk_mq_put_driver_tag(hctx, rq);
+}
+
+static inline void blk_mq_put_driver_tag(struct request *rq)
+{
+	struct blk_mq_hw_ctx *hctx;
+
+	if (rq->tag == -1 || rq->internal_tag == -1)
+		return;
+
+	hctx = blk_mq_map_queue(rq->q, rq->mq_ctx->cpu);
+	__blk_mq_put_driver_tag(hctx, rq);
+}
+
 #endif

commit b0850297c749ea79a5717d597931366b3d7f4b09
Author: Ming Lei <ming.lei@redhat.com>
Date:   Thu Nov 2 23:24:34 2017 +0800

    block: pass 'run_queue' to blk_mq_request_bypass_insert
    
    Block flush need this function without running the queue, so add a
    parameter controlling whether we run it or not.
    
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index f97aceff76e9..9fffec0ad8e9 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -56,7 +56,7 @@ int blk_mq_alloc_rqs(struct blk_mq_tag_set *set, struct blk_mq_tags *tags,
  */
 void __blk_mq_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
 				bool at_head);
-void blk_mq_request_bypass_insert(struct request *rq);
+void blk_mq_request_bypass_insert(struct request *rq, bool run_queue);
 void blk_mq_insert_requests(struct blk_mq_hw_ctx *hctx, struct blk_mq_ctx *ctx,
 				struct list_head *list);
 

commit 88022d7201e96b43f1754b0358fc6bcd8dbdcde1
Author: Ming Lei <ming.lei@redhat.com>
Date:   Sun Nov 5 02:21:12 2017 +0800

    blk-mq: don't handle failure in .get_budget
    
    It is enough to just check if we can get the budget via .get_budget().
    And we don't need to deal with device state change in .get_budget().
    
    For SCSI, one issue to be fixed is that we have to call
    scsi_mq_uninit_cmd() to free allocated ressources if SCSI device fails
    to handle the request. And it isn't enough to simply call
    blk_mq_end_request() to do that if this request is marked as
    RQF_DONTPREP.
    
    Fixes: 0df21c86bdbf(scsi: implement .get_budget and .put_budget for blk-mq)
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 522b420dedc0..f97aceff76e9 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -147,14 +147,13 @@ static inline void blk_mq_put_dispatch_budget(struct blk_mq_hw_ctx *hctx)
 		q->mq_ops->put_budget(hctx);
 }
 
-static inline blk_status_t blk_mq_get_dispatch_budget(
-		struct blk_mq_hw_ctx *hctx)
+static inline bool blk_mq_get_dispatch_budget(struct blk_mq_hw_ctx *hctx)
 {
 	struct request_queue *q = hctx->queue;
 
 	if (q->mq_ops->get_budget)
 		return q->mq_ops->get_budget(hctx);
-	return BLK_STS_OK;
+	return true;
 }
 
 #endif

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index ef15b3414da5..4933af9d61f7 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef INT_BLK_MQ_H
 #define INT_BLK_MQ_H
 

commit b347689ffbca745ac457ee27400ce1affd571c6f
Author: Ming Lei <ming.lei@redhat.com>
Date:   Sat Oct 14 17:22:30 2017 +0800

    blk-mq-sched: improve dispatching from sw queue
    
    SCSI devices use host-wide tagset, and the shared driver tag space is
    often quite big. However, there is also a queue depth for each lun(
    .cmd_per_lun), which is often small, for example, on both lpfc and
    qla2xxx, .cmd_per_lun is just 3.
    
    So lots of requests may stay in sw queue, and we always flush all
    belonging to same hw queue and dispatch them all to driver.
    Unfortunately it is easy to cause queue busy because of the small
    .cmd_per_lun.  Once these requests are flushed out, they have to stay in
    hctx->dispatch, and no bio merge can happen on these requests, and
    sequential IO performance is harmed.
    
    This patch introduces blk_mq_dequeue_from_ctx for dequeuing a request
    from a sw queue, so that we can dispatch them in scheduler's way. We can
    then avoid dequeueing too many requests from sw queue, since we don't
    flush ->dispatch completely.
    
    This patch improves dispatching from sw queue by using the .get_budget
    and .put_budget callbacks.
    
    Reviewed-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index e413b732374e..522b420dedc0 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -35,6 +35,8 @@ void blk_mq_flush_busy_ctxs(struct blk_mq_hw_ctx *hctx, struct list_head *list);
 bool blk_mq_hctx_has_pending(struct blk_mq_hw_ctx *hctx);
 bool blk_mq_get_driver_tag(struct request *rq, struct blk_mq_hw_ctx **hctx,
 				bool wait);
+struct request *blk_mq_dequeue_from_ctx(struct blk_mq_hw_ctx *hctx,
+					struct blk_mq_ctx *start);
 
 /*
  * Internal helpers for allocating/freeing the request map

commit de1482974080ec9ef414bf048b2646b246b63f6e
Author: Ming Lei <ming.lei@redhat.com>
Date:   Sat Oct 14 17:22:29 2017 +0800

    blk-mq: introduce .get_budget and .put_budget in blk_mq_ops
    
    For SCSI devices, there is often a per-request-queue depth, which needs
    to be respected before queuing one request.
    
    Currently blk-mq always dequeues the request first, then calls
    .queue_rq() to dispatch the request to lld. One obvious issue with this
    approach is that I/O merging may not be successful, because when the
    per-request-queue depth can't be respected, .queue_rq() has to return
    BLK_STS_RESOURCE, and then this request has to stay in hctx->dispatch
    list. This means it never gets a chance to be merged with other IO.
    
    This patch introduces .get_budget and .put_budget callback in blk_mq_ops,
    then we can try to get reserved budget first before dequeuing request.
    If the budget for queueing I/O can't be satisfied, we don't need to
    dequeue request at all. Hence the request can be left in the IO
    scheduler queue, for more merging opportunities.
    
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index ef15b3414da5..e413b732374e 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -30,7 +30,7 @@ void blk_mq_freeze_queue(struct request_queue *q);
 void blk_mq_free_queue(struct request_queue *q);
 int blk_mq_update_nr_requests(struct request_queue *q, unsigned int nr);
 void blk_mq_wake_waiters(struct request_queue *q);
-bool blk_mq_dispatch_rq_list(struct request_queue *, struct list_head *);
+bool blk_mq_dispatch_rq_list(struct request_queue *, struct list_head *, bool);
 void blk_mq_flush_busy_ctxs(struct blk_mq_hw_ctx *hctx, struct list_head *list);
 bool blk_mq_hctx_has_pending(struct blk_mq_hw_ctx *hctx);
 bool blk_mq_get_driver_tag(struct request *rq, struct blk_mq_hw_ctx **hctx,
@@ -137,4 +137,22 @@ static inline bool blk_mq_hw_queue_mapped(struct blk_mq_hw_ctx *hctx)
 void blk_mq_in_flight(struct request_queue *q, struct hd_struct *part,
 			unsigned int inflight[2]);
 
+static inline void blk_mq_put_dispatch_budget(struct blk_mq_hw_ctx *hctx)
+{
+	struct request_queue *q = hctx->queue;
+
+	if (q->mq_ops->put_budget)
+		q->mq_ops->put_budget(hctx);
+}
+
+static inline blk_status_t blk_mq_get_dispatch_budget(
+		struct blk_mq_hw_ctx *hctx)
+{
+	struct request_queue *q = hctx->queue;
+
+	if (q->mq_ops->get_budget)
+		return q->mq_ops->get_budget(hctx);
+	return BLK_STS_OK;
+}
+
 #endif

commit 157f377beb710e84bd8bc7a3c4475c0674ebebd7
Author: Jens Axboe <axboe@kernel.dk>
Date:   Mon Sep 11 16:43:57 2017 -0600

    block: directly insert blk-mq request from blk_insert_cloned_request()
    
    A NULL pointer crash was reported for the case of having the BFQ IO
    scheduler attached to the underlying blk-mq paths of a DM multipath
    device.  The crash occured in blk_mq_sched_insert_request()'s call to
    e->type->ops.mq.insert_requests().
    
    Paolo Valente correctly summarized why the crash occured with:
    "the call chain (dm_mq_queue_rq -> map_request -> setup_clone ->
    blk_rq_prep_clone) creates a cloned request without invoking
    e->type->ops.mq.prepare_request for the target elevator e.  The cloned
    request is therefore not initialized for the scheduler, but it is
    however inserted into the scheduler by blk_mq_sched_insert_request."
    
    All said, a request-based DM multipath device's IO scheduler should be
    the only one used -- when the original requests are issued to the
    underlying paths as cloned requests they are inserted directly in the
    underlying dispatch queue(s) rather than through an additional elevator.
    
    But commit bd166ef18 ("blk-mq-sched: add framework for MQ capable IO
    schedulers") switched blk_insert_cloned_request() from using
    blk_mq_insert_request() to blk_mq_sched_insert_request().  Which
    incorrectly added elevator machinery into a call chain that isn't
    supposed to have any.
    
    To fix this introduce a blk-mq private blk_mq_request_bypass_insert()
    that blk_insert_cloned_request() calls to insert the request without
    involving any elevator that may be attached to the cloned request's
    request_queue.
    
    Fixes: bd166ef183c2 ("blk-mq-sched: add framework for MQ capable IO schedulers")
    Cc: stable@vger.kernel.org
    Reported-by: Bart Van Assche <Bart.VanAssche@wdc.com>
    Tested-by: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 98252b79b80b..ef15b3414da5 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -54,6 +54,7 @@ int blk_mq_alloc_rqs(struct blk_mq_tag_set *set, struct blk_mq_tags *tags,
  */
 void __blk_mq_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
 				bool at_head);
+void blk_mq_request_bypass_insert(struct request *rq);
 void blk_mq_insert_requests(struct blk_mq_hw_ctx *hctx, struct blk_mq_ctx *ctx,
 				struct list_head *list);
 

commit f299b7c7a9dee64425e5965bd4f56dc024c1befc
Author: Jens Axboe <axboe@kernel.dk>
Date:   Tue Aug 8 17:51:45 2017 -0600

    blk-mq: provide internal in-flight variant
    
    We don't have to inc/dec some counter, since we can just
    iterate the tags. That makes inc/dec a noop, but means we
    have to iterate busy tags to get an in-flight count.
    
    Reviewed-by: Bart Van Assche <bart.vanassche@wdc.com>
    Reviewed-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 60b01c0309bc..98252b79b80b 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -133,4 +133,7 @@ static inline bool blk_mq_hw_queue_mapped(struct blk_mq_hw_ctx *hctx)
 	return hctx->nr_ctx && hctx->tags;
 }
 
+void blk_mq_in_flight(struct request_queue *q, struct hd_struct *part,
+			unsigned int inflight[2]);
+
 #endif

commit 03ffbcdd7898c0b5299efeb9f18de927487ec1cf
Merge: 1b044f1cfc65 f9632de40ee0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 3 16:50:31 2017 -0700

    Merge branch 'irq-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull irq updates from Thomas Gleixner:
     "The irq department delivers:
    
       - Expand the generic infrastructure handling the irq migration on CPU
         hotplug and convert X86 over to it. (Thomas Gleixner)
    
         Aside of consolidating code this is a preparatory change for:
    
       - Finalizing the affinity management for multi-queue devices. The
         main change here is to shut down interrupts which are affine to a
         outgoing CPU and reenabling them when the CPU comes online again.
         That avoids moving interrupts pointlessly around and breaking and
         reestablishing affinities for no value. (Christoph Hellwig)
    
         Note: This contains also the BLOCK-MQ and NVME changes which depend
         on the rework of the irq core infrastructure. Jens acked them and
         agreed that they should go with the irq changes.
    
       - Consolidation of irq domain code (Marc Zyngier)
    
       - State tracking consolidation in the core code (Jeffy Chen)
    
       - Add debug infrastructure for hierarchical irq domains (Thomas
         Gleixner)
    
       - Infrastructure enhancement for managing generic interrupt chips via
         devmem (Bartosz Golaszewski)
    
       - Constification work all over the place (Tobias Klauser)
    
       - Two new interrupt controller drivers for MVEBU (Thomas Petazzoni)
    
       - The usual set of fixes, updates and enhancements all over the
         place"
    
    * 'irq-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (112 commits)
      irqchip/or1k-pic: Fix interrupt acknowledgement
      irqchip/irq-mvebu-gicp: Allocate enough memory for spi_bitmap
      irqchip/gic-v3: Fix out-of-bound access in gic_set_affinity
      nvme: Allocate queues for all possible CPUs
      blk-mq: Create hctx for each present CPU
      blk-mq: Include all present CPUs in the default queue mapping
      genirq: Avoid unnecessary low level irq function calls
      genirq: Set irq masked state when initializing irq_desc
      genirq/timings: Add infrastructure for estimating the next interrupt arrival time
      genirq/timings: Add infrastructure to track the interrupt timings
      genirq/debugfs: Remove pointless NULL pointer check
      irqchip/gic-v3-its: Don't assume GICv3 hardware supports 16bit INTID
      irqchip/gic-v3-its: Add ACPI NUMA node mapping
      irqchip/gic-v3-its-platform-msi: Make of_device_ids const
      irqchip/gic-v3-its: Make of_device_ids const
      irqchip/irq-mvebu-icu: Add new driver for Marvell ICU
      irqchip/irq-mvebu-gicp: Add new driver for Marvell GICP
      dt-bindings/interrupt-controller: Add DT binding for the Marvell ICU
      genirq/irqdomain: Remove auto-recursive hierarchy support
      irqchip/MSI: Use irq_domain_update_bus_token instead of an open coded access
      ...

commit 4b855ad37194f7bdbb200ce7a1c7051fecb56a08
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Jun 26 12:20:57 2017 +0200

    blk-mq: Create hctx for each present CPU
    
    Currently we only create hctx for online CPUs, which can lead to a lot
    of churn due to frequent soft offline / online operations.  Instead
    allocate one for each present CPU to avoid this and dramatically simplify
    the code.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Jens Axboe <axboe@kernel.dk>
    Cc: Keith Busch <keith.busch@intel.com>
    Cc: linux-block@vger.kernel.org
    Cc: linux-nvme@lists.infradead.org
    Link: http://lkml.kernel.org/r/20170626102058.10200-3-hch@lst.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index cc67b48e3551..558df56544d2 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -56,11 +56,6 @@ void __blk_mq_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
 				bool at_head);
 void blk_mq_insert_requests(struct blk_mq_hw_ctx *hctx, struct blk_mq_ctx *ctx,
 				struct list_head *list);
-/*
- * CPU hotplug helpers
- */
-void blk_mq_enable_hotplug(void);
-void blk_mq_disable_hotplug(void);
 
 /*
  * CPU -> queue mappings

commit e4cdf1a1cb161a648cc1ed7d6148fc3b99a1b3f5
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Jun 16 18:15:27 2017 +0200

    blk-mq: remove __blk_mq_alloc_request
    
    Move most code into blk_mq_rq_ctx_init, and the rest into
    blk_mq_get_request.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 6a509a8eb3fb..1a06fdf9fd4d 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -128,12 +128,6 @@ static inline struct blk_mq_tags *blk_mq_tags_from_data(struct blk_mq_alloc_data
 	return data->hctx->tags;
 }
 
-/*
- * Internal helpers for request allocation/init/free
- */
-struct request *__blk_mq_alloc_request(struct blk_mq_alloc_data *data,
-					unsigned int op);
-
 static inline bool blk_mq_hctx_stopped(struct blk_mq_hw_ctx *hctx)
 {
 	return test_bit(BLK_MQ_S_STOPPED, &hctx->state);

commit 6af54051a07041d8d4e36b1b01136a0db4eb7e23
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Jun 16 18:15:22 2017 +0200

    blk-mq: simplify blk_mq_free_request
    
    Merge three functions only tail-called by blk_mq_free_request into
    blk_mq_free_request.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 806fed53f607..6a509a8eb3fb 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -131,9 +131,6 @@ static inline struct blk_mq_tags *blk_mq_tags_from_data(struct blk_mq_alloc_data
 /*
  * Internal helpers for request allocation/init/free
  */
-void __blk_mq_finish_request(struct blk_mq_hw_ctx *hctx, struct blk_mq_ctx *ctx,
-				struct request *rq);
-void blk_mq_finish_request(struct request *rq);
 struct request *__blk_mq_alloc_request(struct blk_mq_alloc_data *data,
 					unsigned int op);
 

commit 6e15cf2a0bc1a75237ed8ae6293db707e471bb81
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Jun 16 18:15:18 2017 +0200

    blk-mq: mark blk_mq_rq_ctx_init static
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index cc67b48e3551..806fed53f607 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -131,8 +131,6 @@ static inline struct blk_mq_tags *blk_mq_tags_from_data(struct blk_mq_alloc_data
 /*
  * Internal helpers for request allocation/init/free
  */
-void blk_mq_rq_ctx_init(struct request_queue *q, struct blk_mq_ctx *ctx,
-			struct request *rq, unsigned int op);
 void __blk_mq_finish_request(struct blk_mq_hw_ctx *hctx, struct blk_mq_ctx *ctx,
 				struct request *rq);
 void blk_mq_finish_request(struct request *rq);

commit d173a25165c124442182f6b21d0c2ec381a0eebe
Author: Omar Sandoval <osandov@fb.com>
Date:   Thu May 4 00:31:30 2017 -0700

    blk-mq: move debugfs declarations to a separate header file
    
    Preparation for adding more declarations.
    
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 2814a14e529c..cc67b48e3551 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -83,34 +83,6 @@ extern int blk_mq_sysfs_register(struct request_queue *q);
 extern void blk_mq_sysfs_unregister(struct request_queue *q);
 extern void blk_mq_hctx_kobj_init(struct blk_mq_hw_ctx *hctx);
 
-/*
- * debugfs helpers
- */
-#ifdef CONFIG_BLK_DEBUG_FS
-int blk_mq_debugfs_register(struct request_queue *q);
-void blk_mq_debugfs_unregister(struct request_queue *q);
-int blk_mq_debugfs_register_mq(struct request_queue *q);
-void blk_mq_debugfs_unregister_mq(struct request_queue *q);
-#else
-static inline int blk_mq_debugfs_register(struct request_queue *q)
-{
-	return 0;
-}
-
-static inline void blk_mq_debugfs_unregister(struct request_queue *q)
-{
-}
-
-static inline int blk_mq_debugfs_register_mq(struct request_queue *q)
-{
-	return 0;
-}
-
-static inline void blk_mq_debugfs_unregister_mq(struct request_queue *q)
-{
-}
-#endif
-
 extern void blk_mq_rq_timed_out(struct request *req, bool reserved);
 
 void blk_mq_release(struct request_queue *q);

commit 62d6c9496a2bded5262ac9f675b27facb3cd4ce8
Author: Bart Van Assche <bart.vanassche@sandisk.com>
Date:   Wed Apr 26 13:47:50 2017 -0700

    blk-mq-debugfs: Rename functions for registering and unregistering the mq directory
    
    Since the blk_mq_debugfs_*register_hctxs() functions register and
    unregister all attributes under the "mq" directory, rename these
    into blk_mq_debugfs_*register_mq().
    
    Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Reviewed-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 9049c0f11505..2814a14e529c 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -89,8 +89,8 @@ extern void blk_mq_hctx_kobj_init(struct blk_mq_hw_ctx *hctx);
 #ifdef CONFIG_BLK_DEBUG_FS
 int blk_mq_debugfs_register(struct request_queue *q);
 void blk_mq_debugfs_unregister(struct request_queue *q);
-int blk_mq_debugfs_register_hctxs(struct request_queue *q);
-void blk_mq_debugfs_unregister_hctxs(struct request_queue *q);
+int blk_mq_debugfs_register_mq(struct request_queue *q);
+void blk_mq_debugfs_unregister_mq(struct request_queue *q);
 #else
 static inline int blk_mq_debugfs_register(struct request_queue *q)
 {
@@ -101,12 +101,12 @@ static inline void blk_mq_debugfs_unregister(struct request_queue *q)
 {
 }
 
-static inline int blk_mq_debugfs_register_hctxs(struct request_queue *q)
+static inline int blk_mq_debugfs_register_mq(struct request_queue *q)
 {
 	return 0;
 }
 
-static inline void blk_mq_debugfs_unregister_hctxs(struct request_queue *q)
+static inline void blk_mq_debugfs_unregister_mq(struct request_queue *q)
 {
 }
 #endif

commit 4c9e4019f1880bea7ae6e9603d14c2ed0f1c4e23
Author: Bart Van Assche <bart.vanassche@sandisk.com>
Date:   Wed Apr 26 13:47:49 2017 -0700

    blk-mq: Let blk_mq_debugfs_register() look up the queue name
    
    A later patch will move the call of blk_mq_debugfs_register() to
    a function to which the queue name is not passed as an argument.
    To avoid having to add a 'name' argument to multiple callers, let
    blk_mq_debugfs_register() look up the queue name.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Reviewed-by: Omar Sandoval <osandov@fb.com>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 7d955c756810..9049c0f11505 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -87,13 +87,12 @@ extern void blk_mq_hctx_kobj_init(struct blk_mq_hw_ctx *hctx);
  * debugfs helpers
  */
 #ifdef CONFIG_BLK_DEBUG_FS
-int blk_mq_debugfs_register(struct request_queue *q, const char *name);
+int blk_mq_debugfs_register(struct request_queue *q);
 void blk_mq_debugfs_unregister(struct request_queue *q);
 int blk_mq_debugfs_register_hctxs(struct request_queue *q);
 void blk_mq_debugfs_unregister_hctxs(struct request_queue *q);
 #else
-static inline int blk_mq_debugfs_register(struct request_queue *q,
-					  const char *name)
+static inline int blk_mq_debugfs_register(struct request_queue *q)
 {
 	return 0;
 }

commit 2d0364c8c1a97a1d44760895a982221c06af0f35
Author: Bart Van Assche <bart.vanassche@sandisk.com>
Date:   Wed Apr 26 13:47:48 2017 -0700

    blk-mq: Register <dev>/queue/mq after having registered <dev>/queue
    
    A later patch in this series will modify blk_mq_debugfs_register()
    such that it uses q->kobj.parent to determine the name of a
    request queue. Hence make sure that that pointer is initialized
    before blk_mq_debugfs_register() is called. To avoid lock inversion,
    protect sysfs / debugfs registration with the queue sysfs_lock
    instead of the global mutex all_q_mutex.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Reviewed-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 524f44742816..7d955c756810 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -78,6 +78,7 @@ static inline struct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *q,
  */
 extern void blk_mq_sysfs_init(struct request_queue *q);
 extern void blk_mq_sysfs_deinit(struct request_queue *q);
+extern int __blk_mq_register_dev(struct device *dev, struct request_queue *q);
 extern int blk_mq_sysfs_register(struct request_queue *q);
 extern void blk_mq_sysfs_unregister(struct request_queue *q);
 extern void blk_mq_hctx_kobj_init(struct blk_mq_hw_ctx *hctx);

commit 229a92873f3afc20b0d91aaaec08cbc11689dd8b
Author: Omar Sandoval <osandov@fb.com>
Date:   Fri Apr 14 00:59:59 2017 -0700

    blk-mq: add shallow depth option for blk_mq_get_tag()
    
    Wire up the sbitmap_get_shallow() operation to the tag code so that a
    caller can limit the number of tags available to it.
    
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 7e6f2e467696..524f44742816 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -141,6 +141,7 @@ struct blk_mq_alloc_data {
 	/* input parameter */
 	struct request_queue *q;
 	unsigned int flags;
+	unsigned int shallow_depth;
 
 	/* input & output parameter */
 	struct blk_mq_ctx *ctx;

commit 65f619d2535197d97067eeeef75a40f25b552e69
Merge: fbbaf700e7b1 6d8c6c0f97ad
Author: Jens Axboe <axboe@fb.com>
Date:   Fri Apr 7 12:45:20 2017 -0600

    Merge branch 'for-linus' into for-4.12/block
    
    We've added a considerable amount of fixes for stalls and issues
    with the blk-mq scheduling in the 4.11 series since forking
    off the for-4.12/block branch. We need to do improvements on
    top of that for 4.12, so pull in the previous fixes to make
    our lives easier going forward.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

commit 81380ca10778b99dce98940cfc993214712df335
Author: Omar Sandoval <osandov@fb.com>
Date:   Fri Apr 7 08:56:26 2017 -0600

    blk-mq: use the right hctx when getting a driver tag fails
    
    While dispatching requests, if we fail to get a driver tag, we mark the
    hardware queue as waiting for a tag and put the requests on a
    hctx->dispatch list to be run later when a driver tag is freed. However,
    blk_mq_dispatch_rq_list() may dispatch requests from multiple hardware
    queues if using a single-queue scheduler with a multiqueue device. If
    blk_mq_get_driver_tag() fails, it doesn't update the hardware queue we
    are processing. This means we end up using the hardware queue of the
    previous request, which may or may not be the same as that of the
    current request. If it isn't, the wrong hardware queue will end up
    waiting for a tag, and the requests will be on the wrong dispatch list,
    leading to a hang.
    
    The fix is twofold:
    
    1. Make sure we save which hardware queue we were trying to get a
       request for in blk_mq_get_driver_tag() regardless of whether it
       succeeds or not.
    2. Make blk_mq_dispatch_rq_list() take a request_queue instead of a
       blk_mq_hw_queue to make it clear that it must handle multiple
       hardware queues, since I've already messed this up on a couple of
       occasions.
    
    This didn't appear in testing with nvme and mq-deadline because nvme has
    more driver tags than the default number of scheduler tags. However,
    with the blk_mq_update_nr_hw_queues() fix, it showed up with nbd.
    
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index b79f9a7d8cf6..660a17e1d033 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -31,7 +31,7 @@ void blk_mq_freeze_queue(struct request_queue *q);
 void blk_mq_free_queue(struct request_queue *q);
 int blk_mq_update_nr_requests(struct request_queue *q, unsigned int nr);
 void blk_mq_wake_waiters(struct request_queue *q);
-bool blk_mq_dispatch_rq_list(struct blk_mq_hw_ctx *, struct list_head *);
+bool blk_mq_dispatch_rq_list(struct request_queue *, struct list_head *);
 void blk_mq_flush_busy_ctxs(struct blk_mq_hw_ctx *hctx, struct list_head *list);
 bool blk_mq_hctx_has_pending(struct blk_mq_hw_ctx *hctx);
 bool blk_mq_get_driver_tag(struct request *rq, struct blk_mq_hw_ctx **hctx,

commit 34dbad5d26e2f4b88e60f0e9ad03f99480802812
Author: Omar Sandoval <osandov@fb.com>
Date:   Tue Mar 21 08:56:08 2017 -0700

    blk-stat: convert to callback-based statistics reporting
    
    Currently, statistics are gathered in ~0.13s windows, and users grab the
    statistics whenever they need them. This is not ideal for both in-tree
    users:
    
    1. Writeback throttling wants its own dynamically sized window of
       statistics. Since the blk-stats statistics are reset after every
       window and the wbt windows don't line up with the blk-stats windows,
       wbt doesn't see every I/O.
    2. Polling currently grabs the statistics on every I/O. Again, depending
       on how the window lines up, we may miss some I/Os. It's also
       unnecessary overhead to get the statistics on every I/O; the hybrid
       polling heuristic would be just as happy with the statistics from the
       previous full window.
    
    This reworks the blk-stats infrastructure to be callback-based: users
    register a callback that they want called at a given time with all of
    the statistics from the window during which the callback was active.
    Users can dynamically bucketize the statistics. wbt and polling both
    currently use read vs. write, but polling can be extended to further
    subdivide based on request size.
    
    The callbacks are kept on an RCU list, and each callback has percpu
    stats buffers. There will only be a few users, so the overhead on the
    I/O completion side is low. The stats flushing is also simplified
    considerably: since the timer function is responsible for clearing the
    statistics, we don't have to worry about stale statistics.
    
    wbt is a trivial conversion. After the conversion, the windowing problem
    mentioned above is fixed.
    
    For polling, we register an extra callback that caches the previous
    window's statistics in the struct request_queue for the hybrid polling
    heuristic to use.
    
    Since we no longer have a single stats buffer for the request queue,
    this also removes the sysfs and debugfs stats entries. To replace those,
    we add a debugfs entry for the poll statistics.
    
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index b79f9a7d8cf6..8d49c06fc520 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -20,7 +20,6 @@ struct blk_mq_ctx {
 
 	/* incremented at completion time */
 	unsigned long		____cacheline_aligned_in_smp rq_completed[2];
-	struct blk_rq_stat	stat[2];
 
 	struct request_queue	*queue;
 	struct kobject		kobj;

commit 7ea5fe31c12dd8bcf4a9c5a4a7e8e23826a9a3b8
Author: Ming Lei <tom.leiming@gmail.com>
Date:   Wed Feb 22 18:14:00 2017 +0800

    blk-mq: make lifetime consitent between q/ctx and its kobject
    
    Currently from kobject view, both q->mq_kobj and ctx->kobj can
    be released during one cycle of blk_mq_register_dev() and
    blk_mq_unregister_dev(). Actually, sw queue's lifetime is
    same with its request queue's, which is covered by request_queue->kobj.
    
    So we don't need to call kobject_put() for the two kinds of
    kobject in __blk_mq_unregister_dev(), instead we do that
    in release handler of request queue.
    
    Signed-off-by: Ming Lei <tom.leiming@gmail.com>
    Tested-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index ad8bfd7473ef..b79f9a7d8cf6 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -78,6 +78,7 @@ static inline struct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *q,
  * sysfs helpers
  */
 extern void blk_mq_sysfs_init(struct request_queue *q);
+extern void blk_mq_sysfs_deinit(struct request_queue *q);
 extern int blk_mq_sysfs_register(struct request_queue *q);
 extern void blk_mq_sysfs_unregister(struct request_queue *q);
 extern void blk_mq_hctx_kobj_init(struct blk_mq_hw_ctx *hctx);

commit 737f98cfe7de8df7433a4d846850aa8efa44bd48
Author: Ming Lei <tom.leiming@gmail.com>
Date:   Wed Feb 22 18:13:59 2017 +0800

    blk-mq: initialize mq kobjects in blk_mq_init_allocated_queue()
    
    Both q->mq_kobj and sw queues' kobjects should have been initialized
    once, instead of doing that each add_disk context.
    
    Also this patch removes clearing of ctx in blk_mq_init_cpu_queues()
    because percpu allocator fills zero to allocated variable.
    
    This patch fixes one issue[1] reported from Omar.
    
    [1] kernel wearning when doing unbind/bind on one scsi-mq device
    
    [   19.347924] kobject (ffff8800791ea0b8): tried to init an initialized object, something is seriously wrong.
    [   19.349781] CPU: 1 PID: 84 Comm: kworker/u8:1 Not tainted 4.10.0-rc7-00210-g53f39eeaa263 #34
    [   19.350686] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.10.1-20161122_114906-anatol 04/01/2014
    [   19.350920] Workqueue: events_unbound async_run_entry_fn
    [   19.350920] Call Trace:
    [   19.350920]  dump_stack+0x63/0x83
    [   19.350920]  kobject_init+0x77/0x90
    [   19.350920]  blk_mq_register_dev+0x40/0x130
    [   19.350920]  blk_register_queue+0xb6/0x190
    [   19.350920]  device_add_disk+0x1ec/0x4b0
    [   19.350920]  sd_probe_async+0x10d/0x1c0 [sd_mod]
    [   19.350920]  async_run_entry_fn+0x48/0x150
    [   19.350920]  process_one_work+0x1d0/0x480
    [   19.350920]  worker_thread+0x48/0x4e0
    [   19.350920]  kthread+0x101/0x140
    [   19.350920]  ? process_one_work+0x480/0x480
    [   19.350920]  ? kthread_create_on_node+0x60/0x60
    [   19.350920]  ret_from_fork+0x2c/0x40
    
    Cc: Omar Sandoval <osandov@osandov.com>
    Signed-off-by: Ming Lei <tom.leiming@gmail.com>
    Tested-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 088ced003c13..ad8bfd7473ef 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -77,6 +77,7 @@ static inline struct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *q,
 /*
  * sysfs helpers
  */
+extern void blk_mq_sysfs_init(struct request_queue *q);
 extern int blk_mq_sysfs_register(struct request_queue *q);
 extern void blk_mq_sysfs_unregister(struct request_queue *q);
 extern void blk_mq_hctx_kobj_init(struct blk_mq_hw_ctx *hctx);

commit 59748398992c9c3e9d600e56cb2a5c0c546fe129
Author: Omar Sandoval <osandov@fb.com>
Date:   Mon Feb 27 09:47:54 2017 -0800

    blk-mq: kill blk_mq_set_alloc_data()
    
    Nothing is using it anymore.
    
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>
    Tested-by: Sagi Grimberg <sagi@grimberg.me>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 24b2256186f3..088ced003c13 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -146,16 +146,6 @@ struct blk_mq_alloc_data {
 	struct blk_mq_hw_ctx *hctx;
 };
 
-static inline void blk_mq_set_alloc_data(struct blk_mq_alloc_data *data,
-		struct request_queue *q, unsigned int flags,
-		struct blk_mq_ctx *ctx, struct blk_mq_hw_ctx *hctx)
-{
-	data->q = q;
-	data->flags = flags;
-	data->ctx = ctx;
-	data->hctx = hctx;
-}
-
 static inline struct blk_mq_tags *blk_mq_tags_from_data(struct blk_mq_alloc_data *data)
 {
 	if (data->flags & BLK_MQ_REQ_INTERNAL)

commit 18fbda91c6370d520278db9ee1e768b59ef5c4ab
Author: Omar Sandoval <osandov@fb.com>
Date:   Tue Jan 31 14:53:20 2017 -0800

    block: use same block debugfs directory for blk-mq and blktrace
    
    When I added the blk-mq debugging information to debugfs, I didn't
    notice that blktrace also creates a "block" directory in debugfs. Make
    them use the same dentry, now created in the core block code. Based on a
    patch from Jens.
    
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index b52abd62b1b0..24b2256186f3 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -85,16 +85,11 @@ extern void blk_mq_hctx_kobj_init(struct blk_mq_hw_ctx *hctx);
  * debugfs helpers
  */
 #ifdef CONFIG_BLK_DEBUG_FS
-void blk_mq_debugfs_init(void);
 int blk_mq_debugfs_register(struct request_queue *q, const char *name);
 void blk_mq_debugfs_unregister(struct request_queue *q);
 int blk_mq_debugfs_register_hctxs(struct request_queue *q);
 void blk_mq_debugfs_unregister_hctxs(struct request_queue *q);
 #else
-static inline void blk_mq_debugfs_init(void)
-{
-}
-
 static inline int blk_mq_debugfs_register(struct request_queue *q,
 					  const char *name)
 {

commit 400f73b23f457a82288814e21af57dbc9f3f2afd
Author: Omar Sandoval <osandov@fb.com>
Date:   Fri Jan 27 15:03:01 2017 -0700

    blk-mq: fix debugfs compilation issues
    
    This fixes a couple of problems:
    
    1. In the !CONFIG_DEBUG_FS case, the stub definitions were bogus.
    2. In the !CONFIG_BLOCK case, blk-mq-debugfs.c shouldn't be compiled at
       all.
    
    Fix the stub definitions and add a CONFIG_BLK_DEBUG_FS Kconfig option.
    
    Fixes: 07e4fead45e6 ("blk-mq: create debugfs directory tree")
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    
    Augment Kconfig description.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 57cdbf6c0cee..b52abd62b1b0 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -84,7 +84,7 @@ extern void blk_mq_hctx_kobj_init(struct blk_mq_hw_ctx *hctx);
 /*
  * debugfs helpers
  */
-#ifdef CONFIG_DEBUG_FS
+#ifdef CONFIG_BLK_DEBUG_FS
 void blk_mq_debugfs_init(void);
 int blk_mq_debugfs_register(struct request_queue *q, const char *name);
 void blk_mq_debugfs_unregister(struct request_queue *q);
@@ -95,21 +95,22 @@ static inline void blk_mq_debugfs_init(void)
 {
 }
 
-int blk_mq_debugfs_register(struct request_queue *q, const char *name);
+static inline int blk_mq_debugfs_register(struct request_queue *q,
+					  const char *name)
 {
 	return 0;
 }
 
-void blk_mq_debugfs_unregister(struct request_queue *q)
+static inline void blk_mq_debugfs_unregister(struct request_queue *q)
 {
 }
 
-int blk_mq_debugfs_register_hctxs(struct request_queue *q)
+static inline int blk_mq_debugfs_register_hctxs(struct request_queue *q)
 {
 	return 0;
 }
 
-void blk_mq_debugfs_unregister_hctxs(struct request_queue *q)
+static inline void blk_mq_debugfs_unregister_hctxs(struct request_queue *q)
 {
 }
 #endif

commit bd6737f1ae92e2f1c6e8362efe96dbe7f18fa07d
Author: Jens Axboe <axboe@fb.com>
Date:   Fri Jan 27 01:00:47 2017 -0700

    blk-mq-sched: add flush insertion into blk_mq_sched_insert_request()
    
    Instead of letting the caller check this and handle the details
    of inserting a flush request, put the logic in the scheduler
    insertion function. This fixes direct flush insertion outside
    of the usual make_request_fn calls, like from dm via
    blk_insert_cloned_request().
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 077a4003f1fd..57cdbf6c0cee 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -34,6 +34,8 @@ void blk_mq_wake_waiters(struct request_queue *q);
 bool blk_mq_dispatch_rq_list(struct blk_mq_hw_ctx *, struct list_head *);
 void blk_mq_flush_busy_ctxs(struct blk_mq_hw_ctx *hctx, struct list_head *list);
 bool blk_mq_hctx_has_pending(struct blk_mq_hw_ctx *hctx);
+bool blk_mq_get_driver_tag(struct request *rq, struct blk_mq_hw_ctx **hctx,
+				bool wait);
 
 /*
  * Internal helpers for allocating/freeing the request map

commit 50e1dab86aa2c10cbca2f754aae9542169403141
Author: Jens Axboe <axboe@fb.com>
Date:   Thu Jan 26 14:42:34 2017 -0700

    blk-mq-sched: fix starvation for multiple hardware queues and shared tags
    
    If we have both multiple hardware queues and shared tag map between
    devices, we need to ensure that we propagate the hardware queue
    restart bit higher up. This is because we can get into a situation
    where we don't have any IO pending on a hardware queue, yet we fail
    getting a tag to start new IO. If that happens, it's not enough to
    mark the hardware queue as needing a restart, we need to bubble
    that up to the higher level queue as well.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>
    Reviewed-by: Omar Sandoval <osandov@fb.com>
    Tested-by: Hannes Reinecke <hare@suse.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 6c24b901acd7..077a4003f1fd 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -33,6 +33,7 @@ int blk_mq_update_nr_requests(struct request_queue *q, unsigned int nr);
 void blk_mq_wake_waiters(struct request_queue *q);
 bool blk_mq_dispatch_rq_list(struct blk_mq_hw_ctx *, struct list_head *);
 void blk_mq_flush_busy_ctxs(struct blk_mq_hw_ctx *hctx, struct list_head *list);
+bool blk_mq_hctx_has_pending(struct blk_mq_hw_ctx *hctx);
 
 /*
  * Internal helpers for allocating/freeing the request map

commit 07e4fead45e6e1932f0b960655ab554b6aab6a08
Author: Omar Sandoval <osandov@fb.com>
Date:   Wed Jan 25 08:06:40 2017 -0800

    blk-mq: create debugfs directory tree
    
    In preparation for putting blk-mq debugging information in debugfs,
    create a directory tree mirroring the one in sysfs:
    
        # tree -d /sys/kernel/debug/block
        /sys/kernel/debug/block
        |-- nvme0n1
        |   `-- mq
        |       |-- 0
        |       |   `-- cpu0
        |       |-- 1
        |       |   `-- cpu1
        |       |-- 2
        |       |   `-- cpu2
        |       `-- 3
        |           `-- cpu3
        `-- vda
            `-- mq
                `-- 0
                    |-- cpu0
                    |-- cpu1
                    |-- cpu2
                    `-- cpu3
    
    Also add the scaffolding for the actual files that will go in here,
    either under the hardware queue or software queue directories.
    
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 0c7c034d9ddd..6c24b901acd7 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -78,6 +78,39 @@ extern int blk_mq_sysfs_register(struct request_queue *q);
 extern void blk_mq_sysfs_unregister(struct request_queue *q);
 extern void blk_mq_hctx_kobj_init(struct blk_mq_hw_ctx *hctx);
 
+/*
+ * debugfs helpers
+ */
+#ifdef CONFIG_DEBUG_FS
+void blk_mq_debugfs_init(void);
+int blk_mq_debugfs_register(struct request_queue *q, const char *name);
+void blk_mq_debugfs_unregister(struct request_queue *q);
+int blk_mq_debugfs_register_hctxs(struct request_queue *q);
+void blk_mq_debugfs_unregister_hctxs(struct request_queue *q);
+#else
+static inline void blk_mq_debugfs_init(void)
+{
+}
+
+int blk_mq_debugfs_register(struct request_queue *q, const char *name);
+{
+	return 0;
+}
+
+void blk_mq_debugfs_unregister(struct request_queue *q)
+{
+}
+
+int blk_mq_debugfs_register_hctxs(struct request_queue *q)
+{
+	return 0;
+}
+
+void blk_mq_debugfs_unregister_hctxs(struct request_queue *q)
+{
+}
+#endif
+
 extern void blk_mq_rq_timed_out(struct request *req, bool reserved);
 
 void blk_mq_release(struct request_queue *q);

commit bd166ef183c263c5ced656d49ef19c7da4adc774
Author: Jens Axboe <axboe@fb.com>
Date:   Tue Jan 17 06:03:22 2017 -0700

    blk-mq-sched: add framework for MQ capable IO schedulers
    
    This adds a set of hooks that intercepts the blk-mq path of
    allocating/inserting/issuing/completing requests, allowing
    us to develop a scheduler within that framework.
    
    We reuse the existing elevator scheduler API on the registration
    side, but augment that with the scheduler flagging support for
    the blk-mq interfce, and with a separate set of ops hooks for MQ
    devices.
    
    We split driver and scheduler tags, so we can run the scheduling
    independently of device queue depth.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>
    Reviewed-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Reviewed-by: Omar Sandoval <osandov@fb.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 1b279b02d0f6..0c7c034d9ddd 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -52,6 +52,8 @@ int blk_mq_alloc_rqs(struct blk_mq_tag_set *set, struct blk_mq_tags *tags,
  */
 void __blk_mq_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
 				bool at_head);
+void blk_mq_insert_requests(struct blk_mq_hw_ctx *hctx, struct blk_mq_ctx *ctx,
+				struct list_head *list);
 /*
  * CPU hotplug helpers
  */
@@ -124,6 +126,9 @@ static inline void blk_mq_set_alloc_data(struct blk_mq_alloc_data *data,
 
 static inline struct blk_mq_tags *blk_mq_tags_from_data(struct blk_mq_alloc_data *data)
 {
+	if (data->flags & BLK_MQ_REQ_INTERNAL)
+		return data->hctx->sched_tags;
+
 	return data->hctx->tags;
 }
 
@@ -132,8 +137,9 @@ static inline struct blk_mq_tags *blk_mq_tags_from_data(struct blk_mq_alloc_data
  */
 void blk_mq_rq_ctx_init(struct request_queue *q, struct blk_mq_ctx *ctx,
 			struct request *rq, unsigned int op);
-void __blk_mq_free_request(struct blk_mq_hw_ctx *hctx, struct blk_mq_ctx *ctx,
+void __blk_mq_finish_request(struct blk_mq_hw_ctx *hctx, struct blk_mq_ctx *ctx,
 				struct request *rq);
+void blk_mq_finish_request(struct request *rq);
 struct request *__blk_mq_alloc_request(struct blk_mq_alloc_data *data,
 					unsigned int op);
 

commit cc71a6f43886a8af57dbbce2a45b4b2aaf570fe6
Author: Jens Axboe <axboe@fb.com>
Date:   Wed Jan 11 14:29:56 2017 -0700

    blk-mq: abstract out helpers for allocating/freeing tag maps
    
    Prep patch for adding an extra tag map for scheduler requests.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>
    Reviewed-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Reviewed-by: Omar Sandoval <osandov@fb.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 48b7771eb192..1b279b02d0f6 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -37,17 +37,21 @@ void blk_mq_flush_busy_ctxs(struct blk_mq_hw_ctx *hctx, struct list_head *list);
 /*
  * Internal helpers for allocating/freeing the request map
  */
-void blk_mq_free_rq_map(struct blk_mq_tag_set *set, struct blk_mq_tags *tags,
-			unsigned int hctx_idx);
-struct blk_mq_tags *blk_mq_init_rq_map(struct blk_mq_tag_set *set,
-					unsigned int hctx_idx);
+void blk_mq_free_rqs(struct blk_mq_tag_set *set, struct blk_mq_tags *tags,
+		     unsigned int hctx_idx);
+void blk_mq_free_rq_map(struct blk_mq_tags *tags);
+struct blk_mq_tags *blk_mq_alloc_rq_map(struct blk_mq_tag_set *set,
+					unsigned int hctx_idx,
+					unsigned int nr_tags,
+					unsigned int reserved_tags);
+int blk_mq_alloc_rqs(struct blk_mq_tag_set *set, struct blk_mq_tags *tags,
+		     unsigned int hctx_idx, unsigned int depth);
 
 /*
  * Internal helpers for request insertion into sw queues
  */
 void __blk_mq_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
 				bool at_head);
-
 /*
  * CPU hotplug helpers
  */

commit 4941115bef2bc891aa00a2f0edeaf06dc982325a
Author: Jens Axboe <axboe@fb.com>
Date:   Fri Jan 13 08:09:05 2017 -0700

    blk-mq-tag: cleanup the normal/reserved tag allocation
    
    This is in preparation for having another tag set available. Cleanup
    the parameters, and allow passing in of tags for blk_mq_put_tag().
    
    Signed-off-by: Jens Axboe <axboe@fb.com>
    [hch: even more cleanups]
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Omar Sandoval <osandov@fb.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index e59f5ca520a2..48b7771eb192 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -118,6 +118,11 @@ static inline void blk_mq_set_alloc_data(struct blk_mq_alloc_data *data,
 	data->hctx = hctx;
 }
 
+static inline struct blk_mq_tags *blk_mq_tags_from_data(struct blk_mq_alloc_data *data)
+{
+	return data->hctx->tags;
+}
+
 /*
  * Internal helpers for request allocation/init/free
  */

commit 2c3ad667902ef6f4b60ef0a3c6f7d8c2b007769a
Author: Jens Axboe <axboe@fb.com>
Date:   Wed Dec 14 14:34:47 2016 -0700

    blk-mq: export some helpers we need to the scheduling framework
    
    Signed-off-by: Jens Axboe <axboe@fb.com>
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Reviewed-by: Omar Sandoval <osandov@fb.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 63e9116cddbd..e59f5ca520a2 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -32,6 +32,21 @@ void blk_mq_free_queue(struct request_queue *q);
 int blk_mq_update_nr_requests(struct request_queue *q, unsigned int nr);
 void blk_mq_wake_waiters(struct request_queue *q);
 bool blk_mq_dispatch_rq_list(struct blk_mq_hw_ctx *, struct list_head *);
+void blk_mq_flush_busy_ctxs(struct blk_mq_hw_ctx *hctx, struct list_head *list);
+
+/*
+ * Internal helpers for allocating/freeing the request map
+ */
+void blk_mq_free_rq_map(struct blk_mq_tag_set *set, struct blk_mq_tags *tags,
+			unsigned int hctx_idx);
+struct blk_mq_tags *blk_mq_init_rq_map(struct blk_mq_tag_set *set,
+					unsigned int hctx_idx);
+
+/*
+ * Internal helpers for request insertion into sw queues
+ */
+void __blk_mq_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
+				bool at_head);
 
 /*
  * CPU hotplug helpers
@@ -103,6 +118,16 @@ static inline void blk_mq_set_alloc_data(struct blk_mq_alloc_data *data,
 	data->hctx = hctx;
 }
 
+/*
+ * Internal helpers for request allocation/init/free
+ */
+void blk_mq_rq_ctx_init(struct request_queue *q, struct blk_mq_ctx *ctx,
+			struct request *rq, unsigned int op);
+void __blk_mq_free_request(struct blk_mq_hw_ctx *hctx, struct blk_mq_ctx *ctx,
+				struct request *rq);
+struct request *__blk_mq_alloc_request(struct blk_mq_alloc_data *data,
+					unsigned int op);
+
 static inline bool blk_mq_hctx_stopped(struct blk_mq_hw_ctx *hctx)
 {
 	return test_bit(BLK_MQ_S_STOPPED, &hctx->state);

commit a829a8445f09036404060f4d6489cb13433f4304
Merge: 84b607913442 f5b893c94715
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Dec 14 10:49:33 2016 -0800

    Merge tag 'scsi-misc' of git://git.kernel.org/pub/scm/linux/kernel/git/jejb/scsi
    
    Pull SCSI updates from James Bottomley:
     "This update includes the usual round of major driver updates (ncr5380,
      lpfc, hisi_sas, megaraid_sas, ufs, ibmvscsis, mpt3sas).
    
      There's also an assortment of minor fixes, mostly in error legs or
      other not very user visible stuff. The major change is the
      pci_alloc_irq_vectors replacement for the old pci_msix_.. calls; this
      effectively makes IRQ mapping generic for the drivers and allows
      blk_mq to use the information"
    
    * tag 'scsi-misc' of git://git.kernel.org/pub/scm/linux/kernel/git/jejb/scsi: (256 commits)
      scsi: qla4xxx: switch to pci_alloc_irq_vectors
      scsi: hisi_sas: support deferred probe for v2 hw
      scsi: megaraid_sas: switch to pci_alloc_irq_vectors
      scsi: scsi_devinfo: remove synchronous ALUA for NETAPP devices
      scsi: be2iscsi: set errno on error path
      scsi: be2iscsi: set errno on error path
      scsi: hpsa: fallback to use legacy REPORT PHYS command
      scsi: scsi_dh_alua: Fix RCU annotations
      scsi: hpsa: use %phN for short hex dumps
      scsi: hisi_sas: fix free'ing in probe and remove
      scsi: isci: switch to pci_alloc_irq_vectors
      scsi: ipr: Fix runaway IRQs when falling back from MSI to LSI
      scsi: dpt_i2o: double free on error path
      scsi: cxlflash: Migrate scsi command pointer to AFU command
      scsi: cxlflash: Migrate IOARRIN specific routines to function pointers
      scsi: cxlflash: Cleanup queuecommand()
      scsi: cxlflash: Cleanup send_tmf()
      scsi: cxlflash: Remove AFU command lock
      scsi: cxlflash: Wait for active AFU commands to timeout upon tear down
      scsi: cxlflash: Remove private command pool
      ...

commit f04c3df3efeca0d226aeff7ef595e12a0b807ab2
Author: Jens Axboe <axboe@fb.com>
Date:   Wed Dec 7 08:41:17 2016 -0700

    blk-mq: abstract out blk_mq_dispatch_rq_list() helper
    
    Takes a list of requests, and dispatches it. Moves any residual
    requests to the dispatch list.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>
    Reviewed-by: Hannes Reinecke <hare@suse.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index b444370ae05b..3a54dd32a6fc 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -31,6 +31,7 @@ void blk_mq_freeze_queue(struct request_queue *q);
 void blk_mq_free_queue(struct request_queue *q);
 int blk_mq_update_nr_requests(struct request_queue *q, unsigned int nr);
 void blk_mq_wake_waiters(struct request_queue *q);
+bool blk_mq_dispatch_rq_list(struct blk_mq_hw_ctx *, struct list_head *);
 
 /*
  * CPU hotplug helpers

commit cf43e6be865a582ba66ee4747ae27a0513f6bba1
Author: Jens Axboe <axboe@fb.com>
Date:   Mon Nov 7 21:32:37 2016 -0700

    block: add scalable completion tracking of requests
    
    For legacy block, we simply track them in the request queue. For
    blk-mq, we track them on a per-sw queue basis, which we can then
    sum up through the hardware queues and finally to a per device
    state.
    
    The stats are tracked in, roughly, 0.1s interval windows.
    
    Add sysfs files to display the stats.
    
    The feature is off by default, to avoid any extra overhead. In-kernel
    users of it can turn it on by setting QUEUE_FLAG_STATS in the queue
    flags. We currently don't turn it on if someone just reads any of
    the stats files, that is something we could add as well.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index ac772dac7ce8..b444370ae05b 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -1,6 +1,8 @@
 #ifndef INT_BLK_MQ_H
 #define INT_BLK_MQ_H
 
+#include "blk-stat.h"
+
 struct blk_mq_tag_set;
 
 struct blk_mq_ctx {
@@ -18,6 +20,7 @@ struct blk_mq_ctx {
 
 	/* incremented at completion time */
 	unsigned long		____cacheline_aligned_in_smp rq_completed[2];
+	struct blk_rq_stat	stat[2];
 
 	struct request_queue	*queue;
 	struct kobject		kobj;

commit 9e5a7e22951bc12ee45cb617919d57b5efce56b5
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Nov 1 08:12:47 2016 -0600

    blk-mq: export blk_mq_map_queues
    
    This will allow SCSI to have a single blk_mq_ops structure that either
    lets the LLDD map the queues to PCIe MSIx vectors or use the default.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Reviewed-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index e5d25249028c..5347f011e90d 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -38,7 +38,6 @@ void blk_mq_disable_hotplug(void);
 /*
  * CPU -> queue mappings
  */
-int blk_mq_map_queues(struct blk_mq_tag_set *set);
 extern int blk_mq_hw_queue_to_node(unsigned int *map, unsigned int);
 
 static inline struct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *q,

commit 5d1b25c1ecabb37f8eb58c8e9dd74f77f703e5d9
Author: Bart Van Assche <bart.vanassche@sandisk.com>
Date:   Fri Oct 28 17:19:15 2016 -0700

    blk-mq: Introduce blk_mq_hctx_stopped()
    
    Multiple functions test the BLK_MQ_S_STOPPED bit so introduce
    a helper function that performs this test.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Reviewed-by: Ming Lei <tom.leiming@gmail.com>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index e5d25249028c..ac772dac7ce8 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -100,6 +100,11 @@ static inline void blk_mq_set_alloc_data(struct blk_mq_alloc_data *data,
 	data->hctx = hctx;
 }
 
+static inline bool blk_mq_hctx_stopped(struct blk_mq_hw_ctx *hctx)
+{
+	return test_bit(BLK_MQ_S_STOPPED, &hctx->state);
+}
+
 static inline bool blk_mq_hw_queue_mapped(struct blk_mq_hw_ctx *hctx)
 {
 	return hctx->nr_ctx && hctx->tags;

commit 24532f768121b07b16178ffb40442ece43365cbd
Merge: 12e3d3cdd975 97a32864e6de
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Oct 9 17:32:20 2016 -0700

    Merge branch 'for-4.9/block-smp' of git://git.kernel.dk/linux-block
    
    Pull blk-mq CPU hotplug update from Jens Axboe:
     "This is the conversion of blk-mq to the new hotplug state machine"
    
    * 'for-4.9/block-smp' of git://git.kernel.dk/linux-block:
      blk-mq: fixup "Convert to new hotplug state machine"
      blk-mq: Convert to new hotplug state machine
      blk-mq/cpu-notif: Convert to new hotplug state machine

commit 12e3d3cdd975fe986cc5c35f60b1467a8ec20b80
Merge: 48915c2cbc77 8ec2ef2b66ea
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Oct 9 17:29:33 2016 -0700

    Merge branch 'for-4.9/block-irq' of git://git.kernel.dk/linux-block
    
    Pull blk-mq irq/cpu mapping updates from Jens Axboe:
     "This is the block-irq topic branch for 4.9-rc. It's mostly from
      Christoph, and it allows drivers to specify their own mappings, and
      more importantly, to share the blk-mq mappings with the IRQ affinity
      mappings. It's a good step towards making this work better out of the
      box"
    
    * 'for-4.9/block-irq' of git://git.kernel.dk/linux-block:
      blk_mq: linux/blk-mq.h does not include all the headers it depends on
      blk-mq: kill unused blk_mq_create_mq_map()
      blk-mq: get rid of the cpumask in struct blk_mq_tags
      nvme: remove the post_scan callout
      nvme: switch to use pci_alloc_irq_vectors
      blk-mq: provide a default queue mapping for PCI device
      blk-mq: allow the driver to pass in a queue mapping
      blk-mq: remove ->map_queue
      blk-mq: only allocate a single mq_map per tag_set
      blk-mq: don't redistribute hardware queues on a CPU hotplug event

commit 9467f85960a31d56f95371516e55e210e1e3d51c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Sep 22 08:05:17 2016 -0600

    blk-mq/cpu-notif: Convert to new hotplug state machine
    
    Replace the block-mq notifier list management with the multi instance
    facility in the cpu hotplug state machine.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: linux-block@vger.kernel.org
    Cc: rt@linutronix.de
    Cc: Christoph Hellwing <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 9b15d2ef7f7b..8fd3cc4fb715 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -32,13 +32,6 @@ void blk_mq_wake_waiters(struct request_queue *q);
 /*
  * CPU hotplug helpers
  */
-struct blk_mq_cpu_notifier;
-void blk_mq_init_cpu_notifier(struct blk_mq_cpu_notifier *notifier,
-			      int (*fn)(void *, unsigned long, unsigned int),
-			      void *data);
-void blk_mq_register_cpu_notifier(struct blk_mq_cpu_notifier *notifier);
-void blk_mq_unregister_cpu_notifier(struct blk_mq_cpu_notifier *notifier);
-void blk_mq_cpu_init(void);
 void blk_mq_enable_hotplug(void);
 void blk_mq_disable_hotplug(void);
 

commit 40aabb67464d5aad9ca3d2a5fedee56e2ff45aa0
Author: Omar Sandoval <osandov@fb.com>
Date:   Sat Sep 17 01:28:23 2016 -0700

    sbitmap: push per-cpu last_tag into sbitmap_queue
    
    Allocating your own per-cpu allocation hint separately makes for an
    awkward API. Instead, allocate the per-cpu hint as part of the struct
    sbitmap_queue. There's no point for a struct sbitmap_queue without the
    cache, but you can still use a bare struct sbitmap.
    
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 71831f970fd3..9b15d2ef7f7b 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -12,8 +12,6 @@ struct blk_mq_ctx {
 	unsigned int		cpu;
 	unsigned int		index_hw;
 
-	unsigned int		last_tag ____cacheline_aligned_in_smp;
-
 	/* incremented at dispatch time */
 	unsigned long		rq_dispatched[2];
 	unsigned long		rq_merged;

commit 88459642cba452630326b9cab1c651e09577d4e4
Author: Omar Sandoval <osandov@fb.com>
Date:   Sat Sep 17 08:38:44 2016 -0600

    blk-mq: abstract tag allocation out into sbitmap library
    
    This is a generally useful data structure, so make it available to
    anyone else who might want to use it. It's also a nice cleanup
    separating the allocation logic from the rest of the tag handling logic.
    
    The code is behind a new Kconfig option, CONFIG_SBITMAP, which is only
    selected by CONFIG_BLOCK for now.
    
    This should be a complete noop functionality-wise.
    
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 9087b11037b7..71831f970fd3 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -63,15 +63,6 @@ extern void blk_mq_rq_timed_out(struct request *req, bool reserved);
 
 void blk_mq_release(struct request_queue *q);
 
-/*
- * Basic implementation of sparser bitmap, allowing the user to spread
- * the bits over more cachelines.
- */
-struct blk_align_bitmap {
-	unsigned long word;
-	unsigned long depth;
-} ____cacheline_aligned_in_smp;
-
 static inline struct blk_mq_ctx *__blk_mq_get_ctx(struct request_queue *q,
 					   unsigned int cpu)
 {

commit da695ba236b993f07a540d35c17f271ef08c89f3
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Sep 14 16:18:55 2016 +0200

    blk-mq: allow the driver to pass in a queue mapping
    
    This allows drivers specify their own queue mapping by overriding the
    setup-time function that builds the mq_map.  This can be used for
    example to build the map based on the MSI-X vector mapping provided
    by the core interrupt layer for PCI devices.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Keith Busch <keith.busch@intel.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index ec774bf4aea2..c92bb7debf85 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -47,9 +47,7 @@ void blk_mq_disable_hotplug(void);
 /*
  * CPU -> queue mappings
  */
-extern unsigned int *blk_mq_make_queue_map(struct blk_mq_tag_set *set);
-extern int blk_mq_update_queue_map(unsigned int *map, unsigned int nr_queues,
-				   const struct cpumask *online_mask);
+int blk_mq_map_queues(struct blk_mq_tag_set *set);
 extern int blk_mq_hw_queue_to_node(unsigned int *map, unsigned int);
 
 static inline struct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *q,

commit 7d7e0f90b70f6c5367c2d1c9a7e87dd228bd0816
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Sep 14 16:18:54 2016 +0200

    blk-mq: remove ->map_queue
    
    All drivers use the default, so provide an inline version of it.  If we
    ever need other queue mapping we can add an optional method back,
    although supporting will also require major changes to the queue setup
    code.
    
    This provides better code generation, and better debugability as well.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Keith Busch <keith.busch@intel.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 9087b11037b7..ec774bf4aea2 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -52,6 +52,12 @@ extern int blk_mq_update_queue_map(unsigned int *map, unsigned int nr_queues,
 				   const struct cpumask *online_mask);
 extern int blk_mq_hw_queue_to_node(unsigned int *map, unsigned int);
 
+static inline struct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *q,
+		int cpu)
+{
+	return q->queue_hw_ctx[q->mq_map[cpu]];
+}
+
 /*
  * sysfs helpers
  */

commit 868f2f0b72068a097508b6e8870a8950fd8eb7ef
Author: Keith Busch <keith.busch@intel.com>
Date:   Thu Dec 17 17:08:14 2015 -0700

    blk-mq: dynamic h/w context count
    
    The hardware's provided queue count may change at runtime with resource
    provisioning. This patch allows a block driver to alter the number of
    h/w queues available when its resource count changes.
    
    The main part is a new blk-mq API to request a new number of h/w queues
    for a given live tag set. The new API freezes all queues using that set,
    then adjusts the allocated count prior to remapping these to CPUs.
    
    The bulk of the rest just shifts where h/w contexts and all their
    artifacts are allocated and freed.
    
    The number of max h/w contexts is capped to the number of possible cpus
    since there is no use for more than that. As such, all pre-allocated
    memory for pointers need to account for the max possible rather than
    the initial number of queues.
    
    A side effect of this is that the blk-mq will proceed successfully as
    long as it can allocate at least one h/w context. Previously it would
    fail request queue initialization if less than the requested number
    was allocated.
    
    Signed-off-by: Keith Busch <keith.busch@intel.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Tested-by: Jon Derrick <jonathan.derrick@intel.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index eaede8e45c9c..9087b11037b7 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -57,6 +57,7 @@ extern int blk_mq_hw_queue_to_node(unsigned int *map, unsigned int);
  */
 extern int blk_mq_sysfs_register(struct request_queue *q);
 extern void blk_mq_sysfs_unregister(struct request_queue *q);
+extern void blk_mq_hctx_kobj_init(struct blk_mq_hw_ctx *hctx);
 
 extern void blk_mq_rq_timed_out(struct request *req, bool reserved);
 

commit 6f3b0e8bcf3cbb87a7459b3ed018d31d918df3f8
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Nov 26 09:13:05 2015 +0100

    blk-mq: add a flags parameter to blk_mq_alloc_request
    
    We already have the reserved flag, and a nowait flag awkwardly encoded as
    a gfp_t.  Add a real flags argument to make the scheme more extensible and
    allow for a nicer calling convention.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 713820b47b31..eaede8e45c9c 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -96,8 +96,7 @@ static inline void blk_mq_put_ctx(struct blk_mq_ctx *ctx)
 struct blk_mq_alloc_data {
 	/* input parameter */
 	struct request_queue *q;
-	gfp_t gfp;
-	bool reserved;
+	unsigned int flags;
 
 	/* input & output parameter */
 	struct blk_mq_ctx *ctx;
@@ -105,13 +104,11 @@ struct blk_mq_alloc_data {
 };
 
 static inline void blk_mq_set_alloc_data(struct blk_mq_alloc_data *data,
-		struct request_queue *q, gfp_t gfp, bool reserved,
-		struct blk_mq_ctx *ctx,
-		struct blk_mq_hw_ctx *hctx)
+		struct request_queue *q, unsigned int flags,
+		struct blk_mq_ctx *ctx, struct blk_mq_hw_ctx *hctx)
 {
 	data->q = q;
-	data->gfp = gfp;
-	data->reserved = reserved;
+	data->flags = flags;
 	data->ctx = ctx;
 	data->hctx = hctx;
 }

commit 1fa8cc52f46c14fb1afc20c220855c40a5d28fcd
Author: Jens Axboe <axboe@fb.com>
Date:   Thu Nov 5 14:32:55 2015 -0700

    blk-mq: mark __blk_mq_complete_request() static
    
    It's no longer used outside of blk-mq core.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index b44dce165761..713820b47b31 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -25,7 +25,6 @@ struct blk_mq_ctx {
 	struct kobject		kobj;
 } ____cacheline_aligned_in_smp;
 
-void __blk_mq_complete_request(struct request *rq);
 void blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async);
 void blk_mq_freeze_queue(struct request_queue *q);
 void blk_mq_free_queue(struct request_queue *q);

commit 3380f4589f6d9725e275525fd0580c8ee2b5cbbc
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Oct 9 18:57:06 2015 +0200

    blk-mq: remove unused blk_mq_clone_flush_request prototype
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index f4fea7964910..b44dce165761 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -29,8 +29,6 @@ void __blk_mq_complete_request(struct request *rq);
 void blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async);
 void blk_mq_freeze_queue(struct request_queue *q);
 void blk_mq_free_queue(struct request_queue *q);
-void blk_mq_clone_flush_request(struct request *flush_rq,
-		struct request *orig_rq);
 int blk_mq_update_nr_requests(struct request_queue *q, unsigned int nr);
 void blk_mq_wake_waiters(struct request_queue *q);
 

commit 5778322e67ed34dc9f391a4a5cbcbb856071ceba
Author: Akinobu Mita <akinobu.mita@gmail.com>
Date:   Sun Sep 27 02:09:23 2015 +0900

    blk-mq: avoid inserting requests before establishing new mapping
    
    Notifier callbacks for CPU_ONLINE action can be run on the other CPU
    than the CPU which was just onlined.  So it is possible for the
    process running on the just onlined CPU to insert request and run
    hw queue before establishing new mapping which is done by
    blk_mq_queue_reinit_notify().
    
    This can cause a problem when the CPU has just been onlined first time
    since the request queue was initialized.  At this time ctx->index_hw
    for the CPU, which is the index in hctx->ctxs[] for this ctx, is still
    zero before blk_mq_queue_reinit_notify() is called by notifier
    callbacks for CPU_ONLINE action.
    
    For example, there is a single hw queue (hctx) and two CPU queues
    (ctx0 for CPU0, and ctx1 for CPU1).  Now CPU1 is just onlined and
    a request is inserted into ctx1->rq_list and set bit0 in pending
    bitmap as ctx1->index_hw is still zero.
    
    And then while running hw queue, flush_busy_ctxs() finds bit0 is set
    in pending bitmap and tries to retrieve requests in
    hctx->ctxs[0]->rq_list.  But htx->ctxs[0] is a pointer to ctx0, so the
    request in ctx1->rq_list is ignored.
    
    Fix it by ensuring that new mapping is established before onlined cpu
    starts running.
    
    Signed-off-by: Akinobu Mita <akinobu.mita@gmail.com>
    Reviewed-by: Ming Lei <tom.leiming@gmail.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Ming Lei <tom.leiming@gmail.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 6a48c4c0d8a2..f4fea7964910 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -51,7 +51,8 @@ void blk_mq_disable_hotplug(void);
  * CPU -> queue mappings
  */
 extern unsigned int *blk_mq_make_queue_map(struct blk_mq_tag_set *set);
-extern int blk_mq_update_queue_map(unsigned int *map, unsigned int nr_queues);
+extern int blk_mq_update_queue_map(unsigned int *map, unsigned int nr_queues,
+				   const struct cpumask *online_mask);
 extern int blk_mq_hw_queue_to_node(unsigned int *map, unsigned int);
 
 /*

commit e09aae7edec1d20824c60a6f0ca4589f99ada17b
Author: Ming Lei <ming.lei@canonical.com>
Date:   Thu Jan 29 20:17:27 2015 +0800

    blk-mq: release mq's kobjects in blk_release_queue()
    
    The kobject memory inside blk-mq hctx/ctx shouldn't have been freed
    before the kobject is released because driver core can access it freely
    before its release.
    
    We can't do that in all ctx/hctx/mq_kobj's release handler because
    it can be run before blk_cleanup_queue().
    
    Given mq_kobj shouldn't have been introduced, this patch simply moves
    mq's release into blk_release_queue().
    
    Reported-by: Sasha Levin <sasha.levin@oracle.com>
    Signed-off-by: Ming Lei <ming.lei@canonical.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 4f4f943c22c3..6a48c4c0d8a2 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -62,6 +62,8 @@ extern void blk_mq_sysfs_unregister(struct request_queue *q);
 
 extern void blk_mq_rq_timed_out(struct request *req, bool reserved);
 
+void blk_mq_release(struct request_queue *q);
+
 /*
  * Basic implementation of sparser bitmap, allowing the user to spread
  * the bits over more cachelines.

commit aed3ea94bdd2ac0a21ed0103d34097e202ee77f6
Author: Jens Axboe <axboe@fb.com>
Date:   Mon Dec 22 14:04:42 2014 -0700

    block: wake up waiters when a queue is marked dying
    
    If it's dying, we can't expect new request to complete and come
    in an wake up other tasks waiting for requests. So after we
    have marked it as dying, wake up everybody currently waiting
    for a request. Once they wake, they will retry their allocation
    and fail appropriately due to the state of the queue.
    
    Tested-by: Keith Busch <keith.busch@intel.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 206230e64f79..4f4f943c22c3 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -32,6 +32,7 @@ void blk_mq_free_queue(struct request_queue *q);
 void blk_mq_clone_flush_request(struct request *flush_rq,
 		struct request *orig_rq);
 int blk_mq_update_nr_requests(struct request_queue *q, unsigned int nr);
+void blk_mq_wake_waiters(struct request_queue *q);
 
 /*
  * CPU hotplug helpers

commit 19c66e59ce57e7b181625cbb408d48eb10837763
Author: Ming Lei <ming.lei@canonical.com>
Date:   Wed Dec 3 19:38:04 2014 +0800

    blk-mq: prevent unmapped hw queue from being scheduled
    
    When one hardware queue has no mapped software queues, it
    shouldn't have been scheduled. Otherwise WARNING or OOPS
    can triggered.
    
    blk_mq_hw_queue_mapped() helper is introduce for fixing
    the problem.
    
    Signed-off-by: Ming Lei <ming.lei@canonical.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index d567d5283ffa..206230e64f79 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -115,4 +115,9 @@ static inline void blk_mq_set_alloc_data(struct blk_mq_alloc_data *data,
 	data->hctx = hctx;
 }
 
+static inline bool blk_mq_hw_queue_mapped(struct blk_mq_hw_ctx *hctx)
+{
+	return hctx->nr_ctx && hctx->tags;
+}
+
 #endif

commit f355265571440a7db16e784b6edf4e7d26971a03
Author: Ming Lei <ming.lei@canonical.com>
Date:   Thu Sep 25 23:23:40 2014 +0800

    block: introduce blk_init_flush and its pair
    
    These two temporary functions are introduced for holding flush
    initialization and de-initialization, so that we can
    introduce 'flush queue' easier in the following patch. And
    once 'flush queue' and its allocation/free functions are ready,
    they will be removed for sake of code readability.
    
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Ming Lei <ming.lei@canonical.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index ecac69c08937..d567d5283ffa 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -27,7 +27,6 @@ struct blk_mq_ctx {
 
 void __blk_mq_complete_request(struct request *rq);
 void blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async);
-int blk_mq_init_flush(struct request_queue *q);
 void blk_mq_freeze_queue(struct request_queue *q);
 void blk_mq_free_queue(struct request_queue *q);
 void blk_mq_clone_flush_request(struct request *flush_rq,

commit 1bcb1eada4f11a713cbe586d1b5a5d93a48277cb
Author: Ming Lei <ming.lei@canonical.com>
Date:   Thu Sep 25 23:23:39 2014 +0800

    blk-mq: allocate flush_rq in blk_mq_init_flush()
    
    It is reasonable to allocate flush req in blk_mq_init_flush().
    
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Ming Lei <ming.lei@canonical.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index a3c613a9df2d..ecac69c08937 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -27,7 +27,7 @@ struct blk_mq_ctx {
 
 void __blk_mq_complete_request(struct request *rq);
 void blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async);
-void blk_mq_init_flush(struct request_queue *q);
+int blk_mq_init_flush(struct request_queue *q);
 void blk_mq_freeze_queue(struct request_queue *q);
 void blk_mq_free_queue(struct request_queue *q);
 void blk_mq_clone_flush_request(struct request *flush_rq,

commit 90415837659fec54f33584b423dab250eb1e8432
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Sep 22 10:21:48 2014 -0600

    block: fix blk_abort_request on blk-mq
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    
    Moved blk_mq_rq_timed_out() definition to the private blk-mq.h header.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index ca4964a6295d..a3c613a9df2d 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -60,6 +60,8 @@ extern int blk_mq_hw_queue_to_node(unsigned int *map, unsigned int);
 extern int blk_mq_sysfs_register(struct request_queue *q);
 extern void blk_mq_sysfs_unregister(struct request_queue *q);
 
+extern void blk_mq_rq_timed_out(struct request *req, bool reserved);
+
 /*
  * Basic implementation of sparser bitmap, allowing the user to spread
  * the bits over more cachelines.

commit 780db2071ac4d167ee4154ad9c96088f1bba044b
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jul 1 10:31:13 2014 -0600

    blk-mq: decouble blk-mq freezing from generic bypassing
    
    blk_mq freezing is entangled with generic bypassing which bypasses
    blkcg and io scheduler and lets IO requests fall through the block
    layer to the drivers in FIFO order.  This allows forward progress on
    IOs with the advanced features disabled so that those features can be
    configured or altered without worrying about stalling IO which may
    lead to deadlock through memory allocation.
    
    However, generic bypassing doesn't quite fit blk-mq.  blk-mq currently
    doesn't make use of blkcg or ioscheds and it maps bypssing to
    freezing, which blocks request processing and drains all the in-flight
    ones.  This causes problems as bypassing assumes that request
    processing is online.  blk-mq works around this by conditionally
    allowing request processing for the problem case - during queue
    initialization.
    
    Another weirdity is that except for during queue cleanup, bypassing
    started on the generic side prevents blk-mq from processing new
    requests but doesn't drain the in-flight ones.  This shouldn't break
    anything but again highlights that something isn't quite right here.
    
    The root cause is conflating blk-mq freezing and generic bypassing
    which are two different mechanisms.  The only intersecting purpose
    that they serve is during queue cleanup.  Let's properly separate
    blk-mq freezing from generic bypassing and simply use it where
    necessary.
    
    * request_queue->mq_freeze_depth is added and
      blk_mq_[un]freeze_queue() now operate on this counter instead of
      ->bypass_depth.  The replacement for QUEUE_FLAG_BYPASS isn't added
      but the counter is tested directly.  This will be further updated by
      later changes.
    
    * blk_mq_drain_queue() is dropped and "__" prefix is dropped from
      blk_mq_freeze_queue().  Queue cleanup path now calls
      blk_mq_freeze_queue() directly.
    
    * blk_queue_enter()'s fast path condition is simplified to simply
      check @q->mq_freeze_depth.  Previously, the condition was
    
            !blk_queue_dying(q) &&
                (!blk_queue_bypass(q) || !blk_queue_init_done(q))
    
      mq_freeze_depth is incremented right after dying is set and
      blk_queue_init_done() exception isn't necessary as blk-mq doesn't
      start frozen, which only leaves the blk_queue_bypass() test which
      can be replaced by @q->mq_freeze_depth test.
    
    This change simplifies the code and reduces confusion in the area.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Nicholas A. Bellinger <nab@linux-iscsi.org>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 26460884c6cd..ca4964a6295d 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -28,7 +28,7 @@ struct blk_mq_ctx {
 void __blk_mq_complete_request(struct request *rq);
 void blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async);
 void blk_mq_init_flush(struct request_queue *q);
-void blk_mq_drain_queue(struct request_queue *q);
+void blk_mq_freeze_queue(struct request_queue *q);
 void blk_mq_free_queue(struct request_queue *q);
 void blk_mq_clone_flush_request(struct request *flush_rq,
 		struct request *orig_rq);

commit cb96a42cc1f50ba1c7b1e9b2343bec80b926107f
Author: Ming Lei <tom.leiming@gmail.com>
Date:   Sun Jun 1 00:43:37 2014 +0800

    blk-mq: fix schedule from atomic context
    
    blk_mq_put_ctx() has to be called before io_schedule() in
    bt_get().
    
    This patch fixes the problem by taking similar approach from
    percpu_ida allocation for the situation.
    
    Signed-off-by: Ming Lei <tom.leiming@gmail.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 57a7968e47b3..26460884c6cd 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -91,4 +91,27 @@ static inline void blk_mq_put_ctx(struct blk_mq_ctx *ctx)
 	put_cpu();
 }
 
+struct blk_mq_alloc_data {
+	/* input parameter */
+	struct request_queue *q;
+	gfp_t gfp;
+	bool reserved;
+
+	/* input & output parameter */
+	struct blk_mq_ctx *ctx;
+	struct blk_mq_hw_ctx *hctx;
+};
+
+static inline void blk_mq_set_alloc_data(struct blk_mq_alloc_data *data,
+		struct request_queue *q, gfp_t gfp, bool reserved,
+		struct blk_mq_ctx *ctx,
+		struct blk_mq_hw_ctx *hctx)
+{
+	data->q = q;
+	data->gfp = gfp;
+	data->reserved = reserved;
+	data->ctx = ctx;
+	data->hctx = hctx;
+}
+
 #endif

commit 1aecfe4887713838c79bc52f774609a57db4f988
Author: Ming Lei <tom.leiming@gmail.com>
Date:   Sun Jun 1 00:43:36 2014 +0800

    blk-mq: move blk_mq_get_ctx/blk_mq_put_ctx to mq private header
    
    The blk-mq tag code need these helpers.
    
    Signed-off-by: Ming Lei <tom.leiming@gmail.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index de7b3bbd5bd6..57a7968e47b3 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -69,4 +69,26 @@ struct blk_align_bitmap {
 	unsigned long depth;
 } ____cacheline_aligned_in_smp;
 
+static inline struct blk_mq_ctx *__blk_mq_get_ctx(struct request_queue *q,
+					   unsigned int cpu)
+{
+	return per_cpu_ptr(q->queue_ctx, cpu);
+}
+
+/*
+ * This assumes per-cpu software queueing queues. They could be per-node
+ * as well, for instance. For now this is hardcoded as-is. Note that we don't
+ * care about preemption, since we know the ctx's are persistent. This does
+ * mean that we can't rely on ctx always matching the currently running CPU.
+ */
+static inline struct blk_mq_ctx *blk_mq_get_ctx(struct request_queue *q)
+{
+	return __blk_mq_get_ctx(q, get_cpu());
+}
+
+static inline void blk_mq_put_ctx(struct blk_mq_ctx *ctx)
+{
+	put_cpu();
+}
+
 #endif

commit 67aec14ce87fe25bdfff7dbf468556333df11c4e
Author: Jens Axboe <axboe@fb.com>
Date:   Fri May 30 08:25:36 2014 -0600

    blk-mq: make the sysfs mq/ layout reflect current mappings
    
    Currently blk-mq registers all the hardware queues in sysfs,
    regardless of whether it uses them (e.g. they have CPU mappings)
    or not. The unused hardware queues lack the cpux/ directories,
    and the other sysfs entries (like active, pending, etc) are all
    zeroes.
    
    Change this so that sysfs correctly reflects the current mappings
    of the hardware queues.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index ff5e6bf0f691..de7b3bbd5bd6 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -54,6 +54,12 @@ extern unsigned int *blk_mq_make_queue_map(struct blk_mq_tag_set *set);
 extern int blk_mq_update_queue_map(unsigned int *map, unsigned int nr_queues);
 extern int blk_mq_hw_queue_to_node(unsigned int *map, unsigned int);
 
+/*
+ * sysfs helpers
+ */
+extern int blk_mq_sysfs_register(struct request_queue *q);
+extern void blk_mq_sysfs_unregister(struct request_queue *q);
+
 /*
  * Basic implementation of sparser bitmap, allowing the user to spread
  * the bits over more cachelines.

commit f14bbe77a96bb979dc539d8308ee18a9363a544f
Author: Jens Axboe <axboe@fb.com>
Date:   Tue May 27 12:06:53 2014 -0600

    blk-mq: pass in suggested NUMA node to ->alloc_hctx()
    
    Drivers currently have to figure this out on their own, and they
    are missing information to do it properly. The ones that did
    attempt to do it, do it wrong.
    
    So just pass in the suggested node directly to the alloc
    function.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 491dbd4e93f5..ff5e6bf0f691 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -52,6 +52,7 @@ void blk_mq_disable_hotplug(void);
  */
 extern unsigned int *blk_mq_make_queue_map(struct blk_mq_tag_set *set);
 extern int blk_mq_update_queue_map(unsigned int *map, unsigned int nr_queues);
+extern int blk_mq_hw_queue_to_node(unsigned int *map, unsigned int);
 
 /*
  * Basic implementation of sparser bitmap, allowing the user to spread

commit e814e71ba4a6e1d7509b0f4b1928365ea650cace
Author: Jens Axboe <axboe@fb.com>
Date:   Wed May 21 13:59:08 2014 -0600

    blk-mq: allow the hctx cpu hotplug notifier to return errors
    
    Prepare this for the next patch which adds more smarts in the
    plugging logic, so that we can save some memory.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 7db4fe4bd002..491dbd4e93f5 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -39,7 +39,7 @@ int blk_mq_update_nr_requests(struct request_queue *q, unsigned int nr);
  */
 struct blk_mq_cpu_notifier;
 void blk_mq_init_cpu_notifier(struct blk_mq_cpu_notifier *notifier,
-			      void (*fn)(void *, unsigned long, unsigned int),
+			      int (*fn)(void *, unsigned long, unsigned int),
 			      void *data);
 void blk_mq_register_cpu_notifier(struct blk_mq_cpu_notifier *notifier);
 void blk_mq_unregister_cpu_notifier(struct blk_mq_cpu_notifier *notifier);

commit e3a2b3f931f59d5284abd13faf8bded726884ffd
Author: Jens Axboe <axboe@fb.com>
Date:   Tue May 20 11:49:02 2014 -0600

    blk-mq: allow changing of queue depth through sysfs
    
    For request_fn based devices, the block layer exports a 'nr_requests'
    file through sysfs to allow adjusting of queue depth on the fly.
    Currently this returns -EINVAL for blk-mq, since it's not wired up.
    Wire this up for blk-mq, so that it now also always dynamic
    adjustments of the allowed queue depth for any given block device
    managed by blk-mq.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 5e5a378962b7..7db4fe4bd002 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -32,6 +32,7 @@ void blk_mq_drain_queue(struct request_queue *q);
 void blk_mq_free_queue(struct request_queue *q);
 void blk_mq_clone_flush_request(struct request *flush_rq,
 		struct request *orig_rq);
+int blk_mq_update_nr_requests(struct request_queue *q, unsigned int nr);
 
 /*
  * CPU hotplug helpers

commit e93ecf602beb8439f0bdcc1fa2cbc1f31fdfb8e2
Author: Jens Axboe <axboe@fb.com>
Date:   Mon May 19 09:17:48 2014 -0600

    blk-mq: move the cache friendly bitmap type of out blk-mq-tag
    
    We will use it for the pending list in blk-mq core as well.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 97cfab9c092f..5e5a378962b7 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -52,4 +52,13 @@ void blk_mq_disable_hotplug(void);
 extern unsigned int *blk_mq_make_queue_map(struct blk_mq_tag_set *set);
 extern int blk_mq_update_queue_map(unsigned int *map, unsigned int nr_queues);
 
+/*
+ * Basic implementation of sparser bitmap, allowing the user to spread
+ * the bits over more cachelines.
+ */
+struct blk_align_bitmap {
+	unsigned long word;
+	unsigned long depth;
+} ____cacheline_aligned_in_smp;
+
 #endif

commit 4bb659b156996f2993dc16fad71fec9ee070153c
Author: Jens Axboe <axboe@fb.com>
Date:   Fri May 9 09:36:49 2014 -0600

    blk-mq: implement new and more efficient tagging scheme
    
    blk-mq currently uses percpu_ida for tag allocation. But that only
    works well if the ratio between tag space and number of CPUs is
    sufficiently high. For most devices and systems, that is not the
    case. The end result if that we either only utilize the tag space
    partially, or we end up attempting to fully exhaust it and run
    into lots of lock contention with stealing between CPUs. This is
    not optimal.
    
    This new tagging scheme is a hybrid bitmap allocator. It uses
    two tricks to both be SMP friendly and allow full exhaustion
    of the space:
    
    1) We cache the last allocated (or freed) tag on a per blk-mq
       software context basis. This allows us to limit the space
       we have to search. The key element here is not caching it
       in the shared tag structure, otherwise we end up dirtying
       more shared cache lines on each allocate/free operation.
    
    2) The tag space is split into cache line sized groups, and
       each context will start off randomly in that space. Even up
       to full utilization of the space, this divides the tag users
       efficiently into cache line groups, avoiding dirtying the same
       one both between allocators and between allocator and freeer.
    
    This scheme shows drastically better behaviour, both on small
    tag spaces but on large ones as well. It has been tested extensively
    to show better performance for all the cases blk-mq cares about.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 1ae364ceaf8b..97cfab9c092f 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -12,6 +12,8 @@ struct blk_mq_ctx {
 	unsigned int		cpu;
 	unsigned int		index_hw;
 
+	unsigned int		last_tag ____cacheline_aligned_in_smp;
+
 	/* incremented at dispatch time */
 	unsigned long		rq_dispatched[2];
 	unsigned long		rq_merged;
@@ -21,7 +23,7 @@ struct blk_mq_ctx {
 
 	struct request_queue	*queue;
 	struct kobject		kobj;
-};
+} ____cacheline_aligned_in_smp;
 
 void __blk_mq_complete_request(struct request *rq);
 void blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async);

commit 38535201633077cbaf8b32886b5e3005b36c9024
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Apr 25 02:32:53 2014 -0700

    blk-mq: respect rq_affinity
    
    The blk-mq code is using it's own version of the I/O completion affinity
    tunables, which causes a few issues:
    
     - the rq_affinity sysfs file doesn't work for blk-mq devices, even if it
       still is present, thus breaking existing tuning setups.
     - the rq_affinity = 1 mode, which is the defauly for legacy request based
       drivers isn't implemented at all.
     - blk-mq drivers don't implement any completion affinity with the default
       flag settings.
    
    This patches removes the blk-mq ipi_redirect flag and sysfs file, as well
    as the internal BLK_MQ_F_SHOULD_IPI flag and replaces it with code that
    respects the queue-wide rq_affinity flags and also implements the
    rq_affinity = 1 mode.
    
    This means I/O completion affinity can now only be tuned block-queue wide
    instead of per context, which seems more sensible to me anyway.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index b41a784de50d..1ae364ceaf8b 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -11,7 +11,6 @@ struct blk_mq_ctx {
 
 	unsigned int		cpu;
 	unsigned int		index_hw;
-	unsigned int		ipi_redirect;
 
 	/* incremented at dispatch time */
 	unsigned long		rq_dispatched[2];

commit 87ee7b112193bd081ba1a171fa5f6f39c429ef56
Author: Jens Axboe <axboe@fb.com>
Date:   Thu Apr 24 08:51:47 2014 -0600

    blk-mq: fix race with timeouts and requeue events
    
    If a requeue event races with a timeout, we can get into the
    situation where we attempt to complete a request from the
    timeout handler when it's not start anymore. This causes a crash.
    So have the timeout handler check that REQ_ATOM_STARTED is still
    set on the request - if not, we ignore the event. If this happens,
    the request has now been marked as complete. As a consequence, we
    need to ensure to clear REQ_ATOM_COMPLETE in blk_mq_start_request(),
    as to maintain proper request state.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 5fa14f19f752..b41a784de50d 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -51,6 +51,4 @@ void blk_mq_disable_hotplug(void);
 extern unsigned int *blk_mq_make_queue_map(struct blk_mq_tag_set *set);
 extern int blk_mq_update_queue_map(unsigned int *map, unsigned int nr_queues);
 
-void blk_mq_add_timer(struct request *rq);
-
 #endif

commit 24d2f90309b23f2cfe016b2aebc5f0d6e01c57fd
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Apr 15 14:14:00 2014 -0600

    blk-mq: split out tag initialization, support shared tags
    
    Add a new blk_mq_tag_set structure that gets set up before we initialize
    the queue.  A single blk_mq_tag_set structure can be shared by multiple
    queues.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    
    Modular export of blk_mq_{alloc,free}_tagset added by me.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 7964dadb7d64..5fa14f19f752 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -1,6 +1,8 @@
 #ifndef INT_BLK_MQ_H
 #define INT_BLK_MQ_H
 
+struct blk_mq_tag_set;
+
 struct blk_mq_ctx {
 	struct {
 		spinlock_t		lock;
@@ -46,8 +48,7 @@ void blk_mq_disable_hotplug(void);
 /*
  * CPU -> queue mappings
  */
-struct blk_mq_reg;
-extern unsigned int *blk_mq_make_queue_map(struct blk_mq_reg *reg);
+extern unsigned int *blk_mq_make_queue_map(struct blk_mq_tag_set *set);
 extern int blk_mq_update_queue_map(unsigned int *map, unsigned int nr_queues);
 
 void blk_mq_add_timer(struct request *rq);

commit 8727af4b9d45c7503042e3fbd926c1a173876e9c
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Apr 14 10:30:08 2014 +0200

    blk-mq: make ->flush_rq fully transparent to drivers
    
    Drivers shouldn't have to care about the block layer setting aside a
    request to implement the flush state machine.  We already override the
    mq context and tag to make it more transparent, but so far haven't deal
    with the driver private data in the request.  Make sure to override this
    as well, and while we're at it add a proper helper sitting in blk-mq.c
    that implements the full impersonation.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 238379a612e4..7964dadb7d64 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -27,6 +27,8 @@ void blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async);
 void blk_mq_init_flush(struct request_queue *q);
 void blk_mq_drain_queue(struct request_queue *q);
 void blk_mq_free_queue(struct request_queue *q);
+void blk_mq_clone_flush_request(struct request *flush_rq,
+		struct request *orig_rq);
 
 /*
  * CPU hotplug helpers

commit 9d74e25737d73e93ccddeb5a61bcd56b7b8eb57b
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Apr 14 10:30:07 2014 +0200

    blk-mq: do not initialize req->special
    
    Drivers can reach their private data easily using the blk_mq_rq_to_pdu
    helper and don't need req->special.  By not initializing it code can
    be simplified nicely, and we also shave off a few more instructions from
    the I/O path.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index ebbe6bac9d61..238379a612e4 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -27,7 +27,6 @@ void blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async);
 void blk_mq_init_flush(struct request_queue *q);
 void blk_mq_drain_queue(struct request_queue *q);
 void blk_mq_free_queue(struct request_queue *q);
-void blk_mq_rq_init(struct blk_mq_hw_ctx *hctx, struct request *rq);
 
 /*
  * CPU hotplug helpers

commit eeabc850b79336575da7be3dbe186a2da4de8293
Author: Christoph Hellwig <hch@infradead.org>
Date:   Fri Mar 21 08:57:37 2014 -0600

    blk-mq: merge blk_mq_insert_request and blk_mq_run_request
    
    It's almost identical to blk_mq_insert_request, so fold the two into one
    slightly more generic function by making the flush special case a bit
    smarted.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 361f9343dab1..ebbe6bac9d61 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -23,7 +23,6 @@ struct blk_mq_ctx {
 };
 
 void __blk_mq_complete_request(struct request *rq);
-void blk_mq_run_request(struct request *rq, bool run_queue, bool async);
 void blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async);
 void blk_mq_init_flush(struct request_queue *q);
 void blk_mq_drain_queue(struct request_queue *q);

commit 676141e48af7463717896352e69c10f945ac22dd
Author: Jens Axboe <axboe@fb.com>
Date:   Thu Mar 20 13:29:18 2014 -0600

    blk-mq: don't dump CPU -> hw queue map on driver load
    
    Now that we are out of initial debug/bringup mode, remove
    the verbose dump of the mapping table.
    
    Provide the mapping table in sysfs, under the hardware queue
    directory, in the cpu_list file.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index ed0035cd458e..361f9343dab1 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -40,6 +40,8 @@ void blk_mq_init_cpu_notifier(struct blk_mq_cpu_notifier *notifier,
 void blk_mq_register_cpu_notifier(struct blk_mq_cpu_notifier *notifier);
 void blk_mq_unregister_cpu_notifier(struct blk_mq_cpu_notifier *notifier);
 void blk_mq_cpu_init(void);
+void blk_mq_enable_hotplug(void);
+void blk_mq_disable_hotplug(void);
 
 /*
  * CPU -> queue mappings

commit 18741986a4b1dc4b1f171634c4191abc3b0fa023
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Feb 10 09:29:00 2014 -0700

    blk-mq: rework flush sequencing logic
    
    Witch to using a preallocated flush_rq for blk-mq similar to what's done
    with the old request path.  This allows us to set up the request properly
    with a tag from the actually allowed range and ->rq_disk as needed by
    some drivers.  To make life easier we also switch to dynamic allocation
    of ->flush_rq for the old path.
    
    This effectively reverts most of
    
        "blk-mq: fix for flush deadlock"
    
    and
    
        "blk-mq: Don't reserve a tag for flush request"
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index f29b645f0e1c..ed0035cd458e 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -28,6 +28,7 @@ void blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async);
 void blk_mq_init_flush(struct request_queue *q);
 void blk_mq_drain_queue(struct request_queue *q);
 void blk_mq_free_queue(struct request_queue *q);
+void blk_mq_rq_init(struct blk_mq_hw_ctx *hctx, struct request *rq);
 
 /*
  * CPU hotplug helpers

commit 30a91cb4ef385fe1b260df204ef314d86fff2850
Author: Christoph Hellwig <hch@infradead.org>
Date:   Mon Feb 10 03:24:38 2014 -0800

    blk-mq: rework I/O completions
    
    Rework I/O completions to work more like the old code path.  blk_mq_end_io
    now stays out of the business of deferring completions to others CPUs
    and calling blk_mark_rq_complete.  The latter is very important to allow
    completing requests that have timed out and thus are already marked completed,
    the former allows using the IPI callout even for driver specific completions
    instead of having to reimplement them.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 5c3917984b00..f29b645f0e1c 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -22,8 +22,7 @@ struct blk_mq_ctx {
 	struct kobject		kobj;
 };
 
-void __blk_mq_end_io(struct request *rq, int error);
-void blk_mq_complete_request(struct request *rq, int error);
+void __blk_mq_complete_request(struct request *rq);
 void blk_mq_run_request(struct request *rq, bool run_queue, bool async);
 void blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async);
 void blk_mq_init_flush(struct request_queue *q);

commit 3d6efbf62c797a2924785f482e4ce8aa8820ec72
Author: Christoph Hellwig <hch@infradead.org>
Date:   Wed Jan 8 09:33:37 2014 -0800

    blk-mq: use __smp_call_function_single directly
    
    __smp_call_function_single already avoids multiple IPIs by internally
    queing up the items, and now also is available for non-SMP builds as
    a trivially correct stub, so there is no need to wrap it.  If the
    additional lock roundtrip cause problems my patch to convert the
    generic IPI code to llists is waiting to get merged will fix it.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index e151a2f4f171..5c3917984b00 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -40,7 +40,6 @@ void blk_mq_init_cpu_notifier(struct blk_mq_cpu_notifier *notifier,
 void blk_mq_register_cpu_notifier(struct blk_mq_cpu_notifier *notifier);
 void blk_mq_unregister_cpu_notifier(struct blk_mq_cpu_notifier *notifier);
 void blk_mq_cpu_init(void);
-DECLARE_PER_CPU(struct llist_head, ipi_lists);
 
 /*
  * CPU -> queue mappings

commit 3edcc0ce85c59d45d6dfc6a36a6b3f8b31ba9887
Author: Ming Lei <tom.leiming@gmail.com>
Date:   Thu Dec 26 21:31:38 2013 +0800

    block: blk-mq: don't export blk_mq_free_queue()
    
    blk_mq_free_queue() is called from release handler of
    queue kobject, so it needn't be called from drivers.
    
    Cc: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Ming Lei <tom.leiming@gmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index caa614f24409..e151a2f4f171 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -28,6 +28,7 @@ void blk_mq_run_request(struct request *rq, bool run_queue, bool async);
 void blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async);
 void blk_mq_init_flush(struct request_queue *q);
 void blk_mq_drain_queue(struct request_queue *q);
+void blk_mq_free_queue(struct request_queue *q);
 
 /*
  * CPU hotplug helpers

commit 43a5e4e21964a6efb4d14a34644ec7109d0ae891
Author: Ming Lei <tom.leiming@gmail.com>
Date:   Thu Dec 26 21:31:35 2013 +0800

    block: blk-mq: support draining mq queue
    
    blk_mq_drain_queue() is introduced so that we can drain
    mq queue inside blk_cleanup_queue().
    
    Also don't accept new requests any more if queue is marked
    as dying.
    
    Cc: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Ming Lei <tom.leiming@gmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
index 52bf1f96a2c2..caa614f24409 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -27,6 +27,7 @@ void blk_mq_complete_request(struct request *rq, int error);
 void blk_mq_run_request(struct request *rq, bool run_queue, bool async);
 void blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async);
 void blk_mq_init_flush(struct request_queue *q);
+void blk_mq_drain_queue(struct request_queue *q);
 
 /*
  * CPU hotplug helpers

commit 320ae51feed5c2f13664aa05a76bec198967e04d
Author: Jens Axboe <axboe@kernel.dk>
Date:   Thu Oct 24 09:20:05 2013 +0100

    blk-mq: new multi-queue block IO queueing mechanism
    
    Linux currently has two models for block devices:
    
    - The classic request_fn based approach, where drivers use struct
      request units for IO. The block layer provides various helper
      functionalities to let drivers share code, things like tag
      management, timeout handling, queueing, etc.
    
    - The "stacked" approach, where a driver squeezes in between the
      block layer and IO submitter. Since this bypasses the IO stack,
      driver generally have to manage everything themselves.
    
    With drivers being written for new high IOPS devices, the classic
    request_fn based driver doesn't work well enough. The design dates
    back to when both SMP and high IOPS was rare. It has problems with
    scaling to bigger machines, and runs into scaling issues even on
    smaller machines when you have IOPS in the hundreds of thousands
    per device.
    
    The stacked approach is then most often selected as the model
    for the driver. But this means that everybody has to re-invent
    everything, and along with that we get all the problems again
    that the shared approach solved.
    
    This commit introduces blk-mq, block multi queue support. The
    design is centered around per-cpu queues for queueing IO, which
    then funnel down into x number of hardware submission queues.
    We might have a 1:1 mapping between the two, or it might be
    an N:M mapping. That all depends on what the hardware supports.
    
    blk-mq provides various helper functions, which include:
    
    - Scalable support for request tagging. Most devices need to
      be able to uniquely identify a request both in the driver and
      to the hardware. The tagging uses per-cpu caches for freed
      tags, to enable cache hot reuse.
    
    - Timeout handling without tracking request on a per-device
      basis. Basically the driver should be able to get a notification,
      if a request happens to fail.
    
    - Optional support for non 1:1 mappings between issue and
      submission queues. blk-mq can redirect IO completions to the
      desired location.
    
    - Support for per-request payloads. Drivers almost always need
      to associate a request structure with some driver private
      command structure. Drivers can tell blk-mq this at init time,
      and then any request handed to the driver will have the
      required size of memory associated with it.
    
    - Support for merging of IO, and plugging. The stacked model
      gets neither of these. Even for high IOPS devices, merging
      sequential IO reduces per-command overhead and thus
      increases bandwidth.
    
    For now, this is provided as a potential 3rd queueing model, with
    the hope being that, as it matures, it can replace both the classic
    and stacked model. That would get us back to having just 1 real
    model for block devices, leaving the stacked approach to dm/md
    devices (as it was originally intended).
    
    Contributions in this patch from the following people:
    
    Shaohua Li <shli@fusionio.com>
    Alexander Gordeev <agordeev@redhat.com>
    Christoph Hellwig <hch@infradead.org>
    Mike Christie <michaelc@cs.wisc.edu>
    Matias Bjorling <m@bjorling.me>
    Jeff Moyer <jmoyer@redhat.com>
    
    Acked-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq.h b/block/blk-mq.h
new file mode 100644
index 000000000000..52bf1f96a2c2
--- /dev/null
+++ b/block/blk-mq.h
@@ -0,0 +1,52 @@
+#ifndef INT_BLK_MQ_H
+#define INT_BLK_MQ_H
+
+struct blk_mq_ctx {
+	struct {
+		spinlock_t		lock;
+		struct list_head	rq_list;
+	}  ____cacheline_aligned_in_smp;
+
+	unsigned int		cpu;
+	unsigned int		index_hw;
+	unsigned int		ipi_redirect;
+
+	/* incremented at dispatch time */
+	unsigned long		rq_dispatched[2];
+	unsigned long		rq_merged;
+
+	/* incremented at completion time */
+	unsigned long		____cacheline_aligned_in_smp rq_completed[2];
+
+	struct request_queue	*queue;
+	struct kobject		kobj;
+};
+
+void __blk_mq_end_io(struct request *rq, int error);
+void blk_mq_complete_request(struct request *rq, int error);
+void blk_mq_run_request(struct request *rq, bool run_queue, bool async);
+void blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async);
+void blk_mq_init_flush(struct request_queue *q);
+
+/*
+ * CPU hotplug helpers
+ */
+struct blk_mq_cpu_notifier;
+void blk_mq_init_cpu_notifier(struct blk_mq_cpu_notifier *notifier,
+			      void (*fn)(void *, unsigned long, unsigned int),
+			      void *data);
+void blk_mq_register_cpu_notifier(struct blk_mq_cpu_notifier *notifier);
+void blk_mq_unregister_cpu_notifier(struct blk_mq_cpu_notifier *notifier);
+void blk_mq_cpu_init(void);
+DECLARE_PER_CPU(struct llist_head, ipi_lists);
+
+/*
+ * CPU -> queue mappings
+ */
+struct blk_mq_reg;
+extern unsigned int *blk_mq_make_queue_map(struct blk_mq_reg *reg);
+extern int blk_mq_update_queue_map(unsigned int *map, unsigned int nr_queues);
+
+void blk_mq_add_timer(struct request *rq);
+
+#endif
