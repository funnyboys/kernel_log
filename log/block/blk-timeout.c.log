commit 3dcf60bcb603f56361abb364a4cd2f69677453f0
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Apr 30 14:42:43 2019 -0400

    block: add SPDX tags to block layer files missing licensing information
    
    Various block layer files do not have any licensing information at all.
    Add SPDX tags for the default kernel GPLv2 license to those.
    
    Reviewed-by: Chaitanya Kulkarni <chaitanya.kulkarni@wdc.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index 124c26128bf6..8aa68fae96ad 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * Functions related to generic timeout handling of requests.
  */

commit 39795d6534c6e698c4f9c065e0a5f4a2e5af7543
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Nov 14 17:02:06 2018 +0100

    block: don't hold the queue_lock over blk_abort_request
    
    There is nothing it could synchronize against, so don't go through
    the pains of acquiring the lock.
    
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index 3b0179fbdd6a..124c26128bf6 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -75,7 +75,7 @@ ssize_t part_timeout_store(struct device *dev, struct device_attribute *attr,
  * This function requests that the block layer start recovery for the
  * request by deleting the timer and calling the q's timeout function.
  * LLDDs who implement their own error recovery MAY ignore the timeout
- * event if they generated blk_abort_req. Must hold queue lock.
+ * event if they generated blk_abort_request.
  */
 void blk_abort_request(struct request *req)
 {

commit 079076b3416e78ba2bb3ce38e05e320c388c3120
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Nov 14 17:02:05 2018 +0100

    block: remove deadline __deadline manipulation helpers
    
    No users left since the removal of the legacy request interface, we can
    remove all the magic bit stealing now and make it a normal field.
    
    But use WRITE_ONCE/READ_ONCE on the new deadline field, given that we
    don't seem to have any mechanism to guarantee a new value actually
    gets seen by other threads.
    
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index 006cff4390c0..3b0179fbdd6a 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -84,7 +84,7 @@ void blk_abort_request(struct request *req)
 	 * immediately and that scan sees the new timeout value.
 	 * No need for fancy synchronizations.
 	 */
-	blk_rq_set_deadline(req, jiffies);
+	WRITE_ONCE(req->deadline, jiffies);
 	kblockd_schedule_work(&req->q->timeout_work);
 }
 EXPORT_SYMBOL_GPL(blk_abort_request);
@@ -121,14 +121,16 @@ void blk_add_timer(struct request *req)
 		req->timeout = q->rq_timeout;
 
 	req->rq_flags &= ~RQF_TIMED_OUT;
-	blk_rq_set_deadline(req, jiffies + req->timeout);
+
+	expiry = jiffies + req->timeout;
+	WRITE_ONCE(req->deadline, expiry);
 
 	/*
 	 * If the timer isn't already pending or this timeout is earlier
 	 * than an existing one, modify the timer. Round up to next nearest
 	 * second.
 	 */
-	expiry = blk_rq_timeout(round_jiffies_up(blk_rq_deadline(req)));
+	expiry = blk_rq_timeout(round_jiffies_up(expiry));
 
 	if (!timer_pending(&q->timeout) ||
 	    time_before(expiry, q->timeout.expires)) {

commit 9d037ad707ed6069fbea4e38e6ee37e027b13f1d
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Nov 9 19:37:44 2018 +0100

    block: remove req->timeout_list
    
    Unused now that the legacy request path is gone.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index 6428d458072a..006cff4390c0 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -68,16 +68,6 @@ ssize_t part_timeout_store(struct device *dev, struct device_attribute *attr,
 
 #endif /* CONFIG_FAIL_IO_TIMEOUT */
 
-/*
- * blk_delete_timer - Delete/cancel timer for a given function.
- * @req:	request that we are canceling timer for
- *
- */
-void blk_delete_timer(struct request *req)
-{
-	list_del_init(&req->timeout_list);
-}
-
 /**
  * blk_abort_request -- Request request recovery for the specified command
  * @req:	pointer to the request of interest
@@ -123,8 +113,6 @@ void blk_add_timer(struct request *req)
 	struct request_queue *q = req->q;
 	unsigned long expiry;
 
-	BUG_ON(!list_empty(&req->timeout_list));
-
 	/*
 	 * Some LLDs, like scsi, peek at the timeout to prevent a
 	 * command from being retried forever.

commit 4316b79e4321d4140164e42f228778e5bc66c84f
Author: Jens Axboe <axboe@kernel.dk>
Date:   Mon Oct 29 10:25:07 2018 -0600

    block: kill legacy parts of timeout handling
    
    The only user of legacy timing now is BSG, which is invoked
    from the mq timeout handler. Kill the legacy code, and rename
    the q->rq_timed_out_fn to q->bsg_job_timeout_fn.
    
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Tested-by: Ming Lei <ming.lei@redhat.com>
    Reviewed-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index f2cfd56e1606..6428d458072a 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -78,70 +78,6 @@ void blk_delete_timer(struct request *req)
 	list_del_init(&req->timeout_list);
 }
 
-static void blk_rq_timed_out(struct request *req)
-{
-	struct request_queue *q = req->q;
-	enum blk_eh_timer_return ret = BLK_EH_RESET_TIMER;
-
-	if (q->rq_timed_out_fn)
-		ret = q->rq_timed_out_fn(req);
-	switch (ret) {
-	case BLK_EH_RESET_TIMER:
-		blk_add_timer(req);
-		blk_clear_rq_complete(req);
-		break;
-	case BLK_EH_DONE:
-		/*
-		 * LLD handles this for now but in the future
-		 * we can send a request msg to abort the command
-		 * and we can move more of the generic scsi eh code to
-		 * the blk layer.
-		 */
-		break;
-	default:
-		printk(KERN_ERR "block: bad eh return: %d\n", ret);
-		break;
-	}
-}
-
-static void blk_rq_check_expired(struct request *rq, unsigned long *next_timeout,
-			  unsigned int *next_set)
-{
-	const unsigned long deadline = blk_rq_deadline(rq);
-
-	if (time_after_eq(jiffies, deadline)) {
-		list_del_init(&rq->timeout_list);
-
-		/*
-		 * Check if we raced with end io completion
-		 */
-		if (!blk_mark_rq_complete(rq))
-			blk_rq_timed_out(rq);
-	} else if (!*next_set || time_after(*next_timeout, deadline)) {
-		*next_timeout = deadline;
-		*next_set = 1;
-	}
-}
-
-void blk_timeout_work(struct work_struct *work)
-{
-	struct request_queue *q =
-		container_of(work, struct request_queue, timeout_work);
-	unsigned long flags, next = 0;
-	struct request *rq, *tmp;
-	int next_set = 0;
-
-	spin_lock_irqsave(q->queue_lock, flags);
-
-	list_for_each_entry_safe(rq, tmp, &q->timeout_list, timeout_list)
-		blk_rq_check_expired(rq, &next, &next_set);
-
-	if (next_set)
-		mod_timer(&q->timeout, round_jiffies_up(next));
-
-	spin_unlock_irqrestore(q->queue_lock, flags);
-}
-
 /**
  * blk_abort_request -- Request request recovery for the specified command
  * @req:	pointer to the request of interest
@@ -153,20 +89,13 @@ void blk_timeout_work(struct work_struct *work)
  */
 void blk_abort_request(struct request *req)
 {
-	if (req->q->mq_ops) {
-		/*
-		 * All we need to ensure is that timeout scan takes place
-		 * immediately and that scan sees the new timeout value.
-		 * No need for fancy synchronizations.
-		 */
-		blk_rq_set_deadline(req, jiffies);
-		kblockd_schedule_work(&req->q->timeout_work);
-	} else {
-		if (blk_mark_rq_complete(req))
-			return;
-		blk_delete_timer(req);
-		blk_rq_timed_out(req);
-	}
+	/*
+	 * All we need to ensure is that timeout scan takes place
+	 * immediately and that scan sees the new timeout value.
+	 * No need for fancy synchronizations.
+	 */
+	blk_rq_set_deadline(req, jiffies);
+	kblockd_schedule_work(&req->q->timeout_work);
 }
 EXPORT_SYMBOL_GPL(blk_abort_request);
 
@@ -194,13 +123,6 @@ void blk_add_timer(struct request *req)
 	struct request_queue *q = req->q;
 	unsigned long expiry;
 
-	if (!q->mq_ops)
-		lockdep_assert_held(q->queue_lock);
-
-	/* blk-mq has its own handler, so we don't need ->rq_timed_out_fn */
-	if (!q->mq_ops && !q->rq_timed_out_fn)
-		return;
-
 	BUG_ON(!list_empty(&req->timeout_list));
 
 	/*
@@ -213,13 +135,6 @@ void blk_add_timer(struct request *req)
 	req->rq_flags &= ~RQF_TIMED_OUT;
 	blk_rq_set_deadline(req, jiffies + req->timeout);
 
-	/*
-	 * Only the non-mq case needs to add the request to a protected list.
-	 * For the mq case we simply scan the tag map.
-	 */
-	if (!q->mq_ops)
-		list_add_tail(&req->timeout_list, &req->q->timeout_list);
-
 	/*
 	 * If the timer isn't already pending or this timeout is earlier
 	 * than an existing one, modify the timer. Round up to next nearest

commit f5e350f021e04ea41d2e5d58487c33b05ba3d25b
Author: Bart Van Assche <bart.vanassche@wdc.com>
Date:   Fri Jun 22 13:18:09 2018 -0700

    blk-mq: Fix timeout handling in case the timeout handler returns BLK_EH_DONE
    
    Make sure that RQF_TIMED_OUT is cleared when a request is reused
    after a block driver timeout handler has returned BLK_EH_DONE.
    
    Fixes: da6612673988 ("blk-mq: don't time out requests again that are in the timeout handler")
    Signed-off-by: Bart Van Assche <bart.vanassche@wdc.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Jianchao Wang <jianchao.w.wang@oracle.com>
    Cc: Andrew Randrianasulu <randrianasulu@gmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index 4b8a48d48ba1..f2cfd56e1606 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -210,6 +210,7 @@ void blk_add_timer(struct request *req)
 	if (!req->timeout)
 		req->timeout = q->rq_timeout;
 
+	req->rq_flags &= ~RQF_TIMED_OUT;
 	blk_rq_set_deadline(req, jiffies + req->timeout);
 
 	/*

commit f6e7d48a780a95a599d7e325f45e9ac208ac6f13
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue May 29 15:52:37 2018 +0200

    block: remove BLK_EH_HANDLED
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index 11879e98c249..4b8a48d48ba1 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -86,9 +86,6 @@ static void blk_rq_timed_out(struct request *req)
 	if (q->rq_timed_out_fn)
 		ret = q->rq_timed_out_fn(req);
 	switch (ret) {
-	case BLK_EH_HANDLED:
-		__blk_complete_request(req);
-		break;
 	case BLK_EH_RESET_TIMER:
 		blk_add_timer(req);
 		blk_clear_rq_complete(req);

commit 6600593cbd9340b3d4fcde8e58d17653732620c4
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue May 29 15:52:29 2018 +0200

    block: rename BLK_EH_NOT_HANDLED to BLK_EH_DONE
    
    The BLK_EH_NOT_HANDLED implies nothing happen, but very often that
    is not what is happening - instead the driver already completed the
    command.  Fix the symbolic name to reflect that a little better.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index f95d6e6cbc96..11879e98c249 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -93,7 +93,7 @@ static void blk_rq_timed_out(struct request *req)
 		blk_add_timer(req);
 		blk_clear_rq_complete(req);
 		break;
-	case BLK_EH_NOT_HANDLED:
+	case BLK_EH_DONE:
 		/*
 		 * LLD handles this for now but in the future
 		 * we can send a request msg to abort the command

commit 12f5b93145450c750f315657ef239a314811aeeb
Author: Keith Busch <keith.busch@intel.com>
Date:   Tue May 29 15:52:28 2018 +0200

    blk-mq: Remove generation seqeunce
    
    This patch simplifies the timeout handling by relying on the request
    reference counting to ensure the iterator is operating on an inflight
    and truly timed out request. Since the reference counting prevents the
    tag from being reallocated, the block layer no longer needs to prevent
    drivers from completing their requests while the timeout handler is
    operating on it: a driver completing a request is allowed to proceed to
    the next state without additional syncronization with the block layer.
    
    This also removes any need for generation sequence numbers since the
    request lifetime is prevented from being reallocated as a new sequence
    while timeout handling is operating on it.
    
    To enables this a refcount is added to struct request so that request
    users can be sure they're operating on the same request without it
    changing while they're processing it.  The request's tag won't be
    released for reuse until both the timeout handler and the completion
    are done with it.
    
    Signed-off-by: Keith Busch <keith.busch@intel.com>
    [hch: slight cleanups, added back submission side hctx lock, use cmpxchg
     for completions]
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index 652d4d4d3e97..f95d6e6cbc96 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -214,7 +214,6 @@ void blk_add_timer(struct request *req)
 		req->timeout = q->rq_timeout;
 
 	blk_rq_set_deadline(req, jiffies + req->timeout);
-	req->rq_flags &= ~RQF_MQ_TIMEOUT_EXPIRED;
 
 	/*
 	 * Only the non-mq case needs to add the request to a protected list.

commit bc6d65e6dc89c3b7ff78e4ad797117c122ffde8e
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Apr 2 15:04:58 2018 -0700

    blk-mq: Directly schedule q->timeout_work when aborting a request
    
    Request abortion is performed by overriding deadline to now and
    scheduling timeout handling immediately.  For the latter part, the
    code was using mod_timer(timeout, 0) which can't guarantee that the
    timer runs afterwards.  Let's schedule the underlying work item
    directly instead.
    
    This fixes the hangs during probing reported by Sitsofe but it isn't
    yet clear to me how the failure can happen reliably if it's just the
    above described race condition.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reported-by: Sitsofe Wheeler <sitsofe@gmail.com>
    Reported-by: Meelis Roos <mroos@linux.ee>
    Fixes: 358f70da49d7 ("blk-mq: make blk_abort_request() trigger timeout path")
    Cc: stable@vger.kernel.org # v4.16
    Link: http://lkml.kernel.org/r/CALjAwxh-PVYFnYFCJpGOja+m5SzZ8Sa4J7ohxdK=r8NyOF-EMA@mail.gmail.com
    Link: http://lkml.kernel.org/r/alpine.LRH.2.21.1802261049140.4893@math.ut.ee
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index 34a55250f08a..652d4d4d3e97 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -163,7 +163,7 @@ void blk_abort_request(struct request *req)
 		 * No need for fancy synchronizations.
 		 */
 		blk_rq_set_deadline(req, jiffies);
-		mod_timer(&req->q->timeout, 0);
+		kblockd_schedule_work(&req->q->timeout_work);
 	} else {
 		if (blk_mark_rq_complete(req))
 			return;

commit 8814ce8a0f680599a837af18aefdec774e5c7b97
Author: Bart Van Assche <bart.vanassche@wdc.com>
Date:   Wed Mar 7 17:10:04 2018 -0800

    block: Introduce blk_queue_flag_{set,clear,test_and_{set,clear}}()
    
    Introduce functions that modify the queue flags and that protect
    these modifications with the request queue lock. Except for moving
    one wake_up_all() call from inside to outside a critical section,
    this patch does not change any functionality.
    
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Hannes Reinecke <hare@suse.de>
    Cc: Ming Lei <ming.lei@redhat.com>
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Reviewed-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Bart Van Assche <bart.vanassche@wdc.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index a05e3676d24a..34a55250f08a 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -57,12 +57,10 @@ ssize_t part_timeout_store(struct device *dev, struct device_attribute *attr,
 		char *p = (char *) buf;
 
 		val = simple_strtoul(p, &p, 10);
-		spin_lock_irq(q->queue_lock);
 		if (val)
-			queue_flag_set(QUEUE_FLAG_FAIL_IO, q);
+			blk_queue_flag_set(QUEUE_FLAG_FAIL_IO, q);
 		else
-			queue_flag_clear(QUEUE_FLAG_FAIL_IO, q);
-		spin_unlock_irq(q->queue_lock);
+			blk_queue_flag_clear(QUEUE_FLAG_FAIL_IO, q);
 	}
 
 	return count;

commit 0a72e7f44964b9ada3e5c15820372e9cb119bf80
Author: Jens Axboe <axboe@kernel.dk>
Date:   Tue Jan 9 14:23:42 2018 -0700

    block: add accessors for setting/querying request deadline
    
    We reduce the resolution of request expiry, but since we're already
    using jiffies for this where resolution depends on the kernel
    configuration and since the timeout resolution is coarse anyway,
    that should be fine.
    
    Reviewed-by: Bart Van Assche <bart.vanassche@wdc.com>
    Reviewed-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index ebe99963386c..a05e3676d24a 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -112,7 +112,9 @@ static void blk_rq_timed_out(struct request *req)
 static void blk_rq_check_expired(struct request *rq, unsigned long *next_timeout,
 			  unsigned int *next_set)
 {
-	if (time_after_eq(jiffies, rq->deadline)) {
+	const unsigned long deadline = blk_rq_deadline(rq);
+
+	if (time_after_eq(jiffies, deadline)) {
 		list_del_init(&rq->timeout_list);
 
 		/*
@@ -120,8 +122,8 @@ static void blk_rq_check_expired(struct request *rq, unsigned long *next_timeout
 		 */
 		if (!blk_mark_rq_complete(rq))
 			blk_rq_timed_out(rq);
-	} else if (!*next_set || time_after(*next_timeout, rq->deadline)) {
-		*next_timeout = rq->deadline;
+	} else if (!*next_set || time_after(*next_timeout, deadline)) {
+		*next_timeout = deadline;
 		*next_set = 1;
 	}
 }
@@ -162,7 +164,7 @@ void blk_abort_request(struct request *req)
 		 * immediately and that scan sees the new timeout value.
 		 * No need for fancy synchronizations.
 		 */
-		req->deadline = jiffies;
+		blk_rq_set_deadline(req, jiffies);
 		mod_timer(&req->q->timeout, 0);
 	} else {
 		if (blk_mark_rq_complete(req))
@@ -213,7 +215,7 @@ void blk_add_timer(struct request *req)
 	if (!req->timeout)
 		req->timeout = q->rq_timeout;
 
-	req->deadline = jiffies + req->timeout;
+	blk_rq_set_deadline(req, jiffies + req->timeout);
 	req->rq_flags &= ~RQF_MQ_TIMEOUT_EXPIRED;
 
 	/*
@@ -228,7 +230,7 @@ void blk_add_timer(struct request *req)
 	 * than an existing one, modify the timer. Round up to next nearest
 	 * second.
 	 */
-	expiry = blk_rq_timeout(round_jiffies_up(req->deadline));
+	expiry = blk_rq_timeout(round_jiffies_up(blk_rq_deadline(req)));
 
 	if (!timer_pending(&q->timeout) ||
 	    time_before(expiry, q->timeout.expires)) {

commit 634f9e4631a88025d3b90c1884e9a1b6a13d01d2
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jan 9 08:29:51 2018 -0800

    blk-mq: remove REQ_ATOM_COMPLETE usages from blk-mq
    
    After the recent updates to use generation number and state based
    synchronization, blk-mq no longer depends on REQ_ATOM_COMPLETE except
    to avoid firing the same timeout multiple times.
    
    Remove all REQ_ATOM_COMPLETE usages and use a new rq_flags flag
    RQF_MQ_TIMEOUT_EXPIRED to avoid firing the same timeout multiple
    times.  This removes atomic bitops from hot paths too.
    
    v2: Removed blk_clear_rq_complete() from blk_mq_rq_timed_out().
    
    v3: Added RQF_MQ_TIMEOUT_EXPIRED flag.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: "jianchao.wang" <jianchao.w.wang@oracle.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index 4f04cd1e0b74..ebe99963386c 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -214,6 +214,7 @@ void blk_add_timer(struct request *req)
 		req->timeout = q->rq_timeout;
 
 	req->deadline = jiffies + req->timeout;
+	req->rq_flags &= ~RQF_MQ_TIMEOUT_EXPIRED;
 
 	/*
 	 * Only the non-mq case needs to add the request to a protected list.

commit 358f70da49d77c43f2ca11b5da584213b2add29c
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jan 9 08:29:50 2018 -0800

    blk-mq: make blk_abort_request() trigger timeout path
    
    With issue/complete and timeout paths now using the generation number
    and state based synchronization, blk_abort_request() is the only one
    which depends on REQ_ATOM_COMPLETE for arbitrating completion.
    
    There's no reason for blk_abort_request() to be a completely separate
    path.  This patch makes blk_abort_request() piggyback on the timeout
    path instead of trying to terminate the request directly.
    
    This removes the last dependency on REQ_ATOM_COMPLETE in blk-mq.
    
    Note that this makes blk_abort_request() asynchronous - it initiates
    abortion but the actual termination will happen after a short while,
    even when the caller owns the request.  AFAICS, SCSI and ATA should be
    fine with that and I think mtip32xx and dasd should be safe but not
    completely sure.  It'd be great if people who know the drivers take a
    look.
    
    v2: - Add comment explaining the lack of synchronization around
          ->deadline update as requested by Bart.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Asai Thambi SP <asamymuthupa@micron.com>
    Cc: Stefan Haberland <sth@linux.vnet.ibm.com>
    Cc: Jan Hoeppner <hoeppner@linux.vnet.ibm.com>
    Cc: Bart Van Assche <Bart.VanAssche@wdc.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index 6427be7ac363..4f04cd1e0b74 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -156,12 +156,17 @@ void blk_timeout_work(struct work_struct *work)
  */
 void blk_abort_request(struct request *req)
 {
-	if (blk_mark_rq_complete(req))
-		return;
-
 	if (req->q->mq_ops) {
-		blk_mq_rq_timed_out(req, false);
+		/*
+		 * All we need to ensure is that timeout scan takes place
+		 * immediately and that scan sees the new timeout value.
+		 * No need for fancy synchronizations.
+		 */
+		req->deadline = jiffies;
+		mod_timer(&req->q->timeout, 0);
 	} else {
+		if (blk_mark_rq_complete(req))
+			return;
 		blk_delete_timer(req);
 		blk_rq_timed_out(req);
 	}

commit 1d9bd5161ba32db5665a617edc8b0723880f543e
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jan 9 08:29:48 2018 -0800

    blk-mq: replace timeout synchronization with a RCU and generation based scheme
    
    Currently, blk-mq timeout path synchronizes against the usual
    issue/completion path using a complex scheme involving atomic
    bitflags, REQ_ATOM_*, memory barriers and subtle memory coherence
    rules.  Unfortunately, it contains quite a few holes.
    
    There's a complex dancing around REQ_ATOM_STARTED and
    REQ_ATOM_COMPLETE between issue/completion and timeout paths; however,
    they don't have a synchronization point across request recycle
    instances and it isn't clear what the barriers add.
    blk_mq_check_expired() can easily read STARTED from N-2'th iteration,
    deadline from N-1'th, blk_mark_rq_complete() against Nth instance.
    
    In fact, it's pretty easy to make blk_mq_check_expired() terminate a
    later instance of a request.  If we induce 5 sec delay before
    time_after_eq() test in blk_mq_check_expired(), shorten the timeout to
    2s, and issue back-to-back large IOs, blk-mq starts timing out
    requests spuriously pretty quickly.  Nothing actually timed out.  It
    just made the call on a recycle instance of a request and then
    terminated a later instance long after the original instance finished.
    The scenario isn't theoretical either.
    
    This patch replaces the broken synchronization mechanism with a RCU
    and generation number based one.
    
    1. Each request has a u64 generation + state value, which can be
       updated only by the request owner.  Whenever a request becomes
       in-flight, the generation number gets bumped up too.  This provides
       the basis for the timeout path to distinguish different recycle
       instances of the request.
    
       Also, marking a request in-flight and setting its deadline are
       protected with a seqcount so that the timeout path can fetch both
       values coherently.
    
    2. The timeout path fetches the generation, state and deadline.  If
       the verdict is timeout, it records the generation into a dedicated
       request abortion field and does RCU wait.
    
    3. The completion path is also protected by RCU (from the previous
       patch) and checks whether the current generation number and state
       match the abortion field.  If so, it skips completion.
    
    4. The timeout path, after RCU wait, scans requests again and
       terminates the ones whose generation and state still match the ones
       requested for abortion.
    
       By now, the timeout path knows that either the generation number
       and state changed if it lost the race or the completion will yield
       to it and can safely timeout the request.
    
    While it's more lines of code, it's conceptually simpler, doesn't
    depend on direct use of subtle memory ordering or coherence, and
    hopefully doesn't terminate the wrong instance.
    
    While this change makes REQ_ATOM_COMPLETE synchronization unnecessary
    between issue/complete and timeout paths, REQ_ATOM_COMPLETE isn't
    removed yet as it's still used in other places.  Future patches will
    move all state tracking to the new mechanism and remove all bitops in
    the hot paths.
    
    Note that this patch adds a comment explaining a race condition in
    BLK_EH_RESET_TIMER path.  The race has always been there and this
    patch doesn't change it.  It's just documenting the existing race.
    
    v2: - Fixed BLK_EH_RESET_TIMER handling as pointed out by Jianchao.
        - s/request->gstate_seqc/request->gstate_seq/ as suggested by Peter.
        - READ_ONCE() added in blk_mq_rq_update_state() as suggested by Peter.
    
    v3: - Fixed possible extended seqcount / u64_stats_sync read looping
          spotted by Peter.
        - MQ_RQ_IDLE was incorrectly being set in complete_request instead
          of free_request.  Fixed.
    
    v4: - Rebased on top of hctx_lock() refactoring patch.
        - Added comment explaining the use of hctx_lock() in completion path.
    
    v5: - Added comments requested by Bart.
        - Note the addition of BLK_EH_RESET_TIMER race condition in the
          commit message.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: "jianchao.wang" <jianchao.w.wang@oracle.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Bart Van Assche <Bart.VanAssche@wdc.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index 764ecf9aeb30..6427be7ac363 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -208,7 +208,7 @@ void blk_add_timer(struct request *req)
 	if (!req->timeout)
 		req->timeout = q->rq_timeout;
 
-	WRITE_ONCE(req->deadline, jiffies + req->timeout);
+	req->deadline = jiffies + req->timeout;
 
 	/*
 	 * Only the non-mq case needs to add the request to a protected list.

commit 4e9b6f20828ac880dbc1fa2fdbafae779473d1af
Author: Bart Van Assche <bart.vanassche@wdc.com>
Date:   Thu Oct 19 10:00:48 2017 -0700

    block: Fix a race between blk_cleanup_queue() and timeout handling
    
    Make sure that if the timeout timer fires after a queue has been
    marked "dying" that the affected requests are finished.
    
    Reported-by: chenxiang (M) <chenxiang66@hisilicon.com>
    Fixes: commit 287922eb0b18 ("block: defer timeouts to a workqueue")
    Signed-off-by: Bart Van Assche <bart.vanassche@wdc.com>
    Tested-by: chenxiang (M) <chenxiang66@hisilicon.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Keith Busch <keith.busch@intel.com>
    Cc: Hannes Reinecke <hare@suse.com>
    Cc: Ming Lei <ming.lei@redhat.com>
    Cc: Johannes Thumshirn <jthumshirn@suse.de>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index e3e9c9771d36..764ecf9aeb30 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -134,8 +134,6 @@ void blk_timeout_work(struct work_struct *work)
 	struct request *rq, *tmp;
 	int next_set = 0;
 
-	if (blk_queue_enter(q, true))
-		return;
 	spin_lock_irqsave(q->queue_lock, flags);
 
 	list_for_each_entry_safe(rq, tmp, &q->timeout_list, timeout_list)
@@ -145,7 +143,6 @@ void blk_timeout_work(struct work_struct *work)
 		mod_timer(&q->timeout, round_jiffies_up(next));
 
 	spin_unlock_irqrestore(q->queue_lock, flags);
-	blk_queue_exit(q);
 }
 
 /**

commit a7af0af32171c17d881e3e58b0925c4a44fb5a42
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Sep 6 10:00:22 2017 +0200

    blk-mq: attempt to fix atomic flag memory ordering
    
    Attempt to untangle the ordering in blk-mq. The patch introducing the
    single smp_mb__before_atomic() is obviously broken in that it doesn't
    clearly specify a pairing barrier and an obtained guarantee.
    
    The comment is further misleading in that it hints that the
    deadline store and the COMPLETE store also need to be ordered, but
    AFAICT there is no such dependency. However what does appear to be
    important is the clear happening _after_ the store, and that worked by
    pure accident.
    
    This clarifies blk_mq_start_request() -- we should not get there with
    STARTING set -- this simplifies the code and makes the barrier usage
    sane (the old code could be read to allow not having _any_ atomic after
    the barrier, in which case the barrier hasn't got anything to order). We
    then also introduce the missing pairing barrier for it.
    
    Also down-grade the barrier to smp_wmb(), this is cheaper for
    PowerPC/ARM and doesn't cost anything extra on x86.
    
    And it documents the STARTING vs COMPLETE ordering. Although I've not
    been entirely successful in reverse engineering the blk-mq state
    machine so there might still be more funnies around timeout vs
    requeue.
    
    If I got anything wrong, feel free to educate me by adding comments to
    clarify things ;-)
    
    Cc: Alan Stern <stern@rowland.harvard.edu>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Ming Lei <tom.leiming@gmail.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Andrea Parri <parri.andrea@gmail.com>
    Cc: Boqun Feng <boqun.feng@gmail.com>
    Cc: Bart Van Assche <bart.vanassche@wdc.com>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Fixes: 538b75341835 ("blk-mq: request deadline must be visible before marking rq as started")
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index 17ec83bb0900..e3e9c9771d36 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -211,7 +211,7 @@ void blk_add_timer(struct request *req)
 	if (!req->timeout)
 		req->timeout = q->rq_timeout;
 
-	req->deadline = jiffies + req->timeout;
+	WRITE_ONCE(req->deadline, jiffies + req->timeout);
 
 	/*
 	 * Only the non-mq case needs to add the request to a protected list.

commit 2fff8a924d4c614b5a17b2a236a2cf09aa51af5f
Author: Bart Van Assche <bart.vanassche@sandisk.com>
Date:   Tue Jun 20 11:15:45 2017 -0700

    block: Check locking assumptions at runtime
    
    Instead of documenting the locking assumptions of most block layer
    functions as a comment, use lockdep_assert_held() to verify locking
    assumptions at runtime.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Cc: Hannes Reinecke <hare@suse.com>
    Cc: Omar Sandoval <osandov@fb.com>
    Cc: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index cbff183f3d9f..17ec83bb0900 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -189,13 +189,15 @@ unsigned long blk_rq_timeout(unsigned long timeout)
  * Notes:
  *    Each request has its own timer, and as it is added to the queue, we
  *    set up the timer. When the request completes, we cancel the timer.
- *    Queue lock must be held for the non-mq case, mq case doesn't care.
  */
 void blk_add_timer(struct request *req)
 {
 	struct request_queue *q = req->q;
 	unsigned long expiry;
 
+	if (!q->mq_ops)
+		lockdep_assert_held(q->queue_lock);
+
 	/* blk-mq has its own handler, so we don't need ->rq_timed_out_fn */
 	if (!q->mq_ops && !q->rq_timed_out_fn)
 		return;

commit caf7df12272118e0274c8353bcfeaf60c7743a47
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Apr 20 16:03:16 2017 +0200

    block: remove the errors field from struct request
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Bart Van Assche <Bart.VanAssche@sandisk.com>
    Acked-by: Roger Pau Monn√© <roger.pau@citrix.com>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index a30441a200c0..cbff183f3d9f 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -89,7 +89,6 @@ static void blk_rq_timed_out(struct request *req)
 		ret = q->rq_timed_out_fn(req);
 	switch (ret) {
 	case BLK_EH_HANDLED:
-		/* Can we use req->errors here? */
 		__blk_complete_request(req);
 		break;
 	case BLK_EH_RESET_TIMER:

commit bbc758ec04c2f30805ce0fcdfbaa4c3445fafbae
Author: Christoph Hellwig <hch@lst.de>
Date:   Sat Nov 7 09:39:28 2015 +0100

    block: remove REQ_NO_TIMEOUT flag
    
    This was added for the 'magic' AEN requests in the NVMe driver that never
    return.  We now handle them purely inside the driver and don't need this
    core hack any more.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Keith Busch <keith.busch@intel.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index dd4fdfbcb3dd..a30441a200c0 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -197,9 +197,6 @@ void blk_add_timer(struct request *req)
 	struct request_queue *q = req->q;
 	unsigned long expiry;
 
-	if (req->cmd_flags & REQ_NO_TIMEOUT)
-		return;
-
 	/* blk-mq has its own handler, so we don't need ->rq_timed_out_fn */
 	if (!q->mq_ops && !q->rq_timed_out_fn)
 		return;

commit 287922eb0b186e2a5bf54fdd04b734c25c90035c
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Oct 30 20:57:30 2015 +0800

    block: defer timeouts to a workqueue
    
    Timer context is not very useful for drivers to perform any meaningful abort
    action from.  So instead of calling the driver from this useless context
    defer it to a workqueue as soon as possible.
    
    Note that while a delayed_work item would seem the right thing here I didn't
    dare to use it due to the magic in blk_add_timer that pokes deep into timer
    internals.  But maybe this encourages Tejun to add a sensible API for that to
    the workqueue API and we'll all be fine in the end :)
    
    Contains a major update from Keith Bush:
    
    "This patch removes synchronizing the timeout work so that the timer can
     start a freeze on its own queue. The timer enters the queue, so timer
     context can only start a freeze, but not wait for frozen."
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Keith Busch <keith.busch@intel.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index 3610af561748..dd4fdfbcb3dd 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -127,13 +127,16 @@ static void blk_rq_check_expired(struct request *rq, unsigned long *next_timeout
 	}
 }
 
-void blk_rq_timed_out_timer(unsigned long data)
+void blk_timeout_work(struct work_struct *work)
 {
-	struct request_queue *q = (struct request_queue *) data;
+	struct request_queue *q =
+		container_of(work, struct request_queue, timeout_work);
 	unsigned long flags, next = 0;
 	struct request *rq, *tmp;
 	int next_set = 0;
 
+	if (blk_queue_enter(q, true))
+		return;
 	spin_lock_irqsave(q->queue_lock, flags);
 
 	list_for_each_entry_safe(rq, tmp, &q->timeout_list, timeout_list)
@@ -143,6 +146,7 @@ void blk_rq_timed_out_timer(unsigned long data)
 		mod_timer(&q->timeout, round_jiffies_up(next));
 
 	spin_unlock_irqrestore(q->queue_lock, flags);
+	blk_queue_exit(q);
 }
 
 /**

commit 3b627a3f934c493ada71217f14681e5157e95783
Author: Jens Axboe <axboe@fb.com>
Date:   Tue Nov 24 15:58:53 2015 -0700

    block: clarify blk_add_timer() use case for blk-mq
    
    Just a comment update on not needing queue_lock, and that we aren't
    really adding the request to a timeout list for !mq.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index aa40aa93381b..3610af561748 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -186,6 +186,7 @@ unsigned long blk_rq_timeout(unsigned long timeout)
  * Notes:
  *    Each request has its own timer, and as it is added to the queue, we
  *    set up the timer. When the request completes, we cancel the timer.
+ *    Queue lock must be held for the non-mq case, mq case doesn't care.
  */
 void blk_add_timer(struct request *req)
 {
@@ -209,6 +210,11 @@ void blk_add_timer(struct request *req)
 		req->timeout = q->rq_timeout;
 
 	req->deadline = jiffies + req->timeout;
+
+	/*
+	 * Only the non-mq case needs to add the request to a protected list.
+	 * For the mq case we simply scan the tag map.
+	 */
 	if (!q->mq_ops)
 		list_add_tail(&req->timeout_list, &req->q->timeout_list);
 

commit 55ce0da1da287822e5ffb5fcd6e357180d5ba4cd
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Oct 30 20:47:04 2015 +0800

    block: fix blk_abort_request for blk-mq drivers
    
    We only added the request to the request list for the !blk-mq case,
    so we should only delete it in that case as well.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index 246dfb16c3d9..aa40aa93381b 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -158,11 +158,13 @@ void blk_abort_request(struct request *req)
 {
 	if (blk_mark_rq_complete(req))
 		return;
-	blk_delete_timer(req);
-	if (req->q->mq_ops)
+
+	if (req->q->mq_ops) {
 		blk_mq_rq_timed_out(req, false);
-	else
+	} else {
+		blk_delete_timer(req);
 		blk_rq_timed_out(req);
+	}
 }
 EXPORT_SYMBOL_GPL(blk_abort_request);
 

commit 5b3f25fc343690cafd3e27431a69a7bdaf9df001
Author: Keith Busch <keith.busch@intel.com>
Date:   Wed Jan 7 18:55:46 2015 -0700

    blk-mq: Allow requests to never expire
    
    Some types of requests may be started that are not gauranteed to ever
    complete. This adds a request flag that a driver can use so mark the
    request as such.
    
    Signed-off-by: Keith Busch <keith.busch@intel.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index 56c025894cdf..246dfb16c3d9 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -190,6 +190,9 @@ void blk_add_timer(struct request *req)
 	struct request_queue *q = req->q;
 	unsigned long expiry;
 
+	if (req->cmd_flags & REQ_NO_TIMEOUT)
+		return;
+
 	/* blk-mq has its own handler, so we don't need ->rq_timed_out_fn */
 	if (!q->mq_ops && !q->rq_timed_out_fn)
 		return;

commit 90415837659fec54f33584b423dab250eb1e8432
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Sep 22 10:21:48 2014 -0600

    block: fix blk_abort_request on blk-mq
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    
    Moved blk_mq_rq_timed_out() definition to the private blk-mq.h header.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index 8bae410b8a1b..56c025894cdf 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -7,6 +7,7 @@
 #include <linux/fault-inject.h>
 
 #include "blk.h"
+#include "blk-mq.h"
 
 #ifdef CONFIG_FAIL_IO_TIMEOUT
 
@@ -158,7 +159,10 @@ void blk_abort_request(struct request *req)
 	if (blk_mark_rq_complete(req))
 		return;
 	blk_delete_timer(req);
-	blk_rq_timed_out(req);
+	if (req->q->mq_ops)
+		blk_mq_rq_timed_out(req, false);
+	else
+		blk_rq_timed_out(req);
 }
 EXPORT_SYMBOL_GPL(blk_abort_request);
 

commit 5e940aaa597c15e916618240ae5838864f36c91e
Author: Ming Lei <ming.lei@canonical.com>
Date:   Fri Sep 19 21:53:46 2014 +0800

    blk-timeout: fix blk_add_timer
    
    Commit 8cb34819cdd5d(blk-mq: unshared timeout handler) introduces
    blk-mq's own timeout handler, and removes following line:
    
            blk_queue_rq_timed_out(q, blk_mq_rq_timed_out);
    
    which then causes blk_add_timer() to bypass adding the timer,
    since blk-mq no longer has q->rq_timed_out_fn defined.
    
    This patch fixes the problem by bypassing the check for blk-mq,
    so that both request deadlines are still set and the rolling
    timer updated.
    
    Signed-off-by: Ming Lei <ming.lei@canonical.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index 4d448259e622..8bae410b8a1b 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -186,7 +186,8 @@ void blk_add_timer(struct request *req)
 	struct request_queue *q = req->q;
 	unsigned long expiry;
 
-	if (!q->rq_timed_out_fn)
+	/* blk-mq has its own handler, so we don't need ->rq_timed_out_fn */
+	if (!q->mq_ops && !q->rq_timed_out_fn)
 		return;
 
 	BUG_ON(!list_empty(&req->timeout_list));

commit 46f92d42ee37e10970e33891b7b61a342bd97aeb
Author: Christoph Hellwig <hch@lst.de>
Date:   Sat Sep 13 16:40:12 2014 -0700

    blk-mq: unshared timeout handler
    
    Duplicate the (small) timeout handler in blk-mq so that we can pass
    arguments more easily to the driver timeout handler.  This enables
    the next patch.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index 95a09590ccfd..4d448259e622 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -7,7 +7,6 @@
 #include <linux/fault-inject.h>
 
 #include "blk.h"
-#include "blk-mq.h"
 
 #ifdef CONFIG_FAIL_IO_TIMEOUT
 
@@ -90,10 +89,7 @@ static void blk_rq_timed_out(struct request *req)
 	switch (ret) {
 	case BLK_EH_HANDLED:
 		/* Can we use req->errors here? */
-		if (q->mq_ops)
-			__blk_mq_complete_request(req);
-		else
-			__blk_complete_request(req);
+		__blk_complete_request(req);
 		break;
 	case BLK_EH_RESET_TIMER:
 		blk_add_timer(req);
@@ -113,7 +109,7 @@ static void blk_rq_timed_out(struct request *req)
 	}
 }
 
-void blk_rq_check_expired(struct request *rq, unsigned long *next_timeout,
+static void blk_rq_check_expired(struct request *rq, unsigned long *next_timeout,
 			  unsigned int *next_set)
 {
 	if (time_after_eq(jiffies, rq->deadline)) {

commit c7bca4183f73f9d013ea8ae572528b48b5e1ee42
Author: Jens Axboe <axboe@fb.com>
Date:   Fri May 30 15:41:39 2014 -0600

    block: ensure that the timer is always added
    
    Commit f793aa537866 relaxed the timer addition a little too much.
    If the timer isn't pending, we always need to add it.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index 43e8b515806f..95a09590ccfd 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -224,7 +224,7 @@ void blk_add_timer(struct request *req)
 		 * modifying the timer because expires for value X
 		 * will be X + something.
 		 */
-		if (diff >= HZ / 2)
+		if (!timer_pending(&q->timeout) || (diff >= HZ / 2))
 			mod_timer(&q->timeout, expiry);
 	}
 

commit 0d2602ca30e410e84e8bdf05c84ed5688e0a5a44
Author: Jens Axboe <axboe@fb.com>
Date:   Tue May 13 15:10:52 2014 -0600

    blk-mq: improve support for shared tags maps
    
    This adds support for active queue tracking, meaning that the
    blk-mq tagging maintains a count of active users of a tag set.
    This allows us to maintain a notion of fairness between users,
    so that we can distribute the tag depth evenly without starving
    some users while allowing others to try unfair deep queues.
    
    If sharing of a tag set is detected, each hardware queue will
    track the depth of its own queue. And if this exceeds the total
    depth divided by the number of active queues, the user is actively
    throttled down.
    
    The active queue count is done lazily to avoid bouncing that data
    between submitter and completer. Each hardware queue gets marked
    active when it allocates its first tag, and gets marked inactive
    when 1) the last tag is cleared, and 2) the queue timeout grace
    period has passed.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index 448745683d28..43e8b515806f 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -166,6 +166,17 @@ void blk_abort_request(struct request *req)
 }
 EXPORT_SYMBOL_GPL(blk_abort_request);
 
+unsigned long blk_rq_timeout(unsigned long timeout)
+{
+	unsigned long maxt;
+
+	maxt = round_jiffies_up(jiffies + BLK_MAX_TIMEOUT);
+	if (time_after(timeout, maxt))
+		timeout = maxt;
+
+	return timeout;
+}
+
 /**
  * blk_add_timer - Start timeout timer for a single request
  * @req:	request that is about to start running.
@@ -200,7 +211,7 @@ void blk_add_timer(struct request *req)
 	 * than an existing one, modify the timer. Round up to next nearest
 	 * second.
 	 */
-	expiry = round_jiffies_up(req->deadline);
+	expiry = blk_rq_timeout(round_jiffies_up(req->deadline));
 
 	if (!timer_pending(&q->timeout) ||
 	    time_before(expiry, q->timeout.expires)) {

commit c4a634f432376a28c9639b35746e33168ab97709
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Apr 25 14:14:48 2014 +0200

    block: fold __blk_add_timer into blk_add_timer
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index 49988a3ca85c..448745683d28 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -166,8 +166,15 @@ void blk_abort_request(struct request *req)
 }
 EXPORT_SYMBOL_GPL(blk_abort_request);
 
-static void __blk_add_timer(struct request *req,
-			    struct list_head *timeout_list)
+/**
+ * blk_add_timer - Start timeout timer for a single request
+ * @req:	request that is about to start running.
+ *
+ * Notes:
+ *    Each request has its own timer, and as it is added to the queue, we
+ *    set up the timer. When the request completes, we cancel the timer.
+ */
+void blk_add_timer(struct request *req)
 {
 	struct request_queue *q = req->q;
 	unsigned long expiry;
@@ -185,8 +192,8 @@ static void __blk_add_timer(struct request *req,
 		req->timeout = q->rq_timeout;
 
 	req->deadline = jiffies + req->timeout;
-	if (timeout_list)
-		list_add_tail(&req->timeout_list, timeout_list);
+	if (!q->mq_ops)
+		list_add_tail(&req->timeout_list, &req->q->timeout_list);
 
 	/*
 	 * If the timer isn't already pending or this timeout is earlier
@@ -211,22 +218,3 @@ static void __blk_add_timer(struct request *req,
 	}
 
 }
-
-/**
- * blk_add_timer - Start timeout timer for a single request
- * @req:	request that is about to start running.
- *
- * Notes:
- *    Each request has its own timer, and as it is added to the queue, we
- *    set up the timer. When the request completes, we cancel the timer.
- */
-void blk_add_timer(struct request *req)
-{
-	struct request_queue *q = req->q;
-
-	if (q->mq_ops)
-		__blk_add_timer(req, NULL);
-	else
-		__blk_add_timer(req, &req->q->timeout_list);
-}
-

commit 87ee7b112193bd081ba1a171fa5f6f39c429ef56
Author: Jens Axboe <axboe@fb.com>
Date:   Thu Apr 24 08:51:47 2014 -0600

    blk-mq: fix race with timeouts and requeue events
    
    If a requeue event races with a timeout, we can get into the
    situation where we attempt to complete a request from the
    timeout handler when it's not start anymore. This causes a crash.
    So have the timeout handler check that REQ_ATOM_STARTED is still
    set on the request - if not, we ignore the event. If this happens,
    the request has now been marked as complete. As a consequence, we
    need to ensure to clear REQ_ATOM_COMPLETE in blk_mq_start_request(),
    as to maintain proper request state.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index a09e8af8186c..49988a3ca85c 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -96,11 +96,7 @@ static void blk_rq_timed_out(struct request *req)
 			__blk_complete_request(req);
 		break;
 	case BLK_EH_RESET_TIMER:
-		if (q->mq_ops)
-			blk_mq_add_timer(req);
-		else
-			blk_add_timer(req);
-
+		blk_add_timer(req);
 		blk_clear_rq_complete(req);
 		break;
 	case BLK_EH_NOT_HANDLED:
@@ -170,7 +166,8 @@ void blk_abort_request(struct request *req)
 }
 EXPORT_SYMBOL_GPL(blk_abort_request);
 
-void __blk_add_timer(struct request *req, struct list_head *timeout_list)
+static void __blk_add_timer(struct request *req,
+			    struct list_head *timeout_list)
 {
 	struct request_queue *q = req->q;
 	unsigned long expiry;
@@ -225,6 +222,11 @@ void __blk_add_timer(struct request *req, struct list_head *timeout_list)
  */
 void blk_add_timer(struct request *req)
 {
-	__blk_add_timer(req, &req->q->timeout_list);
+	struct request_queue *q = req->q;
+
+	if (q->mq_ops)
+		__blk_add_timer(req, NULL);
+	else
+		__blk_add_timer(req, &req->q->timeout_list);
 }
 

commit f793aa53786668c9e0db5900f70f560e99d62fa0
Author: Jens Axboe <axboe@fb.com>
Date:   Wed Apr 16 11:36:54 2014 -0600

    block: relax when to modify the timeout timer
    
    Since we are now, by default, applying timer slack to expiry times,
    the logic for when to modify a timer in the block code is suboptimal.
    The block layer keeps a forward rolling timer per queue for all
    requests, and modifies this timer if a request has a shorter timeout
    than what the current expiry time is. However, this breaks down
    when our rounded timer values get applied slack. Then each new
    request ends up modifying the timer, since we're still a little
    in front of the timer + slack.
    
    Fix this by allowing a tolerance of HZ / 2, the timeout handling
    doesn't need to be very precise. This drastically cuts down
    the number of timer modifications we have to make.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index d96f7061c6fd..a09e8af8186c 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -199,8 +199,19 @@ void __blk_add_timer(struct request *req, struct list_head *timeout_list)
 	expiry = round_jiffies_up(req->deadline);
 
 	if (!timer_pending(&q->timeout) ||
-	    time_before(expiry, q->timeout.expires))
-		mod_timer(&q->timeout, expiry);
+	    time_before(expiry, q->timeout.expires)) {
+		unsigned long diff = q->timeout.expires - expiry;
+
+		/*
+		 * Due to added timer slack to group timers, the timer
+		 * will often be a little in front of what we asked for.
+		 * So apply some tolerance here too, otherwise we keep
+		 * modifying the timer because expires for value X
+		 * will be X + something.
+		 */
+		if (diff >= HZ / 2)
+			mod_timer(&q->timeout, expiry);
+	}
 
 }
 

commit 30a91cb4ef385fe1b260df204ef314d86fff2850
Author: Christoph Hellwig <hch@infradead.org>
Date:   Mon Feb 10 03:24:38 2014 -0800

    blk-mq: rework I/O completions
    
    Rework I/O completions to work more like the old code path.  blk_mq_end_io
    now stays out of the business of deferring completions to others CPUs
    and calling blk_mark_rq_complete.  The latter is very important to allow
    completing requests that have timed out and thus are already marked completed,
    the former allows using the IPI callout even for driver specific completions
    instead of having to reimplement them.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index bba81c9348e1..d96f7061c6fd 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -91,7 +91,7 @@ static void blk_rq_timed_out(struct request *req)
 	case BLK_EH_HANDLED:
 		/* Can we use req->errors here? */
 		if (q->mq_ops)
-			blk_mq_complete_request(req, req->errors);
+			__blk_mq_complete_request(req);
 		else
 			__blk_complete_request(req);
 		break;

commit e37459b8e2c7db6735e39e019e448b76e5e77647
Merge: c7d1ba417c7c e7e245000110
Author: Jens Axboe <axboe@kernel.dk>
Date:   Fri Nov 8 09:08:12 2013 -0700

    Merge branch 'blk-mq/core' into for-3.13/core
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>
    
    Conflicts:
            block/blk-timeout.c

commit 8616ebb16bcef312024b9d28719f9bf5c5c3aafb
Author: Duan Jiong <duanj.fnst@cn.fujitsu.com>
Date:   Wed Nov 6 15:55:44 2013 +0800

    block: replace IS_ERR and PTR_ERR with PTR_ERR_OR_ZERO
    
    This patch fixes coccinelle error regarding usage of IS_ERR and
    PTR_ERR instead of PTR_ERR_OR_ZERO.
    
    Signed-off-by: Duan Jiong <duanj.fnst@cn.fujitsu.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index 655ba909cd6a..abf725c655fc 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -31,7 +31,7 @@ static int __init fail_io_timeout_debugfs(void)
 	struct dentry *dir = fault_create_debugfs_attr("fail_io_timeout",
 						NULL, &fail_io_timeout);
 
-	return IS_ERR(dir) ? PTR_ERR(dir) : 0;
+	return PTR_ERR_OR_ZERO(dir);
 }
 
 late_initcall(fail_io_timeout_debugfs);

commit 4912aa6c11e6a5d910264deedbec2075c6f1bb73
Author: Jeff Moyer <jmoyer@redhat.com>
Date:   Tue Oct 8 14:36:41 2013 -0400

    block: fix race between request completion and timeout handling
    
    crocode i2c_i801 i2c_core iTCO_wdt iTCO_vendor_support shpchp ioatdma dca be2net sg ses enclosure ext4 mbcache jbd2 sd_mod crc_t10dif ahci megaraid_sas(U) dm_mirror dm_region_hash dm_log dm_mod [last unloaded: scsi_wait_scan]
    
    Pid: 491, comm: scsi_eh_0 Tainted: G        W  ----------------   2.6.32-220.13.1.el6.x86_64 #1 IBM  -[8722PAX]-/00D1461
    RIP: 0010:[<ffffffff8124e424>]  [<ffffffff8124e424>] blk_requeue_request+0x94/0xa0
    RSP: 0018:ffff881057eefd60  EFLAGS: 00010012
    RAX: ffff881d99e3e8a8 RBX: ffff881d99e3e780 RCX: ffff881d99e3e8a8
    RDX: ffff881d99e3e8a8 RSI: ffff881d99e3e780 RDI: ffff881d99e3e780
    RBP: ffff881057eefd80 R08: ffff881057eefe90 R09: 0000000000000000
    R10: 0000000000000000 R11: 0000000000000000 R12: ffff881057f92338
    R13: 0000000000000000 R14: ffff881057f92338 R15: ffff883058188000
    FS:  0000000000000000(0000) GS:ffff880040200000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0018 ES: 0018 CR0: 000000008005003b
    CR2: 00000000006d3ec0 CR3: 000000302cd7d000 CR4: 00000000000406b0
    DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000400
    Process scsi_eh_0 (pid: 491, threadinfo ffff881057eee000, task ffff881057e29540)
    Stack:
     0000000000001057 0000000000000286 ffff8810275efdc0 ffff881057f16000
    <0> ffff881057eefdd0 ffffffff81362323 ffff881057eefe20 ffffffff8135f393
    <0> ffff881057e29af8 ffff8810275efdc0 ffff881057eefe78 ffff881057eefe90
    Call Trace:
     [<ffffffff81362323>] __scsi_queue_insert+0xa3/0x150
     [<ffffffff8135f393>] ? scsi_eh_ready_devs+0x5e3/0x850
     [<ffffffff81362a23>] scsi_queue_insert+0x13/0x20
     [<ffffffff8135e4d4>] scsi_eh_flush_done_q+0x104/0x160
     [<ffffffff8135fb6b>] scsi_error_handler+0x35b/0x660
     [<ffffffff8135f810>] ? scsi_error_handler+0x0/0x660
     [<ffffffff810908c6>] kthread+0x96/0xa0
     [<ffffffff8100c14a>] child_rip+0xa/0x20
     [<ffffffff81090830>] ? kthread+0x0/0xa0
     [<ffffffff8100c140>] ? child_rip+0x0/0x20
    Code: 00 00 eb d1 4c 8b 2d 3c 8f 97 00 4d 85 ed 74 bf 49 8b 45 00 49 83 c5 08 48 89 de 4c 89 e7 ff d0 49 8b 45 00 48 85 c0 75 eb eb a4 <0f> 0b eb fe 0f 1f 84 00 00 00 00 00 55 48 89 e5 0f 1f 44 00 00
    RIP  [<ffffffff8124e424>] blk_requeue_request+0x94/0xa0
     RSP <ffff881057eefd60>
    
    The RIP is this line:
            BUG_ON(blk_queued_rq(rq));
    
    After digging through the code, I think there may be a race between the
    request completion and the timer handler running.
    
    A timer is started for each request put on the device's queue (see
    blk_start_request->blk_add_timer).  If the request does not complete
    before the timer expires, the timer handler (blk_rq_timed_out_timer)
    will mark the request complete atomically:
    
    static inline int blk_mark_rq_complete(struct request *rq)
    {
            return test_and_set_bit(REQ_ATOM_COMPLETE, &rq->atomic_flags);
    }
    
    and then call blk_rq_timed_out.  The latter function will call
    scsi_times_out, which will return one of BLK_EH_HANDLED,
    BLK_EH_RESET_TIMER or BLK_EH_NOT_HANDLED.  If BLK_EH_RESET_TIMER is
    returned, blk_clear_rq_complete is called, and blk_add_timer is again
    called to simply wait longer for the request to complete.
    
    Now, if the request happens to complete while this is going on, what
    happens?  Given that we know the completion handler will bail if it
    finds the REQ_ATOM_COMPLETE bit set, we need to focus on the completion
    handler running after that bit is cleared.  So, from the above
    paragraph, after the call to blk_clear_rq_complete.  If the completion
    sets REQ_ATOM_COMPLETE before the BUG_ON in blk_add_timer, we go boom
    there (I haven't seen this in the cores).  Next, if we get the
    completion before the call to list_add_tail, then the timer will
    eventually fire for an old req, which may either be freed or reallocated
    (there is evidence that this might be the case).  Finally, if the
    completion comes in *after* the addition to the timeout list, I think
    it's harmless.  The request will be removed from the timeout list,
    req_atom_complete will be set, and all will be well.
    
    This will only actually explain the coredumps *IF* the request
    structure was freed, reallocated *and* queued before the error handler
    thread had a chance to process it.  That is possible, but it may make
    sense to keep digging for another race.  I think that if this is what
    was happening, we would see other instances of this problem showing up
    as null pointer or garbage pointer dereferences, for example when the
    request structure was not re-used.  It looks like we actually do run
    into that situation in other reports.
    
    This patch moves the BUG_ON(test_bit(REQ_ATOM_COMPLETE,
    &req->atomic_flags)); from blk_add_timer to the only caller that could
    trip over it (blk_start_request).  It then inverts the calls to
    blk_clear_rq_complete and blk_add_timer in blk_rq_timed_out to address
    the race.  I've boot tested this patch, but nothing more.
    
    Signed-off-by: Jeff Moyer <jmoyer@redhat.com>
    Acked-by: Hannes Reinecke <hare@suse.de>
    Cc: stable@kernel.org
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index 65f103563969..655ba909cd6a 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -91,8 +91,8 @@ static void blk_rq_timed_out(struct request *req)
 		__blk_complete_request(req);
 		break;
 	case BLK_EH_RESET_TIMER:
-		blk_clear_rq_complete(req);
 		blk_add_timer(req);
+		blk_clear_rq_complete(req);
 		break;
 	case BLK_EH_NOT_HANDLED:
 		/*
@@ -174,7 +174,6 @@ void blk_add_timer(struct request *req)
 		return;
 
 	BUG_ON(!list_empty(&req->timeout_list));
-	BUG_ON(test_bit(REQ_ATOM_COMPLETE, &req->atomic_flags));
 
 	/*
 	 * Some LLDs, like scsi, peek at the timeout to prevent a

commit 320ae51feed5c2f13664aa05a76bec198967e04d
Author: Jens Axboe <axboe@kernel.dk>
Date:   Thu Oct 24 09:20:05 2013 +0100

    blk-mq: new multi-queue block IO queueing mechanism
    
    Linux currently has two models for block devices:
    
    - The classic request_fn based approach, where drivers use struct
      request units for IO. The block layer provides various helper
      functionalities to let drivers share code, things like tag
      management, timeout handling, queueing, etc.
    
    - The "stacked" approach, where a driver squeezes in between the
      block layer and IO submitter. Since this bypasses the IO stack,
      driver generally have to manage everything themselves.
    
    With drivers being written for new high IOPS devices, the classic
    request_fn based driver doesn't work well enough. The design dates
    back to when both SMP and high IOPS was rare. It has problems with
    scaling to bigger machines, and runs into scaling issues even on
    smaller machines when you have IOPS in the hundreds of thousands
    per device.
    
    The stacked approach is then most often selected as the model
    for the driver. But this means that everybody has to re-invent
    everything, and along with that we get all the problems again
    that the shared approach solved.
    
    This commit introduces blk-mq, block multi queue support. The
    design is centered around per-cpu queues for queueing IO, which
    then funnel down into x number of hardware submission queues.
    We might have a 1:1 mapping between the two, or it might be
    an N:M mapping. That all depends on what the hardware supports.
    
    blk-mq provides various helper functions, which include:
    
    - Scalable support for request tagging. Most devices need to
      be able to uniquely identify a request both in the driver and
      to the hardware. The tagging uses per-cpu caches for freed
      tags, to enable cache hot reuse.
    
    - Timeout handling without tracking request on a per-device
      basis. Basically the driver should be able to get a notification,
      if a request happens to fail.
    
    - Optional support for non 1:1 mappings between issue and
      submission queues. blk-mq can redirect IO completions to the
      desired location.
    
    - Support for per-request payloads. Drivers almost always need
      to associate a request structure with some driver private
      command structure. Drivers can tell blk-mq this at init time,
      and then any request handed to the driver will have the
      required size of memory associated with it.
    
    - Support for merging of IO, and plugging. The stacked model
      gets neither of these. Even for high IOPS devices, merging
      sequential IO reduces per-command overhead and thus
      increases bandwidth.
    
    For now, this is provided as a potential 3rd queueing model, with
    the hope being that, as it matures, it can replace both the classic
    and stacked model. That would get us back to having just 1 real
    model for block devices, leaving the stacked approach to dm/md
    devices (as it was originally intended).
    
    Contributions in this patch from the following people:
    
    Shaohua Li <shli@fusionio.com>
    Alexander Gordeev <agordeev@redhat.com>
    Christoph Hellwig <hch@infradead.org>
    Mike Christie <michaelc@cs.wisc.edu>
    Matias Bjorling <m@bjorling.me>
    Jeff Moyer <jmoyer@redhat.com>
    
    Acked-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index 65f103563969..22846cf3595a 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -7,6 +7,7 @@
 #include <linux/fault-inject.h>
 
 #include "blk.h"
+#include "blk-mq.h"
 
 #ifdef CONFIG_FAIL_IO_TIMEOUT
 
@@ -88,11 +89,18 @@ static void blk_rq_timed_out(struct request *req)
 		ret = q->rq_timed_out_fn(req);
 	switch (ret) {
 	case BLK_EH_HANDLED:
-		__blk_complete_request(req);
+		/* Can we use req->errors here? */
+		if (q->mq_ops)
+			blk_mq_complete_request(req, req->errors);
+		else
+			__blk_complete_request(req);
 		break;
 	case BLK_EH_RESET_TIMER:
 		blk_clear_rq_complete(req);
-		blk_add_timer(req);
+		if (q->mq_ops)
+			blk_mq_add_timer(req);
+		else
+			blk_add_timer(req);
 		break;
 	case BLK_EH_NOT_HANDLED:
 		/*
@@ -108,6 +116,23 @@ static void blk_rq_timed_out(struct request *req)
 	}
 }
 
+void blk_rq_check_expired(struct request *rq, unsigned long *next_timeout,
+			  unsigned int *next_set)
+{
+	if (time_after_eq(jiffies, rq->deadline)) {
+		list_del_init(&rq->timeout_list);
+
+		/*
+		 * Check if we raced with end io completion
+		 */
+		if (!blk_mark_rq_complete(rq))
+			blk_rq_timed_out(rq);
+	} else if (!*next_set || time_after(*next_timeout, rq->deadline)) {
+		*next_timeout = rq->deadline;
+		*next_set = 1;
+	}
+}
+
 void blk_rq_timed_out_timer(unsigned long data)
 {
 	struct request_queue *q = (struct request_queue *) data;
@@ -117,21 +142,8 @@ void blk_rq_timed_out_timer(unsigned long data)
 
 	spin_lock_irqsave(q->queue_lock, flags);
 
-	list_for_each_entry_safe(rq, tmp, &q->timeout_list, timeout_list) {
-		if (time_after_eq(jiffies, rq->deadline)) {
-			list_del_init(&rq->timeout_list);
-
-			/*
-			 * Check if we raced with end io completion
-			 */
-			if (blk_mark_rq_complete(rq))
-				continue;
-			blk_rq_timed_out(rq);
-		} else if (!next_set || time_after(next, rq->deadline)) {
-			next = rq->deadline;
-			next_set = 1;
-		}
-	}
+	list_for_each_entry_safe(rq, tmp, &q->timeout_list, timeout_list)
+		blk_rq_check_expired(rq, &next, &next_set);
 
 	if (next_set)
 		mod_timer(&q->timeout, round_jiffies_up(next));
@@ -157,15 +169,7 @@ void blk_abort_request(struct request *req)
 }
 EXPORT_SYMBOL_GPL(blk_abort_request);
 
-/**
- * blk_add_timer - Start timeout timer for a single request
- * @req:	request that is about to start running.
- *
- * Notes:
- *    Each request has its own timer, and as it is added to the queue, we
- *    set up the timer. When the request completes, we cancel the timer.
- */
-void blk_add_timer(struct request *req)
+void __blk_add_timer(struct request *req, struct list_head *timeout_list)
 {
 	struct request_queue *q = req->q;
 	unsigned long expiry;
@@ -184,7 +188,8 @@ void blk_add_timer(struct request *req)
 		req->timeout = q->rq_timeout;
 
 	req->deadline = jiffies + req->timeout;
-	list_add_tail(&req->timeout_list, &q->timeout_list);
+	if (timeout_list)
+		list_add_tail(&req->timeout_list, timeout_list);
 
 	/*
 	 * If the timer isn't already pending or this timeout is earlier
@@ -196,5 +201,19 @@ void blk_add_timer(struct request *req)
 	if (!timer_pending(&q->timeout) ||
 	    time_before(expiry, q->timeout.expires))
 		mod_timer(&q->timeout, expiry);
+
+}
+
+/**
+ * blk_add_timer - Start timeout timer for a single request
+ * @req:	request that is about to start running.
+ *
+ * Notes:
+ *    Each request has its own timer, and as it is added to the queue, we
+ *    set up the timer. When the request completes, we cancel the timer.
+ */
+void blk_add_timer(struct request *req)
+{
+	__blk_add_timer(req, &req->q->timeout_list);
 }
 

commit 80bd7181b036c7b4118eb19cfff3b555889596e6
Author: Hannes Reinecke <hare@suse.de>
Date:   Wed Jan 30 09:26:17 2013 +0000

    block: check for timeout function in blk_rq_timed_out()
    
    rq_timed_out_fn might have been unset while the request
    was in flight, so we need to check for it in blk_rq_timed_out().
    
    Acked-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Hannes Reinecke <hare@suse.de>
    Signed-off-by: Stefan Weinhuber <wein@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index 6e4744cbfb56..65f103563969 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -82,9 +82,10 @@ void blk_delete_timer(struct request *req)
 static void blk_rq_timed_out(struct request *req)
 {
 	struct request_queue *q = req->q;
-	enum blk_eh_timer_return ret;
+	enum blk_eh_timer_return ret = BLK_EH_RESET_TIMER;
 
-	ret = q->rq_timed_out_fn(req);
+	if (q->rq_timed_out_fn)
+		ret = q->rq_timed_out_fn(req);
 	switch (ret) {
 	case BLK_EH_HANDLED:
 		__blk_complete_request(req);

commit 76aaa5101fffaef12b45b4c01ed0d0528f23dedf
Author: Asias He <asias@redhat.com>
Date:   Thu Jun 14 09:04:07 2012 +0200

    block: Drop dead function blk_abort_queue()
    
    This function was only used by btrfs code in btrfs_abort_devices()
    (seems in a wrong way).
    
    It was removed in commit d07eb9117050c9ed3f78296ebcc06128b52693be,
    So, Let's remove the dead code to avoid any confusion.
    
    Changes in v2: update commit log, btrfs_abort_devices() was removed
    already.
    
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: linux-kernel@vger.kernel.org
    Cc: Chris Mason <chris.mason@oracle.com>
    Cc: linux-btrfs@vger.kernel.org
    Cc: David Sterba <dave@jikos.cz>
    Signed-off-by: Asias He <asias@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index 780354888958..6e4744cbfb56 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -197,44 +197,3 @@ void blk_add_timer(struct request *req)
 		mod_timer(&q->timeout, expiry);
 }
 
-/**
- * blk_abort_queue -- Abort all request on given queue
- * @queue:	pointer to queue
- *
- */
-void blk_abort_queue(struct request_queue *q)
-{
-	unsigned long flags;
-	struct request *rq, *tmp;
-	LIST_HEAD(list);
-
-	/*
-	 * Not a request based block device, nothing to abort
-	 */
-	if (!q->request_fn)
-		return;
-
-	spin_lock_irqsave(q->queue_lock, flags);
-
-	elv_abort_queue(q);
-
-	/*
-	 * Splice entries to local list, to avoid deadlocking if entries
-	 * get readded to the timeout list by error handling
-	 */
-	list_splice_init(&q->timeout_list, &list);
-
-	list_for_each_entry_safe(rq, tmp, &list, timeout_list)
-		blk_abort_request(rq);
-
-	/*
-	 * Occasionally, blk_abort_request() will return without
-	 * deleting the element from the list. Make sure we add those back
-	 * instead of leaving them on the local stack list.
-	 */
-	list_splice(&list, &q->timeout_list);
-
-	spin_unlock_irqrestore(q->queue_lock, flags);
-
-}
-EXPORT_SYMBOL_GPL(blk_abort_queue);

commit dd48c085c1cdf9446f92826f1fd451167fb6c2fd
Author: Akinobu Mita <akinobu.mita@gmail.com>
Date:   Wed Aug 3 16:21:01 2011 -0700

    fault-injection: add ability to export fault_attr in arbitrary directory
    
    init_fault_attr_dentries() is used to export fault_attr via debugfs.
    But it can only export it in debugfs root directory.
    
    Per Forlin is working on mmc_fail_request which adds support to inject
    data errors after a completed host transfer in MMC subsystem.
    
    The fault_attr for mmc_fail_request should be defined per mmc host and
    export it in debugfs directory per mmc host like
    /sys/kernel/debug/mmc0/mmc_fail_request.
    
    init_fault_attr_dentries() doesn't help for mmc_fail_request.  So this
    introduces fault_create_debugfs_attr() which is able to create a
    directory in the arbitrary directory and replace
    init_fault_attr_dentries().
    
    [akpm@linux-foundation.org: extraneous semicolon, per Randy]
    Signed-off-by: Akinobu Mita <akinobu.mita@gmail.com>
    Tested-by: Per Forlin <per.forlin@linaro.org>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Matt Mackall <mpm@selenic.com>
    Cc: Randy Dunlap <rdunlap@xenotime.net>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index 4f0c06c7a338..780354888958 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -28,7 +28,10 @@ int blk_should_fake_timeout(struct request_queue *q)
 
 static int __init fail_io_timeout_debugfs(void)
 {
-	return init_fault_attr_dentries(&fail_io_timeout, "fail_io_timeout");
+	struct dentry *dir = fault_create_debugfs_attr("fail_io_timeout",
+						NULL, &fail_io_timeout);
+
+	return IS_ERR(dir) ? PTR_ERR(dir) : 0;
 }
 
 late_initcall(fail_io_timeout_debugfs);

commit a534dbe96e9929c7245924d8252d89048c23d569
Author: Richard Kennedy <richard@rsk.demon.co.uk>
Date:   Wed Apr 14 20:54:03 2010 +0200

    block: ensure jiffies wrap is handled correctly in blk_rq_timed_out_timer
    
    blk_rq_timed_out_timer() relied on blk_add_timer() never returning a
    timer value of zero, but commit 7838c15b8dd18e78a523513749e5b54bda07b0cb
    removed the code that bumped this value when it was zero.
    Therefore when jiffies is near wrap we could get unlucky & not set the
    timeout value correctly.
    
    This patch uses a flag to indicate that the timeout value was set and so
    handles jiffies wrap correctly, and it keeps all the logic in one
    function so should be easier to maintain in the future.
    
    Signed-off-by: Richard Kennedy <richard@rsk.demon.co.uk>
    Cc: stable@kernel.org
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index 1ba7e0aca878..4f0c06c7a338 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -109,6 +109,7 @@ void blk_rq_timed_out_timer(unsigned long data)
 	struct request_queue *q = (struct request_queue *) data;
 	unsigned long flags, next = 0;
 	struct request *rq, *tmp;
+	int next_set = 0;
 
 	spin_lock_irqsave(q->queue_lock, flags);
 
@@ -122,16 +123,13 @@ void blk_rq_timed_out_timer(unsigned long data)
 			if (blk_mark_rq_complete(rq))
 				continue;
 			blk_rq_timed_out(rq);
-		} else if (!next || time_after(next, rq->deadline))
+		} else if (!next_set || time_after(next, rq->deadline)) {
 			next = rq->deadline;
+			next_set = 1;
+		}
 	}
 
-	/*
-	 * next can never be 0 here with the list non-empty, since we always
-	 * bump ->deadline to 1 so we can detect if the timer was ever added
-	 * or not. See comment in blk_add_timer()
-	 */
-	if (next)
+	if (next_set)
 		mod_timer(&q->timeout, round_jiffies_up(next));
 
 	spin_unlock_irqrestore(q->queue_lock, flags);

commit 2eef33e439ba9ae387cdc3f1abcef2f3f6c4e7a8
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Apr 23 11:05:18 2009 +0900

    block: clean up misc stuff after block layer timeout conversion
    
    * In blk_rq_timed_out_timer(), else { if } to else if
    
    * In blk_add_timer(), simplify if/else block
    
    [ Impact: cleanup ]
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index 1ec0d503cacd..1ba7e0aca878 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -122,10 +122,8 @@ void blk_rq_timed_out_timer(unsigned long data)
 			if (blk_mark_rq_complete(rq))
 				continue;
 			blk_rq_timed_out(rq);
-		} else {
-			if (!next || time_after(next, rq->deadline))
-				next = rq->deadline;
-		}
+		} else if (!next || time_after(next, rq->deadline))
+			next = rq->deadline;
 	}
 
 	/*
@@ -176,16 +174,14 @@ void blk_add_timer(struct request *req)
 	BUG_ON(!list_empty(&req->timeout_list));
 	BUG_ON(test_bit(REQ_ATOM_COMPLETE, &req->atomic_flags));
 
-	if (req->timeout)
-		req->deadline = jiffies + req->timeout;
-	else {
-		req->deadline = jiffies + q->rq_timeout;
-		/*
-		 * Some LLDs, like scsi, peek at the timeout to prevent
-		 * a command from being retried forever.
-		 */
+	/*
+	 * Some LLDs, like scsi, peek at the timeout to prevent a
+	 * command from being retried forever.
+	 */
+	if (!req->timeout)
 		req->timeout = q->rq_timeout;
-	}
+
+	req->deadline = jiffies + req->timeout;
 	list_add_tail(&req->timeout_list, &q->timeout_list);
 
 	/*

commit 17d5c8ca7572124c9623045f24b0c21d4aa2b47f
Author: Hannes Reinecke <hare@suse.de>
Date:   Thu Apr 23 10:32:59 2009 +0200

    block: fix intermittent dm timeout based oops
    
    Very rarely under stress testing of dm, oopses are occuring as
    something tampers with an old stack frame.  This has been traced back
    to blk_abort_queue() leaving a timeout_list pointing to the stack.
    The reason is that sometimes blk_abort_request() won't delete the
    timer (if the request is marked as complete but before the timer has
    been removed, a small race window).  Fix this by splicing back from
    the ususally empty list to the q->timeout_list.
    
    Signed-off-by: Hannes Reinecke <hare@suse.de>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index 8f570c4c80ee..1ec0d503cacd 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -230,6 +230,13 @@ void blk_abort_queue(struct request_queue *q)
 	list_for_each_entry_safe(rq, tmp, &list, timeout_list)
 		blk_abort_request(rq);
 
+	/*
+	 * Occasionally, blk_abort_request() will return without
+	 * deleting the element from the list. Make sure we add those back
+	 * instead of leaving them on the local stack list.
+	 */
+	list_splice(&list, &q->timeout_list);
+
 	spin_unlock_irqrestore(q->queue_lock, flags);
 
 }

commit b759113499d6c7cb75fab04f56772579308bc0f8
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Fri Apr 17 08:36:50 2009 +0200

    block: make blk_abort_queue() ignore non-request based devices
    
    There's nothing to do for those devices, since the timeout handling is
    based on requests.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index bbbdc4b8ccf2..8f570c4c80ee 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -211,6 +211,12 @@ void blk_abort_queue(struct request_queue *q)
 	struct request *rq, *tmp;
 	LIST_HEAD(list);
 
+	/*
+	 * Not a request based block device, nothing to abort
+	 */
+	if (!q->request_fn)
+		return;
+
 	spin_lock_irqsave(q->queue_lock, flags);
 
 	elv_abort_queue(q);

commit be987fdb55a4726e2fcbab7501f89276bdb57288
Author: Hannes Reinecke <hare@suse.de>
Date:   Wed Feb 18 10:30:15 2009 +0100

    block: fix deadlock in blk_abort_queue() for drivers that readd to timeout list
    
    blk_abort_queue() iterates the timeout list and aborts each request on the
    list, but if the driver error handling readds a request to the timeout list
    during this processing, we could be looping forever. Fix this by splicing
    current entries to a local list and run over that list instead.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index a09535377a94..bbbdc4b8ccf2 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -209,12 +209,19 @@ void blk_abort_queue(struct request_queue *q)
 {
 	unsigned long flags;
 	struct request *rq, *tmp;
+	LIST_HEAD(list);
 
 	spin_lock_irqsave(q->queue_lock, flags);
 
 	elv_abort_queue(q);
 
-	list_for_each_entry_safe(rq, tmp, &q->timeout_list, timeout_list)
+	/*
+	 * Splice entries to local list, to avoid deadlocking if entries
+	 * get readded to the timeout list by error handling
+	 */
+	list_splice_init(&q->timeout_list, &list);
+
+	list_for_each_entry_safe(rq, tmp, &list, timeout_list)
 		blk_abort_request(rq);
 
 	spin_unlock_irqrestore(q->queue_lock, flags);

commit 70ed28b92a786f44750ab64117b03d126dd14656
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Wed Nov 19 14:38:39 2008 +0100

    block: leave the request timeout timer running even on an empty list
    
    For sync IO, we'll often do them serialized. This means we'll be touching
    the queue timer for every IO, as opposed to only occasionally like we
    do for queued IO. Instead of deleting the timer when the last request
    is removed, just let continue running. If a new request comes up soon
    we then don't have to readd the timer again. If no new requests arrive,
    the timer will expire without side effect later.
    
    This improves high iops sync IO by ~1%.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index 99c3efc706b7..a09535377a94 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -73,11 +73,7 @@ ssize_t part_timeout_store(struct device *dev, struct device_attribute *attr,
  */
 void blk_delete_timer(struct request *req)
 {
-	struct request_queue *q = req->q;
-
 	list_del_init(&req->timeout_list);
-	if (list_empty(&q->timeout_list))
-		del_timer(&q->timeout);
 }
 
 static void blk_rq_timed_out(struct request *req)

commit 65d3618ccfe686e8d7b3f01a838d0578182406df
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Thu Oct 30 08:53:02 2008 +0100

    block: add comment in blk_rq_timed_out() about why next can not be 0
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index 116bbf394fb5..99c3efc706b7 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -132,7 +132,12 @@ void blk_rq_timed_out_timer(unsigned long data)
 		}
 	}
 
-	if (next_set && !list_empty(&q->timeout_list))
+	/*
+	 * next can never be 0 here with the list non-empty, since we always
+	 * bump ->deadline to 1 so we can detect if the timer was ever added
+	 * or not. See comment in blk_add_timer()
+	 */
+	if (next)
 		mod_timer(&q->timeout, round_jiffies_up(next));
 
 	spin_unlock_irqrestore(q->queue_lock, flags);

commit 565e411d764eeda006738dfadbccca79d48381e1
Author: malahal@us.ibm.com <malahal@us.ibm.com>
Date:   Thu Oct 30 08:51:58 2008 +0100

    block: optimizations in blk_rq_timed_out_timer()
    
    Now the rq->deadline can't be zero if the request is in the
    timeout_list, so there is no need to have next_set. There is no need to
    access a request's deadline field if blk_rq_timed_out is called on it.
    
    Signed-off-by: Malahal Naineni <malahal@us.ibm.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index 69185ea9fae2..116bbf394fb5 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -111,7 +111,7 @@ static void blk_rq_timed_out(struct request *req)
 void blk_rq_timed_out_timer(unsigned long data)
 {
 	struct request_queue *q = (struct request_queue *) data;
-	unsigned long flags, uninitialized_var(next), next_set = 0;
+	unsigned long flags, next = 0;
 	struct request *rq, *tmp;
 
 	spin_lock_irqsave(q->queue_lock, flags);
@@ -126,12 +126,10 @@ void blk_rq_timed_out_timer(unsigned long data)
 			if (blk_mark_rq_complete(rq))
 				continue;
 			blk_rq_timed_out(rq);
+		} else {
+			if (!next || time_after(next, rq->deadline))
+				next = rq->deadline;
 		}
-		if (!next_set) {
-			next = rq->deadline;
-			next_set = 1;
-		} else if (time_after(next, rq->deadline))
-			next = rq->deadline;
 	}
 
 	if (next_set && !list_empty(&q->timeout_list))

commit 7838c15b8dd18e78a523513749e5b54bda07b0cb
Author: Alan Stern <stern@rowland.harvard.edu>
Date:   Thu Nov 6 08:42:49 2008 +0100

    Block: use round_jiffies_up()
    
    This patch (as1159b) changes the timeout routines in the block core to
    use round_jiffies_up().  There's no point in rounding the timer
    deadline down, since if it expires too early we will have to restart
    it.
    
    The patch also removes some unnecessary tests when a request is
    removed from the queue's timer list.
    
    Signed-off-by: Alan Stern <stern@rowland.harvard.edu>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index 972a63f848fb..69185ea9fae2 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -75,14 +75,7 @@ void blk_delete_timer(struct request *req)
 {
 	struct request_queue *q = req->q;
 
-	/*
-	 * Nothing to detach
-	 */
-	if (!q->rq_timed_out_fn || !req->deadline)
-		return;
-
 	list_del_init(&req->timeout_list);
-
 	if (list_empty(&q->timeout_list))
 		del_timer(&q->timeout);
 }
@@ -142,7 +135,7 @@ void blk_rq_timed_out_timer(unsigned long data)
 	}
 
 	if (next_set && !list_empty(&q->timeout_list))
-		mod_timer(&q->timeout, round_jiffies(next));
+		mod_timer(&q->timeout, round_jiffies_up(next));
 
 	spin_unlock_irqrestore(q->queue_lock, flags);
 }
@@ -198,17 +191,10 @@ void blk_add_timer(struct request *req)
 
 	/*
 	 * If the timer isn't already pending or this timeout is earlier
-	 * than an existing one, modify the timer. Round to next nearest
+	 * than an existing one, modify the timer. Round up to next nearest
 	 * second.
 	 */
-	expiry = round_jiffies(req->deadline);
-
-	/*
-	 * We use ->deadline == 0 to detect whether a timer was added or
-	 * not, so just increase to next jiffy for that specific case
-	 */
-	if (unlikely(!req->deadline))
-		req->deadline = 1;
+	expiry = round_jiffies_up(req->deadline);
 
 	if (!timer_pending(&q->timeout) ||
 	    time_before(expiry, q->timeout.expires))

commit 7ba1fbaa4a478f72fbaf5a56af9c82a77966b4c7
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Tue Sep 16 09:54:11 2008 -0700

    block: use rq complete marking in blk_abort_request()
    
    We cannot abort a request if we raced with the timeout handler already,
    or with the IO completion. So make blk_abort_request() mark the request
    as complete, and only continue if we succeeded.
    
    Found and suggested by Mike Anderson <andmike@linux.vnet.ibm.com>
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index 9b4ad138bb33..972a63f848fb 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -158,6 +158,8 @@ void blk_rq_timed_out_timer(unsigned long data)
  */
 void blk_abort_request(struct request *req)
 {
+	if (blk_mark_rq_complete(req))
+		return;
 	blk_delete_timer(req);
 	blk_rq_timed_out(req);
 }

commit 581d4e28d9195aa8b2231383dbabc288988d615e
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Sun Sep 14 05:56:33 2008 -0700

    block: add fault injection mechanism for faking request timeouts
    
    Only works for the generic request timer handling. Allows one to
    sporadically ignore request completions, thus exercising the timeout
    handling.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index 6e5c781c5af1..9b4ad138bb33 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -4,9 +4,68 @@
 #include <linux/kernel.h>
 #include <linux/module.h>
 #include <linux/blkdev.h>
+#include <linux/fault-inject.h>
 
 #include "blk.h"
 
+#ifdef CONFIG_FAIL_IO_TIMEOUT
+
+static DECLARE_FAULT_ATTR(fail_io_timeout);
+
+static int __init setup_fail_io_timeout(char *str)
+{
+	return setup_fault_attr(&fail_io_timeout, str);
+}
+__setup("fail_io_timeout=", setup_fail_io_timeout);
+
+int blk_should_fake_timeout(struct request_queue *q)
+{
+	if (!test_bit(QUEUE_FLAG_FAIL_IO, &q->queue_flags))
+		return 0;
+
+	return should_fail(&fail_io_timeout, 1);
+}
+
+static int __init fail_io_timeout_debugfs(void)
+{
+	return init_fault_attr_dentries(&fail_io_timeout, "fail_io_timeout");
+}
+
+late_initcall(fail_io_timeout_debugfs);
+
+ssize_t part_timeout_show(struct device *dev, struct device_attribute *attr,
+			  char *buf)
+{
+	struct gendisk *disk = dev_to_disk(dev);
+	int set = test_bit(QUEUE_FLAG_FAIL_IO, &disk->queue->queue_flags);
+
+	return sprintf(buf, "%d\n", set != 0);
+}
+
+ssize_t part_timeout_store(struct device *dev, struct device_attribute *attr,
+			   const char *buf, size_t count)
+{
+	struct gendisk *disk = dev_to_disk(dev);
+	int val;
+
+	if (count) {
+		struct request_queue *q = disk->queue;
+		char *p = (char *) buf;
+
+		val = simple_strtoul(p, &p, 10);
+		spin_lock_irq(q->queue_lock);
+		if (val)
+			queue_flag_set(QUEUE_FLAG_FAIL_IO, q);
+		else
+			queue_flag_clear(QUEUE_FLAG_FAIL_IO, q);
+		spin_unlock_irq(q->queue_lock);
+	}
+
+	return count;
+}
+
+#endif /* CONFIG_FAIL_IO_TIMEOUT */
+
 /*
  * blk_delete_timer - Delete/cancel timer for a given function.
  * @req:	request that we are canceling timer for

commit 11914a53d2ec2974a565311af327b8983d8c820d
Author: Mike Anderson <andmike@linux.vnet.ibm.com>
Date:   Sat Sep 13 20:31:27 2008 +0200

    block: Add interface to abort queued requests
    
    Signed-off-by: Mike Anderson <andmike@linux.vnet.ibm.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index b36d07bf0afb..6e5c781c5af1 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -153,3 +153,25 @@ void blk_add_timer(struct request *req)
 	    time_before(expiry, q->timeout.expires))
 		mod_timer(&q->timeout, expiry);
 }
+
+/**
+ * blk_abort_queue -- Abort all request on given queue
+ * @queue:	pointer to queue
+ *
+ */
+void blk_abort_queue(struct request_queue *q)
+{
+	unsigned long flags;
+	struct request *rq, *tmp;
+
+	spin_lock_irqsave(q->queue_lock, flags);
+
+	elv_abort_queue(q);
+
+	list_for_each_entry_safe(rq, tmp, &q->timeout_list, timeout_list)
+		blk_abort_request(rq);
+
+	spin_unlock_irqrestore(q->queue_lock, flags);
+
+}
+EXPORT_SYMBOL_GPL(blk_abort_queue);

commit 242f9dcb8ba6f68fcd217a119a7648a4f69290e9
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Sun Sep 14 05:55:09 2008 -0700

    block: unify request timeout handling
    
    Right now SCSI and others do their own command timeout handling.
    Move those bits to the block layer.
    
    Instead of having a timer per command, we try to be a bit more clever
    and simply have one per-queue. This avoids the overhead of having to
    tear down and setup a timer for each command, so it will result in a lot
    less timer fiddling.
    
    Signed-off-by: Mike Anderson <andmike@linux.vnet.ibm.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/block/blk-timeout.c b/block/blk-timeout.c
new file mode 100644
index 000000000000..b36d07bf0afb
--- /dev/null
+++ b/block/blk-timeout.c
@@ -0,0 +1,155 @@
+/*
+ * Functions related to generic timeout handling of requests.
+ */
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/blkdev.h>
+
+#include "blk.h"
+
+/*
+ * blk_delete_timer - Delete/cancel timer for a given function.
+ * @req:	request that we are canceling timer for
+ *
+ */
+void blk_delete_timer(struct request *req)
+{
+	struct request_queue *q = req->q;
+
+	/*
+	 * Nothing to detach
+	 */
+	if (!q->rq_timed_out_fn || !req->deadline)
+		return;
+
+	list_del_init(&req->timeout_list);
+
+	if (list_empty(&q->timeout_list))
+		del_timer(&q->timeout);
+}
+
+static void blk_rq_timed_out(struct request *req)
+{
+	struct request_queue *q = req->q;
+	enum blk_eh_timer_return ret;
+
+	ret = q->rq_timed_out_fn(req);
+	switch (ret) {
+	case BLK_EH_HANDLED:
+		__blk_complete_request(req);
+		break;
+	case BLK_EH_RESET_TIMER:
+		blk_clear_rq_complete(req);
+		blk_add_timer(req);
+		break;
+	case BLK_EH_NOT_HANDLED:
+		/*
+		 * LLD handles this for now but in the future
+		 * we can send a request msg to abort the command
+		 * and we can move more of the generic scsi eh code to
+		 * the blk layer.
+		 */
+		break;
+	default:
+		printk(KERN_ERR "block: bad eh return: %d\n", ret);
+		break;
+	}
+}
+
+void blk_rq_timed_out_timer(unsigned long data)
+{
+	struct request_queue *q = (struct request_queue *) data;
+	unsigned long flags, uninitialized_var(next), next_set = 0;
+	struct request *rq, *tmp;
+
+	spin_lock_irqsave(q->queue_lock, flags);
+
+	list_for_each_entry_safe(rq, tmp, &q->timeout_list, timeout_list) {
+		if (time_after_eq(jiffies, rq->deadline)) {
+			list_del_init(&rq->timeout_list);
+
+			/*
+			 * Check if we raced with end io completion
+			 */
+			if (blk_mark_rq_complete(rq))
+				continue;
+			blk_rq_timed_out(rq);
+		}
+		if (!next_set) {
+			next = rq->deadline;
+			next_set = 1;
+		} else if (time_after(next, rq->deadline))
+			next = rq->deadline;
+	}
+
+	if (next_set && !list_empty(&q->timeout_list))
+		mod_timer(&q->timeout, round_jiffies(next));
+
+	spin_unlock_irqrestore(q->queue_lock, flags);
+}
+
+/**
+ * blk_abort_request -- Request request recovery for the specified command
+ * @req:	pointer to the request of interest
+ *
+ * This function requests that the block layer start recovery for the
+ * request by deleting the timer and calling the q's timeout function.
+ * LLDDs who implement their own error recovery MAY ignore the timeout
+ * event if they generated blk_abort_req. Must hold queue lock.
+ */
+void blk_abort_request(struct request *req)
+{
+	blk_delete_timer(req);
+	blk_rq_timed_out(req);
+}
+EXPORT_SYMBOL_GPL(blk_abort_request);
+
+/**
+ * blk_add_timer - Start timeout timer for a single request
+ * @req:	request that is about to start running.
+ *
+ * Notes:
+ *    Each request has its own timer, and as it is added to the queue, we
+ *    set up the timer. When the request completes, we cancel the timer.
+ */
+void blk_add_timer(struct request *req)
+{
+	struct request_queue *q = req->q;
+	unsigned long expiry;
+
+	if (!q->rq_timed_out_fn)
+		return;
+
+	BUG_ON(!list_empty(&req->timeout_list));
+	BUG_ON(test_bit(REQ_ATOM_COMPLETE, &req->atomic_flags));
+
+	if (req->timeout)
+		req->deadline = jiffies + req->timeout;
+	else {
+		req->deadline = jiffies + q->rq_timeout;
+		/*
+		 * Some LLDs, like scsi, peek at the timeout to prevent
+		 * a command from being retried forever.
+		 */
+		req->timeout = q->rq_timeout;
+	}
+	list_add_tail(&req->timeout_list, &q->timeout_list);
+
+	/*
+	 * If the timer isn't already pending or this timeout is earlier
+	 * than an existing one, modify the timer. Round to next nearest
+	 * second.
+	 */
+	expiry = round_jiffies(req->deadline);
+
+	/*
+	 * We use ->deadline == 0 to detect whether a timer was added or
+	 * not, so just increase to next jiffy for that specific case
+	 */
+	if (unlikely(!req->deadline))
+		req->deadline = 1;
+
+	if (!timer_pending(&q->timeout) ||
+	    time_before(expiry, q->timeout.expires))
+		mod_timer(&q->timeout, expiry);
+}
