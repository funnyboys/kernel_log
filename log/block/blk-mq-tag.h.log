commit d94ecfc399715f06da347922e7979c088b1d8834
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Jun 5 19:44:09 2020 +0800

    blk-mq: split out a __blk_mq_get_driver_tag helper
    
    Allocation of the driver tag in the case of using a scheduler shares very
    little code with the "normal" tag allocation.  Split out a new helper to
    streamline this path, and untangle it from the complex normal tag
    allocation.
    
    This way also avoids to fail driver tag allocation because of inactive hctx
    during cpu hotplug, and fixes potential hang risk.
    
    Fixes: bf0beec0607d ("blk-mq: drain I/O when all CPUs in a hctx are offline")
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Tested-by: John Garry <john.garry@huawei.com>
    Cc: Dongli Zhang <dongli.zhang@oracle.com>
    Cc: Hannes Reinecke <hare@suse.de>
    Cc: Daniel Wagner <dwagner@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq-tag.h b/block/blk-mq-tag.h
index d38e48f2a0a4..2e4ef51cdb32 100644
--- a/block/blk-mq-tag.h
+++ b/block/blk-mq-tag.h
@@ -51,6 +51,14 @@ enum {
 	BLK_MQ_TAG_MAX		= BLK_MQ_NO_TAG - 1,
 };
 
+bool __blk_mq_get_driver_tag(struct request *rq);
+static inline bool blk_mq_get_driver_tag(struct request *rq)
+{
+	if (rq->tag != BLK_MQ_NO_TAG)
+		return true;
+	return __blk_mq_get_driver_tag(rq);
+}
+
 extern bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *);
 extern void __blk_mq_tag_idle(struct blk_mq_hw_ctx *);
 

commit 602380d28e28b454683efac41dc4b2862d055d91
Author: Ming Lei <ming.lei@redhat.com>
Date:   Fri May 29 15:53:14 2020 +0200

    blk-mq: add blk_mq_all_tag_iter
    
    Add a new blk_mq_all_tag_iter function to iterate over all allocated
    scheduler tags and driver tags.  This is more flexible than the existing
    blk_mq_all_tag_busy_iter function as it allows the callers to do whatever
    they want on allocated request instead of being limited to started
    requests.
    
    It will be used to implement draining allocated requests on specified
    hctx in this patchset.
    
    [hch: switch from the two booleans to a more readable flags field and
     consolidate the tags iter functions]
    
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Hannes Reinecke <hare@suse.de>
    Reviewed-by: Daniel Wagner <dwagner@suse.de>
    Reviewed-by: Bart van Assche <bvanassche@acm.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq-tag.h b/block/blk-mq-tag.h
index 8a741752af8b..d38e48f2a0a4 100644
--- a/block/blk-mq-tag.h
+++ b/block/blk-mq-tag.h
@@ -34,6 +34,8 @@ extern int blk_mq_tag_update_depth(struct blk_mq_hw_ctx *hctx,
 extern void blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool);
 void blk_mq_queue_tag_busy_iter(struct request_queue *q, busy_iter_fn *fn,
 		void *priv);
+void blk_mq_all_tag_iter(struct blk_mq_tags *tags, busy_tag_iter_fn *fn,
+		void *priv);
 
 static inline struct sbq_wait_state *bt_wait_ptr(struct sbitmap_queue *bt,
 						 struct blk_mq_hw_ctx *hctx)

commit 419c3d5e8012928fbf9a086b07b618146cc9277b
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri May 29 15:53:11 2020 +0200

    blk-mq: rename BLK_MQ_TAG_FAIL to BLK_MQ_NO_TAG
    
    To prepare for wider use of this constant give it a more applicable name.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Hannes Reinecke <hare@suse.de>
    Reviewed-by: Johannes Thumshirn <johannes.thumshirn@wdc.com>
    Reviewed-by: Bart Van Assche <bvanassche@acm.org>
    Reviewed-by: Daniel Wagner <dwagner@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq-tag.h b/block/blk-mq-tag.h
index 2b8321efb682..8a741752af8b 100644
--- a/block/blk-mq-tag.h
+++ b/block/blk-mq-tag.h
@@ -44,9 +44,9 @@ static inline struct sbq_wait_state *bt_wait_ptr(struct sbitmap_queue *bt,
 }
 
 enum {
-	BLK_MQ_TAG_FAIL		= -1U,
+	BLK_MQ_NO_TAG		= -1U,
 	BLK_MQ_TAG_MIN		= 1,
-	BLK_MQ_TAG_MAX		= BLK_MQ_TAG_FAIL - 1,
+	BLK_MQ_TAG_MAX		= BLK_MQ_NO_TAG - 1,
 };
 
 extern bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *);

commit cae740a04b4d6d5166f19ee5faf04ea2a1f34b3d
Author: John Garry <john.garry@huawei.com>
Date:   Wed Feb 26 20:10:15 2020 +0800

    blk-mq: Remove some unused function arguments
    
    The struct blk_mq_hw_ctx pointer argument in blk_mq_put_tag(),
    blk_mq_poll_nsecs(), and blk_mq_poll_hybrid_sleep() is unused, so remove
    it.
    
    Overall obj code size shows a minor reduction, before:
       text    data     bss     dec     hex filename
      27306    1312       0   28618    6fca block/blk-mq.o
       4303     272       0    4575    11df block/blk-mq-tag.o
    
    after:
      27282    1312       0   28594    6fb2 block/blk-mq.o
       4311     272       0    4583    11e7 block/blk-mq-tag.o
    
    Reviewed-by: Johannes Thumshirn <johannes.thumshirn@wdc.com>
    Reviewed-by: Hannes Reinecke <hare@suse.de>
    Signed-off-by: John Garry <john.garry@huawei.com>
    --
    This minor patch had been carried as part of the blk-mq shared tags RFC,
    I'd rather not carry it anymore as it required rebasing, so now or never..
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq-tag.h b/block/blk-mq-tag.h
index 15bc74acb57e..2b8321efb682 100644
--- a/block/blk-mq-tag.h
+++ b/block/blk-mq-tag.h
@@ -26,8 +26,8 @@ extern struct blk_mq_tags *blk_mq_init_tags(unsigned int nr_tags, unsigned int r
 extern void blk_mq_free_tags(struct blk_mq_tags *tags);
 
 extern unsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data);
-extern void blk_mq_put_tag(struct blk_mq_hw_ctx *hctx, struct blk_mq_tags *tags,
-			   struct blk_mq_ctx *ctx, unsigned int tag);
+extern void blk_mq_put_tag(struct blk_mq_tags *tags, struct blk_mq_ctx *ctx,
+			   unsigned int tag);
 extern int blk_mq_tag_update_depth(struct blk_mq_hw_ctx *hctx,
 					struct blk_mq_tags **tags,
 					unsigned int depth, bool can_grow);

commit cb711b91a3c685192f2cabd3735ca3de04694ed8
Author: John Garry <john.garry@huawei.com>
Date:   Thu Nov 14 01:27:21 2019 +0800

    blk-mq: Delete blk_mq_has_free_tags() and blk_mq_can_queue()
    
    These functions are not referenced, so delete them.
    
    Signed-off-by: John Garry <john.garry@huawei.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq-tag.h b/block/blk-mq-tag.h
index 61deab0b5a5a..15bc74acb57e 100644
--- a/block/blk-mq-tag.h
+++ b/block/blk-mq-tag.h
@@ -28,7 +28,6 @@ extern void blk_mq_free_tags(struct blk_mq_tags *tags);
 extern unsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data);
 extern void blk_mq_put_tag(struct blk_mq_hw_ctx *hctx, struct blk_mq_tags *tags,
 			   struct blk_mq_ctx *ctx, unsigned int tag);
-extern bool blk_mq_has_free_tags(struct blk_mq_tags *tags);
 extern int blk_mq_tag_update_depth(struct blk_mq_hw_ctx *hctx,
 					struct blk_mq_tags **tags,
 					unsigned int depth, bool can_grow);

commit e2c5923c349c1738fe8fda980874d93f6fb2e5b6
Merge: abc36be23635 a04b5de5050a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Nov 14 15:32:19 2017 -0800

    Merge branch 'for-4.15/block' of git://git.kernel.dk/linux-block
    
    Pull core block layer updates from Jens Axboe:
     "This is the main pull request for block storage for 4.15-rc1.
    
      Nothing out of the ordinary in here, and no API changes or anything
      like that. Just various new features for drivers, core changes, etc.
      In particular, this pull request contains:
    
       - A patch series from Bart, closing the whole on blk/scsi-mq queue
         quescing.
    
       - A series from Christoph, building towards hidden gendisks (for
         multipath) and ability to move bio chains around.
    
       - NVMe
            - Support for native multipath for NVMe (Christoph).
            - Userspace notifications for AENs (Keith).
            - Command side-effects support (Keith).
            - SGL support (Chaitanya Kulkarni)
            - FC fixes and improvements (James Smart)
            - Lots of fixes and tweaks (Various)
    
       - bcache
            - New maintainer (Michael Lyle)
            - Writeback control improvements (Michael)
            - Various fixes (Coly, Elena, Eric, Liang, et al)
    
       - lightnvm updates, mostly centered around the pblk interface
         (Javier, Hans, and Rakesh).
    
       - Removal of unused bio/bvec kmap atomic interfaces (me, Christoph)
    
       - Writeback series that fix the much discussed hundreds of millions
         of sync-all units. This goes all the way, as discussed previously
         (me).
    
       - Fix for missing wakeup on writeback timer adjustments (Yafang
         Shao).
    
       - Fix laptop mode on blk-mq (me).
    
       - {mq,name} tupple lookup for IO schedulers, allowing us to have
         alias names. This means you can use 'deadline' on both !mq and on
         mq (where it's called mq-deadline). (me).
    
       - blktrace race fix, oopsing on sg load (me).
    
       - blk-mq optimizations (me).
    
       - Obscure waitqueue race fix for kyber (Omar).
    
       - NBD fixes (Josef).
    
       - Disable writeback throttling by default on bfq, like we do on cfq
         (Luca Miccio).
    
       - Series from Ming that enable us to treat flush requests on blk-mq
         like any other request. This is a really nice cleanup.
    
       - Series from Ming that improves merging on blk-mq with schedulers,
         getting us closer to flipping the switch on scsi-mq again.
    
       - BFQ updates (Paolo).
    
       - blk-mq atomic flags memory ordering fixes (Peter Z).
    
       - Loop cgroup support (Shaohua).
    
       - Lots of minor fixes from lots of different folks, both for core and
         driver code"
    
    * 'for-4.15/block' of git://git.kernel.dk/linux-block: (294 commits)
      nvme: fix visibility of "uuid" ns attribute
      blk-mq: fixup some comment typos and lengths
      ide: ide-atapi: fix compile error with defining macro DEBUG
      blk-mq: improve tag waiting setup for non-shared tags
      brd: remove unused brd_mutex
      blk-mq: only run the hardware queue if IO is pending
      block: avoid null pointer dereference on null disk
      fs: guard_bio_eod() needs to consider partitions
      xtensa/simdisk: fix compile error
      nvme: expose subsys attribute to sysfs
      nvme: create 'slaves' and 'holders' entries for hidden controllers
      block: create 'slaves' and 'holders' entries for hidden gendisks
      nvme: also expose the namespace identification sysfs files for mpath nodes
      nvme: implement multipath access to nvme subsystems
      nvme: track shared namespaces
      nvme: introduce a nvme_ns_ids structure
      nvme: track subsystems
      block, nvme: Introduce blk_mq_req_flags_t
      block, scsi: Make SCSI quiesce and resume work reliably
      block: Add the QUEUE_FLAG_PREEMPT_ONLY request queue flag
      ...

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/block/blk-mq-tag.h b/block/blk-mq-tag.h
index 5cb51e53cc03..c190165d92ea 100644
--- a/block/blk-mq-tag.h
+++ b/block/blk-mq-tag.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef INT_BLK_MQ_TAG_H
 #define INT_BLK_MQ_TAG_H
 

commit 5385fa47d883dc7d51de622835427bc2558ed43c
Author: Jens Axboe <axboe@kernel.dk>
Date:   Sun Oct 1 01:26:21 2017 -0600

    blk-mq-tag: kill unused tag enums
    
    We don't have any notion of a tagging cache anymore, and haven't
    for a long time. Kill off the unused enums.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq-tag.h b/block/blk-mq-tag.h
index 5cb51e53cc03..5932a7ac7fc4 100644
--- a/block/blk-mq-tag.h
+++ b/block/blk-mq-tag.h
@@ -43,14 +43,9 @@ static inline struct sbq_wait_state *bt_wait_ptr(struct sbitmap_queue *bt,
 	return sbq_wait_ptr(bt, &hctx->wait_index);
 }
 
-enum {
-	BLK_MQ_TAG_CACHE_MIN	= 1,
-	BLK_MQ_TAG_CACHE_MAX	= 64,
-};
-
 enum {
 	BLK_MQ_TAG_FAIL		= -1U,
-	BLK_MQ_TAG_MIN		= BLK_MQ_TAG_CACHE_MIN,
+	BLK_MQ_TAG_MIN		= 1,
 	BLK_MQ_TAG_MAX		= BLK_MQ_TAG_FAIL - 1,
 };
 

commit 415b806de5576b656f3ff94366589af9a161d0c8
Author: Sagi Grimberg <sagi@grimberg.me>
Date:   Mon Feb 27 10:04:39 2017 -0700

    blk-mq-sched: Allocate sched reserved tags as specified in the original queue tagset
    
    Signed-off-by: Sagi Grimberg <sagi@grimberg.me>
    
    Modified by me to also check at driver tag allocation time if the
    original request was reserved, so we can be sure to allocate a
    properly reserved tag at that point in time, too.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq-tag.h b/block/blk-mq-tag.h
index 63497423c5cd..5cb51e53cc03 100644
--- a/block/blk-mq-tag.h
+++ b/block/blk-mq-tag.h
@@ -85,4 +85,10 @@ static inline void blk_mq_tag_set_rq(struct blk_mq_hw_ctx *hctx,
 	hctx->tags->rqs[tag] = rq;
 }
 
+static inline bool blk_mq_tag_is_reserved(struct blk_mq_tags *tags,
+					  unsigned int tag)
+{
+	return tag < tags->nr_reserved_tags;
+}
+
 #endif

commit d96b37c0af3e4f42928a1361d5cd9f4f8921b4a8
Author: Omar Sandoval <osandov@fb.com>
Date:   Wed Jan 25 08:06:46 2017 -0800

    blk-mq: move tags and sched_tags info from sysfs to debugfs
    
    These are very tied to the blk-mq tag implementation, so exposing them
    to sysfs isn't a great idea. Move the debugging information to debugfs
    and add basic entries for the number of tags and the number of reserved
    tags to sysfs.
    
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq-tag.h b/block/blk-mq-tag.h
index ac22878462e7..63497423c5cd 100644
--- a/block/blk-mq-tag.h
+++ b/block/blk-mq-tag.h
@@ -28,7 +28,6 @@ extern unsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data);
 extern void blk_mq_put_tag(struct blk_mq_hw_ctx *hctx, struct blk_mq_tags *tags,
 			   struct blk_mq_ctx *ctx, unsigned int tag);
 extern bool blk_mq_has_free_tags(struct blk_mq_tags *tags);
-extern ssize_t blk_mq_tag_sysfs_show(struct blk_mq_tags *tags, char *page);
 extern int blk_mq_tag_update_depth(struct blk_mq_hw_ctx *hctx,
 					struct blk_mq_tags **tags,
 					unsigned int depth, bool can_grow);

commit 70f36b6001bf596eb411c4b302e84c4824ae8730
Author: Jens Axboe <axboe@fb.com>
Date:   Thu Jan 19 10:59:07 2017 -0700

    blk-mq: allow resize of scheduler requests
    
    Add support for growing the tags associated with a hardware queue, for
    the scheduler tags. Currently we only support resizing within the
    limits of the original depth, change that so we can grow it as well by
    allocating and replacing the existing scheduler tag set.
    
    This is similar to how we could increase the software queue depth with
    the legacy IO stack and schedulers.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>
    Reviewed-by: Omar Sandoval <osandov@fb.com>

diff --git a/block/blk-mq-tag.h b/block/blk-mq-tag.h
index 41cd15fd1afd..ac22878462e7 100644
--- a/block/blk-mq-tag.h
+++ b/block/blk-mq-tag.h
@@ -29,7 +29,9 @@ extern void blk_mq_put_tag(struct blk_mq_hw_ctx *hctx, struct blk_mq_tags *tags,
 			   struct blk_mq_ctx *ctx, unsigned int tag);
 extern bool blk_mq_has_free_tags(struct blk_mq_tags *tags);
 extern ssize_t blk_mq_tag_sysfs_show(struct blk_mq_tags *tags, char *page);
-extern int blk_mq_tag_update_depth(struct blk_mq_tags *tags, unsigned int depth);
+extern int blk_mq_tag_update_depth(struct blk_mq_hw_ctx *hctx,
+					struct blk_mq_tags **tags,
+					unsigned int depth, bool can_grow);
 extern void blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool);
 void blk_mq_queue_tag_busy_iter(struct request_queue *q, busy_iter_fn *fn,
 		void *priv);

commit 2af8cbe30531eca73c8f3ba277f155fc0020b01a
Author: Jens Axboe <axboe@fb.com>
Date:   Fri Jan 13 14:39:30 2017 -0700

    blk-mq: split tag ->rqs[] into two
    
    This is in preparation for having two sets of tags available. For
    that we need a static index, and a dynamically assignable one.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>
    Reviewed-by: Omar Sandoval <osandov@fb.com>

diff --git a/block/blk-mq-tag.h b/block/blk-mq-tag.h
index 923602dd3bfb..41cd15fd1afd 100644
--- a/block/blk-mq-tag.h
+++ b/block/blk-mq-tag.h
@@ -16,6 +16,7 @@ struct blk_mq_tags {
 	struct sbitmap_queue breserved_tags;
 
 	struct request **rqs;
+	struct request **static_rqs;
 	struct list_head page_list;
 };
 

commit 4941115bef2bc891aa00a2f0edeaf06dc982325a
Author: Jens Axboe <axboe@fb.com>
Date:   Fri Jan 13 08:09:05 2017 -0700

    blk-mq-tag: cleanup the normal/reserved tag allocation
    
    This is in preparation for having another tag set available. Cleanup
    the parameters, and allow passing in of tags for blk_mq_put_tag().
    
    Signed-off-by: Jens Axboe <axboe@fb.com>
    [hch: even more cleanups]
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Omar Sandoval <osandov@fb.com>

diff --git a/block/blk-mq-tag.h b/block/blk-mq-tag.h
index d1662734dc53..923602dd3bfb 100644
--- a/block/blk-mq-tag.h
+++ b/block/blk-mq-tag.h
@@ -24,8 +24,8 @@ extern struct blk_mq_tags *blk_mq_init_tags(unsigned int nr_tags, unsigned int r
 extern void blk_mq_free_tags(struct blk_mq_tags *tags);
 
 extern unsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data);
-extern void blk_mq_put_tag(struct blk_mq_hw_ctx *hctx, struct blk_mq_ctx *ctx,
-			   unsigned int tag);
+extern void blk_mq_put_tag(struct blk_mq_hw_ctx *hctx, struct blk_mq_tags *tags,
+			   struct blk_mq_ctx *ctx, unsigned int tag);
 extern bool blk_mq_has_free_tags(struct blk_mq_tags *tags);
 extern ssize_t blk_mq_tag_sysfs_show(struct blk_mq_tags *tags, char *page);
 extern int blk_mq_tag_update_depth(struct blk_mq_tags *tags, unsigned int depth);

commit 12e3d3cdd975fe986cc5c35f60b1467a8ec20b80
Merge: 48915c2cbc77 8ec2ef2b66ea
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Oct 9 17:29:33 2016 -0700

    Merge branch 'for-4.9/block-irq' of git://git.kernel.dk/linux-block
    
    Pull blk-mq irq/cpu mapping updates from Jens Axboe:
     "This is the block-irq topic branch for 4.9-rc. It's mostly from
      Christoph, and it allows drivers to specify their own mappings, and
      more importantly, to share the blk-mq mappings with the IRQ affinity
      mappings. It's a good step towards making this work better out of the
      box"
    
    * 'for-4.9/block-irq' of git://git.kernel.dk/linux-block:
      blk_mq: linux/blk-mq.h does not include all the headers it depends on
      blk-mq: kill unused blk_mq_create_mq_map()
      blk-mq: get rid of the cpumask in struct blk_mq_tags
      nvme: remove the post_scan callout
      nvme: switch to use pci_alloc_irq_vectors
      blk-mq: provide a default queue mapping for PCI device
      blk-mq: allow the driver to pass in a queue mapping
      blk-mq: remove ->map_queue
      blk-mq: only allocate a single mq_map per tag_set
      blk-mq: don't redistribute hardware queues on a CPU hotplug event

commit 98d95416dbfaf4910caadfb4ddc75e4aacbdff8c
Author: Omar Sandoval <osandov@fb.com>
Date:   Sat Sep 17 01:28:25 2016 -0700

    sbitmap: randomize initial alloc_hint values
    
    In order to get good cache behavior from a sbitmap, we want each CPU to
    stick to its own cacheline(s) as much as possible. This might happen
    naturally as the bitmap gets filled up and the alloc_hint values spread
    out, but we really want this behavior from the start. blk-mq apparently
    intended to do this, but the code to do this was never wired up. Get rid
    of the dead code and make it part of the sbitmap library.
    
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq-tag.h b/block/blk-mq-tag.h
index f90b850ce43d..09f4cc0aaa84 100644
--- a/block/blk-mq-tag.h
+++ b/block/blk-mq-tag.h
@@ -30,7 +30,6 @@ extern void blk_mq_put_tag(struct blk_mq_hw_ctx *hctx, struct blk_mq_ctx *ctx,
 			   unsigned int tag);
 extern bool blk_mq_has_free_tags(struct blk_mq_tags *tags);
 extern ssize_t blk_mq_tag_sysfs_show(struct blk_mq_tags *tags, char *page);
-extern void blk_mq_tag_init_last_tag(struct blk_mq_tags *tags, unsigned int *last_tag);
 extern int blk_mq_tag_update_depth(struct blk_mq_tags *tags, unsigned int depth);
 extern void blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool);
 void blk_mq_queue_tag_busy_iter(struct request_queue *q, busy_iter_fn *fn,

commit f4a644db86669d938c71f19560aebf69d4720d63
Author: Omar Sandoval <osandov@fb.com>
Date:   Sat Sep 17 01:28:24 2016 -0700

    sbitmap: push alloc policy into sbitmap_queue
    
    Again, there's no point in passing this in every time. Make it part of
    struct sbitmap_queue and clean up the API.
    
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq-tag.h b/block/blk-mq-tag.h
index 2b1d52ed82e0..f90b850ce43d 100644
--- a/block/blk-mq-tag.h
+++ b/block/blk-mq-tag.h
@@ -18,7 +18,6 @@ struct blk_mq_tags {
 	struct request **rqs;
 	struct list_head page_list;
 
-	int alloc_policy;
 	cpumask_var_t cpumask;
 };
 

commit 40aabb67464d5aad9ca3d2a5fedee56e2ff45aa0
Author: Omar Sandoval <osandov@fb.com>
Date:   Sat Sep 17 01:28:23 2016 -0700

    sbitmap: push per-cpu last_tag into sbitmap_queue
    
    Allocating your own per-cpu allocation hint separately makes for an
    awkward API. Instead, allocate the per-cpu hint as part of the struct
    sbitmap_queue. There's no point for a struct sbitmap_queue without the
    cache, but you can still use a bare struct sbitmap.
    
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq-tag.h b/block/blk-mq-tag.h
index 3215c08c63cc..2b1d52ed82e0 100644
--- a/block/blk-mq-tag.h
+++ b/block/blk-mq-tag.h
@@ -27,7 +27,8 @@ extern struct blk_mq_tags *blk_mq_init_tags(unsigned int nr_tags, unsigned int r
 extern void blk_mq_free_tags(struct blk_mq_tags *tags);
 
 extern unsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data);
-extern void blk_mq_put_tag(struct blk_mq_hw_ctx *hctx, unsigned int tag, unsigned int *last_tag);
+extern void blk_mq_put_tag(struct blk_mq_hw_ctx *hctx, struct blk_mq_ctx *ctx,
+			   unsigned int tag);
 extern bool blk_mq_has_free_tags(struct blk_mq_tags *tags);
 extern ssize_t blk_mq_tag_sysfs_show(struct blk_mq_tags *tags, char *page);
 extern void blk_mq_tag_init_last_tag(struct blk_mq_tags *tags, unsigned int *last_tag);

commit 88459642cba452630326b9cab1c651e09577d4e4
Author: Omar Sandoval <osandov@fb.com>
Date:   Sat Sep 17 08:38:44 2016 -0600

    blk-mq: abstract tag allocation out into sbitmap library
    
    This is a generally useful data structure, so make it available to
    anyone else who might want to use it. It's also a nice cleanup
    separating the allocation logic from the rest of the tag handling logic.
    
    The code is behind a new Kconfig option, CONFIG_SBITMAP, which is only
    selected by CONFIG_BLOCK for now.
    
    This should be a complete noop functionality-wise.
    
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq-tag.h b/block/blk-mq-tag.h
index d468a79f2c4a..3215c08c63cc 100644
--- a/block/blk-mq-tag.h
+++ b/block/blk-mq-tag.h
@@ -3,31 +3,6 @@
 
 #include "blk-mq.h"
 
-enum {
-	BT_WAIT_QUEUES	= 8,
-	BT_WAIT_BATCH	= 8,
-};
-
-struct bt_wait_state {
-	atomic_t wait_cnt;
-	wait_queue_head_t wait;
-} ____cacheline_aligned_in_smp;
-
-#define TAG_TO_INDEX(bt, tag)	((tag) >> (bt)->bits_per_word)
-#define TAG_TO_BIT(bt, tag)	((tag) & ((1 << (bt)->bits_per_word) - 1))
-
-struct blk_mq_bitmap_tags {
-	unsigned int depth;
-	unsigned int wake_cnt;
-	unsigned int bits_per_word;
-
-	unsigned int map_nr;
-	struct blk_align_bitmap *map;
-
-	atomic_t wake_index;
-	struct bt_wait_state *bs;
-};
-
 /*
  * Tag address space map.
  */
@@ -37,8 +12,8 @@ struct blk_mq_tags {
 
 	atomic_t active_queues;
 
-	struct blk_mq_bitmap_tags bitmap_tags;
-	struct blk_mq_bitmap_tags breserved_tags;
+	struct sbitmap_queue bitmap_tags;
+	struct sbitmap_queue breserved_tags;
 
 	struct request **rqs;
 	struct list_head page_list;
@@ -61,6 +36,14 @@ extern void blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool);
 void blk_mq_queue_tag_busy_iter(struct request_queue *q, busy_iter_fn *fn,
 		void *priv);
 
+static inline struct sbq_wait_state *bt_wait_ptr(struct sbitmap_queue *bt,
+						 struct blk_mq_hw_ctx *hctx)
+{
+	if (!hctx)
+		return &bt->ws[0];
+	return sbq_wait_ptr(bt, &hctx->wait_index);
+}
+
 enum {
 	BLK_MQ_TAG_CACHE_MIN	= 1,
 	BLK_MQ_TAG_CACHE_MAX	= 64,

commit 1b157939f92ae22d10b9d52baaa14f826927f5ff
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Sep 14 16:18:59 2016 +0200

    blk-mq: get rid of the cpumask in struct blk_mq_tags
    
    Unused now that NVMe sets up irq affinity before calling into blk-mq.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Keith Busch <keith.busch@intel.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq-tag.h b/block/blk-mq-tag.h
index d468a79f2c4a..556964134d1c 100644
--- a/block/blk-mq-tag.h
+++ b/block/blk-mq-tag.h
@@ -44,7 +44,6 @@ struct blk_mq_tags {
 	struct list_head page_list;
 
 	int alloc_policy;
-	cpumask_var_t cpumask;
 };
 
 

commit 0bf6cd5b9531bcc29c0a5e504b6ce2984c6fd8d8
Author: Christoph Hellwig <hch@lst.de>
Date:   Sun Sep 27 21:01:51 2015 +0200

    blk-mq: factor out a helper to iterate all tags for a request_queue
    
    And replace the blk_mq_tag_busy_iter with it - the driver use has been
    replaced with a new helper a while ago, and internal to the block we
    only need the new version.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq-tag.h b/block/blk-mq-tag.h
index 9eb2cf4f01cb..d468a79f2c4a 100644
--- a/block/blk-mq-tag.h
+++ b/block/blk-mq-tag.h
@@ -58,6 +58,8 @@ extern ssize_t blk_mq_tag_sysfs_show(struct blk_mq_tags *tags, char *page);
 extern void blk_mq_tag_init_last_tag(struct blk_mq_tags *tags, unsigned int *last_tag);
 extern int blk_mq_tag_update_depth(struct blk_mq_tags *tags, unsigned int depth);
 extern void blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool);
+void blk_mq_queue_tag_busy_iter(struct request_queue *q, busy_iter_fn *fn,
+		void *priv);
 
 enum {
 	BLK_MQ_TAG_CACHE_MIN	= 1,

commit 0048b4837affd153897ed1222283492070027aa9
Author: Ming Lei <ming.lei@canonical.com>
Date:   Sun Aug 9 03:41:51 2015 -0400

    blk-mq: fix race between timeout and freeing request
    
    Inside timeout handler, blk_mq_tag_to_rq() is called
    to retrieve the request from one tag. This way is obviously
    wrong because the request can be freed any time and some
    fiedds of the request can't be trusted, then kernel oops
    might be triggered[1].
    
    Currently wrt. blk_mq_tag_to_rq(), the only special case is
    that the flush request can share same tag with the request
    cloned from, and the two requests can't be active at the same
    time, so this patch fixes the above issue by updating tags->rqs[tag]
    with the active request(either flush rq or the request cloned
    from) of the tag.
    
    Also blk_mq_tag_to_rq() gets much simplified with this patch.
    
    Given blk_mq_tag_to_rq() is mainly for drivers and the caller must
    make sure the request can't be freed, so in bt_for_each() this
    helper is replaced with tags->rqs[tag].
    
    [1] kernel oops log
    [  439.696220] BUG: unable to handle kernel NULL pointer dereference at 0000000000000158^M
    [  439.697162] IP: [<ffffffff812d89ba>] blk_mq_tag_to_rq+0x21/0x6e^M
    [  439.700653] PGD 7ef765067 PUD 7ef764067 PMD 0 ^M
    [  439.700653] Oops: 0000 [#1] PREEMPT SMP DEBUG_PAGEALLOC ^M
    [  439.700653] Dumping ftrace buffer:^M
    [  439.700653]    (ftrace buffer empty)^M
    [  439.700653] Modules linked in: nbd ipv6 kvm_intel kvm serio_raw^M
    [  439.700653] CPU: 6 PID: 2779 Comm: stress-ng-sigfd Not tainted 4.2.0-rc5-next-20150805+ #265^M
    [  439.730500] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS Bochs 01/01/2011^M
    [  439.730500] task: ffff880605308000 ti: ffff88060530c000 task.ti: ffff88060530c000^M
    [  439.730500] RIP: 0010:[<ffffffff812d89ba>]  [<ffffffff812d89ba>] blk_mq_tag_to_rq+0x21/0x6e^M
    [  439.730500] RSP: 0018:ffff880819203da0  EFLAGS: 00010283^M
    [  439.730500] RAX: ffff880811b0e000 RBX: ffff8800bb465f00 RCX: 0000000000000002^M
    [  439.730500] RDX: 0000000000000000 RSI: 0000000000000202 RDI: 0000000000000000^M
    [  439.730500] RBP: ffff880819203db0 R08: 0000000000000002 R09: 0000000000000000^M
    [  439.730500] R10: 0000000000000000 R11: 0000000000000000 R12: 0000000000000202^M
    [  439.730500] R13: ffff880814104800 R14: 0000000000000002 R15: ffff880811a2ea00^M
    [  439.730500] FS:  00007f165b3f5740(0000) GS:ffff880819200000(0000) knlGS:0000000000000000^M
    [  439.730500] CS:  0010 DS: 0000 ES: 0000 CR0: 000000008005003b^M
    [  439.730500] CR2: 0000000000000158 CR3: 00000007ef766000 CR4: 00000000000006e0^M
    [  439.730500] Stack:^M
    [  439.730500]  0000000000000008 ffff8808114eed90 ffff880819203e00 ffffffff812dc104^M
    [  439.755663]  ffff880819203e40 ffffffff812d9f5e 0000020000000000 ffff8808114eed80^M
    [  439.755663] Call Trace:^M
    [  439.755663]  <IRQ> ^M
    [  439.755663]  [<ffffffff812dc104>] bt_for_each+0x6e/0xc8^M
    [  439.755663]  [<ffffffff812d9f5e>] ? blk_mq_rq_timed_out+0x6a/0x6a^M
    [  439.755663]  [<ffffffff812d9f5e>] ? blk_mq_rq_timed_out+0x6a/0x6a^M
    [  439.755663]  [<ffffffff812dc1b3>] blk_mq_tag_busy_iter+0x55/0x5e^M
    [  439.755663]  [<ffffffff812d88b4>] ? blk_mq_bio_to_request+0x38/0x38^M
    [  439.755663]  [<ffffffff812d8911>] blk_mq_rq_timer+0x5d/0xd4^M
    [  439.755663]  [<ffffffff810a3e10>] call_timer_fn+0xf7/0x284^M
    [  439.755663]  [<ffffffff810a3d1e>] ? call_timer_fn+0x5/0x284^M
    [  439.755663]  [<ffffffff812d88b4>] ? blk_mq_bio_to_request+0x38/0x38^M
    [  439.755663]  [<ffffffff810a46d6>] run_timer_softirq+0x1ce/0x1f8^M
    [  439.755663]  [<ffffffff8104c367>] __do_softirq+0x181/0x3a4^M
    [  439.755663]  [<ffffffff8104c76e>] irq_exit+0x40/0x94^M
    [  439.755663]  [<ffffffff81031482>] smp_apic_timer_interrupt+0x33/0x3e^M
    [  439.755663]  [<ffffffff815559a4>] apic_timer_interrupt+0x84/0x90^M
    [  439.755663]  <EOI> ^M
    [  439.755663]  [<ffffffff81554350>] ? _raw_spin_unlock_irq+0x32/0x4a^M
    [  439.755663]  [<ffffffff8106a98b>] finish_task_switch+0xe0/0x163^M
    [  439.755663]  [<ffffffff8106a94d>] ? finish_task_switch+0xa2/0x163^M
    [  439.755663]  [<ffffffff81550066>] __schedule+0x469/0x6cd^M
    [  439.755663]  [<ffffffff8155039b>] schedule+0x82/0x9a^M
    [  439.789267]  [<ffffffff8119b28b>] signalfd_read+0x186/0x49a^M
    [  439.790911]  [<ffffffff8106d86a>] ? wake_up_q+0x47/0x47^M
    [  439.790911]  [<ffffffff811618c2>] __vfs_read+0x28/0x9f^M
    [  439.790911]  [<ffffffff8117a289>] ? __fget_light+0x4d/0x74^M
    [  439.790911]  [<ffffffff811620a7>] vfs_read+0x7a/0xc6^M
    [  439.790911]  [<ffffffff8116292b>] SyS_read+0x49/0x7f^M
    [  439.790911]  [<ffffffff81554c17>] entry_SYSCALL_64_fastpath+0x12/0x6f^M
    [  439.790911] Code: 48 89 e5 e8 a9 b8 e7 ff 5d c3 0f 1f 44 00 00 55 89
    f2 48 89 e5 41 54 41 89 f4 53 48 8b 47 60 48 8b 1c d0 48 8b 7b 30 48 8b
    53 38 <48> 8b 87 58 01 00 00 48 85 c0 75 09 48 8b 97 88 0c 00 00 eb 10
    ^M
    [  439.790911] RIP  [<ffffffff812d89ba>] blk_mq_tag_to_rq+0x21/0x6e^M
    [  439.790911]  RSP <ffff880819203da0>^M
    [  439.790911] CR2: 0000000000000158^M
    [  439.790911] ---[ end trace d40af58949325661 ]---^M
    
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Ming Lei <ming.lei@canonical.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq-tag.h b/block/blk-mq-tag.h
index 75893a34237d..9eb2cf4f01cb 100644
--- a/block/blk-mq-tag.h
+++ b/block/blk-mq-tag.h
@@ -89,4 +89,16 @@ static inline void blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
 	__blk_mq_tag_idle(hctx);
 }
 
+/*
+ * This helper should only be used for flush request to share tag
+ * with the request cloned from, and both the two requests can't be
+ * in flight at the same time. The caller has to make sure the tag
+ * can't be freed.
+ */
+static inline void blk_mq_tag_set_rq(struct blk_mq_hw_ctx *hctx,
+		unsigned int tag, struct request *rq)
+{
+	hctx->tags->rqs[tag] = rq;
+}
+
 #endif

commit f26cdc8536ad50fb802a0445f836b4f94ca09ae7
Author: Keith Busch <keith.busch@intel.com>
Date:   Mon Jun 1 09:29:53 2015 -0600

    blk-mq: Shared tag enhancements
    
    Storage controllers may expose multiple block devices that share hardware
    resources managed by blk-mq. This patch enhances the shared tags so a
    low-level driver can access the shared resources not tied to the unshared
    h/w contexts. This way the LLD can dynamically add and delete disks and
    request queues without having to track all the request_queue hctx's to
    iterate outstanding tags.
    
    Signed-off-by: Keith Busch <keith.busch@intel.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq-tag.h b/block/blk-mq-tag.h
index 90767b370308..75893a34237d 100644
--- a/block/blk-mq-tag.h
+++ b/block/blk-mq-tag.h
@@ -44,6 +44,7 @@ struct blk_mq_tags {
 	struct list_head page_list;
 
 	int alloc_policy;
+	cpumask_var_t cpumask;
 };
 
 

commit 24391c0dc57c3756a219defaa781e68637d6ab7d
Author: Shaohua Li <shli@fb.com>
Date:   Fri Jan 23 14:18:00 2015 -0700

    blk-mq: add tag allocation policy
    
    This is the blk-mq part to support tag allocation policy. The default
    allocation policy isn't changed (though it's not a strict FIFO). The new
    policy is round-robin for libata. But it's a try-best implementation. If
    multiple tasks are competing, the tags returned will be mixed (which is
    unavoidable even with !mq, as requests from different tasks can be
    mixed in queue)
    
    Cc: Jens Axboe <axboe@fb.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Christoph Hellwig <hch@infradead.org>
    Signed-off-by: Shaohua Li <shli@fb.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq-tag.h b/block/blk-mq-tag.h
index a6fa0fc9d41a..90767b370308 100644
--- a/block/blk-mq-tag.h
+++ b/block/blk-mq-tag.h
@@ -42,10 +42,12 @@ struct blk_mq_tags {
 
 	struct request **rqs;
 	struct list_head page_list;
+
+	int alloc_policy;
 };
 
 
-extern struct blk_mq_tags *blk_mq_init_tags(unsigned int nr_tags, unsigned int reserved_tags, int node);
+extern struct blk_mq_tags *blk_mq_init_tags(unsigned int nr_tags, unsigned int reserved_tags, int node, int alloc_policy);
 extern void blk_mq_free_tags(struct blk_mq_tags *tags);
 
 extern unsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data);

commit aed3ea94bdd2ac0a21ed0103d34097e202ee77f6
Author: Jens Axboe <axboe@fb.com>
Date:   Mon Dec 22 14:04:42 2014 -0700

    block: wake up waiters when a queue is marked dying
    
    If it's dying, we can't expect new request to complete and come
    in an wake up other tasks waiting for requests. So after we
    have marked it as dying, wake up everybody currently waiting
    for a request. Once they wake, they will retry their allocation
    and fail appropriately due to the state of the queue.
    
    Tested-by: Keith Busch <keith.busch@intel.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq-tag.h b/block/blk-mq-tag.h
index 6206ed17ef76..a6fa0fc9d41a 100644
--- a/block/blk-mq-tag.h
+++ b/block/blk-mq-tag.h
@@ -54,6 +54,7 @@ extern bool blk_mq_has_free_tags(struct blk_mq_tags *tags);
 extern ssize_t blk_mq_tag_sysfs_show(struct blk_mq_tags *tags, char *page);
 extern void blk_mq_tag_init_last_tag(struct blk_mq_tags *tags, unsigned int *last_tag);
 extern int blk_mq_tag_update_depth(struct blk_mq_tags *tags, unsigned int depth);
+extern void blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool);
 
 enum {
 	BLK_MQ_TAG_CACHE_MIN	= 1,

commit 8537b12034cf1fd3fab3da2c859d71f76846fae9
Author: Alexander Gordeev <agordeev@redhat.com>
Date:   Tue Jun 17 22:12:35 2014 -0700

    blk-mq: bitmap tag: fix races on shared ::wake_index fields
    
    Fix racy updates of shared blk_mq_bitmap_tags::wake_index
    and blk_mq_hw_ctx::wake_index fields.
    
    Cc: Ming Lei <tom.leiming@gmail.com>
    Signed-off-by: Alexander Gordeev <agordeev@redhat.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq-tag.h b/block/blk-mq-tag.h
index 98696a65d4d4..6206ed17ef76 100644
--- a/block/blk-mq-tag.h
+++ b/block/blk-mq-tag.h
@@ -24,7 +24,7 @@ struct blk_mq_bitmap_tags {
 	unsigned int map_nr;
 	struct blk_align_bitmap *map;
 
-	unsigned int wake_index;
+	atomic_t wake_index;
 	struct bt_wait_state *bs;
 };
 

commit cb96a42cc1f50ba1c7b1e9b2343bec80b926107f
Author: Ming Lei <tom.leiming@gmail.com>
Date:   Sun Jun 1 00:43:37 2014 +0800

    blk-mq: fix schedule from atomic context
    
    blk_mq_put_ctx() has to be called before io_schedule() in
    bt_get().
    
    This patch fixes the problem by taking similar approach from
    percpu_ida allocation for the situation.
    
    Signed-off-by: Ming Lei <tom.leiming@gmail.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq-tag.h b/block/blk-mq-tag.h
index c959de58d2a5..98696a65d4d4 100644
--- a/block/blk-mq-tag.h
+++ b/block/blk-mq-tag.h
@@ -48,7 +48,7 @@ struct blk_mq_tags {
 extern struct blk_mq_tags *blk_mq_init_tags(unsigned int nr_tags, unsigned int reserved_tags, int node);
 extern void blk_mq_free_tags(struct blk_mq_tags *tags);
 
-extern unsigned int blk_mq_get_tag(struct blk_mq_hw_ctx *hctx, unsigned int *last_tag, gfp_t gfp, bool reserved);
+extern unsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data);
 extern void blk_mq_put_tag(struct blk_mq_hw_ctx *hctx, unsigned int tag, unsigned int *last_tag);
 extern bool blk_mq_has_free_tags(struct blk_mq_tags *tags);
 extern ssize_t blk_mq_tag_sysfs_show(struct blk_mq_tags *tags, char *page);

commit a3bd77567cae6af700dcd245148befc73fc89a50
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue May 27 20:59:48 2014 +0200

    blk-mq: remove blk_mq_wait_for_tags
    
    The current logic for blocking tag allocation is rather confusing, as we
    first allocated and then free again a tag in blk_mq_wait_for_tags, just
    to attempt a non-blocking allocation and then repeat if someone else
    managed to grab the tag before us.
    
    Instead change blk_mq_alloc_request_pinned to simply do a blocking tag
    allocation itself and use the request we get back from it.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq-tag.h b/block/blk-mq-tag.h
index 2e5e6872d089..c959de58d2a5 100644
--- a/block/blk-mq-tag.h
+++ b/block/blk-mq-tag.h
@@ -49,7 +49,6 @@ extern struct blk_mq_tags *blk_mq_init_tags(unsigned int nr_tags, unsigned int r
 extern void blk_mq_free_tags(struct blk_mq_tags *tags);
 
 extern unsigned int blk_mq_get_tag(struct blk_mq_hw_ctx *hctx, unsigned int *last_tag, gfp_t gfp, bool reserved);
-extern void blk_mq_wait_for_tags(struct blk_mq_hw_ctx *hctx, bool reserved);
 extern void blk_mq_put_tag(struct blk_mq_hw_ctx *hctx, unsigned int tag, unsigned int *last_tag);
 extern bool blk_mq_has_free_tags(struct blk_mq_tags *tags);
 extern ssize_t blk_mq_tag_sysfs_show(struct blk_mq_tags *tags, char *page);

commit edf866b3805c5651bf7d035b72dc0190cb6ff4a7
Author: Sam Bradshaw <sbradshaw@micron.com>
Date:   Fri May 23 13:30:16 2014 -0600

    blk-mq: export blk_mq_tag_busy_iter
    
    Export the blk-mq in-flight tag iterator for driver consumption.
    This is particularly useful in exception paths or SRSI where
    in-flight IOs need to be cancelled and/or reissued. The NVMe driver
    conversion will use this.
    
    Signed-off-by: Sam Bradshaw <sbradshaw@micron.com>
    Signed-off-by: Matias Bjørling <m@bjorling.me>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq-tag.h b/block/blk-mq-tag.h
index e7ff5ceeeb97..2e5e6872d089 100644
--- a/block/blk-mq-tag.h
+++ b/block/blk-mq-tag.h
@@ -51,7 +51,6 @@ extern void blk_mq_free_tags(struct blk_mq_tags *tags);
 extern unsigned int blk_mq_get_tag(struct blk_mq_hw_ctx *hctx, unsigned int *last_tag, gfp_t gfp, bool reserved);
 extern void blk_mq_wait_for_tags(struct blk_mq_hw_ctx *hctx, bool reserved);
 extern void blk_mq_put_tag(struct blk_mq_hw_ctx *hctx, unsigned int tag, unsigned int *last_tag);
-extern void blk_mq_tag_busy_iter(struct blk_mq_tags *tags, void (*fn)(void *data, unsigned long *), void *data);
 extern bool blk_mq_has_free_tags(struct blk_mq_tags *tags);
 extern ssize_t blk_mq_tag_sysfs_show(struct blk_mq_tags *tags, char *page);
 extern void blk_mq_tag_init_last_tag(struct blk_mq_tags *tags, unsigned int *last_tag);

commit e3a2b3f931f59d5284abd13faf8bded726884ffd
Author: Jens Axboe <axboe@fb.com>
Date:   Tue May 20 11:49:02 2014 -0600

    blk-mq: allow changing of queue depth through sysfs
    
    For request_fn based devices, the block layer exports a 'nr_requests'
    file through sysfs to allow adjusting of queue depth on the fly.
    Currently this returns -EINVAL for blk-mq, since it's not wired up.
    Wire this up for blk-mq, so that it now also always dynamic
    adjustments of the allowed queue depth for any given block device
    managed by blk-mq.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq-tag.h b/block/blk-mq-tag.h
index e144f68ec45f..e7ff5ceeeb97 100644
--- a/block/blk-mq-tag.h
+++ b/block/blk-mq-tag.h
@@ -55,6 +55,7 @@ extern void blk_mq_tag_busy_iter(struct blk_mq_tags *tags, void (*fn)(void *data
 extern bool blk_mq_has_free_tags(struct blk_mq_tags *tags);
 extern ssize_t blk_mq_tag_sysfs_show(struct blk_mq_tags *tags, char *page);
 extern void blk_mq_tag_init_last_tag(struct blk_mq_tags *tags, unsigned int *last_tag);
+extern int blk_mq_tag_update_depth(struct blk_mq_tags *tags, unsigned int depth);
 
 enum {
 	BLK_MQ_TAG_CACHE_MIN	= 1,

commit 39a9f97e5ea99e048c4980c23cf197f6e77995cb
Merge: 1429d7c9467e 0d2602ca30e4
Author: Jens Axboe <axboe@fb.com>
Date:   Mon May 19 11:52:35 2014 -0600

    Merge branch 'for-3.16/blk-mq-tagging' into for-3.16/core
    
    Signed-off-by: Jens Axboe <axboe@fb.com>
    
    Conflicts:
            block/blk-mq-tag.c

commit e93ecf602beb8439f0bdcc1fa2cbc1f31fdfb8e2
Author: Jens Axboe <axboe@fb.com>
Date:   Mon May 19 09:17:48 2014 -0600

    blk-mq: move the cache friendly bitmap type of out blk-mq-tag
    
    We will use it for the pending list in blk-mq core as well.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq-tag.h b/block/blk-mq-tag.h
index 7aa9f0665489..9014269f3910 100644
--- a/block/blk-mq-tag.h
+++ b/block/blk-mq-tag.h
@@ -1,6 +1,8 @@
 #ifndef INT_BLK_MQ_TAG_H
 #define INT_BLK_MQ_TAG_H
 
+#include "blk-mq.h"
+
 enum {
 	BT_WAIT_QUEUES	= 8,
 	BT_WAIT_BATCH	= 8,
@@ -14,18 +16,13 @@ struct bt_wait_state {
 #define TAG_TO_INDEX(bt, tag)	((tag) >> (bt)->bits_per_word)
 #define TAG_TO_BIT(bt, tag)	((tag) & ((1 << (bt)->bits_per_word) - 1))
 
-struct blk_mq_bitmap {
-	unsigned long word;
-	unsigned long depth;
-} ____cacheline_aligned_in_smp;
-
 struct blk_mq_bitmap_tags {
 	unsigned int depth;
 	unsigned int wake_cnt;
 	unsigned int bits_per_word;
 
 	unsigned int map_nr;
-	struct blk_mq_bitmap *map;
+	struct blk_align_bitmap *map;
 
 	unsigned int wake_index;
 	struct bt_wait_state *bs;

commit 0d2602ca30e410e84e8bdf05c84ed5688e0a5a44
Author: Jens Axboe <axboe@fb.com>
Date:   Tue May 13 15:10:52 2014 -0600

    blk-mq: improve support for shared tags maps
    
    This adds support for active queue tracking, meaning that the
    blk-mq tagging maintains a count of active users of a tag set.
    This allows us to maintain a notion of fairness between users,
    so that we can distribute the tag depth evenly without starving
    some users while allowing others to try unfair deep queues.
    
    If sharing of a tag set is detected, each hardware queue will
    track the depth of its own queue. And if this exceeds the total
    depth divided by the number of active queues, the user is actively
    throttled down.
    
    The active queue count is done lazily to avoid bouncing that data
    between submitter and completer. Each hardware queue gets marked
    active when it allocates its first tag, and gets marked inactive
    when 1) the last tag is cleared, and 2) the queue timeout grace
    period has passed.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq-tag.h b/block/blk-mq-tag.h
index 7aa9f0665489..0f5ec8b50ef3 100644
--- a/block/blk-mq-tag.h
+++ b/block/blk-mq-tag.h
@@ -38,6 +38,8 @@ struct blk_mq_tags {
 	unsigned int nr_tags;
 	unsigned int nr_reserved_tags;
 
+	atomic_t active_queues;
+
 	struct blk_mq_bitmap_tags bitmap_tags;
 	struct blk_mq_bitmap_tags breserved_tags;
 
@@ -49,9 +51,9 @@ struct blk_mq_tags {
 extern struct blk_mq_tags *blk_mq_init_tags(unsigned int nr_tags, unsigned int reserved_tags, int node);
 extern void blk_mq_free_tags(struct blk_mq_tags *tags);
 
-extern unsigned int blk_mq_get_tag(struct blk_mq_tags *tags, struct blk_mq_hw_ctx *hctx, unsigned int *last_tag, gfp_t gfp, bool reserved);
-extern void blk_mq_wait_for_tags(struct blk_mq_tags *tags, struct blk_mq_hw_ctx *hctx, bool reserved);
-extern void blk_mq_put_tag(struct blk_mq_tags *tags, unsigned int tag, unsigned int *last_tag);
+extern unsigned int blk_mq_get_tag(struct blk_mq_hw_ctx *hctx, unsigned int *last_tag, gfp_t gfp, bool reserved);
+extern void blk_mq_wait_for_tags(struct blk_mq_hw_ctx *hctx, bool reserved);
+extern void blk_mq_put_tag(struct blk_mq_hw_ctx *hctx, unsigned int tag, unsigned int *last_tag);
 extern void blk_mq_tag_busy_iter(struct blk_mq_tags *tags, void (*fn)(void *data, unsigned long *), void *data);
 extern bool blk_mq_has_free_tags(struct blk_mq_tags *tags);
 extern ssize_t blk_mq_tag_sysfs_show(struct blk_mq_tags *tags, char *page);
@@ -68,4 +70,23 @@ enum {
 	BLK_MQ_TAG_MAX		= BLK_MQ_TAG_FAIL - 1,
 };
 
+extern bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *);
+extern void __blk_mq_tag_idle(struct blk_mq_hw_ctx *);
+
+static inline bool blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
+{
+	if (!(hctx->flags & BLK_MQ_F_TAG_SHARED))
+		return false;
+
+	return __blk_mq_tag_busy(hctx);
+}
+
+static inline void blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
+{
+	if (!(hctx->flags & BLK_MQ_F_TAG_SHARED))
+		return;
+
+	__blk_mq_tag_idle(hctx);
+}
+
 #endif

commit 59d13bf5f57ded658c872fa22276f75ab8f12841
Author: Jens Axboe <axboe@fb.com>
Date:   Fri May 9 13:41:15 2014 -0600

    blk-mq: use sparser tag layout for lower queue depth
    
    For best performance, spreading tags over multiple cachelines
    makes the tagging more efficient on multicore systems. But since
    we have 8 * sizeof(unsigned long) tags per cacheline, we don't
    always get a nice spread.
    
    Attempt to spread the tags over at least 4 cachelines, using fewer
    number of bits per unsigned long if we have to. This improves
    tagging performance in setups with 32-128 tags. For higher depths,
    the spread is the same as before (BITS_PER_LONG tags per cacheline).
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq-tag.h b/block/blk-mq-tag.h
index 06d4a2f0f7a0..7aa9f0665489 100644
--- a/block/blk-mq-tag.h
+++ b/block/blk-mq-tag.h
@@ -11,8 +11,8 @@ struct bt_wait_state {
 	wait_queue_head_t wait;
 } ____cacheline_aligned_in_smp;
 
-#define TAG_TO_INDEX(tag)	((tag) / BITS_PER_LONG)
-#define TAG_TO_BIT(tag)		((tag) & (BITS_PER_LONG - 1))
+#define TAG_TO_INDEX(bt, tag)	((tag) >> (bt)->bits_per_word)
+#define TAG_TO_BIT(bt, tag)	((tag) & ((1 << (bt)->bits_per_word) - 1))
 
 struct blk_mq_bitmap {
 	unsigned long word;
@@ -22,9 +22,10 @@ struct blk_mq_bitmap {
 struct blk_mq_bitmap_tags {
 	unsigned int depth;
 	unsigned int wake_cnt;
+	unsigned int bits_per_word;
 
-	struct blk_mq_bitmap *map;
 	unsigned int map_nr;
+	struct blk_mq_bitmap *map;
 
 	unsigned int wake_index;
 	struct bt_wait_state *bs;

commit 4bb659b156996f2993dc16fad71fec9ee070153c
Author: Jens Axboe <axboe@fb.com>
Date:   Fri May 9 09:36:49 2014 -0600

    blk-mq: implement new and more efficient tagging scheme
    
    blk-mq currently uses percpu_ida for tag allocation. But that only
    works well if the ratio between tag space and number of CPUs is
    sufficiently high. For most devices and systems, that is not the
    case. The end result if that we either only utilize the tag space
    partially, or we end up attempting to fully exhaust it and run
    into lots of lock contention with stealing between CPUs. This is
    not optimal.
    
    This new tagging scheme is a hybrid bitmap allocator. It uses
    two tricks to both be SMP friendly and allow full exhaustion
    of the space:
    
    1) We cache the last allocated (or freed) tag on a per blk-mq
       software context basis. This allows us to limit the space
       we have to search. The key element here is not caching it
       in the shared tag structure, otherwise we end up dirtying
       more shared cache lines on each allocate/free operation.
    
    2) The tag space is split into cache line sized groups, and
       each context will start off randomly in that space. Even up
       to full utilization of the space, this divides the tag users
       efficiently into cache line groups, avoiding dirtying the same
       one both between allocators and between allocator and freeer.
    
    This scheme shows drastically better behaviour, both on small
    tag spaces but on large ones as well. It has been tested extensively
    to show better performance for all the cases blk-mq cares about.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq-tag.h b/block/blk-mq-tag.h
index c8e0645ea331..06d4a2f0f7a0 100644
--- a/block/blk-mq-tag.h
+++ b/block/blk-mq-tag.h
@@ -1,7 +1,34 @@
 #ifndef INT_BLK_MQ_TAG_H
 #define INT_BLK_MQ_TAG_H
 
-#include <linux/percpu_ida.h>
+enum {
+	BT_WAIT_QUEUES	= 8,
+	BT_WAIT_BATCH	= 8,
+};
+
+struct bt_wait_state {
+	atomic_t wait_cnt;
+	wait_queue_head_t wait;
+} ____cacheline_aligned_in_smp;
+
+#define TAG_TO_INDEX(tag)	((tag) / BITS_PER_LONG)
+#define TAG_TO_BIT(tag)		((tag) & (BITS_PER_LONG - 1))
+
+struct blk_mq_bitmap {
+	unsigned long word;
+	unsigned long depth;
+} ____cacheline_aligned_in_smp;
+
+struct blk_mq_bitmap_tags {
+	unsigned int depth;
+	unsigned int wake_cnt;
+
+	struct blk_mq_bitmap *map;
+	unsigned int map_nr;
+
+	unsigned int wake_index;
+	struct bt_wait_state *bs;
+};
 
 /*
  * Tag address space map.
@@ -9,11 +36,9 @@
 struct blk_mq_tags {
 	unsigned int nr_tags;
 	unsigned int nr_reserved_tags;
-	unsigned int nr_batch_move;
-	unsigned int nr_max_cache;
 
-	struct percpu_ida free_tags;
-	struct percpu_ida reserved_tags;
+	struct blk_mq_bitmap_tags bitmap_tags;
+	struct blk_mq_bitmap_tags breserved_tags;
 
 	struct request **rqs;
 	struct list_head page_list;
@@ -23,12 +48,13 @@ struct blk_mq_tags {
 extern struct blk_mq_tags *blk_mq_init_tags(unsigned int nr_tags, unsigned int reserved_tags, int node);
 extern void blk_mq_free_tags(struct blk_mq_tags *tags);
 
-extern unsigned int blk_mq_get_tag(struct blk_mq_tags *tags, gfp_t gfp, bool reserved);
-extern void blk_mq_wait_for_tags(struct blk_mq_tags *tags, bool reserved);
-extern void blk_mq_put_tag(struct blk_mq_tags *tags, unsigned int tag);
+extern unsigned int blk_mq_get_tag(struct blk_mq_tags *tags, struct blk_mq_hw_ctx *hctx, unsigned int *last_tag, gfp_t gfp, bool reserved);
+extern void blk_mq_wait_for_tags(struct blk_mq_tags *tags, struct blk_mq_hw_ctx *hctx, bool reserved);
+extern void blk_mq_put_tag(struct blk_mq_tags *tags, unsigned int tag, unsigned int *last_tag);
 extern void blk_mq_tag_busy_iter(struct blk_mq_tags *tags, void (*fn)(void *data, unsigned long *), void *data);
 extern bool blk_mq_has_free_tags(struct blk_mq_tags *tags);
 extern ssize_t blk_mq_tag_sysfs_show(struct blk_mq_tags *tags, char *page);
+extern void blk_mq_tag_init_last_tag(struct blk_mq_tags *tags, unsigned int *last_tag);
 
 enum {
 	BLK_MQ_TAG_CACHE_MIN	= 1,

commit 5810d903fa3459e703ce82a1d45136813c6afad8
Author: Jens Axboe <axboe@fb.com>
Date:   Tue Apr 29 20:49:48 2014 -0600

    blk-mq: fix waiting for reserved tags
    
    blk_mq_wait_for_tags() is only able to wait for "normal" tags,
    not reserved tags. Pass in which one we should attempt to get
    a tag for, so that waiting for reserved tags will work.
    
    Reserved tags are used for internal commands, which are usually
    serialized. Hence no waiting generally takes place, but we should
    ensure that it actually works if users need that functionality.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq-tag.h b/block/blk-mq-tag.h
index b602e3fa66ea..c8e0645ea331 100644
--- a/block/blk-mq-tag.h
+++ b/block/blk-mq-tag.h
@@ -24,7 +24,7 @@ extern struct blk_mq_tags *blk_mq_init_tags(unsigned int nr_tags, unsigned int r
 extern void blk_mq_free_tags(struct blk_mq_tags *tags);
 
 extern unsigned int blk_mq_get_tag(struct blk_mq_tags *tags, gfp_t gfp, bool reserved);
-extern void blk_mq_wait_for_tags(struct blk_mq_tags *tags);
+extern void blk_mq_wait_for_tags(struct blk_mq_tags *tags, bool reserved);
 extern void blk_mq_put_tag(struct blk_mq_tags *tags, unsigned int tag);
 extern void blk_mq_tag_busy_iter(struct blk_mq_tags *tags, void (*fn)(void *data, unsigned long *), void *data);
 extern bool blk_mq_has_free_tags(struct blk_mq_tags *tags);

commit 24d2f90309b23f2cfe016b2aebc5f0d6e01c57fd
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Apr 15 14:14:00 2014 -0600

    blk-mq: split out tag initialization, support shared tags
    
    Add a new blk_mq_tag_set structure that gets set up before we initialize
    the queue.  A single blk_mq_tag_set structure can be shared by multiple
    queues.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    
    Modular export of blk_mq_{alloc,free}_tagset added by me.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-mq-tag.h b/block/blk-mq-tag.h
index 947ba2c6148e..b602e3fa66ea 100644
--- a/block/blk-mq-tag.h
+++ b/block/blk-mq-tag.h
@@ -1,7 +1,24 @@
 #ifndef INT_BLK_MQ_TAG_H
 #define INT_BLK_MQ_TAG_H
 
-struct blk_mq_tags;
+#include <linux/percpu_ida.h>
+
+/*
+ * Tag address space map.
+ */
+struct blk_mq_tags {
+	unsigned int nr_tags;
+	unsigned int nr_reserved_tags;
+	unsigned int nr_batch_move;
+	unsigned int nr_max_cache;
+
+	struct percpu_ida free_tags;
+	struct percpu_ida reserved_tags;
+
+	struct request **rqs;
+	struct list_head page_list;
+};
+
 
 extern struct blk_mq_tags *blk_mq_init_tags(unsigned int nr_tags, unsigned int reserved_tags, int node);
 extern void blk_mq_free_tags(struct blk_mq_tags *tags);

commit 320ae51feed5c2f13664aa05a76bec198967e04d
Author: Jens Axboe <axboe@kernel.dk>
Date:   Thu Oct 24 09:20:05 2013 +0100

    blk-mq: new multi-queue block IO queueing mechanism
    
    Linux currently has two models for block devices:
    
    - The classic request_fn based approach, where drivers use struct
      request units for IO. The block layer provides various helper
      functionalities to let drivers share code, things like tag
      management, timeout handling, queueing, etc.
    
    - The "stacked" approach, where a driver squeezes in between the
      block layer and IO submitter. Since this bypasses the IO stack,
      driver generally have to manage everything themselves.
    
    With drivers being written for new high IOPS devices, the classic
    request_fn based driver doesn't work well enough. The design dates
    back to when both SMP and high IOPS was rare. It has problems with
    scaling to bigger machines, and runs into scaling issues even on
    smaller machines when you have IOPS in the hundreds of thousands
    per device.
    
    The stacked approach is then most often selected as the model
    for the driver. But this means that everybody has to re-invent
    everything, and along with that we get all the problems again
    that the shared approach solved.
    
    This commit introduces blk-mq, block multi queue support. The
    design is centered around per-cpu queues for queueing IO, which
    then funnel down into x number of hardware submission queues.
    We might have a 1:1 mapping between the two, or it might be
    an N:M mapping. That all depends on what the hardware supports.
    
    blk-mq provides various helper functions, which include:
    
    - Scalable support for request tagging. Most devices need to
      be able to uniquely identify a request both in the driver and
      to the hardware. The tagging uses per-cpu caches for freed
      tags, to enable cache hot reuse.
    
    - Timeout handling without tracking request on a per-device
      basis. Basically the driver should be able to get a notification,
      if a request happens to fail.
    
    - Optional support for non 1:1 mappings between issue and
      submission queues. blk-mq can redirect IO completions to the
      desired location.
    
    - Support for per-request payloads. Drivers almost always need
      to associate a request structure with some driver private
      command structure. Drivers can tell blk-mq this at init time,
      and then any request handed to the driver will have the
      required size of memory associated with it.
    
    - Support for merging of IO, and plugging. The stacked model
      gets neither of these. Even for high IOPS devices, merging
      sequential IO reduces per-command overhead and thus
      increases bandwidth.
    
    For now, this is provided as a potential 3rd queueing model, with
    the hope being that, as it matures, it can replace both the classic
    and stacked model. That would get us back to having just 1 real
    model for block devices, leaving the stacked approach to dm/md
    devices (as it was originally intended).
    
    Contributions in this patch from the following people:
    
    Shaohua Li <shli@fusionio.com>
    Alexander Gordeev <agordeev@redhat.com>
    Christoph Hellwig <hch@infradead.org>
    Mike Christie <michaelc@cs.wisc.edu>
    Matias Bjorling <m@bjorling.me>
    Jeff Moyer <jmoyer@redhat.com>
    
    Acked-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-mq-tag.h b/block/blk-mq-tag.h
new file mode 100644
index 000000000000..947ba2c6148e
--- /dev/null
+++ b/block/blk-mq-tag.h
@@ -0,0 +1,27 @@
+#ifndef INT_BLK_MQ_TAG_H
+#define INT_BLK_MQ_TAG_H
+
+struct blk_mq_tags;
+
+extern struct blk_mq_tags *blk_mq_init_tags(unsigned int nr_tags, unsigned int reserved_tags, int node);
+extern void blk_mq_free_tags(struct blk_mq_tags *tags);
+
+extern unsigned int blk_mq_get_tag(struct blk_mq_tags *tags, gfp_t gfp, bool reserved);
+extern void blk_mq_wait_for_tags(struct blk_mq_tags *tags);
+extern void blk_mq_put_tag(struct blk_mq_tags *tags, unsigned int tag);
+extern void blk_mq_tag_busy_iter(struct blk_mq_tags *tags, void (*fn)(void *data, unsigned long *), void *data);
+extern bool blk_mq_has_free_tags(struct blk_mq_tags *tags);
+extern ssize_t blk_mq_tag_sysfs_show(struct blk_mq_tags *tags, char *page);
+
+enum {
+	BLK_MQ_TAG_CACHE_MIN	= 1,
+	BLK_MQ_TAG_CACHE_MAX	= 64,
+};
+
+enum {
+	BLK_MQ_TAG_FAIL		= -1U,
+	BLK_MQ_TAG_MIN		= BLK_MQ_TAG_CACHE_MIN,
+	BLK_MQ_TAG_MAX		= BLK_MQ_TAG_FAIL - 1,
+};
+
+#endif
