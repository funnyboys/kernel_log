commit b5af37ab3a2b143e278340d2c6fa5790d53817e7
Author: Konstantin Khlebnikov <khlebnikov@yandex-team.ru>
Date:   Wed May 27 07:24:16 2020 +0200

    block: add a blk_account_io_merge_bio helper
    
    Move the non-"new_io" branch of blk_account_io_start() into separate
    function.  Fix merge accounting for discards (they were counted as write
    merges).
    
    The new blk_account_io_merge_bio() doesn't call update_io_ticks() unlike
    blk_account_io_start(), as there is no reason for that.
    
    [hch: rebased]
    
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@yandex-team.ru>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-exec.c b/block/blk-exec.c
index e20a852ae432..85324d53d072 100644
--- a/block/blk-exec.c
+++ b/block/blk-exec.c
@@ -55,7 +55,7 @@ void blk_execute_rq_nowait(struct request_queue *q, struct gendisk *bd_disk,
 	rq->rq_disk = bd_disk;
 	rq->end_io = done;
 
-	blk_account_io_start(rq, true);
+	blk_account_io_start(rq);
 
 	/*
 	 * don't check dying flag for MQ because the request won't

commit 48d9b0d43105e0da2b7c135eedd24e51234fb5e4
Author: Logan Gunthorpe <logang@deltatee.com>
Date:   Thu Oct 10 17:36:26 2019 -0600

    block: account statistics for passthrough requests
    
    Presently, passthrough requests are not accounted for because
    blk_do_io_stat() expressly rejects them. Based on some digging
    in the history, this doesn't seem like a concious decision but
    one that evolved from the change from blk_fs_request() to
    blk_rq_is_passthrough().
    
    To support this, call blk_account_io_start() in blk_execute_rq_nowait()
    and remove the passthrough check in blk_do_io_stat().
    
    Link: https://lore.kernel.org/linux-block/20191010100526.GA27209@lst.de/
    Signed-off-by: Logan Gunthorpe <logang@deltatee.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-exec.c b/block/blk-exec.c
index 1db44ca0f4a6..e20a852ae432 100644
--- a/block/blk-exec.c
+++ b/block/blk-exec.c
@@ -55,6 +55,8 @@ void blk_execute_rq_nowait(struct request_queue *q, struct gendisk *bd_disk,
 	rq->rq_disk = bd_disk;
 	rq->end_io = done;
 
+	blk_account_io_start(rq, true);
+
 	/*
 	 * don't check dying flag for MQ because the request won't
 	 * be reused after dying flag is set

commit 3dcf60bcb603f56361abb364a4cd2f69677453f0
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Apr 30 14:42:43 2019 -0400

    block: add SPDX tags to block layer files missing licensing information
    
    Various block layer files do not have any licensing information at all.
    Add SPDX tags for the default kernel GPLv2 license to those.
    
    Reviewed-by: Chaitanya Kulkarni <chaitanya.kulkarni@wdc.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-exec.c b/block/blk-exec.c
index a34b7d918742..1db44ca0f4a6 100644
--- a/block/blk-exec.c
+++ b/block/blk-exec.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * Functions related to setting various queue properties from drivers
  */

commit a1ce35fa49852db60fc6e268038530be533c5b15
Author: Jens Axboe <axboe@kernel.dk>
Date:   Mon Oct 29 10:23:51 2018 -0600

    block: remove dead elevator code
    
    This removes a bunch of core and elevator related code. On the core
    front, we remove anything related to queue running, draining,
    initialization, plugging, and congestions. We also kill anything
    related to request allocation, merging, retrieval, and completion.
    
    Remove any checking for single queue IO schedulers, as they no
    longer exist. This means we can also delete a bunch of code related
    to request issue, adding, completion, etc - and all the SQ related
    ops and helpers.
    
    Also kill the load_default_modules(), as all that did was provide
    for a way to load the default single queue elevator.
    
    Tested-by: Ming Lei <ming.lei@redhat.com>
    Reviewed-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-exec.c b/block/blk-exec.c
index f7b292f12449..a34b7d918742 100644
--- a/block/blk-exec.c
+++ b/block/blk-exec.c
@@ -48,8 +48,6 @@ void blk_execute_rq_nowait(struct request_queue *q, struct gendisk *bd_disk,
 			   struct request *rq, int at_head,
 			   rq_end_io_fn *done)
 {
-	int where = at_head ? ELEVATOR_INSERT_FRONT : ELEVATOR_INSERT_BACK;
-
 	WARN_ON(irqs_disabled());
 	WARN_ON(!blk_rq_is_passthrough(rq));
 
@@ -60,23 +58,7 @@ void blk_execute_rq_nowait(struct request_queue *q, struct gendisk *bd_disk,
 	 * don't check dying flag for MQ because the request won't
 	 * be reused after dying flag is set
 	 */
-	if (q->mq_ops) {
-		blk_mq_sched_insert_request(rq, at_head, true, false);
-		return;
-	}
-
-	spin_lock_irq(q->queue_lock);
-
-	if (unlikely(blk_queue_dying(q))) {
-		rq->rq_flags |= RQF_QUIET;
-		__blk_end_request_all(rq, BLK_STS_IOERR);
-		spin_unlock_irq(q->queue_lock);
-		return;
-	}
-
-	__elv_add_request(q, rq, where);
-	__blk_run_queue(q);
-	spin_unlock_irq(q->queue_lock);
+	blk_mq_sched_insert_request(rq, at_head, true, false);
 }
 EXPORT_SYMBOL_GPL(blk_execute_rq_nowait);
 

commit 9e97d2951a7e6ee6e204f87f6bda4ff754a8cede
Author: Mike Snitzer <snitzer@redhat.com>
Date:   Wed Jan 17 11:25:58 2018 -0500

    blk-mq-sched: remove unused 'can_block' arg from blk_mq_sched_insert_request
    
    After commit:
    
    923218f6166a ("blk-mq: don't allocate driver tag upfront for flush rq")
    
    we no longer use the 'can_block' argument in
    blk_mq_sched_insert_request(). Kill it.
    
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>
    
    Added actual commit message as to why it's being removed.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-exec.c b/block/blk-exec.c
index 5c0f3dc446dc..f7b292f12449 100644
--- a/block/blk-exec.c
+++ b/block/blk-exec.c
@@ -61,7 +61,7 @@ void blk_execute_rq_nowait(struct request_queue *q, struct gendisk *bd_disk,
 	 * be reused after dying flag is set
 	 */
 	if (q->mq_ops) {
-		blk_mq_sched_insert_request(rq, at_head, true, false, false);
+		blk_mq_sched_insert_request(rq, at_head, true, false);
 		return;
 	}
 

commit 2a842acab109f40f0d7d10b38e9ca88390628996
Author: Christoph Hellwig <hch@lst.de>
Date:   Sat Jun 3 09:38:04 2017 +0200

    block: introduce new block status code type
    
    Currently we use nornal Linux errno values in the block layer, and while
    we accept any error a few have overloaded magic meanings.  This patch
    instead introduces a new  blk_status_t value that holds block layer specific
    status codes and explicitly explains their meaning.  Helpers to convert from
    and to the previous special meanings are provided for now, but I suspect
    we want to get rid of them in the long run - those drivers that have a
    errno input (e.g. networking) usually get errnos that don't know about
    the special block layer overloads, and similarly returning them to userspace
    will usually return somethings that strictly speaking isn't correct
    for file system operations, but that's left as an exercise for later.
    
    For now the set of errors is a very limited set that closely corresponds
    to the previous overloaded errno values, but there is some low hanging
    fruite to improve it.
    
    blk_status_t (ab)uses the sparse __bitwise annotations to allow for sparse
    typechecking, so that we can easily catch places passing the wrong values.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-exec.c b/block/blk-exec.c
index a9451e3b8587..5c0f3dc446dc 100644
--- a/block/blk-exec.c
+++ b/block/blk-exec.c
@@ -16,7 +16,7 @@
  * @rq: request to complete
  * @error: end I/O status of the request
  */
-static void blk_end_sync_rq(struct request *rq, int error)
+static void blk_end_sync_rq(struct request *rq, blk_status_t error)
 {
 	struct completion *waiting = rq->end_io_data;
 
@@ -69,7 +69,7 @@ void blk_execute_rq_nowait(struct request_queue *q, struct gendisk *bd_disk,
 
 	if (unlikely(blk_queue_dying(q))) {
 		rq->rq_flags |= RQF_QUIET;
-		__blk_end_request_all(rq, -ENXIO);
+		__blk_end_request_all(rq, BLK_STS_IOERR);
 		spin_unlock_irq(q->queue_lock);
 		return;
 	}

commit caf7df12272118e0274c8353bcfeaf60c7743a47
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Apr 20 16:03:16 2017 +0200

    block: remove the errors field from struct request
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Bart Van Assche <Bart.VanAssche@sandisk.com>
    Acked-by: Roger Pau Monn√© <roger.pau@citrix.com>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-exec.c b/block/blk-exec.c
index afa383248c7c..a9451e3b8587 100644
--- a/block/blk-exec.c
+++ b/block/blk-exec.c
@@ -69,8 +69,7 @@ void blk_execute_rq_nowait(struct request_queue *q, struct gendisk *bd_disk,
 
 	if (unlikely(blk_queue_dying(q))) {
 		rq->rq_flags |= RQF_QUIET;
-		rq->errors = -ENXIO;
-		__blk_end_request_all(rq, rq->errors);
+		__blk_end_request_all(rq, -ENXIO);
 		spin_unlock_irq(q->queue_lock);
 		return;
 	}

commit b7819b9259185dcdcc81eb32182a4dc13d695738
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Apr 20 16:02:55 2017 +0200

    block: remove the blk_execute_rq return value
    
    The function only returns -EIO if rq->errors is non-zero, which is not
    very useful and lets a large number of callers ignore the return value.
    
    Just let the callers figure out their error themselves.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Reviewed-by: Bart Van Assche <Bart.VanAssche@sandisk.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-exec.c b/block/blk-exec.c
index 8cd0e9bc8dc8..afa383248c7c 100644
--- a/block/blk-exec.c
+++ b/block/blk-exec.c
@@ -92,11 +92,10 @@ EXPORT_SYMBOL_GPL(blk_execute_rq_nowait);
  *    Insert a fully prepared request at the back of the I/O scheduler queue
  *    for execution and wait for completion.
  */
-int blk_execute_rq(struct request_queue *q, struct gendisk *bd_disk,
+void blk_execute_rq(struct request_queue *q, struct gendisk *bd_disk,
 		   struct request *rq, int at_head)
 {
 	DECLARE_COMPLETION_ONSTACK(wait);
-	int err = 0;
 	unsigned long hang_check;
 
 	rq->end_io_data = &wait;
@@ -108,10 +107,5 @@ int blk_execute_rq(struct request_queue *q, struct gendisk *bd_disk,
 		while (!wait_for_completion_io_timeout(&wait, hang_check * (HZ/2)));
 	else
 		wait_for_completion_io(&wait);
-
-	if (rq->errors)
-		err = -EIO;
-
-	return err;
 }
 EXPORT_SYMBOL(blk_execute_rq);

commit 57292b58ddb58689e8c3b4c6eadbef10d9ca44dd
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Jan 31 16:57:29 2017 +0100

    block: introduce blk_rq_is_passthrough
    
    This can be used to check for fs vs non-fs requests and basically
    removes all knowledge of BLOCK_PC specific from the block layer,
    as well as preparing for removing the cmd_type field in struct request.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-exec.c b/block/blk-exec.c
index ed51800f4b44..8cd0e9bc8dc8 100644
--- a/block/blk-exec.c
+++ b/block/blk-exec.c
@@ -51,7 +51,7 @@ void blk_execute_rq_nowait(struct request_queue *q, struct gendisk *bd_disk,
 	int where = at_head ? ELEVATOR_INSERT_FRONT : ELEVATOR_INSERT_BACK;
 
 	WARN_ON(irqs_disabled());
-	WARN_ON(rq->cmd_type == REQ_TYPE_FS);
+	WARN_ON(!blk_rq_is_passthrough(rq));
 
 	rq->rq_disk = bd_disk;
 	rq->end_io = done;

commit 82ed4db499b8598f16f8871261bff088d6b0597f
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Jan 27 09:46:29 2017 +0100

    block: split scsi_request out of struct request
    
    And require all drivers that want to support BLOCK_PC to allocate it
    as the first thing of their private data.  To support this the legacy
    IDE and BSG code is switched to set cmd_size on their queues to let
    the block layer allocate the additional space.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-exec.c b/block/blk-exec.c
index ed1f10165268..ed51800f4b44 100644
--- a/block/blk-exec.c
+++ b/block/blk-exec.c
@@ -11,11 +11,6 @@
 #include "blk.h"
 #include "blk-mq-sched.h"
 
-/*
- * for max sense size
- */
-#include <scsi/scsi_cmnd.h>
-
 /**
  * blk_end_sync_rq - executes a completion event on a request
  * @rq: request to complete
@@ -101,16 +96,9 @@ int blk_execute_rq(struct request_queue *q, struct gendisk *bd_disk,
 		   struct request *rq, int at_head)
 {
 	DECLARE_COMPLETION_ONSTACK(wait);
-	char sense[SCSI_SENSE_BUFFERSIZE];
 	int err = 0;
 	unsigned long hang_check;
 
-	if (!rq->sense) {
-		memset(sense, 0, sizeof(sense));
-		rq->sense = sense;
-		rq->sense_len = 0;
-	}
-
 	rq->end_io_data = &wait;
 	blk_execute_rq_nowait(q, bd_disk, rq, at_head, blk_end_sync_rq);
 
@@ -124,11 +112,6 @@ int blk_execute_rq(struct request_queue *q, struct gendisk *bd_disk,
 	if (rq->errors)
 		err = -EIO;
 
-	if (rq->sense == sense)	{
-		rq->sense = NULL;
-		rq->sense_len = 0;
-	}
-
 	return err;
 }
 EXPORT_SYMBOL(blk_execute_rq);

commit bd6737f1ae92e2f1c6e8362efe96dbe7f18fa07d
Author: Jens Axboe <axboe@fb.com>
Date:   Fri Jan 27 01:00:47 2017 -0700

    blk-mq-sched: add flush insertion into blk_mq_sched_insert_request()
    
    Instead of letting the caller check this and handle the details
    of inserting a flush request, put the logic in the scheduler
    insertion function. This fixes direct flush insertion outside
    of the usual make_request_fn calls, like from dm via
    blk_insert_cloned_request().
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-exec.c b/block/blk-exec.c
index 86656fdfa637..ed1f10165268 100644
--- a/block/blk-exec.c
+++ b/block/blk-exec.c
@@ -66,7 +66,7 @@ void blk_execute_rq_nowait(struct request_queue *q, struct gendisk *bd_disk,
 	 * be reused after dying flag is set
 	 */
 	if (q->mq_ops) {
-		blk_mq_sched_insert_request(rq, at_head, true, false);
+		blk_mq_sched_insert_request(rq, at_head, true, false, false);
 		return;
 	}
 

commit bd166ef183c263c5ced656d49ef19c7da4adc774
Author: Jens Axboe <axboe@fb.com>
Date:   Tue Jan 17 06:03:22 2017 -0700

    blk-mq-sched: add framework for MQ capable IO schedulers
    
    This adds a set of hooks that intercepts the blk-mq path of
    allocating/inserting/issuing/completing requests, allowing
    us to develop a scheduler within that framework.
    
    We reuse the existing elevator scheduler API on the registration
    side, but augment that with the scheduler flagging support for
    the blk-mq interfce, and with a separate set of ops hooks for MQ
    devices.
    
    We split driver and scheduler tags, so we can run the scheduling
    independently of device queue depth.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>
    Reviewed-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Reviewed-by: Omar Sandoval <osandov@fb.com>

diff --git a/block/blk-exec.c b/block/blk-exec.c
index 3ecb00a6cf45..86656fdfa637 100644
--- a/block/blk-exec.c
+++ b/block/blk-exec.c
@@ -9,6 +9,7 @@
 #include <linux/sched/sysctl.h>
 
 #include "blk.h"
+#include "blk-mq-sched.h"
 
 /*
  * for max sense size
@@ -65,7 +66,7 @@ void blk_execute_rq_nowait(struct request_queue *q, struct gendisk *bd_disk,
 	 * be reused after dying flag is set
 	 */
 	if (q->mq_ops) {
-		blk_mq_insert_request(rq, at_head, true, false);
+		blk_mq_sched_insert_request(rq, at_head, true, false);
 		return;
 	}
 

commit e806402130c9c494e22c73ae9ead4e79d2a5811c
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Oct 20 15:12:13 2016 +0200

    block: split out request-only flags into a new namespace
    
    A lot of the REQ_* flags are only used on struct requests, and only of
    use to the block layer and a few drivers that dig into struct request
    internals.
    
    This patch adds a new req_flags_t rq_flags field to struct request for
    them, and thus dramatically shrinks the number of common requests.  It
    also removes the unfortunate situation where we have to fit the fields
    from the same enum into 32 bits for struct bio and 64 bits for
    struct request.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Shaun Tancheff <shaun.tancheff@seagate.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-exec.c b/block/blk-exec.c
index 7ea04325d02f..3ecb00a6cf45 100644
--- a/block/blk-exec.c
+++ b/block/blk-exec.c
@@ -72,7 +72,7 @@ void blk_execute_rq_nowait(struct request_queue *q, struct gendisk *bd_disk,
 	spin_lock_irq(q->queue_lock);
 
 	if (unlikely(blk_queue_dying(q))) {
-		rq->cmd_flags |= REQ_QUIET; 
+		rq->rq_flags |= RQF_QUIET;
 		rq->errors = -ENXIO;
 		__blk_end_request_all(rq, rq->errors);
 		spin_unlock_irq(q->queue_lock);

commit 68bdf1ac2aa6318198adfe12407173bc60248b4e
Author: Bart Van Assche <bart.vanassche@sandisk.com>
Date:   Tue Jul 19 08:18:06 2016 -0700

    block: Fix spelling in a source code comment
    
    Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-exec.c b/block/blk-exec.c
index 3fec8a29d0fa..7ea04325d02f 100644
--- a/block/blk-exec.c
+++ b/block/blk-exec.c
@@ -62,7 +62,7 @@ void blk_execute_rq_nowait(struct request_queue *q, struct gendisk *bd_disk,
 
 	/*
 	 * don't check dying flag for MQ because the request won't
-	 * be resued after dying flag is set
+	 * be reused after dying flag is set
 	 */
 	if (q->mq_ops) {
 		blk_mq_insert_request(rq, at_head, true, false);

commit a7928c1578c550bd6f4dec62d65132e6db226c57
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Apr 17 22:37:20 2015 +0200

    block: move PM request support to IDE
    
    This removes the request types and hacks from the block code and into the
    old IDE driver.  There is a small amunt of code duplication due to this,
    but it's not too bad.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-exec.c b/block/blk-exec.c
index 9924725fa50d..3fec8a29d0fa 100644
--- a/block/blk-exec.c
+++ b/block/blk-exec.c
@@ -53,7 +53,6 @@ void blk_execute_rq_nowait(struct request_queue *q, struct gendisk *bd_disk,
 			   rq_end_io_fn *done)
 {
 	int where = at_head ? ELEVATOR_INSERT_FRONT : ELEVATOR_INSERT_BACK;
-	bool is_pm_resume;
 
 	WARN_ON(irqs_disabled());
 	WARN_ON(rq->cmd_type == REQ_TYPE_FS);
@@ -70,12 +69,6 @@ void blk_execute_rq_nowait(struct request_queue *q, struct gendisk *bd_disk,
 		return;
 	}
 
-	/*
-	 * need to check this before __blk_run_queue(), because rq can
-	 * be freed before that returns.
-	 */
-	is_pm_resume = rq->cmd_type == REQ_TYPE_PM_RESUME;
-
 	spin_lock_irq(q->queue_lock);
 
 	if (unlikely(blk_queue_dying(q))) {
@@ -88,9 +81,6 @@ void blk_execute_rq_nowait(struct request_queue *q, struct gendisk *bd_disk,
 
 	__elv_add_request(q, rq, where);
 	__blk_run_queue(q);
-	/* the queue is stopped so it won't be run */
-	if (is_pm_resume)
-		__blk_run_queue_uncond(q);
 	spin_unlock_irq(q->queue_lock);
 }
 EXPORT_SYMBOL_GPL(blk_execute_rq_nowait);

commit a57a178a490345c7236b0077b3de005754389ed6
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Sep 16 14:44:07 2014 -0700

    blk-mq: avoid infinite recursion with the FUA flag
    
    We should not insert requests into the flush state machine from
    blk_mq_insert_request.  All incoming flush requests come through
    blk_{m,s}q_make_request and are handled there, while blk_execute_rq_nowait
    should only be called for BLOCK_PC requests.  All other callers
    deal with requests that already went through the flush statemchine
    and shouldn't be reinserted into it.
    
    Reported-by: Robert Elliott  <Elliott@hp.com>
    Debugged-by: Ming Lei <ming.lei@canonical.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-exec.c b/block/blk-exec.c
index f4d27b12c90b..9924725fa50d 100644
--- a/block/blk-exec.c
+++ b/block/blk-exec.c
@@ -56,6 +56,7 @@ void blk_execute_rq_nowait(struct request_queue *q, struct gendisk *bd_disk,
 	bool is_pm_resume;
 
 	WARN_ON(irqs_disabled());
+	WARN_ON(rq->cmd_type == REQ_TYPE_FS);
 
 	rq->rq_disk = bd_disk;
 	rq->end_io = done;

commit de83953f9d710f84c4a162a1d498a73475c07d98
Author: Rickard Strandqvist <rickard_strandqvist@spectrumdigital.se>
Date:   Sat Jun 7 00:37:26 2014 +0200

    block: blk-exec.c: Cleaning up local variable address returnd
    
    Address of local variable assigned to a function parameter
    
    This was partly found using a static code analysis program called cppcheck.
    
    Signed-off-by: Rickard Strandqvist <rickard_strandqvist@spectrumdigital.se>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-exec.c b/block/blk-exec.c
index dbf4502b1d67..f4d27b12c90b 100644
--- a/block/blk-exec.c
+++ b/block/blk-exec.c
@@ -132,6 +132,11 @@ int blk_execute_rq(struct request_queue *q, struct gendisk *bd_disk,
 	if (rq->errors)
 		err = -EIO;
 
+	if (rq->sense == sense)	{
+		rq->sense = NULL;
+		rq->sense_len = 0;
+	}
+
 	return err;
 }
 EXPORT_SYMBOL(blk_execute_rq);

commit feb71dae1f9e0aeb056f7f639a21e620d327fc66
Author: Christoph Hellwig <hch@infradead.org>
Date:   Thu Feb 20 15:32:37 2014 -0800

    blk-mq: merge blk_mq_insert_request and blk_mq_run_request
    
    It's almost identical to blk_mq_insert_request, so fold the two into one
    slightly more generic function by making the flush special case a bit
    smarted.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-exec.c b/block/blk-exec.c
index c68613bb4c79..dbf4502b1d67 100644
--- a/block/blk-exec.c
+++ b/block/blk-exec.c
@@ -65,7 +65,7 @@ void blk_execute_rq_nowait(struct request_queue *q, struct gendisk *bd_disk,
 	 * be resued after dying flag is set
 	 */
 	if (q->mq_ops) {
-		blk_mq_insert_request(q, rq, at_head, true);
+		blk_mq_insert_request(rq, at_head, true, false);
 		return;
 	}
 

commit 72a0a36e2854a6eadb4cf2561858f613f9cd4639
Author: Christoph Hellwig <hch@infradead.org>
Date:   Fri Feb 7 10:22:36 2014 -0800

    blk-mq: support at_head inserations for blk_execute_rq
    
    This is neede for proper SG_IO operation as well as various uses of
    blk_execute_rq from the SCSI midlayer.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/block/blk-exec.c b/block/blk-exec.c
index bbfc072a79c2..c68613bb4c79 100644
--- a/block/blk-exec.c
+++ b/block/blk-exec.c
@@ -65,7 +65,7 @@ void blk_execute_rq_nowait(struct request_queue *q, struct gendisk *bd_disk,
 	 * be resued after dying flag is set
 	 */
 	if (q->mq_ops) {
-		blk_mq_insert_request(q, rq, true);
+		blk_mq_insert_request(q, rq, at_head, true);
 		return;
 	}
 

commit 43a5e4e21964a6efb4d14a34644ec7109d0ae891
Author: Ming Lei <tom.leiming@gmail.com>
Date:   Thu Dec 26 21:31:35 2013 +0800

    block: blk-mq: support draining mq queue
    
    blk_mq_drain_queue() is introduced so that we can drain
    mq queue inside blk_cleanup_queue().
    
    Also don't accept new requests any more if queue is marked
    as dying.
    
    Cc: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Ming Lei <tom.leiming@gmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-exec.c b/block/blk-exec.c
index c3edf9dff566..bbfc072a79c2 100644
--- a/block/blk-exec.c
+++ b/block/blk-exec.c
@@ -60,6 +60,10 @@ void blk_execute_rq_nowait(struct request_queue *q, struct gendisk *bd_disk,
 	rq->rq_disk = bd_disk;
 	rq->end_io = done;
 
+	/*
+	 * don't check dying flag for MQ because the request won't
+	 * be resued after dying flag is set
+	 */
 	if (q->mq_ops) {
 		blk_mq_insert_request(q, rq, true);
 		return;

commit 320ae51feed5c2f13664aa05a76bec198967e04d
Author: Jens Axboe <axboe@kernel.dk>
Date:   Thu Oct 24 09:20:05 2013 +0100

    blk-mq: new multi-queue block IO queueing mechanism
    
    Linux currently has two models for block devices:
    
    - The classic request_fn based approach, where drivers use struct
      request units for IO. The block layer provides various helper
      functionalities to let drivers share code, things like tag
      management, timeout handling, queueing, etc.
    
    - The "stacked" approach, where a driver squeezes in between the
      block layer and IO submitter. Since this bypasses the IO stack,
      driver generally have to manage everything themselves.
    
    With drivers being written for new high IOPS devices, the classic
    request_fn based driver doesn't work well enough. The design dates
    back to when both SMP and high IOPS was rare. It has problems with
    scaling to bigger machines, and runs into scaling issues even on
    smaller machines when you have IOPS in the hundreds of thousands
    per device.
    
    The stacked approach is then most often selected as the model
    for the driver. But this means that everybody has to re-invent
    everything, and along with that we get all the problems again
    that the shared approach solved.
    
    This commit introduces blk-mq, block multi queue support. The
    design is centered around per-cpu queues for queueing IO, which
    then funnel down into x number of hardware submission queues.
    We might have a 1:1 mapping between the two, or it might be
    an N:M mapping. That all depends on what the hardware supports.
    
    blk-mq provides various helper functions, which include:
    
    - Scalable support for request tagging. Most devices need to
      be able to uniquely identify a request both in the driver and
      to the hardware. The tagging uses per-cpu caches for freed
      tags, to enable cache hot reuse.
    
    - Timeout handling without tracking request on a per-device
      basis. Basically the driver should be able to get a notification,
      if a request happens to fail.
    
    - Optional support for non 1:1 mappings between issue and
      submission queues. blk-mq can redirect IO completions to the
      desired location.
    
    - Support for per-request payloads. Drivers almost always need
      to associate a request structure with some driver private
      command structure. Drivers can tell blk-mq this at init time,
      and then any request handed to the driver will have the
      required size of memory associated with it.
    
    - Support for merging of IO, and plugging. The stacked model
      gets neither of these. Even for high IOPS devices, merging
      sequential IO reduces per-command overhead and thus
      increases bandwidth.
    
    For now, this is provided as a potential 3rd queueing model, with
    the hope being that, as it matures, it can replace both the classic
    and stacked model. That would get us back to having just 1 real
    model for block devices, leaving the stacked approach to dm/md
    devices (as it was originally intended).
    
    Contributions in this patch from the following people:
    
    Shaohua Li <shli@fusionio.com>
    Alexander Gordeev <agordeev@redhat.com>
    Christoph Hellwig <hch@infradead.org>
    Mike Christie <michaelc@cs.wisc.edu>
    Matias Bjorling <m@bjorling.me>
    Jeff Moyer <jmoyer@redhat.com>
    
    Acked-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-exec.c b/block/blk-exec.c
index 6b18d82d91c5..c3edf9dff566 100644
--- a/block/blk-exec.c
+++ b/block/blk-exec.c
@@ -5,6 +5,7 @@
 #include <linux/module.h>
 #include <linux/bio.h>
 #include <linux/blkdev.h>
+#include <linux/blk-mq.h>
 #include <linux/sched/sysctl.h>
 
 #include "blk.h"
@@ -58,6 +59,12 @@ void blk_execute_rq_nowait(struct request_queue *q, struct gendisk *bd_disk,
 
 	rq->rq_disk = bd_disk;
 	rq->end_io = done;
+
+	if (q->mq_ops) {
+		blk_mq_insert_request(q, rq, true);
+		return;
+	}
+
 	/*
 	 * need to check this before __blk_run_queue(), because rq can
 	 * be freed before that returns.

commit 71fe07d040626de7b72244bf6de889c2e0f5aea3
Author: Christoph Hellwig <hch@infradead.org>
Date:   Fri Oct 4 06:49:11 2013 -0700

    block: remove request ref_count
    
    This reference count has been around since before git history, but the only
    place where it's used is in blk_execute_rq, and ther it is entirely useless
    as it is incremented before submitting the request and decremented in the
    end_io handler before waking up the submitter thread.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-exec.c b/block/blk-exec.c
index ae4f27d7944e..6b18d82d91c5 100644
--- a/block/blk-exec.c
+++ b/block/blk-exec.c
@@ -24,7 +24,6 @@ static void blk_end_sync_rq(struct request *rq, int error)
 	struct completion *waiting = rq->end_io_data;
 
 	rq->end_io_data = NULL;
-	__blk_put_request(rq->q, rq);
 
 	/*
 	 * complete last, if this is a stack request the process (and thus
@@ -103,12 +102,6 @@ int blk_execute_rq(struct request_queue *q, struct gendisk *bd_disk,
 	int err = 0;
 	unsigned long hang_check;
 
-	/*
-	 * we need an extra reference to the request, so we can look at
-	 * it after io completion
-	 */
-	rq->ref_count++;
-
 	if (!rq->sense) {
 		memset(sense, 0, sizeof(sense));
 		rq->sense = sense;

commit 7652113c2f508b1c8176640dcd034730fe79bc48
Author: Mike Christie <michaelc@cs.wisc.edu>
Date:   Wed Sep 18 08:33:55 2013 -0600

    If the queue is dying then we only call the rq->end_io callout.
    This leaves bios setup on the request, because the caller assumes when
    the blk_execute_rq_nowait/blk_execute_rq call has completed that
    the rq->bios have been cleaned up.
    
    This patch has blk_execute_rq_nowait use __blk_end_request_all
    to free bios and also call rq->end_io.
    
    Signed-off-by: Mike Christie <michaelc@cs.wisc.edu>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-exec.c b/block/blk-exec.c
index e70621396129..ae4f27d7944e 100644
--- a/block/blk-exec.c
+++ b/block/blk-exec.c
@@ -68,9 +68,9 @@ void blk_execute_rq_nowait(struct request_queue *q, struct gendisk *bd_disk,
 	spin_lock_irq(q->queue_lock);
 
 	if (unlikely(blk_queue_dying(q))) {
+		rq->cmd_flags |= REQ_QUIET; 
 		rq->errors = -ENXIO;
-		if (rq->end_io)
-			rq->end_io(rq, rq->errors);
+		__blk_end_request_all(rq, rq->errors);
 		spin_unlock_irq(q->queue_lock);
 		return;
 	}

commit ee89f81252179dcbf6cd65bd48299f5e52292d88
Merge: 21f3b24da932 de33127d8d3f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Feb 28 12:52:24 2013 -0800

    Merge branch 'for-3.9/core' of git://git.kernel.dk/linux-block
    
    Pull block IO core bits from Jens Axboe:
     "Below are the core block IO bits for 3.9.  It was delayed a few days
      since my workstation kept crashing every 2-8h after pulling it into
      current -git, but turns out it is a bug in the new pstate code (divide
      by zero, will report separately).  In any case, it contains:
    
       - The big cfq/blkcg update from Tejun and and Vivek.
    
       - Additional block and writeback tracepoints from Tejun.
    
       - Improvement of the should sort (based on queues) logic in the plug
         flushing.
    
       - _io() variants of the wait_for_completion() interface, using
         io_schedule() instead of schedule() to contribute to io wait
         properly.
    
       - Various little fixes.
    
      You'll get two trivial merge conflicts, which should be easy enough to
      fix up"
    
    Fix up the trivial conflicts due to hlist traversal cleanups (commit
    b67bfe0d42ca: "hlist: drop the node parameter from iterators").
    
    * 'for-3.9/core' of git://git.kernel.dk/linux-block: (39 commits)
      block: remove redundant check to bd_openers()
      block: use i_size_write() in bd_set_size()
      cfq: fix lock imbalance with failed allocations
      drivers/block/swim3.c: fix null pointer dereference
      block: don't select PERCPU_RWSEM
      block: account iowait time when waiting for completion of IO request
      sched: add wait_for_completion_io[_timeout]
      writeback: add more tracepoints
      block: add block_{touch|dirty}_buffer tracepoint
      buffer: make touch_buffer() an exported function
      block: add @req to bio_{front|back}_merge tracepoints
      block: add missing block_bio_complete() tracepoint
      block: Remove should_sort judgement when flush blk_plug
      block,elevator: use new hashtable implementation
      cfq-iosched: add hierarchical cfq_group statistics
      cfq-iosched: collect stats from dead cfqgs
      cfq-iosched: separate out cfqg_stats_reset() from cfq_pd_reset_stats()
      blkcg: make blkcg_print_blkgs() grab q locks instead of blkcg lock
      block: RCU free request_queue
      blkcg: implement blkg_[rw]stat_recursive_sum() and blkg_[rw]stat_merge()
      ...

commit 5577022f4ed8973762450ebe7fe7ebfd953817db
Author: Vladimir Davydov <vdavydov@parallels.com>
Date:   Thu Feb 14 18:19:59 2013 +0400

    block: account iowait time when waiting for completion of IO request
    
    Using wait_for_completion() for waiting for a IO request to be executed
    results in wrong iowait time accounting. For example, a system having
    the only task doing write() and fdatasync() on a block device can be
    reported being idle instead of iowaiting as it should because
    blkdev_issue_flush() calls wait_for_completion() which in turn calls
    schedule() that does not increment the iowait proc counter and thus does
    not turn on iowait time accounting.
    
    The patch makes block layer use wait_for_completion_io() instead of
    wait_for_completion() where appropriate to account iowait time
    correctly.
    
    Signed-off-by: Vladimir Davydov <vdavydov@parallels.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-exec.c b/block/blk-exec.c
index 74638ec234c8..f634de772b0c 100644
--- a/block/blk-exec.c
+++ b/block/blk-exec.c
@@ -120,9 +120,9 @@ int blk_execute_rq(struct request_queue *q, struct gendisk *bd_disk,
 	/* Prevent hang_check timer from firing at us during very long I/O */
 	hang_check = sysctl_hung_task_timeout_secs;
 	if (hang_check)
-		while (!wait_for_completion_timeout(&wait, hang_check * (HZ/2)));
+		while (!wait_for_completion_io_timeout(&wait, hang_check * (HZ/2)));
 	else
-		wait_for_completion(&wait);
+		wait_for_completion_io(&wait);
 
 	if (rq->errors)
 		err = -EIO;

commit cf4aebc292fac7f34f8345664320e9d4a42ca76c
Author: Clark Williams <williams@redhat.com>
Date:   Thu Feb 7 09:46:59 2013 -0600

    sched: Move sched.h sysctl bits into separate header
    
    Move the sysctl-related bits from include/linux/sched.h into
    a new file: include/linux/sched/sysctl.h. Then update source
    files requiring access to those bits by including the new
    header file.
    
    Signed-off-by: Clark Williams <williams@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/20130207094659.06dced96@riff.lan
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/block/blk-exec.c b/block/blk-exec.c
index 74638ec234c8..c88202f973d9 100644
--- a/block/blk-exec.c
+++ b/block/blk-exec.c
@@ -5,6 +5,7 @@
 #include <linux/module.h>
 #include <linux/bio.h>
 #include <linux/blkdev.h>
+#include <linux/sched/sysctl.h>
 
 #include "blk.h"
 

commit 60da5bf47dd3d301a1d3bd4f0a4b9e29a184515c
Merge: 3c2e81ef344a cbae8d45d61f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Dec 17 08:27:23 2012 -0800

    Merge branch 'for-3.8/core' of git://git.kernel.dk/linux-block
    
    Pull block layer core updates from Jens Axboe:
     "Here are the core block IO bits for 3.8.  The branch contains:
    
       - The final version of the surprise device removal fixups from Bart.
    
       - Don't hide EFI partitions under advanced partition types.  It's
         fairly wide spread these days.  This is especially dangerous for
         systems that have both msdos and efi partition tables, where you
         want to keep them in sync.
    
       - Cleanup of using -1 instead of the proper NUMA_NO_NODE
    
       - Export control of bdi flusher thread CPU mask and default to using
         the home node (if known) from Jeff.
    
       - Export unplug tracepoint for MD.
    
       - Core improvements from Shaohua.  Reinstate the recursive merge, as
         the original bug has been fixed.  Add plugging for discard and also
         fix a problem handling non pow-of-2 discard limits.
    
      There's a trivial merge in block/blk-exec.c due to a fix that went
      into 3.7-rc at a later point than -rc4 where this is based."
    
    * 'for-3.8/core' of git://git.kernel.dk/linux-block:
      block: export block_unplug tracepoint
      block: add plug for blkdev_issue_discard
      block: discard granularity might not be power of 2
      deadline: Allow 0ms deadline latency, increase the read speed
      partitions: enable EFI/GPT support by default
      bsg: Remove unused function bsg_goose_queue()
      block: Make blk_cleanup_queue() wait until request_fn finished
      block: Avoid scheduling delayed work on a dead queue
      block: Avoid that request_fn is invoked on a dead queue
      block: Let blk_drain_queue() caller obtain the queue lock
      block: Rename queue dead flag
      bdi: add a user-tunable cpu_list for the bdi flusher threads
      block: use NUMA_NO_NODE instead of -1
      block: recursive merge requests
      block CFQ: avoid moving request to different queue

commit c246e80d86736312933646896c4157daf511dadc
Author: Bart Van Assche <bvanassche@acm.org>
Date:   Thu Dec 6 14:32:01 2012 +0100

    block: Avoid that request_fn is invoked on a dead queue
    
    A block driver may start cleaning up resources needed by its
    request_fn as soon as blk_cleanup_queue() finished, so request_fn
    must not be invoked after draining finished. This is important
    when blk_run_queue() is invoked without any requests in progress.
    As an example, if blk_drain_queue() and scsi_run_queue() run in
    parallel, blk_drain_queue() may have finished all requests after
    scsi_run_queue() has taken a SCSI device off the starved list but
    before that last function has had a chance to run the queue.
    
    Signed-off-by: Bart Van Assche <bvanassche@acm.org>
    Cc: James Bottomley <JBottomley@Parallels.com>
    Cc: Mike Christie <michaelc@cs.wisc.edu>
    Cc: Chanho Min <chanho.min@lge.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-exec.c b/block/blk-exec.c
index 4aec98df7ba5..1320e74d79b8 100644
--- a/block/blk-exec.c
+++ b/block/blk-exec.c
@@ -72,7 +72,7 @@ void blk_execute_rq_nowait(struct request_queue *q, struct gendisk *bd_disk,
 	__blk_run_queue(q);
 	/* the queue is stopped so it won't be run */
 	if (rq->cmd_type == REQ_TYPE_PM_RESUME)
-		q->request_fn(q);
+		__blk_run_queue_uncond(q);
 	spin_unlock_irq(q->queue_lock);
 }
 EXPORT_SYMBOL_GPL(blk_execute_rq_nowait);

commit 3f3299d5c0268d6cc3f47b446e8aca436e4a5651
Author: Bart Van Assche <bvanassche@acm.org>
Date:   Wed Nov 28 13:42:38 2012 +0100

    block: Rename queue dead flag
    
    QUEUE_FLAG_DEAD is used to indicate that queuing new requests must
    stop. After this flag has been set queue draining starts. However,
    during the queue draining phase it is still safe to invoke the
    queue's request_fn, so QUEUE_FLAG_DYING is a better name for this
    flag.
    
    This patch has been generated by running the following command
    over the kernel source tree:
    
    git grep -lEw 'blk_queue_dead|QUEUE_FLAG_DEAD' |
        xargs sed -i.tmp -e 's/blk_queue_dead/blk_queue_dying/g'      \
            -e 's/QUEUE_FLAG_DEAD/QUEUE_FLAG_DYING/g';                \
    sed -i.tmp -e "s/QUEUE_FLAG_DYING$(printf \\t)*5/QUEUE_FLAG_DYING$(printf \\t)5/g" \
        include/linux/blkdev.h;                                       \
    sed -i.tmp -e 's/ DEAD/ DYING/g' -e 's/dead queue/a dying queue/' \
        -e 's/Dead queue/A dying queue/' block/blk-core.c
    
    Signed-off-by: Bart Van Assche <bvanassche@acm.org>
    Acked-by: Tejun Heo <tj@kernel.org>
    Cc: James Bottomley <JBottomley@Parallels.com>
    Cc: Mike Christie <michaelc@cs.wisc.edu>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Chanho Min <chanho.min@lge.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-exec.c b/block/blk-exec.c
index 8b6dc5bd4dd0..4aec98df7ba5 100644
--- a/block/blk-exec.c
+++ b/block/blk-exec.c
@@ -60,7 +60,7 @@ void blk_execute_rq_nowait(struct request_queue *q, struct gendisk *bd_disk,
 
 	spin_lock_irq(q->queue_lock);
 
-	if (unlikely(blk_queue_dead(q))) {
+	if (unlikely(blk_queue_dying(q))) {
 		rq->errors = -ENXIO;
 		if (rq->end_io)
 			rq->end_io(rq, rq->errors);

commit 893d290f1d7496db97c9471bc352ad4a11dc8a25
Author: Roland Dreier <roland@purestorage.com>
Date:   Thu Nov 22 02:00:11 2012 -0800

    block: Don't access request after it might be freed
    
    After we've done __elv_add_request() and __blk_run_queue() in
    blk_execute_rq_nowait(), the request might finish and be freed
    immediately.  Therefore checking if the type is REQ_TYPE_PM_RESUME
    isn't safe afterwards, because if it isn't, rq might be gone.
    Instead, check beforehand and stash the result in a temporary.
    
    This fixes crashes in blk_execute_rq_nowait() I get occasionally when
    running with lots of memory debugging options enabled -- I think this
    race is usually harmless because the window for rq to be reallocated
    is so small.
    
    Signed-off-by: Roland Dreier <roland@purestorage.com>
    Cc: stable@kernel.org
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-exec.c b/block/blk-exec.c
index 8b6dc5bd4dd0..f71eac35c1b9 100644
--- a/block/blk-exec.c
+++ b/block/blk-exec.c
@@ -52,11 +52,17 @@ void blk_execute_rq_nowait(struct request_queue *q, struct gendisk *bd_disk,
 			   rq_end_io_fn *done)
 {
 	int where = at_head ? ELEVATOR_INSERT_FRONT : ELEVATOR_INSERT_BACK;
+	bool is_pm_resume;
 
 	WARN_ON(irqs_disabled());
 
 	rq->rq_disk = bd_disk;
 	rq->end_io = done;
+	/*
+	 * need to check this before __blk_run_queue(), because rq can
+	 * be freed before that returns.
+	 */
+	is_pm_resume = rq->cmd_type == REQ_TYPE_PM_RESUME;
 
 	spin_lock_irq(q->queue_lock);
 
@@ -71,7 +77,7 @@ void blk_execute_rq_nowait(struct request_queue *q, struct gendisk *bd_disk,
 	__elv_add_request(q, rq, where);
 	__blk_run_queue(q);
 	/* the queue is stopped so it won't be run */
-	if (rq->cmd_type == REQ_TYPE_PM_RESUME)
+	if (is_pm_resume)
 		q->request_fn(q);
 	spin_unlock_irq(q->queue_lock);
 }

commit e81ca6fe85b77109a32489a5db82f575d51dfc98
Author: Muthukumar Ratty <muthur@gmail.com>
Date:   Fri Jun 29 15:31:49 2012 +0000

    [SCSI] block: Fix blk_execute_rq_nowait() dead queue handling
    
    If the queue is dead blk_execute_rq_nowait() doesn't invoke the done()
    callback function. That will result in blk_execute_rq() being stuck
    in wait_for_completion(). Avoid this by initializing rq->end_io to the
    done() callback before we check the queue state. Also, make sure the
    queue lock is held around the invocation of the done() callback. Found
    this through source code review.
    
    Signed-off-by: Muthukumar Ratty <muthur@gmail.com>
    Signed-off-by: Bart Van Assche <bvanassche@acm.org>
    Reviewed-by: Tejun Heo <tj@kernel.org>
    Acked-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: James Bottomley <JBottomley@Parallels.com>

diff --git a/block/blk-exec.c b/block/blk-exec.c
index fb2cbd551621..8b6dc5bd4dd0 100644
--- a/block/blk-exec.c
+++ b/block/blk-exec.c
@@ -43,6 +43,9 @@ static void blk_end_sync_rq(struct request *rq, int error)
  * Description:
  *    Insert a fully prepared request at the back of the I/O scheduler queue
  *    for execution.  Don't wait for completion.
+ *
+ * Note:
+ *    This function will invoke @done directly if the queue is dead.
  */
 void blk_execute_rq_nowait(struct request_queue *q, struct gendisk *bd_disk,
 			   struct request *rq, int at_head,
@@ -51,18 +54,20 @@ void blk_execute_rq_nowait(struct request_queue *q, struct gendisk *bd_disk,
 	int where = at_head ? ELEVATOR_INSERT_FRONT : ELEVATOR_INSERT_BACK;
 
 	WARN_ON(irqs_disabled());
+
+	rq->rq_disk = bd_disk;
+	rq->end_io = done;
+
 	spin_lock_irq(q->queue_lock);
 
 	if (unlikely(blk_queue_dead(q))) {
-		spin_unlock_irq(q->queue_lock);
 		rq->errors = -ENXIO;
 		if (rq->end_io)
 			rq->end_io(rq, rq->errors);
+		spin_unlock_irq(q->queue_lock);
 		return;
 	}
 
-	rq->rq_disk = bd_disk;
-	rq->end_io = done;
 	__elv_add_request(q, rq, where);
 	__blk_run_queue(q);
 	/* the queue is stopped so it won't be run */

commit 8ba61435d73f2274e12d4d823fde06735e8f6a54
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Dec 14 00:33:37 2011 +0100

    block: add missing blk_queue_dead() checks
    
    blk_insert_cloned_request(), blk_execute_rq_nowait() and
    blk_flush_plug_list() either didn't check whether the queue was dead
    or did it without holding queue_lock.  Update them so that dead state
    is checked while holding queue_lock.
    
    AFAICS, this plugs all holes (requeue doesn't matter as the request is
    transitioning atomically from in_flight to queued).
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-exec.c b/block/blk-exec.c
index 60532852b3ab..fb2cbd551621 100644
--- a/block/blk-exec.c
+++ b/block/blk-exec.c
@@ -50,7 +50,11 @@ void blk_execute_rq_nowait(struct request_queue *q, struct gendisk *bd_disk,
 {
 	int where = at_head ? ELEVATOR_INSERT_FRONT : ELEVATOR_INSERT_BACK;
 
+	WARN_ON(irqs_disabled());
+	spin_lock_irq(q->queue_lock);
+
 	if (unlikely(blk_queue_dead(q))) {
+		spin_unlock_irq(q->queue_lock);
 		rq->errors = -ENXIO;
 		if (rq->end_io)
 			rq->end_io(rq, rq->errors);
@@ -59,8 +63,6 @@ void blk_execute_rq_nowait(struct request_queue *q, struct gendisk *bd_disk,
 
 	rq->rq_disk = bd_disk;
 	rq->end_io = done;
-	WARN_ON(irqs_disabled());
-	spin_lock_irq(q->queue_lock);
 	__elv_add_request(q, rq, where);
 	__blk_run_queue(q);
 	/* the queue is stopped so it won't be run */

commit 34f6055c80285e4efb3f602a9119db75239744dc
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Dec 14 00:33:37 2011 +0100

    block: add blk_queue_dead()
    
    There are a number of QUEUE_FLAG_DEAD tests.  Add blk_queue_dead()
    macro and use it.
    
    This patch doesn't introduce any functional difference.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/block/blk-exec.c b/block/blk-exec.c
index a1ebceb332f9..60532852b3ab 100644
--- a/block/blk-exec.c
+++ b/block/blk-exec.c
@@ -50,7 +50,7 @@ void blk_execute_rq_nowait(struct request_queue *q, struct gendisk *bd_disk,
 {
 	int where = at_head ? ELEVATOR_INSERT_FRONT : ELEVATOR_INSERT_BACK;
 
-	if (unlikely(test_bit(QUEUE_FLAG_DEAD, &q->queue_flags))) {
+	if (unlikely(blk_queue_dead(q))) {
 		rq->errors = -ENXIO;
 		if (rq->end_io)
 			rq->end_io(rq, rq->errors);

commit bfe159a51203c15d23cb3158fffdc25ec4b4dda1
Author: James Bottomley <James.Bottomley@HansenPartnership.com>
Date:   Thu Jul 7 15:45:40 2011 -0500

    [SCSI] fix crash in scsi_dispatch_cmd()
    
    USB surprise removal of sr is triggering an oops in
    scsi_dispatch_command().  What seems to be happening is that USB is
    hanging on to a queue reference until the last close of the upper
    device, so the crash is caused by surprise remove of a mounted CD
    followed by attempted unmount.
    
    The problem is that USB doesn't issue its final commands as part of
    the SCSI teardown path, but on last close when the block queue is long
    gone.  The long term fix is probably to make sr do the teardown in the
    same way as sd (so remove all the lower bits on ejection, but keep the
    upper disk alive until last close of user space).  However, the
    current oops can be simply fixed by not allowing any commands to be
    sent to a dead queue.
    
    Cc: stable@kernel.org
    Signed-off-by: James Bottomley <JBottomley@Parallels.com>

diff --git a/block/blk-exec.c b/block/blk-exec.c
index 8a0e7ec056e7..a1ebceb332f9 100644
--- a/block/blk-exec.c
+++ b/block/blk-exec.c
@@ -50,6 +50,13 @@ void blk_execute_rq_nowait(struct request_queue *q, struct gendisk *bd_disk,
 {
 	int where = at_head ? ELEVATOR_INSERT_FRONT : ELEVATOR_INSERT_BACK;
 
+	if (unlikely(test_bit(QUEUE_FLAG_DEAD, &q->queue_flags))) {
+		rq->errors = -ENXIO;
+		if (rq->end_io)
+			rq->end_io(rq, rq->errors);
+		return;
+	}
+
 	rq->rq_disk = bd_disk;
 	rq->end_io = done;
 	WARN_ON(irqs_disabled());

commit addd0a09fc06179f2e02b4221775d9ab265c9fc7
Author: Tao Ma <boyu.mt@taobao.com>
Date:   Thu May 5 15:10:05 2011 -0600

    block: Remove 'plug/unplug' comment in blk_execute_rq_nowait
    
    unplug is replaced with blk_run_queue now in blk_execute_rq_nowait,
    so change the comment accordingly.
    
    Signed-off-by: Tao Ma <boyu.mt@taobao.com>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/block/blk-exec.c b/block/blk-exec.c
index 81e31819a597..8a0e7ec056e7 100644
--- a/block/blk-exec.c
+++ b/block/blk-exec.c
@@ -56,7 +56,7 @@ void blk_execute_rq_nowait(struct request_queue *q, struct gendisk *bd_disk,
 	spin_lock_irq(q->queue_lock);
 	__elv_add_request(q, rq, where);
 	__blk_run_queue(q);
-	/* the queue is stopped so it won't be plugged+unplugged */
+	/* the queue is stopped so it won't be run */
 	if (rq->cmd_type == REQ_TYPE_PM_RESUME)
 		q->request_fn(q);
 	spin_unlock_irq(q->queue_lock);

commit 24ecfbe27f65563909b14492afda2f1c21f7c044
Author: Christoph Hellwig <hch@infradead.org>
Date:   Mon Apr 18 11:41:33 2011 +0200

    block: add blk_run_queue_async
    
    Instead of overloading __blk_run_queue to force an offload to kblockd
    add a new blk_run_queue_async helper to do it explicitly.  I've kept
    the blk_queue_stopped check for now, but I suspect it's not needed
    as the check we do when the workqueue items runs should be enough.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/block/blk-exec.c b/block/blk-exec.c
index 7482b7fa863b..81e31819a597 100644
--- a/block/blk-exec.c
+++ b/block/blk-exec.c
@@ -55,7 +55,7 @@ void blk_execute_rq_nowait(struct request_queue *q, struct gendisk *bd_disk,
 	WARN_ON(irqs_disabled());
 	spin_lock_irq(q->queue_lock);
 	__elv_add_request(q, rq, where);
-	__blk_run_queue(q, false);
+	__blk_run_queue(q);
 	/* the queue is stopped so it won't be plugged+unplugged */
 	if (rq->cmd_type == REQ_TYPE_PM_RESUME)
 		q->request_fn(q);

commit 4c63f5646e405b5010cc9499419060bf2e838f5b
Merge: cafb0bfca1a7 69d60eb96ae8
Author: Jens Axboe <jaxboe@fusionio.com>
Date:   Thu Mar 10 08:58:35 2011 +0100

    Merge branch 'for-2.6.39/stack-plug' into for-2.6.39/core
    
    Conflicts:
            block/blk-core.c
            block/blk-flush.c
            drivers/md/raid1.c
            drivers/md/raid10.c
            drivers/md/raid5.c
            fs/nilfs2/btnode.c
            fs/nilfs2/mdt.c
    
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

commit 7eaceaccab5f40bbfda044629a6298616aeaed50
Author: Jens Axboe <jaxboe@fusionio.com>
Date:   Thu Mar 10 08:52:07 2011 +0100

    block: remove per-queue plugging
    
    Code has been converted over to the new explicit on-stack plugging,
    and delay users have been converted to use the new API for that.
    So lets kill off the old plugging along with aops->sync_page().
    
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/block/blk-exec.c b/block/blk-exec.c
index cf1456a02acd..81e31819a597 100644
--- a/block/blk-exec.c
+++ b/block/blk-exec.c
@@ -54,8 +54,8 @@ void blk_execute_rq_nowait(struct request_queue *q, struct gendisk *bd_disk,
 	rq->end_io = done;
 	WARN_ON(irqs_disabled());
 	spin_lock_irq(q->queue_lock);
-	__elv_add_request(q, rq, where, 1);
-	__generic_unplug_device(q);
+	__elv_add_request(q, rq, where);
+	__blk_run_queue(q);
 	/* the queue is stopped so it won't be plugged+unplugged */
 	if (rq->cmd_type == REQ_TYPE_PM_RESUME)
 		q->request_fn(q);

commit 4b1977698ceb4c4caa800d475127139da49966f9
Author: Mark Lord <kernel@teksavvy.com>
Date:   Fri Sep 24 09:51:13 2010 -0400

    block: Prevent hang_check firing during long I/O
    
    During long I/O operations, the hang_check timer may fire,
    trigger stack dumps that unnecessarily alarm the user.
    
    Eg.  hdparm --security-erase NULL /dev/sdb  ## can take *hours* to complete
    
    So, if hang_check is armed, we should wake up periodically
    to prevent it from triggering.  This patch uses a wake-up interval
    equal to half the hang_check timer period, which keeps overhead low enough.
    
    Signed-off-by: Mark Lord <mlord@pobox.com>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/block/blk-exec.c b/block/blk-exec.c
index e1672f14840e..cf1456a02acd 100644
--- a/block/blk-exec.c
+++ b/block/blk-exec.c
@@ -80,6 +80,7 @@ int blk_execute_rq(struct request_queue *q, struct gendisk *bd_disk,
 	DECLARE_COMPLETION_ONSTACK(wait);
 	char sense[SCSI_SENSE_BUFFERSIZE];
 	int err = 0;
+	unsigned long hang_check;
 
 	/*
 	 * we need an extra reference to the request, so we can look at
@@ -95,7 +96,13 @@ int blk_execute_rq(struct request_queue *q, struct gendisk *bd_disk,
 
 	rq->end_io_data = &wait;
 	blk_execute_rq_nowait(q, bd_disk, rq, at_head, blk_end_sync_rq);
-	wait_for_completion(&wait);
+
+	/* Prevent hang_check timer from firing at us during very long I/O */
+	hang_check = sysctl_hung_task_timeout_secs;
+	if (hang_check)
+		while (!wait_for_completion_timeout(&wait, hang_check * (HZ/2)));
+	else
+		wait_for_completion(&wait);
 
 	if (rq->errors)
 		err = -EIO;

commit 33659ebbae262228eef4e0fe990f393d1f0ed941
Author: Christoph Hellwig <hch@lst.de>
Date:   Sat Aug 7 18:17:56 2010 +0200

    block: remove wrappers for request type/flags
    
    Remove all the trivial wrappers for the cmd_type and cmd_flags fields in
    struct requests.  This allows much easier grepping for different request
    types instead of unwinding through macros.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/block/blk-exec.c b/block/blk-exec.c
index 49557e91f0da..e1672f14840e 100644
--- a/block/blk-exec.c
+++ b/block/blk-exec.c
@@ -57,7 +57,7 @@ void blk_execute_rq_nowait(struct request_queue *q, struct gendisk *bd_disk,
 	__elv_add_request(q, rq, where, 1);
 	__generic_unplug_device(q);
 	/* the queue is stopped so it won't be plugged+unplugged */
-	if (blk_pm_resume_request(rq))
+	if (rq->cmd_type == REQ_TYPE_PM_RESUME)
 		q->request_fn(q);
 	spin_unlock_irq(q->queue_lock);
 }

commit e4025f6c21f1389696c069be2dc647f364925c45
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Apr 23 11:05:17 2009 +0900

    block: don't set REQ_NOMERGE unnecessarily
    
    RQ_NOMERGE_FLAGS already clears defines which REQ flags aren't
    mergeable.  There is no reason to specify it superflously.  It only
    adds to confusion.  Don't set REQ_NOMERGE for barriers and requests
    with specific queueing directive.  REQ_NOMERGE is now exclusively used
    by the merging code.
    
    [ Impact: cleanup ]
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/block/blk-exec.c b/block/blk-exec.c
index 6af716d1e54e..49557e91f0da 100644
--- a/block/blk-exec.c
+++ b/block/blk-exec.c
@@ -51,7 +51,6 @@ void blk_execute_rq_nowait(struct request_queue *q, struct gendisk *bd_disk,
 	int where = at_head ? ELEVATOR_INSERT_FRONT : ELEVATOR_INSERT_BACK;
 
 	rq->rq_disk = bd_disk;
-	rq->cmd_flags |= REQ_NOMERGE;
 	rq->end_io = done;
 	WARN_ON(irqs_disabled());
 	spin_lock_irq(q->queue_lock);

commit 710027a48ede75428cc68eaa8ae2269b1e356e2c
Author: Randy Dunlap <randy.dunlap@oracle.com>
Date:   Tue Aug 19 20:13:11 2008 +0200

    Add some block/ source files to the kernel-api docbook. Fix kernel-doc notation in them as needed. Fix changed function parameter names. Fix typos/spellos. In comments, change REQ_SPECIAL to REQ_TYPE_SPECIAL and REQ_BLOCK_PC to REQ_TYPE_BLOCK_PC.
    
    Signed-off-by: Randy Dunlap <randy.dunlap@oracle.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/block/blk-exec.c b/block/blk-exec.c
index 9bceff7674f2..6af716d1e54e 100644
--- a/block/blk-exec.c
+++ b/block/blk-exec.c
@@ -16,7 +16,7 @@
 /**
  * blk_end_sync_rq - executes a completion event on a request
  * @rq: request to complete
- * @error: end io status of the request
+ * @error: end I/O status of the request
  */
 static void blk_end_sync_rq(struct request *rq, int error)
 {
@@ -41,7 +41,7 @@ static void blk_end_sync_rq(struct request *rq, int error)
  * @done:	I/O completion handler
  *
  * Description:
- *    Insert a fully prepared request at the back of the io scheduler queue
+ *    Insert a fully prepared request at the back of the I/O scheduler queue
  *    for execution.  Don't wait for completion.
  */
 void blk_execute_rq_nowait(struct request_queue *q, struct gendisk *bd_disk,
@@ -72,7 +72,7 @@ EXPORT_SYMBOL_GPL(blk_execute_rq_nowait);
  * @at_head:    insert request at head or tail of queue
  *
  * Description:
- *    Insert a fully prepared request at the back of the io scheduler queue
+ *    Insert a fully prepared request at the back of the I/O scheduler queue
  *    for execution and wait for completion.
  */
 int blk_execute_rq(struct request_queue *q, struct gendisk *bd_disk,

commit 681a561b7ec7fdcd8f35b68e44ac6d6c70aecc04
Author: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
Date:   Tue Jul 15 21:21:45 2008 +0200

    block: unexport blk_end_sync_rq
    
    All the users of blk_end_sync_rq has gone (they are converted to use
    blk_execute_rq). This unexports blk_end_sync_rq.
    
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Cc: Borislav Petkov <petkovbb@gmail.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>
    Signed-off-by: Bartlomiej Zolnierkiewicz <bzolnier@gmail.com>

diff --git a/block/blk-exec.c b/block/blk-exec.c
index 4f52f2792059..9bceff7674f2 100644
--- a/block/blk-exec.c
+++ b/block/blk-exec.c
@@ -18,7 +18,7 @@
  * @rq: request to complete
  * @error: end io status of the request
  */
-void blk_end_sync_rq(struct request *rq, int error)
+static void blk_end_sync_rq(struct request *rq, int error)
 {
 	struct completion *waiting = rq->end_io_data;
 
@@ -31,7 +31,6 @@ void blk_end_sync_rq(struct request *rq, int error)
 	 */
 	complete(waiting);
 }
-EXPORT_SYMBOL(blk_end_sync_rq);
 
 /**
  * blk_execute_rq_nowait - insert a request into queue for execution

commit 9a2d43b7566caeeeb414aa628bc2759028897dbb
Author: Bartlomiej Zolnierkiewicz <bzolnier@gmail.com>
Date:   Tue Jul 15 21:21:43 2008 +0200

    block: handle blk_pm_resume_request() requests in blk_execute_rq_nowait()
    
    For blk_pm_resume_request() requests (which are used only by IDE subsystem
    currently) the queue is stopped so we need to call ->request_fn explicitly.
    
    Thanks to:
    - Rafael for reporting/bisecting the bug
    - Borislav/Rafael for testing the fix
    
    This is a preparation for converting IDE to use blk_execute_rq().
    
    Cc: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Cc: Borislav Petkov <petkovbb@gmail.com>
    Cc: Jens Axboe <jens.axboe@oracle.com>
    Cc: "Rafael J. Wysocki" <rjw@sisk.pl>
    Signed-off-by: Bartlomiej Zolnierkiewicz <bzolnier@gmail.com>

diff --git a/block/blk-exec.c b/block/blk-exec.c
index 391dd6224890..4f52f2792059 100644
--- a/block/blk-exec.c
+++ b/block/blk-exec.c
@@ -58,6 +58,9 @@ void blk_execute_rq_nowait(struct request_queue *q, struct gendisk *bd_disk,
 	spin_lock_irq(q->queue_lock);
 	__elv_add_request(q, rq, where, 1);
 	__generic_unplug_device(q);
+	/* the queue is stopped so it won't be plugged+unplugged */
+	if (blk_pm_resume_request(rq))
+		q->request_fn(q);
 	spin_unlock_irq(q->queue_lock);
 }
 EXPORT_SYMBOL_GPL(blk_execute_rq_nowait);

commit 6728cb0e6343d4068ccec13f07212e6382d3ff33
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Thu Jan 31 13:03:55 2008 +0100

    block: make core bits checkpatch compliant
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/block/blk-exec.c b/block/blk-exec.c
index ebfb44e959a9..391dd6224890 100644
--- a/block/blk-exec.c
+++ b/block/blk-exec.c
@@ -101,5 +101,4 @@ int blk_execute_rq(struct request_queue *q, struct gendisk *bd_disk,
 
 	return err;
 }
-
 EXPORT_SYMBOL(blk_execute_rq);

commit 86db1e29772372155db08ff48a9ceb76e11a2ad1
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Tue Jan 29 14:53:40 2008 +0100

    block: continue ll_rw_blk.c splitup
    
    Adds files for barrier handling, rq execution, io context handling,
    mapping data to requests, and queue settings.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/block/blk-exec.c b/block/blk-exec.c
new file mode 100644
index 000000000000..ebfb44e959a9
--- /dev/null
+++ b/block/blk-exec.c
@@ -0,0 +1,105 @@
+/*
+ * Functions related to setting various queue properties from drivers
+ */
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/bio.h>
+#include <linux/blkdev.h>
+
+#include "blk.h"
+
+/*
+ * for max sense size
+ */
+#include <scsi/scsi_cmnd.h>
+
+/**
+ * blk_end_sync_rq - executes a completion event on a request
+ * @rq: request to complete
+ * @error: end io status of the request
+ */
+void blk_end_sync_rq(struct request *rq, int error)
+{
+	struct completion *waiting = rq->end_io_data;
+
+	rq->end_io_data = NULL;
+	__blk_put_request(rq->q, rq);
+
+	/*
+	 * complete last, if this is a stack request the process (and thus
+	 * the rq pointer) could be invalid right after this complete()
+	 */
+	complete(waiting);
+}
+EXPORT_SYMBOL(blk_end_sync_rq);
+
+/**
+ * blk_execute_rq_nowait - insert a request into queue for execution
+ * @q:		queue to insert the request in
+ * @bd_disk:	matching gendisk
+ * @rq:		request to insert
+ * @at_head:    insert request at head or tail of queue
+ * @done:	I/O completion handler
+ *
+ * Description:
+ *    Insert a fully prepared request at the back of the io scheduler queue
+ *    for execution.  Don't wait for completion.
+ */
+void blk_execute_rq_nowait(struct request_queue *q, struct gendisk *bd_disk,
+			   struct request *rq, int at_head,
+			   rq_end_io_fn *done)
+{
+	int where = at_head ? ELEVATOR_INSERT_FRONT : ELEVATOR_INSERT_BACK;
+
+	rq->rq_disk = bd_disk;
+	rq->cmd_flags |= REQ_NOMERGE;
+	rq->end_io = done;
+	WARN_ON(irqs_disabled());
+	spin_lock_irq(q->queue_lock);
+	__elv_add_request(q, rq, where, 1);
+	__generic_unplug_device(q);
+	spin_unlock_irq(q->queue_lock);
+}
+EXPORT_SYMBOL_GPL(blk_execute_rq_nowait);
+
+/**
+ * blk_execute_rq - insert a request into queue for execution
+ * @q:		queue to insert the request in
+ * @bd_disk:	matching gendisk
+ * @rq:		request to insert
+ * @at_head:    insert request at head or tail of queue
+ *
+ * Description:
+ *    Insert a fully prepared request at the back of the io scheduler queue
+ *    for execution and wait for completion.
+ */
+int blk_execute_rq(struct request_queue *q, struct gendisk *bd_disk,
+		   struct request *rq, int at_head)
+{
+	DECLARE_COMPLETION_ONSTACK(wait);
+	char sense[SCSI_SENSE_BUFFERSIZE];
+	int err = 0;
+
+	/*
+	 * we need an extra reference to the request, so we can look at
+	 * it after io completion
+	 */
+	rq->ref_count++;
+
+	if (!rq->sense) {
+		memset(sense, 0, sizeof(sense));
+		rq->sense = sense;
+		rq->sense_len = 0;
+	}
+
+	rq->end_io_data = &wait;
+	blk_execute_rq_nowait(q, bd_disk, rq, at_head, blk_end_sync_rq);
+	wait_for_completion(&wait);
+
+	if (rq->errors)
+		err = -EIO;
+
+	return err;
+}
+
+EXPORT_SYMBOL(blk_execute_rq);
