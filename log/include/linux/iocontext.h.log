commit 30a2da7b7e225ef6c87a660419ea04d3cef3f6a7
Author: Sahitya Tummala <stummala@codeaurora.org>
Date:   Wed Mar 11 16:07:50 2020 +0530

    block: Fix use-after-free issue accessing struct io_cq
    
    There is a potential race between ioc_release_fn() and
    ioc_clear_queue() as shown below, due to which below kernel
    crash is observed. It also can result into use-after-free
    issue.
    
    context#1:                              context#2:
    ioc_release_fn()                        __ioc_clear_queue() gets the same icq
    ->spin_lock(&ioc->lock);                ->spin_lock(&ioc->lock);
    ->ioc_destroy_icq(icq);
      ->list_del_init(&icq->q_node);
      ->call_rcu(&icq->__rcu_head,
            icq_free_icq_rcu);
    ->spin_unlock(&ioc->lock);
                                            ->ioc_destroy_icq(icq);
                                              ->hlist_del_init(&icq->ioc_node);
                                              This results into below crash as this memory
                                              is now used by icq->__rcu_head in context#1.
                                              There is a chance that icq could be free'd
                                              as well.
    
    22150.386550:   <6> Unable to handle kernel write to read-only memory
    at virtual address ffffffaa8d31ca50
    ...
    Call trace:
    22150.607350:   <2>  ioc_destroy_icq+0x44/0x110
    22150.611202:   <2>  ioc_clear_queue+0xac/0x148
    22150.615056:   <2>  blk_cleanup_queue+0x11c/0x1a0
    22150.619174:   <2>  __scsi_remove_device+0xdc/0x128
    22150.623465:   <2>  scsi_forget_host+0x2c/0x78
    22150.627315:   <2>  scsi_remove_host+0x7c/0x2a0
    22150.631257:   <2>  usb_stor_disconnect+0x74/0xc8
    22150.635371:   <2>  usb_unbind_interface+0xc8/0x278
    22150.639665:   <2>  device_release_driver_internal+0x198/0x250
    22150.644897:   <2>  device_release_driver+0x24/0x30
    22150.649176:   <2>  bus_remove_device+0xec/0x140
    22150.653204:   <2>  device_del+0x270/0x460
    22150.656712:   <2>  usb_disable_device+0x120/0x390
    22150.660918:   <2>  usb_disconnect+0xf4/0x2e0
    22150.664684:   <2>  hub_event+0xd70/0x17e8
    22150.668197:   <2>  process_one_work+0x210/0x480
    22150.672222:   <2>  worker_thread+0x32c/0x4c8
    
    Fix this by adding a new ICQ_DESTROYED flag in ioc_destroy_icq() to
    indicate this icq is once marked as destroyed. Also, ensure
    __ioc_clear_queue() is accessing icq within rcu_read_lock/unlock so
    that icq doesn't get free'd up while it is still using it.
    
    Signed-off-by: Sahitya Tummala <stummala@codeaurora.org>
    Co-developed-by: Pradeep P V K <ppvk@codeaurora.org>
    Signed-off-by: Pradeep P V K <ppvk@codeaurora.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/iocontext.h b/include/linux/iocontext.h
index dba15ca8e60b..1dcd9198beb7 100644
--- a/include/linux/iocontext.h
+++ b/include/linux/iocontext.h
@@ -8,6 +8,7 @@
 
 enum {
 	ICQ_EXITED		= 1 << 2,
+	ICQ_DESTROYED		= 1 << 3,
 };
 
 /*

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/iocontext.h b/include/linux/iocontext.h
index df38db2ef45b..dba15ca8e60b 100644
--- a/include/linux/iocontext.h
+++ b/include/linux/iocontext.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef IOCONTEXT_H
 #define IOCONTEXT_H
 

commit 2b566fa55b9a94b53217c2818e6c5e5756eeb1a1
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Mar 19 15:10:59 2012 -0700

    block: remove ioc_*_changed()
    
    After the previous patch to cfq, there's no ioc_get_changed() user
    left.  This patch yanks out ioc_{ioprio|cgroup|get}_changed() and all
    related stuff.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Vivek Goyal <vgoyal@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/iocontext.h b/include/linux/iocontext.h
index 6f1a2608e91f..df38db2ef45b 100644
--- a/include/linux/iocontext.h
+++ b/include/linux/iocontext.h
@@ -6,11 +6,7 @@
 #include <linux/workqueue.h>
 
 enum {
-	ICQ_IOPRIO_CHANGED	= 1 << 0,
-	ICQ_CGROUP_CHANGED	= 1 << 1,
 	ICQ_EXITED		= 1 << 2,
-
-	ICQ_CHANGED_MASK	= ICQ_IOPRIO_CHANGED | ICQ_CGROUP_CHANGED,
 };
 
 /*
@@ -152,9 +148,6 @@ void put_io_context_active(struct io_context *ioc);
 void exit_io_context(struct task_struct *task);
 struct io_context *get_task_io_context(struct task_struct *task,
 				       gfp_t gfp_flags, int node);
-void ioc_ioprio_changed(struct io_context *ioc, int ioprio);
-void ioc_cgroup_changed(struct io_context *ioc);
-unsigned int icq_get_changed(struct io_cq *icq);
 #else
 struct io_context;
 static inline void put_io_context(struct io_context *ioc) { }

commit f6e8d01bee036460e03bd4f6a79d014f98ba712e
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Mar 5 13:15:26 2012 -0800

    block: add io_context->active_ref
    
    Currently ioc->nr_tasks is used to decide two things - whether an ioc
    is done issuing IOs and whether it's shared by multiple tasks.  This
    patch separate out the first into ioc->active_ref, which is acquired
    and released using {get|put}_io_context_active() respectively.
    
    This will be used to associate bio's with a given task.  This patch
    doesn't introduce any visible behavior change.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Vivek Goyal <vgoyal@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/iocontext.h b/include/linux/iocontext.h
index 81a8870ac224..6f1a2608e91f 100644
--- a/include/linux/iocontext.h
+++ b/include/linux/iocontext.h
@@ -100,6 +100,7 @@ struct io_cq {
  */
 struct io_context {
 	atomic_long_t refcount;
+	atomic_t active_ref;
 	atomic_t nr_tasks;
 
 	/* all the fields below are protected by this lock */
@@ -120,17 +121,34 @@ struct io_context {
 	struct work_struct release_work;
 };
 
-static inline void ioc_task_link(struct io_context *ioc)
+/**
+ * get_io_context_active - get active reference on ioc
+ * @ioc: ioc of interest
+ *
+ * Only iocs with active reference can issue new IOs.  This function
+ * acquires an active reference on @ioc.  The caller must already have an
+ * active reference on @ioc.
+ */
+static inline void get_io_context_active(struct io_context *ioc)
 {
 	WARN_ON_ONCE(atomic_long_read(&ioc->refcount) <= 0);
-	WARN_ON_ONCE(atomic_read(&ioc->nr_tasks) <= 0);
+	WARN_ON_ONCE(atomic_read(&ioc->active_ref) <= 0);
 	atomic_long_inc(&ioc->refcount);
+	atomic_inc(&ioc->active_ref);
+}
+
+static inline void ioc_task_link(struct io_context *ioc)
+{
+	get_io_context_active(ioc);
+
+	WARN_ON_ONCE(atomic_read(&ioc->nr_tasks) <= 0);
 	atomic_inc(&ioc->nr_tasks);
 }
 
 struct task_struct;
 #ifdef CONFIG_BLOCK
 void put_io_context(struct io_context *ioc);
+void put_io_context_active(struct io_context *ioc);
 void exit_io_context(struct task_struct *task);
 struct io_context *get_task_io_context(struct task_struct *task,
 				       gfp_t gfp_flags, int node);

commit 3d48749d93a3dce732dd30a14002ab90ec4355f3
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Mar 5 13:15:25 2012 -0800

    block: ioc_task_link() can't fail
    
    ioc_task_link() is used to share %current's ioc on clone.  If
    %current->io_context is set, %current is guaranteed to have refcount
    on the ioc and, thus, ioc_task_link() can't fail.
    
    Replace error checking in ioc_task_link() with WARN_ON_ONCE() and make
    it just increment refcount and nr_tasks.
    
    -v2: Description typo fix (Vivek).
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Vivek Goyal <vgoyal@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/iocontext.h b/include/linux/iocontext.h
index 1a3018063034..81a8870ac224 100644
--- a/include/linux/iocontext.h
+++ b/include/linux/iocontext.h
@@ -120,18 +120,12 @@ struct io_context {
 	struct work_struct release_work;
 };
 
-static inline struct io_context *ioc_task_link(struct io_context *ioc)
+static inline void ioc_task_link(struct io_context *ioc)
 {
-	/*
-	 * if ref count is zero, don't allow sharing (ioc is going away, it's
-	 * a race).
-	 */
-	if (ioc && atomic_long_inc_not_zero(&ioc->refcount)) {
-		atomic_inc(&ioc->nr_tasks);
-		return ioc;
-	}
-
-	return NULL;
+	WARN_ON_ONCE(atomic_long_read(&ioc->refcount) <= 0);
+	WARN_ON_ONCE(atomic_read(&ioc->nr_tasks) <= 0);
+	atomic_long_inc(&ioc->refcount);
+	atomic_inc(&ioc->nr_tasks);
 }
 
 struct task_struct;

commit 621032ad6eaabf2fe771c4fa0d8f58e1fcfcdba6
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Feb 15 09:45:53 2012 +0100

    block: exit_io_context() should call elevator_exit_icq_fn()
    
    While updating locking, b2efa05265 "block, cfq: unlink
    cfq_io_context's immediately" moved elevator_exit_icq_fn() invocation
    from exit_io_context() to the final ioc put.  While this doesn't cause
    catastrophic failure, it effectively removes task exit notification to
    elevator and cause noticeable IO performance degradation with CFQ.
    
    On task exit, CFQ used to immediately expire the slice if it was being
    used by the exiting task as no more IO would be issued by the task;
    however, after b2efa05265, the notification is lost and disk could sit
    idle needlessly, leading to noticeable IO performance degradation for
    certain workloads.
    
    This patch renames ioc_exit_icq() to ioc_destroy_icq(), separates
    elevator_exit_icq_fn() invocation into ioc_exit_icq() and invokes it
    from exit_io_context().  ICQ_EXITED flag is added to avoid invoking
    the callback more than once for the same icq.
    
    Walking icq_list from ioc side and invoking elevator callback requires
    reverse double locking.  This may be better implemented using RCU;
    unfortunately, using RCU isn't trivial.  e.g. RCU protection would
    need to cover request_queue and queue_lock switch on cleanup makes
    grabbing queue_lock from RCU unsafe.  Reverse double locking should
    do, at least for now.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reported-and-bisected-by: Shaohua Li <shli@kernel.org>
    LKML-Reference: <CANejiEVzs=pUhQSTvUppkDcc2TNZyfohBRLygW5zFmXyk5A-xQ@mail.gmail.com>
    Tested-by: Shaohua Li <shaohua.li@intel.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/iocontext.h b/include/linux/iocontext.h
index 17839c7b9614..1a3018063034 100644
--- a/include/linux/iocontext.h
+++ b/include/linux/iocontext.h
@@ -8,6 +8,7 @@
 enum {
 	ICQ_IOPRIO_CHANGED	= 1 << 0,
 	ICQ_CGROUP_CHANGED	= 1 << 1,
+	ICQ_EXITED		= 1 << 2,
 
 	ICQ_CHANGED_MASK	= ICQ_IOPRIO_CHANGED | ICQ_CGROUP_CHANGED,
 };

commit d705ae6b133f9f6a8beee617b1224b6a5c99c5da
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Feb 15 09:45:49 2012 +0100

    block: replace icq->changed with icq->flags
    
    icq->changed was used for ICQ_*_CHANGED bits.  Rename it to flags and
    access it under ioc->lock instead of using atomic bitops.
    ioc_get_changed() is added so that the changed part can be fetched and
    cleared as before.
    
    icq->flags will be used to carry other flags.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Tested-by: Shaohua Li <shaohua.li@intel.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/iocontext.h b/include/linux/iocontext.h
index 119773eebe31..17839c7b9614 100644
--- a/include/linux/iocontext.h
+++ b/include/linux/iocontext.h
@@ -6,8 +6,10 @@
 #include <linux/workqueue.h>
 
 enum {
-	ICQ_IOPRIO_CHANGED,
-	ICQ_CGROUP_CHANGED,
+	ICQ_IOPRIO_CHANGED	= 1 << 0,
+	ICQ_CGROUP_CHANGED	= 1 << 1,
+
+	ICQ_CHANGED_MASK	= ICQ_IOPRIO_CHANGED | ICQ_CGROUP_CHANGED,
 };
 
 /*
@@ -88,7 +90,7 @@ struct io_cq {
 		struct rcu_head		__rcu_head;
 	};
 
-	unsigned long		changed;
+	unsigned int		flags;
 };
 
 /*
@@ -139,6 +141,7 @@ struct io_context *get_task_io_context(struct task_struct *task,
 				       gfp_t gfp_flags, int node);
 void ioc_ioprio_changed(struct io_context *ioc, int ioprio);
 void ioc_cgroup_changed(struct io_context *ioc);
+unsigned int icq_get_changed(struct io_cq *icq);
 #else
 struct io_context;
 static inline void put_io_context(struct io_context *ioc) { }

commit 11a3122f6cf2d988a77eb8883d0fc49cd013a6d5
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Feb 7 07:51:30 2012 +0100

    block: strip out locking optimization in put_io_context()
    
    put_io_context() performed a complex trylock dancing to avoid
    deferring ioc release to workqueue.  It was also broken on UP because
    trylock was always assumed to succeed which resulted in unbalanced
    preemption count.
    
    While there are ways to fix the UP breakage, even the most
    pathological microbench (forced ioc allocation and tight fork/exit
    loop) fails to show any appreciable performance benefit of the
    optimization.  Strip it out.  If there turns out to be workloads which
    are affected by this change, simpler optimization from the discussion
    thread can be applied later.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    LKML-Reference: <1328514611.21268.66.camel@sli10-conroe>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/iocontext.h b/include/linux/iocontext.h
index 7e1371c4bccf..119773eebe31 100644
--- a/include/linux/iocontext.h
+++ b/include/linux/iocontext.h
@@ -133,7 +133,7 @@ static inline struct io_context *ioc_task_link(struct io_context *ioc)
 
 struct task_struct;
 #ifdef CONFIG_BLOCK
-void put_io_context(struct io_context *ioc, struct request_queue *locked_q);
+void put_io_context(struct io_context *ioc);
 void exit_io_context(struct task_struct *task);
 struct io_context *get_task_io_context(struct task_struct *task,
 				       gfp_t gfp_flags, int node);
@@ -141,8 +141,7 @@ void ioc_ioprio_changed(struct io_context *ioc, int ioprio);
 void ioc_cgroup_changed(struct io_context *ioc);
 #else
 struct io_context;
-static inline void put_io_context(struct io_context *ioc,
-				  struct request_queue *locked_q) { }
+static inline void put_io_context(struct io_context *ioc) { }
 static inline void exit_io_context(struct task_struct *task) { }
 #endif
 

commit f1f8cc94651738b418ba54c039df536303b91704
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Dec 14 00:33:42 2011 +0100

    block, cfq: move icq creation and rq->elv.icq association to block core
    
    Now block layer knows everything necessary to create and associate
    icq's with requests.  Move ioc_create_icq() to blk-ioc.c and update
    get_request() such that, if elevator_type->icq_size is set, requests
    are automatically associated with their matching icq's before
    elv_set_request().  io_context reference is also managed by block core
    on request alloc/free.
    
    * Only ioprio/cgroup changed handling remains from cfq_get_cic().
      Collapsed into cfq_set_request().
    
    * This removes queue kicking on icq allocation failure (for now).  As
      icq allocation failure is rare and the only effect of queue kicking
      achieved was possibily accelerating queue processing, this change
      shouldn't be noticeable.
    
      There is a larger underlying problem.  Unlike request allocation,
      icq allocation is not guaranteed to succeed eventually after
      retries.  The number of icq is unbound and thus mempool can't be the
      solution either.  This effectively adds allocation dependency on
      memory free path and thus possibility of deadlock.
    
      This usually wouldn't happen because icq allocation is not a hot
      path and, even when the condition triggers, it's highly unlikely
      that none of the writeback workers already has icq.
    
      However, this is still possible especially if elevator is being
      switched under high memory pressure, so we better get it fixed.
      Probably the only solution is just bypassing elevator and appending
      to dispatch queue on any elevator allocation failure.
    
    * Comment added to explain how icq's are managed and synchronized.
    
    This completes cleanup of io_context interface.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/iocontext.h b/include/linux/iocontext.h
index ac390a34c0e7..7e1371c4bccf 100644
--- a/include/linux/iocontext.h
+++ b/include/linux/iocontext.h
@@ -10,6 +10,65 @@ enum {
 	ICQ_CGROUP_CHANGED,
 };
 
+/*
+ * An io_cq (icq) is association between an io_context (ioc) and a
+ * request_queue (q).  This is used by elevators which need to track
+ * information per ioc - q pair.
+ *
+ * Elevator can request use of icq by setting elevator_type->icq_size and
+ * ->icq_align.  Both size and align must be larger than that of struct
+ * io_cq and elevator can use the tail area for private information.  The
+ * recommended way to do this is defining a struct which contains io_cq as
+ * the first member followed by private members and using its size and
+ * align.  For example,
+ *
+ *	struct snail_io_cq {
+ *		struct io_cq	icq;
+ *		int		poke_snail;
+ *		int		feed_snail;
+ *	};
+ *
+ *	struct elevator_type snail_elv_type {
+ *		.ops =		{ ... },
+ *		.icq_size =	sizeof(struct snail_io_cq),
+ *		.icq_align =	__alignof__(struct snail_io_cq),
+ *		...
+ *	};
+ *
+ * If icq_size is set, block core will manage icq's.  All requests will
+ * have its ->elv.icq field set before elevator_ops->elevator_set_req_fn()
+ * is called and be holding a reference to the associated io_context.
+ *
+ * Whenever a new icq is created, elevator_ops->elevator_init_icq_fn() is
+ * called and, on destruction, ->elevator_exit_icq_fn().  Both functions
+ * are called with both the associated io_context and queue locks held.
+ *
+ * Elevator is allowed to lookup icq using ioc_lookup_icq() while holding
+ * queue lock but the returned icq is valid only until the queue lock is
+ * released.  Elevators can not and should not try to create or destroy
+ * icq's.
+ *
+ * As icq's are linked from both ioc and q, the locking rules are a bit
+ * complex.
+ *
+ * - ioc lock nests inside q lock.
+ *
+ * - ioc->icq_list and icq->ioc_node are protected by ioc lock.
+ *   q->icq_list and icq->q_node by q lock.
+ *
+ * - ioc->icq_tree and ioc->icq_hint are protected by ioc lock, while icq
+ *   itself is protected by q lock.  However, both the indexes and icq
+ *   itself are also RCU managed and lookup can be performed holding only
+ *   the q lock.
+ *
+ * - icq's are not reference counted.  They are destroyed when either the
+ *   ioc or q goes away.  Each request with icq set holds an extra
+ *   reference to ioc to ensure it stays until the request is completed.
+ *
+ * - Linking and unlinking icq's are performed while holding both ioc and q
+ *   locks.  Due to the lock ordering, q exit is simple but ioc exit
+ *   requires reverse-order double lock dance.
+ */
 struct io_cq {
 	struct request_queue	*q;
 	struct io_context	*ioc;

commit 7e5a8794492e43e9eebb68a98a23be055888ccd0
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Dec 14 00:33:42 2011 +0100

    block, cfq: move io_cq exit/release to blk-ioc.c
    
    With kmem_cache managed by blk-ioc, io_cq exit/release can be moved to
    blk-ioc too.  The odd ->io_cq->exit/release() callbacks are replaced
    with elevator_ops->elevator_exit_icq_fn() with unlinking from both ioc
    and q, and freeing automatically handled by blk-ioc.  The elevator
    operation only need to perform exit operation specific to the elevator
    - in cfq's case, exiting the cfqq's.
    
    Also, clearing of io_cq's on q detach is moved to block core and
    automatically performed on elevator switch and q release.
    
    Because the q io_cq points to might be freed before RCU callback for
    the io_cq runs, blk-ioc code should remember to which cache the io_cq
    needs to be freed when the io_cq is released.  New field
    io_cq->__rcu_icq_cache is added for this purpose.  As both the new
    field and rcu_head are used only after io_cq is released and the
    q/ioc_node fields aren't, they are put into unions.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/iocontext.h b/include/linux/iocontext.h
index d15ca6591f96..ac390a34c0e7 100644
--- a/include/linux/iocontext.h
+++ b/include/linux/iocontext.h
@@ -14,14 +14,22 @@ struct io_cq {
 	struct request_queue	*q;
 	struct io_context	*ioc;
 
-	struct list_head	q_node;
-	struct hlist_node	ioc_node;
+	/*
+	 * q_node and ioc_node link io_cq through icq_list of q and ioc
+	 * respectively.  Both fields are unused once ioc_exit_icq() is
+	 * called and shared with __rcu_icq_cache and __rcu_head which are
+	 * used for RCU free of io_cq.
+	 */
+	union {
+		struct list_head	q_node;
+		struct kmem_cache	*__rcu_icq_cache;
+	};
+	union {
+		struct hlist_node	ioc_node;
+		struct rcu_head		__rcu_head;
+	};
 
 	unsigned long		changed;
-	struct rcu_head		rcu_head;
-
-	void (*exit)(struct io_cq *);
-	void (*release)(struct io_cq *);
 };
 
 /*

commit c58698073218f2c8f2fc5982fa3938c2d3803b9f
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Dec 14 00:33:41 2011 +0100

    block, cfq: reorganize cfq_io_context into generic and cfq specific parts
    
    Currently io_context and cfq logics are mixed without clear boundary.
    Most of io_context is independent from cfq but cfq_io_context handling
    logic is dispersed between generic ioc code and cfq.
    
    cfq_io_context represents association between an io_context and a
    request_queue, which is a concept useful outside of cfq, but it also
    contains fields which are useful only to cfq.
    
    This patch takes out generic part and put it into io_cq (io
    context-queue) and the rest into cfq_io_cq (cic moniker remains the
    same) which contains io_cq.  The following changes are made together.
    
    * cfq_ttime and cfq_io_cq now live in cfq-iosched.c.
    
    * All related fields, functions and constants are renamed accordingly.
    
    * ioc->ioc_data is now "struct io_cq *" instead of "void *" and
      renamed to icq_hint.
    
    This prepares for io_context API cleanup.  Documentation is currently
    sparse.  It will be added later.
    
    Changes in this patch are mechanical and don't cause functional
    change.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/iocontext.h b/include/linux/iocontext.h
index b2b75a54f252..d15ca6591f96 100644
--- a/include/linux/iocontext.h
+++ b/include/linux/iocontext.h
@@ -5,38 +5,23 @@
 #include <linux/rcupdate.h>
 #include <linux/workqueue.h>
 
-struct cfq_queue;
-struct cfq_ttime {
-	unsigned long last_end_request;
-
-	unsigned long ttime_total;
-	unsigned long ttime_samples;
-	unsigned long ttime_mean;
-};
-
 enum {
-	CIC_IOPRIO_CHANGED,
-	CIC_CGROUP_CHANGED,
+	ICQ_IOPRIO_CHANGED,
+	ICQ_CGROUP_CHANGED,
 };
 
-struct cfq_io_context {
-	struct request_queue *q;
-
-	struct cfq_queue *cfqq[2];
-
-	struct io_context *ioc;
-
-	struct cfq_ttime ttime;
-
-	struct list_head queue_list;
-	struct hlist_node cic_list;
+struct io_cq {
+	struct request_queue	*q;
+	struct io_context	*ioc;
 
-	unsigned long changed;
+	struct list_head	q_node;
+	struct hlist_node	ioc_node;
 
-	void (*exit)(struct cfq_io_context *);
-	void (*release)(struct cfq_io_context *);
+	unsigned long		changed;
+	struct rcu_head		rcu_head;
 
-	struct rcu_head rcu_head;
+	void (*exit)(struct io_cq *);
+	void (*release)(struct io_cq *);
 };
 
 /*
@@ -58,9 +43,9 @@ struct io_context {
 	int nr_batch_requests;     /* Number of requests left in the batch */
 	unsigned long last_waited; /* Time last woken after wait for request */
 
-	struct radix_tree_root radix_root;
-	struct hlist_head cic_list;
-	void __rcu *ioc_data;
+	struct radix_tree_root	icq_tree;
+	struct io_cq __rcu	*icq_hint;
+	struct hlist_head	icq_list;
 
 	struct work_struct release_work;
 };

commit 1238033c79e92e5c315af12e45396f1a78c73dec
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Dec 14 00:33:40 2011 +0100

    block, cfq: kill cic->key
    
    Now that lazy paths are removed, cfqd_dead_key() is meaningless and
    cic->q can be used whereever cic->key is used.  Kill cic->key.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/iocontext.h b/include/linux/iocontext.h
index 01e863128780..b2b75a54f252 100644
--- a/include/linux/iocontext.h
+++ b/include/linux/iocontext.h
@@ -20,7 +20,6 @@ enum {
 };
 
 struct cfq_io_context {
-	void *key;
 	struct request_queue *q;
 
 	struct cfq_queue *cfqq[2];

commit b2efa05265d62bc29f3a64400fad4b44340eedb8
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Dec 14 00:33:39 2011 +0100

    block, cfq: unlink cfq_io_context's immediately
    
    cic is association between io_context and request_queue.  A cic is
    linked from both ioc and q and should be destroyed when either one
    goes away.  As ioc and q both have their own locks, locking becomes a
    bit complex - both orders work for removal from one but not from the
    other.
    
    Currently, cfq tries to circumvent this locking order issue with RCU.
    ioc->lock nests inside queue_lock but the radix tree and cic's are
    also protected by RCU allowing either side to walk their lists without
    grabbing lock.
    
    This rather unconventional use of RCU quickly devolves into extremely
    fragile convolution.  e.g. The following is from cfqd going away too
    soon after ioc and q exits raced.
    
     general protection fault: 0000 [#1] PREEMPT SMP
     CPU 2
     Modules linked in:
     [   88.503444]
     Pid: 599, comm: hexdump Not tainted 3.1.0-rc10-work+ #158 Bochs Bochs
     RIP: 0010:[<ffffffff81397628>]  [<ffffffff81397628>] cfq_exit_single_io_context+0x58/0xf0
     ...
     Call Trace:
      [<ffffffff81395a4a>] call_for_each_cic+0x5a/0x90
      [<ffffffff81395ab5>] cfq_exit_io_context+0x15/0x20
      [<ffffffff81389130>] exit_io_context+0x100/0x140
      [<ffffffff81098a29>] do_exit+0x579/0x850
      [<ffffffff81098d5b>] do_group_exit+0x5b/0xd0
      [<ffffffff81098de7>] sys_exit_group+0x17/0x20
      [<ffffffff81b02f2b>] system_call_fastpath+0x16/0x1b
    
    The only real hot path here is cic lookup during request
    initialization and avoiding extra locking requires very confined use
    of RCU.  This patch makes cic removal from both ioc and request_queue
    perform double-locking and unlink immediately.
    
    * From q side, the change is almost trivial as ioc->lock nests inside
      queue_lock.  It just needs to grab each ioc->lock as it walks
      cic_list and unlink it.
    
    * From ioc side, it's a bit more difficult because of inversed lock
      order.  ioc needs its lock to walk its cic_list but can't grab the
      matching queue_lock and needs to perform unlock-relock dancing.
    
      Unlinking is now wholly done from put_io_context() and fast path is
      optimized by using the queue_lock the caller already holds, which is
      by far the most common case.  If the ioc accessed multiple devices,
      it tries with trylock.  In unlikely cases of fast path failure, it
      falls back to full double-locking dance from workqueue.
    
    Double-locking isn't the prettiest thing in the world but it's *far*
    simpler and more understandable than RCU trick without adding any
    meaningful overhead.
    
    This still leaves a lot of now unnecessary RCU logics.  Future patches
    will trim them.
    
    -v2: Vivek pointed out that cic->q was being dereferenced after
         cic->release() was called.  Updated to use local variable @this_q
         instead.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Vivek Goyal <vgoyal@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/iocontext.h b/include/linux/iocontext.h
index 2c2b6da96b3c..01e863128780 100644
--- a/include/linux/iocontext.h
+++ b/include/linux/iocontext.h
@@ -3,6 +3,7 @@
 
 #include <linux/radix-tree.h>
 #include <linux/rcupdate.h>
+#include <linux/workqueue.h>
 
 struct cfq_queue;
 struct cfq_ttime {
@@ -33,8 +34,8 @@ struct cfq_io_context {
 
 	unsigned long changed;
 
-	void (*dtor)(struct io_context *); /* destructor */
-	void (*exit)(struct io_context *); /* called on task exit */
+	void (*exit)(struct cfq_io_context *);
+	void (*release)(struct cfq_io_context *);
 
 	struct rcu_head rcu_head;
 };
@@ -61,6 +62,8 @@ struct io_context {
 	struct radix_tree_root radix_root;
 	struct hlist_head cic_list;
 	void __rcu *ioc_data;
+
+	struct work_struct release_work;
 };
 
 static inline struct io_context *ioc_task_link(struct io_context *ioc)
@@ -79,7 +82,7 @@ static inline struct io_context *ioc_task_link(struct io_context *ioc)
 
 struct task_struct;
 #ifdef CONFIG_BLOCK
-void put_io_context(struct io_context *ioc);
+void put_io_context(struct io_context *ioc, struct request_queue *locked_q);
 void exit_io_context(struct task_struct *task);
 struct io_context *get_task_io_context(struct task_struct *task,
 				       gfp_t gfp_flags, int node);
@@ -87,7 +90,8 @@ void ioc_ioprio_changed(struct io_context *ioc, int ioprio);
 void ioc_cgroup_changed(struct io_context *ioc);
 #else
 struct io_context;
-static inline void put_io_context(struct io_context *ioc) { }
+static inline void put_io_context(struct io_context *ioc,
+				  struct request_queue *locked_q) { }
 static inline void exit_io_context(struct task_struct *task) { }
 #endif
 

commit dc86900e0a8f665122de6faadd27fb4c6d2b3e4d
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Dec 14 00:33:38 2011 +0100

    block, cfq: move ioc ioprio/cgroup changed handling to cic
    
    ioprio/cgroup change was handled by marking the changed state in ioc
    and, on the following access to the ioc, performing RCU-protected
    iteration through all cic's grabbing the matching queue_lock.
    
    This patch moves the changed state to each cic.  When ioprio or cgroup
    changes, the respective bit is set on all cic's of the ioc and when
    each of those cic (not ioc) is accessed, change is applied for that
    specific ioc-queue pair.
    
    This also fixes the following two race conditions between setting and
    clearing of changed states.
    
    * Missing barrier between assign/load of ioprio and ioprio_changed
      allowed applying old ioprio.
    
    * Change requests could happen between application of change and
      clearing of changed variables.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/iocontext.h b/include/linux/iocontext.h
index 079aea8fd8a8..2c2b6da96b3c 100644
--- a/include/linux/iocontext.h
+++ b/include/linux/iocontext.h
@@ -13,6 +13,11 @@ struct cfq_ttime {
 	unsigned long ttime_mean;
 };
 
+enum {
+	CIC_IOPRIO_CHANGED,
+	CIC_CGROUP_CHANGED,
+};
+
 struct cfq_io_context {
 	void *key;
 	struct request_queue *q;
@@ -26,6 +31,8 @@ struct cfq_io_context {
 	struct list_head queue_list;
 	struct hlist_node cic_list;
 
+	unsigned long changed;
+
 	void (*dtor)(struct io_context *); /* destructor */
 	void (*exit)(struct io_context *); /* called on task exit */
 
@@ -44,11 +51,6 @@ struct io_context {
 	spinlock_t lock;
 
 	unsigned short ioprio;
-	unsigned short ioprio_changed;
-
-#if defined(CONFIG_BLK_CGROUP) || defined(CONFIG_BLK_CGROUP_MODULE)
-	unsigned short cgroup_changed;
-#endif
 
 	/*
 	 * For request batching
@@ -81,6 +83,8 @@ void put_io_context(struct io_context *ioc);
 void exit_io_context(struct task_struct *task);
 struct io_context *get_task_io_context(struct task_struct *task,
 				       gfp_t gfp_flags, int node);
+void ioc_ioprio_changed(struct io_context *ioc, int ioprio);
+void ioc_cgroup_changed(struct io_context *ioc);
 #else
 struct io_context;
 static inline void put_io_context(struct io_context *ioc) { }

commit 283287a52e3c3f7f8f9da747f4b8c5202740d776
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Dec 14 00:33:38 2011 +0100

    block, cfq: misc updates to cfq_io_context
    
    Make the following changes to prepare for ioc/cic management cleanup.
    
    * Add cic->q so that ioc can determine the associated queue without
      querying cfq.  This will eventually replace ->key.
    
    * Factor out cfq_release_cic() from cic_free_func().  This function
      assumes that the caller handled locking.
    
    * Rename __cfq_exit_single_io_context() to cfq_exit_cic() and make it
      take only @cic.
    
    * Restructure cfq_cic_link() for future updates.
    
    This patch doesn't introduce any functional changes.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/iocontext.h b/include/linux/iocontext.h
index 28bb621ef5a2..079aea8fd8a8 100644
--- a/include/linux/iocontext.h
+++ b/include/linux/iocontext.h
@@ -15,6 +15,7 @@ struct cfq_ttime {
 
 struct cfq_io_context {
 	void *key;
+	struct request_queue *q;
 
 	struct cfq_queue *cfqq[2];
 

commit 6e736be7f282fff705db7c34a15313281b372a76
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Dec 14 00:33:38 2011 +0100

    block: make ioc get/put interface more conventional and fix race on alloction
    
    Ignoring copy_io() during fork, io_context can be allocated from two
    places - current_io_context() and set_task_ioprio().  The former is
    always called from local task while the latter can be called from
    different task.  The synchornization between them are peculiar and
    dubious.
    
    * current_io_context() doesn't grab task_lock() and assumes that if it
      saw %NULL ->io_context, it would stay that way until allocation and
      assignment is complete.  It has smp_wmb() between alloc/init and
      assignment.
    
    * set_task_ioprio() grabs task_lock() for assignment and does
      smp_read_barrier_depends() between "ioc = task->io_context" and "if
      (ioc)".  Unfortunately, this doesn't achieve anything - the latter
      is not a dependent load of the former.  ie, if ioc itself were being
      dereferenced "ioc->xxx", it would mean something (not sure what tho)
      but as the code currently stands, the dependent read barrier is
      noop.
    
    As only one of the the two test-assignment sequences is task_lock()
    protected, the task_lock() can't do much about race between the two.
    Nothing prevents current_io_context() and set_task_ioprio() allocating
    its own ioc for the same task and overwriting the other's.
    
    Also, set_task_ioprio() can race with exiting task and create a new
    ioc after exit_io_context() is finished.
    
    ioc get/put doesn't have any reason to be complex.  The only hot path
    is accessing the existing ioc of %current, which is simple to achieve
    given that ->io_context is never destroyed as long as the task is
    alive.  All other paths can happily go through task_lock() like all
    other task sub structures without impacting anything.
    
    This patch updates ioc get/put so that it becomes more conventional.
    
    * alloc_io_context() is replaced with get_task_io_context().  This is
      the only interface which can acquire access to ioc of another task.
      On return, the caller has an explicit reference to the object which
      should be put using put_io_context() afterwards.
    
    * The functionality of current_io_context() remains the same but when
      creating a new ioc, it shares the code path with
      get_task_io_context() and always goes through task_lock().
    
    * get_io_context() now means incrementing ref on an ioc which the
      caller already has access to (be that an explicit refcnt or implicit
      %current one).
    
    * PF_EXITING inhibits creation of new io_context and once
      exit_io_context() is finished, it's guaranteed that both ioc
      acquisition functions return %NULL.
    
    * All users are updated.  Most are trivial but
      smp_read_barrier_depends() removal from cfq_get_io_context() needs a
      bit of explanation.  I suppose the original intention was to ensure
      ioc->ioprio is visible when set_task_ioprio() allocates new
      io_context and installs it; however, this wouldn't have worked
      because set_task_ioprio() doesn't have wmb between init and install.
      There are other problems with this which will be fixed in another
      patch.
    
    * While at it, use NUMA_NO_NODE instead of -1 for wildcard node
      specification.
    
    -v2: Vivek spotted contamination from debug patch.  Removed.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Vivek Goyal <vgoyal@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/iocontext.h b/include/linux/iocontext.h
index 8a6ecb66346f..28bb621ef5a2 100644
--- a/include/linux/iocontext.h
+++ b/include/linux/iocontext.h
@@ -78,8 +78,8 @@ struct task_struct;
 #ifdef CONFIG_BLOCK
 void put_io_context(struct io_context *ioc);
 void exit_io_context(struct task_struct *task);
-struct io_context *get_io_context(gfp_t gfp_flags, int node);
-struct io_context *alloc_io_context(gfp_t gfp_flags, int node);
+struct io_context *get_task_io_context(struct task_struct *task,
+				       gfp_t gfp_flags, int node);
 #else
 struct io_context;
 static inline void put_io_context(struct io_context *ioc) { }

commit 42ec57a8f68311bbbf4ff96a5d33c8a2e90b9d05
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Dec 14 00:33:37 2011 +0100

    block: misc ioc cleanups
    
    * int return from put_io_context() wasn't used by anybody.  Make it
      return void like other put functions and docbook-fy the function
      comment.
    
    * Reorder dummy declarations for !CONFIG_BLOCK case a bit.
    
    * Make alloc_ioc_context() use __GFP_ZERO allocation, take init out of
      if block and drop 0'ing.
    
    * Docbook-fy current_io_context() comment.
    
    This patch doesn't introduce any functional change.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/iocontext.h b/include/linux/iocontext.h
index 5037a0ad2312..8a6ecb66346f 100644
--- a/include/linux/iocontext.h
+++ b/include/linux/iocontext.h
@@ -76,20 +76,14 @@ static inline struct io_context *ioc_task_link(struct io_context *ioc)
 
 struct task_struct;
 #ifdef CONFIG_BLOCK
-int put_io_context(struct io_context *ioc);
+void put_io_context(struct io_context *ioc);
 void exit_io_context(struct task_struct *task);
 struct io_context *get_io_context(gfp_t gfp_flags, int node);
 struct io_context *alloc_io_context(gfp_t gfp_flags, int node);
 #else
-static inline void exit_io_context(struct task_struct *task)
-{
-}
-
 struct io_context;
-static inline int put_io_context(struct io_context *ioc)
-{
-	return 1;
-}
+static inline void put_io_context(struct io_context *ioc) { }
+static inline void exit_io_context(struct task_struct *task) { }
 #endif
 
 #endif

commit 383cd7213f95a2784ab5038fe292844178768b82
Author: Shaohua Li <shaohua.li@intel.com>
Date:   Tue Jul 12 14:24:35 2011 +0200

    CFQ: move think time check variables to a separate struct
    
    Move the variables to do think time check to a sepatate struct. This is
    to prepare adding think time check for service tree and group. No
    functional change.
    
    Signed-off-by: Shaohua Li <shaohua.li@intel.com>
    Acked-by: Vivek Goyal <vgoyal@redhat.com>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/iocontext.h b/include/linux/iocontext.h
index b2eee896dcbc..5037a0ad2312 100644
--- a/include/linux/iocontext.h
+++ b/include/linux/iocontext.h
@@ -5,6 +5,14 @@
 #include <linux/rcupdate.h>
 
 struct cfq_queue;
+struct cfq_ttime {
+	unsigned long last_end_request;
+
+	unsigned long ttime_total;
+	unsigned long ttime_samples;
+	unsigned long ttime_mean;
+};
+
 struct cfq_io_context {
 	void *key;
 
@@ -12,11 +20,7 @@ struct cfq_io_context {
 
 	struct io_context *ioc;
 
-	unsigned long last_end_request;
-
-	unsigned long ttime_total;
-	unsigned long ttime_samples;
-	unsigned long ttime_mean;
+	struct cfq_ttime ttime;
 
 	struct list_head queue_list;
 	struct hlist_node cic_list;

commit cedb4a7d9f6aedb0dce94d6285b69dcb3c10fa05
Author: Jens Axboe <jaxboe@fusionio.com>
Date:   Thu Nov 11 13:37:54 2010 +0100

    block: remove unused copy_io_context()
    
    Reported-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/iocontext.h b/include/linux/iocontext.h
index 3e70b21884a9..b2eee896dcbc 100644
--- a/include/linux/iocontext.h
+++ b/include/linux/iocontext.h
@@ -76,7 +76,6 @@ int put_io_context(struct io_context *ioc);
 void exit_io_context(struct task_struct *task);
 struct io_context *get_io_context(gfp_t gfp_flags, int node);
 struct io_context *alloc_io_context(gfp_t gfp_flags, int node);
-void copy_io_context(struct io_context **pdst, struct io_context **psrc);
 #else
 static inline void exit_io_context(struct task_struct *task)
 {

commit 4d2deb40b20c2608486598364e63e37b09a9ac2f
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Wed Feb 24 20:01:56 2010 +0100

    kernel: __rcu annotations
    
    This adds annotations for RCU operations in core kernel components
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Al Viro <viro@ZenIV.linux.org.uk>
    Cc: Jens Axboe <jens.axboe@oracle.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/include/linux/iocontext.h b/include/linux/iocontext.h
index 64d529133031..3e70b21884a9 100644
--- a/include/linux/iocontext.h
+++ b/include/linux/iocontext.h
@@ -53,7 +53,7 @@ struct io_context {
 
 	struct radix_tree_root radix_root;
 	struct hlist_head cic_list;
-	void *ioc_data;
+	void __rcu *ioc_data;
 };
 
 static inline struct io_context *ioc_task_link(struct io_context *ioc)

commit bca4b914b5da3d8e7b9b647f620b71dc85c0c394
Author: Konstantin Khlebnikov <khlebnikov@openvz.org>
Date:   Thu May 20 23:21:34 2010 +0400

    cfq-iosched: remove dead_key from cfq_io_context
    
    Remove ->dead_key field from cfq_io_context to shrink its size to 128 bytes.
    (64 bytes for 32-bit hosts)
    
    Use lower bit in ->key as dead-mark, instead of moving key to separate field.
    After this for dead cfq_io_context we got cic->key != cfqd automatically.
    Thus, io_context's last-hit cache should work without changing.
    
    Now to check ->key for non-dead state compare it with cfqd,
    instead of checking ->key for non-null value as it was before.
    
    Plus remove obsolete race protection in cfq_cic_lookup.
    This race gone after v2.6.24-1728-g4ac845a
    
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/iocontext.h b/include/linux/iocontext.h
index a0bb301afac0..64d529133031 100644
--- a/include/linux/iocontext.h
+++ b/include/linux/iocontext.h
@@ -7,7 +7,6 @@
 struct cfq_queue;
 struct cfq_io_context {
 	void *key;
-	unsigned long dead_key;
 
 	struct cfq_queue *cfqq[2];
 

commit 67523c48aa74d5637848edeccf285af1c60bf14a
Author: Ben Blum <bblum@andrew.cmu.edu>
Date:   Wed Mar 10 15:22:11 2010 -0800

    cgroups: blkio subsystem as module
    
    Modify the Block I/O cgroup subsystem to be able to be built as a module.
    As the CFQ disk scheduler optionally depends on blk-cgroup, config options
    in block/Kconfig, block/Kconfig.iosched, and block/blk-cgroup.h are
    enhanced to support the new module dependency.
    
    Signed-off-by: Ben Blum <bblum@andrew.cmu.edu>
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Paul Menage <menage@google.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: Vivek Goyal <vgoyal@redhat.com>
    Cc: Jens Axboe <jens.axboe@oracle.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/iocontext.h b/include/linux/iocontext.h
index 1195a806fe0c..a0bb301afac0 100644
--- a/include/linux/iocontext.h
+++ b/include/linux/iocontext.h
@@ -42,7 +42,7 @@ struct io_context {
 	unsigned short ioprio;
 	unsigned short ioprio_changed;
 
-#ifdef CONFIG_BLK_CGROUP
+#if defined(CONFIG_BLK_CGROUP) || defined(CONFIG_BLK_CGROUP_MODULE)
 	unsigned short cgroup_changed;
 #endif
 

commit 58c24a61614f5da290068e47fc5ec65370eb61dd
Author: Richard Kennedy <richard@rsk.demon.co.uk>
Date:   Fri Feb 26 14:00:43 2010 +0100

    block: remove padding from io_context on 64bit builds
    
    On 64 bit builds when CONFIG_BLK_CGROUP=n (the default) this removes 8
    bytes of padding from structure io_context and drops its size from 72 to
    64 bytes, so needing one fewer cachelines and allowing more objects per
    slab in it's kmem_cache.
    
    Signed-off-by: Richard Kennedy <richard@rsk.demon.co.uk>
    
    ----
    patch against 2.6.33
    compiled & test on x86_64 AMDX2
    regards
    Richard
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/iocontext.h b/include/linux/iocontext.h
index 78ef023227d4..1195a806fe0c 100644
--- a/include/linux/iocontext.h
+++ b/include/linux/iocontext.h
@@ -49,8 +49,8 @@ struct io_context {
 	/*
 	 * For request batching
 	 */
-	unsigned long last_waited; /* Time last woken after wait for request */
 	int nr_batch_requests;     /* Number of requests left in the batch */
+	unsigned long last_waited; /* Time last woken after wait for request */
 
 	struct radix_tree_root radix_root;
 	struct hlist_head cic_list;

commit ce289321b7dc1eb108e3df0dec872b7429ef49f7
Author: Kirill Afonshin <kirill_nnov@mail.ru>
Date:   Fri Jan 8 22:09:59 2010 +0300

    block: removed unused as_io_context
    
    It isn't used anymore, since AS was deleted.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/iocontext.h b/include/linux/iocontext.h
index a63235996309..78ef023227d4 100644
--- a/include/linux/iocontext.h
+++ b/include/linux/iocontext.h
@@ -4,32 +4,6 @@
 #include <linux/radix-tree.h>
 #include <linux/rcupdate.h>
 
-/*
- * This is the per-process anticipatory I/O scheduler state.
- */
-struct as_io_context {
-	spinlock_t lock;
-
-	void (*dtor)(struct as_io_context *aic); /* destructor */
-	void (*exit)(struct as_io_context *aic); /* called on task exit */
-
-	unsigned long state;
-	atomic_t nr_queued; /* queued reads & sync writes */
-	atomic_t nr_dispatched; /* number of requests gone to the drivers */
-
-	/* IO History tracking */
-	/* Thinktime */
-	unsigned long last_end_request;
-	unsigned long ttime_total;
-	unsigned long ttime_samples;
-	unsigned long ttime_mean;
-	/* Layout pattern */
-	unsigned int seek_samples;
-	sector_t last_request_pos;
-	u64 seek_total;
-	sector_t seek_mean;
-};
-
 struct cfq_queue;
 struct cfq_io_context {
 	void *key;
@@ -78,7 +52,6 @@ struct io_context {
 	unsigned long last_waited; /* Time last woken after wait for request */
 	int nr_batch_requests;     /* Number of requests left in the batch */
 
-	struct as_io_context *aic;
 	struct radix_tree_root radix_root;
 	struct hlist_head cic_list;
 	void *ioc_data;

commit b69f2292063d2caf37ca9aec7d63ded203701bf3
Author: Louis Rilling <louis.rilling@kerlabs.com>
Date:   Fri Dec 4 14:52:42 2009 +0100

    block: Fix io_context leak after failure of clone with CLONE_IO
    
    With CLONE_IO, parent's io_context->nr_tasks is incremented, but never
    decremented whenever copy_process() fails afterwards, which prevents
    exit_io_context() from calling IO schedulers exit functions.
    
    Give a task_struct to exit_io_context(), and call exit_io_context() instead of
    put_io_context() in copy_process() cleanup path.
    
    Signed-off-by: Louis Rilling <louis.rilling@kerlabs.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/iocontext.h b/include/linux/iocontext.h
index d61b0b8b5cd1..a63235996309 100644
--- a/include/linux/iocontext.h
+++ b/include/linux/iocontext.h
@@ -98,14 +98,15 @@ static inline struct io_context *ioc_task_link(struct io_context *ioc)
 	return NULL;
 }
 
+struct task_struct;
 #ifdef CONFIG_BLOCK
 int put_io_context(struct io_context *ioc);
-void exit_io_context(void);
+void exit_io_context(struct task_struct *task);
 struct io_context *get_io_context(gfp_t gfp_flags, int node);
 struct io_context *alloc_io_context(gfp_t gfp_flags, int node);
 void copy_io_context(struct io_context **pdst, struct io_context **psrc);
 #else
-static inline void exit_io_context(void)
+static inline void exit_io_context(struct task_struct *task)
 {
 }
 

commit 31e4c28d95e64f2d5d3c497a3ecf37c62de635b4
Author: Vivek Goyal <vgoyal@redhat.com>
Date:   Thu Dec 3 12:59:42 2009 -0500

    blkio: Introduce blkio controller cgroup interface
    
    o This is basic implementation of blkio controller cgroup interface. This is
      the common interface visible to user space and should be used by different
      IO control policies as we implement those.
    
    Signed-off-by: Vivek Goyal <vgoyal@redhat.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/iocontext.h b/include/linux/iocontext.h
index eb73632440f1..d61b0b8b5cd1 100644
--- a/include/linux/iocontext.h
+++ b/include/linux/iocontext.h
@@ -68,6 +68,10 @@ struct io_context {
 	unsigned short ioprio;
 	unsigned short ioprio_changed;
 
+#ifdef CONFIG_BLK_CGROUP
+	unsigned short cgroup_changed;
+#endif
+
 	/*
 	 * For request batching
 	 */

commit b2c18e1e08a5a9663094d57bb4be2f02226ee61c
Author: Jeff Moyer <jmoyer@redhat.com>
Date:   Fri Oct 23 17:14:49 2009 -0400

    cfq: calculate the seek_mean per cfq_queue not per cfq_io_context
    
    async cfq_queue's are already shared between processes within the same
    priority, and forthcoming patches will change the mapping of cic to sync
    cfq_queue from 1:1 to 1:N.  So, calculate the seekiness of a process
    based on the cfq_queue instead of the cfq_io_context.
    
    Signed-off-by: Jeff Moyer <jmoyer@redhat.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/iocontext.h b/include/linux/iocontext.h
index 4da4a75c3f1e..eb73632440f1 100644
--- a/include/linux/iocontext.h
+++ b/include/linux/iocontext.h
@@ -40,16 +40,11 @@ struct cfq_io_context {
 	struct io_context *ioc;
 
 	unsigned long last_end_request;
-	sector_t last_request_pos;
 
 	unsigned long ttime_total;
 	unsigned long ttime_samples;
 	unsigned long ttime_mean;
 
-	unsigned int seek_samples;
-	u64 seek_total;
-	sector_t seek_mean;
-
 	struct list_head queue_list;
 	struct hlist_node cic_list;
 

commit cbb4f2646d77b536ed2b1500ef6641083228ed8f
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Fri Jul 31 08:55:48 2009 +0200

    io context: fix ref counting
    
    Commit d9c7d394a8ebacb60097b192939ae9f15235225e
    ("block: prevent possible io_context->refcount overflow") mistakenly
    changed atomic_inc(&ioc->nr_tasks) to atomic_long_inc(&ioc->refcount).
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Acked-by: Nikanth Karthikesan <knikanth@suse.de>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/iocontext.h b/include/linux/iocontext.h
index dd05434fa45f..4da4a75c3f1e 100644
--- a/include/linux/iocontext.h
+++ b/include/linux/iocontext.h
@@ -92,7 +92,7 @@ static inline struct io_context *ioc_task_link(struct io_context *ioc)
 	 * a race).
 	 */
 	if (ioc && atomic_long_inc_not_zero(&ioc->refcount)) {
-		atomic_long_inc(&ioc->refcount);
+		atomic_inc(&ioc->nr_tasks);
 		return ioc;
 	}
 

commit d9c7d394a8ebacb60097b192939ae9f15235225e
Author: Nikanth Karthikesan <knikanth@novell.com>
Date:   Wed Jun 10 12:57:06 2009 -0700

    block: prevent possible io_context->refcount overflow
    
    Currently io_context has an atomic_t(32-bit) as refcount.  In the case of
    cfq, for each device against whcih a task does I/O, a reference to the
    io_context would be taken.  And when there are multiple process sharing
    io_contexts(CLONE_IO) would also have a reference to the same io_context.
    
    Theoretically the possible maximum number of processes sharing the same
    io_context + the number of disks/cfq_data referring to the same io_context
    can overflow the 32-bit counter on a very high-end machine.
    
    Even though it is an improbable case, let us make it atomic_long_t.
    
    Signed-off-by: Nikanth Karthikesan <knikanth@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/iocontext.h b/include/linux/iocontext.h
index 08b987bccf89..dd05434fa45f 100644
--- a/include/linux/iocontext.h
+++ b/include/linux/iocontext.h
@@ -64,7 +64,7 @@ struct cfq_io_context {
  * and kmalloc'ed. These could be shared between processes.
  */
 struct io_context {
-	atomic_t refcount;
+	atomic_long_t refcount;
 	atomic_t nr_tasks;
 
 	/* all the fields below are protected by this lock */
@@ -91,8 +91,8 @@ static inline struct io_context *ioc_task_link(struct io_context *ioc)
 	 * if ref count is zero, don't allow sharing (ioc is going away, it's
 	 * a race).
 	 */
-	if (ioc && atomic_inc_not_zero(&ioc->refcount)) {
-		atomic_inc(&ioc->nr_tasks);
+	if (ioc && atomic_long_inc_not_zero(&ioc->refcount)) {
+		atomic_long_inc(&ioc->refcount);
 		return ioc;
 	}
 

commit da9cbc87395308a21465bd25441297bbba0477e1
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Mon Jun 30 20:42:08 2008 +0200

    block: blkdev.h cleanup, move iocontext stuff to iocontext.h
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/iocontext.h b/include/linux/iocontext.h
index 2b7a1187cb29..08b987bccf89 100644
--- a/include/linux/iocontext.h
+++ b/include/linux/iocontext.h
@@ -99,4 +99,22 @@ static inline struct io_context *ioc_task_link(struct io_context *ioc)
 	return NULL;
 }
 
+#ifdef CONFIG_BLOCK
+int put_io_context(struct io_context *ioc);
+void exit_io_context(void);
+struct io_context *get_io_context(gfp_t gfp_flags, int node);
+struct io_context *alloc_io_context(gfp_t gfp_flags, int node);
+void copy_io_context(struct io_context **pdst, struct io_context **psrc);
+#else
+static inline void exit_io_context(void)
+{
+}
+
+struct io_context;
+static inline int put_io_context(struct io_context *ioc)
+{
+	return 1;
+}
+#endif
+
 #endif

commit d237e5c7ce82b6c7e1aa50348520a0c5a47274fa
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Tue Apr 15 09:25:33 2008 +0200

    io context: increment task attachment count in ioc_task_link()
    
    Thanks to Nikanth Karthikesan <knikanth@suse.de> for reporting this.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/iocontext.h b/include/linux/iocontext.h
index cac4b364cd40..2b7a1187cb29 100644
--- a/include/linux/iocontext.h
+++ b/include/linux/iocontext.h
@@ -91,8 +91,10 @@ static inline struct io_context *ioc_task_link(struct io_context *ioc)
 	 * if ref count is zero, don't allow sharing (ioc is going away, it's
 	 * a race).
 	 */
-	if (ioc && atomic_inc_not_zero(&ioc->refcount))
+	if (ioc && atomic_inc_not_zero(&ioc->refcount)) {
+		atomic_inc(&ioc->nr_tasks);
 		return ioc;
+	}
 
 	return NULL;
 }

commit 34e6bbf23c8f43e8713d9bd092680f1660494b4a
Author: Fabio Checconi <fabio@gandalf.sssup.i>
Date:   Wed Apr 2 14:31:02 2008 +0200

    cfq-iosched: fix rcu freeing of cfq io contexts
    
    SLAB_DESTROY_BY_RCU is not a direct substitute for normal call_rcu()
    freeing, since it'll page freeing but NOT object freeing. So change
    cfq to do the freeing on its own.
    
    Signed-off-by: Fabio Checconi <fabio@gandalf.sssup.it>
    Acked-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/iocontext.h b/include/linux/iocontext.h
index 1b4ccf25b4d2..cac4b364cd40 100644
--- a/include/linux/iocontext.h
+++ b/include/linux/iocontext.h
@@ -2,6 +2,7 @@
 #define IOCONTEXT_H
 
 #include <linux/radix-tree.h>
+#include <linux/rcupdate.h>
 
 /*
  * This is the per-process anticipatory I/O scheduler state.
@@ -54,6 +55,8 @@ struct cfq_io_context {
 
 	void (*dtor)(struct io_context *); /* destructor */
 	void (*exit)(struct io_context *); /* called on task exit */
+
+	struct rcu_head rcu_head;
 };
 
 /*

commit ffc4e7595734cf768fa60cea8a4d545dfef8231a
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Tue Feb 19 10:02:29 2008 +0100

    cfq-iosched: add hlist for browsing parallel to the radix tree
    
    It's cumbersome to browse a radix tree from start to finish, especially
    since we modify keys when a process exits. So add a hlist for the single
    purpose of browsing over all known cfq_io_contexts, used for exit,
    io prio change, etc.
    
    This fixes http://bugzilla.kernel.org/show_bug.cgi?id=9948
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/iocontext.h b/include/linux/iocontext.h
index 593b222d9dcc..1b4ccf25b4d2 100644
--- a/include/linux/iocontext.h
+++ b/include/linux/iocontext.h
@@ -50,6 +50,7 @@ struct cfq_io_context {
 	sector_t seek_mean;
 
 	struct list_head queue_list;
+	struct hlist_node cic_list;
 
 	void (*dtor)(struct io_context *); /* destructor */
 	void (*exit)(struct io_context *); /* called on task exit */
@@ -77,6 +78,7 @@ struct io_context {
 
 	struct as_io_context *aic;
 	struct radix_tree_root radix_root;
+	struct hlist_head cic_list;
 	void *ioc_data;
 };
 

commit 4ac845a2e9a816ed5a7b301f56dcc0a3d0b1ba4d
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Thu Jan 24 08:44:49 2008 +0100

    block: cfq: make the io contect sharing lockless
    
    The io context sharing introduced a per-ioc spinlock, that would protect
    the cfq io context lookup. That is a regression from the original, since
    we never needed any locking there because the ioc/cic were process private.
    
    The cic lookup is changed from an rbtree construct to a radix tree, which
    we can then use RCU to make the reader side lockless. That is the performance
    critical path, modifying the radix tree is only done on process creation
    (when that process first does IO, actually) and on process exit (if that
    process has done IO).
    
    As it so happens, radix trees are also much faster for this type of
    lookup where the key is a pointer. It's a very sparse tree.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/iocontext.h b/include/linux/iocontext.h
index cd44d458124a..593b222d9dcc 100644
--- a/include/linux/iocontext.h
+++ b/include/linux/iocontext.h
@@ -1,6 +1,8 @@
 #ifndef IOCONTEXT_H
 #define IOCONTEXT_H
 
+#include <linux/radix-tree.h>
+
 /*
  * This is the per-process anticipatory I/O scheduler state.
  */
@@ -29,8 +31,8 @@ struct as_io_context {
 
 struct cfq_queue;
 struct cfq_io_context {
-	struct rb_node rb_node;
 	void *key;
+	unsigned long dead_key;
 
 	struct cfq_queue *cfqq[2];
 
@@ -74,7 +76,7 @@ struct io_context {
 	int nr_batch_requests;     /* Number of requests left in the batch */
 
 	struct as_io_context *aic;
-	struct rb_root cic_root;
+	struct radix_tree_root radix_root;
 	void *ioc_data;
 };
 

commit d38ecf935fcb10264a6bc190855d9595165e6eeb
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Thu Jan 24 08:53:35 2008 +0100

    io context sharing: preliminary support
    
    Detach task state from ioc, instead keep track of how many processes
    are accessing the ioc.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/iocontext.h b/include/linux/iocontext.h
index 186807ea62e2..cd44d458124a 100644
--- a/include/linux/iocontext.h
+++ b/include/linux/iocontext.h
@@ -54,13 +54,15 @@ struct cfq_io_context {
 };
 
 /*
- * This is the per-process I/O subsystem state.  It is refcounted and
- * kmalloc'ed. Currently all fields are modified in process io context
- * (apart from the atomic refcount), so require no locking.
+ * I/O subsystem state of the associated processes.  It is refcounted
+ * and kmalloc'ed. These could be shared between processes.
  */
 struct io_context {
 	atomic_t refcount;
-	struct task_struct *task;
+	atomic_t nr_tasks;
+
+	/* all the fields below are protected by this lock */
+	spinlock_t lock;
 
 	unsigned short ioprio;
 	unsigned short ioprio_changed;
@@ -76,4 +78,16 @@ struct io_context {
 	void *ioc_data;
 };
 
+static inline struct io_context *ioc_task_link(struct io_context *ioc)
+{
+	/*
+	 * if ref count is zero, don't allow sharing (ioc is going away, it's
+	 * a race).
+	 */
+	if (ioc && atomic_inc_not_zero(&ioc->refcount))
+		return ioc;
+
+	return NULL;
+}
+
 #endif

commit fd0928df98b9578be8a786ac0cb78a47a5e17a20
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Thu Jan 24 08:52:45 2008 +0100

    ioprio: move io priority from task_struct to io_context
    
    This is where it belongs and then it doesn't take up space for a
    process that doesn't do IO.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/iocontext.h b/include/linux/iocontext.h
new file mode 100644
index 000000000000..186807ea62e2
--- /dev/null
+++ b/include/linux/iocontext.h
@@ -0,0 +1,79 @@
+#ifndef IOCONTEXT_H
+#define IOCONTEXT_H
+
+/*
+ * This is the per-process anticipatory I/O scheduler state.
+ */
+struct as_io_context {
+	spinlock_t lock;
+
+	void (*dtor)(struct as_io_context *aic); /* destructor */
+	void (*exit)(struct as_io_context *aic); /* called on task exit */
+
+	unsigned long state;
+	atomic_t nr_queued; /* queued reads & sync writes */
+	atomic_t nr_dispatched; /* number of requests gone to the drivers */
+
+	/* IO History tracking */
+	/* Thinktime */
+	unsigned long last_end_request;
+	unsigned long ttime_total;
+	unsigned long ttime_samples;
+	unsigned long ttime_mean;
+	/* Layout pattern */
+	unsigned int seek_samples;
+	sector_t last_request_pos;
+	u64 seek_total;
+	sector_t seek_mean;
+};
+
+struct cfq_queue;
+struct cfq_io_context {
+	struct rb_node rb_node;
+	void *key;
+
+	struct cfq_queue *cfqq[2];
+
+	struct io_context *ioc;
+
+	unsigned long last_end_request;
+	sector_t last_request_pos;
+
+	unsigned long ttime_total;
+	unsigned long ttime_samples;
+	unsigned long ttime_mean;
+
+	unsigned int seek_samples;
+	u64 seek_total;
+	sector_t seek_mean;
+
+	struct list_head queue_list;
+
+	void (*dtor)(struct io_context *); /* destructor */
+	void (*exit)(struct io_context *); /* called on task exit */
+};
+
+/*
+ * This is the per-process I/O subsystem state.  It is refcounted and
+ * kmalloc'ed. Currently all fields are modified in process io context
+ * (apart from the atomic refcount), so require no locking.
+ */
+struct io_context {
+	atomic_t refcount;
+	struct task_struct *task;
+
+	unsigned short ioprio;
+	unsigned short ioprio_changed;
+
+	/*
+	 * For request batching
+	 */
+	unsigned long last_waited; /* Time last woken after wait for request */
+	int nr_batch_requests;     /* Number of requests left in the batch */
+
+	struct as_io_context *aic;
+	struct rb_root cic_root;
+	void *ioc_data;
+};
+
+#endif
