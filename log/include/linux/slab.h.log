commit 2370ae4b1d5aa7eb70bd7539a420e791d4b0123b
Author: Mauro Carvalho Chehab <mchehab+huawei@kernel.org>
Date:   Fri Apr 10 14:32:25 2020 -0700

    docs: mm: slab.h: fix a broken cross-reference
    
    There is a typo at the cross-reference link, causing this warning:
    
      include/linux/slab.h:11: WARNING: undefined label: memory-allocation (if the link has no caption the label must precede a section header)
    
    Signed-off-by: Mauro Carvalho Chehab <mchehab+huawei@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Link: http://lkml.kernel.org/r/0aeac24235d356ebd935d11e147dcc6edbb6465c.1586359676.git.mchehab+huawei@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 03a389358562..6d454886bcaf 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -501,7 +501,7 @@ static __always_inline void *kmalloc_large(size_t size, gfp_t flags)
  * :ref:`Documentation/core-api/mm-api.rst <mm-api-gfp-flags>`
  *
  * The recommended usage of the @flags is described at
- * :ref:`Documentation/core-api/memory-allocation.rst <memory-allocation>`
+ * :ref:`Documentation/core-api/memory-allocation.rst <memory_allocation>`
  *
  * Below is a brief outline of the most useful GFP flags
  *

commit 1c948715a159d0d02c1e1c9228327ba3c408795c
Author: Florian Westphal <fw@strlen.de>
Date:   Mon Feb 3 17:34:58 2020 -0800

    mm: remove __krealloc
    
    Since 5.5-rc1 the last user of this function is gone, so remove the
    functionality.
    
    See commit
    2ad9d7747c10 ("netfilter: conntrack: free extension area immediately")
    for details.
    
    Link: http://lkml.kernel.org/r/20191212223442.22141-1-fw@strlen.de
    Signed-off-by: Florian Westphal <fw@strlen.de>
    Acked-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: David Rientjes <rientjes@google.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 877a95c6a2d2..03a389358562 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -184,7 +184,6 @@ void memcg_deactivate_kmem_caches(struct mem_cgroup *, struct mem_cgroup *);
 /*
  * Common kmalloc functions provided by all allocators
  */
-void * __must_check __krealloc(const void *, size_t, gfp_t);
 void * __must_check krealloc(const void *, size_t, gfp_t);
 void kfree(const void *);
 void kzfree(const void *);

commit dc0a7f7558dd52e972408ebb535b0153c06d08c2
Author: Pengfei Li <lpf.vector@gmail.com>
Date:   Sat Nov 30 17:49:25 2019 -0800

    mm, slab: remove unused kmalloc_size()
    
    The size of kmalloc can be obtained from kmalloc_info[], so remove
    kmalloc_size() that will not be used anymore.
    
    Link: http://lkml.kernel.org/r/1569241648-26908-3-git-send-email-lpf.vector@gmail.com
    Signed-off-by: Pengfei Li <lpf.vector@gmail.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Acked-by: Roman Gushchin <guro@fb.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 4d2a2fa55ed5..877a95c6a2d2 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -561,26 +561,6 @@ static __always_inline void *kmalloc(size_t size, gfp_t flags)
 	return __kmalloc(size, flags);
 }
 
-/*
- * Determine size used for the nth kmalloc cache.
- * return size or 0 if a kmalloc cache for that
- * size does not exist
- */
-static __always_inline unsigned int kmalloc_size(unsigned int n)
-{
-#ifndef CONFIG_SLOB
-	if (n > 2)
-		return 1U << n;
-
-	if (n == 1 && KMALLOC_MIN_SIZE <= 32)
-		return 96;
-
-	if (n == 2 && KMALLOC_MIN_SIZE <= 64)
-		return 192;
-#endif
-	return 0;
-}
-
 static __always_inline void *kmalloc_node(size_t size, gfp_t flags, int node)
 {
 #ifndef CONFIG_SLOB

commit 59bb47985c1db229ccff8c5deebecd54fc77d2a9
Author: Vlastimil Babka <vbabka@suse.cz>
Date:   Sun Oct 6 17:58:45 2019 -0700

    mm, sl[aou]b: guarantee natural alignment for kmalloc(power-of-two)
    
    In most configurations, kmalloc() happens to return naturally aligned
    (i.e.  aligned to the block size itself) blocks for power of two sizes.
    
    That means some kmalloc() users might unknowingly rely on that
    alignment, until stuff breaks when the kernel is built with e.g.
    CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned.  Then
    developers have to devise workaround such as own kmem caches with
    specified alignment [1], which is not always practical, as recently
    evidenced in [2].
    
    The topic has been discussed at LSF/MM 2019 [3].  Adding a
    'kmalloc_aligned()' variant would not help with code unknowingly relying
    on the implicit alignment.  For slab implementations it would either
    require creating more kmalloc caches, or allocate a larger size and only
    give back part of it.  That would be wasteful, especially with a generic
    alignment parameter (in contrast with a fixed alignment to size).
    
    Ideally we should provide to mm users what they need without difficult
    workarounds or own reimplementations, so let's make the kmalloc()
    alignment to size explicitly guaranteed for power-of-two sizes under all
    configurations.  What this means for the three available allocators?
    
    * SLAB object layout happens to be mostly unchanged by the patch.  The
      implicitly provided alignment could be compromised with
      CONFIG_DEBUG_SLAB due to redzoning, however SLAB disables redzoning for
      caches with alignment larger than unsigned long long.  Practically on at
      least x86 this includes kmalloc caches as they use cache line alignment,
      which is larger than that.  Still, this patch ensures alignment on all
      arches and cache sizes.
    
    * SLUB layout is also unchanged unless redzoning is enabled through
      CONFIG_SLUB_DEBUG and boot parameter for the particular kmalloc cache.
      With this patch, explicit alignment is guaranteed with redzoning as
      well.  This will result in more memory being wasted, but that should be
      acceptable in a debugging scenario.
    
    * SLOB has no implicit alignment so this patch adds it explicitly for
      kmalloc().  The potential downside is increased fragmentation.  While
      pathological allocation scenarios are certainly possible, in my testing,
      after booting a x86_64 kernel+userspace with virtme, around 16MB memory
      was consumed by slab pages both before and after the patch, with
      difference in the noise.
    
    [1] https://lore.kernel.org/linux-btrfs/c3157c8e8e0e7588312b40c853f65c02fe6c957a.1566399731.git.christophe.leroy@c-s.fr/
    [2] https://lore.kernel.org/linux-fsdevel/20190225040904.5557-1-ming.lei@redhat.com/
    [3] https://lwn.net/Articles/787740/
    
    [akpm@linux-foundation.org: documentation fixlet, per Matthew]
    Link: http://lkml.kernel.org/r/20190826111627.7505-3-vbabka@suse.cz
    Signed-off-by: Vlastimil Babka <vbabka@suse.cz>
    Reviewed-by: Matthew Wilcox (Oracle) <willy@infradead.org>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Acked-by: Christoph Hellwig <hch@lst.de>
    Cc: David Sterba <dsterba@suse.cz>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Ming Lei <ming.lei@redhat.com>
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: "Darrick J . Wong" <darrick.wong@oracle.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: James Bottomley <James.Bottomley@HansenPartnership.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index ab2b98ad76e1..4d2a2fa55ed5 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -493,6 +493,10 @@ static __always_inline void *kmalloc_large(size_t size, gfp_t flags)
  * kmalloc is the normal method of allocating memory
  * for objects smaller than page size in the kernel.
  *
+ * The allocated object address is aligned to at least ARCH_KMALLOC_MINALIGN
+ * bytes. For @size of power of two bytes, the alignment is also guaranteed
+ * to be at least to the size.
+ *
  * The @flags argument may be one of the GFP flags defined at
  * include/linux/gfp.h and described at
  * :ref:`Documentation/core-api/mm-api.rst <mm-api-gfp-flags>`

commit 9adeaa226988b97bc15928e12f40a9863134467c
Author: Waiman Long <longman@redhat.com>
Date:   Mon Sep 23 15:33:49 2019 -0700

    mm, slab: move memcg_cache_params structure to mm/slab.h
    
    The memcg_cache_params structure is only embedded into the kmem_cache of
    slab and slub allocators as defined in slab_def.h and slub_def.h and used
    internally by mm code.  There is no needed to expose it in a public
    header.  So move it from include/linux/slab.h to mm/slab.h.  It is just a
    refactoring patch with no code change.
    
    In fact both the slub_def.h and slab_def.h should be moved into the mm
    directory as well, but that will probably cause many merge conflicts.
    
    Link: http://lkml.kernel.org/r/20190718180827.18758-1-longman@redhat.com
    Signed-off-by: Waiman Long <longman@redhat.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Roman Gushchin <guro@fb.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Shakeel Butt <shakeelb@google.com>
    Cc: Vladimir Davydov <vdavydov.dev@gmail.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 56c9c7eed34e..ab2b98ad76e1 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -595,68 +595,6 @@ static __always_inline void *kmalloc_node(size_t size, gfp_t flags, int node)
 	return __kmalloc_node(size, flags, node);
 }
 
-struct memcg_cache_array {
-	struct rcu_head rcu;
-	struct kmem_cache *entries[0];
-};
-
-/*
- * This is the main placeholder for memcg-related information in kmem caches.
- * Both the root cache and the child caches will have it. For the root cache,
- * this will hold a dynamically allocated array large enough to hold
- * information about the currently limited memcgs in the system. To allow the
- * array to be accessed without taking any locks, on relocation we free the old
- * version only after a grace period.
- *
- * Root and child caches hold different metadata.
- *
- * @root_cache:	Common to root and child caches.  NULL for root, pointer to
- *		the root cache for children.
- *
- * The following fields are specific to root caches.
- *
- * @memcg_caches: kmemcg ID indexed table of child caches.  This table is
- *		used to index child cachces during allocation and cleared
- *		early during shutdown.
- *
- * @root_caches_node: List node for slab_root_caches list.
- *
- * @children:	List of all child caches.  While the child caches are also
- *		reachable through @memcg_caches, a child cache remains on
- *		this list until it is actually destroyed.
- *
- * The following fields are specific to child caches.
- *
- * @memcg:	Pointer to the memcg this cache belongs to.
- *
- * @children_node: List node for @root_cache->children list.
- *
- * @kmem_caches_node: List node for @memcg->kmem_caches list.
- */
-struct memcg_cache_params {
-	struct kmem_cache *root_cache;
-	union {
-		struct {
-			struct memcg_cache_array __rcu *memcg_caches;
-			struct list_head __root_caches_node;
-			struct list_head children;
-			bool dying;
-		};
-		struct {
-			struct mem_cgroup *memcg;
-			struct list_head children_node;
-			struct list_head kmem_caches_node;
-			struct percpu_ref refcnt;
-
-			void (*work_fn)(struct kmem_cache *);
-			union {
-				struct rcu_head rcu_head;
-				struct work_struct work;
-			};
-		};
-	};
-};
-
 int memcg_update_all_caches(int num_memcgs);
 
 /**

commit fcf8a1e483490cd249df4e02d5425636c3f43c86
Author: Waiman Long <longman@redhat.com>
Date:   Thu Jul 11 20:56:38 2019 -0700

    mm, memcg: add a memcg_slabinfo debugfs file
    
    There are concerns about memory leaks from extensive use of memory cgroups
    as each memory cgroup creates its own set of kmem caches.  There is a
    possiblity that the memcg kmem caches may remain even after the memory
    cgroups have been offlined.  Therefore, it will be useful to show the
    status of each of memcg kmem caches.
    
    This patch introduces a new <debugfs>/memcg_slabinfo file which is
    somewhat similar to /proc/slabinfo in format, but lists only information
    about kmem caches that have child memcg kmem caches.  Information
    available in /proc/slabinfo are not repeated in memcg_slabinfo.
    
    A portion of a sample output of the file was:
    
      # <name> <css_id[:dead]> <active_objs> <num_objs> <active_slabs> <num_slabs>
      rpc_inode_cache   root          13     51      1      1
      rpc_inode_cache     48           0      0      0      0
      fat_inode_cache   root           1     45      1      1
      fat_inode_cache     41           2     45      1      1
      xfs_inode         root         770    816     24     24
      xfs_inode           92          22     34      1      1
      xfs_inode           88:dead      1     34      1      1
      xfs_inode           89:dead     23     34      1      1
      xfs_inode           85           4     34      1      1
      xfs_inode           84           9     34      1      1
    
    The css id of the memcg is also listed. If a memcg is not online,
    the tag ":dead" will be attached as shown above.
    
    [longman@redhat.com: memcg: add ":deact" tag for reparented kmem caches in memcg_slabinfo]
      Link: http://lkml.kernel.org/r/20190621173005.31514-1-longman@redhat.com
    [longman@redhat.com: set the flag in the common code as suggested by Roman]
      Link: http://lkml.kernel.org/r/20190627184324.5875-1-longman@redhat.com
    Link: http://lkml.kernel.org/r/20190619171621.26209-1-longman@redhat.com
    Signed-off-by: Waiman Long <longman@redhat.com>
    Suggested-by: Shakeel Butt <shakeelb@google.com>
    Reviewed-by: Shakeel Butt <shakeelb@google.com>
    Acked-by: Roman Gushchin <guro@fb.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Vladimir Davydov <vdavydov.dev@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index fd0ef2e16178..56c9c7eed34e 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -116,6 +116,10 @@
 /* Objects are reclaimable */
 #define SLAB_RECLAIM_ACCOUNT	((slab_flags_t __force)0x00020000U)
 #define SLAB_TEMPORARY		SLAB_RECLAIM_ACCOUNT	/* Objects are short-lived */
+
+/* Slab deactivation flag */
+#define SLAB_DEACTIVATED	((slab_flags_t __force)0x10000000U)
+
 /*
  * ZERO_SIZE_PTR will be returned for zero sized kmalloc requests.
  *

commit fb2f2b0adb98bbbbbb51c5a5327f3f90f5dc417e
Author: Roman Gushchin <guro@fb.com>
Date:   Thu Jul 11 20:56:34 2019 -0700

    mm: memcg/slab: reparent memcg kmem_caches on cgroup removal
    
    Let's reparent non-root kmem_caches on memcg offlining.  This allows us to
    release the memory cgroup without waiting for the last outstanding kernel
    object (e.g.  dentry used by another application).
    
    Since the parent cgroup is already charged, everything we need to do is to
    splice the list of kmem_caches to the parent's kmem_caches list, swap the
    memcg pointer, drop the css refcounter for each kmem_cache and adjust the
    parent's css refcounter.
    
    Please, note that kmem_cache->memcg_params.memcg isn't a stable pointer
    anymore.  It's safe to read it under rcu_read_lock(), cgroup_mutex held,
    or any other way that protects the memory cgroup from being released.
    
    We can race with the slab allocation and deallocation paths.  It's not a
    big problem: parent's charge and slab global stats are always correct, and
    we don't care anymore about the child usage and global stats.  The child
    cgroup is already offline, so we don't use or show it anywhere.
    
    Local slab stats (NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE) aren't
    used anywhere except count_shadow_nodes().  But even there it won't break
    anything: after reparenting "nodes" will be 0 on child level (because
    we're already reparenting shrinker lists), and on parent level page stats
    always were 0, and this patch won't change anything.
    
    [guro@fb.com: properly handle kmem_caches reparented to root_mem_cgroup]
      Link: http://lkml.kernel.org/r/20190620213427.1691847-1-guro@fb.com
    Link: http://lkml.kernel.org/r/20190611231813.3148843-11-guro@fb.com
    Signed-off-by: Roman Gushchin <guro@fb.com>
    Acked-by: Vladimir Davydov <vdavydov.dev@gmail.com>
    Reviewed-by: Shakeel Butt <shakeelb@google.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Waiman Long <longman@redhat.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Andrei Vagin <avagin@gmail.com>
    Cc: Qian Cai <cai@lca.pw>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index bc189a43e680..fd0ef2e16178 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -152,7 +152,7 @@ void kmem_cache_destroy(struct kmem_cache *);
 int kmem_cache_shrink(struct kmem_cache *);
 
 void memcg_create_kmem_cache(struct mem_cgroup *, struct kmem_cache *);
-void memcg_deactivate_kmem_caches(struct mem_cgroup *);
+void memcg_deactivate_kmem_caches(struct mem_cgroup *, struct mem_cgroup *);
 
 /*
  * Please use this macro to create slab caches. Simply specify the

commit f0a3a24b532d9a7e56a33c5112b2a212ed6ec580
Author: Roman Gushchin <guro@fb.com>
Date:   Thu Jul 11 20:56:27 2019 -0700

    mm: memcg/slab: rework non-root kmem_cache lifecycle management
    
    Currently each charged slab page holds a reference to the cgroup to which
    it's charged.  Kmem_caches are held by the memcg and are released all
    together with the memory cgroup.  It means that none of kmem_caches are
    released unless at least one reference to the memcg exists, which is very
    far from optimal.
    
    Let's rework it in a way that allows releasing individual kmem_caches as
    soon as the cgroup is offline, the kmem_cache is empty and there are no
    pending allocations.
    
    To make it possible, let's introduce a new percpu refcounter for non-root
    kmem caches.  The counter is initialized to the percpu mode, and is
    switched to the atomic mode during kmem_cache deactivation.  The counter
    is bumped for every charged page and also for every running allocation.
    So the kmem_cache can't be released unless all allocations complete.
    
    To shutdown non-active empty kmem_caches, let's reuse the work queue,
    previously used for the kmem_cache deactivation.  Once the reference
    counter reaches 0, let's schedule an asynchronous kmem_cache release.
    
    * I used the following simple approach to test the performance
    (stolen from another patchset by T. Harding):
    
        time find / -name fname-no-exist
        echo 2 > /proc/sys/vm/drop_caches
        repeat 10 times
    
    Results:
    
            orig            patched
    
    real    0m1.455s        real    0m1.355s
    user    0m0.206s        user    0m0.219s
    sys     0m0.855s        sys     0m0.807s
    
    real    0m1.487s        real    0m1.699s
    user    0m0.221s        user    0m0.256s
    sys     0m0.806s        sys     0m0.948s
    
    real    0m1.515s        real    0m1.505s
    user    0m0.183s        user    0m0.215s
    sys     0m0.876s        sys     0m0.858s
    
    real    0m1.291s        real    0m1.380s
    user    0m0.193s        user    0m0.198s
    sys     0m0.843s        sys     0m0.786s
    
    real    0m1.364s        real    0m1.374s
    user    0m0.180s        user    0m0.182s
    sys     0m0.868s        sys     0m0.806s
    
    real    0m1.352s        real    0m1.312s
    user    0m0.201s        user    0m0.212s
    sys     0m0.820s        sys     0m0.761s
    
    real    0m1.302s        real    0m1.349s
    user    0m0.205s        user    0m0.203s
    sys     0m0.803s        sys     0m0.792s
    
    real    0m1.334s        real    0m1.301s
    user    0m0.194s        user    0m0.201s
    sys     0m0.806s        sys     0m0.779s
    
    real    0m1.426s        real    0m1.434s
    user    0m0.216s        user    0m0.181s
    sys     0m0.824s        sys     0m0.864s
    
    real    0m1.350s        real    0m1.295s
    user    0m0.200s        user    0m0.190s
    sys     0m0.842s        sys     0m0.811s
    
    So it looks like the difference is not noticeable in this test.
    
    [cai@lca.pw: fix an use-after-free in kmemcg_workfn()]
      Link: http://lkml.kernel.org/r/1560977573-10715-1-git-send-email-cai@lca.pw
    Link: http://lkml.kernel.org/r/20190611231813.3148843-9-guro@fb.com
    Signed-off-by: Roman Gushchin <guro@fb.com>
    Signed-off-by: Qian Cai <cai@lca.pw>
    Acked-by: Vladimir Davydov <vdavydov.dev@gmail.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Shakeel Butt <shakeelb@google.com>
    Cc: Waiman Long <longman@redhat.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Andrei Vagin <avagin@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 6008d884e621..bc189a43e680 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -16,6 +16,7 @@
 #include <linux/overflow.h>
 #include <linux/types.h>
 #include <linux/workqueue.h>
+#include <linux/percpu-refcount.h>
 
 
 /*
@@ -152,7 +153,6 @@ int kmem_cache_shrink(struct kmem_cache *);
 
 void memcg_create_kmem_cache(struct mem_cgroup *, struct kmem_cache *);
 void memcg_deactivate_kmem_caches(struct mem_cgroup *);
-void memcg_destroy_kmem_caches(struct mem_cgroup *);
 
 /*
  * Please use this macro to create slab caches. Simply specify the
@@ -642,6 +642,7 @@ struct memcg_cache_params {
 			struct mem_cgroup *memcg;
 			struct list_head children_node;
 			struct list_head kmem_caches_node;
+			struct percpu_ref refcnt;
 
 			void (*work_fn)(struct kmem_cache *);
 			union {

commit 0b14e8aa68223c2c124d408aa4b110b364d13c53
Author: Roman Gushchin <guro@fb.com>
Date:   Thu Jul 11 20:56:06 2019 -0700

    mm: memcg/slab: rename slab delayed deactivation functions and fields
    
    The delayed work/rcu deactivation infrastructure of non-root kmem_caches
    can be also used for asynchronous release of these objects.  Let's get rid
    of the word "deactivation" in corresponding names to make the code look
    better after generalization.
    
    It's easier to make the renaming first, so that the generalized code will
    look consistent from scratch.
    
    Let's rename struct memcg_cache_params fields:
      deact_fn -> work_fn
      deact_rcu_head -> rcu_head
      deact_work -> work
    
    And RCU/delayed work callbacks in slab common code:
      kmemcg_deactivate_rcufn -> kmemcg_rcufn
      kmemcg_deactivate_workfn -> kmemcg_workfn
    
    This patch contains no functional changes, only renamings.
    
    Link: http://lkml.kernel.org/r/20190611231813.3148843-3-guro@fb.com
    Signed-off-by: Roman Gushchin <guro@fb.com>
    Acked-by: Vladimir Davydov <vdavydov.dev@gmail.com>
    Reviewed-by: Shakeel Butt <shakeelb@google.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Waiman Long <longman@redhat.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Andrei Vagin <avagin@gmail.com>
    Cc: Qian Cai <cai@lca.pw>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 98c3d12b7275..6008d884e621 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -643,10 +643,10 @@ struct memcg_cache_params {
 			struct list_head children_node;
 			struct list_head kmem_caches_node;
 
-			void (*deact_fn)(struct kmem_cache *);
+			void (*work_fn)(struct kmem_cache *);
 			union {
-				struct rcu_head deact_rcu_head;
-				struct work_struct deact_work;
+				struct rcu_head rcu_head;
+				struct work_struct work;
 			};
 		};
 	};

commit 10d1f8cb3965a6f633bf23eb984cda552927e3a5
Author: Marco Elver <elver@google.com>
Date:   Thu Jul 11 20:54:14 2019 -0700

    mm/slab: refactor common ksize KASAN logic into slab_common.c
    
    This refactors common code of ksize() between the various allocators into
    slab_common.c: __ksize() is the allocator-specific implementation without
    instrumentation, whereas ksize() includes the required KASAN logic.
    
    Link: http://lkml.kernel.org/r/20190626142014.141844-5-elver@google.com
    Signed-off-by: Marco Elver <elver@google.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Andrey Konovalov <andreyknvl@google.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Kees Cook <keescook@chromium.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 9449b19c5f10..98c3d12b7275 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -184,6 +184,7 @@ void * __must_check __krealloc(const void *, size_t, gfp_t);
 void * __must_check krealloc(const void *, size_t, gfp_t);
 void kfree(const void *);
 void kzfree(const void *);
+size_t __ksize(const void *);
 size_t ksize(const void *);
 
 #ifdef CONFIG_HAVE_HARDENED_USERCOPY_ALLOCATOR

commit 6d6ea1e967a246f12cfe2f5fb743b70b2e608d4a
Author: Nicolas Boichat <drinkcat@chromium.org>
Date:   Thu Mar 28 20:43:42 2019 -0700

    mm: add support for kmem caches in DMA32 zone
    
    Patch series "iommu/io-pgtable-arm-v7s: Use DMA32 zone for page tables",
    v6.
    
    This is a followup to the discussion in [1], [2].
    
    IOMMUs using ARMv7 short-descriptor format require page tables (level 1
    and 2) to be allocated within the first 4GB of RAM, even on 64-bit
    systems.
    
    For L1 tables that are bigger than a page, we can just use
    __get_free_pages with GFP_DMA32 (on arm64 systems only, arm would still
    use GFP_DMA).
    
    For L2 tables that only take 1KB, it would be a waste to allocate a full
    page, so we considered 3 approaches:
     1. This series, adding support for GFP_DMA32 slab caches.
     2. genalloc, which requires pre-allocating the maximum number of L2 page
        tables (4096, so 4MB of memory).
     3. page_frag, which is not very memory-efficient as it is unable to reuse
        freed fragments until the whole page is freed. [3]
    
    This series is the most memory-efficient approach.
    
    stable@ note:
      We confirmed that this is a regression, and IOMMU errors happen on 4.19
      and linux-next/master on MT8173 (elm, Acer Chromebook R13). The issue
      most likely starts from commit ad67f5a6545f ("arm64: replace ZONE_DMA
      with ZONE_DMA32"), i.e. 4.15, and presumably breaks a number of Mediatek
      platforms (and maybe others?).
    
    [1] https://lists.linuxfoundation.org/pipermail/iommu/2018-November/030876.html
    [2] https://lists.linuxfoundation.org/pipermail/iommu/2018-December/031696.html
    [3] https://patchwork.codeaurora.org/patch/671639/
    
    This patch (of 3):
    
    IOMMUs using ARMv7 short-descriptor format require page tables to be
    allocated within the first 4GB of RAM, even on 64-bit systems.  On arm64,
    this is done by passing GFP_DMA32 flag to memory allocation functions.
    
    For IOMMU L2 tables that only take 1KB, it would be a waste to allocate
    a full page using get_free_pages, so we considered 3 approaches:
     1. This patch, adding support for GFP_DMA32 slab caches.
     2. genalloc, which requires pre-allocating the maximum number of L2
        page tables (4096, so 4MB of memory).
     3. page_frag, which is not very memory-efficient as it is unable
        to reuse freed fragments until the whole page is freed.
    
    This change makes it possible to create a custom cache in DMA32 zone using
    kmem_cache_create, then allocate memory using kmem_cache_alloc.
    
    We do not create a DMA32 kmalloc cache array, as there are currently no
    users of kmalloc(..., GFP_DMA32).  These calls will continue to trigger a
    warning, as we keep GFP_DMA32 in GFP_SLAB_BUG_MASK.
    
    This implies that calls to kmem_cache_*alloc on a SLAB_CACHE_DMA32
    kmem_cache must _not_ use GFP_DMA32 (it is anyway redundant and
    unnecessary).
    
    Link: http://lkml.kernel.org/r/20181210011504.122604-2-drinkcat@chromium.org
    Signed-off-by: Nicolas Boichat <drinkcat@chromium.org>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Cc: Robin Murphy <robin.murphy@arm.com>
    Cc: Joerg Roedel <joro@8bytes.org>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Sasha Levin <Alexander.Levin@microsoft.com>
    Cc: Huaisheng Ye <yehs1@lenovo.com>
    Cc: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Cc: Yong Wu <yong.wu@mediatek.com>
    Cc: Matthias Brugger <matthias.bgg@gmail.com>
    Cc: Tomasz Figa <tfiga@google.com>
    Cc: Yingjoe Chen <yingjoe.chen@mediatek.com>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Hsin-Yi Wang <hsinyi@chromium.org>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 11b45f7ae405..9449b19c5f10 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -32,6 +32,8 @@
 #define SLAB_HWCACHE_ALIGN	((slab_flags_t __force)0x00002000U)
 /* Use GFP_DMA memory */
 #define SLAB_CACHE_DMA		((slab_flags_t __force)0x00004000U)
+/* Use GFP_DMA32 memory */
+#define SLAB_CACHE_DMA32	((slab_flags_t __force)0x00008000U)
 /* DEBUG: Store the last owner for bug hunting */
 #define SLAB_STORE_USER		((slab_flags_t __force)0x00010000U)
 /* Panic if kmem_cache_create() fails */

commit 3868772b99e3146d02cf47e739d79022eba1d77c
Merge: 6f9d71c9c759 942104a21ce4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Dec 29 11:21:49 2018 -0800

    Merge tag 'docs-5.0' of git://git.lwn.net/linux
    
    Pull documentation update from Jonathan Corbet:
     "A fairly normal cycle for documentation stuff. We have a new document
      on perf security, more Italian translations, more improvements to the
      memory-management docs, improvements to the pathname lookup
      documentation, and the usual array of smaller fixes.
    
      As is often the case, there are a few reaches outside of
      Documentation/ to adjust kerneldoc comments"
    
    * tag 'docs-5.0' of git://git.lwn.net/linux: (38 commits)
      docs: improve pathname-lookup document structure
      configfs: fix wrong name of struct in documentation
      docs/mm-api: link slab_common.c to "The Slab Cache" section
      slab: make kmem_cache_create{_usercopy} description proper kernel-doc
      doc:process: add links where missing
      docs/core-api: make mm-api.rst more structured
      x86, boot: documentation whitespace fixup
      Documentation: devres: note checking needs when converting
      doc:it: add some process/* translations
      doc:it: fixes in process/1.Intro
      Documentation: convert path-lookup from markdown to resturctured text
      Documentation/admin-guide: update admin-guide index.rst
      Documentation/admin-guide: introduce perf-security.rst file
      scripts/kernel-doc: Fix struct and struct field attribute processing
      Documentation: dev-tools: Fix typos in index.rst
      Correct gen_init_cpio tool's documentation
      Document /proc/pid PID reuse behavior
      Documentation: update path-lookup.md for parallel lookups
      Documentation: Use "while" instead of "whilst"
      dmaengine: Add mailing list address to the documentation
      ...

commit 4e45f712d82c6b7a37e02faf388173ad12ab464d
Author: Vlastimil Babka <vbabka@suse.cz>
Date:   Fri Dec 28 00:33:17 2018 -0800

    include/linux/slab.h: fix sparse warning in kmalloc_type()
    
    Multiple people have reported the following sparse warning:
    
    ./include/linux/slab.h:332:43: warning: dubious: x & !y
    
    The minimal fix would be to change the logical & to boolean &&, which
    emits the same code, but Andrew has suggested that the branch-avoiding
    tricks are maybe not worthwile.  David Laight provided a nice comparison
    of disassembly of multiple variants, which shows that the current version
    produces a 4 deep dependency chain, and fixing the sparse warning by
    changing logical and to multiplication emits an IMUL, making it even more
    expensive.
    
    The code as rewritten by this patch yielded the best disassembly, with a
    single predictable branch for the most common case, and a ternary operator
    for the rest, which gcc seems to compile without a branch or cmov by
    itself.
    
    The result should be more readable, without a sparse warning and probably
    also faster for the common case.
    
    Link: http://lkml.kernel.org/r/80340595-d7c5-97b9-4f6c-23fa893a91e9@suse.cz
    Fixes: 1291523f2c1d ("mm, slab/slub: introduce kmalloc-reclaimable caches")
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Vlastimil Babka <vbabka@suse.cz>
    Reported-by: Bart Van Assche <bvanassche@acm.org>
    Reported-by: Darryl T. Agostinelli <dagostinelli@gmail.com>
    Reported-by: Masahiro Yamada <yamada.masahiro@socionext.com>
    Suggested-by: Andrew Morton <akpm@linux-foundation.org>
    Suggested-by: David Laight <David.Laight@ACULAB.COM>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 351ac48dabc4..6d9bd6fc0c57 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -314,22 +314,22 @@ kmalloc_caches[NR_KMALLOC_TYPES][KMALLOC_SHIFT_HIGH + 1];
 
 static __always_inline enum kmalloc_cache_type kmalloc_type(gfp_t flags)
 {
-	int is_dma = 0;
-	int type_dma = 0;
-	int is_reclaimable;
-
 #ifdef CONFIG_ZONE_DMA
-	is_dma = !!(flags & __GFP_DMA);
-	type_dma = is_dma * KMALLOC_DMA;
-#endif
-
-	is_reclaimable = !!(flags & __GFP_RECLAIMABLE);
+	/*
+	 * The most common case is KMALLOC_NORMAL, so test for it
+	 * with a single branch for both flags.
+	 */
+	if (likely((flags & (__GFP_DMA | __GFP_RECLAIMABLE)) == 0))
+		return KMALLOC_NORMAL;
 
 	/*
-	 * If an allocation is both __GFP_DMA and __GFP_RECLAIMABLE, return
-	 * KMALLOC_DMA and effectively ignore __GFP_RECLAIMABLE
+	 * At least one of the flags has to be set. If both are, __GFP_DMA
+	 * is more important.
 	 */
-	return type_dma + (is_reclaimable & !is_dma) * KMALLOC_RECLAIM;
+	return flags & __GFP_DMA ? KMALLOC_DMA : KMALLOC_RECLAIM;
+#else
+	return flags & __GFP_RECLAIMABLE ? KMALLOC_RECLAIM : KMALLOC_NORMAL;
+#endif
 }
 
 /*

commit 0116523cfffa62aeb5aa3b85ce7419f3dae0c1b8
Author: Andrey Konovalov <andreyknvl@google.com>
Date:   Fri Dec 28 00:29:37 2018 -0800

    kasan, mm: change hooks signatures
    
    Patch series "kasan: add software tag-based mode for arm64", v13.
    
    This patchset adds a new software tag-based mode to KASAN [1].  (Initially
    this mode was called KHWASAN, but it got renamed, see the naming rationale
    at the end of this section).
    
    The plan is to implement HWASan [2] for the kernel with the incentive,
    that it's going to have comparable to KASAN performance, but in the same
    time consume much less memory, trading that off for somewhat imprecise bug
    detection and being supported only for arm64.
    
    The underlying ideas of the approach used by software tag-based KASAN are:
    
    1. By using the Top Byte Ignore (TBI) arm64 CPU feature, we can store
       pointer tags in the top byte of each kernel pointer.
    
    2. Using shadow memory, we can store memory tags for each chunk of kernel
       memory.
    
    3. On each memory allocation, we can generate a random tag, embed it into
       the returned pointer and set the memory tags that correspond to this
       chunk of memory to the same value.
    
    4. By using compiler instrumentation, before each memory access we can add
       a check that the pointer tag matches the tag of the memory that is being
       accessed.
    
    5. On a tag mismatch we report an error.
    
    With this patchset the existing KASAN mode gets renamed to generic KASAN,
    with the word "generic" meaning that the implementation can be supported
    by any architecture as it is purely software.
    
    The new mode this patchset adds is called software tag-based KASAN.  The
    word "tag-based" refers to the fact that this mode uses tags embedded into
    the top byte of kernel pointers and the TBI arm64 CPU feature that allows
    to dereference such pointers.  The word "software" here means that shadow
    memory manipulation and tag checking on pointer dereference is done in
    software.  As it is the only tag-based implementation right now, "software
    tag-based" KASAN is sometimes referred to as simply "tag-based" in this
    patchset.
    
    A potential expansion of this mode is a hardware tag-based mode, which
    would use hardware memory tagging support (announced by Arm [3]) instead
    of compiler instrumentation and manual shadow memory manipulation.
    
    Same as generic KASAN, software tag-based KASAN is strictly a debugging
    feature.
    
    [1] https://www.kernel.org/doc/html/latest/dev-tools/kasan.html
    
    [2] http://clang.llvm.org/docs/HardwareAssistedAddressSanitizerDesign.html
    
    [3] https://community.arm.com/processors/b/blog/posts/arm-a-profile-architecture-2018-developments-armv85a
    
    ====== Rationale
    
    On mobile devices generic KASAN's memory usage is significant problem.
    One of the main reasons to have tag-based KASAN is to be able to perform a
    similar set of checks as the generic one does, but with lower memory
    requirements.
    
    Comment from Vishwath Mohan <vishwath@google.com>:
    
    I don't have data on-hand, but anecdotally both ASAN and KASAN have proven
    problematic to enable for environments that don't tolerate the increased
    memory pressure well.  This includes
    
    (a) Low-memory form factors - Wear, TV, Things, lower-tier phones like Go,
    (c) Connected components like Pixel's visual core [1].
    
    These are both places I'd love to have a low(er) memory footprint option at
    my disposal.
    
    Comment from Evgenii Stepanov <eugenis@google.com>:
    
    Looking at a live Android device under load, slab (according to
    /proc/meminfo) + kernel stack take 8-10% available RAM (~350MB).  KASAN's
    overhead of 2x - 3x on top of it is not insignificant.
    
    Not having this overhead enables near-production use - ex.  running
    KASAN/KHWASAN kernel on a personal, daily-use device to catch bugs that do
    not reproduce in test configuration.  These are the ones that often cost
    the most engineering time to track down.
    
    CPU overhead is bad, but generally tolerable.  RAM is critical, in our
    experience.  Once it gets low enough, OOM-killer makes your life
    miserable.
    
    [1] https://www.blog.google/products/pixel/pixel-visual-core-image-processing-and-machine-learning-pixel-2/
    
    ====== Technical details
    
    Software tag-based KASAN mode is implemented in a very similar way to the
    generic one. This patchset essentially does the following:
    
    1. TCR_TBI1 is set to enable Top Byte Ignore.
    
    2. Shadow memory is used (with a different scale, 1:16, so each shadow
       byte corresponds to 16 bytes of kernel memory) to store memory tags.
    
    3. All slab objects are aligned to shadow scale, which is 16 bytes.
    
    4. All pointers returned from the slab allocator are tagged with a random
       tag and the corresponding shadow memory is poisoned with the same value.
    
    5. Compiler instrumentation is used to insert tag checks. Either by
       calling callbacks or by inlining them (CONFIG_KASAN_OUTLINE and
       CONFIG_KASAN_INLINE flags are reused).
    
    6. When a tag mismatch is detected in callback instrumentation mode
       KASAN simply prints a bug report. In case of inline instrumentation,
       clang inserts a brk instruction, and KASAN has it's own brk handler,
       which reports the bug.
    
    7. The memory in between slab objects is marked with a reserved tag, and
       acts as a redzone.
    
    8. When a slab object is freed it's marked with a reserved tag.
    
    Bug detection is imprecise for two reasons:
    
    1. We won't catch some small out-of-bounds accesses, that fall into the
       same shadow cell, as the last byte of a slab object.
    
    2. We only have 1 byte to store tags, which means we have a 1/256
       probability of a tag match for an incorrect access (actually even
       slightly less due to reserved tag values).
    
    Despite that there's a particular type of bugs that tag-based KASAN can
    detect compared to generic KASAN: use-after-free after the object has been
    allocated by someone else.
    
    ====== Testing
    
    Some kernel developers voiced a concern that changing the top byte of
    kernel pointers may lead to subtle bugs that are difficult to discover.
    To address this concern deliberate testing has been performed.
    
    It doesn't seem feasible to do some kind of static checking to find
    potential issues with pointer tagging, so a dynamic approach was taken.
    All pointer comparisons/subtractions have been instrumented in an LLVM
    compiler pass and a kernel module that would print a bug report whenever
    two pointers with different tags are being compared/subtracted (ignoring
    comparisons with NULL pointers and with pointers obtained by casting an
    error code to a pointer type) has been used.  Then the kernel has been
    booted in QEMU and on an Odroid C2 board and syzkaller has been run.
    
    This yielded the following results.
    
    The two places that look interesting are:
    
    is_vmalloc_addr in include/linux/mm.h
    is_kernel_rodata in mm/util.c
    
    Here we compare a pointer with some fixed untagged values to make sure
    that the pointer lies in a particular part of the kernel address space.
    Since tag-based KASAN doesn't add tags to pointers that belong to rodata
    or vmalloc regions, this should work as is.  To make sure debug checks to
    those two functions that check that the result doesn't change whether we
    operate on pointers with or without untagging has been added.
    
    A few other cases that don't look that interesting:
    
    Comparing pointers to achieve unique sorting order of pointee objects
    (e.g. sorting locks addresses before performing a double lock):
    
    tty_ldisc_lock_pair_timeout in drivers/tty/tty_ldisc.c
    pipe_double_lock in fs/pipe.c
    unix_state_double_lock in net/unix/af_unix.c
    lock_two_nondirectories in fs/inode.c
    mutex_lock_double in kernel/events/core.c
    
    ep_cmp_ffd in fs/eventpoll.c
    fsnotify_compare_groups fs/notify/mark.c
    
    Nothing needs to be done here, since the tags embedded into pointers
    don't change, so the sorting order would still be unique.
    
    Checks that a pointer belongs to some particular allocation:
    
    is_sibling_entry in lib/radix-tree.c
    object_is_on_stack in include/linux/sched/task_stack.h
    
    Nothing needs to be done here either, since two pointers can only belong
    to the same allocation if they have the same tag.
    
    Overall, since the kernel boots and works, there are no critical bugs.
    As for the rest, the traditional kernel testing way (use until fails) is
    the only one that looks feasible.
    
    Another point here is that tag-based KASAN is available under a separate
    config option that needs to be deliberately enabled. Even though it might
    be used in a "near-production" environment to find bugs that are not found
    during fuzzing or running tests, it is still a debug tool.
    
    ====== Benchmarks
    
    The following numbers were collected on Odroid C2 board. Both generic and
    tag-based KASAN were used in inline instrumentation mode.
    
    Boot time [1]:
    * ~1.7 sec for clean kernel
    * ~5.0 sec for generic KASAN
    * ~5.0 sec for tag-based KASAN
    
    Network performance [2]:
    * 8.33 Gbits/sec for clean kernel
    * 3.17 Gbits/sec for generic KASAN
    * 2.85 Gbits/sec for tag-based KASAN
    
    Slab memory usage after boot [3]:
    * ~40 kb for clean kernel
    * ~105 kb (~260% overhead) for generic KASAN
    * ~47 kb (~20% overhead) for tag-based KASAN
    
    KASAN memory overhead consists of three main parts:
    1. Increased slab memory usage due to redzones.
    2. Shadow memory (the whole reserved once during boot).
    3. Quaratine (grows gradually until some preset limit; the more the limit,
       the more the chance to detect a use-after-free).
    
    Comparing tag-based vs generic KASAN for each of these points:
    1. 20% vs 260% overhead.
    2. 1/16th vs 1/8th of physical memory.
    3. Tag-based KASAN doesn't require quarantine.
    
    [1] Time before the ext4 driver is initialized.
    [2] Measured as `iperf -s & iperf -c 127.0.0.1 -t 30`.
    [3] Measured as `cat /proc/meminfo | grep Slab`.
    
    ====== Some notes
    
    A few notes:
    
    1. The patchset can be found here:
       https://github.com/xairy/kasan-prototype/tree/khwasan
    
    2. Building requires a recent Clang version (7.0.0 or later).
    
    3. Stack instrumentation is not supported yet and will be added later.
    
    This patch (of 25):
    
    Tag-based KASAN changes the value of the top byte of pointers returned
    from the kernel allocation functions (such as kmalloc).  This patch
    updates KASAN hooks signatures and their usage in SLAB and SLUB code to
    reflect that.
    
    Link: http://lkml.kernel.org/r/aec2b5e3973781ff8a6bb6760f8543643202c451.1544099024.git.andreyknvl@google.com
    Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
    Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 918f374e7156..351ac48dabc4 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -444,7 +444,7 @@ static __always_inline void *kmem_cache_alloc_trace(struct kmem_cache *s,
 {
 	void *ret = kmem_cache_alloc(s, flags);
 
-	kasan_kmalloc(s, ret, size, flags);
+	ret = kasan_kmalloc(s, ret, size, flags);
 	return ret;
 }
 
@@ -455,7 +455,7 @@ kmem_cache_alloc_node_trace(struct kmem_cache *s,
 {
 	void *ret = kmem_cache_alloc_node(s, gfpflags, node);
 
-	kasan_kmalloc(s, ret, size, gfpflags);
+	ret = kasan_kmalloc(s, ret, size, gfpflags);
 	return ret;
 }
 #endif /* CONFIG_TRACING */

commit 3870a2371263d4f8e6cec4a8b3860c1f6ee601ad
Merge: acf0f57a2cb8 01598ba6b1a8
Author: Jonathan Corbet <corbet@lwn.net>
Date:   Tue Nov 20 09:22:24 2018 -0700

    Merge branch 'kmalloc' into docs-next
    
    jc: fixed conflict with willy's memory-allocation tag patch.

commit 01598ba6b1a863fbd819fc5c36c27886e5072164
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Sun Nov 11 18:48:44 2018 +0200

    docs/mm: update kmalloc kernel-doc description
    
    Add references to GFP documentation and the memory-allocation.rst and remove
    GFP_USER, GFP_DMA and GFP_NOIO descriptions.
    
    While on it slightly change the formatting so that the list of GFP flags
    will be rendered as "description" in the generated html.
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Jonathan Corbet <corbet@lwn.net>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 918f374e7156..4a342eb488f6 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -486,48 +486,47 @@ static __always_inline void *kmalloc_large(size_t size, gfp_t flags)
  * kmalloc is the normal method of allocating memory
  * for objects smaller than page size in the kernel.
  *
- * The @flags argument may be one of:
+ * The @flags argument may be one of the GFP flags defined at
+ * include/linux/gfp.h and described at
+ * :ref:`Documentation/core-api/mm-api.rst <mm-api-gfp-flags>`
  *
- * %GFP_USER - Allocate memory on behalf of user.  May sleep.
+ * The recommended usage of the @flags is described at
+ * :ref:`Documentation/core-api/memory-allocation.rst <memory_allocation>`
  *
- * %GFP_KERNEL - Allocate normal kernel ram.  May sleep.
+ * Below is a brief outline of the most useful GFP flags
  *
- * %GFP_ATOMIC - Allocation will not sleep.  May use emergency pools.
- *   For example, use this inside interrupt handlers.
+ * %GFP_KERNEL
+ *	Allocate normal kernel ram. May sleep.
  *
- * %GFP_HIGHUSER - Allocate pages from high memory.
+ * %GFP_NOWAIT
+ *	Allocation will not sleep.
  *
- * %GFP_NOIO - Do not do any I/O at all while trying to get memory.
+ * %GFP_ATOMIC
+ *	Allocation will not sleep.  May use emergency pools.
  *
- * %GFP_NOFS - Do not make any fs calls while trying to get memory.
- *
- * %GFP_NOWAIT - Allocation will not sleep.
- *
- * %__GFP_THISNODE - Allocate node-local memory only.
- *
- * %GFP_DMA - Allocation suitable for DMA.
- *   Should only be used for kmalloc() caches. Otherwise, use a
- *   slab created with SLAB_DMA.
+ * %GFP_HIGHUSER
+ *	Allocate memory from high memory on behalf of user.
  *
  * Also it is possible to set different flags by OR'ing
  * in one or more of the following additional @flags:
  *
- * %__GFP_HIGH - This allocation has high priority and may use emergency pools.
- *
- * %__GFP_NOFAIL - Indicate that this allocation is in no way allowed to fail
- *   (think twice before using).
+ * %__GFP_HIGH
+ *	This allocation has high priority and may use emergency pools.
  *
- * %__GFP_NORETRY - If memory is not immediately available,
- *   then give up at once.
+ * %__GFP_NOFAIL
+ *	Indicate that this allocation is in no way allowed to fail
+ *	(think twice before using).
  *
- * %__GFP_NOWARN - If allocation fails, don't issue any warnings.
+ * %__GFP_NORETRY
+ *	If memory is not immediately available,
+ *	then give up at once.
  *
- * %__GFP_RETRY_MAYFAIL - Try really hard to succeed the allocation but fail
- *   eventually.
+ * %__GFP_NOWARN
+ *	If allocation fails, don't issue any warnings.
  *
- * There are other flags available as well, but these are not intended
- * for general use, and so are not documented here. For a full list of
- * potential flags, always refer to linux/gfp.h.
+ * %__GFP_RETRY_MAYFAIL
+ *	Try really hard to succeed the allocation but fail
+ *	eventually.
  */
 static __always_inline void *kmalloc(size_t size, gfp_t flags)
 {

commit 1291523f2c1d631fea34102fd241fb54a4e8f7a0
Author: Vlastimil Babka <vbabka@suse.cz>
Date:   Fri Oct 26 15:05:38 2018 -0700

    mm, slab/slub: introduce kmalloc-reclaimable caches
    
    Kmem caches can be created with a SLAB_RECLAIM_ACCOUNT flag, which
    indicates they contain objects which can be reclaimed under memory
    pressure (typically through a shrinker).  This makes the slab pages
    accounted as NR_SLAB_RECLAIMABLE in vmstat, which is reflected also the
    MemAvailable meminfo counter and in overcommit decisions.  The slab pages
    are also allocated with __GFP_RECLAIMABLE, which is good for
    anti-fragmentation through grouping pages by mobility.
    
    The generic kmalloc-X caches are created without this flag, but sometimes
    are used also for objects that can be reclaimed, which due to varying size
    cannot have a dedicated kmem cache with SLAB_RECLAIM_ACCOUNT flag.  A
    prominent example are dcache external names, which prompted the creation
    of a new, manually managed vmstat counter NR_INDIRECTLY_RECLAIMABLE_BYTES
    in commit f1782c9bc547 ("dcache: account external names as indirectly
    reclaimable memory").
    
    To better handle this and any other similar cases, this patch introduces
    SLAB_RECLAIM_ACCOUNT variants of kmalloc caches, named kmalloc-rcl-X.
    They are used whenever the kmalloc() call passes __GFP_RECLAIMABLE among
    gfp flags.  They are added to the kmalloc_caches array as a new type.
    Allocations with both __GFP_DMA and __GFP_RECLAIMABLE will use a dma type
    cache.
    
    This change only applies to SLAB and SLUB, not SLOB.  This is fine, since
    SLOB's target are tiny system and this patch does add some overhead of
    kmem management objects.
    
    Link: http://lkml.kernel.org/r/20180731090649.16028-3-vbabka@suse.cz
    Signed-off-by: Vlastimil Babka <vbabka@suse.cz>
    Acked-by: Mel Gorman <mgorman@techsingularity.net>
    Acked-by: Christoph Lameter <cl@linux.com>
    Acked-by: Roman Gushchin <guro@fb.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Laura Abbott <labbott@redhat.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Sumit Semwal <sumit.semwal@linaro.org>
    Cc: Vijayanand Jitta <vjitta@codeaurora.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 2a7137043e91..918f374e7156 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -295,8 +295,13 @@ static inline void __check_heap_object(const void *ptr, unsigned long n,
 #define SLAB_OBJ_MIN_SIZE      (KMALLOC_MIN_SIZE < 16 ? \
                                (KMALLOC_MIN_SIZE) : 16)
 
+/*
+ * Whenever changing this, take care of that kmalloc_type() and
+ * create_kmalloc_caches() still work as intended.
+ */
 enum kmalloc_cache_type {
 	KMALLOC_NORMAL = 0,
+	KMALLOC_RECLAIM,
 #ifdef CONFIG_ZONE_DMA
 	KMALLOC_DMA,
 #endif
@@ -310,12 +315,21 @@ kmalloc_caches[NR_KMALLOC_TYPES][KMALLOC_SHIFT_HIGH + 1];
 static __always_inline enum kmalloc_cache_type kmalloc_type(gfp_t flags)
 {
 	int is_dma = 0;
+	int type_dma = 0;
+	int is_reclaimable;
 
 #ifdef CONFIG_ZONE_DMA
 	is_dma = !!(flags & __GFP_DMA);
+	type_dma = is_dma * KMALLOC_DMA;
 #endif
 
-	return is_dma;
+	is_reclaimable = !!(flags & __GFP_RECLAIMABLE);
+
+	/*
+	 * If an allocation is both __GFP_DMA and __GFP_RECLAIMABLE, return
+	 * KMALLOC_DMA and effectively ignore __GFP_RECLAIMABLE
+	 */
+	return type_dma + (is_reclaimable & !is_dma) * KMALLOC_RECLAIM;
 }
 
 /*

commit cc252eae85e09552f9c1e7ac0c3227f835efdf2d
Author: Vlastimil Babka <vbabka@suse.cz>
Date:   Fri Oct 26 15:05:34 2018 -0700

    mm, slab: combine kmalloc_caches and kmalloc_dma_caches
    
    Patch series "kmalloc-reclaimable caches", v4.
    
    As discussed at LSF/MM [1] here's a patchset that introduces
    kmalloc-reclaimable caches (more details in the second patch) and uses
    them for dcache external names.  That allows us to repurpose the
    NR_INDIRECTLY_RECLAIMABLE_BYTES counter later in the series.
    
    With patch 3/6, dcache external names are allocated from kmalloc-rcl-*
    caches, eliminating the need for manual accounting.  More importantly, it
    also ensures the reclaimable kmalloc allocations are grouped in pages
    separate from the regular kmalloc allocations.  The need for proper
    accounting of dcache external names has shown it's easy for misbehaving
    process to allocate lots of them, causing premature OOMs.  Without the
    added grouping, it's likely that a similar workload can interleave the
    dcache external names allocations with regular kmalloc allocations (note:
    I haven't searched myself for an example of such regular kmalloc
    allocation, but I would be very surprised if there wasn't some).  A
    pathological case would be e.g.  one 64byte regular allocations with 63
    external dcache names in a page (64x64=4096), which means the page is not
    freed even after reclaiming after all dcache names, and the process can
    thus "steal" the whole page with single 64byte allocation.
    
    If other kmalloc users similar to dcache external names become identified,
    they can also benefit from the new functionality simply by adding
    __GFP_RECLAIMABLE to the kmalloc calls.
    
    Side benefits of the patchset (that could be also merged separately)
    include removed branch for detecting __GFP_DMA kmalloc(), and shortening
    kmalloc cache names in /proc/slabinfo output.  The latter is potentially
    an ABI break in case there are tools parsing the names and expecting the
    values to be in bytes.
    
    This is how /proc/slabinfo looks like after booting in virtme:
    
    ...
    kmalloc-rcl-4M         0      0 4194304    1 1024 : tunables    1    1    0 : slabdata      0      0      0
    ...
    kmalloc-rcl-96         7     32    128   32    1 : tunables  120   60    8 : slabdata      1      1      0
    kmalloc-rcl-64        25    128     64   64    1 : tunables  120   60    8 : slabdata      2      2      0
    kmalloc-rcl-32         0      0     32  124    1 : tunables  120   60    8 : slabdata      0      0      0
    kmalloc-4M             0      0 4194304    1 1024 : tunables    1    1    0 : slabdata      0      0      0
    kmalloc-2M             0      0 2097152    1  512 : tunables    1    1    0 : slabdata      0      0      0
    kmalloc-1M             0      0 1048576    1  256 : tunables    1    1    0 : slabdata      0      0      0
    ...
    
    /proc/vmstat with renamed nr_indirectly_reclaimable_bytes counter:
    
    ...
    nr_slab_reclaimable 2817
    nr_slab_unreclaimable 1781
    ...
    nr_kernel_misc_reclaimable 0
    ...
    
    /proc/meminfo with new KReclaimable counter:
    
    ...
    Shmem:               564 kB
    KReclaimable:      11260 kB
    Slab:              18368 kB
    SReclaimable:      11260 kB
    SUnreclaim:         7108 kB
    KernelStack:        1248 kB
    ...
    
    This patch (of 6):
    
    The kmalloc caches currently mainain separate (optional) array
    kmalloc_dma_caches for __GFP_DMA allocations.  There are tests for
    __GFP_DMA in the allocation hotpaths.  We can avoid the branches by
    combining kmalloc_caches and kmalloc_dma_caches into a single
    two-dimensional array where the outer dimension is cache "type".  This
    will also allow to add kmalloc-reclaimable caches as a third type.
    
    Link: http://lkml.kernel.org/r/20180731090649.16028-2-vbabka@suse.cz
    Signed-off-by: Vlastimil Babka <vbabka@suse.cz>
    Acked-by: Mel Gorman <mgorman@techsingularity.net>
    Acked-by: Christoph Lameter <cl@linux.com>
    Acked-by: Roman Gushchin <guro@fb.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Laura Abbott <labbott@redhat.com>
    Cc: Sumit Semwal <sumit.semwal@linaro.org>
    Cc: Vijayanand Jitta <vjitta@codeaurora.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index ed9cbddeb4a6..2a7137043e91 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -295,12 +295,29 @@ static inline void __check_heap_object(const void *ptr, unsigned long n,
 #define SLAB_OBJ_MIN_SIZE      (KMALLOC_MIN_SIZE < 16 ? \
                                (KMALLOC_MIN_SIZE) : 16)
 
+enum kmalloc_cache_type {
+	KMALLOC_NORMAL = 0,
+#ifdef CONFIG_ZONE_DMA
+	KMALLOC_DMA,
+#endif
+	NR_KMALLOC_TYPES
+};
+
 #ifndef CONFIG_SLOB
-extern struct kmem_cache *kmalloc_caches[KMALLOC_SHIFT_HIGH + 1];
+extern struct kmem_cache *
+kmalloc_caches[NR_KMALLOC_TYPES][KMALLOC_SHIFT_HIGH + 1];
+
+static __always_inline enum kmalloc_cache_type kmalloc_type(gfp_t flags)
+{
+	int is_dma = 0;
+
 #ifdef CONFIG_ZONE_DMA
-extern struct kmem_cache *kmalloc_dma_caches[KMALLOC_SHIFT_HIGH + 1];
+	is_dma = !!(flags & __GFP_DMA);
 #endif
 
+	return is_dma;
+}
+
 /*
  * Figure out which kmalloc slab an allocation of a certain size
  * belongs to.
@@ -501,18 +518,20 @@ static __always_inline void *kmalloc_large(size_t size, gfp_t flags)
 static __always_inline void *kmalloc(size_t size, gfp_t flags)
 {
 	if (__builtin_constant_p(size)) {
+#ifndef CONFIG_SLOB
+		unsigned int index;
+#endif
 		if (size > KMALLOC_MAX_CACHE_SIZE)
 			return kmalloc_large(size, flags);
 #ifndef CONFIG_SLOB
-		if (!(flags & GFP_DMA)) {
-			unsigned int index = kmalloc_index(size);
+		index = kmalloc_index(size);
 
-			if (!index)
-				return ZERO_SIZE_PTR;
+		if (!index)
+			return ZERO_SIZE_PTR;
 
-			return kmem_cache_alloc_trace(kmalloc_caches[index],
-					flags, size);
-		}
+		return kmem_cache_alloc_trace(
+				kmalloc_caches[kmalloc_type(flags)][index],
+				flags, size);
 #endif
 	}
 	return __kmalloc(size, flags);
@@ -542,13 +561,14 @@ static __always_inline void *kmalloc_node(size_t size, gfp_t flags, int node)
 {
 #ifndef CONFIG_SLOB
 	if (__builtin_constant_p(size) &&
-		size <= KMALLOC_MAX_CACHE_SIZE && !(flags & GFP_DMA)) {
+		size <= KMALLOC_MAX_CACHE_SIZE) {
 		unsigned int i = kmalloc_index(size);
 
 		if (!i)
 			return ZERO_SIZE_PTR;
 
-		return kmem_cache_alloc_node_trace(kmalloc_caches[i],
+		return kmem_cache_alloc_node_trace(
+				kmalloc_caches[kmalloc_type(flags)][i],
 						flags, node, size);
 	}
 #endif

commit 84c07d11aa619c6d24c682f469b10f344f0c02aa
Author: Kirill Tkhai <ktkhai@virtuozzo.com>
Date:   Fri Aug 17 15:47:25 2018 -0700

    mm: introduce CONFIG_MEMCG_KMEM as combination of CONFIG_MEMCG && !CONFIG_SLOB
    
    Introduce new config option, which is used to replace repeating
    CONFIG_MEMCG && !CONFIG_SLOB pattern.  Next patches add a little more
    memcg+kmem related code, so let's keep the defines more clearly.
    
    Link: http://lkml.kernel.org/r/153063053670.1818.15013136946600481138.stgit@localhost.localdomain
    Signed-off-by: Kirill Tkhai <ktkhai@virtuozzo.com>
    Acked-by: Vladimir Davydov <vdavydov.dev@gmail.com>
    Tested-by: Shakeel Butt <shakeelb@google.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Guenter Roeck <linux@roeck-us.net>
    Cc: "Huang, Ying" <ying.huang@intel.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Josef Bacik <jbacik@fb.com>
    Cc: Li RongQing <lirongqing@baidu.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Matthias Kaehlcke <mka@chromium.org>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Philippe Ombredanne <pombredanne@nexb.com>
    Cc: Roman Gushchin <guro@fb.com>
    Cc: Sahitya Tummala <stummala@codeaurora.org>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Cc: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Waiman Long <longman@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 14e3fe4bd6a1..ed9cbddeb4a6 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -97,7 +97,7 @@
 # define SLAB_FAILSLAB		0
 #endif
 /* Account to memcg */
-#if defined(CONFIG_MEMCG) && !defined(CONFIG_SLOB)
+#ifdef CONFIG_MEMCG_KMEM
 # define SLAB_ACCOUNT		((slab_flags_t __force)0x04000000U)
 #else
 # define SLAB_ACCOUNT		0

commit 92ee383f6daab4da5471b86f6fdaba775e6928f6
Author: Shakeel Butt <shakeelb@google.com>
Date:   Thu Jun 14 15:26:27 2018 -0700

    mm: fix race between kmem_cache destroy, create and deactivate
    
    The memcg kmem cache creation and deactivation (SLUB only) is
    asynchronous.  If a root kmem cache is destroyed whose memcg cache is in
    the process of creation or deactivation, the kernel may crash.
    
    Example of one such crash:
            general protection fault: 0000 [#1] SMP PTI
            CPU: 1 PID: 1721 Comm: kworker/14:1 Not tainted 4.17.0-smp
            ...
            Workqueue: memcg_kmem_cache kmemcg_deactivate_workfn
            RIP: 0010:has_cpu_slab
            ...
            Call Trace:
            ? on_each_cpu_cond
            __kmem_cache_shrink
            kmemcg_cache_deact_after_rcu
            kmemcg_deactivate_workfn
            process_one_work
            worker_thread
            kthread
            ret_from_fork+0x35/0x40
    
    To fix this race, on root kmem cache destruction, mark the cache as
    dying and flush the workqueue used for memcg kmem cache creation and
    deactivation.  SLUB's memcg kmem cache deactivation also includes RCU
    callback and thus make sure all previous registered RCU callbacks have
    completed as well.
    
    [shakeelb@google.com: handle the RCU callbacks for SLUB deactivation]
      Link: http://lkml.kernel.org/r/20180611192951.195727-1-shakeelb@google.com
    [shakeelb@google.com: add more documentation, rename fields for readability]
      Link: http://lkml.kernel.org/r/20180522201336.196994-1-shakeelb@google.com
    [akpm@linux-foundation.org: fix build, per Shakeel]
    [shakeelb@google.com: v3.  Instead of refcount, flush the workqueue]
      Link: http://lkml.kernel.org/r/20180530001204.183758-1-shakeelb@google.com
    Link: http://lkml.kernel.org/r/20180521174116.171846-1-shakeelb@google.com
    Signed-off-by: Shakeel Butt <shakeelb@google.com>
    Acked-by: Vladimir Davydov <vdavydov.dev@gmail.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Greg Thelen <gthelen@google.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 4d759e1ddc33..14e3fe4bd6a1 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -600,6 +600,7 @@ struct memcg_cache_params {
 			struct memcg_cache_array __rcu *memcg_caches;
 			struct list_head __root_caches_node;
 			struct list_head children;
+			bool dying;
 		};
 		struct {
 			struct mem_cgroup *memcg;

commit 49b7f8983aa78581bfd511a26891b26cd734e293
Author: Kees Cook <keescook@chromium.org>
Date:   Tue May 8 12:52:32 2018 -0700

    mm: Use overflow helpers in kmalloc_array*()
    
    Instead of open-coded multiplication and bounds checking, use the new
    overflow helper.
    
    Suggested-by: Rasmus Villemoes <linux@rasmusvillemoes.dk>
    Signed-off-by: Kees Cook <keescook@chromium.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 81ebd71f8c03..4d759e1ddc33 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -13,6 +13,7 @@
 #define	_LINUX_SLAB_H
 
 #include <linux/gfp.h>
+#include <linux/overflow.h>
 #include <linux/types.h>
 #include <linux/workqueue.h>
 
@@ -624,11 +625,13 @@ int memcg_update_all_caches(int num_memcgs);
  */
 static inline void *kmalloc_array(size_t n, size_t size, gfp_t flags)
 {
-	if (size != 0 && n > SIZE_MAX / size)
+	size_t bytes;
+
+	if (unlikely(check_mul_overflow(n, size, &bytes)))
 		return NULL;
 	if (__builtin_constant_p(n) && __builtin_constant_p(size))
-		return kmalloc(n * size, flags);
-	return __kmalloc(n * size, flags);
+		return kmalloc(bytes, flags);
+	return __kmalloc(bytes, flags);
 }
 
 /**
@@ -657,11 +660,13 @@ extern void *__kmalloc_track_caller(size_t, gfp_t, unsigned long);
 static inline void *kmalloc_array_node(size_t n, size_t size, gfp_t flags,
 				       int node)
 {
-	if (size != 0 && n > SIZE_MAX / size)
+	size_t bytes;
+
+	if (unlikely(check_mul_overflow(n, size, &bytes)))
 		return NULL;
 	if (__builtin_constant_p(n) && __builtin_constant_p(size))
-		return kmalloc_node(n * size, flags, node);
-	return __kmalloc_node(n * size, flags, node);
+		return kmalloc_node(bytes, flags, node);
+	return __kmalloc_node(bytes, flags, node);
 }
 
 static inline void *kcalloc_node(size_t n, size_t size, gfp_t flags, int node)

commit 514c60324960137e74457fdc233a339b985fa8a8
Author: Randy Dunlap <rdunlap@infradead.org>
Date:   Thu Apr 5 16:25:34 2018 -0700

    headers: untangle kmemleak.h from mm.h
    
    Currently <linux/slab.h> #includes <linux/kmemleak.h> for no obvious
    reason.  It looks like it's only a convenience, so remove kmemleak.h
    from slab.h and add <linux/kmemleak.h> to any users of kmemleak_* that
    don't already #include it.  Also remove <linux/kmemleak.h> from source
    files that do not use it.
    
    This is tested on i386 allmodconfig and x86_64 allmodconfig.  It would
    be good to run it through the 0day bot for other $ARCHes.  I have
    neither the horsepower nor the storage space for the other $ARCHes.
    
    Update: This patch has been extensively build-tested by both the 0day
    bot & kisskb/ozlabs build farms.  Both of them reported 2 build failures
    for which patches are included here (in v2).
    
    [ slab.h is the second most used header file after module.h; kernel.h is
      right there with slab.h. There could be some minor error in the
      counting due to some #includes having comments after them and I didn't
      combine all of those. ]
    
    [akpm@linux-foundation.org: security/keys/big_key.c needs vmalloc.h, per sfr]
    Link: http://lkml.kernel.org/r/e4309f98-3749-93e1-4bb7-d9501a39d015@infradead.org
    Link: http://kisskb.ellerman.id.au/kisskb/head/13396/
    Signed-off-by: Randy Dunlap <rdunlap@infradead.org>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Reported-by: Michael Ellerman <mpe@ellerman.id.au>      [2 build failures]
    Reported-by: Fengguang Wu <fengguang.wu@intel.com>      [2 build failures]
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Wei Yongjun <weiyongjun1@huawei.com>
    Cc: Luis R. Rodriguez <mcgrof@kernel.org>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Mimi Zohar <zohar@linux.vnet.ibm.com>
    Cc: John Johansen <john.johansen@canonical.com>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 04402c637171..81ebd71f8c03 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -125,7 +125,6 @@
 #define ZERO_OR_NULL_PTR(x) ((unsigned long)(x) <= \
 				(unsigned long)ZERO_SIZE_PTR)
 
-#include <linux/kmemleak.h>
 #include <linux/kasan.h>
 
 struct mem_cgroup;

commit 7bbdb81ee3de73f2381ceec1bbee831f4c913b5c
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Thu Apr 5 16:21:31 2018 -0700

    slab: make usercopy region 32-bit
    
    If kmem case sizes are 32-bit, then usecopy region should be too.
    
    Link: http://lkml.kernel.org/r/20180305200730.15812-21-adobriyan@gmail.com
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: David Miller <davem@davemloft.net>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index d36e8f03730e..04402c637171 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -143,7 +143,7 @@ struct kmem_cache *kmem_cache_create(const char *name, unsigned int size,
 struct kmem_cache *kmem_cache_create_usercopy(const char *name,
 			unsigned int size, unsigned int align,
 			slab_flags_t flags,
-			size_t useroffset, size_t usersize,
+			unsigned int useroffset, unsigned int usersize,
 			void (*ctor)(void *));
 void kmem_cache_destroy(struct kmem_cache *);
 int kmem_cache_shrink(struct kmem_cache *);

commit f4957d5bd09165b165df851fbf8c658f7fcd9922
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Thu Apr 5 16:20:37 2018 -0700

    slab: make kmem_cache_create() work with 32-bit sizes
    
    struct kmem_cache::size and ::align were always 32-bit.
    
    Out of curiosity I created 4GB kmem_cache, it oopsed with division by 0.
    kmem_cache_create(1UL<<32+1) created 1-byte cache as expected.
    
    size_t doesn't work and never did.
    
    Link: http://lkml.kernel.org/r/20180305200730.15812-6-adobriyan@gmail.com
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index ad157fbf3886..d36e8f03730e 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -137,11 +137,12 @@ bool slab_is_available(void);
 
 extern bool usercopy_fallback;
 
-struct kmem_cache *kmem_cache_create(const char *name, size_t size,
-			size_t align, slab_flags_t flags,
+struct kmem_cache *kmem_cache_create(const char *name, unsigned int size,
+			unsigned int align, slab_flags_t flags,
 			void (*ctor)(void *));
 struct kmem_cache *kmem_cache_create_usercopy(const char *name,
-			size_t size, size_t align, slab_flags_t flags,
+			unsigned int size, unsigned int align,
+			slab_flags_t flags,
 			size_t useroffset, size_t usersize,
 			void (*ctor)(void *));
 void kmem_cache_destroy(struct kmem_cache *);

commit 0be70327ec8cf6dd6847cbd8b75ca51be864a6ea
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Thu Apr 5 16:20:26 2018 -0700

    slab: make kmalloc_size() return "unsigned int"
    
    kmalloc_size() derives size of kmalloc cache from internal index, which
    can't be negative.
    
    Propagate unsignedness a bit.
    
    Link: http://lkml.kernel.org/r/20180305200730.15812-3-adobriyan@gmail.com
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 296f33a512eb..ad157fbf3886 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -522,11 +522,11 @@ static __always_inline void *kmalloc(size_t size, gfp_t flags)
  * return size or 0 if a kmalloc cache for that
  * size does not exist
  */
-static __always_inline int kmalloc_size(int n)
+static __always_inline unsigned int kmalloc_size(unsigned int n)
 {
 #ifndef CONFIG_SLOB
 	if (n > 2)
-		return 1 << n;
+		return 1U << n;
 
 	if (n == 1 && KMALLOC_MIN_SIZE <= 32)
 		return 96;

commit 36071a279b4100afe9fbee18727ad78daa307591
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Thu Apr 5 16:20:22 2018 -0700

    slab: make kmalloc_index() return "unsigned int"
    
    kmalloc_index() return index into an array of kmalloc kmem caches,
    therefore should be unsigned.
    
    Space savings with SLUB on trimmed down .config:
    
            add/remove: 0/1 grow/shrink: 6/56 up/down: 85/-557 (-472)
            Function                                     old     new   delta
            calculate_sizes                              924     983     +59
            on_freelist                                  589     604     +15
            init_cache_random_seq                        122     127      +5
            ext4_mb_init                                1206    1210      +4
            slab_pad_check.part                          270     271      +1
            cpu_partial_store                            112     113      +1
            usersize_show                                 28      27      -1
                    ...
            new_slab                                    1871    1837     -34
            slab_order                                   204       -    -204
    
    This patch start a series of converting SLUB (mostly) to "unsigned int".
    
    1) Most integers in the code are in fact unsigned entities: array
       indexes, lengths, buffer sizes, allocation orders. It is therefore
       better to use unsigned variables
    
    2) Some integers in the code are either "size_t" or "unsigned long" for
       no reason.
    
       size_t usually comes from people trying to maintain type correctness
       and figuring out that "sizeof" operator returns size_t or
       memset/memcpy takes size_t so should everything passed to it.
    
       However the number of 4GB+ objects in the kernel is very small. Most,
       if not all, dynamically allocated objects with kmalloc() or
       kmem_cache_create() aren't actually big. Maintaining wide types
       doesn't do anything.
    
       64-bit ops are bigger than 32-bit on our beloved x86_64,
       so try to not use 64-bit where it isn't necessary
       (read: everywhere where integers are integers not pointers)
    
    3) in case of SLAB allocators, there are additional limitations
       *) page->inuse, page->objects are only 16-/15-bit,
       *) cache size was always 32-bit
       *) slab orders are small, order 20 is needed to go 64-bit on x86_64
          (PAGE_SIZE << order)
    
    Basically everything is 32-bit except kmalloc(1ULL<<32) which gets
    shortcut through page allocator.
    
    Christoph said:
    :
    : That changes with large base page size on power and ARM64 f.e. but then
    : we do not want to encourage larger allocations through slab anyways.
    
    Link: http://lkml.kernel.org/r/20180305200730.15812-2-adobriyan@gmail.com
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 231abc8976c5..296f33a512eb 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -308,7 +308,7 @@ extern struct kmem_cache *kmalloc_dma_caches[KMALLOC_SHIFT_HIGH + 1];
  * 2 = 129 .. 192 bytes
  * n = 2^(n-1)+1 .. 2^n
  */
-static __always_inline int kmalloc_index(size_t size)
+static __always_inline unsigned int kmalloc_index(size_t size)
 {
 	if (!size)
 		return 0;
@@ -504,7 +504,7 @@ static __always_inline void *kmalloc(size_t size, gfp_t flags)
 			return kmalloc_large(size, flags);
 #ifndef CONFIG_SLOB
 		if (!(flags & GFP_DMA)) {
-			int index = kmalloc_index(size);
+			unsigned int index = kmalloc_index(size);
 
 			if (!index)
 				return ZERO_SIZE_PTR;
@@ -542,7 +542,7 @@ static __always_inline void *kmalloc_node(size_t size, gfp_t flags, int node)
 #ifndef CONFIG_SLOB
 	if (__builtin_constant_p(size) &&
 		size <= KMALLOC_MAX_CACHE_SIZE && !(flags & GFP_DMA)) {
-		int i = kmalloc_index(size);
+		unsigned int i = kmalloc_index(size);
 
 		if (!i)
 			return ZERO_SIZE_PTR;

commit 2d891fbc3bb681ba1f826e7ee70dbe38ca7465fe
Author: Kees Cook <keescook@chromium.org>
Date:   Thu Nov 30 13:04:32 2017 -0800

    usercopy: Allow strict enforcement of whitelists
    
    This introduces CONFIG_HARDENED_USERCOPY_FALLBACK to control the
    behavior of hardened usercopy whitelist violations. By default, whitelist
    violations will continue to WARN() so that any bad or missing usercopy
    whitelists can be discovered without being too disruptive.
    
    If this config is disabled at build time or a system is booted with
    "slab_common.usercopy_fallback=0", usercopy whitelists will BUG() instead
    of WARN(). This is useful for admins that want to use usercopy whitelists
    immediately.
    
    Suggested-by: Matthew Garrett <mjg59@google.com>
    Signed-off-by: Kees Cook <keescook@chromium.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 8bf14d9762ec..231abc8976c5 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -135,6 +135,8 @@ struct mem_cgroup;
 void __init kmem_cache_init(void);
 bool slab_is_available(void);
 
+extern bool usercopy_fallback;
+
 struct kmem_cache *kmem_cache_create(const char *name, size_t size,
 			size_t align, slab_flags_t flags,
 			void (*ctor)(void *));

commit 8eb8284b412906181357c2b0110d879d5af95e52
Author: David Windsor <dave@nullcore.net>
Date:   Sat Jun 10 22:50:28 2017 -0400

    usercopy: Prepare for usercopy whitelisting
    
    This patch prepares the slab allocator to handle caches having annotations
    (useroffset and usersize) defining usercopy regions.
    
    This patch is modified from Brad Spengler/PaX Team's PAX_USERCOPY
    whitelisting code in the last public patch of grsecurity/PaX based on
    my understanding of the code. Changes or omissions from the original
    code are mine and don't reflect the original grsecurity/PaX code.
    
    Currently, hardened usercopy performs dynamic bounds checking on slab
    cache objects. This is good, but still leaves a lot of kernel memory
    available to be copied to/from userspace in the face of bugs. To further
    restrict what memory is available for copying, this creates a way to
    whitelist specific areas of a given slab cache object for copying to/from
    userspace, allowing much finer granularity of access control. Slab caches
    that are never exposed to userspace can declare no whitelist for their
    objects, thereby keeping them unavailable to userspace via dynamic copy
    operations. (Note, an implicit form of whitelisting is the use of constant
    sizes in usercopy operations and get_user()/put_user(); these bypass
    hardened usercopy checks since these sizes cannot change at runtime.)
    
    To support this whitelist annotation, usercopy region offset and size
    members are added to struct kmem_cache. The slab allocator receives a
    new function, kmem_cache_create_usercopy(), that creates a new cache
    with a usercopy region defined, suitable for declaring spans of fields
    within the objects that get copied to/from userspace.
    
    In this patch, the default kmem_cache_create() marks the entire allocation
    as whitelisted, leaving it semantically unchanged. Once all fine-grained
    whitelists have been added (in subsequent patches), this will be changed
    to a usersize of 0, making caches created with kmem_cache_create() not
    copyable to/from userspace.
    
    After the entire usercopy whitelist series is applied, less than 15%
    of the slab cache memory remains exposed to potential usercopy bugs
    after a fresh boot:
    
    Total Slab Memory:           48074720
    Usercopyable Memory:          6367532  13.2%
             task_struct                    0.2%         4480/1630720
             RAW                            0.3%            300/96000
             RAWv6                          2.1%           1408/64768
             ext4_inode_cache               3.0%       269760/8740224
             dentry                        11.1%       585984/5273856
             mm_struct                     29.1%         54912/188448
             kmalloc-8                    100.0%          24576/24576
             kmalloc-16                   100.0%          28672/28672
             kmalloc-32                   100.0%          81920/81920
             kmalloc-192                  100.0%          96768/96768
             kmalloc-128                  100.0%        143360/143360
             names_cache                  100.0%        163840/163840
             kmalloc-64                   100.0%        167936/167936
             kmalloc-256                  100.0%        339968/339968
             kmalloc-512                  100.0%        350720/350720
             kmalloc-96                   100.0%        455616/455616
             kmalloc-8192                 100.0%        655360/655360
             kmalloc-1024                 100.0%        812032/812032
             kmalloc-4096                 100.0%        819200/819200
             kmalloc-2048                 100.0%      1310720/1310720
    
    After some kernel build workloads, the percentage (mainly driven by
    dentry and inode caches expanding) drops under 10%:
    
    Total Slab Memory:           95516184
    Usercopyable Memory:          8497452   8.8%
             task_struct                    0.2%         4000/1456000
             RAW                            0.3%            300/96000
             RAWv6                          2.1%           1408/64768
             ext4_inode_cache               3.0%     1217280/39439872
             dentry                        11.1%     1623200/14608800
             mm_struct                     29.1%         73216/251264
             kmalloc-8                    100.0%          24576/24576
             kmalloc-16                   100.0%          28672/28672
             kmalloc-32                   100.0%          94208/94208
             kmalloc-192                  100.0%          96768/96768
             kmalloc-128                  100.0%        143360/143360
             names_cache                  100.0%        163840/163840
             kmalloc-64                   100.0%        245760/245760
             kmalloc-256                  100.0%        339968/339968
             kmalloc-512                  100.0%        350720/350720
             kmalloc-96                   100.0%        563520/563520
             kmalloc-8192                 100.0%        655360/655360
             kmalloc-1024                 100.0%        794624/794624
             kmalloc-4096                 100.0%        819200/819200
             kmalloc-2048                 100.0%      1257472/1257472
    
    Signed-off-by: David Windsor <dave@nullcore.net>
    [kees: adjust commit log, split out a few extra kmalloc hunks]
    [kees: add field names to function declarations]
    [kees: convert BUGs to WARNs and fail closed]
    [kees: add attack surface reduction analysis to commit log]
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: linux-mm@kvack.org
    Cc: linux-xfs@vger.kernel.org
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Acked-by: Christoph Lameter <cl@linux.com>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 2dbeccdcb76b..8bf14d9762ec 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -135,9 +135,13 @@ struct mem_cgroup;
 void __init kmem_cache_init(void);
 bool slab_is_available(void);
 
-struct kmem_cache *kmem_cache_create(const char *, size_t, size_t,
-			slab_flags_t,
-			void (*)(void *));
+struct kmem_cache *kmem_cache_create(const char *name, size_t size,
+			size_t align, slab_flags_t flags,
+			void (*ctor)(void *));
+struct kmem_cache *kmem_cache_create_usercopy(const char *name,
+			size_t size, size_t align, slab_flags_t flags,
+			size_t useroffset, size_t usersize,
+			void (*ctor)(void *));
 void kmem_cache_destroy(struct kmem_cache *);
 int kmem_cache_shrink(struct kmem_cache *);
 
@@ -153,9 +157,20 @@ void memcg_destroy_kmem_caches(struct mem_cgroup *);
  * f.e. add ____cacheline_aligned_in_smp to the struct declaration
  * then the objects will be properly aligned in SMP configurations.
  */
-#define KMEM_CACHE(__struct, __flags) kmem_cache_create(#__struct,\
-		sizeof(struct __struct), __alignof__(struct __struct),\
-		(__flags), NULL)
+#define KMEM_CACHE(__struct, __flags)					\
+		kmem_cache_create(#__struct, sizeof(struct __struct),	\
+			__alignof__(struct __struct), (__flags), NULL)
+
+/*
+ * To whitelist a single field for copying to/from usercopy, use this
+ * macro instead for KMEM_CACHE() above.
+ */
+#define KMEM_CACHE_USERCOPY(__struct, __flags, __field)			\
+		kmem_cache_create_usercopy(#__struct,			\
+			sizeof(struct __struct),			\
+			__alignof__(struct __struct), (__flags),	\
+			offsetof(struct __struct, __field),		\
+			sizeof_field(struct __struct, __field), NULL)
 
 /*
  * Common kmalloc functions provided by all allocators

commit f4e6e289cb9cf67885b6b18b9d56d2c3e1c714a1
Author: Kees Cook <keescook@chromium.org>
Date:   Wed Jan 10 14:48:22 2018 -0800

    usercopy: Include offset in hardened usercopy report
    
    This refactors the hardened usercopy code so that failure reporting can
    happen within the checking functions instead of at the top level. This
    simplifies the return value handling and allows more details and offsets
    to be included in the report. Having the offset can be much more helpful
    in understanding hardened usercopy bugs.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 50697a1d6621..2dbeccdcb76b 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -167,15 +167,11 @@ void kzfree(const void *);
 size_t ksize(const void *);
 
 #ifdef CONFIG_HAVE_HARDENED_USERCOPY_ALLOCATOR
-const char *__check_heap_object(const void *ptr, unsigned long n,
-				struct page *page);
+void __check_heap_object(const void *ptr, unsigned long n, struct page *page,
+			bool to_user);
 #else
-static inline const char *__check_heap_object(const void *ptr,
-					      unsigned long n,
-					      struct page *page)
-{
-	return NULL;
-}
+static inline void __check_heap_object(const void *ptr, unsigned long n,
+				       struct page *page, bool to_user) { }
 #endif
 
 /*

commit 453f85d43fa9ee243f0fc3ac4e1be45615301e3f
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Wed Nov 15 17:38:03 2017 -0800

    mm: remove __GFP_COLD
    
    As the page free path makes no distinction between cache hot and cold
    pages, there is no real useful ordering of pages in the free list that
    allocation requests can take advantage of.  Juding from the users of
    __GFP_COLD, it is likely that a number of them are the result of copying
    other sites instead of actually measuring the impact.  Remove the
    __GFP_COLD parameter which simplifies a number of paths in the page
    allocator.
    
    This is potentially controversial but bear in mind that the size of the
    per-cpu pagelists versus modern cache sizes means that the whole per-cpu
    list can often fit in the L3 cache.  Hence, there is only a potential
    benefit for microbenchmarks that alloc/free pages in a tight loop.  It's
    even worse when THP is taken into account which has little or no chance
    of getting a cache-hot page as the per-cpu list is bypassed and the
    zeroing of multiple pages will thrash the cache anyway.
    
    The truncate microbenchmarks are not shown as this patch affects the
    allocation path and not the free path.  A page fault microbenchmark was
    tested but it showed no sigificant difference which is not surprising
    given that the __GFP_COLD branches are a miniscule percentage of the
    fault path.
    
    Link: http://lkml.kernel.org/r/20171018075952.10627-9-mgorman@techsingularity.net
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 79f6532f8a0b..50697a1d6621 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -467,9 +467,6 @@ static __always_inline void *kmalloc_large(size_t size, gfp_t flags)
  * Also it is possible to set different flags by OR'ing
  * in one or more of the following additional @flags:
  *
- * %__GFP_COLD - Request cache-cold pages instead of
- *   trying to return cache-warm pages.
- *
  * %__GFP_HIGH - This allocation has high priority and may use emergency pools.
  *
  * %__GFP_NOFAIL - Indicate that this allocation is in no way allowed to fail

commit d8be75663cec0069b85f80191abd2682ce4a512f
Author: Levin, Alexander (Sasha Levin) <alexander.levin@verizon.com>
Date:   Wed Nov 15 17:35:58 2017 -0800

    kmemcheck: remove whats left of NOTRACK flags
    
    Now that kmemcheck is gone, we don't need the NOTRACK flags.
    
    Link: http://lkml.kernel.org/r/20171007030159.22241-5-alexander.levin@verizon.com
    Signed-off-by: Sasha Levin <alexander.levin@verizon.com>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Tim Hansen <devtimhansen@gmail.com>
    Cc: Vegard Nossum <vegardno@ifi.uio.no>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index ebddfca9e24b..79f6532f8a0b 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -89,12 +89,6 @@
 /* Avoid kmemleak tracing */
 #define SLAB_NOLEAKTRACE	((slab_flags_t __force)0x00800000U)
 
-/* Don't track use of uninitialized memory */
-#ifdef CONFIG_KMEMCHECK
-# define SLAB_NOTRACK		((slab_flags_t __force)0x01000000U)
-#else
-# define SLAB_NOTRACK		0
-#endif
 /* Fault injection mark */
 #ifdef CONFIG_FAILSLAB
 # define SLAB_FAILSLAB		((slab_flags_t __force)0x02000000U)

commit 5799b255c491d853b73fb9e0e1760210315d06cd
Author: Johannes Thumshirn <jthumshirn@suse.de>
Date:   Wed Nov 15 17:32:29 2017 -0800

    include/linux/slab.h: add kmalloc_array_node() and kcalloc_node()
    
    Patch series "Add kmalloc_array_node() and kcalloc_node()".
    
    Our current memeory allocation routines suffer form an API imbalance,
    for one we have kmalloc_array() and kcalloc() which check for overflows
    in size multiplication and we have kmalloc_node() and kzalloc_node()
    which allow for memory allocation on a certain NUMA node but don't check
    for eventual overflows.
    
    This patch (of 6):
    
    We have kmalloc_array() and kcalloc() wrappers on top of kmalloc() which
    ensure us overflow free multiplication for the size of a memory
    allocation but these implementations are not NUMA-aware.
    
    Likewise we have kmalloc_node() which is a NUMA-aware version of
    kmalloc() but the implementation is not aware of any possible overflows
    in eventual size calculations.
    
    Introduce a combination of the two above cases to have a NUMA-node aware
    version of kmalloc_array() and kcalloc().
    
    Link: http://lkml.kernel.org/r/20170927082038.3782-2-jthumshirn@suse.de
    Signed-off-by: Johannes Thumshirn <jthumshirn@suse.de>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Damien Le Moal <damien.lemoal@wdc.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Doug Ledford <dledford@redhat.com>
    Cc: Hal Rosenstock <hal.rosenstock@gmail.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Mike Marciniszyn <infinipath@intel.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Santosh Shilimkar <santosh.shilimkar@oracle.com>
    Cc: Sean Hefty <sean.hefty@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index f37cb93768ab..ebddfca9e24b 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -650,6 +650,22 @@ extern void *__kmalloc_track_caller(size_t, gfp_t, unsigned long);
 #define kmalloc_track_caller(size, flags) \
 	__kmalloc_track_caller(size, flags, _RET_IP_)
 
+static inline void *kmalloc_array_node(size_t n, size_t size, gfp_t flags,
+				       int node)
+{
+	if (size != 0 && n > SIZE_MAX / size)
+		return NULL;
+	if (__builtin_constant_p(n) && __builtin_constant_p(size))
+		return kmalloc_node(n * size, flags, node);
+	return __kmalloc_node(n * size, flags, node);
+}
+
+static inline void *kcalloc_node(size_t n, size_t size, gfp_t flags, int node)
+{
+	return kmalloc_array_node(n, size, flags | __GFP_ZERO, node);
+}
+
+
 #ifdef CONFIG_NUMA
 extern void *__kmalloc_node_track_caller(size_t, gfp_t, int, unsigned long);
 #define kmalloc_node_track_caller(size, flags, node) \

commit 4fd0b46e898791009b03b2fdd6510044fa8730a6
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Wed Nov 15 17:32:21 2017 -0800

    slab, slub, slob: convert slab_flags_t to 32-bit
    
    struct kmem_cache::flags is "unsigned long" which is unnecessary on
    64-bit as no flags are defined in the higher bits.
    
    Switch the field to 32-bit and save some space on x86_64 until such
    flags appear:
    
            add/remove: 0/0 grow/shrink: 0/107 up/down: 0/-657 (-657)
            function                                     old     new   delta
            sysfs_slab_add                               720     719      -1
                                    ...
            check_object                                 699     676     -23
    
    [akpm@linux-foundation.org: fix printk warning]
    Link: http://lkml.kernel.org/r/20171021100635.GA8287@avx2
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Acked-by: Pekka Enberg <penberg@kernel.org>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 0c4c579f52ed..f37cb93768ab 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -22,19 +22,19 @@
  * The ones marked DEBUG are only valid if CONFIG_DEBUG_SLAB is set.
  */
 /* DEBUG: Perform (expensive) checks on alloc/free */
-#define SLAB_CONSISTENCY_CHECKS	((slab_flags_t __force)0x00000100UL)
+#define SLAB_CONSISTENCY_CHECKS	((slab_flags_t __force)0x00000100U)
 /* DEBUG: Red zone objs in a cache */
-#define SLAB_RED_ZONE		((slab_flags_t __force)0x00000400UL)
+#define SLAB_RED_ZONE		((slab_flags_t __force)0x00000400U)
 /* DEBUG: Poison objects */
-#define SLAB_POISON		((slab_flags_t __force)0x00000800UL)
+#define SLAB_POISON		((slab_flags_t __force)0x00000800U)
 /* Align objs on cache lines */
-#define SLAB_HWCACHE_ALIGN	((slab_flags_t __force)0x00002000UL)
+#define SLAB_HWCACHE_ALIGN	((slab_flags_t __force)0x00002000U)
 /* Use GFP_DMA memory */
-#define SLAB_CACHE_DMA		((slab_flags_t __force)0x00004000UL)
+#define SLAB_CACHE_DMA		((slab_flags_t __force)0x00004000U)
 /* DEBUG: Store the last owner for bug hunting */
-#define SLAB_STORE_USER		((slab_flags_t __force)0x00010000UL)
+#define SLAB_STORE_USER		((slab_flags_t __force)0x00010000U)
 /* Panic if kmem_cache_create() fails */
-#define SLAB_PANIC		((slab_flags_t __force)0x00040000UL)
+#define SLAB_PANIC		((slab_flags_t __force)0x00040000U)
 /*
  * SLAB_TYPESAFE_BY_RCU - **WARNING** READ THIS!
  *
@@ -73,50 +73,50 @@
  * Note that SLAB_TYPESAFE_BY_RCU was originally named SLAB_DESTROY_BY_RCU.
  */
 /* Defer freeing slabs to RCU */
-#define SLAB_TYPESAFE_BY_RCU	((slab_flags_t __force)0x00080000UL)
+#define SLAB_TYPESAFE_BY_RCU	((slab_flags_t __force)0x00080000U)
 /* Spread some memory over cpuset */
-#define SLAB_MEM_SPREAD		((slab_flags_t __force)0x00100000UL)
+#define SLAB_MEM_SPREAD		((slab_flags_t __force)0x00100000U)
 /* Trace allocations and frees */
-#define SLAB_TRACE		((slab_flags_t __force)0x00200000UL)
+#define SLAB_TRACE		((slab_flags_t __force)0x00200000U)
 
 /* Flag to prevent checks on free */
 #ifdef CONFIG_DEBUG_OBJECTS
-# define SLAB_DEBUG_OBJECTS	((slab_flags_t __force)0x00400000UL)
+# define SLAB_DEBUG_OBJECTS	((slab_flags_t __force)0x00400000U)
 #else
-# define SLAB_DEBUG_OBJECTS	((slab_flags_t __force)0x00000000UL)
+# define SLAB_DEBUG_OBJECTS	0
 #endif
 
 /* Avoid kmemleak tracing */
-#define SLAB_NOLEAKTRACE	((slab_flags_t __force)0x00800000UL)
+#define SLAB_NOLEAKTRACE	((slab_flags_t __force)0x00800000U)
 
 /* Don't track use of uninitialized memory */
 #ifdef CONFIG_KMEMCHECK
-# define SLAB_NOTRACK		((slab_flags_t __force)0x01000000UL)
+# define SLAB_NOTRACK		((slab_flags_t __force)0x01000000U)
 #else
-# define SLAB_NOTRACK		((slab_flags_t __force)0x00000000UL)
+# define SLAB_NOTRACK		0
 #endif
 /* Fault injection mark */
 #ifdef CONFIG_FAILSLAB
-# define SLAB_FAILSLAB		((slab_flags_t __force)0x02000000UL)
+# define SLAB_FAILSLAB		((slab_flags_t __force)0x02000000U)
 #else
-# define SLAB_FAILSLAB		((slab_flags_t __force)0x00000000UL)
+# define SLAB_FAILSLAB		0
 #endif
 /* Account to memcg */
 #if defined(CONFIG_MEMCG) && !defined(CONFIG_SLOB)
-# define SLAB_ACCOUNT		((slab_flags_t __force)0x04000000UL)
+# define SLAB_ACCOUNT		((slab_flags_t __force)0x04000000U)
 #else
-# define SLAB_ACCOUNT		((slab_flags_t __force)0x00000000UL)
+# define SLAB_ACCOUNT		0
 #endif
 
 #ifdef CONFIG_KASAN
-#define SLAB_KASAN		((slab_flags_t __force)0x08000000UL)
+#define SLAB_KASAN		((slab_flags_t __force)0x08000000U)
 #else
-#define SLAB_KASAN		((slab_flags_t __force)0x00000000UL)
+#define SLAB_KASAN		0
 #endif
 
 /* The following flags affect the page allocator grouping pages by mobility */
 /* Objects are reclaimable */
-#define SLAB_RECLAIM_ACCOUNT	((slab_flags_t __force)0x00020000UL)
+#define SLAB_RECLAIM_ACCOUNT	((slab_flags_t __force)0x00020000U)
 #define SLAB_TEMPORARY		SLAB_RECLAIM_ACCOUNT	/* Objects are short-lived */
 /*
  * ZERO_SIZE_PTR will be returned for zero sized kmalloc requests.

commit d50112edde1d0c621520e53747044009f11c656b
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Wed Nov 15 17:32:18 2017 -0800

    slab, slub, slob: add slab_flags_t
    
    Add sparse-checked slab_flags_t for struct kmem_cache::flags (SLAB_POISON,
    etc).
    
    SLAB is bloated temporarily by switching to "unsigned long", but only
    temporarily.
    
    Link: http://lkml.kernel.org/r/20171021100225.GA22428@avx2
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Acked-by: Pekka Enberg <penberg@kernel.org>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index af5aa65c7c18..0c4c579f52ed 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -21,13 +21,20 @@
  * Flags to pass to kmem_cache_create().
  * The ones marked DEBUG are only valid if CONFIG_DEBUG_SLAB is set.
  */
-#define SLAB_CONSISTENCY_CHECKS	0x00000100UL	/* DEBUG: Perform (expensive) checks on alloc/free */
-#define SLAB_RED_ZONE		0x00000400UL	/* DEBUG: Red zone objs in a cache */
-#define SLAB_POISON		0x00000800UL	/* DEBUG: Poison objects */
-#define SLAB_HWCACHE_ALIGN	0x00002000UL	/* Align objs on cache lines */
-#define SLAB_CACHE_DMA		0x00004000UL	/* Use GFP_DMA memory */
-#define SLAB_STORE_USER		0x00010000UL	/* DEBUG: Store the last owner for bug hunting */
-#define SLAB_PANIC		0x00040000UL	/* Panic if kmem_cache_create() fails */
+/* DEBUG: Perform (expensive) checks on alloc/free */
+#define SLAB_CONSISTENCY_CHECKS	((slab_flags_t __force)0x00000100UL)
+/* DEBUG: Red zone objs in a cache */
+#define SLAB_RED_ZONE		((slab_flags_t __force)0x00000400UL)
+/* DEBUG: Poison objects */
+#define SLAB_POISON		((slab_flags_t __force)0x00000800UL)
+/* Align objs on cache lines */
+#define SLAB_HWCACHE_ALIGN	((slab_flags_t __force)0x00002000UL)
+/* Use GFP_DMA memory */
+#define SLAB_CACHE_DMA		((slab_flags_t __force)0x00004000UL)
+/* DEBUG: Store the last owner for bug hunting */
+#define SLAB_STORE_USER		((slab_flags_t __force)0x00010000UL)
+/* Panic if kmem_cache_create() fails */
+#define SLAB_PANIC		((slab_flags_t __force)0x00040000UL)
 /*
  * SLAB_TYPESAFE_BY_RCU - **WARNING** READ THIS!
  *
@@ -65,44 +72,51 @@
  *
  * Note that SLAB_TYPESAFE_BY_RCU was originally named SLAB_DESTROY_BY_RCU.
  */
-#define SLAB_TYPESAFE_BY_RCU	0x00080000UL	/* Defer freeing slabs to RCU */
-#define SLAB_MEM_SPREAD		0x00100000UL	/* Spread some memory over cpuset */
-#define SLAB_TRACE		0x00200000UL	/* Trace allocations and frees */
+/* Defer freeing slabs to RCU */
+#define SLAB_TYPESAFE_BY_RCU	((slab_flags_t __force)0x00080000UL)
+/* Spread some memory over cpuset */
+#define SLAB_MEM_SPREAD		((slab_flags_t __force)0x00100000UL)
+/* Trace allocations and frees */
+#define SLAB_TRACE		((slab_flags_t __force)0x00200000UL)
 
 /* Flag to prevent checks on free */
 #ifdef CONFIG_DEBUG_OBJECTS
-# define SLAB_DEBUG_OBJECTS	0x00400000UL
+# define SLAB_DEBUG_OBJECTS	((slab_flags_t __force)0x00400000UL)
 #else
-# define SLAB_DEBUG_OBJECTS	0x00000000UL
+# define SLAB_DEBUG_OBJECTS	((slab_flags_t __force)0x00000000UL)
 #endif
 
-#define SLAB_NOLEAKTRACE	0x00800000UL	/* Avoid kmemleak tracing */
+/* Avoid kmemleak tracing */
+#define SLAB_NOLEAKTRACE	((slab_flags_t __force)0x00800000UL)
 
 /* Don't track use of uninitialized memory */
 #ifdef CONFIG_KMEMCHECK
-# define SLAB_NOTRACK		0x01000000UL
+# define SLAB_NOTRACK		((slab_flags_t __force)0x01000000UL)
 #else
-# define SLAB_NOTRACK		0x00000000UL
+# define SLAB_NOTRACK		((slab_flags_t __force)0x00000000UL)
 #endif
+/* Fault injection mark */
 #ifdef CONFIG_FAILSLAB
-# define SLAB_FAILSLAB		0x02000000UL	/* Fault injection mark */
+# define SLAB_FAILSLAB		((slab_flags_t __force)0x02000000UL)
 #else
-# define SLAB_FAILSLAB		0x00000000UL
+# define SLAB_FAILSLAB		((slab_flags_t __force)0x00000000UL)
 #endif
+/* Account to memcg */
 #if defined(CONFIG_MEMCG) && !defined(CONFIG_SLOB)
-# define SLAB_ACCOUNT		0x04000000UL	/* Account to memcg */
+# define SLAB_ACCOUNT		((slab_flags_t __force)0x04000000UL)
 #else
-# define SLAB_ACCOUNT		0x00000000UL
+# define SLAB_ACCOUNT		((slab_flags_t __force)0x00000000UL)
 #endif
 
 #ifdef CONFIG_KASAN
-#define SLAB_KASAN		0x08000000UL
+#define SLAB_KASAN		((slab_flags_t __force)0x08000000UL)
 #else
-#define SLAB_KASAN		0x00000000UL
+#define SLAB_KASAN		((slab_flags_t __force)0x00000000UL)
 #endif
 
 /* The following flags affect the page allocator grouping pages by mobility */
-#define SLAB_RECLAIM_ACCOUNT	0x00020000UL		/* Objects are reclaimable */
+/* Objects are reclaimable */
+#define SLAB_RECLAIM_ACCOUNT	((slab_flags_t __force)0x00020000UL)
 #define SLAB_TEMPORARY		SLAB_RECLAIM_ACCOUNT	/* Objects are short-lived */
 /*
  * ZERO_SIZE_PTR will be returned for zero sized kmalloc requests.
@@ -128,7 +142,7 @@ void __init kmem_cache_init(void);
 bool slab_is_available(void);
 
 struct kmem_cache *kmem_cache_create(const char *, size_t, size_t,
-			unsigned long,
+			slab_flags_t,
 			void (*)(void *));
 void kmem_cache_destroy(struct kmem_cache *);
 int kmem_cache_shrink(struct kmem_cache *);

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 41473df6dfb0..af5aa65c7c18 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 /*
  * Written by Mark Hemment, 1996 (markhe@nextd.demon.co.uk).
  *

commit dcda9b04713c3f6ff0875652924844fae28286ea
Author: Michal Hocko <mhocko@suse.com>
Date:   Wed Jul 12 14:36:45 2017 -0700

    mm, tree wide: replace __GFP_REPEAT by __GFP_RETRY_MAYFAIL with more useful semantic
    
    __GFP_REPEAT was designed to allow retry-but-eventually-fail semantic to
    the page allocator.  This has been true but only for allocations
    requests larger than PAGE_ALLOC_COSTLY_ORDER.  It has been always
    ignored for smaller sizes.  This is a bit unfortunate because there is
    no way to express the same semantic for those requests and they are
    considered too important to fail so they might end up looping in the
    page allocator for ever, similarly to GFP_NOFAIL requests.
    
    Now that the whole tree has been cleaned up and accidental or misled
    usage of __GFP_REPEAT flag has been removed for !costly requests we can
    give the original flag a better name and more importantly a more useful
    semantic.  Let's rename it to __GFP_RETRY_MAYFAIL which tells the user
    that the allocator would try really hard but there is no promise of a
    success.  This will work independent of the order and overrides the
    default allocator behavior.  Page allocator users have several levels of
    guarantee vs.  cost options (take GFP_KERNEL as an example)
    
     - GFP_KERNEL & ~__GFP_RECLAIM - optimistic allocation without _any_
       attempt to free memory at all. The most light weight mode which even
       doesn't kick the background reclaim. Should be used carefully because
       it might deplete the memory and the next user might hit the more
       aggressive reclaim
    
     - GFP_KERNEL & ~__GFP_DIRECT_RECLAIM (or GFP_NOWAIT)- optimistic
       allocation without any attempt to free memory from the current
       context but can wake kswapd to reclaim memory if the zone is below
       the low watermark. Can be used from either atomic contexts or when
       the request is a performance optimization and there is another
       fallback for a slow path.
    
     - (GFP_KERNEL|__GFP_HIGH) & ~__GFP_DIRECT_RECLAIM (aka GFP_ATOMIC) -
       non sleeping allocation with an expensive fallback so it can access
       some portion of memory reserves. Usually used from interrupt/bh
       context with an expensive slow path fallback.
    
     - GFP_KERNEL - both background and direct reclaim are allowed and the
       _default_ page allocator behavior is used. That means that !costly
       allocation requests are basically nofail but there is no guarantee of
       that behavior so failures have to be checked properly by callers
       (e.g. OOM killer victim is allowed to fail currently).
    
     - GFP_KERNEL | __GFP_NORETRY - overrides the default allocator behavior
       and all allocation requests fail early rather than cause disruptive
       reclaim (one round of reclaim in this implementation). The OOM killer
       is not invoked.
    
     - GFP_KERNEL | __GFP_RETRY_MAYFAIL - overrides the default allocator
       behavior and all allocation requests try really hard. The request
       will fail if the reclaim cannot make any progress. The OOM killer
       won't be triggered.
    
     - GFP_KERNEL | __GFP_NOFAIL - overrides the default allocator behavior
       and all allocation requests will loop endlessly until they succeed.
       This might be really dangerous especially for larger orders.
    
    Existing users of __GFP_REPEAT are changed to __GFP_RETRY_MAYFAIL
    because they already had their semantic.  No new users are added.
    __alloc_pages_slowpath is changed to bail out for __GFP_RETRY_MAYFAIL if
    there is no progress and we have already passed the OOM point.
    
    This means that all the reclaim opportunities have been exhausted except
    the most disruptive one (the OOM killer) and a user defined fallback
    behavior is more sensible than keep retrying in the page allocator.
    
    [akpm@linux-foundation.org: fix arch/sparc/kernel/mdesc.c]
    [mhocko@suse.com: semantic fix]
      Link: http://lkml.kernel.org/r/20170626123847.GM11534@dhcp22.suse.cz
    [mhocko@kernel.org: address other thing spotted by Vlastimil]
      Link: http://lkml.kernel.org/r/20170626124233.GN11534@dhcp22.suse.cz
    Link: http://lkml.kernel.org/r/20170623085345.11304-3-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Alex Belits <alex.belits@cavium.com>
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Darrick J. Wong <darrick.wong@oracle.com>
    Cc: David Daney <david.daney@cavium.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: NeilBrown <neilb@suse.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 04a7f7993e67..41473df6dfb0 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -471,7 +471,8 @@ static __always_inline void *kmalloc_large(size_t size, gfp_t flags)
  *
  * %__GFP_NOWARN - If allocation fails, don't issue any warnings.
  *
- * %__GFP_REPEAT - If allocation fails initially, try once more before failing.
+ * %__GFP_RETRY_MAYFAIL - Try really hard to succeed the allocation but fail
+ *   eventually.
  *
  * There are other flags available as well, but these are not intended
  * for general use, and so are not documented here. For a full list of

commit 5f0d5a3ae7cff0d7fa943c199c3a2e44f23e1fac
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Jan 18 02:53:44 2017 -0800

    mm: Rename SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU
    
    A group of Linux kernel hackers reported chasing a bug that resulted
    from their assumption that SLAB_DESTROY_BY_RCU provided an existence
    guarantee, that is, that no block from such a slab would be reallocated
    during an RCU read-side critical section.  Of course, that is not the
    case.  Instead, SLAB_DESTROY_BY_RCU only prevents freeing of an entire
    slab of blocks.
    
    However, there is a phrase for this, namely "type safety".  This commit
    therefore renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU in order
    to avoid future instances of this sort of confusion.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: <linux-mm@kvack.org>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    [ paulmck: Add comments mentioning the old name, as requested by Eric
      Dumazet, in order to help people familiar with the old name find
      the new one. ]
    Acked-by: David Rientjes <rientjes@google.com>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 3c37a8c51921..04a7f7993e67 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -28,7 +28,7 @@
 #define SLAB_STORE_USER		0x00010000UL	/* DEBUG: Store the last owner for bug hunting */
 #define SLAB_PANIC		0x00040000UL	/* Panic if kmem_cache_create() fails */
 /*
- * SLAB_DESTROY_BY_RCU - **WARNING** READ THIS!
+ * SLAB_TYPESAFE_BY_RCU - **WARNING** READ THIS!
  *
  * This delays freeing the SLAB page by a grace period, it does _NOT_
  * delay object freeing. This means that if you do kmem_cache_free()
@@ -61,8 +61,10 @@
  *
  * rcu_read_lock before reading the address, then rcu_read_unlock after
  * taking the spinlock within the structure expected at that address.
+ *
+ * Note that SLAB_TYPESAFE_BY_RCU was originally named SLAB_DESTROY_BY_RCU.
  */
-#define SLAB_DESTROY_BY_RCU	0x00080000UL	/* Defer freeing slabs to RCU */
+#define SLAB_TYPESAFE_BY_RCU	0x00080000UL	/* Defer freeing slabs to RCU */
 #define SLAB_MEM_SPREAD		0x00100000UL	/* Spread some memory over cpuset */
 #define SLAB_TRACE		0x00200000UL	/* Trace allocations and frees */
 

commit 01fb58bcba63f8fba37581c24c99e9a515dd0335
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Feb 22 15:41:30 2017 -0800

    slab: remove synchronous synchronize_sched() from memcg cache deactivation path
    
    With kmem cgroup support enabled, kmem_caches can be created and
    destroyed frequently and a great number of near empty kmem_caches can
    accumulate if there are a lot of transient cgroups and the system is not
    under memory pressure.  When memory reclaim starts under such
    conditions, it can lead to consecutive deactivation and destruction of
    many kmem_caches, easily hundreds of thousands on moderately large
    systems, exposing scalability issues in the current slab management
    code.  This is one of the patches to address the issue.
    
    slub uses synchronize_sched() to deactivate a memcg cache.
    synchronize_sched() is an expensive and slow operation and doesn't scale
    when a huge number of caches are destroyed back-to-back.  While there
    used to be a simple batching mechanism, the batching was too restricted
    to be helpful.
    
    This patch implements slab_deactivate_memcg_cache_rcu_sched() which slub
    can use to schedule sched RCU callback instead of performing
    synchronize_sched() synchronously while holding cgroup_mutex.  While
    this adds online cpus, mems and slab_mutex operations, operating on
    these locks back-to-back from the same kworker, which is what's gonna
    happen when there are many to deactivate, isn't expensive at all and
    this gets rid of the scalability problem completely.
    
    Link: http://lkml.kernel.org/r/20170117235411.9408-9-tj@kernel.org
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reported-by: Jay Vana <jsvana@fb.com>
    Acked-by: Vladimir Davydov <vdavydov.dev@gmail.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index af1a5bef80f4..3c37a8c51921 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -582,6 +582,12 @@ struct memcg_cache_params {
 			struct mem_cgroup *memcg;
 			struct list_head children_node;
 			struct list_head kmem_caches_node;
+
+			void (*deact_fn)(struct kmem_cache *);
+			union {
+				struct rcu_head deact_rcu_head;
+				struct work_struct deact_work;
+			};
 		};
 	};
 };

commit 510ded33e075c2bd662b1efab0110f4240325fc9
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Feb 22 15:41:24 2017 -0800

    slab: implement slab_root_caches list
    
    With kmem cgroup support enabled, kmem_caches can be created and
    destroyed frequently and a great number of near empty kmem_caches can
    accumulate if there are a lot of transient cgroups and the system is not
    under memory pressure.  When memory reclaim starts under such
    conditions, it can lead to consecutive deactivation and destruction of
    many kmem_caches, easily hundreds of thousands on moderately large
    systems, exposing scalability issues in the current slab management
    code.  This is one of the patches to address the issue.
    
    slab_caches currently lists all caches including root and memcg ones.
    This is the only data structure which lists the root caches and
    iterating root caches can only be done by walking the list while
    skipping over memcg caches.  As there can be a huge number of memcg
    caches, this can become very expensive.
    
    This also can make /proc/slabinfo behave very badly.  seq_file processes
    reads in 4k chunks and seeks to the previous Nth position on slab_caches
    list to resume after each chunk.  With a lot of memcg cache churns on
    the list, reading /proc/slabinfo can become very slow and its content
    often ends up with duplicate and/or missing entries.
    
    This patch adds a new list slab_root_caches which lists only the root
    caches.  When memcg is not enabled, it becomes just an alias of
    slab_caches.  memcg specific list operations are collected into
    memcg_[un]link_cache().
    
    Link: http://lkml.kernel.org/r/20170117235411.9408-7-tj@kernel.org
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reported-by: Jay Vana <jsvana@fb.com>
    Acked-by: Vladimir Davydov <vdavydov@tarantool.org>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index a0cc7a77cda2..af1a5bef80f4 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -556,6 +556,8 @@ struct memcg_cache_array {
  *		used to index child cachces during allocation and cleared
  *		early during shutdown.
  *
+ * @root_caches_node: List node for slab_root_caches list.
+ *
  * @children:	List of all child caches.  While the child caches are also
  *		reachable through @memcg_caches, a child cache remains on
  *		this list until it is actually destroyed.
@@ -573,6 +575,7 @@ struct memcg_cache_params {
 	union {
 		struct {
 			struct memcg_cache_array __rcu *memcg_caches;
+			struct list_head __root_caches_node;
 			struct list_head children;
 		};
 		struct {

commit bc2791f857e1984b7548d2a2de2ffb1a913dee62
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Feb 22 15:41:21 2017 -0800

    slab: link memcg kmem_caches on their associated memory cgroup
    
    With kmem cgroup support enabled, kmem_caches can be created and
    destroyed frequently and a great number of near empty kmem_caches can
    accumulate if there are a lot of transient cgroups and the system is not
    under memory pressure.  When memory reclaim starts under such
    conditions, it can lead to consecutive deactivation and destruction of
    many kmem_caches, easily hundreds of thousands on moderately large
    systems, exposing scalability issues in the current slab management
    code.  This is one of the patches to address the issue.
    
    While a memcg kmem_cache is listed on its root cache's ->children list,
    there is no direct way to iterate all kmem_caches which are assocaited
    with a memory cgroup.  The only way to iterate them is walking all
    caches while filtering out caches which don't match, which would be most
    of them.
    
    This makes memcg destruction operations O(N^2) where N is the total
    number of slab caches which can be huge.  This combined with the
    synchronous RCU operations can tie up a CPU and affect the whole machine
    for many hours when memory reclaim triggers offlining and destruction of
    the stale memcgs.
    
    This patch adds mem_cgroup->kmem_caches list which goes through
    memcg_cache_params->kmem_caches_node of all kmem_caches which are
    associated with the memcg.  All memcg specific iterations, including
    stat file access, are updated to use the new list instead.
    
    Link: http://lkml.kernel.org/r/20170117235411.9408-6-tj@kernel.org
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reported-by: Jay Vana <jsvana@fb.com>
    Acked-by: Vladimir Davydov <vdavydov.dev@gmail.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 1f611ba00f1d..a0cc7a77cda2 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -565,6 +565,8 @@ struct memcg_cache_array {
  * @memcg:	Pointer to the memcg this cache belongs to.
  *
  * @children_node: List node for @root_cache->children list.
+ *
+ * @kmem_caches_node: List node for @memcg->kmem_caches list.
  */
 struct memcg_cache_params {
 	struct kmem_cache *root_cache;
@@ -576,6 +578,7 @@ struct memcg_cache_params {
 		struct {
 			struct mem_cgroup *memcg;
 			struct list_head children_node;
+			struct list_head kmem_caches_node;
 		};
 	};
 };

commit 9eeadc8b6e0e31f9aea1f8886ef472f62c2b7f55
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Feb 22 15:41:17 2017 -0800

    slab: reorganize memcg_cache_params
    
    We're going to change how memcg caches are iterated.  In preparation,
    clean up and reorganize memcg_cache_params.
    
    * The shared ->list is replaced by ->children in root and
      ->children_node in children.
    
    * ->is_root_cache is removed.  Instead ->root_cache is moved out of
      the child union and now used by both root and children.  NULL
      indicates root cache.  Non-NULL a memcg one.
    
    This patch doesn't cause any observable behavior changes.
    
    Link: http://lkml.kernel.org/r/20170117235411.9408-5-tj@kernel.org
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Vladimir Davydov <vdavydov.dev@gmail.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 4c5363566815..1f611ba00f1d 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -545,22 +545,37 @@ struct memcg_cache_array {
  * array to be accessed without taking any locks, on relocation we free the old
  * version only after a grace period.
  *
- * Child caches will hold extra metadata needed for its operation. Fields are:
+ * Root and child caches hold different metadata.
  *
- * @memcg: pointer to the memcg this cache belongs to
- * @root_cache: pointer to the global, root cache, this cache was derived from
+ * @root_cache:	Common to root and child caches.  NULL for root, pointer to
+ *		the root cache for children.
  *
- * Both root and child caches of the same kind are linked into a list chained
- * through @list.
+ * The following fields are specific to root caches.
+ *
+ * @memcg_caches: kmemcg ID indexed table of child caches.  This table is
+ *		used to index child cachces during allocation and cleared
+ *		early during shutdown.
+ *
+ * @children:	List of all child caches.  While the child caches are also
+ *		reachable through @memcg_caches, a child cache remains on
+ *		this list until it is actually destroyed.
+ *
+ * The following fields are specific to child caches.
+ *
+ * @memcg:	Pointer to the memcg this cache belongs to.
+ *
+ * @children_node: List node for @root_cache->children list.
  */
 struct memcg_cache_params {
-	bool is_root_cache;
-	struct list_head list;
+	struct kmem_cache *root_cache;
 	union {
-		struct memcg_cache_array __rcu *memcg_caches;
+		struct {
+			struct memcg_cache_array __rcu *memcg_caches;
+			struct list_head children;
+		};
 		struct {
 			struct mem_cgroup *memcg;
-			struct kmem_cache *root_cache;
+			struct list_head children_node;
 		};
 	};
 };

commit bb1107f7c6052c863692a41f78c000db792334bf
Author: Michal Hocko <mhocko@suse.com>
Date:   Tue Jan 10 16:57:27 2017 -0800

    mm, slab: make sure that KMALLOC_MAX_SIZE will fit into MAX_ORDER
    
    Andrey Konovalov has reported the following warning triggered by the
    syzkaller fuzzer.
    
      WARNING: CPU: 1 PID: 9935 at mm/page_alloc.c:3511 __alloc_pages_nodemask+0x159c/0x1e20
      Kernel panic - not syncing: panic_on_warn set ...
      CPU: 1 PID: 9935 Comm: syz-executor0 Not tainted 4.9.0-rc7+ #34
      Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS Bochs 01/01/2011
      Call Trace:
        __alloc_pages_slowpath mm/page_alloc.c:3511
        __alloc_pages_nodemask+0x159c/0x1e20 mm/page_alloc.c:3781
        alloc_pages_current+0x1c7/0x6b0 mm/mempolicy.c:2072
        alloc_pages include/linux/gfp.h:469
        kmalloc_order+0x1f/0x70 mm/slab_common.c:1015
        kmalloc_order_trace+0x1f/0x160 mm/slab_common.c:1026
        kmalloc_large include/linux/slab.h:422
        __kmalloc+0x210/0x2d0 mm/slub.c:3723
        kmalloc include/linux/slab.h:495
        ep_write_iter+0x167/0xb50 drivers/usb/gadget/legacy/inode.c:664
        new_sync_write fs/read_write.c:499
        __vfs_write+0x483/0x760 fs/read_write.c:512
        vfs_write+0x170/0x4e0 fs/read_write.c:560
        SYSC_write fs/read_write.c:607
        SyS_write+0xfb/0x230 fs/read_write.c:599
        entry_SYSCALL_64_fastpath+0x1f/0xc2
    
    The issue is caused by a lack of size check for the request size in
    ep_write_iter which should be fixed.  It, however, points to another
    problem, that SLUB defines KMALLOC_MAX_SIZE too large because the its
    KMALLOC_SHIFT_MAX is (MAX_ORDER + PAGE_SHIFT) which means that the
    resulting page allocator request might be MAX_ORDER which is too large
    (see __alloc_pages_slowpath).
    
    The same applies to the SLOB allocator which allows even larger sizes.
    Make sure that they are capped properly and never request more than
    MAX_ORDER order.
    
    Link: http://lkml.kernel.org/r/20161220130659.16461-2-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Reported-by: Andrey Konovalov <andreyknvl@google.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Cc: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 084b12bad198..4c5363566815 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -226,7 +226,7 @@ static inline const char *__check_heap_object(const void *ptr,
  * (PAGE_SIZE*2).  Larger requests are passed to the page allocator.
  */
 #define KMALLOC_SHIFT_HIGH	(PAGE_SHIFT + 1)
-#define KMALLOC_SHIFT_MAX	(MAX_ORDER + PAGE_SHIFT)
+#define KMALLOC_SHIFT_MAX	(MAX_ORDER + PAGE_SHIFT - 1)
 #ifndef KMALLOC_SHIFT_LOW
 #define KMALLOC_SHIFT_LOW	3
 #endif
@@ -239,7 +239,7 @@ static inline const char *__check_heap_object(const void *ptr,
  * be allocated from the same page.
  */
 #define KMALLOC_SHIFT_HIGH	PAGE_SHIFT
-#define KMALLOC_SHIFT_MAX	30
+#define KMALLOC_SHIFT_MAX	(MAX_ORDER + PAGE_SHIFT - 1)
 #ifndef KMALLOC_SHIFT_LOW
 #define KMALLOC_SHIFT_LOW	3
 #endif

commit 6731d4f12315aed5f7eefc52dac30428e382d7d0
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Aug 23 14:53:19 2016 +0200

    slab: Convert to hotplug state machine
    
    Install the callbacks via the state machine.
    
    Signed-off-by: Richard Weinberger <richard@nod.at>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Reviewed-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: linux-mm@kvack.org
    Cc: rt@linutronix.de
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Christoph Lameter <cl@linux.com>
    Link: http://lkml.kernel.org/r/20160823125319.abeapfjapf2kfezp@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 4293808d8cfb..084b12bad198 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -650,4 +650,12 @@ static inline void *kzalloc_node(size_t size, gfp_t flags, int node)
 unsigned int kmem_cache_size(struct kmem_cache *s);
 void __init kmem_cache_init_late(void);
 
+#if defined(CONFIG_SMP) && defined(CONFIG_SLAB)
+int slab_prepare_cpu(unsigned int cpu);
+int slab_dead_cpu(unsigned int cpu);
+#else
+#define slab_prepare_cpu	NULL
+#define slab_dead_cpu		NULL
+#endif
+
 #endif	/* _LINUX_SLAB_H */

commit 1eccfa090eaea22558570054bbdc147817e1df5e
Merge: 1bd4403d86a1 ed18adc1cdd0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 8 14:48:14 2016 -0700

    Merge tag 'usercopy-v4.8' of git://git.kernel.org/pub/scm/linux/kernel/git/kees/linux
    
    Pull usercopy protection from Kees Cook:
     "Tbhis implements HARDENED_USERCOPY verification of copy_to_user and
      copy_from_user bounds checking for most architectures on SLAB and
      SLUB"
    
    * tag 'usercopy-v4.8' of git://git.kernel.org/pub/scm/linux/kernel/git/kees/linux:
      mm: SLUB hardened usercopy support
      mm: SLAB hardened usercopy support
      s390/uaccess: Enable hardened usercopy
      sparc/uaccess: Enable hardened usercopy
      powerpc/uaccess: Enable hardened usercopy
      ia64/uaccess: Enable hardened usercopy
      arm64/uaccess: Enable hardened usercopy
      ARM: uaccess: Enable hardened usercopy
      x86/uaccess: Enable hardened usercopy
      mm: Hardened usercopy
      mm: Implement stack frame object validation
      mm: Add is_migrate_cma_page

commit 91c6a05f72a996bee5133e76374ab3ad7d3b9b72
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Tue Jul 26 15:22:08 2016 -0700

    mm: faster kmalloc_array(), kcalloc()
    
    When both arguments to kmalloc_array() or kcalloc() are known at compile
    time then their product is known at compile time but search for kmalloc
    cache happens at runtime not at compile time.
    
    Link: http://lkml.kernel.org/r/20160627213454.GA2440@p183.telecom.by
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index aeb3e6d00a66..1a4ea551aae5 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -565,6 +565,8 @@ static inline void *kmalloc_array(size_t n, size_t size, gfp_t flags)
 {
 	if (size != 0 && n > SIZE_MAX / size)
 		return NULL;
+	if (__builtin_constant_p(n) && __builtin_constant_p(size))
+		return kmalloc(n * size, flags);
 	return __kmalloc(n * size, flags);
 }
 

commit f5509cc18daa7f82bcc553be70df2117c8eedc16
Author: Kees Cook <keescook@chromium.org>
Date:   Tue Jun 7 11:05:33 2016 -0700

    mm: Hardened usercopy
    
    This is the start of porting PAX_USERCOPY into the mainline kernel. This
    is the first set of features, controlled by CONFIG_HARDENED_USERCOPY. The
    work is based on code by PaX Team and Brad Spengler, and an earlier port
    from Casey Schaufler. Additional non-slab page tests are from Rik van Riel.
    
    This patch contains the logic for validating several conditions when
    performing copy_to_user() and copy_from_user() on the kernel object
    being copied to/from:
    - address range doesn't wrap around
    - address range isn't NULL or zero-allocated (with a non-zero copy size)
    - if on the slab allocator:
      - object size must be less than or equal to copy size (when check is
        implemented in the allocator, which appear in subsequent patches)
    - otherwise, object must not span page allocations (excepting Reserved
      and CMA ranges)
    - if on the stack
      - object must not extend before/after the current process stack
      - object must be contained by a valid stack frame (when there is
        arch/build support for identifying stack frames)
    - object must not overlap with kernel text
    
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Tested-by: Valdis Kletnieks <valdis.kletnieks@vt.edu>
    Tested-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index aeb3e6d00a66..96a16a3fb7cb 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -155,6 +155,18 @@ void kfree(const void *);
 void kzfree(const void *);
 size_t ksize(const void *);
 
+#ifdef CONFIG_HAVE_HARDENED_USERCOPY_ALLOCATOR
+const char *__check_heap_object(const void *ptr, unsigned long n,
+				struct page *page);
+#else
+static inline const char *__check_heap_object(const void *ptr,
+					      unsigned long n,
+					      struct page *page)
+{
+	return NULL;
+}
+#endif
+
 /*
  * Some archs want to perform DMA into kmalloc caches and need a guaranteed
  * alignment larger than the alignment of a 64-bit integer.

commit 48a270554a3251681ae11173f2fd6389d943e183
Author: Rasmus Villemoes <linux@rasmusvillemoes.dk>
Date:   Thu May 19 17:10:55 2016 -0700

    include/linux: apply __malloc attribute
    
    Attach the malloc attribute to a few allocation functions.  This helps
    gcc generate better code by telling it that the return value doesn't
    alias any existing pointers (which is even more valuable given the
    pessimizations implied by -fno-strict-aliasing).
    
    A simple example of what this allows gcc to do can be seen by looking at
    the last part of drm_atomic_helper_plane_reset:
    
            plane->state = kzalloc(sizeof(*plane->state), GFP_KERNEL);
    
            if (plane->state) {
                    plane->state->plane = plane;
                    plane->state->rotation = BIT(DRM_ROTATE_0);
            }
    
    which compiles to
    
        e8 99 bf d6 ff          callq  ffffffff8116d540 <kmem_cache_alloc_trace>
        48 85 c0                test   %rax,%rax
        48 89 83 40 02 00 00    mov    %rax,0x240(%rbx)
        74 11                   je     ffffffff814015c4 <drm_atomic_helper_plane_reset+0x64>
        48 89 18                mov    %rbx,(%rax)
        48 8b 83 40 02 00 00    mov    0x240(%rbx),%rax [*]
        c7 40 40 01 00 00 00    movl   $0x1,0x40(%rax)
    
    With this patch applied, the instruction at [*] is elided, since the
    store to plane->state->plane is known to not alter the value of
    plane->state.
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: Rasmus Villemoes <linux@rasmusvillemoes.dk>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 508bd827e6dc..aeb3e6d00a66 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -315,8 +315,8 @@ static __always_inline int kmalloc_index(size_t size)
 }
 #endif /* !CONFIG_SLOB */
 
-void *__kmalloc(size_t size, gfp_t flags) __assume_kmalloc_alignment;
-void *kmem_cache_alloc(struct kmem_cache *, gfp_t flags) __assume_slab_alignment;
+void *__kmalloc(size_t size, gfp_t flags) __assume_kmalloc_alignment __malloc;
+void *kmem_cache_alloc(struct kmem_cache *, gfp_t flags) __assume_slab_alignment __malloc;
 void kmem_cache_free(struct kmem_cache *, void *);
 
 /*
@@ -339,8 +339,8 @@ static __always_inline void kfree_bulk(size_t size, void **p)
 }
 
 #ifdef CONFIG_NUMA
-void *__kmalloc_node(size_t size, gfp_t flags, int node) __assume_kmalloc_alignment;
-void *kmem_cache_alloc_node(struct kmem_cache *, gfp_t flags, int node) __assume_slab_alignment;
+void *__kmalloc_node(size_t size, gfp_t flags, int node) __assume_kmalloc_alignment __malloc;
+void *kmem_cache_alloc_node(struct kmem_cache *, gfp_t flags, int node) __assume_slab_alignment __malloc;
 #else
 static __always_inline void *__kmalloc_node(size_t size, gfp_t flags, int node)
 {
@@ -354,12 +354,12 @@ static __always_inline void *kmem_cache_alloc_node(struct kmem_cache *s, gfp_t f
 #endif
 
 #ifdef CONFIG_TRACING
-extern void *kmem_cache_alloc_trace(struct kmem_cache *, gfp_t, size_t) __assume_slab_alignment;
+extern void *kmem_cache_alloc_trace(struct kmem_cache *, gfp_t, size_t) __assume_slab_alignment __malloc;
 
 #ifdef CONFIG_NUMA
 extern void *kmem_cache_alloc_node_trace(struct kmem_cache *s,
 					   gfp_t gfpflags,
-					   int node, size_t size) __assume_slab_alignment;
+					   int node, size_t size) __assume_slab_alignment __malloc;
 #else
 static __always_inline void *
 kmem_cache_alloc_node_trace(struct kmem_cache *s,
@@ -392,10 +392,10 @@ kmem_cache_alloc_node_trace(struct kmem_cache *s,
 }
 #endif /* CONFIG_TRACING */
 
-extern void *kmalloc_order(size_t size, gfp_t flags, unsigned int order) __assume_page_alignment;
+extern void *kmalloc_order(size_t size, gfp_t flags, unsigned int order) __assume_page_alignment __malloc;
 
 #ifdef CONFIG_TRACING
-extern void *kmalloc_order_trace(size_t size, gfp_t flags, unsigned int order) __assume_page_alignment;
+extern void *kmalloc_order_trace(size_t size, gfp_t flags, unsigned int order) __assume_page_alignment __malloc;
 #else
 static __always_inline void *
 kmalloc_order_trace(size_t size, gfp_t flags, unsigned int order)

commit 505f5dcb1c419e55a9621a01f83eb5745d8d7398
Author: Alexander Potapenko <glider@google.com>
Date:   Fri Mar 25 14:22:02 2016 -0700

    mm, kasan: add GFP flags to KASAN API
    
    Add GFP flags to KASAN hooks for future patches to use.
    
    This patch is based on the "mm: kasan: unified support for SLUB and SLAB
    allocators" patch originally prepared by Dmitry Chernenkov.
    
    Signed-off-by: Alexander Potapenko <glider@google.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Andrey Konovalov <adech.fo@gmail.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Andrey Ryabinin <ryabinin.a.a@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Konstantin Serebryany <kcc@google.com>
    Cc: Dmitry Chernenkov <dmitryc@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index aa61595a1482..508bd827e6dc 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -376,7 +376,7 @@ static __always_inline void *kmem_cache_alloc_trace(struct kmem_cache *s,
 {
 	void *ret = kmem_cache_alloc(s, flags);
 
-	kasan_kmalloc(s, ret, size);
+	kasan_kmalloc(s, ret, size, flags);
 	return ret;
 }
 
@@ -387,7 +387,7 @@ kmem_cache_alloc_node_trace(struct kmem_cache *s,
 {
 	void *ret = kmem_cache_alloc_node(s, gfpflags, node);
 
-	kasan_kmalloc(s, ret, size);
+	kasan_kmalloc(s, ret, size, gfpflags);
 	return ret;
 }
 #endif /* CONFIG_TRACING */

commit 7ed2f9e663854db313f177a511145630e398b402
Author: Alexander Potapenko <glider@google.com>
Date:   Fri Mar 25 14:21:59 2016 -0700

    mm, kasan: SLAB support
    
    Add KASAN hooks to SLAB allocator.
    
    This patch is based on the "mm: kasan: unified support for SLUB and SLAB
    allocators" patch originally prepared by Dmitry Chernenkov.
    
    Signed-off-by: Alexander Potapenko <glider@google.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Andrey Konovalov <adech.fo@gmail.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Andrey Ryabinin <ryabinin.a.a@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Konstantin Serebryany <kcc@google.com>
    Cc: Dmitry Chernenkov <dmitryc@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index e4b568738ca3..aa61595a1482 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -92,6 +92,12 @@
 # define SLAB_ACCOUNT		0x00000000UL
 #endif
 
+#ifdef CONFIG_KASAN
+#define SLAB_KASAN		0x08000000UL
+#else
+#define SLAB_KASAN		0x00000000UL
+#endif
+
 /* The following flags affect the page allocator grouping pages by mobility */
 #define SLAB_RECLAIM_ACCOUNT	0x00020000UL		/* Objects are reclaimable */
 #define SLAB_TEMPORARY		SLAB_RECLAIM_ACCOUNT	/* Objects are short-lived */

commit becfda68abca673d61d5cc953e8e099816db99d9
Author: Laura Abbott <labbott@fedoraproject.org>
Date:   Tue Mar 15 14:55:06 2016 -0700

    slub: convert SLAB_DEBUG_FREE to SLAB_CONSISTENCY_CHECKS
    
    SLAB_DEBUG_FREE allows expensive consistency checks at free to be turned
    on or off.  Expand its use to be able to turn off all consistency
    checks.  This gives a nice speed up if you only want features such as
    poisoning or tracing.
    
    Credit to Mathias Krause for the original work which inspired this
    series
    
    Signed-off-by: Laura Abbott <labbott@fedoraproject.org>
    Acked-by: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <js1304@gmail.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Mathias Krause <minipli@googlemail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 5d49f0c60dcb..e4b568738ca3 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -20,7 +20,7 @@
  * Flags to pass to kmem_cache_create().
  * The ones marked DEBUG are only valid if CONFIG_DEBUG_SLAB is set.
  */
-#define SLAB_DEBUG_FREE		0x00000100UL	/* DEBUG: Perform (expensive) checks on free */
+#define SLAB_CONSISTENCY_CHECKS	0x00000100UL	/* DEBUG: Perform (expensive) checks on alloc/free */
 #define SLAB_RED_ZONE		0x00000400UL	/* DEBUG: Red zone objs in a cache */
 #define SLAB_POISON		0x00000800UL	/* DEBUG: Poison objects */
 #define SLAB_HWCACHE_ALIGN	0x00002000UL	/* Align objs on cache lines */

commit 9f706d6820d3ea776d6b3fc0c1de9f81eb0d021b
Author: Jesper Dangaard Brouer <brouer@redhat.com>
Date:   Tue Mar 15 14:54:03 2016 -0700

    mm: fix some spelling
    
    Fix up trivial spelling errors, noticed while reading the code.
    
    Signed-off-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Vladimir Davydov <vdavydov@virtuozzo.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 9d9a5bdb9b00..5d49f0c60dcb 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -314,7 +314,7 @@ void *kmem_cache_alloc(struct kmem_cache *, gfp_t flags) __assume_slab_alignment
 void kmem_cache_free(struct kmem_cache *, void *);
 
 /*
- * Bulk allocation and freeing operations. These are accellerated in an
+ * Bulk allocation and freeing operations. These are accelerated in an
  * allocator specific way to avoid taking locks repeatedly or building
  * metadata structures unnecessarily.
  *

commit ca257195511d536308700548de008b51729221eb
Author: Jesper Dangaard Brouer <brouer@redhat.com>
Date:   Tue Mar 15 14:54:00 2016 -0700

    mm: new API kfree_bulk() for SLAB+SLUB allocators
    
    This patch introduce a new API call kfree_bulk() for bulk freeing memory
    objects not bound to a single kmem_cache.
    
    Christoph pointed out that it is possible to implement freeing of
    objects, without knowing the kmem_cache pointer as that information is
    available from the object's page->slab_cache.  Proposing to remove the
    kmem_cache argument from the bulk free API.
    
    Jesper demonstrated that these extra steps per object comes at a
    performance cost.  It is only in the case CONFIG_MEMCG_KMEM is compiled
    in and activated runtime that these steps are done anyhow.  The extra
    cost is most visible for SLAB allocator, because the SLUB allocator does
    the page lookup (virt_to_head_page()) anyhow.
    
    Thus, the conclusion was to keep the kmem_cache free bulk API with a
    kmem_cache pointer, but we can still implement a kfree_bulk() API fairly
    easily.  Simply by handling if kmem_cache_free_bulk() gets called with a
    kmem_cache NULL pointer.
    
    This does increase the code size a bit, but implementing a separate
    kfree_bulk() call would likely increase code size even more.
    
    Below benchmarks cost of alloc+free (obj size 256 bytes) on CPU i7-4790K
    @ 4.00GHz, no PREEMPT and CONFIG_MEMCG_KMEM=y.
    
    Code size increase for SLAB:
    
     add/remove: 0/0 grow/shrink: 1/0 up/down: 74/0 (74)
     function                                     old     new   delta
     kmem_cache_free_bulk                         660     734     +74
    
    SLAB fastpath: 87 cycles(tsc) 21.814
      sz - fallback             - kmem_cache_free_bulk - kfree_bulk
       1 - 103 cycles 25.878 ns -  41 cycles 10.498 ns - 81 cycles 20.312 ns
       2 -  94 cycles 23.673 ns -  26 cycles  6.682 ns - 42 cycles 10.649 ns
       3 -  92 cycles 23.181 ns -  21 cycles  5.325 ns - 39 cycles 9.950 ns
       4 -  90 cycles 22.727 ns -  18 cycles  4.673 ns - 26 cycles 6.693 ns
       8 -  89 cycles 22.270 ns -  14 cycles  3.664 ns - 23 cycles 5.835 ns
      16 -  88 cycles 22.038 ns -  14 cycles  3.503 ns - 22 cycles 5.543 ns
      30 -  89 cycles 22.284 ns -  13 cycles  3.310 ns - 20 cycles 5.197 ns
      32 -  88 cycles 22.249 ns -  13 cycles  3.420 ns - 20 cycles 5.166 ns
      34 -  88 cycles 22.224 ns -  14 cycles  3.643 ns - 20 cycles 5.170 ns
      48 -  88 cycles 22.088 ns -  14 cycles  3.507 ns - 20 cycles 5.203 ns
      64 -  88 cycles 22.063 ns -  13 cycles  3.428 ns - 20 cycles 5.152 ns
     128 -  89 cycles 22.483 ns -  15 cycles  3.891 ns - 23 cycles 5.885 ns
     158 -  89 cycles 22.381 ns -  15 cycles  3.779 ns - 22 cycles 5.548 ns
     250 -  91 cycles 22.798 ns -  16 cycles  4.152 ns - 23 cycles 5.967 ns
    
    SLAB when enabling MEMCG_KMEM runtime:
     - kmemcg fastpath: 130 cycles(tsc) 32.684 ns (step:0)
     1 - 148 cycles 37.220 ns -  66 cycles 16.622 ns - 66 cycles 16.583 ns
     2 - 141 cycles 35.510 ns -  51 cycles 12.820 ns - 58 cycles 14.625 ns
     3 - 140 cycles 35.017 ns -  37 cycles 9.326 ns - 33 cycles 8.474 ns
     4 - 137 cycles 34.507 ns -  31 cycles 7.888 ns - 33 cycles 8.300 ns
     8 - 140 cycles 35.069 ns -  25 cycles 6.461 ns - 25 cycles 6.436 ns
     16 - 138 cycles 34.542 ns -  23 cycles 5.945 ns - 22 cycles 5.670 ns
     30 - 136 cycles 34.227 ns -  22 cycles 5.502 ns - 22 cycles 5.587 ns
     32 - 136 cycles 34.253 ns -  21 cycles 5.475 ns - 21 cycles 5.324 ns
     34 - 136 cycles 34.254 ns -  21 cycles 5.448 ns - 20 cycles 5.194 ns
     48 - 136 cycles 34.075 ns -  21 cycles 5.458 ns - 21 cycles 5.367 ns
     64 - 135 cycles 33.994 ns -  21 cycles 5.350 ns - 21 cycles 5.259 ns
     128 - 137 cycles 34.446 ns -  23 cycles 5.816 ns - 22 cycles 5.688 ns
     158 - 137 cycles 34.379 ns -  22 cycles 5.727 ns - 22 cycles 5.602 ns
     250 - 138 cycles 34.755 ns -  24 cycles 6.093 ns - 23 cycles 5.986 ns
    
    Code size increase for SLUB:
     function                                     old     new   delta
     kmem_cache_free_bulk                         717     799     +82
    
    SLUB benchmark:
     SLUB fastpath: 46 cycles(tsc) 11.691 ns (step:0)
      sz - fallback             - kmem_cache_free_bulk - kfree_bulk
       1 -  61 cycles 15.486 ns -  53 cycles 13.364 ns - 57 cycles 14.464 ns
       2 -  54 cycles 13.703 ns -  32 cycles  8.110 ns - 33 cycles 8.482 ns
       3 -  53 cycles 13.272 ns -  25 cycles  6.362 ns - 27 cycles 6.947 ns
       4 -  51 cycles 12.994 ns -  24 cycles  6.087 ns - 24 cycles 6.078 ns
       8 -  50 cycles 12.576 ns -  21 cycles  5.354 ns - 22 cycles 5.513 ns
      16 -  49 cycles 12.368 ns -  20 cycles  5.054 ns - 20 cycles 5.042 ns
      30 -  49 cycles 12.273 ns -  18 cycles  4.748 ns - 19 cycles 4.758 ns
      32 -  49 cycles 12.401 ns -  19 cycles  4.821 ns - 19 cycles 4.810 ns
      34 -  98 cycles 24.519 ns -  24 cycles  6.154 ns - 24 cycles 6.157 ns
      48 -  83 cycles 20.833 ns -  21 cycles  5.446 ns - 21 cycles 5.429 ns
      64 -  75 cycles 18.891 ns -  20 cycles  5.247 ns - 20 cycles 5.238 ns
     128 -  93 cycles 23.271 ns -  27 cycles  6.856 ns - 27 cycles 6.823 ns
     158 - 102 cycles 25.581 ns -  30 cycles  7.714 ns - 30 cycles 7.695 ns
     250 - 107 cycles 26.917 ns -  38 cycles  9.514 ns - 38 cycles 9.506 ns
    
    SLUB when enabling MEMCG_KMEM runtime:
     - kmemcg fastpath: 71 cycles(tsc) 17.897 ns (step:0)
     1 - 85 cycles 21.484 ns -  78 cycles 19.569 ns - 75 cycles 18.938 ns
     2 - 81 cycles 20.363 ns -  45 cycles 11.258 ns - 44 cycles 11.076 ns
     3 - 78 cycles 19.709 ns -  33 cycles 8.354 ns - 32 cycles 8.044 ns
     4 - 77 cycles 19.430 ns -  28 cycles 7.216 ns - 28 cycles 7.003 ns
     8 - 101 cycles 25.288 ns -  23 cycles 5.849 ns - 23 cycles 5.787 ns
     16 - 76 cycles 19.148 ns -  20 cycles 5.162 ns - 20 cycles 5.081 ns
     30 - 76 cycles 19.067 ns -  19 cycles 4.868 ns - 19 cycles 4.821 ns
     32 - 76 cycles 19.052 ns -  19 cycles 4.857 ns - 19 cycles 4.815 ns
     34 - 121 cycles 30.291 ns -  25 cycles 6.333 ns - 25 cycles 6.268 ns
     48 - 108 cycles 27.111 ns -  21 cycles 5.498 ns - 21 cycles 5.458 ns
     64 - 100 cycles 25.164 ns -  20 cycles 5.242 ns - 20 cycles 5.229 ns
     128 - 155 cycles 38.976 ns -  27 cycles 6.886 ns - 27 cycles 6.892 ns
     158 - 132 cycles 33.034 ns -  30 cycles 7.711 ns - 30 cycles 7.728 ns
     250 - 130 cycles 32.612 ns -  38 cycles 9.560 ns - 38 cycles 9.549 ns
    
    Signed-off-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Vladimir Davydov <vdavydov@virtuozzo.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 3627d5c1bc47..9d9a5bdb9b00 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -323,6 +323,15 @@ void kmem_cache_free(struct kmem_cache *, void *);
 void kmem_cache_free_bulk(struct kmem_cache *, size_t, void **);
 int kmem_cache_alloc_bulk(struct kmem_cache *, gfp_t, size_t, void **);
 
+/*
+ * Caller must not use kfree_bulk() on memory not originally allocated
+ * by kmalloc(), because the SLOB allocator cannot handle this.
+ */
+static __always_inline void kfree_bulk(size_t size, void **p)
+{
+	kmem_cache_free_bulk(NULL, size, p);
+}
+
 #ifdef CONFIG_NUMA
 void *__kmalloc_node(size_t size, gfp_t flags, int node) __assume_kmalloc_alignment;
 void *kmem_cache_alloc_node(struct kmem_cache *, gfp_t flags, int node) __assume_slab_alignment;

commit 127424c86bb6cb87f0b563d9fdcfbbaf3c86ecec
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Wed Jan 20 15:02:32 2016 -0800

    mm: memcontrol: move kmem accounting code to CONFIG_MEMCG
    
    The cgroup2 memory controller will account important in-kernel memory
    consumers per default.  Move all necessary components to CONFIG_MEMCG.
    
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Vladimir Davydov <vdavydov@virtuozzo.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 3ffee7422012..3627d5c1bc47 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -86,7 +86,7 @@
 #else
 # define SLAB_FAILSLAB		0x00000000UL
 #endif
-#ifdef CONFIG_MEMCG_KMEM
+#if defined(CONFIG_MEMCG) && !defined(CONFIG_SLOB)
 # define SLAB_ACCOUNT		0x04000000UL	/* Account to memcg */
 #else
 # define SLAB_ACCOUNT		0x00000000UL

commit 230e9fc2860450fbb1f33bdcf9093d92d7d91f5b
Author: Vladimir Davydov <vdavydov@virtuozzo.com>
Date:   Thu Jan 14 15:18:15 2016 -0800

    slab: add SLAB_ACCOUNT flag
    
    Currently, if we want to account all objects of a particular kmem cache,
    we have to pass __GFP_ACCOUNT to each kmem_cache_alloc call, which is
    inconvenient.  This patch introduces SLAB_ACCOUNT flag which if passed
    to kmem_cache_create will force accounting for every allocation from
    this cache even if __GFP_ACCOUNT is not passed.
    
    This patch does not make any of the existing caches use this flag - it
    will be done later in the series.
    
    Note, a cache with SLAB_ACCOUNT cannot be merged with a cache w/o
    SLAB_ACCOUNT, because merged caches share the same kmem_cache struct and
    hence cannot have different sets of SLAB_* flags.  Thus using this flag
    will probably reduce the number of merged slabs even if kmem accounting
    is not used (only compiled in).
    
    Signed-off-by: Vladimir Davydov <vdavydov@virtuozzo.com>
    Suggested-by: Tejun Heo <tj@kernel.org>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Greg Thelen <gthelen@google.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 2037a861e367..3ffee7422012 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -86,6 +86,11 @@
 #else
 # define SLAB_FAILSLAB		0x00000000UL
 #endif
+#ifdef CONFIG_MEMCG_KMEM
+# define SLAB_ACCOUNT		0x04000000UL	/* Account to memcg */
+#else
+# define SLAB_ACCOUNT		0x00000000UL
+#endif
 
 /* The following flags affect the page allocator grouping pages by mobility */
 #define SLAB_RECLAIM_ACCOUNT	0x00020000UL		/* Objects are reclaimable */

commit 865762a8119e74b5f0e236d2d8eaaf8be9292a06
Author: Jesper Dangaard Brouer <brouer@redhat.com>
Date:   Fri Nov 20 15:57:58 2015 -0800

    slab/slub: adjust kmem_cache_alloc_bulk API
    
    Adjust kmem_cache_alloc_bulk API before we have any real users.
    
    Adjust API to return type 'int' instead of previously type 'bool'.  This
    is done to allow future extension of the bulk alloc API.
    
    A future extension could be to allow SLUB to stop at a page boundary, when
    specified by a flag, and then return the number of objects.
    
    The advantage of this approach, would make it easier to make bulk alloc
    run without local IRQs disabled.  With an approach of cmpxchg "stealing"
    the entire c->freelist or page->freelist.  To avoid overshooting we would
    stop processing at a slab-page boundary.  Else we always end up returning
    some objects at the cost of another cmpxchg.
    
    To keep compatible with future users of this API linking against an older
    kernel when using the new flag, we need to return the number of allocated
    objects with this API change.
    
    Signed-off-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Cc: Vladimir Davydov <vdavydov@virtuozzo.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 96940772bb92..2037a861e367 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -316,7 +316,7 @@ void kmem_cache_free(struct kmem_cache *, void *);
  * Note that interrupts must be enabled when calling these functions.
  */
 void kmem_cache_free_bulk(struct kmem_cache *, size_t, void **);
-bool kmem_cache_alloc_bulk(struct kmem_cache *, gfp_t, size_t, void **);
+int kmem_cache_alloc_bulk(struct kmem_cache *, gfp_t, size_t, void **);
 
 #ifdef CONFIG_NUMA
 void *__kmalloc_node(size_t size, gfp_t flags, int node) __assume_kmalloc_alignment;

commit 94a58c360a45c066ab5472cfd2bf2a4ba63aa532
Author: Rasmus Villemoes <linux@rasmusvillemoes.dk>
Date:   Fri Nov 20 15:56:48 2015 -0800

    slab.h: sprinkle __assume_aligned attributes
    
    The various allocators return aligned memory.  Telling the compiler that
    allows it to generate better code in many cases, for example when the
    return value is immediately passed to memset().
    
    Some code does become larger, but at least we win twice as much as we lose:
    
    $ scripts/bloat-o-meter /tmp/vmlinux vmlinux
    add/remove: 0/0 grow/shrink: 13/52 up/down: 995/-2140 (-1145)
    
    An example of the different (and smaller) code can be seen in mm_alloc(). Before:
    
    :       48 8d 78 08             lea    0x8(%rax),%rdi
    :       48 89 c1                mov    %rax,%rcx
    :       48 89 c2                mov    %rax,%rdx
    :       48 c7 00 00 00 00 00    movq   $0x0,(%rax)
    :       48 c7 80 48 03 00 00    movq   $0x0,0x348(%rax)
    :       00 00 00 00
    :       31 c0                   xor    %eax,%eax
    :       48 83 e7 f8             and    $0xfffffffffffffff8,%rdi
    :       48 29 f9                sub    %rdi,%rcx
    :       81 c1 50 03 00 00       add    $0x350,%ecx
    :       c1 e9 03                shr    $0x3,%ecx
    :       f3 48 ab                rep stos %rax,%es:(%rdi)
    
    After:
    
    :       48 89 c2                mov    %rax,%rdx
    :       b9 6a 00 00 00          mov    $0x6a,%ecx
    :       31 c0                   xor    %eax,%eax
    :       48 89 d7                mov    %rdx,%rdi
    :       f3 48 ab                rep stos %rax,%es:(%rdi)
    
    So gcc's strategy is to do two possibly (but not really, of course)
    unaligned stores to the first and last word, then do an aligned rep stos
    covering the middle part with a little overlap.  Maybe arches which do not
    allow unaligned stores gain even more.
    
    I don't know if gcc can actually make use of alignments greater than 8 for
    anything, so one could probably drop the __assume_xyz_alignment macros and
    just use __assume_aligned(8).
    
    The increases in code size are mostly caused by gcc deciding to
    opencode strlen() using the check-four-bytes-at-a-time trick when it
    knows the buffer is sufficiently aligned (one function grew by 200
    bytes). Now it turns out that many of these strlen() calls showing up
    were in fact redundant, and they're gone from -next. Applying the two
    patches to next-20151001 bloat-o-meter instead says
    
    add/remove: 0/0 grow/shrink: 6/52 up/down: 244/-2140 (-1896)
    
    Signed-off-by: Rasmus Villemoes <linux@rasmusvillemoes.dk>
    Acked-by: Christoph Lameter <cl@linux.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 7c82e3b307a3..96940772bb92 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -157,6 +157,24 @@ size_t ksize(const void *);
 #define ARCH_KMALLOC_MINALIGN __alignof__(unsigned long long)
 #endif
 
+/*
+ * Setting ARCH_SLAB_MINALIGN in arch headers allows a different alignment.
+ * Intended for arches that get misalignment faults even for 64 bit integer
+ * aligned buffers.
+ */
+#ifndef ARCH_SLAB_MINALIGN
+#define ARCH_SLAB_MINALIGN __alignof__(unsigned long long)
+#endif
+
+/*
+ * kmalloc and friends return ARCH_KMALLOC_MINALIGN aligned
+ * pointers. kmem_cache_alloc and friends return ARCH_SLAB_MINALIGN
+ * aligned pointers.
+ */
+#define __assume_kmalloc_alignment __assume_aligned(ARCH_KMALLOC_MINALIGN)
+#define __assume_slab_alignment __assume_aligned(ARCH_SLAB_MINALIGN)
+#define __assume_page_alignment __assume_aligned(PAGE_SIZE)
+
 /*
  * Kmalloc array related definitions
  */
@@ -286,8 +304,8 @@ static __always_inline int kmalloc_index(size_t size)
 }
 #endif /* !CONFIG_SLOB */
 
-void *__kmalloc(size_t size, gfp_t flags);
-void *kmem_cache_alloc(struct kmem_cache *, gfp_t flags);
+void *__kmalloc(size_t size, gfp_t flags) __assume_kmalloc_alignment;
+void *kmem_cache_alloc(struct kmem_cache *, gfp_t flags) __assume_slab_alignment;
 void kmem_cache_free(struct kmem_cache *, void *);
 
 /*
@@ -301,8 +319,8 @@ void kmem_cache_free_bulk(struct kmem_cache *, size_t, void **);
 bool kmem_cache_alloc_bulk(struct kmem_cache *, gfp_t, size_t, void **);
 
 #ifdef CONFIG_NUMA
-void *__kmalloc_node(size_t size, gfp_t flags, int node);
-void *kmem_cache_alloc_node(struct kmem_cache *, gfp_t flags, int node);
+void *__kmalloc_node(size_t size, gfp_t flags, int node) __assume_kmalloc_alignment;
+void *kmem_cache_alloc_node(struct kmem_cache *, gfp_t flags, int node) __assume_slab_alignment;
 #else
 static __always_inline void *__kmalloc_node(size_t size, gfp_t flags, int node)
 {
@@ -316,12 +334,12 @@ static __always_inline void *kmem_cache_alloc_node(struct kmem_cache *s, gfp_t f
 #endif
 
 #ifdef CONFIG_TRACING
-extern void *kmem_cache_alloc_trace(struct kmem_cache *, gfp_t, size_t);
+extern void *kmem_cache_alloc_trace(struct kmem_cache *, gfp_t, size_t) __assume_slab_alignment;
 
 #ifdef CONFIG_NUMA
 extern void *kmem_cache_alloc_node_trace(struct kmem_cache *s,
 					   gfp_t gfpflags,
-					   int node, size_t size);
+					   int node, size_t size) __assume_slab_alignment;
 #else
 static __always_inline void *
 kmem_cache_alloc_node_trace(struct kmem_cache *s,
@@ -354,10 +372,10 @@ kmem_cache_alloc_node_trace(struct kmem_cache *s,
 }
 #endif /* CONFIG_TRACING */
 
-extern void *kmalloc_order(size_t size, gfp_t flags, unsigned int order);
+extern void *kmalloc_order(size_t size, gfp_t flags, unsigned int order) __assume_page_alignment;
 
 #ifdef CONFIG_TRACING
-extern void *kmalloc_order_trace(size_t size, gfp_t flags, unsigned int order);
+extern void *kmalloc_order_trace(size_t size, gfp_t flags, unsigned int order) __assume_page_alignment;
 #else
 static __always_inline void *
 kmalloc_order_trace(size_t size, gfp_t flags, unsigned int order)
@@ -482,15 +500,6 @@ static __always_inline void *kmalloc_node(size_t size, gfp_t flags, int node)
 	return __kmalloc_node(size, flags, node);
 }
 
-/*
- * Setting ARCH_SLAB_MINALIGN in arch headers allows a different alignment.
- * Intended for arches that get misalignment faults even for 64 bit integer
- * aligned buffers.
- */
-#ifndef ARCH_SLAB_MINALIGN
-#define ARCH_SLAB_MINALIGN __alignof__(unsigned long long)
-#endif
-
 struct memcg_cache_array {
 	struct rcu_head rcu;
 	struct kmem_cache *entries[0];

commit fda901241fb89449244537db4fb27b06e491b74f
Author: Denis Kirjanov <kda@linux-powerpc.org>
Date:   Thu Nov 5 18:44:59 2015 -0800

    slab: convert slab_is_available() to boolean
    
    A good candidate to return a boolean result.
    
    Signed-off-by: Denis Kirjanov <kda@linux-powerpc.org>
    Cc: Christoph Lameter <cl@linux.com>
    Reviewed-by: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 7e37d448ed91..7c82e3b307a3 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -111,7 +111,7 @@ struct mem_cgroup;
  * struct kmem_cache related prototypes
  */
 void __init kmem_cache_init(void);
-int slab_is_available(void);
+bool slab_is_available(void);
 
 struct kmem_cache *kmem_cache_create(const char *, size_t, size_t,
 			unsigned long,

commit 484748f0b65a1950b2b93f444a2287e8dd2cedd6
Author: Christoph Lameter <cl@linux.com>
Date:   Fri Sep 4 15:45:34 2015 -0700

    slab: infrastructure for bulk object allocation and freeing
    
    Add the basic infrastructure for alloc/free operations on pointer arrays.
    It includes a generic function in the common slab code that is used in
    this infrastructure patch to create the unoptimized functionality for slab
    bulk operations.
    
    Allocators can then provide optimized allocation functions for situations
    in which large numbers of objects are needed.  These optimization may
    avoid taking locks repeatedly and bypass metadata creation if all objects
    in slab pages can be used to provide the objects required.
    
    Allocators can extend the skeletons provided and add their own code to the
    bulk alloc and free functions.  They can keep the generic allocation and
    freeing and just fall back to those if optimizations would not work (like
    for example when debugging is on).
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index a99f0e5243e1..7e37d448ed91 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -290,6 +290,16 @@ void *__kmalloc(size_t size, gfp_t flags);
 void *kmem_cache_alloc(struct kmem_cache *, gfp_t flags);
 void kmem_cache_free(struct kmem_cache *, void *);
 
+/*
+ * Bulk allocation and freeing operations. These are accellerated in an
+ * allocator specific way to avoid taking locks repeatedly or building
+ * metadata structures unnecessarily.
+ *
+ * Note that interrupts must be enabled when calling these functions.
+ */
+void kmem_cache_free_bulk(struct kmem_cache *, size_t, void **);
+bool kmem_cache_alloc_bulk(struct kmem_cache *, gfp_t, size_t, void **);
+
 #ifdef CONFIG_NUMA
 void *__kmalloc_node(size_t size, gfp_t flags, int node);
 void *kmem_cache_alloc_node(struct kmem_cache *, gfp_t flags, int node);

commit a9730fca9946f3697410479e0ef1bd759ba00a77
Author: Christoph Lameter <cl@linux.com>
Date:   Mon Jun 29 09:28:08 2015 -0500

    Fix kmalloc slab creation sequence
    
    This patch restores the slab creation sequence that was broken by commit
    4066c33d0308f8 and also reverts the portions that introduced the
    KMALLOC_LOOP_XXX macros. Those can never really work since the slab creation
    is much more complex than just going from a minimum to a maximum number.
    
    The latest upstream kernel boots cleanly on my machine with a 64 bit x86
    configuration under KVM using either SLAB or SLUB.
    
    Fixes: 4066c33d0308f8 ("support the slub_debug boot option")
    Reported-by: Theodore Ts'o <tytso@mit.edu>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 9de2fdc8b5e4..a99f0e5243e1 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -153,30 +153,8 @@ size_t ksize(const void *);
 #define ARCH_KMALLOC_MINALIGN ARCH_DMA_MINALIGN
 #define KMALLOC_MIN_SIZE ARCH_DMA_MINALIGN
 #define KMALLOC_SHIFT_LOW ilog2(ARCH_DMA_MINALIGN)
-/*
- * The KMALLOC_LOOP_LOW is the definition for the for loop index start number
- * to create the kmalloc_caches object in create_kmalloc_caches(). The first
- * and the second are 96 and 192. You can see that in the kmalloc_index(), if
- * the KMALLOC_MIN_SIZE <= 32, then return 1 (96). If KMALLOC_MIN_SIZE <= 64,
- * then return 2 (192). If the KMALLOC_MIN_SIZE is bigger than 64, we don't
- * need to initialize 96 and 192. Go directly to start the KMALLOC_SHIFT_LOW.
- */
-#if KMALLOC_MIN_SIZE <= 32
-#define KMALLOC_LOOP_LOW 1
-#elif KMALLOC_MIN_SIZE <= 64
-#define KMALLOC_LOOP_LOW 2
-#else
-#define KMALLOC_LOOP_LOW KMALLOC_SHIFT_LOW
-#endif
-
 #else
 #define ARCH_KMALLOC_MINALIGN __alignof__(unsigned long long)
-/*
- * The KMALLOC_MIN_SIZE of slub/slab/slob is 2^3/2^5/2^3. So, even slab is used.
- * The KMALLOC_MIN_SIZE <= 32. The kmalloc-96 and kmalloc-192 should also be
- * initialized.
- */
-#define KMALLOC_LOOP_LOW 1
 #endif
 
 /*

commit 1ed58b6051b67e5cfe9e465fb60bf7d5f55e0a64
Author: Rasmus Villemoes <linux@rasmusvillemoes.dk>
Date:   Wed Jun 24 16:55:59 2015 -0700

    linux/slab.h: fix three off-by-one typos in comment
    
    The first is a keyboard-off-by-one, the other two the ordinary mathy kind.
    
    Signed-off-by: Rasmus Villemoes <linux@rasmusvillemoes.dk>
    Acked-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 96f0ea506b5c..9de2fdc8b5e4 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -262,8 +262,8 @@ extern struct kmem_cache *kmalloc_dma_caches[KMALLOC_SHIFT_HIGH + 1];
  * belongs to.
  * 0 = zero alloc
  * 1 =  65 .. 96 bytes
- * 2 = 120 .. 192 bytes
- * n = 2^(n-1) .. 2^n -1
+ * 2 = 129 .. 192 bytes
+ * n = 2^(n-1)+1 .. 2^n
  */
 static __always_inline int kmalloc_index(size_t size)
 {

commit 4066c33d0308f87e9a3b0c7fafb9141c0bfbfa77
Author: Gavin Guo <gavin.guo@canonical.com>
Date:   Wed Jun 24 16:55:54 2015 -0700

    mm/slab_common: support the slub_debug boot option on specific object size
    
    The slub_debug=PU,kmalloc-xx cannot work because in the
    create_kmalloc_caches() the s->name is created after the
    create_kmalloc_cache() is called.  The name is NULL in the
    create_kmalloc_cache() so the kmem_cache_flags() would not set the
    slub_debug flags to the s->flags.  The fix here set up a kmalloc_names
    string array for the initialization purpose and delete the dynamic name
    creation of kmalloc_caches.
    
    [akpm@linux-foundation.org: s/kmalloc_names/kmalloc_info/, tweak comment text]
    Signed-off-by: Gavin Guo <gavin.guo@canonical.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index ffd24c830151..96f0ea506b5c 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -153,8 +153,30 @@ size_t ksize(const void *);
 #define ARCH_KMALLOC_MINALIGN ARCH_DMA_MINALIGN
 #define KMALLOC_MIN_SIZE ARCH_DMA_MINALIGN
 #define KMALLOC_SHIFT_LOW ilog2(ARCH_DMA_MINALIGN)
+/*
+ * The KMALLOC_LOOP_LOW is the definition for the for loop index start number
+ * to create the kmalloc_caches object in create_kmalloc_caches(). The first
+ * and the second are 96 and 192. You can see that in the kmalloc_index(), if
+ * the KMALLOC_MIN_SIZE <= 32, then return 1 (96). If KMALLOC_MIN_SIZE <= 64,
+ * then return 2 (192). If the KMALLOC_MIN_SIZE is bigger than 64, we don't
+ * need to initialize 96 and 192. Go directly to start the KMALLOC_SHIFT_LOW.
+ */
+#if KMALLOC_MIN_SIZE <= 32
+#define KMALLOC_LOOP_LOW 1
+#elif KMALLOC_MIN_SIZE <= 64
+#define KMALLOC_LOOP_LOW 2
+#else
+#define KMALLOC_LOOP_LOW KMALLOC_SHIFT_LOW
+#endif
+
 #else
 #define ARCH_KMALLOC_MINALIGN __alignof__(unsigned long long)
+/*
+ * The KMALLOC_MIN_SIZE of slub/slab/slob is 2^3/2^5/2^3. So, even slab is used.
+ * The KMALLOC_MIN_SIZE <= 32. The kmalloc-96 and kmalloc-192 should also be
+ * initialized.
+ */
+#define KMALLOC_LOOP_LOW 1
 #endif
 
 /*

commit 124dee09f0669b92cc0073b00984d53541ca0884
Author: David Rientjes <rientjes@google.com>
Date:   Tue Apr 14 15:44:28 2015 -0700

    mm, slab: correct config option in comment
    
    CONFIG_SLAB_DEBUG doesn't exist, CONFIG_DEBUG_SLAB does.
    
    Signed-off-by: David Rientjes <rientjes@google.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 76f1feeabd38..ffd24c830151 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -18,7 +18,7 @@
 
 /*
  * Flags to pass to kmem_cache_create().
- * The ones marked DEBUG are only valid if CONFIG_SLAB_DEBUG is set.
+ * The ones marked DEBUG are only valid if CONFIG_DEBUG_SLAB is set.
  */
 #define SLAB_DEBUG_FREE		0x00000100UL	/* DEBUG: Perform (expensive) checks on free */
 #define SLAB_RED_ZONE		0x00000400UL	/* DEBUG: Red zone objs in a cache */

commit 0316bec22ec95ea2faca6406437b0b5950553b7c
Author: Andrey Ryabinin <a.ryabinin@samsung.com>
Date:   Fri Feb 13 14:39:42 2015 -0800

    mm: slub: add kernel address sanitizer support for slub allocator
    
    With this patch kasan will be able to catch bugs in memory allocated by
    slub.  Initially all objects in newly allocated slab page, marked as
    redzone.  Later, when allocation of slub object happens, requested by
    caller number of bytes marked as accessible, and the rest of the object
    (including slub's metadata) marked as redzone (inaccessible).
    
    We also mark object as accessible if ksize was called for this object.
    There is some places in kernel where ksize function is called to inquire
    size of really allocated area.  Such callers could validly access whole
    allocated memory, so it should be marked as accessible.
    
    Code in slub.c and slab_common.c files could validly access to object's
    metadata, so instrumentation for this files are disabled.
    
    Signed-off-by: Andrey Ryabinin <a.ryabinin@samsung.com>
    Signed-off-by: Dmitry Chernenkov <dmitryc@google.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Konstantin Serebryany <kcc@google.com>
    Signed-off-by: Andrey Konovalov <adech.fo@gmail.com>
    Cc: Yuri Gribov <tetra2005@gmail.com>
    Cc: Konstantin Khlebnikov <koct9i@gmail.com>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index ed2ffaab59ea..76f1feeabd38 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -104,6 +104,7 @@
 				(unsigned long)ZERO_SIZE_PTR)
 
 #include <linux/kmemleak.h>
+#include <linux/kasan.h>
 
 struct mem_cgroup;
 /*
@@ -325,7 +326,10 @@ kmem_cache_alloc_node_trace(struct kmem_cache *s,
 static __always_inline void *kmem_cache_alloc_trace(struct kmem_cache *s,
 		gfp_t flags, size_t size)
 {
-	return kmem_cache_alloc(s, flags);
+	void *ret = kmem_cache_alloc(s, flags);
+
+	kasan_kmalloc(s, ret, size);
+	return ret;
 }
 
 static __always_inline void *
@@ -333,7 +337,10 @@ kmem_cache_alloc_node_trace(struct kmem_cache *s,
 			      gfp_t gfpflags,
 			      int node, size_t size)
 {
-	return kmem_cache_alloc_node(s, gfpflags, node);
+	void *ret = kmem_cache_alloc_node(s, gfpflags, node);
+
+	kasan_kmalloc(s, ret, size);
+	return ret;
 }
 #endif /* CONFIG_TRACING */
 

commit 2a4db7eb9391a544ff58f4fa11d35246e87c87af
Author: Vladimir Davydov <vdavydov@parallels.com>
Date:   Thu Feb 12 14:59:32 2015 -0800

    memcg: free memcg_caches slot on css offline
    
    We need to look up a kmem_cache in ->memcg_params.memcg_caches arrays only
    on allocations, so there is no need to have the array entries set until
    css free - we can clear them on css offline.  This will allow us to reuse
    array entries more efficiently and avoid costly array relocations.
    
    Signed-off-by: Vladimir Davydov <vdavydov@parallels.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Dave Chinner <david@fromorbit.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 26d99f41b410..ed2ffaab59ea 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -115,13 +115,12 @@ int slab_is_available(void);
 struct kmem_cache *kmem_cache_create(const char *, size_t, size_t,
 			unsigned long,
 			void (*)(void *));
-#ifdef CONFIG_MEMCG_KMEM
-void memcg_create_kmem_cache(struct mem_cgroup *, struct kmem_cache *);
-void memcg_destroy_kmem_caches(struct mem_cgroup *);
-#endif
 void kmem_cache_destroy(struct kmem_cache *);
 int kmem_cache_shrink(struct kmem_cache *);
-void kmem_cache_free(struct kmem_cache *, void *);
+
+void memcg_create_kmem_cache(struct mem_cgroup *, struct kmem_cache *);
+void memcg_deactivate_kmem_caches(struct mem_cgroup *);
+void memcg_destroy_kmem_caches(struct mem_cgroup *);
 
 /*
  * Please use this macro to create slab caches. Simply specify the
@@ -288,6 +287,7 @@ static __always_inline int kmalloc_index(size_t size)
 
 void *__kmalloc(size_t size, gfp_t flags);
 void *kmem_cache_alloc(struct kmem_cache *, gfp_t flags);
+void kmem_cache_free(struct kmem_cache *, void *);
 
 #ifdef CONFIG_NUMA
 void *__kmalloc_node(size_t size, gfp_t flags, int node);

commit 426589f571f7d6d5ab2ca33ece73164149279ca1
Author: Vladimir Davydov <vdavydov@parallels.com>
Date:   Thu Feb 12 14:59:23 2015 -0800

    slab: link memcg caches of the same kind into a list
    
    Sometimes, we need to iterate over all memcg copies of a particular root
    kmem cache.  Currently, we use memcg_cache_params->memcg_caches array for
    that, because it contains all existing memcg caches.
    
    However, it's a bad practice to keep all caches, including those that
    belong to offline cgroups, in this array, because it will be growing
    beyond any bounds then.  I'm going to wipe away dead caches from it to
    save space.  To still be able to perform iterations over all memcg caches
    of the same kind, let us link them into a list.
    
    Signed-off-by: Vladimir Davydov <vdavydov@parallels.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Dave Chinner <david@fromorbit.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 1e03c11bbfbd..26d99f41b410 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -491,9 +491,13 @@ struct memcg_cache_array {
  *
  * @memcg: pointer to the memcg this cache belongs to
  * @root_cache: pointer to the global, root cache, this cache was derived from
+ *
+ * Both root and child caches of the same kind are linked into a list chained
+ * through @list.
  */
 struct memcg_cache_params {
 	bool is_root_cache;
+	struct list_head list;
 	union {
 		struct memcg_cache_array __rcu *memcg_caches;
 		struct {

commit f7ce3190c4a35bf887adb7a1aa1ba899b679872d
Author: Vladimir Davydov <vdavydov@parallels.com>
Date:   Thu Feb 12 14:59:20 2015 -0800

    slab: embed memcg_cache_params to kmem_cache
    
    Currently, kmem_cache stores a pointer to struct memcg_cache_params
    instead of embedding it.  The rationale is to save memory when kmem
    accounting is disabled.  However, the memcg_cache_params has shrivelled
    drastically since it was first introduced:
    
    * Initially:
    
    struct memcg_cache_params {
            bool is_root_cache;
            union {
                    struct kmem_cache *memcg_caches[0];
                    struct {
                            struct mem_cgroup *memcg;
                            struct list_head list;
                            struct kmem_cache *root_cache;
                            bool dead;
                            atomic_t nr_pages;
                            struct work_struct destroy;
                    };
            };
    };
    
    * Now:
    
    struct memcg_cache_params {
            bool is_root_cache;
            union {
                    struct {
                            struct rcu_head rcu_head;
                            struct kmem_cache *memcg_caches[0];
                    };
                    struct {
                            struct mem_cgroup *memcg;
                            struct kmem_cache *root_cache;
                    };
            };
    };
    
    So the memory saving does not seem to be a clear win anymore.
    
    OTOH, keeping a pointer to memcg_cache_params struct instead of embedding
    it results in touching one more cache line on kmem alloc/free hot paths.
    Besides, it makes linking kmem caches in a list chained by a field of
    struct memcg_cache_params really painful due to a level of indirection,
    while I want to make them linked in the following patch.  That said, let
    us embed it.
    
    Signed-off-by: Vladimir Davydov <vdavydov@parallels.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: Dan Carpenter <dan.carpenter@oracle.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 2e3b448cfa2d..1e03c11bbfbd 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -473,14 +473,14 @@ static __always_inline void *kmalloc_node(size_t size, gfp_t flags, int node)
 #ifndef ARCH_SLAB_MINALIGN
 #define ARCH_SLAB_MINALIGN __alignof__(unsigned long long)
 #endif
+
+struct memcg_cache_array {
+	struct rcu_head rcu;
+	struct kmem_cache *entries[0];
+};
+
 /*
  * This is the main placeholder for memcg-related information in kmem caches.
- * struct kmem_cache will hold a pointer to it, so the memory cost while
- * disabled is 1 pointer. The runtime cost while enabled, gets bigger than it
- * would otherwise be if that would be bundled in kmem_cache: we'll need an
- * extra pointer chase. But the trade off clearly lays in favor of not
- * penalizing non-users.
- *
  * Both the root cache and the child caches will have it. For the root cache,
  * this will hold a dynamically allocated array large enough to hold
  * information about the currently limited memcgs in the system. To allow the
@@ -495,10 +495,7 @@ static __always_inline void *kmalloc_node(size_t size, gfp_t flags, int node)
 struct memcg_cache_params {
 	bool is_root_cache;
 	union {
-		struct {
-			struct rcu_head rcu_head;
-			struct kmem_cache *memcg_caches[0];
-		};
+		struct memcg_cache_array __rcu *memcg_caches;
 		struct {
 			struct mem_cgroup *memcg;
 			struct kmem_cache *root_cache;

commit d5b3cf7139b8770af4ed8bb36a1ab9d290ac39e9
Author: Vladimir Davydov <vdavydov@parallels.com>
Date:   Tue Feb 10 14:11:47 2015 -0800

    memcg: zap memcg_slab_caches and memcg_slab_mutex
    
    mem_cgroup->memcg_slab_caches is a list of kmem caches corresponding to
    the given cgroup.  Currently, it is only used on css free in order to
    destroy all caches corresponding to the memory cgroup being freed.  The
    list is protected by memcg_slab_mutex.  The mutex is also used to protect
    kmem_cache->memcg_params->memcg_caches arrays and synchronizes
    kmem_cache_destroy vs memcg_unregister_all_caches.
    
    However, we can perfectly get on without these two.  To destroy all caches
    corresponding to a memory cgroup, we can walk over the global list of kmem
    caches, slab_caches, and we can do all the synchronization stuff using the
    slab_mutex instead of the memcg_slab_mutex.  This patch therefore gets rid
    of the memcg_slab_caches and memcg_slab_mutex.
    
    Apart from this nice cleanup, it also:
    
     - assures that rcu_barrier() is called once at max when a root cache is
       destroyed or a memory cgroup is freed, no matter how many caches have
       SLAB_DESTROY_BY_RCU flag set;
    
     - fixes the race between kmem_cache_destroy and kmem_cache_create that
       exists, because memcg_cleanup_cache_params, which is called from
       kmem_cache_destroy after checking that kmem_cache->refcount=0,
       releases the slab_mutex, which gives kmem_cache_create a chance to
       make an alias to a cache doomed to be destroyed.
    
    Signed-off-by: Vladimir Davydov <vdavydov@parallels.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@suse.cz>
    Acked-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index eca9ed303a1b..2e3b448cfa2d 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -116,8 +116,8 @@ struct kmem_cache *kmem_cache_create(const char *, size_t, size_t,
 			unsigned long,
 			void (*)(void *));
 #ifdef CONFIG_MEMCG_KMEM
-struct kmem_cache *memcg_create_kmem_cache(struct mem_cgroup *,
-					   struct kmem_cache *);
+void memcg_create_kmem_cache(struct mem_cgroup *, struct kmem_cache *);
+void memcg_destroy_kmem_caches(struct mem_cgroup *);
 #endif
 void kmem_cache_destroy(struct kmem_cache *);
 int kmem_cache_shrink(struct kmem_cache *);
@@ -490,7 +490,6 @@ static __always_inline void *kmalloc_node(size_t size, gfp_t flags, int node)
  * Child caches will hold extra metadata needed for its operation. Fields are:
  *
  * @memcg: pointer to the memcg this cache belongs to
- * @list: list_head for the list of all caches in this memcg
  * @root_cache: pointer to the global, root cache, this cache was derived from
  */
 struct memcg_cache_params {
@@ -502,7 +501,6 @@ struct memcg_cache_params {
 		};
 		struct {
 			struct mem_cgroup *memcg;
-			struct list_head list;
 			struct kmem_cache *root_cache;
 		};
 	};

commit 3e0350a36414a73c5c2d1e354f8c0ab4ace1296d
Author: Vladimir Davydov <vdavydov@parallels.com>
Date:   Tue Feb 10 14:11:44 2015 -0800

    memcg: zap memcg_name argument of memcg_create_kmem_cache
    
    Instead of passing the name of the memory cgroup which the cache is
    created for in the memcg_name_argument, let's obtain it immediately in
    memcg_create_kmem_cache.
    
    Signed-off-by: Vladimir Davydov <vdavydov@parallels.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 9a139b637069..eca9ed303a1b 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -117,8 +117,7 @@ struct kmem_cache *kmem_cache_create(const char *, size_t, size_t,
 			void (*)(void *));
 #ifdef CONFIG_MEMCG_KMEM
 struct kmem_cache *memcg_create_kmem_cache(struct mem_cgroup *,
-					   struct kmem_cache *,
-					   const char *);
+					   struct kmem_cache *);
 #endif
 void kmem_cache_destroy(struct kmem_cache *);
 int kmem_cache_shrink(struct kmem_cache *);

commit 8135be5a8012f4c7e95218563855e16c09a8271b
Author: Vladimir Davydov <vdavydov@parallels.com>
Date:   Fri Dec 12 16:56:38 2014 -0800

    memcg: fix possible use-after-free in memcg_kmem_get_cache()
    
    Suppose task @t that belongs to a memory cgroup @memcg is going to
    allocate an object from a kmem cache @c.  The copy of @c corresponding to
    @memcg, @mc, is empty.  Then if kmem_cache_alloc races with the memory
    cgroup destruction we can access the memory cgroup's copy of the cache
    after it was destroyed:
    
    CPU0                            CPU1
    ----                            ----
    [ current=@t
      @mc->memcg_params->nr_pages=0 ]
    
    kmem_cache_alloc(@c):
      call memcg_kmem_get_cache(@c);
      proceed to allocation from @mc:
        alloc a page for @mc:
          ...
    
                                    move @t from @memcg
                                    destroy @memcg:
                                      mem_cgroup_css_offline(@memcg):
                                        memcg_unregister_all_caches(@memcg):
                                          kmem_cache_destroy(@mc)
    
        add page to @mc
    
    We could fix this issue by taking a reference to a per-memcg cache, but
    that would require adding a per-cpu reference counter to per-memcg caches,
    which would look cumbersome.
    
    Instead, let's take a reference to a memory cgroup, which already has a
    per-cpu reference counter, in the beginning of kmem_cache_alloc to be
    dropped in the end, and move per memcg caches destruction from css offline
    to css free.  As a side effect, per-memcg caches will be destroyed not one
    by one, but all at once when the last page accounted to the memory cgroup
    is freed.  This doesn't sound as a high price for code readability though.
    
    Note, this patch does add some overhead to the kmem_cache_alloc hot path,
    but it is pretty negligible - it's just a function call plus a per cpu
    counter decrement, which is comparable to what we already have in
    memcg_kmem_get_cache.  Besides, it's only relevant if there are memory
    cgroups with kmem accounting enabled.  I don't think we can find a way to
    handle this race w/o it, because alloc_page called from kmem_cache_alloc
    may sleep so we can't flush all pending kmallocs w/o reference counting.
    
    Signed-off-by: Vladimir Davydov <vdavydov@parallels.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 8a2457d42fc8..9a139b637069 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -493,7 +493,6 @@ static __always_inline void *kmalloc_node(size_t size, gfp_t flags, int node)
  * @memcg: pointer to the memcg this cache belongs to
  * @list: list_head for the list of all caches in this memcg
  * @root_cache: pointer to the global, root cache, this cache was derived from
- * @nr_pages: number of pages that belongs to this cache.
  */
 struct memcg_cache_params {
 	bool is_root_cache;
@@ -506,7 +505,6 @@ struct memcg_cache_params {
 			struct mem_cgroup *memcg;
 			struct list_head list;
 			struct kmem_cache *root_cache;
-			atomic_t nr_pages;
 		};
 	};
 };

commit b047501cd9f11d5e1d54ea0f90e2b10754021a0e
Author: Vladimir Davydov <vdavydov@parallels.com>
Date:   Wed Dec 10 15:44:19 2014 -0800

    memcg: use generic slab iterators for showing slabinfo
    
    Let's use generic slab_start/next/stop for showing memcg caches info.  In
    contrast to the current implementation, this will work even if all memcg
    caches' info doesn't fit into a seq buffer (a page), plus it simply looks
    neater.
    
    Actually, the main reason I do this isn't mere cleanup.  I'm going to zap
    the memcg_slab_caches list, because I find it useless provided we have the
    slab_caches list, and this patch is a step in this direction.
    
    It should be noted that before this patch an attempt to read
    memory.kmem.slabinfo of a cgroup that doesn't have kmem limit set resulted
    in -EIO, while after this patch it will silently show nothing except the
    header, but I don't think it will frustrate anyone.
    
    Signed-off-by: Vladimir Davydov <vdavydov@parallels.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index c265bec6a57d..8a2457d42fc8 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -513,10 +513,6 @@ struct memcg_cache_params {
 
 int memcg_update_all_caches(int num_memcgs);
 
-struct seq_file;
-int cache_show(struct kmem_cache *s, struct seq_file *m);
-void print_slabinfo_header(struct seq_file *m);
-
 /**
  * kmalloc_array - allocate memory for an array.
  * @n: number of elements.

commit 61f47105a2c9c60e950ca808b7560f776f9bfa31
Author: Joonsoo Kim <iamjoonsoo.kim@lge.com>
Date:   Thu Oct 9 15:26:02 2014 -0700

    mm/sl[ao]b: always track caller in kmalloc_(node_)track_caller()
    
    Now, we track caller if tracing or slab debugging is enabled.  If they are
    disabled, we could save one argument passing overhead by calling
    __kmalloc(_node)().  But, I think that it would be marginal.  Furthermore,
    default slab allocator, SLUB, doesn't use this technique so I think that
    it's okay to change this situation.
    
    After this change, we can turn on/off CONFIG_DEBUG_SLAB without full
    kernel build and remove some complicated '#if' defintion.  It looks more
    benefitial to me.
    
    Signed-off-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Zhang Yanfei <zhangyanfei@cn.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 9062e4ad1787..c265bec6a57d 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -549,37 +549,15 @@ static inline void *kcalloc(size_t n, size_t size, gfp_t flags)
  * allocator where we care about the real place the memory allocation
  * request comes from.
  */
-#if defined(CONFIG_DEBUG_SLAB) || defined(CONFIG_SLUB) || \
-	(defined(CONFIG_SLAB) && defined(CONFIG_TRACING)) || \
-	(defined(CONFIG_SLOB) && defined(CONFIG_TRACING))
 extern void *__kmalloc_track_caller(size_t, gfp_t, unsigned long);
 #define kmalloc_track_caller(size, flags) \
 	__kmalloc_track_caller(size, flags, _RET_IP_)
-#else
-#define kmalloc_track_caller(size, flags) \
-	__kmalloc(size, flags)
-#endif /* DEBUG_SLAB */
 
 #ifdef CONFIG_NUMA
-/*
- * kmalloc_node_track_caller is a special version of kmalloc_node that
- * records the calling function of the routine calling it for slab leak
- * tracking instead of just the calling function (confusing, eh?).
- * It's useful when the call to kmalloc_node comes from a widely-used
- * standard allocator where we care about the real place the memory
- * allocation request comes from.
- */
-#if defined(CONFIG_DEBUG_SLAB) || defined(CONFIG_SLUB) || \
-	(defined(CONFIG_SLAB) && defined(CONFIG_TRACING)) || \
-	(defined(CONFIG_SLOB) && defined(CONFIG_TRACING))
 extern void *__kmalloc_node_track_caller(size_t, gfp_t, int, unsigned long);
 #define kmalloc_node_track_caller(size, flags, node) \
 	__kmalloc_node_track_caller(size, flags, node, \
 			_RET_IP_)
-#else
-#define kmalloc_node_track_caller(size, flags, node) \
-	__kmalloc_node(size, flags, node)
-#endif
 
 #else /* CONFIG_NUMA */
 

commit 07f361b2bee38896df8be17d8c3f8af3f3610606
Author: Joonsoo Kim <iamjoonsoo.kim@lge.com>
Date:   Thu Oct 9 15:26:00 2014 -0700

    mm/slab_common: move kmem_cache definition to internal header
    
    We don't need to keep kmem_cache definition in include/linux/slab.h if we
    don't need to inline kmem_cache_size().  According to my code inspection,
    this function is only called at lc_create() in lib/lru_cache.c which may
    be called at initialization phase of something, so we don't need to inline
    it.  Therfore, move it to slab_common.c and move kmem_cache definition to
    internal header.
    
    After this change, we can change kmem_cache definition easily without full
    kernel build.  For instance, we can turn on/off CONFIG_SLUB_STATS without
    full kernel build.
    
    [akpm@linux-foundation.org: export kmem_cache_size() to modules]
    [rdunlap@infradead.org: add header files to fix kmemcheck.c build errors]
    Signed-off-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Zhang Yanfei <zhangyanfei@cn.fujitsu.com>
    Signed-off-by: Randy Dunlap <rdunlap@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 1d9abb7d22a0..9062e4ad1787 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -158,31 +158,6 @@ size_t ksize(const void *);
 #define ARCH_KMALLOC_MINALIGN __alignof__(unsigned long long)
 #endif
 
-#ifdef CONFIG_SLOB
-/*
- * Common fields provided in kmem_cache by all slab allocators
- * This struct is either used directly by the allocator (SLOB)
- * or the allocator must include definitions for all fields
- * provided in kmem_cache_common in their definition of kmem_cache.
- *
- * Once we can do anonymous structs (C11 standard) we could put a
- * anonymous struct definition in these allocators so that the
- * separate allocations in the kmem_cache structure of SLAB and
- * SLUB is no longer needed.
- */
-struct kmem_cache {
-	unsigned int object_size;/* The original size of the object */
-	unsigned int size;	/* The aligned/padded/added on size  */
-	unsigned int align;	/* Alignment as calculated */
-	unsigned long flags;	/* Active flags on the slab */
-	const char *name;	/* Slab name for sysfs */
-	int refcount;		/* Use counter */
-	void (*ctor)(void *);	/* Called on object slot creation */
-	struct list_head list;	/* List of all slab caches on the system */
-};
-
-#endif /* CONFIG_SLOB */
-
 /*
  * Kmalloc array related definitions
  */
@@ -363,14 +338,6 @@ kmem_cache_alloc_node_trace(struct kmem_cache *s,
 }
 #endif /* CONFIG_TRACING */
 
-#ifdef CONFIG_SLAB
-#include <linux/slab_def.h>
-#endif
-
-#ifdef CONFIG_SLUB
-#include <linux/slub_def.h>
-#endif
-
 extern void *kmalloc_order(size_t size, gfp_t flags, unsigned int order);
 
 #ifdef CONFIG_TRACING
@@ -650,14 +617,7 @@ static inline void *kzalloc_node(size_t size, gfp_t flags, int node)
 	return kmalloc_node(size, flags | __GFP_ZERO, node);
 }
 
-/*
- * Determine the size of a slab object
- */
-static inline unsigned int kmem_cache_size(struct kmem_cache *s)
-{
-	return s->object_size;
-}
-
+unsigned int kmem_cache_size(struct kmem_cache *s);
 void __init kmem_cache_init_late(void);
 
 #endif	/* _LINUX_SLAB_H */

commit 776ed0f0377914d1e65fed903c052e9eef3f4cc3
Author: Vladimir Davydov <vdavydov@parallels.com>
Date:   Wed Jun 4 16:10:02 2014 -0700

    memcg: cleanup kmem cache creation/destruction functions naming
    
    Current names are rather inconsistent. Let's try to improve them.
    
    Brief change log:
    
    ** old name **                          ** new name **
    
    kmem_cache_create_memcg                 memcg_create_kmem_cache
    memcg_kmem_create_cache                 memcg_regsiter_cache
    memcg_kmem_destroy_cache                memcg_unregister_cache
    
    kmem_cache_destroy_memcg_children       memcg_cleanup_cache_params
    mem_cgroup_destroy_all_caches           memcg_unregister_all_caches
    
    create_work                             memcg_register_cache_work
    memcg_create_cache_work_func            memcg_register_cache_func
    memcg_create_cache_enqueue              memcg_schedule_register_cache
    
    Signed-off-by: Vladimir Davydov <vdavydov@parallels.com>
    Acked-by: Michal Hocko <mhocko@suse.cz>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 86e5b26fbdab..1d9abb7d22a0 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -116,7 +116,7 @@ struct kmem_cache *kmem_cache_create(const char *, size_t, size_t,
 			unsigned long,
 			void (*)(void *));
 #ifdef CONFIG_MEMCG_KMEM
-struct kmem_cache *kmem_cache_create_memcg(struct mem_cgroup *,
+struct kmem_cache *memcg_create_kmem_cache(struct mem_cgroup *,
 					   struct kmem_cache *,
 					   const char *);
 #endif

commit 073ee1c6cd11cd190f4d0da84d9b4ba79d7b9e70
Author: Vladimir Davydov <vdavydov@parallels.com>
Date:   Wed Jun 4 16:08:23 2014 -0700

    memcg: get rid of memcg_create_cache_name
    
    Instead of calling back to memcontrol.c from kmem_cache_create_memcg in
    order to just create the name of a per memcg cache, let's allocate it in
    place.  We only need to pass the memcg name to kmem_cache_create_memcg for
    that - everything else can be done in slab_common.c.
    
    Signed-off-by: Vladimir Davydov <vdavydov@parallels.com>
    Acked-by: Michal Hocko <mhocko@suse.cz>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index ecbec9ccb80d..86e5b26fbdab 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -117,7 +117,8 @@ struct kmem_cache *kmem_cache_create(const char *, size_t, size_t,
 			void (*)(void *));
 #ifdef CONFIG_MEMCG_KMEM
 struct kmem_cache *kmem_cache_create_memcg(struct mem_cgroup *,
-					   struct kmem_cache *);
+					   struct kmem_cache *,
+					   const char *);
 #endif
 void kmem_cache_destroy(struct kmem_cache *);
 int kmem_cache_shrink(struct kmem_cache *);

commit bd67314586a3d5725e60f2f6587b4cb0f659bb67
Author: Vladimir Davydov <vdavydov@parallels.com>
Date:   Wed Jun 4 16:07:40 2014 -0700

    memcg, slab: simplify synchronization scheme
    
    At present, we have the following mutexes protecting data related to per
    memcg kmem caches:
    
     - slab_mutex.  This one is held during the whole kmem cache creation
       and destruction paths.  We also take it when updating per root cache
       memcg_caches arrays (see memcg_update_all_caches).  As a result, taking
       it guarantees there will be no changes to any kmem cache (including per
       memcg).  Why do we need something else then?  The point is it is
       private to slab implementation and has some internal dependencies with
       other mutexes (get_online_cpus).  So we just don't want to rely upon it
       and prefer to introduce additional mutexes instead.
    
     - activate_kmem_mutex.  Initially it was added to synchronize
       initializing kmem limit (memcg_activate_kmem).  However, since we can
       grow per root cache memcg_caches arrays only on kmem limit
       initialization (see memcg_update_all_caches), we also employ it to
       protect against memcg_caches arrays relocation (e.g.  see
       __kmem_cache_destroy_memcg_children).
    
     - We have a convention not to take slab_mutex in memcontrol.c, but we
       want to walk over per memcg memcg_slab_caches lists there (e.g.  for
       destroying all memcg caches on offline).  So we have per memcg
       slab_caches_mutex's protecting those lists.
    
    The mutexes are taken in the following order:
    
       activate_kmem_mutex -> slab_mutex -> memcg::slab_caches_mutex
    
    Such a syncrhonization scheme has a number of flaws, for instance:
    
     - We can't call kmem_cache_{destroy,shrink} while walking over a
       memcg::memcg_slab_caches list due to locking order.  As a result, in
       mem_cgroup_destroy_all_caches we schedule the
       memcg_cache_params::destroy work shrinking and destroying the cache.
    
     - We don't have a mutex to synchronize per memcg caches destruction
       between memcg offline (mem_cgroup_destroy_all_caches) and root cache
       destruction (__kmem_cache_destroy_memcg_children).  Currently we just
       don't bother about it.
    
    This patch simplifies it by substituting per memcg slab_caches_mutex's
    with the global memcg_slab_mutex.  It will be held whenever a new per
    memcg cache is created or destroyed, so it protects per root cache
    memcg_caches arrays and per memcg memcg_slab_caches lists.  The locking
    order is following:
    
       activate_kmem_mutex -> memcg_slab_mutex -> slab_mutex
    
    This allows us to call kmem_cache_{create,shrink,destroy} under the
    memcg_slab_mutex.  As a result, we don't need memcg_cache_params::destroy
    work any more - we can simply destroy caches while iterating over a per
    memcg slab caches list.
    
    Also using the global mutex simplifies synchronization between concurrent
    per memcg caches creation/destruction, e.g.  mem_cgroup_destroy_all_caches
    vs __kmem_cache_destroy_memcg_children.
    
    The downside of this is that we substitute per-memcg slab_caches_mutex's
    with a hummer-like global mutex, but since we already take either the
    slab_mutex or the cgroup_mutex along with a memcg::slab_caches_mutex, it
    shouldn't hurt concurrency a lot.
    
    Signed-off-by: Vladimir Davydov <vdavydov@parallels.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Glauber Costa <glommer@gmail.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 905541dd3778..ecbec9ccb80d 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -116,7 +116,8 @@ struct kmem_cache *kmem_cache_create(const char *, size_t, size_t,
 			unsigned long,
 			void (*)(void *));
 #ifdef CONFIG_MEMCG_KMEM
-void kmem_cache_create_memcg(struct mem_cgroup *, struct kmem_cache *);
+struct kmem_cache *kmem_cache_create_memcg(struct mem_cgroup *,
+					   struct kmem_cache *);
 #endif
 void kmem_cache_destroy(struct kmem_cache *);
 int kmem_cache_shrink(struct kmem_cache *);
@@ -525,8 +526,6 @@ static __always_inline void *kmalloc_node(size_t size, gfp_t flags, int node)
  * @list: list_head for the list of all caches in this memcg
  * @root_cache: pointer to the global, root cache, this cache was derived from
  * @nr_pages: number of pages that belongs to this cache.
- * @destroy: worker to be called whenever we are ready, or believe we may be
- *           ready, to destroy this cache.
  */
 struct memcg_cache_params {
 	bool is_root_cache;
@@ -540,7 +539,6 @@ struct memcg_cache_params {
 			struct list_head list;
 			struct kmem_cache *root_cache;
 			atomic_t nr_pages;
-			struct work_struct destroy;
 		};
 	};
 };

commit 1e32e77f95d60b121b6072e3e3a650a7f93068f9
Author: Vladimir Davydov <vdavydov@parallels.com>
Date:   Wed Jun 4 16:07:37 2014 -0700

    memcg, slab: do not schedule cache destruction when last page goes away
    
    This patchset is a part of preparations for kmemcg re-parenting.  It
    targets at simplifying kmemcg work-flows and synchronization.
    
    First, it removes async per memcg cache destruction (see patches 1, 2).
    Now caches are only destroyed on memcg offline.  That means the caches
    that are not empty on memcg offline will be leaked.  However, they are
    already leaked, because memcg_cache_params::nr_pages normally never drops
    to 0 so the destruction work is never scheduled except kmem_cache_shrink
    is called explicitly.  In the future I'm planning reaping such dead caches
    on vmpressure or periodically.
    
    Second, it substitutes per memcg slab_caches_mutex's with the global
    memcg_slab_mutex, which should be taken during the whole per memcg cache
    creation/destruction path before the slab_mutex (see patch 3).  This
    greatly simplifies synchronization among various per memcg cache
    creation/destruction paths.
    
    I'm still not quite sure about the end picture, in particular I don't know
    whether we should reap dead memcgs' kmem caches periodically or try to
    merge them with their parents (see https://lkml.org/lkml/2014/4/20/38 for
    more details), but whichever way we choose, this set looks like a
    reasonable change to me, because it greatly simplifies kmemcg work-flows
    and eases further development.
    
    This patch (of 3):
    
    After a memcg is offlined, we mark its kmem caches that cannot be deleted
    right now due to pending objects as dead by setting the
    memcg_cache_params::dead flag, so that memcg_release_pages will schedule
    cache destruction (memcg_cache_params::destroy) as soon as the last slab
    of the cache is freed (memcg_cache_params::nr_pages drops to zero).
    
    I guess the idea was to destroy the caches as soon as possible, i.e.
    immediately after freeing the last object.  However, it just doesn't work
    that way, because kmem caches always preserve some pages for the sake of
    performance, so that nr_pages never gets to zero unless the cache is
    shrunk explicitly using kmem_cache_shrink.  Of course, we could account
    the total number of objects on the cache or check if all the slabs
    allocated for the cache are empty on kmem_cache_free and schedule
    destruction if so, but that would be too costly.
    
    Thus we have a piece of code that works only when we explicitly call
    kmem_cache_shrink, but complicates the whole picture a lot.  Moreover,
    it's racy in fact.  For instance, kmem_cache_shrink may free the last slab
    and thus schedule cache destruction before it finishes checking that the
    cache is empty, which can lead to use-after-free.
    
    So I propose to remove this async cache destruction from
    memcg_release_pages, and check if the cache is empty explicitly after
    calling kmem_cache_shrink instead.  This will simplify things a lot w/o
    introducing any functional changes.
    
    And regarding dead memcg caches (i.e.  those that are left hanging around
    after memcg offline for they have objects), I suppose we should reap them
    either periodically or on vmpressure as Glauber suggested initially.  I'm
    going to implement this later.
    
    Signed-off-by: Vladimir Davydov <vdavydov@parallels.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Glauber Costa <glommer@gmail.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index a6aab2c0dfc5..905541dd3778 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -524,7 +524,6 @@ static __always_inline void *kmalloc_node(size_t size, gfp_t flags, int node)
  * @memcg: pointer to the memcg this cache belongs to
  * @list: list_head for the list of all caches in this memcg
  * @root_cache: pointer to the global, root cache, this cache was derived from
- * @dead: set to true after the memcg dies; the cache may still be around.
  * @nr_pages: number of pages that belongs to this cache.
  * @destroy: worker to be called whenever we are ready, or believe we may be
  *           ready, to destroy this cache.
@@ -540,7 +539,6 @@ struct memcg_cache_params {
 			struct mem_cgroup *memcg;
 			struct list_head list;
 			struct kmem_cache *root_cache;
-			bool dead;
 			atomic_t nr_pages;
 			struct work_struct destroy;
 		};

commit 52383431b37cdbec63944e953ffc2698a7ad9722
Author: Vladimir Davydov <vdavydov@parallels.com>
Date:   Wed Jun 4 16:06:39 2014 -0700

    mm: get rid of __GFP_KMEMCG
    
    Currently to allocate a page that should be charged to kmemcg (e.g.
    threadinfo), we pass __GFP_KMEMCG flag to the page allocator.  The page
    allocated is then to be freed by free_memcg_kmem_pages.  Apart from
    looking asymmetrical, this also requires intrusion to the general
    allocation path.  So let's introduce separate functions that will
    alloc/free pages charged to kmemcg.
    
    The new functions are called alloc_kmem_pages and free_kmem_pages.  They
    should be used when the caller actually would like to use kmalloc, but
    has to fall back to the page allocator for the allocation is large.
    They only differ from alloc_pages and free_pages in that besides
    allocating or freeing pages they also charge them to the kmem resource
    counter of the current memory cgroup.
    
    [sfr@canb.auug.org.au: export kmalloc_order() to modules]
    Signed-off-by: Vladimir Davydov <vdavydov@parallels.com>
    Acked-by: Greg Thelen <gthelen@google.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Michal Hocko <mhocko@suse.cz>
    Cc: Glauber Costa <glommer@gmail.com>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: Pekka Enberg <penberg@kernel.org>
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 307bfbe62387..a6aab2c0dfc5 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -369,16 +369,7 @@ kmem_cache_alloc_node_trace(struct kmem_cache *s,
 #include <linux/slub_def.h>
 #endif
 
-static __always_inline void *
-kmalloc_order(size_t size, gfp_t flags, unsigned int order)
-{
-	void *ret;
-
-	flags |= (__GFP_COMP | __GFP_KMEMCG);
-	ret = (void *) __get_free_pages(flags, order);
-	kmemleak_alloc(ret, size, 1, flags);
-	return ret;
-}
+extern void *kmalloc_order(size_t size, gfp_t flags, unsigned int order);
 
 #ifdef CONFIG_TRACING
 extern void *kmalloc_order_trace(size_t size, gfp_t flags, unsigned int order);

commit bf3a340738bc78008e496257c04fb5a7fc8281e6
Merge: 321d03c86732 34bf6ef94a83
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Apr 13 13:28:13 2014 -0700

    Merge branch 'slab/next' of git://git.kernel.org/pub/scm/linux/kernel/git/penberg/linux
    
    Pull slab changes from Pekka Enberg:
     "The biggest change is byte-sized freelist indices which reduces slab
      freelist memory usage:
    
        https://lkml.org/lkml/2013/12/2/64"
    
    * 'slab/next' of git://git.kernel.org/pub/scm/linux/kernel/git/penberg/linux:
      mm: slab/slub: use page->list consistently instead of page->lru
      mm/slab.c: cleanup outdated comments and unify variables naming
      slab: fix wrongly used macro
      slub: fix high order page allocation problem with __GFP_NOFAIL
      slab: Make allocations with GFP_ZERO slightly more efficient
      slab: make more slab management structure off the slab
      slab: introduce byte sized index for the freelist of a slab
      slab: restrict the number of objects in a slab
      slab: introduce helper functions to get/set free object
      slab: factor out calculate nr objects in cache_estimate

commit 794b1248be4e7e157f5535c3ee49168aa4643349
Author: Vladimir Davydov <vdavydov@parallels.com>
Date:   Mon Apr 7 15:39:26 2014 -0700

    memcg, slab: separate memcg vs root cache creation paths
    
    Memcg-awareness turned kmem_cache_create() into a dirty interweaving of
    memcg-only and except-for-memcg calls.  To clean this up, let's move the
    code responsible for memcg cache creation to a separate function.
    
    Signed-off-by: Vladimir Davydov <vdavydov@parallels.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Glauber Costa <glommer@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index b5b2df60299e..3dd389aa91c7 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -115,9 +115,9 @@ int slab_is_available(void);
 struct kmem_cache *kmem_cache_create(const char *, size_t, size_t,
 			unsigned long,
 			void (*)(void *));
-struct kmem_cache *
-kmem_cache_create_memcg(struct mem_cgroup *, const char *, size_t, size_t,
-			unsigned long, void (*)(void *), struct kmem_cache *);
+#ifdef CONFIG_MEMCG_KMEM
+void kmem_cache_create_memcg(struct mem_cgroup *, struct kmem_cache *);
+#endif
 void kmem_cache_destroy(struct kmem_cache *);
 int kmem_cache_shrink(struct kmem_cache *);
 void kmem_cache_free(struct kmem_cache *, void *);

commit 24f870d8f0adcd38639f2f66e37aa7591a3fc408
Author: Joonsoo Kim <iamjoonsoo.kim@lge.com>
Date:   Wed Mar 12 17:06:19 2014 +0900

    slab: fix wrongly used macro
    
    commit 'slab: restrict the number of objects in a slab' uses
    __builtin_constant_p() on #if macro. It is wrong usage of builtin
    function, but it is compiled on x86 without any problem, so I can't
    find it before 0 day build system find it.
    
    This commit fixes the situation by using KMALLOC_MIN_SIZE, instead of
    KMALLOC_SHIFT_LOW. KMALLOC_SHIFT_LOW is parsed to ilog2() on some
    architecture and this ilog2() uses __builtin_constant_p() and results in
    the problem. This problem would disappear by using KMALLOC_MIN_SIZE,
    since it is just constant.
    
    Tested-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index d015dec02bf3..5df89f777a54 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -201,17 +201,6 @@ struct kmem_cache {
 #ifndef KMALLOC_SHIFT_LOW
 #define KMALLOC_SHIFT_LOW	5
 #endif
-
-/*
- * This restriction comes from byte sized index implementation.
- * Page size is normally 2^12 bytes and, in this case, if we want to use
- * byte sized index which can represent 2^8 entries, the size of the object
- * should be equal or greater to 2^12 / 2^8 = 2^4 = 16.
- * If minimum size of kmalloc is less than 16, we use it as minimum object
- * size and give up to use byte sized index.
- */
-#define SLAB_OBJ_MIN_SIZE	(KMALLOC_SHIFT_LOW < 4 ? \
-				(1 << KMALLOC_SHIFT_LOW) : 16)
 #endif
 
 #ifdef CONFIG_SLUB
@@ -253,6 +242,17 @@ struct kmem_cache {
 #define KMALLOC_MIN_SIZE (1 << KMALLOC_SHIFT_LOW)
 #endif
 
+/*
+ * This restriction comes from byte sized index implementation.
+ * Page size is normally 2^12 bytes and, in this case, if we want to use
+ * byte sized index which can represent 2^8 entries, the size of the object
+ * should be equal or greater to 2^12 / 2^8 = 2^4 = 16.
+ * If minimum size of kmalloc is less than 16, we use it as minimum object
+ * size and give up to use byte sized index.
+ */
+#define SLAB_OBJ_MIN_SIZE      (KMALLOC_MIN_SIZE < 16 ? \
+                               (KMALLOC_MIN_SIZE) : 16)
+
 #ifndef CONFIG_SLOB
 extern struct kmem_cache *kmalloc_caches[KMALLOC_SHIFT_HIGH + 1];
 #ifdef CONFIG_ZONE_DMA

commit e97ca8e5b864f88b028c1759ba8536fa827d6d96
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Mon Mar 10 15:49:43 2014 -0700

    mm: fix GFP_THISNODE callers and clarify
    
    GFP_THISNODE is for callers that implement their own clever fallback to
    remote nodes.  It restricts the allocation to the specified node and
    does not invoke reclaim, assuming that the caller will take care of it
    when the fallback fails, e.g.  through a subsequent allocation request
    without GFP_THISNODE set.
    
    However, many current GFP_THISNODE users only want the node exclusive
    aspect of the flag, without actually implementing their own fallback or
    triggering reclaim if necessary.  This results in things like page
    migration failing prematurely even when there is easily reclaimable
    memory available, unless kswapd happens to be running already or a
    concurrent allocation attempt triggers the necessary reclaim.
    
    Convert all callsites that don't implement their own fallback strategy
    to __GFP_THISNODE.  This restricts the allocation a single node too, but
    at the same time allows the allocator to enter the slowpath, wake
    kswapd, and invoke direct reclaim if necessary, to make the allocation
    happen when memory is full.
    
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Jan Stancek <jstancek@redhat.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 9260abdd67df..b5b2df60299e 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -410,7 +410,7 @@ static __always_inline void *kmalloc_large(size_t size, gfp_t flags)
  *
  * %GFP_NOWAIT - Allocation will not sleep.
  *
- * %GFP_THISNODE - Allocate node-local memory only.
+ * %__GFP_THISNODE - Allocate node-local memory only.
  *
  * %GFP_DMA - Allocation suitable for DMA.
  *   Should only be used for kmalloc() caches. Otherwise, use a

commit f315e3fa1cf5b3317fc948708645fff889ce1e63
Author: Joonsoo Kim <iamjoonsoo.kim@lge.com>
Date:   Mon Dec 2 17:49:41 2013 +0900

    slab: restrict the number of objects in a slab
    
    To prepare to implement byte sized index for managing the freelist
    of a slab, we should restrict the number of objects in a slab to be less
    or equal to 256, since byte only represent 256 different values.
    Setting the size of object to value equal or more than newly introduced
    SLAB_OBJ_MIN_SIZE ensures that the number of objects in a slab is less or
    equal to 256 for a slab with 1 page.
    
    If page size is rather larger than 4096, above assumption would be wrong.
    In this case, we would fall back on 2 bytes sized index.
    
    If minimum size of kmalloc is less than 16, we use it as minimum object
    size and give up this optimization.
    
    Signed-off-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 9260abdd67df..d015dec02bf3 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -201,6 +201,17 @@ struct kmem_cache {
 #ifndef KMALLOC_SHIFT_LOW
 #define KMALLOC_SHIFT_LOW	5
 #endif
+
+/*
+ * This restriction comes from byte sized index implementation.
+ * Page size is normally 2^12 bytes and, in this case, if we want to use
+ * byte sized index which can represent 2^8 entries, the size of the object
+ * should be equal or greater to 2^12 / 2^8 = 2^4 = 16.
+ * If minimum size of kmalloc is less than 16, we use it as minimum object
+ * size and give up to use byte sized index.
+ */
+#define SLAB_OBJ_MIN_SIZE	(KMALLOC_SHIFT_LOW < 4 ? \
+				(1 << KMALLOC_SHIFT_LOW) : 16)
 #endif
 
 #ifdef CONFIG_SLUB

commit 7b383bef25e493cc4f047e44ebd6c3ccfd6d1cc5
Merge: 87af5e5c2256 cb8ee1a3d429
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Feb 2 11:30:08 2014 -0800

    Merge branch 'slab/next' of git://git.kernel.org/pub/scm/linux/kernel/git/penberg/linux
    
    Pull SLAB changes from Pekka Enberg:
     "Random bug fixes that have accumulated in my inbox over the past few
      months"
    
    * 'slab/next' of git://git.kernel.org/pub/scm/linux/kernel/git/penberg/linux:
      mm: Fix warning on make htmldocs caused by slab.c
      mm: slub: work around unneeded lockdep warning
      mm: sl[uo]b: fix misleading comments
      slub: Fix possible format string bug.
      slub: use lockdep_assert_held
      slub: Fix calculation of cpu slabs
      slab.h: remove duplicate kmalloc declaration and fix kernel-doc warnings

commit 433a91ff5fa19e3eb70b12f7056f234aebd09ac2
Author: Dave Hansen <dave.hansen@linux.intel.com>
Date:   Tue Jan 28 14:24:50 2014 -0800

    mm: sl[uo]b: fix misleading comments
    
    On x86, SLUB creates and handles <=8192-byte allocations internally.
    It passes larger ones up to the allocator.  Saying "up to order 2" is,
    at best, ambiguous.  Is that order-1?  Or (order-2 bytes)?  Make
    it more clear.
    
    SLOB commits a similar sin.  It *handles* page-size requests, but the
    comment says that it passes up "all page size and larger requests".
    
    SLOB also swaps around the order of the very-similarly-named
    KMALLOC_SHIFT_HIGH and KMALLOC_SHIFT_MAX #defines.  Make it
    consistent with the order of the other two allocators.
    
    Cc: Matt Mackall <mpm@selenic.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Christoph Lameter <cl@linux-foundation.org>
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 1e2f4fe12773..f76e956b4011 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -205,8 +205,8 @@ struct kmem_cache {
 
 #ifdef CONFIG_SLUB
 /*
- * SLUB allocates up to order 2 pages directly and otherwise
- * passes the request to the page allocator.
+ * SLUB directly allocates requests fitting in to an order-1 page
+ * (PAGE_SIZE*2).  Larger requests are passed to the page allocator.
  */
 #define KMALLOC_SHIFT_HIGH	(PAGE_SHIFT + 1)
 #define KMALLOC_SHIFT_MAX	(MAX_ORDER + PAGE_SHIFT)
@@ -217,12 +217,12 @@ struct kmem_cache {
 
 #ifdef CONFIG_SLOB
 /*
- * SLOB passes all page size and larger requests to the page allocator.
+ * SLOB passes all requests larger than one page to the page allocator.
  * No kmalloc array is necessary since objects of different sizes can
  * be allocated from the same page.
  */
-#define KMALLOC_SHIFT_MAX	30
 #define KMALLOC_SHIFT_HIGH	PAGE_SHIFT
+#define KMALLOC_SHIFT_MAX	30
 #ifndef KMALLOC_SHIFT_LOW
 #define KMALLOC_SHIFT_LOW	3
 #endif

commit f8570263ee16eb1d5038b8e20d7db3a68bbb2b49
Author: Vladimir Davydov <vdavydov@parallels.com>
Date:   Thu Jan 23 15:53:06 2014 -0800

    memcg, slab: RCU protect memcg_params for root caches
    
    We relocate root cache's memcg_params whenever we need to grow the
    memcg_caches array to accommodate all kmem-active memory cgroups.
    Currently on relocation we free the old version immediately, which can
    lead to use-after-free, because the memcg_caches array is accessed
    lock-free (see cache_from_memcg_idx()).  This patch fixes this by making
    memcg_params RCU-protected for root caches.
    
    Signed-off-by: Vladimir Davydov <vdavydov@parallels.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Glauber Costa <glommer@gmail.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Balbir Singh <bsingharora@gmail.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Christoph Lameter <cl@linux.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 1e2f4fe12773..a060142aa5f5 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -513,7 +513,9 @@ static __always_inline void *kmalloc_node(size_t size, gfp_t flags, int node)
  *
  * Both the root cache and the child caches will have it. For the root cache,
  * this will hold a dynamically allocated array large enough to hold
- * information about the currently limited memcgs in the system.
+ * information about the currently limited memcgs in the system. To allow the
+ * array to be accessed without taking any locks, on relocation we free the old
+ * version only after a grace period.
  *
  * Child caches will hold extra metadata needed for its operation. Fields are:
  *
@@ -528,7 +530,10 @@ static __always_inline void *kmalloc_node(size_t size, gfp_t flags, int node)
 struct memcg_cache_params {
 	bool is_root_cache;
 	union {
-		struct kmem_cache *memcg_caches[0];
+		struct {
+			struct rcu_head rcu_head;
+			struct kmem_cache *memcg_caches[0];
+		};
 		struct {
 			struct mem_cgroup *memcg;
 			struct list_head list;

commit e1168c2cc4a5a5e495a53e067a6be6b4f29abfe0
Author: Randy Dunlap <rdunlap@infradead.org>
Date:   Fri Nov 22 18:14:38 2013 -0800

    slab.h: remove duplicate kmalloc declaration and fix kernel-doc warnings
    
    Fix kernel-doc warning for duplicate definition of 'kmalloc':
    
    Documentation/DocBook/kernel-api.xml:9483: element refentry: validity error : ID API-kmalloc already defined
    <refentry id="API-kmalloc">
    
    Also combine the kernel-doc info from the 2 kmalloc definitions into one
    block and remove the "see kcalloc" comment since kmalloc now contains the
    @flags info.
    
    Signed-off-by: Randy Dunlap <rdunlap@infradead.org>
    Acked-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index c2bba248fa63..1e2f4fe12773 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -388,10 +388,55 @@ static __always_inline void *kmalloc_large(size_t size, gfp_t flags)
 /**
  * kmalloc - allocate memory
  * @size: how many bytes of memory are required.
- * @flags: the type of memory to allocate (see kcalloc).
+ * @flags: the type of memory to allocate.
  *
  * kmalloc is the normal method of allocating memory
  * for objects smaller than page size in the kernel.
+ *
+ * The @flags argument may be one of:
+ *
+ * %GFP_USER - Allocate memory on behalf of user.  May sleep.
+ *
+ * %GFP_KERNEL - Allocate normal kernel ram.  May sleep.
+ *
+ * %GFP_ATOMIC - Allocation will not sleep.  May use emergency pools.
+ *   For example, use this inside interrupt handlers.
+ *
+ * %GFP_HIGHUSER - Allocate pages from high memory.
+ *
+ * %GFP_NOIO - Do not do any I/O at all while trying to get memory.
+ *
+ * %GFP_NOFS - Do not make any fs calls while trying to get memory.
+ *
+ * %GFP_NOWAIT - Allocation will not sleep.
+ *
+ * %GFP_THISNODE - Allocate node-local memory only.
+ *
+ * %GFP_DMA - Allocation suitable for DMA.
+ *   Should only be used for kmalloc() caches. Otherwise, use a
+ *   slab created with SLAB_DMA.
+ *
+ * Also it is possible to set different flags by OR'ing
+ * in one or more of the following additional @flags:
+ *
+ * %__GFP_COLD - Request cache-cold pages instead of
+ *   trying to return cache-warm pages.
+ *
+ * %__GFP_HIGH - This allocation has high priority and may use emergency pools.
+ *
+ * %__GFP_NOFAIL - Indicate that this allocation is in no way allowed to fail
+ *   (think twice before using).
+ *
+ * %__GFP_NORETRY - If memory is not immediately available,
+ *   then give up at once.
+ *
+ * %__GFP_NOWARN - If allocation fails, don't issue any warnings.
+ *
+ * %__GFP_REPEAT - If allocation fails initially, try once more before failing.
+ *
+ * There are other flags available as well, but these are not intended
+ * for general use, and so are not documented here. For a full list of
+ * potential flags, always refer to linux/gfp.h.
  */
 static __always_inline void *kmalloc(size_t size, gfp_t flags)
 {
@@ -501,61 +546,6 @@ struct seq_file;
 int cache_show(struct kmem_cache *s, struct seq_file *m);
 void print_slabinfo_header(struct seq_file *m);
 
-/**
- * kmalloc - allocate memory
- * @size: how many bytes of memory are required.
- * @flags: the type of memory to allocate.
- *
- * The @flags argument may be one of:
- *
- * %GFP_USER - Allocate memory on behalf of user.  May sleep.
- *
- * %GFP_KERNEL - Allocate normal kernel ram.  May sleep.
- *
- * %GFP_ATOMIC - Allocation will not sleep.  May use emergency pools.
- *   For example, use this inside interrupt handlers.
- *
- * %GFP_HIGHUSER - Allocate pages from high memory.
- *
- * %GFP_NOIO - Do not do any I/O at all while trying to get memory.
- *
- * %GFP_NOFS - Do not make any fs calls while trying to get memory.
- *
- * %GFP_NOWAIT - Allocation will not sleep.
- *
- * %GFP_THISNODE - Allocate node-local memory only.
- *
- * %GFP_DMA - Allocation suitable for DMA.
- *   Should only be used for kmalloc() caches. Otherwise, use a
- *   slab created with SLAB_DMA.
- *
- * Also it is possible to set different flags by OR'ing
- * in one or more of the following additional @flags:
- *
- * %__GFP_COLD - Request cache-cold pages instead of
- *   trying to return cache-warm pages.
- *
- * %__GFP_HIGH - This allocation has high priority and may use emergency pools.
- *
- * %__GFP_NOFAIL - Indicate that this allocation is in no way allowed to fail
- *   (think twice before using).
- *
- * %__GFP_NORETRY - If memory is not immediately available,
- *   then give up at once.
- *
- * %__GFP_NOWARN - If allocation fails, don't issue any warnings.
- *
- * %__GFP_REPEAT - If allocation fails initially, try once more before failing.
- *
- * There are other flags available as well, but these are not intended
- * for general use, and so are not documented here. For a full list of
- * potential flags, always refer to linux/gfp.h.
- *
- * kmalloc is the normal method of allocating memory
- * in the kernel.
- */
-static __always_inline void *kmalloc(size_t size, gfp_t flags);
-
 /**
  * kmalloc_array - allocate memory for an array.
  * @n: number of elements.

commit 7e3528c3660a2e8602abc7858b0994d611f74bc3
Author: Randy Dunlap <rdunlap@infradead.org>
Date:   Fri Nov 22 18:14:38 2013 -0800

    slab.h: remove duplicate kmalloc declaration and fix kernel-doc warnings
    
    Fix kernel-doc warning for duplicate definition of 'kmalloc':
    
      Documentation/DocBook/kernel-api.xml:9483: element refentry: validity error : ID API-kmalloc already defined
      <refentry id="API-kmalloc">
    
    Also combine the kernel-doc info from the 2 kmalloc definitions into one
    block and remove the "see kcalloc" comment since kmalloc now contains the
    @flags info.
    
    Signed-off-by: Randy Dunlap <rdunlap@infradead.org>
    Acked-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index c2bba248fa63..1e2f4fe12773 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -388,10 +388,55 @@ static __always_inline void *kmalloc_large(size_t size, gfp_t flags)
 /**
  * kmalloc - allocate memory
  * @size: how many bytes of memory are required.
- * @flags: the type of memory to allocate (see kcalloc).
+ * @flags: the type of memory to allocate.
  *
  * kmalloc is the normal method of allocating memory
  * for objects smaller than page size in the kernel.
+ *
+ * The @flags argument may be one of:
+ *
+ * %GFP_USER - Allocate memory on behalf of user.  May sleep.
+ *
+ * %GFP_KERNEL - Allocate normal kernel ram.  May sleep.
+ *
+ * %GFP_ATOMIC - Allocation will not sleep.  May use emergency pools.
+ *   For example, use this inside interrupt handlers.
+ *
+ * %GFP_HIGHUSER - Allocate pages from high memory.
+ *
+ * %GFP_NOIO - Do not do any I/O at all while trying to get memory.
+ *
+ * %GFP_NOFS - Do not make any fs calls while trying to get memory.
+ *
+ * %GFP_NOWAIT - Allocation will not sleep.
+ *
+ * %GFP_THISNODE - Allocate node-local memory only.
+ *
+ * %GFP_DMA - Allocation suitable for DMA.
+ *   Should only be used for kmalloc() caches. Otherwise, use a
+ *   slab created with SLAB_DMA.
+ *
+ * Also it is possible to set different flags by OR'ing
+ * in one or more of the following additional @flags:
+ *
+ * %__GFP_COLD - Request cache-cold pages instead of
+ *   trying to return cache-warm pages.
+ *
+ * %__GFP_HIGH - This allocation has high priority and may use emergency pools.
+ *
+ * %__GFP_NOFAIL - Indicate that this allocation is in no way allowed to fail
+ *   (think twice before using).
+ *
+ * %__GFP_NORETRY - If memory is not immediately available,
+ *   then give up at once.
+ *
+ * %__GFP_NOWARN - If allocation fails, don't issue any warnings.
+ *
+ * %__GFP_REPEAT - If allocation fails initially, try once more before failing.
+ *
+ * There are other flags available as well, but these are not intended
+ * for general use, and so are not documented here. For a full list of
+ * potential flags, always refer to linux/gfp.h.
  */
 static __always_inline void *kmalloc(size_t size, gfp_t flags)
 {
@@ -501,61 +546,6 @@ struct seq_file;
 int cache_show(struct kmem_cache *s, struct seq_file *m);
 void print_slabinfo_header(struct seq_file *m);
 
-/**
- * kmalloc - allocate memory
- * @size: how many bytes of memory are required.
- * @flags: the type of memory to allocate.
- *
- * The @flags argument may be one of:
- *
- * %GFP_USER - Allocate memory on behalf of user.  May sleep.
- *
- * %GFP_KERNEL - Allocate normal kernel ram.  May sleep.
- *
- * %GFP_ATOMIC - Allocation will not sleep.  May use emergency pools.
- *   For example, use this inside interrupt handlers.
- *
- * %GFP_HIGHUSER - Allocate pages from high memory.
- *
- * %GFP_NOIO - Do not do any I/O at all while trying to get memory.
- *
- * %GFP_NOFS - Do not make any fs calls while trying to get memory.
- *
- * %GFP_NOWAIT - Allocation will not sleep.
- *
- * %GFP_THISNODE - Allocate node-local memory only.
- *
- * %GFP_DMA - Allocation suitable for DMA.
- *   Should only be used for kmalloc() caches. Otherwise, use a
- *   slab created with SLAB_DMA.
- *
- * Also it is possible to set different flags by OR'ing
- * in one or more of the following additional @flags:
- *
- * %__GFP_COLD - Request cache-cold pages instead of
- *   trying to return cache-warm pages.
- *
- * %__GFP_HIGH - This allocation has high priority and may use emergency pools.
- *
- * %__GFP_NOFAIL - Indicate that this allocation is in no way allowed to fail
- *   (think twice before using).
- *
- * %__GFP_NORETRY - If memory is not immediately available,
- *   then give up at once.
- *
- * %__GFP_NOWARN - If allocation fails, don't issue any warnings.
- *
- * %__GFP_REPEAT - If allocation fails initially, try once more before failing.
- *
- * There are other flags available as well, but these are not intended
- * for general use, and so are not documented here. For a full list of
- * potential flags, always refer to linux/gfp.h.
- *
- * kmalloc is the normal method of allocating memory
- * in the kernel.
- */
-static __always_inline void *kmalloc(size_t size, gfp_t flags);
-
 /**
  * kmalloc_array - allocate memory for an array.
  * @n: number of elements.

commit 24f971abbda045c24d5d6f2438a7785567d2fde9
Merge: 3bab0bf045e1 721ae22ae1a5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Nov 22 08:10:34 2013 -0800

    Merge branch 'slab/next' of git://git.kernel.org/pub/scm/linux/kernel/git/penberg/linux
    
    Pull SLAB changes from Pekka Enberg:
     "The patches from Joonsoo Kim switch mm/slab.c to use 'struct page' for
      slab internals similar to mm/slub.c.  This reduces memory usage and
      improves performance:
    
        https://lkml.org/lkml/2013/10/16/155
    
      Rest of the changes are bug fixes from various people"
    
    * 'slab/next' of git://git.kernel.org/pub/scm/linux/kernel/git/penberg/linux: (21 commits)
      mm, slub: fix the typo in mm/slub.c
      mm, slub: fix the typo in include/linux/slub_def.h
      slub: Handle NULL parameter in kmem_cache_flags
      slab: replace non-existing 'struct freelist *' with 'void *'
      slab: fix to calm down kmemleak warning
      slub: proper kmemleak tracking if CONFIG_SLUB_DEBUG disabled
      slab: rename slab_bufctl to slab_freelist
      slab: remove useless statement for checking pfmemalloc
      slab: use struct page for slab management
      slab: replace free and inuse in struct slab with newly introduced active
      slab: remove SLAB_LIMIT
      slab: remove kmem_bufctl_t
      slab: change the management method of free objects of the slab
      slab: use __GFP_COMP flag for allocating slab pages
      slab: use well-defined macro, virt_to_slab()
      slab: overloading the RCU head over the LRU for RCU free
      slab: remove cachep in struct slab_rcu
      slab: remove nodeid in struct slab
      slab: remove colouroff in struct slab
      slab: change return type of kmem_getpages() to struct page
      ...

commit 68126702b419fd26ef4946e314bb3a1f57d3a53f
Author: Joonsoo Kim <iamjoonsoo.kim@lge.com>
Date:   Thu Oct 24 10:07:42 2013 +0900

    slab: overloading the RCU head over the LRU for RCU free
    
    With build-time size checking, we can overload the RCU head over the LRU
    of struct page to free pages of a slab in rcu context. This really help to
    implement to overload the struct slab over the struct page and this
    eventually reduce memory usage and cache footprint of the SLAB.
    
    Acked-by: Andi Kleen <ak@linux.intel.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Pekka Enberg <penberg@iki.fi>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 6c5cc0ea8713..caaad51fee1f 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -51,7 +51,14 @@
  *  }
  *  rcu_read_unlock();
  *
- * See also the comment on struct slab_rcu in mm/slab.c.
+ * This is useful if we need to approach a kernel structure obliquely,
+ * from its address obtained without the usual locking. We can lock
+ * the structure to stabilize it and check it's still at the given address,
+ * only if we can be sure that the memory has not been meanwhile reused
+ * for some other kind of object (which our subsystem's lock might corrupt).
+ *
+ * rcu_read_lock before reading the address, then rcu_read_unlock after
+ * taking the spinlock within the structure expected at that address.
  */
 #define SLAB_DESTROY_BY_RCU	0x00080000UL	/* Defer freeing slabs to RCU */
 #define SLAB_MEM_SPREAD		0x00100000UL	/* Spread some memory over cpuset */

commit 23774a2f6fee0848503bfb8004eeeb5adef94f5c
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Sep 4 19:58:08 2013 +0000

    slab: Use correct GFP_DMA constant
    
    On Thu, 5 Sep 2013, kbuild test robot wrote:
    > >> include/linux/slab.h:433:53: sparse: restricted gfp_t degrades to integer
    >    429        static __always_inline void *kmalloc_node(size_t size, gfp_t flags, int node)
    >    430        {
    >    431        #ifndef CONFIG_SLOB
    >    432                if (__builtin_constant_p(size) &&
    >  > 433                        size <= KMALLOC_MAX_CACHE_SIZE && !(flags & SLAB_CACHE_DMA)) {
    >    434                        int i = kmalloc_index(size);
    >    435
    
    flags is of type gfp_t and not a slab internal flag. Therefore
    use GFP_DMA.
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 386af639dcaa..74f105847d13 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -430,7 +430,7 @@ static __always_inline void *kmalloc_node(size_t size, gfp_t flags, int node)
 {
 #ifndef CONFIG_SLOB
 	if (__builtin_constant_p(size) &&
-		size <= KMALLOC_MAX_CACHE_SIZE && !(flags & SLAB_CACHE_DMA)) {
+		size <= KMALLOC_MAX_CACHE_SIZE && !(flags & GFP_DMA)) {
 		int i = kmalloc_index(size);
 
 		if (!i)

commit f1b6eb6e6be149b40ebb013f5bfe2ac86b6f1c1b
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Sep 4 16:35:34 2013 +0000

    mm/sl[aou]b: Move kmallocXXX functions to common code
    
    The kmalloc* functions of all slab allcoators are similar now so
    lets move them into slab.h. This requires some function naming changes
    in slob.
    
    As a results of this patch there is a common set of functions for
    all allocators. Also means that kmalloc_large() is now available
    in general to perform large order allocations that go directly
    via the page allocator. kmalloc_large() can be substituted if
    kmalloc() throws warnings because of too large allocations.
    
    kmalloc_large() has exactly the same semantics as kmalloc but
    can only used for allocations > PAGE_SIZE.
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 6c5cc0ea8713..386af639dcaa 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -4,6 +4,8 @@
  * (C) SGI 2006, Christoph Lameter
  * 	Cleaned up and restructured to ease the addition of alternative
  * 	implementations of SLAB allocators.
+ * (C) Linux Foundation 2008-2013
+ *      Unified interface for all slab allocators
  */
 
 #ifndef _LINUX_SLAB_H
@@ -94,6 +96,7 @@
 #define ZERO_OR_NULL_PTR(x) ((unsigned long)(x) <= \
 				(unsigned long)ZERO_SIZE_PTR)
 
+#include <linux/kmemleak.h>
 
 struct mem_cgroup;
 /*
@@ -289,6 +292,57 @@ static __always_inline int kmalloc_index(size_t size)
 }
 #endif /* !CONFIG_SLOB */
 
+void *__kmalloc(size_t size, gfp_t flags);
+void *kmem_cache_alloc(struct kmem_cache *, gfp_t flags);
+
+#ifdef CONFIG_NUMA
+void *__kmalloc_node(size_t size, gfp_t flags, int node);
+void *kmem_cache_alloc_node(struct kmem_cache *, gfp_t flags, int node);
+#else
+static __always_inline void *__kmalloc_node(size_t size, gfp_t flags, int node)
+{
+	return __kmalloc(size, flags);
+}
+
+static __always_inline void *kmem_cache_alloc_node(struct kmem_cache *s, gfp_t flags, int node)
+{
+	return kmem_cache_alloc(s, flags);
+}
+#endif
+
+#ifdef CONFIG_TRACING
+extern void *kmem_cache_alloc_trace(struct kmem_cache *, gfp_t, size_t);
+
+#ifdef CONFIG_NUMA
+extern void *kmem_cache_alloc_node_trace(struct kmem_cache *s,
+					   gfp_t gfpflags,
+					   int node, size_t size);
+#else
+static __always_inline void *
+kmem_cache_alloc_node_trace(struct kmem_cache *s,
+			      gfp_t gfpflags,
+			      int node, size_t size)
+{
+	return kmem_cache_alloc_trace(s, gfpflags, size);
+}
+#endif /* CONFIG_NUMA */
+
+#else /* CONFIG_TRACING */
+static __always_inline void *kmem_cache_alloc_trace(struct kmem_cache *s,
+		gfp_t flags, size_t size)
+{
+	return kmem_cache_alloc(s, flags);
+}
+
+static __always_inline void *
+kmem_cache_alloc_node_trace(struct kmem_cache *s,
+			      gfp_t gfpflags,
+			      int node, size_t size)
+{
+	return kmem_cache_alloc_node(s, gfpflags, node);
+}
+#endif /* CONFIG_TRACING */
+
 #ifdef CONFIG_SLAB
 #include <linux/slab_def.h>
 #endif
@@ -297,9 +351,60 @@ static __always_inline int kmalloc_index(size_t size)
 #include <linux/slub_def.h>
 #endif
 
-#ifdef CONFIG_SLOB
-#include <linux/slob_def.h>
+static __always_inline void *
+kmalloc_order(size_t size, gfp_t flags, unsigned int order)
+{
+	void *ret;
+
+	flags |= (__GFP_COMP | __GFP_KMEMCG);
+	ret = (void *) __get_free_pages(flags, order);
+	kmemleak_alloc(ret, size, 1, flags);
+	return ret;
+}
+
+#ifdef CONFIG_TRACING
+extern void *kmalloc_order_trace(size_t size, gfp_t flags, unsigned int order);
+#else
+static __always_inline void *
+kmalloc_order_trace(size_t size, gfp_t flags, unsigned int order)
+{
+	return kmalloc_order(size, flags, order);
+}
+#endif
+
+static __always_inline void *kmalloc_large(size_t size, gfp_t flags)
+{
+	unsigned int order = get_order(size);
+	return kmalloc_order_trace(size, flags, order);
+}
+
+/**
+ * kmalloc - allocate memory
+ * @size: how many bytes of memory are required.
+ * @flags: the type of memory to allocate (see kcalloc).
+ *
+ * kmalloc is the normal method of allocating memory
+ * for objects smaller than page size in the kernel.
+ */
+static __always_inline void *kmalloc(size_t size, gfp_t flags)
+{
+	if (__builtin_constant_p(size)) {
+		if (size > KMALLOC_MAX_CACHE_SIZE)
+			return kmalloc_large(size, flags);
+#ifndef CONFIG_SLOB
+		if (!(flags & GFP_DMA)) {
+			int index = kmalloc_index(size);
+
+			if (!index)
+				return ZERO_SIZE_PTR;
+
+			return kmem_cache_alloc_trace(kmalloc_caches[index],
+					flags, size);
+		}
 #endif
+	}
+	return __kmalloc(size, flags);
+}
 
 /*
  * Determine size used for the nth kmalloc cache.
@@ -321,6 +426,23 @@ static __always_inline int kmalloc_size(int n)
 	return 0;
 }
 
+static __always_inline void *kmalloc_node(size_t size, gfp_t flags, int node)
+{
+#ifndef CONFIG_SLOB
+	if (__builtin_constant_p(size) &&
+		size <= KMALLOC_MAX_CACHE_SIZE && !(flags & SLAB_CACHE_DMA)) {
+		int i = kmalloc_index(size);
+
+		if (!i)
+			return ZERO_SIZE_PTR;
+
+		return kmem_cache_alloc_node_trace(kmalloc_caches[i],
+						flags, node, size);
+	}
+#endif
+	return __kmalloc_node(size, flags, node);
+}
+
 /*
  * Setting ARCH_SLAB_MINALIGN in arch headers allows a different alignment.
  * Intended for arches that get misalignment faults even for 64 bit integer
@@ -451,36 +573,6 @@ static inline void *kcalloc(size_t n, size_t size, gfp_t flags)
 	return kmalloc_array(n, size, flags | __GFP_ZERO);
 }
 
-#if !defined(CONFIG_NUMA) && !defined(CONFIG_SLOB)
-/**
- * kmalloc_node - allocate memory from a specific node
- * @size: how many bytes of memory are required.
- * @flags: the type of memory to allocate (see kmalloc).
- * @node: node to allocate from.
- *
- * kmalloc() for non-local nodes, used to allocate from a specific node
- * if available. Equivalent to kmalloc() in the non-NUMA single-node
- * case.
- */
-static inline void *kmalloc_node(size_t size, gfp_t flags, int node)
-{
-	return kmalloc(size, flags);
-}
-
-static inline void *__kmalloc_node(size_t size, gfp_t flags, int node)
-{
-	return __kmalloc(size, flags);
-}
-
-void *kmem_cache_alloc(struct kmem_cache *, gfp_t);
-
-static inline void *kmem_cache_alloc_node(struct kmem_cache *cachep,
-					gfp_t flags, int node)
-{
-	return kmem_cache_alloc(cachep, flags);
-}
-#endif /* !CONFIG_NUMA && !CONFIG_SLOB */
-
 /*
  * kmalloc_track_caller is a special version of kmalloc that records the
  * calling function of the routine calling it for slab leak tracking instead

commit e7efa615ccf78394338144ff0187be331240748a
Author: Michael Opdenacker <michael.opdenacker@free-electrons.com>
Date:   Tue Jun 25 18:16:55 2013 +0200

    slab: add kmalloc() to kernel API documentation
    
    At the moment, kmalloc() isn't even listed in the kernel API
    documentation (DocBook/kernel-api.html after running "make htmldocs").
    
    Another issue is that the documentation for kmalloc_node()
    refers to kcalloc()'s documentation to describe its 'flags' parameter,
    while kcalloc() refered to kmalloc()'s documentation, which doesn't exist!
    
    This patch is a proposed fix for this. It also removes the documentation
    for kmalloc() in include/linux/slob_def.h which isn't included to
    generate the documentation anyway. This way, kmalloc() is described
    in only one place.
    
    Acked-by: Christoph Lameter <cl@linux.com>
    Acked-by: Randy Dunlap <rdunlap@infradead.org>
    Signed-off-by: Michael Opdenacker <michael.opdenacker@free-electrons.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 9690c14eb7fb..6c5cc0ea8713 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -373,9 +373,8 @@ int cache_show(struct kmem_cache *s, struct seq_file *m);
 void print_slabinfo_header(struct seq_file *m);
 
 /**
- * kmalloc_array - allocate memory for an array.
- * @n: number of elements.
- * @size: element size.
+ * kmalloc - allocate memory
+ * @size: how many bytes of memory are required.
  * @flags: the type of memory to allocate.
  *
  * The @flags argument may be one of:
@@ -422,6 +421,17 @@ void print_slabinfo_header(struct seq_file *m);
  * There are other flags available as well, but these are not intended
  * for general use, and so are not documented here. For a full list of
  * potential flags, always refer to linux/gfp.h.
+ *
+ * kmalloc is the normal method of allocating memory
+ * in the kernel.
+ */
+static __always_inline void *kmalloc(size_t size, gfp_t flags);
+
+/**
+ * kmalloc_array - allocate memory for an array.
+ * @n: number of elements.
+ * @size: element size.
+ * @flags: the type of memory to allocate (see kmalloc).
  */
 static inline void *kmalloc_array(size_t n, size_t size, gfp_t flags)
 {
@@ -445,7 +455,7 @@ static inline void *kcalloc(size_t n, size_t size, gfp_t flags)
 /**
  * kmalloc_node - allocate memory from a specific node
  * @size: how many bytes of memory are required.
- * @flags: the type of memory to allocate (see kcalloc).
+ * @flags: the type of memory to allocate (see kmalloc).
  * @node: node to allocate from.
  *
  * kmalloc() for non-local nodes, used to allocate from a specific node

commit 069e2b351de67e7a837b15b3d26c65c19b790cc3
Author: Christoph Lameter <cl@linux.com>
Date:   Fri Jun 14 19:55:13 2013 +0000

    slob: Rework #ifdeffery in slab.h
    
    Make the SLOB specific stuff harmonize more with the way the other allocators
    do it. Create the typical kmalloc constants for that purpose. SLOB does not
    support it but the constants help us avoid #ifdefs.
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 0c621752caa6..9690c14eb7fb 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -169,11 +169,7 @@ struct kmem_cache {
 	struct list_head list;	/* List of all slab caches on the system */
 };
 
-#define KMALLOC_MAX_SIZE (1UL << 30)
-
-#include <linux/slob_def.h>
-
-#else /* CONFIG_SLOB */
+#endif /* CONFIG_SLOB */
 
 /*
  * Kmalloc array related definitions
@@ -195,7 +191,9 @@ struct kmem_cache {
 #ifndef KMALLOC_SHIFT_LOW
 #define KMALLOC_SHIFT_LOW	5
 #endif
-#else
+#endif
+
+#ifdef CONFIG_SLUB
 /*
  * SLUB allocates up to order 2 pages directly and otherwise
  * passes the request to the page allocator.
@@ -207,6 +205,19 @@ struct kmem_cache {
 #endif
 #endif
 
+#ifdef CONFIG_SLOB
+/*
+ * SLOB passes all page size and larger requests to the page allocator.
+ * No kmalloc array is necessary since objects of different sizes can
+ * be allocated from the same page.
+ */
+#define KMALLOC_SHIFT_MAX	30
+#define KMALLOC_SHIFT_HIGH	PAGE_SHIFT
+#ifndef KMALLOC_SHIFT_LOW
+#define KMALLOC_SHIFT_LOW	3
+#endif
+#endif
+
 /* Maximum allocatable size */
 #define KMALLOC_MAX_SIZE	(1UL << KMALLOC_SHIFT_MAX)
 /* Maximum size for which we actually use a slab cache */
@@ -221,6 +232,7 @@ struct kmem_cache {
 #define KMALLOC_MIN_SIZE (1 << KMALLOC_SHIFT_LOW)
 #endif
 
+#ifndef CONFIG_SLOB
 extern struct kmem_cache *kmalloc_caches[KMALLOC_SHIFT_HIGH + 1];
 #ifdef CONFIG_ZONE_DMA
 extern struct kmem_cache *kmalloc_dma_caches[KMALLOC_SHIFT_HIGH + 1];
@@ -275,13 +287,18 @@ static __always_inline int kmalloc_index(size_t size)
 	/* Will never be reached. Needed because the compiler may complain */
 	return -1;
 }
+#endif /* !CONFIG_SLOB */
 
 #ifdef CONFIG_SLAB
 #include <linux/slab_def.h>
-#elif defined(CONFIG_SLUB)
+#endif
+
+#ifdef CONFIG_SLUB
 #include <linux/slub_def.h>
-#else
-#error "Unknown slab allocator"
+#endif
+
+#ifdef CONFIG_SLOB
+#include <linux/slob_def.h>
 #endif
 
 /*
@@ -291,6 +308,7 @@ static __always_inline int kmalloc_index(size_t size)
  */
 static __always_inline int kmalloc_size(int n)
 {
+#ifndef CONFIG_SLOB
 	if (n > 2)
 		return 1 << n;
 
@@ -299,10 +317,9 @@ static __always_inline int kmalloc_size(int n)
 
 	if (n == 2 && KMALLOC_MIN_SIZE <= 64)
 		return 192;
-
+#endif
 	return 0;
 }
-#endif /* !CONFIG_SLOB */
 
 /*
  * Setting ARCH_SLAB_MINALIGN in arch headers allows a different alignment.

commit c601fd6956e92b0eb268d4af754073c76155b99d
Author: Christoph Lameter <cl@linux.com>
Date:   Tue Feb 5 16:36:47 2013 +0000

    slab: Handle ARCH_DMA_MINALIGN correctly
    
    James Hogan hit boot problems in next-20130204 on Meta:
    
      META213-Thread0 DSP [LogF] kobject (4fc03980): tried to init an initialized object, something is seriously wrong.
      META213-Thread0 DSP [LogF]
      META213-Thread0 DSP [LogF] Call trace:
      META213-Thread0 DSP [LogF] [<4000888c>] _show_stack+0x68/0x7c
      META213-Thread0 DSP [LogF] [<400088b4>] _dump_stack+0x14/0x28
      META213-Thread0 DSP [LogF] [<40103794>] _kobject_init+0x58/0x9c
      META213-Thread0 DSP [LogF] [<40103810>] _kobject_create+0x38/0x64
      META213-Thread0 DSP [LogF] [<40103eac>] _kobject_create_and_add+0x14/0x8c
      META213-Thread0 DSP [LogF] [<40190ac4>] _mnt_init+0xd8/0x220
      META213-Thread0 DSP [LogF] [<40190508>] _vfs_caches_init+0xb0/0x160
      META213-Thread0 DSP [LogF] [<401851f4>] _start_kernel+0x274/0x340
      META213-Thread0 DSP [LogF] [<40188424>] _metag_start_kernel+0x58/0x6c
      META213-Thread0 DSP [LogF] [<40000044>] __start+0x44/0x48
      META213-Thread0 DSP [LogF]
      META213-Thread0 DSP [LogF] devtmpfs: initialized
      META213-Thread0 DSP [LogF] L2 Cache: Not present
      META213-Thread0 DSP [LogF] BUG: failure at fs/sysfs/dir.c:736/sysfs_read_ns_type()!
      META213-Thread0 DSP [LogF] Kernel panic - not syncing: BUG!
      META213-Thread0 DSP [Thread Exit] Thread has exited - return code = 4294967295
    
    And bisected the problem to commit 95a05b4 ("slab: Common constants for
    kmalloc boundaries").
    
    As it turns out, a fixed KMALLOC_SHIFT_LOW does not work for arches with
    higher alignment requirements.
    
    Determine KMALLOC_SHIFT_LOW from ARCH_DMA_MINALIGN instead.
    
    Reported-and-tested-by: James Hogan <james.hogan@imgtec.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index f2327a898a85..0c621752caa6 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -133,6 +133,19 @@ void kfree(const void *);
 void kzfree(const void *);
 size_t ksize(const void *);
 
+/*
+ * Some archs want to perform DMA into kmalloc caches and need a guaranteed
+ * alignment larger than the alignment of a 64-bit integer.
+ * Setting ARCH_KMALLOC_MINALIGN in arch headers allows that.
+ */
+#if defined(ARCH_DMA_MINALIGN) && ARCH_DMA_MINALIGN > 8
+#define ARCH_KMALLOC_MINALIGN ARCH_DMA_MINALIGN
+#define KMALLOC_MIN_SIZE ARCH_DMA_MINALIGN
+#define KMALLOC_SHIFT_LOW ilog2(ARCH_DMA_MINALIGN)
+#else
+#define ARCH_KMALLOC_MINALIGN __alignof__(unsigned long long)
+#endif
+
 #ifdef CONFIG_SLOB
 /*
  * Common fields provided in kmem_cache by all slab allocators
@@ -179,7 +192,9 @@ struct kmem_cache {
 #define KMALLOC_SHIFT_HIGH	((MAX_ORDER + PAGE_SHIFT - 1) <= 25 ? \
 				(MAX_ORDER + PAGE_SHIFT - 1) : 25)
 #define KMALLOC_SHIFT_MAX	KMALLOC_SHIFT_HIGH
+#ifndef KMALLOC_SHIFT_LOW
 #define KMALLOC_SHIFT_LOW	5
+#endif
 #else
 /*
  * SLUB allocates up to order 2 pages directly and otherwise
@@ -187,8 +202,10 @@ struct kmem_cache {
  */
 #define KMALLOC_SHIFT_HIGH	(PAGE_SHIFT + 1)
 #define KMALLOC_SHIFT_MAX	(MAX_ORDER + PAGE_SHIFT)
+#ifndef KMALLOC_SHIFT_LOW
 #define KMALLOC_SHIFT_LOW	3
 #endif
+#endif
 
 /* Maximum allocatable size */
 #define KMALLOC_MAX_SIZE	(1UL << KMALLOC_SHIFT_MAX)
@@ -200,9 +217,7 @@ struct kmem_cache {
 /*
  * Kmalloc subsystem.
  */
-#if defined(ARCH_DMA_MINALIGN) && ARCH_DMA_MINALIGN > 8
-#define KMALLOC_MIN_SIZE ARCH_DMA_MINALIGN
-#else
+#ifndef KMALLOC_MIN_SIZE
 #define KMALLOC_MIN_SIZE (1 << KMALLOC_SHIFT_LOW)
 #endif
 
@@ -289,17 +304,6 @@ static __always_inline int kmalloc_size(int n)
 }
 #endif /* !CONFIG_SLOB */
 
-/*
- * Some archs want to perform DMA into kmalloc caches and need a guaranteed
- * alignment larger than the alignment of a 64-bit integer.
- * Setting ARCH_KMALLOC_MINALIGN in arch headers allows that.
- */
-#ifdef ARCH_DMA_MINALIGN
-#define ARCH_KMALLOC_MINALIGN ARCH_DMA_MINALIGN
-#else
-#define ARCH_KMALLOC_MINALIGN __alignof__(unsigned long long)
-#endif
-
 /*
  * Setting ARCH_SLAB_MINALIGN in arch headers allows a different alignment.
  * Intended for arches that get misalignment faults even for 64 bit integer

commit 9425c58e5445277699ff3c2a87bac1cfebc1b48d
Author: Christoph Lameter <cl@linux.com>
Date:   Thu Jan 10 19:12:17 2013 +0000

    slab: Common definition for the array of kmalloc caches
    
    Have a common definition fo the kmalloc cache arrays in
    SLAB and SLUB
    
    Acked-by: Glauber Costa <glommer@parallels.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index c01780540054..f2327a898a85 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -206,6 +206,11 @@ struct kmem_cache {
 #define KMALLOC_MIN_SIZE (1 << KMALLOC_SHIFT_LOW)
 #endif
 
+extern struct kmem_cache *kmalloc_caches[KMALLOC_SHIFT_HIGH + 1];
+#ifdef CONFIG_ZONE_DMA
+extern struct kmem_cache *kmalloc_dma_caches[KMALLOC_SHIFT_HIGH + 1];
+#endif
+
 /*
  * Figure out which kmalloc slab an allocation of a certain size
  * belongs to.

commit 95a05b428cc675694321c8f762591984f3fd2b1e
Author: Christoph Lameter <cl@linux.com>
Date:   Thu Jan 10 19:14:19 2013 +0000

    slab: Common constants for kmalloc boundaries
    
    Standardize the constants that describe the smallest and largest
    object kept in the kmalloc arrays for SLAB and SLUB.
    
    Differentiate between the maximum size for which a slab cache is used
    (KMALLOC_MAX_CACHE_SIZE) and the maximum allocatable size
    (KMALLOC_MAX_SIZE, KMALLOC_MAX_ORDER).
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index c97fe92532d1..c01780540054 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -163,7 +163,12 @@ struct kmem_cache {
 #else /* CONFIG_SLOB */
 
 /*
- * The largest kmalloc size supported by the slab allocators is
+ * Kmalloc array related definitions
+ */
+
+#ifdef CONFIG_SLAB
+/*
+ * The largest kmalloc size supported by the SLAB allocators is
  * 32 megabyte (2^25) or the maximum allocatable page order if that is
  * less than 32 MB.
  *
@@ -173,9 +178,24 @@ struct kmem_cache {
  */
 #define KMALLOC_SHIFT_HIGH	((MAX_ORDER + PAGE_SHIFT - 1) <= 25 ? \
 				(MAX_ORDER + PAGE_SHIFT - 1) : 25)
+#define KMALLOC_SHIFT_MAX	KMALLOC_SHIFT_HIGH
+#define KMALLOC_SHIFT_LOW	5
+#else
+/*
+ * SLUB allocates up to order 2 pages directly and otherwise
+ * passes the request to the page allocator.
+ */
+#define KMALLOC_SHIFT_HIGH	(PAGE_SHIFT + 1)
+#define KMALLOC_SHIFT_MAX	(MAX_ORDER + PAGE_SHIFT)
+#define KMALLOC_SHIFT_LOW	3
+#endif
 
-#define KMALLOC_MAX_SIZE	(1UL << KMALLOC_SHIFT_HIGH)
-#define KMALLOC_MAX_ORDER	(KMALLOC_SHIFT_HIGH - PAGE_SHIFT)
+/* Maximum allocatable size */
+#define KMALLOC_MAX_SIZE	(1UL << KMALLOC_SHIFT_MAX)
+/* Maximum size for which we actually use a slab cache */
+#define KMALLOC_MAX_CACHE_SIZE	(1UL << KMALLOC_SHIFT_HIGH)
+/* Maximum order allocatable via the slab allocagtor */
+#define KMALLOC_MAX_ORDER	(KMALLOC_SHIFT_MAX - PAGE_SHIFT)
 
 /*
  * Kmalloc subsystem.
@@ -183,15 +203,9 @@ struct kmem_cache {
 #if defined(ARCH_DMA_MINALIGN) && ARCH_DMA_MINALIGN > 8
 #define KMALLOC_MIN_SIZE ARCH_DMA_MINALIGN
 #else
-#ifdef CONFIG_SLAB
-#define KMALLOC_MIN_SIZE 32
-#else
-#define KMALLOC_MIN_SIZE 8
-#endif
+#define KMALLOC_MIN_SIZE (1 << KMALLOC_SHIFT_LOW)
 #endif
 
-#define KMALLOC_SHIFT_LOW ilog2(KMALLOC_MIN_SIZE)
-
 /*
  * Figure out which kmalloc slab an allocation of a certain size
  * belongs to.

commit ce6a50263d4ddeba1f0d08f16716a82770c03690
Author: Christoph Lameter <cl@linux.com>
Date:   Thu Jan 10 19:14:19 2013 +0000

    slab: Common kmalloc slab index determination
    
    Extract the function to determine the index of the slab within
    the array of kmalloc caches as well as a function to determine
    maximum object size from the nr of the kmalloc slab.
    
    This is used here only to simplify slub bootstrap but will
    be used later also for SLAB.
    
    Acked-by: Glauber Costa <glommer@parallels.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index ccbb37685c6c..c97fe92532d1 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -94,29 +94,6 @@
 #define ZERO_OR_NULL_PTR(x) ((unsigned long)(x) <= \
 				(unsigned long)ZERO_SIZE_PTR)
 
-/*
- * Common fields provided in kmem_cache by all slab allocators
- * This struct is either used directly by the allocator (SLOB)
- * or the allocator must include definitions for all fields
- * provided in kmem_cache_common in their definition of kmem_cache.
- *
- * Once we can do anonymous structs (C11 standard) we could put a
- * anonymous struct definition in these allocators so that the
- * separate allocations in the kmem_cache structure of SLAB and
- * SLUB is no longer needed.
- */
-#ifdef CONFIG_SLOB
-struct kmem_cache {
-	unsigned int object_size;/* The original size of the object */
-	unsigned int size;	/* The aligned/padded/added on size  */
-	unsigned int align;	/* Alignment as calculated */
-	unsigned long flags;	/* Active flags on the slab */
-	const char *name;	/* Slab name for sysfs */
-	int refcount;		/* Use counter */
-	void (*ctor)(void *);	/* Called on object slot creation */
-	struct list_head list;	/* List of all slab caches on the system */
-};
-#endif
 
 struct mem_cgroup;
 /*
@@ -156,6 +133,35 @@ void kfree(const void *);
 void kzfree(const void *);
 size_t ksize(const void *);
 
+#ifdef CONFIG_SLOB
+/*
+ * Common fields provided in kmem_cache by all slab allocators
+ * This struct is either used directly by the allocator (SLOB)
+ * or the allocator must include definitions for all fields
+ * provided in kmem_cache_common in their definition of kmem_cache.
+ *
+ * Once we can do anonymous structs (C11 standard) we could put a
+ * anonymous struct definition in these allocators so that the
+ * separate allocations in the kmem_cache structure of SLAB and
+ * SLUB is no longer needed.
+ */
+struct kmem_cache {
+	unsigned int object_size;/* The original size of the object */
+	unsigned int size;	/* The aligned/padded/added on size  */
+	unsigned int align;	/* Alignment as calculated */
+	unsigned long flags;	/* Active flags on the slab */
+	const char *name;	/* Slab name for sysfs */
+	int refcount;		/* Use counter */
+	void (*ctor)(void *);	/* Called on object slot creation */
+	struct list_head list;	/* List of all slab caches on the system */
+};
+
+#define KMALLOC_MAX_SIZE (1UL << 30)
+
+#include <linux/slob_def.h>
+
+#else /* CONFIG_SLOB */
+
 /*
  * The largest kmalloc size supported by the slab allocators is
  * 32 megabyte (2^25) or the maximum allocatable page order if that is
@@ -171,6 +177,99 @@ size_t ksize(const void *);
 #define KMALLOC_MAX_SIZE	(1UL << KMALLOC_SHIFT_HIGH)
 #define KMALLOC_MAX_ORDER	(KMALLOC_SHIFT_HIGH - PAGE_SHIFT)
 
+/*
+ * Kmalloc subsystem.
+ */
+#if defined(ARCH_DMA_MINALIGN) && ARCH_DMA_MINALIGN > 8
+#define KMALLOC_MIN_SIZE ARCH_DMA_MINALIGN
+#else
+#ifdef CONFIG_SLAB
+#define KMALLOC_MIN_SIZE 32
+#else
+#define KMALLOC_MIN_SIZE 8
+#endif
+#endif
+
+#define KMALLOC_SHIFT_LOW ilog2(KMALLOC_MIN_SIZE)
+
+/*
+ * Figure out which kmalloc slab an allocation of a certain size
+ * belongs to.
+ * 0 = zero alloc
+ * 1 =  65 .. 96 bytes
+ * 2 = 120 .. 192 bytes
+ * n = 2^(n-1) .. 2^n -1
+ */
+static __always_inline int kmalloc_index(size_t size)
+{
+	if (!size)
+		return 0;
+
+	if (size <= KMALLOC_MIN_SIZE)
+		return KMALLOC_SHIFT_LOW;
+
+	if (KMALLOC_MIN_SIZE <= 32 && size > 64 && size <= 96)
+		return 1;
+	if (KMALLOC_MIN_SIZE <= 64 && size > 128 && size <= 192)
+		return 2;
+	if (size <=          8) return 3;
+	if (size <=         16) return 4;
+	if (size <=         32) return 5;
+	if (size <=         64) return 6;
+	if (size <=        128) return 7;
+	if (size <=        256) return 8;
+	if (size <=        512) return 9;
+	if (size <=       1024) return 10;
+	if (size <=   2 * 1024) return 11;
+	if (size <=   4 * 1024) return 12;
+	if (size <=   8 * 1024) return 13;
+	if (size <=  16 * 1024) return 14;
+	if (size <=  32 * 1024) return 15;
+	if (size <=  64 * 1024) return 16;
+	if (size <= 128 * 1024) return 17;
+	if (size <= 256 * 1024) return 18;
+	if (size <= 512 * 1024) return 19;
+	if (size <= 1024 * 1024) return 20;
+	if (size <=  2 * 1024 * 1024) return 21;
+	if (size <=  4 * 1024 * 1024) return 22;
+	if (size <=  8 * 1024 * 1024) return 23;
+	if (size <=  16 * 1024 * 1024) return 24;
+	if (size <=  32 * 1024 * 1024) return 25;
+	if (size <=  64 * 1024 * 1024) return 26;
+	BUG();
+
+	/* Will never be reached. Needed because the compiler may complain */
+	return -1;
+}
+
+#ifdef CONFIG_SLAB
+#include <linux/slab_def.h>
+#elif defined(CONFIG_SLUB)
+#include <linux/slub_def.h>
+#else
+#error "Unknown slab allocator"
+#endif
+
+/*
+ * Determine size used for the nth kmalloc cache.
+ * return size or 0 if a kmalloc cache for that
+ * size does not exist
+ */
+static __always_inline int kmalloc_size(int n)
+{
+	if (n > 2)
+		return 1 << n;
+
+	if (n == 1 && KMALLOC_MIN_SIZE <= 32)
+		return 96;
+
+	if (n == 2 && KMALLOC_MIN_SIZE <= 64)
+		return 192;
+
+	return 0;
+}
+#endif /* !CONFIG_SLOB */
+
 /*
  * Some archs want to perform DMA into kmalloc caches and need a guaranteed
  * alignment larger than the alignment of a 64-bit integer.
@@ -233,33 +332,6 @@ struct seq_file;
 int cache_show(struct kmem_cache *s, struct seq_file *m);
 void print_slabinfo_header(struct seq_file *m);
 
-/*
- * Allocator specific definitions. These are mainly used to establish optimized
- * ways to convert kmalloc() calls to kmem_cache_alloc() invocations by
- * selecting the appropriate general cache at compile time.
- *
- * Allocators must define at least:
- *
- *	kmem_cache_alloc()
- *	__kmalloc()
- *	kmalloc()
- *
- * Those wishing to support NUMA must also define:
- *
- *	kmem_cache_alloc_node()
- *	kmalloc_node()
- *
- * See each allocator definition file for additional comments and
- * implementation notes.
- */
-#ifdef CONFIG_SLUB
-#include <linux/slub_def.h>
-#elif defined(CONFIG_SLOB)
-#include <linux/slob_def.h>
-#else
-#include <linux/slab_def.h>
-#endif
-
 /**
  * kmalloc_array - allocate memory for an array.
  * @n: number of elements.

commit 345046673449b5c35840e5cc34a60059cbec9305
Author: Christoph Lameter <cl@linux.com>
Date:   Thu Jan 10 19:00:53 2013 +0000

    slab: Move kmalloc related function defs
    
    Move these functions higher up in slab.h so that they are grouped with other
    generic kmalloc related definitions.
    
    Acked-by: Glauber Costa <glommer@parallels.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 5d168d7e0a28..ccbb37685c6c 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -147,6 +147,15 @@ void kmem_cache_free(struct kmem_cache *, void *);
 		sizeof(struct __struct), __alignof__(struct __struct),\
 		(__flags), NULL)
 
+/*
+ * Common kmalloc functions provided by all allocators
+ */
+void * __must_check __krealloc(const void *, size_t, gfp_t);
+void * __must_check krealloc(const void *, size_t, gfp_t);
+void kfree(const void *);
+void kzfree(const void *);
+size_t ksize(const void *);
+
 /*
  * The largest kmalloc size supported by the slab allocators is
  * 32 megabyte (2^25) or the maximum allocatable page order if that is
@@ -224,15 +233,6 @@ struct seq_file;
 int cache_show(struct kmem_cache *s, struct seq_file *m);
 void print_slabinfo_header(struct seq_file *m);
 
-/*
- * Common kmalloc functions provided by all allocators
- */
-void * __must_check __krealloc(const void *, size_t, gfp_t);
-void * __must_check krealloc(const void *, size_t, gfp_t);
-void kfree(const void *);
-void kzfree(const void *);
-size_t ksize(const void *);
-
 /*
  * Allocator specific definitions. These are mainly used to establish optimized
  * ways to convert kmalloc() calls to kmem_cache_alloc() invocations by

commit 943a451a87d229ca564a27274b58eaeae35fde5d
Author: Glauber Costa <glommer@parallels.com>
Date:   Tue Dec 18 14:23:03 2012 -0800

    slab: propagate tunable values
    
    SLAB allows us to tune a particular cache behavior with tunables.  When
    creating a new memcg cache copy, we'd like to preserve any tunables the
    parent cache already had.
    
    This could be done by an explicit call to do_tune_cpucache() after the
    cache is created.  But this is not very convenient now that the caches are
    created from common code, since this function is SLAB-specific.
    
    Another method of doing that is taking advantage of the fact that
    do_tune_cpucache() is always called from enable_cpucache(), which is
    called at cache initialization.  We can just preset the values, and then
    things work as expected.
    
    It can also happen that a root cache has its tunables updated during
    normal system operation.  In this case, we will propagate the change to
    all caches that are already active.
    
    This change will require us to move the assignment of root_cache in
    memcg_params a bit earlier.  We need this to be already set - which
    memcg_kmem_register_cache will do - when we reach __kmem_cache_create()
    
    Signed-off-by: Glauber Costa <glommer@parallels.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Frederic Weisbecker <fweisbec@redhat.com>
    Cc: Greg Thelen <gthelen@google.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: JoonSoo Kim <js1304@gmail.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Suleiman Souhlal <suleiman@google.com>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index b9278663f22a..5d168d7e0a28 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -130,7 +130,7 @@ struct kmem_cache *kmem_cache_create(const char *, size_t, size_t,
 			void (*)(void *));
 struct kmem_cache *
 kmem_cache_create_memcg(struct mem_cgroup *, const char *, size_t, size_t,
-			unsigned long, void (*)(void *));
+			unsigned long, void (*)(void *), struct kmem_cache *);
 void kmem_cache_destroy(struct kmem_cache *);
 int kmem_cache_shrink(struct kmem_cache *);
 void kmem_cache_free(struct kmem_cache *, void *);

commit 749c54151a6e5b229e4ae067dbc651e54b161fbc
Author: Glauber Costa <glommer@parallels.com>
Date:   Tue Dec 18 14:23:01 2012 -0800

    memcg: aggregate memcg cache values in slabinfo
    
    When we create caches in memcgs, we need to display their usage
    information somewhere.  We'll adopt a scheme similar to /proc/meminfo,
    with aggregate totals shown in the global file, and per-group information
    stored in the group itself.
    
    For the time being, only reads are allowed in the per-group cache.
    
    Signed-off-by: Glauber Costa <glommer@parallels.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Frederic Weisbecker <fweisbec@redhat.com>
    Cc: Greg Thelen <gthelen@google.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: JoonSoo Kim <js1304@gmail.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Suleiman Souhlal <suleiman@google.com>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 869efb8d2377..b9278663f22a 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -220,6 +220,10 @@ struct memcg_cache_params {
 
 int memcg_update_all_caches(int num_memcgs);
 
+struct seq_file;
+int cache_show(struct kmem_cache *s, struct seq_file *m);
+void print_slabinfo_header(struct seq_file *m);
+
 /*
  * Common kmalloc functions provided by all allocators
  */

commit 1f458cbf122288b23620ee822e19bcbb76c8d6ec
Author: Glauber Costa <glommer@parallels.com>
Date:   Tue Dec 18 14:22:50 2012 -0800

    memcg: destroy memcg caches
    
    Implement destruction of memcg caches.  Right now, only caches where our
    reference counter is the last remaining are deleted.  If there are any
    other reference counters around, we just leave the caches lying around
    until they go away.
    
    When that happens, a destruction function is called from the cache code.
    Caches are only destroyed in process context, so we queue them up for
    later processing in the general case.
    
    Signed-off-by: Glauber Costa <glommer@parallels.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Frederic Weisbecker <fweisbec@redhat.com>
    Cc: Greg Thelen <gthelen@google.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: JoonSoo Kim <js1304@gmail.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Suleiman Souhlal <suleiman@google.com>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index c0fcf28c15b2..869efb8d2377 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -11,6 +11,8 @@
 
 #include <linux/gfp.h>
 #include <linux/types.h>
+#include <linux/workqueue.h>
+
 
 /*
  * Flags to pass to kmem_cache_create().
@@ -179,7 +181,6 @@ void kmem_cache_free(struct kmem_cache *, void *);
 #ifndef ARCH_SLAB_MINALIGN
 #define ARCH_SLAB_MINALIGN __alignof__(unsigned long long)
 #endif
-
 /*
  * This is the main placeholder for memcg-related information in kmem caches.
  * struct kmem_cache will hold a pointer to it, so the memory cost while
@@ -197,6 +198,10 @@ void kmem_cache_free(struct kmem_cache *, void *);
  * @memcg: pointer to the memcg this cache belongs to
  * @list: list_head for the list of all caches in this memcg
  * @root_cache: pointer to the global, root cache, this cache was derived from
+ * @dead: set to true after the memcg dies; the cache may still be around.
+ * @nr_pages: number of pages that belongs to this cache.
+ * @destroy: worker to be called whenever we are ready, or believe we may be
+ *           ready, to destroy this cache.
  */
 struct memcg_cache_params {
 	bool is_root_cache;
@@ -206,6 +211,9 @@ struct memcg_cache_params {
 			struct mem_cgroup *memcg;
 			struct list_head list;
 			struct kmem_cache *root_cache;
+			bool dead;
+			atomic_t nr_pages;
+			struct work_struct destroy;
 		};
 	};
 };

commit 2633d7a028239a738b793be5ca8fa6ac312f5793
Author: Glauber Costa <glommer@parallels.com>
Date:   Tue Dec 18 14:22:34 2012 -0800

    slab/slub: consider a memcg parameter in kmem_create_cache
    
    Allow a memcg parameter to be passed during cache creation.  When the slub
    allocator is being used, it will only merge caches that belong to the same
    memcg.  We'll do this by scanning the global list, and then translating
    the cache to a memcg-specific cache
    
    Default function is created as a wrapper, passing NULL to the memcg
    version.  We only merge caches that belong to the same memcg.
    
    A helper is provided, memcg_css_id: because slub needs a unique cache name
    for sysfs.  Since this is visible, but not the canonical location for slab
    data, the cache name is not used, the css_id should suffice.
    
    Signed-off-by: Glauber Costa <glommer@parallels.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Frederic Weisbecker <fweisbec@redhat.com>
    Cc: Greg Thelen <gthelen@google.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: JoonSoo Kim <js1304@gmail.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Suleiman Souhlal <suleiman@google.com>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 00efba149222..c0fcf28c15b2 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -116,6 +116,7 @@ struct kmem_cache {
 };
 #endif
 
+struct mem_cgroup;
 /*
  * struct kmem_cache related prototypes
  */
@@ -125,6 +126,9 @@ int slab_is_available(void);
 struct kmem_cache *kmem_cache_create(const char *, size_t, size_t,
 			unsigned long,
 			void (*)(void *));
+struct kmem_cache *
+kmem_cache_create_memcg(struct mem_cgroup *, const char *, size_t, size_t,
+			unsigned long, void (*)(void *));
 void kmem_cache_destroy(struct kmem_cache *);
 int kmem_cache_shrink(struct kmem_cache *);
 void kmem_cache_free(struct kmem_cache *, void *);
@@ -191,15 +195,23 @@ void kmem_cache_free(struct kmem_cache *, void *);
  * Child caches will hold extra metadata needed for its operation. Fields are:
  *
  * @memcg: pointer to the memcg this cache belongs to
+ * @list: list_head for the list of all caches in this memcg
+ * @root_cache: pointer to the global, root cache, this cache was derived from
  */
 struct memcg_cache_params {
 	bool is_root_cache;
 	union {
 		struct kmem_cache *memcg_caches[0];
-		struct mem_cgroup *memcg;
+		struct {
+			struct mem_cgroup *memcg;
+			struct list_head list;
+			struct kmem_cache *root_cache;
+		};
 	};
 };
 
+int memcg_update_all_caches(int num_memcgs);
+
 /*
  * Common kmalloc functions provided by all allocators
  */

commit ba6c496ed834a37a26fc6fc87fc9aecb0fa0014d
Author: Glauber Costa <glommer@parallels.com>
Date:   Tue Dec 18 14:22:27 2012 -0800

    slab/slub: struct memcg_params
    
    For the kmem slab controller, we need to record some extra information in
    the kmem_cache structure.
    
    Signed-off-by: Glauber Costa <glommer@parallels.com>
    Signed-off-by: Suleiman Souhlal <suleiman@google.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Frederic Weisbecker <fweisbec@redhat.com>
    Cc: Greg Thelen <gthelen@google.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: JoonSoo Kim <js1304@gmail.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 743a10415122..00efba149222 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -176,6 +176,30 @@ void kmem_cache_free(struct kmem_cache *, void *);
 #define ARCH_SLAB_MINALIGN __alignof__(unsigned long long)
 #endif
 
+/*
+ * This is the main placeholder for memcg-related information in kmem caches.
+ * struct kmem_cache will hold a pointer to it, so the memory cost while
+ * disabled is 1 pointer. The runtime cost while enabled, gets bigger than it
+ * would otherwise be if that would be bundled in kmem_cache: we'll need an
+ * extra pointer chase. But the trade off clearly lays in favor of not
+ * penalizing non-users.
+ *
+ * Both the root cache and the child caches will have it. For the root cache,
+ * this will hold a dynamically allocated array large enough to hold
+ * information about the currently limited memcgs in the system.
+ *
+ * Child caches will hold extra metadata needed for its operation. Fields are:
+ *
+ * @memcg: pointer to the memcg this cache belongs to
+ */
+struct memcg_cache_params {
+	bool is_root_cache;
+	union {
+		struct kmem_cache *memcg_caches[0];
+		struct mem_cgroup *memcg;
+	};
+};
+
 /*
  * Common kmalloc functions provided by all allocators
  */

commit 242860a47a75b933a79a30f6a40bf4858f4a3ecc
Author: Ezequiel Garcia <elezegarcia@gmail.com>
Date:   Fri Oct 19 09:33:12 2012 -0300

    mm/sl[aou]b: Move common kmem_cache_size() to slab.h
    
    This function is identically defined in all three allocators
    and it's trivial to move it to slab.h
    
    Since now it's static, inline, header-defined function
    this patch also drops the EXPORT_SYMBOL tag.
    
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Matt Mackall <mpm@selenic.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Ezequiel Garcia <elezegarcia@gmail.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 83d1a1454b7e..743a10415122 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -128,7 +128,6 @@ struct kmem_cache *kmem_cache_create(const char *, size_t, size_t,
 void kmem_cache_destroy(struct kmem_cache *);
 int kmem_cache_shrink(struct kmem_cache *);
 void kmem_cache_free(struct kmem_cache *, void *);
-unsigned int kmem_cache_size(struct kmem_cache *);
 
 /*
  * Please use this macro to create slab caches. Simply specify the
@@ -388,6 +387,14 @@ static inline void *kzalloc_node(size_t size, gfp_t flags, int node)
 	return kmalloc_node(size, flags | __GFP_ZERO, node);
 }
 
+/*
+ * Determine the size of a slab object
+ */
+static inline unsigned int kmem_cache_size(struct kmem_cache *s)
+{
+	return s->object_size;
+}
+
 void __init kmem_cache_init_late(void);
 
 #endif	/* _LINUX_SLAB_H */

commit f3f741019595f1e73564d985f5fe8abcbb98c769
Author: Ezequiel Garcia <elezegarcia@gmail.com>
Date:   Sat Sep 8 17:47:53 2012 -0300

    mm, slob: Add support for kmalloc_track_caller()
    
    Currently slob falls back to regular kmalloc for this case.
    With this patch kmalloc_track_caller() is correctly implemented,
    thus tracing the specified caller.
    
    This is important to trace accurately allocations performed by
    krealloc, kstrdup, kmemdup, etc.
    
    Signed-off-by: Ezequiel Garcia <elezegarcia@gmail.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 0dd2dfa7beca..83d1a1454b7e 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -321,7 +321,8 @@ static inline void *kmem_cache_alloc_node(struct kmem_cache *cachep,
  * request comes from.
  */
 #if defined(CONFIG_DEBUG_SLAB) || defined(CONFIG_SLUB) || \
-	(defined(CONFIG_SLAB) && defined(CONFIG_TRACING))
+	(defined(CONFIG_SLAB) && defined(CONFIG_TRACING)) || \
+	(defined(CONFIG_SLOB) && defined(CONFIG_TRACING))
 extern void *__kmalloc_track_caller(size_t, gfp_t, unsigned long);
 #define kmalloc_track_caller(size, flags) \
 	__kmalloc_track_caller(size, flags, _RET_IP_)
@@ -340,7 +341,8 @@ extern void *__kmalloc_track_caller(size_t, gfp_t, unsigned long);
  * allocation request comes from.
  */
 #if defined(CONFIG_DEBUG_SLAB) || defined(CONFIG_SLUB) || \
-	(defined(CONFIG_SLAB) && defined(CONFIG_TRACING))
+	(defined(CONFIG_SLAB) && defined(CONFIG_TRACING)) || \
+	(defined(CONFIG_SLOB) && defined(CONFIG_TRACING))
 extern void *__kmalloc_node_track_caller(size_t, gfp_t, int, unsigned long);
 #define kmalloc_node_track_caller(size, flags, node) \
 	__kmalloc_node_track_caller(size, flags, node, \

commit 97d06609158e61f6bdf538c4a6788e2de492236f
Author: Christoph Lameter <cl@linux.com>
Date:   Fri Jul 6 15:25:11 2012 -0500

    mm, sl[aou]b: Common definition for boot state of the slab allocators
    
    All allocators have some sort of support for the bootstrap status.
    
    Setup a common definition for the boot states and make all slab
    allocators use that definition.
    
    Reviewed-by: Glauber Costa <glommer@parallels.com>
    Reviewed-by: Joonsoo Kim <js1304@gmail.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 0cb7c7eb0416..0dd2dfa7beca 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -130,10 +130,6 @@ int kmem_cache_shrink(struct kmem_cache *);
 void kmem_cache_free(struct kmem_cache *, void *);
 unsigned int kmem_cache_size(struct kmem_cache *);
 
-/* Slab internal function */
-struct kmem_cache *__kmem_cache_create(const char *, size_t, size_t,
-			unsigned long,
-			void (*)(void *));
 /*
  * Please use this macro to create slab caches. Simply specify the
  * name of the structure and maybe some flags that are listed above.

commit 039363f38bfe5f6281e9eae5e0518b11577d9d50
Author: Christoph Lameter <cl@linux.com>
Date:   Fri Jul 6 15:25:10 2012 -0500

    mm, sl[aou]b: Extract common code for kmem_cache_create()
    
    Kmem_cache_create() does a variety of sanity checks but those
    vary depending on the allocator. Use the strictest tests and put them into
    a slab_common file. Make the tests conditional on CONFIG_DEBUG_VM.
    
    This patch has the effect of adding sanity checks for SLUB and SLOB
    under CONFIG_DEBUG_VM and removes the checks in SLAB for !CONFIG_DEBUG_VM.
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 0dd2dfa7beca..0cb7c7eb0416 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -130,6 +130,10 @@ int kmem_cache_shrink(struct kmem_cache *);
 void kmem_cache_free(struct kmem_cache *, void *);
 unsigned int kmem_cache_size(struct kmem_cache *);
 
+/* Slab internal function */
+struct kmem_cache *__kmem_cache_create(const char *, size_t, size_t,
+			unsigned long,
+			void (*)(void *));
 /*
  * Please use this macro to create slab caches. Simply specify the
  * name of the structure and maybe some flags that are listed above.

commit 3b0efdfa1e719303536c04d9abca43abeb40f80a
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Jun 13 10:24:57 2012 -0500

    mm, sl[aou]b: Extract common fields from struct kmem_cache
    
    Define a struct that describes common fields used in all slab allocators.
    A slab allocator either uses the common definition (like SLOB) or is
    required to provide members of kmem_cache with the definition given.
    
    After that it will be possible to share code that
    only operates on those fields of kmem_cache.
    
    The patch basically takes the slob definition of kmem cache and
    uses the field namees for the other allocators.
    
    It also standardizes the names used for basic object lengths in
    allocators:
    
    object_size     Struct size specified at kmem_cache_create. Basically
                    the payload expected to be used by the subsystem.
    
    size            The size of memory allocator for each object. This size
                    is larger than object_size and includes padding, alignment
                    and extra metadata for each object (f.e. for debugging
                    and rcu).
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 67d5d94b783a..0dd2dfa7beca 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -92,6 +92,30 @@
 #define ZERO_OR_NULL_PTR(x) ((unsigned long)(x) <= \
 				(unsigned long)ZERO_SIZE_PTR)
 
+/*
+ * Common fields provided in kmem_cache by all slab allocators
+ * This struct is either used directly by the allocator (SLOB)
+ * or the allocator must include definitions for all fields
+ * provided in kmem_cache_common in their definition of kmem_cache.
+ *
+ * Once we can do anonymous structs (C11 standard) we could put a
+ * anonymous struct definition in these allocators so that the
+ * separate allocations in the kmem_cache structure of SLAB and
+ * SLUB is no longer needed.
+ */
+#ifdef CONFIG_SLOB
+struct kmem_cache {
+	unsigned int object_size;/* The original size of the object */
+	unsigned int size;	/* The aligned/padded/added on size  */
+	unsigned int align;	/* Alignment as calculated */
+	unsigned long flags;	/* Active flags on the slab */
+	const char *name;	/* Slab name for sysfs */
+	int refcount;		/* Use counter */
+	void (*ctor)(void *);	/* Called on object slot creation */
+	struct list_head list;	/* List of all slab caches on the system */
+};
+#endif
+
 /*
  * struct kmem_cache related prototypes
  */

commit a3860c1c5dd1137db23d7786d284939c5761d517
Author: Xi Wang <xi.wang@gmail.com>
Date:   Thu May 31 16:26:04 2012 -0700

    introduce SIZE_MAX
    
    ULONG_MAX is often used to check for integer overflow when calculating
    allocation size.  While ULONG_MAX happens to work on most systems, there
    is no guarantee that `size_t' must be the same size as `long'.
    
    This patch introduces SIZE_MAX, the maximum value of `size_t', to improve
    portability and readability for allocation size validation.
    
    Signed-off-by: Xi Wang <xi.wang@gmail.com>
    Acked-by: Alex Elder <elder@dreamhost.com>
    Cc: David Airlie <airlied@linux.ie>
    Cc: Pekka Enberg <penberg@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index a595dce6b0c7..67d5d94b783a 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -242,7 +242,7 @@ size_t ksize(const void *);
  */
 static inline void *kmalloc_array(size_t n, size_t size, gfp_t flags)
 {
-	if (size != 0 && n > ULONG_MAX / size)
+	if (size != 0 && n > SIZE_MAX / size)
 		return NULL;
 	return __kmalloc(n * size, flags);
 }

commit a8203725dfded5c1f79dca3368a4a273e24b59bb
Author: Xi Wang <xi.wang@gmail.com>
Date:   Mon Mar 5 15:14:41 2012 -0800

    slab: introduce kmalloc_array()
    
    Introduce a kmalloc_array() wrapper that performs integer overflow
    checking without zeroing the memory.
    
    Suggested-by: Andrew Morton <akpm@linux-foundation.org>
    Suggested-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Xi Wang <xi.wang@gmail.com>
    Cc: Dan Carpenter <dan.carpenter@oracle.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 573c809c33d9..a595dce6b0c7 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -190,7 +190,7 @@ size_t ksize(const void *);
 #endif
 
 /**
- * kcalloc - allocate memory for an array. The memory is set to zero.
+ * kmalloc_array - allocate memory for an array.
  * @n: number of elements.
  * @size: element size.
  * @flags: the type of memory to allocate.
@@ -240,11 +240,22 @@ size_t ksize(const void *);
  * for general use, and so are not documented here. For a full list of
  * potential flags, always refer to linux/gfp.h.
  */
-static inline void *kcalloc(size_t n, size_t size, gfp_t flags)
+static inline void *kmalloc_array(size_t n, size_t size, gfp_t flags)
 {
 	if (size != 0 && n > ULONG_MAX / size)
 		return NULL;
-	return __kmalloc(n * size, flags | __GFP_ZERO);
+	return __kmalloc(n * size, flags);
+}
+
+/**
+ * kcalloc - allocate memory for an array. The memory is set to zero.
+ * @n: number of elements.
+ * @size: element size.
+ * @flags: the type of memory to allocate (see kmalloc).
+ */
+static inline void *kcalloc(size_t n, size_t size, gfp_t flags)
+{
+	return kmalloc_array(n, size, flags | __GFP_ZERO);
 }
 
 #if !defined(CONFIG_NUMA) && !defined(CONFIG_SLOB)

commit 90810645f78f894acfb04b3768e8a7d45f2b303a
Author: Christoph Lameter <cl@linux.com>
Date:   Thu Jun 23 09:36:12 2011 -0500

    slab allocators: Provide generic description of alignment defines
    
    Provide description for alignment defines.
    
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 646a639a4aae..573c809c33d9 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -133,12 +133,22 @@ unsigned int kmem_cache_size(struct kmem_cache *);
 #define KMALLOC_MAX_SIZE	(1UL << KMALLOC_SHIFT_HIGH)
 #define KMALLOC_MAX_ORDER	(KMALLOC_SHIFT_HIGH - PAGE_SHIFT)
 
+/*
+ * Some archs want to perform DMA into kmalloc caches and need a guaranteed
+ * alignment larger than the alignment of a 64-bit integer.
+ * Setting ARCH_KMALLOC_MINALIGN in arch headers allows that.
+ */
 #ifdef ARCH_DMA_MINALIGN
 #define ARCH_KMALLOC_MINALIGN ARCH_DMA_MINALIGN
 #else
 #define ARCH_KMALLOC_MINALIGN __alignof__(unsigned long long)
 #endif
 
+/*
+ * Setting ARCH_SLAB_MINALIGN in arch headers allows a different alignment.
+ * Intended for arches that get misalignment faults even for 64 bit integer
+ * aligned buffers.
+ */
 #ifndef ARCH_SLAB_MINALIGN
 #define ARCH_SLAB_MINALIGN __alignof__(unsigned long long)
 #endif

commit 3192b920bf7d0c528ab54e7d3689f44055316a37
Author: Christoph Lameter <cl@linux.com>
Date:   Tue Jun 14 16:16:36 2011 -0500

    slab, slub, slob: Unify alignment definition
    
    Every slab has its on alignment definition in include/linux/sl?b_def.h. Extract those
    and define a common set in include/linux/slab.h.
    
    SLOB: As notes sometimes we need double word alignment on 32 bit. This gives all
    structures allocated by SLOB a unsigned long long alignment like the others do.
    
    SLAB: If ARCH_SLAB_MINALIGN is not set SLAB would set ARCH_SLAB_MINALIGN to
    zero meaning no alignment at all. Give it the default unsigned long long alignment.
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index ad4dd1c8d30a..646a639a4aae 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -133,6 +133,16 @@ unsigned int kmem_cache_size(struct kmem_cache *);
 #define KMALLOC_MAX_SIZE	(1UL << KMALLOC_SHIFT_HIGH)
 #define KMALLOC_MAX_ORDER	(KMALLOC_SHIFT_HIGH - PAGE_SHIFT)
 
+#ifdef ARCH_DMA_MINALIGN
+#define ARCH_KMALLOC_MINALIGN ARCH_DMA_MINALIGN
+#else
+#define ARCH_KMALLOC_MINALIGN __alignof__(unsigned long long)
+#endif
+
+#ifndef ARCH_SLAB_MINALIGN
+#define ARCH_SLAB_MINALIGN __alignof__(unsigned long long)
+#endif
+
 /*
  * Common kmalloc functions provided by all allocators
  */

commit 63310467a3d1ed6a0460ec1f4268126cd1ceec2e
Author: Christoph Lameter <cl@linux.com>
Date:   Thu Jan 20 11:12:26 2011 -0600

    mm: Remove support for kmem_cache_name()
    
    The last user was ext4 and Eric Sandeen removed the call in a recent patch. See
    the following URL for the discussion:
    
    http://marc.info/?l=linux-ext4&m=129546975702198&w=2
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index fa9086647eb7..ad4dd1c8d30a 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -105,7 +105,6 @@ void kmem_cache_destroy(struct kmem_cache *);
 int kmem_cache_shrink(struct kmem_cache *);
 void kmem_cache_free(struct kmem_cache *, void *);
 unsigned int kmem_cache_size(struct kmem_cache *);
-const char *kmem_cache_name(struct kmem_cache *);
 
 /*
  * Please use this macro to create slab caches. Simply specify the

commit ccd35fb9f4da856b105ea0f1e0cab3702e8ae6ba
Author: Nick Piggin <npiggin@kernel.dk>
Date:   Fri Jan 7 17:49:17 2011 +1100

    kernel: kmem_ptr_validate considered harmful
    
    This is a nasty and error prone API. It is no longer used, remove it.
    
    Signed-off-by: Nick Piggin <npiggin@kernel.dk>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 59260e21bdf5..fa9086647eb7 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -106,8 +106,6 @@ int kmem_cache_shrink(struct kmem_cache *);
 void kmem_cache_free(struct kmem_cache *, void *);
 unsigned int kmem_cache_size(struct kmem_cache *);
 const char *kmem_cache_name(struct kmem_cache *);
-int kern_ptr_validate(const void *ptr, unsigned long size);
-int kmem_ptr_validate(struct kmem_cache *cachep, const void *ptr);
 
 /*
  * Please use this macro to create slab caches. Simply specify the

commit 7adde04a2f5a798f04a556dfb3b69bff388e5dc4
Author: Xiaotian Feng <dfeng@redhat.com>
Date:   Wed Jun 30 17:57:22 2010 +0800

    slab: fix caller tracking on !CONFIG_DEBUG_SLAB && CONFIG_TRACING
    
    In slab, all __xxx_track_caller is defined on CONFIG_DEBUG_SLAB || CONFIG_TRACING,
    thus caller tracking function should be worked for CONFIG_TRACING. But if
    CONFIG_DEBUG_SLAB is not set, include/linux/slab.h will define xxx_track_caller to
    __xxx() without consideration of CONFIG_TRACING. This will break the caller tracking
    behaviour then.
    
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: Matt Mackall <mpm@selenic.com>
    Cc: Vegard Nossum <vegard.nossum@gmail.com>
    Cc: Dmitry Monakhov <dmonakhov@openvz.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Xiaotian Feng <dfeng@redhat.com>
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 49d1247cd6d9..59260e21bdf5 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -268,7 +268,8 @@ static inline void *kmem_cache_alloc_node(struct kmem_cache *cachep,
  * allocator where we care about the real place the memory allocation
  * request comes from.
  */
-#if defined(CONFIG_DEBUG_SLAB) || defined(CONFIG_SLUB)
+#if defined(CONFIG_DEBUG_SLAB) || defined(CONFIG_SLUB) || \
+	(defined(CONFIG_SLAB) && defined(CONFIG_TRACING))
 extern void *__kmalloc_track_caller(size_t, gfp_t, unsigned long);
 #define kmalloc_track_caller(size, flags) \
 	__kmalloc_track_caller(size, flags, _RET_IP_)
@@ -286,7 +287,8 @@ extern void *__kmalloc_track_caller(size_t, gfp_t, unsigned long);
  * standard allocator where we care about the real place the memory
  * allocation request comes from.
  */
-#if defined(CONFIG_DEBUG_SLAB) || defined(CONFIG_SLUB)
+#if defined(CONFIG_DEBUG_SLAB) || defined(CONFIG_SLUB) || \
+	(defined(CONFIG_SLAB) && defined(CONFIG_TRACING))
 extern void *__kmalloc_node_track_caller(size_t, gfp_t, int, unsigned long);
 #define kmalloc_node_track_caller(size, flags, node) \
 	__kmalloc_node_track_caller(size, flags, node, \

commit fc1c183353a113c71675fecd0485e5aa0fe68d72
Author: Pekka Enberg <penberg@cs.helsinki.fi>
Date:   Wed Apr 7 19:23:40 2010 +0300

    slab: Generify kernel pointer validation
    
    As suggested by Linus, introduce a kern_ptr_validate() helper that does some
    sanity checks to make sure a pointer is a valid kernel pointer.  This is a
    preparational step for fixing SLUB kmem_ptr_validate().
    
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Matt Mackall <mpm@selenic.com>
    Cc: Nick Piggin <npiggin@suse.de>
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 488446289cab..49d1247cd6d9 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -106,6 +106,7 @@ int kmem_cache_shrink(struct kmem_cache *);
 void kmem_cache_free(struct kmem_cache *, void *);
 unsigned int kmem_cache_size(struct kmem_cache *);
 const char *kmem_cache_name(struct kmem_cache *);
+int kern_ptr_validate(const void *ptr, unsigned long size);
 int kmem_ptr_validate(struct kmem_cache *cachep, const void *ptr);
 
 /*

commit 4c13dd3b48fcb6fbe44f241eb11a057ecd1cba75
Author: Dmitry Monakhov <dmonakhov@openvz.org>
Date:   Fri Feb 26 09:36:12 2010 +0300

    failslab: add ability to filter slab caches
    
    This patch allow to inject faults only for specific slabs.
    In order to preserve default behavior cache filter is off by
    default (all caches are faulty).
    
    One may define specific set of slabs like this:
    # mark skbuff_head_cache as faulty
    echo 1 > /sys/kernel/slab/skbuff_head_cache/failslab
    # Turn on cache filter (off by default)
    echo 1 > /sys/kernel/debug/failslab/cache-filter
    # Turn on fault injection
    echo 1 > /sys/kernel/debug/failslab/times
    echo 1 > /sys/kernel/debug/failslab/probability
    
    Acked-by: David Rientjes <rientjes@google.com>
    Acked-by: Akinobu Mita <akinobu.mita@gmail.com>
    Acked-by: Christoph Lameter <cl@linux-foundation.org>
    Signed-off-by: Dmitry Monakhov <dmonakhov@openvz.org>
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 2da8372519f5..488446289cab 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -70,6 +70,11 @@
 #else
 # define SLAB_NOTRACK		0x00000000UL
 #endif
+#ifdef CONFIG_FAILSLAB
+# define SLAB_FAILSLAB		0x02000000UL	/* Fault injection mark */
+#else
+# define SLAB_FAILSLAB		0x00000000UL
+#endif
 
 /* The following flags affect the page allocator grouping pages by mobility */
 #define SLAB_RECLAIM_ACCOUNT	0x00020000UL		/* Objects are reclaimable */

commit 722f2a6c87f34ee0fd0130a8cf45f81e0705594a
Merge: 7a0aeb14e18a 45e3e1935e28
Author: Vegard Nossum <vegard.nossum@gmail.com>
Date:   Mon Jun 15 15:50:49 2009 +0200

    Merge commit 'linus/master' into HEAD
    
    Conflicts:
            MAINTAINERS
    
    Signed-off-by: Vegard Nossum <vegard.nossum@gmail.com>

commit 2dff440525f8faba8836e9f05297b76f23b4af30
Author: Vegard Nossum <vegard.nossum@gmail.com>
Date:   Sat May 31 15:56:17 2008 +0200

    kmemcheck: add mm functions
    
    With kmemcheck enabled, the slab allocator needs to do this:
    
    1. Tell kmemcheck to allocate the shadow memory which stores the status of
       each byte in the allocation proper, e.g. whether it is initialized or
       uninitialized.
    2. Tell kmemcheck which parts of memory that should be marked uninitialized.
       There are actually a few more states, such as "not yet allocated" and
       "recently freed".
    
    If a slab cache is set up using the SLAB_NOTRACK flag, it will never return
    memory that can take page faults because of kmemcheck.
    
    If a slab cache is NOT set up using the SLAB_NOTRACK flag, callers can still
    request memory with the __GFP_NOTRACK flag. This does not prevent the page
    faults from occuring, however, but marks the object in question as being
    initialized so that no warnings will ever be produced for this object.
    
    In addition to (and in contrast to) __GFP_NOTRACK, the
    __GFP_NOTRACK_FALSE_POSITIVE flag indicates that the allocation should
    not be tracked _because_ it would produce a false positive. Their values
    are identical, but need not be so in the future (for example, we could now
    enable/disable false positives with a config option).
    
    Parts of this patch were contributed by Pekka Enberg but merged for
    atomicity.
    
    Signed-off-by: Vegard Nossum <vegard.nossum@gmail.com>
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    
    [rebased for mainline inclusion]
    Signed-off-by: Vegard Nossum <vegard.nossum@gmail.com>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 48803064cedf..e339fcf17cd3 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -64,6 +64,13 @@
 
 #define SLAB_NOLEAKTRACE	0x00800000UL	/* Avoid kmemleak tracing */
 
+/* Don't track use of uninitialized memory */
+#ifdef CONFIG_KMEMCHECK
+# define SLAB_NOTRACK		0x01000000UL
+#else
+# define SLAB_NOTRACK		0x00000000UL
+#endif
+
 /* The following flags affect the page allocator grouping pages by mobility */
 #define SLAB_RECLAIM_ACCOUNT	0x00020000UL		/* Objects are reclaimable */
 #define SLAB_TEMPORARY		SLAB_RECLAIM_ACCOUNT	/* Objects are short-lived */

commit 7e85ee0c1d15ca5f8bff0f514f158eba1742dd87
Author: Pekka Enberg <penberg@cs.helsinki.fi>
Date:   Fri Jun 12 14:03:06 2009 +0300

    slab,slub: don't enable interrupts during early boot
    
    As explained by Benjamin Herrenschmidt:
    
      Oh and btw, your patch alone doesn't fix powerpc, because it's missing
      a whole bunch of GFP_KERNEL's in the arch code... You would have to
      grep the entire kernel for things that check slab_is_available() and
      even then you'll be missing some.
    
      For example, slab_is_available() didn't always exist, and so in the
      early days on powerpc, we used a mem_init_done global that is set form
      mem_init() (not perfect but works in practice). And we still have code
      using that to do the test.
    
    Therefore, mask out __GFP_WAIT, __GFP_IO, and __GFP_FS in the slab allocators
    in early boot code to avoid enabling interrupts.
    
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 48803064cedf..219b8fb4651d 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -319,4 +319,6 @@ static inline void *kzalloc_node(size_t size, gfp_t flags, int node)
 	return kmalloc_node(size, flags | __GFP_ZERO, node);
 }
 
+void __init kmem_cache_init_late(void);
+
 #endif	/* _LINUX_SLAB_H */

commit d5cff635290aec9ad7e6ee546aa4fae895361cbb
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Thu Jun 11 13:22:40 2009 +0100

    kmemleak: Add the slab memory allocation/freeing hooks
    
    This patch adds the callbacks to kmemleak_(alloc|free) functions from
    the slab allocator. The patch also adds the SLAB_NOLEAKTRACE flag to
    avoid recursive calls to kmemleak when it allocates its own data
    structures.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Pekka Enberg <penberg@cs.helsinki.fi>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 24c5602bee99..48803064cedf 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -62,6 +62,8 @@
 # define SLAB_DEBUG_OBJECTS	0x00000000UL
 #endif
 
+#define SLAB_NOLEAKTRACE	0x00800000UL	/* Avoid kmemleak tracing */
+
 /* The following flags affect the page allocator grouping pages by mobility */
 #define SLAB_RECLAIM_ACCOUNT	0x00020000UL		/* Objects are reclaimable */
 #define SLAB_TEMPORARY		SLAB_RECLAIM_ACCOUNT	/* Objects are short-lived */

commit 3ef0e5ba467366125f04b423f4638baca54a4fc1
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Fri Feb 20 15:38:41 2009 -0800

    slab: introduce kzfree()
    
    kzfree() is a wrapper for kfree() that additionally zeroes the underlying
    memory before releasing it to the slab allocator.
    
    Currently there is code which memset()s the memory region of an object
    before releasing it back to the slab allocator to make sure
    security-sensitive data are really zeroed out after use.
    
    These callsites can then just use kzfree() which saves some code, makes
    users greppable and allows for a stupid destructor that isn't necessarily
    aware of the actual object size.
    
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Reviewed-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: Matt Mackall <mpm@selenic.com>
    Acked-by: Christoph Lameter <cl@linux-foundation.org>
    Cc: Nick Piggin <npiggin@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index f96d13c281e8..24c5602bee99 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -127,6 +127,7 @@ int kmem_ptr_validate(struct kmem_cache *cachep, const void *ptr);
 void * __must_check __krealloc(const void *, size_t, gfp_t);
 void * __must_check krealloc(const void *, size_t, gfp_t);
 void kfree(const void *);
+void kzfree(const void *);
 size_t ksize(const void *);
 
 /*

commit fd37617e69fb865348d012eb1413aef0141ae2de
Merge: 7b8f3b66d9d7 9f6c708e5cbf 249da1665828
Author: Pekka Enberg <penberg@cs.helsinki.fi>
Date:   Mon Dec 29 11:45:47 2008 +0200

    Merge branches 'topic/fixes', 'topic/cleanups' and 'topic/documentation' into for-linus

commit dfcd3610289132a762b7dc0eaf33998262cd9e20
Author: Pascal Terjan <pterjan@mandriva.com>
Date:   Tue Nov 25 15:08:19 2008 +0100

    slab: Fix comment on #endif
    
    This #endif in slab.h is described as closing the inner block while it's for
    the big CONFIG_NUMA one. That makes reading the code a bit harder.
    
    This trivial patch fixes the comment.
    
    Signed-off-by: Pascal Terjan <pterjan@mandriva.com>
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 000da12b5cf0..9d8ca14be3c4 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -285,7 +285,7 @@ extern void *__kmalloc_node_track_caller(size_t, gfp_t, int, void *);
 #define kmalloc_node_track_caller(size, flags, node) \
 	kmalloc_track_caller(size, flags)
 
-#endif /* DEBUG_SLAB */
+#endif /* CONFIG_NUMA */
 
 /*
  * Shortcuts

commit ce71e27c6fdc43c29f36d307b9100bde70c947fc
Author: Eduard - Gabriel Munteanu <eduard.munteanu@linux360.ro>
Date:   Tue Aug 19 20:43:25 2008 +0300

    SLUB: Replace __builtin_return_address(0) with _RET_IP_.
    
    This patch replaces __builtin_return_address(0) with _RET_IP_, since a
    previous patch moved _RET_IP_ and _THIS_IP_ to include/linux/kernel.h and
    they're widely available now. This makes for shorter and easier to read
    code.
    
    [penberg@cs.helsinki.fi: remove _RET_IP_ casts to void pointer]
    Signed-off-by: Eduard - Gabriel Munteanu <eduard.munteanu@linux360.ro>
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 000da12b5cf0..c97ed28559ec 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -253,9 +253,9 @@ static inline void *kmem_cache_alloc_node(struct kmem_cache *cachep,
  * request comes from.
  */
 #if defined(CONFIG_DEBUG_SLAB) || defined(CONFIG_SLUB)
-extern void *__kmalloc_track_caller(size_t, gfp_t, void*);
+extern void *__kmalloc_track_caller(size_t, gfp_t, unsigned long);
 #define kmalloc_track_caller(size, flags) \
-	__kmalloc_track_caller(size, flags, __builtin_return_address(0))
+	__kmalloc_track_caller(size, flags, _RET_IP_)
 #else
 #define kmalloc_track_caller(size, flags) \
 	__kmalloc(size, flags)
@@ -271,10 +271,10 @@ extern void *__kmalloc_track_caller(size_t, gfp_t, void*);
  * allocation request comes from.
  */
 #if defined(CONFIG_DEBUG_SLAB) || defined(CONFIG_SLUB)
-extern void *__kmalloc_node_track_caller(size_t, gfp_t, int, void *);
+extern void *__kmalloc_node_track_caller(size_t, gfp_t, int, unsigned long);
 #define kmalloc_node_track_caller(size, flags, node) \
 	__kmalloc_node_track_caller(size, flags, node, \
-			__builtin_return_address(0))
+			_RET_IP_)
 #else
 #define kmalloc_node_track_caller(size, flags, node) \
 	__kmalloc_node(size, flags, node)

commit d7de4c1dc3a2faca0bf05d9e342f885cb2696766
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Nov 13 20:40:12 2008 +0200

    slab: document SLAB_DESTROY_BY_RCU
    
    Explain this SLAB_DESTROY_BY_RCU thing...
    
    [hugh@veritas.com: add a pointer to comment in mm/slab.c]
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Jens Axboe <jens.axboe@oracle.com>
    Acked-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Acked-by: Christoph Lameter <cl@linux-foundation.org>
    Signed-off-by: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index ba965c84ae06..000da12b5cf0 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -23,6 +23,34 @@
 #define SLAB_CACHE_DMA		0x00004000UL	/* Use GFP_DMA memory */
 #define SLAB_STORE_USER		0x00010000UL	/* DEBUG: Store the last owner for bug hunting */
 #define SLAB_PANIC		0x00040000UL	/* Panic if kmem_cache_create() fails */
+/*
+ * SLAB_DESTROY_BY_RCU - **WARNING** READ THIS!
+ *
+ * This delays freeing the SLAB page by a grace period, it does _NOT_
+ * delay object freeing. This means that if you do kmem_cache_free()
+ * that memory location is free to be reused at any time. Thus it may
+ * be possible to see another object there in the same RCU grace period.
+ *
+ * This feature only ensures the memory location backing the object
+ * stays valid, the trick to using this is relying on an independent
+ * object validation pass. Something like:
+ *
+ *  rcu_read_lock()
+ * again:
+ *  obj = lockless_lookup(key);
+ *  if (obj) {
+ *    if (!try_get_ref(obj)) // might fail for free objects
+ *      goto again;
+ *
+ *    if (obj->key != key) { // not the object we expected
+ *      put_ref(obj);
+ *      goto again;
+ *    }
+ *  }
+ *  rcu_read_unlock();
+ *
+ * See also the comment on struct slab_rcu in mm/slab.c.
+ */
 #define SLAB_DESTROY_BY_RCU	0x00080000UL	/* Defer freeing slabs to RCU */
 #define SLAB_MEM_SPREAD		0x00100000UL	/* Spread some memory over cpuset */
 #define SLAB_TRACE		0x00200000UL	/* Trace allocations and frees */

commit 7b3c3a50a3e0ea46815150d420fa276ac254572b
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Mon Oct 6 02:42:17 2008 +0400

    proc: move /proc/slabinfo boilerplate to mm/slub.c, mm/slab.c
    
    Lose dummy ->write hook in case of SLUB, it's possible now.
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Acked-by: Pekka Enberg <penberg@cs.helsinki.fi>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 5ff9676c1e2c..ba965c84ae06 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -288,9 +288,4 @@ static inline void *kzalloc_node(size_t size, gfp_t flags, int node)
 	return kmalloc_node(size, flags | __GFP_ZERO, node);
 }
 
-#ifdef CONFIG_SLABINFO
-extern const struct seq_operations slabinfo_op;
-ssize_t slabinfo_write(struct file *, const char __user *, size_t, loff_t *);
-#endif
-
 #endif	/* _LINUX_SLAB_H */

commit 228428428138e231a155464239880201e5cc8b44
Merge: 78681ac08a61 6c3b8fc61890
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jul 26 20:17:56 2008 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-2.6
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-2.6:
      netns: fix ip_rt_frag_needed rt_is_expired
      netfilter: nf_conntrack_extend: avoid unnecessary "ct->ext" dereferences
      netfilter: fix double-free and use-after free
      netfilter: arptables in netns for real
      netfilter: ip{,6}tables_security: fix future section mismatch
      selinux: use nf_register_hooks()
      netfilter: ebtables: use nf_register_hooks()
      Revert "pkt_sched: sch_sfq: dump a real number of flows"
      qeth: use dev->ml_priv instead of dev->priv
      syncookies: Make sure ECN is disabled
      net: drop unused BUG_TRAP()
      net: convert BUG_TRAP to generic WARN_ON
      drivers/net: convert BUG_TRAP to generic WARN_ON

commit 93bc4e89c260d91576840c4881d1066d84ccd422
Author: Pekka Enberg <penberg@cs.helsinki.fi>
Date:   Sat Jul 26 17:49:33 2008 -0700

    netfilter: fix double-free and use-after free
    
    As suggested by Patrick McHardy, introduce a __krealloc() that doesn't
    free the original buffer to fix a double-free and use-after-free bug
    introduced by me in netfilter that uses RCU.
    
    Reported-by: Patrick McHardy <kaber@trash.net>
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Tested-by: Dieter Ries <clip2@gmx.de>
    Signed-off-by: Patrick McHardy <kaber@trash.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 9aa90a6f20e0..be6f1d40b66a 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -96,6 +96,7 @@ int kmem_ptr_validate(struct kmem_cache *cachep, const void *ptr);
 /*
  * Common kmalloc functions provided by all allocators
  */
+void * __must_check __krealloc(const void *, size_t, gfp_t);
 void * __must_check krealloc(const void *, size_t, gfp_t);
 void kfree(const void *);
 size_t ksize(const void *);

commit 51cc50685a4275c6a02653670af9f108a64e01cf
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Fri Jul 25 19:45:34 2008 -0700

    SL*B: drop kmem cache argument from constructor
    
    Kmem cache passed to constructor is only needed for constructors that are
    themselves multiplexeres.  Nobody uses this "feature", nor does anybody uses
    passed kmem cache in non-trivial way, so pass only pointer to object.
    
    Non-trivial places are:
            arch/powerpc/mm/init_64.c
            arch/powerpc/mm/hugetlbpage.c
    
    This is flag day, yes.
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Acked-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Acked-by: Christoph Lameter <cl@linux-foundation.org>
    Cc: Jon Tollefson <kniht@linux.vnet.ibm.com>
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Cc: Matt Mackall <mpm@selenic.com>
    [akpm@linux-foundation.org: fix arch/powerpc/mm/hugetlbpage.c]
    [akpm@linux-foundation.org: fix mm/slab.c]
    [akpm@linux-foundation.org: fix ubifs]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 41103910f8a2..9ff8e8499403 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -58,7 +58,7 @@ int slab_is_available(void);
 
 struct kmem_cache *kmem_cache_create(const char *, size_t, size_t,
 			unsigned long,
-			void (*)(struct kmem_cache *, void *));
+			void (*)(void *));
 void kmem_cache_destroy(struct kmem_cache *);
 int kmem_cache_shrink(struct kmem_cache *);
 void kmem_cache_free(struct kmem_cache *, void *);

commit 9ca908f47bc784c90e17a553ce33e756c73feac4
Author: Milton Miller <miltonm@bga.com>
Date:   Wed Jul 23 21:28:20 2008 -0700

    kcalloc: remove runtime division
    
    While in all cases in the kernel we know the size of the elements to be
    created, we don't always know the count of elements.  By commuting the size
    and count in the overflow check, the compiler can reduce the runtime division
    of size_t with a compare to a (unique) constant in these cases.
    
    Signed-off-by: Milton Miller <miltonm@bga.com>
    Cc: Takashi Iwai <tiwai@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 9aa90a6f20e0..41103910f8a2 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -180,7 +180,7 @@ size_t ksize(const void *);
  */
 static inline void *kcalloc(size_t n, size_t size, gfp_t flags)
 {
-	if (n != 0 && size > ULONG_MAX / n)
+	if (size != 0 && n > ULONG_MAX / size)
 		return NULL;
 	return __kmalloc(n * size, flags | __GFP_ZERO);
 }

commit cde53535991fbb5c34a1566f25955297c1487b8d
Author: Christoph Lameter <clameter@sgi.com>
Date:   Fri Jul 4 09:59:22 2008 -0700

    Christoph has moved
    
    Remove all clameter@sgi.com addresses from the kernel tree since they will
    become invalid on June 27th.  Change my maintainer email address for the
    slab allocators to cl@linux-foundation.org (which will be the new email
    address for the future).
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Christoph Lameter <cl@linux-foundation.org>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Cc: Matt Mackall <mpm@selenic.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index c2ad35016599..9aa90a6f20e0 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -1,7 +1,7 @@
 /*
  * Written by Mark Hemment, 1996 (markhe@nextd.demon.co.uk).
  *
- * (C) SGI 2006, Christoph Lameter <clameter@sgi.com>
+ * (C) SGI 2006, Christoph Lameter
  * 	Cleaned up and restructured to ease the addition of alternative
  * 	implementations of SLAB allocators.
  */

commit 979b0fea2d9ae5d57237a368d571cbc84655fba6
Author: Jeff Layton <jlayton@redhat.com>
Date:   Thu Jun 5 22:47:00 2008 -0700

    vm: add kzalloc_node() inline
    
    To get zeroed out memory from a particular NUMA node.  To be used by
    sunrpc.
    
    Signed-off-by: Jeff Layton <jlayton@redhat.com>
    Cc: Christoph Lameter <clameter@sgi.com>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 805ed4b92f9a..c2ad35016599 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -276,6 +276,17 @@ static inline void *kzalloc(size_t size, gfp_t flags)
 	return kmalloc(size, flags | __GFP_ZERO);
 }
 
+/**
+ * kzalloc_node - allocate zeroed memory from a particular memory node.
+ * @size: how many bytes of memory are required.
+ * @flags: the type of memory to allocate (see kmalloc).
+ * @node: memory node from which to allocate
+ */
+static inline void *kzalloc_node(size_t size, gfp_t flags, int node)
+{
+	return kmalloc_node(size, flags | __GFP_ZERO, node);
+}
+
 #ifdef CONFIG_SLABINFO
 extern const struct seq_operations slabinfo_op;
 ssize_t slabinfo_write(struct file *, const char __user *, size_t, loff_t *);

commit 735643ee6cc5249bfac07fcad0946a5e7aff4423
Author: Robert P. J. Day <rpjday@crashcourse.ca>
Date:   Wed Apr 30 00:55:12 2008 -0700

    Remove "#ifdef __KERNEL__" checks from unexported headers
    
    Remove the "#ifdef __KERNEL__" tests from unexported header files in
    linux/include whose entire contents are wrapped in that preprocessor
    test.
    
    Signed-off-by: Robert P. J. Day <rpjday@crashcourse.ca>
    Cc: David Woodhouse <dwmw2@infradead.org>
    Cc: Sam Ravnborg <sam@ravnborg.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 6d03c954f641..805ed4b92f9a 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -9,8 +9,6 @@
 #ifndef _LINUX_SLAB_H
 #define	_LINUX_SLAB_H
 
-#ifdef __KERNEL__
-
 #include <linux/gfp.h>
 #include <linux/types.h>
 
@@ -283,5 +281,4 @@ extern const struct seq_operations slabinfo_op;
 ssize_t slabinfo_write(struct file *, const char __user *, size_t, loff_t *);
 #endif
 
-#endif	/* __KERNEL__ */
 #endif	/* _LINUX_SLAB_H */

commit 30327acf7846c5eb97c8e31c78317a2918d3e515
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Apr 30 00:54:59 2008 -0700

    slab: add a flag to prevent debug_free checks on a kmem_cache
    
    This is a preperatory patch for the debugobjects infrastructure.  The flag
    prevents debug_free checks on kmem_caches.  This is necessary to avoid
    resursive calls into a debug mechanism which uses a kmem_cache itself.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index f62caaad94e0..6d03c954f641 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -29,6 +29,13 @@
 #define SLAB_MEM_SPREAD		0x00100000UL	/* Spread some memory over cpuset */
 #define SLAB_TRACE		0x00200000UL	/* Trace allocations and frees */
 
+/* Flag to prevent checks on free */
+#ifdef CONFIG_DEBUG_OBJECTS
+# define SLAB_DEBUG_OBJECTS	0x00400000UL
+#else
+# define SLAB_DEBUG_OBJECTS	0x00000000UL
+#endif
+
 /* The following flags affect the page allocator grouping pages by mobility */
 #define SLAB_RECLAIM_ACCOUNT	0x00020000UL		/* Objects are reclaimable */
 #define SLAB_TEMPORARY		SLAB_RECLAIM_ACCOUNT	/* Objects are short-lived */

commit 158a962422e4a54dc256b6a9b9562f3d30d34d9c
Author: Linus Torvalds <torvalds@woody.linux-foundation.org>
Date:   Wed Jan 2 13:04:48 2008 -0800

    Unify /proc/slabinfo configuration
    
    Both SLUB and SLAB really did almost exactly the same thing for
    /proc/slabinfo setup, using duplicate code and per-allocator #ifdef's.
    
    This just creates a common CONFIG_SLABINFO that is enabled by both SLUB
    and SLAB, and shares all the setup code.  Maybe SLOB will want this some
    day too.
    
    Reviewed-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index f3a8eecd99f3..f62caaad94e0 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -271,5 +271,10 @@ static inline void *kzalloc(size_t size, gfp_t flags)
 	return kmalloc(size, flags | __GFP_ZERO);
 }
 
+#ifdef CONFIG_SLABINFO
+extern const struct seq_operations slabinfo_op;
+ssize_t slabinfo_write(struct file *, const char __user *, size_t, loff_t *);
+#endif
+
 #endif	/* __KERNEL__ */
 #endif	/* _LINUX_SLAB_H */

commit 4ba9b9d0ba0a49d91fa6417c7510ee36f48cf957
Author: Christoph Lameter <clameter@sgi.com>
Date:   Tue Oct 16 23:25:51 2007 -0700

    Slab API: remove useless ctor parameter and reorder parameters
    
    Slab constructors currently have a flags parameter that is never used.  And
    the order of the arguments is opposite to other slab functions.  The object
    pointer is placed before the kmem_cache pointer.
    
    Convert
    
            ctor(void *object, struct kmem_cache *s, unsigned long flags)
    
    to
    
            ctor(struct kmem_cache *s, void *object)
    
    throughout the kernel
    
    [akpm@linux-foundation.org: coupla fixes]
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 3a5bad3ad126..f3a8eecd99f3 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -53,7 +53,7 @@ int slab_is_available(void);
 
 struct kmem_cache *kmem_cache_create(const char *, size_t, size_t,
 			unsigned long,
-			void (*)(void *, struct kmem_cache *, unsigned long));
+			void (*)(struct kmem_cache *, void *));
 void kmem_cache_destroy(struct kmem_cache *);
 int kmem_cache_shrink(struct kmem_cache *);
 void kmem_cache_free(struct kmem_cache *, void *);

commit e12ba74d8ff3e2f73a583500d7095e406df4d093
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Tue Oct 16 01:25:52 2007 -0700

    Group short-lived and reclaimable kernel allocations
    
    This patch marks a number of allocations that are either short-lived such as
    network buffers or are reclaimable such as inode allocations.  When something
    like updatedb is called, long-lived and unmovable kernel allocations tend to
    be spread throughout the address space which increases fragmentation.
    
    This patch groups these allocations together as much as possible by adding a
    new MIGRATE_TYPE.  The MIGRATE_RECLAIMABLE type is for allocations that can be
    reclaimed on demand, but not moved.  i.e.  they can be migrated by deleting
    them and re-reading the information from elsewhere.
    
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Cc: Andy Whitcroft <apw@shadowen.org>
    Cc: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index d859354b9e51..3a5bad3ad126 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -24,12 +24,14 @@
 #define SLAB_HWCACHE_ALIGN	0x00002000UL	/* Align objs on cache lines */
 #define SLAB_CACHE_DMA		0x00004000UL	/* Use GFP_DMA memory */
 #define SLAB_STORE_USER		0x00010000UL	/* DEBUG: Store the last owner for bug hunting */
-#define SLAB_RECLAIM_ACCOUNT	0x00020000UL	/* Objects are reclaimable */
 #define SLAB_PANIC		0x00040000UL	/* Panic if kmem_cache_create() fails */
 #define SLAB_DESTROY_BY_RCU	0x00080000UL	/* Defer freeing slabs to RCU */
 #define SLAB_MEM_SPREAD		0x00100000UL	/* Spread some memory over cpuset */
 #define SLAB_TRACE		0x00200000UL	/* Trace allocations and frees */
 
+/* The following flags affect the page allocator grouping pages by mobility */
+#define SLAB_RECLAIM_ACCOUNT	0x00020000UL		/* Objects are reclaimable */
+#define SLAB_TEMPORARY		SLAB_RECLAIM_ACCOUNT	/* Objects are short-lived */
 /*
  * ZERO_SIZE_PTR will be returned for zero sized kmalloc requests.
  *

commit 1d4ec7b1d6f130818f9b62dea3411d9ee2ff6ff6
Author: Roland Dreier <rdreier@cisco.com>
Date:   Fri Jul 20 12:13:20 2007 -0700

    Fix ZERO_OR_NULL_PTR(ZERO_SIZE_PTR)
    
    The comparison with ZERO_SIZE_PTR in ZERO_OR_NULL_PTR() needs to be <=
    (not just <) so that ZERO_OR_NULL_PTR(ZERO_SIZE_PTR) is 1.
    
    Signed-off-by: Roland Dreier <rolandd@cisco.com>
    [ Duh!  - Linus ]
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 7d0ecc1659f0..d859354b9e51 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -40,7 +40,7 @@
  */
 #define ZERO_SIZE_PTR ((void *)16)
 
-#define ZERO_OR_NULL_PTR(x) ((unsigned long)(x) < \
+#define ZERO_OR_NULL_PTR(x) ((unsigned long)(x) <= \
 				(unsigned long)ZERO_SIZE_PTR)
 
 /*

commit 20c2df83d25c6a95affe6157a4c9cac4cf5ffaac
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Fri Jul 20 10:11:58 2007 +0900

    mm: Remove slab destructors from kmem_cache_create().
    
    Slab destructors were no longer supported after Christoph's
    c59def9f222d44bb7e2f0a559f2906191a0862d7 change. They've been
    BUGs for both slab and slub, and slob never supported them
    either.
    
    This rips out support for the dtor pointer from kmem_cache_create()
    completely and fixes up every single callsite in the kernel (there were
    about 224, not including the slab allocator definitions themselves,
    or the documentation references).
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 0e1d0daef6a2..7d0ecc1659f0 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -51,7 +51,6 @@ int slab_is_available(void);
 
 struct kmem_cache *kmem_cache_create(const char *, size_t, size_t,
 			unsigned long,
-			void (*)(void *, struct kmem_cache *, unsigned long),
 			void (*)(void *, struct kmem_cache *, unsigned long));
 void kmem_cache_destroy(struct kmem_cache *);
 int kmem_cache_shrink(struct kmem_cache *);
@@ -70,7 +69,7 @@ int kmem_ptr_validate(struct kmem_cache *cachep, const void *ptr);
  */
 #define KMEM_CACHE(__struct, __flags) kmem_cache_create(#__struct,\
 		sizeof(struct __struct), __alignof__(struct __struct),\
-		(__flags), NULL, NULL)
+		(__flags), NULL)
 
 /*
  * The largest kmalloc size supported by the slab allocators is

commit 81cda6626178cd55297831296ba8ecedbfd8b52d
Author: Christoph Lameter <clameter@sgi.com>
Date:   Tue Jul 17 04:03:29 2007 -0700

    Slab allocators: Cleanup zeroing allocations
    
    It becomes now easy to support the zeroing allocs with generic inline
    functions in slab.h.  Provide inline definitions to allow the continued use of
    kzalloc, kmem_cache_zalloc etc but remove other definitions of zeroing
    functions from the slab allocators and util.c.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 0289ec89300a..0e1d0daef6a2 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -55,7 +55,6 @@ struct kmem_cache *kmem_cache_create(const char *, size_t, size_t,
 			void (*)(void *, struct kmem_cache *, unsigned long));
 void kmem_cache_destroy(struct kmem_cache *);
 int kmem_cache_shrink(struct kmem_cache *);
-void *kmem_cache_zalloc(struct kmem_cache *, gfp_t);
 void kmem_cache_free(struct kmem_cache *, void *);
 unsigned int kmem_cache_size(struct kmem_cache *);
 const char *kmem_cache_name(struct kmem_cache *);
@@ -91,11 +90,37 @@ int kmem_ptr_validate(struct kmem_cache *cachep, const void *ptr);
 /*
  * Common kmalloc functions provided by all allocators
  */
-void *__kzalloc(size_t, gfp_t);
 void * __must_check krealloc(const void *, size_t, gfp_t);
 void kfree(const void *);
 size_t ksize(const void *);
 
+/*
+ * Allocator specific definitions. These are mainly used to establish optimized
+ * ways to convert kmalloc() calls to kmem_cache_alloc() invocations by
+ * selecting the appropriate general cache at compile time.
+ *
+ * Allocators must define at least:
+ *
+ *	kmem_cache_alloc()
+ *	__kmalloc()
+ *	kmalloc()
+ *
+ * Those wishing to support NUMA must also define:
+ *
+ *	kmem_cache_alloc_node()
+ *	kmalloc_node()
+ *
+ * See each allocator definition file for additional comments and
+ * implementation notes.
+ */
+#ifdef CONFIG_SLUB
+#include <linux/slub_def.h>
+#elif defined(CONFIG_SLOB)
+#include <linux/slob_def.h>
+#else
+#include <linux/slab_def.h>
+#endif
+
 /**
  * kcalloc - allocate memory for an array. The memory is set to zero.
  * @n: number of elements.
@@ -151,37 +176,9 @@ static inline void *kcalloc(size_t n, size_t size, gfp_t flags)
 {
 	if (n != 0 && size > ULONG_MAX / n)
 		return NULL;
-	return __kzalloc(n * size, flags);
+	return __kmalloc(n * size, flags | __GFP_ZERO);
 }
 
-/*
- * Allocator specific definitions. These are mainly used to establish optimized
- * ways to convert kmalloc() calls to kmem_cache_alloc() invocations by
- * selecting the appropriate general cache at compile time.
- *
- * Allocators must define at least:
- *
- *	kmem_cache_alloc()
- *	__kmalloc()
- *	kmalloc()
- *	kzalloc()
- *
- * Those wishing to support NUMA must also define:
- *
- *	kmem_cache_alloc_node()
- *	kmalloc_node()
- *
- * See each allocator definition file for additional comments and
- * implementation notes.
- */
-#ifdef CONFIG_SLUB
-#include <linux/slub_def.h>
-#elif defined(CONFIG_SLOB)
-#include <linux/slob_def.h>
-#else
-#include <linux/slab_def.h>
-#endif
-
 #if !defined(CONFIG_NUMA) && !defined(CONFIG_SLOB)
 /**
  * kmalloc_node - allocate memory from a specific node
@@ -255,5 +252,23 @@ extern void *__kmalloc_node_track_caller(size_t, gfp_t, int, void *);
 
 #endif /* DEBUG_SLAB */
 
+/*
+ * Shortcuts
+ */
+static inline void *kmem_cache_zalloc(struct kmem_cache *k, gfp_t flags)
+{
+	return kmem_cache_alloc(k, flags | __GFP_ZERO);
+}
+
+/**
+ * kzalloc - allocate memory. The memory is set to zero.
+ * @size: how many bytes of memory are required.
+ * @flags: the type of memory to allocate (see kmalloc).
+ */
+static inline void *kzalloc(size_t size, gfp_t flags)
+{
+	return kmalloc(size, flags | __GFP_ZERO);
+}
+
 #endif	/* __KERNEL__ */
 #endif	/* _LINUX_SLAB_H */

commit 6cb8f91320d3e720351c21741da795fed580b21b
Author: Christoph Lameter <clameter@sgi.com>
Date:   Tue Jul 17 04:03:22 2007 -0700

    Slab allocators: consistent ZERO_SIZE_PTR support and NULL result semantics
    
    Define ZERO_OR_NULL_PTR macro to be able to remove the checks from the
    allocators.  Move ZERO_SIZE_PTR related stuff into slab.h.
    
    Make ZERO_SIZE_PTR work for all slab allocators and get rid of the
    WARN_ON_ONCE(size == 0) that is still remaining in SLAB.
    
    Make slub return NULL like the other allocators if a too large memory segment
    is requested via __kmalloc.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Acked-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 27402fea9b79..0289ec89300a 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -30,6 +30,19 @@
 #define SLAB_MEM_SPREAD		0x00100000UL	/* Spread some memory over cpuset */
 #define SLAB_TRACE		0x00200000UL	/* Trace allocations and frees */
 
+/*
+ * ZERO_SIZE_PTR will be returned for zero sized kmalloc requests.
+ *
+ * Dereferencing ZERO_SIZE_PTR will lead to a distinct access fault.
+ *
+ * ZERO_SIZE_PTR can be passed to kfree though in the same way that NULL can.
+ * Both make kfree a no-op.
+ */
+#define ZERO_SIZE_PTR ((void *)16)
+
+#define ZERO_OR_NULL_PTR(x) ((unsigned long)(x) < \
+				(unsigned long)ZERO_SIZE_PTR)
+
 /*
  * struct kmem_cache related prototypes
  */

commit 6193a2ff180920f84ee06977165ebf32431fc2d2
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Sun Jul 15 23:38:22 2007 -0700

    slob: initial NUMA support
    
    This adds preliminary NUMA support to SLOB, primarily aimed at systems with
    small nodes (tested all the way down to a 128kB SRAM block), whether
    asymmetric or otherwise.
    
    We follow the same conventions as SLAB/SLUB, preferring current node
    placement for new pages, or with explicit placement, if a node has been
    specified.  Presently on UP NUMA this has the side-effect of preferring
    node#0 allocations (since numa_node_id() == 0, though this could be
    reworked if we could hand off a pfn to determine node placement), so
    single-CPU NUMA systems will want to place smaller nodes further out in
    terms of node id.  Once a page has been bound to a node (via explicit node
    id typing), we only do block allocations from partial free pages that have
    a matching node id in the page flags.
    
    The current implementation does have some scalability problems, in that all
    partial free pages are tracked in the global freelist (with contention due
    to the single spinlock).  However, these are things that are being reworked
    for SMP scalability first, while things like per-node freelists can easily
    be built on top of this sort of functionality once it's been added.
    
    More background can be found in:
    
            http://marc.info/?l=linux-mm&m=118117916022379&w=2
            http://marc.info/?l=linux-mm&m=118170446306199&w=2
            http://marc.info/?l=linux-mm&m=118187859420048&w=2
    
    and subsequent threads.
    
    Acked-by: Christoph Lameter <clameter@sgi.com>
    Acked-by: Matt Mackall <mpm@selenic.com>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>
    Acked-by: Nick Piggin <nickpiggin@yahoo.com.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index cd6ab658553f..27402fea9b79 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -42,7 +42,6 @@ struct kmem_cache *kmem_cache_create(const char *, size_t, size_t,
 			void (*)(void *, struct kmem_cache *, unsigned long));
 void kmem_cache_destroy(struct kmem_cache *);
 int kmem_cache_shrink(struct kmem_cache *);
-void *kmem_cache_alloc(struct kmem_cache *, gfp_t);
 void *kmem_cache_zalloc(struct kmem_cache *, gfp_t);
 void kmem_cache_free(struct kmem_cache *, void *);
 unsigned int kmem_cache_size(struct kmem_cache *);
@@ -61,16 +60,6 @@ int kmem_ptr_validate(struct kmem_cache *cachep, const void *ptr);
 		sizeof(struct __struct), __alignof__(struct __struct),\
 		(__flags), NULL, NULL)
 
-#ifdef CONFIG_NUMA
-extern void *kmem_cache_alloc_node(struct kmem_cache *, gfp_t flags, int node);
-#else
-static inline void *kmem_cache_alloc_node(struct kmem_cache *cachep,
-					gfp_t flags, int node)
-{
-	return kmem_cache_alloc(cachep, flags);
-}
-#endif
-
 /*
  * The largest kmalloc size supported by the slab allocators is
  * 32 megabyte (2^25) or the maximum allocatable page order if that is
@@ -89,7 +78,6 @@ static inline void *kmem_cache_alloc_node(struct kmem_cache *cachep,
 /*
  * Common kmalloc functions provided by all allocators
  */
-void *__kmalloc(size_t, gfp_t);
 void *__kzalloc(size_t, gfp_t);
 void * __must_check krealloc(const void *, size_t, gfp_t);
 void kfree(const void *);
@@ -100,40 +88,6 @@ size_t ksize(const void *);
  * @n: number of elements.
  * @size: element size.
  * @flags: the type of memory to allocate.
- */
-static inline void *kcalloc(size_t n, size_t size, gfp_t flags)
-{
-	if (n != 0 && size > ULONG_MAX / n)
-		return NULL;
-	return __kzalloc(n * size, flags);
-}
-
-/*
- * Allocator specific definitions. These are mainly used to establish optimized
- * ways to convert kmalloc() calls to kmem_cache_alloc() invocations by selecting
- * the appropriate general cache at compile time.
- */
-
-#if defined(CONFIG_SLAB) || defined(CONFIG_SLUB)
-#ifdef CONFIG_SLUB
-#include <linux/slub_def.h>
-#else
-#include <linux/slab_def.h>
-#endif /* !CONFIG_SLUB */
-#else
-
-/*
- * Fallback definitions for an allocator not wanting to provide
- * its own optimized kmalloc definitions (like SLOB).
- */
-
-/**
- * kmalloc - allocate memory
- * @size: how many bytes of memory are required.
- * @flags: the type of memory to allocate.
- *
- * kmalloc is the normal method of allocating memory
- * in the kernel.
  *
  * The @flags argument may be one of:
  *
@@ -141,7 +95,7 @@ static inline void *kcalloc(size_t n, size_t size, gfp_t flags)
  *
  * %GFP_KERNEL - Allocate normal kernel ram.  May sleep.
  *
- * %GFP_ATOMIC - Allocation will not sleep.
+ * %GFP_ATOMIC - Allocation will not sleep.  May use emergency pools.
  *   For example, use this inside interrupt handlers.
  *
  * %GFP_HIGHUSER - Allocate pages from high memory.
@@ -150,18 +104,22 @@ static inline void *kcalloc(size_t n, size_t size, gfp_t flags)
  *
  * %GFP_NOFS - Do not make any fs calls while trying to get memory.
  *
+ * %GFP_NOWAIT - Allocation will not sleep.
+ *
+ * %GFP_THISNODE - Allocate node-local memory only.
+ *
+ * %GFP_DMA - Allocation suitable for DMA.
+ *   Should only be used for kmalloc() caches. Otherwise, use a
+ *   slab created with SLAB_DMA.
+ *
  * Also it is possible to set different flags by OR'ing
  * in one or more of the following additional @flags:
  *
  * %__GFP_COLD - Request cache-cold pages instead of
  *   trying to return cache-warm pages.
  *
- * %__GFP_DMA - Request memory from the DMA-capable zone.
- *
  * %__GFP_HIGH - This allocation has high priority and may use emergency pools.
  *
- * %__GFP_HIGHMEM - Allocated memory may be from highmem.
- *
  * %__GFP_NOFAIL - Indicate that this allocation is in no way allowed to fail
  *   (think twice before using).
  *
@@ -171,24 +129,57 @@ static inline void *kcalloc(size_t n, size_t size, gfp_t flags)
  * %__GFP_NOWARN - If allocation fails, don't issue any warnings.
  *
  * %__GFP_REPEAT - If allocation fails initially, try once more before failing.
+ *
+ * There are other flags available as well, but these are not intended
+ * for general use, and so are not documented here. For a full list of
+ * potential flags, always refer to linux/gfp.h.
  */
-static inline void *kmalloc(size_t size, gfp_t flags)
+static inline void *kcalloc(size_t n, size_t size, gfp_t flags)
 {
-	return __kmalloc(size, flags);
+	if (n != 0 && size > ULONG_MAX / n)
+		return NULL;
+	return __kzalloc(n * size, flags);
 }
 
-/**
- * kzalloc - allocate memory. The memory is set to zero.
- * @size: how many bytes of memory are required.
- * @flags: the type of memory to allocate (see kmalloc).
+/*
+ * Allocator specific definitions. These are mainly used to establish optimized
+ * ways to convert kmalloc() calls to kmem_cache_alloc() invocations by
+ * selecting the appropriate general cache at compile time.
+ *
+ * Allocators must define at least:
+ *
+ *	kmem_cache_alloc()
+ *	__kmalloc()
+ *	kmalloc()
+ *	kzalloc()
+ *
+ * Those wishing to support NUMA must also define:
+ *
+ *	kmem_cache_alloc_node()
+ *	kmalloc_node()
+ *
+ * See each allocator definition file for additional comments and
+ * implementation notes.
  */
-static inline void *kzalloc(size_t size, gfp_t flags)
-{
-	return __kzalloc(size, flags);
-}
+#ifdef CONFIG_SLUB
+#include <linux/slub_def.h>
+#elif defined(CONFIG_SLOB)
+#include <linux/slob_def.h>
+#else
+#include <linux/slab_def.h>
 #endif
 
-#ifndef CONFIG_NUMA
+#if !defined(CONFIG_NUMA) && !defined(CONFIG_SLOB)
+/**
+ * kmalloc_node - allocate memory from a specific node
+ * @size: how many bytes of memory are required.
+ * @flags: the type of memory to allocate (see kcalloc).
+ * @node: node to allocate from.
+ *
+ * kmalloc() for non-local nodes, used to allocate from a specific node
+ * if available. Equivalent to kmalloc() in the non-NUMA single-node
+ * case.
+ */
 static inline void *kmalloc_node(size_t size, gfp_t flags, int node)
 {
 	return kmalloc(size, flags);
@@ -198,7 +189,15 @@ static inline void *__kmalloc_node(size_t size, gfp_t flags, int node)
 {
 	return __kmalloc(size, flags);
 }
-#endif /* !CONFIG_NUMA */
+
+void *kmem_cache_alloc(struct kmem_cache *, gfp_t);
+
+static inline void *kmem_cache_alloc_node(struct kmem_cache *cachep,
+					gfp_t flags, int node)
+{
+	return kmem_cache_alloc(cachep, flags);
+}
+#endif /* !CONFIG_NUMA && !CONFIG_SLOB */
 
 /*
  * kmalloc_track_caller is a special version of kmalloc that records the
@@ -245,4 +244,3 @@ extern void *__kmalloc_node_track_caller(size_t, gfp_t, int, void *);
 
 #endif	/* __KERNEL__ */
 #endif	/* _LINUX_SLAB_H */
-

commit 698827fa9f45019df1609bb686bc51c94e127fbc
Author: Robert P. J. Day <rpjday@mindspring.com>
Date:   Sun Jul 15 23:38:06 2007 -0700

    Remove the deprecated "kmem_cache_t" typedef from slab.h.
    
    Given that there is no remaining usage of the deprecated kmem_cache_t
    typedef anywhere in the tree, remove that typedef.
    
    Signed-off-by: Robert P. J. Day <rpjday@mindspring.com>
    Acked-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Acked-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index cebcd3833c76..cd6ab658553f 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -14,8 +14,6 @@
 #include <linux/gfp.h>
 #include <linux/types.h>
 
-typedef struct kmem_cache kmem_cache_t __deprecated;
-
 /*
  * Flags to pass to kmem_cache_create().
  * The ones marked DEBUG are only valid if CONFIG_SLAB_DEBUG is set.

commit debee0768ea1978c6efba03206a414685e4a9ed1
Author: Christoph Lameter <clameter@sgi.com>
Date:   Sat Jun 23 17:16:43 2007 -0700

    slab allocators: MAX_ORDER one off fix
    
    MAX_ORDER is the first order that is not possible.
    
    Use MAX_ORDER - 1 to calculate the larges possible object size in slab.h
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index a015236cc572..cebcd3833c76 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -82,8 +82,8 @@ static inline void *kmem_cache_alloc_node(struct kmem_cache *cachep,
  * to do various tricks to work around compiler limitations in order to
  * ensure proper constant folding.
  */
-#define KMALLOC_SHIFT_HIGH	((MAX_ORDER + PAGE_SHIFT) <= 25 ? \
-				(MAX_ORDER + PAGE_SHIFT) : 25)
+#define KMALLOC_SHIFT_HIGH	((MAX_ORDER + PAGE_SHIFT - 1) <= 25 ? \
+				(MAX_ORDER + PAGE_SHIFT - 1) : 25)
 
 #define KMALLOC_MAX_SIZE	(1UL << KMALLOC_SHIFT_HIGH)
 #define KMALLOC_MAX_ORDER	(KMALLOC_SHIFT_HIGH - PAGE_SHIFT)

commit 0aa817f078b655d0ae36669169d73a5c8a388016
Author: Christoph Lameter <clameter@sgi.com>
Date:   Wed May 16 22:11:01 2007 -0700

    Slab allocators: define common size limitations
    
    Currently we have a maze of configuration variables that determine the
    maximum slab size.  Worst of all it seems to vary between SLAB and SLUB.
    
    So define a common maximum size for kmalloc.  For conveniences sake we use
    the maximum size ever supported which is 32 MB.  We limit the maximum size
    to a lower limit if MAX_ORDER does not allow such large allocations.
    
    For many architectures this patch will have the effect of adding large
    kmalloc sizes.  x86_64 adds 5 new kmalloc sizes.  So a small amount of
    memory will be needed for these caches (contemporary SLAB has dynamically
    sizeable node and cpu structure so the waste is less than in the past)
    
    Most architectures will then be able to allocate object with sizes up to
    MAX_ORDER.  We have had repeated breakage (in fact whenever we doubled the
    number of supported processors) on IA64 because one or the other struct
    grew beyond what the slab allocators supported.  This will avoid future
    issues and f.e.  avoid fixes for 2k and 4k cpu support.
    
    CONFIG_LARGE_ALLOCS is no longer necessary so drop it.
    
    It fixes sparc64 with SLAB.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: "David S. Miller" <davem@davemloft.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 6fb2ae214152..a015236cc572 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -73,6 +73,21 @@ static inline void *kmem_cache_alloc_node(struct kmem_cache *cachep,
 }
 #endif
 
+/*
+ * The largest kmalloc size supported by the slab allocators is
+ * 32 megabyte (2^25) or the maximum allocatable page order if that is
+ * less than 32 MB.
+ *
+ * WARNING: Its not easy to increase this value since the allocators have
+ * to do various tricks to work around compiler limitations in order to
+ * ensure proper constant folding.
+ */
+#define KMALLOC_SHIFT_HIGH	((MAX_ORDER + PAGE_SHIFT) <= 25 ? \
+				(MAX_ORDER + PAGE_SHIFT) : 25)
+
+#define KMALLOC_MAX_SIZE	(1UL << KMALLOC_SHIFT_HIGH)
+#define KMALLOC_MAX_ORDER	(KMALLOC_SHIFT_HIGH - PAGE_SHIFT)
+
 /*
  * Common kmalloc functions provided by all allocators
  */

commit a35afb830f8d71ec211531aeb9a621b09a2efb39
Author: Christoph Lameter <clameter@sgi.com>
Date:   Wed May 16 22:10:57 2007 -0700

    Remove SLAB_CTOR_CONSTRUCTOR
    
    SLAB_CTOR_CONSTRUCTOR is always specified. No point in checking it.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Jens Axboe <jens.axboe@oracle.com>
    Cc: Steven French <sfrench@us.ibm.com>
    Cc: Michael Halcrow <mhalcrow@us.ibm.com>
    Cc: OGAWA Hirofumi <hirofumi@mail.parknet.co.jp>
    Cc: Miklos Szeredi <miklos@szeredi.hu>
    Cc: Steven Whitehouse <swhiteho@redhat.com>
    Cc: Roman Zippel <zippel@linux-m68k.org>
    Cc: David Woodhouse <dwmw2@infradead.org>
    Cc: Dave Kleikamp <shaggy@austin.ibm.com>
    Cc: Trond Myklebust <trond.myklebust@fys.uio.no>
    Cc: "J. Bruce Fields" <bfields@fieldses.org>
    Cc: Anton Altaparmakov <aia21@cantab.net>
    Cc: Mark Fasheh <mark.fasheh@oracle.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Jan Kara <jack@ucw.cz>
    Cc: David Chinner <dgc@sgi.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 5dbc0bae26e3..6fb2ae214152 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -32,9 +32,6 @@ typedef struct kmem_cache kmem_cache_t __deprecated;
 #define SLAB_MEM_SPREAD		0x00100000UL	/* Spread some memory over cpuset */
 #define SLAB_TRACE		0x00200000UL	/* Trace allocations and frees */
 
-/* Flags passed to a constructor functions */
-#define SLAB_CTOR_CONSTRUCTOR	0x001UL		/* If not set, then deconstructor */
-
 /*
  * struct kmem_cache related prototypes
  */

commit 3ca12ee549f7837b8a685dddc9515f9fc28434ee
Author: Christoph Lameter <clameter@sgi.com>
Date:   Wed May 16 22:10:52 2007 -0700

    SLAB: Move two remaining SLAB specific definitions to slab_def.h
    
    Two definitions remained in slab.h that are particular to the SLAB allocator.
    Move to slab_def.h
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 71829efc40ba..5dbc0bae26e3 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -233,9 +233,6 @@ extern void *__kmalloc_node_track_caller(size_t, gfp_t, int, void *);
 
 #endif /* DEBUG_SLAB */
 
-extern const struct seq_operations slabinfo_op;
-ssize_t slabinfo_write(struct file *, const char __user *, size_t, loff_t *);
-
 #endif	/* __KERNEL__ */
 #endif	/* _LINUX_SLAB_H */
 

commit 4f104934591ed98534b3a4c3d17d972b790e9c42
Author: Christoph Lameter <clameter@sgi.com>
Date:   Sun May 6 14:50:17 2007 -0700

    slab allocators: Remove SLAB_CTOR_ATOMIC
    
    SLAB_CTOR atomic is never used which is no surprise since I cannot imagine
    that one would want to do something serious in a constructor or destructor.
     In particular given that the slab allocators run with interrupts disabled.
     Actions in constructors and destructors are by their nature very limited
    and usually do not go beyond initializing variables and list operations.
    
    (The i386 pgd ctor and dtors do take a spinlock in constructor and
    destructor.....  I think that is the furthest we go at this point.)
    
    There is no flag passed to the destructor so removing SLAB_CTOR_ATOMIC also
    establishes a certain symmetry.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 1ffe0a959cd4..71829efc40ba 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -34,7 +34,6 @@ typedef struct kmem_cache kmem_cache_t __deprecated;
 
 /* Flags passed to a constructor functions */
 #define SLAB_CTOR_CONSTRUCTOR	0x001UL		/* If not set, then deconstructor */
-#define SLAB_CTOR_ATOMIC	0x002UL		/* Tell constructor it can't sleep */
 
 /*
  * struct kmem_cache related prototypes

commit 50953fe9e00ebbeffa032a565ab2f08312d51a87
Author: Christoph Lameter <clameter@sgi.com>
Date:   Sun May 6 14:50:16 2007 -0700

    slab allocators: Remove SLAB_DEBUG_INITIAL flag
    
    I have never seen a use of SLAB_DEBUG_INITIAL.  It is only supported by
    SLAB.
    
    I think its purpose was to have a callback after an object has been freed
    to verify that the state is the constructor state again?  The callback is
    performed before each freeing of an object.
    
    I would think that it is much easier to check the object state manually
    before the free.  That also places the check near the code object
    manipulation of the object.
    
    Also the SLAB_DEBUG_INITIAL callback is only performed if the kernel was
    compiled with SLAB debugging on.  If there would be code in a constructor
    handling SLAB_DEBUG_INITIAL then it would have to be conditional on
    SLAB_DEBUG otherwise it would just be dead code.  But there is no such code
    in the kernel.  I think SLUB_DEBUG_INITIAL is too problematic to make real
    use of, difficult to understand and there are easier ways to accomplish the
    same effect (i.e.  add debug code before kfree).
    
    There is a related flag SLAB_CTOR_VERIFY that is frequently checked to be
    clear in fs inode caches.  Remove the pointless checks (they would even be
    pointless without removeal of SLAB_DEBUG_INITIAL) from the fs constructors.
    
    This is the last slab flag that SLUB did not support.  Remove the check for
    unimplemented flags from SLUB.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index e14b4c338b89..1ffe0a959cd4 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -21,7 +21,6 @@ typedef struct kmem_cache kmem_cache_t __deprecated;
  * The ones marked DEBUG are only valid if CONFIG_SLAB_DEBUG is set.
  */
 #define SLAB_DEBUG_FREE		0x00000100UL	/* DEBUG: Perform (expensive) checks on free */
-#define SLAB_DEBUG_INITIAL	0x00000200UL	/* DEBUG: Call constructor (as verifier) */
 #define SLAB_RED_ZONE		0x00000400UL	/* DEBUG: Red zone objs in a cache */
 #define SLAB_POISON		0x00000800UL	/* DEBUG: Poison objects */
 #define SLAB_HWCACHE_ALIGN	0x00002000UL	/* Align objs on cache lines */
@@ -36,7 +35,6 @@ typedef struct kmem_cache kmem_cache_t __deprecated;
 /* Flags passed to a constructor functions */
 #define SLAB_CTOR_CONSTRUCTOR	0x001UL		/* If not set, then deconstructor */
 #define SLAB_CTOR_ATOMIC	0x002UL		/* Tell constructor it can't sleep */
-#define SLAB_CTOR_VERIFY	0x004UL		/* Tell constructor it's a verify call */
 
 /*
  * struct kmem_cache related prototypes

commit 0a31bd5f2bbb6473ef9d24f0063ca91cfa678b64
Author: Christoph Lameter <clameter@sgi.com>
Date:   Sun May 6 14:49:57 2007 -0700

    KMEM_CACHE(): simplify slab cache creation
    
    This patch provides a new macro
    
    KMEM_CACHE(<struct>, <flags>)
    
    to simplify slab creation. KMEM_CACHE creates a slab with the name of the
    struct, with the size of the struct and with the alignment of the struct.
    Additional slab flags may be specified if necessary.
    
    Example
    
    struct test_slab {
            int a,b,c;
            struct list_head;
    } __cacheline_aligned_in_smp;
    
    test_slab_cache = KMEM_CACHE(test_slab, SLAB_PANIC)
    
    will create a new slab named "test_slab" of the size sizeof(struct
    test_slab) and aligned to the alignment of test slab.  If it fails then we
    panic.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index a9befa50d3e3..e14b4c338b89 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -57,6 +57,18 @@ unsigned int kmem_cache_size(struct kmem_cache *);
 const char *kmem_cache_name(struct kmem_cache *);
 int kmem_ptr_validate(struct kmem_cache *cachep, const void *ptr);
 
+/*
+ * Please use this macro to create slab caches. Simply specify the
+ * name of the structure and maybe some flags that are listed above.
+ *
+ * The alignment of the struct determines object alignment. If you
+ * f.e. add ____cacheline_aligned_in_smp to the struct declaration
+ * then the objects will be properly aligned in SMP configurations.
+ */
+#define KMEM_CACHE(__struct, __flags) kmem_cache_create(#__struct,\
+		sizeof(struct __struct), __alignof__(struct __struct),\
+		(__flags), NULL, NULL)
+
 #ifdef CONFIG_NUMA
 extern void *kmem_cache_alloc_node(struct kmem_cache *, gfp_t flags, int node);
 #else

commit 5af60839909b8e3b28ca7cd7912fa0b23475617f
Author: Christoph Lameter <clameter@sgi.com>
Date:   Sun May 6 14:49:56 2007 -0700

    slab allocators: Remove obsolete SLAB_MUST_HWCACHE_ALIGN
    
    This patch was recently posted to lkml and acked by Pekka.
    
    The flag SLAB_MUST_HWCACHE_ALIGN is
    
    1. Never checked by SLAB at all.
    
    2. A duplicate of SLAB_HWCACHE_ALIGN for SLUB
    
    3. Fulfills the role of SLAB_HWCACHE_ALIGN for SLOB.
    
    The only remaining use is in sparc64 and ppc64 and their use there
    reflects some earlier role that the slab flag once may have had. If
    its specified then SLAB_HWCACHE_ALIGN is also specified.
    
    The flag is confusing, inconsistent and has no purpose.
    
    Remove it.
    
    Acked-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 67425c277e12..a9befa50d3e3 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -26,7 +26,6 @@ typedef struct kmem_cache kmem_cache_t __deprecated;
 #define SLAB_POISON		0x00000800UL	/* DEBUG: Poison objects */
 #define SLAB_HWCACHE_ALIGN	0x00002000UL	/* Align objs on cache lines */
 #define SLAB_CACHE_DMA		0x00004000UL	/* Use GFP_DMA memory */
-#define SLAB_MUST_HWCACHE_ALIGN	0x00008000UL	/* Force alignment even if debuggin is active */
 #define SLAB_STORE_USER		0x00010000UL	/* DEBUG: Store the last owner for bug hunting */
 #define SLAB_RECLAIM_ACCOUNT	0x00020000UL	/* Objects are reclaimable */
 #define SLAB_PANIC		0x00040000UL	/* Panic if kmem_cache_create() fails */

commit 81819f0fc8285a2a5a921c019e3e3d7b6169d225
Author: Christoph Lameter <clameter@sgi.com>
Date:   Sun May 6 14:49:36 2007 -0700

    SLUB core
    
    This is a new slab allocator which was motivated by the complexity of the
    existing code in mm/slab.c. It attempts to address a variety of concerns
    with the existing implementation.
    
    A. Management of object queues
    
       A particular concern was the complex management of the numerous object
       queues in SLAB. SLUB has no such queues. Instead we dedicate a slab for
       each allocating CPU and use objects from a slab directly instead of
       queueing them up.
    
    B. Storage overhead of object queues
    
       SLAB Object queues exist per node, per CPU. The alien cache queue even
       has a queue array that contain a queue for each processor on each
       node. For very large systems the number of queues and the number of
       objects that may be caught in those queues grows exponentially. On our
       systems with 1k nodes / processors we have several gigabytes just tied up
       for storing references to objects for those queues  This does not include
       the objects that could be on those queues. One fears that the whole
       memory of the machine could one day be consumed by those queues.
    
    C. SLAB meta data overhead
    
       SLAB has overhead at the beginning of each slab. This means that data
       cannot be naturally aligned at the beginning of a slab block. SLUB keeps
       all meta data in the corresponding page_struct. Objects can be naturally
       aligned in the slab. F.e. a 128 byte object will be aligned at 128 byte
       boundaries and can fit tightly into a 4k page with no bytes left over.
       SLAB cannot do this.
    
    D. SLAB has a complex cache reaper
    
       SLUB does not need a cache reaper for UP systems. On SMP systems
       the per CPU slab may be pushed back into partial list but that
       operation is simple and does not require an iteration over a list
       of objects. SLAB expires per CPU, shared and alien object queues
       during cache reaping which may cause strange hold offs.
    
    E. SLAB has complex NUMA policy layer support
    
       SLUB pushes NUMA policy handling into the page allocator. This means that
       allocation is coarser (SLUB does interleave on a page level) but that
       situation was also present before 2.6.13. SLABs application of
       policies to individual slab objects allocated in SLAB is
       certainly a performance concern due to the frequent references to
       memory policies which may lead a sequence of objects to come from
       one node after another. SLUB will get a slab full of objects
       from one node and then will switch to the next.
    
    F. Reduction of the size of partial slab lists
    
       SLAB has per node partial lists. This means that over time a large
       number of partial slabs may accumulate on those lists. These can
       only be reused if allocator occur on specific nodes. SLUB has a global
       pool of partial slabs and will consume slabs from that pool to
       decrease fragmentation.
    
    G. Tunables
    
       SLAB has sophisticated tuning abilities for each slab cache. One can
       manipulate the queue sizes in detail. However, filling the queues still
       requires the uses of the spin lock to check out slabs. SLUB has a global
       parameter (min_slab_order) for tuning. Increasing the minimum slab
       order can decrease the locking overhead. The bigger the slab order the
       less motions of pages between per CPU and partial lists occur and the
       better SLUB will be scaling.
    
    G. Slab merging
    
       We often have slab caches with similar parameters. SLUB detects those
       on boot up and merges them into the corresponding general caches. This
       leads to more effective memory use. About 50% of all caches can
       be eliminated through slab merging. This will also decrease
       slab fragmentation because partial allocated slabs can be filled
       up again. Slab merging can be switched off by specifying
       slub_nomerge on boot up.
    
       Note that merging can expose heretofore unknown bugs in the kernel
       because corrupted objects may now be placed differently and corrupt
       differing neighboring objects. Enable sanity checks to find those.
    
    H. Diagnostics
    
       The current slab diagnostics are difficult to use and require a
       recompilation of the kernel. SLUB contains debugging code that
       is always available (but is kept out of the hot code paths).
       SLUB diagnostics can be enabled via the "slab_debug" option.
       Parameters can be specified to select a single or a group of
       slab caches for diagnostics. This means that the system is running
       with the usual performance and it is much more likely that
       race conditions can be reproduced.
    
    I. Resiliency
    
       If basic sanity checks are on then SLUB is capable of detecting
       common error conditions and recover as best as possible to allow the
       system to continue.
    
    J. Tracing
    
       Tracing can be enabled via the slab_debug=T,<slabcache> option
       during boot. SLUB will then protocol all actions on that slabcache
       and dump the object contents on free.
    
    K. On demand DMA cache creation.
    
       Generally DMA caches are not needed. If a kmalloc is used with
       __GFP_DMA then just create this single slabcache that is needed.
       For systems that have no ZONE_DMA requirement the support is
       completely eliminated.
    
    L. Performance increase
    
       Some benchmarks have shown speed improvements on kernbench in the
       range of 5-10%. The locking overhead of slub is based on the
       underlying base allocation size. If we can reliably allocate
       larger order pages then it is possible to increase slub
       performance much further. The anti-fragmentation patches may
       enable further performance increases.
    
    Tested on:
    i386 UP + SMP, x86_64 UP + SMP + NUMA emulation, IA64 NUMA + Simulator
    
    SLUB Boot options
    
    slub_nomerge            Disable merging of slabs
    slub_min_order=x        Require a minimum order for slab caches. This
                            increases the managed chunk size and therefore
                            reduces meta data and locking overhead.
    slub_min_objects=x      Mininum objects per slab. Default is 8.
    slub_max_order=x        Avoid generating slabs larger than order specified.
    slub_debug              Enable all diagnostics for all caches
    slub_debug=<options>    Enable selective options for all caches
    slub_debug=<o>,<cache>  Enable selective options for a certain set of
                            caches
    
    Available Debug options
    F               Double Free checking, sanity and resiliency
    R               Red zoning
    P               Object / padding poisoning
    U               Track last free / alloc
    T               Trace all allocs / frees (only use for individual slabs).
    
    To use SLUB: Apply this patch and then select SLUB as the default slab
    allocator.
    
    [hugh@veritas.com: fix an oops-causing locking error]
    [akpm@linux-foundation.org: various stupid cleanups and small fixes]
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index f9ed9346bfd6..67425c277e12 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -32,6 +32,7 @@ typedef struct kmem_cache kmem_cache_t __deprecated;
 #define SLAB_PANIC		0x00040000UL	/* Panic if kmem_cache_create() fails */
 #define SLAB_DESTROY_BY_RCU	0x00080000UL	/* Defer freeing slabs to RCU */
 #define SLAB_MEM_SPREAD		0x00100000UL	/* Spread some memory over cpuset */
+#define SLAB_TRACE		0x00200000UL	/* Trace allocations and frees */
 
 /* Flags passed to a constructor functions */
 #define SLAB_CTOR_CONSTRUCTOR	0x001UL		/* If not set, then deconstructor */
@@ -42,7 +43,7 @@ typedef struct kmem_cache kmem_cache_t __deprecated;
  * struct kmem_cache related prototypes
  */
 void __init kmem_cache_init(void);
-extern int slab_is_available(void);
+int slab_is_available(void);
 
 struct kmem_cache *kmem_cache_create(const char *, size_t, size_t,
 			unsigned long,
@@ -95,9 +96,14 @@ static inline void *kcalloc(size_t n, size_t size, gfp_t flags)
  * the appropriate general cache at compile time.
  */
 
-#ifdef CONFIG_SLAB
+#if defined(CONFIG_SLAB) || defined(CONFIG_SLUB)
+#ifdef CONFIG_SLUB
+#include <linux/slub_def.h>
+#else
 #include <linux/slab_def.h>
+#endif /* !CONFIG_SLUB */
 #else
+
 /*
  * Fallback definitions for an allocator not wanting to provide
  * its own optimized kmalloc definitions (like SLOB).
@@ -184,7 +190,7 @@ static inline void *__kmalloc_node(size_t size, gfp_t flags, int node)
  * allocator where we care about the real place the memory allocation
  * request comes from.
  */
-#ifdef CONFIG_DEBUG_SLAB
+#if defined(CONFIG_DEBUG_SLAB) || defined(CONFIG_SLUB)
 extern void *__kmalloc_track_caller(size_t, gfp_t, void*);
 #define kmalloc_track_caller(size, flags) \
 	__kmalloc_track_caller(size, flags, __builtin_return_address(0))
@@ -202,7 +208,7 @@ extern void *__kmalloc_track_caller(size_t, gfp_t, void*);
  * standard allocator where we care about the real place the memory
  * allocation request comes from.
  */
-#ifdef CONFIG_DEBUG_SLAB
+#if defined(CONFIG_DEBUG_SLAB) || defined(CONFIG_SLUB)
 extern void *__kmalloc_node_track_caller(size_t, gfp_t, int, void *);
 #define kmalloc_node_track_caller(size, flags, node) \
 	__kmalloc_node_track_caller(size, flags, node, \

commit ac267728f13c55017ed5ee243c9c3166e27ab929
Author: Adrian Bunk <bunk@stusta.de>
Date:   Sun May 6 14:49:12 2007 -0700

    mm/slab.c: proper prototypes
    
    Add proper prototypes in include/linux/slab.h.
    
    Signed-off-by: Adrian Bunk <bunk@stusta.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 2f8f60ff294a..f9ed9346bfd6 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -219,6 +219,9 @@ extern void *__kmalloc_node_track_caller(size_t, gfp_t, int, void *);
 
 #endif /* DEBUG_SLAB */
 
+extern const struct seq_operations slabinfo_op;
+ssize_t slabinfo_write(struct file *, const char __user *, size_t, loff_t *);
+
 #endif	/* __KERNEL__ */
 #endif	/* _LINUX_SLAB_H */
 

commit fd76bab2fa6d8f3ef6b326a4c6ae442fa21d30a4
Author: Pekka Enberg <penberg@cs.helsinki.fi>
Date:   Sun May 6 14:48:40 2007 -0700

    slab: introduce krealloc
    
    This introduce krealloc() that reallocates memory while keeping the contents
    unchanged.  The allocator avoids reallocation if the new size fits the
    currently used cache.  I also added a simple non-optimized version for
    mm/slob.c for compatibility.
    
    [akpm@linux-foundation.org: fix warnings]
    Acked-by: Josef Sipek <jsipek@fsl.cs.sunysb.edu>
    Acked-by: Matt Mackall <mpm@selenic.com>
    Acked-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 1ef822e31c77..2f8f60ff294a 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -72,8 +72,9 @@ static inline void *kmem_cache_alloc_node(struct kmem_cache *cachep,
  */
 void *__kmalloc(size_t, gfp_t);
 void *__kzalloc(size_t, gfp_t);
+void * __must_check krealloc(const void *, size_t, gfp_t);
 void kfree(const void *);
-unsigned int ksize(const void *);
+size_t ksize(const void *);
 
 /**
  * kcalloc - allocate memory for an array. The memory is set to zero.

commit 55935a34a428a1497e3b37982e2782c09c6f914d
Author: Christoph Lameter <clameter@sgi.com>
Date:   Wed Dec 13 00:34:24 2006 -0800

    [PATCH] More slab.h cleanups
    
    More cleanups for slab.h
    
    1. Remove tabs from weird locations as suggested by Pekka
    
    2. Drop the check for NUMA and SLAB_DEBUG from the fallback section
       as suggested by Pekka.
    
    3. Uses static inline for the fallback defs as also suggested by Pekka.
    
    4. Make kmem_ptr_valid take a const * argument.
    
    5. Separate the NUMA fallback definitions from the kmalloc_track fallback
       definitions.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index e7a9c6b42412..1ef822e31c77 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -20,11 +20,11 @@ typedef struct kmem_cache kmem_cache_t __deprecated;
  * Flags to pass to kmem_cache_create().
  * The ones marked DEBUG are only valid if CONFIG_SLAB_DEBUG is set.
  */
-#define	SLAB_DEBUG_FREE		0x00000100UL	/* DEBUG: Perform (expensive) checks on free */
-#define	SLAB_DEBUG_INITIAL	0x00000200UL	/* DEBUG: Call constructor (as verifier) */
-#define	SLAB_RED_ZONE		0x00000400UL	/* DEBUG: Red zone objs in a cache */
-#define	SLAB_POISON		0x00000800UL	/* DEBUG: Poison objects */
-#define	SLAB_HWCACHE_ALIGN	0x00002000UL	/* Align objs on cache lines */
+#define SLAB_DEBUG_FREE		0x00000100UL	/* DEBUG: Perform (expensive) checks on free */
+#define SLAB_DEBUG_INITIAL	0x00000200UL	/* DEBUG: Call constructor (as verifier) */
+#define SLAB_RED_ZONE		0x00000400UL	/* DEBUG: Red zone objs in a cache */
+#define SLAB_POISON		0x00000800UL	/* DEBUG: Poison objects */
+#define SLAB_HWCACHE_ALIGN	0x00002000UL	/* Align objs on cache lines */
 #define SLAB_CACHE_DMA		0x00004000UL	/* Use GFP_DMA memory */
 #define SLAB_MUST_HWCACHE_ALIGN	0x00008000UL	/* Force alignment even if debuggin is active */
 #define SLAB_STORE_USER		0x00010000UL	/* DEBUG: Store the last owner for bug hunting */
@@ -34,9 +34,9 @@ typedef struct kmem_cache kmem_cache_t __deprecated;
 #define SLAB_MEM_SPREAD		0x00100000UL	/* Spread some memory over cpuset */
 
 /* Flags passed to a constructor functions */
-#define	SLAB_CTOR_CONSTRUCTOR	0x001UL		/* If not set, then deconstructor */
+#define SLAB_CTOR_CONSTRUCTOR	0x001UL		/* If not set, then deconstructor */
 #define SLAB_CTOR_ATOMIC	0x002UL		/* Tell constructor it can't sleep */
-#define	SLAB_CTOR_VERIFY	0x004UL		/* Tell constructor it's a verify call */
+#define SLAB_CTOR_VERIFY	0x004UL		/* Tell constructor it's a verify call */
 
 /*
  * struct kmem_cache related prototypes
@@ -55,7 +55,7 @@ void *kmem_cache_zalloc(struct kmem_cache *, gfp_t);
 void kmem_cache_free(struct kmem_cache *, void *);
 unsigned int kmem_cache_size(struct kmem_cache *);
 const char *kmem_cache_name(struct kmem_cache *);
-int kmem_ptr_validate(struct kmem_cache *cachep, void *ptr);
+int kmem_ptr_validate(struct kmem_cache *cachep, const void *ptr);
 
 #ifdef CONFIG_NUMA
 extern void *kmem_cache_alloc_node(struct kmem_cache *, gfp_t flags, int node);
@@ -93,19 +93,15 @@ static inline void *kcalloc(size_t n, size_t size, gfp_t flags)
  * ways to convert kmalloc() calls to kmem_cache_alloc() invocations by selecting
  * the appropriate general cache at compile time.
  */
+
 #ifdef CONFIG_SLAB
 #include <linux/slab_def.h>
 #else
-
 /*
  * Fallback definitions for an allocator not wanting to provide
  * its own optimized kmalloc definitions (like SLOB).
  */
 
-#if defined(CONFIG_NUMA) || defined(CONFIG_DEBUG_SLAB)
-#error "SLAB fallback definitions not usable for NUMA or Slab debug"
-#endif
-
 /**
  * kmalloc - allocate memory
  * @size: how many bytes of memory are required.
@@ -151,7 +147,7 @@ static inline void *kcalloc(size_t n, size_t size, gfp_t flags)
  *
  * %__GFP_REPEAT - If allocation fails initially, try once more before failing.
  */
-void *kmalloc(size_t size, gfp_t flags)
+static inline void *kmalloc(size_t size, gfp_t flags)
 {
 	return __kmalloc(size, flags);
 }
@@ -161,12 +157,24 @@ void *kmalloc(size_t size, gfp_t flags)
  * @size: how many bytes of memory are required.
  * @flags: the type of memory to allocate (see kmalloc).
  */
-void *kzalloc(size_t size, gfp_t flags)
+static inline void *kzalloc(size_t size, gfp_t flags)
 {
 	return __kzalloc(size, flags);
 }
 #endif
 
+#ifndef CONFIG_NUMA
+static inline void *kmalloc_node(size_t size, gfp_t flags, int node)
+{
+	return kmalloc(size, flags);
+}
+
+static inline void *__kmalloc_node(size_t size, gfp_t flags, int node)
+{
+	return __kmalloc(size, flags);
+}
+#endif /* !CONFIG_NUMA */
+
 /*
  * kmalloc_track_caller is a special version of kmalloc that records the
  * calling function of the routine calling it for slab leak tracking instead
@@ -208,12 +216,8 @@ extern void *__kmalloc_node_track_caller(size_t, gfp_t, int, void *);
 #define kmalloc_node_track_caller(size, flags, node) \
 	kmalloc_track_caller(size, flags)
 
-static inline void *kmalloc_node(size_t size, gfp_t flags, int node)
-{
-	return kmalloc(size, flags);
-}
+#endif /* DEBUG_SLAB */
 
-#endif /* !CONFIG_NUMA */
 #endif	/* __KERNEL__ */
 #endif	/* _LINUX_SLAB_H */
 

commit 2e892f43ccb602e8ffad73396a1000f2040c9e0b
Author: Christoph Lameter <clameter@sgi.com>
Date:   Wed Dec 13 00:34:23 2006 -0800

    [PATCH] Cleanup slab headers / API to allow easy addition of new slab allocators
    
    This is a response to an earlier discussion on linux-mm about splitting
    slab.h components per allocator.  Patch is against 2.6.19-git11.  See
    http://marc.theaimsgroup.com/?l=linux-mm&m=116469577431008&w=2
    
    This patch cleans up the slab header definitions.  We define the common
    functions of slob and slab in slab.h and put the extra definitions needed
    for slab's kmalloc implementations in <linux/slab_def.h>.  In order to get
    a greater set of common functions we add several empty functions to slob.c
    and also rename slob's kmalloc to __kmalloc.
    
    Slob does not need any special definitions since we introduce a fallback
    case.  If there is no need for a slab implementation to provide its own
    kmalloc mess^H^H^Hacros then we simply fall back to __kmalloc functions.
    That is sufficient for SLOB.
    
    Sort the function in slab.h according to their functionality.  First the
    functions operating on struct kmem_cache * then the kmalloc related
    functions followed by special debug and fallback definitions.
    
    Also redo a lot of comments.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>?
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 2271886744f8..e7a9c6b42412 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -1,7 +1,9 @@
 /*
- * linux/include/linux/slab.h
- * Written by Mark Hemment, 1996.
- * (markhe@nextd.demon.co.uk)
+ * Written by Mark Hemment, 1996 (markhe@nextd.demon.co.uk).
+ *
+ * (C) SGI 2006, Christoph Lameter <clameter@sgi.com>
+ * 	Cleaned up and restructured to ease the addition of alternative
+ * 	implementations of SLAB allocators.
  */
 
 #ifndef _LINUX_SLAB_H
@@ -10,64 +12,99 @@
 #ifdef __KERNEL__
 
 #include <linux/gfp.h>
-#include <linux/init.h>
 #include <linux/types.h>
-#include <asm/page.h>		/* kmalloc_sizes.h needs PAGE_SIZE */
-#include <asm/cache.h>		/* kmalloc_sizes.h needs L1_CACHE_BYTES */
-#include <linux/compiler.h>
 
-/* kmem_cache_t exists for legacy reasons and is not used by code in mm */
 typedef struct kmem_cache kmem_cache_t __deprecated;
 
-/* flags to pass to kmem_cache_create().
- * The first 3 are only valid when the allocator as been build
- * SLAB_DEBUG_SUPPORT.
+/*
+ * Flags to pass to kmem_cache_create().
+ * The ones marked DEBUG are only valid if CONFIG_SLAB_DEBUG is set.
  */
-#define	SLAB_DEBUG_FREE		0x00000100UL	/* Peform (expensive) checks on free */
-#define	SLAB_DEBUG_INITIAL	0x00000200UL	/* Call constructor (as verifier) */
-#define	SLAB_RED_ZONE		0x00000400UL	/* Red zone objs in a cache */
-#define	SLAB_POISON		0x00000800UL	/* Poison objects */
-#define	SLAB_HWCACHE_ALIGN	0x00002000UL	/* align objs on a h/w cache lines */
-#define SLAB_CACHE_DMA		0x00004000UL	/* use GFP_DMA memory */
-#define SLAB_MUST_HWCACHE_ALIGN	0x00008000UL	/* force alignment */
-#define SLAB_STORE_USER		0x00010000UL	/* store the last owner for bug hunting */
-#define SLAB_RECLAIM_ACCOUNT	0x00020000UL	/* track pages allocated to indicate
-						   what is reclaimable later*/
-#define SLAB_PANIC		0x00040000UL	/* panic if kmem_cache_create() fails */
-#define SLAB_DESTROY_BY_RCU	0x00080000UL	/* defer freeing pages to RCU */
+#define	SLAB_DEBUG_FREE		0x00000100UL	/* DEBUG: Perform (expensive) checks on free */
+#define	SLAB_DEBUG_INITIAL	0x00000200UL	/* DEBUG: Call constructor (as verifier) */
+#define	SLAB_RED_ZONE		0x00000400UL	/* DEBUG: Red zone objs in a cache */
+#define	SLAB_POISON		0x00000800UL	/* DEBUG: Poison objects */
+#define	SLAB_HWCACHE_ALIGN	0x00002000UL	/* Align objs on cache lines */
+#define SLAB_CACHE_DMA		0x00004000UL	/* Use GFP_DMA memory */
+#define SLAB_MUST_HWCACHE_ALIGN	0x00008000UL	/* Force alignment even if debuggin is active */
+#define SLAB_STORE_USER		0x00010000UL	/* DEBUG: Store the last owner for bug hunting */
+#define SLAB_RECLAIM_ACCOUNT	0x00020000UL	/* Objects are reclaimable */
+#define SLAB_PANIC		0x00040000UL	/* Panic if kmem_cache_create() fails */
+#define SLAB_DESTROY_BY_RCU	0x00080000UL	/* Defer freeing slabs to RCU */
 #define SLAB_MEM_SPREAD		0x00100000UL	/* Spread some memory over cpuset */
 
-/* flags passed to a constructor func */
-#define	SLAB_CTOR_CONSTRUCTOR	0x001UL		/* if not set, then deconstructor */
-#define SLAB_CTOR_ATOMIC	0x002UL		/* tell constructor it can't sleep */
-#define	SLAB_CTOR_VERIFY	0x004UL		/* tell constructor it's a verify call */
+/* Flags passed to a constructor functions */
+#define	SLAB_CTOR_CONSTRUCTOR	0x001UL		/* If not set, then deconstructor */
+#define SLAB_CTOR_ATOMIC	0x002UL		/* Tell constructor it can't sleep */
+#define	SLAB_CTOR_VERIFY	0x004UL		/* Tell constructor it's a verify call */
 
-#ifndef CONFIG_SLOB
-
-/* prototypes */
-extern void __init kmem_cache_init(void);
+/*
+ * struct kmem_cache related prototypes
+ */
+void __init kmem_cache_init(void);
+extern int slab_is_available(void);
 
-extern struct kmem_cache *kmem_cache_create(const char *, size_t, size_t,
+struct kmem_cache *kmem_cache_create(const char *, size_t, size_t,
 			unsigned long,
 			void (*)(void *, struct kmem_cache *, unsigned long),
 			void (*)(void *, struct kmem_cache *, unsigned long));
-extern void kmem_cache_destroy(struct kmem_cache *);
-extern int kmem_cache_shrink(struct kmem_cache *);
-extern void *kmem_cache_alloc(struct kmem_cache *, gfp_t);
-extern void *kmem_cache_zalloc(struct kmem_cache *, gfp_t);
-extern void kmem_cache_free(struct kmem_cache *, void *);
-extern unsigned int kmem_cache_size(struct kmem_cache *);
-extern const char *kmem_cache_name(struct kmem_cache *);
+void kmem_cache_destroy(struct kmem_cache *);
+int kmem_cache_shrink(struct kmem_cache *);
+void *kmem_cache_alloc(struct kmem_cache *, gfp_t);
+void *kmem_cache_zalloc(struct kmem_cache *, gfp_t);
+void kmem_cache_free(struct kmem_cache *, void *);
+unsigned int kmem_cache_size(struct kmem_cache *);
+const char *kmem_cache_name(struct kmem_cache *);
+int kmem_ptr_validate(struct kmem_cache *cachep, void *ptr);
+
+#ifdef CONFIG_NUMA
+extern void *kmem_cache_alloc_node(struct kmem_cache *, gfp_t flags, int node);
+#else
+static inline void *kmem_cache_alloc_node(struct kmem_cache *cachep,
+					gfp_t flags, int node)
+{
+	return kmem_cache_alloc(cachep, flags);
+}
+#endif
+
+/*
+ * Common kmalloc functions provided by all allocators
+ */
+void *__kmalloc(size_t, gfp_t);
+void *__kzalloc(size_t, gfp_t);
+void kfree(const void *);
+unsigned int ksize(const void *);
+
+/**
+ * kcalloc - allocate memory for an array. The memory is set to zero.
+ * @n: number of elements.
+ * @size: element size.
+ * @flags: the type of memory to allocate.
+ */
+static inline void *kcalloc(size_t n, size_t size, gfp_t flags)
+{
+	if (n != 0 && size > ULONG_MAX / n)
+		return NULL;
+	return __kzalloc(n * size, flags);
+}
 
-/* Size description struct for general caches. */
-struct cache_sizes {
-	size_t		 	cs_size;
-	struct kmem_cache	*cs_cachep;
-	struct kmem_cache	*cs_dmacachep;
-};
-extern struct cache_sizes malloc_sizes[];
+/*
+ * Allocator specific definitions. These are mainly used to establish optimized
+ * ways to convert kmalloc() calls to kmem_cache_alloc() invocations by selecting
+ * the appropriate general cache at compile time.
+ */
+#ifdef CONFIG_SLAB
+#include <linux/slab_def.h>
+#else
 
-extern void *__kmalloc(size_t, gfp_t);
+/*
+ * Fallback definitions for an allocator not wanting to provide
+ * its own optimized kmalloc definitions (like SLOB).
+ */
+
+#if defined(CONFIG_NUMA) || defined(CONFIG_DEBUG_SLAB)
+#error "SLAB fallback definitions not usable for NUMA or Slab debug"
+#endif
 
 /**
  * kmalloc - allocate memory
@@ -114,29 +151,22 @@ extern void *__kmalloc(size_t, gfp_t);
  *
  * %__GFP_REPEAT - If allocation fails initially, try once more before failing.
  */
-static inline void *kmalloc(size_t size, gfp_t flags)
+void *kmalloc(size_t size, gfp_t flags)
 {
-	if (__builtin_constant_p(size)) {
-		int i = 0;
-#define CACHE(x) \
-		if (size <= x) \
-			goto found; \
-		else \
-			i++;
-#include "kmalloc_sizes.h"
-#undef CACHE
-		{
-			extern void __you_cannot_kmalloc_that_much(void);
-			__you_cannot_kmalloc_that_much();
-		}
-found:
-		return kmem_cache_alloc((flags & GFP_DMA) ?
-			malloc_sizes[i].cs_dmacachep :
-			malloc_sizes[i].cs_cachep, flags);
-	}
 	return __kmalloc(size, flags);
 }
 
+/**
+ * kzalloc - allocate memory. The memory is set to zero.
+ * @size: how many bytes of memory are required.
+ * @flags: the type of memory to allocate (see kmalloc).
+ */
+void *kzalloc(size_t size, gfp_t flags)
+{
+	return __kzalloc(size, flags);
+}
+#endif
+
 /*
  * kmalloc_track_caller is a special version of kmalloc that records the
  * calling function of the routine calling it for slab leak tracking instead
@@ -145,89 +175,16 @@ static inline void *kmalloc(size_t size, gfp_t flags)
  * allocator where we care about the real place the memory allocation
  * request comes from.
  */
-#ifndef CONFIG_DEBUG_SLAB
-#define kmalloc_track_caller(size, flags) \
-	__kmalloc(size, flags)
-#else
+#ifdef CONFIG_DEBUG_SLAB
 extern void *__kmalloc_track_caller(size_t, gfp_t, void*);
 #define kmalloc_track_caller(size, flags) \
 	__kmalloc_track_caller(size, flags, __builtin_return_address(0))
-#endif
-
-extern void *__kzalloc(size_t, gfp_t);
-
-/**
- * kzalloc - allocate memory. The memory is set to zero.
- * @size: how many bytes of memory are required.
- * @flags: the type of memory to allocate (see kmalloc).
- */
-static inline void *kzalloc(size_t size, gfp_t flags)
-{
-	if (__builtin_constant_p(size)) {
-		int i = 0;
-#define CACHE(x) \
-		if (size <= x) \
-			goto found; \
-		else \
-			i++;
-#include "kmalloc_sizes.h"
-#undef CACHE
-		{
-			extern void __you_cannot_kzalloc_that_much(void);
-			__you_cannot_kzalloc_that_much();
-		}
-found:
-		return kmem_cache_zalloc((flags & GFP_DMA) ?
-			malloc_sizes[i].cs_dmacachep :
-			malloc_sizes[i].cs_cachep, flags);
-	}
-	return __kzalloc(size, flags);
-}
-
-/**
- * kcalloc - allocate memory for an array. The memory is set to zero.
- * @n: number of elements.
- * @size: element size.
- * @flags: the type of memory to allocate.
- */
-static inline void *kcalloc(size_t n, size_t size, gfp_t flags)
-{
-	if (n != 0 && size > ULONG_MAX / n)
-		return NULL;
-	return kzalloc(n * size, flags);
-}
-
-extern void kfree(const void *);
-extern unsigned int ksize(const void *);
-extern int slab_is_available(void);
+#else
+#define kmalloc_track_caller(size, flags) \
+	__kmalloc(size, flags)
+#endif /* DEBUG_SLAB */
 
 #ifdef CONFIG_NUMA
-extern void *kmem_cache_alloc_node(struct kmem_cache *, gfp_t flags, int node);
-extern void *__kmalloc_node(size_t size, gfp_t flags, int node);
-
-static inline void *kmalloc_node(size_t size, gfp_t flags, int node)
-{
-	if (__builtin_constant_p(size)) {
-		int i = 0;
-#define CACHE(x) \
-		if (size <= x) \
-			goto found; \
-		else \
-			i++;
-#include "kmalloc_sizes.h"
-#undef CACHE
-		{
-			extern void __you_cannot_kmalloc_that_much(void);
-			__you_cannot_kmalloc_that_much();
-		}
-found:
-		return kmem_cache_alloc_node((flags & GFP_DMA) ?
-			malloc_sizes[i].cs_dmacachep :
-			malloc_sizes[i].cs_cachep, flags, node);
-	}
-	return __kmalloc_node(size, flags, node);
-}
-
 /*
  * kmalloc_node_track_caller is a special version of kmalloc_node that
  * records the calling function of the routine calling it for slab leak
@@ -236,70 +193,27 @@ static inline void *kmalloc_node(size_t size, gfp_t flags, int node)
  * standard allocator where we care about the real place the memory
  * allocation request comes from.
  */
-#ifndef CONFIG_DEBUG_SLAB
-#define kmalloc_node_track_caller(size, flags, node) \
-	__kmalloc_node(size, flags, node)
-#else
+#ifdef CONFIG_DEBUG_SLAB
 extern void *__kmalloc_node_track_caller(size_t, gfp_t, int, void *);
 #define kmalloc_node_track_caller(size, flags, node) \
 	__kmalloc_node_track_caller(size, flags, node, \
 			__builtin_return_address(0))
+#else
+#define kmalloc_node_track_caller(size, flags, node) \
+	__kmalloc_node(size, flags, node)
 #endif
+
 #else /* CONFIG_NUMA */
-static inline void *kmem_cache_alloc_node(struct kmem_cache *cachep,
-					gfp_t flags, int node)
-{
-	return kmem_cache_alloc(cachep, flags);
-}
-static inline void *kmalloc_node(size_t size, gfp_t flags, int node)
-{
-	return kmalloc(size, flags);
-}
 
 #define kmalloc_node_track_caller(size, flags, node) \
 	kmalloc_track_caller(size, flags)
-#endif
 
-extern int FASTCALL(kmem_cache_reap(int));
-extern int FASTCALL(kmem_ptr_validate(struct kmem_cache *cachep, void *ptr));
-
-#else /* CONFIG_SLOB */
-
-/* SLOB allocator routines */
-
-void kmem_cache_init(void);
-struct kmem_cache *kmem_cache_create(const char *c, size_t, size_t,
-	unsigned long,
-	void (*)(void *, struct kmem_cache *, unsigned long),
-	void (*)(void *, struct kmem_cache *, unsigned long));
-void kmem_cache_destroy(struct kmem_cache *c);
-void *kmem_cache_alloc(struct kmem_cache *c, gfp_t flags);
-void *kmem_cache_zalloc(struct kmem_cache *, gfp_t);
-void kmem_cache_free(struct kmem_cache *c, void *b);
-const char *kmem_cache_name(struct kmem_cache *);
-void *kmalloc(size_t size, gfp_t flags);
-void *__kzalloc(size_t size, gfp_t flags);
-void kfree(const void *m);
-unsigned int ksize(const void *m);
-unsigned int kmem_cache_size(struct kmem_cache *c);
-
-static inline void *kcalloc(size_t n, size_t size, gfp_t flags)
+static inline void *kmalloc_node(size_t size, gfp_t flags, int node)
 {
-	return __kzalloc(n * size, flags);
+	return kmalloc(size, flags);
 }
 
-#define kmem_cache_shrink(d) (0)
-#define kmem_cache_reap(a)
-#define kmem_ptr_validate(a, b) (0)
-#define kmem_cache_alloc_node(c, f, n) kmem_cache_alloc(c, f)
-#define kmalloc_node(s, f, n) kmalloc(s, f)
-#define kzalloc(s, f) __kzalloc(s, f)
-#define kmalloc_track_caller kmalloc
-
-#define kmalloc_node_track_caller kmalloc_node
-
-#endif /* CONFIG_SLOB */
-
+#endif /* !CONFIG_NUMA */
 #endif	/* __KERNEL__ */
-
 #endif	/* _LINUX_SLAB_H */
+

commit 1b1cec4bbc59feac89670d5d6d222a02545bac94
Author: Andrew Morton <akpm@osdl.org>
Date:   Wed Dec 6 20:33:22 2006 -0800

    [PATCH] slab: deprecate kmem_cache_t
    
    Cc: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index fbcfc208f52b..2271886744f8 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -7,16 +7,17 @@
 #ifndef _LINUX_SLAB_H
 #define	_LINUX_SLAB_H
 
-#if	defined(__KERNEL__)
+#ifdef __KERNEL__
 
-/* kmem_cache_t exists for legacy reasons and is not used by code in mm */
-typedef struct kmem_cache kmem_cache_t;
+#include <linux/gfp.h>
+#include <linux/init.h>
+#include <linux/types.h>
+#include <asm/page.h>		/* kmalloc_sizes.h needs PAGE_SIZE */
+#include <asm/cache.h>		/* kmalloc_sizes.h needs L1_CACHE_BYTES */
+#include <linux/compiler.h>
 
-#include	<linux/gfp.h>
-#include	<linux/init.h>
-#include	<linux/types.h>
-#include	<asm/page.h>		/* kmalloc_sizes.h needs PAGE_SIZE */
-#include	<asm/cache.h>		/* kmalloc_sizes.h needs L1_CACHE_BYTES */
+/* kmem_cache_t exists for legacy reasons and is not used by code in mm */
+typedef struct kmem_cache kmem_cache_t __deprecated;
 
 /* flags to pass to kmem_cache_create().
  * The first 3 are only valid when the allocator as been build

commit 441e143e95f5aa1e04026cb0aa71c801ba53982f
Author: Christoph Lameter <clameter@sgi.com>
Date:   Wed Dec 6 20:33:19 2006 -0800

    [PATCH] slab: remove SLAB_DMA
    
    SLAB_DMA is an alias of GFP_DMA. This is the last one so we
    remove the leftover comment too.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 639f65efa46e..fbcfc208f52b 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -18,9 +18,6 @@ typedef struct kmem_cache kmem_cache_t;
 #include	<asm/page.h>		/* kmalloc_sizes.h needs PAGE_SIZE */
 #include	<asm/cache.h>		/* kmalloc_sizes.h needs L1_CACHE_BYTES */
 
-/* flags for kmem_cache_alloc() */
-#define	SLAB_DMA		GFP_DMA
-
 /* flags to pass to kmem_cache_create().
  * The first 3 are only valid when the allocator as been build
  * SLAB_DEBUG_SUPPORT.

commit e94b1766097d53e6f3ccfb36c8baa562ffeda3fc
Author: Christoph Lameter <clameter@sgi.com>
Date:   Wed Dec 6 20:33:17 2006 -0800

    [PATCH] slab: remove SLAB_KERNEL
    
    SLAB_KERNEL is an alias of GFP_KERNEL.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 34b046ea88f1..639f65efa46e 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -19,7 +19,6 @@ typedef struct kmem_cache kmem_cache_t;
 #include	<asm/cache.h>		/* kmalloc_sizes.h needs L1_CACHE_BYTES */
 
 /* flags for kmem_cache_alloc() */
-#define	SLAB_KERNEL		GFP_KERNEL
 #define	SLAB_DMA		GFP_DMA
 
 /* flags to pass to kmem_cache_create().

commit 54e6ecb23951b195d02433a741c7f7cb0b796c78
Author: Christoph Lameter <clameter@sgi.com>
Date:   Wed Dec 6 20:33:16 2006 -0800

    [PATCH] slab: remove SLAB_ATOMIC
    
    SLAB_ATOMIC is an alias of GFP_ATOMIC
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index d7ee28e51330..34b046ea88f1 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -19,7 +19,6 @@ typedef struct kmem_cache kmem_cache_t;
 #include	<asm/cache.h>		/* kmalloc_sizes.h needs L1_CACHE_BYTES */
 
 /* flags for kmem_cache_alloc() */
-#define	SLAB_ATOMIC		GFP_ATOMIC
 #define	SLAB_KERNEL		GFP_KERNEL
 #define	SLAB_DMA		GFP_DMA
 

commit f7267c0c0721fd02ad3dc37c3d6dd24ccd81d4d6
Author: Christoph Lameter <clameter@sgi.com>
Date:   Wed Dec 6 20:33:15 2006 -0800

    [PATCH] slab: remove SLAB_USER
    
    SLAB_USER is an alias of GFP_USER
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 5b70d52b4f86..d7ee28e51330 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -20,7 +20,6 @@ typedef struct kmem_cache kmem_cache_t;
 
 /* flags for kmem_cache_alloc() */
 #define	SLAB_ATOMIC		GFP_ATOMIC
-#define	SLAB_USER		GFP_USER
 #define	SLAB_KERNEL		GFP_KERNEL
 #define	SLAB_DMA		GFP_DMA
 

commit e6b4f8da3a88457148038bc952043e99a7fdba64
Author: Christoph Lameter <clameter@sgi.com>
Date:   Wed Dec 6 20:33:14 2006 -0800

    [PATCH] slab: remove SLAB_NOFS
    
    SLAB_NOFS is an alias of GFP_NOFS.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 43ced80c327b..5b70d52b4f86 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -19,7 +19,6 @@ typedef struct kmem_cache kmem_cache_t;
 #include	<asm/cache.h>		/* kmalloc_sizes.h needs L1_CACHE_BYTES */
 
 /* flags for kmem_cache_alloc() */
-#define	SLAB_NOFS		GFP_NOFS
 #define	SLAB_ATOMIC		GFP_ATOMIC
 #define	SLAB_USER		GFP_USER
 #define	SLAB_KERNEL		GFP_KERNEL

commit 55acbda0965ca0a29b0ca276e7d17a55edc11d1b
Author: Christoph Lameter <clameter@sgi.com>
Date:   Wed Dec 6 20:33:13 2006 -0800

    [PATCH] slab: remove SLAB_NOIO
    
    SLAB_NOIO is an alias of GFP_NOIO with a single instance of use.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 6f7b9bb43065..43ced80c327b 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -20,7 +20,6 @@ typedef struct kmem_cache kmem_cache_t;
 
 /* flags for kmem_cache_alloc() */
 #define	SLAB_NOFS		GFP_NOFS
-#define	SLAB_NOIO		GFP_NOIO
 #define	SLAB_ATOMIC		GFP_ATOMIC
 #define	SLAB_USER		GFP_USER
 #define	SLAB_KERNEL		GFP_KERNEL

commit a06d72c1dcbff015250df6ad9f0b1d18c02113bf
Author: Christoph Lameter <clameter@sgi.com>
Date:   Wed Dec 6 20:33:12 2006 -0800

    [PATCH] slab: remove SLAB_LEVEL_MASK
    
    SLAB_LEVEL_MASK is only used internally to the slab and is
    and alias of GFP_LEVEL_MASK.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 9ffd1c1616bd..6f7b9bb43065 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -26,8 +26,6 @@ typedef struct kmem_cache kmem_cache_t;
 #define	SLAB_KERNEL		GFP_KERNEL
 #define	SLAB_DMA		GFP_DMA
 
-#define SLAB_LEVEL_MASK		GFP_LEVEL_MASK
-
 /* flags to pass to kmem_cache_create().
  * The first 3 are only valid when the allocator as been build
  * SLAB_DEBUG_SUPPORT.

commit 6e0eaa4b05cf53ca5caa702fd2760a5b3376be69
Author: Christoph Lameter <clameter@sgi.com>
Date:   Wed Dec 6 20:33:10 2006 -0800

    [PATCH] slab: remove SLAB_NO_GROW
    
    It is only used internally in the slab.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index b831776b2fc7..9ffd1c1616bd 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -28,8 +28,6 @@ typedef struct kmem_cache kmem_cache_t;
 
 #define SLAB_LEVEL_MASK		GFP_LEVEL_MASK
 
-#define	SLAB_NO_GROW		__GFP_NO_GROW	/* don't grow a cache */
-
 /* flags to pass to kmem_cache_create().
  * The first 3 are only valid when the allocator as been build
  * SLAB_DEBUG_SUPPORT.

commit ebe29738f3934ad6a93c8bd76e30aa5d797a269d
Author: Christoph Lameter <clameter@sgi.com>
Date:   Wed Dec 6 20:32:59 2006 -0800

    [PATCH] Remove uses of kmem_cache_t from mm/* and include/linux/slab.h
    
    Remove all uses of kmem_cache_t (the most were left in slab.h).  The
    typedef for kmem_cache_t is then only necessary for other kernel
    subsystems.  Add a comment to that effect.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index e67314e4a0a0..b831776b2fc7 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -9,6 +9,7 @@
 
 #if	defined(__KERNEL__)
 
+/* kmem_cache_t exists for legacy reasons and is not used by code in mm */
 typedef struct kmem_cache kmem_cache_t;
 
 #include	<linux/gfp.h>
@@ -57,22 +58,23 @@ typedef struct kmem_cache kmem_cache_t;
 /* prototypes */
 extern void __init kmem_cache_init(void);
 
-extern kmem_cache_t *kmem_cache_create(const char *, size_t, size_t, unsigned long,
-				       void (*)(void *, kmem_cache_t *, unsigned long),
-				       void (*)(void *, kmem_cache_t *, unsigned long));
-extern void kmem_cache_destroy(kmem_cache_t *);
-extern int kmem_cache_shrink(kmem_cache_t *);
-extern void *kmem_cache_alloc(kmem_cache_t *, gfp_t);
+extern struct kmem_cache *kmem_cache_create(const char *, size_t, size_t,
+			unsigned long,
+			void (*)(void *, struct kmem_cache *, unsigned long),
+			void (*)(void *, struct kmem_cache *, unsigned long));
+extern void kmem_cache_destroy(struct kmem_cache *);
+extern int kmem_cache_shrink(struct kmem_cache *);
+extern void *kmem_cache_alloc(struct kmem_cache *, gfp_t);
 extern void *kmem_cache_zalloc(struct kmem_cache *, gfp_t);
-extern void kmem_cache_free(kmem_cache_t *, void *);
-extern unsigned int kmem_cache_size(kmem_cache_t *);
-extern const char *kmem_cache_name(kmem_cache_t *);
+extern void kmem_cache_free(struct kmem_cache *, void *);
+extern unsigned int kmem_cache_size(struct kmem_cache *);
+extern const char *kmem_cache_name(struct kmem_cache *);
 
 /* Size description struct for general caches. */
 struct cache_sizes {
-	size_t		 cs_size;
-	kmem_cache_t	*cs_cachep;
-	kmem_cache_t	*cs_dmacachep;
+	size_t		 	cs_size;
+	struct kmem_cache	*cs_cachep;
+	struct kmem_cache	*cs_dmacachep;
 };
 extern struct cache_sizes malloc_sizes[];
 
@@ -211,7 +213,7 @@ extern unsigned int ksize(const void *);
 extern int slab_is_available(void);
 
 #ifdef CONFIG_NUMA
-extern void *kmem_cache_alloc_node(kmem_cache_t *, gfp_t flags, int node);
+extern void *kmem_cache_alloc_node(struct kmem_cache *, gfp_t flags, int node);
 extern void *__kmalloc_node(size_t size, gfp_t flags, int node);
 
 static inline void *kmalloc_node(size_t size, gfp_t flags, int node)
@@ -255,7 +257,8 @@ extern void *__kmalloc_node_track_caller(size_t, gfp_t, int, void *);
 			__builtin_return_address(0))
 #endif
 #else /* CONFIG_NUMA */
-static inline void *kmem_cache_alloc_node(kmem_cache_t *cachep, gfp_t flags, int node)
+static inline void *kmem_cache_alloc_node(struct kmem_cache *cachep,
+					gfp_t flags, int node)
 {
 	return kmem_cache_alloc(cachep, flags);
 }
@@ -269,7 +272,7 @@ static inline void *kmalloc_node(size_t size, gfp_t flags, int node)
 #endif
 
 extern int FASTCALL(kmem_cache_reap(int));
-extern int FASTCALL(kmem_ptr_validate(kmem_cache_t *cachep, void *ptr));
+extern int FASTCALL(kmem_ptr_validate(struct kmem_cache *cachep, void *ptr));
 
 #else /* CONFIG_SLOB */
 

commit b86c089b83b8ae2bc814db865057768a9ba787b5
Author: Christoph Lameter <clameter@sgi.com>
Date:   Wed Dec 6 20:32:57 2006 -0800

    [PATCH] Move names_cachep to linux/fs.h
    
    The names_cachep is used for getname() and putname().  So lets put it into
    fs.h near those two definitions.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index bfc063ee94e8..e67314e4a0a0 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -308,9 +308,6 @@ static inline void *kcalloc(size_t n, size_t size, gfp_t flags)
 
 #endif /* CONFIG_SLOB */
 
-/* System wide caches */
-extern kmem_cache_t	*names_cachep;
-
 #endif	/* __KERNEL__ */
 
 #endif	/* _LINUX_SLAB_H */

commit aa362a83e78d2e9320da588805cf2a0b53356bc3
Author: Christoph Lameter <clameter@sgi.com>
Date:   Wed Dec 6 20:32:54 2006 -0800

    [PATCH] Move fs_cachep to linux/fs_struct.h
    
    fs_cachep is only used in kernel/exit.c and in kernel/fork.c.
    
    It is used to store fs_struct items so it should be placed in linux/fs_struct.h
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index f9202d60d769..bfc063ee94e8 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -310,7 +310,6 @@ static inline void *kcalloc(size_t n, size_t size, gfp_t flags)
 
 /* System wide caches */
 extern kmem_cache_t	*names_cachep;
-extern kmem_cache_t	*fs_cachep;
 
 #endif	/* __KERNEL__ */
 

commit 8b7d91eb7f6a3e8f0caaa613937bda5ab7dc7dc2
Author: Christoph Lameter <clameter@sgi.com>
Date:   Wed Dec 6 20:32:52 2006 -0800

    [PATCH] Move filep_cachep to include/file.h
    
    filp_cachep is only used in fs/file_table.c and in fs/dcache.c where
    it is defined.
    
    Move it to related definitions in linux/file.h.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index dd9efecff410..f9202d60d769 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -310,7 +310,6 @@ static inline void *kcalloc(size_t n, size_t size, gfp_t flags)
 
 /* System wide caches */
 extern kmem_cache_t	*names_cachep;
-extern kmem_cache_t	*filp_cachep;
 extern kmem_cache_t	*fs_cachep;
 
 #endif	/* __KERNEL__ */

commit 5d6538fcf231faccb2ac42f92851d259d00e62f9
Author: Christoph Lameter <clameter@sgi.com>
Date:   Wed Dec 6 20:32:50 2006 -0800

    [PATCH] Move files_cachep to include/file.h
    
    Proper place is in file.h since files_cachep uses are rated to file I/O.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index a30a4028a92e..dd9efecff410 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -310,7 +310,6 @@ static inline void *kcalloc(size_t n, size_t size, gfp_t flags)
 
 /* System wide caches */
 extern kmem_cache_t	*names_cachep;
-extern kmem_cache_t	*files_cachep;
 extern kmem_cache_t	*filp_cachep;
 extern kmem_cache_t	*fs_cachep;
 

commit c43692e85f306667545b91194c748a6e46c1f8b4
Author: Christoph Lameter <clameter@sgi.com>
Date:   Wed Dec 6 20:32:48 2006 -0800

    [PATCH] Move vm_area_cachep to include/mm.h
    
    vm_area_cachep is used to store vm_area_structs. So move to mm.h.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 467b297d2c6f..a30a4028a92e 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -309,7 +309,6 @@ static inline void *kcalloc(size_t n, size_t size, gfp_t flags)
 #endif /* CONFIG_SLOB */
 
 /* System wide caches */
-extern kmem_cache_t	*vm_area_cachep;
 extern kmem_cache_t	*names_cachep;
 extern kmem_cache_t	*files_cachep;
 extern kmem_cache_t	*filp_cachep;

commit 298ec1e2ac85cecce3eddd167286359358c44d5d
Author: Christoph Lameter <clameter@sgi.com>
Date:   Wed Dec 6 20:32:47 2006 -0800

    [PATCH] Move sighand_cachep to include/signal.h
    
    Move sighand_cachep definitioni to linux/signal.h
    
    The sighand cache is only used in fs/exec.c and kernel/fork.c.  It is defined
    in kernel/fork.c but only used in fs/exec.c.
    
    The sighand_cachep is related to signal processing.  So add the definition to
    signal.h.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 6b7d096f056b..467b297d2c6f 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -314,7 +314,6 @@ extern kmem_cache_t	*names_cachep;
 extern kmem_cache_t	*files_cachep;
 extern kmem_cache_t	*filp_cachep;
 extern kmem_cache_t	*fs_cachep;
-extern kmem_cache_t	*sighand_cachep;
 
 #endif	/* __KERNEL__ */
 

commit 54cc211ce3fc73a9d21c6316886db0676beaca95
Author: Christoph Lameter <clameter@sgi.com>
Date:   Wed Dec 6 20:32:45 2006 -0800

    [PATCH] Remove bio_cachep from slab.h
    
    Remove bio_cachep from slab.h - it no longer exists.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 66c4640d3656..6b7d096f056b 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -315,7 +315,6 @@ extern kmem_cache_t	*files_cachep;
 extern kmem_cache_t	*filp_cachep;
 extern kmem_cache_t	*fs_cachep;
 extern kmem_cache_t	*sighand_cachep;
-extern kmem_cache_t	*bio_cachep;
 
 #endif	/* __KERNEL__ */
 

commit 8b98c1699eba23cfd2e8b366625c50ff5fd1415b
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Dec 6 20:32:30 2006 -0800

    [PATCH] leak tracking for kmalloc_node
    
    We have variants of kmalloc and kmem_cache_alloc that leave leak tracking to
    the caller.  This is used for subsystem-specific allocators like skb_alloc.
    
    To make skb_alloc node-aware we need similar routines for the node-aware slab
    allocator, which this patch adds.
    
    Note that the code is rather ugly, but it mirrors the non-node-aware code 1:1:
    
    [akpm@osdl.org: add module export]
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index c4947b8a2c03..66c4640d3656 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -236,7 +236,25 @@ static inline void *kmalloc_node(size_t size, gfp_t flags, int node)
 	}
 	return __kmalloc_node(size, flags, node);
 }
+
+/*
+ * kmalloc_node_track_caller is a special version of kmalloc_node that
+ * records the calling function of the routine calling it for slab leak
+ * tracking instead of just the calling function (confusing, eh?).
+ * It's useful when the call to kmalloc_node comes from a widely-used
+ * standard allocator where we care about the real place the memory
+ * allocation request comes from.
+ */
+#ifndef CONFIG_DEBUG_SLAB
+#define kmalloc_node_track_caller(size, flags, node) \
+	__kmalloc_node(size, flags, node)
 #else
+extern void *__kmalloc_node_track_caller(size_t, gfp_t, int, void *);
+#define kmalloc_node_track_caller(size, flags, node) \
+	__kmalloc_node_track_caller(size, flags, node, \
+			__builtin_return_address(0))
+#endif
+#else /* CONFIG_NUMA */
 static inline void *kmem_cache_alloc_node(kmem_cache_t *cachep, gfp_t flags, int node)
 {
 	return kmem_cache_alloc(cachep, flags);
@@ -245,6 +263,9 @@ static inline void *kmalloc_node(size_t size, gfp_t flags, int node)
 {
 	return kmalloc(size, flags);
 }
+
+#define kmalloc_node_track_caller(size, flags, node) \
+	kmalloc_track_caller(size, flags)
 #endif
 
 extern int FASTCALL(kmem_cache_reap(int));
@@ -283,6 +304,8 @@ static inline void *kcalloc(size_t n, size_t size, gfp_t flags)
 #define kzalloc(s, f) __kzalloc(s, f)
 #define kmalloc_track_caller kmalloc
 
+#define kmalloc_node_track_caller kmalloc_node
+
 #endif /* CONFIG_SLOB */
 
 /* System wide caches */

commit 1d2c8eea698514cfaa53fc991b960791d09508e1
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Oct 4 02:15:25 2006 -0700

    [PATCH] slab: clean up leak tracking ifdefs a little bit
    
    - rename ____kmalloc to kmalloc_track_caller so that people have a chance
      to guess what it does just from it's name.  Add a comment describing it
      for those who don't.  Also move it after kmalloc in slab.h so people get
      less confused when they are just looking for kmalloc - move things around
      in slab.c a little to reduce the ifdef mess.
    
    [penberg@cs.helsinki.fi: Fix up reversed #ifdef]
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: Christoph Lameter <clameter@engr.sgi.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 70be57d8ae0d..c4947b8a2c03 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -77,13 +77,6 @@ struct cache_sizes {
 extern struct cache_sizes malloc_sizes[];
 
 extern void *__kmalloc(size_t, gfp_t);
-#ifndef CONFIG_DEBUG_SLAB
-#define ____kmalloc(size, flags) __kmalloc(size, flags)
-#else
-extern void *__kmalloc_track_caller(size_t, gfp_t, void*);
-#define ____kmalloc(size, flags) \
-    __kmalloc_track_caller(size, flags, __builtin_return_address(0))
-#endif
 
 /**
  * kmalloc - allocate memory
@@ -153,6 +146,23 @@ static inline void *kmalloc(size_t size, gfp_t flags)
 	return __kmalloc(size, flags);
 }
 
+/*
+ * kmalloc_track_caller is a special version of kmalloc that records the
+ * calling function of the routine calling it for slab leak tracking instead
+ * of just the calling function (confusing, eh?).
+ * It's useful when the call to kmalloc comes from a widely-used standard
+ * allocator where we care about the real place the memory allocation
+ * request comes from.
+ */
+#ifndef CONFIG_DEBUG_SLAB
+#define kmalloc_track_caller(size, flags) \
+	__kmalloc(size, flags)
+#else
+extern void *__kmalloc_track_caller(size_t, gfp_t, void*);
+#define kmalloc_track_caller(size, flags) \
+	__kmalloc_track_caller(size, flags, __builtin_return_address(0))
+#endif
+
 extern void *__kzalloc(size_t, gfp_t);
 
 /**
@@ -271,7 +281,7 @@ static inline void *kcalloc(size_t n, size_t size, gfp_t flags)
 #define kmem_cache_alloc_node(c, f, n) kmem_cache_alloc(c, f)
 #define kmalloc_node(s, f, n) kmalloc(s, f)
 #define kzalloc(s, f) __kzalloc(s, f)
-#define ____kmalloc kmalloc
+#define kmalloc_track_caller kmalloc
 
 #endif /* CONFIG_SLOB */
 

commit f30c2269544bffc7bf1b0d7c0abe5be1be83b8cb
Author: Uwe Zeisberger <Uwe_Zeisberger@digi.com>
Date:   Tue Oct 3 23:01:26 2006 +0200

    fix file specification in comments
    
    Many files include the filename at the beginning, serveral used a wrong one.
    
    Signed-off-by: Uwe Zeisberger <Uwe_Zeisberger@digi.com>
    Signed-off-by: Adrian Bunk <bunk@stusta.de>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index a96fd9310d55..70be57d8ae0d 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -1,5 +1,5 @@
 /*
- * linux/mm/slab.h
+ * linux/include/linux/slab.h
  * Written by Mark Hemment, 1996.
  * (markhe@nextd.demon.co.uk)
  */

commit 133d205a18b7a4d8cb52959c5310f6664277cf61
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Wed Sep 27 01:49:41 2006 -0700

    [PATCH] Make kmem_cache_destroy() return void
    
    un-, de-, -free, -destroy, -exit, etc functions should in general return
    void.  Also,
    
    There is very little, say, filesystem driver code can do upon failed
    kmem_cache_destroy().  If it will be decided to BUG in this case, BUG
    should be put in generic code, instead.
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 66d6eb78d1c6..a96fd9310d55 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -60,7 +60,7 @@ extern void __init kmem_cache_init(void);
 extern kmem_cache_t *kmem_cache_create(const char *, size_t, size_t, unsigned long,
 				       void (*)(void *, kmem_cache_t *, unsigned long),
 				       void (*)(void *, kmem_cache_t *, unsigned long));
-extern int kmem_cache_destroy(kmem_cache_t *);
+extern void kmem_cache_destroy(kmem_cache_t *);
 extern int kmem_cache_shrink(kmem_cache_t *);
 extern void *kmem_cache_alloc(kmem_cache_t *, gfp_t);
 extern void *kmem_cache_zalloc(struct kmem_cache *, gfp_t);
@@ -249,7 +249,7 @@ struct kmem_cache *kmem_cache_create(const char *c, size_t, size_t,
 	unsigned long,
 	void (*)(void *, struct kmem_cache *, unsigned long),
 	void (*)(void *, struct kmem_cache *, unsigned long));
-int kmem_cache_destroy(struct kmem_cache *c);
+void kmem_cache_destroy(struct kmem_cache *c);
 void *kmem_cache_alloc(struct kmem_cache *c, gfp_t flags);
 void *kmem_cache_zalloc(struct kmem_cache *, gfp_t);
 void kmem_cache_free(struct kmem_cache *c, void *b);

commit 972d1a7b140569084439a81265a0f15b74e924e0
Author: Christoph Lameter <clameter@sgi.com>
Date:   Mon Sep 25 23:31:51 2006 -0700

    [PATCH] ZVC: Support NR_SLAB_RECLAIMABLE / NR_SLAB_UNRECLAIMABLE
    
    Remove the atomic counter for slab_reclaim_pages and replace the counter
    and NR_SLAB with two ZVC counter that account for unreclaimable and
    reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE.
    
    Change the check in vmscan.c to refer to to NR_SLAB_RECLAIMABLE.  The
    intend seems to be to check for slab pages that could be freed.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 2f6bef6a98c9..66d6eb78d1c6 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -284,8 +284,6 @@ extern kmem_cache_t	*fs_cachep;
 extern kmem_cache_t	*sighand_cachep;
 extern kmem_cache_t	*bio_cachep;
 
-extern atomic_t slab_reclaim_pages;
-
 #endif	/* __KERNEL__ */
 
 #endif	/* _LINUX_SLAB_H */

commit dbe5e69d2d6e591996ea2b817b887d03b60bb143
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Sep 25 23:31:36 2006 -0700

    [PATCH] slab: optimize kmalloc_node the same way as kmalloc
    
    [akpm@osdl.org: export fix]
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 193c03c547ec..2f6bef6a98c9 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -202,7 +202,30 @@ extern int slab_is_available(void);
 
 #ifdef CONFIG_NUMA
 extern void *kmem_cache_alloc_node(kmem_cache_t *, gfp_t flags, int node);
-extern void *kmalloc_node(size_t size, gfp_t flags, int node);
+extern void *__kmalloc_node(size_t size, gfp_t flags, int node);
+
+static inline void *kmalloc_node(size_t size, gfp_t flags, int node)
+{
+	if (__builtin_constant_p(size)) {
+		int i = 0;
+#define CACHE(x) \
+		if (size <= x) \
+			goto found; \
+		else \
+			i++;
+#include "kmalloc_sizes.h"
+#undef CACHE
+		{
+			extern void __you_cannot_kmalloc_that_much(void);
+			__you_cannot_kmalloc_that_much();
+		}
+found:
+		return kmem_cache_alloc_node((flags & GFP_DMA) ?
+			malloc_sizes[i].cs_dmacachep :
+			malloc_sizes[i].cs_cachep, flags, node);
+	}
+	return __kmalloc_node(size, flags, node);
+}
 #else
 static inline void *kmem_cache_alloc_node(kmem_cache_t *cachep, gfp_t flags, int node)
 {

commit b221385bc41d6789edde3d2fa0cb20d5045730eb
Author: Adrian Bunk <bunk@stusta.de>
Date:   Mon Sep 25 23:31:02 2006 -0700

    [PATCH] mm/: make functions static
    
    This patch makes the following needlessly global functions static:
     - slab.c: kmem_find_general_cachep()
     - swap.c: __page_cache_release()
     - vmalloc.c: __vmalloc_node()
    
    Signed-off-by: Adrian Bunk <bunk@stusta.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 45ad55b70d1c..193c03c547ec 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -67,7 +67,6 @@ extern void *kmem_cache_zalloc(struct kmem_cache *, gfp_t);
 extern void kmem_cache_free(kmem_cache_t *, void *);
 extern unsigned int kmem_cache_size(kmem_cache_t *);
 extern const char *kmem_cache_name(kmem_cache_t *);
-extern kmem_cache_t *kmem_find_general_cachep(size_t size, gfp_t gfpflags);
 
 /* Size description struct for general caches. */
 struct cache_sizes {
@@ -223,7 +222,6 @@ extern int FASTCALL(kmem_ptr_validate(kmem_cache_t *cachep, void *ptr));
 /* SLOB allocator routines */
 
 void kmem_cache_init(void);
-struct kmem_cache *kmem_find_general_cachep(size_t, gfp_t gfpflags);
 struct kmem_cache *kmem_cache_create(const char *c, size_t, size_t,
 	unsigned long,
 	void (*)(void *, struct kmem_cache *, unsigned long),

commit 800590f523bf3bde9fa6c8e4d6763e4bf6a2c8ec
Author: Paul Drynoff <pauldrynoff@gmail.com>
Date:   Fri Jun 23 02:03:48 2006 -0700

    [PATCH] slab: kmalloc, kzalloc comments cleanup and fix
    
    - Move comments for kmalloc to right place, currently it near __do_kmalloc
    
    - Comments for kzalloc
    
    - More detailed comments for kmalloc
    
    - Appearance of "kmalloc" and "kzalloc" man pages after "make mandocs"
    
    [rdunlap@xenotime.net: simplification]
    Signed-off-by: Paul Drynoff <pauldrynoff@gmail.com>
    Acked-by: Randy Dunlap <rdunlap@xenotime.net>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: Manfred Spraul <manfred@colorfullife.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 9dc93163e065..45ad55b70d1c 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -86,6 +86,51 @@ extern void *__kmalloc_track_caller(size_t, gfp_t, void*);
     __kmalloc_track_caller(size, flags, __builtin_return_address(0))
 #endif
 
+/**
+ * kmalloc - allocate memory
+ * @size: how many bytes of memory are required.
+ * @flags: the type of memory to allocate.
+ *
+ * kmalloc is the normal method of allocating memory
+ * in the kernel.
+ *
+ * The @flags argument may be one of:
+ *
+ * %GFP_USER - Allocate memory on behalf of user.  May sleep.
+ *
+ * %GFP_KERNEL - Allocate normal kernel ram.  May sleep.
+ *
+ * %GFP_ATOMIC - Allocation will not sleep.
+ *   For example, use this inside interrupt handlers.
+ *
+ * %GFP_HIGHUSER - Allocate pages from high memory.
+ *
+ * %GFP_NOIO - Do not do any I/O at all while trying to get memory.
+ *
+ * %GFP_NOFS - Do not make any fs calls while trying to get memory.
+ *
+ * Also it is possible to set different flags by OR'ing
+ * in one or more of the following additional @flags:
+ *
+ * %__GFP_COLD - Request cache-cold pages instead of
+ *   trying to return cache-warm pages.
+ *
+ * %__GFP_DMA - Request memory from the DMA-capable zone.
+ *
+ * %__GFP_HIGH - This allocation has high priority and may use emergency pools.
+ *
+ * %__GFP_HIGHMEM - Allocated memory may be from highmem.
+ *
+ * %__GFP_NOFAIL - Indicate that this allocation is in no way allowed to fail
+ *   (think twice before using).
+ *
+ * %__GFP_NORETRY - If memory is not immediately available,
+ *   then give up at once.
+ *
+ * %__GFP_NOWARN - If allocation fails, don't issue any warnings.
+ *
+ * %__GFP_REPEAT - If allocation fails initially, try once more before failing.
+ */
 static inline void *kmalloc(size_t size, gfp_t flags)
 {
 	if (__builtin_constant_p(size)) {
@@ -111,6 +156,11 @@ static inline void *kmalloc(size_t size, gfp_t flags)
 
 extern void *__kzalloc(size_t, gfp_t);
 
+/**
+ * kzalloc - allocate memory. The memory is set to zero.
+ * @size: how many bytes of memory are required.
+ * @flags: the type of memory to allocate (see kmalloc).
+ */
 static inline void *kzalloc(size_t size, gfp_t flags)
 {
 	if (__builtin_constant_p(size)) {

commit 66643de455c27973ac31ad6de9f859d399916842
Merge: 2c23d62abb82 387e2b043902
Author: David Woodhouse <dwmw2@infradead.org>
Date:   Wed May 24 09:22:21 2006 +0100

    Merge branch 'master' of git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux-2.6
    
    Conflicts:
    
            include/asm-powerpc/unistd.h
            include/asm-sparc/unistd.h
            include/asm-sparc64/unistd.h
    
    Signed-off-by: David Woodhouse <dwmw2@infradead.org>

commit 39d24e64263cd3211705d3b61ea4171c65030921
Author: Mike Kravetz <kravetz@us.ibm.com>
Date:   Mon May 15 09:44:13 2006 -0700

    [PATCH] add slab_is_available() routine for boot code
    
    slab_is_available() indicates slab based allocators are available for use.
    SPARSEMEM code needs to know this as it can be called at various times
    during the boot process.
    
    Signed-off-by: Mike Kravetz <kravetz@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 3af03b19c983..2d985d59c7b8 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -150,6 +150,7 @@ static inline void *kcalloc(size_t n, size_t size, gfp_t flags)
 
 extern void kfree(const void *);
 extern unsigned int ksize(const void *);
+extern int slab_is_available(void);
 
 #ifdef CONFIG_NUMA
 extern void *kmem_cache_alloc_node(kmem_cache_t *, gfp_t flags, int node);

commit 62c4f0a2d5a188f73a94f2cb8ea0dba3e7cf0a7f
Author: David Woodhouse <dwmw2@infradead.org>
Date:   Wed Apr 26 12:56:16 2006 +0100

    Don't include linux/config.h from anywhere else in include/
    
    Signed-off-by: David Woodhouse <dwmw2@infradead.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 3af03b19c983..a7d7f131b5da 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -11,7 +11,6 @@
 
 typedef struct kmem_cache kmem_cache_t;
 
-#include	<linux/config.h>	/* kmalloc_sizes.h needs CONFIG_ options */
 #include	<linux/gfp.h>
 #include	<linux/init.h>
 #include	<linux/types.h>

commit 6b3934ef52712ece50605dfc72e55d00c580831a
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Tue Mar 28 16:11:16 2006 -0800

    [PATCH] copy_process: cleanup bad_fork_cleanup_signal
    
    __exit_signal() does important cleanups atomically under ->siglock.  It is
    also called from copy_process's error path.  This is not good, for example we
    can't move __unhash_process() under ->siglock for that reason.
    
    We should not mix these 2 paths, just look at ugly 'if (p->sighand)' under
    'bad_fork_cleanup_sighand:' label.  For copy_process() case it is sufficient
    to just backout copy_signal(), nothing more.
    
    Again, nobody can see this task yet.  For CLONE_THREAD case we just decrement
    signal->count, otherwise nobody can see this ->signal and we can free it
    lockless.
    
    This patch assumes it is safe to do exit_thread_group_keys() without
    tasklist_lock.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Acked-by: David Howells <dhowells@redhat.com>
    Signed-off-by: Adrian Bunk <bunk@stusta.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 15e1d9736b1b..3af03b19c983 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -210,7 +210,6 @@ extern kmem_cache_t	*names_cachep;
 extern kmem_cache_t	*files_cachep;
 extern kmem_cache_t	*filp_cachep;
 extern kmem_cache_t	*fs_cachep;
-extern kmem_cache_t	*signal_cachep;
 extern kmem_cache_t	*sighand_cachep;
 extern kmem_cache_t	*bio_cachep;
 

commit 40c07ae8daa659b8feb149c84731629386873c16
Author: Pekka Enberg <penberg@cs.helsinki.fi>
Date:   Sat Mar 25 03:06:43 2006 -0800

    [PATCH] slab: optimize constant-size kzalloc calls
    
    As suggested by Eric Dumazet, optimize kzalloc() calls that pass a
    compile-time constant size.  Please note that the patch increases kernel
    text slightly (~200 bytes for defconfig on x86).
    
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 1216b09e07b1..15e1d9736b1b 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -110,7 +110,30 @@ static inline void *kmalloc(size_t size, gfp_t flags)
 	return __kmalloc(size, flags);
 }
 
-extern void *kzalloc(size_t, gfp_t);
+extern void *__kzalloc(size_t, gfp_t);
+
+static inline void *kzalloc(size_t size, gfp_t flags)
+{
+	if (__builtin_constant_p(size)) {
+		int i = 0;
+#define CACHE(x) \
+		if (size <= x) \
+			goto found; \
+		else \
+			i++;
+#include "kmalloc_sizes.h"
+#undef CACHE
+		{
+			extern void __you_cannot_kzalloc_that_much(void);
+			__you_cannot_kzalloc_that_much();
+		}
+found:
+		return kmem_cache_zalloc((flags & GFP_DMA) ?
+			malloc_sizes[i].cs_dmacachep :
+			malloc_sizes[i].cs_cachep, flags);
+	}
+	return __kzalloc(size, flags);
+}
 
 /**
  * kcalloc - allocate memory for an array. The memory is set to zero.
@@ -161,14 +184,14 @@ void *kmem_cache_zalloc(struct kmem_cache *, gfp_t);
 void kmem_cache_free(struct kmem_cache *c, void *b);
 const char *kmem_cache_name(struct kmem_cache *);
 void *kmalloc(size_t size, gfp_t flags);
-void *kzalloc(size_t size, gfp_t flags);
+void *__kzalloc(size_t size, gfp_t flags);
 void kfree(const void *m);
 unsigned int ksize(const void *m);
 unsigned int kmem_cache_size(struct kmem_cache *c);
 
 static inline void *kcalloc(size_t n, size_t size, gfp_t flags)
 {
-	return kzalloc(n * size, flags);
+	return __kzalloc(n * size, flags);
 }
 
 #define kmem_cache_shrink(d) (0)
@@ -176,6 +199,7 @@ static inline void *kcalloc(size_t n, size_t size, gfp_t flags)
 #define kmem_ptr_validate(a, b) (0)
 #define kmem_cache_alloc_node(c, f, n) kmem_cache_alloc(c, f)
 #define kmalloc_node(s, f, n) kmalloc(s, f)
+#define kzalloc(s, f) __kzalloc(s, f)
 #define ____kmalloc kmalloc
 
 #endif /* CONFIG_SLOB */

commit a8c0f9a41f88da703ade33f9c1626a55c786e8bb
Author: Pekka Enberg <penberg@cs.helsinki.fi>
Date:   Sat Mar 25 03:06:42 2006 -0800

    [PATCH] slab: introduce kmem_cache_zalloc allocator
    
    Introduce a memory-zeroing variant of kmem_cache_alloc.  The allocator
    already exits in XFS and there are potential users for it so this patch
    makes the allocator available for the general public.
    
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index f88e08a5802c..1216b09e07b1 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -64,6 +64,7 @@ extern kmem_cache_t *kmem_cache_create(const char *, size_t, size_t, unsigned lo
 extern int kmem_cache_destroy(kmem_cache_t *);
 extern int kmem_cache_shrink(kmem_cache_t *);
 extern void *kmem_cache_alloc(kmem_cache_t *, gfp_t);
+extern void *kmem_cache_zalloc(struct kmem_cache *, gfp_t);
 extern void kmem_cache_free(kmem_cache_t *, void *);
 extern unsigned int kmem_cache_size(kmem_cache_t *);
 extern const char *kmem_cache_name(kmem_cache_t *);
@@ -156,6 +157,7 @@ struct kmem_cache *kmem_cache_create(const char *c, size_t, size_t,
 	void (*)(void *, struct kmem_cache *, unsigned long));
 int kmem_cache_destroy(struct kmem_cache *c);
 void *kmem_cache_alloc(struct kmem_cache *c, gfp_t flags);
+void *kmem_cache_zalloc(struct kmem_cache *, gfp_t);
 void kmem_cache_free(struct kmem_cache *c, void *b);
 const char *kmem_cache_name(struct kmem_cache *);
 void *kmalloc(size_t size, gfp_t flags);

commit 871751e25d956ad24f129ca972b7851feaa61d53
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sat Mar 25 03:06:39 2006 -0800

    [PATCH] slab: implement /proc/slab_allocators
    
    Implement /proc/slab_allocators.   It produces output like:
    
    idr_layer_cache: 80 idr_pre_get+0x33/0x4e
    buffer_head: 2555 alloc_buffer_head+0x20/0x75
    mm_struct: 9 mm_alloc+0x1e/0x42
    mm_struct: 20 dup_mm+0x36/0x370
    vm_area_struct: 384 dup_mm+0x18f/0x370
    vm_area_struct: 151 do_mmap_pgoff+0x2e0/0x7c3
    vm_area_struct: 1 split_vma+0x5a/0x10e
    vm_area_struct: 11 do_brk+0x206/0x2e2
    vm_area_struct: 2 copy_vma+0xda/0x142
    vm_area_struct: 9 setup_arg_pages+0x99/0x214
    fs_cache: 8 copy_fs_struct+0x21/0x133
    fs_cache: 29 copy_process+0xf38/0x10e3
    files_cache: 30 alloc_files+0x1b/0xcf
    signal_cache: 81 copy_process+0xbaa/0x10e3
    sighand_cache: 77 copy_process+0xe65/0x10e3
    sighand_cache: 1 de_thread+0x4d/0x5f8
    anon_vma: 241 anon_vma_prepare+0xd9/0xf3
    size-2048: 1 add_sect_attrs+0x5f/0x145
    size-2048: 2 journal_init_revoke+0x99/0x302
    size-2048: 2 journal_init_revoke+0x137/0x302
    size-2048: 2 journal_init_inode+0xf9/0x1c4
    
    Cc: Manfred Spraul <manfred@colorfullife.com>
    Cc: Alexander Nyberg <alexn@telia.com>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: Christoph Lameter <clameter@engr.sgi.com>
    Cc: Ravikiran Thirumalai <kiran@scalex86.org>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    DESC
    slab-leaks3-locking-fix
    EDESC
    From: Andrew Morton <akpm@osdl.org>
    
    Update for slab-remove-cachep-spinlock.patch
    
    Cc: Al Viro <viro@ftp.linux.org.uk>
    Cc: Manfred Spraul <manfred@colorfullife.com>
    Cc: Alexander Nyberg <alexn@telia.com>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: Christoph Lameter <clameter@engr.sgi.com>
    Cc: Ravikiran Thirumalai <kiran@scalex86.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index e2ee5b268797..f88e08a5802c 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -77,11 +77,12 @@ struct cache_sizes {
 };
 extern struct cache_sizes malloc_sizes[];
 
-#ifndef CONFIG_DEBUG_SLAB
 extern void *__kmalloc(size_t, gfp_t);
+#ifndef CONFIG_DEBUG_SLAB
+#define ____kmalloc(size, flags) __kmalloc(size, flags)
 #else
 extern void *__kmalloc_track_caller(size_t, gfp_t, void*);
-#define __kmalloc(size, flags) \
+#define ____kmalloc(size, flags) \
     __kmalloc_track_caller(size, flags, __builtin_return_address(0))
 #endif
 
@@ -173,6 +174,7 @@ static inline void *kcalloc(size_t n, size_t size, gfp_t flags)
 #define kmem_ptr_validate(a, b) (0)
 #define kmem_cache_alloc_node(c, f, n) kmem_cache_alloc(c, f)
 #define kmalloc_node(s, f, n) kmalloc(s, f)
+#define ____kmalloc kmalloc
 
 #endif /* CONFIG_SLOB */
 

commit 101a50019ae5e370d73984ee05d56dd3b08f330a
Author: Paul Jackson <pj@sgi.com>
Date:   Fri Mar 24 03:16:07 2006 -0800

    [PATCH] cpuset memory spread slab cache implementation
    
    Provide the slab cache infrastructure to support cpuset memory spreading.
    
    See the previous patches, cpuset_mem_spread, for an explanation of cpuset
    memory spreading.
    
    This patch provides a slab cache SLAB_MEM_SPREAD flag.  If set in the
    kmem_cache_create() call defining a slab cache, then any task marked with the
    process state flag PF_MEMSPREAD will spread memory page allocations for that
    cache over all the allowed nodes, instead of preferring the local (faulting)
    node.
    
    On systems not configured with CONFIG_NUMA, this results in no change to the
    page allocation code path for slab caches.
    
    On systems with cpusets configured in the kernel, but the "memory_spread"
    cpuset option not enabled for the current tasks cpuset, this adds a call to a
    cpuset routine and failed bit test of the processor state flag PF_SPREAD_SLAB.
    
    For tasks so marked, a second inline test is done for the slab cache flag
    SLAB_MEM_SPREAD, and if that is set and if the allocation is not
    in_interrupt(), this adds a call to to a cpuset routine that computes which of
    the tasks mems_allowed nodes should be preferred for this allocation.
    
    ==> This patch adds another hook into the performance critical
        code path to allocating objects from the slab cache, in the
        ____cache_alloc() chunk, below.  The next patch optimizes this
        hook, reducing the impact of the combined mempolicy plus memory
        spreading hooks on this critical code path to a single check
        against the tasks task_struct flags word.
    
    This patch provides the generic slab flags and logic needed to apply memory
    spreading to a particular slab.
    
    A subsequent patch will mark a few specific slab caches for this placement
    policy.
    
    Signed-off-by: Paul Jackson <pj@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 2b28c849d75a..e2ee5b268797 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -46,6 +46,7 @@ typedef struct kmem_cache kmem_cache_t;
 						   what is reclaimable later*/
 #define SLAB_PANIC		0x00040000UL	/* panic if kmem_cache_create() fails */
 #define SLAB_DESTROY_BY_RCU	0x00080000UL	/* defer freeing pages to RCU */
+#define SLAB_MEM_SPREAD		0x00100000UL	/* Spread some memory over cpuset */
 
 /* flags passed to a constructor func */
 #define	SLAB_CTOR_CONSTRUCTOR	0x001UL		/* if not set, then deconstructor */

commit ac2b898ca6fb06196a26869c23b66afe7944e52e
Author: Christoph Lameter <clameter@engr.sgi.com>
Date:   Wed Mar 22 00:08:15 2006 -0800

    [PATCH] slab: Remove SLAB_NO_REAP option
    
    SLAB_NO_REAP is documented as an option that will cause this slab not to be
    reaped under memory pressure.  However, that is not what happens.  The only
    thing that SLAB_NO_REAP controls at the moment is the reclaim of the unused
    slab elements that were allocated in batch in cache_reap().  Cache_reap()
    is run every few seconds independently of memory pressure.
    
    Could we remove the whole thing?  Its only used by three slabs anyways and
    I cannot find a reason for having this option.
    
    There is an additional problem with SLAB_NO_REAP.  If set then the recovery
    of objects from alien caches is switched off.  Objects not freed on the
    same node where they were initially allocated will only be reused if a
    certain amount of objects accumulates from one alien node (not very likely)
    or if the cache is explicitly shrunk.  (Strangely __cache_shrink does not
    check for SLAB_NO_REAP)
    
    Getting rid of SLAB_NO_REAP fixes the problems with alien cache freeing.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: Manfred Spraul <manfred@colorfullife.com>
    Cc: Mark Fasheh <mark.fasheh@oracle.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 38bed95dda7a..2b28c849d75a 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -38,7 +38,6 @@ typedef struct kmem_cache kmem_cache_t;
 #define	SLAB_DEBUG_INITIAL	0x00000200UL	/* Call constructor (as verifier) */
 #define	SLAB_RED_ZONE		0x00000400UL	/* Red zone objs in a cache */
 #define	SLAB_POISON		0x00000800UL	/* Poison objects */
-#define	SLAB_NO_REAP		0x00001000UL	/* never reap from the cache */
 #define	SLAB_HWCACHE_ALIGN	0x00002000UL	/* align objs on a h/w cache lines */
 #define SLAB_CACHE_DMA		0x00004000UL	/* use GFP_DMA memory */
 #define SLAB_MUST_HWCACHE_ALIGN	0x00008000UL	/* force alignment */

commit b50ec7d8070ae7a39fe78e65a8812bbc3ca2f7ac
Author: Adrian Bunk <bunk@stusta.de>
Date:   Wed Mar 22 00:08:09 2006 -0800

    [PATCH] kcalloc(): INT_MAX -> ULONG_MAX
    
    Since size_t has the same size as a long on all architectures, it's enough
    for overflow checks to check against ULONG_MAX.
    
    This change could allow a compiler better optimization (especially in the
    n=1 case).
    
    The practical effect seems to be positive, but quite small:
    
        text           data     bss      dec            hex filename
    21762380        5859870 1848928 29471178        1c1b1ca vmlinux-old
    21762211        5859870 1848928 29471009        1c1b121 vmlinux-patched
    
    Signed-off-by: Adrian Bunk <bunk@stusta.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 8cf52939d0ab..38bed95dda7a 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -118,7 +118,7 @@ extern void *kzalloc(size_t, gfp_t);
  */
 static inline void *kcalloc(size_t n, size_t size, gfp_t flags)
 {
-	if (n != 0 && size > INT_MAX / n)
+	if (n != 0 && size > ULONG_MAX / n)
 		return NULL;
 	return kzalloc(n * size, flags);
 }

commit 7fd6b1413082c303613fc137aca9a004740cacf0
Author: Pekka Enberg <penberg@cs.helsinki.fi>
Date:   Wed Feb 1 03:05:52 2006 -0800

    [PATCH] slab: fix kzalloc and kstrdup caller report for CONFIG_DEBUG_SLAB
    
    Fix kzalloc() and kstrdup() caller report for CONFIG_DEBUG_SLAB.  We must
    pass the caller to __cache_alloc() instead of directly doing
    __builtin_return_address(0) there; otherwise kzalloc() and kstrdup() are
    reported as the allocation site instead of the real one.
    
    Thanks to Valdis Kletnieks for reporting the problem and Steven Rostedt for
    the original idea.
    
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 1fb77a9cc148..8cf52939d0ab 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -76,7 +76,14 @@ struct cache_sizes {
 	kmem_cache_t	*cs_dmacachep;
 };
 extern struct cache_sizes malloc_sizes[];
+
+#ifndef CONFIG_DEBUG_SLAB
 extern void *__kmalloc(size_t, gfp_t);
+#else
+extern void *__kmalloc_track_caller(size_t, gfp_t, void*);
+#define __kmalloc(size, flags) \
+    __kmalloc_track_caller(size, flags, __builtin_return_address(0))
+#endif
 
 static inline void *kmalloc(size_t size, gfp_t flags)
 {

commit 10cef6029502915bdb3cf0821d425cf9dc30c817
Author: Matt Mackall <mpm@selenic.com>
Date:   Sun Jan 8 01:01:45 2006 -0800

    [PATCH] slob: introduce the SLOB allocator
    
    configurable replacement for slab allocator
    
    This adds a CONFIG_SLAB option under CONFIG_EMBEDDED.  When CONFIG_SLAB is
    disabled, the kernel falls back to using the 'SLOB' allocator.
    
    SLOB is a traditional K&R/UNIX allocator with a SLAB emulation layer,
    similar to the original Linux kmalloc allocator that SLAB replaced.  It's
    signicantly smaller code and is more memory efficient.  But like all
    similar allocators, it scales poorly and suffers from fragmentation more
    than SLAB, so it's only appropriate for small systems.
    
    It's been tested extensively in the Linux-tiny tree.  I've also
    stress-tested it with make -j 8 compiles on a 3G SMP+PREEMPT box (not
    recommended).
    
    Here's a comparison for otherwise identical builds, showing SLOB saving
    nearly half a megabyte of RAM:
    
    $ size vmlinux*
       text    data     bss     dec     hex filename
    3336372  529360  190812 4056544  3de5e0 vmlinux-slab
    3323208  527948  190684 4041840  3dac70 vmlinux-slob
    
    $ size mm/{slab,slob}.o
       text    data     bss     dec     hex filename
      13221     752      48   14021    36c5 mm/slab.o
       1896      52       8    1956     7a4 mm/slob.o
    
    /proc/meminfo:
                      SLAB          SLOB      delta
    MemTotal:        27964 kB      27980 kB     +16 kB
    MemFree:         24596 kB      25092 kB    +496 kB
    Buffers:            36 kB         36 kB       0 kB
    Cached:           1188 kB       1188 kB       0 kB
    SwapCached:          0 kB          0 kB       0 kB
    Active:            608 kB        600 kB      -8 kB
    Inactive:          808 kB        812 kB      +4 kB
    HighTotal:           0 kB          0 kB       0 kB
    HighFree:            0 kB          0 kB       0 kB
    LowTotal:        27964 kB      27980 kB     +16 kB
    LowFree:         24596 kB      25092 kB    +496 kB
    SwapTotal:           0 kB          0 kB       0 kB
    SwapFree:            0 kB          0 kB       0 kB
    Dirty:               4 kB         12 kB      +8 kB
    Writeback:           0 kB          0 kB       0 kB
    Mapped:            560 kB        556 kB      -4 kB
    Slab:             1756 kB          0 kB   -1756 kB
    CommitLimit:     13980 kB      13988 kB      +8 kB
    Committed_AS:     4208 kB       4208 kB       0 kB
    PageTables:         28 kB         28 kB       0 kB
    VmallocTotal:  1007312 kB    1007312 kB       0 kB
    VmallocUsed:        48 kB         48 kB       0 kB
    VmallocChunk:  1007264 kB    1007264 kB       0 kB
    
    (this work has been sponsored in part by CELF)
    
    From: Ingo Molnar <mingo@elte.hu>
    
       Fix 32-bitness bugs in mm/slob.c.
    
    Signed-off-by: Matt Mackall <mpm@selenic.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index d1ea4051b996..1fb77a9cc148 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -53,6 +53,8 @@ typedef struct kmem_cache kmem_cache_t;
 #define SLAB_CTOR_ATOMIC	0x002UL		/* tell constructor it can't sleep */
 #define	SLAB_CTOR_VERIFY	0x004UL		/* tell constructor it's a verify call */
 
+#ifndef CONFIG_SLOB
+
 /* prototypes */
 extern void __init kmem_cache_init(void);
 
@@ -134,6 +136,39 @@ static inline void *kmalloc_node(size_t size, gfp_t flags, int node)
 extern int FASTCALL(kmem_cache_reap(int));
 extern int FASTCALL(kmem_ptr_validate(kmem_cache_t *cachep, void *ptr));
 
+#else /* CONFIG_SLOB */
+
+/* SLOB allocator routines */
+
+void kmem_cache_init(void);
+struct kmem_cache *kmem_find_general_cachep(size_t, gfp_t gfpflags);
+struct kmem_cache *kmem_cache_create(const char *c, size_t, size_t,
+	unsigned long,
+	void (*)(void *, struct kmem_cache *, unsigned long),
+	void (*)(void *, struct kmem_cache *, unsigned long));
+int kmem_cache_destroy(struct kmem_cache *c);
+void *kmem_cache_alloc(struct kmem_cache *c, gfp_t flags);
+void kmem_cache_free(struct kmem_cache *c, void *b);
+const char *kmem_cache_name(struct kmem_cache *);
+void *kmalloc(size_t size, gfp_t flags);
+void *kzalloc(size_t size, gfp_t flags);
+void kfree(const void *m);
+unsigned int ksize(const void *m);
+unsigned int kmem_cache_size(struct kmem_cache *c);
+
+static inline void *kcalloc(size_t n, size_t size, gfp_t flags)
+{
+	return kzalloc(n * size, flags);
+}
+
+#define kmem_cache_shrink(d) (0)
+#define kmem_cache_reap(a)
+#define kmem_ptr_validate(a, b) (0)
+#define kmem_cache_alloc_node(c, f, n) kmem_cache_alloc(c, f)
+#define kmalloc_node(s, f, n) kmalloc(s, f)
+
+#endif /* CONFIG_SLOB */
+
 /* System wide caches */
 extern kmem_cache_t	*vm_area_cachep;
 extern kmem_cache_t	*names_cachep;

commit 2109a2d1b175dfcffbfdac693bdbe4c4ab62f11f
Author: Pekka J Enberg <penberg@cs.Helsinki.FI>
Date:   Mon Nov 7 00:58:01 2005 -0800

    [PATCH] mm: rename kmem_cache_s to kmem_cache
    
    This patch renames struct kmem_cache_s to kmem_cache so we can start using
    it instead of kmem_cache_t typedef.
    
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 09b9aa60063d..d1ea4051b996 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -9,7 +9,7 @@
 
 #if	defined(__KERNEL__)
 
-typedef struct kmem_cache_s kmem_cache_t;
+typedef struct kmem_cache kmem_cache_t;
 
 #include	<linux/config.h>	/* kmalloc_sizes.h needs CONFIG_ options */
 #include	<linux/gfp.h>

commit 6daa0e28627abf362138244a620a821a9027d816
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Fri Oct 21 03:18:50 2005 -0400

    [PATCH] gfp_t: mm/* (easy parts)
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 5fc04a16ecb0..09b9aa60063d 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -121,7 +121,7 @@ extern unsigned int ksize(const void *);
 extern void *kmem_cache_alloc_node(kmem_cache_t *, gfp_t flags, int node);
 extern void *kmalloc_node(size_t size, gfp_t flags, int node);
 #else
-static inline void *kmem_cache_alloc_node(kmem_cache_t *cachep, int flags, int node)
+static inline void *kmem_cache_alloc_node(kmem_cache_t *cachep, gfp_t flags, int node)
 {
 	return kmem_cache_alloc(cachep, flags);
 }

commit dd0fc66fb33cd610bc1a5db8a5e232d34879b4d7
Author: Al Viro <viro@ftp.linux.org.uk>
Date:   Fri Oct 7 07:46:04 2005 +0100

    [PATCH] gfp flags annotations - part 1
    
     - added typedef unsigned int __nocast gfp_t;
    
     - replaced __nocast uses for gfp flags with gfp_t - it gives exactly
       the same warnings as far as sparse is concerned, doesn't change
       generated code (from gcc point of view we replaced unsigned int with
       typedef) and documents what's going on far better.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 1f356f3bbc64..5fc04a16ecb0 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -61,11 +61,11 @@ extern kmem_cache_t *kmem_cache_create(const char *, size_t, size_t, unsigned lo
 				       void (*)(void *, kmem_cache_t *, unsigned long));
 extern int kmem_cache_destroy(kmem_cache_t *);
 extern int kmem_cache_shrink(kmem_cache_t *);
-extern void *kmem_cache_alloc(kmem_cache_t *, unsigned int __nocast);
+extern void *kmem_cache_alloc(kmem_cache_t *, gfp_t);
 extern void kmem_cache_free(kmem_cache_t *, void *);
 extern unsigned int kmem_cache_size(kmem_cache_t *);
 extern const char *kmem_cache_name(kmem_cache_t *);
-extern kmem_cache_t *kmem_find_general_cachep(size_t size, unsigned int __nocast gfpflags);
+extern kmem_cache_t *kmem_find_general_cachep(size_t size, gfp_t gfpflags);
 
 /* Size description struct for general caches. */
 struct cache_sizes {
@@ -74,9 +74,9 @@ struct cache_sizes {
 	kmem_cache_t	*cs_dmacachep;
 };
 extern struct cache_sizes malloc_sizes[];
-extern void *__kmalloc(size_t, unsigned int __nocast);
+extern void *__kmalloc(size_t, gfp_t);
 
-static inline void *kmalloc(size_t size, unsigned int __nocast flags)
+static inline void *kmalloc(size_t size, gfp_t flags)
 {
 	if (__builtin_constant_p(size)) {
 		int i = 0;
@@ -99,7 +99,7 @@ static inline void *kmalloc(size_t size, unsigned int __nocast flags)
 	return __kmalloc(size, flags);
 }
 
-extern void *kzalloc(size_t, unsigned int __nocast);
+extern void *kzalloc(size_t, gfp_t);
 
 /**
  * kcalloc - allocate memory for an array. The memory is set to zero.
@@ -107,7 +107,7 @@ extern void *kzalloc(size_t, unsigned int __nocast);
  * @size: element size.
  * @flags: the type of memory to allocate.
  */
-static inline void *kcalloc(size_t n, size_t size, unsigned int __nocast flags)
+static inline void *kcalloc(size_t n, size_t size, gfp_t flags)
 {
 	if (n != 0 && size > INT_MAX / n)
 		return NULL;
@@ -118,15 +118,14 @@ extern void kfree(const void *);
 extern unsigned int ksize(const void *);
 
 #ifdef CONFIG_NUMA
-extern void *kmem_cache_alloc_node(kmem_cache_t *,
-			unsigned int __nocast flags, int node);
-extern void *kmalloc_node(size_t size, unsigned int __nocast flags, int node);
+extern void *kmem_cache_alloc_node(kmem_cache_t *, gfp_t flags, int node);
+extern void *kmalloc_node(size_t size, gfp_t flags, int node);
 #else
 static inline void *kmem_cache_alloc_node(kmem_cache_t *cachep, int flags, int node)
 {
 	return kmem_cache_alloc(cachep, flags);
 }
-static inline void *kmalloc_node(size_t size, unsigned int __nocast flags, int node)
+static inline void *kmalloc_node(size_t size, gfp_t flags, int node)
 {
 	return kmalloc(size, flags);
 }

commit b2d550736f8b2186b8ef7e206d0bfbfec2238ae8
Author: Victor Fusco <victor@cetuc.puc-rio.br>
Date:   Sat Sep 10 00:26:36 2005 -0700

    [PATCH] mm/slab: fix sparse warnings
    
    Fix the sparse warning "implicit cast to nocast type"
    
    Signed-off-by: Victor Fusco <victor@cetuc.puc-rio.br>
    Signed-off-by: Domen Puncer <domen@coderock.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 42a6bea58af3..1f356f3bbc64 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -118,7 +118,8 @@ extern void kfree(const void *);
 extern unsigned int ksize(const void *);
 
 #ifdef CONFIG_NUMA
-extern void *kmem_cache_alloc_node(kmem_cache_t *, int flags, int node);
+extern void *kmem_cache_alloc_node(kmem_cache_t *,
+			unsigned int __nocast flags, int node);
 extern void *kmalloc_node(size_t size, unsigned int __nocast flags, int node);
 #else
 static inline void *kmem_cache_alloc_node(kmem_cache_t *cachep, int flags, int node)

commit dd3927105b6f65afb7dac17682172cdfb86d3f00
Author: Pekka J Enberg <penberg@cs.Helsinki.FI>
Date:   Tue Sep 6 15:18:31 2005 -0700

    [PATCH] introduce and use kzalloc
    
    This patch introduces a kzalloc wrapper and converts kernel/ to use it.  It
    saves a little program text.
    
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Signed-off-by: Adrian Bunk <bunk@stusta.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 80b2dfde2e80..42a6bea58af3 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -99,7 +99,21 @@ static inline void *kmalloc(size_t size, unsigned int __nocast flags)
 	return __kmalloc(size, flags);
 }
 
-extern void *kcalloc(size_t, size_t, unsigned int __nocast);
+extern void *kzalloc(size_t, unsigned int __nocast);
+
+/**
+ * kcalloc - allocate memory for an array. The memory is set to zero.
+ * @n: number of elements.
+ * @size: element size.
+ * @flags: the type of memory to allocate.
+ */
+static inline void *kcalloc(size_t n, size_t size, unsigned int __nocast flags)
+{
+	if (n != 0 && size > INT_MAX / n)
+		return NULL;
+	return kzalloc(n * size, flags);
+}
+
 extern void kfree(const void *);
 extern unsigned int ksize(const void *);
 

commit c10b873695c6a1de0d8ebab40b525575ca576683
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Wed Jul 27 11:43:27 2005 -0700

    [PATCH] Really __nocast-annotate kmalloc_node()
    
    One chunk was lost somewhere between my and Andrew's machine.
    
    Noticed by Victor Fusco.
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 4c8e552471b0..80b2dfde2e80 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -111,7 +111,7 @@ static inline void *kmem_cache_alloc_node(kmem_cache_t *cachep, int flags, int n
 {
 	return kmem_cache_alloc(cachep, flags);
 }
-static inline void *kmalloc_node(size_t size, int flags, int node)
+static inline void *kmalloc_node(size_t size, unsigned int __nocast flags, int node)
 {
 	return kmalloc(size, flags);
 }

commit 0db925af1db5f3dfe1691c35b39496e2baaff9c9
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Thu Jul 7 17:56:58 2005 -0700

    [PATCH] propagate __nocast annotations
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 76cf7e60216c..4c8e552471b0 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -65,7 +65,7 @@ extern void *kmem_cache_alloc(kmem_cache_t *, unsigned int __nocast);
 extern void kmem_cache_free(kmem_cache_t *, void *);
 extern unsigned int kmem_cache_size(kmem_cache_t *);
 extern const char *kmem_cache_name(kmem_cache_t *);
-extern kmem_cache_t *kmem_find_general_cachep(size_t size, int gfpflags);
+extern kmem_cache_t *kmem_find_general_cachep(size_t size, unsigned int __nocast gfpflags);
 
 /* Size description struct for general caches. */
 struct cache_sizes {
@@ -105,7 +105,7 @@ extern unsigned int ksize(const void *);
 
 #ifdef CONFIG_NUMA
 extern void *kmem_cache_alloc_node(kmem_cache_t *, int flags, int node);
-extern void *kmalloc_node(size_t size, int flags, int node);
+extern void *kmalloc_node(size_t size, unsigned int __nocast flags, int node);
 #else
 static inline void *kmem_cache_alloc_node(kmem_cache_t *cachep, int flags, int node)
 {

commit 1944972d3bb651474a5021c9da8d0166ae19f1eb
Author: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
Date:   Sat Jun 18 22:46:19 2005 -0700

    [SLAB] Introduce kmem_cache_name
    
    This is for use with slab users that pass a dynamically allocated slab name in
    kmem_cache_create, so that before destroying the slab one can retrieve the name
    and free its memory.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 7d66385ae750..76cf7e60216c 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -64,6 +64,7 @@ extern int kmem_cache_shrink(kmem_cache_t *);
 extern void *kmem_cache_alloc(kmem_cache_t *, unsigned int __nocast);
 extern void kmem_cache_free(kmem_cache_t *, void *);
 extern unsigned int kmem_cache_size(kmem_cache_t *);
+extern const char *kmem_cache_name(kmem_cache_t *);
 extern kmem_cache_t *kmem_find_general_cachep(size_t size, int gfpflags);
 
 /* Size description struct for general caches. */

commit 97e2bde47f886a317909c8a8f9bd2fcd8ce2f0b0
Author: Manfred Spraul <manfred@dbl.q-ag.de>
Date:   Sun May 1 08:58:38 2005 -0700

    [PATCH] add kmalloc_node, inline cleanup
    
    The patch makes the following function calls available to allocate memory
    on a specific node without changing the basic operation of the slab
    allocator:
    
     kmem_cache_alloc_node(kmem_cache_t *cachep, unsigned int flags, int node);
     kmalloc_node(size_t size, unsigned int flags, int node);
    
    in a similar way to the existing node-blind functions:
    
     kmem_cache_alloc(kmem_cache_t *cachep, unsigned int flags);
     kmalloc(size, flags);
    
    kmem_cache_alloc_node was changed to pass flags and the node information
    through the existing layers of the slab allocator (which lead to some minor
    rearrangements).  The functions at the lowest layer (kmem_getpages,
    cache_grow) are already node aware.  Also __alloc_percpu can call
    kmalloc_node now.
    
    Performance measurements (using the pageset localization patch) yields:
    
    w/o patches:
    Tasks    jobs/min  jti  jobs/min/task      real       cpu
        1      484.27  100       484.2736     12.02      1.97   Wed Mar 30 20:50:43 2005
      100    25170.83   91       251.7083     23.12    150.10   Wed Mar 30 20:51:06 2005
      200    34601.66   84       173.0083     33.64    294.14   Wed Mar 30 20:51:40 2005
      300    37154.47   86       123.8482     46.99    436.56   Wed Mar 30 20:52:28 2005
      400    39839.82   80        99.5995     58.43    580.46   Wed Mar 30 20:53:27 2005
      500    40036.32   79        80.0726     72.68    728.60   Wed Mar 30 20:54:40 2005
      600    44074.21   79        73.4570     79.23    872.10   Wed Mar 30 20:55:59 2005
      700    44016.60   78        62.8809     92.56   1015.84   Wed Mar 30 20:57:32 2005
      800    40411.05   80        50.5138    115.22   1161.13   Wed Mar 30 20:59:28 2005
      900    42298.56   79        46.9984    123.83   1303.42   Wed Mar 30 21:01:33 2005
     1000    40955.05   80        40.9551    142.11   1441.92   Wed Mar 30 21:03:55 2005
    
    with pageset localization and slab API patches:
    Tasks    jobs/min  jti  jobs/min/task      real       cpu
        1      484.19  100       484.1930     12.02      1.98   Wed Mar 30 21:10:18 2005
      100    27428.25   92       274.2825     21.22    149.79   Wed Mar 30 21:10:40 2005
      200    37228.94   86       186.1447     31.27    293.49   Wed Mar 30 21:11:12 2005
      300    41725.42   85       139.0847     41.84    434.10   Wed Mar 30 21:11:54 2005
      400    43032.22   82       107.5805     54.10    582.06   Wed Mar 30 21:12:48 2005
      500    42211.23   83        84.4225     68.94    722.61   Wed Mar 30 21:13:58 2005
      600    40084.49   82        66.8075     87.12    873.11   Wed Mar 30 21:15:25 2005
      700    44169.30   79        63.0990     92.24   1008.77   Wed Mar 30 21:16:58 2005
      800    43097.94   79        53.8724    108.03   1155.88   Wed Mar 30 21:18:47 2005
      900    41846.75   79        46.4964    125.17   1303.38   Wed Mar 30 21:20:52 2005
     1000    40247.85   79        40.2478    144.60   1442.21   Wed Mar 30 21:23:17 2005
    
    Signed-off-by: Christoph Lameter <christoph@lameter.com>
    Signed-off-by: Manfred Spraul <manfred@colorfullife.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/slab.h b/include/linux/slab.h
index 3e3c3ab8ff94..7d66385ae750 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -62,16 +62,9 @@ extern kmem_cache_t *kmem_cache_create(const char *, size_t, size_t, unsigned lo
 extern int kmem_cache_destroy(kmem_cache_t *);
 extern int kmem_cache_shrink(kmem_cache_t *);
 extern void *kmem_cache_alloc(kmem_cache_t *, unsigned int __nocast);
-#ifdef CONFIG_NUMA
-extern void *kmem_cache_alloc_node(kmem_cache_t *, int);
-#else
-static inline void *kmem_cache_alloc_node(kmem_cache_t *cachep, int node)
-{
-	return kmem_cache_alloc(cachep, GFP_KERNEL);
-}
-#endif
 extern void kmem_cache_free(kmem_cache_t *, void *);
 extern unsigned int kmem_cache_size(kmem_cache_t *);
+extern kmem_cache_t *kmem_find_general_cachep(size_t size, int gfpflags);
 
 /* Size description struct for general caches. */
 struct cache_sizes {
@@ -109,6 +102,20 @@ extern void *kcalloc(size_t, size_t, unsigned int __nocast);
 extern void kfree(const void *);
 extern unsigned int ksize(const void *);
 
+#ifdef CONFIG_NUMA
+extern void *kmem_cache_alloc_node(kmem_cache_t *, int flags, int node);
+extern void *kmalloc_node(size_t size, int flags, int node);
+#else
+static inline void *kmem_cache_alloc_node(kmem_cache_t *cachep, int flags, int node)
+{
+	return kmem_cache_alloc(cachep, flags);
+}
+static inline void *kmalloc_node(size_t size, int flags, int node)
+{
+	return kmalloc(size, flags);
+}
+#endif
+
 extern int FASTCALL(kmem_cache_reap(int));
 extern int FASTCALL(kmem_ptr_validate(kmem_cache_t *cachep, void *ptr));
 

commit 1da177e4c3f41524e886b7f1b8a0c1fc7321cac2
Author: Linus Torvalds <torvalds@ppc970.osdl.org>
Date:   Sat Apr 16 15:20:36 2005 -0700

    Linux-2.6.12-rc2
    
    Initial git repository build. I'm not bothering with the full history,
    even though we have it. We can create a separate "historical" git
    archive of that later if we want to, and in the meantime it's about
    3.2GB when imported into git - space that would just make the early
    git days unnecessarily complicated, when we don't have a lot of good
    infrastructure for it.
    
    Let it rip!

diff --git a/include/linux/slab.h b/include/linux/slab.h
new file mode 100644
index 000000000000..3e3c3ab8ff94
--- /dev/null
+++ b/include/linux/slab.h
@@ -0,0 +1,129 @@
+/*
+ * linux/mm/slab.h
+ * Written by Mark Hemment, 1996.
+ * (markhe@nextd.demon.co.uk)
+ */
+
+#ifndef _LINUX_SLAB_H
+#define	_LINUX_SLAB_H
+
+#if	defined(__KERNEL__)
+
+typedef struct kmem_cache_s kmem_cache_t;
+
+#include	<linux/config.h>	/* kmalloc_sizes.h needs CONFIG_ options */
+#include	<linux/gfp.h>
+#include	<linux/init.h>
+#include	<linux/types.h>
+#include	<asm/page.h>		/* kmalloc_sizes.h needs PAGE_SIZE */
+#include	<asm/cache.h>		/* kmalloc_sizes.h needs L1_CACHE_BYTES */
+
+/* flags for kmem_cache_alloc() */
+#define	SLAB_NOFS		GFP_NOFS
+#define	SLAB_NOIO		GFP_NOIO
+#define	SLAB_ATOMIC		GFP_ATOMIC
+#define	SLAB_USER		GFP_USER
+#define	SLAB_KERNEL		GFP_KERNEL
+#define	SLAB_DMA		GFP_DMA
+
+#define SLAB_LEVEL_MASK		GFP_LEVEL_MASK
+
+#define	SLAB_NO_GROW		__GFP_NO_GROW	/* don't grow a cache */
+
+/* flags to pass to kmem_cache_create().
+ * The first 3 are only valid when the allocator as been build
+ * SLAB_DEBUG_SUPPORT.
+ */
+#define	SLAB_DEBUG_FREE		0x00000100UL	/* Peform (expensive) checks on free */
+#define	SLAB_DEBUG_INITIAL	0x00000200UL	/* Call constructor (as verifier) */
+#define	SLAB_RED_ZONE		0x00000400UL	/* Red zone objs in a cache */
+#define	SLAB_POISON		0x00000800UL	/* Poison objects */
+#define	SLAB_NO_REAP		0x00001000UL	/* never reap from the cache */
+#define	SLAB_HWCACHE_ALIGN	0x00002000UL	/* align objs on a h/w cache lines */
+#define SLAB_CACHE_DMA		0x00004000UL	/* use GFP_DMA memory */
+#define SLAB_MUST_HWCACHE_ALIGN	0x00008000UL	/* force alignment */
+#define SLAB_STORE_USER		0x00010000UL	/* store the last owner for bug hunting */
+#define SLAB_RECLAIM_ACCOUNT	0x00020000UL	/* track pages allocated to indicate
+						   what is reclaimable later*/
+#define SLAB_PANIC		0x00040000UL	/* panic if kmem_cache_create() fails */
+#define SLAB_DESTROY_BY_RCU	0x00080000UL	/* defer freeing pages to RCU */
+
+/* flags passed to a constructor func */
+#define	SLAB_CTOR_CONSTRUCTOR	0x001UL		/* if not set, then deconstructor */
+#define SLAB_CTOR_ATOMIC	0x002UL		/* tell constructor it can't sleep */
+#define	SLAB_CTOR_VERIFY	0x004UL		/* tell constructor it's a verify call */
+
+/* prototypes */
+extern void __init kmem_cache_init(void);
+
+extern kmem_cache_t *kmem_cache_create(const char *, size_t, size_t, unsigned long,
+				       void (*)(void *, kmem_cache_t *, unsigned long),
+				       void (*)(void *, kmem_cache_t *, unsigned long));
+extern int kmem_cache_destroy(kmem_cache_t *);
+extern int kmem_cache_shrink(kmem_cache_t *);
+extern void *kmem_cache_alloc(kmem_cache_t *, unsigned int __nocast);
+#ifdef CONFIG_NUMA
+extern void *kmem_cache_alloc_node(kmem_cache_t *, int);
+#else
+static inline void *kmem_cache_alloc_node(kmem_cache_t *cachep, int node)
+{
+	return kmem_cache_alloc(cachep, GFP_KERNEL);
+}
+#endif
+extern void kmem_cache_free(kmem_cache_t *, void *);
+extern unsigned int kmem_cache_size(kmem_cache_t *);
+
+/* Size description struct for general caches. */
+struct cache_sizes {
+	size_t		 cs_size;
+	kmem_cache_t	*cs_cachep;
+	kmem_cache_t	*cs_dmacachep;
+};
+extern struct cache_sizes malloc_sizes[];
+extern void *__kmalloc(size_t, unsigned int __nocast);
+
+static inline void *kmalloc(size_t size, unsigned int __nocast flags)
+{
+	if (__builtin_constant_p(size)) {
+		int i = 0;
+#define CACHE(x) \
+		if (size <= x) \
+			goto found; \
+		else \
+			i++;
+#include "kmalloc_sizes.h"
+#undef CACHE
+		{
+			extern void __you_cannot_kmalloc_that_much(void);
+			__you_cannot_kmalloc_that_much();
+		}
+found:
+		return kmem_cache_alloc((flags & GFP_DMA) ?
+			malloc_sizes[i].cs_dmacachep :
+			malloc_sizes[i].cs_cachep, flags);
+	}
+	return __kmalloc(size, flags);
+}
+
+extern void *kcalloc(size_t, size_t, unsigned int __nocast);
+extern void kfree(const void *);
+extern unsigned int ksize(const void *);
+
+extern int FASTCALL(kmem_cache_reap(int));
+extern int FASTCALL(kmem_ptr_validate(kmem_cache_t *cachep, void *ptr));
+
+/* System wide caches */
+extern kmem_cache_t	*vm_area_cachep;
+extern kmem_cache_t	*names_cachep;
+extern kmem_cache_t	*files_cachep;
+extern kmem_cache_t	*filp_cachep;
+extern kmem_cache_t	*fs_cachep;
+extern kmem_cache_t	*signal_cachep;
+extern kmem_cache_t	*sighand_cachep;
+extern kmem_cache_t	*bio_cachep;
+
+extern atomic_t slab_reclaim_pages;
+
+#endif	/* __KERNEL__ */
+
+#endif	/* _LINUX_SLAB_H */
