commit bf0beec0607db3c6f6fb7bd2c6d503792b05cf3f
Author: Ming Lei <ming.lei@redhat.com>
Date:   Fri May 29 15:53:15 2020 +0200

    blk-mq: drain I/O when all CPUs in a hctx are offline
    
    Most of blk-mq drivers depend on managed IRQ's auto-affinity to setup
    up queue mapping. Thomas mentioned the following point[1]:
    
    "That was the constraint of managed interrupts from the very beginning:
    
     The driver/subsystem has to quiesce the interrupt line and the associated
     queue _before_ it gets shutdown in CPU unplug and not fiddle with it
     until it's restarted by the core when the CPU is plugged in again."
    
    However, current blk-mq implementation doesn't quiesce hw queue before
    the last CPU in the hctx is shutdown.  Even worse, CPUHP_BLK_MQ_DEAD is a
    cpuhp state handled after the CPU is down, so there isn't any chance to
    quiesce the hctx before shutting down the CPU.
    
    Add new CPUHP_AP_BLK_MQ_ONLINE state to stop allocating from blk-mq hctxs
    where the last CPU goes away, and wait for completion of in-flight
    requests.  This guarantees that there is no inflight I/O before shutting
    down the managed IRQ.
    
    Add a BLK_MQ_F_STACKING and set it for dm-rq and loop, so we don't need
    to wait for completion of in-flight requests from these drivers to avoid
    a potential dead-lock. It is safe to do this for stacking drivers as those
    do not use interrupts at all and their I/O completions are triggered by
    underlying devices I/O completion.
    
    [1] https://lore.kernel.org/linux-block/alpine.DEB.2.21.1904051331270.1802@nanos.tec.linutronix.de/
    
    [hch: different retry mechanism, merged two patches, minor cleanups]
    
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Hannes Reinecke <hare@suse.de>
    Reviewed-by: Daniel Wagner <dwagner@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 856bb10993cf..d6fcae17da5a 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -140,6 +140,8 @@ struct blk_mq_hw_ctx {
 	 */
 	atomic_t		nr_active;
 
+	/** @cpuhp_online: List to store request if CPU is going to die */
+	struct hlist_node	cpuhp_online;
 	/** @cpuhp_dead: List to store request if some CPU die. */
 	struct hlist_node	cpuhp_dead;
 	/** @kobj: Kernel object for sysfs. */
@@ -391,6 +393,11 @@ struct blk_mq_ops {
 enum {
 	BLK_MQ_F_SHOULD_MERGE	= 1 << 0,
 	BLK_MQ_F_TAG_SHARED	= 1 << 1,
+	/*
+	 * Set when this device requires underlying blk-mq device for
+	 * completing IO:
+	 */
+	BLK_MQ_F_STACKING	= 1 << 2,
 	BLK_MQ_F_BLOCKING	= 1 << 5,
 	BLK_MQ_F_NO_SCHED	= 1 << 6,
 	BLK_MQ_F_ALLOC_POLICY_START_BIT = 8,
@@ -400,6 +407,9 @@ enum {
 	BLK_MQ_S_TAG_ACTIVE	= 1,
 	BLK_MQ_S_SCHED_RESTART	= 2,
 
+	/* hw queue is inactive after all its CPUs become offline */
+	BLK_MQ_S_INACTIVE	= 3,
+
 	BLK_MQ_MAX_DEPTH	= 10240,
 
 	BLK_MQ_CPU_WORK_BATCH	= 8,

commit 7b11eab041dacfeaaa6d27d9183b247a995bc16d
Author: Keith Busch <kbusch@kernel.org>
Date:   Fri May 29 07:51:59 2020 -0700

    blk-mq: blk-mq: provide forced completion method
    
    Drivers may need to bypass error injection for error recovery. Rename
    __blk_mq_complete_request() to blk_mq_force_complete_rq() and export
    that function so drivers may skip potential fake timeouts after they've
    reclaimed lost requests.
    
    Signed-off-by: Keith Busch <kbusch@kernel.org>
    Reviewed-by: Daniel Wagner <dwagner@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index d7307795439a..856bb10993cf 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -494,6 +494,7 @@ void blk_mq_requeue_request(struct request *rq, bool kick_requeue_list);
 void blk_mq_kick_requeue_list(struct request_queue *q);
 void blk_mq_delay_kick_requeue_list(struct request_queue *q, unsigned long msecs);
 bool blk_mq_complete_request(struct request *rq);
+void blk_mq_force_complete_rq(struct request *rq);
 bool blk_mq_bio_list_merge(struct request_queue *q, struct list_head *list,
 			   struct bio *bio, unsigned int nr_segs);
 bool blk_mq_queue_stopped(struct request_queue *q);

commit 8cf7961dab42c9177a556b719c15f5b9449c24d1
Author: Christoph Hellwig <hch@lst.de>
Date:   Sat Apr 25 09:53:36 2020 +0200

    block: bypass ->make_request_fn for blk-mq drivers
    
    Call blk_mq_make_request when no ->make_request_fn is set.  This is
    safe now that blk_alloc_queue always sets up the pointer for make_request
    based drivers.  This avoids an indirect call in the blk-mq driver I/O
    fast path, which is rather expensive due to spectre mitigations.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 51fbf6f76593..d7307795439a 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -578,4 +578,6 @@ static inline void blk_mq_cleanup_rq(struct request *rq)
 		rq->q->mq_ops->cleanup_rq(rq);
 }
 
+blk_qc_t blk_mq_make_request(struct request_queue *q, struct bio *bio);
+
 #endif

commit b9151e7bca82a17ff7aa442a168e0f924a07443c
Author: Douglas Anderson <dianders@chromium.org>
Date:   Mon Apr 20 09:24:52 2020 -0700

    blk-mq: Add blk_mq_delay_run_hw_queues() API call
    
    We have:
    * blk_mq_run_hw_queue()
    * blk_mq_delay_run_hw_queue()
    * blk_mq_run_hw_queues()
    
    ...but not blk_mq_delay_run_hw_queues(), presumably because nobody
    needed it before now.  Since we need it for a later patch in this
    series, add it.
    
    Signed-off-by: Douglas Anderson <dianders@chromium.org>
    Reviewed-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index b45148ba3291..51fbf6f76593 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -508,6 +508,7 @@ void blk_mq_unquiesce_queue(struct request_queue *q);
 void blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, unsigned long msecs);
 void blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async);
 void blk_mq_run_hw_queues(struct request_queue *q, bool async);
+void blk_mq_delay_run_hw_queues(struct request_queue *q, unsigned long msecs);
 void blk_mq_tagset_busy_iter(struct blk_mq_tag_set *tagset,
 		busy_tag_iter_fn *fn, void *priv);
 void blk_mq_tagset_wait_completed_request(struct blk_mq_tag_set *tagset);

commit f36aaf8be421099103193c49796a14213d3be315
Author: Gustavo A. R. Silva <gustavo@embeddedor.com>
Date:   Mon Mar 23 16:43:39 2020 -0500

    blk-mq: Replace zero-length array with flexible-array member
    
    The current codebase makes use of the zero-length array language
    extension to the C90 standard, but the preferred mechanism to declare
    variable-length types such as these ones is a flexible array member[1][2],
    introduced in C99:
    
    struct foo {
            int stuff;
            struct boo array[];
    };
    
    By making use of the mechanism above, we will get a compiler warning
    in case the flexible array does not occur last in the structure, which
    will help us prevent some kind of undefined behavior bugs from being
    inadvertently introduced[3] to the codebase from now on.
    
    Also, notice that, dynamic memory allocations won't be affected by
    this change:
    
    "Flexible array members have incomplete type, and so the sizeof operator
    may not be applied. As a quirk of the original implementation of
    zero-length arrays, sizeof evaluates to zero."[1]
    
    This issue was found with the help of Coccinelle.
    
    [1] https://gcc.gnu.org/onlinedocs/gcc/Zero-Length.html
    [2] https://github.com/KSPP/linux/issues/21
    [3] commit 76497732932f ("cxgb3/l2t: Fix undefined behaviour")
    
    Signed-off-by: Gustavo A. R. Silva <gustavo@embeddedor.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index f389d7c724bd..b45148ba3291 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -173,7 +173,7 @@ struct blk_mq_hw_ctx {
 	 * blocking (BLK_MQ_F_BLOCKING). Must be the last member - see also
 	 * blk_mq_hw_ctx_size().
 	 */
-	struct srcu_struct	srcu[0];
+	struct srcu_struct	srcu[];
 };
 
 /**

commit 2f227bb99934a4faa6dfe2cda2594bce8897a323
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Mar 27 09:30:08 2020 +0100

    block: add a blk_mq_init_queue_data helper
    
    This allows a driver to pass a queuedata member before ->init_hctx is
    called.  null_blk currently open codes this logic, but I'd rather have
    it in the core to ease future maintainance.
    
    Reviewed-by: Johannes Thumshirn <johannes.thumshirn@wdc.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 31344d5f83e2..f389d7c724bd 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -412,6 +412,8 @@ enum {
 		<< BLK_MQ_F_ALLOC_POLICY_START_BIT)
 
 struct request_queue *blk_mq_init_queue(struct blk_mq_tag_set *);
+struct request_queue *blk_mq_init_queue_data(struct blk_mq_tag_set *set,
+		void *queuedata);
 struct request_queue *blk_mq_init_allocated_queue(struct blk_mq_tag_set *set,
 						  struct request_queue *q,
 						  bool elevator_init);

commit 2dd209f00fc5a1caafa493066c7cd692fd2fd57c
Author: Bart Van Assche <bvanassche@acm.org>
Date:   Mon Mar 9 21:26:16 2020 -0700

    blk-mq: Fix a comment in include/linux/blk-mq.h
    
    The 'hctx_list' member of struct blk_mq_hw_ctx is not a list head but
    instead an entry in q->unused_hctx_list. Fix the comment above this
    struct member.
    
    Fixes: d386732bc142 ("blk-mq: fill header with kernel-doc")
    Signed-off-by: Bart Van Assche <bvanassche@acm.org>
    Reviewed-by: Chaitanya Kulkarni <chaitanya.kulkarni@wdc.com>
    Cc: Andr√© Almeida <andrealmeid@collabora.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 11cfd6470b1a..31344d5f83e2 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -162,7 +162,10 @@ struct blk_mq_hw_ctx {
 	struct dentry		*sched_debugfs_dir;
 #endif
 
-	/** @hctx_list:	List of all hardware queues. */
+	/**
+	 * @hctx_list: if this hctx is not in use, this is an entry in
+	 * q->unused_hctx_list.
+	 */
 	struct list_head	hctx_list;
 
 	/**

commit cb711b91a3c685192f2cabd3735ca3de04694ed8
Author: John Garry <john.garry@huawei.com>
Date:   Thu Nov 14 01:27:21 2019 +0800

    blk-mq: Delete blk_mq_has_free_tags() and blk_mq_can_queue()
    
    These functions are not referenced, so delete them.
    
    Signed-off-by: John Garry <john.garry@huawei.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index dc03e059fdff..11cfd6470b1a 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -424,7 +424,6 @@ void blk_mq_free_tag_set(struct blk_mq_tag_set *set);
 void blk_mq_flush_plug_list(struct blk_plug *plug, bool from_schedule);
 
 void blk_mq_free_request(struct request *rq);
-bool blk_mq_can_queue(struct blk_mq_hw_ctx *);
 
 bool blk_mq_queue_inflight(struct request_queue *q);
 

commit 626fb735a43ddcb7b2c58c27cb03b098acc03339
Author: John Garry <john.garry@huawei.com>
Date:   Wed Oct 30 00:59:30 2019 +0800

    blk-mq: Make blk_mq_run_hw_queue() return void
    
    Since commit 97889f9ac24f ("blk-mq: remove synchronize_rcu() from
    blk_mq_del_queue_tag_set()"), the return value of blk_mq_run_hw_queue()
    is never checked, so make it return void, which very marginally simplifies
    the code.
    
    Reviewed-by: Bob Liu <bob.liu@oracle.com>
    Signed-off-by: John Garry <john.garry@huawei.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 4919d22e1aff..dc03e059fdff 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -502,7 +502,7 @@ void blk_mq_start_stopped_hw_queues(struct request_queue *q, bool async);
 void blk_mq_quiesce_queue(struct request_queue *q);
 void blk_mq_unquiesce_queue(struct request_queue *q);
 void blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, unsigned long msecs);
-bool blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async);
+void blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async);
 void blk_mq_run_hw_queues(struct request_queue *q, bool async);
 void blk_mq_tagset_busy_iter(struct blk_mq_tag_set *tagset,
 		busy_tag_iter_fn *fn, void *priv);

commit d386732bc142c63b9f676fed098bc06f91ee964a
Author: Andr√© Almeida <andrealmeid@collabora.com>
Date:   Mon Oct 21 21:07:24 2019 -0300

    blk-mq: fill header with kernel-doc
    
    Insert documentation for structs, enums and functions at header file.
    Format existing and new comments at struct blk_mq_ops as
    kernel-doc comments.
    
    Reviewed-by: Bart Van Assche <bvanassche@acm.org>
    Signed-off-by: Andr√© Almeida <andrealmeid@collabora.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index e0fce93ac127..4919d22e1aff 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -10,74 +10,171 @@ struct blk_mq_tags;
 struct blk_flush_queue;
 
 /**
- * struct blk_mq_hw_ctx - State for a hardware queue facing the hardware block device
+ * struct blk_mq_hw_ctx - State for a hardware queue facing the hardware
+ * block device
  */
 struct blk_mq_hw_ctx {
 	struct {
+		/** @lock: Protects the dispatch list. */
 		spinlock_t		lock;
+		/**
+		 * @dispatch: Used for requests that are ready to be
+		 * dispatched to the hardware but for some reason (e.g. lack of
+		 * resources) could not be sent to the hardware. As soon as the
+		 * driver can send new requests, requests at this list will
+		 * be sent first for a fairer dispatch.
+		 */
 		struct list_head	dispatch;
-		unsigned long		state;		/* BLK_MQ_S_* flags */
+		 /**
+		  * @state: BLK_MQ_S_* flags. Defines the state of the hw
+		  * queue (active, scheduled to restart, stopped).
+		  */
+		unsigned long		state;
 	} ____cacheline_aligned_in_smp;
 
+	/**
+	 * @run_work: Used for scheduling a hardware queue run at a later time.
+	 */
 	struct delayed_work	run_work;
+	/** @cpumask: Map of available CPUs where this hctx can run. */
 	cpumask_var_t		cpumask;
+	/**
+	 * @next_cpu: Used by blk_mq_hctx_next_cpu() for round-robin CPU
+	 * selection from @cpumask.
+	 */
 	int			next_cpu;
+	/**
+	 * @next_cpu_batch: Counter of how many works left in the batch before
+	 * changing to the next CPU.
+	 */
 	int			next_cpu_batch;
 
-	unsigned long		flags;		/* BLK_MQ_F_* flags */
+	/** @flags: BLK_MQ_F_* flags. Defines the behaviour of the queue. */
+	unsigned long		flags;
 
+	/**
+	 * @sched_data: Pointer owned by the IO scheduler attached to a request
+	 * queue. It's up to the IO scheduler how to use this pointer.
+	 */
 	void			*sched_data;
+	/**
+	 * @queue: Pointer to the request queue that owns this hardware context.
+	 */
 	struct request_queue	*queue;
+	/** @fq: Queue of requests that need to perform a flush operation. */
 	struct blk_flush_queue	*fq;
 
+	/**
+	 * @driver_data: Pointer to data owned by the block driver that created
+	 * this hctx
+	 */
 	void			*driver_data;
 
+	/**
+	 * @ctx_map: Bitmap for each software queue. If bit is on, there is a
+	 * pending request in that software queue.
+	 */
 	struct sbitmap		ctx_map;
 
+	/**
+	 * @dispatch_from: Software queue to be used when no scheduler was
+	 * selected.
+	 */
 	struct blk_mq_ctx	*dispatch_from;
+	/**
+	 * @dispatch_busy: Number used by blk_mq_update_dispatch_busy() to
+	 * decide if the hw_queue is busy using Exponential Weighted Moving
+	 * Average algorithm.
+	 */
 	unsigned int		dispatch_busy;
 
+	/** @type: HCTX_TYPE_* flags. Type of hardware queue. */
 	unsigned short		type;
+	/** @nr_ctx: Number of software queues. */
 	unsigned short		nr_ctx;
+	/** @ctxs: Array of software queues. */
 	struct blk_mq_ctx	**ctxs;
 
+	/** @dispatch_wait_lock: Lock for dispatch_wait queue. */
 	spinlock_t		dispatch_wait_lock;
+	/**
+	 * @dispatch_wait: Waitqueue to put requests when there is no tag
+	 * available at the moment, to wait for another try in the future.
+	 */
 	wait_queue_entry_t	dispatch_wait;
+
+	/**
+	 * @wait_index: Index of next available dispatch_wait queue to insert
+	 * requests.
+	 */
 	atomic_t		wait_index;
 
+	/**
+	 * @tags: Tags owned by the block driver. A tag at this set is only
+	 * assigned when a request is dispatched from a hardware queue.
+	 */
 	struct blk_mq_tags	*tags;
+	/**
+	 * @sched_tags: Tags owned by I/O scheduler. If there is an I/O
+	 * scheduler associated with a request queue, a tag is assigned when
+	 * that request is allocated. Else, this member is not used.
+	 */
 	struct blk_mq_tags	*sched_tags;
 
+	/** @queued: Number of queued requests. */
 	unsigned long		queued;
+	/** @run: Number of dispatched requests. */
 	unsigned long		run;
 #define BLK_MQ_MAX_DISPATCH_ORDER	7
+	/** @dispatched: Number of dispatch requests by queue. */
 	unsigned long		dispatched[BLK_MQ_MAX_DISPATCH_ORDER];
 
+	/** @numa_node: NUMA node the storage adapter has been connected to. */
 	unsigned int		numa_node;
+	/** @queue_num: Index of this hardware queue. */
 	unsigned int		queue_num;
 
+	/**
+	 * @nr_active: Number of active requests. Only used when a tag set is
+	 * shared across request queues.
+	 */
 	atomic_t		nr_active;
 
+	/** @cpuhp_dead: List to store request if some CPU die. */
 	struct hlist_node	cpuhp_dead;
+	/** @kobj: Kernel object for sysfs. */
 	struct kobject		kobj;
 
+	/** @poll_considered: Count times blk_poll() was called. */
 	unsigned long		poll_considered;
+	/** @poll_invoked: Count how many requests blk_poll() polled. */
 	unsigned long		poll_invoked;
+	/** @poll_success: Count how many polled requests were completed. */
 	unsigned long		poll_success;
 
 #ifdef CONFIG_BLK_DEBUG_FS
+	/**
+	 * @debugfs_dir: debugfs directory for this hardware queue. Named
+	 * as cpu<cpu_number>.
+	 */
 	struct dentry		*debugfs_dir;
+	/** @sched_debugfs_dir:	debugfs directory for the scheduler. */
 	struct dentry		*sched_debugfs_dir;
 #endif
 
+	/** @hctx_list:	List of all hardware queues. */
 	struct list_head	hctx_list;
 
-	/* Must be the last member - see also blk_mq_hw_ctx_size(). */
+	/**
+	 * @srcu: Sleepable RCU. Use as lock when type of the hardware queue is
+	 * blocking (BLK_MQ_F_BLOCKING). Must be the last member - see also
+	 * blk_mq_hw_ctx_size().
+	 */
 	struct srcu_struct	srcu[0];
 };
 
 /**
- * struct blk_mq_queue_map - ctx -> hctx mapping
+ * struct blk_mq_queue_map - Map software queues to hardware queues
  * @mq_map:       CPU ID to hardware queue index map. This is an array
  *	with nr_cpu_ids elements. Each element has a value in the range
  *	[@queue_offset, @queue_offset + @nr_queues).
@@ -92,10 +189,17 @@ struct blk_mq_queue_map {
 	unsigned int queue_offset;
 };
 
+/**
+ * enum hctx_type - Type of hardware queue
+ * @HCTX_TYPE_DEFAULT:	All I/O not otherwise accounted for.
+ * @HCTX_TYPE_READ:	Just for READ I/O.
+ * @HCTX_TYPE_POLL:	Polled I/O of any kind.
+ * @HCTX_MAX_TYPES:	Number of types of hctx.
+ */
 enum hctx_type {
-	HCTX_TYPE_DEFAULT,	/* all I/O not otherwise accounted for */
-	HCTX_TYPE_READ,		/* just for READ I/O */
-	HCTX_TYPE_POLL,		/* polled I/O of any kind */
+	HCTX_TYPE_DEFAULT,
+	HCTX_TYPE_READ,
+	HCTX_TYPE_POLL,
 
 	HCTX_MAX_TYPES,
 };
@@ -147,6 +251,12 @@ struct blk_mq_tag_set {
 	struct list_head	tag_list;
 };
 
+/**
+ * struct blk_mq_queue_data - Data about a request inserted in a queue
+ *
+ * @rq:   Request pointer.
+ * @last: If it is the last request in the queue.
+ */
 struct blk_mq_queue_data {
 	struct request *rq;
 	bool last;
@@ -174,81 +284,101 @@ typedef bool (busy_fn)(struct request_queue *);
 typedef void (complete_fn)(struct request *);
 typedef void (cleanup_rq_fn)(struct request *);
 
-
+/**
+ * struct blk_mq_ops - Callback functions that implements block driver
+ * behaviour.
+ */
 struct blk_mq_ops {
-	/*
-	 * Queue request
+	/**
+	 * @queue_rq: Queue a new request from block IO.
 	 */
 	queue_rq_fn		*queue_rq;
 
-	/*
-	 * If a driver uses bd->last to judge when to submit requests to
-	 * hardware, it must define this function. In case of errors that
-	 * make us stop issuing further requests, this hook serves the
+	/**
+	 * @commit_rqs: If a driver uses bd->last to judge when to submit
+	 * requests to hardware, it must define this function. In case of errors
+	 * that make us stop issuing further requests, this hook serves the
 	 * purpose of kicking the hardware (which the last request otherwise
 	 * would have done).
 	 */
 	commit_rqs_fn		*commit_rqs;
 
-	/*
-	 * Reserve budget before queue request, once .queue_rq is
+	/**
+	 * @get_budget: Reserve budget before queue request, once .queue_rq is
 	 * run, it is driver's responsibility to release the
 	 * reserved budget. Also we have to handle failure case
 	 * of .get_budget for avoiding I/O deadlock.
 	 */
 	get_budget_fn		*get_budget;
+	/**
+	 * @put_budget: Release the reserved budget.
+	 */
 	put_budget_fn		*put_budget;
 
-	/*
-	 * Called on request timeout
+	/**
+	 * @timeout: Called on request timeout.
 	 */
 	timeout_fn		*timeout;
 
-	/*
-	 * Called to poll for completion of a specific tag.
+	/**
+	 * @poll: Called to poll for completion of a specific tag.
 	 */
 	poll_fn			*poll;
 
+	/**
+	 * @complete: Mark the request as complete.
+	 */
 	complete_fn		*complete;
 
-	/*
-	 * Called when the block layer side of a hardware queue has been
-	 * set up, allowing the driver to allocate/init matching structures.
-	 * Ditto for exit/teardown.
+	/**
+	 * @init_hctx: Called when the block layer side of a hardware queue has
+	 * been set up, allowing the driver to allocate/init matching
+	 * structures.
 	 */
 	init_hctx_fn		*init_hctx;
+	/**
+	 * @exit_hctx: Ditto for exit/teardown.
+	 */
 	exit_hctx_fn		*exit_hctx;
 
-	/*
-	 * Called for every command allocated by the block layer to allow
-	 * the driver to set up driver specific data.
+	/**
+	 * @init_request: Called for every command allocated by the block layer
+	 * to allow the driver to set up driver specific data.
 	 *
 	 * Tag greater than or equal to queue_depth is for setting up
 	 * flush request.
-	 *
-	 * Ditto for exit/teardown.
 	 */
 	init_request_fn		*init_request;
+	/**
+	 * @exit_request: Ditto for exit/teardown.
+	 */
 	exit_request_fn		*exit_request;
-	/* Called from inside blk_get_request() */
+
+	/**
+	 * @initialize_rq_fn: Called from inside blk_get_request().
+	 */
 	void (*initialize_rq_fn)(struct request *rq);
 
-	/*
-	 * Called before freeing one request which isn't completed yet,
-	 * and usually for freeing the driver private data
+	/**
+	 * @cleanup_rq: Called before freeing one request which isn't completed
+	 * yet, and usually for freeing the driver private data.
 	 */
 	cleanup_rq_fn		*cleanup_rq;
 
-	/*
-	 * If set, returns whether or not this queue currently is busy
+	/**
+	 * @busy: If set, returns whether or not this queue currently is busy.
 	 */
 	busy_fn			*busy;
 
+	/**
+	 * @map_queues: This allows drivers specify their own queue mapping by
+	 * overriding the setup-time function that builds the mq_map.
+	 */
 	map_queues_fn		*map_queues;
 
 #ifdef CONFIG_BLK_DEBUG_FS
-	/*
-	 * Used by the debugfs implementation to show driver-specific
+	/**
+	 * @show_rq: Used by the debugfs implementation to show driver-specific
 	 * information about a request.
 	 */
 	void (*show_rq)(struct seq_file *m, struct request *rq);
@@ -391,14 +521,29 @@ void blk_mq_quiesce_queue_nowait(struct request_queue *q);
 
 unsigned int blk_mq_rq_cpu(struct request *rq);
 
-/*
+/**
+ * blk_mq_rq_from_pdu - cast a PDU to a request
+ * @pdu: the PDU (Protocol Data Unit) to be casted
+ *
+ * Return: request
+ *
  * Driver command data is immediately after the request. So subtract request
- * size to get back to the original request, add request size to get the PDU.
+ * size to get back to the original request.
  */
 static inline struct request *blk_mq_rq_from_pdu(void *pdu)
 {
 	return pdu - sizeof(struct request);
 }
+
+/**
+ * blk_mq_rq_to_pdu - cast a request to a PDU
+ * @rq: the request to be casted
+ *
+ * Return: pointer to the PDU
+ *
+ * Driver command data is immediately after the request. So add request to get
+ * the PDU.
+ */
 static inline void *blk_mq_rq_to_pdu(struct request *rq)
 {
 	return rq + 1;

commit 27a46989a82c71028f2ba15a3f2c8f30451fda33
Author: Pavel Begunkov <asml.silence@gmail.com>
Date:   Mon Sep 30 11:25:49 2019 +0300

    blk-mq: Inline status checkers
    
    blk_mq_request_completed() and blk_mq_request_started() are
    short, inline it.
    
    Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index a96b5cc957ab..e0fce93ac127 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -333,9 +333,25 @@ static inline u16 blk_mq_unique_tag_to_tag(u32 unique_tag)
 	return unique_tag & BLK_MQ_UNIQUE_TAG_MASK;
 }
 
+/**
+ * blk_mq_rq_state() - read the current MQ_RQ_* state of a request
+ * @rq: target request.
+ */
+static inline enum mq_rq_state blk_mq_rq_state(struct request *rq)
+{
+	return READ_ONCE(rq->state);
+}
+
+static inline int blk_mq_request_started(struct request *rq)
+{
+	return blk_mq_rq_state(rq) != MQ_RQ_IDLE;
+}
+
+static inline int blk_mq_request_completed(struct request *rq)
+{
+	return blk_mq_rq_state(rq) == MQ_RQ_COMPLETE;
+}
 
-int blk_mq_request_started(struct request *rq);
-int blk_mq_request_completed(struct request *rq);
 void blk_mq_start_request(struct request *rq);
 void blk_mq_end_request(struct request *rq, blk_status_t error);
 void __blk_mq_end_request(struct request *rq, blk_status_t error);

commit 7a18312c739aeace7c8ea448d39a0313d5ad5d5d
Author: Bart Van Assche <bvanassche@acm.org>
Date:   Mon Sep 30 16:00:45 2019 -0700

    block: Document all members of blk_mq_tag_set and bkl_mq_queue_map
    
    The meaning of several member variables of these two data structures is
    nontrivial. Hence document all member variables using the kernel-doc
    syntax.
    
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Ming Lei <ming.lei@redhat.com>
    Cc: Hannes Reinecke <hare@suse.com>
    Cc: Johannes Thumshirn <jthumshirn@suse.de>
    Signed-off-by: Bart Van Assche <bvanassche@acm.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 0bf056de5cc3..a96b5cc957ab 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -76,6 +76,16 @@ struct blk_mq_hw_ctx {
 	struct srcu_struct	srcu[0];
 };
 
+/**
+ * struct blk_mq_queue_map - ctx -> hctx mapping
+ * @mq_map:       CPU ID to hardware queue index map. This is an array
+ *	with nr_cpu_ids elements. Each element has a value in the range
+ *	[@queue_offset, @queue_offset + @nr_queues).
+ * @nr_queues:    Number of hardware queues to map CPU IDs onto.
+ * @queue_offset: First hardware queue to map onto. Used by the PCIe NVMe
+ *	driver to map each hardware queue type (enum hctx_type) onto a distinct
+ *	set of hardware queues.
+ */
 struct blk_mq_queue_map {
 	unsigned int *mq_map;
 	unsigned int nr_queues;
@@ -90,23 +100,45 @@ enum hctx_type {
 	HCTX_MAX_TYPES,
 };
 
+/**
+ * struct blk_mq_tag_set - tag set that can be shared between request queues
+ * @map:	   One or more ctx -> hctx mappings. One map exists for each
+ *		   hardware queue type (enum hctx_type) that the driver wishes
+ *		   to support. There are no restrictions on maps being of the
+ *		   same size, and it's perfectly legal to share maps between
+ *		   types.
+ * @nr_maps:	   Number of elements in the @map array. A number in the range
+ *		   [1, HCTX_MAX_TYPES].
+ * @ops:	   Pointers to functions that implement block driver behavior.
+ * @nr_hw_queues:  Number of hardware queues supported by the block driver that
+ *		   owns this data structure.
+ * @queue_depth:   Number of tags per hardware queue, reserved tags included.
+ * @reserved_tags: Number of tags to set aside for BLK_MQ_REQ_RESERVED tag
+ *		   allocations.
+ * @cmd_size:	   Number of additional bytes to allocate per request. The block
+ *		   driver owns these additional bytes.
+ * @numa_node:	   NUMA node the storage adapter has been connected to.
+ * @timeout:	   Request processing timeout in jiffies.
+ * @flags:	   Zero or more BLK_MQ_F_* flags.
+ * @driver_data:   Pointer to data owned by the block driver that created this
+ *		   tag set.
+ * @tags:	   Tag sets. One tag set per hardware queue. Has @nr_hw_queues
+ *		   elements.
+ * @tag_list_lock: Serializes tag_list accesses.
+ * @tag_list:	   List of the request queues that use this tag set. See also
+ *		   request_queue.tag_set_list.
+ */
 struct blk_mq_tag_set {
-	/*
-	 * map[] holds ctx -> hctx mappings, one map exists for each type
-	 * that the driver wishes to support. There are no restrictions
-	 * on maps being of the same size, and it's perfectly legal to
-	 * share maps between types.
-	 */
 	struct blk_mq_queue_map	map[HCTX_MAX_TYPES];
-	unsigned int		nr_maps;	/* nr entries in map[] */
+	unsigned int		nr_maps;
 	const struct blk_mq_ops	*ops;
-	unsigned int		nr_hw_queues;	/* nr hw queues across maps */
-	unsigned int		queue_depth;	/* max hw supported */
+	unsigned int		nr_hw_queues;
+	unsigned int		queue_depth;
 	unsigned int		reserved_tags;
-	unsigned int		cmd_size;	/* per-request extra data */
+	unsigned int		cmd_size;
 	int			numa_node;
 	unsigned int		timeout;
-	unsigned int		flags;		/* BLK_MQ_F_* */
+	unsigned int		flags;
 	void			*driver_data;
 
 	struct blk_mq_tags	**tags;

commit 737eb78e82d52d35df166d29af32bf61992de71d
Author: Damien Le Moal <damien.lemoal@wdc.com>
Date:   Thu Sep 5 18:51:33 2019 +0900

    block: Delay default elevator initialization
    
    When elevator_init_mq() is called from blk_mq_init_allocated_queue(),
    the only information known about the device is the number of hardware
    queues as the block device scan by the device driver is not completed
    yet for most drivers. The device type and elevator required features
    are not set yet, preventing to correctly select the default elevator
    most suitable for the device.
    
    This currently affects all multi-queue zoned block devices which default
    to the "none" elevator instead of the required "mq-deadline" elevator.
    These drives currently include host-managed SMR disks connected to a
    smartpqi HBA and null_blk block devices with zoned mode enabled.
    Upcoming NVMe Zoned Namespace devices will also be affected.
    
    Fix this by adding the boolean elevator_init argument to
    blk_mq_init_allocated_queue() to control the execution of
    elevator_init_mq(). Two cases exist:
    1) elevator_init = false is used for calls to
       blk_mq_init_allocated_queue() within blk_mq_init_queue(). In this
       case, a call to elevator_init_mq() is added to __device_add_disk(),
       resulting in the delayed initialization of the queue elevator
       after the device driver finished probing the device information. This
       effectively allows elevator_init_mq() access to more information
       about the device.
    2) elevator_init = true preserves the current behavior of initializing
       the elevator directly from blk_mq_init_allocated_queue(). This case
       is used for the special request based DM devices where the device
       gendisk is created before the queue initialization and device
       information (e.g. queue limits) is already known when the queue
       initialization is executed.
    
    Additionally, to make sure that the elevator initialization is never
    done while requests are in-flight (there should be none when the device
    driver calls device_add_disk()), freeze and quiesce the device request
    queue before calling blk_mq_init_sched() in elevator_init_mq().
    
    Reviewed-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Damien Le Moal <damien.lemoal@wdc.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 62a3bb715899..0bf056de5cc3 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -248,7 +248,8 @@ enum {
 
 struct request_queue *blk_mq_init_queue(struct blk_mq_tag_set *);
 struct request_queue *blk_mq_init_allocated_queue(struct blk_mq_tag_set *set,
-						  struct request_queue *q);
+						  struct request_queue *q,
+						  bool elevator_init);
 struct request_queue *blk_mq_init_sq_queue(struct blk_mq_tag_set *set,
 						const struct blk_mq_ops *ops,
 						unsigned int queue_depth,

commit 9685b2270211628e27ea7880a02b52efd4524099
Author: Bart Van Assche <bvanassche@acm.org>
Date:   Tue Aug 27 19:01:44 2019 +0800

    block: Remove blk_mq_register_dev()
    
    This function has no callers. Hence remove it.
    
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Ming Lei <ming.lei@redhat.com>
    Cc: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Bart Van Assche <bvanassche@acm.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 21cebe901ac0..62a3bb715899 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -253,7 +253,6 @@ struct request_queue *blk_mq_init_sq_queue(struct blk_mq_tag_set *set,
 						const struct blk_mq_ops *ops,
 						unsigned int queue_depth,
 						unsigned int set_flags);
-int blk_mq_register_dev(struct device *, struct request_queue *);
 void blk_mq_unregister_dev(struct device *, struct request_queue *);
 
 int blk_mq_alloc_tag_set(struct blk_mq_tag_set *set);

commit 226b4fc75c78f9c497c5182d939101b260cfb9f3
Author: Ming Lei <ming.lei@redhat.com>
Date:   Thu Jul 25 10:04:59 2019 +0800

    blk-mq: add callback of .cleanup_rq
    
    SCSI maintains its own driver private data hooked off of each SCSI
    request, and the pridate data won't be freed after scsi_queue_rq()
    returns BLK_STS_RESOURCE or BLK_STS_DEV_RESOURCE. An upper layer driver
    (e.g. dm-rq) may need to retry these SCSI requests, before SCSI has
    fully dispatched them, due to a lower level SCSI driver's resource
    limitation identified in scsi_queue_rq(). Currently SCSI's per-request
    private data is leaked when the upper layer driver (dm-rq) frees and
    then retries these requests in response to BLK_STS_RESOURCE or
    BLK_STS_DEV_RESOURCE returns from scsi_queue_rq().
    
    This usecase is so specialized that it doesn't warrant training an
    existing blk-mq interface (e.g. blk_mq_free_request) to allow SCSI to
    account for freeing its driver private data -- doing so would add an
    extra branch for handling a special case that all other consumers of
    SCSI (and blk-mq) won't ever need to worry about.
    
    So the most pragmatic way forward is to delegate freeing SCSI driver
    private data to the upper layer driver (dm-rq).  Do so by adding
    new .cleanup_rq callback and calling a new blk_mq_cleanup_rq() method
    from dm-rq.  A following commit will implement the .cleanup_rq() hook
    in scsi_mq_ops.
    
    Cc: Ewan D. Milne <emilne@redhat.com>
    Cc: Bart Van Assche <bvanassche@acm.org>
    Cc: Hannes Reinecke <hare@suse.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Mike Snitzer <snitzer@redhat.com>
    Cc: dm-devel@redhat.com
    Cc: <stable@vger.kernel.org>
    Fixes: 396eaf21ee17 ("blk-mq: improve DM's blk-mq IO merging via blk_insert_cloned_request feedback")
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 1cdd2788cfa6..21cebe901ac0 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -140,6 +140,7 @@ typedef int (poll_fn)(struct blk_mq_hw_ctx *);
 typedef int (map_queues_fn)(struct blk_mq_tag_set *set);
 typedef bool (busy_fn)(struct request_queue *);
 typedef void (complete_fn)(struct request *);
+typedef void (cleanup_rq_fn)(struct request *);
 
 
 struct blk_mq_ops {
@@ -200,6 +201,12 @@ struct blk_mq_ops {
 	/* Called from inside blk_get_request() */
 	void (*initialize_rq_fn)(struct request *rq);
 
+	/*
+	 * Called before freeing one request which isn't completed yet,
+	 * and usually for freeing the driver private data
+	 */
+	cleanup_rq_fn		*cleanup_rq;
+
 	/*
 	 * If set, returns whether or not this queue currently is busy
 	 */
@@ -367,4 +374,10 @@ static inline blk_qc_t request_to_qc_t(struct blk_mq_hw_ctx *hctx,
 			BLK_QC_T_INTERNAL;
 }
 
+static inline void blk_mq_cleanup_rq(struct request *rq)
+{
+	if (rq->q->mq_ops->cleanup_rq)
+		rq->q->mq_ops->cleanup_rq(rq);
+}
+
 #endif

commit a87ccce0b5a06ee546931859fa62e10f1bce54f9
Author: Ming Lei <ming.lei@redhat.com>
Date:   Wed Jul 24 11:48:43 2019 +0800

    blk-mq: remove blk_mq_complete_request_sync
    
    blk_mq_tagset_wait_completed_request() has been applied for waiting
    for completed request's fn, so not necessary to use
    blk_mq_complete_request_sync() any more.
    
    Cc: Max Gurtovoy <maxg@mellanox.com>
    Cc: Sagi Grimberg <sagi@grimberg.me>
    Cc: Keith Busch <keith.busch@intel.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index ee0719b649b6..1cdd2788cfa6 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -305,7 +305,6 @@ void blk_mq_requeue_request(struct request *rq, bool kick_requeue_list);
 void blk_mq_kick_requeue_list(struct request_queue *q);
 void blk_mq_delay_kick_requeue_list(struct request_queue *q, unsigned long msecs);
 bool blk_mq_complete_request(struct request *rq);
-void blk_mq_complete_request_sync(struct request *rq);
 bool blk_mq_bio_list_merge(struct request_queue *q, struct list_head *list,
 			   struct bio *bio, unsigned int nr_segs);
 bool blk_mq_queue_stopped(struct request_queue *q);

commit f9934a80f91dba8c7029ba7601459e41ea7770aa
Author: Ming Lei <ming.lei@redhat.com>
Date:   Wed Jul 24 11:48:40 2019 +0800

    blk-mq: introduce blk_mq_tagset_wait_completed_request()
    
    blk-mq may schedule to call queue's complete function on remote CPU via
    IPI, but doesn't provide any way to synchronize the request's complete
    fn. The current queue freeze interface can't provide the synchonization
    because aborted requests stay at blk-mq queues during EH.
    
    In some driver's EH(such as NVMe), hardware queue's resource may be freed &
    re-allocated. If the completed request's complete fn is run finally after the
    hardware queue's resource is released, kernel crash will be triggered.
    
    Prepare for fixing this kind of issue by introducing
    blk_mq_tagset_wait_completed_request().
    
    Cc: Max Gurtovoy <maxg@mellanox.com>
    Cc: Sagi Grimberg <sagi@grimberg.me>
    Cc: Keith Busch <keith.busch@intel.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index baac2926e54a..ee0719b649b6 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -322,6 +322,7 @@ bool blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async);
 void blk_mq_run_hw_queues(struct request_queue *q, bool async);
 void blk_mq_tagset_busy_iter(struct blk_mq_tag_set *tagset,
 		busy_tag_iter_fn *fn, void *priv);
+void blk_mq_tagset_wait_completed_request(struct blk_mq_tag_set *tagset);
 void blk_mq_freeze_queue(struct request_queue *q);
 void blk_mq_unfreeze_queue(struct request_queue *q);
 void blk_freeze_queue_start(struct request_queue *q);

commit aa306ab703e9452b1e25cc8e8f04b8df523d0bb8
Author: Ming Lei <ming.lei@redhat.com>
Date:   Wed Jul 24 11:48:39 2019 +0800

    blk-mq: introduce blk_mq_request_completed()
    
    NVMe needs this function to decide if one request to be aborted has
    been completed in normal IO path already.
    
    So introduce it.
    
    Cc: Max Gurtovoy <maxg@mellanox.com>
    Cc: Sagi Grimberg <sagi@grimberg.me>
    Cc: Keith Busch <keith.busch@intel.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 3fa1fa59f9b2..baac2926e54a 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -296,6 +296,7 @@ static inline u16 blk_mq_unique_tag_to_tag(u32 unique_tag)
 
 
 int blk_mq_request_started(struct request *rq);
+int blk_mq_request_completed(struct request *rq);
 void blk_mq_start_request(struct request *rq);
 void blk_mq_end_request(struct request *rq, blk_status_t error);
 void __blk_mq_end_request(struct request *rq, blk_status_t error);

commit 14ccb66b3f585b2bc21e7256c96090abed5a512c
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Jun 6 12:29:01 2019 +0200

    block: remove the bi_phys_segments field in struct bio
    
    We only need the number of segments in the blk-mq submission path.
    Remove the field from struct bio, and return it from a variant of
    blk_queue_split instead of that it can passed as an argument to
    those functions that need the value.
    
    This also means we stop recounting segments except for cloning
    and partial segments.
    
    To keep the number of arguments in this how path down remove
    pointless struct request_queue arguments from any of the functions
    that had it and grew a nr_segs argument.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 15d1aa53d96c..3fa1fa59f9b2 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -306,7 +306,7 @@ void blk_mq_delay_kick_requeue_list(struct request_queue *q, unsigned long msecs
 bool blk_mq_complete_request(struct request *rq);
 void blk_mq_complete_request_sync(struct request *rq);
 bool blk_mq_bio_list_merge(struct request_queue *q, struct list_head *list,
-			   struct bio *bio);
+			   struct bio *bio, unsigned int nr_segs);
 bool blk_mq_queue_stopped(struct request_queue *q);
 void blk_mq_stop_hw_queue(struct blk_mq_hw_ctx *hctx);
 void blk_mq_start_hw_queue(struct blk_mq_hw_ctx *hctx);

commit 2f8f1336a48bd5186de3476da0a3e2ec06d0533a
Author: Ming Lei <ming.lei@redhat.com>
Date:   Tue Apr 30 09:52:27 2019 +0800

    blk-mq: always free hctx after request queue is freed
    
    In normal queue cleanup path, hctx is released after request queue
    is freed, see blk_mq_release().
    
    However, in __blk_mq_update_nr_hw_queues(), hctx may be freed because
    of hw queues shrinking. This way is easy to cause use-after-free,
    because: one implicit rule is that it is safe to call almost all block
    layer APIs if the request queue is alive; and one hctx may be retrieved
    by one API, then the hctx can be freed by blk_mq_update_nr_hw_queues();
    finally use-after-free is triggered.
    
    Fixes this issue by always freeing hctx after releasing request queue.
    If some hctxs are removed in blk_mq_update_nr_hw_queues(), introduce
    a per-queue list to hold them, then try to resuse these hctxs if numa
    node is matched.
    
    Cc: Dongli Zhang <dongli.zhang@oracle.com>
    Cc: James Smart <james.smart@broadcom.com>
    Cc: Bart Van Assche <bart.vanassche@wdc.com>
    Cc: linux-scsi@vger.kernel.org,
    Cc: Martin K . Petersen <martin.petersen@oracle.com>,
    Cc: Christoph Hellwig <hch@lst.de>,
    Cc: James E . J . Bottomley <jejb@linux.vnet.ibm.com>,
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Tested-by: James Smart <james.smart@broadcom.com>
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index db29928de467..15d1aa53d96c 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -70,6 +70,8 @@ struct blk_mq_hw_ctx {
 	struct dentry		*sched_debugfs_dir;
 #endif
 
+	struct list_head	hctx_list;
+
 	/* Must be the last member - see also blk_mq_hw_ctx_size(). */
 	struct srcu_struct	srcu[0];
 };

commit 1b8f21b74c3c9c82fce5a751d7aefb7cc0b8d33d
Author: Ming Lei <ming.lei@redhat.com>
Date:   Tue Apr 9 06:31:21 2019 +0800

    blk-mq: introduce blk_mq_complete_request_sync()
    
    In NVMe's error handler, follows the typical steps of tearing down
    hardware for recovering controller:
    
    1) stop blk_mq hw queues
    2) stop the real hw queues
    3) cancel in-flight requests via
            blk_mq_tagset_busy_iter(tags, cancel_request, ...)
    cancel_request():
            mark the request as abort
            blk_mq_complete_request(req);
    4) destroy real hw queues
    
    However, there may be race between #3 and #4, because blk_mq_complete_request()
    may run q->mq_ops->complete(rq) remotelly and asynchronously, and
    ->complete(rq) may be run after #4.
    
    This patch introduces blk_mq_complete_request_sync() for fixing the
    above race.
    
    Cc: Sagi Grimberg <sagi@grimberg.me>
    Cc: Bart Van Assche <bvanassche@acm.org>
    Cc: James Smart <james.smart@broadcom.com>
    Cc: linux-nvme@lists.infradead.org
    Reviewed-by: Keith Busch <keith.busch@intel.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index cb2aa7ecafff..db29928de467 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -302,6 +302,7 @@ void blk_mq_requeue_request(struct request *rq, bool kick_requeue_list);
 void blk_mq_kick_requeue_list(struct request_queue *q);
 void blk_mq_delay_kick_requeue_list(struct request_queue *q, unsigned long msecs);
 bool blk_mq_complete_request(struct request *rq);
+void blk_mq_complete_request_sync(struct request *rq);
 bool blk_mq_bio_list_merge(struct request_queue *q, struct list_head *list,
 			   struct bio *bio);
 bool blk_mq_queue_stopped(struct request_queue *q);

commit e6c987120e24cb913cb7bd4e675129a30fa49e0d
Author: Bart Van Assche <bvanassche@acm.org>
Date:   Wed Mar 20 13:14:37 2019 -0700

    block: Unexport blk_mq_add_to_requeue_list()
    
    This function is not used outside the block layer core. Hence unexport it.
    
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Bart Van Assche <bvanassche@acm.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 35359697318b..cb2aa7ecafff 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -299,8 +299,6 @@ void blk_mq_end_request(struct request *rq, blk_status_t error);
 void __blk_mq_end_request(struct request *rq, blk_status_t error);
 
 void blk_mq_requeue_request(struct request *rq, bool kick_requeue_list);
-void blk_mq_add_to_requeue_list(struct request *rq, bool at_head,
-				bool kick_requeue_list);
 void blk_mq_kick_requeue_list(struct request_queue *q);
 void blk_mq_delay_kick_requeue_list(struct request_queue *q, unsigned long msecs);
 bool blk_mq_complete_request(struct request *rq);

commit 9496c015ed39ddfce971d63a1442e6d258504a7d
Author: Dongli Zhang <dongli.zhang@oracle.com>
Date:   Tue Mar 19 23:05:18 2019 +0800

    blk-mq: remove unused 'nr_expired' from blk_mq_hw_ctx
    
    There is no usage of 'nr_expired'.
    
    The 'nr_expired' was introduced by commit 1d9bd5161ba3 ("blk-mq: replace
    timeout synchronization with a RCU and generation based scheme"). Its usage
    was removed since commit 12f5b9314545 ("blk-mq: Remove generation
    seqeunce").
    
    Signed-off-by: Dongli Zhang <dongli.zhang@oracle.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index b0c814bcc7e3..35359697318b 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -57,7 +57,6 @@ struct blk_mq_hw_ctx {
 	unsigned int		queue_num;
 
 	atomic_t		nr_active;
-	unsigned int		nr_expired;
 
 	struct hlist_node	cpuhp_dead;
 	struct kobject		kobj;

commit 56d18f62f556b80105e38e7975975cf7465aae3e
Author: Ming Lei <ming.lei@redhat.com>
Date:   Fri Feb 15 19:13:24 2019 +0800

    block: kill BLK_MQ_F_SG_MERGE
    
    QUEUE_FLAG_NO_SG_MERGE has been killed, so kill BLK_MQ_F_SG_MERGE too.
    
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 0e030f5f76b6..b0c814bcc7e3 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -218,7 +218,6 @@ struct blk_mq_ops {
 enum {
 	BLK_MQ_F_SHOULD_MERGE	= 1 << 0,
 	BLK_MQ_F_TAG_SHARED	= 1 << 1,
-	BLK_MQ_F_SG_MERGE	= 1 << 2,
 	BLK_MQ_F_BLOCKING	= 1 << 5,
 	BLK_MQ_F_NO_SCHED	= 1 << 6,
 	BLK_MQ_F_ALLOC_POLICY_START_BIT = 8,

commit 7b7ab780a048699d2b9f416bf2d5c089d8d1028c
Author: Sagi Grimberg <sagi@grimberg.me>
Date:   Fri Dec 14 11:06:06 2018 -0800

    block: make request_to_qc_t public
    
    block consumers will need it for polling requests that
    are sent with blk_execute_rq_nowait. Also, get rid of
    blk_tag_to_qc_t and open-code it instead.
    
    Reviewed-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index d3c0a0d2680b..0e030f5f76b6 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -357,4 +357,14 @@ static inline void *blk_mq_rq_to_pdu(struct request *rq)
 	for ((i) = 0; (i) < (hctx)->nr_ctx &&				\
 	     ({ ctx = (hctx)->ctxs[(i)]; 1; }); (i)++)
 
+static inline blk_qc_t request_to_qc_t(struct blk_mq_hw_ctx *hctx,
+		struct request *rq)
+{
+	if (rq->tag != -1)
+		return rq->tag | (hctx->queue_num << BLK_QC_T_SHIFT);
+
+	return rq->internal_tag | (hctx->queue_num << BLK_QC_T_SHIFT) |
+			BLK_QC_T_INTERNAL;
+}
+
 #endif

commit 3c94d83cb352627f221d971b05f163c17527de74
Author: Jens Axboe <axboe@kernel.dk>
Date:   Mon Dec 17 21:11:17 2018 -0700

    blk-mq: change blk_mq_queue_busy() to blk_mq_queue_inflight()
    
    There's a single user of this function, dm, and dm just wants
    to check if IO is inflight, not that it's just allocated.
    
    This fixes a hang with srp/002 in blktests with dm, where it tries
    to suspend but waits for inflight IO to finish first. As it checks
    for just allocated requests, this fails.
    
    Tested-by: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 57eda7b20243..d3c0a0d2680b 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -257,7 +257,7 @@ void blk_mq_flush_plug_list(struct blk_plug *plug, bool from_schedule);
 void blk_mq_free_request(struct request *rq);
 bool blk_mq_can_queue(struct blk_mq_hw_ctx *);
 
-bool blk_mq_queue_busy(struct request_queue *q);
+bool blk_mq_queue_inflight(struct request_queue *q);
 
 enum {
 	/* return when out of requests */

commit e20ba6e1da029136ded295f33076483d65ddf50a
Author: Christoph Hellwig <hch@lst.de>
Date:   Sun Dec 2 17:46:16 2018 +0100

    block: move queues types to the block layer
    
    Having another indirect all in the fast path doesn't really help
    in our post-spectre world.  Also having too many queue type is just
    going to create confusion, so I'd rather manage them centrally.
    
    Note that the queue type naming and ordering changes a bit - the
    first index now is the default queue for everything not explicitly
    marked, the optional ones are read and poll queues.
    
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 467f1dd21ccf..57eda7b20243 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -81,8 +81,12 @@ struct blk_mq_queue_map {
 	unsigned int queue_offset;
 };
 
-enum {
-	HCTX_MAX_TYPES = 3,
+enum hctx_type {
+	HCTX_TYPE_DEFAULT,	/* all I/O not otherwise accounted for */
+	HCTX_TYPE_READ,		/* just for READ I/O */
+	HCTX_TYPE_POLL,		/* polled I/O of any kind */
+
+	HCTX_MAX_TYPES,
 };
 
 struct blk_mq_tag_set {
@@ -118,8 +122,6 @@ struct blk_mq_queue_data {
 typedef blk_status_t (queue_rq_fn)(struct blk_mq_hw_ctx *,
 		const struct blk_mq_queue_data *);
 typedef void (commit_rqs_fn)(struct blk_mq_hw_ctx *);
-/* takes rq->cmd_flags as input, returns a hardware type index */
-typedef int (rq_flags_to_type_fn)(struct request_queue *, unsigned int);
 typedef bool (get_budget_fn)(struct blk_mq_hw_ctx *);
 typedef void (put_budget_fn)(struct blk_mq_hw_ctx *);
 typedef enum blk_eh_timer_return (timeout_fn)(struct request *, bool);
@@ -154,11 +156,6 @@ struct blk_mq_ops {
 	 */
 	commit_rqs_fn		*commit_rqs;
 
-	/*
-	 * Return a queue map type for the given request/bio flags
-	 */
-	rq_flags_to_type_fn	*rq_flags_to_type;
-
 	/*
 	 * Reserve budget before queue request, once .queue_rq is
 	 * run, it is driver's responsibility to release the

commit d666ba98f849ad44c4405ecc2180390ebe80f4f9
Author: Jens Axboe <axboe@kernel.dk>
Date:   Tue Nov 27 17:02:25 2018 -0700

    blk-mq: add mq_ops->commit_rqs()
    
    blk-mq passes information to the hardware about any given request being
    the last that we will issue in this sequence. The point is that hardware
    can defer costly doorbell type writes to the last request. But if we run
    into errors issuing a sequence of requests, we may never send the request
    with bd->last == true set. For that case, we need a hook that tells the
    hardware that nothing else is coming right now.
    
    For failures returned by the drivers ->queue_rq() hook, the driver is
    responsible for flushing pending requests, if it uses bd->last to
    optimize that part. This works like before, no changes there.
    
    Reviewed-by: Omar Sandoval <osandov@fb.com>
    Reviewed-by: Ming Lei <ming.lei@redhat.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index b8de11e0603b..467f1dd21ccf 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -117,6 +117,7 @@ struct blk_mq_queue_data {
 
 typedef blk_status_t (queue_rq_fn)(struct blk_mq_hw_ctx *,
 		const struct blk_mq_queue_data *);
+typedef void (commit_rqs_fn)(struct blk_mq_hw_ctx *);
 /* takes rq->cmd_flags as input, returns a hardware type index */
 typedef int (rq_flags_to_type_fn)(struct request_queue *, unsigned int);
 typedef bool (get_budget_fn)(struct blk_mq_hw_ctx *);
@@ -144,6 +145,15 @@ struct blk_mq_ops {
 	 */
 	queue_rq_fn		*queue_rq;
 
+	/*
+	 * If a driver uses bd->last to judge when to submit requests to
+	 * hardware, it must define this function. In case of errors that
+	 * make us stop issuing further requests, this hook serves the
+	 * purpose of kicking the hardware (which the last request otherwise
+	 * would have done).
+	 */
+	commit_rqs_fn		*commit_rqs;
+
 	/*
 	 * Return a queue map type for the given request/bio flags
 	 */

commit af78ff7c6e66832afcdf5418f67b11c409f9e7a1
Author: Keith Busch <keith.busch@intel.com>
Date:   Mon Nov 26 09:54:30 2018 -0700

    blk-mq: Simplify request completion state
    
    There are no more users relying on blk-mq request states to prevent
    double completions, so replace the relatively expensive cmpxchg operation
    with WRITE_ONCE.
    
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Keith Busch <keith.busch@intel.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 6e3da356a8eb..b8de11e0603b 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -329,20 +329,6 @@ void blk_mq_quiesce_queue_nowait(struct request_queue *q);
 
 unsigned int blk_mq_rq_cpu(struct request *rq);
 
-/**
- * blk_mq_mark_complete() - Set request state to complete
- * @rq: request to set to complete state
- *
- * Returns true if request state was successfully set to complete. If
- * successful, the caller is responsibile for seeing this request is ended, as
- * blk_mq_complete_request will not work again.
- */
-static inline bool blk_mq_mark_complete(struct request *rq)
-{
-	return cmpxchg(&rq->state, MQ_RQ_IN_FLIGHT, MQ_RQ_COMPLETE) ==
-			MQ_RQ_IN_FLIGHT;
-}
-
 /*
  * Driver command data is immediately after the request. So subtract request
  * size to get back to the original request, add request size to get the PDU.

commit 16c15eb16a793f2d81ae52f41f43fb6831b34212
Author: Keith Busch <keith.busch@intel.com>
Date:   Mon Nov 26 09:54:28 2018 -0700

    blk-mq: Return true if request was completed
    
    A driver may have internal state to cleanup if we're pretending a request
    didn't complete. Return 'false' if the command wasn't actually completed
    due to the timeout error injection, and true otherwise.
    
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Keith Busch <keith.busch@intel.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index ca0520ca6437..6e3da356a8eb 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -298,7 +298,7 @@ void blk_mq_add_to_requeue_list(struct request *rq, bool at_head,
 				bool kick_requeue_list);
 void blk_mq_kick_requeue_list(struct request_queue *q);
 void blk_mq_delay_kick_requeue_list(struct request_queue *q, unsigned long msecs);
-void blk_mq_complete_request(struct request *rq);
+bool blk_mq_complete_request(struct request *rq);
 bool blk_mq_bio_list_merge(struct request_queue *q, struct list_head *list,
 			   struct bio *bio);
 bool blk_mq_queue_stopped(struct request_queue *q);

commit 9743139c5d11ab170f70a308dcb88c342390adfb
Author: Jens Axboe <axboe@kernel.dk>
Date:   Fri Nov 16 09:48:21 2018 -0700

    blk-mq: remove 'tag' parameter from mq_ops->poll()
    
    We always pass in -1 now and none of the callers use the tag value,
    remove the parameter.
    
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 929e8abc5535..ca0520ca6437 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -132,7 +132,7 @@ typedef void (exit_request_fn)(struct blk_mq_tag_set *set, struct request *,
 typedef bool (busy_iter_fn)(struct blk_mq_hw_ctx *, struct request *, void *,
 		bool);
 typedef bool (busy_tag_iter_fn)(struct request *, void *, bool);
-typedef int (poll_fn)(struct blk_mq_hw_ctx *, unsigned int);
+typedef int (poll_fn)(struct blk_mq_hw_ctx *);
 typedef int (map_queues_fn)(struct blk_mq_tag_set *set);
 typedef bool (busy_fn)(struct request_queue *);
 typedef void (complete_fn)(struct request *);

commit ae8799125d565c798e49dcab4bf182dbfc483524
Author: Jens Axboe <axboe@kernel.dk>
Date:   Thu Nov 8 09:03:51 2018 -0700

    blk-mq: provide a helper to check if a queue is busy
    
    Returns true if the queue currently has requests pending,
    false if not.
    
    DM can use this to replace the atomic_inc/dec they do per device
    to see if a device is busy.
    
    Reviewed-by: Mike Snitzer <snitzer@redhat.com>
    Reviewed-by: Bart Van Assche <bvanassche@acm.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index ff497dfcbbf9..929e8abc5535 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -250,6 +250,8 @@ void blk_mq_flush_plug_list(struct blk_plug *plug, bool from_schedule);
 void blk_mq_free_request(struct request *rq);
 bool blk_mq_can_queue(struct blk_mq_hw_ctx *);
 
+bool blk_mq_queue_busy(struct request_queue *q);
+
 enum {
 	/* return when out of requests */
 	BLK_MQ_REQ_NOWAIT	= (__force blk_mq_req_flags_t)(1 << 0),

commit 7baa85727d0406ffd2b2303cd803a145aa35c505
Author: Jens Axboe <axboe@kernel.dk>
Date:   Thu Nov 8 10:24:07 2018 -0700

    blk-mq-tag: change busy_iter_fn to return whether to continue or not
    
    We have this functionality in sbitmap, but we don't export it in
    blk-mq for users of the tags busy iteration. This can be useful
    for stopping the iteration, if the caller doesn't need to find
    more requests.
    
    Reviewed-by: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 9f5e93f40857..ff497dfcbbf9 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -129,9 +129,9 @@ typedef int (init_request_fn)(struct blk_mq_tag_set *set, struct request *,
 typedef void (exit_request_fn)(struct blk_mq_tag_set *set, struct request *,
 		unsigned int);
 
-typedef void (busy_iter_fn)(struct blk_mq_hw_ctx *, struct request *, void *,
+typedef bool (busy_iter_fn)(struct blk_mq_hw_ctx *, struct request *, void *,
 		bool);
-typedef void (busy_tag_iter_fn)(struct request *, void *, bool);
+typedef bool (busy_tag_iter_fn)(struct request *, void *, bool);
 typedef int (poll_fn)(struct blk_mq_hw_ctx *, unsigned int);
 typedef int (map_queues_fn)(struct blk_mq_tag_set *set);
 typedef bool (busy_fn)(struct request_queue *);

commit 4b04cc6a8f86c4842314def22332de1f15de8523
Author: Jens Axboe <axboe@kernel.dk>
Date:   Mon Nov 5 12:44:33 2018 -0700

    nvme: add separate poll queue map
    
    Adds support for defining a variable number of poll queues, currently
    configurable with the 'poll_queues' module parameter. Defaults to
    a single poll queue.
    
    And now we finally have poll support without triggering interrupts!
    
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Reviewed-by: Keith Busch <keith.busch@intel.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 729ce0f00433..9f5e93f40857 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -82,7 +82,7 @@ struct blk_mq_queue_map {
 };
 
 enum {
-	HCTX_MAX_TYPES = 2,
+	HCTX_MAX_TYPES = 3,
 };
 
 struct blk_mq_tag_set {

commit 843477d4cc5c4bb4e346c561ecd3b9d0bd67e8c8
Author: Jens Axboe <axboe@kernel.dk>
Date:   Wed Oct 24 13:16:11 2018 -0600

    blk-mq: initial support for multiple queue maps
    
    Add a queue offset to the tag map. This enables users to map
    iteratively, for each queue map type they support.
    
    Bump maximum number of supported maps to 2, we're now fully
    able to support more than 1 map.
    
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Reviewed-by: Keith Busch <keith.busch@intel.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 8994c95056a8..729ce0f00433 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -78,10 +78,11 @@ struct blk_mq_hw_ctx {
 struct blk_mq_queue_map {
 	unsigned int *mq_map;
 	unsigned int nr_queues;
+	unsigned int queue_offset;
 };
 
 enum {
-	HCTX_MAX_TYPES = 1,
+	HCTX_MAX_TYPES = 2,
 };
 
 struct blk_mq_tag_set {

commit b3c661b15d5ab11d982e58bee23e05c1780528a1
Author: Jens Axboe <axboe@kernel.dk>
Date:   Tue Oct 30 10:36:06 2018 -0600

    blk-mq: support multiple hctx maps
    
    Add support for the tag set carrying multiple queue maps, and
    for the driver to inform blk-mq how many it wishes to support
    through setting set->nr_maps.
    
    This adds an mq_ops helper for drivers that support more than 1
    map, mq_ops->rq_flags_to_type(). The function takes request/bio
    flags and CPU, and returns a queue map index for that. We then
    use the type information in blk_mq_map_queue() to index the map
    set.
    
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Reviewed-by: Keith Busch <keith.busch@intel.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 6c39d546c50b..8994c95056a8 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -85,7 +85,14 @@ enum {
 };
 
 struct blk_mq_tag_set {
+	/*
+	 * map[] holds ctx -> hctx mappings, one map exists for each type
+	 * that the driver wishes to support. There are no restrictions
+	 * on maps being of the same size, and it's perfectly legal to
+	 * share maps between types.
+	 */
 	struct blk_mq_queue_map	map[HCTX_MAX_TYPES];
+	unsigned int		nr_maps;	/* nr entries in map[] */
 	const struct blk_mq_ops	*ops;
 	unsigned int		nr_hw_queues;	/* nr hw queues across maps */
 	unsigned int		queue_depth;	/* max hw supported */
@@ -109,6 +116,8 @@ struct blk_mq_queue_data {
 
 typedef blk_status_t (queue_rq_fn)(struct blk_mq_hw_ctx *,
 		const struct blk_mq_queue_data *);
+/* takes rq->cmd_flags as input, returns a hardware type index */
+typedef int (rq_flags_to_type_fn)(struct request_queue *, unsigned int);
 typedef bool (get_budget_fn)(struct blk_mq_hw_ctx *);
 typedef void (put_budget_fn)(struct blk_mq_hw_ctx *);
 typedef enum blk_eh_timer_return (timeout_fn)(struct request *, bool);
@@ -134,6 +143,11 @@ struct blk_mq_ops {
 	 */
 	queue_rq_fn		*queue_rq;
 
+	/*
+	 * Return a queue map type for the given request/bio flags
+	 */
+	rq_flags_to_type_fn	*rq_flags_to_type;
+
 	/*
 	 * Reserve budget before queue request, once .queue_rq is
 	 * run, it is driver's responsibility to release the

commit f31967f0e455d08d3ea1d2f849bf62dafc92dbf4
Author: Jens Axboe <axboe@kernel.dk>
Date:   Mon Oct 29 13:13:29 2018 -0600

    blk-mq: allow software queue to map to multiple hardware queues
    
    The mapping used to be dependent on just the CPU location, but
    now it's a tuple of (type, cpu) instead. This is a prep patch
    for allowing a single software queue to map to multiple hardware
    queues. No functional changes in this patch.
    
    This changes the software queue count to an unsigned short
    to save a bit of space. We can still support 64K-1 CPUs,
    which should be enough. Add a check to catch a wrap.
    
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Reviewed-by: Keith Busch <keith.busch@intel.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 176164888628..6c39d546c50b 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -37,7 +37,8 @@ struct blk_mq_hw_ctx {
 	struct blk_mq_ctx	*dispatch_from;
 	unsigned int		dispatch_busy;
 
-	unsigned int		nr_ctx;
+	unsigned short		type;
+	unsigned short		nr_ctx;
 	struct blk_mq_ctx	**ctxs;
 
 	spinlock_t		dispatch_wait_lock;

commit ed76e329d74a4b15ac0f5fd3adbd52ec0178a134
Author: Jens Axboe <axboe@kernel.dk>
Date:   Mon Oct 29 13:06:14 2018 -0600

    blk-mq: abstract out queue map
    
    This is in preparation for allowing multiple sets of maps per
    queue, if so desired.
    
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Reviewed-by: Bart Van Assche <bvanassche@acm.org>
    Reviewed-by: Keith Busch <keith.busch@intel.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index d83a26fb37e5..176164888628 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -74,10 +74,19 @@ struct blk_mq_hw_ctx {
 	struct srcu_struct	srcu[0];
 };
 
+struct blk_mq_queue_map {
+	unsigned int *mq_map;
+	unsigned int nr_queues;
+};
+
+enum {
+	HCTX_MAX_TYPES = 1,
+};
+
 struct blk_mq_tag_set {
-	unsigned int		*mq_map;
+	struct blk_mq_queue_map	map[HCTX_MAX_TYPES];
 	const struct blk_mq_ops	*ops;
-	unsigned int		nr_hw_queues;
+	unsigned int		nr_hw_queues;	/* nr hw queues across maps */
 	unsigned int		queue_depth;	/* max hw supported */
 	unsigned int		reserved_tags;
 	unsigned int		cmd_size;	/* per-request extra data */
@@ -295,7 +304,7 @@ void blk_mq_freeze_queue_wait(struct request_queue *q);
 int blk_mq_freeze_queue_wait_timeout(struct request_queue *q,
 				     unsigned long timeout);
 
-int blk_mq_map_queues(struct blk_mq_tag_set *set);
+int blk_mq_map_queues(struct blk_mq_queue_map *qmap);
 void blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set, int nr_hw_queues);
 
 void blk_mq_quiesce_queue_nowait(struct request_queue *q);

commit 9cf2bab6307659b940da65d16dcc8f82c69f3a97
Author: Jens Axboe <axboe@kernel.dk>
Date:   Wed Oct 31 17:01:22 2018 -0600

    block: kill request ->cpu member
    
    This was used for completion placement for the legacy path,
    but for mq we have rq->mq_ctx->cpu for that. Add a helper
    to get the request CPU assignment, as the mq_ctx type is
    private to blk-mq.
    
    Reviewed-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 9dd574e5436a..d83a26fb37e5 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -300,6 +300,8 @@ void blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set, int nr_hw_queues);
 
 void blk_mq_quiesce_queue_nowait(struct request_queue *q);
 
+unsigned int blk_mq_rq_cpu(struct request *rq);
+
 /**
  * blk_mq_mark_complete() - Set request state to complete
  * @rq: request to set to complete state

commit c7bb9ad1744ea14e61e5fff99ee5282709b0c9d9
Author: Jens Axboe <axboe@kernel.dk>
Date:   Wed Oct 31 09:43:30 2018 -0600

    block: get rid of q->softirq_done_fn()
    
    With the legacy path gone, all we do is funnel it through the
    mq_ops->complete() operation.
    
    Tested-by: Ming Lei <ming.lei@redhat.com>
    Reviewed-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 5c8418ebbfd6..9dd574e5436a 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -115,6 +115,7 @@ typedef void (busy_tag_iter_fn)(struct request *, void *, bool);
 typedef int (poll_fn)(struct blk_mq_hw_ctx *, unsigned int);
 typedef int (map_queues_fn)(struct blk_mq_tag_set *set);
 typedef bool (busy_fn)(struct request_queue *);
+typedef void (complete_fn)(struct request *);
 
 
 struct blk_mq_ops {
@@ -142,7 +143,7 @@ struct blk_mq_ops {
 	 */
 	poll_fn			*poll;
 
-	softirq_done_fn		*complete;
+	complete_fn		*complete;
 
 	/*
 	 * Called when the block layer side of a hardware queue has been

commit 9ba20527f4d1430b5f3e5f566be5af3e156a3284
Author: Jens Axboe <axboe@kernel.dk>
Date:   Mon Oct 29 10:15:10 2018 -0600

    blk-mq: provide mq_ops->busy() hook
    
    We'll hook into this from blk_lld_busy(), allowing blk-mq to also
    return whether or not a given queue currently has requests in
    progress.
    
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Tested-by: Ming Lei <ming.lei@redhat.com>
    Reviewed-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 2286dc12c6bc..5c8418ebbfd6 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -114,6 +114,7 @@ typedef void (busy_iter_fn)(struct blk_mq_hw_ctx *, struct request *, void *,
 typedef void (busy_tag_iter_fn)(struct request *, void *, bool);
 typedef int (poll_fn)(struct blk_mq_hw_ctx *, unsigned int);
 typedef int (map_queues_fn)(struct blk_mq_tag_set *set);
+typedef bool (busy_fn)(struct request_queue *);
 
 
 struct blk_mq_ops {
@@ -165,6 +166,11 @@ struct blk_mq_ops {
 	/* Called from inside blk_get_request() */
 	void (*initialize_rq_fn)(struct request *rq);
 
+	/*
+	 * If set, returns whether or not this queue currently is busy
+	 */
+	busy_fn			*busy;
+
 	map_queues_fn		*map_queues;
 
 #ifdef CONFIG_BLK_DEBUG_FS

commit 9316a9ed6895c4ad2f0cde171d486f80c55d8283
Author: Jens Axboe <axboe@kernel.dk>
Date:   Mon Oct 15 08:40:37 2018 -0600

    blk-mq: provide helper for setting up an SQ queue and tag set
    
    This pattern is repeated throughout all the blk-mq conversions.
    Provide a basic helper to get it done.
    
    Reviewed-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 1da59c16f637..2286dc12c6bc 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -203,6 +203,10 @@ enum {
 struct request_queue *blk_mq_init_queue(struct blk_mq_tag_set *);
 struct request_queue *blk_mq_init_allocated_queue(struct blk_mq_tag_set *set,
 						  struct request_queue *q);
+struct request_queue *blk_mq_init_sq_queue(struct blk_mq_tag_set *set,
+						const struct blk_mq_ops *ops,
+						unsigned int queue_depth,
+						unsigned int set_flags);
 int blk_mq_register_dev(struct device *, struct request_queue *);
 void blk_mq_unregister_dev(struct device *, struct request_queue *);
 

commit 73ba2fb33c492916853dfe63e3b3163da0be661d
Merge: 958f338e96f8 b86d865cb1ca
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Aug 14 10:23:25 2018 -0700

    Merge tag 'for-4.19/block-20180812' of git://git.kernel.dk/linux-block
    
    Pull block updates from Jens Axboe:
     "First pull request for this merge window, there will also be a
      followup request with some stragglers.
    
      This pull request contains:
    
       - Fix for a thundering heard issue in the wbt block code (Anchal
         Agarwal)
    
       - A few NVMe pull requests:
          * Improved tracepoints (Keith)
          * Larger inline data support for RDMA (Steve Wise)
          * RDMA setup/teardown fixes (Sagi)
          * Effects log suppor for NVMe target (Chaitanya Kulkarni)
          * Buffered IO suppor for NVMe target (Chaitanya Kulkarni)
          * TP4004 (ANA) support (Christoph)
          * Various NVMe fixes
    
       - Block io-latency controller support. Much needed support for
         properly containing block devices. (Josef)
    
       - Series improving how we handle sense information on the stack
         (Kees)
    
       - Lightnvm fixes and updates/improvements (Mathias/Javier et al)
    
       - Zoned device support for null_blk (Matias)
    
       - AIX partition fixes (Mauricio Faria de Oliveira)
    
       - DIF checksum code made generic (Max Gurtovoy)
    
       - Add support for discard in iostats (Michael Callahan / Tejun)
    
       - Set of updates for BFQ (Paolo)
    
       - Removal of async write support for bsg (Christoph)
    
       - Bio page dirtying and clone fixups (Christoph)
    
       - Set of bcache fix/changes (via Coly)
    
       - Series improving blk-mq queue setup/teardown speed (Ming)
    
       - Series improving merging performance on blk-mq (Ming)
    
       - Lots of other fixes and cleanups from a slew of folks"
    
    * tag 'for-4.19/block-20180812' of git://git.kernel.dk/linux-block: (190 commits)
      blkcg: Make blkg_root_lookup() work for queues in bypass mode
      bcache: fix error setting writeback_rate through sysfs interface
      null_blk: add lock drop/acquire annotation
      Blk-throttle: reduce tail io latency when iops limit is enforced
      block: paride: pd: mark expected switch fall-throughs
      block: Ensure that a request queue is dissociated from the cgroup controller
      block: Introduce blk_exit_queue()
      blkcg: Introduce blkg_root_lookup()
      block: Remove two superfluous #include directives
      blk-mq: count the hctx as active before allocating tag
      block: bvec_nr_vecs() returns value for wrong slab
      bcache: trivial - remove tailing backslash in macro BTREE_FLAG
      bcache: make the pr_err statement used for ENOENT only in sysfs_attatch section
      bcache: set max writeback rate when I/O request is idle
      bcache: add code comments for bset.c
      bcache: fix mistaken comments in request.c
      bcache: fix mistaken code comments in bcache.h
      bcache: add a comment in super.c
      bcache: avoid unncessary cache prefetch bch_btree_node_get()
      bcache: display rate debug parameters to 0 when writeback is not running
      ...

commit 0fc09f920983f61be625658c62cc40ac25a7b3a5
Author: Keith Busch <keith.busch@intel.com>
Date:   Mon Jul 23 08:37:50 2018 -0600

    blk-mq: export setting request completion state
    
    This is preparing for drivers that want to directly alter the state of
    their requests. No functional change here.
    
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Keith Busch <keith.busch@intel.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index e3147eb74222..ca3f2c2edd85 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -287,6 +287,20 @@ void blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set, int nr_hw_queues);
 
 void blk_mq_quiesce_queue_nowait(struct request_queue *q);
 
+/**
+ * blk_mq_mark_complete() - Set request state to complete
+ * @rq: request to set to complete state
+ *
+ * Returns true if request state was successfully set to complete. If
+ * successful, the caller is responsibile for seeing this request is ended, as
+ * blk_mq_complete_request will not work again.
+ */
+static inline bool blk_mq_mark_complete(struct request *rq)
+{
+	return cmpxchg(&rq->state, MQ_RQ_IN_FLIGHT, MQ_RQ_COMPLETE) ==
+			MQ_RQ_IN_FLIGHT;
+}
+
 /*
  * Driver command data is immediately after the request. So subtract request
  * size to get back to the original request, add request size to get the PDU.

commit 6e768717304bdbe8d2897ca8298f6b58863fdc41
Author: Ming Lei <ming.lei@redhat.com>
Date:   Tue Jul 3 09:03:16 2018 -0600

    blk-mq: dequeue request one by one from sw queue if hctx is busy
    
    It won't be efficient to dequeue request one by one from sw queue,
    but we have to do that when queue is busy for better merge performance.
    
    This patch takes the Exponential Weighted Moving Average(EWMA) to figure
    out if queue is busy, then only dequeue request one by one from sw queue
    when queue is busy.
    
    Fixes: b347689ffbca ("blk-mq-sched: improve dispatching from sw queue")
    Cc: Kashyap Desai <kashyap.desai@broadcom.com>
    Cc: Laurence Oberman <loberman@redhat.com>
    Cc: Omar Sandoval <osandov@fb.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Bart Van Assche <bart.vanassche@wdc.com>
    Cc: Hannes Reinecke <hare@suse.de>
    Reported-by: Kashyap Desai <kashyap.desai@broadcom.com>
    Tested-by: Kashyap Desai <kashyap.desai@broadcom.com>
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index ea690254dab7..d710e92874cc 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -35,9 +35,10 @@ struct blk_mq_hw_ctx {
 	struct sbitmap		ctx_map;
 
 	struct blk_mq_ctx	*dispatch_from;
+	unsigned int		dispatch_busy;
 
-	struct blk_mq_ctx	**ctxs;
 	unsigned int		nr_ctx;
+	struct blk_mq_ctx	**ctxs;
 
 	spinlock_t		dispatch_wait_lock;
 	wait_queue_entry_t	dispatch_wait;

commit 5815839b3ca16bb1d45939270871169f6803a121
Author: Ming Lei <ming.lei@redhat.com>
Date:   Mon Jun 25 19:31:47 2018 +0800

    blk-mq: introduce new lock for protecting hctx->dispatch_wait
    
    Now hctx->lock is only acquired when adding hctx->dispatch_wait to
    one wait queue, but not held when removing it from the wait queue.
    
    IO hang can be observed easily if SCHED RESTART is disabled, that means
    now RESTART exits just for fixing the issue in blk_mq_mark_tag_wait().
    
    This patch fixes the issue by introducing hctx->dispatch_wait_lock and
    holding it for removing hctx->dispatch_wait in blk_mq_dispatch_wake(),
    since we need to avoid acquiring hctx->lock in irq context.
    
    Fixes: eb619fdb2d4cb8b3d3419 ("blk-mq: fix issue with shared tag queue re-running")
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Omar Sandoval <osandov@fb.com>
    Cc: Bart Van Assche <bart.vanassche@wdc.com>
    Tested-by: Andrew Jones <drjones@redhat.com>
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index e3147eb74222..ea690254dab7 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -39,6 +39,7 @@ struct blk_mq_hw_ctx {
 	struct blk_mq_ctx	**ctxs;
 	unsigned int		nr_ctx;
 
+	spinlock_t		dispatch_wait_lock;
 	wait_queue_entry_t	dispatch_wait;
 	atomic_t		wait_index;
 

commit e6c3456aa897c9799de5423b28550efad14a51b0
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Jun 14 14:26:47 2018 +0200

    blk-mq: remove blk_mq_tagset_iter
    
    Unused now that nvme stopped using it.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index fb355173f3c7..e3147eb74222 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -281,8 +281,6 @@ void blk_freeze_queue_start(struct request_queue *q);
 void blk_mq_freeze_queue_wait(struct request_queue *q);
 int blk_mq_freeze_queue_wait_timeout(struct request_queue *q,
 				     unsigned long timeout);
-int blk_mq_tagset_iter(struct blk_mq_tag_set *set, void *data,
-		int (reinit_request)(void *, struct request *));
 
 int blk_mq_map_queues(struct blk_mq_tag_set *set);
 void blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set, int nr_hw_queues);

commit 9c5587346490ad4355e8de6ae402b76e55c411d5
Author: Jens Axboe <axboe@kernel.dk>
Date:   Wed May 30 15:26:07 2018 +0800

    blk-mq: abstract out blk-mq-sched rq list iteration bio merge helper
    
    No functional changes in this patch, just a prep patch for utilizing
    this in an IO scheduler.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>
    Reviewed-by: Omar Sandoval <osandov@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index ebc34a5686dc..fb355173f3c7 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -259,7 +259,8 @@ void blk_mq_add_to_requeue_list(struct request *rq, bool at_head,
 void blk_mq_kick_requeue_list(struct request_queue *q);
 void blk_mq_delay_kick_requeue_list(struct request_queue *q, unsigned long msecs);
 void blk_mq_complete_request(struct request *rq);
-
+bool blk_mq_bio_list_merge(struct request_queue *q, struct list_head *list,
+			   struct bio *bio);
 bool blk_mq_queue_stopped(struct request_queue *q);
 void blk_mq_stop_hw_queue(struct blk_mq_hw_ctx *hctx);
 void blk_mq_start_hw_queue(struct blk_mq_hw_ctx *hctx);

commit fe644072dfee069d97a66ea9a80f4bc461499e6a
Author: Linus Walleij <linus.walleij@linaro.org>
Date:   Fri Apr 20 10:29:51 2018 +0200

    block: mq: Add some minor doc for core structs
    
    As it came up in discussion on the mailing list that the semantic
    meaning of 'blk_mq_ctx' and 'blk_mq_hw_ctx' isn't completely
    obvious to everyone, let's add some minimal kerneldoc for a
    starter.
    
    Signed-off-by: Linus Walleij <linus.walleij@linaro.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index e3986f4b3461..ebc34a5686dc 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -9,6 +9,9 @@
 struct blk_mq_tags;
 struct blk_flush_queue;
 
+/**
+ * struct blk_mq_hw_ctx - State for a hardware queue facing the hardware block device
+ */
 struct blk_mq_hw_ctx {
 	struct {
 		spinlock_t		lock;

commit 15fe8a90bb45b953ca36f074194fcb519a05fdec
Author: Ming Lei <ming.lei@redhat.com>
Date:   Sun Apr 8 17:48:11 2018 +0800

    blk-mq: remove blk_mq_delay_queue()
    
    No driver uses this interface any more, so remove it.
    
    Cc: Stefan Haberland <sth@linux.vnet.ibm.com>
    Tested-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 8efcf49796a3..e3986f4b3461 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -183,7 +183,6 @@ enum {
 	BLK_MQ_S_STOPPED	= 0,
 	BLK_MQ_S_TAG_ACTIVE	= 1,
 	BLK_MQ_S_SCHED_RESTART	= 2,
-	BLK_MQ_S_START_ON_RUN	= 3,
 
 	BLK_MQ_MAX_DEPTH	= 10240,
 
@@ -270,7 +269,6 @@ void blk_mq_unquiesce_queue(struct request_queue *q);
 void blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, unsigned long msecs);
 bool blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async);
 void blk_mq_run_hw_queues(struct request_queue *q, bool async);
-void blk_mq_delay_queue(struct blk_mq_hw_ctx *hctx, unsigned long msecs);
 void blk_mq_tagset_busy_iter(struct blk_mq_tag_set *tagset,
 		busy_tag_iter_fn *fn, void *priv);
 void blk_mq_freeze_queue(struct request_queue *q);

commit 05707b64aed8f5f1674b25334fb720d651459d5e
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jan 9 08:29:53 2018 -0800

    blk-mq: rename blk_mq_hw_ctx->queue_rq_srcu to ->srcu
    
    The RCU protection has been expanded to cover both queueing and
    completion paths making ->queue_rq_srcu a misnomer.  Rename it to
    ->srcu as suggested by Bart.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Bart Van Assche <Bart.VanAssche@wdc.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 460798dbac1f..8efcf49796a3 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -66,7 +66,7 @@ struct blk_mq_hw_ctx {
 #endif
 
 	/* Must be the last member - see also blk_mq_hw_ctx_size(). */
-	struct srcu_struct	queue_rq_srcu[0];
+	struct srcu_struct	srcu[0];
 };
 
 struct blk_mq_tag_set {

commit 1d9bd5161ba32db5665a617edc8b0723880f543e
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jan 9 08:29:48 2018 -0800

    blk-mq: replace timeout synchronization with a RCU and generation based scheme
    
    Currently, blk-mq timeout path synchronizes against the usual
    issue/completion path using a complex scheme involving atomic
    bitflags, REQ_ATOM_*, memory barriers and subtle memory coherence
    rules.  Unfortunately, it contains quite a few holes.
    
    There's a complex dancing around REQ_ATOM_STARTED and
    REQ_ATOM_COMPLETE between issue/completion and timeout paths; however,
    they don't have a synchronization point across request recycle
    instances and it isn't clear what the barriers add.
    blk_mq_check_expired() can easily read STARTED from N-2'th iteration,
    deadline from N-1'th, blk_mark_rq_complete() against Nth instance.
    
    In fact, it's pretty easy to make blk_mq_check_expired() terminate a
    later instance of a request.  If we induce 5 sec delay before
    time_after_eq() test in blk_mq_check_expired(), shorten the timeout to
    2s, and issue back-to-back large IOs, blk-mq starts timing out
    requests spuriously pretty quickly.  Nothing actually timed out.  It
    just made the call on a recycle instance of a request and then
    terminated a later instance long after the original instance finished.
    The scenario isn't theoretical either.
    
    This patch replaces the broken synchronization mechanism with a RCU
    and generation number based one.
    
    1. Each request has a u64 generation + state value, which can be
       updated only by the request owner.  Whenever a request becomes
       in-flight, the generation number gets bumped up too.  This provides
       the basis for the timeout path to distinguish different recycle
       instances of the request.
    
       Also, marking a request in-flight and setting its deadline are
       protected with a seqcount so that the timeout path can fetch both
       values coherently.
    
    2. The timeout path fetches the generation, state and deadline.  If
       the verdict is timeout, it records the generation into a dedicated
       request abortion field and does RCU wait.
    
    3. The completion path is also protected by RCU (from the previous
       patch) and checks whether the current generation number and state
       match the abortion field.  If so, it skips completion.
    
    4. The timeout path, after RCU wait, scans requests again and
       terminates the ones whose generation and state still match the ones
       requested for abortion.
    
       By now, the timeout path knows that either the generation number
       and state changed if it lost the race or the completion will yield
       to it and can safely timeout the request.
    
    While it's more lines of code, it's conceptually simpler, doesn't
    depend on direct use of subtle memory ordering or coherence, and
    hopefully doesn't terminate the wrong instance.
    
    While this change makes REQ_ATOM_COMPLETE synchronization unnecessary
    between issue/complete and timeout paths, REQ_ATOM_COMPLETE isn't
    removed yet as it's still used in other places.  Future patches will
    move all state tracking to the new mechanism and remove all bitops in
    the hot paths.
    
    Note that this patch adds a comment explaining a race condition in
    BLK_EH_RESET_TIMER path.  The race has always been there and this
    patch doesn't change it.  It's just documenting the existing race.
    
    v2: - Fixed BLK_EH_RESET_TIMER handling as pointed out by Jianchao.
        - s/request->gstate_seqc/request->gstate_seq/ as suggested by Peter.
        - READ_ONCE() added in blk_mq_rq_update_state() as suggested by Peter.
    
    v3: - Fixed possible extended seqcount / u64_stats_sync read looping
          spotted by Peter.
        - MQ_RQ_IDLE was incorrectly being set in complete_request instead
          of free_request.  Fixed.
    
    v4: - Rebased on top of hctx_lock() refactoring patch.
        - Added comment explaining the use of hctx_lock() in completion path.
    
    v5: - Added comments requested by Bart.
        - Note the addition of BLK_EH_RESET_TIMER race condition in the
          commit message.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: "jianchao.wang" <jianchao.w.wang@oracle.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Bart Van Assche <Bart.VanAssche@wdc.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 95c9a5c862e2..460798dbac1f 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -51,6 +51,7 @@ struct blk_mq_hw_ctx {
 	unsigned int		queue_num;
 
 	atomic_t		nr_active;
+	unsigned int		nr_expired;
 
 	struct hlist_node	cpuhp_dead;
 	struct kobject		kobj;

commit e2c5923c349c1738fe8fda980874d93f6fb2e5b6
Merge: abc36be23635 a04b5de5050a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Nov 14 15:32:19 2017 -0800

    Merge branch 'for-4.15/block' of git://git.kernel.dk/linux-block
    
    Pull core block layer updates from Jens Axboe:
     "This is the main pull request for block storage for 4.15-rc1.
    
      Nothing out of the ordinary in here, and no API changes or anything
      like that. Just various new features for drivers, core changes, etc.
      In particular, this pull request contains:
    
       - A patch series from Bart, closing the whole on blk/scsi-mq queue
         quescing.
    
       - A series from Christoph, building towards hidden gendisks (for
         multipath) and ability to move bio chains around.
    
       - NVMe
            - Support for native multipath for NVMe (Christoph).
            - Userspace notifications for AENs (Keith).
            - Command side-effects support (Keith).
            - SGL support (Chaitanya Kulkarni)
            - FC fixes and improvements (James Smart)
            - Lots of fixes and tweaks (Various)
    
       - bcache
            - New maintainer (Michael Lyle)
            - Writeback control improvements (Michael)
            - Various fixes (Coly, Elena, Eric, Liang, et al)
    
       - lightnvm updates, mostly centered around the pblk interface
         (Javier, Hans, and Rakesh).
    
       - Removal of unused bio/bvec kmap atomic interfaces (me, Christoph)
    
       - Writeback series that fix the much discussed hundreds of millions
         of sync-all units. This goes all the way, as discussed previously
         (me).
    
       - Fix for missing wakeup on writeback timer adjustments (Yafang
         Shao).
    
       - Fix laptop mode on blk-mq (me).
    
       - {mq,name} tupple lookup for IO schedulers, allowing us to have
         alias names. This means you can use 'deadline' on both !mq and on
         mq (where it's called mq-deadline). (me).
    
       - blktrace race fix, oopsing on sg load (me).
    
       - blk-mq optimizations (me).
    
       - Obscure waitqueue race fix for kyber (Omar).
    
       - NBD fixes (Josef).
    
       - Disable writeback throttling by default on bfq, like we do on cfq
         (Luca Miccio).
    
       - Series from Ming that enable us to treat flush requests on blk-mq
         like any other request. This is a really nice cleanup.
    
       - Series from Ming that improves merging on blk-mq with schedulers,
         getting us closer to flipping the switch on scsi-mq again.
    
       - BFQ updates (Paolo).
    
       - blk-mq atomic flags memory ordering fixes (Peter Z).
    
       - Loop cgroup support (Shaohua).
    
       - Lots of minor fixes from lots of different folks, both for core and
         driver code"
    
    * 'for-4.15/block' of git://git.kernel.dk/linux-block: (294 commits)
      nvme: fix visibility of "uuid" ns attribute
      blk-mq: fixup some comment typos and lengths
      ide: ide-atapi: fix compile error with defining macro DEBUG
      blk-mq: improve tag waiting setup for non-shared tags
      brd: remove unused brd_mutex
      blk-mq: only run the hardware queue if IO is pending
      block: avoid null pointer dereference on null disk
      fs: guard_bio_eod() needs to consider partitions
      xtensa/simdisk: fix compile error
      nvme: expose subsys attribute to sysfs
      nvme: create 'slaves' and 'holders' entries for hidden controllers
      block: create 'slaves' and 'holders' entries for hidden gendisks
      nvme: also expose the namespace identification sysfs files for mpath nodes
      nvme: implement multipath access to nvme subsystems
      nvme: track shared namespaces
      nvme: introduce a nvme_ns_ids structure
      nvme: track subsystems
      block, nvme: Introduce blk_mq_req_flags_t
      block, scsi: Make SCSI quiesce and resume work reliably
      block: Add the QUEUE_FLAG_PREEMPT_ONLY request queue flag
      ...

commit 79f720a751cad613620d0237e3b44f89f4a69181
Author: Jens Axboe <axboe@kernel.dk>
Date:   Fri Nov 10 09:13:21 2017 -0700

    blk-mq: only run the hardware queue if IO is pending
    
    Currently we are inconsistent in when we decide to run the queue. Using
    blk_mq_run_hw_queues() we check if the hctx has pending IO before
    running it, but we don't do that from the individual queue run function,
    blk_mq_run_hw_queue(). This results in a lot of extra and pointless
    queue runs, potentially, on flush requests and (much worse) on tag
    starvation situations. This is observable just looking at top output,
    with lots of kworkers active. For the !async runs, it just adds to the
    CPU overhead of blk-mq.
    
    Move the has-pending check into the run function instead of having
    callers do it.
    
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index b326208277ee..eb1e2cdffb31 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -266,7 +266,7 @@ void blk_mq_start_stopped_hw_queues(struct request_queue *q, bool async);
 void blk_mq_quiesce_queue(struct request_queue *q);
 void blk_mq_unquiesce_queue(struct request_queue *q);
 void blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, unsigned long msecs);
-void blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async);
+bool blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async);
 void blk_mq_run_hw_queues(struct request_queue *q, bool async);
 void blk_mq_delay_queue(struct blk_mq_hw_ctx *hctx, unsigned long msecs);
 void blk_mq_tagset_busy_iter(struct blk_mq_tag_set *tagset,

commit 9a95e4ef709533efac4aafcb8bddf73f96db50ed
Author: Bart Van Assche <bart.vanassche@wdc.com>
Date:   Thu Nov 9 10:49:59 2017 -0800

    block, nvme: Introduce blk_mq_req_flags_t
    
    Several block layer and NVMe core functions accept a combination
    of BLK_MQ_REQ_* flags through the 'flags' argument but there is
    no verification at compile time whether the right type of block
    layer flags is passed. Make it possible for sparse to verify this.
    This patch does not change any functionality.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@wdc.com>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Tested-by: Oleksandr Natalenko <oleksandr@natalenko.name>
    Cc: linux-nvme@lists.infradead.org
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Johannes Thumshirn <jthumshirn@suse.de>
    Cc: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 82b56609736a..b326208277ee 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -209,16 +209,21 @@ void blk_mq_free_request(struct request *rq);
 bool blk_mq_can_queue(struct blk_mq_hw_ctx *);
 
 enum {
-	BLK_MQ_REQ_NOWAIT	= (1 << 0), /* return when out of requests */
-	BLK_MQ_REQ_RESERVED	= (1 << 1), /* allocate from reserved pool */
-	BLK_MQ_REQ_INTERNAL	= (1 << 2), /* allocate internal/sched tag */
-	BLK_MQ_REQ_PREEMPT	= (1 << 3), /* set RQF_PREEMPT */
+	/* return when out of requests */
+	BLK_MQ_REQ_NOWAIT	= (__force blk_mq_req_flags_t)(1 << 0),
+	/* allocate from reserved pool */
+	BLK_MQ_REQ_RESERVED	= (__force blk_mq_req_flags_t)(1 << 1),
+	/* allocate internal/sched tag */
+	BLK_MQ_REQ_INTERNAL	= (__force blk_mq_req_flags_t)(1 << 2),
+	/* set RQF_PREEMPT */
+	BLK_MQ_REQ_PREEMPT	= (__force blk_mq_req_flags_t)(1 << 3),
 };
 
 struct request *blk_mq_alloc_request(struct request_queue *q, unsigned int op,
-		unsigned int flags);
+		blk_mq_req_flags_t flags);
 struct request *blk_mq_alloc_request_hctx(struct request_queue *q,
-		unsigned int op, unsigned int flags, unsigned int hctx_idx);
+		unsigned int op, blk_mq_req_flags_t flags,
+		unsigned int hctx_idx);
 struct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag);
 
 enum {

commit 1b6d65a0bfb5df2a6029c1430e99fcc5d96bb59a
Author: Bart Van Assche <bart.vanassche@wdc.com>
Date:   Thu Nov 9 10:49:55 2017 -0800

    block: Introduce BLK_MQ_REQ_PREEMPT
    
    Set RQF_PREEMPT if BLK_MQ_REQ_PREEMPT is passed to
    blk_get_request_flags().
    
    Signed-off-by: Bart Van Assche <bart.vanassche@wdc.com>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Tested-by: Martin Steigerwald <martin@lichtvoll.de>
    Tested-by: Oleksandr Natalenko <oleksandr@natalenko.name>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Ming Lei <ming.lei@redhat.com>
    Cc: Johannes Thumshirn <jthumshirn@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 4ae987c2352c..82b56609736a 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -212,6 +212,7 @@ enum {
 	BLK_MQ_REQ_NOWAIT	= (1 << 0), /* return when out of requests */
 	BLK_MQ_REQ_RESERVED	= (1 << 1), /* allocate from reserved pool */
 	BLK_MQ_REQ_INTERNAL	= (1 << 2), /* allocate internal/sched tag */
+	BLK_MQ_REQ_PREEMPT	= (1 << 3), /* set RQF_PREEMPT */
 };
 
 struct request *blk_mq_alloc_request(struct request_queue *q, unsigned int op,

commit eb619fdb2d4cb8b3d3419e9113921e87e7daf557
Author: Jens Axboe <axboe@kernel.dk>
Date:   Thu Nov 9 08:32:43 2017 -0700

    blk-mq: fix issue with shared tag queue re-running
    
    This patch attempts to make the case of hctx re-running on driver tag
    failure more robust. Without this patch, it's pretty easy to trigger a
    stall condition with shared tags. An example is using null_blk like
    this:
    
    modprobe null_blk queue_mode=2 nr_devices=4 shared_tags=1 submit_queues=1 hw_queue_depth=1
    
    which sets up 4 devices, sharing the same tag set with a depth of 1.
    Running a fio job ala:
    
    [global]
    bs=4k
    rw=randread
    norandommap
    direct=1
    ioengine=libaio
    iodepth=4
    
    [nullb0]
    filename=/dev/nullb0
    [nullb1]
    filename=/dev/nullb1
    [nullb2]
    filename=/dev/nullb2
    [nullb3]
    filename=/dev/nullb3
    
    will inevitably end with one or more threads being stuck waiting for a
    scheduler tag. That IO is then stuck forever, until someone else
    triggers a run of the queue.
    
    Ensure that we always re-run the hardware queue, if the driver tag we
    were waiting for got freed before we added our leftover request entries
    back on the dispatch list.
    
    Reviewed-by: Bart Van Assche <bart.vanassche@wdc.com>
    Tested-by: Bart Van Assche <bart.vanassche@wdc.com>
    Reviewed-by: Ming Lei <ming.lei@redhat.com>
    Reviewed-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 674641527da7..4ae987c2352c 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -35,7 +35,7 @@ struct blk_mq_hw_ctx {
 	struct blk_mq_ctx	**ctxs;
 	unsigned int		nr_ctx;
 
-	wait_queue_entry_t		dispatch_wait;
+	wait_queue_entry_t	dispatch_wait;
 	atomic_t		wait_index;
 
 	struct blk_mq_tags	*tags;
@@ -181,8 +181,7 @@ enum {
 	BLK_MQ_S_STOPPED	= 0,
 	BLK_MQ_S_TAG_ACTIVE	= 1,
 	BLK_MQ_S_SCHED_RESTART	= 2,
-	BLK_MQ_S_TAG_WAITING	= 3,
-	BLK_MQ_S_START_ON_RUN	= 4,
+	BLK_MQ_S_START_ON_RUN	= 3,
 
 	BLK_MQ_MAX_DEPTH	= 10240,
 

commit 88022d7201e96b43f1754b0358fc6bcd8dbdcde1
Author: Ming Lei <ming.lei@redhat.com>
Date:   Sun Nov 5 02:21:12 2017 +0800

    blk-mq: don't handle failure in .get_budget
    
    It is enough to just check if we can get the budget via .get_budget().
    And we don't need to deal with device state change in .get_budget().
    
    For SCSI, one issue to be fixed is that we have to call
    scsi_mq_uninit_cmd() to free allocated ressources if SCSI device fails
    to handle the request. And it isn't enough to simply call
    blk_mq_end_request() to do that if this request is marked as
    RQF_DONTPREP.
    
    Fixes: 0df21c86bdbf(scsi: implement .get_budget and .put_budget for blk-mq)
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index f2e3079eecdd..674641527da7 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -92,7 +92,7 @@ struct blk_mq_queue_data {
 
 typedef blk_status_t (queue_rq_fn)(struct blk_mq_hw_ctx *,
 		const struct blk_mq_queue_data *);
-typedef blk_status_t (get_budget_fn)(struct blk_mq_hw_ctx *);
+typedef bool (get_budget_fn)(struct blk_mq_hw_ctx *);
 typedef void (put_budget_fn)(struct blk_mq_hw_ctx *);
 typedef enum blk_eh_timer_return (timeout_fn)(struct request *, bool);
 typedef int (init_hctx_fn)(struct blk_mq_hw_ctx *, void *, unsigned int);

commit 3e2cb3ad47500ed12d4c8b4cbd737dca352e38e4
Merge: 474f5da2354e a806c6c81e6c
Author: Jens Axboe <axboe@kernel.dk>
Date:   Fri Nov 3 10:28:51 2017 -0600

    Merge branch 'nvme-4.15' of git://git.infradead.org/nvme into for-4.15/block
    
    Pull NVMe changes from Christoph:
    
    "Below are the currently queue nvme updates for Linux 4.15.  There are
    a few more things that could make it for this merge window, but I'd
    like to get things into linux-next, especially for the unlikely case
    that Linus decided to cut -rc8.
    
    Highlights:
     - support for SGLs in the PCIe driver (Chaitanya Kulkarni)
     - disable I/O schedulers for the admin queue (Israel Rukshin)
     - various Fibre Channel fixes and enhancements (James Smart)
     - various refactoring for better code sharing between transports
       (Sagi Grimberg and me)
    
    as well as lots of little bits from various contributors."

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 50c6485cb04f..994cbb0f7ffc 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef BLK_MQ_H
 #define BLK_MQ_H
 

commit b347689ffbca745ac457ee27400ce1affd571c6f
Author: Ming Lei <ming.lei@redhat.com>
Date:   Sat Oct 14 17:22:30 2017 +0800

    blk-mq-sched: improve dispatching from sw queue
    
    SCSI devices use host-wide tagset, and the shared driver tag space is
    often quite big. However, there is also a queue depth for each lun(
    .cmd_per_lun), which is often small, for example, on both lpfc and
    qla2xxx, .cmd_per_lun is just 3.
    
    So lots of requests may stay in sw queue, and we always flush all
    belonging to same hw queue and dispatch them all to driver.
    Unfortunately it is easy to cause queue busy because of the small
    .cmd_per_lun.  Once these requests are flushed out, they have to stay in
    hctx->dispatch, and no bio merge can happen on these requests, and
    sequential IO performance is harmed.
    
    This patch introduces blk_mq_dequeue_from_ctx for dequeuing a request
    from a sw queue, so that we can dispatch them in scheduler's way. We can
    then avoid dequeueing too many requests from sw queue, since we don't
    flush ->dispatch completely.
    
    This patch improves dispatching from sw queue by using the .get_budget
    and .put_budget callbacks.
    
    Reviewed-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 901457df3d64..e5e6becd57d3 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -30,6 +30,8 @@ struct blk_mq_hw_ctx {
 
 	struct sbitmap		ctx_map;
 
+	struct blk_mq_ctx	*dispatch_from;
+
 	struct blk_mq_ctx	**ctxs;
 	unsigned int		nr_ctx;
 

commit de1482974080ec9ef414bf048b2646b246b63f6e
Author: Ming Lei <ming.lei@redhat.com>
Date:   Sat Oct 14 17:22:29 2017 +0800

    blk-mq: introduce .get_budget and .put_budget in blk_mq_ops
    
    For SCSI devices, there is often a per-request-queue depth, which needs
    to be respected before queuing one request.
    
    Currently blk-mq always dequeues the request first, then calls
    .queue_rq() to dispatch the request to lld. One obvious issue with this
    approach is that I/O merging may not be successful, because when the
    per-request-queue depth can't be respected, .queue_rq() has to return
    BLK_STS_RESOURCE, and then this request has to stay in hctx->dispatch
    list. This means it never gets a chance to be merged with other IO.
    
    This patch introduces .get_budget and .put_budget callback in blk_mq_ops,
    then we can try to get reserved budget first before dequeuing request.
    If the budget for queueing I/O can't be satisfied, we don't need to
    dequeue request at all. Hence the request can be left in the IO
    scheduler queue, for more merging opportunities.
    
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 50c6485cb04f..901457df3d64 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -90,6 +90,8 @@ struct blk_mq_queue_data {
 
 typedef blk_status_t (queue_rq_fn)(struct blk_mq_hw_ctx *,
 		const struct blk_mq_queue_data *);
+typedef blk_status_t (get_budget_fn)(struct blk_mq_hw_ctx *);
+typedef void (put_budget_fn)(struct blk_mq_hw_ctx *);
 typedef enum blk_eh_timer_return (timeout_fn)(struct request *, bool);
 typedef int (init_hctx_fn)(struct blk_mq_hw_ctx *, void *, unsigned int);
 typedef void (exit_hctx_fn)(struct blk_mq_hw_ctx *, unsigned int);
@@ -111,6 +113,15 @@ struct blk_mq_ops {
 	 */
 	queue_rq_fn		*queue_rq;
 
+	/*
+	 * Reserve budget before queue request, once .queue_rq is
+	 * run, it is driver's responsibility to release the
+	 * reserved budget. Also we have to handle failure case
+	 * of .get_budget for avoiding I/O deadlock.
+	 */
+	get_budget_fn		*get_budget;
+	put_budget_fn		*put_budget;
+
 	/*
 	 * Called on request timeout
 	 */

commit dab7487bdf63c6f2d70aac604494d023b189a9fd
Author: Sagi Grimberg <sagi@grimberg.me>
Date:   Wed Oct 11 12:53:08 2017 +0300

    block: remove blk_mq_reinit_tagset
    
    No callers left.
    
    Reviewed-by: Jens Axboe <axboe@kernel.dk>
    Reviewed-by: Bart Van Assche <bart.vanassche@wdc.com>
    Reviewed-by: Max Gurtovoy <maxg@mellanox.com>
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Signed-off-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 6bc29f73a9aa..cfd64e500d82 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -261,8 +261,6 @@ int blk_mq_freeze_queue_wait_timeout(struct request_queue *q,
 				     unsigned long timeout);
 int blk_mq_tagset_iter(struct blk_mq_tag_set *set, void *data,
 		int (reinit_request)(void *, struct request *));
-int blk_mq_reinit_tagset(struct blk_mq_tag_set *set,
-			 int (reinit_request)(void *, struct request *));
 
 int blk_mq_map_queues(struct blk_mq_tag_set *set);
 void blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set, int nr_hw_queues);

commit 149e10f8ff71439014dff97987e90e87e2684a16
Author: Sagi Grimberg <sagi@grimberg.me>
Date:   Wed Oct 11 12:53:06 2017 +0300

    block: introduce blk_mq_tagset_iter
    
    Iterator helper to apply a function on all the
    tags in a given tagset. export it as it will be used
    outside the block layer later on.
    
    Reviewed-by: Bart Van Assche <bart.vanassche@wdc.com>
    Reviewed-by: Jens Axboe <axboe@kernel.dk>
    Reviewed-by: Max Gurtovoy <maxg@mellanox.com>
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Signed-off-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 50c6485cb04f..6bc29f73a9aa 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -259,6 +259,8 @@ void blk_freeze_queue_start(struct request_queue *q);
 void blk_mq_freeze_queue_wait(struct request_queue *q);
 int blk_mq_freeze_queue_wait_timeout(struct request_queue *q,
 				     unsigned long timeout);
+int blk_mq_tagset_iter(struct blk_mq_tag_set *set, void *data,
+		int (reinit_request)(void *, struct request *));
 int blk_mq_reinit_tagset(struct blk_mq_tag_set *set,
 			 int (reinit_request)(void *, struct request *));
 

commit d352ae205d8b05f3f7558d10f474d8436581b3e2
Author: Bart Van Assche <bart.vanassche@wdc.com>
Date:   Thu Aug 17 16:23:03 2017 -0700

    blk-mq: Make blk_mq_reinit_tagset() calls easier to read
    
    Since blk_mq_ops.reinit_request is only called from inside
    blk_mq_reinit_tagset(), make this function pointer an argument of
    blk_mq_reinit_tagset() instead of a member of struct blk_mq_ops.
    This patch does not change any functionality but makes
    blk_mq_reinit_tagset() calls easier to read and to analyze.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Sagi Grimberg <sagi@grimberg.me>
    Cc: James Smart <james.smart@broadcom.com>
    Cc: Johannes Thumshirn <jthumshirn@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 14542308d25b..50c6485cb04f 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -97,7 +97,6 @@ typedef int (init_request_fn)(struct blk_mq_tag_set *set, struct request *,
 		unsigned int, unsigned int);
 typedef void (exit_request_fn)(struct blk_mq_tag_set *set, struct request *,
 		unsigned int);
-typedef int (reinit_request_fn)(void *, struct request *);
 
 typedef void (busy_iter_fn)(struct blk_mq_hw_ctx *, struct request *, void *,
 		bool);
@@ -143,7 +142,6 @@ struct blk_mq_ops {
 	 */
 	init_request_fn		*init_request;
 	exit_request_fn		*exit_request;
-	reinit_request_fn	*reinit_request;
 	/* Called from inside blk_get_request() */
 	void (*initialize_rq_fn)(struct request *rq);
 
@@ -261,7 +259,8 @@ void blk_freeze_queue_start(struct request_queue *q);
 void blk_mq_freeze_queue_wait(struct request_queue *q);
 int blk_mq_freeze_queue_wait_timeout(struct request_queue *q,
 				     unsigned long timeout);
-int blk_mq_reinit_tagset(struct blk_mq_tag_set *set);
+int blk_mq_reinit_tagset(struct blk_mq_tag_set *set,
+			 int (reinit_request)(void *, struct request *));
 
 int blk_mq_map_queues(struct blk_mq_tag_set *set);
 void blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set, int nr_hw_queues);

commit 9bd42183b951051f73de121f7ee17091e7d26fbb
Merge: 7447d56217e2 72298e5c92c5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 3 13:08:04 2017 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - Add the SYSTEM_SCHEDULING bootup state to move various scheduler
         debug checks earlier into the bootup. This turns silent and
         sporadically deadly bugs into nice, deterministic splats. Fix some
         of the splats that triggered. (Thomas Gleixner)
    
       - A round of restructuring and refactoring of the load-balancing and
         topology code (Peter Zijlstra)
    
       - Another round of consolidating ~20 of incremental scheduler code
         history: this time in terms of wait-queue nomenclature. (I didn't
         get much feedback on these renaming patches, and we can still
         easily change any names I might have misplaced, so if anyone hates
         a new name, please holler and I'll fix it.) (Ingo Molnar)
    
       - sched/numa improvements, fixes and updates (Rik van Riel)
    
       - Another round of x86/tsc scheduler clock code improvements, in hope
         of making it more robust (Peter Zijlstra)
    
       - Improve NOHZ behavior (Frederic Weisbecker)
    
       - Deadline scheduler improvements and fixes (Luca Abeni, Daniel
         Bristot de Oliveira)
    
       - Simplify and optimize the topology setup code (Lauro Ramos
         Venancio)
    
       - Debloat and decouple scheduler code some more (Nicolas Pitre)
    
       - Simplify code by making better use of llist primitives (Byungchul
         Park)
    
       - ... plus other fixes and improvements"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (103 commits)
      sched/cputime: Refactor the cputime_adjust() code
      sched/debug: Expose the number of RT/DL tasks that can migrate
      sched/numa: Hide numa_wake_affine() from UP build
      sched/fair: Remove effective_load()
      sched/numa: Implement NUMA node level wake_affine()
      sched/fair: Simplify wake_affine() for the single socket case
      sched/numa: Override part of migrate_degrades_locality() when idle balancing
      sched/rt: Move RT related code from sched/core.c to sched/rt.c
      sched/deadline: Move DL related code from sched/core.c to sched/deadline.c
      sched/cpuset: Only offer CONFIG_CPUSETS if SMP is enabled
      sched/fair: Spare idle load balancing on nohz_full CPUs
      nohz: Move idle balancer registration to the idle path
      sched/loadavg: Generalize "_idle" naming to "_nohz"
      sched/core: Drop the unused try_get_task_struct() helper function
      sched/fair: WARN() and refuse to set buddy when !se->on_rq
      sched/debug: Fix SCHED_WARN_ON() to return a value on !CONFIG_SCHED_DEBUG as well
      sched/wait: Disambiguate wq_entry->task_list and wq_head->task_list naming
      sched/wait: Move bit_wait_table[] and related functionality from sched/core.c to sched/wait_bit.c
      sched/wait: Split out the wait_bit*() APIs from <linux/wait.h> into <linux/wait_bit.h>
      sched/wait: Re-adjust macro line continuation backslashes in <linux/wait.h>
      ...

commit 852ec80983d682dc08a0573d37eeaa9814c4f6b1
Author: Bart Van Assche <bart.vanassche@wdc.com>
Date:   Wed Jun 21 10:55:47 2017 -0700

    blk-mq: Make it safe to quiesce and unquiesce from an interrupt handler
    
    Since blk_mq_quiesce_queue_nowait() can be called from interrupt
    context, make this safe. Since this function is not in the hot
    path, uninline it.
    
    Fixes: commit f4560ffe8cec ("blk-mq: use QUEUE_FLAG_QUIESCED to quiesce queue")
    Signed-off-by: Bart Van Assche <bart.vanassche@wdc.com>
    Cc: Ming Lei <ming.lei@redhat.com>
    Cc: Hannes Reinecke <hare@suse.com>
    Cc: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 366b83cee955..23d32ff0b462 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -266,15 +266,7 @@ int blk_mq_reinit_tagset(struct blk_mq_tag_set *set);
 int blk_mq_map_queues(struct blk_mq_tag_set *set);
 void blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set, int nr_hw_queues);
 
-/*
- * FIXME: this helper is just for working around mpt3sas.
- */
-static inline void blk_mq_quiesce_queue_nowait(struct request_queue *q)
-{
-	spin_lock_irq(q->queue_lock);
-	queue_flag_set(QUEUE_FLAG_QUIESCED, q);
-	spin_unlock_irq(q->queue_lock);
-}
+void blk_mq_quiesce_queue_nowait(struct request_queue *q);
 
 /*
  * Driver command data is immediately after the request. So subtract request

commit d280bab305431c1836423f3cd6a5ff0e35a601ef
Author: Bart Van Assche <bart.vanassche@sandisk.com>
Date:   Tue Jun 20 11:15:40 2017 -0700

    block: Introduce request_queue.initialize_rq_fn()
    
    Several block drivers need to initialize the driver-private request
    data after having called blk_get_request() and before .prep_rq_fn()
    is called, e.g. when submitting a REQ_OP_SCSI_* request. Avoid that
    that initialization code has to be repeated after every
    blk_get_request() call by adding new callback functions to struct
    request_queue and to struct blk_mq_ops.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Hannes Reinecke <hare@suse.com>
    Cc: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 3077714250ce..366b83cee955 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -144,6 +144,8 @@ struct blk_mq_ops {
 	init_request_fn		*init_request;
 	exit_request_fn		*exit_request;
 	reinit_request_fn	*reinit_request;
+	/* Called from inside blk_get_request() */
+	void (*initialize_rq_fn)(struct request *rq);
 
 	map_queues_fn		*map_queues;
 

commit cd6ce1482fd9e691bb68c660fa918c90f6b1bc25
Author: Bart Van Assche <bart.vanassche@sandisk.com>
Date:   Tue Jun 20 11:15:39 2017 -0700

    block: Make request operation type argument declarations consistent
    
    Instead of declaring the second argument of blk_*_get_request()
    as int and passing it to functions that expect an unsigned int,
    declare that second argument as unsigned int. Also because of
    consistency, rename that second argument from 'rw' into 'op'.
    This patch does not change any functionality.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Cc: Hannes Reinecke <hare@suse.com>
    Cc: Omar Sandoval <osandov@fb.com>
    Cc: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 3f2c22a42df6..3077714250ce 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -202,10 +202,10 @@ enum {
 	BLK_MQ_REQ_INTERNAL	= (1 << 2), /* allocate internal/sched tag */
 };
 
-struct request *blk_mq_alloc_request(struct request_queue *q, int rw,
+struct request *blk_mq_alloc_request(struct request_queue *q, unsigned int op,
 		unsigned int flags);
-struct request *blk_mq_alloc_request_hctx(struct request_queue *q, int op,
-		unsigned int flags, unsigned int hctx_idx);
+struct request *blk_mq_alloc_request_hctx(struct request_queue *q,
+		unsigned int op, unsigned int flags, unsigned int hctx_idx);
 struct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag);
 
 enum {

commit 073196787727e454e17a96d222ea55eba2000978
Author: Bart Van Assche <bart.vanassche@sandisk.com>
Date:   Tue Jun 20 11:15:38 2017 -0700

    blk-mq: Reduce blk_mq_hw_ctx size
    
    Since the srcu structure is rather large (184 bytes on an x86-64
    system with kernel debugging disabled), only allocate it if needed.
    
    Reported-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Ming Lei <ming.lei@redhat.com>
    Cc: Hannes Reinecke <hare@suse.com>
    Cc: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index f1bd13ae8f57..3f2c22a42df6 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -39,8 +39,6 @@ struct blk_mq_hw_ctx {
 	struct blk_mq_tags	*tags;
 	struct blk_mq_tags	*sched_tags;
 
-	struct srcu_struct	queue_rq_srcu;
-
 	unsigned long		queued;
 	unsigned long		run;
 #define BLK_MQ_MAX_DISPATCH_ORDER	7
@@ -62,6 +60,9 @@ struct blk_mq_hw_ctx {
 	struct dentry		*debugfs_dir;
 	struct dentry		*sched_debugfs_dir;
 #endif
+
+	/* Must be the last member - see also blk_mq_hw_ctx_size(). */
+	struct srcu_struct	queue_rq_srcu[0];
 };
 
 struct blk_mq_tag_set {

commit ac6424b981bce1c4bc55675c6ce11bfe1bbfa64f
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Jun 20 12:06:13 2017 +0200

    sched/wait: Rename wait_queue_t => wait_queue_entry_t
    
    Rename:
    
            wait_queue_t            =>      wait_queue_entry_t
    
    'wait_queue_t' was always a slight misnomer: its name implies that it's a "queue",
    but in reality it's a queue *entry*. The 'real' queue is the wait queue head,
    which had to carry the name.
    
    Start sorting this out by renaming it to 'wait_queue_entry_t'.
    
    This also allows the real structure name 'struct __wait_queue' to
    lose its double underscore and become 'struct wait_queue_entry',
    which is the more canonical nomenclature for such data types.
    
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index fcd641032f8d..95ba83806c5d 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -33,7 +33,7 @@ struct blk_mq_hw_ctx {
 	struct blk_mq_ctx	**ctxs;
 	unsigned int		nr_ctx;
 
-	wait_queue_t		dispatch_wait;
+	wait_queue_entry_t		dispatch_wait;
 	atomic_t		wait_index;
 
 	struct blk_mq_tags	*tags;

commit 1d9e9bc6b56e1bb7e33e7e2e1b99d7088356c006
Author: Ming Lei <ming.lei@redhat.com>
Date:   Tue Jun 6 23:22:08 2017 +0800

    blk-mq: don't stop queue for quiescing
    
    Queue can be started by other blk-mq APIs and can be used in
    different cases, this limits uses of blk_mq_quiesce_queue()
    if it is based on stopping queue, and make its usage very
    difficult, especially users have to use the stop queue APIs
    carefully for avoiding to break blk_mq_quiesce_queue().
    
    We have applied the QUIESCED flag for draining and blocking
    dispatch, so it isn't necessary to stop queue any more.
    
    After stopping queue is removed, blk_mq_quiesce_queue() can
    be used safely and easily, then users won't worry about queue
    restarting during quiescing at all.
    
    Reviewed-by: Bart Van Assche <Bart.VanAssche@sandisk.com>
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index de6536c14ae7..f1bd13ae8f57 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -271,8 +271,6 @@ static inline void blk_mq_quiesce_queue_nowait(struct request_queue *q)
 	spin_lock_irq(q->queue_lock);
 	queue_flag_set(QUEUE_FLAG_QUIESCED, q);
 	spin_unlock_irq(q->queue_lock);
-
-	blk_mq_stop_hw_queues(q);
 }
 
 /*

commit f4560ffe8cec1361b1021d81aca6a4173f8e7c87
Author: Ming Lei <ming.lei@redhat.com>
Date:   Sun Jun 18 14:24:27 2017 -0600

    blk-mq: use QUEUE_FLAG_QUIESCED to quiesce queue
    
    It is required that no dispatch can happen any more once
    blk_mq_quiesce_queue() returns, and we don't have such requirement
    on APIs of stopping queue.
    
    But blk_mq_quiesce_queue() still may not block/drain dispatch in the
    the case of BLK_MQ_S_START_ON_RUN, so use the new introduced flag of
    QUEUE_FLAG_QUIESCED and evaluate it inside RCU read-side critical
    sections for fixing this issue.
    
    Also blk_mq_quiesce_queue() is implemented via stopping queue, which
    limits its uses, and easy to cause race, because any queue restart in
    other paths may break blk_mq_quiesce_queue(). With the introduced
    flag of QUEUE_FLAG_QUIESCED, we don't need to depend on stopping queue
    for quiescing any more.
    
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Reviewed-by: Bart Van Assche <Bart.VanAssche@sandisk.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 787d8a2a2ac6..de6536c14ae7 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -268,6 +268,10 @@ void blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set, int nr_hw_queues);
  */
 static inline void blk_mq_quiesce_queue_nowait(struct request_queue *q)
 {
+	spin_lock_irq(q->queue_lock);
+	queue_flag_set(QUEUE_FLAG_QUIESCED, q);
+	spin_unlock_irq(q->queue_lock);
+
 	blk_mq_stop_hw_queues(q);
 }
 

commit e4e739131ac93d373cd2d2fd92820a6a39115ba5
Author: Ming Lei <ming.lei@redhat.com>
Date:   Tue Jun 6 23:22:03 2017 +0800

    blk-mq: introduce blk_mq_unquiesce_queue
    
    blk_mq_start_stopped_hw_queues() is used implictly
    as counterpart of blk_mq_quiesce_queue() for unquiescing queue,
    so we introduce blk_mq_unquiesce_queue() and make it
    as counterpart of blk_mq_quiesce_queue() explicitly.
    
    This function is for improving the current quiescing mechanism
    in the following patches.
    
    Reviewed-by: Bart Van Assche <Bart.VanAssche@sandisk.com>
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 78a8b64074ea..787d8a2a2ac6 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -245,6 +245,7 @@ void blk_mq_start_hw_queues(struct request_queue *q);
 void blk_mq_start_stopped_hw_queue(struct blk_mq_hw_ctx *hctx, bool async);
 void blk_mq_start_stopped_hw_queues(struct request_queue *q, bool async);
 void blk_mq_quiesce_queue(struct request_queue *q);
+void blk_mq_unquiesce_queue(struct request_queue *q);
 void blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, unsigned long msecs);
 void blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async);
 void blk_mq_run_hw_queues(struct request_queue *q, bool async);

commit 4f084b41a0c04a69067be98a210e6b50969f9945
Author: Ming Lei <ming.lei@redhat.com>
Date:   Tue Jun 6 23:22:02 2017 +0800

    blk-mq: introduce blk_mq_quiesce_queue_nowait()
    
    This patch introduces blk_mq_quiesce_queue_nowait() so
    that we can workaround mpt3sas for quiescing its queue.
    
    Once mpt3sas is fixed, we can remove this helper.
    
    Reviewed-by: Bart Van Assche <Bart.VanAssche@sandisk.com>
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 99348adb3e16..78a8b64074ea 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -262,6 +262,14 @@ int blk_mq_reinit_tagset(struct blk_mq_tag_set *set);
 int blk_mq_map_queues(struct blk_mq_tag_set *set);
 void blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set, int nr_hw_queues);
 
+/*
+ * FIXME: this helper is just for working around mpt3sas.
+ */
+static inline void blk_mq_quiesce_queue_nowait(struct request_queue *q)
+{
+	blk_mq_stop_hw_queues(q);
+}
+
 /*
  * Driver command data is immediately after the request. So subtract request
  * size to get back to the original request, add request size to get the PDU.

commit 97e0120990f4a7037f72c0e115e5c7f514025738
Author: Ming Lei <ming.lei@redhat.com>
Date:   Tue Jun 6 23:22:01 2017 +0800

    blk-mq: move blk_mq_quiesce_queue() into include/linux/blk-mq.h
    
    We usually put blk_mq_*() into include/linux/blk-mq.h, so
    move this API into there.
    
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Reviewed-by: Bart Van Assche <Bart.VanAssche@sandisk.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index b144b7b0e104..99348adb3e16 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -244,6 +244,7 @@ void blk_mq_stop_hw_queues(struct request_queue *q);
 void blk_mq_start_hw_queues(struct request_queue *q);
 void blk_mq_start_stopped_hw_queue(struct blk_mq_hw_ctx *hctx, bool async);
 void blk_mq_start_stopped_hw_queues(struct request_queue *q, bool async);
+void blk_mq_quiesce_queue(struct request_queue *q);
 void blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, unsigned long msecs);
 void blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async);
 void blk_mq_run_hw_queues(struct request_queue *q, bool async);

commit fc17b6534eb8395f0b3133eb31d87deec32c642b
Author: Christoph Hellwig <hch@lst.de>
Date:   Sat Jun 3 09:38:05 2017 +0200

    blk-mq: switch ->queue_rq return value to blk_status_t
    
    Use the same values for use for request completion errors as the return
    value from ->queue_rq.  BLK_STS_RESOURCE is special cased to cause
    a requeue, and all the others are completed as-is.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 0cf6735046d3..b144b7b0e104 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -87,7 +87,8 @@ struct blk_mq_queue_data {
 	bool last;
 };
 
-typedef int (queue_rq_fn)(struct blk_mq_hw_ctx *, const struct blk_mq_queue_data *);
+typedef blk_status_t (queue_rq_fn)(struct blk_mq_hw_ctx *,
+		const struct blk_mq_queue_data *);
 typedef enum blk_eh_timer_return (timeout_fn)(struct request *, bool);
 typedef int (init_hctx_fn)(struct blk_mq_hw_ctx *, void *, unsigned int);
 typedef void (exit_hctx_fn)(struct blk_mq_hw_ctx *, unsigned int);
@@ -155,10 +156,6 @@ struct blk_mq_ops {
 };
 
 enum {
-	BLK_MQ_RQ_QUEUE_OK	= 0,	/* queued fine */
-	BLK_MQ_RQ_QUEUE_BUSY	= 1,	/* requeue IO for later */
-	BLK_MQ_RQ_QUEUE_ERROR	= 2,	/* end IO with error */
-
 	BLK_MQ_F_SHOULD_MERGE	= 1 << 0,
 	BLK_MQ_F_TAG_SHARED	= 1 << 1,
 	BLK_MQ_F_SG_MERGE	= 1 << 2,

commit 2a842acab109f40f0d7d10b38e9ca88390628996
Author: Christoph Hellwig <hch@lst.de>
Date:   Sat Jun 3 09:38:04 2017 +0200

    block: introduce new block status code type
    
    Currently we use nornal Linux errno values in the block layer, and while
    we accept any error a few have overloaded magic meanings.  This patch
    instead introduces a new  blk_status_t value that holds block layer specific
    status codes and explicitly explains their meaning.  Helpers to convert from
    and to the previous special meanings are provided for now, but I suspect
    we want to get rid of them in the long run - those drivers that have a
    errno input (e.g. networking) usually get errnos that don't know about
    the special block layer overloads, and similarly returning them to userspace
    will usually return somethings that strictly speaking isn't correct
    for file system operations, but that's left as an exercise for later.
    
    For now the set of errors is a very limited set that closely corresponds
    to the previous overloaded errno values, but there is some low hanging
    fruite to improve it.
    
    blk_status_t (ab)uses the sparse __bitwise annotations to allow for sparse
    typechecking, so that we can easily catch places passing the wrong values.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index fcd641032f8d..0cf6735046d3 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -230,8 +230,8 @@ static inline u16 blk_mq_unique_tag_to_tag(u32 unique_tag)
 
 int blk_mq_request_started(struct request *rq);
 void blk_mq_start_request(struct request *rq);
-void blk_mq_end_request(struct request *rq, int error);
-void __blk_mq_end_request(struct request *rq, int error);
+void blk_mq_end_request(struct request *rq, blk_status_t error);
+void __blk_mq_end_request(struct request *rq, blk_status_t error);
 
 void blk_mq_requeue_request(struct request *rq, bool kick_requeue_list);
 void blk_mq_add_to_requeue_list(struct request *rq, bool at_head,

commit 7254a50a5db40ca6739ddf37e0a45e6912532b2c
Author: Ming Lei <ming.lei@redhat.com>
Date:   Mon May 22 23:05:05 2017 +0800

    blk-mq: remove blk_mq_abort_requeue_list()
    
    No one uses it any more, so remove it.
    
    Reviewed-by: Keith Busch <keith.busch@intel.com>
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index c47aa248c640..fcd641032f8d 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -238,7 +238,6 @@ void blk_mq_add_to_requeue_list(struct request *rq, bool at_head,
 				bool kick_requeue_list);
 void blk_mq_kick_requeue_list(struct request_queue *q);
 void blk_mq_delay_kick_requeue_list(struct request_queue *q, unsigned long msecs);
-void blk_mq_abort_requeue_list(struct request_queue *q);
 void blk_mq_complete_request(struct request *rq);
 
 bool blk_mq_queue_stopped(struct request_queue *q);

commit d332ce091813d11a46144354baa72b755833392f
Author: Omar Sandoval <osandov@fb.com>
Date:   Thu May 4 08:24:40 2017 -0600

    blk-mq-debugfs: allow schedulers to register debugfs attributes
    
    This provides the infrastructure for schedulers to expose their internal
    state through debugfs. We add a list of queue attributes and a list of
    hctx attributes to struct elevator_type and wire them up when switching
    schedulers.
    
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    
    Add missing seq_file.h header in blk-mq-debugfs.h
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index de8ed9aaa156..c47aa248c640 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -60,6 +60,7 @@ struct blk_mq_hw_ctx {
 
 #ifdef CONFIG_BLK_DEBUG_FS
 	struct dentry		*debugfs_dir;
+	struct dentry		*sched_debugfs_dir;
 #endif
 };
 

commit 9c1051aacde828073dbbab5e8e59c0fc802efa9a
Author: Omar Sandoval <osandov@fb.com>
Date:   Thu May 4 08:17:21 2017 -0600

    blk-mq: untangle debugfs and sysfs
    
    Originally, I tied debugfs registration/unregistration together with
    sysfs. There's no reason to do this, and it's getting in the way of
    letting schedulers define their own debugfs attributes. Instead, tie the
    debugfs registration to the lifetime of the structures themselves.
    
    The saner lifetimes mean we can also get rid of the extra mq directory
    and move everything one level up. I.e., nvme0n1/mq/hctx0/tags is now
    just nvme0n1/hctx0/tags.
    
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index a104832e7ae5..de8ed9aaa156 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -57,6 +57,10 @@ struct blk_mq_hw_ctx {
 	unsigned long		poll_considered;
 	unsigned long		poll_invoked;
 	unsigned long		poll_success;
+
+#ifdef CONFIG_BLK_DEBUG_FS
+	struct dentry		*debugfs_dir;
+#endif
 };
 
 struct blk_mq_tag_set {

commit d6296d39e90c9075bc2fc15f1e86dac44930d4b5
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon May 1 10:19:08 2017 -0600

    blk-mq: update ->init_request and ->exit_request prototypes
    
    Remove the request_idx parameter, which can't be used safely now that we
    support I/O schedulers with blk-mq.  Except for a superflous check in
    mtip32xx it was unused anyway.
    
    Also pass the tag_set instead of just the driver data - this allows drivers
    to avoid some code duplication in a follow on cleanup.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index f3e5e1de1bdb..a104832e7ae5 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -86,9 +86,9 @@ typedef int (queue_rq_fn)(struct blk_mq_hw_ctx *, const struct blk_mq_queue_data
 typedef enum blk_eh_timer_return (timeout_fn)(struct request *, bool);
 typedef int (init_hctx_fn)(struct blk_mq_hw_ctx *, void *, unsigned int);
 typedef void (exit_hctx_fn)(struct blk_mq_hw_ctx *, unsigned int);
-typedef int (init_request_fn)(void *, struct request *, unsigned int,
+typedef int (init_request_fn)(struct blk_mq_tag_set *set, struct request *,
 		unsigned int, unsigned int);
-typedef void (exit_request_fn)(void *, struct request *, unsigned int,
+typedef void (exit_request_fn)(struct blk_mq_tag_set *set, struct request *,
 		unsigned int);
 typedef int (reinit_request_fn)(void *, struct request *);
 

commit 21c6e939a9f6bb06fe616a87defec0f92a7c3df0
Author: Jens Axboe <axboe@fb.com>
Date:   Mon Apr 10 09:54:56 2017 -0600

    blk-mq: unify hctx delay_work and run_work
    
    The only difference between ->run_work and ->delay_work, is that
    the latter is used to defer running a queue. This is done by
    marking the queue stopped, and scheduling ->delay_work to run
    sometime in the future. While the queue is stopped, direct runs
    or runs through ->run_work will not run the queue.
    
    If we combine the handlers, then we need to handle two things:
    
    1) If a delayed/stopped run is scheduled, then we should not run
       the queue before that has been completed.
    2) If a queue is delayed/stopped, the handler needs to restart
       the queue. Normally a run of a queue with the stopped bit set
       would be a no-op.
    
    Case 1 is handled by modifying a currently pending queue run
    to the deadline set by the caller of blk_mq_delay_queue().
    Subsequent attempts to queue a queue run will find the work
    item already pending, and direct runs will see a stopped queue
    as before.
    
    Case 2 is handled by adding a new bit, BLK_MQ_S_START_ON_RUN,
    that tells the work handler that it should clear a stopped
    queue and run the handler.
    
    Reviewed-by: Bart Van Assche <Bart.VanAssche@sandisk.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index c7cc90328426..f3e5e1de1bdb 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -51,8 +51,6 @@ struct blk_mq_hw_ctx {
 
 	atomic_t		nr_active;
 
-	struct delayed_work	delay_work;
-
 	struct hlist_node	cpuhp_dead;
 	struct kobject		kobj;
 
@@ -168,6 +166,7 @@ enum {
 	BLK_MQ_S_TAG_ACTIVE	= 1,
 	BLK_MQ_S_SCHED_RESTART	= 2,
 	BLK_MQ_S_TAG_WAITING	= 3,
+	BLK_MQ_S_START_ON_RUN	= 4,
 
 	BLK_MQ_MAX_DEPTH	= 10240,
 

commit 9f993737906b30d7b2454a38637d1f70ffd60f2f
Author: Jens Axboe <axboe@fb.com>
Date:   Mon Apr 10 09:54:54 2017 -0600

    blk-mq: unify hctx delayed_run_work and run_work
    
    They serve the exact same purpose. Get rid of the non-delayed
    work variant, and just run it without delay for the normal case.
    
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Bart Van Assche <Bart.VanAssche@sandisk.com>
    Reviewed-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 32bd8eb5ba67..c7cc90328426 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -15,7 +15,7 @@ struct blk_mq_hw_ctx {
 		unsigned long		state;		/* BLK_MQ_S_* flags */
 	} ____cacheline_aligned_in_smp;
 
-	struct work_struct	run_work;
+	struct delayed_work	run_work;
 	cpumask_var_t		cpumask;
 	int			next_cpu;
 	int			next_cpu_batch;
@@ -51,7 +51,6 @@ struct blk_mq_hw_ctx {
 
 	atomic_t		nr_active;
 
-	struct delayed_work	delayed_run_work;
 	struct delayed_work	delay_work;
 
 	struct hlist_node	cpuhp_dead;

commit 2836ee4b1acbe7b396219d0677426885f14cd792
Author: Bart Van Assche <bart.vanassche@sandisk.com>
Date:   Wed Apr 26 13:47:56 2017 -0700

    blk-mq: Add blk_mq_ops.show_rq()
    
    This new callback function will be used in the next patch to show
    more information about SCSI requests.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Reviewed-by: Omar Sandoval <osandov@fb.com>
    Cc: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 0c4dadb85f62..32bd8eb5ba67 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -142,6 +142,14 @@ struct blk_mq_ops {
 	reinit_request_fn	*reinit_request;
 
 	map_queues_fn		*map_queues;
+
+#ifdef CONFIG_BLK_DEBUG_FS
+	/*
+	 * Used by the debugfs implementation to show driver-specific
+	 * information about a request.
+	 */
+	void (*show_rq)(struct seq_file *m, struct request *rq);
+#endif
 };
 
 enum {

commit 08e0029aa2a4acdd365613ce88a1184e5351a8a1
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Apr 20 16:03:09 2017 +0200

    blk-mq: remove the error argument to blk_mq_complete_request
    
    Now that all drivers that call blk_mq_complete_requests have a
    ->complete callback we can remove the direct call to blk_mq_end_request,
    as well as the error argument to blk_mq_complete_request.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Reviewed-by: Bart Van Assche <Bart.VanAssche@sandisk.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index d75de612845d..0c4dadb85f62 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -228,7 +228,7 @@ void blk_mq_add_to_requeue_list(struct request *rq, bool at_head,
 void blk_mq_kick_requeue_list(struct request_queue *q);
 void blk_mq_delay_kick_requeue_list(struct request_queue *q, unsigned long msecs);
 void blk_mq_abort_requeue_list(struct request_queue *q);
-void blk_mq_complete_request(struct request *rq, int error);
+void blk_mq_complete_request(struct request *rq);
 
 bool blk_mq_queue_stopped(struct request_queue *q);
 void blk_mq_stop_hw_queue(struct blk_mq_hw_ctx *hctx);

commit 5b72727299307e53888277729f980ab03264dac8
Author: Omar Sandoval <osandov@fb.com>
Date:   Fri Apr 14 01:00:00 2017 -0700

    blk-mq: export helpers
    
    blk_mq_finish_request() is required for schedulers that define their own
    put_request(). blk_mq_run_hw_queue() is required for schedulers that
    hold back requests to be run later.
    
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index b90c3d5766cd..d75de612845d 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -238,6 +238,7 @@ void blk_mq_start_hw_queues(struct request_queue *q);
 void blk_mq_start_stopped_hw_queue(struct blk_mq_hw_ctx *hctx, bool async);
 void blk_mq_start_stopped_hw_queues(struct request_queue *q, bool async);
 void blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, unsigned long msecs);
+void blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async);
 void blk_mq_run_hw_queues(struct request_queue *q, bool async);
 void blk_mq_delay_queue(struct blk_mq_hw_ctx *hctx, unsigned long msecs);
 void blk_mq_tagset_busy_iter(struct blk_mq_tag_set *tagset,

commit 65f619d2535197d97067eeeef75a40f25b552e69
Merge: fbbaf700e7b1 6d8c6c0f97ad
Author: Jens Axboe <axboe@fb.com>
Date:   Fri Apr 7 12:45:20 2017 -0600

    Merge branch 'for-linus' into for-4.12/block
    
    We've added a considerable amount of fixes for stalls and issues
    with the blk-mq scheduling in the 4.11 series since forking
    off the for-4.12/block branch. We need to do improvements on
    top of that for 4.12, so pull in the previous fixes to make
    our lives easier going forward.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

commit 7587a5ae7eef0439f7be31f1b5959af062bbc5ec
Author: Bart Van Assche <bart.vanassche@sandisk.com>
Date:   Fri Apr 7 11:16:52 2017 -0700

    blk-mq: Introduce blk_mq_delay_run_hw_queue()
    
    Introduce a function that runs a hardware queue unconditionally
    after a delay. Note: there is already a function that stops and
    restarts a hardware queue after a delay, namely blk_mq_delay_queue().
    
    This function will be used in the next patch in this series.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Hannes Reinecke <hare@suse.de>
    Cc: Long Li <longli@microsoft.com>
    Cc: K. Y. Srinivasan <kys@microsoft.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index b296a9006117..9382c5da7a2e 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -51,6 +51,7 @@ struct blk_mq_hw_ctx {
 
 	atomic_t		nr_active;
 
+	struct delayed_work	delayed_run_work;
 	struct delayed_work	delay_work;
 
 	struct hlist_node	cpuhp_dead;
@@ -238,6 +239,7 @@ void blk_mq_stop_hw_queues(struct request_queue *q);
 void blk_mq_start_hw_queues(struct request_queue *q);
 void blk_mq_start_stopped_hw_queue(struct blk_mq_hw_ctx *hctx, bool async);
 void blk_mq_start_stopped_hw_queues(struct request_queue *q, bool async);
+void blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, unsigned long msecs);
 void blk_mq_run_hw_queues(struct request_queue *q, bool async);
 void blk_mq_delay_queue(struct blk_mq_hw_ctx *hctx, unsigned long msecs);
 void blk_mq_tagset_busy_iter(struct blk_mq_tag_set *tagset,

commit f2fbc9dd78970accd7649e8b87c7f00a0da0cdbc
Author: Bart Van Assche <bart.vanassche@sandisk.com>
Date:   Wed Apr 5 08:39:18 2017 -0700

    blk-mq: Remove blk_mq_queue_data.list
    
    The block layer core sets blk_mq_queue_data.list but no block
    drivers read that member. Hence remove it and also the code that
    is used to set this member.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index ea2e9dcd3aef..bdea90d75274 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -81,7 +81,6 @@ struct blk_mq_tag_set {
 
 struct blk_mq_queue_data {
 	struct request *rq;
-	struct list_head *list;
 	bool last;
 };
 

commit 1671d522cdd9933dee7dddfcf9f62c561283824a
Author: Ming Lei <tom.leiming@gmail.com>
Date:   Mon Mar 27 20:06:57 2017 +0800

    block: rename blk_mq_freeze_queue_start()
    
    As the .q_usage_counter is used by both legacy and
    mq path, we need to block new I/O if queue becomes
    dead in blk_queue_enter().
    
    So rename it and we can use this function in both
    paths.
    
    Reviewed-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Ming Lei <tom.leiming@gmail.com>
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 5b3e201c8d4f..ea2e9dcd3aef 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -243,7 +243,7 @@ void blk_mq_tagset_busy_iter(struct blk_mq_tag_set *tagset,
 		busy_tag_iter_fn *fn, void *priv);
 void blk_mq_freeze_queue(struct request_queue *q);
 void blk_mq_unfreeze_queue(struct request_queue *q);
-void blk_mq_freeze_queue_start(struct request_queue *q);
+void blk_freeze_queue_start(struct request_queue *q);
 void blk_mq_freeze_queue_wait(struct request_queue *q);
 int blk_mq_freeze_queue_wait_timeout(struct request_queue *q,
 				     unsigned long timeout);

commit 7642747d674aff1f7cfe74ad9af7e9b12ab1d5ee
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Mar 22 15:01:49 2017 -0400

    blk-mq: remove BLK_MQ_F_DEFER_ISSUE
    
    This flag was never used since it was introduced.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index b296a9006117..5b3e201c8d4f 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -152,7 +152,6 @@ enum {
 	BLK_MQ_F_SHOULD_MERGE	= 1 << 0,
 	BLK_MQ_F_TAG_SHARED	= 1 << 1,
 	BLK_MQ_F_SG_MERGE	= 1 << 2,
-	BLK_MQ_F_DEFER_ISSUE	= 1 << 4,
 	BLK_MQ_F_BLOCKING	= 1 << 5,
 	BLK_MQ_F_NO_SCHED	= 1 << 6,
 	BLK_MQ_F_ALLOC_POLICY_START_BIT = 8,

commit f91328c40a559362b6e7b7bfee01ca17fda87592
Author: Keith Busch <keith.busch@intel.com>
Date:   Wed Mar 1 14:22:11 2017 -0500

    blk-mq: Provide freeze queue timeout
    
    A driver may wish to take corrective action if queued requests do not
    complete within a set time.
    
    Signed-off-by: Keith Busch <keith.busch@intel.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 8dacf680c851..b296a9006117 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -246,6 +246,8 @@ void blk_mq_freeze_queue(struct request_queue *q);
 void blk_mq_unfreeze_queue(struct request_queue *q);
 void blk_mq_freeze_queue_start(struct request_queue *q);
 void blk_mq_freeze_queue_wait(struct request_queue *q);
+int blk_mq_freeze_queue_wait_timeout(struct request_queue *q,
+				     unsigned long timeout);
 int blk_mq_reinit_tagset(struct blk_mq_tag_set *set);
 
 int blk_mq_map_queues(struct blk_mq_tag_set *set);

commit 6bae363ee3057a14eec93440826813603559273a
Author: Keith Busch <keith.busch@intel.com>
Date:   Wed Mar 1 14:22:10 2017 -0500

    blk-mq: Export blk_mq_freeze_queue_wait
    
    Drivers can start a freeze, so this provides a way to wait for frozen.
    
    Signed-off-by: Keith Busch <keith.busch@intel.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 001d30d727c5..8dacf680c851 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -245,6 +245,7 @@ void blk_mq_tagset_busy_iter(struct blk_mq_tag_set *tagset,
 void blk_mq_freeze_queue(struct request_queue *q);
 void blk_mq_unfreeze_queue(struct request_queue *q);
 void blk_mq_freeze_queue_start(struct request_queue *q);
+void blk_mq_freeze_queue_wait(struct request_queue *q);
 int blk_mq_reinit_tagset(struct blk_mq_tag_set *set);
 
 int blk_mq_map_queues(struct blk_mq_tag_set *set);

commit da55f2cc78418dee88400aafbbaed19d7ac8188e
Author: Omar Sandoval <osandov@fb.com>
Date:   Wed Feb 22 10:58:29 2017 -0800

    blk-mq: use sbq wait queues instead of restart for driver tags
    
    Commit 50e1dab86aa2 ("blk-mq-sched: fix starvation for multiple hardware
    queues and shared tags") fixed one starvation issue for shared tags.
    However, we can still get into a situation where we fail to allocate a
    tag because all tags are allocated but we don't have any pending
    requests on any hardware queue.
    
    One solution for this would be to restart all queues that share a tag
    map, but that really sucks. Ideally, we could just block and wait for a
    tag, but that isn't always possible from blk_mq_dispatch_rq_list().
    
    However, we can still use the struct sbitmap_queue wait queues with a
    custom callback instead of blocking. This has a few benefits:
    
    1. It avoids iterating over all hardware queues when completing an I/O,
       which the current restart code has to do.
    2. It benefits from the existing rolling wakeup code.
    3. It avoids punting to another thread just to have it block.
    
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 8e4df3d6c8cd..001d30d727c5 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -33,6 +33,7 @@ struct blk_mq_hw_ctx {
 	struct blk_mq_ctx	**ctxs;
 	unsigned int		nr_ctx;
 
+	wait_queue_t		dispatch_wait;
 	atomic_t		wait_index;
 
 	struct blk_mq_tags	*tags;
@@ -160,6 +161,7 @@ enum {
 	BLK_MQ_S_STOPPED	= 0,
 	BLK_MQ_S_TAG_ACTIVE	= 1,
 	BLK_MQ_S_SCHED_RESTART	= 2,
+	BLK_MQ_S_TAG_WAITING	= 3,
 
 	BLK_MQ_MAX_DEPTH	= 10240,
 

commit d34849913819a5e0cbfbe724dbe79df89278c524
Author: Jens Axboe <axboe@fb.com>
Date:   Fri Jan 13 14:43:58 2017 -0700

    blk-mq-sched: allow setting of default IO scheduler
    
    Add Kconfig entries to manage what devices get assigned an MQ
    scheduler, and add a blk-mq flag for drivers to opt out of scheduling.
    The latter is useful for admin type queues that still allocate a blk-mq
    queue and tag set, but aren't use for normal IO.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>
    Reviewed-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Reviewed-by: Omar Sandoval <osandov@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 63569eb46d15..8e4df3d6c8cd 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -153,6 +153,7 @@ enum {
 	BLK_MQ_F_SG_MERGE	= 1 << 2,
 	BLK_MQ_F_DEFER_ISSUE	= 1 << 4,
 	BLK_MQ_F_BLOCKING	= 1 << 5,
+	BLK_MQ_F_NO_SCHED	= 1 << 6,
 	BLK_MQ_F_ALLOC_POLICY_START_BIT = 8,
 	BLK_MQ_F_ALLOC_POLICY_BITS = 1,
 

commit bd166ef183c263c5ced656d49ef19c7da4adc774
Author: Jens Axboe <axboe@fb.com>
Date:   Tue Jan 17 06:03:22 2017 -0700

    blk-mq-sched: add framework for MQ capable IO schedulers
    
    This adds a set of hooks that intercepts the blk-mq path of
    allocating/inserting/issuing/completing requests, allowing
    us to develop a scheduler within that framework.
    
    We reuse the existing elevator scheduler API on the registration
    side, but augment that with the scheduler flagging support for
    the blk-mq interfce, and with a separate set of ops hooks for MQ
    devices.
    
    We split driver and scheduler tags, so we can run the scheduling
    independently of device queue depth.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>
    Reviewed-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Reviewed-by: Omar Sandoval <osandov@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 2686f9e7302a..63569eb46d15 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -22,6 +22,7 @@ struct blk_mq_hw_ctx {
 
 	unsigned long		flags;		/* BLK_MQ_F_* flags */
 
+	void			*sched_data;
 	struct request_queue	*queue;
 	struct blk_flush_queue	*fq;
 
@@ -35,6 +36,7 @@ struct blk_mq_hw_ctx {
 	atomic_t		wait_index;
 
 	struct blk_mq_tags	*tags;
+	struct blk_mq_tags	*sched_tags;
 
 	struct srcu_struct	queue_rq_srcu;
 
@@ -156,6 +158,7 @@ enum {
 
 	BLK_MQ_S_STOPPED	= 0,
 	BLK_MQ_S_TAG_ACTIVE	= 1,
+	BLK_MQ_S_SCHED_RESTART	= 2,
 
 	BLK_MQ_MAX_DEPTH	= 10240,
 
@@ -179,13 +182,13 @@ void blk_mq_free_tag_set(struct blk_mq_tag_set *set);
 
 void blk_mq_flush_plug_list(struct blk_plug *plug, bool from_schedule);
 
-void blk_mq_insert_request(struct request *, bool, bool, bool);
 void blk_mq_free_request(struct request *rq);
 bool blk_mq_can_queue(struct blk_mq_hw_ctx *);
 
 enum {
 	BLK_MQ_REQ_NOWAIT	= (1 << 0), /* return when out of requests */
 	BLK_MQ_REQ_RESERVED	= (1 << 1), /* allocate from reserved pool */
+	BLK_MQ_REQ_INTERNAL	= (1 << 2), /* allocate internal/sched tag */
 };
 
 struct request *blk_mq_alloc_request(struct request_queue *q, int rw,

commit 16a3c2a70cad5ccdc2dc0a4544bff82554807493
Author: Jens Axboe <axboe@fb.com>
Date:   Thu Dec 15 14:27:46 2016 -0700

    blk-mq: un-export blk_mq_free_hctx_request()
    
    It's only used in blk-mq, kill it from the main exported header
    and kill the symbol export as well.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Reviewed-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Omar Sandoval <osandov@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index afc81d77e471..2686f9e7302a 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -181,7 +181,6 @@ void blk_mq_flush_plug_list(struct blk_plug *plug, bool from_schedule);
 
 void blk_mq_insert_request(struct request *, bool, bool, bool);
 void blk_mq_free_request(struct request *rq);
-void blk_mq_free_hctx_request(struct blk_mq_hw_ctx *, struct request *rq);
 bool blk_mq_can_queue(struct blk_mq_hw_ctx *);
 
 enum {

commit f8a5b12247fe18f7fed801ad262a7ab190e1f848
Author: Jens Axboe <axboe@fb.com>
Date:   Tue Dec 13 09:24:51 2016 -0700

    blk-mq: make mq_ops a const pointer
    
    We never change it, make that clear.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>
    Reviewed-by: Bart Van Assche <bart.vanassche@sandisk.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 4a2ab5d99ff7..afc81d77e471 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -60,7 +60,7 @@ struct blk_mq_hw_ctx {
 
 struct blk_mq_tag_set {
 	unsigned int		*mq_map;
-	struct blk_mq_ops	*ops;
+	const struct blk_mq_ops	*ops;
 	unsigned int		nr_hw_queues;
 	unsigned int		queue_depth;	/* max hw supported */
 	unsigned int		reserved_tags;

commit a829a8445f09036404060f4d6489cb13433f4304
Merge: 84b607913442 f5b893c94715
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Dec 14 10:49:33 2016 -0800

    Merge tag 'scsi-misc' of git://git.kernel.org/pub/scm/linux/kernel/git/jejb/scsi
    
    Pull SCSI updates from James Bottomley:
     "This update includes the usual round of major driver updates (ncr5380,
      lpfc, hisi_sas, megaraid_sas, ufs, ibmvscsis, mpt3sas).
    
      There's also an assortment of minor fixes, mostly in error legs or
      other not very user visible stuff. The major change is the
      pci_alloc_irq_vectors replacement for the old pci_msix_.. calls; this
      effectively makes IRQ mapping generic for the drivers and allows
      blk_mq to use the information"
    
    * tag 'scsi-misc' of git://git.kernel.org/pub/scm/linux/kernel/git/jejb/scsi: (256 commits)
      scsi: qla4xxx: switch to pci_alloc_irq_vectors
      scsi: hisi_sas: support deferred probe for v2 hw
      scsi: megaraid_sas: switch to pci_alloc_irq_vectors
      scsi: scsi_devinfo: remove synchronous ALUA for NETAPP devices
      scsi: be2iscsi: set errno on error path
      scsi: be2iscsi: set errno on error path
      scsi: hpsa: fallback to use legacy REPORT PHYS command
      scsi: scsi_dh_alua: Fix RCU annotations
      scsi: hpsa: use %phN for short hex dumps
      scsi: hisi_sas: fix free'ing in probe and remove
      scsi: isci: switch to pci_alloc_irq_vectors
      scsi: ipr: Fix runaway IRQs when falling back from MSI to LSI
      scsi: dpt_i2o: double free on error path
      scsi: cxlflash: Migrate scsi command pointer to AFU command
      scsi: cxlflash: Migrate IOARRIN specific routines to function pointers
      scsi: cxlflash: Cleanup queuecommand()
      scsi: cxlflash: Cleanup send_tmf()
      scsi: cxlflash: Remove AFU command lock
      scsi: cxlflash: Wait for active AFU commands to timeout upon tear down
      scsi: cxlflash: Remove private command pool
      ...

commit ae911c5e796d51cb2d1ed3a55e73b9cc88d176cf
Author: Jens Axboe <axboe@fb.com>
Date:   Thu Dec 8 13:19:30 2016 -0700

    blk-mq: add blk_mq_start_stopped_hw_queue()
    
    We have a variant for all hardware queues, but not one for a single
    hardware queue.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>
    Reviewed-by: Hannes Reinecke <hare@suse.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 35a0af5ede6d..87e404aae267 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -231,6 +231,7 @@ void blk_mq_stop_hw_queue(struct blk_mq_hw_ctx *hctx);
 void blk_mq_start_hw_queue(struct blk_mq_hw_ctx *hctx);
 void blk_mq_stop_hw_queues(struct request_queue *q);
 void blk_mq_start_hw_queues(struct request_queue *q);
+void blk_mq_start_stopped_hw_queue(struct blk_mq_hw_ctx *hctx, bool async);
 void blk_mq_start_stopped_hw_queues(struct request_queue *q, bool async);
 void blk_mq_run_hw_queues(struct request_queue *q, bool async);
 void blk_mq_delay_queue(struct blk_mq_hw_ctx *hctx, unsigned long msecs);

commit 9e5a7e22951bc12ee45cb617919d57b5efce56b5
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Nov 1 08:12:47 2016 -0600

    blk-mq: export blk_mq_map_queues
    
    This will allow SCSI to have a single blk_mq_ops structure that either
    lets the LLDD map the queues to PCIe MSIx vectors or use the default.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Reviewed-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 535ab2e13d2e..6c0fb259581f 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -237,6 +237,7 @@ void blk_mq_unfreeze_queue(struct request_queue *q);
 void blk_mq_freeze_queue_start(struct request_queue *q);
 int blk_mq_reinit_tagset(struct blk_mq_tag_set *set);
 
+int blk_mq_map_queues(struct blk_mq_tag_set *set);
 void blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set, int nr_hw_queues);
 
 /*

commit 2b053aca76b48e681be57b34ca3a8c2c10b275c5
Author: Bart Van Assche <bart.vanassche@sandisk.com>
Date:   Fri Oct 28 17:21:41 2016 -0700

    blk-mq: Add a kick_requeue_list argument to blk_mq_requeue_request()
    
    Most blk_mq_requeue_request() and blk_mq_add_to_requeue_list() calls
    are followed by kicking the requeue list. Hence add an argument to
    these two functions that allows to kick the requeue list. This was
    proposed by Christoph Hellwig.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Cc: Hannes Reinecke <hare@suse.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index ed20ac74c62a..35a0af5ede6d 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -218,8 +218,9 @@ void blk_mq_start_request(struct request *rq);
 void blk_mq_end_request(struct request *rq, int error);
 void __blk_mq_end_request(struct request *rq, int error);
 
-void blk_mq_requeue_request(struct request *rq);
-void blk_mq_add_to_requeue_list(struct request *rq, bool at_head);
+void blk_mq_requeue_request(struct request *rq, bool kick_requeue_list);
+void blk_mq_add_to_requeue_list(struct request *rq, bool at_head,
+				bool kick_requeue_list);
 void blk_mq_kick_requeue_list(struct request_queue *q);
 void blk_mq_delay_kick_requeue_list(struct request_queue *q, unsigned long msecs);
 void blk_mq_abort_requeue_list(struct request_queue *q);

commit 6a83e74d214a47a1371cd2e6a783264fcba7d428
Author: Bart Van Assche <bart.vanassche@sandisk.com>
Date:   Wed Nov 2 10:09:51 2016 -0600

    blk-mq: Introduce blk_mq_quiesce_queue()
    
    blk_mq_quiesce_queue() waits until ongoing .queue_rq() invocations
    have finished. This function does *not* wait until all outstanding
    requests have finished (this means invocation of request.end_io()).
    The algorithm used by blk_mq_quiesce_queue() is as follows:
    * Hold either an RCU read lock or an SRCU read lock around
      .queue_rq() calls. The former is used if .queue_rq() does not
      block and the latter if .queue_rq() may block.
    * blk_mq_quiesce_queue() first calls blk_mq_stop_hw_queues()
      followed by synchronize_srcu() or synchronize_rcu(). The latter
      call waits for .queue_rq() invocations that started before
      blk_mq_quiesce_queue() was called.
    * The blk_mq_hctx_stopped() calls that control whether or not
      .queue_rq() will be called are called with the (S)RCU read lock
      held. This is necessary to avoid race conditions against
      blk_mq_quiesce_queue().
    
    Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Cc: Hannes Reinecke <hare@suse.com>
    Cc: Johannes Thumshirn <jthumshirn@suse.de>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Reviewed-by: Ming Lei <tom.leiming@gmail.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index a85a20f80aaa..ed20ac74c62a 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -3,6 +3,7 @@
 
 #include <linux/blkdev.h>
 #include <linux/sbitmap.h>
+#include <linux/srcu.h>
 
 struct blk_mq_tags;
 struct blk_flush_queue;
@@ -35,6 +36,8 @@ struct blk_mq_hw_ctx {
 
 	struct blk_mq_tags	*tags;
 
+	struct srcu_struct	queue_rq_srcu;
+
 	unsigned long		queued;
 	unsigned long		run;
 #define BLK_MQ_MAX_DISPATCH_ORDER	7

commit 9b7dd572cc439fa92e120290eb74d0295567c5a0
Author: Bart Van Assche <bart.vanassche@sandisk.com>
Date:   Fri Oct 28 17:20:49 2016 -0700

    blk-mq: Remove blk_mq_cancel_requeue_work()
    
    Since blk_mq_requeue_work() no longer restarts stopped queues
    canceling requeue work is no longer needed to prevent that a
    stopped queue would be restarted. Hence remove this function.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Cc: Mike Snitzer <snitzer@redhat.com>
    Cc: Keith Busch <keith.busch@intel.com>
    Cc: Hannes Reinecke <hare@suse.com>
    Cc: Johannes Thumshirn <jthumshirn@suse.de>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index aa930009fcd3..a85a20f80aaa 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -217,7 +217,6 @@ void __blk_mq_end_request(struct request *rq, int error);
 
 void blk_mq_requeue_request(struct request *rq);
 void blk_mq_add_to_requeue_list(struct request *rq, bool at_head);
-void blk_mq_cancel_requeue_work(struct request_queue *q);
 void blk_mq_kick_requeue_list(struct request_queue *q);
 void blk_mq_delay_kick_requeue_list(struct request_queue *q, unsigned long msecs);
 void blk_mq_abort_requeue_list(struct request_queue *q);

commit fd00144301d64f1742541a3c5e64cd1c51f39c55
Author: Bart Van Assche <bart.vanassche@sandisk.com>
Date:   Fri Oct 28 17:19:37 2016 -0700

    blk-mq: Introduce blk_mq_queue_stopped()
    
    The function blk_queue_stopped() allows to test whether or not a
    traditional request queue has been stopped. Introduce a helper
    function that allows block drivers to query easily whether or not
    one or more hardware contexts of a blk-mq queue have been stopped.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 535ab2e13d2e..aa930009fcd3 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -223,6 +223,7 @@ void blk_mq_delay_kick_requeue_list(struct request_queue *q, unsigned long msecs
 void blk_mq_abort_requeue_list(struct request_queue *q);
 void blk_mq_complete_request(struct request *rq, int error);
 
+bool blk_mq_queue_stopped(struct request_queue *q);
 void blk_mq_stop_hw_queue(struct blk_mq_hw_ctx *hctx);
 void blk_mq_start_hw_queue(struct blk_mq_hw_ctx *hctx);
 void blk_mq_stop_hw_queues(struct request_queue *q);

commit 24532f768121b07b16178ffb40442ece43365cbd
Merge: 12e3d3cdd975 97a32864e6de
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Oct 9 17:32:20 2016 -0700

    Merge branch 'for-4.9/block-smp' of git://git.kernel.dk/linux-block
    
    Pull blk-mq CPU hotplug update from Jens Axboe:
     "This is the conversion of blk-mq to the new hotplug state machine"
    
    * 'for-4.9/block-smp' of git://git.kernel.dk/linux-block:
      blk-mq: fixup "Convert to new hotplug state machine"
      blk-mq: Convert to new hotplug state machine
      blk-mq/cpu-notif: Convert to new hotplug state machine

commit 12e3d3cdd975fe986cc5c35f60b1467a8ec20b80
Merge: 48915c2cbc77 8ec2ef2b66ea
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Oct 9 17:29:33 2016 -0700

    Merge branch 'for-4.9/block-irq' of git://git.kernel.dk/linux-block
    
    Pull blk-mq irq/cpu mapping updates from Jens Axboe:
     "This is the block-irq topic branch for 4.9-rc. It's mostly from
      Christoph, and it allows drivers to specify their own mappings, and
      more importantly, to share the blk-mq mappings with the IRQ affinity
      mappings. It's a good step towards making this work better out of the
      box"
    
    * 'for-4.9/block-irq' of git://git.kernel.dk/linux-block:
      blk_mq: linux/blk-mq.h does not include all the headers it depends on
      blk-mq: kill unused blk_mq_create_mq_map()
      blk-mq: get rid of the cpumask in struct blk_mq_tags
      nvme: remove the post_scan callout
      nvme: switch to use pci_alloc_irq_vectors
      blk-mq: provide a default queue mapping for PCI device
      blk-mq: allow the driver to pass in a queue mapping
      blk-mq: remove ->map_queue
      blk-mq: only allocate a single mq_map per tag_set
      blk-mq: don't redistribute hardware queues on a CPU hotplug event

commit 1b792f2f92784c00db2e6431496e437855d6f12a
Author: Jens Axboe <axboe@fb.com>
Date:   Wed Sep 21 10:12:13 2016 -0600

    blk-mq: add flag for drivers wanting blocking ->queue_rq()
    
    If a driver sets BLK_MQ_F_BLOCKING, it is allowed to block in its
    ->queue_rq() handler. For that case, blk-mq ensures that we always
    calls it from a safe context.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>
    Tested-by: Josef Bacik <jbacik@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index fbcfdf323243..5daa0ef756dd 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -155,6 +155,7 @@ enum {
 	BLK_MQ_F_TAG_SHARED	= 1 << 1,
 	BLK_MQ_F_SG_MERGE	= 1 << 2,
 	BLK_MQ_F_DEFER_ISSUE	= 1 << 4,
+	BLK_MQ_F_BLOCKING	= 1 << 5,
 	BLK_MQ_F_ALLOC_POLICY_START_BIT = 8,
 	BLK_MQ_F_ALLOC_POLICY_BITS = 1,
 

commit 9467f85960a31d56f95371516e55e210e1e3d51c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Sep 22 08:05:17 2016 -0600

    blk-mq/cpu-notif: Convert to new hotplug state machine
    
    Replace the block-mq notifier list management with the multi instance
    facility in the cpu hotplug state machine.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: linux-block@vger.kernel.org
    Cc: rt@linutronix.de
    Cc: Christoph Hellwing <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index fbcfdf323243..b3d1a7f4b5f2 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -7,12 +7,6 @@
 struct blk_mq_tags;
 struct blk_flush_queue;
 
-struct blk_mq_cpu_notifier {
-	struct list_head list;
-	void *data;
-	int (*notify)(void *data, unsigned long action, unsigned int cpu);
-};
-
 struct blk_mq_hw_ctx {
 	struct {
 		spinlock_t		lock;
@@ -53,7 +47,7 @@ struct blk_mq_hw_ctx {
 
 	struct delayed_work	delay_work;
 
-	struct blk_mq_cpu_notifier	cpu_notifier;
+	struct hlist_node	cpuhp_dead;
 	struct kobject		kobj;
 
 	unsigned long		poll_considered;

commit b21d5b301794ae332eaa6e177d71fe8b77d3664c
Author: Matias Bj√∏rling <m@bjorling.me>
Date:   Fri Sep 16 14:25:06 2016 +0200

    blk-mq: register device instead of disk
    
    Enable devices without a gendisk instance to register itself with blk-mq
    and expose the associated multi-queue sysfs entries.
    
    Signed-off-by: Matias Bj√∏rling <m@bjorling.me>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 2575779cf13f..fbcfdf323243 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -175,8 +175,8 @@ enum {
 struct request_queue *blk_mq_init_queue(struct blk_mq_tag_set *);
 struct request_queue *blk_mq_init_allocated_queue(struct blk_mq_tag_set *set,
 						  struct request_queue *q);
-int blk_mq_register_disk(struct gendisk *);
-void blk_mq_unregister_disk(struct gendisk *);
+int blk_mq_register_dev(struct device *, struct request_queue *);
+void blk_mq_unregister_dev(struct device *, struct request_queue *);
 
 int blk_mq_alloc_tag_set(struct blk_mq_tag_set *set);
 void blk_mq_free_tag_set(struct blk_mq_tag_set *set);

commit 88459642cba452630326b9cab1c651e09577d4e4
Author: Omar Sandoval <osandov@fb.com>
Date:   Sat Sep 17 08:38:44 2016 -0600

    blk-mq: abstract tag allocation out into sbitmap library
    
    This is a generally useful data structure, so make it available to
    anyone else who might want to use it. It's also a nice cleanup
    separating the allocation logic from the rest of the tag handling logic.
    
    The code is behind a new Kconfig option, CONFIG_SBITMAP, which is only
    selected by CONFIG_BLOCK for now.
    
    This should be a complete noop functionality-wise.
    
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 60ef14cbcd2d..2575779cf13f 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -2,6 +2,7 @@
 #define BLK_MQ_H
 
 #include <linux/blkdev.h>
+#include <linux/sbitmap.h>
 
 struct blk_mq_tags;
 struct blk_flush_queue;
@@ -12,12 +13,6 @@ struct blk_mq_cpu_notifier {
 	int (*notify)(void *data, unsigned long action, unsigned int cpu);
 };
 
-struct blk_mq_ctxmap {
-	unsigned int size;
-	unsigned int bits_per_word;
-	struct blk_align_bitmap *map;
-};
-
 struct blk_mq_hw_ctx {
 	struct {
 		spinlock_t		lock;
@@ -37,7 +32,7 @@ struct blk_mq_hw_ctx {
 
 	void			*driver_data;
 
-	struct blk_mq_ctxmap	ctx_map;
+	struct sbitmap		ctx_map;
 
 	struct blk_mq_ctx	**ctxs;
 	unsigned int		nr_ctx;

commit 1b157939f92ae22d10b9d52baaa14f826927f5ff
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Sep 14 16:18:59 2016 +0200

    blk-mq: get rid of the cpumask in struct blk_mq_tags
    
    Unused now that NVMe sets up irq affinity before calling into blk-mq.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Keith Busch <keith.busch@intel.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 6737fd7946f4..c5a97d7cef93 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -201,7 +201,6 @@ struct request *blk_mq_alloc_request(struct request_queue *q, int rw,
 struct request *blk_mq_alloc_request_hctx(struct request_queue *q, int op,
 		unsigned int flags, unsigned int hctx_idx);
 struct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag);
-struct cpumask *blk_mq_tags_cpumask(struct blk_mq_tags *tags);
 
 enum {
 	BLK_MQ_UNIQUE_TAG_BITS = 16,

commit da695ba236b993f07a540d35c17f271ef08c89f3
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Sep 14 16:18:55 2016 +0200

    blk-mq: allow the driver to pass in a queue mapping
    
    This allows drivers specify their own queue mapping by overriding the
    setup-time function that builds the mq_map.  This can be used for
    example to build the map based on the MSI-X vector mapping provided
    by the core interrupt layer for PCI devices.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Keith Busch <keith.busch@intel.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index f01379f2b0ac..6737fd7946f4 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -104,6 +104,7 @@ typedef void (busy_iter_fn)(struct blk_mq_hw_ctx *, struct request *, void *,
 		bool);
 typedef void (busy_tag_iter_fn)(struct request *, void *, bool);
 typedef int (poll_fn)(struct blk_mq_hw_ctx *, unsigned int);
+typedef int (map_queues_fn)(struct blk_mq_tag_set *set);
 
 
 struct blk_mq_ops {
@@ -144,6 +145,8 @@ struct blk_mq_ops {
 	init_request_fn		*init_request;
 	exit_request_fn		*exit_request;
 	reinit_request_fn	*reinit_request;
+
+	map_queues_fn		*map_queues;
 };
 
 enum {

commit 7d7e0f90b70f6c5367c2d1c9a7e87dd228bd0816
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Sep 14 16:18:54 2016 +0200

    blk-mq: remove ->map_queue
    
    All drivers use the default, so provide an inline version of it.  If we
    ever need other queue mapping we can add an optional method back,
    although supporting will also require major changes to the queue setup
    code.
    
    This provides better code generation, and better debugability as well.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Keith Busch <keith.busch@intel.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index deda16a9bde4..f01379f2b0ac 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -91,7 +91,6 @@ struct blk_mq_queue_data {
 };
 
 typedef int (queue_rq_fn)(struct blk_mq_hw_ctx *, const struct blk_mq_queue_data *);
-typedef struct blk_mq_hw_ctx *(map_queue_fn)(struct request_queue *, const int);
 typedef enum blk_eh_timer_return (timeout_fn)(struct request *, bool);
 typedef int (init_hctx_fn)(struct blk_mq_hw_ctx *, void *, unsigned int);
 typedef void (exit_hctx_fn)(struct blk_mq_hw_ctx *, unsigned int);
@@ -113,11 +112,6 @@ struct blk_mq_ops {
 	 */
 	queue_rq_fn		*queue_rq;
 
-	/*
-	 * Map to specific hardware queue
-	 */
-	map_queue_fn		*map_queue;
-
 	/*
 	 * Called on request timeout
 	 */
@@ -223,7 +217,6 @@ static inline u16 blk_mq_unique_tag_to_tag(u32 unique_tag)
 	return unique_tag & BLK_MQ_UNIQUE_TAG_MASK;
 }
 
-struct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *, const int ctx_index);
 
 int blk_mq_request_started(struct request *rq);
 void blk_mq_start_request(struct request *rq);

commit bdd17e75cd97c5c39feee409890a91d0396640fe
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Sep 14 16:18:53 2016 +0200

    blk-mq: only allocate a single mq_map per tag_set
    
    The mapping is identical for all queues in a tag_set, so stop wasting
    memory for building multiple.  Note that for now I've kept the mq_map
    pointer in the request_queue, but we'll need to investigate if we can
    remove it without suffering too much from the additional pointer chasing.
    The same would apply to the mq_ops pointer as well.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Keith Busch <keith.busch@intel.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 60ef14cbcd2d..deda16a9bde4 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -67,6 +67,7 @@ struct blk_mq_hw_ctx {
 };
 
 struct blk_mq_tag_set {
+	unsigned int		*mq_map;
 	struct blk_mq_ops	*ops;
 	unsigned int		nr_hw_queues;
 	unsigned int		queue_depth;	/* max hw supported */

commit 2849450ad39d2e699fda2d5c6f41e05d87fd7004
Author: Mike Snitzer <snitzer@redhat.com>
Date:   Wed Sep 14 13:28:30 2016 -0400

    blk-mq: introduce blk_mq_delay_kick_requeue_list()
    
    blk_mq_delay_kick_requeue_list() provides the ability to kick the
    q->requeue_list after a specified time.  To do this the request_queue's
    'requeue_work' member was changed to a delayed_work.
    
    blk_mq_delay_kick_requeue_list() allows DM to defer processing requeued
    requests while it doesn't make sense to immediately requeue them
    (e.g. when all paths in a DM multipath have failed).
    
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index ff14f68067aa..60ef14cbcd2d 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -233,6 +233,7 @@ void blk_mq_requeue_request(struct request *rq);
 void blk_mq_add_to_requeue_list(struct request *rq, bool at_head);
 void blk_mq_cancel_requeue_work(struct request_queue *q);
 void blk_mq_kick_requeue_list(struct request_queue *q);
+void blk_mq_delay_kick_requeue_list(struct request_queue *q, unsigned long msecs);
 void blk_mq_abort_requeue_list(struct request_queue *q);
 void blk_mq_complete_request(struct request *rq, int error);
 

commit abe47114b192a9e0167905a3418d815b4fcf87de
Author: Linus Walleij <linus.walleij@linaro.org>
Date:   Wed Sep 14 14:33:15 2016 +0200

    block: remove blk_mq_alloc_single_hw_queue() prototype
    
    The blk_mq_alloc_single_hw_queue() is a prototype artifact that
    should have been removed with
    commit cdef54dd85ad66e77262ea57796a3e81683dd5d6
    "blk-mq: remove alloc_hctx and free_hctx methods" where the last
    users of it were deleted.
    
    Fixes: cdef54dd85ad ("blk-mq: remove alloc_hctx and free_hctx methods")
    Signed-off-by: Linus Walleij <linus.walleij@linaro.org>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 7710f795d7c2..ff14f68067aa 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -223,7 +223,6 @@ static inline u16 blk_mq_unique_tag_to_tag(u32 unique_tag)
 }
 
 struct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *, const int ctx_index);
-struct blk_mq_hw_ctx *blk_mq_alloc_single_hw_queue(struct blk_mq_tag_set *, unsigned int, int);
 
 int blk_mq_request_started(struct request *rq);
 void blk_mq_start_request(struct request *rq);

commit 6e219353afa1f67f453141f7462b01708ebf5574
Author: Stephen Bates <sbates@raithlin.com>
Date:   Tue Sep 13 12:23:15 2016 -0600

    block: add poll_considered statistic
    
    In order to help determine the effectiveness of polling in a running
    system it is usful to determine the ratio of how often the poll
    function is called vs how often the completion is checked. For this
    reason we add a poll_considered variable and add it to the sysfs entry
    for io_poll.
    
    Signed-off-by: Stephen Bates <sbates@raithlin.com>
    Acked-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index e1544f0f8c21..7710f795d7c2 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -61,6 +61,7 @@ struct blk_mq_hw_ctx {
 	struct blk_mq_cpu_notifier	cpu_notifier;
 	struct kobject		kobj;
 
+	unsigned long		poll_considered;
 	unsigned long		poll_invoked;
 	unsigned long		poll_success;
 };

commit 8d354f133e86dd03ea7885a91df398c55ff699ff
Author: Jens Axboe <axboe@fb.com>
Date:   Thu Aug 25 08:00:28 2016 -0600

    blk-mq: improve layout of blk_mq_hw_ctx
    
    Various cache line optimizations:
    
    - Move delay_work towards the end. It's huge, and we don't use it
      a lot (only SCSI).
    
    - Move the atomic state into the same cacheline as the the dispatch
      list and lock.
    
    - Rearrange a few members to pack it better.
    
    - Shrink the max-order for dispatch accounting from 10 to 7. This
      means that ->dispatched[] and ->run now take up their own
      cacheline.
    
    This shrinks struct blk_mq_hw_ctx down to 8 cachelines.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index d579252e6463..e1544f0f8c21 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -22,11 +22,10 @@ struct blk_mq_hw_ctx {
 	struct {
 		spinlock_t		lock;
 		struct list_head	dispatch;
+		unsigned long		state;		/* BLK_MQ_S_* flags */
 	} ____cacheline_aligned_in_smp;
 
-	unsigned long		state;		/* BLK_MQ_S_* flags */
 	struct work_struct	run_work;
-	struct delayed_work	delay_work;
 	cpumask_var_t		cpumask;
 	int			next_cpu;
 	int			next_cpu_batch;
@@ -40,8 +39,8 @@ struct blk_mq_hw_ctx {
 
 	struct blk_mq_ctxmap	ctx_map;
 
-	unsigned int		nr_ctx;
 	struct blk_mq_ctx	**ctxs;
+	unsigned int		nr_ctx;
 
 	atomic_t		wait_index;
 
@@ -49,7 +48,7 @@ struct blk_mq_hw_ctx {
 
 	unsigned long		queued;
 	unsigned long		run;
-#define BLK_MQ_MAX_DISPATCH_ORDER	10
+#define BLK_MQ_MAX_DISPATCH_ORDER	7
 	unsigned long		dispatched[BLK_MQ_MAX_DISPATCH_ORDER];
 
 	unsigned int		numa_node;
@@ -57,6 +56,8 @@ struct blk_mq_hw_ctx {
 
 	atomic_t		nr_active;
 
+	struct delayed_work	delay_work;
+
 	struct blk_mq_cpu_notifier	cpu_notifier;
 	struct kobject		kobj;
 

commit 27489a3c827b7eebba26eda0320bb0f100bef167
Author: Jens Axboe <axboe@fb.com>
Date:   Wed Aug 24 15:54:25 2016 -0600

    blk-mq: turn hctx->run_work into a regular work struct
    
    We don't need the larger delayed work struct, since we always run it
    immediately.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index e43bbffb5b7a..d579252e6463 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -25,7 +25,7 @@ struct blk_mq_hw_ctx {
 	} ____cacheline_aligned_in_smp;
 
 	unsigned long		state;		/* BLK_MQ_S_* flags */
-	struct delayed_work	run_work;
+	struct work_struct	run_work;
 	struct delayed_work	delay_work;
 	cpumask_var_t		cpumask;
 	int			next_cpu;

commit 486cf9899e311838b6ab95d19ff87c4da44d6508
Author: Sagi Grimberg <sagi@grimberg.me>
Date:   Wed Jul 6 21:55:48 2016 +0900

    blk-mq: Introduce blk_mq_reinit_tagset
    
    The new nvme-rdma driver will need to reinitialize all the tags as part of
    the error recovery procedure (realloc the tag memory region). Add a helper
    in blk-mq for it that can iterate over all requests in a tagset to make
    this easier.
    
    Signed-off-by: Sagi Grimberg <sagi@grimberg.me>
    Tested-by: Ming Lin <ming.l@ssi.samsung.com>
    Reviewed-by: Stephen Bates <Stephen.Bates@pmcs.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index cbfd8ca5f13e..e43bbffb5b7a 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -96,6 +96,7 @@ typedef int (init_request_fn)(void *, struct request *, unsigned int,
 		unsigned int, unsigned int);
 typedef void (exit_request_fn)(void *, struct request *, unsigned int,
 		unsigned int);
+typedef int (reinit_request_fn)(void *, struct request *);
 
 typedef void (busy_iter_fn)(struct blk_mq_hw_ctx *, struct request *, void *,
 		bool);
@@ -145,6 +146,7 @@ struct blk_mq_ops {
 	 */
 	init_request_fn		*init_request;
 	exit_request_fn		*exit_request;
+	reinit_request_fn	*reinit_request;
 };
 
 enum {
@@ -245,6 +247,7 @@ void blk_mq_tagset_busy_iter(struct blk_mq_tag_set *tagset,
 void blk_mq_freeze_queue(struct request_queue *q);
 void blk_mq_unfreeze_queue(struct request_queue *q);
 void blk_mq_freeze_queue_start(struct request_queue *q);
+int blk_mq_reinit_tagset(struct blk_mq_tag_set *set);
 
 void blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set, int nr_hw_queues);
 

commit 1f5bd336b9150560458b03460cbcfcfbcf8995b1
Author: Ming Lin <ming.l@ssi.samsung.com>
Date:   Mon Jun 13 16:45:21 2016 +0200

    blk-mq: add blk_mq_alloc_request_hctx
    
    For some protocols like NVMe over Fabrics we need to be able to send
    initialization commands to a specific queue.
    
    Based on an earlier patch from Christoph Hellwig <hch@lst.de>.
    
    Signed-off-by: Ming Lin <ming.l@ssi.samsung.com>
    [hch: disallow sleeping allocation, req_op fixes]
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Keith Busch <keith.busch@intel.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 2498fdf3a503..cbfd8ca5f13e 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -196,6 +196,8 @@ enum {
 
 struct request *blk_mq_alloc_request(struct request_queue *q, int rw,
 		unsigned int flags);
+struct request *blk_mq_alloc_request_hctx(struct request_queue *q, int op,
+		unsigned int flags, unsigned int hctx_idx);
 struct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag);
 struct cpumask *blk_mq_tags_cpumask(struct blk_mq_tags *tags);
 

commit e8f1e1630b0a98685d1a3521e8aba0dc7e68082c
Author: Sagi Grimberg <sagig@mellanox.com>
Date:   Thu Mar 10 13:58:49 2016 +0200

    blk-mq: Make blk_mq_all_tag_busy_iter static
    
    No caller outside the blk-mq code so we can settle
    with it static.
    
    Signed-off-by: Sagi Grimberg <sagig@mellanox.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index c808fec1ce44..2498fdf3a503 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -238,8 +238,6 @@ void blk_mq_start_hw_queues(struct request_queue *q);
 void blk_mq_start_stopped_hw_queues(struct request_queue *q, bool async);
 void blk_mq_run_hw_queues(struct request_queue *q, bool async);
 void blk_mq_delay_queue(struct blk_mq_hw_ctx *hctx, unsigned long msecs);
-void blk_mq_all_tag_busy_iter(struct blk_mq_tags *tags, busy_tag_iter_fn *fn,
-		void *priv);
 void blk_mq_tagset_busy_iter(struct blk_mq_tag_set *tagset,
 		busy_tag_iter_fn *fn, void *priv);
 void blk_mq_freeze_queue(struct request_queue *q);

commit e0489487ec9cd79ee1fa0dc5d3789c08b0e51a2c
Author: Sagi Grimberg <sagig@mellanox.com>
Date:   Thu Mar 10 13:58:46 2016 +0200

    blk-mq: Export tagset iter function
    
    Its useful to iterate on all the active tags in cases
    where we will need to fail all the queues IO.
    
    Signed-off-by: Sagi Grimberg <sagig@mellanox.com>
    [hch: carefully check for valid tagsets]
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 9ac9799b702b..c808fec1ce44 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -240,6 +240,8 @@ void blk_mq_run_hw_queues(struct request_queue *q, bool async);
 void blk_mq_delay_queue(struct blk_mq_hw_ctx *hctx, unsigned long msecs);
 void blk_mq_all_tag_busy_iter(struct blk_mq_tags *tags, busy_tag_iter_fn *fn,
 		void *priv);
+void blk_mq_tagset_busy_iter(struct blk_mq_tag_set *tagset,
+		busy_tag_iter_fn *fn, void *priv);
 void blk_mq_freeze_queue(struct request_queue *q);
 void blk_mq_unfreeze_queue(struct request_queue *q);
 void blk_mq_freeze_queue_start(struct request_queue *q);

commit 897bb0c7f1ea82d7cc882b19790b5e1df00ffc29
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Mar 19 11:30:33 2016 +0100

    blk-mq: Use proper cpumask iterator
    
    queue_for_each_ctx() iterates over per_cpu variables under the assumption that
    the possible cpu mask cannot have holes. That's wrong as all cpumasks can have
    holes. In case there are holes the iteration ends up accessing uninitialized
    memory and crashing as a result.
    
    Replace the macro by a proper for_each_possible_cpu() loop and drop the unused
    macro blk_ctx_sum() which references queue_for_each_ctx().
    
    Reported-by: Xiong Zhou <jencce.kernel@gmail.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 15a73d49fd1d..9ac9799b702b 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -263,22 +263,8 @@ static inline void *blk_mq_rq_to_pdu(struct request *rq)
 	for ((i) = 0; (i) < (q)->nr_hw_queues &&			\
 	     ({ hctx = (q)->queue_hw_ctx[i]; 1; }); (i)++)
 
-#define queue_for_each_ctx(q, ctx, i)					\
-	for ((i) = 0; (i) < (q)->nr_queues &&				\
-	     ({ ctx = per_cpu_ptr((q)->queue_ctx, (i)); 1; }); (i)++)
-
 #define hctx_for_each_ctx(hctx, ctx, i)					\
 	for ((i) = 0; (i) < (hctx)->nr_ctx &&				\
 	     ({ ctx = (hctx)->ctxs[(i)]; 1; }); (i)++)
 
-#define blk_ctx_sum(q, sum)						\
-({									\
-	struct blk_mq_ctx *__x;						\
-	unsigned int __ret = 0, __i;					\
-									\
-	queue_for_each_ctx((q), __x, __i)				\
-		__ret += sum;						\
-	__ret;								\
-})
-
 #endif

commit 868f2f0b72068a097508b6e8870a8950fd8eb7ef
Author: Keith Busch <keith.busch@intel.com>
Date:   Thu Dec 17 17:08:14 2015 -0700

    blk-mq: dynamic h/w context count
    
    The hardware's provided queue count may change at runtime with resource
    provisioning. This patch allows a block driver to alter the number of
    h/w queues available when its resource count changes.
    
    The main part is a new blk-mq API to request a new number of h/w queues
    for a given live tag set. The new API freezes all queues using that set,
    then adjusts the allocated count prior to remapping these to CPUs.
    
    The bulk of the rest just shifts where h/w contexts and all their
    artifacts are allocated and freed.
    
    The number of max h/w contexts is capped to the number of possible cpus
    since there is no use for more than that. As such, all pre-allocated
    memory for pointers need to account for the max possible rather than
    the initial number of queues.
    
    A side effect of this is that the blk-mq will proceed successfully as
    long as it can allocate at least one h/w context. Previously it would
    fail request queue initialization if less than the requested number
    was allocated.
    
    Signed-off-by: Keith Busch <keith.busch@intel.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Tested-by: Jon Derrick <jonathan.derrick@intel.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 7fc9296b5742..15a73d49fd1d 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -244,6 +244,8 @@ void blk_mq_freeze_queue(struct request_queue *q);
 void blk_mq_unfreeze_queue(struct request_queue *q);
 void blk_mq_freeze_queue_start(struct request_queue *q);
 
+void blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set, int nr_hw_queues);
+
 /*
  * Driver command data is immediately after the request. So subtract request
  * size to get back to the original request, add request size to get the PDU.

commit 6f3b0e8bcf3cbb87a7459b3ed018d31d918df3f8
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Nov 26 09:13:05 2015 +0100

    blk-mq: add a flags parameter to blk_mq_alloc_request
    
    We already have the reserved flag, and a nowait flag awkwardly encoded as
    a gfp_t.  Add a real flags argument to make the scheme more extensible and
    allow for a nicer calling convention.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index daf17d70aeca..7fc9296b5742 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -188,8 +188,14 @@ void blk_mq_insert_request(struct request *, bool, bool, bool);
 void blk_mq_free_request(struct request *rq);
 void blk_mq_free_hctx_request(struct blk_mq_hw_ctx *, struct request *rq);
 bool blk_mq_can_queue(struct blk_mq_hw_ctx *);
+
+enum {
+	BLK_MQ_REQ_NOWAIT	= (1 << 0), /* return when out of requests */
+	BLK_MQ_REQ_RESERVED	= (1 << 1), /* allocate from reserved pool */
+};
+
 struct request *blk_mq_alloc_request(struct request_queue *q, int rw,
-		gfp_t gfp, bool reserved);
+		unsigned int flags);
 struct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag);
 struct cpumask *blk_mq_tags_cpumask(struct blk_mq_tags *tags);
 

commit 05229beeddf7e75e2e616ddaad4b70e7fca9528d
Author: Jens Axboe <axboe@fb.com>
Date:   Thu Nov 5 10:44:55 2015 -0700

    block: add block polling support
    
    Add basic support for polling for specific IO to complete. This uses
    the cookie that blk-mq passes back, which enables the block layer
    to pass this cookie to the driver to spin for a specific request.
    
    This will be combined with request latency tracking, so we can make
    qualified decisions about when to poll and when not to. For now, for
    benchmark purposes, we add a sysfs file that controls whether polling
    is enabled or not.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>
    Acked-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Keith Busch <keith.busch@intel.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 83cc9d4e5455..daf17d70aeca 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -59,6 +59,9 @@ struct blk_mq_hw_ctx {
 
 	struct blk_mq_cpu_notifier	cpu_notifier;
 	struct kobject		kobj;
+
+	unsigned long		poll_invoked;
+	unsigned long		poll_success;
 };
 
 struct blk_mq_tag_set {
@@ -97,6 +100,8 @@ typedef void (exit_request_fn)(void *, struct request *, unsigned int,
 typedef void (busy_iter_fn)(struct blk_mq_hw_ctx *, struct request *, void *,
 		bool);
 typedef void (busy_tag_iter_fn)(struct request *, void *, bool);
+typedef int (poll_fn)(struct blk_mq_hw_ctx *, unsigned int);
+
 
 struct blk_mq_ops {
 	/*
@@ -114,6 +119,11 @@ struct blk_mq_ops {
 	 */
 	timeout_fn		*timeout;
 
+	/*
+	 * Called to poll for completion of a specific tag.
+	 */
+	poll_fn			*poll;
+
 	softirq_done_fn		*complete;
 
 	/*

commit 3ef28e83ab15799742e55fd13243a5f678b04242
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Wed Oct 21 13:20:12 2015 -0400

    block: generic request_queue reference counting
    
    Allow pmem, and other synchronous/bio-based block drivers, to fallback
    on a per-cpu reference count managed by the core for tracking queue
    live/dead state.
    
    The existing per-cpu reference count for the blk_mq case is promoted to
    be used in all block i/o scenarios.  This involves initializing it by
    default, waiting for it to drop to zero at exit, and holding a live
    reference over the invocation of q->make_request_fn() in
    generic_make_request().  The blk_mq code continues to take its own
    reference per blk_mq request and retains the ability to freeze the
    queue, but the check that the queue is frozen is moved to
    generic_make_request().
    
    This fixes crash signatures like the following:
    
     BUG: unable to handle kernel paging request at ffff880140000000
     [..]
     Call Trace:
      [<ffffffff8145e8bf>] ? copy_user_handle_tail+0x5f/0x70
      [<ffffffffa004e1e0>] pmem_do_bvec.isra.11+0x70/0xf0 [nd_pmem]
      [<ffffffffa004e331>] pmem_make_request+0xd1/0x200 [nd_pmem]
      [<ffffffff811c3162>] ? mempool_alloc+0x72/0x1a0
      [<ffffffff8141f8b6>] generic_make_request+0xd6/0x110
      [<ffffffff8141f966>] submit_bio+0x76/0x170
      [<ffffffff81286dff>] submit_bh_wbc+0x12f/0x160
      [<ffffffff81286e62>] submit_bh+0x12/0x20
      [<ffffffff813395bd>] jbd2_write_superblock+0x8d/0x170
      [<ffffffff8133974d>] jbd2_mark_journal_empty+0x5d/0x90
      [<ffffffff813399cb>] jbd2_journal_destroy+0x24b/0x270
      [<ffffffff810bc4ca>] ? put_pwq_unlocked+0x2a/0x30
      [<ffffffff810bc6f5>] ? destroy_workqueue+0x225/0x250
      [<ffffffff81303494>] ext4_put_super+0x64/0x360
      [<ffffffff8124ab1a>] generic_shutdown_super+0x6a/0xf0
    
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Keith Busch <keith.busch@intel.com>
    Cc: Ross Zwisler <ross.zwisler@linux.intel.com>
    Suggested-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Tested-by: Ross Zwisler <ross.zwisler@linux.intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 5e7d43ab61c0..83cc9d4e5455 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -166,7 +166,6 @@ enum {
 struct request_queue *blk_mq_init_queue(struct blk_mq_tag_set *);
 struct request_queue *blk_mq_init_allocated_queue(struct blk_mq_tag_set *set,
 						  struct request_queue *q);
-void blk_mq_finish_init(struct request_queue *q);
 int blk_mq_register_disk(struct gendisk *);
 void blk_mq_unregister_disk(struct gendisk *);
 

commit 0bf6cd5b9531bcc29c0a5e504b6ce2984c6fd8d8
Author: Christoph Hellwig <hch@lst.de>
Date:   Sun Sep 27 21:01:51 2015 +0200

    blk-mq: factor out a helper to iterate all tags for a request_queue
    
    And replace the blk_mq_tag_busy_iter with it - the driver use has been
    replaced with a new helper a while ago, and internal to the block we
    only need the new version.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index c1b5c867ff07..5e7d43ab61c0 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -223,8 +223,6 @@ void blk_mq_start_hw_queues(struct request_queue *q);
 void blk_mq_start_stopped_hw_queues(struct request_queue *q, bool async);
 void blk_mq_run_hw_queues(struct request_queue *q, bool async);
 void blk_mq_delay_queue(struct blk_mq_hw_ctx *hctx, unsigned long msecs);
-void blk_mq_tag_busy_iter(struct blk_mq_hw_ctx *hctx, busy_iter_fn *fn,
-		void *priv);
 void blk_mq_all_tag_busy_iter(struct blk_mq_tags *tags, busy_tag_iter_fn *fn,
 		void *priv);
 void blk_mq_freeze_queue(struct request_queue *q);

commit f4829a9b7a61e159367350008a608b062c4f6840
Author: Christoph Hellwig <hch@lst.de>
Date:   Sun Sep 27 21:01:50 2015 +0200

    blk-mq: fix racy updates of rq->errors
    
    blk_mq_complete_request may be a no-op if the request has already
    been completed by others means (e.g. a timeout or cancellation), but
    currently drivers have to set rq->errors before calling
    blk_mq_complete_request, which might leave us with the wrong error value.
    
    Add an error parameter to blk_mq_complete_request so that we can
    defer setting rq->errors until we known we won the race to complete the
    request.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index b80ba4572a31..c1b5c867ff07 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -214,7 +214,7 @@ void blk_mq_add_to_requeue_list(struct request *rq, bool at_head);
 void blk_mq_cancel_requeue_work(struct request_queue *q);
 void blk_mq_kick_requeue_list(struct request_queue *q);
 void blk_mq_abort_requeue_list(struct request_queue *q);
-void blk_mq_complete_request(struct request *rq);
+void blk_mq_complete_request(struct request *rq, int error);
 
 void blk_mq_stop_hw_queue(struct blk_mq_hw_ctx *hctx);
 void blk_mq_start_hw_queue(struct blk_mq_hw_ctx *hctx);

commit 4593fdbe7a2f44d5e64c627c715dd0bcec9bdf14
Author: Akinobu Mita <akinobu.mita@gmail.com>
Date:   Sun Sep 27 02:09:20 2015 +0900

    blk-mq: fix sysfs registration/unregistration race
    
    There is a race between cpu hotplug handling and adding/deleting
    gendisk for blk-mq, where both are trying to register and unregister
    the same sysfs entries.
    
    null_add_dev
        --> blk_mq_init_queue
            --> blk_mq_init_allocated_queue
                --> add to 'all_q_list' (*)
        --> add_disk
            --> blk_register_queue
                --> blk_mq_register_disk (++)
    
    null_del_dev
        --> del_gendisk
            --> blk_unregister_queue
                --> blk_mq_unregister_disk (--)
        --> blk_cleanup_queue
            --> blk_mq_free_queue
                --> del from 'all_q_list' (*)
    
    blk_mq_queue_reinit
        --> blk_mq_sysfs_unregister (-)
        --> blk_mq_sysfs_register (+)
    
    While the request queue is added to 'all_q_list' (*),
    blk_mq_queue_reinit() can be called for the queue anytime by CPU
    hotplug callback.  But blk_mq_sysfs_unregister (-) and
    blk_mq_sysfs_register (+) in blk_mq_queue_reinit must not be called
    before blk_mq_register_disk (++) and after blk_mq_unregister_disk (--)
    is finished.  Because '/sys/block/*/mq/' is not exists.
    
    There has already been BLK_MQ_F_SYSFS_UP flag in hctx->flags which can
    be used to track these sysfs stuff, but it is only fixing this issue
    partially.
    
    In order to fix it completely, we just need per-queue flag instead of
    per-hctx flag with appropriate locking.  So this introduces
    q->mq_sysfs_init_done which is properly protected with all_q_mutex.
    
    Also, we need to ensure that blk_mq_map_swqueue() is called with
    all_q_mutex is held.  Since hctx->nr_ctx is reset temporarily and
    updated in blk_mq_map_swqueue(), so we should avoid
    blk_mq_register_hctx() seeing the temporary hctx->nr_ctx value
    in CPU hotplug handling or adding/deleting gendisk .
    
    Signed-off-by: Akinobu Mita <akinobu.mita@gmail.com>
    Reviewed-by: Ming Lei <tom.leiming@gmail.com>
    Cc: Ming Lei <tom.leiming@gmail.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 37d1602c4f7a..b80ba4572a31 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -145,7 +145,6 @@ enum {
 	BLK_MQ_F_SHOULD_MERGE	= 1 << 0,
 	BLK_MQ_F_TAG_SHARED	= 1 << 1,
 	BLK_MQ_F_SG_MERGE	= 1 << 2,
-	BLK_MQ_F_SYSFS_UP	= 1 << 3,
 	BLK_MQ_F_DEFER_ISSUE	= 1 << 4,
 	BLK_MQ_F_ALLOC_POLICY_START_BIT = 8,
 	BLK_MQ_F_ALLOC_POLICY_BITS = 1,

commit f26cdc8536ad50fb802a0445f836b4f94ca09ae7
Author: Keith Busch <keith.busch@intel.com>
Date:   Mon Jun 1 09:29:53 2015 -0600

    blk-mq: Shared tag enhancements
    
    Storage controllers may expose multiple block devices that share hardware
    resources managed by blk-mq. This patch enhances the shared tags so a
    low-level driver can access the shared resources not tied to the unshared
    h/w contexts. This way the LLD can dynamically add and delete disks and
    request queues without having to track all the request_queue hctx's to
    iterate outstanding tags.
    
    Signed-off-by: Keith Busch <keith.busch@intel.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 2056a99b92f8..37d1602c4f7a 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -96,6 +96,7 @@ typedef void (exit_request_fn)(void *, struct request *, unsigned int,
 
 typedef void (busy_iter_fn)(struct blk_mq_hw_ctx *, struct request *, void *,
 		bool);
+typedef void (busy_tag_iter_fn)(struct request *, void *, bool);
 
 struct blk_mq_ops {
 	/*
@@ -182,6 +183,7 @@ bool blk_mq_can_queue(struct blk_mq_hw_ctx *);
 struct request *blk_mq_alloc_request(struct request_queue *q, int rw,
 		gfp_t gfp, bool reserved);
 struct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag);
+struct cpumask *blk_mq_tags_cpumask(struct blk_mq_tags *tags);
 
 enum {
 	BLK_MQ_UNIQUE_TAG_BITS = 16,
@@ -224,6 +226,8 @@ void blk_mq_run_hw_queues(struct request_queue *q, bool async);
 void blk_mq_delay_queue(struct blk_mq_hw_ctx *hctx, unsigned long msecs);
 void blk_mq_tag_busy_iter(struct blk_mq_hw_ctx *hctx, busy_iter_fn *fn,
 		void *priv);
+void blk_mq_all_tag_busy_iter(struct blk_mq_tags *tags, busy_tag_iter_fn *fn,
+		void *priv);
 void blk_mq_freeze_queue(struct request_queue *q);
 void blk_mq_unfreeze_queue(struct request_queue *q);
 void blk_mq_freeze_queue_start(struct request_queue *q);

commit 569fd0ce96087283866ab8c438dac4bcf1738846
Author: Jens Axboe <axboe@fb.com>
Date:   Fri Apr 17 08:28:50 2015 -0600

    blk-mq: fix iteration of busy bitmap
    
    Commit 889fa31f00b2 was a bit too eager in reducing the loop count,
    so we ended up missing queues in some configurations. Ensure that
    our division rounds up, so that's not the case.
    
    Reported-by: Guenter Roeck <linux@roeck-us.net>
    Fixes: 889fa31f00b2 ("blk-mq: reduce unnecessary software queue looping")
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 8210e8797c12..2056a99b92f8 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -13,7 +13,7 @@ struct blk_mq_cpu_notifier {
 };
 
 struct blk_mq_ctxmap {
-	unsigned int map_size;
+	unsigned int size;
 	unsigned int bits_per_word;
 	struct blk_align_bitmap *map;
 };

commit 2963e3f7e8e3465895897a175560210120b932ac
Author: Jens Axboe <axboe@fb.com>
Date:   Thu Apr 9 15:54:05 2015 -0600

    blk-mq: cleanup blk_mq_rq_to_pdu()
    
    Casting to void and adding the size of the request is "shit code" and
    only a "crazy monkey on crack" would write that. So lets clean it up.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index ebfe707cf722..8210e8797c12 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -230,7 +230,7 @@ void blk_mq_freeze_queue_start(struct request_queue *q);
 
 /*
  * Driver command data is immediately after the request. So subtract request
- * size to get back to the original request.
+ * size to get back to the original request, add request size to get the PDU.
  */
 static inline struct request *blk_mq_rq_from_pdu(void *pdu)
 {
@@ -238,7 +238,7 @@ static inline struct request *blk_mq_rq_from_pdu(void *pdu)
 }
 static inline void *blk_mq_rq_to_pdu(struct request *rq)
 {
-	return (void *) rq + sizeof(*rq);
+	return rq + 1;
 }
 
 #define queue_for_each_hw_ctx(q, hctx, i)				\

commit b94ec296403e99d5ac9a8c48332cec4118d44b94
Author: Mike Snitzer <snitzer@redhat.com>
Date:   Wed Mar 11 23:56:38 2015 -0400

    blk-mq: export blk_mq_run_hw_queues
    
    Rename blk_mq_run_queues to blk_mq_run_hw_queues, add async argument,
    and export it.
    
    DM's suspend support must be able to run the queue without starting
    stopped hw queues.
    
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 9a75c88e8908..ebfe707cf722 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -220,6 +220,7 @@ void blk_mq_start_hw_queue(struct blk_mq_hw_ctx *hctx);
 void blk_mq_stop_hw_queues(struct request_queue *q);
 void blk_mq_start_hw_queues(struct request_queue *q);
 void blk_mq_start_stopped_hw_queues(struct request_queue *q, bool async);
+void blk_mq_run_hw_queues(struct request_queue *q, bool async);
 void blk_mq_delay_queue(struct blk_mq_hw_ctx *hctx, unsigned long msecs);
 void blk_mq_tag_busy_iter(struct blk_mq_hw_ctx *hctx, busy_iter_fn *fn,
 		void *priv);

commit b62c21b71f08b7a4bfd025616ff1da2913a82904
Author: Mike Snitzer <snitzer@redhat.com>
Date:   Thu Mar 12 23:56:02 2015 -0400

    blk-mq: add blk_mq_init_allocated_queue and export blk_mq_register_disk
    
    Add a variant of blk_mq_init_queue that allows a previously allocated
    queue to be initialized.  blk_mq_init_allocated_queue models
    blk_init_allocated_queue -- which was also created for DM's use.
    
    DM's approach to device creation requires a placeholder request_queue be
    allocated for use with alloc_dev() but the decision about what type of
    request_queue will be ultimately created is deferred until all component
    devices referenced in the DM table are processed to determine the table
    type (request-based, blk-mq request-based, or bio-based).
    
    Also, because of DM's late finalization of the request_queue type
    the call to blk_mq_register_disk() doesn't happen during alloc_dev().
    Must export blk_mq_register_disk() so that DM can backfill the 'mq' dir
    once the blk-mq queue is fully allocated.
    
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>
    Reviewed-by: Ming Lei <ming.lei@canonical.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 7aec86127335..9a75c88e8908 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -164,6 +164,8 @@ enum {
 		<< BLK_MQ_F_ALLOC_POLICY_START_BIT)
 
 struct request_queue *blk_mq_init_queue(struct blk_mq_tag_set *);
+struct request_queue *blk_mq_init_allocated_queue(struct blk_mq_tag_set *set,
+						  struct request_queue *q);
 void blk_mq_finish_init(struct request_queue *q);
 int blk_mq_register_disk(struct gendisk *);
 void blk_mq_unregister_disk(struct gendisk *);

commit 3e12cefbe143b4947171ff92dd50024c4841e291
Merge: 6bec00352861 d427e3c82ef4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Feb 12 14:13:23 2015 -0800

    Merge branch 'for-3.20/core' of git://git.kernel.dk/linux-block
    
    Pull core block IO changes from Jens Axboe:
     "This contains:
    
       - A series from Christoph that cleans up and refactors various parts
         of the REQ_BLOCK_PC handling.  Contributions in that series from
         Dongsu Park and Kent Overstreet as well.
    
       - CFQ:
            - A bug fix for cfq for realtime IO scheduling from Jeff Moyer.
            - A stable patch fixing a potential crash in CFQ in OOM
              situations.  From Konstantin Khlebnikov.
    
       - blk-mq:
            - Add support for tag allocation policies, from Shaohua. This is
              a prep patch enabling libata (and other SCSI parts) to use the
              blk-mq tagging, instead of rolling their own.
            - Various little tweaks from Keith and Mike, in preparation for
              DM blk-mq support.
            - Minor little fixes or tweaks from me.
            - A double free error fix from Tony Battersby.
    
       - The partition 4k issue fixes from Matthew and Boaz.
    
       - Add support for zero+unprovision for blkdev_issue_zeroout() from
         Martin"
    
    * 'for-3.20/core' of git://git.kernel.dk/linux-block: (27 commits)
      block: remove unused function blk_bio_map_sg
      block: handle the null_mapped flag correctly in blk_rq_map_user_iov
      blk-mq: fix double-free in error path
      block: prevent request-to-request merging with gaps if not allowed
      blk-mq: make blk_mq_run_queues() static
      dm: fix multipath regression due to initializing wrong request
      cfq-iosched: handle failure of cfq group allocation
      block: Quiesce zeroout wrapper
      block: rewrite and split __bio_copy_iov()
      block: merge __bio_map_user_iov into bio_map_user_iov
      block: merge __bio_map_kern into bio_map_kern
      block: pass iov_iter to the BLOCK_PC mapping functions
      block: add a helper to free bio bounce buffer pages
      block: use blk_rq_map_user_iov to implement blk_rq_map_user
      block: simplify bio_map_kern
      block: mark blk-mq devices as stackable
      block: keep established cmd_flags when cloning into a blk-mq request
      block: add blk-mq support to blk_insert_cloned_request()
      block: require blk_rq_prep_clone() be given an initialized clone request
      blk-mq: add tag allocation policy
      ...

commit 201f201c33220f53856fd300e1990b779538d67f
Author: Jens Axboe <axboe@fb.com>
Date:   Tue Feb 10 13:31:34 2015 -0700

    blk-mq: make blk_mq_run_queues() static
    
    We no longer use it outside of blk-mq.c, so we can make it static
    and stop exporting it. Additionally, kill the 'async' argument, as
    there's only one used of it.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 86b08b1a5eba..ac6c7f534de1 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -175,7 +175,6 @@ void blk_mq_free_tag_set(struct blk_mq_tag_set *set);
 void blk_mq_flush_plug_list(struct blk_plug *plug, bool from_schedule);
 
 void blk_mq_insert_request(struct request *, bool, bool, bool);
-void blk_mq_run_queues(struct request_queue *q, bool async);
 void blk_mq_free_request(struct request *rq);
 void blk_mq_free_hctx_request(struct blk_mq_hw_ctx *, struct request *rq);
 bool blk_mq_can_queue(struct blk_mq_hw_ctx *);

commit 24391c0dc57c3756a219defaa781e68637d6ab7d
Author: Shaohua Li <shli@fb.com>
Date:   Fri Jan 23 14:18:00 2015 -0700

    blk-mq: add tag allocation policy
    
    This is the blk-mq part to support tag allocation policy. The default
    allocation policy isn't changed (though it's not a strict FIFO). The new
    policy is round-robin for libata. But it's a try-best implementation. If
    multiple tasks are competing, the tags returned will be mixed (which is
    unavoidable even with !mq, as requests from different tasks can be
    mixed in queue)
    
    Cc: Jens Axboe <axboe@fb.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Christoph Hellwig <hch@infradead.org>
    Signed-off-by: Shaohua Li <shli@fb.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 5b6500c77ed2..86b08b1a5eba 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -147,6 +147,8 @@ enum {
 	BLK_MQ_F_SG_MERGE	= 1 << 2,
 	BLK_MQ_F_SYSFS_UP	= 1 << 3,
 	BLK_MQ_F_DEFER_ISSUE	= 1 << 4,
+	BLK_MQ_F_ALLOC_POLICY_START_BIT = 8,
+	BLK_MQ_F_ALLOC_POLICY_BITS = 1,
 
 	BLK_MQ_S_STOPPED	= 0,
 	BLK_MQ_S_TAG_ACTIVE	= 1,
@@ -155,6 +157,12 @@ enum {
 
 	BLK_MQ_CPU_WORK_BATCH	= 8,
 };
+#define BLK_MQ_FLAG_TO_ALLOC_POLICY(flags) \
+	((flags >> BLK_MQ_F_ALLOC_POLICY_START_BIT) & \
+		((1 << BLK_MQ_F_ALLOC_POLICY_BITS) - 1))
+#define BLK_ALLOC_POLICY_TO_MQ_FLAG(policy) \
+	((policy & ((1 << BLK_MQ_F_ALLOC_POLICY_BITS) - 1)) \
+		<< BLK_MQ_F_ALLOC_POLICY_START_BIT)
 
 struct request_queue *blk_mq_init_queue(struct blk_mq_tag_set *);
 void blk_mq_finish_init(struct request_queue *q);

commit 1885b24d23716e09b9c952822b05fd7f68099cdb
Author: Jens Axboe <axboe@fb.com>
Date:   Wed Jan 7 18:55:45 2015 -0700

    blk-mq: Add helper to abort requeued requests
    
    Adds a helper function a driver can use to abort requeued requests in
    case any are pending when h/w queues are being removed.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 18684e0bdb8a..5735e7130d63 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -203,6 +203,7 @@ void blk_mq_requeue_request(struct request *rq);
 void blk_mq_add_to_requeue_list(struct request *rq, bool at_head);
 void blk_mq_cancel_requeue_work(struct request_queue *q);
 void blk_mq_kick_requeue_list(struct request_queue *q);
+void blk_mq_abort_requeue_list(struct request_queue *q);
 void blk_mq_complete_request(struct request *rq);
 
 void blk_mq_stop_hw_queue(struct blk_mq_hw_ctx *hctx);

commit c68ed59f534c318716c6189050af3c5ea03b8071
Author: Keith Busch <keith.busch@intel.com>
Date:   Wed Jan 7 18:55:44 2015 -0700

    blk-mq: Let drivers cancel requeue_work
    
    Kicking requeued requests will start h/w queues in a work_queue, which
    may alter the driver's requested state to temporarily stop them. This
    patch exports a method to cancel the q->requeue_work so a driver can be
    assured stopped h/w queues won't be started up before it is ready.
    
    Signed-off-by: Keith Busch <keith.busch@intel.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index addf8a19d806..18684e0bdb8a 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -201,6 +201,7 @@ void __blk_mq_end_request(struct request *rq, int error);
 
 void blk_mq_requeue_request(struct request *rq);
 void blk_mq_add_to_requeue_list(struct request *rq, bool at_head);
+void blk_mq_cancel_requeue_work(struct request_queue *q);
 void blk_mq_kick_requeue_list(struct request_queue *q);
 void blk_mq_complete_request(struct request *rq);
 

commit 973c01919bce7e3559b62a856b29211ec5ac325c
Author: Keith Busch <keith.busch@intel.com>
Date:   Wed Jan 7 18:55:43 2015 -0700

    blk-mq: Export if requests were started
    
    Drivers can iterate over all allocated request tags, but their callback
    needs a way to know if the driver started the request in the first place.
    
    Signed-off-by: Keith Busch <keith.busch@intel.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index d0de259e92b2..addf8a19d806 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -194,6 +194,7 @@ static inline u16 blk_mq_unique_tag_to_tag(u32 unique_tag)
 struct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *, const int ctx_index);
 struct blk_mq_hw_ctx *blk_mq_alloc_single_hw_queue(struct blk_mq_tag_set *, unsigned int, int);
 
+int blk_mq_request_started(struct request *rq);
 void blk_mq_start_request(struct request *rq);
 void blk_mq_end_request(struct request *rq, int error);
 void __blk_mq_end_request(struct request *rq, int error);

commit 17ded320706c6316376059cfbe9dccab32c62b42
Author: Jens Axboe <axboe@fb.com>
Date:   Wed Jan 7 10:44:04 2015 -0700

    blk-mq: get rid of ->cmd_size in the hardware queue
    
    We store it in the tag set, we don't need it in the hardware queue.
    While removing cmd_size, place ->queue_num further down to avoid
    a hole on 64-bit archs. It's not used in any fast paths, so we
    can safely move it.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 3b43f509432c..d0de259e92b2 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -34,7 +34,6 @@ struct blk_mq_hw_ctx {
 	unsigned long		flags;		/* BLK_MQ_F_* flags */
 
 	struct request_queue	*queue;
-	unsigned int		queue_num;
 	struct blk_flush_queue	*fq;
 
 	void			*driver_data;
@@ -54,7 +53,7 @@ struct blk_mq_hw_ctx {
 	unsigned long		dispatched[BLK_MQ_MAX_DISPATCH_ORDER];
 
 	unsigned int		numa_node;
-	unsigned int		cmd_size;	/* per-request extra data */
+	unsigned int		queue_num;
 
 	atomic_t		nr_active;
 

commit c761d96b079e99d106fa4064e730ef7d0f312f9d
Author: Jens Axboe <axboe@fb.com>
Date:   Fri Jan 2 15:05:12 2015 -0700

    blk-mq: export blk_mq_freeze_queue()
    
    Commit b4c6a028774b exported the start and unfreeze, but we need
    the regular blk_mq_freeze_queue() for the loop conversion.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 3b43f509432c..5b6500c77ed2 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -212,6 +212,7 @@ void blk_mq_start_stopped_hw_queues(struct request_queue *q, bool async);
 void blk_mq_delay_queue(struct blk_mq_hw_ctx *hctx, unsigned long msecs);
 void blk_mq_tag_busy_iter(struct blk_mq_hw_ctx *hctx, busy_iter_fn *fn,
 		void *priv);
+void blk_mq_freeze_queue(struct request_queue *q);
 void blk_mq_unfreeze_queue(struct request_queue *q);
 void blk_mq_freeze_queue_start(struct request_queue *q);
 

commit b4c6a028774bcf3f20ed1e66c27a05aa51a8cf55
Author: Keith Busch <keith.busch@intel.com>
Date:   Fri Dec 19 17:54:14 2014 -0700

    blk-mq: Export freeze/unfreeze functions
    
    Let drivers prevent entering a queue that isn't available.
    
    Signed-off-by: Keith Busch <keith.busch@intel.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 8aded9ab2e4e..3b43f509432c 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -212,6 +212,8 @@ void blk_mq_start_stopped_hw_queues(struct request_queue *q, bool async);
 void blk_mq_delay_queue(struct blk_mq_hw_ctx *hctx, unsigned long msecs);
 void blk_mq_tag_busy_iter(struct blk_mq_hw_ctx *hctx, busy_iter_fn *fn,
 		void *priv);
+void blk_mq_unfreeze_queue(struct request_queue *q);
+void blk_mq_freeze_queue_start(struct request_queue *q);
 
 /*
  * Driver command data is immediately after the request. So subtract request

commit caf292ae5bb9d57198ce001d8b762f7abae3a94d
Merge: 8f4385d590d4 fcbf6a087a7e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Dec 13 14:14:23 2014 -0800

    Merge branch 'for-3.19/core' of git://git.kernel.dk/linux-block
    
    Pull block driver core update from Jens Axboe:
     "This is the pull request for the core block IO changes for 3.19.  Not
      a huge round this time, mostly lots of little good fixes:
    
       - Fix a bug in sysfs blktrace interface causing a NULL pointer
         dereference, when enabled/disabled through that API.  From Arianna
         Avanzini.
    
       - Various updates/fixes/improvements for blk-mq:
    
            - A set of updates from Bart, mostly fixing buts in the tag
              handling.
    
            - Cleanup/code consolidation from Christoph.
    
            - Extend queue_rq API to be able to handle batching issues of IO
              requests. NVMe will utilize this shortly. From me.
    
            - A few tag and request handling updates from me.
    
            - Cleanup of the preempt handling for running queues from Paolo.
    
            - Prevent running of unmapped hardware queues from Ming Lei.
    
            - Move the kdump memory limiting check to be in the correct
              location, from Shaohua.
    
            - Initialize all software queues at init time from Takashi. This
              prevents a kobject warning when CPUs are brought online that
              weren't online when a queue was registered.
    
       - Single writeback fix for I_DIRTY clearing from Tejun.  Queued with
         the core IO changes, since it's just a single fix.
    
       - Version X of the __bio_add_page() segment addition retry from
         Maurizio.  Hope the Xth time is the charm.
    
       - Documentation fixup for IO scheduler merging from Jan.
    
       - Introduce (and use) generic IO stat accounting helpers for non-rq
         drivers, from Gu Zheng.
    
       - Kill off artificial limiting of max sectors in a request from
         Christoph"
    
    * 'for-3.19/core' of git://git.kernel.dk/linux-block: (26 commits)
      bio: modify __bio_add_page() to accept pages that don't start a new segment
      blk-mq: Fix uninitialized kobject at CPU hotplugging
      blktrace: don't let the sysfs interface remove trace from running list
      blk-mq: Use all available hardware queues
      blk-mq: Micro-optimize bt_get()
      blk-mq: Fix a race between bt_clear_tag() and bt_get()
      blk-mq: Avoid that __bt_get_word() wraps multiple times
      blk-mq: Fix a use-after-free
      blk-mq: prevent unmapped hw queue from being scheduled
      blk-mq: re-check for available tags after running the hardware queue
      blk-mq: fix hang in bt_get()
      blk-mq: move the kdump check to blk_mq_alloc_tag_set
      blk-mq: cleanup tag free handling
      blk-mq: use 'nr_cpu_ids' as highest CPU ID count for hwq <-> cpu map
      blk: introduce generic io stat accounting help function
      blk-mq: handle the single queue case in blk_mq_hctx_next_cpu
      genhd: check for int overflow in disk_expand_part_tbl()
      blk-mq: add blk_mq_free_hctx_request()
      blk-mq: export blk_mq_free_request()
      blk-mq: use get_cpu/put_cpu instead of preempt_disable/preempt_enable
      ...

commit 7c7f2f2bc9a63f9605a16eabac59fc655dfe7c9a
Author: Jens Axboe <axboe@fb.com>
Date:   Mon Nov 17 10:41:57 2014 -0700

    blk-mq: add blk_mq_free_hctx_request()
    
    It's silly to use blk_mq_free_request() which in turn maps the
    request to the hardware queue, for places where we already know
    what the hardware queue is. This saves us an extra mapping of a
    hardware queue on request completion, if the caller knows this
    information already.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index c3b64ec5321e..fb0a4fb3dc2b 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -169,6 +169,7 @@ void blk_mq_flush_plug_list(struct blk_plug *plug, bool from_schedule);
 void blk_mq_insert_request(struct request *, bool, bool, bool);
 void blk_mq_run_queues(struct request_queue *q, bool async);
 void blk_mq_free_request(struct request *rq);
+void blk_mq_free_hctx_request(struct blk_mq_hw_ctx *, struct request *rq);
 bool blk_mq_can_queue(struct blk_mq_hw_ctx *);
 struct request *blk_mq_alloc_request(struct request_queue *q, int rw,
 		gfp_t gfp, bool reserved);

commit 205fb5f5ba1d8edcf18009998ed05b80b7d186af
Author: Bart Van Assche <bvanassche@acm.org>
Date:   Thu Oct 30 14:45:11 2014 +0100

    blk-mq: add blk_mq_unique_tag()
    
    The queuecommand() callback functions in SCSI low-level drivers
    need to know which hardware context has been selected by the
    block layer. Since this information is not available in the
    request structure, and since passing the hctx pointer directly to
    the queuecommand callback function would require modification of
    all SCSI LLDs, add a function to the block layer that allows to
    query the hardware context index.
    
    Signed-off-by: Bart Van Assche <bvanassche@acm.org>
    Acked-by: Jens Axboe <axboe@kernel.dk>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Reviewed-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index c9be1589415a..15f7034aa377 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -167,6 +167,23 @@ struct request *blk_mq_alloc_request(struct request_queue *q, int rw,
 		gfp_t gfp, bool reserved);
 struct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag);
 
+enum {
+	BLK_MQ_UNIQUE_TAG_BITS = 16,
+	BLK_MQ_UNIQUE_TAG_MASK = (1 << BLK_MQ_UNIQUE_TAG_BITS) - 1,
+};
+
+u32 blk_mq_unique_tag(struct request *rq);
+
+static inline u16 blk_mq_unique_tag_to_hwq(u32 unique_tag)
+{
+	return unique_tag >> BLK_MQ_UNIQUE_TAG_BITS;
+}
+
+static inline u16 blk_mq_unique_tag_to_tag(u32 unique_tag)
+{
+	return unique_tag & BLK_MQ_UNIQUE_TAG_MASK;
+}
+
 struct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *, const int ctx_index);
 struct blk_mq_hw_ctx *blk_mq_alloc_single_hw_queue(struct blk_mq_tag_set *, unsigned int, int);
 

commit e167dfb53cb85fde7b15f644e9dbef7ba31896b6
Author: Jens Axboe <axboe@fb.com>
Date:   Wed Oct 29 11:18:26 2014 -0600

    blk-mq: add BLK_MQ_F_DEFER_ISSUE support flag
    
    Drivers can now tell blk-mq if they take advantage of the deferred
    issue through 'last' or not. If they do, don't do queue-direct
    for sync IO. This is a preparation patch for the nvme conversion.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index be01d7a687d4..c3b64ec5321e 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -146,6 +146,7 @@ enum {
 	BLK_MQ_F_TAG_SHARED	= 1 << 1,
 	BLK_MQ_F_SG_MERGE	= 1 << 2,
 	BLK_MQ_F_SYSFS_UP	= 1 << 3,
+	BLK_MQ_F_DEFER_ISSUE	= 1 << 4,
 
 	BLK_MQ_S_STOPPED	= 0,
 	BLK_MQ_S_TAG_ACTIVE	= 1,

commit 74c450521dd8d245b982da62592a18aa6f88b045
Author: Jens Axboe <axboe@fb.com>
Date:   Wed Oct 29 11:14:52 2014 -0600

    blk-mq: add a 'list' parameter to ->queue_rq()
    
    Since we have the notion of a 'last' request in a chain, we can use
    this to have the hardware optimize the issuing of requests. Add
    a list_head parameter to queue_rq that the driver can use to
    temporarily store hw commands for issue when 'last' is true. If we
    are doing a chain of requests, pass in a NULL list for the first
    request to force issue of that immediately, then batch the remainder
    for deferred issue until the last request has been sent.
    
    Instead of adding yet another argument to the hot ->queue_rq path,
    encapsulate the passed arguments in a blk_mq_queue_data structure.
    This is passed as a constant, and has been tested as faster than
    passing 4 (or even 3) args through ->queue_rq. Update drivers for
    the new ->queue_rq() prototype. There are no functional changes
    in this patch for drivers - if they don't use the passed in list,
    then they will just queue requests individually like before.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index c9be1589415a..be01d7a687d4 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -79,7 +79,13 @@ struct blk_mq_tag_set {
 	struct list_head	tag_list;
 };
 
-typedef int (queue_rq_fn)(struct blk_mq_hw_ctx *, struct request *, bool);
+struct blk_mq_queue_data {
+	struct request *rq;
+	struct list_head *list;
+	bool last;
+};
+
+typedef int (queue_rq_fn)(struct blk_mq_hw_ctx *, const struct blk_mq_queue_data *);
 typedef struct blk_mq_hw_ctx *(map_queue_fn)(struct request_queue *, const int);
 typedef enum blk_eh_timer_return (timeout_fn)(struct request *, bool);
 typedef int (init_hctx_fn)(struct blk_mq_hw_ctx *, void *, unsigned int);

commit d3dc366bbaf07c125561e90d6da4bb147741101a
Merge: 511c41d9e666 e19a8a0ad2d2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Oct 18 11:53:51 2014 -0700

    Merge branch 'for-3.18/core' of git://git.kernel.dk/linux-block
    
    Pull core block layer changes from Jens Axboe:
     "This is the core block IO pull request for 3.18.  Apart from the new
      and improved flush machinery for blk-mq, this is all mostly bug fixes
      and cleanups.
    
       - blk-mq timeout updates and fixes from Christoph.
    
       - Removal of REQ_END, also from Christoph.  We pass it through the
         ->queue_rq() hook for blk-mq instead, freeing up one of the request
         bits.  The space was overly tight on 32-bit, so Martin also killed
         REQ_KERNEL since it's no longer used.
    
       - blk integrity updates and fixes from Martin and Gu Zheng.
    
       - Update to the flush machinery for blk-mq from Ming Lei.  Now we
         have a per hardware context flush request, which both cleans up the
         code should scale better for flush intensive workloads on blk-mq.
    
       - Improve the error printing, from Rob Elliott.
    
       - Backing device improvements and cleanups from Tejun.
    
       - Fixup of a misplaced rq_complete() tracepoint from Hannes.
    
       - Make blk_get_request() return error pointers, fixing up issues
         where we NULL deref when a device goes bad or missing.  From Joe
         Lawrence.
    
       - Prep work for drastically reducing the memory consumption of dm
         devices from Junichi Nomura.  This allows creating clone bio sets
         without preallocating a lot of memory.
    
       - Fix a blk-mq hang on certain combinations of queue depths and
         hardware queues from me.
    
       - Limit memory consumption for blk-mq devices for crash dump
         scenarios and drivers that use crazy high depths (certain SCSI
         shared tag setups).  We now just use a single queue and limited
         depth for that"
    
    * 'for-3.18/core' of git://git.kernel.dk/linux-block: (58 commits)
      block: Remove REQ_KERNEL
      blk-mq: allocate cpumask on the home node
      bio-integrity: remove the needless fail handle of bip_slab creating
      block: include func name in __get_request prints
      block: make blk_update_request print prefix match ratelimited prefix
      blk-merge: don't compute bi_phys_segments from bi_vcnt for cloned bio
      block: fix alignment_offset math that assumes io_min is a power-of-2
      blk-mq: Make bt_clear_tag() easier to read
      blk-mq: fix potential hang if rolling wakeup depth is too high
      block: add bioset_create_nobvec()
      block: use bio_clone_fast() in blk_rq_prep_clone()
      block: misplaced rq_complete tracepoint
      sd: Honor block layer integrity handling flags
      block: Replace strnicmp with strncasecmp
      block: Add T10 Protection Information functions
      block: Don't merge requests if integrity flags differ
      block: Integrity checksum flag
      block: Relocate bio integrity flags
      block: Add a disk flag to block integrity profile
      block: Add prefix to block integrity profile flags
      ...

commit f70ced09170761acb69840cafaace4abc72cba4b
Author: Ming Lei <ming.lei@canonical.com>
Date:   Thu Sep 25 23:23:47 2014 +0800

    blk-mq: support per-distpatch_queue flush machinery
    
    This patch supports to run one single flush machinery for
    each blk-mq dispatch queue, so that:
    
    - current init_request and exit_request callbacks can
    cover flush request too, then the buggy copying way of
    initializing flush request's pdu can be fixed
    
    - flushing performance gets improved in case of multi hw-queue
    
    In fio sync write test over virtio-blk(4 hw queues, ioengine=sync,
    iodepth=64, numjobs=4, bs=4K), it is observed that througput gets
    increased a lot over my test environment:
            - throughput: +70% in case of virtio-blk over null_blk
            - throughput: +30% in case of virtio-blk over SSD image
    
    The multi virtqueue feature isn't merged to QEMU yet, and patches for
    the feature can be found in below tree:
    
            git://kernel.ubuntu.com/ming/qemu.git   v2.1.0-mq.4
    
    And simply passing 'num_queues=4 vectors=5' should be enough to
    enable multi queue(quad queue) feature for QEMU virtio-blk.
    
    Suggested-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Ming Lei <ming.lei@canonical.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 325349559fb0..02c5d950f444 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -4,6 +4,7 @@
 #include <linux/blkdev.h>
 
 struct blk_mq_tags;
+struct blk_flush_queue;
 
 struct blk_mq_cpu_notifier {
 	struct list_head list;
@@ -34,6 +35,7 @@ struct blk_mq_hw_ctx {
 
 	struct request_queue	*queue;
 	unsigned int		queue_num;
+	struct blk_flush_queue	*fq;
 
 	void			*driver_data;
 
@@ -119,6 +121,10 @@ struct blk_mq_ops {
 	/*
 	 * Called for every command allocated by the block layer to allow
 	 * the driver to set up driver specific data.
+	 *
+	 * Tag greater than or equal to queue_depth is for setting up
+	 * flush request.
+	 *
 	 * Ditto for exit/teardown.
 	 */
 	init_request_fn		*init_request;

commit 17497acbdce9506fd6a75115dee4ab80c3cc5ee5
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Sep 24 13:31:50 2014 -0400

    blk-mq, percpu_ref: start q->mq_usage_counter in atomic mode
    
    blk-mq uses percpu_ref for its usage counter which tracks the number
    of in-flight commands and used to synchronously drain the queue on
    freeze.  percpu_ref shutdown takes measureable wallclock time as it
    involves a sched RCU grace period.  This means that draining a blk-mq
    takes measureable wallclock time.  One would think that this shouldn't
    matter as queue shutdown should be a rare event which takes place
    asynchronously w.r.t. userland.
    
    Unfortunately, SCSI probing involves synchronously setting up and then
    tearing down a lot of request_queues back-to-back for non-existent
    LUNs.  This means that SCSI probing may take above ten seconds when
    scsi-mq is used.
    
      [    0.949892] scsi host0: Virtio SCSI HBA
      [    1.007864] scsi 0:0:0:0: Direct-Access     QEMU     QEMU HARDDISK    1.1. PQ: 0 ANSI: 5
      [    1.021299] scsi 0:0:1:0: Direct-Access     QEMU     QEMU HARDDISK    1.1. PQ: 0 ANSI: 5
      [    1.520356] tsc: Refined TSC clocksource calibration: 2491.910 MHz
    
      <stall>
    
      [   16.186549] sd 0:0:0:0: Attached scsi generic sg0 type 0
      [   16.190478] sd 0:0:1:0: Attached scsi generic sg1 type 0
      [   16.194099] osd: LOADED open-osd 0.2.1
      [   16.203202] sd 0:0:0:0: [sda] 31457280 512-byte logical blocks: (16.1 GB/15.0 GiB)
      [   16.208478] sd 0:0:0:0: [sda] Write Protect is off
      [   16.211439] sd 0:0:0:0: [sda] Write cache: enabled, read cache: enabled, doesn't support DPO or FUA
      [   16.218771] sd 0:0:1:0: [sdb] 31457280 512-byte logical blocks: (16.1 GB/15.0 GiB)
      [   16.223264] sd 0:0:1:0: [sdb] Write Protect is off
      [   16.225682] sd 0:0:1:0: [sdb] Write cache: enabled, read cache: enabled, doesn't support DPO or FUA
    
    This is also the reason why request_queues start in bypass mode which
    is ended on blk_register_queue() as shutting down a fully functional
    queue also involves a RCU grace period and the queues for non-existent
    SCSI devices never reach registration.
    
    blk-mq basically needs to do the same thing - start the mq in a
    degraded mode which is faster to shut down and then make it fully
    functional only after the queue reaches registration.  percpu_ref
    recently grew facilities to force atomic operation until explicitly
    switched to percpu mode, which can be used for this purpose.  This
    patch makes blk-mq initialize q->mq_usage_counter in atomic mode and
    switch it to percpu mode only once blk_register_queue() is reached.
    
    Note that this issue was previously worked around by 0a30288da1ae
    ("blk-mq, percpu_ref: implement a kludge for SCSI blk-mq stall during
    probe") for v3.17.  The temp fix was reverted in preparation of adding
    persistent atomic mode to percpu_ref by 9eca80461a45 ("Revert "blk-mq,
    percpu_ref: implement a kludge for SCSI blk-mq stall during probe"").
    This patch and the prerequisite percpu_ref changes will be merged
    during v3.18 devel cycle.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reported-by: Christoph Hellwig <hch@infradead.org>
    Link: http://lkml.kernel.org/g/20140919113815.GA10791@lst.de
    Fixes: add703fda981 ("blk-mq: use percpu_ref for mq usage count")
    Reviewed-by: Kent Overstreet <kmo@daterainc.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Johannes Weiner <hannes@cmpxchg.org>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index a1e31f274fcd..c13a0c09faea 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -140,6 +140,7 @@ enum {
 };
 
 struct request_queue *blk_mq_init_queue(struct blk_mq_tag_set *);
+void blk_mq_finish_init(struct request_queue *q);
 int blk_mq_register_disk(struct gendisk *);
 void blk_mq_unregister_disk(struct gendisk *);
 

commit 0152fb6b57c4fae769ee75ea2ae670f4ff39fba9
Author: Christoph Hellwig <hch@lst.de>
Date:   Sat Sep 13 16:40:13 2014 -0700

    blk-mq: pass a reserved argument to the timeout handler
    
    Allow blk-mq to pass an argument to the timeout handler to indicate
    if we're timing out a reserved or regular command.  For many drivers
    those need to be handled different.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 0eb0f642be4b..325349559fb0 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -79,6 +79,7 @@ struct blk_mq_tag_set {
 
 typedef int (queue_rq_fn)(struct blk_mq_hw_ctx *, struct request *, bool);
 typedef struct blk_mq_hw_ctx *(map_queue_fn)(struct request_queue *, const int);
+typedef enum blk_eh_timer_return (timeout_fn)(struct request *, bool);
 typedef int (init_hctx_fn)(struct blk_mq_hw_ctx *, void *, unsigned int);
 typedef void (exit_hctx_fn)(struct blk_mq_hw_ctx *, unsigned int);
 typedef int (init_request_fn)(void *, struct request *, unsigned int,
@@ -103,7 +104,7 @@ struct blk_mq_ops {
 	/*
 	 * Called on request timeout
 	 */
-	rq_timed_out_fn		*timeout;
+	timeout_fn		*timeout;
 
 	softirq_done_fn		*complete;
 

commit 81481eb423c295c5480a3fab9bb961cf286c91e7
Author: Christoph Hellwig <hch@lst.de>
Date:   Sat Sep 13 16:40:11 2014 -0700

    blk-mq: fix and simplify tag iteration for the timeout handler
    
    Don't do a kmalloc from timer to handle timeouts, chances are we could be
    under heavy load or similar and thus just miss out on the timeouts.
    Fortunately it is very easy to just iterate over all in use tags, and doing
    this properly actually cleans up the blk_mq_busy_iter API as well, and
    prepares us for the next patch by passing a reserved argument to the
    iterator.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index cb217c16990d..0eb0f642be4b 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -86,6 +86,9 @@ typedef int (init_request_fn)(void *, struct request *, unsigned int,
 typedef void (exit_request_fn)(void *, struct request *, unsigned int,
 		unsigned int);
 
+typedef void (busy_iter_fn)(struct blk_mq_hw_ctx *, struct request *, void *,
+		bool);
+
 struct blk_mq_ops {
 	/*
 	 * Queue request
@@ -174,7 +177,8 @@ void blk_mq_stop_hw_queues(struct request_queue *q);
 void blk_mq_start_hw_queues(struct request_queue *q);
 void blk_mq_start_stopped_hw_queues(struct request_queue *q, bool async);
 void blk_mq_delay_queue(struct blk_mq_hw_ctx *hctx, unsigned long msecs);
-void blk_mq_tag_busy_iter(struct blk_mq_tags *tags, void (*fn)(void *data, unsigned long *), void *data);
+void blk_mq_tag_busy_iter(struct blk_mq_hw_ctx *hctx, busy_iter_fn *fn,
+		void *priv);
 
 /*
  * Driver command data is immediately after the request. So subtract request

commit c8a446ad695ada43a885ec12b38411dbd190a11b
Author: Christoph Hellwig <hch@lst.de>
Date:   Sat Sep 13 16:40:10 2014 -0700

    blk-mq: rename blk_mq_end_io to blk_mq_end_request
    
    Now that we've changed the driver API on the submission side use the
    opportunity to fix up the name on the completion side to fit into the
    general scheme.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 878b6f71da48..cb217c16990d 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -160,8 +160,8 @@ struct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *, const int ctx_ind
 struct blk_mq_hw_ctx *blk_mq_alloc_single_hw_queue(struct blk_mq_tag_set *, unsigned int, int);
 
 void blk_mq_start_request(struct request *rq);
-void blk_mq_end_io(struct request *rq, int error);
-void __blk_mq_end_io(struct request *rq, int error);
+void blk_mq_end_request(struct request *rq, int error);
+void __blk_mq_end_request(struct request *rq, int error);
 
 void blk_mq_requeue_request(struct request *rq);
 void blk_mq_add_to_requeue_list(struct request *rq, bool at_head);

commit e2490073cd7c3d6f6ef6e029a208edd4d38efac4
Author: Christoph Hellwig <hch@lst.de>
Date:   Sat Sep 13 16:40:09 2014 -0700

    blk-mq: call blk_mq_start_request from ->queue_rq
    
    When we call blk_mq_start_request from the core blk-mq code before calling into
    ->queue_rq there is a racy window where the timeout handler can hit before we've
    fully set up the driver specific part of the command.
    
    Move the call to blk_mq_start_request into the driver so the driver can start
    the request only once it is fully set up.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 9c4e306a9217..878b6f71da48 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -159,6 +159,7 @@ struct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag);
 struct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *, const int ctx_index);
 struct blk_mq_hw_ctx *blk_mq_alloc_single_hw_queue(struct blk_mq_tag_set *, unsigned int, int);
 
+void blk_mq_start_request(struct request *rq);
 void blk_mq_end_io(struct request *rq, int error);
 void __blk_mq_end_io(struct request *rq, int error);
 

commit bf57229745f849e500ba69ff91e35bc8160a7373
Author: Christoph Hellwig <hch@lst.de>
Date:   Sat Sep 13 16:40:08 2014 -0700

    blk-mq: remove REQ_END
    
    Pass an explicit parameter for the last request in a batch to ->queue_rq
    instead of using a request flag.  Besides being a cleaner and non-stateful
    interface this is also required for the next patch, which fixes the blk-mq
    I/O submission code to not start a time too early.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index a1e31f274fcd..9c4e306a9217 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -77,7 +77,7 @@ struct blk_mq_tag_set {
 	struct list_head	tag_list;
 };
 
-typedef int (queue_rq_fn)(struct blk_mq_hw_ctx *, struct request *);
+typedef int (queue_rq_fn)(struct blk_mq_hw_ctx *, struct request *, bool);
 typedef struct blk_mq_hw_ctx *(map_queue_fn)(struct request_queue *, const int);
 typedef int (init_hctx_fn)(struct blk_mq_hw_ctx *, void *, unsigned int);
 typedef void (exit_hctx_fn)(struct blk_mq_hw_ctx *, unsigned int);

commit 8a58d1f1f373238cb0d6d7f8d3dd723aa164b8ac
Author: Jens Axboe <axboe@fb.com>
Date:   Fri Aug 15 12:38:41 2014 -0600

    blk-mq: get rid of unused BLK_MQ_F_SHOULD_SORT flag
    
    We used to use this for determining whether to sort the dispatch list,
    but it's unused now.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index eb726b9c5762..a1e31f274fcd 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -127,10 +127,9 @@ enum {
 	BLK_MQ_RQ_QUEUE_ERROR	= 2,	/* end IO with error */
 
 	BLK_MQ_F_SHOULD_MERGE	= 1 << 0,
-	BLK_MQ_F_SHOULD_SORT	= 1 << 1,
-	BLK_MQ_F_TAG_SHARED	= 1 << 2,
-	BLK_MQ_F_SG_MERGE	= 1 << 3,
-	BLK_MQ_F_SYSFS_UP	= 1 << 4,
+	BLK_MQ_F_TAG_SHARED	= 1 << 1,
+	BLK_MQ_F_SG_MERGE	= 1 << 2,
+	BLK_MQ_F_SYSFS_UP	= 1 << 3,
 
 	BLK_MQ_S_STOPPED	= 0,
 	BLK_MQ_S_TAG_ACTIVE	= 1,

commit 8537b12034cf1fd3fab3da2c859d71f76846fae9
Author: Alexander Gordeev <agordeev@redhat.com>
Date:   Tue Jun 17 22:12:35 2014 -0700

    blk-mq: bitmap tag: fix races on shared ::wake_index fields
    
    Fix racy updates of shared blk_mq_bitmap_tags::wake_index
    and blk_mq_hw_ctx::wake_index fields.
    
    Cc: Ming Lei <tom.leiming@gmail.com>
    Signed-off-by: Alexander Gordeev <agordeev@redhat.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index a002cf191427..eb726b9c5762 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -42,7 +42,7 @@ struct blk_mq_hw_ctx {
 	unsigned int		nr_ctx;
 	struct blk_mq_ctx	**ctxs;
 
-	unsigned int		wait_index;
+	atomic_t		wait_index;
 
 	struct blk_mq_tags	*tags;
 

commit a4391c6465d9c978fd4bded12e34bdde3f5458f0
Author: Jens Axboe <axboe@fb.com>
Date:   Thu Jun 5 15:21:56 2014 -0600

    blk-mq: bump max tag depth to 10K tags
    
    For some scsi-mq cases, the tag map can be huge. So increase the
    max number of tags we support.
    
    Additionally, don't fail with EINVAL if a user requests too many
    tags. Warn that the tag depth has been adjusted down, and store
    the new value inside the tag_set passed in.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 0feedebfde48..a002cf191427 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -135,7 +135,7 @@ enum {
 	BLK_MQ_S_STOPPED	= 0,
 	BLK_MQ_S_TAG_ACTIVE	= 1,
 
-	BLK_MQ_MAX_DEPTH	= 2048,
+	BLK_MQ_MAX_DEPTH	= 10240,
 
 	BLK_MQ_CPU_WORK_BATCH	= 8,
 };

commit 0e62f51f8753b048f391ee2d7f2af1f7297b0be5
Author: Jens Axboe <axboe@fb.com>
Date:   Wed Jun 4 10:23:49 2014 -0600

    blk-mq: let blk_mq_tag_to_rq() take blk_mq_tags as the main parameter
    
    We currently pass in the hardware queue, and get the tags from there.
    But from scsi-mq, with a shared tag space, it's a lot more convenient
    to pass in the blk_mq_tags instead as the hardware queue isn't always
    directly available. So instead of having to re-map to a given
    hardware queue from rq->mq_ctx, just pass in the tags structure.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index c15128833100..0feedebfde48 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -155,7 +155,7 @@ void blk_mq_free_request(struct request *rq);
 bool blk_mq_can_queue(struct blk_mq_hw_ctx *);
 struct request *blk_mq_alloc_request(struct request_queue *q, int rw,
 		gfp_t gfp, bool reserved);
-struct request *blk_mq_tag_to_rq(struct blk_mq_hw_ctx *hctx, unsigned int tag);
+struct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag);
 
 struct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *, const int ctx_index);
 struct blk_mq_hw_ctx *blk_mq_alloc_single_hw_queue(struct blk_mq_tag_set *, unsigned int, int);

commit 67aec14ce87fe25bdfff7dbf468556333df11c4e
Author: Jens Axboe <axboe@fb.com>
Date:   Fri May 30 08:25:36 2014 -0600

    blk-mq: make the sysfs mq/ layout reflect current mappings
    
    Currently blk-mq registers all the hardware queues in sysfs,
    regardless of whether it uses them (e.g. they have CPU mappings)
    or not. The unused hardware queues lack the cpux/ directories,
    and the other sysfs entries (like active, pending, etc) are all
    zeroes.
    
    Change this so that sysfs correctly reflects the current mappings
    of the hardware queues.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index ad3adb73cc70..c15128833100 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -130,6 +130,7 @@ enum {
 	BLK_MQ_F_SHOULD_SORT	= 1 << 1,
 	BLK_MQ_F_TAG_SHARED	= 1 << 2,
 	BLK_MQ_F_SG_MERGE	= 1 << 3,
+	BLK_MQ_F_SYSFS_UP	= 1 << 4,
 
 	BLK_MQ_S_STOPPED	= 0,
 	BLK_MQ_S_TAG_ACTIVE	= 1,

commit 2230237500821aedfcf2bba2a79d9cbca389233c
Author: Shaohua Li <shli@kernel.org>
Date:   Fri May 30 08:06:42 2014 -0600

    blk-mq: blk_mq_tag_to_rq should handle flush request
    
    flush request is special, which borrows the tag from the parent
    request. Hence blk_mq_tag_to_rq needs special handling to return
    the flush request from the tag.
    
    Signed-off-by: Shaohua Li <shli@fusionio.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 95de239444d2..ad3adb73cc70 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -154,7 +154,7 @@ void blk_mq_free_request(struct request *rq);
 bool blk_mq_can_queue(struct blk_mq_hw_ctx *);
 struct request *blk_mq_alloc_request(struct request_queue *q, int rw,
 		gfp_t gfp, bool reserved);
-struct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag);
+struct request *blk_mq_tag_to_rq(struct blk_mq_hw_ctx *hctx, unsigned int tag);
 
 struct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *, const int ctx_index);
 struct blk_mq_hw_ctx *blk_mq_alloc_single_hw_queue(struct blk_mq_tag_set *, unsigned int, int);

commit 05f1dd5315217398fc8d122bdee80f96a9f21274
Author: Jens Axboe <axboe@fb.com>
Date:   Thu May 29 09:53:32 2014 -0600

    block: add queue flag for disabling SG merging
    
    If devices are not SG starved, we waste a lot of time potentially
    collapsing SG segments. Enough that 1.5% of the CPU time goes
    to this, at only 400K IOPS. Add a queue flag, QUEUE_FLAG_NO_SG_MERGE,
    which just returns the number of vectors in a bio instead of looping
    over all segments and checking for collapsible ones.
    
    Add a BLK_MQ_F_SG_MERGE flag so that drivers can opt-in on the sg
    merging, if they so desire.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 91dfb75ce39f..95de239444d2 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -129,6 +129,7 @@ enum {
 	BLK_MQ_F_SHOULD_MERGE	= 1 << 0,
 	BLK_MQ_F_SHOULD_SORT	= 1 << 1,
 	BLK_MQ_F_TAG_SHARED	= 1 << 2,
+	BLK_MQ_F_SG_MERGE	= 1 << 3,
 
 	BLK_MQ_S_STOPPED	= 0,
 	BLK_MQ_S_TAG_ACTIVE	= 1,

commit cdef54dd85ad66e77262ea57796a3e81683dd5d6
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed May 28 18:11:06 2014 +0200

    blk-mq: remove alloc_hctx and free_hctx methods
    
    There is no need for drivers to control hardware context allocation
    now that we do the context to node mapping in common code.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 2bd82f399128..91dfb75ce39f 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -79,9 +79,6 @@ struct blk_mq_tag_set {
 
 typedef int (queue_rq_fn)(struct blk_mq_hw_ctx *, struct request *);
 typedef struct blk_mq_hw_ctx *(map_queue_fn)(struct request_queue *, const int);
-typedef struct blk_mq_hw_ctx *(alloc_hctx_fn)(struct blk_mq_tag_set *,
-		unsigned int, int);
-typedef void (free_hctx_fn)(struct blk_mq_hw_ctx *, unsigned int);
 typedef int (init_hctx_fn)(struct blk_mq_hw_ctx *, void *, unsigned int);
 typedef void (exit_hctx_fn)(struct blk_mq_hw_ctx *, unsigned int);
 typedef int (init_request_fn)(void *, struct request *, unsigned int,
@@ -107,12 +104,6 @@ struct blk_mq_ops {
 
 	softirq_done_fn		*complete;
 
-	/*
-	 * Override for hctx allocations (should probably go)
-	 */
-	alloc_hctx_fn		*alloc_hctx;
-	free_hctx_fn		*free_hctx;
-
 	/*
 	 * Called when the block layer side of a hardware queue has been
 	 * set up, allowing the driver to allocate/init matching structures.
@@ -166,7 +157,6 @@ struct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag);
 
 struct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *, const int ctx_index);
 struct blk_mq_hw_ctx *blk_mq_alloc_single_hw_queue(struct blk_mq_tag_set *, unsigned int, int);
-void blk_mq_free_single_hw_queue(struct blk_mq_hw_ctx *, unsigned int);
 
 void blk_mq_end_io(struct request *rq, int error);
 void __blk_mq_end_io(struct request *rq, int error);

commit 4ce01dd1a07d9cf3eaf44fbf4ea9a61b11badccc
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue May 27 20:59:46 2014 +0200

    blk-mq: merge blk_mq_alloc_reserved_request into blk_mq_alloc_request
    
    Instead of having two almost identical copies of the same code just let
    the callers pass in the reserved flag directly.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index b9a74a386dbc..2bd82f399128 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -160,8 +160,8 @@ void blk_mq_insert_request(struct request *, bool, bool, bool);
 void blk_mq_run_queues(struct request_queue *q, bool async);
 void blk_mq_free_request(struct request *rq);
 bool blk_mq_can_queue(struct blk_mq_hw_ctx *);
-struct request *blk_mq_alloc_request(struct request_queue *q, int rw, gfp_t gfp);
-struct request *blk_mq_alloc_reserved_request(struct request_queue *q, int rw, gfp_t gfp);
+struct request *blk_mq_alloc_request(struct request_queue *q, int rw,
+		gfp_t gfp, bool reserved);
 struct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag);
 
 struct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *, const int ctx_index);

commit 6fca6a611c27f1f0d90fbe1cc3c229dbf8c09e48
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed May 28 08:08:02 2014 -0600

    blk-mq: add helper to insert requests from irq context
    
    Both the cache flush state machine and the SCSI midlayer want to submit
    requests from irq context, and the current per-request requeue_work
    unfortunately causes corruption due to sharing with the csd field for
    flushes.  Replace them with a per-request_queue list of requests to
    be requeued.
    
    Based on an earlier test by Ming Lei.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reported-by: Ming Lei <tom.leiming@gmail.com>
    Tested-by: Ming Lei <tom.leiming@gmail.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 5b171fbe95c5..b9a74a386dbc 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -172,6 +172,8 @@ void blk_mq_end_io(struct request *rq, int error);
 void __blk_mq_end_io(struct request *rq, int error);
 
 void blk_mq_requeue_request(struct request *rq);
+void blk_mq_add_to_requeue_list(struct request *rq, bool at_head);
+void blk_mq_kick_requeue_list(struct request_queue *q);
 void blk_mq_complete_request(struct request *rq);
 
 void blk_mq_stop_hw_queue(struct blk_mq_hw_ctx *hctx);

commit 7738dac4f697ffbd0ed4c4aeb69a714ef9d876da
Author: Jens Axboe <axboe@fb.com>
Date:   Wed May 28 08:06:34 2014 -0600

    blk-mq: remove stale comment for blk_mq_complete_request()
    
    It works for both IPI and local completions as of commit
    95f096849932.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 1dfeb1529a61..5b171fbe95c5 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -172,11 +172,6 @@ void blk_mq_end_io(struct request *rq, int error);
 void __blk_mq_end_io(struct request *rq, int error);
 
 void blk_mq_requeue_request(struct request *rq);
-
-/*
- * Complete request through potential IPI for right placement. Driver must
- * have defined a mq_ops->complete() hook for this.
- */
 void blk_mq_complete_request(struct request *rq);
 
 void blk_mq_stop_hw_queue(struct blk_mq_hw_ctx *hctx);

commit 95f096849932fe5eaa7bfec887530cf556744a76
Author: Jens Axboe <axboe@fb.com>
Date:   Tue May 27 17:46:48 2014 -0600

    blk-mq: allow non-softirq completions
    
    Right now we export two ways of completing a request:
    
    1) blk_mq_complete_request(). This uses an IPI (if needed) and
       completes through q->softirq_done_fn(). It also works with
       timeouts.
    
    2) blk_mq_end_io(). This completes inline, and ignores any timeout
       state of the request.
    
    Let blk_mq_complete_request() handle non-softirq_done_fn completions
    as well, by just completing inline. If a driver has enough completion
    ports to place completions correctly, it need not define a
    mq_ops->complete() and we can avoid an indirect function call by
    doing the completion inline.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index afeb93496907..1dfeb1529a61 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -173,6 +173,10 @@ void __blk_mq_end_io(struct request *rq, int error);
 
 void blk_mq_requeue_request(struct request *rq);
 
+/*
+ * Complete request through potential IPI for right placement. Driver must
+ * have defined a mq_ops->complete() hook for this.
+ */
 void blk_mq_complete_request(struct request *rq);
 
 void blk_mq_stop_hw_queue(struct blk_mq_hw_ctx *hctx);

commit f14bbe77a96bb979dc539d8308ee18a9363a544f
Author: Jens Axboe <axboe@fb.com>
Date:   Tue May 27 12:06:53 2014 -0600

    blk-mq: pass in suggested NUMA node to ->alloc_hctx()
    
    Drivers currently have to figure this out on their own, and they
    are missing information to do it properly. The ones that did
    attempt to do it, do it wrong.
    
    So just pass in the suggested node directly to the alloc
    function.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index f76bb18350af..afeb93496907 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -80,7 +80,7 @@ struct blk_mq_tag_set {
 typedef int (queue_rq_fn)(struct blk_mq_hw_ctx *, struct request *);
 typedef struct blk_mq_hw_ctx *(map_queue_fn)(struct request_queue *, const int);
 typedef struct blk_mq_hw_ctx *(alloc_hctx_fn)(struct blk_mq_tag_set *,
-		unsigned int);
+		unsigned int, int);
 typedef void (free_hctx_fn)(struct blk_mq_hw_ctx *, unsigned int);
 typedef int (init_hctx_fn)(struct blk_mq_hw_ctx *, void *, unsigned int);
 typedef void (exit_hctx_fn)(struct blk_mq_hw_ctx *, unsigned int);
@@ -165,7 +165,7 @@ struct request *blk_mq_alloc_reserved_request(struct request_queue *q, int rw, g
 struct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag);
 
 struct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *, const int ctx_index);
-struct blk_mq_hw_ctx *blk_mq_alloc_single_hw_queue(struct blk_mq_tag_set *, unsigned int);
+struct blk_mq_hw_ctx *blk_mq_alloc_single_hw_queue(struct blk_mq_tag_set *, unsigned int, int);
 void blk_mq_free_single_hw_queue(struct blk_mq_hw_ctx *, unsigned int);
 
 void blk_mq_end_io(struct request *rq, int error);

commit edf866b3805c5651bf7d035b72dc0190cb6ff4a7
Author: Sam Bradshaw <sbradshaw@micron.com>
Date:   Fri May 23 13:30:16 2014 -0600

    blk-mq: export blk_mq_tag_busy_iter
    
    Export the blk-mq in-flight tag iterator for driver consumption.
    This is particularly useful in exception paths or SRSI where
    in-flight IOs need to be cancelled and/or reissued. The NVMe driver
    conversion will use this.
    
    Signed-off-by: Sam Bradshaw <sbradshaw@micron.com>
    Signed-off-by: Matias Bj√∏rling <m@bjorling.me>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 4d2800567aad..f76bb18350af 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -181,6 +181,7 @@ void blk_mq_stop_hw_queues(struct request_queue *q);
 void blk_mq_start_hw_queues(struct request_queue *q);
 void blk_mq_start_stopped_hw_queues(struct request_queue *q, bool async);
 void blk_mq_delay_queue(struct blk_mq_hw_ctx *hctx, unsigned long msecs);
+void blk_mq_tag_busy_iter(struct blk_mq_tags *tags, void (*fn)(void *data, unsigned long *), void *data);
 
 /*
  * Driver command data is immediately after the request. So subtract request

commit e814e71ba4a6e1d7509b0f4b1928365ea650cace
Author: Jens Axboe <axboe@fb.com>
Date:   Wed May 21 13:59:08 2014 -0600

    blk-mq: allow the hctx cpu hotplug notifier to return errors
    
    Prepare this for the next patch which adds more smarts in the
    plugging logic, so that we can save some memory.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index f45424453338..4d2800567aad 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -8,7 +8,7 @@ struct blk_mq_tags;
 struct blk_mq_cpu_notifier {
 	struct list_head list;
 	void *data;
-	void (*notify)(void *data, unsigned long action, unsigned int cpu);
+	int (*notify)(void *data, unsigned long action, unsigned int cpu);
 };
 
 struct blk_mq_ctxmap {

commit e3a2b3f931f59d5284abd13faf8bded726884ffd
Author: Jens Axboe <axboe@fb.com>
Date:   Tue May 20 11:49:02 2014 -0600

    blk-mq: allow changing of queue depth through sysfs
    
    For request_fn based devices, the block layer exports a 'nr_requests'
    file through sysfs to allow adjusting of queue depth on the fly.
    Currently this returns -EINVAL for blk-mq, since it's not wired up.
    Wire this up for blk-mq, so that it now also always dynamic
    adjustments of the allowed queue depth for any given block device
    managed by blk-mq.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index a06ca7b5ea05..f45424453338 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -63,7 +63,7 @@ struct blk_mq_hw_ctx {
 struct blk_mq_tag_set {
 	struct blk_mq_ops	*ops;
 	unsigned int		nr_hw_queues;
-	unsigned int		queue_depth;
+	unsigned int		queue_depth;	/* max hw supported */
 	unsigned int		reserved_tags;
 	unsigned int		cmd_size;	/* per-request extra data */
 	int			numa_node;

commit 39a9f97e5ea99e048c4980c23cf197f6e77995cb
Merge: 1429d7c9467e 0d2602ca30e4
Author: Jens Axboe <axboe@fb.com>
Date:   Mon May 19 11:52:35 2014 -0600

    Merge branch 'for-3.16/blk-mq-tagging' into for-3.16/core
    
    Signed-off-by: Jens Axboe <axboe@fb.com>
    
    Conflicts:
            block/blk-mq-tag.c

commit 1429d7c9467e1e3de0b0ff91d7e4d67c1a92f8a3
Author: Jens Axboe <axboe@fb.com>
Date:   Mon May 19 09:23:55 2014 -0600

    blk-mq: switch ctx pending map to the sparser blk_align_bitmap
    
    Each hardware queue has a bitmap of software queues with pending
    requests. When new IO is queued on a software queue, the bit is
    set, and when IO is pruned on a hardware queue run, the bit is
    cleared. This causes a lot of traffic. Switch this from the regular
    BITS_PER_LONG bitmap to a sparser layout, similarly to what was
    done for blk-mq tagging.
    
    20% performance increase was observed for single threaded IO, and
    about 15% performanc increase on multiple threads driving the
    same device.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index f83d15f6e1c1..952e558ee598 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -11,6 +11,12 @@ struct blk_mq_cpu_notifier {
 	void (*notify)(void *data, unsigned long action, unsigned int cpu);
 };
 
+struct blk_mq_ctxmap {
+	unsigned int map_size;
+	unsigned int bits_per_word;
+	struct blk_align_bitmap *map;
+};
+
 struct blk_mq_hw_ctx {
 	struct {
 		spinlock_t		lock;
@@ -31,8 +37,8 @@ struct blk_mq_hw_ctx {
 
 	void			*driver_data;
 
-	unsigned int 		nr_ctx_map;
-	unsigned long		*ctx_map;
+	struct blk_mq_ctxmap	ctx_map;
+
 	unsigned int		nr_ctx;
 	struct blk_mq_ctx	**ctxs;
 

commit 0d2602ca30e410e84e8bdf05c84ed5688e0a5a44
Author: Jens Axboe <axboe@fb.com>
Date:   Tue May 13 15:10:52 2014 -0600

    blk-mq: improve support for shared tags maps
    
    This adds support for active queue tracking, meaning that the
    blk-mq tagging maintains a count of active users of a tag set.
    This allows us to maintain a notion of fairness between users,
    so that we can distribute the tag depth evenly without starving
    some users while allowing others to try unfair deep queues.
    
    If sharing of a tag set is detected, each hardware queue will
    track the depth of its own queue. And if this exceeds the total
    depth divided by the number of active queues, the user is actively
    throttled down.
    
    The active queue count is done lazily to avoid bouncing that data
    between submitter and completer. Each hardware queue gets marked
    active when it allocates its first tag, and gets marked inactive
    when 1) the last tag is cleared, and 2) the queue timeout grace
    period has passed.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index f83d15f6e1c1..379f88d5c44d 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -48,6 +48,8 @@ struct blk_mq_hw_ctx {
 	unsigned int		numa_node;
 	unsigned int		cmd_size;	/* per-request extra data */
 
+	atomic_t		nr_active;
+
 	struct blk_mq_cpu_notifier	cpu_notifier;
 	struct kobject		kobj;
 };
@@ -64,6 +66,9 @@ struct blk_mq_tag_set {
 	void			*driver_data;
 
 	struct blk_mq_tags	**tags;
+
+	struct mutex		tag_list_lock;
+	struct list_head	tag_list;
 };
 
 typedef int (queue_rq_fn)(struct blk_mq_hw_ctx *, struct request *);
@@ -126,8 +131,10 @@ enum {
 
 	BLK_MQ_F_SHOULD_MERGE	= 1 << 0,
 	BLK_MQ_F_SHOULD_SORT	= 1 << 1,
+	BLK_MQ_F_TAG_SHARED	= 1 << 2,
 
 	BLK_MQ_S_STOPPED	= 0,
+	BLK_MQ_S_TAG_ACTIVE	= 1,
 
 	BLK_MQ_MAX_DEPTH	= 2048,
 

commit 4bb659b156996f2993dc16fad71fec9ee070153c
Author: Jens Axboe <axboe@fb.com>
Date:   Fri May 9 09:36:49 2014 -0600

    blk-mq: implement new and more efficient tagging scheme
    
    blk-mq currently uses percpu_ida for tag allocation. But that only
    works well if the ratio between tag space and number of CPUs is
    sufficiently high. For most devices and systems, that is not the
    case. The end result if that we either only utilize the tag space
    partially, or we end up attempting to fully exhaust it and run
    into lots of lock contention with stealing between CPUs. This is
    not optimal.
    
    This new tagging scheme is a hybrid bitmap allocator. It uses
    two tricks to both be SMP friendly and allow full exhaustion
    of the space:
    
    1) We cache the last allocated (or freed) tag on a per blk-mq
       software context basis. This allows us to limit the space
       we have to search. The key element here is not caching it
       in the shared tag structure, otherwise we end up dirtying
       more shared cache lines on each allocate/free operation.
    
    2) The tag space is split into cache line sized groups, and
       each context will start off randomly in that space. Even up
       to full utilization of the space, this divides the tag users
       efficiently into cache line groups, avoiding dirtying the same
       one both between allocators and between allocator and freeer.
    
    This scheme shows drastically better behaviour, both on small
    tag spaces but on large ones as well. It has been tested extensively
    to show better performance for all the cases blk-mq cares about.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 5bd677e2dcb7..f83d15f6e1c1 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -31,10 +31,12 @@ struct blk_mq_hw_ctx {
 
 	void			*driver_data;
 
-	unsigned int		nr_ctx;
-	struct blk_mq_ctx	**ctxs;
 	unsigned int 		nr_ctx_map;
 	unsigned long		*ctx_map;
+	unsigned int		nr_ctx;
+	struct blk_mq_ctx	**ctxs;
+
+	unsigned int		wait_index;
 
 	struct blk_mq_tags	*tags;
 

commit 506e931f92defdc60c1dc4aa2ff4a19a5dcd8618
Author: Jens Axboe <axboe@fb.com>
Date:   Wed May 7 10:26:44 2014 -0600

    blk-mq: add basic round-robin of what CPU to queue workqueue work on
    
    Right now we just pick the first CPU in the mask, but that can
    easily overload that one. Add some basic batching and round-robin
    all the entries in the mask instead.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 3b561d651a02..5bd677e2dcb7 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -21,6 +21,8 @@ struct blk_mq_hw_ctx {
 	struct delayed_work	run_work;
 	struct delayed_work	delay_work;
 	cpumask_var_t		cpumask;
+	int			next_cpu;
+	int			next_cpu_batch;
 
 	unsigned long		flags;		/* BLK_MQ_F_* flags */
 
@@ -126,6 +128,8 @@ enum {
 	BLK_MQ_S_STOPPED	= 0,
 
 	BLK_MQ_MAX_DEPTH	= 2048,
+
+	BLK_MQ_CPU_WORK_BATCH	= 8,
 };
 
 struct request_queue *blk_mq_init_queue(struct blk_mq_tag_set *);

commit 38535201633077cbaf8b32886b5e3005b36c9024
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Apr 25 02:32:53 2014 -0700

    blk-mq: respect rq_affinity
    
    The blk-mq code is using it's own version of the I/O completion affinity
    tunables, which causes a few issues:
    
     - the rq_affinity sysfs file doesn't work for blk-mq devices, even if it
       still is present, thus breaking existing tuning setups.
     - the rq_affinity = 1 mode, which is the defauly for legacy request based
       drivers isn't implemented at all.
     - blk-mq drivers don't implement any completion affinity with the default
       flag settings.
    
    This patches removes the blk-mq ipi_redirect flag and sysfs file, as well
    as the internal BLK_MQ_F_SHOULD_IPI flag and replaces it with code that
    respects the queue-wide rq_affinity flags and also implements the
    rq_affinity = 1 mode.
    
    This means I/O completion affinity can now only be tuned block-queue wide
    instead of per context, which seems more sensible to me anyway.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index ab469d525894..3b561d651a02 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -122,7 +122,6 @@ enum {
 
 	BLK_MQ_F_SHOULD_MERGE	= 1 << 0,
 	BLK_MQ_F_SHOULD_SORT	= 1 << 1,
-	BLK_MQ_F_SHOULD_IPI	= 1 << 2,
 
 	BLK_MQ_S_STOPPED	= 0,
 

commit ed0791b2f83cec4e77d88c4e9baabcebf9254a78
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Apr 16 09:44:57 2014 +0200

    blk-mq: add blk_mq_requeue_request
    
    This allows to requeue a request that has been accepted by ->queue_rq
    earlier.  This is needed by the SCSI layer in various error conditions.
    
    The existing internal blk_mq_requeue_request is renamed to
    __blk_mq_requeue_request as it is a lower level building block for this
    funtionality.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 391377e53367..ab469d525894 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -153,6 +153,8 @@ void blk_mq_free_single_hw_queue(struct blk_mq_hw_ctx *, unsigned int);
 void blk_mq_end_io(struct request *rq, int error);
 void __blk_mq_end_io(struct request *rq, int error);
 
+void blk_mq_requeue_request(struct request *rq);
+
 void blk_mq_complete_request(struct request *rq);
 
 void blk_mq_stop_hw_queue(struct blk_mq_hw_ctx *hctx);

commit 2f268556567ebeb3538f99b9bdad177581439dcb
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Apr 16 09:44:56 2014 +0200

    blk-mq: add blk_mq_start_hw_queues
    
    Add a helper to unconditionally kick contexts of a queue.  This will
    be needed by the SCSI layer to provide fair queueing between multiple
    devices on a single host.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index ae868e77bc2f..391377e53367 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -158,6 +158,7 @@ void blk_mq_complete_request(struct request *rq);
 void blk_mq_stop_hw_queue(struct blk_mq_hw_ctx *hctx);
 void blk_mq_start_hw_queue(struct blk_mq_hw_ctx *hctx);
 void blk_mq_stop_hw_queues(struct request_queue *q);
+void blk_mq_start_hw_queues(struct request_queue *q);
 void blk_mq_start_stopped_hw_queues(struct request_queue *q, bool async);
 void blk_mq_delay_queue(struct blk_mq_hw_ctx *hctx, unsigned long msecs);
 

commit 70f4db639c5b2479e08657392cbf3ba3cceea11c
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Apr 16 10:48:08 2014 -0600

    blk-mq: add blk_mq_delay_queue
    
    Add a blk-mq equivalent to blk_delay_queue so that the scsi layer can ask
    to be kicked again after a delay.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    
    Modified by me to kill the unnecessary preempt disable/enable
    in the delayed workqueue handler.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 9ecfab96d8c9..ae868e77bc2f 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -18,7 +18,8 @@ struct blk_mq_hw_ctx {
 	} ____cacheline_aligned_in_smp;
 
 	unsigned long		state;		/* BLK_MQ_S_* flags */
-	struct delayed_work	delayed_work;
+	struct delayed_work	run_work;
+	struct delayed_work	delay_work;
 	cpumask_var_t		cpumask;
 
 	unsigned long		flags;		/* BLK_MQ_F_* flags */
@@ -158,6 +159,7 @@ void blk_mq_stop_hw_queue(struct blk_mq_hw_ctx *hctx);
 void blk_mq_start_hw_queue(struct blk_mq_hw_ctx *hctx);
 void blk_mq_stop_hw_queues(struct request_queue *q);
 void blk_mq_start_stopped_hw_queues(struct request_queue *q, bool async);
+void blk_mq_delay_queue(struct blk_mq_hw_ctx *hctx, unsigned long msecs);
 
 /*
  * Driver command data is immediately after the request. So subtract request

commit 1b4a325858f695a9b5041313602d34b36f463724
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Apr 16 09:44:54 2014 +0200

    blk-mq: add async parameter to blk_mq_start_stopped_hw_queues
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index a81b474b794f..9ecfab96d8c9 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -157,7 +157,7 @@ void blk_mq_complete_request(struct request *rq);
 void blk_mq_stop_hw_queue(struct blk_mq_hw_ctx *hctx);
 void blk_mq_start_hw_queue(struct blk_mq_hw_ctx *hctx);
 void blk_mq_stop_hw_queues(struct request_queue *q);
-void blk_mq_start_stopped_hw_queues(struct request_queue *q);
+void blk_mq_start_stopped_hw_queues(struct request_queue *q, bool async);
 
 /*
  * Driver command data is immediately after the request. So subtract request

commit 63151a449ebaef062ffac5b302206565ff5ef62e
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Apr 16 09:44:52 2014 +0200

    blk-mq: allow drivers to hook into I/O completion
    
    Split out the bottom half of blk_mq_end_io so that drivers can perform
    work when they know a request has been completed, but before it has been
    freed.  This also obsoletes blk_mq_end_io_partial as drivers can now
    pass any value to blk_update_request directly.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index a4ea0ce83b07..a81b474b794f 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -149,13 +149,8 @@ struct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *, const int ctx_ind
 struct blk_mq_hw_ctx *blk_mq_alloc_single_hw_queue(struct blk_mq_tag_set *, unsigned int);
 void blk_mq_free_single_hw_queue(struct blk_mq_hw_ctx *, unsigned int);
 
-bool blk_mq_end_io_partial(struct request *rq, int error,
-		unsigned int nr_bytes);
-static inline void blk_mq_end_io(struct request *rq, int error)
-{
-	bool done = !blk_mq_end_io_partial(rq, error, blk_rq_bytes(rq));
-	BUG_ON(!done);
-}
+void blk_mq_end_io(struct request *rq, int error);
+void __blk_mq_end_io(struct request *rq, int error);
 
 void blk_mq_complete_request(struct request *rq);
 

commit 24d2f90309b23f2cfe016b2aebc5f0d6e01c57fd
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Apr 15 14:14:00 2014 -0600

    blk-mq: split out tag initialization, support shared tags
    
    Add a new blk_mq_tag_set structure that gets set up before we initialize
    the queue.  A single blk_mq_tag_set structure can be shared by multiple
    queues.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    
    Modular export of blk_mq_{alloc,free}_tagset added by me.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 29c1a6e83814..a4ea0ce83b07 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -33,8 +33,6 @@ struct blk_mq_hw_ctx {
 	unsigned int 		nr_ctx_map;
 	unsigned long		*ctx_map;
 
-	struct request		**rqs;
-	struct list_head	page_list;
 	struct blk_mq_tags	*tags;
 
 	unsigned long		queued;
@@ -42,7 +40,6 @@ struct blk_mq_hw_ctx {
 #define BLK_MQ_MAX_DISPATCH_ORDER	10
 	unsigned long		dispatched[BLK_MQ_MAX_DISPATCH_ORDER];
 
-	unsigned int		queue_depth;
 	unsigned int		numa_node;
 	unsigned int		cmd_size;	/* per-request extra data */
 
@@ -50,7 +47,7 @@ struct blk_mq_hw_ctx {
 	struct kobject		kobj;
 };
 
-struct blk_mq_reg {
+struct blk_mq_tag_set {
 	struct blk_mq_ops	*ops;
 	unsigned int		nr_hw_queues;
 	unsigned int		queue_depth;
@@ -59,18 +56,22 @@ struct blk_mq_reg {
 	int			numa_node;
 	unsigned int		timeout;
 	unsigned int		flags;		/* BLK_MQ_F_* */
+	void			*driver_data;
+
+	struct blk_mq_tags	**tags;
 };
 
 typedef int (queue_rq_fn)(struct blk_mq_hw_ctx *, struct request *);
 typedef struct blk_mq_hw_ctx *(map_queue_fn)(struct request_queue *, const int);
-typedef struct blk_mq_hw_ctx *(alloc_hctx_fn)(struct blk_mq_reg *,unsigned int);
+typedef struct blk_mq_hw_ctx *(alloc_hctx_fn)(struct blk_mq_tag_set *,
+		unsigned int);
 typedef void (free_hctx_fn)(struct blk_mq_hw_ctx *, unsigned int);
 typedef int (init_hctx_fn)(struct blk_mq_hw_ctx *, void *, unsigned int);
 typedef void (exit_hctx_fn)(struct blk_mq_hw_ctx *, unsigned int);
-typedef int (init_request_fn)(void *, struct blk_mq_hw_ctx *,
-		struct request *, unsigned int);
-typedef void (exit_request_fn)(void *, struct blk_mq_hw_ctx *,
-		struct request *, unsigned int);
+typedef int (init_request_fn)(void *, struct request *, unsigned int,
+		unsigned int, unsigned int);
+typedef void (exit_request_fn)(void *, struct request *, unsigned int,
+		unsigned int);
 
 struct blk_mq_ops {
 	/*
@@ -127,10 +128,13 @@ enum {
 	BLK_MQ_MAX_DEPTH	= 2048,
 };
 
-struct request_queue *blk_mq_init_queue(struct blk_mq_reg *, void *);
+struct request_queue *blk_mq_init_queue(struct blk_mq_tag_set *);
 int blk_mq_register_disk(struct gendisk *);
 void blk_mq_unregister_disk(struct gendisk *);
 
+int blk_mq_alloc_tag_set(struct blk_mq_tag_set *set);
+void blk_mq_free_tag_set(struct blk_mq_tag_set *set);
+
 void blk_mq_flush_plug_list(struct blk_plug *plug, bool from_schedule);
 
 void blk_mq_insert_request(struct request *, bool, bool, bool);
@@ -139,10 +143,10 @@ void blk_mq_free_request(struct request *rq);
 bool blk_mq_can_queue(struct blk_mq_hw_ctx *);
 struct request *blk_mq_alloc_request(struct request_queue *q, int rw, gfp_t gfp);
 struct request *blk_mq_alloc_reserved_request(struct request_queue *q, int rw, gfp_t gfp);
-struct request *blk_mq_rq_from_tag(struct request_queue *q, unsigned int tag);
+struct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag);
 
 struct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *, const int ctx_index);
-struct blk_mq_hw_ctx *blk_mq_alloc_single_hw_queue(struct blk_mq_reg *, unsigned int);
+struct blk_mq_hw_ctx *blk_mq_alloc_single_hw_queue(struct blk_mq_tag_set *, unsigned int);
 void blk_mq_free_single_hw_queue(struct blk_mq_hw_ctx *, unsigned int);
 
 bool blk_mq_end_io_partial(struct request *rq, int error,
@@ -173,12 +177,6 @@ static inline void *blk_mq_rq_to_pdu(struct request *rq)
 	return (void *) rq + sizeof(*rq);
 }
 
-static inline struct request *blk_mq_tag_to_rq(struct blk_mq_hw_ctx *hctx,
-					       unsigned int tag)
-{
-	return hctx->rqs[tag];
-}
-
 #define queue_for_each_hw_ctx(q, hctx, i)				\
 	for ((i) = 0; (i) < (q)->nr_hw_queues &&			\
 	     ({ hctx = (q)->queue_hw_ctx[i]; 1; }); (i)++)

commit e9b267d91f6ddbc694cb40aa962b0b2cec03971d
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Apr 15 13:59:10 2014 -0600

    blk-mq: add ->init_request and ->exit_request methods
    
    The current blk_mq_init_commands/blk_mq_free_commands interface has a
    two problems:
    
     1) Because only the constructor is passed to blk_mq_init_commands there
        is no easy way to clean up when a comman initialization failed.  The
        current code simply leaks the allocations done in the constructor.
    
     2) There is no good place to call blk_mq_free_commands: before
        blk_cleanup_queue there is no guarantee that all outstanding
        commands have completed, so we can't free them yet.  After
        blk_cleanup_queue the queue has usually been freed.  This can be
        worked around by grabbing an unconditional reference before calling
        blk_cleanup_queue and dropping it after blk_mq_free_commands is
        done, although that's not exatly pretty and driver writers are
        guaranteed to get it wrong sooner or later.
    
    Both issues are easily fixed by making the request constructor and
    destructor normal blk_mq_ops methods.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index b6ee48740458..29c1a6e83814 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -67,6 +67,10 @@ typedef struct blk_mq_hw_ctx *(alloc_hctx_fn)(struct blk_mq_reg *,unsigned int);
 typedef void (free_hctx_fn)(struct blk_mq_hw_ctx *, unsigned int);
 typedef int (init_hctx_fn)(struct blk_mq_hw_ctx *, void *, unsigned int);
 typedef void (exit_hctx_fn)(struct blk_mq_hw_ctx *, unsigned int);
+typedef int (init_request_fn)(void *, struct blk_mq_hw_ctx *,
+		struct request *, unsigned int);
+typedef void (exit_request_fn)(void *, struct blk_mq_hw_ctx *,
+		struct request *, unsigned int);
 
 struct blk_mq_ops {
 	/*
@@ -99,6 +103,14 @@ struct blk_mq_ops {
 	 */
 	init_hctx_fn		*init_hctx;
 	exit_hctx_fn		*exit_hctx;
+
+	/*
+	 * Called for every command allocated by the block layer to allow
+	 * the driver to set up driver specific data.
+	 * Ditto for exit/teardown.
+	 */
+	init_request_fn		*init_request;
+	exit_request_fn		*exit_request;
 };
 
 enum {
@@ -118,8 +130,6 @@ enum {
 struct request_queue *blk_mq_init_queue(struct blk_mq_reg *, void *);
 int blk_mq_register_disk(struct gendisk *);
 void blk_mq_unregister_disk(struct gendisk *);
-int blk_mq_init_commands(struct request_queue *, int (*init)(void *data, struct blk_mq_hw_ctx *, struct request *, unsigned int), void *data);
-void blk_mq_free_commands(struct request_queue *, void (*free)(void *data, struct blk_mq_hw_ctx *, struct request *, unsigned int), void *data);
 
 void blk_mq_flush_plug_list(struct blk_plug *plug, bool from_schedule);
 

commit e4043dcf30811f5db15181168e2aac172514302a
Author: Jens Axboe <axboe@fb.com>
Date:   Wed Apr 9 10:18:23 2014 -0600

    blk-mq: ensure that hardware queues are always run on the mapped CPUs
    
    Instead of providing soft mappings with no guarantees on hardware
    queues always being run on the right CPU, switch to a hard mapping
    guarantee that ensure that we always run the hardware queue on
    (one of, if more) the mapped CPU.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 0120451545d8..b6ee48740458 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -19,6 +19,7 @@ struct blk_mq_hw_ctx {
 
 	unsigned long		state;		/* BLK_MQ_S_* flags */
 	struct delayed_work	delayed_work;
+	cpumask_var_t		cpumask;
 
 	unsigned long		flags;		/* BLK_MQ_F_* flags */
 

commit 7a48837732f87a574ee3e1855927dc250117f565
Merge: 1a0b6abaea78 27fbf4e87c16
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 1 19:19:15 2014 -0700

    Merge branch 'for-3.15/core' of git://git.kernel.dk/linux-block
    
    Pull core block layer updates from Jens Axboe:
     "This is the pull request for the core block IO bits for the 3.15
      kernel.  It's a smaller round this time, it contains:
    
       - Various little blk-mq fixes and additions from Christoph and
         myself.
    
       - Cleanup of the IPI usage from the block layer, and associated
         helper code.  From Frederic Weisbecker and Jan Kara.
    
       - Duplicate code cleanup in bio-integrity from Gu Zheng.  This will
         give you a merge conflict, but that should be easy to resolve.
    
       - blk-mq notify spinlock fix for RT from Mike Galbraith.
    
       - A blktrace partial accounting bug fix from Roman Pen.
    
       - Missing REQ_SYNC detection fix for blk-mq from Shaohua Li"
    
    * 'for-3.15/core' of git://git.kernel.dk/linux-block: (25 commits)
      blk-mq: add REQ_SYNC early
      rt,blk,mq: Make blk_mq_cpu_notify_lock a raw spinlock
      blk-mq: support partial I/O completions
      blk-mq: merge blk_mq_insert_request and blk_mq_run_request
      blk-mq: remove blk_mq_alloc_rq
      blk-mq: don't dump CPU -> hw queue map on driver load
      blk-mq: fix wrong usage of hctx->state vs hctx->flags
      blk-mq: allow blk_mq_init_commands() to return failure
      block: remove old blk_iopoll_enabled variable
      blktrace: fix accounting of partially completed requests
      smp: Rename __smp_call_function_single() to smp_call_function_single_async()
      smp: Remove wait argument from __smp_call_function_single()
      watchdog: Simplify a little the IPI call
      smp: Move __smp_call_function_single() below its safe version
      smp: Consolidate the various smp_call_function_single() declensions
      smp: Teach __smp_call_function_single() to check for offline cpus
      smp: Remove unused list_head from csd
      smp: Iterate functions through llist_for_each_entry_safe()
      block: Stop abusing rq->csd.list in blk-softirq
      block: Remove useless IPI struct initialization
      ...

commit eeabc850b79336575da7be3dbe186a2da4de8293
Author: Christoph Hellwig <hch@infradead.org>
Date:   Fri Mar 21 08:57:37 2014 -0600

    blk-mq: merge blk_mq_insert_request and blk_mq_run_request
    
    It's almost identical to blk_mq_insert_request, so fold the two into one
    slightly more generic function by making the flush special case a bit
    smarted.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index bb68b03741be..349c730a579e 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -122,8 +122,7 @@ void blk_mq_free_commands(struct request_queue *, void (*free)(void *data, struc
 
 void blk_mq_flush_plug_list(struct blk_plug *plug, bool from_schedule);
 
-void blk_mq_insert_request(struct request_queue *, struct request *,
-		bool, bool);
+void blk_mq_insert_request(struct request *, bool, bool, bool);
 void blk_mq_run_queues(struct request_queue *q, bool async);
 void blk_mq_free_request(struct request *rq);
 bool blk_mq_can_queue(struct blk_mq_hw_ctx *);

commit 5d12f905cc50c0810628d0deedd478ec2db48659
Author: Jens Axboe <axboe@fb.com>
Date:   Wed Mar 19 15:25:02 2014 -0600

    blk-mq: fix wrong usage of hctx->state vs hctx->flags
    
    BLK_MQ_F_* flags are for hctx->flags, and are non-atomic and
    set at registration time. BLK_MQ_S_* flags are dynamic and
    atomic, and are accessed through hctx->state.
    
    Some of the BLK_MQ_S_STOPPED uses were wrong. Additionally,
    the header file should not use a bit shift for the _S_ flags,
    as they are done through the set/test_bit functions.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 33ff10ebcabb..bb68b03741be 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -109,7 +109,7 @@ enum {
 	BLK_MQ_F_SHOULD_SORT	= 1 << 1,
 	BLK_MQ_F_SHOULD_IPI	= 1 << 2,
 
-	BLK_MQ_S_STOPPED	= 1 << 0,
+	BLK_MQ_S_STOPPED	= 0,
 
 	BLK_MQ_MAX_DEPTH	= 2048,
 };

commit 95363efde193079541cb379eb47140e9c4d355d5
Author: Jens Axboe <axboe@fb.com>
Date:   Fri Mar 14 10:43:15 2014 -0600

    blk-mq: allow blk_mq_init_commands() to return failure
    
    If drivers do dynamic allocation in the hardware command init
    path, then we need to be able to handle and return failures.
    
    And if they do allocations or mappings in the init command path,
    then we need a cleanup function to free up that space at exit
    time. So add blk_mq_free_commands() as the cleanup function.
    
    This is required for the mtip32xx driver conversion to blk-mq.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 18ba8a627f46..33ff10ebcabb 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -117,7 +117,8 @@ enum {
 struct request_queue *blk_mq_init_queue(struct blk_mq_reg *, void *);
 int blk_mq_register_disk(struct gendisk *);
 void blk_mq_unregister_disk(struct gendisk *);
-void blk_mq_init_commands(struct request_queue *, void (*init)(void *data, struct blk_mq_hw_ctx *, struct request *, unsigned int), void *data);
+int blk_mq_init_commands(struct request_queue *, int (*init)(void *data, struct blk_mq_hw_ctx *, struct request *, unsigned int), void *data);
+void blk_mq_free_commands(struct request_queue *, void (*free)(void *data, struct blk_mq_hw_ctx *, struct request *, unsigned int), void *data);
 
 void blk_mq_flush_plug_list(struct blk_plug *plug, bool from_schedule);
 

commit d6a25b31315327eef7785b895c354cc45c3f3742
Author: Christoph Hellwig <hch@infradead.org>
Date:   Thu Feb 20 15:32:38 2014 -0800

    blk-mq: support partial I/O completions
    
    Add a new blk_mq_end_io_partial function to partially complete requests
    as needed by the SCSI layer.  We do this by reusing blk_update_request
    to advance the bio instead of having a simplified version of it in
    the blk-mq code.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index ff28fe37ddda..2ff2e8d982be 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -133,7 +133,13 @@ struct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *, const int ctx_ind
 struct blk_mq_hw_ctx *blk_mq_alloc_single_hw_queue(struct blk_mq_reg *, unsigned int);
 void blk_mq_free_single_hw_queue(struct blk_mq_hw_ctx *, unsigned int);
 
-void blk_mq_end_io(struct request *rq, int error);
+bool blk_mq_end_io_partial(struct request *rq, int error,
+		unsigned int nr_bytes);
+static inline void blk_mq_end_io(struct request *rq, int error)
+{
+	bool done = !blk_mq_end_io_partial(rq, error, blk_rq_bytes(rq));
+	BUG_ON(!done);
+}
 
 void blk_mq_complete_request(struct request *rq);
 

commit feb71dae1f9e0aeb056f7f639a21e620d327fc66
Author: Christoph Hellwig <hch@infradead.org>
Date:   Thu Feb 20 15:32:37 2014 -0800

    blk-mq: merge blk_mq_insert_request and blk_mq_run_request
    
    It's almost identical to blk_mq_insert_request, so fold the two into one
    slightly more generic function by making the flush special case a bit
    smarted.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 18ba8a627f46..ff28fe37ddda 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -121,8 +121,7 @@ void blk_mq_init_commands(struct request_queue *, void (*init)(void *data, struc
 
 void blk_mq_flush_plug_list(struct blk_plug *plug, bool from_schedule);
 
-void blk_mq_insert_request(struct request_queue *, struct request *,
-		bool, bool);
+void blk_mq_insert_request(struct request *, bool, bool, bool);
 void blk_mq_run_queues(struct request_queue *q, bool async);
 void blk_mq_free_request(struct request *rq);
 bool blk_mq_can_queue(struct blk_mq_hw_ctx *);

commit 18741986a4b1dc4b1f171634c4191abc3b0fa023
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Feb 10 09:29:00 2014 -0700

    blk-mq: rework flush sequencing logic
    
    Witch to using a preallocated flush_rq for blk-mq similar to what's done
    with the old request path.  This allows us to set up the request properly
    with a tag from the actually allowed range and ->rq_disk as needed by
    some drivers.  To make life easier we also switch to dynamic allocation
    of ->flush_rq for the old path.
    
    This effectively reverts most of
    
        "blk-mq: fix for flush deadlock"
    
    and
    
        "blk-mq: Don't reserve a tag for flush request"
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 468be242db90..18ba8a627f46 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -36,15 +36,12 @@ struct blk_mq_hw_ctx {
 	struct list_head	page_list;
 	struct blk_mq_tags	*tags;
 
-	atomic_t		pending_flush;
-
 	unsigned long		queued;
 	unsigned long		run;
 #define BLK_MQ_MAX_DISPATCH_ORDER	10
 	unsigned long		dispatched[BLK_MQ_MAX_DISPATCH_ORDER];
 
 	unsigned int		queue_depth;
-	unsigned int		reserved_tags;
 	unsigned int		numa_node;
 	unsigned int		cmd_size;	/* per-request extra data */
 
@@ -129,7 +126,7 @@ void blk_mq_insert_request(struct request_queue *, struct request *,
 void blk_mq_run_queues(struct request_queue *q, bool async);
 void blk_mq_free_request(struct request *rq);
 bool blk_mq_can_queue(struct blk_mq_hw_ctx *);
-struct request *blk_mq_alloc_request(struct request_queue *q, int rw, gfp_t gfp, bool reserved);
+struct request *blk_mq_alloc_request(struct request_queue *q, int rw, gfp_t gfp);
 struct request *blk_mq_alloc_reserved_request(struct request_queue *q, int rw, gfp_t gfp);
 struct request *blk_mq_rq_from_tag(struct request_queue *q, unsigned int tag);
 

commit 30a91cb4ef385fe1b260df204ef314d86fff2850
Author: Christoph Hellwig <hch@infradead.org>
Date:   Mon Feb 10 03:24:38 2014 -0800

    blk-mq: rework I/O completions
    
    Rework I/O completions to work more like the old code path.  blk_mq_end_io
    now stays out of the business of deferring completions to others CPUs
    and calling blk_mark_rq_complete.  The latter is very important to allow
    completing requests that have timed out and thus are already marked completed,
    the former allows using the IPI callout even for driver specific completions
    instead of having to reimplement them.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index b7638be58599..468be242db90 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -86,6 +86,8 @@ struct blk_mq_ops {
 	 */
 	rq_timed_out_fn		*timeout;
 
+	softirq_done_fn		*complete;
+
 	/*
 	 * Override for hctx allocations (should probably go)
 	 */
@@ -137,6 +139,8 @@ void blk_mq_free_single_hw_queue(struct blk_mq_hw_ctx *, unsigned int);
 
 void blk_mq_end_io(struct request *rq, int error);
 
+void blk_mq_complete_request(struct request *rq);
+
 void blk_mq_stop_hw_queue(struct blk_mq_hw_ctx *hctx);
 void blk_mq_start_hw_queue(struct blk_mq_hw_ctx *hctx);
 void blk_mq_stop_hw_queues(struct request_queue *q);

commit 72a0a36e2854a6eadb4cf2561858f613f9cd4639
Author: Christoph Hellwig <hch@infradead.org>
Date:   Fri Feb 7 10:22:36 2014 -0800

    blk-mq: support at_head inserations for blk_execute_rq
    
    This is neede for proper SG_IO operation as well as various uses of
    blk_execute_rq from the SCSI midlayer.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 1e8f16f65af4..b7638be58599 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -122,7 +122,8 @@ void blk_mq_init_commands(struct request_queue *, void (*init)(void *data, struc
 
 void blk_mq_flush_plug_list(struct blk_plug *plug, bool from_schedule);
 
-void blk_mq_insert_request(struct request_queue *, struct request *, bool);
+void blk_mq_insert_request(struct request_queue *, struct request *,
+		bool, bool);
 void blk_mq_run_queues(struct request_queue *q, bool async);
 void blk_mq_free_request(struct request *rq);
 bool blk_mq_can_queue(struct blk_mq_hw_ctx *);

commit f0276924fa35a3607920a58cf5d878212824b951
Author: Shaohua Li <shli@kernel.org>
Date:   Tue Dec 31 11:38:50 2013 +0800

    blk-mq: Don't reserve a tag for flush request
    
    Reserving a tag (request) for flush to avoid dead lock is a overkill. A
    tag is valuable resource. We can track the number of flush requests and
    disallow having too many pending flush requests allocated. With this
    patch, blk_mq_alloc_request_pinned() could do a busy nop (but not a dead
    loop) if too many pending requests are allocated and new flush request
    is allocated. But this should not be a problem, too many pending flush
    requests are very rare case.
    
    I verified this can fix the deadlock caused by too many pending flush
    requests.
    
    Signed-off-by: Shaohua Li <shli@fusionio.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 161b23105b1e..1e8f16f65af4 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -36,12 +36,15 @@ struct blk_mq_hw_ctx {
 	struct list_head	page_list;
 	struct blk_mq_tags	*tags;
 
+	atomic_t		pending_flush;
+
 	unsigned long		queued;
 	unsigned long		run;
 #define BLK_MQ_MAX_DISPATCH_ORDER	10
 	unsigned long		dispatched[BLK_MQ_MAX_DISPATCH_ORDER];
 
 	unsigned int		queue_depth;
+	unsigned int		reserved_tags;
 	unsigned int		numa_node;
 	unsigned int		cmd_size;	/* per-request extra data */
 

commit 0d0b7d427987f6e98b6f32e84ee071f36f85c3d4
Author: Jose Alonso <joalonsof@gmail.com>
Date:   Tue Jan 28 08:09:46 2014 -0700

    blk-mq: for_each_* macro correctness
    
    I observed that there are for_each macros that do an extra memory access
    beyond the defined area.
    Normally this does not cause problems.
    But, this can cause exceptions. For example: if the area is allocated at
    the end of a page and the next page is not accessible.
    
    For correctness, I suggest changing the arguments of the 'for loop' like
    others 'for_each' do in the kernel.
    
    Signed-off-by: Jose Alonso <joalonsof@gmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 851d34b7ac26..161b23105b1e 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -158,16 +158,16 @@ static inline struct request *blk_mq_tag_to_rq(struct blk_mq_hw_ctx *hctx,
 }
 
 #define queue_for_each_hw_ctx(q, hctx, i)				\
-	for ((i) = 0, hctx = (q)->queue_hw_ctx[0];			\
-	     (i) < (q)->nr_hw_queues; (i)++, hctx = (q)->queue_hw_ctx[i])
+	for ((i) = 0; (i) < (q)->nr_hw_queues &&			\
+	     ({ hctx = (q)->queue_hw_ctx[i]; 1; }); (i)++)
 
 #define queue_for_each_ctx(q, ctx, i)					\
-	for ((i) = 0, ctx = per_cpu_ptr((q)->queue_ctx, 0);		\
-	     (i) < (q)->nr_queues; (i)++, ctx = per_cpu_ptr(q->queue_ctx, (i)))
+	for ((i) = 0; (i) < (q)->nr_queues &&				\
+	     ({ ctx = per_cpu_ptr((q)->queue_ctx, (i)); 1; }); (i)++)
 
 #define hctx_for_each_ctx(hctx, ctx, i)					\
-	for ((i) = 0, ctx = (hctx)->ctxs[0];				\
-	     (i) < (hctx)->nr_ctx; (i)++, ctx = (hctx)->ctxs[(i)])
+	for ((i) = 0; (i) < (hctx)->nr_ctx &&				\
+	     ({ ctx = (hctx)->ctxs[(i)]; 1; }); (i)++)
 
 #define blk_ctx_sum(q, sum)						\
 ({									\

commit 3edcc0ce85c59d45d6dfc6a36a6b3f8b31ba9887
Author: Ming Lei <tom.leiming@gmail.com>
Date:   Thu Dec 26 21:31:38 2013 +0800

    block: blk-mq: don't export blk_mq_free_queue()
    
    blk_mq_free_queue() is called from release handler of
    queue kobject, so it needn't be called from drivers.
    
    Cc: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Ming Lei <tom.leiming@gmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index ab0e9b2025b3..851d34b7ac26 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -113,7 +113,6 @@ enum {
 };
 
 struct request_queue *blk_mq_init_queue(struct blk_mq_reg *, void *);
-void blk_mq_free_queue(struct request_queue *);
 int blk_mq_register_disk(struct gendisk *);
 void blk_mq_unregister_disk(struct gendisk *);
 void blk_mq_init_commands(struct request_queue *, void (*init)(void *data, struct blk_mq_hw_ctx *, struct request *, unsigned int), void *data);

commit 3228f48be2d19b2dd90db96ec16a40187a2946f3
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Oct 28 13:33:58 2013 -0600

    blk-mq: fix for flush deadlock
    
    The flush state machine takes in a struct request, which then is
    submitted multiple times to the underling driver.  The old block code
    requeses the same request for each of those, so it does not have an
    issue with tapping into the request pool.  The new one on the other hand
    allocates a new request for each of the actualy steps of the flush
    sequence. If have already allocated all of the tags for IO, we will
    fail allocating the flush request.
    
    Set aside a reserved request just for flushes.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 3368b97bee73..ab0e9b2025b3 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -124,7 +124,7 @@ void blk_mq_insert_request(struct request_queue *, struct request *, bool);
 void blk_mq_run_queues(struct request_queue *q, bool async);
 void blk_mq_free_request(struct request *rq);
 bool blk_mq_can_queue(struct blk_mq_hw_ctx *);
-struct request *blk_mq_alloc_request(struct request_queue *q, int rw, gfp_t gfp);
+struct request *blk_mq_alloc_request(struct request_queue *q, int rw, gfp_t gfp, bool reserved);
 struct request *blk_mq_alloc_reserved_request(struct request_queue *q, int rw, gfp_t gfp);
 struct request *blk_mq_rq_from_tag(struct request_queue *q, unsigned int tag);
 

commit 280d45f6c35d8d7a0fe20c36caf426e3ac139cf9
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Oct 25 14:45:58 2013 +0100

    blk-mq: add blk_mq_stop_hw_queues
    
    Add a helper to iterate over all hw queues and stop them.  This is useful
    for driver that implement PM suspend functionality.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    
    Modified to just call blk_mq_stop_hw_queue() by Jens.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 746042ff321a..3368b97bee73 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -136,6 +136,7 @@ void blk_mq_end_io(struct request *rq, int error);
 
 void blk_mq_stop_hw_queue(struct blk_mq_hw_ctx *hctx);
 void blk_mq_start_hw_queue(struct blk_mq_hw_ctx *hctx);
+void blk_mq_stop_hw_queues(struct request_queue *q);
 void blk_mq_start_stopped_hw_queues(struct request_queue *q);
 
 /*

commit 320ae51feed5c2f13664aa05a76bec198967e04d
Author: Jens Axboe <axboe@kernel.dk>
Date:   Thu Oct 24 09:20:05 2013 +0100

    blk-mq: new multi-queue block IO queueing mechanism
    
    Linux currently has two models for block devices:
    
    - The classic request_fn based approach, where drivers use struct
      request units for IO. The block layer provides various helper
      functionalities to let drivers share code, things like tag
      management, timeout handling, queueing, etc.
    
    - The "stacked" approach, where a driver squeezes in between the
      block layer and IO submitter. Since this bypasses the IO stack,
      driver generally have to manage everything themselves.
    
    With drivers being written for new high IOPS devices, the classic
    request_fn based driver doesn't work well enough. The design dates
    back to when both SMP and high IOPS was rare. It has problems with
    scaling to bigger machines, and runs into scaling issues even on
    smaller machines when you have IOPS in the hundreds of thousands
    per device.
    
    The stacked approach is then most often selected as the model
    for the driver. But this means that everybody has to re-invent
    everything, and along with that we get all the problems again
    that the shared approach solved.
    
    This commit introduces blk-mq, block multi queue support. The
    design is centered around per-cpu queues for queueing IO, which
    then funnel down into x number of hardware submission queues.
    We might have a 1:1 mapping between the two, or it might be
    an N:M mapping. That all depends on what the hardware supports.
    
    blk-mq provides various helper functions, which include:
    
    - Scalable support for request tagging. Most devices need to
      be able to uniquely identify a request both in the driver and
      to the hardware. The tagging uses per-cpu caches for freed
      tags, to enable cache hot reuse.
    
    - Timeout handling without tracking request on a per-device
      basis. Basically the driver should be able to get a notification,
      if a request happens to fail.
    
    - Optional support for non 1:1 mappings between issue and
      submission queues. blk-mq can redirect IO completions to the
      desired location.
    
    - Support for per-request payloads. Drivers almost always need
      to associate a request structure with some driver private
      command structure. Drivers can tell blk-mq this at init time,
      and then any request handed to the driver will have the
      required size of memory associated with it.
    
    - Support for merging of IO, and plugging. The stacked model
      gets neither of these. Even for high IOPS devices, merging
      sequential IO reduces per-command overhead and thus
      increases bandwidth.
    
    For now, this is provided as a potential 3rd queueing model, with
    the hope being that, as it matures, it can replace both the classic
    and stacked model. That would get us back to having just 1 real
    model for block devices, leaving the stacked approach to dm/md
    devices (as it was originally intended).
    
    Contributions in this patch from the following people:
    
    Shaohua Li <shli@fusionio.com>
    Alexander Gordeev <agordeev@redhat.com>
    Christoph Hellwig <hch@infradead.org>
    Mike Christie <michaelc@cs.wisc.edu>
    Matias Bjorling <m@bjorling.me>
    Jeff Moyer <jmoyer@redhat.com>
    
    Acked-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
new file mode 100644
index 000000000000..746042ff321a
--- /dev/null
+++ b/include/linux/blk-mq.h
@@ -0,0 +1,182 @@
+#ifndef BLK_MQ_H
+#define BLK_MQ_H
+
+#include <linux/blkdev.h>
+
+struct blk_mq_tags;
+
+struct blk_mq_cpu_notifier {
+	struct list_head list;
+	void *data;
+	void (*notify)(void *data, unsigned long action, unsigned int cpu);
+};
+
+struct blk_mq_hw_ctx {
+	struct {
+		spinlock_t		lock;
+		struct list_head	dispatch;
+	} ____cacheline_aligned_in_smp;
+
+	unsigned long		state;		/* BLK_MQ_S_* flags */
+	struct delayed_work	delayed_work;
+
+	unsigned long		flags;		/* BLK_MQ_F_* flags */
+
+	struct request_queue	*queue;
+	unsigned int		queue_num;
+
+	void			*driver_data;
+
+	unsigned int		nr_ctx;
+	struct blk_mq_ctx	**ctxs;
+	unsigned int 		nr_ctx_map;
+	unsigned long		*ctx_map;
+
+	struct request		**rqs;
+	struct list_head	page_list;
+	struct blk_mq_tags	*tags;
+
+	unsigned long		queued;
+	unsigned long		run;
+#define BLK_MQ_MAX_DISPATCH_ORDER	10
+	unsigned long		dispatched[BLK_MQ_MAX_DISPATCH_ORDER];
+
+	unsigned int		queue_depth;
+	unsigned int		numa_node;
+	unsigned int		cmd_size;	/* per-request extra data */
+
+	struct blk_mq_cpu_notifier	cpu_notifier;
+	struct kobject		kobj;
+};
+
+struct blk_mq_reg {
+	struct blk_mq_ops	*ops;
+	unsigned int		nr_hw_queues;
+	unsigned int		queue_depth;
+	unsigned int		reserved_tags;
+	unsigned int		cmd_size;	/* per-request extra data */
+	int			numa_node;
+	unsigned int		timeout;
+	unsigned int		flags;		/* BLK_MQ_F_* */
+};
+
+typedef int (queue_rq_fn)(struct blk_mq_hw_ctx *, struct request *);
+typedef struct blk_mq_hw_ctx *(map_queue_fn)(struct request_queue *, const int);
+typedef struct blk_mq_hw_ctx *(alloc_hctx_fn)(struct blk_mq_reg *,unsigned int);
+typedef void (free_hctx_fn)(struct blk_mq_hw_ctx *, unsigned int);
+typedef int (init_hctx_fn)(struct blk_mq_hw_ctx *, void *, unsigned int);
+typedef void (exit_hctx_fn)(struct blk_mq_hw_ctx *, unsigned int);
+
+struct blk_mq_ops {
+	/*
+	 * Queue request
+	 */
+	queue_rq_fn		*queue_rq;
+
+	/*
+	 * Map to specific hardware queue
+	 */
+	map_queue_fn		*map_queue;
+
+	/*
+	 * Called on request timeout
+	 */
+	rq_timed_out_fn		*timeout;
+
+	/*
+	 * Override for hctx allocations (should probably go)
+	 */
+	alloc_hctx_fn		*alloc_hctx;
+	free_hctx_fn		*free_hctx;
+
+	/*
+	 * Called when the block layer side of a hardware queue has been
+	 * set up, allowing the driver to allocate/init matching structures.
+	 * Ditto for exit/teardown.
+	 */
+	init_hctx_fn		*init_hctx;
+	exit_hctx_fn		*exit_hctx;
+};
+
+enum {
+	BLK_MQ_RQ_QUEUE_OK	= 0,	/* queued fine */
+	BLK_MQ_RQ_QUEUE_BUSY	= 1,	/* requeue IO for later */
+	BLK_MQ_RQ_QUEUE_ERROR	= 2,	/* end IO with error */
+
+	BLK_MQ_F_SHOULD_MERGE	= 1 << 0,
+	BLK_MQ_F_SHOULD_SORT	= 1 << 1,
+	BLK_MQ_F_SHOULD_IPI	= 1 << 2,
+
+	BLK_MQ_S_STOPPED	= 1 << 0,
+
+	BLK_MQ_MAX_DEPTH	= 2048,
+};
+
+struct request_queue *blk_mq_init_queue(struct blk_mq_reg *, void *);
+void blk_mq_free_queue(struct request_queue *);
+int blk_mq_register_disk(struct gendisk *);
+void blk_mq_unregister_disk(struct gendisk *);
+void blk_mq_init_commands(struct request_queue *, void (*init)(void *data, struct blk_mq_hw_ctx *, struct request *, unsigned int), void *data);
+
+void blk_mq_flush_plug_list(struct blk_plug *plug, bool from_schedule);
+
+void blk_mq_insert_request(struct request_queue *, struct request *, bool);
+void blk_mq_run_queues(struct request_queue *q, bool async);
+void blk_mq_free_request(struct request *rq);
+bool blk_mq_can_queue(struct blk_mq_hw_ctx *);
+struct request *blk_mq_alloc_request(struct request_queue *q, int rw, gfp_t gfp);
+struct request *blk_mq_alloc_reserved_request(struct request_queue *q, int rw, gfp_t gfp);
+struct request *blk_mq_rq_from_tag(struct request_queue *q, unsigned int tag);
+
+struct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *, const int ctx_index);
+struct blk_mq_hw_ctx *blk_mq_alloc_single_hw_queue(struct blk_mq_reg *, unsigned int);
+void blk_mq_free_single_hw_queue(struct blk_mq_hw_ctx *, unsigned int);
+
+void blk_mq_end_io(struct request *rq, int error);
+
+void blk_mq_stop_hw_queue(struct blk_mq_hw_ctx *hctx);
+void blk_mq_start_hw_queue(struct blk_mq_hw_ctx *hctx);
+void blk_mq_start_stopped_hw_queues(struct request_queue *q);
+
+/*
+ * Driver command data is immediately after the request. So subtract request
+ * size to get back to the original request.
+ */
+static inline struct request *blk_mq_rq_from_pdu(void *pdu)
+{
+	return pdu - sizeof(struct request);
+}
+static inline void *blk_mq_rq_to_pdu(struct request *rq)
+{
+	return (void *) rq + sizeof(*rq);
+}
+
+static inline struct request *blk_mq_tag_to_rq(struct blk_mq_hw_ctx *hctx,
+					       unsigned int tag)
+{
+	return hctx->rqs[tag];
+}
+
+#define queue_for_each_hw_ctx(q, hctx, i)				\
+	for ((i) = 0, hctx = (q)->queue_hw_ctx[0];			\
+	     (i) < (q)->nr_hw_queues; (i)++, hctx = (q)->queue_hw_ctx[i])
+
+#define queue_for_each_ctx(q, ctx, i)					\
+	for ((i) = 0, ctx = per_cpu_ptr((q)->queue_ctx, 0);		\
+	     (i) < (q)->nr_queues; (i)++, ctx = per_cpu_ptr(q->queue_ctx, (i)))
+
+#define hctx_for_each_ctx(hctx, ctx, i)					\
+	for ((i) = 0, ctx = (hctx)->ctxs[0];				\
+	     (i) < (hctx)->nr_ctx; (i)++, ctx = (hctx)->ctxs[(i)])
+
+#define blk_ctx_sum(q, sum)						\
+({									\
+	struct blk_mq_ctx *__x;						\
+	unsigned int __ret = 0, __i;					\
+									\
+	queue_for_each_ctx((q), __x, __i)				\
+		__ret += sum;						\
+	__ret;								\
+})
+
+#endif
