commit 567f6a6eba0c09e5f502e0290e57651befa8aacb
Author: Nicolas Saenz Julienne <nsaenzjulienne@suse.de>
Date:   Tue Jul 14 14:39:25 2020 +0200

    dma-direct: provide function to check physical memory area validity
    
    dma_coherent_ok() checks if a physical memory area fits a device's DMA
    constraints.
    
    Signed-off-by: Nicolas Saenz Julienne <nsaenzjulienne@suse.de>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/dma-direct.h b/include/linux/dma-direct.h
index 5184735a0fe8..ab2e20cba951 100644
--- a/include/linux/dma-direct.h
+++ b/include/linux/dma-direct.h
@@ -69,6 +69,7 @@ static inline bool dma_capable(struct device *dev, dma_addr_t addr, size_t size,
 u64 dma_direct_get_required_mask(struct device *dev);
 gfp_t dma_direct_optimal_gfp_mask(struct device *dev, u64 dma_mask,
 				  u64 *phys_mask);
+bool dma_coherent_ok(struct device *dev, phys_addr_t phys, size_t size);
 void *dma_direct_alloc(struct device *dev, size_t size, dma_addr_t *dma_handle,
 		gfp_t gfp, unsigned long attrs);
 void dma_direct_free(struct device *dev, size_t size, void *cpu_addr,

commit 5a764898afec0bc097003e8c3e727792289f76d6
Merge: 9321f1aaf63e 1195c7cebb95
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jul 10 18:16:22 2020 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net
    
    Pull networking fixes from David Miller:
    
     1) Restore previous behavior of CAP_SYS_ADMIN wrt loading networking
        BPF programs, from Maciej Żenczykowski.
    
     2) Fix dropped broadcasts in mac80211 code, from Seevalamuthu
        Mariappan.
    
     3) Slay memory leak in nl80211 bss color attribute parsing code, from
        Luca Coelho.
    
     4) Get route from skb properly in ip_route_use_hint(), from Miaohe Lin.
    
     5) Don't allow anything other than ARPHRD_ETHER in llc code, from Eric
        Dumazet.
    
     6) xsk code dips too deeply into DMA mapping implementation internals.
        Add dma_need_sync and use it. From Christoph Hellwig
    
     7) Enforce power-of-2 for BPF ringbuf sizes. From Andrii Nakryiko.
    
     8) Check for disallowed attributes when loading flow dissector BPF
        programs. From Lorenz Bauer.
    
     9) Correct packet injection to L3 tunnel devices via AF_PACKET, from
        Jason A. Donenfeld.
    
    10) Don't advertise checksum offload on ipa devices that don't support
        it. From Alex Elder.
    
    11) Resolve several issues in TCP MD5 signature support. Missing memory
        barriers, bogus options emitted when using syncookies, and failure
        to allow md5 key changes in established states. All from Eric
        Dumazet.
    
    12) Fix interface leak in hsr code, from Taehee Yoo.
    
    13) VF reset fixes in hns3 driver, from Huazhong Tan.
    
    14) Make loopback work again with ipv6 anycast, from David Ahern.
    
    15) Fix TX starvation under high load in fec driver, from Tobias
        Waldekranz.
    
    16) MLD2 payload lengths not checked properly in bridge multicast code,
        from Linus Lüssing.
    
    17) Packet scheduler code that wants to find the inner protocol
        currently only works for one level of VLAN encapsulation. Allow
        Q-in-Q situations to work properly here, from Toke
        Høiland-Jørgensen.
    
    18) Fix route leak in l2tp, from Xin Long.
    
    19) Resolve conflict between the sk->sk_user_data usage of bpf reuseport
        support and various protocols. From Martin KaFai Lau.
    
    20) Fix socket cgroup v2 reference counting in some situations, from
        Cong Wang.
    
    21) Cure memory leak in mlx5 connection tracking offload support, from
        Eli Britstein.
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net: (146 commits)
      mlxsw: pci: Fix use-after-free in case of failed devlink reload
      mlxsw: spectrum_router: Remove inappropriate usage of WARN_ON()
      net: macb: fix call to pm_runtime in the suspend/resume functions
      net: macb: fix macb_suspend() by removing call to netif_carrier_off()
      net: macb: fix macb_get/set_wol() when moving to phylink
      net: macb: mark device wake capable when "magic-packet" property present
      net: macb: fix wakeup test in runtime suspend/resume routines
      bnxt_en: fix NULL dereference in case SR-IOV configuration fails
      libbpf: Fix libbpf hashmap on (I)LP32 architectures
      net/mlx5e: CT: Fix memory leak in cleanup
      net/mlx5e: Fix port buffers cell size value
      net/mlx5e: Fix 50G per lane indication
      net/mlx5e: Fix CPU mapping after function reload to avoid aRFS RX crash
      net/mlx5e: Fix VXLAN configuration restore after function reload
      net/mlx5e: Fix usage of rcu-protected pointer
      net/mxl5e: Verify that rpriv is not NULL
      net/mlx5: E-Switch, Fix vlan or qos setting in legacy mode
      net/mlx5: Fix eeprom support for SFP module
      cgroup: Fix sock_cgroup_data on big-endian.
      selftests: bpf: Fix detach from sockmap tests
      ...

commit 3aa91625007807bfca4155df1867a5c924a08662
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Jun 29 15:03:56 2020 +0200

    dma-mapping: Add a new dma_need_sync API
    
    Add a new API to check if calls to dma_sync_single_for_{device,cpu} are
    required for a given DMA streaming mapping.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Link: https://lore.kernel.org/bpf/20200629130359.2690853-2-hch@lst.de

diff --git a/include/linux/dma-direct.h b/include/linux/dma-direct.h
index 136f984df0d9..8b006730687b 100644
--- a/include/linux/dma-direct.h
+++ b/include/linux/dma-direct.h
@@ -87,4 +87,5 @@ int dma_direct_mmap(struct device *dev, struct vm_area_struct *vma,
 		void *cpu_addr, dma_addr_t dma_addr, size_t size,
 		unsigned long attrs);
 int dma_direct_supported(struct device *dev, u64 mask);
+bool dma_direct_need_sync(struct device *dev, dma_addr_t dma_addr);
 #endif /* _LINUX_DMA_DIRECT_H */

commit 26749b3201ab05e288fbf78fbc8585dfa2da3218
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Jun 15 08:52:31 2020 +0200

    dma-direct: mark __dma_direct_alloc_pages static
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/dma-direct.h b/include/linux/dma-direct.h
index 136f984df0d9..cdfa400f89b3 100644
--- a/include/linux/dma-direct.h
+++ b/include/linux/dma-direct.h
@@ -77,8 +77,6 @@ void *dma_direct_alloc_pages(struct device *dev, size_t size,
 		dma_addr_t *dma_handle, gfp_t gfp, unsigned long attrs);
 void dma_direct_free_pages(struct device *dev, size_t size, void *cpu_addr,
 		dma_addr_t dma_addr, unsigned long attrs);
-struct page *__dma_direct_alloc_pages(struct device *dev, size_t size,
-		gfp_t gfp, unsigned long attrs);
 int dma_direct_get_sgtable(struct device *dev, struct sg_table *sgt,
 		void *cpu_addr, dma_addr_t dma_addr, size_t size,
 		unsigned long attrs);

commit c84dc6e68a1d2464e050d9694be4e4ff49e32bfd
Author: David Rientjes <rientjes@google.com>
Date:   Tue Apr 14 17:04:55 2020 -0700

    dma-pool: add additional coherent pools to map to gfp mask
    
    The single atomic pool is allocated from the lowest zone possible since
    it is guaranteed to be applicable for any DMA allocation.
    
    Devices may allocate through the DMA API but not have a strict reliance
    on GFP_DMA memory.  Since the atomic pool will be used for all
    non-blockable allocations, returning all memory from ZONE_DMA may
    unnecessarily deplete the zone.
    
    Provision for multiple atomic pools that will map to the optimal gfp
    mask of the device.
    
    When allocating non-blockable memory, determine the optimal gfp mask of
    the device and use the appropriate atomic pool.
    
    The coherent DMA mask will remain the same between allocation and free
    and, thus, memory will be freed to the same atomic pool it was allocated
    from.
    
    __dma_atomic_pool_init() will be changed to return struct gen_pool *
    later once dynamic expansion is added.
    
    Signed-off-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/dma-direct.h b/include/linux/dma-direct.h
index 24b8684aa21d..136f984df0d9 100644
--- a/include/linux/dma-direct.h
+++ b/include/linux/dma-direct.h
@@ -67,6 +67,8 @@ static inline bool dma_capable(struct device *dev, dma_addr_t addr, size_t size,
 }
 
 u64 dma_direct_get_required_mask(struct device *dev);
+gfp_t dma_direct_optimal_gfp_mask(struct device *dev, u64 dma_mask,
+				  u64 *phys_mask);
 void *dma_direct_alloc(struct device *dev, size_t size, dma_addr_t *dma_handle,
 		gfp_t gfp, unsigned long attrs);
 void dma_direct_free(struct device *dev, size_t size, void *cpu_addr,

commit a7ba70f1787f977f970cd116076c6fce4b9e01cc
Author: Nicolas Saenz Julienne <nsaenzjulienne@suse.de>
Date:   Thu Nov 21 10:26:44 2019 +0100

    dma-mapping: treat dev->bus_dma_mask as a DMA limit
    
    Using a mask to represent bus DMA constraints has a set of limitations.
    The biggest one being it can only hold a power of two (minus one). The
    DMA mapping code is already aware of this and treats dev->bus_dma_mask
    as a limit. This quirk is already used by some architectures although
    still rare.
    
    With the introduction of the Raspberry Pi 4 we've found a new contender
    for the use of bus DMA limits, as its PCIe bus can only address the
    lower 3GB of memory (of a total of 4GB). This is impossible to represent
    with a mask. To make things worse the device-tree code rounds non power
    of two bus DMA limits to the next power of two, which is unacceptable in
    this case.
    
    In the light of this, rename dev->bus_dma_mask to dev->bus_dma_limit all
    over the tree and treat it as such. Note that dev->bus_dma_limit should
    contain the higher accessible DMA address.
    
    Signed-off-by: Nicolas Saenz Julienne <nsaenzjulienne@suse.de>
    Reviewed-by: Robin Murphy <robin.murphy@arm.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/dma-direct.h b/include/linux/dma-direct.h
index 452f5280cde3..24b8684aa21d 100644
--- a/include/linux/dma-direct.h
+++ b/include/linux/dma-direct.h
@@ -63,7 +63,7 @@ static inline bool dma_capable(struct device *dev, dma_addr_t addr, size_t size,
 	    min(addr, end) < phys_to_dma(dev, PFN_PHYS(min_low_pfn)))
 		return false;
 
-	return end <= min_not_zero(*dev->dma_mask, dev->bus_dma_mask);
+	return end <= min_not_zero(*dev->dma_mask, dev->bus_dma_limit);
 }
 
 u64 dma_direct_get_required_mask(struct device *dev);

commit d7293f79caea45c50c0ab4294847e7af96501ced
Merge: 68a33b179466 bff3b04460a8
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Nov 21 18:13:03 2019 +0100

    Merge branch 'for-next/zone-dma' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux into dma-mapping-for-next
    
    Pull in a stable branch from the arm64 tree that adds the zone_dma_bits
    variable to avoid creating hard to resolve conflicts with that addition.

commit 68a33b1794665ba8a1d1ef1d3bfcc7c587d380a6
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Nov 19 17:38:58 2019 +0100

    dma-direct: exclude dma_direct_map_resource from the min_low_pfn check
    
    The valid memory address check in dma_capable only makes sense when mapping
    normal memory, not when using dma_map_resource to map a device resource.
    Add a new boolean argument to dma_capable to exclude that check for the
    dma_map_resource case.
    
    Fixes: b12d66278dd6 ("dma-direct: check for overflows on 32 bit DMA addresses")
    Reported-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Tested-by: Marek Szyprowski <m.szyprowski@samsung.com>

diff --git a/include/linux/dma-direct.h b/include/linux/dma-direct.h
index f8959f75e496..99b77dd5f79b 100644
--- a/include/linux/dma-direct.h
+++ b/include/linux/dma-direct.h
@@ -49,14 +49,15 @@ static inline phys_addr_t dma_to_phys(struct device *dev, dma_addr_t daddr)
 	return __sme_clr(__dma_to_phys(dev, daddr));
 }
 
-static inline bool dma_capable(struct device *dev, dma_addr_t addr, size_t size)
+static inline bool dma_capable(struct device *dev, dma_addr_t addr, size_t size,
+		bool is_ram)
 {
 	dma_addr_t end = addr + size - 1;
 
 	if (!dev->dma_mask)
 		return false;
 
-	if (!IS_ENABLED(CONFIG_ARCH_DMA_ADDR_T_64BIT) &&
+	if (is_ram && !IS_ENABLED(CONFIG_ARCH_DMA_ADDR_T_64BIT) &&
 	    min(addr, end) < phys_to_dma(dev, PFN_PHYS(min_low_pfn)))
 		return false;
 

commit c7345159f7db6fb69ec1c3b3f8f28cd05c731be2
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Nov 12 17:07:43 2019 +0100

    dma-direct: avoid a forward declaration for phys_to_dma
    
    Move dma_capable down a bit so that we don't need a forward declaration
    for phys_to_dma.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Nicolas Saenz Julienne <nsaenzjulienne@suse.de>

diff --git a/include/linux/dma-direct.h b/include/linux/dma-direct.h
index 991f8aa2676e..f8959f75e496 100644
--- a/include/linux/dma-direct.h
+++ b/include/linux/dma-direct.h
@@ -6,8 +6,6 @@
 #include <linux/memblock.h> /* for min_low_pfn */
 #include <linux/mem_encrypt.h>
 
-static inline dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr);
-
 #ifdef CONFIG_ARCH_HAS_PHYS_TO_DMA
 #include <asm/dma-direct.h>
 #else
@@ -26,20 +24,6 @@ static inline phys_addr_t __dma_to_phys(struct device *dev, dma_addr_t dev_addr)
 }
 #endif /* !CONFIG_ARCH_HAS_PHYS_TO_DMA */
 
-static inline bool dma_capable(struct device *dev, dma_addr_t addr, size_t size)
-{
-	dma_addr_t end = addr + size - 1;
-
-	if (!dev->dma_mask)
-		return false;
-
-	if (!IS_ENABLED(CONFIG_ARCH_DMA_ADDR_T_64BIT) &&
-	    min(addr, end) < phys_to_dma(dev, PFN_PHYS(min_low_pfn)))
-		return false;
-
-	return end <= min_not_zero(*dev->dma_mask, dev->bus_dma_mask);
-}
-
 #ifdef CONFIG_ARCH_HAS_FORCE_DMA_UNENCRYPTED
 bool force_dma_unencrypted(struct device *dev);
 #else
@@ -65,6 +49,20 @@ static inline phys_addr_t dma_to_phys(struct device *dev, dma_addr_t daddr)
 	return __sme_clr(__dma_to_phys(dev, daddr));
 }
 
+static inline bool dma_capable(struct device *dev, dma_addr_t addr, size_t size)
+{
+	dma_addr_t end = addr + size - 1;
+
+	if (!dev->dma_mask)
+		return false;
+
+	if (!IS_ENABLED(CONFIG_ARCH_DMA_ADDR_T_64BIT) &&
+	    min(addr, end) < phys_to_dma(dev, PFN_PHYS(min_low_pfn)))
+		return false;
+
+	return end <= min_not_zero(*dev->dma_mask, dev->bus_dma_mask);
+}
+
 u64 dma_direct_get_required_mask(struct device *dev);
 void *dma_direct_alloc(struct device *dev, size_t size, dma_addr_t *dma_handle,
 		gfp_t gfp, unsigned long attrs);

commit 130c1ccbf55330b55e82612a6e54eebb82c9d746
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Nov 12 17:06:04 2019 +0100

    dma-direct: unify the dma_capable definitions
    
    Currently each architectures that wants to override dma_to_phys and
    phys_to_dma also has to provide dma_capable.  But there isn't really
    any good reason for that.  powerpc and mips just have copies of the
    generic one minus the latests fix, and the arm one was the inspiration
    for said fix, but misses the bus_dma_mask handling.
    Make all architectures use the generic version instead.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Michael Ellerman <mpe@ellerman.id.au> (powerpc)
    Reviewed-by: Nicolas Saenz Julienne <nsaenzjulienne@suse.de>

diff --git a/include/linux/dma-direct.h b/include/linux/dma-direct.h
index 6db863c3eb93..991f8aa2676e 100644
--- a/include/linux/dma-direct.h
+++ b/include/linux/dma-direct.h
@@ -24,6 +24,7 @@ static inline phys_addr_t __dma_to_phys(struct device *dev, dma_addr_t dev_addr)
 
 	return paddr + ((phys_addr_t)dev->dma_pfn_offset << PAGE_SHIFT);
 }
+#endif /* !CONFIG_ARCH_HAS_PHYS_TO_DMA */
 
 static inline bool dma_capable(struct device *dev, dma_addr_t addr, size_t size)
 {
@@ -38,7 +39,6 @@ static inline bool dma_capable(struct device *dev, dma_addr_t addr, size_t size)
 
 	return end <= min_not_zero(*dev->dma_mask, dev->bus_dma_mask);
 }
-#endif /* !CONFIG_ARCH_HAS_PHYS_TO_DMA */
 
 #ifdef CONFIG_ARCH_HAS_FORCE_DMA_UNENCRYPTED
 bool force_dma_unencrypted(struct device *dev);

commit b12d66278dd627cbe1ea7c000aa4715aaf8830c8
Author: Nicolas Saenz Julienne <nsaenzjulienne@suse.de>
Date:   Thu Nov 7 16:06:44 2019 +0100

    dma-direct: check for overflows on 32 bit DMA addresses
    
    As seen on the new Raspberry Pi 4 and sta2x11's DMA implementation it is
    possible for a device configured with 32 bit DMA addresses and a partial
    DMA mapping located at the end of the address space to overflow. It
    happens when a higher physical address, not DMAable, is translated to
    it's DMA counterpart.
    
    For example the Raspberry Pi 4, configurable up to 4 GB of memory, has
    an interconnect capable of addressing the lower 1 GB of physical memory
    with a DMA offset of 0xc0000000. It transpires that, any attempt to
    translate physical addresses higher than the first GB will result in an
    overflow which dma_capable() can't detect as it only checks for
    addresses bigger then the maximum allowed DMA address.
    
    Fix this by verifying in dma_capable() if the DMA address range provided
    is at any point lower than the minimum possible DMA address on the bus.
    
    Signed-off-by: Nicolas Saenz Julienne <nsaenzjulienne@suse.de>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/dma-direct.h b/include/linux/dma-direct.h
index bcd953fb1f5a..6db863c3eb93 100644
--- a/include/linux/dma-direct.h
+++ b/include/linux/dma-direct.h
@@ -3,8 +3,11 @@
 #define _LINUX_DMA_DIRECT_H 1
 
 #include <linux/dma-mapping.h>
+#include <linux/memblock.h> /* for min_low_pfn */
 #include <linux/mem_encrypt.h>
 
+static inline dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr);
+
 #ifdef CONFIG_ARCH_HAS_PHYS_TO_DMA
 #include <asm/dma-direct.h>
 #else
@@ -24,11 +27,16 @@ static inline phys_addr_t __dma_to_phys(struct device *dev, dma_addr_t dev_addr)
 
 static inline bool dma_capable(struct device *dev, dma_addr_t addr, size_t size)
 {
+	dma_addr_t end = addr + size - 1;
+
 	if (!dev->dma_mask)
 		return false;
 
-	return addr + size - 1 <=
-		min_not_zero(*dev->dma_mask, dev->bus_dma_mask);
+	if (!IS_ENABLED(CONFIG_ARCH_DMA_ADDR_T_64BIT) &&
+	    min(addr, end) < phys_to_dma(dev, PFN_PHYS(min_low_pfn)))
+		return false;
+
+	return end <= min_not_zero(*dev->dma_mask, dev->bus_dma_mask);
 }
 #endif /* !CONFIG_ARCH_HAS_PHYS_TO_DMA */
 

commit 34dc0ea6bc960f1f57b2148f01a3f4da23f87013
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Oct 29 11:01:37 2019 +0100

    dma-direct: provide mmap and get_sgtable method overrides
    
    For dma-direct we know that the DMA address is an encoding of the
    physical address that we can trivially decode.  Use that fact to
    provide implementations that do not need the arch_dma_coherent_to_pfn
    architecture hook.  Note that we still can only support mmap of
    non-coherent memory only if the architecture provides a way to set an
    uncached bit in the page tables.  This must be true for architectures
    that use the generic remap helpers, but other architectures can also
    manually select it.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Max Filippov <jcmvbkbc@gmail.com>

diff --git a/include/linux/dma-direct.h b/include/linux/dma-direct.h
index ff3d5edc44b9..bcd953fb1f5a 100644
--- a/include/linux/dma-direct.h
+++ b/include/linux/dma-direct.h
@@ -68,5 +68,12 @@ void dma_direct_free_pages(struct device *dev, size_t size, void *cpu_addr,
 		dma_addr_t dma_addr, unsigned long attrs);
 struct page *__dma_direct_alloc_pages(struct device *dev, size_t size,
 		gfp_t gfp, unsigned long attrs);
+int dma_direct_get_sgtable(struct device *dev, struct sg_table *sgt,
+		void *cpu_addr, dma_addr_t dma_addr, size_t size,
+		unsigned long attrs);
+bool dma_direct_can_mmap(struct device *dev);
+int dma_direct_mmap(struct device *dev, struct vm_area_struct *vma,
+		void *cpu_addr, dma_addr_t dma_addr, size_t size,
+		unsigned long attrs);
 int dma_direct_supported(struct device *dev, u64 mask);
 #endif /* _LINUX_DMA_DIRECT_H */

commit 4e1003aa56a7d60ddb048e43a7a51368fcfe36af
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Oct 29 09:57:32 2019 +0100

    dma-direct: remove the dma_handle argument to __dma_direct_alloc_pages
    
    The argument isn't used anywhere, so stop passing it.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Max Filippov <jcmvbkbc@gmail.com>

diff --git a/include/linux/dma-direct.h b/include/linux/dma-direct.h
index dec3b3bb121d..ff3d5edc44b9 100644
--- a/include/linux/dma-direct.h
+++ b/include/linux/dma-direct.h
@@ -67,6 +67,6 @@ void *dma_direct_alloc_pages(struct device *dev, size_t size,
 void dma_direct_free_pages(struct device *dev, size_t size, void *cpu_addr,
 		dma_addr_t dma_addr, unsigned long attrs);
 struct page *__dma_direct_alloc_pages(struct device *dev, size_t size,
-		dma_addr_t *dma_handle, gfp_t gfp, unsigned long attrs);
+		gfp_t gfp, unsigned long attrs);
 int dma_direct_supported(struct device *dev, u64 mask);
 #endif /* _LINUX_DMA_DIRECT_H */

commit acaade1af3587132e7ea585f470a95261e14f60c
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Oct 29 09:57:09 2019 +0100

    dma-direct: remove __dma_direct_free_pages
    
    We can just call dma_free_contiguous directly instead of wrapping it.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Max Filippov <jcmvbkbc@gmail.com>

diff --git a/include/linux/dma-direct.h b/include/linux/dma-direct.h
index adf993a3bd58..dec3b3bb121d 100644
--- a/include/linux/dma-direct.h
+++ b/include/linux/dma-direct.h
@@ -68,6 +68,5 @@ void dma_direct_free_pages(struct device *dev, size_t size, void *cpu_addr,
 		dma_addr_t dma_addr, unsigned long attrs);
 struct page *__dma_direct_alloc_pages(struct device *dev, size_t size,
 		dma_addr_t *dma_handle, gfp_t gfp, unsigned long attrs);
-void __dma_direct_free_pages(struct device *dev, size_t size, struct page *page);
 int dma_direct_supported(struct device *dev, u64 mask);
 #endif /* _LINUX_DMA_DIRECT_H */

commit 8b5369ea580964dbc982781bfb9fb93459fc5e8d
Author: Nicolas Saenz Julienne <nsaenzjulienne@suse.de>
Date:   Mon Oct 14 20:31:03 2019 +0200

    dma/direct: turn ARCH_ZONE_DMA_BITS into a variable
    
    Some architectures, notably ARM, are interested in tweaking this
    depending on their runtime DMA addressing limitations.
    
    Acked-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Nicolas Saenz Julienne <nsaenzjulienne@suse.de>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/include/linux/dma-direct.h b/include/linux/dma-direct.h
index adf993a3bd58..d03af3605460 100644
--- a/include/linux/dma-direct.h
+++ b/include/linux/dma-direct.h
@@ -5,6 +5,8 @@
 #include <linux/dma-mapping.h>
 #include <linux/mem_encrypt.h>
 
+extern unsigned int zone_dma_bits;
+
 #ifdef CONFIG_ARCH_HAS_PHYS_TO_DMA
 #include <asm/dma-direct.h>
 #else

commit 9087c37584fb7d8315877bb55f85e4268cc0b4f4
Author: Tom Lendacky <thomas.lendacky@amd.com>
Date:   Wed Jul 10 19:01:19 2019 +0000

    dma-direct: Force unencrypted DMA under SME for certain DMA masks
    
    If a device doesn't support DMA to a physical address that includes the
    encryption bit (currently bit 47, so 48-bit DMA), then the DMA must
    occur to unencrypted memory. SWIOTLB is used to satisfy that requirement
    if an IOMMU is not active (enabled or configured in passthrough mode).
    
    However, commit fafadcd16595 ("swiotlb: don't dip into swiotlb pool for
    coherent allocations") modified the coherent allocation support in
    SWIOTLB to use the DMA direct coherent allocation support. When an IOMMU
    is not active, this resulted in dma_alloc_coherent() failing for devices
    that didn't support DMA addresses that included the encryption bit.
    
    Addressing this requires changes to the force_dma_unencrypted() function
    in kernel/dma/direct.c. Since the function is now non-trivial and
    SME/SEV specific, update the DMA direct support to add an arch override
    for the force_dma_unencrypted() function. The arch override is selected
    when CONFIG_AMD_MEM_ENCRYPT is set. The arch override function resides in
    the arch/x86/mm/mem_encrypt.c file and forces unencrypted DMA when either
    SEV is active or SME is active and the device does not support DMA to
    physical addresses that include the encryption bit.
    
    Fixes: fafadcd16595 ("swiotlb: don't dip into swiotlb pool for coherent allocations")
    Suggested-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    [hch: moved the force_dma_unencrypted declaration to dma-mapping.h,
          fold the s390 fix from Halil Pasic]
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/dma-direct.h b/include/linux/dma-direct.h
index b7338702592a..adf993a3bd58 100644
--- a/include/linux/dma-direct.h
+++ b/include/linux/dma-direct.h
@@ -32,6 +32,15 @@ static inline bool dma_capable(struct device *dev, dma_addr_t addr, size_t size)
 }
 #endif /* !CONFIG_ARCH_HAS_PHYS_TO_DMA */
 
+#ifdef CONFIG_ARCH_HAS_FORCE_DMA_UNENCRYPTED
+bool force_dma_unencrypted(struct device *dev);
+#else
+static inline bool force_dma_unencrypted(struct device *dev)
+{
+	return false;
+}
+#endif /* CONFIG_ARCH_HAS_FORCE_DMA_UNENCRYPTED */
+
 /*
  * If memory encryption is supported, phys_to_dma will set the memory encryption
  * bit in the DMA address, and dma_to_phys will clear it.  The raw __phys_to_dma

commit 356da6d0cde3323236977fce54c1f9612a742036
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Dec 6 13:39:32 2018 -0800

    dma-mapping: bypass indirect calls for dma-direct
    
    Avoid expensive indirect calls in the fast path DMA mapping
    operations by directly calling the dma_direct_* ops if we are using
    the directly mapped DMA operations.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Tested-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Tested-by: Tony Luck <tony.luck@intel.com>

diff --git a/include/linux/dma-direct.h b/include/linux/dma-direct.h
index 3b0a3ea3876d..b7338702592a 100644
--- a/include/linux/dma-direct.h
+++ b/include/linux/dma-direct.h
@@ -60,22 +60,5 @@ void dma_direct_free_pages(struct device *dev, size_t size, void *cpu_addr,
 struct page *__dma_direct_alloc_pages(struct device *dev, size_t size,
 		dma_addr_t *dma_handle, gfp_t gfp, unsigned long attrs);
 void __dma_direct_free_pages(struct device *dev, size_t size, struct page *page);
-dma_addr_t dma_direct_map_page(struct device *dev, struct page *page,
-		unsigned long offset, size_t size, enum dma_data_direction dir,
-		unsigned long attrs);
-void dma_direct_unmap_page(struct device *dev, dma_addr_t addr,
-		size_t size, enum dma_data_direction dir, unsigned long attrs);
-int dma_direct_map_sg(struct device *dev, struct scatterlist *sgl, int nents,
-		enum dma_data_direction dir, unsigned long attrs);
-void dma_direct_unmap_sg(struct device *dev, struct scatterlist *sgl,
-		int nents, enum dma_data_direction dir, unsigned long attrs);
-void dma_direct_sync_single_for_device(struct device *dev,
-		dma_addr_t addr, size_t size, enum dma_data_direction dir);
-void dma_direct_sync_sg_for_device(struct device *dev,
-		struct scatterlist *sgl, int nents, enum dma_data_direction dir);
-void dma_direct_sync_single_for_cpu(struct device *dev,
-		dma_addr_t addr, size_t size, enum dma_data_direction dir);
-void dma_direct_sync_sg_for_cpu(struct device *dev,
-		struct scatterlist *sgl, int nents, enum dma_data_direction dir);
 int dma_direct_supported(struct device *dev, u64 mask);
 #endif /* _LINUX_DMA_DIRECT_H */

commit 55897af63091ebc2c3f239c6a6666f748113ac50
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Dec 3 11:43:54 2018 +0100

    dma-direct: merge swiotlb_dma_ops into the dma_direct code
    
    While the dma-direct code is (relatively) clean and simple we actually
    have to use the swiotlb ops for the mapping on many architectures due
    to devices with addressing limits.  Instead of keeping two
    implementations around this commit allows the dma-direct
    implementation to call the swiotlb bounce buffering functions and
    thus share the guts of the mapping implementation.  This also
    simplified the dma-mapping setup on a few architectures where we
    don't have to differenciate which implementation to use.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Tested-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Tested-by: Tony Luck <tony.luck@intel.com>

diff --git a/include/linux/dma-direct.h b/include/linux/dma-direct.h
index 1aa73f4907ae..3b0a3ea3876d 100644
--- a/include/linux/dma-direct.h
+++ b/include/linux/dma-direct.h
@@ -63,7 +63,19 @@ void __dma_direct_free_pages(struct device *dev, size_t size, struct page *page)
 dma_addr_t dma_direct_map_page(struct device *dev, struct page *page,
 		unsigned long offset, size_t size, enum dma_data_direction dir,
 		unsigned long attrs);
+void dma_direct_unmap_page(struct device *dev, dma_addr_t addr,
+		size_t size, enum dma_data_direction dir, unsigned long attrs);
 int dma_direct_map_sg(struct device *dev, struct scatterlist *sgl, int nents,
 		enum dma_data_direction dir, unsigned long attrs);
+void dma_direct_unmap_sg(struct device *dev, struct scatterlist *sgl,
+		int nents, enum dma_data_direction dir, unsigned long attrs);
+void dma_direct_sync_single_for_device(struct device *dev,
+		dma_addr_t addr, size_t size, enum dma_data_direction dir);
+void dma_direct_sync_sg_for_device(struct device *dev,
+		struct scatterlist *sgl, int nents, enum dma_data_direction dir);
+void dma_direct_sync_single_for_cpu(struct device *dev,
+		dma_addr_t addr, size_t size, enum dma_data_direction dir);
+void dma_direct_sync_sg_for_cpu(struct device *dev,
+		struct scatterlist *sgl, int nents, enum dma_data_direction dir);
 int dma_direct_supported(struct device *dev, u64 mask);
 #endif /* _LINUX_DMA_DIRECT_H */

commit 68c608345cc569bcfa1c1b2add4c00c343ecf933
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Dec 6 07:06:04 2018 -0800

    swiotlb: remove dma_mark_clean
    
    Instead of providing a special dma_mark_clean hook just for ia64, switch
    ia64 to use the normal arch_sync_dma_for_cpu hooks instead.
    
    This means that we now also set the PG_arch_1 bit for pages in the
    swiotlb buffer, which isn't stricly needed as we will never execute code
    out of the swiotlb buffer, but otherwise harmless.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Tested-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Tested-by: Tony Luck <tony.luck@intel.com>

diff --git a/include/linux/dma-direct.h b/include/linux/dma-direct.h
index 6e5a47ae7d64..1aa73f4907ae 100644
--- a/include/linux/dma-direct.h
+++ b/include/linux/dma-direct.h
@@ -48,14 +48,6 @@ static inline phys_addr_t dma_to_phys(struct device *dev, dma_addr_t daddr)
 	return __sme_clr(__dma_to_phys(dev, daddr));
 }
 
-#ifdef CONFIG_ARCH_HAS_DMA_MARK_CLEAN
-void dma_mark_clean(void *addr, size_t size);
-#else
-static inline void dma_mark_clean(void *addr, size_t size)
-{
-}
-#endif /* CONFIG_ARCH_HAS_DMA_MARK_CLEAN */
-
 u64 dma_direct_get_required_mask(struct device *dev);
 void *dma_direct_alloc(struct device *dev, size_t size, dma_addr_t *dma_handle,
 		gfp_t gfp, unsigned long attrs);

commit b0cbeae4944924640bf550b75487729a20204c14
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Nov 21 18:52:35 2018 +0100

    dma-direct: remove the mapping_error dma_map_ops method
    
    The dma-direct code already returns (~(dma_addr_t)0x0) on mapping
    failures, so we can switch over to returning DMA_MAPPING_ERROR and let
    the core dma-mapping code handle the rest.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/dma-direct.h b/include/linux/dma-direct.h
index 61b78f934f64..6e5a47ae7d64 100644
--- a/include/linux/dma-direct.h
+++ b/include/linux/dma-direct.h
@@ -5,8 +5,6 @@
 #include <linux/dma-mapping.h>
 #include <linux/mem_encrypt.h>
 
-#define DIRECT_MAPPING_ERROR		(~(dma_addr_t)0)
-
 #ifdef CONFIG_ARCH_HAS_PHYS_TO_DMA
 #include <asm/dma-direct.h>
 #else
@@ -76,5 +74,4 @@ dma_addr_t dma_direct_map_page(struct device *dev, struct page *page,
 int dma_direct_map_sg(struct device *dev, struct scatterlist *sgl, int nents,
 		enum dma_data_direction dir, unsigned long attrs);
 int dma_direct_supported(struct device *dev, u64 mask);
-int dma_direct_mapping_error(struct device *dev, dma_addr_t dma_addr);
 #endif /* _LINUX_DMA_DIRECT_H */

commit b18814e767a445534ab9ccba02e82a31208f85d6
Author: Christoph Hellwig <hch@lst.de>
Date:   Sun Nov 4 17:27:56 2018 +0100

    dma-direct: provide page based alloc/free helpers
    
    Some architectures support remapping highmem into DMA coherent
    allocations.  To use the common code for them we need variants of
    dma_direct_{alloc,free}_pages that do not use kernel virtual addresses.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Robin Murphy <robin.murphy@arm.com>

diff --git a/include/linux/dma-direct.h b/include/linux/dma-direct.h
index 9e66bfe369aa..61b78f934f64 100644
--- a/include/linux/dma-direct.h
+++ b/include/linux/dma-direct.h
@@ -67,6 +67,9 @@ void *dma_direct_alloc_pages(struct device *dev, size_t size,
 		dma_addr_t *dma_handle, gfp_t gfp, unsigned long attrs);
 void dma_direct_free_pages(struct device *dev, size_t size, void *cpu_addr,
 		dma_addr_t dma_addr, unsigned long attrs);
+struct page *__dma_direct_alloc_pages(struct device *dev, size_t size,
+		dma_addr_t *dma_handle, gfp_t gfp, unsigned long attrs);
+void __dma_direct_free_pages(struct device *dev, size_t size, struct page *page);
 dma_addr_t dma_direct_map_page(struct device *dev, struct page *page,
 		unsigned long offset, size_t size, enum dma_data_direction dir,
 		unsigned long attrs);

commit b34087157dd76e8d96e5e52808134a791ac61e57
Author: Robin Murphy <robin.murphy@arm.com>
Date:   Wed Nov 21 16:00:50 2018 +0000

    dma-direct: Make DIRECT_MAPPING_ERROR viable for SWIOTLB
    
    With the overflow buffer removed, we no longer have a unique address
    which is guaranteed not to be a valid DMA target to use as an error
    token. The DIRECT_MAPPING_ERROR value of 0 tries to at least represent
    an unlikely DMA target, but unfortunately there are already SWIOTLB
    users with DMA-able memory at physical address 0 which now gets falsely
    treated as a mapping failure and leads to all manner of misbehaviour.
    
    The best we can do to mitigate that is flip DIRECT_MAPPING_ERROR to the
    other commonly-used error value of all-bits-set, since the last single
    byte of memory is by far the least-likely-valid DMA target.
    
    Fixes: dff8d6c1ed58 ("swiotlb: remove the overflow buffer")
    Reported-by: John Stultz <john.stultz@linaro.org>
    Tested-by: John Stultz <john.stultz@linaro.org>
    Acked-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Robin Murphy <robin.murphy@arm.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/dma-direct.h b/include/linux/dma-direct.h
index bd73e7a91410..9e66bfe369aa 100644
--- a/include/linux/dma-direct.h
+++ b/include/linux/dma-direct.h
@@ -5,7 +5,7 @@
 #include <linux/dma-mapping.h>
 #include <linux/mem_encrypt.h>
 
-#define DIRECT_MAPPING_ERROR		0
+#define DIRECT_MAPPING_ERROR		(~(dma_addr_t)0)
 
 #ifdef CONFIG_ARCH_HAS_PHYS_TO_DMA
 #include <asm/dma-direct.h>

commit dff8d6c1ed584de65aac40494d3e7468c50980c3
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Aug 16 15:30:39 2018 +0300

    swiotlb: remove the overflow buffer
    
    Like all other dma mapping drivers just return an error code instead
    of an actual memory buffer.  The reason for the overflow buffer was
    that at the time swiotlb was invented there was no way to check for
    dma mapping errors, but this has long been fixed.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Robin Murphy <robin.murphy@arm.com>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/include/linux/dma-direct.h b/include/linux/dma-direct.h
index fbca184ff5a0..bd73e7a91410 100644
--- a/include/linux/dma-direct.h
+++ b/include/linux/dma-direct.h
@@ -5,6 +5,8 @@
 #include <linux/dma-mapping.h>
 #include <linux/mem_encrypt.h>
 
+#define DIRECT_MAPPING_ERROR		0
+
 #ifdef CONFIG_ARCH_HAS_PHYS_TO_DMA
 #include <asm/dma-direct.h>
 #else

commit b4ebe6063204da58e48600b810a97c29ae9e5d12
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Sep 20 14:04:08 2018 +0200

    dma-direct: implement complete bus_dma_mask handling
    
    Instead of rejecting devices with a too small bus_dma_mask we can handle
    by taking the bus dma_mask into account for allocations and bounce
    buffering decisions.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/dma-direct.h b/include/linux/dma-direct.h
index b79496d8c75b..fbca184ff5a0 100644
--- a/include/linux/dma-direct.h
+++ b/include/linux/dma-direct.h
@@ -27,7 +27,8 @@ static inline bool dma_capable(struct device *dev, dma_addr_t addr, size_t size)
 	if (!dev->dma_mask)
 		return false;
 
-	return addr + size - 1 <= *dev->dma_mask;
+	return addr + size - 1 <=
+		min_not_zero(*dev->dma_mask, dev->bus_dma_mask);
 }
 #endif /* !CONFIG_ARCH_HAS_PHYS_TO_DMA */
 

commit a20bb058375147cb639c7aa17ef86ad68b32d847
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Sep 20 13:26:13 2018 +0200

    dma-direct: add an explicit dma_direct_get_required_mask
    
    This is somewhat modelled after the powerpc version, and differs from
    the legacy fallback in use fls64 instead of pointlessly splitting up the
    address into low and high dwords and in that it takes (__)phys_to_dma
    into account.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Reviewed-by: Robin Murphy <robin.murphy@arm.com>

diff --git a/include/linux/dma-direct.h b/include/linux/dma-direct.h
index 86a59ba5a7f3..b79496d8c75b 100644
--- a/include/linux/dma-direct.h
+++ b/include/linux/dma-direct.h
@@ -55,6 +55,7 @@ static inline void dma_mark_clean(void *addr, size_t size)
 }
 #endif /* CONFIG_ARCH_HAS_DMA_MARK_CLEAN */
 
+u64 dma_direct_get_required_mask(struct device *dev);
 void *dma_direct_alloc(struct device *dev, size_t size, dma_addr_t *dma_handle,
 		gfp_t gfp, unsigned long attrs);
 void dma_direct_free(struct device *dev, size_t size, void *cpu_addr,

commit bc3ec75de5452db59b683487867ba562b950708a
Author: Christoph Hellwig <hch@lst.de>
Date:   Sat Sep 8 11:22:43 2018 +0200

    dma-mapping: merge direct and noncoherent ops
    
    All the cache maintainance is already stubbed out when not enabled,
    but merging the two allows us to nicely handle the case where
    cache maintainance is required for some devices, but not others.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Paul Burton <paul.burton@mips.com> # MIPS parts

diff --git a/include/linux/dma-direct.h b/include/linux/dma-direct.h
index 8d9f33febde5..86a59ba5a7f3 100644
--- a/include/linux/dma-direct.h
+++ b/include/linux/dma-direct.h
@@ -59,6 +59,10 @@ void *dma_direct_alloc(struct device *dev, size_t size, dma_addr_t *dma_handle,
 		gfp_t gfp, unsigned long attrs);
 void dma_direct_free(struct device *dev, size_t size, void *cpu_addr,
 		dma_addr_t dma_addr, unsigned long attrs);
+void *dma_direct_alloc_pages(struct device *dev, size_t size,
+		dma_addr_t *dma_handle, gfp_t gfp, unsigned long attrs);
+void dma_direct_free_pages(struct device *dev, size_t size, void *cpu_addr,
+		dma_addr_t dma_addr, unsigned long attrs);
 dma_addr_t dma_direct_map_page(struct device *dev, struct page *page,
 		unsigned long offset, size_t size, enum dma_data_direction dir,
 		unsigned long attrs);

commit 782e6769c0df744e773dc2acff71c974b3bba4e9
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Apr 16 15:24:51 2018 +0200

    dma-mapping: provide a generic dma-noncoherent implementation
    
    Add a new dma_map_ops implementation that uses dma-direct for the
    address mapping of streaming mappings, and which requires arch-specific
    implemenations of coherent allocate/free.
    
    Architectures have to provide flushing helpers to ownership trasnfers
    to the device and/or CPU, and can provide optional implementations of
    the coherent mmap functionality, and the cache_flush routines for
    non-coherent long term allocations.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Tested-by: Alexey Brodkin <abrodkin@synopsys.com>
    Acked-by: Vineet Gupta <vgupta@synopsys.com>

diff --git a/include/linux/dma-direct.h b/include/linux/dma-direct.h
index 53ad6a47f513..8d9f33febde5 100644
--- a/include/linux/dma-direct.h
+++ b/include/linux/dma-direct.h
@@ -59,6 +59,11 @@ void *dma_direct_alloc(struct device *dev, size_t size, dma_addr_t *dma_handle,
 		gfp_t gfp, unsigned long attrs);
 void dma_direct_free(struct device *dev, size_t size, void *cpu_addr,
 		dma_addr_t dma_addr, unsigned long attrs);
+dma_addr_t dma_direct_map_page(struct device *dev, struct page *page,
+		unsigned long offset, size_t size, enum dma_data_direction dir,
+		unsigned long attrs);
+int dma_direct_map_sg(struct device *dev, struct scatterlist *sgl, int nents,
+		enum dma_data_direction dir, unsigned long attrs);
 int dma_direct_supported(struct device *dev, u64 mask);
-
+int dma_direct_mapping_error(struct device *dev, dma_addr_t dma_addr);
 #endif /* _LINUX_DMA_DIRECT_H */

commit b6e05477c10c12e36141558fc14f04b00ea634d4
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Mar 19 11:38:24 2018 +0100

    dma/direct: Handle the memory encryption bit in common code
    
    Give the basic phys_to_dma() and dma_to_phys() helpers a __-prefix and add
    the memory encryption mask to the non-prefixed versions.  Use the
    __-prefixed versions directly instead of clearing the mask again in
    various places.
    
    Tested-by: Tom Lendacky <thomas.lendacky@amd.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: David Woodhouse <dwmw2@infradead.org>
    Cc: Joerg Roedel <joro@8bytes.org>
    Cc: Jon Mason <jdmason@kudzu.us>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Muli Ben-Yehuda <mulix@mulix.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: iommu@lists.linux-foundation.org
    Link: http://lkml.kernel.org/r/20180319103826.12853-13-hch@lst.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/dma-direct.h b/include/linux/dma-direct.h
index bcdb1a3e4b1f..53ad6a47f513 100644
--- a/include/linux/dma-direct.h
+++ b/include/linux/dma-direct.h
@@ -3,18 +3,19 @@
 #define _LINUX_DMA_DIRECT_H 1
 
 #include <linux/dma-mapping.h>
+#include <linux/mem_encrypt.h>
 
 #ifdef CONFIG_ARCH_HAS_PHYS_TO_DMA
 #include <asm/dma-direct.h>
 #else
-static inline dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr)
+static inline dma_addr_t __phys_to_dma(struct device *dev, phys_addr_t paddr)
 {
 	dma_addr_t dev_addr = (dma_addr_t)paddr;
 
 	return dev_addr - ((dma_addr_t)dev->dma_pfn_offset << PAGE_SHIFT);
 }
 
-static inline phys_addr_t dma_to_phys(struct device *dev, dma_addr_t dev_addr)
+static inline phys_addr_t __dma_to_phys(struct device *dev, dma_addr_t dev_addr)
 {
 	phys_addr_t paddr = (phys_addr_t)dev_addr;
 
@@ -30,6 +31,22 @@ static inline bool dma_capable(struct device *dev, dma_addr_t addr, size_t size)
 }
 #endif /* !CONFIG_ARCH_HAS_PHYS_TO_DMA */
 
+/*
+ * If memory encryption is supported, phys_to_dma will set the memory encryption
+ * bit in the DMA address, and dma_to_phys will clear it.  The raw __phys_to_dma
+ * and __dma_to_phys versions should only be used on non-encrypted memory for
+ * special occasions like DMA coherent buffers.
+ */
+static inline dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr)
+{
+	return __sme_set(__phys_to_dma(dev, paddr));
+}
+
+static inline phys_addr_t dma_to_phys(struct device *dev, dma_addr_t daddr)
+{
+	return __sme_clr(__dma_to_phys(dev, daddr));
+}
+
 #ifdef CONFIG_ARCH_HAS_DMA_MARK_CLEAN
 void dma_mark_clean(void *addr, size_t size);
 #else

commit 1a9777a8a01fb88659a3dda48080c95c34cab7d3
Author: Christoph Hellwig <hch@lst.de>
Date:   Sun Dec 24 15:04:32 2017 +0100

    dma-direct: reject too small dma masks
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Robin Murphy <robin.murphy@arm.com>

diff --git a/include/linux/dma-direct.h b/include/linux/dma-direct.h
index 4788bf0bf683..bcdb1a3e4b1f 100644
--- a/include/linux/dma-direct.h
+++ b/include/linux/dma-direct.h
@@ -42,5 +42,6 @@ void *dma_direct_alloc(struct device *dev, size_t size, dma_addr_t *dma_handle,
 		gfp_t gfp, unsigned long attrs);
 void dma_direct_free(struct device *dev, size_t size, void *cpu_addr,
 		dma_addr_t dma_addr, unsigned long attrs);
+int dma_direct_supported(struct device *dev, u64 mask);
 
 #endif /* _LINUX_DMA_DIRECT_H */

commit 19dca8c0efa30e0a45e79f237060d0f307045752
Author: Christoph Hellwig <hch@lst.de>
Date:   Sat Dec 23 13:46:06 2017 +0100

    dma-direct: make dma_direct_{alloc,free} available to other implementations
    
    So that they don't need to indirect through the operation vector.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Vladimir Murzin <vladimir.murzin@arm.com>

diff --git a/include/linux/dma-direct.h b/include/linux/dma-direct.h
index 10e924b7cba7..4788bf0bf683 100644
--- a/include/linux/dma-direct.h
+++ b/include/linux/dma-direct.h
@@ -38,4 +38,9 @@ static inline void dma_mark_clean(void *addr, size_t size)
 }
 #endif /* CONFIG_ARCH_HAS_DMA_MARK_CLEAN */
 
+void *dma_direct_alloc(struct device *dev, size_t size, dma_addr_t *dma_handle,
+		gfp_t gfp, unsigned long attrs);
+void dma_direct_free(struct device *dev, size_t size, void *cpu_addr,
+		dma_addr_t dma_addr, unsigned long attrs);
+
 #endif /* _LINUX_DMA_DIRECT_H */

commit b49efd76248242169f28ffd20ada05064d01ed9f
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Jan 9 22:11:31 2018 +0100

    dma-mapping: move dma_mark_clean to dma-direct.h
    
    And unlike the other helpers we don't require a <asm/dma-direct.h> as
    this helper is a special case for ia64 only, and this keeps it as
    simple as possible.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/dma-direct.h b/include/linux/dma-direct.h
index 2cc1b6558944..10e924b7cba7 100644
--- a/include/linux/dma-direct.h
+++ b/include/linux/dma-direct.h
@@ -29,4 +29,13 @@ static inline bool dma_capable(struct device *dev, dma_addr_t addr, size_t size)
 	return addr + size - 1 <= *dev->dma_mask;
 }
 #endif /* !CONFIG_ARCH_HAS_PHYS_TO_DMA */
+
+#ifdef CONFIG_ARCH_HAS_DMA_MARK_CLEAN
+void dma_mark_clean(void *addr, size_t size);
+#else
+static inline void dma_mark_clean(void *addr, size_t size)
+{
+}
+#endif /* CONFIG_ARCH_HAS_DMA_MARK_CLEAN */
+
 #endif /* _LINUX_DMA_DIRECT_H */

commit ea8c64ace86647260ec4255f483e5844d62af2df
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Jan 10 16:21:13 2018 +0100

    dma-mapping: move swiotlb arch helpers to a new header
    
    phys_to_dma, dma_to_phys and dma_capable are helpers published by
    architecture code for use of swiotlb and xen-swiotlb only.  Drivers are
    not supposed to use these directly, but use the DMA API instead.
    
    Move these to a new asm/dma-direct.h helper, included by a
    linux/dma-direct.h wrapper that provides the default linear mapping
    unless the architecture wants to override it.
    
    In the MIPS case the existing dma-coherent.h is reused for now as
    untangling it will take a bit of work.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Robin Murphy <robin.murphy@arm.com>

diff --git a/include/linux/dma-direct.h b/include/linux/dma-direct.h
new file mode 100644
index 000000000000..2cc1b6558944
--- /dev/null
+++ b/include/linux/dma-direct.h
@@ -0,0 +1,32 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef _LINUX_DMA_DIRECT_H
+#define _LINUX_DMA_DIRECT_H 1
+
+#include <linux/dma-mapping.h>
+
+#ifdef CONFIG_ARCH_HAS_PHYS_TO_DMA
+#include <asm/dma-direct.h>
+#else
+static inline dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr)
+{
+	dma_addr_t dev_addr = (dma_addr_t)paddr;
+
+	return dev_addr - ((dma_addr_t)dev->dma_pfn_offset << PAGE_SHIFT);
+}
+
+static inline phys_addr_t dma_to_phys(struct device *dev, dma_addr_t dev_addr)
+{
+	phys_addr_t paddr = (phys_addr_t)dev_addr;
+
+	return paddr + ((phys_addr_t)dev->dma_pfn_offset << PAGE_SHIFT);
+}
+
+static inline bool dma_capable(struct device *dev, dma_addr_t addr, size_t size)
+{
+	if (!dev->dma_mask)
+		return false;
+
+	return addr + size - 1 <= *dev->dma_mask;
+}
+#endif /* !CONFIG_ARCH_HAS_PHYS_TO_DMA */
+#endif /* _LINUX_DMA_DIRECT_H */
