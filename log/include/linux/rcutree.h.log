commit 07325d4a90d2d84de45cc07b134fd0f023dbb971
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu May 21 22:05:16 2020 +0200

    rcu: Provide rcu_irq_exit_check_preempt()
    
    Provide a debug check which can be invoked from exception return to kernel
    mode before an attempt is made to schedule. Warn if RCU is not ready for
    this.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Cc: Paul E. McKenney <paulmck@kernel.org>
    Link: https://lore.kernel.org/r/20200521202117.089709607@linutronix.de

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 9366fa4d0717..d5cc9d675987 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -51,6 +51,12 @@ void rcu_irq_exit_preempt(void);
 void rcu_irq_enter_irqson(void);
 void rcu_irq_exit_irqson(void);
 
+#ifdef CONFIG_PROVE_RCU
+void rcu_irq_exit_check_preempt(void);
+#else
+static inline void rcu_irq_exit_check_preempt(void) { }
+#endif
+
 void exit_rcu(void);
 
 void rcu_scheduler_starting(void);

commit b1fcf9b83c4149c63d1e0c699e85f93cbe28e211
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue May 12 09:44:43 2020 +0200

    rcu: Provide __rcu_is_watching()
    
    Same as rcu_is_watching() but without the preempt_disable/enable() pair
    inside the function. It is merked noinstr so it ends up in the
    non-instrumentable text section.
    
    This is useful for non-preemptible code especially in the low level entry
    section. Using rcu_is_watching() there results in a call to the
    preempt_schedule_notrace() thunk which triggers noinstr section warnings in
    objtool.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200512213810.518709291@linutronix.de

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 02016e0aa8eb..9366fa4d0717 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -58,6 +58,7 @@ extern int rcu_scheduler_active __read_mostly;
 void rcu_end_inkernel_boot(void);
 bool rcu_inkernel_boot_has_ended(void);
 bool rcu_is_watching(void);
+bool __rcu_is_watching(void);
 #ifndef CONFIG_PREEMPTION
 void rcu_all_qs(void);
 #endif

commit 8ae0ae6737ad449c8ae21e2bb01d9736f360a933
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun May 3 15:08:52 2020 +0200

    rcu: Provide rcu_irq_exit_preempt()
    
    Interrupts and exceptions invoke rcu_irq_enter() on entry and need to
    invoke rcu_irq_exit() before they either return to the interrupted code or
    invoke the scheduler due to preemption.
    
    The general assumption is that RCU idle code has to have preemption
    disabled so that a return from interrupt cannot schedule. So the return
    from interrupt code invokes rcu_irq_exit() and preempt_schedule_irq().
    
    If there is any imbalance in the rcu_irq/nmi* invocations or RCU idle code
    had preemption enabled then this goes unnoticed until the CPU goes idle or
    some other RCU check is executed.
    
    Provide rcu_irq_exit_preempt() which can be invoked from the
    interrupt/exception return code in case that preemption is enabled. It
    invokes rcu_irq_exit() and contains a few sanity checks in case that
    CONFIG_PROVE_RCU is enabled to catch such issues directly.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Paul E. McKenney <paulmck@kernel.org>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200505134904.364456424@linutronix.de

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index fbc26274af4d..02016e0aa8eb 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -47,6 +47,7 @@ void rcu_idle_enter(void);
 void rcu_idle_exit(void);
 void rcu_irq_enter(void);
 void rcu_irq_exit(void);
+void rcu_irq_exit_preempt(void);
 void rcu_irq_enter_irqson(void);
 void rcu_irq_exit_irqson(void);
 

commit 6be7436d2245d3dd8b9a8f949367c13841c23308
Author: Paul E. McKenney <paulmck@kernel.org>
Date:   Fri Apr 10 13:47:41 2020 -0700

    rcu: Add rcu_gp_might_be_stalled()
    
    This commit adds rcu_gp_might_be_stalled(), which returns true if there
    is some reason to believe that the RCU grace period is stalled.  The use
    case is where an RCU free-memory path needs to allocate memory in order
    to free it, a situation that should be avoided where possible.
    
    But where it is necessary, there is always the alternative of using
    synchronize_rcu() to wait for a grace period in order to avoid the
    allocation.  And if the grace period is stalled, allocating memory to
    asynchronously wait for it is a bad idea of epic proportions: Far better
    to let others use the memory, because these others might actually be
    able to free that memory before the grace period ends.
    
    Thus, rcu_gp_might_be_stalled() can be used to help decide whether
    allocating memory on an RCU free path is a semi-reasonable course
    of action.
    
    Cc: Joel Fernandes <joel@joelfernandes.org>
    Cc: Uladzislau Rezki <urezki@gmail.com>
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 45f3f66bb04d..fbc26274af4d 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -39,6 +39,7 @@ void rcu_barrier(void);
 bool rcu_eqs_special_set(int cpu);
 void rcu_momentary_dyntick_idle(void);
 void kfree_rcu_scheduler_running(void);
+bool rcu_gp_might_be_stalled(void);
 unsigned long get_state_synchronize_rcu(void);
 void cond_synchronize_rcu(unsigned long oldstate);
 

commit 59ee0326ccf712f9a637d5df2465a16a784cbfb0
Author: Paul E. McKenney <paulmck@kernel.org>
Date:   Thu Nov 28 18:54:06 2019 -0800

    rcutorture: Suppress forward-progress complaints during early boot
    
    Some larger systems can take in excess of 50 seconds to complete their
    early boot initcalls prior to spawing init.  This does not in any way
    help the forward-progress judgments of built-in rcutorture (when
    rcutorture is built as a module, the insmod or modprobe command normally
    cannot happen until some time after boot completes).  This commit
    therefore suppresses such complaints until about the time that init
    is spawned.
    
    This also includes a fix to a stupid error located by kbuild test robot.
    
    [ paulmck: Apply kbuild test robot feedback. ]
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>
    [ paulmck: Fix to nohz_full slow-expediting recovery logic, per bpetkov. ]
    [ paulmck: Restrict splat to CONFIG_PREEMPT_RT=y kernels and simplify. ]
    Tested-by: Borislav Petkov <bp@alien8.de>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 2f787b9029d1..45f3f66bb04d 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -54,6 +54,7 @@ void exit_rcu(void);
 void rcu_scheduler_starting(void);
 extern int rcu_scheduler_active __read_mostly;
 void rcu_end_inkernel_boot(void);
+bool rcu_inkernel_boot_has_ended(void);
 bool rcu_is_watching(void);
 #ifndef CONFIG_PREEMPTION
 void rcu_all_qs(void);

commit 189a6883dcf7fa70e17403ae4225c60ffc9e404b
Author: Joel Fernandes (Google) <joel@joelfernandes.org>
Date:   Fri Aug 30 12:36:33 2019 -0400

    rcu: Remove kfree_call_rcu_nobatch()
    
    Now that the kfree_rcu() special-casing has been removed from tree RCU,
    this commit removes kfree_call_rcu_nobatch() since it is no longer needed.
    
    Signed-off-by: Joel Fernandes (Google) <joel@joelfernandes.org>
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 6a65d3a16dbd..2f787b9029d1 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -34,7 +34,6 @@ static inline void rcu_virt_note_context_switch(int cpu)
 
 void synchronize_rcu_expedited(void);
 void kfree_call_rcu(struct rcu_head *head, rcu_callback_t func);
-void kfree_call_rcu_nobatch(struct rcu_head *head, rcu_callback_t func);
 
 void rcu_barrier(void);
 bool rcu_eqs_special_set(int cpu);

commit a35d16905efc6ad5523d864a5c6efcb1e657e386
Author: Byungchul Park <byungchul.park@lge.com>
Date:   Mon Aug 5 18:22:27 2019 -0400

    rcu: Add basic support for kfree_rcu() batching
    
    Recently a discussion about stability and performance of a system
    involving a high rate of kfree_rcu() calls surfaced on the list [1]
    which led to another discussion how to prepare for this situation.
    
    This patch adds basic batching support for kfree_rcu(). It is "basic"
    because we do none of the slab management, dynamic allocation, code
    moving or any of the other things, some of which previous attempts did
    [2]. These fancier improvements can be follow-up patches and there are
    different ideas being discussed in those regards. This is an effort to
    start simple, and build up from there. In the future, an extension to
    use kfree_bulk and possibly per-slab batching could be done to further
    improve performance due to cache-locality and slab-specific bulk free
    optimizations. By using an array of pointers, the worker thread
    processing the work would need to read lesser data since it does not
    need to deal with large rcu_head(s) any longer.
    
    Torture tests follow in the next patch and show improvements of around
    5x reduction in number of  grace periods on a 16 CPU system. More
    details and test data are in that patch.
    
    There is an implication with rcu_barrier() with this patch. Since the
    kfree_rcu() calls can be batched, and may not be handed yet to the RCU
    machinery in fact, the monitor may not have even run yet to do the
    queue_rcu_work(), there seems no easy way of implementing rcu_barrier()
    to wait for those kfree_rcu()s that are already made. So this means a
    kfree_rcu() followed by an rcu_barrier() does not imply that memory will
    be freed once rcu_barrier() returns.
    
    Another implication is higher active memory usage (although not
    run-away..) until the kfree_rcu() flooding ends, in comparison to
    without batching. More details about this are in the second patch which
    adds an rcuperf test.
    
    Finally, in the near future we will get rid of kfree_rcu() special casing
    within RCU such as in rcu_do_batch and switch everything to just
    batching. Currently we don't do that since timer subsystem is not yet up
    and we cannot schedule the kfree_rcu() monitor as the timer subsystem's
    lock are not initialized. That would also mean getting rid of
    kfree_call_rcu_nobatch() entirely.
    
    [1] http://lore.kernel.org/lkml/20190723035725-mutt-send-email-mst@kernel.org
    [2] https://lkml.org/lkml/2017/12/19/824
    
    Cc: kernel-team@android.com
    Cc: kernel-team@lge.com
    Co-developed-by: Byungchul Park <byungchul.park@lge.com>
    Signed-off-by: Byungchul Park <byungchul.park@lge.com>
    Signed-off-by: Joel Fernandes (Google) <joel@joelfernandes.org>
    [ paulmck: Applied 0day and Paul Walmsley feedback on ->monitor_todo. ]
    [ paulmck: Make it work during early boot. ]
    [ paulmck: Add a crude early boot self-test. ]
    [ paulmck: Style adjustments and experimental docbook structure header. ]
    Link: https://lore.kernel.org/lkml/alpine.DEB.2.21.9999.1908161931110.32497@viisi.sifive.com/T/#me9956f66cb611b95d26ae92700e1d901f46e8c59
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index c5147de885ec..6a65d3a16dbd 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -34,10 +34,12 @@ static inline void rcu_virt_note_context_switch(int cpu)
 
 void synchronize_rcu_expedited(void);
 void kfree_call_rcu(struct rcu_head *head, rcu_callback_t func);
+void kfree_call_rcu_nobatch(struct rcu_head *head, rcu_callback_t func);
 
 void rcu_barrier(void);
 bool rcu_eqs_special_set(int cpu);
 void rcu_momentary_dyntick_idle(void);
+void kfree_rcu_scheduler_running(void);
 unsigned long get_state_synchronize_rcu(void);
 void cond_synchronize_rcu(unsigned long oldstate);
 

commit 366237e7b0833faa2d8da7a8d7d7da8c3ca802e5
Author: Paul E. McKenney <paulmck@kernel.org>
Date:   Wed Jul 10 08:01:01 2019 -0700

    stop_machine: Provide RCU quiescent state in multi_cpu_stop()
    
    When multi_cpu_stop() loops waiting for other tasks, it can trigger an RCU
    CPU stall warning.  This can be misleading because what is instead needed
    is information on whatever task is blocking multi_cpu_stop().  This commit
    therefore inserts an RCU quiescent state into the multi_cpu_stop()
    function's waitloop.
    
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 18b1ed9864b0..c5147de885ec 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -37,6 +37,7 @@ void kfree_call_rcu(struct rcu_head *head, rcu_callback_t func);
 
 void rcu_barrier(void);
 bool rcu_eqs_special_set(int cpu);
+void rcu_momentary_dyntick_idle(void);
 unsigned long get_state_synchronize_rcu(void);
 void cond_synchronize_rcu(unsigned long oldstate);
 

commit 01b1d88b09824bea1a75b0bac04dcf50d9893875
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Jul 26 23:19:38 2019 +0200

    rcu: Use CONFIG_PREEMPTION
    
    CONFIG_PREEMPTION is selected by CONFIG_PREEMPT and by
    CONFIG_PREEMPT_RT. Both PREEMPT and PREEMPT_RT require the same
    functionality which today depends on CONFIG_PREEMPT.
    
    Switch the conditionals in RCU to use CONFIG_PREEMPTION.
    
    That's the first step towards RCU on RT. The further tweaks are work in
    progress. This neither touches the selftest bits which need a closer look
    by Paul.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Paul E. McKenney <paulmck@linux.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/20190726212124.210156346@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 735601ac27d3..18b1ed9864b0 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -53,7 +53,7 @@ void rcu_scheduler_starting(void);
 extern int rcu_scheduler_active __read_mostly;
 void rcu_end_inkernel_boot(void);
 bool rcu_is_watching(void);
-#ifndef CONFIG_PREEMPT
+#ifndef CONFIG_PREEMPTION
 void rcu_all_qs(void);
 #endif
 

commit a9b7343ec1a2f061967e4a17eb9276d129b679f4
Author: Paul E. McKenney <paulmck@linux.ibm.com>
Date:   Thu Jan 17 10:36:27 2019 -0800

    linux/rcutree: Convert to SPDX license identifier
    
    Replace the license boiler plate with a SPDX license identifier.
    While in the area, update an email address.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>
    [ paulmck: Update .h SPDX format per Joe Perches. ]
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 7f83179177d1..735601ac27d3 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -1,26 +1,13 @@
+/* SPDX-License-Identifier: GPL-2.0+ */
 /*
  * Read-Copy Update mechanism for mutual exclusion (tree-based version)
  *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License as published by
- * the Free Software Foundation; either version 2 of the License, or
- * (at your option) any later version.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program; if not, you can access it online at
- * http://www.gnu.org/licenses/gpl-2.0.html.
- *
  * Copyright IBM Corporation, 2008
  *
  * Author: Dipankar Sarma <dipankar@in.ibm.com>
- *	   Paul E. McKenney <paulmck@linux.vnet.ibm.com> Hierarchical algorithm
+ *	   Paul E. McKenney <paulmck@linux.ibm.com> Hierarchical algorithm
  *
- * Based on the original work by Paul McKenney <paulmck@us.ibm.com>
+ * Based on the original work by Paul McKenney <paulmck@linux.ibm.com>
  * and inputs from Rusty Russell, Andrea Arcangeli and Andi Kleen.
  *
  * For detailed explanation of Read-Copy Update mechanism see -

commit 395a2f097ebdddf2bfa286b6119f1b231025c2f1
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Jul 10 14:00:14 2018 -0700

    rcu: Define rcu_all_qs() only in !PREEMPT builds
    
    Now that rcu_all_qs() is used only in !PREEMPT builds, move it to
    tree_plugin.h so that it is defined only in those builds.  This in
    turn means that rcu_momentary_dyntick_idle() is only used in !PREEMPT
    builds, but it is simply marked __maybe_unused in order to keep it
    near the rest of the dyntick-idle code.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index d09a9abe9440..7f83179177d1 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -66,7 +66,9 @@ void rcu_scheduler_starting(void);
 extern int rcu_scheduler_active __read_mostly;
 void rcu_end_inkernel_boot(void);
 bool rcu_is_watching(void);
+#ifndef CONFIG_PREEMPT
 void rcu_all_qs(void);
+#endif
 
 /* RCUtree hotplug events */
 int rcutree_prepare_cpu(unsigned int cpu);

commit a8bb74acd8efe2eb934d524ae20859980975b602
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Jul 6 11:46:47 2018 -0700

    rcu: Consolidate RCU-sched update-side function definitions
    
    This commit saves a few lines by consolidating the RCU-sched function
    definitions at the end of include/linux/rcupdate.h.  This consolidation
    also makes it easier to remove them all when the time comes.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 94820156aa62..d09a9abe9440 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -46,21 +46,12 @@ static inline void rcu_virt_note_context_switch(int cpu)
 }
 
 void synchronize_rcu_expedited(void);
-
-static inline void synchronize_sched_expedited(void)
-{
-	synchronize_rcu_expedited();
-}
-
 void kfree_call_rcu(struct rcu_head *head, rcu_callback_t func);
 
 void rcu_barrier(void);
-void rcu_barrier_sched(void);
 bool rcu_eqs_special_set(int cpu);
 unsigned long get_state_synchronize_rcu(void);
 void cond_synchronize_rcu(unsigned long oldstate);
-unsigned long get_state_synchronize_sched(void);
-void cond_synchronize_sched(unsigned long oldstate);
 
 void rcu_idle_enter(void);
 void rcu_idle_exit(void);

commit 4c7e9c1434c6fc960774a5475f2fbccbf557fdeb
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Jul 6 09:54:25 2018 -0700

    rcu: Consolidate RCU-bh update-side function definitions
    
    This commit saves a few lines by consolidating the RCU-bh function
    definitions at the end of include/linux/rcupdate.h.  This consolidation
    also makes it easier to remove them all when the time comes.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 6d30a0809300..94820156aa62 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -45,11 +45,6 @@ static inline void rcu_virt_note_context_switch(int cpu)
 	rcu_note_context_switch(false);
 }
 
-static inline void synchronize_rcu_bh(void)
-{
-	synchronize_rcu();
-}
-
 void synchronize_rcu_expedited(void);
 
 static inline void synchronize_sched_expedited(void)
@@ -59,19 +54,7 @@ static inline void synchronize_sched_expedited(void)
 
 void kfree_call_rcu(struct rcu_head *head, rcu_callback_t func);
 
-/**
- * synchronize_rcu_bh_expedited - Brute-force RCU-bh grace period
- *
- * This is a transitional API and will soon be removed, with all
- * callers converted to synchronize_rcu_expedited().
- */
-static inline void synchronize_rcu_bh_expedited(void)
-{
-	synchronize_rcu_expedited();
-}
-
 void rcu_barrier(void);
-void rcu_barrier_bh(void);
 void rcu_barrier_sched(void);
 bool rcu_eqs_special_set(int cpu);
 unsigned long get_state_synchronize_rcu(void);

commit 709fdce7545c978e69f52eb19082ea3af44332f5
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Jul 3 10:44:44 2018 -0700

    rcu: Express Tiny RCU updates in terms of RCU rather than RCU-sched
    
    This commit renames Tiny RCU functions so that the lowest level of
    functionality is RCU (e.g., synchronize_rcu()) rather than RCU-sched
    (e.g., synchronize_sched()).  This provides greater naming compatibility
    with Tree RCU, which will in turn permit more LoC removal once
    the RCU-sched and RCU-bh update-side API is removed.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    [ paulmck: Fix Tiny call_rcu()'s EXPORT_SYMBOL() in response to a bug
      report from kbuild test robot. ]

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 0c44720f0e84..6d30a0809300 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -45,7 +45,6 @@ static inline void rcu_virt_note_context_switch(int cpu)
 	rcu_note_context_switch(false);
 }
 
-void synchronize_rcu(void);
 static inline void synchronize_rcu_bh(void)
 {
 	synchronize_rcu();

commit 45975c7d21a1c0aba97e3d8007e2a7c123145748
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Jul 2 14:30:37 2018 -0700

    rcu: Define RCU-sched API in terms of RCU for Tree RCU PREEMPT builds
    
    Now that RCU-preempt knows about preemption disabling, its implementation
    of synchronize_rcu() works for synchronize_sched(), and likewise for the
    other RCU-sched update-side API members.  This commit therefore confines
    the RCU-sched update-side code to CONFIG_PREEMPT=n builds, and defines
    RCU-sched's update-side API members in terms of those of RCU-preempt.
    
    This means that any given build of the Linux kernel has only one
    update-side flavor of RCU, namely RCU-preempt for CONFIG_PREEMPT=y builds
    and RCU-sched for CONFIG_PREEMPT=n builds.  This in turn means that kernels
    built with CONFIG_RCU_NOCB_CPU=y have only one rcuo kthread per CPU.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Andi Kleen <ak@linux.intel.com>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index f7a41323aa54..0c44720f0e84 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -45,14 +45,19 @@ static inline void rcu_virt_note_context_switch(int cpu)
 	rcu_note_context_switch(false);
 }
 
+void synchronize_rcu(void);
 static inline void synchronize_rcu_bh(void)
 {
 	synchronize_rcu();
 }
 
-void synchronize_sched_expedited(void);
 void synchronize_rcu_expedited(void);
 
+static inline void synchronize_sched_expedited(void)
+{
+	synchronize_rcu_expedited();
+}
+
 void kfree_call_rcu(struct rcu_head *head, rcu_callback_t func);
 
 /**

commit 82fcecfa81855924cc69f3078113cf63dd6c2964
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Jul 2 09:04:27 2018 -0700

    rcu: Update comments and help text for no more RCU-bh updaters
    
    This commit updates comments and help text to account for the fact that
    RCU-bh update-side functions are now simple wrappers for their RCU or
    RCU-sched counterparts.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index c789c302a2c9..f7a41323aa54 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -58,18 +58,8 @@ void kfree_call_rcu(struct rcu_head *head, rcu_callback_t func);
 /**
  * synchronize_rcu_bh_expedited - Brute-force RCU-bh grace period
  *
- * Wait for an RCU-bh grace period to elapse, but use a "big hammer"
- * approach to force the grace period to end quickly.  This consumes
- * significant time on all CPUs and is unfriendly to real-time workloads,
- * so is thus not recommended for any sort of common-case code.  In fact,
- * if you are using synchronize_rcu_bh_expedited() in a loop, please
- * restructure your code to batch your updates, and then use a single
- * synchronize_rcu_bh() instead.
- *
- * Note that it is illegal to call this function while holding any lock
- * that is acquired by a CPU-hotplug notifier.  And yes, it is also illegal
- * to call this function from a CPU-hotplug notifier.  Failing to observe
- * these restriction will result in deadlock.
+ * This is a transitional API and will soon be removed, with all
+ * callers converted to synchronize_rcu_expedited().
  */
 static inline void synchronize_rcu_bh_expedited(void)
 {

commit 65cfe3583b612a22e12fba9a7bbd2d37ca5ad941
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sun Jul 1 07:40:52 2018 -0700

    rcu: Define RCU-bh update API in terms of RCU
    
    Now that the main RCU API knows about softirq disabling and softirq's
    quiescent states, the RCU-bh update code can be dispensed with.
    This commit therefore removes the RCU-bh update-side implementation and
    defines RCU-bh's update-side API in terms of that of either RCU-preempt or
    RCU-sched, depending on the setting of the CONFIG_PREEMPT Kconfig option.
    
    In kernels built with CONFIG_RCU_NOCB_CPU=y this has the knock-on effect
    of reducing by one the number of rcuo kthreads per CPU.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 664b580695d6..c789c302a2c9 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -45,7 +45,11 @@ static inline void rcu_virt_note_context_switch(int cpu)
 	rcu_note_context_switch(false);
 }
 
-void synchronize_rcu_bh(void);
+static inline void synchronize_rcu_bh(void)
+{
+	synchronize_rcu();
+}
+
 void synchronize_sched_expedited(void);
 void synchronize_rcu_expedited(void);
 
@@ -69,7 +73,7 @@ void kfree_call_rcu(struct rcu_head *head, rcu_callback_t func);
  */
 static inline void synchronize_rcu_bh_expedited(void)
 {
-	synchronize_sched_expedited();
+	synchronize_rcu_expedited();
 }
 
 void rcu_barrier(void);

commit d28139c4e96713d52a300fb9036c5be2f45e0741
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Jun 28 14:45:25 2018 -0700

    rcu: Apply RCU-bh QSes to RCU-sched and RCU-preempt when safe
    
    One necessary step towards consolidating the three flavors of RCU is to
    make sure that the resulting consolidated "one flavor to rule them all"
    correctly handles networking denial-of-service attacks.  One thing that
    allows RCU-bh to do so is that __do_softirq() invokes rcu_bh_qs() every
    so often, and so something similar has to happen for consolidated RCU.
    
    This must be done carefully.  For example, if a preemption-disabled
    region of code takes an interrupt which does softirq processing before
    returning, consolidated RCU must ignore the resulting rcu_bh_qs()
    invocations -- preemption is still disabled, and that means an RCU
    reader for the consolidated flavor.
    
    This commit therefore creates a new rcu_softirq_qs() that is called only
    from the ksoftirqd task, thus avoiding the interrupted-a-preempted-region
    problem.  This new rcu_softirq_qs() function invokes rcu_sched_qs(),
    rcu_preempt_qs(), and rcu_preempt_deferred_qs().  The latter call handles
    any deferred quiescent states.
    
    Note that __do_softirq() still invokes rcu_bh_qs().  It will continue to
    do so until a later stage of cleanup when the RCU-bh flavor is removed.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    [ paulmck: Fix !SMP issue located by kbuild test robot. ]

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 914655848ef6..664b580695d6 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -30,6 +30,7 @@
 #ifndef __LINUX_RCUTREE_H
 #define __LINUX_RCUTREE_H
 
+void rcu_softirq_qs(void);
 void rcu_note_context_switch(bool preempt);
 int rcu_needs_cpu(u64 basem, u64 *nextevt);
 void rcu_cpu_stall_reset(void);

commit f64c6013a2029316ea552f99823d116ee5f5f955
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue May 22 09:50:53 2018 -0700

    rcu/x86: Provide early rcu_cpu_starting() callback
    
    The x86/mtrr code does horrific things because hardware. It uses
    stop_machine_from_inactive_cpu(), which does a wakeup (of the stopper
    thread on another CPU), which uses RCU, all before the CPU is onlined.
    
    RCU complains about this, because wakeups use RCU and RCU does
    (rightfully) not consider offline CPUs for grace-periods.
    
    Fix this by initializing RCU way early in the MTRR case.
    
    Tested-by: Mike Galbraith <efault@gmx.de>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    [ paulmck: Add !SMP support, per 0day Test Robot report. ]

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 448f20f27396..914655848ef6 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -101,5 +101,6 @@ int rcutree_online_cpu(unsigned int cpu);
 int rcutree_offline_cpu(unsigned int cpu);
 int rcutree_dead_cpu(unsigned int cpu);
 int rcutree_dying_cpu(unsigned int cpu);
+void rcu_cpu_starting(unsigned int cpu);
 
 #endif /* __LINUX_RCUTREE_H */

commit 17672480fb1e953f999623b598a98130f8aacfbc
Author: Yury Norov <ynorov@caviumnetworks.com>
Date:   Sun Mar 25 20:50:03 2018 +0300

    rcu: Declare rcu_eqs_special_set() in public header
    
    Because rcu_eqs_special_set() is declared only in internal header
    kernel/rcu/tree.h and stubbed in include/linux/rcutiny.h, it is
    inaccessible outside of the RCU implementation.  This patch therefore
    moves the  rcu_eqs_special_set() declaration to include/linux/rcutree.h,
    which allows it to be used in non-rcu kernel code.
    
    Signed-off-by: Yury Norov <ynorov@caviumnetworks.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Tested-by: Nicholas Piggin <npiggin@gmail.com>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index fd996cdf1833..448f20f27396 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -74,6 +74,7 @@ static inline void synchronize_rcu_bh_expedited(void)
 void rcu_barrier(void);
 void rcu_barrier_bh(void);
 void rcu_barrier_sched(void);
+bool rcu_eqs_special_set(int cpu);
 unsigned long get_state_synchronize_rcu(void);
 void cond_synchronize_rcu(unsigned long oldstate);
 unsigned long get_state_synchronize_sched(void);

commit 844ccdd7dce2c1a6ea9b437fcf8c3265b136e4a5
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Oct 3 16:51:47 2017 -0700

    rcu: Eliminate rcu_irq_enter_disabled()
    
    Now that the irq path uses the rcu_nmi_{enter,exit}() algorithm,
    rcu_irq_enter() and rcu_irq_exit() may be used from any context.  There is
    thus no need for rcu_irq_enter_disabled() and for the checks using it.
    This commit therefore eliminates rcu_irq_enter_disabled().
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 37d6fd3b7ff8..fd996cdf1833 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -85,7 +85,6 @@ void rcu_irq_enter(void);
 void rcu_irq_exit(void);
 void rcu_irq_enter_irqson(void);
 void rcu_irq_exit_irqson(void);
-bool rcu_irq_enter_disabled(void);
 
 void exit_rcu(void);
 

commit d2b1654f91f9e928011fbea7138854ee2044f470
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu May 11 12:01:50 2017 -0700

    rcu: Remove #ifdef moving rcu_end_inkernel_boot from rcupdate.h
    
    This commit removes a #ifdef and saves a few lines of code by moving
    the rcu_end_inkernel_boot() function from include/linux/rcupdate.h to
    include/linux/rcutiny.h (for TINY_RCU) and to include/linux/rcutree.h
    (for TREE_RCU).
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index d6aa89d15d47..37d6fd3b7ff8 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -91,6 +91,7 @@ void exit_rcu(void);
 
 void rcu_scheduler_starting(void);
 extern int rcu_scheduler_active __read_mostly;
+void rcu_end_inkernel_boot(void);
 bool rcu_is_watching(void);
 void rcu_all_qs(void);
 

commit fe21a27e8ca0937a5ac298de1f4b46382e9c5c88
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed May 3 13:45:51 2017 -0700

    rcu: Move rcu_request_urgent_qs_task() out of rcutiny.h and rcutree.h
    
    The rcu_request_urgent_qs_task() function is used only within RCU,
    so there is no point in exporting it to the rest of the kernel from
    nclude/linux/rcutiny.h and include/linux/rcutree.h.  This commit therefore
    moves this function to kernel/rcu/rcu.h.
    
    Reported-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 43113323ca09..d6aa89d15d47 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -91,10 +91,7 @@ void exit_rcu(void);
 
 void rcu_scheduler_starting(void);
 extern int rcu_scheduler_active __read_mostly;
-
 bool rcu_is_watching(void);
-void rcu_request_urgent_qs_task(struct task_struct *t);
-
 void rcu_all_qs(void);
 
 /* RCUtree hotplug events */

commit e3c8d51e1a58c73a557eb38a9a6afb4f704a3379
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed May 3 13:37:16 2017 -0700

    rcu: Move torture-related functions out of rcutiny.h and rcutree.h
    
    The various functions similar to rcu_batches_started(), the
    function show_rcu_gp_kthreads(), the various functions similar to
    rcu_force_quiescent_state(), and the variables rcutorture_testseq and
    rcutorture_vernum are used only within RCU.  There is therefore no point
    in exporting them to the kernel at large from include/linux/rcutiny.h
    and include/linux/rcutree.h.  This commit therefore moves all of these
    to kernel/rcu/rcu.h.
    
    Reported-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 28af91a19573..43113323ca09 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -79,22 +79,6 @@ void cond_synchronize_rcu(unsigned long oldstate);
 unsigned long get_state_synchronize_sched(void);
 void cond_synchronize_sched(unsigned long oldstate);
 
-extern unsigned long rcutorture_testseq;
-extern unsigned long rcutorture_vernum;
-unsigned long rcu_batches_started(void);
-unsigned long rcu_batches_started_bh(void);
-unsigned long rcu_batches_started_sched(void);
-unsigned long rcu_batches_completed(void);
-unsigned long rcu_batches_completed_bh(void);
-unsigned long rcu_batches_completed_sched(void);
-unsigned long rcu_exp_batches_completed(void);
-unsigned long rcu_exp_batches_completed_sched(void);
-void show_rcu_gp_kthreads(void);
-
-void rcu_force_quiescent_state(void);
-void rcu_bh_force_quiescent_state(void);
-void rcu_sched_force_quiescent_state(void);
-
 void rcu_idle_enter(void);
 void rcu_idle_exit(void);
 void rcu_irq_enter(void);

commit cad7b3897279c869de61dc88133037b941f84233
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed May 3 10:22:57 2017 -0700

    rcu: Move torture-related definitions from rcupdate.h to rcu.h
    
    The include/linux/rcupdate.h file contains a number of definitions that
    are used only to communicate between rcutorture, rcuperf, and the RCU code
    itself.  There is no point in having these definitions exposed globally
    throughout the kernel, so this commit moves them to kernel/rcu/rcu.h.
    This change has the added benefit of shrinking rcupdate.h.
    
    Reported-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 0bacb6b2af69..28af91a19573 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -101,6 +101,7 @@ void rcu_irq_enter(void);
 void rcu_irq_exit(void);
 void rcu_irq_enter_irqson(void);
 void rcu_irq_exit_irqson(void);
+bool rcu_irq_enter_disabled(void);
 
 void exit_rcu(void);
 

commit bcbfdd01dce5556a952fae84ef16fd0f12525e7b
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Apr 11 15:50:41 2017 -0700

    rcu: Make non-preemptive schedule be Tasks RCU quiescent state
    
    Currently, a call to schedule() acts as a Tasks RCU quiescent state
    only if a context switch actually takes place.  However, just the
    call to schedule() guarantees that the calling task has moved off of
    whatever tracing trampoline that it might have been one previously.
    This commit therefore plumbs schedule()'s "preempt" parameter into
    rcu_note_context_switch(), which then records the Tasks RCU quiescent
    state, but only if this call to schedule() was -not- due to a preemption.
    
    To avoid adding overhead to the common-case context-switch path,
    this commit hides the rcu_note_context_switch() check under an existing
    non-common-case check.
    
    Suggested-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 63a4e4cf40a5..0bacb6b2af69 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -30,7 +30,7 @@
 #ifndef __LINUX_RCUTREE_H
 #define __LINUX_RCUTREE_H
 
-void rcu_note_context_switch(void);
+void rcu_note_context_switch(bool preempt);
 int rcu_needs_cpu(u64 basem, u64 *nextevt);
 void rcu_cpu_stall_reset(void);
 
@@ -41,7 +41,7 @@ void rcu_cpu_stall_reset(void);
  */
 static inline void rcu_virt_note_context_switch(int cpu)
 {
-	rcu_note_context_switch();
+	rcu_note_context_switch(false);
 }
 
 void synchronize_rcu_bh(void);
@@ -108,6 +108,7 @@ void rcu_scheduler_starting(void);
 extern int rcu_scheduler_active __read_mostly;
 
 bool rcu_is_watching(void);
+void rcu_request_urgent_qs_task(struct task_struct *t);
 
 void rcu_all_qs(void);
 

commit 4df8374254ea9294dfe4b8c447a1b7eddc543dbf
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jul 13 17:17:03 2016 +0000

    rcu: Convert rcutree to hotplug state machine
    
    Straight forward conversion to the state machine. Though the question arises
    whether this needs really all these state transitions to work.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Anna-Maria Gleixner <anna-maria@linutronix.de>
    Reviewed-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: rt@linutronix.de
    Link: http://lkml.kernel.org/r/20160713153337.982013161@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 5043cb823fb2..63a4e4cf40a5 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -111,4 +111,11 @@ bool rcu_is_watching(void);
 
 void rcu_all_qs(void);
 
+/* RCUtree hotplug events */
+int rcutree_prepare_cpu(unsigned int cpu);
+int rcutree_online_cpu(unsigned int cpu);
+int rcutree_offline_cpu(unsigned int cpu);
+int rcutree_dead_cpu(unsigned int cpu);
+int rcutree_dying_cpu(unsigned int cpu);
+
 #endif /* __LINUX_RCUTREE_H */

commit 291783b8ad77a83a6fdf91d55eee7f1ad72ed4d1
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Jan 12 13:43:30 2016 -0800

    rcutorture: Expedited-GP batch progress access to torturing
    
    This commit provides rcu_exp_batches_completed() and
    rcu_exp_batches_completed_sched() functions to allow torture-test modules
    to check how many expedited grace period batches have completed.
    These are analogous to the existing rcu_batches_completed(),
    rcu_batches_completed_bh(), and rcu_batches_completed_sched() functions.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index ad1eda9fa4da..5043cb823fb2 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -87,6 +87,8 @@ unsigned long rcu_batches_started_sched(void);
 unsigned long rcu_batches_completed(void);
 unsigned long rcu_batches_completed_bh(void);
 unsigned long rcu_batches_completed_sched(void);
+unsigned long rcu_exp_batches_completed(void);
+unsigned long rcu_exp_batches_completed_sched(void);
 void show_rcu_gp_kthreads(void);
 
 void rcu_force_quiescent_state(void);

commit 7c9906ca5e582a773fff696975e312cef58a7386
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sat Oct 31 00:59:01 2015 -0700

    rcu: Don't redundantly disable irqs in rcu_irq_{enter,exit}()
    
    This commit replaces a local_irq_save()/local_irq_restore() pair with
    a lockdep assertion that interrupts are already disabled.  This should
    remove the corresponding overhead from the interrupt entry/exit fastpaths.
    
    This change was inspired by the fact that Iftekhar Ahmed's mutation
    testing showed that removing rcu_irq_enter()'s call to local_ird_restore()
    had no effect, which might indicate that interrupts were always enabled
    anyway.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 9d3eda39bcd2..ad1eda9fa4da 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -97,6 +97,8 @@ void rcu_idle_enter(void);
 void rcu_idle_exit(void);
 void rcu_irq_enter(void);
 void rcu_irq_exit(void);
+void rcu_irq_enter_irqson(void);
+void rcu_irq_exit_irqson(void);
 
 void exit_rcu(void);
 

commit 46a5d164db53ba6066b11889abb7fa6bddbe5cf7
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Oct 7 09:10:48 2015 -0700

    rcu: Stop disabling interrupts in scheduler fastpaths
    
    We need the scheduler's fastpaths to be, well, fast, and unnecessarily
    disabling and re-enabling interrupts is not necessarily consistent with
    this goal.  Especially given that there are regions of the scheduler that
    already have interrupts disabled.
    
    This commit therefore moves the call to rcu_note_context_switch()
    to one of the interrupts-disabled regions of the scheduler, and
    removes the now-redundant disabling and re-enabling of interrupts from
    rcu_note_context_switch() and the functions it calls.
    
    Reported-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    [ paulmck: Shift rcu_note_context_switch() to avoid deadlock, as suggested
      by Peter Zijlstra. ]

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 60d15a080d7c..9d3eda39bcd2 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -37,7 +37,7 @@ void rcu_cpu_stall_reset(void);
 /*
  * Note a virtualization-based context switch.  This is simply a
  * wrapper around rcu_note_context_switch(), which allows TINY_RCU
- * to save a few bytes.
+ * to save a few bytes. The caller must have disabled interrupts.
  */
 static inline void rcu_virt_note_context_switch(int cpu)
 {

commit b6a4ae766e3133a4db73fabc81e522d1601cb623
Author: Boqun Feng <boqun.feng@gmail.com>
Date:   Wed Jul 29 13:29:38 2015 +0800

    rcu: Use rcu_callback_t in call_rcu*() and friends
    
    As we now have rcu_callback_t typedefs as the type of rcu callbacks, we
    should use it in call_rcu*() and friends as the type of parameters. This
    could save us a few lines of code and make it clear which function
    requires an rcu callbacks rather than other callbacks as its argument.
    
    Besides, this can also help cscope to generate a better database for
    code reading.
    
    Signed-off-by: Boqun Feng <boqun.feng@gmail.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 5abec82f325e..60d15a080d7c 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -48,7 +48,7 @@ void synchronize_rcu_bh(void);
 void synchronize_sched_expedited(void);
 void synchronize_rcu_expedited(void);
 
-void kfree_call_rcu(struct rcu_head *head, void (*func)(struct rcu_head *rcu));
+void kfree_call_rcu(struct rcu_head *head, rcu_callback_t func);
 
 /**
  * synchronize_rcu_bh_expedited - Brute-force RCU-bh grace period

commit 24560056de61d86153cecb84d04e4237437f5888
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sat May 30 10:11:24 2015 -0700

    rcu: Add RCU-sched flavors of get-state and cond-sync
    
    The get_state_synchronize_rcu() and cond_synchronize_rcu() functions
    allow polling for grace-period completion, with an actual wait for a
    grace period occurring only when cond_synchronize_rcu() is called too
    soon after the corresponding get_state_synchronize_rcu().  However,
    these functions work only for vanilla RCU.  This commit adds the
    get_state_synchronize_sched() and cond_synchronize_sched(), which provide
    the same capability for RCU-sched.
    
    Reported-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 456879143f89..5abec82f325e 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -76,6 +76,8 @@ void rcu_barrier_bh(void);
 void rcu_barrier_sched(void);
 unsigned long get_state_synchronize_rcu(void);
 void cond_synchronize_rcu(unsigned long oldstate);
+unsigned long get_state_synchronize_sched(void);
+void cond_synchronize_sched(unsigned long oldstate);
 
 extern unsigned long rcutorture_testseq;
 extern unsigned long rcutorture_vernum;

commit 43224b96af3154cedd7220f7b90094905f07ac78
Merge: d70b3ef54cea 1cb6c2151850
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 22 18:57:44 2015 -0700

    Merge branch 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull timer updates from Thomas Gleixner:
     "A rather largish update for everything time and timer related:
    
       - Cache footprint optimizations for both hrtimers and timer wheel
    
       - Lower the NOHZ impact on systems which have NOHZ or timer migration
         disabled at runtime.
    
       - Optimize run time overhead of hrtimer interrupt by making the clock
         offset updates smarter
    
       - hrtimer cleanups and removal of restrictions to tackle some
         problems in sched/perf
    
       - Some more leap second tweaks
    
       - Another round of changes addressing the 2038 problem
    
       - First step to change the internals of clock event devices by
         introducing the necessary infrastructure
    
       - Allow constant folding for usecs/msecs_to_jiffies()
    
       - The usual pile of clockevent/clocksource driver updates
    
      The hrtimer changes contain updates to sched, perf and x86 as they
      depend on them plus changes all over the tree to cleanup API changes
      and redundant code, which got copied all over the place.  The y2038
      changes touch s390 to remove the last non 2038 safe code related to
      boot/persistant clock"
    
    * 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (114 commits)
      clocksource: Increase dependencies of timer-stm32 to limit build wreckage
      timer: Minimize nohz off overhead
      timer: Reduce timer migration overhead if disabled
      timer: Stats: Simplify the flags handling
      timer: Replace timer base by a cpu index
      timer: Use hlist for the timer wheel hash buckets
      timer: Remove FIFO "guarantee"
      timers: Sanitize catchup_timer_jiffies() usage
      hrtimer: Allow hrtimer::function() to free the timer
      seqcount: Introduce raw_write_seqcount_barrier()
      seqcount: Rename write_seqcount_barrier()
      hrtimer: Fix hrtimer_is_queued() hole
      hrtimer: Remove HRTIMER_STATE_MIGRATE
      selftest: Timers: Avoid signal deadlock in leap-a-day
      timekeeping: Copy the shadow-timekeeper over the real timekeeper last
      clockevents: Check state instead of mode in suspend/resume path
      selftests: timers: Add leap-second timer edge testing to leap-a-day.c
      ntp: Do leapsecond adjustment in adjtimex read path
      time: Prevent early expiry of hrtimers[CLOCK_REALTIME] at the leap second edge
      ntp: Introduce and use SECS_PER_DAY macro instead of 86400
      ...

commit 0868aa22167d93dd974c253d259c3e6fd47a16c8
Merge: 29c6820f5164 ed38446424dd f517700cce37 927da9dfd13a 1ce46ee597bc 6e91f8cb1386 7d3bb54adeb1
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed May 27 13:00:49 2015 -0700

    Merge branches 'array.2015.05.27a', 'doc.2015.05.27a', 'fixes.2015.05.27a', 'hotplug.2015.05.27a', 'init.2015.05.27a', 'tiny.2015.05.27a' and 'torture.2015.05.27a' into HEAD
    
    array.2015.05.27a:  Remove all uses of RCU-protected array indexes.
    doc.2015.05.27a:  Docuemntation updates.
    fixes.2015.05.27a:  Miscellaneous fixes.
    hotplug.2015.05.27a:  CPU-hotplug updates.
    init.2015.05.27a:  Initialization/Kconfig updates.
    tiny.2015.05.27a:  Updates to Tiny RCU.
    torture.2015.05.27a:  Torture-testing updates.

commit 51952bc633064311410b041fad38da1614f4539e
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Apr 21 11:15:30 2015 -0700

    rcu: Further shrink Tiny RCU by making empty functions static inlines
    
    The Tiny RCU counterparts to rcu_idle_enter(), rcu_idle_exit(),
    rcu_irq_enter(), and rcu_irq_exit() are empty functions, but each has
    EXPORT_SYMBOL_GPL(), which needlessly consumes extra memory, especially
    in kernels built with module support.  This commit therefore moves these
    functions to static inlines in rcutiny.h, removing the need for exports.
    
    This won't affect the size of the tiniest kernels, which are likely
    built without module support, but might help semi-tiny kernels that
    might include module support.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index d2e583a6aaca..f22d83f49e56 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -93,6 +93,11 @@ void rcu_force_quiescent_state(void);
 void rcu_bh_force_quiescent_state(void);
 void rcu_sched_force_quiescent_state(void);
 
+void rcu_idle_enter(void);
+void rcu_idle_exit(void);
+void rcu_irq_enter(void);
+void rcu_irq_exit(void);
+
 void exit_rcu(void);
 
 void rcu_scheduler_starting(void);

commit 3382adbc1bb8c80ea512243acf6059564287620b
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Mar 4 15:41:24 2015 -0800

    rcu: Eliminate a few CONFIG_RCU_NOCB_CPU_ALL #ifdefs
    
    This commit converts several CONFIG_RCU_NOCB_CPU_ALL #ifdefs to
    instead use IS_ENABLED().  This change should help avoid hiding
    code from compiler diagnostics.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index d2e583a6aaca..0bd400b02430 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -31,9 +31,7 @@
 #define __LINUX_RCUTREE_H
 
 void rcu_note_context_switch(void);
-#ifndef CONFIG_RCU_NOCB_CPU_ALL
 int rcu_needs_cpu(unsigned long *delta_jiffies);
-#endif /* #ifndef CONFIG_RCU_NOCB_CPU_ALL */
 void rcu_cpu_stall_reset(void);
 
 /*

commit c1ad348b452aacd784fb97403d03d71723c72ee1
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Apr 14 21:08:58 2015 +0000

    tick: Nohz: Rework next timer evaluation
    
    The evaluation of the next timer in the nohz code is based on jiffies
    while all the tick internals are nano seconds based. We have also to
    convert hrtimer nanoseconds to jiffies in the !highres case. That's
    just wrong and introduces interesting corner cases.
    
    Turn it around and convert the next timer wheel timer expiry and the
    rcu event to clock monotonic and base all calculations on
    nanoseconds. That identifies the case where no timer is pending
    clearly with an absolute expiry value of KTIME_MAX.
    
    Makes the code more readable and gets rid of the jiffies magic in the
    nohz code.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Cc: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Link: http://lkml.kernel.org/r/20150414203502.184198593@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index d2e583a6aaca..db2e31beaae7 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -32,7 +32,7 @@
 
 void rcu_note_context_switch(void);
 #ifndef CONFIG_RCU_NOCB_CPU_ALL
-int rcu_needs_cpu(unsigned long *delta_jiffies);
+int rcu_needs_cpu(u64 basem, u64 *nextevt);
 #endif /* #ifndef CONFIG_RCU_NOCB_CPU_ALL */
 void rcu_cpu_stall_reset(void);
 

commit 78e691f4ae2d5edea0199ca802bb505b9cdced88
Merge: d87510c5a6e3 60479676eb6e ab954c167ed9 83fe27ea5311 630181c4a915 7602de4af192
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Jan 15 23:34:34 2015 -0800

    Merge branches 'doc.2015.01.07a', 'fixes.2015.01.15a', 'preempt.2015.01.06a', 'srcu.2015.01.06a', 'stall.2015.01.16a' and 'torture.2015.01.11a' into HEAD
    
    doc.2015.01.07a: Documentation updates.
    fixes.2015.01.15a: Miscellaneous fixes.
    preempt.2015.01.06a: Changes to handling of lists of preempted tasks.
    srcu.2015.01.06a: SRCU updates.
    stall.2015.01.16a: RCU CPU stall-warning updates and fixes.
    torture.2015.01.11a: RCU torture-test updates and fixes.

commit 5cd37193ce8539be1e6ef76be226f4bcc984e0f5
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sat Dec 13 20:32:04 2014 -0800

    rcu: Make cond_resched_rcu_qs() apply to normal RCU flavors
    
    Although cond_resched_rcu_qs() only applies to TASKS_RCU, it is used
    in places where it would be useful for it to apply to the normal RCU
    flavors, rcu_preempt, rcu_sched, and rcu_bh.  This is especially the
    case for workloads that aggressively overload the system, particularly
    those that generate large numbers of RCU updates on systems running
    NO_HZ_FULL CPUs.  This commit therefore communicates quiescent states
    from cond_resched_rcu_qs() to the normal RCU flavors.
    
    Note that it is unfortunately necessary to leave the old ->passed_quiesce
    mechanism in place to allow quiescent states that apply to only one
    flavor to be recorded.  (Yes, we could decrement ->rcu_qs_ctr_snap in
    that case, but that is not so good for debugging of RCU internals.)
    In addition, if one of the RCU flavor's grace period has stalled, this
    will invoke rcu_momentary_dyntick_idle(), resulting in a heavy-weight
    quiescent state visible from other CPUs.
    
    Reported-by: Sasha Levin <sasha.levin@oracle.com>
    Reported-by: Dave Jones <davej@redhat.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    [ paulmck: Merge commit from Sasha Levin fixing a bug where __this_cpu()
      was used in preemptible code. ]

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 52953790dcca..ddba927f7316 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -97,4 +97,6 @@ extern int rcu_scheduler_active __read_mostly;
 
 bool rcu_is_watching(void);
 
+void rcu_all_qs(void);
+
 #endif /* __LINUX_RCUTREE_H */

commit 917963d0b30f9c4153c372c165178501d97b6b55
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Nov 21 17:10:16 2014 -0800

    rcutorture: Check from beginning to end of grace period
    
    Currently, rcutorture's Reader Batch checks measure from the end of
    the previous grace period to the end of the current one.  This commit
    tightens up these checks by measuring from the start and end of the same
    grace period.  This involves adding rcu_batches_started() and friends
    corresponding to the existing rcu_batches_completed() and friends.
    
    We leave SRCU alone for the moment, as it does not yet have a way of
    tracking both ends of its grace periods.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 9885bfb6b123..c0dd124e69ec 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -81,6 +81,9 @@ void cond_synchronize_rcu(unsigned long oldstate);
 
 extern unsigned long rcutorture_testseq;
 extern unsigned long rcutorture_vernum;
+unsigned long rcu_batches_started(void);
+unsigned long rcu_batches_started_bh(void);
+unsigned long rcu_batches_started_sched(void);
 unsigned long rcu_batches_completed(void);
 unsigned long rcu_batches_completed_bh(void);
 unsigned long rcu_batches_completed_sched(void);

commit 9733e4f0a973a354034f5dd603b4142a3095c85f
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Nov 21 12:49:13 2014 -0800

    rcu: Make _batches_completed() functions return unsigned long
    
    Long ago, the various ->completed fields were of type long, but now are
    unsigned long due to signed-integer-overflow concerns.  However, the
    various _batches_completed() functions remained of type long, even though
    their only purpose in life is to return the corresponding ->completed
    field.  This patch cleans this up by changing these functions' return
    types to unsigned long.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 52953790dcca..9885bfb6b123 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -81,9 +81,9 @@ void cond_synchronize_rcu(unsigned long oldstate);
 
 extern unsigned long rcutorture_testseq;
 extern unsigned long rcutorture_vernum;
-long rcu_batches_completed(void);
-long rcu_batches_completed_bh(void);
-long rcu_batches_completed_sched(void);
+unsigned long rcu_batches_completed(void);
+unsigned long rcu_batches_completed_bh(void);
+unsigned long rcu_batches_completed_sched(void);
 void show_rcu_gp_kthreads(void);
 
 void rcu_force_quiescent_state(void);

commit aa6da5140b784ece799f670bf532096f67aa7785
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Oct 21 13:23:08 2014 -0700

    rcu: Remove "cpu" argument to rcu_needs_cpu()
    
    The "cpu" argument to rcu_needs_cpu() is always the current CPU, so drop
    it.  This in turn allows the "cpu" argument to rcu_cpu_has_callbacks()
    to be removed, which allows the uses of "cpu" in both functions to be
    replaced with a this_cpu_ptr().  Again, the anticipated cross-CPU uses
    of these functions has been replaced by NO_HZ_FULL.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Pranith Kumar <bobby.prani@gmail.com>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 7b5484db1857..52953790dcca 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -32,7 +32,7 @@
 
 void rcu_note_context_switch(void);
 #ifndef CONFIG_RCU_NOCB_CPU_ALL
-int rcu_needs_cpu(int cpu, unsigned long *delta_jiffies);
+int rcu_needs_cpu(unsigned long *delta_jiffies);
 #endif /* #ifndef CONFIG_RCU_NOCB_CPU_ALL */
 void rcu_cpu_stall_reset(void);
 

commit 38200cf24702e5d79ce6c8f4c62036c41845c62d
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Oct 21 12:50:04 2014 -0700

    rcu: Remove "cpu" argument to rcu_note_context_switch()
    
    The "cpu" argument to rcu_note_context_switch() is always the current
    CPU, so drop it.  This in turn allows the "cpu" argument to
    rcu_preempt_note_context_switch() to be removed, which allows the sole
    use of "cpu" in both functions to be replaced with a this_cpu_ptr().
    Again, the anticipated cross-CPU uses of these functions has been
    replaced by NO_HZ_FULL.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Pranith Kumar <bobby.prani@gmail.com>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 3e2f5d432743..7b5484db1857 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -30,7 +30,7 @@
 #ifndef __LINUX_RCUTREE_H
 #define __LINUX_RCUTREE_H
 
-void rcu_note_context_switch(int cpu);
+void rcu_note_context_switch(void);
 #ifndef CONFIG_RCU_NOCB_CPU_ALL
 int rcu_needs_cpu(int cpu, unsigned long *delta_jiffies);
 #endif /* #ifndef CONFIG_RCU_NOCB_CPU_ALL */
@@ -43,7 +43,7 @@ void rcu_cpu_stall_reset(void);
  */
 static inline void rcu_virt_note_context_switch(int cpu)
 {
-	rcu_note_context_switch(cpu);
+	rcu_note_context_switch();
 }
 
 void synchronize_rcu_bh(void);

commit afea227fd4acf4f097a9e77bbc2f07d4856ebd01
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Mar 12 07:10:41 2014 -0700

    rcutorture: Export RCU grace-period kthread wait state to rcutorture
    
    This commit allows rcutorture to print additional state for the
    RCU grace-period kthreads in cases where RCU seems reluctant to
    start a new grace period.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index a59ca05fd4e3..3e2f5d432743 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -84,6 +84,7 @@ extern unsigned long rcutorture_vernum;
 long rcu_batches_completed(void);
 long rcu_batches_completed_bh(void);
 long rcu_batches_completed_sched(void);
+void show_rcu_gp_kthreads(void);
 
 void rcu_force_quiescent_state(void);
 void rcu_bh_force_quiescent_state(void);

commit 765a3f4fed708ae429ee095914a7897acb3a65bd
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Mar 14 16:37:08 2014 -0700

    rcu: Provide grace-period piggybacking API
    
    The following pattern is currently not well supported by RCU:
    
    1.      Make data element inaccessible to RCU readers.
    
    2.      Do work that probably lasts for more than one grace period.
    
    3.      Do something to make sure RCU readers in flight before #1 above
            have completed.
    
    Here are some things that could currently be done:
    
    a.      Do a synchronize_rcu() unconditionally at either #1 or #3 above.
            This works, but imposes needless work and latency.
    
    b.      Post an RCU callback at #1 above that does a wakeup, then
            wait for the wakeup at #3.  This works well, but likely results
            in an extra unneeded grace period.  Open-coding this is also
            a bit more semi-tricky code than would be good.
    
    This commit therefore adds get_state_synchronize_rcu() and
    cond_synchronize_rcu() APIs.  Call get_state_synchronize_rcu() at #1
    above and pass its return value to cond_synchronize_rcu() at #3 above.
    This results in a call to synchronize_rcu() if no grace period has
    elapsed between #1 and #3, but requires only a load, comparison, and
    memory barrier if a full grace period did elapse.
    
    Requested-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index e9c63884df0a..a59ca05fd4e3 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -76,6 +76,8 @@ static inline void synchronize_rcu_bh_expedited(void)
 void rcu_barrier(void);
 void rcu_barrier_bh(void);
 void rcu_barrier_sched(void);
+unsigned long get_state_synchronize_rcu(void);
+void cond_synchronize_rcu(unsigned long oldstate);
 
 extern unsigned long rcutorture_testseq;
 extern unsigned long rcutorture_vernum;

commit 322efba5b6442f331ac8aa24e92a817d804cc938
Merge: 8dd853d7b6ef 5cb5c6e18f82 f1f399d1281e
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Feb 26 06:36:09 2014 -0800

    Merge branches 'doc.2014.02.24a', 'fixes.2014.02.26a' and 'rt.2014.02.17b' into HEAD
    
    doc.2014.02.24a: Documentation changes
    fixes.2014.02.26a: Miscellaneous fixes
    rt.2014.02.17b: Response-time-related changes

commit ffa83fb565fbc397cbafb4b71fd1cce276d4c3b6
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sun Nov 17 19:27:16 2013 -0800

    rcu: Optimize rcu_needs_cpu() for RCU_NOCB_CPU_ALL
    
    If CONFIG_RCU_NOCB_CPU_ALL=y, then rcu_needs_cpu() will always
    return false, however, the current version nevertheless checks
    for RCU callbacks.  This commit therefore creates a static inline
    implementation of rcu_needs_cpu() that unconditionally returns false
    when CONFIG_RCU_NOCB_CPU_ALL=y.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 72137ee8c603..81198c84e268 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -31,7 +31,9 @@
 #define __LINUX_RCUTREE_H
 
 void rcu_note_context_switch(int cpu);
+#ifndef CONFIG_RCU_NOCB_CPU_ALL
 int rcu_needs_cpu(int cpu, unsigned long *delta_jiffies);
+#endif /* #ifndef CONFIG_RCU_NOCB_CPU_ALL */
 void rcu_cpu_stall_reset(void);
 
 /*

commit 87de1cfdc55b16b794e245b07322340725149d62
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Dec 3 10:02:52 2013 -0800

    rcu: Stop tracking FSF's postal address
    
    All of the RCU source files have the usual GPL header, which contains a
    long-obsolete postal address for FSF.  To avoid the need to track the
    FSF office's movements, this commit substitutes the URL where GPL may
    be found.
    
    Reported-by: Greg KH <gregkh@linuxfoundation.org>
    Reported-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 72137ee8c603..08b084068967 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -12,8 +12,8 @@
  * GNU General Public License for more details.
  *
  * You should have received a copy of the GNU General Public License
- * along with this program; if not, write to the Free Software
- * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
  *
  * Copyright IBM Corporation, 2008
  *

commit 584dc4ce55267765b415a8517613d1207f1741e5
Author: Teodora Baluta <teobaluta@gmail.com>
Date:   Mon Nov 11 17:11:23 2013 +0200

    rcu: Remove "extern" from function declarations in include/linux/*rcu*.h
    
    Function prototypes don't need to have the "extern" keyword since this
    is the default behavior. Its explicit use is redundant.  This commit
    therefore removes them.
    
    Signed-off-by: Teodora Baluta <teobaluta@gmail.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 4b9c81548742..72137ee8c603 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -30,9 +30,9 @@
 #ifndef __LINUX_RCUTREE_H
 #define __LINUX_RCUTREE_H
 
-extern void rcu_note_context_switch(int cpu);
-extern int rcu_needs_cpu(int cpu, unsigned long *delta_jiffies);
-extern void rcu_cpu_stall_reset(void);
+void rcu_note_context_switch(int cpu);
+int rcu_needs_cpu(int cpu, unsigned long *delta_jiffies);
+void rcu_cpu_stall_reset(void);
 
 /*
  * Note a virtualization-based context switch.  This is simply a
@@ -44,9 +44,9 @@ static inline void rcu_virt_note_context_switch(int cpu)
 	rcu_note_context_switch(cpu);
 }
 
-extern void synchronize_rcu_bh(void);
-extern void synchronize_sched_expedited(void);
-extern void synchronize_rcu_expedited(void);
+void synchronize_rcu_bh(void);
+void synchronize_sched_expedited(void);
+void synchronize_rcu_expedited(void);
 
 void kfree_call_rcu(struct rcu_head *head, void (*func)(struct rcu_head *rcu));
 
@@ -71,25 +71,25 @@ static inline void synchronize_rcu_bh_expedited(void)
 	synchronize_sched_expedited();
 }
 
-extern void rcu_barrier(void);
-extern void rcu_barrier_bh(void);
-extern void rcu_barrier_sched(void);
+void rcu_barrier(void);
+void rcu_barrier_bh(void);
+void rcu_barrier_sched(void);
 
 extern unsigned long rcutorture_testseq;
 extern unsigned long rcutorture_vernum;
-extern long rcu_batches_completed(void);
-extern long rcu_batches_completed_bh(void);
-extern long rcu_batches_completed_sched(void);
+long rcu_batches_completed(void);
+long rcu_batches_completed_bh(void);
+long rcu_batches_completed_sched(void);
 
-extern void rcu_force_quiescent_state(void);
-extern void rcu_bh_force_quiescent_state(void);
-extern void rcu_sched_force_quiescent_state(void);
+void rcu_force_quiescent_state(void);
+void rcu_bh_force_quiescent_state(void);
+void rcu_sched_force_quiescent_state(void);
 
-extern void exit_rcu(void);
+void exit_rcu(void);
 
-extern void rcu_scheduler_starting(void);
+void rcu_scheduler_starting(void);
 extern int rcu_scheduler_active __read_mostly;
 
-extern bool rcu_is_watching(void);
+bool rcu_is_watching(void);
 
 #endif /* __LINUX_RCUTREE_H */

commit 5c173eb8bcb9c1aa888bd6d14a4cb746f3dd2420
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Sep 13 17:20:11 2013 -0700

    rcu: Consistent rcu_is_watching() naming
    
    The old rcu_is_cpu_idle() function is just __rcu_is_watching() with
    preemption disabled.  This commit therefore renames rcu_is_cpu_idle()
    to rcu_is_watching.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 293613dfd2a5..4b9c81548742 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -90,6 +90,6 @@ extern void exit_rcu(void);
 extern void rcu_scheduler_starting(void);
 extern int rcu_scheduler_active __read_mostly;
 
-extern bool __rcu_is_watching(void);
+extern bool rcu_is_watching(void);
 
 #endif /* __LINUX_RCUTREE_H */

commit cc6783f788d8fe8b23ec6fc2762f5e8c9a418eee
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Sep 6 17:39:49 2013 -0700

    rcu: Is it safe to enter an RCU read-side critical section?
    
    There is currently no way for kernel code to determine whether it
    is safe to enter an RCU read-side critical section, in other words,
    whether or not RCU is paying attention to the currently running CPU.
    Given the large and increasing quantity of code shared by the idle loop
    and non-idle code, the this shortcoming is becoming increasingly painful.
    
    This commit therefore adds __rcu_is_watching(), which returns true if
    it is safe to enter an RCU read-side critical section on the currently
    running CPU.  This function is quite fast, using only a __this_cpu_read().
    However, the caller must disable preemption.
    
    Reported-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 226169d1bd2b..293613dfd2a5 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -90,4 +90,6 @@ extern void exit_rcu(void);
 extern void rcu_scheduler_starting(void);
 extern int rcu_scheduler_active __read_mostly;
 
+extern bool __rcu_is_watching(void);
+
 #endif /* __LINUX_RCUTREE_H */

commit 2439b696cb5303f1eeb6aeebcee19e0056c3dd6e
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Apr 11 10:15:52 2013 -0700

    rcu: Shrink TINY_RCU by moving exit_rcu()
    
    Now that TINY_PREEMPT_RCU is no more, exit_rcu() is always an empty
    function.  But if TINY_RCU is going to have an empty function, it should
    be in include/linux/rcutiny.h, where it does not bloat the kernel.
    This commit therefore moves exit_rcu() out of kernel/rcupdate.c to
    kernel/rcutree_plugin.h, and places a static inline empty function in
    include/linux/rcutiny.h in order to shrink TINY_RCU a bit.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 3f1aa8f98666..226169d1bd2b 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -85,6 +85,8 @@ extern void rcu_force_quiescent_state(void);
 extern void rcu_bh_force_quiescent_state(void);
 extern void rcu_sched_force_quiescent_state(void);
 
+extern void exit_rcu(void);
+
 extern void rcu_scheduler_starting(void);
 extern int rcu_scheduler_active __read_mostly;
 

commit 9dc5ad32488a75504349372330cc228d4dd678db
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Mar 27 10:11:15 2013 -0700

    rcu: Simplify RCU_TINY RCU callback invocation
    
    TINY_PREEMPT_RCU could use a kthread to handle RCU callback invocation,
    which required an API to abstract kthread vs. softirq invocation.
    Now that TINY_PREEMPT_RCU is no longer with us, this commit retires
    this API in favor of direct use of the relevant softirq primitives.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 952b79339304..3f1aa8f98666 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -30,7 +30,6 @@
 #ifndef __LINUX_RCUTREE_H
 #define __LINUX_RCUTREE_H
 
-extern void rcu_init(void);
 extern void rcu_note_context_switch(int cpu);
 extern int rcu_needs_cpu(int cpu, unsigned long *delta_jiffies);
 extern void rcu_cpu_stall_reset(void);

commit aa9b16306e3243229580ff889cc59fd66bf77973
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Thu May 10 16:41:44 2012 -0700

    rcu: Precompute RCU_FAST_NO_HZ timer offsets
    
    When a CPU is entering dyntick-idle mode, tick_nohz_stop_sched_tick()
    calls rcu_needs_cpu() see if RCU needs that CPU, and, if not, computes the
    next wakeup time based on the timer wheels.  Only later, when actually
    entering the idle loop, rcu_prepare_for_idle() will be invoked.  In some
    cases, rcu_prepare_for_idle() will post timers to wake the CPU back up.
    But all for naught: The next wakeup time for the CPU has already been
    computed, and posting a timer afterwards does not force that wakeup
    time to be recomputed.  This means that rcu_prepare_for_idle()'s have
    no effect.
    
    This is not a problem on a busy system because something else will wake
    up the CPU soon enough.  However, on lightly loaded systems, the CPU
    might stay asleep for a considerable length of time.  If that CPU has
    a callback that the rest of the system is waiting on, the system might
    run very slowly or (in theory) even hang.
    
    This commit avoids this problem by having rcu_needs_cpu() give
    tick_nohz_stop_sched_tick() an estimate of when RCU will need the CPU
    to wake back up, which tick_nohz_stop_sched_tick() takes into account
    when programming the CPU's wakeup time.  An alternative approach is
    for rcu_prepare_for_idle() to use hrtimers instead of normal timers,
    but timers are much more efficient than are hrtimers for frequently
    and repeatedly posting and cancelling a given timer, which is exactly
    what RCU_FAST_NO_HZ does.
    
    Reported-by: Pascal Chapperon <pascal.chapperon@wanadoo.fr>
    Reported-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Tested-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Tested-by: Pascal Chapperon <pascal.chapperon@wanadoo.fr>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 3c6083cde4fc..952b79339304 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -32,7 +32,7 @@
 
 extern void rcu_init(void);
 extern void rcu_note_context_switch(int cpu);
-extern int rcu_needs_cpu(int cpu);
+extern int rcu_needs_cpu(int cpu, unsigned long *delta_jiffies);
 extern void rcu_cpu_stall_reset(void);
 
 /*

commit dc36be4419311fd57becdf54bfeef6bd04a6741d
Merge: b1420f1c8bfc 048a0e8f5e1d 9dd8fb16c361 9fab97876af8
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri May 11 10:14:09 2012 -0700

    Merge branches 'barrier.2012.05.09a', 'fixes.2012.04.26a', 'inline.2012.05.02b' and 'srcu.2012.05.07b' into HEAD
    
    barrier:  Reduce the amount of disturbance by rcu_barrier() to the rest of
            the system.  This branch also includes improvements to
            RCU_FAST_NO_HZ, which are included here due to conflicts.
    fixes:  Miscellaneous fixes.
    inline:  Remaining changes from an abortive attempt to inline
            preemptible RCU's __rcu_read_lock().  These are (1) making
            exit_rcu() avoid unnecessary work and (2) avoiding having
            preemptible RCU record a blocked thread when the scheduler
            declines to do a context switch.
    srcu:   Lai Jiangshan's algorithmic implementation of SRCU, including
            call_srcu().

commit 9dd8fb16c36178df2066387d2abd44d8b4dca8c8
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Fri Apr 13 12:54:22 2012 -0700

    rcu: Make exit_rcu() more precise and consolidate
    
    When running preemptible RCU, if a task exits in an RCU read-side
    critical section having blocked within that same RCU read-side critical
    section, the task must be removed from the list of tasks blocking a
    grace period (perhaps the current grace period, perhaps the next grace
    period, depending on timing).  The exit() path invokes exit_rcu() to
    do this cleanup.
    
    However, the current implementation of exit_rcu() needlessly does the
    cleanup even if the task did not block within the current RCU read-side
    critical section, which wastes time and needlessly increases the size
    of the state space.  Fix this by only doing the cleanup if the current
    task is actually on the list of tasks blocking some grace period.
    
    While we are at it, consolidate the two identical exit_rcu() functions
    into a single function.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Tested-by: Linus Torvalds <torvalds@linux-foundation.org>
    
    Conflicts:
    
            kernel/rcupdate.c

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index e8ee5dd0854c..782a8ab51bc1 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -45,18 +45,6 @@ static inline void rcu_virt_note_context_switch(int cpu)
 	rcu_note_context_switch(cpu);
 }
 
-#ifdef CONFIG_TREE_PREEMPT_RCU
-
-extern void exit_rcu(void);
-
-#else /* #ifdef CONFIG_TREE_PREEMPT_RCU */
-
-static inline void exit_rcu(void)
-{
-}
-
-#endif /* #else #ifdef CONFIG_TREE_PREEMPT_RCU */
-
 extern void synchronize_rcu_bh(void);
 extern void synchronize_sched_expedited(void);
 extern void synchronize_rcu_expedited(void);

commit 6d8133919bac4270883b24328500875a49e71b36
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Thu Feb 23 13:30:16 2012 -0800

    rcu: Document why rcu_blocking_is_gp() is safe
    
    The rcu_blocking_is_gp() function tests to see if there is only one
    online CPU, and if so, synchronize_sched() and friends become no-ops.
    However, for larger systems, num_online_cpus() scans a large vector,
    and might be preempted while doing so.  While preempted, any number
    of CPUs might come online and go offline, potentially resulting in
    num_online_cpus() returning 1 when there never had only been one
    CPU online.  This could result in a too-short RCU grace period, which
    could in turn result in total failure, except that the only way that
    the grace period is too short is if there is an RCU read-side critical
    section spanning it.  For RCU-sched and RCU-bh (which are the only
    cases using rcu_blocking_is_gp()), RCU read-side critical sections
    have either preemption or bh disabled, which prevents CPUs from going
    offline.  This in turn prevents actual failures from occurring.
    
    This commit therefore adds a large block comment to rcu_blocking_is_gp()
    documenting why it is safe.  This commit also moves rcu_blocking_is_gp()
    into kernel/rcutree.c, which should help prevent unwary developers from
    mistaking it for a generally useful function.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index e8ee5dd0854c..b06363055ef8 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -98,13 +98,6 @@ extern void rcu_force_quiescent_state(void);
 extern void rcu_bh_force_quiescent_state(void);
 extern void rcu_sched_force_quiescent_state(void);
 
-/* A context switch is a grace period for RCU-sched and RCU-bh. */
-static inline int rcu_blocking_is_gp(void)
-{
-	might_sleep();  /* Check for RCU read-side critical section. */
-	return num_online_cpus() == 1;
-}
-
 extern void rcu_scheduler_starting(void);
 extern int rcu_scheduler_active __read_mostly;
 

commit 236fefafe5d3d34b78ed2ccf5510909716112326
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Jan 31 14:00:41 2012 -0800

    rcu: Call out dangers of expedited RCU primitives
    
    The expedited RCU primitives can be quite useful, but they have some
    high costs as well.  This commit updates and creates docbook comments
    calling out the costs, and updates the RCU documentation as well.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 73892483fd05..e8ee5dd0854c 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -63,6 +63,22 @@ extern void synchronize_rcu_expedited(void);
 
 void kfree_call_rcu(struct rcu_head *head, void (*func)(struct rcu_head *rcu));
 
+/**
+ * synchronize_rcu_bh_expedited - Brute-force RCU-bh grace period
+ *
+ * Wait for an RCU-bh grace period to elapse, but use a "big hammer"
+ * approach to force the grace period to end quickly.  This consumes
+ * significant time on all CPUs and is unfriendly to real-time workloads,
+ * so is thus not recommended for any sort of common-case code.  In fact,
+ * if you are using synchronize_rcu_bh_expedited() in a loop, please
+ * restructure your code to batch your updates, and then use a single
+ * synchronize_rcu_bh() instead.
+ *
+ * Note that it is illegal to call this function while holding any lock
+ * that is acquired by a CPU-hotplug notifier.  And yes, it is also illegal
+ * to call this function from a CPU-hotplug notifier.  Failing to observe
+ * these restriction will result in deadlock.
+ */
 static inline void synchronize_rcu_bh_expedited(void)
 {
 	synchronize_sched_expedited();

commit 486e259340fc4c60474f2c14703e3b3634bb58ca
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Fri Jan 6 14:11:30 2012 -0800

    rcu: Avoid waking up CPUs having only kfree_rcu() callbacks
    
    When CONFIG_RCU_FAST_NO_HZ is enabled, RCU will allow a given CPU to
    enter dyntick-idle mode even if it still has RCU callbacks queued.
    RCU avoids system hangs in this case by scheduling a timer for several
    jiffies in the future.  However, if all of the callbacks on that CPU
    are from kfree_rcu(), there is no reason to wake the CPU up, as it is
    not a problem to defer freeing of memory.
    
    This commit therefore tracks the number of callbacks on a given CPU
    that are from kfree_rcu(), and avoids scheduling the timer if all of
    a given CPU's callbacks are from kfree_rcu().
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 73e7195f9997..73892483fd05 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -61,6 +61,8 @@ extern void synchronize_rcu_bh(void);
 extern void synchronize_sched_expedited(void);
 extern void synchronize_rcu_expedited(void);
 
+void kfree_call_rcu(struct rcu_head *head, void (*func)(struct rcu_head *rcu));
+
 static inline void synchronize_rcu_bh_expedited(void)
 {
 	synchronize_sched_expedited();

commit 18fec7d8758dd416904da205375e6fa667defc80
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Jan 4 11:44:57 2012 -0800

    rcu: Improve synchronize_rcu() diagnostics
    
    Although TREE_PREEMPT_RCU indirectly uses might_sleep() to detect illegal
    use of synchronize_sched() and synchronize_rcu_bh() from within an RCU
    read-side critical section, this might_sleep() check is bypassed when
    there is only a single CPU (for example, when running an SMP kernel on
    a single-CPU system).  This patch therefore adds a might_sleep() call
    to the rcu_blocking_is_gp() check that is unconditionally invoked from
    both synchronize_sched() and synchronize_rcu_bh().
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 67458468f1a8..73e7195f9997 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -83,6 +83,7 @@ extern void rcu_sched_force_quiescent_state(void);
 /* A context switch is a grace period for RCU-sched and RCU-bh. */
 static inline int rcu_blocking_is_gp(void)
 {
+	might_sleep();  /* Check for RCU read-side critical section. */
 	return num_online_cpus() == 1;
 }
 

commit 2c42818e962e2858334bf45bfc56662b3752df34
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu May 26 22:14:36 2011 -0700

    rcu: Abstract common code for RCU grace-period-wait primitives
    
    Pull the code that waits for an RCU grace period into a single function,
    which is then called by synchronize_rcu() and friends in the case of
    TREE_RCU and TREE_PREEMPT_RCU, and from rcu_barrier() and friends in
    the case of TINY_RCU and TINY_PREEMPT_RCU.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index e65d06634dd8..67458468f1a8 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -67,6 +67,8 @@ static inline void synchronize_rcu_bh_expedited(void)
 }
 
 extern void rcu_barrier(void);
+extern void rcu_barrier_bh(void);
+extern void rcu_barrier_sched(void);
 
 extern unsigned long rcutorture_testseq;
 extern unsigned long rcutorture_vernum;

commit 29ce831000081dd757d3116bf774aafffc4b6b20
Author: Gleb Natapov <gleb@redhat.com>
Date:   Wed May 4 16:31:03 2011 +0300

    rcu: provide rcu_virt_note_context_switch() function.
    
    Provide rcu_virt_note_context_switch() for vitalization use to note
    quiescent state during guest entry.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 284dad10c55b..e65d06634dd8 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -35,6 +35,16 @@ extern void rcu_note_context_switch(int cpu);
 extern int rcu_needs_cpu(int cpu);
 extern void rcu_cpu_stall_reset(void);
 
+/*
+ * Note a virtualization-based context switch.  This is simply a
+ * wrapper around rcu_note_context_switch(), which allows TINY_RCU
+ * to save a few bytes.
+ */
+static inline void rcu_virt_note_context_switch(int cpu)
+{
+	rcu_note_context_switch(cpu);
+}
+
 #ifdef CONFIG_TREE_PREEMPT_RCU
 
 extern void exit_rcu(void);

commit 4a29865689dbb87a02e3b0fff4a4ae5041273173
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Sun Apr 3 21:33:51 2011 -0700

    rcu: make rcutorture version numbers available through debugfs
    
    It is not possible to accurately correlate rcutorture output with that
    of debugfs.  This patch therefore adds a debugfs file that prints out
    the rcutorture version number, permitting easy correlation.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 3a933482734a..284dad10c55b 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -58,9 +58,12 @@ static inline void synchronize_rcu_bh_expedited(void)
 
 extern void rcu_barrier(void);
 
+extern unsigned long rcutorture_testseq;
+extern unsigned long rcutorture_vernum;
 extern long rcu_batches_completed(void);
 extern long rcu_batches_completed_bh(void);
 extern long rcu_batches_completed_sched(void);
+
 extern void rcu_force_quiescent_state(void);
 extern void rcu_bh_force_quiescent_state(void);
 extern void rcu_sched_force_quiescent_state(void);

commit 7b27d5475f86186914e54e4a6bb994e9a985337b
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Thu Oct 21 11:29:05 2010 +0800

    rcu,cleanup: move synchronize_sched_expedited() out of sched.c
    
    The first version of synchronize_sched_expedited() used the migration
    code in the scheduler, and was therefore implemented in kernel/sched.c.
    However, the more recent version of this code no longer uses the
    migration code, so this commit moves it to the main RCU source files.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index c0e96833aa73..3a933482734a 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -48,6 +48,7 @@ static inline void exit_rcu(void)
 #endif /* #else #ifdef CONFIG_TREE_PREEMPT_RCU */
 
 extern void synchronize_rcu_bh(void);
+extern void synchronize_sched_expedited(void);
 extern void synchronize_rcu_expedited(void);
 
 static inline void synchronize_rcu_bh_expedited(void)

commit b2c0710c464ede15e1fc52fb1e7ee9ba54cea186
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Thu Sep 9 13:40:39 2010 -0700

    rcu: move TINY_RCU from softirq to kthread
    
    If RCU priority boosting is to be meaningful, callback invocation must
    be boosted in addition to preempted RCU readers.  Otherwise, in presence
    of CPU real-time threads, the grace period ends, but the callbacks don't
    get invoked.  If the callbacks don't get invoked, the associated memory
    doesn't get freed, so the system is still subject to OOM.
    
    But it is not reasonable to priority-boost RCU_SOFTIRQ, so this commit
    moves the callback invocations to a kthread, which can be boosted easily.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 95518e628794..c0e96833aa73 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -30,6 +30,7 @@
 #ifndef __LINUX_RCUTREE_H
 #define __LINUX_RCUTREE_H
 
+extern void rcu_init(void);
 extern void rcu_note_context_switch(int cpu);
 extern int rcu_needs_cpu(int cpu);
 extern void rcu_cpu_stall_reset(void);

commit 7b0b759b65247cbc66384a912be9acf8d4800636
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Aug 17 14:18:46 2010 -0700

    rcu: combine duplicate code, courtesy of CONFIG_PREEMPT_RCU
    
    The CONFIG_PREEMPT_RCU kernel configuration parameter was recently
    re-introduced, but as an indication of the type of RCU (preemptible
    vs. non-preemptible) instead of as selecting a given implementation.
    This commit uses CONFIG_PREEMPT_RCU to combine duplicate code
    from include/linux/rcutiny.h and include/linux/rcutree.h into
    include/linux/rcupdate.h.  This commit also combines a few other pieces
    of duplicate code that have accumulated.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 54a20c11f98d..95518e628794 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -30,59 +30,23 @@
 #ifndef __LINUX_RCUTREE_H
 #define __LINUX_RCUTREE_H
 
-struct notifier_block;
-
-extern void rcu_sched_qs(int cpu);
-extern void rcu_bh_qs(int cpu);
 extern void rcu_note_context_switch(int cpu);
 extern int rcu_needs_cpu(int cpu);
 extern void rcu_cpu_stall_reset(void);
 
 #ifdef CONFIG_TREE_PREEMPT_RCU
 
-extern void __rcu_read_lock(void);
-extern void __rcu_read_unlock(void);
-extern void synchronize_rcu(void);
 extern void exit_rcu(void);
 
 #else /* #ifdef CONFIG_TREE_PREEMPT_RCU */
 
-static inline void __rcu_read_lock(void)
-{
-	preempt_disable();
-}
-
-static inline void __rcu_read_unlock(void)
-{
-	preempt_enable();
-}
-
-#define synchronize_rcu synchronize_sched
-
 static inline void exit_rcu(void)
 {
 }
 
-static inline int rcu_preempt_depth(void)
-{
-	return 0;
-}
-
 #endif /* #else #ifdef CONFIG_TREE_PREEMPT_RCU */
 
-static inline void __rcu_read_lock_bh(void)
-{
-	local_bh_disable();
-}
-static inline void __rcu_read_unlock_bh(void)
-{
-	local_bh_enable();
-}
-
-extern void call_rcu_sched(struct rcu_head *head,
-			   void (*func)(struct rcu_head *rcu));
 extern void synchronize_rcu_bh(void);
-extern void synchronize_sched(void);
 extern void synchronize_rcu_expedited(void);
 
 static inline void synchronize_rcu_bh_expedited(void)
@@ -92,8 +56,6 @@ static inline void synchronize_rcu_bh_expedited(void)
 
 extern void rcu_barrier(void);
 
-extern void rcu_check_callbacks(int cpu, int user);
-
 extern long rcu_batches_completed(void);
 extern long rcu_batches_completed_bh(void);
 extern long rcu_batches_completed_sched(void);
@@ -101,18 +63,6 @@ extern void rcu_force_quiescent_state(void);
 extern void rcu_bh_force_quiescent_state(void);
 extern void rcu_sched_force_quiescent_state(void);
 
-#ifdef CONFIG_NO_HZ
-void rcu_enter_nohz(void);
-void rcu_exit_nohz(void);
-#else /* CONFIG_NO_HZ */
-static inline void rcu_enter_nohz(void)
-{
-}
-static inline void rcu_exit_nohz(void)
-{
-}
-#endif /* CONFIG_NO_HZ */
-
 /* A context switch is a grace period for RCU-sched and RCU-bh. */
 static inline int rcu_blocking_is_gp(void)
 {

commit a3dc3fb161f9b4066c0fce22db72638af8baf83b
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Aug 13 16:16:25 2010 -0700

    rcu: repair code-duplication FIXMEs
    
    Combine the duplicate definitions of ULONG_CMP_GE(), ULONG_CMP_LT(),
    and rcu_preempt_depth() into include/linux/rcupdate.h.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 0726809497ba..54a20c11f98d 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -45,12 +45,6 @@ extern void __rcu_read_unlock(void);
 extern void synchronize_rcu(void);
 extern void exit_rcu(void);
 
-/*
- * Defined as macro as it is a very low level header
- * included from areas that don't even know about current
- */
-#define rcu_preempt_depth() (current->rcu_read_lock_nesting)
-
 #else /* #ifdef CONFIG_TREE_PREEMPT_RCU */
 
 static inline void __rcu_read_lock(void)

commit 53d84e004d5e8c018be395c4330dc72fd60bd13e
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Aug 10 14:28:53 2010 -0700

    rcu: permit suppressing current grace period's CPU stall warnings
    
    When using a kernel debugger, a long sojourn in the debugger can get
    you lots of RCU CPU stall warnings once you resume.  This might not be
    helpful, especially if you are using the system console.  This patch
    therefore allows RCU CPU stall warnings to be suppressed, but only for
    the duration of the current set of grace periods.
    
    This differs from Jason's original patch in that it adds support for
    tiny RCU and preemptible RCU, and uses a slightly different method for
    suppressing the RCU CPU stall warning messages.
    
    Signed-off-by: Jason Wessel <jason.wessel@windriver.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Tested-by: Jason Wessel <jason.wessel@windriver.com>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index c13b85dd22bc..0726809497ba 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -36,6 +36,7 @@ extern void rcu_sched_qs(int cpu);
 extern void rcu_bh_qs(int cpu);
 extern void rcu_note_context_switch(int cpu);
 extern int rcu_needs_cpu(int cpu);
+extern void rcu_cpu_stall_reset(void);
 
 #ifdef CONFIG_TREE_PREEMPT_RCU
 

commit a57eb940d130477a799dfb24a570ee04979c0f7f
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Jun 29 16:49:16 2010 -0700

    rcu: Add a TINY_PREEMPT_RCU
    
    Implement a small-memory-footprint uniprocessor-only implementation of
    preemptible RCU.  This implementation uses but a single blocked-tasks
    list rather than the combinatorial number used per leaf rcu_node by
    TREE_PREEMPT_RCU, which reduces memory consumption and greatly simplifies
    processing.  This version also takes advantage of uniprocessor execution
    to accelerate grace periods in the case where there are no readers.
    
    The general design is otherwise broadly similar to that of TREE_PREEMPT_RCU.
    
    This implementation is a step towards having RCU implementation driven
    off of the SMP and PREEMPT kernel configuration variables, which can
    happen once this implementation has accumulated sufficient experience.
    
    Removed ACCESS_ONCE() from __rcu_read_unlock() and added barrier() as
    suggested by Steve Rostedt in order to avoid the compiler-reordering
    issue noted by Mathieu Desnoyers (http://lkml.org/lkml/2010/8/16/183).
    
    As can be seen below, CONFIG_TINY_PREEMPT_RCU represents almost 5Kbyte
    savings compared to CONFIG_TREE_PREEMPT_RCU.  Of course, for non-real-time
    workloads, CONFIG_TINY_RCU is even better.
    
            CONFIG_TREE_PREEMPT_RCU
    
               text    data     bss     dec    filename
                 13       0       0      13    kernel/rcupdate.o
               6170     825      28    7023    kernel/rcutree.o
                                       ----
                                       7026    Total
    
            CONFIG_TINY_PREEMPT_RCU
    
               text    data     bss     dec    filename
                 13       0       0      13    kernel/rcupdate.o
               2081      81       8    2170    kernel/rcutiny.o
                                       ----
                                       2183    Total
    
            CONFIG_TINY_RCU (non-preemptible)
    
               text    data     bss     dec    filename
                 13       0       0      13    kernel/rcupdate.o
                719      25       0     744    kernel/rcutiny.o
                                        ---
                                        757    Total
    
    Requested-by: Loc Minier <loic.minier@canonical.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index c0ed1c056f29..c13b85dd22bc 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -95,6 +95,8 @@ static inline void synchronize_rcu_bh_expedited(void)
 	synchronize_sched_expedited();
 }
 
+extern void rcu_barrier(void);
+
 extern void rcu_check_callbacks(int cpu, int user);
 
 extern long rcu_batches_completed(void);

commit b8ae30ee26d379db436b0b8c8c3ff1b52f69e5d1
Merge: 4d7b4ac22fbe 9c6f7e43b4e0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 18 08:27:54 2010 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (49 commits)
      stop_machine: Move local variable closer to the usage site in cpu_stop_cpu_callback()
      sched, wait: Use wrapper functions
      sched: Remove a stale comment
      ondemand: Make the iowait-is-busy time a sysfs tunable
      ondemand: Solve a big performance issue by counting IOWAIT time as busy
      sched: Intoduce get_cpu_iowait_time_us()
      sched: Eliminate the ts->idle_lastupdate field
      sched: Fold updating of the last_update_time_info into update_ts_time_stats()
      sched: Update the idle statistics in get_cpu_idle_time_us()
      sched: Introduce a function to update the idle statistics
      sched: Add a comment to get_cpu_idle_time_us()
      cpu_stop: add dummy implementation for UP
      sched: Remove rq argument to the tracepoints
      rcu: need barrier() in UP synchronize_sched_expedited()
      sched: correctly place paranioa memory barriers in synchronize_sched_expedited()
      sched: kill paranoia check in synchronize_sched_expedited()
      sched: replace migration_thread with cpu_stop
      stop_machine: reimplement using cpu_stop
      cpu_stop: implement stop_cpu[s]()
      sched: Fix select_idle_sibling() logic in select_task_rq_fair()
      ...

commit bbad937983147c017c25406860287cb94da9af7c
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Apr 2 16:17:17 2010 -0700

    rcu: slim down rcutiny by removing rcu_scheduler_active and friends
    
    TINY_RCU does not need rcu_scheduler_active unless CONFIG_DEBUG_LOCK_ALLOC.
    So conditionally compile rcu_scheduler_active in order to slim down
    rcutiny a bit more.  Also gets rid of an EXPORT_SYMBOL_GPL, which is
    responsible for most of the slimming.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index b9f74606f320..48282055e83d 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -123,4 +123,7 @@ static inline int rcu_blocking_is_gp(void)
 	return num_online_cpus() == 1;
 }
 
+extern void rcu_scheduler_starting(void);
+extern int rcu_scheduler_active __read_mostly;
+
 #endif /* __LINUX_RCUTREE_H */

commit 25502a6c13745f4650cc59322bd198194f55e796
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Apr 1 17:37:01 2010 -0700

    rcu: refactor RCU's context-switch handling
    
    The addition of preemptible RCU to treercu resulted in a bit of
    confusion and inefficiency surrounding the handling of context switches
    for RCU-sched and for RCU-preempt.  For RCU-sched, a context switch
    is a quiescent state, pure and simple, just like it always has been.
    For RCU-preempt, a context switch is in no way a quiescent state, but
    special handling is required when a task blocks in an RCU read-side
    critical section.
    
    However, the callout from the scheduler and the outer loop in ksoftirqd
    still calls something named rcu_sched_qs(), whose name is no longer
    accurate.  Furthermore, when rcu_check_callbacks() notes an RCU-sched
    quiescent state, it ends up unnecessarily (though harmlessly, aside
    from the performance hit) enqueuing the current task if it happens to
    be running in an RCU-preempt read-side critical section.  This not only
    increases the maximum latency of scheduler_tick(), it also needlessly
    increases the overhead of the next outermost rcu_read_unlock() invocation.
    
    This patch addresses this situation by separating the notion of RCU's
    context-switch handling from that of RCU-sched's quiescent states.
    The context-switch handling is covered by rcu_note_context_switch() in
    general and by rcu_preempt_note_context_switch() for preemptible RCU.
    This permits rcu_sched_qs() to handle quiescent states and only quiescent
    states.  It also reduces the maximum latency of scheduler_tick(), though
    probably by much less than a microsecond.  Finally, it means that tasks
    within preemptible-RCU read-side critical sections avoid incurring the
    overhead of queuing unless there really is a context switch.
    
    Suggested-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Acked-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <peterz@infradead.org>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 7484fe66a3aa..b9f74606f320 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -34,6 +34,7 @@ struct notifier_block;
 
 extern void rcu_sched_qs(int cpu);
 extern void rcu_bh_qs(int cpu);
+extern void rcu_note_context_switch(int cpu);
 extern int rcu_needs_cpu(int cpu);
 extern int rcu_expedited_torture_stats(char *page);
 

commit da848c47bc6e873a54a445ea1960423a495b6b32
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Mar 30 15:46:01 2010 -0700

    rcu: shrink rcutiny by making synchronize_rcu_bh() be inline
    
    Because synchronize_rcu_bh() is identical to synchronize_sched(),
    make the former a static inline invoking the latter, saving the
    overhead of an EXPORT_SYMBOL_GPL() and the duplicate code.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 42cc3a04779e..7484fe66a3aa 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -86,6 +86,8 @@ static inline void __rcu_read_unlock_bh(void)
 
 extern void call_rcu_sched(struct rcu_head *head,
 			   void (*func)(struct rcu_head *rcu));
+extern void synchronize_rcu_bh(void);
+extern void synchronize_sched(void);
 extern void synchronize_rcu_expedited(void);
 
 static inline void synchronize_rcu_bh_expedited(void)

commit 969c79215a35b06e5e3efe69b9412f858df7856c
Author: Tejun Heo <tj@kernel.org>
Date:   Thu May 6 18:49:21 2010 +0200

    sched: replace migration_thread with cpu_stop
    
    Currently migration_thread is serving three purposes - migration
    pusher, context to execute active_load_balance() and forced context
    switcher for expedited RCU synchronize_sched.  All three roles are
    hardcoded into migration_thread() and determining which job is
    scheduled is slightly messy.
    
    This patch kills migration_thread and replaces all three uses with
    cpu_stop.  The three different roles of migration_thread() are
    splitted into three separate cpu_stop callbacks -
    migration_cpu_stop(), active_load_balance_cpu_stop() and
    synchronize_sched_expedited_cpu_stop() - and each use case now simply
    asks cpu_stop to execute the callback as necessary.
    
    synchronize_sched_expedited() was implemented with private
    preallocated resources and custom multi-cpu queueing and waiting
    logic, both of which are provided by cpu_stop.
    synchronize_sched_expedited_count is made atomic and all other shared
    resources along with the mutex are dropped.
    
    synchronize_sched_expedited() also implemented a check to detect cases
    where not all the callback got executed on their assigned cpus and
    fall back to synchronize_sched().  If called with cpu hotplug blocked,
    cpu_stop already guarantees that and the condition cannot happen;
    otherwise, stop_machine() would break.  However, this patch preserves
    the paranoid check using a cpumask to record on which cpus the stopper
    ran so that it can serve as a bisection point if something actually
    goes wrong theree.
    
    Because the internal execution state is no longer visible,
    rcu_expedited_torture_stats() is removed.
    
    This patch also renames cpu_stop threads to from "stopper/%d" to
    "migration/%d".  The names of these threads ultimately don't matter
    and there's no reason to make unnecessary userland visible changes.
    
    With this patch applied, stop_machine() and sched now share the same
    resources.  stop_machine() is faster without wasting any resources and
    sched migration users are much cleaner.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Dipankar Sarma <dipankar@in.ibm.com>
    Cc: Josh Triplett <josh@freedesktop.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Dimitri Sivanich <sivanich@sgi.com>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 42cc3a04779e..24e467e526b8 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -35,7 +35,6 @@ struct notifier_block;
 extern void rcu_sched_qs(int cpu);
 extern void rcu_bh_qs(int cpu);
 extern int rcu_needs_cpu(int cpu);
-extern int rcu_expedited_torture_stats(char *page);
 
 #ifdef CONFIG_TREE_PREEMPT_RCU
 

commit d9f1bb6ad7fc53c406706f47858dd5ff030b14a3
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Feb 25 14:06:47 2010 -0800

    rcu: Make rcu_read_lock_sched_held() take boot time into account
    
    Before the scheduler starts, all tasks are non-preemptible by
    definition. So, during that time, rcu_read_lock_sched_held()
    needs to always return "true".  This patch makes that be so.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: laijs@cn.fujitsu.com
    Cc: dipankar@in.ibm.com
    Cc: mathieu.desnoyers@polymtl.ca
    Cc: josh@joshtriplett.org
    Cc: dvhltc@us.ibm.com
    Cc: niv@us.ibm.com
    Cc: peterz@infradead.org
    Cc: rostedt@goodmis.org
    Cc: Valdis.Kletnieks@vt.edu
    Cc: dhowells@redhat.com
    LKML-Reference: <1267135607-7056-2-git-send-email-paulmck@linux.vnet.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 704a010f686c..42cc3a04779e 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -35,7 +35,6 @@ struct notifier_block;
 extern void rcu_sched_qs(int cpu);
 extern void rcu_bh_qs(int cpu);
 extern int rcu_needs_cpu(int cpu);
-extern void rcu_scheduler_starting(void);
 extern int rcu_expedited_torture_stats(char *page);
 
 #ifdef CONFIG_TREE_PREEMPT_RCU

commit bf66f18e79e34c421bbd8f6511e2c556b779df2f
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Jan 4 15:09:10 2010 -0800

    rcu: Add force_quiescent_state() testing to rcutorture
    
    Add force_quiescent_state() testing to rcutorture, with a
    separate thread that repeatedly invokes force_quiescent_state()
    in bursts. This can greatly increase the probability of
    encountering certain types of race conditions.
    
    Suggested-by: Josh Triplett <josh@joshtriplett.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: laijs@cn.fujitsu.com
    Cc: dipankar@in.ibm.com
    Cc: mathieu.desnoyers@polymtl.ca
    Cc: josh@joshtriplett.org
    Cc: dvhltc@us.ibm.com
    Cc: niv@us.ibm.com
    Cc: peterz@infradead.org
    Cc: rostedt@goodmis.org
    Cc: Valdis.Kletnieks@vt.edu
    Cc: dhowells@redhat.com
    LKML-Reference: <1262646551116-git-send-email->
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 8044b1b94333..704a010f686c 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -99,6 +99,9 @@ extern void rcu_check_callbacks(int cpu, int user);
 extern long rcu_batches_completed(void);
 extern long rcu_batches_completed_bh(void);
 extern long rcu_batches_completed_sched(void);
+extern void rcu_force_quiescent_state(void);
+extern void rcu_bh_force_quiescent_state(void);
+extern void rcu_sched_force_quiescent_state(void);
 
 #ifdef CONFIG_NO_HZ
 void rcu_enter_nohz(void);

commit 234da7bcdc7aaa935846534c3b726dbc79a9cdd5
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Dec 16 20:21:05 2009 +0100

    sched: Teach might_sleep() about preemptible RCU
    
    In practice, it is harmless to voluntarily sleep in a
    rcu_read_lock() section if we are running under preempt rcu, but
    it is illegal if we build a kernel running non-preemptable rcu.
    
    Currently, might_sleep() doesn't notice sleepable operations
    under rcu_read_lock() sections if we are running under
    preemptable rcu because preempt_count() is left untouched after
    rcu_read_lock() in this case. But we want developers who test
    their changes under such config to notice the "sleeping while
    atomic" issues.
    
    So we add rcu_read_lock_nesting to prempt_count() in
    might_sleep() checks.
    
    [ v2: Handle rcu-tiny ]
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Reviewed-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    LKML-Reference: <1260991265-8451-1-git-send-regression-fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index c93eee5911b0..8044b1b94333 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -45,6 +45,12 @@ extern void __rcu_read_unlock(void);
 extern void synchronize_rcu(void);
 extern void exit_rcu(void);
 
+/*
+ * Defined as macro as it is a very low level header
+ * included from areas that don't even know about current
+ */
+#define rcu_preempt_depth() (current->rcu_read_lock_nesting)
+
 #else /* #ifdef CONFIG_TREE_PREEMPT_RCU */
 
 static inline void __rcu_read_lock(void)
@@ -63,6 +69,11 @@ static inline void exit_rcu(void)
 {
 }
 
+static inline int rcu_preempt_depth(void)
+{
+	return 0;
+}
+
 #endif /* #else #ifdef CONFIG_TREE_PREEMPT_RCU */
 
 static inline void __rcu_read_lock_bh(void)

commit 6ebb237bece23275d1da149b61a342f0d4d06a08
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sun Nov 22 08:53:50 2009 -0800

    rcu: Re-arrange code to reduce #ifdef pain
    
    Remove #ifdefs from kernel/rcupdate.c and
    include/linux/rcupdate.h by moving code to
    include/linux/rcutiny.h, include/linux/rcutree.h, and
    kernel/rcutree.c.
    
    Also remove some definitions that are no longer used.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: laijs@cn.fujitsu.com
    Cc: dipankar@in.ibm.com
    Cc: mathieu.desnoyers@polymtl.ca
    Cc: josh@joshtriplett.org
    Cc: dvhltc@us.ibm.com
    Cc: niv@us.ibm.com
    Cc: peterz@infradead.org
    Cc: rostedt@goodmis.org
    Cc: Valdis.Kletnieks@vt.edu
    Cc: dhowells@redhat.com
    LKML-Reference: <1258908830885-git-send-email->
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 111a65257350..c93eee5911b0 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -35,12 +35,14 @@ struct notifier_block;
 extern void rcu_sched_qs(int cpu);
 extern void rcu_bh_qs(int cpu);
 extern int rcu_needs_cpu(int cpu);
+extern void rcu_scheduler_starting(void);
 extern int rcu_expedited_torture_stats(char *page);
 
 #ifdef CONFIG_TREE_PREEMPT_RCU
 
 extern void __rcu_read_lock(void);
 extern void __rcu_read_unlock(void);
+extern void synchronize_rcu(void);
 extern void exit_rcu(void);
 
 #else /* #ifdef CONFIG_TREE_PREEMPT_RCU */
@@ -55,7 +57,7 @@ static inline void __rcu_read_unlock(void)
 	preempt_enable();
 }
 
-#define __synchronize_sched() synchronize_rcu()
+#define synchronize_rcu synchronize_sched
 
 static inline void exit_rcu(void)
 {

commit 9f680ab41485edfdc96331b70afa7513aa0a7720
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sun Nov 22 08:53:49 2009 -0800

    rcu: Eliminate unneeded function wrapping
    
    The functions rcu_init() is a wrapper for __rcu_init(), and also
    sets up the CPU-hotplug notifier for rcu_barrier_cpu_hotplug().
    But TINY_RCU doesn't need CPU-hotplug notification, and the
    rcu_barrier_cpu_hotplug() is a simple wrapper for
    rcu_cpu_notify().
    
    So push rcu_init() out to kernel/rcutree.c and kernel/rcutiny.c
    and get rid of the wrapper function rcu_barrier_cpu_hotplug().
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: laijs@cn.fujitsu.com
    Cc: dipankar@in.ibm.com
    Cc: mathieu.desnoyers@polymtl.ca
    Cc: josh@joshtriplett.org
    Cc: dvhltc@us.ibm.com
    Cc: niv@us.ibm.com
    Cc: peterz@infradead.org
    Cc: rostedt@goodmis.org
    Cc: Valdis.Kletnieks@vt.edu
    Cc: dhowells@redhat.com
    LKML-Reference: <12589088302320-git-send-email->
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 9642c6bcb399..111a65257350 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -34,8 +34,6 @@ struct notifier_block;
 
 extern void rcu_sched_qs(int cpu);
 extern void rcu_bh_qs(int cpu);
-extern int rcu_cpu_notify(struct notifier_block *self,
-			  unsigned long action, void *hcpu);
 extern int rcu_needs_cpu(int cpu);
 extern int rcu_expedited_torture_stats(char *page);
 
@@ -83,7 +81,6 @@ static inline void synchronize_rcu_bh_expedited(void)
 	synchronize_sched_expedited();
 }
 
-extern void __rcu_init(void);
 extern void rcu_check_callbacks(int cpu, int user);
 
 extern long rcu_batches_completed(void);

commit 019129d595caaa5bd0b41d128308da1be6a91869
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Oct 14 10:15:56 2009 -0700

    rcu: Stopgap fix for synchronize_rcu_expedited() for TREE_PREEMPT_RCU
    
    For the short term, map synchronize_rcu_expedited() to
    synchronize_rcu() for TREE_PREEMPT_RCU and to
    synchronize_sched_expedited() for TREE_RCU.
    
    Longer term, there needs to be a real expedited grace period for
    TREE_PREEMPT_RCU, but candidate patches to date are considerably
    more complex and intrusive.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: laijs@cn.fujitsu.com
    Cc: dipankar@in.ibm.com
    Cc: mathieu.desnoyers@polymtl.ca
    Cc: josh@joshtriplett.org
    Cc: dvhltc@us.ibm.com
    Cc: niv@us.ibm.com
    Cc: peterz@infradead.org
    Cc: rostedt@goodmis.org
    Cc: Valdis.Kletnieks@vt.edu
    Cc: dhowells@redhat.com
    Cc: npiggin@suse.de
    Cc: jens.axboe@oracle.com
    LKML-Reference: <12555405592331-git-send-email->
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 46e9ab3ee6e1..9642c6bcb399 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -76,11 +76,7 @@ static inline void __rcu_read_unlock_bh(void)
 
 extern void call_rcu_sched(struct rcu_head *head,
 			   void (*func)(struct rcu_head *rcu));
-
-static inline void synchronize_rcu_expedited(void)
-{
-	synchronize_sched_expedited();
-}
+extern void synchronize_rcu_expedited(void);
 
 static inline void synchronize_rcu_bh_expedited(void)
 {

commit 9b2619aff0332e95ea5eb7a0d75b0208818d871c
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Sep 23 09:50:43 2009 -0700

    rcu: Clean up code to address Ingo's checkpatch feedback
    
    Move declarations and update storage classes to make checkpatch happy.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: laijs@cn.fujitsu.com
    Cc: dipankar@in.ibm.com
    Cc: akpm@linux-foundation.org
    Cc: mathieu.desnoyers@polymtl.ca
    Cc: josh@joshtriplett.org
    Cc: dvhltc@us.ibm.com
    Cc: niv@us.ibm.com
    Cc: peterz@infradead.org
    Cc: rostedt@goodmis.org
    Cc: Valdis.Kletnieks@vt.edu
    Cc: dhowells@redhat.com
    LKML-Reference: <12537246441701-git-send-email->
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 19a3b06943e0..46e9ab3ee6e1 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -30,10 +30,14 @@
 #ifndef __LINUX_RCUTREE_H
 #define __LINUX_RCUTREE_H
 
+struct notifier_block;
+
 extern void rcu_sched_qs(int cpu);
 extern void rcu_bh_qs(int cpu);
-
+extern int rcu_cpu_notify(struct notifier_block *self,
+			  unsigned long action, void *hcpu);
 extern int rcu_needs_cpu(int cpu);
+extern int rcu_expedited_torture_stats(char *page);
 
 #ifdef CONFIG_TREE_PREEMPT_RCU
 

commit 1eba8f84380bede3c602bd7758dea96925cead01
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Sep 23 09:50:42 2009 -0700

    rcu: Clean up code based on review feedback from Josh Triplett, part 2
    
    These issues identified during an old-fashioned face-to-face code
    review extending over many hours.
    
    o       Add comments for tricky parts of code, and correct comments
            that have passed their sell-by date.
    
    o       Get rid of the vestiges of rcu_init_sched(), which is no
            longer needed now that PREEMPT_RCU is gone.
    
    o       Move the #include of rcutree_plugin.h to the end of
            rcutree.c, which means that, rather than having a random
            collection of forward declarations, the new set of forward
            declarations document the set of plugins.  The new home for
            this #include also allows __rcu_init_preempt() to move into
            rcutree_plugin.h.
    
    o       Fix rcu_preempt_check_callbacks() to be static.
    
    Suggested-by: Josh Triplett <josh@joshtriplett.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: laijs@cn.fujitsu.com
    Cc: dipankar@in.ibm.com
    Cc: akpm@linux-foundation.org
    Cc: mathieu.desnoyers@polymtl.ca
    Cc: josh@joshtriplett.org
    Cc: dvhltc@us.ibm.com
    Cc: niv@us.ibm.com
    Cc: peterz@infradead.org
    Cc: rostedt@goodmis.org
    Cc: Valdis.Kletnieks@vt.edu
    Cc: dhowells@redhat.com
    LKML-Reference: <12537246443924-git-send-email->
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Peter Zijlstra <peterz@infradead.org>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 88109c87f29c..19a3b06943e0 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -90,10 +90,6 @@ extern long rcu_batches_completed(void);
 extern long rcu_batches_completed_bh(void);
 extern long rcu_batches_completed_sched(void);
 
-static inline void rcu_init_sched(void)
-{
-}
-
 #ifdef CONFIG_NO_HZ
 void rcu_enter_nohz(void);
 void rcu_exit_nohz(void);
@@ -106,7 +102,7 @@ static inline void rcu_exit_nohz(void)
 }
 #endif /* CONFIG_NO_HZ */
 
-/* A context switch is a grace period for rcutree. */
+/* A context switch is a grace period for RCU-sched and RCU-bh. */
 static inline int rcu_blocking_is_gp(void)
 {
 	return num_online_cpus() == 1;

commit fc2219d49ef1606e7fd2c88af2b423b01ff3d319
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Sep 23 09:50:41 2009 -0700

    rcu: Clean up code based on review feedback from Josh Triplett
    
    These issues identified during an old-fashioned face-to-face code
    review extended over many hours.
    
    o       Bury various forms of the "rsp->completed == rsp->gpnum"
            comparison into an rcu_gp_in_progress() function, which has
            the beneficial side-effect of forcing consistent use of
            ACCESS_ONCE().
    
    o       Replace hand-coded arithmetic with DIV_ROUND_UP().
    
    o       Bury several "!list_empty(&rnp->blocked_tasks[rnp->gpnum & 0x01])"
            instances into an rcu_preempted_readers() function, as this
            expression indicates that there are no readers blocked
            within RCU read-side critical sections blocking the current
            grace period.  (Though there might well be similar readers
            blocking the next grace period.)
    
    o       Remove a dangling rcu_restart_cpu() declaration that has
            been dangling for almost 20 minor releases of the kernel.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: laijs@cn.fujitsu.com
    Cc: dipankar@in.ibm.com
    Cc: akpm@linux-foundation.org
    Cc: mathieu.desnoyers@polymtl.ca
    Cc: josh@joshtriplett.org
    Cc: dvhltc@us.ibm.com
    Cc: niv@us.ibm.com
    Cc: rostedt@goodmis.org
    Cc: Valdis.Kletnieks@vt.edu
    Cc: dhowells@redhat.com
    LKML-Reference: <12537246442687-git-send-email->
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 37682770e9d2..88109c87f29c 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -85,7 +85,6 @@ static inline void synchronize_rcu_bh_expedited(void)
 
 extern void __rcu_init(void);
 extern void rcu_check_callbacks(int cpu, int user);
-extern void rcu_restart_cpu(int cpu);
 
 extern long rcu_batches_completed(void);
 extern long rcu_batches_completed_bh(void);

commit a71fca58b7f4abca551ae2256ac08dd9123a03f9
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Sep 18 10:28:19 2009 -0700

    rcu: Fix whitespace inconsistencies
    
    Fix a number of whitespace ^Ierrors in the include/linux/rcu*
    and the kernel/rcu* files.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: laijs@cn.fujitsu.com
    Cc: dipankar@in.ibm.com
    Cc: akpm@linux-foundation.org
    Cc: mathieu.desnoyers@polymtl.ca
    Cc: josh@joshtriplett.org
    Cc: dvhltc@us.ibm.com
    Cc: niv@us.ibm.com
    Cc: peterz@infradead.org
    Cc: rostedt@goodmis.org
    Cc: Valdis.Kletnieks@vt.edu
    LKML-Reference: <20090918172819.GA24405@linux.vnet.ibm.com>
    [ did more checkpatch fixlets ]
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 00d08c0cbcc1..37682770e9d2 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -24,7 +24,7 @@
  * and inputs from Rusty Russell, Andrea Arcangeli and Andi Kleen.
  *
  * For detailed explanation of Read-Copy Update mechanism see -
- * 	Documentation/RCU
+ *	Documentation/RCU
  */
 
 #ifndef __LINUX_RCUTREE_H

commit 16e3081191837a6a04733de5cd5d1d1b303140d4
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sun Sep 13 09:15:11 2009 -0700

    rcu: Fix synchronize_rcu() for TREE_PREEMPT_RCU
    
    The redirection of synchronize_sched() to synchronize_rcu() was
    appropriate for TREE_RCU, but not for TREE_PREEMPT_RCU.
    
    Fix this by creating an underlying synchronize_sched().  TREE_RCU
    then redirects synchronize_rcu() to synchronize_sched(), while
    TREE_PREEMPT_RCU has its own version of synchronize_rcu().
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: laijs@cn.fujitsu.com
    Cc: dipankar@in.ibm.com
    Cc: akpm@linux-foundation.org
    Cc: mathieu.desnoyers@polymtl.ca
    Cc: josh@joshtriplett.org
    Cc: dvhltc@us.ibm.com
    Cc: niv@us.ibm.com
    Cc: peterz@infradead.org
    Cc: rostedt@goodmis.org
    Cc: Valdis.Kletnieks@vt.edu
    LKML-Reference: <12528585111916-git-send-email->
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index a89307717825..00d08c0cbcc1 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -53,6 +53,8 @@ static inline void __rcu_read_unlock(void)
 	preempt_enable();
 }
 
+#define __synchronize_sched() synchronize_rcu()
+
 static inline void exit_rcu(void)
 {
 }
@@ -68,8 +70,6 @@ static inline void __rcu_read_unlock_bh(void)
 	local_bh_enable();
 }
 
-#define __synchronize_sched() synchronize_rcu()
-
 extern void call_rcu_sched(struct rcu_head *head,
 			   void (*func)(struct rcu_head *rcu));
 

commit f41d911f8c49a5d65c86504c19e8204bb605c4fd
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sat Aug 22 13:56:52 2009 -0700

    rcu: Merge preemptable-RCU functionality into hierarchical RCU
    
    Create a kernel/rcutree_plugin.h file that contains definitions
    for preemptable RCU (or, under the #else branch of the #ifdef,
    empty definitions for the classic non-preemptable semantics).
    These definitions fit into plugins defined in kernel/rcutree.c
    for this purpose.
    
    This variant of preemptable RCU uses a new algorithm whose
    read-side expense is roughly that of classic hierarchical RCU
    under CONFIG_PREEMPT. This new algorithm's update-side expense
    is similar to that of classic hierarchical RCU, and, in absence
    of read-side preemption or blocking, is exactly that of classic
    hierarchical RCU.  Perhaps more important, this new algorithm
    has a much simpler implementation, saving well over 1,000 lines
    of code compared to mainline's implementation of preemptable
    RCU, which will hopefully be retired in favor of this new
    algorithm.
    
    The simplifications are obtained by maintaining per-task
    nesting state for running tasks, and using a simple
    lock-protected algorithm to handle accounting when tasks block
    within RCU read-side critical sections, making use of lessons
    learned while creating numerous user-level RCU implementations
    over the past 18 months.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: laijs@cn.fujitsu.com
    Cc: dipankar@in.ibm.com
    Cc: akpm@linux-foundation.org
    Cc: mathieu.desnoyers@polymtl.ca
    Cc: josht@linux.vnet.ibm.com
    Cc: dvhltc@us.ibm.com
    Cc: niv@us.ibm.com
    Cc: peterz@infradead.org
    Cc: rostedt@goodmis.org
    LKML-Reference: <12509746134003-git-send-email->
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index c739d90f5e68..a89307717825 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -35,14 +35,30 @@ extern void rcu_bh_qs(int cpu);
 
 extern int rcu_needs_cpu(int cpu);
 
+#ifdef CONFIG_TREE_PREEMPT_RCU
+
+extern void __rcu_read_lock(void);
+extern void __rcu_read_unlock(void);
+extern void exit_rcu(void);
+
+#else /* #ifdef CONFIG_TREE_PREEMPT_RCU */
+
 static inline void __rcu_read_lock(void)
 {
 	preempt_disable();
 }
+
 static inline void __rcu_read_unlock(void)
 {
 	preempt_enable();
 }
+
+static inline void exit_rcu(void)
+{
+}
+
+#endif /* #else #ifdef CONFIG_TREE_PREEMPT_RCU */
+
 static inline void __rcu_read_lock_bh(void)
 {
 	local_bh_disable();

commit a157229cabd6dd8cfa82525fc9bf730c94cc9ac2
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sat Aug 22 13:56:51 2009 -0700

    rcu: Simplify rcu_pending()/rcu_check_callbacks() API
    
    All calls from outside RCU are of the form:
    
            if (rcu_pending(cpu))
                    rcu_check_callbacks(cpu, user);
    
    This is silly, instead we put a call to rcu_pending() in
    rcu_check_callbacks(), and then make the outside calls be to
    rcu_check_callbacks().  This cuts down on the code a bit and
    also gives the compiler a better chance of optimizing.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: laijs@cn.fujitsu.com
    Cc: dipankar@in.ibm.com
    Cc: akpm@linux-foundation.org
    Cc: mathieu.desnoyers@polymtl.ca
    Cc: josht@linux.vnet.ibm.com
    Cc: dvhltc@us.ibm.com
    Cc: niv@us.ibm.com
    Cc: peterz@infradead.org
    Cc: rostedt@goodmis.org
    LKML-Reference: <125097461311-git-send-email->
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 8a0222ce3b13..c739d90f5e68 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -33,7 +33,6 @@
 extern void rcu_sched_qs(int cpu);
 extern void rcu_bh_qs(int cpu);
 
-extern int rcu_pending(int cpu);
 extern int rcu_needs_cpu(int cpu);
 
 static inline void __rcu_read_lock(void)

commit bc33f24bdca8b6e97376e3a182ab69e6cdefa989
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sat Aug 22 13:56:47 2009 -0700

    rcu: Consolidate sparse and lockdep declarations in include/linux/rcupdate.h
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: laijs@cn.fujitsu.com
    Cc: dipankar@in.ibm.com
    Cc: akpm@linux-foundation.org
    Cc: mathieu.desnoyers@polymtl.ca
    Cc: josht@linux.vnet.ibm.com
    Cc: dvhltc@us.ibm.com
    Cc: niv@us.ibm.com
    Cc: peterz@infradead.org
    Cc: rostedt@goodmis.org
    LKML-Reference: <12509746132349-git-send-email->
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index a0852d0d915b..8a0222ce3b13 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -36,38 +36,20 @@ extern void rcu_bh_qs(int cpu);
 extern int rcu_pending(int cpu);
 extern int rcu_needs_cpu(int cpu);
 
-#ifdef CONFIG_DEBUG_LOCK_ALLOC
-extern struct lockdep_map rcu_lock_map;
-# define rcu_read_acquire()	\
-			lock_acquire(&rcu_lock_map, 0, 0, 2, 1, NULL, _THIS_IP_)
-# define rcu_read_release()	lock_release(&rcu_lock_map, 1, _THIS_IP_)
-#else
-# define rcu_read_acquire()	do { } while (0)
-# define rcu_read_release()	do { } while (0)
-#endif
-
 static inline void __rcu_read_lock(void)
 {
 	preempt_disable();
-	__acquire(RCU);
-	rcu_read_acquire();
 }
 static inline void __rcu_read_unlock(void)
 {
-	rcu_read_release();
-	__release(RCU);
 	preempt_enable();
 }
 static inline void __rcu_read_lock_bh(void)
 {
 	local_bh_disable();
-	__acquire(RCU_BH);
-	rcu_read_acquire();
 }
 static inline void __rcu_read_unlock_bh(void)
 {
-	rcu_read_release();
-	__release(RCU_BH);
 	local_bh_enable();
 }
 

commit d6714c22b43fbcbead7e7b706ff270e15f04a791
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sat Aug 22 13:56:46 2009 -0700

    rcu: Renamings to increase RCU clarity
    
    Make RCU-sched, RCU-bh, and RCU-preempt be underlying
    implementations, with "RCU" defined in terms of one of the
    three.  Update the outdated rcu_qsctr_inc() names, as these
    functions no longer increment anything.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: laijs@cn.fujitsu.com
    Cc: dipankar@in.ibm.com
    Cc: akpm@linux-foundation.org
    Cc: mathieu.desnoyers@polymtl.ca
    Cc: josht@linux.vnet.ibm.com
    Cc: dvhltc@us.ibm.com
    Cc: niv@us.ibm.com
    Cc: peterz@infradead.org
    Cc: rostedt@goodmis.org
    LKML-Reference: <12509746132696-git-send-email->
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index e37d5e2a8353..a0852d0d915b 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -30,8 +30,8 @@
 #ifndef __LINUX_RCUTREE_H
 #define __LINUX_RCUTREE_H
 
-extern void rcu_qsctr_inc(int cpu);
-extern void rcu_bh_qsctr_inc(int cpu);
+extern void rcu_sched_qs(int cpu);
+extern void rcu_bh_qs(int cpu);
 
 extern int rcu_pending(int cpu);
 extern int rcu_needs_cpu(int cpu);
@@ -73,7 +73,8 @@ static inline void __rcu_read_unlock_bh(void)
 
 #define __synchronize_sched() synchronize_rcu()
 
-#define call_rcu_sched(head, func) call_rcu(head, func)
+extern void call_rcu_sched(struct rcu_head *head,
+			   void (*func)(struct rcu_head *rcu));
 
 static inline void synchronize_rcu_expedited(void)
 {
@@ -91,6 +92,7 @@ extern void rcu_restart_cpu(int cpu);
 
 extern long rcu_batches_completed(void);
 extern long rcu_batches_completed_bh(void);
+extern long rcu_batches_completed_sched(void);
 
 static inline void rcu_init_sched(void)
 {

commit 9f77da9f40045253e91f55c12d4481254b513d2d
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sat Aug 22 13:56:45 2009 -0700

    rcu: Move private definitions from include/linux/rcutree.h to kernel/rcutree.h
    
    Some information hiding that makes it easier to merge
    preemptability into rcutree without descending into #include
    hell.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: laijs@cn.fujitsu.com
    Cc: dipankar@in.ibm.com
    Cc: akpm@linux-foundation.org
    Cc: mathieu.desnoyers@polymtl.ca
    Cc: josht@linux.vnet.ibm.com
    Cc: dvhltc@us.ibm.com
    Cc: niv@us.ibm.com
    Cc: peterz@infradead.org
    Cc: rostedt@goodmis.org
    LKML-Reference: <1250974613373-git-send-email->
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index d4dfd2489633..e37d5e2a8353 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -30,217 +30,6 @@
 #ifndef __LINUX_RCUTREE_H
 #define __LINUX_RCUTREE_H
 
-#include <linux/cache.h>
-#include <linux/spinlock.h>
-#include <linux/threads.h>
-#include <linux/cpumask.h>
-#include <linux/seqlock.h>
-
-/*
- * Define shape of hierarchy based on NR_CPUS and CONFIG_RCU_FANOUT.
- * In theory, it should be possible to add more levels straightforwardly.
- * In practice, this has not been tested, so there is probably some
- * bug somewhere.
- */
-#define MAX_RCU_LVLS 3
-#define RCU_FANOUT	      (CONFIG_RCU_FANOUT)
-#define RCU_FANOUT_SQ	      (RCU_FANOUT * RCU_FANOUT)
-#define RCU_FANOUT_CUBE	      (RCU_FANOUT_SQ * RCU_FANOUT)
-
-#if NR_CPUS <= RCU_FANOUT
-#  define NUM_RCU_LVLS	      1
-#  define NUM_RCU_LVL_0	      1
-#  define NUM_RCU_LVL_1	      (NR_CPUS)
-#  define NUM_RCU_LVL_2	      0
-#  define NUM_RCU_LVL_3	      0
-#elif NR_CPUS <= RCU_FANOUT_SQ
-#  define NUM_RCU_LVLS	      2
-#  define NUM_RCU_LVL_0	      1
-#  define NUM_RCU_LVL_1	      (((NR_CPUS) + RCU_FANOUT - 1) / RCU_FANOUT)
-#  define NUM_RCU_LVL_2	      (NR_CPUS)
-#  define NUM_RCU_LVL_3	      0
-#elif NR_CPUS <= RCU_FANOUT_CUBE
-#  define NUM_RCU_LVLS	      3
-#  define NUM_RCU_LVL_0	      1
-#  define NUM_RCU_LVL_1	      (((NR_CPUS) + RCU_FANOUT_SQ - 1) / RCU_FANOUT_SQ)
-#  define NUM_RCU_LVL_2	      (((NR_CPUS) + (RCU_FANOUT) - 1) / (RCU_FANOUT))
-#  define NUM_RCU_LVL_3	      NR_CPUS
-#else
-# error "CONFIG_RCU_FANOUT insufficient for NR_CPUS"
-#endif /* #if (NR_CPUS) <= RCU_FANOUT */
-
-#define RCU_SUM (NUM_RCU_LVL_0 + NUM_RCU_LVL_1 + NUM_RCU_LVL_2 + NUM_RCU_LVL_3)
-#define NUM_RCU_NODES (RCU_SUM - NR_CPUS)
-
-/*
- * Dynticks per-CPU state.
- */
-struct rcu_dynticks {
-	int dynticks_nesting;	/* Track nesting level, sort of. */
-	int dynticks;		/* Even value for dynticks-idle, else odd. */
-	int dynticks_nmi;	/* Even value for either dynticks-idle or */
-				/*  not in nmi handler, else odd.  So this */
-				/*  remains even for nmi from irq handler. */
-};
-
-/*
- * Definition for node within the RCU grace-period-detection hierarchy.
- */
-struct rcu_node {
-	spinlock_t lock;
-	unsigned long qsmask;	/* CPUs or groups that need to switch in */
-				/*  order for current grace period to proceed.*/
-	unsigned long qsmaskinit;
-				/* Per-GP initialization for qsmask. */
-	unsigned long grpmask;	/* Mask to apply to parent qsmask. */
-	int	grplo;		/* lowest-numbered CPU or group here. */
-	int	grphi;		/* highest-numbered CPU or group here. */
-	u8	grpnum;		/* CPU/group number for next level up. */
-	u8	level;		/* root is at level 0. */
-	struct rcu_node *parent;
-} ____cacheline_internodealigned_in_smp;
-
-/* Index values for nxttail array in struct rcu_data. */
-#define RCU_DONE_TAIL		0	/* Also RCU_WAIT head. */
-#define RCU_WAIT_TAIL		1	/* Also RCU_NEXT_READY head. */
-#define RCU_NEXT_READY_TAIL	2	/* Also RCU_NEXT head. */
-#define RCU_NEXT_TAIL		3
-#define RCU_NEXT_SIZE		4
-
-/* Per-CPU data for read-copy update. */
-struct rcu_data {
-	/* 1) quiescent-state and grace-period handling : */
-	long		completed;	/* Track rsp->completed gp number */
-					/*  in order to detect GP end. */
-	long		gpnum;		/* Highest gp number that this CPU */
-					/*  is aware of having started. */
-	long		passed_quiesc_completed;
-					/* Value of completed at time of qs. */
-	bool		passed_quiesc;	/* User-mode/idle loop etc. */
-	bool		qs_pending;	/* Core waits for quiesc state. */
-	bool		beenonline;	/* CPU online at least once. */
-	struct rcu_node *mynode;	/* This CPU's leaf of hierarchy */
-	unsigned long grpmask;		/* Mask to apply to leaf qsmask. */
-
-	/* 2) batch handling */
-	/*
-	 * If nxtlist is not NULL, it is partitioned as follows.
-	 * Any of the partitions might be empty, in which case the
-	 * pointer to that partition will be equal to the pointer for
-	 * the following partition.  When the list is empty, all of
-	 * the nxttail elements point to nxtlist, which is NULL.
-	 *
-	 * [*nxttail[RCU_NEXT_READY_TAIL], NULL = *nxttail[RCU_NEXT_TAIL]):
-	 *	Entries that might have arrived after current GP ended
-	 * [*nxttail[RCU_WAIT_TAIL], *nxttail[RCU_NEXT_READY_TAIL]):
-	 *	Entries known to have arrived before current GP ended
-	 * [*nxttail[RCU_DONE_TAIL], *nxttail[RCU_WAIT_TAIL]):
-	 *	Entries that batch # <= ->completed - 1: waiting for current GP
-	 * [nxtlist, *nxttail[RCU_DONE_TAIL]):
-	 *	Entries that batch # <= ->completed
-	 *	The grace period for these entries has completed, and
-	 *	the other grace-period-completed entries may be moved
-	 *	here temporarily in rcu_process_callbacks().
-	 */
-	struct rcu_head *nxtlist;
-	struct rcu_head **nxttail[RCU_NEXT_SIZE];
-	long		qlen; 	 	/* # of queued callbacks */
-	long		blimit;		/* Upper limit on a processed batch */
-
-#ifdef CONFIG_NO_HZ
-	/* 3) dynticks interface. */
-	struct rcu_dynticks *dynticks;	/* Shared per-CPU dynticks state. */
-	int dynticks_snap;		/* Per-GP tracking for dynticks. */
-	int dynticks_nmi_snap;		/* Per-GP tracking for dynticks_nmi. */
-#endif /* #ifdef CONFIG_NO_HZ */
-
-	/* 4) reasons this CPU needed to be kicked by force_quiescent_state */
-#ifdef CONFIG_NO_HZ
-	unsigned long dynticks_fqs;	/* Kicked due to dynticks idle. */
-#endif /* #ifdef CONFIG_NO_HZ */
-	unsigned long offline_fqs;	/* Kicked due to being offline. */
-	unsigned long resched_ipi;	/* Sent a resched IPI. */
-
-	/* 5) __rcu_pending() statistics. */
-	long n_rcu_pending;		/* rcu_pending() calls since boot. */
-	long n_rp_qs_pending;
-	long n_rp_cb_ready;
-	long n_rp_cpu_needs_gp;
-	long n_rp_gp_completed;
-	long n_rp_gp_started;
-	long n_rp_need_fqs;
-	long n_rp_need_nothing;
-
-	int cpu;
-};
-
-/* Values for signaled field in struct rcu_state. */
-#define RCU_GP_INIT		0	/* Grace period being initialized. */
-#define RCU_SAVE_DYNTICK	1	/* Need to scan dyntick state. */
-#define RCU_FORCE_QS		2	/* Need to force quiescent state. */
-#ifdef CONFIG_NO_HZ
-#define RCU_SIGNAL_INIT		RCU_SAVE_DYNTICK
-#else /* #ifdef CONFIG_NO_HZ */
-#define RCU_SIGNAL_INIT		RCU_FORCE_QS
-#endif /* #else #ifdef CONFIG_NO_HZ */
-
-#define RCU_JIFFIES_TILL_FORCE_QS	 3	/* for rsp->jiffies_force_qs */
-#ifdef CONFIG_RCU_CPU_STALL_DETECTOR
-#define RCU_SECONDS_TILL_STALL_CHECK   (10 * HZ)  /* for rsp->jiffies_stall */
-#define RCU_SECONDS_TILL_STALL_RECHECK (30 * HZ)  /* for rsp->jiffies_stall */
-#define RCU_STALL_RAT_DELAY		2	  /* Allow other CPUs time */
-						  /*  to take at least one */
-						  /*  scheduling clock irq */
-						  /*  before ratting on them. */
-
-#endif /* #ifdef CONFIG_RCU_CPU_STALL_DETECTOR */
-
-/*
- * RCU global state, including node hierarchy.  This hierarchy is
- * represented in "heap" form in a dense array.  The root (first level)
- * of the hierarchy is in ->node[0] (referenced by ->level[0]), the second
- * level in ->node[1] through ->node[m] (->node[1] referenced by ->level[1]),
- * and the third level in ->node[m+1] and following (->node[m+1] referenced
- * by ->level[2]).  The number of levels is determined by the number of
- * CPUs and by CONFIG_RCU_FANOUT.  Small systems will have a "hierarchy"
- * consisting of a single rcu_node.
- */
-struct rcu_state {
-	struct rcu_node node[NUM_RCU_NODES];	/* Hierarchy. */
-	struct rcu_node *level[NUM_RCU_LVLS];	/* Hierarchy levels. */
-	u32 levelcnt[MAX_RCU_LVLS + 1];		/* # nodes in each level. */
-	u8 levelspread[NUM_RCU_LVLS];		/* kids/node in each level. */
-	struct rcu_data *rda[NR_CPUS];		/* array of rdp pointers. */
-
-	/* The following fields are guarded by the root rcu_node's lock. */
-
-	u8	signaled ____cacheline_internodealigned_in_smp;
-						/* Force QS state. */
-	long	gpnum;				/* Current gp number. */
-	long	completed;			/* # of last completed gp. */
-	spinlock_t onofflock;			/* exclude on/offline and */
-						/*  starting new GP. */
-	spinlock_t fqslock;			/* Only one task forcing */
-						/*  quiescent states. */
-	unsigned long jiffies_force_qs;		/* Time at which to invoke */
-						/*  force_quiescent_state(). */
-	unsigned long n_force_qs;		/* Number of calls to */
-						/*  force_quiescent_state(). */
-	unsigned long n_force_qs_lh;		/* ~Number of calls leaving */
-						/*  due to lock unavailable. */
-	unsigned long n_force_qs_ngp;		/* Number of calls leaving */
-						/*  due to no GP active. */
-#ifdef CONFIG_RCU_CPU_STALL_DETECTOR
-	unsigned long gp_start;			/* Time at which GP started, */
-						/*  but in jiffies. */
-	unsigned long jiffies_stall;		/* Time at which to check */
-						/*  for CPU stalls. */
-#endif /* #ifdef CONFIG_RCU_CPU_STALL_DETECTOR */
-#ifdef CONFIG_NO_HZ
-	long dynticks_completed;		/* Value of completed @ snap. */
-#endif /* #ifdef CONFIG_NO_HZ */
-};
-
 extern void rcu_qsctr_inc(int cpu);
 extern void rcu_bh_qsctr_inc(int cpu);
 

commit 03b042bf1dc14a268a3d65d38b4ec2a4261e8477
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Jun 25 09:08:16 2009 -0700

    rcu: Add synchronize_sched_expedited() primitive
    
    This adds the synchronize_sched_expedited() primitive that
    implements the "big hammer" expedited RCU grace periods.
    
    This primitive is placed in kernel/sched.c rather than
    kernel/rcupdate.c due to its need to interact closely with the
    migration_thread() kthread.
    
    The idea is to wake up this kthread with req->task set to NULL,
    in response to which the kthread reports the quiescent state
    resulting from the kthread having been scheduled.
    
    Because this patch needs to fallback to the slow versions of
    the primitives in response to some races with CPU onlining and
    offlining, a new synchronize_rcu_bh() primitive is added as
    well.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: akpm@linux-foundation.org
    Cc: torvalds@linux-foundation.org
    Cc: davem@davemloft.net
    Cc: dada1@cosmosbay.com
    Cc: zbr@ioremap.net
    Cc: jeff.chua.linux@gmail.com
    Cc: paulus@samba.org
    Cc: laijs@cn.fujitsu.com
    Cc: jengelh@medozas.de
    Cc: r000n@r000n.net
    Cc: benh@kernel.crashing.org
    Cc: mathieu.desnoyers@polymtl.ca
    LKML-Reference: <12459460982947-git-send-email->
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 5a5153806c42..d4dfd2489633 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -286,8 +286,14 @@ static inline void __rcu_read_unlock_bh(void)
 
 #define call_rcu_sched(head, func) call_rcu(head, func)
 
-static inline void rcu_init_sched(void)
+static inline void synchronize_rcu_expedited(void)
+{
+	synchronize_sched_expedited();
+}
+
+static inline void synchronize_rcu_bh_expedited(void)
 {
+	synchronize_sched_expedited();
 }
 
 extern void __rcu_init(void);
@@ -297,6 +303,10 @@ extern void rcu_restart_cpu(int cpu);
 extern long rcu_batches_completed(void);
 extern long rcu_batches_completed_bh(void);
 
+static inline void rcu_init_sched(void)
+{
+}
+
 #ifdef CONFIG_NO_HZ
 void rcu_enter_nohz(void);
 void rcu_exit_nohz(void);

commit 7ba5c840e64d4a967379f1ae3eca73278180b11d
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Apr 13 21:31:17 2009 -0700

    rcu: Add __rcu_pending tracing to hierarchical RCU
    
    Add tracing to __rcu_pending() to provide information on why RCU
    processing was kicked off.  This is helpful for debugging hierarchical
    RCU, and might also be helpful in learning how hierarchical RCU operates.
    
    Located-by: Anton Blanchard <anton@au1.ibm.com>
    Tested-by: Anton Blanchard <anton@au1.ibm.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: anton@samba.org
    Cc: akpm@linux-foundation.org
    Cc: dipankar@in.ibm.com
    Cc: manfred@colorfullife.com
    Cc: cl@linux-foundation.org
    Cc: josht@linux.vnet.ibm.com
    Cc: schamp@sgi.com
    Cc: niv@us.ibm.com
    Cc: dvhltc@us.ibm.com
    Cc: ego@in.ibm.com
    Cc: laijs@cn.fujitsu.com
    Cc: rostedt@goodmis.org
    Cc: peterz@infradead.org
    Cc: penberg@cs.helsinki.fi
    Cc: andi@firstfloor.org
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    LKML-Reference: <1239683479943-git-send-email->
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 58b2aa5312b9..5a5153806c42 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -161,8 +161,15 @@ struct rcu_data {
 	unsigned long offline_fqs;	/* Kicked due to being offline. */
 	unsigned long resched_ipi;	/* Sent a resched IPI. */
 
-	/* 5) For future __rcu_pending statistics. */
+	/* 5) __rcu_pending() statistics. */
 	long n_rcu_pending;		/* rcu_pending() calls since boot. */
+	long n_rp_qs_pending;
+	long n_rp_cb_ready;
+	long n_rp_cpu_needs_gp;
+	long n_rp_gp_completed;
+	long n_rp_gp_started;
+	long n_rp_need_fqs;
+	long n_rp_need_nothing;
 
 	int cpu;
 };

commit ef631b0ca01655d24e9ca7e199262c4a46416a26
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Apr 13 21:31:16 2009 -0700

    rcu: Make hierarchical RCU less IPI-happy
    
    This patch fixes a hierarchical-RCU performance bug located by Anton
    Blanchard.  The problem stems from a misguided attempt to provide a
    work-around for jiffies-counter failure.  This work-around uses a per-CPU
    n_rcu_pending counter, which is incremented on each call to rcu_pending(),
    which in turn is called from each scheduling-clock interrupt.  Each CPU
    then treats this counter as a surrogate for the jiffies counter, so
    that if the jiffies counter fails to advance, the per-CPU n_rcu_pending
    counter will cause RCU to invoke force_quiescent_state(), which in turn
    will (among other things) send resched IPIs to CPUs that have thus far
    failed to pass through an RCU quiescent state.
    
    Unfortunately, each CPU resets only its own counter after sending a
    batch of IPIs.  This means that the other CPUs will also (needlessly)
    send -another- round of IPIs, for a full N-squared set of IPIs in the
    worst case every three scheduler-clock ticks until the grace period
    finally ends.  It is not reasonable for a given CPU to reset each and
    every n_rcu_pending for all the other CPUs, so this patch instead simply
    disables the jiffies-counter "training wheels", thus eliminating the
    excessive IPIs.
    
    Note that the jiffies-counter IPIs do not have this problem due to
    the fact that the jiffies counter is global, so that the CPU sending
    the IPIs can easily reset things, thus preventing the other CPUs from
    sending redundant IPIs.
    
    Note also that the n_rcu_pending counter remains, as it will continue to
    be used for tracing.  It may also see use to update the jiffies counter,
    should an appropriate kick-the-jiffies-counter API appear.
    
    Located-by: Anton Blanchard <anton@au1.ibm.com>
    Tested-by: Anton Blanchard <anton@au1.ibm.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: anton@samba.org
    Cc: akpm@linux-foundation.org
    Cc: dipankar@in.ibm.com
    Cc: manfred@colorfullife.com
    Cc: cl@linux-foundation.org
    Cc: josht@linux.vnet.ibm.com
    Cc: schamp@sgi.com
    Cc: niv@us.ibm.com
    Cc: dvhltc@us.ibm.com
    Cc: ego@in.ibm.com
    Cc: laijs@cn.fujitsu.com
    Cc: rostedt@goodmis.org
    Cc: peterz@infradead.org
    Cc: penberg@cs.helsinki.fi
    Cc: andi@firstfloor.org
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    LKML-Reference: <12396834793575-git-send-email->
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 0cdda00f2b2a..58b2aa5312b9 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -161,9 +161,8 @@ struct rcu_data {
 	unsigned long offline_fqs;	/* Kicked due to being offline. */
 	unsigned long resched_ipi;	/* Sent a resched IPI. */
 
-	/* 5) state to allow this CPU to force_quiescent_state on others */
+	/* 5) For future __rcu_pending statistics. */
 	long n_rcu_pending;		/* rcu_pending() calls since boot. */
-	long n_rcu_pending_force_qs;	/* when to force quiescent states. */
 
 	int cpu;
 };

commit ac44021fccd8f1f2b267b004f23a2e8d7ef05f7b
Author: Eduard - Gabriel Munteanu <eduard.munteanu@linux360.ro>
Date:   Mon Mar 23 15:12:21 2009 +0200

    kmemtrace, rcu: don't include unnecessary headers, allow kmemtrace w/ tracepoints
    
    Impact: cleanup
    
    linux/percpu.h includes linux/slab.h, which generates circular inclusion
    dependencies when trying to switch kmemtrace to use tracepoints instead
    of markers.
    
    This patch allows tracing within slab headers' inline functions.
    
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: Eduard - Gabriel Munteanu <eduard.munteanu@linux360.ro>
    Cc: paulmck@linux.vnet.ibm.com
    LKML-Reference: <1237898630.25315.83.camel@penberg-laptop>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index 5d6f425260bc..0cdda00f2b2a 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -33,7 +33,6 @@
 #include <linux/cache.h>
 #include <linux/spinlock.h>
 #include <linux/threads.h>
-#include <linux/percpu.h>
 #include <linux/cpumask.h>
 #include <linux/seqlock.h>
 

commit b1f77b0581b8fd837acb4a973f7d5496cae6efee
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Mar 13 03:20:49 2009 +0100

    kmemtrace, rcu: fix linux/rcutree.h and linux/rcuclassic.h dependencies
    
    Impact: build fix for all non-x86 architectures
    
    We want to remove percpu.h from rcuclassic.h/rcutree.h (for upcoming
    kmemtrace changes) but that would break the DECLARE_PER_CPU based
    declarations in these files.
    
    Move the quiescent counter management functions to their respective
    RCU implementation .c files - they were slightly above the inlining
    limit anyway.
    
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: Eduard - Gabriel Munteanu <eduard.munteanu@linux360.ro>
    Cc: paulmck@linux.vnet.ibm.com
    LKML-Reference: <1237898630.25315.83.camel@penberg-laptop>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index a722fb67bb2d..5d6f425260bc 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -236,30 +236,8 @@ struct rcu_state {
 #endif /* #ifdef CONFIG_NO_HZ */
 };
 
-extern struct rcu_state rcu_state;
-DECLARE_PER_CPU(struct rcu_data, rcu_data);
-
-extern struct rcu_state rcu_bh_state;
-DECLARE_PER_CPU(struct rcu_data, rcu_bh_data);
-
-/*
- * Increment the quiescent state counter.
- * The counter is a bit degenerated: We do not need to know
- * how many quiescent states passed, just if there was at least
- * one since the start of the grace period. Thus just a flag.
- */
-static inline void rcu_qsctr_inc(int cpu)
-{
-	struct rcu_data *rdp = &per_cpu(rcu_data, cpu);
-	rdp->passed_quiesc = 1;
-	rdp->passed_quiesc_completed = rdp->completed;
-}
-static inline void rcu_bh_qsctr_inc(int cpu)
-{
-	struct rcu_data *rdp = &per_cpu(rcu_bh_data, cpu);
-	rdp->passed_quiesc = 1;
-	rdp->passed_quiesc_completed = rdp->completed;
-}
+extern void rcu_qsctr_inc(int cpu);
+extern void rcu_bh_qsctr_inc(int cpu);
 
 extern int rcu_pending(int cpu);
 extern int rcu_needs_cpu(int cpu);

commit a682604838763981613e42015cd0e39f2989d6bb
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Feb 25 18:03:42 2009 -0800

    rcu: Teach RCU that idle task is not quiscent state at boot
    
    This patch fixes a bug located by Vegard Nossum with the aid of
    kmemcheck, updated based on review comments from Nick Piggin,
    Ingo Molnar, and Andrew Morton.  And cleans up the variable-name
    and function-name language.  ;-)
    
    The boot CPU runs in the context of its idle thread during boot-up.
    During this time, idle_cpu(0) will always return nonzero, which will
    fool Classic and Hierarchical RCU into deciding that a large chunk of
    the boot-up sequence is a big long quiescent state.  This in turn causes
    RCU to prematurely end grace periods during this time.
    
    This patch changes the rcutree.c and rcuclassic.c rcu_check_callbacks()
    function to ignore the idle task as a quiescent state until the
    system has started up the scheduler in rest_init(), introducing a
    new non-API function rcu_idle_now_means_idle() to inform RCU of this
    transition.  RCU maintains an internal rcu_idle_cpu_truthful variable
    to track this state, which is then used by rcu_check_callback() to
    determine if it should believe idle_cpu().
    
    Because this patch has the effect of disallowing RCU grace periods
    during long stretches of the boot-up sequence, this patch also introduces
    Josh Triplett's UP-only optimization that makes synchronize_rcu() be a
    no-op if num_online_cpus() returns 1.  This allows boot-time code that
    calls synchronize_rcu() to proceed normally.  Note, however, that RCU
    callbacks registered by call_rcu() will likely queue up until later in
    the boot sequence.  Although rcuclassic and rcutree can also use this
    same optimization after boot completes, rcupreempt must restrict its
    use of this optimization to the portion of the boot sequence before the
    scheduler starts up, given that an rcupreempt RCU read-side critical
    section may be preeempted.
    
    In addition, this patch takes Nick Piggin's suggestion to make the
    system_state global variable be __read_mostly.
    
    Changes since v4:
    
    o       Changes the name of the introduced function and variable to
            be less emotional.  ;-)
    
    Changes since v3:
    
    o       WARN_ON(nr_context_switches() > 0) to verify that RCU
            switches out of boot-time mode before the first context
            switch, as suggested by Nick Piggin.
    
    Changes since v2:
    
    o       Created rcu_blocking_is_gp() internal-to-RCU API that
            determines whether a call to synchronize_rcu() is itself
            a grace period.
    
    o       The definition of rcu_blocking_is_gp() for rcuclassic and
            rcutree checks to see if but a single CPU is online.
    
    o       The definition of rcu_blocking_is_gp() for rcupreempt
            checks to see both if but a single CPU is online and if
            the system is still in early boot.
    
            This allows rcupreempt to again work correctly if running
            on a single CPU after booting is complete.
    
    o       Added check to rcupreempt's synchronize_sched() for there
            being but one online CPU.
    
    Tested all three variants both SMP and !SMP, booted fine, passed a short
    rcutorture test on both x86 and Power.
    
    Located-by: Vegard Nossum <vegard.nossum@gmail.com>
    Tested-by: Vegard Nossum <vegard.nossum@gmail.com>
    Tested-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
index d4368b7975c3..a722fb67bb2d 100644
--- a/include/linux/rcutree.h
+++ b/include/linux/rcutree.h
@@ -326,4 +326,10 @@ static inline void rcu_exit_nohz(void)
 }
 #endif /* CONFIG_NO_HZ */
 
+/* A context switch is a grace period for rcutree. */
+static inline int rcu_blocking_is_gp(void)
+{
+	return num_online_cpus() == 1;
+}
+
 #endif /* __LINUX_RCUTREE_H */

commit 64db4cfff99c04cd5f550357edcc8780f96b54a2
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Dec 18 21:55:32 2008 +0100

    "Tree RCU": scalable classic RCU implementation
    
    This patch fixes a long-standing performance bug in classic RCU that
    results in massive internal-to-RCU lock contention on systems with
    more than a few hundred CPUs.  Although this patch creates a separate
    flavor of RCU for ease of review and patch maintenance, it is intended
    to replace classic RCU.
    
    This patch still handles stress better than does mainline, so I am still
    calling it ready for inclusion.  This patch is against the -tip tree.
    Nevertheless, experience on an actual 1000+ CPU machine would still be
    most welcome.
    
    Most of the changes noted below were found while creating an rcutiny
    (which should permit ejecting the current rcuclassic) and while doing
    detailed line-by-line documentation.
    
    Updates from v9 (http://lkml.org/lkml/2008/12/2/334):
    
    o       Fixes from remainder of line-by-line code walkthrough,
            including comment spelling, initialization, undesirable
            narrowing due to type conversion, removing redundant memory
            barriers, removing redundant local-variable initialization,
            and removing redundant local variables.
    
            I do not believe that any of these fixes address the CPU-hotplug
            issues that Andi Kleen was seeing, but please do give it a whirl
            in case the machine is smarter than I am.
    
            A writeup from the walkthrough may be found at the following
            URL, in case you are suffering from terminal insomnia or
            masochism:
    
            http://www.kernel.org/pub/linux/kernel/people/paulmck/tmp/rcutree-walkthrough.2008.12.16a.pdf
    
    o       Made rcutree tracing use seq_file, as suggested some time
            ago by Lai Jiangshan.
    
    o       Added a .csv variant of the rcudata debugfs trace file, to allow
            people having thousands of CPUs to drop the data into
            a spreadsheet.  Tested with oocalc and gnumeric.  Updated
            documentation to suit.
    
    Updates from v8 (http://lkml.org/lkml/2008/11/15/139):
    
    o       Fix a theoretical race between grace-period initialization and
            force_quiescent_state() that could occur if more than three
            jiffies were required to carry out the grace-period
            initialization.  Which it might, if you had enough CPUs.
    
    o       Apply Ingo's printk-standardization patch.
    
    o       Substitute local variables for repeated accesses to global
            variables.
    
    o       Fix comment misspellings and redundant (but harmless) increments
            of ->n_rcu_pending (this latter after having explicitly added it).
    
    o       Apply checkpatch fixes.
    
    Updates from v7 (http://lkml.org/lkml/2008/10/10/291):
    
    o       Fixed a number of problems noted by Gautham Shenoy, including
            the cpu-stall-detection bug that he was having difficulty
            convincing me was real.  ;-)
    
    o       Changed cpu-stall detection to wait for ten seconds rather than
            three in order to reduce false positive, as suggested by Ingo
            Molnar.
    
    o       Produced a design document (http://lwn.net/Articles/305782/).
            The act of writing this document uncovered a number of both
            theoretical and "here and now" bugs as noted below.
    
    o       Fix dynticks_nesting accounting confusion, simplify WARN_ON()
            condition, fix kerneldoc comments, and add memory barriers
            in dynticks interface functions.
    
    o       Add more data to tracing.
    
    o       Remove unused "rcu_barrier" field from rcu_data structure.
    
    o       Count calls to rcu_pending() from scheduling-clock interrupt
            to use as a surrogate timebase should jiffies stop counting.
    
    o       Fix a theoretical race between force_quiescent_state() and
            grace-period initialization.  Yes, initialization does have to
            go on for some jiffies for this race to occur, but given enough
            CPUs...
    
    Updates from v6 (http://lkml.org/lkml/2008/9/23/448):
    
    o       Fix a number of checkpatch.pl complaints.
    
    o       Apply review comments from Ingo Molnar and Lai Jiangshan
            on the stall-detection code.
    
    o       Fix several bugs in !CONFIG_SMP builds.
    
    o       Fix a misspelled config-parameter name so that RCU now announces
            at boot time if stall detection is configured.
    
    o       Run tests on numerous combinations of configurations parameters,
            which after the fixes above, now build and run correctly.
    
    Updates from v5 (http://lkml.org/lkml/2008/9/15/92, bad subject line):
    
    o       Fix a compiler error in the !CONFIG_FANOUT_EXACT case (blew a
            changeset some time ago, and finally got around to retesting
            this option).
    
    o       Fix some tracing bugs in rcupreempt that caused incorrect
            totals to be printed.
    
    o       I now test with a more brutal random-selection online/offline
            script (attached).  Probably more brutal than it needs to be
            on the people reading it as well, but so it goes.
    
    o       A number of optimizations and usability improvements:
    
            o       Make rcu_pending() ignore the grace-period timeout when
                    there is no grace period in progress.
    
            o       Make force_quiescent_state() avoid going for a global
                    lock in the case where there is no grace period in
                    progress.
    
            o       Rearrange struct fields to improve struct layout.
    
            o       Make call_rcu() initiate a grace period if RCU was
                    idle, rather than waiting for the next scheduling
                    clock interrupt.
    
            o       Invoke rcu_irq_enter() and rcu_irq_exit() only when
                    idle, as suggested by Andi Kleen.  I still don't
                    completely trust this change, and might back it out.
    
            o       Make CONFIG_RCU_TRACE be the single config variable
                    manipulated for all forms of RCU, instead of the prior
                    confusion.
    
            o       Document tracing files and formats for both rcupreempt
                    and rcutree.
    
    Updates from v4 for those missing v5 given its bad subject line:
    
    o       Separated dynticks interface so that NMIs and irqs call separate
            functions, greatly simplifying it.  In particular, this code
            no longer requires a proof of correctness.  ;-)
    
    o       Separated dynticks state out into its own per-CPU structure,
            avoiding the duplicated accounting.
    
    o       The case where a dynticks-idle CPU runs an irq handler that
            invokes call_rcu() is now correctly handled, forcing that CPU
            out of dynticks-idle mode.
    
    o       Review comments have been applied (thank you all!!!).
            For but one example, fixed the dynticks-ordering issue that
            Manfred pointed out, saving me much debugging.  ;-)
    
    o       Adjusted rcuclassic and rcupreempt to handle dynticks changes.
    
    Attached is an updated patch to Classic RCU that applies a hierarchy,
    greatly reducing the contention on the top-level lock for large machines.
    This passes 10-hour concurrent rcutorture and online-offline testing on
    128-CPU ppc64 without dynticks enabled, and exposes some timekeeping
    bugs in presence of dynticks (exciting working on a system where
    "sleep 1" hangs until interrupted...), which were fixed in the
    2.6.27 kernel.  It is getting more reliable than mainline by some
    measures, so the next version will be against -tip for inclusion.
    See also Manfred Spraul's recent patches (or his earlier work from
    2004 at http://marc.info/?l=linux-kernel&m=108546384711797&w=2).
    We will converge onto a common patch in the fullness of time, but are
    currently exploring different regions of the design space.  That said,
    I have already gratefully stolen quite a few of Manfred's ideas.
    
    This patch provides CONFIG_RCU_FANOUT, which controls the bushiness
    of the RCU hierarchy.  Defaults to 32 on 32-bit machines and 64 on
    64-bit machines.  If CONFIG_NR_CPUS is less than CONFIG_RCU_FANOUT,
    there is no hierarchy.  By default, the RCU initialization code will
    adjust CONFIG_RCU_FANOUT to balance the hierarchy, so strongly NUMA
    architectures may choose to set CONFIG_RCU_FANOUT_EXACT to disable
    this balancing, allowing the hierarchy to be exactly aligned to the
    underlying hardware.  Up to two levels of hierarchy are permitted
    (in addition to the root node), allowing up to 16,384 CPUs on 32-bit
    systems and up to 262,144 CPUs on 64-bit systems.  I just know that I
    am going to regret saying this, but this seems more than sufficient
    for the foreseeable future.  (Some architectures might wish to set
    CONFIG_RCU_FANOUT=4, which would limit such architectures to 64 CPUs.
    If this becomes a real problem, additional levels can be added, but I
    doubt that it will make a significant difference on real hardware.)
    
    In the common case, a given CPU will manipulate its private rcu_data
    structure and the rcu_node structure that it shares with its immediate
    neighbors.  This can reduce both lock and memory contention by multiple
    orders of magnitude, which should eliminate the need for the strange
    manipulations that are reported to be required when running Linux on
    very large systems.
    
    Some shortcomings:
    
    o       More bugs will probably surface as a result of an ongoing
            line-by-line code inspection.
    
            Patches will be provided as required.
    
    o       There are probably hangs, rcutorture failures, &c.  Seems
            quite stable on a 128-CPU machine, but that is kind of small
            compared to 4096 CPUs.  However, seems to do better than
            mainline.
    
            Patches will be provided as required.
    
    o       The memory footprint of this version is several KB larger
            than rcuclassic.
    
            A separate UP-only rcutiny patch will be provided, which will
            reduce the memory footprint significantly, even compared
            to the old rcuclassic.  One such patch passes light testing,
            and has a memory footprint smaller even than rcuclassic.
            Initial reaction from various embedded guys was "it is not
            worth it", so am putting it aside.
    
    Credits:
    
    o       Manfred Spraul for ideas, review comments, and bugs spotted,
            as well as some good friendly competition.  ;-)
    
    o       Josh Triplett, Ingo Molnar, Peter Zijlstra, Mathieu Desnoyers,
            Lai Jiangshan, Andi Kleen, Andy Whitcroft, and Andrew Morton
            for reviews and comments.
    
    o       Thomas Gleixner for much-needed help with some timer issues
            (see patches below).
    
    o       Jon M. Tollefson, Tim Pepper, Andrew Theurer, Jose R. Santos,
            Andy Whitcroft, Darrick Wong, Nishanth Aravamudan, Anton
            Blanchard, Dave Kleikamp, and Nathan Lynch for keeping machines
            alive despite my heavy abuse^Wtesting.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
new file mode 100644
index 000000000000..d4368b7975c3
--- /dev/null
+++ b/include/linux/rcutree.h
@@ -0,0 +1,329 @@
+/*
+ * Read-Copy Update mechanism for mutual exclusion (tree-based version)
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+ *
+ * Copyright IBM Corporation, 2008
+ *
+ * Author: Dipankar Sarma <dipankar@in.ibm.com>
+ *	   Paul E. McKenney <paulmck@linux.vnet.ibm.com> Hierarchical algorithm
+ *
+ * Based on the original work by Paul McKenney <paulmck@us.ibm.com>
+ * and inputs from Rusty Russell, Andrea Arcangeli and Andi Kleen.
+ *
+ * For detailed explanation of Read-Copy Update mechanism see -
+ * 	Documentation/RCU
+ */
+
+#ifndef __LINUX_RCUTREE_H
+#define __LINUX_RCUTREE_H
+
+#include <linux/cache.h>
+#include <linux/spinlock.h>
+#include <linux/threads.h>
+#include <linux/percpu.h>
+#include <linux/cpumask.h>
+#include <linux/seqlock.h>
+
+/*
+ * Define shape of hierarchy based on NR_CPUS and CONFIG_RCU_FANOUT.
+ * In theory, it should be possible to add more levels straightforwardly.
+ * In practice, this has not been tested, so there is probably some
+ * bug somewhere.
+ */
+#define MAX_RCU_LVLS 3
+#define RCU_FANOUT	      (CONFIG_RCU_FANOUT)
+#define RCU_FANOUT_SQ	      (RCU_FANOUT * RCU_FANOUT)
+#define RCU_FANOUT_CUBE	      (RCU_FANOUT_SQ * RCU_FANOUT)
+
+#if NR_CPUS <= RCU_FANOUT
+#  define NUM_RCU_LVLS	      1
+#  define NUM_RCU_LVL_0	      1
+#  define NUM_RCU_LVL_1	      (NR_CPUS)
+#  define NUM_RCU_LVL_2	      0
+#  define NUM_RCU_LVL_3	      0
+#elif NR_CPUS <= RCU_FANOUT_SQ
+#  define NUM_RCU_LVLS	      2
+#  define NUM_RCU_LVL_0	      1
+#  define NUM_RCU_LVL_1	      (((NR_CPUS) + RCU_FANOUT - 1) / RCU_FANOUT)
+#  define NUM_RCU_LVL_2	      (NR_CPUS)
+#  define NUM_RCU_LVL_3	      0
+#elif NR_CPUS <= RCU_FANOUT_CUBE
+#  define NUM_RCU_LVLS	      3
+#  define NUM_RCU_LVL_0	      1
+#  define NUM_RCU_LVL_1	      (((NR_CPUS) + RCU_FANOUT_SQ - 1) / RCU_FANOUT_SQ)
+#  define NUM_RCU_LVL_2	      (((NR_CPUS) + (RCU_FANOUT) - 1) / (RCU_FANOUT))
+#  define NUM_RCU_LVL_3	      NR_CPUS
+#else
+# error "CONFIG_RCU_FANOUT insufficient for NR_CPUS"
+#endif /* #if (NR_CPUS) <= RCU_FANOUT */
+
+#define RCU_SUM (NUM_RCU_LVL_0 + NUM_RCU_LVL_1 + NUM_RCU_LVL_2 + NUM_RCU_LVL_3)
+#define NUM_RCU_NODES (RCU_SUM - NR_CPUS)
+
+/*
+ * Dynticks per-CPU state.
+ */
+struct rcu_dynticks {
+	int dynticks_nesting;	/* Track nesting level, sort of. */
+	int dynticks;		/* Even value for dynticks-idle, else odd. */
+	int dynticks_nmi;	/* Even value for either dynticks-idle or */
+				/*  not in nmi handler, else odd.  So this */
+				/*  remains even for nmi from irq handler. */
+};
+
+/*
+ * Definition for node within the RCU grace-period-detection hierarchy.
+ */
+struct rcu_node {
+	spinlock_t lock;
+	unsigned long qsmask;	/* CPUs or groups that need to switch in */
+				/*  order for current grace period to proceed.*/
+	unsigned long qsmaskinit;
+				/* Per-GP initialization for qsmask. */
+	unsigned long grpmask;	/* Mask to apply to parent qsmask. */
+	int	grplo;		/* lowest-numbered CPU or group here. */
+	int	grphi;		/* highest-numbered CPU or group here. */
+	u8	grpnum;		/* CPU/group number for next level up. */
+	u8	level;		/* root is at level 0. */
+	struct rcu_node *parent;
+} ____cacheline_internodealigned_in_smp;
+
+/* Index values for nxttail array in struct rcu_data. */
+#define RCU_DONE_TAIL		0	/* Also RCU_WAIT head. */
+#define RCU_WAIT_TAIL		1	/* Also RCU_NEXT_READY head. */
+#define RCU_NEXT_READY_TAIL	2	/* Also RCU_NEXT head. */
+#define RCU_NEXT_TAIL		3
+#define RCU_NEXT_SIZE		4
+
+/* Per-CPU data for read-copy update. */
+struct rcu_data {
+	/* 1) quiescent-state and grace-period handling : */
+	long		completed;	/* Track rsp->completed gp number */
+					/*  in order to detect GP end. */
+	long		gpnum;		/* Highest gp number that this CPU */
+					/*  is aware of having started. */
+	long		passed_quiesc_completed;
+					/* Value of completed at time of qs. */
+	bool		passed_quiesc;	/* User-mode/idle loop etc. */
+	bool		qs_pending;	/* Core waits for quiesc state. */
+	bool		beenonline;	/* CPU online at least once. */
+	struct rcu_node *mynode;	/* This CPU's leaf of hierarchy */
+	unsigned long grpmask;		/* Mask to apply to leaf qsmask. */
+
+	/* 2) batch handling */
+	/*
+	 * If nxtlist is not NULL, it is partitioned as follows.
+	 * Any of the partitions might be empty, in which case the
+	 * pointer to that partition will be equal to the pointer for
+	 * the following partition.  When the list is empty, all of
+	 * the nxttail elements point to nxtlist, which is NULL.
+	 *
+	 * [*nxttail[RCU_NEXT_READY_TAIL], NULL = *nxttail[RCU_NEXT_TAIL]):
+	 *	Entries that might have arrived after current GP ended
+	 * [*nxttail[RCU_WAIT_TAIL], *nxttail[RCU_NEXT_READY_TAIL]):
+	 *	Entries known to have arrived before current GP ended
+	 * [*nxttail[RCU_DONE_TAIL], *nxttail[RCU_WAIT_TAIL]):
+	 *	Entries that batch # <= ->completed - 1: waiting for current GP
+	 * [nxtlist, *nxttail[RCU_DONE_TAIL]):
+	 *	Entries that batch # <= ->completed
+	 *	The grace period for these entries has completed, and
+	 *	the other grace-period-completed entries may be moved
+	 *	here temporarily in rcu_process_callbacks().
+	 */
+	struct rcu_head *nxtlist;
+	struct rcu_head **nxttail[RCU_NEXT_SIZE];
+	long		qlen; 	 	/* # of queued callbacks */
+	long		blimit;		/* Upper limit on a processed batch */
+
+#ifdef CONFIG_NO_HZ
+	/* 3) dynticks interface. */
+	struct rcu_dynticks *dynticks;	/* Shared per-CPU dynticks state. */
+	int dynticks_snap;		/* Per-GP tracking for dynticks. */
+	int dynticks_nmi_snap;		/* Per-GP tracking for dynticks_nmi. */
+#endif /* #ifdef CONFIG_NO_HZ */
+
+	/* 4) reasons this CPU needed to be kicked by force_quiescent_state */
+#ifdef CONFIG_NO_HZ
+	unsigned long dynticks_fqs;	/* Kicked due to dynticks idle. */
+#endif /* #ifdef CONFIG_NO_HZ */
+	unsigned long offline_fqs;	/* Kicked due to being offline. */
+	unsigned long resched_ipi;	/* Sent a resched IPI. */
+
+	/* 5) state to allow this CPU to force_quiescent_state on others */
+	long n_rcu_pending;		/* rcu_pending() calls since boot. */
+	long n_rcu_pending_force_qs;	/* when to force quiescent states. */
+
+	int cpu;
+};
+
+/* Values for signaled field in struct rcu_state. */
+#define RCU_GP_INIT		0	/* Grace period being initialized. */
+#define RCU_SAVE_DYNTICK	1	/* Need to scan dyntick state. */
+#define RCU_FORCE_QS		2	/* Need to force quiescent state. */
+#ifdef CONFIG_NO_HZ
+#define RCU_SIGNAL_INIT		RCU_SAVE_DYNTICK
+#else /* #ifdef CONFIG_NO_HZ */
+#define RCU_SIGNAL_INIT		RCU_FORCE_QS
+#endif /* #else #ifdef CONFIG_NO_HZ */
+
+#define RCU_JIFFIES_TILL_FORCE_QS	 3	/* for rsp->jiffies_force_qs */
+#ifdef CONFIG_RCU_CPU_STALL_DETECTOR
+#define RCU_SECONDS_TILL_STALL_CHECK   (10 * HZ)  /* for rsp->jiffies_stall */
+#define RCU_SECONDS_TILL_STALL_RECHECK (30 * HZ)  /* for rsp->jiffies_stall */
+#define RCU_STALL_RAT_DELAY		2	  /* Allow other CPUs time */
+						  /*  to take at least one */
+						  /*  scheduling clock irq */
+						  /*  before ratting on them. */
+
+#endif /* #ifdef CONFIG_RCU_CPU_STALL_DETECTOR */
+
+/*
+ * RCU global state, including node hierarchy.  This hierarchy is
+ * represented in "heap" form in a dense array.  The root (first level)
+ * of the hierarchy is in ->node[0] (referenced by ->level[0]), the second
+ * level in ->node[1] through ->node[m] (->node[1] referenced by ->level[1]),
+ * and the third level in ->node[m+1] and following (->node[m+1] referenced
+ * by ->level[2]).  The number of levels is determined by the number of
+ * CPUs and by CONFIG_RCU_FANOUT.  Small systems will have a "hierarchy"
+ * consisting of a single rcu_node.
+ */
+struct rcu_state {
+	struct rcu_node node[NUM_RCU_NODES];	/* Hierarchy. */
+	struct rcu_node *level[NUM_RCU_LVLS];	/* Hierarchy levels. */
+	u32 levelcnt[MAX_RCU_LVLS + 1];		/* # nodes in each level. */
+	u8 levelspread[NUM_RCU_LVLS];		/* kids/node in each level. */
+	struct rcu_data *rda[NR_CPUS];		/* array of rdp pointers. */
+
+	/* The following fields are guarded by the root rcu_node's lock. */
+
+	u8	signaled ____cacheline_internodealigned_in_smp;
+						/* Force QS state. */
+	long	gpnum;				/* Current gp number. */
+	long	completed;			/* # of last completed gp. */
+	spinlock_t onofflock;			/* exclude on/offline and */
+						/*  starting new GP. */
+	spinlock_t fqslock;			/* Only one task forcing */
+						/*  quiescent states. */
+	unsigned long jiffies_force_qs;		/* Time at which to invoke */
+						/*  force_quiescent_state(). */
+	unsigned long n_force_qs;		/* Number of calls to */
+						/*  force_quiescent_state(). */
+	unsigned long n_force_qs_lh;		/* ~Number of calls leaving */
+						/*  due to lock unavailable. */
+	unsigned long n_force_qs_ngp;		/* Number of calls leaving */
+						/*  due to no GP active. */
+#ifdef CONFIG_RCU_CPU_STALL_DETECTOR
+	unsigned long gp_start;			/* Time at which GP started, */
+						/*  but in jiffies. */
+	unsigned long jiffies_stall;		/* Time at which to check */
+						/*  for CPU stalls. */
+#endif /* #ifdef CONFIG_RCU_CPU_STALL_DETECTOR */
+#ifdef CONFIG_NO_HZ
+	long dynticks_completed;		/* Value of completed @ snap. */
+#endif /* #ifdef CONFIG_NO_HZ */
+};
+
+extern struct rcu_state rcu_state;
+DECLARE_PER_CPU(struct rcu_data, rcu_data);
+
+extern struct rcu_state rcu_bh_state;
+DECLARE_PER_CPU(struct rcu_data, rcu_bh_data);
+
+/*
+ * Increment the quiescent state counter.
+ * The counter is a bit degenerated: We do not need to know
+ * how many quiescent states passed, just if there was at least
+ * one since the start of the grace period. Thus just a flag.
+ */
+static inline void rcu_qsctr_inc(int cpu)
+{
+	struct rcu_data *rdp = &per_cpu(rcu_data, cpu);
+	rdp->passed_quiesc = 1;
+	rdp->passed_quiesc_completed = rdp->completed;
+}
+static inline void rcu_bh_qsctr_inc(int cpu)
+{
+	struct rcu_data *rdp = &per_cpu(rcu_bh_data, cpu);
+	rdp->passed_quiesc = 1;
+	rdp->passed_quiesc_completed = rdp->completed;
+}
+
+extern int rcu_pending(int cpu);
+extern int rcu_needs_cpu(int cpu);
+
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+extern struct lockdep_map rcu_lock_map;
+# define rcu_read_acquire()	\
+			lock_acquire(&rcu_lock_map, 0, 0, 2, 1, NULL, _THIS_IP_)
+# define rcu_read_release()	lock_release(&rcu_lock_map, 1, _THIS_IP_)
+#else
+# define rcu_read_acquire()	do { } while (0)
+# define rcu_read_release()	do { } while (0)
+#endif
+
+static inline void __rcu_read_lock(void)
+{
+	preempt_disable();
+	__acquire(RCU);
+	rcu_read_acquire();
+}
+static inline void __rcu_read_unlock(void)
+{
+	rcu_read_release();
+	__release(RCU);
+	preempt_enable();
+}
+static inline void __rcu_read_lock_bh(void)
+{
+	local_bh_disable();
+	__acquire(RCU_BH);
+	rcu_read_acquire();
+}
+static inline void __rcu_read_unlock_bh(void)
+{
+	rcu_read_release();
+	__release(RCU_BH);
+	local_bh_enable();
+}
+
+#define __synchronize_sched() synchronize_rcu()
+
+#define call_rcu_sched(head, func) call_rcu(head, func)
+
+static inline void rcu_init_sched(void)
+{
+}
+
+extern void __rcu_init(void);
+extern void rcu_check_callbacks(int cpu, int user);
+extern void rcu_restart_cpu(int cpu);
+
+extern long rcu_batches_completed(void);
+extern long rcu_batches_completed_bh(void);
+
+#ifdef CONFIG_NO_HZ
+void rcu_enter_nohz(void);
+void rcu_exit_nohz(void);
+#else /* CONFIG_NO_HZ */
+static inline void rcu_enter_nohz(void)
+{
+}
+static inline void rcu_exit_nohz(void)
+{
+}
+#endif /* CONFIG_NO_HZ */
+
+#endif /* __LINUX_RCUTREE_H */
