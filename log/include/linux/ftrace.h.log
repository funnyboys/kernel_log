commit 1c3837266214c1e6fbbb96ff36bee13e923057d8
Merge: 3a8557e1aed0 ef9d965bc8b6
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 10 16:05:54 2020 -0700

    Merge branch 'work.sysctl' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs
    
    Pull sysctl fixes from Al Viro:
     "Fixups to regressions in sysctl series"
    
    * 'work.sysctl' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs:
      sysctl: reject gigantic reads/write to sysctl files
      cdrom: fix an incorrect __user annotation on cdrom_sysctl_info
      trace: fix an incorrect __user annotation on stack_trace_sysctl
      random: fix an incorrect __user annotation on proc_do_entropy
      net/sysctl: remove leftover __user annotations on neigh_proc_dointvec*
      net/sysctl: use cpumask_parse in flow_limit_cpu_sysctl

commit 7ff0d4490ebadae8037a079a70c5cac13385a808
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Jun 3 07:52:37 2020 +0200

    trace: fix an incorrect __user annotation on stack_trace_sysctl
    
    No user pointers for sysctls anymore.
    
    Fixes: 32927393dc1c ("sysctl: pass kernel pointers to ->proc_handler")
    Reported-by: build test robot <lkp@intel.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index ddfc377de0d2..fce81238f304 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -319,9 +319,8 @@ static inline void arch_ftrace_set_direct_caller(struct pt_regs *regs,
 
 extern int stack_tracer_enabled;
 
-int stack_trace_sysctl(struct ctl_table *table, int write,
-		       void __user *buffer, size_t *lenp,
-		       loff_t *ppos);
+int stack_trace_sysctl(struct ctl_table *table, int write, void *buffer,
+		       size_t *lenp, loff_t *ppos);
 
 /* DO NOT MODIFY THIS VARIABLE DIRECTLY! */
 DECLARE_PER_CPU(int, disable_stack_tracer);

commit da07f52d3caf6c24c6dbffb5500f379d819e04bd
Merge: 93d43e58683e f85c1598ddfe
Author: David S. Miller <davem@davemloft.net>
Date:   Fri May 15 13:48:59 2020 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net
    
    Move the bpf verifier trace check into the new switch statement in
    HEAD.
    
    Resolve the overlapping changes in hinic, where bug fixes overlap
    the addition of VF support.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 59566b0b622e3e6ea928c0b8cac8a5601b00b383
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu Apr 30 20:21:47 2020 -0400

    x86/ftrace: Have ftrace trampolines turn read-only at the end of system boot up
    
    Booting one of my machines, it triggered the following crash:
    
     Kernel/User page tables isolation: enabled
     ftrace: allocating 36577 entries in 143 pages
     Starting tracer 'function'
     BUG: unable to handle page fault for address: ffffffffa000005c
     #PF: supervisor write access in kernel mode
     #PF: error_code(0x0003) - permissions violation
     PGD 2014067 P4D 2014067 PUD 2015063 PMD 7b253067 PTE 7b252061
     Oops: 0003 [#1] PREEMPT SMP PTI
     CPU: 0 PID: 0 Comm: swapper Not tainted 5.4.0-test+ #24
     Hardware name: To Be Filled By O.E.M. To Be Filled By O.E.M./To be filled by O.E.M., BIOS SDBLI944.86P 05/08/2007
     RIP: 0010:text_poke_early+0x4a/0x58
     Code: 34 24 48 89 54 24 08 e8 bf 72 0b 00 48 8b 34 24 48 8b 4c 24 08 84 c0 74 0b 48 89 df f3 a4 48 83 c4 10 5b c3 9c 58 fa 48 89 df <f3> a4 50 9d 48 83 c4 10 5b e9 d6 f9 ff ff
    0 41 57 49
     RSP: 0000:ffffffff82003d38 EFLAGS: 00010046
     RAX: 0000000000000046 RBX: ffffffffa000005c RCX: 0000000000000005
     RDX: 0000000000000005 RSI: ffffffff825b9a90 RDI: ffffffffa000005c
     RBP: ffffffffa000005c R08: 0000000000000000 R09: ffffffff8206e6e0
     R10: ffff88807b01f4c0 R11: ffffffff8176c106 R12: ffffffff8206e6e0
     R13: ffffffff824f2440 R14: 0000000000000000 R15: ffffffff8206eac0
     FS:  0000000000000000(0000) GS:ffff88807d400000(0000) knlGS:0000000000000000
     CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
     CR2: ffffffffa000005c CR3: 0000000002012000 CR4: 00000000000006b0
     Call Trace:
      text_poke_bp+0x27/0x64
      ? mutex_lock+0x36/0x5d
      arch_ftrace_update_trampoline+0x287/0x2d5
      ? ftrace_replace_code+0x14b/0x160
      ? ftrace_update_ftrace_func+0x65/0x6c
      __register_ftrace_function+0x6d/0x81
      ftrace_startup+0x23/0xc1
      register_ftrace_function+0x20/0x37
      func_set_flag+0x59/0x77
      __set_tracer_option.isra.19+0x20/0x3e
      trace_set_options+0xd6/0x13e
      apply_trace_boot_options+0x44/0x6d
      register_tracer+0x19e/0x1ac
      early_trace_init+0x21b/0x2c9
      start_kernel+0x241/0x518
      ? load_ucode_intel_bsp+0x21/0x52
      secondary_startup_64+0xa4/0xb0
    
    I was able to trigger it on other machines, when I added to the kernel
    command line of both "ftrace=function" and "trace_options=func_stack_trace".
    
    The cause is the "ftrace=function" would register the function tracer
    and create a trampoline, and it will set it as executable and
    read-only. Then the "trace_options=func_stack_trace" would then update
    the same trampoline to include the stack tracer version of the function
    tracer. But since the trampoline already exists, it updates it with
    text_poke_bp(). The problem is that text_poke_bp() called while
    system_state == SYSTEM_BOOTING, it will simply do a memcpy() and not
    the page mapping, as it would think that the text is still read-write.
    But in this case it is not, and we take a fault and crash.
    
    Instead, lets keep the ftrace trampolines read-write during boot up,
    and then when the kernel executable text is set to read-only, the
    ftrace trampolines get set to read-only as well.
    
    Link: https://lkml.kernel.org/r/20200430202147.4dc6e2de@oasis.local.home
    
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: stable@vger.kernel.org
    Fixes: 768ae4406a5c ("x86/ftrace: Use text_poke()")
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index db95244a62d4..ab4bd15cbcdb 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -210,6 +210,29 @@ struct ftrace_ops {
 #endif
 };
 
+extern struct ftrace_ops __rcu *ftrace_ops_list;
+extern struct ftrace_ops ftrace_list_end;
+
+/*
+ * Traverse the ftrace_global_list, invoking all entries.  The reason that we
+ * can use rcu_dereference_raw_check() is that elements removed from this list
+ * are simply leaked, so there is no need to interact with a grace-period
+ * mechanism.  The rcu_dereference_raw_check() calls are needed to handle
+ * concurrent insertions into the ftrace_global_list.
+ *
+ * Silly Alpha and silly pointer-speculation compiler optimizations!
+ */
+#define do_for_each_ftrace_op(op, list)			\
+	op = rcu_dereference_raw_check(list);			\
+	do
+
+/*
+ * Optimized for just a single item in the list (as that is the normal case).
+ */
+#define while_for_each_ftrace_op(op)				\
+	while (likely(op = rcu_dereference_raw_check((op)->next)) &&	\
+	       unlikely((op) != &ftrace_list_end))
+
 /*
  * Type of the current tracing.
  */

commit 32927393dc1ccd60fb2bdc05b9e8e88753761469
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Apr 24 08:43:38 2020 +0200

    sysctl: pass kernel pointers to ->proc_handler
    
    Instead of having all the sysctl handlers deal with user pointers, which
    is rather hairy in terms of the BPF interaction, copy the input to and
    from  userspace in common code.  This also means that the strings are
    always NUL-terminated by the common code, making the API a little bit
    safer.
    
    As most handler just pass through the data to one of the common handlers
    a lot of the changes are mechnical.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Andrey Ignatov <rdna@fb.com>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index db95244a62d4..ddfc377de0d2 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -1005,8 +1005,7 @@ extern void disable_trace_on_warning(void);
 extern int __disable_trace_on_warning;
 
 int tracepoint_printk_sysctl(struct ctl_table *table, int write,
-			     void __user *buffer, size_t *lenp,
-			     loff_t *ppos);
+			     void *buffer, size_t *lenp, loff_t *ppos);
 
 #else /* CONFIG_TRACING */
 static inline void  disable_trace_on_warning(void) { }

commit 6674fdb25a9effc620c95d4c231a6ccc97b2f9b1
Merge: d1c6a2aa02af ff205766dbbe
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Dec 11 12:22:38 2019 -0800

    Merge tag 'trace-v5.5-3' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull tracing fixes from Steven Rostedt:
    
     - Remove code I accidentally applied when doing a minor fix up to a
       patch, and then using "git commit -a --amend", which pulled in some
       other changes I was playing with.
    
     - Remove an used variable in trace_events_inject code
    
     - Fix function graph tracer when it traces a ftrace direct function.
       It will now ignore tracing a function that has a ftrace direct
       tramploine attached. This is needed for eBPF to use the ftrace direct
       code.
    
    * tag 'trace-v5.5-3' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace:
      ftrace: Fix function_graph tracer interaction with BPF trampoline
      tracing: remove set but not used variable 'buffer'
      module: Remove accidental change of module_enable_x()

commit ff205766dbbee024a4a716638868d98ffb17748a
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Sun Dec 8 16:01:12 2019 -0800

    ftrace: Fix function_graph tracer interaction with BPF trampoline
    
    Depending on type of BPF programs served by BPF trampoline it can call original
    function. In such case the trampoline will skip one stack frame while
    returning. That will confuse function_graph tracer and will cause crashes with
    bad RIP. Teach graph tracer to skip functions that have BPF trampoline attached.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 232806d5689d..987c2dc55bde 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -264,6 +264,7 @@ int ftrace_modify_direct_caller(struct ftrace_func_entry *entry,
 				struct dyn_ftrace *rec,
 				unsigned long old_addr,
 				unsigned long new_addr);
+unsigned long ftrace_find_rec_direct(unsigned long ip);
 #else
 # define ftrace_direct_func_count 0
 static inline int register_ftrace_direct(unsigned long ip, unsigned long addr)
@@ -290,6 +291,10 @@ static inline int ftrace_modify_direct_caller(struct ftrace_func_entry *entry,
 {
 	return -ENODEV;
 }
+static inline unsigned long ftrace_find_rec_direct(unsigned long ip)
+{
+	return 0;
+}
 #endif /* CONFIG_DYNAMIC_FTRACE_WITH_DIRECT_CALLS */
 
 #ifndef CONFIG_HAVE_DYNAMIC_FTRACE_WITH_DIRECT_CALLS

commit 95f1fa9e3418d50ce099e67280b5497b9c93843b
Merge: 477093b3e144 16c0f03f629a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Nov 27 11:42:01 2019 -0800

    Merge tag 'trace-v5.5' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull tracing updates from Steven Rostedt:
     "New tracing features:
    
       - New PERMANENT flag to ftrace_ops when attaching a callback to a
         function.
    
         As /proc/sys/kernel/ftrace_enabled when set to zero will disable
         all attached callbacks in ftrace, this has a detrimental impact on
         live kernel tracing, as it disables all that it patched. If a
         ftrace_ops is registered to ftrace with the PERMANENT flag set, it
         will prevent ftrace_enabled from being disabled, and if
         ftrace_enabled is already disabled, it will prevent a ftrace_ops
         with PREMANENT flag set from being registered.
    
       - New register_ftrace_direct().
    
         As eBPF would like to register its own trampolines to be called by
         the ftrace nop locations directly, without going through the ftrace
         trampoline, this function has been added. This allows for eBPF
         trampolines to live along side of ftrace, perf, kprobe and live
         patching. It also utilizes the ftrace enabled_functions file that
         keeps track of functions that have been modified in the kernel, to
         allow for security auditing.
    
       - Allow for kernel internal use of ftrace instances.
    
         Subsystems in the kernel can now create and destroy their own
         tracing instances which allows them to have their own tracing
         buffer, and be able to record events without worrying about other
         users from writing over their data.
    
       - New seq_buf_hex_dump() that lets users use the hex_dump() in their
         seq_buf usage.
    
       - Notifications now added to tracing_max_latency to allow user space
         to know when a new max latency is hit by one of the latency
         tracers.
    
       - Wider spread use of generic compare operations for use of bsearch
         and friends.
    
       - More synthetic event fields may be defined (32 up from 16)
    
       - Use of xarray for architectures with sparse system calls, for the
         system call trace events.
    
      This along with small clean ups and fixes"
    
    * tag 'trace-v5.5' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace: (51 commits)
      tracing: Enable syscall optimization for MIPS
      tracing: Use xarray for syscall trace events
      tracing: Sample module to demonstrate kernel access to Ftrace instances.
      tracing: Adding new functions for kernel access to Ftrace instances
      tracing: Fix Kconfig indentation
      ring-buffer: Fix typos in function ring_buffer_producer
      ftrace: Use BIT() macro
      ftrace: Return ENOTSUPP when DYNAMIC_FTRACE_WITH_DIRECT_CALLS is not configured
      ftrace: Rename ftrace_graph_stub to ftrace_stub_graph
      ftrace: Add a helper function to modify_ftrace_direct() to allow arch optimization
      ftrace: Add helper find_direct_entry() to consolidate code
      ftrace: Add another check for match in register_ftrace_direct()
      ftrace: Fix accounting bug with direct->count in register_ftrace_direct()
      ftrace/selftests: Fix spelling mistake "wakeing" -> "waking"
      tracing: Increase SYNTH_FIELDS_MAX for synthetic_events
      ftrace/samples: Add a sample module that implements modify_ftrace_direct()
      ftrace: Add modify_ftrace_direct()
      tracing: Add missing "inline" in stub function of latency_fsnotify()
      tracing: Remove stray tab in TRACE_EVAL_MAP_FILE's help text
      tracing: Use seq_buf_hex_dump() to dump buffers
      ...

commit b41db132821fdad9d80a177344a47468791fbe62
Author: Enrico Weigelt, metux IT consult <info@metux.net>
Date:   Thu Nov 21 14:38:15 2019 +0100

    ftrace: Use BIT() macro
    
    It's cleaner to use the BIT() macro instead of raw shift operation.
    
    Link: http://lkml.kernel.org/r/20191121133815.15040-1-info@metux.net
    
    Signed-off-by: Enrico Weigelt, metux IT consult <info@metux.net>
    [ Added BIT() for bits 16 and 17 ]
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index af79b0f8cdc1..232806d5689d 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -149,24 +149,24 @@ ftrace_func_t ftrace_ops_get_func(struct ftrace_ops *ops);
  *            (internal ftrace only, should not be used by others)
  */
 enum {
-	FTRACE_OPS_FL_ENABLED			= 1 << 0,
-	FTRACE_OPS_FL_DYNAMIC			= 1 << 1,
-	FTRACE_OPS_FL_SAVE_REGS			= 1 << 2,
-	FTRACE_OPS_FL_SAVE_REGS_IF_SUPPORTED	= 1 << 3,
-	FTRACE_OPS_FL_RECURSION_SAFE		= 1 << 4,
-	FTRACE_OPS_FL_STUB			= 1 << 5,
-	FTRACE_OPS_FL_INITIALIZED		= 1 << 6,
-	FTRACE_OPS_FL_DELETED			= 1 << 7,
-	FTRACE_OPS_FL_ADDING			= 1 << 8,
-	FTRACE_OPS_FL_REMOVING			= 1 << 9,
-	FTRACE_OPS_FL_MODIFYING			= 1 << 10,
-	FTRACE_OPS_FL_ALLOC_TRAMP		= 1 << 11,
-	FTRACE_OPS_FL_IPMODIFY			= 1 << 12,
-	FTRACE_OPS_FL_PID			= 1 << 13,
-	FTRACE_OPS_FL_RCU			= 1 << 14,
-	FTRACE_OPS_FL_TRACE_ARRAY		= 1 << 15,
-	FTRACE_OPS_FL_PERMANENT                 = 1 << 16,
-	FTRACE_OPS_FL_DIRECT			= 1 << 17,
+	FTRACE_OPS_FL_ENABLED			= BIT(0),
+	FTRACE_OPS_FL_DYNAMIC			= BIT(1),
+	FTRACE_OPS_FL_SAVE_REGS			= BIT(2),
+	FTRACE_OPS_FL_SAVE_REGS_IF_SUPPORTED	= BIT(3),
+	FTRACE_OPS_FL_RECURSION_SAFE		= BIT(4),
+	FTRACE_OPS_FL_STUB			= BIT(5),
+	FTRACE_OPS_FL_INITIALIZED		= BIT(6),
+	FTRACE_OPS_FL_DELETED			= BIT(7),
+	FTRACE_OPS_FL_ADDING			= BIT(8),
+	FTRACE_OPS_FL_REMOVING			= BIT(9),
+	FTRACE_OPS_FL_MODIFYING			= BIT(10),
+	FTRACE_OPS_FL_ALLOC_TRAMP		= BIT(11),
+	FTRACE_OPS_FL_IPMODIFY			= BIT(12),
+	FTRACE_OPS_FL_PID			= BIT(13),
+	FTRACE_OPS_FL_RCU			= BIT(14),
+	FTRACE_OPS_FL_TRACE_ARRAY		= BIT(15),
+	FTRACE_OPS_FL_PERMANENT                 = BIT(16),
+	FTRACE_OPS_FL_DIRECT			= BIT(17),
 };
 
 #ifdef CONFIG_DYNAMIC_FTRACE

commit eb01fedc3d539f9443082aa2384c5d1ca26ed5c1
Author: Alexei Starovoitov <alexei.starovoitov@gmail.com>
Date:   Wed Nov 20 18:32:25 2019 -0500

    ftrace: Return ENOTSUPP when DYNAMIC_FTRACE_WITH_DIRECT_CALLS is not configured
    
    When CONFIG_DYNAMIC_FTRACE_WITH_DIRECT_CALLS is not set it's best to
    have the stub functions return ENOTSUPP instead of ENODEV,
    otherwise ENODEV is a valid error when ip is incorrect which is
    indistinguishable from ftrace not compiled in.
    
    Link: http://lkml.kernel.org/r/CAADnVQ+OzTikM9EhrfsC7NFsVYhATW1SVHxK64w3xn9qpk81pg@mail.gmail.com
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index dfaa37e1943d..af79b0f8cdc1 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -268,16 +268,16 @@ int ftrace_modify_direct_caller(struct ftrace_func_entry *entry,
 # define ftrace_direct_func_count 0
 static inline int register_ftrace_direct(unsigned long ip, unsigned long addr)
 {
-	return -ENODEV;
+	return -ENOTSUPP;
 }
 static inline int unregister_ftrace_direct(unsigned long ip, unsigned long addr)
 {
-	return -ENODEV;
+	return -ENOTSUPP;
 }
 static inline int modify_ftrace_direct(unsigned long ip,
 				       unsigned long old_addr, unsigned long new_addr)
 {
-	return -ENODEV;
+	return -ENOTSUPP;
 }
 static inline struct ftrace_direct_func *ftrace_find_direct_func(unsigned long addr)
 {

commit ea806eb3eab35528b578a061b2c4b28f0f92c465
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Sun Nov 17 17:04:15 2019 -0500

    ftrace: Add a helper function to modify_ftrace_direct() to allow arch optimization
    
    If a direct ftrace callback is at a location that does not have any other
    ftrace helpers attached to it, it is possible to simply just change the
    text to call the new caller (if the architecture supports it). But this
    requires special architecture code. Currently, modify_ftrace_direct() uses a
    trick to add a stub ftrace callback to the location forcing it to call the
    ftrace iterator. Then it can change the direct helper to call the new
    function in C, and then remove the stub. Removing the stub will have the
    location now call the new location that the direct helper is using.
    
    The new helper function does the registering the stub trick, but is a weak
    function, allowing an architecture to override it to do something a bit more
    direct.
    
    Link: https://lore.kernel.org/r/20191115215125.mbqv7taqnx376yed@ast-mbp.dhcp.thefacebook.com
    
    Suggested-by: Alexei Starovoitov <alexei.starovoitov@gmail.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 73eb2e93593f..dfaa37e1943d 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -246,12 +246,24 @@ static inline void ftrace_free_init_mem(void) { }
 static inline void ftrace_free_mem(struct module *mod, void *start, void *end) { }
 #endif /* CONFIG_FUNCTION_TRACER */
 
+struct ftrace_func_entry {
+	struct hlist_node hlist;
+	unsigned long ip;
+	unsigned long direct; /* for direct lookup only */
+};
+
+struct dyn_ftrace;
+
 #ifdef CONFIG_DYNAMIC_FTRACE_WITH_DIRECT_CALLS
 extern int ftrace_direct_func_count;
 int register_ftrace_direct(unsigned long ip, unsigned long addr);
 int unregister_ftrace_direct(unsigned long ip, unsigned long addr);
 int modify_ftrace_direct(unsigned long ip, unsigned long old_addr, unsigned long new_addr);
 struct ftrace_direct_func *ftrace_find_direct_func(unsigned long addr);
+int ftrace_modify_direct_caller(struct ftrace_func_entry *entry,
+				struct dyn_ftrace *rec,
+				unsigned long old_addr,
+				unsigned long new_addr);
 #else
 # define ftrace_direct_func_count 0
 static inline int register_ftrace_direct(unsigned long ip, unsigned long addr)
@@ -271,6 +283,13 @@ static inline struct ftrace_direct_func *ftrace_find_direct_func(unsigned long a
 {
 	return NULL;
 }
+static inline int ftrace_modify_direct_caller(struct ftrace_func_entry *entry,
+					      struct dyn_ftrace *rec,
+					      unsigned long old_addr,
+					      unsigned long new_addr)
+{
+	return -ENODEV;
+}
 #endif /* CONFIG_DYNAMIC_FTRACE_WITH_DIRECT_CALLS */
 
 #ifndef CONFIG_HAVE_DYNAMIC_FTRACE_WITH_DIRECT_CALLS
@@ -343,8 +362,6 @@ static inline void stack_tracer_enable(void) { }
 int ftrace_arch_code_modify_prepare(void);
 int ftrace_arch_code_modify_post_process(void);
 
-struct dyn_ftrace;
-
 enum ftrace_bug_type {
 	FTRACE_BUG_UNKNOWN,
 	FTRACE_BUG_INIT,

commit 0567d6809182df53da03636fad36c507c5cf07a5
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu Nov 14 14:39:35 2019 -0500

    ftrace: Add modify_ftrace_direct()
    
    Add a new function modify_ftrace_direct() that will allow a user to update
    an existing direct caller to a new trampoline, without missing hits due to
    unregistering one and then adding another.
    
    Link: https://lore.kernel.org/r/20191109022907.6zzo6orhxpt5n2sv@ast-mbp.dhcp.thefacebook.com
    
    Suggested-by: Alexei Starovoitov <alexei.starovoitov@gmail.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 55647e185141..73eb2e93593f 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -250,6 +250,7 @@ static inline void ftrace_free_mem(struct module *mod, void *start, void *end) {
 extern int ftrace_direct_func_count;
 int register_ftrace_direct(unsigned long ip, unsigned long addr);
 int unregister_ftrace_direct(unsigned long ip, unsigned long addr);
+int modify_ftrace_direct(unsigned long ip, unsigned long old_addr, unsigned long new_addr);
 struct ftrace_direct_func *ftrace_find_direct_func(unsigned long addr);
 #else
 # define ftrace_direct_func_count 0
@@ -261,6 +262,11 @@ static inline int unregister_ftrace_direct(unsigned long ip, unsigned long addr)
 {
 	return -ENODEV;
 }
+static inline int modify_ftrace_direct(unsigned long ip,
+				       unsigned long old_addr, unsigned long new_addr)
+{
+	return -ENODEV;
+}
 static inline struct ftrace_direct_func *ftrace_find_direct_func(unsigned long addr)
 {
 	return NULL;

commit a3ad1a7e39689005cb04a4f2adb82f9d55b4724f
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Fri Nov 8 13:12:57 2019 -0500

    ftrace/x86: Add a counter to test function_graph with direct
    
    As testing for direct calls from the function graph tracer adds a little
    overhead (which is a lot when tracing every function), add a counter that
    can be used to test if function_graph tracer needs to test for a direct
    caller or not.
    
    It would have been nicer if we could use a static branch, but the static
    branch logic fails when used within the function graph tracer trampoline.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 2bc7bd6b8387..55647e185141 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -247,10 +247,12 @@ static inline void ftrace_free_mem(struct module *mod, void *start, void *end) {
 #endif /* CONFIG_FUNCTION_TRACER */
 
 #ifdef CONFIG_DYNAMIC_FTRACE_WITH_DIRECT_CALLS
+extern int ftrace_direct_func_count;
 int register_ftrace_direct(unsigned long ip, unsigned long addr);
 int unregister_ftrace_direct(unsigned long ip, unsigned long addr);
 struct ftrace_direct_func *ftrace_find_direct_func(unsigned long addr);
 #else
+# define ftrace_direct_func_count 0
 static inline int register_ftrace_direct(unsigned long ip, unsigned long addr)
 {
 	return -ENODEV;

commit 562955fe6a558b9ef98ad87c470314946338cb2f
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Fri Nov 8 13:11:39 2019 -0500

    ftrace/x86: Add register_ftrace_direct() for custom trampolines
    
    Enable x86 to allow for register_ftrace_direct(), where a custom trampoline
    may be called directly from an ftrace mcount/fentry location.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 8b37b8105398..2bc7bd6b8387 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -272,6 +272,12 @@ static inline struct ftrace_direct_func *ftrace_find_direct_func(unsigned long a
  * via ftrace (because there's other callbacks besides the
  * direct call), can inform the architecture's trampoline that this
  * routine has a direct caller, and what the caller is.
+ *
+ * For example, in x86, it returns the direct caller
+ * callback function via the regs->orig_ax parameter.
+ * Then in the ftrace trampoline, if this is set, it makes
+ * the return from the trampoline jump to the direct caller
+ * instead of going back to the function it just traced.
  */
 static inline void arch_ftrace_set_direct_caller(struct pt_regs *regs,
 						 unsigned long addr) { }

commit 013bf0da0474816f57739daa006c8564ad7396a3
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Fri Nov 8 13:11:27 2019 -0500

    ftrace: Add ftrace_find_direct_func()
    
    As function_graph tracer modifies the return address to insert a trampoline
    to trace the return of a function, it must be aware of a direct caller, as
    when it gets called, the function's return address may not be at on the
    stack where it expects. It may have to see if that return address points to
    the a direct caller and adjust if it is.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index efe3e521aff4..8b37b8105398 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -51,6 +51,7 @@ static inline void early_trace_init(void) { }
 
 struct module;
 struct ftrace_hash;
+struct ftrace_direct_func;
 
 #if defined(CONFIG_FUNCTION_TRACER) && defined(CONFIG_MODULES) && \
 	defined(CONFIG_DYNAMIC_FTRACE)
@@ -248,6 +249,7 @@ static inline void ftrace_free_mem(struct module *mod, void *start, void *end) {
 #ifdef CONFIG_DYNAMIC_FTRACE_WITH_DIRECT_CALLS
 int register_ftrace_direct(unsigned long ip, unsigned long addr);
 int unregister_ftrace_direct(unsigned long ip, unsigned long addr);
+struct ftrace_direct_func *ftrace_find_direct_func(unsigned long addr);
 #else
 static inline int register_ftrace_direct(unsigned long ip, unsigned long addr)
 {
@@ -257,6 +259,10 @@ static inline int unregister_ftrace_direct(unsigned long ip, unsigned long addr)
 {
 	return -ENODEV;
 }
+static inline struct ftrace_direct_func *ftrace_find_direct_func(unsigned long addr)
+{
+	return NULL;
+}
 #endif /* CONFIG_DYNAMIC_FTRACE_WITH_DIRECT_CALLS */
 
 #ifndef CONFIG_HAVE_DYNAMIC_FTRACE_WITH_DIRECT_CALLS

commit 763e34e74bb7d5c316015e2e39fcc8520bfd071c
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Fri Nov 8 13:07:06 2019 -0500

    ftrace: Add register_ftrace_direct()
    
    Add the start of the functionality to allow other trampolines to use the
    ftrace mcount/fentry/nop location. This adds two new functions:
    
     register_ftrace_direct() and unregister_ftrace_direct()
    
    Both take two parameters: the first is the instruction address of where the
    mcount/fentry/nop exists, and the second is the trampoline to have that
    location called.
    
    This will handle cases where ftrace is already used on that same location,
    and will make it still work, where the registered direct called trampoline
    will get called after all the registered ftrace callers are handled.
    
    Currently, it will not allow for IP_MODIFY functions to be called at the
    same locations, which include some kprobes and live kernel patching.
    
    At this point, no architecture supports this. This is only the start of
    implementing the framework.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 8385cafe4f9f..efe3e521aff4 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -144,6 +144,8 @@ ftrace_func_t ftrace_ops_get_func(struct ftrace_ops *ops);
  * TRACE_ARRAY - The ops->private points to a trace_array descriptor.
  * PERMANENT - Set when the ops is permanent and should not be affected by
  *             ftrace_enabled.
+ * DIRECT - Used by the direct ftrace_ops helper for direct functions
+ *            (internal ftrace only, should not be used by others)
  */
 enum {
 	FTRACE_OPS_FL_ENABLED			= 1 << 0,
@@ -163,6 +165,7 @@ enum {
 	FTRACE_OPS_FL_RCU			= 1 << 14,
 	FTRACE_OPS_FL_TRACE_ARRAY		= 1 << 15,
 	FTRACE_OPS_FL_PERMANENT                 = 1 << 16,
+	FTRACE_OPS_FL_DIRECT			= 1 << 17,
 };
 
 #ifdef CONFIG_DYNAMIC_FTRACE
@@ -242,6 +245,32 @@ static inline void ftrace_free_init_mem(void) { }
 static inline void ftrace_free_mem(struct module *mod, void *start, void *end) { }
 #endif /* CONFIG_FUNCTION_TRACER */
 
+#ifdef CONFIG_DYNAMIC_FTRACE_WITH_DIRECT_CALLS
+int register_ftrace_direct(unsigned long ip, unsigned long addr);
+int unregister_ftrace_direct(unsigned long ip, unsigned long addr);
+#else
+static inline int register_ftrace_direct(unsigned long ip, unsigned long addr)
+{
+	return -ENODEV;
+}
+static inline int unregister_ftrace_direct(unsigned long ip, unsigned long addr)
+{
+	return -ENODEV;
+}
+#endif /* CONFIG_DYNAMIC_FTRACE_WITH_DIRECT_CALLS */
+
+#ifndef CONFIG_HAVE_DYNAMIC_FTRACE_WITH_DIRECT_CALLS
+/*
+ * This must be implemented by the architecture.
+ * It is the way the ftrace direct_ops helper, when called
+ * via ftrace (because there's other callbacks besides the
+ * direct call), can inform the architecture's trampoline that this
+ * routine has a direct caller, and what the caller is.
+ */
+static inline void arch_ftrace_set_direct_caller(struct pt_regs *regs,
+						 unsigned long addr) { }
+#endif /* CONFIG_HAVE_DYNAMIC_FTRACE_WITH_DIRECT_CALLS */
+
 #ifdef CONFIG_STACK_TRACER
 
 extern int stack_tracer_enabled;
@@ -333,6 +362,7 @@ bool is_ftrace_trampoline(unsigned long addr);
  *  REGS_EN - the function is set up to save regs.
  *  IPMODIFY - the record allows for the IP address to be changed.
  *  DISABLED - the record is not ready to be touched yet
+ *  DIRECT   - there is a direct function to call
  *
  * When a new ftrace_ops is registered and wants a function to save
  * pt_regs, the rec->flag REGS is set. When the function has been
@@ -348,10 +378,12 @@ enum {
 	FTRACE_FL_TRAMP_EN	= (1UL << 27),
 	FTRACE_FL_IPMODIFY	= (1UL << 26),
 	FTRACE_FL_DISABLED	= (1UL << 25),
+	FTRACE_FL_DIRECT	= (1UL << 24),
+	FTRACE_FL_DIRECT_EN	= (1UL << 23),
 };
 
-#define FTRACE_REF_MAX_SHIFT	25
-#define FTRACE_FL_BITS		7
+#define FTRACE_REF_MAX_SHIFT	23
+#define FTRACE_FL_BITS		9
 #define FTRACE_FL_MASKED_BITS	((1UL << FTRACE_FL_BITS) - 1)
 #define FTRACE_FL_MASK		(FTRACE_FL_MASKED_BITS << FTRACE_REF_MAX_SHIFT)
 #define FTRACE_REF_MAX		((1UL << FTRACE_REF_MAX_SHIFT) - 1)

commit a1326b17ac03a9012cb3d01e434aacb4d67a416c
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Wed Oct 16 18:17:11 2019 +0100

    module/ftrace: handle patchable-function-entry
    
    When using patchable-function-entry, the compiler will record the
    callsites into a section named "__patchable_function_entries" rather
    than "__mcount_loc". Let's abstract this difference behind a new
    FTRACE_CALLSITE_SECTION, so that architectures don't have to handle this
    explicitly (e.g. with custom module linker scripts).
    
    As parisc currently handles this explicitly, it is fixed up accordingly,
    with its custom linker script removed. Since FTRACE_CALLSITE_SECTION is
    only defined when DYNAMIC_FTRACE is selected, the parisc module loading
    code is updated to only use the definition in that case. When
    DYNAMIC_FTRACE is not selected, modules shouldn't have this section, so
    this removes some redundant work in that case.
    
    To make sure that this is keep up-to-date for modules and the main
    kernel, a comment is added to vmlinux.lds.h, with the existing ifdeffery
    simplified for legibility.
    
    I built parisc generic-{32,64}bit_defconfig with DYNAMIC_FTRACE enabled,
    and verified that the section made it into the .ko files for modules.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Helge Deller <deller@gmx.de>
    Acked-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Reviewed-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Reviewed-by: Torsten Duwe <duwe@suse.de>
    Tested-by: Amit Daniel Kachhap <amit.kachhap@arm.com>
    Tested-by: Sven Schnelle <svens@stackframe.org>
    Tested-by: Torsten Duwe <duwe@suse.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: James E.J. Bottomley <James.Bottomley@HansenPartnership.com>
    Cc: Jessica Yu <jeyu@kernel.org>
    Cc: linux-parisc@vger.kernel.org

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 9867d90d635e..9141f2263286 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -738,6 +738,11 @@ static inline unsigned long get_lock_parent_ip(void)
 
 #ifdef CONFIG_FTRACE_MCOUNT_RECORD
 extern void ftrace_init(void);
+#ifdef CC_USING_PATCHABLE_FUNCTION_ENTRY
+#define FTRACE_CALLSITE_SECTION	"__patchable_function_entries"
+#else
+#define FTRACE_CALLSITE_SECTION	"__mcount_loc"
+#endif
 #else
 static inline void ftrace_init(void) { }
 #endif

commit fbf6c73c5b264c25484fa9f449b5546569fe11f0
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Wed Oct 16 17:51:10 2019 +0100

    ftrace: add ftrace_init_nop()
    
    Architectures may need to perform special initialization of ftrace
    callsites, and today they do so by special-casing ftrace_make_nop() when
    the expected branch address is MCOUNT_ADDR. In some cases (e.g. for
    patchable-function-entry), we don't have an mcount-like symbol and don't
    want a synthetic MCOUNT_ADDR, but we may need to perform some
    initialization of callsites.
    
    To make it possible to separate initialization from runtime
    modification, and to handle cases without an mcount-like symbol, this
    patch adds an optional ftrace_init_nop() function that architectures can
    implement, which does not pass a branch address.
    
    Where an architecture does not provide ftrace_init_nop(), we will fall
    back to the existing behaviour of calling ftrace_make_nop() with
    MCOUNT_ADDR.
    
    At the same time, ftrace_code_disable() is renamed to
    ftrace_nop_initialize() to make it clearer that it is intended to
    intialize a callsite into a disabled state, and is not for disabling a
    callsite that has been runtime enabled. The kerneldoc description of rec
    arguments is updated to cover non-mcount callsites.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Amit Daniel Kachhap <amit.kachhap@arm.com>
    Reviewed-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Reviewed-by: Miroslav Benes <mbenes@suse.cz>
    Reviewed-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Reviewed-by: Torsten Duwe <duwe@suse.de>
    Tested-by: Amit Daniel Kachhap <amit.kachhap@arm.com>
    Tested-by: Sven Schnelle <svens@stackframe.org>
    Tested-by: Torsten Duwe <duwe@suse.de>
    Cc: Ingo Molnar <mingo@redhat.com>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 8a8cb3c401b2..9867d90d635e 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -499,7 +499,7 @@ static inline int ftrace_disable_ftrace_graph_caller(void) { return 0; }
 /**
  * ftrace_make_nop - convert code into nop
  * @mod: module structure if called by module load initialization
- * @rec: the mcount call site record
+ * @rec: the call site record (e.g. mcount/fentry)
  * @addr: the address that the call site should be calling
  *
  * This is a very sensitive operation and great care needs
@@ -520,9 +520,38 @@ static inline int ftrace_disable_ftrace_graph_caller(void) { return 0; }
 extern int ftrace_make_nop(struct module *mod,
 			   struct dyn_ftrace *rec, unsigned long addr);
 
+
+/**
+ * ftrace_init_nop - initialize a nop call site
+ * @mod: module structure if called by module load initialization
+ * @rec: the call site record (e.g. mcount/fentry)
+ *
+ * This is a very sensitive operation and great care needs
+ * to be taken by the arch.  The operation should carefully
+ * read the location, check to see if what is read is indeed
+ * what we expect it to be, and then on success of the compare,
+ * it should write to the location.
+ *
+ * The code segment at @rec->ip should contain the contents created by
+ * the compiler
+ *
+ * Return must be:
+ *  0 on success
+ *  -EFAULT on error reading the location
+ *  -EINVAL on a failed compare of the contents
+ *  -EPERM  on error writing to the location
+ * Any other value will be considered a failure.
+ */
+#ifndef ftrace_init_nop
+static inline int ftrace_init_nop(struct module *mod, struct dyn_ftrace *rec)
+{
+	return ftrace_make_nop(mod, rec, MCOUNT_ADDR);
+}
+#endif
+
 /**
  * ftrace_make_call - convert a nop call site into a call to addr
- * @rec: the mcount call site record
+ * @rec: the call site record (e.g. mcount/fentry)
  * @addr: the address that the call site should call
  *
  * This is a very sensitive operation and great care needs
@@ -545,7 +574,7 @@ extern int ftrace_make_call(struct dyn_ftrace *rec, unsigned long addr);
 #ifdef CONFIG_DYNAMIC_FTRACE_WITH_REGS
 /**
  * ftrace_modify_call - convert from one addr to another (no nop)
- * @rec: the mcount call site record
+ * @rec: the call site record (e.g. mcount/fentry)
  * @old_addr: the address expected to be currently called to
  * @addr: the address to change to
  *

commit 7162431dcf72032835d369c8d7b51311df407938
Author: Miroslav Benes <mbenes@suse.cz>
Date:   Wed Oct 16 13:33:13 2019 +0200

    ftrace: Introduce PERMANENT ftrace_ops flag
    
    Livepatch uses ftrace for redirection to new patched functions. It means
    that if ftrace is disabled, all live patched functions are disabled as
    well. Toggling global 'ftrace_enabled' sysctl thus affect it directly.
    It is not a problem per se, because only administrator can set sysctl
    values, but it still may be surprising.
    
    Introduce PERMANENT ftrace_ops flag to amend this. If the
    FTRACE_OPS_FL_PERMANENT is set on any ftrace ops, the tracing cannot be
    disabled by disabling ftrace_enabled. Equally, a callback with the flag
    set cannot be registered if ftrace_enabled is disabled.
    
    Link: http://lkml.kernel.org/r/20191016113316.13415-2-mbenes@suse.cz
    
    Reviewed-by: Petr Mladek <pmladek@suse.com>
    Reviewed-by: Kamalesh Babulal <kamalesh@linux.vnet.ibm.com>
    Signed-off-by: Miroslav Benes <mbenes@suse.cz>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 8a8cb3c401b2..8385cafe4f9f 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -142,6 +142,8 @@ ftrace_func_t ftrace_ops_get_func(struct ftrace_ops *ops);
  * PID     - Is affected by set_ftrace_pid (allows filtering on those pids)
  * RCU     - Set when the ops can only be called when RCU is watching.
  * TRACE_ARRAY - The ops->private points to a trace_array descriptor.
+ * PERMANENT - Set when the ops is permanent and should not be affected by
+ *             ftrace_enabled.
  */
 enum {
 	FTRACE_OPS_FL_ENABLED			= 1 << 0,
@@ -160,6 +162,7 @@ enum {
 	FTRACE_OPS_FL_PID			= 1 << 13,
 	FTRACE_OPS_FL_RCU			= 1 << 14,
 	FTRACE_OPS_FL_TRACE_ARRAY		= 1 << 15,
+	FTRACE_OPS_FL_PERMANENT                 = 1 << 16,
 };
 
 #ifdef CONFIG_DYNAMIC_FTRACE

commit 7375dca1647fa978310f2d706ddbff537f72110b
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Mon May 20 09:26:24 2019 -0400

    ftrace: Make enable and update parameters bool when applicable
    
    The code modification functions have "enable" and "update" variables that
    are sometimes "int" but used as "bool". Remove the ambiguity and make them
    "bool" when they are only used for true or false values.
    
    Link: http://lkml.kernel.org/r/e1429923d9eda92a3cf5ee9e33c7eacce539781d.1558115654.git.naveen.n.rao@linux.vnet.ibm.com
    
    Reported-by: "Naveen N. Rao" <naveen.n.rao@linux.vnet.ibm.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 25e2995d4a4c..8a8cb3c401b2 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -427,8 +427,8 @@ struct dyn_ftrace *ftrace_rec_iter_record(struct ftrace_rec_iter *iter);
 	     iter = ftrace_rec_iter_next(iter))
 
 
-int ftrace_update_record(struct dyn_ftrace *rec, int enable);
-int ftrace_test_record(struct dyn_ftrace *rec, int enable);
+int ftrace_update_record(struct dyn_ftrace *rec, bool enable);
+int ftrace_test_record(struct dyn_ftrace *rec, bool enable);
 void ftrace_run_stop_machine(int command);
 unsigned long ftrace_location(unsigned long ip);
 unsigned long ftrace_location_range(unsigned long start, unsigned long end);

commit d2d8b146043ae7e250aef1fb312971f6f479d487
Merge: 2bbacd1a9278 693713cbdb3a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed May 15 16:05:47 2019 -0700

    Merge tag 'trace-v5.2' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull tracing updates from Steven Rostedt:
     "The major changes in this tracing update includes:
    
       - Removal of non-DYNAMIC_FTRACE from 32bit x86
    
       - Removal of mcount support from x86
    
       - Emulating a call from int3 on x86_64, fixes live kernel patching
    
       - Consolidated Tracing Error logs file
    
      Minor updates:
    
       - Removal of klp_check_compiler_support()
    
       - kdb ftrace dumping output changes
    
       - Accessing and creating ftrace instances from inside the kernel
    
       - Clean up of #define if macro
    
       - Introduction of TRACE_EVENT_NOP() to disable trace events based on
         config options
    
      And other minor fixes and clean ups"
    
    * tag 'trace-v5.2' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace: (44 commits)
      x86: Hide the int3_emulate_call/jmp functions from UML
      livepatch: Remove klp_check_compiler_support()
      ftrace/x86: Remove mcount support
      ftrace/x86_32: Remove support for non DYNAMIC_FTRACE
      tracing: Simplify "if" macro code
      tracing: Fix documentation about disabling options using trace_options
      tracing: Replace kzalloc with kcalloc
      tracing: Fix partial reading of trace event's id file
      tracing: Allow RCU to run between postponed startup tests
      tracing: Fix white space issues in parse_pred() function
      tracing: Eliminate const char[] auto variables
      ring-buffer: Fix mispelling of Calculate
      tracing: probeevent: Fix to make the type of $comm string
      tracing: probeevent: Do not accumulate on ret variable
      tracing: uprobes: Re-enable $comm support for uprobe events
      ftrace/x86_64: Emulate call function while updating in breakpoint handler
      x86_64: Allow breakpoints to emulate call instructions
      x86_64: Add gap to int3 to allow for call emulation
      tracing: kdb: Allow ftdump to skip all but the last few entries
      tracing: Add trace_total_entries() / trace_total_entries_cpu()
      ...

commit e8025bab7bfbd48d262c01c26c15a9d465fbb083
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Wed Apr 24 12:34:46 2019 -0400

    function_graph: Place ftrace_graph_entry_stub() prototype in include/linux/ftrace.h
    
    ftrace_graph_entry_stub() is defined in generic code, its prototype should
    be in the generic header and not defined throughout architecture specific
    code in order to use it.
    
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Cc: "James E.J. Bottomley" <James.Bottomley@HansenPartnership.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: linux-parisc@vger.kernel.org
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 730876187344..9b28fce436ca 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -751,6 +751,8 @@ struct ftrace_graph_ret {
 typedef void (*trace_func_graph_ret_t)(struct ftrace_graph_ret *); /* return */
 typedef int (*trace_func_graph_ent_t)(struct ftrace_graph_ent *); /* entry */
 
+extern int ftrace_graph_entry_stub(struct ftrace_graph_ent *trace);
+
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
 
 struct fgraph_ops {

commit 3d9a8072915366b5932beeed97f158f8d4955768
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Apr 25 11:44:54 2019 +0200

    tracing: Cleanup stack trace code
    
    - Remove the extra array member of stack_dump_trace[] along with the
      ARRAY_SIZE - 1 initialization for struct stack_trace :: max_entries.
    
      Both are historical leftovers of no value. The stack tracer never exceeds
      the array and there is no extra storage requirement either.
    
    - Make variables which are only used in trace_stack.c static.
    
    - Simplify the enable/disable logic.
    
    - Rename stack_trace_print() as it's using the stack_trace_ namespace. Free
      the name up for stack trace related functions.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Steven Rostedt <rostedt@goodmis.org>
    Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: linux-mm@kvack.org
    Cc: David Rientjes <rientjes@google.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: kasan-dev@googlegroups.com
    Cc: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Cc: Akinobu Mita <akinobu.mita@gmail.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: iommu@lists.linux-foundation.org
    Cc: Robin Murphy <robin.murphy@arm.com>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Cc: Johannes Thumshirn <jthumshirn@suse.de>
    Cc: David Sterba <dsterba@suse.com>
    Cc: Chris Mason <clm@fb.com>
    Cc: Josef Bacik <josef@toxicpanda.com>
    Cc: linux-btrfs@vger.kernel.org
    Cc: dm-devel@redhat.com
    Cc: Mike Snitzer <snitzer@redhat.com>
    Cc: Alasdair Kergon <agk@redhat.com>
    Cc: Daniel Vetter <daniel@ffwll.ch>
    Cc: intel-gfx@lists.freedesktop.org
    Cc: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Cc: Maarten Lankhorst <maarten.lankhorst@linux.intel.com>
    Cc: dri-devel@lists.freedesktop.org
    Cc: David Airlie <airlied@linux.ie>
    Cc: Jani Nikula <jani.nikula@linux.intel.com>
    Cc: Rodrigo Vivi <rodrigo.vivi@intel.com>
    Cc: Tom Zanussi <tom.zanussi@linux.intel.com>
    Cc: Miroslav Benes <mbenes@suse.cz>
    Cc: linux-arch@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190425094801.230654524@linutronix.de

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 730876187344..20899919ead8 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -241,21 +241,11 @@ static inline void ftrace_free_mem(struct module *mod, void *start, void *end) {
 
 #ifdef CONFIG_STACK_TRACER
 
-#define STACK_TRACE_ENTRIES 500
-
-struct stack_trace;
-
-extern unsigned stack_trace_index[];
-extern struct stack_trace stack_trace_max;
-extern unsigned long stack_trace_max_size;
-extern arch_spinlock_t stack_trace_max_lock;
-
 extern int stack_tracer_enabled;
-void stack_trace_print(void);
-int
-stack_trace_sysctl(struct ctl_table *table, int write,
-		   void __user *buffer, size_t *lenp,
-		   loff_t *ppos);
+
+int stack_trace_sysctl(struct ctl_table *table, int write,
+		       void __user *buffer, size_t *lenp,
+		       loff_t *ppos);
 
 /* DO NOT MODIFY THIS VARIABLE DIRECTLY! */
 DECLARE_PER_CPU(int, disable_stack_tracer);

commit 495d714ad140e1732e66c45d0409054b24c1a0d6
Merge: f12e840c819b 3d739c1f6156
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Dec 31 11:46:59 2018 -0800

    Merge tag 'trace-v4.21' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull tracing updates from Steven Rostedt:
    
     - Rework of the kprobe/uprobe and synthetic events to consolidate all
       the dynamic event code. This will make changes in the future easier.
    
     - Partial rewrite of the function graph tracing infrastructure. This
       will allow for multiple users of hooking onto functions to get the
       callback (return) of the function. This is the ground work for having
       kprobes and function graph tracer using one code base.
    
     - Clean up of the histogram code that will facilitate adding more
       features to the histograms in the future.
    
     - Addition of str_has_prefix() and a few use cases. There currently is
       a similar function strstart() that is used in a few places, but only
       returns a bool and not a length. These instances will be removed in
       the future to use str_has_prefix() instead.
    
     - A few other various clean ups as well.
    
    * tag 'trace-v4.21' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace: (57 commits)
      tracing: Use the return of str_has_prefix() to remove open coded numbers
      tracing: Have the historgram use the result of str_has_prefix() for len of prefix
      tracing: Use str_has_prefix() instead of using fixed sizes
      tracing: Use str_has_prefix() helper for histogram code
      string.h: Add str_has_prefix() helper function
      tracing: Make function ‘ftrace_exports’ static
      tracing: Simplify printf'ing in seq_print_sym
      tracing: Avoid -Wformat-nonliteral warning
      tracing: Merge seq_print_sym_short() and seq_print_sym_offset()
      tracing: Add hist trigger comments for variable-related fields
      tracing: Remove hist trigger synth_var_refs
      tracing: Use hist trigger's var_ref array to destroy var_refs
      tracing: Remove open-coding of hist trigger var_ref management
      tracing: Use var_refs[] for hist trigger reference checking
      tracing: Change strlen to sizeof for hist trigger static strings
      tracing: Remove unnecessary hist trigger struct field
      tracing: Fix ftrace_graph_get_ret_stack() to use task and not current
      seq_buf: Use size_t for len in seq_buf_puts()
      seq_buf: Make seq_buf_puts() null-terminate the buffer
      arm64: Use ftrace_graph_get_ret_stack() instead of curr_ret_stack
      ...

commit a0572f687fb3c46e15554f4789797a077cc393b4
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Wed Dec 5 12:48:53 2018 -0500

    ftrace: Allow ftrace_replace_code() to be schedulable
    
    The function ftrace_replace_code() is the ftrace engine that does the
    work to modify all the nops into the calls to the function callback in
    all the functions being traced.
    
    The generic version which is normally called from stop machine, but an
    architecture can implement a non stop machine version and still use the
    generic ftrace_replace_code(). When an architecture does this,
    ftrace_replace_code() may be called from a schedulable context, where
    it can allow the code to be preemptible, and schedule out.
    
    In order to allow an architecture to make ftrace_replace_code()
    schedulable, a new command flag is added called:
    
     FTRACE_MAY_SLEEP
    
    Which can be or'd to the command that is passed to
    ftrace_modify_all_code() that calls ftrace_replace_code() and will have
    it call cond_resched() in the loop that modifies the nops into the
    calls to the ftrace trampolines.
    
    Link: http://lkml.kernel.org/r/20181204192903.8193-1-anders.roxell@linaro.org
    Link: http://lkml.kernel.org/r/20181205183303.828422192@goodmis.org
    
    Reported-by: Anders Roxell <anders.roxell@linaro.org>
    Tested-by: Anders Roxell <anders.roxell@linaro.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 98e141c71ad0..13485a19e964 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -389,6 +389,7 @@ enum {
 	FTRACE_UPDATE_TRACE_FUNC	= (1 << 2),
 	FTRACE_START_FUNC_RET		= (1 << 3),
 	FTRACE_STOP_FUNC_RET		= (1 << 4),
+	FTRACE_MAY_SLEEP		= (1 << 5),
 };
 
 /*

commit b0e21a61d3196762b61f43ae994ffd255f646774
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Mon Nov 19 20:54:08 2018 -0500

    function_graph: Have profiler use new helper ftrace_graph_get_ret_stack()
    
    The ret_stack processing is going to change, and that is going
    to break anything that is accessing the ret_stack directly. One user is the
    function graph profiler. By using the ftrace_graph_get_ret_stack() helper
    function, the profiler can access the ret_stack entry without relying on the
    implementation details of the stack itself.
    
    Reviewed-by: Joel Fernandes (Google) <joel@joelfernandes.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 21c80491ccde..98e141c71ad0 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -785,6 +785,9 @@ extern int
 function_graph_enter(unsigned long ret, unsigned long func,
 		     unsigned long frame_pointer, unsigned long *retp);
 
+struct ftrace_ret_stack *
+ftrace_graph_get_ret_stack(struct task_struct *task, int idx);
+
 unsigned long ftrace_graph_ret_addr(struct task_struct *task, int *idx,
 				    unsigned long ret, unsigned long *retp);
 

commit 688f7089d8851b1a81106f0c0b9b29181b2f2dc8
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu Nov 15 14:06:47 2018 -0500

    fgraph: Add new fgraph_ops structure to enable function graph hooks
    
    Currently the registering of function graph is to pass in a entry and return
    function. We need to have a way to associate those functions together where
    the entry can determine to run the return hook. Having a structure that
    contains both functions will facilitate the process of converting the code
    to be able to do such.
    
    This is similar to the way function hooks are enabled (it passes in
    ftrace_ops). Instead of passing in the functions to use, a single structure
    is passed in to the registering function.
    
    The unregister function is now passed in the fgraph_ops handle. When we
    allow more than one callback to the function graph hooks, this will let the
    system know which one to remove.
    
    Reviewed-by: Joel Fernandes (Google) <joel@joelfernandes.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 98625f10d982..21c80491ccde 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -749,6 +749,11 @@ typedef int (*trace_func_graph_ent_t)(struct ftrace_graph_ent *); /* entry */
 
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
 
+struct fgraph_ops {
+	trace_func_graph_ent_t		entryfunc;
+	trace_func_graph_ret_t		retfunc;
+};
+
 /*
  * Stack of return addresses for functions
  * of a thread.
@@ -792,8 +797,9 @@ unsigned long ftrace_graph_ret_addr(struct task_struct *task, int *idx,
 
 #define FTRACE_RETFUNC_DEPTH 50
 #define FTRACE_RETSTACK_ALLOC_SIZE 32
-extern int register_ftrace_graph(trace_func_graph_ret_t retfunc,
-				trace_func_graph_ent_t entryfunc);
+
+extern int register_ftrace_graph(struct fgraph_ops *ops);
+extern void unregister_ftrace_graph(struct fgraph_ops *ops);
 
 extern bool ftrace_graph_is_dead(void);
 extern void ftrace_graph_stop(void);
@@ -802,8 +808,6 @@ extern void ftrace_graph_stop(void);
 extern trace_func_graph_ret_t ftrace_graph_return;
 extern trace_func_graph_ent_t ftrace_graph_entry;
 
-extern void unregister_ftrace_graph(void);
-
 extern void ftrace_graph_init_task(struct task_struct *t);
 extern void ftrace_graph_exit_task(struct task_struct *t);
 extern void ftrace_graph_init_idle_task(struct task_struct *t, int cpu);
@@ -825,12 +829,9 @@ static inline void ftrace_graph_init_task(struct task_struct *t) { }
 static inline void ftrace_graph_exit_task(struct task_struct *t) { }
 static inline void ftrace_graph_init_idle_task(struct task_struct *t, int cpu) { }
 
-static inline int register_ftrace_graph(trace_func_graph_ret_t retfunc,
-			  trace_func_graph_ent_t entryfunc)
-{
-	return -1;
-}
-static inline void unregister_ftrace_graph(void) { }
+/* Define as macros as fgraph_ops may not be defined */
+#define register_ftrace_graph(ops) ({ -1; })
+#define unregister_ftrace_graph(ops) do { } while (0)
 
 static inline unsigned long
 ftrace_graph_ret_addr(struct task_struct *task, int *idx, unsigned long ret,

commit 761efe8a94cfcd0a3dd90f2008411550f3520b63
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Sun Nov 18 18:44:04 2018 -0500

    function_graph: Remove the use of FTRACE_NOTRACE_DEPTH
    
    The curr_ret_stack is no longer set to a negative value when a function is
    not to be traced by the function graph tracer. Remove the usage of
    FTRACE_NOTRACE_DEPTH, as it is no longer needed.
    
    Reviewed-by: Joel Fernandes (Google) <joel@joelfernandes.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 10bd46434908..98625f10d982 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -790,7 +790,6 @@ unsigned long ftrace_graph_ret_addr(struct task_struct *task, int *idx,
  */
 #define __notrace_funcgraph		notrace
 
-#define FTRACE_NOTRACE_DEPTH 65536
 #define FTRACE_RETFUNC_DEPTH 50
 #define FTRACE_RETSTACK_ALLOC_SIZE 32
 extern int register_ftrace_graph(trace_func_graph_ret_t retfunc,

commit df60673198ae678f68af54873b8904ba93fe13a0
Merge: 89f579ce99f7 2595646791c3
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Dec 3 10:47:53 2018 +0100

    Merge tag 'v4.20-rc5' into x86/cleanups, to sync up the tree
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 23621fac32ec9dbc4afada344cbf82b0f6281be3
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Sun Nov 18 18:32:40 2018 -0500

    function_graph: Remove unused task_curr_ret_stack()
    
    The static inline function task_curr_ret_stack() is unused, remove it.
    
    Reviewed-by: Joel Fernandes (Google) <joel@joelfernandes.org>
    Reviewed-by: Masami Hiramatsu <mhiramat@kernel.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index dd16e8218db3..10bd46434908 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -809,11 +809,6 @@ extern void ftrace_graph_init_task(struct task_struct *t);
 extern void ftrace_graph_exit_task(struct task_struct *t);
 extern void ftrace_graph_init_idle_task(struct task_struct *t, int cpu);
 
-static inline int task_curr_ret_stack(struct task_struct *t)
-{
-	return t->curr_ret_stack;
-}
-
 static inline void pause_graph_tracing(void)
 {
 	atomic_inc(&current->tracing_graph_pause);
@@ -838,11 +833,6 @@ static inline int register_ftrace_graph(trace_func_graph_ret_t retfunc,
 }
 static inline void unregister_ftrace_graph(void) { }
 
-static inline int task_curr_ret_stack(struct task_struct *tsk)
-{
-	return -1;
-}
-
 static inline unsigned long
 ftrace_graph_ret_addr(struct task_struct *task, int *idx, unsigned long ret,
 		      unsigned long *retp)

commit d125f3f866df88da5a85df00291f88f0baa89f7c
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Mon Nov 19 07:40:39 2018 -0500

    function_graph: Make ftrace_push_return_trace() static
    
    As all architectures now call function_graph_enter() to do the entry work,
    no architecture should ever call ftrace_push_return_trace(). Make it static.
    
    This is needed to prepare for a fix of a design bug on how the curr_ret_stack
    is used.
    
    Cc: stable@kernel.org
    Fixes: 03274a3ffb449 ("tracing/fgraph: Adjust fgraph depth before calling trace return callback")
    Reviewed-by: Masami Hiramatsu <mhiramat@kernel.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 5717e8f81c59..dd16e8218db3 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -776,9 +776,6 @@ struct ftrace_ret_stack {
  */
 extern void return_to_handler(void);
 
-extern int
-ftrace_push_return_trace(unsigned long ret, unsigned long func, int *depth,
-			 unsigned long frame_pointer, unsigned long *retp);
 extern int
 function_graph_enter(unsigned long ret, unsigned long func,
 		     unsigned long frame_pointer, unsigned long *retp);

commit 8114865ff82e200b383e46821c25cb0625b842b5
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Sun Nov 18 17:10:15 2018 -0500

    function_graph: Create function_graph_enter() to consolidate architecture code
    
    Currently all the architectures do basically the same thing in preparing the
    function graph tracer on entry to a function. This code can be pulled into a
    generic location and then this will allow the function graph tracer to be
    fixed, as well as extended.
    
    Create a new function graph helper function_graph_enter() that will call the
    hook function (ftrace_graph_entry) and the shadow stack operation
    (ftrace_push_return_trace), and remove the need of the architecture code to
    manage the shadow stack.
    
    This is needed to prepare for a fix of a design bug on how the curr_ret_stack
    is used.
    
    Cc: stable@kernel.org
    Fixes: 03274a3ffb449 ("tracing/fgraph: Adjust fgraph depth before calling trace return callback")
    Reviewed-by: Masami Hiramatsu <mhiramat@kernel.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index a397907e8d72..5717e8f81c59 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -779,6 +779,9 @@ extern void return_to_handler(void);
 extern int
 ftrace_push_return_trace(unsigned long ret, unsigned long func, int *depth,
 			 unsigned long frame_pointer, unsigned long *retp);
+extern int
+function_graph_enter(unsigned long ret, unsigned long func,
+		     unsigned long frame_pointer, unsigned long *retp);
 
 unsigned long ftrace_graph_ret_addr(struct task_struct *task, int *idx,
 				    unsigned long ret, unsigned long *retp);

commit 89f579ce99f7e028e81885d3965f973c0f787611
Author: Yi Wang <wang.yi59@zte.com.cn>
Date:   Thu Nov 22 10:04:09 2018 +0800

    x86/headers: Fix -Wmissing-prototypes warning
    
    When building the kernel with W=1 we get a lot of -Wmissing-prototypes
    warnings, which are trivial in nature and easy to fix - and which may
    mask some real future bugs if the prototypes get out of sync with
    the function definition.
    
    This patch fixes most of -Wmissing-prototypes warnings which
    are in the root directory of arch/x86/kernel, not including
    the subdirectories.
    
    These are the warnings fixed in this patch:
    
      arch/x86/kernel/signal.c:865:17: warning: no previous prototype for ‘sys32_x32_rt_sigreturn’ [-Wmissing-prototypes]
      arch/x86/kernel/signal_compat.c:164:6: warning: no previous prototype for ‘sigaction_compat_abi’ [-Wmissing-prototypes]
      arch/x86/kernel/traps.c:625:46: warning: no previous prototype for ‘sync_regs’ [-Wmissing-prototypes]
      arch/x86/kernel/traps.c:640:24: warning: no previous prototype for ‘fixup_bad_iret’ [-Wmissing-prototypes]
      arch/x86/kernel/traps.c:929:13: warning: no previous prototype for ‘trap_init’ [-Wmissing-prototypes]
      arch/x86/kernel/irq.c:270:28: warning: no previous prototype for ‘smp_x86_platform_ipi’ [-Wmissing-prototypes]
      arch/x86/kernel/irq.c:301:16: warning: no previous prototype for ‘smp_kvm_posted_intr_ipi’ [-Wmissing-prototypes]
      arch/x86/kernel/irq.c:314:16: warning: no previous prototype for ‘smp_kvm_posted_intr_wakeup_ipi’ [-Wmissing-prototypes]
      arch/x86/kernel/irq.c:328:16: warning: no previous prototype for ‘smp_kvm_posted_intr_nested_ipi’ [-Wmissing-prototypes]
      arch/x86/kernel/irq_work.c:16:28: warning: no previous prototype for ‘smp_irq_work_interrupt’ [-Wmissing-prototypes]
      arch/x86/kernel/irqinit.c:79:13: warning: no previous prototype for ‘init_IRQ’ [-Wmissing-prototypes]
      arch/x86/kernel/quirks.c:672:13: warning: no previous prototype for ‘early_platform_quirks’ [-Wmissing-prototypes]
      arch/x86/kernel/tsc.c:1499:15: warning: no previous prototype for ‘calibrate_delay_is_known’ [-Wmissing-prototypes]
      arch/x86/kernel/process.c:653:13: warning: no previous prototype for ‘arch_post_acpi_subsys_init’ [-Wmissing-prototypes]
      arch/x86/kernel/process.c:717:15: warning: no previous prototype for ‘arch_randomize_brk’ [-Wmissing-prototypes]
      arch/x86/kernel/process.c:784:6: warning: no previous prototype for ‘do_arch_prctl_common’ [-Wmissing-prototypes]
      arch/x86/kernel/reboot.c:869:6: warning: no previous prototype for ‘nmi_panic_self_stop’ [-Wmissing-prototypes]
      arch/x86/kernel/smp.c:176:27: warning: no previous prototype for ‘smp_reboot_interrupt’ [-Wmissing-prototypes]
      arch/x86/kernel/smp.c:260:28: warning: no previous prototype for ‘smp_reschedule_interrupt’ [-Wmissing-prototypes]
      arch/x86/kernel/smp.c:281:28: warning: no previous prototype for ‘smp_call_function_interrupt’ [-Wmissing-prototypes]
      arch/x86/kernel/smp.c:291:28: warning: no previous prototype for ‘smp_call_function_single_interrupt’ [-Wmissing-prototypes]
      arch/x86/kernel/ftrace.c:840:6: warning: no previous prototype for ‘arch_ftrace_update_trampoline’ [-Wmissing-prototypes]
      arch/x86/kernel/ftrace.c:934:7: warning: no previous prototype for ‘arch_ftrace_trampoline_func’ [-Wmissing-prototypes]
      arch/x86/kernel/ftrace.c:946:6: warning: no previous prototype for ‘arch_ftrace_trampoline_free’ [-Wmissing-prototypes]
      arch/x86/kernel/crash.c:114:6: warning: no previous prototype for ‘crash_smp_send_stop’ [-Wmissing-prototypes]
      arch/x86/kernel/crash.c:351:5: warning: no previous prototype for ‘crash_setup_memmap_entries’ [-Wmissing-prototypes]
      arch/x86/kernel/crash.c:424:5: warning: no previous prototype for ‘crash_load_segments’ [-Wmissing-prototypes]
      arch/x86/kernel/machine_kexec_64.c:372:7: warning: no previous prototype for ‘arch_kexec_kernel_image_load’ [-Wmissing-prototypes]
      arch/x86/kernel/paravirt-spinlocks.c:12:16: warning: no previous prototype for ‘__native_queued_spin_unlock’ [-Wmissing-prototypes]
      arch/x86/kernel/paravirt-spinlocks.c:18:6: warning: no previous prototype for ‘pv_is_native_spin_unlock’ [-Wmissing-prototypes]
      arch/x86/kernel/paravirt-spinlocks.c:24:16: warning: no previous prototype for ‘__native_vcpu_is_preempted’ [-Wmissing-prototypes]
      arch/x86/kernel/paravirt-spinlocks.c:30:6: warning: no previous prototype for ‘pv_is_native_vcpu_is_preempted’ [-Wmissing-prototypes]
      arch/x86/kernel/kvm.c:258:1: warning: no previous prototype for ‘do_async_page_fault’ [-Wmissing-prototypes]
      arch/x86/kernel/jailhouse.c:200:6: warning: no previous prototype for ‘jailhouse_paravirt’ [-Wmissing-prototypes]
      arch/x86/kernel/check.c:91:13: warning: no previous prototype for ‘setup_bios_corruption_check’ [-Wmissing-prototypes]
      arch/x86/kernel/check.c:139:6: warning: no previous prototype for ‘check_for_bios_corruption’ [-Wmissing-prototypes]
      arch/x86/kernel/devicetree.c:32:13: warning: no previous prototype for ‘early_init_dt_scan_chosen_arch’ [-Wmissing-prototypes]
      arch/x86/kernel/devicetree.c:42:13: warning: no previous prototype for ‘add_dtb’ [-Wmissing-prototypes]
      arch/x86/kernel/devicetree.c:108:6: warning: no previous prototype for ‘x86_of_pci_init’ [-Wmissing-prototypes]
      arch/x86/kernel/devicetree.c:314:13: warning: no previous prototype for ‘x86_dtb_init’ [-Wmissing-prototypes]
      arch/x86/kernel/tracepoint.c:16:5: warning: no previous prototype for ‘trace_pagefault_reg’ [-Wmissing-prototypes]
      arch/x86/kernel/tracepoint.c:22:6: warning: no previous prototype for ‘trace_pagefault_unreg’ [-Wmissing-prototypes]
      arch/x86/kernel/head64.c:113:22: warning: no previous prototype for ‘__startup_64’ [-Wmissing-prototypes]
      arch/x86/kernel/head64.c:262:15: warning: no previous prototype for ‘__startup_secondary_64’ [-Wmissing-prototypes]
      arch/x86/kernel/head64.c:350:12: warning: no previous prototype for ‘early_make_pgtable’ [-Wmissing-prototypes]
    
    [ mingo: rewrote the changelog, fixed build errors. ]
    
    Signed-off-by: Yi Wang <wang.yi59@zte.com.cn>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: akataria@vmware.com
    Cc: akpm@linux-foundation.org
    Cc: andy.shevchenko@gmail.com
    Cc: anton@enomsg.org
    Cc: ard.biesheuvel@linaro.org
    Cc: bhe@redhat.com
    Cc: bhelgaas@google.com
    Cc: bp@alien8.de
    Cc: ccross@android.com
    Cc: devicetree@vger.kernel.org
    Cc: douly.fnst@cn.fujitsu.com
    Cc: dwmw@amazon.co.uk
    Cc: dyoung@redhat.com
    Cc: ebiederm@xmission.com
    Cc: frank.rowand@sony.com
    Cc: frowand.list@gmail.com
    Cc: ivan.gorinov@intel.com
    Cc: jailhouse-dev@googlegroups.com
    Cc: jan.kiszka@siemens.com
    Cc: jgross@suse.com
    Cc: jroedel@suse.de
    Cc: keescook@chromium.org
    Cc: kexec@lists.infradead.org
    Cc: konrad.wilk@oracle.com
    Cc: kvm@vger.kernel.org
    Cc: linux-efi@vger.kernel.org
    Cc: linux-pci@vger.kernel.org
    Cc: luto@kernel.org
    Cc: m.mizuma@jp.fujitsu.com
    Cc: namit@vmware.com
    Cc: oleg@redhat.com
    Cc: pasha.tatashin@oracle.com
    Cc: pbonzini@redhat.com
    Cc: prarit@redhat.com
    Cc: pravin.shedge4linux@gmail.com
    Cc: rajvi.jingar@intel.com
    Cc: rkrcmar@redhat.com
    Cc: robh+dt@kernel.org
    Cc: robh@kernel.org
    Cc: rostedt@goodmis.org
    Cc: takahiro.akashi@linaro.org
    Cc: thomas.lendacky@amd.com
    Cc: tony.luck@intel.com
    Cc: up2wing@gmail.com
    Cc: virtualization@lists.linux-foundation.org
    Cc: zhe.he@windriver.com
    Cc: zhong.weidong@zte.com.cn
    Link: http://lkml.kernel.org/r/1542852249-19820-1-git-send-email-wang.yi59@zte.com.cn
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index a397907e8d72..182d669cc918 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -420,6 +420,9 @@ enum {
 };
 
 void arch_ftrace_update_code(int command);
+void arch_ftrace_update_trampoline(struct ftrace_ops *ops);
+void *arch_ftrace_trampoline_func(struct ftrace_ops *ops, struct dyn_ftrace *rec);
+void arch_ftrace_trampoline_free(struct ftrace_ops *ops);
 
 struct ftrace_rec_iter;
 

commit c3bc8fd637a9623f5c507bd18f9677effbddf584
Author: Joel Fernandes (Google) <joel@joelfernandes.org>
Date:   Mon Jul 30 15:24:23 2018 -0700

    tracing: Centralize preemptirq tracepoints and unify their usage
    
    This patch detaches the preemptirq tracepoints from the tracers and
    keeps it separate.
    
    Advantages:
    * Lockdep and irqsoff event can now run in parallel since they no longer
    have their own calls.
    
    * This unifies the usecase of adding hooks to an irqsoff and irqson
    event, and a preemptoff and preempton event.
      3 users of the events exist:
      - Lockdep
      - irqsoff and preemptoff tracers
      - irqs and preempt trace events
    
    The unification cleans up several ifdefs and makes the code in preempt
    tracer and irqsoff tracers simpler. It gets rid of all the horrific
    ifdeferry around PROVE_LOCKING and makes configuration of the different
    users of the tracepoints more easy and understandable. It also gets rid
    of the time_* function calls from the lockdep hooks used to call into
    the preemptirq tracer which is not needed anymore. The negative delta in
    lines of code in this patch is quite large too.
    
    In the patch we introduce a new CONFIG option PREEMPTIRQ_TRACEPOINTS
    as a single point for registering probes onto the tracepoints. With
    this,
    the web of config options for preempt/irq toggle tracepoints and its
    users becomes:
    
     PREEMPT_TRACER   PREEMPTIRQ_EVENTS  IRQSOFF_TRACER PROVE_LOCKING
           |                 |     \         |           |
           \    (selects)    /      \        \ (selects) /
          TRACE_PREEMPT_TOGGLE       ----> TRACE_IRQFLAGS
                          \                  /
                           \ (depends on)   /
                         PREEMPTIRQ_TRACEPOINTS
    
    Other than the performance tests mentioned in the previous patch, I also
    ran the locking API test suite. I verified that all tests cases are
    passing.
    
    I also injected issues by not registering lockdep probes onto the
    tracepoints and I see failures to confirm that the probes are indeed
    working.
    
    This series + lockdep probes not registered (just to inject errors):
    [    0.000000]      hard-irqs-on + irq-safe-A/21:  ok  |  ok  |  ok  |
    [    0.000000]      soft-irqs-on + irq-safe-A/21:  ok  |  ok  |  ok  |
    [    0.000000]        sirq-safe-A => hirqs-on/12:FAILED|FAILED|  ok  |
    [    0.000000]        sirq-safe-A => hirqs-on/21:FAILED|FAILED|  ok  |
    [    0.000000]          hard-safe-A + irqs-on/12:FAILED|FAILED|  ok  |
    [    0.000000]          soft-safe-A + irqs-on/12:FAILED|FAILED|  ok  |
    [    0.000000]          hard-safe-A + irqs-on/21:FAILED|FAILED|  ok  |
    [    0.000000]          soft-safe-A + irqs-on/21:FAILED|FAILED|  ok  |
    [    0.000000]     hard-safe-A + unsafe-B #1/123:  ok  |  ok  |  ok  |
    [    0.000000]     soft-safe-A + unsafe-B #1/123:  ok  |  ok  |  ok  |
    
    With this series + lockdep probes registered, all locking tests pass:
    
    [    0.000000]      hard-irqs-on + irq-safe-A/21:  ok  |  ok  |  ok  |
    [    0.000000]      soft-irqs-on + irq-safe-A/21:  ok  |  ok  |  ok  |
    [    0.000000]        sirq-safe-A => hirqs-on/12:  ok  |  ok  |  ok  |
    [    0.000000]        sirq-safe-A => hirqs-on/21:  ok  |  ok  |  ok  |
    [    0.000000]          hard-safe-A + irqs-on/12:  ok  |  ok  |  ok  |
    [    0.000000]          soft-safe-A + irqs-on/12:  ok  |  ok  |  ok  |
    [    0.000000]          hard-safe-A + irqs-on/21:  ok  |  ok  |  ok  |
    [    0.000000]          soft-safe-A + irqs-on/21:  ok  |  ok  |  ok  |
    [    0.000000]     hard-safe-A + unsafe-B #1/123:  ok  |  ok  |  ok  |
    [    0.000000]     soft-safe-A + unsafe-B #1/123:  ok  |  ok  |  ok  |
    
    Link: http://lkml.kernel.org/r/20180730222423.196630-4-joel@joelfernandes.org
    
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Joel Fernandes (Google) <joel@joelfernandes.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 63af5eb0ff46..a397907e8d72 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -701,16 +701,7 @@ static inline unsigned long get_lock_parent_ip(void)
 	return CALLER_ADDR2;
 }
 
-#ifdef CONFIG_IRQSOFF_TRACER
-  extern void time_hardirqs_on(unsigned long a0, unsigned long a1);
-  extern void time_hardirqs_off(unsigned long a0, unsigned long a1);
-#else
-  static inline void time_hardirqs_on(unsigned long a0, unsigned long a1) { }
-  static inline void time_hardirqs_off(unsigned long a0, unsigned long a1) { }
-#endif
-
-#if defined(CONFIG_PREEMPT_TRACER) || \
-	(defined(CONFIG_DEBUG_PREEMPT) && defined(CONFIG_PREEMPTIRQ_EVENTS))
+#ifdef CONFIG_TRACE_PREEMPT_TOGGLE
   extern void trace_preempt_on(unsigned long a0, unsigned long a1);
   extern void trace_preempt_off(unsigned long a0, unsigned long a1);
 #else

commit 72809cbf6748830ae4a59a45bcb2367a6c24d74d
Author: Masami Hiramatsu <mhiramat@kernel.org>
Date:   Thu Jul 26 21:44:32 2018 +0900

    tracing: Remove orphaned function ftrace_nr_registered_ops()
    
    Remove ftrace_nr_registered_ops() because it is no longer used.
    
    ftrace_nr_registered_ops() has been introduced by commit ea701f11da44
    ("ftrace: Add selftest to test function trace recursion protection"), but
    its caller has been removed by commit 05cbbf643b8e ("tracing: Fix selftest
    function recursion accounting"). So it is not called anymore.
    
    Link: http://lkml.kernel.org/r/153260907227.12474.5234899025934963683.stgit@devbox
    
    Signed-off-by: Masami Hiramatsu <mhiramat@kernel.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index ebb77674be90..63af5eb0ff46 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -234,10 +234,6 @@ extern void ftrace_stub(unsigned long a0, unsigned long a1,
  */
 #define register_ftrace_function(ops) ({ 0; })
 #define unregister_ftrace_function(ops) ({ 0; })
-static inline int ftrace_nr_registered_ops(void)
-{
-	return 0;
-}
 static inline void ftrace_kill(void) { }
 static inline void ftrace_free_init_mem(void) { }
 static inline void ftrace_free_mem(struct module *mod, void *start, void *end) { }
@@ -328,8 +324,6 @@ struct seq_file;
 
 extern int ftrace_text_reserved(const void *start, const void *end);
 
-extern int ftrace_nr_registered_ops(void);
-
 struct ftrace_ops *ftrace_ops_trampoline(unsigned long addr);
 
 bool is_ftrace_trampoline(unsigned long addr);

commit 5ccba64a560fa6ca06008d4001f5d46ebeb34b41
Author: Yisheng Xie <xieyisheng1@huawei.com>
Date:   Fri Feb 2 10:14:49 2018 +0800

    ftrace: Nuke clear_ftrace_function
    
    clear_ftrace_function is not used outside of ftrace.c and is not help to
    use a function, so nuke it per Steve's suggestion.
    
    Link: http://lkml.kernel.org/r/1517537689-34947-1-git-send-email-xieyisheng1@huawei.com
    
    Suggested-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Yisheng Xie <xieyisheng1@huawei.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 8154f4920fcb..ebb77674be90 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -223,7 +223,6 @@ extern enum ftrace_tracing_type_t ftrace_tracing_type;
  */
 int register_ftrace_function(struct ftrace_ops *ops);
 int unregister_ftrace_function(struct ftrace_ops *ops);
-void clear_ftrace_function(void);
 
 extern void ftrace_stub(unsigned long a0, unsigned long a1,
 			struct ftrace_ops *op, struct pt_regs *regs);
@@ -239,7 +238,6 @@ static inline int ftrace_nr_registered_ops(void)
 {
 	return 0;
 }
-static inline void clear_ftrace_function(void) { }
 static inline void ftrace_kill(void) { }
 static inline void ftrace_free_init_mem(void) { }
 static inline void ftrace_free_mem(struct module *mod, void *start, void *end) { }

commit 5fb94e9ca333f0fe1d96de06704a79942b3832c3
Author: Mauro Carvalho Chehab <mchehab+samsung@kernel.org>
Date:   Tue May 8 15:14:57 2018 -0300

    docs: Fix some broken references
    
    As we move stuff around, some doc references are broken. Fix some of
    them via this script:
            ./scripts/documentation-file-ref-check --fix
    
    Manually checked if the produced result is valid, removing a few
    false-positives.
    
    Acked-by: Takashi Iwai <tiwai@suse.de>
    Acked-by: Masami Hiramatsu <mhiramat@kernel.org>
    Acked-by: Stephen Boyd <sboyd@kernel.org>
    Acked-by: Charles Keepax <ckeepax@opensource.wolfsonmicro.com>
    Acked-by: Mathieu Poirier <mathieu.poirier@linaro.org>
    Reviewed-by: Coly Li <colyli@suse.de>
    Signed-off-by: Mauro Carvalho Chehab <mchehab+samsung@kernel.org>
    Acked-by: Jonathan Corbet <corbet@lwn.net>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 9c3c9a319e48..8154f4920fcb 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 */
 /*
  * Ftrace header.  For implementation details beyond the random comments
- * scattered below, see: Documentation/trace/ftrace-design.txt
+ * scattered below, see: Documentation/trace/ftrace-design.rst
  */
 
 #ifndef _LINUX_FTRACE_H

commit 49f9c3552ccc30f4f98c45d94d7f9b335596913f
Merge: d8a5b80568a9 e1e871aff3de
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jan 29 09:08:34 2018 -0800

    Merge tag 'init_task-20180117' of git://git.kernel.org/pub/scm/linux/kernel/git/dhowells/linux-fs
    
    Pull init_task initializer cleanups from David Howells:
     "It doesn't seem useful to have the init_task in a header file rather
      than in a normal source file. We could consolidate init_task handling
      instead and expand out various macros.
    
      Here's a series of patches that consolidate init_task handling:
    
       (1) Make THREAD_SIZE available to vmlinux.lds for cris, hexagon and
           openrisc.
    
       (2) Alter the INIT_TASK_DATA linker script macro to set
           init_thread_union and init_stack rather than defining these in C.
    
           Insert init_task and init_thread_into into the init_stack area in
           the linker script as appropriate to the configuration, with
           different section markers so that they end up correctly ordered.
    
           We can then get merge ia64's init_task.c into the main one.
    
           We then have a bunch of single-use INIT_*() macros that seem only
           to be macros because they used to be used per-arch. We can then
           expand these in place of the user and get rid of a few lines and
           a lot of backslashes.
    
       (3) Expand INIT_TASK() in place.
    
       (4) Expand in place various small INIT_*() macros that are defined
           conditionally. Expand them and surround them by #if[n]def/#endif
           in the .c file as it takes fewer lines.
    
       (5) Expand INIT_SIGNALS() and INIT_SIGHAND() in place.
    
       (6) Expand INIT_STRUCT_PID in place.
    
      These macros can then be discarded"
    
    * tag 'init_task-20180117' of git://git.kernel.org/pub/scm/linux/kernel/git/dhowells/linux-fs:
      Expand INIT_STRUCT_PID and remove
      Expand the INIT_SIGNALS and INIT_SIGHAND macros and remove
      Expand various INIT_* macros and remove
      Expand INIT_TASK() in init/init_task.c and remove
      Construct init thread stack in the linker script rather than by union
      openrisc: Make THREAD_SIZE available to vmlinux.lds
      hexagon: Make THREAD_SIZE available to vmlinux.lds
      cris: Make THREAD_SIZE available to vmlinux.lds

commit 6be7fa3c74d1e0cd50f2157b5c1524f152bf641e
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Mon Jan 22 22:32:51 2018 -0500

    ftrace, orc, x86: Handle ftrace dynamically allocated trampolines
    
    The function tracer can create a dynamically allocated trampoline that is
    called by the function mcount or fentry hook that is used to call the
    function callback that is registered. The problem is that the orc undwinder
    will bail if it encounters one of these trampolines. This breaks the stack
    trace of function callbacks, which include the stack tracer and setting the
    stack trace for individual functions.
    
    Since these dynamic trampolines are basically copies of the static ftrace
    trampolines defined in ftrace_*.S, we do not need to create new orc entries
    for the dynamic trampolines. Finding the return address on the stack will be
    identical as the functions that were copied to create the dynamic
    trampolines. When encountering a ftrace dynamic trampoline, we can just use
    the orc entry of the ftrace static function that was copied for that
    trampoline.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 2bab81951ced..3319df9727aa 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -332,6 +332,8 @@ extern int ftrace_text_reserved(const void *start, const void *end);
 
 extern int ftrace_nr_registered_ops(void);
 
+struct ftrace_ops *ftrace_ops_trampoline(unsigned long addr);
+
 bool is_ftrace_trampoline(unsigned long addr);
 
 /*

commit 4e7e3adbba5224604b34b0d42003ff6dbdc8ddd9
Author: David Howells <dhowells@redhat.com>
Date:   Tue Jan 2 15:12:01 2018 +0000

    Expand various INIT_* macros and remove
    
    Expand various INIT_* macros into the single places they're used in
    init/init_task.c and remove them.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Tested-by: Tony Luck <tony.luck@intel.com>
    Tested-by: Will Deacon <will.deacon@arm.com> (arm64)
    Tested-by: Palmer Dabbelt <palmer@sifive.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 2bab81951ced..6311f35acbc4 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -764,9 +764,6 @@ typedef int (*trace_func_graph_ent_t)(struct ftrace_graph_ent *); /* entry */
 
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
 
-/* for init task */
-#define INIT_FTRACE_GRAPH		.ret_stack = NULL,
-
 /*
  * Stack of return addresses for functions
  * of a thread.
@@ -844,7 +841,6 @@ static inline void unpause_graph_tracing(void)
 #else /* !CONFIG_FUNCTION_GRAPH_TRACER */
 
 #define __notrace_funcgraph
-#define INIT_FTRACE_GRAPH
 
 static inline void ftrace_graph_init_task(struct task_struct *t) { }
 static inline void ftrace_graph_exit_task(struct task_struct *t) { }
@@ -923,10 +919,6 @@ extern int tracepoint_printk;
 extern void disable_trace_on_warning(void);
 extern int __disable_trace_on_warning;
 
-#ifdef CONFIG_PREEMPT
-#define INIT_TRACE_RECURSION		.trace_recursion = 0,
-#endif
-
 int tracepoint_printk_sysctl(struct ctl_table *table, int write,
 			     void __user *buffer, size_t *lenp,
 			     loff_t *ppos);
@@ -935,10 +927,6 @@ int tracepoint_printk_sysctl(struct ctl_table *table, int write,
 static inline void  disable_trace_on_warning(void) { }
 #endif /* CONFIG_TRACING */
 
-#ifndef INIT_TRACE_RECURSION
-#define INIT_TRACE_RECURSION
-#endif
-
 #ifdef CONFIG_FTRACE_SYSCALLS
 
 unsigned long arch_syscall_addr(int nr);

commit 2dcd9c71c1ffa9a036e09047f60e08383bb0abb6
Merge: b1c2a344cc19 a96a5037ed0f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Nov 17 14:58:01 2017 -0800

    Merge tag 'trace-v4.15' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull tracing updates from
    
     - allow module init functions to be traced
    
     - clean up some unused or not used by config events (saves space)
    
     - clean up of trace histogram code
    
     - add support for preempt and interrupt enabled/disable events
    
     - other various clean ups
    
    * tag 'trace-v4.15' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace: (30 commits)
      tracing, thermal: Hide cpu cooling trace events when not in use
      tracing, thermal: Hide devfreq trace events when not in use
      ftrace: Kill FTRACE_OPS_FL_PER_CPU
      perf/ftrace: Small cleanup
      perf/ftrace: Fix function trace events
      perf/ftrace: Revert ("perf/ftrace: Fix double traces of perf on ftrace:function")
      tracing, dma-buf: Remove unused trace event dma_fence_annotate_wait_on
      tracing, memcg, vmscan: Hide trace events when not in use
      tracing/xen: Hide events that are not used when X86_PAE is not defined
      tracing: mark trace_test_buffer as __maybe_unused
      printk: Remove superfluous memory barriers from printk_safe
      ftrace: Clear hashes of stale ips of init memory
      tracing: Add support for preempt and irq enable/disable events
      tracing: Prepare to add preempt and irq trace events
      ftrace/kallsyms: Have /proc/kallsyms show saved mod init functions
      ftrace: Add freeing algorithm to free ftrace_mod_maps
      ftrace: Save module init functions kallsyms symbols for tracing
      ftrace: Allow module init functions to be traced
      ftrace: Add a ftrace_free_mem() function for modules to use
      tracing: Reimplement log2
      ...

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 2e028854bac7..e54d257983f2 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 /*
  * Ftrace header.  For implementation details beyond the random comments
  * scattered below, see: Documentation/trace/ftrace-design.txt

commit b3a88803ac5b4bda26017b485c8722a8487fefb7
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Oct 11 09:45:32 2017 +0200

    ftrace: Kill FTRACE_OPS_FL_PER_CPU
    
    The one and only user of FTRACE_OPS_FL_PER_CPU is gone, remove the
    lot.
    
    Link: http://lkml.kernel.org/r/20171011080224.372422809@infradead.org
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 1f8545caa691..252e334e7b5f 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -102,10 +102,6 @@ ftrace_func_t ftrace_ops_get_func(struct ftrace_ops *ops);
  * ENABLED - set/unset when ftrace_ops is registered/unregistered
  * DYNAMIC - set when ftrace_ops is registered to denote dynamically
  *           allocated ftrace_ops which need special care
- * PER_CPU - set manualy by ftrace_ops user to denote the ftrace_ops
- *           could be controlled by following calls:
- *             ftrace_function_local_enable
- *             ftrace_function_local_disable
  * SAVE_REGS - The ftrace_ops wants regs saved at each function called
  *            and passed to the callback. If this flag is set, but the
  *            architecture does not support passing regs
@@ -149,21 +145,20 @@ ftrace_func_t ftrace_ops_get_func(struct ftrace_ops *ops);
 enum {
 	FTRACE_OPS_FL_ENABLED			= 1 << 0,
 	FTRACE_OPS_FL_DYNAMIC			= 1 << 1,
-	FTRACE_OPS_FL_PER_CPU			= 1 << 2,
-	FTRACE_OPS_FL_SAVE_REGS			= 1 << 3,
-	FTRACE_OPS_FL_SAVE_REGS_IF_SUPPORTED	= 1 << 4,
-	FTRACE_OPS_FL_RECURSION_SAFE		= 1 << 5,
-	FTRACE_OPS_FL_STUB			= 1 << 6,
-	FTRACE_OPS_FL_INITIALIZED		= 1 << 7,
-	FTRACE_OPS_FL_DELETED			= 1 << 8,
-	FTRACE_OPS_FL_ADDING			= 1 << 9,
-	FTRACE_OPS_FL_REMOVING			= 1 << 10,
-	FTRACE_OPS_FL_MODIFYING			= 1 << 11,
-	FTRACE_OPS_FL_ALLOC_TRAMP		= 1 << 12,
-	FTRACE_OPS_FL_IPMODIFY			= 1 << 13,
-	FTRACE_OPS_FL_PID			= 1 << 14,
-	FTRACE_OPS_FL_RCU			= 1 << 15,
-	FTRACE_OPS_FL_TRACE_ARRAY		= 1 << 16,
+	FTRACE_OPS_FL_SAVE_REGS			= 1 << 2,
+	FTRACE_OPS_FL_SAVE_REGS_IF_SUPPORTED	= 1 << 3,
+	FTRACE_OPS_FL_RECURSION_SAFE		= 1 << 4,
+	FTRACE_OPS_FL_STUB			= 1 << 5,
+	FTRACE_OPS_FL_INITIALIZED		= 1 << 6,
+	FTRACE_OPS_FL_DELETED			= 1 << 7,
+	FTRACE_OPS_FL_ADDING			= 1 << 8,
+	FTRACE_OPS_FL_REMOVING			= 1 << 9,
+	FTRACE_OPS_FL_MODIFYING			= 1 << 10,
+	FTRACE_OPS_FL_ALLOC_TRAMP		= 1 << 11,
+	FTRACE_OPS_FL_IPMODIFY			= 1 << 12,
+	FTRACE_OPS_FL_PID			= 1 << 13,
+	FTRACE_OPS_FL_RCU			= 1 << 14,
+	FTRACE_OPS_FL_TRACE_ARRAY		= 1 << 15,
 };
 
 #ifdef CONFIG_DYNAMIC_FTRACE
@@ -198,7 +193,6 @@ struct ftrace_ops {
 	unsigned long			flags;
 	void				*private;
 	ftrace_func_t			saved_func;
-	int __percpu			*disabled;
 #ifdef CONFIG_DYNAMIC_FTRACE
 	struct ftrace_ops_hash		local_hash;
 	struct ftrace_ops_hash		*func_hash;
@@ -230,55 +224,6 @@ int register_ftrace_function(struct ftrace_ops *ops);
 int unregister_ftrace_function(struct ftrace_ops *ops);
 void clear_ftrace_function(void);
 
-/**
- * ftrace_function_local_enable - enable ftrace_ops on current cpu
- *
- * This function enables tracing on current cpu by decreasing
- * the per cpu control variable.
- * It must be called with preemption disabled and only on ftrace_ops
- * registered with FTRACE_OPS_FL_PER_CPU. If called without preemption
- * disabled, this_cpu_ptr will complain when CONFIG_DEBUG_PREEMPT is enabled.
- */
-static inline void ftrace_function_local_enable(struct ftrace_ops *ops)
-{
-	if (WARN_ON_ONCE(!(ops->flags & FTRACE_OPS_FL_PER_CPU)))
-		return;
-
-	(*this_cpu_ptr(ops->disabled))--;
-}
-
-/**
- * ftrace_function_local_disable - disable ftrace_ops on current cpu
- *
- * This function disables tracing on current cpu by increasing
- * the per cpu control variable.
- * It must be called with preemption disabled and only on ftrace_ops
- * registered with FTRACE_OPS_FL_PER_CPU. If called without preemption
- * disabled, this_cpu_ptr will complain when CONFIG_DEBUG_PREEMPT is enabled.
- */
-static inline void ftrace_function_local_disable(struct ftrace_ops *ops)
-{
-	if (WARN_ON_ONCE(!(ops->flags & FTRACE_OPS_FL_PER_CPU)))
-		return;
-
-	(*this_cpu_ptr(ops->disabled))++;
-}
-
-/**
- * ftrace_function_local_disabled - returns ftrace_ops disabled value
- *                                  on current cpu
- *
- * This function returns value of ftrace_ops::disabled on current cpu.
- * It must be called with preemption disabled and only on ftrace_ops
- * registered with FTRACE_OPS_FL_PER_CPU. If called without preemption
- * disabled, this_cpu_ptr will complain when CONFIG_DEBUG_PREEMPT is enabled.
- */
-static inline int ftrace_function_local_disabled(struct ftrace_ops *ops)
-{
-	WARN_ON_ONCE(!(ops->flags & FTRACE_OPS_FL_PER_CPU));
-	return *this_cpu_ptr(ops->disabled);
-}
-
 extern void ftrace_stub(unsigned long a0, unsigned long a1,
 			struct ftrace_ops *op, struct pt_regs *regs);
 

commit d59158162e032917a428704160a2063a02405ec6
Author: Joel Fernandes <joelaf@google.com>
Date:   Tue Oct 10 15:51:37 2017 -0700

    tracing: Add support for preempt and irq enable/disable events
    
    Preempt and irq trace events can be used for tracing the start and
    end of an atomic section which can be used by a trace viewer like
    systrace to graphically view the start and end of an atomic section and
    correlate them with latencies and scheduling issues.
    
    This also serves as a prelude to using synthetic events or probes to
    rewrite the preempt and irqsoff tracers, along with numerous benefits of
    using trace events features for these events.
    Link: http://lkml.kernel.org/r/20171006005432.14244-3-joelaf@google.com
    Link: http://lkml.kernel.org/r/20171010225137.17370-1-joelaf@google.com
    
    Cc: Peter Zilstra <peterz@infradead.org>
    Cc: kernel-team@android.com
    Signed-off-by: Joel Fernandes <joelaf@google.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 346f8294e40a..1f8545caa691 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -769,7 +769,8 @@ static inline unsigned long get_lock_parent_ip(void)
   static inline void time_hardirqs_off(unsigned long a0, unsigned long a1) { }
 #endif
 
-#ifdef CONFIG_PREEMPT_TRACER
+#if defined(CONFIG_PREEMPT_TRACER) || \
+	(defined(CONFIG_DEBUG_PREEMPT) && defined(CONFIG_PREEMPTIRQ_EVENTS))
   extern void trace_preempt_on(unsigned long a0, unsigned long a1);
   extern void trace_preempt_off(unsigned long a0, unsigned long a1);
 #else

commit 6171a0310a06a7a0cb83713fa7068bdd4192de19
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Wed Sep 6 08:40:41 2017 -0400

    ftrace/kallsyms: Have /proc/kallsyms show saved mod init functions
    
    If a module is loaded while tracing is enabled, then there's a possibility
    that the module init functions were traced. These functions have their name
    and address stored by ftrace such that it can translate the function address
    that is written into the buffer into a human readable function name.
    
    As userspace tools may be doing the same, they need a way to map function
    names to their address as well. This is done through reading /proc/kallsyms.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 202b40784c4e..346f8294e40a 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -56,6 +56,9 @@ struct ftrace_hash;
 const char *
 ftrace_mod_address_lookup(unsigned long addr, unsigned long *size,
 		   unsigned long *off, char **modname, char *sym);
+int ftrace_mod_get_kallsym(unsigned int symnum, unsigned long *value,
+			   char *type, char *name,
+			   char *module_name, int *exported);
 #else
 static inline const char *
 ftrace_mod_address_lookup(unsigned long addr, unsigned long *size,
@@ -63,6 +66,12 @@ ftrace_mod_address_lookup(unsigned long addr, unsigned long *size,
 {
 	return NULL;
 }
+static inline int ftrace_mod_get_kallsym(unsigned int symnum, unsigned long *value,
+					 char *type, char *name,
+					 char *module_name, int *exported)
+{
+	return -1;
+}
 #endif
 
 

commit aba4b5c22cbac296f4081a0476d0c55828f135b4
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Fri Sep 1 08:35:38 2017 -0400

    ftrace: Save module init functions kallsyms symbols for tracing
    
    If function tracing is active when the module init functions are freed, then
    store them to be referenced by kallsyms. As module init functions can now be
    traced on module load, they were useless:
    
     ># echo ':mod:snd_seq' > set_ftrace_filter
     ># echo function > current_tracer
     ># modprobe snd_seq
     ># cat trace
     # tracer: function
     #
     #                              _-----=> irqs-off
     #                             / _----=> need-resched
     #                            | / _---=> hardirq/softirq
     #                            || / _--=> preempt-depth
     #                            ||| /     delay
     #           TASK-PID   CPU#  ||||    TIMESTAMP  FUNCTION
     #              | |       |   ||||       |         |
             modprobe-2786  [000] ....  3189.037874: 0xffffffffa0860000 <-do_one_initcall
             modprobe-2786  [000] ....  3189.037876: 0xffffffffa086004d <-0xffffffffa086000f
             modprobe-2786  [000] ....  3189.037876: 0xffffffffa086010d <-0xffffffffa0860018
             modprobe-2786  [000] ....  3189.037877: 0xffffffffa086011a <-0xffffffffa0860021
             modprobe-2786  [000] ....  3189.037877: 0xffffffffa0860080 <-0xffffffffa086002a
             modprobe-2786  [000] ....  3189.039523: 0xffffffffa0860400 <-0xffffffffa0860033
             modprobe-2786  [000] ....  3189.039523: 0xffffffffa086038a <-0xffffffffa086041c
             modprobe-2786  [000] ....  3189.039591: 0xffffffffa086038a <-0xffffffffa0860436
             modprobe-2786  [000] ....  3189.039657: 0xffffffffa086038a <-0xffffffffa0860450
             modprobe-2786  [000] ....  3189.039719: 0xffffffffa0860127 <-0xffffffffa086003c
             modprobe-2786  [000] ....  3189.039742: snd_seq_create_kernel_client <-0xffffffffa08601f6
    
    When the output is shown, the kallsyms for the module init functions have
    already been freed, and the output of the trace can not convert them to
    their function names.
    
    Now this looks like this:
    
     # tracer: function
     #
     #                              _-----=> irqs-off
     #                             / _----=> need-resched
     #                            | / _---=> hardirq/softirq
     #                            || / _--=> preempt-depth
     #                            ||| /     delay
     #           TASK-PID   CPU#  ||||    TIMESTAMP  FUNCTION
     #              | |       |   ||||       |         |
             modprobe-2463  [002] ....   174.243237: alsa_seq_init <-do_one_initcall
             modprobe-2463  [002] ....   174.243239: client_init_data <-alsa_seq_init
             modprobe-2463  [002] ....   174.243240: snd_sequencer_memory_init <-alsa_seq_init
             modprobe-2463  [002] ....   174.243240: snd_seq_queues_init <-alsa_seq_init
             modprobe-2463  [002] ....   174.243240: snd_sequencer_device_init <-alsa_seq_init
             modprobe-2463  [002] ....   174.244860: snd_seq_info_init <-alsa_seq_init
             modprobe-2463  [002] ....   174.244861: create_info_entry <-snd_seq_info_init
             modprobe-2463  [002] ....   174.244936: create_info_entry <-snd_seq_info_init
             modprobe-2463  [002] ....   174.245003: create_info_entry <-snd_seq_info_init
             modprobe-2463  [002] ....   174.245072: snd_seq_system_client_init <-alsa_seq_init
             modprobe-2463  [002] ....   174.245094: snd_seq_create_kernel_client <-snd_seq_system_client_init
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 47fc404ad233..202b40784c4e 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -51,6 +51,21 @@ static inline void early_trace_init(void) { }
 struct module;
 struct ftrace_hash;
 
+#if defined(CONFIG_FUNCTION_TRACER) && defined(CONFIG_MODULES) && \
+	defined(CONFIG_DYNAMIC_FTRACE)
+const char *
+ftrace_mod_address_lookup(unsigned long addr, unsigned long *size,
+		   unsigned long *off, char **modname, char *sym);
+#else
+static inline const char *
+ftrace_mod_address_lookup(unsigned long addr, unsigned long *size,
+		   unsigned long *off, char **modname, char *sym)
+{
+	return NULL;
+}
+#endif
+
+
 #ifdef CONFIG_FUNCTION_TRACER
 
 extern int ftrace_enabled;
@@ -151,10 +166,10 @@ struct ftrace_ops_hash {
 };
 
 void ftrace_free_init_mem(void);
-void ftrace_free_mem(void *start, void *end);
+void ftrace_free_mem(struct module *mod, void *start, void *end);
 #else
 static inline void ftrace_free_init_mem(void) { }
-static inline void ftrace_free_mem(void *start, void *end) { }
+static inline void ftrace_free_mem(struct module *mod, void *start, void *end) { }
 #endif
 
 /*
@@ -272,6 +287,7 @@ static inline int ftrace_nr_registered_ops(void)
 static inline void clear_ftrace_function(void) { }
 static inline void ftrace_kill(void) { }
 static inline void ftrace_free_init_mem(void) { }
+static inline void ftrace_free_mem(struct module *mod, void *start, void *end) { }
 #endif /* CONFIG_FUNCTION_TRACER */
 
 #ifdef CONFIG_STACK_TRACER

commit 6cafbe159416822f6d3dfd711bf4c39050c650ba
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Tue Jun 20 10:44:58 2017 -0400

    ftrace: Add a ftrace_free_mem() function for modules to use
    
    In order to be able to trace module init functions, the module code needs to
    tell ftrace what is being freed when the init sections are freed. Use the
    code that the main init calls to tell ftrace to free the main init sections.
    This requires passing in a start and end address to free.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 2e028854bac7..47fc404ad233 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -151,8 +151,10 @@ struct ftrace_ops_hash {
 };
 
 void ftrace_free_init_mem(void);
+void ftrace_free_mem(void *start, void *end);
 #else
 static inline void ftrace_free_init_mem(void) { }
+static inline void ftrace_free_mem(void *start, void *end) { }
 #endif
 
 /*

commit 60361e12d01676e23a8de89a5ef4a349ae97f616
Author: Zev Weiss <zev@bewilderbeest.net>
Date:   Wed Aug 30 05:36:38 2017 -0500

    ftrace: Fix debug preempt config name in stack_tracer_{en,dis}able
    
    stack_tracer_disable()/stack_tracer_enable() had been using the wrong
    name for the config symbol to enable their preempt-debugging checks --
    fix with a word swap.
    
    Link: http://lkml.kernel.org/r/20170831154036.4xldyakmmhuts5x7@hatter.bewilderbeest.net
    
    Cc: stable@vger.kernel.org
    Fixes: 8aaf1ee70e ("tracing: Rename trace_active to disable_stack_tracer and inline its modification")
    Signed-off-by: Zev Weiss <zev@bewilderbeest.net>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 6383115e9d2c..2e028854bac7 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -307,7 +307,7 @@ DECLARE_PER_CPU(int, disable_stack_tracer);
 static inline void stack_tracer_disable(void)
 {
 	/* Preemption or interupts must be disabled */
-	if (IS_ENABLED(CONFIG_PREEMPT_DEBUG))
+	if (IS_ENABLED(CONFIG_DEBUG_PREEMPT))
 		WARN_ON_ONCE(!preempt_count() || !irqs_disabled());
 	this_cpu_inc(disable_stack_tracer);
 }
@@ -320,7 +320,7 @@ static inline void stack_tracer_disable(void)
  */
 static inline void stack_tracer_enable(void)
 {
-	if (IS_ENABLED(CONFIG_PREEMPT_DEBUG))
+	if (IS_ENABLED(CONFIG_DEBUG_PREEMPT))
 		WARN_ON_ONCE(!preempt_count() || !irqs_disabled());
 	this_cpu_dec(disable_stack_tracer);
 }

commit f86f418059b94aa01f9342611a272ca60c583e89
Author: Chunyan Zhang <zhang.chunyan@linaro.org>
Date:   Wed Jun 7 16:12:51 2017 +0800

    trace: fix the errors caused by incompatible type of RCU variables
    
    The variables which are processed by RCU functions should be annotated
    as RCU, otherwise sparse will report the errors like below:
    
    "error: incompatible types in comparison expression (different
    address spaces)"
    
    Link: http://lkml.kernel.org/r/1496823171-7758-1-git-send-email-zhang.chunyan@linaro.org
    
    Signed-off-by: Chunyan Zhang <zhang.chunyan@linaro.org>
    [ Updated to not be 100% 80 column strict ]
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 5857390ac35a..6383115e9d2c 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -145,8 +145,8 @@ enum {
 #ifdef CONFIG_DYNAMIC_FTRACE
 /* The hash used to know what functions callbacks trace */
 struct ftrace_ops_hash {
-	struct ftrace_hash		*notrace_hash;
-	struct ftrace_hash		*filter_hash;
+	struct ftrace_hash __rcu	*notrace_hash;
+	struct ftrace_hash __rcu	*filter_hash;
 	struct mutex			regex_lock;
 };
 
@@ -168,7 +168,7 @@ static inline void ftrace_free_init_mem(void) { }
  */
 struct ftrace_ops {
 	ftrace_func_t			func;
-	struct ftrace_ops		*next;
+	struct ftrace_ops __rcu		*next;
 	unsigned long			flags;
 	void				*private;
 	ftrace_func_t			saved_func;

commit 8c08f0d5c6fb10ff93ffb1cbf416f4f1c3a52a80
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Mon Jun 26 11:47:31 2017 -0400

    ftrace: Have cached module filters be an active filter
    
    When a module filter is added to set_ftrace_filter, if the module is not
    loaded, it is cached. This should be considered an active filter, and
    function tracing should be filtered by this. That is, if a cached module
    filter is the only filter set, then no function tracing should be happening,
    as all the functions available will be filtered out.
    
    This makes sense, as the reason to add a cached module filter, is to trace
    the module when you load it. There shouldn't be any other tracing happening
    until then.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 9fb9a67dc9d4..5857390ac35a 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -120,6 +120,7 @@ ftrace_func_t ftrace_ops_get_func(struct ftrace_ops *ops);
  *            this ops will fail to register or set_filter_ip.
  * PID     - Is affected by set_ftrace_pid (allows filtering on those pids)
  * RCU     - Set when the ops can only be called when RCU is watching.
+ * TRACE_ARRAY - The ops->private points to a trace_array descriptor.
  */
 enum {
 	FTRACE_OPS_FL_ENABLED			= 1 << 0,
@@ -138,6 +139,7 @@ enum {
 	FTRACE_OPS_FL_IPMODIFY			= 1 << 13,
 	FTRACE_OPS_FL_PID			= 1 << 14,
 	FTRACE_OPS_FL_RCU			= 1 << 15,
+	FTRACE_OPS_FL_TRACE_ARRAY		= 1 << 16,
 };
 
 #ifdef CONFIG_DYNAMIC_FTRACE

commit 5985ea8bd5d1b820b909af49fbc2767a990080a6
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Fri Jun 23 16:05:11 2017 -0400

    ftrace: Have the cached module list show in set_ftrace_filter
    
    When writing in a module filter into set_ftrace_filter for a module that is
    not yet loaded, it it cached, and will be executed when the module is loaded
    (although that is not implemented yet at this commit). Display the list of
    cached modules to be traced.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 1b6992e994e6..9fb9a67dc9d4 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -446,7 +446,8 @@ enum {
 	FTRACE_ITER_PRINTALL	= (1 << 2),
 	FTRACE_ITER_DO_PROBES	= (1 << 3),
 	FTRACE_ITER_PROBE	= (1 << 4),
-	FTRACE_ITER_ENABLED	= (1 << 5),
+	FTRACE_ITER_MOD		= (1 << 5),
+	FTRACE_ITER_ENABLED	= (1 << 6),
 };
 
 void arch_ftrace_update_code(int command);

commit d0ba52f1d7649a3f088e410e860559cf36d479d0
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Wed Jun 21 13:39:13 2017 -0400

    ftrace: Add missing comment for FTRACE_OPS_FL_RCU
    
    All the enum flags for FTRACE_OPS has a comment except for the RCU one. Add
    the comment for that.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 473f088aabea..1b6992e994e6 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -119,6 +119,7 @@ ftrace_func_t ftrace_ops_get_func(struct ftrace_ops *ops);
  *            for any of the functions that this ops will be registered for, then
  *            this ops will fail to register or set_filter_ip.
  * PID     - Is affected by set_ftrace_pid (allows filtering on those pids)
+ * RCU     - Set when the ops can only be called when RCU is watching.
  */
 enum {
 	FTRACE_OPS_FL_ENABLED			= 1 << 0,

commit ad61dd303a0f2439bb104349e2d2ec91a3010ce0
Author: Stephen Boyd <sboyd@codeaurora.org>
Date:   Mon May 8 15:57:50 2017 -0700

    scripts/spelling.txt: add regsiter -> register spelling mistake
    
    This typo is quite common.  Fix it and add it to the spelling file so
    that checkpatch catches it earlier.
    
    Link: http://lkml.kernel.org/r/20170317011131.6881-2-sboyd@codeaurora.org
    Signed-off-by: Stephen Boyd <sboyd@codeaurora.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 6d2a63e4ea52..473f088aabea 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -72,7 +72,7 @@ ftrace_func_t ftrace_ops_get_func(struct ftrace_ops *ops);
  * CONTROL, SAVE_REGS, SAVE_REGS_IF_SUPPORTED, RECURSION_SAFE, STUB and
  * IPMODIFY are a kind of attribute flags which can be set only before
  * registering the ftrace_ops, and can not be modified while registered.
- * Changing those attribute flags after regsitering ftrace_ops will
+ * Changing those attribute flags after registering ftrace_ops will
  * cause unexpected results.
  *
  * ENABLED - set/unset when ftrace_ops is registered/unregistered

commit eee8ded131f15e0f5b1897c9c4a7687fabd28822
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Tue Apr 4 21:31:28 2017 -0400

    ftrace: Have the function probes call their own function
    
    Now that the function probes have their own ftrace_ops, there's no reason to
    continue using the ftrace_func_hash to find which probe to call in the
    function callback. The ops that is passed in to the function callback is
    part of the probe_ops to call.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 774e7a95c201..6d2a63e4ea52 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -443,8 +443,8 @@ enum {
 	FTRACE_ITER_FILTER	= (1 << 0),
 	FTRACE_ITER_NOTRACE	= (1 << 1),
 	FTRACE_ITER_PRINTALL	= (1 << 2),
-	FTRACE_ITER_DO_HASH	= (1 << 3),
-	FTRACE_ITER_HASH	= (1 << 4),
+	FTRACE_ITER_DO_PROBES	= (1 << 3),
+	FTRACE_ITER_PROBE	= (1 << 4),
 	FTRACE_ITER_ENABLED	= (1 << 5),
 };
 

commit 92a68fa047ca5b8e1991af2d50b23ad9452613cd
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Fri Mar 31 19:21:41 2017 -0400

    ftrace: Move the function commands into the tracing directory
    
    As nothing outside the tracing directory uses the function command mechanism,
    I'm moving the prototypes out of the include/linux/ftrace.h and into the
    local kernel/trace/trace.h header. I plan on making them hook to the
    trace_array structure which is local to kernel/trace, and I do not want to
    expose it to the rest of the kernel. This requires that the command functions
    must also be local to tracing. But luckily nothing else uses them.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 3e790ff1c501..774e7a95c201 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -326,14 +326,6 @@ static inline void stack_tracer_disable(void) { }
 static inline void stack_tracer_enable(void) { }
 #endif
 
-struct ftrace_func_command {
-	struct list_head	list;
-	char			*name;
-	int			(*func)(struct ftrace_hash *hash,
-					char *func, char *cmd,
-					char *params, int enable);
-};
-
 #ifdef CONFIG_DYNAMIC_FTRACE
 
 int ftrace_arch_code_modify_prepare(void);
@@ -421,9 +413,6 @@ void ftrace_set_global_notrace(unsigned char *buf, int len, int reset);
 void ftrace_free_filter(struct ftrace_ops *ops);
 void ftrace_ops_set_global_filter(struct ftrace_ops *ops);
 
-int register_ftrace_command(struct ftrace_func_command *cmd);
-int unregister_ftrace_command(struct ftrace_func_command *cmd);
-
 enum {
 	FTRACE_UPDATE_CALLS		= (1 << 0),
 	FTRACE_DISABLE_CALLS		= (1 << 1),
@@ -639,14 +628,6 @@ static inline void ftrace_enable_daemon(void) { }
 static inline void ftrace_module_init(struct module *mod) { }
 static inline void ftrace_module_enable(struct module *mod) { }
 static inline void ftrace_release_mod(struct module *mod) { }
-static inline __init int register_ftrace_command(struct ftrace_func_command *cmd)
-{
-	return -EINVAL;
-}
-static inline __init int unregister_ftrace_command(char *cmd_name)
-{
-	return -EINVAL;
-}
 static inline int ftrace_text_reserved(const void *start, const void *end)
 {
 	return 0;

commit ec19b85913486993d7d6f747beed1a711afd47d8
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Fri Mar 31 19:01:14 2017 -0400

    ftrace: Move the probe function into the tracing directory
    
    As nothing outside the tracing directory uses the function probes mechanism,
    I'm moving the prototypes out of the include/linux/ftrace.h and into the
    local kernel/trace/trace.h header. I plan on making them hook to the
    trace_array structure which is local to kernel/trace, and I do not want to
    expose it to the rest of the kernel. This requires that the probe functions
    must also be local to tracing. But luckily nothing else uses them.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 06b2990a35e4..3e790ff1c501 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -360,30 +360,6 @@ void ftrace_bug(int err, struct dyn_ftrace *rec);
 
 struct seq_file;
 
-struct ftrace_probe_ops {
-	void			(*func)(unsigned long ip,
-					unsigned long parent_ip,
-					void **data);
-	int			(*init)(struct ftrace_probe_ops *ops,
-					unsigned long ip, void **data);
-	void			(*free)(struct ftrace_probe_ops *ops,
-					unsigned long ip, void **data);
-	int			(*print)(struct seq_file *m,
-					 unsigned long ip,
-					 struct ftrace_probe_ops *ops,
-					 void *data);
-};
-
-extern int
-register_ftrace_function_probe(char *glob, struct ftrace_probe_ops *ops,
-			      void *data);
-extern void
-unregister_ftrace_function_probe(char *glob, struct ftrace_probe_ops *ops,
-				void *data);
-extern void
-unregister_ftrace_function_probe_func(char *glob, struct ftrace_probe_ops *ops);
-extern void unregister_ftrace_function_probe_all(char *glob);
-
 extern int ftrace_text_reserved(const void *start, const void *end);
 
 extern int ftrace_nr_registered_ops(void);

commit 8aaf1ee70e19ac74cbbb81098edfa328d1ab4bd7
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu Apr 6 15:47:32 2017 -0400

    tracing: Rename trace_active to disable_stack_tracer and inline its modification
    
    In order to eliminate a function call, make "trace_active" into
    "disable_stack_tracer" and convert stack_tracer_disable() and friends into
    static inline functions.
    
    Acked-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 7b4e6572ab21..06b2990a35e4 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -287,8 +287,40 @@ stack_trace_sysctl(struct ctl_table *table, int write,
 		   void __user *buffer, size_t *lenp,
 		   loff_t *ppos);
 
-void stack_tracer_disable(void);
-void stack_tracer_enable(void);
+/* DO NOT MODIFY THIS VARIABLE DIRECTLY! */
+DECLARE_PER_CPU(int, disable_stack_tracer);
+
+/**
+ * stack_tracer_disable - temporarily disable the stack tracer
+ *
+ * There's a few locations (namely in RCU) where stack tracing
+ * cannot be executed. This function is used to disable stack
+ * tracing during those critical sections.
+ *
+ * This function must be called with preemption or interrupts
+ * disabled and stack_tracer_enable() must be called shortly after
+ * while preemption or interrupts are still disabled.
+ */
+static inline void stack_tracer_disable(void)
+{
+	/* Preemption or interupts must be disabled */
+	if (IS_ENABLED(CONFIG_PREEMPT_DEBUG))
+		WARN_ON_ONCE(!preempt_count() || !irqs_disabled());
+	this_cpu_inc(disable_stack_tracer);
+}
+
+/**
+ * stack_tracer_enable - re-enable the stack tracer
+ *
+ * After stack_tracer_disable() is called, stack_tracer_enable()
+ * must be called shortly afterward.
+ */
+static inline void stack_tracer_enable(void)
+{
+	if (IS_ENABLED(CONFIG_PREEMPT_DEBUG))
+		WARN_ON_ONCE(!preempt_count() || !irqs_disabled());
+	this_cpu_dec(disable_stack_tracer);
+}
 #else
 static inline void stack_tracer_disable(void) { }
 static inline void stack_tracer_enable(void) { }

commit 5367278cb7ba74537bcad1470d75f30d95b09c14
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu Apr 6 12:26:20 2017 -0400

    tracing: Add stack_tracer_disable/enable() functions
    
    There are certain parts of the kernel that cannot let stack tracing
    proceed (namely in RCU), because the stack tracer uses RCU, and parts of RCU
    internals cannot handle having RCU read side locks taken.
    
    Add stack_tracer_disable() and stack_tracer_enable() functions to let RCU
    stop stack tracing on the current CPU when it is in those critical sections.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index ef7123219f14..7b4e6572ab21 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -286,6 +286,12 @@ int
 stack_trace_sysctl(struct ctl_table *table, int write,
 		   void __user *buffer, size_t *lenp,
 		   loff_t *ppos);
+
+void stack_tracer_disable(void);
+void stack_tracer_enable(void);
+#else
+static inline void stack_tracer_disable(void) { }
+static inline void stack_tracer_enable(void) { }
 #endif
 
 struct ftrace_func_command {

commit b80f0f6c9ed3958ff4002b6135f43a1ef312a610
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Mon Apr 3 12:57:35 2017 -0400

    ftrace: Have init/main.c call ftrace directly to free init memory
    
    Relying on free_reserved_area() to call ftrace to free init memory proved to
    not be sufficient. The issue is that on x86, when debug_pagealloc is
    enabled, the init memory is not freed, but simply set as not present. Since
    ftrace was uninformed of this, starting function tracing still tries to
    update pages that are not present according to the page tables, causing
    ftrace to bug, as well as killing the kernel itself.
    
    Instead of relying on free_reserved_area(), have init/main.c call ftrace
    directly just before it frees the init memory. Then it needs to use
    __init_begin and __init_end to know where the init memory location is.
    Looking at all archs (and testing what I can), it appears that this should
    work for each of them.
    
    Reported-by: kernel test robot <xiaolong.ye@intel.com>
    Reported-by: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 0276a2c487e6..ef7123219f14 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -147,9 +147,9 @@ struct ftrace_ops_hash {
 	struct mutex			regex_lock;
 };
 
-void ftrace_free_mem(void *start, void *end);
+void ftrace_free_init_mem(void);
 #else
-static inline void ftrace_free_mem(void *start, void *end) { }
+static inline void ftrace_free_init_mem(void) { }
 #endif
 
 /*
@@ -266,7 +266,7 @@ static inline int ftrace_nr_registered_ops(void)
 }
 static inline void clear_ftrace_function(void) { }
 static inline void ftrace_kill(void) { }
-static inline void ftrace_free_mem(void *start, void *end) { }
+static inline void ftrace_free_init_mem(void) { }
 #endif /* CONFIG_FUNCTION_TRACER */
 
 #ifdef CONFIG_STACK_TRACER

commit 42c269c88dc146982a54a8267f71abc99f12852a
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Fri Mar 3 16:15:39 2017 -0500

    ftrace: Allow for function tracing to record init functions on boot up
    
    Adding a hook into free_reserve_area() that informs ftrace that boot up init
    text is being free, lets ftrace safely remove those init functions from its
    records, which keeps ftrace from trying to modify text that no longer
    exists.
    
    Note, this still does not allow for tracing .init text of modules, as
    modules require different work for freeing its init code.
    
    Link: http://lkml.kernel.org/r/1488502497.7212.24.camel@linux.intel.com
    
    Cc: linux-mm@kvack.org
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Requested-by: Todd Brandt <todd.e.brandt@linux.intel.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 569db5589851..0276a2c487e6 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -146,6 +146,10 @@ struct ftrace_ops_hash {
 	struct ftrace_hash		*filter_hash;
 	struct mutex			regex_lock;
 };
+
+void ftrace_free_mem(void *start, void *end);
+#else
+static inline void ftrace_free_mem(void *start, void *end) { }
 #endif
 
 /*
@@ -262,6 +266,7 @@ static inline int ftrace_nr_registered_ops(void)
 }
 static inline void clear_ftrace_function(void) { }
 static inline void ftrace_kill(void) { }
+static inline void ftrace_free_mem(void *start, void *end) { }
 #endif /* CONFIG_FUNCTION_TRACER */
 
 #ifdef CONFIG_STACK_TRACER

commit e725c731e3bb1e892e7b564c945b121cb41d1087
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Fri Mar 3 13:37:33 2017 -0500

    tracing: Split tracing initialization into two for early initialization
    
    Create an early_trace_init() function that will initialize the buffers and
    allow for ealier use of trace_printk(). This will also allow for future work
    to have function tracing start earlier at boot up.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 3633e8beff39..569db5589851 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -42,8 +42,10 @@
 /* Main tracing buffer and events set up */
 #ifdef CONFIG_TRACING
 void trace_init(void);
+void early_trace_init(void);
 #else
 static inline void trace_init(void) { }
+static inline void early_trace_init(void) { }
 #endif
 
 struct module;

commit 179a7ba6806805bd4cd7a5e4574b83353c5615ad
Merge: 5e176d6973bd 3dbb16b87b57
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Dec 15 13:49:34 2016 -0800

    Merge tag 'trace-v4.10' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull tracing updates from Steven Rostedt:
     "This release has a few updates:
    
       - STM can hook into the function tracer
       - Function filtering now supports more advance glob matching
       - Ftrace selftests updates and added tests
       - Softirq tag in traces now show only softirqs
       - ARM nop added to non traced locations at compile time
       - New trace_marker_raw file that allows for binary input
       - Optimizations to the ring buffer
       - Removal of kmap in trace_marker
       - Wakeup and irqsoff tracers now adhere to the set_graph_notrace file
       - Other various fixes and clean ups"
    
    * tag 'trace-v4.10' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace: (42 commits)
      selftests: ftrace: Shift down default message verbosity
      kprobes/trace: Fix kprobe selftest for newer gcc
      tracing/kprobes: Add a helper method to return number of probe hits
      tracing/rb: Init the CPU mask on allocation
      tracing: Use SOFTIRQ_OFFSET for softirq dectection for more accurate results
      tracing/fgraph: Have wakeup and irqsoff tracers ignore graph functions too
      fgraph: Handle a case where a tracer ignores set_graph_notrace
      tracing: Replace kmap with copy_from_user() in trace_marker writing
      ftrace/x86_32: Set ftrace_stub to weak to prevent gcc from using short jumps to it
      tracing: Allow benchmark to be enabled at early_initcall()
      tracing: Have system enable return error if one of the events fail
      tracing: Do not start benchmark on boot up
      tracing: Have the reg function allow to fail
      ring-buffer: Force rb_end_commit() and rb_set_commit_to_write() inline
      ring-buffer: Froce rb_update_write_stamp() to be inlined
      ring-buffer: Force inline of hotpath helper functions
      tracing: Make __buffer_unlock_commit() always_inline
      tracing: Make tracepoint_printk a static_key
      ring-buffer: Always inline rb_event_data()
      ring-buffer: Make rb_reserve_next_event() always inlined
      ...

commit 4239174570da080f3623724d97062bf55de7e36b
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed Nov 23 15:52:45 2016 -0500

    tracing: Make tracepoint_printk a static_key
    
    Currently, when tracepoint_printk is set (enabled by the "tp_printk" kernel
    command line), it causes trace events to print via printk(). This is a very
    dangerous operation, but is useful for debugging.
    
    The issue is, it's seldom used, but it is always checked even if it's not
    enabled by the kernel command line. Instead of having this feature called by
    a branch against a variable, turn that variable into a static key, and this
    will remove the test and jump.
    
    To simplify things, the functions output_printk() and
    trace_event_buffer_commit() were moved from trace_events.c to trace.c.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index b3d34d3e0e7e..8700049fd0e5 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -945,6 +945,10 @@ extern int __disable_trace_on_warning;
 #define INIT_TRACE_RECURSION		.trace_recursion = 0,
 #endif
 
+int tracepoint_printk_sysctl(struct ctl_table *table, int write,
+			     void __user *buffer, size_t *lenp,
+			     loff_t *ppos);
+
 #else /* CONFIG_TRACING */
 static inline void  disable_trace_on_warning(void) { }
 #endif /* CONFIG_TRACING */

commit d032ae8921ea792c1e6b2abb44022b2403f651f6
Author: Joel Fernandes <joelaf@google.com>
Date:   Tue Nov 15 12:31:20 2016 -0800

    ftrace: Provide API to use global filtering for ftrace ops
    
    Currently the global_ops filtering hash is not available to outside users
    registering for function tracing. Provide an API for those users to be
    able to choose global filtering.
    
    This is in preparation for pstore's ftrace feature to be able to
    use the global filters.
    
    Suggested-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Anton Vorontsov <anton@enomsg.org>
    Cc: Colin Cross <ccross@android.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Signed-off-by: Joel Fernandes <joelaf@google.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Kees Cook <keescook@chromium.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index b3d34d3e0e7e..d4a884db16a3 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -398,6 +398,7 @@ int ftrace_set_notrace(struct ftrace_ops *ops, unsigned char *buf,
 void ftrace_set_global_filter(unsigned char *buf, int len, int reset);
 void ftrace_set_global_notrace(unsigned char *buf, int len, int reset);
 void ftrace_free_filter(struct ftrace_ops *ops);
+void ftrace_ops_set_global_filter(struct ftrace_ops *ops);
 
 int register_ftrace_command(struct ftrace_func_command *cmd);
 int unregister_ftrace_command(struct ftrace_func_command *cmd);
@@ -645,6 +646,7 @@ static inline unsigned long ftrace_location(unsigned long ip)
 #define ftrace_set_filter(ops, buf, len, reset) ({ -ENODEV; })
 #define ftrace_set_notrace(ops, buf, len, reset) ({ -ENODEV; })
 #define ftrace_free_filter(ops) do { } while (0)
+#define ftrace_ops_set_global_filter(ops) do { } while (0)
 
 static inline ssize_t ftrace_filter_write(struct file *file, const char __user *ubuf,
 			    size_t cnt, loff_t *ppos) { return -ENODEV; }

commit 95107b30be68953e3a4f1c3994c2233500502ccf
Merge: 541efb763264 a0d0c6216afa
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 6 11:48:41 2016 -0700

    Merge tag 'trace-v4.9' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull tracing updates from Steven Rostedt:
     "This release cycle is rather small.  Just a few fixes to tracing.
    
      The big change is the addition of the hwlat tracer. It not only
      detects SMIs, but also other latency that's caused by the hardware. I
      have detected some latency from large boxes having bus contention"
    
    * tag 'trace-v4.9' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace:
      tracing: Call traceoff trigger after event is recorded
      ftrace/scripts: Add helper script to bisect function tracing problem functions
      tracing: Have max_latency be defined for HWLAT_TRACER as well
      tracing: Add NMI tracing in hwlat detector
      tracing: Have hwlat trace migrate across tracing_cpumask CPUs
      tracing: Add documentation for hwlat_detector tracer
      tracing: Added hardware latency tracer
      ftrace: Access ret_stack->subtime only in the function profiler
      function_graph: Handle TRACE_BPUTS in print_graph_comment
      tracing/uprobe: Drop isdigit() check in create_trace_uprobe

commit 8861dd303cba879bae9a9dcee74042fb642bf03b
Author: Namhyung Kim <namhyung@kernel.org>
Date:   Wed Aug 31 11:55:29 2016 +0900

    ftrace: Access ret_stack->subtime only in the function profiler
    
    The subtime is used only for function profiler with function graph
    tracer enabled.  Move the definition of subtime under
    CONFIG_FUNCTION_PROFILER to reduce the memory usage.  Also move the
    initialization of subtime into the graph entry callback.
    
    Link: http://lkml.kernel.org/r/20160831025529.24018-1-namhyung@kernel.org
    
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 7d565afe35d2..1e2b316d6693 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -794,7 +794,9 @@ struct ftrace_ret_stack {
 	unsigned long ret;
 	unsigned long func;
 	unsigned long long calltime;
+#ifdef CONFIG_FUNCTION_PROFILER
 	unsigned long long subtime;
+#endif
 	unsigned long fp;
 };
 

commit 223918e32a87c79ac55ca4aa513ba405ba4d57cd
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Fri Aug 19 06:52:58 2016 -0500

    ftrace: Add ftrace_graph_ret_addr() stack unwinding helpers
    
    When function graph tracing is enabled for a function, ftrace modifies
    the stack by replacing the original return address with the address of a
    hook function (return_to_handler).
    
    Stack unwinders need a way to get the original return address.  Add an
    arch-independent helper function for that named ftrace_graph_ret_addr().
    
    This adds two variations of the function: one depends on
    HAVE_FUNCTION_GRAPH_RET_ADDR_PTR, and the other relies on an index state
    variable.
    
    The former is recommended because, in some cases, the latter can cause
    problems when the unwinder skips stack frames.  It can get out of sync
    with the ret_stack index and wrong addresses can be reported for the
    stack trace.
    
    Once all arches have been ported to use
    HAVE_FUNCTION_GRAPH_RET_ADDR_PTR, we can get rid of the distinction.
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Byungchul Park <byungchul.park@lge.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Nilay Vaish <nilayvaish@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/36bd90f762fc5e5af3929e3797a68a64906421cf.1471607358.git.jpoimboe@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 483e02a50d37..6f93ac46e7f0 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -814,6 +814,9 @@ extern int
 ftrace_push_return_trace(unsigned long ret, unsigned long func, int *depth,
 			 unsigned long frame_pointer, unsigned long *retp);
 
+unsigned long ftrace_graph_ret_addr(struct task_struct *task, int *idx,
+				    unsigned long ret, unsigned long *retp);
+
 /*
  * Sometimes we don't want to trace a function with the function
  * graph tracer but we want them to keep traced by the usual function
@@ -875,6 +878,13 @@ static inline int task_curr_ret_stack(struct task_struct *tsk)
 	return -1;
 }
 
+static inline unsigned long
+ftrace_graph_ret_addr(struct task_struct *task, int *idx, unsigned long ret,
+		      unsigned long *retp)
+{
+	return ret;
+}
+
 static inline void pause_graph_tracing(void) { }
 static inline void unpause_graph_tracing(void) { }
 #endif /* CONFIG_FUNCTION_GRAPH_TRACER */

commit 9a7c348ba6a46f6270d4fe49577649dad5664fe7
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Fri Aug 19 06:52:57 2016 -0500

    ftrace: Add return address pointer to ftrace_ret_stack
    
    Storing this value will help prevent unwinders from getting out of sync
    with the function graph tracer ret_stack.  Now instead of needing a
    stateful iterator, they can compare the return address pointer to find
    the right ret_stack entry.
    
    Note that an array of 50 ftrace_ret_stack structs is allocated for every
    task.  So when an arch implements this, it will add either 200 or 400
    bytes of memory usage per task (depending on whether it's a 32-bit or
    64-bit platform).
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Byungchul Park <byungchul.park@lge.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Nilay Vaish <nilayvaish@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/a95cfcc39e8f26b89a430c56926af0bb217bc0a1.1471607358.git.jpoimboe@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 4ad9ccc60e38..483e02a50d37 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -798,6 +798,9 @@ struct ftrace_ret_stack {
 #ifdef HAVE_FUNCTION_GRAPH_FP_TEST
 	unsigned long fp;
 #endif
+#ifdef HAVE_FUNCTION_GRAPH_RET_ADDR_PTR
+	unsigned long *retp;
+#endif
 };
 
 /*
@@ -809,7 +812,7 @@ extern void return_to_handler(void);
 
 extern int
 ftrace_push_return_trace(unsigned long ret, unsigned long func, int *depth,
-			 unsigned long frame_pointer);
+			 unsigned long frame_pointer, unsigned long *retp);
 
 /*
  * Sometimes we don't want to trace a function with the function

commit daa460a88c09b26b68e8b017de589c217e901afb
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Fri Aug 19 06:52:56 2016 -0500

    ftrace: Only allocate the ret_stack 'fp' field when needed
    
    This saves some memory when HAVE_FUNCTION_GRAPH_FP_TEST isn't defined.
    On x86_64 with newer versions of gcc which have -mfentry, it saves 400
    bytes per task.
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Byungchul Park <byungchul.park@lge.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Nilay Vaish <nilayvaish@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/5c7747d9ea7b5cb47ef0a8ce8a6cea6bf7aa94bf.1471607358.git.jpoimboe@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 7d565afe35d2..4ad9ccc60e38 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -795,7 +795,9 @@ struct ftrace_ret_stack {
 	unsigned long func;
 	unsigned long long calltime;
 	unsigned long long subtime;
+#ifdef HAVE_FUNCTION_GRAPH_FP_TEST
 	unsigned long fp;
+#endif
 };
 
 /*

commit a4a551b8f1d4c4ebffd0f49dfef44df3128546f8
Author: Namhyung Kim <namhyung@kernel.org>
Date:   Wed Jun 29 19:56:48 2016 +0900

    ftrace: Reduce size of function graph entries
    
    Currently ftrace_graph_ent{,_entry} and ftrace_graph_ret{,_entry} struct
    can have padding bytes at the end due to alignment in 64-bit data type.
    As these data are recorded so frequently, those paddings waste
    non-negligible space.  As the ring buffer maintains alignment properly
    for each architecture, just to remove the extra padding using 'packed'
    attribute.
    
      ftrace_graph_ent_entry:  24 -> 20
      ftrace_graph_ret_entry:  48 -> 44
    
    Also I moved the 'overrun' field in struct ftrace_graph_ret to minimize
    the padding in the middle.
    
    Tested on x86_64 only.
    
    Link: http://lkml.kernel.org/r/1467197808-13578-1-git-send-email-namhyung@kernel.org
    
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: linux-arch@vger.kernel.org
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 66a36a815f0a..7d565afe35d2 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -754,23 +754,27 @@ static inline void ftrace_init(void) { }
 
 /*
  * Structure that defines an entry function trace.
+ * It's already packed but the attribute "packed" is needed
+ * to remove extra padding at the end.
  */
 struct ftrace_graph_ent {
 	unsigned long func; /* Current function */
 	int depth;
-};
+} __packed;
 
 /*
  * Structure that defines a return function trace.
+ * It's already packed but the attribute "packed" is needed
+ * to remove extra padding at the end.
  */
 struct ftrace_graph_ret {
 	unsigned long func; /* Current function */
-	unsigned long long calltime;
-	unsigned long long rettime;
 	/* Number of functions that overran the depth limit for current task */
 	unsigned long overrun;
+	unsigned long long calltime;
+	unsigned long long rettime;
 	int depth;
-};
+} __packed;
 
 /* Type of the callback handlers for tracing function graph*/
 typedef void (*trace_func_graph_ret_t)(struct ftrace_graph_ret *); /* return */

commit 0b86c75db6e7f68c22ac5d0dae0f551c4897cdf5
Merge: 16bf8348055f be69f70e6395
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 17 17:11:27 2016 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/livepatching
    
    Pull livepatching updates from Jiri Kosina:
    
     - remove of our own implementation of architecture-specific relocation
       code and leveraging existing code in the module loader to perform
       arch-dependent work, from Jessica Yu.
    
       The relevant patches have been acked by Rusty (for module.c) and
       Heiko (for s390).
    
     - live patching support for ppc64le, which is a joint work of Michael
       Ellerman and Torsten Duwe.  This is coming from topic branch that is
       share between livepatching.git and ppc tree.
    
     - addition of livepatching documentation from Petr Mladek
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/livepatching:
      livepatch: make object/func-walking helpers more robust
      livepatch: Add some basic livepatch documentation
      powerpc/livepatch: Add live patching support on ppc64le
      powerpc/livepatch: Add livepatch stack to struct thread_info
      powerpc/livepatch: Add livepatch header
      livepatch: Allow architectures to specify an alternate ftrace location
      ftrace: Make ftrace_location_range() global
      livepatch: robustify klp_register_patch() API error checking
      Documentation: livepatch: outline Elf format and requirements for patch modules
      livepatch: reuse module loader code to write relocations
      module: s390: keep mod_arch_specific for livepatch modules
      module: preserve Elf information for livepatch modules
      Elf: add livepatch-specific Elf constants

commit 4d4fb97a62105c07dcccd350c391a65f576726c4
Merge: 61bf12d3304d 85baa095497f
Author: Jiri Kosina <jkosina@suse.cz>
Date:   Fri Apr 15 11:31:51 2016 +0200

    Merge branch 'topic/livepatch' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux into for-4.7/livepatching-ppc64le
    
    Pull livepatching support for ppc64 architecture from Michael Ellerman.
    
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

commit 04cf31a759ef575f750a63777cee95500e410994
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Thu Mar 24 22:04:01 2016 +1100

    ftrace: Make ftrace_location_range() global
    
    In order to support live patching on powerpc we would like to call
    ftrace_location_range(), so make it global.
    
    Signed-off-by: Torsten Duwe <duwe@suse.de>
    Signed-off-by: Balbir Singh <bsingharora@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 81de7123959d..3481a8e405f9 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -455,6 +455,7 @@ int ftrace_update_record(struct dyn_ftrace *rec, int enable);
 int ftrace_test_record(struct dyn_ftrace *rec, int enable);
 void ftrace_run_stop_machine(int command);
 unsigned long ftrace_location(unsigned long ip);
+unsigned long ftrace_location_range(unsigned long start, unsigned long end);
 unsigned long ftrace_get_addr_new(struct dyn_ftrace *rec);
 unsigned long ftrace_get_addr_curr(struct dyn_ftrace *rec);
 

commit be7635e7287e0e8013af3c89a6354a9e0182594c
Author: Alexander Potapenko <glider@google.com>
Date:   Fri Mar 25 14:22:05 2016 -0700

    arch, ftrace: for KASAN put hard/soft IRQ entries into separate sections
    
    KASAN needs to know whether the allocation happens in an IRQ handler.
    This lets us strip everything below the IRQ entry point to reduce the
    number of unique stack traces needed to be stored.
    
    Move the definition of __irq_entry to <linux/interrupt.h> so that the
    users don't need to pull in <linux/ftrace.h>.  Also introduce the
    __softirq_entry macro which is similar to __irq_entry, but puts the
    corresponding functions to the .softirqentry.text section.
    
    Signed-off-by: Alexander Potapenko <glider@google.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Andrey Konovalov <adech.fo@gmail.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Andrey Ryabinin <ryabinin.a.a@gmail.com>
    Cc: Konstantin Serebryany <kcc@google.com>
    Cc: Dmitry Chernenkov <dmitryc@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 6d9df3f7e334..dea12a6e413b 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -811,16 +811,6 @@ ftrace_push_return_trace(unsigned long ret, unsigned long func, int *depth,
  */
 #define __notrace_funcgraph		notrace
 
-/*
- * We want to which function is an entrypoint of a hardirq.
- * That will help us to put a signal on output.
- */
-#define __irq_entry		 __attribute__((__section__(".irqentry.text")))
-
-/* Limits of hardirq entrypoints */
-extern char __irqentry_text_start[];
-extern char __irqentry_text_end[];
-
 #define FTRACE_NOTRACE_DEPTH 65536
 #define FTRACE_RETFUNC_DEPTH 50
 #define FTRACE_RETSTACK_ALLOC_SIZE 32
@@ -857,7 +847,6 @@ static inline void unpause_graph_tracing(void)
 #else /* !CONFIG_FUNCTION_GRAPH_TRACER */
 
 #define __notrace_funcgraph
-#define __irq_entry
 #define INIT_FTRACE_GRAPH
 
 static inline void ftrace_graph_init_task(struct task_struct *t) { }

commit f904f58263e1df5f70feb8b283f4bbe662847334
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Fri Feb 26 14:54:56 2016 +0100

    sched/debug: Fix preempt_disable_ip recording for preempt_disable()
    
    The preempt_disable() invokes preempt_count_add() which saves the caller
    in ->preempt_disable_ip. It uses CALLER_ADDR1 which does not look for
    its caller but for the parent of the caller. Which means we get the correct
    caller for something like spin_lock() unless the architectures inlines
    those invocations. It is always wrong for preempt_disable() or
    local_bh_disable().
    
    This patch makes the function get_lock_parent_ip() which tries
    CALLER_ADDR0,1,2 if the former is a locking function.
    This seems to record the preempt_disable() caller properly for
    preempt_disable() itself as well as for get_cpu_var() or
    local_bh_disable().
    
    Steven asked for the get_parent_ip() -> get_lock_parent_ip() rename.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20160226135456.GB18244@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index c2b340e23f62..6d9df3f7e334 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -713,6 +713,18 @@ static inline void __ftrace_enabled_restore(int enabled)
 #define CALLER_ADDR5 ((unsigned long)ftrace_return_address(5))
 #define CALLER_ADDR6 ((unsigned long)ftrace_return_address(6))
 
+static inline unsigned long get_lock_parent_ip(void)
+{
+	unsigned long addr = CALLER_ADDR0;
+
+	if (!in_lock_functions(addr))
+		return addr;
+	addr = CALLER_ADDR1;
+	if (!in_lock_functions(addr))
+		return addr;
+	return CALLER_ADDR2;
+}
+
 #ifdef CONFIG_IRQSOFF_TRACER
   extern void time_hardirqs_on(unsigned long a0, unsigned long a1);
   extern void time_hardirqs_off(unsigned long a0, unsigned long a1);

commit 705d43dbe10d6e213a75187ac92b61f9bd00af0b
Merge: dd8fc10e6061 7dcd182bec27
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Feb 18 16:34:15 2016 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/livepatching
    
    Pull livepatching fixes from Jiri Kosina:
    
     - regression (from 4.4) fix for ordering issue, introduced by an
       earlier ftrace change, that broke live patching of modules.
    
       The fix replaces the ftrace module notifier by direct call in order
       to make the ordering guaranteed and well-defined.  The patch, from
       Jessica Yu, has been acked both by Steven and Rusty
    
     - error message fix from Miroslav Benes
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/livepatching:
      ftrace/module: remove ftrace module notifier
      livepatch: change the error message in asm/livepatch.h header files

commit 7dcd182bec271ab341b05b66b6006995795fc0e7
Author: Jessica Yu <jeyu@redhat.com>
Date:   Tue Feb 16 17:32:33 2016 -0500

    ftrace/module: remove ftrace module notifier
    
    Remove the ftrace module notifier in favor of directly calling
    ftrace_module_enable() and ftrace_release_mod() in the module loader.
    Hard-coding the function calls directly in the module loader removes
    dependence on the module notifier call chain and provides better
    visibility and control over what gets called when, which is important
    to kernel utilities such as livepatch.
    
    This fixes a notifier ordering issue in which the ftrace module notifier
    (and hence ftrace_module_enable()) for coming modules was being called
    after klp_module_notify(), which caused livepatch modules to initialize
    incorrectly. This patch removes dependence on the module notifier call
    chain in favor of hard coding the corresponding function calls in the
    module loader. This ensures that ftrace and livepatch code get called in
    the correct order on patch module load and unload.
    
    Fixes: 5156dca34a3e ("ftrace: Fix the race between ftrace and insmod")
    Signed-off-by: Jessica Yu <jeyu@redhat.com>
    Reviewed-by: Steven Rostedt <rostedt@goodmis.org>
    Reviewed-by: Petr Mladek <pmladek@suse.cz>
    Acked-by: Rusty Russell <rusty@rustcorp.com.au>
    Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Reviewed-by: Miroslav Benes <mbenes@suse.cz>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 0639dcc98195..76dc15e171eb 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -604,6 +604,7 @@ extern int ftrace_arch_read_dyn_info(char *buf, int size);
 
 extern int skip_trace(unsigned long ip);
 extern void ftrace_module_init(struct module *mod);
+extern void ftrace_module_enable(struct module *mod);
 extern void ftrace_release_mod(struct module *mod);
 
 extern void ftrace_disable_daemon(void);
@@ -613,8 +614,9 @@ static inline int skip_trace(unsigned long ip) { return 0; }
 static inline int ftrace_force_update(void) { return 0; }
 static inline void ftrace_disable_daemon(void) { }
 static inline void ftrace_enable_daemon(void) { }
-static inline void ftrace_release_mod(struct module *mod) {}
-static inline void ftrace_module_init(struct module *mod) {}
+static inline void ftrace_module_init(struct module *mod) { }
+static inline void ftrace_module_enable(struct module *mod) { }
+static inline void ftrace_release_mod(struct module *mod) { }
 static inline __init int register_ftrace_command(struct ftrace_func_command *cmd)
 {
 	return -EINVAL;

commit 26cd83670f2f5a3d5b5514a1f7d96567cdb9558b
Merge: 03c21cb775a3 7fd13615992a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jan 28 17:00:50 2016 -0800

    Merge tag 'trace-v4.5-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull minor tracing fixes from Steven Rostedt:
     "This includes three minor fixes, mostly due to cut-and-paste issues.
    
      The first is a cut and paste issue that changed the amount of stack to
      skip when tracing a stack dump from 0 to 6, which basically made the
      stack disappear for small stack traces.
    
      The second fix is just removing an unused field in a struct that is no
      longer used, and currently just wastes space.
    
      The third is another cut-and-paste fix that had a tracepoint recording
      the wrong field (it was recording the previous field a second time)"
    
    * tag 'trace-v4.5-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace:
      tracing/dma-buf/fence: Fix timeline str value on fence_annotate_wait_on
      ftrace: Remove unused nr_trampolines var
      tracing: Fix stacktrace skip depth in trace_buffer_unlock_commit_regs()

commit b7522056ec1a62a5f3246627e12ef48e6274c21c
Author: Dmitry Safonov <0x7f454c46@gmail.com>
Date:   Wed Jan 13 18:39:58 2016 +0300

    ftrace: Remove unused nr_trampolines var
    
    It's not needed & not used since introducing old_hash: commit fef5aeeee9e371
    ("ftrace: Replace tramp_hash with old_*_hash to save space").
    
    Link: http://lkml.kernel.org/r/1452699598-27610-1-git-send-email-0x7f454c46@gmail.com
    
    Signed-off-by: Dmitry Safonov <0x7f454c46@gmail.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 60048c50404e..65460ad93c7b 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -164,7 +164,6 @@ struct ftrace_ops {
 	ftrace_func_t			saved_func;
 	int __percpu			*disabled;
 #ifdef CONFIG_DYNAMIC_FTRACE
-	int				nr_trampolines;
 	struct ftrace_ops_hash		local_hash;
 	struct ftrace_ops_hash		*func_hash;
 	struct ftrace_ops_hash		old_hash;

commit c17488d06666153a14dd3f21bd10eba58383f6c1
Merge: 34a9304a96d6 5156dca34a3e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jan 12 20:04:15 2016 -0800

    Merge tag 'trace-v4.5' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull tracing updates from Steven Rostedt:
     "Not much new with tracing for this release.  Mostly just clean ups and
      minor fixes.
    
      Here's what else is new:
    
       - A new TRACE_EVENT_FN_COND macro, combining both _FN and _COND for
         those that want both.
    
       - New selftest to test the instance create and delete
    
       - Better debug output when ftrace fails"
    
    * tag 'trace-v4.5' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace: (24 commits)
      ftrace: Fix the race between ftrace and insmod
      ftrace: Add infrastructure for delayed enabling of module functions
      x86: ftrace: Fix the comments for ftrace_modify_code_direct()
      tracing: Fix comment to use tracing_on over tracing_enable
      metag: ftrace: Fix the comments for ftrace_modify_code
      sh: ftrace: Fix the comments for ftrace_modify_code()
      ia64: ftrace: Fix the comments for ftrace_modify_code()
      ftrace: Clean up ftrace_module_init() code
      ftrace: Join functions ftrace_module_init() and ftrace_init_module()
      tracing: Introduce TRACE_EVENT_FN_COND macro
      tracing: Use seq_buf_used() in seq_buf_to_user() instead of len
      bpf: Constify bpf_verifier_ops structure
      ftrace: Have ftrace_ops_get_func() handle RCU and PER_CPU flags too
      ftrace: Remove use of control list and ops
      ftrace: Fix output of enabled_functions for showing tramp
      ftrace: Fix a typo in comment
      ftrace: Show all tramps registered to a record on ftrace_bug()
      ftrace: Add variable ftrace_expected for archs to show expected code
      ftrace: Add new type to distinguish what kind of ftrace_bug()
      tracing: Update cond flag when enabling or disabling a trigger
      ...

commit b7ffffbb46f205e7727a18bcc7a46c3c2b534f7c
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Thu Jan 7 15:40:01 2016 -0500

    ftrace: Add infrastructure for delayed enabling of module functions
    
    Qiu Peiyang pointed out that there's a race when enabling function tracing
    and loading a module. In order to make the modifications of converting nops
    in the prologue of functions into callbacks, the text needs to be converted
    from read-only to read-write. When enabling function tracing, the text
    permission is updated, the functions are modified, and then they are put
    back.
    
    When loading a module, the updates to convert function calls to mcount is
    done before the module text is set to read-only. But after it is done, the
    module text is visible by the function tracer. Thus we have the following
    race:
    
            CPU 0                   CPU 1
            -----                   -----
       start function tracing
       set text to read-write
                                 load_module
                                 add functions to ftrace
                                 set module text read-only
    
       update all functions to callbacks
       modify module functions too
       < Can't it's read-only >
    
    When this happens, ftrace detects the issue and disables itself till the
    next reboot.
    
    To fix this, a new DISABLED flag is added for ftrace records, which all
    module functions get when they are added. Then later, after the module code
    is all set, the records will have the DISABLED flag cleared, and they will
    be enabled if any callback wants all functions to be traced.
    
    Note, this doesn't add the delay to later. It simply changes the
    ftrace_module_init() to do both the setting of DISABLED records, and then
    immediately calls the enable code. This helps with testing this new code as
    it has the same behavior as previously. Another change will come after this
    to have the ftrace_module_enable() called after the text is set to
    read-only.
    
    Cc: Qiu Peiyang <peiyangx.qiu@intel.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 4736a826baf5..660e7c698f3b 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -357,6 +357,7 @@ bool is_ftrace_trampoline(unsigned long addr);
  *  REGS    - the record wants the function to save regs
  *  REGS_EN - the function is set up to save regs.
  *  IPMODIFY - the record allows for the IP address to be changed.
+ *  DISABLED - the record is not ready to be touched yet
  *
  * When a new ftrace_ops is registered and wants a function to save
  * pt_regs, the rec->flag REGS is set. When the function has been
@@ -371,10 +372,11 @@ enum {
 	FTRACE_FL_TRAMP		= (1UL << 28),
 	FTRACE_FL_TRAMP_EN	= (1UL << 27),
 	FTRACE_FL_IPMODIFY	= (1UL << 26),
+	FTRACE_FL_DISABLED	= (1UL << 25),
 };
 
-#define FTRACE_REF_MAX_SHIFT	26
-#define FTRACE_FL_BITS		6
+#define FTRACE_REF_MAX_SHIFT	25
+#define FTRACE_FL_BITS		7
 #define FTRACE_FL_MASKED_BITS	((1UL << FTRACE_FL_BITS) - 1)
 #define FTRACE_FL_MASK		(FTRACE_FL_MASKED_BITS << FTRACE_REF_MAX_SHIFT)
 #define FTRACE_REF_MAX		((1UL << FTRACE_REF_MAX_SHIFT) - 1)

commit 049fb9bd416077b3622d317a45796be4f2431df3
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Jan 5 20:32:47 2016 -0500

    ftrace/module: Call clean up function when module init fails early
    
    If the module init code fails after calling ftrace_module_init() and before
    calling do_init_module(), we can suffer from a memory leak. This is because
    ftrace_module_init() allocates pages to store the locations that ftrace
    hooks are placed in the module text. If do_init_module() fails, it still
    calls the MODULE_GOING notifiers which will tell ftrace to do a clean up of
    the pages it allocated for the module. But if load_module() fails before
    then, the pages allocated by ftrace_module_init() will never be freed.
    
    Call ftrace_release_mod() on the module if load_module() fails before
    getting to do_init_module().
    
    Link: http://lkml.kernel.org/r/567CEA31.1070507@intel.com
    
    Reported-by: "Qiu, PeiyangX" <peiyangx.qiu@intel.com>
    Fixes: a949ae560a511 "ftrace/module: Hardcode ftrace_module_init() call into load_module()"
    Cc: stable@vger.kernel.org # v2.6.38+
    Acked-by: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index eae6548efbf0..60048c50404e 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -586,6 +586,7 @@ extern int ftrace_arch_read_dyn_info(char *buf, int size);
 
 extern int skip_trace(unsigned long ip);
 extern void ftrace_module_init(struct module *mod);
+extern void ftrace_release_mod(struct module *mod);
 
 extern void ftrace_disable_daemon(void);
 extern void ftrace_enable_daemon(void);

commit ba27f2bc731135a0396f3968bdddb54f3bc72e64
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Mon Nov 30 17:23:39 2015 -0500

    ftrace: Remove use of control list and ops
    
    Currently perf has its own list function within the ftrace infrastructure
    that seems to be used only to allow for it to have per-cpu disabling as well
    as a check to make sure that it's not called while RCU is not watching. It
    uses something called the "control_ops" which is used to iterate over ops
    under it with the control_list_func().
    
    The problem is that this control_ops and control_list_func unnecessarily
    complicates the code. By replacing FTRACE_OPS_FL_CONTROL with two new flags
    (FTRACE_OPS_FL_RCU and FTRACE_OPS_FL_PER_CPU) we can remove all the code
    that is special with the control ops and add the needed checks within the
    generic ftrace_list_func().
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 134f8d45b35b..4736a826baf5 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -76,8 +76,8 @@ ftrace_func_t ftrace_ops_get_func(struct ftrace_ops *ops);
  * ENABLED - set/unset when ftrace_ops is registered/unregistered
  * DYNAMIC - set when ftrace_ops is registered to denote dynamically
  *           allocated ftrace_ops which need special care
- * CONTROL - set manualy by ftrace_ops user to denote the ftrace_ops
- *           could be controled by following calls:
+ * PER_CPU - set manualy by ftrace_ops user to denote the ftrace_ops
+ *           could be controlled by following calls:
  *             ftrace_function_local_enable
  *             ftrace_function_local_disable
  * SAVE_REGS - The ftrace_ops wants regs saved at each function called
@@ -121,7 +121,7 @@ ftrace_func_t ftrace_ops_get_func(struct ftrace_ops *ops);
 enum {
 	FTRACE_OPS_FL_ENABLED			= 1 << 0,
 	FTRACE_OPS_FL_DYNAMIC			= 1 << 1,
-	FTRACE_OPS_FL_CONTROL			= 1 << 2,
+	FTRACE_OPS_FL_PER_CPU			= 1 << 2,
 	FTRACE_OPS_FL_SAVE_REGS			= 1 << 3,
 	FTRACE_OPS_FL_SAVE_REGS_IF_SUPPORTED	= 1 << 4,
 	FTRACE_OPS_FL_RECURSION_SAFE		= 1 << 5,
@@ -134,6 +134,7 @@ enum {
 	FTRACE_OPS_FL_ALLOC_TRAMP		= 1 << 12,
 	FTRACE_OPS_FL_IPMODIFY			= 1 << 13,
 	FTRACE_OPS_FL_PID			= 1 << 14,
+	FTRACE_OPS_FL_RCU			= 1 << 15,
 };
 
 #ifdef CONFIG_DYNAMIC_FTRACE
@@ -146,11 +147,11 @@ struct ftrace_ops_hash {
 #endif
 
 /*
- * Note, ftrace_ops can be referenced outside of RCU protection.
- * (Although, for perf, the control ops prevent that). If ftrace_ops is
- * allocated and not part of kernel core data, the unregistering of it will
- * perform a scheduling on all CPUs to make sure that there are no more users.
- * Depending on the load of the system that may take a bit of time.
+ * Note, ftrace_ops can be referenced outside of RCU protection, unless
+ * the RCU flag is set. If ftrace_ops is allocated and not part of kernel
+ * core data, the unregistering of it will perform a scheduling on all CPUs
+ * to make sure that there are no more users. Depending on the load of the
+ * system that may take a bit of time.
  *
  * Any private data added must also take care not to be freed and if private
  * data is added to a ftrace_ops that is in core code, the user of the
@@ -196,34 +197,34 @@ int unregister_ftrace_function(struct ftrace_ops *ops);
 void clear_ftrace_function(void);
 
 /**
- * ftrace_function_local_enable - enable controlled ftrace_ops on current cpu
+ * ftrace_function_local_enable - enable ftrace_ops on current cpu
  *
  * This function enables tracing on current cpu by decreasing
  * the per cpu control variable.
  * It must be called with preemption disabled and only on ftrace_ops
- * registered with FTRACE_OPS_FL_CONTROL. If called without preemption
+ * registered with FTRACE_OPS_FL_PER_CPU. If called without preemption
  * disabled, this_cpu_ptr will complain when CONFIG_DEBUG_PREEMPT is enabled.
  */
 static inline void ftrace_function_local_enable(struct ftrace_ops *ops)
 {
-	if (WARN_ON_ONCE(!(ops->flags & FTRACE_OPS_FL_CONTROL)))
+	if (WARN_ON_ONCE(!(ops->flags & FTRACE_OPS_FL_PER_CPU)))
 		return;
 
 	(*this_cpu_ptr(ops->disabled))--;
 }
 
 /**
- * ftrace_function_local_disable - enable controlled ftrace_ops on current cpu
+ * ftrace_function_local_disable - disable ftrace_ops on current cpu
  *
- * This function enables tracing on current cpu by decreasing
+ * This function disables tracing on current cpu by increasing
  * the per cpu control variable.
  * It must be called with preemption disabled and only on ftrace_ops
- * registered with FTRACE_OPS_FL_CONTROL. If called without preemption
+ * registered with FTRACE_OPS_FL_PER_CPU. If called without preemption
  * disabled, this_cpu_ptr will complain when CONFIG_DEBUG_PREEMPT is enabled.
  */
 static inline void ftrace_function_local_disable(struct ftrace_ops *ops)
 {
-	if (WARN_ON_ONCE(!(ops->flags & FTRACE_OPS_FL_CONTROL)))
+	if (WARN_ON_ONCE(!(ops->flags & FTRACE_OPS_FL_PER_CPU)))
 		return;
 
 	(*this_cpu_ptr(ops->disabled))++;
@@ -235,12 +236,12 @@ static inline void ftrace_function_local_disable(struct ftrace_ops *ops)
  *
  * This function returns value of ftrace_ops::disabled on current cpu.
  * It must be called with preemption disabled and only on ftrace_ops
- * registered with FTRACE_OPS_FL_CONTROL. If called without preemption
+ * registered with FTRACE_OPS_FL_PER_CPU. If called without preemption
  * disabled, this_cpu_ptr will complain when CONFIG_DEBUG_PREEMPT is enabled.
  */
 static inline int ftrace_function_local_disabled(struct ftrace_ops *ops)
 {
-	WARN_ON_ONCE(!(ops->flags & FTRACE_OPS_FL_CONTROL));
+	WARN_ON_ONCE(!(ops->flags & FTRACE_OPS_FL_PER_CPU));
 	return *this_cpu_ptr(ops->disabled);
 }
 

commit b05086c77a162dd8ef79606cb4723f1fc1448bb1
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed Nov 25 14:13:11 2015 -0500

    ftrace: Add variable ftrace_expected for archs to show expected code
    
    When an anomaly is found while modifying function code, ftrace_bug() is
    called which disables the function tracing infrastructure and reports
    information about what failed. If the code that is to be replaced does not
    match what is expected, then actual code is shown. Currently there is no
    arch generic way to show what was expected.
    
    Add a new variable pointer calld ftrace_expected that the arch code can set
    to point to what it expected so that ftrace_bug() can report the actual text
    as well as the text that was expected to be there.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 870c8eea38cd..134f8d45b35b 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -305,6 +305,12 @@ enum ftrace_bug_type {
 };
 extern enum ftrace_bug_type ftrace_bug_type;
 
+/*
+ * Archs can set this to point to a variable that holds the value that was
+ * expected at the call site before calling ftrace_bug().
+ */
+extern const void *ftrace_expected;
+
 void ftrace_bug(int err, struct dyn_ftrace *rec);
 
 struct seq_file;

commit 02a392a0439ffdc62b4d8f17bd18d68736b166a9
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed Nov 25 12:50:47 2015 -0500

    ftrace: Add new type to distinguish what kind of ftrace_bug()
    
    The ftrace function hook utility has several internal checks to make sure
    that whatever it modifies is exactly what it expects to be modifying. This
    is essential as modifying running code can be extremely dangerous to the
    system.
    
    When an anomaly is detected, ftrace_bug() is called which sends a splat to
    the console and disables function tracing. There's some extra information
    that is printed to help diagnose the issue.
    
    One thing that is missing though is output of what ftrace was doing at the
    time of the crash. Was it updating a call site or perhaps converting a call
    site to a nop? A new global enum variable is created to state what ftrace
    was doing at the time of the anomaly, and this is reported in ftrace_bug().
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index eae6548efbf0..870c8eea38cd 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -296,6 +296,15 @@ int ftrace_arch_code_modify_post_process(void);
 
 struct dyn_ftrace;
 
+enum ftrace_bug_type {
+	FTRACE_BUG_UNKNOWN,
+	FTRACE_BUG_INIT,
+	FTRACE_BUG_NOP,
+	FTRACE_BUG_CALL,
+	FTRACE_BUG_UPDATE,
+};
+extern enum ftrace_bug_type ftrace_bug_type;
+
 void ftrace_bug(int err, struct dyn_ftrace *rec);
 
 struct seq_file;

commit d332736df0c277905de06311ae084e2c76580a3f
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Nov 3 14:50:15 2015 -0500

    tracing: Rename max_stack_lock to stack_trace_max_lock
    
    Now that max_stack_lock is a global variable, it requires a naming
    convention that is unlikely to collide. Rename it to the same naming
    convention that the other stack_trace variables have.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index b4c92ab9e08b..eae6548efbf0 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -271,7 +271,7 @@ struct stack_trace;
 extern unsigned stack_trace_index[];
 extern struct stack_trace stack_trace_max;
 extern unsigned long stack_trace_max_size;
-extern arch_spinlock_t max_stack_lock;
+extern arch_spinlock_t stack_trace_max_lock;
 
 extern int stack_tracer_enabled;
 void stack_trace_print(void);

commit bb99d8ccec7f83a2730a29d1ae7eee5ffa446a9e
Author: AKASHI Takahiro <takahiro.akashi@linaro.org>
Date:   Fri Oct 30 14:25:39 2015 +0900

    tracing: Allow arch-specific stack tracer
    
    A stack frame may be used in a different way depending on cpu architecture.
    Thus it is not always appropriate to slurp the stack contents, as current
    check_stack() does, in order to calcurate a stack index (height) at a given
    function call. At least not on arm64.
    In addition, there is a possibility that we will mistakenly detect a stale
    stack frame which has not been overwritten.
    
    This patch makes check_stack() a weak function so as to later implement
    arch-specific version.
    
    Link: http://lkml.kernel.org/r/1446182741-31019-5-git-send-email-takahiro.akashi@linaro.org
    
    Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 6cd8c0ee4b6f..b4c92ab9e08b 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -263,7 +263,18 @@ static inline void ftrace_kill(void) { }
 #endif /* CONFIG_FUNCTION_TRACER */
 
 #ifdef CONFIG_STACK_TRACER
+
+#define STACK_TRACE_ENTRIES 500
+
+struct stack_trace;
+
+extern unsigned stack_trace_index[];
+extern struct stack_trace stack_trace_max;
+extern unsigned long stack_trace_max_size;
+extern arch_spinlock_t max_stack_lock;
+
 extern int stack_tracer_enabled;
+void stack_trace_print(void);
 int
 stack_trace_sysctl(struct ctl_table *table, int write,
 		   void __user *buffer, size_t *lenp,

commit e3eea1404f5ff7a2ceb7b5e7ba412a6fd94f2935
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Fri Jul 24 10:38:12 2015 -0400

    ftrace: Fix breakage of set_ftrace_pid
    
    Commit 4104d326b670 ("ftrace: Remove global function list and call function
    directly") simplified the ftrace code by removing the global_ops list with a
    new design. But this cleanup also broke the filtering of PIDs that are added
    to the set_ftrace_pid file.
    
    Add back the proper hooks to have pid filtering working once again.
    
    Cc: stable@vger.kernel.org # 3.16+
    Reported-by: Matt Fleming <matt@console-pimps.org>
    Reported-by: Richard Weinberger <richard.weinberger@gmail.com>
    Tested-by: Matt Fleming <matt@console-pimps.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 1da602982cf9..6cd8c0ee4b6f 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -116,6 +116,7 @@ ftrace_func_t ftrace_ops_get_func(struct ftrace_ops *ops);
  *            SAVE_REGS. If another ops with this flag set is already registered
  *            for any of the functions that this ops will be registered for, then
  *            this ops will fail to register or set_filter_ip.
+ * PID     - Is affected by set_ftrace_pid (allows filtering on those pids)
  */
 enum {
 	FTRACE_OPS_FL_ENABLED			= 1 << 0,
@@ -132,6 +133,7 @@ enum {
 	FTRACE_OPS_FL_MODIFYING			= 1 << 11,
 	FTRACE_OPS_FL_ALLOC_TRAMP		= 1 << 12,
 	FTRACE_OPS_FL_IPMODIFY			= 1 << 13,
+	FTRACE_OPS_FL_PID			= 1 << 14,
 };
 
 #ifdef CONFIG_DYNAMIC_FTRACE
@@ -159,6 +161,7 @@ struct ftrace_ops {
 	struct ftrace_ops		*next;
 	unsigned long			flags;
 	void				*private;
+	ftrace_func_t			saved_func;
 	int __percpu			*disabled;
 #ifdef CONFIG_DYNAMIC_FTRACE
 	int				nr_trampolines;

commit 0daa2302968c13b657118d6ac92471f8fd2f3f28
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Fri Dec 12 22:27:10 2014 -0500

    tracing: Add tp_printk cmdline to have tracepoints go to printk()
    
    Add the kernel command line tp_printk option that will have tracepoints
    that are active sent to printk() as well as to the trace buffer.
    
    Passing "tp_printk" will activate this. To turn it off, the sysctl
    /proc/sys/kernel/tracepoint_printk can have '0' echoed into it. Note,
    this only works if the cmdline option is used. Echoing 1 into the sysctl
    file without the cmdline option will have no affect.
    
    Note, this is a dangerous option. Having high frequency tracepoints send
    their data to printk() can possibly cause a live lock. This is another
    reason why this is only active if the command line option is used.
    
    Link: http://lkml.kernel.org/r/alpine.DEB.2.11.1412121539300.16494@nanos
    
    Suggested-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index f4bc14b7d444..1da602982cf9 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -879,6 +879,7 @@ static inline int test_tsk_trace_graph(struct task_struct *tsk)
 enum ftrace_dump_mode;
 
 extern enum ftrace_dump_mode ftrace_dump_on_oops;
+extern int tracepoint_printk;
 
 extern void disable_trace_on_warning(void);
 extern int __disable_trace_on_warning;

commit 5f893b2639b21ffe6834b1aebba392c37d2b83f9
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Fri Dec 12 20:05:10 2014 -0500

    tracing: Move enabling tracepoints to just after rcu_init()
    
    Enabling tracepoints at boot up can be very useful. The tracepoint
    can be initialized right after RCU has been. There's no need to
    wait for the early_initcall() to be called. That's too late for some
    things that can use tracepoints for debugging. Move the logic to
    enable tracepoints out of the initcalls and into init/main.c to
    right after rcu_init().
    
    This also allows trace_printk() to be used early too.
    
    Link: http://lkml.kernel.org/r/alpine.DEB.2.11.1412121539300.16494@nanos
    Link: http://lkml.kernel.org/r/20141214164104.307127356@goodmis.org
    
    Reviewed-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Suggested-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index ed501953f0b2..f4bc14b7d444 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -39,6 +39,12 @@
 # define FTRACE_FORCE_LIST_FUNC 0
 #endif
 
+/* Main tracing buffer and events set up */
+#ifdef CONFIG_TRACING
+void trace_init(void);
+#else
+static inline void trace_init(void) { }
+#endif
 
 struct module;
 struct ftrace_hash;

commit f8b8be8a310a55856fd2c369dade08088d85df3b
Author: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
Date:   Fri Nov 21 05:25:16 2014 -0500

    ftrace, kprobes: Support IPMODIFY flag to find IP modify conflict
    
    Introduce FTRACE_OPS_FL_IPMODIFY to avoid conflict among
    ftrace users who may modify regs->ip to change the execution
    path. If two or more users modify the regs->ip on the same
    function entry, one of them will be broken. So they must add
    IPMODIFY flag and make sure that ftrace_set_filter_ip() succeeds.
    
    Note that ftrace doesn't allow ftrace_ops which has IPMODIFY
    flag to have notrace hash, and the ftrace_ops must have a
    filter hash (so that the ftrace_ops can hook only specific
    entries), because it strongly depends on the address and
    must be allowed for only few selected functions.
    
    Link: http://lkml.kernel.org/r/20141121102516.11844.27829.stgit@localhost.localdomain
    
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Seth Jennings <sjenning@redhat.com>
    Cc: Petr Mladek <pmladek@suse.cz>
    Cc: Vojtech Pavlik <vojtech@suse.cz>
    Cc: Miroslav Benes <mbenes@suse.cz>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    [ fixed up some of the comments ]
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 7b2616fa2472..ed501953f0b2 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -61,6 +61,11 @@ ftrace_func_t ftrace_ops_get_func(struct ftrace_ops *ops);
 /*
  * FTRACE_OPS_FL_* bits denote the state of ftrace_ops struct and are
  * set in the flags member.
+ * CONTROL, SAVE_REGS, SAVE_REGS_IF_SUPPORTED, RECURSION_SAFE, STUB and
+ * IPMODIFY are a kind of attribute flags which can be set only before
+ * registering the ftrace_ops, and can not be modified while registered.
+ * Changing those attribute flags after regsitering ftrace_ops will
+ * cause unexpected results.
  *
  * ENABLED - set/unset when ftrace_ops is registered/unregistered
  * DYNAMIC - set when ftrace_ops is registered to denote dynamically
@@ -101,6 +106,10 @@ ftrace_func_t ftrace_ops_get_func(struct ftrace_ops *ops);
  *            The ftrace_ops trampoline can be set by the ftrace users, and
  *            in such cases the arch must not modify it. Only the arch ftrace
  *            core code should set this flag.
+ * IPMODIFY - The ops can modify the IP register. This can only be set with
+ *            SAVE_REGS. If another ops with this flag set is already registered
+ *            for any of the functions that this ops will be registered for, then
+ *            this ops will fail to register or set_filter_ip.
  */
 enum {
 	FTRACE_OPS_FL_ENABLED			= 1 << 0,
@@ -116,6 +125,7 @@ enum {
 	FTRACE_OPS_FL_REMOVING			= 1 << 10,
 	FTRACE_OPS_FL_MODIFYING			= 1 << 11,
 	FTRACE_OPS_FL_ALLOC_TRAMP		= 1 << 12,
+	FTRACE_OPS_FL_IPMODIFY			= 1 << 13,
 };
 
 #ifdef CONFIG_DYNAMIC_FTRACE
@@ -310,6 +320,7 @@ bool is_ftrace_trampoline(unsigned long addr);
  *  ENABLED - the function is being traced
  *  REGS    - the record wants the function to save regs
  *  REGS_EN - the function is set up to save regs.
+ *  IPMODIFY - the record allows for the IP address to be changed.
  *
  * When a new ftrace_ops is registered and wants a function to save
  * pt_regs, the rec->flag REGS is set. When the function has been
@@ -323,10 +334,11 @@ enum {
 	FTRACE_FL_REGS_EN	= (1UL << 29),
 	FTRACE_FL_TRAMP		= (1UL << 28),
 	FTRACE_FL_TRAMP_EN	= (1UL << 27),
+	FTRACE_FL_IPMODIFY	= (1UL << 26),
 };
 
-#define FTRACE_REF_MAX_SHIFT	27
-#define FTRACE_FL_BITS		5
+#define FTRACE_REF_MAX_SHIFT	26
+#define FTRACE_FL_BITS		6
 #define FTRACE_FL_MASKED_BITS	((1UL << FTRACE_FL_BITS) - 1)
 #define FTRACE_FL_MASK		(FTRACE_FL_MASKED_BITS << FTRACE_REF_MAX_SHIFT)
 #define FTRACE_REF_MAX		((1UL << FTRACE_REF_MAX_SHIFT) - 1)

commit aec0be2d6e9f02dbef41ee54854c2e003e55c23e
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Nov 18 21:14:11 2014 -0500

    ftrace/x86/extable: Add is_ftrace_trampoline() function
    
    Stack traces that happen from function tracing check if the address
    on the stack is a __kernel_text_address(). That is, is the address
    kernel code. This calls core_kernel_text() which returns true
    if the address is part of the builtin kernel code. It also calls
    is_module_text_address() which returns true if the address belongs
    to module code.
    
    But what is missing is ftrace dynamically allocated trampolines.
    These trampolines are allocated for individual ftrace_ops that
    call the ftrace_ops callback functions directly. But if they do a
    stack trace, the code checking the stack wont detect them as they
    are neither core kernel code nor module address space.
    
    Adding another field to ftrace_ops that also stores the size of
    the trampoline assigned to it we can create a new function called
    is_ftrace_trampoline() that returns true if the address is a
    dynamically allocate ftrace trampoline. Note, it ignores trampolines
    that are not dynamically allocated as they will return true with
    the core_kernel_text() function.
    
    Link: http://lkml.kernel.org/r/20141119034829.497125839@goodmis.org
    
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 619e37cc17fd..7b2616fa2472 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -150,6 +150,7 @@ struct ftrace_ops {
 	struct ftrace_ops_hash		*func_hash;
 	struct ftrace_ops_hash		old_hash;
 	unsigned long			trampoline;
+	unsigned long			trampoline_size;
 #endif
 };
 
@@ -297,6 +298,8 @@ extern int ftrace_text_reserved(const void *start, const void *end);
 
 extern int ftrace_nr_registered_ops(void);
 
+bool is_ftrace_trampoline(unsigned long addr);
+
 /*
  * The dyn_ftrace record's flags field is split into two parts.
  * the first part which is '0-FTRACE_REF_MAX' is a counter of
@@ -596,6 +599,11 @@ static inline ssize_t ftrace_notrace_write(struct file *file, const char __user
 			     size_t cnt, loff_t *ppos) { return -ENODEV; }
 static inline int
 ftrace_regex_release(struct inode *inode, struct file *file) { return -ENODEV; }
+
+static inline bool is_ftrace_trampoline(unsigned long addr)
+{
+	return false;
+}
 #endif /* CONFIG_DYNAMIC_FTRACE */
 
 /* totally disable ftrace - can not re-enable after this */

commit 4fd3279b48605ae3ea509b9b2c02e46aa0975930
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Fri Oct 24 17:56:04 2014 -0400

    ftrace: Add more information to ftrace_bug() output
    
    With the introduction of the dynamic trampolines, it is useful that if
    things go wrong that ftrace_bug() produces more information about what
    the current state is. This can help debug issues that may arise.
    
    Ftrace has lots of checks to make sure that the state of the system it
    touchs is exactly what it expects it to be. When it detects an abnormality
    it calls ftrace_bug() and disables itself to prevent any further damage.
    It is crucial that ftrace_bug() produces sufficient information that
    can be used to debug the situation.
    
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Acked-by: Borislav Petkov <bp@suse.de>
    Tested-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Tested-by: Jiri Kosina <jkosina@suse.cz>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 06e3ca5a5083..619e37cc17fd 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -263,7 +263,9 @@ struct ftrace_func_command {
 int ftrace_arch_code_modify_prepare(void);
 int ftrace_arch_code_modify_post_process(void);
 
-void ftrace_bug(int err, unsigned long ip);
+struct dyn_ftrace;
+
+void ftrace_bug(int err, struct dyn_ftrace *rec);
 
 struct seq_file;
 

commit f3bea49115b21e0995abf41402ad2f4d9c69eda4
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed Jul 2 23:23:31 2014 -0400

    ftrace/x86: Add dynamic allocated trampoline for ftrace_ops
    
    The current method of handling multiple function callbacks is to register
    a list function callback that calls all the other callbacks based on
    their hash tables and compare it to the function that the callback was
    called on. But this is very inefficient.
    
    For example, if you are tracing all functions in the kernel and then
    add a kprobe to a function such that the kprobe uses ftrace, the
    mcount trampoline will switch from calling the function trace callback
    to calling the list callback that will iterate over all registered
    ftrace_ops (in this case, the function tracer and the kprobes callback).
    That means for every function being traced it checks the hash of the
    ftrace_ops for function tracing and kprobes, even though the kprobes
    is only set at a single function. The kprobes ftrace_ops is checked
    for every function being traced!
    
    Instead of calling the list function for functions that are only being
    traced by a single callback, we can call a dynamically allocated
    trampoline that calls the callback directly. The function graph tracer
    already uses a direct call trampoline when it is being traced by itself
    but it is not dynamically allocated. It's trampoline is static in the
    kernel core. The infrastructure that called the function graph trampoline
    can also be used to call a dynamically allocated one.
    
    For now, only ftrace_ops that are not dynamically allocated can have
    a trampoline. That is, users such as function tracer or stack tracer.
    kprobes and perf allocate their ftrace_ops, and until there's a safe
    way to free the trampoline, it can not be used. The dynamically allocated
    ftrace_ops may, although, use the trampoline if the kernel is not
    compiled with CONFIG_PREEMPT. But that will come later.
    
    Tested-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Tested-by: Jiri Kosina <jkosina@suse.cz>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 662697babd48..06e3ca5a5083 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -94,6 +94,13 @@ ftrace_func_t ftrace_ops_get_func(struct ftrace_ops *ops);
  * ADDING  - The ops is in the process of being added.
  * REMOVING - The ops is in the process of being removed.
  * MODIFYING - The ops is in the process of changing its filter functions.
+ * ALLOC_TRAMP - A dynamic trampoline was allocated by the core code.
+ *            The arch specific code sets this flag when it allocated a
+ *            trampoline. This lets the arch know that it can update the
+ *            trampoline in case the callback function changes.
+ *            The ftrace_ops trampoline can be set by the ftrace users, and
+ *            in such cases the arch must not modify it. Only the arch ftrace
+ *            core code should set this flag.
  */
 enum {
 	FTRACE_OPS_FL_ENABLED			= 1 << 0,
@@ -108,6 +115,7 @@ enum {
 	FTRACE_OPS_FL_ADDING			= 1 << 9,
 	FTRACE_OPS_FL_REMOVING			= 1 << 10,
 	FTRACE_OPS_FL_MODIFYING			= 1 << 11,
+	FTRACE_OPS_FL_ALLOC_TRAMP		= 1 << 12,
 };
 
 #ifdef CONFIG_DYNAMIC_FTRACE

commit fef5aeeee9e3717e7aea991a7ae9ff6a7a2d4c85
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Thu Jul 24 12:25:47 2014 -0400

    ftrace: Replace tramp_hash with old_*_hash to save space
    
    Allowing function callbacks to declare their own trampolines requires
    that each ftrace_ops that has a trampoline must have some sort of
    accounting that keeps track of which ops has a trampoline attached
    to a record.
    
    The easy way to solve this was to add a "tramp_hash" that created a
    hash entry for every function that a ops uses with a trampoline.
    But since we can have literally tens of thousands of functions being
    traced, that means we need tens of thousands of descriptors to map
    the ops to the function in the hash. This is quite expensive and
    can cause enabling and disabling the function graph tracer to take
    some time to start and stop. It can take up to several seconds to
    disable or enable all functions in the function graph tracer for this
    reason.
    
    The better approach albeit more complex, is to keep track of how ops
    are being enabled and disabled, and use that along with the counting
    of the number of ops attached to records, to determive what ops has
    a trampoline attached to a record at enabling and disabling of
    tracing.
    
    To do this, the tramp_hash has been replaced with an old_filter_hash
    and old_notrace_hash, which get the copy of the ops filter_hash and
    notrace_hash respectively. The old hashes is kept until the ops has
    been modified or removed and the old hashes are used with the logic
    of the accounting to determine the ops that have the trampoline of
    a record. The reason this has less of a footprint is due to the trick
    that an "empty" hash in the filter_hash means "all functions" and
    an empty hash in the notrace hash means "no functions" in the hash.
    
    This is much more efficienct, doesn't have the delay, and takes up
    much less memory, as we do not need to map all the functions but
    just figure out which functions are mapped at the time it is
    enabled or disabled.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index d9216f6385d9..662697babd48 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -140,7 +140,7 @@ struct ftrace_ops {
 	int				nr_trampolines;
 	struct ftrace_ops_hash		local_hash;
 	struct ftrace_ops_hash		*func_hash;
-	struct ftrace_hash		*tramp_hash;
+	struct ftrace_ops_hash		old_hash;
 	unsigned long			trampoline;
 #endif
 };

commit e1effa0144a1ddf5b456c388ffaf784f3c5163fd
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Aug 5 17:19:38 2014 -0400

    ftrace: Annotate the ops operation on update
    
    Add three new flags for ftrace_ops:
    
      FTRACE_OPS_FL_ADDING
      FTRACE_OPS_FL_REMOVING
      FTRACE_OPS_FL_MODIFYING
    
    These will be set for the ftrace_ops when they are first added
    to the function tracing, being removed from function tracing
    or just having their functions changed from function tracing,
    respectively.
    
    This will be needed to remove the tramp_hash, which can grow quite
    big. The tramp_hash is used to note what functions a ftrace_ops
    is using a trampoline for. Denoting which ftrace_ops is being
    modified, will allow us to use the ftrace_ops hashes themselves,
    which are much smaller as they have a global flag to denote if
    a ftrace_ops is tracing all functions, as well as a notrace hash
    if the ftrace_ops is tracing all but a few. The tramp_hash just
    creates a hash item for every function, which can go into the 10s
    of thousands if all functions are using the ftrace_ops trampoline.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index ef37286547fc..d9216f6385d9 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -91,6 +91,9 @@ ftrace_func_t ftrace_ops_get_func(struct ftrace_ops *ops);
  * INITIALIZED - The ftrace_ops has already been initialized (first use time
  *            register_ftrace_function() is called, it will initialized the ops)
  * DELETED - The ops are being deleted, do not let them be registered again.
+ * ADDING  - The ops is in the process of being added.
+ * REMOVING - The ops is in the process of being removed.
+ * MODIFYING - The ops is in the process of changing its filter functions.
  */
 enum {
 	FTRACE_OPS_FL_ENABLED			= 1 << 0,
@@ -102,6 +105,9 @@ enum {
 	FTRACE_OPS_FL_STUB			= 1 << 6,
 	FTRACE_OPS_FL_INITIALIZED		= 1 << 7,
 	FTRACE_OPS_FL_DELETED			= 1 << 8,
+	FTRACE_OPS_FL_ADDING			= 1 << 9,
+	FTRACE_OPS_FL_REMOVING			= 1 << 10,
+	FTRACE_OPS_FL_MODIFYING			= 1 << 11,
 };
 
 #ifdef CONFIG_DYNAMIC_FTRACE

commit 87354059881ce9315181604dc17076c535f4d744
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Jul 22 20:41:42 2014 -0400

    ftrace: Add helper function ftrace_ops_get_func()
    
    Add the helper function to what the mcount trampoline is to call
    for a ftrace_ops function. This helper will be used by arch code
    in the future to set up dynamic trampolines. But as this does the
    same tests that are performed in choosing what function to call for
    the default mcount trampoline, might as well use it to clean up
    the existing code.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index f0b0edbf55a9..ef37286547fc 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -56,6 +56,8 @@ struct ftrace_ops;
 typedef void (*ftrace_func_t)(unsigned long ip, unsigned long parent_ip,
 			      struct ftrace_ops *op, struct pt_regs *regs);
 
+ftrace_func_t ftrace_ops_get_func(struct ftrace_ops *ops);
+
 /*
  * FTRACE_OPS_FL_* bits denote the state of ftrace_ops struct and are
  * set in the flags member.

commit 33b7f99cf003ca6c1d31c42b50e1100ad71aaec0
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Fri Aug 15 17:23:02 2014 -0400

    ftrace: Allow ftrace_ops to use the hashes from other ops
    
    Currently the top level debug file system function tracer shares its
    ftrace_ops with the function graph tracer. This was thought to be fine
    because the tracers are not used together, as one can only enable
    function or function_graph tracer in the current_tracer file.
    
    But that assumption proved to be incorrect. The function profiler
    can use the function graph tracer when function tracing is enabled.
    Since all function graph users uses the function tracing ftrace_ops
    this causes a conflict and when a user enables both function profiling
    as well as the function tracer it will crash ftrace and disable it.
    
    The quick solution so far is to move them as separate ftrace_ops like
    it was earlier. The problem though is to synchronize the functions that
    are traced because both function and function_graph tracer are limited
    by the selections made in the set_ftrace_filter and set_ftrace_notrace
    files.
    
    To handle this, a new structure is made called ftrace_ops_hash. This
    structure will now hold the filter_hash and notrace_hash, and the
    ftrace_ops will point to this structure. That will allow two ftrace_ops
    to share the same hashes.
    
    Since most ftrace_ops do not share the hashes, and to keep allocation
    simple, the ftrace_ops structure will include both a pointer to the
    ftrace_ops_hash called func_hash, as well as the structure itself,
    called local_hash. When the ops are registered, the func_hash pointer
    will be initialized to point to the local_hash within the ftrace_ops
    structure. Some of the ftrace internal ftrace_ops will be initialized
    statically. This will allow for the function and function_graph tracer
    to have separate ops but still share the same hash tables that determine
    what functions they trace.
    
    Cc: stable@vger.kernel.org # 3.16 (apply after 3.17-rc4 is out)
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 6bb5e3f2a3b4..f0b0edbf55a9 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -102,6 +102,15 @@ enum {
 	FTRACE_OPS_FL_DELETED			= 1 << 8,
 };
 
+#ifdef CONFIG_DYNAMIC_FTRACE
+/* The hash used to know what functions callbacks trace */
+struct ftrace_ops_hash {
+	struct ftrace_hash		*notrace_hash;
+	struct ftrace_hash		*filter_hash;
+	struct mutex			regex_lock;
+};
+#endif
+
 /*
  * Note, ftrace_ops can be referenced outside of RCU protection.
  * (Although, for perf, the control ops prevent that). If ftrace_ops is
@@ -121,10 +130,9 @@ struct ftrace_ops {
 	int __percpu			*disabled;
 #ifdef CONFIG_DYNAMIC_FTRACE
 	int				nr_trampolines;
-	struct ftrace_hash		*notrace_hash;
-	struct ftrace_hash		*filter_hash;
+	struct ftrace_ops_hash		local_hash;
+	struct ftrace_ops_hash		*func_hash;
 	struct ftrace_hash		*tramp_hash;
-	struct mutex			regex_lock;
 	unsigned long			trampoline;
 #endif
 };

commit 0162d621ddf3bd02bf7de324dcf002d9c84c5059
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed Jul 23 15:03:00 2014 -0400

    ftrace: Rename ftrace_ops field from trampolines to nr_trampolines
    
    Having two fields within the same struct that is off by one character
    can be confusing and error prone. Rename the counter "trampolines"
    to "nr_trampolines" to explicitly show it is a counter and not to
    be confused by the "trampoline" field.
    
    Suggested-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 7a5b7b97e539..6bb5e3f2a3b4 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -120,7 +120,7 @@ struct ftrace_ops {
 	void				*private;
 	int __percpu			*disabled;
 #ifdef CONFIG_DYNAMIC_FTRACE
-	int				trampolines;
+	int				nr_trampolines;
 	struct ftrace_hash		*notrace_hash;
 	struct ftrace_hash		*filter_hash;
 	struct ftrace_hash		*tramp_hash;

commit 3a636388bae8390d23f31e061c0c6fdc14525786
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Thu Jun 26 11:24:52 2014 -0400

    tracing: Remove function_trace_stop and HAVE_FUNCTION_TRACE_MCOUNT_TEST
    
    All users of function_trace_stop and HAVE_FUNCTION_TRACE_MCOUNT_TEST have
    been removed. We can safely remove them from the kernel.
    
    Reviewed-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index c800906235e1..7a5b7b97e539 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -129,8 +129,6 @@ struct ftrace_ops {
 #endif
 };
 
-extern int function_trace_stop;
-
 /*
  * Type of the current tracing.
  */

commit 7544256aa20356e506b0d179f9b6abc661847e2f
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed Jun 25 13:26:59 2014 -0400

    ftrace: Remove check for HAVE_FUNCTION_TRACE_MCOUNT_TEST
    
    function_trace_stop is no longer used to disable function tracing.
    This means that archs are no longer limited if it does not support
    checking this variable in the mcount trampoline.
    
    No need to use the list_func for archs that do not support this
    obsolete method.
    
    Acked-by: James Hogan <james.hogan@imgtec.com>
    Reviewed-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index b7333794554f..c800906235e1 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -33,8 +33,7 @@
  * features, then it must call an indirect function that
  * does. Or at least does enough to prevent any unwelcomed side effects.
  */
-#if !defined(CONFIG_HAVE_FUNCTION_TRACE_MCOUNT_TEST) || \
-	!ARCH_SUPPORTS_FTRACE_OPS
+#if !ARCH_SUPPORTS_FTRACE_OPS
 # define FTRACE_FORCE_LIST_FUNC 1
 #else
 # define FTRACE_FORCE_LIST_FUNC 0

commit 0ef1b9e0cfd98f91b2341d581ea9424eb4ba3aa7
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed Jun 25 11:25:19 2014 -0400

    ftrace: Remove ftrace_start/stop()
    
    There are no more kernel users of ftrace_stop() and ftrace_start().
    Remove them.
    
    Reviewed-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 18fb2c4a3f7f..b7333794554f 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -143,32 +143,6 @@ enum ftrace_tracing_type_t {
 /* Current tracing type, default is FTRACE_TYPE_ENTER */
 extern enum ftrace_tracing_type_t ftrace_tracing_type;
 
-/**
- * ftrace_stop - stop function tracer.
- *
- * A quick way to stop the function tracer. Note this an on off switch,
- * it is not something that is recursive like preempt_disable.
- * This does not disable the calling of mcount, it only stops the
- * calling of functions from mcount.
- */
-static inline void ftrace_stop(void)
-{
-	function_trace_stop = 1;
-}
-
-/**
- * ftrace_start - start the function tracer.
- *
- * This function is the inverse of ftrace_stop. This does not enable
- * the function tracing if the function tracer is disabled. This only
- * sets the function tracer flag to continue calling the functions
- * from mcount.
- */
-static inline void ftrace_start(void)
-{
-	function_trace_stop = 0;
-}
-
 /*
  * The ftrace_ops must be a static and should also
  * be read_mostly.  These functions do modify read_mostly variables
@@ -245,8 +219,6 @@ static inline int ftrace_nr_registered_ops(void)
 }
 static inline void clear_ftrace_function(void) { }
 static inline void ftrace_kill(void) { }
-static inline void ftrace_stop(void) { }
-static inline void ftrace_start(void) { }
 #endif /* CONFIG_FUNCTION_TRACER */
 
 #ifdef CONFIG_STACK_TRACER

commit 1b2f121c1418249e56048d816754b479b3cb6fb3
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed Jun 25 10:39:46 2014 -0400

    ftrace-graph: Remove dependency of ftrace_stop() from ftrace_graph_stop()
    
    ftrace_stop() is going away as it disables parts of function tracing
    that affects users that should not be affected. But ftrace_graph_stop()
    is built on ftrace_stop(). Here's another example of killing all of
    function tracing because something went wrong with function graph
    tracing.
    
    Instead of disabling all users of function tracing on function graph
    error, disable only function graph tracing.
    
    A new function is created called ftrace_graph_is_dead(). This is called
    in strategic paths to prevent function graph from doing more harm and
    allowing at least a warning to be printed before the system crashes.
    
    NOTE: ftrace_stop() is still used until all the archs are converted over
    to use ftrace_graph_is_dead(). After that, ftrace_stop() will be removed.
    
    Reviewed-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 4807a39e7ae1..18fb2c4a3f7f 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -760,6 +760,7 @@ extern char __irqentry_text_end[];
 extern int register_ftrace_graph(trace_func_graph_ret_t retfunc,
 				trace_func_graph_ent_t entryfunc);
 
+extern bool ftrace_graph_is_dead(void);
 extern void ftrace_graph_stop(void);
 
 /* The current handlers in use */

commit 646d7043adf3d92de5d3db1244a82a12628303de
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Fri Jul 11 14:39:10 2014 -0400

    ftrace: Allow archs to specify if they need a separate function graph trampoline
    
    Currently if an arch supports function graph tracing, the core code will
    just assign the function graph trampoline to the function graph addr that
    gets called.
    
    But as the old method for function graph tracing always calls the function
    trampoline first and that calls the function graph trampoline, some
    archs may have the function graph trampoline dependent on operations that
    were done in the function trampoline. This causes function graph tracer
    to break on those archs.
    
    Instead of having the default be to set the function graph ftrace_ops
    to the function graph trampoline, have it instead just set it to zero
    which will keep it from jumping to a trampoline that is not set up
    to be jumped directly too.
    
    Link: http://lkml.kernel.org/r/53BED155.9040607@nvidia.com
    
    Reported-by: Tuomas Tynkkynen <ttynkkynen@nvidia.com>
    Tested-by: Tuomas Tynkkynen <ttynkkynen@nvidia.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 11e18fd58b1a..4807a39e7ae1 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -453,6 +453,16 @@ void ftrace_modify_all_code(int command);
 #endif
 #endif
 
+/*
+ * If an arch would like functions that are only traced
+ * by the function graph tracer to jump directly to its own
+ * trampoline, then they can define FTRACE_GRAPH_TRAMP_ADDR
+ * to be that address to jump to.
+ */
+#ifndef FTRACE_GRAPH_TRAMP_ADDR
+#define FTRACE_GRAPH_TRAMP_ADDR ((unsigned long) 0)
+#endif
+
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
 extern void ftrace_graph_caller(void);
 extern int ftrace_enable_ftrace_graph_caller(void);

commit 79922b8009c074e30d3a97f5a24519f11814ad03
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue May 6 21:56:17 2014 -0400

    ftrace: Optimize function graph to be called directly
    
    Function graph tracing is a bit different than the function tracers, as
    it is processed after either the ftrace_caller or ftrace_regs_caller
    and we only have one place to modify the jump to ftrace_graph_caller,
    the jump needs to happen after the restore of registeres.
    
    The function graph tracer is dependent on the function tracer, where
    even if the function graph tracing is going on by itself, the save and
    restore of registers is still done for function tracing regardless of
    if function tracing is happening, before it calls the function graph
    code.
    
    If there's no function tracing happening, it is possible to just call
    the function graph tracer directly, and avoid the wasted effort to save
    and restore regs for function tracing.
    
    This requires adding new flags to the dyn_ftrace records:
    
      FTRACE_FL_TRAMP
      FTRACE_FL_TRAMP_EN
    
    The first is set if the count for the record is one, and the ftrace_ops
    associated to that record has its own trampoline. That way the mcount code
    can call that trampoline directly.
    
    In the future, trampolines can be added to arbitrary ftrace_ops, where you
    can have two or more ftrace_ops registered to ftrace (like kprobes and perf)
    and if they are not tracing the same functions, then instead of doing a
    loop to check all registered ftrace_ops against their hashes, just call the
    ftrace_ops trampoline directly, which would call the registered ftrace_ops
    function directly.
    
    Without this patch perf showed:
    
      0.05%  hackbench  [kernel.kallsyms]  [k] ftrace_caller
      0.05%  hackbench  [kernel.kallsyms]  [k] arch_local_irq_save
      0.05%  hackbench  [kernel.kallsyms]  [k] native_sched_clock
      0.04%  hackbench  [kernel.kallsyms]  [k] __buffer_unlock_commit
      0.04%  hackbench  [kernel.kallsyms]  [k] preempt_trace
      0.04%  hackbench  [kernel.kallsyms]  [k] prepare_ftrace_return
      0.04%  hackbench  [kernel.kallsyms]  [k] __this_cpu_preempt_check
      0.04%  hackbench  [kernel.kallsyms]  [k] ftrace_graph_caller
    
    See that the ftrace_caller took up more time than the ftrace_graph_caller
    did.
    
    With this patch:
    
      0.05%  hackbench  [kernel.kallsyms]  [k] __buffer_unlock_commit
      0.04%  hackbench  [kernel.kallsyms]  [k] call_filter_check_discard
      0.04%  hackbench  [kernel.kallsyms]  [k] ftrace_graph_caller
      0.04%  hackbench  [kernel.kallsyms]  [k] sched_clock
    
    The ftrace_caller is no where to be found and ftrace_graph_caller still
    takes up the same percentage.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index e5baa6b2c93f..11e18fd58b1a 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -118,12 +118,15 @@ struct ftrace_ops {
 	ftrace_func_t			func;
 	struct ftrace_ops		*next;
 	unsigned long			flags;
-	int __percpu			*disabled;
 	void				*private;
+	int __percpu			*disabled;
 #ifdef CONFIG_DYNAMIC_FTRACE
+	int				trampolines;
 	struct ftrace_hash		*notrace_hash;
 	struct ftrace_hash		*filter_hash;
+	struct ftrace_hash		*tramp_hash;
 	struct mutex			regex_lock;
+	unsigned long			trampoline;
 #endif
 };
 
@@ -317,13 +320,15 @@ extern int ftrace_nr_registered_ops(void);
  * from tracing that function.
  */
 enum {
-	FTRACE_FL_ENABLED	= (1UL << 29),
+	FTRACE_FL_ENABLED	= (1UL << 31),
 	FTRACE_FL_REGS		= (1UL << 30),
-	FTRACE_FL_REGS_EN	= (1UL << 31)
+	FTRACE_FL_REGS_EN	= (1UL << 29),
+	FTRACE_FL_TRAMP		= (1UL << 28),
+	FTRACE_FL_TRAMP_EN	= (1UL << 27),
 };
 
-#define FTRACE_REF_MAX_SHIFT	29
-#define FTRACE_FL_BITS		3
+#define FTRACE_REF_MAX_SHIFT	27
+#define FTRACE_FL_BITS		5
 #define FTRACE_FL_MASKED_BITS	((1UL << FTRACE_FL_BITS) - 1)
 #define FTRACE_FL_MASK		(FTRACE_FL_MASKED_BITS << FTRACE_REF_MAX_SHIFT)
 #define FTRACE_REF_MAX		((1UL << FTRACE_REF_MAX_SHIFT) - 1)
@@ -436,6 +441,10 @@ void ftrace_modify_all_code(int command);
 #define FTRACE_ADDR ((unsigned long)ftrace_caller)
 #endif
 
+#ifndef FTRACE_GRAPH_ADDR
+#define FTRACE_GRAPH_ADDR ((unsigned long)ftrace_graph_caller)
+#endif
+
 #ifndef FTRACE_REGS_ADDR
 #ifdef CONFIG_DYNAMIC_FTRACE_WITH_REGS
 # define FTRACE_REGS_ADDR ((unsigned long)ftrace_regs_caller)

commit 0376bde11be5b87c9fd7d6813ac5fd7e1798b1bf
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed May 7 13:46:45 2014 -0400

    ftrace: Add ftrace_rec_counter() macro to simplify the code
    
    The ftrace dynamic record has a flags element that also has a counter.
    Instead of hard coding "rec->flags & ~FTRACE_FL_MASK" all over the
    place. Use a macro instead.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index e4e7df422021..e5baa6b2c93f 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -328,6 +328,8 @@ enum {
 #define FTRACE_FL_MASK		(FTRACE_FL_MASKED_BITS << FTRACE_REF_MAX_SHIFT)
 #define FTRACE_REF_MAX		((1UL << FTRACE_REF_MAX_SHIFT) - 1)
 
+#define ftrace_rec_count(rec)	((rec)->flags & ~FTRACE_FL_MASK)
+
 struct dyn_ftrace {
 	unsigned long		ip; /* address of mcount call-site */
 	unsigned long		flags;

commit cf2cb0b27116883c23761e974acba5f3bd719d21
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed May 7 12:42:28 2014 -0400

    ftrace: Use macros for numbers in ftrace rec shift bits
    
    As new flags will be added to the ftrace dynamic record, and since
    the flags field is also a counter, converting the numbers used to
    do the shifting and masking into a set of macros where we only need
    to deal with the max bit count of the counter and the number of bits
    for the flags will prevent mistakes in the future.
    
    Dealing with only two numbers is much easier than updating all the
    macros that deal with shifting and masking.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 404a686a3644..e4e7df422021 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -322,8 +322,11 @@ enum {
 	FTRACE_FL_REGS_EN	= (1UL << 31)
 };
 
-#define FTRACE_FL_MASK		(0x7UL << 29)
-#define FTRACE_REF_MAX		((1UL << 29) - 1)
+#define FTRACE_REF_MAX_SHIFT	29
+#define FTRACE_FL_BITS		3
+#define FTRACE_FL_MASKED_BITS	((1UL << FTRACE_FL_BITS) - 1)
+#define FTRACE_FL_MASK		(FTRACE_FL_MASKED_BITS << FTRACE_REF_MAX_SHIFT)
+#define FTRACE_REF_MAX		((1UL << FTRACE_REF_MAX_SHIFT) - 1)
 
 struct dyn_ftrace {
 	unsigned long		ip; /* address of mcount call-site */

commit 214b93132023cc9305d5801add812515bea4d7d0
Merge: 14208b0ec569 a9fcaaac37b3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 9 16:39:15 2014 -0700

    Merge tag 'trace-3.16' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull tracing updates from Steven Rostedt:
     "Lots of tweaks, small fixes, optimizations, and some helper functions
      to help out the rest of the kernel to ease their use of trace events.
    
      The big change for this release is the allowing of other tracers, such
      as the latency tracers, to be used in the trace instances and allow
      for function or function graph tracing to be in the top level
      simultaneously"
    
    * tag 'trace-3.16' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace: (44 commits)
      tracing: Fix memory leak on instance deletion
      tracing: Fix leak of ring buffer data when new instances creation fails
      tracing/kprobes: Avoid self tests if tracing is disabled on boot up
      tracing: Return error if ftrace_trace_arrays list is empty
      tracing: Only calculate stats of tracepoint benchmarks for 2^32 times
      tracing: Convert stddev into u64 in tracepoint benchmark
      tracing: Introduce saved_cmdlines_size file
      tracing: Add __get_dynamic_array_len() macro for trace events
      tracing: Remove unused variable in trace_benchmark
      tracing: Eliminate double free on failure of allocation on boot up
      ftrace/x86: Call text_ip_addr() instead of the duplicated code
      tracing: Print max callstack on stacktrace bug
      tracing: Move locking of trace_cmdline_lock into start/stop seq calls
      tracing: Try again for saved cmdline if failed due to locking
      tracing: Have saved_cmdlines use the seq_read infrastructure
      tracing: Add tracepoint benchmark tracepoint
      tracing: Print nasty banner when trace_printk() is in use
      tracing: Add funcgraph_tail option to print function name after closing braces
      tracing: Eliminate duplicate TRACE_GRAPH_PRINT_xx defines
      tracing: Add __bitmask() macro to trace events to cpumasks and other bitmasks
      ...

commit eed542d6962ba33a689b4007a389f466e407bd74
Author: AKASHI Takahiro <takahiro.akashi@linaro.org>
Date:   Tue May 20 20:31:04 2014 +0900

    ftrace: Make CALLER_ADDRx macros more generic
    
    Most archs with HAVE_ARCH_CALLER_ADDR have pretty much the same
    definitions of CALLER_ADDRx(n). Instead of duplicating the code for all
    the archs, define a ftrace_return_address0() and
    ftrace_return_address(n) that can be overwritten by the archs if they
    need to do something different. Instead of 7 macros in every arch, we
    now only have at most 2 (and actually only 1 as
    ftrace_return_address0() should be the same for all archs).
    
    The CALLER_ADDRx(n) will now be defined in linux/ftrace.h and use the
    ftrace_return_address*(n?) macros. This removes a lot of the duplicate
    code.
    
    Link: http://lkml.kernel.org/p/1400585464-30333-1-git-send-email-takahiro.akashi@linaro.org
    
    Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index ae9504b4b67d..2018751cad9e 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -616,25 +616,27 @@ static inline void __ftrace_enabled_restore(int enabled)
 #endif
 }
 
-#ifndef HAVE_ARCH_CALLER_ADDR
+/* All archs should have this, but we define it for consistency */
+#ifndef ftrace_return_address0
+# define ftrace_return_address0 __builtin_return_address(0)
+#endif
+
+/* Archs may use other ways for ADDR1 and beyond */
+#ifndef ftrace_return_address
 # ifdef CONFIG_FRAME_POINTER
-#  define CALLER_ADDR0 ((unsigned long)__builtin_return_address(0))
-#  define CALLER_ADDR1 ((unsigned long)__builtin_return_address(1))
-#  define CALLER_ADDR2 ((unsigned long)__builtin_return_address(2))
-#  define CALLER_ADDR3 ((unsigned long)__builtin_return_address(3))
-#  define CALLER_ADDR4 ((unsigned long)__builtin_return_address(4))
-#  define CALLER_ADDR5 ((unsigned long)__builtin_return_address(5))
-#  define CALLER_ADDR6 ((unsigned long)__builtin_return_address(6))
+#  define ftrace_return_address(n) __builtin_return_address(n)
 # else
-#  define CALLER_ADDR0 ((unsigned long)__builtin_return_address(0))
-#  define CALLER_ADDR1 0UL
-#  define CALLER_ADDR2 0UL
-#  define CALLER_ADDR3 0UL
-#  define CALLER_ADDR4 0UL
-#  define CALLER_ADDR5 0UL
-#  define CALLER_ADDR6 0UL
+#  define ftrace_return_address(n) 0UL
 # endif
-#endif /* ifndef HAVE_ARCH_CALLER_ADDR */
+#endif
+
+#define CALLER_ADDR0 ((unsigned long)ftrace_return_address0)
+#define CALLER_ADDR1 ((unsigned long)ftrace_return_address(1))
+#define CALLER_ADDR2 ((unsigned long)ftrace_return_address(2))
+#define CALLER_ADDR3 ((unsigned long)ftrace_return_address(3))
+#define CALLER_ADDR4 ((unsigned long)ftrace_return_address(4))
+#define CALLER_ADDR5 ((unsigned long)ftrace_return_address(5))
+#define CALLER_ADDR6 ((unsigned long)ftrace_return_address(6))
 
 #ifdef CONFIG_IRQSOFF_TRACER
   extern void time_hardirqs_on(unsigned long a0, unsigned long a1);

commit f1b2f2bd5821c6ab7feed2e133343dd54b212ed9
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed May 7 16:09:49 2014 -0400

    ftrace: Remove FTRACE_UPDATE_MODIFY_CALL_REGS flag
    
    As the decision to what needs to be done (converting a call to the
    ftrace_caller to ftrace_caller_regs or to convert from ftrace_caller_regs
    to ftrace_caller) can easily be determined from the rec->flags of
    FTRACE_FL_REGS and FTRACE_FL_REGS_EN, there's no need to have the
    ftrace_check_record() return either a UPDATE_MODIFY_CALL_REGS or a
    UPDATE_MODIFY_CALL. Just he latter is enough. This added flag causes
    more complexity than is required. Remove it.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 2f8cbffecd3d..3e6dfb31f8e6 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -362,14 +362,12 @@ enum {
  *  IGNORE           - The function is already what we want it to be
  *  MAKE_CALL        - Start tracing the function
  *  MODIFY_CALL      - Stop saving regs for the function
- *  MODIFY_CALL_REGS - Start saving regs for the function
  *  MAKE_NOP         - Stop tracing the function
  */
 enum {
 	FTRACE_UPDATE_IGNORE,
 	FTRACE_UPDATE_MAKE_CALL,
 	FTRACE_UPDATE_MODIFY_CALL,
-	FTRACE_UPDATE_MODIFY_CALL_REGS,
 	FTRACE_UPDATE_MAKE_NOP,
 };
 

commit 7413af1fb70e7efa6dbc7f27663e7a5126b3aa33
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue May 6 21:34:14 2014 -0400

    ftrace: Make get_ftrace_addr() and get_ftrace_addr_old() global
    
    Move and rename get_ftrace_addr() and get_ftrace_addr_old() to
    ftrace_get_addr_new() and ftrace_get_addr_curr() respectively.
    
    This moves these two helper functions in the generic code out from
    the arch specific code, and renames them to have a better generic
    name. This will allow other archs to use them as well as makes it
    a bit easier to work on getting separate trampolines for different
    functions.
    
    ftrace_get_addr_new() returns the trampoline address that the mcount
    call address will be converted to.
    
    ftrace_get_addr_curr() returns the trampoline address of what the
    mcount call address currently jumps to.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index f0ff2c2453e7..2f8cbffecd3d 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -400,6 +400,8 @@ int ftrace_update_record(struct dyn_ftrace *rec, int enable);
 int ftrace_test_record(struct dyn_ftrace *rec, int enable);
 void ftrace_run_stop_machine(int command);
 unsigned long ftrace_location(unsigned long ip);
+unsigned long ftrace_get_addr_new(struct dyn_ftrace *rec);
+unsigned long ftrace_get_addr_curr(struct dyn_ftrace *rec);
 
 extern ftrace_func_t ftrace_trace_function;
 

commit a949ae560a511fe4e3adf48fa44fefded93e5c2b
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Thu Apr 24 10:40:12 2014 -0400

    ftrace/module: Hardcode ftrace_module_init() call into load_module()
    
    A race exists between module loading and enabling of function tracer.
    
            CPU 1                           CPU 2
            -----                           -----
      load_module()
       module->state = MODULE_STATE_COMING
    
                                    register_ftrace_function()
                                     mutex_lock(&ftrace_lock);
                                     ftrace_startup()
                                      update_ftrace_function();
                                       ftrace_arch_code_modify_prepare()
                                        set_all_module_text_rw();
                                       <enables-ftrace>
                                        ftrace_arch_code_modify_post_process()
                                         set_all_module_text_ro();
    
                                    [ here all module text is set to RO,
                                      including the module that is
                                      loading!! ]
    
       blocking_notifier_call_chain(MODULE_STATE_COMING);
        ftrace_init_module()
    
         [ tries to modify code, but it's RO, and fails!
           ftrace_bug() is called]
    
    When this race happens, ftrace_bug() will produces a nasty warning and
    all of the function tracing features will be disabled until reboot.
    
    The simple solution is to treate module load the same way the core
    kernel is treated at boot. To hardcode the ftrace function modification
    of converting calls to mcount into nops. This is done in init/main.c
    there's no reason it could not be done in load_module(). This gives
    a better control of the changes and doesn't tie the state of the
    module to its notifiers as much. Ftrace is special, it needs to be
    treated as such.
    
    The reason this would work, is that the ftrace_module_init() would be
    called while the module is in MODULE_STATE_UNFORMED, which is ignored
    by the set_all_module_text_ro() call.
    
    Link: http://lkml.kernel.org/r/1395637826-3312-1-git-send-email-indou.takao@jp.fujitsu.com
    
    Reported-by: Takao Indoh <indou.takao@jp.fujitsu.com>
    Acked-by: Rusty Russell <rusty@rustcorp.com.au>
    Cc: stable@vger.kernel.org # 2.6.38+
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 9212b017bc72..ae9504b4b67d 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -535,6 +535,7 @@ static inline int ftrace_modify_call(struct dyn_ftrace *rec, unsigned long old_a
 extern int ftrace_arch_read_dyn_info(char *buf, int size);
 
 extern int skip_trace(unsigned long ip);
+extern void ftrace_module_init(struct module *mod);
 
 extern void ftrace_disable_daemon(void);
 extern void ftrace_enable_daemon(void);
@@ -544,6 +545,7 @@ static inline int ftrace_force_update(void) { return 0; }
 static inline void ftrace_disable_daemon(void) { }
 static inline void ftrace_enable_daemon(void) { }
 static inline void ftrace_release_mod(struct module *mod) {}
+static inline void ftrace_module_init(struct module *mod) {}
 static inline __init int register_ftrace_command(struct ftrace_func_command *cmd)
 {
 	return -EINVAL;

commit 4104d326b670c2b66f575d2004daa28b2d1b4c8d
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Fri Jan 10 17:01:58 2014 -0500

    ftrace: Remove global function list and call function directly
    
    Instead of having a list of global functions that are called,
    as only one global function is allow to be enabled at a time, there's
    no reason to have a list.
    
    Instead, simply have all the users of the global ops, use the global ops
    directly, instead of registering their own ftrace_ops. Just switch what
    function is used before enabling the function tracer.
    
    This removes a lot of code as well as the complexity involved with it.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 9212b017bc72..f0ff2c2453e7 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -62,9 +62,6 @@ typedef void (*ftrace_func_t)(unsigned long ip, unsigned long parent_ip,
  * set in the flags member.
  *
  * ENABLED - set/unset when ftrace_ops is registered/unregistered
- * GLOBAL  - set manualy by ftrace_ops user to denote the ftrace_ops
- *           is part of the global tracers sharing the same filter
- *           via set_ftrace_* debugfs files.
  * DYNAMIC - set when ftrace_ops is registered to denote dynamically
  *           allocated ftrace_ops which need special care
  * CONTROL - set manualy by ftrace_ops user to denote the ftrace_ops
@@ -96,15 +93,14 @@ typedef void (*ftrace_func_t)(unsigned long ip, unsigned long parent_ip,
  */
 enum {
 	FTRACE_OPS_FL_ENABLED			= 1 << 0,
-	FTRACE_OPS_FL_GLOBAL			= 1 << 1,
-	FTRACE_OPS_FL_DYNAMIC			= 1 << 2,
-	FTRACE_OPS_FL_CONTROL			= 1 << 3,
-	FTRACE_OPS_FL_SAVE_REGS			= 1 << 4,
-	FTRACE_OPS_FL_SAVE_REGS_IF_SUPPORTED	= 1 << 5,
-	FTRACE_OPS_FL_RECURSION_SAFE		= 1 << 6,
-	FTRACE_OPS_FL_STUB			= 1 << 7,
-	FTRACE_OPS_FL_INITIALIZED		= 1 << 8,
-	FTRACE_OPS_FL_DELETED			= 1 << 9,
+	FTRACE_OPS_FL_DYNAMIC			= 1 << 1,
+	FTRACE_OPS_FL_CONTROL			= 1 << 2,
+	FTRACE_OPS_FL_SAVE_REGS			= 1 << 3,
+	FTRACE_OPS_FL_SAVE_REGS_IF_SUPPORTED	= 1 << 4,
+	FTRACE_OPS_FL_RECURSION_SAFE		= 1 << 5,
+	FTRACE_OPS_FL_STUB			= 1 << 6,
+	FTRACE_OPS_FL_INITIALIZED		= 1 << 7,
+	FTRACE_OPS_FL_DELETED			= 1 << 8,
 };
 
 /*

commit d88471cb8b17a72b1edf5ab62e1704d78373c066
Author: Sasha Levin <sasha.levin@oracle.com>
Date:   Wed Jan 9 18:09:20 2013 -0500

    ftrace: Constify ftrace_text_reserved
    
    Link: http://lkml.kernel.org/r/1357772960-4436-5-git-send-email-sasha.levin@oracle.com
    
    Signed-off-by: Sasha Levin <sasha.levin@oracle.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 20da0561b868..9212b017bc72 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -299,7 +299,7 @@ extern void
 unregister_ftrace_function_probe_func(char *glob, struct ftrace_probe_ops *ops);
 extern void unregister_ftrace_function_probe_all(char *glob);
 
-extern int ftrace_text_reserved(void *start, void *end);
+extern int ftrace_text_reserved(const void *start, const void *end);
 
 extern int ftrace_nr_registered_ops(void);
 
@@ -552,7 +552,7 @@ static inline __init int unregister_ftrace_command(char *cmd_name)
 {
 	return -EINVAL;
 }
-static inline int ftrace_text_reserved(void *start, void *end)
+static inline int ftrace_text_reserved(const void *start, const void *end)
 {
 	return 0;
 }

commit a762782d780b341b1836489692190d26be624ed7
Author: Jiri Slaby <jslaby@suse.cz>
Date:   Mon Feb 24 20:00:00 2014 +0100

    ftrace: Remove freelist from struct dyn_ftrace
    
    The 'freelist' member was introduced to 'struct dyn_ftrace' in commit
    ee000b7f9fe429d2470c674ccec8d344f6789e0d (tracing: use union for
    multi-usages field), but the use of this member was later removed in
    3208230983a0ee3d95be22d463257e530c684956 (ftrace: Remove usage of
    "freed" records). Remove also the 'freelist' member now.
    
    Link: http://lkml.kernel.org/r/1393268401-24379-5-git-send-email-jslaby@suse.cz
    
    Signed-off-by: Jiri Slaby <jslaby@suse.cz>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 1bbb2cd631de..20da0561b868 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -330,12 +330,9 @@ enum {
 #define FTRACE_REF_MAX		((1UL << 29) - 1)
 
 struct dyn_ftrace {
-	union {
-		unsigned long		ip; /* address of mcount call-site */
-		struct dyn_ftrace	*freelist;
-	};
+	unsigned long		ip; /* address of mcount call-site */
 	unsigned long		flags;
-	struct dyn_arch_ftrace		arch;
+	struct dyn_arch_ftrace	arch;
 };
 
 int ftrace_force_update(void);

commit 3a36cb11ca65cd6804972eaf1000378ba4384ea7
Author: Jiri Slaby <jslaby@suse.cz>
Date:   Mon Feb 24 19:59:59 2014 +0100

    ftrace: Do not pass data to ftrace_dyn_arch_init
    
    As the data parameter is not really used by any ftrace_dyn_arch_init,
    remove that from ftrace_dyn_arch_init. This also removes the addr
    local variable from ftrace_init which is now unused.
    
    Note the documentation was imprecise as it did not suggest to set
    (*data) to 0.
    
    Link: http://lkml.kernel.org/r/1393268401-24379-4-git-send-email-jslaby@suse.cz
    
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: linux-arch@vger.kernel.org
    Signed-off-by: Jiri Slaby <jslaby@suse.cz>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index e6141be2fad5..1bbb2cd631de 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -423,7 +423,7 @@ ftrace_set_early_filter(struct ftrace_ops *ops, char *buf, int enable);
 
 /* defined in arch */
 extern int ftrace_ip_converted(unsigned long ip);
-extern int ftrace_dyn_arch_init(void *data);
+extern int ftrace_dyn_arch_init(void);
 extern void ftrace_replace_code(int enable);
 extern int ftrace_update_ftrace_func(ftrace_func_t func);
 extern void ftrace_caller(void);

commit 591dffdade9f07692a7dd3ed16830ec24e901ece
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Fri Jan 10 16:17:45 2014 -0500

    ftrace: Allow for function tracing instance to filter functions
    
    Create a "set_ftrace_filter" and "set_ftrace_notrace" files in the instance
    directories to let users filter of functions to trace for the given instance.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index ef1607ed7044..e6141be2fad5 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -92,6 +92,7 @@ typedef void (*ftrace_func_t)(unsigned long ip, unsigned long parent_ip,
  * STUB   - The ftrace_ops is just a place holder.
  * INITIALIZED - The ftrace_ops has already been initialized (first use time
  *            register_ftrace_function() is called, it will initialized the ops)
+ * DELETED - The ops are being deleted, do not let them be registered again.
  */
 enum {
 	FTRACE_OPS_FL_ENABLED			= 1 << 0,
@@ -103,6 +104,7 @@ enum {
 	FTRACE_OPS_FL_RECURSION_SAFE		= 1 << 6,
 	FTRACE_OPS_FL_STUB			= 1 << 7,
 	FTRACE_OPS_FL_INITIALIZED		= 1 << 8,
+	FTRACE_OPS_FL_DELETED			= 1 << 9,
 };
 
 /*

commit b7e00a6c53e9134d5cf7631582acaf027a5ded26
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Thu Nov 7 09:36:25 2013 -0500

    ftrace: Add private data to ftrace_ops
    
    Passing data to the function callback was originally done by adding the
    ftrace_ops in another structure, and using the container_of() to get
    the field. But this adds a bit more complexity than it is worth, and
    adding a simple .private field to ftrace_ops makes things a lot easier.
    
    But be warned, the .private data should not be freed once it is used
    unless the ftrace_ops itself has gone through the necessary freeing
    routines. A simple synchronize_sched() is not enough as functions
    can be traced that are called outside the view of RCU and all its
    concoctions.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index f4233b195dab..ef1607ed7044 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -105,11 +105,23 @@ enum {
 	FTRACE_OPS_FL_INITIALIZED		= 1 << 8,
 };
 
+/*
+ * Note, ftrace_ops can be referenced outside of RCU protection.
+ * (Although, for perf, the control ops prevent that). If ftrace_ops is
+ * allocated and not part of kernel core data, the unregistering of it will
+ * perform a scheduling on all CPUs to make sure that there are no more users.
+ * Depending on the load of the system that may take a bit of time.
+ *
+ * Any private data added must also take care not to be freed and if private
+ * data is added to a ftrace_ops that is in core code, the user of the
+ * ftrace_ops must perform a schedule_on_each_cpu() before freeing it.
+ */
 struct ftrace_ops {
 	ftrace_func_t			func;
 	struct ftrace_ops		*next;
 	unsigned long			flags;
 	int __percpu			*disabled;
+	void				*private;
 #ifdef CONFIG_DYNAMIC_FTRACE
 	struct ftrace_hash		*notrace_hash;
 	struct ftrace_hash		*filter_hash;

commit 098c879e1f2d6ee7afbfe959f6b04070065cec90
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Sat Dec 21 17:39:40 2013 -0500

    tracing: Add generic tracing_lseek() function
    
    Trace event triggers added a lseek that uses the ftrace_filter_lseek()
    function. Unfortunately, when function tracing is not configured in
    that function is not defined and the kernel fails to build.
    
    This is the second time that function was added to a file ops and
    it broke the build due to requiring special config dependencies.
    
    Make a generic tracing_lseek() that all the tracing utilities may
    use.
    
    Also, modify the old ftrace_filter_lseek() to return 0 instead of
    1 on WRONLY. Not sure why it was a 1 as that does not make sense.
    
    This also changes the old tracing_seek() to modify the file pos
    pointer on WRONLY as well.
    
    Reported-by: kbuild test robot <fengguang.wu@intel.com>
    Tested-by: Tom Zanussi <tom.zanussi@linux.intel.com>
    Acked-by: Tom Zanussi <tom.zanussi@linux.intel.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 31ea4b428360..f4233b195dab 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -570,8 +570,6 @@ static inline int
 ftrace_regex_release(struct inode *inode, struct file *file) { return -ENODEV; }
 #endif /* CONFIG_DYNAMIC_FTRACE */
 
-loff_t ftrace_filter_lseek(struct file *file, loff_t offset, int whence);
-
 /* totally disable ftrace - can not re-enable after this */
 void ftrace_kill(void);
 

commit 38de93abec8d8acd8d6dbbe9b0d92d6d5cdb3090
Author: Tom Zanussi <tom.zanussi@linux.intel.com>
Date:   Thu Oct 24 08:34:18 2013 -0500

    tracing: Make register/unregister_ftrace_command __init
    
    register/unregister_ftrace_command() are only ever called from __init
    functions, so can themselves be made __init.
    
    Also make register_snapshot_cmd() __init for the same reason.
    
    Link: http://lkml.kernel.org/r/d4042c8cadb7ae6f843ac9a89a24e1c6a3099727.1382620672.git.tom.zanussi@linux.intel.com
    
    Signed-off-by: Tom Zanussi <tom.zanussi@linux.intel.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index ec85d48619e1..31ea4b428360 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -533,11 +533,11 @@ static inline int ftrace_force_update(void) { return 0; }
 static inline void ftrace_disable_daemon(void) { }
 static inline void ftrace_enable_daemon(void) { }
 static inline void ftrace_release_mod(struct module *mod) {}
-static inline int register_ftrace_command(struct ftrace_func_command *cmd)
+static inline __init int register_ftrace_command(struct ftrace_func_command *cmd)
 {
 	return -EINVAL;
 }
-static inline int unregister_ftrace_command(char *cmd_name)
+static inline __init int unregister_ftrace_command(char *cmd_name)
 {
 	return -EINVAL;
 }

commit 29ad23b00474c34e3b5040dda508c78d33a1a3eb
Author: Namhyung Kim <namhyung.kim@lge.com>
Date:   Mon Oct 14 17:24:26 2013 +0900

    ftrace: Add set_graph_notrace filter
    
    The set_graph_notrace filter is analogous to set_ftrace_notrace and
    can be used for eliminating uninteresting part of function graph trace
    output.  It also works with set_graph_function nicely.
    
      # cd /sys/kernel/debug/tracing/
      # echo do_page_fault > set_graph_function
      # perf ftrace live true
       2)               |  do_page_fault() {
       2)               |    __do_page_fault() {
       2)   0.381 us    |      down_read_trylock();
       2)   0.055 us    |      __might_sleep();
       2)   0.696 us    |      find_vma();
       2)               |      handle_mm_fault() {
       2)               |        handle_pte_fault() {
       2)               |          __do_fault() {
       2)               |            filemap_fault() {
       2)               |              find_get_page() {
       2)   0.033 us    |                __rcu_read_lock();
       2)   0.035 us    |                __rcu_read_unlock();
       2)   1.696 us    |              }
       2)   0.031 us    |              __might_sleep();
       2)   2.831 us    |            }
       2)               |            _raw_spin_lock() {
       2)   0.046 us    |              add_preempt_count();
       2)   0.841 us    |            }
       2)   0.033 us    |            page_add_file_rmap();
       2)               |            _raw_spin_unlock() {
       2)   0.057 us    |              sub_preempt_count();
       2)   0.568 us    |            }
       2)               |            unlock_page() {
       2)   0.084 us    |              page_waitqueue();
       2)   0.126 us    |              __wake_up_bit();
       2)   1.117 us    |            }
       2)   7.729 us    |          }
       2)   8.397 us    |        }
       2)   8.956 us    |      }
       2)   0.085 us    |      up_read();
       2) + 12.745 us   |    }
       2) + 13.401 us   |  }
      ...
    
      # echo handle_mm_fault > set_graph_notrace
      # perf ftrace live true
       1)               |  do_page_fault() {
       1)               |    __do_page_fault() {
       1)   0.205 us    |      down_read_trylock();
       1)   0.041 us    |      __might_sleep();
       1)   0.344 us    |      find_vma();
       1)   0.069 us    |      up_read();
       1)   4.692 us    |    }
       1)   5.311 us    |  }
      ...
    
    Link: http://lkml.kernel.org/r/1381739066-7531-5-git-send-email-namhyung@kernel.org
    
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 9f15c0064c50..ec85d48619e1 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -721,6 +721,7 @@ ftrace_push_return_trace(unsigned long ret, unsigned long func, int *depth,
 extern char __irqentry_text_start[];
 extern char __irqentry_text_end[];
 
+#define FTRACE_NOTRACE_DEPTH 65536
 #define FTRACE_RETFUNC_DEPTH 50
 #define FTRACE_RETSTACK_ALLOC_SIZE 32
 extern int register_ftrace_graph(trace_func_graph_ret_t retfunc,

commit de7edd31457b626e54a0b2a7e8ff4d65492f01ad
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Fri Jun 14 16:21:43 2013 -0400

    tracing: Disable tracing on warning
    
    Add a traceoff_on_warning option in both the kernel command line as well
    as a sysctl option. When set, any WARN*() function that is hit will cause
    the tracing_on variable to be cleared, which disables writing to the
    ring buffer.
    
    This is useful especially when tracing a bug with function tracing. When
    a warning is hit, the print caused by the warning can flood the trace with
    the functions that producing the output for the warning. This can make the
    resulting trace useless by either hiding where the bug happened, or worse,
    by overflowing the buffer and losing the trace of the bug totally.
    
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index e48ed1d7876d..9f15c0064c50 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -824,10 +824,15 @@ enum ftrace_dump_mode;
 
 extern enum ftrace_dump_mode ftrace_dump_on_oops;
 
+extern void disable_trace_on_warning(void);
+extern int __disable_trace_on_warning;
+
 #ifdef CONFIG_PREEMPT
 #define INIT_TRACE_RECURSION		.trace_recursion = 0,
 #endif
 
+#else /* CONFIG_TRACING */
+static inline void  disable_trace_on_warning(void) { }
 #endif /* CONFIG_TRACING */
 
 #ifndef INIT_TRACE_RECURSION

commit 1b3d0623cd9da35fe1d52281ed06c2ff543cd660
Author: Li Zefan <lizefan@huawei.com>
Date:   Fri Jun 7 17:02:53 2013 +0800

    ftrace: Remove ftrace_regex_lseek()
    
    This is a leftover after ftrace_regex_lseek() was renamed to
    ftrace_filter_lseek() and then ftrace_filter_lseek() was moved
    out side of CONFIG_DYNAMIC_FTRACE.
    
    Link: http://lkml.kernel.org/r/51B1A1BD.40905@huawei.com
    
    Signed-off-by: Li Zefan <lizefan@huawei.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 99d0fbcbaf79..e48ed1d7876d 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -566,10 +566,6 @@ static inline ssize_t ftrace_filter_write(struct file *file, const char __user *
 			    size_t cnt, loff_t *ppos) { return -ENODEV; }
 static inline ssize_t ftrace_notrace_write(struct file *file, const char __user *ubuf,
 			     size_t cnt, loff_t *ppos) { return -ENODEV; }
-static inline loff_t ftrace_regex_lseek(struct file *file, loff_t offset, int whence)
-{
-	return -ENODEV;
-}
 static inline int
 ftrace_regex_release(struct inode *inode, struct file *file) { return -ENODEV; }
 #endif /* CONFIG_DYNAMIC_FTRACE */

commit f04f24fb7e48d446bd89a01c6056571f25972511
Author: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
Date:   Thu May 9 14:44:17 2013 +0900

    ftrace, kprobes: Fix a deadlock on ftrace_regex_lock
    
    Fix a deadlock on ftrace_regex_lock which happens when setting
    an enable_event trigger on dynamic kprobe event as below.
    
    ----
    sh-2.05b# echo p vfs_symlink > kprobe_events
    sh-2.05b# echo vfs_symlink:enable_event:kprobes:p_vfs_symlink_0 > set_ftrace_filter
    
    =============================================
    [ INFO: possible recursive locking detected ]
    3.9.0+ #35 Not tainted
    ---------------------------------------------
    sh/72 is trying to acquire lock:
     (ftrace_regex_lock){+.+.+.}, at: [<ffffffff810ba6c1>] ftrace_set_hash+0x81/0x1f0
    
    but task is already holding lock:
     (ftrace_regex_lock){+.+.+.}, at: [<ffffffff810b7cbd>] ftrace_regex_write.isra.29.part.30+0x3d/0x220
    
    other info that might help us debug this:
     Possible unsafe locking scenario:
    
           CPU0
           ----
      lock(ftrace_regex_lock);
      lock(ftrace_regex_lock);
    
     *** DEADLOCK ***
    ----
    
    To fix that, this introduces a finer regex_lock for each ftrace_ops.
    ftrace_regex_lock is too big of a lock which protects all
    filter/notrace_hash operations, but it doesn't need to be a global
    lock after supporting multiple ftrace_ops because each ftrace_ops
    has its own filter/notrace_hash.
    
    Link: http://lkml.kernel.org/r/20130509054417.30398.84254.stgit@mhiramat-M0-7522
    
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Tom Zanussi <tom.zanussi@intel.com>
    Signed-off-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    [ Added initialization flag and automate mutex initialization for
      non ftrace.c ftrace_probes. ]
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index f83e17a40e8b..99d0fbcbaf79 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -90,6 +90,8 @@ typedef void (*ftrace_func_t)(unsigned long ip, unsigned long parent_ip,
  *            not set this, then the ftrace infrastructure will add recursion
  *            protection for the caller.
  * STUB   - The ftrace_ops is just a place holder.
+ * INITIALIZED - The ftrace_ops has already been initialized (first use time
+ *            register_ftrace_function() is called, it will initialized the ops)
  */
 enum {
 	FTRACE_OPS_FL_ENABLED			= 1 << 0,
@@ -100,6 +102,7 @@ enum {
 	FTRACE_OPS_FL_SAVE_REGS_IF_SUPPORTED	= 1 << 5,
 	FTRACE_OPS_FL_RECURSION_SAFE		= 1 << 6,
 	FTRACE_OPS_FL_STUB			= 1 << 7,
+	FTRACE_OPS_FL_INITIALIZED		= 1 << 8,
 };
 
 struct ftrace_ops {
@@ -110,6 +113,7 @@ struct ftrace_ops {
 #ifdef CONFIG_DYNAMIC_FTRACE
 	struct ftrace_hash		*notrace_hash;
 	struct ftrace_hash		*filter_hash;
+	struct mutex			regex_lock;
 #endif
 };
 

commit 9e8529afc4518f4e5d610001545ebc97e1333c79
Merge: ec25e246b94a 4c69e6ea415a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Apr 29 13:55:38 2013 -0700

    Merge tag 'trace-3.10' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull tracing updates from Steven Rostedt:
     "Along with the usual minor fixes and clean ups there are a few major
      changes with this pull request.
    
       1) Multiple buffers for the ftrace facility
    
      This feature has been requested by many people over the last few
      years.  I even heard that Google was about to implement it themselves.
      I finally had time and cleaned up the code such that you can now
      create multiple instances of the ftrace buffer and have different
      events go to different buffers.  This way, a low frequency event will
      not be lost in the noise of a high frequency event.
    
      Note, currently only events can go to different buffers, the tracers
      (ie function, function_graph and the latency tracers) still can only
      be written to the main buffer.
    
       2) The function tracer triggers have now been extended.
    
      The function tracer had two triggers.  One to enable tracing when a
      function is hit, and one to disable tracing.  Now you can record a
      stack trace on a single (or many) function(s), take a snapshot of the
      buffer (copy it to the snapshot buffer), and you can enable or disable
      an event to be traced when a function is hit.
    
       3) A perf clock has been added.
    
      A "perf" clock can be chosen to be used when tracing.  This will cause
      ftrace to use the same clock as perf uses, and hopefully this will
      make it easier to interleave the perf and ftrace data for analysis."
    
    * tag 'trace-3.10' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace: (82 commits)
      tracepoints: Prevent null probe from being added
      tracing: Compare to 1 instead of zero for is_signed_type()
      tracing: Remove obsolete macro guard _TRACE_PROFILE_INIT
      ftrace: Get rid of ftrace_profile_bits
      tracing: Check return value of tracing_init_dentry()
      tracing: Get rid of unneeded key calculation in ftrace_hash_move()
      tracing: Reset ftrace_graph_filter_enabled if count is zero
      tracing: Fix off-by-one on allocating stat->pages
      kernel: tracing: Use strlcpy instead of strncpy
      tracing: Update debugfs README file
      tracing: Fix ftrace_dump()
      tracing: Rename trace_event_mutex to trace_event_sem
      tracing: Fix comment about prefix in arch_syscall_match_sym_name()
      tracing: Convert trace_destroy_fields() to static
      tracing: Move find_event_field() into trace_events.c
      tracing: Use TRACE_MAX_PRINT instead of constant
      tracing: Use pr_warn_once instead of open coded implementation
      ring-buffer: Add ring buffer startup selftest
      tracing: Bring Documentation/trace/ftrace.txt up to date
      tracing: Add "perf" trace_clock
      ...
    
    Conflicts:
            kernel/trace/ftrace.c
            kernel/trace/trace.c

commit 7f49ef69db6bbf756c0abca7e9b65b32e999eec8
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Fri Apr 12 16:40:13 2013 -0400

    ftrace: Move ftrace_filter_lseek out of CONFIG_DYNAMIC_FTRACE section
    
    As ftrace_filter_lseek is now used with ftrace_pid_fops, it needs to
    be moved out of the #ifdef CONFIG_DYNAMIC_FTRACE section as the
    ftrace_pid_fops is defined when DYNAMIC_FTRACE is not.
    
    Cc: stable@vger.kernel.org
    Cc: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index eb3ce327b975..52da2a250795 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -396,7 +396,6 @@ ssize_t ftrace_filter_write(struct file *file, const char __user *ubuf,
 			    size_t cnt, loff_t *ppos);
 ssize_t ftrace_notrace_write(struct file *file, const char __user *ubuf,
 			     size_t cnt, loff_t *ppos);
-loff_t ftrace_filter_lseek(struct file *file, loff_t offset, int whence);
 int ftrace_regex_release(struct inode *inode, struct file *file);
 
 void __init
@@ -569,6 +568,8 @@ static inline int
 ftrace_regex_release(struct inode *inode, struct file *file) { return -ENODEV; }
 #endif /* CONFIG_DYNAMIC_FTRACE */
 
+loff_t ftrace_filter_lseek(struct file *file, loff_t offset, int whence);
+
 /* totally disable ftrace - can not re-enable after this */
 void ftrace_kill(void);
 

commit 6a76f8c0ab19f215af2a3442870eeb5f0e81998d
Author: Namhyung Kim <namhyung.kim@lge.com>
Date:   Thu Apr 11 15:55:01 2013 +0900

    tracing: Fix possible NULL pointer dereferences
    
    Currently set_ftrace_pid and set_graph_function files use seq_lseek
    for their fops.  However seq_open() is called only for FMODE_READ in
    the fops->open() so that if an user tries to seek one of those file
    when she open it for writing, it sees NULL seq_file and then panic.
    
    It can be easily reproduced with following command:
    
      $ cd /sys/kernel/debug/tracing
      $ echo 1234 | sudo tee -a set_ftrace_pid
    
    In this example, GNU coreutils' tee opens the file with fopen(, "a")
    and then the fopen() internally calls lseek().
    
    Link: http://lkml.kernel.org/r/1365663302-2170-1-git-send-email-namhyung@kernel.org
    
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Namhyung Kim <namhyung.kim@lge.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 167abf907802..eb3ce327b975 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -396,7 +396,7 @@ ssize_t ftrace_filter_write(struct file *file, const char __user *ubuf,
 			    size_t cnt, loff_t *ppos);
 ssize_t ftrace_notrace_write(struct file *file, const char __user *ubuf,
 			     size_t cnt, loff_t *ppos);
-loff_t ftrace_regex_lseek(struct file *file, loff_t offset, int whence);
+loff_t ftrace_filter_lseek(struct file *file, loff_t offset, int whence);
 int ftrace_regex_release(struct inode *inode, struct file *file);
 
 void __init

commit 395b97a3aeff0b8d949ee3e67bf8c11c5ffd6861
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed Mar 27 09:31:28 2013 -0400

    ftrace: Do not call stub functions in control loop
    
    The function tracing control loop used by perf spits out a warning
    if the called function is not a control function. This is because
    the control function references a per cpu allocated data structure
    on struct ftrace_ops that is not allocated for other types of
    functions.
    
    commit 0a016409e42 "ftrace: Optimize the function tracer list loop"
    
    Had an optimization done to all function tracing loops to optimize
    for a single registered ops. Unfortunately, this allows for a slight
    race when tracing starts or ends, where the stub function might be
    called after the current registered ops is removed. In this case we
    get the following dump:
    
    root# perf stat -e ftrace:function sleep 1
    [   74.339105] WARNING: at include/linux/ftrace.h:209 ftrace_ops_control_func+0xde/0xf0()
    [   74.349522] Hardware name: PRIMERGY RX200 S6
    [   74.357149] Modules linked in: sg igb iTCO_wdt ptp pps_core iTCO_vendor_support i7core_edac dca lpc_ich i2c_i801 coretemp edac_core crc32c_intel mfd_core ghash_clmulni_intel dm_multipath acpi_power_meter pcspk
    r microcode vhost_net tun macvtap macvlan nfsd kvm_intel kvm auth_rpcgss nfs_acl lockd sunrpc uinput xfs libcrc32c sd_mod crc_t10dif sr_mod cdrom mgag200 i2c_algo_bit drm_kms_helper ttm qla2xxx mptsas ahci drm li
    bahci scsi_transport_sas mptscsih libata scsi_transport_fc i2c_core mptbase scsi_tgt dm_mirror dm_region_hash dm_log dm_mod
    [   74.446233] Pid: 1377, comm: perf Tainted: G        W    3.9.0-rc1 #1
    [   74.453458] Call Trace:
    [   74.456233]  [<ffffffff81062e3f>] warn_slowpath_common+0x7f/0xc0
    [   74.462997]  [<ffffffff810fbc60>] ? rcu_note_context_switch+0xa0/0xa0
    [   74.470272]  [<ffffffff811041a2>] ? __unregister_ftrace_function+0xa2/0x1a0
    [   74.478117]  [<ffffffff81062e9a>] warn_slowpath_null+0x1a/0x20
    [   74.484681]  [<ffffffff81102ede>] ftrace_ops_control_func+0xde/0xf0
    [   74.491760]  [<ffffffff8162f400>] ftrace_call+0x5/0x2f
    [   74.497511]  [<ffffffff8162f400>] ? ftrace_call+0x5/0x2f
    [   74.503486]  [<ffffffff8162f400>] ? ftrace_call+0x5/0x2f
    [   74.509500]  [<ffffffff810fbc65>] ? synchronize_sched+0x5/0x50
    [   74.516088]  [<ffffffff816254d5>] ? _cond_resched+0x5/0x40
    [   74.522268]  [<ffffffff810fbc65>] ? synchronize_sched+0x5/0x50
    [   74.528837]  [<ffffffff811041a2>] ? __unregister_ftrace_function+0xa2/0x1a0
    [   74.536696]  [<ffffffff816254d5>] ? _cond_resched+0x5/0x40
    [   74.542878]  [<ffffffff8162402d>] ? mutex_lock+0x1d/0x50
    [   74.548869]  [<ffffffff81105c67>] unregister_ftrace_function+0x27/0x50
    [   74.556243]  [<ffffffff8111eadf>] perf_ftrace_event_register+0x9f/0x140
    [   74.563709]  [<ffffffff816254d5>] ? _cond_resched+0x5/0x40
    [   74.569887]  [<ffffffff8162402d>] ? mutex_lock+0x1d/0x50
    [   74.575898]  [<ffffffff8111e94e>] perf_trace_destroy+0x2e/0x50
    [   74.582505]  [<ffffffff81127ba9>] tp_perf_event_destroy+0x9/0x10
    [   74.589298]  [<ffffffff811295d0>] free_event+0x70/0x1a0
    [   74.595208]  [<ffffffff8112a579>] perf_event_release_kernel+0x69/0xa0
    [   74.602460]  [<ffffffff816254d5>] ? _cond_resched+0x5/0x40
    [   74.608667]  [<ffffffff8112a640>] put_event+0x90/0xc0
    [   74.614373]  [<ffffffff8112a740>] perf_release+0x10/0x20
    [   74.620367]  [<ffffffff811a3044>] __fput+0xf4/0x280
    [   74.625894]  [<ffffffff811a31de>] ____fput+0xe/0x10
    [   74.631387]  [<ffffffff81083697>] task_work_run+0xa7/0xe0
    [   74.637452]  [<ffffffff81014981>] do_notify_resume+0x71/0xb0
    [   74.643843]  [<ffffffff8162fa92>] int_signal+0x12/0x17
    
    To fix this a new ftrace_ops flag is added that denotes the ftrace_list_end
    ftrace_ops stub as just that, a stub. This flag is now checked in the
    control loop and the function is not called if the flag is set.
    
    Thanks to Jovi for not just reporting the bug, but also pointing out
    where the bug was in the code.
    
    Link: http://lkml.kernel.org/r/514A8855.7090402@redhat.com
    Link: http://lkml.kernel.org/r/1364377499-1900-15-git-send-email-jovi.zhangwei@huawei.com
    
    Tested-by: WANG Chao <chaowang@redhat.com>
    Reported-by: WANG Chao <chaowang@redhat.com>
    Reported-by: zhangwei(Jovi) <jovi.zhangwei@huawei.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index e5ca8ef50e9b..167abf907802 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -89,6 +89,7 @@ typedef void (*ftrace_func_t)(unsigned long ip, unsigned long parent_ip,
  *            that the call back has its own recursion protection. If it does
  *            not set this, then the ftrace infrastructure will add recursion
  *            protection for the caller.
+ * STUB   - The ftrace_ops is just a place holder.
  */
 enum {
 	FTRACE_OPS_FL_ENABLED			= 1 << 0,
@@ -98,6 +99,7 @@ enum {
 	FTRACE_OPS_FL_SAVE_REGS			= 1 << 4,
 	FTRACE_OPS_FL_SAVE_REGS_IF_SUPPORTED	= 1 << 5,
 	FTRACE_OPS_FL_RECURSION_SAFE		= 1 << 6,
+	FTRACE_OPS_FL_STUB			= 1 << 7,
 };
 
 struct ftrace_ops {

commit e67efb93f0e9130174293ffaa5975f87b301b531
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Mar 12 15:07:59 2013 -0400

    ftrace: Clean up function probe methods
    
    When a function probe is created, each function that the probe is
    attached to, a "callback" method is called. On release of the probe,
    each function entry calls the "free" method.
    
    First, "callback" is a confusing name and does not really match what
    it does. Callback sounds like it will be called when the probe
    triggers. But that's not the case. This is really an "init" function,
    so lets rename it as such.
    
    Secondly, both "init" and "free" do not pass enough information back
    to the handlers. Pass back the ops, ip and data for each time the
    method is called. We have the information, might as well use it.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index e5ca8ef50e9b..832422d706f4 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -259,8 +259,10 @@ struct ftrace_probe_ops {
 	void			(*func)(unsigned long ip,
 					unsigned long parent_ip,
 					void **data);
-	int			(*callback)(unsigned long ip, void **data);
-	void			(*free)(void **data);
+	int			(*init)(struct ftrace_probe_ops *ops,
+					unsigned long ip, void **data);
+	void			(*free)(struct ftrace_probe_ops *ops,
+					unsigned long ip, void **data);
 	int			(*print)(struct seq_file *m,
 					 unsigned long ip,
 					 struct ftrace_probe_ops *ops,

commit 06aeaaeabf69da4a3e86df532425640f51b01cef
Author: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
Date:   Fri Sep 28 17:15:17 2012 +0900

    ftrace: Move ARCH_SUPPORTS_FTRACE_SAVE_REGS in Kconfig
    
    Move SAVE_REGS support flag into Kconfig and rename
    it to CONFIG_DYNAMIC_FTRACE_WITH_REGS. This also introduces
    CONFIG_HAVE_DYNAMIC_FTRACE_WITH_REGS which indicates
    the architecture depending part of ftrace has a code
    that saves full registers.
    On the other hand, CONFIG_DYNAMIC_FTRACE_WITH_REGS indicates
    the code is enabled.
    
    Link: http://lkml.kernel.org/r/20120928081516.3560.72534.stgit@ltc138.sdl.hitachi.co.jp
    
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 92691d85c320..e5ca8ef50e9b 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -74,7 +74,7 @@ typedef void (*ftrace_func_t)(unsigned long ip, unsigned long parent_ip,
  * SAVE_REGS - The ftrace_ops wants regs saved at each function called
  *            and passed to the callback. If this flag is set, but the
  *            architecture does not support passing regs
- *            (ARCH_SUPPORTS_FTRACE_SAVE_REGS is not defined), then the
+ *            (CONFIG_DYNAMIC_FTRACE_WITH_REGS is not defined), then the
  *            ftrace_ops will fail to register, unless the next flag
  *            is set.
  * SAVE_REGS_IF_SUPPORTED - This is the same as SAVE_REGS, but if the
@@ -418,7 +418,7 @@ void ftrace_modify_all_code(int command);
 #endif
 
 #ifndef FTRACE_REGS_ADDR
-#ifdef ARCH_SUPPORTS_FTRACE_SAVE_REGS
+#ifdef CONFIG_DYNAMIC_FTRACE_WITH_REGS
 # define FTRACE_REGS_ADDR ((unsigned long)ftrace_regs_caller)
 #else
 # define FTRACE_REGS_ADDR FTRACE_ADDR
@@ -480,7 +480,7 @@ extern int ftrace_make_nop(struct module *mod,
  */
 extern int ftrace_make_call(struct dyn_ftrace *rec, unsigned long addr);
 
-#ifdef ARCH_SUPPORTS_FTRACE_SAVE_REGS
+#ifdef CONFIG_DYNAMIC_FTRACE_WITH_REGS
 /**
  * ftrace_modify_call - convert from one addr to another (no nop)
  * @rec: the mcount call site record

commit 965c8e59cfcf845ecde2265a1d1bfee5f011d302
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Mon Dec 17 15:59:39 2012 -0800

    lseek: the "whence" argument is called "whence"
    
    But the kernel decided to call it "origin" instead.  Fix most of the
    sites.
    
    Acked-by: Hugh Dickins <hughd@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index a52f2f4fe030..92691d85c320 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -394,7 +394,7 @@ ssize_t ftrace_filter_write(struct file *file, const char __user *ubuf,
 			    size_t cnt, loff_t *ppos);
 ssize_t ftrace_notrace_write(struct file *file, const char __user *ubuf,
 			     size_t cnt, loff_t *ppos);
-loff_t ftrace_regex_lseek(struct file *file, loff_t offset, int origin);
+loff_t ftrace_regex_lseek(struct file *file, loff_t offset, int whence);
 int ftrace_regex_release(struct inode *inode, struct file *file);
 
 void __init
@@ -559,7 +559,7 @@ static inline ssize_t ftrace_filter_write(struct file *file, const char __user *
 			    size_t cnt, loff_t *ppos) { return -ENODEV; }
 static inline ssize_t ftrace_notrace_write(struct file *file, const char __user *ubuf,
 			     size_t cnt, loff_t *ppos) { return -ENODEV; }
-static inline loff_t ftrace_regex_lseek(struct file *file, loff_t offset, int origin)
+static inline loff_t ftrace_regex_lseek(struct file *file, loff_t offset, int whence)
 {
 	return -ENODEV;
 }

commit 4dc936769e8a6382a4cc12375e8a4daa2b829fda
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed Jun 6 13:45:31 2012 -0400

    ftrace: Make ftrace_location() a nop on !DYNAMIC_FTRACE
    
    When CONFIG_DYNAMIC_FTRACE is not set, ftrace_location() is not defined.
    If a user (like kprobes) references this function, it will break
    the compile when CONFIG_DYNAMIC_FTRACE is not set.
    
    Add ftrace_location() as a nop (return 0) when DYNAMIC_FTRACE
    is not defined.
    
    Link: http://lkml.kernel.org/r/20120612225426.961092717@goodmis.org
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 3e711122f66c..a52f2f4fe030 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -520,7 +520,7 @@ extern int skip_trace(unsigned long ip);
 
 extern void ftrace_disable_daemon(void);
 extern void ftrace_enable_daemon(void);
-#else
+#else /* CONFIG_DYNAMIC_FTRACE */
 static inline int skip_trace(unsigned long ip) { return 0; }
 static inline int ftrace_force_update(void) { return 0; }
 static inline void ftrace_disable_daemon(void) { }
@@ -538,6 +538,10 @@ static inline int ftrace_text_reserved(void *start, void *end)
 {
 	return 0;
 }
+static inline unsigned long ftrace_location(unsigned long ip)
+{
+	return 0;
+}
 
 /*
  * Again users of functions that have ftrace_ops may not

commit 647664eaf4033501739ac1f42dd52ce8c9266ccc
Author: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
Date:   Tue Jun 5 19:28:08 2012 +0900

    ftrace: add ftrace_set_filter_ip() for address based filter
    
    Add a new filter update interface ftrace_set_filter_ip()
    to set ftrace filter by ip address, not only glob pattern.
    
    Link: http://lkml.kernel.org/r/20120605102808.27845.67952.stgit@localhost.localdomain
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: "Frank Ch. Eigler" <fche@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 9962e954a633..3e711122f66c 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -317,6 +317,8 @@ struct dyn_ftrace {
 };
 
 int ftrace_force_update(void);
+int ftrace_set_filter_ip(struct ftrace_ops *ops, unsigned long ip,
+			 int remove, int reset);
 int ftrace_set_filter(struct ftrace_ops *ops, unsigned char *buf,
 		       int len, int reset);
 int ftrace_set_notrace(struct ftrace_ops *ops, unsigned char *buf,
@@ -544,6 +546,7 @@ static inline int ftrace_text_reserved(void *start, void *end)
  */
 #define ftrace_regex_open(ops, flag, inod, file) ({ -ENODEV; })
 #define ftrace_set_early_filter(ops, buf, enable) do { } while (0)
+#define ftrace_set_filter_ip(ops, ip, remove, reset) ({ -ENODEV; })
 #define ftrace_set_filter(ops, buf, len, reset) ({ -ENODEV; })
 #define ftrace_set_notrace(ops, buf, len, reset) ({ -ENODEV; })
 #define ftrace_free_filter(ops) do { } while (0)

commit ea701f11da44b44907af226fe5a5f57d2f26eeb2
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Jul 20 13:08:05 2012 -0400

    ftrace: Add selftest to test function trace recursion protection
    
    Add selftests to test the function tracing recursion protection actually
    does work. It also tests if a ftrace_ops states it will perform its own
    protection. Although, even if the ftrace_ops states it will protect itself,
    the ftrace infrastructure may still provide protection if the arch does
    not support all features or another ftrace_ops is registered.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 65a14e47a0db..9962e954a633 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -220,6 +220,10 @@ extern void ftrace_stub(unsigned long a0, unsigned long a1,
  */
 #define register_ftrace_function(ops) ({ 0; })
 #define unregister_ftrace_function(ops) ({ 0; })
+static inline int ftrace_nr_registered_ops(void)
+{
+	return 0;
+}
 static inline void clear_ftrace_function(void) { }
 static inline void ftrace_kill(void) { }
 static inline void ftrace_stop(void) { }
@@ -275,6 +279,8 @@ extern void unregister_ftrace_function_probe_all(char *glob);
 
 extern int ftrace_text_reserved(void *start, void *end);
 
+extern int ftrace_nr_registered_ops(void);
+
 /*
  * The dyn_ftrace record's flags field is split into two parts.
  * the first part which is '0-FTRACE_REF_MAX' is a counter of

commit 4740974a6844156c14d741b0080b59d275679a23
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Jul 20 11:04:44 2012 -0400

    ftrace: Add default recursion protection for function tracing
    
    As more users of the function tracer utility are being added, they do
    not always add the necessary recursion protection. To protect from
    function recursion due to tracing, if the callback ftrace_ops does not
    specifically specify that it protects against recursion (by setting
    the FTRACE_OPS_FL_RECURSION_SAFE flag), the list operation will be
    called by the mcount trampoline which adds recursion protection.
    
    If the flag is set, then the function will be called directly with no
    extra protection.
    
    Note, the list operation is called if more than one function callback
    is registered, or if the arch does not support all of the function
    tracer features.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index ab39990cc43f..65a14e47a0db 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -85,6 +85,10 @@ typedef void (*ftrace_func_t)(unsigned long ip, unsigned long parent_ip,
  *            passing regs to the handler.
  *            Note, if this flag is set, the SAVE_REGS flag will automatically
  *            get set upon registering the ftrace_ops, if the arch supports it.
+ * RECURSION_SAFE - The ftrace_ops can set this to tell the ftrace infrastructure
+ *            that the call back has its own recursion protection. If it does
+ *            not set this, then the ftrace infrastructure will add recursion
+ *            protection for the caller.
  */
 enum {
 	FTRACE_OPS_FL_ENABLED			= 1 << 0,
@@ -93,6 +97,7 @@ enum {
 	FTRACE_OPS_FL_CONTROL			= 1 << 3,
 	FTRACE_OPS_FL_SAVE_REGS			= 1 << 4,
 	FTRACE_OPS_FL_SAVE_REGS_IF_SUPPORTED	= 1 << 5,
+	FTRACE_OPS_FL_RECURSION_SAFE		= 1 << 6,
 };
 
 struct ftrace_ops {

commit 08f6fba503111e0336f2b4d6915a4a18f9b60e51
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon Apr 30 16:20:23 2012 -0400

    ftrace/x86: Add separate function to save regs
    
    Add a way to have different functions calling different trampolines.
    If a ftrace_ops wants regs saved on the return, then have only the
    functions with ops registered to save regs. Functions registered by
    other ops would not be affected, unless the functions overlap.
    
    If one ftrace_ops registered functions A, B and C and another ops
    registered fucntions to save regs on A, and D, then only functions
    A and D would be saving regs. Function B and C would work as normal.
    Although A is registered by both ops: normal and saves regs; this is fine
    as saving the regs is needed to satisfy one of the ops that calls it
    but the regs are ignored by the other ops function.
    
    x86_64 implements the full regs saving, and i386 just passes a NULL
    for regs to satisfy the ftrace_ops passing. Where an arch must supply
    both regs and ftrace_ops parameters, even if regs is just NULL.
    
    It is OK for an arch to pass NULL regs. All function trace users that
    require regs passing must add the flag FTRACE_OPS_FL_SAVE_REGS when
    registering the ftrace_ops. If the arch does not support saving regs
    then the ftrace_ops will fail to register. The flag
    FTRACE_OPS_FL_SAVE_REGS_IF_SUPPORTED may be set that will prevent the
    ftrace_ops from failing to register. In this case, the handler may
    either check if regs is not NULL or check if ARCH_SUPPORTS_FTRACE_SAVE_REGS.
    If the arch supports passing regs it will set this macro and pass regs
    for ops that request them. All other archs will just pass NULL.
    
    Link: Link: http://lkml.kernel.org/r/20120711195745.107705970@goodmis.org
    
    Cc: Alexander van Heukelum <heukelum@fastmail.fm>
    Reviewed-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index e4202881fb00..ab39990cc43f 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -71,12 +71,28 @@ typedef void (*ftrace_func_t)(unsigned long ip, unsigned long parent_ip,
  *           could be controled by following calls:
  *             ftrace_function_local_enable
  *             ftrace_function_local_disable
+ * SAVE_REGS - The ftrace_ops wants regs saved at each function called
+ *            and passed to the callback. If this flag is set, but the
+ *            architecture does not support passing regs
+ *            (ARCH_SUPPORTS_FTRACE_SAVE_REGS is not defined), then the
+ *            ftrace_ops will fail to register, unless the next flag
+ *            is set.
+ * SAVE_REGS_IF_SUPPORTED - This is the same as SAVE_REGS, but if the
+ *            handler can handle an arch that does not save regs
+ *            (the handler tests if regs == NULL), then it can set
+ *            this flag instead. It will not fail registering the ftrace_ops
+ *            but, the regs field will be NULL if the arch does not support
+ *            passing regs to the handler.
+ *            Note, if this flag is set, the SAVE_REGS flag will automatically
+ *            get set upon registering the ftrace_ops, if the arch supports it.
  */
 enum {
-	FTRACE_OPS_FL_ENABLED		= 1 << 0,
-	FTRACE_OPS_FL_GLOBAL		= 1 << 1,
-	FTRACE_OPS_FL_DYNAMIC		= 1 << 2,
-	FTRACE_OPS_FL_CONTROL		= 1 << 3,
+	FTRACE_OPS_FL_ENABLED			= 1 << 0,
+	FTRACE_OPS_FL_GLOBAL			= 1 << 1,
+	FTRACE_OPS_FL_DYNAMIC			= 1 << 2,
+	FTRACE_OPS_FL_CONTROL			= 1 << 3,
+	FTRACE_OPS_FL_SAVE_REGS			= 1 << 4,
+	FTRACE_OPS_FL_SAVE_REGS_IF_SUPPORTED	= 1 << 5,
 };
 
 struct ftrace_ops {
@@ -254,12 +270,31 @@ extern void unregister_ftrace_function_probe_all(char *glob);
 
 extern int ftrace_text_reserved(void *start, void *end);
 
+/*
+ * The dyn_ftrace record's flags field is split into two parts.
+ * the first part which is '0-FTRACE_REF_MAX' is a counter of
+ * the number of callbacks that have registered the function that
+ * the dyn_ftrace descriptor represents.
+ *
+ * The second part is a mask:
+ *  ENABLED - the function is being traced
+ *  REGS    - the record wants the function to save regs
+ *  REGS_EN - the function is set up to save regs.
+ *
+ * When a new ftrace_ops is registered and wants a function to save
+ * pt_regs, the rec->flag REGS is set. When the function has been
+ * set up to save regs, the REG_EN flag is set. Once a function
+ * starts saving regs it will do so until all ftrace_ops are removed
+ * from tracing that function.
+ */
 enum {
-	FTRACE_FL_ENABLED	= (1 << 30),
+	FTRACE_FL_ENABLED	= (1UL << 29),
+	FTRACE_FL_REGS		= (1UL << 30),
+	FTRACE_FL_REGS_EN	= (1UL << 31)
 };
 
-#define FTRACE_FL_MASK		(0x3UL << 30)
-#define FTRACE_REF_MAX		((1 << 30) - 1)
+#define FTRACE_FL_MASK		(0x7UL << 29)
+#define FTRACE_REF_MAX		((1UL << 29) - 1)
 
 struct dyn_ftrace {
 	union {
@@ -290,9 +325,23 @@ enum {
 	FTRACE_STOP_FUNC_RET		= (1 << 4),
 };
 
+/*
+ * The FTRACE_UPDATE_* enum is used to pass information back
+ * from the ftrace_update_record() and ftrace_test_record()
+ * functions. These are called by the code update routines
+ * to find out what is to be done for a given function.
+ *
+ *  IGNORE           - The function is already what we want it to be
+ *  MAKE_CALL        - Start tracing the function
+ *  MODIFY_CALL      - Stop saving regs for the function
+ *  MODIFY_CALL_REGS - Start saving regs for the function
+ *  MAKE_NOP         - Stop tracing the function
+ */
 enum {
 	FTRACE_UPDATE_IGNORE,
 	FTRACE_UPDATE_MAKE_CALL,
+	FTRACE_UPDATE_MODIFY_CALL,
+	FTRACE_UPDATE_MODIFY_CALL_REGS,
 	FTRACE_UPDATE_MAKE_NOP,
 };
 
@@ -344,7 +393,9 @@ extern int ftrace_dyn_arch_init(void *data);
 extern void ftrace_replace_code(int enable);
 extern int ftrace_update_ftrace_func(ftrace_func_t func);
 extern void ftrace_caller(void);
+extern void ftrace_regs_caller(void);
 extern void ftrace_call(void);
+extern void ftrace_regs_call(void);
 extern void mcount_call(void);
 
 void ftrace_modify_all_code(int command);
@@ -352,6 +403,15 @@ void ftrace_modify_all_code(int command);
 #ifndef FTRACE_ADDR
 #define FTRACE_ADDR ((unsigned long)ftrace_caller)
 #endif
+
+#ifndef FTRACE_REGS_ADDR
+#ifdef ARCH_SUPPORTS_FTRACE_SAVE_REGS
+# define FTRACE_REGS_ADDR ((unsigned long)ftrace_regs_caller)
+#else
+# define FTRACE_REGS_ADDR FTRACE_ADDR
+#endif
+#endif
+
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
 extern void ftrace_graph_caller(void);
 extern int ftrace_enable_ftrace_graph_caller(void);
@@ -407,6 +467,39 @@ extern int ftrace_make_nop(struct module *mod,
  */
 extern int ftrace_make_call(struct dyn_ftrace *rec, unsigned long addr);
 
+#ifdef ARCH_SUPPORTS_FTRACE_SAVE_REGS
+/**
+ * ftrace_modify_call - convert from one addr to another (no nop)
+ * @rec: the mcount call site record
+ * @old_addr: the address expected to be currently called to
+ * @addr: the address to change to
+ *
+ * This is a very sensitive operation and great care needs
+ * to be taken by the arch.  The operation should carefully
+ * read the location, check to see if what is read is indeed
+ * what we expect it to be, and then on success of the compare,
+ * it should write to the location.
+ *
+ * The code segment at @rec->ip should be a caller to @old_addr
+ *
+ * Return must be:
+ *  0 on success
+ *  -EFAULT on error reading the location
+ *  -EINVAL on a failed compare of the contents
+ *  -EPERM  on error writing to the location
+ * Any other value will be considered a failure.
+ */
+extern int ftrace_modify_call(struct dyn_ftrace *rec, unsigned long old_addr,
+			      unsigned long addr);
+#else
+/* Should never be called */
+static inline int ftrace_modify_call(struct dyn_ftrace *rec, unsigned long old_addr,
+				     unsigned long addr)
+{
+	return -EINVAL;
+}
+#endif
+
 /* May be defined in arch */
 extern int ftrace_arch_read_dyn_info(char *buf, int size);
 

commit a1e2e31d175a1349274eba3465d17616c6725f8c
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Aug 9 12:50:46 2011 -0400

    ftrace: Return pt_regs to function trace callback
    
    Return as the 4th paramater to the function tracer callback the pt_regs.
    
    Later patches that implement regs passing for the architectures will require
    having the ftrace_ops set the SAVE_REGS flag, which will tell the arch
    to take the time to pass a full set of pt_regs to the ftrace_ops callback
    function. If the arch does not support it then it should pass NULL.
    
    If an arch can pass full regs, then it should define:
     ARCH_SUPPORTS_FTRACE_SAVE_REGS to 1
    
    Link: http://lkml.kernel.org/r/20120702201821.019966811@goodmis.org
    
    Reviewed-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 3651fdc3bec9..e4202881fb00 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -10,6 +10,7 @@
 #include <linux/kallsyms.h>
 #include <linux/linkage.h>
 #include <linux/bitops.h>
+#include <linux/ptrace.h>
 #include <linux/ktime.h>
 #include <linux/sched.h>
 #include <linux/types.h>
@@ -54,7 +55,7 @@ ftrace_enable_sysctl(struct ctl_table *table, int write,
 struct ftrace_ops;
 
 typedef void (*ftrace_func_t)(unsigned long ip, unsigned long parent_ip,
-			      struct ftrace_ops *op);
+			      struct ftrace_ops *op, struct pt_regs *regs);
 
 /*
  * FTRACE_OPS_FL_* bits denote the state of ftrace_ops struct and are
@@ -188,7 +189,8 @@ static inline int ftrace_function_local_disabled(struct ftrace_ops *ops)
 	return *this_cpu_ptr(ops->disabled);
 }
 
-extern void ftrace_stub(unsigned long a0, unsigned long a1, struct ftrace_ops *op);
+extern void ftrace_stub(unsigned long a0, unsigned long a1,
+			struct ftrace_ops *op, struct pt_regs *regs);
 
 #else /* !CONFIG_FUNCTION_TRACER */
 /*

commit ccf3672d530170c98c734dfc5db07d64bcbad2ad
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Jun 5 09:44:25 2012 -0400

    ftrace: Consolidate arch dependent functions with 'list' function
    
    As the function tracer starts to get more features, the support for
    theses features will spread out throughout the different architectures
    over time. These features boil down to what each arch does in the
    mcount trampoline (the ftrace_caller).
    
    Currently there's two features that are not the same throughout the
    archs.
    
     1) Support to stop function tracing before the callback
     2) passing of the ftrace ops
    
    Both of these require placing an indirect function to support the
    features if the mcount trampoline does not.
    
    On a side note, for all architectures, when more than one callback
    is registered to the function tracer, an intermediate 'list' function
    is called by the mcount trampoline to iterate through the callbacks
    that are registered.
    
    Instead of making a separate function for each of these features,
    and requiring several indirect calls, just use the single 'list' function
    as the intermediate, to handle all cases. If an arch does not support
    the 'stop function tracing' or the passing of ftrace ops, just force
    it to use the list function that will handle the features required.
    
    This makes the code cleaner and simpler and removes a lot of
     #ifdefs in the code.
    
    Link: http://lkml.kernel.org/r/20120612225424.495625483@goodmis.org
    
    Reviewed-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 2d5964119885..3651fdc3bec9 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -27,6 +27,19 @@
 #define ARCH_SUPPORTS_FTRACE_OPS 0
 #endif
 
+/*
+ * If the arch's mcount caller does not support all of ftrace's
+ * features, then it must call an indirect function that
+ * does. Or at least does enough to prevent any unwelcomed side effects.
+ */
+#if !defined(CONFIG_HAVE_FUNCTION_TRACE_MCOUNT_TEST) || \
+	!ARCH_SUPPORTS_FTRACE_OPS
+# define FTRACE_FORCE_LIST_FUNC 1
+#else
+# define FTRACE_FORCE_LIST_FUNC 0
+#endif
+
+
 struct module;
 struct ftrace_hash;
 

commit 2f5f6ad9390c1ebbf738d130dbfe80b60eaa167e
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon Aug 8 16:57:47 2011 -0400

    ftrace: Pass ftrace_ops as third parameter to function trace callback
    
    Currently the function trace callback receives only the ip and parent_ip
    of the function that it traced. It would be more powerful to also return
    the ops that registered the function as well. This allows the same function
    to act differently depending on what ftrace_ops registered it.
    
    Link: http://lkml.kernel.org/r/20120612225424.267254552@goodmis.org
    
    Reviewed-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 55e6d63d46d0..2d5964119885 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -18,6 +18,15 @@
 
 #include <asm/ftrace.h>
 
+/*
+ * If the arch supports passing the variable contents of
+ * function_trace_op as the third parameter back from the
+ * mcount call, then the arch should define this as 1.
+ */
+#ifndef ARCH_SUPPORTS_FTRACE_OPS
+#define ARCH_SUPPORTS_FTRACE_OPS 0
+#endif
+
 struct module;
 struct ftrace_hash;
 
@@ -29,7 +38,10 @@ ftrace_enable_sysctl(struct ctl_table *table, int write,
 		     void __user *buffer, size_t *lenp,
 		     loff_t *ppos);
 
-typedef void (*ftrace_func_t)(unsigned long ip, unsigned long parent_ip);
+struct ftrace_ops;
+
+typedef void (*ftrace_func_t)(unsigned long ip, unsigned long parent_ip,
+			      struct ftrace_ops *op);
 
 /*
  * FTRACE_OPS_FL_* bits denote the state of ftrace_ops struct and are
@@ -163,7 +175,7 @@ static inline int ftrace_function_local_disabled(struct ftrace_ops *ops)
 	return *this_cpu_ptr(ops->disabled);
 }
 
-extern void ftrace_stub(unsigned long a0, unsigned long a1);
+extern void ftrace_stub(unsigned long a0, unsigned long a1, struct ftrace_ops *op);
 
 #else /* !CONFIG_FUNCTION_TRACER */
 /*

commit e4f5d5440bb860a3e8942ca8f7277a7f31798965
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Apr 27 09:13:18 2012 -0400

    ftrace/x86: Have x86 ftrace use the ftrace_modify_all_code()
    
    To remove duplicate code, have the ftrace arch_ftrace_update_code()
    use the generic ftrace_modify_all_code(). This requires that the
    default ftrace_replace_code() becomes a weak function so that an
    arch may override it.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index cd72ace7ade3..55e6d63d46d0 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -314,6 +314,7 @@ ftrace_set_early_filter(struct ftrace_ops *ops, char *buf, int enable);
 /* defined in arch */
 extern int ftrace_ip_converted(unsigned long ip);
 extern int ftrace_dyn_arch_init(void *data);
+extern void ftrace_replace_code(int enable);
 extern int ftrace_update_ftrace_func(ftrace_func_t func);
 extern void ftrace_caller(void);
 extern void ftrace_call(void);

commit 8ed3e2cfe40ffe43630fd8efa34fc97c95b4c298
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Apr 26 14:59:43 2012 -0400

    ftrace: Make ftrace_modify_all_code() global for archs to use
    
    Rename __ftrace_modify_code() to ftrace_modify_all_code() and make
    it global for all archs to use. This will remove the duplication
    of code, as archs that can modify code without stop_machine()
    can use it directly outside of the stop_machine() call.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 609948eb2b0a..cd72ace7ade3 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -319,6 +319,8 @@ extern void ftrace_caller(void);
 extern void ftrace_call(void);
 extern void mcount_call(void);
 
+void ftrace_modify_all_code(int command);
+
 #ifndef FTRACE_ADDR
 #define FTRACE_ADDR ((unsigned long)ftrace_caller)
 #endif

commit f0cf973a224a3e3c1dec3395af3ba01cf14b1ff4
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed Apr 25 14:39:54 2012 -0400

    ftrace: Return record ip addr for ftrace_location()
    
    ftrace_location() is passed an addr, and returns 1 if the addr is
    on a ftrace nop (or caller to ftrace_caller), and 0 otherwise.
    
    To let kprobes know if it should move a breakpoint or not, it
    must return the actual addr that is the start of the ftrace nop.
    This way a kprobe placed on the location of a ftrace nop, can
    instead be placed on the instruction after the nop. Even if the
    probe addr is on the second or later byte of the nop, it can
    simply be moved forward.
    
    Cc: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index d32cc5e4b0cc..609948eb2b0a 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -295,7 +295,7 @@ struct dyn_ftrace *ftrace_rec_iter_record(struct ftrace_rec_iter *iter);
 int ftrace_update_record(struct dyn_ftrace *rec, int enable);
 int ftrace_test_record(struct dyn_ftrace *rec, int enable);
 void ftrace_run_stop_machine(int command);
-int ftrace_location(unsigned long ip);
+unsigned long ftrace_location(unsigned long ip);
 
 extern ftrace_func_t ftrace_trace_function;
 

commit b02ee9a33b65bcc4ad13c12a0b04afdaab3ddd8d
Author: Minho Ban <mhban@samsung.com>
Date:   Mon May 7 11:36:00 2012 +0900

    tracing: Prevent wasting time evaluating parameters in trace_preempt_on/off
    
    This fixes spending time for evaluating parameters in trace_preempt_on/off when
    the tracer config is off.
    
    The patch mainly inspired by Steven Rostedt, thanks Steven.
    
    Link: http://lkml.kernel.org/r/4FA73510.7070705@samsung.com
    
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Turner <pjt@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Hidetoshi Seto <seto.hidetoshi@jp.fujitsu.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Signed-off-by: Minho Ban <mhban@samsung.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 0b5590330bca..d32cc5e4b0cc 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -491,8 +491,12 @@ static inline void __ftrace_enabled_restore(int enabled)
   extern void trace_preempt_on(unsigned long a0, unsigned long a1);
   extern void trace_preempt_off(unsigned long a0, unsigned long a1);
 #else
-  static inline void trace_preempt_on(unsigned long a0, unsigned long a1) { }
-  static inline void trace_preempt_off(unsigned long a0, unsigned long a1) { }
+/*
+ * Use defines instead of static inlines because some arches will make code out
+ * of the CALLER_ADDR, when we really want these to be a real nop.
+ */
+# define trace_preempt_on(a0, a1) do { } while (0)
+# define trace_preempt_off(a0, a1) do { } while (0)
 #endif
 
 #ifdef CONFIG_FTRACE_MCOUNT_RECORD

commit 08d636b6d4fb80647fe8869ea1cd97b2c26a4751
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Aug 16 09:57:10 2011 -0400

    ftrace/x86: Have arch x86_64 use breakpoints instead of stop machine
    
    This method changes x86 to add a breakpoint to the mcount locations
    instead of calling stop machine.
    
    Now that iret can be handled by NMIs, we perform the following to
    update code:
    
    1) Add a breakpoint to all locations that will be modified
    
    2) Sync all cores
    
    3) Update all locations to be either a nop or call (except breakpoint
       op)
    
    4) Sync all cores
    
    5) Remove the breakpoint with the new code.
    
    6) Sync all cores
    
    [
      Added updates that Masami suggested:
       Use unlikely(modifying_ftrace_code) in int3 trap to keep kprobes efficient.
       Don't use NOTIFY_* in ftrace handler in int3 as it is not a notifier.
    ]
    
    Cc: H. Peter Anvin <hpa@zytor.com>
    Acked-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 72a6cabb4d5b..0b5590330bca 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -286,6 +286,12 @@ struct ftrace_rec_iter *ftrace_rec_iter_start(void);
 struct ftrace_rec_iter *ftrace_rec_iter_next(struct ftrace_rec_iter *iter);
 struct dyn_ftrace *ftrace_rec_iter_record(struct ftrace_rec_iter *iter);
 
+#define for_ftrace_rec_iter(iter)		\
+	for (iter = ftrace_rec_iter_start();	\
+	     iter;				\
+	     iter = ftrace_rec_iter_next(iter))
+
+
 int ftrace_update_record(struct dyn_ftrace *rec, int enable);
 int ftrace_test_record(struct dyn_ftrace *rec, int enable);
 void ftrace_run_stop_machine(int command);

commit 5500fa51199aee770ce53718853732600543619e
Author: Jiri Olsa <jolsa@redhat.com>
Date:   Wed Feb 15 15:51:54 2012 +0100

    ftrace, perf: Add filter support for function trace event
    
    Adding support to filter function trace event via perf
    interface. It is now possible to use filter interface
    in the perf tool like:
    
      perf record -e ftrace:function --filter="(ip == mm_*)" ls
    
    The filter syntax is restricted to the the 'ip' field only,
    and following operators are accepted '==' '!=' '||', ending
    up with the filter strings like:
    
      ip == f1[, ]f2 ... || ip != f3[, ]f4 ...
    
    with comma ',' or space ' ' as a function separator. If the
    space ' ' is used as a separator, the right side of the
    assignment needs to be enclosed in double quotes '"', e.g.:
    
      perf record -e ftrace:function --filter '(ip == do_execve,sys_*,ext*)' ls
      perf record -e ftrace:function --filter '(ip == "do_execve,sys_*,ext*")' ls
      perf record -e ftrace:function --filter '(ip == "do_execve sys_* ext*")' ls
    
    The '==' operator adds trace filter with same effect as would
    be added via set_ftrace_filter file.
    
    The '!=' operator adds trace filter with same effect as would
    be added via set_ftrace_notrace file.
    
    The right side of the '!=', '==' operators is list of functions
    or regexp. to be added to filter separated by space.
    
    The '||' operator is used for connecting multiple filter definitions
    together. It is possible to have more than one '==' and '!='
    operators within one filter string.
    
    Link: http://lkml.kernel.org/r/1329317514-8131-8-git-send-email-jolsa@redhat.com
    
    Signed-off-by: Jiri Olsa <jolsa@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 64a309d2fbd6..72a6cabb4d5b 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -250,6 +250,7 @@ int ftrace_set_notrace(struct ftrace_ops *ops, unsigned char *buf,
 			int len, int reset);
 void ftrace_set_global_filter(unsigned char *buf, int len, int reset);
 void ftrace_set_global_notrace(unsigned char *buf, int len, int reset);
+void ftrace_free_filter(struct ftrace_ops *ops);
 
 int register_ftrace_command(struct ftrace_func_command *cmd);
 int unregister_ftrace_command(struct ftrace_func_command *cmd);
@@ -380,9 +381,6 @@ extern void ftrace_enable_daemon(void);
 #else
 static inline int skip_trace(unsigned long ip) { return 0; }
 static inline int ftrace_force_update(void) { return 0; }
-static inline void ftrace_set_filter(unsigned char *buf, int len, int reset)
-{
-}
 static inline void ftrace_disable_daemon(void) { }
 static inline void ftrace_enable_daemon(void) { }
 static inline void ftrace_release_mod(struct module *mod) {}
@@ -406,6 +404,9 @@ static inline int ftrace_text_reserved(void *start, void *end)
  */
 #define ftrace_regex_open(ops, flag, inod, file) ({ -ENODEV; })
 #define ftrace_set_early_filter(ops, buf, enable) do { } while (0)
+#define ftrace_set_filter(ops, buf, len, reset) ({ -ENODEV; })
+#define ftrace_set_notrace(ops, buf, len, reset) ({ -ENODEV; })
+#define ftrace_free_filter(ops) do { } while (0)
 
 static inline ssize_t ftrace_filter_write(struct file *file, const char __user *ubuf,
 			    size_t cnt, loff_t *ppos) { return -ENODEV; }

commit e248491ac283b516958ca9ab62c8e74b6718bca8
Author: Jiri Olsa <jolsa@redhat.com>
Date:   Wed Feb 15 15:51:48 2012 +0100

    ftrace: Add enable/disable ftrace_ops control interface
    
    Adding a way to temporarily enable/disable ftrace_ops. The change
    follows the same way as 'global' ftrace_ops are done.
    
    Introducing 2 global ftrace_ops - control_ops and ftrace_control_list
    which take over all ftrace_ops registered with FTRACE_OPS_FL_CONTROL
    flag. In addition new per cpu flag called 'disabled' is also added to
    ftrace_ops to provide the control information for each cpu.
    
    When ftrace_ops with FTRACE_OPS_FL_CONTROL is registered, it is
    set as disabled for all cpus.
    
    The ftrace_control_list contains all the registered 'control' ftrace_ops.
    The control_ops provides function which iterates ftrace_control_list
    and does the check for 'disabled' flag on current cpu.
    
    Adding 3 inline functions:
      ftrace_function_local_disable/ftrace_function_local_enable
      - enable/disable the ftrace_ops on current cpu
      ftrace_function_local_disabled
      - get disabled ftrace_ops::disabled value for current cpu
    
    Link: http://lkml.kernel.org/r/1329317514-8131-2-git-send-email-jolsa@redhat.com
    
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Jiri Olsa <jolsa@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index f33fb3b041c6..64a309d2fbd6 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -31,16 +31,33 @@ ftrace_enable_sysctl(struct ctl_table *table, int write,
 
 typedef void (*ftrace_func_t)(unsigned long ip, unsigned long parent_ip);
 
+/*
+ * FTRACE_OPS_FL_* bits denote the state of ftrace_ops struct and are
+ * set in the flags member.
+ *
+ * ENABLED - set/unset when ftrace_ops is registered/unregistered
+ * GLOBAL  - set manualy by ftrace_ops user to denote the ftrace_ops
+ *           is part of the global tracers sharing the same filter
+ *           via set_ftrace_* debugfs files.
+ * DYNAMIC - set when ftrace_ops is registered to denote dynamically
+ *           allocated ftrace_ops which need special care
+ * CONTROL - set manualy by ftrace_ops user to denote the ftrace_ops
+ *           could be controled by following calls:
+ *             ftrace_function_local_enable
+ *             ftrace_function_local_disable
+ */
 enum {
 	FTRACE_OPS_FL_ENABLED		= 1 << 0,
 	FTRACE_OPS_FL_GLOBAL		= 1 << 1,
 	FTRACE_OPS_FL_DYNAMIC		= 1 << 2,
+	FTRACE_OPS_FL_CONTROL		= 1 << 3,
 };
 
 struct ftrace_ops {
 	ftrace_func_t			func;
 	struct ftrace_ops		*next;
 	unsigned long			flags;
+	int __percpu			*disabled;
 #ifdef CONFIG_DYNAMIC_FTRACE
 	struct ftrace_hash		*notrace_hash;
 	struct ftrace_hash		*filter_hash;
@@ -97,6 +114,55 @@ int register_ftrace_function(struct ftrace_ops *ops);
 int unregister_ftrace_function(struct ftrace_ops *ops);
 void clear_ftrace_function(void);
 
+/**
+ * ftrace_function_local_enable - enable controlled ftrace_ops on current cpu
+ *
+ * This function enables tracing on current cpu by decreasing
+ * the per cpu control variable.
+ * It must be called with preemption disabled and only on ftrace_ops
+ * registered with FTRACE_OPS_FL_CONTROL. If called without preemption
+ * disabled, this_cpu_ptr will complain when CONFIG_DEBUG_PREEMPT is enabled.
+ */
+static inline void ftrace_function_local_enable(struct ftrace_ops *ops)
+{
+	if (WARN_ON_ONCE(!(ops->flags & FTRACE_OPS_FL_CONTROL)))
+		return;
+
+	(*this_cpu_ptr(ops->disabled))--;
+}
+
+/**
+ * ftrace_function_local_disable - enable controlled ftrace_ops on current cpu
+ *
+ * This function enables tracing on current cpu by decreasing
+ * the per cpu control variable.
+ * It must be called with preemption disabled and only on ftrace_ops
+ * registered with FTRACE_OPS_FL_CONTROL. If called without preemption
+ * disabled, this_cpu_ptr will complain when CONFIG_DEBUG_PREEMPT is enabled.
+ */
+static inline void ftrace_function_local_disable(struct ftrace_ops *ops)
+{
+	if (WARN_ON_ONCE(!(ops->flags & FTRACE_OPS_FL_CONTROL)))
+		return;
+
+	(*this_cpu_ptr(ops->disabled))++;
+}
+
+/**
+ * ftrace_function_local_disabled - returns ftrace_ops disabled value
+ *                                  on current cpu
+ *
+ * This function returns value of ftrace_ops::disabled on current cpu.
+ * It must be called with preemption disabled and only on ftrace_ops
+ * registered with FTRACE_OPS_FL_CONTROL. If called without preemption
+ * disabled, this_cpu_ptr will complain when CONFIG_DEBUG_PREEMPT is enabled.
+ */
+static inline int ftrace_function_local_disabled(struct ftrace_ops *ops)
+{
+	WARN_ON_ONCE(!(ops->flags & FTRACE_OPS_FL_CONTROL));
+	return *this_cpu_ptr(ops->disabled);
+}
+
 extern void ftrace_stub(unsigned long a0, unsigned long a1);
 
 #else /* !CONFIG_FUNCTION_TRACER */

commit ac483c446b67870444c9eeaf8325d3d2af9b91bc
Author: Jiri Olsa <jolsa@redhat.com>
Date:   Mon Jan 2 10:04:14 2012 +0100

    ftrace: Change filter/notrace set functions to return exit code
    
    Currently the ftrace_set_filter and ftrace_set_notrace functions
    do not return any return code. So there's no way for ftrace_ops
    user to tell wether the filter was correctly applied.
    
    The set_ftrace_filter interface returns error in case the filter
    did not match:
    
      # echo krava > set_ftrace_filter
      bash: echo: write error: Invalid argument
    
    Changing both ftrace_set_filter and ftrace_set_notrace functions
    to return zero if the filter was applied correctly or -E* values
    in case of error.
    
    Link: http://lkml.kernel.org/r/1325495060-6402-2-git-send-email-jolsa@redhat.com
    
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Jiri Olsa <jolsa@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 028e26f0bf08..f33fb3b041c6 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -178,9 +178,9 @@ struct dyn_ftrace {
 };
 
 int ftrace_force_update(void);
-void ftrace_set_filter(struct ftrace_ops *ops, unsigned char *buf,
+int ftrace_set_filter(struct ftrace_ops *ops, unsigned char *buf,
 		       int len, int reset);
-void ftrace_set_notrace(struct ftrace_ops *ops, unsigned char *buf,
+int ftrace_set_notrace(struct ftrace_ops *ops, unsigned char *buf,
 			int len, int reset);
 void ftrace_set_global_filter(unsigned char *buf, int len, int reset);
 void ftrace_set_global_notrace(unsigned char *buf, int len, int reset);

commit 96de37b62ca525cd77d2e85aea1472846ee31c4d
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Sat Jan 7 17:26:49 2012 -0500

    tracing: Fix compile error when static ftrace is enabled
    
    The stack tracer uses the call ftrace_set_early_filter() function
    to allow the stack tracer to pick its own functions on boot.
    But this function is not defined if dynamic ftrace is not set.
    This causes a compiler error when stack tracer is enabled and
    dynamic ftrace is not.
    
    Reported-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 41df6f501656..028e26f0bf08 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -339,6 +339,7 @@ static inline int ftrace_text_reserved(void *start, void *end)
  * functions may still be called. Use a macro instead of inline.
  */
 #define ftrace_regex_open(ops, flag, inod, file) ({ -ENODEV; })
+#define ftrace_set_early_filter(ops, buf, enable) do { } while (0)
 
 static inline ssize_t ftrace_filter_write(struct file *file, const char __user *ubuf,
 			    size_t cnt, loff_t *ppos) { return -ENODEV; }

commit 2a85a37f168d2b4d74d493b578af4dc9032be92e
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon Dec 19 21:57:44 2011 -0500

    ftrace: Allow access to the boot time function enabling
    
    Change set_ftrace_early_filter() to ftrace_set_early_filter()
    and make it a global function. This will allow other subsystems
    in the kernel to be able to enable function tracing at start
    up and reuse the ftrace function parsing code.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index d1ff0de18970..41df6f501656 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -235,6 +235,9 @@ ssize_t ftrace_notrace_write(struct file *file, const char __user *ubuf,
 loff_t ftrace_regex_lseek(struct file *file, loff_t offset, int origin);
 int ftrace_regex_release(struct inode *inode, struct file *file);
 
+void __init
+ftrace_set_early_filter(struct ftrace_ops *ops, char *buf, int enable);
+
 /* defined in arch */
 extern int ftrace_ip_converted(unsigned long ip);
 extern int ftrace_dyn_arch_init(void *data);

commit 69a3083c4a7df0322d97bb2b43a33cb12af8131a
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon Dec 19 15:21:16 2011 -0500

    ftrace: Decouple hash items from showing filtered functions
    
    The set_ftrace_filter shows "hashed" functions, which are functions
    that are added with operations to them (like traceon and traceoff).
    
    As other subsystems may be able to show what functions they are
    using for function tracing, the hash items should no longer
    be shown just because the FILTER flag is set. As they have nothing
    to do with other subsystems filters.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index aa7559f0a224..d1ff0de18970 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -206,8 +206,9 @@ enum {
 	FTRACE_ITER_FILTER	= (1 << 0),
 	FTRACE_ITER_NOTRACE	= (1 << 1),
 	FTRACE_ITER_PRINTALL	= (1 << 2),
-	FTRACE_ITER_HASH	= (1 << 3),
-	FTRACE_ITER_ENABLED	= (1 << 4),
+	FTRACE_ITER_DO_HASH	= (1 << 3),
+	FTRACE_ITER_HASH	= (1 << 4),
+	FTRACE_ITER_ENABLED	= (1 << 5),
 };
 
 void arch_ftrace_update_code(int command);

commit fc13cb0ce45296f331263a6034aa1814203e1ac3
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon Dec 19 14:41:25 2011 -0500

    ftrace: Allow other users of function tracing to use the output listing
    
    The function tracer is set up to allow any other subsystem (like perf)
    to use it. Ftrace already has a way to list what functions are enabled
    by the global_ops. It would be very helpful to let other users of
    the function tracer to be able to use the same code.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 31b9fd7aedcd..aa7559f0a224 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -202,6 +202,14 @@ enum {
 	FTRACE_UPDATE_MAKE_NOP,
 };
 
+enum {
+	FTRACE_ITER_FILTER	= (1 << 0),
+	FTRACE_ITER_NOTRACE	= (1 << 1),
+	FTRACE_ITER_PRINTALL	= (1 << 2),
+	FTRACE_ITER_HASH	= (1 << 3),
+	FTRACE_ITER_ENABLED	= (1 << 4),
+};
+
 void arch_ftrace_update_code(int command);
 
 struct ftrace_rec_iter;
@@ -217,6 +225,15 @@ int ftrace_location(unsigned long ip);
 
 extern ftrace_func_t ftrace_trace_function;
 
+int ftrace_regex_open(struct ftrace_ops *ops, int flag,
+		  struct inode *inode, struct file *file);
+ssize_t ftrace_filter_write(struct file *file, const char __user *ubuf,
+			    size_t cnt, loff_t *ppos);
+ssize_t ftrace_notrace_write(struct file *file, const char __user *ubuf,
+			     size_t cnt, loff_t *ppos);
+loff_t ftrace_regex_lseek(struct file *file, loff_t offset, int origin);
+int ftrace_regex_release(struct inode *inode, struct file *file);
+
 /* defined in arch */
 extern int ftrace_ip_converted(unsigned long ip);
 extern int ftrace_dyn_arch_init(void *data);
@@ -311,6 +328,24 @@ static inline int ftrace_text_reserved(void *start, void *end)
 {
 	return 0;
 }
+
+/*
+ * Again users of functions that have ftrace_ops may not
+ * have them defined when ftrace is not enabled, but these
+ * functions may still be called. Use a macro instead of inline.
+ */
+#define ftrace_regex_open(ops, flag, inod, file) ({ -ENODEV; })
+
+static inline ssize_t ftrace_filter_write(struct file *file, const char __user *ubuf,
+			    size_t cnt, loff_t *ppos) { return -ENODEV; }
+static inline ssize_t ftrace_notrace_write(struct file *file, const char __user *ubuf,
+			     size_t cnt, loff_t *ppos) { return -ENODEV; }
+static inline loff_t ftrace_regex_lseek(struct file *file, loff_t offset, int origin)
+{
+	return -ENODEV;
+}
+static inline int
+ftrace_regex_release(struct inode *inode, struct file *file) { return -ENODEV; }
 #endif /* CONFIG_DYNAMIC_FTRACE */
 
 /* totally disable ftrace - can not re-enable after this */

commit 85ae32ae019bc1c2cc22e5f51fe0c9f2812ef68c
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Dec 16 16:30:31 2011 -0500

    ftrace: Replace record newlist with record page list
    
    As new functions come in to be initalized from mcount to nop,
    they are done by groups of pages. Whether it is the core kernel
    or a module. There's no need to keep track of these on a per record
    basis.
    
    At startup, and as any module is loaded, the functions to be
    traced are stored in a group of pages and added to the function
    list at the end. We just need to keep a pointer to the first
    page of the list that was added, and use that to know where to
    start on the list for initializing functions.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 3f79bc458bff..31b9fd7aedcd 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -173,10 +173,7 @@ struct dyn_ftrace {
 		unsigned long		ip; /* address of mcount call-site */
 		struct dyn_ftrace	*freelist;
 	};
-	union {
-		unsigned long		flags;
-		struct dyn_ftrace	*newlist;
-	};
+	unsigned long		flags;
 	struct dyn_arch_ftrace		arch;
 };
 

commit 3208230983a0ee3d95be22d463257e530c684956
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Dec 16 14:42:37 2011 -0500

    ftrace: Remove usage of "freed" records
    
    Records that are added to the function trace table are
    permanently there, except for modules. By separating out the
    modules to their own pages that can be freed in one shot
    we can remove the "freed" flag and simplify some of the record
    management.
    
    Another benefit of doing this is that we can also move the
    records around; sort them.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 4f0b6fec379d..3f79bc458bff 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -163,7 +163,6 @@ extern int ftrace_text_reserved(void *start, void *end);
 
 enum {
 	FTRACE_FL_ENABLED	= (1 << 30),
-	FTRACE_FL_FREE		= (1 << 31),
 };
 
 #define FTRACE_FL_MASK		(0x3UL << 30)

commit c88fd8634ea68e74c7d19fd2621b4078fd22864c
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Aug 16 09:53:39 2011 -0400

    ftrace: Allow archs to modify code without stop machine
    
    The stop machine method to modify all functions in the kernel
    (some 20,000 of them) is the safest way to do so across all archs.
    But some archs may not need this big hammer approach to modify code
    on SMP machines, and can simply just update the code it needs.
    
    Adding a weak function arch_ftrace_update_code() that now does the
    stop machine, will also let any arch override this method.
    
    If the arch needs to check the system and then decide if it can
    avoid stop machine, it can still call ftrace_run_stop_machine() to
    use the old method.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 26eafcef75be..4f0b6fec379d 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -133,6 +133,8 @@ struct ftrace_func_command {
 int ftrace_arch_code_modify_prepare(void);
 int ftrace_arch_code_modify_post_process(void);
 
+void ftrace_bug(int err, unsigned long ip);
+
 struct seq_file;
 
 struct ftrace_probe_ops {
@@ -190,6 +192,35 @@ void ftrace_set_global_notrace(unsigned char *buf, int len, int reset);
 int register_ftrace_command(struct ftrace_func_command *cmd);
 int unregister_ftrace_command(struct ftrace_func_command *cmd);
 
+enum {
+	FTRACE_UPDATE_CALLS		= (1 << 0),
+	FTRACE_DISABLE_CALLS		= (1 << 1),
+	FTRACE_UPDATE_TRACE_FUNC	= (1 << 2),
+	FTRACE_START_FUNC_RET		= (1 << 3),
+	FTRACE_STOP_FUNC_RET		= (1 << 4),
+};
+
+enum {
+	FTRACE_UPDATE_IGNORE,
+	FTRACE_UPDATE_MAKE_CALL,
+	FTRACE_UPDATE_MAKE_NOP,
+};
+
+void arch_ftrace_update_code(int command);
+
+struct ftrace_rec_iter;
+
+struct ftrace_rec_iter *ftrace_rec_iter_start(void);
+struct ftrace_rec_iter *ftrace_rec_iter_next(struct ftrace_rec_iter *iter);
+struct dyn_ftrace *ftrace_rec_iter_record(struct ftrace_rec_iter *iter);
+
+int ftrace_update_record(struct dyn_ftrace *rec, int enable);
+int ftrace_test_record(struct dyn_ftrace *rec, int enable);
+void ftrace_run_stop_machine(int command);
+int ftrace_location(unsigned long ip);
+
+extern ftrace_func_t ftrace_trace_function;
+
 /* defined in arch */
 extern int ftrace_ip_converted(unsigned long ip);
 extern int ftrace_dyn_arch_init(void *data);

commit de47725421ad5627a5c905f4e40bb844ebc06d29
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Thu May 26 13:46:22 2011 -0400

    include: replace linux/module.h with "struct module" wherever possible
    
    The <linux/module.h> pretty much brings in the kitchen sink along
    with it, so it should be avoided wherever reasonably possible in
    terms of being included from other commonly used <linux/something.h>
    files, as it results in a measureable increase on compile times.
    
    The worst culprit was probably device.h since it is used everywhere.
    This file also had an implicit dependency/usage of mutex.h which was
    masked by module.h, and is also fixed here at the same time.
    
    There are over a dozen other headers that simply declare the
    struct instead of pulling in the whole file, so follow their lead
    and simply make it a few more.
    
    Most of the implicit dependencies on module.h being present by
    these headers pulling it in have been now weeded out, so we can
    finally make this change with hopefully minimal breakage.
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index f0c0e8a47ae6..26eafcef75be 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -10,7 +10,6 @@
 #include <linux/kallsyms.h>
 #include <linux/linkage.h>
 #include <linux/bitops.h>
-#include <linux/module.h>
 #include <linux/ktime.h>
 #include <linux/sched.h>
 #include <linux/types.h>
@@ -19,6 +18,7 @@
 
 #include <asm/ftrace.h>
 
+struct module;
 struct ftrace_hash;
 
 #ifdef CONFIG_FUNCTION_TRACER

commit 04da85b86188f224cc9b391b5bdd92a3ba20ffcf
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon Jul 11 10:12:59 2011 -0400

    ftrace: Fix warning when CONFIG_FUNCTION_TRACER is not defined
    
    The struct ftrace_hash was declared within CONFIG_FUNCTION_TRACER
    but was referenced outside of it.
    
    Reported-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index ed0eb5254d1c..f0c0e8a47ae6 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -19,6 +19,8 @@
 
 #include <asm/ftrace.h>
 
+struct ftrace_hash;
+
 #ifdef CONFIG_FUNCTION_TRACER
 
 extern int ftrace_enabled;
@@ -29,8 +31,6 @@ ftrace_enable_sysctl(struct ctl_table *table, int write,
 
 typedef void (*ftrace_func_t)(unsigned long ip, unsigned long parent_ip);
 
-struct ftrace_hash;
-
 enum {
 	FTRACE_OPS_FL_ENABLED		= 1 << 0,
 	FTRACE_OPS_FL_GLOBAL		= 1 << 1,

commit 43dd61c9a09bd413e837df829e6bfb42159be52a
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Jul 7 11:09:22 2011 -0400

    ftrace: Fix regression of :mod:module function enabling
    
    The new code that allows different utilities to pick and choose
    what functions they trace broke the :mod: hook that allows users
    to trace only functions of a particular module.
    
    The reason is that the :mod: hook bypasses the hash that is setup
    to allow individual users to trace their own functions and uses
    the global hash directly. But if the global hash has not been
    set up, it will cause a bug:
    
    echo '*:mod:radeon' > /sys/kernel/debug/set_ftrace_filter
    
    produces:
    
     [drm:drm_mode_getfb] *ERROR* invalid framebuffer id
     [drm:radeon_crtc_page_flip] *ERROR* failed to reserve new rbo buffer before flip
     BUG: unable to handle kernel paging request at ffffffff8160ec90
     IP: [<ffffffff810d9136>] add_hash_entry+0x66/0xd0
     PGD 1a05067 PUD 1a09063 PMD 80000000016001e1
     Oops: 0003 [#1] SMP Jul  7 04:02:28 phyllis kernel: [55303.858604] CPU 1
     Modules linked in: cryptd aes_x86_64 aes_generic binfmt_misc rfcomm bnep ip6table_filter hid radeon r8169 ahci libahci mii ttm drm_kms_helper drm video i2c_algo_bit intel_agp intel_gtt
    
     Pid: 10344, comm: bash Tainted: G        WC  3.0.0-rc5 #1 Dell Inc. Inspiron N5010/0YXXJJ
     RIP: 0010:[<ffffffff810d9136>]  [<ffffffff810d9136>] add_hash_entry+0x66/0xd0
     RSP: 0018:ffff88003a96bda8  EFLAGS: 00010246
     RAX: ffff8801301735c0 RBX: ffffffff8160ec80 RCX: 0000000000306ee0
     RDX: 0000000000000000 RSI: 0000000000000000 RDI: ffff880137c92940
     RBP: ffff88003a96bdb8 R08: ffff880137c95680 R09: 0000000000000000
     R10: 0000000000000001 R11: 0000000000000000 R12: ffffffff81c9df78
     R13: ffff8801153d1000 R14: 0000000000000000 R15: 0000000000000000
     FS: 00007f329c18a700(0000) GS:ffff880137c80000(0000) knlGS:0000000000000000
     CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
     CR2: ffffffff8160ec90 CR3: 000000003002b000 CR4: 00000000000006e0
     DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
     DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000400
     Process bash (pid: 10344, threadinfo ffff88003a96a000, task ffff88012fcfc470)
     Stack:
      0000000000000fd0 00000000000000fc ffff88003a96be38 ffffffff810d92f5
      ffff88011c4c4e00 ffff880000000000 000000000b69f4d0 ffffffff8160ec80
      ffff8800300e6f06 0000000081130295 0000000000000282 ffff8800300e6f00
     Call Trace:
      [<ffffffff810d92f5>] match_records+0x155/0x1b0
      [<ffffffff810d940c>] ftrace_mod_callback+0xbc/0x100
      [<ffffffff810dafdf>] ftrace_regex_write+0x16f/0x210
      [<ffffffff810db09f>] ftrace_filter_write+0xf/0x20
      [<ffffffff81166e48>] vfs_write+0xc8/0x190
      [<ffffffff81167001>] sys_write+0x51/0x90
      [<ffffffff815c7e02>] system_call_fastpath+0x16/0x1b
     Code: 48 8b 33 31 d2 48 85 f6 75 33 49 89 d4 4c 03 63 08 49 8b 14 24 48 85 d2 48 89 10 74 04 48 89 42 08 49 89 04 24 4c 89 60 08 31 d2
     RIP [<ffffffff810d9136>] add_hash_entry+0x66/0xd0
      RSP <ffff88003a96bda8>
     CR2: ffffffff8160ec90
     ---[ end trace a5d031828efdd88e ]---
    
    Reported-by: Brian Marete <marete@toshnix.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 9d88e1cb5dbb..ed0eb5254d1c 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -123,7 +123,8 @@ stack_trace_sysctl(struct ctl_table *table, int write,
 struct ftrace_func_command {
 	struct list_head	list;
 	char			*name;
-	int			(*func)(char *func, char *cmd,
+	int			(*func)(struct ftrace_hash *hash,
+					char *func, char *cmd,
 					char *params, int enable);
 };
 

commit 936e074b286ae779f134312178dbab139ee7ea52
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu May 5 22:54:01 2011 -0400

    ftrace: Modify ftrace_set_filter/notrace to take ops
    
    Since users of the function tracer can now pick and choose which
    functions they want to trace agnostically from other users of the
    function tracer, we need to pass the ops struct to the ftrace_set_filter()
    functions.
    
    The functions ftrace_set_global_filter() and ftrace_set_global_notrace()
    is added to keep the old filter functions which are used to modify
    the generic function tracers.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index caba694a62b6..9d88e1cb5dbb 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -179,7 +179,12 @@ struct dyn_ftrace {
 };
 
 int ftrace_force_update(void);
-void ftrace_set_filter(unsigned char *buf, int len, int reset);
+void ftrace_set_filter(struct ftrace_ops *ops, unsigned char *buf,
+		       int len, int reset);
+void ftrace_set_notrace(struct ftrace_ops *ops, unsigned char *buf,
+			int len, int reset);
+void ftrace_set_global_filter(unsigned char *buf, int len, int reset);
+void ftrace_set_global_notrace(unsigned char *buf, int len, int reset);
 
 int register_ftrace_command(struct ftrace_func_command *cmd);
 int unregister_ftrace_command(struct ftrace_func_command *cmd);

commit cdbe61bfe70440939e457fb4a8d0995eaaed17de
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu May 5 21:14:55 2011 -0400

    ftrace: Allow dynamically allocated function tracers
    
    Now that functions may be selected individually, it only makes sense
    that we should allow dynamically allocated trace structures to
    be traced. This will allow perf to allocate a ftrace_ops structure
    at runtime and use it to pick and choose which functions that
    structure will trace.
    
    Note, a dynamically allocated ftrace_ops will always be called
    indirectly instead of being called directly from the mcount in
    entry.S. This is because there's no safe way to prevent mcount
    from being preempted before calling the function, unless we
    modify every entry.S to do so (not likely). Thus, dynamically allocated
    functions will now be called by the ftrace_ops_list_func() that
    loops through the ops that are allocated if there are more than
    one op allocated at a time. This loop is protected with a
    preempt_disable.
    
    To determine if an ftrace_ops structure is allocated or not, a new
    util function was added to the kernel/extable.c called
    core_kernel_data(), which returns 1 if the address is between
    _sdata and _edata.
    
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 4609c0ece79a..caba694a62b6 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -34,6 +34,7 @@ struct ftrace_hash;
 enum {
 	FTRACE_OPS_FL_ENABLED		= 1 << 0,
 	FTRACE_OPS_FL_GLOBAL		= 1 << 1,
+	FTRACE_OPS_FL_DYNAMIC		= 1 << 2,
 };
 
 struct ftrace_ops {

commit b848914ce39589d89ee0078a6d1ef452b464729e
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed May 4 09:27:52 2011 -0400

    ftrace: Implement separate user function filtering
    
    ftrace_ops that are registered to trace functions can now be
    agnostic to each other in respect to what functions they trace.
    Each ops has their own hash of the functions they want to trace
    and a hash to what they do not want to trace. A empty hash for
    the functions they want to trace denotes all functions should
    be traced that are not in the notrace hash.
    
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index ab1c46e70bb6..4609c0ece79a 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -31,13 +31,18 @@ typedef void (*ftrace_func_t)(unsigned long ip, unsigned long parent_ip);
 
 struct ftrace_hash;
 
+enum {
+	FTRACE_OPS_FL_ENABLED		= 1 << 0,
+	FTRACE_OPS_FL_GLOBAL		= 1 << 1,
+};
+
 struct ftrace_ops {
 	ftrace_func_t			func;
 	struct ftrace_ops		*next;
+	unsigned long			flags;
 #ifdef CONFIG_DYNAMIC_FTRACE
 	struct ftrace_hash		*notrace_hash;
 	struct ftrace_hash		*filter_hash;
-	unsigned long			flags;
 #endif
 };
 

commit ed926f9b35cda0988234c356e16a7cb30f4e5338
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue May 3 13:25:24 2011 -0400

    ftrace: Use counters to enable functions to trace
    
    Every function has its own record that stores the instruction
    pointer and flags for the function to be traced. There are only
    two flags: enabled and free. The enabled flag states that tracing
    for the function has been enabled (actively traced), and the free
    flag states that the record no longer points to a function and can
    be used by new functions (loaded modules).
    
    These flags are now moved to the MSB of the flags (actually just
    the top 32bits). The rest of the bits (30 bits) are now used as
    a ref counter. Everytime a tracer register functions to trace,
    those functions will have its counter incremented.
    
    When tracing is enabled, to determine if a function should be traced,
    the counter is examined, and if it is non-zero it is set to trace.
    
    When a ftrace_ops is registered to trace functions, its hashes
    are examined. If the ftrace_ops filter_hash count is zero, then
    all functions are set to be traced, otherwise only the functions
    in the hash are to be traced. The exception to this is if a function
    is also in the ftrace_ops notrace_hash. Then that function's counter
    is not incremented for this ftrace_ops.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 6658a51390fe..ab1c46e70bb6 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -37,6 +37,7 @@ struct ftrace_ops {
 #ifdef CONFIG_DYNAMIC_FTRACE
 	struct ftrace_hash		*notrace_hash;
 	struct ftrace_hash		*filter_hash;
+	unsigned long			flags;
 #endif
 };
 
@@ -152,10 +153,13 @@ extern void unregister_ftrace_function_probe_all(char *glob);
 extern int ftrace_text_reserved(void *start, void *end);
 
 enum {
-	FTRACE_FL_FREE		= (1 << 0),
-	FTRACE_FL_ENABLED	= (1 << 1),
+	FTRACE_FL_ENABLED	= (1 << 30),
+	FTRACE_FL_FREE		= (1 << 31),
 };
 
+#define FTRACE_FL_MASK		(0x3UL << 30)
+#define FTRACE_REF_MAX		((1 << 30) - 1)
+
 struct dyn_ftrace {
 	union {
 		unsigned long		ip; /* address of mcount call-site */

commit f45948e898e7bc76a73a468796d2ce80dd040058
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon May 2 12:29:25 2011 -0400

    ftrace: Create a global_ops to hold the filter and notrace hashes
    
    Combine the filter and notrace hashes to be accessed by a single entity,
    the global_ops. The global_ops is a ftrace_ops structure that is passed
    to different functions that can read or modify the filtering of the
    function tracer.
    
    The ftrace_ops structure was modified to hold a filter and notrace
    hashes so that later patches may allow each ftrace_ops to have its own
    set of rules to what functions may be filtered.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 52fc5d4e6edd..6658a51390fe 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -29,9 +29,15 @@ ftrace_enable_sysctl(struct ctl_table *table, int write,
 
 typedef void (*ftrace_func_t)(unsigned long ip, unsigned long parent_ip);
 
+struct ftrace_hash;
+
 struct ftrace_ops {
-	ftrace_func_t	  func;
-	struct ftrace_ops *next;
+	ftrace_func_t			func;
+	struct ftrace_ops		*next;
+#ifdef CONFIG_DYNAMIC_FTRACE
+	struct ftrace_hash		*notrace_hash;
+	struct ftrace_hash		*filter_hash;
+#endif
 };
 
 extern int function_trace_stop;

commit 1cf41dd79993389b012e4542ab502ce36ae7343f
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Apr 29 20:59:51 2011 -0400

    ftrace: Use hash instead for FTRACE_FL_FILTER
    
    When multiple users are allowed to have their own set of functions
    to trace, having the FTRACE_FL_FILTER flag will not be enough to
    handle the accounting of those users. Each user will need their own
    set of functions.
    
    Replace the FTRACE_FL_FILTER with a filter_hash instead. This is
    temporary until the rest of the function filtering accounting
    gets in.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index fe0a90a09243..52fc5d4e6edd 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -147,8 +147,7 @@ extern int ftrace_text_reserved(void *start, void *end);
 
 enum {
 	FTRACE_FL_FREE		= (1 << 0),
-	FTRACE_FL_FILTER	= (1 << 1),
-	FTRACE_FL_ENABLED	= (1 << 2),
+	FTRACE_FL_ENABLED	= (1 << 1),
 };
 
 struct dyn_ftrace {

commit b448c4e3ae6d20108dba1d7833f2c0d3dbad87ce
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Apr 29 15:12:32 2011 -0400

    ftrace: Replace FTRACE_FL_NOTRACE flag with a hash of ignored functions
    
    To prepare for the accounting system that will allow multiple users of
    the function tracer, having the FTRACE_FL_NOTRACE as a flag in the
    dyn_trace record does not make sense.
    
    All ftrace_ops will soon have a hash of functions they should trace
    and not trace. By making a global hash of functions not to trace makes
    this easier for the transition.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 32047449b309..fe0a90a09243 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -149,7 +149,6 @@ enum {
 	FTRACE_FL_FREE		= (1 << 0),
 	FTRACE_FL_FILTER	= (1 << 1),
 	FTRACE_FL_ENABLED	= (1 << 2),
-	FTRACE_FL_NOTRACE	= (1 << 3),
 };
 
 struct dyn_ftrace {

commit d2c8c3eafbf715306ec891e7ca52d3d999acbe31
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon Apr 25 14:32:42 2011 -0400

    ftrace: Remove FTRACE_FL_CONVERTED flag
    
    Since we disable all function tracer processing if we detect
    that a modification of a instruction had failed, we do not need
    to track that the record has failed. No more ftrace processing
    is allowed, and the FTRACE_FL_CONVERTED flag is pointless.
    
    The FTRACE_FL_CONVERTED flag was used to denote records that were
    successfully converted from mcount calls into nops. But if a single
    record fails, all of ftrace is disabled.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 2a195ffd4269..32047449b309 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -150,7 +150,6 @@ enum {
 	FTRACE_FL_FILTER	= (1 << 1),
 	FTRACE_FL_ENABLED	= (1 << 2),
 	FTRACE_FL_NOTRACE	= (1 << 3),
-	FTRACE_FL_CONVERTED	= (1 << 4),
 };
 
 struct dyn_ftrace {

commit 45a4a2372b364107cabea79f255b333236626416
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Apr 21 23:16:46 2011 -0400

    ftrace: Remove FTRACE_FL_FAILED flag
    
    Since we disable all function tracer processing if we detect
    that a modification of a instruction had failed, we do not need
    to track that the record has failed. No more ftrace processing
    is allowed, and the FTRACE_FL_FAILED flag is pointless.
    
    Removing this flag simplifies some of the code, but some ftrace_disabled
    checks needed to be added or move around a little.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index ca29e03c1fac..2a195ffd4269 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -147,11 +147,10 @@ extern int ftrace_text_reserved(void *start, void *end);
 
 enum {
 	FTRACE_FL_FREE		= (1 << 0),
-	FTRACE_FL_FAILED	= (1 << 1),
-	FTRACE_FL_FILTER	= (1 << 2),
-	FTRACE_FL_ENABLED	= (1 << 3),
-	FTRACE_FL_NOTRACE	= (1 << 4),
-	FTRACE_FL_CONVERTED	= (1 << 5),
+	FTRACE_FL_FILTER	= (1 << 1),
+	FTRACE_FL_ENABLED	= (1 << 2),
+	FTRACE_FL_NOTRACE	= (1 << 3),
+	FTRACE_FL_CONVERTED	= (1 << 4),
 };
 
 struct dyn_ftrace {

commit 868baf07b1a259f5f3803c1dc2777b6c358f83cf
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Feb 10 21:26:13 2011 -0500

    ftrace: Fix memory leak with function graph and cpu hotplug
    
    When the fuction graph tracer starts, it needs to make a special
    stack for each task to save the real return values of the tasks.
    All running tasks have this stack created, as well as any new
    tasks.
    
    On CPU hot plug, the new idle task will allocate a stack as well
    when init_idle() is called. The problem is that cpu hotplug does
    not create a new idle_task. Instead it uses the idle task that
    existed when the cpu went down.
    
    ftrace_graph_init_task() will add a new ret_stack to the task
    that is given to it. Because a clone will make the task
    have a stack of its parent it does not check if the task's
    ret_stack is already NULL or not. When the CPU hotplug code
    starts a CPU up again, it will allocate a new stack even
    though one already existed for it.
    
    The solution is to treat the idle_task specially. In fact, the
    function_graph code already does, just not at init_idle().
    Instead of using the ftrace_graph_init_task() for the idle task,
    which that function expects the task to be a clone, have a
    separate ftrace_graph_init_idle_task(). Also, we will create a
    per_cpu ret_stack that is used by the idle task. When we call
    ftrace_graph_init_idle_task() it will check if the idle task's
    ret_stack is NULL, if it is, then it will assign it the per_cpu
    ret_stack.
    
    Reported-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Suggested-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Stable Tree <stable@kernel.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index dcd6a7c3a435..ca29e03c1fac 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -428,6 +428,7 @@ extern void unregister_ftrace_graph(void);
 
 extern void ftrace_graph_init_task(struct task_struct *t);
 extern void ftrace_graph_exit_task(struct task_struct *t);
+extern void ftrace_graph_init_idle_task(struct task_struct *t, int cpu);
 
 static inline int task_curr_ret_stack(struct task_struct *t)
 {
@@ -451,6 +452,7 @@ static inline void unpause_graph_tracing(void)
 
 static inline void ftrace_graph_init_task(struct task_struct *t) { }
 static inline void ftrace_graph_exit_task(struct task_struct *t) { }
+static inline void ftrace_graph_init_idle_task(struct task_struct *t, int cpu) { }
 
 static inline int register_ftrace_graph(trace_func_graph_ret_t retfunc,
 			  trace_func_graph_ent_t entryfunc)

commit 9849ed4d72251d273524efb8b70be0be9aecb1df
Author: Mike Frysinger <vapier@gentoo.org>
Date:   Tue Jul 20 03:13:35 2010 -0400

    tracing/documentation: Document dynamic ftracer internals
    
    Add more details to the dynamic function tracing design implementation.
    
    Signed-off-by: Mike Frysinger <vapier@gentoo.org>
    LKML-Reference: <1279610015-10250-1-git-send-email-vapier@gentoo.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 41e46330d9be..dcd6a7c3a435 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -1,3 +1,8 @@
+/*
+ * Ftrace header.  For implementation details beyond the random comments
+ * scattered below, see: Documentation/trace/ftrace-design.txt
+ */
+
 #ifndef _LINUX_FTRACE_H
 #define _LINUX_FTRACE_H
 

commit 752f114fb83c5839de37a250b4f8257ed5438341
Merge: b8ae30ee26d3 ad56b0797e67
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 18 08:35:04 2010 -0700

    Merge branch 'tracing-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'tracing-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      tracing: Fix "integer as NULL pointer" warning.
      tracing: Fix tracepoint.h DECLARE_TRACE() to allow more than one header
      tracing: Make the documentation clear on trace_event boot option
      ring-buffer: Wrap open-coded WARN_ONCE
      tracing: Convert nop macros to static inlines
      tracing: Fix sleep time function profiling
      tracing: Show sample std dev in function profiling
      tracing: Add documentation for trace commands mod, traceon/traceoff
      ring-buffer: Make benchmark handle missed events
      ring-buffer: Make non-consuming read less expensive with lots of cpus.
      tracing: Add graph output support for irqsoff tracer
      tracing: Have graph flags passed in to ouput functions
      tracing: Add ftrace events for graph tracer
      tracing: Dump either the oops's cpu source or all cpus buffers
      tracing: Fix uninitialized variable of tracing/trace output

commit 4dbf6bc239c169b032777616806ecc648058f6b2
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue May 4 11:24:01 2010 -0400

    tracing: Convert nop macros to static inlines
    
    The ftrace.h file contains several functions as macros when the
    functions are disabled due to config options. This patch converts
    most of them to static inlines.
    
    There are two exceptions:
    
      register_ftrace_function() and unregister_ftrace_function()
    
    This is because their parameter "ops" must not be evaluated since
    code using the function is allowed to #ifdef out the creation of
    the parameter.
    
    This also fixes an error caused by recent changes:
    
     kernel/trace/trace_irqsoff.c: In function 'start_irqsoff_tracer':
     kernel/trace/trace_irqsoff.c:571: error: expected expression before 'do'
    
    Reported-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 8415a522f430..e0ae83bbd9cc 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -82,9 +82,13 @@ void clear_ftrace_function(void);
 extern void ftrace_stub(unsigned long a0, unsigned long a1);
 
 #else /* !CONFIG_FUNCTION_TRACER */
-# define register_ftrace_function(ops) do { } while (0)
-# define unregister_ftrace_function(ops) do { } while (0)
-# define clear_ftrace_function(ops) do { } while (0)
+/*
+ * (un)register_ftrace_function must be a macro since the ops parameter
+ * must not be evaluated.
+ */
+#define register_ftrace_function(ops) ({ 0; })
+#define unregister_ftrace_function(ops) ({ 0; })
+static inline void clear_ftrace_function(void) { }
 static inline void ftrace_kill(void) { }
 static inline void ftrace_stop(void) { }
 static inline void ftrace_start(void) { }
@@ -237,11 +241,13 @@ extern int skip_trace(unsigned long ip);
 extern void ftrace_disable_daemon(void);
 extern void ftrace_enable_daemon(void);
 #else
-# define skip_trace(ip)				({ 0; })
-# define ftrace_force_update()			({ 0; })
-# define ftrace_set_filter(buf, len, reset)	do { } while (0)
-# define ftrace_disable_daemon()		do { } while (0)
-# define ftrace_enable_daemon()			do { } while (0)
+static inline int skip_trace(unsigned long ip) { return 0; }
+static inline int ftrace_force_update(void) { return 0; }
+static inline void ftrace_set_filter(unsigned char *buf, int len, int reset)
+{
+}
+static inline void ftrace_disable_daemon(void) { }
+static inline void ftrace_enable_daemon(void) { }
 static inline void ftrace_release_mod(struct module *mod) {}
 static inline int register_ftrace_command(struct ftrace_func_command *cmd)
 {
@@ -314,16 +320,16 @@ static inline void __ftrace_enabled_restore(int enabled)
   extern void time_hardirqs_on(unsigned long a0, unsigned long a1);
   extern void time_hardirqs_off(unsigned long a0, unsigned long a1);
 #else
-# define time_hardirqs_on(a0, a1)		do { } while (0)
-# define time_hardirqs_off(a0, a1)		do { } while (0)
+  static inline void time_hardirqs_on(unsigned long a0, unsigned long a1) { }
+  static inline void time_hardirqs_off(unsigned long a0, unsigned long a1) { }
 #endif
 
 #ifdef CONFIG_PREEMPT_TRACER
   extern void trace_preempt_on(unsigned long a0, unsigned long a1);
   extern void trace_preempt_off(unsigned long a0, unsigned long a1);
 #else
-# define trace_preempt_on(a0, a1)		do { } while (0)
-# define trace_preempt_off(a0, a1)		do { } while (0)
+  static inline void trace_preempt_on(unsigned long a0, unsigned long a1) { }
+  static inline void trace_preempt_off(unsigned long a0, unsigned long a1) { }
 #endif
 
 #ifdef CONFIG_FTRACE_MCOUNT_RECORD

commit 62b915f1060996a8e1f69be50e3b8e9e43b710cb
Author: Jiri Olsa <jolsa@redhat.com>
Date:   Fri Apr 2 19:01:22 2010 +0200

    tracing: Add graph output support for irqsoff tracer
    
    Add function graph output to irqsoff tracer.
    
    The graph output is enabled by setting new 'display-graph' trace option.
    
    Signed-off-by: Jiri Olsa <jolsa@redhat.com>
    LKML-Reference: <1270227683-14631-4-git-send-email-jolsa@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index ea5b1aae0e8b..8415a522f430 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -352,6 +352,10 @@ struct ftrace_graph_ret {
 	int depth;
 };
 
+/* Type of the callback handlers for tracing function graph*/
+typedef void (*trace_func_graph_ret_t)(struct ftrace_graph_ret *); /* return */
+typedef int (*trace_func_graph_ent_t)(struct ftrace_graph_ent *); /* entry */
+
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
 
 /* for init task */
@@ -400,10 +404,6 @@ extern char __irqentry_text_end[];
 
 #define FTRACE_RETFUNC_DEPTH 50
 #define FTRACE_RETSTACK_ALLOC_SIZE 32
-/* Type of the callback handlers for tracing function graph*/
-typedef void (*trace_func_graph_ret_t)(struct ftrace_graph_ret *); /* return */
-typedef int (*trace_func_graph_ent_t)(struct ftrace_graph_ent *); /* entry */
-
 extern int register_ftrace_graph(trace_func_graph_ret_t retfunc,
 				trace_func_graph_ent_t entryfunc);
 
@@ -441,6 +441,13 @@ static inline void unpause_graph_tracing(void)
 static inline void ftrace_graph_init_task(struct task_struct *t) { }
 static inline void ftrace_graph_exit_task(struct task_struct *t) { }
 
+static inline int register_ftrace_graph(trace_func_graph_ret_t retfunc,
+			  trace_func_graph_ent_t entryfunc)
+{
+	return -1;
+}
+static inline void unregister_ftrace_graph(void) { }
+
 static inline int task_curr_ret_stack(struct task_struct *tsk)
 {
 	return -1;

commit cecbca96da387428e220e307a9c945e37e2f4d9e
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sun Apr 18 19:08:41 2010 +0200

    tracing: Dump either the oops's cpu source or all cpus buffers
    
    The ftrace_dump_on_oops kernel parameter, sysctl and sysrq let one
    dump every cpu buffers when an oops or panic happens.
    
    It's nice when you have few cpus but it may take ages if have many,
    plus you miss the real origin of the problem in all the cpu traces.
    
    Sometimes, all you need is to dump the cpu buffer that triggered the
    opps, most of the time it is our main interest.
    
    This patch modifies ftrace_dump_on_oops to handle this choice.
    
    The ftrace_dump_on_oops kernel parameter, when it comes alone, has
    the same behaviour than before. But ftrace_dump_on_oops=orig_cpu
    will only dump the buffer of the cpu that oops'ed.
    
    Similarly, sysctl kernel.ftrace_dump_on_oops=1 and
    echo 1 > /proc/sys/kernel/ftrace_dump_on_oops keep their previous
    behaviour. But setting 2 jumps into cpu origin dump mode.
    
    v2: Fix double setup
    v3: Fix spelling issues reported by Randy Dunlap
    v4: Also update __ftrace_dump in the selftests
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: David S. Miller <davem@davemloft.net>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Lai Jiangshan <laijs@cn.fujitsu.com>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 01e6adea07ec..ea5b1aae0e8b 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -492,7 +492,9 @@ static inline int test_tsk_trace_graph(struct task_struct *tsk)
 	return tsk->trace & TSK_TRACE_FL_GRAPH;
 }
 
-extern int ftrace_dump_on_oops;
+enum ftrace_dump_mode;
+
+extern enum ftrace_dump_mode ftrace_dump_on_oops;
 
 #ifdef CONFIG_PREEMPT
 #define INIT_TRACE_RECURSION		.trace_recursion = 0,

commit faa4602e47690fb11221e00f9b9697c8dc0d4b19
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Mar 25 14:51:50 2010 +0100

    x86, perf, bts, mm: Delete the never used BTS-ptrace code
    
    Support for the PMU's BTS features has been upstreamed in
    v2.6.32, but we still have the old and disabled ptrace-BTS,
    as Linus noticed it not so long ago.
    
    It's buggy: TIF_DEBUGCTLMSR is trampling all over that MSR without
    regard for other uses (perf) and doesn't provide the flexibility
    needed for perf either.
    
    Its users are ptrace-block-step and ptrace-bts, since ptrace-bts
    was never used and ptrace-block-step can be implemented using a
    much simpler approach.
    
    So axe all 3000 lines of it. That includes the *locked_memory*()
    APIs in mm/mlock.c as well.
    
    Reported-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Markus Metzger <markus.t.metzger@intel.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    LKML-Reference: <20100325135413.938004390@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 01e6adea07ec..cc12b3c556b3 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -504,18 +504,6 @@ extern int ftrace_dump_on_oops;
 #define INIT_TRACE_RECURSION
 #endif
 
-#ifdef CONFIG_HW_BRANCH_TRACER
-
-void trace_hw_branch(u64 from, u64 to);
-void trace_hw_branch_oops(void);
-
-#else /* CONFIG_HW_BRANCH_TRACER */
-
-static inline void trace_hw_branch(u64 from, u64 to) {}
-static inline void trace_hw_branch_oops(void) {}
-
-#endif /* CONFIG_HW_BRANCH_TRACER */
-
 #ifdef CONFIG_FTRACE_SYSCALLS
 
 unsigned long arch_syscall_addr(int nr);

commit 6556a6743549defc32e5f90ee2cb1ecd833a44c3
Merge: e0d272429a34 1dd2980d9900
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Feb 28 10:20:25 2010 -0800

    Merge branch 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (172 commits)
      perf_event, amd: Fix spinlock initialization
      perf_event: Fix preempt warning in perf_clock()
      perf tools: Flush maps on COMM events
      perf_events, x86: Split PMU definitions into separate files
      perf annotate: Handle samples not at objdump output addr boundaries
      perf_events, x86: Remove superflous MSR writes
      perf_events: Simplify code by removing cpu argument to hw_perf_group_sched_in()
      perf_events, x86: AMD event scheduling
      perf_events: Add new start/stop PMU callbacks
      perf_events: Report the MMAP pgoff value in bytes
      perf annotate: Defer allocating sym_priv->hist array
      perf symbols: Improve debugging information about symtab origins
      perf top: Use a macro instead of a constant variable
      perf symbols: Check the right return variable
      perf/scripts: Tag syscall_name helper as not yet available
      perf/scripts: Add perf-trace-python Documentation
      perf/scripts: Remove unnecessary PyTuple resizes
      perf/scripts: Add syscall tracing scripts
      perf/scripts: Add Python scripting engine
      perf/scripts: Remove check-perf-trace from listed scripts
      ...
    
    Fix trivial conflict in tools/perf/util/probe-event.c

commit e7b8e675d9c71b868b66f62f725a948047514719
Author: Mike Frysinger <vapier@gentoo.org>
Date:   Tue Jan 26 04:40:03 2010 -0500

    tracing: Unify arch_syscall_addr() implementations
    
    Most implementations of arch_syscall_addr() are the same, so create a
    default version in common code and move the one piece that differs (the
    syscall table) to asm/syscall.h.  New arch ports don't have to waste
    time copying & pasting this simple function.
    
    The s390/sparc versions need to be different, so document why.
    
    Signed-off-by: Mike Frysinger <vapier@gentoo.org>
    Acked-by: David S. Miller <davem@davemloft.net>
    Acked-by: Paul Mundt <lethal@linux-sh.org>
    Acked-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    LKML-Reference: <1264498803-17278-1-git-send-email-vapier@gentoo.org>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 0b4f97d24d7f..1cbb36f2759c 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -511,4 +511,10 @@ static inline void trace_hw_branch_oops(void) {}
 
 #endif /* CONFIG_HW_BRANCH_TRACER */
 
+#ifdef CONFIG_FTRACE_SYSCALLS
+
+unsigned long arch_syscall_addr(int nr);
+
+#endif /* CONFIG_FTRACE_SYSCALLS */
+
 #endif /* _LINUX_FTRACE_H */

commit f24bb999d2b9f2950e5cac5b69bffedf73c24ea4
Author: Masami Hiramatsu <mhiramat@redhat.com>
Date:   Tue Feb 2 16:49:25 2010 -0500

    ftrace: Remove record freezing
    
    Remove record freezing. Because kprobes never puts probe on
    ftrace's mcount call anymore, it doesn't need ftrace to check
    whether kprobes on it.
    
    Signed-off-by: Masami Hiramatsu <mhiramat@redhat.com>
    Cc: systemtap <systemtap@sources.redhat.com>
    Cc: DLE <dle-develop@lists.sourceforge.net>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: przemyslaw@pawelczyk.it
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <20100202214925.4694.73469.stgit@dhcp-100-2-132.bos.redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 9d127efed43c..eb054ae95605 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -143,7 +143,6 @@ enum {
 	FTRACE_FL_ENABLED	= (1 << 3),
 	FTRACE_FL_NOTRACE	= (1 << 4),
 	FTRACE_FL_CONVERTED	= (1 << 5),
-	FTRACE_FL_FROZEN	= (1 << 6),
 };
 
 struct dyn_ftrace {

commit 2cfa19780d61740f65790c5bae363b759d7c96fa
Author: Masami Hiramatsu <mhiramat@redhat.com>
Date:   Tue Feb 2 16:49:11 2010 -0500

    ftrace/alternatives: Introducing *_text_reserved functions
    
    Introducing *_text_reserved functions for checking the text
    address range is partially reserved or not. This patch provides
    checking routines for x86 smp alternatives and dynamic ftrace.
    Since both functions modify fixed pieces of kernel text, they
    should reserve and protect those from other dynamic text
    modifier, like kprobes.
    
    This will also be extended when introducing other subsystems
    which modify fixed pieces of kernel text. Dynamic text modifiers
    should avoid those.
    
    Signed-off-by: Masami Hiramatsu <mhiramat@redhat.com>
    Cc: systemtap <systemtap@sources.redhat.com>
    Cc: DLE <dle-develop@lists.sourceforge.net>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: przemyslaw@pawelczyk.it
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Jim Keniston <jkenisto@us.ibm.com>
    Cc: Mathieu Desnoyers <compudj@krystal.dyndns.org>
    Cc: Jason Baron <jbaron@redhat.com>
    LKML-Reference: <20100202214911.4694.16587.stgit@dhcp-100-2-132.bos.redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 0b4f97d24d7f..9d127efed43c 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -134,6 +134,8 @@ extern void
 unregister_ftrace_function_probe_func(char *glob, struct ftrace_probe_ops *ops);
 extern void unregister_ftrace_function_probe_all(char *glob);
 
+extern int ftrace_text_reserved(void *start, void *end);
+
 enum {
 	FTRACE_FL_FREE		= (1 << 0),
 	FTRACE_FL_FAILED	= (1 << 1),
@@ -250,6 +252,10 @@ static inline int unregister_ftrace_command(char *cmd_name)
 {
 	return -EINVAL;
 }
+static inline int ftrace_text_reserved(void *start, void *end)
+{
+	return 0;
+}
 #endif /* CONFIG_DYNAMIC_FTRACE */
 
 /* totally disable ftrace - can not re-enable after this */

commit e7247a15ff3bbdab0a8b402dffa1171e5c05a8e0
Author: jolsa@redhat.com <jolsa@redhat.com>
Date:   Wed Oct 7 19:00:35 2009 +0200

    tracing: correct module boundaries for ftrace_release
    
    When the module is about the unload we release its call records.
    The ftrace_release function was given wrong values representing
    the module core boundaries, thus not releasing its call records.
    
    Plus making ftrace_release function module specific.
    
    Signed-off-by: Jiri Olsa <jolsa@redhat.com>
    LKML-Reference: <1254934835-363-3-git-send-email-jolsa@redhat.com>
    Cc: stable@kernel.org
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index cd3d2abaf30a..0b4f97d24d7f 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -241,7 +241,7 @@ extern void ftrace_enable_daemon(void);
 # define ftrace_set_filter(buf, len, reset)	do { } while (0)
 # define ftrace_disable_daemon()		do { } while (0)
 # define ftrace_enable_daemon()			do { } while (0)
-static inline void ftrace_release(void *start, unsigned long size) { }
+static inline void ftrace_release_mod(struct module *mod) {}
 static inline int register_ftrace_command(struct ftrace_func_command *cmd)
 {
 	return -EINVAL;

commit 8d65af789f3e2cf4cfbdbf71a0f7a61ebcd41d38
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Wed Sep 23 15:57:19 2009 -0700

    sysctl: remove "struct file *" argument of ->proc_handler
    
    It's unused.
    
    It isn't needed -- read or write flag is already passed and sysctl
    shouldn't care about the rest.
    
    It _was_ used in two places at arch/frv for some reason.
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: James Morris <jmorris@namei.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 3c0924a18daf..cd3d2abaf30a 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -19,7 +19,7 @@
 extern int ftrace_enabled;
 extern int
 ftrace_enable_sysctl(struct ctl_table *table, int write,
-		     struct file *filp, void __user *buffer, size_t *lenp,
+		     void __user *buffer, size_t *lenp,
 		     loff_t *ppos);
 
 typedef void (*ftrace_func_t)(unsigned long ip, unsigned long parent_ip);
@@ -94,7 +94,7 @@ static inline void ftrace_start(void) { }
 extern int stack_tracer_enabled;
 int
 stack_trace_sysctl(struct ctl_table *table, int write,
-		   struct file *file, void __user *buffer, size_t *lenp,
+		   void __user *buffer, size_t *lenp,
 		   loff_t *ppos);
 #endif
 

commit 83ba7c34d2b82dc608647f629616df393ab883f9
Author: Jaswinder Singh Rajput <jaswinderrajput@gmail.com>
Date:   Sun Sep 20 16:24:00 2009 +0530

    includecheck fix: include/linux, ftrace.h
    
    fix the following 'make includecheck' warning:
    
      include/linux/ftrace.h: linux/sched.h is included more than once.
    
    Signed-off-by: Jaswinder Singh Rajput <jaswinderrajput@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Sam Ravnborg <sam@ravnborg.org>
    LKML-Reference: <1247068321.4382.102.camel@ht.satnam>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index dc3b1328aaeb..3c0924a18daf 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -446,7 +446,6 @@ static inline void unpause_graph_tracing(void) { }
 #endif /* CONFIG_FUNCTION_GRAPH_TRACER */
 
 #ifdef CONFIG_TRACING
-#include <linux/sched.h>
 
 /* flags for current->trace */
 enum {

commit 71e308a239c098673570d0b417d42262bb535909
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Jun 18 12:45:08 2009 -0400

    function-graph: add stack frame test
    
    In case gcc does something funny with the stack frames, or the return
    from function code, we would like to detect that.
    
    An arch may implement passing of a variable that is unique to the
    function and can be saved on entering a function and can be tested
    when exiting the function. Usually the frame pointer can be used for
    this purpose.
    
    This patch also implements this for x86. Where it passes in the stack
    frame of the parent function, and will test that frame on exit.
    
    There was a case in x86_32 with optimize for size (-Os) where, for a
    few functions, gcc would align the stack frame and place a copy of the
    return address into it. The function graph tracer modified the copy and
    not the actual return address. On return from the funtion, it did not go
    to the tracer hook, but returned to the parent. This broke the function
    graph tracer, because the return of the parent (where gcc did not do
    this funky manipulation) returned to the location that the child function
    was suppose to. This caused strange kernel crashes.
    
    This test detected the problem and pointed out where the issue was.
    
    This modifies the parameters of one of the functions that the arch
    specific code calls, so it includes changes to arch code to accommodate
    the new prototype.
    
    Note, I notice that the parsic arch implements its own push_return_trace.
    This is now a generic function and the ftrace_push_return_trace should be
    used instead. This patch does not touch that code.
    
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Kyle McMartin <kyle@mcmartin.ca>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 39b95c56587e..dc3b1328aaeb 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -362,6 +362,7 @@ struct ftrace_ret_stack {
 	unsigned long func;
 	unsigned long long calltime;
 	unsigned long long subtime;
+	unsigned long fp;
 };
 
 /*
@@ -372,7 +373,8 @@ struct ftrace_ret_stack {
 extern void return_to_handler(void);
 
 extern int
-ftrace_push_return_trace(unsigned long ret, unsigned long func, int *depth);
+ftrace_push_return_trace(unsigned long ret, unsigned long func, int *depth,
+			 unsigned long frame_pointer);
 
 /*
  * Sometimes we don't want to trace a function with the function

commit 261842b7c9099f56de2eb969c8ad65402d68e00e
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Apr 16 21:41:52 2009 -0400

    tracing: add same level recursion detection
    
    The tracing infrastructure allows for recursion. That is, an interrupt
    may interrupt the act of tracing an event, and that interrupt may very well
    perform its own trace. This is a recursive trace, and is fine to do.
    
    The problem arises when there is a bug, and the utility doing the trace
    calls something that recurses back into the tracer. This recursion is not
    caused by an external event like an interrupt, but by code that is not
    expected to recurse. The result could be a lockup.
    
    This patch adds a bitmask to the task structure that keeps track
    of the trace recursion. To find the interrupt depth, the following
    algorithm is used:
    
      level = hardirq_count() + softirq_count() + in_nmi;
    
    Here, level will be the depth of interrutps and softirqs, and even handles
    the nmi. Then the corresponding bit is set in the recursion bitmask.
    If the bit was already set, we know we had a recursion at the same level
    and we warn about it and fail the writing to the buffer.
    
    After the data has been committed to the buffer, we clear the bit.
    No atomics are needed. The only races are with interrupts and they reset
    the bitmask before returning anywy.
    
    [ Impact: detect same irq level trace recursion ]
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 97c83e1bc589..39b95c56587e 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -488,8 +488,15 @@ static inline int test_tsk_trace_graph(struct task_struct *tsk)
 
 extern int ftrace_dump_on_oops;
 
+#ifdef CONFIG_PREEMPT
+#define INIT_TRACE_RECURSION		.trace_recursion = 0,
+#endif
+
 #endif /* CONFIG_TRACING */
 
+#ifndef INIT_TRACE_RECURSION
+#define INIT_TRACE_RECURSION
+#endif
 
 #ifdef CONFIG_HW_BRANCH_TRACER
 

commit 93eb677d74a4f7d3edfb678c94f6c0544d9fbad2
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed Apr 15 13:24:06 2009 -0400

    ftrace: use module notifier for function tracer
    
    The hooks in the module code for the function tracer must be called
    before any of that module code runs. The function tracer hooks
    modify the module (replacing calls to mcount to nops). If the code
    is executed while the change occurs, then the CPU can take a GPF.
    
    To handle the above with a bit of paranoia, I originally implemented
    the hooks as calls directly from the module code.
    
    After examining the notifier calls, it looks as though the start up
    notify is called before any of the module's code is executed. This makes
    the use of the notify safe with ftrace.
    
    Only the startup notify is required to be "safe". The shutdown simply
    removes the entries from the ftrace function list, and does not modify
    any code.
    
    This change has another benefit. It removes a issue with a reverse dependency
    in the mutexes of ftrace_lock and module_mutex.
    
    [ Impact: fix lock dependency bug, cleanup ]
    
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 53869bef6102..97c83e1bc589 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -233,8 +233,6 @@ extern int ftrace_arch_read_dyn_info(char *buf, int size);
 
 extern int skip_trace(unsigned long ip);
 
-extern void ftrace_release(void *start, unsigned long size);
-
 extern void ftrace_disable_daemon(void);
 extern void ftrace_enable_daemon(void);
 #else
@@ -325,13 +323,8 @@ static inline void __ftrace_enabled_restore(int enabled)
 
 #ifdef CONFIG_FTRACE_MCOUNT_RECORD
 extern void ftrace_init(void);
-extern void ftrace_init_module(struct module *mod,
-			       unsigned long *start, unsigned long *end);
 #else
 static inline void ftrace_init(void) { }
-static inline void
-ftrace_init_module(struct module *mod,
-		   unsigned long *start, unsigned long *end) { }
 #endif
 
 /*

commit 1cad1252ed279ea59f3f8d3d3a5817eeb2f7a4d3
Merge: dcef788eb965 93cfb3c9fd83
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Apr 10 12:46:28 2009 +0200

    Merge branch 'tracing/urgent' into tracing/core
    
    Merge reason: pick up both v2.6.30-rc1 [which includes tracing/urgent fixes]
                  and pick up the current lineup of tracing/urgent fixes as well
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 47788c58e66c050982241d9a05eb690daceb05a9
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Apr 8 20:40:59 2009 +0200

    tracing/syscalls: use a dedicated file header
    
    Impact: fix build warnings and possibe compat misbehavior on IA64
    
    Building a kernel on ia64 might trigger these ugly build warnings:
    
    CC      arch/ia64/ia32/sys_ia32.o
    In file included from arch/ia64/ia32/sys_ia32.c:55:
    arch/ia64/ia32/ia32priv.h:290:1: warning: "elf_check_arch" redefined
    In file included from include/linux/elf.h:7,
                     from include/linux/module.h:14,
                     from include/linux/ftrace.h:8,
                     from include/linux/syscalls.h:68,
                     from arch/ia64/ia32/sys_ia32.c:18:
    arch/ia64/include/asm/elf.h:19:1: warning: this is the location of the previous definition
    [...]
    
    sys_ia32.c includes linux/syscalls.h which in turn includes linux/ftrace.h
    to import the syscalls tracing prototypes.
    
    But including ftrace.h can pull too much things for a low level file,
    especially on ia64 where the ia32 private headers conflict with higher
    level headers.
    
    Now we isolate the syscall tracing headers in their own lightweight file.
    
    Reported-by: Tony Luck <tony.luck@intel.com>
    Tested-by: Tony Luck <tony.luck@intel.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Tony Luck <tony.luck@intel.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Jason Baron <jbaron@redhat.com>
    Cc: "Frank Ch. Eigler" <fche@redhat.com>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: Jiaying Zhang <jiayingz@google.com>
    Cc: Michael Rubin <mrubin@google.com>
    Cc: Martin Bligh <mbligh@google.com>
    Cc: Michael Davidson <md@google.com>
    LKML-Reference: <20090408184058.GB6017@nowhere>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index ff112a872d75..8a0c2f221e6b 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -511,33 +511,4 @@ static inline void trace_hw_branch_oops(void) {}
 
 #endif /* CONFIG_HW_BRANCH_TRACER */
 
-/*
- * A syscall entry in the ftrace syscalls array.
- *
- * @name: name of the syscall
- * @nb_args: number of parameters it takes
- * @types: list of types as strings
- * @args: list of args as strings (args[i] matches types[i])
- */
-struct syscall_metadata {
-	const char	*name;
-	int		nb_args;
-	const char	**types;
-	const char	**args;
-};
-
-#ifdef CONFIG_FTRACE_SYSCALLS
-extern void arch_init_ftrace_syscalls(void);
-extern struct syscall_metadata *syscall_nr_to_meta(int nr);
-extern void start_ftrace_syscalls(void);
-extern void stop_ftrace_syscalls(void);
-extern void ftrace_syscall_enter(struct pt_regs *regs);
-extern void ftrace_syscall_exit(struct pt_regs *regs);
-#else
-static inline void start_ftrace_syscalls(void) { }
-static inline void stop_ftrace_syscalls(void) { }
-static inline void ftrace_syscall_enter(struct pt_regs *regs) { }
-static inline void ftrace_syscall_exit(struct pt_regs *regs) { }
-#endif
-
 #endif /* _LINUX_FTRACE_H */

commit f876d346e3807647b1de411de6a86c44821896ca
Author: Tetsuo Handa <penguin-kernel@i-love.sakura.ne.jp>
Date:   Wed Apr 8 14:05:43 2009 +0900

    tracing: append a comma to INIT_FTRACE_GRAPH
    
    Impact: dont break future extensions of INIT_TASK
    
    While not a problem right now, due to lack of a comma, build fails if
    elements are appended to INIT_TASK() macro in development code:
    
     arch/x86/kernel/init_task.c:33: error: request for member `XXXXXXXXXX' in something not a structure or union
     arch/x86/kernel/init_task.c:33: error: initializer element is not constant
     arch/x86/kernel/init_task.c:33: error: (near initialization for `init_task.ret_stack')
     make[1]: *** [arch/x86/kernel/init_task.o] Error 1
     make: *** [arch/x86/kernel] Error 2
    
    Signed-off-by: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Cc: srostedt@redhat.com
    LKML-Reference: <200904080505.n3855hcn017109@www262.sakura.ne.jp>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index da5405dce347..ff112a872d75 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -357,7 +357,7 @@ struct ftrace_graph_ret {
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
 
 /* for init task */
-#define INIT_FTRACE_GRAPH		.ret_stack = NULL
+#define INIT_FTRACE_GRAPH		.ret_stack = NULL,
 
 /*
  * Stack of return addresses for functions

commit 86665c75da41889f92b774f31ea5a9a436f392a8
Merge: 93776a8ec746 1bbe2a83ab68
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Apr 7 14:41:14 2009 +0200

    Merge branch 'tracing/urgent' into tracing/ftrace

commit 5ac9f62267dc92c7735c642a5942d9e6c1190308
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Wed Mar 25 20:55:00 2009 -0400

    function-graph: add proper initialization for init task
    
    Impact: fix to crash going to kexec
    
    The init task did not properly initialize the function graph pointers.
    Altough these pointers are NULL, they can not be assumed to be NULL
    for the init task, and must still be properly initialize.
    
    This usually is not an issue since a problem only arises when a task
    exits, and the init tasks do not usually exit. But when doing tests
    with kexec, the init tasks do exit, and the bug appears.
    
    This patch properly initializes the init tasks function graph data
    structures.
    
    Reported-and-Tested-by: Yinghai Lu <yinghai@kernel.org>
    LKML-Reference: <alpine.DEB.2.00.0903252053080.5675@gandalf.stny.rr.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 015a3d22cf74..da5405dce347 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -356,6 +356,9 @@ struct ftrace_graph_ret {
 
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
 
+/* for init task */
+#define INIT_FTRACE_GRAPH		.ret_stack = NULL
+
 /*
  * Stack of return addresses for functions
  * of a thread.
@@ -430,10 +433,11 @@ static inline void unpause_graph_tracing(void)
 {
 	atomic_dec(&current->tracing_graph_pause);
 }
-#else
+#else /* !CONFIG_FUNCTION_GRAPH_TRACER */
 
 #define __notrace_funcgraph
 #define __irq_entry
+#define INIT_FTRACE_GRAPH
 
 static inline void ftrace_graph_init_task(struct task_struct *t) { }
 static inline void ftrace_graph_exit_task(struct task_struct *t) { }
@@ -445,7 +449,7 @@ static inline int task_curr_ret_stack(struct task_struct *tsk)
 
 static inline void pause_graph_tracing(void) { }
 static inline void unpause_graph_tracing(void) { }
-#endif
+#endif /* CONFIG_FUNCTION_GRAPH_TRACER */
 
 #ifdef CONFIG_TRACING
 #include <linux/sched.h>

commit a2a16d6a3156ef7309ca7328a20c35df9418e670
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Mar 24 23:17:58 2009 -0400

    function-graph: add option to calculate graph time or not
    
    graph time is the time that a function is executing another function.
    Thus if function A calls B, if graph-time is set, then the time for
    A includes B. This is the default behavior. But if graph-time is off,
    then the time spent executing B is subtracted from A.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 015a3d22cf74..9e0a8d245e55 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -365,6 +365,7 @@ struct ftrace_ret_stack {
 	unsigned long ret;
 	unsigned long func;
 	unsigned long long calltime;
+	unsigned long long subtime;
 };
 
 /*
@@ -376,8 +377,6 @@ extern void return_to_handler(void);
 
 extern int
 ftrace_push_return_trace(unsigned long ret, unsigned long func, int *depth);
-extern void
-ftrace_pop_return_trace(struct ftrace_graph_ret *trace, unsigned long *ret);
 
 /*
  * Sometimes we don't want to trace a function with the function

commit 493762fc534c71d11d489f872c4b4a2c61173668
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon Mar 23 17:12:36 2009 -0400

    tracing: move function profiler data out of function struct
    
    Impact: reduce size of memory in function profiler
    
    The function profiler originally introduces its counters into the
    function records itself. There is 20 thousand different functions on
    a normal system, and that is adding 20 thousand counters for profiling
    event when not needed.
    
    A normal run of the profiler yields only a couple of thousand functions
    executed, depending on what is being profiled. This means we have around
    18 thousand useless counters.
    
    This patch rectifies this by moving the data out of the function
    records used by dynamic ftrace. Data is preallocated to hold the functions
    when the profiling begins. Checks are made during profiling to see if
    more recorcds should be allocated, and they are allocated if it is safe
    to do so.
    
    This also removes the dependency from using dynamic ftrace, and also
    removes the overhead by having it enabled.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 0456c3a51c66..015a3d22cf74 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -153,10 +153,6 @@ struct dyn_ftrace {
 		unsigned long		flags;
 		struct dyn_ftrace	*newlist;
 	};
-#ifdef CONFIG_FUNCTION_PROFILER
-	unsigned long			counter;
-	struct hlist_node		node;
-#endif
 	struct dyn_arch_ftrace		arch;
 };
 

commit bac429f037f1a51a74d62bad6d1518c3be065df3
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Mar 20 12:50:56 2009 -0400

    tracing: add function profiler
    
    Impact: new profiling feature
    
    This patch adds a function profiler. In debugfs/tracing/ two new
    files are created.
    
      function_profile_enabled  - to enable or disable profiling
    
      trace_stat/functions   - the profiled functions.
    
    For example:
    
      echo 1 > /debugfs/tracing/function_profile_enabled
      ./hackbench 50
      echo 0 > /debugfs/tracing/function_profile_enabled
    
    yields:
    
      cat /debugfs/tracing/trace_stat/functions
    
      Function                               Hit
      --------                               ---
      _spin_lock                        10106442
      _spin_unlock                      10097492
      kfree                              6013704
      _spin_unlock_irqrestore            4423941
      _spin_lock_irqsave                 4406825
      __phys_addr                        4181686
      __slab_free                        4038222
      dput                               4030130
      path_put                           4023387
      unroll_tree_refs                   4019532
    [...]
    
    The most hit functions are listed first. Functions that are not
    hit are not listed.
    
    This feature depends on and uses dynamic function tracing. When the
    function profiling is disabled, no overhead occurs. But it still
    takes up around 300KB to hold the data, thus it is not recomended
    to keep it enabled for systems low on memory.
    
    When a '1' is echoed into the function_profile_enabled file, the
    counters for is function is reset back to zero. Thus you can see what
    functions are hit most by different programs.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 015a3d22cf74..0456c3a51c66 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -153,6 +153,10 @@ struct dyn_ftrace {
 		unsigned long		flags;
 		struct dyn_ftrace	*newlist;
 	};
+#ifdef CONFIG_FUNCTION_PROFILER
+	unsigned long			counter;
+	struct hlist_node		node;
+#endif
 	struct dyn_arch_ftrace		arch;
 };
 

commit ee000b7f9fe429d2470c674ccec8d344f6789e0d
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Tue Mar 24 13:38:06 2009 +0800

    tracing: use union for multi-usages field
    
    Impact: cleanup
    
    struct dyn_ftrace::ip has different usages in his lifecycle,
    we use union for it. And also for struct dyn_ftrace::flags.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: Steven Rostedt <srostedt@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <49C871BE.3080405@cn.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 1141248c84ee..015a3d22cf74 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -145,9 +145,15 @@ enum {
 };
 
 struct dyn_ftrace {
-	unsigned long		ip; /* address of mcount call-site */
-	unsigned long		flags;
-	struct dyn_arch_ftrace	arch;
+	union {
+		unsigned long		ip; /* address of mcount call-site */
+		struct dyn_ftrace	*freelist;
+	};
+	union {
+		unsigned long		flags;
+		struct dyn_ftrace	*newlist;
+	};
+	struct dyn_arch_ftrace		arch;
 };
 
 int ftrace_force_update(void);

commit 5d1a03dc541dc6672e60e57249ed22f40654ca47
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon Mar 23 23:38:49 2009 -0400

    function-graph: moved the timestamp from arch to generic code
    
    This patch move the timestamp from happening in the arch specific
    code into the general code. This allows for better control by the tracer
    to time manipulation.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index db3fed630db3..1141248c84ee 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -369,8 +369,7 @@ struct ftrace_ret_stack {
 extern void return_to_handler(void);
 
 extern int
-ftrace_push_return_trace(unsigned long ret, unsigned long long time,
-			 unsigned long func, int *depth);
+ftrace_push_return_trace(unsigned long ret, unsigned long func, int *depth);
 extern void
 ftrace_pop_return_trace(struct ftrace_graph_ret *trace, unsigned long *ret);
 

commit 7243f2145a9b06e5cf9a49fc9b8b9a4fff6fb42e
Merge: b478b782e110 62395efdb0ef 5bee17f18b59
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Mar 16 09:12:42 2009 +0100

    Merge branches 'tracing/ftrace', 'tracing/syscalls' and 'linus' into tracing/core
    
    Conflicts:
            arch/parisc/kernel/irq.c

commit bed1ffca022cc876fb83161d26670e9b5d3cf36b
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Fri Mar 13 15:42:11 2009 +0100

    tracing/syscalls: core infrastructure for syscalls tracing, enhancements
    
    Impact: new feature
    
    This adds the generic support for syscalls tracing. This is
    currently exploited through a devoted tracer but other tracing
    engines can use it. (They just have to play with
    {start,stop}_ftrace_syscalls() and use the display callbacks
    unless they want to override them.)
    
    The syscalls prototypes definitions are abused here to steal
    some metadata informations:
    
    - syscall name, param types, param names, number of params
    
    The syscall addr is not directly saved during this definition
    because we don't know if its prototype is available in the
    namespace. But we don't really need it. The arch has just to
    build a function able to resolve the syscall number to its
    metadata struct.
    
    The current tracer prints the syscall names, parameters names
    and values (and their types optionally). Currently the value is
    a raw hex but higher level values diplaying is on my TODO list.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <1236955332-10133-2-git-send-email-fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index c146c1021a29..6dc1c652447e 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -506,13 +506,21 @@ static inline void trace_hw_branch_oops(void) {}
 /*
  * A syscall entry in the ftrace syscalls array.
  *
- * @syscall_nr: syscall number
+ * @name: name of the syscall
+ * @nb_args: number of parameters it takes
+ * @types: list of types as strings
+ * @args: list of args as strings (args[i] matches types[i])
  */
-struct syscall_trace_entry {
-	int		syscall_nr;
+struct syscall_metadata {
+	const char	*name;
+	int		nb_args;
+	const char	**types;
+	const char	**args;
 };
 
 #ifdef CONFIG_FTRACE_SYSCALLS
+extern void arch_init_ftrace_syscalls(void);
+extern struct syscall_metadata *syscall_nr_to_meta(int nr);
 extern void start_ftrace_syscalls(void);
 extern void stop_ftrace_syscalls(void);
 extern void ftrace_syscall_enter(struct pt_regs *regs);

commit e94142a67f8bad494c593f0a07c9fc2fbec98c0e
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Fri Mar 13 17:51:27 2009 +0800

    ftrace: remove struct list_head from struct dyn_ftrace
    
    Impact: save memory
    
    The struct dyn_ftrace table is very large, this patch will save
    about 50%.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: Steven Rostedt <srostedt@redhat.com>
    LKML-Reference: <49BA2C9F.8020009@cn.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index c146c1021a29..9d598bbf28a6 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -145,7 +145,6 @@ enum {
 };
 
 struct dyn_ftrace {
-	struct list_head	list;
 	unsigned long		ip; /* address of mcount call-site */
 	unsigned long		flags;
 	struct dyn_arch_ftrace	arch;

commit ee08c6eccb7d1295516f7cf420fddf7b14e9146f
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Mar 7 05:52:59 2009 +0100

    tracing/ftrace: syscall tracing infrastructure, basics
    
    Provide basic callbacks to do syscall tracing.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Lai Jiangshan <laijs@cn.fujitsu.com>
    LKML-Reference: <1236401580-5758-2-git-send-email-fweisbec@gmail.com>
    [ simplified it to a trace_printk() for now. ]
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index e1583f2639b0..c146c1021a29 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -503,4 +503,25 @@ static inline void trace_hw_branch_oops(void) {}
 
 #endif /* CONFIG_HW_BRANCH_TRACER */
 
+/*
+ * A syscall entry in the ftrace syscalls array.
+ *
+ * @syscall_nr: syscall number
+ */
+struct syscall_trace_entry {
+	int		syscall_nr;
+};
+
+#ifdef CONFIG_FTRACE_SYSCALLS
+extern void start_ftrace_syscalls(void);
+extern void stop_ftrace_syscalls(void);
+extern void ftrace_syscall_enter(struct pt_regs *regs);
+extern void ftrace_syscall_exit(struct pt_regs *regs);
+#else
+static inline void start_ftrace_syscalls(void) { }
+static inline void stop_ftrace_syscalls(void) { }
+static inline void ftrace_syscall_enter(struct pt_regs *regs) { }
+static inline void ftrace_syscall_exit(struct pt_regs *regs) { }
+#endif
+
 #endif /* _LINUX_FTRACE_H */

commit 769b0441f438c4bb4872cb8560eb6fe51bcc09ee
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Fri Mar 6 17:21:49 2009 +0100

    tracing/core: drop the old trace_printk() implementation in favour of trace_bprintk()
    
    Impact: faster and lighter tracing
    
    Now that we have trace_bprintk() which is faster and consume lesser
    memory than trace_printk() and has the same purpose, we can now drop
    the old implementation in favour of the binary one from trace_bprintk(),
    which means we move all the implementation of trace_bprintk() to
    trace_printk(), so the Api doesn't change except that we must now use
    trace_seq_bprintk() to print the TRACE_PRINT entries.
    
    Some changes result of this:
    
    - Previously, trace_bprintk depended of a single tracer and couldn't
      work without. This tracer has been dropped and the whole implementation
      of trace_printk() (like the module formats management) is now integrated
      in the tracing core (comes with CONFIG_TRACING), though we keep the file
      trace_printk (previously trace_bprintk.c) where we can find the module
      management. Thus we don't overflow trace.c
    
    - changes some parts to use trace_seq_bprintk() to print TRACE_PRINT entries.
    
    - change a bit trace_printk/trace_vprintk macros to support non-builtin formats
      constants, and fix 'const' qualifiers warnings. But this is all transparent for
      developers.
    
    - etc...
    
    V2:
    
    - Rebase against last changes
    - Fix mispell on the changelog
    
    V3:
    
    - Rebase against last changes (moving trace_printk() to kernel.h)
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    LKML-Reference: <1236356510-8381-5-git-send-email-fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 1cc8ca453a9b..e1583f2639b0 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -223,31 +223,6 @@ extern int ftrace_make_nop(struct module *mod,
  */
 extern int ftrace_make_call(struct dyn_ftrace *rec, unsigned long addr);
 
-#ifdef CONFIG_TRACE_BPRINTK
-extern int trace_vbprintk(unsigned long ip, const char *fmt, va_list args);
-extern int __trace_bprintk(unsigned long ip, const char *fmt, ...)
-		__attribute__ ((format (printf, 2, 3)));
-
-static inline void  ____trace_bprintk_check_format(const char *fmt, ...)
-		__attribute__ ((format (printf, 1, 2)));
-static inline void ____trace_bprintk_check_format(const char *fmt, ...) {}
-#define __trace_bprintk_check_format(fmt, args...)			\
-do {									\
-	if (0)								\
-		____trace_bprintk_check_format(fmt, ##args);		\
-} while (0)
-
-#define trace_bprintk(fmt, args...)					\
-do {									\
-	static char *__attribute__((section("__trace_bprintk_fmt")))	\
-			trace_bprintk_fmt = fmt;			\
-	__trace_bprintk_check_format(fmt, ##args);			\
-	__trace_bprintk(_THIS_IP_, trace_bprintk_fmt, ##args);	\
-} while (0)
-#else
-#define trace_bprintk trace_printk
-#endif
-
 /* May be defined in arch */
 extern int ftrace_arch_read_dyn_info(char *buf, int size);
 

commit 1ba28e02a18cbdbea123836f6c98efb09cbf59ec
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Fri Mar 6 17:21:48 2009 +0100

    tracing: add trace_bprintk()
    
    Impact: add a generic printk() for tracing, like trace_printk()
    
    trace_bprintk() uses the infrastructure to record events on ring_buffer.
    
    [ fweisbec@gmail.com: ported to latest -tip, made it work if
      !CONFIG_MODULES, never free the format strings from modules
      because we can't keep track of them and conditionnaly create
      the ftrace format strings section (reported by Steven Rostedt) ]
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    LKML-Reference: <1236356510-8381-4-git-send-email-fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 1c9cdca02580..1cc8ca453a9b 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -225,6 +225,27 @@ extern int ftrace_make_call(struct dyn_ftrace *rec, unsigned long addr);
 
 #ifdef CONFIG_TRACE_BPRINTK
 extern int trace_vbprintk(unsigned long ip, const char *fmt, va_list args);
+extern int __trace_bprintk(unsigned long ip, const char *fmt, ...)
+		__attribute__ ((format (printf, 2, 3)));
+
+static inline void  ____trace_bprintk_check_format(const char *fmt, ...)
+		__attribute__ ((format (printf, 1, 2)));
+static inline void ____trace_bprintk_check_format(const char *fmt, ...) {}
+#define __trace_bprintk_check_format(fmt, args...)			\
+do {									\
+	if (0)								\
+		____trace_bprintk_check_format(fmt, ##args);		\
+} while (0)
+
+#define trace_bprintk(fmt, args...)					\
+do {									\
+	static char *__attribute__((section("__trace_bprintk_fmt")))	\
+			trace_bprintk_fmt = fmt;			\
+	__trace_bprintk_check_format(fmt, ##args);			\
+	__trace_bprintk(_THIS_IP_, trace_bprintk_fmt, ##args);	\
+} while (0)
+#else
+#define trace_bprintk trace_printk
 #endif
 
 /* May be defined in arch */

commit 1427cdf0592368bdec57276edaf714040ee8744f
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Fri Mar 6 17:21:47 2009 +0100

    tracing: infrastructure for supporting binary record
    
    Impact: save on memory for tracing
    
    Current tracers are typically using a struct(like struct ftrace_entry,
    struct ctx_switch_entry, struct special_entr etc...)to record a binary
    event. These structs can only record a their own kind of events.
    A new kind of tracer need a new struct and a lot of code too handle it.
    
    So we need a generic binary record for events. This infrastructure
    is for this purpose.
    
    [fweisbec@gmail.com: rebase against latest -tip, make it safe while sched
    tracing as reported by Steven Rostedt]
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    LKML-Reference: <1236356510-8381-3-git-send-email-fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 498769425eb2..1c9cdca02580 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -223,6 +223,9 @@ extern int ftrace_make_nop(struct module *mod,
  */
 extern int ftrace_make_call(struct dyn_ftrace *rec, unsigned long addr);
 
+#ifdef CONFIG_TRACE_BPRINTK
+extern int trace_vbprintk(unsigned long ip, const char *fmt, va_list args);
+#endif
 
 /* May be defined in arch */
 extern int ftrace_arch_read_dyn_info(char *buf, int size);

commit 16097439703bcd38e9fe5608c12add6dacb825ea
Merge: 40ada30f9621 0012693ad4f6
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Mar 6 11:39:18 2009 +0100

    Merge branches 'tracing/ftrace' and 'tracing/function-graph-tracer' into tracing/core

commit 0012693ad4f636c720fed3802027f9427962f540
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Mar 5 01:49:22 2009 +0100

    tracing/function-graph-tracer: use the more lightweight local clock
    
    Impact: decrease hangs risks with the graph tracer on slow systems
    
    Since the function graph tracer can spend too much time on timer
    interrupts, it's better now to use the more lightweight local
    clock. Anyway, the function graph traces are more reliable on a
    per cpu trace.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    LKML-Reference: <49af243d.06e9300a.53ad.ffff840c@mx.google.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 1f69ac7c1587..6ea62acbe4b6 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -1,15 +1,16 @@
 #ifndef _LINUX_FTRACE_H
 #define _LINUX_FTRACE_H
 
-#include <linux/linkage.h>
-#include <linux/fs.h>
-#include <linux/ktime.h>
-#include <linux/init.h>
-#include <linux/types.h>
-#include <linux/module.h>
+#include <linux/trace_clock.h>
 #include <linux/kallsyms.h>
+#include <linux/linkage.h>
 #include <linux/bitops.h>
+#include <linux/module.h>
+#include <linux/ktime.h>
 #include <linux/sched.h>
+#include <linux/types.h>
+#include <linux/init.h>
+#include <linux/fs.h>
 
 #include <asm/ftrace.h>
 

commit 526211bc58c4b3265352801c5a7f469af5c34711
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Mar 5 10:28:45 2009 +0100

    tracing: move utility functions from ftrace.h to kernel.h
    
    Make common utility functions such as trace_printk() and
    tracing_start()/tracing_stop() generally available to kernel
    code.
    
    Cc: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index fbb9c364e166..5b64303ec9f2 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -318,62 +318,6 @@ static inline void __ftrace_enabled_restore(int enabled)
 # define trace_preempt_off(a0, a1)		do { } while (0)
 #endif
 
-#ifdef CONFIG_TRACING
-extern int ftrace_dump_on_oops;
-
-extern void tracing_start(void);
-extern void tracing_stop(void);
-extern void ftrace_off_permanent(void);
-
-extern void
-ftrace_special(unsigned long arg1, unsigned long arg2, unsigned long arg3);
-
-/**
- * trace_printk - printf formatting in the ftrace buffer
- * @fmt: the printf format for printing
- *
- * Note: __trace_printk is an internal function for trace_printk and
- *       the @ip is passed in via the trace_printk macro.
- *
- * This function allows a kernel developer to debug fast path sections
- * that printk is not appropriate for. By scattering in various
- * printk like tracing in the code, a developer can quickly see
- * where problems are occurring.
- *
- * This is intended as a debugging tool for the developer only.
- * Please refrain from leaving trace_printks scattered around in
- * your code.
- */
-# define trace_printk(fmt...) __trace_printk(_THIS_IP_, fmt)
-extern int
-__trace_printk(unsigned long ip, const char *fmt, ...)
-	__attribute__ ((format (printf, 2, 3)));
-# define ftrace_vprintk(fmt, ap) __trace_printk(_THIS_IP_, fmt, ap)
-extern int
-__ftrace_vprintk(unsigned long ip, const char *fmt, va_list ap);
-extern void ftrace_dump(void);
-#else
-static inline void
-ftrace_special(unsigned long arg1, unsigned long arg2, unsigned long arg3) { }
-static inline int
-trace_printk(const char *fmt, ...) __attribute__ ((format (printf, 1, 2)));
-
-static inline void tracing_start(void) { }
-static inline void tracing_stop(void) { }
-static inline void ftrace_off_permanent(void) { }
-static inline int
-trace_printk(const char *fmt, ...)
-{
-	return 0;
-}
-static inline int
-ftrace_vprintk(const char *fmt, va_list ap)
-{
-	return 0;
-}
-static inline void ftrace_dump(void) { }
-#endif
-
 #ifdef CONFIG_FTRACE_MCOUNT_RECORD
 extern void ftrace_init(void);
 extern void ftrace_init_module(struct module *mod,
@@ -542,6 +486,8 @@ static inline int test_tsk_trace_graph(struct task_struct *tsk)
 	return tsk->trace & TSK_TRACE_FL_GRAPH;
 }
 
+extern int ftrace_dump_on_oops;
+
 #endif /* CONFIG_TRACING */
 
 

commit 5e1607a00bd082972629d3d68c95c8bcf902b55a
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Mar 5 10:24:48 2009 +0100

    tracing: rename ftrace_printk() => trace_printk()
    
    Impact: cleanup
    
    Use a more generic name - this also allows the prototype to move
    to kernel.h and be generally available to kernel developers who
    want to do some quick tracing.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 1f69ac7c1587..fbb9c364e166 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -329,11 +329,11 @@ extern void
 ftrace_special(unsigned long arg1, unsigned long arg2, unsigned long arg3);
 
 /**
- * ftrace_printk - printf formatting in the ftrace buffer
+ * trace_printk - printf formatting in the ftrace buffer
  * @fmt: the printf format for printing
  *
- * Note: __ftrace_printk is an internal function for ftrace_printk and
- *       the @ip is passed in via the ftrace_printk macro.
+ * Note: __trace_printk is an internal function for trace_printk and
+ *       the @ip is passed in via the trace_printk macro.
  *
  * This function allows a kernel developer to debug fast path sections
  * that printk is not appropriate for. By scattering in various
@@ -341,14 +341,14 @@ ftrace_special(unsigned long arg1, unsigned long arg2, unsigned long arg3);
  * where problems are occurring.
  *
  * This is intended as a debugging tool for the developer only.
- * Please refrain from leaving ftrace_printks scattered around in
+ * Please refrain from leaving trace_printks scattered around in
  * your code.
  */
-# define ftrace_printk(fmt...) __ftrace_printk(_THIS_IP_, fmt)
+# define trace_printk(fmt...) __trace_printk(_THIS_IP_, fmt)
 extern int
-__ftrace_printk(unsigned long ip, const char *fmt, ...)
+__trace_printk(unsigned long ip, const char *fmt, ...)
 	__attribute__ ((format (printf, 2, 3)));
-# define ftrace_vprintk(fmt, ap) __ftrace_printk(_THIS_IP_, fmt, ap)
+# define ftrace_vprintk(fmt, ap) __trace_printk(_THIS_IP_, fmt, ap)
 extern int
 __ftrace_vprintk(unsigned long ip, const char *fmt, va_list ap);
 extern void ftrace_dump(void);
@@ -356,13 +356,13 @@ extern void ftrace_dump(void);
 static inline void
 ftrace_special(unsigned long arg1, unsigned long arg2, unsigned long arg3) { }
 static inline int
-ftrace_printk(const char *fmt, ...) __attribute__ ((format (printf, 1, 2)));
+trace_printk(const char *fmt, ...) __attribute__ ((format (printf, 1, 2)));
 
 static inline void tracing_start(void) { }
 static inline void tracing_stop(void) { }
 static inline void ftrace_off_permanent(void) { }
 static inline int
-ftrace_printk(const char *fmt, ...)
+trace_printk(const char *fmt, ...)
 {
 	return 0;
 }

commit c79a61f55773d2519fd0525bf58385f7d20752d3
Author: Uwe Kleine-Koenig <u.kleine-koenig@pengutronix.de>
Date:   Fri Feb 27 21:30:03 2009 +0100

    tracing: make CALLER_ADDRx overwriteable
    
    The current definition of CALLER_ADDRx isn't suitable for all platforms.
    E.g. for ARM __builtin_return_address(N) doesn't work for N > 0 and
    AFAIK for powerpc there are no frame pointers needed to have a working
    __builtin_return_address.  This patch allows defining the CALLER_ADDRx
    macros in <asm/ftrace.h> and let these take precedence.
    
    Because now <asm/ftrace.h> is included unconditionally in
    <linux/ftrace.h> all archs that don't already had this include get an
    empty one for free.
    
    Signed-off-by: Uwe Kleine-Koenig <u.kleine-koenig@pengutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Reviewed-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 847bb3c48dd0..1f69ac7c1587 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -11,6 +11,8 @@
 #include <linux/bitops.h>
 #include <linux/sched.h>
 
+#include <asm/ftrace.h>
+
 #ifdef CONFIG_FUNCTION_TRACER
 
 extern int ftrace_enabled;
@@ -103,8 +105,6 @@ struct ftrace_func_command {
 };
 
 #ifdef CONFIG_DYNAMIC_FTRACE
-/* asm/ftrace.h must be defined for archs supporting dynamic ftrace */
-#include <asm/ftrace.h>
 
 int ftrace_arch_code_modify_prepare(void);
 int ftrace_arch_code_modify_post_process(void);
@@ -282,24 +282,25 @@ static inline void __ftrace_enabled_restore(int enabled)
 #endif
 }
 
-#ifdef CONFIG_FRAME_POINTER
-/* TODO: need to fix this for ARM */
-# define CALLER_ADDR0 ((unsigned long)__builtin_return_address(0))
-# define CALLER_ADDR1 ((unsigned long)__builtin_return_address(1))
-# define CALLER_ADDR2 ((unsigned long)__builtin_return_address(2))
-# define CALLER_ADDR3 ((unsigned long)__builtin_return_address(3))
-# define CALLER_ADDR4 ((unsigned long)__builtin_return_address(4))
-# define CALLER_ADDR5 ((unsigned long)__builtin_return_address(5))
-# define CALLER_ADDR6 ((unsigned long)__builtin_return_address(6))
-#else
-# define CALLER_ADDR0 ((unsigned long)__builtin_return_address(0))
-# define CALLER_ADDR1 0UL
-# define CALLER_ADDR2 0UL
-# define CALLER_ADDR3 0UL
-# define CALLER_ADDR4 0UL
-# define CALLER_ADDR5 0UL
-# define CALLER_ADDR6 0UL
-#endif
+#ifndef HAVE_ARCH_CALLER_ADDR
+# ifdef CONFIG_FRAME_POINTER
+#  define CALLER_ADDR0 ((unsigned long)__builtin_return_address(0))
+#  define CALLER_ADDR1 ((unsigned long)__builtin_return_address(1))
+#  define CALLER_ADDR2 ((unsigned long)__builtin_return_address(2))
+#  define CALLER_ADDR3 ((unsigned long)__builtin_return_address(3))
+#  define CALLER_ADDR4 ((unsigned long)__builtin_return_address(4))
+#  define CALLER_ADDR5 ((unsigned long)__builtin_return_address(5))
+#  define CALLER_ADDR6 ((unsigned long)__builtin_return_address(6))
+# else
+#  define CALLER_ADDR0 ((unsigned long)__builtin_return_address(0))
+#  define CALLER_ADDR1 0UL
+#  define CALLER_ADDR2 0UL
+#  define CALLER_ADDR3 0UL
+#  define CALLER_ADDR4 0UL
+#  define CALLER_ADDR5 0UL
+#  define CALLER_ADDR6 0UL
+# endif
+#endif /* ifndef HAVE_ARCH_CALLER_ADDR */
 
 #ifdef CONFIG_IRQSOFF_TRACER
   extern void time_hardirqs_on(unsigned long a0, unsigned long a1);

commit c478f8786973d6d7552c652ddad3f6fd86b5af28
Merge: 843adf2379c1 4377245aa93b
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Feb 22 18:12:01 2009 +0100

    Merge branch 'tip/x86/ftrace' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-2.6-trace into tracing/ftrace
    
    Conflicts:
            include/linux/ftrace.h
            kernel/trace/ftrace.c

commit 000ab691172db3921efa3cb7f17fc79235a1de7f
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Feb 17 13:35:06 2009 -0500

    ftrace: allow archs to preform pre and post process for code modification
    
    This patch creates the weak functions: ftrace_arch_code_modify_prepare
    and ftrace_arch_code_modify_post_process that are called before and
    after the stop machine is called to modify the kernel text.
    
    If the arch needs to do pre or post processing, it only needs to define
    these functions.
    
    [ Update: Ingo Molnar suggested using the name ftrace_arch_code_modify_*
              over using ftrace_arch_modify_* ]
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 677432b9cb7e..fdb2a89ae543 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -99,6 +99,9 @@ stack_trace_sysctl(struct ctl_table *table, int write,
 /* asm/ftrace.h must be defined for archs supporting dynamic ftrace */
 #include <asm/ftrace.h>
 
+int ftrace_arch_code_modify_prepare(void);
+int ftrace_arch_code_modify_post_process(void);
+
 enum {
 	FTRACE_FL_FREE		= (1 << 0),
 	FTRACE_FL_FAILED	= (1 << 1),

commit 4cd0332db7e8f57cc082bab11d82c064a9721737
Merge: 40999096e8b9 712406a6bf59
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Feb 19 12:13:33 2009 +0100

    Merge branch 'mainline/function-graph' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-2.6-trace into tracing/function-graph-tracer

commit 712406a6bf59ebf4a00358bb59a4a2a1b2953d90
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon Feb 9 10:54:03 2009 -0800

    tracing/function-graph-tracer: make arch generic push pop functions
    
    There is nothing really arch specific of the push and pop functions
    used by the function graph tracer. This patch moves them to generic
    code.
    
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 677432b9cb7e..a7f8134c594e 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -379,6 +379,30 @@ struct ftrace_graph_ret {
 
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
 
+/*
+ * Stack of return addresses for functions
+ * of a thread.
+ * Used in struct thread_info
+ */
+struct ftrace_ret_stack {
+	unsigned long ret;
+	unsigned long func;
+	unsigned long long calltime;
+};
+
+/*
+ * Primary handler of a function return.
+ * It relays on ftrace_return_to_handler.
+ * Defined in entry_32/64.S
+ */
+extern void return_to_handler(void);
+
+extern int
+ftrace_push_return_trace(unsigned long ret, unsigned long long time,
+			 unsigned long func, int *depth);
+extern void
+ftrace_pop_return_trace(struct ftrace_graph_ret *trace, unsigned long *ret);
+
 /*
  * Sometimes we don't want to trace a function with the function
  * graph tracer but we want them to keep traced by the usual function

commit b6887d7916e44c1d8913084fb6aa5004d9473f1a
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Feb 17 12:32:04 2009 -0500

    ftrace: rename _hook to _probe
    
    Impact: clean up
    
    Ingo Molnar did not like the _hook naming convention used by the
    select function tracer. Luis Claudio R. Goncalves suggested using
    the "_probe" extension. This patch implements the change of
    calling the functions and variables "_hook" and replacing them
    with "_probe".
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 63281228ce3e..9d224c43e634 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -108,7 +108,7 @@ struct ftrace_func_command {
 
 struct seq_file;
 
-struct ftrace_hook_ops {
+struct ftrace_probe_ops {
 	void			(*func)(unsigned long ip,
 					unsigned long parent_ip,
 					void **data);
@@ -116,19 +116,19 @@ struct ftrace_hook_ops {
 	void			(*free)(void **data);
 	int			(*print)(struct seq_file *m,
 					 unsigned long ip,
-					 struct ftrace_hook_ops *ops,
+					 struct ftrace_probe_ops *ops,
 					 void *data);
 };
 
 extern int
-register_ftrace_function_hook(char *glob, struct ftrace_hook_ops *ops,
+register_ftrace_function_probe(char *glob, struct ftrace_probe_ops *ops,
 			      void *data);
 extern void
-unregister_ftrace_function_hook(char *glob, struct ftrace_hook_ops *ops,
+unregister_ftrace_function_probe(char *glob, struct ftrace_probe_ops *ops,
 				void *data);
 extern void
-unregister_ftrace_function_hook_func(char *glob, struct ftrace_hook_ops *ops);
-extern void unregister_ftrace_function_hook_all(char *glob);
+unregister_ftrace_function_probe_func(char *glob, struct ftrace_probe_ops *ops);
+extern void unregister_ftrace_function_probe_all(char *glob);
 
 enum {
 	FTRACE_FL_FREE		= (1 << 0),

commit 97d0bb8dcd8c2812e1927cdb51d7b1f9c98352b5
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Feb 17 11:47:39 2009 +0100

    ftrace: fix !CONFIG_FTRACE [un_]register_ftrace_command() prototypes
    
    Impact: build fix
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index b331e216d8a1..63281228ce3e 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -238,9 +238,11 @@ extern void ftrace_enable_daemon(void);
 static inline void ftrace_release(void *start, unsigned long size) { }
 static inline int register_ftrace_command(struct ftrace_func_command *cmd)
 {
+	return -EINVAL;
 }
 static inline int unregister_ftrace_command(char *cmd_name)
 {
+	return -EINVAL;
 }
 #endif /* CONFIG_DYNAMIC_FTRACE */
 

commit 809dcf29ce4e1723709910878e050bd187617e0e
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon Feb 16 23:06:01 2009 -0500

    ftrace: add pretty print to selected fuction traces
    
    This patch adds a call back for the tracers that have hooks to
    selected functions. This allows the tracer to show better output
    in the set_ftrace_filter file.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 13918c4400ad..b331e216d8a1 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -106,12 +106,18 @@ struct ftrace_func_command {
 /* asm/ftrace.h must be defined for archs supporting dynamic ftrace */
 #include <asm/ftrace.h>
 
+struct seq_file;
+
 struct ftrace_hook_ops {
 	void			(*func)(unsigned long ip,
 					unsigned long parent_ip,
 					void **data);
 	int			(*callback)(unsigned long ip, void **data);
 	void			(*free)(void **data);
+	int			(*print)(struct seq_file *m,
+					 unsigned long ip,
+					 struct ftrace_hook_ops *ops,
+					 void *data);
 };
 
 extern int

commit 59df055f1991c9fc0c71a9230663c39188f6972f
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Sat Feb 14 15:29:06 2009 -0500

    ftrace: trace different functions with a different tracer
    
    Impact: new feature
    
    Currently, the function tracer only gives you an ability to hook
    a tracer to all functions being traced. The dynamic function trace
    allows you to pick and choose which of those functions will be
    traced, but all functions being traced will call all tracers that
    registered with the function tracer.
    
    This patch adds a new feature that allows a tracer to hook to specific
    functions, even when all functions are being traced. It allows for
    different functions to call different tracer hooks.
    
    The way this is accomplished is by a special function that will hook
    to the function tracer and will set up a hash table knowing which
    tracer hook to call with which function. This is the most general
    and easiest method to accomplish this. Later, an arch may choose
    to supply their own method in changing the mcount call of a function
    to call a different tracer. But that will be an exercise for the
    future.
    
    To register a function:
    
     struct ftrace_hook_ops {
            void                    (*func)(unsigned long ip,
                                            unsigned long parent_ip,
                                            void **data);
            int                     (*callback)(unsigned long ip, void **data);
            void                    (*free)(void **data);
     };
    
     int register_ftrace_function_hook(char *glob, struct ftrace_hook_ops *ops,
                                      void *data);
    
    glob is a simple glob to search for the functions to hook.
    ops is a pointer to the operations (listed below)
    data is the default data to be passed to the hook functions when traced
    
    ops:
     func is the hook function to call when the functions are traced
     callback is a callback function that is called when setting up the hash.
       That is, if the tracer needs to do something special for each
       function, that is being traced, and wants to give each function
       its own data. The address of the entry data is passed to this
       callback, so that the callback may wish to update the entry to
       whatever it would like.
     free is a callback for when the entry is freed. In case the tracer
       allocated any data, it is give the chance to free it.
    
    To unregister we have three functions:
    
      void
      unregister_ftrace_function_hook(char *glob, struct ftrace_hook_ops *ops,
                                    void *data)
    
    This will unregister all hooks that match glob, point to ops, and
    have its data matching data. (note, if glob is NULL, blank or '*',
    all functions will be tested).
    
      void
      unregister_ftrace_function_hook_func(char *glob,
                                     struct ftrace_hook_ops *ops)
    
    This will unregister all functions matching glob that has an entry
    pointing to ops.
    
      void unregister_ftrace_function_hook_all(char *glob)
    
    This simply unregisters all funcs.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index f0a0ecc63b5c..13918c4400ad 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -106,6 +106,24 @@ struct ftrace_func_command {
 /* asm/ftrace.h must be defined for archs supporting dynamic ftrace */
 #include <asm/ftrace.h>
 
+struct ftrace_hook_ops {
+	void			(*func)(unsigned long ip,
+					unsigned long parent_ip,
+					void **data);
+	int			(*callback)(unsigned long ip, void **data);
+	void			(*free)(void **data);
+};
+
+extern int
+register_ftrace_function_hook(char *glob, struct ftrace_hook_ops *ops,
+			      void *data);
+extern void
+unregister_ftrace_function_hook(char *glob, struct ftrace_hook_ops *ops,
+				void *data);
+extern void
+unregister_ftrace_function_hook_func(char *glob, struct ftrace_hook_ops *ops);
+extern void unregister_ftrace_function_hook_all(char *glob);
+
 enum {
 	FTRACE_FL_FREE		= (1 << 0),
 	FTRACE_FL_FAILED	= (1 << 1),

commit f6180773d90595650e11de0118bb112018290915
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Sat Feb 14 00:40:25 2009 -0500

    ftrace: add command interface for function selection
    
    Allow for other tracers to add their own commands for function
    selection. This interface gives a trace the ability to name a
    command for function selection. Right now it is pretty limited
    in what it offers, but this is a building step for more features.
    
    The :mod: command is converted to this interface and also serves
    as a template for other implementations.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 106b7909d500..f0a0ecc63b5c 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -95,6 +95,13 @@ stack_trace_sysctl(struct ctl_table *table, int write,
 		   loff_t *ppos);
 #endif
 
+struct ftrace_func_command {
+	struct list_head	list;
+	char			*name;
+	int			(*func)(char *func, char *cmd,
+					char *params, int enable);
+};
+
 #ifdef CONFIG_DYNAMIC_FTRACE
 /* asm/ftrace.h must be defined for archs supporting dynamic ftrace */
 #include <asm/ftrace.h>
@@ -119,6 +126,9 @@ struct dyn_ftrace {
 int ftrace_force_update(void);
 void ftrace_set_filter(unsigned char *buf, int len, int reset);
 
+int register_ftrace_command(struct ftrace_func_command *cmd);
+int unregister_ftrace_command(struct ftrace_func_command *cmd);
+
 /* defined in arch */
 extern int ftrace_ip_converted(unsigned long ip);
 extern int ftrace_dyn_arch_init(void *data);
@@ -202,6 +212,12 @@ extern void ftrace_enable_daemon(void);
 # define ftrace_disable_daemon()		do { } while (0)
 # define ftrace_enable_daemon()			do { } while (0)
 static inline void ftrace_release(void *start, unsigned long size) { }
+static inline int register_ftrace_command(struct ftrace_func_command *cmd)
+{
+}
+static inline int unregister_ftrace_command(char *cmd_name)
+{
+}
 #endif /* CONFIG_DYNAMIC_FTRACE */
 
 /* totally disable ftrace - can not re-enable after this */

commit 1292211058aaf872eeb2a0e2677d237916b4501f
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Feb 7 22:16:12 2009 +0100

    tracing/power: move the power trace headers to a dedicated file
    
    Impact: cleanup
    
    Move the power tracer headers to trace/power.h to keep ftrace.h and power bits
    more easy to maintain as separated topics.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Arjan van de Ven <arjan@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 5e302d636fc2..106b7909d500 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -339,36 +339,6 @@ ftrace_init_module(struct module *mod,
 		   unsigned long *start, unsigned long *end) { }
 #endif
 
-enum {
-	POWER_NONE = 0,
-	POWER_CSTATE = 1,
-	POWER_PSTATE = 2,
-};
-
-struct power_trace {
-#ifdef CONFIG_POWER_TRACER
-	ktime_t			stamp;
-	ktime_t			end;
-	int			type;
-	int			state;
-#endif
-};
-
-#ifdef CONFIG_POWER_TRACER
-extern void trace_power_start(struct power_trace *it, unsigned int type,
-					unsigned int state);
-extern void trace_power_mark(struct power_trace *it, unsigned int type,
-					unsigned int state);
-extern void trace_power_end(struct power_trace *it);
-#else
-static inline void trace_power_start(struct power_trace *it, unsigned int type,
-					unsigned int state) { }
-static inline void trace_power_mark(struct power_trace *it, unsigned int type,
-					unsigned int state) { }
-static inline void trace_power_end(struct power_trace *it) { }
-#endif
-
-
 /*
  * Structure that defines an entry function trace.
  */

commit 57794a9d48b63e34acbe63282628c9f029603308
Author: Wenji Huang <wenji.huang@oracle.com>
Date:   Fri Feb 6 17:33:27 2009 +0800

    trace: trivial fixes in comment typos.
    
    Impact: clean up
    
    Fixed several typos in the comments.
    
    Signed-off-by: Wenji Huang <wenji.huang@oracle.com>
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 7840e718c6c7..5e302d636fc2 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -140,7 +140,7 @@ static inline int ftrace_disable_ftrace_graph_caller(void) { return 0; }
 #endif
 
 /**
- * ftrace_make_nop - convert code into top
+ * ftrace_make_nop - convert code into nop
  * @mod: module structure if called by module load initialization
  * @rec: the mcount call site record
  * @addr: the address that the call site should be calling

commit 9011262a37cb438f0fa9394b5e83840db8f9680a
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Fri Jan 23 12:06:23 2009 -0200

    ftrace: add ftrace_vprintk
    
    Impact: new helper function
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 9f7880d87c39..7840e718c6c7 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -302,6 +302,9 @@ ftrace_special(unsigned long arg1, unsigned long arg2, unsigned long arg3);
 extern int
 __ftrace_printk(unsigned long ip, const char *fmt, ...)
 	__attribute__ ((format (printf, 2, 3)));
+# define ftrace_vprintk(fmt, ap) __ftrace_printk(_THIS_IP_, fmt, ap)
+extern int
+__ftrace_vprintk(unsigned long ip, const char *fmt, va_list ap);
 extern void ftrace_dump(void);
 #else
 static inline void
@@ -317,6 +320,11 @@ ftrace_printk(const char *fmt, ...)
 {
 	return 0;
 }
+static inline int
+ftrace_vprintk(const char *fmt, va_list ap)
+{
+	return 0;
+}
 static inline void ftrace_dump(void) { }
 #endif
 

commit b1818748b0cf9427e48acf9713295e829a0d715f
Author: Markus Metzger <markus.t.metzger@intel.com>
Date:   Mon Jan 19 10:31:01 2009 +0100

    x86, ftrace, hw-branch-tracer: dump trace on oops
    
    Dump the branch trace on an oops (based on ftrace_dump_on_oops).
    
    Signed-off-by: Markus Metzger <markus.t.metzger@intel.com>
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 054721487574..9f7880d87c39 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -496,4 +496,17 @@ static inline int test_tsk_trace_graph(struct task_struct *tsk)
 
 #endif /* CONFIG_TRACING */
 
+
+#ifdef CONFIG_HW_BRANCH_TRACER
+
+void trace_hw_branch(u64 from, u64 to);
+void trace_hw_branch_oops(void);
+
+#else /* CONFIG_HW_BRANCH_TRACER */
+
+static inline void trace_hw_branch(u64 from, u64 to) {}
+static inline void trace_hw_branch_oops(void) {}
+
+#endif /* CONFIG_HW_BRANCH_TRACER */
+
 #endif /* _LINUX_FTRACE_H */

commit f00012074b1a1a67d9c8603617bbbab267347ca6
Author: Shaohua Li <shaohua.li@intel.com>
Date:   Fri Jan 9 11:29:42 2009 +0800

    ftrace, ia64: Add macro for ftrace_caller
    
    Define FTRACE_ADDR. In IA64, a function pointer isn't a 'unsigned long' but a
    'struct {unsigned long ip, unsigned long gp}'.
    
    Signed-off-by: Shaohua Li <shaohua.li@intel.com>
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 677432b9cb7e..054721487574 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -126,6 +126,10 @@ extern int ftrace_update_ftrace_func(ftrace_func_t func);
 extern void ftrace_caller(void);
 extern void ftrace_call(void);
 extern void mcount_call(void);
+
+#ifndef FTRACE_ADDR
+#define FTRACE_ADDR ((unsigned long)ftrace_caller)
+#endif
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
 extern void ftrace_graph_caller(void);
 extern int ftrace_enable_ftrace_graph_caller(void);

commit 3ddeb912f41801fd1968c7880d031702a396e4d0
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Sat Dec 20 17:15:14 2008 +0800

    ftrace: enable format arguments checking
    
    Impact: broaden gcc printf format checks for ftrace_printk()
    
    format arguments checking for ftrace_printk() is __printf(1, 2),
    not __printf(1, 0).
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 04b52e6ebc66..677432b9cb7e 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -303,7 +303,7 @@ extern void ftrace_dump(void);
 static inline void
 ftrace_special(unsigned long arg1, unsigned long arg2, unsigned long arg3) { }
 static inline int
-ftrace_printk(const char *fmt, ...) __attribute__ ((format (printf, 1, 0)));
+ftrace_printk(const char *fmt, ...) __attribute__ ((format (printf, 1, 2)));
 
 static inline void tracing_start(void) { }
 static inline void tracing_stop(void) { }

commit 30cd324e9787ccc9a5ede59742d5409857550692
Merge: c71dd42db2c6 6d102bc68f3d 3d9101e92529
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Dec 19 09:42:40 2008 +0100

    Merge branches 'tracing/ftrace', 'tracing/ring-buffer' and 'tracing/urgent' into tracing/core
    
    Conflicts:
            include/linux/ftrace.h

commit f38f1d2aa5a3520cf05da7cd6bd12fe2b0c509b7
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Dec 16 23:06:40 2008 -0500

    trace: add a way to enable or disable the stack tracer
    
    Impact: enhancement to stack tracer
    
    The stack tracer currently is either on when configured in or
    off when it is not. It can not be disabled when it is configured on.
    (besides disabling the function tracer that it uses)
    
    This patch adds a way to enable or disable the stack tracer at
    run time. It defaults off on bootup, but a kernel parameter 'stacktrace'
    has been added to enable it on bootup.
    
    A new sysctl has been added "kernel.stack_tracer_enabled" to let
    the user enable or disable the stack tracer at run time.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 44020f31bd81..6b0db53caa7d 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -86,6 +86,14 @@ static inline void ftrace_stop(void) { }
 static inline void ftrace_start(void) { }
 #endif /* CONFIG_FUNCTION_TRACER */
 
+#ifdef CONFIG_STACK_TRACER
+extern int stack_tracer_enabled;
+int
+stack_trace_sysctl(struct ctl_table *table, int write,
+		   struct file *file, void __user *buffer, size_t *lenp,
+		   loff_t *ppos);
+#endif
+
 #ifdef CONFIG_DYNAMIC_FTRACE
 /* asm/ftrace.h must be defined for archs supporting dynamic ftrace */
 #include <asm/ftrace.h>

commit bcbc4f20b52c2c40c43a4d2337707dcdfe81bc3a
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Dec 9 23:54:20 2008 +0100

    tracing/function-graph-tracer: annotate do_IRQ and smp_apic_timer_interrupt
    
    Impact: move most important x86 irq entry-points to a separate subsection
    
    Annotate do_IRQ and smp_apic_timer_interrupt to put them into the .irqentry.text
    subsection. These function will so be recognized as hardirq entrypoints for the
    function-graph-tracer. We could also annotate other irq entries but the others
    are far less important but they can be added on request.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 11cac81eed08..44020f31bd81 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -377,6 +377,16 @@ struct ftrace_graph_ret {
  */
 #define __notrace_funcgraph		notrace
 
+/*
+ * We want to which function is an entrypoint of a hardirq.
+ * That will help us to put a signal on output.
+ */
+#define __irq_entry		 __attribute__((__section__(".irqentry.text")))
+
+/* Limits of hardirq entrypoints */
+extern char __irqentry_text_start[];
+extern char __irqentry_text_end[];
+
 #define FTRACE_RETFUNC_DEPTH 50
 #define FTRACE_RETSTACK_ALLOC_SIZE 32
 /* Type of the callback handlers for tracing function graph*/
@@ -414,6 +424,7 @@ static inline void unpause_graph_tracing(void)
 #else
 
 #define __notrace_funcgraph
+#define __irq_entry
 
 static inline void ftrace_graph_init_task(struct task_struct *t) { }
 static inline void ftrace_graph_exit_task(struct task_struct *t) { }

commit 9c24624727f6d6c460e45762a408ca5f5b9b8ef2
Author: Hugh Dickins <hugh@veritas.com>
Date:   Tue Dec 9 13:14:27 2008 -0800

    KSYM_SYMBOL_LEN fixes
    
    Miles Lane tailing /sys files hit a BUG which Pekka Enberg has tracked
    to my 966c8c12dc9e77f931e2281ba25d2f0244b06949 sprint_symbol(): use
    less stack exposing a bug in slub's list_locations() -
    kallsyms_lookup() writes a 0 to namebuf[KSYM_NAME_LEN-1], but that was
    beyond the end of page provided.
    
    The 100 slop which list_locations() allows at end of page looks roughly
    enough for all the other stuff it might print after the symbol before
    it checks again: break out KSYM_SYMBOL_LEN earlier than before.
    
    Latencytop and ftrace and are using KSYM_NAME_LEN buffers where they
    need KSYM_SYMBOL_LEN buffers, and vmallocinfo a 2*KSYM_NAME_LEN buffer
    where it wants a KSYM_SYMBOL_LEN buffer: fix those before anyone copies
    them.
    
    [akpm@linux-foundation.org: ftrace.h needs module.h]
    Signed-off-by: Hugh Dickins <hugh@veritas.com>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc Miles Lane <miles.lane@gmail.com>
    Acked-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Acked-by: Steven Rostedt <srostedt@redhat.com>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 703eb53cfa2b..9c5bc6be2b09 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -6,6 +6,7 @@
 #include <linux/ktime.h>
 #include <linux/init.h>
 #include <linux/types.h>
+#include <linux/module.h>
 #include <linux/kallsyms.h>
 
 #ifdef CONFIG_FUNCTION_TRACER
@@ -231,7 +232,7 @@ ftrace_init_module(unsigned long *start, unsigned long *end) { }
 
 struct boot_trace {
 	pid_t			caller;
-	char			func[KSYM_NAME_LEN];
+	char			func[KSYM_SYMBOL_LEN];
 	int			result;
 	unsigned long long	duration;		/* usecs */
 	ktime_t			calltime;

commit 380c4b1411ccd6885f92b2c8ceb08433a720f44e
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Dec 6 03:43:41 2008 +0100

    tracing/function-graph-tracer: append the tracing_graph_flag
    
    Impact: Provide a way to pause the function graph tracer
    
    As suggested by Steven Rostedt, the previous patch that prevented from
    spinlock function tracing shouldn't use the raw_spinlock to fix it.
    It's much better to follow lockdep with normal spinlock, so this patch
    adds a new flag for each task to make the function graph tracer able
    to be paused. We also can send an ftrace_printk whithout worrying of
    the irrelevant traced spinlock during insertion.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 449fa8e9e34f..11cac81eed08 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -401,6 +401,16 @@ static inline int task_curr_ret_stack(struct task_struct *t)
 {
 	return t->curr_ret_stack;
 }
+
+static inline void pause_graph_tracing(void)
+{
+	atomic_inc(&current->tracing_graph_pause);
+}
+
+static inline void unpause_graph_tracing(void)
+{
+	atomic_dec(&current->tracing_graph_pause);
+}
 #else
 
 #define __notrace_funcgraph
@@ -412,6 +422,9 @@ static inline int task_curr_ret_stack(struct task_struct *tsk)
 {
 	return -1;
 }
+
+static inline void pause_graph_tracing(void) { }
+static inline void unpause_graph_tracing(void) { }
 #endif
 
 #ifdef CONFIG_TRACING

commit 8b96f0119818964e4944fd1c423bf6770027d3ac
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Dec 6 03:40:00 2008 +0100

    tracing/function-graph-tracer: introduce __notrace_funcgraph to filter special functions
    
    Impact: trace more functions
    
    When the function graph tracer is configured, three more files are not
    traced to prevent only four functions to be traced. And this impacts the
    normal function tracer too.
    
    arch/x86/kernel/process_64/32.c:
    
    I had crashes when I let this file traced. After some debugging, I saw
    that the "current" task point was changed inside__swtich_to(), ie:
    "write_pda(pcurrent, next_p);" inside process_64.c Since the tracer store
    the original return address of the function inside current, we had
    crashes. Only __switch_to() has to be excluded from tracing.
    
    kernel/module.c and kernel/extable.c:
    
    Because of a function used internally by the function graph tracer:
    __kernel_text_address()
    
    To let the other functions inside these files to be traced, this patch
    introduces the __notrace_funcgraph function prefix which is __notrace if
    function graph tracer is configured and nothing if not.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index b9b4d0a22d10..449fa8e9e34f 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -369,6 +369,14 @@ struct ftrace_graph_ret {
 };
 
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
+
+/*
+ * Sometimes we don't want to trace a function with the function
+ * graph tracer but we want them to keep traced by the usual function
+ * tracer if the function graph tracer is not configured.
+ */
+#define __notrace_funcgraph		notrace
+
 #define FTRACE_RETFUNC_DEPTH 50
 #define FTRACE_RETSTACK_ALLOC_SIZE 32
 /* Type of the callback handlers for tracing function graph*/
@@ -394,6 +402,9 @@ static inline int task_curr_ret_stack(struct task_struct *t)
 	return t->curr_ret_stack;
 }
 #else
+
+#define __notrace_funcgraph
+
 static inline void ftrace_graph_init_task(struct task_struct *t) { }
 static inline void ftrace_graph_exit_task(struct task_struct *t) { }
 

commit 21a8c466f99063eeb8567318b4e305eda9015408
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Dec 4 23:51:23 2008 +0100

    tracing/ftrace: provide the macro task_curr_ret_stack()
    
    Impact: cleanup
    
    As suggested by Steven Rostedt, this patch provide a new macro
    task_curr_ret_stack() to move the cpp conditionnal CONFIG into
    the linux/ftrace.h headers.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index b295d3106bfe..b9b4d0a22d10 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -8,6 +8,7 @@
 #include <linux/types.h>
 #include <linux/kallsyms.h>
 #include <linux/bitops.h>
+#include <linux/sched.h>
 
 #ifdef CONFIG_FUNCTION_TRACER
 
@@ -387,9 +388,19 @@ extern void unregister_ftrace_graph(void);
 
 extern void ftrace_graph_init_task(struct task_struct *t);
 extern void ftrace_graph_exit_task(struct task_struct *t);
+
+static inline int task_curr_ret_stack(struct task_struct *t)
+{
+	return t->curr_ret_stack;
+}
 #else
 static inline void ftrace_graph_init_task(struct task_struct *t) { }
 static inline void ftrace_graph_exit_task(struct task_struct *t) { }
+
+static inline int task_curr_ret_stack(struct task_struct *tsk)
+{
+	return -1;
+}
 #endif
 
 #ifdef CONFIG_TRACING

commit ea4e2bc4d9f7370e57a343ccb5e7c0ad3222ec3c
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed Dec 3 15:36:57 2008 -0500

    ftrace: graph of a single function
    
    This patch adds the file:
    
       /debugfs/tracing/set_graph_function
    
    which can be used along with the function graph tracer.
    
    When this file is empty, the function graph tracer will act as
    usual. When the file has a function in it, the function graph
    tracer will only trace that function.
    
    For example:
    
     # echo blk_unplug > /debugfs/tracing/set_graph_function
     # cat /debugfs/tracing/trace
     [...]
     ------------------------------------------
     | 2)  make-19003  =>  kjournald-2219
     ------------------------------------------
    
     2)               |  blk_unplug() {
     2)               |    dm_unplug_all() {
     2)               |      dm_get_table() {
     2)      1.381 us |        _read_lock();
     2)      0.911 us |        dm_table_get();
     2)      1. 76 us |        _read_unlock();
     2) +   12.912 us |      }
     2)               |      dm_table_unplug_all() {
     2)               |        blk_unplug() {
     2)      0.778 us |          generic_unplug_device();
     2)      2.409 us |        }
     2)      5.992 us |      }
     2)      0.813 us |      dm_table_put();
     2) +   29. 90 us |    }
     2) +   34.532 us |  }
    
    You can add up to 32 functions into this file. Currently we limit it
    to 32, but this may change with later improvements.
    
    To add another function, use the append '>>':
    
      # echo sys_read >> /debugfs/tracing/set_graph_function
      # cat /debugfs/tracing/set_graph_function
      blk_unplug
      sys_read
    
    Using the '>' will clear out the function and write anew:
    
      # echo sys_write > /debug/tracing/set_graph_function
      # cat /debug/tracing/set_graph_function
      sys_write
    
    Note, if you have function graph running while doing this, the small
    time between clearing it and updating it will cause the graph to
    record all functions. This should not be an issue because after
    it sets the filter, only those functions will be recorded from then on.
    If you need to only record a particular function then set this
    file first before starting the function graph tracer. In the future
    this side effect may be corrected.
    
    The set_graph_function file is similar to the set_ftrace_filter but
    it does not take wild cards nor does it allow for more than one
    function to be set with a single write. There is no technical reason why
    this is the case, I just do not have the time yet to implement that.
    
    Note, dynamic ftrace must be enabled for this to appear because it
    uses the dynamic ftrace records to match the name to the mcount
    call sites.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 469ceb3e85ba..b295d3106bfe 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -7,6 +7,7 @@
 #include <linux/init.h>
 #include <linux/types.h>
 #include <linux/kallsyms.h>
+#include <linux/bitops.h>
 
 #ifdef CONFIG_FUNCTION_TRACER
 
@@ -391,4 +392,49 @@ static inline void ftrace_graph_init_task(struct task_struct *t) { }
 static inline void ftrace_graph_exit_task(struct task_struct *t) { }
 #endif
 
+#ifdef CONFIG_TRACING
+#include <linux/sched.h>
+
+/* flags for current->trace */
+enum {
+	TSK_TRACE_FL_TRACE_BIT	= 0,
+	TSK_TRACE_FL_GRAPH_BIT	= 1,
+};
+enum {
+	TSK_TRACE_FL_TRACE	= 1 << TSK_TRACE_FL_TRACE_BIT,
+	TSK_TRACE_FL_GRAPH	= 1 << TSK_TRACE_FL_GRAPH_BIT,
+};
+
+static inline void set_tsk_trace_trace(struct task_struct *tsk)
+{
+	set_bit(TSK_TRACE_FL_TRACE_BIT, &tsk->trace);
+}
+
+static inline void clear_tsk_trace_trace(struct task_struct *tsk)
+{
+	clear_bit(TSK_TRACE_FL_TRACE_BIT, &tsk->trace);
+}
+
+static inline int test_tsk_trace_trace(struct task_struct *tsk)
+{
+	return tsk->trace & TSK_TRACE_FL_TRACE;
+}
+
+static inline void set_tsk_trace_graph(struct task_struct *tsk)
+{
+	set_bit(TSK_TRACE_FL_GRAPH_BIT, &tsk->trace);
+}
+
+static inline void clear_tsk_trace_graph(struct task_struct *tsk)
+{
+	clear_bit(TSK_TRACE_FL_GRAPH_BIT, &tsk->trace);
+}
+
+static inline int test_tsk_trace_graph(struct task_struct *tsk)
+{
+	return tsk->trace & TSK_TRACE_FL_GRAPH;
+}
+
+#endif /* CONFIG_TRACING */
+
 #endif /* _LINUX_FTRACE_H */

commit e49dc19c6a19ea112fcb94b7c62ec62cdd5c08aa
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Dec 2 23:50:05 2008 -0500

    ftrace: function graph return for function entry
    
    Impact: feature, let entry function decide to trace or not
    
    This patch lets the graph tracer entry function decide if the tracing
    should be done at the end as well. This requires all function graph
    entry functions return 1 if it should trace, or 0 if the return should
    not be traced.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 58ca1c3a3f4d..469ceb3e85ba 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -371,7 +371,7 @@ struct ftrace_graph_ret {
 #define FTRACE_RETSTACK_ALLOC_SIZE 32
 /* Type of the callback handlers for tracing function graph*/
 typedef void (*trace_func_graph_ret_t)(struct ftrace_graph_ret *); /* return */
-typedef void (*trace_func_graph_ent_t)(struct ftrace_graph_ent *); /* entry */
+typedef int (*trace_func_graph_ent_t)(struct ftrace_graph_ent *); /* entry */
 
 extern int register_ftrace_graph(trace_func_graph_ret_t retfunc,
 				trace_func_graph_ent_t entryfunc);

commit 14a866c567e040ccf6240d68b083dd1dbbde63e6
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Dec 2 23:50:02 2008 -0500

    ftrace: add ftrace_graph_stop()
    
    Impact: new ftrace_graph_stop function
    
    While developing more features of function graph, I hit a bug that
    caused the WARN_ON to trigger in the prepare_ftrace_return function.
    Well, it was hard for me to find out that was happening because the
    bug would not print, it would just cause a hard lockup or reboot.
    The reason is that it is not safe to call printk from this function.
    
    Looking further, I also found that it calls unregister_ftrace_graph,
    which grabs a mutex and calls kstop machine. This would definitely
    lock the box up if it were to trigger.
    
    This patch adds a fast and safe ftrace_graph_stop() which will
    stop the function tracer. Then it is safe to call the WARN ON.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index afba918c623c..58ca1c3a3f4d 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -376,6 +376,8 @@ typedef void (*trace_func_graph_ent_t)(struct ftrace_graph_ent *); /* entry */
 extern int register_ftrace_graph(trace_func_graph_ret_t retfunc,
 				trace_func_graph_ent_t entryfunc);
 
+extern void ftrace_graph_stop(void);
+
 /* The current handlers in use */
 extern trace_func_graph_ret_t ftrace_graph_return;
 extern trace_func_graph_ent_t ftrace_graph_entry;

commit c7cc77307669336a08928ab8668bdb3f3bcc021b
Merge: 0bfc24559d79 d144d5ee6a26 437f24fb897d f3f47a6768a2
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Nov 27 10:56:13 2008 +0100

    Merge branches 'tracing/blktrace', 'tracing/ftrace', 'tracing/function-graph-tracer' and 'tracing/power-tracer' into tracing/core

commit f3f47a6768a29448866da4422b6f6bee485c947f
Author: Arjan van de Ven <arjan@infradead.org>
Date:   Sun Nov 23 16:49:58 2008 -0800

    tracing: add "power-tracer": C/P state tracer to help power optimization
    
    Impact: new "power-tracer" ftrace plugin
    
    This patch adds a C/P-state ftrace plugin that will generate
    detailed statistics about the C/P-states that are being used,
    so that we can look at detailed decisions that the C/P-state
    code is making, rather than the too high level "average"
    that we have today.
    
    An example way of using this is:
    
     mount -t debugfs none /sys/kernel/debug
     echo cstate > /sys/kernel/debug/tracing/current_tracer
     echo 1 > /sys/kernel/debug/tracing/tracing_enabled
     sleep 1
     echo 0 > /sys/kernel/debug/tracing/tracing_enabled
     cat /sys/kernel/debug/tracing/trace | perl scripts/trace/cstate.pl > out.svg
    
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 7854d87b97b2..0df288666201 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -311,6 +311,35 @@ ftrace_init_module(struct module *mod,
 		   unsigned long *start, unsigned long *end) { }
 #endif
 
+enum {
+	POWER_NONE = 0,
+	POWER_CSTATE = 1,
+	POWER_PSTATE = 2,
+};
+
+struct power_trace {
+#ifdef CONFIG_POWER_TRACER
+	ktime_t			stamp;
+	ktime_t			end;
+	int			type;
+	int			state;
+#endif
+};
+
+#ifdef CONFIG_POWER_TRACER
+extern void trace_power_start(struct power_trace *it, unsigned int type,
+					unsigned int state);
+extern void trace_power_mark(struct power_trace *it, unsigned int type,
+					unsigned int state);
+extern void trace_power_end(struct power_trace *it);
+#else
+static inline void trace_power_start(struct power_trace *it, unsigned int type,
+					unsigned int state) { }
+static inline void trace_power_mark(struct power_trace *it, unsigned int type,
+					unsigned int state) { }
+static inline void trace_power_end(struct power_trace *it) { }
+#endif
+
 
 /*
  * Structure that defines a return function trace.

commit 5a45cfe1c64862e8cd3b0d79d7c4ba71c3118915
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Wed Nov 26 00:16:24 2008 -0500

    ftrace: use code patching for ftrace graph tracer
    
    Impact: more efficient code for ftrace graph tracer
    
    This patch uses the dynamic patching, when available, to patch
    the function graph code into the kernel.
    
    This patch will ease the way for letting both function tracing
    and function graph tracing run together.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index fc2d54987198..f9792c0d73f6 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -117,6 +117,11 @@ extern void ftrace_call(void);
 extern void mcount_call(void);
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
 extern void ftrace_graph_caller(void);
+extern int ftrace_enable_ftrace_graph_caller(void);
+extern int ftrace_disable_ftrace_graph_caller(void);
+#else
+static inline int ftrace_enable_ftrace_graph_caller(void) { return 0; }
+static inline int ftrace_disable_ftrace_graph_caller(void) { return 0; }
 #endif
 
 /**

commit 287b6e68ca7209caec40b2f44f837c580a413bae
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Nov 26 00:57:25 2008 +0100

    tracing/function-return-tracer: set a more human readable output
    
    Impact: feature
    
    This patch sets a C-like output for the function graph tracing.
    For this aim, we now call two handler for each function: one on the entry
    and one other on return. This way we can draw a well-ordered call stack.
    
    The pid of the previous trace is loosely stored to be compared against
    the one of the current trace to see if there were a context switch.
    
    Without this little feature, the call tree would seem broken at
    some locations.
    We could use the sched_tracer to capture these sched_events but this
    way of processing is much more simpler.
    
    2 spaces have been chosen for indentation to fit the screen while deep
    calls. The time of execution in nanosecs is printed just after closed
    braces, it seems more easy this way to find the corresponding function.
    If the time was printed as a first column, it would be not so easy to
    find the corresponding function if it is called on a deep depth.
    
    I plan to output the return value but on 32 bits CPU, the return value
    can be 32 or 64, and its difficult to guess on which case we are.
    I don't know what would be the better solution on X86-32: only print
    eax (low-part) or even edx (high-part).
    
    Actually it's thee same problem when a function return a 8 bits value, the
    high part of eax could contain junk values...
    
    Here is an example of trace:
    
    sys_read() {
      fget_light() {
      } 526
      vfs_read() {
        rw_verify_area() {
          security_file_permission() {
            cap_file_permission() {
            } 519
          } 1564
        } 2640
        do_sync_read() {
          pipe_read() {
            __might_sleep() {
            } 511
            pipe_wait() {
              prepare_to_wait() {
              } 760
              deactivate_task() {
                dequeue_task() {
                  dequeue_task_fair() {
                    dequeue_entity() {
                      update_curr() {
                        update_min_vruntime() {
                        } 504
                      } 1587
                      clear_buddies() {
                      } 512
                      add_cfs_task_weight() {
                      } 519
                      update_min_vruntime() {
                      } 511
                    } 5602
                    dequeue_entity() {
                      update_curr() {
                        update_min_vruntime() {
                        } 496
                      } 1631
                      clear_buddies() {
                      } 496
                      update_min_vruntime() {
                      } 527
                    } 4580
                    hrtick_update() {
                      hrtick_start_fair() {
                      } 488
                    } 1489
                  } 13700
                } 14949
              } 16016
              msecs_to_jiffies() {
              } 496
              put_prev_task_fair() {
              } 504
              pick_next_task_fair() {
              } 489
              pick_next_task_rt() {
              } 496
              pick_next_task_fair() {
              } 489
              pick_next_task_idle() {
              } 489
    
    ------------8<---------- thread 4 ------------8<----------
    
    finish_task_switch() {
    } 1203
    do_softirq() {
      __do_softirq() {
        __local_bh_disable() {
        } 669
        rcu_process_callbacks() {
          __rcu_process_callbacks() {
            cpu_quiet() {
              rcu_start_batch() {
              } 503
            } 1647
          } 3128
          __rcu_process_callbacks() {
          } 542
        } 5362
        _local_bh_enable() {
        } 587
      } 8880
    } 9986
    kthread_should_stop() {
    } 669
    deactivate_task() {
      dequeue_task() {
        dequeue_task_fair() {
          dequeue_entity() {
            update_curr() {
              calc_delta_mine() {
              } 511
              update_min_vruntime() {
              } 511
            } 2813
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index b4ac734ad8d6..fc2d54987198 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -312,27 +312,40 @@ ftrace_init_module(struct module *mod,
 #endif
 
 
+/*
+ * Structure that defines an entry function trace.
+ */
+struct ftrace_graph_ent {
+	unsigned long func; /* Current function */
+	int depth;
+};
+
 /*
  * Structure that defines a return function trace.
  */
 struct ftrace_graph_ret {
-	unsigned long ret; /* Return address */
 	unsigned long func; /* Current function */
 	unsigned long long calltime;
 	unsigned long long rettime;
 	/* Number of functions that overran the depth limit for current task */
 	unsigned long overrun;
+	int depth;
 };
 
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
 #define FTRACE_RETFUNC_DEPTH 50
 #define FTRACE_RETSTACK_ALLOC_SIZE 32
-/* Type of a callback handler of tracing return function */
-typedef void (*trace_function_graph_t)(struct ftrace_graph_ret *);
+/* Type of the callback handlers for tracing function graph*/
+typedef void (*trace_func_graph_ret_t)(struct ftrace_graph_ret *); /* return */
+typedef void (*trace_func_graph_ent_t)(struct ftrace_graph_ent *); /* entry */
+
+extern int register_ftrace_graph(trace_func_graph_ret_t retfunc,
+				trace_func_graph_ent_t entryfunc);
+
+/* The current handlers in use */
+extern trace_func_graph_ret_t ftrace_graph_return;
+extern trace_func_graph_ent_t ftrace_graph_entry;
 
-extern int register_ftrace_graph(trace_function_graph_t func);
-/* The current handler in use */
-extern trace_function_graph_t ftrace_graph_function;
 extern void unregister_ftrace_graph(void);
 
 extern void ftrace_graph_init_task(struct task_struct *t);

commit fb52607afcd0629776f1dc9e657647ceae81dd50
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Nov 25 21:07:04 2008 +0100

    tracing/function-return-tracer: change the name into function-graph-tracer
    
    Impact: cleanup
    
    This patch changes the name of the "return function tracer" into
    function-graph-tracer which is a more suitable name for a tracing
    which makes one able to retrieve the ordered call stack during
    the code flow.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 7854d87b97b2..b4ac734ad8d6 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -115,8 +115,8 @@ extern int ftrace_update_ftrace_func(ftrace_func_t func);
 extern void ftrace_caller(void);
 extern void ftrace_call(void);
 extern void mcount_call(void);
-#ifdef CONFIG_FUNCTION_RET_TRACER
-extern void ftrace_return_caller(void);
+#ifdef CONFIG_FUNCTION_GRAPH_TRACER
+extern void ftrace_graph_caller(void);
 #endif
 
 /**
@@ -315,7 +315,7 @@ ftrace_init_module(struct module *mod,
 /*
  * Structure that defines a return function trace.
  */
-struct ftrace_retfunc {
+struct ftrace_graph_ret {
 	unsigned long ret; /* Return address */
 	unsigned long func; /* Current function */
 	unsigned long long calltime;
@@ -324,22 +324,22 @@ struct ftrace_retfunc {
 	unsigned long overrun;
 };
 
-#ifdef CONFIG_FUNCTION_RET_TRACER
+#ifdef CONFIG_FUNCTION_GRAPH_TRACER
 #define FTRACE_RETFUNC_DEPTH 50
 #define FTRACE_RETSTACK_ALLOC_SIZE 32
 /* Type of a callback handler of tracing return function */
-typedef void (*trace_function_return_t)(struct ftrace_retfunc *);
+typedef void (*trace_function_graph_t)(struct ftrace_graph_ret *);
 
-extern int register_ftrace_return(trace_function_return_t func);
+extern int register_ftrace_graph(trace_function_graph_t func);
 /* The current handler in use */
-extern trace_function_return_t ftrace_function_return;
-extern void unregister_ftrace_return(void);
+extern trace_function_graph_t ftrace_graph_function;
+extern void unregister_ftrace_graph(void);
 
-extern void ftrace_retfunc_init_task(struct task_struct *t);
-extern void ftrace_retfunc_exit_task(struct task_struct *t);
+extern void ftrace_graph_init_task(struct task_struct *t);
+extern void ftrace_graph_exit_task(struct task_struct *t);
 #else
-static inline void ftrace_retfunc_init_task(struct task_struct *t) { }
-static inline void ftrace_retfunc_exit_task(struct task_struct *t) { }
+static inline void ftrace_graph_init_task(struct task_struct *t) { }
+static inline void ftrace_graph_exit_task(struct task_struct *t) { }
 #endif
 
 #endif /* _LINUX_FTRACE_H */

commit 6f893fb2e89287a4d755f928c3cda9d18440355c
Merge: 0429149fb5e0 1d926f275639 69bb54ec05f5 65afa5e603d5 cbe2f5a6e84e 813b8520f5c2 033601a32b20 958086d17844 fb91ee6cf5b8
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Nov 24 17:46:24 2008 +0100

    Merge branches 'tracing/branch-tracer', 'tracing/fastboot', 'tracing/ftrace', 'tracing/function-return-tracer', 'tracing/power-tracer', 'tracing/powerpc', 'tracing/ring-buffer', 'tracing/stack-tracer' and 'tracing/urgent' into tracing/core

commit 69bb54ec05f57da7f6fac2cec0820cbc970df20f
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Nov 21 12:59:38 2008 -0500

    ftrace: add ftrace_off_permanent
    
    Impact: add new API to disable all of ftrace on anomalies
    
    It case of a serious anomaly being detected (like something caught by
    lockdep) it is a good idea to disable all tracing immediately, without
    grabing any locks.
    
    This patch adds ftrace_off_permanent that disables the tracers, function
    tracing and ring buffers without a way to enable them again. This should
    only be used when something serious has been detected.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index f7ba4ea5e128..13e9cfc09928 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -257,6 +257,7 @@ extern int ftrace_dump_on_oops;
 
 extern void tracing_start(void);
 extern void tracing_stop(void);
+extern void ftrace_off_permanent(void);
 
 extern void
 ftrace_special(unsigned long arg1, unsigned long arg2, unsigned long arg3);
@@ -290,6 +291,7 @@ ftrace_printk(const char *fmt, ...) __attribute__ ((format (printf, 1, 0)));
 
 static inline void tracing_start(void) { }
 static inline void tracing_stop(void) { }
+static inline void ftrace_off_permanent(void) { }
 static inline int
 ftrace_printk(const char *fmt, ...)
 {

commit 82f60f0bc854aada696f27d863c03bef91f1509d
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Nov 23 09:18:56 2008 +0100

    tracing/function-return-tracer: clean up task start/exit callbacks
    
    Impact: cleanup
    
    Eliminate #ifdefs in core code by using empty inline functions.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 2ba259b2defa..938ca1942641 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -335,6 +335,9 @@ extern void unregister_ftrace_return(void);
 
 extern void ftrace_retfunc_init_task(struct task_struct *t);
 extern void ftrace_retfunc_exit_task(struct task_struct *t);
+#else
+static inline void ftrace_retfunc_init_task(struct task_struct *t) { }
+static inline void ftrace_retfunc_exit_task(struct task_struct *t) { }
 #endif
 
 #endif /* _LINUX_FTRACE_H */

commit f201ae2356c74bcae130b2177b3dca903ea98071
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sun Nov 23 06:22:56 2008 +0100

    tracing/function-return-tracer: store return stack into task_struct and allocate it dynamically
    
    Impact: use deeper function tracing depth safely
    
    Some tests showed that function return tracing needed a more deeper depth
    of function calls. But it could be unsafe to store these return addresses
    to the stack.
    
    So these arrays will now be allocated dynamically into task_struct of current
    only when the tracer is activated.
    
    Typical scheme when tracer is activated:
    - allocate a return stack for each task in global list.
    - fork: allocate the return stack for the newly created task
    - exit: free return stack of current
    - idle init: same as fork
    
    I chose a default depth of 50. I don't have overruns anymore.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index f7ba4ea5e128..2ba259b2defa 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -323,6 +323,8 @@ struct ftrace_retfunc {
 };
 
 #ifdef CONFIG_FUNCTION_RET_TRACER
+#define FTRACE_RETFUNC_DEPTH 50
+#define FTRACE_RETSTACK_ALLOC_SIZE 32
 /* Type of a callback handler of tracing return function */
 typedef void (*trace_function_return_t)(struct ftrace_retfunc *);
 
@@ -330,6 +332,9 @@ extern int register_ftrace_return(trace_function_return_t func);
 /* The current handler in use */
 extern trace_function_return_t ftrace_function_return;
 extern void unregister_ftrace_return(void);
+
+extern void ftrace_retfunc_init_task(struct task_struct *t);
+extern void ftrace_retfunc_exit_task(struct task_struct *t);
 #endif
 
 #endif /* _LINUX_FTRACE_H */

commit 0231022cc32d5f2e7f3c06b75691dda0ad6aec33
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Mon Nov 17 03:22:41 2008 +0100

    tracing/function-return-tracer: add the overrun field
    
    Impact: help to find the better depth of trace
    
    We decided to arbitrary define the depth of function return trace as
    "20". Perhaps this is not enough. To help finding an optimal depth, we
    measure now the overrun: the number of functions that have been missed
    for the current thread. By default this is not displayed, we have to
    do set a particular flag on the return tracer: echo overrun >
    /debug/tracing/trace_options And the overrun will be printed on the
    right.
    
    As the trace shows below, the current 20 depth is not enough.
    
    update_wall_time+0x37f/0x8c0 -> update_xtime_cache (345 ns) (Overruns: 2838)
    update_wall_time+0x384/0x8c0 -> clocksource_get_next (1141 ns) (Overruns: 2838)
    do_timer+0x23/0x100 -> update_wall_time (3882 ns) (Overruns: 2838)
    tick_do_update_jiffies64+0xbf/0x160 -> do_timer (5339 ns) (Overruns: 2838)
    tick_sched_timer+0x6a/0xf0 -> tick_do_update_jiffies64 (7209 ns) (Overruns: 2838)
    vgacon_set_cursor_size+0x98/0x120 -> native_io_delay (2613 ns) (Overruns: 274)
    vgacon_cursor+0x16e/0x1d0 -> vgacon_set_cursor_size (33151 ns) (Overruns: 274)
    set_cursor+0x5f/0x80 -> vgacon_cursor (36432 ns) (Overruns: 274)
    con_flush_chars+0x34/0x40 -> set_cursor (38790 ns) (Overruns: 274)
    release_console_sem+0x1ec/0x230 -> up (721 ns) (Overruns: 274)
    release_console_sem+0x225/0x230 -> wake_up_klogd (316 ns) (Overruns: 274)
    con_flush_chars+0x39/0x40 -> release_console_sem (2996 ns) (Overruns: 274)
    con_write+0x22/0x30 -> con_flush_chars (46067 ns) (Overruns: 274)
    n_tty_write+0x1cc/0x360 -> con_write (292670 ns) (Overruns: 274)
    smp_apic_timer_interrupt+0x2a/0x90 -> native_apic_mem_write (330 ns) (Overruns: 274)
    irq_enter+0x17/0x70 -> idle_cpu (413 ns) (Overruns: 274)
    smp_apic_timer_interrupt+0x2f/0x90 -> irq_enter (1525 ns) (Overruns: 274)
    ktime_get_ts+0x40/0x70 -> getnstimeofday (465 ns) (Overruns: 274)
    ktime_get_ts+0x60/0x70 -> set_normalized_timespec (436 ns) (Overruns: 274)
    ktime_get+0x16/0x30 -> ktime_get_ts (2501 ns) (Overruns: 274)
    hrtimer_interrupt+0x77/0x1a0 -> ktime_get (3439 ns) (Overruns: 274)
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index f1af1aab00e6..f7ba4ea5e128 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -318,6 +318,8 @@ struct ftrace_retfunc {
 	unsigned long func; /* Current function */
 	unsigned long long calltime;
 	unsigned long long rettime;
+	/* Number of functions that overran the depth limit for current task */
+	unsigned long overrun;
 };
 
 #ifdef CONFIG_FUNCTION_RET_TRACER

commit e7d3737ea1b102030f44e96c97754101e41515f0
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sun Nov 16 06:02:06 2008 +0100

    tracing/function-return-tracer: support for dynamic ftrace on function return tracer
    
    This patch adds the support for dynamic tracing on the function return tracer.
    The whole difference with normal dynamic function tracing is that we don't need
    to hook on a particular callback. The only pro that we want is to nop or set
    dynamically the calls to ftrace_caller (which is ftrace_return_caller here).
    
    Some security checks ensure that we are not trying to launch dynamic tracing for
    return tracing while normal function tracing is already running.
    
    An example of trace with getnstimeofday set as a filter:
    
    ktime_get_ts+0x22/0x50 -> getnstimeofday (2283 ns)
    ktime_get_ts+0x22/0x50 -> getnstimeofday (1396 ns)
    ktime_get_ts+0x22/0x50 -> getnstimeofday (1382 ns)
    ktime_get_ts+0x22/0x50 -> getnstimeofday (1825 ns)
    ktime_get_ts+0x22/0x50 -> getnstimeofday (1426 ns)
    ktime_get_ts+0x22/0x50 -> getnstimeofday (1464 ns)
    ktime_get_ts+0x22/0x50 -> getnstimeofday (1524 ns)
    ktime_get_ts+0x22/0x50 -> getnstimeofday (1382 ns)
    ktime_get_ts+0x22/0x50 -> getnstimeofday (1382 ns)
    ktime_get_ts+0x22/0x50 -> getnstimeofday (1434 ns)
    ktime_get_ts+0x22/0x50 -> getnstimeofday (1464 ns)
    ktime_get_ts+0x22/0x50 -> getnstimeofday (1502 ns)
    ktime_get_ts+0x22/0x50 -> getnstimeofday (1404 ns)
    ktime_get_ts+0x22/0x50 -> getnstimeofday (1397 ns)
    ktime_get_ts+0x22/0x50 -> getnstimeofday (1051 ns)
    ktime_get_ts+0x22/0x50 -> getnstimeofday (1314 ns)
    ktime_get_ts+0x22/0x50 -> getnstimeofday (1344 ns)
    ktime_get_ts+0x22/0x50 -> getnstimeofday (1163 ns)
    ktime_get_ts+0x22/0x50 -> getnstimeofday (1390 ns)
    ktime_get_ts+0x22/0x50 -> getnstimeofday (1374 ns)
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 166a2070ef65..f1af1aab00e6 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -25,6 +25,17 @@ struct ftrace_ops {
 
 extern int function_trace_stop;
 
+/*
+ * Type of the current tracing.
+ */
+enum ftrace_tracing_type_t {
+	FTRACE_TYPE_ENTER = 0, /* Hook the call of the function */
+	FTRACE_TYPE_RETURN,	/* Hook the return of the function */
+};
+
+/* Current tracing type, default is FTRACE_TYPE_ENTER */
+extern enum ftrace_tracing_type_t ftrace_tracing_type;
+
 /**
  * ftrace_stop - stop function tracer.
  *
@@ -104,6 +115,9 @@ extern int ftrace_update_ftrace_func(ftrace_func_t func);
 extern void ftrace_caller(void);
 extern void ftrace_call(void);
 extern void mcount_call(void);
+#ifdef CONFIG_FUNCTION_RET_TRACER
+extern void ftrace_return_caller(void);
+#endif
 
 /**
  * ftrace_make_nop - convert code into top
@@ -310,7 +324,7 @@ struct ftrace_retfunc {
 /* Type of a callback handler of tracing return function */
 typedef void (*trace_function_return_t)(struct ftrace_retfunc *);
 
-extern void register_ftrace_return(trace_function_return_t func);
+extern int register_ftrace_return(trace_function_return_t func);
 /* The current handler in use */
 extern trace_function_return_t ftrace_function_return;
 extern void unregister_ftrace_return(void);

commit 31e889098a80ceb3e9e3c555d522b2686a6663c6
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Nov 14 16:21:19 2008 -0800

    ftrace: pass module struct to arch dynamic ftrace functions
    
    Impact: allow archs more flexibility on dynamic ftrace implementations
    
    Dynamic ftrace has largly been developed on x86. Since x86 does not
    have the same limitations as other architectures, the ftrace interaction
    between the generic code and the architecture specific code was not
    flexible enough to handle some of the issues that other architectures
    have.
    
    Most notably, module trampolines. Due to the limited branch distance
    that archs make in calling kernel core code from modules, the module
    load code must create a trampoline to jump to what will make the
    larger jump into core kernel code.
    
    The problem arises when this happens to a call to mcount. Ftrace checks
    all code before modifying it and makes sure the current code is what
    it expects. Right now, there is not enough information to handle modifying
    module trampolines.
    
    This patch changes the API between generic dynamic ftrace code and
    the arch dependent code. There is now two functions for modifying code:
    
      ftrace_make_nop(mod, rec, addr) - convert the code at rec->ip into
           a nop, where the original text is calling addr. (mod is the
           module struct if called by module init)
    
      ftrace_make_caller(rec, addr) - convert the code rec->ip that should
           be a nop into a caller to addr.
    
    The record "rec" now has a new field called "arch" where the architecture
    can add any special attributes to each call site record.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 4fbc4a8b86a5..166a2070ef65 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -74,6 +74,9 @@ static inline void ftrace_start(void) { }
 #endif /* CONFIG_FUNCTION_TRACER */
 
 #ifdef CONFIG_DYNAMIC_FTRACE
+/* asm/ftrace.h must be defined for archs supporting dynamic ftrace */
+#include <asm/ftrace.h>
+
 enum {
 	FTRACE_FL_FREE		= (1 << 0),
 	FTRACE_FL_FAILED	= (1 << 1),
@@ -88,6 +91,7 @@ struct dyn_ftrace {
 	struct list_head	list;
 	unsigned long		ip; /* address of mcount call-site */
 	unsigned long		flags;
+	struct dyn_arch_ftrace	arch;
 };
 
 int ftrace_force_update(void);
@@ -95,22 +99,40 @@ void ftrace_set_filter(unsigned char *buf, int len, int reset);
 
 /* defined in arch */
 extern int ftrace_ip_converted(unsigned long ip);
-extern unsigned char *ftrace_nop_replace(void);
-extern unsigned char *ftrace_call_replace(unsigned long ip, unsigned long addr);
 extern int ftrace_dyn_arch_init(void *data);
 extern int ftrace_update_ftrace_func(ftrace_func_t func);
 extern void ftrace_caller(void);
 extern void ftrace_call(void);
 extern void mcount_call(void);
 
-/* May be defined in arch */
-extern int ftrace_arch_read_dyn_info(char *buf, int size);
+/**
+ * ftrace_make_nop - convert code into top
+ * @mod: module structure if called by module load initialization
+ * @rec: the mcount call site record
+ * @addr: the address that the call site should be calling
+ *
+ * This is a very sensitive operation and great care needs
+ * to be taken by the arch.  The operation should carefully
+ * read the location, check to see if what is read is indeed
+ * what we expect it to be, and then on success of the compare,
+ * it should write to the location.
+ *
+ * The code segment at @rec->ip should be a caller to @addr
+ *
+ * Return must be:
+ *  0 on success
+ *  -EFAULT on error reading the location
+ *  -EINVAL on a failed compare of the contents
+ *  -EPERM  on error writing to the location
+ * Any other value will be considered a failure.
+ */
+extern int ftrace_make_nop(struct module *mod,
+			   struct dyn_ftrace *rec, unsigned long addr);
 
 /**
- * ftrace_modify_code - modify code segment
- * @ip: the address of the code segment
- * @old_code: the contents of what is expected to be there
- * @new_code: the code to patch in
+ * ftrace_make_call - convert a nop call site into a call to addr
+ * @rec: the mcount call site record
+ * @addr: the address that the call site should call
  *
  * This is a very sensitive operation and great care needs
  * to be taken by the arch.  The operation should carefully
@@ -118,6 +140,8 @@ extern int ftrace_arch_read_dyn_info(char *buf, int size);
  * what we expect it to be, and then on success of the compare,
  * it should write to the location.
  *
+ * The code segment at @rec->ip should be a nop
+ *
  * Return must be:
  *  0 on success
  *  -EFAULT on error reading the location
@@ -125,8 +149,11 @@ extern int ftrace_arch_read_dyn_info(char *buf, int size);
  *  -EPERM  on error writing to the location
  * Any other value will be considered a failure.
  */
-extern int ftrace_modify_code(unsigned long ip, unsigned char *old_code,
-			      unsigned char *new_code);
+extern int ftrace_make_call(struct dyn_ftrace *rec, unsigned long addr);
+
+
+/* May be defined in arch */
+extern int ftrace_arch_read_dyn_info(char *buf, int size);
 
 extern int skip_trace(unsigned long ip);
 
@@ -259,11 +286,13 @@ static inline void ftrace_dump(void) { }
 
 #ifdef CONFIG_FTRACE_MCOUNT_RECORD
 extern void ftrace_init(void);
-extern void ftrace_init_module(unsigned long *start, unsigned long *end);
+extern void ftrace_init_module(struct module *mod,
+			       unsigned long *start, unsigned long *end);
 #else
 static inline void ftrace_init(void) { }
 static inline void
-ftrace_init_module(unsigned long *start, unsigned long *end) { }
+ftrace_init_module(struct module *mod,
+		   unsigned long *start, unsigned long *end) { }
 #endif
 
 

commit 3f5ec13696fd4a33bde42f385406cbb1d3cc96fd
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Nov 11 23:21:31 2008 +0100

    tracing/fastboot: move boot tracer structs and funcs into their own header.
    
    Impact: Cleanups on the boot tracer and ftrace
    
    This patch bring some cleanups about the boot tracer headers. The
    functions and structures of this tracer have nothing related to ftrace
    and should have so their own header file.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index dcbbf72a88b1..4fbc4a8b86a5 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -287,45 +287,4 @@ extern trace_function_return_t ftrace_function_return;
 extern void unregister_ftrace_return(void);
 #endif
 
-/*
- * Structure which defines the trace of an initcall.
- * You don't have to fill the func field since it is
- * only used internally by the tracer.
- */
-struct boot_trace {
-	pid_t			caller;
-	char			func[KSYM_NAME_LEN];
-	int			result;
-	unsigned long long	duration;		/* usecs */
-	ktime_t			calltime;
-	ktime_t			rettime;
-};
-
-#ifdef CONFIG_BOOT_TRACER
-/* Append the trace on the ring-buffer */
-extern void trace_boot(struct boot_trace *it, initcall_t fn);
-
-/* Tells the tracer that smp_pre_initcall is finished.
- * So we can start the tracing
- */
-extern void start_boot_trace(void);
-
-/* Resume the tracing of other necessary events
- * such as sched switches
- */
-extern void enable_boot_trace(void);
-
-/* Suspend this tracing. Actually, only sched_switches tracing have
- * to be suspended. Initcalls doesn't need it.)
- */
-extern void disable_boot_trace(void);
-#else
-static inline void trace_boot(struct boot_trace *it, initcall_t fn) { }
-static inline void start_boot_trace(void) { }
-static inline void enable_boot_trace(void) { }
-static inline void disable_boot_trace(void) { }
-#endif
-
-
-
 #endif /* _LINUX_FTRACE_H */

commit caf4b323b02a16c92fba449952ac6515ddc76d7a
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Nov 11 07:03:45 2008 +0100

    tracing, x86: add low level support for ftrace return tracing
    
    Impact: add infrastructure for function-return tracing
    
    Add low level support for ftrace return tracing.
    
    This plug-in stores return addresses on the thread_info structure of
    the current task.
    
    The index of the current return address is initialized when the task
    is the first one (init) and when a process forks (the child). It is
    not needed when a task does a sys_execve because after this syscall,
    it still needs to return on the kernel functions it called.
    
    Note that the code of return_to_handler has been suggested by Steven
    Rostedt as almost all of the ideas of improvements in this V3.
    
    For purpose of security, arch/x86/kernel/process_32.c is not traced
    because __switch_to() changes the current task during its execution.
    That could cause inconsistency in the stored return address of this
    function even if I didn't have any crash after testing with tracing on
    this function enabled.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 1f5608c11023..dcbbf72a88b1 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -267,6 +267,26 @@ ftrace_init_module(unsigned long *start, unsigned long *end) { }
 #endif
 
 
+/*
+ * Structure that defines a return function trace.
+ */
+struct ftrace_retfunc {
+	unsigned long ret; /* Return address */
+	unsigned long func; /* Current function */
+	unsigned long long calltime;
+	unsigned long long rettime;
+};
+
+#ifdef CONFIG_FUNCTION_RET_TRACER
+/* Type of a callback handler of tracing return function */
+typedef void (*trace_function_return_t)(struct ftrace_retfunc *);
+
+extern void register_ftrace_return(trace_function_return_t func);
+/* The current handler in use */
+extern trace_function_return_t ftrace_function_return;
+extern void unregister_ftrace_return(void);
+#endif
+
 /*
  * Structure which defines the trace of an initcall.
  * You don't have to fill the func field since it is

commit a6b0786f7f83bcc4d414a2977aaebe2941ebe1de
Merge: 3e03fb7f1da2 6a60dd121c5b 072ba49838b4
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sat Nov 8 09:34:35 2008 +0100

    Merge branches 'tracing/ftrace', 'tracing/fastboot', 'tracing/nmisafe' and 'tracing/urgent' into tracing/core

commit 6a60dd121c5b6c2d827e99b38c1326f2600c3891
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Nov 6 15:55:21 2008 -0500

    ftrace: split out hardirq ftrace code into own header
    
    Impact: moving of function prototypes into own header file
    
    ftrace.h is too big of a file for hardirq.h, and some archs will fail
    to build because of the include dependencies not being met.
    
    This patch pulls out the required prototypes for hardirq.h into a smaller
    and safer ftrace_irq.h file.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 0ad1b48aea69..1b340e3fa249 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -104,9 +104,6 @@ extern void ftrace_release(void *start, unsigned long size);
 
 extern void ftrace_disable_daemon(void);
 extern void ftrace_enable_daemon(void);
-extern void ftrace_nmi_enter(void);
-extern void ftrace_nmi_exit(void);
-
 #else
 # define skip_trace(ip)				({ 0; })
 # define ftrace_force_update()			({ 0; })
@@ -114,8 +111,6 @@ extern void ftrace_nmi_exit(void);
 # define ftrace_disable_daemon()		do { } while (0)
 # define ftrace_enable_daemon()			do { } while (0)
 static inline void ftrace_release(void *start, unsigned long size) { }
-static inline void ftrace_nmi_enter(void) { }
-static inline void ftrace_nmi_exit(void) { }
 #endif /* CONFIG_DYNAMIC_FTRACE */
 
 /* totally disable ftrace - can not re-enable after this */

commit 0f04870148ecb825133bc2733f473b1c5773ac0b
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed Nov 5 16:05:44 2008 -0500

    ftrace: soft tracing stop and start
    
    Impact: add way to quickly start stop tracing from the kernel
    
    This patch adds a soft stop and start to the trace. This simply
    disables function tracing via the ftrace_disabled flag, and
    disables the trace buffers to prevent recording. The tracing
    code may still be executed, but the trace will not be recorded.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 794ab907dbfe..7a75fc6d41f4 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -216,6 +216,9 @@ static inline void __ftrace_enabled_restore(int enabled)
 #ifdef CONFIG_TRACING
 extern int ftrace_dump_on_oops;
 
+extern void tracing_start(void);
+extern void tracing_stop(void);
+
 extern void
 ftrace_special(unsigned long arg1, unsigned long arg2, unsigned long arg3);
 
@@ -246,6 +249,8 @@ ftrace_special(unsigned long arg1, unsigned long arg2, unsigned long arg3) { }
 static inline int
 ftrace_printk(const char *fmt, ...) __attribute__ ((format (printf, 1, 0)));
 
+static inline void tracing_start(void) { }
+static inline void tracing_stop(void) { }
 static inline int
 ftrace_printk(const char *fmt, ...)
 {

commit 60a7ecf42661f2b22168751298592da6ee210c9e
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed Nov 5 16:05:44 2008 -0500

    ftrace: add quick function trace stop
    
    Impact: quick start and stop of function tracer
    
    This patch adds a way to disable the function tracer quickly without
    the need to run kstop_machine. It adds a new variable called
    function_trace_stop which will stop the calls to functions from mcount
    when set.  This is just an on/off switch and does not handle recursion
    like preempt_disable().
    
    It's main purpose is to help other tracers/debuggers start and stop tracing
    fuctions without the need to call kstop_machine.
    
    The config option HAVE_FUNCTION_TRACE_MCOUNT_TEST is added for archs
    that implement the testing of the function_trace_stop in the mcount
    arch dependent code. Otherwise, the test is done in the C code.
    
    x86 is the only arch at the moment that supports this.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 4642959e5bda..794ab907dbfe 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -23,6 +23,34 @@ struct ftrace_ops {
 	struct ftrace_ops *next;
 };
 
+extern int function_trace_stop;
+
+/**
+ * ftrace_stop - stop function tracer.
+ *
+ * A quick way to stop the function tracer. Note this an on off switch,
+ * it is not something that is recursive like preempt_disable.
+ * This does not disable the calling of mcount, it only stops the
+ * calling of functions from mcount.
+ */
+static inline void ftrace_stop(void)
+{
+	function_trace_stop = 1;
+}
+
+/**
+ * ftrace_start - start the function tracer.
+ *
+ * This function is the inverse of ftrace_stop. This does not enable
+ * the function tracing if the function tracer is disabled. This only
+ * sets the function tracer flag to continue calling the functions
+ * from mcount.
+ */
+static inline void ftrace_start(void)
+{
+	function_trace_stop = 0;
+}
+
 /*
  * The ftrace_ops must be a static and should also
  * be read_mostly.  These functions do modify read_mostly variables
@@ -41,6 +69,8 @@ extern void ftrace_stub(unsigned long a0, unsigned long a1);
 # define unregister_ftrace_function(ops) do { } while (0)
 # define clear_ftrace_function(ops) do { } while (0)
 static inline void ftrace_kill(void) { }
+static inline void ftrace_stop(void) { }
+static inline void ftrace_start(void) { }
 #endif /* CONFIG_FUNCTION_TRACER */
 
 #ifdef CONFIG_DYNAMIC_FTRACE

commit 71566a0d161edec70361b7f90f6e54af6a6d5d05
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Fri Oct 31 12:57:20 2008 +0100

    tracing/fastboot: Enable boot tracing only during initcalls
    
    Impact: modify boot tracer
    
    We used to disable the initcall tracing at a specified time (IE: end
    of builtin initcalls). But we don't need it anymore. It will be
    stopped when initcalls are finished.
    
    However we want two things:
    
    _Start this tracing only after pre-smp initcalls are finished.
    
    _Since we are planning to trace sched_switches at the same time, we
    want to enable them only during the initcall execution.
    
    For this purpose, this patch introduce two functions to enable/disable
    the sched_switch tracing during boot.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index e46a7b34037c..4642959e5bda 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -234,6 +234,11 @@ ftrace_init_module(unsigned long *start, unsigned long *end) { }
 #endif
 
 
+/*
+ * Structure which defines the trace of an initcall.
+ * You don't have to fill the func field since it is
+ * only used internally by the tracer.
+ */
 struct boot_trace {
 	pid_t			caller;
 	char			func[KSYM_NAME_LEN];
@@ -244,13 +249,28 @@ struct boot_trace {
 };
 
 #ifdef CONFIG_BOOT_TRACER
+/* Append the trace on the ring-buffer */
 extern void trace_boot(struct boot_trace *it, initcall_t fn);
+
+/* Tells the tracer that smp_pre_initcall is finished.
+ * So we can start the tracing
+ */
 extern void start_boot_trace(void);
-extern void stop_boot_trace(void);
+
+/* Resume the tracing of other necessary events
+ * such as sched switches
+ */
+extern void enable_boot_trace(void);
+
+/* Suspend this tracing. Actually, only sched_switches tracing have
+ * to be suspended. Initcalls doesn't need it.)
+ */
+extern void disable_boot_trace(void);
 #else
 static inline void trace_boot(struct boot_trace *it, initcall_t fn) { }
 static inline void start_boot_trace(void) { }
-static inline void stop_boot_trace(void) { }
+static inline void enable_boot_trace(void) { }
+static inline void disable_boot_trace(void) { }
 #endif
 
 

commit 7e5e26a3d8ac4bcadb380073dc9604c07a9a6198
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Fri Oct 31 09:36:38 2008 -0400

    ftrace: fix hardirq header for non ftrace archs
    
    Impact: build fix for non-ftrace architectures
    
    Not all archs implement ftrace, and therefore do not have an asm/ftrace.h.
    This patch corrects the problem.
    
    The ftrace_nmi_enter/exit now must be defined for all archs that implement
    dynamic ftrace. Currently, only x86 does.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index e46a7b34037c..0ad1b48aea69 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -44,7 +44,6 @@ static inline void ftrace_kill(void) { }
 #endif /* CONFIG_FUNCTION_TRACER */
 
 #ifdef CONFIG_DYNAMIC_FTRACE
-
 enum {
 	FTRACE_FL_FREE		= (1 << 0),
 	FTRACE_FL_FAILED	= (1 << 1),
@@ -105,6 +104,8 @@ extern void ftrace_release(void *start, unsigned long size);
 
 extern void ftrace_disable_daemon(void);
 extern void ftrace_enable_daemon(void);
+extern void ftrace_nmi_enter(void);
+extern void ftrace_nmi_exit(void);
 
 #else
 # define skip_trace(ip)				({ 0; })
@@ -113,6 +114,8 @@ extern void ftrace_enable_daemon(void);
 # define ftrace_disable_daemon()		do { } while (0)
 # define ftrace_enable_daemon()			do { } while (0)
 static inline void ftrace_release(void *start, unsigned long size) { }
+static inline void ftrace_nmi_enter(void) { }
+static inline void ftrace_nmi_exit(void) { }
 #endif /* CONFIG_DYNAMIC_FTRACE */
 
 /* totally disable ftrace - can not re-enable after this */

commit 7a895f53cda9d9362c30144e42c124a1ae996b9e
Merge: d9e540762f5c 5d9881ea1440 fd3fdf11d3c6 a26a2a27396c 127cafbb2762 c2c805294600
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Nov 3 10:34:23 2008 +0100

    Merge branches 'tracing/ftrace', 'tracing/markers', 'tracing/mmiotrace', 'tracing/nmisafe', 'tracing/tracepoints' and 'tracing/urgent' into tracing/core

commit a26a2a27396c0a0877aa701f8f92d08ba550a6c9
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Fri Oct 31 00:03:22 2008 -0400

    ftrace: nmi safe code clean ups
    
    Impact: cleanup
    
    This patch cleans up the NMI safe code for dynamic ftrace as suggested
    by Andrew Morton.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 703eb53cfa2b..22240dfe912e 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -74,6 +74,9 @@ extern void ftrace_caller(void);
 extern void ftrace_call(void);
 extern void mcount_call(void);
 
+/* May be defined in arch */
+extern int ftrace_arch_read_dyn_info(char *buf, int size);
+
 /**
  * ftrace_modify_code - modify code segment
  * @ip: the address of the code segment

commit e1e302d8a9ab06ba8d7d5ec503d8996e6cf0eca4
Merge: 944ac4259e39 7f82f000ed03
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Oct 31 00:38:21 2008 +0100

    Merge branch 'linus' into tracing/ftrace

commit 944ac4259e39801c843a915c3da8194ac9af0440
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Thu Oct 23 19:26:08 2008 -0400

    ftrace: ftrace dump on oops control
    
    Impact: add (default-off) dump-trace-on-oops flag
    
    Currently, ftrace is set up to dump its contents to the console if the
    kernel panics or oops. This can be annoying if you have trace data in
    the buffers and you experience an oops, but the trace data is old or
    static.
    
    Usually when you want ftrace to dump its contents is when you are debugging
    your system and you have set up ftrace to trace the events leading to
    an oops.
    
    This patch adds a control variable called "ftrace_dump_on_oops" that will
    enable the ftrace dump to console on oops. This variable is default off
    but a developer can enable it either through the kernel command line
    by adding "ftrace_dump_on_oops" or at run time by setting (or disabling)
    /proc/sys/kernel/ftrace_dump_on_oops.
    
    v2:
    
       Replaced /** with /* as Randy explained that kernel-doc does
        not yet handle variables.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index a3d46151be19..9623b7b9e5a5 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -165,6 +165,8 @@ static inline void __ftrace_enabled_restore(int enabled)
 #endif
 
 #ifdef CONFIG_TRACING
+extern int ftrace_dump_on_oops;
+
 extern void
 ftrace_special(unsigned long arg1, unsigned long arg2, unsigned long arg3);
 

commit 08f5ac906d2c0faf96d608c54a0b03177376da8d
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Thu Oct 23 09:33:07 2008 -0400

    ftrace: remove ftrace hash
    
    The ftrace hash was used by the ftrace_daemon code. The record ip function
    would place the calling address (ip) into the hash. The daemon would later
    read the hash and modify that code.
    
    The hash complicates the code. This patch removes it.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 1c4835f86911..703eb53cfa2b 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -44,8 +44,6 @@ static inline void ftrace_kill(void) { }
 #endif /* CONFIG_FUNCTION_TRACER */
 
 #ifdef CONFIG_DYNAMIC_FTRACE
-# define FTRACE_HASHBITS	10
-# define FTRACE_HASHSIZE	(1<<FTRACE_HASHBITS)
 
 enum {
 	FTRACE_FL_FREE		= (1 << 0),
@@ -58,9 +56,9 @@ enum {
 };
 
 struct dyn_ftrace {
-	struct hlist_node node;
-	unsigned long	  ip; /* address of mcount call-site */
-	unsigned long	  flags;
+	struct list_head	list;
+	unsigned long		ip; /* address of mcount call-site */
+	unsigned long		flags;
 };
 
 int ftrace_force_update(void);

commit 4d296c24326783bff1282ac72f310d8bac8df413
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Thu Oct 23 09:33:06 2008 -0400

    ftrace: remove mcount set
    
    The arch dependent function ftrace_mcount_set was only used by the daemon
    start up code. This patch removes it.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index ac58e94668b7..1c4835f86911 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -71,7 +71,6 @@ extern int ftrace_ip_converted(unsigned long ip);
 extern unsigned char *ftrace_nop_replace(void);
 extern unsigned char *ftrace_call_replace(unsigned long ip, unsigned long addr);
 extern int ftrace_dyn_arch_init(void *data);
-extern int ftrace_mcount_set(unsigned long *data);
 extern int ftrace_update_ftrace_func(ftrace_func_t func);
 extern void ftrace_caller(void);
 extern void ftrace_call(void);

commit 81adbdc029ecc416d56563e7f159100181dd711d
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Thu Oct 23 09:33:02 2008 -0400

    ftrace: only have ftrace_kill atomic
    
    When an anomaly is detected, we need a way to completely disable
    ftrace. Right now we have two functions: ftrace_kill and ftrace_kill_atomic.
    The ftrace_kill tries to do it in a "nice" way by converting everything
    back to a nop.
    
    The "nice" way is dangerous itself, so this patch removes it and only
    has the "atomic" version, which is all that is needed.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 79fa10cbdcfb..ac58e94668b7 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -40,7 +40,7 @@ extern void ftrace_stub(unsigned long a0, unsigned long a1);
 # define register_ftrace_function(ops) do { } while (0)
 # define unregister_ftrace_function(ops) do { } while (0)
 # define clear_ftrace_function(ops) do { } while (0)
-static inline void ftrace_kill_atomic(void) { }
+static inline void ftrace_kill(void) { }
 #endif /* CONFIG_FUNCTION_TRACER */
 
 #ifdef CONFIG_DYNAMIC_FTRACE
@@ -117,7 +117,6 @@ static inline void ftrace_release(void *start, unsigned long size) { }
 
 /* totally disable ftrace - can not re-enable after this */
 void ftrace_kill(void);
-void ftrace_kill_atomic(void);
 
 static inline void tracer_disable(void)
 {

commit 593eb8a2d63e95772a5f22d746f18a997c5ee463
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Thu Oct 23 09:32:59 2008 -0400

    ftrace: return error on failed modified text.
    
    Have the ftrace_modify_code return error values:
    
      -EFAULT on error of reading the address
    
      -EINVAL if what is read does not match what it expected
    
      -EPERM  if the write fails to update after a successful match.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 0e9529589151..79fa10cbdcfb 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -72,13 +72,33 @@ extern unsigned char *ftrace_nop_replace(void);
 extern unsigned char *ftrace_call_replace(unsigned long ip, unsigned long addr);
 extern int ftrace_dyn_arch_init(void *data);
 extern int ftrace_mcount_set(unsigned long *data);
-extern int ftrace_modify_code(unsigned long ip, unsigned char *old_code,
-			      unsigned char *new_code);
 extern int ftrace_update_ftrace_func(ftrace_func_t func);
 extern void ftrace_caller(void);
 extern void ftrace_call(void);
 extern void mcount_call(void);
 
+/**
+ * ftrace_modify_code - modify code segment
+ * @ip: the address of the code segment
+ * @old_code: the contents of what is expected to be there
+ * @new_code: the code to patch in
+ *
+ * This is a very sensitive operation and great care needs
+ * to be taken by the arch.  The operation should carefully
+ * read the location, check to see if what is read is indeed
+ * what we expect it to be, and then on success of the compare,
+ * it should write to the location.
+ *
+ * Return must be:
+ *  0 on success
+ *  -EFAULT on error reading the location
+ *  -EINVAL on a failed compare of the contents
+ *  -EPERM  on error writing to the location
+ * Any other value will be considered a failure.
+ */
+extern int ftrace_modify_code(unsigned long ip, unsigned char *old_code,
+			      unsigned char *new_code);
+
 extern int skip_trace(unsigned long ip);
 
 extern void ftrace_release(void *start, unsigned long size);

commit 606576ce816603d9fe1fb453a88bc6eea16ca709
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Mon Oct 6 19:06:12 2008 -0400

    ftrace: rename FTRACE to FUNCTION_TRACER
    
    Due to confusion between the ftrace infrastructure and the gcc profiling
    tracer "ftrace", this patch renames the config options from FTRACE to
    FUNCTION_TRACER.  The other two names that are offspring from FTRACE
    DYNAMIC_FTRACE and FTRACE_MCOUNT_RECORD will stay the same.
    
    This patch was generated mostly by script, and partially by hand.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index a3d46151be19..0e9529589151 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -8,7 +8,7 @@
 #include <linux/types.h>
 #include <linux/kallsyms.h>
 
-#ifdef CONFIG_FTRACE
+#ifdef CONFIG_FUNCTION_TRACER
 
 extern int ftrace_enabled;
 extern int
@@ -36,12 +36,12 @@ void clear_ftrace_function(void);
 
 extern void ftrace_stub(unsigned long a0, unsigned long a1);
 
-#else /* !CONFIG_FTRACE */
+#else /* !CONFIG_FUNCTION_TRACER */
 # define register_ftrace_function(ops) do { } while (0)
 # define unregister_ftrace_function(ops) do { } while (0)
 # define clear_ftrace_function(ops) do { } while (0)
 static inline void ftrace_kill_atomic(void) { }
-#endif /* CONFIG_FTRACE */
+#endif /* CONFIG_FUNCTION_TRACER */
 
 #ifdef CONFIG_DYNAMIC_FTRACE
 # define FTRACE_HASHBITS	10
@@ -101,7 +101,7 @@ void ftrace_kill_atomic(void);
 
 static inline void tracer_disable(void)
 {
-#ifdef CONFIG_FTRACE
+#ifdef CONFIG_FUNCTION_TRACER
 	ftrace_enabled = 0;
 #endif
 }
@@ -113,7 +113,7 @@ static inline void tracer_disable(void)
  */
 static inline int __ftrace_enabled_save(void)
 {
-#ifdef CONFIG_FTRACE
+#ifdef CONFIG_FUNCTION_TRACER
 	int saved_ftrace_enabled = ftrace_enabled;
 	ftrace_enabled = 0;
 	return saved_ftrace_enabled;
@@ -124,7 +124,7 @@ static inline int __ftrace_enabled_save(void)
 
 static inline void __ftrace_enabled_restore(int enabled)
 {
-#ifdef CONFIG_FTRACE
+#ifdef CONFIG_FUNCTION_TRACER
 	ftrace_enabled = enabled;
 #endif
 }

commit ca538f6bbe583406f941f3041d40c41f9a13d1de
Author: Tim Bird <tim.bird@am.sony.com>
Date:   Thu Oct 9 15:23:05 2008 -0700

    tracing/fastboot: add better resolution to initcall debug/tracing
    
    Change the time resolution for initcall_debug to microseconds, from
    milliseconds.  This is handy to determine which initcalls you want to work
    on for faster booting.
    
    One one of my test machines, over 90% of the initcalls are less than a
    millisecond and (without this patch) these are all reported as 0 msecs.
    Working on the 900 us ones is more important than the 4 us ones.
    
    With 'quiet' on the kernel command line, this adds no significant overhead
    to kernel boot time.
    
    Signed-off-by: Tim Bird <tim.bird@am.sony.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 5812dba4ee24..a3d46151be19 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -215,9 +215,9 @@ ftrace_init_module(unsigned long *start, unsigned long *end) { }
 
 struct boot_trace {
 	pid_t			caller;
-	char 			func[KSYM_NAME_LEN];
+	char			func[KSYM_NAME_LEN];
 	int			result;
-	unsigned long long	duration;
+	unsigned long long	duration;		/* usecs */
 	ktime_t			calltime;
 	ktime_t			rettime;
 };

commit 097d036a2f25eecc42435c57e010aaf4a2eed2d9
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Fri Oct 3 15:39:21 2008 +0200

    tracing/fastboot: only trace non-module initcalls
    
    At this time, only built-in initcalls interest us.
    We can't really produce a relevant graph if we include
    the modules initcall too.
    
    I had good results after this patch (see svg in attachment).
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index ed53265d1f63..5812dba4ee24 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -225,9 +225,11 @@ struct boot_trace {
 #ifdef CONFIG_BOOT_TRACER
 extern void trace_boot(struct boot_trace *it, initcall_t fn);
 extern void start_boot_trace(void);
+extern void stop_boot_trace(void);
 #else
 static inline void trace_boot(struct boot_trace *it, initcall_t fn) { }
 static inline void start_boot_trace(void) { }
+static inline void stop_boot_trace(void) { }
 #endif
 
 

commit eb7fa935274bb233686fdf7a53f40c5d9ee76ed6
Author: Steven Noonan <steven@uplinklabs.net>
Date:   Thu Oct 2 12:00:07 2008 -0700

    ftrace: ktime.h not included in ftrace.h
    
    Including <linux/ktime.h> eliminates the following error:
    
    include/linux/ftrace.h:220: error: expected specifier-qualifier-list
    before 'ktime_t'
    
    Signed-off-by: Steven Noonan <steven@uplinklabs.net>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index deded114dffd..ed53265d1f63 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -3,6 +3,7 @@
 
 #include <linux/linkage.h>
 #include <linux/fs.h>
+#include <linux/ktime.h>
 #include <linux/init.h>
 #include <linux/types.h>
 #include <linux/kallsyms.h>

commit 3e1932ad59726d794a865cc159c0593d54bf0cb6
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Oct 2 17:45:47 2008 +0200

    tracing/fastboot: build fix
    
    fix:
    
     In file included from kernel/sysctl.c:52:
     include/linux/ftrace.h:217: error: 'KSYM_NAME_LEN' undeclared here (not in a function)
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index e672e51c40a9..deded114dffd 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -1,14 +1,14 @@
 #ifndef _LINUX_FTRACE_H
 #define _LINUX_FTRACE_H
 
-#ifdef CONFIG_FTRACE
-
 #include <linux/linkage.h>
 #include <linux/fs.h>
 #include <linux/init.h>
 #include <linux/types.h>
 #include <linux/kallsyms.h>
 
+#ifdef CONFIG_FTRACE
+
 extern int ftrace_enabled;
 extern int
 ftrace_enable_sysctl(struct ctl_table *table, int write,

commit 5601020feb0c3010e9e3e0131e9697ac6a06777b
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Oct 2 13:26:05 2008 +0200

    tracing/fastboot: get the initcall name before it disappears
    
    After some initcall traces, some initcall names may be inconsistent.
    That's because these functions will disappear from the .init section
    and also their name from the symbols table.
    
    So we have to copy the name of the function in a buffer large enough
    during the trace appending. It is not costly for the ring_buffer because
    the number of initcall entries is commonly not really large.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 4455490d91bd..e672e51c40a9 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -7,6 +7,7 @@
 #include <linux/fs.h>
 #include <linux/init.h>
 #include <linux/types.h>
+#include <linux/kallsyms.h>
 
 extern int ftrace_enabled;
 extern int
@@ -213,7 +214,7 @@ ftrace_init_module(unsigned long *start, unsigned long *end) { }
 
 struct boot_trace {
 	pid_t			caller;
-	initcall_t		func;
+	char 			func[KSYM_NAME_LEN];
 	int			result;
 	unsigned long long	duration;
 	ktime_t			calltime;
@@ -221,10 +222,10 @@ struct boot_trace {
 };
 
 #ifdef CONFIG_BOOT_TRACER
-extern void trace_boot(struct boot_trace *it);
+extern void trace_boot(struct boot_trace *it, initcall_t fn);
 extern void start_boot_trace(void);
 #else
-static inline void trace_boot(struct boot_trace *it) { }
+static inline void trace_boot(struct boot_trace *it, initcall_t fn) { }
 static inline void start_boot_trace(void) { }
 #endif
 

commit cb5ab74204a6e2579d1119bf1348eb806526b12b
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Oct 2 12:59:20 2008 +0200

    tracing/fastboot: change the printing of boot tracer according to bootgraph.pl
    
    Change the boot tracer printing to make it parsable for
    the scripts/bootgraph.pl script.
    
    We have now to output two lines for each initcall, according to the
    printk in do_one_initcall() in init/main.c
    We need now the call's time and the return's time.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 91954eb6460f..4455490d91bd 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -216,6 +216,8 @@ struct boot_trace {
 	initcall_t		func;
 	int			result;
 	unsigned long long	duration;
+	ktime_t			calltime;
+	ktime_t			rettime;
 };
 
 #ifdef CONFIG_BOOT_TRACER

commit d13744cd6e3fef373a3fe656ac349b4e7c49ff79
Author: Frédéric Weisbecker <fweisbec@gmail.com>
Date:   Tue Sep 23 11:32:08 2008 +0100

    tracing/ftrace: add the boot tracer
    
    Add the boot/initcall tracer.
    
    It's primary purpose is to be able to trace the initcalls.
    
    It is intended to be used with scripts/bootgraph.pl after some small
    improvements.
    
    Note that it is not active after its init. To avoid tracing (and so
    crashing) before the whole tracing engine init, you have to explicitly
    call start_boot_trace() after do_pre_smp_initcalls() to enable it.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 5de9903645d5..91954eb6460f 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -5,6 +5,8 @@
 
 #include <linux/linkage.h>
 #include <linux/fs.h>
+#include <linux/init.h>
+#include <linux/types.h>
 
 extern int ftrace_enabled;
 extern int
@@ -209,4 +211,21 @@ ftrace_init_module(unsigned long *start, unsigned long *end) { }
 #endif
 
 
+struct boot_trace {
+	pid_t			caller;
+	initcall_t		func;
+	int			result;
+	unsigned long long	duration;
+};
+
+#ifdef CONFIG_BOOT_TRACER
+extern void trace_boot(struct boot_trace *it);
+extern void start_boot_trace(void);
+#else
+static inline void trace_boot(struct boot_trace *it) { }
+static inline void start_boot_trace(void) { }
+#endif
+
+
+
 #endif /* _LINUX_FTRACE_H */

commit c0719e5a4b1ccc04180b7a7b71095c9fb7131919
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Sat Sep 6 01:06:03 2008 -0400

    ftrace: use ftrace_release for all dynamic ftrace functions
    
    ftrace_release is necessary for all uses of dynamic ftrace and not just
    the archs that have CONFIG_FTRACE_MCOUNT_RECORD defined.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 8b4cf38c80d2..5de9903645d5 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -77,8 +77,10 @@ extern void mcount_call(void);
 
 extern int skip_trace(unsigned long ip);
 
-void ftrace_disable_daemon(void);
-void ftrace_enable_daemon(void);
+extern void ftrace_release(void *start, unsigned long size);
+
+extern void ftrace_disable_daemon(void);
+extern void ftrace_enable_daemon(void);
 
 #else
 # define skip_trace(ip)				({ 0; })
@@ -86,6 +88,7 @@ void ftrace_enable_daemon(void);
 # define ftrace_set_filter(buf, len, reset)	do { } while (0)
 # define ftrace_disable_daemon()		do { } while (0)
 # define ftrace_enable_daemon()			do { } while (0)
+static inline void ftrace_release(void *start, unsigned long size) { }
 #endif /* CONFIG_DYNAMIC_FTRACE */
 
 /* totally disable ftrace - can not re-enable after this */
@@ -199,12 +202,10 @@ static inline void ftrace_dump(void) { }
 #ifdef CONFIG_FTRACE_MCOUNT_RECORD
 extern void ftrace_init(void);
 extern void ftrace_init_module(unsigned long *start, unsigned long *end);
-extern void ftrace_release(void *start, unsigned long size);
 #else
 static inline void ftrace_init(void) { }
 static inline void
 ftrace_init_module(unsigned long *start, unsigned long *end) { }
-static inline void ftrace_release(void *start, unsigned long size) { }
 #endif
 
 

commit 3700273586ee6a58b95dd07d9f8a02db4a9b476f
Author: Huang Ying <ying.huang@intel.com>
Date:   Mon Aug 18 16:24:56 2008 +0800

    ftrace: fix incorrect comment style of __ftrace_enabled_save()
    
    This patch fixes incorrect comment style of __ftrace_enabled_save().
    
    Signed-off-by: Huang Ying <ying.huang@intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 36c439927ff1..8b4cf38c80d2 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -99,9 +99,11 @@ static inline void tracer_disable(void)
 #endif
 }
 
-/* Ftrace disable/restore without lock. Some synchronization mechanism
+/*
+ * Ftrace disable/restore without lock. Some synchronization mechanism
  * must be used to prevent ftrace_enabled to be changed between
- * disable/restore. */
+ * disable/restore.
+ */
 static inline int __ftrace_enabled_save(void)
 {
 #ifdef CONFIG_FTRACE

commit c5131ad6c3cbe8f6674993e29a76cecf8deb4384
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Aug 15 18:22:09 2008 +0200

    ftrace: ftrace_kill_atomic() build fix
    
    fix:
    
     kernel/built-in.o: In function `ftrace_dump':
     (.text+0x2e2ea): undefined reference to `ftrace_kill_atomic'
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index ce929cb55435..36c439927ff1 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -36,6 +36,7 @@ extern void ftrace_stub(unsigned long a0, unsigned long a1);
 # define register_ftrace_function(ops) do { } while (0)
 # define unregister_ftrace_function(ops) do { } while (0)
 # define clear_ftrace_function(ops) do { } while (0)
+static inline void ftrace_kill_atomic(void) { }
 #endif /* CONFIG_FTRACE */
 
 #ifdef CONFIG_DYNAMIC_FTRACE

commit 7b928c23fa3e9fa37d1d4ba52ba963f41ee5aae0
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Aug 15 17:48:02 2008 +0200

    ftrace: build fix
    
    fix:
    
     In file included from init/main.c:65:
     include/linux/ftrace.h:166: error: expected ‘,' or ‘;' before ‘{' token
     make[1]: *** [init/main.o] Error 1
     make: *** [init/main.o] Error 2
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index f7fb92045bf0..ce929cb55435 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -183,7 +183,10 @@ extern void ftrace_dump(void);
 static inline void
 ftrace_special(unsigned long arg1, unsigned long arg2, unsigned long arg3) { }
 static inline int
-ftrace_printk(const char *fmt, ...) __attribute__ ((format (printf, 1, 0)))
+ftrace_printk(const char *fmt, ...) __attribute__ ((format (printf, 1, 0)));
+
+static inline int
+ftrace_printk(const char *fmt, ...)
 {
 	return 0;
 }

commit 3f5a54e371ca20b119b73704f6c01b71295c1714
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Wed Jul 30 22:36:46 2008 -0400

    ftrace: dump out ftrace buffers to console on panic
    
    At OLS I had a lot of interest to be able to have the ftrace buffers
    dumped on panic.  Usually one would expect to uses kexec and examine
    the buffers after a new kernel is loaded. But sometimes the resources
    do not permit kdump and kexec, so having an option to still see the
    sequence of events up to the crash is very advantageous.
    
    This patch adds the option to have the ftrace buffers dumped to the
    console in the latency_trace format on a panic. When the option is set,
    the default entries per CPU buffer are lowered to 16384, since the writing
    to the serial (if that is the console) may take an awful long time
    otherwise.
    
    [
     Changes since -v1:
      Got alpine to send correctly (as well as spell check working).
      Removed config option.
      Moved the static variables into ftrace_dump itself.
      Gave printk a log level.
    ]
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 018af16bce5c..f7fb92045bf0 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -178,6 +178,7 @@ ftrace_special(unsigned long arg1, unsigned long arg2, unsigned long arg3);
 extern int
 __ftrace_printk(unsigned long ip, const char *fmt, ...)
 	__attribute__ ((format (printf, 2, 3)));
+extern void ftrace_dump(void);
 #else
 static inline void
 ftrace_special(unsigned long arg1, unsigned long arg2, unsigned long arg3) { }
@@ -186,6 +187,7 @@ ftrace_printk(const char *fmt, ...) __attribute__ ((format (printf, 1, 0)))
 {
 	return 0;
 }
+static inline void ftrace_dump(void) { }
 #endif
 
 #ifdef CONFIG_FTRACE_MCOUNT_RECORD

commit 2f2c99dba2398ef7d9c21f7c793180a50e68b1f0
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Fri Aug 1 16:45:49 2008 -0400

    ftrace: ftrace_printk doc moved
    
    Based on Randy Dunlap's suggestion, the ftrace_printk kernel-doc belongs
    with the ftrace_printk macro that should be used. Not with the
    __ftrace_printk internal function.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Acked-by: Randy Dunlap <randy.dunlap@oracle.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index f53b975e32fa..018af16bce5c 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -157,7 +157,24 @@ static inline void __ftrace_enabled_restore(int enabled)
 #ifdef CONFIG_TRACING
 extern void
 ftrace_special(unsigned long arg1, unsigned long arg2, unsigned long arg3);
-# define ftrace_printk(x...) __ftrace_printk(_THIS_IP_, x)
+
+/**
+ * ftrace_printk - printf formatting in the ftrace buffer
+ * @fmt: the printf format for printing
+ *
+ * Note: __ftrace_printk is an internal function for ftrace_printk and
+ *       the @ip is passed in via the ftrace_printk macro.
+ *
+ * This function allows a kernel developer to debug fast path sections
+ * that printk is not appropriate for. By scattering in various
+ * printk like tracing in the code, a developer can quickly see
+ * where problems are occurring.
+ *
+ * This is intended as a debugging tool for the developer only.
+ * Please refrain from leaving ftrace_printks scattered around in
+ * your code.
+ */
+# define ftrace_printk(fmt...) __ftrace_printk(_THIS_IP_, fmt)
 extern int
 __ftrace_printk(unsigned long ip, const char *fmt, ...)
 	__attribute__ ((format (printf, 2, 3)));

commit dd0e545f061f90099a3dcc13aa77e29c6295cf23
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Fri Aug 1 12:26:41 2008 -0400

    ftrace: printk formatting infrastructure
    
    This patch adds a feature that can help kernel developers debug their
    code using ftrace.
    
      int ftrace_printk(const char *fmt, ...);
    
    This records into the ftrace buffer using printf formatting. The entry
    size in the buffers are still a fixed length. A new type has been added
    that allows for more entries to be used for a single recording.
    
    The start of the print is still the same as the other entries.
    
    It returns the number of characters written to the ftrace buffer.
    
    For example:
    
    Having a module with the following code:
    
    static int __init ftrace_print_test(void)
    {
            ftrace_printk("jiffies are %ld\n", jiffies);
            return 0;
    }
    
    Gives me:
    
      insmod-5441  3...1 7569us : ftrace_print_test: jiffies are 4296626666
    
    for the latency_trace file and:
    
              insmod-5441  [03]  1959.370498: ftrace_print_test jiffies are 4296626666
    
    for the trace file.
    
    Note: Only the infrastructure should go into the kernel. It is to help
    facilitate debugging for other kernel developers. Calls to ftrace_printk
    is not intended to be left in the kernel, and should be frowned upon just
    like scattering printks around in the code.
    
    But having this easily at your fingertips helps the debugging go faster
    and bugs be solved quicker.
    
    Maybe later on, we can hook this with markers and have their printf format
    be sucked into ftrace output.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 6b232a2460c0..f53b975e32fa 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -157,9 +157,18 @@ static inline void __ftrace_enabled_restore(int enabled)
 #ifdef CONFIG_TRACING
 extern void
 ftrace_special(unsigned long arg1, unsigned long arg2, unsigned long arg3);
+# define ftrace_printk(x...) __ftrace_printk(_THIS_IP_, x)
+extern int
+__ftrace_printk(unsigned long ip, const char *fmt, ...)
+	__attribute__ ((format (printf, 2, 3)));
 #else
 static inline void
 ftrace_special(unsigned long arg1, unsigned long arg2, unsigned long arg3) { }
+static inline int
+ftrace_printk(const char *fmt, ...) __attribute__ ((format (printf, 1, 0)))
+{
+	return 0;
+}
 #endif
 
 #ifdef CONFIG_FTRACE_MCOUNT_RECORD
@@ -173,4 +182,5 @@ ftrace_init_module(unsigned long *start, unsigned long *end) { }
 static inline void ftrace_release(void *start, unsigned long size) { }
 #endif
 
+
 #endif /* _LINUX_FTRACE_H */

commit fed1939c64d2288938fdc1c367d49082da65e195
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Thu Aug 14 22:47:19 2008 -0400

    ftrace: remove old pointers to mcount
    
    When a mcount pointer is recorded into a table, it is used to add or
    remove calls to mcount (replacing them with nops). If the code is removed
    via removing a module, the pointers still exist.  At modifying the code
    a check is always made to make sure the code being replaced is the code
    expected. In-other-words, the code being replaced is compared to what
    it is expected to be before being replaced.
    
    There is a very small chance that the code being replaced just happens
    to look like code that calls mcount (very small since the call to mcount
    is relative). To remove this chance, this patch adds ftrace_release to
    allow module unloading to remove the pointers to mcount within the module.
    
    Another change for init calls is made to not trace calls marked with
    __init. The tracing can not be started until after init is done anyway.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 4936489f9ed8..6b232a2460c0 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -165,10 +165,12 @@ ftrace_special(unsigned long arg1, unsigned long arg2, unsigned long arg3) { }
 #ifdef CONFIG_FTRACE_MCOUNT_RECORD
 extern void ftrace_init(void);
 extern void ftrace_init_module(unsigned long *start, unsigned long *end);
+extern void ftrace_release(void *start, unsigned long size);
 #else
 static inline void ftrace_init(void) { }
 static inline void
 ftrace_init_module(unsigned long *start, unsigned long *end) { }
+static inline void ftrace_release(void *start, unsigned long size) { }
 #endif
 
 #endif /* _LINUX_FTRACE_H */

commit 90d595fe5ca4b685465c068907e6e554760abea8
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Thu Aug 14 15:45:09 2008 -0400

    ftrace: enable mcount recording for modules
    
    This patch enables the loading of the __mcount_section of modules and
    changing all the callers of mcount into nops.
    
    The modification is done before the init_module function is called, so
    again, we do not need to use kstop_machine to make these changes.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index d4d6ab453b78..4936489f9ed8 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -164,8 +164,11 @@ ftrace_special(unsigned long arg1, unsigned long arg2, unsigned long arg3) { }
 
 #ifdef CONFIG_FTRACE_MCOUNT_RECORD
 extern void ftrace_init(void);
+extern void ftrace_init_module(unsigned long *start, unsigned long *end);
 #else
 static inline void ftrace_init(void) { }
+static inline void
+ftrace_init_module(unsigned long *start, unsigned long *end) { }
 #endif
 
 #endif /* _LINUX_FTRACE_H */

commit 68bf21aa15c85d2e9b623dcda2b1ed8893275fa1
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Thu Aug 14 15:45:08 2008 -0400

    ftrace: mcount call site on boot nops core
    
    This is the infrastructure to the converting the mcount call sites
    recorded by the __mcount_loc section into nops on boot. It also allows
    for using these sites to enable tracing as normal. When the __mcount_loc
    section is used, the "ftraced" kernel thread is disabled.
    
    This uses the current infrastructure to record the mcount call sites
    as well as convert them to nops. The mcount function is kept as a stub
    on boot up and not converted to the ftrace_record_ip function. We use the
    ftrace_record_ip to only record from the table.
    
    This patch does not handle modules. That comes with a later patch.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index bb384068272e..d4d6ab453b78 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -162,4 +162,10 @@ static inline void
 ftrace_special(unsigned long arg1, unsigned long arg2, unsigned long arg3) { }
 #endif
 
+#ifdef CONFIG_FTRACE_MCOUNT_RECORD
+extern void ftrace_init(void);
+#else
+static inline void ftrace_init(void) { }
+#endif
+
 #endif /* _LINUX_FTRACE_H */

commit 9bdeb7b5d34f197dea7859d24475943395ffea5e
Author: Huang Ying <ying.huang@intel.com>
Date:   Fri Aug 15 00:40:25 2008 -0700

    kexec jump: __ftrace_enabled_save/restore
    
    Add __ftrace_enabled_save/restore, used to disable ftrace for a while.
    Now, this is used by kexec jump, which need a version without lock, for
    general situation, a locked version should be used.
    
    Signed-off-by: Huang Ying <ying.huang@intel.com>
    Cc: Pavel Machek <pavel@ucw.cz>
    Cc: "Rafael J. Wysocki" <rjw@sisk.pl>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Vivek Goyal <vgoyal@redhat.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index f368d041e02d..bb384068272e 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -98,6 +98,27 @@ static inline void tracer_disable(void)
 #endif
 }
 
+/* Ftrace disable/restore without lock. Some synchronization mechanism
+ * must be used to prevent ftrace_enabled to be changed between
+ * disable/restore. */
+static inline int __ftrace_enabled_save(void)
+{
+#ifdef CONFIG_FTRACE
+	int saved_ftrace_enabled = ftrace_enabled;
+	ftrace_enabled = 0;
+	return saved_ftrace_enabled;
+#else
+	return 0;
+#endif
+}
+
+static inline void __ftrace_enabled_restore(int enabled)
+{
+#ifdef CONFIG_FTRACE
+	ftrace_enabled = enabled;
+#endif
+}
+
 #ifdef CONFIG_FRAME_POINTER
 /* TODO: need to fix this for ARM */
 # define CALLER_ADDR0 ((unsigned long)__builtin_return_address(0))

commit a2bb6a3d85ef3124cd336403a95abc0540d3fbe2
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Thu Jul 10 20:58:15 2008 -0400

    ftrace: add ftrace_kill_atomic
    
    It has been suggested that I add a way to disable the function tracer
    on an oops. This code adds a ftrace_kill_atomic. It is not meant to be
    used in normal situations. It will disable the ftrace tracer, but will
    not perform the nice shutdown that requires scheduling.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Cc: Steven Rostedt <srostedt@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 3121b95443d9..f368d041e02d 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -89,6 +89,7 @@ void ftrace_enable_daemon(void);
 
 /* totally disable ftrace - can not re-enable after this */
 void ftrace_kill(void);
+void ftrace_kill_atomic(void);
 
 static inline void tracer_disable(void)
 {

commit ecea656d1d5e912d2f3d332657ea4a6d8380f891
Author: Abhishek Sagar <sagar.abhishek@gmail.com>
Date:   Sat Jun 21 23:47:53 2008 +0530

    ftrace: freeze kprobe'd records
    
    Let records identified as being kprobe'd be marked as "frozen". The trouble
    with records which have a kprobe installed on their mcount call-site is
    that they don't get updated. So if such a function which is currently being
    traced gets its tracing disabled due to a new filter rule (or because it
    was added to the notrace list) then it won't be updated and continue being
    traced. This patch allows scanning of all frozen records during tracing to
    check if they should be traced.
    
    Signed-off-by: Abhishek Sagar <sagar.abhishek@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 366098d591de..3121b95443d9 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -49,6 +49,7 @@ enum {
 	FTRACE_FL_ENABLED	= (1 << 3),
 	FTRACE_FL_NOTRACE	= (1 << 4),
 	FTRACE_FL_CONVERTED	= (1 << 5),
+	FTRACE_FL_FROZEN	= (1 << 6),
 };
 
 struct dyn_ftrace {
@@ -73,15 +74,18 @@ extern void ftrace_caller(void);
 extern void ftrace_call(void);
 extern void mcount_call(void);
 
+extern int skip_trace(unsigned long ip);
+
 void ftrace_disable_daemon(void);
 void ftrace_enable_daemon(void);
 
 #else
+# define skip_trace(ip)				({ 0; })
 # define ftrace_force_update()			({ 0; })
 # define ftrace_set_filter(buf, len, reset)	do { } while (0)
 # define ftrace_disable_daemon()		do { } while (0)
 # define ftrace_enable_daemon()			do { } while (0)
-#endif
+#endif /* CONFIG_DYNAMIC_FTRACE */
 
 /* totally disable ftrace - can not re-enable after this */
 void ftrace_kill(void);

commit 395a59d0f8e86bb39cd700c3d185d30c670bb958
Author: Abhishek Sagar <sagar.abhishek@gmail.com>
Date:   Sat Jun 21 23:47:27 2008 +0530

    ftrace: store mcount address in rec->ip
    
    Record the address of the mcount call-site. Currently all archs except sparc64
    record the address of the instruction following the mcount call-site. Some
    general cleanups are entailed. Storing mcount addresses in rec->ip enables
    looking them up in the kprobe hash table later on to check if they're kprobe'd.
    
    Signed-off-by: Abhishek Sagar <sagar.abhishek@gmail.com>
    Cc: davem@davemloft.net
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 20e14d0093c7..366098d591de 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -31,7 +31,6 @@ int unregister_ftrace_function(struct ftrace_ops *ops);
 void clear_ftrace_function(void);
 
 extern void ftrace_stub(unsigned long a0, unsigned long a1);
-extern void mcount(void);
 
 #else /* !CONFIG_FTRACE */
 # define register_ftrace_function(ops) do { } while (0)
@@ -54,7 +53,7 @@ enum {
 
 struct dyn_ftrace {
 	struct hlist_node node;
-	unsigned long	  ip;
+	unsigned long	  ip; /* address of mcount call-site */
 	unsigned long	  flags;
 };
 

commit 0eb967012ea15e6e8cfab483d9fa37bc602d400c
Author: Abhishek Sagar <sagar.abhishek@gmail.com>
Date:   Sun Jun 1 21:47:30 2008 +0530

    ftrace: prevent freeing of all failed updates
    
    Prevent freeing of records which cause problems and correspond to function from
    core kernel text. A new flag, FTRACE_FL_CONVERTED is used to mark a record
    as "converted". All other records are patched lazily to NOPs. Failed records
    now also remain on frace_hash table. Each invocation of ftrace_record_ip now
    checks whether the traced function has ever been recorded (including past
    failures) and doesn't re-record it again.
    
    Signed-off-by: Abhishek Sagar <sagar.abhishek@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 623819433ed5..20e14d0093c7 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -49,6 +49,7 @@ enum {
 	FTRACE_FL_FILTER	= (1 << 2),
 	FTRACE_FL_ENABLED	= (1 << 3),
 	FTRACE_FL_NOTRACE	= (1 << 4),
+	FTRACE_FL_CONVERTED	= (1 << 5),
 };
 
 struct dyn_ftrace {

commit ad90c0e3ce8d20d6873b57e36181ef6d7a0097fe
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Tue May 27 20:48:37 2008 -0400

    ftrace: user update and disable dynamic ftrace daemon
    
    In dynamic ftrace, the mcount function starts off pointing to a stub
    function that just returns.
    
    On start up, the call to the stub is modified to point to a "record_ip"
    function. The job of the record_ip function is to add the function to
    a pre-allocated hash list. If the function is already there, it simply is
    ignored, otherwise it is added to the list.
    
    Later, a ftraced daemon wakes up and calls kstop_machine if any functions
    have been recorded, and changes the calls to the recorded functions to
    a simple nop.  If no functions were recorded, the daemon goes back to sleep.
    
    The daemon wakes up once a second to see if it needs to update any newly
    recorded functions into nops.  Usually it does not, but if a lot of code
    has been executed for the first time in the kernel, the ftraced daemon
    will call kstop_machine to update those into nops.
    
    The problem currently is that there's no way to stop the daemon from doing
    this, and it can cause unneeded latencies (800us which for some is bothersome).
    
    This patch adds a new file /debugfs/tracing/ftraced_enabled. If the daemon
    is active, reading this will return "enabled\n" and "disabled\n" when the
    daemon is not running. To disable the daemon, the user can echo "0" or
    "disable" into this file, and "1" or "enable" to re-enable the daemon.
    
    Since the daemon is used to convert the functions into nops to increase
    the performance of the system, I also added that anytime something is
    written into the ftraced_enabled file, kstop_machine will run if there
    are new functions that have been detected that need to be converted.
    
    This way the user can disable the daemon but still be able to control the
    conversion of the mcount calls to nops by simply,
    
      "echo 0 > /debugfs/tracing/ftraced_enabled"
    
    when they need to do more conversions.
    
    To see the number of converted functions:
    
      "cat /debugfs/tracing/dyn_ftrace_total_info"
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index b482fe88bc04..623819433ed5 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -72,9 +72,15 @@ extern int ftrace_update_ftrace_func(ftrace_func_t func);
 extern void ftrace_caller(void);
 extern void ftrace_call(void);
 extern void mcount_call(void);
+
+void ftrace_disable_daemon(void);
+void ftrace_enable_daemon(void);
+
 #else
 # define ftrace_force_update()			({ 0; })
 # define ftrace_set_filter(buf, len, reset)	do { } while (0)
+# define ftrace_disable_daemon()		do { } while (0)
+# define ftrace_enable_daemon()			do { } while (0)
 #endif
 
 /* totally disable ftrace - can not re-enable after this */

commit b1829d2705daa7cb72eb1e08bdc8b7e9fad34266
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed May 28 01:22:08 2008 +0200

    ftrace: fix merge
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index ffbbd54a720e..b482fe88bc04 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -122,7 +122,7 @@ static inline void tracer_disable(void)
 # define trace_preempt_off(a0, a1)		do { } while (0)
 #endif
 
-#ifdef CONFIG_CONTEXT_SWITCH_TRACER
+#ifdef CONFIG_TRACING
 extern void
 ftrace_special(unsigned long arg1, unsigned long arg2, unsigned long arg3);
 #else

commit 41c52c0db9607e59f90da7da5309489fa06e887f
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Thu May 22 11:46:33 2008 -0400

    ftrace: set_ftrace_notrace feature
    
    While debugging latencies in the RT kernel, I found that it would be nice
    to be able to filter away functions from the trace than just to filter
    on functions.
    
    I added a new interface to the debugfs tracing directory called
    
      set_ftrace_notrace
    
    When dynamic frace is enabled, this lets you filter away functions that will
    not be recorded in the trace. It is similar to adding 'notrace' to those
    functions but by doing it without recompiling the kernel.
    
    Here's how set_ftrace_filter and set_ftrace_notrace interact. Remember, if
    set_ftrace_filter is set, it removes all functions from the trace execpt for
    those listed in the set_ftrace_filter. set_ftrace_notrace will prevent those
    functions from being traced.
    
    If you were to set one function in both set_ftrace_filter and
    set_ftrace_notrace and that function was the same, then you would end up
    with an empty trace.
    
    the set of functions to trace is:
    
      set_ftrace_filter == empty then
    
         all functions not in set_ftrace_notrace
    
      else
    
         set of the set_ftrace_filter and not in set of set_ftrace_notrace.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 922e23d0196f..ffbbd54a720e 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -48,6 +48,7 @@ enum {
 	FTRACE_FL_FAILED	= (1 << 1),
 	FTRACE_FL_FILTER	= (1 << 2),
 	FTRACE_FL_ENABLED	= (1 << 3),
+	FTRACE_FL_NOTRACE	= (1 << 4),
 };
 
 struct dyn_ftrace {

commit 489f139614596cbc956a06f5e4bb41288e276fe3
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Feb 25 13:38:05 2008 +0100

    ftrace: fix build bug
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 911d5d80b49f..922e23d0196f 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -106,16 +106,16 @@ static inline void tracer_disable(void)
 #endif
 
 #ifdef CONFIG_IRQSOFF_TRACER
-  extern void notrace time_hardirqs_on(unsigned long a0, unsigned long a1);
-  extern void notrace time_hardirqs_off(unsigned long a0, unsigned long a1);
+  extern void time_hardirqs_on(unsigned long a0, unsigned long a1);
+  extern void time_hardirqs_off(unsigned long a0, unsigned long a1);
 #else
 # define time_hardirqs_on(a0, a1)		do { } while (0)
 # define time_hardirqs_off(a0, a1)		do { } while (0)
 #endif
 
 #ifdef CONFIG_PREEMPT_TRACER
-  extern void notrace trace_preempt_on(unsigned long a0, unsigned long a1);
-  extern void notrace trace_preempt_off(unsigned long a0, unsigned long a1);
+  extern void trace_preempt_on(unsigned long a0, unsigned long a1);
+  extern void trace_preempt_off(unsigned long a0, unsigned long a1);
 #else
 # define trace_preempt_on(a0, a1)		do { } while (0)
 # define trace_preempt_off(a0, a1)		do { } while (0)

commit d49dbf33f0bf8748ee3662b973eb57e60525d622
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri May 16 10:41:53 2008 +0200

    ftrace: fix include file dependency
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 017ab44d572a..911d5d80b49f 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -4,6 +4,7 @@
 #ifdef CONFIG_FTRACE
 
 #include <linux/linkage.h>
+#include <linux/fs.h>
 
 extern int ftrace_enabled;
 extern int

commit 74f4e369fc5b52433ad824cef32d3bf1304549be
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon May 12 21:21:15 2008 +0200

    ftrace: stacktrace fix
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 0d3714e7110b..017ab44d572a 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -120,4 +120,12 @@ static inline void tracer_disable(void)
 # define trace_preempt_off(a0, a1)		do { } while (0)
 #endif
 
+#ifdef CONFIG_CONTEXT_SWITCH_TRACER
+extern void
+ftrace_special(unsigned long arg1, unsigned long arg2, unsigned long arg3);
+#else
+static inline void
+ftrace_special(unsigned long arg1, unsigned long arg2, unsigned long arg3) { }
+#endif
+
 #endif /* _LINUX_FTRACE_H */

commit 86387f7ee5d3273ff4859e2c64ce656639b6ca65
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon May 12 21:20:51 2008 +0200

    ftrace: add stack tracing
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 08fbef1744cc..0d3714e7110b 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -93,6 +93,7 @@ static inline void tracer_disable(void)
 # define CALLER_ADDR3 ((unsigned long)__builtin_return_address(3))
 # define CALLER_ADDR4 ((unsigned long)__builtin_return_address(4))
 # define CALLER_ADDR5 ((unsigned long)__builtin_return_address(5))
+# define CALLER_ADDR6 ((unsigned long)__builtin_return_address(6))
 #else
 # define CALLER_ADDR0 ((unsigned long)__builtin_return_address(0))
 # define CALLER_ADDR1 0UL
@@ -100,6 +101,7 @@ static inline void tracer_disable(void)
 # define CALLER_ADDR3 0UL
 # define CALLER_ADDR4 0UL
 # define CALLER_ADDR5 0UL
+# define CALLER_ADDR6 0UL
 #endif
 
 #ifdef CONFIG_IRQSOFF_TRACER

commit aeaee8a2c9cb4489f166ca0e39c568e8254faaa6
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon May 12 21:20:49 2008 +0200

    ftrace: build fix
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 4650a3160b7f..08fbef1744cc 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -58,9 +58,6 @@ struct dyn_ftrace {
 int ftrace_force_update(void);
 void ftrace_set_filter(unsigned char *buf, int len, int reset);
 
-/* totally disable ftrace - can not re-enable after this */
-void ftrace_kill(void);
-
 /* defined in arch */
 extern int ftrace_ip_converted(unsigned long ip);
 extern unsigned char *ftrace_nop_replace(void);
@@ -74,10 +71,13 @@ extern void ftrace_caller(void);
 extern void ftrace_call(void);
 extern void mcount_call(void);
 #else
-# define ftrace_force_update() ({ 0; })
-# define ftrace_set_filter(buf, len, reset) do { } while (0)
+# define ftrace_force_update()			({ 0; })
+# define ftrace_set_filter(buf, len, reset)	do { } while (0)
 #endif
 
+/* totally disable ftrace - can not re-enable after this */
+void ftrace_kill(void);
+
 static inline void tracer_disable(void)
 {
 #ifdef CONFIG_FTRACE

commit 4eebcc81a33fbc45e28542b50197ed7b3c486d90
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon May 12 21:20:48 2008 +0200

    ftrace: disable tracing on failure
    
    Since ftrace touches practically every function. If we detect any
    anomaly, we want to fully disable ftrace. This patch adds code
    to try shutdown ftrace as much as possible without doing any more
    harm is something is detected not quite correct.
    
    This only kills ftrace, this patch does have checks for other parts of
    the tracer (irqsoff, wakeup, etc.).
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 61e757bd2350..4650a3160b7f 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -58,6 +58,9 @@ struct dyn_ftrace {
 int ftrace_force_update(void);
 void ftrace_set_filter(unsigned char *buf, int len, int reset);
 
+/* totally disable ftrace - can not re-enable after this */
+void ftrace_kill(void);
+
 /* defined in arch */
 extern int ftrace_ip_converted(unsigned long ip);
 extern unsigned char *ftrace_nop_replace(void);

commit 37ad508419f0fdfda7b378756eb1f35cfd26d96d
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon May 12 21:20:48 2008 +0200

    ftrace - fix dynamic ftrace memory leak
    
    The ftrace dynamic function update allocates a record to store the
    instruction pointers that are being modified. If the modified
    instruction pointer fails to update, then the record is marked as
    failed and nothing more is done.
    
    Worse, if the modification fails, but the record ip function is still
    called, it will allocate a new record and try again. In just a matter
    of time, will this cause a serious memory leak and crash the system.
    
    This patch plugs this memory leak. When a record fails, it is
    included back into the pool of records to be used. Now a record may
    fail over and over again, but the number of allocated records will
    not increase.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index a842d96c6343..61e757bd2350 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -43,9 +43,10 @@ extern void mcount(void);
 # define FTRACE_HASHSIZE	(1<<FTRACE_HASHBITS)
 
 enum {
-	FTRACE_FL_FAILED	= (1 << 0),
-	FTRACE_FL_FILTER	= (1 << 1),
-	FTRACE_FL_ENABLED	= (1 << 2),
+	FTRACE_FL_FREE		= (1 << 0),
+	FTRACE_FL_FAILED	= (1 << 1),
+	FTRACE_FL_FILTER	= (1 << 2),
+	FTRACE_FL_ENABLED	= (1 << 3),
 };
 
 struct dyn_ftrace {

commit 77a2b37d227483fe52aead242652aee406c25bf0
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon May 12 21:20:45 2008 +0200

    ftrace: startup tester on dynamic tracing.
    
    This patch adds a startup self test on dynamic code modification
    and filters. The test filters on a specific function, makes sure that
    no other function is traced, exectutes the function, then makes sure that
    the function is traced.
    
    This patch also fixes a slight bug with the ftrace selftest, where
    tracer_enabled was not being set.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 953a36d6a199..a842d96c6343 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -55,6 +55,7 @@ struct dyn_ftrace {
 };
 
 int ftrace_force_update(void);
+void ftrace_set_filter(unsigned char *buf, int len, int reset);
 
 /* defined in arch */
 extern int ftrace_ip_converted(unsigned long ip);
@@ -70,6 +71,7 @@ extern void ftrace_call(void);
 extern void mcount_call(void);
 #else
 # define ftrace_force_update() ({ 0; })
+# define ftrace_set_filter(buf, len, reset) do { } while (0)
 #endif
 
 static inline void tracer_disable(void)

commit c7aafc549766b87819285d3480648fc652a47bc4
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon May 12 21:20:45 2008 +0200

    ftrace: cleanups
    
    factor out code and clean it up.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 2c1670c65236..953a36d6a199 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -69,7 +69,7 @@ extern void ftrace_caller(void);
 extern void ftrace_call(void);
 extern void mcount_call(void);
 #else
-# define ftrace_force_update() do { } while (0)
+# define ftrace_force_update() ({ 0; })
 #endif
 
 static inline void tracer_disable(void)

commit e1c08bdd9fa73e44096e5a82c0d5928b04ab02c8
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon May 12 21:20:44 2008 +0200

    ftrace: force recording
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index a42390c1d6e1..2c1670c65236 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -54,6 +54,8 @@ struct dyn_ftrace {
 	unsigned long	  flags;
 };
 
+int ftrace_force_update(void);
+
 /* defined in arch */
 extern int ftrace_ip_converted(unsigned long ip);
 extern unsigned char *ftrace_nop_replace(void);
@@ -66,6 +68,8 @@ extern int ftrace_update_ftrace_func(ftrace_func_t func);
 extern void ftrace_caller(void);
 extern void ftrace_call(void);
 extern void mcount_call(void);
+#else
+# define ftrace_force_update() do { } while (0)
 #endif
 
 static inline void tracer_disable(void)

commit f43fdad8627fec2d21df92799b254dceb66c9c3c
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon May 12 21:20:43 2008 +0200

    ftrace: fix kexec
    
    disable the tracer while kexec pulls the rug from under the old
    kernel.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index f5911d2d42c3..a42390c1d6e1 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -68,6 +68,13 @@ extern void ftrace_call(void);
 extern void mcount_call(void);
 #endif
 
+static inline void tracer_disable(void)
+{
+#ifdef CONFIG_FTRACE
+	ftrace_enabled = 0;
+#endif
+}
+
 #ifdef CONFIG_FRAME_POINTER
 /* TODO: need to fix this for ARM */
 # define CALLER_ADDR0 ((unsigned long)__builtin_return_address(0))

commit 5072c59fd45e9976d02ee6f18c7336ef97623cbc
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon May 12 21:20:43 2008 +0200

    ftrace: add filter select functions to trace
    
    This patch adds two files to the debugfs system:
    
     /debugfs/tracing/available_filter_functions
    
    and
    
     /debugfs/tracing/set_ftrace_filter
    
    The available_filter_functions lists all functions that has been
    recorded by the ftraced that has called the ftrace_record_ip function.
    This is to allow users to see what functions have been converted
    to nops and can be enabled for tracing.
    
    To enable functions, simply echo the names (whitespace delimited)
    into set_ftrace_filter. Simple wildcards are also allowed.
    
    echo 'scheduler' > /debugfs/tracing/set_ftrace_filter
    
    Will have only the scheduler be activated when tracing is enabled.
    
    echo 'sched_*' > /debugfs/tracing/set_ftrace_filter
    
    Will have only the functions starting with 'sched_' be activated.
    
    echo '*lock' > /debugfs/tracing/set_ftrace_filter
    
    Will have only functions ending with 'lock' be activated.
    
    echo '*lock*' > /debugfs/tracing/set_ftrace_filter
    
    Will have only functions with 'lock' in its name be activated.
    
    Note: 'sched*lock' will not work. The only wildcards that are
    allowed is an asterisk and the beginning and or end of the string
    passed in.
    
    Multiple names can be passed in with whitespace delimited:
    
    echo 'scheduler *lock *acpi*' > /debugfs/tracing/set_ftrace_filter
    
    is also the same as:
    
    echo 'scheduler' > /debugfs/tracing/set_ftrace_filter
    echo '*lock' >> /debugfs/tracing/set_ftrace_filter
    echo '*acpi*' >> /debugfs/tracing/set_ftrace_filter
    
    Appending does just that. It appends to the list.
    
    To disable all filters simply echo an empty line in:
    
    echo > /debugfs/tracing/set_ftrace_filter
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index b0dd0093058f..f5911d2d42c3 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -43,7 +43,9 @@ extern void mcount(void);
 # define FTRACE_HASHSIZE	(1<<FTRACE_HASHBITS)
 
 enum {
-	FTRACE_FL_FAILED	= (1<<0),
+	FTRACE_FL_FAILED	= (1 << 0),
+	FTRACE_FL_FILTER	= (1 << 1),
+	FTRACE_FL_ENABLED	= (1 << 2),
 };
 
 struct dyn_ftrace {

commit d61f82d06672f57fca410da6f7fffd15867db622
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon May 12 21:20:43 2008 +0200

    ftrace: use dynamic patching for updating mcount calls
    
    This patch replaces the indirect call to the mcount function
    pointer with a direct call that will be patched by the
    dynamic ftrace routines.
    
    On boot up, the mcount function calls the ftace_stub function.
    When the dynamic ftrace code is initialized, the ftrace_stub
    is replaced with a call to the ftrace_record_ip, which records
    the instruction pointers of the locations that call it.
    
    Later, the ftraced daemon will call kstop_machine and patch all
    the locations to nops.
    
    When a ftrace is enabled, the original calls to mcount will now
    be set top call ftrace_caller, which will do a direct call
    to the registered ftrace function. This direct call is also patched
    when the function that should be called is updated.
    
    All patching is performed by a kstop_machine routine to prevent any
    type of race conditions that is associated with modifying code
    on the fly.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index d509ad6c9cb8..b0dd0093058f 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -56,9 +56,14 @@ struct dyn_ftrace {
 extern int ftrace_ip_converted(unsigned long ip);
 extern unsigned char *ftrace_nop_replace(void);
 extern unsigned char *ftrace_call_replace(unsigned long ip, unsigned long addr);
-extern int ftrace_dyn_arch_init(void);
+extern int ftrace_dyn_arch_init(void *data);
+extern int ftrace_mcount_set(unsigned long *data);
 extern int ftrace_modify_code(unsigned long ip, unsigned char *old_code,
 			      unsigned char *new_code);
+extern int ftrace_update_ftrace_func(ftrace_func_t func);
+extern void ftrace_caller(void);
+extern void ftrace_call(void);
+extern void mcount_call(void);
 #endif
 
 #ifdef CONFIG_FRAME_POINTER

commit 3c1720f00bb619302ba19d55986ab565e74d06db
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon May 12 21:20:43 2008 +0200

    ftrace: move memory management out of arch code
    
    This patch moves the memory management of the ftrace
    records out of the arch code and into the generic code
    making the arch code simpler.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index ccd8537dbdb7..d509ad6c9cb8 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -42,19 +42,23 @@ extern void mcount(void);
 # define FTRACE_HASHBITS	10
 # define FTRACE_HASHSIZE	(1<<FTRACE_HASHBITS)
 
+enum {
+	FTRACE_FL_FAILED	= (1<<0),
+};
+
 struct dyn_ftrace {
 	struct hlist_node node;
 	unsigned long	  ip;
+	unsigned long	  flags;
 };
 
 /* defined in arch */
-extern struct dyn_ftrace *
-ftrace_alloc_shutdown_node(unsigned long ip);
-extern int ftrace_shutdown_arch_init(void);
-extern void ftrace_code_disable(struct dyn_ftrace *rec);
-extern void ftrace_startup_code(void);
-extern void ftrace_shutdown_code(void);
-extern void ftrace_shutdown_replenish(void);
+extern int ftrace_ip_converted(unsigned long ip);
+extern unsigned char *ftrace_nop_replace(void);
+extern unsigned char *ftrace_call_replace(unsigned long ip, unsigned long addr);
+extern int ftrace_dyn_arch_init(void);
+extern int ftrace_modify_code(unsigned long ip, unsigned char *old_code,
+			      unsigned char *new_code);
 #endif
 
 #ifdef CONFIG_FRAME_POINTER

commit b0fc494fae96a7089f3651cb451f461c7291244c
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon May 12 21:20:43 2008 +0200

    ftrace: add ftrace_enabled sysctl to disable mcount function
    
    This patch adds back the sysctl ftrace_enabled. This time it is
    defaulted to on, if DYNAMIC_FTRACE is configured. When ftrace_enabled
    is disabled, the ftrace function is set to the stub return.
    
    If DYNAMIC_FTRACE is also configured, on ftrace_enabled = 0,
    the registered ftrace functions will all be set to jmps, but no more
    new calls to ftrace recording (used to find the ftrace calling sites)
    will be called.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 90dbc0ee2046..ccd8537dbdb7 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -5,6 +5,12 @@
 
 #include <linux/linkage.h>
 
+extern int ftrace_enabled;
+extern int
+ftrace_enable_sysctl(struct ctl_table *table, int write,
+		     struct file *filp, void __user *buffer, size_t *lenp,
+		     loff_t *ppos);
+
 typedef void (*ftrace_func_t)(unsigned long ip, unsigned long parent_ip);
 
 struct ftrace_ops {

commit 3d0833953e1b98b79ddf491dd49229eef9baeac1
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon May 12 21:20:42 2008 +0200

    ftrace: dynamic enabling/disabling of function calls
    
    This patch adds a feature to dynamically replace the ftrace code
    with the jmps to allow a kernel with ftrace configured to run
    as fast as it can without it configured.
    
    The way this works, is on bootup (if ftrace is enabled), a ftrace
    function is registered to record the instruction pointer of all
    places that call the function.
    
    Later, if there's still any code to patch, a kthread is awoken
    (rate limited to at most once a second) that performs a stop_machine,
    and replaces all the code that was called with a jmp over the call
    to ftrace. It only replaces what was found the previous time. Typically
    the system reaches equilibrium quickly after bootup and there's no code
    patching needed at all.
    
    e.g.
    
      call ftrace  /* 5 bytes */
    
    is replaced with
    
      jmp 3f  /* jmp is 2 bytes and we jump 3 forward */
    3:
    
    When we want to enable ftrace for function tracing, the IP recording
    is removed, and stop_machine is called again to replace all the locations
    of that were recorded back to the call of ftrace.  When it is disabled,
    we replace the code back to the jmp.
    
    Allocation is done by the kthread. If the ftrace recording function is
    called, and we don't have any record slots available, then we simply
    skip that call. Once a second a new page (if needed) is allocated for
    recording new ftrace function calls.  A large batch is allocated at
    boot up to get most of the calls there.
    
    Because we do this via stop_machine, we don't have to worry about another
    CPU executing a ftrace call as we modify it. But we do need to worry
    about NMI's so all functions that might be called via nmi must be
    annotated with notrace_nmi. When this code is configured in, the NMI code
    will not call notrace.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 740c97dcf9cb..90dbc0ee2046 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -32,6 +32,24 @@ extern void mcount(void);
 # define clear_ftrace_function(ops) do { } while (0)
 #endif /* CONFIG_FTRACE */
 
+#ifdef CONFIG_DYNAMIC_FTRACE
+# define FTRACE_HASHBITS	10
+# define FTRACE_HASHSIZE	(1<<FTRACE_HASHBITS)
+
+struct dyn_ftrace {
+	struct hlist_node node;
+	unsigned long	  ip;
+};
+
+/* defined in arch */
+extern struct dyn_ftrace *
+ftrace_alloc_shutdown_node(unsigned long ip);
+extern int ftrace_shutdown_arch_init(void);
+extern void ftrace_code_disable(struct dyn_ftrace *rec);
+extern void ftrace_startup_code(void);
+extern void ftrace_shutdown_code(void);
+extern void ftrace_shutdown_replenish(void);
+#endif
 
 #ifdef CONFIG_FRAME_POINTER
 /* TODO: need to fix this for ARM */

commit 6cd8a4bb2f97527a9ceb30bc77ea4e959c6a95e3
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon May 12 21:20:42 2008 +0200

    ftrace: trace preempt off critical timings
    
    Add preempt off timings. A lot of kernel core code is taken from the RT patch
    latency trace that was written by Ingo Molnar.
    
    This adds "preemptoff" and "preemptirqsoff" to /debugfs/tracing/available_tracers
    
    Now instead of just tracing irqs off, preemption off can be selected
    to be recorded.
    
    When this is selected, it shares the same files as irqs off timings.
    One can either trace preemption off, irqs off, or one or the other off.
    
    By echoing "preemptoff" into /debugfs/tracing/current_tracer, recording
    of preempt off only is performed. "irqsoff" will only record the time
    irqs are disabled, but "preemptirqsoff" will take the total time irqs
    or preemption are disabled. Runtime switching of these options is now
    supported by simpling echoing in the appropriate trace name into
    /debugfs/tracing/current_tracer.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 0a20445dcbcc..740c97dcf9cb 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -58,4 +58,12 @@ extern void mcount(void);
 # define time_hardirqs_off(a0, a1)		do { } while (0)
 #endif
 
+#ifdef CONFIG_PREEMPT_TRACER
+  extern void notrace trace_preempt_on(unsigned long a0, unsigned long a1);
+  extern void notrace trace_preempt_off(unsigned long a0, unsigned long a1);
+#else
+# define trace_preempt_on(a0, a1)		do { } while (0)
+# define trace_preempt_off(a0, a1)		do { } while (0)
+#endif
+
 #endif /* _LINUX_FTRACE_H */

commit 81d68a96a39844853b37f20cc8282d9b65b78ef3
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon May 12 21:20:42 2008 +0200

    ftrace: trace irq disabled critical timings
    
    This patch adds latency tracing for critical timings
    (how long interrupts are disabled for).
    
     "irqsoff" is added to /debugfs/tracing/available_tracers
    
    Note:
      tracing_max_latency
        also holds the max latency for irqsoff (in usecs).
       (default to large number so one must start latency tracing)
    
      tracing_thresh
        threshold (in usecs) to always print out if irqs off
        is detected to be longer than stated here.
        If irq_thresh is non-zero, then max_irq_latency
        is ignored.
    
    Here's an example of a trace with ftrace_enabled = 0
    
    =======
    preemption latency trace v1.1.5 on 2.6.24-rc7
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    --------------------------------------------------------------------
     latency: 100 us, #3/3, CPU#1 | (M:rt VP:0, KP:0, SP:0 HP:0 #P:2)
        -----------------
        | task: swapper-0 (uid:0 nice:0 policy:0 rt_prio:0)
        -----------------
     => started at: _spin_lock_irqsave+0x2a/0xb7
     => ended at:   _spin_unlock_irqrestore+0x32/0x5f
    
                     _------=> CPU#
                    / _-----=> irqs-off
                   | / _----=> need-resched
                   || / _---=> hardirq/softirq
                   ||| / _--=> preempt-depth
                   |||| /
                   |||||     delay
       cmd     pid ||||| time  |   caller
          \   /    |||||   \   |   /
     swapper-0     1d.s3    0us+: _spin_lock_irqsave+0x2a/0xb7 (e1000_update_stats+0x47/0x64c [e1000])
     swapper-0     1d.s3  100us : _spin_unlock_irqrestore+0x32/0x5f (e1000_update_stats+0x641/0x64c [e1000])
     swapper-0     1d.s3  100us : trace_hardirqs_on_caller+0x75/0x89 (_spin_unlock_irqrestore+0x32/0x5f)
    
    vim:ft=help
    =======
    
    And this is a trace with ftrace_enabled == 1
    
    =======
    preemption latency trace v1.1.5 on 2.6.24-rc7
    --------------------------------------------------------------------
     latency: 102 us, #12/12, CPU#1 | (M:rt VP:0, KP:0, SP:0 HP:0 #P:2)
        -----------------
        | task: swapper-0 (uid:0 nice:0 policy:0 rt_prio:0)
        -----------------
     => started at: _spin_lock_irqsave+0x2a/0xb7
     => ended at:   _spin_unlock_irqrestore+0x32/0x5f
    
                     _------=> CPU#
                    / _-----=> irqs-off
                   | / _----=> need-resched
                   || / _---=> hardirq/softirq
                   ||| / _--=> preempt-depth
                   |||| /
                   |||||     delay
       cmd     pid ||||| time  |   caller
          \   /    |||||   \   |   /
     swapper-0     1dNs3    0us+: _spin_lock_irqsave+0x2a/0xb7 (e1000_update_stats+0x47/0x64c [e1000])
     swapper-0     1dNs3   46us : e1000_read_phy_reg+0x16/0x225 [e1000] (e1000_update_stats+0x5e2/0x64c [e1000])
     swapper-0     1dNs3   46us : e1000_swfw_sync_acquire+0x10/0x99 [e1000] (e1000_read_phy_reg+0x49/0x225 [e1000])
     swapper-0     1dNs3   46us : e1000_get_hw_eeprom_semaphore+0x12/0xa6 [e1000] (e1000_swfw_sync_acquire+0x36/0x99 [e1000])
     swapper-0     1dNs3   47us : __const_udelay+0x9/0x47 (e1000_read_phy_reg+0x116/0x225 [e1000])
     swapper-0     1dNs3   47us+: __delay+0x9/0x50 (__const_udelay+0x45/0x47)
     swapper-0     1dNs3   97us : preempt_schedule+0xc/0x84 (__delay+0x4e/0x50)
     swapper-0     1dNs3   98us : e1000_swfw_sync_release+0xc/0x55 [e1000] (e1000_read_phy_reg+0x211/0x225 [e1000])
     swapper-0     1dNs3   99us+: e1000_put_hw_eeprom_semaphore+0x9/0x35 [e1000] (e1000_swfw_sync_release+0x50/0x55 [e1000])
     swapper-0     1dNs3  101us : _spin_unlock_irqrestore+0xe/0x5f (e1000_update_stats+0x641/0x64c [e1000])
     swapper-0     1dNs3  102us : _spin_unlock_irqrestore+0x32/0x5f (e1000_update_stats+0x641/0x64c [e1000])
     swapper-0     1dNs3  102us : trace_hardirqs_on_caller+0x75/0x89 (_spin_unlock_irqrestore+0x32/0x5f)
    
    vim:ft=help
    =======
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index db8a5e7abe41..0a20445dcbcc 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -50,4 +50,12 @@ extern void mcount(void);
 # define CALLER_ADDR5 0UL
 #endif
 
+#ifdef CONFIG_IRQSOFF_TRACER
+  extern void notrace time_hardirqs_on(unsigned long a0, unsigned long a1);
+  extern void notrace time_hardirqs_off(unsigned long a0, unsigned long a1);
+#else
+# define time_hardirqs_on(a0, a1)		do { } while (0)
+# define time_hardirqs_off(a0, a1)		do { } while (0)
+#endif
+
 #endif /* _LINUX_FTRACE_H */

commit 352ad25aa4a189c667cb2af333948d34692a2d27
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon May 12 21:20:42 2008 +0200

    ftrace: tracer for scheduler wakeup latency
    
    This patch adds the tracer that tracks the wakeup latency of the
    highest priority waking task.
    
      "wakeup" is added to /debugfs/tracing/available_tracers
    
    Also added to /debugfs/tracing
    
      tracing_max_latency
         holds the current max latency for the wakeup
    
      wakeup_thresh
         if set to other than zero, a log will be recorded
         for every wakeup that takes longer than the number
         entered in here (usecs for all counters)
         (deletes previous trace)
    
    Examples:
    
      (with ftrace_enabled = 0)
    
    ============
    preemption latency trace v1.1.5 on 2.6.24-rc8
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    --------------------------------------------------------------------
     latency: 26 us, #2/2, CPU#1 | (M:rt VP:0, KP:0, SP:0 HP:0 #P:2)
        -----------------
        | task: migration/0-3 (uid:0 nice:-5 policy:1 rt_prio:99)
        -----------------
    
                     _------=> CPU#
                    / _-----=> irqs-off
                   | / _----=> need-resched
                   || / _---=> hardirq/softirq
                   ||| / _--=> preempt-depth
                   |||| /
                   |||||     delay
       cmd     pid ||||| time  |   caller
          \   /    |||||   \   |   /
       quilt-8551  0d..3    0us+: wake_up_process+0x15/0x17 <ffffffff80233e80> (sched_exec+0xc9/0x100 <ffffffff80235343>)
       quilt-8551  0d..4   26us : sched_switch_callback+0x73/0x81 <ffffffff80338d2f> (schedule+0x483/0x6d5 <ffffffff8048b3ee>)
    
    vim:ft=help
    ============
    
      (with ftrace_enabled = 1)
    
    ============
    preemption latency trace v1.1.5 on 2.6.24-rc8
    --------------------------------------------------------------------
     latency: 36 us, #45/45, CPU#0 | (M:rt VP:0, KP:0, SP:0 HP:0 #P:2)
        -----------------
        | task: migration/1-5 (uid:0 nice:-5 policy:1 rt_prio:99)
        -----------------
    
                     _------=> CPU#
                    / _-----=> irqs-off
                   | / _----=> need-resched
                   || / _---=> hardirq/softirq
                   ||| / _--=> preempt-depth
                   |||| /
                   |||||     delay
       cmd     pid ||||| time  |   caller
          \   /    |||||   \   |   /
        bash-10653 1d..3    0us : wake_up_process+0x15/0x17 <ffffffff80233e80> (sched_exec+0xc9/0x100 <ffffffff80235343>)
        bash-10653 1d..3    1us : try_to_wake_up+0x271/0x2e7 <ffffffff80233dcf> (sub_preempt_count+0xc/0x7a <ffffffff8023309e>)
        bash-10653 1d..2    2us : try_to_wake_up+0x296/0x2e7 <ffffffff80233df4> (update_rq_clock+0x9/0x20 <ffffffff802303f3>)
        bash-10653 1d..2    2us : update_rq_clock+0x1e/0x20 <ffffffff80230408> (__update_rq_clock+0xc/0x90 <ffffffff80230366>)
        bash-10653 1d..2    3us : __update_rq_clock+0x1b/0x90 <ffffffff80230375> (sched_clock+0x9/0x29 <ffffffff80214529>)
        bash-10653 1d..2    4us : try_to_wake_up+0x2a6/0x2e7 <ffffffff80233e04> (activate_task+0xc/0x3f <ffffffff8022ffca>)
        bash-10653 1d..2    4us : activate_task+0x2d/0x3f <ffffffff8022ffeb> (enqueue_task+0xe/0x66 <ffffffff8022ff66>)
        bash-10653 1d..2    5us : enqueue_task+0x5b/0x66 <ffffffff8022ffb3> (enqueue_task_rt+0x9/0x3c <ffffffff80233351>)
        bash-10653 1d..2    6us : try_to_wake_up+0x2ba/0x2e7 <ffffffff80233e18> (check_preempt_wakeup+0x12/0x99 <ffffffff80234f84>)
    [...]
        bash-10653 1d..5   33us : tracing_record_cmdline+0xcf/0xd4 <ffffffff80338aad> (_spin_unlock+0x9/0x33 <ffffffff8048d3ec>)
        bash-10653 1d..5   34us : _spin_unlock+0x19/0x33 <ffffffff8048d3fc> (sub_preempt_count+0xc/0x7a <ffffffff8023309e>)
        bash-10653 1d..4   35us : wakeup_sched_switch+0x65/0x2ff <ffffffff80339f66> (_spin_lock_irqsave+0xc/0xa9 <ffffffff8048d08b>)
        bash-10653 1d..4   35us : _spin_lock_irqsave+0x19/0xa9 <ffffffff8048d098> (add_preempt_count+0xe/0x77 <ffffffff8023311a>)
        bash-10653 1d..4   36us : sched_switch_callback+0x73/0x81 <ffffffff80338d2f> (schedule+0x483/0x6d5 <ffffffff8048b3ee>)
    
    vim:ft=help
    ============
    
    The [...] was added here to not waste your email box space.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index b96ef14c249a..db8a5e7abe41 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -5,10 +5,6 @@
 
 #include <linux/linkage.h>
 
-#define CALLER_ADDR0 ((unsigned long)__builtin_return_address(0))
-#define CALLER_ADDR1 ((unsigned long)__builtin_return_address(1))
-#define CALLER_ADDR2 ((unsigned long)__builtin_return_address(2))
-
 typedef void (*ftrace_func_t)(unsigned long ip, unsigned long parent_ip);
 
 struct ftrace_ops {
@@ -35,4 +31,23 @@ extern void mcount(void);
 # define unregister_ftrace_function(ops) do { } while (0)
 # define clear_ftrace_function(ops) do { } while (0)
 #endif /* CONFIG_FTRACE */
+
+
+#ifdef CONFIG_FRAME_POINTER
+/* TODO: need to fix this for ARM */
+# define CALLER_ADDR0 ((unsigned long)__builtin_return_address(0))
+# define CALLER_ADDR1 ((unsigned long)__builtin_return_address(1))
+# define CALLER_ADDR2 ((unsigned long)__builtin_return_address(2))
+# define CALLER_ADDR3 ((unsigned long)__builtin_return_address(3))
+# define CALLER_ADDR4 ((unsigned long)__builtin_return_address(4))
+# define CALLER_ADDR5 ((unsigned long)__builtin_return_address(5))
+#else
+# define CALLER_ADDR0 ((unsigned long)__builtin_return_address(0))
+# define CALLER_ADDR1 0UL
+# define CALLER_ADDR2 0UL
+# define CALLER_ADDR3 0UL
+# define CALLER_ADDR4 0UL
+# define CALLER_ADDR5 0UL
+#endif
+
 #endif /* _LINUX_FTRACE_H */

commit 16444a8a40d4c7b4f6de34af0cae1f76a4f6c901
Author: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
Date:   Mon May 12 21:20:42 2008 +0200

    ftrace: add basic support for gcc profiler instrumentation
    
    If CONFIG_FTRACE is selected and /proc/sys/kernel/ftrace_enabled is
    set to a non-zero value the ftrace routine will be called everytime
    we enter a kernel function that is not marked with the "notrace"
    attribute.
    
    The ftrace routine will then call a registered function if a function
    happens to be registered.
    
    [ This code has been highly hacked by Steven Rostedt and Ingo Molnar,
      so don't blame Arnaldo for all of this ;-) ]
    
    Update:
      It is now possible to register more than one ftrace function.
      If only one ftrace function is registered, that will be the
      function that ftrace calls directly. If more than one function
      is registered, then ftrace will call a function that will loop
      through the functions to call.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
new file mode 100644
index 000000000000..b96ef14c249a
--- /dev/null
+++ b/include/linux/ftrace.h
@@ -0,0 +1,38 @@
+#ifndef _LINUX_FTRACE_H
+#define _LINUX_FTRACE_H
+
+#ifdef CONFIG_FTRACE
+
+#include <linux/linkage.h>
+
+#define CALLER_ADDR0 ((unsigned long)__builtin_return_address(0))
+#define CALLER_ADDR1 ((unsigned long)__builtin_return_address(1))
+#define CALLER_ADDR2 ((unsigned long)__builtin_return_address(2))
+
+typedef void (*ftrace_func_t)(unsigned long ip, unsigned long parent_ip);
+
+struct ftrace_ops {
+	ftrace_func_t	  func;
+	struct ftrace_ops *next;
+};
+
+/*
+ * The ftrace_ops must be a static and should also
+ * be read_mostly.  These functions do modify read_mostly variables
+ * so use them sparely. Never free an ftrace_op or modify the
+ * next pointer after it has been registered. Even after unregistering
+ * it, the next pointer may still be used internally.
+ */
+int register_ftrace_function(struct ftrace_ops *ops);
+int unregister_ftrace_function(struct ftrace_ops *ops);
+void clear_ftrace_function(void);
+
+extern void ftrace_stub(unsigned long a0, unsigned long a1);
+extern void mcount(void);
+
+#else /* !CONFIG_FTRACE */
+# define register_ftrace_function(ops) do { } while (0)
+# define unregister_ftrace_function(ops) do { } while (0)
+# define clear_ftrace_function(ops) do { } while (0)
+#endif /* CONFIG_FTRACE */
+#endif /* _LINUX_FTRACE_H */
